{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름이 너무 짧으면 훈련 X..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/minji/.local/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n",
      "2020-11-13 01:52:24.040474: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n",
      "2020-11-13 01:52:25.621340: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2020-11-13 01:52:25.718600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:52:25.720593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:52:25.720666: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-13 01:52:25.722967: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-13 01:52:25.724943: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-13 01:52:25.725347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-13 01:52:25.727766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-13 01:52:25.729024: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-13 01:52:25.733952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-13 01:52:25.741918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-13 01:52:25.742339: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-13 01:52:25.767586: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3499910000 Hz\n",
      "2020-11-13 01:52:25.768379: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x72ffd90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-13 01:52:25.768411: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-13 01:52:26.192419: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7303880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-13 01:52:26.192464: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-13 01:52:26.192475: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-13 01:52:26.193844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:52:26.194941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:52:26.195027: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-13 01:52:26.195063: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-13 01:52:26.195081: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-13 01:52:26.195097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-13 01:52:26.195112: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-13 01:52:26.195127: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-13 01:52:26.195144: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-13 01:52:26.199090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-13 01:52:26.199194: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-13 01:52:27.887592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-11-13 01:52:27.887653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \n",
      "2020-11-13 01:52:27.887665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y \n",
      "2020-11-13 01:52:27.887673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N \n",
      "2020-11-13 01:52:27.891153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22418 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:65:00.0, compute capability: 7.5)\n",
      "2020-11-13 01:52:27.892749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22466 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:b3:00.0, compute capability: 7.5)\n",
      "using cached model\n",
      "using cached model\n",
      "0\n",
      "using cached model\n",
      "tokenizer ending\n",
      "(71885,)\n",
      "Read_Dataset ok\n",
      "KoGPT-2 Transfer Learning Start\n",
      "epoch no.0 train no.0  loss = 8.28269 avg_loss = 8.28269\n",
      "epoch no.0 train no.10  loss = 4.89772 avg_loss = 6.39124\n",
      "epoch no.0 train no.20  loss = 6.25278 avg_loss = 6.28035\n",
      "epoch no.0 train no.30  loss = 5.00738 avg_loss = 6.14038\n",
      "epoch no.0 train no.40  loss = 5.70924 avg_loss = 6.05556\n",
      "epoch no.0 train no.50  loss = 4.76112 avg_loss = 5.84173\n",
      "epoch no.0 train no.60  loss = 7.53188 avg_loss = 5.87898\n",
      "epoch no.0 train no.70  loss = 5.36932 avg_loss = 5.84294\n",
      "epoch no.0 train no.80  loss = 5.59106 avg_loss = 5.72677\n",
      "epoch no.0 train no.90  loss = 5.13192 avg_loss = 5.62128\n",
      "epoch no.0 train no.100  loss = 3.96750 avg_loss = 5.56336\n",
      "epoch no.0 train no.110  loss = 5.38238 avg_loss = 5.62733\n",
      "epoch no.0 train no.120  loss = 4.41919 avg_loss = 5.59543\n",
      "epoch no.0 train no.130  loss = 4.69661 avg_loss = 5.58982\n",
      "epoch no.0 train no.140  loss = 6.78282 avg_loss = 5.55492\n",
      "epoch no.0 train no.150  loss = 5.21386 avg_loss = 5.54778\n",
      "epoch no.0 train no.160  loss = 3.35415 avg_loss = 5.51365\n",
      "epoch no.0 train no.170  loss = 4.52024 avg_loss = 5.53866\n",
      "epoch no.0 train no.180  loss = 3.00426 avg_loss = 5.48003\n",
      "epoch no.0 train no.190  loss = 3.89611 avg_loss = 5.44734\n",
      "epoch no.0 train no.200  loss = 4.18548 avg_loss = 5.39176\n",
      "epoch no.0 train no.210  loss = 5.00864 avg_loss = 5.35784\n",
      "epoch no.0 train no.220  loss = 5.14469 avg_loss = 5.31822\n",
      "epoch no.0 train no.230  loss = 3.71528 avg_loss = 5.28153\n",
      "epoch no.0 train no.240  loss = 5.79159 avg_loss = 5.23861\n",
      "epoch no.0 train no.250  loss = 3.48786 avg_loss = 5.15419\n",
      "epoch no.0 train no.260  loss = 6.10122 avg_loss = 5.19654\n",
      "epoch no.0 train no.270  loss = 3.37899 avg_loss = 5.11807\n",
      "epoch no.0 train no.280  loss = 5.72242 avg_loss = 5.10676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.290  loss = 3.22765 avg_loss = 5.02232\n",
      "epoch no.0 train no.300  loss = 3.62799 avg_loss = 5.02391\n",
      "epoch no.0 train no.310  loss = 3.51965 avg_loss = 4.98954\n",
      "epoch no.0 train no.320  loss = 5.17246 avg_loss = 4.95243\n",
      "epoch no.0 train no.330  loss = 3.20898 avg_loss = 4.88214\n",
      "epoch no.0 train no.340  loss = 4.37707 avg_loss = 4.87286\n",
      "epoch no.0 train no.350  loss = 4.15639 avg_loss = 4.82378\n",
      "epoch no.0 train no.360  loss = 5.80727 avg_loss = 4.89874\n",
      "epoch no.0 train no.370  loss = 5.49379 avg_loss = 4.93885\n",
      "epoch no.0 train no.380  loss = 4.59380 avg_loss = 4.92089\n",
      "epoch no.0 train no.390  loss = 5.29272 avg_loss = 4.92489\n",
      "epoch no.0 train no.400  loss = 5.23180 avg_loss = 4.85469\n",
      "epoch no.0 train no.410  loss = 3.68182 avg_loss = 4.89144\n",
      "epoch no.0 train no.420  loss = 3.37597 avg_loss = 4.87339\n",
      "epoch no.0 train no.430  loss = 4.32142 avg_loss = 4.86368\n",
      "epoch no.0 train no.440  loss = 3.22587 avg_loss = 4.89776\n",
      "epoch no.0 train no.450  loss = 3.79730 avg_loss = 4.86238\n",
      "epoch no.0 train no.460  loss = 4.23796 avg_loss = 4.86927\n",
      "epoch no.0 train no.470  loss = 4.81631 avg_loss = 4.89671\n",
      "epoch no.0 train no.480  loss = 4.71191 avg_loss = 4.85822\n",
      "epoch no.0 train no.490  loss = 4.18039 avg_loss = 4.87069\n",
      "epoch no.0 train no.500  loss = 4.78813 avg_loss = 4.87515\n",
      "epoch no.0 train no.510  loss = 5.89077 avg_loss = 4.89734\n",
      "epoch no.0 train no.520  loss = 4.12355 avg_loss = 4.93627\n",
      "epoch no.0 train no.530  loss = 6.63417 avg_loss = 4.96006\n",
      "epoch no.0 train no.540  loss = 5.33091 avg_loss = 4.96076\n",
      "epoch no.0 train no.550  loss = 5.63776 avg_loss = 4.94276\n",
      "epoch no.0 train no.560  loss = 5.05630 avg_loss = 4.90601\n",
      "epoch no.0 train no.570  loss = 5.13324 avg_loss = 4.91192\n",
      "epoch no.0 train no.580  loss = 4.52422 avg_loss = 4.88043\n",
      "epoch no.0 train no.590  loss = 4.38953 avg_loss = 4.88179\n",
      "epoch no.0 train no.600  loss = 4.19062 avg_loss = 4.85612\n",
      "epoch no.0 train no.610  loss = 4.45147 avg_loss = 4.75644\n",
      "epoch no.0 train no.620  loss = 5.22827 avg_loss = 4.75798\n",
      "epoch no.0 train no.630  loss = 2.79467 avg_loss = 4.73926\n",
      "epoch no.0 train no.640  loss = 4.24644 avg_loss = 4.72869\n",
      "epoch no.0 train no.650  loss = 4.97423 avg_loss = 4.72717\n",
      "epoch no.0 train no.660  loss = 5.18092 avg_loss = 4.79096\n",
      "epoch no.0 train no.670  loss = 4.84681 avg_loss = 4.81241\n",
      "epoch no.0 train no.680  loss = 5.29390 avg_loss = 4.80567\n",
      "epoch no.0 train no.690  loss = 4.94538 avg_loss = 4.78172\n",
      "epoch no.0 train no.700  loss = 2.85382 avg_loss = 4.69046\n",
      "epoch no.0 train no.710  loss = 3.95074 avg_loss = 4.66031\n",
      "epoch no.0 train no.720  loss = 4.31280 avg_loss = 4.60698\n",
      "epoch no.0 train no.730  loss = 5.10106 avg_loss = 4.60379\n",
      "epoch no.0 train no.740  loss = 5.40221 avg_loss = 4.67431\n",
      "epoch no.0 train no.750  loss = 7.68611 avg_loss = 4.71557\n",
      "epoch no.0 train no.760  loss = 4.43982 avg_loss = 4.64809\n",
      "epoch no.0 train no.770  loss = 5.22250 avg_loss = 4.62051\n",
      "epoch no.0 train no.780  loss = 4.06345 avg_loss = 4.69397\n",
      "epoch no.0 train no.790  loss = 4.69174 avg_loss = 4.69142\n",
      "epoch no.0 train no.800  loss = 5.95714 avg_loss = 4.69346\n",
      "epoch no.0 train no.810  loss = 3.77327 avg_loss = 4.73727\n",
      "epoch no.0 train no.820  loss = 5.07587 avg_loss = 4.74646\n",
      "epoch no.0 train no.830  loss = 4.74321 avg_loss = 4.75895\n",
      "epoch no.0 train no.840  loss = 4.49104 avg_loss = 4.72585\n",
      "epoch no.0 train no.850  loss = 3.76263 avg_loss = 4.71598\n",
      "epoch no.0 train no.860  loss = 3.15364 avg_loss = 4.68815\n",
      "epoch no.0 train no.870  loss = 3.97033 avg_loss = 4.69490\n",
      "epoch no.0 train no.880  loss = 4.23865 avg_loss = 4.69901\n",
      "epoch no.0 train no.890  loss = 3.71867 avg_loss = 4.65951\n",
      "epoch no.0 train no.900  loss = 3.92557 avg_loss = 4.63273\n",
      "epoch no.0 train no.910  loss = 3.58883 avg_loss = 4.61949\n",
      "epoch no.0 train no.920  loss = 6.43633 avg_loss = 4.65295\n",
      "epoch no.0 train no.930  loss = 3.15467 avg_loss = 4.61505\n",
      "epoch no.0 train no.940  loss = 2.18013 avg_loss = 4.64617\n",
      "epoch no.0 train no.950  loss = 4.47773 avg_loss = 4.61301\n",
      "epoch no.0 train no.960  loss = 4.51295 avg_loss = 4.58431\n",
      "epoch no.0 train no.970  loss = 4.26375 avg_loss = 4.60034\n",
      "epoch no.0 train no.980  loss = 5.10996 avg_loss = 4.64126\n",
      "epoch no.0 train no.990  loss = 6.68360 avg_loss = 4.65965\n",
      "epoch no.0 train no.1000  loss = 3.50652 avg_loss = 4.62263\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '의', '▁감성', '</s>', '▁노래', '</s>']\n",
      "여름밤의 낭만적인 음악</s>\n",
      "epoch no.0 train no.1010  loss = 5.66075 avg_loss = 4.63601\n",
      "epoch no.0 train no.1020  loss = 5.87960 avg_loss = 4.68349\n",
      "epoch no.0 train no.1030  loss = 6.75317 avg_loss = 4.69531\n",
      "epoch no.0 train no.1040  loss = 5.14615 avg_loss = 4.71857\n",
      "epoch no.0 train no.1050  loss = 5.43646 avg_loss = 4.66130\n",
      "epoch no.0 train no.1060  loss = 4.31789 avg_loss = 4.62283\n",
      "epoch no.0 train no.1070  loss = 5.03878 avg_loss = 4.59091\n",
      "epoch no.0 train no.1080  loss = 3.80318 avg_loss = 4.63888\n",
      "epoch no.0 train no.1090  loss = 6.48292 avg_loss = 4.66195\n",
      "epoch no.0 train no.1100  loss = 4.36363 avg_loss = 4.64039\n",
      "epoch no.0 train no.1110  loss = 6.14630 avg_loss = 4.66217\n",
      "epoch no.0 train no.1120  loss = 4.61000 avg_loss = 4.65844\n",
      "epoch no.0 train no.1130  loss = 6.45543 avg_loss = 4.65948\n",
      "epoch no.0 train no.1140  loss = 3.81917 avg_loss = 4.59124\n",
      "epoch no.0 train no.1150  loss = 6.29451 avg_loss = 4.58256\n",
      "epoch no.0 train no.1160  loss = 5.08301 avg_loss = 4.54512\n",
      "epoch no.0 train no.1170  loss = 4.68622 avg_loss = 4.53919\n",
      "epoch no.0 train no.1180  loss = 3.44287 avg_loss = 4.49691\n",
      "epoch no.0 train no.1190  loss = 3.29565 avg_loss = 4.44810\n",
      "epoch no.0 train no.1200  loss = 4.31054 avg_loss = 4.36657\n",
      "epoch no.0 train no.1210  loss = 2.55916 avg_loss = 4.30544\n",
      "epoch no.0 train no.1220  loss = 2.79862 avg_loss = 4.32719\n",
      "epoch no.0 train no.1230  loss = 6.11008 avg_loss = 4.40984\n",
      "epoch no.0 train no.1240  loss = 4.02563 avg_loss = 4.41119\n",
      "epoch no.0 train no.1250  loss = 2.55165 avg_loss = 4.40503\n",
      "epoch no.0 train no.1260  loss = 4.44593 avg_loss = 4.44311\n",
      "epoch no.0 train no.1270  loss = 3.55882 avg_loss = 4.43333\n",
      "epoch no.0 train no.1280  loss = 2.97369 avg_loss = 4.44991\n",
      "epoch no.0 train no.1290  loss = 4.92792 avg_loss = 4.50365\n",
      "epoch no.0 train no.1300  loss = 6.80253 avg_loss = 4.51978\n",
      "epoch no.0 train no.1310  loss = 5.52031 avg_loss = 4.54027\n",
      "epoch no.0 train no.1320  loss = 3.89156 avg_loss = 4.48527\n",
      "epoch no.0 train no.1330  loss = 4.31503 avg_loss = 4.46630\n",
      "epoch no.0 train no.1340  loss = 7.15398 avg_loss = 4.46704\n",
      "epoch no.0 train no.1350  loss = 4.17936 avg_loss = 4.46480\n",
      "epoch no.0 train no.1360  loss = 4.08683 avg_loss = 4.44882\n",
      "epoch no.0 train no.1370  loss = 3.27708 avg_loss = 4.46822\n",
      "epoch no.0 train no.1380  loss = 5.65516 avg_loss = 4.47904\n",
      "epoch no.0 train no.1390  loss = 4.91124 avg_loss = 4.48438\n",
      "epoch no.0 train no.1400  loss = 5.23795 avg_loss = 4.48946\n",
      "epoch no.0 train no.1410  loss = 3.30496 avg_loss = 4.43397\n",
      "epoch no.0 train no.1420  loss = 3.22934 avg_loss = 4.47236\n",
      "epoch no.0 train no.1430  loss = 3.78493 avg_loss = 4.51860\n",
      "epoch no.0 train no.1440  loss = 4.76779 avg_loss = 4.51323\n",
      "epoch no.0 train no.1450  loss = 3.74272 avg_loss = 4.53136\n",
      "epoch no.0 train no.1460  loss = 5.14116 avg_loss = 4.56507\n",
      "epoch no.0 train no.1470  loss = 5.56308 avg_loss = 4.52157\n",
      "epoch no.0 train no.1480  loss = 5.20727 avg_loss = 4.48007\n",
      "epoch no.0 train no.1490  loss = 3.23464 avg_loss = 4.51744\n",
      "epoch no.0 train no.1500  loss = 3.77392 avg_loss = 4.50289\n",
      "epoch no.0 train no.1510  loss = 3.50651 avg_loss = 4.54048\n",
      "epoch no.0 train no.1520  loss = 6.08308 avg_loss = 4.50927\n",
      "epoch no.0 train no.1530  loss = 3.10351 avg_loss = 4.47470\n",
      "epoch no.0 train no.1540  loss = 3.36452 avg_loss = 4.50912\n",
      "epoch no.0 train no.1550  loss = 4.09634 avg_loss = 4.46003\n",
      "epoch no.0 train no.1560  loss = 6.63420 avg_loss = 4.47941\n",
      "epoch no.0 train no.1570  loss = 6.53106 avg_loss = 4.51999\n",
      "epoch no.0 train no.1580  loss = 3.25924 avg_loss = 4.46194\n",
      "epoch no.0 train no.1590  loss = 6.54042 avg_loss = 4.49614\n",
      "epoch no.0 train no.1600  loss = 8.36893 avg_loss = 4.47865\n",
      "epoch no.0 train no.1610  loss = 6.21619 avg_loss = 4.48397\n",
      "epoch no.0 train no.1620  loss = 3.60940 avg_loss = 4.46835\n",
      "epoch no.0 train no.1630  loss = 5.91059 avg_loss = 4.49126\n",
      "epoch no.0 train no.1640  loss = 4.73593 avg_loss = 4.43762\n",
      "epoch no.0 train no.1650  loss = 5.13580 avg_loss = 4.39284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.1660  loss = 4.75536 avg_loss = 4.35955\n",
      "epoch no.0 train no.1670  loss = 4.28833 avg_loss = 4.37198\n",
      "epoch no.0 train no.1680  loss = 5.43197 avg_loss = 4.36213\n",
      "epoch no.0 train no.1690  loss = 5.18812 avg_loss = 4.44773\n",
      "epoch no.0 train no.1700  loss = 4.61176 avg_loss = 4.42554\n",
      "epoch no.0 train no.1710  loss = 3.22187 avg_loss = 4.40648\n",
      "epoch no.0 train no.1720  loss = 4.75342 avg_loss = 4.45824\n",
      "epoch no.0 train no.1730  loss = 5.30843 avg_loss = 4.49030\n",
      "epoch no.0 train no.1740  loss = 4.69676 avg_loss = 4.42862\n",
      "epoch no.0 train no.1750  loss = 6.13722 avg_loss = 4.46519\n",
      "epoch no.0 train no.1760  loss = 2.91434 avg_loss = 4.40534\n",
      "epoch no.0 train no.1770  loss = 5.99110 avg_loss = 4.43086\n",
      "epoch no.0 train no.1780  loss = 4.26185 avg_loss = 4.41931\n",
      "epoch no.0 train no.1790  loss = 4.88730 avg_loss = 4.41643\n",
      "epoch no.0 train no.1800  loss = 2.99283 avg_loss = 4.47043\n",
      "epoch no.0 train no.1810  loss = 6.60903 avg_loss = 4.50976\n",
      "epoch no.0 train no.1820  loss = 5.34691 avg_loss = 4.52176\n",
      "epoch no.0 train no.1830  loss = 6.04719 avg_loss = 4.51019\n",
      "epoch no.0 train no.1840  loss = 4.54244 avg_loss = 4.47116\n",
      "epoch no.0 train no.1850  loss = 4.69348 avg_loss = 4.46064\n",
      "epoch no.0 train no.1860  loss = 4.75085 avg_loss = 4.45068\n",
      "epoch no.0 train no.1870  loss = 2.14748 avg_loss = 4.46451\n",
      "epoch no.0 train no.1880  loss = 4.11446 avg_loss = 4.47142\n",
      "epoch no.0 train no.1890  loss = 5.10113 avg_loss = 4.46773\n",
      "epoch no.0 train no.1900  loss = 5.76179 avg_loss = 4.46229\n",
      "epoch no.0 train no.1910  loss = 5.81545 avg_loss = 4.44022\n",
      "epoch no.0 train no.1920  loss = 3.07393 avg_loss = 4.45910\n",
      "epoch no.0 train no.1930  loss = 7.36456 avg_loss = 4.49704\n",
      "epoch no.0 train no.1940  loss = 3.06122 avg_loss = 4.53273\n",
      "epoch no.0 train no.1950  loss = 6.18790 avg_loss = 4.47383\n",
      "epoch no.0 train no.1960  loss = 2.88446 avg_loss = 4.43581\n",
      "epoch no.0 train no.1970  loss = 4.92827 avg_loss = 4.49036\n",
      "epoch no.0 train no.1980  loss = 3.69746 avg_loss = 4.52895\n",
      "epoch no.0 train no.1990  loss = 3.78374 avg_loss = 4.56482\n",
      "epoch no.0 train no.2000  loss = 3.45863 avg_loss = 4.53410\n",
      "6\n",
      "to_tokens: ['▁', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>', '</s>']\n",
      "여름밤에 듣기 좋은 노래들</s>\n",
      "epoch no.0 train no.2010  loss = 4.15673 avg_loss = 4.51738\n",
      "epoch no.0 train no.2020  loss = 3.49431 avg_loss = 4.47508\n",
      "epoch no.0 train no.2030  loss = 3.28387 avg_loss = 4.42401\n",
      "epoch no.0 train no.2040  loss = 3.08531 avg_loss = 4.39566\n",
      "epoch no.0 train no.2050  loss = 3.54387 avg_loss = 4.36206\n",
      "epoch no.0 train no.2060  loss = 2.55798 avg_loss = 4.33532\n",
      "epoch no.0 train no.2070  loss = 6.99966 avg_loss = 4.38912\n",
      "epoch no.0 train no.2080  loss = 2.93903 avg_loss = 4.40271\n",
      "epoch no.0 train no.2090  loss = 2.80991 avg_loss = 4.40319\n",
      "epoch no.0 train no.2100  loss = 5.45831 avg_loss = 4.43653\n",
      "epoch no.0 train no.2110  loss = 6.30153 avg_loss = 4.49615\n",
      "epoch no.0 train no.2120  loss = 5.03233 avg_loss = 4.52191\n",
      "epoch no.0 train no.2130  loss = 4.36649 avg_loss = 4.53093\n",
      "epoch no.0 train no.2140  loss = 2.22978 avg_loss = 4.48258\n",
      "epoch no.0 train no.2150  loss = 4.01926 avg_loss = 4.47129\n",
      "epoch no.0 train no.2160  loss = 4.08515 avg_loss = 4.42565\n",
      "epoch no.0 train no.2170  loss = 5.23961 avg_loss = 4.39377\n",
      "epoch no.0 train no.2180  loss = 4.73534 avg_loss = 4.36107\n",
      "epoch no.0 train no.2190  loss = 4.43209 avg_loss = 4.33135\n",
      "epoch no.0 train no.2200  loss = 3.75261 avg_loss = 4.29775\n",
      "epoch no.0 train no.2210  loss = 7.65229 avg_loss = 4.30229\n",
      "epoch no.0 train no.2220  loss = 6.54230 avg_loss = 4.30497\n",
      "epoch no.0 train no.2230  loss = 4.81501 avg_loss = 4.30495\n",
      "epoch no.0 train no.2240  loss = 4.24672 avg_loss = 4.32612\n",
      "epoch no.0 train no.2250  loss = 6.03273 avg_loss = 4.35398\n",
      "epoch no.0 train no.2260  loss = 3.77400 avg_loss = 4.36214\n",
      "epoch no.0 train no.2270  loss = 6.12593 avg_loss = 4.39284\n",
      "epoch no.0 train no.2280  loss = 3.64288 avg_loss = 4.31977\n",
      "epoch no.0 train no.2290  loss = 3.85224 avg_loss = 4.36090\n",
      "epoch no.0 train no.2300  loss = 3.82168 avg_loss = 4.37899\n",
      "epoch no.0 train no.2310  loss = 3.26928 avg_loss = 4.35961\n",
      "epoch no.0 train no.2320  loss = 3.19261 avg_loss = 4.31600\n",
      "epoch no.0 train no.2330  loss = 2.96670 avg_loss = 4.28972\n",
      "epoch no.0 train no.2340  loss = 4.87560 avg_loss = 4.27854\n",
      "epoch no.0 train no.2350  loss = 3.82431 avg_loss = 4.27514\n",
      "epoch no.0 train no.2360  loss = 6.63482 avg_loss = 4.23972\n",
      "epoch no.0 train no.2370  loss = 6.84381 avg_loss = 4.31323\n",
      "epoch no.0 train no.2380  loss = 2.74302 avg_loss = 4.31740\n",
      "epoch no.0 train no.2390  loss = 5.30646 avg_loss = 4.33901\n",
      "epoch no.0 train no.2400  loss = 3.65216 avg_loss = 4.41595\n",
      "epoch no.0 train no.2410  loss = 3.79474 avg_loss = 4.40987\n",
      "epoch no.0 train no.2420  loss = 2.95749 avg_loss = 4.34193\n",
      "epoch no.0 train no.2430  loss = 3.59772 avg_loss = 4.34875\n",
      "epoch no.0 train no.2440  loss = 2.42125 avg_loss = 4.31733\n",
      "epoch no.0 train no.2450  loss = 5.03264 avg_loss = 4.34439\n",
      "epoch no.0 train no.2460  loss = 3.72983 avg_loss = 4.35735\n",
      "epoch no.0 train no.2470  loss = 4.03979 avg_loss = 4.36816\n",
      "epoch no.0 train no.2480  loss = 3.45199 avg_loss = 4.37378\n",
      "epoch no.0 train no.2490  loss = 2.11892 avg_loss = 4.38817\n",
      "epoch no.0 train no.2500  loss = 2.86767 avg_loss = 4.40456\n",
      "epoch no.0 train no.2510  loss = 4.16138 avg_loss = 4.35240\n",
      "epoch no.0 train no.2520  loss = 5.04858 avg_loss = 4.40347\n",
      "epoch no.0 train no.2530  loss = 4.25026 avg_loss = 4.43866\n",
      "epoch no.0 train no.2540  loss = 2.73191 avg_loss = 4.41685\n",
      "epoch no.0 train no.2550  loss = 3.56995 avg_loss = 4.44756\n",
      "epoch no.0 train no.2560  loss = 5.94372 avg_loss = 4.45241\n",
      "epoch no.0 train no.2570  loss = 6.45270 avg_loss = 4.46435\n",
      "epoch no.0 train no.2580  loss = 3.68205 avg_loss = 4.43792\n",
      "epoch no.0 train no.2590  loss = 6.53755 avg_loss = 4.48353\n",
      "epoch no.0 train no.2600  loss = 5.36845 avg_loss = 4.48972\n",
      "epoch no.0 train no.2610  loss = 2.25140 avg_loss = 4.48285\n",
      "epoch no.0 train no.2620  loss = 4.16877 avg_loss = 4.45357\n",
      "epoch no.0 train no.2630  loss = 5.46210 avg_loss = 4.52192\n",
      "epoch no.0 train no.2640  loss = 3.13913 avg_loss = 4.52958\n",
      "epoch no.0 train no.2650  loss = 4.16144 avg_loss = 4.46088\n",
      "epoch no.0 train no.2660  loss = 3.82146 avg_loss = 4.46890\n",
      "epoch no.0 train no.2670  loss = 3.88888 avg_loss = 4.45696\n",
      "epoch no.0 train no.2680  loss = 3.28046 avg_loss = 4.38825\n",
      "epoch no.0 train no.2690  loss = 4.15547 avg_loss = 4.35687\n",
      "epoch no.0 train no.2700  loss = 2.95645 avg_loss = 4.33233\n",
      "epoch no.0 train no.2710  loss = 5.53376 avg_loss = 4.32752\n",
      "epoch no.0 train no.2720  loss = 3.05776 avg_loss = 4.29544\n",
      "epoch no.0 train no.2730  loss = 5.63332 avg_loss = 4.37360\n",
      "epoch no.0 train no.2740  loss = 3.51671 avg_loss = 4.37018\n",
      "epoch no.0 train no.2750  loss = 4.95949 avg_loss = 4.42864\n",
      "epoch no.0 train no.2760  loss = 3.01446 avg_loss = 4.41371\n",
      "epoch no.0 train no.2770  loss = 5.89667 avg_loss = 4.51762\n",
      "epoch no.0 train no.2780  loss = 6.84957 avg_loss = 4.51386\n",
      "epoch no.0 train no.2790  loss = 3.80298 avg_loss = 4.48901\n",
      "epoch no.0 train no.2800  loss = 3.07457 avg_loss = 4.46784\n",
      "epoch no.0 train no.2810  loss = 6.13779 avg_loss = 4.45708\n",
      "epoch no.0 train no.2820  loss = 3.36180 avg_loss = 4.45296\n",
      "epoch no.0 train no.2830  loss = 3.50896 avg_loss = 4.38843\n",
      "epoch no.0 train no.2840  loss = 2.60319 avg_loss = 4.35122\n",
      "epoch no.0 train no.2850  loss = 5.64182 avg_loss = 4.36252\n",
      "epoch no.0 train no.2860  loss = 3.52884 avg_loss = 4.32989\n",
      "epoch no.0 train no.2870  loss = 6.13276 avg_loss = 4.38430\n",
      "epoch no.0 train no.2880  loss = 4.81846 avg_loss = 4.42709\n",
      "epoch no.0 train no.2890  loss = 5.89144 avg_loss = 4.46362\n",
      "epoch no.0 train no.2900  loss = 5.32824 avg_loss = 4.46485\n",
      "epoch no.0 train no.2910  loss = 4.46250 avg_loss = 4.48305\n",
      "epoch no.0 train no.2920  loss = 4.45619 avg_loss = 4.47380\n",
      "epoch no.0 train no.2930  loss = 1.92098 avg_loss = 4.37876\n",
      "epoch no.0 train no.2940  loss = 2.26109 avg_loss = 4.34028\n",
      "epoch no.0 train no.2950  loss = 2.65855 avg_loss = 4.29587\n",
      "epoch no.0 train no.2960  loss = 5.20408 avg_loss = 4.32673\n",
      "epoch no.0 train no.2970  loss = 5.22793 avg_loss = 4.39892\n",
      "epoch no.0 train no.2980  loss = 2.31926 avg_loss = 4.38102\n",
      "epoch no.0 train no.2990  loss = 4.93838 avg_loss = 4.38061\n",
      "epoch no.0 train no.3000  loss = 3.13669 avg_loss = 4.33074\n",
      "4\n",
      "to_tokens: ['▁', '밤', '의', '▁듣는', '▁노래', '</s>']\n",
      "여름밤에 듣는 노래</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.3010  loss = 5.90371 avg_loss = 4.43190\n",
      "epoch no.0 train no.3020  loss = 5.05794 avg_loss = 4.48242\n",
      "epoch no.0 train no.3030  loss = 4.95090 avg_loss = 4.43644\n",
      "epoch no.0 train no.3040  loss = 3.63831 avg_loss = 4.38271\n",
      "epoch no.0 train no.3050  loss = 4.57470 avg_loss = 4.36757\n",
      "epoch no.0 train no.3060  loss = 4.10346 avg_loss = 4.40918\n",
      "epoch no.0 train no.3070  loss = 5.16034 avg_loss = 4.39827\n",
      "epoch no.0 train no.3080  loss = 3.38878 avg_loss = 4.34076\n",
      "epoch no.0 train no.3090  loss = 3.80609 avg_loss = 4.35875\n",
      "epoch no.0 train no.3100  loss = 4.56433 avg_loss = 4.41618\n",
      "epoch no.0 train no.3110  loss = 3.72620 avg_loss = 4.36733\n",
      "epoch no.0 train no.3120  loss = 3.05597 avg_loss = 4.37213\n",
      "epoch no.0 train no.3130  loss = 4.11339 avg_loss = 4.35435\n",
      "epoch no.0 train no.3140  loss = 5.82943 avg_loss = 4.36917\n",
      "epoch no.0 train no.3150  loss = 3.14913 avg_loss = 4.34531\n",
      "epoch no.0 train no.3160  loss = 5.45650 avg_loss = 4.36212\n",
      "epoch no.0 train no.3170  loss = 5.39975 avg_loss = 4.35200\n",
      "epoch no.0 train no.3180  loss = 3.54569 avg_loss = 4.30592\n",
      "epoch no.0 train no.3190  loss = 3.99517 avg_loss = 4.24833\n",
      "epoch no.0 train no.3200  loss = 4.08804 avg_loss = 4.22675\n",
      "epoch no.0 train no.3210  loss = 5.06633 avg_loss = 4.20729\n",
      "epoch no.0 train no.3220  loss = 2.79987 avg_loss = 4.16074\n",
      "epoch no.0 train no.3230  loss = 4.35552 avg_loss = 4.24292\n",
      "epoch no.0 train no.3240  loss = 3.22408 avg_loss = 4.27457\n",
      "epoch no.0 train no.3250  loss = 6.35463 avg_loss = 4.26691\n",
      "epoch no.0 train no.3260  loss = 5.99359 avg_loss = 4.28103\n",
      "epoch no.0 train no.3270  loss = 3.71072 avg_loss = 4.30812\n",
      "epoch no.0 train no.3280  loss = 5.76352 avg_loss = 4.33629\n",
      "epoch no.0 train no.3290  loss = 4.15484 avg_loss = 4.32181\n",
      "epoch no.0 train no.3300  loss = 3.65560 avg_loss = 4.31592\n",
      "epoch no.0 train no.3310  loss = 2.39349 avg_loss = 4.36801\n",
      "epoch no.0 train no.3320  loss = 3.10121 avg_loss = 4.32386\n",
      "epoch no.0 train no.3330  loss = 4.11070 avg_loss = 4.32573\n",
      "epoch no.0 train no.3340  loss = 4.34516 avg_loss = 4.37800\n",
      "epoch no.0 train no.3350  loss = 3.09933 avg_loss = 4.39480\n",
      "epoch no.0 train no.3360  loss = 4.38011 avg_loss = 4.42426\n",
      "epoch no.0 train no.3370  loss = 5.18380 avg_loss = 4.43726\n",
      "epoch no.0 train no.3380  loss = 3.75621 avg_loss = 4.45753\n",
      "epoch no.0 train no.3390  loss = 3.37568 avg_loss = 4.44183\n",
      "epoch no.0 train no.3400  loss = 4.07088 avg_loss = 4.40252\n",
      "epoch no.0 train no.3410  loss = 6.71578 avg_loss = 4.43246\n",
      "epoch no.0 train no.3420  loss = 3.95721 avg_loss = 4.40810\n",
      "epoch no.0 train no.3430  loss = 4.67840 avg_loss = 4.46028\n",
      "epoch no.0 train no.3440  loss = 5.03359 avg_loss = 4.43913\n",
      "epoch no.0 train no.3450  loss = 3.88313 avg_loss = 4.44806\n",
      "epoch no.0 train no.3460  loss = 4.72252 avg_loss = 4.38989\n",
      "epoch no.0 train no.3470  loss = 4.78884 avg_loss = 4.40063\n",
      "epoch no.0 train no.3480  loss = 2.65293 avg_loss = 4.37152\n",
      "epoch no.0 train no.3490  loss = 4.03188 avg_loss = 4.31917\n",
      "epoch no.0 train no.3500  loss = 4.88976 avg_loss = 4.30968\n",
      "epoch no.0 train no.3510  loss = 5.33627 avg_loss = 4.33729\n",
      "epoch no.0 train no.3520  loss = 4.28642 avg_loss = 4.35945\n",
      "epoch no.0 train no.3530  loss = 4.23860 avg_loss = 4.30938\n",
      "epoch no.0 train no.3540  loss = 4.46371 avg_loss = 4.29262\n",
      "epoch no.0 train no.3550  loss = 3.36156 avg_loss = 4.27993\n",
      "epoch no.0 train no.3560  loss = 2.33531 avg_loss = 4.29264\n",
      "epoch no.0 train no.3570  loss = 6.27290 avg_loss = 4.29479\n",
      "epoch no.0 train no.3580  loss = 5.27587 avg_loss = 4.32855\n",
      "epoch no.0 train no.3590  loss = 4.67520 avg_loss = 4.28036\n",
      "epoch no.0 train no.3600  loss = 3.99726 avg_loss = 4.28254\n",
      "epoch no.0 train no.3610  loss = 2.77265 avg_loss = 4.26019\n",
      "epoch no.0 train no.3620  loss = 3.52639 avg_loss = 4.26245\n",
      "epoch no.0 train no.3630  loss = 5.18459 avg_loss = 4.27986\n",
      "epoch no.0 train no.3640  loss = 6.95594 avg_loss = 4.26938\n",
      "epoch no.0 train no.3650  loss = 3.82466 avg_loss = 4.29274\n",
      "epoch no.0 train no.3660  loss = 3.71288 avg_loss = 4.25385\n",
      "epoch no.0 train no.3670  loss = 3.00351 avg_loss = 4.25527\n",
      "epoch no.0 train no.3680  loss = 3.73043 avg_loss = 4.21857\n",
      "epoch no.0 train no.3690  loss = 4.60448 avg_loss = 4.26387\n",
      "epoch no.0 train no.3700  loss = 4.23702 avg_loss = 4.25932\n",
      "epoch no.0 train no.3710  loss = 5.96110 avg_loss = 4.25598\n",
      "epoch no.0 train no.3720  loss = 5.46108 avg_loss = 4.30713\n",
      "epoch no.0 train no.3730  loss = 3.86378 avg_loss = 4.26456\n",
      "epoch no.0 train no.3740  loss = 2.76659 avg_loss = 4.21716\n",
      "epoch no.0 train no.3750  loss = 2.77853 avg_loss = 4.18744\n",
      "epoch no.0 train no.3760  loss = 4.25444 avg_loss = 4.18010\n",
      "epoch no.0 train no.3770  loss = 3.50867 avg_loss = 4.17201\n",
      "epoch no.0 train no.3780  loss = 2.68498 avg_loss = 4.13191\n",
      "epoch no.0 train no.3790  loss = 5.82807 avg_loss = 4.16781\n",
      "epoch no.0 train no.3800  loss = 3.51355 avg_loss = 4.15614\n",
      "epoch no.0 train no.3810  loss = 3.92251 avg_loss = 4.16647\n",
      "epoch no.0 train no.3820  loss = 4.50968 avg_loss = 4.16254\n",
      "epoch no.0 train no.3830  loss = 4.32648 avg_loss = 4.19610\n",
      "epoch no.0 train no.3840  loss = 4.54432 avg_loss = 4.27586\n",
      "epoch no.0 train no.3850  loss = 2.33259 avg_loss = 4.23763\n",
      "epoch no.0 train no.3860  loss = 2.96972 avg_loss = 4.20620\n",
      "epoch no.0 train no.3870  loss = 3.20121 avg_loss = 4.23204\n",
      "epoch no.0 train no.3880  loss = 3.92082 avg_loss = 4.23404\n",
      "epoch no.0 train no.3890  loss = 6.43126 avg_loss = 4.23217\n",
      "epoch no.0 train no.3900  loss = 5.27774 avg_loss = 4.24070\n",
      "epoch no.0 train no.3910  loss = 3.32848 avg_loss = 4.21380\n",
      "epoch no.0 train no.3920  loss = 4.14801 avg_loss = 4.20733\n",
      "epoch no.0 train no.3930  loss = 3.74932 avg_loss = 4.26085\n",
      "epoch no.0 train no.3940  loss = 4.33413 avg_loss = 4.24244\n",
      "epoch no.0 train no.3950  loss = 5.39365 avg_loss = 4.28233\n",
      "epoch no.0 train no.3960  loss = 2.89038 avg_loss = 4.26627\n",
      "epoch no.0 train no.3970  loss = 5.23302 avg_loss = 4.28114\n",
      "epoch no.0 train no.3980  loss = 4.32878 avg_loss = 4.30303\n",
      "epoch no.0 train no.3990  loss = 2.38300 avg_loss = 4.25520\n",
      "epoch no.0 train no.4000  loss = 5.77862 avg_loss = 4.26148\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '▁오면', '전', '</s>', '▁위한', '</s>', '▁좋은', '▁노래', '송']\n",
      "여름이 오기 전 여름을 맞아 듣기 좋은 팝</s>\n",
      "epoch no.0 train no.4010  loss = 4.05059 avg_loss = 4.20830\n",
      "epoch no.0 train no.4020  loss = 2.92109 avg_loss = 4.20733\n",
      "epoch no.0 train no.4030  loss = 7.22303 avg_loss = 4.27002\n",
      "epoch no.0 train no.4040  loss = 2.52087 avg_loss = 4.27222\n",
      "epoch no.0 train no.4050  loss = 2.58993 avg_loss = 4.32098\n",
      "epoch no.0 train no.4060  loss = 6.24964 avg_loss = 4.36633\n",
      "epoch no.0 train no.4070  loss = 3.18513 avg_loss = 4.33124\n",
      "epoch no.0 train no.4080  loss = 6.11507 avg_loss = 4.31167\n",
      "epoch no.0 train no.4090  loss = 2.45860 avg_loss = 4.32179\n",
      "epoch no.0 train no.4100  loss = 6.98511 avg_loss = 4.29711\n",
      "epoch no.0 train no.4110  loss = 3.98468 avg_loss = 4.30032\n",
      "epoch no.0 train no.4120  loss = 5.65371 avg_loss = 4.30919\n",
      "epoch no.0 train no.4130  loss = 4.68624 avg_loss = 4.31384\n",
      "epoch no.0 train no.4140  loss = 3.47622 avg_loss = 4.28845\n",
      "epoch no.0 train no.4150  loss = 3.32332 avg_loss = 4.24384\n",
      "epoch no.0 train no.4160  loss = 4.32750 avg_loss = 4.26951\n",
      "epoch no.0 train no.4170  loss = 3.85264 avg_loss = 4.25364\n",
      "epoch no.0 train no.4180  loss = 3.88389 avg_loss = 4.27933\n",
      "epoch no.0 train no.4190  loss = 4.06466 avg_loss = 4.27278\n",
      "epoch no.0 train no.4200  loss = 4.60666 avg_loss = 4.35252\n",
      "epoch no.0 train no.4210  loss = 4.99281 avg_loss = 4.31821\n",
      "epoch no.0 train no.4220  loss = 4.60091 avg_loss = 4.35210\n",
      "epoch no.0 train no.4230  loss = 4.04745 avg_loss = 4.34618\n",
      "epoch no.0 train no.4240  loss = 3.05188 avg_loss = 4.43210\n",
      "epoch no.0 train no.4250  loss = 3.34574 avg_loss = 4.41128\n",
      "epoch no.0 train no.4260  loss = 4.40595 avg_loss = 4.42780\n",
      "epoch no.0 train no.4270  loss = 3.09214 avg_loss = 4.39884\n",
      "epoch no.0 train no.4280  loss = 3.32360 avg_loss = 4.38619\n",
      "epoch no.0 train no.4290  loss = 3.91096 avg_loss = 4.41287\n",
      "epoch no.0 train no.4300  loss = 6.04136 avg_loss = 4.39466\n",
      "epoch no.0 train no.4310  loss = 4.55621 avg_loss = 4.48720\n",
      "epoch no.0 train no.4320  loss = 3.47276 avg_loss = 4.48563\n",
      "epoch no.0 train no.4330  loss = 4.39719 avg_loss = 4.47259\n",
      "epoch no.0 train no.4340  loss = 4.83822 avg_loss = 4.51393\n",
      "epoch no.0 train no.4350  loss = 3.71334 avg_loss = 4.46974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.4360  loss = 5.22497 avg_loss = 4.44151\n",
      "epoch no.0 train no.4370  loss = 4.86108 avg_loss = 4.49926\n",
      "epoch no.0 train no.4380  loss = 2.58096 avg_loss = 4.44437\n",
      "epoch no.0 train no.4390  loss = 3.32510 avg_loss = 4.49470\n",
      "epoch no.0 train no.4400  loss = 4.67116 avg_loss = 4.43048\n",
      "epoch no.0 train no.4410  loss = 4.13102 avg_loss = 4.42159\n",
      "epoch no.0 train no.4420  loss = 2.81009 avg_loss = 4.42368\n",
      "epoch no.0 train no.4430  loss = 4.21869 avg_loss = 4.41403\n",
      "epoch no.0 train no.4440  loss = 2.74932 avg_loss = 4.42127\n",
      "epoch no.0 train no.4450  loss = 5.12404 avg_loss = 4.39679\n",
      "epoch no.0 train no.4460  loss = 4.81863 avg_loss = 4.42755\n",
      "epoch no.0 train no.4470  loss = 5.03410 avg_loss = 4.44940\n",
      "epoch no.0 train no.4480  loss = 4.21377 avg_loss = 4.43264\n",
      "epoch no.0 train no.4490  loss = 3.65533 avg_loss = 4.45854\n",
      "epoch no.0 train no.4500  loss = 4.48389 avg_loss = 4.42370\n",
      "epoch no.0 train no.4510  loss = 4.33202 avg_loss = 4.41928\n",
      "epoch no.0 train no.4520  loss = 4.53642 avg_loss = 4.46461\n",
      "epoch no.0 train no.4530  loss = 4.90857 avg_loss = 4.46808\n",
      "epoch no.0 train no.4540  loss = 2.95057 avg_loss = 4.47421\n",
      "epoch no.0 train no.4550  loss = 2.71913 avg_loss = 4.45456\n",
      "epoch no.0 train no.4560  loss = 5.69369 avg_loss = 4.41124\n",
      "epoch no.0 train no.4570  loss = 7.02844 avg_loss = 4.39556\n",
      "epoch no.0 train no.4580  loss = 3.65323 avg_loss = 4.36122\n",
      "epoch no.0 train no.4590  loss = 3.59512 avg_loss = 4.36104\n",
      "epoch no.0 train no.4600  loss = 3.15495 avg_loss = 4.34740\n",
      "epoch no.0 train no.4610  loss = 5.36961 avg_loss = 4.39698\n",
      "epoch no.0 train no.4620  loss = 3.21013 avg_loss = 4.35100\n",
      "epoch no.0 train no.4630  loss = 2.85259 avg_loss = 4.37194\n",
      "epoch no.0 train no.4640  loss = 4.39234 avg_loss = 4.40336\n",
      "epoch no.0 train no.4650  loss = 4.25001 avg_loss = 4.37533\n",
      "epoch no.0 train no.4660  loss = 5.06186 avg_loss = 4.38202\n",
      "epoch no.0 train no.4670  loss = 2.80582 avg_loss = 4.38775\n",
      "epoch no.0 train no.4680  loss = 4.67023 avg_loss = 4.39131\n",
      "epoch no.0 train no.4690  loss = 5.89134 avg_loss = 4.38218\n",
      "epoch no.0 train no.4700  loss = 3.75452 avg_loss = 4.31688\n",
      "epoch no.0 train no.4710  loss = 4.15821 avg_loss = 4.28708\n",
      "epoch no.0 train no.4720  loss = 3.86770 avg_loss = 4.32710\n",
      "epoch no.0 train no.4730  loss = 3.36317 avg_loss = 4.33728\n",
      "epoch no.0 train no.4740  loss = 5.18055 avg_loss = 4.37045\n",
      "epoch no.0 train no.4750  loss = 5.63005 avg_loss = 4.34318\n",
      "epoch no.0 train no.4760  loss = 3.43750 avg_loss = 4.31283\n",
      "epoch no.0 train no.4770  loss = 3.15852 avg_loss = 4.28955\n",
      "epoch no.0 train no.4780  loss = 4.52560 avg_loss = 4.27640\n",
      "epoch no.0 train no.4790  loss = 4.90967 avg_loss = 4.26709\n",
      "epoch no.0 train no.4800  loss = 4.04828 avg_loss = 4.31189\n",
      "epoch no.0 train no.4810  loss = 5.23246 avg_loss = 4.24265\n",
      "epoch no.0 train no.4820  loss = 3.41166 avg_loss = 4.28082\n",
      "epoch no.0 train no.4830  loss = 2.36803 avg_loss = 4.25752\n",
      "epoch no.0 train no.4840  loss = 3.83506 avg_loss = 4.24338\n",
      "epoch no.0 train no.4850  loss = 2.73272 avg_loss = 4.14694\n",
      "epoch no.0 train no.4860  loss = 2.70973 avg_loss = 4.20737\n",
      "epoch no.0 train no.4870  loss = 4.10269 avg_loss = 4.21932\n",
      "epoch no.0 train no.4880  loss = 3.85833 avg_loss = 4.27013\n",
      "epoch no.0 train no.4890  loss = 4.05149 avg_loss = 4.22480\n",
      "epoch no.0 train no.4900  loss = 5.62486 avg_loss = 4.20771\n",
      "epoch no.0 train no.4910  loss = 4.72325 avg_loss = 4.20861\n",
      "epoch no.0 train no.4920  loss = 5.40979 avg_loss = 4.22437\n",
      "epoch no.0 train no.4930  loss = 2.64878 avg_loss = 4.22848\n",
      "epoch no.0 train no.4940  loss = 3.66356 avg_loss = 4.26778\n",
      "epoch no.0 train no.4950  loss = 3.32797 avg_loss = 4.19452\n",
      "epoch no.0 train no.4960  loss = 5.59929 avg_loss = 4.18355\n",
      "epoch no.0 train no.4970  loss = 4.18196 avg_loss = 4.16031\n",
      "epoch no.0 train no.4980  loss = 6.52924 avg_loss = 4.20753\n",
      "epoch no.0 train no.4990  loss = 4.81132 avg_loss = 4.19006\n",
      "epoch no.0 train no.5000  loss = 3.32206 avg_loss = 4.17863\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '의', '▁듣기', '▁노래', '</s>']\n",
      "여름밤에 듣는 클래식</s>\n",
      "epoch no.0 train no.5010  loss = 2.14019 avg_loss = 4.19983\n",
      "epoch no.0 train no.5020  loss = 3.49917 avg_loss = 4.14436\n",
      "epoch no.0 train no.5030  loss = 5.26477 avg_loss = 4.21163\n",
      "epoch no.0 train no.5040  loss = 3.39547 avg_loss = 4.15542\n",
      "epoch no.0 train no.5050  loss = 4.36116 avg_loss = 4.18192\n",
      "epoch no.0 train no.5060  loss = 4.86818 avg_loss = 4.14042\n",
      "epoch no.0 train no.5070  loss = 4.35169 avg_loss = 4.25302\n",
      "epoch no.0 train no.5080  loss = 5.88116 avg_loss = 4.26336\n",
      "epoch no.0 train no.5090  loss = 4.93276 avg_loss = 4.20960\n",
      "epoch no.0 train no.5100  loss = 4.37887 avg_loss = 4.21849\n",
      "epoch no.0 train no.5110  loss = 2.30268 avg_loss = 4.22223\n",
      "epoch no.0 train no.5120  loss = 3.81687 avg_loss = 4.25795\n",
      "epoch no.0 train no.5130  loss = 4.16624 avg_loss = 4.29149\n",
      "epoch no.0 train no.5140  loss = 3.67320 avg_loss = 4.24723\n",
      "epoch no.0 train no.5150  loss = 4.56285 avg_loss = 4.22870\n",
      "epoch no.0 train no.5160  loss = 4.13016 avg_loss = 4.23205\n",
      "epoch no.0 train no.5170  loss = 3.40974 avg_loss = 4.22087\n",
      "epoch no.0 train no.5180  loss = 2.65642 avg_loss = 4.20181\n",
      "epoch no.0 train no.5190  loss = 2.93632 avg_loss = 4.20323\n",
      "epoch no.0 train no.5200  loss = 3.83079 avg_loss = 4.23089\n",
      "epoch no.0 train no.5210  loss = 3.78646 avg_loss = 4.21741\n",
      "epoch no.0 train no.5220  loss = 6.92804 avg_loss = 4.36635\n",
      "epoch no.0 train no.5230  loss = 4.94303 avg_loss = 4.39398\n",
      "epoch no.0 train no.5240  loss = 4.01850 avg_loss = 4.41572\n",
      "epoch no.0 train no.5250  loss = 5.16317 avg_loss = 4.45569\n",
      "epoch no.0 train no.5260  loss = 4.01481 avg_loss = 4.50392\n",
      "epoch no.0 train no.5270  loss = 6.93866 avg_loss = 4.50281\n",
      "epoch no.0 train no.5280  loss = 3.97112 avg_loss = 4.45637\n",
      "epoch no.0 train no.5290  loss = 3.17768 avg_loss = 4.45183\n",
      "epoch no.0 train no.5300  loss = 2.66550 avg_loss = 4.41999\n",
      "epoch no.0 train no.5310  loss = 4.60231 avg_loss = 4.36279\n",
      "epoch no.0 train no.5320  loss = 5.82077 avg_loss = 4.36525\n",
      "epoch no.0 train no.5330  loss = 5.40627 avg_loss = 4.31429\n",
      "epoch no.0 train no.5340  loss = 4.94626 avg_loss = 4.42044\n",
      "epoch no.0 train no.5350  loss = 4.36883 avg_loss = 4.42398\n",
      "epoch no.0 train no.5360  loss = 4.48840 avg_loss = 4.43759\n",
      "epoch no.0 train no.5370  loss = 5.63774 avg_loss = 4.42616\n",
      "epoch no.0 train no.5380  loss = 3.32096 avg_loss = 4.38383\n",
      "epoch no.0 train no.5390  loss = 5.69202 avg_loss = 4.44161\n",
      "epoch no.0 train no.5400  loss = 2.70984 avg_loss = 4.42639\n",
      "epoch no.0 train no.5410  loss = 5.82376 avg_loss = 4.41365\n",
      "epoch no.0 train no.5420  loss = 2.56558 avg_loss = 4.34802\n",
      "epoch no.0 train no.5430  loss = 5.43924 avg_loss = 4.36743\n",
      "epoch no.0 train no.5440  loss = 4.63528 avg_loss = 4.39626\n",
      "epoch no.0 train no.5450  loss = 5.42289 avg_loss = 4.33576\n",
      "epoch no.0 train no.5460  loss = 3.88891 avg_loss = 4.28646\n",
      "epoch no.0 train no.5470  loss = 5.45752 avg_loss = 4.28998\n",
      "epoch no.0 train no.5480  loss = 5.09364 avg_loss = 4.21464\n",
      "epoch no.0 train no.5490  loss = 4.03502 avg_loss = 4.27277\n",
      "epoch no.0 train no.5500  loss = 5.24011 avg_loss = 4.23524\n",
      "epoch no.0 train no.5510  loss = 3.75381 avg_loss = 4.24238\n",
      "epoch no.0 train no.5520  loss = 3.89033 avg_loss = 4.27089\n",
      "epoch no.0 train no.5530  loss = 7.34157 avg_loss = 4.32599\n",
      "epoch no.0 train no.5540  loss = 4.75584 avg_loss = 4.36906\n",
      "epoch no.0 train no.5550  loss = 3.78165 avg_loss = 4.27856\n",
      "epoch no.0 train no.5560  loss = 3.51891 avg_loss = 4.26440\n",
      "epoch no.0 train no.5570  loss = 3.08240 avg_loss = 4.26408\n",
      "epoch no.0 train no.5580  loss = 3.35031 avg_loss = 4.29188\n",
      "epoch no.0 train no.5590  loss = 3.06861 avg_loss = 4.27383\n",
      "epoch no.0 train no.5600  loss = 5.38478 avg_loss = 4.27264\n",
      "epoch no.0 train no.5610  loss = 5.85199 avg_loss = 4.28390\n",
      "epoch no.0 train no.5620  loss = 3.09361 avg_loss = 4.24572\n",
      "epoch no.0 train no.5630  loss = 3.18253 avg_loss = 4.20389\n",
      "epoch no.0 train no.5640  loss = 2.65465 avg_loss = 4.16058\n",
      "epoch no.0 train no.5650  loss = 3.66374 avg_loss = 4.14779\n",
      "epoch no.0 train no.5660  loss = 3.88812 avg_loss = 4.15508\n",
      "epoch no.0 train no.5670  loss = 5.07557 avg_loss = 4.22271\n",
      "epoch no.0 train no.5680  loss = 4.74744 avg_loss = 4.25505\n",
      "epoch no.0 train no.5690  loss = 4.98321 avg_loss = 4.22788\n",
      "epoch no.0 train no.5700  loss = 2.78063 avg_loss = 4.15220\n",
      "epoch no.0 train no.5710  loss = 2.26931 avg_loss = 4.10611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.5720  loss = 3.87002 avg_loss = 4.09850\n",
      "epoch no.0 train no.5730  loss = 6.70514 avg_loss = 4.04964\n",
      "epoch no.0 train no.5740  loss = 5.66632 avg_loss = 4.09488\n",
      "epoch no.0 train no.5750  loss = 3.95231 avg_loss = 4.15790\n",
      "epoch no.0 train no.5760  loss = 4.99246 avg_loss = 4.15306\n",
      "epoch no.0 train no.5770  loss = 4.00597 avg_loss = 4.17963\n",
      "epoch no.0 train no.5780  loss = 3.02954 avg_loss = 4.12291\n",
      "epoch no.0 train no.5790  loss = 4.48894 avg_loss = 4.13507\n",
      "epoch no.0 train no.5800  loss = 6.94654 avg_loss = 4.12693\n",
      "epoch no.0 train no.5810  loss = 5.17660 avg_loss = 4.16628\n",
      "epoch no.0 train no.5820  loss = 4.50669 avg_loss = 4.19150\n",
      "epoch no.0 train no.5830  loss = 3.76545 avg_loss = 4.24727\n",
      "epoch no.0 train no.5840  loss = 5.70830 avg_loss = 4.17242\n",
      "epoch no.0 train no.5850  loss = 3.26423 avg_loss = 4.21295\n",
      "epoch no.0 train no.5860  loss = 4.35875 avg_loss = 4.18195\n",
      "epoch no.0 train no.5870  loss = 3.46373 avg_loss = 4.16351\n",
      "epoch no.0 train no.5880  loss = 3.91234 avg_loss = 4.17280\n",
      "epoch no.0 train no.5890  loss = 6.04950 avg_loss = 4.21578\n",
      "epoch no.0 train no.5900  loss = 3.60174 avg_loss = 4.24958\n",
      "epoch no.0 train no.5910  loss = 5.60949 avg_loss = 4.24209\n",
      "epoch no.0 train no.5920  loss = 5.16874 avg_loss = 4.32941\n",
      "epoch no.0 train no.5930  loss = 5.87742 avg_loss = 4.34821\n",
      "epoch no.0 train no.5940  loss = 4.51932 avg_loss = 4.36305\n",
      "epoch no.0 train no.5950  loss = 5.43687 avg_loss = 4.31261\n",
      "epoch no.0 train no.5960  loss = 2.94913 avg_loss = 4.31121\n",
      "epoch no.0 train no.5970  loss = 3.96890 avg_loss = 4.27632\n",
      "epoch no.0 train no.5980  loss = 5.43038 avg_loss = 4.25395\n",
      "epoch no.0 train no.5990  loss = 5.02976 avg_loss = 4.24013\n",
      "epoch no.0 train no.6000  loss = 4.16954 avg_loss = 4.25171\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.0 train no.6010  loss = 3.64145 avg_loss = 4.24849\n",
      "epoch no.0 train no.6020  loss = 4.61595 avg_loss = 4.23512\n",
      "epoch no.0 train no.6030  loss = 3.46349 avg_loss = 4.22346\n",
      "epoch no.0 train no.6040  loss = 4.45631 avg_loss = 4.21920\n",
      "epoch no.0 train no.6050  loss = 3.54386 avg_loss = 4.25586\n",
      "epoch no.0 train no.6060  loss = 3.91527 avg_loss = 4.24870\n",
      "epoch no.0 train no.6070  loss = 2.31059 avg_loss = 4.27486\n",
      "epoch no.0 train no.6080  loss = 4.99273 avg_loss = 4.26117\n",
      "epoch no.0 train no.6090  loss = 4.37213 avg_loss = 4.24191\n",
      "epoch no.0 train no.6100  loss = 4.29571 avg_loss = 4.20773\n",
      "epoch no.0 train no.6110  loss = 3.35535 avg_loss = 4.16655\n",
      "epoch no.0 train no.6120  loss = 3.21990 avg_loss = 4.15362\n",
      "epoch no.0 train no.6130  loss = 3.09123 avg_loss = 4.13448\n",
      "epoch no.0 train no.6140  loss = 5.37220 avg_loss = 4.09284\n",
      "epoch no.0 train no.6150  loss = 3.69438 avg_loss = 4.07848\n",
      "epoch no.0 train no.6160  loss = 4.29254 avg_loss = 4.16984\n",
      "epoch no.0 train no.6170  loss = 6.02687 avg_loss = 4.19431\n",
      "epoch no.0 train no.6180  loss = 2.81137 avg_loss = 4.16759\n",
      "epoch no.0 train no.6190  loss = 3.86570 avg_loss = 4.23613\n",
      "epoch no.0 train no.6200  loss = 6.82868 avg_loss = 4.28241\n",
      "epoch no.0 train no.6210  loss = 3.77409 avg_loss = 4.28666\n",
      "epoch no.0 train no.6220  loss = 3.47278 avg_loss = 4.22475\n",
      "epoch no.0 train no.6230  loss = 5.09884 avg_loss = 4.25671\n",
      "epoch no.0 train no.6240  loss = 3.91184 avg_loss = 4.23427\n",
      "epoch no.0 train no.6250  loss = 4.68953 avg_loss = 4.20523\n",
      "epoch no.0 train no.6260  loss = 3.45626 avg_loss = 4.20502\n",
      "epoch no.0 train no.6270  loss = 3.88970 avg_loss = 4.19009\n",
      "epoch no.0 train no.6280  loss = 4.37051 avg_loss = 4.14842\n",
      "epoch no.0 train no.6290  loss = 3.18591 avg_loss = 4.12217\n",
      "epoch no.0 train no.6300  loss = 4.25372 avg_loss = 4.12313\n",
      "epoch no.0 train no.6310  loss = 4.34048 avg_loss = 4.17533\n",
      "epoch no.0 train no.6320  loss = 4.37182 avg_loss = 4.17340\n",
      "epoch no.0 train no.6330  loss = 2.78540 avg_loss = 4.17696\n",
      "epoch no.0 train no.6340  loss = 3.06137 avg_loss = 4.14776\n",
      "epoch no.0 train no.6350  loss = 5.42648 avg_loss = 4.20280\n",
      "epoch no.0 train no.6360  loss = 2.33395 avg_loss = 4.19239\n",
      "epoch no.0 train no.6370  loss = 3.90648 avg_loss = 4.21678\n",
      "epoch no.0 train no.6380  loss = 3.80746 avg_loss = 4.21744\n",
      "epoch no.0 train no.6390  loss = 4.38542 avg_loss = 4.21552\n",
      "epoch no.0 train no.6400  loss = 3.41298 avg_loss = 4.17669\n",
      "epoch no.0 train no.6410  loss = 4.32639 avg_loss = 4.17141\n",
      "epoch no.0 train no.6420  loss = 4.27787 avg_loss = 4.21764\n",
      "epoch no.0 train no.6430  loss = 4.97056 avg_loss = 4.24937\n",
      "epoch no.0 train no.6440  loss = 5.44921 avg_loss = 4.28701\n",
      "epoch no.0 train no.6450  loss = 4.42855 avg_loss = 4.26351\n",
      "epoch no.0 train no.6460  loss = 5.07429 avg_loss = 4.24934\n",
      "epoch no.0 train no.6470  loss = 5.39856 avg_loss = 4.27531\n",
      "epoch no.0 train no.6480  loss = 3.35600 avg_loss = 4.26107\n",
      "epoch no.0 train no.6490  loss = 4.99990 avg_loss = 4.25329\n",
      "epoch no.0 train no.6500  loss = 4.69249 avg_loss = 4.25342\n",
      "epoch no.0 train no.6510  loss = 5.52643 avg_loss = 4.24161\n",
      "epoch no.0 train no.6520  loss = 4.30685 avg_loss = 4.22113\n",
      "epoch no.0 train no.6530  loss = 4.10736 avg_loss = 4.25677\n",
      "epoch no.0 train no.6540  loss = 2.96437 avg_loss = 4.24775\n",
      "epoch no.0 train no.6550  loss = 2.66455 avg_loss = 4.24239\n",
      "epoch no.0 train no.6560  loss = 4.22176 avg_loss = 4.28358\n",
      "epoch no.0 train no.6570  loss = 3.48219 avg_loss = 4.27937\n",
      "epoch no.0 train no.6580  loss = 4.28031 avg_loss = 4.30799\n",
      "epoch no.0 train no.6590  loss = 5.60390 avg_loss = 4.35567\n",
      "epoch no.0 train no.6600  loss = 4.02033 avg_loss = 4.31050\n",
      "epoch no.0 train no.6610  loss = 5.68527 avg_loss = 4.30715\n",
      "epoch no.0 train no.6620  loss = 4.57752 avg_loss = 4.27465\n",
      "epoch no.0 train no.6630  loss = 3.41690 avg_loss = 4.22792\n",
      "epoch no.0 train no.6640  loss = 3.77494 avg_loss = 4.21363\n",
      "epoch no.0 train no.6650  loss = 5.05883 avg_loss = 4.19424\n",
      "epoch no.0 train no.6660  loss = 5.36237 avg_loss = 4.26644\n",
      "epoch no.0 train no.6670  loss = 6.18187 avg_loss = 4.33904\n",
      "epoch no.0 train no.6680  loss = 7.21599 avg_loss = 4.32715\n",
      "epoch no.0 train no.6690  loss = 3.96602 avg_loss = 4.30978\n",
      "epoch no.0 train no.6700  loss = 4.72820 avg_loss = 4.26495\n",
      "epoch no.0 train no.6710  loss = 3.57530 avg_loss = 4.24758\n",
      "epoch no.0 train no.6720  loss = 5.02842 avg_loss = 4.23567\n",
      "epoch no.0 train no.6730  loss = 4.94994 avg_loss = 4.20823\n",
      "epoch no.0 train no.6740  loss = 3.46537 avg_loss = 4.19110\n",
      "epoch no.0 train no.6750  loss = 3.09342 avg_loss = 4.19184\n",
      "epoch no.0 train no.6760  loss = 4.60536 avg_loss = 4.23327\n",
      "epoch no.0 train no.6770  loss = 3.64785 avg_loss = 4.24557\n",
      "epoch no.0 train no.6780  loss = 4.09376 avg_loss = 4.22886\n",
      "epoch no.0 train no.6790  loss = 4.27957 avg_loss = 4.16233\n",
      "epoch no.0 train no.6800  loss = 5.02669 avg_loss = 4.14590\n",
      "epoch no.0 train no.6810  loss = 7.56826 avg_loss = 4.19735\n",
      "epoch no.0 train no.6820  loss = 4.03266 avg_loss = 4.21338\n",
      "epoch no.0 train no.6830  loss = 5.07437 avg_loss = 4.14882\n",
      "epoch no.0 train no.6840  loss = 2.66247 avg_loss = 4.16077\n",
      "epoch no.0 train no.6850  loss = 4.20750 avg_loss = 4.23327\n",
      "epoch no.0 train no.6860  loss = 7.24815 avg_loss = 4.26721\n",
      "epoch no.0 train no.6870  loss = 3.73736 avg_loss = 4.22010\n",
      "epoch no.0 train no.6880  loss = 3.80765 avg_loss = 4.21976\n",
      "epoch no.0 train no.6890  loss = 5.20362 avg_loss = 4.28083\n",
      "epoch no.0 train no.6900  loss = 2.66600 avg_loss = 4.22031\n",
      "epoch no.0 train no.6910  loss = 3.38639 avg_loss = 4.19215\n",
      "epoch no.0 train no.6920  loss = 4.28993 avg_loss = 4.16620\n",
      "epoch no.0 train no.6930  loss = 4.16596 avg_loss = 4.20815\n",
      "epoch no.0 train no.6940  loss = 5.72345 avg_loss = 4.23485\n",
      "epoch no.0 train no.6950  loss = 4.18148 avg_loss = 4.16396\n",
      "epoch no.0 train no.6960  loss = 4.02389 avg_loss = 4.14998\n",
      "epoch no.0 train no.6970  loss = 5.24322 avg_loss = 4.08509\n",
      "epoch no.0 train no.6980  loss = 4.77587 avg_loss = 4.09567\n",
      "epoch no.0 train no.6990  loss = 7.15191 avg_loss = 4.11705\n",
      "epoch no.0 train no.7000  loss = 4.02442 avg_loss = 4.11526\n",
      "5\n",
      "to_tokens: ['▁가을', '엔', '▁오면', '▁듣는', '▁노래', '</s>', '</s>']\n",
      "여름이 오면 좋은 노래들</s>\n",
      "epoch no.0 train no.7010  loss = 5.14522 avg_loss = 4.18022\n",
      "epoch no.0 train no.7020  loss = 1.89854 avg_loss = 4.23804\n",
      "epoch no.0 train no.7030  loss = 4.78582 avg_loss = 4.25718\n",
      "epoch no.0 train no.7040  loss = 6.14092 avg_loss = 4.30254\n",
      "epoch no.0 train no.7050  loss = 4.67205 avg_loss = 4.26012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.7060  loss = 5.14359 avg_loss = 4.26515\n",
      "epoch no.0 train no.7070  loss = 4.33531 avg_loss = 4.29027\n",
      "epoch no.0 train no.7080  loss = 6.15222 avg_loss = 4.32793\n",
      "epoch no.0 train no.7090  loss = 4.01679 avg_loss = 4.30818\n",
      "epoch no.0 train no.7100  loss = 5.41558 avg_loss = 4.32168\n",
      "epoch no.0 train no.7110  loss = 4.92325 avg_loss = 4.32106\n",
      "epoch no.0 train no.7120  loss = 5.20900 avg_loss = 4.31312\n",
      "epoch no.0 train no.7130  loss = 4.68934 avg_loss = 4.33451\n",
      "epoch no.0 train no.7140  loss = 4.63494 avg_loss = 4.35563\n",
      "epoch no.0 train no.7150  loss = 3.39930 avg_loss = 4.32445\n",
      "epoch no.0 train no.7160  loss = 4.03290 avg_loss = 4.36115\n",
      "epoch no.0 train no.7170  loss = 4.00025 avg_loss = 4.35496\n",
      "epoch no.0 train no.7180  loss = 3.63664 avg_loss = 4.30520\n",
      "epoch no.0 train no.7190  loss = 4.54994 avg_loss = 4.29994\n",
      "epoch no.0 train no.7200  loss = 2.25057 avg_loss = 4.21328\n",
      "epoch no.0 train no.7210  loss = 3.55454 avg_loss = 4.16320\n",
      "epoch no.0 train no.7220  loss = 4.42343 avg_loss = 4.18870\n",
      "epoch no.0 train no.7230  loss = 2.20732 avg_loss = 4.19086\n",
      "epoch no.0 train no.7240  loss = 6.24176 avg_loss = 4.23184\n",
      "epoch no.0 train no.7250  loss = 3.00795 avg_loss = 4.22656\n",
      "epoch no.0 train no.7260  loss = 2.49663 avg_loss = 4.21761\n",
      "epoch no.0 train no.7270  loss = 3.66549 avg_loss = 4.18659\n",
      "epoch no.0 train no.7280  loss = 4.85864 avg_loss = 4.18860\n",
      "epoch no.0 train no.7290  loss = 6.31372 avg_loss = 4.24128\n",
      "epoch no.0 train no.7300  loss = 5.82745 avg_loss = 4.30702\n",
      "epoch no.0 train no.7310  loss = 3.02541 avg_loss = 4.31116\n",
      "epoch no.0 train no.7320  loss = 4.46187 avg_loss = 4.29773\n",
      "epoch no.0 train no.7330  loss = 3.64088 avg_loss = 4.27869\n",
      "epoch no.0 train no.7340  loss = 5.04349 avg_loss = 4.23552\n",
      "epoch no.0 train no.7350  loss = 3.74774 avg_loss = 4.21434\n",
      "epoch no.0 train no.7360  loss = 3.28703 avg_loss = 4.14687\n",
      "epoch no.0 train no.7370  loss = 4.29683 avg_loss = 4.22145\n",
      "epoch no.0 train no.7380  loss = 6.06474 avg_loss = 4.22254\n",
      "epoch no.0 train no.7390  loss = 3.84266 avg_loss = 4.18711\n",
      "epoch no.0 train no.7400  loss = 2.70637 avg_loss = 4.11886\n",
      "epoch no.0 train no.7410  loss = 2.94846 avg_loss = 4.13963\n",
      "epoch no.0 train no.7420  loss = 5.29134 avg_loss = 4.12701\n",
      "epoch no.0 train no.7430  loss = 2.42697 avg_loss = 4.03879\n",
      "epoch no.0 train no.7440  loss = 3.80814 avg_loss = 4.05269\n",
      "epoch no.0 train no.7450  loss = 7.12210 avg_loss = 4.16937\n",
      "epoch no.0 train no.7460  loss = 5.01971 avg_loss = 4.26540\n",
      "epoch no.0 train no.7470  loss = 4.76813 avg_loss = 4.25170\n",
      "epoch no.0 train no.7480  loss = 4.59475 avg_loss = 4.32178\n",
      "epoch no.0 train no.7490  loss = 4.86117 avg_loss = 4.31104\n",
      "epoch no.0 train no.7500  loss = 2.81382 avg_loss = 4.24588\n",
      "epoch no.0 train no.7510  loss = 3.58058 avg_loss = 4.21781\n",
      "epoch no.0 train no.7520  loss = 3.48487 avg_loss = 4.19005\n",
      "epoch no.0 train no.7530  loss = 5.56179 avg_loss = 4.18668\n",
      "epoch no.0 train no.7540  loss = 3.93661 avg_loss = 4.17243\n",
      "epoch no.0 train no.7550  loss = 3.09253 avg_loss = 4.17451\n",
      "epoch no.0 train no.7560  loss = 2.59354 avg_loss = 4.17411\n",
      "epoch no.0 train no.7570  loss = 3.79663 avg_loss = 4.15519\n",
      "epoch no.0 train no.7580  loss = 3.88927 avg_loss = 4.14371\n",
      "epoch no.0 train no.7590  loss = 5.29726 avg_loss = 4.11910\n",
      "epoch no.0 train no.7600  loss = 3.04641 avg_loss = 4.11423\n",
      "epoch no.0 train no.7610  loss = 3.82878 avg_loss = 4.12645\n",
      "epoch no.0 train no.7620  loss = 4.66811 avg_loss = 4.12698\n",
      "epoch no.0 train no.7630  loss = 2.58728 avg_loss = 4.13530\n",
      "epoch no.0 train no.7640  loss = 4.57243 avg_loss = 4.18564\n",
      "epoch no.0 train no.7650  loss = 2.90587 avg_loss = 4.18089\n",
      "epoch no.0 train no.7660  loss = 5.66001 avg_loss = 4.19018\n",
      "epoch no.0 train no.7670  loss = 4.25911 avg_loss = 4.25069\n",
      "epoch no.0 train no.7680  loss = 3.98429 avg_loss = 4.24460\n",
      "epoch no.0 train no.7690  loss = 3.90272 avg_loss = 4.24336\n",
      "epoch no.0 train no.7700  loss = 5.23789 avg_loss = 4.22703\n",
      "epoch no.0 train no.7710  loss = 2.61499 avg_loss = 4.26690\n",
      "epoch no.0 train no.7720  loss = 3.41722 avg_loss = 4.30279\n",
      "epoch no.0 train no.7730  loss = 4.55600 avg_loss = 4.24811\n",
      "epoch no.0 train no.7740  loss = 4.84448 avg_loss = 4.23787\n",
      "epoch no.0 train no.7750  loss = 5.19906 avg_loss = 4.28535\n",
      "epoch no.0 train no.7760  loss = 2.93100 avg_loss = 4.30442\n",
      "epoch no.0 train no.7770  loss = 4.44951 avg_loss = 4.25013\n",
      "epoch no.0 train no.7780  loss = 4.68192 avg_loss = 4.25086\n",
      "epoch no.0 train no.7790  loss = 3.39047 avg_loss = 4.20825\n",
      "epoch no.0 train no.7800  loss = 5.00492 avg_loss = 4.20490\n",
      "epoch no.0 train no.7810  loss = 5.88095 avg_loss = 4.19090\n",
      "epoch no.0 train no.7820  loss = 3.72006 avg_loss = 4.14119\n",
      "epoch no.0 train no.7830  loss = 2.96563 avg_loss = 4.10648\n",
      "epoch no.0 train no.7840  loss = 5.37471 avg_loss = 4.15063\n",
      "epoch no.0 train no.7850  loss = 6.63649 avg_loss = 4.16554\n",
      "epoch no.0 train no.7860  loss = 2.69804 avg_loss = 4.11515\n",
      "epoch no.0 train no.7870  loss = 6.13286 avg_loss = 4.19106\n",
      "epoch no.0 train no.7880  loss = 4.81403 avg_loss = 4.16989\n",
      "epoch no.0 train no.7890  loss = 4.23546 avg_loss = 4.13328\n",
      "epoch no.0 train no.7900  loss = 3.88968 avg_loss = 4.15943\n",
      "epoch no.0 train no.7910  loss = 3.56559 avg_loss = 4.12902\n",
      "epoch no.0 train no.7920  loss = 3.11060 avg_loss = 4.12127\n",
      "epoch no.0 train no.7930  loss = 4.26559 avg_loss = 4.08627\n",
      "epoch no.0 train no.7940  loss = 3.28689 avg_loss = 4.05298\n",
      "epoch no.0 train no.7950  loss = 3.86340 avg_loss = 4.08137\n",
      "epoch no.0 train no.7960  loss = 4.59337 avg_loss = 4.04625\n",
      "epoch no.0 train no.7970  loss = 5.04613 avg_loss = 4.04848\n",
      "epoch no.0 train no.7980  loss = 4.74855 avg_loss = 4.03217\n",
      "epoch no.0 train no.7990  loss = 3.95526 avg_loss = 4.07187\n",
      "epoch no.0 train no.8000  loss = 4.52967 avg_loss = 4.09284\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.0 train no.8010  loss = 5.47267 avg_loss = 4.12104\n",
      "epoch no.0 train no.8020  loss = 2.53772 avg_loss = 4.15112\n",
      "epoch no.0 train no.8030  loss = 3.90337 avg_loss = 4.13470\n",
      "epoch no.0 train no.8040  loss = 4.48666 avg_loss = 4.17063\n",
      "epoch no.0 train no.8050  loss = 5.86947 avg_loss = 4.20626\n",
      "epoch no.0 train no.8060  loss = 3.33285 avg_loss = 4.21052\n",
      "epoch no.0 train no.8070  loss = 4.98368 avg_loss = 4.20019\n",
      "epoch no.0 train no.8080  loss = 2.89778 avg_loss = 4.20964\n",
      "epoch no.0 train no.8090  loss = 4.47687 avg_loss = 4.25354\n",
      "epoch no.0 train no.8100  loss = 5.49812 avg_loss = 4.27351\n",
      "epoch no.0 train no.8110  loss = 7.11544 avg_loss = 4.28734\n",
      "epoch no.0 train no.8120  loss = 5.51809 avg_loss = 4.28080\n",
      "epoch no.0 train no.8130  loss = 3.10710 avg_loss = 4.28084\n",
      "epoch no.0 train no.8140  loss = 2.85491 avg_loss = 4.28129\n",
      "epoch no.0 train no.8150  loss = 6.93312 avg_loss = 4.32290\n",
      "epoch no.0 train no.8160  loss = 3.78073 avg_loss = 4.32189\n",
      "epoch no.0 train no.8170  loss = 3.42527 avg_loss = 4.27555\n",
      "epoch no.0 train no.8180  loss = 1.94573 avg_loss = 4.22819\n",
      "epoch no.0 train no.8190  loss = 3.49534 avg_loss = 4.24249\n",
      "epoch no.0 train no.8200  loss = 3.68954 avg_loss = 4.21784\n",
      "epoch no.0 train no.8210  loss = 2.84773 avg_loss = 4.15605\n",
      "epoch no.0 train no.8220  loss = 5.63339 avg_loss = 4.15195\n",
      "epoch no.0 train no.8230  loss = 2.97106 avg_loss = 4.13425\n",
      "epoch no.0 train no.8240  loss = 4.79262 avg_loss = 4.11792\n",
      "epoch no.0 train no.8250  loss = 6.21361 avg_loss = 4.13261\n",
      "epoch no.0 train no.8260  loss = 2.85857 avg_loss = 4.12829\n",
      "epoch no.0 train no.8270  loss = 3.13370 avg_loss = 4.04039\n",
      "epoch no.0 train no.8280  loss = 3.37727 avg_loss = 4.04886\n",
      "epoch no.0 train no.8290  loss = 6.07445 avg_loss = 4.05264\n",
      "epoch no.0 train no.8300  loss = 2.66501 avg_loss = 4.07546\n",
      "epoch no.0 train no.8310  loss = 5.03894 avg_loss = 4.08908\n",
      "epoch no.0 train no.8320  loss = 2.37465 avg_loss = 4.07956\n",
      "epoch no.0 train no.8330  loss = 4.81366 avg_loss = 4.07829\n",
      "epoch no.0 train no.8340  loss = 4.85948 avg_loss = 4.08267\n",
      "epoch no.0 train no.8350  loss = 4.08798 avg_loss = 4.08617\n",
      "epoch no.0 train no.8360  loss = 5.99210 avg_loss = 4.06981\n",
      "epoch no.0 train no.8370  loss = 5.16086 avg_loss = 4.04947\n",
      "epoch no.0 train no.8380  loss = 3.61404 avg_loss = 4.03511\n",
      "epoch no.0 train no.8390  loss = 2.84941 avg_loss = 4.12123\n",
      "epoch no.0 train no.8400  loss = 3.84942 avg_loss = 4.12517\n",
      "epoch no.0 train no.8410  loss = 4.40177 avg_loss = 4.10902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.8420  loss = 2.53185 avg_loss = 4.10549\n",
      "epoch no.0 train no.8430  loss = 3.84123 avg_loss = 4.10126\n",
      "epoch no.0 train no.8440  loss = 5.32766 avg_loss = 4.11322\n",
      "epoch no.0 train no.8450  loss = 3.00794 avg_loss = 4.08687\n",
      "epoch no.0 train no.8460  loss = 5.35225 avg_loss = 4.05853\n",
      "epoch no.0 train no.8470  loss = 5.29300 avg_loss = 4.08085\n",
      "epoch no.0 train no.8480  loss = 2.75292 avg_loss = 4.07996\n",
      "epoch no.0 train no.8490  loss = 4.35366 avg_loss = 4.03354\n",
      "epoch no.0 train no.8500  loss = 7.09679 avg_loss = 4.05842\n",
      "epoch no.0 train no.8510  loss = 4.98063 avg_loss = 4.11570\n",
      "epoch no.0 train no.8520  loss = 2.73263 avg_loss = 4.10549\n",
      "epoch no.0 train no.8530  loss = 4.54299 avg_loss = 4.12526\n",
      "epoch no.0 train no.8540  loss = 3.80718 avg_loss = 4.13193\n",
      "epoch no.0 train no.8550  loss = 6.14250 avg_loss = 4.18000\n",
      "epoch no.0 train no.8560  loss = 3.10257 avg_loss = 4.18924\n",
      "epoch no.0 train no.8570  loss = 5.59534 avg_loss = 4.26205\n",
      "epoch no.0 train no.8580  loss = 4.34682 avg_loss = 4.23700\n",
      "epoch no.0 train no.8590  loss = 2.64109 avg_loss = 4.19149\n",
      "epoch no.0 train no.8600  loss = 3.53431 avg_loss = 4.22774\n",
      "epoch no.0 train no.8610  loss = 3.60971 avg_loss = 4.23859\n",
      "epoch no.0 train no.8620  loss = 3.60818 avg_loss = 4.17513\n",
      "epoch no.0 train no.8630  loss = 6.19671 avg_loss = 4.20183\n",
      "epoch no.0 train no.8640  loss = 5.22050 avg_loss = 4.22893\n",
      "epoch no.0 train no.8650  loss = 3.68215 avg_loss = 4.22891\n",
      "epoch no.0 train no.8660  loss = 6.71226 avg_loss = 4.25274\n",
      "epoch no.0 train no.8670  loss = 5.42964 avg_loss = 4.26075\n",
      "epoch no.0 train no.8680  loss = 5.61095 avg_loss = 4.21433\n",
      "epoch no.0 train no.8690  loss = 4.22132 avg_loss = 4.20192\n",
      "epoch no.0 train no.8700  loss = 3.42006 avg_loss = 4.19246\n",
      "epoch no.0 train no.8710  loss = 4.31731 avg_loss = 4.20598\n",
      "epoch no.0 train no.8720  loss = 3.83891 avg_loss = 4.20988\n",
      "epoch no.0 train no.8730  loss = 2.97375 avg_loss = 4.22335\n",
      "epoch no.0 train no.8740  loss = 3.53736 avg_loss = 4.21428\n",
      "epoch no.0 train no.8750  loss = 2.57695 avg_loss = 4.26532\n",
      "epoch no.0 train no.8760  loss = 4.43252 avg_loss = 4.27207\n",
      "epoch no.0 train no.8770  loss = 2.90421 avg_loss = 4.21116\n",
      "epoch no.0 train no.8780  loss = 4.20917 avg_loss = 4.16554\n",
      "epoch no.0 train no.8790  loss = 3.87055 avg_loss = 4.13910\n",
      "epoch no.0 train no.8800  loss = 4.22094 avg_loss = 4.07400\n",
      "epoch no.0 train no.8810  loss = 3.87421 avg_loss = 4.07661\n",
      "epoch no.0 train no.8820  loss = 4.82847 avg_loss = 4.06208\n",
      "epoch no.0 train no.8830  loss = 5.62058 avg_loss = 4.05200\n",
      "epoch no.0 train no.8840  loss = 4.78306 avg_loss = 4.09793\n",
      "epoch no.0 train no.8850  loss = 2.14817 avg_loss = 4.08871\n",
      "epoch no.0 train no.8860  loss = 7.08957 avg_loss = 4.11391\n",
      "epoch no.0 train no.8870  loss = 6.67446 avg_loss = 4.14726\n",
      "epoch no.0 train no.8880  loss = 3.58863 avg_loss = 4.11025\n",
      "epoch no.0 train no.8890  loss = 2.95420 avg_loss = 4.10735\n",
      "epoch no.0 train no.8900  loss = 4.23577 avg_loss = 4.06217\n",
      "epoch no.0 train no.8910  loss = 3.31352 avg_loss = 4.00426\n",
      "epoch no.0 train no.8920  loss = 6.00105 avg_loss = 4.10349\n",
      "epoch no.0 train no.8930  loss = 6.55807 avg_loss = 4.17339\n",
      "epoch no.0 train no.8940  loss = 2.77304 avg_loss = 4.10606\n",
      "epoch no.0 train no.8950  loss = 3.79697 avg_loss = 4.09726\n",
      "epoch no.0 train no.8960  loss = 5.66205 avg_loss = 4.11684\n",
      "epoch no.0 train no.8970  loss = 3.43860 avg_loss = 4.15951\n",
      "epoch no.0 train no.8980  loss = 3.92591 avg_loss = 4.14878\n",
      "epoch no.0 train no.8990  loss = 4.88550 avg_loss = 4.22659\n",
      "epoch no.0 train no.9000  loss = 4.11508 avg_loss = 4.23203\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁시원한', '▁노래', '닉', '</s>', '</s>']\n",
      "여름엔 신나는 일렉트로닉 음악</s>\n",
      "epoch no.0 train no.9010  loss = 2.10733 avg_loss = 4.15938\n",
      "epoch no.0 train no.9020  loss = 2.67253 avg_loss = 4.12055\n",
      "epoch no.0 train no.9030  loss = 5.32489 avg_loss = 4.13253\n",
      "epoch no.0 train no.9040  loss = 3.31548 avg_loss = 4.10855\n",
      "epoch no.0 train no.9050  loss = 4.96970 avg_loss = 4.12443\n",
      "epoch no.0 train no.9060  loss = 3.60805 avg_loss = 4.15164\n",
      "epoch no.0 train no.9070  loss = 3.76106 avg_loss = 4.10432\n",
      "epoch no.0 train no.9080  loss = 2.82821 avg_loss = 4.10645\n",
      "epoch no.0 train no.9090  loss = 3.64738 avg_loss = 4.04731\n",
      "epoch no.0 train no.9100  loss = 4.66948 avg_loss = 4.08870\n",
      "epoch no.0 train no.9110  loss = 4.55320 avg_loss = 4.10814\n",
      "epoch no.0 train no.9120  loss = 3.46886 avg_loss = 4.10498\n",
      "epoch no.0 train no.9130  loss = 3.90386 avg_loss = 4.12014\n",
      "epoch no.0 train no.9140  loss = 3.85707 avg_loss = 4.11488\n",
      "epoch no.0 train no.9150  loss = 3.86913 avg_loss = 4.08270\n",
      "epoch no.0 train no.9160  loss = 6.28089 avg_loss = 4.07741\n",
      "epoch no.0 train no.9170  loss = 8.24571 avg_loss = 4.13047\n",
      "epoch no.0 train no.9180  loss = 3.56463 avg_loss = 4.11788\n",
      "epoch no.0 train no.9190  loss = 4.29285 avg_loss = 4.14718\n",
      "epoch no.0 train no.9200  loss = 4.64611 avg_loss = 4.15453\n",
      "epoch no.0 train no.9210  loss = 4.21942 avg_loss = 4.13472\n",
      "epoch no.0 train no.9220  loss = 3.24489 avg_loss = 4.09932\n",
      "epoch no.0 train no.9230  loss = 3.01067 avg_loss = 4.08374\n",
      "epoch no.0 train no.9240  loss = 4.63996 avg_loss = 4.12079\n",
      "epoch no.0 train no.9250  loss = 4.02698 avg_loss = 4.08119\n",
      "epoch no.0 train no.9260  loss = 3.89856 avg_loss = 4.05302\n",
      "epoch no.0 train no.9270  loss = 2.74152 avg_loss = 4.01641\n",
      "epoch no.0 train no.9280  loss = 4.08930 avg_loss = 4.01908\n",
      "epoch no.0 train no.9290  loss = 2.91439 avg_loss = 4.04829\n",
      "epoch no.0 train no.9300  loss = 4.58743 avg_loss = 4.08655\n",
      "epoch no.0 train no.9310  loss = 4.45295 avg_loss = 4.12561\n",
      "epoch no.0 train no.9320  loss = 3.46542 avg_loss = 4.12656\n",
      "epoch no.0 train no.9330  loss = 5.38336 avg_loss = 4.16329\n",
      "epoch no.0 train no.9340  loss = 2.62906 avg_loss = 4.12611\n",
      "epoch no.0 train no.9350  loss = 5.36121 avg_loss = 4.12182\n",
      "epoch no.0 train no.9360  loss = 2.66833 avg_loss = 4.04614\n",
      "epoch no.0 train no.9370  loss = 4.05667 avg_loss = 4.14699\n",
      "epoch no.0 train no.9380  loss = 4.30039 avg_loss = 4.16675\n",
      "epoch no.0 train no.9390  loss = 4.62478 avg_loss = 4.11284\n",
      "epoch no.0 train no.9400  loss = 6.64591 avg_loss = 4.15045\n",
      "epoch no.0 train no.9410  loss = 5.59440 avg_loss = 4.18734\n",
      "epoch no.0 train no.9420  loss = 4.19353 avg_loss = 4.18403\n",
      "epoch no.0 train no.9430  loss = 4.85956 avg_loss = 4.20206\n",
      "epoch no.0 train no.9440  loss = 4.36507 avg_loss = 4.22218\n",
      "epoch no.0 train no.9450  loss = 4.79027 avg_loss = 4.24093\n",
      "epoch no.0 train no.9460  loss = 5.11661 avg_loss = 4.20352\n",
      "epoch no.0 train no.9470  loss = 5.36891 avg_loss = 4.18333\n",
      "epoch no.0 train no.9480  loss = 3.93705 avg_loss = 4.16049\n",
      "epoch no.0 train no.9490  loss = 4.82608 avg_loss = 4.15729\n",
      "epoch no.0 train no.9500  loss = 5.24551 avg_loss = 4.13984\n",
      "epoch no.0 train no.9510  loss = 1.73192 avg_loss = 4.08817\n",
      "epoch no.0 train no.9520  loss = 3.29992 avg_loss = 4.04674\n",
      "epoch no.0 train no.9530  loss = 4.18644 avg_loss = 4.11435\n",
      "epoch no.0 train no.9540  loss = 5.99896 avg_loss = 4.11235\n",
      "epoch no.0 train no.9550  loss = 3.26646 avg_loss = 4.12300\n",
      "epoch no.0 train no.9560  loss = 3.67405 avg_loss = 4.08028\n",
      "epoch no.0 train no.9570  loss = 3.43621 avg_loss = 4.02035\n",
      "epoch no.0 train no.9580  loss = 4.91641 avg_loss = 3.98577\n",
      "epoch no.0 train no.9590  loss = 4.74174 avg_loss = 3.96762\n",
      "epoch no.0 train no.9600  loss = 2.36567 avg_loss = 3.97180\n",
      "epoch no.0 train no.9610  loss = 4.39059 avg_loss = 4.01249\n",
      "epoch no.0 train no.9620  loss = 4.15589 avg_loss = 4.02725\n",
      "epoch no.0 train no.9630  loss = 3.95400 avg_loss = 4.06042\n",
      "epoch no.0 train no.9640  loss = 5.10600 avg_loss = 4.12306\n",
      "epoch no.0 train no.9650  loss = 3.55162 avg_loss = 4.08485\n",
      "epoch no.0 train no.9660  loss = 3.88958 avg_loss = 4.07749\n",
      "epoch no.0 train no.9670  loss = 3.43933 avg_loss = 4.08262\n",
      "epoch no.0 train no.9680  loss = 4.61008 avg_loss = 4.10466\n",
      "epoch no.0 train no.9690  loss = 2.15181 avg_loss = 4.05160\n",
      "epoch no.0 train no.9700  loss = 5.89759 avg_loss = 4.01008\n",
      "epoch no.0 train no.9710  loss = 7.29672 avg_loss = 4.10368\n",
      "epoch no.0 train no.9720  loss = 6.01774 avg_loss = 4.13941\n",
      "epoch no.0 train no.9730  loss = 2.65509 avg_loss = 4.14742\n",
      "epoch no.0 train no.9740  loss = 3.04916 avg_loss = 4.19893\n",
      "epoch no.0 train no.9750  loss = 5.17934 avg_loss = 4.26060\n",
      "epoch no.0 train no.9760  loss = 3.17162 avg_loss = 4.25644\n",
      "epoch no.0 train no.9770  loss = 4.65907 avg_loss = 4.24947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.9780  loss = 4.19541 avg_loss = 4.31375\n",
      "epoch no.0 train no.9790  loss = 4.93749 avg_loss = 4.35789\n",
      "epoch no.0 train no.9800  loss = 4.59388 avg_loss = 4.37320\n",
      "epoch no.0 train no.9810  loss = 4.54237 avg_loss = 4.35626\n",
      "epoch no.0 train no.9820  loss = 2.18354 avg_loss = 4.27794\n",
      "epoch no.0 train no.9830  loss = 5.04692 avg_loss = 4.30365\n",
      "epoch no.0 train no.9840  loss = 6.13879 avg_loss = 4.25535\n",
      "epoch no.0 train no.9850  loss = 4.54848 avg_loss = 4.23259\n",
      "epoch no.0 train no.9860  loss = 4.49177 avg_loss = 4.25152\n",
      "epoch no.0 train no.9870  loss = 3.75605 avg_loss = 4.21234\n",
      "epoch no.0 train no.9880  loss = 3.92567 avg_loss = 4.18067\n",
      "epoch no.0 train no.9890  loss = 2.83081 avg_loss = 4.19770\n",
      "epoch no.0 train no.9900  loss = 3.09698 avg_loss = 4.16652\n",
      "epoch no.0 train no.9910  loss = 4.42197 avg_loss = 4.16636\n",
      "epoch no.0 train no.9920  loss = 3.65090 avg_loss = 4.15980\n",
      "epoch no.0 train no.9930  loss = 5.36837 avg_loss = 4.27594\n",
      "epoch no.0 train no.9940  loss = 3.07107 avg_loss = 4.26675\n",
      "epoch no.0 train no.9950  loss = 3.44850 avg_loss = 4.29259\n",
      "epoch no.0 train no.9960  loss = 5.67139 avg_loss = 4.30244\n",
      "epoch no.0 train no.9970  loss = 4.67526 avg_loss = 4.28993\n",
      "epoch no.0 train no.9980  loss = 4.60550 avg_loss = 4.26811\n",
      "epoch no.0 train no.9990  loss = 4.64511 avg_loss = 4.22077\n",
      "epoch no.0 train no.10000  loss = 2.88838 avg_loss = 4.25176\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁좋은', '▁노래', '한', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 잔잔한 재즈</s>\n",
      "epoch no.0 train no.10010  loss = 2.67792 avg_loss = 4.28464\n",
      "epoch no.0 train no.10020  loss = 4.56698 avg_loss = 4.27555\n",
      "epoch no.0 train no.10030  loss = 5.00044 avg_loss = 4.30095\n",
      "epoch no.0 train no.10040  loss = 4.21762 avg_loss = 4.29103\n",
      "epoch no.0 train no.10050  loss = 3.94429 avg_loss = 4.26460\n",
      "epoch no.0 train no.10060  loss = 4.23549 avg_loss = 4.28685\n",
      "epoch no.0 train no.10070  loss = 3.62432 avg_loss = 4.27826\n",
      "epoch no.0 train no.10080  loss = 3.27793 avg_loss = 4.25556\n",
      "epoch no.0 train no.10090  loss = 4.17185 avg_loss = 4.22568\n",
      "epoch no.0 train no.10100  loss = 3.97808 avg_loss = 4.22843\n",
      "epoch no.0 train no.10110  loss = 2.38772 avg_loss = 4.16908\n",
      "epoch no.0 train no.10120  loss = 1.67024 avg_loss = 4.14917\n",
      "epoch no.0 train no.10130  loss = 3.55616 avg_loss = 4.14776\n",
      "epoch no.0 train no.10140  loss = 4.05375 avg_loss = 4.20711\n",
      "epoch no.0 train no.10150  loss = 4.12819 avg_loss = 4.19233\n",
      "epoch no.0 train no.10160  loss = 3.36104 avg_loss = 4.21472\n",
      "epoch no.0 train no.10170  loss = 3.18267 avg_loss = 4.17097\n",
      "epoch no.0 train no.10180  loss = 2.81888 avg_loss = 4.10777\n",
      "epoch no.0 train no.10190  loss = 4.11210 avg_loss = 4.15463\n",
      "epoch no.0 train no.10200  loss = 5.47010 avg_loss = 4.16479\n",
      "epoch no.0 train no.10210  loss = 3.53851 avg_loss = 4.13834\n",
      "epoch no.0 train no.10220  loss = 2.97288 avg_loss = 4.09183\n",
      "epoch no.0 train no.10230  loss = 4.08624 avg_loss = 4.11866\n",
      "epoch no.0 train no.10240  loss = 2.44128 avg_loss = 4.11966\n",
      "epoch no.0 train no.10250  loss = 6.71522 avg_loss = 4.12569\n",
      "epoch no.0 train no.10260  loss = 3.27860 avg_loss = 4.08903\n",
      "epoch no.0 train no.10270  loss = 2.28988 avg_loss = 4.01472\n",
      "epoch no.0 train no.10280  loss = 2.36050 avg_loss = 4.04583\n",
      "epoch no.0 train no.10290  loss = 7.45850 avg_loss = 4.16949\n",
      "epoch no.0 train no.10300  loss = 2.66459 avg_loss = 4.12269\n",
      "epoch no.0 train no.10310  loss = 3.70755 avg_loss = 4.10120\n",
      "epoch no.0 train no.10320  loss = 3.98208 avg_loss = 4.04959\n",
      "epoch no.0 train no.10330  loss = 3.02083 avg_loss = 4.04190\n",
      "epoch no.0 train no.10340  loss = 3.92951 avg_loss = 4.02981\n",
      "epoch no.0 train no.10350  loss = 5.53639 avg_loss = 4.07190\n",
      "epoch no.0 train no.10360  loss = 7.18479 avg_loss = 4.12375\n",
      "epoch no.0 train no.10370  loss = 5.24512 avg_loss = 4.11468\n",
      "epoch no.0 train no.10380  loss = 3.10816 avg_loss = 4.12366\n",
      "epoch no.0 train no.10390  loss = 3.42421 avg_loss = 4.18357\n",
      "epoch no.0 train no.10400  loss = 3.68146 avg_loss = 4.17434\n",
      "epoch no.0 train no.10410  loss = 2.50327 avg_loss = 4.18763\n",
      "epoch no.0 train no.10420  loss = 3.31088 avg_loss = 4.17093\n",
      "epoch no.0 train no.10430  loss = 4.01681 avg_loss = 4.17642\n",
      "epoch no.0 train no.10440  loss = 2.85506 avg_loss = 4.18998\n",
      "epoch no.0 train no.10450  loss = 5.39707 avg_loss = 4.18413\n",
      "epoch no.0 train no.10460  loss = 3.12963 avg_loss = 4.13977\n",
      "epoch no.0 train no.10470  loss = 3.33303 avg_loss = 4.14829\n",
      "epoch no.0 train no.10480  loss = 2.53598 avg_loss = 4.13919\n",
      "epoch no.0 train no.10490  loss = 3.94385 avg_loss = 4.17973\n",
      "epoch no.0 train no.10500  loss = 4.08617 avg_loss = 4.17188\n",
      "epoch no.0 train no.10510  loss = 3.77964 avg_loss = 4.16221\n",
      "epoch no.0 train no.10520  loss = 4.32828 avg_loss = 4.16083\n",
      "epoch no.0 train no.10530  loss = 4.88329 avg_loss = 4.21347\n",
      "epoch no.0 train no.10540  loss = 2.84750 avg_loss = 4.17621\n",
      "epoch no.0 train no.10550  loss = 2.26457 avg_loss = 4.15910\n",
      "epoch no.0 train no.10560  loss = 5.17900 avg_loss = 4.14427\n",
      "epoch no.0 train no.10570  loss = 3.76057 avg_loss = 4.14995\n",
      "epoch no.0 train no.10580  loss = 3.36366 avg_loss = 4.14734\n",
      "epoch no.0 train no.10590  loss = 4.37974 avg_loss = 4.14005\n",
      "epoch no.0 train no.10600  loss = 1.76964 avg_loss = 4.07527\n",
      "epoch no.0 train no.10610  loss = 4.21521 avg_loss = 4.04052\n",
      "epoch no.0 train no.10620  loss = 2.67248 avg_loss = 4.08906\n",
      "epoch no.0 train no.10630  loss = 3.36505 avg_loss = 4.16920\n",
      "epoch no.0 train no.10640  loss = 3.18234 avg_loss = 4.14674\n",
      "epoch no.0 train no.10650  loss = 1.94388 avg_loss = 4.15543\n",
      "epoch no.0 train no.10660  loss = 4.10407 avg_loss = 4.16501\n",
      "epoch no.0 train no.10670  loss = 4.76520 avg_loss = 4.09734\n",
      "epoch no.0 train no.10680  loss = 3.69871 avg_loss = 4.18105\n",
      "epoch no.0 train no.10690  loss = 3.82666 avg_loss = 4.17272\n",
      "epoch no.0 train no.10700  loss = 3.24116 avg_loss = 4.23004\n",
      "epoch no.0 train no.10710  loss = 6.50796 avg_loss = 4.29746\n",
      "epoch no.0 train no.10720  loss = 3.79204 avg_loss = 4.28874\n",
      "epoch no.0 train no.10730  loss = 2.94720 avg_loss = 4.28347\n",
      "epoch no.0 train no.10740  loss = 5.73250 avg_loss = 4.30883\n",
      "epoch no.0 train no.10750  loss = 4.71284 avg_loss = 4.31261\n",
      "epoch no.0 train no.10760  loss = 2.93898 avg_loss = 4.32179\n",
      "epoch no.0 train no.10770  loss = 5.66604 avg_loss = 4.32448\n",
      "epoch no.0 train no.10780  loss = 5.32320 avg_loss = 4.31409\n",
      "epoch no.0 train no.10790  loss = 3.83220 avg_loss = 4.27164\n",
      "epoch no.0 train no.10800  loss = 5.47277 avg_loss = 4.27411\n",
      "epoch no.0 train no.10810  loss = 3.58144 avg_loss = 4.27281\n",
      "epoch no.0 train no.10820  loss = 3.56318 avg_loss = 4.26805\n",
      "epoch no.0 train no.10830  loss = 5.45972 avg_loss = 4.23596\n",
      "epoch no.0 train no.10840  loss = 3.95756 avg_loss = 4.22685\n",
      "epoch no.0 train no.10850  loss = 3.87737 avg_loss = 4.22919\n",
      "epoch no.0 train no.10860  loss = 3.34024 avg_loss = 4.15576\n",
      "epoch no.0 train no.10870  loss = 4.84911 avg_loss = 4.17531\n",
      "epoch no.0 train no.10880  loss = 2.92862 avg_loss = 4.12588\n",
      "epoch no.0 train no.10890  loss = 2.89762 avg_loss = 4.14139\n",
      "epoch no.0 train no.10900  loss = 4.57657 avg_loss = 4.18756\n",
      "epoch no.0 train no.10910  loss = 4.62098 avg_loss = 4.15351\n",
      "epoch no.0 train no.10920  loss = 3.82025 avg_loss = 4.17230\n",
      "epoch no.0 train no.10930  loss = 2.40670 avg_loss = 4.14309\n",
      "epoch no.0 train no.10940  loss = 2.64659 avg_loss = 4.16675\n",
      "epoch no.0 train no.10950  loss = 3.17386 avg_loss = 4.17400\n",
      "epoch no.0 train no.10960  loss = 6.59704 avg_loss = 4.21000\n",
      "epoch no.0 train no.10970  loss = 3.39800 avg_loss = 4.20268\n",
      "epoch no.0 train no.10980  loss = 4.39182 avg_loss = 4.18266\n",
      "epoch no.0 train no.10990  loss = 4.74239 avg_loss = 4.20150\n",
      "epoch no.0 train no.11000  loss = 4.46040 avg_loss = 4.15464\n",
      "3\n",
      "to_tokens: ['▁가을', '밤', '▁시원한', '노래', '</s>']\n",
      "여름엔 여름노래</s>\n",
      "epoch no.0 train no.11010  loss = 3.56554 avg_loss = 4.11272\n",
      "epoch no.0 train no.11020  loss = 3.63888 avg_loss = 4.17707\n",
      "epoch no.0 train no.11030  loss = 4.48162 avg_loss = 4.12320\n",
      "epoch no.0 train no.11040  loss = 5.13902 avg_loss = 4.20024\n",
      "epoch no.0 train no.11050  loss = 2.13840 avg_loss = 4.18150\n",
      "epoch no.0 train no.11060  loss = 2.83419 avg_loss = 4.15676\n",
      "epoch no.0 train no.11070  loss = 3.71059 avg_loss = 4.16269\n",
      "epoch no.0 train no.11080  loss = 3.39345 avg_loss = 4.10253\n",
      "epoch no.0 train no.11090  loss = 4.18114 avg_loss = 4.09831\n",
      "epoch no.0 train no.11100  loss = 3.84925 avg_loss = 4.09383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.11110  loss = 4.99018 avg_loss = 4.08800\n",
      "epoch no.0 train no.11120  loss = 3.01057 avg_loss = 4.04300\n",
      "epoch no.0 train no.11130  loss = 1.90003 avg_loss = 3.98632\n",
      "epoch no.0 train no.11140  loss = 4.48541 avg_loss = 3.97654\n",
      "epoch no.0 train no.11150  loss = 3.41614 avg_loss = 3.96750\n",
      "epoch no.0 train no.11160  loss = 2.43729 avg_loss = 3.94801\n",
      "epoch no.0 train no.11170  loss = 3.07465 avg_loss = 3.96997\n",
      "epoch no.0 train no.11180  loss = 4.09933 avg_loss = 4.00421\n",
      "epoch no.0 train no.11190  loss = 2.92849 avg_loss = 4.04401\n",
      "epoch no.0 train no.11200  loss = 3.77608 avg_loss = 4.09775\n",
      "epoch no.0 train no.11210  loss = 4.81079 avg_loss = 4.12123\n",
      "epoch no.0 train no.11220  loss = 3.41264 avg_loss = 4.08352\n",
      "epoch no.0 train no.11230  loss = 2.64244 avg_loss = 4.06498\n",
      "epoch no.0 train no.11240  loss = 2.47460 avg_loss = 4.01409\n",
      "epoch no.0 train no.11250  loss = 2.68874 avg_loss = 3.97237\n",
      "epoch no.0 train no.11260  loss = 4.08915 avg_loss = 3.99008\n",
      "epoch no.0 train no.11270  loss = 4.02302 avg_loss = 4.02309\n",
      "epoch no.0 train no.11280  loss = 5.88747 avg_loss = 4.07099\n",
      "epoch no.0 train no.11290  loss = 2.02558 avg_loss = 4.05978\n",
      "epoch no.0 train no.11300  loss = 1.93705 avg_loss = 3.98503\n",
      "epoch no.0 train no.11310  loss = 4.02013 avg_loss = 4.01222\n",
      "epoch no.0 train no.11320  loss = 3.58838 avg_loss = 4.06532\n",
      "epoch no.0 train no.11330  loss = 3.77009 avg_loss = 4.12127\n",
      "epoch no.0 train no.11340  loss = 4.04036 avg_loss = 4.11472\n",
      "epoch no.0 train no.11350  loss = 3.44567 avg_loss = 4.11852\n",
      "epoch no.0 train no.11360  loss = 2.45800 avg_loss = 4.10209\n",
      "epoch no.0 train no.11370  loss = 3.24492 avg_loss = 4.05396\n",
      "epoch no.0 train no.11380  loss = 2.90686 avg_loss = 4.09127\n",
      "epoch no.0 train no.11390  loss = 6.08019 avg_loss = 4.14511\n",
      "epoch no.0 train no.11400  loss = 2.90705 avg_loss = 4.10938\n",
      "epoch no.0 train no.11410  loss = 3.47084 avg_loss = 4.11363\n",
      "epoch no.0 train no.11420  loss = 4.53757 avg_loss = 4.11801\n",
      "epoch no.0 train no.11430  loss = 8.42843 avg_loss = 4.17609\n",
      "epoch no.0 train no.11440  loss = 2.76247 avg_loss = 4.12673\n",
      "epoch no.0 train no.11450  loss = 2.78230 avg_loss = 4.05567\n",
      "epoch no.0 train no.11460  loss = 2.40637 avg_loss = 4.08164\n",
      "epoch no.0 train no.11470  loss = 4.15551 avg_loss = 4.04557\n",
      "epoch no.0 train no.11480  loss = 5.16111 avg_loss = 4.09012\n",
      "epoch no.0 train no.11490  loss = 3.04460 avg_loss = 4.07611\n",
      "epoch no.0 train no.11500  loss = 3.56505 avg_loss = 4.07981\n",
      "epoch no.0 train no.11510  loss = 3.53474 avg_loss = 4.09009\n",
      "epoch no.0 train no.11520  loss = 3.33303 avg_loss = 4.01789\n",
      "epoch no.0 train no.11530  loss = 3.92909 avg_loss = 4.05558\n",
      "epoch no.0 train no.11540  loss = 4.05349 avg_loss = 4.04244\n",
      "epoch no.0 train no.11550  loss = 4.27696 avg_loss = 4.03871\n",
      "epoch no.0 train no.11560  loss = 4.34808 avg_loss = 4.03866\n",
      "epoch no.0 train no.11570  loss = 3.88922 avg_loss = 4.07610\n",
      "epoch no.0 train no.11580  loss = 3.69833 avg_loss = 4.02110\n",
      "epoch no.0 train no.11590  loss = 6.26543 avg_loss = 4.03958\n",
      "epoch no.0 train no.11600  loss = 3.50675 avg_loss = 3.98332\n",
      "epoch no.0 train no.11610  loss = 5.10732 avg_loss = 4.01237\n",
      "epoch no.0 train no.11620  loss = 4.79385 avg_loss = 4.06455\n",
      "epoch no.0 train no.11630  loss = 6.28540 avg_loss = 4.14017\n",
      "epoch no.0 train no.11640  loss = 4.91847 avg_loss = 4.13442\n",
      "epoch no.0 train no.11650  loss = 5.32870 avg_loss = 4.15135\n",
      "epoch no.0 train no.11660  loss = 3.83962 avg_loss = 4.16095\n",
      "epoch no.0 train no.11670  loss = 4.21178 avg_loss = 4.12476\n",
      "epoch no.0 train no.11680  loss = 2.31689 avg_loss = 4.12135\n",
      "epoch no.0 train no.11690  loss = 3.40462 avg_loss = 4.13370\n",
      "epoch no.0 train no.11700  loss = 3.07751 avg_loss = 4.14685\n",
      "epoch no.0 train no.11710  loss = 4.16318 avg_loss = 4.11292\n",
      "epoch no.0 train no.11720  loss = 2.65123 avg_loss = 4.11577\n",
      "epoch no.0 train no.11730  loss = 5.52255 avg_loss = 4.08151\n",
      "epoch no.0 train no.11740  loss = 4.78907 avg_loss = 4.04577\n",
      "epoch no.0 train no.11750  loss = 4.95992 avg_loss = 4.07067\n",
      "epoch no.0 train no.11760  loss = 3.53899 avg_loss = 4.11130\n",
      "epoch no.0 train no.11770  loss = 2.81196 avg_loss = 4.08604\n",
      "epoch no.0 train no.11780  loss = 5.05009 avg_loss = 4.13323\n",
      "epoch no.0 train no.11790  loss = 4.64341 avg_loss = 4.14225\n",
      "epoch no.0 train no.11800  loss = 4.89854 avg_loss = 4.12166\n",
      "epoch no.0 train no.11810  loss = 4.36064 avg_loss = 4.13294\n",
      "epoch no.0 train no.11820  loss = 3.25678 avg_loss = 4.16588\n",
      "epoch no.0 train no.11830  loss = 5.08319 avg_loss = 4.20381\n",
      "epoch no.0 train no.11840  loss = 4.54472 avg_loss = 4.18933\n",
      "epoch no.0 train no.11850  loss = 3.83560 avg_loss = 4.19149\n",
      "epoch no.0 train no.11860  loss = 3.09105 avg_loss = 4.14232\n",
      "epoch no.0 train no.11870  loss = 4.81765 avg_loss = 4.14756\n",
      "epoch no.0 train no.11880  loss = 4.56399 avg_loss = 4.18277\n",
      "epoch no.0 train no.11890  loss = 5.64883 avg_loss = 4.17873\n",
      "epoch no.0 train no.11900  loss = 5.10517 avg_loss = 4.20581\n",
      "epoch no.0 train no.11910  loss = 3.22153 avg_loss = 4.17078\n",
      "epoch no.0 train no.11920  loss = 3.31091 avg_loss = 4.12092\n",
      "epoch no.0 train no.11930  loss = 2.04957 avg_loss = 4.18889\n",
      "epoch no.0 train no.11940  loss = 5.50439 avg_loss = 4.16891\n",
      "epoch no.0 train no.11950  loss = 4.42929 avg_loss = 4.18308\n",
      "epoch no.0 train no.11960  loss = 5.02771 avg_loss = 4.13954\n",
      "epoch no.0 train no.11970  loss = 4.82499 avg_loss = 4.17685\n",
      "epoch no.0 train no.11980  loss = 3.48370 avg_loss = 4.14655\n",
      "epoch no.0 train no.11990  loss = 4.62033 avg_loss = 4.14784\n",
      "epoch no.0 train no.12000  loss = 3.99444 avg_loss = 4.16301\n",
      "3\n",
      "to_tokens: ['▁', '밤', '▁시원한', '▁맥주', '</s>']\n",
      "여름엔 시원한 음악</s>\n",
      "epoch no.0 train no.12010  loss = 4.35494 avg_loss = 4.13150\n",
      "epoch no.0 train no.12020  loss = 3.44792 avg_loss = 4.13500\n",
      "epoch no.0 train no.12030  loss = 3.43211 avg_loss = 4.15471\n",
      "epoch no.0 train no.12040  loss = 4.16124 avg_loss = 4.14783\n",
      "epoch no.0 train no.12050  loss = 3.62411 avg_loss = 4.13762\n",
      "epoch no.0 train no.12060  loss = 5.67451 avg_loss = 4.14165\n",
      "epoch no.0 train no.12070  loss = 3.51730 avg_loss = 4.17307\n",
      "epoch no.0 train no.12080  loss = 5.75898 avg_loss = 4.10564\n",
      "epoch no.0 train no.12090  loss = 5.00249 avg_loss = 4.08625\n",
      "epoch no.0 train no.12100  loss = 3.92495 avg_loss = 4.11743\n",
      "epoch no.0 train no.12110  loss = 3.85999 avg_loss = 4.04928\n",
      "epoch no.0 train no.12120  loss = 4.65258 avg_loss = 4.02922\n",
      "epoch no.0 train no.12130  loss = 4.56152 avg_loss = 4.00465\n",
      "epoch no.0 train no.12140  loss = 4.85480 avg_loss = 4.02830\n",
      "epoch no.0 train no.12150  loss = 3.52341 avg_loss = 4.03548\n",
      "epoch no.0 train no.12160  loss = 2.41438 avg_loss = 4.09199\n",
      "epoch no.0 train no.12170  loss = 5.30153 avg_loss = 4.16103\n",
      "epoch no.0 train no.12180  loss = 4.05745 avg_loss = 4.10645\n",
      "epoch no.0 train no.12190  loss = 5.59642 avg_loss = 4.13365\n",
      "epoch no.0 train no.12200  loss = 3.15851 avg_loss = 4.08911\n",
      "epoch no.0 train no.12210  loss = 3.41561 avg_loss = 4.03854\n",
      "epoch no.0 train no.12220  loss = 5.24628 avg_loss = 4.07251\n",
      "epoch no.0 train no.12230  loss = 3.56219 avg_loss = 4.05522\n",
      "epoch no.0 train no.12240  loss = 3.29520 avg_loss = 4.04294\n",
      "epoch no.0 train no.12250  loss = 4.40630 avg_loss = 4.01070\n",
      "epoch no.0 train no.12260  loss = 3.09708 avg_loss = 4.00063\n",
      "epoch no.0 train no.12270  loss = 3.03982 avg_loss = 3.99583\n",
      "epoch no.0 train no.12280  loss = 5.26587 avg_loss = 4.03254\n",
      "epoch no.0 train no.12290  loss = 3.21089 avg_loss = 4.06308\n",
      "epoch no.0 train no.12300  loss = 3.62040 avg_loss = 4.04088\n",
      "epoch no.0 train no.12310  loss = 3.97183 avg_loss = 4.09324\n",
      "epoch no.0 train no.12320  loss = 4.65972 avg_loss = 4.07958\n",
      "epoch no.0 train no.12330  loss = 4.40626 avg_loss = 4.08046\n",
      "epoch no.0 train no.12340  loss = 4.39437 avg_loss = 4.06007\n",
      "epoch no.0 train no.12350  loss = 2.78629 avg_loss = 4.01076\n",
      "epoch no.0 train no.12360  loss = 4.39778 avg_loss = 4.06908\n",
      "epoch no.0 train no.12370  loss = 3.37597 avg_loss = 4.04734\n",
      "epoch no.0 train no.12380  loss = 5.48082 avg_loss = 4.05185\n",
      "epoch no.0 train no.12390  loss = 5.39434 avg_loss = 4.04165\n",
      "epoch no.0 train no.12400  loss = 4.15382 avg_loss = 4.05483\n",
      "epoch no.0 train no.12410  loss = 4.01340 avg_loss = 4.09070\n",
      "epoch no.0 train no.12420  loss = 3.15446 avg_loss = 4.08518\n",
      "epoch no.0 train no.12430  loss = 3.49651 avg_loss = 4.09729\n",
      "epoch no.0 train no.12440  loss = 3.40886 avg_loss = 4.09150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.12450  loss = 6.49628 avg_loss = 4.14420\n",
      "epoch no.0 train no.12460  loss = 3.71975 avg_loss = 4.11214\n",
      "epoch no.0 train no.12470  loss = 4.43069 avg_loss = 4.12516\n",
      "epoch no.0 train no.12480  loss = 3.93484 avg_loss = 4.11908\n",
      "epoch no.0 train no.12490  loss = 4.34320 avg_loss = 4.10398\n",
      "epoch no.0 train no.12500  loss = 4.27889 avg_loss = 4.14201\n",
      "epoch no.0 train no.12510  loss = 5.29517 avg_loss = 4.11962\n",
      "epoch no.0 train no.12520  loss = 4.42678 avg_loss = 4.11051\n",
      "epoch no.0 train no.12530  loss = 4.97102 avg_loss = 4.09810\n",
      "epoch no.0 train no.12540  loss = 4.38166 avg_loss = 4.08527\n",
      "epoch no.0 train no.12550  loss = 3.20485 avg_loss = 4.07096\n",
      "epoch no.0 train no.12560  loss = 2.38592 avg_loss = 4.05073\n",
      "epoch no.0 train no.12570  loss = 4.18932 avg_loss = 4.09811\n",
      "epoch no.0 train no.12580  loss = 5.23694 avg_loss = 4.08795\n",
      "epoch no.0 train no.12590  loss = 6.78891 avg_loss = 4.12531\n",
      "epoch no.0 train no.12600  loss = 4.76378 avg_loss = 4.12712\n",
      "epoch no.0 train no.12610  loss = 4.73115 avg_loss = 4.14994\n",
      "epoch no.0 train no.12620  loss = 4.19650 avg_loss = 4.15913\n",
      "epoch no.0 train no.12630  loss = 3.93569 avg_loss = 4.15434\n",
      "epoch no.0 train no.12640  loss = 3.37242 avg_loss = 4.08175\n",
      "epoch no.0 train no.12650  loss = 5.47036 avg_loss = 4.01812\n",
      "epoch no.0 train no.12660  loss = 5.80715 avg_loss = 4.00329\n",
      "epoch no.0 train no.12670  loss = 4.30315 avg_loss = 3.94587\n",
      "epoch no.0 train no.12680  loss = 4.87427 avg_loss = 4.00560\n",
      "epoch no.0 train no.12690  loss = 3.22588 avg_loss = 3.98880\n",
      "epoch no.0 train no.12700  loss = 2.48412 avg_loss = 3.98517\n",
      "epoch no.0 train no.12710  loss = 3.91792 avg_loss = 3.93233\n",
      "epoch no.0 train no.12720  loss = 3.27187 avg_loss = 3.89309\n",
      "epoch no.0 train no.12730  loss = 5.84537 avg_loss = 3.91360\n",
      "epoch no.0 train no.12740  loss = 4.44393 avg_loss = 3.87766\n",
      "epoch no.0 train no.12750  loss = 2.80372 avg_loss = 3.84224\n",
      "epoch no.0 train no.12760  loss = 3.07187 avg_loss = 3.86569\n",
      "epoch no.0 train no.12770  loss = 4.62145 avg_loss = 3.88194\n",
      "epoch no.0 train no.12780  loss = 6.41118 avg_loss = 3.92239\n",
      "epoch no.0 train no.12790  loss = 3.80121 avg_loss = 3.93425\n",
      "epoch no.0 train no.12800  loss = 3.10599 avg_loss = 3.93041\n",
      "epoch no.0 train no.12810  loss = 3.87717 avg_loss = 3.93265\n",
      "epoch no.0 train no.12820  loss = 5.01157 avg_loss = 3.93007\n",
      "epoch no.0 train no.12830  loss = 4.01006 avg_loss = 4.02608\n",
      "epoch no.0 train no.12840  loss = 3.85519 avg_loss = 4.02191\n",
      "epoch no.0 train no.12850  loss = 4.27704 avg_loss = 4.06381\n",
      "epoch no.0 train no.12860  loss = 3.85427 avg_loss = 4.01895\n",
      "epoch no.0 train no.12870  loss = 3.17628 avg_loss = 4.04695\n",
      "epoch no.0 train no.12880  loss = 4.90411 avg_loss = 4.06674\n",
      "epoch no.0 train no.12890  loss = 4.32950 avg_loss = 4.04088\n",
      "epoch no.0 train no.12900  loss = 5.61089 avg_loss = 3.99528\n",
      "epoch no.0 train no.12910  loss = 3.40310 avg_loss = 3.97189\n",
      "epoch no.0 train no.12920  loss = 3.35355 avg_loss = 4.04368\n",
      "epoch no.0 train no.12930  loss = 3.51236 avg_loss = 4.02848\n",
      "epoch no.0 train no.12940  loss = 3.96515 avg_loss = 4.04578\n",
      "epoch no.0 train no.12950  loss = 2.63781 avg_loss = 4.01591\n",
      "epoch no.0 train no.12960  loss = 1.67507 avg_loss = 4.00354\n",
      "epoch no.0 train no.12970  loss = 5.74161 avg_loss = 4.06758\n",
      "epoch no.0 train no.12980  loss = 3.88481 avg_loss = 4.09016\n",
      "epoch no.0 train no.12990  loss = 6.83194 avg_loss = 4.13192\n",
      "epoch no.0 train no.13000  loss = 6.60550 avg_loss = 4.16738\n",
      "5\n",
      "to_tokens: ['▁', '밤', '▁되면', '▁생각', '나는', '▁노래', '</s>']\n",
      "여름이 되면 생각나는 노래</s>\n",
      "epoch no.0 train no.13010  loss = 2.51168 avg_loss = 4.18201\n",
      "epoch no.0 train no.13020  loss = 3.86826 avg_loss = 4.11397\n",
      "epoch no.0 train no.13030  loss = 4.15227 avg_loss = 4.10315\n",
      "epoch no.0 train no.13040  loss = 3.64880 avg_loss = 4.06343\n",
      "epoch no.0 train no.13050  loss = 4.61939 avg_loss = 4.11508\n",
      "epoch no.0 train no.13060  loss = 3.79827 avg_loss = 4.11426\n",
      "epoch no.0 train no.13070  loss = 4.31175 avg_loss = 4.09713\n",
      "epoch no.0 train no.13080  loss = 4.29267 avg_loss = 4.05367\n",
      "epoch no.0 train no.13090  loss = 4.07526 avg_loss = 4.07533\n",
      "epoch no.0 train no.13100  loss = 3.85591 avg_loss = 4.10402\n",
      "epoch no.0 train no.13110  loss = 2.07371 avg_loss = 4.13102\n",
      "epoch no.0 train no.13120  loss = 3.45848 avg_loss = 4.14229\n",
      "epoch no.0 train no.13130  loss = 3.71378 avg_loss = 4.12662\n",
      "epoch no.0 train no.13140  loss = 4.30488 avg_loss = 4.12487\n",
      "epoch no.0 train no.13150  loss = 4.48046 avg_loss = 4.17592\n",
      "epoch no.0 train no.13160  loss = 3.05646 avg_loss = 4.14153\n",
      "epoch no.0 train no.13170  loss = 4.11973 avg_loss = 4.20583\n",
      "epoch no.0 train no.13180  loss = 4.15186 avg_loss = 4.14887\n",
      "epoch no.0 train no.13190  loss = 5.28981 avg_loss = 4.13129\n",
      "epoch no.0 train no.13200  loss = 3.38177 avg_loss = 4.12269\n",
      "epoch no.0 train no.13210  loss = 4.63587 avg_loss = 4.11291\n",
      "epoch no.0 train no.13220  loss = 6.53396 avg_loss = 4.13208\n",
      "epoch no.0 train no.13230  loss = 4.75786 avg_loss = 4.15156\n",
      "epoch no.0 train no.13240  loss = 5.27378 avg_loss = 4.17933\n",
      "epoch no.0 train no.13250  loss = 3.90357 avg_loss = 4.13137\n",
      "epoch no.0 train no.13260  loss = 3.55759 avg_loss = 4.10080\n",
      "epoch no.0 train no.13270  loss = 3.48137 avg_loss = 4.08742\n",
      "epoch no.0 train no.13280  loss = 3.86820 avg_loss = 4.11206\n",
      "epoch no.0 train no.13290  loss = 2.87856 avg_loss = 4.08602\n",
      "epoch no.0 train no.13300  loss = 3.96934 avg_loss = 4.10912\n",
      "epoch no.0 train no.13310  loss = 3.45957 avg_loss = 4.09554\n",
      "epoch no.0 train no.13320  loss = 4.03892 avg_loss = 4.08950\n",
      "epoch no.0 train no.13330  loss = 4.74115 avg_loss = 4.09482\n",
      "epoch no.0 train no.13340  loss = 4.35318 avg_loss = 4.08157\n",
      "epoch no.0 train no.13350  loss = 5.13519 avg_loss = 4.10378\n",
      "epoch no.0 train no.13360  loss = 2.97781 avg_loss = 4.08444\n",
      "epoch no.0 train no.13370  loss = 4.13752 avg_loss = 4.05870\n",
      "epoch no.0 train no.13380  loss = 4.03053 avg_loss = 4.05754\n",
      "epoch no.0 train no.13390  loss = 1.69522 avg_loss = 4.01616\n",
      "epoch no.0 train no.13400  loss = 4.85255 avg_loss = 4.07497\n",
      "epoch no.0 train no.13410  loss = 3.92439 avg_loss = 4.03588\n",
      "epoch no.0 train no.13420  loss = 6.25636 avg_loss = 4.10791\n",
      "epoch no.0 train no.13430  loss = 2.38867 avg_loss = 4.06187\n",
      "epoch no.0 train no.13440  loss = 3.67053 avg_loss = 4.04118\n",
      "epoch no.0 train no.13450  loss = 3.51219 avg_loss = 4.06424\n",
      "epoch no.0 train no.13460  loss = 2.40339 avg_loss = 4.01534\n",
      "epoch no.0 train no.13470  loss = 3.91389 avg_loss = 4.04820\n",
      "epoch no.0 train no.13480  loss = 4.47390 avg_loss = 4.00939\n",
      "epoch no.0 train no.13490  loss = 3.40168 avg_loss = 3.93234\n",
      "epoch no.0 train no.13500  loss = 4.38601 avg_loss = 3.98211\n",
      "epoch no.0 train no.13510  loss = 5.34420 avg_loss = 3.96073\n",
      "epoch no.0 train no.13520  loss = 3.16938 avg_loss = 3.92065\n",
      "epoch no.0 train no.13530  loss = 4.61216 avg_loss = 3.93168\n",
      "epoch no.0 train no.13540  loss = 3.42733 avg_loss = 3.94672\n",
      "epoch no.0 train no.13550  loss = 5.53305 avg_loss = 3.98993\n",
      "epoch no.0 train no.13560  loss = 4.98029 avg_loss = 3.98115\n",
      "epoch no.0 train no.13570  loss = 5.58133 avg_loss = 3.98277\n",
      "epoch no.0 train no.13580  loss = 4.37462 avg_loss = 4.06432\n",
      "epoch no.0 train no.13590  loss = 5.86971 avg_loss = 4.08822\n",
      "epoch no.0 train no.13600  loss = 4.22361 avg_loss = 4.05371\n",
      "epoch no.0 train no.13610  loss = 5.56546 avg_loss = 4.03689\n",
      "epoch no.0 train no.13620  loss = 4.82011 avg_loss = 4.09749\n",
      "epoch no.0 train no.13630  loss = 4.46237 avg_loss = 4.14738\n",
      "epoch no.0 train no.13640  loss = 4.44478 avg_loss = 4.12978\n",
      "epoch no.0 train no.13650  loss = 3.20433 avg_loss = 4.17380\n",
      "epoch no.0 train no.13660  loss = 3.67790 avg_loss = 4.17352\n",
      "epoch no.0 train no.13670  loss = 3.41123 avg_loss = 4.15306\n",
      "epoch no.0 train no.13680  loss = 4.63114 avg_loss = 4.11203\n",
      "epoch no.0 train no.13690  loss = 2.76438 avg_loss = 4.07438\n",
      "epoch no.0 train no.13700  loss = 3.88717 avg_loss = 4.03966\n",
      "epoch no.0 train no.13710  loss = 3.92561 avg_loss = 4.02720\n",
      "epoch no.0 train no.13720  loss = 2.33772 avg_loss = 4.01353\n",
      "epoch no.0 train no.13730  loss = 3.28515 avg_loss = 4.02880\n",
      "epoch no.0 train no.13740  loss = 3.04976 avg_loss = 3.99242\n",
      "epoch no.0 train no.13750  loss = 3.85080 avg_loss = 3.95503\n",
      "epoch no.0 train no.13760  loss = 3.43754 avg_loss = 3.93695\n",
      "epoch no.0 train no.13770  loss = 6.42743 avg_loss = 4.00129\n",
      "epoch no.0 train no.13780  loss = 4.08976 avg_loss = 4.01492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.13790  loss = 2.69876 avg_loss = 3.97657\n",
      "epoch no.0 train no.13800  loss = 3.40659 avg_loss = 3.96558\n",
      "epoch no.0 train no.13810  loss = 8.15325 avg_loss = 4.00800\n",
      "epoch no.0 train no.13820  loss = 3.51846 avg_loss = 4.01373\n",
      "epoch no.0 train no.13830  loss = 2.11808 avg_loss = 4.00682\n",
      "epoch no.0 train no.13840  loss = 3.29000 avg_loss = 4.01880\n",
      "epoch no.0 train no.13850  loss = 2.37729 avg_loss = 3.99743\n",
      "epoch no.0 train no.13860  loss = 4.36446 avg_loss = 3.98044\n",
      "epoch no.0 train no.13870  loss = 4.27645 avg_loss = 4.03173\n",
      "epoch no.0 train no.13880  loss = 6.82731 avg_loss = 4.14791\n",
      "epoch no.0 train no.13890  loss = 4.37916 avg_loss = 4.16581\n",
      "epoch no.0 train no.13900  loss = 3.22206 avg_loss = 4.17140\n",
      "epoch no.0 train no.13910  loss = 5.77539 avg_loss = 4.25834\n",
      "epoch no.0 train no.13920  loss = 2.97440 avg_loss = 4.26247\n",
      "epoch no.0 train no.13930  loss = 5.99385 avg_loss = 4.24548\n",
      "epoch no.0 train no.13940  loss = 2.87754 avg_loss = 4.21845\n",
      "epoch no.0 train no.13950  loss = 3.94546 avg_loss = 4.23170\n",
      "epoch no.0 train no.13960  loss = 3.56630 avg_loss = 4.11520\n",
      "epoch no.0 train no.13970  loss = 2.47359 avg_loss = 4.04895\n",
      "epoch no.0 train no.13980  loss = 3.97518 avg_loss = 4.02059\n",
      "epoch no.0 train no.13990  loss = 3.79285 avg_loss = 4.07202\n",
      "epoch no.0 train no.14000  loss = 4.83977 avg_loss = 4.09093\n",
      "5\n",
      "to_tokens: ['▁', '밤', '에', '▁듣기', '▁감성', '리스트', '</s>']\n",
      "여름밤에 듣는 플레이리스트</s>\n",
      "epoch no.0 train no.14010  loss = 2.99468 avg_loss = 4.09341\n",
      "epoch no.0 train no.14020  loss = 3.54193 avg_loss = 4.20866\n",
      "epoch no.0 train no.14030  loss = 3.31589 avg_loss = 4.17372\n",
      "epoch no.0 train no.14040  loss = 5.50261 avg_loss = 4.16061\n",
      "epoch no.0 train no.14050  loss = 1.90264 avg_loss = 4.07757\n",
      "epoch no.0 train no.14060  loss = 1.80645 avg_loss = 4.06801\n",
      "epoch no.0 train no.14070  loss = 5.00045 avg_loss = 4.04572\n",
      "epoch no.0 train no.14080  loss = 2.98769 avg_loss = 4.07096\n",
      "epoch no.0 train no.14090  loss = 2.92248 avg_loss = 4.04722\n",
      "epoch no.0 train no.14100  loss = 4.05134 avg_loss = 4.05717\n",
      "epoch no.0 train no.14110  loss = 4.72409 avg_loss = 4.02792\n",
      "epoch no.0 train no.14120  loss = 6.10431 avg_loss = 4.00621\n",
      "epoch no.0 train no.14130  loss = 2.36299 avg_loss = 3.97169\n",
      "epoch no.0 train no.14140  loss = 4.03146 avg_loss = 3.93341\n",
      "epoch no.0 train no.14150  loss = 2.47163 avg_loss = 3.89944\n",
      "epoch no.0 train no.14160  loss = 4.75948 avg_loss = 3.93088\n",
      "epoch no.0 train no.14170  loss = 3.64437 avg_loss = 3.92538\n",
      "epoch no.0 train no.14180  loss = 3.92847 avg_loss = 3.94697\n",
      "epoch no.0 train no.14190  loss = 2.77601 avg_loss = 3.99105\n",
      "epoch no.0 train no.14200  loss = 3.85707 avg_loss = 4.00403\n",
      "epoch no.0 train no.14210  loss = 5.36881 avg_loss = 4.02558\n",
      "epoch no.0 train no.14220  loss = 4.69907 avg_loss = 4.01906\n",
      "epoch no.0 train no.14230  loss = 4.74673 avg_loss = 4.04077\n",
      "epoch no.0 train no.14240  loss = 2.68696 avg_loss = 4.06585\n",
      "epoch no.0 train no.14250  loss = 3.29953 avg_loss = 4.02144\n",
      "epoch no.0 train no.14260  loss = 5.97608 avg_loss = 4.06276\n",
      "epoch no.0 train no.14270  loss = 4.25156 avg_loss = 4.05566\n",
      "epoch no.0 train no.14280  loss = 4.11385 avg_loss = 4.07691\n",
      "epoch no.0 train no.14290  loss = 4.78239 avg_loss = 4.13674\n",
      "epoch no.0 train no.14300  loss = 2.89162 avg_loss = 4.17666\n",
      "epoch no.0 train no.14310  loss = 2.49268 avg_loss = 4.15349\n",
      "epoch no.0 train no.14320  loss = 4.65468 avg_loss = 4.18672\n",
      "epoch no.0 train no.14330  loss = 4.11913 avg_loss = 4.15855\n",
      "epoch no.0 train no.14340  loss = 5.25928 avg_loss = 4.19473\n",
      "epoch no.0 train no.14350  loss = 3.34451 avg_loss = 4.19139\n",
      "epoch no.0 train no.14360  loss = 2.37992 avg_loss = 4.17048\n",
      "epoch no.0 train no.14370  loss = 4.77075 avg_loss = 4.14506\n",
      "epoch no.0 train no.14380  loss = 6.01033 avg_loss = 4.14147\n",
      "epoch no.0 train no.14390  loss = 4.06063 avg_loss = 4.12574\n",
      "epoch no.0 train no.14400  loss = 5.06821 avg_loss = 4.08989\n",
      "epoch no.0 train no.14410  loss = 2.78598 avg_loss = 4.09053\n",
      "epoch no.0 train no.14420  loss = 3.86885 avg_loss = 4.08429\n",
      "epoch no.0 train no.14430  loss = 2.35159 avg_loss = 4.04280\n",
      "epoch no.0 train no.14440  loss = 7.23530 avg_loss = 4.04635\n",
      "epoch no.0 train no.14450  loss = 4.48182 avg_loss = 4.01900\n",
      "epoch no.0 train no.14460  loss = 4.40515 avg_loss = 4.02538\n",
      "epoch no.0 train no.14470  loss = 3.52484 avg_loss = 4.00081\n",
      "epoch no.0 train no.14480  loss = 2.81977 avg_loss = 4.01221\n",
      "epoch no.0 train no.14490  loss = 5.35868 avg_loss = 4.06992\n",
      "epoch no.0 train no.14500  loss = 4.98042 avg_loss = 4.05799\n",
      "epoch no.0 train no.14510  loss = 4.31975 avg_loss = 4.02294\n",
      "epoch no.0 train no.14520  loss = 4.27024 avg_loss = 3.99538\n",
      "epoch no.0 train no.14530  loss = 4.82521 avg_loss = 3.97397\n",
      "epoch no.0 train no.14540  loss = 2.91867 avg_loss = 3.99178\n",
      "epoch no.0 train no.14550  loss = 3.40558 avg_loss = 3.97093\n",
      "epoch no.0 train no.14560  loss = 4.59837 avg_loss = 3.98577\n",
      "epoch no.0 train no.14570  loss = 3.82816 avg_loss = 3.97101\n",
      "epoch no.0 train no.14580  loss = 3.41355 avg_loss = 4.04784\n",
      "epoch no.0 train no.14590  loss = 2.69760 avg_loss = 4.05646\n",
      "epoch no.0 train no.14600  loss = 3.49949 avg_loss = 4.00430\n",
      "epoch no.0 train no.14610  loss = 5.49152 avg_loss = 4.09790\n",
      "epoch no.0 train no.14620  loss = 5.24017 avg_loss = 4.08966\n",
      "epoch no.0 train no.14630  loss = 5.79469 avg_loss = 4.08585\n",
      "epoch no.0 train no.14640  loss = 5.09135 avg_loss = 4.06299\n",
      "epoch no.0 train no.14650  loss = 4.85002 avg_loss = 4.12231\n",
      "epoch no.0 train no.14660  loss = 3.31162 avg_loss = 4.09903\n",
      "epoch no.0 train no.14670  loss = 6.45495 avg_loss = 4.10972\n",
      "epoch no.0 train no.14680  loss = 2.96582 avg_loss = 4.08648\n",
      "epoch no.0 train no.14690  loss = 6.60519 avg_loss = 4.16208\n",
      "epoch no.0 train no.14700  loss = 2.59072 avg_loss = 4.11119\n",
      "epoch no.0 train no.14710  loss = 3.63550 avg_loss = 4.12061\n",
      "epoch no.0 train no.14720  loss = 4.50064 avg_loss = 4.15091\n",
      "epoch no.0 train no.14730  loss = 4.96903 avg_loss = 4.13409\n",
      "epoch no.0 train no.14740  loss = 1.94141 avg_loss = 4.02841\n",
      "epoch no.0 train no.14750  loss = 5.15796 avg_loss = 4.02327\n",
      "epoch no.0 train no.14760  loss = 4.89345 avg_loss = 4.06900\n",
      "epoch no.0 train no.14770  loss = 4.27743 avg_loss = 4.06734\n",
      "epoch no.0 train no.14780  loss = 3.08697 avg_loss = 4.05354\n",
      "epoch no.0 train no.14790  loss = 3.70185 avg_loss = 4.05985\n",
      "epoch no.0 train no.14800  loss = 5.92867 avg_loss = 4.06991\n",
      "epoch no.0 train no.14810  loss = 2.69845 avg_loss = 4.06083\n",
      "epoch no.0 train no.14820  loss = 5.10033 avg_loss = 4.09650\n",
      "epoch no.0 train no.14830  loss = 2.90231 avg_loss = 4.06883\n",
      "epoch no.0 train no.14840  loss = 4.69319 avg_loss = 4.03598\n",
      "epoch no.0 train no.14850  loss = 7.98846 avg_loss = 4.07602\n",
      "epoch no.0 train no.14860  loss = 4.85642 avg_loss = 4.09306\n",
      "epoch no.0 train no.14870  loss = 2.20133 avg_loss = 4.06293\n",
      "epoch no.0 train no.14880  loss = 3.31752 avg_loss = 4.12079\n",
      "epoch no.0 train no.14890  loss = 5.69375 avg_loss = 4.16123\n",
      "epoch no.0 train no.14900  loss = 2.16728 avg_loss = 4.15027\n",
      "epoch no.0 train no.14910  loss = 4.43588 avg_loss = 4.12367\n",
      "epoch no.0 train no.14920  loss = 4.95531 avg_loss = 4.08360\n",
      "epoch no.0 train no.14930  loss = 4.39989 avg_loss = 4.01267\n",
      "epoch no.0 train no.14940  loss = 7.13065 avg_loss = 4.01597\n",
      "epoch no.0 train no.14950  loss = 4.90987 avg_loss = 3.98974\n",
      "epoch no.0 train no.14960  loss = 2.69582 avg_loss = 3.92580\n",
      "epoch no.0 train no.14970  loss = 4.26901 avg_loss = 3.96343\n",
      "epoch no.0 train no.14980  loss = 3.49387 avg_loss = 3.95083\n",
      "epoch no.0 train no.14990  loss = 6.65923 avg_loss = 3.96142\n",
      "epoch no.0 train no.15000  loss = 5.15012 avg_loss = 3.93724\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁책임질', '▁플레이', '▁노래', '</s>']\n",
      "여름밤을 위한 신나는 음악</s>\n",
      "epoch no.0 train no.15010  loss = 5.20197 avg_loss = 4.01073\n",
      "epoch no.0 train no.15020  loss = 2.40223 avg_loss = 4.00369\n",
      "epoch no.0 train no.15030  loss = 3.01132 avg_loss = 3.99447\n",
      "epoch no.0 train no.15040  loss = 5.04348 avg_loss = 4.02896\n",
      "epoch no.0 train no.15050  loss = 3.87808 avg_loss = 4.05764\n",
      "epoch no.0 train no.15060  loss = 3.86129 avg_loss = 4.05283\n",
      "epoch no.0 train no.15070  loss = 4.12156 avg_loss = 4.08425\n",
      "epoch no.0 train no.15080  loss = 5.74342 avg_loss = 4.12484\n",
      "epoch no.0 train no.15090  loss = 5.33575 avg_loss = 4.14079\n",
      "epoch no.0 train no.15100  loss = 4.63310 avg_loss = 4.15365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.15110  loss = 2.98393 avg_loss = 4.11116\n",
      "epoch no.0 train no.15120  loss = 3.69296 avg_loss = 4.15284\n",
      "epoch no.0 train no.15130  loss = 2.25657 avg_loss = 4.12824\n",
      "epoch no.0 train no.15140  loss = 2.51310 avg_loss = 4.15045\n",
      "epoch no.0 train no.15150  loss = 1.69606 avg_loss = 4.14216\n",
      "epoch no.0 train no.15160  loss = 5.54618 avg_loss = 4.22651\n",
      "epoch no.0 train no.15170  loss = 3.19738 avg_loss = 4.21220\n",
      "epoch no.0 train no.15180  loss = 2.35680 avg_loss = 4.22077\n",
      "epoch no.0 train no.15190  loss = 5.07004 avg_loss = 4.22446\n",
      "epoch no.0 train no.15200  loss = 4.77442 avg_loss = 4.21353\n",
      "epoch no.0 train no.15210  loss = 3.15993 avg_loss = 4.24822\n",
      "epoch no.0 train no.15220  loss = 5.37333 avg_loss = 4.25876\n",
      "epoch no.0 train no.15230  loss = 6.61971 avg_loss = 4.24888\n",
      "epoch no.0 train no.15240  loss = 5.21866 avg_loss = 4.31287\n",
      "epoch no.0 train no.15250  loss = 4.31817 avg_loss = 4.31331\n",
      "epoch no.0 train no.15260  loss = 3.32890 avg_loss = 4.30851\n",
      "epoch no.0 train no.15270  loss = 3.60502 avg_loss = 4.34935\n",
      "epoch no.0 train no.15280  loss = 3.53700 avg_loss = 4.29992\n",
      "epoch no.0 train no.15290  loss = 2.78936 avg_loss = 4.25897\n",
      "epoch no.0 train no.15300  loss = 3.02196 avg_loss = 4.26060\n",
      "epoch no.0 train no.15310  loss = 4.97955 avg_loss = 4.22964\n",
      "epoch no.0 train no.15320  loss = 3.33924 avg_loss = 4.22074\n",
      "epoch no.0 train no.15330  loss = 6.54766 avg_loss = 4.20769\n",
      "epoch no.0 train no.15340  loss = 5.42518 avg_loss = 4.17625\n",
      "epoch no.0 train no.15350  loss = 3.57889 avg_loss = 4.12063\n",
      "epoch no.0 train no.15360  loss = 5.54730 avg_loss = 4.13913\n",
      "epoch no.0 train no.15370  loss = 3.07161 avg_loss = 4.08819\n",
      "epoch no.0 train no.15380  loss = 4.50304 avg_loss = 4.09080\n",
      "epoch no.0 train no.15390  loss = 3.06580 avg_loss = 4.05039\n",
      "epoch no.0 train no.15400  loss = 3.57522 avg_loss = 4.10549\n",
      "epoch no.0 train no.15410  loss = 4.94064 avg_loss = 4.09067\n",
      "epoch no.0 train no.15420  loss = 4.30038 avg_loss = 4.08285\n",
      "epoch no.0 train no.15430  loss = 4.20946 avg_loss = 4.08582\n",
      "epoch no.0 train no.15440  loss = 2.66372 avg_loss = 4.02660\n",
      "epoch no.0 train no.15450  loss = 3.97028 avg_loss = 4.10835\n",
      "epoch no.0 train no.15460  loss = 3.66558 avg_loss = 4.13736\n",
      "epoch no.0 train no.15470  loss = 2.73051 avg_loss = 4.09144\n",
      "epoch no.0 train no.15480  loss = 6.30670 avg_loss = 4.12507\n",
      "epoch no.0 train no.15490  loss = 3.82178 avg_loss = 4.13441\n",
      "epoch no.0 train no.15500  loss = 3.98712 avg_loss = 4.13384\n",
      "epoch no.0 train no.15510  loss = 2.63188 avg_loss = 4.05212\n",
      "epoch no.0 train no.15520  loss = 4.44698 avg_loss = 4.07031\n",
      "epoch no.0 train no.15530  loss = 4.05920 avg_loss = 4.05735\n",
      "epoch no.0 train no.15540  loss = 4.67411 avg_loss = 4.07677\n",
      "epoch no.0 train no.15550  loss = 4.35480 avg_loss = 4.13247\n",
      "epoch no.0 train no.15560  loss = 6.59125 avg_loss = 4.09510\n",
      "epoch no.0 train no.15570  loss = 5.67744 avg_loss = 4.06426\n",
      "epoch no.0 train no.15580  loss = 3.37166 avg_loss = 4.03578\n",
      "epoch no.0 train no.15590  loss = 3.62957 avg_loss = 4.08030\n",
      "epoch no.0 train no.15600  loss = 5.73554 avg_loss = 4.06506\n",
      "epoch no.0 train no.15610  loss = 2.26952 avg_loss = 4.05390\n",
      "epoch no.0 train no.15620  loss = 6.34512 avg_loss = 4.06120\n",
      "epoch no.0 train no.15630  loss = 3.83290 avg_loss = 4.10664\n",
      "epoch no.0 train no.15640  loss = 3.51259 avg_loss = 4.09506\n",
      "epoch no.0 train no.15650  loss = 3.45747 avg_loss = 4.05845\n",
      "epoch no.0 train no.15660  loss = 4.70911 avg_loss = 4.05151\n",
      "epoch no.0 train no.15670  loss = 2.96222 avg_loss = 3.99886\n",
      "epoch no.0 train no.15680  loss = 3.80379 avg_loss = 3.98373\n",
      "epoch no.0 train no.15690  loss = 2.46814 avg_loss = 3.99403\n",
      "epoch no.0 train no.15700  loss = 4.95061 avg_loss = 4.03721\n",
      "epoch no.0 train no.15710  loss = 4.38947 avg_loss = 4.13232\n",
      "epoch no.0 train no.15720  loss = 2.92558 avg_loss = 4.12919\n",
      "epoch no.0 train no.15730  loss = 5.66485 avg_loss = 4.10566\n",
      "epoch no.0 train no.15740  loss = 3.16849 avg_loss = 4.06092\n",
      "epoch no.0 train no.15750  loss = 3.10083 avg_loss = 4.05402\n",
      "epoch no.0 train no.15760  loss = 2.21286 avg_loss = 4.03149\n",
      "epoch no.0 train no.15770  loss = 4.91308 avg_loss = 4.05530\n",
      "epoch no.0 train no.15780  loss = 3.61669 avg_loss = 4.04630\n",
      "epoch no.0 train no.15790  loss = 2.59893 avg_loss = 4.04385\n",
      "epoch no.0 train no.15800  loss = 5.58273 avg_loss = 4.04889\n",
      "epoch no.0 train no.15810  loss = 2.51717 avg_loss = 3.96749\n",
      "epoch no.0 train no.15820  loss = 2.49844 avg_loss = 3.93220\n",
      "epoch no.0 train no.15830  loss = 3.68766 avg_loss = 3.93558\n",
      "epoch no.0 train no.15840  loss = 2.40735 avg_loss = 3.92502\n",
      "epoch no.0 train no.15850  loss = 2.70173 avg_loss = 3.94656\n",
      "epoch no.0 train no.15860  loss = 4.94679 avg_loss = 4.00784\n",
      "epoch no.0 train no.15870  loss = 4.90957 avg_loss = 4.02503\n",
      "epoch no.0 train no.15880  loss = 2.97171 avg_loss = 3.96126\n",
      "epoch no.0 train no.15890  loss = 5.57013 avg_loss = 3.96656\n",
      "epoch no.0 train no.15900  loss = 5.21864 avg_loss = 3.98479\n",
      "epoch no.0 train no.15910  loss = 2.66151 avg_loss = 4.00732\n",
      "epoch no.0 train no.15920  loss = 4.77071 avg_loss = 3.97098\n",
      "epoch no.0 train no.15930  loss = 2.71479 avg_loss = 4.02166\n",
      "epoch no.0 train no.15940  loss = 3.16482 avg_loss = 3.99076\n",
      "epoch no.0 train no.15950  loss = 2.48132 avg_loss = 3.92584\n",
      "epoch no.0 train no.15960  loss = 5.38755 avg_loss = 3.93656\n",
      "epoch no.0 train no.15970  loss = 7.18805 avg_loss = 3.95750\n",
      "epoch no.0 train no.15980  loss = 3.15983 avg_loss = 3.94120\n",
      "epoch no.0 train no.15990  loss = 4.08586 avg_loss = 3.97507\n",
      "epoch no.0 train no.16000  loss = 2.81552 avg_loss = 3.95699\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>', '</s>']\n",
      "여름밤에 듣기 좋은 노래들</s>\n",
      "epoch no.0 train no.16010  loss = 3.11102 avg_loss = 4.00509\n",
      "epoch no.0 train no.16020  loss = 5.86327 avg_loss = 4.02014\n",
      "epoch no.0 train no.16030  loss = 3.64020 avg_loss = 4.03032\n",
      "epoch no.0 train no.16040  loss = 2.65453 avg_loss = 4.04557\n",
      "epoch no.0 train no.16050  loss = 5.29063 avg_loss = 4.02706\n",
      "epoch no.0 train no.16060  loss = 3.77210 avg_loss = 3.97885\n",
      "epoch no.0 train no.16070  loss = 6.79731 avg_loss = 4.05501\n",
      "epoch no.0 train no.16080  loss = 4.63900 avg_loss = 3.99542\n",
      "epoch no.0 train no.16090  loss = 2.30526 avg_loss = 3.98255\n",
      "epoch no.0 train no.16100  loss = 3.30056 avg_loss = 3.98175\n",
      "epoch no.0 train no.16110  loss = 3.93148 avg_loss = 3.97455\n",
      "epoch no.0 train no.16120  loss = 3.89711 avg_loss = 3.97687\n",
      "epoch no.0 train no.16130  loss = 4.21240 avg_loss = 4.06437\n",
      "epoch no.0 train no.16140  loss = 4.98544 avg_loss = 4.04751\n",
      "epoch no.0 train no.16150  loss = 4.67464 avg_loss = 4.04431\n",
      "epoch no.0 train no.16160  loss = 4.24364 avg_loss = 4.10070\n",
      "epoch no.0 train no.16170  loss = 3.22785 avg_loss = 4.10520\n",
      "epoch no.0 train no.16180  loss = 3.48937 avg_loss = 4.07683\n",
      "epoch no.0 train no.16190  loss = 5.26831 avg_loss = 4.09218\n",
      "epoch no.0 train no.16200  loss = 4.72891 avg_loss = 4.09127\n",
      "epoch no.0 train no.16210  loss = 4.97166 avg_loss = 4.10007\n",
      "epoch no.0 train no.16220  loss = 4.96907 avg_loss = 4.06092\n",
      "epoch no.0 train no.16230  loss = 6.24704 avg_loss = 4.08240\n",
      "epoch no.0 train no.16240  loss = 3.89225 avg_loss = 4.13515\n",
      "epoch no.0 train no.16250  loss = 4.94896 avg_loss = 4.14274\n",
      "epoch no.0 train no.16260  loss = 4.76357 avg_loss = 4.10758\n",
      "epoch no.0 train no.16270  loss = 3.06458 avg_loss = 4.09450\n",
      "epoch no.0 train no.16280  loss = 4.62745 avg_loss = 4.05905\n",
      "epoch no.0 train no.16290  loss = 2.76000 avg_loss = 4.03419\n",
      "epoch no.0 train no.16300  loss = 3.89128 avg_loss = 4.07300\n",
      "epoch no.0 train no.16310  loss = 4.40971 avg_loss = 4.05194\n",
      "epoch no.0 train no.16320  loss = 3.38216 avg_loss = 4.05468\n",
      "epoch no.0 train no.16330  loss = 5.95816 avg_loss = 4.09747\n",
      "epoch no.0 train no.16340  loss = 3.75240 avg_loss = 4.06495\n",
      "epoch no.0 train no.16350  loss = 5.22099 avg_loss = 4.10563\n",
      "epoch no.0 train no.16360  loss = 3.28693 avg_loss = 4.10764\n",
      "epoch no.0 train no.16370  loss = 3.89735 avg_loss = 4.06796\n",
      "epoch no.0 train no.16380  loss = 5.92577 avg_loss = 4.09127\n",
      "epoch no.0 train no.16390  loss = 4.44471 avg_loss = 4.07380\n",
      "epoch no.0 train no.16400  loss = 5.47786 avg_loss = 4.07517\n",
      "epoch no.0 train no.16410  loss = 4.06805 avg_loss = 4.03407\n",
      "epoch no.0 train no.16420  loss = 2.61628 avg_loss = 4.07189\n",
      "epoch no.0 train no.16430  loss = 7.40551 avg_loss = 4.13712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.16440  loss = 4.59727 avg_loss = 4.15041\n",
      "epoch no.0 train no.16450  loss = 3.68867 avg_loss = 4.17065\n",
      "epoch no.0 train no.16460  loss = 6.40093 avg_loss = 4.20595\n",
      "epoch no.0 train no.16470  loss = 3.05345 avg_loss = 4.21198\n",
      "epoch no.0 train no.16480  loss = 4.02123 avg_loss = 4.17687\n",
      "epoch no.0 train no.16490  loss = 4.04905 avg_loss = 4.13670\n",
      "epoch no.0 train no.16500  loss = 4.99876 avg_loss = 4.13160\n",
      "epoch no.0 train no.16510  loss = 1.97561 avg_loss = 4.09415\n",
      "epoch no.0 train no.16520  loss = 4.04194 avg_loss = 4.08986\n",
      "epoch no.0 train no.16530  loss = 6.14309 avg_loss = 4.08791\n",
      "epoch no.0 train no.16540  loss = 5.84813 avg_loss = 4.10612\n",
      "epoch no.0 train no.16550  loss = 5.43462 avg_loss = 4.14270\n",
      "epoch no.0 train no.16560  loss = 5.18508 avg_loss = 4.09339\n",
      "epoch no.0 train no.16570  loss = 4.35411 avg_loss = 4.07067\n",
      "epoch no.0 train no.16580  loss = 4.99132 avg_loss = 4.07606\n",
      "epoch no.0 train no.16590  loss = 6.03886 avg_loss = 4.04645\n",
      "epoch no.0 train no.16600  loss = 5.41756 avg_loss = 4.02770\n",
      "epoch no.0 train no.16610  loss = 4.01036 avg_loss = 4.05813\n",
      "epoch no.0 train no.16620  loss = 2.94053 avg_loss = 4.04056\n",
      "epoch no.0 train no.16630  loss = 5.30312 avg_loss = 4.02548\n",
      "epoch no.0 train no.16640  loss = 3.90033 avg_loss = 4.09576\n",
      "epoch no.0 train no.16650  loss = 5.14181 avg_loss = 4.10618\n",
      "epoch no.0 train no.16660  loss = 3.88986 avg_loss = 4.09182\n",
      "epoch no.0 train no.16670  loss = 3.26304 avg_loss = 4.09790\n",
      "epoch no.0 train no.16680  loss = 5.51900 avg_loss = 4.13895\n",
      "epoch no.0 train no.16690  loss = 4.21482 avg_loss = 4.12349\n",
      "epoch no.0 train no.16700  loss = 4.60489 avg_loss = 4.15862\n",
      "epoch no.0 train no.16710  loss = 4.98177 avg_loss = 4.14941\n",
      "epoch no.0 train no.16720  loss = 3.23472 avg_loss = 4.10741\n",
      "epoch no.0 train no.16730  loss = 3.88843 avg_loss = 4.10754\n",
      "epoch no.0 train no.16740  loss = 3.53393 avg_loss = 4.06996\n",
      "epoch no.0 train no.16750  loss = 5.76763 avg_loss = 4.01678\n",
      "epoch no.0 train no.16760  loss = 2.86038 avg_loss = 4.03249\n",
      "epoch no.0 train no.16770  loss = 4.16475 avg_loss = 4.06347\n",
      "epoch no.0 train no.16780  loss = 3.74502 avg_loss = 4.06239\n",
      "epoch no.0 train no.16790  loss = 4.64816 avg_loss = 4.07221\n",
      "epoch no.0 train no.16800  loss = 8.22227 avg_loss = 4.16040\n",
      "epoch no.0 train no.16810  loss = 5.26456 avg_loss = 4.16360\n",
      "epoch no.0 train no.16820  loss = 2.69696 avg_loss = 4.20262\n",
      "epoch no.0 train no.16830  loss = 4.42010 avg_loss = 4.18567\n",
      "epoch no.0 train no.16840  loss = 2.63478 avg_loss = 4.10088\n",
      "epoch no.0 train no.16850  loss = 3.03910 avg_loss = 4.08562\n",
      "epoch no.0 train no.16860  loss = 3.93985 avg_loss = 4.11531\n",
      "epoch no.0 train no.16870  loss = 4.37051 avg_loss = 4.07102\n",
      "epoch no.0 train no.16880  loss = 4.06325 avg_loss = 4.06492\n",
      "epoch no.0 train no.16890  loss = 5.29982 avg_loss = 4.04751\n",
      "epoch no.0 train no.16900  loss = 3.20581 avg_loss = 4.03543\n",
      "epoch no.0 train no.16910  loss = 4.11203 avg_loss = 4.01861\n",
      "epoch no.0 train no.16920  loss = 4.98775 avg_loss = 4.08560\n",
      "epoch no.0 train no.16930  loss = 8.91536 avg_loss = 4.15952\n",
      "epoch no.0 train no.16940  loss = 2.70518 avg_loss = 4.14163\n",
      "epoch no.0 train no.16950  loss = 4.95640 avg_loss = 4.13366\n",
      "epoch no.0 train no.16960  loss = 5.27941 avg_loss = 4.15203\n",
      "epoch no.0 train no.16970  loss = 4.97995 avg_loss = 4.14033\n",
      "epoch no.0 train no.16980  loss = 4.44710 avg_loss = 4.09198\n",
      "epoch no.0 train no.16990  loss = 6.23371 avg_loss = 4.13187\n",
      "epoch no.0 train no.17000  loss = 4.68472 avg_loss = 4.11165\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁드라이브', '한', '▁음악', '</s>']\n",
      "여름밤의 잔잔한 음악</s>\n",
      "epoch no.0 train no.17010  loss = 3.69164 avg_loss = 4.20692\n",
      "epoch no.0 train no.17020  loss = 3.54021 avg_loss = 4.17409\n",
      "epoch no.0 train no.17030  loss = 3.75980 avg_loss = 4.13685\n",
      "epoch no.0 train no.17040  loss = 6.08730 avg_loss = 4.16064\n",
      "epoch no.0 train no.17050  loss = 2.82475 avg_loss = 4.11200\n",
      "epoch no.0 train no.17060  loss = 4.68067 avg_loss = 4.10004\n",
      "epoch no.0 train no.17070  loss = 5.26330 avg_loss = 4.13876\n",
      "epoch no.0 train no.17080  loss = 4.26332 avg_loss = 4.08969\n",
      "epoch no.0 train no.17090  loss = 2.51054 avg_loss = 4.08713\n",
      "epoch no.0 train no.17100  loss = 3.55515 avg_loss = 4.05649\n",
      "epoch no.0 train no.17110  loss = 3.10940 avg_loss = 4.07214\n",
      "epoch no.0 train no.17120  loss = 3.90217 avg_loss = 4.09036\n",
      "epoch no.0 train no.17130  loss = 3.66916 avg_loss = 4.09653\n",
      "epoch no.0 train no.17140  loss = 2.47756 avg_loss = 4.06438\n",
      "epoch no.0 train no.17150  loss = 6.10560 avg_loss = 4.11293\n",
      "epoch no.0 train no.17160  loss = 4.33331 avg_loss = 4.09130\n",
      "epoch no.0 train no.17170  loss = 4.05965 avg_loss = 4.08771\n",
      "epoch no.0 train no.17180  loss = 7.28309 avg_loss = 4.06936\n",
      "epoch no.0 train no.17190  loss = 2.36832 avg_loss = 3.97770\n",
      "epoch no.0 train no.17200  loss = 3.26897 avg_loss = 3.94950\n",
      "epoch no.0 train no.17210  loss = 3.58442 avg_loss = 4.00105\n",
      "epoch no.0 train no.17220  loss = 3.75009 avg_loss = 3.94847\n",
      "epoch no.0 train no.17230  loss = 4.98465 avg_loss = 3.92889\n",
      "epoch no.0 train no.17240  loss = 5.19708 avg_loss = 3.88860\n",
      "epoch no.0 train no.17250  loss = 3.02066 avg_loss = 3.93848\n",
      "epoch no.0 train no.17260  loss = 5.61707 avg_loss = 4.00876\n",
      "epoch no.0 train no.17270  loss = 3.58620 avg_loss = 3.99857\n",
      "epoch no.0 train no.17280  loss = 3.01978 avg_loss = 3.97336\n",
      "epoch no.0 train no.17290  loss = 2.76675 avg_loss = 3.96055\n",
      "epoch no.0 train no.17300  loss = 2.70985 avg_loss = 4.01055\n",
      "epoch no.0 train no.17310  loss = 4.18328 avg_loss = 4.05517\n",
      "epoch no.0 train no.17320  loss = 4.44857 avg_loss = 4.01501\n",
      "epoch no.0 train no.17330  loss = 3.94389 avg_loss = 4.03177\n",
      "epoch no.0 train no.17340  loss = 4.44110 avg_loss = 4.03871\n",
      "epoch no.0 train no.17350  loss = 3.57812 avg_loss = 4.00859\n",
      "epoch no.0 train no.17360  loss = 2.77505 avg_loss = 3.98748\n",
      "epoch no.0 train no.17370  loss = 4.85286 avg_loss = 4.01653\n",
      "epoch no.0 train no.17380  loss = 4.43689 avg_loss = 4.04863\n",
      "epoch no.0 train no.17390  loss = 3.69179 avg_loss = 4.04072\n",
      "epoch no.0 train no.17400  loss = 4.67759 avg_loss = 4.11010\n",
      "epoch no.0 train no.17410  loss = 5.14287 avg_loss = 4.18733\n",
      "epoch no.0 train no.17420  loss = 2.24121 avg_loss = 4.15377\n",
      "epoch no.0 train no.17430  loss = 2.82953 avg_loss = 4.15076\n",
      "epoch no.0 train no.17440  loss = 4.97971 avg_loss = 4.14836\n",
      "epoch no.0 train no.17450  loss = 3.87044 avg_loss = 4.14462\n",
      "epoch no.0 train no.17460  loss = 5.47881 avg_loss = 4.16289\n",
      "epoch no.0 train no.17470  loss = 3.10097 avg_loss = 4.12971\n",
      "epoch no.0 train no.17480  loss = 3.11715 avg_loss = 4.08999\n",
      "epoch no.0 train no.17490  loss = 5.11808 avg_loss = 4.08322\n",
      "epoch no.0 train no.17500  loss = 4.19199 avg_loss = 4.09923\n",
      "epoch no.0 train no.17510  loss = 4.13822 avg_loss = 4.04673\n",
      "epoch no.0 train no.17520  loss = 4.83790 avg_loss = 4.04043\n",
      "epoch no.0 train no.17530  loss = 5.17967 avg_loss = 4.01196\n",
      "epoch no.0 train no.17540  loss = 3.25651 avg_loss = 4.01832\n",
      "epoch no.0 train no.17550  loss = 4.31734 avg_loss = 4.01278\n",
      "epoch no.0 train no.17560  loss = 4.80389 avg_loss = 3.98518\n",
      "epoch no.0 train no.17570  loss = 5.08588 avg_loss = 4.00133\n",
      "epoch no.0 train no.17580  loss = 1.93543 avg_loss = 3.95342\n",
      "epoch no.0 train no.17590  loss = 3.29837 avg_loss = 3.97106\n",
      "epoch no.0 train no.17600  loss = 4.67299 avg_loss = 3.99996\n",
      "epoch no.0 train no.17610  loss = 6.43970 avg_loss = 4.01534\n",
      "epoch no.0 train no.17620  loss = 2.50893 avg_loss = 3.99712\n",
      "epoch no.0 train no.17630  loss = 2.34546 avg_loss = 3.99487\n",
      "epoch no.0 train no.17640  loss = 3.97353 avg_loss = 3.97402\n",
      "epoch no.0 train no.17650  loss = 2.97717 avg_loss = 3.93413\n",
      "epoch no.0 train no.17660  loss = 4.53265 avg_loss = 3.89857\n",
      "epoch no.0 train no.17670  loss = 1.45989 avg_loss = 3.90229\n",
      "epoch no.0 train no.17680  loss = 3.29468 avg_loss = 3.90342\n",
      "epoch no.0 train no.17690  loss = 2.10348 avg_loss = 3.88349\n",
      "epoch no.0 train no.17700  loss = 4.19879 avg_loss = 3.91552\n",
      "epoch no.0 train no.17710  loss = 2.66036 avg_loss = 3.95925\n",
      "epoch no.0 train no.17720  loss = 4.18183 avg_loss = 4.00022\n",
      "epoch no.0 train no.17730  loss = 3.58259 avg_loss = 3.94100\n",
      "epoch no.0 train no.17740  loss = 4.08937 avg_loss = 3.99713\n",
      "epoch no.0 train no.17750  loss = 3.70898 avg_loss = 3.97029\n",
      "epoch no.0 train no.17760  loss = 4.43188 avg_loss = 3.94532\n",
      "epoch no.0 train no.17770  loss = 5.26369 avg_loss = 3.91046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.17780  loss = 4.77908 avg_loss = 3.95134\n",
      "epoch no.0 train no.17790  loss = 3.27291 avg_loss = 3.96315\n",
      "epoch no.0 train no.17800  loss = 2.78407 avg_loss = 4.00192\n",
      "epoch no.0 train no.17810  loss = 3.84736 avg_loss = 3.98962\n",
      "epoch no.0 train no.17820  loss = 5.39899 avg_loss = 4.05702\n",
      "epoch no.0 train no.17830  loss = 4.95805 avg_loss = 4.03988\n",
      "epoch no.0 train no.17840  loss = 2.63032 avg_loss = 3.98914\n",
      "epoch no.0 train no.17850  loss = 2.30196 avg_loss = 4.00398\n",
      "epoch no.0 train no.17860  loss = 3.53652 avg_loss = 3.98604\n",
      "epoch no.0 train no.17870  loss = 4.49198 avg_loss = 3.97283\n",
      "epoch no.0 train no.17880  loss = 2.29887 avg_loss = 3.98849\n",
      "epoch no.0 train no.17890  loss = 5.17787 avg_loss = 4.00295\n",
      "epoch no.0 train no.17900  loss = 3.83830 avg_loss = 3.93328\n",
      "epoch no.0 train no.17910  loss = 3.20291 avg_loss = 3.94099\n",
      "epoch no.0 train no.17920  loss = 2.96007 avg_loss = 3.94564\n",
      "epoch no.0 train no.17930  loss = 5.40960 avg_loss = 4.03507\n",
      "epoch no.0 train no.17940  loss = 4.83832 avg_loss = 3.96277\n",
      "epoch no.0 train no.17950  loss = 3.53904 avg_loss = 3.98086\n",
      "epoch no.0 train no.17960  loss = 5.60983 avg_loss = 3.98137\n",
      "epoch no.0 train no.17970  loss = 3.30870 avg_loss = 4.02214\n",
      "epoch no.0 train no.17980  loss = 3.76244 avg_loss = 3.99552\n",
      "epoch no.0 train no.17990  loss = 4.31498 avg_loss = 4.03836\n",
      "epoch no.0 train no.18000  loss = 3.51662 avg_loss = 3.97093\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '을', '▁어울리는', '▁감성', '한', '▁노래', '송']\n",
      "여름밤에 듣는 잔잔한 팝</s>\n",
      "epoch no.0 train no.18010  loss = 3.62783 avg_loss = 3.99337\n",
      "epoch no.0 train no.18020  loss = 5.33870 avg_loss = 4.01615\n",
      "epoch no.0 train no.18030  loss = 5.19259 avg_loss = 4.00169\n",
      "epoch no.0 train no.18040  loss = 7.05043 avg_loss = 4.02015\n",
      "epoch no.0 train no.18050  loss = 5.33620 avg_loss = 4.05289\n",
      "epoch no.0 train no.18060  loss = 4.12487 avg_loss = 4.05340\n",
      "epoch no.0 train no.18070  loss = 2.94576 avg_loss = 4.05928\n",
      "epoch no.0 train no.18080  loss = 2.51375 avg_loss = 4.02796\n",
      "epoch no.0 train no.18090  loss = 3.99215 avg_loss = 4.03959\n",
      "epoch no.0 train no.18100  loss = 3.93708 avg_loss = 3.99979\n",
      "epoch no.0 train no.18110  loss = 4.67257 avg_loss = 4.00719\n",
      "epoch no.0 train no.18120  loss = 3.19416 avg_loss = 4.04586\n",
      "epoch no.0 train no.18130  loss = 3.87498 avg_loss = 4.02057\n",
      "epoch no.0 train no.18140  loss = 3.84072 avg_loss = 4.04940\n",
      "epoch no.0 train no.18150  loss = 4.24949 avg_loss = 4.05611\n",
      "epoch no.0 train no.18160  loss = 6.27318 avg_loss = 4.04562\n",
      "epoch no.0 train no.18170  loss = 2.95542 avg_loss = 4.07197\n",
      "epoch no.0 train no.18180  loss = 4.22363 avg_loss = 4.05087\n",
      "epoch no.0 train no.18190  loss = 3.60037 avg_loss = 4.04376\n",
      "epoch no.0 train no.18200  loss = 4.93224 avg_loss = 4.03356\n",
      "epoch no.0 train no.18210  loss = 2.38502 avg_loss = 4.01231\n",
      "epoch no.0 train no.18220  loss = 4.61029 avg_loss = 3.95682\n",
      "epoch no.0 train no.18230  loss = 3.41602 avg_loss = 3.92662\n",
      "epoch no.0 train no.18240  loss = 3.49048 avg_loss = 3.88539\n",
      "epoch no.0 train no.18250  loss = 5.51387 avg_loss = 3.93852\n",
      "epoch no.0 train no.18260  loss = 2.08545 avg_loss = 4.02456\n",
      "epoch no.0 train no.18270  loss = 5.58164 avg_loss = 3.95165\n",
      "epoch no.0 train no.18280  loss = 4.77724 avg_loss = 3.97574\n",
      "epoch no.0 train no.18290  loss = 4.11781 avg_loss = 3.96155\n",
      "epoch no.0 train no.18300  loss = 6.04482 avg_loss = 3.97198\n",
      "epoch no.0 train no.18310  loss = 2.57786 avg_loss = 3.96182\n",
      "epoch no.0 train no.18320  loss = 3.00571 avg_loss = 4.02375\n",
      "epoch no.0 train no.18330  loss = 2.72553 avg_loss = 4.00625\n",
      "epoch no.0 train no.18340  loss = 4.75444 avg_loss = 3.99794\n",
      "epoch no.0 train no.18350  loss = 2.38293 avg_loss = 4.00301\n",
      "epoch no.0 train no.18360  loss = 7.51355 avg_loss = 3.94910\n",
      "epoch no.0 train no.18370  loss = 2.44274 avg_loss = 3.95209\n",
      "epoch no.0 train no.18380  loss = 4.07822 avg_loss = 3.94897\n",
      "epoch no.0 train no.18390  loss = 5.46760 avg_loss = 4.01352\n",
      "epoch no.0 train no.18400  loss = 3.87462 avg_loss = 3.97586\n",
      "epoch no.0 train no.18410  loss = 5.51404 avg_loss = 4.01263\n",
      "epoch no.0 train no.18420  loss = 4.38119 avg_loss = 4.02347\n",
      "epoch no.0 train no.18430  loss = 4.95442 avg_loss = 4.00076\n",
      "epoch no.0 train no.18440  loss = 4.52212 avg_loss = 4.03597\n",
      "epoch no.0 train no.18450  loss = 3.06788 avg_loss = 3.98012\n",
      "epoch no.0 train no.18460  loss = 5.90333 avg_loss = 4.00809\n",
      "epoch no.0 train no.18470  loss = 4.70595 avg_loss = 3.99739\n",
      "epoch no.0 train no.18480  loss = 4.41892 avg_loss = 3.96318\n",
      "epoch no.0 train no.18490  loss = 5.30801 avg_loss = 3.94414\n",
      "epoch no.0 train no.18500  loss = 5.04958 avg_loss = 3.94073\n",
      "epoch no.0 train no.18510  loss = 4.32736 avg_loss = 3.97037\n",
      "epoch no.0 train no.18520  loss = 2.73384 avg_loss = 3.93430\n",
      "epoch no.0 train no.18530  loss = 2.29384 avg_loss = 3.86758\n",
      "epoch no.0 train no.18540  loss = 4.12407 avg_loss = 3.88843\n",
      "epoch no.0 train no.18550  loss = 3.17552 avg_loss = 3.89380\n",
      "epoch no.0 train no.18560  loss = 5.34824 avg_loss = 3.92564\n",
      "epoch no.0 train no.18570  loss = 4.07369 avg_loss = 3.98799\n",
      "epoch no.0 train no.18580  loss = 4.11565 avg_loss = 3.97890\n",
      "epoch no.0 train no.18590  loss = 5.87847 avg_loss = 4.01558\n",
      "epoch no.0 train no.18600  loss = 5.41492 avg_loss = 4.07018\n",
      "epoch no.0 train no.18610  loss = 3.40126 avg_loss = 4.03836\n",
      "epoch no.0 train no.18620  loss = 4.63698 avg_loss = 4.06266\n",
      "epoch no.0 train no.18630  loss = 2.96845 avg_loss = 4.03856\n",
      "epoch no.0 train no.18640  loss = 4.77795 avg_loss = 4.01783\n",
      "epoch no.0 train no.18650  loss = 6.23591 avg_loss = 4.11481\n",
      "epoch no.0 train no.18660  loss = 2.55845 avg_loss = 4.03900\n",
      "epoch no.0 train no.18670  loss = 7.38897 avg_loss = 4.09115\n",
      "epoch no.0 train no.18680  loss = 4.37314 avg_loss = 4.01257\n",
      "epoch no.0 train no.18690  loss = 3.88622 avg_loss = 4.03981\n",
      "epoch no.0 train no.18700  loss = 2.79269 avg_loss = 3.96425\n",
      "epoch no.0 train no.18710  loss = 3.70690 avg_loss = 3.95408\n",
      "epoch no.0 train no.18720  loss = 4.34644 avg_loss = 3.96557\n",
      "epoch no.0 train no.18730  loss = 5.13206 avg_loss = 3.97859\n",
      "epoch no.0 train no.18740  loss = 3.06908 avg_loss = 3.93371\n",
      "epoch no.0 train no.18750  loss = 3.48055 avg_loss = 3.93106\n",
      "epoch no.0 train no.18760  loss = 6.62026 avg_loss = 3.95791\n",
      "epoch no.0 train no.18770  loss = 3.02741 avg_loss = 4.01415\n",
      "epoch no.0 train no.18780  loss = 5.98529 avg_loss = 4.10468\n",
      "epoch no.0 train no.18790  loss = 4.45055 avg_loss = 4.14075\n",
      "epoch no.0 train no.18800  loss = 5.75091 avg_loss = 4.11975\n",
      "epoch no.0 train no.18810  loss = 6.86463 avg_loss = 4.10128\n",
      "epoch no.0 train no.18820  loss = 4.36631 avg_loss = 4.12890\n",
      "epoch no.0 train no.18830  loss = 4.31229 avg_loss = 4.09671\n",
      "epoch no.0 train no.18840  loss = 3.61124 avg_loss = 4.10001\n",
      "epoch no.0 train no.18850  loss = 4.41657 avg_loss = 4.09336\n",
      "epoch no.0 train no.18860  loss = 3.17136 avg_loss = 4.11210\n",
      "epoch no.0 train no.18870  loss = 4.47198 avg_loss = 4.13635\n",
      "epoch no.0 train no.18880  loss = 5.07533 avg_loss = 4.11277\n",
      "epoch no.0 train no.18890  loss = 2.05064 avg_loss = 4.05752\n",
      "epoch no.0 train no.18900  loss = 4.44069 avg_loss = 4.06477\n",
      "epoch no.0 train no.18910  loss = 4.39548 avg_loss = 4.04887\n",
      "epoch no.0 train no.18920  loss = 4.70708 avg_loss = 4.08766\n",
      "epoch no.0 train no.18930  loss = 5.43180 avg_loss = 4.08360\n",
      "epoch no.0 train no.18940  loss = 5.25411 avg_loss = 4.11551\n",
      "epoch no.0 train no.18950  loss = 3.00608 avg_loss = 4.06595\n",
      "epoch no.0 train no.18960  loss = 5.74295 avg_loss = 4.04992\n",
      "epoch no.0 train no.18970  loss = 5.50586 avg_loss = 4.03282\n",
      "epoch no.0 train no.18980  loss = 5.17771 avg_loss = 4.00821\n",
      "epoch no.0 train no.18990  loss = 6.69165 avg_loss = 4.05620\n",
      "epoch no.0 train no.19000  loss = 3.94162 avg_loss = 4.05135\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '▁시원한', '히', '섭', '하고', '▁여름', '</s>']\n",
      "여름은 시원섭섭한 음악</s>\n",
      "epoch no.0 train no.19010  loss = 2.24400 avg_loss = 4.01854\n",
      "epoch no.0 train no.19020  loss = 3.10123 avg_loss = 4.02340\n",
      "epoch no.0 train no.19030  loss = 2.43618 avg_loss = 3.99149\n",
      "epoch no.0 train no.19040  loss = 3.57014 avg_loss = 3.95430\n",
      "epoch no.0 train no.19050  loss = 4.52282 avg_loss = 3.92310\n",
      "epoch no.0 train no.19060  loss = 2.14587 avg_loss = 3.91275\n",
      "epoch no.0 train no.19070  loss = 3.10551 avg_loss = 3.89098\n",
      "epoch no.0 train no.19080  loss = 3.41596 avg_loss = 3.89197\n",
      "epoch no.0 train no.19090  loss = 5.00535 avg_loss = 3.91967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.19100  loss = 3.61595 avg_loss = 3.91499\n",
      "epoch no.0 train no.19110  loss = 3.37141 avg_loss = 3.90491\n",
      "epoch no.0 train no.19120  loss = 2.37440 avg_loss = 3.83933\n",
      "epoch no.0 train no.19130  loss = 4.61007 avg_loss = 3.88536\n",
      "epoch no.0 train no.19140  loss = 4.84476 avg_loss = 3.96809\n",
      "epoch no.0 train no.19150  loss = 3.62401 avg_loss = 3.93838\n",
      "epoch no.0 train no.19160  loss = 5.21989 avg_loss = 3.99479\n",
      "epoch no.0 train no.19170  loss = 4.20525 avg_loss = 3.97800\n",
      "epoch no.0 train no.19180  loss = 2.66072 avg_loss = 3.99752\n",
      "epoch no.0 train no.19190  loss = 2.57980 avg_loss = 3.96627\n",
      "epoch no.0 train no.19200  loss = 4.18187 avg_loss = 3.96874\n",
      "epoch no.0 train no.19210  loss = 2.50652 avg_loss = 3.98377\n",
      "epoch no.0 train no.19220  loss = 4.73937 avg_loss = 3.96978\n",
      "epoch no.0 train no.19230  loss = 2.01265 avg_loss = 3.98063\n",
      "epoch no.0 train no.19240  loss = 6.47472 avg_loss = 4.06873\n",
      "epoch no.0 train no.19250  loss = 5.94849 avg_loss = 4.06092\n",
      "epoch no.0 train no.19260  loss = 2.63251 avg_loss = 4.06712\n",
      "epoch no.0 train no.19270  loss = 3.41429 avg_loss = 4.07191\n",
      "epoch no.0 train no.19280  loss = 3.45289 avg_loss = 4.06887\n",
      "epoch no.0 train no.19290  loss = 3.67443 avg_loss = 4.08376\n",
      "epoch no.0 train no.19300  loss = 3.86796 avg_loss = 4.14672\n",
      "epoch no.0 train no.19310  loss = 4.70145 avg_loss = 4.15358\n",
      "epoch no.0 train no.19320  loss = 3.71148 avg_loss = 4.17558\n",
      "epoch no.0 train no.19330  loss = 4.12336 avg_loss = 4.18230\n",
      "epoch no.0 train no.19340  loss = 4.43035 avg_loss = 4.15186\n",
      "epoch no.0 train no.19350  loss = 3.83811 avg_loss = 4.11493\n",
      "epoch no.0 train no.19360  loss = 3.20718 avg_loss = 4.14670\n",
      "epoch no.0 train no.19370  loss = 3.11131 avg_loss = 4.12626\n",
      "epoch no.0 train no.19380  loss = 4.31659 avg_loss = 4.05272\n",
      "epoch no.0 train no.19390  loss = 3.39899 avg_loss = 4.05953\n",
      "epoch no.0 train no.19400  loss = 5.19728 avg_loss = 4.05392\n",
      "epoch no.0 train no.19410  loss = 2.06633 avg_loss = 4.07192\n",
      "epoch no.0 train no.19420  loss = 4.98977 avg_loss = 4.07426\n",
      "epoch no.0 train no.19430  loss = 3.72579 avg_loss = 4.07941\n",
      "epoch no.0 train no.19440  loss = 4.37632 avg_loss = 4.11439\n",
      "epoch no.0 train no.19450  loss = 4.63072 avg_loss = 4.02235\n",
      "epoch no.0 train no.19460  loss = 4.33929 avg_loss = 4.07093\n",
      "epoch no.0 train no.19470  loss = 4.80519 avg_loss = 4.08141\n",
      "epoch no.0 train no.19480  loss = 4.35467 avg_loss = 4.03037\n",
      "epoch no.0 train no.19490  loss = 1.69371 avg_loss = 3.98167\n",
      "epoch no.0 train no.19500  loss = 4.60869 avg_loss = 3.99086\n",
      "epoch no.0 train no.19510  loss = 5.85346 avg_loss = 3.99496\n",
      "epoch no.0 train no.19520  loss = 2.46469 avg_loss = 3.99648\n",
      "epoch no.0 train no.19530  loss = 4.60167 avg_loss = 4.01879\n",
      "epoch no.0 train no.19540  loss = 3.38629 avg_loss = 4.00784\n",
      "epoch no.0 train no.19550  loss = 4.52942 avg_loss = 4.08174\n",
      "epoch no.0 train no.19560  loss = 3.20016 avg_loss = 4.05970\n",
      "epoch no.0 train no.19570  loss = 4.94757 avg_loss = 4.10508\n",
      "epoch no.0 train no.19580  loss = 4.44264 avg_loss = 4.05293\n",
      "epoch no.0 train no.19590  loss = 3.22931 avg_loss = 4.06717\n",
      "epoch no.0 train no.19600  loss = 4.97976 avg_loss = 4.06886\n",
      "epoch no.0 train no.19610  loss = 5.44594 avg_loss = 4.09443\n",
      "epoch no.0 train no.19620  loss = 4.64505 avg_loss = 4.09962\n",
      "epoch no.0 train no.19630  loss = 3.66140 avg_loss = 4.06698\n",
      "epoch no.0 train no.19640  loss = 3.27029 avg_loss = 4.05646\n",
      "epoch no.0 train no.19650  loss = 5.11366 avg_loss = 4.08190\n",
      "epoch no.0 train no.19660  loss = 2.84368 avg_loss = 4.16372\n",
      "epoch no.0 train no.19670  loss = 2.67165 avg_loss = 4.19251\n",
      "epoch no.0 train no.19680  loss = 4.12600 avg_loss = 4.15781\n",
      "epoch no.0 train no.19690  loss = 5.56369 avg_loss = 4.17005\n",
      "epoch no.0 train no.19700  loss = 5.03432 avg_loss = 4.18344\n",
      "epoch no.0 train no.19710  loss = 3.62312 avg_loss = 4.19359\n",
      "epoch no.0 train no.19720  loss = 4.54761 avg_loss = 4.22123\n",
      "epoch no.0 train no.19730  loss = 5.49000 avg_loss = 4.25247\n",
      "epoch no.0 train no.19740  loss = 3.40541 avg_loss = 4.22841\n",
      "epoch no.0 train no.19750  loss = 5.62680 avg_loss = 4.21296\n",
      "epoch no.0 train no.19760  loss = 3.31431 avg_loss = 4.18873\n",
      "epoch no.0 train no.19770  loss = 5.44774 avg_loss = 4.12968\n",
      "epoch no.0 train no.19780  loss = 3.60545 avg_loss = 4.09276\n",
      "epoch no.0 train no.19790  loss = 4.56313 avg_loss = 4.17247\n",
      "epoch no.0 train no.19800  loss = 4.86378 avg_loss = 4.17521\n",
      "epoch no.0 train no.19810  loss = 5.59872 avg_loss = 4.14439\n",
      "epoch no.0 train no.19820  loss = 5.19029 avg_loss = 4.16864\n",
      "epoch no.0 train no.19830  loss = 5.54180 avg_loss = 4.20460\n",
      "epoch no.0 train no.19840  loss = 3.81519 avg_loss = 4.20439\n",
      "epoch no.0 train no.19850  loss = 6.18083 avg_loss = 4.20064\n",
      "epoch no.0 train no.19860  loss = 5.03844 avg_loss = 4.20697\n",
      "epoch no.0 train no.19870  loss = 4.53429 avg_loss = 4.28014\n",
      "epoch no.0 train no.19880  loss = 3.17869 avg_loss = 4.28724\n",
      "epoch no.0 train no.19890  loss = 5.96725 avg_loss = 4.26581\n",
      "epoch no.0 train no.19900  loss = 3.95171 avg_loss = 4.22085\n",
      "epoch no.0 train no.19910  loss = 4.68331 avg_loss = 4.21059\n",
      "epoch no.0 train no.19920  loss = 3.95299 avg_loss = 4.17225\n",
      "epoch no.0 train no.19930  loss = 3.70739 avg_loss = 4.15720\n",
      "epoch no.0 train no.19940  loss = 3.16826 avg_loss = 4.12548\n",
      "epoch no.0 train no.19950  loss = 4.71456 avg_loss = 4.19641\n",
      "epoch no.0 train no.19960  loss = 4.29976 avg_loss = 4.13859\n",
      "epoch no.0 train no.19970  loss = 3.64878 avg_loss = 4.11650\n",
      "epoch no.0 train no.19980  loss = 4.40267 avg_loss = 4.18696\n",
      "epoch no.0 train no.19990  loss = 2.93411 avg_loss = 4.15916\n",
      "epoch no.0 train no.20000  loss = 3.39317 avg_loss = 4.20038\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁시원하게', '▁해줄', '▁감성', '▁노래', '</s>']\n",
      "여름밤을 시원하게 해줄 신나는 노래</s>\n",
      "epoch no.0 train no.20010  loss = 3.52126 avg_loss = 4.16589\n",
      "epoch no.0 train no.20020  loss = 4.26493 avg_loss = 4.08929\n",
      "epoch no.0 train no.20030  loss = 2.98131 avg_loss = 4.08784\n",
      "epoch no.0 train no.20040  loss = 4.24641 avg_loss = 4.12048\n",
      "epoch no.0 train no.20050  loss = 5.40892 avg_loss = 4.09057\n",
      "epoch no.0 train no.20060  loss = 6.56566 avg_loss = 4.16290\n",
      "epoch no.0 train no.20070  loss = 4.61785 avg_loss = 4.09703\n",
      "epoch no.0 train no.20080  loss = 2.86965 avg_loss = 4.13720\n",
      "epoch no.0 train no.20090  loss = 4.82155 avg_loss = 4.10282\n",
      "epoch no.0 train no.20100  loss = 4.51490 avg_loss = 4.10021\n",
      "epoch no.0 train no.20110  loss = 3.50105 avg_loss = 4.01322\n",
      "epoch no.0 train no.20120  loss = 3.29363 avg_loss = 4.01438\n",
      "epoch no.0 train no.20130  loss = 3.99587 avg_loss = 3.98652\n",
      "epoch no.0 train no.20140  loss = 4.51991 avg_loss = 4.03358\n",
      "epoch no.0 train no.20150  loss = 3.21991 avg_loss = 4.06607\n",
      "epoch no.0 train no.20160  loss = 8.24333 avg_loss = 4.07848\n",
      "epoch no.0 train no.20170  loss = 3.79654 avg_loss = 4.09762\n",
      "epoch no.0 train no.20180  loss = 2.70684 avg_loss = 4.03069\n",
      "epoch no.0 train no.20190  loss = 3.53081 avg_loss = 3.98587\n",
      "epoch no.0 train no.20200  loss = 4.02105 avg_loss = 3.95763\n",
      "epoch no.0 train no.20210  loss = 1.67567 avg_loss = 3.99185\n",
      "epoch no.0 train no.20220  loss = 4.28108 avg_loss = 3.99005\n",
      "epoch no.0 train no.20230  loss = 6.21464 avg_loss = 4.02598\n",
      "epoch no.0 train no.20240  loss = 4.93896 avg_loss = 4.07506\n",
      "epoch no.0 train no.20250  loss = 3.78637 avg_loss = 4.05701\n",
      "epoch no.0 train no.20260  loss = 4.09348 avg_loss = 4.03393\n",
      "epoch no.0 train no.20270  loss = 4.45033 avg_loss = 4.06335\n",
      "epoch no.0 train no.20280  loss = 1.94744 avg_loss = 4.00974\n",
      "epoch no.0 train no.20290  loss = 2.83293 avg_loss = 3.96507\n",
      "epoch no.0 train no.20300  loss = 4.47871 avg_loss = 3.97953\n",
      "epoch no.0 train no.20310  loss = 3.88414 avg_loss = 4.05457\n",
      "epoch no.0 train no.20320  loss = 5.86232 avg_loss = 4.03630\n",
      "epoch no.0 train no.20330  loss = 2.94261 avg_loss = 4.03258\n",
      "epoch no.0 train no.20340  loss = 3.95469 avg_loss = 4.03254\n",
      "epoch no.0 train no.20350  loss = 2.94364 avg_loss = 4.02964\n",
      "epoch no.0 train no.20360  loss = 3.15697 avg_loss = 4.05389\n",
      "epoch no.0 train no.20370  loss = 3.54796 avg_loss = 4.04282\n",
      "epoch no.0 train no.20380  loss = 3.02535 avg_loss = 4.06911\n",
      "epoch no.0 train no.20390  loss = 4.52440 avg_loss = 4.08654\n",
      "epoch no.0 train no.20400  loss = 3.40277 avg_loss = 4.05994\n",
      "epoch no.0 train no.20410  loss = 3.30547 avg_loss = 4.02150\n",
      "epoch no.0 train no.20420  loss = 2.97464 avg_loss = 3.97160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.20430  loss = 4.43258 avg_loss = 4.00717\n",
      "epoch no.0 train no.20440  loss = 5.70202 avg_loss = 4.01994\n",
      "epoch no.0 train no.20450  loss = 3.93902 avg_loss = 4.02702\n",
      "epoch no.0 train no.20460  loss = 3.69250 avg_loss = 4.01440\n",
      "epoch no.0 train no.20470  loss = 3.04395 avg_loss = 4.02725\n",
      "epoch no.0 train no.20480  loss = 2.73040 avg_loss = 4.01792\n",
      "epoch no.0 train no.20490  loss = 3.31590 avg_loss = 4.06843\n",
      "epoch no.0 train no.20500  loss = 4.36947 avg_loss = 4.04520\n",
      "epoch no.0 train no.20510  loss = 3.01812 avg_loss = 4.04805\n",
      "epoch no.0 train no.20520  loss = 3.85960 avg_loss = 4.02837\n",
      "epoch no.0 train no.20530  loss = 4.75526 avg_loss = 3.98533\n",
      "epoch no.0 train no.20540  loss = 5.20939 avg_loss = 3.98166\n",
      "epoch no.0 train no.20550  loss = 3.40468 avg_loss = 4.00391\n",
      "epoch no.0 train no.20560  loss = 4.35415 avg_loss = 4.02182\n",
      "epoch no.0 train no.20570  loss = 3.69055 avg_loss = 4.06988\n",
      "epoch no.0 train no.20580  loss = 2.53354 avg_loss = 4.08688\n",
      "epoch no.0 train no.20590  loss = 5.53766 avg_loss = 4.13923\n",
      "epoch no.0 train no.20600  loss = 2.27865 avg_loss = 4.14112\n",
      "epoch no.0 train no.20610  loss = 2.84764 avg_loss = 4.11648\n",
      "epoch no.0 train no.20620  loss = 3.25379 avg_loss = 4.06226\n",
      "epoch no.0 train no.20630  loss = 3.58848 avg_loss = 3.99606\n",
      "epoch no.0 train no.20640  loss = 3.30160 avg_loss = 4.04422\n",
      "epoch no.0 train no.20650  loss = 5.16721 avg_loss = 4.04805\n",
      "epoch no.0 train no.20660  loss = 4.49638 avg_loss = 3.99760\n",
      "epoch no.0 train no.20670  loss = 3.02441 avg_loss = 3.98152\n",
      "epoch no.0 train no.20680  loss = 2.98906 avg_loss = 4.03629\n",
      "epoch no.0 train no.20690  loss = 3.07862 avg_loss = 4.02926\n",
      "epoch no.0 train no.20700  loss = 8.03776 avg_loss = 4.10985\n",
      "epoch no.0 train no.20710  loss = 4.96332 avg_loss = 4.12414\n",
      "epoch no.0 train no.20720  loss = 4.26680 avg_loss = 4.18782\n",
      "epoch no.0 train no.20730  loss = 3.75762 avg_loss = 4.18590\n",
      "epoch no.0 train no.20740  loss = 5.12946 avg_loss = 4.12100\n",
      "epoch no.0 train no.20750  loss = 7.31993 avg_loss = 4.11614\n",
      "epoch no.0 train no.20760  loss = 2.89054 avg_loss = 4.08173\n",
      "epoch no.0 train no.20770  loss = 2.60928 avg_loss = 4.08144\n",
      "epoch no.0 train no.20780  loss = 5.63901 avg_loss = 4.15136\n",
      "epoch no.0 train no.20790  loss = 3.14875 avg_loss = 4.24158\n",
      "epoch no.0 train no.20800  loss = 4.20028 avg_loss = 4.24960\n",
      "epoch no.0 train no.20810  loss = 4.51492 avg_loss = 4.23781\n",
      "epoch no.0 train no.20820  loss = 4.33934 avg_loss = 4.23355\n",
      "epoch no.0 train no.20830  loss = 3.95104 avg_loss = 4.28222\n",
      "epoch no.0 train no.20840  loss = 4.59255 avg_loss = 4.25081\n",
      "epoch no.0 train no.20850  loss = 4.05484 avg_loss = 4.25945\n",
      "epoch no.0 train no.20860  loss = 5.56087 avg_loss = 4.21270\n",
      "epoch no.0 train no.20870  loss = 4.91634 avg_loss = 4.21306\n",
      "epoch no.0 train no.20880  loss = 5.55585 avg_loss = 4.19749\n",
      "epoch no.0 train no.20890  loss = 3.74260 avg_loss = 4.21470\n",
      "epoch no.0 train no.20900  loss = 3.84515 avg_loss = 4.15868\n",
      "epoch no.0 train no.20910  loss = 5.78893 avg_loss = 4.14723\n",
      "epoch no.0 train no.20920  loss = 5.43432 avg_loss = 4.12193\n",
      "epoch no.0 train no.20930  loss = 6.08291 avg_loss = 4.14059\n",
      "epoch no.0 train no.20940  loss = 6.11686 avg_loss = 4.10575\n",
      "epoch no.0 train no.20950  loss = 6.31477 avg_loss = 4.08912\n",
      "epoch no.0 train no.20960  loss = 3.72147 avg_loss = 4.08075\n",
      "epoch no.0 train no.20970  loss = 3.54832 avg_loss = 4.10669\n",
      "epoch no.0 train no.20980  loss = 4.10693 avg_loss = 4.05584\n",
      "epoch no.0 train no.20990  loss = 5.79423 avg_loss = 4.08271\n",
      "epoch no.0 train no.21000  loss = 4.92767 avg_loss = 4.12943\n",
      "3\n",
      "to_tokens: ['▁비', '밤', '▁시원한', '▁청량', '</s>']\n",
      "여름엔 시원한 재즈</s>\n",
      "epoch no.0 train no.21010  loss = 3.19056 avg_loss = 4.13197\n",
      "epoch no.0 train no.21020  loss = 4.69465 avg_loss = 4.11946\n",
      "epoch no.0 train no.21030  loss = 3.28862 avg_loss = 4.02778\n",
      "epoch no.0 train no.21040  loss = 3.90325 avg_loss = 3.99526\n",
      "epoch no.0 train no.21050  loss = 3.77062 avg_loss = 4.04465\n",
      "epoch no.0 train no.21060  loss = 4.00026 avg_loss = 4.04262\n",
      "epoch no.0 train no.21070  loss = 3.30384 avg_loss = 4.06672\n",
      "epoch no.0 train no.21080  loss = 2.75585 avg_loss = 3.96395\n",
      "epoch no.0 train no.21090  loss = 2.80950 avg_loss = 3.94072\n",
      "epoch no.0 train no.21100  loss = 3.91007 avg_loss = 3.99315\n",
      "epoch no.0 train no.21110  loss = 2.92204 avg_loss = 3.99190\n",
      "epoch no.0 train no.21120  loss = 4.92635 avg_loss = 4.02083\n",
      "epoch no.0 train no.21130  loss = 2.57597 avg_loss = 3.99325\n",
      "epoch no.0 train no.21140  loss = 2.97979 avg_loss = 3.99431\n",
      "epoch no.0 train no.21150  loss = 2.61464 avg_loss = 4.00798\n",
      "epoch no.0 train no.21160  loss = 5.70239 avg_loss = 3.96787\n",
      "epoch no.0 train no.21170  loss = 4.37360 avg_loss = 3.92212\n",
      "epoch no.0 train no.21180  loss = 3.69365 avg_loss = 3.93456\n",
      "epoch no.0 train no.21190  loss = 4.14957 avg_loss = 3.89636\n",
      "epoch no.0 train no.21200  loss = 2.13399 avg_loss = 3.91074\n",
      "epoch no.0 train no.21210  loss = 8.18778 avg_loss = 3.94502\n",
      "epoch no.0 train no.21220  loss = 3.03194 avg_loss = 3.95222\n",
      "epoch no.0 train no.21230  loss = 3.20292 avg_loss = 3.98318\n",
      "epoch no.0 train no.21240  loss = 5.11828 avg_loss = 3.97787\n",
      "epoch no.0 train no.21250  loss = 3.69498 avg_loss = 3.94889\n",
      "epoch no.0 train no.21260  loss = 2.54424 avg_loss = 3.92804\n",
      "epoch no.0 train no.21270  loss = 5.36287 avg_loss = 3.94793\n",
      "epoch no.0 train no.21280  loss = 5.50767 avg_loss = 3.95969\n",
      "epoch no.0 train no.21290  loss = 6.77822 avg_loss = 4.03156\n",
      "epoch no.0 train no.21300  loss = 4.52858 avg_loss = 4.02838\n",
      "epoch no.0 train no.21310  loss = 4.38082 avg_loss = 4.00947\n",
      "epoch no.0 train no.21320  loss = 4.78752 avg_loss = 3.99603\n",
      "epoch no.0 train no.21330  loss = 3.55321 avg_loss = 4.00866\n",
      "epoch no.0 train no.21340  loss = 3.28972 avg_loss = 3.99907\n",
      "epoch no.0 train no.21350  loss = 6.13776 avg_loss = 4.00539\n",
      "epoch no.0 train no.21360  loss = 2.85618 avg_loss = 3.99324\n",
      "epoch no.0 train no.21370  loss = 4.12065 avg_loss = 3.94825\n",
      "epoch no.0 train no.21380  loss = 3.94745 avg_loss = 3.98653\n",
      "epoch no.0 train no.21390  loss = 6.58418 avg_loss = 4.00250\n",
      "epoch no.0 train no.21400  loss = 3.82564 avg_loss = 4.04056\n",
      "epoch no.0 train no.21410  loss = 3.88994 avg_loss = 4.02889\n",
      "epoch no.0 train no.21420  loss = 7.55400 avg_loss = 4.05853\n",
      "epoch no.0 train no.21430  loss = 4.18677 avg_loss = 4.03594\n",
      "epoch no.0 train no.21440  loss = 4.27881 avg_loss = 4.01479\n",
      "epoch no.0 train no.21450  loss = 4.27785 avg_loss = 4.02136\n",
      "epoch no.0 train no.21460  loss = 2.62718 avg_loss = 3.99251\n",
      "epoch no.0 train no.21470  loss = 3.05748 avg_loss = 4.02995\n",
      "epoch no.0 train no.21480  loss = 3.59217 avg_loss = 4.02458\n",
      "epoch no.0 train no.21490  loss = 6.84811 avg_loss = 4.06522\n",
      "epoch no.0 train no.21500  loss = 6.67724 avg_loss = 4.08653\n",
      "epoch no.0 train no.21510  loss = 4.53413 avg_loss = 4.01675\n",
      "epoch no.0 train no.21520  loss = 3.31245 avg_loss = 4.03163\n",
      "epoch no.0 train no.21530  loss = 2.54701 avg_loss = 4.05588\n",
      "epoch no.0 train no.21540  loss = 4.05517 avg_loss = 4.03187\n",
      "epoch no.0 train no.21550  loss = 3.51212 avg_loss = 3.99328\n",
      "epoch no.0 train no.21560  loss = 3.44426 avg_loss = 3.97469\n",
      "epoch no.0 train no.21570  loss = 5.07960 avg_loss = 3.97193\n",
      "epoch no.0 train no.21580  loss = 4.74602 avg_loss = 3.96906\n",
      "epoch no.0 train no.21590  loss = 5.67361 avg_loss = 3.94237\n",
      "epoch no.0 train no.21600  loss = 2.81892 avg_loss = 3.94071\n",
      "epoch no.0 train no.21610  loss = 3.65057 avg_loss = 3.94096\n",
      "epoch no.0 train no.21620  loss = 3.36084 avg_loss = 3.94362\n",
      "epoch no.0 train no.21630  loss = 3.53147 avg_loss = 3.97522\n",
      "epoch no.0 train no.21640  loss = 3.65827 avg_loss = 3.92764\n",
      "epoch no.0 train no.21650  loss = 4.32506 avg_loss = 3.95026\n",
      "epoch no.0 train no.21660  loss = 3.87890 avg_loss = 3.99500\n",
      "epoch no.0 train no.21670  loss = 4.32052 avg_loss = 3.98409\n",
      "epoch no.0 train no.21680  loss = 3.13613 avg_loss = 3.96868\n",
      "epoch no.0 train no.21690  loss = 4.52492 avg_loss = 3.99260\n",
      "epoch no.0 train no.21700  loss = 6.73834 avg_loss = 3.99311\n",
      "epoch no.0 train no.21710  loss = 2.62706 avg_loss = 4.05004\n",
      "epoch no.0 train no.21720  loss = 4.17182 avg_loss = 4.02405\n",
      "epoch no.0 train no.21730  loss = 3.02485 avg_loss = 4.08518\n",
      "epoch no.0 train no.21740  loss = 5.40896 avg_loss = 4.02247\n",
      "epoch no.0 train no.21750  loss = 2.11901 avg_loss = 4.08923\n",
      "epoch no.0 train no.21760  loss = 3.51258 avg_loss = 4.03179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.21770  loss = 3.63895 avg_loss = 3.97230\n",
      "epoch no.0 train no.21780  loss = 2.17282 avg_loss = 3.90852\n",
      "epoch no.0 train no.21790  loss = 4.98926 avg_loss = 3.98558\n",
      "epoch no.0 train no.21800  loss = 1.64107 avg_loss = 3.97028\n",
      "epoch no.0 train no.21810  loss = 3.75725 avg_loss = 3.95548\n",
      "epoch no.0 train no.21820  loss = 2.84467 avg_loss = 3.95698\n",
      "epoch no.0 train no.21830  loss = 2.64645 avg_loss = 3.97392\n",
      "epoch no.0 train no.21840  loss = 3.86281 avg_loss = 4.00027\n",
      "epoch no.0 train no.21850  loss = 6.80488 avg_loss = 3.97391\n",
      "epoch no.0 train no.21860  loss = 4.83711 avg_loss = 3.94925\n",
      "epoch no.0 train no.21870  loss = 3.10794 avg_loss = 3.94493\n",
      "epoch no.0 train no.21880  loss = 2.70838 avg_loss = 3.97011\n",
      "epoch no.0 train no.21890  loss = 6.20770 avg_loss = 3.98584\n",
      "epoch no.0 train no.21900  loss = 5.10785 avg_loss = 3.97359\n",
      "epoch no.0 train no.21910  loss = 4.28357 avg_loss = 3.98163\n",
      "epoch no.0 train no.21920  loss = 6.00344 avg_loss = 3.96469\n",
      "epoch no.0 train no.21930  loss = 4.08630 avg_loss = 3.98634\n",
      "epoch no.0 train no.21940  loss = 5.93572 avg_loss = 3.96464\n",
      "epoch no.0 train no.21950  loss = 7.31369 avg_loss = 4.02898\n",
      "epoch no.0 train no.21960  loss = 2.83345 avg_loss = 4.02765\n",
      "epoch no.0 train no.21970  loss = 6.09477 avg_loss = 4.01801\n",
      "epoch no.0 train no.21980  loss = 3.82882 avg_loss = 4.01428\n",
      "epoch no.0 train no.21990  loss = 5.90966 avg_loss = 4.00708\n",
      "epoch no.0 train no.22000  loss = 3.83788 avg_loss = 4.05593\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '</s>']\n",
      "여름밤에 어울리는 재즈</s>\n",
      "epoch no.0 train no.22010  loss = 4.08175 avg_loss = 4.12192\n",
      "epoch no.0 train no.22020  loss = 3.86360 avg_loss = 4.09835\n",
      "epoch no.0 train no.22030  loss = 4.74080 avg_loss = 4.13829\n",
      "epoch no.0 train no.22040  loss = 5.14241 avg_loss = 4.15390\n",
      "epoch no.0 train no.22050  loss = 2.48264 avg_loss = 4.16220\n",
      "epoch no.0 train no.22060  loss = 3.45531 avg_loss = 4.20897\n",
      "epoch no.0 train no.22070  loss = 4.53919 avg_loss = 4.19073\n",
      "epoch no.0 train no.22080  loss = 3.65954 avg_loss = 4.16086\n",
      "epoch no.0 train no.22090  loss = 4.75187 avg_loss = 4.17658\n",
      "epoch no.0 train no.22100  loss = 2.73406 avg_loss = 4.15145\n",
      "epoch no.0 train no.22110  loss = 4.05037 avg_loss = 4.13031\n",
      "epoch no.0 train no.22120  loss = 4.09256 avg_loss = 4.14475\n",
      "epoch no.0 train no.22130  loss = 4.83143 avg_loss = 4.11739\n",
      "epoch no.0 train no.22140  loss = 2.66624 avg_loss = 4.12361\n",
      "epoch no.0 train no.22150  loss = 4.45203 avg_loss = 4.10008\n",
      "epoch no.0 train no.22160  loss = 3.82395 avg_loss = 4.11131\n",
      "epoch no.0 train no.22170  loss = 6.81503 avg_loss = 4.11886\n",
      "epoch no.0 train no.22180  loss = 1.73949 avg_loss = 4.10283\n",
      "epoch no.0 train no.22190  loss = 3.94286 avg_loss = 4.18557\n",
      "epoch no.0 train no.22200  loss = 4.95866 avg_loss = 4.22634\n",
      "epoch no.0 train no.22210  loss = 3.95518 avg_loss = 4.18177\n",
      "epoch no.0 train no.22220  loss = 3.60329 avg_loss = 4.17545\n",
      "epoch no.0 train no.22230  loss = 5.01260 avg_loss = 4.22185\n",
      "epoch no.0 train no.22240  loss = 4.20713 avg_loss = 4.18651\n",
      "epoch no.0 train no.22250  loss = 5.19150 avg_loss = 4.23161\n",
      "epoch no.0 train no.22260  loss = 6.45766 avg_loss = 4.16659\n",
      "epoch no.0 train no.22270  loss = 3.38736 avg_loss = 4.17550\n",
      "epoch no.0 train no.22280  loss = 4.61693 avg_loss = 4.15345\n",
      "epoch no.0 train no.22290  loss = 4.24598 avg_loss = 4.20575\n",
      "epoch no.0 train no.22300  loss = 5.00028 avg_loss = 4.18685\n",
      "epoch no.0 train no.22310  loss = 3.31031 avg_loss = 4.13273\n",
      "epoch no.0 train no.22320  loss = 4.45465 avg_loss = 4.13613\n",
      "epoch no.0 train no.22330  loss = 6.24464 avg_loss = 4.13562\n",
      "epoch no.0 train no.22340  loss = 1.79469 avg_loss = 4.10253\n",
      "epoch no.0 train no.22350  loss = 7.33858 avg_loss = 4.18130\n",
      "epoch no.0 train no.22360  loss = 6.40704 avg_loss = 4.22828\n",
      "epoch no.0 train no.22370  loss = 6.96271 avg_loss = 4.26245\n",
      "epoch no.0 train no.22380  loss = 5.47421 avg_loss = 4.25284\n",
      "epoch no.0 train no.22390  loss = 5.18820 avg_loss = 4.27840\n",
      "epoch no.0 train no.22400  loss = 2.69806 avg_loss = 4.23238\n",
      "epoch no.0 train no.22410  loss = 4.17062 avg_loss = 4.21138\n",
      "epoch no.0 train no.22420  loss = 2.30589 avg_loss = 4.21692\n",
      "epoch no.0 train no.22430  loss = 2.78543 avg_loss = 4.22181\n",
      "epoch no.0 train no.22440  loss = 3.20215 avg_loss = 4.16441\n",
      "epoch no.0 train no.22450  loss = 6.07961 avg_loss = 4.13259\n",
      "epoch no.0 train no.22460  loss = 3.36496 avg_loss = 4.13206\n",
      "epoch no.0 train no.22470  loss = 4.12676 avg_loss = 4.10982\n",
      "epoch no.0 train no.22480  loss = 3.47986 avg_loss = 4.14865\n",
      "epoch no.0 train no.22490  loss = 1.92502 avg_loss = 4.09457\n",
      "epoch no.0 train no.22500  loss = 3.96128 avg_loss = 4.07371\n",
      "epoch no.0 train no.22510  loss = 2.40969 avg_loss = 4.05860\n",
      "epoch no.0 train no.22520  loss = 4.31346 avg_loss = 4.08997\n",
      "epoch no.0 train no.22530  loss = 4.94858 avg_loss = 4.04856\n",
      "epoch no.0 train no.22540  loss = 5.49373 avg_loss = 4.05190\n",
      "epoch no.0 train no.22550  loss = 4.60189 avg_loss = 4.05268\n",
      "epoch no.0 train no.22560  loss = 2.96111 avg_loss = 4.02108\n",
      "epoch no.0 train no.22570  loss = 4.21685 avg_loss = 4.07407\n",
      "epoch no.0 train no.22580  loss = 5.12424 avg_loss = 4.08671\n",
      "epoch no.0 train no.22590  loss = 5.37119 avg_loss = 4.10734\n",
      "epoch no.0 train no.22600  loss = 5.78095 avg_loss = 4.13611\n",
      "epoch no.0 train no.22610  loss = 4.59507 avg_loss = 4.11427\n",
      "epoch no.0 train no.22620  loss = 2.91039 avg_loss = 4.08711\n",
      "epoch no.0 train no.22630  loss = 3.41060 avg_loss = 4.04119\n",
      "epoch no.0 train no.22640  loss = 2.67531 avg_loss = 4.03559\n",
      "epoch no.0 train no.22650  loss = 3.60741 avg_loss = 4.03934\n",
      "epoch no.0 train no.22660  loss = 5.66651 avg_loss = 4.00995\n",
      "epoch no.0 train no.22670  loss = 3.42833 avg_loss = 4.01415\n",
      "epoch no.0 train no.22680  loss = 4.66426 avg_loss = 4.01387\n",
      "epoch no.0 train no.22690  loss = 4.03368 avg_loss = 4.00711\n",
      "epoch no.0 train no.22700  loss = 4.40282 avg_loss = 4.01332\n",
      "epoch no.0 train no.22710  loss = 7.50242 avg_loss = 4.03203\n",
      "epoch no.0 train no.22720  loss = 6.52093 avg_loss = 4.12667\n",
      "epoch no.0 train no.22730  loss = 2.42177 avg_loss = 4.08793\n",
      "epoch no.0 train no.22740  loss = 2.15168 avg_loss = 4.05651\n",
      "epoch no.0 train no.22750  loss = 4.66292 avg_loss = 4.05055\n",
      "epoch no.0 train no.22760  loss = 2.92204 avg_loss = 4.01970\n",
      "epoch no.0 train no.22770  loss = 3.55555 avg_loss = 4.01267\n",
      "epoch no.0 train no.22780  loss = 3.65506 avg_loss = 4.01646\n",
      "epoch no.0 train no.22790  loss = 3.93659 avg_loss = 4.04037\n",
      "epoch no.0 train no.22800  loss = 3.12671 avg_loss = 4.03008\n",
      "epoch no.0 train no.22810  loss = 5.82972 avg_loss = 4.02817\n",
      "epoch no.0 train no.22820  loss = 4.68199 avg_loss = 4.03618\n",
      "epoch no.0 train no.22830  loss = 4.41856 avg_loss = 4.07478\n",
      "epoch no.0 train no.22840  loss = 5.42622 avg_loss = 4.05025\n",
      "epoch no.0 train no.22850  loss = 3.55896 avg_loss = 4.06177\n",
      "epoch no.0 train no.22860  loss = 5.35989 avg_loss = 4.05689\n",
      "epoch no.0 train no.22870  loss = 2.34252 avg_loss = 4.01155\n",
      "epoch no.0 train no.22880  loss = 3.34619 avg_loss = 4.05906\n",
      "epoch no.0 train no.22890  loss = 3.33773 avg_loss = 4.08678\n",
      "epoch no.0 train no.22900  loss = 4.35559 avg_loss = 4.06559\n",
      "epoch no.0 train no.22910  loss = 6.01649 avg_loss = 4.08414\n",
      "epoch no.0 train no.22920  loss = 4.69028 avg_loss = 4.12329\n",
      "epoch no.0 train no.22930  loss = 3.30383 avg_loss = 4.10152\n",
      "epoch no.0 train no.22940  loss = 5.17656 avg_loss = 4.10930\n",
      "epoch no.0 train no.22950  loss = 5.33119 avg_loss = 4.08384\n",
      "epoch no.0 train no.22960  loss = 5.25502 avg_loss = 4.10171\n",
      "epoch no.0 train no.22970  loss = 5.86986 avg_loss = 4.06350\n",
      "epoch no.0 train no.22980  loss = 3.45223 avg_loss = 4.03026\n",
      "epoch no.0 train no.22990  loss = 2.81685 avg_loss = 3.98604\n",
      "epoch no.0 train no.23000  loss = 3.60134 avg_loss = 4.01675\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁위한', '▁재즈', '들', '</s>']\n",
      "여름밤을 담은 음악들</s>\n",
      "epoch no.0 train no.23010  loss = 2.30882 avg_loss = 3.99378\n",
      "epoch no.0 train no.23020  loss = 3.50149 avg_loss = 3.97244\n",
      "epoch no.0 train no.23030  loss = 3.68038 avg_loss = 3.91760\n",
      "epoch no.0 train no.23040  loss = 3.72222 avg_loss = 3.93272\n",
      "epoch no.0 train no.23050  loss = 2.97068 avg_loss = 3.96362\n",
      "epoch no.0 train no.23060  loss = 3.68694 avg_loss = 3.95930\n",
      "epoch no.0 train no.23070  loss = 4.33581 avg_loss = 4.00821\n",
      "epoch no.0 train no.23080  loss = 2.31394 avg_loss = 3.95410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.23090  loss = 4.43873 avg_loss = 3.98905\n",
      "epoch no.0 train no.23100  loss = 4.38255 avg_loss = 3.95407\n",
      "epoch no.0 train no.23110  loss = 4.91212 avg_loss = 3.97879\n",
      "epoch no.0 train no.23120  loss = 4.52841 avg_loss = 3.99262\n",
      "epoch no.0 train no.23130  loss = 4.21213 avg_loss = 3.94986\n",
      "epoch no.0 train no.23140  loss = 5.82035 avg_loss = 4.00191\n",
      "epoch no.0 train no.23150  loss = 4.17987 avg_loss = 3.98836\n",
      "epoch no.0 train no.23160  loss = 6.83464 avg_loss = 4.04729\n",
      "epoch no.0 train no.23170  loss = 2.79195 avg_loss = 4.03104\n",
      "epoch no.0 train no.23180  loss = 3.82872 avg_loss = 4.07535\n",
      "epoch no.0 train no.23190  loss = 3.85120 avg_loss = 4.09645\n",
      "epoch no.0 train no.23200  loss = 6.27854 avg_loss = 4.07689\n",
      "epoch no.0 train no.23210  loss = 2.48969 avg_loss = 4.08435\n",
      "epoch no.0 train no.23220  loss = 5.80819 avg_loss = 4.18602\n",
      "epoch no.0 train no.23230  loss = 2.84154 avg_loss = 4.12759\n",
      "epoch no.0 train no.23240  loss = 6.33658 avg_loss = 4.14305\n",
      "epoch no.0 train no.23250  loss = 5.03973 avg_loss = 4.17832\n",
      "epoch no.0 train no.23260  loss = 3.29429 avg_loss = 4.16002\n",
      "epoch no.0 train no.23270  loss = 5.06049 avg_loss = 4.17645\n",
      "epoch no.0 train no.23280  loss = 6.30703 avg_loss = 4.18765\n",
      "epoch no.0 train no.23290  loss = 3.38517 avg_loss = 4.16740\n",
      "epoch no.0 train no.23300  loss = 5.49374 avg_loss = 4.13630\n",
      "epoch no.0 train no.23310  loss = 4.69051 avg_loss = 4.14936\n",
      "epoch no.0 train no.23320  loss = 6.43834 avg_loss = 4.14564\n",
      "epoch no.0 train no.23330  loss = 4.74619 avg_loss = 4.18261\n",
      "epoch no.0 train no.23340  loss = 2.64040 avg_loss = 4.18174\n",
      "epoch no.0 train no.23350  loss = 5.73337 avg_loss = 4.19534\n",
      "epoch no.0 train no.23360  loss = 3.60652 avg_loss = 4.16553\n",
      "epoch no.0 train no.23370  loss = 4.55563 avg_loss = 4.16168\n",
      "epoch no.0 train no.23380  loss = 4.43404 avg_loss = 4.13256\n",
      "epoch no.0 train no.23390  loss = 4.76498 avg_loss = 4.16718\n",
      "epoch no.0 train no.23400  loss = 5.27929 avg_loss = 4.11359\n",
      "epoch no.0 train no.23410  loss = 5.44853 avg_loss = 4.08023\n",
      "epoch no.0 train no.23420  loss = 4.24071 avg_loss = 4.09755\n",
      "epoch no.0 train no.23430  loss = 3.59983 avg_loss = 4.06964\n",
      "epoch no.0 train no.23440  loss = 2.61371 avg_loss = 4.03861\n",
      "epoch no.0 train no.23450  loss = 8.72985 avg_loss = 4.07868\n",
      "epoch no.0 train no.23460  loss = 2.61820 avg_loss = 4.09438\n",
      "epoch no.0 train no.23470  loss = 5.32530 avg_loss = 4.09811\n",
      "epoch no.0 train no.23480  loss = 4.67820 avg_loss = 4.10309\n",
      "epoch no.0 train no.23490  loss = 4.64311 avg_loss = 4.08427\n",
      "epoch no.0 train no.23500  loss = 3.99321 avg_loss = 4.04459\n",
      "epoch no.0 train no.23510  loss = 2.41210 avg_loss = 4.01093\n",
      "epoch no.0 train no.23520  loss = 4.25908 avg_loss = 4.00012\n",
      "epoch no.0 train no.23530  loss = 3.53350 avg_loss = 3.95937\n",
      "epoch no.0 train no.23540  loss = 4.03081 avg_loss = 3.97970\n",
      "epoch no.0 train no.23550  loss = 2.81965 avg_loss = 3.96059\n",
      "epoch no.0 train no.23560  loss = 2.56852 avg_loss = 3.94311\n",
      "epoch no.0 train no.23570  loss = 4.76853 avg_loss = 3.92467\n",
      "epoch no.0 train no.23580  loss = 4.06280 avg_loss = 3.85687\n",
      "epoch no.0 train no.23590  loss = 2.76751 avg_loss = 3.82664\n",
      "epoch no.0 train no.23600  loss = 4.45107 avg_loss = 3.90351\n",
      "epoch no.0 train no.23610  loss = 3.49712 avg_loss = 3.96662\n",
      "epoch no.0 train no.23620  loss = 3.76961 avg_loss = 3.93374\n",
      "epoch no.0 train no.23630  loss = 3.66627 avg_loss = 3.92526\n",
      "epoch no.0 train no.23640  loss = 5.89497 avg_loss = 3.92822\n",
      "epoch no.0 train no.23650  loss = 3.07172 avg_loss = 3.88543\n",
      "epoch no.0 train no.23660  loss = 4.24372 avg_loss = 3.87468\n",
      "epoch no.0 train no.23670  loss = 4.36849 avg_loss = 3.91975\n",
      "epoch no.0 train no.23680  loss = 4.49264 avg_loss = 3.96316\n",
      "epoch no.0 train no.23690  loss = 4.15274 avg_loss = 3.98077\n",
      "epoch no.0 train no.23700  loss = 6.48195 avg_loss = 3.99741\n",
      "epoch no.0 train no.23710  loss = 6.40264 avg_loss = 4.00150\n",
      "epoch no.0 train no.23720  loss = 4.36614 avg_loss = 3.97954\n",
      "epoch no.0 train no.23730  loss = 5.28304 avg_loss = 4.00043\n",
      "epoch no.0 train no.23740  loss = 3.42976 avg_loss = 3.98603\n",
      "epoch no.0 train no.23750  loss = 4.44621 avg_loss = 3.99789\n",
      "epoch no.0 train no.23760  loss = 1.82222 avg_loss = 3.94537\n",
      "epoch no.0 train no.23770  loss = 3.14672 avg_loss = 3.93578\n",
      "epoch no.0 train no.23780  loss = 4.14372 avg_loss = 3.96858\n",
      "epoch no.0 train no.23790  loss = 3.07789 avg_loss = 3.96585\n",
      "epoch no.0 train no.23800  loss = 4.68186 avg_loss = 3.95942\n",
      "epoch no.0 train no.23810  loss = 3.09016 avg_loss = 3.94557\n",
      "epoch no.0 train no.23820  loss = 4.51264 avg_loss = 3.95317\n",
      "epoch no.0 train no.23830  loss = 3.29137 avg_loss = 3.94027\n",
      "epoch no.0 train no.23840  loss = 3.21322 avg_loss = 3.89444\n",
      "epoch no.0 train no.23850  loss = 6.37702 avg_loss = 3.95042\n",
      "epoch no.0 train no.23860  loss = 3.89876 avg_loss = 3.92961\n",
      "epoch no.0 train no.23870  loss = 4.73156 avg_loss = 3.98953\n",
      "epoch no.0 train no.23880  loss = 3.63043 avg_loss = 4.02069\n",
      "epoch no.0 train no.23890  loss = 4.05321 avg_loss = 4.07374\n",
      "epoch no.0 train no.23900  loss = 4.68443 avg_loss = 4.03749\n",
      "epoch no.0 train no.23910  loss = 3.62994 avg_loss = 4.04925\n",
      "epoch no.0 train no.23920  loss = 2.74749 avg_loss = 3.98904\n",
      "epoch no.0 train no.23930  loss = 2.66261 avg_loss = 4.00032\n",
      "epoch no.0 train no.23940  loss = 4.26492 avg_loss = 3.96968\n",
      "epoch no.0 train no.23950  loss = 4.85247 avg_loss = 3.98827\n",
      "epoch no.0 train no.23960  loss = 4.40140 avg_loss = 4.01971\n",
      "epoch no.0 train no.23970  loss = 2.60838 avg_loss = 3.99427\n",
      "epoch no.0 train no.23980  loss = 3.61016 avg_loss = 3.96456\n",
      "epoch no.0 train no.23990  loss = 3.96163 avg_loss = 3.95656\n",
      "epoch no.0 train no.24000  loss = 3.70049 avg_loss = 3.92450\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '▁드라이브', '에', '▁듣기', '▁좋은', '▁노래', '음악', '</s>']\n",
      "여름 밤 드라이브 하면서 듣기 좋은 인디음악</s>\n",
      "epoch no.0 train no.24010  loss = 3.94017 avg_loss = 3.92621\n",
      "epoch no.0 train no.24020  loss = 2.11254 avg_loss = 3.90834\n",
      "epoch no.0 train no.24030  loss = 4.37145 avg_loss = 3.88436\n",
      "epoch no.0 train no.24040  loss = 2.80172 avg_loss = 3.86034\n",
      "epoch no.0 train no.24050  loss = 4.61074 avg_loss = 3.89642\n",
      "epoch no.0 train no.24060  loss = 3.30137 avg_loss = 3.91359\n",
      "epoch no.0 train no.24070  loss = 3.80867 avg_loss = 3.89888\n",
      "epoch no.0 train no.24080  loss = 2.16572 avg_loss = 3.90628\n",
      "epoch no.0 train no.24090  loss = 4.41397 avg_loss = 3.92717\n",
      "epoch no.0 train no.24100  loss = 3.55840 avg_loss = 3.91516\n",
      "epoch no.0 train no.24110  loss = 2.43743 avg_loss = 3.90920\n",
      "epoch no.0 train no.24120  loss = 2.73839 avg_loss = 3.87725\n",
      "epoch no.0 train no.24130  loss = 3.60720 avg_loss = 3.88273\n",
      "epoch no.0 train no.24140  loss = 3.48092 avg_loss = 3.92123\n",
      "epoch no.0 train no.24150  loss = 4.16351 avg_loss = 3.94548\n",
      "epoch no.0 train no.24160  loss = 3.36607 avg_loss = 3.93047\n",
      "epoch no.0 train no.24170  loss = 3.98644 avg_loss = 3.94523\n",
      "epoch no.0 train no.24180  loss = 3.97664 avg_loss = 3.94649\n",
      "epoch no.0 train no.24190  loss = 5.58312 avg_loss = 3.95726\n",
      "epoch no.0 train no.24200  loss = 1.64877 avg_loss = 3.90317\n",
      "epoch no.0 train no.24210  loss = 3.70443 avg_loss = 3.92806\n",
      "epoch no.0 train no.24220  loss = 3.71289 avg_loss = 3.91051\n",
      "epoch no.0 train no.24230  loss = 4.26520 avg_loss = 3.93176\n",
      "epoch no.0 train no.24240  loss = 3.78423 avg_loss = 3.88483\n",
      "epoch no.0 train no.24250  loss = 2.37933 avg_loss = 3.89121\n",
      "epoch no.0 train no.24260  loss = 4.27611 avg_loss = 3.91610\n",
      "epoch no.0 train no.24270  loss = 4.42005 avg_loss = 3.93828\n",
      "epoch no.0 train no.24280  loss = 3.82574 avg_loss = 3.88560\n",
      "epoch no.0 train no.24290  loss = 3.22606 avg_loss = 3.90432\n",
      "epoch no.0 train no.24300  loss = 3.76672 avg_loss = 3.92863\n",
      "epoch no.0 train no.24310  loss = 4.06002 avg_loss = 3.98855\n",
      "epoch no.0 train no.24320  loss = 7.70140 avg_loss = 4.02447\n",
      "epoch no.0 train no.24330  loss = 4.25069 avg_loss = 4.07930\n",
      "epoch no.0 train no.24340  loss = 3.97460 avg_loss = 4.03641\n",
      "epoch no.0 train no.24350  loss = 2.79913 avg_loss = 4.00612\n",
      "epoch no.0 train no.24360  loss = 3.78632 avg_loss = 4.00734\n",
      "epoch no.0 train no.24370  loss = 3.73761 avg_loss = 3.99720\n",
      "epoch no.0 train no.24380  loss = 2.83737 avg_loss = 4.01532\n",
      "epoch no.0 train no.24390  loss = 5.48933 avg_loss = 4.00645\n",
      "epoch no.0 train no.24400  loss = 3.73302 avg_loss = 3.96395\n",
      "epoch no.0 train no.24410  loss = 2.84701 avg_loss = 3.98601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.24420  loss = 5.72692 avg_loss = 4.00251\n",
      "epoch no.0 train no.24430  loss = 3.24852 avg_loss = 3.96094\n",
      "epoch no.0 train no.24440  loss = 6.33009 avg_loss = 3.99202\n",
      "epoch no.0 train no.24450  loss = 2.20020 avg_loss = 3.94670\n",
      "epoch no.0 train no.24460  loss = 3.05415 avg_loss = 3.97861\n",
      "epoch no.0 train no.24470  loss = 3.55610 avg_loss = 3.97823\n",
      "epoch no.0 train no.24480  loss = 2.82510 avg_loss = 3.96586\n",
      "epoch no.0 train no.24490  loss = 3.84856 avg_loss = 3.97256\n",
      "epoch no.0 train no.24500  loss = 3.10722 avg_loss = 3.96226\n",
      "epoch no.0 train no.24510  loss = 5.14734 avg_loss = 3.95353\n",
      "epoch no.0 train no.24520  loss = 6.97016 avg_loss = 3.96917\n",
      "epoch no.0 train no.24530  loss = 3.41919 avg_loss = 3.91719\n",
      "epoch no.0 train no.24540  loss = 3.88107 avg_loss = 3.90425\n",
      "epoch no.0 train no.24550  loss = 2.06564 avg_loss = 3.86230\n",
      "epoch no.0 train no.24560  loss = 3.79493 avg_loss = 3.84950\n",
      "epoch no.0 train no.24570  loss = 3.46341 avg_loss = 3.79154\n",
      "epoch no.0 train no.24580  loss = 2.60501 avg_loss = 3.76867\n",
      "epoch no.0 train no.24590  loss = 5.27007 avg_loss = 3.79139\n",
      "epoch no.0 train no.24600  loss = 5.09705 avg_loss = 3.78519\n",
      "epoch no.0 train no.24610  loss = 3.08811 avg_loss = 3.80196\n",
      "epoch no.0 train no.24620  loss = 2.75898 avg_loss = 3.80029\n",
      "epoch no.0 train no.24630  loss = 3.80767 avg_loss = 3.80128\n",
      "epoch no.0 train no.24640  loss = 3.09386 avg_loss = 3.86049\n",
      "epoch no.0 train no.24650  loss = 3.16365 avg_loss = 3.87747\n",
      "epoch no.0 train no.24660  loss = 4.83867 avg_loss = 3.87439\n",
      "epoch no.0 train no.24670  loss = 3.62688 avg_loss = 3.86888\n",
      "epoch no.0 train no.24680  loss = 4.12402 avg_loss = 3.91285\n",
      "epoch no.0 train no.24690  loss = 2.11056 avg_loss = 3.94461\n",
      "epoch no.0 train no.24700  loss = 5.92702 avg_loss = 4.01125\n",
      "epoch no.0 train no.24710  loss = 4.71719 avg_loss = 3.98226\n",
      "epoch no.0 train no.24720  loss = 5.34617 avg_loss = 4.01467\n",
      "epoch no.0 train no.24730  loss = 3.44022 avg_loss = 3.99940\n",
      "epoch no.0 train no.24740  loss = 3.31056 avg_loss = 3.97462\n",
      "epoch no.0 train no.24750  loss = 3.38535 avg_loss = 3.92933\n",
      "epoch no.0 train no.24760  loss = 5.99546 avg_loss = 3.94812\n",
      "epoch no.0 train no.24770  loss = 4.22928 avg_loss = 3.95315\n",
      "epoch no.0 train no.24780  loss = 4.36036 avg_loss = 3.99024\n",
      "epoch no.0 train no.24790  loss = 3.09289 avg_loss = 3.95392\n",
      "epoch no.0 train no.24800  loss = 5.43634 avg_loss = 4.00157\n",
      "epoch no.0 train no.24810  loss = 5.90002 avg_loss = 4.00691\n",
      "epoch no.0 train no.24820  loss = 3.32697 avg_loss = 4.01528\n",
      "epoch no.0 train no.24830  loss = 4.39463 avg_loss = 4.00312\n",
      "epoch no.0 train no.24840  loss = 4.04379 avg_loss = 4.06805\n",
      "epoch no.0 train no.24850  loss = 4.57235 avg_loss = 4.00278\n",
      "epoch no.0 train no.24860  loss = 4.24801 avg_loss = 4.01745\n",
      "epoch no.0 train no.24870  loss = 5.01302 avg_loss = 4.00844\n",
      "epoch no.0 train no.24880  loss = 2.70593 avg_loss = 3.97930\n",
      "epoch no.0 train no.24890  loss = 3.28805 avg_loss = 3.95829\n",
      "epoch no.0 train no.24900  loss = 2.20196 avg_loss = 3.97513\n",
      "epoch no.0 train no.24910  loss = 1.61578 avg_loss = 3.98691\n",
      "epoch no.0 train no.24920  loss = 2.92601 avg_loss = 3.94300\n",
      "epoch no.0 train no.24930  loss = 3.83947 avg_loss = 3.93640\n",
      "epoch no.0 train no.24940  loss = 3.55319 avg_loss = 3.89156\n",
      "epoch no.0 train no.24950  loss = 3.22907 avg_loss = 3.88670\n",
      "epoch no.0 train no.24960  loss = 4.77155 avg_loss = 3.91959\n",
      "epoch no.0 train no.24970  loss = 4.16957 avg_loss = 3.94571\n",
      "epoch no.0 train no.24980  loss = 3.38003 avg_loss = 3.95912\n",
      "epoch no.0 train no.24990  loss = 3.00357 avg_loss = 3.90069\n",
      "epoch no.0 train no.25000  loss = 2.60807 avg_loss = 3.88880\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁시원한', '▁신나는', '▁노래', '</s>']\n",
      "여름엔 역시 시원한 노래</s>\n",
      "epoch no.0 train no.25010  loss = 3.17182 avg_loss = 3.90742\n",
      "epoch no.0 train no.25020  loss = 3.01634 avg_loss = 3.91081\n",
      "epoch no.0 train no.25030  loss = 3.71373 avg_loss = 3.88748\n",
      "epoch no.0 train no.25040  loss = 3.57718 avg_loss = 3.88742\n",
      "epoch no.0 train no.25050  loss = 4.12284 avg_loss = 3.82760\n",
      "epoch no.0 train no.25060  loss = 4.14937 avg_loss = 3.89493\n",
      "epoch no.0 train no.25070  loss = 2.46397 avg_loss = 3.94341\n",
      "epoch no.0 train no.25080  loss = 2.87425 avg_loss = 4.00320\n",
      "epoch no.0 train no.25090  loss = 3.14793 avg_loss = 4.04537\n",
      "epoch no.0 train no.25100  loss = 6.62241 avg_loss = 4.06346\n",
      "epoch no.0 train no.25110  loss = 3.29974 avg_loss = 4.03402\n",
      "epoch no.0 train no.25120  loss = 5.25259 avg_loss = 4.07678\n",
      "epoch no.0 train no.25130  loss = 4.73945 avg_loss = 4.08409\n",
      "epoch no.0 train no.25140  loss = 3.23693 avg_loss = 4.10756\n",
      "epoch no.0 train no.25150  loss = 3.25067 avg_loss = 4.05420\n",
      "epoch no.0 train no.25160  loss = 2.42453 avg_loss = 4.02957\n",
      "epoch no.0 train no.25170  loss = 3.72780 avg_loss = 4.02037\n",
      "epoch no.0 train no.25180  loss = 4.65773 avg_loss = 4.05182\n",
      "epoch no.0 train no.25190  loss = 4.20195 avg_loss = 3.99322\n",
      "epoch no.0 train no.25200  loss = 2.90925 avg_loss = 3.98552\n",
      "epoch no.0 train no.25210  loss = 2.56374 avg_loss = 3.95521\n",
      "epoch no.0 train no.25220  loss = 4.68386 avg_loss = 3.92519\n",
      "epoch no.0 train no.25230  loss = 3.81040 avg_loss = 3.93357\n",
      "epoch no.0 train no.25240  loss = 4.95711 avg_loss = 3.96347\n",
      "epoch no.0 train no.25250  loss = 4.63579 avg_loss = 3.99266\n",
      "epoch no.0 train no.25260  loss = 3.72897 avg_loss = 4.05003\n",
      "epoch no.0 train no.25270  loss = 4.48607 avg_loss = 4.02660\n",
      "epoch no.0 train no.25280  loss = 4.29134 avg_loss = 4.03485\n",
      "epoch no.0 train no.25290  loss = 5.14740 avg_loss = 4.03274\n",
      "epoch no.0 train no.25300  loss = 3.95442 avg_loss = 4.00332\n",
      "epoch no.0 train no.25310  loss = 1.63362 avg_loss = 3.99265\n",
      "epoch no.0 train no.25320  loss = 1.70297 avg_loss = 3.99474\n",
      "epoch no.0 train no.25330  loss = 4.02310 avg_loss = 3.90867\n",
      "epoch no.0 train no.25340  loss = 6.87381 avg_loss = 3.91301\n",
      "epoch no.0 train no.25350  loss = 3.10219 avg_loss = 3.91128\n",
      "epoch no.0 train no.25360  loss = 4.32826 avg_loss = 3.90344\n",
      "epoch no.0 train no.25370  loss = 3.91667 avg_loss = 3.93408\n",
      "epoch no.0 train no.25380  loss = 4.26707 avg_loss = 3.93685\n",
      "epoch no.0 train no.25390  loss = 2.03653 avg_loss = 3.93846\n",
      "epoch no.0 train no.25400  loss = 2.99851 avg_loss = 3.91082\n",
      "epoch no.0 train no.25410  loss = 6.93282 avg_loss = 3.96910\n",
      "epoch no.0 train no.25420  loss = 3.10760 avg_loss = 3.98498\n",
      "epoch no.0 train no.25430  loss = 4.27526 avg_loss = 3.98982\n",
      "epoch no.0 train no.25440  loss = 2.31954 avg_loss = 3.95527\n",
      "epoch no.0 train no.25450  loss = 5.26017 avg_loss = 4.02176\n",
      "epoch no.0 train no.25460  loss = 3.16260 avg_loss = 3.99308\n",
      "epoch no.0 train no.25470  loss = 2.41020 avg_loss = 4.02129\n",
      "epoch no.0 train no.25480  loss = 4.10314 avg_loss = 4.02190\n",
      "epoch no.0 train no.25490  loss = 3.32222 avg_loss = 4.06605\n",
      "epoch no.0 train no.25500  loss = 4.93048 avg_loss = 4.05569\n",
      "epoch no.0 train no.25510  loss = 3.59649 avg_loss = 4.00734\n",
      "epoch no.0 train no.25520  loss = 4.54753 avg_loss = 3.98940\n",
      "epoch no.0 train no.25530  loss = 3.24757 avg_loss = 4.00088\n",
      "epoch no.0 train no.25540  loss = 5.44419 avg_loss = 4.00906\n",
      "epoch no.0 train no.25550  loss = 2.10477 avg_loss = 3.95755\n",
      "epoch no.0 train no.25560  loss = 3.67013 avg_loss = 3.94285\n",
      "epoch no.0 train no.25570  loss = 4.40290 avg_loss = 3.96183\n",
      "epoch no.0 train no.25580  loss = 4.00024 avg_loss = 3.98655\n",
      "epoch no.0 train no.25590  loss = 2.16433 avg_loss = 4.03587\n",
      "epoch no.0 train no.25600  loss = 4.93747 avg_loss = 3.97089\n",
      "epoch no.0 train no.25610  loss = 5.85211 avg_loss = 3.99023\n",
      "epoch no.0 train no.25620  loss = 2.53519 avg_loss = 3.95483\n",
      "epoch no.0 train no.25630  loss = 3.88751 avg_loss = 3.96571\n",
      "epoch no.0 train no.25640  loss = 4.83385 avg_loss = 3.99064\n",
      "epoch no.0 train no.25650  loss = 4.86946 avg_loss = 3.98251\n",
      "epoch no.0 train no.25660  loss = 3.77686 avg_loss = 3.93117\n",
      "epoch no.0 train no.25670  loss = 3.69705 avg_loss = 3.91153\n",
      "epoch no.0 train no.25680  loss = 3.89689 avg_loss = 3.91899\n",
      "epoch no.0 train no.25690  loss = 4.19569 avg_loss = 3.87575\n",
      "epoch no.0 train no.25700  loss = 4.34732 avg_loss = 3.86329\n",
      "epoch no.0 train no.25710  loss = 4.18611 avg_loss = 3.87940\n",
      "epoch no.0 train no.25720  loss = 5.33532 avg_loss = 3.99237\n",
      "epoch no.0 train no.25730  loss = 4.60951 avg_loss = 4.07441\n",
      "epoch no.0 train no.25740  loss = 2.68344 avg_loss = 4.04554\n",
      "epoch no.0 train no.25750  loss = 2.84683 avg_loss = 4.08718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.25760  loss = 3.14570 avg_loss = 4.07874\n",
      "epoch no.0 train no.25770  loss = 5.27430 avg_loss = 4.11101\n",
      "epoch no.0 train no.25780  loss = 4.24016 avg_loss = 4.14026\n",
      "epoch no.0 train no.25790  loss = 4.50230 avg_loss = 4.13469\n",
      "epoch no.0 train no.25800  loss = 3.20112 avg_loss = 4.06312\n",
      "epoch no.0 train no.25810  loss = 6.92259 avg_loss = 4.05316\n",
      "epoch no.0 train no.25820  loss = 3.40055 avg_loss = 4.05789\n",
      "epoch no.0 train no.25830  loss = 3.59896 avg_loss = 4.03123\n",
      "epoch no.0 train no.25840  loss = 2.42842 avg_loss = 3.98716\n",
      "epoch no.0 train no.25850  loss = 6.01929 avg_loss = 4.01714\n",
      "epoch no.0 train no.25860  loss = 6.03712 avg_loss = 4.00576\n",
      "epoch no.0 train no.25870  loss = 4.11961 avg_loss = 4.04791\n",
      "epoch no.0 train no.25880  loss = 2.32848 avg_loss = 4.03906\n",
      "epoch no.0 train no.25890  loss = 3.94400 avg_loss = 4.00765\n",
      "epoch no.0 train no.25900  loss = 4.38029 avg_loss = 3.97390\n",
      "epoch no.0 train no.25910  loss = 3.43332 avg_loss = 4.02943\n",
      "epoch no.0 train no.25920  loss = 4.02786 avg_loss = 4.00631\n",
      "epoch no.0 train no.25930  loss = 4.12086 avg_loss = 3.97898\n",
      "epoch no.0 train no.25940  loss = 3.36462 avg_loss = 3.93900\n",
      "epoch no.0 train no.25950  loss = 2.75517 avg_loss = 3.89775\n",
      "epoch no.0 train no.25960  loss = 5.27349 avg_loss = 3.94266\n",
      "epoch no.0 train no.25970  loss = 4.35253 avg_loss = 3.92694\n",
      "epoch no.0 train no.25980  loss = 2.60304 avg_loss = 3.92971\n",
      "epoch no.0 train no.25990  loss = 4.01499 avg_loss = 3.94269\n",
      "epoch no.0 train no.26000  loss = 3.35152 avg_loss = 3.94325\n",
      "3\n",
      "to_tokens: ['▁비', '밤', '▁신나는', '▁노래', '</s>']\n",
      "여름엔 신나는 노래</s>\n",
      "epoch no.0 train no.26010  loss = 3.98197 avg_loss = 3.94408\n",
      "epoch no.0 train no.26020  loss = 2.32272 avg_loss = 3.98408\n",
      "epoch no.0 train no.26030  loss = 2.80765 avg_loss = 3.98894\n",
      "epoch no.0 train no.26040  loss = 2.67651 avg_loss = 4.01968\n",
      "epoch no.0 train no.26050  loss = 4.45339 avg_loss = 4.02847\n",
      "epoch no.0 train no.26060  loss = 3.20350 avg_loss = 4.04939\n",
      "epoch no.0 train no.26070  loss = 4.20620 avg_loss = 4.04503\n",
      "epoch no.0 train no.26080  loss = 5.94482 avg_loss = 4.08731\n",
      "epoch no.0 train no.26090  loss = 3.28353 avg_loss = 4.10182\n",
      "epoch no.0 train no.26100  loss = 3.54536 avg_loss = 4.11845\n",
      "epoch no.0 train no.26110  loss = 3.87510 avg_loss = 4.11179\n",
      "epoch no.0 train no.26120  loss = 7.33725 avg_loss = 4.13357\n",
      "epoch no.0 train no.26130  loss = 2.55638 avg_loss = 4.04768\n",
      "epoch no.0 train no.26140  loss = 5.12634 avg_loss = 4.02484\n",
      "epoch no.0 train no.26150  loss = 3.28053 avg_loss = 4.02664\n",
      "epoch no.0 train no.26160  loss = 5.78449 avg_loss = 4.11901\n",
      "epoch no.0 train no.26170  loss = 3.73672 avg_loss = 4.11806\n",
      "epoch no.0 train no.26180  loss = 3.43200 avg_loss = 4.11705\n",
      "epoch no.0 train no.26190  loss = 5.68649 avg_loss = 4.15094\n",
      "epoch no.0 train no.26200  loss = 3.52461 avg_loss = 4.10807\n",
      "epoch no.0 train no.26210  loss = 2.17279 avg_loss = 4.03037\n",
      "epoch no.0 train no.26220  loss = 5.20147 avg_loss = 4.04113\n",
      "epoch no.0 train no.26230  loss = 3.83980 avg_loss = 4.04951\n",
      "epoch no.0 train no.26240  loss = 3.51185 avg_loss = 4.05995\n",
      "epoch no.0 train no.26250  loss = 2.29844 avg_loss = 4.02339\n",
      "epoch no.0 train no.26260  loss = 4.23185 avg_loss = 4.02302\n",
      "epoch no.0 train no.26270  loss = 5.92481 avg_loss = 4.02471\n",
      "epoch no.0 train no.26280  loss = 4.41909 avg_loss = 4.05971\n",
      "epoch no.0 train no.26290  loss = 3.12875 avg_loss = 4.03555\n",
      "epoch no.0 train no.26300  loss = 3.80483 avg_loss = 4.00695\n",
      "epoch no.0 train no.26310  loss = 2.92583 avg_loss = 3.95510\n",
      "epoch no.0 train no.26320  loss = 5.65522 avg_loss = 4.05108\n",
      "epoch no.0 train no.26330  loss = 4.07020 avg_loss = 4.12426\n",
      "epoch no.0 train no.26340  loss = 6.00387 avg_loss = 4.13775\n",
      "epoch no.0 train no.26350  loss = 4.65187 avg_loss = 4.10818\n",
      "epoch no.0 train no.26360  loss = 2.57433 avg_loss = 4.07069\n",
      "epoch no.0 train no.26370  loss = 3.30590 avg_loss = 4.01744\n",
      "epoch no.0 train no.26380  loss = 3.03689 avg_loss = 3.95461\n",
      "epoch no.0 train no.26390  loss = 2.14456 avg_loss = 3.90055\n",
      "epoch no.0 train no.26400  loss = 5.02150 avg_loss = 3.89233\n",
      "epoch no.0 train no.26410  loss = 4.66481 avg_loss = 3.90182\n",
      "epoch no.0 train no.26420  loss = 3.38021 avg_loss = 3.90667\n",
      "epoch no.0 train no.26430  loss = 2.16643 avg_loss = 3.87408\n",
      "epoch no.0 train no.26440  loss = 2.43820 avg_loss = 3.87488\n",
      "epoch no.0 train no.26450  loss = 2.02084 avg_loss = 3.86270\n",
      "epoch no.0 train no.26460  loss = 3.71958 avg_loss = 3.91579\n",
      "epoch no.0 train no.26470  loss = 5.30996 avg_loss = 3.92606\n",
      "epoch no.0 train no.26480  loss = 8.07557 avg_loss = 3.96668\n",
      "epoch no.0 train no.26490  loss = 3.39633 avg_loss = 3.93627\n",
      "epoch no.0 train no.26500  loss = 3.56249 avg_loss = 3.93850\n",
      "epoch no.0 train no.26510  loss = 4.59895 avg_loss = 3.93424\n",
      "epoch no.0 train no.26520  loss = 1.87006 avg_loss = 3.92003\n",
      "epoch no.0 train no.26530  loss = 3.00479 avg_loss = 3.94683\n",
      "epoch no.0 train no.26540  loss = 4.51496 avg_loss = 3.94156\n",
      "epoch no.0 train no.26550  loss = 4.96404 avg_loss = 3.88705\n",
      "epoch no.0 train no.26560  loss = 3.29396 avg_loss = 3.87463\n",
      "epoch no.0 train no.26570  loss = 3.45236 avg_loss = 3.86803\n",
      "epoch no.0 train no.26580  loss = 3.66248 avg_loss = 3.79690\n",
      "epoch no.0 train no.26590  loss = 2.88248 avg_loss = 3.78428\n",
      "epoch no.0 train no.26600  loss = 4.64369 avg_loss = 3.84405\n",
      "epoch no.0 train no.26610  loss = 4.60222 avg_loss = 3.80541\n",
      "epoch no.0 train no.26620  loss = 3.70565 avg_loss = 3.87509\n",
      "epoch no.0 train no.26630  loss = 2.96317 avg_loss = 3.86531\n",
      "epoch no.0 train no.26640  loss = 3.32147 avg_loss = 3.81439\n",
      "epoch no.0 train no.26650  loss = 3.43161 avg_loss = 3.81628\n",
      "epoch no.0 train no.26660  loss = 3.06608 avg_loss = 3.85091\n",
      "epoch no.0 train no.26670  loss = 3.76835 avg_loss = 3.85685\n",
      "epoch no.0 train no.26680  loss = 3.43107 avg_loss = 3.86111\n",
      "epoch no.0 train no.26690  loss = 7.37449 avg_loss = 3.89320\n",
      "epoch no.0 train no.26700  loss = 6.80564 avg_loss = 3.90815\n",
      "epoch no.0 train no.26710  loss = 6.12528 avg_loss = 3.88274\n",
      "epoch no.0 train no.26720  loss = 4.22434 avg_loss = 3.83691\n",
      "epoch no.0 train no.26730  loss = 5.57706 avg_loss = 3.85207\n",
      "epoch no.0 train no.26740  loss = 5.05976 avg_loss = 3.86719\n",
      "epoch no.0 train no.26750  loss = 2.54006 avg_loss = 3.87132\n",
      "epoch no.0 train no.26760  loss = 3.11506 avg_loss = 3.84090\n",
      "epoch no.0 train no.26770  loss = 3.88101 avg_loss = 3.88381\n",
      "epoch no.0 train no.26780  loss = 4.63027 avg_loss = 3.91660\n",
      "epoch no.0 train no.26790  loss = 3.50472 avg_loss = 3.93676\n",
      "epoch no.0 train no.26800  loss = 7.51346 avg_loss = 3.97006\n",
      "epoch no.0 train no.26810  loss = 6.64629 avg_loss = 4.01427\n",
      "epoch no.0 train no.26820  loss = 4.42548 avg_loss = 4.10749\n",
      "epoch no.0 train no.26830  loss = 3.82565 avg_loss = 4.05804\n",
      "epoch no.0 train no.26840  loss = 2.80513 avg_loss = 4.04516\n",
      "epoch no.0 train no.26850  loss = 4.85810 avg_loss = 4.13275\n",
      "epoch no.0 train no.26860  loss = 3.12112 avg_loss = 4.03159\n",
      "epoch no.0 train no.26870  loss = 3.47637 avg_loss = 4.00758\n",
      "epoch no.0 train no.26880  loss = 3.32964 avg_loss = 4.06153\n",
      "epoch no.0 train no.26890  loss = 3.94340 avg_loss = 4.07896\n",
      "epoch no.0 train no.26900  loss = 1.70557 avg_loss = 4.06022\n",
      "epoch no.0 train no.26910  loss = 3.82419 avg_loss = 4.04529\n",
      "epoch no.0 train no.26920  loss = 5.06602 avg_loss = 4.02080\n",
      "epoch no.0 train no.26930  loss = 4.85097 avg_loss = 4.05122\n",
      "epoch no.0 train no.26940  loss = 5.42124 avg_loss = 4.02659\n",
      "epoch no.0 train no.26950  loss = 4.03992 avg_loss = 3.97141\n",
      "epoch no.0 train no.26960  loss = 3.94076 avg_loss = 3.95801\n",
      "epoch no.0 train no.26970  loss = 5.43136 avg_loss = 3.94303\n",
      "epoch no.0 train no.26980  loss = 5.00873 avg_loss = 3.96165\n",
      "epoch no.0 train no.26990  loss = 4.30574 avg_loss = 3.93348\n",
      "epoch no.0 train no.27000  loss = 3.30966 avg_loss = 3.96861\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁어울리는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.0 train no.27010  loss = 3.75423 avg_loss = 3.92090\n",
      "epoch no.0 train no.27020  loss = 3.12929 avg_loss = 3.93975\n",
      "epoch no.0 train no.27030  loss = 3.55167 avg_loss = 3.92324\n",
      "epoch no.0 train no.27040  loss = 2.74736 avg_loss = 3.89928\n",
      "epoch no.0 train no.27050  loss = 4.29032 avg_loss = 3.88450\n",
      "epoch no.0 train no.27060  loss = 4.58583 avg_loss = 3.89202\n",
      "epoch no.0 train no.27070  loss = 4.68700 avg_loss = 3.91225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.27080  loss = 3.84525 avg_loss = 3.90928\n",
      "epoch no.0 train no.27090  loss = 5.36421 avg_loss = 3.86946\n",
      "epoch no.0 train no.27100  loss = 3.27871 avg_loss = 3.88772\n",
      "epoch no.0 train no.27110  loss = 4.28244 avg_loss = 3.84844\n",
      "epoch no.0 train no.27120  loss = 5.63148 avg_loss = 3.82447\n",
      "epoch no.0 train no.27130  loss = 4.06056 avg_loss = 3.85725\n",
      "epoch no.0 train no.27140  loss = 2.73371 avg_loss = 3.83801\n",
      "epoch no.0 train no.27150  loss = 3.25225 avg_loss = 3.84961\n",
      "epoch no.0 train no.27160  loss = 3.17328 avg_loss = 3.83110\n",
      "epoch no.0 train no.27170  loss = 5.06048 avg_loss = 3.85957\n",
      "epoch no.0 train no.27180  loss = 4.59846 avg_loss = 3.84794\n",
      "epoch no.0 train no.27190  loss = 2.29379 avg_loss = 3.86127\n",
      "epoch no.0 train no.27200  loss = 3.99185 avg_loss = 3.81113\n",
      "epoch no.0 train no.27210  loss = 5.45692 avg_loss = 3.84966\n",
      "epoch no.0 train no.27220  loss = 4.39276 avg_loss = 3.86808\n",
      "epoch no.0 train no.27230  loss = 2.68274 avg_loss = 3.85902\n",
      "epoch no.0 train no.27240  loss = 4.48990 avg_loss = 3.85734\n",
      "epoch no.0 train no.27250  loss = 3.72395 avg_loss = 3.86979\n",
      "epoch no.0 train no.27260  loss = 3.62150 avg_loss = 3.89961\n",
      "epoch no.0 train no.27270  loss = 3.27746 avg_loss = 3.82385\n",
      "epoch no.0 train no.27280  loss = 4.07457 avg_loss = 3.81147\n",
      "epoch no.0 train no.27290  loss = 2.31111 avg_loss = 3.82044\n",
      "epoch no.0 train no.27300  loss = 3.39757 avg_loss = 3.81469\n",
      "epoch no.0 train no.27310  loss = 1.96271 avg_loss = 3.85566\n",
      "epoch no.0 train no.27320  loss = 2.90443 avg_loss = 3.85227\n",
      "epoch no.0 train no.27330  loss = 2.02876 avg_loss = 3.89452\n",
      "epoch no.0 train no.27340  loss = 3.50726 avg_loss = 3.88290\n",
      "epoch no.0 train no.27350  loss = 5.00325 avg_loss = 3.87023\n",
      "epoch no.0 train no.27360  loss = 4.49001 avg_loss = 3.86822\n",
      "epoch no.0 train no.27370  loss = 4.35373 avg_loss = 3.89228\n",
      "epoch no.0 train no.27380  loss = 2.52993 avg_loss = 3.87965\n",
      "epoch no.0 train no.27390  loss = 2.78627 avg_loss = 3.94805\n",
      "epoch no.0 train no.27400  loss = 3.41467 avg_loss = 3.96040\n",
      "epoch no.0 train no.27410  loss = 3.46028 avg_loss = 3.98571\n",
      "epoch no.0 train no.27420  loss = 2.98702 avg_loss = 3.94855\n",
      "epoch no.0 train no.27430  loss = 3.18959 avg_loss = 3.96495\n",
      "epoch no.0 train no.27440  loss = 6.41163 avg_loss = 4.01286\n",
      "epoch no.0 train no.27450  loss = 5.55522 avg_loss = 4.02667\n",
      "epoch no.0 train no.27460  loss = 3.07437 avg_loss = 4.00840\n",
      "epoch no.0 train no.27470  loss = 4.73037 avg_loss = 4.00936\n",
      "epoch no.0 train no.27480  loss = 4.04207 avg_loss = 3.95755\n",
      "epoch no.0 train no.27490  loss = 5.82522 avg_loss = 4.03063\n",
      "epoch no.0 train no.27500  loss = 4.73376 avg_loss = 4.03300\n",
      "epoch no.0 train no.27510  loss = 4.32006 avg_loss = 4.01230\n",
      "epoch no.0 train no.27520  loss = 4.31769 avg_loss = 4.02253\n",
      "epoch no.0 train no.27530  loss = 3.32865 avg_loss = 3.97730\n",
      "epoch no.0 train no.27540  loss = 2.95076 avg_loss = 3.97614\n",
      "epoch no.0 train no.27550  loss = 7.87874 avg_loss = 4.00883\n",
      "epoch no.0 train no.27560  loss = 5.71535 avg_loss = 4.04312\n",
      "epoch no.0 train no.27570  loss = 2.21117 avg_loss = 4.00452\n",
      "epoch no.0 train no.27580  loss = 7.69785 avg_loss = 4.01775\n",
      "epoch no.0 train no.27590  loss = 2.48330 avg_loss = 3.97716\n",
      "epoch no.0 train no.27600  loss = 8.76097 avg_loss = 4.04563\n",
      "epoch no.0 train no.27610  loss = 2.35221 avg_loss = 3.97053\n",
      "epoch no.0 train no.27620  loss = 6.00726 avg_loss = 4.01358\n",
      "epoch no.0 train no.27630  loss = 7.16817 avg_loss = 3.99296\n",
      "epoch no.0 train no.27640  loss = 6.92710 avg_loss = 3.99703\n",
      "epoch no.0 train no.27650  loss = 3.02372 avg_loss = 4.00637\n",
      "epoch no.0 train no.27660  loss = 5.66701 avg_loss = 4.01156\n",
      "epoch no.0 train no.27670  loss = 4.02928 avg_loss = 3.96686\n",
      "epoch no.0 train no.27680  loss = 3.54171 avg_loss = 3.98468\n",
      "epoch no.0 train no.27690  loss = 4.52638 avg_loss = 4.04241\n",
      "epoch no.0 train no.27700  loss = 3.64897 avg_loss = 4.02281\n",
      "epoch no.0 train no.27710  loss = 3.23600 avg_loss = 3.99678\n",
      "epoch no.0 train no.27720  loss = 4.49936 avg_loss = 3.98572\n",
      "epoch no.0 train no.27730  loss = 3.94616 avg_loss = 4.00459\n",
      "epoch no.0 train no.27740  loss = 2.75873 avg_loss = 4.01922\n",
      "epoch no.0 train no.27750  loss = 3.46297 avg_loss = 4.02876\n",
      "epoch no.0 train no.27760  loss = 5.92345 avg_loss = 4.00880\n",
      "epoch no.0 train no.27770  loss = 2.76868 avg_loss = 4.00672\n",
      "epoch no.0 train no.27780  loss = 3.22718 avg_loss = 3.97352\n",
      "epoch no.0 train no.27790  loss = 4.58182 avg_loss = 3.99670\n",
      "epoch no.0 train no.27800  loss = 3.57969 avg_loss = 4.04674\n",
      "epoch no.0 train no.27810  loss = 2.12380 avg_loss = 3.99862\n",
      "epoch no.0 train no.27820  loss = 3.36847 avg_loss = 3.97886\n",
      "epoch no.0 train no.27830  loss = 3.61687 avg_loss = 3.98224\n",
      "epoch no.0 train no.27840  loss = 2.84063 avg_loss = 3.95371\n",
      "epoch no.0 train no.27850  loss = 2.51838 avg_loss = 3.89129\n",
      "epoch no.0 train no.27860  loss = 3.73265 avg_loss = 3.90829\n",
      "epoch no.0 train no.27870  loss = 2.77742 avg_loss = 3.89278\n",
      "epoch no.0 train no.27880  loss = 3.50195 avg_loss = 3.90181\n",
      "epoch no.0 train no.27890  loss = 2.62086 avg_loss = 3.93886\n",
      "epoch no.0 train no.27900  loss = 3.94699 avg_loss = 3.91169\n",
      "epoch no.0 train no.27910  loss = 4.36669 avg_loss = 3.92234\n",
      "epoch no.0 train no.27920  loss = 4.75935 avg_loss = 3.98384\n",
      "epoch no.0 train no.27930  loss = 2.51593 avg_loss = 3.94074\n",
      "epoch no.0 train no.27940  loss = 5.34813 avg_loss = 3.95500\n",
      "epoch no.0 train no.27950  loss = 6.22887 avg_loss = 3.95852\n",
      "epoch no.0 train no.27960  loss = 4.62777 avg_loss = 3.87845\n",
      "epoch no.0 train no.27970  loss = 3.47428 avg_loss = 3.89756\n",
      "epoch no.0 train no.27980  loss = 3.02056 avg_loss = 3.85926\n",
      "epoch no.0 train no.27990  loss = 3.50432 avg_loss = 3.81646\n",
      "epoch no.0 train no.28000  loss = 6.03263 avg_loss = 3.92843\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '▁발라', '▁노래', '</s>']\n",
      "여름밤에 어울리는 감성적인 노래</s>\n",
      "epoch no.0 train no.28010  loss = 3.39100 avg_loss = 3.90757\n",
      "epoch no.0 train no.28020  loss = 7.39006 avg_loss = 3.94103\n",
      "epoch no.0 train no.28030  loss = 4.53589 avg_loss = 3.96446\n",
      "epoch no.0 train no.28040  loss = 4.11411 avg_loss = 3.97857\n",
      "epoch no.0 train no.28050  loss = 4.54128 avg_loss = 3.92888\n",
      "epoch no.0 train no.28060  loss = 3.00421 avg_loss = 3.97279\n",
      "epoch no.0 train no.28070  loss = 6.28252 avg_loss = 3.92033\n",
      "epoch no.0 train no.28080  loss = 4.96380 avg_loss = 3.91149\n",
      "epoch no.0 train no.28090  loss = 2.92391 avg_loss = 3.86324\n",
      "epoch no.0 train no.28100  loss = 6.39638 avg_loss = 3.86700\n",
      "epoch no.0 train no.28110  loss = 3.82271 avg_loss = 3.93773\n",
      "epoch no.0 train no.28120  loss = 3.68484 avg_loss = 3.92932\n",
      "epoch no.0 train no.28130  loss = 4.18901 avg_loss = 3.94541\n",
      "epoch no.0 train no.28140  loss = 2.86830 avg_loss = 4.00819\n",
      "epoch no.0 train no.28150  loss = 4.34750 avg_loss = 3.98207\n",
      "epoch no.0 train no.28160  loss = 3.61872 avg_loss = 3.95821\n",
      "epoch no.0 train no.28170  loss = 3.55506 avg_loss = 3.96465\n",
      "epoch no.0 train no.28180  loss = 5.17423 avg_loss = 4.01793\n",
      "epoch no.0 train no.28190  loss = 3.25161 avg_loss = 3.99363\n",
      "epoch no.0 train no.28200  loss = 3.80877 avg_loss = 3.95412\n",
      "epoch no.0 train no.28210  loss = 3.02815 avg_loss = 3.90526\n",
      "epoch no.0 train no.28220  loss = 5.78867 avg_loss = 3.93758\n",
      "epoch no.0 train no.28230  loss = 4.58628 avg_loss = 3.99830\n",
      "epoch no.0 train no.28240  loss = 3.21491 avg_loss = 3.97622\n",
      "epoch no.0 train no.28250  loss = 2.39715 avg_loss = 4.02449\n",
      "epoch no.0 train no.28260  loss = 3.52748 avg_loss = 4.01953\n",
      "epoch no.0 train no.28270  loss = 3.51980 avg_loss = 4.00824\n",
      "epoch no.0 train no.28280  loss = 2.62281 avg_loss = 3.98091\n",
      "epoch no.0 train no.28290  loss = 3.97557 avg_loss = 3.94845\n",
      "epoch no.0 train no.28300  loss = 3.33632 avg_loss = 3.96803\n",
      "epoch no.0 train no.28310  loss = 4.12393 avg_loss = 3.91166\n",
      "epoch no.0 train no.28320  loss = 4.14653 avg_loss = 3.96668\n",
      "epoch no.0 train no.28330  loss = 2.80237 avg_loss = 3.96728\n",
      "epoch no.0 train no.28340  loss = 4.17785 avg_loss = 3.95074\n",
      "epoch no.0 train no.28350  loss = 6.24413 avg_loss = 4.01028\n",
      "epoch no.0 train no.28360  loss = 2.82176 avg_loss = 4.00636\n",
      "epoch no.0 train no.28370  loss = 6.61991 avg_loss = 4.04690\n",
      "epoch no.0 train no.28380  loss = 3.74501 avg_loss = 4.01675\n",
      "epoch no.0 train no.28390  loss = 5.10781 avg_loss = 4.05187\n",
      "epoch no.0 train no.28400  loss = 4.16844 avg_loss = 3.97506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.28410  loss = 3.58035 avg_loss = 3.95180\n",
      "epoch no.0 train no.28420  loss = 5.47844 avg_loss = 3.92861\n",
      "epoch no.0 train no.28430  loss = 6.05456 avg_loss = 3.94128\n",
      "epoch no.0 train no.28440  loss = 7.93339 avg_loss = 4.06613\n",
      "epoch no.0 train no.28450  loss = 3.84782 avg_loss = 4.06391\n",
      "epoch no.0 train no.28460  loss = 4.90892 avg_loss = 4.03118\n",
      "epoch no.0 train no.28470  loss = 3.80196 avg_loss = 4.06602\n",
      "epoch no.0 train no.28480  loss = 2.84093 avg_loss = 4.10273\n",
      "epoch no.0 train no.28490  loss = 5.80264 avg_loss = 4.07875\n",
      "epoch no.0 train no.28500  loss = 2.90899 avg_loss = 4.08753\n",
      "epoch no.0 train no.28510  loss = 4.60867 avg_loss = 4.06540\n",
      "epoch no.0 train no.28520  loss = 2.30855 avg_loss = 4.01980\n",
      "epoch no.0 train no.28530  loss = 6.26272 avg_loss = 4.07059\n",
      "epoch no.0 train no.28540  loss = 4.21215 avg_loss = 4.00739\n",
      "epoch no.0 train no.28550  loss = 3.89358 avg_loss = 3.98599\n",
      "epoch no.0 train no.28560  loss = 3.13173 avg_loss = 3.93878\n",
      "epoch no.0 train no.28570  loss = 4.81812 avg_loss = 3.94690\n",
      "epoch no.0 train no.28580  loss = 4.37333 avg_loss = 3.97499\n",
      "epoch no.0 train no.28590  loss = 5.59880 avg_loss = 3.96704\n",
      "epoch no.0 train no.28600  loss = 1.92762 avg_loss = 4.03683\n",
      "epoch no.0 train no.28610  loss = 3.79678 avg_loss = 4.06470\n",
      "epoch no.0 train no.28620  loss = 2.83748 avg_loss = 4.07150\n",
      "epoch no.0 train no.28630  loss = 3.07824 avg_loss = 4.05678\n",
      "epoch no.0 train no.28640  loss = 5.81853 avg_loss = 4.04052\n",
      "epoch no.0 train no.28650  loss = 3.48883 avg_loss = 3.95050\n",
      "epoch no.0 train no.28660  loss = 4.35018 avg_loss = 3.97944\n",
      "epoch no.0 train no.28670  loss = 5.11695 avg_loss = 3.97877\n",
      "epoch no.0 train no.28680  loss = 2.69959 avg_loss = 3.97962\n",
      "epoch no.0 train no.28690  loss = 2.63437 avg_loss = 3.97313\n",
      "epoch no.0 train no.28700  loss = 4.30491 avg_loss = 3.99893\n",
      "epoch no.0 train no.28710  loss = 2.59519 avg_loss = 3.96913\n",
      "epoch no.0 train no.28720  loss = 4.44473 avg_loss = 3.97162\n",
      "epoch no.0 train no.28730  loss = 3.79098 avg_loss = 3.94865\n",
      "epoch no.0 train no.28740  loss = 5.33952 avg_loss = 4.01098\n",
      "epoch no.0 train no.28750  loss = 7.20494 avg_loss = 4.06508\n",
      "epoch no.0 train no.28760  loss = 4.77583 avg_loss = 4.13277\n",
      "epoch no.0 train no.28770  loss = 4.23371 avg_loss = 4.12336\n",
      "epoch no.0 train no.28780  loss = 3.95284 avg_loss = 4.06606\n",
      "epoch no.0 train no.28790  loss = 3.07860 avg_loss = 4.04152\n",
      "epoch no.0 train no.28800  loss = 4.15708 avg_loss = 4.07157\n",
      "epoch no.0 train no.28810  loss = 2.71929 avg_loss = 4.00058\n",
      "epoch no.0 train no.28820  loss = 3.45613 avg_loss = 3.98394\n",
      "epoch no.0 train no.28830  loss = 4.22778 avg_loss = 3.96636\n",
      "epoch no.0 train no.28840  loss = 3.92472 avg_loss = 3.91454\n",
      "epoch no.0 train no.28850  loss = 2.50875 avg_loss = 3.85766\n",
      "epoch no.0 train no.28860  loss = 3.97567 avg_loss = 3.85758\n",
      "epoch no.0 train no.28870  loss = 5.69337 avg_loss = 3.87730\n",
      "epoch no.0 train no.28880  loss = 4.64848 avg_loss = 3.96986\n",
      "epoch no.0 train no.28890  loss = 3.35574 avg_loss = 4.02642\n",
      "epoch no.0 train no.28900  loss = 4.01465 avg_loss = 3.98198\n",
      "epoch no.0 train no.28910  loss = 3.81069 avg_loss = 3.97862\n",
      "epoch no.0 train no.28920  loss = 4.32449 avg_loss = 4.02860\n",
      "epoch no.0 train no.28930  loss = 4.37083 avg_loss = 4.03851\n",
      "epoch no.0 train no.28940  loss = 2.27437 avg_loss = 3.95965\n",
      "epoch no.0 train no.28950  loss = 3.91523 avg_loss = 3.95759\n",
      "epoch no.0 train no.28960  loss = 3.07576 avg_loss = 3.91452\n",
      "epoch no.0 train no.28970  loss = 6.37392 avg_loss = 3.91983\n",
      "epoch no.0 train no.28980  loss = 2.53290 avg_loss = 3.91119\n",
      "epoch no.0 train no.28990  loss = 3.31740 avg_loss = 3.90361\n",
      "epoch no.0 train no.29000  loss = 5.96753 avg_loss = 3.93320\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '▁신나는', '▁댄스', '곡', '</s>']\n",
      "여름엔 신나는 댄스곡</s>\n",
      "epoch no.0 train no.29010  loss = 3.33088 avg_loss = 3.91242\n",
      "epoch no.0 train no.29020  loss = 4.13853 avg_loss = 3.95083\n",
      "epoch no.0 train no.29030  loss = 3.95355 avg_loss = 3.90896\n",
      "epoch no.0 train no.29040  loss = 5.77914 avg_loss = 3.97768\n",
      "epoch no.0 train no.29050  loss = 4.02441 avg_loss = 4.04604\n",
      "epoch no.0 train no.29060  loss = 2.98887 avg_loss = 4.00688\n",
      "epoch no.0 train no.29070  loss = 6.44906 avg_loss = 4.02787\n",
      "epoch no.0 train no.29080  loss = 2.29319 avg_loss = 3.96701\n",
      "epoch no.0 train no.29090  loss = 1.98811 avg_loss = 3.87865\n",
      "epoch no.0 train no.29100  loss = 3.64867 avg_loss = 3.93138\n",
      "epoch no.0 train no.29110  loss = 3.88126 avg_loss = 3.91524\n",
      "epoch no.0 train no.29120  loss = 6.26126 avg_loss = 3.97116\n",
      "epoch no.0 train no.29130  loss = 2.80760 avg_loss = 3.96647\n",
      "epoch no.0 train no.29140  loss = 3.50651 avg_loss = 4.02606\n",
      "epoch no.0 train no.29150  loss = 2.95122 avg_loss = 3.97596\n",
      "epoch no.0 train no.29160  loss = 3.93629 avg_loss = 4.01387\n",
      "epoch no.0 train no.29170  loss = 3.81974 avg_loss = 4.06125\n",
      "epoch no.0 train no.29180  loss = 4.14779 avg_loss = 4.00150\n",
      "epoch no.0 train no.29190  loss = 4.50737 avg_loss = 3.99370\n",
      "epoch no.0 train no.29200  loss = 3.22781 avg_loss = 3.98591\n",
      "epoch no.0 train no.29210  loss = 2.88383 avg_loss = 3.96614\n",
      "epoch no.0 train no.29220  loss = 3.32288 avg_loss = 4.03902\n",
      "epoch no.0 train no.29230  loss = 3.25811 avg_loss = 3.96696\n",
      "epoch no.0 train no.29240  loss = 2.99297 avg_loss = 3.94958\n",
      "epoch no.0 train no.29250  loss = 3.03843 avg_loss = 3.94797\n",
      "epoch no.0 train no.29260  loss = 2.11335 avg_loss = 3.93009\n",
      "epoch no.0 train no.29270  loss = 3.22257 avg_loss = 3.87511\n",
      "epoch no.0 train no.29280  loss = 6.40728 avg_loss = 3.95741\n",
      "epoch no.0 train no.29290  loss = 1.91474 avg_loss = 3.93167\n",
      "epoch no.0 train no.29300  loss = 2.70458 avg_loss = 3.96828\n",
      "epoch no.0 train no.29310  loss = 5.88443 avg_loss = 4.02280\n",
      "epoch no.0 train no.29320  loss = 3.83903 avg_loss = 4.04976\n",
      "epoch no.0 train no.29330  loss = 3.92562 avg_loss = 4.07034\n",
      "epoch no.0 train no.29340  loss = 3.37207 avg_loss = 4.03145\n",
      "epoch no.0 train no.29350  loss = 6.40166 avg_loss = 4.01085\n",
      "epoch no.0 train no.29360  loss = 4.19742 avg_loss = 3.94452\n",
      "epoch no.0 train no.29370  loss = 3.04817 avg_loss = 4.03365\n",
      "epoch no.0 train no.29380  loss = 5.53753 avg_loss = 4.03733\n",
      "epoch no.0 train no.29390  loss = 2.97545 avg_loss = 4.04296\n",
      "epoch no.0 train no.29400  loss = 3.13222 avg_loss = 4.03049\n",
      "epoch no.0 train no.29410  loss = 2.69892 avg_loss = 3.98742\n",
      "epoch no.0 train no.29420  loss = 3.29134 avg_loss = 3.97053\n",
      "epoch no.0 train no.29430  loss = 2.29273 avg_loss = 3.92687\n",
      "epoch no.0 train no.29440  loss = 4.80759 avg_loss = 3.94463\n",
      "epoch no.0 train no.29450  loss = 5.14020 avg_loss = 3.95256\n",
      "epoch no.0 train no.29460  loss = 6.72650 avg_loss = 3.96963\n",
      "epoch no.0 train no.29470  loss = 4.93454 avg_loss = 3.92704\n",
      "epoch no.0 train no.29480  loss = 2.88875 avg_loss = 3.94508\n",
      "epoch no.0 train no.29490  loss = 3.91625 avg_loss = 3.92236\n",
      "epoch no.0 train no.29500  loss = 5.06626 avg_loss = 3.92045\n",
      "epoch no.0 train no.29510  loss = 3.08005 avg_loss = 3.92365\n",
      "epoch no.0 train no.29520  loss = 3.60912 avg_loss = 3.87764\n",
      "epoch no.0 train no.29530  loss = 3.42739 avg_loss = 3.85711\n",
      "epoch no.0 train no.29540  loss = 6.24949 avg_loss = 3.86867\n",
      "epoch no.0 train no.29550  loss = 4.44219 avg_loss = 3.91085\n",
      "epoch no.0 train no.29560  loss = 4.00239 avg_loss = 3.93411\n",
      "epoch no.0 train no.29570  loss = 4.92436 avg_loss = 3.96875\n",
      "epoch no.0 train no.29580  loss = 3.76361 avg_loss = 3.95311\n",
      "epoch no.0 train no.29590  loss = 3.48684 avg_loss = 3.97218\n",
      "epoch no.0 train no.29600  loss = 2.78681 avg_loss = 3.96041\n",
      "epoch no.0 train no.29610  loss = 5.03159 avg_loss = 3.95096\n",
      "epoch no.0 train no.29620  loss = 3.61731 avg_loss = 4.03118\n",
      "epoch no.0 train no.29630  loss = 3.25069 avg_loss = 4.07356\n",
      "epoch no.0 train no.29640  loss = 3.55806 avg_loss = 4.03348\n",
      "epoch no.0 train no.29650  loss = 3.67866 avg_loss = 3.96458\n",
      "epoch no.0 train no.29660  loss = 3.90620 avg_loss = 3.95196\n",
      "epoch no.0 train no.29670  loss = 6.19919 avg_loss = 3.96501\n",
      "epoch no.0 train no.29680  loss = 2.98270 avg_loss = 3.97617\n",
      "epoch no.0 train no.29690  loss = 3.95956 avg_loss = 4.00122\n",
      "epoch no.0 train no.29700  loss = 3.73784 avg_loss = 4.02795\n",
      "epoch no.0 train no.29710  loss = 6.04395 avg_loss = 4.05894\n",
      "epoch no.0 train no.29720  loss = 4.11058 avg_loss = 4.04765\n",
      "epoch no.0 train no.29730  loss = 2.14561 avg_loss = 3.97968\n",
      "epoch no.0 train no.29740  loss = 4.40165 avg_loss = 3.97385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.29750  loss = 2.17674 avg_loss = 3.95682\n",
      "epoch no.0 train no.29760  loss = 3.07242 avg_loss = 4.00542\n",
      "epoch no.0 train no.29770  loss = 3.30589 avg_loss = 3.94428\n",
      "epoch no.0 train no.29780  loss = 3.24677 avg_loss = 4.03365\n",
      "epoch no.0 train no.29790  loss = 2.34307 avg_loss = 3.98749\n",
      "epoch no.0 train no.29800  loss = 2.74771 avg_loss = 4.00304\n",
      "epoch no.0 train no.29810  loss = 4.05662 avg_loss = 3.97452\n",
      "epoch no.0 train no.29820  loss = 3.99598 avg_loss = 3.97934\n",
      "epoch no.0 train no.29830  loss = 5.02351 avg_loss = 4.00286\n",
      "epoch no.0 train no.29840  loss = 2.73086 avg_loss = 3.99594\n",
      "epoch no.0 train no.29850  loss = 3.36480 avg_loss = 4.01485\n",
      "epoch no.0 train no.29860  loss = 3.38983 avg_loss = 4.02584\n",
      "epoch no.0 train no.29870  loss = 4.16488 avg_loss = 4.03419\n",
      "epoch no.0 train no.29880  loss = 4.55960 avg_loss = 4.02655\n",
      "epoch no.0 train no.29890  loss = 3.28675 avg_loss = 4.00766\n",
      "epoch no.0 train no.29900  loss = 3.19056 avg_loss = 3.91847\n",
      "epoch no.0 train no.29910  loss = 3.04698 avg_loss = 3.87631\n",
      "epoch no.0 train no.29920  loss = 4.53232 avg_loss = 3.88602\n",
      "epoch no.0 train no.29930  loss = 3.23507 avg_loss = 3.90837\n",
      "epoch no.0 train no.29940  loss = 4.55597 avg_loss = 3.84630\n",
      "epoch no.0 train no.29950  loss = 2.73284 avg_loss = 3.90162\n",
      "epoch no.0 train no.29960  loss = 2.50438 avg_loss = 3.86990\n",
      "epoch no.0 train no.29970  loss = 3.44044 avg_loss = 3.85986\n",
      "epoch no.0 train no.29980  loss = 3.83685 avg_loss = 3.82809\n",
      "epoch no.0 train no.29990  loss = 3.03309 avg_loss = 3.85191\n",
      "epoch no.0 train no.30000  loss = 4.35836 avg_loss = 3.81692\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁어울리는', '▁노래', '▁음악', '</s>']\n",
      "여름밤에 듣는 시원한 음악</s>\n",
      "epoch no.0 train no.30010  loss = 2.98524 avg_loss = 3.91427\n",
      "epoch no.0 train no.30020  loss = 2.55055 avg_loss = 3.95922\n",
      "epoch no.0 train no.30030  loss = 3.14405 avg_loss = 3.94481\n",
      "epoch no.0 train no.30040  loss = 4.45960 avg_loss = 3.99306\n",
      "epoch no.0 train no.30050  loss = 4.52835 avg_loss = 3.97685\n",
      "epoch no.0 train no.30060  loss = 4.31655 avg_loss = 3.99609\n",
      "epoch no.0 train no.30070  loss = 4.90373 avg_loss = 3.96044\n",
      "epoch no.0 train no.30080  loss = 3.21318 avg_loss = 3.94938\n",
      "epoch no.0 train no.30090  loss = 4.80490 avg_loss = 3.97491\n",
      "epoch no.0 train no.30100  loss = 3.27158 avg_loss = 3.95395\n",
      "epoch no.0 train no.30110  loss = 5.75668 avg_loss = 3.99818\n",
      "epoch no.0 train no.30120  loss = 4.70302 avg_loss = 4.00482\n",
      "epoch no.0 train no.30130  loss = 4.21861 avg_loss = 4.02078\n",
      "epoch no.0 train no.30140  loss = 2.22947 avg_loss = 4.04143\n",
      "epoch no.0 train no.30150  loss = 3.64908 avg_loss = 4.11602\n",
      "epoch no.0 train no.30160  loss = 6.34993 avg_loss = 4.12879\n",
      "epoch no.0 train no.30170  loss = 5.27157 avg_loss = 4.10269\n",
      "epoch no.0 train no.30180  loss = 4.32154 avg_loss = 4.10173\n",
      "epoch no.0 train no.30190  loss = 3.00255 avg_loss = 4.11607\n",
      "epoch no.0 train no.30200  loss = 3.65148 avg_loss = 4.09263\n",
      "epoch no.0 train no.30210  loss = 3.11106 avg_loss = 4.09708\n",
      "epoch no.0 train no.30220  loss = 4.65133 avg_loss = 4.08995\n",
      "epoch no.0 train no.30230  loss = 2.04237 avg_loss = 4.03110\n",
      "epoch no.0 train no.30240  loss = 4.56231 avg_loss = 4.02196\n",
      "epoch no.0 train no.30250  loss = 3.95851 avg_loss = 4.02336\n",
      "epoch no.0 train no.30260  loss = 3.43099 avg_loss = 3.98638\n",
      "epoch no.0 train no.30270  loss = 4.71096 avg_loss = 4.00058\n",
      "epoch no.0 train no.30280  loss = 3.04631 avg_loss = 3.94134\n",
      "epoch no.0 train no.30290  loss = 3.73179 avg_loss = 3.90707\n",
      "epoch no.0 train no.30300  loss = 4.29089 avg_loss = 3.94566\n",
      "epoch no.0 train no.30310  loss = 4.69488 avg_loss = 3.95964\n",
      "epoch no.0 train no.30320  loss = 4.64345 avg_loss = 3.93056\n",
      "epoch no.0 train no.30330  loss = 3.47068 avg_loss = 3.94825\n",
      "epoch no.0 train no.30340  loss = 3.20862 avg_loss = 3.91291\n",
      "epoch no.0 train no.30350  loss = 3.03935 avg_loss = 3.86655\n",
      "epoch no.0 train no.30360  loss = 3.96303 avg_loss = 3.89918\n",
      "epoch no.0 train no.30370  loss = 7.33817 avg_loss = 3.93161\n",
      "epoch no.0 train no.30380  loss = 3.06721 avg_loss = 3.93014\n",
      "epoch no.0 train no.30390  loss = 3.72545 avg_loss = 3.95393\n",
      "epoch no.0 train no.30400  loss = 4.25374 avg_loss = 3.90149\n",
      "epoch no.0 train no.30410  loss = 5.43187 avg_loss = 3.88110\n",
      "epoch no.0 train no.30420  loss = 2.36861 avg_loss = 3.88429\n",
      "epoch no.0 train no.30430  loss = 7.26806 avg_loss = 3.94670\n",
      "epoch no.0 train no.30440  loss = 6.65051 avg_loss = 3.96080\n",
      "epoch no.0 train no.30450  loss = 3.16400 avg_loss = 3.98935\n",
      "epoch no.0 train no.30460  loss = 2.26702 avg_loss = 3.93064\n",
      "epoch no.0 train no.30470  loss = 5.63177 avg_loss = 3.94036\n",
      "epoch no.0 train no.30480  loss = 4.67481 avg_loss = 3.94181\n",
      "epoch no.0 train no.30490  loss = 2.62352 avg_loss = 3.89222\n",
      "epoch no.0 train no.30500  loss = 3.84333 avg_loss = 3.92047\n",
      "epoch no.0 train no.30510  loss = 3.47348 avg_loss = 4.01313\n",
      "epoch no.0 train no.30520  loss = 2.70735 avg_loss = 4.00746\n",
      "epoch no.0 train no.30530  loss = 4.70829 avg_loss = 3.99039\n",
      "epoch no.0 train no.30540  loss = 5.02753 avg_loss = 3.96456\n",
      "epoch no.0 train no.30550  loss = 2.34330 avg_loss = 3.92400\n",
      "epoch no.0 train no.30560  loss = 4.84348 avg_loss = 3.91660\n",
      "epoch no.0 train no.30570  loss = 3.24299 avg_loss = 3.89410\n",
      "epoch no.0 train no.30580  loss = 4.77078 avg_loss = 3.93256\n",
      "epoch no.0 train no.30590  loss = 4.80437 avg_loss = 3.86863\n",
      "epoch no.0 train no.30600  loss = 3.75987 avg_loss = 3.85971\n",
      "epoch no.0 train no.30610  loss = 3.88588 avg_loss = 3.88313\n",
      "epoch no.0 train no.30620  loss = 5.47936 avg_loss = 3.84060\n",
      "epoch no.0 train no.30630  loss = 4.84893 avg_loss = 3.84214\n",
      "epoch no.0 train no.30640  loss = 2.73486 avg_loss = 3.83866\n",
      "epoch no.0 train no.30650  loss = 3.27877 avg_loss = 3.86782\n",
      "epoch no.0 train no.30660  loss = 5.29789 avg_loss = 3.90115\n",
      "epoch no.0 train no.30670  loss = 5.86864 avg_loss = 3.90067\n",
      "epoch no.0 train no.30680  loss = 5.23321 avg_loss = 3.88432\n",
      "epoch no.0 train no.30690  loss = 4.70138 avg_loss = 3.90195\n",
      "epoch no.0 train no.30700  loss = 4.86126 avg_loss = 3.93018\n",
      "epoch no.0 train no.30710  loss = 5.61969 avg_loss = 3.92375\n",
      "epoch no.0 train no.30720  loss = 4.07347 avg_loss = 3.95089\n",
      "epoch no.0 train no.30730  loss = 3.93539 avg_loss = 3.97799\n",
      "epoch no.0 train no.30740  loss = 3.34631 avg_loss = 3.94831\n",
      "epoch no.0 train no.30750  loss = 1.96262 avg_loss = 3.95802\n",
      "epoch no.0 train no.30760  loss = 5.83176 avg_loss = 3.94362\n",
      "epoch no.0 train no.30770  loss = 4.66022 avg_loss = 3.95271\n",
      "epoch no.0 train no.30780  loss = 3.98431 avg_loss = 4.00159\n",
      "epoch no.0 train no.30790  loss = 4.10478 avg_loss = 4.02870\n",
      "epoch no.0 train no.30800  loss = 4.05491 avg_loss = 4.08019\n",
      "epoch no.0 train no.30810  loss = 4.42388 avg_loss = 4.12145\n",
      "epoch no.0 train no.30820  loss = 3.21228 avg_loss = 4.10572\n",
      "epoch no.0 train no.30830  loss = 2.36247 avg_loss = 4.11130\n",
      "epoch no.0 train no.30840  loss = 3.19425 avg_loss = 4.02463\n",
      "epoch no.0 train no.30850  loss = 4.64584 avg_loss = 3.99993\n",
      "epoch no.0 train no.30860  loss = 3.09679 avg_loss = 4.05617\n",
      "epoch no.0 train no.30870  loss = 3.79957 avg_loss = 4.04435\n",
      "epoch no.0 train no.30880  loss = 3.89413 avg_loss = 4.06268\n",
      "epoch no.0 train no.30890  loss = 2.75292 avg_loss = 4.01312\n",
      "epoch no.0 train no.30900  loss = 4.84424 avg_loss = 3.99048\n",
      "epoch no.0 train no.30910  loss = 3.44112 avg_loss = 3.92572\n",
      "epoch no.0 train no.30920  loss = 5.46425 avg_loss = 3.97172\n",
      "epoch no.0 train no.30930  loss = 4.04512 avg_loss = 3.94882\n",
      "epoch no.0 train no.30940  loss = 3.50236 avg_loss = 4.00142\n",
      "epoch no.0 train no.30950  loss = 1.79635 avg_loss = 4.00060\n",
      "epoch no.0 train no.30960  loss = 4.51350 avg_loss = 4.00203\n",
      "epoch no.0 train no.30970  loss = 3.60732 avg_loss = 4.01228\n",
      "epoch no.0 train no.30980  loss = 4.82882 avg_loss = 4.06394\n",
      "epoch no.0 train no.30990  loss = 3.51695 avg_loss = 4.03882\n",
      "epoch no.0 train no.31000  loss = 2.45570 avg_loss = 4.03439\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁위한', '▁노래', '힙', '합', '</s>']\n",
      "여름밤을 책임질 감성힙합</s>\n",
      "epoch no.0 train no.31010  loss = 5.02178 avg_loss = 4.03858\n",
      "epoch no.0 train no.31020  loss = 3.75217 avg_loss = 4.07326\n",
      "epoch no.0 train no.31030  loss = 3.81355 avg_loss = 4.06142\n",
      "epoch no.0 train no.31040  loss = 7.05280 avg_loss = 4.08277\n",
      "epoch no.0 train no.31050  loss = 4.52729 avg_loss = 4.05822\n",
      "epoch no.0 train no.31060  loss = 5.34347 avg_loss = 4.12990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.31070  loss = 6.39431 avg_loss = 4.07435\n",
      "epoch no.0 train no.31080  loss = 4.40077 avg_loss = 4.07775\n",
      "epoch no.0 train no.31090  loss = 2.64885 avg_loss = 4.03654\n",
      "epoch no.0 train no.31100  loss = 6.59244 avg_loss = 4.04153\n",
      "epoch no.0 train no.31110  loss = 4.83112 avg_loss = 4.06091\n",
      "epoch no.0 train no.31120  loss = 3.02665 avg_loss = 4.03104\n",
      "epoch no.0 train no.31130  loss = 2.84967 avg_loss = 4.02440\n",
      "epoch no.0 train no.31140  loss = 3.73803 avg_loss = 4.06389\n",
      "epoch no.0 train no.31150  loss = 4.21172 avg_loss = 4.01393\n",
      "epoch no.0 train no.31160  loss = 6.32166 avg_loss = 4.01199\n",
      "epoch no.0 train no.31170  loss = 3.22494 avg_loss = 3.99943\n",
      "epoch no.0 train no.31180  loss = 2.58290 avg_loss = 4.01984\n",
      "epoch no.0 train no.31190  loss = 4.18946 avg_loss = 4.02904\n",
      "epoch no.0 train no.31200  loss = 5.24687 avg_loss = 4.00822\n",
      "epoch no.0 train no.31210  loss = 2.93513 avg_loss = 4.01110\n",
      "epoch no.0 train no.31220  loss = 5.94729 avg_loss = 3.97120\n",
      "epoch no.0 train no.31230  loss = 3.90262 avg_loss = 4.01601\n",
      "epoch no.0 train no.31240  loss = 5.17940 avg_loss = 4.00182\n",
      "epoch no.0 train no.31250  loss = 3.33873 avg_loss = 4.06689\n",
      "epoch no.0 train no.31260  loss = 2.88427 avg_loss = 3.99515\n",
      "epoch no.0 train no.31270  loss = 4.70455 avg_loss = 3.94522\n",
      "epoch no.0 train no.31280  loss = 3.59973 avg_loss = 3.99205\n",
      "epoch no.0 train no.31290  loss = 2.10207 avg_loss = 3.92730\n",
      "epoch no.0 train no.31300  loss = 3.57495 avg_loss = 3.88962\n",
      "epoch no.0 train no.31310  loss = 3.09108 avg_loss = 3.79909\n",
      "epoch no.0 train no.31320  loss = 3.65952 avg_loss = 3.83541\n",
      "epoch no.0 train no.31330  loss = 1.86154 avg_loss = 3.81538\n",
      "epoch no.0 train no.31340  loss = 2.61092 avg_loss = 3.79451\n",
      "epoch no.0 train no.31350  loss = 2.52562 avg_loss = 3.76885\n",
      "epoch no.0 train no.31360  loss = 2.17988 avg_loss = 3.74593\n",
      "epoch no.0 train no.31370  loss = 5.73512 avg_loss = 3.81182\n",
      "epoch no.0 train no.31380  loss = 8.14390 avg_loss = 3.87064\n",
      "epoch no.0 train no.31390  loss = 5.17181 avg_loss = 3.90840\n",
      "epoch no.0 train no.31400  loss = 2.91140 avg_loss = 3.88683\n",
      "epoch no.0 train no.31410  loss = 2.53638 avg_loss = 3.87275\n",
      "epoch no.0 train no.31420  loss = 2.42836 avg_loss = 3.90982\n",
      "epoch no.0 train no.31430  loss = 4.66785 avg_loss = 3.94301\n",
      "epoch no.0 train no.31440  loss = 4.19935 avg_loss = 3.91507\n",
      "epoch no.0 train no.31450  loss = 3.06424 avg_loss = 3.93482\n",
      "epoch no.0 train no.31460  loss = 2.41089 avg_loss = 3.92792\n",
      "epoch no.0 train no.31470  loss = 4.98978 avg_loss = 3.91571\n",
      "epoch no.0 train no.31480  loss = 2.89448 avg_loss = 3.90989\n",
      "epoch no.0 train no.31490  loss = 3.58606 avg_loss = 3.96783\n",
      "epoch no.0 train no.31500  loss = 5.72538 avg_loss = 4.00808\n",
      "epoch no.0 train no.31510  loss = 2.87707 avg_loss = 3.97086\n",
      "epoch no.0 train no.31520  loss = 4.70924 avg_loss = 3.99077\n",
      "epoch no.0 train no.31530  loss = 3.57474 avg_loss = 3.97465\n",
      "epoch no.0 train no.31540  loss = 4.53028 avg_loss = 4.00915\n",
      "epoch no.0 train no.31550  loss = 3.53425 avg_loss = 4.02128\n",
      "epoch no.0 train no.31560  loss = 4.05669 avg_loss = 4.01792\n",
      "epoch no.0 train no.31570  loss = 3.47159 avg_loss = 4.02813\n",
      "epoch no.0 train no.31580  loss = 1.58803 avg_loss = 4.07533\n",
      "epoch no.0 train no.31590  loss = 3.38043 avg_loss = 4.00336\n",
      "epoch no.0 train no.31600  loss = 2.76548 avg_loss = 3.94798\n",
      "epoch no.0 train no.31610  loss = 3.22988 avg_loss = 3.91715\n",
      "epoch no.0 train no.31620  loss = 2.18689 avg_loss = 3.91741\n",
      "epoch no.0 train no.31630  loss = 4.36769 avg_loss = 3.87708\n",
      "epoch no.0 train no.31640  loss = 5.17252 avg_loss = 3.84095\n",
      "epoch no.0 train no.31650  loss = 3.21370 avg_loss = 3.85717\n",
      "epoch no.0 train no.31660  loss = 4.34701 avg_loss = 3.89405\n",
      "epoch no.0 train no.31670  loss = 4.00198 avg_loss = 3.91759\n",
      "epoch no.0 train no.31680  loss = 4.70320 avg_loss = 3.93503\n",
      "epoch no.0 train no.31690  loss = 3.39418 avg_loss = 3.92006\n",
      "epoch no.0 train no.31700  loss = 3.34754 avg_loss = 3.91162\n",
      "epoch no.0 train no.31710  loss = 3.42491 avg_loss = 3.91498\n",
      "epoch no.0 train no.31720  loss = 2.61398 avg_loss = 3.88202\n",
      "epoch no.0 train no.31730  loss = 3.37412 avg_loss = 3.95315\n",
      "epoch no.0 train no.31740  loss = 3.01341 avg_loss = 3.97704\n",
      "epoch no.0 train no.31750  loss = 3.88187 avg_loss = 3.96013\n",
      "epoch no.0 train no.31760  loss = 4.46753 avg_loss = 3.93631\n",
      "epoch no.0 train no.31770  loss = 5.52128 avg_loss = 3.95488\n",
      "epoch no.0 train no.31780  loss = 4.34373 avg_loss = 3.99086\n",
      "epoch no.0 train no.31790  loss = 4.18169 avg_loss = 3.98853\n",
      "epoch no.0 train no.31800  loss = 4.94868 avg_loss = 4.00098\n",
      "epoch no.0 train no.31810  loss = 4.57630 avg_loss = 4.02915\n",
      "epoch no.0 train no.31820  loss = 2.12620 avg_loss = 3.95082\n",
      "epoch no.0 train no.31830  loss = 5.83483 avg_loss = 3.98810\n",
      "epoch no.0 train no.31840  loss = 4.40958 avg_loss = 4.03246\n",
      "epoch no.0 train no.31850  loss = 5.53631 avg_loss = 4.01329\n",
      "epoch no.0 train no.31860  loss = 4.35948 avg_loss = 4.02582\n",
      "epoch no.0 train no.31870  loss = 2.66029 avg_loss = 4.07094\n",
      "epoch no.0 train no.31880  loss = 4.74552 avg_loss = 4.05178\n",
      "epoch no.0 train no.31890  loss = 2.76238 avg_loss = 4.05343\n",
      "epoch no.0 train no.31900  loss = 5.48722 avg_loss = 4.06192\n",
      "epoch no.0 train no.31910  loss = 5.84282 avg_loss = 4.14345\n",
      "epoch no.0 train no.31920  loss = 3.01378 avg_loss = 4.12953\n",
      "epoch no.0 train no.31930  loss = 3.44768 avg_loss = 4.19399\n",
      "epoch no.0 train no.31940  loss = 5.48495 avg_loss = 4.24744\n",
      "epoch no.0 train no.31950  loss = 4.53044 avg_loss = 4.24318\n",
      "epoch no.0 train no.31960  loss = 2.99710 avg_loss = 4.22752\n",
      "epoch no.0 train no.31970  loss = 3.93478 avg_loss = 4.16177\n",
      "epoch no.0 train no.31980  loss = 4.02942 avg_loss = 4.14237\n",
      "epoch no.0 train no.31990  loss = 2.95877 avg_loss = 4.17900\n",
      "epoch no.0 train no.32000  loss = 4.59822 avg_loss = 4.13378\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁청량', '리스트', '</s>']\n",
      "여름밤에 어울리는 플레이리스트</s>\n",
      "epoch no.0 train no.32010  loss = 4.64965 avg_loss = 4.12520\n",
      "epoch no.0 train no.32020  loss = 3.34892 avg_loss = 4.14410\n",
      "epoch no.0 train no.32030  loss = 4.56764 avg_loss = 4.14620\n",
      "epoch no.0 train no.32040  loss = 3.89069 avg_loss = 4.18263\n",
      "epoch no.0 train no.32050  loss = 3.08316 avg_loss = 4.15597\n",
      "epoch no.0 train no.32060  loss = 3.09787 avg_loss = 4.08779\n",
      "epoch no.0 train no.32070  loss = 4.80029 avg_loss = 4.12633\n",
      "epoch no.0 train no.32080  loss = 3.64422 avg_loss = 4.05711\n",
      "epoch no.0 train no.32090  loss = 3.19480 avg_loss = 4.03211\n",
      "epoch no.0 train no.32100  loss = 4.59166 avg_loss = 4.03247\n",
      "epoch no.0 train no.32110  loss = 4.90374 avg_loss = 4.03712\n",
      "epoch no.0 train no.32120  loss = 3.30245 avg_loss = 4.02135\n",
      "epoch no.0 train no.32130  loss = 4.31943 avg_loss = 3.99243\n",
      "epoch no.0 train no.32140  loss = 4.32379 avg_loss = 3.99768\n",
      "epoch no.0 train no.32150  loss = 2.60967 avg_loss = 3.95758\n",
      "epoch no.0 train no.32160  loss = 3.46006 avg_loss = 3.98819\n",
      "epoch no.0 train no.32170  loss = 3.97251 avg_loss = 3.95629\n",
      "epoch no.0 train no.32180  loss = 4.28688 avg_loss = 4.03274\n",
      "epoch no.0 train no.32190  loss = 2.20259 avg_loss = 3.95265\n",
      "epoch no.0 train no.32200  loss = 5.25181 avg_loss = 3.97021\n",
      "epoch no.0 train no.32210  loss = 3.67683 avg_loss = 3.94375\n",
      "epoch no.0 train no.32220  loss = 3.43486 avg_loss = 3.94957\n",
      "epoch no.0 train no.32230  loss = 2.32679 avg_loss = 3.96502\n",
      "epoch no.0 train no.32240  loss = 4.51248 avg_loss = 3.95608\n",
      "epoch no.0 train no.32250  loss = 2.68182 avg_loss = 3.97934\n",
      "epoch no.0 train no.32260  loss = 5.60815 avg_loss = 3.96151\n",
      "epoch no.0 train no.32270  loss = 4.94644 avg_loss = 4.00206\n",
      "epoch no.0 train no.32280  loss = 4.94761 avg_loss = 3.99863\n",
      "epoch no.0 train no.32290  loss = 2.66957 avg_loss = 3.98299\n",
      "epoch no.0 train no.32300  loss = 3.33075 avg_loss = 3.99500\n",
      "epoch no.0 train no.32310  loss = 5.10168 avg_loss = 4.00342\n",
      "epoch no.0 train no.32320  loss = 3.52092 avg_loss = 3.94718\n",
      "epoch no.0 train no.32330  loss = 2.69213 avg_loss = 3.93923\n",
      "epoch no.0 train no.32340  loss = 3.58246 avg_loss = 3.94157\n",
      "epoch no.0 train no.32350  loss = 6.20255 avg_loss = 3.96349\n",
      "epoch no.0 train no.32360  loss = 2.51251 avg_loss = 3.93981\n",
      "epoch no.0 train no.32370  loss = 2.92548 avg_loss = 4.00526\n",
      "epoch no.0 train no.32380  loss = 2.95993 avg_loss = 4.02100\n",
      "epoch no.0 train no.32390  loss = 5.42315 avg_loss = 3.99462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.32400  loss = 2.26574 avg_loss = 3.95390\n",
      "epoch no.0 train no.32410  loss = 6.22990 avg_loss = 3.95366\n",
      "epoch no.0 train no.32420  loss = 2.89944 avg_loss = 3.95524\n",
      "epoch no.0 train no.32430  loss = 4.19818 avg_loss = 3.98217\n",
      "epoch no.0 train no.32440  loss = 2.98508 avg_loss = 3.94669\n",
      "epoch no.0 train no.32450  loss = 4.10944 avg_loss = 3.93999\n",
      "epoch no.0 train no.32460  loss = 3.35987 avg_loss = 3.93946\n",
      "epoch no.0 train no.32470  loss = 3.13384 avg_loss = 3.95047\n",
      "epoch no.0 train no.32480  loss = 5.29457 avg_loss = 3.96217\n",
      "epoch no.0 train no.32490  loss = 3.17880 avg_loss = 3.95700\n",
      "epoch no.0 train no.32500  loss = 3.62835 avg_loss = 3.94046\n",
      "epoch no.0 train no.32510  loss = 5.12153 avg_loss = 3.94442\n",
      "epoch no.0 train no.32520  loss = 3.64112 avg_loss = 3.95657\n",
      "epoch no.0 train no.32530  loss = 3.48734 avg_loss = 3.95302\n",
      "epoch no.0 train no.32540  loss = 5.85804 avg_loss = 4.03018\n",
      "epoch no.0 train no.32550  loss = 6.44723 avg_loss = 4.04738\n",
      "epoch no.0 train no.32560  loss = 2.78129 avg_loss = 3.97869\n",
      "epoch no.0 train no.32570  loss = 3.16986 avg_loss = 3.93195\n",
      "epoch no.0 train no.32580  loss = 6.08935 avg_loss = 3.90743\n",
      "epoch no.0 train no.32590  loss = 5.50122 avg_loss = 3.92554\n",
      "epoch no.0 train no.32600  loss = 3.45342 avg_loss = 3.94870\n",
      "epoch no.0 train no.32610  loss = 4.64811 avg_loss = 4.01582\n",
      "epoch no.0 train no.32620  loss = 3.77359 avg_loss = 4.01533\n",
      "epoch no.0 train no.32630  loss = 5.63850 avg_loss = 4.01817\n",
      "epoch no.0 train no.32640  loss = 2.02918 avg_loss = 3.97708\n",
      "epoch no.0 train no.32650  loss = 2.55860 avg_loss = 3.89025\n",
      "epoch no.0 train no.32660  loss = 3.41772 avg_loss = 3.87195\n",
      "epoch no.0 train no.32670  loss = 3.92377 avg_loss = 3.87218\n",
      "epoch no.0 train no.32680  loss = 3.35599 avg_loss = 3.91604\n",
      "epoch no.0 train no.32690  loss = 7.26260 avg_loss = 3.94094\n",
      "epoch no.0 train no.32700  loss = 3.95848 avg_loss = 3.93344\n",
      "epoch no.0 train no.32710  loss = 3.13495 avg_loss = 3.98731\n",
      "epoch no.0 train no.32720  loss = 4.81557 avg_loss = 4.02397\n",
      "epoch no.0 train no.32730  loss = 4.38455 avg_loss = 4.07929\n",
      "epoch no.0 train no.32740  loss = 2.97426 avg_loss = 4.06293\n",
      "epoch no.0 train no.32750  loss = 3.29902 avg_loss = 4.02744\n",
      "epoch no.0 train no.32760  loss = 4.12383 avg_loss = 3.99919\n",
      "epoch no.0 train no.32770  loss = 3.59348 avg_loss = 3.99312\n",
      "epoch no.0 train no.32780  loss = 4.59351 avg_loss = 3.92940\n",
      "epoch no.0 train no.32790  loss = 2.99114 avg_loss = 3.93623\n",
      "epoch no.0 train no.32800  loss = 3.16876 avg_loss = 3.91604\n",
      "epoch no.0 train no.32810  loss = 2.78219 avg_loss = 3.97557\n",
      "epoch no.0 train no.32820  loss = 4.19580 avg_loss = 3.99541\n",
      "epoch no.0 train no.32830  loss = 2.26601 avg_loss = 3.96621\n",
      "epoch no.0 train no.32840  loss = 3.25158 avg_loss = 3.95402\n",
      "epoch no.0 train no.32850  loss = 3.23752 avg_loss = 3.96511\n",
      "epoch no.0 train no.32860  loss = 3.97949 avg_loss = 3.94485\n",
      "epoch no.0 train no.32870  loss = 4.28268 avg_loss = 4.02139\n",
      "epoch no.0 train no.32880  loss = 2.59432 avg_loss = 3.99896\n",
      "epoch no.0 train no.32890  loss = 5.07969 avg_loss = 4.00280\n",
      "epoch no.0 train no.32900  loss = 3.76025 avg_loss = 3.99069\n",
      "epoch no.0 train no.32910  loss = 4.18329 avg_loss = 3.97404\n",
      "epoch no.0 train no.32920  loss = 3.21089 avg_loss = 3.95009\n",
      "epoch no.0 train no.32930  loss = 3.80748 avg_loss = 4.02044\n",
      "epoch no.0 train no.32940  loss = 2.23129 avg_loss = 4.04309\n",
      "epoch no.0 train no.32950  loss = 7.33568 avg_loss = 4.01933\n",
      "epoch no.0 train no.32960  loss = 3.35539 avg_loss = 4.01091\n",
      "epoch no.0 train no.32970  loss = 4.26104 avg_loss = 3.96277\n",
      "epoch no.0 train no.32980  loss = 4.73624 avg_loss = 4.00242\n",
      "epoch no.0 train no.32990  loss = 2.40720 avg_loss = 3.90095\n",
      "epoch no.0 train no.33000  loss = 5.26974 avg_loss = 3.95568\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '▁p', '에이지', '</s>']\n",
      "여름밤의 감성 뉴에이지</s>\n",
      "epoch no.0 train no.33010  loss = 2.10414 avg_loss = 3.93023\n",
      "epoch no.0 train no.33020  loss = 3.60771 avg_loss = 3.96261\n",
      "epoch no.0 train no.33030  loss = 2.14553 avg_loss = 3.97191\n",
      "epoch no.0 train no.33040  loss = 5.02252 avg_loss = 3.99009\n",
      "epoch no.0 train no.33050  loss = 6.69744 avg_loss = 4.01103\n",
      "epoch no.0 train no.33060  loss = 5.36466 avg_loss = 4.04459\n",
      "epoch no.0 train no.33070  loss = 4.18074 avg_loss = 4.04308\n",
      "epoch no.0 train no.33080  loss = 2.65098 avg_loss = 4.05318\n",
      "epoch no.0 train no.33090  loss = 4.17457 avg_loss = 3.98176\n",
      "epoch no.0 train no.33100  loss = 4.39770 avg_loss = 4.02735\n",
      "epoch no.0 train no.33110  loss = 4.76071 avg_loss = 4.02836\n",
      "epoch no.0 train no.33120  loss = 1.89081 avg_loss = 4.07372\n",
      "epoch no.0 train no.33130  loss = 6.25318 avg_loss = 4.07914\n",
      "epoch no.0 train no.33140  loss = 3.71882 avg_loss = 4.04494\n",
      "epoch no.0 train no.33150  loss = 2.42181 avg_loss = 4.00212\n",
      "epoch no.0 train no.33160  loss = 3.55586 avg_loss = 3.95813\n",
      "epoch no.0 train no.33170  loss = 5.71098 avg_loss = 3.99037\n",
      "epoch no.0 train no.33180  loss = 1.88968 avg_loss = 4.01915\n",
      "epoch no.0 train no.33190  loss = 3.31565 avg_loss = 4.04478\n",
      "epoch no.0 train no.33200  loss = 2.25047 avg_loss = 4.06606\n",
      "epoch no.0 train no.33210  loss = 3.20534 avg_loss = 4.06910\n",
      "epoch no.0 train no.33220  loss = 5.45295 avg_loss = 4.02034\n",
      "epoch no.0 train no.33230  loss = 6.83222 avg_loss = 4.00331\n",
      "epoch no.0 train no.33240  loss = 4.93211 avg_loss = 4.00176\n",
      "epoch no.0 train no.33250  loss = 6.65772 avg_loss = 4.00484\n",
      "epoch no.0 train no.33260  loss = 4.07266 avg_loss = 4.00692\n",
      "epoch no.0 train no.33270  loss = 3.41361 avg_loss = 3.93978\n",
      "epoch no.0 train no.33280  loss = 7.89179 avg_loss = 4.02006\n",
      "epoch no.0 train no.33290  loss = 3.45398 avg_loss = 4.01947\n",
      "epoch no.0 train no.33300  loss = 2.59509 avg_loss = 3.98182\n",
      "epoch no.0 train no.33310  loss = 3.17649 avg_loss = 3.97783\n",
      "epoch no.0 train no.33320  loss = 4.24901 avg_loss = 3.95800\n",
      "epoch no.0 train no.33330  loss = 6.64708 avg_loss = 3.97199\n",
      "epoch no.0 train no.33340  loss = 4.27832 avg_loss = 3.96590\n",
      "epoch no.0 train no.33350  loss = 5.83906 avg_loss = 4.01937\n",
      "epoch no.0 train no.33360  loss = 4.33125 avg_loss = 3.99639\n",
      "epoch no.0 train no.33370  loss = 2.71998 avg_loss = 3.95074\n",
      "epoch no.0 train no.33380  loss = 3.68320 avg_loss = 3.97806\n",
      "epoch no.0 train no.33390  loss = 3.32937 avg_loss = 3.96930\n",
      "epoch no.0 train no.33400  loss = 4.06591 avg_loss = 3.92727\n",
      "epoch no.0 train no.33410  loss = 5.43850 avg_loss = 3.94698\n",
      "epoch no.0 train no.33420  loss = 4.10136 avg_loss = 3.89896\n",
      "epoch no.0 train no.33430  loss = 6.89136 avg_loss = 3.91488\n",
      "epoch no.0 train no.33440  loss = 3.20190 avg_loss = 3.81295\n",
      "epoch no.0 train no.33450  loss = 5.86050 avg_loss = 3.84941\n",
      "epoch no.0 train no.33460  loss = 4.80992 avg_loss = 3.84889\n",
      "epoch no.0 train no.33470  loss = 7.11184 avg_loss = 3.90066\n",
      "epoch no.0 train no.33480  loss = 4.82228 avg_loss = 3.99533\n",
      "epoch no.0 train no.33490  loss = 4.45025 avg_loss = 3.98304\n",
      "epoch no.0 train no.33500  loss = 2.35000 avg_loss = 3.99724\n",
      "epoch no.0 train no.33510  loss = 2.55398 avg_loss = 3.99948\n",
      "epoch no.0 train no.33520  loss = 2.15377 avg_loss = 3.92588\n",
      "epoch no.0 train no.33530  loss = 5.14131 avg_loss = 3.93236\n",
      "epoch no.0 train no.33540  loss = 4.73862 avg_loss = 3.90479\n",
      "epoch no.0 train no.33550  loss = 2.37749 avg_loss = 3.92599\n",
      "epoch no.0 train no.33560  loss = 3.02891 avg_loss = 3.89712\n",
      "epoch no.0 train no.33570  loss = 2.71159 avg_loss = 3.86758\n",
      "epoch no.0 train no.33580  loss = 3.62069 avg_loss = 3.84865\n",
      "epoch no.0 train no.33590  loss = 4.46957 avg_loss = 3.84787\n",
      "epoch no.0 train no.33600  loss = 5.06832 avg_loss = 3.82251\n",
      "epoch no.0 train no.33610  loss = 2.31530 avg_loss = 3.87092\n",
      "epoch no.0 train no.33620  loss = 3.06794 avg_loss = 3.85053\n",
      "epoch no.0 train no.33630  loss = 5.81976 avg_loss = 3.86950\n",
      "epoch no.0 train no.33640  loss = 5.66302 avg_loss = 3.97111\n",
      "epoch no.0 train no.33650  loss = 2.93258 avg_loss = 3.99522\n",
      "epoch no.0 train no.33660  loss = 2.60081 avg_loss = 3.93617\n",
      "epoch no.0 train no.33670  loss = 3.21700 avg_loss = 3.90970\n",
      "epoch no.0 train no.33680  loss = 2.70036 avg_loss = 3.92549\n",
      "epoch no.0 train no.33690  loss = 6.53520 avg_loss = 3.93321\n",
      "epoch no.0 train no.33700  loss = 3.12364 avg_loss = 3.96718\n",
      "epoch no.0 train no.33710  loss = 2.92411 avg_loss = 4.02126\n",
      "epoch no.0 train no.33720  loss = 2.88798 avg_loss = 3.98464\n",
      "epoch no.0 train no.33730  loss = 4.68738 avg_loss = 4.03310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.33740  loss = 2.41212 avg_loss = 4.09722\n",
      "epoch no.0 train no.33750  loss = 2.91729 avg_loss = 4.09977\n",
      "epoch no.0 train no.33760  loss = 3.27462 avg_loss = 4.05339\n",
      "epoch no.0 train no.33770  loss = 4.57717 avg_loss = 4.01391\n",
      "epoch no.0 train no.33780  loss = 1.80280 avg_loss = 3.94411\n",
      "epoch no.0 train no.33790  loss = 5.11482 avg_loss = 3.94893\n",
      "epoch no.0 train no.33800  loss = 1.94033 avg_loss = 4.03384\n",
      "epoch no.0 train no.33810  loss = 3.48625 avg_loss = 4.04069\n",
      "epoch no.0 train no.33820  loss = 2.57962 avg_loss = 3.93695\n",
      "epoch no.0 train no.33830  loss = 3.33546 avg_loss = 3.91984\n",
      "epoch no.0 train no.33840  loss = 4.99964 avg_loss = 3.88943\n",
      "epoch no.0 train no.33850  loss = 2.93384 avg_loss = 3.85364\n",
      "epoch no.0 train no.33860  loss = 3.79567 avg_loss = 3.89427\n",
      "epoch no.0 train no.33870  loss = 1.98015 avg_loss = 3.86164\n",
      "epoch no.0 train no.33880  loss = 5.42904 avg_loss = 3.87639\n",
      "epoch no.0 train no.33890  loss = 2.92779 avg_loss = 3.85453\n",
      "epoch no.0 train no.33900  loss = 4.00918 avg_loss = 3.87022\n",
      "epoch no.0 train no.33910  loss = 4.77729 avg_loss = 3.84965\n",
      "epoch no.0 train no.33920  loss = 3.21893 avg_loss = 3.84072\n",
      "epoch no.0 train no.33930  loss = 5.21454 avg_loss = 3.88330\n",
      "epoch no.0 train no.33940  loss = 5.81615 avg_loss = 3.87713\n",
      "epoch no.0 train no.33950  loss = 3.01987 avg_loss = 3.83211\n",
      "epoch no.0 train no.33960  loss = 4.29951 avg_loss = 3.80550\n",
      "epoch no.0 train no.33970  loss = 4.71132 avg_loss = 3.83758\n",
      "epoch no.0 train no.33980  loss = 3.89383 avg_loss = 3.87068\n",
      "epoch no.0 train no.33990  loss = 3.63054 avg_loss = 3.91878\n",
      "epoch no.0 train no.34000  loss = 3.56391 avg_loss = 3.88278\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁노래', '음악', '</s>']\n",
      "여름밤에 어울리는 인디음악</s>\n",
      "epoch no.0 train no.34010  loss = 3.46446 avg_loss = 3.84916\n",
      "epoch no.0 train no.34020  loss = 2.69383 avg_loss = 3.87259\n",
      "epoch no.0 train no.34030  loss = 3.61143 avg_loss = 3.82327\n",
      "epoch no.0 train no.34040  loss = 5.00227 avg_loss = 3.81040\n",
      "epoch no.0 train no.34050  loss = 4.96428 avg_loss = 3.80928\n",
      "epoch no.0 train no.34060  loss = 2.67387 avg_loss = 3.81691\n",
      "epoch no.0 train no.34070  loss = 3.32224 avg_loss = 3.88134\n",
      "epoch no.0 train no.34080  loss = 6.98509 avg_loss = 3.96743\n",
      "epoch no.0 train no.34090  loss = 6.35786 avg_loss = 3.97800\n",
      "epoch no.0 train no.34100  loss = 2.98686 avg_loss = 3.99068\n",
      "epoch no.0 train no.34110  loss = 3.14933 avg_loss = 3.95926\n",
      "epoch no.0 train no.34120  loss = 3.18904 avg_loss = 3.92359\n",
      "epoch no.0 train no.34130  loss = 3.99154 avg_loss = 3.92318\n",
      "epoch no.0 train no.34140  loss = 2.99528 avg_loss = 3.92020\n",
      "epoch no.0 train no.34150  loss = 4.98036 avg_loss = 3.97998\n",
      "epoch no.0 train no.34160  loss = 2.83358 avg_loss = 3.96556\n",
      "epoch no.0 train no.34170  loss = 7.65413 avg_loss = 4.01979\n",
      "epoch no.0 train no.34180  loss = 4.45883 avg_loss = 4.03849\n",
      "epoch no.0 train no.34190  loss = 6.47521 avg_loss = 4.08456\n",
      "epoch no.0 train no.34200  loss = 4.33280 avg_loss = 4.08114\n",
      "epoch no.0 train no.34210  loss = 4.32888 avg_loss = 4.05487\n",
      "epoch no.0 train no.34220  loss = 3.22180 avg_loss = 3.98514\n",
      "epoch no.0 train no.34230  loss = 3.21218 avg_loss = 3.97268\n",
      "epoch no.0 train no.34240  loss = 4.84249 avg_loss = 3.94182\n",
      "epoch no.0 train no.34250  loss = 3.69006 avg_loss = 3.95871\n",
      "epoch no.0 train no.34260  loss = 4.44634 avg_loss = 3.92422\n",
      "epoch no.0 train no.34270  loss = 3.87898 avg_loss = 3.94253\n",
      "epoch no.0 train no.34280  loss = 3.06697 avg_loss = 3.91641\n",
      "epoch no.0 train no.34290  loss = 2.78112 avg_loss = 3.86939\n",
      "epoch no.0 train no.34300  loss = 3.70180 avg_loss = 3.85544\n",
      "epoch no.0 train no.34310  loss = 3.79969 avg_loss = 3.87261\n",
      "epoch no.0 train no.34320  loss = 3.02079 avg_loss = 3.93409\n",
      "epoch no.0 train no.34330  loss = 3.83266 avg_loss = 3.93676\n",
      "epoch no.0 train no.34340  loss = 3.01008 avg_loss = 3.98830\n",
      "epoch no.0 train no.34350  loss = 4.85059 avg_loss = 4.02500\n",
      "epoch no.0 train no.34360  loss = 1.44810 avg_loss = 4.00673\n",
      "epoch no.0 train no.34370  loss = 4.41747 avg_loss = 3.98485\n",
      "epoch no.0 train no.34380  loss = 3.01116 avg_loss = 3.95437\n",
      "epoch no.0 train no.34390  loss = 3.68739 avg_loss = 3.92612\n",
      "epoch no.0 train no.34400  loss = 4.62691 avg_loss = 3.94228\n",
      "epoch no.0 train no.34410  loss = 4.20917 avg_loss = 3.90324\n",
      "epoch no.0 train no.34420  loss = 6.10340 avg_loss = 3.91409\n",
      "epoch no.0 train no.34430  loss = 4.15707 avg_loss = 3.92183\n",
      "epoch no.0 train no.34440  loss = 5.09432 avg_loss = 3.95026\n",
      "epoch no.0 train no.34450  loss = 2.98641 avg_loss = 3.94682\n",
      "epoch no.0 train no.34460  loss = 5.25320 avg_loss = 3.91583\n",
      "epoch no.0 train no.34470  loss = 1.89116 avg_loss = 3.84884\n",
      "epoch no.0 train no.34480  loss = 4.29737 avg_loss = 3.86792\n",
      "epoch no.0 train no.34490  loss = 3.28334 avg_loss = 3.85051\n",
      "epoch no.0 train no.34500  loss = 4.62134 avg_loss = 3.83496\n",
      "epoch no.0 train no.34510  loss = 4.67722 avg_loss = 3.88787\n",
      "epoch no.0 train no.34520  loss = 5.45175 avg_loss = 3.87574\n",
      "epoch no.0 train no.34530  loss = 5.71402 avg_loss = 3.87425\n",
      "epoch no.0 train no.34540  loss = 4.67348 avg_loss = 3.90714\n",
      "epoch no.0 train no.34550  loss = 4.13789 avg_loss = 3.85331\n",
      "epoch no.0 train no.34560  loss = 3.55973 avg_loss = 3.89846\n",
      "epoch no.0 train no.34570  loss = 2.62413 avg_loss = 3.92302\n",
      "epoch no.0 train no.34580  loss = 3.10380 avg_loss = 3.90021\n",
      "epoch no.0 train no.34590  loss = 4.59417 avg_loss = 3.94735\n",
      "epoch no.0 train no.34600  loss = 5.99121 avg_loss = 3.94598\n",
      "epoch no.0 train no.34610  loss = 4.43922 avg_loss = 4.00699\n",
      "epoch no.0 train no.34620  loss = 2.47862 avg_loss = 3.98754\n",
      "epoch no.0 train no.34630  loss = 2.67752 avg_loss = 3.93643\n",
      "epoch no.0 train no.34640  loss = 3.75636 avg_loss = 3.92020\n",
      "epoch no.0 train no.34650  loss = 2.04721 avg_loss = 3.91113\n",
      "epoch no.0 train no.34660  loss = 6.44898 avg_loss = 3.94568\n",
      "epoch no.0 train no.34670  loss = 3.42257 avg_loss = 3.93786\n",
      "epoch no.0 train no.34680  loss = 3.78282 avg_loss = 3.92658\n",
      "epoch no.0 train no.34690  loss = 2.90648 avg_loss = 3.94039\n",
      "epoch no.0 train no.34700  loss = 3.94333 avg_loss = 3.91588\n",
      "epoch no.0 train no.34710  loss = 2.76766 avg_loss = 3.90838\n",
      "epoch no.0 train no.34720  loss = 5.11447 avg_loss = 3.94460\n",
      "epoch no.0 train no.34730  loss = 3.53134 avg_loss = 3.92745\n",
      "epoch no.0 train no.34740  loss = 3.63521 avg_loss = 3.91582\n",
      "epoch no.0 train no.34750  loss = 2.99640 avg_loss = 3.90677\n",
      "epoch no.0 train no.34760  loss = 3.36029 avg_loss = 3.92931\n",
      "epoch no.0 train no.34770  loss = 3.78844 avg_loss = 3.94579\n",
      "epoch no.0 train no.34780  loss = 4.10515 avg_loss = 3.92894\n",
      "epoch no.0 train no.34790  loss = 3.05190 avg_loss = 3.95441\n",
      "epoch no.0 train no.34800  loss = 3.53006 avg_loss = 3.98235\n",
      "epoch no.0 train no.34810  loss = 4.59823 avg_loss = 3.96357\n",
      "epoch no.0 train no.34820  loss = 5.92806 avg_loss = 4.02775\n",
      "epoch no.0 train no.34830  loss = 3.26352 avg_loss = 3.99208\n",
      "epoch no.0 train no.34840  loss = 5.20279 avg_loss = 4.01034\n",
      "epoch no.0 train no.34850  loss = 6.56909 avg_loss = 4.06059\n",
      "epoch no.0 train no.34860  loss = 3.72321 avg_loss = 4.00006\n",
      "epoch no.0 train no.34870  loss = 4.37855 avg_loss = 4.03387\n",
      "epoch no.0 train no.34880  loss = 5.40830 avg_loss = 4.01381\n",
      "epoch no.0 train no.34890  loss = 4.63118 avg_loss = 4.03956\n",
      "epoch no.0 train no.34900  loss = 4.97874 avg_loss = 3.99083\n",
      "epoch no.0 train no.34910  loss = 2.34180 avg_loss = 3.93141\n",
      "epoch no.0 train no.34920  loss = 3.04564 avg_loss = 3.90337\n",
      "epoch no.0 train no.34930  loss = 4.23188 avg_loss = 3.88401\n",
      "epoch no.0 train no.34940  loss = 3.05823 avg_loss = 3.81608\n",
      "epoch no.0 train no.34950  loss = 3.11962 avg_loss = 3.82367\n",
      "epoch no.0 train no.34960  loss = 3.69929 avg_loss = 3.82683\n",
      "epoch no.0 train no.34970  loss = 2.63434 avg_loss = 3.87769\n",
      "epoch no.0 train no.34980  loss = 2.67771 avg_loss = 3.87149\n",
      "epoch no.0 train no.34990  loss = 2.18332 avg_loss = 3.87198\n",
      "epoch no.0 train no.35000  loss = 5.38085 avg_loss = 3.87733\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '▁잘', '▁시원한', '▁노래', '</s>']\n",
      "여름과 어울리는 시원한 노래</s>\n",
      "epoch no.0 train no.35010  loss = 4.70845 avg_loss = 3.90770\n",
      "epoch no.0 train no.35020  loss = 2.21706 avg_loss = 3.92939\n",
      "epoch no.0 train no.35030  loss = 3.18488 avg_loss = 3.91426\n",
      "epoch no.0 train no.35040  loss = 3.69942 avg_loss = 3.93191\n",
      "epoch no.0 train no.35050  loss = 3.94423 avg_loss = 3.92645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.35060  loss = 3.27647 avg_loss = 4.00288\n",
      "epoch no.0 train no.35070  loss = 5.83833 avg_loss = 4.02202\n",
      "epoch no.0 train no.35080  loss = 3.60133 avg_loss = 4.00008\n",
      "epoch no.0 train no.35090  loss = 3.18073 avg_loss = 3.98426\n",
      "epoch no.0 train no.35100  loss = 3.66944 avg_loss = 3.98015\n",
      "epoch no.0 train no.35110  loss = 4.85394 avg_loss = 3.93477\n",
      "epoch no.0 train no.35120  loss = 3.88718 avg_loss = 3.88458\n",
      "epoch no.0 train no.35130  loss = 4.57481 avg_loss = 3.86349\n",
      "epoch no.0 train no.35140  loss = 3.20446 avg_loss = 3.78250\n",
      "epoch no.0 train no.35150  loss = 5.66624 avg_loss = 3.74800\n",
      "epoch no.0 train no.35160  loss = 4.58487 avg_loss = 3.75856\n",
      "epoch no.0 train no.35170  loss = 2.84954 avg_loss = 3.78246\n",
      "epoch no.0 train no.35180  loss = 6.20490 avg_loss = 3.80943\n",
      "epoch no.0 train no.35190  loss = 3.79449 avg_loss = 3.79023\n",
      "epoch no.0 train no.35200  loss = 2.33342 avg_loss = 3.79700\n",
      "epoch no.0 train no.35210  loss = 5.98253 avg_loss = 3.78070\n",
      "epoch no.0 train no.35220  loss = 4.99180 avg_loss = 3.86602\n",
      "epoch no.0 train no.35230  loss = 4.54564 avg_loss = 3.91361\n",
      "epoch no.0 train no.35240  loss = 4.56785 avg_loss = 3.94002\n",
      "epoch no.0 train no.35250  loss = 4.09979 avg_loss = 3.94246\n",
      "epoch no.0 train no.35260  loss = 5.39726 avg_loss = 3.94979\n",
      "epoch no.0 train no.35270  loss = 4.77868 avg_loss = 3.98814\n",
      "epoch no.0 train no.35280  loss = 6.51720 avg_loss = 3.99406\n",
      "epoch no.0 train no.35290  loss = 2.18658 avg_loss = 3.97880\n",
      "epoch no.0 train no.35300  loss = 4.70741 avg_loss = 4.01988\n",
      "epoch no.0 train no.35310  loss = 4.93984 avg_loss = 4.07132\n",
      "epoch no.0 train no.35320  loss = 4.93885 avg_loss = 4.08406\n",
      "epoch no.0 train no.35330  loss = 4.47633 avg_loss = 4.04908\n",
      "epoch no.0 train no.35340  loss = 2.15767 avg_loss = 3.98081\n",
      "epoch no.0 train no.35350  loss = 2.39538 avg_loss = 3.96749\n",
      "epoch no.0 train no.35360  loss = 3.64390 avg_loss = 3.99333\n",
      "epoch no.0 train no.35370  loss = 3.38617 avg_loss = 3.98069\n",
      "epoch no.0 train no.35380  loss = 6.63526 avg_loss = 3.94744\n",
      "epoch no.0 train no.35390  loss = 3.01696 avg_loss = 3.94022\n",
      "epoch no.0 train no.35400  loss = 2.86288 avg_loss = 3.94217\n",
      "epoch no.0 train no.35410  loss = 4.92565 avg_loss = 3.96110\n",
      "epoch no.0 train no.35420  loss = 7.18357 avg_loss = 3.98374\n",
      "epoch no.0 train no.35430  loss = 5.52345 avg_loss = 4.02621\n",
      "epoch no.0 train no.35440  loss = 4.05113 avg_loss = 4.01818\n",
      "epoch no.0 train no.35450  loss = 5.32905 avg_loss = 4.06684\n",
      "epoch no.0 train no.35460  loss = 2.63905 avg_loss = 4.05192\n",
      "epoch no.0 train no.35470  loss = 3.27674 avg_loss = 4.08154\n",
      "epoch no.0 train no.35480  loss = 2.05012 avg_loss = 4.04466\n",
      "epoch no.0 train no.35490  loss = 1.93647 avg_loss = 3.97025\n",
      "epoch no.0 train no.35500  loss = 2.65408 avg_loss = 3.97640\n",
      "epoch no.0 train no.35510  loss = 1.75383 avg_loss = 3.93129\n",
      "epoch no.0 train no.35520  loss = 3.78700 avg_loss = 3.92426\n",
      "epoch no.0 train no.35530  loss = 3.10061 avg_loss = 4.00631\n",
      "epoch no.0 train no.35540  loss = 5.25210 avg_loss = 4.03138\n",
      "epoch no.0 train no.35550  loss = 1.26072 avg_loss = 3.95020\n",
      "epoch no.0 train no.35560  loss = 4.29944 avg_loss = 3.98282\n",
      "epoch no.0 train no.35570  loss = 3.31233 avg_loss = 3.97769\n",
      "epoch no.0 train no.35580  loss = 6.32878 avg_loss = 4.02046\n",
      "epoch no.0 train no.35590  loss = 7.30589 avg_loss = 4.01125\n",
      "epoch no.0 train no.35600  loss = 3.27064 avg_loss = 4.02492\n",
      "epoch no.0 train no.35610  loss = 4.78893 avg_loss = 4.05388\n",
      "epoch no.0 train no.35620  loss = 3.92679 avg_loss = 4.04199\n",
      "epoch no.0 train no.35630  loss = 6.46556 avg_loss = 4.07743\n",
      "epoch no.0 train no.35640  loss = 3.01626 avg_loss = 4.10443\n",
      "epoch no.0 train no.35650  loss = 2.65269 avg_loss = 4.01077\n",
      "epoch no.0 train no.35660  loss = 3.08527 avg_loss = 3.99063\n",
      "epoch no.0 train no.35670  loss = 2.67520 avg_loss = 3.93567\n",
      "epoch no.0 train no.35680  loss = 3.66107 avg_loss = 3.88374\n",
      "epoch no.0 train no.35690  loss = 3.04953 avg_loss = 3.87462\n",
      "epoch no.0 train no.35700  loss = 5.59113 avg_loss = 3.92790\n",
      "epoch no.0 train no.35710  loss = 2.92097 avg_loss = 3.86015\n",
      "epoch no.0 train no.35720  loss = 3.96390 avg_loss = 3.93777\n",
      "epoch no.0 train no.35730  loss = 5.61499 avg_loss = 3.91631\n",
      "epoch no.0 train no.35740  loss = 4.31686 avg_loss = 3.91981\n",
      "epoch no.0 train no.35750  loss = 3.35730 avg_loss = 3.94425\n",
      "epoch no.0 train no.35760  loss = 5.33233 avg_loss = 3.97599\n",
      "epoch no.0 train no.35770  loss = 3.08444 avg_loss = 3.91710\n",
      "epoch no.0 train no.35780  loss = 4.63764 avg_loss = 3.91218\n",
      "epoch no.0 train no.35790  loss = 5.00826 avg_loss = 3.88102\n",
      "epoch no.0 train no.35800  loss = 5.33467 avg_loss = 3.94843\n",
      "epoch no.0 train no.35810  loss = 3.14719 avg_loss = 3.95086\n",
      "epoch no.0 train no.35820  loss = 2.48868 avg_loss = 3.98887\n",
      "epoch no.0 train no.35830  loss = 2.68555 avg_loss = 3.94084\n",
      "epoch no.0 train no.35840  loss = 2.55915 avg_loss = 3.86276\n",
      "epoch no.0 train no.35850  loss = 2.53551 avg_loss = 3.78896\n",
      "epoch no.0 train no.35860  loss = 4.49696 avg_loss = 3.84571\n",
      "epoch no.0 train no.35870  loss = 4.67232 avg_loss = 3.83755\n",
      "epoch no.0 train no.35880  loss = 4.37915 avg_loss = 3.85266\n",
      "epoch no.0 train no.35890  loss = 4.08037 avg_loss = 3.91629\n",
      "epoch no.0 train no.35900  loss = 4.05610 avg_loss = 3.95849\n",
      "epoch no.0 train no.35910  loss = 2.88099 avg_loss = 3.94523\n",
      "epoch no.0 train no.35920  loss = 6.37823 avg_loss = 4.00862\n",
      "epoch no.0 train no.35930  loss = 3.77104 avg_loss = 3.98770\n",
      "epoch no.0 train no.35940  loss = 2.39933 avg_loss = 3.89577\n",
      "epoch no.0 train no.35950  loss = 2.94571 avg_loss = 3.86736\n",
      "epoch no.0 train no.35960  loss = 4.16427 avg_loss = 3.84179\n",
      "epoch no.0 train no.35970  loss = 3.19601 avg_loss = 3.80594\n",
      "epoch no.0 train no.35980  loss = 3.20311 avg_loss = 3.81653\n",
      "epoch no.0 train no.35990  loss = 2.43852 avg_loss = 3.82061\n",
      "epoch no.0 train no.36000  loss = 3.64734 avg_loss = 3.84718\n",
      "3\n",
      "to_tokens: ['▁비', '밤', '▁신나는', '트', '</s>']\n",
      "여름엔 트로트</s>\n",
      "epoch no.0 train no.36010  loss = 3.81221 avg_loss = 3.86830\n",
      "epoch no.0 train no.36020  loss = 3.79587 avg_loss = 3.85069\n",
      "epoch no.0 train no.36030  loss = 5.09420 avg_loss = 3.80937\n",
      "epoch no.0 train no.36040  loss = 6.52741 avg_loss = 3.86433\n",
      "epoch no.0 train no.36050  loss = 4.55257 avg_loss = 3.92497\n",
      "epoch no.0 train no.36060  loss = 6.12838 avg_loss = 3.92907\n",
      "epoch no.0 train no.36070  loss = 3.19946 avg_loss = 3.92099\n",
      "epoch no.0 train no.36080  loss = 5.97564 avg_loss = 3.94612\n",
      "epoch no.0 train no.36090  loss = 3.83296 avg_loss = 3.92581\n",
      "epoch no.0 train no.36100  loss = 3.93103 avg_loss = 3.96774\n",
      "epoch no.0 train no.36110  loss = 7.28113 avg_loss = 4.00744\n",
      "epoch no.0 train no.36120  loss = 2.75246 avg_loss = 3.99887\n",
      "epoch no.0 train no.36130  loss = 3.22109 avg_loss = 4.02396\n",
      "epoch no.0 train no.36140  loss = 4.18054 avg_loss = 4.07266\n",
      "epoch no.0 train no.36150  loss = 4.33398 avg_loss = 4.07581\n",
      "epoch no.0 train no.36160  loss = 3.34476 avg_loss = 4.07569\n",
      "epoch no.0 train no.36170  loss = 2.95258 avg_loss = 4.01941\n",
      "epoch no.0 train no.36180  loss = 3.23751 avg_loss = 4.01074\n",
      "epoch no.0 train no.36190  loss = 4.53187 avg_loss = 4.00113\n",
      "epoch no.0 train no.36200  loss = 2.35351 avg_loss = 4.03532\n",
      "epoch no.0 train no.36210  loss = 3.46958 avg_loss = 3.98267\n",
      "epoch no.0 train no.36220  loss = 6.30819 avg_loss = 4.05369\n",
      "epoch no.0 train no.36230  loss = 2.28933 avg_loss = 3.93759\n",
      "epoch no.0 train no.36240  loss = 3.90375 avg_loss = 3.96416\n",
      "epoch no.0 train no.36250  loss = 3.06606 avg_loss = 3.92063\n",
      "epoch no.0 train no.36260  loss = 3.15186 avg_loss = 3.90719\n",
      "epoch no.0 train no.36270  loss = 3.05176 avg_loss = 3.94456\n",
      "epoch no.0 train no.36280  loss = 4.56579 avg_loss = 3.97625\n",
      "epoch no.0 train no.36290  loss = 6.60914 avg_loss = 4.04481\n",
      "epoch no.0 train no.36300  loss = 4.51920 avg_loss = 4.03164\n",
      "epoch no.0 train no.36310  loss = 4.84115 avg_loss = 4.03989\n",
      "epoch no.0 train no.36320  loss = 2.61590 avg_loss = 3.99569\n",
      "epoch no.0 train no.36330  loss = 4.47666 avg_loss = 4.01583\n",
      "epoch no.0 train no.36340  loss = 2.78127 avg_loss = 4.08416\n",
      "epoch no.0 train no.36350  loss = 3.01488 avg_loss = 4.07636\n",
      "epoch no.0 train no.36360  loss = 2.16668 avg_loss = 3.97659\n",
      "epoch no.0 train no.36370  loss = 6.38263 avg_loss = 3.98216\n",
      "epoch no.0 train no.36380  loss = 2.39153 avg_loss = 3.97950\n",
      "epoch no.0 train no.36390  loss = 4.21625 avg_loss = 3.97603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.36400  loss = 4.71934 avg_loss = 3.98861\n",
      "epoch no.0 train no.36410  loss = 3.78737 avg_loss = 3.96596\n",
      "epoch no.0 train no.36420  loss = 5.69720 avg_loss = 3.93441\n",
      "epoch no.0 train no.36430  loss = 3.42986 avg_loss = 3.88698\n",
      "epoch no.0 train no.36440  loss = 4.47734 avg_loss = 3.89631\n",
      "epoch no.0 train no.36450  loss = 3.30423 avg_loss = 3.94343\n",
      "epoch no.0 train no.36460  loss = 4.90101 avg_loss = 3.94156\n",
      "epoch no.0 train no.36470  loss = 5.97852 avg_loss = 4.01349\n",
      "epoch no.0 train no.36480  loss = 4.41266 avg_loss = 4.03818\n",
      "epoch no.0 train no.36490  loss = 3.26793 avg_loss = 3.94533\n",
      "epoch no.0 train no.36500  loss = 4.25611 avg_loss = 3.95275\n",
      "epoch no.0 train no.36510  loss = 4.22830 avg_loss = 3.92574\n",
      "epoch no.0 train no.36520  loss = 3.68034 avg_loss = 3.91319\n",
      "epoch no.0 train no.36530  loss = 4.01359 avg_loss = 3.91744\n",
      "epoch no.0 train no.36540  loss = 4.55104 avg_loss = 3.95771\n",
      "epoch no.0 train no.36550  loss = 3.50852 avg_loss = 3.88768\n",
      "epoch no.0 train no.36560  loss = 3.40304 avg_loss = 3.96913\n",
      "epoch no.0 train no.36570  loss = 2.51477 avg_loss = 3.95118\n",
      "epoch no.0 train no.36580  loss = 4.15488 avg_loss = 3.95804\n",
      "epoch no.0 train no.36590  loss = 3.57429 avg_loss = 3.89067\n",
      "epoch no.0 train no.36600  loss = 5.67910 avg_loss = 3.86740\n",
      "epoch no.0 train no.36610  loss = 3.35214 avg_loss = 3.88238\n",
      "epoch no.0 train no.36620  loss = 4.46444 avg_loss = 3.88648\n",
      "epoch no.0 train no.36630  loss = 2.29222 avg_loss = 3.85773\n",
      "epoch no.0 train no.36640  loss = 3.27861 avg_loss = 3.84301\n",
      "epoch no.0 train no.36650  loss = 3.31265 avg_loss = 3.81338\n",
      "epoch no.0 train no.36660  loss = 3.49613 avg_loss = 3.88110\n",
      "epoch no.0 train no.36670  loss = 3.15303 avg_loss = 3.85282\n",
      "epoch no.0 train no.36680  loss = 4.17828 avg_loss = 3.89642\n",
      "epoch no.0 train no.36690  loss = 3.77928 avg_loss = 3.85993\n",
      "epoch no.0 train no.36700  loss = 3.22199 avg_loss = 3.83956\n",
      "epoch no.0 train no.36710  loss = 2.56789 avg_loss = 3.83151\n",
      "epoch no.0 train no.36720  loss = 5.09244 avg_loss = 3.79900\n",
      "epoch no.0 train no.36730  loss = 5.63112 avg_loss = 3.79926\n",
      "epoch no.0 train no.36740  loss = 4.21852 avg_loss = 3.90253\n",
      "epoch no.0 train no.36750  loss = 3.23669 avg_loss = 3.87916\n",
      "epoch no.0 train no.36760  loss = 4.53167 avg_loss = 3.89121\n",
      "epoch no.0 train no.36770  loss = 2.43690 avg_loss = 3.89247\n",
      "epoch no.0 train no.36780  loss = 3.79444 avg_loss = 3.90252\n",
      "epoch no.0 train no.36790  loss = 4.79409 avg_loss = 3.86572\n",
      "epoch no.0 train no.36800  loss = 3.58721 avg_loss = 3.85783\n",
      "epoch no.0 train no.36810  loss = 4.42031 avg_loss = 3.90961\n",
      "epoch no.0 train no.36820  loss = 2.42116 avg_loss = 3.89674\n",
      "epoch no.0 train no.36830  loss = 3.63790 avg_loss = 3.86029\n",
      "epoch no.0 train no.36840  loss = 4.09895 avg_loss = 3.87958\n",
      "epoch no.0 train no.36850  loss = 6.54496 avg_loss = 3.97630\n",
      "epoch no.0 train no.36860  loss = 4.39124 avg_loss = 3.94882\n",
      "epoch no.0 train no.36870  loss = 3.09597 avg_loss = 3.99466\n",
      "epoch no.0 train no.36880  loss = 3.69104 avg_loss = 4.05263\n",
      "epoch no.0 train no.36890  loss = 7.94525 avg_loss = 4.13045\n",
      "epoch no.0 train no.36900  loss = 4.40205 avg_loss = 4.10009\n",
      "epoch no.0 train no.36910  loss = 2.14684 avg_loss = 4.10322\n",
      "epoch no.0 train no.36920  loss = 4.55746 avg_loss = 4.09624\n",
      "epoch no.0 train no.36930  loss = 3.35387 avg_loss = 4.06527\n",
      "epoch no.0 train no.36940  loss = 5.09336 avg_loss = 4.04523\n",
      "epoch no.0 train no.36950  loss = 6.43459 avg_loss = 4.07425\n",
      "epoch no.0 train no.36960  loss = 3.80570 avg_loss = 4.01986\n",
      "epoch no.0 train no.36970  loss = 5.11883 avg_loss = 4.00758\n",
      "epoch no.0 train no.36980  loss = 3.17195 avg_loss = 4.00036\n",
      "epoch no.0 train no.36990  loss = 3.36751 avg_loss = 3.97631\n",
      "epoch no.0 train no.37000  loss = 3.81614 avg_loss = 3.91243\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '드', '</s>']\n",
      "여름밤에 듣기 좋은 발라드</s>\n",
      "epoch no.0 train no.37010  loss = 5.68271 avg_loss = 3.90744\n",
      "epoch no.0 train no.37020  loss = 2.69859 avg_loss = 3.94771\n",
      "epoch no.0 train no.37030  loss = 3.37882 avg_loss = 3.95426\n",
      "epoch no.0 train no.37040  loss = 2.67365 avg_loss = 3.97753\n",
      "epoch no.0 train no.37050  loss = 3.40303 avg_loss = 4.01523\n",
      "epoch no.0 train no.37060  loss = 3.38049 avg_loss = 4.05450\n",
      "epoch no.0 train no.37070  loss = 4.78260 avg_loss = 3.98457\n",
      "epoch no.0 train no.37080  loss = 2.34134 avg_loss = 3.91762\n",
      "epoch no.0 train no.37090  loss = 4.77816 avg_loss = 3.93538\n",
      "epoch no.0 train no.37100  loss = 4.46006 avg_loss = 4.01216\n",
      "epoch no.0 train no.37110  loss = 4.02224 avg_loss = 3.95156\n",
      "epoch no.0 train no.37120  loss = 3.50355 avg_loss = 3.95670\n",
      "epoch no.0 train no.37130  loss = 3.26430 avg_loss = 3.96146\n",
      "epoch no.0 train no.37140  loss = 6.05581 avg_loss = 3.94231\n",
      "epoch no.0 train no.37150  loss = 3.83680 avg_loss = 3.91818\n",
      "epoch no.0 train no.37160  loss = 2.39179 avg_loss = 3.89668\n",
      "epoch no.0 train no.37170  loss = 6.53987 avg_loss = 3.86607\n",
      "epoch no.0 train no.37180  loss = 4.13266 avg_loss = 3.78805\n",
      "epoch no.0 train no.37190  loss = 4.51399 avg_loss = 3.78817\n",
      "epoch no.0 train no.37200  loss = 6.16572 avg_loss = 3.84990\n",
      "epoch no.0 train no.37210  loss = 2.62041 avg_loss = 3.87806\n",
      "epoch no.0 train no.37220  loss = 5.23301 avg_loss = 3.85993\n",
      "epoch no.0 train no.37230  loss = 3.01977 avg_loss = 3.77219\n",
      "epoch no.0 train no.37240  loss = 3.44708 avg_loss = 3.80294\n",
      "epoch no.0 train no.37250  loss = 5.58720 avg_loss = 3.78997\n",
      "epoch no.0 train no.37260  loss = 6.38265 avg_loss = 3.85895\n",
      "epoch no.0 train no.37270  loss = 4.94108 avg_loss = 3.87884\n",
      "epoch no.0 train no.37280  loss = 2.91712 avg_loss = 3.83617\n",
      "epoch no.0 train no.37290  loss = 4.61599 avg_loss = 3.80849\n",
      "epoch no.0 train no.37300  loss = 5.06119 avg_loss = 3.86501\n",
      "epoch no.0 train no.37310  loss = 5.24046 avg_loss = 3.88359\n",
      "epoch no.0 train no.37320  loss = 3.20111 avg_loss = 3.87631\n",
      "epoch no.0 train no.37330  loss = 3.26729 avg_loss = 3.87828\n",
      "epoch no.0 train no.37340  loss = 4.13666 avg_loss = 3.91119\n",
      "epoch no.0 train no.37350  loss = 2.72673 avg_loss = 3.91074\n",
      "epoch no.0 train no.37360  loss = 5.23361 avg_loss = 3.94130\n",
      "epoch no.0 train no.37370  loss = 5.09820 avg_loss = 3.88387\n",
      "epoch no.0 train no.37380  loss = 3.24646 avg_loss = 3.84235\n",
      "epoch no.0 train no.37390  loss = 3.61214 avg_loss = 3.87687\n",
      "epoch no.0 train no.37400  loss = 3.84586 avg_loss = 3.92619\n",
      "epoch no.0 train no.37410  loss = 3.43349 avg_loss = 3.90377\n",
      "epoch no.0 train no.37420  loss = 4.50226 avg_loss = 3.88272\n",
      "epoch no.0 train no.37430  loss = 3.75469 avg_loss = 3.86691\n",
      "epoch no.0 train no.37440  loss = 3.23883 avg_loss = 3.85710\n",
      "epoch no.0 train no.37450  loss = 2.90456 avg_loss = 3.83975\n",
      "epoch no.0 train no.37460  loss = 3.19649 avg_loss = 3.84541\n",
      "epoch no.0 train no.37470  loss = 5.21413 avg_loss = 3.87440\n",
      "epoch no.0 train no.37480  loss = 1.60994 avg_loss = 3.85346\n",
      "epoch no.0 train no.37490  loss = 6.18410 avg_loss = 3.87608\n",
      "epoch no.0 train no.37500  loss = 4.46788 avg_loss = 3.83839\n",
      "epoch no.0 train no.37510  loss = 3.10266 avg_loss = 3.85663\n",
      "epoch no.0 train no.37520  loss = 2.76226 avg_loss = 3.85589\n",
      "epoch no.0 train no.37530  loss = 3.81348 avg_loss = 3.89970\n",
      "epoch no.0 train no.37540  loss = 3.14580 avg_loss = 3.87675\n",
      "epoch no.0 train no.37550  loss = 3.84446 avg_loss = 3.88455\n",
      "epoch no.0 train no.37560  loss = 2.19162 avg_loss = 3.88598\n",
      "epoch no.0 train no.37570  loss = 1.76284 avg_loss = 3.86945\n",
      "epoch no.0 train no.37580  loss = 4.15559 avg_loss = 3.90454\n",
      "epoch no.0 train no.37590  loss = 3.41484 avg_loss = 3.90775\n",
      "epoch no.0 train no.37600  loss = 3.23901 avg_loss = 3.92960\n",
      "epoch no.0 train no.37610  loss = 3.33394 avg_loss = 3.95300\n",
      "epoch no.0 train no.37620  loss = 3.00292 avg_loss = 3.98301\n",
      "epoch no.0 train no.37630  loss = 4.92468 avg_loss = 3.98480\n",
      "epoch no.0 train no.37640  loss = 6.50429 avg_loss = 3.97004\n",
      "epoch no.0 train no.37650  loss = 5.73296 avg_loss = 3.99144\n",
      "epoch no.0 train no.37660  loss = 4.78727 avg_loss = 3.94208\n",
      "epoch no.0 train no.37670  loss = 1.82037 avg_loss = 3.91595\n",
      "epoch no.0 train no.37680  loss = 1.82849 avg_loss = 3.83874\n",
      "epoch no.0 train no.37690  loss = 4.35080 avg_loss = 3.85787\n",
      "epoch no.0 train no.37700  loss = 5.89026 avg_loss = 3.88623\n",
      "epoch no.0 train no.37710  loss = 2.95868 avg_loss = 3.87140\n",
      "epoch no.0 train no.37720  loss = 3.21962 avg_loss = 3.90209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.37730  loss = 4.79281 avg_loss = 3.94032\n",
      "epoch no.0 train no.37740  loss = 2.97273 avg_loss = 3.97974\n",
      "epoch no.0 train no.37750  loss = 2.99986 avg_loss = 3.94676\n",
      "epoch no.0 train no.37760  loss = 5.30412 avg_loss = 3.96735\n",
      "epoch no.0 train no.37770  loss = 1.80053 avg_loss = 3.91307\n",
      "epoch no.0 train no.37780  loss = 5.87008 avg_loss = 3.92804\n",
      "epoch no.0 train no.37790  loss = 5.79636 avg_loss = 3.88619\n",
      "epoch no.0 train no.37800  loss = 7.32272 avg_loss = 3.97925\n",
      "epoch no.0 train no.37810  loss = 4.52271 avg_loss = 3.91997\n",
      "epoch no.0 train no.37820  loss = 7.34861 avg_loss = 4.04362\n",
      "epoch no.0 train no.37830  loss = 4.33695 avg_loss = 4.10649\n",
      "epoch no.0 train no.37840  loss = 2.42838 avg_loss = 4.07994\n",
      "epoch no.0 train no.37850  loss = 4.95058 avg_loss = 4.05641\n",
      "epoch no.0 train no.37860  loss = 4.75567 avg_loss = 4.06795\n",
      "epoch no.0 train no.37870  loss = 3.54879 avg_loss = 4.07143\n",
      "epoch no.0 train no.37880  loss = 3.41797 avg_loss = 4.04151\n",
      "epoch no.0 train no.37890  loss = 4.47972 avg_loss = 4.09104\n",
      "epoch no.0 train no.37900  loss = 3.35140 avg_loss = 4.06290\n",
      "epoch no.0 train no.37910  loss = 2.44427 avg_loss = 4.09723\n",
      "epoch no.0 train no.37920  loss = 3.05621 avg_loss = 4.06266\n",
      "epoch no.0 train no.37930  loss = 2.83981 avg_loss = 4.01023\n",
      "epoch no.0 train no.37940  loss = 3.08991 avg_loss = 3.98433\n",
      "epoch no.0 train no.37950  loss = 5.18568 avg_loss = 4.02001\n",
      "epoch no.0 train no.37960  loss = 3.79573 avg_loss = 3.99527\n",
      "epoch no.0 train no.37970  loss = 2.67113 avg_loss = 3.94737\n",
      "epoch no.0 train no.37980  loss = 3.43958 avg_loss = 3.95444\n",
      "epoch no.0 train no.37990  loss = 4.00878 avg_loss = 3.97195\n",
      "epoch no.0 train no.38000  loss = 4.93470 avg_loss = 3.94828\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '함을', '</s>']\n",
      "여름밤의 시원함</s>\n",
      "epoch no.0 train no.38010  loss = 3.62955 avg_loss = 3.90121\n",
      "epoch no.0 train no.38020  loss = 4.05304 avg_loss = 3.89051\n",
      "epoch no.0 train no.38030  loss = 5.27851 avg_loss = 3.95766\n",
      "epoch no.0 train no.38040  loss = 4.70951 avg_loss = 3.94278\n",
      "epoch no.0 train no.38050  loss = 5.17852 avg_loss = 3.88196\n",
      "epoch no.0 train no.38060  loss = 3.00578 avg_loss = 3.83398\n",
      "epoch no.0 train no.38070  loss = 4.78974 avg_loss = 3.82783\n",
      "epoch no.0 train no.38080  loss = 4.72451 avg_loss = 3.90328\n",
      "epoch no.0 train no.38090  loss = 4.24661 avg_loss = 3.86653\n",
      "epoch no.0 train no.38100  loss = 3.97003 avg_loss = 3.96672\n",
      "epoch no.0 train no.38110  loss = 4.16594 avg_loss = 3.98028\n",
      "epoch no.0 train no.38120  loss = 3.81370 avg_loss = 3.92160\n",
      "epoch no.0 train no.38130  loss = 4.34300 avg_loss = 3.94893\n",
      "epoch no.0 train no.38140  loss = 3.76461 avg_loss = 3.94398\n",
      "epoch no.0 train no.38150  loss = 3.95180 avg_loss = 3.97901\n",
      "epoch no.0 train no.38160  loss = 3.44038 avg_loss = 3.95064\n",
      "epoch no.0 train no.38170  loss = 2.46079 avg_loss = 3.91398\n",
      "epoch no.0 train no.38180  loss = 4.14670 avg_loss = 3.94539\n",
      "epoch no.0 train no.38190  loss = 2.77483 avg_loss = 3.93986\n",
      "epoch no.0 train no.38200  loss = 4.67092 avg_loss = 3.88552\n",
      "epoch no.0 train no.38210  loss = 2.14866 avg_loss = 3.88337\n",
      "epoch no.0 train no.38220  loss = 4.04392 avg_loss = 3.87791\n",
      "epoch no.0 train no.38230  loss = 3.48561 avg_loss = 3.86423\n",
      "epoch no.0 train no.38240  loss = 5.25161 avg_loss = 3.85256\n",
      "epoch no.0 train no.38250  loss = 5.08920 avg_loss = 3.90170\n",
      "epoch no.0 train no.38260  loss = 3.58059 avg_loss = 3.84431\n",
      "epoch no.0 train no.38270  loss = 4.32284 avg_loss = 3.83553\n",
      "epoch no.0 train no.38280  loss = 4.72238 avg_loss = 3.90335\n",
      "epoch no.0 train no.38290  loss = 4.38026 avg_loss = 3.90597\n",
      "epoch no.0 train no.38300  loss = 4.98448 avg_loss = 3.91118\n",
      "epoch no.0 train no.38310  loss = 3.43379 avg_loss = 3.87807\n",
      "epoch no.0 train no.38320  loss = 2.35222 avg_loss = 3.84391\n",
      "epoch no.0 train no.38330  loss = 1.69074 avg_loss = 3.81292\n",
      "epoch no.0 train no.38340  loss = 3.06831 avg_loss = 3.78238\n",
      "epoch no.0 train no.38350  loss = 5.09494 avg_loss = 3.76430\n",
      "epoch no.0 train no.38360  loss = 4.00189 avg_loss = 3.79988\n",
      "epoch no.0 train no.38370  loss = 4.50194 avg_loss = 3.78910\n",
      "epoch no.0 train no.38380  loss = 4.71090 avg_loss = 3.79523\n",
      "epoch no.0 train no.38390  loss = 5.29795 avg_loss = 3.82264\n",
      "epoch no.0 train no.38400  loss = 3.27322 avg_loss = 3.82315\n",
      "epoch no.0 train no.38410  loss = 4.45571 avg_loss = 3.83537\n",
      "epoch no.0 train no.38420  loss = 3.45166 avg_loss = 3.79497\n",
      "epoch no.0 train no.38430  loss = 2.07693 avg_loss = 3.76573\n",
      "epoch no.0 train no.38440  loss = 3.71227 avg_loss = 3.74366\n",
      "epoch no.0 train no.38450  loss = 3.32918 avg_loss = 3.72866\n",
      "epoch no.0 train no.38460  loss = 3.87440 avg_loss = 3.77756\n",
      "epoch no.0 train no.38470  loss = 4.30597 avg_loss = 3.78096\n",
      "epoch no.0 train no.38480  loss = 2.95164 avg_loss = 3.79416\n",
      "epoch no.0 train no.38490  loss = 2.20612 avg_loss = 3.77046\n",
      "epoch no.0 train no.38500  loss = 2.95805 avg_loss = 3.83201\n",
      "epoch no.0 train no.38510  loss = 3.60734 avg_loss = 3.79807\n",
      "epoch no.0 train no.38520  loss = 1.80347 avg_loss = 3.74835\n",
      "epoch no.0 train no.38530  loss = 2.49309 avg_loss = 3.72418\n",
      "epoch no.0 train no.38540  loss = 5.79396 avg_loss = 3.75270\n",
      "epoch no.0 train no.38550  loss = 3.49208 avg_loss = 3.73498\n",
      "epoch no.0 train no.38560  loss = 3.11422 avg_loss = 3.78090\n",
      "epoch no.0 train no.38570  loss = 4.39418 avg_loss = 3.79609\n",
      "epoch no.0 train no.38580  loss = 3.41314 avg_loss = 3.81327\n",
      "epoch no.0 train no.38590  loss = 3.81839 avg_loss = 3.79925\n",
      "epoch no.0 train no.38600  loss = 6.12839 avg_loss = 3.84800\n",
      "epoch no.0 train no.38610  loss = 4.80813 avg_loss = 3.90420\n",
      "epoch no.0 train no.38620  loss = 3.40119 avg_loss = 3.92530\n",
      "epoch no.0 train no.38630  loss = 5.63937 avg_loss = 3.92281\n",
      "epoch no.0 train no.38640  loss = 4.18084 avg_loss = 3.89874\n",
      "epoch no.0 train no.38650  loss = 5.74517 avg_loss = 3.91725\n",
      "epoch no.0 train no.38660  loss = 5.20150 avg_loss = 3.93007\n",
      "epoch no.0 train no.38670  loss = 1.55733 avg_loss = 3.91607\n",
      "epoch no.0 train no.38680  loss = 2.16489 avg_loss = 3.85268\n",
      "epoch no.0 train no.38690  loss = 3.94702 avg_loss = 3.83311\n",
      "epoch no.0 train no.38700  loss = 2.87989 avg_loss = 3.81876\n",
      "epoch no.0 train no.38710  loss = 6.37080 avg_loss = 3.86945\n",
      "epoch no.0 train no.38720  loss = 2.71412 avg_loss = 3.82075\n",
      "epoch no.0 train no.38730  loss = 6.02165 avg_loss = 3.82638\n",
      "epoch no.0 train no.38740  loss = 3.59439 avg_loss = 3.88331\n",
      "epoch no.0 train no.38750  loss = 3.42949 avg_loss = 3.79807\n",
      "epoch no.0 train no.38760  loss = 3.27785 avg_loss = 3.79130\n",
      "epoch no.0 train no.38770  loss = 3.97364 avg_loss = 3.80218\n",
      "epoch no.0 train no.38780  loss = 4.85314 avg_loss = 3.82294\n",
      "epoch no.0 train no.38790  loss = 4.11239 avg_loss = 3.80989\n",
      "epoch no.0 train no.38800  loss = 6.09623 avg_loss = 3.89979\n",
      "epoch no.0 train no.38810  loss = 3.16477 avg_loss = 3.93507\n",
      "epoch no.0 train no.38820  loss = 3.37411 avg_loss = 3.95879\n",
      "epoch no.0 train no.38830  loss = 4.18219 avg_loss = 3.87737\n",
      "epoch no.0 train no.38840  loss = 2.63440 avg_loss = 3.87235\n",
      "epoch no.0 train no.38850  loss = 3.96421 avg_loss = 3.87856\n",
      "epoch no.0 train no.38860  loss = 4.66437 avg_loss = 3.87424\n",
      "epoch no.0 train no.38870  loss = 3.79724 avg_loss = 3.90949\n",
      "epoch no.0 train no.38880  loss = 2.65507 avg_loss = 3.90105\n",
      "epoch no.0 train no.38890  loss = 2.51862 avg_loss = 3.88260\n",
      "epoch no.0 train no.38900  loss = 4.98542 avg_loss = 3.91903\n",
      "epoch no.0 train no.38910  loss = 3.12888 avg_loss = 3.93505\n",
      "epoch no.0 train no.38920  loss = 3.50542 avg_loss = 3.94863\n",
      "epoch no.0 train no.38930  loss = 3.66019 avg_loss = 3.95702\n",
      "epoch no.0 train no.38940  loss = 4.19931 avg_loss = 3.90468\n",
      "epoch no.0 train no.38950  loss = 4.25399 avg_loss = 3.91523\n",
      "epoch no.0 train no.38960  loss = 2.58452 avg_loss = 3.89688\n",
      "epoch no.0 train no.38970  loss = 6.78563 avg_loss = 3.94935\n",
      "epoch no.0 train no.38980  loss = 4.98532 avg_loss = 3.94067\n",
      "epoch no.0 train no.38990  loss = 4.58330 avg_loss = 3.96604\n",
      "epoch no.0 train no.39000  loss = 3.75610 avg_loss = 4.00822\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁어울리는', '▁좋은', '▁노래', '적인', '드', '</s>']\n",
      "여름밤에 듣기 좋은 감성 발라드</s>\n",
      "epoch no.0 train no.39010  loss = 2.95778 avg_loss = 4.01510\n",
      "epoch no.0 train no.39020  loss = 3.30044 avg_loss = 3.96439\n",
      "epoch no.0 train no.39030  loss = 1.71429 avg_loss = 3.91573\n",
      "epoch no.0 train no.39040  loss = 4.47684 avg_loss = 3.95632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.39050  loss = 4.43916 avg_loss = 3.98941\n",
      "epoch no.0 train no.39060  loss = 4.23220 avg_loss = 3.98667\n",
      "epoch no.0 train no.39070  loss = 3.87299 avg_loss = 4.06921\n",
      "epoch no.0 train no.39080  loss = 2.19399 avg_loss = 4.09026\n",
      "epoch no.0 train no.39090  loss = 3.67903 avg_loss = 4.04102\n",
      "epoch no.0 train no.39100  loss = 7.28135 avg_loss = 4.07363\n",
      "epoch no.0 train no.39110  loss = 4.00353 avg_loss = 4.02291\n",
      "epoch no.0 train no.39120  loss = 3.34892 avg_loss = 4.01932\n",
      "epoch no.0 train no.39130  loss = 4.59990 avg_loss = 4.02990\n",
      "epoch no.0 train no.39140  loss = 2.77424 avg_loss = 4.06189\n",
      "epoch no.0 train no.39150  loss = 5.94571 avg_loss = 4.06656\n",
      "epoch no.0 train no.39160  loss = 4.13354 avg_loss = 4.11919\n",
      "epoch no.0 train no.39170  loss = 5.39040 avg_loss = 4.11477\n",
      "epoch no.0 train no.39180  loss = 6.82591 avg_loss = 4.15094\n",
      "epoch no.0 train no.39190  loss = 4.58957 avg_loss = 4.21478\n",
      "epoch no.0 train no.39200  loss = 3.66567 avg_loss = 4.17709\n",
      "epoch no.0 train no.39210  loss = 2.72758 avg_loss = 4.12872\n",
      "epoch no.0 train no.39220  loss = 5.56373 avg_loss = 4.08207\n",
      "epoch no.0 train no.39230  loss = 2.58378 avg_loss = 4.00070\n",
      "epoch no.0 train no.39240  loss = 4.30015 avg_loss = 4.00498\n",
      "epoch no.0 train no.39250  loss = 2.64328 avg_loss = 4.00574\n",
      "epoch no.0 train no.39260  loss = 2.84236 avg_loss = 4.03221\n",
      "epoch no.0 train no.39270  loss = 4.52549 avg_loss = 4.01898\n",
      "epoch no.0 train no.39280  loss = 5.57825 avg_loss = 4.02645\n",
      "epoch no.0 train no.39290  loss = 3.30323 avg_loss = 3.97404\n",
      "epoch no.0 train no.39300  loss = 2.91563 avg_loss = 3.91436\n",
      "epoch no.0 train no.39310  loss = 2.65522 avg_loss = 3.87842\n",
      "epoch no.0 train no.39320  loss = 3.82794 avg_loss = 3.87382\n",
      "epoch no.0 train no.39330  loss = 4.15325 avg_loss = 3.87318\n",
      "epoch no.0 train no.39340  loss = 4.64219 avg_loss = 3.86802\n",
      "epoch no.0 train no.39350  loss = 3.93349 avg_loss = 3.87493\n",
      "epoch no.0 train no.39360  loss = 3.08308 avg_loss = 3.90301\n",
      "epoch no.0 train no.39370  loss = 3.04608 avg_loss = 3.88413\n",
      "epoch no.0 train no.39380  loss = 2.66951 avg_loss = 3.88290\n",
      "epoch no.0 train no.39390  loss = 2.37947 avg_loss = 3.87984\n",
      "epoch no.0 train no.39400  loss = 4.64271 avg_loss = 3.83840\n",
      "epoch no.0 train no.39410  loss = 5.74065 avg_loss = 3.84861\n",
      "epoch no.0 train no.39420  loss = 5.20464 avg_loss = 3.85767\n",
      "epoch no.0 train no.39430  loss = 3.52209 avg_loss = 3.85770\n",
      "epoch no.0 train no.39440  loss = 3.09687 avg_loss = 3.87460\n",
      "epoch no.0 train no.39450  loss = 3.64501 avg_loss = 3.88246\n",
      "epoch no.0 train no.39460  loss = 2.96304 avg_loss = 3.85063\n",
      "epoch no.0 train no.39470  loss = 7.65184 avg_loss = 3.99322\n",
      "epoch no.0 train no.39480  loss = 3.33240 avg_loss = 3.96568\n",
      "epoch no.0 train no.39490  loss = 3.82658 avg_loss = 3.94478\n",
      "epoch no.0 train no.39500  loss = 2.74654 avg_loss = 3.96690\n",
      "epoch no.0 train no.39510  loss = 3.70665 avg_loss = 3.96181\n",
      "epoch no.0 train no.39520  loss = 2.65240 avg_loss = 3.96108\n",
      "epoch no.0 train no.39530  loss = 5.13736 avg_loss = 4.07886\n",
      "epoch no.0 train no.39540  loss = 3.75442 avg_loss = 4.05989\n",
      "epoch no.0 train no.39550  loss = 2.66136 avg_loss = 4.02312\n",
      "epoch no.0 train no.39560  loss = 2.91553 avg_loss = 3.97635\n",
      "epoch no.0 train no.39570  loss = 3.96208 avg_loss = 4.01192\n",
      "epoch no.0 train no.39580  loss = 3.06031 avg_loss = 3.99292\n",
      "epoch no.0 train no.39590  loss = 3.62645 avg_loss = 3.96253\n",
      "epoch no.0 train no.39600  loss = 2.92250 avg_loss = 3.89971\n",
      "epoch no.0 train no.39610  loss = 2.96343 avg_loss = 3.92169\n",
      "epoch no.0 train no.39620  loss = 4.50686 avg_loss = 3.95394\n",
      "epoch no.0 train no.39630  loss = 2.84215 avg_loss = 3.94660\n",
      "epoch no.0 train no.39640  loss = 5.13284 avg_loss = 3.93802\n",
      "epoch no.0 train no.39650  loss = 2.87157 avg_loss = 3.86720\n",
      "epoch no.0 train no.39660  loss = 2.58801 avg_loss = 3.81586\n",
      "epoch no.0 train no.39670  loss = 2.58346 avg_loss = 3.79307\n",
      "epoch no.0 train no.39680  loss = 2.61349 avg_loss = 3.72569\n",
      "epoch no.0 train no.39690  loss = 2.66235 avg_loss = 3.70028\n",
      "epoch no.0 train no.39700  loss = 4.92929 avg_loss = 3.74540\n",
      "epoch no.0 train no.39710  loss = 3.34997 avg_loss = 3.78439\n",
      "epoch no.0 train no.39720  loss = 3.19348 avg_loss = 3.82828\n",
      "epoch no.0 train no.39730  loss = 2.90072 avg_loss = 3.82871\n",
      "epoch no.0 train no.39740  loss = 4.33506 avg_loss = 3.84224\n",
      "epoch no.0 train no.39750  loss = 3.91678 avg_loss = 3.82705\n",
      "epoch no.0 train no.39760  loss = 5.07501 avg_loss = 3.83745\n",
      "epoch no.0 train no.39770  loss = 6.52846 avg_loss = 3.84198\n",
      "epoch no.0 train no.39780  loss = 5.45946 avg_loss = 3.89885\n",
      "epoch no.0 train no.39790  loss = 4.92802 avg_loss = 3.97750\n",
      "epoch no.0 train no.39800  loss = 3.75526 avg_loss = 4.03650\n",
      "epoch no.0 train no.39810  loss = 4.52805 avg_loss = 4.06818\n",
      "epoch no.0 train no.39820  loss = 3.32729 avg_loss = 4.01864\n",
      "epoch no.0 train no.39830  loss = 3.60769 avg_loss = 3.96344\n",
      "epoch no.0 train no.39840  loss = 5.02487 avg_loss = 3.95790\n",
      "epoch no.0 train no.39850  loss = 3.58607 avg_loss = 3.86985\n",
      "epoch no.0 train no.39860  loss = 2.52381 avg_loss = 3.82225\n",
      "epoch no.0 train no.39870  loss = 2.91599 avg_loss = 3.91598\n",
      "epoch no.0 train no.39880  loss = 4.52564 avg_loss = 3.92938\n",
      "epoch no.0 train no.39890  loss = 2.33318 avg_loss = 3.83799\n",
      "epoch no.0 train no.39900  loss = 3.76913 avg_loss = 3.78878\n",
      "epoch no.0 train no.39910  loss = 3.48478 avg_loss = 3.78964\n",
      "epoch no.0 train no.39920  loss = 3.81531 avg_loss = 3.81446\n",
      "epoch no.0 train no.39930  loss = 5.51100 avg_loss = 3.83640\n",
      "epoch no.0 train no.39940  loss = 6.46509 avg_loss = 3.85246\n",
      "epoch no.0 train no.39950  loss = 3.86047 avg_loss = 3.86935\n",
      "epoch no.0 train no.39960  loss = 3.27431 avg_loss = 3.90297\n",
      "epoch no.0 train no.39970  loss = 5.18501 avg_loss = 3.89289\n",
      "epoch no.0 train no.39980  loss = 5.75466 avg_loss = 3.90591\n",
      "epoch no.0 train no.39990  loss = 4.10969 avg_loss = 3.88462\n",
      "epoch no.0 train no.40000  loss = 3.11902 avg_loss = 3.87792\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '함을', '▁담은', '▁수', '▁있는', '▁음악', '</s>']\n",
      "여름밤의 시원함을 느낄 수 있는 음악</s>\n",
      "epoch no.0 train no.40010  loss = 3.99176 avg_loss = 3.83866\n",
      "epoch no.0 train no.40020  loss = 2.65985 avg_loss = 3.81393\n",
      "epoch no.0 train no.40030  loss = 2.36598 avg_loss = 3.78568\n",
      "epoch no.0 train no.40040  loss = 2.43010 avg_loss = 3.76257\n",
      "epoch no.0 train no.40050  loss = 3.88563 avg_loss = 3.76521\n",
      "epoch no.0 train no.40060  loss = 3.46231 avg_loss = 3.74887\n",
      "epoch no.0 train no.40070  loss = 2.98221 avg_loss = 3.79726\n",
      "epoch no.0 train no.40080  loss = 3.64337 avg_loss = 3.85369\n",
      "epoch no.0 train no.40090  loss = 3.06521 avg_loss = 3.84852\n",
      "epoch no.0 train no.40100  loss = 3.53614 avg_loss = 3.81719\n",
      "epoch no.0 train no.40110  loss = 3.64790 avg_loss = 3.84197\n",
      "epoch no.0 train no.40120  loss = 4.87073 avg_loss = 3.87328\n",
      "epoch no.0 train no.40130  loss = 3.87002 avg_loss = 3.88656\n",
      "epoch no.0 train no.40140  loss = 4.36586 avg_loss = 3.90591\n",
      "epoch no.0 train no.40150  loss = 3.01375 avg_loss = 3.91377\n",
      "epoch no.0 train no.40160  loss = 4.88022 avg_loss = 3.91365\n",
      "epoch no.0 train no.40170  loss = 3.29175 avg_loss = 3.87411\n",
      "epoch no.0 train no.40180  loss = 2.56432 avg_loss = 3.83888\n",
      "epoch no.0 train no.40190  loss = 3.63317 avg_loss = 3.86765\n",
      "epoch no.0 train no.40200  loss = 3.97106 avg_loss = 3.92090\n",
      "epoch no.0 train no.40210  loss = 3.08941 avg_loss = 3.87550\n",
      "epoch no.0 train no.40220  loss = 4.39803 avg_loss = 3.88459\n",
      "epoch no.0 train no.40230  loss = 3.41447 avg_loss = 3.85730\n",
      "epoch no.0 train no.40240  loss = 4.53429 avg_loss = 3.90077\n",
      "epoch no.0 train no.40250  loss = 2.63834 avg_loss = 3.86742\n",
      "epoch no.0 train no.40260  loss = 4.92179 avg_loss = 3.89234\n",
      "epoch no.0 train no.40270  loss = 5.01730 avg_loss = 3.92007\n",
      "epoch no.0 train no.40280  loss = 3.12030 avg_loss = 3.89585\n",
      "epoch no.0 train no.40290  loss = 7.02714 avg_loss = 3.88304\n",
      "epoch no.0 train no.40300  loss = 6.14715 avg_loss = 3.90478\n",
      "epoch no.0 train no.40310  loss = 4.41395 avg_loss = 3.83717\n",
      "epoch no.0 train no.40320  loss = 3.12115 avg_loss = 3.85330\n",
      "epoch no.0 train no.40330  loss = 4.16346 avg_loss = 3.90449\n",
      "epoch no.0 train no.40340  loss = 4.24476 avg_loss = 3.84502\n",
      "epoch no.0 train no.40350  loss = 7.60880 avg_loss = 3.86082\n",
      "epoch no.0 train no.40360  loss = 4.52016 avg_loss = 3.86259\n",
      "epoch no.0 train no.40370  loss = 3.78676 avg_loss = 3.84175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.40380  loss = 3.47283 avg_loss = 3.87512\n",
      "epoch no.0 train no.40390  loss = 4.77869 avg_loss = 3.80698\n",
      "epoch no.0 train no.40400  loss = 2.83862 avg_loss = 3.79436\n",
      "epoch no.0 train no.40410  loss = 5.06203 avg_loss = 3.81406\n",
      "epoch no.0 train no.40420  loss = 2.55582 avg_loss = 3.83371\n",
      "epoch no.0 train no.40430  loss = 3.67482 avg_loss = 3.84488\n",
      "epoch no.0 train no.40440  loss = 3.50098 avg_loss = 3.84605\n",
      "epoch no.0 train no.40450  loss = 4.77978 avg_loss = 3.87643\n",
      "epoch no.0 train no.40460  loss = 3.32425 avg_loss = 3.87034\n",
      "epoch no.0 train no.40470  loss = 5.24657 avg_loss = 3.84731\n",
      "epoch no.0 train no.40480  loss = 4.94468 avg_loss = 3.82660\n",
      "epoch no.0 train no.40490  loss = 1.20214 avg_loss = 3.81936\n",
      "epoch no.0 train no.40500  loss = 2.68540 avg_loss = 3.84787\n",
      "epoch no.0 train no.40510  loss = 3.89389 avg_loss = 3.90539\n",
      "epoch no.0 train no.40520  loss = 4.04808 avg_loss = 3.91718\n",
      "epoch no.0 train no.40530  loss = 3.21831 avg_loss = 3.87027\n",
      "epoch no.0 train no.40540  loss = 4.62613 avg_loss = 3.93292\n",
      "epoch no.0 train no.40550  loss = 2.55657 avg_loss = 3.96657\n",
      "epoch no.0 train no.40560  loss = 4.34490 avg_loss = 4.03370\n",
      "epoch no.0 train no.40570  loss = 2.48754 avg_loss = 4.01502\n",
      "epoch no.0 train no.40580  loss = 4.33954 avg_loss = 3.98961\n",
      "epoch no.0 train no.40590  loss = 3.15672 avg_loss = 4.00623\n",
      "epoch no.0 train no.40600  loss = 3.82845 avg_loss = 3.99414\n",
      "epoch no.0 train no.40610  loss = 3.82733 avg_loss = 3.94137\n",
      "epoch no.0 train no.40620  loss = 4.92639 avg_loss = 4.00202\n",
      "epoch no.0 train no.40630  loss = 3.22372 avg_loss = 3.96912\n",
      "epoch no.0 train no.40640  loss = 2.95131 avg_loss = 3.97469\n",
      "epoch no.0 train no.40650  loss = 5.16582 avg_loss = 3.98948\n",
      "epoch no.0 train no.40660  loss = 2.59287 avg_loss = 3.92501\n",
      "epoch no.0 train no.40670  loss = 3.03891 avg_loss = 3.94802\n",
      "epoch no.0 train no.40680  loss = 4.55728 avg_loss = 3.95341\n",
      "epoch no.0 train no.40690  loss = 3.86777 avg_loss = 3.94316\n",
      "epoch no.0 train no.40700  loss = 6.46002 avg_loss = 3.95348\n",
      "epoch no.0 train no.40710  loss = 4.85284 avg_loss = 3.97325\n",
      "epoch no.0 train no.40720  loss = 2.62318 avg_loss = 3.87827\n",
      "epoch no.0 train no.40730  loss = 3.96954 avg_loss = 3.89229\n",
      "epoch no.0 train no.40740  loss = 3.71018 avg_loss = 3.89171\n",
      "epoch no.0 train no.40750  loss = 6.53479 avg_loss = 3.86523\n",
      "epoch no.0 train no.40760  loss = 2.10152 avg_loss = 3.84856\n",
      "epoch no.0 train no.40770  loss = 2.79817 avg_loss = 3.80255\n",
      "epoch no.0 train no.40780  loss = 3.35548 avg_loss = 3.82543\n",
      "epoch no.0 train no.40790  loss = 4.71146 avg_loss = 3.81910\n",
      "epoch no.0 train no.40800  loss = 2.84768 avg_loss = 3.72266\n",
      "epoch no.0 train no.40810  loss = 4.97294 avg_loss = 3.76305\n",
      "epoch no.0 train no.40820  loss = 5.41837 avg_loss = 3.78100\n",
      "epoch no.0 train no.40830  loss = 3.04080 avg_loss = 3.78552\n",
      "epoch no.0 train no.40840  loss = 6.35601 avg_loss = 3.85756\n",
      "epoch no.0 train no.40850  loss = 3.05814 avg_loss = 3.90772\n",
      "epoch no.0 train no.40860  loss = 3.21465 avg_loss = 3.87587\n",
      "epoch no.0 train no.40870  loss = 4.05206 avg_loss = 3.82996\n",
      "epoch no.0 train no.40880  loss = 4.39927 avg_loss = 3.81708\n",
      "epoch no.0 train no.40890  loss = 6.83613 avg_loss = 3.89128\n",
      "epoch no.0 train no.40900  loss = 2.46618 avg_loss = 3.90954\n",
      "epoch no.0 train no.40910  loss = 3.38449 avg_loss = 3.88254\n",
      "epoch no.0 train no.40920  loss = 3.26912 avg_loss = 3.88842\n",
      "epoch no.0 train no.40930  loss = 5.30091 avg_loss = 3.88734\n",
      "epoch no.0 train no.40940  loss = 4.60578 avg_loss = 3.84193\n",
      "epoch no.0 train no.40950  loss = 2.98105 avg_loss = 3.86945\n",
      "epoch no.0 train no.40960  loss = 4.16987 avg_loss = 3.86549\n",
      "epoch no.0 train no.40970  loss = 3.60309 avg_loss = 3.87515\n",
      "epoch no.0 train no.40980  loss = 5.50979 avg_loss = 3.88738\n",
      "epoch no.0 train no.40990  loss = 3.01169 avg_loss = 3.87974\n",
      "epoch no.0 train no.41000  loss = 1.71123 avg_loss = 3.85329\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁책임질', '놓을', '▁감성', '노래', '</s>']\n",
      "여름밤을 수놓을 여름노래</s>\n",
      "epoch no.0 train no.41010  loss = 1.48680 avg_loss = 3.79612\n",
      "epoch no.0 train no.41020  loss = 4.19833 avg_loss = 3.81763\n",
      "epoch no.0 train no.41030  loss = 3.83211 avg_loss = 3.83118\n",
      "epoch no.0 train no.41040  loss = 2.88489 avg_loss = 3.90042\n",
      "epoch no.0 train no.41050  loss = 4.09004 avg_loss = 3.89593\n",
      "epoch no.0 train no.41060  loss = 4.94989 avg_loss = 3.92554\n",
      "epoch no.0 train no.41070  loss = 3.91288 avg_loss = 3.92172\n",
      "epoch no.0 train no.41080  loss = 3.48987 avg_loss = 3.84868\n",
      "epoch no.0 train no.41090  loss = 3.22482 avg_loss = 3.81530\n",
      "epoch no.0 train no.41100  loss = 3.83764 avg_loss = 3.88814\n",
      "epoch no.0 train no.41110  loss = 5.12802 avg_loss = 3.88615\n",
      "epoch no.0 train no.41120  loss = 4.20031 avg_loss = 3.91360\n",
      "epoch no.0 train no.41130  loss = 4.61207 avg_loss = 3.94655\n",
      "epoch no.0 train no.41140  loss = 3.70824 avg_loss = 3.95913\n",
      "epoch no.0 train no.41150  loss = 3.43297 avg_loss = 3.91711\n",
      "epoch no.0 train no.41160  loss = 3.27286 avg_loss = 3.91940\n",
      "epoch no.0 train no.41170  loss = 3.70677 avg_loss = 3.86636\n",
      "epoch no.0 train no.41180  loss = 2.00618 avg_loss = 3.86756\n",
      "epoch no.0 train no.41190  loss = 3.18822 avg_loss = 3.85331\n",
      "epoch no.0 train no.41200  loss = 4.53484 avg_loss = 3.88464\n",
      "epoch no.0 train no.41210  loss = 4.42468 avg_loss = 3.90974\n",
      "epoch no.0 train no.41220  loss = 3.17259 avg_loss = 3.88175\n",
      "epoch no.0 train no.41230  loss = 3.23127 avg_loss = 3.87864\n",
      "epoch no.0 train no.41240  loss = 2.89451 avg_loss = 3.83718\n",
      "epoch no.0 train no.41250  loss = 3.76870 avg_loss = 3.88666\n",
      "epoch no.0 train no.41260  loss = 2.41980 avg_loss = 3.83623\n",
      "epoch no.0 train no.41270  loss = 3.35986 avg_loss = 3.78651\n",
      "epoch no.0 train no.41280  loss = 3.47968 avg_loss = 3.75290\n",
      "epoch no.0 train no.41290  loss = 3.36197 avg_loss = 3.77621\n",
      "epoch no.0 train no.41300  loss = 3.29257 avg_loss = 3.79217\n",
      "epoch no.0 train no.41310  loss = 7.44473 avg_loss = 3.81097\n",
      "epoch no.0 train no.41320  loss = 2.26713 avg_loss = 3.85820\n",
      "epoch no.0 train no.41330  loss = 2.07624 avg_loss = 3.87159\n",
      "epoch no.0 train no.41340  loss = 3.64898 avg_loss = 3.86455\n",
      "epoch no.0 train no.41350  loss = 5.09076 avg_loss = 3.86126\n",
      "epoch no.0 train no.41360  loss = 4.56705 avg_loss = 3.86889\n",
      "epoch no.0 train no.41370  loss = 5.18272 avg_loss = 3.92343\n",
      "epoch no.0 train no.41380  loss = 4.78559 avg_loss = 3.90505\n",
      "epoch no.0 train no.41390  loss = 6.05932 avg_loss = 3.92208\n",
      "epoch no.0 train no.41400  loss = 3.13146 avg_loss = 3.88979\n",
      "epoch no.0 train no.41410  loss = 3.21060 avg_loss = 3.84139\n",
      "epoch no.0 train no.41420  loss = 4.33566 avg_loss = 3.89377\n",
      "epoch no.0 train no.41430  loss = 4.45836 avg_loss = 3.90680\n",
      "epoch no.0 train no.41440  loss = 6.05711 avg_loss = 3.90466\n",
      "epoch no.0 train no.41450  loss = 3.49562 avg_loss = 3.95018\n",
      "epoch no.0 train no.41460  loss = 2.96035 avg_loss = 3.93473\n",
      "epoch no.0 train no.41470  loss = 3.51515 avg_loss = 3.94334\n",
      "epoch no.0 train no.41480  loss = 2.34743 avg_loss = 3.95913\n",
      "epoch no.0 train no.41490  loss = 5.90402 avg_loss = 4.00194\n",
      "epoch no.0 train no.41500  loss = 6.44888 avg_loss = 4.01889\n",
      "epoch no.0 train no.41510  loss = 3.78280 avg_loss = 4.00547\n",
      "epoch no.0 train no.41520  loss = 4.82980 avg_loss = 3.97795\n",
      "epoch no.0 train no.41530  loss = 5.56342 avg_loss = 4.03515\n",
      "epoch no.0 train no.41540  loss = 3.52299 avg_loss = 3.99102\n",
      "epoch no.0 train no.41550  loss = 2.29928 avg_loss = 3.93290\n",
      "epoch no.0 train no.41560  loss = 3.04632 avg_loss = 3.94290\n",
      "epoch no.0 train no.41570  loss = 2.98710 avg_loss = 3.94036\n",
      "epoch no.0 train no.41580  loss = 4.07268 avg_loss = 3.96181\n",
      "epoch no.0 train no.41590  loss = 5.59459 avg_loss = 4.01653\n",
      "epoch no.0 train no.41600  loss = 5.10813 avg_loss = 3.98243\n",
      "epoch no.0 train no.41610  loss = 2.62260 avg_loss = 4.00624\n",
      "epoch no.0 train no.41620  loss = 3.46521 avg_loss = 3.97845\n",
      "epoch no.0 train no.41630  loss = 4.39160 avg_loss = 3.98964\n",
      "epoch no.0 train no.41640  loss = 3.90225 avg_loss = 3.97385\n",
      "epoch no.0 train no.41650  loss = 3.04133 avg_loss = 4.00932\n",
      "epoch no.0 train no.41660  loss = 3.94473 avg_loss = 3.99567\n",
      "epoch no.0 train no.41670  loss = 2.25774 avg_loss = 4.00493\n",
      "epoch no.0 train no.41680  loss = 2.69085 avg_loss = 3.98546\n",
      "epoch no.0 train no.41690  loss = 3.38581 avg_loss = 3.93857\n",
      "epoch no.0 train no.41700  loss = 3.27057 avg_loss = 3.89823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.41710  loss = 2.31965 avg_loss = 3.93422\n",
      "epoch no.0 train no.41720  loss = 3.03747 avg_loss = 3.88822\n",
      "epoch no.0 train no.41730  loss = 6.36220 avg_loss = 3.86772\n",
      "epoch no.0 train no.41740  loss = 5.15563 avg_loss = 3.88044\n",
      "epoch no.0 train no.41750  loss = 4.54146 avg_loss = 3.96786\n",
      "epoch no.0 train no.41760  loss = 5.54575 avg_loss = 3.96225\n",
      "epoch no.0 train no.41770  loss = 4.00933 avg_loss = 4.05200\n",
      "epoch no.0 train no.41780  loss = 2.66892 avg_loss = 4.01762\n",
      "epoch no.0 train no.41790  loss = 5.41195 avg_loss = 4.03677\n",
      "epoch no.0 train no.41800  loss = 3.49805 avg_loss = 4.02009\n",
      "epoch no.0 train no.41810  loss = 5.52620 avg_loss = 4.06624\n",
      "epoch no.0 train no.41820  loss = 5.67129 avg_loss = 4.03631\n",
      "epoch no.0 train no.41830  loss = 3.02368 avg_loss = 4.05880\n",
      "epoch no.0 train no.41840  loss = 3.53388 avg_loss = 4.00728\n",
      "epoch no.0 train no.41850  loss = 6.48688 avg_loss = 4.03959\n",
      "epoch no.0 train no.41860  loss = 5.62876 avg_loss = 4.02740\n",
      "epoch no.0 train no.41870  loss = 3.77138 avg_loss = 3.97002\n",
      "epoch no.0 train no.41880  loss = 3.71637 avg_loss = 3.97422\n",
      "epoch no.0 train no.41890  loss = 4.47819 avg_loss = 4.01515\n",
      "epoch no.0 train no.41900  loss = 2.99901 avg_loss = 3.96666\n",
      "epoch no.0 train no.41910  loss = 2.00212 avg_loss = 3.94178\n",
      "epoch no.0 train no.41920  loss = 3.88348 avg_loss = 3.91969\n",
      "epoch no.0 train no.41930  loss = 4.47169 avg_loss = 3.93771\n",
      "epoch no.0 train no.41940  loss = 5.25229 avg_loss = 3.96255\n",
      "epoch no.0 train no.41950  loss = 4.37808 avg_loss = 4.00444\n",
      "epoch no.0 train no.41960  loss = 4.23674 avg_loss = 4.01828\n",
      "epoch no.0 train no.41970  loss = 5.66762 avg_loss = 4.03881\n",
      "epoch no.0 train no.41980  loss = 6.53424 avg_loss = 4.07003\n",
      "epoch no.0 train no.41990  loss = 3.22021 avg_loss = 4.05275\n",
      "epoch no.0 train no.42000  loss = 2.44712 avg_loss = 4.00226\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁노래', '▁노래', '</s>', '</s>']\n",
      "여름밤에 듣는 신나는 노래들</s>\n",
      "epoch no.0 train no.42010  loss = 5.50460 avg_loss = 4.01681\n",
      "epoch no.0 train no.42020  loss = 5.78274 avg_loss = 4.08789\n",
      "epoch no.0 train no.42030  loss = 3.23871 avg_loss = 4.07778\n",
      "epoch no.0 train no.42040  loss = 3.22999 avg_loss = 4.01033\n",
      "epoch no.0 train no.42050  loss = 6.64311 avg_loss = 4.00034\n",
      "epoch no.0 train no.42060  loss = 2.98502 avg_loss = 3.92064\n",
      "epoch no.0 train no.42070  loss = 3.82226 avg_loss = 3.95041\n",
      "epoch no.0 train no.42080  loss = 6.45072 avg_loss = 3.94342\n",
      "epoch no.0 train no.42090  loss = 3.55359 avg_loss = 3.94517\n",
      "epoch no.0 train no.42100  loss = 4.54815 avg_loss = 3.97073\n",
      "epoch no.0 train no.42110  loss = 8.57814 avg_loss = 4.00092\n",
      "epoch no.0 train no.42120  loss = 2.85549 avg_loss = 3.98557\n",
      "epoch no.0 train no.42130  loss = 2.90970 avg_loss = 3.99541\n",
      "epoch no.0 train no.42140  loss = 4.38385 avg_loss = 3.96759\n",
      "epoch no.0 train no.42150  loss = 4.51132 avg_loss = 4.02053\n",
      "epoch no.0 train no.42160  loss = 5.20857 avg_loss = 4.06066\n",
      "epoch no.0 train no.42170  loss = 3.05371 avg_loss = 4.01967\n",
      "epoch no.0 train no.42180  loss = 2.98144 avg_loss = 4.06122\n",
      "epoch no.0 train no.42190  loss = 3.19850 avg_loss = 4.04829\n",
      "epoch no.0 train no.42200  loss = 5.82011 avg_loss = 4.02301\n",
      "epoch no.0 train no.42210  loss = 4.68327 avg_loss = 4.07540\n",
      "epoch no.0 train no.42220  loss = 2.93520 avg_loss = 4.06003\n",
      "epoch no.0 train no.42230  loss = 4.50099 avg_loss = 4.02364\n",
      "epoch no.0 train no.42240  loss = 3.65579 avg_loss = 4.01254\n",
      "epoch no.0 train no.42250  loss = 3.56570 avg_loss = 3.96980\n",
      "epoch no.0 train no.42260  loss = 2.53947 avg_loss = 3.96037\n",
      "epoch no.0 train no.42270  loss = 1.89659 avg_loss = 3.91710\n",
      "epoch no.0 train no.42280  loss = 2.88336 avg_loss = 3.92498\n",
      "epoch no.0 train no.42290  loss = 2.59169 avg_loss = 3.83762\n",
      "epoch no.0 train no.42300  loss = 5.97242 avg_loss = 3.85576\n",
      "epoch no.0 train no.42310  loss = 4.36728 avg_loss = 3.84196\n",
      "epoch no.0 train no.42320  loss = 2.81082 avg_loss = 3.82158\n",
      "epoch no.0 train no.42330  loss = 4.78119 avg_loss = 3.77759\n",
      "epoch no.0 train no.42340  loss = 2.49926 avg_loss = 3.74455\n",
      "epoch no.0 train no.42350  loss = 4.04866 avg_loss = 3.81247\n",
      "epoch no.0 train no.42360  loss = 1.88872 avg_loss = 3.78338\n",
      "epoch no.0 train no.42370  loss = 3.69375 avg_loss = 3.79958\n",
      "epoch no.0 train no.42380  loss = 2.09424 avg_loss = 3.76451\n",
      "epoch no.0 train no.42390  loss = 3.21401 avg_loss = 3.81623\n",
      "epoch no.0 train no.42400  loss = 2.84521 avg_loss = 3.83421\n",
      "epoch no.0 train no.42410  loss = 5.82257 avg_loss = 3.87333\n",
      "epoch no.0 train no.42420  loss = 4.80289 avg_loss = 3.87170\n",
      "epoch no.0 train no.42430  loss = 3.88071 avg_loss = 3.80679\n",
      "epoch no.0 train no.42440  loss = 4.31599 avg_loss = 3.81562\n",
      "epoch no.0 train no.42450  loss = 2.68933 avg_loss = 3.79585\n",
      "epoch no.0 train no.42460  loss = 5.21446 avg_loss = 3.84123\n",
      "epoch no.0 train no.42470  loss = 4.11866 avg_loss = 3.76702\n",
      "epoch no.0 train no.42480  loss = 2.33282 avg_loss = 3.73835\n",
      "epoch no.0 train no.42490  loss = 2.38537 avg_loss = 3.78402\n",
      "epoch no.0 train no.42500  loss = 4.03965 avg_loss = 3.77967\n",
      "epoch no.0 train no.42510  loss = 3.78728 avg_loss = 3.82766\n",
      "epoch no.0 train no.42520  loss = 2.48373 avg_loss = 3.81894\n",
      "epoch no.0 train no.42530  loss = 2.49960 avg_loss = 3.83938\n",
      "epoch no.0 train no.42540  loss = 6.23100 avg_loss = 3.81194\n",
      "epoch no.0 train no.42550  loss = 3.09433 avg_loss = 3.85072\n",
      "epoch no.0 train no.42560  loss = 3.94221 avg_loss = 3.90591\n",
      "epoch no.0 train no.42570  loss = 2.65686 avg_loss = 3.96606\n",
      "epoch no.0 train no.42580  loss = 3.30140 avg_loss = 3.90853\n",
      "epoch no.0 train no.42590  loss = 3.46137 avg_loss = 3.88944\n",
      "epoch no.0 train no.42600  loss = 3.73226 avg_loss = 3.92807\n",
      "epoch no.0 train no.42610  loss = 3.98040 avg_loss = 3.95644\n",
      "epoch no.0 train no.42620  loss = 5.77400 avg_loss = 3.94901\n",
      "epoch no.0 train no.42630  loss = 6.95680 avg_loss = 4.00986\n",
      "epoch no.0 train no.42640  loss = 3.61623 avg_loss = 3.99390\n",
      "epoch no.0 train no.42650  loss = 3.70026 avg_loss = 3.97934\n",
      "epoch no.0 train no.42660  loss = 4.68022 avg_loss = 4.04716\n",
      "epoch no.0 train no.42670  loss = 5.06778 avg_loss = 4.02718\n",
      "epoch no.0 train no.42680  loss = 2.43993 avg_loss = 3.99042\n",
      "epoch no.0 train no.42690  loss = 6.17851 avg_loss = 3.94822\n",
      "epoch no.0 train no.42700  loss = 5.56248 avg_loss = 3.95023\n",
      "epoch no.0 train no.42710  loss = 5.13959 avg_loss = 3.94086\n",
      "epoch no.0 train no.42720  loss = 2.94459 avg_loss = 3.91956\n",
      "epoch no.0 train no.42730  loss = 3.20232 avg_loss = 3.96060\n",
      "epoch no.0 train no.42740  loss = 3.79540 avg_loss = 3.98578\n",
      "epoch no.0 train no.42750  loss = 3.23598 avg_loss = 3.93925\n",
      "epoch no.0 train no.42760  loss = 4.80875 avg_loss = 3.91788\n",
      "epoch no.0 train no.42770  loss = 2.21080 avg_loss = 3.84457\n",
      "epoch no.0 train no.42780  loss = 6.07184 avg_loss = 3.91022\n",
      "epoch no.0 train no.42790  loss = 5.26977 avg_loss = 3.88781\n",
      "epoch no.0 train no.42800  loss = 3.26097 avg_loss = 3.88405\n",
      "epoch no.0 train no.42810  loss = 3.68367 avg_loss = 3.93252\n",
      "epoch no.0 train no.42820  loss = 4.26763 avg_loss = 3.90530\n",
      "epoch no.0 train no.42830  loss = 3.78136 avg_loss = 3.89222\n",
      "epoch no.0 train no.42840  loss = 4.21024 avg_loss = 3.88921\n",
      "epoch no.0 train no.42850  loss = 2.95534 avg_loss = 3.82777\n",
      "epoch no.0 train no.42860  loss = 5.23411 avg_loss = 3.86621\n",
      "epoch no.0 train no.42870  loss = 3.37279 avg_loss = 3.84188\n",
      "epoch no.0 train no.42880  loss = 8.22119 avg_loss = 3.89648\n",
      "epoch no.0 train no.42890  loss = 5.22517 avg_loss = 3.90775\n",
      "epoch no.0 train no.42900  loss = 6.03260 avg_loss = 3.93904\n",
      "epoch no.0 train no.42910  loss = 2.72249 avg_loss = 3.95883\n",
      "epoch no.0 train no.42920  loss = 3.38904 avg_loss = 3.91041\n",
      "epoch no.0 train no.42930  loss = 4.04889 avg_loss = 3.88072\n",
      "epoch no.0 train no.42940  loss = 4.65778 avg_loss = 3.83483\n",
      "epoch no.0 train no.42950  loss = 4.45373 avg_loss = 3.88696\n",
      "epoch no.0 train no.42960  loss = 4.37137 avg_loss = 3.92045\n",
      "epoch no.0 train no.42970  loss = 3.28428 avg_loss = 3.86497\n",
      "epoch no.0 train no.42980  loss = 4.40989 avg_loss = 3.89987\n",
      "epoch no.0 train no.42990  loss = 3.52449 avg_loss = 3.96854\n",
      "epoch no.0 train no.43000  loss = 2.91272 avg_loss = 3.93869\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '에', '▁시원한', '▁날려', '줄', '▁노래', '한', '▁노래', '</s>']\n",
      "여름밤의 더위를 날려줄 청량한 음악</s>\n",
      "epoch no.0 train no.43010  loss = 4.87512 avg_loss = 3.90464\n",
      "epoch no.0 train no.43020  loss = 3.26108 avg_loss = 3.85910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.43030  loss = 2.45490 avg_loss = 3.83394\n",
      "epoch no.0 train no.43040  loss = 3.98245 avg_loss = 3.83092\n",
      "epoch no.0 train no.43050  loss = 2.72774 avg_loss = 3.77700\n",
      "epoch no.0 train no.43060  loss = 3.51836 avg_loss = 3.72837\n",
      "epoch no.0 train no.43070  loss = 6.66832 avg_loss = 3.74910\n",
      "epoch no.0 train no.43080  loss = 5.54494 avg_loss = 3.78335\n",
      "epoch no.0 train no.43090  loss = 7.58385 avg_loss = 3.86589\n",
      "epoch no.0 train no.43100  loss = 5.76034 avg_loss = 3.92462\n",
      "epoch no.0 train no.43110  loss = 3.07669 avg_loss = 3.89530\n",
      "epoch no.0 train no.43120  loss = 4.88568 avg_loss = 3.85940\n",
      "epoch no.0 train no.43130  loss = 2.77637 avg_loss = 3.78772\n",
      "epoch no.0 train no.43140  loss = 1.73346 avg_loss = 3.77850\n",
      "epoch no.0 train no.43150  loss = 4.12853 avg_loss = 3.75146\n",
      "epoch no.0 train no.43160  loss = 2.74061 avg_loss = 3.77461\n",
      "epoch no.0 train no.43170  loss = 3.97600 avg_loss = 3.82271\n",
      "epoch no.0 train no.43180  loss = 3.63218 avg_loss = 3.76051\n",
      "epoch no.0 train no.43190  loss = 3.45253 avg_loss = 3.75633\n",
      "epoch no.0 train no.43200  loss = 4.12213 avg_loss = 3.80636\n",
      "epoch no.0 train no.43210  loss = 3.48232 avg_loss = 3.82547\n",
      "epoch no.0 train no.43220  loss = 1.87318 avg_loss = 3.83177\n",
      "epoch no.0 train no.43230  loss = 5.03780 avg_loss = 3.81602\n",
      "epoch no.0 train no.43240  loss = 6.78427 avg_loss = 3.86019\n",
      "epoch no.0 train no.43250  loss = 3.86178 avg_loss = 3.90658\n",
      "epoch no.0 train no.43260  loss = 3.91101 avg_loss = 3.89862\n",
      "epoch no.0 train no.43270  loss = 2.86521 avg_loss = 3.91461\n",
      "epoch no.0 train no.43280  loss = 3.91305 avg_loss = 4.00836\n",
      "epoch no.0 train no.43290  loss = 4.39111 avg_loss = 4.03218\n",
      "epoch no.0 train no.43300  loss = 5.51341 avg_loss = 4.08019\n",
      "epoch no.0 train no.43310  loss = 2.18258 avg_loss = 4.03441\n",
      "epoch no.0 train no.43320  loss = 6.35777 avg_loss = 4.06779\n",
      "epoch no.0 train no.43330  loss = 3.31835 avg_loss = 4.04674\n",
      "epoch no.0 train no.43340  loss = 3.13089 avg_loss = 3.98822\n",
      "epoch no.0 train no.43350  loss = 4.17997 avg_loss = 4.02739\n",
      "epoch no.0 train no.43360  loss = 4.55600 avg_loss = 3.97090\n",
      "epoch no.0 train no.43370  loss = 4.14786 avg_loss = 3.97983\n",
      "epoch no.0 train no.43380  loss = 6.74882 avg_loss = 3.99875\n",
      "epoch no.0 train no.43390  loss = 4.91524 avg_loss = 4.01328\n",
      "epoch no.0 train no.43400  loss = 3.89199 avg_loss = 4.05598\n",
      "epoch no.0 train no.43410  loss = 3.51289 avg_loss = 4.02781\n",
      "epoch no.0 train no.43420  loss = 5.88246 avg_loss = 4.01798\n",
      "epoch no.0 train no.43430  loss = 6.92236 avg_loss = 4.05161\n",
      "epoch no.0 train no.43440  loss = 2.37370 avg_loss = 3.99938\n",
      "epoch no.0 train no.43450  loss = 6.97311 avg_loss = 4.04417\n",
      "epoch no.0 train no.43460  loss = 2.65144 avg_loss = 4.01505\n",
      "epoch no.0 train no.43470  loss = 4.67166 avg_loss = 4.03197\n",
      "epoch no.0 train no.43480  loss = 2.65878 avg_loss = 4.04491\n",
      "epoch no.0 train no.43490  loss = 3.38176 avg_loss = 4.06082\n",
      "epoch no.0 train no.43500  loss = 4.42808 avg_loss = 4.08447\n",
      "epoch no.0 train no.43510  loss = 3.38259 avg_loss = 3.97919\n",
      "epoch no.0 train no.43520  loss = 2.81660 avg_loss = 3.94169\n",
      "epoch no.0 train no.43530  loss = 2.56617 avg_loss = 3.96433\n",
      "epoch no.0 train no.43540  loss = 3.17418 avg_loss = 3.94320\n",
      "epoch no.0 train no.43550  loss = 3.74038 avg_loss = 3.96482\n",
      "epoch no.0 train no.43560  loss = 3.18209 avg_loss = 3.92326\n",
      "epoch no.0 train no.43570  loss = 2.58503 avg_loss = 3.89282\n",
      "epoch no.0 train no.43580  loss = 4.18492 avg_loss = 3.90146\n",
      "epoch no.0 train no.43590  loss = 4.30635 avg_loss = 3.91441\n",
      "epoch no.0 train no.43600  loss = 3.63509 avg_loss = 3.89195\n",
      "epoch no.0 train no.43610  loss = 4.44448 avg_loss = 3.88812\n",
      "epoch no.0 train no.43620  loss = 4.40170 avg_loss = 3.93173\n",
      "epoch no.0 train no.43630  loss = 2.45295 avg_loss = 3.95233\n",
      "epoch no.0 train no.43640  loss = 1.92551 avg_loss = 3.91460\n",
      "epoch no.0 train no.43650  loss = 3.70577 avg_loss = 3.83981\n",
      "epoch no.0 train no.43660  loss = 2.64408 avg_loss = 3.82874\n",
      "epoch no.0 train no.43670  loss = 3.40537 avg_loss = 3.89339\n",
      "epoch no.0 train no.43680  loss = 1.92386 avg_loss = 3.90047\n",
      "epoch no.0 train no.43690  loss = 2.92258 avg_loss = 3.93394\n",
      "epoch no.0 train no.43700  loss = 4.52624 avg_loss = 3.96037\n",
      "epoch no.0 train no.43710  loss = 5.67089 avg_loss = 3.93804\n",
      "epoch no.0 train no.43720  loss = 2.23938 avg_loss = 3.92523\n",
      "epoch no.0 train no.43730  loss = 6.26992 avg_loss = 3.95319\n",
      "epoch no.0 train no.43740  loss = 3.23213 avg_loss = 3.94042\n",
      "epoch no.0 train no.43750  loss = 2.08467 avg_loss = 3.90396\n",
      "epoch no.0 train no.43760  loss = 5.02643 avg_loss = 3.90645\n",
      "epoch no.0 train no.43770  loss = 3.50768 avg_loss = 3.88679\n",
      "epoch no.0 train no.43780  loss = 4.09849 avg_loss = 3.89992\n",
      "epoch no.0 train no.43790  loss = 4.44526 avg_loss = 3.88788\n",
      "epoch no.0 train no.43800  loss = 3.23937 avg_loss = 3.88714\n",
      "epoch no.0 train no.43810  loss = 5.27570 avg_loss = 3.86220\n",
      "epoch no.0 train no.43820  loss = 6.08191 avg_loss = 3.89923\n",
      "epoch no.0 train no.43830  loss = 4.31500 avg_loss = 3.90485\n",
      "epoch no.0 train no.43840  loss = 4.34823 avg_loss = 3.92909\n",
      "epoch no.0 train no.43850  loss = 4.25457 avg_loss = 4.01191\n",
      "epoch no.0 train no.43860  loss = 2.86506 avg_loss = 4.01110\n",
      "epoch no.0 train no.43870  loss = 3.77496 avg_loss = 4.03746\n",
      "epoch no.0 train no.43880  loss = 3.36857 avg_loss = 3.96624\n",
      "epoch no.0 train no.43890  loss = 5.30246 avg_loss = 3.91708\n",
      "epoch no.0 train no.43900  loss = 3.40560 avg_loss = 3.87252\n",
      "epoch no.0 train no.43910  loss = 5.56281 avg_loss = 3.87675\n",
      "epoch no.0 train no.43920  loss = 3.68312 avg_loss = 3.91338\n",
      "epoch no.0 train no.43930  loss = 3.60913 avg_loss = 3.84958\n",
      "epoch no.0 train no.43940  loss = 2.95937 avg_loss = 3.83370\n",
      "epoch no.0 train no.43950  loss = 3.41638 avg_loss = 3.85230\n",
      "epoch no.0 train no.43960  loss = 3.01370 avg_loss = 3.85005\n",
      "epoch no.0 train no.43970  loss = 5.83572 avg_loss = 3.90207\n",
      "epoch no.0 train no.43980  loss = 4.31756 avg_loss = 3.87214\n",
      "epoch no.0 train no.43990  loss = 4.27951 avg_loss = 3.85807\n",
      "epoch no.0 train no.44000  loss = 3.31175 avg_loss = 3.84160\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '</s>', '리스트', '</s>']\n",
      "여름밤의 감성 플레이리스트</s>\n",
      "epoch no.0 train no.44010  loss = 2.80027 avg_loss = 3.91568\n",
      "epoch no.0 train no.44020  loss = 5.79343 avg_loss = 3.92769\n",
      "epoch no.0 train no.44030  loss = 4.59432 avg_loss = 3.89230\n",
      "epoch no.0 train no.44040  loss = 3.14162 avg_loss = 3.87462\n",
      "epoch no.0 train no.44050  loss = 4.41366 avg_loss = 3.93474\n",
      "epoch no.0 train no.44060  loss = 6.01179 avg_loss = 4.00581\n",
      "epoch no.0 train no.44070  loss = 4.03674 avg_loss = 3.92745\n",
      "epoch no.0 train no.44080  loss = 4.93012 avg_loss = 3.87716\n",
      "epoch no.0 train no.44090  loss = 3.29039 avg_loss = 3.88975\n",
      "epoch no.0 train no.44100  loss = 3.88679 avg_loss = 3.94633\n",
      "epoch no.0 train no.44110  loss = 2.71842 avg_loss = 4.01887\n",
      "epoch no.0 train no.44120  loss = 2.88988 avg_loss = 3.98824\n",
      "epoch no.0 train no.44130  loss = 1.80796 avg_loss = 3.99653\n",
      "epoch no.0 train no.44140  loss = 4.70444 avg_loss = 3.94550\n",
      "epoch no.0 train no.44150  loss = 3.32118 avg_loss = 3.94960\n",
      "epoch no.0 train no.44160  loss = 3.30715 avg_loss = 3.95499\n",
      "epoch no.0 train no.44170  loss = 6.61085 avg_loss = 3.94835\n",
      "epoch no.0 train no.44180  loss = 6.28966 avg_loss = 3.97259\n",
      "epoch no.0 train no.44190  loss = 4.21455 avg_loss = 3.93702\n",
      "epoch no.0 train no.44200  loss = 3.29832 avg_loss = 3.93496\n",
      "epoch no.0 train no.44210  loss = 3.81150 avg_loss = 3.93346\n",
      "epoch no.0 train no.44220  loss = 3.96143 avg_loss = 3.93959\n",
      "epoch no.0 train no.44230  loss = 3.15154 avg_loss = 3.92753\n",
      "epoch no.0 train no.44240  loss = 4.78969 avg_loss = 3.91428\n",
      "epoch no.0 train no.44250  loss = 4.18946 avg_loss = 3.97015\n",
      "epoch no.0 train no.44260  loss = 2.89231 avg_loss = 3.94909\n",
      "epoch no.0 train no.44270  loss = 5.88733 avg_loss = 3.91738\n",
      "epoch no.0 train no.44280  loss = 5.13919 avg_loss = 3.96813\n",
      "epoch no.0 train no.44290  loss = 5.08390 avg_loss = 3.94235\n",
      "epoch no.0 train no.44300  loss = 4.20746 avg_loss = 3.91279\n",
      "epoch no.0 train no.44310  loss = 5.01633 avg_loss = 3.89227\n",
      "epoch no.0 train no.44320  loss = 4.00069 avg_loss = 3.91126\n",
      "epoch no.0 train no.44330  loss = 2.31294 avg_loss = 3.87461\n",
      "epoch no.0 train no.44340  loss = 2.55405 avg_loss = 3.86515\n",
      "epoch no.0 train no.44350  loss = 4.16782 avg_loss = 3.86576\n",
      "epoch no.0 train no.44360  loss = 5.01683 avg_loss = 3.90716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.44370  loss = 3.31949 avg_loss = 3.89787\n",
      "epoch no.0 train no.44380  loss = 2.15721 avg_loss = 3.87151\n",
      "epoch no.0 train no.44390  loss = 4.64939 avg_loss = 3.86267\n",
      "epoch no.0 train no.44400  loss = 2.09740 avg_loss = 3.86104\n",
      "epoch no.0 train no.44410  loss = 3.57964 avg_loss = 3.83484\n",
      "epoch no.0 train no.44420  loss = 3.40971 avg_loss = 3.81650\n",
      "epoch no.0 train no.44430  loss = 5.49379 avg_loss = 3.85488\n",
      "epoch no.0 train no.44440  loss = 4.16555 avg_loss = 3.84877\n",
      "epoch no.0 train no.44450  loss = 4.15868 avg_loss = 3.83811\n",
      "epoch no.0 train no.44460  loss = 3.22582 avg_loss = 3.84366\n",
      "epoch no.0 train no.44470  loss = 5.07631 avg_loss = 3.86559\n",
      "epoch no.0 train no.44480  loss = 3.31509 avg_loss = 3.85997\n",
      "epoch no.0 train no.44490  loss = 3.61148 avg_loss = 3.84839\n",
      "epoch no.0 train no.44500  loss = 2.21042 avg_loss = 3.83867\n",
      "epoch no.0 train no.44510  loss = 2.16294 avg_loss = 3.82486\n",
      "epoch no.0 train no.44520  loss = 4.81694 avg_loss = 3.88416\n",
      "epoch no.0 train no.44530  loss = 4.66105 avg_loss = 3.93823\n",
      "epoch no.0 train no.44540  loss = 2.63591 avg_loss = 3.88485\n",
      "epoch no.0 train no.44550  loss = 3.88344 avg_loss = 3.86483\n",
      "epoch no.0 train no.44560  loss = 3.04146 avg_loss = 3.85052\n",
      "epoch no.0 train no.44570  loss = 2.63798 avg_loss = 3.81946\n",
      "epoch no.0 train no.44580  loss = 3.00202 avg_loss = 3.82703\n",
      "epoch no.0 train no.44590  loss = 3.36267 avg_loss = 3.83769\n",
      "epoch no.0 train no.44600  loss = 2.57614 avg_loss = 3.80318\n",
      "epoch no.0 train no.44610  loss = 3.85647 avg_loss = 3.82750\n",
      "epoch no.0 train no.44620  loss = 2.76464 avg_loss = 3.81685\n",
      "epoch no.0 train no.44630  loss = 3.71941 avg_loss = 3.86613\n",
      "epoch no.0 train no.44640  loss = 4.10733 avg_loss = 3.78562\n",
      "epoch no.0 train no.44650  loss = 3.41423 avg_loss = 3.75393\n",
      "epoch no.0 train no.44660  loss = 2.78939 avg_loss = 3.78260\n",
      "epoch no.0 train no.44670  loss = 3.83404 avg_loss = 3.77810\n",
      "epoch no.0 train no.44680  loss = 4.97703 avg_loss = 3.79659\n",
      "epoch no.0 train no.44690  loss = 6.61824 avg_loss = 3.80412\n",
      "epoch no.0 train no.44700  loss = 5.90024 avg_loss = 3.81334\n",
      "epoch no.0 train no.44710  loss = 3.44915 avg_loss = 3.83799\n",
      "epoch no.0 train no.44720  loss = 3.54761 avg_loss = 3.87683\n",
      "epoch no.0 train no.44730  loss = 4.10683 avg_loss = 3.88321\n",
      "epoch no.0 train no.44740  loss = 4.83707 avg_loss = 3.90984\n",
      "epoch no.0 train no.44750  loss = 4.10509 avg_loss = 3.92724\n",
      "epoch no.0 train no.44760  loss = 4.76253 avg_loss = 3.93649\n",
      "epoch no.0 train no.44770  loss = 3.52763 avg_loss = 3.90945\n",
      "epoch no.0 train no.44780  loss = 3.81290 avg_loss = 3.90105\n",
      "epoch no.0 train no.44790  loss = 2.55393 avg_loss = 3.86434\n",
      "epoch no.0 train no.44800  loss = 1.77054 avg_loss = 3.80701\n",
      "epoch no.0 train no.44810  loss = 2.76419 avg_loss = 3.82365\n",
      "epoch no.0 train no.44820  loss = 2.34121 avg_loss = 3.81691\n",
      "epoch no.0 train no.44830  loss = 2.88953 avg_loss = 3.80698\n",
      "epoch no.0 train no.44840  loss = 4.11602 avg_loss = 3.77718\n",
      "epoch no.0 train no.44850  loss = 3.84733 avg_loss = 3.85563\n",
      "epoch no.0 train no.44860  loss = 6.64698 avg_loss = 3.80893\n",
      "epoch no.0 train no.44870  loss = 4.24767 avg_loss = 3.80440\n",
      "epoch no.0 train no.44880  loss = 4.22723 avg_loss = 3.76688\n",
      "epoch no.0 train no.44890  loss = 3.08353 avg_loss = 3.75012\n",
      "epoch no.0 train no.44900  loss = 7.59804 avg_loss = 3.81961\n",
      "epoch no.0 train no.44910  loss = 2.07196 avg_loss = 3.82796\n",
      "epoch no.0 train no.44920  loss = 3.59077 avg_loss = 3.85344\n",
      "epoch no.0 train no.44930  loss = 3.57765 avg_loss = 3.81268\n",
      "epoch no.0 train no.44940  loss = 3.47751 avg_loss = 3.80341\n",
      "epoch no.0 train no.44950  loss = 2.57967 avg_loss = 3.85612\n",
      "epoch no.0 train no.44960  loss = 2.92237 avg_loss = 3.85741\n",
      "epoch no.0 train no.44970  loss = 1.79030 avg_loss = 3.80239\n",
      "epoch no.0 train no.44980  loss = 3.54364 avg_loss = 3.76567\n",
      "epoch no.0 train no.44990  loss = 3.87416 avg_loss = 3.78269\n",
      "epoch no.0 train no.45000  loss = 2.80010 avg_loss = 3.81688\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '</s>', '</s>']\n",
      "여름밤에 어울리는 재즈 음악</s>\n",
      "epoch no.0 train no.45010  loss = 6.01665 avg_loss = 3.83688\n",
      "epoch no.0 train no.45020  loss = 4.81018 avg_loss = 3.87997\n",
      "epoch no.0 train no.45030  loss = 2.11623 avg_loss = 3.87065\n",
      "epoch no.0 train no.45040  loss = 4.24627 avg_loss = 3.89931\n",
      "epoch no.0 train no.45050  loss = 4.49622 avg_loss = 3.93885\n",
      "epoch no.0 train no.45060  loss = 2.40664 avg_loss = 3.88210\n",
      "epoch no.0 train no.45070  loss = 3.78410 avg_loss = 3.81491\n",
      "epoch no.0 train no.45080  loss = 2.49384 avg_loss = 3.79024\n",
      "epoch no.0 train no.45090  loss = 5.42747 avg_loss = 3.85945\n",
      "epoch no.0 train no.45100  loss = 4.56474 avg_loss = 3.87505\n",
      "epoch no.0 train no.45110  loss = 2.99497 avg_loss = 3.84763\n",
      "epoch no.0 train no.45120  loss = 3.77945 avg_loss = 3.88599\n",
      "epoch no.0 train no.45130  loss = 5.43304 avg_loss = 3.84068\n",
      "epoch no.0 train no.45140  loss = 3.55239 avg_loss = 3.80187\n",
      "epoch no.0 train no.45150  loss = 4.26058 avg_loss = 3.86015\n",
      "epoch no.0 train no.45160  loss = 5.43917 avg_loss = 3.87355\n",
      "epoch no.0 train no.45170  loss = 2.84033 avg_loss = 3.87710\n",
      "epoch no.0 train no.45180  loss = 3.71382 avg_loss = 3.80721\n",
      "epoch no.0 train no.45190  loss = 4.37784 avg_loss = 3.85179\n",
      "epoch no.0 train no.45200  loss = 2.66828 avg_loss = 3.84505\n",
      "epoch no.0 train no.45210  loss = 3.16912 avg_loss = 3.88988\n",
      "epoch no.0 train no.45220  loss = 5.01913 avg_loss = 3.97101\n",
      "epoch no.0 train no.45230  loss = 4.54773 avg_loss = 3.96147\n",
      "epoch no.0 train no.45240  loss = 4.10315 avg_loss = 3.90842\n",
      "epoch no.0 train no.45250  loss = 5.36411 avg_loss = 3.91275\n",
      "epoch no.0 train no.45260  loss = 4.84294 avg_loss = 3.94230\n",
      "epoch no.0 train no.45270  loss = 3.41547 avg_loss = 3.92543\n",
      "epoch no.0 train no.45280  loss = 3.44946 avg_loss = 3.95372\n",
      "epoch no.0 train no.45290  loss = 3.33681 avg_loss = 3.94425\n",
      "epoch no.0 train no.45300  loss = 2.47513 avg_loss = 3.96391\n",
      "epoch no.0 train no.45310  loss = 3.88137 avg_loss = 3.94212\n",
      "epoch no.0 train no.45320  loss = 2.63856 avg_loss = 3.88796\n",
      "epoch no.0 train no.45330  loss = 5.12856 avg_loss = 3.86184\n",
      "epoch no.0 train no.45340  loss = 3.85669 avg_loss = 3.92568\n",
      "epoch no.0 train no.45350  loss = 2.64884 avg_loss = 3.91359\n",
      "epoch no.0 train no.45360  loss = 3.00963 avg_loss = 3.95144\n",
      "epoch no.0 train no.45370  loss = 4.93530 avg_loss = 3.96892\n",
      "epoch no.0 train no.45380  loss = 4.93073 avg_loss = 3.99427\n",
      "epoch no.0 train no.45390  loss = 4.85851 avg_loss = 4.06780\n",
      "epoch no.0 train no.45400  loss = 3.15514 avg_loss = 4.03251\n",
      "epoch no.0 train no.45410  loss = 4.64950 avg_loss = 4.05910\n",
      "epoch no.0 train no.45420  loss = 2.63528 avg_loss = 4.08284\n",
      "epoch no.0 train no.45430  loss = 5.89809 avg_loss = 4.06218\n",
      "epoch no.0 train no.45440  loss = 4.87684 avg_loss = 4.00903\n",
      "epoch no.0 train no.45450  loss = 2.24276 avg_loss = 3.95133\n",
      "epoch no.0 train no.45460  loss = 2.64939 avg_loss = 3.93165\n",
      "epoch no.0 train no.45470  loss = 2.72615 avg_loss = 3.88069\n",
      "epoch no.0 train no.45480  loss = 4.00014 avg_loss = 3.89409\n",
      "epoch no.0 train no.45490  loss = 5.45670 avg_loss = 3.91067\n",
      "epoch no.0 train no.45500  loss = 4.58669 avg_loss = 3.97181\n",
      "epoch no.0 train no.45510  loss = 3.88295 avg_loss = 3.99529\n",
      "epoch no.0 train no.45520  loss = 5.95024 avg_loss = 4.02433\n",
      "epoch no.0 train no.45530  loss = 4.91913 avg_loss = 4.04608\n",
      "epoch no.0 train no.45540  loss = 5.07939 avg_loss = 4.07685\n",
      "epoch no.0 train no.45550  loss = 2.24479 avg_loss = 4.04567\n",
      "epoch no.0 train no.45560  loss = 4.14945 avg_loss = 4.09584\n",
      "epoch no.0 train no.45570  loss = 5.71585 avg_loss = 4.11964\n",
      "epoch no.0 train no.45580  loss = 3.74426 avg_loss = 4.04104\n",
      "epoch no.0 train no.45590  loss = 3.53563 avg_loss = 3.99497\n",
      "epoch no.0 train no.45600  loss = 4.56382 avg_loss = 3.98226\n",
      "epoch no.0 train no.45610  loss = 4.29610 avg_loss = 3.98062\n",
      "epoch no.0 train no.45620  loss = 3.52688 avg_loss = 3.99390\n",
      "epoch no.0 train no.45630  loss = 6.70621 avg_loss = 4.03137\n",
      "epoch no.0 train no.45640  loss = 5.34477 avg_loss = 4.10035\n",
      "epoch no.0 train no.45650  loss = 5.52807 avg_loss = 4.06186\n",
      "epoch no.0 train no.45660  loss = 3.72511 avg_loss = 4.01019\n",
      "epoch no.0 train no.45670  loss = 3.75399 avg_loss = 3.97638\n",
      "epoch no.0 train no.45680  loss = 4.65970 avg_loss = 4.01089\n",
      "epoch no.0 train no.45690  loss = 2.73061 avg_loss = 3.97964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.45700  loss = 3.24684 avg_loss = 3.97315\n",
      "epoch no.0 train no.45710  loss = 3.77768 avg_loss = 4.00989\n",
      "epoch no.0 train no.45720  loss = 6.23783 avg_loss = 4.11949\n",
      "epoch no.0 train no.45730  loss = 3.85292 avg_loss = 4.05718\n",
      "epoch no.0 train no.45740  loss = 3.05326 avg_loss = 4.00426\n",
      "epoch no.0 train no.45750  loss = 3.25488 avg_loss = 4.00550\n",
      "epoch no.0 train no.45760  loss = 2.40857 avg_loss = 3.95428\n",
      "epoch no.0 train no.45770  loss = 3.05249 avg_loss = 3.93795\n",
      "epoch no.0 train no.45780  loss = 2.59759 avg_loss = 3.96661\n",
      "epoch no.0 train no.45790  loss = 2.09728 avg_loss = 3.96753\n",
      "epoch no.0 train no.45800  loss = 6.03993 avg_loss = 4.06203\n",
      "epoch no.0 train no.45810  loss = 5.06532 avg_loss = 4.05364\n",
      "epoch no.0 train no.45820  loss = 4.54168 avg_loss = 4.09231\n",
      "epoch no.0 train no.45830  loss = 2.93996 avg_loss = 4.13027\n",
      "epoch no.0 train no.45840  loss = 4.73065 avg_loss = 4.17027\n",
      "epoch no.0 train no.45850  loss = 3.63739 avg_loss = 4.13847\n",
      "epoch no.0 train no.45860  loss = 3.65666 avg_loss = 4.12243\n",
      "epoch no.0 train no.45870  loss = 4.25544 avg_loss = 4.10257\n",
      "epoch no.0 train no.45880  loss = 5.51455 avg_loss = 4.07245\n",
      "epoch no.0 train no.45890  loss = 3.69467 avg_loss = 4.09422\n",
      "epoch no.0 train no.45900  loss = 3.03407 avg_loss = 4.08961\n",
      "epoch no.0 train no.45910  loss = 3.39340 avg_loss = 4.03024\n",
      "epoch no.0 train no.45920  loss = 3.14148 avg_loss = 4.04297\n",
      "epoch no.0 train no.45930  loss = 3.77697 avg_loss = 4.08338\n",
      "epoch no.0 train no.45940  loss = 3.08258 avg_loss = 4.00993\n",
      "epoch no.0 train no.45950  loss = 3.66312 avg_loss = 4.05664\n",
      "epoch no.0 train no.45960  loss = 3.51910 avg_loss = 4.05698\n",
      "epoch no.0 train no.45970  loss = 3.71948 avg_loss = 4.08720\n",
      "epoch no.0 train no.45980  loss = 3.74260 avg_loss = 4.05011\n",
      "epoch no.0 train no.45990  loss = 3.36126 avg_loss = 4.02607\n",
      "epoch no.0 train no.46000  loss = 6.74754 avg_loss = 4.00694\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁오면', '▁생각', '나는', '▁노래', '</s>']\n",
      "여름이 오면 생각나는 노래</s>\n",
      "epoch no.0 train no.46010  loss = 3.25403 avg_loss = 3.98916\n",
      "epoch no.0 train no.46020  loss = 4.76349 avg_loss = 3.91833\n",
      "epoch no.0 train no.46030  loss = 4.60993 avg_loss = 3.90641\n",
      "epoch no.0 train no.46040  loss = 4.50860 avg_loss = 3.90065\n",
      "epoch no.0 train no.46050  loss = 4.36820 avg_loss = 3.95406\n",
      "epoch no.0 train no.46060  loss = 3.18250 avg_loss = 3.95964\n",
      "epoch no.0 train no.46070  loss = 6.12682 avg_loss = 3.97902\n",
      "epoch no.0 train no.46080  loss = 3.44234 avg_loss = 3.94306\n",
      "epoch no.0 train no.46090  loss = 3.70887 avg_loss = 3.97448\n",
      "epoch no.0 train no.46100  loss = 2.15242 avg_loss = 3.95984\n",
      "epoch no.0 train no.46110  loss = 3.47055 avg_loss = 3.92742\n",
      "epoch no.0 train no.46120  loss = 6.59939 avg_loss = 3.95326\n",
      "epoch no.0 train no.46130  loss = 4.12845 avg_loss = 3.99972\n",
      "epoch no.0 train no.46140  loss = 3.33887 avg_loss = 3.98243\n",
      "epoch no.0 train no.46150  loss = 3.82132 avg_loss = 3.89693\n",
      "epoch no.0 train no.46160  loss = 2.19243 avg_loss = 3.84574\n",
      "epoch no.0 train no.46170  loss = 5.90839 avg_loss = 3.80743\n",
      "epoch no.0 train no.46180  loss = 4.55680 avg_loss = 3.80721\n",
      "epoch no.0 train no.46190  loss = 3.15477 avg_loss = 3.77954\n",
      "epoch no.0 train no.46200  loss = 4.13010 avg_loss = 3.87745\n",
      "epoch no.0 train no.46210  loss = 4.43134 avg_loss = 3.83967\n",
      "epoch no.0 train no.46220  loss = 2.42686 avg_loss = 3.84948\n",
      "epoch no.0 train no.46230  loss = 4.08400 avg_loss = 3.83943\n",
      "epoch no.0 train no.46240  loss = 2.74067 avg_loss = 3.85158\n",
      "epoch no.0 train no.46250  loss = 3.68759 avg_loss = 3.82971\n",
      "epoch no.0 train no.46260  loss = 4.04841 avg_loss = 3.80508\n",
      "epoch no.0 train no.46270  loss = 3.39488 avg_loss = 3.81343\n",
      "epoch no.0 train no.46280  loss = 2.17982 avg_loss = 3.86054\n",
      "epoch no.0 train no.46290  loss = 6.24750 avg_loss = 3.90799\n",
      "epoch no.0 train no.46300  loss = 2.85231 avg_loss = 3.92372\n",
      "epoch no.0 train no.46310  loss = 6.00438 avg_loss = 3.93129\n",
      "epoch no.0 train no.46320  loss = 4.55382 avg_loss = 3.95781\n",
      "epoch no.0 train no.46330  loss = 5.45904 avg_loss = 4.02085\n",
      "epoch no.0 train no.46340  loss = 4.57129 avg_loss = 4.08066\n",
      "epoch no.0 train no.46350  loss = 4.31467 avg_loss = 4.03928\n",
      "epoch no.0 train no.46360  loss = 3.20308 avg_loss = 3.96930\n",
      "epoch no.0 train no.46370  loss = 5.01200 avg_loss = 4.00189\n",
      "epoch no.0 train no.46380  loss = 3.85609 avg_loss = 3.96490\n",
      "epoch no.0 train no.46390  loss = 4.40918 avg_loss = 3.97383\n",
      "epoch no.0 train no.46400  loss = 3.94658 avg_loss = 3.95894\n",
      "epoch no.0 train no.46410  loss = 5.69126 avg_loss = 4.01163\n",
      "epoch no.0 train no.46420  loss = 4.64778 avg_loss = 4.03188\n",
      "epoch no.0 train no.46430  loss = 5.70604 avg_loss = 4.03952\n",
      "epoch no.0 train no.46440  loss = 4.73331 avg_loss = 4.02338\n",
      "epoch no.0 train no.46450  loss = 3.60818 avg_loss = 4.03103\n",
      "epoch no.0 train no.46460  loss = 3.58969 avg_loss = 3.98232\n",
      "epoch no.0 train no.46470  loss = 4.20175 avg_loss = 4.02306\n",
      "epoch no.0 train no.46480  loss = 3.44607 avg_loss = 4.07194\n",
      "epoch no.0 train no.46490  loss = 3.76676 avg_loss = 4.05283\n",
      "epoch no.0 train no.46500  loss = 3.33988 avg_loss = 4.03861\n",
      "epoch no.0 train no.46510  loss = 4.57575 avg_loss = 4.01599\n",
      "epoch no.0 train no.46520  loss = 4.68110 avg_loss = 4.02610\n",
      "epoch no.0 train no.46530  loss = 2.37905 avg_loss = 4.05038\n",
      "epoch no.0 train no.46540  loss = 4.00562 avg_loss = 4.01946\n",
      "epoch no.0 train no.46550  loss = 2.66485 avg_loss = 3.97149\n",
      "epoch no.0 train no.46560  loss = 2.40554 avg_loss = 3.92739\n",
      "epoch no.0 train no.46570  loss = 4.98024 avg_loss = 3.93491\n",
      "epoch no.0 train no.46580  loss = 2.05493 avg_loss = 3.93407\n",
      "epoch no.0 train no.46590  loss = 3.21466 avg_loss = 3.88164\n",
      "epoch no.0 train no.46600  loss = 4.49288 avg_loss = 3.85635\n",
      "epoch no.0 train no.46610  loss = 3.33506 avg_loss = 3.84971\n",
      "epoch no.0 train no.46620  loss = 4.85418 avg_loss = 3.89392\n",
      "epoch no.0 train no.46630  loss = 4.80111 avg_loss = 3.90041\n",
      "epoch no.0 train no.46640  loss = 2.09207 avg_loss = 3.85541\n",
      "epoch no.0 train no.46650  loss = 3.36516 avg_loss = 3.86069\n",
      "epoch no.0 train no.46660  loss = 5.49841 avg_loss = 3.84150\n",
      "epoch no.0 train no.46670  loss = 3.58798 avg_loss = 3.84471\n",
      "epoch no.0 train no.46680  loss = 3.83299 avg_loss = 3.84550\n",
      "epoch no.0 train no.46690  loss = 1.44286 avg_loss = 3.83321\n",
      "epoch no.0 train no.46700  loss = 4.08883 avg_loss = 3.88359\n",
      "epoch no.0 train no.46710  loss = 4.38830 avg_loss = 3.91781\n",
      "epoch no.0 train no.46720  loss = 3.53912 avg_loss = 3.85518\n",
      "epoch no.0 train no.46730  loss = 2.79568 avg_loss = 3.80021\n",
      "epoch no.0 train no.46740  loss = 1.96085 avg_loss = 3.76505\n",
      "epoch no.0 train no.46750  loss = 2.88670 avg_loss = 3.75948\n",
      "epoch no.0 train no.46760  loss = 4.45899 avg_loss = 3.74175\n",
      "epoch no.0 train no.46770  loss = 4.73990 avg_loss = 3.78324\n",
      "epoch no.0 train no.46780  loss = 4.02919 avg_loss = 3.82353\n",
      "epoch no.0 train no.46790  loss = 4.34229 avg_loss = 3.78155\n",
      "epoch no.0 train no.46800  loss = 4.65027 avg_loss = 3.78749\n",
      "epoch no.0 train no.46810  loss = 3.38460 avg_loss = 3.79710\n",
      "epoch no.0 train no.46820  loss = 6.19392 avg_loss = 3.80998\n",
      "epoch no.0 train no.46830  loss = 3.08038 avg_loss = 3.81822\n",
      "epoch no.0 train no.46840  loss = 4.97370 avg_loss = 3.86338\n",
      "epoch no.0 train no.46850  loss = 4.44185 avg_loss = 3.86082\n",
      "epoch no.0 train no.46860  loss = 4.41373 avg_loss = 3.85914\n",
      "epoch no.0 train no.46870  loss = 3.85914 avg_loss = 3.85208\n",
      "epoch no.0 train no.46880  loss = 4.67017 avg_loss = 3.81508\n",
      "epoch no.0 train no.46890  loss = 4.81930 avg_loss = 3.81308\n",
      "epoch no.0 train no.46900  loss = 5.70539 avg_loss = 3.80101\n",
      "epoch no.0 train no.46910  loss = 4.36326 avg_loss = 3.78674\n",
      "epoch no.0 train no.46920  loss = 4.27426 avg_loss = 3.84653\n",
      "epoch no.0 train no.46930  loss = 3.32373 avg_loss = 3.81718\n",
      "epoch no.0 train no.46940  loss = 4.79571 avg_loss = 3.86729\n",
      "epoch no.0 train no.46950  loss = 4.22205 avg_loss = 3.90809\n",
      "epoch no.0 train no.46960  loss = 5.14997 avg_loss = 3.89528\n",
      "epoch no.0 train no.46970  loss = 2.84577 avg_loss = 3.87977\n",
      "epoch no.0 train no.46980  loss = 4.24862 avg_loss = 3.88831\n",
      "epoch no.0 train no.46990  loss = 3.43097 avg_loss = 3.91973\n",
      "epoch no.0 train no.47000  loss = 3.09312 avg_loss = 3.91919\n",
      "5\n",
      "to_tokens: ['▁', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 곡</s>\n",
      "epoch no.0 train no.47010  loss = 3.42784 avg_loss = 3.96903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.47020  loss = 5.87575 avg_loss = 3.97169\n",
      "epoch no.0 train no.47030  loss = 6.33982 avg_loss = 3.98520\n",
      "epoch no.0 train no.47040  loss = 5.19183 avg_loss = 3.99956\n",
      "epoch no.0 train no.47050  loss = 2.94884 avg_loss = 3.94074\n",
      "epoch no.0 train no.47060  loss = 4.57800 avg_loss = 3.97306\n",
      "epoch no.0 train no.47070  loss = 2.62324 avg_loss = 3.95722\n",
      "epoch no.0 train no.47080  loss = 3.53066 avg_loss = 3.92164\n",
      "epoch no.0 train no.47090  loss = 4.18949 avg_loss = 3.95071\n",
      "epoch no.0 train no.47100  loss = 4.75441 avg_loss = 4.03444\n",
      "epoch no.0 train no.47110  loss = 2.16205 avg_loss = 3.96074\n",
      "epoch no.0 train no.47120  loss = 2.28974 avg_loss = 3.95434\n",
      "epoch no.0 train no.47130  loss = 5.22987 avg_loss = 3.95644\n",
      "epoch no.0 train no.47140  loss = 2.93164 avg_loss = 4.02087\n",
      "epoch no.0 train no.47150  loss = 4.37117 avg_loss = 3.99733\n",
      "epoch no.0 train no.47160  loss = 3.20871 avg_loss = 3.92280\n",
      "epoch no.0 train no.47170  loss = 2.73318 avg_loss = 3.91407\n",
      "epoch no.0 train no.47180  loss = 1.87889 avg_loss = 3.88285\n",
      "epoch no.0 train no.47190  loss = 3.70057 avg_loss = 3.94994\n",
      "epoch no.0 train no.47200  loss = 3.56323 avg_loss = 3.98289\n",
      "epoch no.0 train no.47210  loss = 3.31004 avg_loss = 3.99145\n",
      "epoch no.0 train no.47220  loss = 3.62988 avg_loss = 4.03769\n",
      "epoch no.0 train no.47230  loss = 7.16883 avg_loss = 4.03212\n",
      "epoch no.0 train no.47240  loss = 3.41465 avg_loss = 4.00899\n",
      "epoch no.0 train no.47250  loss = 5.27179 avg_loss = 3.99006\n",
      "epoch no.0 train no.47260  loss = 6.22165 avg_loss = 3.96200\n",
      "epoch no.0 train no.47270  loss = 2.82032 avg_loss = 3.96562\n",
      "epoch no.0 train no.47280  loss = 4.26262 avg_loss = 3.95438\n",
      "epoch no.0 train no.47290  loss = 2.79819 avg_loss = 3.93915\n",
      "epoch no.0 train no.47300  loss = 4.10746 avg_loss = 3.91627\n",
      "epoch no.0 train no.47310  loss = 3.25834 avg_loss = 3.89693\n",
      "epoch no.0 train no.47320  loss = 3.33672 avg_loss = 3.88959\n",
      "epoch no.0 train no.47330  loss = 2.53684 avg_loss = 3.87868\n",
      "epoch no.0 train no.47340  loss = 2.50191 avg_loss = 3.85478\n",
      "epoch no.0 train no.47350  loss = 3.84568 avg_loss = 3.90034\n",
      "epoch no.0 train no.47360  loss = 3.37634 avg_loss = 3.89153\n",
      "epoch no.0 train no.47370  loss = 2.83985 avg_loss = 3.86770\n",
      "epoch no.0 train no.47380  loss = 3.64951 avg_loss = 3.92663\n",
      "epoch no.0 train no.47390  loss = 4.61749 avg_loss = 3.96922\n",
      "epoch no.0 train no.47400  loss = 3.47966 avg_loss = 3.98277\n",
      "epoch no.0 train no.47410  loss = 5.04881 avg_loss = 3.93270\n",
      "epoch no.0 train no.47420  loss = 5.34379 avg_loss = 4.01081\n",
      "epoch no.0 train no.47430  loss = 3.09672 avg_loss = 3.97474\n",
      "epoch no.0 train no.47440  loss = 5.88345 avg_loss = 4.04741\n",
      "epoch no.0 train no.47450  loss = 3.56897 avg_loss = 4.00804\n",
      "epoch no.0 train no.47460  loss = 2.62427 avg_loss = 4.03882\n",
      "epoch no.0 train no.47470  loss = 4.25452 avg_loss = 4.04801\n",
      "epoch no.0 train no.47480  loss = 2.97731 avg_loss = 4.04697\n",
      "epoch no.0 train no.47490  loss = 4.40082 avg_loss = 4.02673\n",
      "epoch no.0 train no.47500  loss = 3.51063 avg_loss = 4.01594\n",
      "epoch no.0 train no.47510  loss = 5.17326 avg_loss = 4.03110\n",
      "epoch no.0 train no.47520  loss = 4.24045 avg_loss = 4.00042\n",
      "epoch no.0 train no.47530  loss = 4.37730 avg_loss = 3.97158\n",
      "epoch no.0 train no.47540  loss = 6.05613 avg_loss = 4.01978\n",
      "epoch no.0 train no.47550  loss = 2.98748 avg_loss = 4.01253\n",
      "epoch no.0 train no.47560  loss = 2.87995 avg_loss = 4.01792\n",
      "epoch no.0 train no.47570  loss = 1.54231 avg_loss = 4.00511\n",
      "epoch no.0 train no.47580  loss = 6.80023 avg_loss = 4.02568\n",
      "epoch no.0 train no.47590  loss = 4.47428 avg_loss = 3.99675\n",
      "epoch no.0 train no.47600  loss = 4.81734 avg_loss = 3.99137\n",
      "epoch no.0 train no.47610  loss = 4.59937 avg_loss = 3.97965\n",
      "epoch no.0 train no.47620  loss = 4.98556 avg_loss = 4.03716\n",
      "epoch no.0 train no.47630  loss = 3.76848 avg_loss = 3.98407\n",
      "epoch no.0 train no.47640  loss = 2.63202 avg_loss = 3.94388\n",
      "epoch no.0 train no.47650  loss = 2.92815 avg_loss = 3.92846\n",
      "epoch no.0 train no.47660  loss = 2.72544 avg_loss = 3.83784\n",
      "epoch no.0 train no.47670  loss = 3.91901 avg_loss = 3.85825\n",
      "epoch no.0 train no.47680  loss = 3.50124 avg_loss = 3.83687\n",
      "epoch no.0 train no.47690  loss = 2.91301 avg_loss = 3.81959\n",
      "epoch no.0 train no.47700  loss = 2.05697 avg_loss = 3.82585\n",
      "epoch no.0 train no.47710  loss = 2.32892 avg_loss = 3.79824\n",
      "epoch no.0 train no.47720  loss = 4.71498 avg_loss = 3.80791\n",
      "epoch no.0 train no.47730  loss = 3.50651 avg_loss = 3.82657\n",
      "epoch no.0 train no.47740  loss = 4.78202 avg_loss = 3.85947\n",
      "epoch no.0 train no.47750  loss = 4.47302 avg_loss = 3.83176\n",
      "epoch no.0 train no.47760  loss = 2.81853 avg_loss = 3.84284\n",
      "epoch no.0 train no.47770  loss = 5.17123 avg_loss = 3.88009\n",
      "epoch no.0 train no.47780  loss = 5.73092 avg_loss = 3.88938\n",
      "epoch no.0 train no.47790  loss = 3.23727 avg_loss = 3.87476\n",
      "epoch no.0 train no.47800  loss = 2.85794 avg_loss = 3.88877\n",
      "epoch no.0 train no.47810  loss = 3.56595 avg_loss = 3.91745\n",
      "epoch no.0 train no.47820  loss = 2.65648 avg_loss = 3.88896\n",
      "epoch no.0 train no.47830  loss = 3.45348 avg_loss = 3.90016\n",
      "epoch no.0 train no.47840  loss = 1.86818 avg_loss = 3.90160\n",
      "epoch no.0 train no.47850  loss = 4.59079 avg_loss = 3.86820\n",
      "epoch no.0 train no.47860  loss = 5.77774 avg_loss = 3.87277\n",
      "epoch no.0 train no.47870  loss = 4.86325 avg_loss = 3.92964\n",
      "epoch no.0 train no.47880  loss = 3.54109 avg_loss = 4.00037\n",
      "epoch no.0 train no.47890  loss = 3.21159 avg_loss = 3.94551\n",
      "epoch no.0 train no.47900  loss = 3.68250 avg_loss = 3.97080\n",
      "epoch no.0 train no.47910  loss = 4.50733 avg_loss = 3.96355\n",
      "epoch no.0 train no.47920  loss = 5.16291 avg_loss = 3.99083\n",
      "epoch no.0 train no.47930  loss = 3.46898 avg_loss = 3.99153\n",
      "epoch no.0 train no.47940  loss = 4.26467 avg_loss = 4.01010\n",
      "epoch no.0 train no.47950  loss = 4.43132 avg_loss = 3.93614\n",
      "epoch no.0 train no.47960  loss = 4.90493 avg_loss = 3.93488\n",
      "epoch no.0 train no.47970  loss = 3.97047 avg_loss = 3.94863\n",
      "epoch no.0 train no.47980  loss = 5.96253 avg_loss = 3.96343\n",
      "epoch no.0 train no.47990  loss = 2.92157 avg_loss = 3.93960\n",
      "epoch no.0 train no.48000  loss = 4.84793 avg_loss = 3.88193\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁어울리는', '▁노래', '</s>']\n",
      "여름밤에 어울리는 노래</s>\n",
      "epoch no.0 train no.48010  loss = 4.91287 avg_loss = 3.91268\n",
      "epoch no.0 train no.48020  loss = 3.29102 avg_loss = 3.95270\n",
      "epoch no.0 train no.48030  loss = 4.90046 avg_loss = 3.96626\n",
      "epoch no.0 train no.48040  loss = 4.46110 avg_loss = 3.96856\n",
      "epoch no.0 train no.48050  loss = 2.49873 avg_loss = 3.94759\n",
      "epoch no.0 train no.48060  loss = 2.70788 avg_loss = 3.88676\n",
      "epoch no.0 train no.48070  loss = 2.84290 avg_loss = 3.91632\n",
      "epoch no.0 train no.48080  loss = 4.30062 avg_loss = 3.93635\n",
      "epoch no.0 train no.48090  loss = 3.04707 avg_loss = 4.02649\n",
      "epoch no.0 train no.48100  loss = 3.16087 avg_loss = 3.99810\n",
      "epoch no.0 train no.48110  loss = 2.85951 avg_loss = 3.96639\n",
      "epoch no.0 train no.48120  loss = 5.32572 avg_loss = 4.00389\n",
      "epoch no.0 train no.48130  loss = 5.18410 avg_loss = 4.03843\n",
      "epoch no.0 train no.48140  loss = 4.29439 avg_loss = 4.06340\n",
      "epoch no.0 train no.48150  loss = 4.27349 avg_loss = 4.06779\n",
      "epoch no.0 train no.48160  loss = 2.20344 avg_loss = 4.11101\n",
      "epoch no.0 train no.48170  loss = 2.70151 avg_loss = 4.12522\n",
      "epoch no.0 train no.48180  loss = 2.72175 avg_loss = 4.07988\n",
      "epoch no.0 train no.48190  loss = 5.28177 avg_loss = 4.09744\n",
      "epoch no.0 train no.48200  loss = 3.81571 avg_loss = 4.06536\n",
      "epoch no.0 train no.48210  loss = 4.73723 avg_loss = 4.05732\n",
      "epoch no.0 train no.48220  loss = 4.33967 avg_loss = 4.02363\n",
      "epoch no.0 train no.48230  loss = 4.03544 avg_loss = 3.99698\n",
      "epoch no.0 train no.48240  loss = 2.16181 avg_loss = 3.96562\n",
      "epoch no.0 train no.48250  loss = 4.78506 avg_loss = 3.97328\n",
      "epoch no.0 train no.48260  loss = 2.61114 avg_loss = 3.90850\n",
      "epoch no.0 train no.48270  loss = 4.52668 avg_loss = 3.88807\n",
      "epoch no.0 train no.48280  loss = 2.10167 avg_loss = 3.88757\n",
      "epoch no.0 train no.48290  loss = 4.93352 avg_loss = 3.93806\n",
      "epoch no.0 train no.48300  loss = 4.67956 avg_loss = 3.92612\n",
      "epoch no.0 train no.48310  loss = 3.49401 avg_loss = 3.89106\n",
      "epoch no.0 train no.48320  loss = 3.23948 avg_loss = 3.88033\n",
      "epoch no.0 train no.48330  loss = 4.43152 avg_loss = 3.81487\n",
      "epoch no.0 train no.48340  loss = 2.64560 avg_loss = 3.82723\n",
      "epoch no.0 train no.48350  loss = 5.34889 avg_loss = 3.80765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.48360  loss = 4.84154 avg_loss = 3.84791\n",
      "epoch no.0 train no.48370  loss = 4.43375 avg_loss = 3.86237\n",
      "epoch no.0 train no.48380  loss = 4.54853 avg_loss = 3.92880\n",
      "epoch no.0 train no.48390  loss = 2.76187 avg_loss = 3.91599\n",
      "epoch no.0 train no.48400  loss = 3.94731 avg_loss = 3.86320\n",
      "epoch no.0 train no.48410  loss = 2.65654 avg_loss = 3.86640\n",
      "epoch no.0 train no.48420  loss = 2.12463 avg_loss = 3.79729\n",
      "epoch no.0 train no.48430  loss = 3.20107 avg_loss = 3.78090\n",
      "epoch no.0 train no.48440  loss = 5.84650 avg_loss = 3.84470\n",
      "epoch no.0 train no.48450  loss = 2.69927 avg_loss = 3.85172\n",
      "epoch no.0 train no.48460  loss = 5.02455 avg_loss = 3.86089\n",
      "epoch no.0 train no.48470  loss = 3.19131 avg_loss = 3.91971\n",
      "epoch no.0 train no.48480  loss = 5.32473 avg_loss = 3.96887\n",
      "epoch no.0 train no.48490  loss = 2.62472 avg_loss = 3.94153\n",
      "epoch no.0 train no.48500  loss = 3.68145 avg_loss = 3.95847\n",
      "epoch no.0 train no.48510  loss = 2.58059 avg_loss = 3.94640\n",
      "epoch no.0 train no.48520  loss = 5.35345 avg_loss = 3.94676\n",
      "epoch no.0 train no.48530  loss = 6.16429 avg_loss = 3.96497\n",
      "epoch no.0 train no.48540  loss = 4.04006 avg_loss = 3.95999\n",
      "epoch no.0 train no.48550  loss = 4.85308 avg_loss = 3.96981\n",
      "epoch no.0 train no.48560  loss = 3.94553 avg_loss = 3.92802\n",
      "epoch no.0 train no.48570  loss = 6.44440 avg_loss = 3.94695\n",
      "epoch no.0 train no.48580  loss = 3.10652 avg_loss = 3.93869\n",
      "epoch no.0 train no.48590  loss = 2.63308 avg_loss = 3.93833\n",
      "epoch no.0 train no.48600  loss = 3.39710 avg_loss = 3.90820\n",
      "epoch no.0 train no.48610  loss = 2.32747 avg_loss = 3.85184\n",
      "epoch no.0 train no.48620  loss = 2.94463 avg_loss = 3.80338\n",
      "epoch no.0 train no.48630  loss = 6.80228 avg_loss = 3.82786\n",
      "epoch no.0 train no.48640  loss = 3.21052 avg_loss = 3.80672\n",
      "epoch no.0 train no.48650  loss = 3.05455 avg_loss = 3.80821\n",
      "epoch no.0 train no.48660  loss = 2.63959 avg_loss = 3.75730\n",
      "epoch no.0 train no.48670  loss = 6.27011 avg_loss = 3.83669\n",
      "epoch no.0 train no.48680  loss = 4.46991 avg_loss = 3.77717\n",
      "epoch no.0 train no.48690  loss = 4.84956 avg_loss = 3.81600\n",
      "epoch no.0 train no.48700  loss = 3.58789 avg_loss = 3.82154\n",
      "epoch no.0 train no.48710  loss = 2.77709 avg_loss = 3.84032\n",
      "epoch no.0 train no.48720  loss = 2.28539 avg_loss = 3.83895\n",
      "epoch no.0 train no.48730  loss = 3.61831 avg_loss = 3.82668\n",
      "epoch no.0 train no.48740  loss = 3.56317 avg_loss = 3.75332\n",
      "epoch no.0 train no.48750  loss = 2.30958 avg_loss = 3.77017\n",
      "epoch no.0 train no.48760  loss = 2.58711 avg_loss = 3.75754\n",
      "epoch no.0 train no.48770  loss = 3.93326 avg_loss = 3.77231\n",
      "epoch no.0 train no.48780  loss = 3.79300 avg_loss = 3.78663\n",
      "epoch no.0 train no.48790  loss = 4.68878 avg_loss = 3.76772\n",
      "epoch no.0 train no.48800  loss = 4.53938 avg_loss = 3.80379\n",
      "epoch no.0 train no.48810  loss = 4.19544 avg_loss = 3.79342\n",
      "epoch no.0 train no.48820  loss = 2.46723 avg_loss = 3.79071\n",
      "epoch no.0 train no.48830  loss = 3.47032 avg_loss = 3.77805\n",
      "epoch no.0 train no.48840  loss = 3.52935 avg_loss = 3.80453\n",
      "epoch no.0 train no.48850  loss = 2.70292 avg_loss = 3.83844\n",
      "epoch no.0 train no.48860  loss = 3.19671 avg_loss = 3.86851\n",
      "epoch no.0 train no.48870  loss = 1.62837 avg_loss = 3.80791\n",
      "epoch no.0 train no.48880  loss = 2.51175 avg_loss = 3.81461\n",
      "epoch no.0 train no.48890  loss = 4.83793 avg_loss = 3.88344\n",
      "epoch no.0 train no.48900  loss = 4.82802 avg_loss = 3.86274\n",
      "epoch no.0 train no.48910  loss = 3.13945 avg_loss = 3.85041\n",
      "epoch no.0 train no.48920  loss = 4.48771 avg_loss = 3.81377\n",
      "epoch no.0 train no.48930  loss = 5.10476 avg_loss = 3.85167\n",
      "epoch no.0 train no.48940  loss = 2.89827 avg_loss = 3.88533\n",
      "epoch no.0 train no.48950  loss = 4.76777 avg_loss = 3.87231\n",
      "epoch no.0 train no.48960  loss = 4.50057 avg_loss = 3.82114\n",
      "epoch no.0 train no.48970  loss = 3.36875 avg_loss = 3.79249\n",
      "epoch no.0 train no.48980  loss = 3.18531 avg_loss = 3.81564\n",
      "epoch no.0 train no.48990  loss = 3.89323 avg_loss = 3.80740\n",
      "epoch no.0 train no.49000  loss = 8.00736 avg_loss = 3.87818\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '의', '▁재즈', '▁자극하는', '줄', '▁노래', '▁발라', '</s>']\n",
      "여름밤의 감성을 채워줄 감성음악</s>\n",
      "epoch no.0 train no.49010  loss = 2.00597 avg_loss = 3.91361\n",
      "epoch no.0 train no.49020  loss = 6.39805 avg_loss = 3.97189\n",
      "epoch no.0 train no.49030  loss = 5.29604 avg_loss = 4.02152\n",
      "epoch no.0 train no.49040  loss = 2.59715 avg_loss = 3.98113\n",
      "epoch no.0 train no.49050  loss = 4.50576 avg_loss = 3.98692\n",
      "epoch no.0 train no.49060  loss = 4.20724 avg_loss = 3.91754\n",
      "epoch no.0 train no.49070  loss = 3.51180 avg_loss = 3.87651\n",
      "epoch no.0 train no.49080  loss = 4.86032 avg_loss = 3.91329\n",
      "epoch no.0 train no.49090  loss = 3.82772 avg_loss = 3.89894\n",
      "epoch no.0 train no.49100  loss = 2.61983 avg_loss = 3.88794\n",
      "epoch no.0 train no.49110  loss = 4.33684 avg_loss = 3.88461\n",
      "epoch no.0 train no.49120  loss = 3.32502 avg_loss = 3.88160\n",
      "epoch no.0 train no.49130  loss = 2.93208 avg_loss = 3.89854\n",
      "epoch no.0 train no.49140  loss = 4.49954 avg_loss = 3.90586\n",
      "epoch no.0 train no.49150  loss = 5.31676 avg_loss = 3.92623\n",
      "epoch no.0 train no.49160  loss = 4.29404 avg_loss = 3.86559\n",
      "epoch no.0 train no.49170  loss = 4.93336 avg_loss = 3.87679\n",
      "epoch no.0 train no.49180  loss = 2.50288 avg_loss = 3.90121\n",
      "epoch no.0 train no.49190  loss = 4.94140 avg_loss = 4.00660\n",
      "epoch no.0 train no.49200  loss = 2.73657 avg_loss = 3.97840\n",
      "epoch no.0 train no.49210  loss = 2.03823 avg_loss = 3.92319\n",
      "epoch no.0 train no.49220  loss = 4.12788 avg_loss = 3.94628\n",
      "epoch no.0 train no.49230  loss = 4.63635 avg_loss = 3.96685\n",
      "epoch no.0 train no.49240  loss = 5.04576 avg_loss = 3.89648\n",
      "epoch no.0 train no.49250  loss = 3.42532 avg_loss = 3.89028\n",
      "epoch no.0 train no.49260  loss = 4.82605 avg_loss = 3.94640\n",
      "epoch no.0 train no.49270  loss = 3.04256 avg_loss = 3.88488\n",
      "epoch no.0 train no.49280  loss = 3.68458 avg_loss = 3.84793\n",
      "epoch no.0 train no.49290  loss = 2.61950 avg_loss = 3.84077\n",
      "epoch no.0 train no.49300  loss = 4.81812 avg_loss = 3.88547\n",
      "epoch no.0 train no.49310  loss = 2.82156 avg_loss = 3.89691\n",
      "epoch no.0 train no.49320  loss = 2.82003 avg_loss = 3.88588\n",
      "epoch no.0 train no.49330  loss = 2.54336 avg_loss = 3.84115\n",
      "epoch no.0 train no.49340  loss = 2.74544 avg_loss = 3.85492\n",
      "epoch no.0 train no.49350  loss = 1.92376 avg_loss = 3.81992\n",
      "epoch no.0 train no.49360  loss = 4.70467 avg_loss = 3.80754\n",
      "epoch no.0 train no.49370  loss = 4.89529 avg_loss = 3.82865\n",
      "epoch no.0 train no.49380  loss = 5.29209 avg_loss = 3.90490\n",
      "epoch no.0 train no.49390  loss = 5.14466 avg_loss = 3.90993\n",
      "epoch no.0 train no.49400  loss = 5.18035 avg_loss = 3.89383\n",
      "epoch no.0 train no.49410  loss = 4.38166 avg_loss = 3.89760\n",
      "epoch no.0 train no.49420  loss = 1.81546 avg_loss = 3.87735\n",
      "epoch no.0 train no.49430  loss = 6.70122 avg_loss = 3.91805\n",
      "epoch no.0 train no.49440  loss = 5.20138 avg_loss = 3.89746\n",
      "epoch no.0 train no.49450  loss = 2.63706 avg_loss = 3.80102\n",
      "epoch no.0 train no.49460  loss = 4.21408 avg_loss = 3.89173\n",
      "epoch no.0 train no.49470  loss = 3.51142 avg_loss = 4.01123\n",
      "epoch no.0 train no.49480  loss = 4.19358 avg_loss = 3.99294\n",
      "epoch no.0 train no.49490  loss = 5.67466 avg_loss = 3.94156\n",
      "epoch no.0 train no.49500  loss = 5.49586 avg_loss = 3.89151\n",
      "epoch no.0 train no.49510  loss = 5.18155 avg_loss = 3.89768\n",
      "epoch no.0 train no.49520  loss = 2.30130 avg_loss = 3.85523\n",
      "epoch no.0 train no.49530  loss = 3.13819 avg_loss = 3.86239\n",
      "epoch no.0 train no.49540  loss = 4.15141 avg_loss = 3.88513\n",
      "epoch no.0 train no.49550  loss = 2.97556 avg_loss = 3.92248\n",
      "epoch no.0 train no.49560  loss = 4.12334 avg_loss = 3.88303\n",
      "epoch no.0 train no.49570  loss = 4.41388 avg_loss = 3.88659\n",
      "epoch no.0 train no.49580  loss = 2.02581 avg_loss = 3.88714\n",
      "epoch no.0 train no.49590  loss = 2.80207 avg_loss = 3.89550\n",
      "epoch no.0 train no.49600  loss = 6.85666 avg_loss = 3.98769\n",
      "epoch no.0 train no.49610  loss = 2.94799 avg_loss = 3.94815\n",
      "epoch no.0 train no.49620  loss = 5.53878 avg_loss = 3.97237\n",
      "epoch no.0 train no.49630  loss = 2.89219 avg_loss = 3.92626\n",
      "epoch no.0 train no.49640  loss = 2.41118 avg_loss = 3.87926\n",
      "epoch no.0 train no.49650  loss = 3.54697 avg_loss = 3.83149\n",
      "epoch no.0 train no.49660  loss = 4.52120 avg_loss = 3.83816\n",
      "epoch no.0 train no.49670  loss = 4.53103 avg_loss = 3.83314\n",
      "epoch no.0 train no.49680  loss = 3.25996 avg_loss = 3.87954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.49690  loss = 4.61194 avg_loss = 3.87000\n",
      "epoch no.0 train no.49700  loss = 4.69408 avg_loss = 3.83155\n",
      "epoch no.0 train no.49710  loss = 3.84714 avg_loss = 3.84792\n",
      "epoch no.0 train no.49720  loss = 4.40408 avg_loss = 3.85173\n",
      "epoch no.0 train no.49730  loss = 4.75324 avg_loss = 3.93624\n",
      "epoch no.0 train no.49740  loss = 3.86450 avg_loss = 3.91637\n",
      "epoch no.0 train no.49750  loss = 4.84381 avg_loss = 3.88707\n",
      "epoch no.0 train no.49760  loss = 4.78926 avg_loss = 3.90905\n",
      "epoch no.0 train no.49770  loss = 3.65736 avg_loss = 3.88498\n",
      "epoch no.0 train no.49780  loss = 2.87961 avg_loss = 3.89371\n",
      "epoch no.0 train no.49790  loss = 3.78704 avg_loss = 3.87764\n",
      "epoch no.0 train no.49800  loss = 2.31245 avg_loss = 3.78215\n",
      "epoch no.0 train no.49810  loss = 5.05175 avg_loss = 3.83022\n",
      "epoch no.0 train no.49820  loss = 5.60151 avg_loss = 3.91029\n",
      "epoch no.0 train no.49830  loss = 3.77058 avg_loss = 3.94066\n",
      "epoch no.0 train no.49840  loss = 2.59019 avg_loss = 3.95981\n",
      "epoch no.0 train no.49850  loss = 2.55801 avg_loss = 3.95740\n",
      "epoch no.0 train no.49860  loss = 4.93422 avg_loss = 3.97177\n",
      "epoch no.0 train no.49870  loss = 3.93209 avg_loss = 3.92885\n",
      "epoch no.0 train no.49880  loss = 4.28339 avg_loss = 3.89843\n",
      "epoch no.0 train no.49890  loss = 5.01414 avg_loss = 3.93205\n",
      "epoch no.0 train no.49900  loss = 3.02098 avg_loss = 3.89655\n",
      "epoch no.0 train no.49910  loss = 3.45700 avg_loss = 3.87265\n",
      "epoch no.0 train no.49920  loss = 4.08038 avg_loss = 3.85368\n",
      "epoch no.0 train no.49930  loss = 2.71623 avg_loss = 3.84245\n",
      "epoch no.0 train no.49940  loss = 3.00925 avg_loss = 3.86657\n",
      "epoch no.0 train no.49950  loss = 7.80087 avg_loss = 3.90613\n",
      "epoch no.0 train no.49960  loss = 5.14074 avg_loss = 3.96313\n",
      "epoch no.0 train no.49970  loss = 2.85938 avg_loss = 4.01424\n",
      "epoch no.0 train no.49980  loss = 4.27193 avg_loss = 4.07656\n",
      "epoch no.0 train no.49990  loss = 2.55119 avg_loss = 4.04508\n",
      "epoch no.0 train no.50000  loss = 4.32121 avg_loss = 4.02332\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '</s>', '</s>']\n",
      "여름밤의 재즈 음악</s>\n",
      "epoch no.0 train no.50010  loss = 5.51793 avg_loss = 4.02293\n",
      "epoch no.0 train no.50020  loss = 3.54526 avg_loss = 4.00367\n",
      "epoch no.0 train no.50030  loss = 5.26948 avg_loss = 4.00900\n",
      "epoch no.0 train no.50040  loss = 4.03873 avg_loss = 4.05215\n",
      "epoch no.0 train no.50050  loss = 4.44836 avg_loss = 4.02236\n",
      "epoch no.0 train no.50060  loss = 4.46714 avg_loss = 4.02188\n",
      "epoch no.0 train no.50070  loss = 4.89877 avg_loss = 4.05657\n",
      "epoch no.0 train no.50080  loss = 3.68656 avg_loss = 4.01548\n",
      "epoch no.0 train no.50090  loss = 5.53449 avg_loss = 4.02710\n",
      "epoch no.0 train no.50100  loss = 2.79008 avg_loss = 4.00500\n",
      "epoch no.0 train no.50110  loss = 5.51879 avg_loss = 3.97510\n",
      "epoch no.0 train no.50120  loss = 3.54156 avg_loss = 3.96121\n",
      "epoch no.0 train no.50130  loss = 3.41434 avg_loss = 3.94356\n",
      "epoch no.0 train no.50140  loss = 2.43706 avg_loss = 3.98767\n",
      "epoch no.0 train no.50150  loss = 2.55929 avg_loss = 3.99370\n",
      "epoch no.0 train no.50160  loss = 3.56445 avg_loss = 4.03919\n",
      "epoch no.0 train no.50170  loss = 2.97480 avg_loss = 4.06522\n",
      "epoch no.0 train no.50180  loss = 2.71741 avg_loss = 4.02987\n",
      "epoch no.0 train no.50190  loss = 4.32841 avg_loss = 3.99542\n",
      "epoch no.0 train no.50200  loss = 2.43431 avg_loss = 3.99800\n",
      "epoch no.0 train no.50210  loss = 2.82461 avg_loss = 4.04689\n",
      "epoch no.0 train no.50220  loss = 4.58802 avg_loss = 4.03690\n",
      "epoch no.0 train no.50230  loss = 3.38658 avg_loss = 4.06383\n",
      "epoch no.0 train no.50240  loss = 7.60182 avg_loss = 4.03197\n",
      "epoch no.0 train no.50250  loss = 4.11971 avg_loss = 3.99601\n",
      "epoch no.0 train no.50260  loss = 2.41464 avg_loss = 3.94552\n",
      "epoch no.0 train no.50270  loss = 4.00055 avg_loss = 3.97235\n",
      "epoch no.0 train no.50280  loss = 3.91439 avg_loss = 3.98418\n",
      "epoch no.0 train no.50290  loss = 4.80247 avg_loss = 3.96804\n",
      "epoch no.0 train no.50300  loss = 4.73934 avg_loss = 3.99114\n",
      "epoch no.0 train no.50310  loss = 3.86078 avg_loss = 3.96581\n",
      "epoch no.0 train no.50320  loss = 3.34809 avg_loss = 3.90967\n",
      "epoch no.0 train no.50330  loss = 3.10929 avg_loss = 3.87961\n",
      "epoch no.0 train no.50340  loss = 5.18681 avg_loss = 3.86636\n",
      "epoch no.0 train no.50350  loss = 3.35059 avg_loss = 3.82569\n",
      "epoch no.0 train no.50360  loss = 5.41284 avg_loss = 3.85008\n",
      "epoch no.0 train no.50370  loss = 6.40771 avg_loss = 3.88440\n",
      "epoch no.0 train no.50380  loss = 1.90335 avg_loss = 3.84295\n",
      "epoch no.0 train no.50390  loss = 5.23665 avg_loss = 3.94982\n",
      "epoch no.0 train no.50400  loss = 5.96007 avg_loss = 3.92978\n",
      "epoch no.0 train no.50410  loss = 3.48299 avg_loss = 3.95990\n",
      "epoch no.0 train no.50420  loss = 4.05162 avg_loss = 3.96479\n",
      "epoch no.0 train no.50430  loss = 5.16591 avg_loss = 3.95557\n",
      "epoch no.0 train no.50440  loss = 5.21429 avg_loss = 3.92637\n",
      "epoch no.0 train no.50450  loss = 3.77362 avg_loss = 3.89532\n",
      "epoch no.0 train no.50460  loss = 4.12369 avg_loss = 3.88303\n",
      "epoch no.0 train no.50470  loss = 3.21670 avg_loss = 3.89455\n",
      "epoch no.0 train no.50480  loss = 4.75416 avg_loss = 3.85442\n",
      "epoch no.0 train no.50490  loss = 2.12452 avg_loss = 3.83179\n",
      "epoch no.0 train no.50500  loss = 4.20519 avg_loss = 3.85065\n",
      "epoch no.0 train no.50510  loss = 4.77679 avg_loss = 3.81644\n",
      "epoch no.0 train no.50520  loss = 2.65302 avg_loss = 3.79134\n",
      "epoch no.0 train no.50530  loss = 5.13589 avg_loss = 3.80478\n",
      "epoch no.0 train no.50540  loss = 2.00627 avg_loss = 3.81708\n",
      "epoch no.0 train no.50550  loss = 4.72605 avg_loss = 3.87079\n",
      "epoch no.0 train no.50560  loss = 3.52110 avg_loss = 3.89432\n",
      "epoch no.0 train no.50570  loss = 2.38897 avg_loss = 3.93694\n",
      "epoch no.0 train no.50580  loss = 3.20185 avg_loss = 3.91369\n",
      "epoch no.0 train no.50590  loss = 6.89035 avg_loss = 3.89967\n",
      "epoch no.0 train no.50600  loss = 4.08629 avg_loss = 3.87238\n",
      "epoch no.0 train no.50610  loss = 4.40917 avg_loss = 3.85630\n",
      "epoch no.0 train no.50620  loss = 5.56224 avg_loss = 3.83218\n",
      "epoch no.0 train no.50630  loss = 5.87664 avg_loss = 3.84833\n",
      "epoch no.0 train no.50640  loss = 3.88105 avg_loss = 3.80902\n",
      "epoch no.0 train no.50650  loss = 1.85915 avg_loss = 3.81142\n",
      "epoch no.0 train no.50660  loss = 3.26399 avg_loss = 3.80682\n",
      "epoch no.0 train no.50670  loss = 5.31906 avg_loss = 3.84561\n",
      "epoch no.0 train no.50680  loss = 2.55509 avg_loss = 3.86275\n",
      "epoch no.0 train no.50690  loss = 3.04047 avg_loss = 3.85375\n",
      "epoch no.0 train no.50700  loss = 4.11204 avg_loss = 3.81796\n",
      "epoch no.0 train no.50710  loss = 3.09177 avg_loss = 3.89767\n",
      "epoch no.0 train no.50720  loss = 5.22649 avg_loss = 3.86226\n",
      "epoch no.0 train no.50730  loss = 4.82621 avg_loss = 3.86687\n",
      "epoch no.0 train no.50740  loss = 5.25047 avg_loss = 3.86005\n",
      "epoch no.0 train no.50750  loss = 3.71990 avg_loss = 3.84727\n",
      "epoch no.0 train no.50760  loss = 3.12268 avg_loss = 3.83650\n",
      "epoch no.0 train no.50770  loss = 4.21130 avg_loss = 3.86873\n",
      "epoch no.0 train no.50780  loss = 3.48434 avg_loss = 3.83978\n",
      "epoch no.0 train no.50790  loss = 2.62247 avg_loss = 3.82619\n",
      "epoch no.0 train no.50800  loss = 6.44592 avg_loss = 3.80350\n",
      "epoch no.0 train no.50810  loss = 3.64579 avg_loss = 3.83596\n",
      "epoch no.0 train no.50820  loss = 6.82130 avg_loss = 3.88108\n",
      "epoch no.0 train no.50830  loss = 4.76360 avg_loss = 3.86754\n",
      "epoch no.0 train no.50840  loss = 2.32338 avg_loss = 3.88486\n",
      "epoch no.0 train no.50850  loss = 4.46559 avg_loss = 3.97136\n",
      "epoch no.0 train no.50860  loss = 4.69497 avg_loss = 3.97488\n",
      "epoch no.0 train no.50870  loss = 3.45418 avg_loss = 4.00609\n",
      "epoch no.0 train no.50880  loss = 3.40632 avg_loss = 4.00953\n",
      "epoch no.0 train no.50890  loss = 3.54765 avg_loss = 3.99254\n",
      "epoch no.0 train no.50900  loss = 3.01392 avg_loss = 3.97043\n",
      "epoch no.0 train no.50910  loss = 3.56344 avg_loss = 3.91231\n",
      "epoch no.0 train no.50920  loss = 4.89810 avg_loss = 3.90774\n",
      "epoch no.0 train no.50930  loss = 4.57947 avg_loss = 3.93478\n",
      "epoch no.0 train no.50940  loss = 3.53332 avg_loss = 3.91754\n",
      "epoch no.0 train no.50950  loss = 3.77610 avg_loss = 3.89231\n",
      "epoch no.0 train no.50960  loss = 4.04767 avg_loss = 3.88018\n",
      "epoch no.0 train no.50970  loss = 5.82513 avg_loss = 3.90731\n",
      "epoch no.0 train no.50980  loss = 3.86301 avg_loss = 3.85906\n",
      "epoch no.0 train no.50990  loss = 2.71808 avg_loss = 3.82673\n",
      "epoch no.0 train no.51000  loss = 6.48055 avg_loss = 3.89905\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣기', '▁노래', '</s>']\n",
      "여름밤에 듣는 노래</s>\n",
      "epoch no.0 train no.51010  loss = 4.50175 avg_loss = 3.88367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.51020  loss = 2.26563 avg_loss = 3.83987\n",
      "epoch no.0 train no.51030  loss = 3.68241 avg_loss = 3.84558\n",
      "epoch no.0 train no.51040  loss = 3.61959 avg_loss = 3.88126\n",
      "epoch no.0 train no.51050  loss = 2.68013 avg_loss = 3.90949\n",
      "epoch no.0 train no.51060  loss = 5.46991 avg_loss = 3.93358\n",
      "epoch no.0 train no.51070  loss = 3.50158 avg_loss = 3.93388\n",
      "epoch no.0 train no.51080  loss = 7.51031 avg_loss = 3.95237\n",
      "epoch no.0 train no.51090  loss = 4.78755 avg_loss = 3.88479\n",
      "epoch no.0 train no.51100  loss = 4.32300 avg_loss = 3.83072\n",
      "epoch no.0 train no.51110  loss = 2.56130 avg_loss = 3.82330\n",
      "epoch no.0 train no.51120  loss = 5.03712 avg_loss = 3.89230\n",
      "epoch no.0 train no.51130  loss = 5.48086 avg_loss = 3.88190\n",
      "epoch no.0 train no.51140  loss = 2.53306 avg_loss = 3.89825\n",
      "epoch no.0 train no.51150  loss = 5.28803 avg_loss = 3.95380\n",
      "epoch no.0 train no.51160  loss = 5.30137 avg_loss = 3.91088\n",
      "epoch no.0 train no.51170  loss = 4.93339 avg_loss = 3.94829\n",
      "epoch no.0 train no.51180  loss = 3.72343 avg_loss = 3.92345\n",
      "epoch no.0 train no.51190  loss = 3.19558 avg_loss = 3.94228\n",
      "epoch no.0 train no.51200  loss = 8.15126 avg_loss = 3.98219\n",
      "epoch no.0 train no.51210  loss = 3.88410 avg_loss = 3.97739\n",
      "epoch no.0 train no.51220  loss = 4.47354 avg_loss = 3.96543\n",
      "epoch no.0 train no.51230  loss = 4.46204 avg_loss = 3.95665\n",
      "epoch no.0 train no.51240  loss = 4.73049 avg_loss = 4.00000\n",
      "epoch no.0 train no.51250  loss = 3.59955 avg_loss = 3.95061\n",
      "epoch no.0 train no.51260  loss = 3.66711 avg_loss = 4.00308\n",
      "epoch no.0 train no.51270  loss = 3.22944 avg_loss = 4.00458\n",
      "epoch no.0 train no.51280  loss = 2.26282 avg_loss = 4.02398\n",
      "epoch no.0 train no.51290  loss = 3.57197 avg_loss = 4.00767\n",
      "epoch no.0 train no.51300  loss = 1.58552 avg_loss = 4.00157\n",
      "epoch no.0 train no.51310  loss = 6.90154 avg_loss = 4.01355\n",
      "epoch no.0 train no.51320  loss = 3.98436 avg_loss = 4.00411\n",
      "epoch no.0 train no.51330  loss = 2.64322 avg_loss = 3.96126\n",
      "epoch no.0 train no.51340  loss = 3.03170 avg_loss = 3.90274\n",
      "epoch no.0 train no.51350  loss = 3.02519 avg_loss = 3.96348\n",
      "epoch no.0 train no.51360  loss = 4.80307 avg_loss = 3.96743\n",
      "epoch no.0 train no.51370  loss = 2.81584 avg_loss = 3.97325\n",
      "epoch no.0 train no.51380  loss = 2.54137 avg_loss = 3.98554\n",
      "epoch no.0 train no.51390  loss = 4.71830 avg_loss = 4.01466\n",
      "epoch no.0 train no.51400  loss = 3.96701 avg_loss = 4.00601\n",
      "epoch no.0 train no.51410  loss = 5.31059 avg_loss = 4.01716\n",
      "epoch no.0 train no.51420  loss = 3.99626 avg_loss = 3.98903\n",
      "epoch no.0 train no.51430  loss = 6.39068 avg_loss = 4.00164\n",
      "epoch no.0 train no.51440  loss = 3.31704 avg_loss = 3.97162\n",
      "epoch no.0 train no.51450  loss = 4.88217 avg_loss = 4.00103\n",
      "epoch no.0 train no.51460  loss = 5.04170 avg_loss = 4.03300\n",
      "epoch no.0 train no.51470  loss = 1.98493 avg_loss = 3.94073\n",
      "epoch no.0 train no.51480  loss = 3.27939 avg_loss = 3.97764\n",
      "epoch no.0 train no.51490  loss = 5.44614 avg_loss = 3.96155\n",
      "epoch no.0 train no.51500  loss = 2.52394 avg_loss = 3.89954\n",
      "epoch no.0 train no.51510  loss = 3.28787 avg_loss = 3.87901\n",
      "epoch no.0 train no.51520  loss = 2.78589 avg_loss = 3.89736\n",
      "epoch no.0 train no.51530  loss = 2.90810 avg_loss = 3.92685\n",
      "epoch no.0 train no.51540  loss = 4.31666 avg_loss = 3.90858\n",
      "epoch no.0 train no.51550  loss = 4.35171 avg_loss = 3.88949\n",
      "epoch no.0 train no.51560  loss = 3.07975 avg_loss = 3.86167\n",
      "epoch no.0 train no.51570  loss = 3.02630 avg_loss = 3.90857\n",
      "epoch no.0 train no.51580  loss = 4.57321 avg_loss = 3.92003\n",
      "epoch no.0 train no.51590  loss = 3.21736 avg_loss = 3.86109\n",
      "epoch no.0 train no.51600  loss = 2.38323 avg_loss = 3.87054\n",
      "epoch no.0 train no.51610  loss = 5.15554 avg_loss = 3.89376\n",
      "epoch no.0 train no.51620  loss = 4.11396 avg_loss = 3.91354\n",
      "epoch no.0 train no.51630  loss = 3.13756 avg_loss = 3.87932\n",
      "epoch no.0 train no.51640  loss = 3.63844 avg_loss = 3.84277\n",
      "epoch no.0 train no.51650  loss = 2.98001 avg_loss = 3.87524\n",
      "epoch no.0 train no.51660  loss = 6.94585 avg_loss = 3.90358\n",
      "epoch no.0 train no.51670  loss = 4.50623 avg_loss = 3.94775\n",
      "epoch no.0 train no.51680  loss = 3.46648 avg_loss = 3.92538\n",
      "epoch no.0 train no.51690  loss = 5.31858 avg_loss = 3.96387\n",
      "epoch no.0 train no.51700  loss = 3.16699 avg_loss = 3.96103\n",
      "epoch no.0 train no.51710  loss = 1.83158 avg_loss = 3.96164\n",
      "epoch no.0 train no.51720  loss = 5.24630 avg_loss = 3.91918\n",
      "epoch no.0 train no.51730  loss = 2.48640 avg_loss = 3.88857\n",
      "epoch no.0 train no.51740  loss = 4.43556 avg_loss = 3.83760\n",
      "epoch no.0 train no.51750  loss = 3.80989 avg_loss = 3.81723\n",
      "epoch no.0 train no.51760  loss = 3.51901 avg_loss = 3.84816\n",
      "epoch no.0 train no.51770  loss = 2.88513 avg_loss = 3.85842\n",
      "epoch no.0 train no.51780  loss = 3.51353 avg_loss = 3.80220\n",
      "epoch no.0 train no.51790  loss = 3.82067 avg_loss = 3.78145\n",
      "epoch no.0 train no.51800  loss = 4.73554 avg_loss = 3.78864\n",
      "epoch no.0 train no.51810  loss = 5.29541 avg_loss = 3.81097\n",
      "epoch no.0 train no.51820  loss = 1.80629 avg_loss = 3.81416\n",
      "epoch no.0 train no.51830  loss = 3.69206 avg_loss = 3.80724\n",
      "epoch no.0 train no.51840  loss = 3.85898 avg_loss = 3.79323\n",
      "epoch no.0 train no.51850  loss = 2.43475 avg_loss = 3.78758\n",
      "epoch no.0 train no.51860  loss = 4.52946 avg_loss = 3.83845\n",
      "epoch no.0 train no.51870  loss = 3.30310 avg_loss = 3.84113\n",
      "epoch no.0 train no.51880  loss = 3.99603 avg_loss = 3.82518\n",
      "epoch no.0 train no.51890  loss = 5.38156 avg_loss = 3.85004\n",
      "epoch no.0 train no.51900  loss = 7.28406 avg_loss = 3.87315\n",
      "epoch no.0 train no.51910  loss = 3.22053 avg_loss = 3.95251\n",
      "epoch no.0 train no.51920  loss = 3.61891 avg_loss = 3.91132\n",
      "epoch no.0 train no.51930  loss = 3.22341 avg_loss = 3.89645\n",
      "epoch no.0 train no.51940  loss = 3.85298 avg_loss = 3.96308\n",
      "epoch no.0 train no.51950  loss = 2.57423 avg_loss = 3.92102\n",
      "epoch no.0 train no.51960  loss = 3.18812 avg_loss = 3.90979\n",
      "epoch no.0 train no.51970  loss = 2.28349 avg_loss = 3.89726\n",
      "epoch no.0 train no.51980  loss = 3.89153 avg_loss = 3.89182\n",
      "epoch no.0 train no.51990  loss = 4.28323 avg_loss = 3.90289\n",
      "epoch no.0 train no.52000  loss = 5.73671 avg_loss = 3.94737\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '에', '에', '▁딱', '인', '▁노래', '한', '▁음악', '</s>']\n",
      "여름밤 드라이브에 딱인 청량한 음악</s>\n",
      "epoch no.0 train no.52010  loss = 5.02985 avg_loss = 3.89767\n",
      "epoch no.0 train no.52020  loss = 4.18302 avg_loss = 3.89817\n",
      "epoch no.0 train no.52030  loss = 5.07878 avg_loss = 3.96081\n",
      "epoch no.0 train no.52040  loss = 4.06595 avg_loss = 3.98442\n",
      "epoch no.0 train no.52050  loss = 4.07106 avg_loss = 3.98583\n",
      "epoch no.0 train no.52060  loss = 2.96131 avg_loss = 3.91621\n",
      "epoch no.0 train no.52070  loss = 2.31309 avg_loss = 3.92328\n",
      "epoch no.0 train no.52080  loss = 2.98619 avg_loss = 3.93450\n",
      "epoch no.0 train no.52090  loss = 3.18423 avg_loss = 3.93721\n",
      "epoch no.0 train no.52100  loss = 4.29650 avg_loss = 3.92693\n",
      "epoch no.0 train no.52110  loss = 4.10971 avg_loss = 3.94008\n",
      "epoch no.0 train no.52120  loss = 4.49025 avg_loss = 3.95024\n",
      "epoch no.0 train no.52130  loss = 3.33886 avg_loss = 3.98111\n",
      "epoch no.0 train no.52140  loss = 3.24752 avg_loss = 3.90406\n",
      "epoch no.0 train no.52150  loss = 3.01358 avg_loss = 3.84764\n",
      "epoch no.0 train no.52160  loss = 1.21282 avg_loss = 3.78535\n",
      "epoch no.0 train no.52170  loss = 2.77881 avg_loss = 3.81305\n",
      "epoch no.0 train no.52180  loss = 4.03730 avg_loss = 3.82915\n",
      "epoch no.0 train no.52190  loss = 2.10073 avg_loss = 3.82160\n",
      "epoch no.0 train no.52200  loss = 4.86624 avg_loss = 3.87936\n",
      "epoch no.0 train no.52210  loss = 3.13263 avg_loss = 3.81283\n",
      "epoch no.0 train no.52220  loss = 3.58532 avg_loss = 3.82911\n",
      "epoch no.0 train no.52230  loss = 4.48423 avg_loss = 3.83442\n",
      "epoch no.0 train no.52240  loss = 3.31377 avg_loss = 3.90116\n",
      "epoch no.0 train no.52250  loss = 2.61185 avg_loss = 3.89808\n",
      "epoch no.0 train no.52260  loss = 4.57187 avg_loss = 3.95701\n",
      "epoch no.0 train no.52270  loss = 4.53548 avg_loss = 3.93964\n",
      "epoch no.0 train no.52280  loss = 3.49358 avg_loss = 3.90141\n",
      "epoch no.0 train no.52290  loss = 1.92089 avg_loss = 3.88739\n",
      "epoch no.0 train no.52300  loss = 4.08614 avg_loss = 3.89539\n",
      "epoch no.0 train no.52310  loss = 4.18805 avg_loss = 4.02301\n",
      "epoch no.0 train no.52320  loss = 3.04536 avg_loss = 4.02243\n",
      "epoch no.0 train no.52330  loss = 6.30091 avg_loss = 4.02642\n",
      "epoch no.0 train no.52340  loss = 5.28285 avg_loss = 4.04382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.52350  loss = 4.09275 avg_loss = 4.10017\n",
      "epoch no.0 train no.52360  loss = 5.36140 avg_loss = 4.09865\n",
      "epoch no.0 train no.52370  loss = 2.95138 avg_loss = 4.09186\n",
      "epoch no.0 train no.52380  loss = 3.29748 avg_loss = 4.18483\n",
      "epoch no.0 train no.52390  loss = 2.71035 avg_loss = 4.14775\n",
      "epoch no.0 train no.52400  loss = 3.99191 avg_loss = 4.09051\n",
      "epoch no.0 train no.52410  loss = 3.20435 avg_loss = 4.02955\n",
      "epoch no.0 train no.52420  loss = 3.30329 avg_loss = 4.04239\n",
      "epoch no.0 train no.52430  loss = 3.07023 avg_loss = 4.02239\n",
      "epoch no.0 train no.52440  loss = 2.78750 avg_loss = 3.94406\n",
      "epoch no.0 train no.52450  loss = 3.45092 avg_loss = 3.91766\n",
      "epoch no.0 train no.52460  loss = 3.27477 avg_loss = 3.85663\n",
      "epoch no.0 train no.52470  loss = 3.54940 avg_loss = 3.89186\n",
      "epoch no.0 train no.52480  loss = 3.36666 avg_loss = 3.84160\n",
      "epoch no.0 train no.52490  loss = 3.32363 avg_loss = 3.84889\n",
      "epoch no.0 train no.52500  loss = 2.70994 avg_loss = 3.85231\n",
      "epoch no.0 train no.52510  loss = 4.84702 avg_loss = 3.83387\n",
      "epoch no.0 train no.52520  loss = 1.92545 avg_loss = 3.81997\n",
      "epoch no.0 train no.52530  loss = 6.69484 avg_loss = 3.87878\n",
      "epoch no.0 train no.52540  loss = 2.70649 avg_loss = 3.85167\n",
      "epoch no.0 train no.52550  loss = 6.50500 avg_loss = 3.91710\n",
      "epoch no.0 train no.52560  loss = 4.10825 avg_loss = 3.91540\n",
      "epoch no.0 train no.52570  loss = 3.44576 avg_loss = 3.91270\n",
      "epoch no.0 train no.52580  loss = 3.71309 avg_loss = 3.91436\n",
      "epoch no.0 train no.52590  loss = 4.18780 avg_loss = 3.91635\n",
      "epoch no.0 train no.52600  loss = 3.78544 avg_loss = 3.90712\n",
      "epoch no.0 train no.52610  loss = 2.56590 avg_loss = 3.90214\n",
      "epoch no.0 train no.52620  loss = 4.29405 avg_loss = 3.93453\n",
      "epoch no.0 train no.52630  loss = 4.10137 avg_loss = 3.97092\n",
      "epoch no.0 train no.52640  loss = 4.85847 avg_loss = 3.89913\n",
      "epoch no.0 train no.52650  loss = 4.23111 avg_loss = 3.97404\n",
      "epoch no.0 train no.52660  loss = 4.79733 avg_loss = 3.94635\n",
      "epoch no.0 train no.52670  loss = 6.06708 avg_loss = 4.00762\n",
      "epoch no.0 train no.52680  loss = 2.54194 avg_loss = 4.01897\n",
      "epoch no.0 train no.52690  loss = 3.19279 avg_loss = 3.95300\n",
      "epoch no.0 train no.52700  loss = 3.95062 avg_loss = 3.96817\n",
      "epoch no.0 train no.52710  loss = 3.24117 avg_loss = 4.03604\n",
      "epoch no.0 train no.52720  loss = 4.64782 avg_loss = 4.04402\n",
      "epoch no.0 train no.52730  loss = 3.59143 avg_loss = 4.02305\n",
      "epoch no.0 train no.52740  loss = 1.99423 avg_loss = 3.92503\n",
      "epoch no.0 train no.52750  loss = 4.80969 avg_loss = 3.93415\n",
      "epoch no.0 train no.52760  loss = 3.68675 avg_loss = 3.87273\n",
      "epoch no.0 train no.52770  loss = 4.97539 avg_loss = 3.87295\n",
      "epoch no.0 train no.52780  loss = 4.79903 avg_loss = 3.90911\n",
      "epoch no.0 train no.52790  loss = 4.93933 avg_loss = 3.85416\n",
      "epoch no.0 train no.52800  loss = 2.79469 avg_loss = 3.83158\n",
      "epoch no.0 train no.52810  loss = 4.22186 avg_loss = 3.90709\n",
      "epoch no.0 train no.52820  loss = 4.18218 avg_loss = 3.94374\n",
      "epoch no.0 train no.52830  loss = 4.27735 avg_loss = 3.98382\n",
      "epoch no.0 train no.52840  loss = 4.52020 avg_loss = 3.93131\n",
      "epoch no.0 train no.52850  loss = 2.10400 avg_loss = 3.93088\n",
      "epoch no.0 train no.52860  loss = 3.56032 avg_loss = 3.95276\n",
      "epoch no.0 train no.52870  loss = 3.51845 avg_loss = 3.92890\n",
      "epoch no.0 train no.52880  loss = 2.57845 avg_loss = 3.90669\n",
      "epoch no.0 train no.52890  loss = 6.77717 avg_loss = 3.89771\n",
      "epoch no.0 train no.52900  loss = 1.75463 avg_loss = 3.83644\n",
      "epoch no.0 train no.52910  loss = 4.61649 avg_loss = 3.85745\n",
      "epoch no.0 train no.52920  loss = 2.92572 avg_loss = 3.87355\n",
      "epoch no.0 train no.52930  loss = 4.18964 avg_loss = 3.86916\n",
      "epoch no.0 train no.52940  loss = 4.40434 avg_loss = 3.86800\n",
      "epoch no.0 train no.52950  loss = 3.83752 avg_loss = 3.86254\n",
      "epoch no.0 train no.52960  loss = 3.40831 avg_loss = 3.84753\n",
      "epoch no.0 train no.52970  loss = 3.88470 avg_loss = 3.87373\n",
      "epoch no.0 train no.52980  loss = 1.55164 avg_loss = 3.88334\n",
      "epoch no.0 train no.52990  loss = 4.30549 avg_loss = 3.85358\n",
      "epoch no.0 train no.53000  loss = 3.62908 avg_loss = 3.87720\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '힙', '▁노래', '</s>']\n",
      "여름밤에 어울리는 감성적인 노래</s>\n",
      "epoch no.0 train no.53010  loss = 3.71917 avg_loss = 3.85517\n",
      "epoch no.0 train no.53020  loss = 5.33403 avg_loss = 3.80915\n",
      "epoch no.0 train no.53030  loss = 3.07011 avg_loss = 3.85026\n",
      "epoch no.0 train no.53040  loss = 4.53631 avg_loss = 3.88929\n",
      "epoch no.0 train no.53050  loss = 5.17812 avg_loss = 3.85786\n",
      "epoch no.0 train no.53060  loss = 2.56267 avg_loss = 3.86493\n",
      "epoch no.0 train no.53070  loss = 2.96338 avg_loss = 3.84878\n",
      "epoch no.0 train no.53080  loss = 2.40306 avg_loss = 3.89314\n",
      "epoch no.0 train no.53090  loss = 3.31966 avg_loss = 3.92210\n",
      "epoch no.0 train no.53100  loss = 2.41868 avg_loss = 3.90279\n",
      "epoch no.0 train no.53110  loss = 3.73351 avg_loss = 3.87038\n",
      "epoch no.0 train no.53120  loss = 3.32124 avg_loss = 3.83826\n",
      "epoch no.0 train no.53130  loss = 3.91614 avg_loss = 3.77125\n",
      "epoch no.0 train no.53140  loss = 4.68835 avg_loss = 3.77394\n",
      "epoch no.0 train no.53150  loss = 5.77468 avg_loss = 3.78483\n",
      "epoch no.0 train no.53160  loss = 3.27368 avg_loss = 3.76344\n",
      "epoch no.0 train no.53170  loss = 2.25950 avg_loss = 3.84065\n",
      "epoch no.0 train no.53180  loss = 3.57699 avg_loss = 3.87076\n",
      "epoch no.0 train no.53190  loss = 3.18249 avg_loss = 3.91236\n",
      "epoch no.0 train no.53200  loss = 4.87799 avg_loss = 3.96730\n",
      "epoch no.0 train no.53210  loss = 5.39841 avg_loss = 3.95960\n",
      "epoch no.0 train no.53220  loss = 3.25967 avg_loss = 3.97606\n",
      "epoch no.0 train no.53230  loss = 2.74597 avg_loss = 3.98962\n",
      "epoch no.0 train no.53240  loss = 5.89186 avg_loss = 3.99509\n",
      "epoch no.0 train no.53250  loss = 4.66357 avg_loss = 4.01338\n",
      "epoch no.0 train no.53260  loss = 4.90317 avg_loss = 3.93561\n",
      "epoch no.0 train no.53270  loss = 2.97960 avg_loss = 3.90847\n",
      "epoch no.0 train no.53280  loss = 3.32842 avg_loss = 3.87071\n",
      "epoch no.0 train no.53290  loss = 5.37182 avg_loss = 3.85350\n",
      "epoch no.0 train no.53300  loss = 2.25220 avg_loss = 3.91097\n",
      "epoch no.0 train no.53310  loss = 4.70878 avg_loss = 3.89602\n",
      "epoch no.0 train no.53320  loss = 3.27570 avg_loss = 3.96388\n",
      "epoch no.0 train no.53330  loss = 2.60201 avg_loss = 3.93844\n",
      "epoch no.0 train no.53340  loss = 2.30037 avg_loss = 3.86160\n",
      "epoch no.0 train no.53350  loss = 2.08556 avg_loss = 3.85441\n",
      "epoch no.0 train no.53360  loss = 3.82116 avg_loss = 3.81621\n",
      "epoch no.0 train no.53370  loss = 2.63340 avg_loss = 3.84967\n",
      "epoch no.0 train no.53380  loss = 3.55901 avg_loss = 3.90520\n",
      "epoch no.0 train no.53390  loss = 3.52066 avg_loss = 3.92311\n",
      "epoch no.0 train no.53400  loss = 3.00809 avg_loss = 3.91303\n",
      "epoch no.0 train no.53410  loss = 6.17962 avg_loss = 3.85216\n",
      "epoch no.0 train no.53420  loss = 3.43763 avg_loss = 3.81875\n",
      "epoch no.0 train no.53430  loss = 3.52888 avg_loss = 3.76968\n",
      "epoch no.0 train no.53440  loss = 2.45645 avg_loss = 3.73206\n",
      "epoch no.0 train no.53450  loss = 6.28557 avg_loss = 3.78308\n",
      "epoch no.0 train no.53460  loss = 5.13046 avg_loss = 3.81256\n",
      "epoch no.0 train no.53470  loss = 6.22626 avg_loss = 3.88684\n",
      "epoch no.0 train no.53480  loss = 4.85910 avg_loss = 3.91781\n",
      "epoch no.0 train no.53490  loss = 2.31378 avg_loss = 3.84716\n",
      "epoch no.0 train no.53500  loss = 2.74660 avg_loss = 3.76489\n",
      "epoch no.0 train no.53510  loss = 1.61044 avg_loss = 3.75290\n",
      "epoch no.0 train no.53520  loss = 3.43014 avg_loss = 3.83855\n",
      "epoch no.0 train no.53530  loss = 4.63516 avg_loss = 3.90495\n",
      "epoch no.0 train no.53540  loss = 3.53119 avg_loss = 3.87306\n",
      "epoch no.0 train no.53550  loss = 6.20050 avg_loss = 3.86763\n",
      "epoch no.0 train no.53560  loss = 3.53949 avg_loss = 3.87962\n",
      "epoch no.0 train no.53570  loss = 4.86293 avg_loss = 3.92552\n",
      "epoch no.0 train no.53580  loss = 3.09286 avg_loss = 3.92859\n",
      "epoch no.0 train no.53590  loss = 4.82552 avg_loss = 3.99020\n",
      "epoch no.0 train no.53600  loss = 5.53800 avg_loss = 3.96900\n",
      "epoch no.0 train no.53610  loss = 4.40197 avg_loss = 3.97488\n",
      "epoch no.0 train no.53620  loss = 3.51598 avg_loss = 3.91800\n",
      "epoch no.0 train no.53630  loss = 6.87188 avg_loss = 3.91993\n",
      "epoch no.0 train no.53640  loss = 3.84289 avg_loss = 3.90386\n",
      "epoch no.0 train no.53650  loss = 3.34852 avg_loss = 3.90462\n",
      "epoch no.0 train no.53660  loss = 3.80760 avg_loss = 3.89432\n",
      "epoch no.0 train no.53670  loss = 2.20143 avg_loss = 3.87904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.53680  loss = 2.87689 avg_loss = 3.90194\n",
      "epoch no.0 train no.53690  loss = 2.19320 avg_loss = 3.89260\n",
      "epoch no.0 train no.53700  loss = 5.09955 avg_loss = 3.85831\n",
      "epoch no.0 train no.53710  loss = 2.79144 avg_loss = 3.90169\n",
      "epoch no.0 train no.53720  loss = 2.04939 avg_loss = 3.86436\n",
      "epoch no.0 train no.53730  loss = 3.83324 avg_loss = 3.82939\n",
      "epoch no.0 train no.53740  loss = 2.39918 avg_loss = 3.84671\n",
      "epoch no.0 train no.53750  loss = 4.94867 avg_loss = 3.85810\n",
      "epoch no.0 train no.53760  loss = 1.46679 avg_loss = 3.83182\n",
      "epoch no.0 train no.53770  loss = 5.41410 avg_loss = 3.83087\n",
      "epoch no.0 train no.53780  loss = 4.37345 avg_loss = 3.83285\n",
      "epoch no.0 train no.53790  loss = 2.55816 avg_loss = 3.80542\n",
      "epoch no.0 train no.53800  loss = 4.96564 avg_loss = 3.84620\n",
      "epoch no.0 train no.53810  loss = 3.02801 avg_loss = 3.90971\n",
      "epoch no.0 train no.53820  loss = 4.64922 avg_loss = 3.92219\n",
      "epoch no.0 train no.53830  loss = 3.14146 avg_loss = 3.86955\n",
      "epoch no.0 train no.53840  loss = 1.85702 avg_loss = 3.81342\n",
      "epoch no.0 train no.53850  loss = 3.66766 avg_loss = 3.78244\n",
      "epoch no.0 train no.53860  loss = 6.81229 avg_loss = 3.82739\n",
      "epoch no.0 train no.53870  loss = 3.00488 avg_loss = 3.78779\n",
      "epoch no.0 train no.53880  loss = 5.91796 avg_loss = 3.81647\n",
      "epoch no.0 train no.53890  loss = 4.77354 avg_loss = 3.90708\n",
      "epoch no.0 train no.53900  loss = 3.24462 avg_loss = 3.90722\n",
      "epoch no.0 train no.53910  loss = 4.24666 avg_loss = 3.93074\n",
      "epoch no.0 train no.53920  loss = 3.55683 avg_loss = 3.91808\n",
      "epoch no.0 train no.53930  loss = 2.93902 avg_loss = 3.88992\n",
      "epoch no.0 train no.53940  loss = 6.56929 avg_loss = 3.87585\n",
      "epoch no.0 train no.53950  loss = 2.97713 avg_loss = 3.83498\n",
      "epoch no.0 train no.53960  loss = 1.66627 avg_loss = 3.78290\n",
      "epoch no.0 train no.53970  loss = 3.79211 avg_loss = 3.78841\n",
      "epoch no.0 train no.53980  loss = 4.73112 avg_loss = 3.80401\n",
      "epoch no.0 train no.53990  loss = 4.02008 avg_loss = 3.78376\n",
      "epoch no.0 train no.54000  loss = 3.23584 avg_loss = 3.79460\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '</s>', 'g', 'm', '</s>']\n",
      "여름밤의 드라이브 bgm</s>\n",
      "epoch no.0 train no.54010  loss = 6.16554 avg_loss = 3.88630\n",
      "epoch no.0 train no.54020  loss = 5.47759 avg_loss = 3.92147\n",
      "epoch no.0 train no.54030  loss = 5.48240 avg_loss = 3.87205\n",
      "epoch no.0 train no.54040  loss = 4.81474 avg_loss = 3.90239\n",
      "epoch no.0 train no.54050  loss = 3.25653 avg_loss = 3.87168\n",
      "epoch no.0 train no.54060  loss = 3.00060 avg_loss = 3.82772\n",
      "epoch no.0 train no.54070  loss = 3.57340 avg_loss = 3.88574\n",
      "epoch no.0 train no.54080  loss = 2.36482 avg_loss = 3.91726\n",
      "epoch no.0 train no.54090  loss = 5.19505 avg_loss = 3.91177\n",
      "epoch no.0 train no.54100  loss = 4.02223 avg_loss = 3.89714\n",
      "epoch no.0 train no.54110  loss = 4.00761 avg_loss = 3.93785\n",
      "epoch no.0 train no.54120  loss = 4.72536 avg_loss = 3.88723\n",
      "epoch no.0 train no.54130  loss = 3.40185 avg_loss = 3.84633\n",
      "epoch no.0 train no.54140  loss = 5.26442 avg_loss = 3.86255\n",
      "epoch no.0 train no.54150  loss = 3.06482 avg_loss = 3.87767\n",
      "epoch no.0 train no.54160  loss = 4.30341 avg_loss = 3.93063\n",
      "epoch no.0 train no.54170  loss = 5.55358 avg_loss = 3.92789\n",
      "epoch no.0 train no.54180  loss = 2.66577 avg_loss = 3.91449\n",
      "epoch no.0 train no.54190  loss = 3.53163 avg_loss = 3.87951\n",
      "epoch no.0 train no.54200  loss = 2.98224 avg_loss = 3.89974\n",
      "epoch no.0 train no.54210  loss = 4.15398 avg_loss = 3.91003\n",
      "epoch no.0 train no.54220  loss = 2.93610 avg_loss = 3.91127\n",
      "epoch no.0 train no.54230  loss = 3.14157 avg_loss = 3.89692\n",
      "epoch no.0 train no.54240  loss = 2.31332 avg_loss = 3.89042\n",
      "epoch no.0 train no.54250  loss = 5.64136 avg_loss = 3.87624\n",
      "epoch no.0 train no.54260  loss = 4.80549 avg_loss = 3.91381\n",
      "epoch no.0 train no.54270  loss = 5.95621 avg_loss = 3.89302\n",
      "epoch no.0 train no.54280  loss = 2.50876 avg_loss = 3.91074\n",
      "epoch no.0 train no.54290  loss = 2.74087 avg_loss = 3.96538\n",
      "epoch no.0 train no.54300  loss = 6.70722 avg_loss = 4.02855\n",
      "epoch no.0 train no.54310  loss = 3.47120 avg_loss = 3.99737\n",
      "epoch no.0 train no.54320  loss = 4.66614 avg_loss = 3.99092\n",
      "epoch no.0 train no.54330  loss = 3.22384 avg_loss = 3.95558\n",
      "epoch no.0 train no.54340  loss = 4.02348 avg_loss = 3.91039\n",
      "epoch no.0 train no.54350  loss = 4.35445 avg_loss = 3.88211\n",
      "epoch no.0 train no.54360  loss = 2.95854 avg_loss = 3.90481\n",
      "epoch no.0 train no.54370  loss = 3.83327 avg_loss = 3.84984\n",
      "epoch no.0 train no.54380  loss = 4.77125 avg_loss = 3.88869\n",
      "epoch no.0 train no.54390  loss = 5.59756 avg_loss = 3.90636\n",
      "epoch no.0 train no.54400  loss = 3.26092 avg_loss = 3.88795\n",
      "epoch no.0 train no.54410  loss = 2.66940 avg_loss = 3.92565\n",
      "epoch no.0 train no.54420  loss = 3.64501 avg_loss = 3.87938\n",
      "epoch no.0 train no.54430  loss = 5.63125 avg_loss = 3.85019\n",
      "epoch no.0 train no.54440  loss = 3.17761 avg_loss = 3.81668\n",
      "epoch no.0 train no.54450  loss = 3.06020 avg_loss = 3.79321\n",
      "epoch no.0 train no.54460  loss = 6.28514 avg_loss = 3.75771\n",
      "epoch no.0 train no.54470  loss = 5.02331 avg_loss = 3.78357\n",
      "epoch no.0 train no.54480  loss = 4.91623 avg_loss = 3.81033\n",
      "epoch no.0 train no.54490  loss = 3.89868 avg_loss = 3.85228\n",
      "epoch no.0 train no.54500  loss = 3.04914 avg_loss = 3.83816\n",
      "epoch no.0 train no.54510  loss = 1.81381 avg_loss = 3.80208\n",
      "epoch no.0 train no.54520  loss = 2.90787 avg_loss = 3.80493\n",
      "epoch no.0 train no.54530  loss = 6.24722 avg_loss = 3.85944\n",
      "epoch no.0 train no.54540  loss = 3.14711 avg_loss = 3.85845\n",
      "epoch no.0 train no.54550  loss = 4.05366 avg_loss = 3.88975\n",
      "epoch no.0 train no.54560  loss = 3.50593 avg_loss = 3.86754\n",
      "epoch no.0 train no.54570  loss = 4.18987 avg_loss = 3.82606\n",
      "epoch no.0 train no.54580  loss = 3.03545 avg_loss = 3.82159\n",
      "epoch no.0 train no.54590  loss = 4.11318 avg_loss = 3.80932\n",
      "epoch no.0 train no.54600  loss = 4.94692 avg_loss = 3.82823\n",
      "epoch no.0 train no.54610  loss = 4.96500 avg_loss = 3.87953\n",
      "epoch no.0 train no.54620  loss = 2.54370 avg_loss = 3.89592\n",
      "epoch no.0 train no.54630  loss = 4.55147 avg_loss = 3.89672\n",
      "epoch no.0 train no.54640  loss = 3.90523 avg_loss = 3.89562\n",
      "epoch no.0 train no.54650  loss = 5.26799 avg_loss = 3.89471\n",
      "epoch no.0 train no.54660  loss = 4.94959 avg_loss = 3.89648\n",
      "epoch no.0 train no.54670  loss = 1.75609 avg_loss = 3.88925\n",
      "epoch no.0 train no.54680  loss = 3.19425 avg_loss = 3.85269\n",
      "epoch no.0 train no.54690  loss = 3.47960 avg_loss = 3.87756\n",
      "epoch no.0 train no.54700  loss = 3.75788 avg_loss = 3.87314\n",
      "epoch no.0 train no.54710  loss = 3.18195 avg_loss = 3.92475\n",
      "epoch no.0 train no.54720  loss = 5.23467 avg_loss = 3.88948\n",
      "epoch no.0 train no.54730  loss = 6.76873 avg_loss = 3.93188\n",
      "epoch no.0 train no.54740  loss = 5.26434 avg_loss = 3.92303\n",
      "epoch no.0 train no.54750  loss = 2.49291 avg_loss = 3.88502\n",
      "epoch no.0 train no.54760  loss = 2.19150 avg_loss = 3.83566\n",
      "epoch no.0 train no.54770  loss = 2.29271 avg_loss = 3.85796\n",
      "epoch no.0 train no.54780  loss = 3.53511 avg_loss = 3.83346\n",
      "epoch no.0 train no.54790  loss = 5.66769 avg_loss = 3.88707\n",
      "epoch no.0 train no.54800  loss = 4.14369 avg_loss = 3.97717\n",
      "epoch no.0 train no.54810  loss = 4.41677 avg_loss = 3.98018\n",
      "epoch no.0 train no.54820  loss = 3.22250 avg_loss = 3.97894\n",
      "epoch no.0 train no.54830  loss = 3.41218 avg_loss = 3.93926\n",
      "epoch no.0 train no.54840  loss = 2.72709 avg_loss = 3.89439\n",
      "epoch no.0 train no.54850  loss = 2.25908 avg_loss = 3.87599\n",
      "epoch no.0 train no.54860  loss = 4.57817 avg_loss = 3.91448\n",
      "epoch no.0 train no.54870  loss = 4.93719 avg_loss = 3.96139\n",
      "epoch no.0 train no.54880  loss = 6.52219 avg_loss = 3.94391\n",
      "epoch no.0 train no.54890  loss = 1.65951 avg_loss = 3.87361\n",
      "epoch no.0 train no.54900  loss = 1.83078 avg_loss = 3.81340\n",
      "epoch no.0 train no.54910  loss = 5.04261 avg_loss = 3.85174\n",
      "epoch no.0 train no.54920  loss = 4.06128 avg_loss = 3.86445\n",
      "epoch no.0 train no.54930  loss = 6.01931 avg_loss = 3.94814\n",
      "epoch no.0 train no.54940  loss = 3.03554 avg_loss = 3.97728\n",
      "epoch no.0 train no.54950  loss = 3.63402 avg_loss = 3.92876\n",
      "epoch no.0 train no.54960  loss = 2.60753 avg_loss = 3.94875\n",
      "epoch no.0 train no.54970  loss = 5.16468 avg_loss = 4.03255\n",
      "epoch no.0 train no.54980  loss = 7.16571 avg_loss = 4.04694\n",
      "epoch no.0 train no.54990  loss = 5.30985 avg_loss = 4.03422\n",
      "epoch no.0 train no.55000  loss = 2.20318 avg_loss = 4.01110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁할', '때', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 드라이브 할때 듣기 좋은 노래</s>\n",
      "epoch no.0 train no.55010  loss = 2.79370 avg_loss = 3.98999\n",
      "epoch no.0 train no.55020  loss = 3.20322 avg_loss = 3.99965\n",
      "epoch no.0 train no.55030  loss = 3.27141 avg_loss = 4.00802\n",
      "epoch no.0 train no.55040  loss = 4.42892 avg_loss = 3.95659\n",
      "epoch no.0 train no.55050  loss = 1.70973 avg_loss = 3.96668\n",
      "epoch no.0 train no.55060  loss = 4.71631 avg_loss = 4.00764\n",
      "epoch no.0 train no.55070  loss = 4.84322 avg_loss = 3.97351\n",
      "epoch no.0 train no.55080  loss = 3.45442 avg_loss = 3.92594\n",
      "epoch no.0 train no.55090  loss = 2.99312 avg_loss = 3.88890\n",
      "epoch no.0 train no.55100  loss = 2.44178 avg_loss = 3.89730\n",
      "epoch no.0 train no.55110  loss = 4.28840 avg_loss = 3.90308\n",
      "epoch no.0 train no.55120  loss = 3.27244 avg_loss = 3.90348\n",
      "epoch no.0 train no.55130  loss = 2.41094 avg_loss = 3.83507\n",
      "epoch no.0 train no.55140  loss = 5.66342 avg_loss = 3.84427\n",
      "epoch no.0 train no.55150  loss = 3.93532 avg_loss = 3.84484\n",
      "epoch no.0 train no.55160  loss = 4.94756 avg_loss = 3.86070\n",
      "epoch no.0 train no.55170  loss = 4.51437 avg_loss = 3.84992\n",
      "epoch no.0 train no.55180  loss = 3.30323 avg_loss = 3.83235\n",
      "epoch no.0 train no.55190  loss = 6.45088 avg_loss = 3.87667\n",
      "epoch no.0 train no.55200  loss = 4.67672 avg_loss = 3.88031\n",
      "epoch no.0 train no.55210  loss = 2.81342 avg_loss = 3.87249\n",
      "epoch no.0 train no.55220  loss = 3.58962 avg_loss = 3.85996\n",
      "epoch no.0 train no.55230  loss = 4.29969 avg_loss = 3.84546\n",
      "epoch no.0 train no.55240  loss = 4.55706 avg_loss = 3.85064\n",
      "epoch no.0 train no.55250  loss = 2.96541 avg_loss = 3.89443\n",
      "epoch no.0 train no.55260  loss = 3.13519 avg_loss = 3.88162\n",
      "epoch no.0 train no.55270  loss = 3.89543 avg_loss = 3.94941\n",
      "epoch no.0 train no.55280  loss = 3.59395 avg_loss = 4.02521\n",
      "epoch no.0 train no.55290  loss = 3.42031 avg_loss = 4.02283\n",
      "epoch no.0 train no.55300  loss = 2.12212 avg_loss = 3.95811\n",
      "epoch no.0 train no.55310  loss = 3.98361 avg_loss = 3.91719\n",
      "epoch no.0 train no.55320  loss = 2.99317 avg_loss = 3.90865\n",
      "epoch no.0 train no.55330  loss = 4.53971 avg_loss = 3.91717\n",
      "epoch no.0 train no.55340  loss = 2.98425 avg_loss = 3.89951\n",
      "epoch no.0 train no.55350  loss = 2.36201 avg_loss = 3.89479\n",
      "epoch no.0 train no.55360  loss = 4.31789 avg_loss = 3.92650\n",
      "epoch no.0 train no.55370  loss = 2.29568 avg_loss = 3.90919\n",
      "epoch no.0 train no.55380  loss = 5.08182 avg_loss = 3.86852\n",
      "epoch no.0 train no.55390  loss = 3.89448 avg_loss = 3.82248\n",
      "epoch no.0 train no.55400  loss = 3.77314 avg_loss = 3.86161\n",
      "epoch no.0 train no.55410  loss = 2.59215 avg_loss = 3.80096\n",
      "epoch no.0 train no.55420  loss = 3.00327 avg_loss = 3.79219\n",
      "epoch no.0 train no.55430  loss = 5.79856 avg_loss = 3.74643\n",
      "epoch no.0 train no.55440  loss = 4.57781 avg_loss = 3.80002\n",
      "epoch no.0 train no.55450  loss = 1.68499 avg_loss = 3.80915\n",
      "epoch no.0 train no.55460  loss = 2.70437 avg_loss = 3.76567\n",
      "epoch no.0 train no.55470  loss = 5.41966 avg_loss = 3.78686\n",
      "epoch no.0 train no.55480  loss = 3.79631 avg_loss = 3.78252\n",
      "epoch no.0 train no.55490  loss = 5.21240 avg_loss = 3.80372\n",
      "epoch no.0 train no.55500  loss = 3.17884 avg_loss = 3.77471\n",
      "epoch no.0 train no.55510  loss = 2.98819 avg_loss = 3.78181\n",
      "epoch no.0 train no.55520  loss = 4.80869 avg_loss = 3.90793\n",
      "epoch no.0 train no.55530  loss = 4.17186 avg_loss = 3.91947\n",
      "epoch no.0 train no.55540  loss = 3.85578 avg_loss = 3.90587\n",
      "epoch no.0 train no.55550  loss = 3.90322 avg_loss = 3.95720\n",
      "epoch no.0 train no.55560  loss = 3.21585 avg_loss = 3.97196\n",
      "epoch no.0 train no.55570  loss = 3.74971 avg_loss = 4.02500\n",
      "epoch no.0 train no.55580  loss = 2.69206 avg_loss = 3.99596\n",
      "epoch no.0 train no.55590  loss = 5.74380 avg_loss = 4.03345\n",
      "epoch no.0 train no.55600  loss = 1.89892 avg_loss = 3.95180\n",
      "epoch no.0 train no.55610  loss = 3.86014 avg_loss = 3.96685\n",
      "epoch no.0 train no.55620  loss = 2.92699 avg_loss = 3.99641\n",
      "epoch no.0 train no.55630  loss = 4.48095 avg_loss = 4.05539\n",
      "epoch no.0 train no.55640  loss = 3.63561 avg_loss = 4.03470\n",
      "epoch no.0 train no.55650  loss = 2.69131 avg_loss = 4.00310\n",
      "epoch no.0 train no.55660  loss = 5.02631 avg_loss = 4.02994\n",
      "epoch no.0 train no.55670  loss = 4.04140 avg_loss = 4.00291\n",
      "epoch no.0 train no.55680  loss = 2.96453 avg_loss = 3.99372\n",
      "epoch no.0 train no.55690  loss = 4.89595 avg_loss = 3.98965\n",
      "epoch no.0 train no.55700  loss = 2.44236 avg_loss = 3.95755\n",
      "epoch no.0 train no.55710  loss = 4.90347 avg_loss = 4.00423\n",
      "epoch no.0 train no.55720  loss = 3.50020 avg_loss = 3.96634\n",
      "epoch no.0 train no.55730  loss = 7.06798 avg_loss = 4.02836\n",
      "epoch no.0 train no.55740  loss = 3.06054 avg_loss = 3.99915\n",
      "epoch no.0 train no.55750  loss = 3.49845 avg_loss = 4.00288\n",
      "epoch no.0 train no.55760  loss = 4.79063 avg_loss = 3.96415\n",
      "epoch no.0 train no.55770  loss = 3.83079 avg_loss = 3.95475\n",
      "epoch no.0 train no.55780  loss = 4.93179 avg_loss = 3.93218\n",
      "epoch no.0 train no.55790  loss = 4.39370 avg_loss = 4.01029\n",
      "epoch no.0 train no.55800  loss = 4.68015 avg_loss = 3.98543\n",
      "epoch no.0 train no.55810  loss = 5.18274 avg_loss = 3.96353\n",
      "epoch no.0 train no.55820  loss = 2.88245 avg_loss = 3.93351\n",
      "epoch no.0 train no.55830  loss = 1.96219 avg_loss = 3.89146\n",
      "epoch no.0 train no.55840  loss = 6.48664 avg_loss = 3.88263\n",
      "epoch no.0 train no.55850  loss = 4.64706 avg_loss = 3.90675\n",
      "epoch no.0 train no.55860  loss = 4.03643 avg_loss = 3.93958\n",
      "epoch no.0 train no.55870  loss = 2.64550 avg_loss = 3.92075\n",
      "epoch no.0 train no.55880  loss = 2.99986 avg_loss = 3.91607\n",
      "epoch no.0 train no.55890  loss = 4.38406 avg_loss = 3.93578\n",
      "epoch no.0 train no.55900  loss = 6.37150 avg_loss = 3.95845\n",
      "epoch no.0 train no.55910  loss = 4.91652 avg_loss = 3.96927\n",
      "epoch no.0 train no.55920  loss = 3.63930 avg_loss = 3.94826\n",
      "epoch no.0 train no.55930  loss = 3.07621 avg_loss = 3.92827\n",
      "epoch no.0 train no.55940  loss = 2.26391 avg_loss = 3.90624\n",
      "epoch no.0 train no.55950  loss = 3.38700 avg_loss = 3.92190\n",
      "epoch no.0 train no.55960  loss = 4.72782 avg_loss = 3.92455\n",
      "epoch no.0 train no.55970  loss = 3.70363 avg_loss = 3.88178\n",
      "epoch no.0 train no.55980  loss = 2.93074 avg_loss = 3.84208\n",
      "epoch no.0 train no.55990  loss = 4.98942 avg_loss = 3.93060\n",
      "epoch no.0 train no.56000  loss = 4.45373 avg_loss = 3.94093\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 곡</s>\n",
      "epoch no.0 train no.56010  loss = 2.79925 avg_loss = 3.89301\n",
      "epoch no.0 train no.56020  loss = 5.25744 avg_loss = 3.92450\n",
      "epoch no.0 train no.56030  loss = 2.27679 avg_loss = 3.92762\n",
      "epoch no.0 train no.56040  loss = 4.31058 avg_loss = 3.92690\n",
      "epoch no.0 train no.56050  loss = 4.45962 avg_loss = 3.96154\n",
      "epoch no.0 train no.56060  loss = 3.43781 avg_loss = 3.96720\n",
      "epoch no.0 train no.56070  loss = 4.30653 avg_loss = 3.96420\n",
      "epoch no.0 train no.56080  loss = 6.14122 avg_loss = 3.97783\n",
      "epoch no.0 train no.56090  loss = 4.31436 avg_loss = 3.93819\n",
      "epoch no.0 train no.56100  loss = 2.80389 avg_loss = 3.93788\n",
      "epoch no.0 train no.56110  loss = 3.42588 avg_loss = 3.92715\n",
      "epoch no.0 train no.56120  loss = 3.18007 avg_loss = 3.85691\n",
      "epoch no.0 train no.56130  loss = 3.72637 avg_loss = 3.88748\n",
      "epoch no.0 train no.56140  loss = 4.05576 avg_loss = 3.91311\n",
      "epoch no.0 train no.56150  loss = 4.50683 avg_loss = 3.94248\n",
      "epoch no.0 train no.56160  loss = 3.17236 avg_loss = 3.93212\n",
      "epoch no.0 train no.56170  loss = 4.29024 avg_loss = 3.97146\n",
      "epoch no.0 train no.56180  loss = 4.41115 avg_loss = 3.94819\n",
      "epoch no.0 train no.56190  loss = 3.75859 avg_loss = 3.87370\n",
      "epoch no.0 train no.56200  loss = 2.47920 avg_loss = 3.86050\n",
      "epoch no.0 train no.56210  loss = 4.68747 avg_loss = 3.87474\n",
      "epoch no.0 train no.56220  loss = 3.44557 avg_loss = 3.82023\n",
      "epoch no.0 train no.56230  loss = 3.20447 avg_loss = 3.75179\n",
      "epoch no.0 train no.56240  loss = 5.61047 avg_loss = 3.80706\n",
      "epoch no.0 train no.56250  loss = 4.11735 avg_loss = 3.84222\n",
      "epoch no.0 train no.56260  loss = 3.47638 avg_loss = 3.82081\n",
      "epoch no.0 train no.56270  loss = 4.84638 avg_loss = 3.85021\n",
      "epoch no.0 train no.56280  loss = 2.95167 avg_loss = 3.80334\n",
      "epoch no.0 train no.56290  loss = 4.43390 avg_loss = 3.82596\n",
      "epoch no.0 train no.56300  loss = 3.63055 avg_loss = 3.81759\n",
      "epoch no.0 train no.56310  loss = 4.83538 avg_loss = 3.77646\n",
      "epoch no.0 train no.56320  loss = 3.22318 avg_loss = 3.74159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.56330  loss = 3.27355 avg_loss = 3.71868\n",
      "epoch no.0 train no.56340  loss = 3.98610 avg_loss = 3.74788\n",
      "epoch no.0 train no.56350  loss = 3.78619 avg_loss = 3.81488\n",
      "epoch no.0 train no.56360  loss = 4.91585 avg_loss = 3.86723\n",
      "epoch no.0 train no.56370  loss = 3.46228 avg_loss = 3.89444\n",
      "epoch no.0 train no.56380  loss = 5.25981 avg_loss = 3.93086\n",
      "epoch no.0 train no.56390  loss = 1.93854 avg_loss = 3.94044\n",
      "epoch no.0 train no.56400  loss = 3.30796 avg_loss = 3.92172\n",
      "epoch no.0 train no.56410  loss = 2.64959 avg_loss = 3.89021\n",
      "epoch no.0 train no.56420  loss = 2.75868 avg_loss = 3.87846\n",
      "epoch no.0 train no.56430  loss = 3.85062 avg_loss = 3.89594\n",
      "epoch no.0 train no.56440  loss = 4.10639 avg_loss = 3.88535\n",
      "epoch no.0 train no.56450  loss = 2.72628 avg_loss = 3.88307\n",
      "epoch no.0 train no.56460  loss = 3.55111 avg_loss = 3.87323\n",
      "epoch no.0 train no.56470  loss = 2.76743 avg_loss = 3.83195\n",
      "epoch no.0 train no.56480  loss = 3.95785 avg_loss = 3.85411\n",
      "epoch no.0 train no.56490  loss = 3.75143 avg_loss = 3.86830\n",
      "epoch no.0 train no.56500  loss = 1.58261 avg_loss = 3.90439\n",
      "epoch no.0 train no.56510  loss = 4.73377 avg_loss = 3.96068\n",
      "epoch no.0 train no.56520  loss = 3.97726 avg_loss = 3.96142\n",
      "epoch no.0 train no.56530  loss = 5.83153 avg_loss = 3.96348\n",
      "epoch no.0 train no.56540  loss = 3.82283 avg_loss = 3.96263\n",
      "epoch no.0 train no.56550  loss = 2.43964 avg_loss = 3.93350\n",
      "epoch no.0 train no.56560  loss = 2.84122 avg_loss = 3.93022\n",
      "epoch no.0 train no.56570  loss = 3.74158 avg_loss = 3.93183\n",
      "epoch no.0 train no.56580  loss = 5.69565 avg_loss = 3.95411\n",
      "epoch no.0 train no.56590  loss = 6.00715 avg_loss = 3.97276\n",
      "epoch no.0 train no.56600  loss = 3.97684 avg_loss = 3.92060\n",
      "epoch no.0 train no.56610  loss = 4.63889 avg_loss = 3.85421\n",
      "epoch no.0 train no.56620  loss = 3.04079 avg_loss = 3.83999\n",
      "epoch no.0 train no.56630  loss = 4.03633 avg_loss = 3.84560\n",
      "epoch no.0 train no.56640  loss = 3.88913 avg_loss = 3.82933\n",
      "epoch no.0 train no.56650  loss = 3.71289 avg_loss = 3.83385\n",
      "epoch no.0 train no.56660  loss = 5.88795 avg_loss = 3.89322\n",
      "epoch no.0 train no.56670  loss = 2.99521 avg_loss = 3.86923\n",
      "epoch no.0 train no.56680  loss = 5.39842 avg_loss = 3.89615\n",
      "epoch no.0 train no.56690  loss = 3.05836 avg_loss = 3.89714\n",
      "epoch no.0 train no.56700  loss = 5.27877 avg_loss = 3.92616\n",
      "epoch no.0 train no.56710  loss = 3.65629 avg_loss = 3.92321\n",
      "epoch no.0 train no.56720  loss = 4.77306 avg_loss = 3.92932\n",
      "epoch no.0 train no.56730  loss = 4.32717 avg_loss = 4.02442\n",
      "epoch no.0 train no.56740  loss = 3.35086 avg_loss = 4.03932\n",
      "epoch no.0 train no.56750  loss = 2.47616 avg_loss = 4.03573\n",
      "epoch no.0 train no.56760  loss = 3.58243 avg_loss = 4.01688\n",
      "epoch no.0 train no.56770  loss = 3.25889 avg_loss = 3.96198\n",
      "epoch no.0 train no.56780  loss = 2.10967 avg_loss = 3.88504\n",
      "epoch no.0 train no.56790  loss = 2.54028 avg_loss = 3.88183\n",
      "epoch no.0 train no.56800  loss = 3.03587 avg_loss = 3.89823\n",
      "epoch no.0 train no.56810  loss = 2.91023 avg_loss = 3.85151\n",
      "epoch no.0 train no.56820  loss = 2.77054 avg_loss = 3.84451\n",
      "epoch no.0 train no.56830  loss = 4.75038 avg_loss = 3.88540\n",
      "epoch no.0 train no.56840  loss = 2.22232 avg_loss = 3.84269\n",
      "epoch no.0 train no.56850  loss = 5.01833 avg_loss = 3.88108\n",
      "epoch no.0 train no.56860  loss = 4.29254 avg_loss = 3.83395\n",
      "epoch no.0 train no.56870  loss = 3.35917 avg_loss = 3.83948\n",
      "epoch no.0 train no.56880  loss = 4.54485 avg_loss = 3.82084\n",
      "epoch no.0 train no.56890  loss = 4.14332 avg_loss = 3.84543\n",
      "epoch no.0 train no.56900  loss = 3.37938 avg_loss = 3.82214\n",
      "epoch no.0 train no.56910  loss = 4.51134 avg_loss = 3.82909\n",
      "epoch no.0 train no.56920  loss = 2.54848 avg_loss = 3.88811\n",
      "epoch no.0 train no.56930  loss = 2.52319 avg_loss = 3.89797\n",
      "epoch no.0 train no.56940  loss = 2.25568 avg_loss = 3.79086\n",
      "epoch no.0 train no.56950  loss = 3.69973 avg_loss = 3.81507\n",
      "epoch no.0 train no.56960  loss = 4.06367 avg_loss = 3.81438\n",
      "epoch no.0 train no.56970  loss = 4.36133 avg_loss = 3.88076\n",
      "epoch no.0 train no.56980  loss = 4.36197 avg_loss = 3.93559\n",
      "epoch no.0 train no.56990  loss = 2.39838 avg_loss = 3.93389\n",
      "epoch no.0 train no.57000  loss = 2.21480 avg_loss = 3.90150\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.0 train no.57010  loss = 3.69340 avg_loss = 3.87170\n",
      "epoch no.0 train no.57020  loss = 2.11633 avg_loss = 3.87780\n",
      "epoch no.0 train no.57030  loss = 3.58882 avg_loss = 3.85654\n",
      "epoch no.0 train no.57040  loss = 3.48105 avg_loss = 3.87908\n",
      "epoch no.0 train no.57050  loss = 4.88155 avg_loss = 3.88814\n",
      "epoch no.0 train no.57060  loss = 5.38001 avg_loss = 3.89549\n",
      "epoch no.0 train no.57070  loss = 3.22807 avg_loss = 3.89124\n",
      "epoch no.0 train no.57080  loss = 2.56355 avg_loss = 3.92088\n",
      "epoch no.0 train no.57090  loss = 3.41187 avg_loss = 3.88794\n",
      "epoch no.0 train no.57100  loss = 2.90770 avg_loss = 3.87349\n",
      "epoch no.0 train no.57110  loss = 4.67642 avg_loss = 3.85136\n",
      "epoch no.0 train no.57120  loss = 2.62253 avg_loss = 3.81487\n",
      "epoch no.0 train no.57130  loss = 3.19943 avg_loss = 3.81261\n",
      "epoch no.0 train no.57140  loss = 4.31103 avg_loss = 3.84132\n",
      "epoch no.0 train no.57150  loss = 4.39984 avg_loss = 3.90054\n",
      "epoch no.0 train no.57160  loss = 4.04734 avg_loss = 3.91901\n",
      "epoch no.0 train no.57170  loss = 4.23017 avg_loss = 3.84537\n",
      "epoch no.0 train no.57180  loss = 3.73602 avg_loss = 3.85824\n",
      "epoch no.0 train no.57190  loss = 3.79987 avg_loss = 3.84762\n",
      "epoch no.0 train no.57200  loss = 5.38120 avg_loss = 3.85937\n",
      "epoch no.0 train no.57210  loss = 5.13968 avg_loss = 3.85255\n",
      "epoch no.0 train no.57220  loss = 2.07347 avg_loss = 3.81754\n",
      "epoch no.0 train no.57230  loss = 4.88557 avg_loss = 3.85456\n",
      "epoch no.0 train no.57240  loss = 6.25468 avg_loss = 3.86752\n",
      "epoch no.0 train no.57250  loss = 3.00170 avg_loss = 3.92248\n",
      "epoch no.0 train no.57260  loss = 4.77974 avg_loss = 3.87136\n",
      "epoch no.0 train no.57270  loss = 4.53275 avg_loss = 3.83518\n",
      "epoch no.0 train no.57280  loss = 5.74427 avg_loss = 3.86385\n",
      "epoch no.0 train no.57290  loss = 6.62955 avg_loss = 3.92572\n",
      "epoch no.0 train no.57300  loss = 4.23449 avg_loss = 3.93400\n",
      "epoch no.0 train no.57310  loss = 4.85621 avg_loss = 3.93976\n",
      "epoch no.0 train no.57320  loss = 3.66805 avg_loss = 3.92104\n",
      "epoch no.0 train no.57330  loss = 4.97389 avg_loss = 3.97445\n",
      "epoch no.0 train no.57340  loss = 3.30725 avg_loss = 3.96456\n",
      "epoch no.0 train no.57350  loss = 4.74187 avg_loss = 3.93999\n",
      "epoch no.0 train no.57360  loss = 4.33141 avg_loss = 3.94136\n",
      "epoch no.0 train no.57370  loss = 3.63932 avg_loss = 3.95938\n",
      "epoch no.0 train no.57380  loss = 2.77542 avg_loss = 3.91517\n",
      "epoch no.0 train no.57390  loss = 3.90405 avg_loss = 3.90688\n",
      "epoch no.0 train no.57400  loss = 2.82112 avg_loss = 3.85588\n",
      "epoch no.0 train no.57410  loss = 3.13641 avg_loss = 3.84090\n",
      "epoch no.0 train no.57420  loss = 3.49048 avg_loss = 3.87888\n",
      "epoch no.0 train no.57430  loss = 1.81036 avg_loss = 3.85785\n",
      "epoch no.0 train no.57440  loss = 2.89545 avg_loss = 3.84938\n",
      "epoch no.0 train no.57450  loss = 3.45761 avg_loss = 3.85161\n",
      "epoch no.0 train no.57460  loss = 2.79539 avg_loss = 3.91294\n",
      "epoch no.0 train no.57470  loss = 2.37648 avg_loss = 3.89526\n",
      "epoch no.0 train no.57480  loss = 3.59212 avg_loss = 3.85171\n",
      "epoch no.0 train no.57490  loss = 3.44531 avg_loss = 3.81184\n",
      "epoch no.0 train no.57500  loss = 3.10062 avg_loss = 3.80084\n",
      "epoch no.0 train no.57510  loss = 6.80704 avg_loss = 3.87421\n",
      "epoch no.0 train no.57520  loss = 2.49177 avg_loss = 3.85233\n",
      "epoch no.0 train no.57530  loss = 6.70911 avg_loss = 3.86538\n",
      "epoch no.0 train no.57540  loss = 4.36327 avg_loss = 3.85207\n",
      "epoch no.0 train no.57550  loss = 3.25139 avg_loss = 3.85126\n",
      "epoch no.0 train no.57560  loss = 4.62650 avg_loss = 3.91095\n",
      "epoch no.0 train no.57570  loss = 4.50366 avg_loss = 3.90153\n",
      "epoch no.0 train no.57580  loss = 4.01547 avg_loss = 3.89130\n",
      "epoch no.0 train no.57590  loss = 3.11384 avg_loss = 3.97009\n",
      "epoch no.0 train no.57600  loss = 4.69132 avg_loss = 3.92967\n",
      "epoch no.0 train no.57610  loss = 3.47619 avg_loss = 3.95735\n",
      "epoch no.0 train no.57620  loss = 2.79333 avg_loss = 3.88510\n",
      "epoch no.0 train no.57630  loss = 5.42567 avg_loss = 3.87697\n",
      "epoch no.0 train no.57640  loss = 3.19666 avg_loss = 3.90221\n",
      "epoch no.0 train no.57650  loss = 2.80064 avg_loss = 3.92047\n",
      "epoch no.0 train no.57660  loss = 3.15433 avg_loss = 3.91910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.57670  loss = 3.45629 avg_loss = 3.92828\n",
      "epoch no.0 train no.57680  loss = 3.43093 avg_loss = 3.91925\n",
      "epoch no.0 train no.57690  loss = 3.52642 avg_loss = 3.94563\n",
      "epoch no.0 train no.57700  loss = 3.39886 avg_loss = 3.96141\n",
      "epoch no.0 train no.57710  loss = 5.31650 avg_loss = 4.01654\n",
      "epoch no.0 train no.57720  loss = 2.74937 avg_loss = 4.00434\n",
      "epoch no.0 train no.57730  loss = 2.67249 avg_loss = 3.99297\n",
      "epoch no.0 train no.57740  loss = 3.57539 avg_loss = 3.99372\n",
      "epoch no.0 train no.57750  loss = 5.23262 avg_loss = 3.97672\n",
      "epoch no.0 train no.57760  loss = 3.39554 avg_loss = 3.89933\n",
      "epoch no.0 train no.57770  loss = 3.14838 avg_loss = 3.92097\n",
      "epoch no.0 train no.57780  loss = 4.33758 avg_loss = 3.82803\n",
      "epoch no.0 train no.57790  loss = 4.40725 avg_loss = 3.80751\n",
      "epoch no.0 train no.57800  loss = 4.13526 avg_loss = 3.79505\n",
      "epoch no.0 train no.57810  loss = 4.47838 avg_loss = 3.75670\n",
      "epoch no.0 train no.57820  loss = 4.65341 avg_loss = 3.81658\n",
      "epoch no.0 train no.57830  loss = 3.33651 avg_loss = 3.80115\n",
      "epoch no.0 train no.57840  loss = 4.39711 avg_loss = 3.81851\n",
      "epoch no.0 train no.57850  loss = 3.48896 avg_loss = 3.77550\n",
      "epoch no.0 train no.57860  loss = 3.69045 avg_loss = 3.77979\n",
      "epoch no.0 train no.57870  loss = 4.35971 avg_loss = 3.76073\n",
      "epoch no.0 train no.57880  loss = 6.06861 avg_loss = 3.77826\n",
      "epoch no.0 train no.57890  loss = 2.63053 avg_loss = 3.75300\n",
      "epoch no.0 train no.57900  loss = 5.76736 avg_loss = 3.75803\n",
      "epoch no.0 train no.57910  loss = 3.65783 avg_loss = 3.77800\n",
      "epoch no.0 train no.57920  loss = 5.06247 avg_loss = 3.82828\n",
      "epoch no.0 train no.57930  loss = 3.85512 avg_loss = 3.76749\n",
      "epoch no.0 train no.57940  loss = 4.63342 avg_loss = 3.78061\n",
      "epoch no.0 train no.57950  loss = 3.53639 avg_loss = 3.78330\n",
      "epoch no.0 train no.57960  loss = 2.33396 avg_loss = 3.78526\n",
      "epoch no.0 train no.57970  loss = 2.62677 avg_loss = 3.82170\n",
      "epoch no.0 train no.57980  loss = 3.09111 avg_loss = 3.82058\n",
      "epoch no.0 train no.57990  loss = 4.47609 avg_loss = 3.75879\n",
      "epoch no.0 train no.58000  loss = 4.65426 avg_loss = 3.80883\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '노래', '</s>']\n",
      "여름밤에 듣는 여름노래</s>\n",
      "epoch no.0 train no.58010  loss = 3.93934 avg_loss = 3.81583\n",
      "epoch no.0 train no.58020  loss = 3.90905 avg_loss = 3.81265\n",
      "epoch no.0 train no.58030  loss = 4.37456 avg_loss = 3.83392\n",
      "epoch no.0 train no.58040  loss = 2.59951 avg_loss = 3.89142\n",
      "epoch no.0 train no.58050  loss = 2.95484 avg_loss = 3.83863\n",
      "epoch no.0 train no.58060  loss = 3.25708 avg_loss = 3.84698\n",
      "epoch no.0 train no.58070  loss = 5.06639 avg_loss = 3.87201\n",
      "epoch no.0 train no.58080  loss = 3.68512 avg_loss = 3.85612\n",
      "epoch no.0 train no.58090  loss = 7.59518 avg_loss = 3.87575\n",
      "epoch no.0 train no.58100  loss = 2.92973 avg_loss = 3.88660\n",
      "epoch no.0 train no.58110  loss = 4.45519 avg_loss = 3.89839\n",
      "epoch no.0 train no.58120  loss = 4.42081 avg_loss = 3.87917\n",
      "epoch no.0 train no.58130  loss = 5.66323 avg_loss = 3.91400\n",
      "epoch no.0 train no.58140  loss = 4.59819 avg_loss = 3.95856\n",
      "epoch no.0 train no.58150  loss = 2.25937 avg_loss = 4.00541\n",
      "epoch no.0 train no.58160  loss = 3.47679 avg_loss = 4.03944\n",
      "epoch no.0 train no.58170  loss = 2.87434 avg_loss = 4.00061\n",
      "epoch no.0 train no.58180  loss = 6.63387 avg_loss = 4.00625\n",
      "epoch no.0 train no.58190  loss = 2.30172 avg_loss = 3.91747\n",
      "epoch no.0 train no.58200  loss = 5.78834 avg_loss = 3.93059\n",
      "epoch no.0 train no.58210  loss = 2.65338 avg_loss = 3.93626\n",
      "epoch no.0 train no.58220  loss = 4.95714 avg_loss = 3.98682\n",
      "epoch no.0 train no.58230  loss = 4.45247 avg_loss = 3.93315\n",
      "epoch no.0 train no.58240  loss = 4.83383 avg_loss = 3.92630\n",
      "epoch no.0 train no.58250  loss = 2.08454 avg_loss = 3.85669\n",
      "epoch no.0 train no.58260  loss = 3.84789 avg_loss = 3.88621\n",
      "epoch no.0 train no.58270  loss = 3.68143 avg_loss = 3.84195\n",
      "epoch no.0 train no.58280  loss = 4.78612 avg_loss = 3.90885\n",
      "epoch no.0 train no.58290  loss = 3.29823 avg_loss = 3.93222\n",
      "epoch no.0 train no.58300  loss = 1.90092 avg_loss = 3.91933\n",
      "epoch no.0 train no.58310  loss = 3.22169 avg_loss = 3.92129\n",
      "epoch no.0 train no.58320  loss = 3.34769 avg_loss = 3.85581\n",
      "epoch no.0 train no.58330  loss = 4.42908 avg_loss = 3.81423\n",
      "epoch no.0 train no.58340  loss = 6.41071 avg_loss = 3.81379\n",
      "epoch no.0 train no.58350  loss = 5.23258 avg_loss = 3.81870\n",
      "epoch no.0 train no.58360  loss = 2.39861 avg_loss = 3.75674\n",
      "epoch no.0 train no.58370  loss = 3.18252 avg_loss = 3.70920\n",
      "epoch no.0 train no.58380  loss = 6.29877 avg_loss = 3.85318\n",
      "epoch no.0 train no.58390  loss = 2.66148 avg_loss = 3.81191\n",
      "epoch no.0 train no.58400  loss = 3.90193 avg_loss = 3.77202\n",
      "epoch no.0 train no.58410  loss = 3.15424 avg_loss = 3.75482\n",
      "epoch no.0 train no.58420  loss = 4.89209 avg_loss = 3.72501\n",
      "epoch no.0 train no.58430  loss = 2.71542 avg_loss = 3.72689\n",
      "epoch no.0 train no.58440  loss = 2.84951 avg_loss = 3.71737\n",
      "epoch no.0 train no.58450  loss = 3.21793 avg_loss = 3.70352\n",
      "epoch no.0 train no.58460  loss = 2.86155 avg_loss = 3.73627\n",
      "epoch no.0 train no.58470  loss = 4.13040 avg_loss = 3.72097\n",
      "epoch no.0 train no.58480  loss = 5.52539 avg_loss = 3.75983\n",
      "epoch no.0 train no.58490  loss = 4.02493 avg_loss = 3.79547\n",
      "epoch no.0 train no.58500  loss = 3.94876 avg_loss = 3.78499\n",
      "epoch no.0 train no.58510  loss = 5.46537 avg_loss = 3.79241\n",
      "epoch no.0 train no.58520  loss = 3.83640 avg_loss = 3.76070\n",
      "epoch no.0 train no.58530  loss = 1.94357 avg_loss = 3.73228\n",
      "epoch no.0 train no.58540  loss = 6.42065 avg_loss = 3.85017\n",
      "epoch no.0 train no.58550  loss = 1.48754 avg_loss = 3.78206\n",
      "epoch no.0 train no.58560  loss = 1.63592 avg_loss = 3.76064\n",
      "epoch no.0 train no.58570  loss = 2.07757 avg_loss = 3.74446\n",
      "epoch no.0 train no.58580  loss = 3.98011 avg_loss = 3.75161\n",
      "epoch no.0 train no.58590  loss = 4.14008 avg_loss = 3.71035\n",
      "epoch no.0 train no.58600  loss = 4.50348 avg_loss = 3.76900\n",
      "epoch no.0 train no.58610  loss = 4.47234 avg_loss = 3.75915\n",
      "epoch no.0 train no.58620  loss = 3.78762 avg_loss = 3.80630\n",
      "epoch no.0 train no.58630  loss = 4.10272 avg_loss = 3.81142\n",
      "epoch no.0 train no.58640  loss = 5.31761 avg_loss = 3.85344\n",
      "epoch no.0 train no.58650  loss = 3.07094 avg_loss = 3.82448\n",
      "epoch no.0 train no.58660  loss = 4.30024 avg_loss = 3.84451\n",
      "epoch no.0 train no.58670  loss = 5.17262 avg_loss = 3.80855\n",
      "epoch no.0 train no.58680  loss = 5.32153 avg_loss = 3.85221\n",
      "epoch no.0 train no.58690  loss = 2.71194 avg_loss = 3.89503\n",
      "epoch no.0 train no.58700  loss = 3.63795 avg_loss = 3.86680\n",
      "epoch no.0 train no.58710  loss = 3.30139 avg_loss = 3.82705\n",
      "epoch no.0 train no.58720  loss = 3.78858 avg_loss = 3.85157\n",
      "epoch no.0 train no.58730  loss = 4.49574 avg_loss = 3.90093\n",
      "epoch no.0 train no.58740  loss = 4.22388 avg_loss = 3.93794\n",
      "epoch no.0 train no.58750  loss = 4.76876 avg_loss = 3.95386\n",
      "epoch no.0 train no.58760  loss = 4.06186 avg_loss = 3.94083\n",
      "epoch no.0 train no.58770  loss = 4.77279 avg_loss = 3.95802\n",
      "epoch no.0 train no.58780  loss = 4.77577 avg_loss = 3.94763\n",
      "epoch no.0 train no.58790  loss = 2.68912 avg_loss = 3.94398\n",
      "epoch no.0 train no.58800  loss = 2.86963 avg_loss = 3.94727\n",
      "epoch no.0 train no.58810  loss = 3.11788 avg_loss = 3.89368\n",
      "epoch no.0 train no.58820  loss = 3.63502 avg_loss = 3.91249\n",
      "epoch no.0 train no.58830  loss = 3.43218 avg_loss = 3.88800\n",
      "epoch no.0 train no.58840  loss = 3.44806 avg_loss = 3.86530\n",
      "epoch no.0 train no.58850  loss = 2.20890 avg_loss = 3.86752\n",
      "epoch no.0 train no.58860  loss = 5.14869 avg_loss = 3.91506\n",
      "epoch no.0 train no.58870  loss = 6.05531 avg_loss = 3.99296\n",
      "epoch no.0 train no.58880  loss = 5.09444 avg_loss = 4.04122\n",
      "epoch no.0 train no.58890  loss = 5.12678 avg_loss = 4.05875\n",
      "epoch no.0 train no.58900  loss = 3.09361 avg_loss = 4.03095\n",
      "epoch no.0 train no.58910  loss = 4.03248 avg_loss = 4.03661\n",
      "epoch no.0 train no.58920  loss = 2.30167 avg_loss = 3.93547\n",
      "epoch no.0 train no.58930  loss = 3.77865 avg_loss = 3.93658\n",
      "epoch no.0 train no.58940  loss = 4.12098 avg_loss = 3.86735\n",
      "epoch no.0 train no.58950  loss = 2.11265 avg_loss = 3.87646\n",
      "epoch no.0 train no.58960  loss = 5.23556 avg_loss = 3.88208\n",
      "epoch no.0 train no.58970  loss = 3.14945 avg_loss = 3.85439\n",
      "epoch no.0 train no.58980  loss = 3.83180 avg_loss = 3.88761\n",
      "epoch no.0 train no.58990  loss = 2.57066 avg_loss = 3.88794\n",
      "epoch no.0 train no.59000  loss = 3.36226 avg_loss = 3.92076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '</s>']\n",
      "여름밤에 듣는 노래</s>\n",
      "epoch no.0 train no.59010  loss = 5.77230 avg_loss = 3.86975\n",
      "epoch no.0 train no.59020  loss = 3.47096 avg_loss = 3.84256\n",
      "epoch no.0 train no.59030  loss = 5.84215 avg_loss = 3.87223\n",
      "epoch no.0 train no.59040  loss = 4.72522 avg_loss = 3.89557\n",
      "epoch no.0 train no.59050  loss = 3.18463 avg_loss = 3.87544\n",
      "epoch no.0 train no.59060  loss = 3.97267 avg_loss = 3.84326\n",
      "epoch no.0 train no.59070  loss = 4.19760 avg_loss = 3.85332\n",
      "epoch no.0 train no.59080  loss = 3.99548 avg_loss = 3.83174\n",
      "epoch no.0 train no.59090  loss = 3.97804 avg_loss = 3.80241\n",
      "epoch no.0 train no.59100  loss = 2.39745 avg_loss = 3.79321\n",
      "epoch no.0 train no.59110  loss = 5.48183 avg_loss = 3.85446\n",
      "epoch no.0 train no.59120  loss = 4.17513 avg_loss = 3.92475\n",
      "epoch no.0 train no.59130  loss = 4.53493 avg_loss = 3.91933\n",
      "epoch no.0 train no.59140  loss = 3.39641 avg_loss = 3.92623\n",
      "epoch no.0 train no.59150  loss = 4.11803 avg_loss = 3.91188\n",
      "epoch no.0 train no.59160  loss = 2.51833 avg_loss = 3.88483\n",
      "epoch no.0 train no.59170  loss = 4.05030 avg_loss = 3.84139\n",
      "epoch no.0 train no.59180  loss = 4.07193 avg_loss = 3.90174\n",
      "epoch no.0 train no.59190  loss = 2.64115 avg_loss = 3.88940\n",
      "epoch no.0 train no.59200  loss = 3.92440 avg_loss = 3.90790\n",
      "epoch no.0 train no.59210  loss = 2.32814 avg_loss = 3.91968\n",
      "epoch no.0 train no.59220  loss = 4.45639 avg_loss = 3.85031\n",
      "epoch no.0 train no.59230  loss = 3.10325 avg_loss = 3.89750\n",
      "epoch no.0 train no.59240  loss = 3.58211 avg_loss = 3.89031\n",
      "epoch no.0 train no.59250  loss = 2.52316 avg_loss = 3.87117\n",
      "epoch no.0 train no.59260  loss = 3.38293 avg_loss = 3.89155\n",
      "epoch no.0 train no.59270  loss = 5.77457 avg_loss = 3.92501\n",
      "epoch no.0 train no.59280  loss = 3.26659 avg_loss = 3.93882\n",
      "epoch no.0 train no.59290  loss = 3.35678 avg_loss = 3.90292\n",
      "epoch no.0 train no.59300  loss = 3.23599 avg_loss = 3.89920\n",
      "epoch no.0 train no.59310  loss = 3.44814 avg_loss = 3.97026\n",
      "epoch no.0 train no.59320  loss = 4.66468 avg_loss = 3.93333\n",
      "epoch no.0 train no.59330  loss = 3.89816 avg_loss = 3.90485\n",
      "epoch no.0 train no.59340  loss = 3.09729 avg_loss = 3.88354\n",
      "epoch no.0 train no.59350  loss = 4.03360 avg_loss = 3.90606\n",
      "epoch no.0 train no.59360  loss = 2.55798 avg_loss = 3.91074\n",
      "epoch no.0 train no.59370  loss = 5.60744 avg_loss = 3.93836\n",
      "epoch no.0 train no.59380  loss = 3.00742 avg_loss = 3.87754\n",
      "epoch no.0 train no.59390  loss = 4.22498 avg_loss = 3.88281\n",
      "epoch no.0 train no.59400  loss = 3.96597 avg_loss = 3.86875\n",
      "epoch no.0 train no.59410  loss = 5.17989 avg_loss = 3.84665\n",
      "epoch no.0 train no.59420  loss = 2.90968 avg_loss = 3.83400\n",
      "epoch no.0 train no.59430  loss = 4.65425 avg_loss = 3.82931\n",
      "epoch no.0 train no.59440  loss = 3.46240 avg_loss = 3.83680\n",
      "epoch no.0 train no.59450  loss = 1.94884 avg_loss = 3.86066\n",
      "epoch no.0 train no.59460  loss = 4.48561 avg_loss = 3.80471\n",
      "epoch no.0 train no.59470  loss = 3.57914 avg_loss = 3.79591\n",
      "epoch no.0 train no.59480  loss = 3.89505 avg_loss = 3.85711\n",
      "epoch no.0 train no.59490  loss = 3.05630 avg_loss = 3.88192\n",
      "epoch no.0 train no.59500  loss = 4.69203 avg_loss = 3.92123\n",
      "epoch no.0 train no.59510  loss = 4.98589 avg_loss = 3.92546\n",
      "epoch no.0 train no.59520  loss = 2.60846 avg_loss = 3.96923\n",
      "epoch no.0 train no.59530  loss = 3.05811 avg_loss = 3.97240\n",
      "epoch no.0 train no.59540  loss = 3.02565 avg_loss = 3.99322\n",
      "epoch no.0 train no.59550  loss = 3.33032 avg_loss = 3.96815\n",
      "epoch no.0 train no.59560  loss = 4.10133 avg_loss = 4.01760\n",
      "epoch no.0 train no.59570  loss = 2.43887 avg_loss = 3.96891\n",
      "epoch no.0 train no.59580  loss = 2.86908 avg_loss = 3.91487\n",
      "epoch no.0 train no.59590  loss = 3.23271 avg_loss = 3.94923\n",
      "epoch no.0 train no.59600  loss = 4.38633 avg_loss = 3.91970\n",
      "epoch no.0 train no.59610  loss = 4.79605 avg_loss = 3.91056\n",
      "epoch no.0 train no.59620  loss = 3.66839 avg_loss = 3.91775\n",
      "epoch no.0 train no.59630  loss = 2.68990 avg_loss = 3.87766\n",
      "epoch no.0 train no.59640  loss = 2.91089 avg_loss = 3.86235\n",
      "epoch no.0 train no.59650  loss = 3.87313 avg_loss = 3.85152\n",
      "epoch no.0 train no.59660  loss = 4.98737 avg_loss = 3.88853\n",
      "epoch no.0 train no.59670  loss = 3.28100 avg_loss = 3.85896\n",
      "epoch no.0 train no.59680  loss = 2.54833 avg_loss = 3.85310\n",
      "epoch no.0 train no.59690  loss = 4.43679 avg_loss = 3.87666\n",
      "epoch no.0 train no.59700  loss = 3.24104 avg_loss = 3.89650\n",
      "epoch no.0 train no.59710  loss = 4.78770 avg_loss = 3.96390\n",
      "epoch no.0 train no.59720  loss = 2.80524 avg_loss = 3.93931\n",
      "epoch no.0 train no.59730  loss = 2.76558 avg_loss = 3.89287\n",
      "epoch no.0 train no.59740  loss = 3.92962 avg_loss = 3.83769\n",
      "epoch no.0 train no.59750  loss = 2.90798 avg_loss = 3.83302\n",
      "epoch no.0 train no.59760  loss = 2.96327 avg_loss = 3.81930\n",
      "epoch no.0 train no.59770  loss = 5.19862 avg_loss = 3.87560\n",
      "epoch no.0 train no.59780  loss = 3.81713 avg_loss = 3.84656\n",
      "epoch no.0 train no.59790  loss = 3.44130 avg_loss = 3.81228\n",
      "epoch no.0 train no.59800  loss = 3.05179 avg_loss = 3.79322\n",
      "epoch no.0 train no.59810  loss = 3.16361 avg_loss = 3.80990\n",
      "epoch no.0 train no.59820  loss = 4.61533 avg_loss = 3.75073\n",
      "epoch no.0 train no.59830  loss = 3.48665 avg_loss = 3.77767\n",
      "epoch no.0 train no.59840  loss = 4.77294 avg_loss = 3.77867\n",
      "epoch no.0 train no.59850  loss = 4.96605 avg_loss = 3.81289\n",
      "epoch no.0 train no.59860  loss = 6.31694 avg_loss = 3.80512\n",
      "epoch no.0 train no.59870  loss = 4.03564 avg_loss = 3.82696\n",
      "epoch no.0 train no.59880  loss = 6.49034 avg_loss = 3.82454\n",
      "epoch no.0 train no.59890  loss = 3.11680 avg_loss = 3.78727\n",
      "epoch no.0 train no.59900  loss = 4.83761 avg_loss = 3.95229\n",
      "epoch no.0 train no.59910  loss = 3.00859 avg_loss = 3.97070\n",
      "epoch no.0 train no.59920  loss = 4.51435 avg_loss = 4.01117\n",
      "epoch no.0 train no.59930  loss = 3.30694 avg_loss = 3.94502\n",
      "epoch no.0 train no.59940  loss = 4.82667 avg_loss = 3.94144\n",
      "epoch no.0 train no.59950  loss = 5.34133 avg_loss = 3.93924\n",
      "epoch no.0 train no.59960  loss = 3.44420 avg_loss = 3.88342\n",
      "epoch no.0 train no.59970  loss = 4.53976 avg_loss = 3.87493\n",
      "epoch no.0 train no.59980  loss = 2.89517 avg_loss = 3.78646\n",
      "epoch no.0 train no.59990  loss = 4.09750 avg_loss = 3.79720\n",
      "epoch no.0 train no.60000  loss = 4.44504 avg_loss = 3.82944\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.0 train no.60010  loss = 2.76821 avg_loss = 3.85765\n",
      "epoch no.0 train no.60020  loss = 6.72260 avg_loss = 3.87112\n",
      "epoch no.0 train no.60030  loss = 2.58778 avg_loss = 3.83975\n",
      "epoch no.0 train no.60040  loss = 4.45629 avg_loss = 3.84129\n",
      "epoch no.0 train no.60050  loss = 5.38002 avg_loss = 3.82630\n",
      "epoch no.0 train no.60060  loss = 3.54982 avg_loss = 3.78899\n",
      "epoch no.0 train no.60070  loss = 2.11270 avg_loss = 3.78864\n",
      "epoch no.0 train no.60080  loss = 6.33387 avg_loss = 3.81953\n",
      "epoch no.0 train no.60090  loss = 3.13307 avg_loss = 3.82791\n",
      "epoch no.0 train no.60100  loss = 3.00210 avg_loss = 3.82072\n",
      "epoch no.0 train no.60110  loss = 2.90892 avg_loss = 3.85483\n",
      "epoch no.0 train no.60120  loss = 5.27633 avg_loss = 3.84999\n",
      "epoch no.0 train no.60130  loss = 3.74304 avg_loss = 3.88969\n",
      "epoch no.0 train no.60140  loss = 1.95317 avg_loss = 3.91984\n",
      "epoch no.0 train no.60150  loss = 3.39583 avg_loss = 3.89507\n",
      "epoch no.0 train no.60160  loss = 4.15642 avg_loss = 3.88578\n",
      "epoch no.0 train no.60170  loss = 6.59575 avg_loss = 3.87161\n",
      "epoch no.0 train no.60180  loss = 4.61714 avg_loss = 3.91085\n",
      "epoch no.0 train no.60190  loss = 4.11764 avg_loss = 3.86164\n",
      "epoch no.0 train no.60200  loss = 2.19895 avg_loss = 3.86625\n",
      "epoch no.0 train no.60210  loss = 4.05983 avg_loss = 3.85901\n",
      "epoch no.0 train no.60220  loss = 4.00099 avg_loss = 3.83152\n",
      "epoch no.0 train no.60230  loss = 4.95256 avg_loss = 3.86888\n",
      "epoch no.0 train no.60240  loss = 3.45192 avg_loss = 3.82029\n",
      "epoch no.0 train no.60250  loss = 3.85422 avg_loss = 3.83048\n",
      "epoch no.0 train no.60260  loss = 2.81728 avg_loss = 3.78421\n",
      "epoch no.0 train no.60270  loss = 3.95955 avg_loss = 3.77487\n",
      "epoch no.0 train no.60280  loss = 5.18537 avg_loss = 3.86131\n",
      "epoch no.0 train no.60290  loss = 4.10215 avg_loss = 3.82946\n",
      "epoch no.0 train no.60300  loss = 1.32552 avg_loss = 3.80534\n",
      "epoch no.0 train no.60310  loss = 3.65106 avg_loss = 3.83043\n",
      "epoch no.0 train no.60320  loss = 5.01029 avg_loss = 3.85864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.60330  loss = 5.31020 avg_loss = 3.83970\n",
      "epoch no.0 train no.60340  loss = 5.38635 avg_loss = 3.84840\n",
      "epoch no.0 train no.60350  loss = 5.05384 avg_loss = 3.84116\n",
      "epoch no.0 train no.60360  loss = 4.92604 avg_loss = 3.89848\n",
      "epoch no.0 train no.60370  loss = 2.96189 avg_loss = 3.89610\n",
      "epoch no.0 train no.60380  loss = 2.81137 avg_loss = 3.89749\n",
      "epoch no.0 train no.60390  loss = 3.03346 avg_loss = 3.91733\n",
      "epoch no.0 train no.60400  loss = 3.93799 avg_loss = 3.93634\n",
      "epoch no.0 train no.60410  loss = 4.54321 avg_loss = 3.92192\n",
      "epoch no.0 train no.60420  loss = 5.59442 avg_loss = 3.89327\n",
      "epoch no.0 train no.60430  loss = 2.62499 avg_loss = 3.82120\n",
      "epoch no.0 train no.60440  loss = 2.66898 avg_loss = 3.78258\n",
      "epoch no.0 train no.60450  loss = 3.65141 avg_loss = 3.76396\n",
      "epoch no.0 train no.60460  loss = 3.70298 avg_loss = 3.80334\n",
      "epoch no.0 train no.60470  loss = 2.88673 avg_loss = 3.73710\n",
      "epoch no.0 train no.60480  loss = 6.32684 avg_loss = 3.79117\n",
      "epoch no.0 train no.60490  loss = 4.19439 avg_loss = 3.76955\n",
      "epoch no.0 train no.60500  loss = 3.09262 avg_loss = 3.79096\n",
      "epoch no.0 train no.60510  loss = 4.46021 avg_loss = 3.80671\n",
      "epoch no.0 train no.60520  loss = 5.74195 avg_loss = 3.78147\n",
      "epoch no.0 train no.60530  loss = 3.65593 avg_loss = 3.81495\n",
      "epoch no.0 train no.60540  loss = 4.47468 avg_loss = 3.79574\n",
      "epoch no.0 train no.60550  loss = 1.69159 avg_loss = 3.77437\n",
      "epoch no.0 train no.60560  loss = 4.59930 avg_loss = 3.78732\n",
      "epoch no.0 train no.60570  loss = 3.10906 avg_loss = 3.78782\n",
      "epoch no.0 train no.60580  loss = 2.21919 avg_loss = 3.75421\n",
      "epoch no.0 train no.60590  loss = 2.63544 avg_loss = 3.71533\n",
      "epoch no.0 train no.60600  loss = 2.42254 avg_loss = 3.74912\n",
      "epoch no.0 train no.60610  loss = 6.60830 avg_loss = 3.82921\n",
      "epoch no.0 train no.60620  loss = 3.55906 avg_loss = 3.79865\n",
      "epoch no.0 train no.60630  loss = 5.14426 avg_loss = 3.84806\n",
      "epoch no.0 train no.60640  loss = 2.92102 avg_loss = 3.89382\n",
      "epoch no.0 train no.60650  loss = 5.47264 avg_loss = 3.87681\n",
      "epoch no.0 train no.60660  loss = 6.71396 avg_loss = 3.90842\n",
      "epoch no.0 train no.60670  loss = 6.02452 avg_loss = 3.97146\n",
      "epoch no.0 train no.60680  loss = 3.08348 avg_loss = 3.96433\n",
      "epoch no.0 train no.60690  loss = 2.06276 avg_loss = 3.95052\n",
      "epoch no.0 train no.60700  loss = 5.04265 avg_loss = 3.87503\n",
      "epoch no.0 train no.60710  loss = 4.53973 avg_loss = 3.85380\n",
      "epoch no.0 train no.60720  loss = 3.35206 avg_loss = 3.83777\n",
      "epoch no.0 train no.60730  loss = 5.52385 avg_loss = 3.87906\n",
      "epoch no.0 train no.60740  loss = 2.84522 avg_loss = 3.86068\n",
      "epoch no.0 train no.60750  loss = 5.52532 avg_loss = 3.92449\n",
      "epoch no.0 train no.60760  loss = 4.40046 avg_loss = 3.91806\n",
      "epoch no.0 train no.60770  loss = 2.71105 avg_loss = 3.83900\n",
      "epoch no.0 train no.60780  loss = 4.22120 avg_loss = 3.87960\n",
      "epoch no.0 train no.60790  loss = 4.60071 avg_loss = 3.94748\n",
      "epoch no.0 train no.60800  loss = 2.04957 avg_loss = 3.93170\n",
      "epoch no.0 train no.60810  loss = 2.64555 avg_loss = 3.89267\n",
      "epoch no.0 train no.60820  loss = 4.49844 avg_loss = 3.83492\n",
      "epoch no.0 train no.60830  loss = 5.08514 avg_loss = 3.84305\n",
      "epoch no.0 train no.60840  loss = 2.11838 avg_loss = 3.84527\n",
      "epoch no.0 train no.60850  loss = 4.42889 avg_loss = 3.84204\n",
      "epoch no.0 train no.60860  loss = 3.88286 avg_loss = 3.81601\n",
      "epoch no.0 train no.60870  loss = 2.65338 avg_loss = 3.84501\n",
      "epoch no.0 train no.60880  loss = 2.84927 avg_loss = 3.85805\n",
      "epoch no.0 train no.60890  loss = 3.10482 avg_loss = 3.88321\n",
      "epoch no.0 train no.60900  loss = 2.11788 avg_loss = 3.85577\n",
      "epoch no.0 train no.60910  loss = 3.39380 avg_loss = 3.85477\n",
      "epoch no.0 train no.60920  loss = 2.83718 avg_loss = 3.88591\n",
      "epoch no.0 train no.60930  loss = 3.91604 avg_loss = 3.85103\n",
      "epoch no.0 train no.60940  loss = 3.98851 avg_loss = 3.87028\n",
      "epoch no.0 train no.60950  loss = 2.93672 avg_loss = 3.84029\n",
      "epoch no.0 train no.60960  loss = 2.91510 avg_loss = 3.81219\n",
      "epoch no.0 train no.60970  loss = 4.56727 avg_loss = 3.80354\n",
      "epoch no.0 train no.60980  loss = 5.82721 avg_loss = 3.79573\n",
      "epoch no.0 train no.60990  loss = 4.84471 avg_loss = 3.80605\n",
      "epoch no.0 train no.61000  loss = 4.42361 avg_loss = 3.79643\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '▁다가올', '▁전에', '▁듣는', '▁음악', '</s>']\n",
      "여름이 가기전에 듣는 노래</s>\n",
      "epoch no.0 train no.61010  loss = 2.83864 avg_loss = 3.81421\n",
      "epoch no.0 train no.61020  loss = 3.40943 avg_loss = 3.79655\n",
      "epoch no.0 train no.61030  loss = 6.43465 avg_loss = 3.77044\n",
      "epoch no.0 train no.61040  loss = 2.82245 avg_loss = 3.78504\n",
      "epoch no.0 train no.61050  loss = 5.06755 avg_loss = 3.77988\n",
      "epoch no.0 train no.61060  loss = 5.98990 avg_loss = 3.82778\n",
      "epoch no.0 train no.61070  loss = 3.36380 avg_loss = 3.81819\n",
      "epoch no.0 train no.61080  loss = 4.11230 avg_loss = 3.82451\n",
      "epoch no.0 train no.61090  loss = 3.88526 avg_loss = 3.86026\n",
      "epoch no.0 train no.61100  loss = 3.76739 avg_loss = 3.79985\n",
      "epoch no.0 train no.61110  loss = 4.97084 avg_loss = 3.78607\n",
      "epoch no.0 train no.61120  loss = 3.16065 avg_loss = 3.78491\n",
      "epoch no.0 train no.61130  loss = 2.95329 avg_loss = 3.79117\n",
      "epoch no.0 train no.61140  loss = 3.57198 avg_loss = 3.79699\n",
      "epoch no.0 train no.61150  loss = 3.46627 avg_loss = 3.81625\n",
      "epoch no.0 train no.61160  loss = 3.12278 avg_loss = 3.79428\n",
      "epoch no.0 train no.61170  loss = 4.75554 avg_loss = 3.85570\n",
      "epoch no.0 train no.61180  loss = 5.01999 avg_loss = 3.79337\n",
      "epoch no.0 train no.61190  loss = 3.07429 avg_loss = 3.79444\n",
      "epoch no.0 train no.61200  loss = 4.26294 avg_loss = 3.76616\n",
      "epoch no.0 train no.61210  loss = 4.25297 avg_loss = 3.76442\n",
      "epoch no.0 train no.61220  loss = 3.82841 avg_loss = 3.77963\n",
      "epoch no.0 train no.61230  loss = 3.97629 avg_loss = 3.78101\n",
      "epoch no.0 train no.61240  loss = 2.29749 avg_loss = 3.72439\n",
      "epoch no.0 train no.61250  loss = 4.00415 avg_loss = 3.72588\n",
      "epoch no.0 train no.61260  loss = 2.78449 avg_loss = 3.70032\n",
      "epoch no.0 train no.61270  loss = 5.05736 avg_loss = 3.78317\n",
      "epoch no.0 train no.61280  loss = 2.32482 avg_loss = 3.77499\n",
      "epoch no.0 train no.61290  loss = 3.31772 avg_loss = 3.73223\n",
      "epoch no.0 train no.61300  loss = 4.05349 avg_loss = 3.71620\n",
      "epoch no.0 train no.61310  loss = 4.28433 avg_loss = 3.75792\n",
      "epoch no.0 train no.61320  loss = 3.02215 avg_loss = 3.73000\n",
      "epoch no.0 train no.61330  loss = 6.42093 avg_loss = 3.73401\n",
      "epoch no.0 train no.61340  loss = 3.71683 avg_loss = 3.83060\n",
      "epoch no.0 train no.61350  loss = 3.15580 avg_loss = 3.88543\n",
      "epoch no.0 train no.61360  loss = 3.09255 avg_loss = 3.83322\n",
      "epoch no.0 train no.61370  loss = 4.32640 avg_loss = 3.80558\n",
      "epoch no.0 train no.61380  loss = 5.06617 avg_loss = 3.82561\n",
      "epoch no.0 train no.61390  loss = 4.11793 avg_loss = 3.81408\n",
      "epoch no.0 train no.61400  loss = 3.73513 avg_loss = 3.79389\n",
      "epoch no.0 train no.61410  loss = 3.21956 avg_loss = 3.78875\n",
      "epoch no.0 train no.61420  loss = 2.54218 avg_loss = 3.79874\n",
      "epoch no.0 train no.61430  loss = 2.02098 avg_loss = 3.81433\n",
      "epoch no.0 train no.61440  loss = 2.79070 avg_loss = 3.77855\n",
      "epoch no.0 train no.61450  loss = 3.83976 avg_loss = 3.71599\n",
      "epoch no.0 train no.61460  loss = 4.00841 avg_loss = 3.70594\n",
      "epoch no.0 train no.61470  loss = 3.56845 avg_loss = 3.73632\n",
      "epoch no.0 train no.61480  loss = 2.01012 avg_loss = 3.69755\n",
      "epoch no.0 train no.61490  loss = 5.02645 avg_loss = 3.70813\n",
      "epoch no.0 train no.61500  loss = 3.30522 avg_loss = 3.69200\n",
      "epoch no.0 train no.61510  loss = 2.41316 avg_loss = 3.66745\n",
      "epoch no.0 train no.61520  loss = 5.23527 avg_loss = 3.69665\n",
      "epoch no.0 train no.61530  loss = 2.62334 avg_loss = 3.68896\n",
      "epoch no.0 train no.61540  loss = 5.04653 avg_loss = 3.72924\n",
      "epoch no.0 train no.61550  loss = 1.84676 avg_loss = 3.72696\n",
      "epoch no.0 train no.61560  loss = 3.75140 avg_loss = 3.70133\n",
      "epoch no.0 train no.61570  loss = 2.78702 avg_loss = 3.75759\n",
      "epoch no.0 train no.61580  loss = 5.95440 avg_loss = 3.78851\n",
      "epoch no.0 train no.61590  loss = 3.28257 avg_loss = 3.75102\n",
      "epoch no.0 train no.61600  loss = 5.22450 avg_loss = 3.71424\n",
      "epoch no.0 train no.61610  loss = 6.03179 avg_loss = 3.73977\n",
      "epoch no.0 train no.61620  loss = 6.07829 avg_loss = 3.73248\n",
      "epoch no.0 train no.61630  loss = 2.70372 avg_loss = 3.71481\n",
      "epoch no.0 train no.61640  loss = 4.45726 avg_loss = 3.75892\n",
      "epoch no.0 train no.61650  loss = 5.70890 avg_loss = 3.83482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.61660  loss = 3.56768 avg_loss = 3.84208\n",
      "epoch no.0 train no.61670  loss = 2.38318 avg_loss = 3.80113\n",
      "epoch no.0 train no.61680  loss = 6.06059 avg_loss = 3.82340\n",
      "epoch no.0 train no.61690  loss = 2.66536 avg_loss = 3.78929\n",
      "epoch no.0 train no.61700  loss = 4.03787 avg_loss = 3.82023\n",
      "epoch no.0 train no.61710  loss = 3.05412 avg_loss = 3.78812\n",
      "epoch no.0 train no.61720  loss = 5.95825 avg_loss = 3.77085\n",
      "epoch no.0 train no.61730  loss = 2.52659 avg_loss = 3.72227\n",
      "epoch no.0 train no.61740  loss = 3.23516 avg_loss = 3.69300\n",
      "epoch no.0 train no.61750  loss = 2.81389 avg_loss = 3.70187\n",
      "epoch no.0 train no.61760  loss = 2.19728 avg_loss = 3.74678\n",
      "epoch no.0 train no.61770  loss = 2.92448 avg_loss = 3.72974\n",
      "epoch no.0 train no.61780  loss = 3.52448 avg_loss = 3.81846\n",
      "epoch no.0 train no.61790  loss = 3.53559 avg_loss = 3.82220\n",
      "epoch no.0 train no.61800  loss = 2.37062 avg_loss = 3.79779\n",
      "epoch no.0 train no.61810  loss = 2.85745 avg_loss = 3.86794\n",
      "epoch no.0 train no.61820  loss = 2.23319 avg_loss = 3.85254\n",
      "epoch no.0 train no.61830  loss = 3.93493 avg_loss = 3.81594\n",
      "epoch no.0 train no.61840  loss = 3.17241 avg_loss = 3.80400\n",
      "epoch no.0 train no.61850  loss = 3.05980 avg_loss = 3.83043\n",
      "epoch no.0 train no.61860  loss = 4.77644 avg_loss = 3.80274\n",
      "epoch no.0 train no.61870  loss = 1.35199 avg_loss = 3.78677\n",
      "epoch no.0 train no.61880  loss = 2.59685 avg_loss = 3.76207\n",
      "epoch no.0 train no.61890  loss = 3.92363 avg_loss = 3.76153\n",
      "epoch no.0 train no.61900  loss = 3.91960 avg_loss = 3.78724\n",
      "epoch no.0 train no.61910  loss = 3.44807 avg_loss = 3.73402\n",
      "epoch no.0 train no.61920  loss = 4.09835 avg_loss = 3.74682\n",
      "epoch no.0 train no.61930  loss = 3.24952 avg_loss = 3.82953\n",
      "epoch no.0 train no.61940  loss = 4.13951 avg_loss = 3.85312\n",
      "epoch no.0 train no.61950  loss = 4.35991 avg_loss = 3.84609\n",
      "epoch no.0 train no.61960  loss = 4.80631 avg_loss = 3.85194\n",
      "epoch no.0 train no.61970  loss = 4.81750 avg_loss = 3.91525\n",
      "epoch no.0 train no.61980  loss = 1.57247 avg_loss = 3.84804\n",
      "epoch no.0 train no.61990  loss = 5.27569 avg_loss = 3.89075\n",
      "epoch no.0 train no.62000  loss = 2.88687 avg_loss = 3.87354\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁하는', '▁여름', '한', '▁노래', '</s>', '</s>']\n",
      "여름과 함께 듣는 청량한 음악들</s>\n",
      "epoch no.0 train no.62010  loss = 4.06168 avg_loss = 3.91938\n",
      "epoch no.0 train no.62020  loss = 4.36089 avg_loss = 3.88402\n",
      "epoch no.0 train no.62030  loss = 3.56714 avg_loss = 3.88657\n",
      "epoch no.0 train no.62040  loss = 5.16890 avg_loss = 3.86453\n",
      "epoch no.0 train no.62050  loss = 3.04144 avg_loss = 3.95625\n",
      "epoch no.0 train no.62060  loss = 3.01806 avg_loss = 3.93912\n",
      "epoch no.0 train no.62070  loss = 5.23287 avg_loss = 3.90768\n",
      "epoch no.0 train no.62080  loss = 4.59784 avg_loss = 3.90600\n",
      "epoch no.0 train no.62090  loss = 1.37341 avg_loss = 3.84590\n",
      "epoch no.0 train no.62100  loss = 3.45443 avg_loss = 3.84787\n",
      "epoch no.0 train no.62110  loss = 3.60043 avg_loss = 3.83679\n",
      "epoch no.0 train no.62120  loss = 2.44881 avg_loss = 3.80572\n",
      "epoch no.0 train no.62130  loss = 3.42403 avg_loss = 3.77813\n",
      "epoch no.0 train no.62140  loss = 4.02702 avg_loss = 3.72020\n",
      "epoch no.0 train no.62150  loss = 5.15129 avg_loss = 3.76766\n",
      "epoch no.0 train no.62160  loss = 4.47868 avg_loss = 3.76947\n",
      "epoch no.0 train no.62170  loss = 6.01025 avg_loss = 3.80032\n",
      "epoch no.0 train no.62180  loss = 3.42792 avg_loss = 3.78235\n",
      "epoch no.0 train no.62190  loss = 3.29763 avg_loss = 3.77399\n",
      "epoch no.0 train no.62200  loss = 4.22903 avg_loss = 3.87906\n",
      "epoch no.0 train no.62210  loss = 3.83160 avg_loss = 3.88654\n",
      "epoch no.0 train no.62220  loss = 2.42007 avg_loss = 3.83855\n",
      "epoch no.0 train no.62230  loss = 2.52859 avg_loss = 3.80067\n",
      "epoch no.0 train no.62240  loss = 4.00580 avg_loss = 3.76122\n",
      "epoch no.0 train no.62250  loss = 3.90211 avg_loss = 3.77701\n",
      "epoch no.0 train no.62260  loss = 2.70268 avg_loss = 3.74894\n",
      "epoch no.0 train no.62270  loss = 4.59760 avg_loss = 3.77973\n",
      "epoch no.0 train no.62280  loss = 2.20890 avg_loss = 3.78840\n",
      "epoch no.0 train no.62290  loss = 2.98549 avg_loss = 3.77101\n",
      "epoch no.0 train no.62300  loss = 3.67678 avg_loss = 3.76173\n",
      "epoch no.0 train no.62310  loss = 3.71743 avg_loss = 3.77789\n",
      "epoch no.0 train no.62320  loss = 3.44981 avg_loss = 3.74667\n",
      "epoch no.0 train no.62330  loss = 4.51922 avg_loss = 3.71428\n",
      "epoch no.0 train no.62340  loss = 3.47293 avg_loss = 3.66965\n",
      "epoch no.0 train no.62350  loss = 4.39339 avg_loss = 3.70911\n",
      "epoch no.0 train no.62360  loss = 5.36609 avg_loss = 3.67754\n",
      "epoch no.0 train no.62370  loss = 3.73425 avg_loss = 3.77382\n",
      "epoch no.0 train no.62380  loss = 4.05057 avg_loss = 3.81401\n",
      "epoch no.0 train no.62390  loss = 5.72100 avg_loss = 3.83532\n",
      "epoch no.0 train no.62400  loss = 6.65034 avg_loss = 3.81555\n",
      "epoch no.0 train no.62410  loss = 2.98592 avg_loss = 3.84131\n",
      "epoch no.0 train no.62420  loss = 6.52879 avg_loss = 3.89293\n",
      "epoch no.0 train no.62430  loss = 6.18437 avg_loss = 3.91876\n",
      "epoch no.0 train no.62440  loss = 3.49196 avg_loss = 3.91382\n",
      "epoch no.0 train no.62450  loss = 2.81673 avg_loss = 3.90320\n",
      "epoch no.0 train no.62460  loss = 3.24744 avg_loss = 3.93375\n",
      "epoch no.0 train no.62470  loss = 2.64716 avg_loss = 3.91508\n",
      "epoch no.0 train no.62480  loss = 4.82810 avg_loss = 3.90545\n",
      "epoch no.0 train no.62490  loss = 2.52304 avg_loss = 3.92801\n",
      "epoch no.0 train no.62500  loss = 4.22409 avg_loss = 3.97204\n",
      "epoch no.0 train no.62510  loss = 4.55781 avg_loss = 3.88345\n",
      "epoch no.0 train no.62520  loss = 3.53228 avg_loss = 3.87462\n",
      "epoch no.0 train no.62530  loss = 3.80516 avg_loss = 3.82865\n",
      "epoch no.0 train no.62540  loss = 5.56472 avg_loss = 3.87122\n",
      "epoch no.0 train no.62550  loss = 4.09060 avg_loss = 3.84376\n",
      "epoch no.0 train no.62560  loss = 3.38902 avg_loss = 3.89077\n",
      "epoch no.0 train no.62570  loss = 2.91772 avg_loss = 3.89412\n",
      "epoch no.0 train no.62580  loss = 5.44617 avg_loss = 3.91421\n",
      "epoch no.0 train no.62590  loss = 3.44692 avg_loss = 3.90716\n",
      "epoch no.0 train no.62600  loss = 5.38622 avg_loss = 3.86065\n",
      "epoch no.0 train no.62610  loss = 2.87247 avg_loss = 3.84889\n",
      "epoch no.0 train no.62620  loss = 6.27788 avg_loss = 3.94818\n",
      "epoch no.0 train no.62630  loss = 6.11447 avg_loss = 4.01770\n",
      "epoch no.0 train no.62640  loss = 4.72144 avg_loss = 3.99142\n",
      "epoch no.0 train no.62650  loss = 3.88825 avg_loss = 3.93911\n",
      "epoch no.0 train no.62660  loss = 3.46755 avg_loss = 3.94341\n",
      "epoch no.0 train no.62670  loss = 3.43248 avg_loss = 3.89167\n",
      "epoch no.0 train no.62680  loss = 2.99058 avg_loss = 3.84578\n",
      "epoch no.0 train no.62690  loss = 5.47995 avg_loss = 3.82741\n",
      "epoch no.0 train no.62700  loss = 3.29251 avg_loss = 3.84239\n",
      "epoch no.0 train no.62710  loss = 3.32600 avg_loss = 3.81248\n",
      "epoch no.0 train no.62720  loss = 4.37081 avg_loss = 3.78382\n",
      "epoch no.0 train no.62730  loss = 5.09341 avg_loss = 3.80038\n",
      "epoch no.0 train no.62740  loss = 7.77102 avg_loss = 3.90457\n",
      "epoch no.0 train no.62750  loss = 3.59774 avg_loss = 3.95187\n",
      "epoch no.0 train no.62760  loss = 4.41408 avg_loss = 3.95227\n",
      "epoch no.0 train no.62770  loss = 2.30199 avg_loss = 3.91678\n",
      "epoch no.0 train no.62780  loss = 4.07751 avg_loss = 3.93507\n",
      "epoch no.0 train no.62790  loss = 3.09981 avg_loss = 3.90912\n",
      "epoch no.0 train no.62800  loss = 4.45720 avg_loss = 3.88925\n",
      "epoch no.0 train no.62810  loss = 4.16073 avg_loss = 3.88947\n",
      "epoch no.0 train no.62820  loss = 4.38146 avg_loss = 3.88557\n",
      "epoch no.0 train no.62830  loss = 3.41848 avg_loss = 3.79869\n",
      "epoch no.0 train no.62840  loss = 3.02804 avg_loss = 3.81779\n",
      "epoch no.0 train no.62850  loss = 4.25821 avg_loss = 3.79656\n",
      "epoch no.0 train no.62860  loss = 2.61597 avg_loss = 3.77324\n",
      "epoch no.0 train no.62870  loss = 2.82478 avg_loss = 3.77305\n",
      "epoch no.0 train no.62880  loss = 2.21707 avg_loss = 3.82226\n",
      "epoch no.0 train no.62890  loss = 2.30022 avg_loss = 3.79962\n",
      "epoch no.0 train no.62900  loss = 3.79809 avg_loss = 3.78590\n",
      "epoch no.0 train no.62910  loss = 4.08638 avg_loss = 3.81226\n",
      "epoch no.0 train no.62920  loss = 3.28708 avg_loss = 3.81154\n",
      "epoch no.0 train no.62930  loss = 4.29070 avg_loss = 3.81997\n",
      "epoch no.0 train no.62940  loss = 5.59651 avg_loss = 3.84101\n",
      "epoch no.0 train no.62950  loss = 6.12701 avg_loss = 3.89650\n",
      "epoch no.0 train no.62960  loss = 6.83397 avg_loss = 3.98664\n",
      "epoch no.0 train no.62970  loss = 3.89902 avg_loss = 4.01182\n",
      "epoch no.0 train no.62980  loss = 5.88432 avg_loss = 3.99186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.62990  loss = 3.00702 avg_loss = 3.96696\n",
      "epoch no.0 train no.63000  loss = 3.53424 avg_loss = 3.96941\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁다가오는', '가는', '▁지금', '</s>']\n",
      "여름이 끝나가는 오후</s>\n",
      "epoch no.0 train no.63010  loss = 2.80105 avg_loss = 3.92873\n",
      "epoch no.0 train no.63020  loss = 4.00503 avg_loss = 3.88799\n",
      "epoch no.0 train no.63030  loss = 4.76549 avg_loss = 3.98045\n",
      "epoch no.0 train no.63040  loss = 2.43076 avg_loss = 3.99036\n",
      "epoch no.0 train no.63050  loss = 4.13336 avg_loss = 3.93283\n",
      "epoch no.0 train no.63060  loss = 3.85697 avg_loss = 3.98108\n",
      "epoch no.0 train no.63070  loss = 7.14908 avg_loss = 3.97759\n",
      "epoch no.0 train no.63080  loss = 2.08868 avg_loss = 3.93650\n",
      "epoch no.0 train no.63090  loss = 3.67572 avg_loss = 3.91803\n",
      "epoch no.0 train no.63100  loss = 3.72751 avg_loss = 3.89427\n",
      "epoch no.0 train no.63110  loss = 3.21203 avg_loss = 3.87822\n",
      "epoch no.0 train no.63120  loss = 3.33831 avg_loss = 3.88421\n",
      "epoch no.0 train no.63130  loss = 3.76236 avg_loss = 3.92514\n",
      "epoch no.0 train no.63140  loss = 4.70745 avg_loss = 3.89199\n",
      "epoch no.0 train no.63150  loss = 4.96750 avg_loss = 3.92286\n",
      "epoch no.0 train no.63160  loss = 4.68429 avg_loss = 3.99116\n",
      "epoch no.0 train no.63170  loss = 5.33075 avg_loss = 4.04265\n",
      "epoch no.0 train no.63180  loss = 4.93219 avg_loss = 4.00693\n",
      "epoch no.0 train no.63190  loss = 4.87532 avg_loss = 4.09687\n",
      "epoch no.0 train no.63200  loss = 5.80538 avg_loss = 4.14286\n",
      "epoch no.0 train no.63210  loss = 4.02977 avg_loss = 4.17459\n",
      "epoch no.0 train no.63220  loss = 4.94145 avg_loss = 4.14605\n",
      "epoch no.0 train no.63230  loss = 4.90480 avg_loss = 4.11233\n",
      "epoch no.0 train no.63240  loss = 4.52495 avg_loss = 4.05019\n",
      "epoch no.0 train no.63250  loss = 3.45785 avg_loss = 3.96075\n",
      "epoch no.0 train no.63260  loss = 5.80218 avg_loss = 4.06697\n",
      "epoch no.0 train no.63270  loss = 7.18640 avg_loss = 4.06688\n",
      "epoch no.0 train no.63280  loss = 6.18157 avg_loss = 4.04386\n",
      "epoch no.0 train no.63290  loss = 2.71868 avg_loss = 4.08142\n",
      "epoch no.0 train no.63300  loss = 3.74235 avg_loss = 4.03544\n",
      "epoch no.0 train no.63310  loss = 3.24724 avg_loss = 4.05989\n",
      "epoch no.0 train no.63320  loss = 3.37302 avg_loss = 4.08163\n",
      "epoch no.0 train no.63330  loss = 3.11252 avg_loss = 4.01473\n",
      "epoch no.0 train no.63340  loss = 5.13394 avg_loss = 4.04834\n",
      "epoch no.0 train no.63350  loss = 4.40282 avg_loss = 4.00027\n",
      "epoch no.0 train no.63360  loss = 2.00328 avg_loss = 3.93886\n",
      "epoch no.0 train no.63370  loss = 4.68497 avg_loss = 3.95695\n",
      "epoch no.0 train no.63380  loss = 5.07113 avg_loss = 3.88740\n",
      "epoch no.0 train no.63390  loss = 2.39669 avg_loss = 3.89033\n",
      "epoch no.0 train no.63400  loss = 2.16280 avg_loss = 3.87450\n",
      "epoch no.0 train no.63410  loss = 3.28574 avg_loss = 3.88150\n",
      "epoch no.0 train no.63420  loss = 5.90566 avg_loss = 3.84576\n",
      "epoch no.0 train no.63430  loss = 4.50992 avg_loss = 3.85682\n",
      "epoch no.0 train no.63440  loss = 5.02680 avg_loss = 3.80715\n",
      "epoch no.0 train no.63450  loss = 3.03165 avg_loss = 3.74198\n",
      "epoch no.0 train no.63460  loss = 2.60298 avg_loss = 3.72839\n",
      "epoch no.0 train no.63470  loss = 3.30428 avg_loss = 3.75092\n",
      "epoch no.0 train no.63480  loss = 3.57981 avg_loss = 3.74725\n",
      "epoch no.0 train no.63490  loss = 5.01679 avg_loss = 3.71480\n",
      "epoch no.0 train no.63500  loss = 3.08376 avg_loss = 3.71330\n",
      "epoch no.0 train no.63510  loss = 4.19104 avg_loss = 3.70944\n",
      "epoch no.0 train no.63520  loss = 3.10853 avg_loss = 3.70479\n",
      "epoch no.0 train no.63530  loss = 5.70506 avg_loss = 3.76307\n",
      "epoch no.0 train no.63540  loss = 4.87741 avg_loss = 3.81264\n",
      "epoch no.0 train no.63550  loss = 3.84938 avg_loss = 3.81093\n",
      "epoch no.0 train no.63560  loss = 3.81174 avg_loss = 3.82698\n",
      "epoch no.0 train no.63570  loss = 3.97577 avg_loss = 3.88029\n",
      "epoch no.0 train no.63580  loss = 3.49464 avg_loss = 3.89488\n",
      "epoch no.0 train no.63590  loss = 3.06012 avg_loss = 3.89444\n",
      "epoch no.0 train no.63600  loss = 3.90122 avg_loss = 3.86445\n",
      "epoch no.0 train no.63610  loss = 5.05016 avg_loss = 3.89975\n",
      "epoch no.0 train no.63620  loss = 3.58726 avg_loss = 3.86446\n",
      "epoch no.0 train no.63630  loss = 3.67218 avg_loss = 3.89521\n",
      "epoch no.0 train no.63640  loss = 3.10201 avg_loss = 3.90657\n",
      "epoch no.0 train no.63650  loss = 1.85206 avg_loss = 3.89839\n",
      "epoch no.0 train no.63660  loss = 2.59195 avg_loss = 3.89184\n",
      "epoch no.0 train no.63670  loss = 2.60508 avg_loss = 3.86525\n",
      "epoch no.0 train no.63680  loss = 4.67903 avg_loss = 3.92908\n",
      "epoch no.0 train no.63690  loss = 4.52392 avg_loss = 3.99021\n",
      "epoch no.0 train no.63700  loss = 3.62487 avg_loss = 3.96886\n",
      "epoch no.0 train no.63710  loss = 4.52738 avg_loss = 4.00342\n",
      "epoch no.0 train no.63720  loss = 3.57461 avg_loss = 3.92753\n",
      "epoch no.0 train no.63730  loss = 7.29628 avg_loss = 3.95965\n",
      "epoch no.0 train no.63740  loss = 3.27932 avg_loss = 3.92938\n",
      "epoch no.0 train no.63750  loss = 2.09341 avg_loss = 3.87831\n",
      "epoch no.0 train no.63760  loss = 2.54165 avg_loss = 3.83655\n",
      "epoch no.0 train no.63770  loss = 4.39956 avg_loss = 3.78582\n",
      "epoch no.0 train no.63780  loss = 3.04606 avg_loss = 3.81056\n",
      "epoch no.0 train no.63790  loss = 2.56906 avg_loss = 3.81249\n",
      "epoch no.0 train no.63800  loss = 2.12332 avg_loss = 3.77430\n",
      "epoch no.0 train no.63810  loss = 3.58662 avg_loss = 3.80023\n",
      "epoch no.0 train no.63820  loss = 4.96727 avg_loss = 3.79535\n",
      "epoch no.0 train no.63830  loss = 3.65398 avg_loss = 3.76756\n",
      "epoch no.0 train no.63840  loss = 4.99114 avg_loss = 3.80498\n",
      "epoch no.0 train no.63850  loss = 5.56344 avg_loss = 3.83349\n",
      "epoch no.0 train no.63860  loss = 4.11724 avg_loss = 3.86574\n",
      "epoch no.0 train no.63870  loss = 2.48005 avg_loss = 3.88619\n",
      "epoch no.0 train no.63880  loss = 3.22091 avg_loss = 3.90893\n",
      "epoch no.0 train no.63890  loss = 2.17066 avg_loss = 3.85931\n",
      "epoch no.0 train no.63900  loss = 6.73857 avg_loss = 3.89099\n",
      "epoch no.0 train no.63910  loss = 2.97587 avg_loss = 3.89423\n",
      "epoch no.0 train no.63920  loss = 5.75012 avg_loss = 3.94227\n",
      "epoch no.0 train no.63930  loss = 3.67315 avg_loss = 3.92858\n",
      "epoch no.0 train no.63940  loss = 4.21534 avg_loss = 3.90693\n",
      "epoch no.0 train no.63950  loss = 3.28566 avg_loss = 3.89650\n",
      "epoch no.0 train no.63960  loss = 3.20781 avg_loss = 3.95584\n",
      "epoch no.0 train no.63970  loss = 5.23120 avg_loss = 3.97581\n",
      "epoch no.0 train no.63980  loss = 3.91522 avg_loss = 3.92425\n",
      "epoch no.0 train no.63990  loss = 4.27780 avg_loss = 3.90868\n",
      "epoch no.0 train no.64000  loss = 5.02387 avg_loss = 3.87276\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '에', '▁위한', '▁플레이', '</s>']\n",
      "여름밤 드라이브를 위한 재즈</s>\n",
      "epoch no.0 train no.64010  loss = 2.94962 avg_loss = 3.83242\n",
      "epoch no.0 train no.64020  loss = 5.34405 avg_loss = 3.85704\n",
      "epoch no.0 train no.64030  loss = 3.59532 avg_loss = 3.81493\n",
      "epoch no.0 train no.64040  loss = 3.61756 avg_loss = 3.80655\n",
      "epoch no.0 train no.64050  loss = 1.34034 avg_loss = 3.82055\n",
      "epoch no.0 train no.64060  loss = 2.19153 avg_loss = 3.81328\n",
      "epoch no.0 train no.64070  loss = 6.59856 avg_loss = 3.81790\n",
      "epoch no.0 train no.64080  loss = 4.22265 avg_loss = 3.83074\n",
      "epoch no.0 train no.64090  loss = 4.21307 avg_loss = 3.79132\n",
      "epoch no.0 train no.64100  loss = 2.79924 avg_loss = 3.78646\n",
      "epoch no.0 train no.64110  loss = 3.06646 avg_loss = 3.80203\n",
      "epoch no.0 train no.64120  loss = 4.33825 avg_loss = 3.79727\n",
      "epoch no.0 train no.64130  loss = 4.06418 avg_loss = 3.77264\n",
      "epoch no.0 train no.64140  loss = 5.01791 avg_loss = 3.79990\n",
      "epoch no.0 train no.64150  loss = 6.68484 avg_loss = 3.77831\n",
      "epoch no.0 train no.64160  loss = 4.87871 avg_loss = 3.77195\n",
      "epoch no.0 train no.64170  loss = 3.05603 avg_loss = 3.74909\n",
      "epoch no.0 train no.64180  loss = 4.92066 avg_loss = 3.78769\n",
      "epoch no.0 train no.64190  loss = 4.12389 avg_loss = 3.83371\n",
      "epoch no.0 train no.64200  loss = 4.07654 avg_loss = 3.80323\n",
      "epoch no.0 train no.64210  loss = 3.13520 avg_loss = 3.77588\n",
      "epoch no.0 train no.64220  loss = 2.50952 avg_loss = 3.75833\n",
      "epoch no.0 train no.64230  loss = 2.10173 avg_loss = 3.77094\n",
      "epoch no.0 train no.64240  loss = 4.96225 avg_loss = 3.81562\n",
      "epoch no.0 train no.64250  loss = 2.77903 avg_loss = 3.81385\n",
      "epoch no.0 train no.64260  loss = 3.33975 avg_loss = 3.86717\n",
      "epoch no.0 train no.64270  loss = 4.53258 avg_loss = 3.87555\n",
      "epoch no.0 train no.64280  loss = 7.52300 avg_loss = 3.95379\n",
      "epoch no.0 train no.64290  loss = 3.63038 avg_loss = 3.99851\n",
      "epoch no.0 train no.64300  loss = 4.10978 avg_loss = 4.02986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.64310  loss = 2.97943 avg_loss = 3.98468\n",
      "epoch no.0 train no.64320  loss = 4.59922 avg_loss = 3.97871\n",
      "epoch no.0 train no.64330  loss = 3.87325 avg_loss = 3.99502\n",
      "epoch no.0 train no.64340  loss = 3.24557 avg_loss = 3.95479\n",
      "epoch no.0 train no.64350  loss = 4.96390 avg_loss = 3.89601\n",
      "epoch no.0 train no.64360  loss = 3.22136 avg_loss = 3.88061\n",
      "epoch no.0 train no.64370  loss = 4.33923 avg_loss = 3.86448\n",
      "epoch no.0 train no.64380  loss = 5.66265 avg_loss = 3.88288\n",
      "epoch no.0 train no.64390  loss = 2.81781 avg_loss = 3.83071\n",
      "epoch no.0 train no.64400  loss = 3.38324 avg_loss = 3.90998\n",
      "epoch no.0 train no.64410  loss = 5.39262 avg_loss = 3.95211\n",
      "epoch no.0 train no.64420  loss = 4.87590 avg_loss = 3.99590\n",
      "epoch no.0 train no.64430  loss = 3.76402 avg_loss = 3.92616\n",
      "epoch no.0 train no.64440  loss = 3.39784 avg_loss = 3.92065\n",
      "epoch no.0 train no.64450  loss = 2.78017 avg_loss = 3.88987\n",
      "epoch no.0 train no.64460  loss = 2.19749 avg_loss = 3.89729\n",
      "epoch no.0 train no.64470  loss = 2.64802 avg_loss = 3.85926\n",
      "epoch no.0 train no.64480  loss = 4.86773 avg_loss = 3.89373\n",
      "epoch no.0 train no.64490  loss = 4.42963 avg_loss = 3.87366\n",
      "epoch no.0 train no.64500  loss = 5.23196 avg_loss = 3.80995\n",
      "epoch no.0 train no.64510  loss = 3.72775 avg_loss = 3.85229\n",
      "epoch no.0 train no.64520  loss = 3.08346 avg_loss = 3.83281\n",
      "epoch no.0 train no.64530  loss = 2.46953 avg_loss = 3.82948\n",
      "epoch no.0 train no.64540  loss = 4.65680 avg_loss = 3.87543\n",
      "epoch no.0 train no.64550  loss = 2.52382 avg_loss = 3.86602\n",
      "epoch no.0 train no.64560  loss = 8.32137 avg_loss = 3.87956\n",
      "epoch no.0 train no.64570  loss = 6.13271 avg_loss = 3.87209\n",
      "epoch no.0 train no.64580  loss = 4.49926 avg_loss = 3.90964\n",
      "epoch no.0 train no.64590  loss = 3.75951 avg_loss = 3.95881\n",
      "epoch no.0 train no.64600  loss = 2.38613 avg_loss = 3.95316\n",
      "epoch no.0 train no.64610  loss = 2.59547 avg_loss = 3.97920\n",
      "epoch no.0 train no.64620  loss = 3.27950 avg_loss = 4.06659\n",
      "epoch no.0 train no.64630  loss = 3.66696 avg_loss = 4.09267\n",
      "epoch no.0 train no.64640  loss = 4.12557 avg_loss = 4.11674\n",
      "epoch no.0 train no.64650  loss = 4.29842 avg_loss = 4.06394\n",
      "epoch no.0 train no.64660  loss = 3.04673 avg_loss = 4.00735\n",
      "epoch no.0 train no.64670  loss = 5.08529 avg_loss = 4.05648\n",
      "epoch no.0 train no.64680  loss = 5.44489 avg_loss = 4.11443\n",
      "epoch no.0 train no.64690  loss = 3.35569 avg_loss = 4.12258\n",
      "epoch no.0 train no.64700  loss = 3.75594 avg_loss = 4.18095\n",
      "epoch no.0 train no.64710  loss = 3.89406 avg_loss = 4.13154\n",
      "epoch no.0 train no.64720  loss = 2.98263 avg_loss = 4.15421\n",
      "epoch no.0 train no.64730  loss = 3.14445 avg_loss = 4.12029\n",
      "epoch no.0 train no.64740  loss = 3.98331 avg_loss = 4.12052\n",
      "epoch no.0 train no.64750  loss = 3.97502 avg_loss = 4.15114\n",
      "epoch no.0 train no.64760  loss = 2.44026 avg_loss = 4.09381\n",
      "epoch no.0 train no.64770  loss = 4.57366 avg_loss = 4.07399\n",
      "epoch no.0 train no.64780  loss = 3.13947 avg_loss = 4.02358\n",
      "epoch no.0 train no.64790  loss = 5.05458 avg_loss = 4.00276\n",
      "epoch no.0 train no.64800  loss = 5.40541 avg_loss = 4.03755\n",
      "epoch no.0 train no.64810  loss = 5.89510 avg_loss = 4.12372\n",
      "epoch no.0 train no.64820  loss = 4.92173 avg_loss = 4.15959\n",
      "epoch no.0 train no.64830  loss = 2.81321 avg_loss = 4.13439\n",
      "epoch no.0 train no.64840  loss = 2.40800 avg_loss = 4.03290\n",
      "epoch no.0 train no.64850  loss = 4.43894 avg_loss = 3.97338\n",
      "epoch no.0 train no.64860  loss = 6.00604 avg_loss = 3.98414\n",
      "epoch no.0 train no.64870  loss = 2.71006 avg_loss = 3.96203\n",
      "epoch no.0 train no.64880  loss = 5.11089 avg_loss = 3.95397\n",
      "epoch no.0 train no.64890  loss = 4.33151 avg_loss = 3.94009\n",
      "epoch no.0 train no.64900  loss = 4.69157 avg_loss = 3.91263\n",
      "epoch no.0 train no.64910  loss = 2.10270 avg_loss = 3.87167\n",
      "epoch no.0 train no.64920  loss = 3.99273 avg_loss = 3.86523\n",
      "epoch no.0 train no.64930  loss = 4.32185 avg_loss = 3.90736\n",
      "epoch no.0 train no.64940  loss = 4.15835 avg_loss = 3.86301\n",
      "epoch no.0 train no.64950  loss = 2.77362 avg_loss = 3.80694\n",
      "epoch no.0 train no.64960  loss = 5.00941 avg_loss = 3.86562\n",
      "epoch no.0 train no.64970  loss = 4.49194 avg_loss = 3.92333\n",
      "epoch no.0 train no.64980  loss = 3.20780 avg_loss = 3.88172\n",
      "epoch no.0 train no.64990  loss = 5.61944 avg_loss = 3.89862\n",
      "epoch no.0 train no.65000  loss = 4.28863 avg_loss = 3.89009\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣는', '▁노래', '한', '▁노래', '</s>']\n",
      "여름밤에 듣는 잔잔한 노래</s>\n",
      "epoch no.0 train no.65010  loss = 2.43075 avg_loss = 3.87907\n",
      "epoch no.0 train no.65020  loss = 6.84214 avg_loss = 3.84449\n",
      "epoch no.0 train no.65030  loss = 3.35812 avg_loss = 3.87194\n",
      "epoch no.0 train no.65040  loss = 4.44441 avg_loss = 3.90980\n",
      "epoch no.0 train no.65050  loss = 2.89828 avg_loss = 3.88129\n",
      "epoch no.0 train no.65060  loss = 5.16274 avg_loss = 3.90280\n",
      "epoch no.0 train no.65070  loss = 5.19619 avg_loss = 3.89466\n",
      "epoch no.0 train no.65080  loss = 2.65511 avg_loss = 3.90362\n",
      "epoch no.0 train no.65090  loss = 3.82625 avg_loss = 3.89590\n",
      "epoch no.0 train no.65100  loss = 4.90577 avg_loss = 3.89844\n",
      "epoch no.0 train no.65110  loss = 3.03516 avg_loss = 3.92136\n",
      "epoch no.0 train no.65120  loss = 2.72850 avg_loss = 3.93823\n",
      "epoch no.0 train no.65130  loss = 3.74905 avg_loss = 3.95523\n",
      "epoch no.0 train no.65140  loss = 6.19927 avg_loss = 4.02452\n",
      "epoch no.0 train no.65150  loss = 2.55961 avg_loss = 4.04739\n",
      "epoch no.0 train no.65160  loss = 2.08879 avg_loss = 4.01617\n",
      "epoch no.0 train no.65170  loss = 3.66611 avg_loss = 4.03220\n",
      "epoch no.0 train no.65180  loss = 3.01606 avg_loss = 4.01993\n",
      "epoch no.0 train no.65190  loss = 2.02951 avg_loss = 3.99157\n",
      "epoch no.0 train no.65200  loss = 2.10868 avg_loss = 3.91356\n",
      "epoch no.0 train no.65210  loss = 3.55674 avg_loss = 3.91932\n",
      "epoch no.0 train no.65220  loss = 4.56696 avg_loss = 3.90865\n",
      "epoch no.0 train no.65230  loss = 3.71500 avg_loss = 3.88335\n",
      "epoch no.0 train no.65240  loss = 3.35551 avg_loss = 3.88530\n",
      "epoch no.0 train no.65250  loss = 2.77495 avg_loss = 3.86693\n",
      "epoch no.0 train no.65260  loss = 2.55019 avg_loss = 3.85164\n",
      "epoch no.0 train no.65270  loss = 5.02750 avg_loss = 3.88644\n",
      "epoch no.0 train no.65280  loss = 1.76169 avg_loss = 3.87821\n",
      "epoch no.0 train no.65290  loss = 5.58079 avg_loss = 3.87486\n",
      "epoch no.0 train no.65300  loss = 3.11960 avg_loss = 3.88176\n",
      "epoch no.0 train no.65310  loss = 4.49116 avg_loss = 3.90706\n",
      "epoch no.0 train no.65320  loss = 3.37219 avg_loss = 3.89581\n",
      "epoch no.0 train no.65330  loss = 5.04934 avg_loss = 3.91214\n",
      "epoch no.0 train no.65340  loss = 3.75463 avg_loss = 3.99221\n",
      "epoch no.0 train no.65350  loss = 4.72557 avg_loss = 3.96010\n",
      "epoch no.0 train no.65360  loss = 6.87220 avg_loss = 3.92990\n",
      "epoch no.0 train no.65370  loss = 6.53189 avg_loss = 3.96876\n",
      "epoch no.0 train no.65380  loss = 3.32673 avg_loss = 3.92644\n",
      "epoch no.0 train no.65390  loss = 3.37845 avg_loss = 3.89318\n",
      "epoch no.0 train no.65400  loss = 5.94153 avg_loss = 3.90983\n",
      "epoch no.0 train no.65410  loss = 3.10623 avg_loss = 3.91194\n",
      "epoch no.0 train no.65420  loss = 3.33804 avg_loss = 3.89801\n",
      "epoch no.0 train no.65430  loss = 3.22220 avg_loss = 3.93433\n",
      "epoch no.0 train no.65440  loss = 2.70808 avg_loss = 3.88874\n",
      "epoch no.0 train no.65450  loss = 3.08280 avg_loss = 3.91132\n",
      "epoch no.0 train no.65460  loss = 4.49840 avg_loss = 3.89181\n",
      "epoch no.0 train no.65470  loss = 3.39707 avg_loss = 3.85625\n",
      "epoch no.0 train no.65480  loss = 3.07979 avg_loss = 3.84105\n",
      "epoch no.0 train no.65490  loss = 4.24396 avg_loss = 3.80242\n",
      "epoch no.0 train no.65500  loss = 3.35970 avg_loss = 3.79353\n",
      "epoch no.0 train no.65510  loss = 5.21858 avg_loss = 3.78133\n",
      "epoch no.0 train no.65520  loss = 3.55573 avg_loss = 3.77433\n",
      "epoch no.0 train no.65530  loss = 4.91883 avg_loss = 3.84617\n",
      "epoch no.0 train no.65540  loss = 3.78082 avg_loss = 3.82132\n",
      "epoch no.0 train no.65550  loss = 2.75162 avg_loss = 3.82972\n",
      "epoch no.0 train no.65560  loss = 4.44298 avg_loss = 3.83375\n",
      "epoch no.0 train no.65570  loss = 5.18936 avg_loss = 3.89471\n",
      "epoch no.0 train no.65580  loss = 3.03970 avg_loss = 3.90741\n",
      "epoch no.0 train no.65590  loss = 4.11550 avg_loss = 3.95833\n",
      "epoch no.0 train no.65600  loss = 4.40042 avg_loss = 3.91721\n",
      "epoch no.0 train no.65610  loss = 2.12791 avg_loss = 3.88938\n",
      "epoch no.0 train no.65620  loss = 3.81552 avg_loss = 3.85027\n",
      "epoch no.0 train no.65630  loss = 3.23866 avg_loss = 3.85106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.65640  loss = 2.92314 avg_loss = 3.89543\n",
      "epoch no.0 train no.65650  loss = 2.29754 avg_loss = 3.93072\n",
      "epoch no.0 train no.65660  loss = 3.78496 avg_loss = 3.91349\n",
      "epoch no.0 train no.65670  loss = 1.99655 avg_loss = 3.90260\n",
      "epoch no.0 train no.65680  loss = 2.57508 avg_loss = 3.83296\n",
      "epoch no.0 train no.65690  loss = 3.39459 avg_loss = 3.82919\n",
      "epoch no.0 train no.65700  loss = 3.17358 avg_loss = 3.87078\n",
      "epoch no.0 train no.65710  loss = 6.03961 avg_loss = 3.87497\n",
      "epoch no.0 train no.65720  loss = 4.31575 avg_loss = 3.88037\n",
      "epoch no.0 train no.65730  loss = 5.55769 avg_loss = 3.93219\n",
      "epoch no.0 train no.65740  loss = 3.81650 avg_loss = 3.93402\n",
      "epoch no.0 train no.65750  loss = 3.11524 avg_loss = 3.88557\n",
      "epoch no.0 train no.65760  loss = 4.39079 avg_loss = 3.90367\n",
      "epoch no.0 train no.65770  loss = 5.24748 avg_loss = 3.90261\n",
      "epoch no.0 train no.65780  loss = 3.41370 avg_loss = 3.85598\n",
      "epoch no.0 train no.65790  loss = 2.06135 avg_loss = 3.83182\n",
      "epoch no.0 train no.65800  loss = 5.16974 avg_loss = 3.81854\n",
      "epoch no.0 train no.65810  loss = 3.88237 avg_loss = 3.87968\n",
      "epoch no.0 train no.65820  loss = 4.19730 avg_loss = 3.89234\n",
      "epoch no.0 train no.65830  loss = 5.96575 avg_loss = 3.91754\n",
      "epoch no.0 train no.65840  loss = 4.67902 avg_loss = 3.90009\n",
      "epoch no.0 train no.65850  loss = 3.04483 avg_loss = 3.83787\n",
      "epoch no.0 train no.65860  loss = 5.17767 avg_loss = 3.84642\n",
      "epoch no.0 train no.65870  loss = 5.15103 avg_loss = 3.88033\n",
      "epoch no.0 train no.65880  loss = 2.83446 avg_loss = 3.84176\n",
      "epoch no.0 train no.65890  loss = 3.66961 avg_loss = 3.91970\n",
      "epoch no.0 train no.65900  loss = 5.99167 avg_loss = 3.88437\n",
      "epoch no.0 train no.65910  loss = 2.33395 avg_loss = 3.80659\n",
      "epoch no.0 train no.65920  loss = 3.29342 avg_loss = 3.86513\n",
      "epoch no.0 train no.65930  loss = 4.79651 avg_loss = 3.89976\n",
      "epoch no.0 train no.65940  loss = 3.24731 avg_loss = 3.89257\n",
      "epoch no.0 train no.65950  loss = 3.67348 avg_loss = 3.86905\n",
      "epoch no.0 train no.65960  loss = 4.56233 avg_loss = 3.90613\n",
      "epoch no.0 train no.65970  loss = 4.31522 avg_loss = 3.91495\n",
      "epoch no.0 train no.65980  loss = 5.51765 avg_loss = 3.88484\n",
      "epoch no.0 train no.65990  loss = 5.18515 avg_loss = 3.93395\n",
      "epoch no.0 train no.66000  loss = 2.97929 avg_loss = 3.89205\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '하게', '▁노래', '▁선', '곡', '</s>']\n",
      "여름밤 잔잔한 피아노 연주곡</s>\n",
      "epoch no.0 train no.66010  loss = 2.79436 avg_loss = 3.86034\n",
      "epoch no.0 train no.66020  loss = 1.94759 avg_loss = 3.87710\n",
      "epoch no.0 train no.66030  loss = 3.29740 avg_loss = 3.84689\n",
      "epoch no.0 train no.66040  loss = 3.86041 avg_loss = 3.89073\n",
      "epoch no.0 train no.66050  loss = 3.80414 avg_loss = 3.85252\n",
      "epoch no.0 train no.66060  loss = 4.70081 avg_loss = 3.80741\n",
      "epoch no.0 train no.66070  loss = 5.38989 avg_loss = 3.81276\n",
      "epoch no.0 train no.66080  loss = 1.37210 avg_loss = 3.76328\n",
      "epoch no.0 train no.66090  loss = 2.84609 avg_loss = 3.76569\n",
      "epoch no.0 train no.66100  loss = 3.24409 avg_loss = 3.79754\n",
      "epoch no.0 train no.66110  loss = 5.97166 avg_loss = 3.83429\n",
      "epoch no.0 train no.66120  loss = 3.16348 avg_loss = 3.83680\n",
      "epoch no.0 train no.66130  loss = 4.49981 avg_loss = 3.82881\n",
      "epoch no.0 train no.66140  loss = 3.38784 avg_loss = 3.80019\n",
      "epoch no.0 train no.66150  loss = 2.87290 avg_loss = 3.82824\n",
      "epoch no.0 train no.66160  loss = 1.36834 avg_loss = 3.82479\n",
      "epoch no.0 train no.66170  loss = 3.77899 avg_loss = 3.85941\n",
      "epoch no.0 train no.66180  loss = 2.49470 avg_loss = 3.84168\n",
      "epoch no.0 train no.66190  loss = 3.41725 avg_loss = 3.80497\n",
      "epoch no.0 train no.66200  loss = 5.00910 avg_loss = 3.78681\n",
      "epoch no.0 train no.66210  loss = 6.25608 avg_loss = 3.85596\n",
      "epoch no.0 train no.66220  loss = 4.40453 avg_loss = 3.79939\n",
      "epoch no.0 train no.66230  loss = 3.71635 avg_loss = 3.83225\n",
      "epoch no.0 train no.66240  loss = 6.05373 avg_loss = 3.85180\n",
      "epoch no.0 train no.66250  loss = 3.53545 avg_loss = 3.88808\n",
      "epoch no.0 train no.66260  loss = 3.97148 avg_loss = 3.92180\n",
      "epoch no.0 train no.66270  loss = 3.87342 avg_loss = 3.91114\n",
      "epoch no.0 train no.66280  loss = 2.54615 avg_loss = 3.89001\n",
      "epoch no.0 train no.66290  loss = 6.04146 avg_loss = 3.92189\n",
      "epoch no.0 train no.66300  loss = 3.93319 avg_loss = 3.91083\n",
      "epoch no.0 train no.66310  loss = 3.66744 avg_loss = 3.83430\n",
      "epoch no.0 train no.66320  loss = 3.74719 avg_loss = 3.83901\n",
      "epoch no.0 train no.66330  loss = 4.33480 avg_loss = 3.87372\n",
      "epoch no.0 train no.66340  loss = 3.79628 avg_loss = 3.88516\n",
      "epoch no.0 train no.66350  loss = 2.91122 avg_loss = 3.85460\n",
      "epoch no.0 train no.66360  loss = 2.96771 avg_loss = 3.85347\n",
      "epoch no.0 train no.66370  loss = 4.41826 avg_loss = 3.84164\n",
      "epoch no.0 train no.66380  loss = 1.78102 avg_loss = 3.88106\n",
      "epoch no.0 train no.66390  loss = 3.25810 avg_loss = 3.85689\n",
      "epoch no.0 train no.66400  loss = 4.46405 avg_loss = 3.87889\n",
      "epoch no.0 train no.66410  loss = 4.55328 avg_loss = 3.84047\n",
      "epoch no.0 train no.66420  loss = 4.18925 avg_loss = 3.84354\n",
      "epoch no.0 train no.66430  loss = 5.73009 avg_loss = 3.91359\n",
      "epoch no.0 train no.66440  loss = 2.98857 avg_loss = 3.92051\n",
      "epoch no.0 train no.66450  loss = 2.25607 avg_loss = 3.88695\n",
      "epoch no.0 train no.66460  loss = 3.99679 avg_loss = 3.86677\n",
      "epoch no.0 train no.66470  loss = 3.59527 avg_loss = 3.92518\n",
      "epoch no.0 train no.66480  loss = 4.26872 avg_loss = 3.97112\n",
      "epoch no.0 train no.66490  loss = 5.63481 avg_loss = 3.95728\n",
      "epoch no.0 train no.66500  loss = 3.86937 avg_loss = 3.94164\n",
      "epoch no.0 train no.66510  loss = 3.58432 avg_loss = 3.94205\n",
      "epoch no.0 train no.66520  loss = 4.22311 avg_loss = 3.94004\n",
      "epoch no.0 train no.66530  loss = 4.16525 avg_loss = 3.96000\n",
      "epoch no.0 train no.66540  loss = 3.02934 avg_loss = 3.97266\n",
      "epoch no.0 train no.66550  loss = 3.03972 avg_loss = 3.91348\n",
      "epoch no.0 train no.66560  loss = 4.38402 avg_loss = 3.95241\n",
      "epoch no.0 train no.66570  loss = 3.49459 avg_loss = 3.94339\n",
      "epoch no.0 train no.66580  loss = 3.33965 avg_loss = 3.93473\n",
      "epoch no.0 train no.66590  loss = 2.37149 avg_loss = 3.92103\n",
      "epoch no.0 train no.66600  loss = 4.50736 avg_loss = 3.95182\n",
      "epoch no.0 train no.66610  loss = 2.89367 avg_loss = 3.96574\n",
      "epoch no.0 train no.66620  loss = 3.85095 avg_loss = 3.93196\n",
      "epoch no.0 train no.66630  loss = 3.85519 avg_loss = 3.95511\n",
      "epoch no.0 train no.66640  loss = 3.06889 avg_loss = 3.90132\n",
      "epoch no.0 train no.66650  loss = 4.27527 avg_loss = 3.92878\n",
      "epoch no.0 train no.66660  loss = 5.09427 avg_loss = 3.91932\n",
      "epoch no.0 train no.66670  loss = 3.80821 avg_loss = 3.89001\n",
      "epoch no.0 train no.66680  loss = 3.26032 avg_loss = 3.88016\n",
      "epoch no.0 train no.66690  loss = 3.29804 avg_loss = 3.88155\n",
      "epoch no.0 train no.66700  loss = 2.86565 avg_loss = 3.93447\n",
      "epoch no.0 train no.66710  loss = 2.37963 avg_loss = 4.01212\n",
      "epoch no.0 train no.66720  loss = 4.00014 avg_loss = 3.99073\n",
      "epoch no.0 train no.66730  loss = 3.15102 avg_loss = 3.95753\n",
      "epoch no.0 train no.66740  loss = 5.30976 avg_loss = 3.98108\n",
      "epoch no.0 train no.66750  loss = 2.59147 avg_loss = 4.00095\n",
      "epoch no.0 train no.66760  loss = 4.60070 avg_loss = 4.02805\n",
      "epoch no.0 train no.66770  loss = 4.04529 avg_loss = 3.96153\n",
      "epoch no.0 train no.66780  loss = 3.33196 avg_loss = 4.02929\n",
      "epoch no.0 train no.66790  loss = 2.64169 avg_loss = 3.98585\n",
      "epoch no.0 train no.66800  loss = 3.98840 avg_loss = 3.94783\n",
      "epoch no.0 train no.66810  loss = 1.89354 avg_loss = 3.91902\n",
      "epoch no.0 train no.66820  loss = 3.39660 avg_loss = 3.87837\n",
      "epoch no.0 train no.66830  loss = 4.31124 avg_loss = 3.89771\n",
      "epoch no.0 train no.66840  loss = 3.10281 avg_loss = 3.86028\n",
      "epoch no.0 train no.66850  loss = 3.67812 avg_loss = 3.85869\n",
      "epoch no.0 train no.66860  loss = 3.64067 avg_loss = 3.84106\n",
      "epoch no.0 train no.66870  loss = 2.38605 avg_loss = 3.83708\n",
      "epoch no.0 train no.66880  loss = 4.69546 avg_loss = 3.84600\n",
      "epoch no.0 train no.66890  loss = 4.50899 avg_loss = 3.90718\n",
      "epoch no.0 train no.66900  loss = 4.44280 avg_loss = 3.91785\n",
      "epoch no.0 train no.66910  loss = 3.37325 avg_loss = 3.89898\n",
      "epoch no.0 train no.66920  loss = 2.83463 avg_loss = 3.83588\n",
      "epoch no.0 train no.66930  loss = 2.39931 avg_loss = 3.78687\n",
      "epoch no.0 train no.66940  loss = 4.35616 avg_loss = 3.76910\n",
      "epoch no.0 train no.66950  loss = 2.58576 avg_loss = 3.75370\n",
      "epoch no.0 train no.66960  loss = 4.03216 avg_loss = 3.77063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.66970  loss = 5.99248 avg_loss = 3.79760\n",
      "epoch no.0 train no.66980  loss = 7.58392 avg_loss = 3.84313\n",
      "epoch no.0 train no.66990  loss = 2.66166 avg_loss = 3.78713\n",
      "epoch no.0 train no.67000  loss = 2.09035 avg_loss = 3.73044\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '의', '▁감성', '적인', '▁음악', '</s>']\n",
      "여름밤의 감성적인 노래</s>\n",
      "epoch no.0 train no.67010  loss = 5.06356 avg_loss = 3.70982\n",
      "epoch no.0 train no.67020  loss = 3.88401 avg_loss = 3.74206\n",
      "epoch no.0 train no.67030  loss = 2.52937 avg_loss = 3.74775\n",
      "epoch no.0 train no.67040  loss = 3.30090 avg_loss = 3.77866\n",
      "epoch no.0 train no.67050  loss = 4.49550 avg_loss = 3.79461\n",
      "epoch no.0 train no.67060  loss = 3.34820 avg_loss = 3.82865\n",
      "epoch no.0 train no.67070  loss = 4.83520 avg_loss = 3.81122\n",
      "epoch no.0 train no.67080  loss = 3.02064 avg_loss = 3.80507\n",
      "epoch no.0 train no.67090  loss = 7.77770 avg_loss = 3.82832\n",
      "epoch no.0 train no.67100  loss = 3.67643 avg_loss = 3.81988\n",
      "epoch no.0 train no.67110  loss = 2.04329 avg_loss = 3.78357\n",
      "epoch no.0 train no.67120  loss = 2.94407 avg_loss = 3.75900\n",
      "epoch no.0 train no.67130  loss = 3.05251 avg_loss = 3.76057\n",
      "epoch no.0 train no.67140  loss = 5.26406 avg_loss = 3.78166\n",
      "epoch no.0 train no.67150  loss = 2.10380 avg_loss = 3.77437\n",
      "epoch no.0 train no.67160  loss = 4.32867 avg_loss = 3.76954\n",
      "epoch no.0 train no.67170  loss = 3.55306 avg_loss = 3.79723\n",
      "epoch no.0 train no.67180  loss = 3.99008 avg_loss = 3.81332\n",
      "epoch no.0 train no.67190  loss = 2.68656 avg_loss = 3.80817\n",
      "epoch no.0 train no.67200  loss = 4.70248 avg_loss = 3.83722\n",
      "epoch no.0 train no.67210  loss = 5.70852 avg_loss = 3.89008\n",
      "epoch no.0 train no.67220  loss = 4.57189 avg_loss = 3.84344\n",
      "epoch no.0 train no.67230  loss = 3.05098 avg_loss = 3.83577\n",
      "epoch no.0 train no.67240  loss = 3.42231 avg_loss = 3.82407\n",
      "epoch no.0 train no.67250  loss = 3.29310 avg_loss = 3.78425\n",
      "epoch no.0 train no.67260  loss = 2.48433 avg_loss = 3.77615\n",
      "epoch no.0 train no.67270  loss = 3.42321 avg_loss = 3.74647\n",
      "epoch no.0 train no.67280  loss = 2.83336 avg_loss = 3.78017\n",
      "epoch no.0 train no.67290  loss = 4.30793 avg_loss = 3.90525\n",
      "epoch no.0 train no.67300  loss = 6.08348 avg_loss = 3.94663\n",
      "epoch no.0 train no.67310  loss = 4.14135 avg_loss = 3.91221\n",
      "epoch no.0 train no.67320  loss = 4.54481 avg_loss = 3.89913\n",
      "epoch no.0 train no.67330  loss = 2.31602 avg_loss = 3.89972\n",
      "epoch no.0 train no.67340  loss = 2.91613 avg_loss = 3.79571\n",
      "epoch no.0 train no.67350  loss = 3.79783 avg_loss = 3.82803\n",
      "epoch no.0 train no.67360  loss = 2.66092 avg_loss = 3.89351\n",
      "epoch no.0 train no.67370  loss = 2.75902 avg_loss = 3.89250\n",
      "epoch no.0 train no.67380  loss = 5.50078 avg_loss = 3.86093\n",
      "epoch no.0 train no.67390  loss = 2.81747 avg_loss = 3.87012\n",
      "epoch no.0 train no.67400  loss = 3.80415 avg_loss = 3.87619\n",
      "epoch no.0 train no.67410  loss = 4.59206 avg_loss = 3.95283\n",
      "epoch no.0 train no.67420  loss = 4.13921 avg_loss = 3.97481\n",
      "epoch no.0 train no.67430  loss = 2.74945 avg_loss = 3.97131\n",
      "epoch no.0 train no.67440  loss = 2.60174 avg_loss = 3.99991\n",
      "epoch no.0 train no.67450  loss = 2.92084 avg_loss = 4.01439\n",
      "epoch no.0 train no.67460  loss = 1.86048 avg_loss = 3.98682\n",
      "epoch no.0 train no.67470  loss = 2.83171 avg_loss = 4.00570\n",
      "epoch no.0 train no.67480  loss = 3.54435 avg_loss = 3.99830\n",
      "epoch no.0 train no.67490  loss = 4.78344 avg_loss = 4.04235\n",
      "epoch no.0 train no.67500  loss = 6.92146 avg_loss = 4.00177\n",
      "epoch no.0 train no.67510  loss = 4.34026 avg_loss = 3.97738\n",
      "epoch no.0 train no.67520  loss = 3.67233 avg_loss = 3.92937\n",
      "epoch no.0 train no.67530  loss = 3.49424 avg_loss = 3.95326\n",
      "epoch no.0 train no.67540  loss = 3.26010 avg_loss = 3.95849\n",
      "epoch no.0 train no.67550  loss = 2.66682 avg_loss = 3.98696\n",
      "epoch no.0 train no.67560  loss = 3.54352 avg_loss = 3.94304\n",
      "epoch no.0 train no.67570  loss = 5.82225 avg_loss = 4.02179\n",
      "epoch no.0 train no.67580  loss = 4.08251 avg_loss = 3.93543\n",
      "epoch no.0 train no.67590  loss = 2.31717 avg_loss = 3.95828\n",
      "epoch no.0 train no.67600  loss = 3.80999 avg_loss = 3.93157\n",
      "epoch no.0 train no.67610  loss = 1.59563 avg_loss = 3.87058\n",
      "epoch no.0 train no.67620  loss = 3.79383 avg_loss = 3.88253\n",
      "epoch no.0 train no.67630  loss = 8.07984 avg_loss = 3.90927\n",
      "epoch no.0 train no.67640  loss = 3.44714 avg_loss = 3.98167\n",
      "epoch no.0 train no.67650  loss = 3.38858 avg_loss = 3.98264\n",
      "epoch no.0 train no.67660  loss = 3.78703 avg_loss = 3.92670\n",
      "epoch no.0 train no.67670  loss = 2.18484 avg_loss = 3.93241\n",
      "epoch no.0 train no.67680  loss = 3.37378 avg_loss = 3.91334\n",
      "epoch no.0 train no.67690  loss = 2.28388 avg_loss = 3.83587\n",
      "epoch no.0 train no.67700  loss = 6.06504 avg_loss = 3.81040\n",
      "epoch no.0 train no.67710  loss = 4.37650 avg_loss = 3.80239\n",
      "epoch no.0 train no.67720  loss = 1.93689 avg_loss = 3.75263\n",
      "epoch no.0 train no.67730  loss = 3.42076 avg_loss = 3.75348\n",
      "epoch no.0 train no.67740  loss = 5.27094 avg_loss = 3.75829\n",
      "epoch no.0 train no.67750  loss = 3.04209 avg_loss = 3.77197\n",
      "epoch no.0 train no.67760  loss = 6.56225 avg_loss = 3.81551\n",
      "epoch no.0 train no.67770  loss = 3.58872 avg_loss = 3.83132\n",
      "epoch no.0 train no.67780  loss = 4.53405 avg_loss = 3.82903\n",
      "epoch no.0 train no.67790  loss = 5.72744 avg_loss = 3.90653\n",
      "epoch no.0 train no.67800  loss = 4.88443 avg_loss = 3.90795\n",
      "epoch no.0 train no.67810  loss = 3.90868 avg_loss = 3.93621\n",
      "epoch no.0 train no.67820  loss = 2.83130 avg_loss = 3.89701\n",
      "epoch no.0 train no.67830  loss = 6.32398 avg_loss = 3.88461\n",
      "epoch no.0 train no.67840  loss = 4.95534 avg_loss = 3.86814\n",
      "epoch no.0 train no.67850  loss = 3.54063 avg_loss = 3.79974\n",
      "epoch no.0 train no.67860  loss = 4.08977 avg_loss = 3.79175\n",
      "epoch no.0 train no.67870  loss = 4.42814 avg_loss = 3.77395\n",
      "epoch no.0 train no.67880  loss = 5.51685 avg_loss = 3.86937\n",
      "epoch no.0 train no.67890  loss = 3.52864 avg_loss = 3.84340\n",
      "epoch no.0 train no.67900  loss = 1.86269 avg_loss = 3.79741\n",
      "epoch no.0 train no.67910  loss = 3.00268 avg_loss = 3.77383\n",
      "epoch no.0 train no.67920  loss = 3.02281 avg_loss = 3.71772\n",
      "epoch no.0 train no.67930  loss = 2.49956 avg_loss = 3.70353\n",
      "epoch no.0 train no.67940  loss = 4.36942 avg_loss = 3.77539\n",
      "epoch no.0 train no.67950  loss = 2.78151 avg_loss = 3.80236\n",
      "epoch no.0 train no.67960  loss = 4.21446 avg_loss = 3.79214\n",
      "epoch no.0 train no.67970  loss = 3.69612 avg_loss = 3.88028\n",
      "epoch no.0 train no.67980  loss = 3.15734 avg_loss = 3.82489\n",
      "epoch no.0 train no.67990  loss = 4.60403 avg_loss = 3.86259\n",
      "epoch no.0 train no.68000  loss = 2.18243 avg_loss = 3.85701\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁위한', '▁노래', '발', '라드', '</s>']\n",
      "여름밤을 함께할 감성발라드</s>\n",
      "epoch no.0 train no.68010  loss = 2.91383 avg_loss = 3.84232\n",
      "epoch no.0 train no.68020  loss = 3.70866 avg_loss = 3.87518\n",
      "epoch no.0 train no.68030  loss = 3.89989 avg_loss = 3.81269\n",
      "epoch no.0 train no.68040  loss = 4.19875 avg_loss = 3.76269\n",
      "epoch no.0 train no.68050  loss = 1.94242 avg_loss = 3.75427\n",
      "epoch no.0 train no.68060  loss = 5.88805 avg_loss = 3.81921\n",
      "epoch no.0 train no.68070  loss = 2.11253 avg_loss = 3.88024\n",
      "epoch no.0 train no.68080  loss = 5.82560 avg_loss = 3.86814\n",
      "epoch no.0 train no.68090  loss = 3.20687 avg_loss = 3.85123\n",
      "epoch no.0 train no.68100  loss = 2.55725 avg_loss = 3.80275\n",
      "epoch no.0 train no.68110  loss = 4.77166 avg_loss = 3.80229\n",
      "epoch no.0 train no.68120  loss = 4.05113 avg_loss = 3.84649\n",
      "epoch no.0 train no.68130  loss = 1.85621 avg_loss = 3.82676\n",
      "epoch no.0 train no.68140  loss = 2.77292 avg_loss = 3.77688\n",
      "epoch no.0 train no.68150  loss = 2.80640 avg_loss = 3.76431\n",
      "epoch no.0 train no.68160  loss = 5.13840 avg_loss = 3.75921\n",
      "epoch no.0 train no.68170  loss = 4.69206 avg_loss = 3.77919\n",
      "epoch no.0 train no.68180  loss = 1.28665 avg_loss = 3.79260\n",
      "epoch no.0 train no.68190  loss = 2.30116 avg_loss = 3.80280\n",
      "epoch no.0 train no.68200  loss = 4.03433 avg_loss = 3.75000\n",
      "epoch no.0 train no.68210  loss = 5.33672 avg_loss = 3.86315\n",
      "epoch no.0 train no.68220  loss = 2.86569 avg_loss = 3.83795\n",
      "epoch no.0 train no.68230  loss = 4.03492 avg_loss = 3.82122\n",
      "epoch no.0 train no.68240  loss = 7.52660 avg_loss = 3.83912\n",
      "epoch no.0 train no.68250  loss = 3.81910 avg_loss = 3.79219\n",
      "epoch no.0 train no.68260  loss = 7.41351 avg_loss = 3.87371\n",
      "epoch no.0 train no.68270  loss = 6.24643 avg_loss = 3.91017\n",
      "epoch no.0 train no.68280  loss = 4.62456 avg_loss = 3.93878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.68290  loss = 3.45910 avg_loss = 3.88702\n",
      "epoch no.0 train no.68300  loss = 3.12803 avg_loss = 3.87817\n",
      "epoch no.0 train no.68310  loss = 3.48177 avg_loss = 3.90417\n",
      "epoch no.0 train no.68320  loss = 4.98677 avg_loss = 3.95727\n",
      "epoch no.0 train no.68330  loss = 6.01022 avg_loss = 3.98454\n",
      "epoch no.0 train no.68340  loss = 4.14447 avg_loss = 3.99105\n",
      "epoch no.0 train no.68350  loss = 3.49449 avg_loss = 4.00987\n",
      "epoch no.0 train no.68360  loss = 6.65794 avg_loss = 4.05526\n",
      "epoch no.0 train no.68370  loss = 4.18593 avg_loss = 4.04934\n",
      "epoch no.0 train no.68380  loss = 4.41992 avg_loss = 4.12177\n",
      "epoch no.0 train no.68390  loss = 3.42107 avg_loss = 4.09375\n",
      "epoch no.0 train no.68400  loss = 3.28167 avg_loss = 4.08522\n",
      "epoch no.0 train no.68410  loss = 5.36218 avg_loss = 4.09283\n",
      "epoch no.0 train no.68420  loss = 3.83156 avg_loss = 4.00432\n",
      "epoch no.0 train no.68430  loss = 2.42270 avg_loss = 3.94182\n",
      "epoch no.0 train no.68440  loss = 4.46711 avg_loss = 3.92754\n",
      "epoch no.0 train no.68450  loss = 2.92795 avg_loss = 3.93790\n",
      "epoch no.0 train no.68460  loss = 2.40256 avg_loss = 3.90617\n",
      "epoch no.0 train no.68470  loss = 2.87719 avg_loss = 3.88185\n",
      "epoch no.0 train no.68480  loss = 5.46953 avg_loss = 3.93073\n",
      "epoch no.0 train no.68490  loss = 4.77581 avg_loss = 3.96472\n",
      "epoch no.0 train no.68500  loss = 4.86472 avg_loss = 4.00156\n",
      "epoch no.0 train no.68510  loss = 4.04961 avg_loss = 3.99625\n",
      "epoch no.0 train no.68520  loss = 2.96682 avg_loss = 3.93625\n",
      "epoch no.0 train no.68530  loss = 3.98532 avg_loss = 3.97535\n",
      "epoch no.0 train no.68540  loss = 4.40863 avg_loss = 3.95481\n",
      "epoch no.0 train no.68550  loss = 2.79254 avg_loss = 3.95340\n",
      "epoch no.0 train no.68560  loss = 3.63430 avg_loss = 3.98406\n",
      "epoch no.0 train no.68570  loss = 5.45974 avg_loss = 3.96409\n",
      "epoch no.0 train no.68580  loss = 3.11415 avg_loss = 3.97550\n",
      "epoch no.0 train no.68590  loss = 4.06324 avg_loss = 3.97506\n",
      "epoch no.0 train no.68600  loss = 5.45071 avg_loss = 4.04467\n",
      "epoch no.0 train no.68610  loss = 4.18080 avg_loss = 3.98668\n",
      "epoch no.0 train no.68620  loss = 4.90150 avg_loss = 4.01286\n",
      "epoch no.0 train no.68630  loss = 3.59131 avg_loss = 3.95937\n",
      "epoch no.0 train no.68640  loss = 3.22018 avg_loss = 3.97834\n",
      "epoch no.0 train no.68650  loss = 4.42741 avg_loss = 3.98960\n",
      "epoch no.0 train no.68660  loss = 4.79407 avg_loss = 3.97013\n",
      "epoch no.0 train no.68670  loss = 8.25329 avg_loss = 3.99790\n",
      "epoch no.0 train no.68680  loss = 4.64176 avg_loss = 4.02413\n",
      "epoch no.0 train no.68690  loss = 3.46681 avg_loss = 4.03374\n",
      "epoch no.0 train no.68700  loss = 4.88107 avg_loss = 4.10718\n",
      "epoch no.0 train no.68710  loss = 2.82668 avg_loss = 4.10858\n",
      "epoch no.0 train no.68720  loss = 5.59513 avg_loss = 4.10439\n",
      "epoch no.0 train no.68730  loss = 3.49967 avg_loss = 4.11258\n",
      "epoch no.0 train no.68740  loss = 5.71628 avg_loss = 4.10037\n",
      "epoch no.0 train no.68750  loss = 5.46448 avg_loss = 4.08841\n",
      "epoch no.0 train no.68760  loss = 4.63099 avg_loss = 4.09231\n",
      "epoch no.0 train no.68770  loss = 3.08523 avg_loss = 4.10381\n",
      "epoch no.0 train no.68780  loss = 5.81097 avg_loss = 4.11208\n",
      "epoch no.0 train no.68790  loss = 6.33072 avg_loss = 4.09748\n",
      "epoch no.0 train no.68800  loss = 5.84403 avg_loss = 4.07713\n",
      "epoch no.0 train no.68810  loss = 3.18600 avg_loss = 4.07000\n",
      "epoch no.0 train no.68820  loss = 5.81826 avg_loss = 4.10571\n",
      "epoch no.0 train no.68830  loss = 3.28017 avg_loss = 4.11103\n",
      "epoch no.0 train no.68840  loss = 4.50129 avg_loss = 4.03983\n",
      "epoch no.0 train no.68850  loss = 4.57525 avg_loss = 3.99029\n",
      "epoch no.0 train no.68860  loss = 2.51305 avg_loss = 3.93217\n",
      "epoch no.0 train no.68870  loss = 2.54677 avg_loss = 3.97900\n",
      "epoch no.0 train no.68880  loss = 2.30603 avg_loss = 3.97105\n",
      "epoch no.0 train no.68890  loss = 3.21141 avg_loss = 3.94759\n",
      "epoch no.0 train no.68900  loss = 3.14243 avg_loss = 3.87939\n",
      "epoch no.0 train no.68910  loss = 5.12142 avg_loss = 3.88369\n",
      "epoch no.0 train no.68920  loss = 2.88339 avg_loss = 3.88634\n",
      "epoch no.0 train no.68930  loss = 4.68569 avg_loss = 3.89498\n",
      "epoch no.0 train no.68940  loss = 3.78602 avg_loss = 3.86234\n",
      "epoch no.0 train no.68950  loss = 3.68414 avg_loss = 3.92344\n",
      "epoch no.0 train no.68960  loss = 6.08824 avg_loss = 3.87743\n",
      "epoch no.0 train no.68970  loss = 2.44547 avg_loss = 3.86305\n",
      "epoch no.0 train no.68980  loss = 2.54158 avg_loss = 3.82648\n",
      "epoch no.0 train no.68990  loss = 3.28298 avg_loss = 3.87287\n",
      "epoch no.0 train no.69000  loss = 3.81307 avg_loss = 3.86915\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁노래', '</s>']\n",
      "여름밤에 어울리는 음악</s>\n",
      "epoch no.0 train no.69010  loss = 6.11702 avg_loss = 3.93522\n",
      "epoch no.0 train no.69020  loss = 4.64674 avg_loss = 3.90064\n",
      "epoch no.0 train no.69030  loss = 3.11611 avg_loss = 3.88357\n",
      "epoch no.0 train no.69040  loss = 3.20803 avg_loss = 3.89225\n",
      "epoch no.0 train no.69050  loss = 4.65264 avg_loss = 3.86966\n",
      "epoch no.0 train no.69060  loss = 4.22467 avg_loss = 3.87456\n",
      "epoch no.0 train no.69070  loss = 3.62557 avg_loss = 3.87560\n",
      "epoch no.0 train no.69080  loss = 2.93198 avg_loss = 3.84175\n",
      "epoch no.0 train no.69090  loss = 3.23438 avg_loss = 3.82842\n",
      "epoch no.0 train no.69100  loss = 2.96875 avg_loss = 3.86157\n",
      "epoch no.0 train no.69110  loss = 2.84917 avg_loss = 3.86512\n",
      "epoch no.0 train no.69120  loss = 3.61941 avg_loss = 3.85361\n",
      "epoch no.0 train no.69130  loss = 4.73877 avg_loss = 3.86084\n",
      "epoch no.0 train no.69140  loss = 3.35290 avg_loss = 3.83352\n",
      "epoch no.0 train no.69150  loss = 2.46454 avg_loss = 3.79885\n",
      "epoch no.0 train no.69160  loss = 3.11554 avg_loss = 3.72366\n",
      "epoch no.0 train no.69170  loss = 4.46282 avg_loss = 3.77997\n",
      "epoch no.0 train no.69180  loss = 4.25750 avg_loss = 3.84862\n",
      "epoch no.0 train no.69190  loss = 4.07394 avg_loss = 3.88562\n",
      "epoch no.0 train no.69200  loss = 2.16967 avg_loss = 3.86079\n",
      "epoch no.0 train no.69210  loss = 3.55977 avg_loss = 3.80769\n",
      "epoch no.0 train no.69220  loss = 4.07585 avg_loss = 3.80394\n",
      "epoch no.0 train no.69230  loss = 7.54335 avg_loss = 3.80482\n",
      "epoch no.0 train no.69240  loss = 4.35479 avg_loss = 3.89798\n",
      "epoch no.0 train no.69250  loss = 1.94461 avg_loss = 3.84837\n",
      "epoch no.0 train no.69260  loss = 4.24711 avg_loss = 3.80625\n",
      "epoch no.0 train no.69270  loss = 6.09573 avg_loss = 3.86709\n",
      "epoch no.0 train no.69280  loss = 4.51693 avg_loss = 3.87487\n",
      "epoch no.0 train no.69290  loss = 3.09326 avg_loss = 3.86574\n",
      "epoch no.0 train no.69300  loss = 5.15014 avg_loss = 3.89975\n",
      "epoch no.0 train no.69310  loss = 3.95990 avg_loss = 3.89914\n",
      "epoch no.0 train no.69320  loss = 6.05349 avg_loss = 3.90167\n",
      "epoch no.0 train no.69330  loss = 3.50178 avg_loss = 3.91827\n",
      "epoch no.0 train no.69340  loss = 3.26093 avg_loss = 3.95057\n",
      "epoch no.0 train no.69350  loss = 6.00273 avg_loss = 4.01118\n",
      "epoch no.0 train no.69360  loss = 4.01273 avg_loss = 4.02296\n",
      "epoch no.0 train no.69370  loss = 4.62866 avg_loss = 4.03898\n",
      "epoch no.0 train no.69380  loss = 3.38947 avg_loss = 4.01442\n",
      "epoch no.0 train no.69390  loss = 5.28848 avg_loss = 4.00068\n",
      "epoch no.0 train no.69400  loss = 4.61090 avg_loss = 4.00358\n",
      "epoch no.0 train no.69410  loss = 4.03712 avg_loss = 3.99086\n",
      "epoch no.0 train no.69420  loss = 4.30171 avg_loss = 3.99336\n",
      "epoch no.0 train no.69430  loss = 2.41538 avg_loss = 3.99241\n",
      "epoch no.0 train no.69440  loss = 5.46132 avg_loss = 3.98729\n",
      "epoch no.0 train no.69450  loss = 4.70948 avg_loss = 4.03271\n",
      "epoch no.0 train no.69460  loss = 2.82712 avg_loss = 3.96124\n",
      "epoch no.0 train no.69470  loss = 3.93758 avg_loss = 3.99329\n",
      "epoch no.0 train no.69480  loss = 3.14526 avg_loss = 3.99708\n",
      "epoch no.0 train no.69490  loss = 3.25187 avg_loss = 3.98534\n",
      "epoch no.0 train no.69500  loss = 4.20770 avg_loss = 3.93334\n",
      "epoch no.0 train no.69510  loss = 5.32625 avg_loss = 3.93908\n",
      "epoch no.0 train no.69520  loss = 4.42264 avg_loss = 3.95269\n",
      "epoch no.0 train no.69530  loss = 2.86950 avg_loss = 3.92585\n",
      "epoch no.0 train no.69540  loss = 6.55105 avg_loss = 3.99966\n",
      "epoch no.0 train no.69550  loss = 3.39456 avg_loss = 3.99140\n",
      "epoch no.0 train no.69560  loss = 3.64039 avg_loss = 3.96291\n",
      "epoch no.0 train no.69570  loss = 3.18839 avg_loss = 3.89545\n",
      "epoch no.0 train no.69580  loss = 2.17824 avg_loss = 3.82170\n",
      "epoch no.0 train no.69590  loss = 4.23430 avg_loss = 3.88130\n",
      "epoch no.0 train no.69600  loss = 4.63117 avg_loss = 3.89124\n",
      "epoch no.0 train no.69610  loss = 6.07724 avg_loss = 3.86311\n",
      "epoch no.0 train no.69620  loss = 2.69360 avg_loss = 3.79722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.69630  loss = 5.06528 avg_loss = 3.78823\n",
      "epoch no.0 train no.69640  loss = 4.21651 avg_loss = 3.79682\n",
      "epoch no.0 train no.69650  loss = 3.09247 avg_loss = 3.80418\n",
      "epoch no.0 train no.69660  loss = 5.43704 avg_loss = 3.87745\n",
      "epoch no.0 train no.69670  loss = 2.89557 avg_loss = 3.91576\n",
      "epoch no.0 train no.69680  loss = 4.16864 avg_loss = 3.88913\n",
      "epoch no.0 train no.69690  loss = 4.23305 avg_loss = 3.84227\n",
      "epoch no.0 train no.69700  loss = 4.49644 avg_loss = 3.82460\n",
      "epoch no.0 train no.69710  loss = 3.26869 avg_loss = 3.79473\n",
      "epoch no.0 train no.69720  loss = 4.02141 avg_loss = 3.84506\n",
      "epoch no.0 train no.69730  loss = 3.58637 avg_loss = 3.84756\n",
      "epoch no.0 train no.69740  loss = 3.34942 avg_loss = 3.81413\n",
      "epoch no.0 train no.69750  loss = 4.06532 avg_loss = 3.84493\n",
      "epoch no.0 train no.69760  loss = 4.82254 avg_loss = 3.89261\n",
      "epoch no.0 train no.69770  loss = 4.17989 avg_loss = 3.92957\n",
      "epoch no.0 train no.69780  loss = 3.96915 avg_loss = 3.94581\n",
      "epoch no.0 train no.69790  loss = 3.91274 avg_loss = 3.91140\n",
      "epoch no.0 train no.69800  loss = 2.95987 avg_loss = 3.94956\n",
      "epoch no.0 train no.69810  loss = 2.78651 avg_loss = 3.96859\n",
      "epoch no.0 train no.69820  loss = 3.41576 avg_loss = 3.92701\n",
      "epoch no.0 train no.69830  loss = 3.23242 avg_loss = 3.93312\n",
      "epoch no.0 train no.69840  loss = 3.99209 avg_loss = 3.92064\n",
      "epoch no.0 train no.69850  loss = 3.89084 avg_loss = 3.90334\n",
      "epoch no.0 train no.69860  loss = 2.93652 avg_loss = 3.92916\n",
      "epoch no.0 train no.69870  loss = 3.25328 avg_loss = 3.93543\n",
      "epoch no.0 train no.69880  loss = 5.40433 avg_loss = 3.96794\n",
      "epoch no.0 train no.69890  loss = 3.12442 avg_loss = 4.01621\n",
      "epoch no.0 train no.69900  loss = 5.28998 avg_loss = 3.99502\n",
      "epoch no.0 train no.69910  loss = 3.40180 avg_loss = 3.99971\n",
      "epoch no.0 train no.69920  loss = 4.98899 avg_loss = 4.00978\n",
      "epoch no.0 train no.69930  loss = 4.38101 avg_loss = 4.00168\n",
      "epoch no.0 train no.69940  loss = 5.15056 avg_loss = 4.01994\n",
      "epoch no.0 train no.69950  loss = 2.96876 avg_loss = 4.05263\n",
      "epoch no.0 train no.69960  loss = 4.54282 avg_loss = 4.00108\n",
      "epoch no.0 train no.69970  loss = 3.26968 avg_loss = 4.00207\n",
      "epoch no.0 train no.69980  loss = 2.34254 avg_loss = 3.95593\n",
      "epoch no.0 train no.69990  loss = 3.69306 avg_loss = 3.93602\n",
      "epoch no.0 train no.70000  loss = 4.19705 avg_loss = 3.97300\n",
      "3\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '</s>']\n",
      "여름밤의 감성</s>\n",
      "epoch no.0 train no.70010  loss = 2.64748 avg_loss = 3.94750\n",
      "epoch no.0 train no.70020  loss = 2.62279 avg_loss = 3.91596\n",
      "epoch no.0 train no.70030  loss = 3.02216 avg_loss = 3.88770\n",
      "epoch no.0 train no.70040  loss = 3.40545 avg_loss = 3.87289\n",
      "epoch no.0 train no.70050  loss = 5.64292 avg_loss = 3.84936\n",
      "epoch no.0 train no.70060  loss = 4.39616 avg_loss = 3.91267\n",
      "epoch no.0 train no.70070  loss = 2.60056 avg_loss = 3.90511\n",
      "epoch no.0 train no.70080  loss = 3.14163 avg_loss = 3.88346\n",
      "epoch no.0 train no.70090  loss = 4.59483 avg_loss = 3.92406\n",
      "epoch no.0 train no.70100  loss = 2.83071 avg_loss = 3.87876\n",
      "epoch no.0 train no.70110  loss = 3.28771 avg_loss = 3.88895\n",
      "epoch no.0 train no.70120  loss = 2.57070 avg_loss = 3.87212\n",
      "epoch no.0 train no.70130  loss = 3.36773 avg_loss = 3.90972\n",
      "epoch no.0 train no.70140  loss = 3.26712 avg_loss = 3.88478\n",
      "epoch no.0 train no.70150  loss = 3.74601 avg_loss = 3.90158\n",
      "epoch no.0 train no.70160  loss = 3.84781 avg_loss = 3.87273\n",
      "epoch no.0 train no.70170  loss = 3.74644 avg_loss = 3.89922\n",
      "epoch no.0 train no.70180  loss = 2.76627 avg_loss = 3.92042\n",
      "epoch no.0 train no.70190  loss = 4.65616 avg_loss = 3.92571\n",
      "epoch no.0 train no.70200  loss = 3.95179 avg_loss = 3.94190\n",
      "epoch no.0 train no.70210  loss = 4.05679 avg_loss = 3.90794\n",
      "epoch no.0 train no.70220  loss = 2.63469 avg_loss = 3.85178\n",
      "epoch no.0 train no.70230  loss = 3.82820 avg_loss = 3.83949\n",
      "epoch no.0 train no.70240  loss = 5.22396 avg_loss = 3.86436\n",
      "epoch no.0 train no.70250  loss = 2.83983 avg_loss = 3.80101\n",
      "epoch no.0 train no.70260  loss = 4.59049 avg_loss = 3.81904\n",
      "epoch no.0 train no.70270  loss = 5.71536 avg_loss = 3.81393\n",
      "epoch no.0 train no.70280  loss = 5.85333 avg_loss = 3.86540\n",
      "epoch no.0 train no.70290  loss = 4.01326 avg_loss = 3.87653\n",
      "epoch no.0 train no.70300  loss = 7.05891 avg_loss = 3.88557\n",
      "epoch no.0 train no.70310  loss = 2.63299 avg_loss = 3.89959\n",
      "epoch no.0 train no.70320  loss = 2.98084 avg_loss = 3.91501\n",
      "epoch no.0 train no.70330  loss = 4.37835 avg_loss = 3.91694\n",
      "epoch no.0 train no.70340  loss = 3.63999 avg_loss = 3.89352\n",
      "epoch no.0 train no.70350  loss = 4.88863 avg_loss = 3.91104\n",
      "epoch no.0 train no.70360  loss = 5.68943 avg_loss = 3.89739\n",
      "epoch no.0 train no.70370  loss = 4.86784 avg_loss = 3.91329\n",
      "epoch no.0 train no.70380  loss = 4.11368 avg_loss = 3.91820\n",
      "epoch no.0 train no.70390  loss = 6.22868 avg_loss = 3.94623\n",
      "epoch no.0 train no.70400  loss = 3.18097 avg_loss = 3.92882\n",
      "epoch no.0 train no.70410  loss = 2.76424 avg_loss = 3.90775\n",
      "epoch no.0 train no.70420  loss = 2.62415 avg_loss = 3.93030\n",
      "epoch no.0 train no.70430  loss = 4.04796 avg_loss = 4.01715\n",
      "epoch no.0 train no.70440  loss = 2.65892 avg_loss = 4.02790\n",
      "epoch no.0 train no.70450  loss = 2.57876 avg_loss = 3.97309\n",
      "epoch no.0 train no.70460  loss = 3.79853 avg_loss = 3.90906\n",
      "epoch no.0 train no.70470  loss = 5.56385 avg_loss = 3.86158\n",
      "epoch no.0 train no.70480  loss = 3.41879 avg_loss = 3.93030\n",
      "epoch no.0 train no.70490  loss = 3.22805 avg_loss = 3.91310\n",
      "epoch no.0 train no.70500  loss = 4.40371 avg_loss = 3.88449\n",
      "epoch no.0 train no.70510  loss = 3.25875 avg_loss = 3.88895\n",
      "epoch no.0 train no.70520  loss = 4.18318 avg_loss = 3.87357\n",
      "epoch no.0 train no.70530  loss = 2.44069 avg_loss = 3.84988\n",
      "epoch no.0 train no.70540  loss = 2.00869 avg_loss = 3.85714\n",
      "epoch no.0 train no.70550  loss = 5.16269 avg_loss = 3.89642\n",
      "epoch no.0 train no.70560  loss = 4.28190 avg_loss = 3.92773\n",
      "epoch no.0 train no.70570  loss = 3.40555 avg_loss = 3.85072\n",
      "epoch no.0 train no.70580  loss = 4.79971 avg_loss = 3.89093\n",
      "epoch no.0 train no.70590  loss = 2.10565 avg_loss = 3.80843\n",
      "epoch no.0 train no.70600  loss = 2.08588 avg_loss = 3.81919\n",
      "epoch no.0 train no.70610  loss = 3.73349 avg_loss = 3.74262\n",
      "epoch no.0 train no.70620  loss = 4.75907 avg_loss = 3.77980\n",
      "epoch no.0 train no.70630  loss = 2.79905 avg_loss = 3.75907\n",
      "epoch no.0 train no.70640  loss = 7.89074 avg_loss = 3.82603\n",
      "epoch no.0 train no.70650  loss = 5.14297 avg_loss = 3.87880\n",
      "epoch no.0 train no.70660  loss = 4.39095 avg_loss = 3.88843\n",
      "epoch no.0 train no.70670  loss = 6.53172 avg_loss = 3.87672\n",
      "epoch no.0 train no.70680  loss = 3.56693 avg_loss = 3.88309\n",
      "epoch no.0 train no.70690  loss = 3.11074 avg_loss = 3.87059\n",
      "epoch no.0 train no.70700  loss = 2.85857 avg_loss = 3.84035\n",
      "epoch no.0 train no.70710  loss = 2.47625 avg_loss = 3.87968\n",
      "epoch no.0 train no.70720  loss = 2.84748 avg_loss = 3.83897\n",
      "epoch no.0 train no.70730  loss = 3.52501 avg_loss = 3.84793\n",
      "epoch no.0 train no.70740  loss = 3.00927 avg_loss = 3.84545\n",
      "epoch no.0 train no.70750  loss = 4.88473 avg_loss = 3.84086\n",
      "epoch no.0 train no.70760  loss = 3.89436 avg_loss = 3.78024\n",
      "epoch no.0 train no.70770  loss = 4.81258 avg_loss = 3.78132\n",
      "epoch no.0 train no.70780  loss = 2.93887 avg_loss = 3.76293\n",
      "epoch no.0 train no.70790  loss = 3.95986 avg_loss = 3.79301\n",
      "epoch no.0 train no.70800  loss = 3.43820 avg_loss = 3.78175\n",
      "epoch no.0 train no.70810  loss = 5.38925 avg_loss = 3.86394\n",
      "epoch no.0 train no.70820  loss = 2.57026 avg_loss = 3.87621\n",
      "epoch no.0 train no.70830  loss = 3.62786 avg_loss = 3.93288\n",
      "epoch no.0 train no.70840  loss = 3.11243 avg_loss = 3.93644\n",
      "epoch no.0 train no.70850  loss = 2.84247 avg_loss = 3.88611\n",
      "epoch no.0 train no.70860  loss = 2.24544 avg_loss = 3.78923\n",
      "epoch no.0 train no.70870  loss = 4.00689 avg_loss = 3.77175\n",
      "epoch no.0 train no.70880  loss = 4.63979 avg_loss = 3.80484\n",
      "epoch no.0 train no.70890  loss = 2.15067 avg_loss = 3.73911\n",
      "epoch no.0 train no.70900  loss = 6.91057 avg_loss = 3.79409\n",
      "epoch no.0 train no.70910  loss = 4.45624 avg_loss = 3.71452\n",
      "epoch no.0 train no.70920  loss = 2.25905 avg_loss = 3.67926\n",
      "epoch no.0 train no.70930  loss = 6.99729 avg_loss = 3.72925\n",
      "epoch no.0 train no.70940  loss = 6.44294 avg_loss = 3.74476\n",
      "epoch no.0 train no.70950  loss = 5.39046 avg_loss = 3.72099\n",
      "epoch no.0 train no.70960  loss = 3.44197 avg_loss = 3.65712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.70970  loss = 2.36224 avg_loss = 3.63319\n",
      "epoch no.0 train no.70980  loss = 2.28797 avg_loss = 3.67120\n",
      "epoch no.0 train no.70990  loss = 5.14841 avg_loss = 3.68482\n",
      "epoch no.0 train no.71000  loss = 5.29381 avg_loss = 3.74367\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁위한', '▁할', '▁노래', '힙', 'op', '</s>']\n",
      "여름밤을 함께해줄 감성 pop</s>\n",
      "epoch no.0 train no.71010  loss = 6.32598 avg_loss = 3.73863\n",
      "epoch no.0 train no.71020  loss = 2.36278 avg_loss = 3.68966\n",
      "epoch no.0 train no.71030  loss = 2.57009 avg_loss = 3.64299\n",
      "epoch no.0 train no.71040  loss = 4.30531 avg_loss = 3.66837\n",
      "epoch no.0 train no.71050  loss = 3.27513 avg_loss = 3.66347\n",
      "epoch no.0 train no.71060  loss = 2.49686 avg_loss = 3.65675\n",
      "epoch no.0 train no.71070  loss = 4.77799 avg_loss = 3.65329\n",
      "epoch no.0 train no.71080  loss = 5.52606 avg_loss = 3.63489\n",
      "epoch no.0 train no.71090  loss = 3.75633 avg_loss = 3.64215\n",
      "epoch no.0 train no.71100  loss = 2.62659 avg_loss = 3.71646\n",
      "epoch no.0 train no.71110  loss = 2.74569 avg_loss = 3.75586\n",
      "epoch no.0 train no.71120  loss = 3.20197 avg_loss = 3.80144\n",
      "epoch no.0 train no.71130  loss = 3.96662 avg_loss = 3.81209\n",
      "epoch no.0 train no.71140  loss = 2.72208 avg_loss = 3.80385\n",
      "epoch no.0 train no.71150  loss = 4.35142 avg_loss = 3.79631\n",
      "epoch no.0 train no.71160  loss = 2.82473 avg_loss = 3.76270\n",
      "epoch no.0 train no.71170  loss = 3.90008 avg_loss = 3.80268\n",
      "epoch no.0 train no.71180  loss = 2.96009 avg_loss = 3.73517\n",
      "epoch no.0 train no.71190  loss = 6.74748 avg_loss = 3.71507\n",
      "epoch no.0 train no.71200  loss = 4.95769 avg_loss = 3.78524\n",
      "epoch no.0 train no.71210  loss = 1.82734 avg_loss = 3.76453\n",
      "epoch no.0 train no.71220  loss = 3.58160 avg_loss = 3.79269\n",
      "epoch no.0 train no.71230  loss = 2.63717 avg_loss = 3.79276\n",
      "epoch no.0 train no.71240  loss = 2.21792 avg_loss = 3.76141\n",
      "epoch no.0 train no.71250  loss = 3.92742 avg_loss = 3.84465\n",
      "epoch no.0 train no.71260  loss = 4.20038 avg_loss = 3.81703\n",
      "epoch no.0 train no.71270  loss = 5.80034 avg_loss = 3.79081\n",
      "epoch no.0 train no.71280  loss = 5.42923 avg_loss = 3.83541\n",
      "epoch no.0 train no.71290  loss = 2.74014 avg_loss = 3.81525\n",
      "epoch no.0 train no.71300  loss = 2.83443 avg_loss = 3.82914\n",
      "epoch no.0 train no.71310  loss = 2.22566 avg_loss = 3.73596\n",
      "epoch no.0 train no.71320  loss = 3.23686 avg_loss = 3.73857\n",
      "epoch no.0 train no.71330  loss = 4.14800 avg_loss = 3.74018\n",
      "epoch no.0 train no.71340  loss = 2.57468 avg_loss = 3.72516\n",
      "epoch no.0 train no.71350  loss = 3.61160 avg_loss = 3.81776\n",
      "epoch no.0 train no.71360  loss = 3.93093 avg_loss = 3.78784\n",
      "epoch no.0 train no.71370  loss = 3.16849 avg_loss = 3.77343\n",
      "epoch no.0 train no.71380  loss = 3.57817 avg_loss = 3.76507\n",
      "epoch no.0 train no.71390  loss = 6.46052 avg_loss = 3.79690\n",
      "epoch no.0 train no.71400  loss = 3.29341 avg_loss = 3.81413\n",
      "epoch no.0 train no.71410  loss = 4.15621 avg_loss = 3.75286\n",
      "epoch no.0 train no.71420  loss = 2.44191 avg_loss = 3.83345\n",
      "epoch no.0 train no.71430  loss = 1.41184 avg_loss = 3.81084\n",
      "epoch no.0 train no.71440  loss = 5.09017 avg_loss = 3.82473\n",
      "epoch no.0 train no.71450  loss = 2.86820 avg_loss = 3.80013\n",
      "epoch no.0 train no.71460  loss = 2.69869 avg_loss = 3.75095\n",
      "epoch no.0 train no.71470  loss = 5.12597 avg_loss = 3.77753\n",
      "epoch no.0 train no.71480  loss = 3.09692 avg_loss = 3.69322\n",
      "epoch no.0 train no.71490  loss = 2.53616 avg_loss = 3.70770\n",
      "epoch no.0 train no.71500  loss = 5.18112 avg_loss = 3.75990\n",
      "epoch no.0 train no.71510  loss = 4.73436 avg_loss = 3.78570\n",
      "epoch no.0 train no.71520  loss = 3.17137 avg_loss = 3.83549\n",
      "epoch no.0 train no.71530  loss = 2.40332 avg_loss = 3.84856\n",
      "epoch no.0 train no.71540  loss = 3.95648 avg_loss = 3.83212\n",
      "epoch no.0 train no.71550  loss = 5.93423 avg_loss = 3.87004\n",
      "epoch no.0 train no.71560  loss = 2.62669 avg_loss = 3.86993\n",
      "epoch no.0 train no.71570  loss = 5.24832 avg_loss = 3.88308\n",
      "epoch no.0 train no.71580  loss = 5.65680 avg_loss = 3.86354\n",
      "epoch no.0 train no.71590  loss = 3.62458 avg_loss = 3.80544\n",
      "epoch no.0 train no.71600  loss = 3.06410 avg_loss = 3.81227\n",
      "epoch no.0 train no.71610  loss = 2.60945 avg_loss = 3.82079\n",
      "epoch no.0 train no.71620  loss = 5.31975 avg_loss = 3.85801\n",
      "epoch no.0 train no.71630  loss = 5.03934 avg_loss = 3.86919\n",
      "epoch no.0 train no.71640  loss = 3.83767 avg_loss = 3.87698\n",
      "epoch no.0 train no.71650  loss = 6.17013 avg_loss = 3.87667\n",
      "epoch no.0 train no.71660  loss = 4.64215 avg_loss = 3.96553\n",
      "epoch no.0 train no.71670  loss = 3.09674 avg_loss = 4.00447\n",
      "epoch no.0 train no.71680  loss = 1.91345 avg_loss = 3.96842\n",
      "epoch no.0 train no.71690  loss = 4.63911 avg_loss = 3.97177\n",
      "epoch no.0 train no.71700  loss = 3.18374 avg_loss = 3.94989\n",
      "epoch no.0 train no.71710  loss = 1.69054 avg_loss = 3.92926\n",
      "epoch no.0 train no.71720  loss = 6.60054 avg_loss = 4.01239\n",
      "epoch no.0 train no.71730  loss = 3.44243 avg_loss = 3.98437\n",
      "epoch no.0 train no.71740  loss = 2.54849 avg_loss = 3.95311\n",
      "epoch no.0 train no.71750  loss = 4.50831 avg_loss = 3.92671\n",
      "epoch no.0 train no.71760  loss = 5.95197 avg_loss = 3.94636\n",
      "epoch no.0 train no.71770  loss = 2.80415 avg_loss = 3.89273\n",
      "epoch no.0 train no.71780  loss = 4.75923 avg_loss = 3.84269\n",
      "epoch no.0 train no.71790  loss = 3.33322 avg_loss = 3.86409\n",
      "epoch no.0 train no.71800  loss = 4.86169 avg_loss = 3.87100\n",
      "epoch no.0 train no.71810  loss = 2.36555 avg_loss = 3.87303\n",
      "epoch no.0 train no.71820  loss = 7.48072 avg_loss = 3.90549\n",
      "epoch no.0 train no.71830  loss = 4.07212 avg_loss = 3.92014\n",
      "epoch no.0 train no.71840  loss = 2.96835 avg_loss = 3.90097\n",
      "epoch no.0 train no.71850  loss = 4.71972 avg_loss = 3.97525\n",
      "epoch no.0 train no.71860  loss = 3.73477 avg_loss = 3.94867\n",
      "epoch no.0 train no.71870  loss = 3.87774 avg_loss = 3.98283\n",
      "epoch no.0 train no.71880  loss = 5.63125 avg_loss = 4.06573\n",
      "epoch no.1 train no.71890  loss = 4.32821 avg_loss = 4.05383\n",
      "epoch no.1 train no.71900  loss = 4.01330 avg_loss = 4.03377\n",
      "epoch no.1 train no.71910  loss = 4.60344 avg_loss = 3.96988\n",
      "epoch no.1 train no.71920  loss = 6.03588 avg_loss = 3.94569\n",
      "epoch no.1 train no.71930  loss = 4.40153 avg_loss = 3.96079\n",
      "epoch no.1 train no.71940  loss = 4.26554 avg_loss = 3.85482\n",
      "epoch no.1 train no.71950  loss = 3.66240 avg_loss = 3.81869\n",
      "epoch no.1 train no.71960  loss = 4.83588 avg_loss = 3.81756\n",
      "epoch no.1 train no.71970  loss = 4.51048 avg_loss = 3.81561\n",
      "epoch no.1 train no.71980  loss = 4.04059 avg_loss = 3.73046\n",
      "epoch no.1 train no.71990  loss = 1.81728 avg_loss = 3.72152\n",
      "epoch no.1 train no.72000  loss = 2.38432 avg_loss = 3.66422\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '팝', '▁노래', '</s>']\n",
      "여름밤에 듣는 감성적인 노래</s>\n",
      "epoch no.1 train no.72010  loss = 2.41923 avg_loss = 3.64670\n",
      "epoch no.1 train no.72020  loss = 4.67696 avg_loss = 3.62581\n",
      "epoch no.1 train no.72030  loss = 3.17113 avg_loss = 3.62928\n",
      "epoch no.1 train no.72040  loss = 3.69944 avg_loss = 3.65511\n",
      "epoch no.1 train no.72050  loss = 2.45464 avg_loss = 3.58935\n",
      "epoch no.1 train no.72060  loss = 2.23504 avg_loss = 3.63630\n",
      "epoch no.1 train no.72070  loss = 3.93798 avg_loss = 3.62659\n",
      "epoch no.1 train no.72080  loss = 3.84856 avg_loss = 3.61738\n",
      "epoch no.1 train no.72090  loss = 3.09654 avg_loss = 3.62399\n",
      "epoch no.1 train no.72100  loss = 4.20570 avg_loss = 3.66713\n",
      "epoch no.1 train no.72110  loss = 4.35150 avg_loss = 3.67685\n",
      "epoch no.1 train no.72120  loss = 4.17494 avg_loss = 3.69033\n",
      "epoch no.1 train no.72130  loss = 3.38653 avg_loss = 3.67073\n",
      "epoch no.1 train no.72140  loss = 2.73964 avg_loss = 3.62054\n",
      "epoch no.1 train no.72150  loss = 1.90135 avg_loss = 3.58246\n",
      "epoch no.1 train no.72160  loss = 3.96210 avg_loss = 3.59703\n",
      "epoch no.1 train no.72170  loss = 5.06919 avg_loss = 3.57611\n",
      "epoch no.1 train no.72180  loss = 2.05351 avg_loss = 3.53665\n",
      "epoch no.1 train no.72190  loss = 4.15246 avg_loss = 3.60212\n",
      "epoch no.1 train no.72200  loss = 3.87956 avg_loss = 3.61022\n",
      "epoch no.1 train no.72210  loss = 3.27420 avg_loss = 3.59006\n",
      "epoch no.1 train no.72220  loss = 5.91075 avg_loss = 3.57320\n",
      "epoch no.1 train no.72230  loss = 5.43778 avg_loss = 3.55140\n",
      "epoch no.1 train no.72240  loss = 2.81057 avg_loss = 3.59108\n",
      "epoch no.1 train no.72250  loss = 4.91063 avg_loss = 3.62608\n",
      "epoch no.1 train no.72260  loss = 5.75617 avg_loss = 3.66534\n",
      "epoch no.1 train no.72270  loss = 3.43310 avg_loss = 3.66332\n",
      "epoch no.1 train no.72280  loss = 1.97060 avg_loss = 3.60388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.72290  loss = 2.02483 avg_loss = 3.56403\n",
      "epoch no.1 train no.72300  loss = 2.57288 avg_loss = 3.54325\n",
      "epoch no.1 train no.72310  loss = 4.43788 avg_loss = 3.53989\n",
      "epoch no.1 train no.72320  loss = 4.37854 avg_loss = 3.55568\n",
      "epoch no.1 train no.72330  loss = 3.35918 avg_loss = 3.53148\n",
      "epoch no.1 train no.72340  loss = 3.03537 avg_loss = 3.53654\n",
      "epoch no.1 train no.72350  loss = 5.45090 avg_loss = 3.59682\n",
      "epoch no.1 train no.72360  loss = 4.17552 avg_loss = 3.63219\n",
      "epoch no.1 train no.72370  loss = 5.15929 avg_loss = 3.69890\n",
      "epoch no.1 train no.72380  loss = 3.28519 avg_loss = 3.67396\n",
      "epoch no.1 train no.72390  loss = 2.50167 avg_loss = 3.65279\n",
      "epoch no.1 train no.72400  loss = 4.76013 avg_loss = 3.65452\n",
      "epoch no.1 train no.72410  loss = 3.58741 avg_loss = 3.66514\n",
      "epoch no.1 train no.72420  loss = 5.78152 avg_loss = 3.66037\n",
      "epoch no.1 train no.72430  loss = 5.37034 avg_loss = 3.64975\n",
      "epoch no.1 train no.72440  loss = 5.43191 avg_loss = 3.64458\n",
      "epoch no.1 train no.72450  loss = 5.41769 avg_loss = 3.66584\n",
      "epoch no.1 train no.72460  loss = 2.48132 avg_loss = 3.59806\n",
      "epoch no.1 train no.72470  loss = 5.12766 avg_loss = 3.57208\n",
      "epoch no.1 train no.72480  loss = 5.10885 avg_loss = 3.63352\n",
      "epoch no.1 train no.72490  loss = 3.09593 avg_loss = 3.60477\n",
      "epoch no.1 train no.72500  loss = 2.64856 avg_loss = 3.59202\n",
      "epoch no.1 train no.72510  loss = 4.39206 avg_loss = 3.57155\n",
      "epoch no.1 train no.72520  loss = 4.07865 avg_loss = 3.63944\n",
      "epoch no.1 train no.72530  loss = 4.08491 avg_loss = 3.65092\n",
      "epoch no.1 train no.72540  loss = 4.05584 avg_loss = 3.62245\n",
      "epoch no.1 train no.72550  loss = 6.22537 avg_loss = 3.70165\n",
      "epoch no.1 train no.72560  loss = 4.79783 avg_loss = 3.70448\n",
      "epoch no.1 train no.72570  loss = 4.46358 avg_loss = 3.71453\n",
      "epoch no.1 train no.72580  loss = 2.90553 avg_loss = 3.67711\n",
      "epoch no.1 train no.72590  loss = 4.69683 avg_loss = 3.70353\n",
      "epoch no.1 train no.72600  loss = 4.28629 avg_loss = 3.68368\n",
      "epoch no.1 train no.72610  loss = 4.92869 avg_loss = 3.71528\n",
      "epoch no.1 train no.72620  loss = 1.76020 avg_loss = 3.72943\n",
      "epoch no.1 train no.72630  loss = 3.36333 avg_loss = 3.72795\n",
      "epoch no.1 train no.72640  loss = 3.67612 avg_loss = 3.71560\n",
      "epoch no.1 train no.72650  loss = 3.59633 avg_loss = 3.69637\n",
      "epoch no.1 train no.72660  loss = 4.42354 avg_loss = 3.68108\n",
      "epoch no.1 train no.72670  loss = 4.21105 avg_loss = 3.67316\n",
      "epoch no.1 train no.72680  loss = 2.46333 avg_loss = 3.63355\n",
      "epoch no.1 train no.72690  loss = 2.09177 avg_loss = 3.58655\n",
      "epoch no.1 train no.72700  loss = 2.64578 avg_loss = 3.58344\n",
      "epoch no.1 train no.72710  loss = 5.03780 avg_loss = 3.59785\n",
      "epoch no.1 train no.72720  loss = 6.85884 avg_loss = 3.64293\n",
      "epoch no.1 train no.72730  loss = 3.86439 avg_loss = 3.65122\n",
      "epoch no.1 train no.72740  loss = 3.75613 avg_loss = 3.66222\n",
      "epoch no.1 train no.72750  loss = 3.97435 avg_loss = 3.65097\n",
      "epoch no.1 train no.72760  loss = 5.10202 avg_loss = 3.66935\n",
      "epoch no.1 train no.72770  loss = 1.91298 avg_loss = 3.64445\n",
      "epoch no.1 train no.72780  loss = 3.99546 avg_loss = 3.65503\n",
      "epoch no.1 train no.72790  loss = 4.33845 avg_loss = 3.62783\n",
      "epoch no.1 train no.72800  loss = 2.40328 avg_loss = 3.59041\n",
      "epoch no.1 train no.72810  loss = 3.71807 avg_loss = 3.56749\n",
      "epoch no.1 train no.72820  loss = 4.31488 avg_loss = 3.58050\n",
      "epoch no.1 train no.72830  loss = 2.75564 avg_loss = 3.58340\n",
      "epoch no.1 train no.72840  loss = 2.19056 avg_loss = 3.53184\n",
      "epoch no.1 train no.72850  loss = 3.99083 avg_loss = 3.59055\n",
      "epoch no.1 train no.72860  loss = 2.11685 avg_loss = 3.61268\n",
      "epoch no.1 train no.72870  loss = 3.32098 avg_loss = 3.62693\n",
      "epoch no.1 train no.72880  loss = 2.19622 avg_loss = 3.63092\n",
      "epoch no.1 train no.72890  loss = 4.54389 avg_loss = 3.58918\n",
      "epoch no.1 train no.72900  loss = 3.10940 avg_loss = 3.57727\n",
      "epoch no.1 train no.72910  loss = 4.59479 avg_loss = 3.59247\n",
      "epoch no.1 train no.72920  loss = 3.19867 avg_loss = 3.60707\n",
      "epoch no.1 train no.72930  loss = 3.08529 avg_loss = 3.55363\n",
      "epoch no.1 train no.72940  loss = 3.27408 avg_loss = 3.54176\n",
      "epoch no.1 train no.72950  loss = 4.06601 avg_loss = 3.51276\n",
      "epoch no.1 train no.72960  loss = 5.49885 avg_loss = 3.55436\n",
      "epoch no.1 train no.72970  loss = 3.10747 avg_loss = 3.54231\n",
      "epoch no.1 train no.72980  loss = 3.55463 avg_loss = 3.61922\n",
      "epoch no.1 train no.72990  loss = 3.57776 avg_loss = 3.57421\n",
      "epoch no.1 train no.73000  loss = 2.50570 avg_loss = 3.54835\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁할', '▁딱', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 드라이브에 딱 좋은 곡</s>\n",
      "epoch no.1 train no.73010  loss = 3.35389 avg_loss = 3.48997\n",
      "epoch no.1 train no.73020  loss = 2.67183 avg_loss = 3.53819\n",
      "epoch no.1 train no.73030  loss = 2.35112 avg_loss = 3.56051\n",
      "epoch no.1 train no.73040  loss = 3.63510 avg_loss = 3.53115\n",
      "epoch no.1 train no.73050  loss = 1.21784 avg_loss = 3.48024\n",
      "epoch no.1 train no.73060  loss = 3.47640 avg_loss = 3.49666\n",
      "epoch no.1 train no.73070  loss = 2.67323 avg_loss = 3.50980\n",
      "epoch no.1 train no.73080  loss = 3.67423 avg_loss = 3.53312\n",
      "epoch no.1 train no.73090  loss = 4.04457 avg_loss = 3.55544\n",
      "epoch no.1 train no.73100  loss = 5.23490 avg_loss = 3.54474\n",
      "epoch no.1 train no.73110  loss = 1.93854 avg_loss = 3.55811\n",
      "epoch no.1 train no.73120  loss = 2.76776 avg_loss = 3.53808\n",
      "epoch no.1 train no.73130  loss = 4.93106 avg_loss = 3.51894\n",
      "epoch no.1 train no.73140  loss = 2.52279 avg_loss = 3.50165\n",
      "epoch no.1 train no.73150  loss = 2.96555 avg_loss = 3.49870\n",
      "epoch no.1 train no.73160  loss = 4.98159 avg_loss = 3.51312\n",
      "epoch no.1 train no.73170  loss = 3.13996 avg_loss = 3.54501\n",
      "epoch no.1 train no.73180  loss = 2.34581 avg_loss = 3.56391\n",
      "epoch no.1 train no.73190  loss = 2.28466 avg_loss = 3.54760\n",
      "epoch no.1 train no.73200  loss = 2.23974 avg_loss = 3.55564\n",
      "epoch no.1 train no.73210  loss = 2.81571 avg_loss = 3.51900\n",
      "epoch no.1 train no.73220  loss = 3.72624 avg_loss = 3.47863\n",
      "epoch no.1 train no.73230  loss = 4.45126 avg_loss = 3.56796\n",
      "epoch no.1 train no.73240  loss = 4.90735 avg_loss = 3.57690\n",
      "epoch no.1 train no.73250  loss = 4.98004 avg_loss = 3.54945\n",
      "epoch no.1 train no.73260  loss = 5.56069 avg_loss = 3.56074\n",
      "epoch no.1 train no.73270  loss = 5.71386 avg_loss = 3.60216\n",
      "epoch no.1 train no.73280  loss = 3.23111 avg_loss = 3.61741\n",
      "epoch no.1 train no.73290  loss = 4.36094 avg_loss = 3.65161\n",
      "epoch no.1 train no.73300  loss = 3.58735 avg_loss = 3.70861\n",
      "epoch no.1 train no.73310  loss = 3.12844 avg_loss = 3.73129\n",
      "epoch no.1 train no.73320  loss = 3.74015 avg_loss = 3.67473\n",
      "epoch no.1 train no.73330  loss = 3.34714 avg_loss = 3.70606\n",
      "epoch no.1 train no.73340  loss = 3.66116 avg_loss = 3.68172\n",
      "epoch no.1 train no.73350  loss = 1.50118 avg_loss = 3.65445\n",
      "epoch no.1 train no.73360  loss = 3.78872 avg_loss = 3.63689\n",
      "epoch no.1 train no.73370  loss = 3.57816 avg_loss = 3.62767\n",
      "epoch no.1 train no.73380  loss = 4.03615 avg_loss = 3.72288\n",
      "epoch no.1 train no.73390  loss = 3.79797 avg_loss = 3.70317\n",
      "epoch no.1 train no.73400  loss = 3.99476 avg_loss = 3.67702\n",
      "epoch no.1 train no.73410  loss = 5.09181 avg_loss = 3.71447\n",
      "epoch no.1 train no.73420  loss = 4.62679 avg_loss = 3.67779\n",
      "epoch no.1 train no.73430  loss = 2.99738 avg_loss = 3.71445\n",
      "epoch no.1 train no.73440  loss = 2.47104 avg_loss = 3.69972\n",
      "epoch no.1 train no.73450  loss = 2.08692 avg_loss = 3.66163\n",
      "epoch no.1 train no.73460  loss = 2.24880 avg_loss = 3.62309\n",
      "epoch no.1 train no.73470  loss = 3.02033 avg_loss = 3.61137\n",
      "epoch no.1 train no.73480  loss = 4.87602 avg_loss = 3.57806\n",
      "epoch no.1 train no.73490  loss = 3.83680 avg_loss = 3.60269\n",
      "epoch no.1 train no.73500  loss = 3.08107 avg_loss = 3.58270\n",
      "epoch no.1 train no.73510  loss = 2.83037 avg_loss = 3.57113\n",
      "epoch no.1 train no.73520  loss = 2.37696 avg_loss = 3.54373\n",
      "epoch no.1 train no.73530  loss = 4.52125 avg_loss = 3.50881\n",
      "epoch no.1 train no.73540  loss = 3.65293 avg_loss = 3.51593\n",
      "epoch no.1 train no.73550  loss = 2.72482 avg_loss = 3.51058\n",
      "epoch no.1 train no.73560  loss = 4.13617 avg_loss = 3.46656\n",
      "epoch no.1 train no.73570  loss = 4.90220 avg_loss = 3.44843\n",
      "epoch no.1 train no.73580  loss = 4.04392 avg_loss = 3.45175\n",
      "epoch no.1 train no.73590  loss = 4.35531 avg_loss = 3.41536\n",
      "epoch no.1 train no.73600  loss = 2.72372 avg_loss = 3.46286\n",
      "epoch no.1 train no.73610  loss = 3.43083 avg_loss = 3.48766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.73620  loss = 2.83903 avg_loss = 3.47540\n",
      "epoch no.1 train no.73630  loss = 2.76170 avg_loss = 3.47472\n",
      "epoch no.1 train no.73640  loss = 3.37918 avg_loss = 3.50296\n",
      "epoch no.1 train no.73650  loss = 3.29330 avg_loss = 3.55754\n",
      "epoch no.1 train no.73660  loss = 3.47118 avg_loss = 3.55927\n",
      "epoch no.1 train no.73670  loss = 5.05200 avg_loss = 3.56247\n",
      "epoch no.1 train no.73680  loss = 4.18770 avg_loss = 3.63902\n",
      "epoch no.1 train no.73690  loss = 3.40721 avg_loss = 3.60943\n",
      "epoch no.1 train no.73700  loss = 3.78951 avg_loss = 3.59638\n",
      "epoch no.1 train no.73710  loss = 2.21060 avg_loss = 3.69134\n",
      "epoch no.1 train no.73720  loss = 4.17723 avg_loss = 3.69160\n",
      "epoch no.1 train no.73730  loss = 3.07490 avg_loss = 3.66590\n",
      "epoch no.1 train no.73740  loss = 2.84752 avg_loss = 3.62815\n",
      "epoch no.1 train no.73750  loss = 3.21048 avg_loss = 3.58745\n",
      "epoch no.1 train no.73760  loss = 3.51103 avg_loss = 3.60988\n",
      "epoch no.1 train no.73770  loss = 3.79174 avg_loss = 3.60063\n",
      "epoch no.1 train no.73780  loss = 4.08842 avg_loss = 3.64486\n",
      "epoch no.1 train no.73790  loss = 2.89373 avg_loss = 3.60661\n",
      "epoch no.1 train no.73800  loss = 4.13092 avg_loss = 3.60634\n",
      "epoch no.1 train no.73810  loss = 2.31822 avg_loss = 3.58138\n",
      "epoch no.1 train no.73820  loss = 2.59859 avg_loss = 3.58900\n",
      "epoch no.1 train no.73830  loss = 2.87225 avg_loss = 3.60135\n",
      "epoch no.1 train no.73840  loss = 3.13236 avg_loss = 3.58014\n",
      "epoch no.1 train no.73850  loss = 3.39763 avg_loss = 3.64740\n",
      "epoch no.1 train no.73860  loss = 5.39529 avg_loss = 3.67621\n",
      "epoch no.1 train no.73870  loss = 3.09525 avg_loss = 3.69888\n",
      "epoch no.1 train no.73880  loss = 2.20271 avg_loss = 3.70838\n",
      "epoch no.1 train no.73890  loss = 3.12435 avg_loss = 3.71099\n",
      "epoch no.1 train no.73900  loss = 3.56749 avg_loss = 3.66697\n",
      "epoch no.1 train no.73910  loss = 3.36561 avg_loss = 3.65675\n",
      "epoch no.1 train no.73920  loss = 2.41150 avg_loss = 3.67632\n",
      "epoch no.1 train no.73930  loss = 4.59264 avg_loss = 3.64046\n",
      "epoch no.1 train no.73940  loss = 3.38000 avg_loss = 3.60677\n",
      "epoch no.1 train no.73950  loss = 2.54702 avg_loss = 3.61601\n",
      "epoch no.1 train no.73960  loss = 4.26909 avg_loss = 3.57198\n",
      "epoch no.1 train no.73970  loss = 4.96517 avg_loss = 3.54501\n",
      "epoch no.1 train no.73980  loss = 4.15600 avg_loss = 3.56956\n",
      "epoch no.1 train no.73990  loss = 3.55411 avg_loss = 3.61812\n",
      "epoch no.1 train no.74000  loss = 3.55300 avg_loss = 3.64509\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '에', '▁감성', '적인', '</s>']\n",
      "여름밤의 감성 노래</s>\n",
      "epoch no.1 train no.74010  loss = 2.85670 avg_loss = 3.63563\n",
      "epoch no.1 train no.74020  loss = 5.45957 avg_loss = 3.67925\n",
      "epoch no.1 train no.74030  loss = 3.92367 avg_loss = 3.68044\n",
      "epoch no.1 train no.74040  loss = 2.53949 avg_loss = 3.70793\n",
      "epoch no.1 train no.74050  loss = 2.83314 avg_loss = 3.70020\n",
      "epoch no.1 train no.74060  loss = 4.15844 avg_loss = 3.71281\n",
      "epoch no.1 train no.74070  loss = 3.59084 avg_loss = 3.70211\n",
      "epoch no.1 train no.74080  loss = 3.78359 avg_loss = 3.65533\n",
      "epoch no.1 train no.74090  loss = 6.01323 avg_loss = 3.66141\n",
      "epoch no.1 train no.74100  loss = 2.47133 avg_loss = 3.63173\n",
      "epoch no.1 train no.74110  loss = 3.25358 avg_loss = 3.66716\n",
      "epoch no.1 train no.74120  loss = 2.75885 avg_loss = 3.71244\n",
      "epoch no.1 train no.74130  loss = 3.24917 avg_loss = 3.65946\n",
      "epoch no.1 train no.74140  loss = 3.89714 avg_loss = 3.63829\n",
      "epoch no.1 train no.74150  loss = 3.22011 avg_loss = 3.61047\n",
      "epoch no.1 train no.74160  loss = 3.26880 avg_loss = 3.61190\n",
      "epoch no.1 train no.74170  loss = 4.60053 avg_loss = 3.63364\n",
      "epoch no.1 train no.74180  loss = 2.86388 avg_loss = 3.60104\n",
      "epoch no.1 train no.74190  loss = 3.48459 avg_loss = 3.61536\n",
      "epoch no.1 train no.74200  loss = 2.48936 avg_loss = 3.64895\n",
      "epoch no.1 train no.74210  loss = 1.97840 avg_loss = 3.63040\n",
      "epoch no.1 train no.74220  loss = 2.63575 avg_loss = 3.68610\n",
      "epoch no.1 train no.74230  loss = 1.50241 avg_loss = 3.61208\n",
      "epoch no.1 train no.74240  loss = 3.51256 avg_loss = 3.60269\n",
      "epoch no.1 train no.74250  loss = 3.81739 avg_loss = 3.59669\n",
      "epoch no.1 train no.74260  loss = 5.18660 avg_loss = 3.67246\n",
      "epoch no.1 train no.74270  loss = 3.52454 avg_loss = 3.71059\n",
      "epoch no.1 train no.74280  loss = 3.64218 avg_loss = 3.78069\n",
      "epoch no.1 train no.74290  loss = 1.51519 avg_loss = 3.71909\n",
      "epoch no.1 train no.74300  loss = 2.51926 avg_loss = 3.67080\n",
      "epoch no.1 train no.74310  loss = 2.87694 avg_loss = 3.68532\n",
      "epoch no.1 train no.74320  loss = 2.47072 avg_loss = 3.70696\n",
      "epoch no.1 train no.74330  loss = 2.82915 avg_loss = 3.72991\n",
      "epoch no.1 train no.74340  loss = 3.22868 avg_loss = 3.67312\n",
      "epoch no.1 train no.74350  loss = 2.36628 avg_loss = 3.65814\n",
      "epoch no.1 train no.74360  loss = 5.18297 avg_loss = 3.65363\n",
      "epoch no.1 train no.74370  loss = 1.91911 avg_loss = 3.63215\n",
      "epoch no.1 train no.74380  loss = 4.90807 avg_loss = 3.63840\n",
      "epoch no.1 train no.74390  loss = 2.24789 avg_loss = 3.64069\n",
      "epoch no.1 train no.74400  loss = 2.66315 avg_loss = 3.71081\n",
      "epoch no.1 train no.74410  loss = 3.74019 avg_loss = 3.73129\n",
      "epoch no.1 train no.74420  loss = 5.20272 avg_loss = 3.74356\n",
      "epoch no.1 train no.74430  loss = 5.84642 avg_loss = 3.73546\n",
      "epoch no.1 train no.74440  loss = 2.80001 avg_loss = 3.78629\n",
      "epoch no.1 train no.74450  loss = 3.42550 avg_loss = 3.80746\n",
      "epoch no.1 train no.74460  loss = 2.78932 avg_loss = 3.77988\n",
      "epoch no.1 train no.74470  loss = 4.03921 avg_loss = 3.77856\n",
      "epoch no.1 train no.74480  loss = 3.85096 avg_loss = 3.71637\n",
      "epoch no.1 train no.74490  loss = 5.41024 avg_loss = 3.73724\n",
      "epoch no.1 train no.74500  loss = 2.56839 avg_loss = 3.75572\n",
      "epoch no.1 train no.74510  loss = 2.60096 avg_loss = 3.69968\n",
      "epoch no.1 train no.74520  loss = 4.71444 avg_loss = 3.68605\n",
      "epoch no.1 train no.74530  loss = 3.28182 avg_loss = 3.70163\n",
      "epoch no.1 train no.74540  loss = 2.34511 avg_loss = 3.71047\n",
      "epoch no.1 train no.74550  loss = 1.49414 avg_loss = 3.65863\n",
      "epoch no.1 train no.74560  loss = 3.55118 avg_loss = 3.64091\n",
      "epoch no.1 train no.74570  loss = 2.19857 avg_loss = 3.60954\n",
      "epoch no.1 train no.74580  loss = 5.04382 avg_loss = 3.66433\n",
      "epoch no.1 train no.74590  loss = 3.04415 avg_loss = 3.64625\n",
      "epoch no.1 train no.74600  loss = 4.47880 avg_loss = 3.63276\n",
      "epoch no.1 train no.74610  loss = 3.30180 avg_loss = 3.60602\n",
      "epoch no.1 train no.74620  loss = 3.72108 avg_loss = 3.55917\n",
      "epoch no.1 train no.74630  loss = 5.02390 avg_loss = 3.55339\n",
      "epoch no.1 train no.74640  loss = 5.04375 avg_loss = 3.60882\n",
      "epoch no.1 train no.74650  loss = 2.80678 avg_loss = 3.56761\n",
      "epoch no.1 train no.74660  loss = 3.94086 avg_loss = 3.56720\n",
      "epoch no.1 train no.74670  loss = 6.26758 avg_loss = 3.63300\n",
      "epoch no.1 train no.74680  loss = 4.16328 avg_loss = 3.59295\n",
      "epoch no.1 train no.74690  loss = 3.00265 avg_loss = 3.61766\n",
      "epoch no.1 train no.74700  loss = 2.16990 avg_loss = 3.60401\n",
      "epoch no.1 train no.74710  loss = 3.52374 avg_loss = 3.63595\n",
      "epoch no.1 train no.74720  loss = 3.96411 avg_loss = 3.60705\n",
      "epoch no.1 train no.74730  loss = 3.84328 avg_loss = 3.60123\n",
      "epoch no.1 train no.74740  loss = 2.92033 avg_loss = 3.59989\n",
      "epoch no.1 train no.74750  loss = 3.38196 avg_loss = 3.56238\n",
      "epoch no.1 train no.74760  loss = 3.69305 avg_loss = 3.62086\n",
      "epoch no.1 train no.74770  loss = 3.70538 avg_loss = 3.58259\n",
      "epoch no.1 train no.74780  loss = 3.17738 avg_loss = 3.57864\n",
      "epoch no.1 train no.74790  loss = 2.26369 avg_loss = 3.53381\n",
      "epoch no.1 train no.74800  loss = 3.07540 avg_loss = 3.53807\n",
      "epoch no.1 train no.74810  loss = 2.19984 avg_loss = 3.51507\n",
      "epoch no.1 train no.74820  loss = 2.74369 avg_loss = 3.47826\n",
      "epoch no.1 train no.74830  loss = 2.61133 avg_loss = 3.49830\n",
      "epoch no.1 train no.74840  loss = 3.66256 avg_loss = 3.51506\n",
      "epoch no.1 train no.74850  loss = 4.64322 avg_loss = 3.54822\n",
      "epoch no.1 train no.74860  loss = 4.96640 avg_loss = 3.56888\n",
      "epoch no.1 train no.74870  loss = 5.26955 avg_loss = 3.64082\n",
      "epoch no.1 train no.74880  loss = 4.27754 avg_loss = 3.60716\n",
      "epoch no.1 train no.74890  loss = 3.31180 avg_loss = 3.59184\n",
      "epoch no.1 train no.74900  loss = 5.78914 avg_loss = 3.65100\n",
      "epoch no.1 train no.74910  loss = 5.47071 avg_loss = 3.67223\n",
      "epoch no.1 train no.74920  loss = 2.16137 avg_loss = 3.61949\n",
      "epoch no.1 train no.74930  loss = 2.89335 avg_loss = 3.58763\n",
      "epoch no.1 train no.74940  loss = 3.41069 avg_loss = 3.63385\n",
      "epoch no.1 train no.74950  loss = 3.95085 avg_loss = 3.69810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.74960  loss = 4.09764 avg_loss = 3.70970\n",
      "epoch no.1 train no.74970  loss = 4.23223 avg_loss = 3.76780\n",
      "epoch no.1 train no.74980  loss = 3.57338 avg_loss = 3.77488\n",
      "epoch no.1 train no.74990  loss = 3.72352 avg_loss = 3.79178\n",
      "epoch no.1 train no.75000  loss = 3.11044 avg_loss = 3.77536\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '적인', '▁노래', '</s>']\n",
      "여름밤에 듣는 감성적인 노래</s>\n",
      "epoch no.1 train no.75010  loss = 2.86907 avg_loss = 3.76292\n",
      "epoch no.1 train no.75020  loss = 3.19124 avg_loss = 3.73823\n",
      "epoch no.1 train no.75030  loss = 4.06017 avg_loss = 3.67235\n",
      "epoch no.1 train no.75040  loss = 4.97481 avg_loss = 3.69016\n",
      "epoch no.1 train no.75050  loss = 2.60939 avg_loss = 3.72704\n",
      "epoch no.1 train no.75060  loss = 2.55186 avg_loss = 3.73117\n",
      "epoch no.1 train no.75070  loss = 3.71450 avg_loss = 3.64695\n",
      "epoch no.1 train no.75080  loss = 3.83173 avg_loss = 3.67305\n",
      "epoch no.1 train no.75090  loss = 3.89746 avg_loss = 3.62656\n",
      "epoch no.1 train no.75100  loss = 3.10957 avg_loss = 3.64331\n",
      "epoch no.1 train no.75110  loss = 2.67264 avg_loss = 3.64214\n",
      "epoch no.1 train no.75120  loss = 2.91526 avg_loss = 3.67704\n",
      "epoch no.1 train no.75130  loss = 4.51258 avg_loss = 3.67813\n",
      "epoch no.1 train no.75140  loss = 3.19161 avg_loss = 3.68539\n",
      "epoch no.1 train no.75150  loss = 4.32080 avg_loss = 3.63372\n",
      "epoch no.1 train no.75160  loss = 3.19634 avg_loss = 3.65066\n",
      "epoch no.1 train no.75170  loss = 4.42864 avg_loss = 3.62397\n",
      "epoch no.1 train no.75180  loss = 4.42242 avg_loss = 3.66596\n",
      "epoch no.1 train no.75190  loss = 3.72144 avg_loss = 3.67533\n",
      "epoch no.1 train no.75200  loss = 4.51846 avg_loss = 3.62098\n",
      "epoch no.1 train no.75210  loss = 2.99324 avg_loss = 3.63978\n",
      "epoch no.1 train no.75220  loss = 2.26991 avg_loss = 3.61798\n",
      "epoch no.1 train no.75230  loss = 5.91016 avg_loss = 3.66944\n",
      "epoch no.1 train no.75240  loss = 5.44936 avg_loss = 3.71516\n",
      "epoch no.1 train no.75250  loss = 4.87260 avg_loss = 3.71184\n",
      "epoch no.1 train no.75260  loss = 3.21527 avg_loss = 3.70450\n",
      "epoch no.1 train no.75270  loss = 3.25168 avg_loss = 3.68566\n",
      "epoch no.1 train no.75280  loss = 2.53734 avg_loss = 3.63596\n",
      "epoch no.1 train no.75290  loss = 3.31605 avg_loss = 3.64578\n",
      "epoch no.1 train no.75300  loss = 2.53840 avg_loss = 3.60436\n",
      "epoch no.1 train no.75310  loss = 5.33187 avg_loss = 3.61058\n",
      "epoch no.1 train no.75320  loss = 2.19815 avg_loss = 3.58313\n",
      "epoch no.1 train no.75330  loss = 2.95502 avg_loss = 3.57997\n",
      "epoch no.1 train no.75340  loss = 5.17249 avg_loss = 3.62550\n",
      "epoch no.1 train no.75350  loss = 3.48821 avg_loss = 3.65891\n",
      "epoch no.1 train no.75360  loss = 2.79965 avg_loss = 3.62176\n",
      "epoch no.1 train no.75370  loss = 2.12065 avg_loss = 3.56387\n",
      "epoch no.1 train no.75380  loss = 2.41238 avg_loss = 3.57116\n",
      "epoch no.1 train no.75390  loss = 3.56447 avg_loss = 3.55446\n",
      "epoch no.1 train no.75400  loss = 4.77545 avg_loss = 3.59203\n",
      "epoch no.1 train no.75410  loss = 3.04998 avg_loss = 3.61678\n",
      "epoch no.1 train no.75420  loss = 5.39332 avg_loss = 3.59109\n",
      "epoch no.1 train no.75430  loss = 4.60679 avg_loss = 3.56666\n",
      "epoch no.1 train no.75440  loss = 3.39568 avg_loss = 3.55924\n",
      "epoch no.1 train no.75450  loss = 3.25580 avg_loss = 3.55783\n",
      "epoch no.1 train no.75460  loss = 2.71232 avg_loss = 3.54843\n",
      "epoch no.1 train no.75470  loss = 2.42821 avg_loss = 3.52313\n",
      "epoch no.1 train no.75480  loss = 3.12241 avg_loss = 3.54213\n",
      "epoch no.1 train no.75490  loss = 2.77644 avg_loss = 3.58708\n",
      "epoch no.1 train no.75500  loss = 3.33326 avg_loss = 3.61367\n",
      "epoch no.1 train no.75510  loss = 5.48608 avg_loss = 3.58804\n",
      "epoch no.1 train no.75520  loss = 5.16765 avg_loss = 3.53974\n",
      "epoch no.1 train no.75530  loss = 2.67060 avg_loss = 3.54732\n",
      "epoch no.1 train no.75540  loss = 3.00709 avg_loss = 3.53046\n",
      "epoch no.1 train no.75550  loss = 3.66389 avg_loss = 3.52958\n",
      "epoch no.1 train no.75560  loss = 5.78786 avg_loss = 3.54745\n",
      "epoch no.1 train no.75570  loss = 5.17254 avg_loss = 3.52980\n",
      "epoch no.1 train no.75580  loss = 3.69641 avg_loss = 3.53833\n",
      "epoch no.1 train no.75590  loss = 2.81807 avg_loss = 3.50301\n",
      "epoch no.1 train no.75600  loss = 5.73194 avg_loss = 3.49382\n",
      "epoch no.1 train no.75610  loss = 1.60499 avg_loss = 3.48743\n",
      "epoch no.1 train no.75620  loss = 3.11438 avg_loss = 3.46448\n",
      "epoch no.1 train no.75630  loss = 3.32055 avg_loss = 3.45579\n",
      "epoch no.1 train no.75640  loss = 4.09129 avg_loss = 3.45031\n",
      "epoch no.1 train no.75650  loss = 3.82485 avg_loss = 3.46810\n",
      "epoch no.1 train no.75660  loss = 3.52880 avg_loss = 3.47826\n",
      "epoch no.1 train no.75670  loss = 2.52894 avg_loss = 3.43371\n",
      "epoch no.1 train no.75680  loss = 2.62062 avg_loss = 3.40990\n",
      "epoch no.1 train no.75690  loss = 3.14144 avg_loss = 3.41484\n",
      "epoch no.1 train no.75700  loss = 2.34862 avg_loss = 3.41362\n",
      "epoch no.1 train no.75710  loss = 3.45630 avg_loss = 3.46288\n",
      "epoch no.1 train no.75720  loss = 3.64287 avg_loss = 3.45897\n",
      "epoch no.1 train no.75730  loss = 3.77846 avg_loss = 3.48160\n",
      "epoch no.1 train no.75740  loss = 3.33105 avg_loss = 3.55665\n",
      "epoch no.1 train no.75750  loss = 2.54994 avg_loss = 3.56584\n",
      "epoch no.1 train no.75760  loss = 3.47730 avg_loss = 3.58903\n",
      "epoch no.1 train no.75770  loss = 2.10646 avg_loss = 3.54071\n",
      "epoch no.1 train no.75780  loss = 3.65368 avg_loss = 3.53301\n",
      "epoch no.1 train no.75790  loss = 3.66304 avg_loss = 3.52681\n",
      "epoch no.1 train no.75800  loss = 6.22410 avg_loss = 3.50883\n",
      "epoch no.1 train no.75810  loss = 4.48492 avg_loss = 3.54453\n",
      "epoch no.1 train no.75820  loss = 3.50447 avg_loss = 3.49059\n",
      "epoch no.1 train no.75830  loss = 4.10022 avg_loss = 3.54704\n",
      "epoch no.1 train no.75840  loss = 3.18697 avg_loss = 3.53569\n",
      "epoch no.1 train no.75850  loss = 4.47848 avg_loss = 3.58635\n",
      "epoch no.1 train no.75860  loss = 2.62167 avg_loss = 3.58888\n",
      "epoch no.1 train no.75870  loss = 4.33959 avg_loss = 3.60826\n",
      "epoch no.1 train no.75880  loss = 4.34179 avg_loss = 3.67194\n",
      "epoch no.1 train no.75890  loss = 2.47635 avg_loss = 3.62864\n",
      "epoch no.1 train no.75900  loss = 2.76567 avg_loss = 3.63350\n",
      "epoch no.1 train no.75910  loss = 2.71387 avg_loss = 3.59174\n",
      "epoch no.1 train no.75920  loss = 3.29589 avg_loss = 3.59802\n",
      "epoch no.1 train no.75930  loss = 4.20618 avg_loss = 3.56620\n",
      "epoch no.1 train no.75940  loss = 3.28219 avg_loss = 3.55754\n",
      "epoch no.1 train no.75950  loss = 3.64200 avg_loss = 3.59133\n",
      "epoch no.1 train no.75960  loss = 4.77901 avg_loss = 3.60512\n",
      "epoch no.1 train no.75970  loss = 4.12690 avg_loss = 3.65458\n",
      "epoch no.1 train no.75980  loss = 5.03584 avg_loss = 3.67508\n",
      "epoch no.1 train no.75990  loss = 4.31144 avg_loss = 3.64451\n",
      "epoch no.1 train no.76000  loss = 4.96925 avg_loss = 3.64277\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁듣기', '▁듣는', '▁청량', '▁노래', '</s>']\n",
      "여름밤에 듣는 신나는 노래</s>\n",
      "epoch no.1 train no.76010  loss = 3.85797 avg_loss = 3.65536\n",
      "epoch no.1 train no.76020  loss = 2.81715 avg_loss = 3.64862\n",
      "epoch no.1 train no.76030  loss = 3.95590 avg_loss = 3.63075\n",
      "epoch no.1 train no.76040  loss = 2.44469 avg_loss = 3.59113\n",
      "epoch no.1 train no.76050  loss = 4.28355 avg_loss = 3.58737\n",
      "epoch no.1 train no.76060  loss = 3.26291 avg_loss = 3.54122\n",
      "epoch no.1 train no.76070  loss = 4.00459 avg_loss = 3.61527\n",
      "epoch no.1 train no.76080  loss = 3.84789 avg_loss = 3.62996\n",
      "epoch no.1 train no.76090  loss = 4.84683 avg_loss = 3.60937\n",
      "epoch no.1 train no.76100  loss = 3.42687 avg_loss = 3.64108\n",
      "epoch no.1 train no.76110  loss = 2.58113 avg_loss = 3.65171\n",
      "epoch no.1 train no.76120  loss = 3.40021 avg_loss = 3.61950\n",
      "epoch no.1 train no.76130  loss = 3.34696 avg_loss = 3.58030\n",
      "epoch no.1 train no.76140  loss = 2.39590 avg_loss = 3.61490\n",
      "epoch no.1 train no.76150  loss = 4.44355 avg_loss = 3.58176\n",
      "epoch no.1 train no.76160  loss = 3.89856 avg_loss = 3.57373\n",
      "epoch no.1 train no.76170  loss = 2.34066 avg_loss = 3.52658\n",
      "epoch no.1 train no.76180  loss = 3.94389 avg_loss = 3.51332\n",
      "epoch no.1 train no.76190  loss = 3.54908 avg_loss = 3.52373\n",
      "epoch no.1 train no.76200  loss = 3.75237 avg_loss = 3.51500\n",
      "epoch no.1 train no.76210  loss = 4.29047 avg_loss = 3.50750\n",
      "epoch no.1 train no.76220  loss = 3.85913 avg_loss = 3.50745\n",
      "epoch no.1 train no.76230  loss = 5.66096 avg_loss = 3.50988\n",
      "epoch no.1 train no.76240  loss = 3.30034 avg_loss = 3.50686\n",
      "epoch no.1 train no.76250  loss = 2.34260 avg_loss = 3.46294\n",
      "epoch no.1 train no.76260  loss = 5.88576 avg_loss = 3.51001\n",
      "epoch no.1 train no.76270  loss = 3.85381 avg_loss = 3.62267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.76280  loss = 3.41190 avg_loss = 3.61285\n",
      "epoch no.1 train no.76290  loss = 5.88768 avg_loss = 3.57792\n",
      "epoch no.1 train no.76300  loss = 2.31332 avg_loss = 3.63143\n",
      "epoch no.1 train no.76310  loss = 3.73332 avg_loss = 3.61350\n",
      "epoch no.1 train no.76320  loss = 3.39784 avg_loss = 3.60941\n",
      "epoch no.1 train no.76330  loss = 2.47956 avg_loss = 3.59483\n",
      "epoch no.1 train no.76340  loss = 3.56603 avg_loss = 3.58923\n",
      "epoch no.1 train no.76350  loss = 2.32853 avg_loss = 3.55631\n",
      "epoch no.1 train no.76360  loss = 2.91676 avg_loss = 3.49026\n",
      "epoch no.1 train no.76370  loss = 4.06477 avg_loss = 3.46562\n",
      "epoch no.1 train no.76380  loss = 5.39679 avg_loss = 3.50400\n",
      "epoch no.1 train no.76390  loss = 3.47196 avg_loss = 3.46157\n",
      "epoch no.1 train no.76400  loss = 4.56790 avg_loss = 3.52054\n",
      "epoch no.1 train no.76410  loss = 4.59959 avg_loss = 3.57986\n",
      "epoch no.1 train no.76420  loss = 5.27727 avg_loss = 3.60595\n",
      "epoch no.1 train no.76430  loss = 2.29337 avg_loss = 3.55460\n",
      "epoch no.1 train no.76440  loss = 3.56831 avg_loss = 3.58482\n",
      "epoch no.1 train no.76450  loss = 3.05402 avg_loss = 3.56061\n",
      "epoch no.1 train no.76460  loss = 2.21916 avg_loss = 3.54285\n",
      "epoch no.1 train no.76470  loss = 2.39611 avg_loss = 3.53662\n",
      "epoch no.1 train no.76480  loss = 3.32397 avg_loss = 3.55110\n",
      "epoch no.1 train no.76490  loss = 2.82412 avg_loss = 3.52037\n",
      "epoch no.1 train no.76500  loss = 2.37022 avg_loss = 3.53628\n",
      "epoch no.1 train no.76510  loss = 3.57822 avg_loss = 3.50013\n",
      "epoch no.1 train no.76520  loss = 3.27718 avg_loss = 3.52032\n",
      "epoch no.1 train no.76530  loss = 4.71328 avg_loss = 3.51072\n",
      "epoch no.1 train no.76540  loss = 2.52447 avg_loss = 3.52423\n",
      "epoch no.1 train no.76550  loss = 4.28663 avg_loss = 3.52589\n",
      "epoch no.1 train no.76560  loss = 2.91696 avg_loss = 3.53529\n",
      "epoch no.1 train no.76570  loss = 3.75985 avg_loss = 3.53009\n",
      "epoch no.1 train no.76580  loss = 4.53400 avg_loss = 3.57868\n",
      "epoch no.1 train no.76590  loss = 4.53968 avg_loss = 3.59284\n",
      "epoch no.1 train no.76600  loss = 2.29792 avg_loss = 3.61192\n",
      "epoch no.1 train no.76610  loss = 3.22392 avg_loss = 3.59275\n",
      "epoch no.1 train no.76620  loss = 4.62339 avg_loss = 3.55685\n",
      "epoch no.1 train no.76630  loss = 3.70366 avg_loss = 3.59783\n",
      "epoch no.1 train no.76640  loss = 4.42667 avg_loss = 3.58855\n",
      "epoch no.1 train no.76650  loss = 4.20311 avg_loss = 3.55271\n",
      "epoch no.1 train no.76660  loss = 3.53513 avg_loss = 3.59177\n",
      "epoch no.1 train no.76670  loss = 2.32177 avg_loss = 3.55666\n",
      "epoch no.1 train no.76680  loss = 2.84875 avg_loss = 3.57403\n",
      "epoch no.1 train no.76690  loss = 3.36505 avg_loss = 3.61214\n",
      "epoch no.1 train no.76700  loss = 3.01462 avg_loss = 3.59155\n",
      "epoch no.1 train no.76710  loss = 3.53223 avg_loss = 3.59889\n",
      "epoch no.1 train no.76720  loss = 2.66065 avg_loss = 3.57205\n",
      "epoch no.1 train no.76730  loss = 5.78757 avg_loss = 3.59965\n",
      "epoch no.1 train no.76740  loss = 3.45929 avg_loss = 3.60751\n",
      "epoch no.1 train no.76750  loss = 2.66330 avg_loss = 3.55708\n",
      "epoch no.1 train no.76760  loss = 3.03498 avg_loss = 3.55113\n",
      "epoch no.1 train no.76770  loss = 5.27973 avg_loss = 3.57006\n",
      "epoch no.1 train no.76780  loss = 3.85437 avg_loss = 3.57574\n",
      "epoch no.1 train no.76790  loss = 4.70992 avg_loss = 3.67437\n",
      "epoch no.1 train no.76800  loss = 2.11509 avg_loss = 3.69164\n",
      "epoch no.1 train no.76810  loss = 4.44691 avg_loss = 3.63886\n",
      "epoch no.1 train no.76820  loss = 3.23743 avg_loss = 3.67073\n",
      "epoch no.1 train no.76830  loss = 2.73088 avg_loss = 3.60633\n",
      "epoch no.1 train no.76840  loss = 4.05874 avg_loss = 3.62816\n",
      "epoch no.1 train no.76850  loss = 4.44132 avg_loss = 3.61021\n",
      "epoch no.1 train no.76860  loss = 2.47700 avg_loss = 3.58511\n",
      "epoch no.1 train no.76870  loss = 2.44210 avg_loss = 3.50956\n",
      "epoch no.1 train no.76880  loss = 3.87924 avg_loss = 3.53880\n",
      "epoch no.1 train no.76890  loss = 2.74713 avg_loss = 3.51033\n",
      "epoch no.1 train no.76900  loss = 3.23718 avg_loss = 3.52458\n",
      "epoch no.1 train no.76910  loss = 1.80481 avg_loss = 3.52900\n",
      "epoch no.1 train no.76920  loss = 3.50841 avg_loss = 3.52550\n",
      "epoch no.1 train no.76930  loss = 6.34483 avg_loss = 3.57636\n",
      "epoch no.1 train no.76940  loss = 3.11691 avg_loss = 3.60902\n",
      "epoch no.1 train no.76950  loss = 3.00983 avg_loss = 3.56565\n",
      "epoch no.1 train no.76960  loss = 3.87118 avg_loss = 3.59991\n",
      "epoch no.1 train no.76970  loss = 3.39220 avg_loss = 3.53907\n",
      "epoch no.1 train no.76980  loss = 2.23602 avg_loss = 3.52197\n",
      "epoch no.1 train no.76990  loss = 3.51408 avg_loss = 3.48439\n",
      "epoch no.1 train no.77000  loss = 3.88803 avg_loss = 3.48858\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '에', '▁감성', '</s>', '</s>']\n",
      "여름밤의 감성음악</s>\n",
      "epoch no.1 train no.77010  loss = 3.24810 avg_loss = 3.50396\n",
      "epoch no.1 train no.77020  loss = 3.03358 avg_loss = 3.50189\n",
      "epoch no.1 train no.77030  loss = 2.50921 avg_loss = 3.55189\n",
      "epoch no.1 train no.77040  loss = 3.34894 avg_loss = 3.58367\n",
      "epoch no.1 train no.77050  loss = 6.05230 avg_loss = 3.71471\n",
      "epoch no.1 train no.77060  loss = 3.45947 avg_loss = 3.70277\n",
      "epoch no.1 train no.77070  loss = 3.09285 avg_loss = 3.68632\n",
      "epoch no.1 train no.77080  loss = 4.96535 avg_loss = 3.75452\n",
      "epoch no.1 train no.77090  loss = 1.67453 avg_loss = 3.72901\n",
      "epoch no.1 train no.77100  loss = 2.72306 avg_loss = 3.68231\n",
      "epoch no.1 train no.77110  loss = 3.28285 avg_loss = 3.68367\n",
      "epoch no.1 train no.77120  loss = 5.17135 avg_loss = 3.64656\n",
      "epoch no.1 train no.77130  loss = 3.39592 avg_loss = 3.69026\n",
      "epoch no.1 train no.77140  loss = 2.39036 avg_loss = 3.68440\n",
      "epoch no.1 train no.77150  loss = 3.14698 avg_loss = 3.72547\n",
      "epoch no.1 train no.77160  loss = 5.99425 avg_loss = 3.75451\n",
      "epoch no.1 train no.77170  loss = 3.92194 avg_loss = 3.73078\n",
      "epoch no.1 train no.77180  loss = 2.58645 avg_loss = 3.67481\n",
      "epoch no.1 train no.77190  loss = 5.70604 avg_loss = 3.68322\n",
      "epoch no.1 train no.77200  loss = 2.68545 avg_loss = 3.64005\n",
      "epoch no.1 train no.77210  loss = 2.36151 avg_loss = 3.64087\n",
      "epoch no.1 train no.77220  loss = 3.70704 avg_loss = 3.62785\n",
      "epoch no.1 train no.77230  loss = 2.31306 avg_loss = 3.62605\n",
      "epoch no.1 train no.77240  loss = 4.50062 avg_loss = 3.65385\n",
      "epoch no.1 train no.77250  loss = 4.74925 avg_loss = 3.64816\n",
      "epoch no.1 train no.77260  loss = 3.69131 avg_loss = 3.68468\n",
      "epoch no.1 train no.77270  loss = 2.74694 avg_loss = 3.66422\n",
      "epoch no.1 train no.77280  loss = 3.85653 avg_loss = 3.65311\n",
      "epoch no.1 train no.77290  loss = 3.94229 avg_loss = 3.63034\n",
      "epoch no.1 train no.77300  loss = 3.28186 avg_loss = 3.60822\n",
      "epoch no.1 train no.77310  loss = 4.19470 avg_loss = 3.62064\n",
      "epoch no.1 train no.77320  loss = 3.09473 avg_loss = 3.64075\n",
      "epoch no.1 train no.77330  loss = 2.71319 avg_loss = 3.63286\n",
      "epoch no.1 train no.77340  loss = 3.96310 avg_loss = 3.60622\n",
      "epoch no.1 train no.77350  loss = 3.48662 avg_loss = 3.60762\n",
      "epoch no.1 train no.77360  loss = 4.70715 avg_loss = 3.58319\n",
      "epoch no.1 train no.77370  loss = 3.08083 avg_loss = 3.57981\n",
      "epoch no.1 train no.77380  loss = 3.15543 avg_loss = 3.56022\n",
      "epoch no.1 train no.77390  loss = 2.56774 avg_loss = 3.58360\n",
      "epoch no.1 train no.77400  loss = 3.83009 avg_loss = 3.58426\n",
      "epoch no.1 train no.77410  loss = 4.12278 avg_loss = 3.61738\n",
      "epoch no.1 train no.77420  loss = 3.13922 avg_loss = 3.59471\n",
      "epoch no.1 train no.77430  loss = 6.01238 avg_loss = 3.57467\n",
      "epoch no.1 train no.77440  loss = 3.46631 avg_loss = 3.56598\n",
      "epoch no.1 train no.77450  loss = 2.69208 avg_loss = 3.58824\n",
      "epoch no.1 train no.77460  loss = 3.98384 avg_loss = 3.56834\n",
      "epoch no.1 train no.77470  loss = 4.97933 avg_loss = 3.58335\n",
      "epoch no.1 train no.77480  loss = 3.15689 avg_loss = 3.58119\n",
      "epoch no.1 train no.77490  loss = 3.67092 avg_loss = 3.56401\n",
      "epoch no.1 train no.77500  loss = 3.33005 avg_loss = 3.58735\n",
      "epoch no.1 train no.77510  loss = 2.83365 avg_loss = 3.59662\n",
      "epoch no.1 train no.77520  loss = 3.72972 avg_loss = 3.58209\n",
      "epoch no.1 train no.77530  loss = 4.27535 avg_loss = 3.61600\n",
      "epoch no.1 train no.77540  loss = 1.99498 avg_loss = 3.57683\n",
      "epoch no.1 train no.77550  loss = 4.93114 avg_loss = 3.63337\n",
      "epoch no.1 train no.77560  loss = 4.86022 avg_loss = 3.59802\n",
      "epoch no.1 train no.77570  loss = 5.21378 avg_loss = 3.67386\n",
      "epoch no.1 train no.77580  loss = 4.21353 avg_loss = 3.70433\n",
      "epoch no.1 train no.77590  loss = 4.27765 avg_loss = 3.69533\n",
      "epoch no.1 train no.77600  loss = 6.34856 avg_loss = 3.74621\n",
      "epoch no.1 train no.77610  loss = 2.50179 avg_loss = 3.71131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.77620  loss = 2.33777 avg_loss = 3.69111\n",
      "epoch no.1 train no.77630  loss = 3.51255 avg_loss = 3.65911\n",
      "epoch no.1 train no.77640  loss = 3.42947 avg_loss = 3.63942\n",
      "epoch no.1 train no.77650  loss = 2.47869 avg_loss = 3.61017\n",
      "epoch no.1 train no.77660  loss = 3.81899 avg_loss = 3.57566\n",
      "epoch no.1 train no.77670  loss = 2.45573 avg_loss = 3.62861\n",
      "epoch no.1 train no.77680  loss = 3.34759 avg_loss = 3.58974\n",
      "epoch no.1 train no.77690  loss = 3.37097 avg_loss = 3.57133\n",
      "epoch no.1 train no.77700  loss = 4.75546 avg_loss = 3.52192\n",
      "epoch no.1 train no.77710  loss = 2.96001 avg_loss = 3.55516\n",
      "epoch no.1 train no.77720  loss = 4.05560 avg_loss = 3.58223\n",
      "epoch no.1 train no.77730  loss = 3.30363 avg_loss = 3.53159\n",
      "epoch no.1 train no.77740  loss = 3.55923 avg_loss = 3.49699\n",
      "epoch no.1 train no.77750  loss = 4.40672 avg_loss = 3.52282\n",
      "epoch no.1 train no.77760  loss = 4.53993 avg_loss = 3.54628\n",
      "epoch no.1 train no.77770  loss = 3.97037 avg_loss = 3.54080\n",
      "epoch no.1 train no.77780  loss = 4.15843 avg_loss = 3.60054\n",
      "epoch no.1 train no.77790  loss = 5.74562 avg_loss = 3.61730\n",
      "epoch no.1 train no.77800  loss = 3.12327 avg_loss = 3.59920\n",
      "epoch no.1 train no.77810  loss = 4.20541 avg_loss = 3.62125\n",
      "epoch no.1 train no.77820  loss = 2.95213 avg_loss = 3.56925\n",
      "epoch no.1 train no.77830  loss = 3.17910 avg_loss = 3.50253\n",
      "epoch no.1 train no.77840  loss = 4.21479 avg_loss = 3.46271\n",
      "epoch no.1 train no.77850  loss = 2.98570 avg_loss = 3.44545\n",
      "epoch no.1 train no.77860  loss = 3.36644 avg_loss = 3.53168\n",
      "epoch no.1 train no.77870  loss = 4.81707 avg_loss = 3.52258\n",
      "epoch no.1 train no.77880  loss = 1.93084 avg_loss = 3.51473\n",
      "epoch no.1 train no.77890  loss = 4.61920 avg_loss = 3.53530\n",
      "epoch no.1 train no.77900  loss = 2.18655 avg_loss = 3.53931\n",
      "epoch no.1 train no.77910  loss = 3.85928 avg_loss = 3.55817\n",
      "epoch no.1 train no.77920  loss = 3.86063 avg_loss = 3.61576\n",
      "epoch no.1 train no.77930  loss = 3.02947 avg_loss = 3.60872\n",
      "epoch no.1 train no.77940  loss = 4.70690 avg_loss = 3.64542\n",
      "epoch no.1 train no.77950  loss = 4.46077 avg_loss = 3.69676\n",
      "epoch no.1 train no.77960  loss = 5.10885 avg_loss = 3.65626\n",
      "epoch no.1 train no.77970  loss = 2.18105 avg_loss = 3.67243\n",
      "epoch no.1 train no.77980  loss = 2.27709 avg_loss = 3.69004\n",
      "epoch no.1 train no.77990  loss = 3.70866 avg_loss = 3.71791\n",
      "epoch no.1 train no.78000  loss = 3.43853 avg_loss = 3.67448\n",
      "5\n",
      "to_tokens: ['▁비', '엔', '▁오면', '전에', '▁듣는', '▁노래', '</s>']\n",
      "여름이 가기전에 듣는 노래</s>\n",
      "epoch no.1 train no.78010  loss = 6.50081 avg_loss = 3.67627\n",
      "epoch no.1 train no.78020  loss = 3.33999 avg_loss = 3.70920\n",
      "epoch no.1 train no.78030  loss = 4.12511 avg_loss = 3.71348\n",
      "epoch no.1 train no.78040  loss = 4.02353 avg_loss = 3.79818\n",
      "epoch no.1 train no.78050  loss = 3.28835 avg_loss = 3.73157\n",
      "epoch no.1 train no.78060  loss = 3.09999 avg_loss = 3.64746\n",
      "epoch no.1 train no.78070  loss = 4.90137 avg_loss = 3.64320\n",
      "epoch no.1 train no.78080  loss = 2.30264 avg_loss = 3.61112\n",
      "epoch no.1 train no.78090  loss = 3.26991 avg_loss = 3.61853\n",
      "epoch no.1 train no.78100  loss = 4.77014 avg_loss = 3.65586\n",
      "epoch no.1 train no.78110  loss = 6.14768 avg_loss = 3.70663\n",
      "epoch no.1 train no.78120  loss = 3.59003 avg_loss = 3.66286\n",
      "epoch no.1 train no.78130  loss = 4.79542 avg_loss = 3.67749\n",
      "epoch no.1 train no.78140  loss = 3.18334 avg_loss = 3.69933\n",
      "epoch no.1 train no.78150  loss = 3.32456 avg_loss = 3.68326\n",
      "epoch no.1 train no.78160  loss = 3.92792 avg_loss = 3.73438\n",
      "epoch no.1 train no.78170  loss = 3.82822 avg_loss = 3.75450\n",
      "epoch no.1 train no.78180  loss = 3.47834 avg_loss = 3.74000\n",
      "epoch no.1 train no.78190  loss = 4.13931 avg_loss = 3.77989\n",
      "epoch no.1 train no.78200  loss = 4.34951 avg_loss = 3.81151\n",
      "epoch no.1 train no.78210  loss = 5.62539 avg_loss = 3.79289\n",
      "epoch no.1 train no.78220  loss = 3.60081 avg_loss = 3.79111\n",
      "epoch no.1 train no.78230  loss = 4.28543 avg_loss = 3.76110\n",
      "epoch no.1 train no.78240  loss = 4.64245 avg_loss = 3.75803\n",
      "epoch no.1 train no.78250  loss = 3.42207 avg_loss = 3.73926\n",
      "epoch no.1 train no.78260  loss = 5.20540 avg_loss = 3.72962\n",
      "epoch no.1 train no.78270  loss = 2.47268 avg_loss = 3.73090\n",
      "epoch no.1 train no.78280  loss = 3.67936 avg_loss = 3.77076\n",
      "epoch no.1 train no.78290  loss = 2.35011 avg_loss = 3.73496\n",
      "epoch no.1 train no.78300  loss = 3.45240 avg_loss = 3.70779\n",
      "epoch no.1 train no.78310  loss = 2.70903 avg_loss = 3.72616\n",
      "epoch no.1 train no.78320  loss = 1.61815 avg_loss = 3.74702\n",
      "epoch no.1 train no.78330  loss = 4.17455 avg_loss = 3.75634\n",
      "epoch no.1 train no.78340  loss = 5.93329 avg_loss = 3.81239\n",
      "epoch no.1 train no.78350  loss = 2.73605 avg_loss = 3.77615\n",
      "epoch no.1 train no.78360  loss = 2.13913 avg_loss = 3.80773\n",
      "epoch no.1 train no.78370  loss = 2.57169 avg_loss = 3.74063\n",
      "epoch no.1 train no.78380  loss = 3.62202 avg_loss = 3.70875\n",
      "epoch no.1 train no.78390  loss = 5.19871 avg_loss = 3.71888\n",
      "epoch no.1 train no.78400  loss = 3.05873 avg_loss = 3.72673\n",
      "epoch no.1 train no.78410  loss = 4.19365 avg_loss = 3.71627\n",
      "epoch no.1 train no.78420  loss = 3.54594 avg_loss = 3.73461\n",
      "epoch no.1 train no.78430  loss = 1.48069 avg_loss = 3.65841\n",
      "epoch no.1 train no.78440  loss = 3.04889 avg_loss = 3.69486\n",
      "epoch no.1 train no.78450  loss = 7.10190 avg_loss = 3.72127\n",
      "epoch no.1 train no.78460  loss = 3.39106 avg_loss = 3.67619\n",
      "epoch no.1 train no.78470  loss = 3.87102 avg_loss = 3.64437\n",
      "epoch no.1 train no.78480  loss = 4.43995 avg_loss = 3.65634\n",
      "epoch no.1 train no.78490  loss = 2.40140 avg_loss = 3.61872\n",
      "epoch no.1 train no.78500  loss = 5.39032 avg_loss = 3.62421\n",
      "epoch no.1 train no.78510  loss = 6.05984 avg_loss = 3.72328\n",
      "epoch no.1 train no.78520  loss = 2.49832 avg_loss = 3.69150\n",
      "epoch no.1 train no.78530  loss = 5.69366 avg_loss = 3.74177\n",
      "epoch no.1 train no.78540  loss = 2.94888 avg_loss = 3.68950\n",
      "epoch no.1 train no.78550  loss = 3.98972 avg_loss = 3.68662\n",
      "epoch no.1 train no.78560  loss = 5.08943 avg_loss = 3.70400\n",
      "epoch no.1 train no.78570  loss = 4.35385 avg_loss = 3.70588\n",
      "epoch no.1 train no.78580  loss = 2.90155 avg_loss = 3.65077\n",
      "epoch no.1 train no.78590  loss = 5.70956 avg_loss = 3.63344\n",
      "epoch no.1 train no.78600  loss = 4.20565 avg_loss = 3.72047\n",
      "epoch no.1 train no.78610  loss = 3.64220 avg_loss = 3.73775\n",
      "epoch no.1 train no.78620  loss = 2.49901 avg_loss = 3.71672\n",
      "epoch no.1 train no.78630  loss = 2.87326 avg_loss = 3.66721\n",
      "epoch no.1 train no.78640  loss = 4.16425 avg_loss = 3.62937\n",
      "epoch no.1 train no.78650  loss = 4.85422 avg_loss = 3.64104\n",
      "epoch no.1 train no.78660  loss = 5.30720 avg_loss = 3.66255\n",
      "epoch no.1 train no.78670  loss = 3.06907 avg_loss = 3.61217\n",
      "epoch no.1 train no.78680  loss = 4.84507 avg_loss = 3.56219\n",
      "epoch no.1 train no.78690  loss = 5.67638 avg_loss = 3.58527\n",
      "epoch no.1 train no.78700  loss = 3.66688 avg_loss = 3.59422\n",
      "epoch no.1 train no.78710  loss = 3.82768 avg_loss = 3.61892\n",
      "epoch no.1 train no.78720  loss = 3.63754 avg_loss = 3.61016\n",
      "epoch no.1 train no.78730  loss = 5.64155 avg_loss = 3.68197\n",
      "epoch no.1 train no.78740  loss = 3.43490 avg_loss = 3.64952\n",
      "epoch no.1 train no.78750  loss = 4.01783 avg_loss = 3.68531\n",
      "epoch no.1 train no.78760  loss = 2.62103 avg_loss = 3.68409\n",
      "epoch no.1 train no.78770  loss = 3.99781 avg_loss = 3.71694\n",
      "epoch no.1 train no.78780  loss = 2.95997 avg_loss = 3.74204\n",
      "epoch no.1 train no.78790  loss = 4.93745 avg_loss = 3.73282\n",
      "epoch no.1 train no.78800  loss = 3.12189 avg_loss = 3.75607\n",
      "epoch no.1 train no.78810  loss = 4.37437 avg_loss = 3.72341\n",
      "epoch no.1 train no.78820  loss = 2.76145 avg_loss = 3.72261\n",
      "epoch no.1 train no.78830  loss = 4.20441 avg_loss = 3.73490\n",
      "epoch no.1 train no.78840  loss = 3.18732 avg_loss = 3.69998\n",
      "epoch no.1 train no.78850  loss = 2.29824 avg_loss = 3.65021\n",
      "epoch no.1 train no.78860  loss = 4.15268 avg_loss = 3.65024\n",
      "epoch no.1 train no.78870  loss = 3.28206 avg_loss = 3.66007\n",
      "epoch no.1 train no.78880  loss = 5.56543 avg_loss = 3.68727\n",
      "epoch no.1 train no.78890  loss = 2.28512 avg_loss = 3.67209\n",
      "epoch no.1 train no.78900  loss = 4.36734 avg_loss = 3.65868\n",
      "epoch no.1 train no.78910  loss = 5.90902 avg_loss = 3.65030\n",
      "epoch no.1 train no.78920  loss = 2.25131 avg_loss = 3.55382\n",
      "epoch no.1 train no.78930  loss = 4.63259 avg_loss = 3.54746\n",
      "epoch no.1 train no.78940  loss = 5.32683 avg_loss = 3.59715\n",
      "epoch no.1 train no.78950  loss = 4.28534 avg_loss = 3.55079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.78960  loss = 4.86261 avg_loss = 3.55906\n",
      "epoch no.1 train no.78970  loss = 2.46029 avg_loss = 3.55682\n",
      "epoch no.1 train no.78980  loss = 2.30557 avg_loss = 3.56257\n",
      "epoch no.1 train no.78990  loss = 3.69705 avg_loss = 3.53629\n",
      "epoch no.1 train no.79000  loss = 3.18964 avg_loss = 3.48460\n",
      "7\n",
      "to_tokens: ['▁가을', '엔', '에', '▁듣기', '▁감성', '▁있는', '▁재즈', '</s>', '</s>']\n",
      "여름밤에 어울리는 분위기있는 팝송</s>\n",
      "epoch no.1 train no.79010  loss = 3.86728 avg_loss = 3.49864\n",
      "epoch no.1 train no.79020  loss = 2.53592 avg_loss = 3.51310\n",
      "epoch no.1 train no.79030  loss = 3.51360 avg_loss = 3.55791\n",
      "epoch no.1 train no.79040  loss = 3.90521 avg_loss = 3.52831\n",
      "epoch no.1 train no.79050  loss = 4.24881 avg_loss = 3.53383\n",
      "epoch no.1 train no.79060  loss = 3.73240 avg_loss = 3.51530\n",
      "epoch no.1 train no.79070  loss = 2.80013 avg_loss = 3.50051\n",
      "epoch no.1 train no.79080  loss = 3.07666 avg_loss = 3.51353\n",
      "epoch no.1 train no.79090  loss = 3.89878 avg_loss = 3.50341\n",
      "epoch no.1 train no.79100  loss = 2.74140 avg_loss = 3.54199\n",
      "epoch no.1 train no.79110  loss = 4.89660 avg_loss = 3.54388\n",
      "epoch no.1 train no.79120  loss = 3.67484 avg_loss = 3.52236\n",
      "epoch no.1 train no.79130  loss = 3.10248 avg_loss = 3.50287\n",
      "epoch no.1 train no.79140  loss = 4.44001 avg_loss = 3.55684\n",
      "epoch no.1 train no.79150  loss = 5.62342 avg_loss = 3.62648\n",
      "epoch no.1 train no.79160  loss = 2.40062 avg_loss = 3.64870\n",
      "epoch no.1 train no.79170  loss = 2.92350 avg_loss = 3.60835\n",
      "epoch no.1 train no.79180  loss = 5.07295 avg_loss = 3.61319\n",
      "epoch no.1 train no.79190  loss = 5.27789 avg_loss = 3.65272\n",
      "epoch no.1 train no.79200  loss = 4.01500 avg_loss = 3.66188\n",
      "epoch no.1 train no.79210  loss = 4.47797 avg_loss = 3.66443\n",
      "epoch no.1 train no.79220  loss = 6.23016 avg_loss = 3.65705\n",
      "epoch no.1 train no.79230  loss = 2.55166 avg_loss = 3.61349\n",
      "epoch no.1 train no.79240  loss = 3.17289 avg_loss = 3.58398\n",
      "epoch no.1 train no.79250  loss = 3.18839 avg_loss = 3.59784\n",
      "epoch no.1 train no.79260  loss = 3.32263 avg_loss = 3.53904\n",
      "epoch no.1 train no.79270  loss = 5.33237 avg_loss = 3.57674\n",
      "epoch no.1 train no.79280  loss = 3.11024 avg_loss = 3.57744\n",
      "epoch no.1 train no.79290  loss = 2.65227 avg_loss = 3.58278\n",
      "epoch no.1 train no.79300  loss = 5.06569 avg_loss = 3.58364\n",
      "epoch no.1 train no.79310  loss = 1.93570 avg_loss = 3.55100\n",
      "epoch no.1 train no.79320  loss = 2.66694 avg_loss = 3.49057\n",
      "epoch no.1 train no.79330  loss = 3.18713 avg_loss = 3.50540\n",
      "epoch no.1 train no.79340  loss = 2.67304 avg_loss = 3.46699\n",
      "epoch no.1 train no.79350  loss = 4.18173 avg_loss = 3.47539\n",
      "epoch no.1 train no.79360  loss = 3.22633 avg_loss = 3.47911\n",
      "epoch no.1 train no.79370  loss = 4.09646 avg_loss = 3.49443\n",
      "epoch no.1 train no.79380  loss = 3.78167 avg_loss = 3.51736\n",
      "epoch no.1 train no.79390  loss = 3.28607 avg_loss = 3.51159\n",
      "epoch no.1 train no.79400  loss = 2.14708 avg_loss = 3.52854\n",
      "epoch no.1 train no.79410  loss = 2.59866 avg_loss = 3.55261\n",
      "epoch no.1 train no.79420  loss = 2.72285 avg_loss = 3.52771\n",
      "epoch no.1 train no.79430  loss = 4.61915 avg_loss = 3.52982\n",
      "epoch no.1 train no.79440  loss = 2.08073 avg_loss = 3.47627\n",
      "epoch no.1 train no.79450  loss = 5.50837 avg_loss = 3.46622\n",
      "epoch no.1 train no.79460  loss = 2.77271 avg_loss = 3.52560\n",
      "epoch no.1 train no.79470  loss = 3.54102 avg_loss = 3.55578\n",
      "epoch no.1 train no.79480  loss = 3.35947 avg_loss = 3.58454\n",
      "epoch no.1 train no.79490  loss = 2.73726 avg_loss = 3.58452\n",
      "epoch no.1 train no.79500  loss = 5.32580 avg_loss = 3.61210\n",
      "epoch no.1 train no.79510  loss = 5.00916 avg_loss = 3.60716\n",
      "epoch no.1 train no.79520  loss = 5.68134 avg_loss = 3.62299\n",
      "epoch no.1 train no.79530  loss = 5.13256 avg_loss = 3.66214\n",
      "epoch no.1 train no.79540  loss = 4.33244 avg_loss = 3.68094\n",
      "epoch no.1 train no.79550  loss = 3.70811 avg_loss = 3.70384\n",
      "epoch no.1 train no.79560  loss = 3.97919 avg_loss = 3.68270\n",
      "epoch no.1 train no.79570  loss = 2.73148 avg_loss = 3.65434\n",
      "epoch no.1 train no.79580  loss = 2.46207 avg_loss = 3.57974\n",
      "epoch no.1 train no.79590  loss = 2.06217 avg_loss = 3.57850\n",
      "epoch no.1 train no.79600  loss = 3.24237 avg_loss = 3.55521\n",
      "epoch no.1 train no.79610  loss = 3.44799 avg_loss = 3.59047\n",
      "epoch no.1 train no.79620  loss = 4.78714 avg_loss = 3.57318\n",
      "epoch no.1 train no.79630  loss = 5.19664 avg_loss = 3.59584\n",
      "epoch no.1 train no.79640  loss = 2.88182 avg_loss = 3.56530\n",
      "epoch no.1 train no.79650  loss = 3.27303 avg_loss = 3.55314\n",
      "epoch no.1 train no.79660  loss = 4.27809 avg_loss = 3.56419\n",
      "epoch no.1 train no.79670  loss = 2.78480 avg_loss = 3.59553\n",
      "epoch no.1 train no.79680  loss = 6.14752 avg_loss = 3.56709\n",
      "epoch no.1 train no.79690  loss = 4.85669 avg_loss = 3.60483\n",
      "epoch no.1 train no.79700  loss = 4.94556 avg_loss = 3.62144\n",
      "epoch no.1 train no.79710  loss = 4.12754 avg_loss = 3.60144\n",
      "epoch no.1 train no.79720  loss = 4.49476 avg_loss = 3.53512\n",
      "epoch no.1 train no.79730  loss = 2.53013 avg_loss = 3.55306\n",
      "epoch no.1 train no.79740  loss = 3.08799 avg_loss = 3.57670\n",
      "epoch no.1 train no.79750  loss = 5.15476 avg_loss = 3.62691\n",
      "epoch no.1 train no.79760  loss = 3.49928 avg_loss = 3.55720\n",
      "epoch no.1 train no.79770  loss = 2.64301 avg_loss = 3.55179\n",
      "epoch no.1 train no.79780  loss = 3.66459 avg_loss = 3.58246\n",
      "epoch no.1 train no.79790  loss = 2.23008 avg_loss = 3.60849\n",
      "epoch no.1 train no.79800  loss = 2.64672 avg_loss = 3.63829\n",
      "epoch no.1 train no.79810  loss = 3.05309 avg_loss = 3.66212\n",
      "epoch no.1 train no.79820  loss = 5.58340 avg_loss = 3.69403\n",
      "epoch no.1 train no.79830  loss = 2.87706 avg_loss = 3.67178\n",
      "epoch no.1 train no.79840  loss = 2.66770 avg_loss = 3.58476\n",
      "epoch no.1 train no.79850  loss = 4.29968 avg_loss = 3.61443\n",
      "epoch no.1 train no.79860  loss = 2.99126 avg_loss = 3.63572\n",
      "epoch no.1 train no.79870  loss = 2.31739 avg_loss = 3.67191\n",
      "epoch no.1 train no.79880  loss = 3.01489 avg_loss = 3.69683\n",
      "epoch no.1 train no.79890  loss = 2.49728 avg_loss = 3.69457\n",
      "epoch no.1 train no.79900  loss = 3.03811 avg_loss = 3.67410\n",
      "epoch no.1 train no.79910  loss = 4.04453 avg_loss = 3.65107\n",
      "epoch no.1 train no.79920  loss = 3.18333 avg_loss = 3.61103\n",
      "epoch no.1 train no.79930  loss = 1.76112 avg_loss = 3.59431\n",
      "epoch no.1 train no.79940  loss = 3.17021 avg_loss = 3.57673\n",
      "epoch no.1 train no.79950  loss = 2.93933 avg_loss = 3.57814\n",
      "epoch no.1 train no.79960  loss = 3.26602 avg_loss = 3.53371\n",
      "epoch no.1 train no.79970  loss = 4.75872 avg_loss = 3.55712\n",
      "epoch no.1 train no.79980  loss = 2.64545 avg_loss = 3.54107\n",
      "epoch no.1 train no.79990  loss = 2.74578 avg_loss = 3.52454\n",
      "epoch no.1 train no.80000  loss = 4.66289 avg_loss = 3.47250\n",
      "9\n",
      "to_tokens: ['▁비', '밤', '에', '▁위한', '놓을', '▁감성', '한', '▁노래', '에이지', '</s>']\n",
      "여름밤을 수놓는  잔잔한 뉴에이지</s>\n",
      "epoch no.1 train no.80010  loss = 3.69083 avg_loss = 3.47610\n",
      "epoch no.1 train no.80020  loss = 5.05744 avg_loss = 3.53231\n",
      "epoch no.1 train no.80030  loss = 2.83816 avg_loss = 3.53412\n",
      "epoch no.1 train no.80040  loss = 3.70210 avg_loss = 3.57012\n",
      "epoch no.1 train no.80050  loss = 2.57123 avg_loss = 3.55654\n",
      "epoch no.1 train no.80060  loss = 5.20354 avg_loss = 3.59262\n",
      "epoch no.1 train no.80070  loss = 4.29905 avg_loss = 3.56808\n",
      "epoch no.1 train no.80080  loss = 5.30230 avg_loss = 3.55876\n",
      "epoch no.1 train no.80090  loss = 3.36316 avg_loss = 3.58581\n",
      "epoch no.1 train no.80100  loss = 4.52596 avg_loss = 3.58514\n",
      "epoch no.1 train no.80110  loss = 4.62384 avg_loss = 3.66082\n",
      "epoch no.1 train no.80120  loss = 2.50805 avg_loss = 3.64099\n",
      "epoch no.1 train no.80130  loss = 3.95065 avg_loss = 3.68485\n",
      "epoch no.1 train no.80140  loss = 4.18477 avg_loss = 3.68025\n",
      "epoch no.1 train no.80150  loss = 3.87176 avg_loss = 3.73733\n",
      "epoch no.1 train no.80160  loss = 4.66501 avg_loss = 3.70492\n",
      "epoch no.1 train no.80170  loss = 3.08291 avg_loss = 3.70904\n",
      "epoch no.1 train no.80180  loss = 4.59326 avg_loss = 3.66886\n",
      "epoch no.1 train no.80190  loss = 2.26959 avg_loss = 3.64242\n",
      "epoch no.1 train no.80200  loss = 3.19055 avg_loss = 3.64127\n",
      "epoch no.1 train no.80210  loss = 4.12233 avg_loss = 3.63904\n",
      "epoch no.1 train no.80220  loss = 5.61350 avg_loss = 3.65752\n",
      "epoch no.1 train no.80230  loss = 2.68497 avg_loss = 3.66134\n",
      "epoch no.1 train no.80240  loss = 3.90522 avg_loss = 3.68338\n",
      "epoch no.1 train no.80250  loss = 3.94159 avg_loss = 3.69160\n",
      "epoch no.1 train no.80260  loss = 3.41229 avg_loss = 3.64513\n",
      "epoch no.1 train no.80270  loss = 3.26975 avg_loss = 3.66287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.80280  loss = 3.68605 avg_loss = 3.69065\n",
      "epoch no.1 train no.80290  loss = 3.34868 avg_loss = 3.61521\n",
      "epoch no.1 train no.80300  loss = 5.49554 avg_loss = 3.60796\n",
      "epoch no.1 train no.80310  loss = 5.58887 avg_loss = 3.64708\n",
      "epoch no.1 train no.80320  loss = 2.40561 avg_loss = 3.71897\n",
      "epoch no.1 train no.80330  loss = 4.28067 avg_loss = 3.69858\n",
      "epoch no.1 train no.80340  loss = 3.93699 avg_loss = 3.65080\n",
      "epoch no.1 train no.80350  loss = 4.27243 avg_loss = 3.65733\n",
      "epoch no.1 train no.80360  loss = 2.73579 avg_loss = 3.64816\n",
      "epoch no.1 train no.80370  loss = 3.44705 avg_loss = 3.63654\n",
      "epoch no.1 train no.80380  loss = 5.20809 avg_loss = 3.58913\n",
      "epoch no.1 train no.80390  loss = 2.85066 avg_loss = 3.62033\n",
      "epoch no.1 train no.80400  loss = 3.76841 avg_loss = 3.64198\n",
      "epoch no.1 train no.80410  loss = 3.00596 avg_loss = 3.61032\n",
      "epoch no.1 train no.80420  loss = 2.34808 avg_loss = 3.57854\n",
      "epoch no.1 train no.80430  loss = 3.22603 avg_loss = 3.61281\n",
      "epoch no.1 train no.80440  loss = 3.22127 avg_loss = 3.59207\n",
      "epoch no.1 train no.80450  loss = 3.63207 avg_loss = 3.57455\n",
      "epoch no.1 train no.80460  loss = 2.18710 avg_loss = 3.57246\n",
      "epoch no.1 train no.80470  loss = 2.71171 avg_loss = 3.62591\n",
      "epoch no.1 train no.80480  loss = 3.86551 avg_loss = 3.62528\n",
      "epoch no.1 train no.80490  loss = 3.99669 avg_loss = 3.61348\n",
      "epoch no.1 train no.80500  loss = 3.70182 avg_loss = 3.59071\n",
      "epoch no.1 train no.80510  loss = 5.75121 avg_loss = 3.59552\n",
      "epoch no.1 train no.80520  loss = 4.65090 avg_loss = 3.61388\n",
      "epoch no.1 train no.80530  loss = 4.22088 avg_loss = 3.68095\n",
      "epoch no.1 train no.80540  loss = 4.49660 avg_loss = 3.65730\n",
      "epoch no.1 train no.80550  loss = 3.06315 avg_loss = 3.66851\n",
      "epoch no.1 train no.80560  loss = 4.62619 avg_loss = 3.66860\n",
      "epoch no.1 train no.80570  loss = 2.94433 avg_loss = 3.62787\n",
      "epoch no.1 train no.80580  loss = 2.93472 avg_loss = 3.62264\n",
      "epoch no.1 train no.80590  loss = 4.01761 avg_loss = 3.61931\n",
      "epoch no.1 train no.80600  loss = 5.26541 avg_loss = 3.61002\n",
      "epoch no.1 train no.80610  loss = 3.79329 avg_loss = 3.63551\n",
      "epoch no.1 train no.80620  loss = 4.97030 avg_loss = 3.68981\n",
      "epoch no.1 train no.80630  loss = 4.34481 avg_loss = 3.64751\n",
      "epoch no.1 train no.80640  loss = 3.69461 avg_loss = 3.60632\n",
      "epoch no.1 train no.80650  loss = 5.52884 avg_loss = 3.66104\n",
      "epoch no.1 train no.80660  loss = 4.25808 avg_loss = 3.68985\n",
      "epoch no.1 train no.80670  loss = 2.16719 avg_loss = 3.61066\n",
      "epoch no.1 train no.80680  loss = 3.20329 avg_loss = 3.62035\n",
      "epoch no.1 train no.80690  loss = 5.04776 avg_loss = 3.61114\n",
      "epoch no.1 train no.80700  loss = 2.86024 avg_loss = 3.59306\n",
      "epoch no.1 train no.80710  loss = 5.36456 avg_loss = 3.62252\n",
      "epoch no.1 train no.80720  loss = 4.19784 avg_loss = 3.55246\n",
      "epoch no.1 train no.80730  loss = 5.66881 avg_loss = 3.54820\n",
      "epoch no.1 train no.80740  loss = 4.52589 avg_loss = 3.60639\n",
      "epoch no.1 train no.80750  loss = 2.49120 avg_loss = 3.59174\n",
      "epoch no.1 train no.80760  loss = 3.79713 avg_loss = 3.60816\n",
      "epoch no.1 train no.80770  loss = 3.99659 avg_loss = 3.61266\n",
      "epoch no.1 train no.80780  loss = 2.53984 avg_loss = 3.57818\n",
      "epoch no.1 train no.80790  loss = 2.83642 avg_loss = 3.53414\n",
      "epoch no.1 train no.80800  loss = 2.72725 avg_loss = 3.50288\n",
      "epoch no.1 train no.80810  loss = 4.20752 avg_loss = 3.55955\n",
      "epoch no.1 train no.80820  loss = 4.28308 avg_loss = 3.55905\n",
      "epoch no.1 train no.80830  loss = 3.75148 avg_loss = 3.52603\n",
      "epoch no.1 train no.80840  loss = 3.67789 avg_loss = 3.52636\n",
      "epoch no.1 train no.80850  loss = 3.52841 avg_loss = 3.54491\n",
      "epoch no.1 train no.80860  loss = 3.77885 avg_loss = 3.49852\n",
      "epoch no.1 train no.80870  loss = 3.49402 avg_loss = 3.48487\n",
      "epoch no.1 train no.80880  loss = 2.27836 avg_loss = 3.50704\n",
      "epoch no.1 train no.80890  loss = 5.93360 avg_loss = 3.51940\n",
      "epoch no.1 train no.80900  loss = 2.64175 avg_loss = 3.54536\n",
      "epoch no.1 train no.80910  loss = 2.47838 avg_loss = 3.50323\n",
      "epoch no.1 train no.80920  loss = 3.60829 avg_loss = 3.52161\n",
      "epoch no.1 train no.80930  loss = 3.47112 avg_loss = 3.51808\n",
      "epoch no.1 train no.80940  loss = 3.39933 avg_loss = 3.53883\n",
      "epoch no.1 train no.80950  loss = 4.13217 avg_loss = 3.51705\n",
      "epoch no.1 train no.80960  loss = 2.86365 avg_loss = 3.54631\n",
      "epoch no.1 train no.80970  loss = 1.84336 avg_loss = 3.52621\n",
      "epoch no.1 train no.80980  loss = 6.02716 avg_loss = 3.58010\n",
      "epoch no.1 train no.80990  loss = 5.53320 avg_loss = 3.59247\n",
      "epoch no.1 train no.81000  loss = 3.43096 avg_loss = 3.59178\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁어울리는', '▁감성', '적인', '</s>']\n",
      "여름밤에 듣는 감성 힙합</s>\n",
      "epoch no.1 train no.81010  loss = 3.89464 avg_loss = 3.60654\n",
      "epoch no.1 train no.81020  loss = 4.02886 avg_loss = 3.68135\n",
      "epoch no.1 train no.81030  loss = 2.73698 avg_loss = 3.67552\n",
      "epoch no.1 train no.81040  loss = 2.74899 avg_loss = 3.59834\n",
      "epoch no.1 train no.81050  loss = 3.82638 avg_loss = 3.59749\n",
      "epoch no.1 train no.81060  loss = 3.09681 avg_loss = 3.63798\n",
      "epoch no.1 train no.81070  loss = 2.89624 avg_loss = 3.66989\n",
      "epoch no.1 train no.81080  loss = 3.88480 avg_loss = 3.67877\n",
      "epoch no.1 train no.81090  loss = 3.39461 avg_loss = 3.68286\n",
      "epoch no.1 train no.81100  loss = 3.39156 avg_loss = 3.66234\n",
      "epoch no.1 train no.81110  loss = 2.74631 avg_loss = 3.66003\n",
      "epoch no.1 train no.81120  loss = 3.38243 avg_loss = 3.60591\n",
      "epoch no.1 train no.81130  loss = 3.46932 avg_loss = 3.62777\n",
      "epoch no.1 train no.81140  loss = 4.23451 avg_loss = 3.63866\n",
      "epoch no.1 train no.81150  loss = 4.18363 avg_loss = 3.60985\n",
      "epoch no.1 train no.81160  loss = 2.80655 avg_loss = 3.61503\n",
      "epoch no.1 train no.81170  loss = 4.46849 avg_loss = 3.61133\n",
      "epoch no.1 train no.81180  loss = 5.54543 avg_loss = 3.62506\n",
      "epoch no.1 train no.81190  loss = 3.27521 avg_loss = 3.60275\n",
      "epoch no.1 train no.81200  loss = 2.43852 avg_loss = 3.61004\n",
      "epoch no.1 train no.81210  loss = 3.71316 avg_loss = 3.65175\n",
      "epoch no.1 train no.81220  loss = 3.30919 avg_loss = 3.63920\n",
      "epoch no.1 train no.81230  loss = 2.65079 avg_loss = 3.63277\n",
      "epoch no.1 train no.81240  loss = 3.67761 avg_loss = 3.67311\n",
      "epoch no.1 train no.81250  loss = 2.80265 avg_loss = 3.68091\n",
      "epoch no.1 train no.81260  loss = 3.50681 avg_loss = 3.68604\n",
      "epoch no.1 train no.81270  loss = 4.72079 avg_loss = 3.68848\n",
      "epoch no.1 train no.81280  loss = 5.40027 avg_loss = 3.68654\n",
      "epoch no.1 train no.81290  loss = 2.27043 avg_loss = 3.65648\n",
      "epoch no.1 train no.81300  loss = 3.55865 avg_loss = 3.65187\n",
      "epoch no.1 train no.81310  loss = 3.56567 avg_loss = 3.65674\n",
      "epoch no.1 train no.81320  loss = 2.87727 avg_loss = 3.63805\n",
      "epoch no.1 train no.81330  loss = 3.31942 avg_loss = 3.59757\n",
      "epoch no.1 train no.81340  loss = 4.04386 avg_loss = 3.60343\n",
      "epoch no.1 train no.81350  loss = 3.83336 avg_loss = 3.60005\n",
      "epoch no.1 train no.81360  loss = 3.90999 avg_loss = 3.62599\n",
      "epoch no.1 train no.81370  loss = 4.41826 avg_loss = 3.62069\n",
      "epoch no.1 train no.81380  loss = 4.18350 avg_loss = 3.62295\n",
      "epoch no.1 train no.81390  loss = 2.90911 avg_loss = 3.65718\n",
      "epoch no.1 train no.81400  loss = 4.22797 avg_loss = 3.68174\n",
      "epoch no.1 train no.81410  loss = 3.47452 avg_loss = 3.67410\n",
      "epoch no.1 train no.81420  loss = 4.50125 avg_loss = 3.66209\n",
      "epoch no.1 train no.81430  loss = 4.76188 avg_loss = 3.66624\n",
      "epoch no.1 train no.81440  loss = 4.21036 avg_loss = 3.69840\n",
      "epoch no.1 train no.81450  loss = 3.80158 avg_loss = 3.68135\n",
      "epoch no.1 train no.81460  loss = 4.26727 avg_loss = 3.67036\n",
      "epoch no.1 train no.81470  loss = 5.34911 avg_loss = 3.63759\n",
      "epoch no.1 train no.81480  loss = 4.63152 avg_loss = 3.66217\n",
      "epoch no.1 train no.81490  loss = 2.45390 avg_loss = 3.64986\n",
      "epoch no.1 train no.81500  loss = 3.33314 avg_loss = 3.70276\n",
      "epoch no.1 train no.81510  loss = 3.77033 avg_loss = 3.74004\n",
      "epoch no.1 train no.81520  loss = 3.42272 avg_loss = 3.76280\n",
      "epoch no.1 train no.81530  loss = 5.16717 avg_loss = 3.77678\n",
      "epoch no.1 train no.81540  loss = 3.29325 avg_loss = 3.76881\n",
      "epoch no.1 train no.81550  loss = 3.19866 avg_loss = 3.69583\n",
      "epoch no.1 train no.81560  loss = 2.70096 avg_loss = 3.71612\n",
      "epoch no.1 train no.81570  loss = 2.78385 avg_loss = 3.69030\n",
      "epoch no.1 train no.81580  loss = 4.68026 avg_loss = 3.68325\n",
      "epoch no.1 train no.81590  loss = 4.66151 avg_loss = 3.65070\n",
      "epoch no.1 train no.81600  loss = 3.45395 avg_loss = 3.63765\n",
      "epoch no.1 train no.81610  loss = 4.03072 avg_loss = 3.60205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.81620  loss = 3.71901 avg_loss = 3.53586\n",
      "epoch no.1 train no.81630  loss = 3.67478 avg_loss = 3.55364\n",
      "epoch no.1 train no.81640  loss = 5.37478 avg_loss = 3.60270\n",
      "epoch no.1 train no.81650  loss = 4.77470 avg_loss = 3.62691\n",
      "epoch no.1 train no.81660  loss = 2.48356 avg_loss = 3.59045\n",
      "epoch no.1 train no.81670  loss = 3.86767 avg_loss = 3.56699\n",
      "epoch no.1 train no.81680  loss = 2.21965 avg_loss = 3.57957\n",
      "epoch no.1 train no.81690  loss = 2.86380 avg_loss = 3.57121\n",
      "epoch no.1 train no.81700  loss = 4.88303 avg_loss = 3.60367\n",
      "epoch no.1 train no.81710  loss = 2.80760 avg_loss = 3.56368\n",
      "epoch no.1 train no.81720  loss = 2.62881 avg_loss = 3.59617\n",
      "epoch no.1 train no.81730  loss = 1.77637 avg_loss = 3.57733\n",
      "epoch no.1 train no.81740  loss = 4.45889 avg_loss = 3.56991\n",
      "epoch no.1 train no.81750  loss = 2.18807 avg_loss = 3.53596\n",
      "epoch no.1 train no.81760  loss = 3.68365 avg_loss = 3.50261\n",
      "epoch no.1 train no.81770  loss = 5.07208 avg_loss = 3.53660\n",
      "epoch no.1 train no.81780  loss = 3.80166 avg_loss = 3.61975\n",
      "epoch no.1 train no.81790  loss = 3.93408 avg_loss = 3.61925\n",
      "epoch no.1 train no.81800  loss = 3.68839 avg_loss = 3.70351\n",
      "epoch no.1 train no.81810  loss = 2.78885 avg_loss = 3.74743\n",
      "epoch no.1 train no.81820  loss = 2.63136 avg_loss = 3.72730\n",
      "epoch no.1 train no.81830  loss = 3.14776 avg_loss = 3.73987\n",
      "epoch no.1 train no.81840  loss = 3.26834 avg_loss = 3.72601\n",
      "epoch no.1 train no.81850  loss = 2.74243 avg_loss = 3.73284\n",
      "epoch no.1 train no.81860  loss = 3.31496 avg_loss = 3.71385\n",
      "epoch no.1 train no.81870  loss = 6.03658 avg_loss = 3.71931\n",
      "epoch no.1 train no.81880  loss = 3.18880 avg_loss = 3.70949\n",
      "epoch no.1 train no.81890  loss = 4.45464 avg_loss = 3.72755\n",
      "epoch no.1 train no.81900  loss = 3.92160 avg_loss = 3.72472\n",
      "epoch no.1 train no.81910  loss = 2.39298 avg_loss = 3.73167\n",
      "epoch no.1 train no.81920  loss = 2.81840 avg_loss = 3.74947\n",
      "epoch no.1 train no.81930  loss = 4.18786 avg_loss = 3.75449\n",
      "epoch no.1 train no.81940  loss = 3.13242 avg_loss = 3.74698\n",
      "epoch no.1 train no.81950  loss = 3.35643 avg_loss = 3.79890\n",
      "epoch no.1 train no.81960  loss = 2.00371 avg_loss = 3.74045\n",
      "epoch no.1 train no.81970  loss = 3.26693 avg_loss = 3.73523\n",
      "epoch no.1 train no.81980  loss = 3.86492 avg_loss = 3.71514\n",
      "epoch no.1 train no.81990  loss = 3.00625 avg_loss = 3.69203\n",
      "epoch no.1 train no.82000  loss = 2.23782 avg_loss = 3.62731\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁드라이브', '▁좋은', '▁노래', '▁음악', '</s>']\n",
      "여름 밤 듣기 좋은 인디 음악</s>\n",
      "epoch no.1 train no.82010  loss = 5.20953 avg_loss = 3.65864\n",
      "epoch no.1 train no.82020  loss = 5.37338 avg_loss = 3.65081\n",
      "epoch no.1 train no.82030  loss = 3.63038 avg_loss = 3.63515\n",
      "epoch no.1 train no.82040  loss = 3.01393 avg_loss = 3.74375\n",
      "epoch no.1 train no.82050  loss = 4.02371 avg_loss = 3.73204\n",
      "epoch no.1 train no.82060  loss = 2.64724 avg_loss = 3.72709\n",
      "epoch no.1 train no.82070  loss = 1.82039 avg_loss = 3.70053\n",
      "epoch no.1 train no.82080  loss = 3.93387 avg_loss = 3.71297\n",
      "epoch no.1 train no.82090  loss = 4.65687 avg_loss = 3.68105\n",
      "epoch no.1 train no.82100  loss = 2.80868 avg_loss = 3.68062\n",
      "epoch no.1 train no.82110  loss = 3.94949 avg_loss = 3.71415\n",
      "epoch no.1 train no.82120  loss = 2.90926 avg_loss = 3.65558\n",
      "epoch no.1 train no.82130  loss = 2.53972 avg_loss = 3.66999\n",
      "epoch no.1 train no.82140  loss = 1.31969 avg_loss = 3.56163\n",
      "epoch no.1 train no.82150  loss = 3.01535 avg_loss = 3.62581\n",
      "epoch no.1 train no.82160  loss = 4.12045 avg_loss = 3.61781\n",
      "epoch no.1 train no.82170  loss = 5.14820 avg_loss = 3.67799\n",
      "epoch no.1 train no.82180  loss = 4.41325 avg_loss = 3.70898\n",
      "epoch no.1 train no.82190  loss = 4.83081 avg_loss = 3.74566\n",
      "epoch no.1 train no.82200  loss = 5.38229 avg_loss = 3.83843\n",
      "epoch no.1 train no.82210  loss = 6.05732 avg_loss = 3.88118\n",
      "epoch no.1 train no.82220  loss = 2.03009 avg_loss = 3.79390\n",
      "epoch no.1 train no.82230  loss = 5.62749 avg_loss = 3.77979\n",
      "epoch no.1 train no.82240  loss = 2.34786 avg_loss = 3.76151\n",
      "epoch no.1 train no.82250  loss = 3.42201 avg_loss = 3.77982\n",
      "epoch no.1 train no.82260  loss = 3.62053 avg_loss = 3.76447\n",
      "epoch no.1 train no.82270  loss = 3.89655 avg_loss = 3.74548\n",
      "epoch no.1 train no.82280  loss = 4.97607 avg_loss = 3.73228\n",
      "epoch no.1 train no.82290  loss = 4.14625 avg_loss = 3.73802\n",
      "epoch no.1 train no.82300  loss = 3.82727 avg_loss = 3.71625\n",
      "epoch no.1 train no.82310  loss = 2.42463 avg_loss = 3.74008\n",
      "epoch no.1 train no.82320  loss = 3.82449 avg_loss = 3.72647\n",
      "epoch no.1 train no.82330  loss = 2.70966 avg_loss = 3.68515\n",
      "epoch no.1 train no.82340  loss = 3.16048 avg_loss = 3.69979\n",
      "epoch no.1 train no.82350  loss = 3.32658 avg_loss = 3.68575\n",
      "epoch no.1 train no.82360  loss = 3.16580 avg_loss = 3.67291\n",
      "epoch no.1 train no.82370  loss = 4.65006 avg_loss = 3.63342\n",
      "epoch no.1 train no.82380  loss = 3.25435 avg_loss = 3.61542\n",
      "epoch no.1 train no.82390  loss = 2.54301 avg_loss = 3.62078\n",
      "epoch no.1 train no.82400  loss = 3.34951 avg_loss = 3.61713\n",
      "epoch no.1 train no.82410  loss = 2.37422 avg_loss = 3.63561\n",
      "epoch no.1 train no.82420  loss = 4.39433 avg_loss = 3.61695\n",
      "epoch no.1 train no.82430  loss = 3.80726 avg_loss = 3.64702\n",
      "epoch no.1 train no.82440  loss = 3.07760 avg_loss = 3.64708\n",
      "epoch no.1 train no.82450  loss = 2.35844 avg_loss = 3.61855\n",
      "epoch no.1 train no.82460  loss = 3.48582 avg_loss = 3.56911\n",
      "epoch no.1 train no.82470  loss = 3.08176 avg_loss = 3.58715\n",
      "epoch no.1 train no.82480  loss = 4.59798 avg_loss = 3.54552\n",
      "epoch no.1 train no.82490  loss = 2.98498 avg_loss = 3.56795\n",
      "epoch no.1 train no.82500  loss = 2.38696 avg_loss = 3.53506\n",
      "epoch no.1 train no.82510  loss = 4.10118 avg_loss = 3.51724\n",
      "epoch no.1 train no.82520  loss = 3.47368 avg_loss = 3.57099\n",
      "epoch no.1 train no.82530  loss = 3.27463 avg_loss = 3.59699\n",
      "epoch no.1 train no.82540  loss = 4.90027 avg_loss = 3.55872\n",
      "epoch no.1 train no.82550  loss = 2.62366 avg_loss = 3.56202\n",
      "epoch no.1 train no.82560  loss = 7.72683 avg_loss = 3.60579\n",
      "epoch no.1 train no.82570  loss = 2.56425 avg_loss = 3.57021\n",
      "epoch no.1 train no.82580  loss = 3.77683 avg_loss = 3.56711\n",
      "epoch no.1 train no.82590  loss = 4.33762 avg_loss = 3.58119\n",
      "epoch no.1 train no.82600  loss = 3.56989 avg_loss = 3.59250\n",
      "epoch no.1 train no.82610  loss = 1.80053 avg_loss = 3.56338\n",
      "epoch no.1 train no.82620  loss = 5.56663 avg_loss = 3.61103\n",
      "epoch no.1 train no.82630  loss = 4.34284 avg_loss = 3.61659\n",
      "epoch no.1 train no.82640  loss = 3.48872 avg_loss = 3.65522\n",
      "epoch no.1 train no.82650  loss = 4.85059 avg_loss = 3.65319\n",
      "epoch no.1 train no.82660  loss = 4.70104 avg_loss = 3.65188\n",
      "epoch no.1 train no.82670  loss = 2.48125 avg_loss = 3.63082\n",
      "epoch no.1 train no.82680  loss = 3.06728 avg_loss = 3.66742\n",
      "epoch no.1 train no.82690  loss = 3.09682 avg_loss = 3.66264\n",
      "epoch no.1 train no.82700  loss = 4.08975 avg_loss = 3.62860\n",
      "epoch no.1 train no.82710  loss = 4.55670 avg_loss = 3.61964\n",
      "epoch no.1 train no.82720  loss = 4.78842 avg_loss = 3.69224\n",
      "epoch no.1 train no.82730  loss = 2.86056 avg_loss = 3.79276\n",
      "epoch no.1 train no.82740  loss = 2.54508 avg_loss = 3.75354\n",
      "epoch no.1 train no.82750  loss = 4.03573 avg_loss = 3.76664\n",
      "epoch no.1 train no.82760  loss = 4.44347 avg_loss = 3.77046\n",
      "epoch no.1 train no.82770  loss = 4.51444 avg_loss = 3.75804\n",
      "epoch no.1 train no.82780  loss = 3.96853 avg_loss = 3.78340\n",
      "epoch no.1 train no.82790  loss = 3.59791 avg_loss = 3.79070\n",
      "epoch no.1 train no.82800  loss = 1.83320 avg_loss = 3.73361\n",
      "epoch no.1 train no.82810  loss = 4.74584 avg_loss = 3.72661\n",
      "epoch no.1 train no.82820  loss = 5.97041 avg_loss = 3.76097\n",
      "epoch no.1 train no.82830  loss = 3.55875 avg_loss = 3.72426\n",
      "epoch no.1 train no.82840  loss = 5.18142 avg_loss = 3.73857\n",
      "epoch no.1 train no.82850  loss = 4.37347 avg_loss = 3.75010\n",
      "epoch no.1 train no.82860  loss = 3.22018 avg_loss = 3.76341\n",
      "epoch no.1 train no.82870  loss = 3.46690 avg_loss = 3.74141\n",
      "epoch no.1 train no.82880  loss = 4.14991 avg_loss = 3.72734\n",
      "epoch no.1 train no.82890  loss = 2.38181 avg_loss = 3.70412\n",
      "epoch no.1 train no.82900  loss = 4.35688 avg_loss = 3.72342\n",
      "epoch no.1 train no.82910  loss = 4.72042 avg_loss = 3.71992\n",
      "epoch no.1 train no.82920  loss = 4.61460 avg_loss = 3.73589\n",
      "epoch no.1 train no.82930  loss = 5.23054 avg_loss = 3.78158\n",
      "epoch no.1 train no.82940  loss = 3.27857 avg_loss = 3.72114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.82950  loss = 3.29863 avg_loss = 3.76936\n",
      "epoch no.1 train no.82960  loss = 3.10960 avg_loss = 3.70481\n",
      "epoch no.1 train no.82970  loss = 2.75468 avg_loss = 3.68878\n",
      "epoch no.1 train no.82980  loss = 4.47689 avg_loss = 3.69519\n",
      "epoch no.1 train no.82990  loss = 4.54451 avg_loss = 3.69654\n",
      "epoch no.1 train no.83000  loss = 4.35513 avg_loss = 3.75036\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁신나는', '피', '컬', '▁하우스', '</s>']\n",
      "여름엔 트로피컬 하우스</s>\n",
      "epoch no.1 train no.83010  loss = 4.19208 avg_loss = 3.69013\n",
      "epoch no.1 train no.83020  loss = 3.36064 avg_loss = 3.64539\n",
      "epoch no.1 train no.83030  loss = 2.24726 avg_loss = 3.62442\n",
      "epoch no.1 train no.83040  loss = 4.11108 avg_loss = 3.63868\n",
      "epoch no.1 train no.83050  loss = 3.40046 avg_loss = 3.59006\n",
      "epoch no.1 train no.83060  loss = 5.74722 avg_loss = 3.62970\n",
      "epoch no.1 train no.83070  loss = 6.29730 avg_loss = 3.63913\n",
      "epoch no.1 train no.83080  loss = 5.10281 avg_loss = 3.67857\n",
      "epoch no.1 train no.83090  loss = 4.39784 avg_loss = 3.72333\n",
      "epoch no.1 train no.83100  loss = 3.72677 avg_loss = 3.74867\n",
      "epoch no.1 train no.83110  loss = 2.22469 avg_loss = 3.66333\n",
      "epoch no.1 train no.83120  loss = 3.18531 avg_loss = 3.61116\n",
      "epoch no.1 train no.83130  loss = 2.85255 avg_loss = 3.56047\n",
      "epoch no.1 train no.83140  loss = 3.88254 avg_loss = 3.57510\n",
      "epoch no.1 train no.83150  loss = 4.48546 avg_loss = 3.59327\n",
      "epoch no.1 train no.83160  loss = 3.06806 avg_loss = 3.60146\n",
      "epoch no.1 train no.83170  loss = 3.56510 avg_loss = 3.60188\n",
      "epoch no.1 train no.83180  loss = 3.32960 avg_loss = 3.65355\n",
      "epoch no.1 train no.83190  loss = 4.08438 avg_loss = 3.67421\n",
      "epoch no.1 train no.83200  loss = 3.56589 avg_loss = 3.70166\n",
      "epoch no.1 train no.83210  loss = 5.15421 avg_loss = 3.66213\n",
      "epoch no.1 train no.83220  loss = 3.82130 avg_loss = 3.62768\n",
      "epoch no.1 train no.83230  loss = 5.41601 avg_loss = 3.63651\n",
      "epoch no.1 train no.83240  loss = 2.75360 avg_loss = 3.57512\n",
      "epoch no.1 train no.83250  loss = 4.48970 avg_loss = 3.62271\n",
      "epoch no.1 train no.83260  loss = 2.48797 avg_loss = 3.63296\n",
      "epoch no.1 train no.83270  loss = 3.25429 avg_loss = 3.55707\n",
      "epoch no.1 train no.83280  loss = 4.96031 avg_loss = 3.58799\n",
      "epoch no.1 train no.83290  loss = 3.57979 avg_loss = 3.60070\n",
      "epoch no.1 train no.83300  loss = 3.16104 avg_loss = 3.57648\n",
      "epoch no.1 train no.83310  loss = 2.91042 avg_loss = 3.61407\n",
      "epoch no.1 train no.83320  loss = 4.88600 avg_loss = 3.66143\n",
      "epoch no.1 train no.83330  loss = 4.87154 avg_loss = 3.67758\n",
      "epoch no.1 train no.83340  loss = 2.70268 avg_loss = 3.63258\n",
      "epoch no.1 train no.83350  loss = 3.78242 avg_loss = 3.66463\n",
      "epoch no.1 train no.83360  loss = 2.61366 avg_loss = 3.64893\n",
      "epoch no.1 train no.83370  loss = 2.06334 avg_loss = 3.66830\n",
      "epoch no.1 train no.83380  loss = 3.63803 avg_loss = 3.63050\n",
      "epoch no.1 train no.83390  loss = 4.50180 avg_loss = 3.65792\n",
      "epoch no.1 train no.83400  loss = 3.76335 avg_loss = 3.62418\n",
      "epoch no.1 train no.83410  loss = 4.61647 avg_loss = 3.62585\n",
      "epoch no.1 train no.83420  loss = 5.55423 avg_loss = 3.59691\n",
      "epoch no.1 train no.83430  loss = 3.34236 avg_loss = 3.62243\n",
      "epoch no.1 train no.83440  loss = 5.74587 avg_loss = 3.68528\n",
      "epoch no.1 train no.83450  loss = 4.60776 avg_loss = 3.73712\n",
      "epoch no.1 train no.83460  loss = 4.51755 avg_loss = 3.74214\n",
      "epoch no.1 train no.83470  loss = 4.17461 avg_loss = 3.73088\n",
      "epoch no.1 train no.83480  loss = 2.66333 avg_loss = 3.72721\n",
      "epoch no.1 train no.83490  loss = 5.06794 avg_loss = 3.72327\n",
      "epoch no.1 train no.83500  loss = 3.33334 avg_loss = 3.73360\n",
      "epoch no.1 train no.83510  loss = 5.43954 avg_loss = 3.67058\n",
      "epoch no.1 train no.83520  loss = 3.10187 avg_loss = 3.69075\n",
      "epoch no.1 train no.83530  loss = 2.24241 avg_loss = 3.62491\n",
      "epoch no.1 train no.83540  loss = 5.37975 avg_loss = 3.65977\n",
      "epoch no.1 train no.83550  loss = 3.88795 avg_loss = 3.63215\n",
      "epoch no.1 train no.83560  loss = 4.80970 avg_loss = 3.59771\n",
      "epoch no.1 train no.83570  loss = 4.84404 avg_loss = 3.65289\n",
      "epoch no.1 train no.83580  loss = 3.59326 avg_loss = 3.63158\n",
      "epoch no.1 train no.83590  loss = 4.53251 avg_loss = 3.61400\n",
      "epoch no.1 train no.83600  loss = 5.09382 avg_loss = 3.69459\n",
      "epoch no.1 train no.83610  loss = 4.45157 avg_loss = 3.70879\n",
      "epoch no.1 train no.83620  loss = 2.58729 avg_loss = 3.71899\n",
      "epoch no.1 train no.83630  loss = 3.66572 avg_loss = 3.74052\n",
      "epoch no.1 train no.83640  loss = 3.13707 avg_loss = 3.69511\n",
      "epoch no.1 train no.83650  loss = 5.23792 avg_loss = 3.65578\n",
      "epoch no.1 train no.83660  loss = 2.73117 avg_loss = 3.64707\n",
      "epoch no.1 train no.83670  loss = 5.05547 avg_loss = 3.67053\n",
      "epoch no.1 train no.83680  loss = 6.19679 avg_loss = 3.70916\n",
      "epoch no.1 train no.83690  loss = 2.53827 avg_loss = 3.65062\n",
      "epoch no.1 train no.83700  loss = 4.97207 avg_loss = 3.66426\n",
      "epoch no.1 train no.83710  loss = 6.77225 avg_loss = 3.74506\n",
      "epoch no.1 train no.83720  loss = 4.03231 avg_loss = 3.75691\n",
      "epoch no.1 train no.83730  loss = 3.19762 avg_loss = 3.74014\n",
      "epoch no.1 train no.83740  loss = 4.01080 avg_loss = 3.78062\n",
      "epoch no.1 train no.83750  loss = 3.81164 avg_loss = 3.77371\n",
      "epoch no.1 train no.83760  loss = 5.18017 avg_loss = 3.79987\n",
      "epoch no.1 train no.83770  loss = 3.37178 avg_loss = 3.77686\n",
      "epoch no.1 train no.83780  loss = 2.90993 avg_loss = 3.74345\n",
      "epoch no.1 train no.83790  loss = 2.97144 avg_loss = 3.69247\n",
      "epoch no.1 train no.83800  loss = 3.80572 avg_loss = 3.67087\n",
      "epoch no.1 train no.83810  loss = 4.19647 avg_loss = 3.69083\n",
      "epoch no.1 train no.83820  loss = 5.91383 avg_loss = 3.66123\n",
      "epoch no.1 train no.83830  loss = 3.49919 avg_loss = 3.63562\n",
      "epoch no.1 train no.83840  loss = 6.93878 avg_loss = 3.63602\n",
      "epoch no.1 train no.83850  loss = 6.37799 avg_loss = 3.64794\n",
      "epoch no.1 train no.83860  loss = 3.50270 avg_loss = 3.60984\n",
      "epoch no.1 train no.83870  loss = 1.69939 avg_loss = 3.57591\n",
      "epoch no.1 train no.83880  loss = 3.60950 avg_loss = 3.56158\n",
      "epoch no.1 train no.83890  loss = 3.42590 avg_loss = 3.58429\n",
      "epoch no.1 train no.83900  loss = 2.31745 avg_loss = 3.58917\n",
      "epoch no.1 train no.83910  loss = 2.27961 avg_loss = 3.53386\n",
      "epoch no.1 train no.83920  loss = 4.42740 avg_loss = 3.56765\n",
      "epoch no.1 train no.83930  loss = 3.12349 avg_loss = 3.53100\n",
      "epoch no.1 train no.83940  loss = 5.48267 avg_loss = 3.58937\n",
      "epoch no.1 train no.83950  loss = 3.85403 avg_loss = 3.64385\n",
      "epoch no.1 train no.83960  loss = 2.95637 avg_loss = 3.64424\n",
      "epoch no.1 train no.83970  loss = 3.75542 avg_loss = 3.66039\n",
      "epoch no.1 train no.83980  loss = 2.42334 avg_loss = 3.61721\n",
      "epoch no.1 train no.83990  loss = 2.67581 avg_loss = 3.59398\n",
      "epoch no.1 train no.84000  loss = 2.56050 avg_loss = 3.56531\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '니까', '▁생각', '▁싶은', '▁노래', '</s>']\n",
      "여름이 되면 듣고 싶은 노래</s>\n",
      "epoch no.1 train no.84010  loss = 4.56826 avg_loss = 3.55359\n",
      "epoch no.1 train no.84020  loss = 4.03375 avg_loss = 3.56133\n",
      "epoch no.1 train no.84030  loss = 3.75733 avg_loss = 3.50942\n",
      "epoch no.1 train no.84040  loss = 3.99603 avg_loss = 3.51962\n",
      "epoch no.1 train no.84050  loss = 4.03161 avg_loss = 3.55352\n",
      "epoch no.1 train no.84060  loss = 2.64357 avg_loss = 3.49282\n",
      "epoch no.1 train no.84070  loss = 3.79987 avg_loss = 3.52968\n",
      "epoch no.1 train no.84080  loss = 2.61416 avg_loss = 3.47979\n",
      "epoch no.1 train no.84090  loss = 4.07593 avg_loss = 3.56534\n",
      "epoch no.1 train no.84100  loss = 4.35708 avg_loss = 3.58032\n",
      "epoch no.1 train no.84110  loss = 3.12869 avg_loss = 3.58740\n",
      "epoch no.1 train no.84120  loss = 3.95813 avg_loss = 3.58381\n",
      "epoch no.1 train no.84130  loss = 3.47827 avg_loss = 3.58606\n",
      "epoch no.1 train no.84140  loss = 1.81771 avg_loss = 3.52405\n",
      "epoch no.1 train no.84150  loss = 3.61631 avg_loss = 3.57393\n",
      "epoch no.1 train no.84160  loss = 5.57895 avg_loss = 3.66193\n",
      "epoch no.1 train no.84170  loss = 3.82837 avg_loss = 3.63054\n",
      "epoch no.1 train no.84180  loss = 3.99700 avg_loss = 3.60691\n",
      "epoch no.1 train no.84190  loss = 5.89950 avg_loss = 3.62397\n",
      "epoch no.1 train no.84200  loss = 2.54374 avg_loss = 3.60109\n",
      "epoch no.1 train no.84210  loss = 3.62850 avg_loss = 3.63277\n",
      "epoch no.1 train no.84220  loss = 3.48907 avg_loss = 3.60699\n",
      "epoch no.1 train no.84230  loss = 2.98846 avg_loss = 3.62926\n",
      "epoch no.1 train no.84240  loss = 4.91561 avg_loss = 3.64995\n",
      "epoch no.1 train no.84250  loss = 3.07455 avg_loss = 3.65471\n",
      "epoch no.1 train no.84260  loss = 1.91042 avg_loss = 3.62216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.84270  loss = 3.91037 avg_loss = 3.65042\n",
      "epoch no.1 train no.84280  loss = 4.96579 avg_loss = 3.68969\n",
      "epoch no.1 train no.84290  loss = 3.47101 avg_loss = 3.69555\n",
      "epoch no.1 train no.84300  loss = 2.17655 avg_loss = 3.68105\n",
      "epoch no.1 train no.84310  loss = 4.84485 avg_loss = 3.68258\n",
      "epoch no.1 train no.84320  loss = 4.16049 avg_loss = 3.64776\n",
      "epoch no.1 train no.84330  loss = 2.93428 avg_loss = 3.62393\n",
      "epoch no.1 train no.84340  loss = 2.95113 avg_loss = 3.63455\n",
      "epoch no.1 train no.84350  loss = 2.38988 avg_loss = 3.64875\n",
      "epoch no.1 train no.84360  loss = 4.14761 avg_loss = 3.66458\n",
      "epoch no.1 train no.84370  loss = 3.80649 avg_loss = 3.60792\n",
      "epoch no.1 train no.84380  loss = 2.12134 avg_loss = 3.53492\n",
      "epoch no.1 train no.84390  loss = 3.82401 avg_loss = 3.55242\n",
      "epoch no.1 train no.84400  loss = 2.98334 avg_loss = 3.53431\n",
      "epoch no.1 train no.84410  loss = 3.25312 avg_loss = 3.51298\n",
      "epoch no.1 train no.84420  loss = 4.21725 avg_loss = 3.54393\n",
      "epoch no.1 train no.84430  loss = 3.63126 avg_loss = 3.53867\n",
      "epoch no.1 train no.84440  loss = 5.04907 avg_loss = 3.56552\n",
      "epoch no.1 train no.84450  loss = 2.67019 avg_loss = 3.60768\n",
      "epoch no.1 train no.84460  loss = 4.67645 avg_loss = 3.68731\n",
      "epoch no.1 train no.84470  loss = 1.89688 avg_loss = 3.64189\n",
      "epoch no.1 train no.84480  loss = 3.02694 avg_loss = 3.65847\n",
      "epoch no.1 train no.84490  loss = 3.11957 avg_loss = 3.62187\n",
      "epoch no.1 train no.84500  loss = 2.41185 avg_loss = 3.57390\n",
      "epoch no.1 train no.84510  loss = 3.06103 avg_loss = 3.54877\n",
      "epoch no.1 train no.84520  loss = 3.23029 avg_loss = 3.54342\n",
      "epoch no.1 train no.84530  loss = 4.24298 avg_loss = 3.49712\n",
      "epoch no.1 train no.84540  loss = 2.91158 avg_loss = 3.56283\n",
      "epoch no.1 train no.84550  loss = 4.16944 avg_loss = 3.55659\n",
      "epoch no.1 train no.84560  loss = 5.41459 avg_loss = 3.57943\n",
      "epoch no.1 train no.84570  loss = 2.07769 avg_loss = 3.55432\n",
      "epoch no.1 train no.84580  loss = 5.40273 avg_loss = 3.56153\n",
      "epoch no.1 train no.84590  loss = 2.91808 avg_loss = 3.55795\n",
      "epoch no.1 train no.84600  loss = 4.20917 avg_loss = 3.61000\n",
      "epoch no.1 train no.84610  loss = 3.03463 avg_loss = 3.59307\n",
      "epoch no.1 train no.84620  loss = 2.67561 avg_loss = 3.65432\n",
      "epoch no.1 train no.84630  loss = 2.28997 avg_loss = 3.68622\n",
      "epoch no.1 train no.84640  loss = 4.26752 avg_loss = 3.64858\n",
      "epoch no.1 train no.84650  loss = 3.07056 avg_loss = 3.67527\n",
      "epoch no.1 train no.84660  loss = 2.22107 avg_loss = 3.62104\n",
      "epoch no.1 train no.84670  loss = 4.14696 avg_loss = 3.61651\n",
      "epoch no.1 train no.84680  loss = 2.10432 avg_loss = 3.63387\n",
      "epoch no.1 train no.84690  loss = 3.27409 avg_loss = 3.64233\n",
      "epoch no.1 train no.84700  loss = 3.68719 avg_loss = 3.62904\n",
      "epoch no.1 train no.84710  loss = 1.83242 avg_loss = 3.60273\n",
      "epoch no.1 train no.84720  loss = 3.93444 avg_loss = 3.65464\n",
      "epoch no.1 train no.84730  loss = 2.46441 avg_loss = 3.65449\n",
      "epoch no.1 train no.84740  loss = 4.35905 avg_loss = 3.61723\n",
      "epoch no.1 train no.84750  loss = 3.91419 avg_loss = 3.64127\n",
      "epoch no.1 train no.84760  loss = 1.90890 avg_loss = 3.61738\n",
      "epoch no.1 train no.84770  loss = 6.07766 avg_loss = 3.68113\n",
      "epoch no.1 train no.84780  loss = 3.16291 avg_loss = 3.68256\n",
      "epoch no.1 train no.84790  loss = 3.57988 avg_loss = 3.68230\n",
      "epoch no.1 train no.84800  loss = 3.09874 avg_loss = 3.65852\n",
      "epoch no.1 train no.84810  loss = 5.02735 avg_loss = 3.68132\n",
      "epoch no.1 train no.84820  loss = 3.20722 avg_loss = 3.64796\n",
      "epoch no.1 train no.84830  loss = 3.57254 avg_loss = 3.60573\n",
      "epoch no.1 train no.84840  loss = 5.69430 avg_loss = 3.63939\n",
      "epoch no.1 train no.84850  loss = 2.94282 avg_loss = 3.57825\n",
      "epoch no.1 train no.84860  loss = 1.88592 avg_loss = 3.55944\n",
      "epoch no.1 train no.84870  loss = 3.04009 avg_loss = 3.61980\n",
      "epoch no.1 train no.84880  loss = 5.39366 avg_loss = 3.62733\n",
      "epoch no.1 train no.84890  loss = 3.41215 avg_loss = 3.61440\n",
      "epoch no.1 train no.84900  loss = 3.27511 avg_loss = 3.53379\n",
      "epoch no.1 train no.84910  loss = 2.48712 avg_loss = 3.55406\n",
      "epoch no.1 train no.84920  loss = 3.89783 avg_loss = 3.54338\n",
      "epoch no.1 train no.84930  loss = 5.02128 avg_loss = 3.56797\n",
      "epoch no.1 train no.84940  loss = 2.91006 avg_loss = 3.56786\n",
      "epoch no.1 train no.84950  loss = 5.70965 avg_loss = 3.54660\n",
      "epoch no.1 train no.84960  loss = 2.44412 avg_loss = 3.57842\n",
      "epoch no.1 train no.84970  loss = 3.93092 avg_loss = 3.55158\n",
      "epoch no.1 train no.84980  loss = 5.95978 avg_loss = 3.59604\n",
      "epoch no.1 train no.84990  loss = 4.93173 avg_loss = 3.60683\n",
      "epoch no.1 train no.85000  loss = 3.71148 avg_loss = 3.62626\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '보', '▁힐링']\n",
      "여름밤의 재즈로</s>\n",
      "epoch no.1 train no.85010  loss = 4.79991 avg_loss = 3.66078\n",
      "epoch no.1 train no.85020  loss = 3.71811 avg_loss = 3.62873\n",
      "epoch no.1 train no.85030  loss = 3.55485 avg_loss = 3.60333\n",
      "epoch no.1 train no.85040  loss = 3.51346 avg_loss = 3.57339\n",
      "epoch no.1 train no.85050  loss = 2.63441 avg_loss = 3.58478\n",
      "epoch no.1 train no.85060  loss = 3.00549 avg_loss = 3.58890\n",
      "epoch no.1 train no.85070  loss = 3.49499 avg_loss = 3.57442\n",
      "epoch no.1 train no.85080  loss = 3.35181 avg_loss = 3.56941\n",
      "epoch no.1 train no.85090  loss = 3.25749 avg_loss = 3.54002\n",
      "epoch no.1 train no.85100  loss = 3.58896 avg_loss = 3.55811\n",
      "epoch no.1 train no.85110  loss = 2.77862 avg_loss = 3.55269\n",
      "epoch no.1 train no.85120  loss = 2.69225 avg_loss = 3.54218\n",
      "epoch no.1 train no.85130  loss = 5.87413 avg_loss = 3.58584\n",
      "epoch no.1 train no.85140  loss = 5.07607 avg_loss = 3.60796\n",
      "epoch no.1 train no.85150  loss = 5.80525 avg_loss = 3.67267\n",
      "epoch no.1 train no.85160  loss = 2.65727 avg_loss = 3.70221\n",
      "epoch no.1 train no.85170  loss = 7.11689 avg_loss = 3.73599\n",
      "epoch no.1 train no.85180  loss = 4.28229 avg_loss = 3.75964\n",
      "epoch no.1 train no.85190  loss = 2.89165 avg_loss = 3.74366\n",
      "epoch no.1 train no.85200  loss = 2.30300 avg_loss = 3.66338\n",
      "epoch no.1 train no.85210  loss = 4.30279 avg_loss = 3.72458\n",
      "epoch no.1 train no.85220  loss = 2.69689 avg_loss = 3.72779\n",
      "epoch no.1 train no.85230  loss = 5.65292 avg_loss = 3.74909\n",
      "epoch no.1 train no.85240  loss = 4.08159 avg_loss = 3.72708\n",
      "epoch no.1 train no.85250  loss = 2.57359 avg_loss = 3.72155\n",
      "epoch no.1 train no.85260  loss = 5.04251 avg_loss = 3.67202\n",
      "epoch no.1 train no.85270  loss = 5.58357 avg_loss = 3.71748\n",
      "epoch no.1 train no.85280  loss = 4.38599 avg_loss = 3.70800\n",
      "epoch no.1 train no.85290  loss = 4.37858 avg_loss = 3.72063\n",
      "epoch no.1 train no.85300  loss = 2.76649 avg_loss = 3.75990\n",
      "epoch no.1 train no.85310  loss = 3.43598 avg_loss = 3.76487\n",
      "epoch no.1 train no.85320  loss = 3.52077 avg_loss = 3.72924\n",
      "epoch no.1 train no.85330  loss = 4.61540 avg_loss = 3.77005\n",
      "epoch no.1 train no.85340  loss = 5.27346 avg_loss = 3.76470\n",
      "epoch no.1 train no.85350  loss = 2.91953 avg_loss = 3.77912\n",
      "epoch no.1 train no.85360  loss = 2.37666 avg_loss = 3.76294\n",
      "epoch no.1 train no.85370  loss = 2.63303 avg_loss = 3.75308\n",
      "epoch no.1 train no.85380  loss = 3.93131 avg_loss = 3.74984\n",
      "epoch no.1 train no.85390  loss = 2.56911 avg_loss = 3.69233\n",
      "epoch no.1 train no.85400  loss = 2.34203 avg_loss = 3.64773\n",
      "epoch no.1 train no.85410  loss = 4.19624 avg_loss = 3.66078\n",
      "epoch no.1 train no.85420  loss = 3.78226 avg_loss = 3.68122\n",
      "epoch no.1 train no.85430  loss = 3.29605 avg_loss = 3.64587\n",
      "epoch no.1 train no.85440  loss = 3.62335 avg_loss = 3.64150\n",
      "epoch no.1 train no.85450  loss = 3.06951 avg_loss = 3.63532\n",
      "epoch no.1 train no.85460  loss = 3.96344 avg_loss = 3.60768\n",
      "epoch no.1 train no.85470  loss = 4.17362 avg_loss = 3.62966\n",
      "epoch no.1 train no.85480  loss = 4.42237 avg_loss = 3.67067\n",
      "epoch no.1 train no.85490  loss = 2.67091 avg_loss = 3.61000\n",
      "epoch no.1 train no.85500  loss = 5.26605 avg_loss = 3.65184\n",
      "epoch no.1 train no.85510  loss = 3.71487 avg_loss = 3.68690\n",
      "epoch no.1 train no.85520  loss = 3.57061 avg_loss = 3.68117\n",
      "epoch no.1 train no.85530  loss = 5.63137 avg_loss = 3.66300\n",
      "epoch no.1 train no.85540  loss = 4.71000 avg_loss = 3.67790\n",
      "epoch no.1 train no.85550  loss = 3.71540 avg_loss = 3.70393\n",
      "epoch no.1 train no.85560  loss = 2.39290 avg_loss = 3.72020\n",
      "epoch no.1 train no.85570  loss = 5.08025 avg_loss = 3.69808\n",
      "epoch no.1 train no.85580  loss = 4.17175 avg_loss = 3.74036\n",
      "epoch no.1 train no.85590  loss = 4.08080 avg_loss = 3.69921\n",
      "epoch no.1 train no.85600  loss = 3.41148 avg_loss = 3.62070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.85610  loss = 4.40948 avg_loss = 3.66162\n",
      "epoch no.1 train no.85620  loss = 3.45835 avg_loss = 3.64430\n",
      "epoch no.1 train no.85630  loss = 5.64723 avg_loss = 3.68255\n",
      "epoch no.1 train no.85640  loss = 3.49545 avg_loss = 3.70864\n",
      "epoch no.1 train no.85650  loss = 3.88006 avg_loss = 3.73848\n",
      "epoch no.1 train no.85660  loss = 3.58369 avg_loss = 3.73382\n",
      "epoch no.1 train no.85670  loss = 3.59019 avg_loss = 3.76358\n",
      "epoch no.1 train no.85680  loss = 3.58812 avg_loss = 3.71706\n",
      "epoch no.1 train no.85690  loss = 4.91833 avg_loss = 3.68705\n",
      "epoch no.1 train no.85700  loss = 3.86462 avg_loss = 3.71573\n",
      "epoch no.1 train no.85710  loss = 4.01349 avg_loss = 3.75201\n",
      "epoch no.1 train no.85720  loss = 3.10929 avg_loss = 3.75436\n",
      "epoch no.1 train no.85730  loss = 3.17030 avg_loss = 3.67264\n",
      "epoch no.1 train no.85740  loss = 4.46079 avg_loss = 3.66663\n",
      "epoch no.1 train no.85750  loss = 4.00595 avg_loss = 3.65560\n",
      "epoch no.1 train no.85760  loss = 3.72093 avg_loss = 3.65545\n",
      "epoch no.1 train no.85770  loss = 3.70513 avg_loss = 3.64431\n",
      "epoch no.1 train no.85780  loss = 3.55713 avg_loss = 3.63517\n",
      "epoch no.1 train no.85790  loss = 2.78347 avg_loss = 3.64990\n",
      "epoch no.1 train no.85800  loss = 3.88287 avg_loss = 3.66084\n",
      "epoch no.1 train no.85810  loss = 3.82117 avg_loss = 3.70029\n",
      "epoch no.1 train no.85820  loss = 4.72806 avg_loss = 3.70308\n",
      "epoch no.1 train no.85830  loss = 4.73678 avg_loss = 3.71531\n",
      "epoch no.1 train no.85840  loss = 3.90824 avg_loss = 3.71812\n",
      "epoch no.1 train no.85850  loss = 2.18102 avg_loss = 3.69043\n",
      "epoch no.1 train no.85860  loss = 4.88276 avg_loss = 3.72140\n",
      "epoch no.1 train no.85870  loss = 3.23295 avg_loss = 3.74423\n",
      "epoch no.1 train no.85880  loss = 4.99771 avg_loss = 3.75456\n",
      "epoch no.1 train no.85890  loss = 3.14044 avg_loss = 3.77981\n",
      "epoch no.1 train no.85900  loss = 3.03670 avg_loss = 3.73331\n",
      "epoch no.1 train no.85910  loss = 3.35053 avg_loss = 3.72049\n",
      "epoch no.1 train no.85920  loss = 2.44334 avg_loss = 3.71089\n",
      "epoch no.1 train no.85930  loss = 3.22841 avg_loss = 3.69317\n",
      "epoch no.1 train no.85940  loss = 4.09761 avg_loss = 3.73300\n",
      "epoch no.1 train no.85950  loss = 4.44891 avg_loss = 3.73165\n",
      "epoch no.1 train no.85960  loss = 5.93336 avg_loss = 3.70946\n",
      "epoch no.1 train no.85970  loss = 1.84568 avg_loss = 3.66037\n",
      "epoch no.1 train no.85980  loss = 6.31764 avg_loss = 3.70938\n",
      "epoch no.1 train no.85990  loss = 2.52735 avg_loss = 3.68407\n",
      "epoch no.1 train no.86000  loss = 1.74086 avg_loss = 3.70187\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '하며', '때', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 산책할때 듣기 좋은 노래</s>\n",
      "epoch no.1 train no.86010  loss = 2.42616 avg_loss = 3.68757\n",
      "epoch no.1 train no.86020  loss = 5.10532 avg_loss = 3.70615\n",
      "epoch no.1 train no.86030  loss = 3.07276 avg_loss = 3.68717\n",
      "epoch no.1 train no.86040  loss = 3.64167 avg_loss = 3.71123\n",
      "epoch no.1 train no.86050  loss = 2.72765 avg_loss = 3.72432\n",
      "epoch no.1 train no.86060  loss = 3.05946 avg_loss = 3.78598\n",
      "epoch no.1 train no.86070  loss = 2.42393 avg_loss = 3.75501\n",
      "epoch no.1 train no.86080  loss = 2.73252 avg_loss = 3.70354\n",
      "epoch no.1 train no.86090  loss = 5.98771 avg_loss = 3.76987\n",
      "epoch no.1 train no.86100  loss = 4.42364 avg_loss = 3.76722\n",
      "epoch no.1 train no.86110  loss = 3.79915 avg_loss = 3.76315\n",
      "epoch no.1 train no.86120  loss = 2.43268 avg_loss = 3.77299\n",
      "epoch no.1 train no.86130  loss = 4.66354 avg_loss = 3.78199\n",
      "epoch no.1 train no.86140  loss = 5.20356 avg_loss = 3.77965\n",
      "epoch no.1 train no.86150  loss = 3.17032 avg_loss = 3.73834\n",
      "epoch no.1 train no.86160  loss = 4.57359 avg_loss = 3.66580\n",
      "epoch no.1 train no.86170  loss = 4.02781 avg_loss = 3.65708\n",
      "epoch no.1 train no.86180  loss = 4.04643 avg_loss = 3.65695\n",
      "epoch no.1 train no.86190  loss = 6.19533 avg_loss = 3.69156\n",
      "epoch no.1 train no.86200  loss = 3.39456 avg_loss = 3.61032\n",
      "epoch no.1 train no.86210  loss = 3.51322 avg_loss = 3.56994\n",
      "epoch no.1 train no.86220  loss = 2.53355 avg_loss = 3.57623\n",
      "epoch no.1 train no.86230  loss = 2.39780 avg_loss = 3.60229\n",
      "epoch no.1 train no.86240  loss = 4.44910 avg_loss = 3.57881\n",
      "epoch no.1 train no.86250  loss = 2.81227 avg_loss = 3.59273\n",
      "epoch no.1 train no.86260  loss = 4.03704 avg_loss = 3.64411\n",
      "epoch no.1 train no.86270  loss = 4.25717 avg_loss = 3.63806\n",
      "epoch no.1 train no.86280  loss = 1.19673 avg_loss = 3.59841\n",
      "epoch no.1 train no.86290  loss = 2.66985 avg_loss = 3.58792\n",
      "epoch no.1 train no.86300  loss = 2.83591 avg_loss = 3.58205\n",
      "epoch no.1 train no.86310  loss = 1.88986 avg_loss = 3.61072\n",
      "epoch no.1 train no.86320  loss = 1.99851 avg_loss = 3.56051\n",
      "epoch no.1 train no.86330  loss = 5.45467 avg_loss = 3.54206\n",
      "epoch no.1 train no.86340  loss = 3.59331 avg_loss = 3.58523\n",
      "epoch no.1 train no.86350  loss = 3.45069 avg_loss = 3.63502\n",
      "epoch no.1 train no.86360  loss = 2.29406 avg_loss = 3.59154\n",
      "epoch no.1 train no.86370  loss = 2.39840 avg_loss = 3.54951\n",
      "epoch no.1 train no.86380  loss = 2.72341 avg_loss = 3.60093\n",
      "epoch no.1 train no.86390  loss = 3.49064 avg_loss = 3.57806\n",
      "epoch no.1 train no.86400  loss = 2.39782 avg_loss = 3.56161\n",
      "epoch no.1 train no.86410  loss = 4.32334 avg_loss = 3.54520\n",
      "epoch no.1 train no.86420  loss = 3.08366 avg_loss = 3.50399\n",
      "epoch no.1 train no.86430  loss = 2.65462 avg_loss = 3.47151\n",
      "epoch no.1 train no.86440  loss = 3.58963 avg_loss = 3.47003\n",
      "epoch no.1 train no.86450  loss = 3.98709 avg_loss = 3.51171\n",
      "epoch no.1 train no.86460  loss = 2.70016 avg_loss = 3.50397\n",
      "epoch no.1 train no.86470  loss = 2.47013 avg_loss = 3.47177\n",
      "epoch no.1 train no.86480  loss = 3.30270 avg_loss = 3.46835\n",
      "epoch no.1 train no.86490  loss = 3.14088 avg_loss = 3.50629\n",
      "epoch no.1 train no.86500  loss = 3.18619 avg_loss = 3.52854\n",
      "epoch no.1 train no.86510  loss = 3.45789 avg_loss = 3.58016\n",
      "epoch no.1 train no.86520  loss = 3.15728 avg_loss = 3.58477\n",
      "epoch no.1 train no.86530  loss = 1.68112 avg_loss = 3.48910\n",
      "epoch no.1 train no.86540  loss = 3.95440 avg_loss = 3.51928\n",
      "epoch no.1 train no.86550  loss = 3.12714 avg_loss = 3.53518\n",
      "epoch no.1 train no.86560  loss = 4.85353 avg_loss = 3.56413\n",
      "epoch no.1 train no.86570  loss = 5.20872 avg_loss = 3.54483\n",
      "epoch no.1 train no.86580  loss = 2.38033 avg_loss = 3.53399\n",
      "epoch no.1 train no.86590  loss = 4.17235 avg_loss = 3.58867\n",
      "epoch no.1 train no.86600  loss = 2.29955 avg_loss = 3.54320\n",
      "epoch no.1 train no.86610  loss = 5.18085 avg_loss = 3.57813\n",
      "epoch no.1 train no.86620  loss = 4.12114 avg_loss = 3.60686\n",
      "epoch no.1 train no.86630  loss = 3.74916 avg_loss = 3.59555\n",
      "epoch no.1 train no.86640  loss = 5.12503 avg_loss = 3.65275\n",
      "epoch no.1 train no.86650  loss = 3.49443 avg_loss = 3.67376\n",
      "epoch no.1 train no.86660  loss = 2.96266 avg_loss = 3.72169\n",
      "epoch no.1 train no.86670  loss = 2.98010 avg_loss = 3.75974\n",
      "epoch no.1 train no.86680  loss = 2.83000 avg_loss = 3.71652\n",
      "epoch no.1 train no.86690  loss = 2.98129 avg_loss = 3.69170\n",
      "epoch no.1 train no.86700  loss = 4.75023 avg_loss = 3.66597\n",
      "epoch no.1 train no.86710  loss = 3.07535 avg_loss = 3.63607\n",
      "epoch no.1 train no.86720  loss = 3.53178 avg_loss = 3.60388\n",
      "epoch no.1 train no.86730  loss = 2.52897 avg_loss = 3.58659\n",
      "epoch no.1 train no.86740  loss = 3.51912 avg_loss = 3.61584\n",
      "epoch no.1 train no.86750  loss = 6.46463 avg_loss = 3.68339\n",
      "epoch no.1 train no.86760  loss = 3.85478 avg_loss = 3.66446\n",
      "epoch no.1 train no.86770  loss = 2.83656 avg_loss = 3.64757\n",
      "epoch no.1 train no.86780  loss = 2.49046 avg_loss = 3.58323\n",
      "epoch no.1 train no.86790  loss = 2.25528 avg_loss = 3.55680\n",
      "epoch no.1 train no.86800  loss = 3.94788 avg_loss = 3.61051\n",
      "epoch no.1 train no.86810  loss = 2.47565 avg_loss = 3.59292\n",
      "epoch no.1 train no.86820  loss = 3.51802 avg_loss = 3.51654\n",
      "epoch no.1 train no.86830  loss = 2.41420 avg_loss = 3.48494\n",
      "epoch no.1 train no.86840  loss = 2.88337 avg_loss = 3.50454\n",
      "epoch no.1 train no.86850  loss = 5.09376 avg_loss = 3.55911\n",
      "epoch no.1 train no.86860  loss = 3.71113 avg_loss = 3.57024\n",
      "epoch no.1 train no.86870  loss = 2.69815 avg_loss = 3.57013\n",
      "epoch no.1 train no.86880  loss = 2.51004 avg_loss = 3.50677\n",
      "epoch no.1 train no.86890  loss = 3.91265 avg_loss = 3.50067\n",
      "epoch no.1 train no.86900  loss = 3.38730 avg_loss = 3.47684\n",
      "epoch no.1 train no.86910  loss = 5.28715 avg_loss = 3.50502\n",
      "epoch no.1 train no.86920  loss = 5.56162 avg_loss = 3.51725\n",
      "epoch no.1 train no.86930  loss = 5.37491 avg_loss = 3.50748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.86940  loss = 2.59827 avg_loss = 3.51356\n",
      "epoch no.1 train no.86950  loss = 2.79907 avg_loss = 3.51976\n",
      "epoch no.1 train no.86960  loss = 3.02967 avg_loss = 3.49923\n",
      "epoch no.1 train no.86970  loss = 5.62635 avg_loss = 3.57716\n",
      "epoch no.1 train no.86980  loss = 4.03110 avg_loss = 3.56612\n",
      "epoch no.1 train no.86990  loss = 3.57480 avg_loss = 3.53742\n",
      "epoch no.1 train no.87000  loss = 4.43331 avg_loss = 3.52253\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '의', '▁드라이브', '이', '▁되어', '▁노래', '</s>']\n",
      "여름밤의 추억이 담긴 노래</s>\n",
      "epoch no.1 train no.87010  loss = 5.01568 avg_loss = 3.50878\n",
      "epoch no.1 train no.87020  loss = 2.77938 avg_loss = 3.51982\n",
      "epoch no.1 train no.87030  loss = 3.02625 avg_loss = 3.55836\n",
      "epoch no.1 train no.87040  loss = 2.48690 avg_loss = 3.52469\n",
      "epoch no.1 train no.87050  loss = 4.54434 avg_loss = 3.52613\n",
      "epoch no.1 train no.87060  loss = 3.51177 avg_loss = 3.51877\n",
      "epoch no.1 train no.87070  loss = 4.89220 avg_loss = 3.49627\n",
      "epoch no.1 train no.87080  loss = 3.10150 avg_loss = 3.46040\n",
      "epoch no.1 train no.87090  loss = 2.75825 avg_loss = 3.45443\n",
      "epoch no.1 train no.87100  loss = 4.06027 avg_loss = 3.51298\n",
      "epoch no.1 train no.87110  loss = 6.35818 avg_loss = 3.56860\n",
      "epoch no.1 train no.87120  loss = 2.82988 avg_loss = 3.52946\n",
      "epoch no.1 train no.87130  loss = 3.18086 avg_loss = 3.49459\n",
      "epoch no.1 train no.87140  loss = 6.33673 avg_loss = 3.55140\n",
      "epoch no.1 train no.87150  loss = 2.50990 avg_loss = 3.50423\n",
      "epoch no.1 train no.87160  loss = 3.27889 avg_loss = 3.58502\n",
      "epoch no.1 train no.87170  loss = 5.25959 avg_loss = 3.58247\n",
      "epoch no.1 train no.87180  loss = 3.16072 avg_loss = 3.60331\n",
      "epoch no.1 train no.87190  loss = 3.76516 avg_loss = 3.64935\n",
      "epoch no.1 train no.87200  loss = 3.06803 avg_loss = 3.71164\n",
      "epoch no.1 train no.87210  loss = 2.80965 avg_loss = 3.68854\n",
      "epoch no.1 train no.87220  loss = 3.78304 avg_loss = 3.69715\n",
      "epoch no.1 train no.87230  loss = 2.59545 avg_loss = 3.66184\n",
      "epoch no.1 train no.87240  loss = 3.34365 avg_loss = 3.64928\n",
      "epoch no.1 train no.87250  loss = 3.09008 avg_loss = 3.69913\n",
      "epoch no.1 train no.87260  loss = 1.50646 avg_loss = 3.69972\n",
      "epoch no.1 train no.87270  loss = 3.64252 avg_loss = 3.66269\n",
      "epoch no.1 train no.87280  loss = 5.43971 avg_loss = 3.65980\n",
      "epoch no.1 train no.87290  loss = 1.93158 avg_loss = 3.64113\n",
      "epoch no.1 train no.87300  loss = 4.68026 avg_loss = 3.64429\n",
      "epoch no.1 train no.87310  loss = 3.30815 avg_loss = 3.62889\n",
      "epoch no.1 train no.87320  loss = 3.77731 avg_loss = 3.63886\n",
      "epoch no.1 train no.87330  loss = 3.37408 avg_loss = 3.64268\n",
      "epoch no.1 train no.87340  loss = 3.46465 avg_loss = 3.65760\n",
      "epoch no.1 train no.87350  loss = 4.25982 avg_loss = 3.61788\n",
      "epoch no.1 train no.87360  loss = 2.75261 avg_loss = 3.56994\n",
      "epoch no.1 train no.87370  loss = 4.35901 avg_loss = 3.56554\n",
      "epoch no.1 train no.87380  loss = 3.83863 avg_loss = 3.56664\n",
      "epoch no.1 train no.87390  loss = 3.47244 avg_loss = 3.56140\n",
      "epoch no.1 train no.87400  loss = 2.73307 avg_loss = 3.53734\n",
      "epoch no.1 train no.87410  loss = 4.36760 avg_loss = 3.52167\n",
      "epoch no.1 train no.87420  loss = 3.85081 avg_loss = 3.46235\n",
      "epoch no.1 train no.87430  loss = 3.19947 avg_loss = 3.39995\n",
      "epoch no.1 train no.87440  loss = 3.74701 avg_loss = 3.41640\n",
      "epoch no.1 train no.87450  loss = 3.26666 avg_loss = 3.42958\n",
      "epoch no.1 train no.87460  loss = 5.12863 avg_loss = 3.42053\n",
      "epoch no.1 train no.87470  loss = 4.82360 avg_loss = 3.50754\n",
      "epoch no.1 train no.87480  loss = 3.62986 avg_loss = 3.53334\n",
      "epoch no.1 train no.87490  loss = 4.30487 avg_loss = 3.53477\n",
      "epoch no.1 train no.87500  loss = 2.82287 avg_loss = 3.54728\n",
      "epoch no.1 train no.87510  loss = 2.37285 avg_loss = 3.50792\n",
      "epoch no.1 train no.87520  loss = 1.52095 avg_loss = 3.48622\n",
      "epoch no.1 train no.87530  loss = 3.72402 avg_loss = 3.55750\n",
      "epoch no.1 train no.87540  loss = 3.12465 avg_loss = 3.57945\n",
      "epoch no.1 train no.87550  loss = 3.76306 avg_loss = 3.57743\n",
      "epoch no.1 train no.87560  loss = 2.86363 avg_loss = 3.62489\n",
      "epoch no.1 train no.87570  loss = 5.04120 avg_loss = 3.61749\n",
      "epoch no.1 train no.87580  loss = 5.51502 avg_loss = 3.62662\n",
      "epoch no.1 train no.87590  loss = 3.01601 avg_loss = 3.68656\n",
      "epoch no.1 train no.87600  loss = 3.89568 avg_loss = 3.70106\n",
      "epoch no.1 train no.87610  loss = 2.07288 avg_loss = 3.64705\n",
      "epoch no.1 train no.87620  loss = 3.67633 avg_loss = 3.60124\n",
      "epoch no.1 train no.87630  loss = 5.17491 avg_loss = 3.58326\n",
      "epoch no.1 train no.87640  loss = 4.01551 avg_loss = 3.60430\n",
      "epoch no.1 train no.87650  loss = 2.47438 avg_loss = 3.68233\n",
      "epoch no.1 train no.87660  loss = 2.03431 avg_loss = 3.65023\n",
      "epoch no.1 train no.87670  loss = 5.82945 avg_loss = 3.62585\n",
      "epoch no.1 train no.87680  loss = 4.24721 avg_loss = 3.63987\n",
      "epoch no.1 train no.87690  loss = 1.71978 avg_loss = 3.59340\n",
      "epoch no.1 train no.87700  loss = 3.62028 avg_loss = 3.55345\n",
      "epoch no.1 train no.87710  loss = 3.65517 avg_loss = 3.52792\n",
      "epoch no.1 train no.87720  loss = 2.23538 avg_loss = 3.53434\n",
      "epoch no.1 train no.87730  loss = 3.30401 avg_loss = 3.55055\n",
      "epoch no.1 train no.87740  loss = 4.58677 avg_loss = 3.58065\n",
      "epoch no.1 train no.87750  loss = 4.52395 avg_loss = 3.56657\n",
      "epoch no.1 train no.87760  loss = 5.77479 avg_loss = 3.60018\n",
      "epoch no.1 train no.87770  loss = 6.06339 avg_loss = 3.68018\n",
      "epoch no.1 train no.87780  loss = 2.68118 avg_loss = 3.60280\n",
      "epoch no.1 train no.87790  loss = 6.44835 avg_loss = 3.64850\n",
      "epoch no.1 train no.87800  loss = 1.58892 avg_loss = 3.65745\n",
      "epoch no.1 train no.87810  loss = 2.25970 avg_loss = 3.66137\n",
      "epoch no.1 train no.87820  loss = 2.88054 avg_loss = 3.66293\n",
      "epoch no.1 train no.87830  loss = 4.36867 avg_loss = 3.69306\n",
      "epoch no.1 train no.87840  loss = 5.10281 avg_loss = 3.68602\n",
      "epoch no.1 train no.87850  loss = 4.39129 avg_loss = 3.64436\n",
      "epoch no.1 train no.87860  loss = 2.19323 avg_loss = 3.64696\n",
      "epoch no.1 train no.87870  loss = 2.04212 avg_loss = 3.60007\n",
      "epoch no.1 train no.87880  loss = 2.34901 avg_loss = 3.61770\n",
      "epoch no.1 train no.87890  loss = 4.77198 avg_loss = 3.62901\n",
      "epoch no.1 train no.87900  loss = 3.34366 avg_loss = 3.61570\n",
      "epoch no.1 train no.87910  loss = 4.19699 avg_loss = 3.62121\n",
      "epoch no.1 train no.87920  loss = 3.20184 avg_loss = 3.68352\n",
      "epoch no.1 train no.87930  loss = 3.07884 avg_loss = 3.68848\n",
      "epoch no.1 train no.87940  loss = 4.61149 avg_loss = 3.65035\n",
      "epoch no.1 train no.87950  loss = 4.55338 avg_loss = 3.66014\n",
      "epoch no.1 train no.87960  loss = 4.48885 avg_loss = 3.64293\n",
      "epoch no.1 train no.87970  loss = 3.94795 avg_loss = 3.65668\n",
      "epoch no.1 train no.87980  loss = 3.09284 avg_loss = 3.71793\n",
      "epoch no.1 train no.87990  loss = 3.51218 avg_loss = 3.73033\n",
      "epoch no.1 train no.88000  loss = 5.52928 avg_loss = 3.72409\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓을', '▁노래', '▁노래', '송', '</s>']\n",
      "여름밤을 수놓을 신나는 팝송</s>\n",
      "epoch no.1 train no.88010  loss = 2.21187 avg_loss = 3.73150\n",
      "epoch no.1 train no.88020  loss = 2.44808 avg_loss = 3.69590\n",
      "epoch no.1 train no.88030  loss = 4.64345 avg_loss = 3.67162\n",
      "epoch no.1 train no.88040  loss = 4.48103 avg_loss = 3.66474\n",
      "epoch no.1 train no.88050  loss = 3.08719 avg_loss = 3.70356\n",
      "epoch no.1 train no.88060  loss = 5.18750 avg_loss = 3.71128\n",
      "epoch no.1 train no.88070  loss = 2.43341 avg_loss = 3.67775\n",
      "epoch no.1 train no.88080  loss = 3.80571 avg_loss = 3.67243\n",
      "epoch no.1 train no.88090  loss = 2.37622 avg_loss = 3.61823\n",
      "epoch no.1 train no.88100  loss = 3.64775 avg_loss = 3.65600\n",
      "epoch no.1 train no.88110  loss = 4.16785 avg_loss = 3.65230\n",
      "epoch no.1 train no.88120  loss = 4.55330 avg_loss = 3.66299\n",
      "epoch no.1 train no.88130  loss = 1.72857 avg_loss = 3.67214\n",
      "epoch no.1 train no.88140  loss = 3.14338 avg_loss = 3.66877\n",
      "epoch no.1 train no.88150  loss = 2.18929 avg_loss = 3.66861\n",
      "epoch no.1 train no.88160  loss = 3.35762 avg_loss = 3.65910\n",
      "epoch no.1 train no.88170  loss = 3.75815 avg_loss = 3.66729\n",
      "epoch no.1 train no.88180  loss = 5.11810 avg_loss = 3.71707\n",
      "epoch no.1 train no.88190  loss = 4.02252 avg_loss = 3.71434\n",
      "epoch no.1 train no.88200  loss = 3.41544 avg_loss = 3.64952\n",
      "epoch no.1 train no.88210  loss = 6.05124 avg_loss = 3.65579\n",
      "epoch no.1 train no.88220  loss = 3.91487 avg_loss = 3.73021\n",
      "epoch no.1 train no.88230  loss = 3.95954 avg_loss = 3.71593\n",
      "epoch no.1 train no.88240  loss = 1.83061 avg_loss = 3.71595\n",
      "epoch no.1 train no.88250  loss = 2.86136 avg_loss = 3.72137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.88260  loss = 4.25504 avg_loss = 3.75330\n",
      "epoch no.1 train no.88270  loss = 2.32105 avg_loss = 3.72623\n",
      "epoch no.1 train no.88280  loss = 2.50167 avg_loss = 3.75573\n",
      "epoch no.1 train no.88290  loss = 3.11490 avg_loss = 3.75310\n",
      "epoch no.1 train no.88300  loss = 3.28128 avg_loss = 3.74564\n",
      "epoch no.1 train no.88310  loss = 4.57400 avg_loss = 3.82428\n",
      "epoch no.1 train no.88320  loss = 2.20710 avg_loss = 3.78277\n",
      "epoch no.1 train no.88330  loss = 3.49128 avg_loss = 3.81084\n",
      "epoch no.1 train no.88340  loss = 2.26690 avg_loss = 3.78375\n",
      "epoch no.1 train no.88350  loss = 4.34506 avg_loss = 3.79975\n",
      "epoch no.1 train no.88360  loss = 5.32718 avg_loss = 3.74148\n",
      "epoch no.1 train no.88370  loss = 3.07167 avg_loss = 3.73707\n",
      "epoch no.1 train no.88380  loss = 2.28786 avg_loss = 3.73120\n",
      "epoch no.1 train no.88390  loss = 3.09712 avg_loss = 3.72777\n",
      "epoch no.1 train no.88400  loss = 3.09213 avg_loss = 3.71865\n",
      "epoch no.1 train no.88410  loss = 2.92190 avg_loss = 3.69957\n",
      "epoch no.1 train no.88420  loss = 3.31628 avg_loss = 3.74492\n",
      "epoch no.1 train no.88430  loss = 2.20776 avg_loss = 3.76041\n",
      "epoch no.1 train no.88440  loss = 3.81672 avg_loss = 3.76478\n",
      "epoch no.1 train no.88450  loss = 3.79359 avg_loss = 3.75849\n",
      "epoch no.1 train no.88460  loss = 2.81044 avg_loss = 3.73923\n",
      "epoch no.1 train no.88470  loss = 4.40788 avg_loss = 3.73981\n",
      "epoch no.1 train no.88480  loss = 3.79093 avg_loss = 3.68896\n",
      "epoch no.1 train no.88490  loss = 3.40572 avg_loss = 3.63770\n",
      "epoch no.1 train no.88500  loss = 6.43974 avg_loss = 3.67391\n",
      "epoch no.1 train no.88510  loss = 4.89582 avg_loss = 3.67223\n",
      "epoch no.1 train no.88520  loss = 3.56006 avg_loss = 3.63122\n",
      "epoch no.1 train no.88530  loss = 2.73380 avg_loss = 3.58203\n",
      "epoch no.1 train no.88540  loss = 3.02481 avg_loss = 3.58692\n",
      "epoch no.1 train no.88550  loss = 3.55538 avg_loss = 3.57564\n",
      "epoch no.1 train no.88560  loss = 5.15912 avg_loss = 3.56667\n",
      "epoch no.1 train no.88570  loss = 5.03611 avg_loss = 3.56582\n",
      "epoch no.1 train no.88580  loss = 5.17045 avg_loss = 3.61238\n",
      "epoch no.1 train no.88590  loss = 3.82455 avg_loss = 3.62671\n",
      "epoch no.1 train no.88600  loss = 3.12311 avg_loss = 3.61545\n",
      "epoch no.1 train no.88610  loss = 3.60475 avg_loss = 3.63057\n",
      "epoch no.1 train no.88620  loss = 6.00977 avg_loss = 3.63594\n",
      "epoch no.1 train no.88630  loss = 2.93164 avg_loss = 3.60428\n",
      "epoch no.1 train no.88640  loss = 2.45952 avg_loss = 3.63570\n",
      "epoch no.1 train no.88650  loss = 3.68921 avg_loss = 3.64678\n",
      "epoch no.1 train no.88660  loss = 3.66096 avg_loss = 3.64074\n",
      "epoch no.1 train no.88670  loss = 3.75557 avg_loss = 3.66917\n",
      "epoch no.1 train no.88680  loss = 4.68454 avg_loss = 3.67663\n",
      "epoch no.1 train no.88690  loss = 3.56022 avg_loss = 3.68925\n",
      "epoch no.1 train no.88700  loss = 3.36262 avg_loss = 3.66996\n",
      "epoch no.1 train no.88710  loss = 3.30028 avg_loss = 3.67500\n",
      "epoch no.1 train no.88720  loss = 6.11438 avg_loss = 3.71815\n",
      "epoch no.1 train no.88730  loss = 3.65756 avg_loss = 3.71198\n",
      "epoch no.1 train no.88740  loss = 1.85897 avg_loss = 3.68166\n",
      "epoch no.1 train no.88750  loss = 4.15537 avg_loss = 3.66536\n",
      "epoch no.1 train no.88760  loss = 3.05345 avg_loss = 3.67075\n",
      "epoch no.1 train no.88770  loss = 5.40475 avg_loss = 3.71433\n",
      "epoch no.1 train no.88780  loss = 3.61311 avg_loss = 3.74074\n",
      "epoch no.1 train no.88790  loss = 3.93665 avg_loss = 3.69871\n",
      "epoch no.1 train no.88800  loss = 2.27600 avg_loss = 3.72512\n",
      "epoch no.1 train no.88810  loss = 2.82675 avg_loss = 3.67332\n",
      "epoch no.1 train no.88820  loss = 2.52702 avg_loss = 3.65067\n",
      "epoch no.1 train no.88830  loss = 2.37700 avg_loss = 3.65282\n",
      "epoch no.1 train no.88840  loss = 4.18813 avg_loss = 3.61320\n",
      "epoch no.1 train no.88850  loss = 4.56978 avg_loss = 3.61185\n",
      "epoch no.1 train no.88860  loss = 4.15929 avg_loss = 3.63072\n",
      "epoch no.1 train no.88870  loss = 4.46338 avg_loss = 3.67609\n",
      "epoch no.1 train no.88880  loss = 3.28054 avg_loss = 3.64152\n",
      "epoch no.1 train no.88890  loss = 3.95821 avg_loss = 3.60443\n",
      "epoch no.1 train no.88900  loss = 3.66184 avg_loss = 3.54753\n",
      "epoch no.1 train no.88910  loss = 4.06012 avg_loss = 3.54655\n",
      "epoch no.1 train no.88920  loss = 4.45366 avg_loss = 3.54941\n",
      "epoch no.1 train no.88930  loss = 2.52966 avg_loss = 3.57055\n",
      "epoch no.1 train no.88940  loss = 2.85420 avg_loss = 3.57542\n",
      "epoch no.1 train no.88950  loss = 4.15844 avg_loss = 3.61993\n",
      "epoch no.1 train no.88960  loss = 2.61315 avg_loss = 3.59126\n",
      "epoch no.1 train no.88970  loss = 3.56011 avg_loss = 3.59625\n",
      "epoch no.1 train no.88980  loss = 6.78158 avg_loss = 3.69295\n",
      "epoch no.1 train no.88990  loss = 3.26267 avg_loss = 3.65315\n",
      "epoch no.1 train no.89000  loss = 3.99638 avg_loss = 3.62733\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '음악', '송', '</s>']\n",
      "여름밤에 듣기 좋은 감성 팝송</s>\n",
      "epoch no.1 train no.89010  loss = 5.32077 avg_loss = 3.60054\n",
      "epoch no.1 train no.89020  loss = 4.41133 avg_loss = 3.60058\n",
      "epoch no.1 train no.89030  loss = 3.11351 avg_loss = 3.60140\n",
      "epoch no.1 train no.89040  loss = 6.19014 avg_loss = 3.60253\n",
      "epoch no.1 train no.89050  loss = 4.48939 avg_loss = 3.55655\n",
      "epoch no.1 train no.89060  loss = 2.32905 avg_loss = 3.56045\n",
      "epoch no.1 train no.89070  loss = 1.92773 avg_loss = 3.54927\n",
      "epoch no.1 train no.89080  loss = 3.18413 avg_loss = 3.62059\n",
      "epoch no.1 train no.89090  loss = 3.73811 avg_loss = 3.63168\n",
      "epoch no.1 train no.89100  loss = 5.62436 avg_loss = 3.69704\n",
      "epoch no.1 train no.89110  loss = 2.39487 avg_loss = 3.64156\n",
      "epoch no.1 train no.89120  loss = 2.98039 avg_loss = 3.64912\n",
      "epoch no.1 train no.89130  loss = 5.18419 avg_loss = 3.69480\n",
      "epoch no.1 train no.89140  loss = 3.43222 avg_loss = 3.74127\n",
      "epoch no.1 train no.89150  loss = 2.97671 avg_loss = 3.78534\n",
      "epoch no.1 train no.89160  loss = 2.81217 avg_loss = 3.75950\n",
      "epoch no.1 train no.89170  loss = 2.65659 avg_loss = 3.72113\n",
      "epoch no.1 train no.89180  loss = 3.40091 avg_loss = 3.70847\n",
      "epoch no.1 train no.89190  loss = 2.46476 avg_loss = 3.71457\n",
      "epoch no.1 train no.89200  loss = 4.56200 avg_loss = 3.73942\n",
      "epoch no.1 train no.89210  loss = 3.78599 avg_loss = 3.70455\n",
      "epoch no.1 train no.89220  loss = 5.26214 avg_loss = 3.70557\n",
      "epoch no.1 train no.89230  loss = 2.26250 avg_loss = 3.62158\n",
      "epoch no.1 train no.89240  loss = 2.42470 avg_loss = 3.60623\n",
      "epoch no.1 train no.89250  loss = 2.95261 avg_loss = 3.59565\n",
      "epoch no.1 train no.89260  loss = 6.42274 avg_loss = 3.66667\n",
      "epoch no.1 train no.89270  loss = 3.90930 avg_loss = 3.67832\n",
      "epoch no.1 train no.89280  loss = 2.34227 avg_loss = 3.69310\n",
      "epoch no.1 train no.89290  loss = 3.66995 avg_loss = 3.66338\n",
      "epoch no.1 train no.89300  loss = 4.05174 avg_loss = 3.63396\n",
      "epoch no.1 train no.89310  loss = 5.36053 avg_loss = 3.67219\n",
      "epoch no.1 train no.89320  loss = 4.61678 avg_loss = 3.69324\n",
      "epoch no.1 train no.89330  loss = 3.79796 avg_loss = 3.73645\n",
      "epoch no.1 train no.89340  loss = 2.23698 avg_loss = 3.75983\n",
      "epoch no.1 train no.89350  loss = 4.87240 avg_loss = 3.70166\n",
      "epoch no.1 train no.89360  loss = 3.03455 avg_loss = 3.64438\n",
      "epoch no.1 train no.89370  loss = 5.98831 avg_loss = 3.65064\n",
      "epoch no.1 train no.89380  loss = 3.82480 avg_loss = 3.60804\n",
      "epoch no.1 train no.89390  loss = 2.77822 avg_loss = 3.57762\n",
      "epoch no.1 train no.89400  loss = 3.51857 avg_loss = 3.53585\n",
      "epoch no.1 train no.89410  loss = 2.91402 avg_loss = 3.57673\n",
      "epoch no.1 train no.89420  loss = 2.03307 avg_loss = 3.55048\n",
      "epoch no.1 train no.89430  loss = 3.45528 avg_loss = 3.56073\n",
      "epoch no.1 train no.89440  loss = 3.98943 avg_loss = 3.59192\n",
      "epoch no.1 train no.89450  loss = 4.15126 avg_loss = 3.60266\n",
      "epoch no.1 train no.89460  loss = 2.87568 avg_loss = 3.57983\n",
      "epoch no.1 train no.89470  loss = 3.39988 avg_loss = 3.59216\n",
      "epoch no.1 train no.89480  loss = 2.95379 avg_loss = 3.61580\n",
      "epoch no.1 train no.89490  loss = 3.79508 avg_loss = 3.58709\n",
      "epoch no.1 train no.89500  loss = 3.64745 avg_loss = 3.58743\n",
      "epoch no.1 train no.89510  loss = 3.47327 avg_loss = 3.57267\n",
      "epoch no.1 train no.89520  loss = 2.12759 avg_loss = 3.52537\n",
      "epoch no.1 train no.89530  loss = 2.68628 avg_loss = 3.56098\n",
      "epoch no.1 train no.89540  loss = 2.98214 avg_loss = 3.50081\n",
      "epoch no.1 train no.89550  loss = 2.10431 avg_loss = 3.45223\n",
      "epoch no.1 train no.89560  loss = 2.99765 avg_loss = 3.44765\n",
      "epoch no.1 train no.89570  loss = 3.87187 avg_loss = 3.49171\n",
      "epoch no.1 train no.89580  loss = 4.02568 avg_loss = 3.48048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.89590  loss = 5.91269 avg_loss = 3.53308\n",
      "epoch no.1 train no.89600  loss = 3.63100 avg_loss = 3.52646\n",
      "epoch no.1 train no.89610  loss = 4.05322 avg_loss = 3.52349\n",
      "epoch no.1 train no.89620  loss = 5.51965 avg_loss = 3.49277\n",
      "epoch no.1 train no.89630  loss = 3.41846 avg_loss = 3.50551\n",
      "epoch no.1 train no.89640  loss = 4.75808 avg_loss = 3.52632\n",
      "epoch no.1 train no.89650  loss = 2.85721 avg_loss = 3.51554\n",
      "epoch no.1 train no.89660  loss = 2.50141 avg_loss = 3.55663\n",
      "epoch no.1 train no.89670  loss = 4.68598 avg_loss = 3.60385\n",
      "epoch no.1 train no.89680  loss = 4.14656 avg_loss = 3.57565\n",
      "epoch no.1 train no.89690  loss = 4.88996 avg_loss = 3.54676\n",
      "epoch no.1 train no.89700  loss = 2.56028 avg_loss = 3.54359\n",
      "epoch no.1 train no.89710  loss = 2.32747 avg_loss = 3.51559\n",
      "epoch no.1 train no.89720  loss = 5.14565 avg_loss = 3.55492\n",
      "epoch no.1 train no.89730  loss = 4.73600 avg_loss = 3.58634\n",
      "epoch no.1 train no.89740  loss = 3.46617 avg_loss = 3.56350\n",
      "epoch no.1 train no.89750  loss = 5.28505 avg_loss = 3.51644\n",
      "epoch no.1 train no.89760  loss = 6.42808 avg_loss = 3.53227\n",
      "epoch no.1 train no.89770  loss = 4.95123 avg_loss = 3.56966\n",
      "epoch no.1 train no.89780  loss = 3.26273 avg_loss = 3.56044\n",
      "epoch no.1 train no.89790  loss = 3.02444 avg_loss = 3.53402\n",
      "epoch no.1 train no.89800  loss = 4.32645 avg_loss = 3.51774\n",
      "epoch no.1 train no.89810  loss = 3.09509 avg_loss = 3.52921\n",
      "epoch no.1 train no.89820  loss = 4.18566 avg_loss = 3.54463\n",
      "epoch no.1 train no.89830  loss = 2.97226 avg_loss = 3.61673\n",
      "epoch no.1 train no.89840  loss = 2.96199 avg_loss = 3.68414\n",
      "epoch no.1 train no.89850  loss = 3.59532 avg_loss = 3.61207\n",
      "epoch no.1 train no.89860  loss = 2.84326 avg_loss = 3.57579\n",
      "epoch no.1 train no.89870  loss = 2.56847 avg_loss = 3.61200\n",
      "epoch no.1 train no.89880  loss = 2.85909 avg_loss = 3.61886\n",
      "epoch no.1 train no.89890  loss = 3.87794 avg_loss = 3.64372\n",
      "epoch no.1 train no.89900  loss = 3.13124 avg_loss = 3.60455\n",
      "epoch no.1 train no.89910  loss = 3.10488 avg_loss = 3.62829\n",
      "epoch no.1 train no.89920  loss = 2.99213 avg_loss = 3.62788\n",
      "epoch no.1 train no.89930  loss = 6.28226 avg_loss = 3.68068\n",
      "epoch no.1 train no.89940  loss = 2.67108 avg_loss = 3.61817\n",
      "epoch no.1 train no.89950  loss = 5.23439 avg_loss = 3.65805\n",
      "epoch no.1 train no.89960  loss = 4.62855 avg_loss = 3.63698\n",
      "epoch no.1 train no.89970  loss = 3.35124 avg_loss = 3.60987\n",
      "epoch no.1 train no.89980  loss = 3.93148 avg_loss = 3.58731\n",
      "epoch no.1 train no.89990  loss = 3.02615 avg_loss = 3.58211\n",
      "epoch no.1 train no.90000  loss = 3.17449 avg_loss = 3.54015\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '의', '▁재즈', '음악', '</s>']\n",
      "여름밤의 감성음악</s>\n",
      "epoch no.1 train no.90010  loss = 4.88625 avg_loss = 3.51986\n",
      "epoch no.1 train no.90020  loss = 2.88051 avg_loss = 3.46329\n",
      "epoch no.1 train no.90030  loss = 2.29332 avg_loss = 3.49884\n",
      "epoch no.1 train no.90040  loss = 1.90291 avg_loss = 3.49388\n",
      "epoch no.1 train no.90050  loss = 3.70163 avg_loss = 3.47343\n",
      "epoch no.1 train no.90060  loss = 4.83826 avg_loss = 3.43514\n",
      "epoch no.1 train no.90070  loss = 4.76203 avg_loss = 3.43239\n",
      "epoch no.1 train no.90080  loss = 5.97123 avg_loss = 3.47355\n",
      "epoch no.1 train no.90090  loss = 2.69550 avg_loss = 3.44533\n",
      "epoch no.1 train no.90100  loss = 3.31829 avg_loss = 3.41558\n",
      "epoch no.1 train no.90110  loss = 5.13767 avg_loss = 3.47446\n",
      "epoch no.1 train no.90120  loss = 4.14797 avg_loss = 3.49741\n",
      "epoch no.1 train no.90130  loss = 4.42648 avg_loss = 3.48628\n",
      "epoch no.1 train no.90140  loss = 4.17499 avg_loss = 3.57403\n",
      "epoch no.1 train no.90150  loss = 5.39014 avg_loss = 3.56014\n",
      "epoch no.1 train no.90160  loss = 4.39603 avg_loss = 3.56686\n",
      "epoch no.1 train no.90170  loss = 2.50429 avg_loss = 3.61690\n",
      "epoch no.1 train no.90180  loss = 4.23150 avg_loss = 3.63758\n",
      "epoch no.1 train no.90190  loss = 3.71334 avg_loss = 3.57524\n",
      "epoch no.1 train no.90200  loss = 5.10265 avg_loss = 3.60574\n",
      "epoch no.1 train no.90210  loss = 4.83169 avg_loss = 3.68585\n",
      "epoch no.1 train no.90220  loss = 4.29025 avg_loss = 3.68829\n",
      "epoch no.1 train no.90230  loss = 3.32216 avg_loss = 3.65651\n",
      "epoch no.1 train no.90240  loss = 4.09281 avg_loss = 3.69180\n",
      "epoch no.1 train no.90250  loss = 4.87832 avg_loss = 3.65470\n",
      "epoch no.1 train no.90260  loss = 5.19507 avg_loss = 3.69694\n",
      "epoch no.1 train no.90270  loss = 3.27386 avg_loss = 3.72120\n",
      "epoch no.1 train no.90280  loss = 4.62311 avg_loss = 3.72682\n",
      "epoch no.1 train no.90290  loss = 3.34681 avg_loss = 3.73072\n",
      "epoch no.1 train no.90300  loss = 3.14647 avg_loss = 3.73912\n",
      "epoch no.1 train no.90310  loss = 2.89022 avg_loss = 3.68757\n",
      "epoch no.1 train no.90320  loss = 2.99194 avg_loss = 3.69875\n",
      "epoch no.1 train no.90330  loss = 4.04622 avg_loss = 3.68048\n",
      "epoch no.1 train no.90340  loss = 1.81561 avg_loss = 3.65577\n",
      "epoch no.1 train no.90350  loss = 2.16829 avg_loss = 3.68864\n",
      "epoch no.1 train no.90360  loss = 3.44983 avg_loss = 3.67911\n",
      "epoch no.1 train no.90370  loss = 3.40957 avg_loss = 3.68505\n",
      "epoch no.1 train no.90380  loss = 3.80863 avg_loss = 3.67111\n",
      "epoch no.1 train no.90390  loss = 2.35748 avg_loss = 3.64399\n",
      "epoch no.1 train no.90400  loss = 3.28216 avg_loss = 3.61632\n",
      "epoch no.1 train no.90410  loss = 4.12468 avg_loss = 3.64385\n",
      "epoch no.1 train no.90420  loss = 2.61461 avg_loss = 3.59064\n",
      "epoch no.1 train no.90430  loss = 1.84004 avg_loss = 3.59677\n",
      "epoch no.1 train no.90440  loss = 3.75996 avg_loss = 3.59495\n",
      "epoch no.1 train no.90450  loss = 3.78581 avg_loss = 3.60688\n",
      "epoch no.1 train no.90460  loss = 3.29328 avg_loss = 3.57993\n",
      "epoch no.1 train no.90470  loss = 2.25271 avg_loss = 3.55901\n",
      "epoch no.1 train no.90480  loss = 7.06259 avg_loss = 3.65873\n",
      "epoch no.1 train no.90490  loss = 4.32053 avg_loss = 3.65462\n",
      "epoch no.1 train no.90500  loss = 2.25992 avg_loss = 3.64707\n",
      "epoch no.1 train no.90510  loss = 2.20625 avg_loss = 3.69148\n",
      "epoch no.1 train no.90520  loss = 3.93758 avg_loss = 3.70504\n",
      "epoch no.1 train no.90530  loss = 2.55581 avg_loss = 3.72470\n",
      "epoch no.1 train no.90540  loss = 2.37956 avg_loss = 3.70485\n",
      "epoch no.1 train no.90550  loss = 4.91899 avg_loss = 3.68969\n",
      "epoch no.1 train no.90560  loss = 2.25738 avg_loss = 3.68806\n",
      "epoch no.1 train no.90570  loss = 4.71626 avg_loss = 3.72463\n",
      "epoch no.1 train no.90580  loss = 3.85555 avg_loss = 3.68760\n",
      "epoch no.1 train no.90590  loss = 3.07378 avg_loss = 3.71547\n",
      "epoch no.1 train no.90600  loss = 1.83223 avg_loss = 3.71358\n",
      "epoch no.1 train no.90610  loss = 5.69638 avg_loss = 3.74997\n",
      "epoch no.1 train no.90620  loss = 4.12546 avg_loss = 3.68507\n",
      "epoch no.1 train no.90630  loss = 4.06728 avg_loss = 3.70933\n",
      "epoch no.1 train no.90640  loss = 2.98767 avg_loss = 3.69280\n",
      "epoch no.1 train no.90650  loss = 3.23326 avg_loss = 3.68259\n",
      "epoch no.1 train no.90660  loss = 4.50507 avg_loss = 3.69487\n",
      "epoch no.1 train no.90670  loss = 3.99642 avg_loss = 3.71999\n",
      "epoch no.1 train no.90680  loss = 4.77939 avg_loss = 3.71069\n",
      "epoch no.1 train no.90690  loss = 5.10011 avg_loss = 3.73216\n",
      "epoch no.1 train no.90700  loss = 3.83858 avg_loss = 3.73235\n",
      "epoch no.1 train no.90710  loss = 3.03557 avg_loss = 3.77453\n",
      "epoch no.1 train no.90720  loss = 2.06889 avg_loss = 3.74984\n",
      "epoch no.1 train no.90730  loss = 2.68147 avg_loss = 3.81964\n",
      "epoch no.1 train no.90740  loss = 2.65200 avg_loss = 3.76759\n",
      "epoch no.1 train no.90750  loss = 5.04926 avg_loss = 3.78115\n",
      "epoch no.1 train no.90760  loss = 2.35934 avg_loss = 3.69169\n",
      "epoch no.1 train no.90770  loss = 3.00222 avg_loss = 3.68642\n",
      "epoch no.1 train no.90780  loss = 2.78171 avg_loss = 3.67695\n",
      "epoch no.1 train no.90790  loss = 2.98116 avg_loss = 3.67558\n",
      "epoch no.1 train no.90800  loss = 4.28041 avg_loss = 3.64468\n",
      "epoch no.1 train no.90810  loss = 2.98516 avg_loss = 3.62756\n",
      "epoch no.1 train no.90820  loss = 4.32207 avg_loss = 3.67104\n",
      "epoch no.1 train no.90830  loss = 2.51644 avg_loss = 3.68730\n",
      "epoch no.1 train no.90840  loss = 3.33216 avg_loss = 3.66145\n",
      "epoch no.1 train no.90850  loss = 3.05273 avg_loss = 3.70850\n",
      "epoch no.1 train no.90860  loss = 4.22079 avg_loss = 3.70788\n",
      "epoch no.1 train no.90870  loss = 3.61350 avg_loss = 3.67375\n",
      "epoch no.1 train no.90880  loss = 5.74050 avg_loss = 3.67598\n",
      "epoch no.1 train no.90890  loss = 2.61940 avg_loss = 3.68443\n",
      "epoch no.1 train no.90900  loss = 3.36210 avg_loss = 3.62797\n",
      "epoch no.1 train no.90910  loss = 3.33377 avg_loss = 3.65445\n",
      "epoch no.1 train no.90920  loss = 3.42939 avg_loss = 3.67648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.90930  loss = 2.92169 avg_loss = 3.64768\n",
      "epoch no.1 train no.90940  loss = 2.25431 avg_loss = 3.59392\n",
      "epoch no.1 train no.90950  loss = 3.47497 avg_loss = 3.58470\n",
      "epoch no.1 train no.90960  loss = 3.62745 avg_loss = 3.57300\n",
      "epoch no.1 train no.90970  loss = 2.30746 avg_loss = 3.53041\n",
      "epoch no.1 train no.90980  loss = 3.85270 avg_loss = 3.58355\n",
      "epoch no.1 train no.90990  loss = 2.66552 avg_loss = 3.62432\n",
      "epoch no.1 train no.91000  loss = 2.73454 avg_loss = 3.62754\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '▁역시', '피', '칼', '▁팝', '송', '</s>']\n",
      "여름엔 트로피칼 팝송</s>\n",
      "epoch no.1 train no.91010  loss = 2.99055 avg_loss = 3.60609\n",
      "epoch no.1 train no.91020  loss = 3.88683 avg_loss = 3.56476\n",
      "epoch no.1 train no.91030  loss = 3.65500 avg_loss = 3.57062\n",
      "epoch no.1 train no.91040  loss = 4.99571 avg_loss = 3.61255\n",
      "epoch no.1 train no.91050  loss = 4.28612 avg_loss = 3.61677\n",
      "epoch no.1 train no.91060  loss = 3.37976 avg_loss = 3.59861\n",
      "epoch no.1 train no.91070  loss = 4.90810 avg_loss = 3.67557\n",
      "epoch no.1 train no.91080  loss = 2.78294 avg_loss = 3.64127\n",
      "epoch no.1 train no.91090  loss = 4.60138 avg_loss = 3.59527\n",
      "epoch no.1 train no.91100  loss = 4.09039 avg_loss = 3.59056\n",
      "epoch no.1 train no.91110  loss = 2.89273 avg_loss = 3.59976\n",
      "epoch no.1 train no.91120  loss = 4.91806 avg_loss = 3.64346\n",
      "epoch no.1 train no.91130  loss = 3.40058 avg_loss = 3.65476\n",
      "epoch no.1 train no.91140  loss = 4.70786 avg_loss = 3.64886\n",
      "epoch no.1 train no.91150  loss = 4.82153 avg_loss = 3.62162\n",
      "epoch no.1 train no.91160  loss = 3.33793 avg_loss = 3.63887\n",
      "epoch no.1 train no.91170  loss = 3.12856 avg_loss = 3.63405\n",
      "epoch no.1 train no.91180  loss = 3.69165 avg_loss = 3.63099\n",
      "epoch no.1 train no.91190  loss = 2.42016 avg_loss = 3.64070\n",
      "epoch no.1 train no.91200  loss = 2.82990 avg_loss = 3.64796\n",
      "epoch no.1 train no.91210  loss = 6.19565 avg_loss = 3.65541\n",
      "epoch no.1 train no.91220  loss = 2.75601 avg_loss = 3.62888\n",
      "epoch no.1 train no.91230  loss = 3.09328 avg_loss = 3.63830\n",
      "epoch no.1 train no.91240  loss = 7.01323 avg_loss = 3.65174\n",
      "epoch no.1 train no.91250  loss = 4.19096 avg_loss = 3.65737\n",
      "epoch no.1 train no.91260  loss = 2.74668 avg_loss = 3.63849\n",
      "epoch no.1 train no.91270  loss = 3.22899 avg_loss = 3.61258\n",
      "epoch no.1 train no.91280  loss = 2.24538 avg_loss = 3.54658\n",
      "epoch no.1 train no.91290  loss = 4.95296 avg_loss = 3.51496\n",
      "epoch no.1 train no.91300  loss = 3.17508 avg_loss = 3.54090\n",
      "epoch no.1 train no.91310  loss = 2.27199 avg_loss = 3.51598\n",
      "epoch no.1 train no.91320  loss = 5.44165 avg_loss = 3.54407\n",
      "epoch no.1 train no.91330  loss = 2.93201 avg_loss = 3.62307\n",
      "epoch no.1 train no.91340  loss = 3.22090 avg_loss = 3.62479\n",
      "epoch no.1 train no.91350  loss = 2.77598 avg_loss = 3.55737\n",
      "epoch no.1 train no.91360  loss = 3.30259 avg_loss = 3.52601\n",
      "epoch no.1 train no.91370  loss = 2.91579 avg_loss = 3.51992\n",
      "epoch no.1 train no.91380  loss = 3.08499 avg_loss = 3.46636\n",
      "epoch no.1 train no.91390  loss = 3.98858 avg_loss = 3.51067\n",
      "epoch no.1 train no.91400  loss = 2.75903 avg_loss = 3.49414\n",
      "epoch no.1 train no.91410  loss = 5.58834 avg_loss = 3.55561\n",
      "epoch no.1 train no.91420  loss = 3.69814 avg_loss = 3.54732\n",
      "epoch no.1 train no.91430  loss = 2.35783 avg_loss = 3.56260\n",
      "epoch no.1 train no.91440  loss = 3.34141 avg_loss = 3.56645\n",
      "epoch no.1 train no.91450  loss = 3.10203 avg_loss = 3.53518\n",
      "epoch no.1 train no.91460  loss = 5.01809 avg_loss = 3.58069\n",
      "epoch no.1 train no.91470  loss = 2.88356 avg_loss = 3.60490\n",
      "epoch no.1 train no.91480  loss = 5.87593 avg_loss = 3.63853\n",
      "epoch no.1 train no.91490  loss = 3.60629 avg_loss = 3.63391\n",
      "epoch no.1 train no.91500  loss = 5.59306 avg_loss = 3.63916\n",
      "epoch no.1 train no.91510  loss = 5.33207 avg_loss = 3.62594\n",
      "epoch no.1 train no.91520  loss = 3.05180 avg_loss = 3.61891\n",
      "epoch no.1 train no.91530  loss = 4.39124 avg_loss = 3.71009\n",
      "epoch no.1 train no.91540  loss = 5.26572 avg_loss = 3.71217\n",
      "epoch no.1 train no.91550  loss = 2.70603 avg_loss = 3.70573\n",
      "epoch no.1 train no.91560  loss = 6.34411 avg_loss = 3.69698\n",
      "epoch no.1 train no.91570  loss = 3.80298 avg_loss = 3.70658\n",
      "epoch no.1 train no.91580  loss = 2.83288 avg_loss = 3.71211\n",
      "epoch no.1 train no.91590  loss = 4.89143 avg_loss = 3.74386\n",
      "epoch no.1 train no.91600  loss = 3.16933 avg_loss = 3.80643\n",
      "epoch no.1 train no.91610  loss = 4.23580 avg_loss = 3.80351\n",
      "epoch no.1 train no.91620  loss = 2.57234 avg_loss = 3.78048\n",
      "epoch no.1 train no.91630  loss = 7.26038 avg_loss = 3.74716\n",
      "epoch no.1 train no.91640  loss = 2.73252 avg_loss = 3.73005\n",
      "epoch no.1 train no.91650  loss = 4.30494 avg_loss = 3.73258\n",
      "epoch no.1 train no.91660  loss = 4.16686 avg_loss = 3.74367\n",
      "epoch no.1 train no.91670  loss = 4.05317 avg_loss = 3.70580\n",
      "epoch no.1 train no.91680  loss = 3.51140 avg_loss = 3.72240\n",
      "epoch no.1 train no.91690  loss = 3.49902 avg_loss = 3.66510\n",
      "epoch no.1 train no.91700  loss = 3.02551 avg_loss = 3.65107\n",
      "epoch no.1 train no.91710  loss = 4.35058 avg_loss = 3.69111\n",
      "epoch no.1 train no.91720  loss = 3.44145 avg_loss = 3.66677\n",
      "epoch no.1 train no.91730  loss = 3.33608 avg_loss = 3.64819\n",
      "epoch no.1 train no.91740  loss = 2.10021 avg_loss = 3.60970\n",
      "epoch no.1 train no.91750  loss = 3.27228 avg_loss = 3.64636\n",
      "epoch no.1 train no.91760  loss = 3.95209 avg_loss = 3.66271\n",
      "epoch no.1 train no.91770  loss = 3.39151 avg_loss = 3.65924\n",
      "epoch no.1 train no.91780  loss = 4.22925 avg_loss = 3.66360\n",
      "epoch no.1 train no.91790  loss = 2.53727 avg_loss = 3.68040\n",
      "epoch no.1 train no.91800  loss = 2.05429 avg_loss = 3.63640\n",
      "epoch no.1 train no.91810  loss = 1.70240 avg_loss = 3.60654\n",
      "epoch no.1 train no.91820  loss = 5.42986 avg_loss = 3.60568\n",
      "epoch no.1 train no.91830  loss = 1.88178 avg_loss = 3.59770\n",
      "epoch no.1 train no.91840  loss = 3.91086 avg_loss = 3.61605\n",
      "epoch no.1 train no.91850  loss = 5.94769 avg_loss = 3.71350\n",
      "epoch no.1 train no.91860  loss = 3.62906 avg_loss = 3.63306\n",
      "epoch no.1 train no.91870  loss = 5.09224 avg_loss = 3.60230\n",
      "epoch no.1 train no.91880  loss = 3.03905 avg_loss = 3.55736\n",
      "epoch no.1 train no.91890  loss = 3.73076 avg_loss = 3.53309\n",
      "epoch no.1 train no.91900  loss = 3.64473 avg_loss = 3.56353\n",
      "epoch no.1 train no.91910  loss = 5.31554 avg_loss = 3.52953\n",
      "epoch no.1 train no.91920  loss = 3.60192 avg_loss = 3.60128\n",
      "epoch no.1 train no.91930  loss = 2.81920 avg_loss = 3.61558\n",
      "epoch no.1 train no.91940  loss = 3.19957 avg_loss = 3.58859\n",
      "epoch no.1 train no.91950  loss = 2.63706 avg_loss = 3.55920\n",
      "epoch no.1 train no.91960  loss = 4.05038 avg_loss = 3.63227\n",
      "epoch no.1 train no.91970  loss = 3.45613 avg_loss = 3.61910\n",
      "epoch no.1 train no.91980  loss = 2.76691 avg_loss = 3.68812\n",
      "epoch no.1 train no.91990  loss = 2.99252 avg_loss = 3.68727\n",
      "epoch no.1 train no.92000  loss = 1.96771 avg_loss = 3.62393\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '음악', '</s>']\n",
      "여름밤에 어울리는 인디음악</s>\n",
      "epoch no.1 train no.92010  loss = 5.83156 avg_loss = 3.67351\n",
      "epoch no.1 train no.92020  loss = 4.94896 avg_loss = 3.60153\n",
      "epoch no.1 train no.92030  loss = 3.25766 avg_loss = 3.54821\n",
      "epoch no.1 train no.92040  loss = 4.42171 avg_loss = 3.52522\n",
      "epoch no.1 train no.92050  loss = 3.03908 avg_loss = 3.58374\n",
      "epoch no.1 train no.92060  loss = 4.38443 avg_loss = 3.57419\n",
      "epoch no.1 train no.92070  loss = 3.14728 avg_loss = 3.61376\n",
      "epoch no.1 train no.92080  loss = 5.08995 avg_loss = 3.66243\n",
      "epoch no.1 train no.92090  loss = 3.73822 avg_loss = 3.64211\n",
      "epoch no.1 train no.92100  loss = 4.28310 avg_loss = 3.67451\n",
      "epoch no.1 train no.92110  loss = 2.85692 avg_loss = 3.69340\n",
      "epoch no.1 train no.92120  loss = 5.39217 avg_loss = 3.73028\n",
      "epoch no.1 train no.92130  loss = 1.95706 avg_loss = 3.71947\n",
      "epoch no.1 train no.92140  loss = 2.84696 avg_loss = 3.66044\n",
      "epoch no.1 train no.92150  loss = 2.71995 avg_loss = 3.66751\n",
      "epoch no.1 train no.92160  loss = 1.99400 avg_loss = 3.58664\n",
      "epoch no.1 train no.92170  loss = 3.87157 avg_loss = 3.60116\n",
      "epoch no.1 train no.92180  loss = 3.92012 avg_loss = 3.64242\n",
      "epoch no.1 train no.92190  loss = 3.10631 avg_loss = 3.64569\n",
      "epoch no.1 train no.92200  loss = 2.55363 avg_loss = 3.63227\n",
      "epoch no.1 train no.92210  loss = 3.51410 avg_loss = 3.58135\n",
      "epoch no.1 train no.92220  loss = 2.65029 avg_loss = 3.56228\n",
      "epoch no.1 train no.92230  loss = 2.78375 avg_loss = 3.51731\n",
      "epoch no.1 train no.92240  loss = 3.45432 avg_loss = 3.51807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.92250  loss = 3.70421 avg_loss = 3.49042\n",
      "epoch no.1 train no.92260  loss = 4.24458 avg_loss = 3.47589\n",
      "epoch no.1 train no.92270  loss = 3.42380 avg_loss = 3.58753\n",
      "epoch no.1 train no.92280  loss = 3.94195 avg_loss = 3.62870\n",
      "epoch no.1 train no.92290  loss = 2.56325 avg_loss = 3.64021\n",
      "epoch no.1 train no.92300  loss = 4.51553 avg_loss = 3.61056\n",
      "epoch no.1 train no.92310  loss = 3.13867 avg_loss = 3.56494\n",
      "epoch no.1 train no.92320  loss = 2.92873 avg_loss = 3.53188\n",
      "epoch no.1 train no.92330  loss = 4.90212 avg_loss = 3.56397\n",
      "epoch no.1 train no.92340  loss = 2.43761 avg_loss = 3.56374\n",
      "epoch no.1 train no.92350  loss = 5.29781 avg_loss = 3.60675\n",
      "epoch no.1 train no.92360  loss = 2.32534 avg_loss = 3.59874\n",
      "epoch no.1 train no.92370  loss = 1.81387 avg_loss = 3.55270\n",
      "epoch no.1 train no.92380  loss = 3.60799 avg_loss = 3.54493\n",
      "epoch no.1 train no.92390  loss = 2.18535 avg_loss = 3.55901\n",
      "epoch no.1 train no.92400  loss = 5.47866 avg_loss = 3.58857\n",
      "epoch no.1 train no.92410  loss = 5.09056 avg_loss = 3.65878\n",
      "epoch no.1 train no.92420  loss = 3.07823 avg_loss = 3.61339\n",
      "epoch no.1 train no.92430  loss = 3.91258 avg_loss = 3.60805\n",
      "epoch no.1 train no.92440  loss = 2.09324 avg_loss = 3.56915\n",
      "epoch no.1 train no.92450  loss = 3.07404 avg_loss = 3.59919\n",
      "epoch no.1 train no.92460  loss = 1.90563 avg_loss = 3.57656\n",
      "epoch no.1 train no.92470  loss = 3.67934 avg_loss = 3.54759\n",
      "epoch no.1 train no.92480  loss = 4.64725 avg_loss = 3.59317\n",
      "epoch no.1 train no.92490  loss = 5.53236 avg_loss = 3.58651\n",
      "epoch no.1 train no.92500  loss = 3.39141 avg_loss = 3.65526\n",
      "epoch no.1 train no.92510  loss = 4.44337 avg_loss = 3.61271\n",
      "epoch no.1 train no.92520  loss = 2.65073 avg_loss = 3.57205\n",
      "epoch no.1 train no.92530  loss = 2.93786 avg_loss = 3.54514\n",
      "epoch no.1 train no.92540  loss = 1.83240 avg_loss = 3.50831\n",
      "epoch no.1 train no.92550  loss = 2.17520 avg_loss = 3.48728\n",
      "epoch no.1 train no.92560  loss = 3.69650 avg_loss = 3.52843\n",
      "epoch no.1 train no.92570  loss = 7.40396 avg_loss = 3.56136\n",
      "epoch no.1 train no.92580  loss = 2.00181 avg_loss = 3.54067\n",
      "epoch no.1 train no.92590  loss = 2.19947 avg_loss = 3.54533\n",
      "epoch no.1 train no.92600  loss = 2.96259 avg_loss = 3.54251\n",
      "epoch no.1 train no.92610  loss = 4.31394 avg_loss = 3.59478\n",
      "epoch no.1 train no.92620  loss = 2.58337 avg_loss = 3.58275\n",
      "epoch no.1 train no.92630  loss = 3.84470 avg_loss = 3.59604\n",
      "epoch no.1 train no.92640  loss = 6.02192 avg_loss = 3.64026\n",
      "epoch no.1 train no.92650  loss = 3.93732 avg_loss = 3.68637\n",
      "epoch no.1 train no.92660  loss = 3.85460 avg_loss = 3.67441\n",
      "epoch no.1 train no.92670  loss = 3.95875 avg_loss = 3.62625\n",
      "epoch no.1 train no.92680  loss = 3.73026 avg_loss = 3.63050\n",
      "epoch no.1 train no.92690  loss = 2.68026 avg_loss = 3.63637\n",
      "epoch no.1 train no.92700  loss = 3.62997 avg_loss = 3.64680\n",
      "epoch no.1 train no.92710  loss = 4.89318 avg_loss = 3.64659\n",
      "epoch no.1 train no.92720  loss = 3.46475 avg_loss = 3.65656\n",
      "epoch no.1 train no.92730  loss = 4.51555 avg_loss = 3.69440\n",
      "epoch no.1 train no.92740  loss = 3.86360 avg_loss = 3.63804\n",
      "epoch no.1 train no.92750  loss = 5.43426 avg_loss = 3.61764\n",
      "epoch no.1 train no.92760  loss = 4.00926 avg_loss = 3.59669\n",
      "epoch no.1 train no.92770  loss = 3.83711 avg_loss = 3.56014\n",
      "epoch no.1 train no.92780  loss = 4.74619 avg_loss = 3.62833\n",
      "epoch no.1 train no.92790  loss = 4.35566 avg_loss = 3.61998\n",
      "epoch no.1 train no.92800  loss = 2.86059 avg_loss = 3.59831\n",
      "epoch no.1 train no.92810  loss = 4.06888 avg_loss = 3.57290\n",
      "epoch no.1 train no.92820  loss = 2.71476 avg_loss = 3.61589\n",
      "epoch no.1 train no.92830  loss = 2.48433 avg_loss = 3.65093\n",
      "epoch no.1 train no.92840  loss = 3.04995 avg_loss = 3.68758\n",
      "epoch no.1 train no.92850  loss = 3.46324 avg_loss = 3.66568\n",
      "epoch no.1 train no.92860  loss = 4.32394 avg_loss = 3.69805\n",
      "epoch no.1 train no.92870  loss = 4.78902 avg_loss = 3.70782\n",
      "epoch no.1 train no.92880  loss = 3.80392 avg_loss = 3.71607\n",
      "epoch no.1 train no.92890  loss = 1.75237 avg_loss = 3.65838\n",
      "epoch no.1 train no.92900  loss = 4.96148 avg_loss = 3.67964\n",
      "epoch no.1 train no.92910  loss = 3.07266 avg_loss = 3.68130\n",
      "epoch no.1 train no.92920  loss = 4.60059 avg_loss = 3.66428\n",
      "epoch no.1 train no.92930  loss = 4.13262 avg_loss = 3.68030\n",
      "epoch no.1 train no.92940  loss = 3.63231 avg_loss = 3.69587\n",
      "epoch no.1 train no.92950  loss = 1.43728 avg_loss = 3.59695\n",
      "epoch no.1 train no.92960  loss = 3.39104 avg_loss = 3.65407\n",
      "epoch no.1 train no.92970  loss = 4.86611 avg_loss = 3.68854\n",
      "epoch no.1 train no.92980  loss = 3.35986 avg_loss = 3.68518\n",
      "epoch no.1 train no.92990  loss = 4.49912 avg_loss = 3.70635\n",
      "epoch no.1 train no.93000  loss = 4.62423 avg_loss = 3.69105\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '적인', '</s>']\n",
      "여름밤에 듣기 좋은 감성팝</s>\n",
      "epoch no.1 train no.93010  loss = 2.44524 avg_loss = 3.65477\n",
      "epoch no.1 train no.93020  loss = 3.72508 avg_loss = 3.65695\n",
      "epoch no.1 train no.93030  loss = 2.70232 avg_loss = 3.64963\n",
      "epoch no.1 train no.93040  loss = 2.93799 avg_loss = 3.62306\n",
      "epoch no.1 train no.93050  loss = 4.04110 avg_loss = 3.63139\n",
      "epoch no.1 train no.93060  loss = 5.23009 avg_loss = 3.66252\n",
      "epoch no.1 train no.93070  loss = 3.26311 avg_loss = 3.64584\n",
      "epoch no.1 train no.93080  loss = 2.85253 avg_loss = 3.57854\n",
      "epoch no.1 train no.93090  loss = 3.63425 avg_loss = 3.60973\n",
      "epoch no.1 train no.93100  loss = 5.06078 avg_loss = 3.58962\n",
      "epoch no.1 train no.93110  loss = 3.39633 avg_loss = 3.59361\n",
      "epoch no.1 train no.93120  loss = 3.41889 avg_loss = 3.63192\n",
      "epoch no.1 train no.93130  loss = 3.89737 avg_loss = 3.59420\n",
      "epoch no.1 train no.93140  loss = 3.77906 avg_loss = 3.63259\n",
      "epoch no.1 train no.93150  loss = 4.66793 avg_loss = 3.58213\n",
      "epoch no.1 train no.93160  loss = 3.95394 avg_loss = 3.63757\n",
      "epoch no.1 train no.93170  loss = 3.25426 avg_loss = 3.61013\n",
      "epoch no.1 train no.93180  loss = 3.91956 avg_loss = 3.62767\n",
      "epoch no.1 train no.93190  loss = 2.14616 avg_loss = 3.58964\n",
      "epoch no.1 train no.93200  loss = 4.78781 avg_loss = 3.60296\n",
      "epoch no.1 train no.93210  loss = 3.23769 avg_loss = 3.58830\n",
      "epoch no.1 train no.93220  loss = 2.45212 avg_loss = 3.59309\n",
      "epoch no.1 train no.93230  loss = 3.36833 avg_loss = 3.65106\n",
      "epoch no.1 train no.93240  loss = 3.66509 avg_loss = 3.62516\n",
      "epoch no.1 train no.93250  loss = 3.04649 avg_loss = 3.65737\n",
      "epoch no.1 train no.93260  loss = 5.89710 avg_loss = 3.67105\n",
      "epoch no.1 train no.93270  loss = 3.32691 avg_loss = 3.64438\n",
      "epoch no.1 train no.93280  loss = 3.15358 avg_loss = 3.68597\n",
      "epoch no.1 train no.93290  loss = 2.02600 avg_loss = 3.71522\n",
      "epoch no.1 train no.93300  loss = 2.15356 avg_loss = 3.67274\n",
      "epoch no.1 train no.93310  loss = 2.78310 avg_loss = 3.64071\n",
      "epoch no.1 train no.93320  loss = 3.29790 avg_loss = 3.71787\n",
      "epoch no.1 train no.93330  loss = 3.50007 avg_loss = 3.75097\n",
      "epoch no.1 train no.93340  loss = 4.53893 avg_loss = 3.72214\n",
      "epoch no.1 train no.93350  loss = 3.20872 avg_loss = 3.71255\n",
      "epoch no.1 train no.93360  loss = 4.02008 avg_loss = 3.65339\n",
      "epoch no.1 train no.93370  loss = 3.26715 avg_loss = 3.60473\n",
      "epoch no.1 train no.93380  loss = 4.67389 avg_loss = 3.59898\n",
      "epoch no.1 train no.93390  loss = 4.00069 avg_loss = 3.62252\n",
      "epoch no.1 train no.93400  loss = 3.47898 avg_loss = 3.58566\n",
      "epoch no.1 train no.93410  loss = 3.98445 avg_loss = 3.60239\n",
      "epoch no.1 train no.93420  loss = 2.67301 avg_loss = 3.57600\n",
      "epoch no.1 train no.93430  loss = 5.62979 avg_loss = 3.60237\n",
      "epoch no.1 train no.93440  loss = 4.87734 avg_loss = 3.59061\n",
      "epoch no.1 train no.93450  loss = 2.52224 avg_loss = 3.62402\n",
      "epoch no.1 train no.93460  loss = 4.55619 avg_loss = 3.60565\n",
      "epoch no.1 train no.93470  loss = 2.50314 avg_loss = 3.56037\n",
      "epoch no.1 train no.93480  loss = 4.22102 avg_loss = 3.68303\n",
      "epoch no.1 train no.93490  loss = 2.52448 avg_loss = 3.68834\n",
      "epoch no.1 train no.93500  loss = 2.27978 avg_loss = 3.65526\n",
      "epoch no.1 train no.93510  loss = 4.07400 avg_loss = 3.68529\n",
      "epoch no.1 train no.93520  loss = 2.47157 avg_loss = 3.65996\n",
      "epoch no.1 train no.93530  loss = 6.65772 avg_loss = 3.65041\n",
      "epoch no.1 train no.93540  loss = 4.13068 avg_loss = 3.62958\n",
      "epoch no.1 train no.93550  loss = 3.44401 avg_loss = 3.66184\n",
      "epoch no.1 train no.93560  loss = 4.64785 avg_loss = 3.69268\n",
      "epoch no.1 train no.93570  loss = 5.94650 avg_loss = 3.65197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.93580  loss = 4.01232 avg_loss = 3.60690\n",
      "epoch no.1 train no.93590  loss = 1.94895 avg_loss = 3.56440\n",
      "epoch no.1 train no.93600  loss = 3.62856 avg_loss = 3.56194\n",
      "epoch no.1 train no.93610  loss = 5.04144 avg_loss = 3.60436\n",
      "epoch no.1 train no.93620  loss = 4.04654 avg_loss = 3.57642\n",
      "epoch no.1 train no.93630  loss = 3.45376 avg_loss = 3.62424\n",
      "epoch no.1 train no.93640  loss = 4.88828 avg_loss = 3.72274\n",
      "epoch no.1 train no.93650  loss = 4.64848 avg_loss = 3.75759\n",
      "epoch no.1 train no.93660  loss = 3.77462 avg_loss = 3.70677\n",
      "epoch no.1 train no.93670  loss = 4.16305 avg_loss = 3.74715\n",
      "epoch no.1 train no.93680  loss = 5.00736 avg_loss = 3.71624\n",
      "epoch no.1 train no.93690  loss = 2.80701 avg_loss = 3.70760\n",
      "epoch no.1 train no.93700  loss = 4.92389 avg_loss = 3.73209\n",
      "epoch no.1 train no.93710  loss = 2.75232 avg_loss = 3.72820\n",
      "epoch no.1 train no.93720  loss = 2.78312 avg_loss = 3.70862\n",
      "epoch no.1 train no.93730  loss = 3.28593 avg_loss = 3.71508\n",
      "epoch no.1 train no.93740  loss = 5.28350 avg_loss = 3.76217\n",
      "epoch no.1 train no.93750  loss = 4.53362 avg_loss = 3.71337\n",
      "epoch no.1 train no.93760  loss = 3.33303 avg_loss = 3.73542\n",
      "epoch no.1 train no.93770  loss = 5.24436 avg_loss = 3.77570\n",
      "epoch no.1 train no.93780  loss = 4.37156 avg_loss = 3.76950\n",
      "epoch no.1 train no.93790  loss = 4.05203 avg_loss = 3.76689\n",
      "epoch no.1 train no.93800  loss = 2.86563 avg_loss = 3.72659\n",
      "epoch no.1 train no.93810  loss = 2.24629 avg_loss = 3.76322\n",
      "epoch no.1 train no.93820  loss = 2.78743 avg_loss = 3.71909\n",
      "epoch no.1 train no.93830  loss = 3.70205 avg_loss = 3.69600\n",
      "epoch no.1 train no.93840  loss = 2.18479 avg_loss = 3.70213\n",
      "epoch no.1 train no.93850  loss = 3.41507 avg_loss = 3.68539\n",
      "epoch no.1 train no.93860  loss = 5.33182 avg_loss = 3.70918\n",
      "epoch no.1 train no.93870  loss = 3.41323 avg_loss = 3.71881\n",
      "epoch no.1 train no.93880  loss = 2.78594 avg_loss = 3.73822\n",
      "epoch no.1 train no.93890  loss = 2.80264 avg_loss = 3.73309\n",
      "epoch no.1 train no.93900  loss = 2.69379 avg_loss = 3.71663\n",
      "epoch no.1 train no.93910  loss = 3.54408 avg_loss = 3.72899\n",
      "epoch no.1 train no.93920  loss = 5.41927 avg_loss = 3.70266\n",
      "epoch no.1 train no.93930  loss = 3.28890 avg_loss = 3.77180\n",
      "epoch no.1 train no.93940  loss = 2.55973 avg_loss = 3.81993\n",
      "epoch no.1 train no.93950  loss = 3.48899 avg_loss = 3.84730\n",
      "epoch no.1 train no.93960  loss = 2.61870 avg_loss = 3.82087\n",
      "epoch no.1 train no.93970  loss = 2.04432 avg_loss = 3.73735\n",
      "epoch no.1 train no.93980  loss = 3.26135 avg_loss = 3.71011\n",
      "epoch no.1 train no.93990  loss = 5.56384 avg_loss = 3.73085\n",
      "epoch no.1 train no.94000  loss = 3.05135 avg_loss = 3.68845\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '의', '에', '▁위한', '▁음악', 'op', '</s>']\n",
      "여름밤 드라이브를 위한 pop</s>\n",
      "epoch no.1 train no.94010  loss = 3.24748 avg_loss = 3.72784\n",
      "epoch no.1 train no.94020  loss = 2.48143 avg_loss = 3.64743\n",
      "epoch no.1 train no.94030  loss = 4.00999 avg_loss = 3.65332\n",
      "epoch no.1 train no.94040  loss = 3.16562 avg_loss = 3.64237\n",
      "epoch no.1 train no.94050  loss = 5.00349 avg_loss = 3.65678\n",
      "epoch no.1 train no.94060  loss = 3.02227 avg_loss = 3.65964\n",
      "epoch no.1 train no.94070  loss = 2.15339 avg_loss = 3.60839\n",
      "epoch no.1 train no.94080  loss = 2.74516 avg_loss = 3.63059\n",
      "epoch no.1 train no.94090  loss = 4.52199 avg_loss = 3.69820\n",
      "epoch no.1 train no.94100  loss = 4.94437 avg_loss = 3.71303\n",
      "epoch no.1 train no.94110  loss = 3.43443 avg_loss = 3.68839\n",
      "epoch no.1 train no.94120  loss = 3.11954 avg_loss = 3.66277\n",
      "epoch no.1 train no.94130  loss = 2.65662 avg_loss = 3.74381\n",
      "epoch no.1 train no.94140  loss = 4.67026 avg_loss = 3.74260\n",
      "epoch no.1 train no.94150  loss = 4.70890 avg_loss = 3.72840\n",
      "epoch no.1 train no.94160  loss = 3.47208 avg_loss = 3.68290\n",
      "epoch no.1 train no.94170  loss = 3.37642 avg_loss = 3.65719\n",
      "epoch no.1 train no.94180  loss = 2.11870 avg_loss = 3.66200\n",
      "epoch no.1 train no.94190  loss = 3.01008 avg_loss = 3.68098\n",
      "epoch no.1 train no.94200  loss = 6.19276 avg_loss = 3.69794\n",
      "epoch no.1 train no.94210  loss = 3.84720 avg_loss = 3.68550\n",
      "epoch no.1 train no.94220  loss = 2.40679 avg_loss = 3.67646\n",
      "epoch no.1 train no.94230  loss = 1.94740 avg_loss = 3.70794\n",
      "epoch no.1 train no.94240  loss = 4.33578 avg_loss = 3.71836\n",
      "epoch no.1 train no.94250  loss = 2.80578 avg_loss = 3.70862\n",
      "epoch no.1 train no.94260  loss = 2.99559 avg_loss = 3.66178\n",
      "epoch no.1 train no.94270  loss = 4.38155 avg_loss = 3.70536\n",
      "epoch no.1 train no.94280  loss = 3.11666 avg_loss = 3.63670\n",
      "epoch no.1 train no.94290  loss = 5.43735 avg_loss = 3.67039\n",
      "epoch no.1 train no.94300  loss = 5.15020 avg_loss = 3.76593\n",
      "epoch no.1 train no.94310  loss = 3.71193 avg_loss = 3.76289\n",
      "epoch no.1 train no.94320  loss = 4.26481 avg_loss = 3.74250\n",
      "epoch no.1 train no.94330  loss = 3.10683 avg_loss = 3.78630\n",
      "epoch no.1 train no.94340  loss = 3.08428 avg_loss = 3.74346\n",
      "epoch no.1 train no.94350  loss = 3.25043 avg_loss = 3.73278\n",
      "epoch no.1 train no.94360  loss = 5.50522 avg_loss = 3.67935\n",
      "epoch no.1 train no.94370  loss = 2.75811 avg_loss = 3.61158\n",
      "epoch no.1 train no.94380  loss = 2.60875 avg_loss = 3.65846\n",
      "epoch no.1 train no.94390  loss = 4.00660 avg_loss = 3.69213\n",
      "epoch no.1 train no.94400  loss = 2.92036 avg_loss = 3.67041\n",
      "epoch no.1 train no.94410  loss = 4.40018 avg_loss = 3.63690\n",
      "epoch no.1 train no.94420  loss = 3.44129 avg_loss = 3.62985\n",
      "epoch no.1 train no.94430  loss = 5.44080 avg_loss = 3.66003\n",
      "epoch no.1 train no.94440  loss = 6.23619 avg_loss = 3.65412\n",
      "epoch no.1 train no.94450  loss = 2.62065 avg_loss = 3.64061\n",
      "epoch no.1 train no.94460  loss = 3.26166 avg_loss = 3.66969\n",
      "epoch no.1 train no.94470  loss = 4.89701 avg_loss = 3.69966\n",
      "epoch no.1 train no.94480  loss = 5.76614 avg_loss = 3.70955\n",
      "epoch no.1 train no.94490  loss = 1.80299 avg_loss = 3.78716\n",
      "epoch no.1 train no.94500  loss = 3.41585 avg_loss = 3.73877\n",
      "epoch no.1 train no.94510  loss = 3.78911 avg_loss = 3.75816\n",
      "epoch no.1 train no.94520  loss = 3.71048 avg_loss = 3.71290\n",
      "epoch no.1 train no.94530  loss = 3.58162 avg_loss = 3.70676\n",
      "epoch no.1 train no.94540  loss = 1.84443 avg_loss = 3.68320\n",
      "epoch no.1 train no.94550  loss = 2.73862 avg_loss = 3.68399\n",
      "epoch no.1 train no.94560  loss = 4.28588 avg_loss = 3.74957\n",
      "epoch no.1 train no.94570  loss = 2.99632 avg_loss = 3.74964\n",
      "epoch no.1 train no.94580  loss = 2.94201 avg_loss = 3.76822\n",
      "epoch no.1 train no.94590  loss = 5.39086 avg_loss = 3.76143\n",
      "epoch no.1 train no.94600  loss = 4.25554 avg_loss = 3.77607\n",
      "epoch no.1 train no.94610  loss = 3.86031 avg_loss = 3.78605\n",
      "epoch no.1 train no.94620  loss = 3.93557 avg_loss = 3.80534\n",
      "epoch no.1 train no.94630  loss = 2.95378 avg_loss = 3.82637\n",
      "epoch no.1 train no.94640  loss = 5.74475 avg_loss = 3.82649\n",
      "epoch no.1 train no.94650  loss = 2.21086 avg_loss = 3.80713\n",
      "epoch no.1 train no.94660  loss = 2.35337 avg_loss = 3.76650\n",
      "epoch no.1 train no.94670  loss = 3.86661 avg_loss = 3.82715\n",
      "epoch no.1 train no.94680  loss = 3.06405 avg_loss = 3.76536\n",
      "epoch no.1 train no.94690  loss = 3.09772 avg_loss = 3.70525\n",
      "epoch no.1 train no.94700  loss = 2.10944 avg_loss = 3.66571\n",
      "epoch no.1 train no.94710  loss = 2.82778 avg_loss = 3.58835\n",
      "epoch no.1 train no.94720  loss = 3.73721 avg_loss = 3.60368\n",
      "epoch no.1 train no.94730  loss = 4.12057 avg_loss = 3.65895\n",
      "epoch no.1 train no.94740  loss = 3.37476 avg_loss = 3.65741\n",
      "epoch no.1 train no.94750  loss = 3.41203 avg_loss = 3.63706\n",
      "epoch no.1 train no.94760  loss = 3.49361 avg_loss = 3.62586\n",
      "epoch no.1 train no.94770  loss = 5.07679 avg_loss = 3.65567\n",
      "epoch no.1 train no.94780  loss = 2.64185 avg_loss = 3.64432\n",
      "epoch no.1 train no.94790  loss = 2.24373 avg_loss = 3.61227\n",
      "epoch no.1 train no.94800  loss = 4.45037 avg_loss = 3.65318\n",
      "epoch no.1 train no.94810  loss = 3.55394 avg_loss = 3.62633\n",
      "epoch no.1 train no.94820  loss = 2.76871 avg_loss = 3.61565\n",
      "epoch no.1 train no.94830  loss = 4.19086 avg_loss = 3.60225\n",
      "epoch no.1 train no.94840  loss = 3.36813 avg_loss = 3.57122\n",
      "epoch no.1 train no.94850  loss = 3.64934 avg_loss = 3.55774\n",
      "epoch no.1 train no.94860  loss = 2.37905 avg_loss = 3.58916\n",
      "epoch no.1 train no.94870  loss = 2.62009 avg_loss = 3.66279\n",
      "epoch no.1 train no.94880  loss = 3.48679 avg_loss = 3.66574\n",
      "epoch no.1 train no.94890  loss = 2.50913 avg_loss = 3.66276\n",
      "epoch no.1 train no.94900  loss = 2.58563 avg_loss = 3.71725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.94910  loss = 2.41761 avg_loss = 3.72515\n",
      "epoch no.1 train no.94920  loss = 3.52472 avg_loss = 3.69954\n",
      "epoch no.1 train no.94930  loss = 3.32772 avg_loss = 3.73991\n",
      "epoch no.1 train no.94940  loss = 2.40366 avg_loss = 3.73162\n",
      "epoch no.1 train no.94950  loss = 4.28764 avg_loss = 3.74114\n",
      "epoch no.1 train no.94960  loss = 2.01266 avg_loss = 3.76124\n",
      "epoch no.1 train no.94970  loss = 3.33400 avg_loss = 3.73417\n",
      "epoch no.1 train no.94980  loss = 3.14860 avg_loss = 3.71302\n",
      "epoch no.1 train no.94990  loss = 4.80643 avg_loss = 3.71169\n",
      "epoch no.1 train no.95000  loss = 2.91068 avg_loss = 3.71546\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣기', '▁감성', '힙', '드', '</s>']\n",
      "여름밤에 듣는 감성 발라드</s>\n",
      "epoch no.1 train no.95010  loss = 3.35957 avg_loss = 3.64892\n",
      "epoch no.1 train no.95020  loss = 2.41313 avg_loss = 3.61075\n",
      "epoch no.1 train no.95030  loss = 3.82896 avg_loss = 3.57241\n",
      "epoch no.1 train no.95040  loss = 3.86462 avg_loss = 3.59941\n",
      "epoch no.1 train no.95050  loss = 4.85088 avg_loss = 3.60229\n",
      "epoch no.1 train no.95060  loss = 4.06346 avg_loss = 3.61209\n",
      "epoch no.1 train no.95070  loss = 4.32364 avg_loss = 3.66324\n",
      "epoch no.1 train no.95080  loss = 3.77922 avg_loss = 3.62065\n",
      "epoch no.1 train no.95090  loss = 2.01488 avg_loss = 3.61984\n",
      "epoch no.1 train no.95100  loss = 4.67425 avg_loss = 3.63238\n",
      "epoch no.1 train no.95110  loss = 4.21982 avg_loss = 3.62066\n",
      "epoch no.1 train no.95120  loss = 4.18180 avg_loss = 3.60630\n",
      "epoch no.1 train no.95130  loss = 3.65533 avg_loss = 3.62793\n",
      "epoch no.1 train no.95140  loss = 3.97910 avg_loss = 3.65100\n",
      "epoch no.1 train no.95150  loss = 4.22834 avg_loss = 3.65812\n",
      "epoch no.1 train no.95160  loss = 5.20465 avg_loss = 3.68125\n",
      "epoch no.1 train no.95170  loss = 3.38627 avg_loss = 3.68974\n",
      "epoch no.1 train no.95180  loss = 3.60804 avg_loss = 3.63493\n",
      "epoch no.1 train no.95190  loss = 3.73492 avg_loss = 3.65367\n",
      "epoch no.1 train no.95200  loss = 2.13150 avg_loss = 3.69056\n",
      "epoch no.1 train no.95210  loss = 2.71636 avg_loss = 3.64211\n",
      "epoch no.1 train no.95220  loss = 3.03400 avg_loss = 3.65358\n",
      "epoch no.1 train no.95230  loss = 4.35629 avg_loss = 3.66977\n",
      "epoch no.1 train no.95240  loss = 1.94922 avg_loss = 3.63974\n",
      "epoch no.1 train no.95250  loss = 2.64191 avg_loss = 3.65556\n",
      "epoch no.1 train no.95260  loss = 3.75211 avg_loss = 3.68544\n",
      "epoch no.1 train no.95270  loss = 3.95222 avg_loss = 3.65169\n",
      "epoch no.1 train no.95280  loss = 2.53409 avg_loss = 3.68570\n",
      "epoch no.1 train no.95290  loss = 2.42945 avg_loss = 3.62885\n",
      "epoch no.1 train no.95300  loss = 3.99751 avg_loss = 3.64513\n",
      "epoch no.1 train no.95310  loss = 4.72630 avg_loss = 3.61347\n",
      "epoch no.1 train no.95320  loss = 3.22250 avg_loss = 3.63786\n",
      "epoch no.1 train no.95330  loss = 3.02717 avg_loss = 3.61061\n",
      "epoch no.1 train no.95340  loss = 5.05754 avg_loss = 3.68498\n",
      "epoch no.1 train no.95350  loss = 3.81422 avg_loss = 3.62803\n",
      "epoch no.1 train no.95360  loss = 5.73768 avg_loss = 3.65493\n",
      "epoch no.1 train no.95370  loss = 3.18870 avg_loss = 3.64053\n",
      "epoch no.1 train no.95380  loss = 2.00847 avg_loss = 3.58319\n",
      "epoch no.1 train no.95390  loss = 2.26832 avg_loss = 3.50154\n",
      "epoch no.1 train no.95400  loss = 3.47795 avg_loss = 3.52224\n",
      "epoch no.1 train no.95410  loss = 3.69739 avg_loss = 3.53716\n",
      "epoch no.1 train no.95420  loss = 6.14189 avg_loss = 3.57327\n",
      "epoch no.1 train no.95430  loss = 4.07613 avg_loss = 3.60329\n",
      "epoch no.1 train no.95440  loss = 4.50575 avg_loss = 3.60745\n",
      "epoch no.1 train no.95450  loss = 4.87238 avg_loss = 3.63540\n",
      "epoch no.1 train no.95460  loss = 2.81849 avg_loss = 3.67304\n",
      "epoch no.1 train no.95470  loss = 4.41185 avg_loss = 3.68229\n",
      "epoch no.1 train no.95480  loss = 3.15253 avg_loss = 3.69171\n",
      "epoch no.1 train no.95490  loss = 4.06652 avg_loss = 3.67681\n",
      "epoch no.1 train no.95500  loss = 2.04888 avg_loss = 3.72076\n",
      "epoch no.1 train no.95510  loss = 2.56483 avg_loss = 3.71568\n",
      "epoch no.1 train no.95520  loss = 2.52706 avg_loss = 3.72002\n",
      "epoch no.1 train no.95530  loss = 2.39256 avg_loss = 3.66649\n",
      "epoch no.1 train no.95540  loss = 2.37226 avg_loss = 3.62439\n",
      "epoch no.1 train no.95550  loss = 5.26222 avg_loss = 3.57430\n",
      "epoch no.1 train no.95560  loss = 2.46957 avg_loss = 3.65454\n",
      "epoch no.1 train no.95570  loss = 6.00212 avg_loss = 3.70168\n",
      "epoch no.1 train no.95580  loss = 3.96282 avg_loss = 3.68488\n",
      "epoch no.1 train no.95590  loss = 7.06451 avg_loss = 3.72607\n",
      "epoch no.1 train no.95600  loss = 4.26854 avg_loss = 3.70216\n",
      "epoch no.1 train no.95610  loss = 3.34851 avg_loss = 3.68654\n",
      "epoch no.1 train no.95620  loss = 2.35012 avg_loss = 3.61647\n",
      "epoch no.1 train no.95630  loss = 3.26618 avg_loss = 3.59585\n",
      "epoch no.1 train no.95640  loss = 3.30415 avg_loss = 3.58206\n",
      "epoch no.1 train no.95650  loss = 2.71729 avg_loss = 3.60419\n",
      "epoch no.1 train no.95660  loss = 5.04506 avg_loss = 3.65302\n",
      "epoch no.1 train no.95670  loss = 3.13044 avg_loss = 3.63597\n",
      "epoch no.1 train no.95680  loss = 2.65326 avg_loss = 3.65061\n",
      "epoch no.1 train no.95690  loss = 3.36619 avg_loss = 3.69603\n",
      "epoch no.1 train no.95700  loss = 3.07846 avg_loss = 3.70613\n",
      "epoch no.1 train no.95710  loss = 3.28742 avg_loss = 3.67016\n",
      "epoch no.1 train no.95720  loss = 5.20197 avg_loss = 3.68649\n",
      "epoch no.1 train no.95730  loss = 2.29420 avg_loss = 3.69757\n",
      "epoch no.1 train no.95740  loss = 2.62664 avg_loss = 3.70212\n",
      "epoch no.1 train no.95750  loss = 5.66741 avg_loss = 3.72389\n",
      "epoch no.1 train no.95760  loss = 3.83441 avg_loss = 3.72263\n",
      "epoch no.1 train no.95770  loss = 2.37474 avg_loss = 3.66798\n",
      "epoch no.1 train no.95780  loss = 2.32049 avg_loss = 3.64726\n",
      "epoch no.1 train no.95790  loss = 4.09209 avg_loss = 3.67891\n",
      "epoch no.1 train no.95800  loss = 3.84703 avg_loss = 3.66826\n",
      "epoch no.1 train no.95810  loss = 5.60922 avg_loss = 3.61837\n",
      "epoch no.1 train no.95820  loss = 1.83748 avg_loss = 3.57343\n",
      "epoch no.1 train no.95830  loss = 5.63381 avg_loss = 3.65218\n",
      "epoch no.1 train no.95840  loss = 4.17959 avg_loss = 3.64851\n",
      "epoch no.1 train no.95850  loss = 3.50807 avg_loss = 3.66407\n",
      "epoch no.1 train no.95860  loss = 3.46722 avg_loss = 3.62166\n",
      "epoch no.1 train no.95870  loss = 2.21255 avg_loss = 3.63275\n",
      "epoch no.1 train no.95880  loss = 2.38909 avg_loss = 3.55447\n",
      "epoch no.1 train no.95890  loss = 1.92489 avg_loss = 3.55394\n",
      "epoch no.1 train no.95900  loss = 4.04289 avg_loss = 3.56433\n",
      "epoch no.1 train no.95910  loss = 4.16412 avg_loss = 3.58252\n",
      "epoch no.1 train no.95920  loss = 3.26718 avg_loss = 3.59887\n",
      "epoch no.1 train no.95930  loss = 3.01649 avg_loss = 3.57812\n",
      "epoch no.1 train no.95940  loss = 4.49583 avg_loss = 3.60945\n",
      "epoch no.1 train no.95950  loss = 2.91803 avg_loss = 3.59442\n",
      "epoch no.1 train no.95960  loss = 3.91335 avg_loss = 3.62283\n",
      "epoch no.1 train no.95970  loss = 3.24441 avg_loss = 3.62856\n",
      "epoch no.1 train no.95980  loss = 5.08002 avg_loss = 3.62618\n",
      "epoch no.1 train no.95990  loss = 5.49481 avg_loss = 3.67309\n",
      "epoch no.1 train no.96000  loss = 2.42173 avg_loss = 3.64788\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '▁역시', '감', '▁넘치는', '▁걸그룹', '송', '</s>']\n",
      "여름엔 청량미 넘치는 팝송</s>\n",
      "epoch no.1 train no.96010  loss = 4.70279 avg_loss = 3.57803\n",
      "epoch no.1 train no.96020  loss = 3.89955 avg_loss = 3.61548\n",
      "epoch no.1 train no.96030  loss = 3.61538 avg_loss = 3.59001\n",
      "epoch no.1 train no.96040  loss = 4.79802 avg_loss = 3.57115\n",
      "epoch no.1 train no.96050  loss = 6.00402 avg_loss = 3.56688\n",
      "epoch no.1 train no.96060  loss = 1.66125 avg_loss = 3.58500\n",
      "epoch no.1 train no.96070  loss = 3.16064 avg_loss = 3.57040\n",
      "epoch no.1 train no.96080  loss = 3.23661 avg_loss = 3.55105\n",
      "epoch no.1 train no.96090  loss = 2.84509 avg_loss = 3.52377\n",
      "epoch no.1 train no.96100  loss = 2.40413 avg_loss = 3.55284\n",
      "epoch no.1 train no.96110  loss = 2.53508 avg_loss = 3.61785\n",
      "epoch no.1 train no.96120  loss = 2.27340 avg_loss = 3.60833\n",
      "epoch no.1 train no.96130  loss = 2.42289 avg_loss = 3.54432\n",
      "epoch no.1 train no.96140  loss = 4.61076 avg_loss = 3.54619\n",
      "epoch no.1 train no.96150  loss = 5.14133 avg_loss = 3.58342\n",
      "epoch no.1 train no.96160  loss = 2.72166 avg_loss = 3.56911\n",
      "epoch no.1 train no.96170  loss = 4.55532 avg_loss = 3.62088\n",
      "epoch no.1 train no.96180  loss = 3.95570 avg_loss = 3.60280\n",
      "epoch no.1 train no.96190  loss = 4.60344 avg_loss = 3.59545\n",
      "epoch no.1 train no.96200  loss = 3.64138 avg_loss = 3.60248\n",
      "epoch no.1 train no.96210  loss = 4.59676 avg_loss = 3.66494\n",
      "epoch no.1 train no.96220  loss = 2.34659 avg_loss = 3.64504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.96230  loss = 4.49851 avg_loss = 3.64925\n",
      "epoch no.1 train no.96240  loss = 5.53222 avg_loss = 3.67382\n",
      "epoch no.1 train no.96250  loss = 3.35697 avg_loss = 3.70740\n",
      "epoch no.1 train no.96260  loss = 3.41082 avg_loss = 3.73419\n",
      "epoch no.1 train no.96270  loss = 6.80489 avg_loss = 3.76075\n",
      "epoch no.1 train no.96280  loss = 3.58784 avg_loss = 3.71311\n",
      "epoch no.1 train no.96290  loss = 3.36642 avg_loss = 3.69120\n",
      "epoch no.1 train no.96300  loss = 4.17757 avg_loss = 3.66713\n",
      "epoch no.1 train no.96310  loss = 2.92468 avg_loss = 3.67061\n",
      "epoch no.1 train no.96320  loss = 2.43885 avg_loss = 3.65035\n",
      "epoch no.1 train no.96330  loss = 5.61332 avg_loss = 3.65697\n",
      "epoch no.1 train no.96340  loss = 2.47206 avg_loss = 3.65836\n",
      "epoch no.1 train no.96350  loss = 3.95153 avg_loss = 3.59592\n",
      "epoch no.1 train no.96360  loss = 4.05982 avg_loss = 3.62420\n",
      "epoch no.1 train no.96370  loss = 2.77166 avg_loss = 3.58287\n",
      "epoch no.1 train no.96380  loss = 3.69239 avg_loss = 3.59547\n",
      "epoch no.1 train no.96390  loss = 2.93724 avg_loss = 3.63907\n",
      "epoch no.1 train no.96400  loss = 2.82878 avg_loss = 3.63955\n",
      "epoch no.1 train no.96410  loss = 4.35749 avg_loss = 3.66654\n",
      "epoch no.1 train no.96420  loss = 2.88369 avg_loss = 3.65446\n",
      "epoch no.1 train no.96430  loss = 3.67984 avg_loss = 3.61694\n",
      "epoch no.1 train no.96440  loss = 3.82045 avg_loss = 3.61990\n",
      "epoch no.1 train no.96450  loss = 3.90898 avg_loss = 3.63302\n",
      "epoch no.1 train no.96460  loss = 2.80671 avg_loss = 3.69981\n",
      "epoch no.1 train no.96470  loss = 3.35458 avg_loss = 3.65591\n",
      "epoch no.1 train no.96480  loss = 6.85481 avg_loss = 3.74530\n",
      "epoch no.1 train no.96490  loss = 5.67380 avg_loss = 3.73404\n",
      "epoch no.1 train no.96500  loss = 3.56905 avg_loss = 3.72426\n",
      "epoch no.1 train no.96510  loss = 3.02559 avg_loss = 3.68646\n",
      "epoch no.1 train no.96520  loss = 3.51003 avg_loss = 3.74100\n",
      "epoch no.1 train no.96530  loss = 3.11705 avg_loss = 3.70318\n",
      "epoch no.1 train no.96540  loss = 4.23683 avg_loss = 3.66227\n",
      "epoch no.1 train no.96550  loss = 6.09450 avg_loss = 3.65993\n",
      "epoch no.1 train no.96560  loss = 2.22291 avg_loss = 3.62778\n",
      "epoch no.1 train no.96570  loss = 1.90616 avg_loss = 3.63431\n",
      "epoch no.1 train no.96580  loss = 2.20165 avg_loss = 3.62514\n",
      "epoch no.1 train no.96590  loss = 3.73236 avg_loss = 3.63240\n",
      "epoch no.1 train no.96600  loss = 3.97498 avg_loss = 3.63574\n",
      "epoch no.1 train no.96610  loss = 4.36023 avg_loss = 3.65826\n",
      "epoch no.1 train no.96620  loss = 3.84786 avg_loss = 3.71097\n",
      "epoch no.1 train no.96630  loss = 2.33410 avg_loss = 3.67985\n",
      "epoch no.1 train no.96640  loss = 6.02889 avg_loss = 3.65386\n",
      "epoch no.1 train no.96650  loss = 5.24804 avg_loss = 3.66790\n",
      "epoch no.1 train no.96660  loss = 3.02105 avg_loss = 3.63413\n",
      "epoch no.1 train no.96670  loss = 1.77680 avg_loss = 3.62826\n",
      "epoch no.1 train no.96680  loss = 2.48082 avg_loss = 3.65972\n",
      "epoch no.1 train no.96690  loss = 4.35459 avg_loss = 3.66407\n",
      "epoch no.1 train no.96700  loss = 5.33494 avg_loss = 3.67521\n",
      "epoch no.1 train no.96710  loss = 2.81377 avg_loss = 3.65192\n",
      "epoch no.1 train no.96720  loss = 4.63588 avg_loss = 3.64194\n",
      "epoch no.1 train no.96730  loss = 4.84496 avg_loss = 3.70448\n",
      "epoch no.1 train no.96740  loss = 5.87048 avg_loss = 3.70935\n",
      "epoch no.1 train no.96750  loss = 3.51717 avg_loss = 3.68347\n",
      "epoch no.1 train no.96760  loss = 3.00274 avg_loss = 3.68691\n",
      "epoch no.1 train no.96770  loss = 3.85089 avg_loss = 3.63522\n",
      "epoch no.1 train no.96780  loss = 4.10873 avg_loss = 3.60802\n",
      "epoch no.1 train no.96790  loss = 3.84811 avg_loss = 3.61924\n",
      "epoch no.1 train no.96800  loss = 6.58006 avg_loss = 3.62280\n",
      "epoch no.1 train no.96810  loss = 5.13658 avg_loss = 3.62689\n",
      "epoch no.1 train no.96820  loss = 3.28849 avg_loss = 3.62386\n",
      "epoch no.1 train no.96830  loss = 5.98918 avg_loss = 3.64638\n",
      "epoch no.1 train no.96840  loss = 3.55617 avg_loss = 3.68233\n",
      "epoch no.1 train no.96850  loss = 3.71857 avg_loss = 3.67749\n",
      "epoch no.1 train no.96860  loss = 2.95843 avg_loss = 3.71234\n",
      "epoch no.1 train no.96870  loss = 2.42059 avg_loss = 3.67436\n",
      "epoch no.1 train no.96880  loss = 4.91580 avg_loss = 3.68550\n",
      "epoch no.1 train no.96890  loss = 4.46103 avg_loss = 3.67776\n",
      "epoch no.1 train no.96900  loss = 3.18644 avg_loss = 3.71651\n",
      "epoch no.1 train no.96910  loss = 2.30645 avg_loss = 3.76958\n",
      "epoch no.1 train no.96920  loss = 3.74889 avg_loss = 3.77123\n",
      "epoch no.1 train no.96930  loss = 3.37370 avg_loss = 3.72225\n",
      "epoch no.1 train no.96940  loss = 6.22324 avg_loss = 3.70613\n",
      "epoch no.1 train no.96950  loss = 2.62386 avg_loss = 3.67939\n",
      "epoch no.1 train no.96960  loss = 3.84096 avg_loss = 3.62761\n",
      "epoch no.1 train no.96970  loss = 3.50026 avg_loss = 3.59591\n",
      "epoch no.1 train no.96980  loss = 3.79720 avg_loss = 3.58100\n",
      "epoch no.1 train no.96990  loss = 3.59468 avg_loss = 3.63695\n",
      "epoch no.1 train no.97000  loss = 1.97155 avg_loss = 3.62105\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁오면', '나는', '▁노래', '</s>']\n",
      "여름이 생각나는 노래</s>\n",
      "epoch no.1 train no.97010  loss = 3.10936 avg_loss = 3.68445\n",
      "epoch no.1 train no.97020  loss = 2.59768 avg_loss = 3.60233\n",
      "epoch no.1 train no.97030  loss = 5.77125 avg_loss = 3.67374\n",
      "epoch no.1 train no.97040  loss = 2.56601 avg_loss = 3.60897\n",
      "epoch no.1 train no.97050  loss = 2.19054 avg_loss = 3.58228\n",
      "epoch no.1 train no.97060  loss = 4.35975 avg_loss = 3.60474\n",
      "epoch no.1 train no.97070  loss = 3.34119 avg_loss = 3.60038\n",
      "epoch no.1 train no.97080  loss = 3.69021 avg_loss = 3.61409\n",
      "epoch no.1 train no.97090  loss = 2.89112 avg_loss = 3.60065\n",
      "epoch no.1 train no.97100  loss = 2.79992 avg_loss = 3.62021\n",
      "epoch no.1 train no.97110  loss = 4.46510 avg_loss = 3.61148\n",
      "epoch no.1 train no.97120  loss = 6.76803 avg_loss = 3.65546\n",
      "epoch no.1 train no.97130  loss = 3.84471 avg_loss = 3.66164\n",
      "epoch no.1 train no.97140  loss = 3.30910 avg_loss = 3.66364\n",
      "epoch no.1 train no.97150  loss = 1.89205 avg_loss = 3.64861\n",
      "epoch no.1 train no.97160  loss = 6.11392 avg_loss = 3.70242\n",
      "epoch no.1 train no.97170  loss = 3.77318 avg_loss = 3.74740\n",
      "epoch no.1 train no.97180  loss = 3.71792 avg_loss = 3.75345\n",
      "epoch no.1 train no.97190  loss = 3.74055 avg_loss = 3.73965\n",
      "epoch no.1 train no.97200  loss = 3.55675 avg_loss = 3.76850\n",
      "epoch no.1 train no.97210  loss = 2.85630 avg_loss = 3.72670\n",
      "epoch no.1 train no.97220  loss = 2.56031 avg_loss = 3.73659\n",
      "epoch no.1 train no.97230  loss = 6.32386 avg_loss = 3.73383\n",
      "epoch no.1 train no.97240  loss = 5.54389 avg_loss = 3.72052\n",
      "epoch no.1 train no.97250  loss = 4.43239 avg_loss = 3.71065\n",
      "epoch no.1 train no.97260  loss = 5.90480 avg_loss = 3.73046\n",
      "epoch no.1 train no.97270  loss = 3.06205 avg_loss = 3.67317\n",
      "epoch no.1 train no.97280  loss = 3.17236 avg_loss = 3.65018\n",
      "epoch no.1 train no.97290  loss = 3.03182 avg_loss = 3.65901\n",
      "epoch no.1 train no.97300  loss = 3.45161 avg_loss = 3.59902\n",
      "epoch no.1 train no.97310  loss = 4.31621 avg_loss = 3.58296\n",
      "epoch no.1 train no.97320  loss = 5.93094 avg_loss = 3.57152\n",
      "epoch no.1 train no.97330  loss = 5.81729 avg_loss = 3.54894\n",
      "epoch no.1 train no.97340  loss = 2.87559 avg_loss = 3.53466\n",
      "epoch no.1 train no.97350  loss = 2.60637 avg_loss = 3.53978\n",
      "epoch no.1 train no.97360  loss = 5.16635 avg_loss = 3.57342\n",
      "epoch no.1 train no.97370  loss = 4.57394 avg_loss = 3.54465\n",
      "epoch no.1 train no.97380  loss = 3.68985 avg_loss = 3.56057\n",
      "epoch no.1 train no.97390  loss = 4.41921 avg_loss = 3.58750\n",
      "epoch no.1 train no.97400  loss = 4.34971 avg_loss = 3.66303\n",
      "epoch no.1 train no.97410  loss = 3.33966 avg_loss = 3.66709\n",
      "epoch no.1 train no.97420  loss = 2.90205 avg_loss = 3.62175\n",
      "epoch no.1 train no.97430  loss = 3.69780 avg_loss = 3.56253\n",
      "epoch no.1 train no.97440  loss = 3.80996 avg_loss = 3.55141\n",
      "epoch no.1 train no.97450  loss = 4.38219 avg_loss = 3.55571\n",
      "epoch no.1 train no.97460  loss = 4.64190 avg_loss = 3.55921\n",
      "epoch no.1 train no.97470  loss = 5.18618 avg_loss = 3.57569\n",
      "epoch no.1 train no.97480  loss = 4.39297 avg_loss = 3.61929\n",
      "epoch no.1 train no.97490  loss = 4.62060 avg_loss = 3.62552\n",
      "epoch no.1 train no.97500  loss = 2.92550 avg_loss = 3.64373\n",
      "epoch no.1 train no.97510  loss = 4.05392 avg_loss = 3.69554\n",
      "epoch no.1 train no.97520  loss = 5.07415 avg_loss = 3.75012\n",
      "epoch no.1 train no.97530  loss = 2.52230 avg_loss = 3.73287\n",
      "epoch no.1 train no.97540  loss = 3.68106 avg_loss = 3.74035\n",
      "epoch no.1 train no.97550  loss = 4.02187 avg_loss = 3.73270\n",
      "epoch no.1 train no.97560  loss = 2.50196 avg_loss = 3.74807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.97570  loss = 3.93540 avg_loss = 3.71909\n",
      "epoch no.1 train no.97580  loss = 5.12897 avg_loss = 3.75255\n",
      "epoch no.1 train no.97590  loss = 4.39982 avg_loss = 3.75309\n",
      "epoch no.1 train no.97600  loss = 3.52895 avg_loss = 3.74704\n",
      "epoch no.1 train no.97610  loss = 3.97733 avg_loss = 3.77860\n",
      "epoch no.1 train no.97620  loss = 2.37676 avg_loss = 3.74087\n",
      "epoch no.1 train no.97630  loss = 2.10747 avg_loss = 3.69534\n",
      "epoch no.1 train no.97640  loss = 2.04873 avg_loss = 3.72911\n",
      "epoch no.1 train no.97650  loss = 5.67850 avg_loss = 3.69126\n",
      "epoch no.1 train no.97660  loss = 1.83283 avg_loss = 3.70058\n",
      "epoch no.1 train no.97670  loss = 3.05998 avg_loss = 3.70614\n",
      "epoch no.1 train no.97680  loss = 4.98568 avg_loss = 3.72918\n",
      "epoch no.1 train no.97690  loss = 3.22489 avg_loss = 3.78783\n",
      "epoch no.1 train no.97700  loss = 3.31847 avg_loss = 3.78520\n",
      "epoch no.1 train no.97710  loss = 4.06693 avg_loss = 3.74773\n",
      "epoch no.1 train no.97720  loss = 3.08902 avg_loss = 3.72589\n",
      "epoch no.1 train no.97730  loss = 2.91877 avg_loss = 3.75851\n",
      "epoch no.1 train no.97740  loss = 3.54160 avg_loss = 3.67713\n",
      "epoch no.1 train no.97750  loss = 3.82359 avg_loss = 3.71839\n",
      "epoch no.1 train no.97760  loss = 3.10970 avg_loss = 3.70337\n",
      "epoch no.1 train no.97770  loss = 2.43365 avg_loss = 3.63474\n",
      "epoch no.1 train no.97780  loss = 3.05385 avg_loss = 3.63894\n",
      "epoch no.1 train no.97790  loss = 3.44453 avg_loss = 3.68766\n",
      "epoch no.1 train no.97800  loss = 4.44541 avg_loss = 3.72528\n",
      "epoch no.1 train no.97810  loss = 3.63533 avg_loss = 3.76408\n",
      "epoch no.1 train no.97820  loss = 3.16153 avg_loss = 3.74790\n",
      "epoch no.1 train no.97830  loss = 2.64535 avg_loss = 3.74164\n",
      "epoch no.1 train no.97840  loss = 2.76628 avg_loss = 3.68340\n",
      "epoch no.1 train no.97850  loss = 5.66978 avg_loss = 3.67689\n",
      "epoch no.1 train no.97860  loss = 4.65528 avg_loss = 3.68783\n",
      "epoch no.1 train no.97870  loss = 4.28167 avg_loss = 3.68178\n",
      "epoch no.1 train no.97880  loss = 4.97069 avg_loss = 3.71868\n",
      "epoch no.1 train no.97890  loss = 2.81290 avg_loss = 3.69021\n",
      "epoch no.1 train no.97900  loss = 2.73033 avg_loss = 3.65414\n",
      "epoch no.1 train no.97910  loss = 7.47316 avg_loss = 3.68313\n",
      "epoch no.1 train no.97920  loss = 3.20304 avg_loss = 3.65840\n",
      "epoch no.1 train no.97930  loss = 4.02614 avg_loss = 3.59716\n",
      "epoch no.1 train no.97940  loss = 2.53245 avg_loss = 3.55928\n",
      "epoch no.1 train no.97950  loss = 6.26639 avg_loss = 3.59656\n",
      "epoch no.1 train no.97960  loss = 2.73303 avg_loss = 3.59754\n",
      "epoch no.1 train no.97970  loss = 5.43882 avg_loss = 3.64397\n",
      "epoch no.1 train no.97980  loss = 2.89846 avg_loss = 3.63959\n",
      "epoch no.1 train no.97990  loss = 4.15644 avg_loss = 3.62883\n",
      "epoch no.1 train no.98000  loss = 2.91133 avg_loss = 3.62366\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.1 train no.98010  loss = 4.10452 avg_loss = 3.66934\n",
      "epoch no.1 train no.98020  loss = 3.47527 avg_loss = 3.63011\n",
      "epoch no.1 train no.98030  loss = 4.20227 avg_loss = 3.67099\n",
      "epoch no.1 train no.98040  loss = 5.17094 avg_loss = 3.72848\n",
      "epoch no.1 train no.98050  loss = 5.66253 avg_loss = 3.71648\n",
      "epoch no.1 train no.98060  loss = 2.90209 avg_loss = 3.67845\n",
      "epoch no.1 train no.98070  loss = 2.93085 avg_loss = 3.63878\n",
      "epoch no.1 train no.98080  loss = 4.24342 avg_loss = 3.64706\n",
      "epoch no.1 train no.98090  loss = 2.41764 avg_loss = 3.65792\n",
      "epoch no.1 train no.98100  loss = 4.50645 avg_loss = 3.68652\n",
      "epoch no.1 train no.98110  loss = 2.80721 avg_loss = 3.67400\n",
      "epoch no.1 train no.98120  loss = 2.29640 avg_loss = 3.64630\n",
      "epoch no.1 train no.98130  loss = 2.56355 avg_loss = 3.63071\n",
      "epoch no.1 train no.98140  loss = 5.53538 avg_loss = 3.66962\n",
      "epoch no.1 train no.98150  loss = 2.09938 avg_loss = 3.60922\n",
      "epoch no.1 train no.98160  loss = 8.61997 avg_loss = 3.71787\n",
      "epoch no.1 train no.98170  loss = 4.07131 avg_loss = 3.71256\n",
      "epoch no.1 train no.98180  loss = 6.80644 avg_loss = 3.73819\n",
      "epoch no.1 train no.98190  loss = 1.82255 avg_loss = 3.69150\n",
      "epoch no.1 train no.98200  loss = 4.41200 avg_loss = 3.68589\n",
      "epoch no.1 train no.98210  loss = 5.29124 avg_loss = 3.69035\n",
      "epoch no.1 train no.98220  loss = 4.04918 avg_loss = 3.66434\n",
      "epoch no.1 train no.98230  loss = 3.33079 avg_loss = 3.68425\n",
      "epoch no.1 train no.98240  loss = 4.23156 avg_loss = 3.64667\n",
      "epoch no.1 train no.98250  loss = 4.10151 avg_loss = 3.66086\n",
      "epoch no.1 train no.98260  loss = 3.57271 avg_loss = 3.65618\n",
      "epoch no.1 train no.98270  loss = 2.79107 avg_loss = 3.65865\n",
      "epoch no.1 train no.98280  loss = 2.80393 avg_loss = 3.72114\n",
      "epoch no.1 train no.98290  loss = 2.52108 avg_loss = 3.66815\n",
      "epoch no.1 train no.98300  loss = 7.31731 avg_loss = 3.67618\n",
      "epoch no.1 train no.98310  loss = 1.95623 avg_loss = 3.66036\n",
      "epoch no.1 train no.98320  loss = 3.77095 avg_loss = 3.69254\n",
      "epoch no.1 train no.98330  loss = 2.84178 avg_loss = 3.68918\n",
      "epoch no.1 train no.98340  loss = 4.97625 avg_loss = 3.71309\n",
      "epoch no.1 train no.98350  loss = 3.88999 avg_loss = 3.68381\n",
      "epoch no.1 train no.98360  loss = 3.27536 avg_loss = 3.61547\n",
      "epoch no.1 train no.98370  loss = 4.86946 avg_loss = 3.61856\n",
      "epoch no.1 train no.98380  loss = 3.82625 avg_loss = 3.62129\n",
      "epoch no.1 train no.98390  loss = 4.43588 avg_loss = 3.61896\n",
      "epoch no.1 train no.98400  loss = 4.47953 avg_loss = 3.61765\n",
      "epoch no.1 train no.98410  loss = 4.98986 avg_loss = 3.61130\n",
      "epoch no.1 train no.98420  loss = 2.94898 avg_loss = 3.63462\n",
      "epoch no.1 train no.98430  loss = 4.49369 avg_loss = 3.61322\n",
      "epoch no.1 train no.98440  loss = 2.35599 avg_loss = 3.59160\n",
      "epoch no.1 train no.98450  loss = 2.37724 avg_loss = 3.62640\n",
      "epoch no.1 train no.98460  loss = 4.06389 avg_loss = 3.58113\n",
      "epoch no.1 train no.98470  loss = 4.09391 avg_loss = 3.58464\n",
      "epoch no.1 train no.98480  loss = 3.46220 avg_loss = 3.58502\n",
      "epoch no.1 train no.98490  loss = 3.94426 avg_loss = 3.59814\n",
      "epoch no.1 train no.98500  loss = 3.31923 avg_loss = 3.62891\n",
      "epoch no.1 train no.98510  loss = 2.70536 avg_loss = 3.59724\n",
      "epoch no.1 train no.98520  loss = 2.54801 avg_loss = 3.55185\n",
      "epoch no.1 train no.98530  loss = 3.45061 avg_loss = 3.55551\n",
      "epoch no.1 train no.98540  loss = 3.19022 avg_loss = 3.53104\n",
      "epoch no.1 train no.98550  loss = 4.61401 avg_loss = 3.62063\n",
      "epoch no.1 train no.98560  loss = 2.39340 avg_loss = 3.62465\n",
      "epoch no.1 train no.98570  loss = 2.00887 avg_loss = 3.60703\n",
      "epoch no.1 train no.98580  loss = 4.05819 avg_loss = 3.60268\n",
      "epoch no.1 train no.98590  loss = 3.35774 avg_loss = 3.55845\n",
      "epoch no.1 train no.98600  loss = 5.49254 avg_loss = 3.54850\n",
      "epoch no.1 train no.98610  loss = 2.08404 avg_loss = 3.53815\n",
      "epoch no.1 train no.98620  loss = 6.02111 avg_loss = 3.63973\n",
      "epoch no.1 train no.98630  loss = 2.85354 avg_loss = 3.61140\n",
      "epoch no.1 train no.98640  loss = 1.18482 avg_loss = 3.61366\n",
      "epoch no.1 train no.98650  loss = 3.57700 avg_loss = 3.68904\n",
      "epoch no.1 train no.98660  loss = 2.55272 avg_loss = 3.61122\n",
      "epoch no.1 train no.98670  loss = 4.53216 avg_loss = 3.64608\n",
      "epoch no.1 train no.98680  loss = 2.60364 avg_loss = 3.66495\n",
      "epoch no.1 train no.98690  loss = 3.23137 avg_loss = 3.66177\n",
      "epoch no.1 train no.98700  loss = 4.49934 avg_loss = 3.66131\n",
      "epoch no.1 train no.98710  loss = 3.70506 avg_loss = 3.64037\n",
      "epoch no.1 train no.98720  loss = 2.92875 avg_loss = 3.62262\n",
      "epoch no.1 train no.98730  loss = 2.44437 avg_loss = 3.63967\n",
      "epoch no.1 train no.98740  loss = 3.66072 avg_loss = 3.61237\n",
      "epoch no.1 train no.98750  loss = 3.85330 avg_loss = 3.69944\n",
      "epoch no.1 train no.98760  loss = 5.53267 avg_loss = 3.70096\n",
      "epoch no.1 train no.98770  loss = 6.22011 avg_loss = 3.75216\n",
      "epoch no.1 train no.98780  loss = 2.42285 avg_loss = 3.73652\n",
      "epoch no.1 train no.98790  loss = 2.58085 avg_loss = 3.75203\n",
      "epoch no.1 train no.98800  loss = 4.96928 avg_loss = 3.73196\n",
      "epoch no.1 train no.98810  loss = 2.39461 avg_loss = 3.72106\n",
      "epoch no.1 train no.98820  loss = 1.89677 avg_loss = 3.74617\n",
      "epoch no.1 train no.98830  loss = 3.45911 avg_loss = 3.71462\n",
      "epoch no.1 train no.98840  loss = 2.50760 avg_loss = 3.69583\n",
      "epoch no.1 train no.98850  loss = 2.38898 avg_loss = 3.66806\n",
      "epoch no.1 train no.98860  loss = 2.49149 avg_loss = 3.58932\n",
      "epoch no.1 train no.98870  loss = 3.73648 avg_loss = 3.60285\n",
      "epoch no.1 train no.98880  loss = 6.75473 avg_loss = 3.63663\n",
      "epoch no.1 train no.98890  loss = 6.82421 avg_loss = 3.64036\n",
      "epoch no.1 train no.98900  loss = 4.23105 avg_loss = 3.64264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.98910  loss = 2.39636 avg_loss = 3.66556\n",
      "epoch no.1 train no.98920  loss = 2.89024 avg_loss = 3.66100\n",
      "epoch no.1 train no.98930  loss = 3.44636 avg_loss = 3.63990\n",
      "epoch no.1 train no.98940  loss = 2.29685 avg_loss = 3.64288\n",
      "epoch no.1 train no.98950  loss = 2.21091 avg_loss = 3.65895\n",
      "epoch no.1 train no.98960  loss = 2.84088 avg_loss = 3.65271\n",
      "epoch no.1 train no.98970  loss = 2.41666 avg_loss = 3.60543\n",
      "epoch no.1 train no.98980  loss = 4.40209 avg_loss = 3.64838\n",
      "epoch no.1 train no.98990  loss = 3.74068 avg_loss = 3.64090\n",
      "epoch no.1 train no.99000  loss = 4.21242 avg_loss = 3.67427\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '▁플레이', '리스트', '</s>']\n",
      "여름밤의 감성 플레이리스트</s>\n",
      "epoch no.1 train no.99010  loss = 3.49930 avg_loss = 3.64866\n",
      "epoch no.1 train no.99020  loss = 3.07825 avg_loss = 3.59934\n",
      "epoch no.1 train no.99030  loss = 3.29029 avg_loss = 3.59663\n",
      "epoch no.1 train no.99040  loss = 2.99859 avg_loss = 3.59269\n",
      "epoch no.1 train no.99050  loss = 4.16373 avg_loss = 3.61591\n",
      "epoch no.1 train no.99060  loss = 2.98613 avg_loss = 3.58517\n",
      "epoch no.1 train no.99070  loss = 2.67649 avg_loss = 3.60626\n",
      "epoch no.1 train no.99080  loss = 4.30532 avg_loss = 3.60781\n",
      "epoch no.1 train no.99090  loss = 2.55959 avg_loss = 3.62005\n",
      "epoch no.1 train no.99100  loss = 3.62166 avg_loss = 3.65365\n",
      "epoch no.1 train no.99110  loss = 3.61575 avg_loss = 3.71396\n",
      "epoch no.1 train no.99120  loss = 2.73920 avg_loss = 3.66200\n",
      "epoch no.1 train no.99130  loss = 5.37541 avg_loss = 3.61956\n",
      "epoch no.1 train no.99140  loss = 3.51169 avg_loss = 3.65309\n",
      "epoch no.1 train no.99150  loss = 4.95565 avg_loss = 3.64049\n",
      "epoch no.1 train no.99160  loss = 2.96444 avg_loss = 3.61236\n",
      "epoch no.1 train no.99170  loss = 2.83378 avg_loss = 3.62300\n",
      "epoch no.1 train no.99180  loss = 3.79867 avg_loss = 3.61786\n",
      "epoch no.1 train no.99190  loss = 3.09562 avg_loss = 3.57509\n",
      "epoch no.1 train no.99200  loss = 4.86692 avg_loss = 3.62197\n",
      "epoch no.1 train no.99210  loss = 5.71627 avg_loss = 3.60113\n",
      "epoch no.1 train no.99220  loss = 3.08090 avg_loss = 3.57415\n",
      "epoch no.1 train no.99230  loss = 2.65551 avg_loss = 3.55542\n",
      "epoch no.1 train no.99240  loss = 2.00471 avg_loss = 3.56348\n",
      "epoch no.1 train no.99250  loss = 3.11035 avg_loss = 3.56383\n",
      "epoch no.1 train no.99260  loss = 5.60212 avg_loss = 3.58147\n",
      "epoch no.1 train no.99270  loss = 2.20342 avg_loss = 3.66251\n",
      "epoch no.1 train no.99280  loss = 3.09209 avg_loss = 3.62332\n",
      "epoch no.1 train no.99290  loss = 4.10385 avg_loss = 3.61519\n",
      "epoch no.1 train no.99300  loss = 4.09918 avg_loss = 3.65311\n",
      "epoch no.1 train no.99310  loss = 2.79096 avg_loss = 3.69092\n",
      "epoch no.1 train no.99320  loss = 2.25478 avg_loss = 3.67071\n",
      "epoch no.1 train no.99330  loss = 3.06476 avg_loss = 3.74215\n",
      "epoch no.1 train no.99340  loss = 3.27924 avg_loss = 3.76302\n",
      "epoch no.1 train no.99350  loss = 4.13785 avg_loss = 3.75252\n",
      "epoch no.1 train no.99360  loss = 2.39243 avg_loss = 3.73057\n",
      "epoch no.1 train no.99370  loss = 5.22099 avg_loss = 3.75321\n",
      "epoch no.1 train no.99380  loss = 3.04156 avg_loss = 3.75459\n",
      "epoch no.1 train no.99390  loss = 2.23832 avg_loss = 3.70406\n",
      "epoch no.1 train no.99400  loss = 4.42511 avg_loss = 3.66029\n",
      "epoch no.1 train no.99410  loss = 5.06553 avg_loss = 3.74279\n",
      "epoch no.1 train no.99420  loss = 3.88610 avg_loss = 3.76105\n",
      "epoch no.1 train no.99430  loss = 4.76278 avg_loss = 3.74084\n",
      "epoch no.1 train no.99440  loss = 3.93639 avg_loss = 3.70314\n",
      "epoch no.1 train no.99450  loss = 4.39638 avg_loss = 3.73033\n",
      "epoch no.1 train no.99460  loss = 3.28068 avg_loss = 3.68256\n",
      "epoch no.1 train no.99470  loss = 4.02510 avg_loss = 3.69047\n",
      "epoch no.1 train no.99480  loss = 3.24253 avg_loss = 3.70461\n",
      "epoch no.1 train no.99490  loss = 3.69573 avg_loss = 3.70402\n",
      "epoch no.1 train no.99500  loss = 4.17336 avg_loss = 3.70121\n",
      "epoch no.1 train no.99510  loss = 3.75186 avg_loss = 3.68212\n",
      "epoch no.1 train no.99520  loss = 6.38950 avg_loss = 3.68496\n",
      "epoch no.1 train no.99530  loss = 3.86998 avg_loss = 3.66279\n",
      "epoch no.1 train no.99540  loss = 3.69878 avg_loss = 3.61950\n",
      "epoch no.1 train no.99550  loss = 1.78038 avg_loss = 3.58663\n",
      "epoch no.1 train no.99560  loss = 3.81038 avg_loss = 3.60135\n",
      "epoch no.1 train no.99570  loss = 5.58119 avg_loss = 3.67855\n",
      "epoch no.1 train no.99580  loss = 2.20828 avg_loss = 3.65729\n",
      "epoch no.1 train no.99590  loss = 6.11203 avg_loss = 3.71210\n",
      "epoch no.1 train no.99600  loss = 3.62306 avg_loss = 3.71295\n",
      "epoch no.1 train no.99610  loss = 2.32054 avg_loss = 3.70068\n",
      "epoch no.1 train no.99620  loss = 3.03862 avg_loss = 3.72908\n",
      "epoch no.1 train no.99630  loss = 4.66232 avg_loss = 3.69930\n",
      "epoch no.1 train no.99640  loss = 3.05855 avg_loss = 3.67397\n",
      "epoch no.1 train no.99650  loss = 3.23796 avg_loss = 3.62774\n",
      "epoch no.1 train no.99660  loss = 3.85373 avg_loss = 3.62741\n",
      "epoch no.1 train no.99670  loss = 4.51155 avg_loss = 3.61117\n",
      "epoch no.1 train no.99680  loss = 3.80872 avg_loss = 3.59180\n",
      "epoch no.1 train no.99690  loss = 3.04231 avg_loss = 3.59124\n",
      "epoch no.1 train no.99700  loss = 2.07008 avg_loss = 3.62341\n",
      "epoch no.1 train no.99710  loss = 2.40678 avg_loss = 3.68913\n",
      "epoch no.1 train no.99720  loss = 4.00273 avg_loss = 3.68408\n",
      "epoch no.1 train no.99730  loss = 6.12797 avg_loss = 3.70656\n",
      "epoch no.1 train no.99740  loss = 2.63747 avg_loss = 3.72224\n",
      "epoch no.1 train no.99750  loss = 4.78692 avg_loss = 3.73639\n",
      "epoch no.1 train no.99760  loss = 2.43691 avg_loss = 3.72903\n",
      "epoch no.1 train no.99770  loss = 4.21927 avg_loss = 3.79217\n",
      "epoch no.1 train no.99780  loss = 3.33415 avg_loss = 3.78121\n",
      "epoch no.1 train no.99790  loss = 4.44711 avg_loss = 3.75771\n",
      "epoch no.1 train no.99800  loss = 3.00428 avg_loss = 3.73888\n",
      "epoch no.1 train no.99810  loss = 5.08009 avg_loss = 3.74067\n",
      "epoch no.1 train no.99820  loss = 3.27343 avg_loss = 3.74213\n",
      "epoch no.1 train no.99830  loss = 2.80509 avg_loss = 3.71904\n",
      "epoch no.1 train no.99840  loss = 3.40905 avg_loss = 3.67056\n",
      "epoch no.1 train no.99850  loss = 5.13107 avg_loss = 3.64563\n",
      "epoch no.1 train no.99860  loss = 5.22258 avg_loss = 3.73542\n",
      "epoch no.1 train no.99870  loss = 2.76962 avg_loss = 3.71996\n",
      "epoch no.1 train no.99880  loss = 3.41159 avg_loss = 3.69417\n",
      "epoch no.1 train no.99890  loss = 5.15003 avg_loss = 3.67624\n",
      "epoch no.1 train no.99900  loss = 4.27053 avg_loss = 3.74113\n",
      "epoch no.1 train no.99910  loss = 2.43626 avg_loss = 3.66995\n",
      "epoch no.1 train no.99920  loss = 5.34938 avg_loss = 3.67252\n",
      "epoch no.1 train no.99930  loss = 3.96248 avg_loss = 3.73108\n",
      "epoch no.1 train no.99940  loss = 3.33159 avg_loss = 3.77972\n",
      "epoch no.1 train no.99950  loss = 3.40446 avg_loss = 3.78439\n",
      "epoch no.1 train no.99960  loss = 4.23897 avg_loss = 3.78295\n",
      "epoch no.1 train no.99970  loss = 2.79530 avg_loss = 3.77983\n",
      "epoch no.1 train no.99980  loss = 4.79585 avg_loss = 3.75131\n",
      "epoch no.1 train no.99990  loss = 3.27368 avg_loss = 3.73380\n",
      "epoch no.1 train no.100000  loss = 5.45146 avg_loss = 3.66895\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '▁해줄', '▁노래', '</s>']\n",
      "여름밤을 따뜻하게 해줄 노래</s>\n",
      "epoch no.1 train no.100010  loss = 5.45326 avg_loss = 3.65866\n",
      "epoch no.1 train no.100020  loss = 3.38985 avg_loss = 3.65005\n",
      "epoch no.1 train no.100030  loss = 3.91475 avg_loss = 3.60782\n",
      "epoch no.1 train no.100040  loss = 3.57945 avg_loss = 3.59835\n",
      "epoch no.1 train no.100050  loss = 1.89690 avg_loss = 3.63092\n",
      "epoch no.1 train no.100060  loss = 3.39216 avg_loss = 3.63339\n",
      "epoch no.1 train no.100070  loss = 2.59005 avg_loss = 3.60783\n",
      "epoch no.1 train no.100080  loss = 2.53390 avg_loss = 3.62628\n",
      "epoch no.1 train no.100090  loss = 3.42332 avg_loss = 3.59791\n",
      "epoch no.1 train no.100100  loss = 3.67205 avg_loss = 3.59409\n",
      "epoch no.1 train no.100110  loss = 3.36087 avg_loss = 3.60224\n",
      "epoch no.1 train no.100120  loss = 2.60420 avg_loss = 3.56988\n",
      "epoch no.1 train no.100130  loss = 2.81257 avg_loss = 3.59470\n",
      "epoch no.1 train no.100140  loss = 4.92924 avg_loss = 3.63107\n",
      "epoch no.1 train no.100150  loss = 3.23153 avg_loss = 3.64881\n",
      "epoch no.1 train no.100160  loss = 3.39605 avg_loss = 3.65065\n",
      "epoch no.1 train no.100170  loss = 5.83534 avg_loss = 3.63228\n",
      "epoch no.1 train no.100180  loss = 2.38246 avg_loss = 3.63950\n",
      "epoch no.1 train no.100190  loss = 3.71667 avg_loss = 3.67510\n",
      "epoch no.1 train no.100200  loss = 4.50481 avg_loss = 3.68101\n",
      "epoch no.1 train no.100210  loss = 3.83394 avg_loss = 3.66248\n",
      "epoch no.1 train no.100220  loss = 3.53165 avg_loss = 3.62202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.100230  loss = 3.90294 avg_loss = 3.64115\n",
      "epoch no.1 train no.100240  loss = 3.16863 avg_loss = 3.63696\n",
      "epoch no.1 train no.100250  loss = 1.99774 avg_loss = 3.62057\n",
      "epoch no.1 train no.100260  loss = 2.92125 avg_loss = 3.58718\n",
      "epoch no.1 train no.100270  loss = 2.32877 avg_loss = 3.60896\n",
      "epoch no.1 train no.100280  loss = 2.95677 avg_loss = 3.65487\n",
      "epoch no.1 train no.100290  loss = 5.11079 avg_loss = 3.65961\n",
      "epoch no.1 train no.100300  loss = 3.63598 avg_loss = 3.64893\n",
      "epoch no.1 train no.100310  loss = 3.45399 avg_loss = 3.62817\n",
      "epoch no.1 train no.100320  loss = 3.00420 avg_loss = 3.62897\n",
      "epoch no.1 train no.100330  loss = 3.53802 avg_loss = 3.62868\n",
      "epoch no.1 train no.100340  loss = 5.62232 avg_loss = 3.66816\n",
      "epoch no.1 train no.100350  loss = 2.64172 avg_loss = 3.64225\n",
      "epoch no.1 train no.100360  loss = 4.91966 avg_loss = 3.67035\n",
      "epoch no.1 train no.100370  loss = 4.33480 avg_loss = 3.67617\n",
      "epoch no.1 train no.100380  loss = 3.15230 avg_loss = 3.63857\n",
      "epoch no.1 train no.100390  loss = 2.95471 avg_loss = 3.67485\n",
      "epoch no.1 train no.100400  loss = 4.86233 avg_loss = 3.68790\n",
      "epoch no.1 train no.100410  loss = 6.50851 avg_loss = 3.68930\n",
      "epoch no.1 train no.100420  loss = 5.43939 avg_loss = 3.66946\n",
      "epoch no.1 train no.100430  loss = 3.56036 avg_loss = 3.69554\n",
      "epoch no.1 train no.100440  loss = 2.80759 avg_loss = 3.70740\n",
      "epoch no.1 train no.100450  loss = 2.37945 avg_loss = 3.69799\n",
      "epoch no.1 train no.100460  loss = 5.44179 avg_loss = 3.68962\n",
      "epoch no.1 train no.100470  loss = 3.50796 avg_loss = 3.68277\n",
      "epoch no.1 train no.100480  loss = 5.09651 avg_loss = 3.66717\n",
      "epoch no.1 train no.100490  loss = 3.09746 avg_loss = 3.67361\n",
      "epoch no.1 train no.100500  loss = 2.14316 avg_loss = 3.59339\n",
      "epoch no.1 train no.100510  loss = 3.02443 avg_loss = 3.56477\n",
      "epoch no.1 train no.100520  loss = 4.10998 avg_loss = 3.57358\n",
      "epoch no.1 train no.100530  loss = 2.15416 avg_loss = 3.58699\n",
      "epoch no.1 train no.100540  loss = 4.03913 avg_loss = 3.60851\n",
      "epoch no.1 train no.100550  loss = 5.16301 avg_loss = 3.66644\n",
      "epoch no.1 train no.100560  loss = 5.18529 avg_loss = 3.64393\n",
      "epoch no.1 train no.100570  loss = 3.00800 avg_loss = 3.61518\n",
      "epoch no.1 train no.100580  loss = 3.07060 avg_loss = 3.59706\n",
      "epoch no.1 train no.100590  loss = 5.06335 avg_loss = 3.62592\n",
      "epoch no.1 train no.100600  loss = 3.51937 avg_loss = 3.58624\n",
      "epoch no.1 train no.100610  loss = 4.34354 avg_loss = 3.54414\n",
      "epoch no.1 train no.100620  loss = 4.09552 avg_loss = 3.57227\n",
      "epoch no.1 train no.100630  loss = 3.55014 avg_loss = 3.55520\n",
      "epoch no.1 train no.100640  loss = 3.17061 avg_loss = 3.55499\n",
      "epoch no.1 train no.100650  loss = 2.49140 avg_loss = 3.54866\n",
      "epoch no.1 train no.100660  loss = 3.36937 avg_loss = 3.56799\n",
      "epoch no.1 train no.100670  loss = 3.36278 avg_loss = 3.59586\n",
      "epoch no.1 train no.100680  loss = 3.52733 avg_loss = 3.61160\n",
      "epoch no.1 train no.100690  loss = 5.48626 avg_loss = 3.63919\n",
      "epoch no.1 train no.100700  loss = 3.01999 avg_loss = 3.58617\n",
      "epoch no.1 train no.100710  loss = 2.01090 avg_loss = 3.59957\n",
      "epoch no.1 train no.100720  loss = 3.28460 avg_loss = 3.62106\n",
      "epoch no.1 train no.100730  loss = 2.93537 avg_loss = 3.63578\n",
      "epoch no.1 train no.100740  loss = 1.87520 avg_loss = 3.61166\n",
      "epoch no.1 train no.100750  loss = 3.04256 avg_loss = 3.62985\n",
      "epoch no.1 train no.100760  loss = 3.88967 avg_loss = 3.63093\n",
      "epoch no.1 train no.100770  loss = 5.70451 avg_loss = 3.67121\n",
      "epoch no.1 train no.100780  loss = 3.55427 avg_loss = 3.65974\n",
      "epoch no.1 train no.100790  loss = 3.82201 avg_loss = 3.69169\n",
      "epoch no.1 train no.100800  loss = 4.40621 avg_loss = 3.70850\n",
      "epoch no.1 train no.100810  loss = 1.94767 avg_loss = 3.71200\n",
      "epoch no.1 train no.100820  loss = 3.06786 avg_loss = 3.64802\n",
      "epoch no.1 train no.100830  loss = 7.31030 avg_loss = 3.68532\n",
      "epoch no.1 train no.100840  loss = 2.30950 avg_loss = 3.63541\n",
      "epoch no.1 train no.100850  loss = 2.64458 avg_loss = 3.61606\n",
      "epoch no.1 train no.100860  loss = 3.71872 avg_loss = 3.59498\n",
      "epoch no.1 train no.100870  loss = 4.09678 avg_loss = 3.61977\n",
      "epoch no.1 train no.100880  loss = 3.22919 avg_loss = 3.65394\n",
      "epoch no.1 train no.100890  loss = 4.33103 avg_loss = 3.66247\n",
      "epoch no.1 train no.100900  loss = 5.82519 avg_loss = 3.69851\n",
      "epoch no.1 train no.100910  loss = 4.51517 avg_loss = 3.65800\n",
      "epoch no.1 train no.100920  loss = 3.65607 avg_loss = 3.60694\n",
      "epoch no.1 train no.100930  loss = 4.58656 avg_loss = 3.62240\n",
      "epoch no.1 train no.100940  loss = 4.71926 avg_loss = 3.64841\n",
      "epoch no.1 train no.100950  loss = 3.06013 avg_loss = 3.64150\n",
      "epoch no.1 train no.100960  loss = 3.13948 avg_loss = 3.64286\n",
      "epoch no.1 train no.100970  loss = 2.88400 avg_loss = 3.57896\n",
      "epoch no.1 train no.100980  loss = 2.17994 avg_loss = 3.57477\n",
      "epoch no.1 train no.100990  loss = 2.19849 avg_loss = 3.58840\n",
      "epoch no.1 train no.101000  loss = 3.03157 avg_loss = 3.56671\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '▁끝', '자', '락', '에서', '</s>', '▁음악', '음악', '</s>']\n",
      "여름의 끝자락에서 듣는 인디음악</s>\n",
      "epoch no.1 train no.101010  loss = 2.42081 avg_loss = 3.61576\n",
      "epoch no.1 train no.101020  loss = 3.76339 avg_loss = 3.64195\n",
      "epoch no.1 train no.101030  loss = 4.32056 avg_loss = 3.62728\n",
      "epoch no.1 train no.101040  loss = 2.72551 avg_loss = 3.68735\n",
      "epoch no.1 train no.101050  loss = 3.88387 avg_loss = 3.66889\n",
      "epoch no.1 train no.101060  loss = 4.44574 avg_loss = 3.64530\n",
      "epoch no.1 train no.101070  loss = 4.92122 avg_loss = 3.63134\n",
      "epoch no.1 train no.101080  loss = 3.31975 avg_loss = 3.59212\n",
      "epoch no.1 train no.101090  loss = 3.13781 avg_loss = 3.58322\n",
      "epoch no.1 train no.101100  loss = 4.88473 avg_loss = 3.61648\n",
      "epoch no.1 train no.101110  loss = 2.49578 avg_loss = 3.62573\n",
      "epoch no.1 train no.101120  loss = 3.88099 avg_loss = 3.60808\n",
      "epoch no.1 train no.101130  loss = 2.59025 avg_loss = 3.63289\n",
      "epoch no.1 train no.101140  loss = 3.83840 avg_loss = 3.61221\n",
      "epoch no.1 train no.101150  loss = 1.76126 avg_loss = 3.65068\n",
      "epoch no.1 train no.101160  loss = 3.11837 avg_loss = 3.62010\n",
      "epoch no.1 train no.101170  loss = 4.38844 avg_loss = 3.59932\n",
      "epoch no.1 train no.101180  loss = 3.30184 avg_loss = 3.57848\n",
      "epoch no.1 train no.101190  loss = 3.91762 avg_loss = 3.62339\n",
      "epoch no.1 train no.101200  loss = 5.04621 avg_loss = 3.64812\n",
      "epoch no.1 train no.101210  loss = 4.44710 avg_loss = 3.62082\n",
      "epoch no.1 train no.101220  loss = 2.42775 avg_loss = 3.58282\n",
      "epoch no.1 train no.101230  loss = 2.62806 avg_loss = 3.57959\n",
      "epoch no.1 train no.101240  loss = 2.41122 avg_loss = 3.53326\n",
      "epoch no.1 train no.101250  loss = 3.94637 avg_loss = 3.55861\n",
      "epoch no.1 train no.101260  loss = 3.54189 avg_loss = 3.53780\n",
      "epoch no.1 train no.101270  loss = 3.46795 avg_loss = 3.58416\n",
      "epoch no.1 train no.101280  loss = 3.71933 avg_loss = 3.58505\n",
      "epoch no.1 train no.101290  loss = 2.42088 avg_loss = 3.63905\n",
      "epoch no.1 train no.101300  loss = 4.46837 avg_loss = 3.65203\n",
      "epoch no.1 train no.101310  loss = 3.09112 avg_loss = 3.66055\n",
      "epoch no.1 train no.101320  loss = 4.20249 avg_loss = 3.60274\n",
      "epoch no.1 train no.101330  loss = 4.01292 avg_loss = 3.61851\n",
      "epoch no.1 train no.101340  loss = 2.56285 avg_loss = 3.60938\n",
      "epoch no.1 train no.101350  loss = 3.48739 avg_loss = 3.60696\n",
      "epoch no.1 train no.101360  loss = 1.96022 avg_loss = 3.60765\n",
      "epoch no.1 train no.101370  loss = 4.33216 avg_loss = 3.63993\n",
      "epoch no.1 train no.101380  loss = 2.83632 avg_loss = 3.66350\n",
      "epoch no.1 train no.101390  loss = 5.48295 avg_loss = 3.72448\n",
      "epoch no.1 train no.101400  loss = 3.47567 avg_loss = 3.70274\n",
      "epoch no.1 train no.101410  loss = 3.84403 avg_loss = 3.68498\n",
      "epoch no.1 train no.101420  loss = 6.74188 avg_loss = 3.71813\n",
      "epoch no.1 train no.101430  loss = 4.31516 avg_loss = 3.66828\n",
      "epoch no.1 train no.101440  loss = 3.31538 avg_loss = 3.69077\n",
      "epoch no.1 train no.101450  loss = 2.01739 avg_loss = 3.74438\n",
      "epoch no.1 train no.101460  loss = 1.94002 avg_loss = 3.72741\n",
      "epoch no.1 train no.101470  loss = 4.70682 avg_loss = 3.80871\n",
      "epoch no.1 train no.101480  loss = 2.04670 avg_loss = 3.77332\n",
      "epoch no.1 train no.101490  loss = 3.65004 avg_loss = 3.71610\n",
      "epoch no.1 train no.101500  loss = 3.08488 avg_loss = 3.74939\n",
      "epoch no.1 train no.101510  loss = 2.50116 avg_loss = 3.69792\n",
      "epoch no.1 train no.101520  loss = 5.54475 avg_loss = 3.67819\n",
      "epoch no.1 train no.101530  loss = 2.53153 avg_loss = 3.67817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.101540  loss = 4.34527 avg_loss = 3.67788\n",
      "epoch no.1 train no.101550  loss = 3.12510 avg_loss = 3.68269\n",
      "epoch no.1 train no.101560  loss = 6.69646 avg_loss = 3.72724\n",
      "epoch no.1 train no.101570  loss = 5.16967 avg_loss = 3.69427\n",
      "epoch no.1 train no.101580  loss = 3.53840 avg_loss = 3.71972\n",
      "epoch no.1 train no.101590  loss = 2.43695 avg_loss = 3.77174\n",
      "epoch no.1 train no.101600  loss = 3.01540 avg_loss = 3.74855\n",
      "epoch no.1 train no.101610  loss = 1.69237 avg_loss = 3.69602\n",
      "epoch no.1 train no.101620  loss = 4.21533 avg_loss = 3.68338\n",
      "epoch no.1 train no.101630  loss = 4.39118 avg_loss = 3.67806\n",
      "epoch no.1 train no.101640  loss = 5.43720 avg_loss = 3.68726\n",
      "epoch no.1 train no.101650  loss = 7.13574 avg_loss = 3.74408\n",
      "epoch no.1 train no.101660  loss = 7.65851 avg_loss = 3.74948\n",
      "epoch no.1 train no.101670  loss = 3.41643 avg_loss = 3.71539\n",
      "epoch no.1 train no.101680  loss = 2.75565 avg_loss = 3.68233\n",
      "epoch no.1 train no.101690  loss = 3.46331 avg_loss = 3.72132\n",
      "epoch no.1 train no.101700  loss = 3.19668 avg_loss = 3.67661\n",
      "epoch no.1 train no.101710  loss = 3.46993 avg_loss = 3.70343\n",
      "epoch no.1 train no.101720  loss = 4.08073 avg_loss = 3.68956\n",
      "epoch no.1 train no.101730  loss = 3.37078 avg_loss = 3.69743\n",
      "epoch no.1 train no.101740  loss = 4.09381 avg_loss = 3.68971\n",
      "epoch no.1 train no.101750  loss = 2.38466 avg_loss = 3.70249\n",
      "epoch no.1 train no.101760  loss = 4.35074 avg_loss = 3.76718\n",
      "epoch no.1 train no.101770  loss = 3.97687 avg_loss = 3.74767\n",
      "epoch no.1 train no.101780  loss = 2.17686 avg_loss = 3.72395\n",
      "epoch no.1 train no.101790  loss = 2.31486 avg_loss = 3.73370\n",
      "epoch no.1 train no.101800  loss = 4.70192 avg_loss = 3.73654\n",
      "epoch no.1 train no.101810  loss = 4.93631 avg_loss = 3.76420\n",
      "epoch no.1 train no.101820  loss = 7.52811 avg_loss = 3.79953\n",
      "epoch no.1 train no.101830  loss = 3.94389 avg_loss = 3.72980\n",
      "epoch no.1 train no.101840  loss = 4.06012 avg_loss = 3.65996\n",
      "epoch no.1 train no.101850  loss = 3.91685 avg_loss = 3.61234\n",
      "epoch no.1 train no.101860  loss = 2.86008 avg_loss = 3.62746\n",
      "epoch no.1 train no.101870  loss = 4.22130 avg_loss = 3.64422\n",
      "epoch no.1 train no.101880  loss = 1.90543 avg_loss = 3.61925\n",
      "epoch no.1 train no.101890  loss = 4.80854 avg_loss = 3.64305\n",
      "epoch no.1 train no.101900  loss = 2.58371 avg_loss = 3.62463\n",
      "epoch no.1 train no.101910  loss = 2.96812 avg_loss = 3.63008\n",
      "epoch no.1 train no.101920  loss = 3.46214 avg_loss = 3.60300\n",
      "epoch no.1 train no.101930  loss = 3.70894 avg_loss = 3.60085\n",
      "epoch no.1 train no.101940  loss = 1.81376 avg_loss = 3.60132\n",
      "epoch no.1 train no.101950  loss = 3.20845 avg_loss = 3.57841\n",
      "epoch no.1 train no.101960  loss = 2.18211 avg_loss = 3.54149\n",
      "epoch no.1 train no.101970  loss = 2.67045 avg_loss = 3.52616\n",
      "epoch no.1 train no.101980  loss = 3.91325 avg_loss = 3.57636\n",
      "epoch no.1 train no.101990  loss = 3.56984 avg_loss = 3.56482\n",
      "epoch no.1 train no.102000  loss = 2.81128 avg_loss = 3.55391\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '의', '▁듣는', '▁감성', '한', '▁노래', '</s>']\n",
      "여름밤에 듣는 잔잔한 노래</s>\n",
      "epoch no.1 train no.102010  loss = 4.73639 avg_loss = 3.53799\n",
      "epoch no.1 train no.102020  loss = 3.22080 avg_loss = 3.55321\n",
      "epoch no.1 train no.102030  loss = 3.59514 avg_loss = 3.62349\n",
      "epoch no.1 train no.102040  loss = 3.97499 avg_loss = 3.59500\n",
      "epoch no.1 train no.102050  loss = 2.79577 avg_loss = 3.55426\n",
      "epoch no.1 train no.102060  loss = 3.62266 avg_loss = 3.57260\n",
      "epoch no.1 train no.102070  loss = 2.57076 avg_loss = 3.57601\n",
      "epoch no.1 train no.102080  loss = 3.27133 avg_loss = 3.53934\n",
      "epoch no.1 train no.102090  loss = 3.81150 avg_loss = 3.55853\n",
      "epoch no.1 train no.102100  loss = 4.14816 avg_loss = 3.54872\n",
      "epoch no.1 train no.102110  loss = 3.24539 avg_loss = 3.54015\n",
      "epoch no.1 train no.102120  loss = 2.96574 avg_loss = 3.58003\n",
      "epoch no.1 train no.102130  loss = 3.01124 avg_loss = 3.59527\n",
      "epoch no.1 train no.102140  loss = 4.59153 avg_loss = 3.64486\n",
      "epoch no.1 train no.102150  loss = 3.51102 avg_loss = 3.64931\n",
      "epoch no.1 train no.102160  loss = 2.94200 avg_loss = 3.61614\n",
      "epoch no.1 train no.102170  loss = 2.28376 avg_loss = 3.61661\n",
      "epoch no.1 train no.102180  loss = 4.68602 avg_loss = 3.64267\n",
      "epoch no.1 train no.102190  loss = 3.74097 avg_loss = 3.64104\n",
      "epoch no.1 train no.102200  loss = 5.82445 avg_loss = 3.63288\n",
      "epoch no.1 train no.102210  loss = 4.23662 avg_loss = 3.71212\n",
      "epoch no.1 train no.102220  loss = 2.96240 avg_loss = 3.67691\n",
      "epoch no.1 train no.102230  loss = 4.89113 avg_loss = 3.70341\n",
      "epoch no.1 train no.102240  loss = 2.35654 avg_loss = 3.70303\n",
      "epoch no.1 train no.102250  loss = 3.68046 avg_loss = 3.74727\n",
      "epoch no.1 train no.102260  loss = 2.82356 avg_loss = 3.68085\n",
      "epoch no.1 train no.102270  loss = 2.54429 avg_loss = 3.70623\n",
      "epoch no.1 train no.102280  loss = 3.44586 avg_loss = 3.66877\n",
      "epoch no.1 train no.102290  loss = 3.80877 avg_loss = 3.60373\n",
      "epoch no.1 train no.102300  loss = 2.72845 avg_loss = 3.58478\n",
      "epoch no.1 train no.102310  loss = 4.33981 avg_loss = 3.61002\n",
      "epoch no.1 train no.102320  loss = 3.89055 avg_loss = 3.60287\n",
      "epoch no.1 train no.102330  loss = 4.86057 avg_loss = 3.60561\n",
      "epoch no.1 train no.102340  loss = 3.76044 avg_loss = 3.63491\n",
      "epoch no.1 train no.102350  loss = 1.98821 avg_loss = 3.65617\n",
      "epoch no.1 train no.102360  loss = 2.18480 avg_loss = 3.63938\n",
      "epoch no.1 train no.102370  loss = 3.74197 avg_loss = 3.66450\n",
      "epoch no.1 train no.102380  loss = 2.81644 avg_loss = 3.69439\n",
      "epoch no.1 train no.102390  loss = 6.10996 avg_loss = 3.70944\n",
      "epoch no.1 train no.102400  loss = 3.46696 avg_loss = 3.72171\n",
      "epoch no.1 train no.102410  loss = 4.23630 avg_loss = 3.73664\n",
      "epoch no.1 train no.102420  loss = 3.00775 avg_loss = 3.71101\n",
      "epoch no.1 train no.102430  loss = 6.04908 avg_loss = 3.74831\n",
      "epoch no.1 train no.102440  loss = 5.92518 avg_loss = 3.76268\n",
      "epoch no.1 train no.102450  loss = 4.00424 avg_loss = 3.74876\n",
      "epoch no.1 train no.102460  loss = 3.07038 avg_loss = 3.73965\n",
      "epoch no.1 train no.102470  loss = 2.78810 avg_loss = 3.72445\n",
      "epoch no.1 train no.102480  loss = 5.98367 avg_loss = 3.73713\n",
      "epoch no.1 train no.102490  loss = 1.57350 avg_loss = 3.77033\n",
      "epoch no.1 train no.102500  loss = 3.22292 avg_loss = 3.74988\n",
      "epoch no.1 train no.102510  loss = 3.36381 avg_loss = 3.73995\n",
      "epoch no.1 train no.102520  loss = 4.66123 avg_loss = 3.69058\n",
      "epoch no.1 train no.102530  loss = 3.56717 avg_loss = 3.68134\n",
      "epoch no.1 train no.102540  loss = 3.52015 avg_loss = 3.70136\n",
      "epoch no.1 train no.102550  loss = 3.41791 avg_loss = 3.69133\n",
      "epoch no.1 train no.102560  loss = 2.43552 avg_loss = 3.73279\n",
      "epoch no.1 train no.102570  loss = 2.60280 avg_loss = 3.67491\n",
      "epoch no.1 train no.102580  loss = 2.88544 avg_loss = 3.67110\n",
      "epoch no.1 train no.102590  loss = 4.87986 avg_loss = 3.67747\n",
      "epoch no.1 train no.102600  loss = 5.01604 avg_loss = 3.69527\n",
      "epoch no.1 train no.102610  loss = 1.98094 avg_loss = 3.61527\n",
      "epoch no.1 train no.102620  loss = 4.59692 avg_loss = 3.65704\n",
      "epoch no.1 train no.102630  loss = 2.75272 avg_loss = 3.68338\n",
      "epoch no.1 train no.102640  loss = 3.82152 avg_loss = 3.64064\n",
      "epoch no.1 train no.102650  loss = 4.98559 avg_loss = 3.64542\n",
      "epoch no.1 train no.102660  loss = 3.57487 avg_loss = 3.63535\n",
      "epoch no.1 train no.102670  loss = 3.39889 avg_loss = 3.63149\n",
      "epoch no.1 train no.102680  loss = 4.27699 avg_loss = 3.62844\n",
      "epoch no.1 train no.102690  loss = 4.49200 avg_loss = 3.63916\n",
      "epoch no.1 train no.102700  loss = 2.85186 avg_loss = 3.63688\n",
      "epoch no.1 train no.102710  loss = 2.82172 avg_loss = 3.67716\n",
      "epoch no.1 train no.102720  loss = 2.78556 avg_loss = 3.63987\n",
      "epoch no.1 train no.102730  loss = 3.67358 avg_loss = 3.63356\n",
      "epoch no.1 train no.102740  loss = 4.26013 avg_loss = 3.60312\n",
      "epoch no.1 train no.102750  loss = 4.33151 avg_loss = 3.60471\n",
      "epoch no.1 train no.102760  loss = 2.55059 avg_loss = 3.56350\n",
      "epoch no.1 train no.102770  loss = 3.60505 avg_loss = 3.62785\n",
      "epoch no.1 train no.102780  loss = 2.56104 avg_loss = 3.57577\n",
      "epoch no.1 train no.102790  loss = 5.03248 avg_loss = 3.61104\n",
      "epoch no.1 train no.102800  loss = 3.63274 avg_loss = 3.66689\n",
      "epoch no.1 train no.102810  loss = 3.06729 avg_loss = 3.68737\n",
      "epoch no.1 train no.102820  loss = 3.27465 avg_loss = 3.66744\n",
      "epoch no.1 train no.102830  loss = 1.26784 avg_loss = 3.61195\n",
      "epoch no.1 train no.102840  loss = 3.35532 avg_loss = 3.65445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.102850  loss = 2.08053 avg_loss = 3.63147\n",
      "epoch no.1 train no.102860  loss = 5.62566 avg_loss = 3.67048\n",
      "epoch no.1 train no.102870  loss = 2.57919 avg_loss = 3.63409\n",
      "epoch no.1 train no.102880  loss = 4.99056 avg_loss = 3.70401\n",
      "epoch no.1 train no.102890  loss = 4.26573 avg_loss = 3.69410\n",
      "epoch no.1 train no.102900  loss = 2.07975 avg_loss = 3.67235\n",
      "epoch no.1 train no.102910  loss = 3.39596 avg_loss = 3.66794\n",
      "epoch no.1 train no.102920  loss = 5.85929 avg_loss = 3.71683\n",
      "epoch no.1 train no.102930  loss = 5.25047 avg_loss = 3.74186\n",
      "epoch no.1 train no.102940  loss = 2.27773 avg_loss = 3.71996\n",
      "epoch no.1 train no.102950  loss = 4.03066 avg_loss = 3.74002\n",
      "epoch no.1 train no.102960  loss = 4.27128 avg_loss = 3.69162\n",
      "epoch no.1 train no.102970  loss = 3.61163 avg_loss = 3.68160\n",
      "epoch no.1 train no.102980  loss = 7.31053 avg_loss = 3.68741\n",
      "epoch no.1 train no.102990  loss = 1.92920 avg_loss = 3.63297\n",
      "epoch no.1 train no.103000  loss = 3.93879 avg_loss = 3.62140\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣는', '▁노래', '</s>']\n",
      "여름밤에 어울리는 노래</s>\n",
      "epoch no.1 train no.103010  loss = 4.06888 avg_loss = 3.64867\n",
      "epoch no.1 train no.103020  loss = 3.12104 avg_loss = 3.60515\n",
      "epoch no.1 train no.103030  loss = 4.87917 avg_loss = 3.61857\n",
      "epoch no.1 train no.103040  loss = 4.26737 avg_loss = 3.60746\n",
      "epoch no.1 train no.103050  loss = 2.89471 avg_loss = 3.62120\n",
      "epoch no.1 train no.103060  loss = 3.60738 avg_loss = 3.58793\n",
      "epoch no.1 train no.103070  loss = 2.76977 avg_loss = 3.54940\n",
      "epoch no.1 train no.103080  loss = 3.66890 avg_loss = 3.54772\n",
      "epoch no.1 train no.103090  loss = 4.62803 avg_loss = 3.58989\n",
      "epoch no.1 train no.103100  loss = 7.41857 avg_loss = 3.63305\n",
      "epoch no.1 train no.103110  loss = 4.19089 avg_loss = 3.66972\n",
      "epoch no.1 train no.103120  loss = 2.24886 avg_loss = 3.69727\n",
      "epoch no.1 train no.103130  loss = 4.24091 avg_loss = 3.73765\n",
      "epoch no.1 train no.103140  loss = 4.69800 avg_loss = 3.71410\n",
      "epoch no.1 train no.103150  loss = 3.56377 avg_loss = 3.67980\n",
      "epoch no.1 train no.103160  loss = 4.27974 avg_loss = 3.68085\n",
      "epoch no.1 train no.103170  loss = 3.06472 avg_loss = 3.67534\n",
      "epoch no.1 train no.103180  loss = 3.47992 avg_loss = 3.72816\n",
      "epoch no.1 train no.103190  loss = 4.98720 avg_loss = 3.72062\n",
      "epoch no.1 train no.103200  loss = 3.48244 avg_loss = 3.70577\n",
      "epoch no.1 train no.103210  loss = 4.14658 avg_loss = 3.69885\n",
      "epoch no.1 train no.103220  loss = 2.74906 avg_loss = 3.68482\n",
      "epoch no.1 train no.103230  loss = 4.52374 avg_loss = 3.69044\n",
      "epoch no.1 train no.103240  loss = 2.69110 avg_loss = 3.63442\n",
      "epoch no.1 train no.103250  loss = 2.89012 avg_loss = 3.59446\n",
      "epoch no.1 train no.103260  loss = 3.83455 avg_loss = 3.62048\n",
      "epoch no.1 train no.103270  loss = 3.87819 avg_loss = 3.63526\n",
      "epoch no.1 train no.103280  loss = 5.33761 avg_loss = 3.64401\n",
      "epoch no.1 train no.103290  loss = 3.66578 avg_loss = 3.59351\n",
      "epoch no.1 train no.103300  loss = 4.48364 avg_loss = 3.62183\n",
      "epoch no.1 train no.103310  loss = 1.86509 avg_loss = 3.62836\n",
      "epoch no.1 train no.103320  loss = 3.94894 avg_loss = 3.66008\n",
      "epoch no.1 train no.103330  loss = 3.53313 avg_loss = 3.66725\n",
      "epoch no.1 train no.103340  loss = 3.57641 avg_loss = 3.70804\n",
      "epoch no.1 train no.103350  loss = 3.59464 avg_loss = 3.68577\n",
      "epoch no.1 train no.103360  loss = 1.82652 avg_loss = 3.65867\n",
      "epoch no.1 train no.103370  loss = 2.41514 avg_loss = 3.70890\n",
      "epoch no.1 train no.103380  loss = 5.90599 avg_loss = 3.80509\n",
      "epoch no.1 train no.103390  loss = 4.84536 avg_loss = 3.78456\n",
      "epoch no.1 train no.103400  loss = 3.70873 avg_loss = 3.82363\n",
      "epoch no.1 train no.103410  loss = 3.97735 avg_loss = 3.81471\n",
      "epoch no.1 train no.103420  loss = 5.46386 avg_loss = 3.85200\n",
      "epoch no.1 train no.103430  loss = 4.79940 avg_loss = 3.84437\n",
      "epoch no.1 train no.103440  loss = 4.11748 avg_loss = 3.83165\n",
      "epoch no.1 train no.103450  loss = 4.83722 avg_loss = 3.77804\n",
      "epoch no.1 train no.103460  loss = 2.24141 avg_loss = 3.73527\n",
      "epoch no.1 train no.103470  loss = 3.06440 avg_loss = 3.75712\n",
      "epoch no.1 train no.103480  loss = 6.43801 avg_loss = 3.72917\n",
      "epoch no.1 train no.103490  loss = 3.74112 avg_loss = 3.75250\n",
      "epoch no.1 train no.103500  loss = 2.25391 avg_loss = 3.75733\n",
      "epoch no.1 train no.103510  loss = 4.37904 avg_loss = 3.79670\n",
      "epoch no.1 train no.103520  loss = 2.66878 avg_loss = 3.77954\n",
      "epoch no.1 train no.103530  loss = 2.85952 avg_loss = 3.74887\n",
      "epoch no.1 train no.103540  loss = 2.85095 avg_loss = 3.70761\n",
      "epoch no.1 train no.103550  loss = 3.11982 avg_loss = 3.70155\n",
      "epoch no.1 train no.103560  loss = 3.96024 avg_loss = 3.69736\n",
      "epoch no.1 train no.103570  loss = 2.79045 avg_loss = 3.70343\n",
      "epoch no.1 train no.103580  loss = 6.73550 avg_loss = 3.80469\n",
      "epoch no.1 train no.103590  loss = 6.31906 avg_loss = 3.83727\n",
      "epoch no.1 train no.103600  loss = 1.89317 avg_loss = 3.81831\n",
      "epoch no.1 train no.103610  loss = 6.32584 avg_loss = 3.84622\n",
      "epoch no.1 train no.103620  loss = 3.44031 avg_loss = 3.83216\n",
      "epoch no.1 train no.103630  loss = 4.16592 avg_loss = 3.82361\n",
      "epoch no.1 train no.103640  loss = 3.54479 avg_loss = 3.86214\n",
      "epoch no.1 train no.103650  loss = 5.08120 avg_loss = 3.86934\n",
      "epoch no.1 train no.103660  loss = 3.63974 avg_loss = 3.83422\n",
      "epoch no.1 train no.103670  loss = 4.66051 avg_loss = 3.80687\n",
      "epoch no.1 train no.103680  loss = 3.01329 avg_loss = 3.84368\n",
      "epoch no.1 train no.103690  loss = 2.98402 avg_loss = 3.80241\n",
      "epoch no.1 train no.103700  loss = 2.88947 avg_loss = 3.76545\n",
      "epoch no.1 train no.103710  loss = 5.08974 avg_loss = 3.79904\n",
      "epoch no.1 train no.103720  loss = 1.97173 avg_loss = 3.74725\n",
      "epoch no.1 train no.103730  loss = 2.33321 avg_loss = 3.78266\n",
      "epoch no.1 train no.103740  loss = 4.58618 avg_loss = 3.83300\n",
      "epoch no.1 train no.103750  loss = 2.99817 avg_loss = 3.84657\n",
      "epoch no.1 train no.103760  loss = 4.40266 avg_loss = 3.82629\n",
      "epoch no.1 train no.103770  loss = 3.39126 avg_loss = 3.82996\n",
      "epoch no.1 train no.103780  loss = 2.22006 avg_loss = 3.78429\n",
      "epoch no.1 train no.103790  loss = 2.14718 avg_loss = 3.75992\n",
      "epoch no.1 train no.103800  loss = 3.49400 avg_loss = 3.76895\n",
      "epoch no.1 train no.103810  loss = 4.60663 avg_loss = 3.77835\n",
      "epoch no.1 train no.103820  loss = 4.25040 avg_loss = 3.75286\n",
      "epoch no.1 train no.103830  loss = 3.28288 avg_loss = 3.78162\n",
      "epoch no.1 train no.103840  loss = 5.87691 avg_loss = 3.77076\n",
      "epoch no.1 train no.103850  loss = 2.19223 avg_loss = 3.79649\n",
      "epoch no.1 train no.103860  loss = 5.82167 avg_loss = 3.79120\n",
      "epoch no.1 train no.103870  loss = 4.02647 avg_loss = 3.81297\n",
      "epoch no.1 train no.103880  loss = 3.39287 avg_loss = 3.72830\n",
      "epoch no.1 train no.103890  loss = 4.17667 avg_loss = 3.67947\n",
      "epoch no.1 train no.103900  loss = 3.74891 avg_loss = 3.72561\n",
      "epoch no.1 train no.103910  loss = 5.12854 avg_loss = 3.71032\n",
      "epoch no.1 train no.103920  loss = 4.69699 avg_loss = 3.75347\n",
      "epoch no.1 train no.103930  loss = 4.46553 avg_loss = 3.77962\n",
      "epoch no.1 train no.103940  loss = 2.40590 avg_loss = 3.76619\n",
      "epoch no.1 train no.103950  loss = 4.92351 avg_loss = 3.75922\n",
      "epoch no.1 train no.103960  loss = 3.51733 avg_loss = 3.67400\n",
      "epoch no.1 train no.103970  loss = 4.65890 avg_loss = 3.66837\n",
      "epoch no.1 train no.103980  loss = 3.32972 avg_loss = 3.63651\n",
      "epoch no.1 train no.103990  loss = 2.21192 avg_loss = 3.65378\n",
      "epoch no.1 train no.104000  loss = 1.99403 avg_loss = 3.63151\n",
      "10\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '에서', '락', '에서', '▁듣는', '오는', '▁음악', '적인', '</s>']\n",
      "여름밤의 끝자락에서 흘러나오는 감성 음악</s>\n",
      "epoch no.1 train no.104010  loss = 3.53440 avg_loss = 3.58756\n",
      "epoch no.1 train no.104020  loss = 3.17685 avg_loss = 3.60335\n",
      "epoch no.1 train no.104030  loss = 2.42608 avg_loss = 3.59818\n",
      "epoch no.1 train no.104040  loss = 2.24741 avg_loss = 3.62685\n",
      "epoch no.1 train no.104050  loss = 4.16288 avg_loss = 3.65436\n",
      "epoch no.1 train no.104060  loss = 3.32745 avg_loss = 3.74123\n",
      "epoch no.1 train no.104070  loss = 4.72772 avg_loss = 3.73983\n",
      "epoch no.1 train no.104080  loss = 4.13239 avg_loss = 3.70990\n",
      "epoch no.1 train no.104090  loss = 3.41002 avg_loss = 3.66838\n",
      "epoch no.1 train no.104100  loss = 3.56372 avg_loss = 3.70256\n",
      "epoch no.1 train no.104110  loss = 2.84868 avg_loss = 3.70668\n",
      "epoch no.1 train no.104120  loss = 2.90035 avg_loss = 3.70540\n",
      "epoch no.1 train no.104130  loss = 4.88449 avg_loss = 3.76011\n",
      "epoch no.1 train no.104140  loss = 2.94895 avg_loss = 3.70840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.104150  loss = 3.94117 avg_loss = 3.70311\n",
      "epoch no.1 train no.104160  loss = 3.28709 avg_loss = 3.68522\n",
      "epoch no.1 train no.104170  loss = 3.43898 avg_loss = 3.67639\n",
      "epoch no.1 train no.104180  loss = 4.01370 avg_loss = 3.68120\n",
      "epoch no.1 train no.104190  loss = 4.95943 avg_loss = 3.69833\n",
      "epoch no.1 train no.104200  loss = 3.85107 avg_loss = 3.67913\n",
      "epoch no.1 train no.104210  loss = 4.50707 avg_loss = 3.74702\n",
      "epoch no.1 train no.104220  loss = 4.88168 avg_loss = 3.72949\n",
      "epoch no.1 train no.104230  loss = 1.96973 avg_loss = 3.68444\n",
      "epoch no.1 train no.104240  loss = 2.88867 avg_loss = 3.68131\n",
      "epoch no.1 train no.104250  loss = 2.49814 avg_loss = 3.70850\n",
      "epoch no.1 train no.104260  loss = 4.19252 avg_loss = 3.65918\n",
      "epoch no.1 train no.104270  loss = 3.79729 avg_loss = 3.65484\n",
      "epoch no.1 train no.104280  loss = 2.86542 avg_loss = 3.61615\n",
      "epoch no.1 train no.104290  loss = 2.97987 avg_loss = 3.60519\n",
      "epoch no.1 train no.104300  loss = 2.95375 avg_loss = 3.59848\n",
      "epoch no.1 train no.104310  loss = 2.55408 avg_loss = 3.59980\n",
      "epoch no.1 train no.104320  loss = 3.62732 avg_loss = 3.61466\n",
      "epoch no.1 train no.104330  loss = 4.11196 avg_loss = 3.63086\n",
      "epoch no.1 train no.104340  loss = 4.25269 avg_loss = 3.70008\n",
      "epoch no.1 train no.104350  loss = 3.70736 avg_loss = 3.69156\n",
      "epoch no.1 train no.104360  loss = 4.46110 avg_loss = 3.68883\n",
      "epoch no.1 train no.104370  loss = 3.35987 avg_loss = 3.74731\n",
      "epoch no.1 train no.104380  loss = 2.41767 avg_loss = 3.68230\n",
      "epoch no.1 train no.104390  loss = 3.04031 avg_loss = 3.66472\n",
      "epoch no.1 train no.104400  loss = 2.76119 avg_loss = 3.65812\n",
      "epoch no.1 train no.104410  loss = 4.28374 avg_loss = 3.70562\n",
      "epoch no.1 train no.104420  loss = 2.49757 avg_loss = 3.62238\n",
      "epoch no.1 train no.104430  loss = 2.61287 avg_loss = 3.62531\n",
      "epoch no.1 train no.104440  loss = 3.02690 avg_loss = 3.64021\n",
      "epoch no.1 train no.104450  loss = 2.91469 avg_loss = 3.67719\n",
      "epoch no.1 train no.104460  loss = 2.99632 avg_loss = 3.63983\n",
      "epoch no.1 train no.104470  loss = 5.17493 avg_loss = 3.64617\n",
      "epoch no.1 train no.104480  loss = 4.60583 avg_loss = 3.56593\n",
      "epoch no.1 train no.104490  loss = 4.40505 avg_loss = 3.56641\n",
      "epoch no.1 train no.104500  loss = 2.38446 avg_loss = 3.53673\n",
      "epoch no.1 train no.104510  loss = 4.01549 avg_loss = 3.58352\n",
      "epoch no.1 train no.104520  loss = 4.54240 avg_loss = 3.59920\n",
      "epoch no.1 train no.104530  loss = 3.94668 avg_loss = 3.61594\n",
      "epoch no.1 train no.104540  loss = 1.93904 avg_loss = 3.60772\n",
      "epoch no.1 train no.104550  loss = 4.15119 avg_loss = 3.63633\n",
      "epoch no.1 train no.104560  loss = 1.71199 avg_loss = 3.60251\n",
      "epoch no.1 train no.104570  loss = 4.06801 avg_loss = 3.55443\n",
      "epoch no.1 train no.104580  loss = 3.01437 avg_loss = 3.53888\n",
      "epoch no.1 train no.104590  loss = 3.77878 avg_loss = 3.52222\n",
      "epoch no.1 train no.104600  loss = 3.60065 avg_loss = 3.54862\n",
      "epoch no.1 train no.104610  loss = 3.88455 avg_loss = 3.58967\n",
      "epoch no.1 train no.104620  loss = 6.50560 avg_loss = 3.64707\n",
      "epoch no.1 train no.104630  loss = 3.20379 avg_loss = 3.69919\n",
      "epoch no.1 train no.104640  loss = 2.88503 avg_loss = 3.76112\n",
      "epoch no.1 train no.104650  loss = 3.07391 avg_loss = 3.78489\n",
      "epoch no.1 train no.104660  loss = 4.05115 avg_loss = 3.79934\n",
      "epoch no.1 train no.104670  loss = 3.51930 avg_loss = 3.77440\n",
      "epoch no.1 train no.104680  loss = 3.03396 avg_loss = 3.71954\n",
      "epoch no.1 train no.104690  loss = 6.75005 avg_loss = 3.76935\n",
      "epoch no.1 train no.104700  loss = 4.43319 avg_loss = 3.71901\n",
      "epoch no.1 train no.104710  loss = 3.05842 avg_loss = 3.68806\n",
      "epoch no.1 train no.104720  loss = 3.26777 avg_loss = 3.69974\n",
      "epoch no.1 train no.104730  loss = 5.58767 avg_loss = 3.70550\n",
      "epoch no.1 train no.104740  loss = 3.19184 avg_loss = 3.67821\n",
      "epoch no.1 train no.104750  loss = 3.15661 avg_loss = 3.64746\n",
      "epoch no.1 train no.104760  loss = 4.06412 avg_loss = 3.66202\n",
      "epoch no.1 train no.104770  loss = 2.66501 avg_loss = 3.70242\n",
      "epoch no.1 train no.104780  loss = 5.31630 avg_loss = 3.65536\n",
      "epoch no.1 train no.104790  loss = 4.33882 avg_loss = 3.74486\n",
      "epoch no.1 train no.104800  loss = 5.24361 avg_loss = 3.70827\n",
      "epoch no.1 train no.104810  loss = 3.75412 avg_loss = 3.72189\n",
      "epoch no.1 train no.104820  loss = 3.24287 avg_loss = 3.75312\n",
      "epoch no.1 train no.104830  loss = 5.69302 avg_loss = 3.76365\n",
      "epoch no.1 train no.104840  loss = 3.44745 avg_loss = 3.78216\n",
      "epoch no.1 train no.104850  loss = 4.39820 avg_loss = 3.81902\n",
      "epoch no.1 train no.104860  loss = 2.58085 avg_loss = 3.82884\n",
      "epoch no.1 train no.104870  loss = 4.34080 avg_loss = 3.86487\n",
      "epoch no.1 train no.104880  loss = 3.27665 avg_loss = 3.84111\n",
      "epoch no.1 train no.104890  loss = 3.46770 avg_loss = 3.80693\n",
      "epoch no.1 train no.104900  loss = 2.86641 avg_loss = 3.79547\n",
      "epoch no.1 train no.104910  loss = 3.73517 avg_loss = 3.79596\n",
      "epoch no.1 train no.104920  loss = 3.83545 avg_loss = 3.78841\n",
      "epoch no.1 train no.104930  loss = 3.86431 avg_loss = 3.82847\n",
      "epoch no.1 train no.104940  loss = 3.82199 avg_loss = 3.76795\n",
      "epoch no.1 train no.104950  loss = 2.43417 avg_loss = 3.74526\n",
      "epoch no.1 train no.104960  loss = 3.62740 avg_loss = 3.70177\n",
      "epoch no.1 train no.104970  loss = 3.31190 avg_loss = 3.75799\n",
      "epoch no.1 train no.104980  loss = 2.35479 avg_loss = 3.74568\n",
      "epoch no.1 train no.104990  loss = 3.05684 avg_loss = 3.72031\n",
      "epoch no.1 train no.105000  loss = 3.64799 avg_loss = 3.73623\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '▁시원한', '▁여름', 'mmer', '▁p', 'op', '</s>']\n",
      "여름맞이 시원한 summer pop</s>\n",
      "epoch no.1 train no.105010  loss = 2.24843 avg_loss = 3.75483\n",
      "epoch no.1 train no.105020  loss = 4.39951 avg_loss = 3.78018\n",
      "epoch no.1 train no.105030  loss = 3.07539 avg_loss = 3.74108\n",
      "epoch no.1 train no.105040  loss = 5.62070 avg_loss = 3.75623\n",
      "epoch no.1 train no.105050  loss = 5.39956 avg_loss = 3.75539\n",
      "epoch no.1 train no.105060  loss = 2.74526 avg_loss = 3.72666\n",
      "epoch no.1 train no.105070  loss = 3.21716 avg_loss = 3.70243\n",
      "epoch no.1 train no.105080  loss = 3.89616 avg_loss = 3.66988\n",
      "epoch no.1 train no.105090  loss = 2.55920 avg_loss = 3.66435\n",
      "epoch no.1 train no.105100  loss = 3.03970 avg_loss = 3.69525\n",
      "epoch no.1 train no.105110  loss = 5.74711 avg_loss = 3.70429\n",
      "epoch no.1 train no.105120  loss = 4.37564 avg_loss = 3.71514\n",
      "epoch no.1 train no.105130  loss = 1.80333 avg_loss = 3.70346\n",
      "epoch no.1 train no.105140  loss = 3.12433 avg_loss = 3.75718\n",
      "epoch no.1 train no.105150  loss = 3.80775 avg_loss = 3.73549\n",
      "epoch no.1 train no.105160  loss = 4.49300 avg_loss = 3.69001\n",
      "epoch no.1 train no.105170  loss = 4.08784 avg_loss = 3.62033\n",
      "epoch no.1 train no.105180  loss = 6.20830 avg_loss = 3.66555\n",
      "epoch no.1 train no.105190  loss = 4.45980 avg_loss = 3.65166\n",
      "epoch no.1 train no.105200  loss = 2.95115 avg_loss = 3.67492\n",
      "epoch no.1 train no.105210  loss = 2.09673 avg_loss = 3.64687\n",
      "epoch no.1 train no.105220  loss = 3.48799 avg_loss = 3.64669\n",
      "epoch no.1 train no.105230  loss = 1.93420 avg_loss = 3.64443\n",
      "epoch no.1 train no.105240  loss = 2.60148 avg_loss = 3.58687\n",
      "epoch no.1 train no.105250  loss = 3.07884 avg_loss = 3.57710\n",
      "epoch no.1 train no.105260  loss = 1.80991 avg_loss = 3.55948\n",
      "epoch no.1 train no.105270  loss = 6.56369 avg_loss = 3.64498\n",
      "epoch no.1 train no.105280  loss = 4.00482 avg_loss = 3.59235\n",
      "epoch no.1 train no.105290  loss = 2.47314 avg_loss = 3.57562\n",
      "epoch no.1 train no.105300  loss = 2.79271 avg_loss = 3.59056\n",
      "epoch no.1 train no.105310  loss = 4.44603 avg_loss = 3.55249\n",
      "epoch no.1 train no.105320  loss = 1.68597 avg_loss = 3.54594\n",
      "epoch no.1 train no.105330  loss = 5.26931 avg_loss = 3.56471\n",
      "epoch no.1 train no.105340  loss = 4.56777 avg_loss = 3.53416\n",
      "epoch no.1 train no.105350  loss = 3.23158 avg_loss = 3.53689\n",
      "epoch no.1 train no.105360  loss = 4.01634 avg_loss = 3.61722\n",
      "epoch no.1 train no.105370  loss = 3.99745 avg_loss = 3.66587\n",
      "epoch no.1 train no.105380  loss = 4.57177 avg_loss = 3.64280\n",
      "epoch no.1 train no.105390  loss = 3.01422 avg_loss = 3.61468\n",
      "epoch no.1 train no.105400  loss = 2.51688 avg_loss = 3.63254\n",
      "epoch no.1 train no.105410  loss = 4.31516 avg_loss = 3.67978\n",
      "epoch no.1 train no.105420  loss = 4.79046 avg_loss = 3.71506\n",
      "epoch no.1 train no.105430  loss = 5.40239 avg_loss = 3.72373\n",
      "epoch no.1 train no.105440  loss = 5.45922 avg_loss = 3.74970\n",
      "epoch no.1 train no.105450  loss = 3.65030 avg_loss = 3.79591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.105460  loss = 2.79899 avg_loss = 3.79992\n",
      "epoch no.1 train no.105470  loss = 4.91589 avg_loss = 3.77627\n",
      "epoch no.1 train no.105480  loss = 2.08482 avg_loss = 3.75935\n",
      "epoch no.1 train no.105490  loss = 3.83615 avg_loss = 3.78506\n",
      "epoch no.1 train no.105500  loss = 6.23909 avg_loss = 3.79772\n",
      "epoch no.1 train no.105510  loss = 7.24408 avg_loss = 3.79650\n",
      "epoch no.1 train no.105520  loss = 4.60201 avg_loss = 3.80853\n",
      "epoch no.1 train no.105530  loss = 4.25781 avg_loss = 3.84544\n",
      "epoch no.1 train no.105540  loss = 3.56336 avg_loss = 3.89933\n",
      "epoch no.1 train no.105550  loss = 2.25617 avg_loss = 3.85755\n",
      "epoch no.1 train no.105560  loss = 2.94007 avg_loss = 3.81739\n",
      "epoch no.1 train no.105570  loss = 6.02064 avg_loss = 3.84044\n",
      "epoch no.1 train no.105580  loss = 1.90025 avg_loss = 3.84114\n",
      "epoch no.1 train no.105590  loss = 1.99277 avg_loss = 3.80992\n",
      "epoch no.1 train no.105600  loss = 4.95716 avg_loss = 3.81130\n",
      "epoch no.1 train no.105610  loss = 4.91820 avg_loss = 3.80669\n",
      "epoch no.1 train no.105620  loss = 3.39123 avg_loss = 3.78703\n",
      "epoch no.1 train no.105630  loss = 3.65342 avg_loss = 3.80979\n",
      "epoch no.1 train no.105640  loss = 4.04713 avg_loss = 3.83397\n",
      "epoch no.1 train no.105650  loss = 6.50729 avg_loss = 3.86087\n",
      "epoch no.1 train no.105660  loss = 3.96716 avg_loss = 3.80701\n",
      "epoch no.1 train no.105670  loss = 3.18799 avg_loss = 3.77542\n",
      "epoch no.1 train no.105680  loss = 4.44667 avg_loss = 3.79644\n",
      "epoch no.1 train no.105690  loss = 4.08752 avg_loss = 3.79931\n",
      "epoch no.1 train no.105700  loss = 3.75349 avg_loss = 3.76288\n",
      "epoch no.1 train no.105710  loss = 4.80520 avg_loss = 3.70504\n",
      "epoch no.1 train no.105720  loss = 4.88694 avg_loss = 3.68883\n",
      "epoch no.1 train no.105730  loss = 3.63802 avg_loss = 3.68051\n",
      "epoch no.1 train no.105740  loss = 2.12263 avg_loss = 3.63023\n",
      "epoch no.1 train no.105750  loss = 3.01788 avg_loss = 3.64333\n",
      "epoch no.1 train no.105760  loss = 3.47920 avg_loss = 3.69628\n",
      "epoch no.1 train no.105770  loss = 3.53899 avg_loss = 3.68530\n",
      "epoch no.1 train no.105780  loss = 2.15439 avg_loss = 3.71077\n",
      "epoch no.1 train no.105790  loss = 2.80214 avg_loss = 3.71262\n",
      "epoch no.1 train no.105800  loss = 4.10254 avg_loss = 3.76308\n",
      "epoch no.1 train no.105810  loss = 3.13306 avg_loss = 3.75457\n",
      "epoch no.1 train no.105820  loss = 3.14281 avg_loss = 3.71361\n",
      "epoch no.1 train no.105830  loss = 3.74482 avg_loss = 3.69638\n",
      "epoch no.1 train no.105840  loss = 3.06122 avg_loss = 3.68029\n",
      "epoch no.1 train no.105850  loss = 3.41944 avg_loss = 3.67518\n",
      "epoch no.1 train no.105860  loss = 4.49411 avg_loss = 3.69387\n",
      "epoch no.1 train no.105870  loss = 2.87626 avg_loss = 3.71334\n",
      "epoch no.1 train no.105880  loss = 5.61051 avg_loss = 3.69903\n",
      "epoch no.1 train no.105890  loss = 2.11901 avg_loss = 3.72597\n",
      "epoch no.1 train no.105900  loss = 4.79644 avg_loss = 3.71186\n",
      "epoch no.1 train no.105910  loss = 3.79975 avg_loss = 3.75104\n",
      "epoch no.1 train no.105920  loss = 3.04152 avg_loss = 3.78260\n",
      "epoch no.1 train no.105930  loss = 4.63133 avg_loss = 3.76455\n",
      "epoch no.1 train no.105940  loss = 2.70999 avg_loss = 3.75888\n",
      "epoch no.1 train no.105950  loss = 2.22164 avg_loss = 3.78502\n",
      "epoch no.1 train no.105960  loss = 4.99963 avg_loss = 3.84460\n",
      "epoch no.1 train no.105970  loss = 4.39409 avg_loss = 3.84887\n",
      "epoch no.1 train no.105980  loss = 1.90601 avg_loss = 3.81544\n",
      "epoch no.1 train no.105990  loss = 4.17549 avg_loss = 3.72957\n",
      "epoch no.1 train no.106000  loss = 4.20552 avg_loss = 3.73030\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁플레이', '보', '리스트', '</s>']\n",
      "여름밤의 재즈 플레이리스트</s>\n",
      "epoch no.1 train no.106010  loss = 4.12396 avg_loss = 3.68423\n",
      "epoch no.1 train no.106020  loss = 2.50718 avg_loss = 3.70048\n",
      "epoch no.1 train no.106030  loss = 3.34832 avg_loss = 3.74544\n",
      "epoch no.1 train no.106040  loss = 4.19116 avg_loss = 3.76625\n",
      "epoch no.1 train no.106050  loss = 5.10964 avg_loss = 3.79267\n",
      "epoch no.1 train no.106060  loss = 3.75713 avg_loss = 3.71486\n",
      "epoch no.1 train no.106070  loss = 5.12490 avg_loss = 3.79926\n",
      "epoch no.1 train no.106080  loss = 6.09825 avg_loss = 3.79825\n",
      "epoch no.1 train no.106090  loss = 4.80957 avg_loss = 3.84026\n",
      "epoch no.1 train no.106100  loss = 1.56884 avg_loss = 3.81531\n",
      "epoch no.1 train no.106110  loss = 6.20970 avg_loss = 3.88514\n",
      "epoch no.1 train no.106120  loss = 3.96610 avg_loss = 3.86335\n",
      "epoch no.1 train no.106130  loss = 4.90987 avg_loss = 3.86031\n",
      "epoch no.1 train no.106140  loss = 4.94397 avg_loss = 3.77106\n",
      "epoch no.1 train no.106150  loss = 2.62076 avg_loss = 3.77647\n",
      "epoch no.1 train no.106160  loss = 3.50744 avg_loss = 3.72106\n",
      "epoch no.1 train no.106170  loss = 2.78972 avg_loss = 3.75439\n",
      "epoch no.1 train no.106180  loss = 4.36274 avg_loss = 3.72314\n",
      "epoch no.1 train no.106190  loss = 4.61561 avg_loss = 3.75390\n",
      "epoch no.1 train no.106200  loss = 2.93622 avg_loss = 3.73115\n",
      "epoch no.1 train no.106210  loss = 3.23854 avg_loss = 3.73159\n",
      "epoch no.1 train no.106220  loss = 1.81342 avg_loss = 3.71199\n",
      "epoch no.1 train no.106230  loss = 3.27928 avg_loss = 3.67938\n",
      "epoch no.1 train no.106240  loss = 6.50782 avg_loss = 3.69634\n",
      "epoch no.1 train no.106250  loss = 2.45652 avg_loss = 3.64532\n",
      "epoch no.1 train no.106260  loss = 6.49908 avg_loss = 3.68920\n",
      "epoch no.1 train no.106270  loss = 3.92071 avg_loss = 3.66857\n",
      "epoch no.1 train no.106280  loss = 4.21720 avg_loss = 3.62097\n",
      "epoch no.1 train no.106290  loss = 4.94912 avg_loss = 3.61602\n",
      "epoch no.1 train no.106300  loss = 3.11969 avg_loss = 3.60397\n",
      "epoch no.1 train no.106310  loss = 2.41500 avg_loss = 3.53935\n",
      "epoch no.1 train no.106320  loss = 2.49129 avg_loss = 3.55064\n",
      "epoch no.1 train no.106330  loss = 3.14537 avg_loss = 3.56280\n",
      "epoch no.1 train no.106340  loss = 3.04984 avg_loss = 3.53413\n",
      "epoch no.1 train no.106350  loss = 5.18280 avg_loss = 3.62283\n",
      "epoch no.1 train no.106360  loss = 2.51946 avg_loss = 3.66911\n",
      "epoch no.1 train no.106370  loss = 2.37844 avg_loss = 3.62276\n",
      "epoch no.1 train no.106380  loss = 3.77455 avg_loss = 3.57442\n",
      "epoch no.1 train no.106390  loss = 4.14688 avg_loss = 3.59056\n",
      "epoch no.1 train no.106400  loss = 4.73177 avg_loss = 3.63776\n",
      "epoch no.1 train no.106410  loss = 4.05577 avg_loss = 3.59009\n",
      "epoch no.1 train no.106420  loss = 3.02743 avg_loss = 3.59960\n",
      "epoch no.1 train no.106430  loss = 3.58451 avg_loss = 3.66796\n",
      "epoch no.1 train no.106440  loss = 4.26504 avg_loss = 3.68457\n",
      "epoch no.1 train no.106450  loss = 3.58114 avg_loss = 3.69232\n",
      "epoch no.1 train no.106460  loss = 2.90645 avg_loss = 3.67416\n",
      "epoch no.1 train no.106470  loss = 4.57286 avg_loss = 3.64643\n",
      "epoch no.1 train no.106480  loss = 3.31311 avg_loss = 3.66708\n",
      "epoch no.1 train no.106490  loss = 3.28703 avg_loss = 3.67107\n",
      "epoch no.1 train no.106500  loss = 3.59751 avg_loss = 3.67981\n",
      "epoch no.1 train no.106510  loss = 5.37173 avg_loss = 3.64828\n",
      "epoch no.1 train no.106520  loss = 5.65533 avg_loss = 3.64031\n",
      "epoch no.1 train no.106530  loss = 2.98685 avg_loss = 3.63005\n",
      "epoch no.1 train no.106540  loss = 4.09247 avg_loss = 3.60101\n",
      "epoch no.1 train no.106550  loss = 4.32391 avg_loss = 3.64461\n",
      "epoch no.1 train no.106560  loss = 2.99694 avg_loss = 3.61912\n",
      "epoch no.1 train no.106570  loss = 4.88695 avg_loss = 3.63681\n",
      "epoch no.1 train no.106580  loss = 3.99092 avg_loss = 3.67008\n",
      "epoch no.1 train no.106590  loss = 4.55872 avg_loss = 3.69707\n",
      "epoch no.1 train no.106600  loss = 3.71994 avg_loss = 3.71195\n",
      "epoch no.1 train no.106610  loss = 3.75504 avg_loss = 3.73645\n",
      "epoch no.1 train no.106620  loss = 3.71088 avg_loss = 3.70170\n",
      "epoch no.1 train no.106630  loss = 3.52980 avg_loss = 3.67203\n",
      "epoch no.1 train no.106640  loss = 2.47147 avg_loss = 3.62461\n",
      "epoch no.1 train no.106650  loss = 3.24678 avg_loss = 3.72556\n",
      "epoch no.1 train no.106660  loss = 4.78175 avg_loss = 3.77947\n",
      "epoch no.1 train no.106670  loss = 3.82397 avg_loss = 3.74051\n",
      "epoch no.1 train no.106680  loss = 4.73054 avg_loss = 3.81182\n",
      "epoch no.1 train no.106690  loss = 3.22216 avg_loss = 3.81706\n",
      "epoch no.1 train no.106700  loss = 5.68588 avg_loss = 3.81174\n",
      "epoch no.1 train no.106710  loss = 2.21974 avg_loss = 3.77155\n",
      "epoch no.1 train no.106720  loss = 2.54183 avg_loss = 3.73967\n",
      "epoch no.1 train no.106730  loss = 3.10497 avg_loss = 3.71381\n",
      "epoch no.1 train no.106740  loss = 4.01516 avg_loss = 3.72340\n",
      "epoch no.1 train no.106750  loss = 3.27893 avg_loss = 3.69846\n",
      "epoch no.1 train no.106760  loss = 1.91841 avg_loss = 3.67272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.106770  loss = 3.62027 avg_loss = 3.71252\n",
      "epoch no.1 train no.106780  loss = 2.17043 avg_loss = 3.64600\n",
      "epoch no.1 train no.106790  loss = 3.99337 avg_loss = 3.63079\n",
      "epoch no.1 train no.106800  loss = 4.71676 avg_loss = 3.67638\n",
      "epoch no.1 train no.106810  loss = 2.47844 avg_loss = 3.66129\n",
      "epoch no.1 train no.106820  loss = 2.06487 avg_loss = 3.67501\n",
      "epoch no.1 train no.106830  loss = 3.70838 avg_loss = 3.69887\n",
      "epoch no.1 train no.106840  loss = 2.74892 avg_loss = 3.72682\n",
      "epoch no.1 train no.106850  loss = 5.14348 avg_loss = 3.76968\n",
      "epoch no.1 train no.106860  loss = 3.18108 avg_loss = 3.68339\n",
      "epoch no.1 train no.106870  loss = 4.88098 avg_loss = 3.68958\n",
      "epoch no.1 train no.106880  loss = 2.16314 avg_loss = 3.66112\n",
      "epoch no.1 train no.106890  loss = 2.91635 avg_loss = 3.70110\n",
      "epoch no.1 train no.106900  loss = 3.82170 avg_loss = 3.70684\n",
      "epoch no.1 train no.106910  loss = 4.06271 avg_loss = 3.71592\n",
      "epoch no.1 train no.106920  loss = 2.42857 avg_loss = 3.69989\n",
      "epoch no.1 train no.106930  loss = 4.58754 avg_loss = 3.66672\n",
      "epoch no.1 train no.106940  loss = 4.07734 avg_loss = 3.63386\n",
      "epoch no.1 train no.106950  loss = 2.91126 avg_loss = 3.61719\n",
      "epoch no.1 train no.106960  loss = 3.23709 avg_loss = 3.61112\n",
      "epoch no.1 train no.106970  loss = 2.54945 avg_loss = 3.63173\n",
      "epoch no.1 train no.106980  loss = 4.04659 avg_loss = 3.60777\n",
      "epoch no.1 train no.106990  loss = 4.60038 avg_loss = 3.65327\n",
      "epoch no.1 train no.107000  loss = 4.83161 avg_loss = 3.65098\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '하며', '때', '▁듣기', '▁노래', '</s>']\n",
      "여름밤 드라이브 할 때 듣는 음악</s>\n",
      "epoch no.1 train no.107010  loss = 2.87164 avg_loss = 3.65801\n",
      "epoch no.1 train no.107020  loss = 3.53203 avg_loss = 3.65904\n",
      "epoch no.1 train no.107030  loss = 2.38920 avg_loss = 3.64913\n",
      "epoch no.1 train no.107040  loss = 4.26637 avg_loss = 3.66218\n",
      "epoch no.1 train no.107050  loss = 3.59070 avg_loss = 3.66939\n",
      "epoch no.1 train no.107060  loss = 3.09533 avg_loss = 3.67326\n",
      "epoch no.1 train no.107070  loss = 3.26492 avg_loss = 3.61354\n",
      "epoch no.1 train no.107080  loss = 4.14699 avg_loss = 3.63310\n",
      "epoch no.1 train no.107090  loss = 2.63111 avg_loss = 3.65629\n",
      "epoch no.1 train no.107100  loss = 6.05900 avg_loss = 3.73368\n",
      "epoch no.1 train no.107110  loss = 4.13467 avg_loss = 3.77275\n",
      "epoch no.1 train no.107120  loss = 3.40746 avg_loss = 3.81365\n",
      "epoch no.1 train no.107130  loss = 3.53962 avg_loss = 3.79965\n",
      "epoch no.1 train no.107140  loss = 5.14331 avg_loss = 3.84386\n",
      "epoch no.1 train no.107150  loss = 2.60465 avg_loss = 3.78878\n",
      "epoch no.1 train no.107160  loss = 5.28943 avg_loss = 3.76875\n",
      "epoch no.1 train no.107170  loss = 3.26125 avg_loss = 3.77073\n",
      "epoch no.1 train no.107180  loss = 2.25227 avg_loss = 3.78356\n",
      "epoch no.1 train no.107190  loss = 3.93669 avg_loss = 3.71433\n",
      "epoch no.1 train no.107200  loss = 3.48433 avg_loss = 3.70817\n",
      "epoch no.1 train no.107210  loss = 2.76902 avg_loss = 3.66145\n",
      "epoch no.1 train no.107220  loss = 2.37031 avg_loss = 3.72013\n",
      "epoch no.1 train no.107230  loss = 5.03060 avg_loss = 3.71080\n",
      "epoch no.1 train no.107240  loss = 4.24681 avg_loss = 3.69947\n",
      "epoch no.1 train no.107250  loss = 3.62452 avg_loss = 3.71052\n",
      "epoch no.1 train no.107260  loss = 3.24378 avg_loss = 3.73272\n",
      "epoch no.1 train no.107270  loss = 4.67385 avg_loss = 3.69859\n",
      "epoch no.1 train no.107280  loss = 3.06521 avg_loss = 3.66581\n",
      "epoch no.1 train no.107290  loss = 3.78710 avg_loss = 3.67708\n",
      "epoch no.1 train no.107300  loss = 2.47656 avg_loss = 3.65289\n",
      "epoch no.1 train no.107310  loss = 2.69874 avg_loss = 3.69165\n",
      "epoch no.1 train no.107320  loss = 2.56688 avg_loss = 3.65716\n",
      "epoch no.1 train no.107330  loss = 2.75287 avg_loss = 3.62777\n",
      "epoch no.1 train no.107340  loss = 3.00827 avg_loss = 3.62224\n",
      "epoch no.1 train no.107350  loss = 1.97119 avg_loss = 3.63533\n",
      "epoch no.1 train no.107360  loss = 3.32761 avg_loss = 3.69664\n",
      "epoch no.1 train no.107370  loss = 1.78417 avg_loss = 3.62332\n",
      "epoch no.1 train no.107380  loss = 3.27083 avg_loss = 3.63770\n",
      "epoch no.1 train no.107390  loss = 2.54081 avg_loss = 3.64096\n",
      "epoch no.1 train no.107400  loss = 3.51601 avg_loss = 3.66287\n",
      "epoch no.1 train no.107410  loss = 5.09607 avg_loss = 3.65985\n",
      "epoch no.1 train no.107420  loss = 4.13980 avg_loss = 3.69199\n",
      "epoch no.1 train no.107430  loss = 3.79573 avg_loss = 3.66899\n",
      "epoch no.1 train no.107440  loss = 2.49966 avg_loss = 3.70747\n",
      "epoch no.1 train no.107450  loss = 3.23672 avg_loss = 3.72616\n",
      "epoch no.1 train no.107460  loss = 3.79430 avg_loss = 3.68866\n",
      "epoch no.1 train no.107470  loss = 2.46900 avg_loss = 3.63120\n",
      "epoch no.1 train no.107480  loss = 2.99285 avg_loss = 3.62668\n",
      "epoch no.1 train no.107490  loss = 4.97836 avg_loss = 3.63154\n",
      "epoch no.1 train no.107500  loss = 1.33379 avg_loss = 3.61888\n",
      "epoch no.1 train no.107510  loss = 4.23535 avg_loss = 3.59359\n",
      "epoch no.1 train no.107520  loss = 2.96787 avg_loss = 3.58212\n",
      "epoch no.1 train no.107530  loss = 5.17565 avg_loss = 3.63719\n",
      "epoch no.1 train no.107540  loss = 2.74567 avg_loss = 3.60442\n",
      "epoch no.1 train no.107550  loss = 3.12751 avg_loss = 3.61939\n",
      "epoch no.1 train no.107560  loss = 2.19882 avg_loss = 3.63224\n",
      "epoch no.1 train no.107570  loss = 4.31211 avg_loss = 3.64366\n",
      "epoch no.1 train no.107580  loss = 3.05561 avg_loss = 3.62741\n",
      "epoch no.1 train no.107590  loss = 3.26378 avg_loss = 3.62617\n",
      "epoch no.1 train no.107600  loss = 2.65438 avg_loss = 3.63346\n",
      "epoch no.1 train no.107610  loss = 4.86161 avg_loss = 3.59708\n",
      "epoch no.1 train no.107620  loss = 3.60632 avg_loss = 3.54934\n",
      "epoch no.1 train no.107630  loss = 4.00561 avg_loss = 3.56456\n",
      "epoch no.1 train no.107640  loss = 3.35920 avg_loss = 3.63637\n",
      "epoch no.1 train no.107650  loss = 4.81502 avg_loss = 3.66803\n",
      "epoch no.1 train no.107660  loss = 2.50128 avg_loss = 3.65890\n",
      "epoch no.1 train no.107670  loss = 6.05364 avg_loss = 3.64248\n",
      "epoch no.1 train no.107680  loss = 4.86573 avg_loss = 3.64039\n",
      "epoch no.1 train no.107690  loss = 2.10417 avg_loss = 3.63816\n",
      "epoch no.1 train no.107700  loss = 4.99364 avg_loss = 3.63046\n",
      "epoch no.1 train no.107710  loss = 2.30111 avg_loss = 3.56215\n",
      "epoch no.1 train no.107720  loss = 4.58455 avg_loss = 3.59489\n",
      "epoch no.1 train no.107730  loss = 2.65663 avg_loss = 3.57454\n",
      "epoch no.1 train no.107740  loss = 5.74477 avg_loss = 3.61288\n",
      "epoch no.1 train no.107750  loss = 3.20963 avg_loss = 3.58200\n",
      "epoch no.1 train no.107760  loss = 4.74558 avg_loss = 3.66642\n",
      "epoch no.1 train no.107770  loss = 3.00028 avg_loss = 3.67057\n",
      "epoch no.1 train no.107780  loss = 3.81976 avg_loss = 3.65217\n",
      "epoch no.1 train no.107790  loss = 4.90488 avg_loss = 3.63303\n",
      "epoch no.1 train no.107800  loss = 2.89570 avg_loss = 3.64610\n",
      "epoch no.1 train no.107810  loss = 4.11363 avg_loss = 3.65988\n",
      "epoch no.1 train no.107820  loss = 3.44539 avg_loss = 3.62912\n",
      "epoch no.1 train no.107830  loss = 4.71986 avg_loss = 3.60535\n",
      "epoch no.1 train no.107840  loss = 3.37368 avg_loss = 3.58285\n",
      "epoch no.1 train no.107850  loss = 5.04213 avg_loss = 3.54972\n",
      "epoch no.1 train no.107860  loss = 5.24804 avg_loss = 3.55297\n",
      "epoch no.1 train no.107870  loss = 3.21095 avg_loss = 3.54286\n",
      "epoch no.1 train no.107880  loss = 4.61822 avg_loss = 3.56835\n",
      "epoch no.1 train no.107890  loss = 2.34596 avg_loss = 3.55893\n",
      "epoch no.1 train no.107900  loss = 4.26414 avg_loss = 3.58046\n",
      "epoch no.1 train no.107910  loss = 2.70378 avg_loss = 3.57678\n",
      "epoch no.1 train no.107920  loss = 4.26943 avg_loss = 3.63725\n",
      "epoch no.1 train no.107930  loss = 3.37122 avg_loss = 3.56633\n",
      "epoch no.1 train no.107940  loss = 2.30803 avg_loss = 3.55341\n",
      "epoch no.1 train no.107950  loss = 6.24342 avg_loss = 3.59686\n",
      "epoch no.1 train no.107960  loss = 5.56224 avg_loss = 3.58476\n",
      "epoch no.1 train no.107970  loss = 2.14842 avg_loss = 3.58337\n",
      "epoch no.1 train no.107980  loss = 2.91819 avg_loss = 3.53700\n",
      "epoch no.1 train no.107990  loss = 2.66199 avg_loss = 3.50052\n",
      "epoch no.1 train no.108000  loss = 2.28826 avg_loss = 3.45763\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁노래', '한', '▁음악', 'op', '</s>']\n",
      "여름밤에 어울리는 청량한 pop</s>\n",
      "epoch no.1 train no.108010  loss = 2.92598 avg_loss = 3.45226\n",
      "epoch no.1 train no.108020  loss = 3.61377 avg_loss = 3.46788\n",
      "epoch no.1 train no.108030  loss = 3.71810 avg_loss = 3.44200\n",
      "epoch no.1 train no.108040  loss = 3.46642 avg_loss = 3.49739\n",
      "epoch no.1 train no.108050  loss = 4.66175 avg_loss = 3.50199\n",
      "epoch no.1 train no.108060  loss = 4.09570 avg_loss = 3.52316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.108070  loss = 3.99322 avg_loss = 3.48977\n",
      "epoch no.1 train no.108080  loss = 2.34013 avg_loss = 3.45430\n",
      "epoch no.1 train no.108090  loss = 3.02557 avg_loss = 3.46698\n",
      "epoch no.1 train no.108100  loss = 4.45815 avg_loss = 3.51892\n",
      "epoch no.1 train no.108110  loss = 6.14140 avg_loss = 3.55399\n",
      "epoch no.1 train no.108120  loss = 2.67811 avg_loss = 3.56385\n",
      "epoch no.1 train no.108130  loss = 4.91346 avg_loss = 3.59891\n",
      "epoch no.1 train no.108140  loss = 4.59239 avg_loss = 3.60805\n",
      "epoch no.1 train no.108150  loss = 3.80223 avg_loss = 3.58314\n",
      "epoch no.1 train no.108160  loss = 3.86597 avg_loss = 3.59850\n",
      "epoch no.1 train no.108170  loss = 3.99211 avg_loss = 3.56360\n",
      "epoch no.1 train no.108180  loss = 5.09815 avg_loss = 3.62656\n",
      "epoch no.1 train no.108190  loss = 3.03826 avg_loss = 3.63914\n",
      "epoch no.1 train no.108200  loss = 5.27520 avg_loss = 3.66461\n",
      "epoch no.1 train no.108210  loss = 2.89533 avg_loss = 3.62113\n",
      "epoch no.1 train no.108220  loss = 3.76674 avg_loss = 3.59748\n",
      "epoch no.1 train no.108230  loss = 4.42171 avg_loss = 3.58035\n",
      "epoch no.1 train no.108240  loss = 1.76972 avg_loss = 3.54151\n",
      "epoch no.1 train no.108250  loss = 4.16527 avg_loss = 3.56019\n",
      "epoch no.1 train no.108260  loss = 2.17381 avg_loss = 3.52535\n",
      "epoch no.1 train no.108270  loss = 4.29980 avg_loss = 3.53396\n",
      "epoch no.1 train no.108280  loss = 4.69099 avg_loss = 3.53725\n",
      "epoch no.1 train no.108290  loss = 5.62803 avg_loss = 3.59376\n",
      "epoch no.1 train no.108300  loss = 2.35199 avg_loss = 3.57305\n",
      "epoch no.1 train no.108310  loss = 1.83052 avg_loss = 3.48525\n",
      "epoch no.1 train no.108320  loss = 3.21055 avg_loss = 3.51501\n",
      "epoch no.1 train no.108330  loss = 3.82329 avg_loss = 3.49835\n",
      "epoch no.1 train no.108340  loss = 3.88040 avg_loss = 3.54127\n",
      "epoch no.1 train no.108350  loss = 2.50483 avg_loss = 3.54750\n",
      "epoch no.1 train no.108360  loss = 3.89479 avg_loss = 3.63319\n",
      "epoch no.1 train no.108370  loss = 3.14233 avg_loss = 3.65365\n",
      "epoch no.1 train no.108380  loss = 3.94396 avg_loss = 3.63675\n",
      "epoch no.1 train no.108390  loss = 3.81173 avg_loss = 3.68218\n",
      "epoch no.1 train no.108400  loss = 2.30522 avg_loss = 3.68168\n",
      "epoch no.1 train no.108410  loss = 4.08777 avg_loss = 3.66749\n",
      "epoch no.1 train no.108420  loss = 4.30916 avg_loss = 3.65567\n",
      "epoch no.1 train no.108430  loss = 4.80759 avg_loss = 3.66209\n",
      "epoch no.1 train no.108440  loss = 3.53994 avg_loss = 3.67812\n",
      "epoch no.1 train no.108450  loss = 3.64856 avg_loss = 3.65799\n",
      "epoch no.1 train no.108460  loss = 5.48773 avg_loss = 3.62227\n",
      "epoch no.1 train no.108470  loss = 3.69727 avg_loss = 3.62765\n",
      "epoch no.1 train no.108480  loss = 1.83889 avg_loss = 3.61493\n",
      "epoch no.1 train no.108490  loss = 3.06834 avg_loss = 3.64746\n",
      "epoch no.1 train no.108500  loss = 5.35454 avg_loss = 3.70942\n",
      "epoch no.1 train no.108510  loss = 3.43304 avg_loss = 3.68501\n",
      "epoch no.1 train no.108520  loss = 2.28928 avg_loss = 3.66503\n",
      "epoch no.1 train no.108530  loss = 5.88903 avg_loss = 3.67643\n",
      "epoch no.1 train no.108540  loss = 4.64659 avg_loss = 3.71485\n",
      "epoch no.1 train no.108550  loss = 2.47050 avg_loss = 3.76563\n",
      "epoch no.1 train no.108560  loss = 3.97316 avg_loss = 3.72234\n",
      "epoch no.1 train no.108570  loss = 3.99886 avg_loss = 3.73268\n",
      "epoch no.1 train no.108580  loss = 3.14362 avg_loss = 3.69360\n",
      "epoch no.1 train no.108590  loss = 4.00969 avg_loss = 3.75475\n",
      "epoch no.1 train no.108600  loss = 4.36625 avg_loss = 3.77951\n",
      "epoch no.1 train no.108610  loss = 5.21219 avg_loss = 3.80946\n",
      "epoch no.1 train no.108620  loss = 4.74161 avg_loss = 3.84064\n",
      "epoch no.1 train no.108630  loss = 5.69052 avg_loss = 3.81854\n",
      "epoch no.1 train no.108640  loss = 3.79257 avg_loss = 3.80908\n",
      "epoch no.1 train no.108650  loss = 3.99169 avg_loss = 3.76293\n",
      "epoch no.1 train no.108660  loss = 2.81017 avg_loss = 3.76363\n",
      "epoch no.1 train no.108670  loss = 4.25533 avg_loss = 3.77183\n",
      "epoch no.1 train no.108680  loss = 2.44015 avg_loss = 3.73415\n",
      "epoch no.1 train no.108690  loss = 2.77279 avg_loss = 3.72099\n",
      "epoch no.1 train no.108700  loss = 3.70872 avg_loss = 3.71649\n",
      "epoch no.1 train no.108710  loss = 5.51476 avg_loss = 3.69224\n",
      "epoch no.1 train no.108720  loss = 4.72350 avg_loss = 3.70794\n",
      "epoch no.1 train no.108730  loss = 5.04145 avg_loss = 3.69395\n",
      "epoch no.1 train no.108740  loss = 3.47463 avg_loss = 3.66585\n",
      "epoch no.1 train no.108750  loss = 2.84858 avg_loss = 3.61268\n",
      "epoch no.1 train no.108760  loss = 2.68530 avg_loss = 3.61908\n",
      "epoch no.1 train no.108770  loss = 6.20974 avg_loss = 3.63080\n",
      "epoch no.1 train no.108780  loss = 2.78520 avg_loss = 3.68791\n",
      "epoch no.1 train no.108790  loss = 3.01443 avg_loss = 3.70181\n",
      "epoch no.1 train no.108800  loss = 3.99006 avg_loss = 3.69354\n",
      "epoch no.1 train no.108810  loss = 2.47645 avg_loss = 3.68785\n",
      "epoch no.1 train no.108820  loss = 3.02042 avg_loss = 3.67174\n",
      "epoch no.1 train no.108830  loss = 2.12320 avg_loss = 3.65521\n",
      "epoch no.1 train no.108840  loss = 4.39141 avg_loss = 3.68203\n",
      "epoch no.1 train no.108850  loss = 5.48813 avg_loss = 3.72132\n",
      "epoch no.1 train no.108860  loss = 5.30690 avg_loss = 3.69130\n",
      "epoch no.1 train no.108870  loss = 2.79749 avg_loss = 3.67589\n",
      "epoch no.1 train no.108880  loss = 5.28545 avg_loss = 3.68916\n",
      "epoch no.1 train no.108890  loss = 4.20125 avg_loss = 3.67256\n",
      "epoch no.1 train no.108900  loss = 4.02115 avg_loss = 3.73262\n",
      "epoch no.1 train no.108910  loss = 5.38993 avg_loss = 3.76120\n",
      "epoch no.1 train no.108920  loss = 2.75255 avg_loss = 3.75343\n",
      "epoch no.1 train no.108930  loss = 4.70999 avg_loss = 3.76797\n",
      "epoch no.1 train no.108940  loss = 3.96589 avg_loss = 3.75710\n",
      "epoch no.1 train no.108950  loss = 3.06622 avg_loss = 3.79571\n",
      "epoch no.1 train no.108960  loss = 3.46117 avg_loss = 3.74822\n",
      "epoch no.1 train no.108970  loss = 4.80130 avg_loss = 3.78548\n",
      "epoch no.1 train no.108980  loss = 3.20325 avg_loss = 3.75952\n",
      "epoch no.1 train no.108990  loss = 3.01165 avg_loss = 3.74903\n",
      "epoch no.1 train no.109000  loss = 5.91489 avg_loss = 3.79425\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '음악', '</s>']\n",
      "여름밤에 듣는 인디음악</s>\n",
      "epoch no.1 train no.109010  loss = 3.40732 avg_loss = 3.81183\n",
      "epoch no.1 train no.109020  loss = 4.15292 avg_loss = 3.76569\n",
      "epoch no.1 train no.109030  loss = 3.29989 avg_loss = 3.74025\n",
      "epoch no.1 train no.109040  loss = 2.84419 avg_loss = 3.75603\n",
      "epoch no.1 train no.109050  loss = 0.86685 avg_loss = 3.74417\n",
      "epoch no.1 train no.109060  loss = 3.76217 avg_loss = 3.76942\n",
      "epoch no.1 train no.109070  loss = 4.22040 avg_loss = 3.74188\n",
      "epoch no.1 train no.109080  loss = 2.32194 avg_loss = 3.74558\n",
      "epoch no.1 train no.109090  loss = 3.00462 avg_loss = 3.66864\n",
      "epoch no.1 train no.109100  loss = 5.22607 avg_loss = 3.64445\n",
      "epoch no.1 train no.109110  loss = 3.82378 avg_loss = 3.67071\n",
      "epoch no.1 train no.109120  loss = 2.75452 avg_loss = 3.64428\n",
      "epoch no.1 train no.109130  loss = 2.48358 avg_loss = 3.60413\n",
      "epoch no.1 train no.109140  loss = 5.72113 avg_loss = 3.66134\n",
      "epoch no.1 train no.109150  loss = 4.63917 avg_loss = 3.68324\n",
      "epoch no.1 train no.109160  loss = 2.42379 avg_loss = 3.65729\n",
      "epoch no.1 train no.109170  loss = 2.96303 avg_loss = 3.63729\n",
      "epoch no.1 train no.109180  loss = 3.16652 avg_loss = 3.67409\n",
      "epoch no.1 train no.109190  loss = 2.45502 avg_loss = 3.69783\n",
      "epoch no.1 train no.109200  loss = 3.45681 avg_loss = 3.66118\n",
      "epoch no.1 train no.109210  loss = 5.01784 avg_loss = 3.71008\n",
      "epoch no.1 train no.109220  loss = 3.12230 avg_loss = 3.68320\n",
      "epoch no.1 train no.109230  loss = 3.74648 avg_loss = 3.64976\n",
      "epoch no.1 train no.109240  loss = 4.61066 avg_loss = 3.65881\n",
      "epoch no.1 train no.109250  loss = 1.33681 avg_loss = 3.65426\n",
      "epoch no.1 train no.109260  loss = 2.94908 avg_loss = 3.72376\n",
      "epoch no.1 train no.109270  loss = 1.93659 avg_loss = 3.74361\n",
      "epoch no.1 train no.109280  loss = 3.09877 avg_loss = 3.65382\n",
      "epoch no.1 train no.109290  loss = 3.51280 avg_loss = 3.63663\n",
      "epoch no.1 train no.109300  loss = 3.17436 avg_loss = 3.66412\n",
      "epoch no.1 train no.109310  loss = 4.74345 avg_loss = 3.73986\n",
      "epoch no.1 train no.109320  loss = 3.16340 avg_loss = 3.76000\n",
      "epoch no.1 train no.109330  loss = 3.05917 avg_loss = 3.75890\n",
      "epoch no.1 train no.109340  loss = 2.80993 avg_loss = 3.70643\n",
      "epoch no.1 train no.109350  loss = 3.03136 avg_loss = 3.68505\n",
      "epoch no.1 train no.109360  loss = 6.18209 avg_loss = 3.66364\n",
      "epoch no.1 train no.109370  loss = 2.38451 avg_loss = 3.63849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.109380  loss = 6.11920 avg_loss = 3.62962\n",
      "epoch no.1 train no.109390  loss = 3.41488 avg_loss = 3.67145\n",
      "epoch no.1 train no.109400  loss = 4.52710 avg_loss = 3.66320\n",
      "epoch no.1 train no.109410  loss = 4.18008 avg_loss = 3.67653\n",
      "epoch no.1 train no.109420  loss = 3.66073 avg_loss = 3.70836\n",
      "epoch no.1 train no.109430  loss = 3.03957 avg_loss = 3.68851\n",
      "epoch no.1 train no.109440  loss = 5.16312 avg_loss = 3.75995\n",
      "epoch no.1 train no.109450  loss = 4.08366 avg_loss = 3.76689\n",
      "epoch no.1 train no.109460  loss = 2.60444 avg_loss = 3.75043\n",
      "epoch no.1 train no.109470  loss = 2.90675 avg_loss = 3.76672\n",
      "epoch no.1 train no.109480  loss = 3.32463 avg_loss = 3.73863\n",
      "epoch no.1 train no.109490  loss = 5.02887 avg_loss = 3.77178\n",
      "epoch no.1 train no.109500  loss = 3.45621 avg_loss = 3.75953\n",
      "epoch no.1 train no.109510  loss = 4.29398 avg_loss = 3.77160\n",
      "epoch no.1 train no.109520  loss = 5.85760 avg_loss = 3.75511\n",
      "epoch no.1 train no.109530  loss = 3.52555 avg_loss = 3.71098\n",
      "epoch no.1 train no.109540  loss = 5.95462 avg_loss = 3.67761\n",
      "epoch no.1 train no.109550  loss = 5.31116 avg_loss = 3.75842\n",
      "epoch no.1 train no.109560  loss = 2.94501 avg_loss = 3.79010\n",
      "epoch no.1 train no.109570  loss = 4.65143 avg_loss = 3.75288\n",
      "epoch no.1 train no.109580  loss = 3.74346 avg_loss = 3.72258\n",
      "epoch no.1 train no.109590  loss = 3.20141 avg_loss = 3.74344\n",
      "epoch no.1 train no.109600  loss = 3.42816 avg_loss = 3.74009\n",
      "epoch no.1 train no.109610  loss = 4.14478 avg_loss = 3.71740\n",
      "epoch no.1 train no.109620  loss = 3.07247 avg_loss = 3.67425\n",
      "epoch no.1 train no.109630  loss = 3.01702 avg_loss = 3.66919\n",
      "epoch no.1 train no.109640  loss = 3.44238 avg_loss = 3.66554\n",
      "epoch no.1 train no.109650  loss = 3.25395 avg_loss = 3.61602\n",
      "epoch no.1 train no.109660  loss = 2.53066 avg_loss = 3.63397\n",
      "epoch no.1 train no.109670  loss = 2.41853 avg_loss = 3.63483\n",
      "epoch no.1 train no.109680  loss = 6.03724 avg_loss = 3.61650\n",
      "epoch no.1 train no.109690  loss = 5.63978 avg_loss = 3.65284\n",
      "epoch no.1 train no.109700  loss = 4.68055 avg_loss = 3.64264\n",
      "epoch no.1 train no.109710  loss = 3.33874 avg_loss = 3.60177\n",
      "epoch no.1 train no.109720  loss = 3.19162 avg_loss = 3.64528\n",
      "epoch no.1 train no.109730  loss = 2.63141 avg_loss = 3.57648\n",
      "epoch no.1 train no.109740  loss = 2.69004 avg_loss = 3.67081\n",
      "epoch no.1 train no.109750  loss = 5.08287 avg_loss = 3.66750\n",
      "epoch no.1 train no.109760  loss = 3.66291 avg_loss = 3.66334\n",
      "epoch no.1 train no.109770  loss = 4.16322 avg_loss = 3.57992\n",
      "epoch no.1 train no.109780  loss = 3.26686 avg_loss = 3.60029\n",
      "epoch no.1 train no.109790  loss = 4.00784 avg_loss = 3.67388\n",
      "epoch no.1 train no.109800  loss = 2.97940 avg_loss = 3.63614\n",
      "epoch no.1 train no.109810  loss = 3.00119 avg_loss = 3.59586\n",
      "epoch no.1 train no.109820  loss = 4.97067 avg_loss = 3.61359\n",
      "epoch no.1 train no.109830  loss = 4.40773 avg_loss = 3.64400\n",
      "epoch no.1 train no.109840  loss = 2.59857 avg_loss = 3.57615\n",
      "epoch no.1 train no.109850  loss = 4.26144 avg_loss = 3.56570\n",
      "epoch no.1 train no.109860  loss = 4.19931 avg_loss = 3.61550\n",
      "epoch no.1 train no.109870  loss = 3.76336 avg_loss = 3.69338\n",
      "epoch no.1 train no.109880  loss = 2.39533 avg_loss = 3.67625\n",
      "epoch no.1 train no.109890  loss = 3.41218 avg_loss = 3.67968\n",
      "epoch no.1 train no.109900  loss = 5.00251 avg_loss = 3.70209\n",
      "epoch no.1 train no.109910  loss = 5.39193 avg_loss = 3.76424\n",
      "epoch no.1 train no.109920  loss = 2.75429 avg_loss = 3.71860\n",
      "epoch no.1 train no.109930  loss = 2.84447 avg_loss = 3.67660\n",
      "epoch no.1 train no.109940  loss = 4.94529 avg_loss = 3.73233\n",
      "epoch no.1 train no.109950  loss = 3.60580 avg_loss = 3.68324\n",
      "epoch no.1 train no.109960  loss = 3.10179 avg_loss = 3.67159\n",
      "epoch no.1 train no.109970  loss = 2.12922 avg_loss = 3.68037\n",
      "epoch no.1 train no.109980  loss = 4.46048 avg_loss = 3.70672\n",
      "epoch no.1 train no.109990  loss = 2.18206 avg_loss = 3.64988\n",
      "epoch no.1 train no.110000  loss = 5.97767 avg_loss = 3.60462\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '같은', '▁음악', '들', '</s>']\n",
      "여름밤의 꿀같은 노래들</s>\n",
      "epoch no.1 train no.110010  loss = 2.70537 avg_loss = 3.65304\n",
      "epoch no.1 train no.110020  loss = 2.99422 avg_loss = 3.65973\n",
      "epoch no.1 train no.110030  loss = 5.32566 avg_loss = 3.68993\n",
      "epoch no.1 train no.110040  loss = 3.51583 avg_loss = 3.66518\n",
      "epoch no.1 train no.110050  loss = 6.00106 avg_loss = 3.65594\n",
      "epoch no.1 train no.110060  loss = 3.96979 avg_loss = 3.69897\n",
      "epoch no.1 train no.110070  loss = 2.88369 avg_loss = 3.68116\n",
      "epoch no.1 train no.110080  loss = 3.10438 avg_loss = 3.73512\n",
      "epoch no.1 train no.110090  loss = 2.07909 avg_loss = 3.72234\n",
      "epoch no.1 train no.110100  loss = 4.32421 avg_loss = 3.73537\n",
      "epoch no.1 train no.110110  loss = 5.21865 avg_loss = 3.70310\n",
      "epoch no.1 train no.110120  loss = 4.32123 avg_loss = 3.67357\n",
      "epoch no.1 train no.110130  loss = 3.11074 avg_loss = 3.63308\n",
      "epoch no.1 train no.110140  loss = 3.31468 avg_loss = 3.66214\n",
      "epoch no.1 train no.110150  loss = 2.62648 avg_loss = 3.63533\n",
      "epoch no.1 train no.110160  loss = 3.71833 avg_loss = 3.63030\n",
      "epoch no.1 train no.110170  loss = 2.89154 avg_loss = 3.63285\n",
      "epoch no.1 train no.110180  loss = 4.31449 avg_loss = 3.66257\n",
      "epoch no.1 train no.110190  loss = 5.30888 avg_loss = 3.64467\n",
      "epoch no.1 train no.110200  loss = 3.16964 avg_loss = 3.66586\n",
      "epoch no.1 train no.110210  loss = 6.27100 avg_loss = 3.74839\n",
      "epoch no.1 train no.110220  loss = 5.65505 avg_loss = 3.79479\n",
      "epoch no.1 train no.110230  loss = 3.15171 avg_loss = 3.76868\n",
      "epoch no.1 train no.110240  loss = 4.22950 avg_loss = 3.75222\n",
      "epoch no.1 train no.110250  loss = 3.52882 avg_loss = 3.74247\n",
      "epoch no.1 train no.110260  loss = 3.37378 avg_loss = 3.70461\n",
      "epoch no.1 train no.110270  loss = 4.16228 avg_loss = 3.66787\n",
      "epoch no.1 train no.110280  loss = 3.58915 avg_loss = 3.68116\n",
      "epoch no.1 train no.110290  loss = 2.92224 avg_loss = 3.64610\n",
      "epoch no.1 train no.110300  loss = 7.24033 avg_loss = 3.71451\n",
      "epoch no.1 train no.110310  loss = 6.18505 avg_loss = 3.72069\n",
      "epoch no.1 train no.110320  loss = 3.66618 avg_loss = 3.72387\n",
      "epoch no.1 train no.110330  loss = 2.65021 avg_loss = 3.69296\n",
      "epoch no.1 train no.110340  loss = 4.94052 avg_loss = 3.72887\n",
      "epoch no.1 train no.110350  loss = 6.02970 avg_loss = 3.73106\n",
      "epoch no.1 train no.110360  loss = 2.57812 avg_loss = 3.66735\n",
      "epoch no.1 train no.110370  loss = 2.85857 avg_loss = 3.58766\n",
      "epoch no.1 train no.110380  loss = 3.65258 avg_loss = 3.59646\n",
      "epoch no.1 train no.110390  loss = 2.66164 avg_loss = 3.65106\n",
      "epoch no.1 train no.110400  loss = 7.11373 avg_loss = 3.69431\n",
      "epoch no.1 train no.110410  loss = 2.19590 avg_loss = 3.66060\n",
      "epoch no.1 train no.110420  loss = 3.04268 avg_loss = 3.63548\n",
      "epoch no.1 train no.110430  loss = 3.14915 avg_loss = 3.62699\n",
      "epoch no.1 train no.110440  loss = 2.73909 avg_loss = 3.63204\n",
      "epoch no.1 train no.110450  loss = 5.25775 avg_loss = 3.60532\n",
      "epoch no.1 train no.110460  loss = 3.69885 avg_loss = 3.60858\n",
      "epoch no.1 train no.110470  loss = 2.21310 avg_loss = 3.57480\n",
      "epoch no.1 train no.110480  loss = 2.58556 avg_loss = 3.61109\n",
      "epoch no.1 train no.110490  loss = 4.32819 avg_loss = 3.67646\n",
      "epoch no.1 train no.110500  loss = 2.94389 avg_loss = 3.68647\n",
      "epoch no.1 train no.110510  loss = 2.63328 avg_loss = 3.72613\n",
      "epoch no.1 train no.110520  loss = 2.50909 avg_loss = 3.76744\n",
      "epoch no.1 train no.110530  loss = 3.67938 avg_loss = 3.76975\n",
      "epoch no.1 train no.110540  loss = 4.40725 avg_loss = 3.75307\n",
      "epoch no.1 train no.110550  loss = 4.55316 avg_loss = 3.76501\n",
      "epoch no.1 train no.110560  loss = 3.78400 avg_loss = 3.75534\n",
      "epoch no.1 train no.110570  loss = 3.05282 avg_loss = 3.73592\n",
      "epoch no.1 train no.110580  loss = 1.33738 avg_loss = 3.68858\n",
      "epoch no.1 train no.110590  loss = 2.85201 avg_loss = 3.69163\n",
      "epoch no.1 train no.110600  loss = 2.84714 avg_loss = 3.66571\n",
      "epoch no.1 train no.110610  loss = 3.49565 avg_loss = 3.62706\n",
      "epoch no.1 train no.110620  loss = 3.62115 avg_loss = 3.66798\n",
      "epoch no.1 train no.110630  loss = 3.47461 avg_loss = 3.62114\n",
      "epoch no.1 train no.110640  loss = 1.83739 avg_loss = 3.61338\n",
      "epoch no.1 train no.110650  loss = 3.71562 avg_loss = 3.60983\n",
      "epoch no.1 train no.110660  loss = 4.56871 avg_loss = 3.61374\n",
      "epoch no.1 train no.110670  loss = 2.97832 avg_loss = 3.59149\n",
      "epoch no.1 train no.110680  loss = 5.05648 avg_loss = 3.59836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.110690  loss = 3.64309 avg_loss = 3.57766\n",
      "epoch no.1 train no.110700  loss = 2.15647 avg_loss = 3.59247\n",
      "epoch no.1 train no.110710  loss = 2.65345 avg_loss = 3.51947\n",
      "epoch no.1 train no.110720  loss = 3.06477 avg_loss = 3.49572\n",
      "epoch no.1 train no.110730  loss = 2.91724 avg_loss = 3.51696\n",
      "epoch no.1 train no.110740  loss = 1.90881 avg_loss = 3.50839\n",
      "epoch no.1 train no.110750  loss = 3.45908 avg_loss = 3.50173\n",
      "epoch no.1 train no.110760  loss = 3.92819 avg_loss = 3.52446\n",
      "epoch no.1 train no.110770  loss = 3.58474 avg_loss = 3.53106\n",
      "epoch no.1 train no.110780  loss = 3.74366 avg_loss = 3.55650\n",
      "epoch no.1 train no.110790  loss = 2.83520 avg_loss = 3.49983\n",
      "epoch no.1 train no.110800  loss = 2.94356 avg_loss = 3.51064\n",
      "epoch no.1 train no.110810  loss = 3.59453 avg_loss = 3.49591\n",
      "epoch no.1 train no.110820  loss = 3.99163 avg_loss = 3.52209\n",
      "epoch no.1 train no.110830  loss = 4.51814 avg_loss = 3.54063\n",
      "epoch no.1 train no.110840  loss = 2.24249 avg_loss = 3.57050\n",
      "epoch no.1 train no.110850  loss = 4.39912 avg_loss = 3.57754\n",
      "epoch no.1 train no.110860  loss = 1.54991 avg_loss = 3.55313\n",
      "epoch no.1 train no.110870  loss = 2.18211 avg_loss = 3.57476\n",
      "epoch no.1 train no.110880  loss = 2.54048 avg_loss = 3.61726\n",
      "epoch no.1 train no.110890  loss = 3.09965 avg_loss = 3.56195\n",
      "epoch no.1 train no.110900  loss = 4.44366 avg_loss = 3.59519\n",
      "epoch no.1 train no.110910  loss = 2.96034 avg_loss = 3.60823\n",
      "epoch no.1 train no.110920  loss = 5.75611 avg_loss = 3.61224\n",
      "epoch no.1 train no.110930  loss = 4.38501 avg_loss = 3.62890\n",
      "epoch no.1 train no.110940  loss = 5.38631 avg_loss = 3.64284\n",
      "epoch no.1 train no.110950  loss = 3.36271 avg_loss = 3.60570\n",
      "epoch no.1 train no.110960  loss = 3.00816 avg_loss = 3.59698\n",
      "epoch no.1 train no.110970  loss = 4.27562 avg_loss = 3.59675\n",
      "epoch no.1 train no.110980  loss = 2.79221 avg_loss = 3.58155\n",
      "epoch no.1 train no.110990  loss = 3.77488 avg_loss = 3.55818\n",
      "epoch no.1 train no.111000  loss = 3.63065 avg_loss = 3.59132\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '을', '▁듣는', '▁감성', '▁발라', '에이지', '</s>']\n",
      "여름밤에 듣는 감성 뉴에이지</s>\n",
      "epoch no.1 train no.111010  loss = 4.20113 avg_loss = 3.57918\n",
      "epoch no.1 train no.111020  loss = 3.76297 avg_loss = 3.62249\n",
      "epoch no.1 train no.111030  loss = 3.47320 avg_loss = 3.61556\n",
      "epoch no.1 train no.111040  loss = 2.94554 avg_loss = 3.68639\n",
      "epoch no.1 train no.111050  loss = 2.96053 avg_loss = 3.63747\n",
      "epoch no.1 train no.111060  loss = 2.04631 avg_loss = 3.66019\n",
      "epoch no.1 train no.111070  loss = 4.54345 avg_loss = 3.66384\n",
      "epoch no.1 train no.111080  loss = 5.52690 avg_loss = 3.65549\n",
      "epoch no.1 train no.111090  loss = 2.94934 avg_loss = 3.60360\n",
      "epoch no.1 train no.111100  loss = 5.07484 avg_loss = 3.65574\n",
      "epoch no.1 train no.111110  loss = 2.91842 avg_loss = 3.63045\n",
      "epoch no.1 train no.111120  loss = 3.02131 avg_loss = 3.65933\n",
      "epoch no.1 train no.111130  loss = 4.61033 avg_loss = 3.66113\n",
      "epoch no.1 train no.111140  loss = 2.84051 avg_loss = 3.61926\n",
      "epoch no.1 train no.111150  loss = 2.17982 avg_loss = 3.61208\n",
      "epoch no.1 train no.111160  loss = 4.62201 avg_loss = 3.61540\n",
      "epoch no.1 train no.111170  loss = 2.83446 avg_loss = 3.60441\n",
      "epoch no.1 train no.111180  loss = 5.77960 avg_loss = 3.60118\n",
      "epoch no.1 train no.111190  loss = 5.37655 avg_loss = 3.65786\n",
      "epoch no.1 train no.111200  loss = 4.38381 avg_loss = 3.67641\n",
      "epoch no.1 train no.111210  loss = 5.28481 avg_loss = 3.68742\n",
      "epoch no.1 train no.111220  loss = 2.09467 avg_loss = 3.63324\n",
      "epoch no.1 train no.111230  loss = 2.98289 avg_loss = 3.67638\n",
      "epoch no.1 train no.111240  loss = 3.19019 avg_loss = 3.70451\n",
      "epoch no.1 train no.111250  loss = 4.25212 avg_loss = 3.80018\n",
      "epoch no.1 train no.111260  loss = 3.56877 avg_loss = 3.78951\n",
      "epoch no.1 train no.111270  loss = 4.74053 avg_loss = 3.76908\n",
      "epoch no.1 train no.111280  loss = 4.44323 avg_loss = 3.80333\n",
      "epoch no.1 train no.111290  loss = 2.53322 avg_loss = 3.80332\n",
      "epoch no.1 train no.111300  loss = 3.11078 avg_loss = 3.81144\n",
      "epoch no.1 train no.111310  loss = 5.81652 avg_loss = 3.83436\n",
      "epoch no.1 train no.111320  loss = 5.73634 avg_loss = 3.82298\n",
      "epoch no.1 train no.111330  loss = 4.82244 avg_loss = 3.80591\n",
      "epoch no.1 train no.111340  loss = 4.89897 avg_loss = 3.82910\n",
      "epoch no.1 train no.111350  loss = 4.94128 avg_loss = 3.85039\n",
      "epoch no.1 train no.111360  loss = 2.40660 avg_loss = 3.81962\n",
      "epoch no.1 train no.111370  loss = 3.02782 avg_loss = 3.82980\n",
      "epoch no.1 train no.111380  loss = 5.68882 avg_loss = 3.91088\n",
      "epoch no.1 train no.111390  loss = 3.39432 avg_loss = 3.90510\n",
      "epoch no.1 train no.111400  loss = 3.76483 avg_loss = 3.84940\n",
      "epoch no.1 train no.111410  loss = 5.64724 avg_loss = 3.80800\n",
      "epoch no.1 train no.111420  loss = 3.33789 avg_loss = 3.77883\n",
      "epoch no.1 train no.111430  loss = 3.68318 avg_loss = 3.72538\n",
      "epoch no.1 train no.111440  loss = 3.75188 avg_loss = 3.81531\n",
      "epoch no.1 train no.111450  loss = 3.36433 avg_loss = 3.74775\n",
      "epoch no.1 train no.111460  loss = 3.36595 avg_loss = 3.76725\n",
      "epoch no.1 train no.111470  loss = 3.71681 avg_loss = 3.76586\n",
      "epoch no.1 train no.111480  loss = 5.64242 avg_loss = 3.77691\n",
      "epoch no.1 train no.111490  loss = 3.79342 avg_loss = 3.76623\n",
      "epoch no.1 train no.111500  loss = 3.52657 avg_loss = 3.78201\n",
      "epoch no.1 train no.111510  loss = 4.37796 avg_loss = 3.84881\n",
      "epoch no.1 train no.111520  loss = 3.55048 avg_loss = 3.81002\n",
      "epoch no.1 train no.111530  loss = 5.03854 avg_loss = 3.81614\n",
      "epoch no.1 train no.111540  loss = 4.48294 avg_loss = 3.79588\n",
      "epoch no.1 train no.111550  loss = 3.07219 avg_loss = 3.76171\n",
      "epoch no.1 train no.111560  loss = 2.87698 avg_loss = 3.74042\n",
      "epoch no.1 train no.111570  loss = 2.05921 avg_loss = 3.76112\n",
      "epoch no.1 train no.111580  loss = 1.72543 avg_loss = 3.77303\n",
      "epoch no.1 train no.111590  loss = 2.78370 avg_loss = 3.74420\n",
      "epoch no.1 train no.111600  loss = 4.93873 avg_loss = 3.74333\n",
      "epoch no.1 train no.111610  loss = 4.08778 avg_loss = 3.75575\n",
      "epoch no.1 train no.111620  loss = 5.35829 avg_loss = 3.74650\n",
      "epoch no.1 train no.111630  loss = 3.60993 avg_loss = 3.73301\n",
      "epoch no.1 train no.111640  loss = 2.87248 avg_loss = 3.72272\n",
      "epoch no.1 train no.111650  loss = 1.78608 avg_loss = 3.72128\n",
      "epoch no.1 train no.111660  loss = 2.79280 avg_loss = 3.73161\n",
      "epoch no.1 train no.111670  loss = 2.51613 avg_loss = 3.70647\n",
      "epoch no.1 train no.111680  loss = 2.49153 avg_loss = 3.65614\n",
      "epoch no.1 train no.111690  loss = 3.63060 avg_loss = 3.67829\n",
      "epoch no.1 train no.111700  loss = 1.74307 avg_loss = 3.64583\n",
      "epoch no.1 train no.111710  loss = 4.92503 avg_loss = 3.68709\n",
      "epoch no.1 train no.111720  loss = 3.29597 avg_loss = 3.70783\n",
      "epoch no.1 train no.111730  loss = 2.66804 avg_loss = 3.68860\n",
      "epoch no.1 train no.111740  loss = 3.28931 avg_loss = 3.70272\n",
      "epoch no.1 train no.111750  loss = 3.49223 avg_loss = 3.68456\n",
      "epoch no.1 train no.111760  loss = 2.91089 avg_loss = 3.67426\n",
      "epoch no.1 train no.111770  loss = 4.27196 avg_loss = 3.72296\n",
      "epoch no.1 train no.111780  loss = 3.58459 avg_loss = 3.69043\n",
      "epoch no.1 train no.111790  loss = 2.97061 avg_loss = 3.64475\n",
      "epoch no.1 train no.111800  loss = 5.10234 avg_loss = 3.62224\n",
      "epoch no.1 train no.111810  loss = 4.17928 avg_loss = 3.63118\n",
      "epoch no.1 train no.111820  loss = 3.65356 avg_loss = 3.61816\n",
      "epoch no.1 train no.111830  loss = 2.04926 avg_loss = 3.61009\n",
      "epoch no.1 train no.111840  loss = 4.38525 avg_loss = 3.55846\n",
      "epoch no.1 train no.111850  loss = 3.01816 avg_loss = 3.56729\n",
      "epoch no.1 train no.111860  loss = 2.29806 avg_loss = 3.56899\n",
      "epoch no.1 train no.111870  loss = 1.21091 avg_loss = 3.57700\n",
      "epoch no.1 train no.111880  loss = 3.97232 avg_loss = 3.62540\n",
      "epoch no.1 train no.111890  loss = 2.69958 avg_loss = 3.58705\n",
      "epoch no.1 train no.111900  loss = 4.10430 avg_loss = 3.60915\n",
      "epoch no.1 train no.111910  loss = 3.32217 avg_loss = 3.60642\n",
      "epoch no.1 train no.111920  loss = 4.84957 avg_loss = 3.63809\n",
      "epoch no.1 train no.111930  loss = 3.54190 avg_loss = 3.63014\n",
      "epoch no.1 train no.111940  loss = 3.72905 avg_loss = 3.55414\n",
      "epoch no.1 train no.111950  loss = 2.67335 avg_loss = 3.54380\n",
      "epoch no.1 train no.111960  loss = 2.19624 avg_loss = 3.55155\n",
      "epoch no.1 train no.111970  loss = 4.22334 avg_loss = 3.53101\n",
      "epoch no.1 train no.111980  loss = 4.47162 avg_loss = 3.56897\n",
      "epoch no.1 train no.111990  loss = 4.15992 avg_loss = 3.57455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.112000  loss = 2.21418 avg_loss = 3.60957\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁잘', '▁시원한', '▁노래', '</s>']\n",
      "여름과 어울리는 시원한 음악</s>\n",
      "epoch no.1 train no.112010  loss = 4.43788 avg_loss = 3.61609\n",
      "epoch no.1 train no.112020  loss = 4.55594 avg_loss = 3.59919\n",
      "epoch no.1 train no.112030  loss = 2.01530 avg_loss = 3.60671\n",
      "epoch no.1 train no.112040  loss = 3.63066 avg_loss = 3.57336\n",
      "epoch no.1 train no.112050  loss = 2.18721 avg_loss = 3.51966\n",
      "epoch no.1 train no.112060  loss = 1.28768 avg_loss = 3.48665\n",
      "epoch no.1 train no.112070  loss = 6.14476 avg_loss = 3.49317\n",
      "epoch no.1 train no.112080  loss = 3.40574 avg_loss = 3.52209\n",
      "epoch no.1 train no.112090  loss = 3.63051 avg_loss = 3.58027\n",
      "epoch no.1 train no.112100  loss = 3.62986 avg_loss = 3.63709\n",
      "epoch no.1 train no.112110  loss = 3.21316 avg_loss = 3.66812\n",
      "epoch no.1 train no.112120  loss = 5.07483 avg_loss = 3.64983\n",
      "epoch no.1 train no.112130  loss = 3.98129 avg_loss = 3.68697\n",
      "epoch no.1 train no.112140  loss = 2.66034 avg_loss = 3.70267\n",
      "epoch no.1 train no.112150  loss = 4.00124 avg_loss = 3.68026\n",
      "epoch no.1 train no.112160  loss = 2.39786 avg_loss = 3.69915\n",
      "epoch no.1 train no.112170  loss = 3.21449 avg_loss = 3.65726\n",
      "epoch no.1 train no.112180  loss = 4.08339 avg_loss = 3.67969\n",
      "epoch no.1 train no.112190  loss = 5.68667 avg_loss = 3.63948\n",
      "epoch no.1 train no.112200  loss = 3.84614 avg_loss = 3.63561\n",
      "epoch no.1 train no.112210  loss = 3.36811 avg_loss = 3.63439\n",
      "epoch no.1 train no.112220  loss = 2.49838 avg_loss = 3.59983\n",
      "epoch no.1 train no.112230  loss = 3.76399 avg_loss = 3.57516\n",
      "epoch no.1 train no.112240  loss = 3.21578 avg_loss = 3.51740\n",
      "epoch no.1 train no.112250  loss = 2.43282 avg_loss = 3.55585\n",
      "epoch no.1 train no.112260  loss = 2.77832 avg_loss = 3.53097\n",
      "epoch no.1 train no.112270  loss = 3.17610 avg_loss = 3.57790\n",
      "epoch no.1 train no.112280  loss = 4.55838 avg_loss = 3.54491\n",
      "epoch no.1 train no.112290  loss = 4.08826 avg_loss = 3.51560\n",
      "epoch no.1 train no.112300  loss = 3.28313 avg_loss = 3.53053\n",
      "epoch no.1 train no.112310  loss = 4.51459 avg_loss = 3.55441\n",
      "epoch no.1 train no.112320  loss = 2.35078 avg_loss = 3.56732\n",
      "epoch no.1 train no.112330  loss = 3.55865 avg_loss = 3.55214\n",
      "epoch no.1 train no.112340  loss = 4.57453 avg_loss = 3.57906\n",
      "epoch no.1 train no.112350  loss = 2.52949 avg_loss = 3.56850\n",
      "epoch no.1 train no.112360  loss = 3.88934 avg_loss = 3.61232\n",
      "epoch no.1 train no.112370  loss = 3.61182 avg_loss = 3.68975\n",
      "epoch no.1 train no.112380  loss = 2.82779 avg_loss = 3.72518\n",
      "epoch no.1 train no.112390  loss = 4.24081 avg_loss = 3.78127\n",
      "epoch no.1 train no.112400  loss = 2.55975 avg_loss = 3.74655\n",
      "epoch no.1 train no.112410  loss = 2.90271 avg_loss = 3.66824\n",
      "epoch no.1 train no.112420  loss = 3.47135 avg_loss = 3.70696\n",
      "epoch no.1 train no.112430  loss = 5.95594 avg_loss = 3.68834\n",
      "epoch no.1 train no.112440  loss = 4.50617 avg_loss = 3.66661\n",
      "epoch no.1 train no.112450  loss = 1.70181 avg_loss = 3.65097\n",
      "epoch no.1 train no.112460  loss = 2.38355 avg_loss = 3.62908\n",
      "epoch no.1 train no.112470  loss = 4.03855 avg_loss = 3.65442\n",
      "epoch no.1 train no.112480  loss = 3.51225 avg_loss = 3.66266\n",
      "epoch no.1 train no.112490  loss = 4.28979 avg_loss = 3.67466\n",
      "epoch no.1 train no.112500  loss = 4.03617 avg_loss = 3.71811\n",
      "epoch no.1 train no.112510  loss = 3.18109 avg_loss = 3.67654\n",
      "epoch no.1 train no.112520  loss = 2.86627 avg_loss = 3.65848\n",
      "epoch no.1 train no.112530  loss = 5.02111 avg_loss = 3.67493\n",
      "epoch no.1 train no.112540  loss = 4.11048 avg_loss = 3.63051\n",
      "epoch no.1 train no.112550  loss = 4.32621 avg_loss = 3.66078\n",
      "epoch no.1 train no.112560  loss = 5.12130 avg_loss = 3.69897\n",
      "epoch no.1 train no.112570  loss = 2.32551 avg_loss = 3.69364\n",
      "epoch no.1 train no.112580  loss = 4.20039 avg_loss = 3.71567\n",
      "epoch no.1 train no.112590  loss = 2.73860 avg_loss = 3.66357\n",
      "epoch no.1 train no.112600  loss = 3.93078 avg_loss = 3.65810\n",
      "epoch no.1 train no.112610  loss = 3.71030 avg_loss = 3.62526\n",
      "epoch no.1 train no.112620  loss = 2.76169 avg_loss = 3.57345\n",
      "epoch no.1 train no.112630  loss = 2.93790 avg_loss = 3.53545\n",
      "epoch no.1 train no.112640  loss = 3.29043 avg_loss = 3.53791\n",
      "epoch no.1 train no.112650  loss = 4.09495 avg_loss = 3.54368\n",
      "epoch no.1 train no.112660  loss = 3.76872 avg_loss = 3.53714\n",
      "epoch no.1 train no.112670  loss = 3.21733 avg_loss = 3.55647\n",
      "epoch no.1 train no.112680  loss = 2.23665 avg_loss = 3.54379\n",
      "epoch no.1 train no.112690  loss = 4.76955 avg_loss = 3.62557\n",
      "epoch no.1 train no.112700  loss = 2.20270 avg_loss = 3.60163\n",
      "epoch no.1 train no.112710  loss = 3.44875 avg_loss = 3.65679\n",
      "epoch no.1 train no.112720  loss = 3.99629 avg_loss = 3.70490\n",
      "epoch no.1 train no.112730  loss = 4.72885 avg_loss = 3.70424\n",
      "epoch no.1 train no.112740  loss = 5.92600 avg_loss = 3.71940\n",
      "epoch no.1 train no.112750  loss = 5.02200 avg_loss = 3.78143\n",
      "epoch no.1 train no.112760  loss = 3.54667 avg_loss = 3.78895\n",
      "epoch no.1 train no.112770  loss = 4.32045 avg_loss = 3.75807\n",
      "epoch no.1 train no.112780  loss = 3.94284 avg_loss = 3.76512\n",
      "epoch no.1 train no.112790  loss = 3.90456 avg_loss = 3.73000\n",
      "epoch no.1 train no.112800  loss = 5.23805 avg_loss = 3.73889\n",
      "epoch no.1 train no.112810  loss = 2.44504 avg_loss = 3.73804\n",
      "epoch no.1 train no.112820  loss = 5.00384 avg_loss = 3.69489\n",
      "epoch no.1 train no.112830  loss = 4.35817 avg_loss = 3.71896\n",
      "epoch no.1 train no.112840  loss = 2.52925 avg_loss = 3.65533\n",
      "epoch no.1 train no.112850  loss = 1.41006 avg_loss = 3.61724\n",
      "epoch no.1 train no.112860  loss = 2.29782 avg_loss = 3.65848\n",
      "epoch no.1 train no.112870  loss = 6.21083 avg_loss = 3.63185\n",
      "epoch no.1 train no.112880  loss = 3.81892 avg_loss = 3.63742\n",
      "epoch no.1 train no.112890  loss = 2.35168 avg_loss = 3.63344\n",
      "epoch no.1 train no.112900  loss = 2.35867 avg_loss = 3.65625\n",
      "epoch no.1 train no.112910  loss = 2.43336 avg_loss = 3.66715\n",
      "epoch no.1 train no.112920  loss = 2.50575 avg_loss = 3.68909\n",
      "epoch no.1 train no.112930  loss = 1.58669 avg_loss = 3.64197\n",
      "epoch no.1 train no.112940  loss = 3.80953 avg_loss = 3.62115\n",
      "epoch no.1 train no.112950  loss = 2.41503 avg_loss = 3.60887\n",
      "epoch no.1 train no.112960  loss = 2.11889 avg_loss = 3.57358\n",
      "epoch no.1 train no.112970  loss = 2.93684 avg_loss = 3.56879\n",
      "epoch no.1 train no.112980  loss = 4.06726 avg_loss = 3.62139\n",
      "epoch no.1 train no.112990  loss = 3.22849 avg_loss = 3.64116\n",
      "epoch no.1 train no.113000  loss = 4.05899 avg_loss = 3.63607\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.1 train no.113010  loss = 4.36674 avg_loss = 3.73721\n",
      "epoch no.1 train no.113020  loss = 3.94104 avg_loss = 3.70059\n",
      "epoch no.1 train no.113030  loss = 3.92129 avg_loss = 3.71877\n",
      "epoch no.1 train no.113040  loss = 3.68430 avg_loss = 3.71503\n",
      "epoch no.1 train no.113050  loss = 3.85977 avg_loss = 3.71994\n",
      "epoch no.1 train no.113060  loss = 3.53285 avg_loss = 3.67789\n",
      "epoch no.1 train no.113070  loss = 3.06406 avg_loss = 3.62926\n",
      "epoch no.1 train no.113080  loss = 2.79015 avg_loss = 3.61811\n",
      "epoch no.1 train no.113090  loss = 4.43783 avg_loss = 3.64289\n",
      "epoch no.1 train no.113100  loss = 3.72286 avg_loss = 3.62434\n",
      "epoch no.1 train no.113110  loss = 3.02119 avg_loss = 3.62022\n",
      "epoch no.1 train no.113120  loss = 3.01057 avg_loss = 3.60004\n",
      "epoch no.1 train no.113130  loss = 4.07964 avg_loss = 3.60467\n",
      "epoch no.1 train no.113140  loss = 4.05056 avg_loss = 3.64223\n",
      "epoch no.1 train no.113150  loss = 5.87498 avg_loss = 3.67020\n",
      "epoch no.1 train no.113160  loss = 2.56838 avg_loss = 3.67722\n",
      "epoch no.1 train no.113170  loss = 4.09285 avg_loss = 3.70917\n",
      "epoch no.1 train no.113180  loss = 2.99835 avg_loss = 3.71419\n",
      "epoch no.1 train no.113190  loss = 4.81664 avg_loss = 3.73115\n",
      "epoch no.1 train no.113200  loss = 3.84148 avg_loss = 3.76699\n",
      "epoch no.1 train no.113210  loss = 3.31398 avg_loss = 3.71572\n",
      "epoch no.1 train no.113220  loss = 4.10632 avg_loss = 3.68380\n",
      "epoch no.1 train no.113230  loss = 4.83883 avg_loss = 3.72337\n",
      "epoch no.1 train no.113240  loss = 4.57289 avg_loss = 3.74055\n",
      "epoch no.1 train no.113250  loss = 3.27761 avg_loss = 3.69699\n",
      "epoch no.1 train no.113260  loss = 2.88137 avg_loss = 3.65587\n",
      "epoch no.1 train no.113270  loss = 1.15376 avg_loss = 3.62082\n",
      "epoch no.1 train no.113280  loss = 3.99031 avg_loss = 3.66844\n",
      "epoch no.1 train no.113290  loss = 4.89990 avg_loss = 3.69636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.113300  loss = 2.73039 avg_loss = 3.74454\n",
      "epoch no.1 train no.113310  loss = 2.97173 avg_loss = 3.78697\n",
      "epoch no.1 train no.113320  loss = 3.15922 avg_loss = 3.78481\n",
      "epoch no.1 train no.113330  loss = 2.64637 avg_loss = 3.69002\n",
      "epoch no.1 train no.113340  loss = 2.41478 avg_loss = 3.68641\n",
      "epoch no.1 train no.113350  loss = 4.84963 avg_loss = 3.73064\n",
      "epoch no.1 train no.113360  loss = 2.84412 avg_loss = 3.67122\n",
      "epoch no.1 train no.113370  loss = 4.13689 avg_loss = 3.66820\n",
      "epoch no.1 train no.113380  loss = 5.14540 avg_loss = 3.70599\n",
      "epoch no.1 train no.113390  loss = 5.78214 avg_loss = 3.70061\n",
      "epoch no.1 train no.113400  loss = 2.41806 avg_loss = 3.66832\n",
      "epoch no.1 train no.113410  loss = 3.88104 avg_loss = 3.66849\n",
      "epoch no.1 train no.113420  loss = 4.24054 avg_loss = 3.70095\n",
      "epoch no.1 train no.113430  loss = 4.91663 avg_loss = 3.65645\n",
      "epoch no.1 train no.113440  loss = 4.50152 avg_loss = 3.72462\n",
      "epoch no.1 train no.113450  loss = 4.95031 avg_loss = 3.73398\n",
      "epoch no.1 train no.113460  loss = 4.66912 avg_loss = 3.78898\n",
      "epoch no.1 train no.113470  loss = 4.10108 avg_loss = 3.76284\n",
      "epoch no.1 train no.113480  loss = 2.78963 avg_loss = 3.72803\n",
      "epoch no.1 train no.113490  loss = 4.09315 avg_loss = 3.69832\n",
      "epoch no.1 train no.113500  loss = 3.38932 avg_loss = 3.71433\n",
      "epoch no.1 train no.113510  loss = 4.21868 avg_loss = 3.76437\n",
      "epoch no.1 train no.113520  loss = 3.23119 avg_loss = 3.75982\n",
      "epoch no.1 train no.113530  loss = 4.21465 avg_loss = 3.78152\n",
      "epoch no.1 train no.113540  loss = 3.16064 avg_loss = 3.79970\n",
      "epoch no.1 train no.113550  loss = 5.70314 avg_loss = 3.81219\n",
      "epoch no.1 train no.113560  loss = 2.18500 avg_loss = 3.76269\n",
      "epoch no.1 train no.113570  loss = 3.18203 avg_loss = 3.71140\n",
      "epoch no.1 train no.113580  loss = 4.09108 avg_loss = 3.76455\n",
      "epoch no.1 train no.113590  loss = 5.18940 avg_loss = 3.72080\n",
      "epoch no.1 train no.113600  loss = 3.03647 avg_loss = 3.72408\n",
      "epoch no.1 train no.113610  loss = 2.80570 avg_loss = 3.74370\n",
      "epoch no.1 train no.113620  loss = 2.74278 avg_loss = 3.76412\n",
      "epoch no.1 train no.113630  loss = 1.84315 avg_loss = 3.72479\n",
      "epoch no.1 train no.113640  loss = 4.22574 avg_loss = 3.66810\n",
      "epoch no.1 train no.113650  loss = 2.53325 avg_loss = 3.64491\n",
      "epoch no.1 train no.113660  loss = 2.44517 avg_loss = 3.63052\n",
      "epoch no.1 train no.113670  loss = 4.86664 avg_loss = 3.62999\n",
      "epoch no.1 train no.113680  loss = 5.90070 avg_loss = 3.63094\n",
      "epoch no.1 train no.113690  loss = 3.05478 avg_loss = 3.62503\n",
      "epoch no.1 train no.113700  loss = 5.10379 avg_loss = 3.62977\n",
      "epoch no.1 train no.113710  loss = 2.30747 avg_loss = 3.55268\n",
      "epoch no.1 train no.113720  loss = 2.75228 avg_loss = 3.53061\n",
      "epoch no.1 train no.113730  loss = 4.64998 avg_loss = 3.54240\n",
      "epoch no.1 train no.113740  loss = 5.14659 avg_loss = 3.54337\n",
      "epoch no.1 train no.113750  loss = 2.62213 avg_loss = 3.59054\n",
      "epoch no.1 train no.113760  loss = 5.80290 avg_loss = 3.60814\n",
      "epoch no.1 train no.113770  loss = 6.30625 avg_loss = 3.68798\n",
      "epoch no.1 train no.113780  loss = 2.45405 avg_loss = 3.64230\n",
      "epoch no.1 train no.113790  loss = 3.05617 avg_loss = 3.62156\n",
      "epoch no.1 train no.113800  loss = 4.51434 avg_loss = 3.65465\n",
      "epoch no.1 train no.113810  loss = 4.93588 avg_loss = 3.66845\n",
      "epoch no.1 train no.113820  loss = 7.78706 avg_loss = 3.71115\n",
      "epoch no.1 train no.113830  loss = 2.90345 avg_loss = 3.69201\n",
      "epoch no.1 train no.113840  loss = 4.03089 avg_loss = 3.68764\n",
      "epoch no.1 train no.113850  loss = 5.58362 avg_loss = 3.75154\n",
      "epoch no.1 train no.113860  loss = 3.76922 avg_loss = 3.71749\n",
      "epoch no.1 train no.113870  loss = 2.94269 avg_loss = 3.73856\n",
      "epoch no.1 train no.113880  loss = 3.60318 avg_loss = 3.73893\n",
      "epoch no.1 train no.113890  loss = 3.38247 avg_loss = 3.73868\n",
      "epoch no.1 train no.113900  loss = 3.34442 avg_loss = 3.73534\n",
      "epoch no.1 train no.113910  loss = 7.31148 avg_loss = 3.74750\n",
      "epoch no.1 train no.113920  loss = 6.31877 avg_loss = 3.79180\n",
      "epoch no.1 train no.113930  loss = 2.86383 avg_loss = 3.70902\n",
      "epoch no.1 train no.113940  loss = 1.69356 avg_loss = 3.68181\n",
      "epoch no.1 train no.113950  loss = 2.68777 avg_loss = 3.69011\n",
      "epoch no.1 train no.113960  loss = 3.58909 avg_loss = 3.67243\n",
      "epoch no.1 train no.113970  loss = 3.03319 avg_loss = 3.68619\n",
      "epoch no.1 train no.113980  loss = 1.77721 avg_loss = 3.73343\n",
      "epoch no.1 train no.113990  loss = 2.20187 avg_loss = 3.72869\n",
      "epoch no.1 train no.114000  loss = 5.84235 avg_loss = 3.77453\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '▁노래', '</s>']\n",
      "여름밤에 듣는 시원한 노래</s>\n",
      "epoch no.1 train no.114010  loss = 5.92969 avg_loss = 3.80593\n",
      "epoch no.1 train no.114020  loss = 3.31623 avg_loss = 3.79650\n",
      "epoch no.1 train no.114030  loss = 2.93204 avg_loss = 3.82669\n",
      "epoch no.1 train no.114040  loss = 2.91286 avg_loss = 3.75969\n",
      "epoch no.1 train no.114050  loss = 2.52963 avg_loss = 3.76134\n",
      "epoch no.1 train no.114060  loss = 2.81787 avg_loss = 3.71483\n",
      "epoch no.1 train no.114070  loss = 2.18647 avg_loss = 3.70236\n",
      "epoch no.1 train no.114080  loss = 3.69326 avg_loss = 3.71303\n",
      "epoch no.1 train no.114090  loss = 3.86599 avg_loss = 3.67261\n",
      "epoch no.1 train no.114100  loss = 3.00471 avg_loss = 3.67691\n",
      "epoch no.1 train no.114110  loss = 2.78374 avg_loss = 3.67450\n",
      "epoch no.1 train no.114120  loss = 2.93604 avg_loss = 3.67821\n",
      "epoch no.1 train no.114130  loss = 4.97506 avg_loss = 3.73617\n",
      "epoch no.1 train no.114140  loss = 3.10463 avg_loss = 3.69631\n",
      "epoch no.1 train no.114150  loss = 3.35295 avg_loss = 3.74289\n",
      "epoch no.1 train no.114160  loss = 3.60250 avg_loss = 3.72883\n",
      "epoch no.1 train no.114170  loss = 4.36359 avg_loss = 3.76320\n",
      "epoch no.1 train no.114180  loss = 2.55128 avg_loss = 3.75095\n",
      "epoch no.1 train no.114190  loss = 3.67329 avg_loss = 3.72067\n",
      "epoch no.1 train no.114200  loss = 3.88261 avg_loss = 3.69368\n",
      "epoch no.1 train no.114210  loss = 4.34446 avg_loss = 3.73947\n",
      "epoch no.1 train no.114220  loss = 3.38258 avg_loss = 3.72863\n",
      "epoch no.1 train no.114230  loss = 4.55197 avg_loss = 3.73356\n",
      "epoch no.1 train no.114240  loss = 2.71432 avg_loss = 3.72970\n",
      "epoch no.1 train no.114250  loss = 3.74403 avg_loss = 3.70506\n",
      "epoch no.1 train no.114260  loss = 4.61848 avg_loss = 3.69313\n",
      "epoch no.1 train no.114270  loss = 3.16313 avg_loss = 3.69201\n",
      "epoch no.1 train no.114280  loss = 2.02852 avg_loss = 3.67617\n",
      "epoch no.1 train no.114290  loss = 3.03883 avg_loss = 3.69200\n",
      "epoch no.1 train no.114300  loss = 3.60691 avg_loss = 3.67197\n",
      "epoch no.1 train no.114310  loss = 3.04427 avg_loss = 3.62494\n",
      "epoch no.1 train no.114320  loss = 2.37866 avg_loss = 3.61631\n",
      "epoch no.1 train no.114330  loss = 2.80088 avg_loss = 3.64677\n",
      "epoch no.1 train no.114340  loss = 3.20128 avg_loss = 3.65092\n",
      "epoch no.1 train no.114350  loss = 3.85282 avg_loss = 3.68836\n",
      "epoch no.1 train no.114360  loss = 6.67285 avg_loss = 3.74062\n",
      "epoch no.1 train no.114370  loss = 3.25095 avg_loss = 3.74337\n",
      "epoch no.1 train no.114380  loss = 3.34889 avg_loss = 3.76025\n",
      "epoch no.1 train no.114390  loss = 6.09046 avg_loss = 3.81415\n",
      "epoch no.1 train no.114400  loss = 5.17919 avg_loss = 3.77283\n",
      "epoch no.1 train no.114410  loss = 4.29660 avg_loss = 3.73663\n",
      "epoch no.1 train no.114420  loss = 2.91054 avg_loss = 3.75144\n",
      "epoch no.1 train no.114430  loss = 3.59478 avg_loss = 3.74302\n",
      "epoch no.1 train no.114440  loss = 2.80187 avg_loss = 3.72146\n",
      "epoch no.1 train no.114450  loss = 4.82004 avg_loss = 3.73577\n",
      "epoch no.1 train no.114460  loss = 2.31071 avg_loss = 3.74406\n",
      "epoch no.1 train no.114470  loss = 3.94606 avg_loss = 3.73526\n",
      "epoch no.1 train no.114480  loss = 3.80954 avg_loss = 3.80826\n",
      "epoch no.1 train no.114490  loss = 3.84282 avg_loss = 3.80400\n",
      "epoch no.1 train no.114500  loss = 1.90738 avg_loss = 3.73928\n",
      "epoch no.1 train no.114510  loss = 2.76343 avg_loss = 3.75893\n",
      "epoch no.1 train no.114520  loss = 3.03907 avg_loss = 3.77111\n",
      "epoch no.1 train no.114530  loss = 4.95671 avg_loss = 3.77749\n",
      "epoch no.1 train no.114540  loss = 3.80684 avg_loss = 3.74066\n",
      "epoch no.1 train no.114550  loss = 2.92750 avg_loss = 3.74584\n",
      "epoch no.1 train no.114560  loss = 4.80318 avg_loss = 3.75972\n",
      "epoch no.1 train no.114570  loss = 4.51551 avg_loss = 3.74986\n",
      "epoch no.1 train no.114580  loss = 4.35896 avg_loss = 3.71903\n",
      "epoch no.1 train no.114590  loss = 3.91392 avg_loss = 3.74799\n",
      "epoch no.1 train no.114600  loss = 2.55243 avg_loss = 3.75565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.114610  loss = 3.78223 avg_loss = 3.73624\n",
      "epoch no.1 train no.114620  loss = 4.88561 avg_loss = 3.71671\n",
      "epoch no.1 train no.114630  loss = 3.75339 avg_loss = 3.72521\n",
      "epoch no.1 train no.114640  loss = 3.54816 avg_loss = 3.73383\n",
      "epoch no.1 train no.114650  loss = 2.12737 avg_loss = 3.69317\n",
      "epoch no.1 train no.114660  loss = 4.30944 avg_loss = 3.68589\n",
      "epoch no.1 train no.114670  loss = 4.49074 avg_loss = 3.67512\n",
      "epoch no.1 train no.114680  loss = 4.39790 avg_loss = 3.71925\n",
      "epoch no.1 train no.114690  loss = 3.33263 avg_loss = 3.75448\n",
      "epoch no.1 train no.114700  loss = 4.48812 avg_loss = 3.73983\n",
      "epoch no.1 train no.114710  loss = 3.98175 avg_loss = 3.73658\n",
      "epoch no.1 train no.114720  loss = 5.79297 avg_loss = 3.75339\n",
      "epoch no.1 train no.114730  loss = 3.63250 avg_loss = 3.76010\n",
      "epoch no.1 train no.114740  loss = 5.05208 avg_loss = 3.77464\n",
      "epoch no.1 train no.114750  loss = 3.51645 avg_loss = 3.79650\n",
      "epoch no.1 train no.114760  loss = 4.44786 avg_loss = 3.79263\n",
      "epoch no.1 train no.114770  loss = 2.63008 avg_loss = 3.79013\n",
      "epoch no.1 train no.114780  loss = 1.37945 avg_loss = 3.76486\n",
      "epoch no.1 train no.114790  loss = 4.32365 avg_loss = 3.75399\n",
      "epoch no.1 train no.114800  loss = 2.29926 avg_loss = 3.69051\n",
      "epoch no.1 train no.114810  loss = 2.97001 avg_loss = 3.62194\n",
      "epoch no.1 train no.114820  loss = 3.18865 avg_loss = 3.58956\n",
      "epoch no.1 train no.114830  loss = 2.96743 avg_loss = 3.54740\n",
      "epoch no.1 train no.114840  loss = 4.43934 avg_loss = 3.57405\n",
      "epoch no.1 train no.114850  loss = 3.17881 avg_loss = 3.55902\n",
      "epoch no.1 train no.114860  loss = 4.74606 avg_loss = 3.53841\n",
      "epoch no.1 train no.114870  loss = 2.44437 avg_loss = 3.52680\n",
      "epoch no.1 train no.114880  loss = 2.53335 avg_loss = 3.48129\n",
      "epoch no.1 train no.114890  loss = 3.38175 avg_loss = 3.52656\n",
      "epoch no.1 train no.114900  loss = 4.60036 avg_loss = 3.52903\n",
      "epoch no.1 train no.114910  loss = 3.74981 avg_loss = 3.50682\n",
      "epoch no.1 train no.114920  loss = 4.45156 avg_loss = 3.54536\n",
      "epoch no.1 train no.114930  loss = 3.07001 avg_loss = 3.58796\n",
      "epoch no.1 train no.114940  loss = 3.21307 avg_loss = 3.56501\n",
      "epoch no.1 train no.114950  loss = 2.80034 avg_loss = 3.61679\n",
      "epoch no.1 train no.114960  loss = 2.25260 avg_loss = 3.58881\n",
      "epoch no.1 train no.114970  loss = 5.46322 avg_loss = 3.59281\n",
      "epoch no.1 train no.114980  loss = 4.43086 avg_loss = 3.59623\n",
      "epoch no.1 train no.114990  loss = 4.65414 avg_loss = 3.63640\n",
      "epoch no.1 train no.115000  loss = 2.94535 avg_loss = 3.59850\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '발', '</s>']\n",
      "여름밤에 듣는 감성팝</s>\n",
      "epoch no.1 train no.115010  loss = 2.40940 avg_loss = 3.60806\n",
      "epoch no.1 train no.115020  loss = 3.56689 avg_loss = 3.61352\n",
      "epoch no.1 train no.115030  loss = 3.23394 avg_loss = 3.64618\n",
      "epoch no.1 train no.115040  loss = 2.95960 avg_loss = 3.67938\n",
      "epoch no.1 train no.115050  loss = 2.76143 avg_loss = 3.65985\n",
      "epoch no.1 train no.115060  loss = 3.77106 avg_loss = 3.70019\n",
      "epoch no.1 train no.115070  loss = 3.37263 avg_loss = 3.68930\n",
      "epoch no.1 train no.115080  loss = 2.82604 avg_loss = 3.70744\n",
      "epoch no.1 train no.115090  loss = 3.53312 avg_loss = 3.67340\n",
      "epoch no.1 train no.115100  loss = 4.40039 avg_loss = 3.69005\n",
      "epoch no.1 train no.115110  loss = 3.03379 avg_loss = 3.65377\n",
      "epoch no.1 train no.115120  loss = 5.51325 avg_loss = 3.69029\n",
      "epoch no.1 train no.115130  loss = 5.20735 avg_loss = 3.66355\n",
      "epoch no.1 train no.115140  loss = 2.56301 avg_loss = 3.63557\n",
      "epoch no.1 train no.115150  loss = 2.72395 avg_loss = 3.63002\n",
      "epoch no.1 train no.115160  loss = 4.04758 avg_loss = 3.59451\n",
      "epoch no.1 train no.115170  loss = 4.64165 avg_loss = 3.58610\n",
      "epoch no.1 train no.115180  loss = 5.60347 avg_loss = 3.62024\n",
      "epoch no.1 train no.115190  loss = 4.07292 avg_loss = 3.58494\n",
      "epoch no.1 train no.115200  loss = 6.44444 avg_loss = 3.59727\n",
      "epoch no.1 train no.115210  loss = 5.27247 avg_loss = 3.60375\n",
      "epoch no.1 train no.115220  loss = 1.98964 avg_loss = 3.63423\n",
      "epoch no.1 train no.115230  loss = 3.57107 avg_loss = 3.65117\n",
      "epoch no.1 train no.115240  loss = 3.43507 avg_loss = 3.65571\n",
      "epoch no.1 train no.115250  loss = 2.61190 avg_loss = 3.65218\n",
      "epoch no.1 train no.115260  loss = 3.20258 avg_loss = 3.68165\n",
      "epoch no.1 train no.115270  loss = 1.77462 avg_loss = 3.67558\n",
      "epoch no.1 train no.115280  loss = 2.73928 avg_loss = 3.63281\n",
      "epoch no.1 train no.115290  loss = 2.75426 avg_loss = 3.64411\n",
      "epoch no.1 train no.115300  loss = 4.36103 avg_loss = 3.67782\n",
      "epoch no.1 train no.115310  loss = 3.51778 avg_loss = 3.65847\n",
      "epoch no.1 train no.115320  loss = 3.37521 avg_loss = 3.61377\n",
      "epoch no.1 train no.115330  loss = 3.22164 avg_loss = 3.67923\n",
      "epoch no.1 train no.115340  loss = 2.73447 avg_loss = 3.68645\n",
      "epoch no.1 train no.115350  loss = 3.66140 avg_loss = 3.66423\n",
      "epoch no.1 train no.115360  loss = 4.10083 avg_loss = 3.69984\n",
      "epoch no.1 train no.115370  loss = 2.98880 avg_loss = 3.73392\n",
      "epoch no.1 train no.115380  loss = 3.11224 avg_loss = 3.68062\n",
      "epoch no.1 train no.115390  loss = 1.68625 avg_loss = 3.62703\n",
      "epoch no.1 train no.115400  loss = 2.06359 avg_loss = 3.62538\n",
      "epoch no.1 train no.115410  loss = 2.13628 avg_loss = 3.66522\n",
      "epoch no.1 train no.115420  loss = 2.83565 avg_loss = 3.66875\n",
      "epoch no.1 train no.115430  loss = 5.29907 avg_loss = 3.67834\n",
      "epoch no.1 train no.115440  loss = 5.21665 avg_loss = 3.73507\n",
      "epoch no.1 train no.115450  loss = 4.38117 avg_loss = 3.69419\n",
      "epoch no.1 train no.115460  loss = 4.04901 avg_loss = 3.69718\n",
      "epoch no.1 train no.115470  loss = 4.93795 avg_loss = 3.69461\n",
      "epoch no.1 train no.115480  loss = 3.29250 avg_loss = 3.71093\n",
      "epoch no.1 train no.115490  loss = 2.74122 avg_loss = 3.75591\n",
      "epoch no.1 train no.115500  loss = 2.42663 avg_loss = 3.76953\n",
      "epoch no.1 train no.115510  loss = 2.34156 avg_loss = 3.71586\n",
      "epoch no.1 train no.115520  loss = 2.46419 avg_loss = 3.67728\n",
      "epoch no.1 train no.115530  loss = 5.77652 avg_loss = 3.68019\n",
      "epoch no.1 train no.115540  loss = 4.41925 avg_loss = 3.65841\n",
      "epoch no.1 train no.115550  loss = 4.74816 avg_loss = 3.66290\n",
      "epoch no.1 train no.115560  loss = 5.12179 avg_loss = 3.64817\n",
      "epoch no.1 train no.115570  loss = 2.73986 avg_loss = 3.68505\n",
      "epoch no.1 train no.115580  loss = 3.62487 avg_loss = 3.65508\n",
      "epoch no.1 train no.115590  loss = 4.11676 avg_loss = 3.64337\n",
      "epoch no.1 train no.115600  loss = 3.01111 avg_loss = 3.64291\n",
      "epoch no.1 train no.115610  loss = 4.20946 avg_loss = 3.68348\n",
      "epoch no.1 train no.115620  loss = 4.81140 avg_loss = 3.71600\n",
      "epoch no.1 train no.115630  loss = 5.77169 avg_loss = 3.75747\n",
      "epoch no.1 train no.115640  loss = 2.94376 avg_loss = 3.69514\n",
      "epoch no.1 train no.115650  loss = 4.96580 avg_loss = 3.71799\n",
      "epoch no.1 train no.115660  loss = 4.23845 avg_loss = 3.74428\n",
      "epoch no.1 train no.115670  loss = 2.82961 avg_loss = 3.77799\n",
      "epoch no.1 train no.115680  loss = 4.13047 avg_loss = 3.72730\n",
      "epoch no.1 train no.115690  loss = 1.93893 avg_loss = 3.68796\n",
      "epoch no.1 train no.115700  loss = 4.90550 avg_loss = 3.66125\n",
      "epoch no.1 train no.115710  loss = 2.29054 avg_loss = 3.66887\n",
      "epoch no.1 train no.115720  loss = 4.15436 avg_loss = 3.68558\n",
      "epoch no.1 train no.115730  loss = 4.91361 avg_loss = 3.68644\n",
      "epoch no.1 train no.115740  loss = 4.01186 avg_loss = 3.73520\n",
      "epoch no.1 train no.115750  loss = 2.93177 avg_loss = 3.70642\n",
      "epoch no.1 train no.115760  loss = 2.97459 avg_loss = 3.74538\n",
      "epoch no.1 train no.115770  loss = 4.58920 avg_loss = 3.74675\n",
      "epoch no.1 train no.115780  loss = 3.38868 avg_loss = 3.74408\n",
      "epoch no.1 train no.115790  loss = 3.53240 avg_loss = 3.76259\n",
      "epoch no.1 train no.115800  loss = 5.13161 avg_loss = 3.75230\n",
      "epoch no.1 train no.115810  loss = 6.01134 avg_loss = 3.83789\n",
      "epoch no.1 train no.115820  loss = 2.62086 avg_loss = 3.77856\n",
      "epoch no.1 train no.115830  loss = 2.43572 avg_loss = 3.74944\n",
      "epoch no.1 train no.115840  loss = 4.11718 avg_loss = 3.69748\n",
      "epoch no.1 train no.115850  loss = 3.57876 avg_loss = 3.69399\n",
      "epoch no.1 train no.115860  loss = 3.26099 avg_loss = 3.71252\n",
      "epoch no.1 train no.115870  loss = 2.34823 avg_loss = 3.71609\n",
      "epoch no.1 train no.115880  loss = 4.45816 avg_loss = 3.69746\n",
      "epoch no.1 train no.115890  loss = 4.42338 avg_loss = 3.76942\n",
      "epoch no.1 train no.115900  loss = 4.95035 avg_loss = 3.82019\n",
      "epoch no.1 train no.115910  loss = 3.42953 avg_loss = 3.79901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.115920  loss = 4.81837 avg_loss = 3.80166\n",
      "epoch no.1 train no.115930  loss = 3.63967 avg_loss = 3.75426\n",
      "epoch no.1 train no.115940  loss = 2.36429 avg_loss = 3.73647\n",
      "epoch no.1 train no.115950  loss = 2.80713 avg_loss = 3.70568\n",
      "epoch no.1 train no.115960  loss = 5.34847 avg_loss = 3.66710\n",
      "epoch no.1 train no.115970  loss = 3.01914 avg_loss = 3.61901\n",
      "epoch no.1 train no.115980  loss = 2.63994 avg_loss = 3.60774\n",
      "epoch no.1 train no.115990  loss = 2.21353 avg_loss = 3.58243\n",
      "epoch no.1 train no.116000  loss = 4.14404 avg_loss = 3.56219\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '음악', '</s>']\n",
      "여름밤에 듣기 좋은 인디음악</s>\n",
      "epoch no.1 train no.116010  loss = 3.39014 avg_loss = 3.51735\n",
      "epoch no.1 train no.116020  loss = 2.97564 avg_loss = 3.53065\n",
      "epoch no.1 train no.116030  loss = 4.35985 avg_loss = 3.51270\n",
      "epoch no.1 train no.116040  loss = 5.15698 avg_loss = 3.50374\n",
      "epoch no.1 train no.116050  loss = 3.10300 avg_loss = 3.53368\n",
      "epoch no.1 train no.116060  loss = 3.78158 avg_loss = 3.58802\n",
      "epoch no.1 train no.116070  loss = 3.06487 avg_loss = 3.59164\n",
      "epoch no.1 train no.116080  loss = 5.31569 avg_loss = 3.61312\n",
      "epoch no.1 train no.116090  loss = 2.97694 avg_loss = 3.60561\n",
      "epoch no.1 train no.116100  loss = 2.74337 avg_loss = 3.57390\n",
      "epoch no.1 train no.116110  loss = 6.37833 avg_loss = 3.65308\n",
      "epoch no.1 train no.116120  loss = 4.05631 avg_loss = 3.64074\n",
      "epoch no.1 train no.116130  loss = 3.77608 avg_loss = 3.63707\n",
      "epoch no.1 train no.116140  loss = 2.96919 avg_loss = 3.60037\n",
      "epoch no.1 train no.116150  loss = 5.51355 avg_loss = 3.72656\n",
      "epoch no.1 train no.116160  loss = 4.61101 avg_loss = 3.71754\n",
      "epoch no.1 train no.116170  loss = 2.98107 avg_loss = 3.70600\n",
      "epoch no.1 train no.116180  loss = 2.67651 avg_loss = 3.71210\n",
      "epoch no.1 train no.116190  loss = 2.90112 avg_loss = 3.68848\n",
      "epoch no.1 train no.116200  loss = 5.33862 avg_loss = 3.64086\n",
      "epoch no.1 train no.116210  loss = 3.50306 avg_loss = 3.65082\n",
      "epoch no.1 train no.116220  loss = 3.85458 avg_loss = 3.70923\n",
      "epoch no.1 train no.116230  loss = 3.15852 avg_loss = 3.69021\n",
      "epoch no.1 train no.116240  loss = 3.62737 avg_loss = 3.72980\n",
      "epoch no.1 train no.116250  loss = 4.99296 avg_loss = 3.71942\n",
      "epoch no.1 train no.116260  loss = 2.36688 avg_loss = 3.74447\n",
      "epoch no.1 train no.116270  loss = 2.62972 avg_loss = 3.71565\n",
      "epoch no.1 train no.116280  loss = 1.83563 avg_loss = 3.71762\n",
      "epoch no.1 train no.116290  loss = 6.50866 avg_loss = 3.70903\n",
      "epoch no.1 train no.116300  loss = 4.04265 avg_loss = 3.71421\n",
      "epoch no.1 train no.116310  loss = 2.54357 avg_loss = 3.66415\n",
      "epoch no.1 train no.116320  loss = 3.15257 avg_loss = 3.67736\n",
      "epoch no.1 train no.116330  loss = 4.18659 avg_loss = 3.69631\n",
      "epoch no.1 train no.116340  loss = 1.73258 avg_loss = 3.64395\n",
      "epoch no.1 train no.116350  loss = 6.70019 avg_loss = 3.74844\n",
      "epoch no.1 train no.116360  loss = 1.94988 avg_loss = 3.71064\n",
      "epoch no.1 train no.116370  loss = 3.36267 avg_loss = 3.63199\n",
      "epoch no.1 train no.116380  loss = 2.13533 avg_loss = 3.60882\n",
      "epoch no.1 train no.116390  loss = 2.46879 avg_loss = 3.60341\n",
      "epoch no.1 train no.116400  loss = 3.39196 avg_loss = 3.58980\n",
      "epoch no.1 train no.116410  loss = 3.81838 avg_loss = 3.59847\n",
      "epoch no.1 train no.116420  loss = 2.89975 avg_loss = 3.55775\n",
      "epoch no.1 train no.116430  loss = 2.71876 avg_loss = 3.57749\n",
      "epoch no.1 train no.116440  loss = 4.59154 avg_loss = 3.58546\n",
      "epoch no.1 train no.116450  loss = 5.03700 avg_loss = 3.59761\n",
      "epoch no.1 train no.116460  loss = 4.75675 avg_loss = 3.69817\n",
      "epoch no.1 train no.116470  loss = 3.41483 avg_loss = 3.69875\n",
      "epoch no.1 train no.116480  loss = 3.29355 avg_loss = 3.74011\n",
      "epoch no.1 train no.116490  loss = 2.98540 avg_loss = 3.69125\n",
      "epoch no.1 train no.116500  loss = 4.60574 avg_loss = 3.72796\n",
      "epoch no.1 train no.116510  loss = 4.50492 avg_loss = 3.74221\n",
      "epoch no.1 train no.116520  loss = 2.21062 avg_loss = 3.74303\n",
      "epoch no.1 train no.116530  loss = 2.76622 avg_loss = 3.74188\n",
      "epoch no.1 train no.116540  loss = 3.15973 avg_loss = 3.69573\n",
      "epoch no.1 train no.116550  loss = 5.54030 avg_loss = 3.72074\n",
      "epoch no.1 train no.116560  loss = 3.81406 avg_loss = 3.72883\n",
      "epoch no.1 train no.116570  loss = 4.00927 avg_loss = 3.73724\n",
      "epoch no.1 train no.116580  loss = 5.05450 avg_loss = 3.79957\n",
      "epoch no.1 train no.116590  loss = 3.73971 avg_loss = 3.81196\n",
      "epoch no.1 train no.116600  loss = 3.59996 avg_loss = 3.76010\n",
      "epoch no.1 train no.116610  loss = 3.14589 avg_loss = 3.71301\n",
      "epoch no.1 train no.116620  loss = 2.82730 avg_loss = 3.70284\n",
      "epoch no.1 train no.116630  loss = 3.45691 avg_loss = 3.72342\n",
      "epoch no.1 train no.116640  loss = 2.70237 avg_loss = 3.66715\n",
      "epoch no.1 train no.116650  loss = 4.20943 avg_loss = 3.73872\n",
      "epoch no.1 train no.116660  loss = 2.97224 avg_loss = 3.70455\n",
      "epoch no.1 train no.116670  loss = 6.87694 avg_loss = 3.72582\n",
      "epoch no.1 train no.116680  loss = 4.27864 avg_loss = 3.71538\n",
      "epoch no.1 train no.116690  loss = 3.05149 avg_loss = 3.68049\n",
      "epoch no.1 train no.116700  loss = 2.72792 avg_loss = 3.62366\n",
      "epoch no.1 train no.116710  loss = 4.33370 avg_loss = 3.68538\n",
      "epoch no.1 train no.116720  loss = 2.81199 avg_loss = 3.66097\n",
      "epoch no.1 train no.116730  loss = 4.70552 avg_loss = 3.69295\n",
      "epoch no.1 train no.116740  loss = 2.05953 avg_loss = 3.71201\n",
      "epoch no.1 train no.116750  loss = 2.59879 avg_loss = 3.67492\n",
      "epoch no.1 train no.116760  loss = 2.74028 avg_loss = 3.72433\n",
      "epoch no.1 train no.116770  loss = 2.22880 avg_loss = 3.69070\n",
      "epoch no.1 train no.116780  loss = 3.13934 avg_loss = 3.71975\n",
      "epoch no.1 train no.116790  loss = 1.86646 avg_loss = 3.70979\n",
      "epoch no.1 train no.116800  loss = 5.35948 avg_loss = 3.70171\n",
      "epoch no.1 train no.116810  loss = 1.55177 avg_loss = 3.66360\n",
      "epoch no.1 train no.116820  loss = 4.49264 avg_loss = 3.64017\n",
      "epoch no.1 train no.116830  loss = 1.99564 avg_loss = 3.60950\n",
      "epoch no.1 train no.116840  loss = 3.84233 avg_loss = 3.67478\n",
      "epoch no.1 train no.116850  loss = 4.04992 avg_loss = 3.64275\n",
      "epoch no.1 train no.116860  loss = 2.29536 avg_loss = 3.62648\n",
      "epoch no.1 train no.116870  loss = 4.80985 avg_loss = 3.66237\n",
      "epoch no.1 train no.116880  loss = 5.15431 avg_loss = 3.68814\n",
      "epoch no.1 train no.116890  loss = 3.24042 avg_loss = 3.72302\n",
      "epoch no.1 train no.116900  loss = 1.99834 avg_loss = 3.67803\n",
      "epoch no.1 train no.116910  loss = 2.77083 avg_loss = 3.63402\n",
      "epoch no.1 train no.116920  loss = 3.65880 avg_loss = 3.63682\n",
      "epoch no.1 train no.116930  loss = 4.61877 avg_loss = 3.61705\n",
      "epoch no.1 train no.116940  loss = 3.36279 avg_loss = 3.68550\n",
      "epoch no.1 train no.116950  loss = 3.31089 avg_loss = 3.68734\n",
      "epoch no.1 train no.116960  loss = 4.72049 avg_loss = 3.68331\n",
      "epoch no.1 train no.116970  loss = 2.34139 avg_loss = 3.70336\n",
      "epoch no.1 train no.116980  loss = 3.99823 avg_loss = 3.66060\n",
      "epoch no.1 train no.116990  loss = 4.34079 avg_loss = 3.66790\n",
      "epoch no.1 train no.117000  loss = 3.78734 avg_loss = 3.66602\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁감성', '▁음악', '▁함께', '</s>']\n",
      "여름밤의 힐링과 감성</s>\n",
      "epoch no.1 train no.117010  loss = 3.75604 avg_loss = 3.71149\n",
      "epoch no.1 train no.117020  loss = 5.13561 avg_loss = 3.76915\n",
      "epoch no.1 train no.117030  loss = 3.96127 avg_loss = 3.76033\n",
      "epoch no.1 train no.117040  loss = 4.85076 avg_loss = 3.73338\n",
      "epoch no.1 train no.117050  loss = 2.65804 avg_loss = 3.75017\n",
      "epoch no.1 train no.117060  loss = 2.26959 avg_loss = 3.74360\n",
      "epoch no.1 train no.117070  loss = 3.69561 avg_loss = 3.70038\n",
      "epoch no.1 train no.117080  loss = 3.69100 avg_loss = 3.69169\n",
      "epoch no.1 train no.117090  loss = 2.18227 avg_loss = 3.72225\n",
      "epoch no.1 train no.117100  loss = 3.51533 avg_loss = 3.73194\n",
      "epoch no.1 train no.117110  loss = 1.35078 avg_loss = 3.67763\n",
      "epoch no.1 train no.117120  loss = 2.50562 avg_loss = 3.62023\n",
      "epoch no.1 train no.117130  loss = 2.93955 avg_loss = 3.57465\n",
      "epoch no.1 train no.117140  loss = 2.12375 avg_loss = 3.62043\n",
      "epoch no.1 train no.117150  loss = 3.73458 avg_loss = 3.62839\n",
      "epoch no.1 train no.117160  loss = 3.33638 avg_loss = 3.61083\n",
      "epoch no.1 train no.117170  loss = 3.29595 avg_loss = 3.61800\n",
      "epoch no.1 train no.117180  loss = 3.24887 avg_loss = 3.62545\n",
      "epoch no.1 train no.117190  loss = 4.35043 avg_loss = 3.59406\n",
      "epoch no.1 train no.117200  loss = 2.45858 avg_loss = 3.56651\n",
      "epoch no.1 train no.117210  loss = 2.99407 avg_loss = 3.54513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.117220  loss = 2.84673 avg_loss = 3.58763\n",
      "epoch no.1 train no.117230  loss = 3.27289 avg_loss = 3.61002\n",
      "epoch no.1 train no.117240  loss = 4.23319 avg_loss = 3.63544\n",
      "epoch no.1 train no.117250  loss = 4.00327 avg_loss = 3.65994\n",
      "epoch no.1 train no.117260  loss = 3.37507 avg_loss = 3.67127\n",
      "epoch no.1 train no.117270  loss = 2.65217 avg_loss = 3.66791\n",
      "epoch no.1 train no.117280  loss = 3.33874 avg_loss = 3.65132\n",
      "epoch no.1 train no.117290  loss = 5.60791 avg_loss = 3.66169\n",
      "epoch no.1 train no.117300  loss = 2.14407 avg_loss = 3.65486\n",
      "epoch no.1 train no.117310  loss = 2.93471 avg_loss = 3.67958\n",
      "epoch no.1 train no.117320  loss = 3.45318 avg_loss = 3.71008\n",
      "epoch no.1 train no.117330  loss = 3.13316 avg_loss = 3.67142\n",
      "epoch no.1 train no.117340  loss = 2.19103 avg_loss = 3.63775\n",
      "epoch no.1 train no.117350  loss = 3.64752 avg_loss = 3.66930\n",
      "epoch no.1 train no.117360  loss = 4.52360 avg_loss = 3.71009\n",
      "epoch no.1 train no.117370  loss = 4.36692 avg_loss = 3.71284\n",
      "epoch no.1 train no.117380  loss = 3.91682 avg_loss = 3.72953\n",
      "epoch no.1 train no.117390  loss = 4.15155 avg_loss = 3.70427\n",
      "epoch no.1 train no.117400  loss = 4.38940 avg_loss = 3.68659\n",
      "epoch no.1 train no.117410  loss = 5.18384 avg_loss = 3.71995\n",
      "epoch no.1 train no.117420  loss = 4.22374 avg_loss = 3.73525\n",
      "epoch no.1 train no.117430  loss = 4.58499 avg_loss = 3.72290\n",
      "epoch no.1 train no.117440  loss = 3.24605 avg_loss = 3.73121\n",
      "epoch no.1 train no.117450  loss = 1.99633 avg_loss = 3.72542\n",
      "epoch no.1 train no.117460  loss = 4.83476 avg_loss = 3.74537\n",
      "epoch no.1 train no.117470  loss = 3.13600 avg_loss = 3.75423\n",
      "epoch no.1 train no.117480  loss = 4.11299 avg_loss = 3.70198\n",
      "epoch no.1 train no.117490  loss = 2.39377 avg_loss = 3.69301\n",
      "epoch no.1 train no.117500  loss = 2.74920 avg_loss = 3.66875\n",
      "epoch no.1 train no.117510  loss = 3.55980 avg_loss = 3.66974\n",
      "epoch no.1 train no.117520  loss = 3.11290 avg_loss = 3.67599\n",
      "epoch no.1 train no.117530  loss = 2.27175 avg_loss = 3.63578\n",
      "epoch no.1 train no.117540  loss = 2.97095 avg_loss = 3.60632\n",
      "epoch no.1 train no.117550  loss = 4.53652 avg_loss = 3.62187\n",
      "epoch no.1 train no.117560  loss = 3.42299 avg_loss = 3.56915\n",
      "epoch no.1 train no.117570  loss = 3.63417 avg_loss = 3.59657\n",
      "epoch no.1 train no.117580  loss = 3.25030 avg_loss = 3.59300\n",
      "epoch no.1 train no.117590  loss = 3.89805 avg_loss = 3.61755\n",
      "epoch no.1 train no.117600  loss = 3.42727 avg_loss = 3.58490\n",
      "epoch no.1 train no.117610  loss = 3.41256 avg_loss = 3.62972\n",
      "epoch no.1 train no.117620  loss = 3.60086 avg_loss = 3.61863\n",
      "epoch no.1 train no.117630  loss = 3.66700 avg_loss = 3.65519\n",
      "epoch no.1 train no.117640  loss = 4.51457 avg_loss = 3.61964\n",
      "epoch no.1 train no.117650  loss = 5.30445 avg_loss = 3.62978\n",
      "epoch no.1 train no.117660  loss = 2.98175 avg_loss = 3.64202\n",
      "epoch no.1 train no.117670  loss = 2.66339 avg_loss = 3.62433\n",
      "epoch no.1 train no.117680  loss = 4.65726 avg_loss = 3.66039\n",
      "epoch no.1 train no.117690  loss = 4.24802 avg_loss = 3.70808\n",
      "epoch no.1 train no.117700  loss = 3.05498 avg_loss = 3.71241\n",
      "epoch no.1 train no.117710  loss = 3.96662 avg_loss = 3.68213\n",
      "epoch no.1 train no.117720  loss = 2.00744 avg_loss = 3.66557\n",
      "epoch no.1 train no.117730  loss = 2.16311 avg_loss = 3.65424\n",
      "epoch no.1 train no.117740  loss = 4.86540 avg_loss = 3.64713\n",
      "epoch no.1 train no.117750  loss = 3.24338 avg_loss = 3.64726\n",
      "epoch no.1 train no.117760  loss = 3.58215 avg_loss = 3.67601\n",
      "epoch no.1 train no.117770  loss = 2.86794 avg_loss = 3.64464\n",
      "epoch no.1 train no.117780  loss = 2.20750 avg_loss = 3.63935\n",
      "epoch no.1 train no.117790  loss = 4.63086 avg_loss = 3.75509\n",
      "epoch no.1 train no.117800  loss = 2.13020 avg_loss = 3.74738\n",
      "epoch no.1 train no.117810  loss = 4.85686 avg_loss = 3.71900\n",
      "epoch no.1 train no.117820  loss = 3.05261 avg_loss = 3.72085\n",
      "epoch no.1 train no.117830  loss = 2.43026 avg_loss = 3.74275\n",
      "epoch no.1 train no.117840  loss = 4.95547 avg_loss = 3.79293\n",
      "epoch no.1 train no.117850  loss = 4.68177 avg_loss = 3.82739\n",
      "epoch no.1 train no.117860  loss = 2.91924 avg_loss = 3.79036\n",
      "epoch no.1 train no.117870  loss = 2.90285 avg_loss = 3.75185\n",
      "epoch no.1 train no.117880  loss = 1.01347 avg_loss = 3.74857\n",
      "epoch no.1 train no.117890  loss = 1.98287 avg_loss = 3.80228\n",
      "epoch no.1 train no.117900  loss = 4.15171 avg_loss = 3.77800\n",
      "epoch no.1 train no.117910  loss = 3.89942 avg_loss = 3.74668\n",
      "epoch no.1 train no.117920  loss = 1.74753 avg_loss = 3.71554\n",
      "epoch no.1 train no.117930  loss = 5.12625 avg_loss = 3.73977\n",
      "epoch no.1 train no.117940  loss = 5.84685 avg_loss = 3.75071\n",
      "epoch no.1 train no.117950  loss = 4.35793 avg_loss = 3.77762\n",
      "epoch no.1 train no.117960  loss = 3.78330 avg_loss = 3.79177\n",
      "epoch no.1 train no.117970  loss = 2.78518 avg_loss = 3.78470\n",
      "epoch no.1 train no.117980  loss = 2.38985 avg_loss = 3.75173\n",
      "epoch no.1 train no.117990  loss = 4.33010 avg_loss = 3.75867\n",
      "epoch no.1 train no.118000  loss = 3.71026 avg_loss = 3.75290\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 음악</s>\n",
      "epoch no.1 train no.118010  loss = 2.55509 avg_loss = 3.77544\n",
      "epoch no.1 train no.118020  loss = 2.93056 avg_loss = 3.76398\n",
      "epoch no.1 train no.118030  loss = 2.72434 avg_loss = 3.78508\n",
      "epoch no.1 train no.118040  loss = 5.30496 avg_loss = 3.78383\n",
      "epoch no.1 train no.118050  loss = 4.96880 avg_loss = 3.75082\n",
      "epoch no.1 train no.118060  loss = 2.32517 avg_loss = 3.72318\n",
      "epoch no.1 train no.118070  loss = 4.90938 avg_loss = 3.73634\n",
      "epoch no.1 train no.118080  loss = 2.21208 avg_loss = 3.70573\n",
      "epoch no.1 train no.118090  loss = 6.07699 avg_loss = 3.72377\n",
      "epoch no.1 train no.118100  loss = 3.93157 avg_loss = 3.75137\n",
      "epoch no.1 train no.118110  loss = 3.39271 avg_loss = 3.68762\n",
      "epoch no.1 train no.118120  loss = 2.79769 avg_loss = 3.70469\n",
      "epoch no.1 train no.118130  loss = 1.76558 avg_loss = 3.63847\n",
      "epoch no.1 train no.118140  loss = 3.83503 avg_loss = 3.66881\n",
      "epoch no.1 train no.118150  loss = 3.35015 avg_loss = 3.64527\n",
      "epoch no.1 train no.118160  loss = 3.60686 avg_loss = 3.65797\n",
      "epoch no.1 train no.118170  loss = 6.62182 avg_loss = 3.69631\n",
      "epoch no.1 train no.118180  loss = 3.58695 avg_loss = 3.74944\n",
      "epoch no.1 train no.118190  loss = 3.80499 avg_loss = 3.77192\n",
      "epoch no.1 train no.118200  loss = 3.60576 avg_loss = 3.76653\n",
      "epoch no.1 train no.118210  loss = 2.10280 avg_loss = 3.76085\n",
      "epoch no.1 train no.118220  loss = 3.48509 avg_loss = 3.76202\n",
      "epoch no.1 train no.118230  loss = 3.01766 avg_loss = 3.74305\n",
      "epoch no.1 train no.118240  loss = 5.70992 avg_loss = 3.77328\n",
      "epoch no.1 train no.118250  loss = 1.87065 avg_loss = 3.72677\n",
      "epoch no.1 train no.118260  loss = 4.48161 avg_loss = 3.67410\n",
      "epoch no.1 train no.118270  loss = 2.36044 avg_loss = 3.60512\n",
      "epoch no.1 train no.118280  loss = 3.59378 avg_loss = 3.63438\n",
      "epoch no.1 train no.118290  loss = 3.56020 avg_loss = 3.63433\n",
      "epoch no.1 train no.118300  loss = 3.01329 avg_loss = 3.62041\n",
      "epoch no.1 train no.118310  loss = 4.22174 avg_loss = 3.57401\n",
      "epoch no.1 train no.118320  loss = 2.85100 avg_loss = 3.56836\n",
      "epoch no.1 train no.118330  loss = 2.86746 avg_loss = 3.50813\n",
      "epoch no.1 train no.118340  loss = 2.49513 avg_loss = 3.48845\n",
      "epoch no.1 train no.118350  loss = 3.83248 avg_loss = 3.50549\n",
      "epoch no.1 train no.118360  loss = 4.06842 avg_loss = 3.52873\n",
      "epoch no.1 train no.118370  loss = 3.06004 avg_loss = 3.48989\n",
      "epoch no.1 train no.118380  loss = 3.83709 avg_loss = 3.50438\n",
      "epoch no.1 train no.118390  loss = 2.63047 avg_loss = 3.60489\n",
      "epoch no.1 train no.118400  loss = 5.08123 avg_loss = 3.56968\n",
      "epoch no.1 train no.118410  loss = 2.67241 avg_loss = 3.58265\n",
      "epoch no.1 train no.118420  loss = 4.47366 avg_loss = 3.63284\n",
      "epoch no.1 train no.118430  loss = 6.01579 avg_loss = 3.59475\n",
      "epoch no.1 train no.118440  loss = 3.64830 avg_loss = 3.57251\n",
      "epoch no.1 train no.118450  loss = 3.66777 avg_loss = 3.54953\n",
      "epoch no.1 train no.118460  loss = 1.99844 avg_loss = 3.52555\n",
      "epoch no.1 train no.118470  loss = 2.77139 avg_loss = 3.51655\n",
      "epoch no.1 train no.118480  loss = 3.20741 avg_loss = 3.52763\n",
      "epoch no.1 train no.118490  loss = 2.76928 avg_loss = 3.53858\n",
      "epoch no.1 train no.118500  loss = 2.40901 avg_loss = 3.51551\n",
      "epoch no.1 train no.118510  loss = 3.81436 avg_loss = 3.53156\n",
      "epoch no.1 train no.118520  loss = 5.25422 avg_loss = 3.56787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.118530  loss = 2.75349 avg_loss = 3.58056\n",
      "epoch no.1 train no.118540  loss = 5.85909 avg_loss = 3.61913\n",
      "epoch no.1 train no.118550  loss = 2.12034 avg_loss = 3.58016\n",
      "epoch no.1 train no.118560  loss = 2.27052 avg_loss = 3.54118\n",
      "epoch no.1 train no.118570  loss = 4.15503 avg_loss = 3.53327\n",
      "epoch no.1 train no.118580  loss = 2.61811 avg_loss = 3.52487\n",
      "epoch no.1 train no.118590  loss = 2.23967 avg_loss = 3.50948\n",
      "epoch no.1 train no.118600  loss = 1.96268 avg_loss = 3.51264\n",
      "epoch no.1 train no.118610  loss = 4.34773 avg_loss = 3.46476\n",
      "epoch no.1 train no.118620  loss = 1.93959 avg_loss = 3.48488\n",
      "epoch no.1 train no.118630  loss = 4.99834 avg_loss = 3.55292\n",
      "epoch no.1 train no.118640  loss = 2.78562 avg_loss = 3.55005\n",
      "epoch no.1 train no.118650  loss = 3.11923 avg_loss = 3.55902\n",
      "epoch no.1 train no.118660  loss = 5.02586 avg_loss = 3.61066\n",
      "epoch no.1 train no.118670  loss = 5.65461 avg_loss = 3.65113\n",
      "epoch no.1 train no.118680  loss = 2.08343 avg_loss = 3.58364\n",
      "epoch no.1 train no.118690  loss = 4.26596 avg_loss = 3.55492\n",
      "epoch no.1 train no.118700  loss = 2.89110 avg_loss = 3.53517\n",
      "epoch no.1 train no.118710  loss = 3.92602 avg_loss = 3.59003\n",
      "epoch no.1 train no.118720  loss = 3.56564 avg_loss = 3.60459\n",
      "epoch no.1 train no.118730  loss = 4.32354 avg_loss = 3.61100\n",
      "epoch no.1 train no.118740  loss = 3.37384 avg_loss = 3.60678\n",
      "epoch no.1 train no.118750  loss = 3.36027 avg_loss = 3.57937\n",
      "epoch no.1 train no.118760  loss = 4.22579 avg_loss = 3.56721\n",
      "epoch no.1 train no.118770  loss = 3.71256 avg_loss = 3.55330\n",
      "epoch no.1 train no.118780  loss = 6.44073 avg_loss = 3.62460\n",
      "epoch no.1 train no.118790  loss = 2.73179 avg_loss = 3.56392\n",
      "epoch no.1 train no.118800  loss = 2.15595 avg_loss = 3.56744\n",
      "epoch no.1 train no.118810  loss = 4.71215 avg_loss = 3.63304\n",
      "epoch no.1 train no.118820  loss = 2.61710 avg_loss = 3.63787\n",
      "epoch no.1 train no.118830  loss = 1.58121 avg_loss = 3.55257\n",
      "epoch no.1 train no.118840  loss = 2.80681 avg_loss = 3.54044\n",
      "epoch no.1 train no.118850  loss = 5.00378 avg_loss = 3.55187\n",
      "epoch no.1 train no.118860  loss = 2.25511 avg_loss = 3.58422\n",
      "epoch no.1 train no.118870  loss = 4.24480 avg_loss = 3.54288\n",
      "epoch no.1 train no.118880  loss = 2.90621 avg_loss = 3.54881\n",
      "epoch no.1 train no.118890  loss = 3.46262 avg_loss = 3.53230\n",
      "epoch no.1 train no.118900  loss = 3.41744 avg_loss = 3.53794\n",
      "epoch no.1 train no.118910  loss = 2.51233 avg_loss = 3.57481\n",
      "epoch no.1 train no.118920  loss = 3.01527 avg_loss = 3.62803\n",
      "epoch no.1 train no.118930  loss = 3.39038 avg_loss = 3.66104\n",
      "epoch no.1 train no.118940  loss = 3.61318 avg_loss = 3.68944\n",
      "epoch no.1 train no.118950  loss = 6.36086 avg_loss = 3.72192\n",
      "epoch no.1 train no.118960  loss = 3.47169 avg_loss = 3.73034\n",
      "epoch no.1 train no.118970  loss = 2.32106 avg_loss = 3.76574\n",
      "epoch no.1 train no.118980  loss = 4.31278 avg_loss = 3.71625\n",
      "epoch no.1 train no.118990  loss = 4.40729 avg_loss = 3.70794\n",
      "epoch no.1 train no.119000  loss = 1.43894 avg_loss = 3.68041\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '음악', '</s>']\n",
      "여름밤의 인디음악</s>\n",
      "epoch no.1 train no.119010  loss = 4.10621 avg_loss = 3.68499\n",
      "epoch no.1 train no.119020  loss = 3.86673 avg_loss = 3.74348\n",
      "epoch no.1 train no.119030  loss = 2.94964 avg_loss = 3.73428\n",
      "epoch no.1 train no.119040  loss = 3.89947 avg_loss = 3.74675\n",
      "epoch no.1 train no.119050  loss = 2.93283 avg_loss = 3.69837\n",
      "epoch no.1 train no.119060  loss = 2.34871 avg_loss = 3.69592\n",
      "epoch no.1 train no.119070  loss = 3.41509 avg_loss = 3.69972\n",
      "epoch no.1 train no.119080  loss = 2.00927 avg_loss = 3.74688\n",
      "epoch no.1 train no.119090  loss = 3.16671 avg_loss = 3.70052\n",
      "epoch no.1 train no.119100  loss = 4.27295 avg_loss = 3.71541\n",
      "epoch no.1 train no.119110  loss = 3.98594 avg_loss = 3.70252\n",
      "epoch no.1 train no.119120  loss = 4.72192 avg_loss = 3.68480\n",
      "epoch no.1 train no.119130  loss = 2.80273 avg_loss = 3.70868\n",
      "epoch no.1 train no.119140  loss = 3.11674 avg_loss = 3.71427\n",
      "epoch no.1 train no.119150  loss = 5.50182 avg_loss = 3.71012\n",
      "epoch no.1 train no.119160  loss = 4.55081 avg_loss = 3.68763\n",
      "epoch no.1 train no.119170  loss = 3.76711 avg_loss = 3.79264\n",
      "epoch no.1 train no.119180  loss = 2.78462 avg_loss = 3.76089\n",
      "epoch no.1 train no.119190  loss = 2.99190 avg_loss = 3.73428\n",
      "epoch no.1 train no.119200  loss = 4.66050 avg_loss = 3.69799\n",
      "epoch no.1 train no.119210  loss = 4.53016 avg_loss = 3.68419\n",
      "epoch no.1 train no.119220  loss = 3.74052 avg_loss = 3.66485\n",
      "epoch no.1 train no.119230  loss = 2.90913 avg_loss = 3.67672\n",
      "epoch no.1 train no.119240  loss = 4.43788 avg_loss = 3.67286\n",
      "epoch no.1 train no.119250  loss = 3.72451 avg_loss = 3.70219\n",
      "epoch no.1 train no.119260  loss = 5.47952 avg_loss = 3.69162\n",
      "epoch no.1 train no.119270  loss = 4.64950 avg_loss = 3.69179\n",
      "epoch no.1 train no.119280  loss = 3.08934 avg_loss = 3.69920\n",
      "epoch no.1 train no.119290  loss = 2.60316 avg_loss = 3.67499\n",
      "epoch no.1 train no.119300  loss = 3.18420 avg_loss = 3.60003\n",
      "epoch no.1 train no.119310  loss = 4.40264 avg_loss = 3.62575\n",
      "epoch no.1 train no.119320  loss = 7.45575 avg_loss = 3.72168\n",
      "epoch no.1 train no.119330  loss = 3.18099 avg_loss = 3.71365\n",
      "epoch no.1 train no.119340  loss = 4.36590 avg_loss = 3.67006\n",
      "epoch no.1 train no.119350  loss = 2.55302 avg_loss = 3.67339\n",
      "epoch no.1 train no.119360  loss = 2.72218 avg_loss = 3.63739\n",
      "epoch no.1 train no.119370  loss = 4.04835 avg_loss = 3.66942\n",
      "epoch no.1 train no.119380  loss = 2.66157 avg_loss = 3.71049\n",
      "epoch no.1 train no.119390  loss = 3.07263 avg_loss = 3.73703\n",
      "epoch no.1 train no.119400  loss = 5.36909 avg_loss = 3.75535\n",
      "epoch no.1 train no.119410  loss = 4.41953 avg_loss = 3.76847\n",
      "epoch no.1 train no.119420  loss = 3.32078 avg_loss = 3.72507\n",
      "epoch no.1 train no.119430  loss = 2.38492 avg_loss = 3.70537\n",
      "epoch no.1 train no.119440  loss = 3.81728 avg_loss = 3.70683\n",
      "epoch no.1 train no.119450  loss = 4.29261 avg_loss = 3.71282\n",
      "epoch no.1 train no.119460  loss = 2.63671 avg_loss = 3.71545\n",
      "epoch no.1 train no.119470  loss = 1.63311 avg_loss = 3.69898\n",
      "epoch no.1 train no.119480  loss = 3.00996 avg_loss = 3.70380\n",
      "epoch no.1 train no.119490  loss = 6.10362 avg_loss = 3.73449\n",
      "epoch no.1 train no.119500  loss = 0.98882 avg_loss = 3.67616\n",
      "epoch no.1 train no.119510  loss = 1.67251 avg_loss = 3.64834\n",
      "epoch no.1 train no.119520  loss = 2.11567 avg_loss = 3.64735\n",
      "epoch no.1 train no.119530  loss = 5.87731 avg_loss = 3.71084\n",
      "epoch no.1 train no.119540  loss = 4.40997 avg_loss = 3.77502\n",
      "epoch no.1 train no.119550  loss = 4.26910 avg_loss = 3.72489\n",
      "epoch no.1 train no.119560  loss = 2.39304 avg_loss = 3.75424\n",
      "epoch no.1 train no.119570  loss = 4.44063 avg_loss = 3.72729\n",
      "epoch no.1 train no.119580  loss = 5.80137 avg_loss = 3.73534\n",
      "epoch no.1 train no.119590  loss = 3.92494 avg_loss = 3.79151\n",
      "epoch no.1 train no.119600  loss = 2.90004 avg_loss = 3.77247\n",
      "epoch no.1 train no.119610  loss = 2.93840 avg_loss = 3.81048\n",
      "epoch no.1 train no.119620  loss = 4.38715 avg_loss = 3.87430\n",
      "epoch no.1 train no.119630  loss = 2.44114 avg_loss = 3.78902\n",
      "epoch no.1 train no.119640  loss = 4.23352 avg_loss = 3.78112\n",
      "epoch no.1 train no.119650  loss = 2.54702 avg_loss = 3.79025\n",
      "epoch no.1 train no.119660  loss = 1.58423 avg_loss = 3.77337\n",
      "epoch no.1 train no.119670  loss = 4.34678 avg_loss = 3.75617\n",
      "epoch no.1 train no.119680  loss = 4.36124 avg_loss = 3.74273\n",
      "epoch no.1 train no.119690  loss = 3.99708 avg_loss = 3.74967\n",
      "epoch no.1 train no.119700  loss = 3.17500 avg_loss = 3.72737\n",
      "epoch no.1 train no.119710  loss = 1.75261 avg_loss = 3.69349\n",
      "epoch no.1 train no.119720  loss = 3.14597 avg_loss = 3.69567\n",
      "epoch no.1 train no.119730  loss = 3.67365 avg_loss = 3.67071\n",
      "epoch no.1 train no.119740  loss = 2.55596 avg_loss = 3.66739\n",
      "epoch no.1 train no.119750  loss = 1.98287 avg_loss = 3.64032\n",
      "epoch no.1 train no.119760  loss = 3.75192 avg_loss = 3.61317\n",
      "epoch no.1 train no.119770  loss = 2.37735 avg_loss = 3.65686\n",
      "epoch no.1 train no.119780  loss = 2.81631 avg_loss = 3.69389\n",
      "epoch no.1 train no.119790  loss = 2.54976 avg_loss = 3.69378\n",
      "epoch no.1 train no.119800  loss = 3.62933 avg_loss = 3.76637\n",
      "epoch no.1 train no.119810  loss = 4.20985 avg_loss = 3.76002\n",
      "epoch no.1 train no.119820  loss = 4.19190 avg_loss = 3.77172\n",
      "epoch no.1 train no.119830  loss = 2.18815 avg_loss = 3.75037\n",
      "epoch no.1 train no.119840  loss = 3.66324 avg_loss = 3.76566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.119850  loss = 4.42545 avg_loss = 3.79733\n",
      "epoch no.1 train no.119860  loss = 6.27054 avg_loss = 3.86109\n",
      "epoch no.1 train no.119870  loss = 2.17541 avg_loss = 3.82509\n",
      "epoch no.1 train no.119880  loss = 3.51046 avg_loss = 3.83575\n",
      "epoch no.1 train no.119890  loss = 1.62222 avg_loss = 3.77917\n",
      "epoch no.1 train no.119900  loss = 4.06354 avg_loss = 3.81156\n",
      "epoch no.1 train no.119910  loss = 3.60676 avg_loss = 3.83363\n",
      "epoch no.1 train no.119920  loss = 2.97323 avg_loss = 3.88175\n",
      "epoch no.1 train no.119930  loss = 3.91316 avg_loss = 3.82892\n",
      "epoch no.1 train no.119940  loss = 2.40446 avg_loss = 3.78837\n",
      "epoch no.1 train no.119950  loss = 3.17846 avg_loss = 3.72675\n",
      "epoch no.1 train no.119960  loss = 4.21318 avg_loss = 3.70688\n",
      "epoch no.1 train no.119970  loss = 3.18464 avg_loss = 3.69903\n",
      "epoch no.1 train no.119980  loss = 3.82879 avg_loss = 3.71846\n",
      "epoch no.1 train no.119990  loss = 3.74303 avg_loss = 3.72689\n",
      "epoch no.1 train no.120000  loss = 4.46745 avg_loss = 3.72684\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기좋은 노래</s>\n",
      "epoch no.1 train no.120010  loss = 4.36607 avg_loss = 3.73300\n",
      "epoch no.1 train no.120020  loss = 2.52349 avg_loss = 3.71370\n",
      "epoch no.1 train no.120030  loss = 2.31783 avg_loss = 3.72013\n",
      "epoch no.1 train no.120040  loss = 2.75430 avg_loss = 3.65451\n",
      "epoch no.1 train no.120050  loss = 4.23944 avg_loss = 3.65900\n",
      "epoch no.1 train no.120060  loss = 3.20585 avg_loss = 3.66603\n",
      "epoch no.1 train no.120070  loss = 2.89746 avg_loss = 3.65052\n",
      "epoch no.1 train no.120080  loss = 4.51261 avg_loss = 3.67931\n",
      "epoch no.1 train no.120090  loss = 3.32068 avg_loss = 3.60695\n",
      "epoch no.1 train no.120100  loss = 4.86181 avg_loss = 3.60715\n",
      "epoch no.1 train no.120110  loss = 4.02934 avg_loss = 3.63890\n",
      "epoch no.1 train no.120120  loss = 2.08193 avg_loss = 3.64738\n",
      "epoch no.1 train no.120130  loss = 1.90278 avg_loss = 3.59478\n",
      "epoch no.1 train no.120140  loss = 2.75582 avg_loss = 3.61406\n",
      "epoch no.1 train no.120150  loss = 3.64971 avg_loss = 3.60468\n",
      "epoch no.1 train no.120160  loss = 2.48203 avg_loss = 3.64116\n",
      "epoch no.1 train no.120170  loss = 5.20938 avg_loss = 3.74730\n",
      "epoch no.1 train no.120180  loss = 3.86586 avg_loss = 3.74076\n",
      "epoch no.1 train no.120190  loss = 2.95103 avg_loss = 3.77171\n",
      "epoch no.1 train no.120200  loss = 3.35715 avg_loss = 3.76893\n",
      "epoch no.1 train no.120210  loss = 3.08822 avg_loss = 3.73134\n",
      "epoch no.1 train no.120220  loss = 3.40269 avg_loss = 3.70587\n",
      "epoch no.1 train no.120230  loss = 3.00605 avg_loss = 3.70329\n",
      "epoch no.1 train no.120240  loss = 3.80672 avg_loss = 3.68331\n",
      "epoch no.1 train no.120250  loss = 4.41298 avg_loss = 3.69531\n",
      "epoch no.1 train no.120260  loss = 3.54708 avg_loss = 3.69206\n",
      "epoch no.1 train no.120270  loss = 1.79422 avg_loss = 3.68193\n",
      "epoch no.1 train no.120280  loss = 4.02366 avg_loss = 3.65221\n",
      "epoch no.1 train no.120290  loss = 5.55631 avg_loss = 3.66387\n",
      "epoch no.1 train no.120300  loss = 3.15588 avg_loss = 3.63477\n",
      "epoch no.1 train no.120310  loss = 3.45175 avg_loss = 3.68509\n",
      "epoch no.1 train no.120320  loss = 2.59210 avg_loss = 3.74747\n",
      "epoch no.1 train no.120330  loss = 5.53233 avg_loss = 3.80305\n",
      "epoch no.1 train no.120340  loss = 5.55883 avg_loss = 3.78001\n",
      "epoch no.1 train no.120350  loss = 4.03368 avg_loss = 3.77506\n",
      "epoch no.1 train no.120360  loss = 2.22009 avg_loss = 3.73366\n",
      "epoch no.1 train no.120370  loss = 3.38196 avg_loss = 3.74360\n",
      "epoch no.1 train no.120380  loss = 5.88348 avg_loss = 3.74335\n",
      "epoch no.1 train no.120390  loss = 3.40733 avg_loss = 3.75611\n",
      "epoch no.1 train no.120400  loss = 3.55702 avg_loss = 3.75227\n",
      "epoch no.1 train no.120410  loss = 3.00023 avg_loss = 3.77390\n",
      "epoch no.1 train no.120420  loss = 4.12596 avg_loss = 3.73870\n",
      "epoch no.1 train no.120430  loss = 3.48787 avg_loss = 3.75060\n",
      "epoch no.1 train no.120440  loss = 4.01079 avg_loss = 3.75347\n",
      "epoch no.1 train no.120450  loss = 2.61833 avg_loss = 3.75922\n",
      "epoch no.1 train no.120460  loss = 2.47014 avg_loss = 3.74084\n",
      "epoch no.1 train no.120470  loss = 3.17498 avg_loss = 3.74159\n",
      "epoch no.1 train no.120480  loss = 2.59896 avg_loss = 3.67312\n",
      "epoch no.1 train no.120490  loss = 5.97626 avg_loss = 3.67394\n",
      "epoch no.1 train no.120500  loss = 4.34566 avg_loss = 3.71480\n",
      "epoch no.1 train no.120510  loss = 2.35562 avg_loss = 3.73539\n",
      "epoch no.1 train no.120520  loss = 3.02258 avg_loss = 3.72971\n",
      "epoch no.1 train no.120530  loss = 4.44334 avg_loss = 3.75719\n",
      "epoch no.1 train no.120540  loss = 5.13562 avg_loss = 3.73953\n",
      "epoch no.1 train no.120550  loss = 1.84581 avg_loss = 3.71535\n",
      "epoch no.1 train no.120560  loss = 2.85199 avg_loss = 3.64527\n",
      "epoch no.1 train no.120570  loss = 2.01194 avg_loss = 3.64286\n",
      "epoch no.1 train no.120580  loss = 2.86698 avg_loss = 3.71306\n",
      "epoch no.1 train no.120590  loss = 4.70117 avg_loss = 3.66720\n",
      "epoch no.1 train no.120600  loss = 2.35059 avg_loss = 3.64260\n",
      "epoch no.1 train no.120610  loss = 3.08145 avg_loss = 3.69064\n",
      "epoch no.1 train no.120620  loss = 4.38551 avg_loss = 3.76748\n",
      "epoch no.1 train no.120630  loss = 5.56468 avg_loss = 3.73687\n",
      "epoch no.1 train no.120640  loss = 2.66098 avg_loss = 3.73523\n",
      "epoch no.1 train no.120650  loss = 1.69368 avg_loss = 3.65922\n",
      "epoch no.1 train no.120660  loss = 4.96714 avg_loss = 3.66941\n",
      "epoch no.1 train no.120670  loss = 3.18644 avg_loss = 3.70648\n",
      "epoch no.1 train no.120680  loss = 3.24247 avg_loss = 3.70260\n",
      "epoch no.1 train no.120690  loss = 3.28023 avg_loss = 3.66179\n",
      "epoch no.1 train no.120700  loss = 3.93363 avg_loss = 3.68232\n",
      "epoch no.1 train no.120710  loss = 6.09178 avg_loss = 3.65187\n",
      "epoch no.1 train no.120720  loss = 4.01037 avg_loss = 3.64621\n",
      "epoch no.1 train no.120730  loss = 2.32041 avg_loss = 3.61427\n",
      "epoch no.1 train no.120740  loss = 2.75099 avg_loss = 3.65631\n",
      "epoch no.1 train no.120750  loss = 1.99551 avg_loss = 3.61583\n",
      "epoch no.1 train no.120760  loss = 4.25210 avg_loss = 3.57585\n",
      "epoch no.1 train no.120770  loss = 3.62774 avg_loss = 3.52905\n",
      "epoch no.1 train no.120780  loss = 2.82149 avg_loss = 3.58625\n",
      "epoch no.1 train no.120790  loss = 5.63952 avg_loss = 3.64219\n",
      "epoch no.1 train no.120800  loss = 4.83053 avg_loss = 3.59131\n",
      "epoch no.1 train no.120810  loss = 2.25832 avg_loss = 3.64711\n",
      "epoch no.1 train no.120820  loss = 2.91458 avg_loss = 3.65216\n",
      "epoch no.1 train no.120830  loss = 4.68113 avg_loss = 3.66755\n",
      "epoch no.1 train no.120840  loss = 5.82299 avg_loss = 3.70860\n",
      "epoch no.1 train no.120850  loss = 5.51927 avg_loss = 3.65040\n",
      "epoch no.1 train no.120860  loss = 3.15768 avg_loss = 3.61950\n",
      "epoch no.1 train no.120870  loss = 3.24666 avg_loss = 3.67967\n",
      "epoch no.1 train no.120880  loss = 2.40087 avg_loss = 3.72299\n",
      "epoch no.1 train no.120890  loss = 3.81930 avg_loss = 3.77305\n",
      "epoch no.1 train no.120900  loss = 3.04680 avg_loss = 3.76903\n",
      "epoch no.1 train no.120910  loss = 4.97470 avg_loss = 3.76479\n",
      "epoch no.1 train no.120920  loss = 3.23690 avg_loss = 3.74342\n",
      "epoch no.1 train no.120930  loss = 2.76780 avg_loss = 3.75321\n",
      "epoch no.1 train no.120940  loss = 4.43581 avg_loss = 3.74281\n",
      "epoch no.1 train no.120950  loss = 2.44380 avg_loss = 3.75538\n",
      "epoch no.1 train no.120960  loss = 3.46682 avg_loss = 3.71484\n",
      "epoch no.1 train no.120970  loss = 2.22845 avg_loss = 3.62949\n",
      "epoch no.1 train no.120980  loss = 3.73877 avg_loss = 3.66323\n",
      "epoch no.1 train no.120990  loss = 2.17767 avg_loss = 3.60709\n",
      "epoch no.1 train no.121000  loss = 2.47142 avg_loss = 3.58177\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '음악', '성', '</s>']\n",
      "여름밤의 인디감성</s>\n",
      "epoch no.1 train no.121010  loss = 3.12890 avg_loss = 3.53600\n",
      "epoch no.1 train no.121020  loss = 3.52320 avg_loss = 3.57537\n",
      "epoch no.1 train no.121030  loss = 2.63663 avg_loss = 3.61471\n",
      "epoch no.1 train no.121040  loss = 5.87934 avg_loss = 3.61335\n",
      "epoch no.1 train no.121050  loss = 4.01269 avg_loss = 3.59774\n",
      "epoch no.1 train no.121060  loss = 2.57214 avg_loss = 3.64261\n",
      "epoch no.1 train no.121070  loss = 5.89626 avg_loss = 3.66493\n",
      "epoch no.1 train no.121080  loss = 3.13250 avg_loss = 3.63843\n",
      "epoch no.1 train no.121090  loss = 1.90535 avg_loss = 3.65709\n",
      "epoch no.1 train no.121100  loss = 4.72954 avg_loss = 3.66683\n",
      "epoch no.1 train no.121110  loss = 6.01526 avg_loss = 3.64899\n",
      "epoch no.1 train no.121120  loss = 2.78842 avg_loss = 3.64225\n",
      "epoch no.1 train no.121130  loss = 2.18312 avg_loss = 3.64996\n",
      "epoch no.1 train no.121140  loss = 4.80363 avg_loss = 3.59818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.121150  loss = 4.89118 avg_loss = 3.64627\n",
      "epoch no.1 train no.121160  loss = 4.19814 avg_loss = 3.64948\n",
      "epoch no.1 train no.121170  loss = 3.68880 avg_loss = 3.67141\n",
      "epoch no.1 train no.121180  loss = 4.90507 avg_loss = 3.70100\n",
      "epoch no.1 train no.121190  loss = 5.23135 avg_loss = 3.71378\n",
      "epoch no.1 train no.121200  loss = 2.47965 avg_loss = 3.71402\n",
      "epoch no.1 train no.121210  loss = 2.30914 avg_loss = 3.69323\n",
      "epoch no.1 train no.121220  loss = 3.46584 avg_loss = 3.75276\n",
      "epoch no.1 train no.121230  loss = 2.97732 avg_loss = 3.74617\n",
      "epoch no.1 train no.121240  loss = 4.12372 avg_loss = 3.73775\n",
      "epoch no.1 train no.121250  loss = 4.48152 avg_loss = 3.74615\n",
      "epoch no.1 train no.121260  loss = 2.78953 avg_loss = 3.75180\n",
      "epoch no.1 train no.121270  loss = 3.75514 avg_loss = 3.70761\n",
      "epoch no.1 train no.121280  loss = 3.35387 avg_loss = 3.70118\n",
      "epoch no.1 train no.121290  loss = 3.51886 avg_loss = 3.70765\n",
      "epoch no.1 train no.121300  loss = 3.18865 avg_loss = 3.69742\n",
      "epoch no.1 train no.121310  loss = 2.76313 avg_loss = 3.70750\n",
      "epoch no.1 train no.121320  loss = 3.01582 avg_loss = 3.73552\n",
      "epoch no.1 train no.121330  loss = 6.44932 avg_loss = 3.73165\n",
      "epoch no.1 train no.121340  loss = 4.19748 avg_loss = 3.72933\n",
      "epoch no.1 train no.121350  loss = 4.07206 avg_loss = 3.73499\n",
      "epoch no.1 train no.121360  loss = 3.47414 avg_loss = 3.70320\n",
      "epoch no.1 train no.121370  loss = 3.77311 avg_loss = 3.71809\n",
      "epoch no.1 train no.121380  loss = 5.25611 avg_loss = 3.70915\n",
      "epoch no.1 train no.121390  loss = 5.01123 avg_loss = 3.70139\n",
      "epoch no.1 train no.121400  loss = 1.90755 avg_loss = 3.74431\n",
      "epoch no.1 train no.121410  loss = 3.41677 avg_loss = 3.71774\n",
      "epoch no.1 train no.121420  loss = 4.13856 avg_loss = 3.75475\n",
      "epoch no.1 train no.121430  loss = 3.36641 avg_loss = 3.70110\n",
      "epoch no.1 train no.121440  loss = 2.76506 avg_loss = 3.72015\n",
      "epoch no.1 train no.121450  loss = 4.17046 avg_loss = 3.70576\n",
      "epoch no.1 train no.121460  loss = 4.51669 avg_loss = 3.71137\n",
      "epoch no.1 train no.121470  loss = 2.63281 avg_loss = 3.66740\n",
      "epoch no.1 train no.121480  loss = 3.07083 avg_loss = 3.65232\n",
      "epoch no.1 train no.121490  loss = 4.81532 avg_loss = 3.62785\n",
      "epoch no.1 train no.121500  loss = 2.94753 avg_loss = 3.64099\n",
      "epoch no.1 train no.121510  loss = 1.98687 avg_loss = 3.61821\n",
      "epoch no.1 train no.121520  loss = 2.70432 avg_loss = 3.65460\n",
      "epoch no.1 train no.121530  loss = 3.02534 avg_loss = 3.60772\n",
      "epoch no.1 train no.121540  loss = 3.37368 avg_loss = 3.57305\n",
      "epoch no.1 train no.121550  loss = 3.56519 avg_loss = 3.58772\n",
      "epoch no.1 train no.121560  loss = 4.08288 avg_loss = 3.62001\n",
      "epoch no.1 train no.121570  loss = 5.76708 avg_loss = 3.63075\n",
      "epoch no.1 train no.121580  loss = 2.58838 avg_loss = 3.61118\n",
      "epoch no.1 train no.121590  loss = 2.16157 avg_loss = 3.59555\n",
      "epoch no.1 train no.121600  loss = 3.24332 avg_loss = 3.58805\n",
      "epoch no.1 train no.121610  loss = 3.38703 avg_loss = 3.61945\n",
      "epoch no.1 train no.121620  loss = 4.28280 avg_loss = 3.60615\n",
      "epoch no.1 train no.121630  loss = 3.04149 avg_loss = 3.57124\n",
      "epoch no.1 train no.121640  loss = 4.32219 avg_loss = 3.61016\n",
      "epoch no.1 train no.121650  loss = 1.96039 avg_loss = 3.59477\n",
      "epoch no.1 train no.121660  loss = 6.46344 avg_loss = 3.70176\n",
      "epoch no.1 train no.121670  loss = 1.88389 avg_loss = 3.68199\n",
      "epoch no.1 train no.121680  loss = 4.22347 avg_loss = 3.72251\n",
      "epoch no.1 train no.121690  loss = 5.80844 avg_loss = 3.78487\n",
      "epoch no.1 train no.121700  loss = 2.82676 avg_loss = 3.69257\n",
      "epoch no.1 train no.121710  loss = 2.76806 avg_loss = 3.68067\n",
      "epoch no.1 train no.121720  loss = 2.40111 avg_loss = 3.66915\n",
      "epoch no.1 train no.121730  loss = 2.70554 avg_loss = 3.65972\n",
      "epoch no.1 train no.121740  loss = 3.19574 avg_loss = 3.63883\n",
      "epoch no.1 train no.121750  loss = 3.08977 avg_loss = 3.59203\n",
      "epoch no.1 train no.121760  loss = 2.26668 avg_loss = 3.61969\n",
      "epoch no.1 train no.121770  loss = 3.11191 avg_loss = 3.56553\n",
      "epoch no.1 train no.121780  loss = 4.39956 avg_loss = 3.59964\n",
      "epoch no.1 train no.121790  loss = 3.40467 avg_loss = 3.65399\n",
      "epoch no.1 train no.121800  loss = 2.21506 avg_loss = 3.67488\n",
      "epoch no.1 train no.121810  loss = 4.45661 avg_loss = 3.65034\n",
      "epoch no.1 train no.121820  loss = 2.55638 avg_loss = 3.67494\n",
      "epoch no.1 train no.121830  loss = 4.00746 avg_loss = 3.68787\n",
      "epoch no.1 train no.121840  loss = 3.97828 avg_loss = 3.70433\n",
      "epoch no.1 train no.121850  loss = 2.78021 avg_loss = 3.70835\n",
      "epoch no.1 train no.121860  loss = 2.84733 avg_loss = 3.72761\n",
      "epoch no.1 train no.121870  loss = 3.93848 avg_loss = 3.68743\n",
      "epoch no.1 train no.121880  loss = 4.26614 avg_loss = 3.67160\n",
      "epoch no.1 train no.121890  loss = 4.29063 avg_loss = 3.70626\n",
      "epoch no.1 train no.121900  loss = 4.30657 avg_loss = 3.76663\n",
      "epoch no.1 train no.121910  loss = 4.47074 avg_loss = 3.71445\n",
      "epoch no.1 train no.121920  loss = 3.36643 avg_loss = 3.71264\n",
      "epoch no.1 train no.121930  loss = 4.02338 avg_loss = 3.73010\n",
      "epoch no.1 train no.121940  loss = 3.38632 avg_loss = 3.77204\n",
      "epoch no.1 train no.121950  loss = 2.86912 avg_loss = 3.76079\n",
      "epoch no.1 train no.121960  loss = 6.26818 avg_loss = 3.77099\n",
      "epoch no.1 train no.121970  loss = 2.80256 avg_loss = 3.72911\n",
      "epoch no.1 train no.121980  loss = 6.52657 avg_loss = 3.84156\n",
      "epoch no.1 train no.121990  loss = 4.40885 avg_loss = 3.85956\n",
      "epoch no.1 train no.122000  loss = 3.41125 avg_loss = 3.80318\n",
      "6\n",
      "to_tokens: ['▁비', '엔', '에', '전환', '아', '지는', '▁노래', '</s>']\n",
      "여름날 기분좋아지는 음악</s>\n",
      "epoch no.1 train no.122010  loss = 4.25833 avg_loss = 3.81395\n",
      "epoch no.1 train no.122020  loss = 3.31860 avg_loss = 3.81888\n",
      "epoch no.1 train no.122030  loss = 2.83489 avg_loss = 3.80379\n",
      "epoch no.1 train no.122040  loss = 3.68427 avg_loss = 3.77949\n",
      "epoch no.1 train no.122050  loss = 2.83114 avg_loss = 3.77890\n",
      "epoch no.1 train no.122060  loss = 3.53725 avg_loss = 3.75898\n",
      "epoch no.1 train no.122070  loss = 1.15552 avg_loss = 3.69718\n",
      "epoch no.1 train no.122080  loss = 3.54831 avg_loss = 3.65562\n",
      "epoch no.1 train no.122090  loss = 2.85655 avg_loss = 3.71402\n",
      "epoch no.1 train no.122100  loss = 4.25492 avg_loss = 3.70166\n",
      "epoch no.1 train no.122110  loss = 4.12179 avg_loss = 3.67770\n",
      "epoch no.1 train no.122120  loss = 2.45618 avg_loss = 3.63099\n",
      "epoch no.1 train no.122130  loss = 3.58802 avg_loss = 3.65010\n",
      "epoch no.1 train no.122140  loss = 2.70482 avg_loss = 3.61307\n",
      "epoch no.1 train no.122150  loss = 2.67547 avg_loss = 3.60446\n",
      "epoch no.1 train no.122160  loss = 2.99463 avg_loss = 3.57922\n",
      "epoch no.1 train no.122170  loss = 3.05921 avg_loss = 3.61946\n",
      "epoch no.1 train no.122180  loss = 2.35066 avg_loss = 3.58303\n",
      "epoch no.1 train no.122190  loss = 4.05296 avg_loss = 3.60280\n",
      "epoch no.1 train no.122200  loss = 2.80323 avg_loss = 3.60796\n",
      "epoch no.1 train no.122210  loss = 5.79350 avg_loss = 3.59002\n",
      "epoch no.1 train no.122220  loss = 4.41529 avg_loss = 3.63876\n",
      "epoch no.1 train no.122230  loss = 3.64541 avg_loss = 3.66684\n",
      "epoch no.1 train no.122240  loss = 2.56130 avg_loss = 3.65080\n",
      "epoch no.1 train no.122250  loss = 2.80682 avg_loss = 3.63239\n",
      "epoch no.1 train no.122260  loss = 4.11708 avg_loss = 3.61994\n",
      "epoch no.1 train no.122270  loss = 4.91760 avg_loss = 3.66244\n",
      "epoch no.1 train no.122280  loss = 2.56489 avg_loss = 3.62819\n",
      "epoch no.1 train no.122290  loss = 2.11868 avg_loss = 3.61029\n",
      "epoch no.1 train no.122300  loss = 2.93010 avg_loss = 3.57176\n",
      "epoch no.1 train no.122310  loss = 3.67658 avg_loss = 3.58134\n",
      "epoch no.1 train no.122320  loss = 5.04713 avg_loss = 3.54311\n",
      "epoch no.1 train no.122330  loss = 4.58388 avg_loss = 3.57354\n",
      "epoch no.1 train no.122340  loss = 2.95046 avg_loss = 3.53678\n",
      "epoch no.1 train no.122350  loss = 2.01545 avg_loss = 3.51542\n",
      "epoch no.1 train no.122360  loss = 2.41792 avg_loss = 3.46875\n",
      "epoch no.1 train no.122370  loss = 2.14161 avg_loss = 3.48421\n",
      "epoch no.1 train no.122380  loss = 4.39220 avg_loss = 3.46408\n",
      "epoch no.1 train no.122390  loss = 4.75089 avg_loss = 3.52391\n",
      "epoch no.1 train no.122400  loss = 3.55129 avg_loss = 3.50604\n",
      "epoch no.1 train no.122410  loss = 2.83901 avg_loss = 3.46790\n",
      "epoch no.1 train no.122420  loss = 2.85826 avg_loss = 3.49380\n",
      "epoch no.1 train no.122430  loss = 4.05864 avg_loss = 3.56900\n",
      "epoch no.1 train no.122440  loss = 4.37352 avg_loss = 3.63393\n",
      "epoch no.1 train no.122450  loss = 3.26699 avg_loss = 3.62192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.122460  loss = 3.42585 avg_loss = 3.64533\n",
      "epoch no.1 train no.122470  loss = 3.08048 avg_loss = 3.62264\n",
      "epoch no.1 train no.122480  loss = 3.63605 avg_loss = 3.62319\n",
      "epoch no.1 train no.122490  loss = 2.84732 avg_loss = 3.64435\n",
      "epoch no.1 train no.122500  loss = 6.45223 avg_loss = 3.62521\n",
      "epoch no.1 train no.122510  loss = 3.25059 avg_loss = 3.62414\n",
      "epoch no.1 train no.122520  loss = 3.51199 avg_loss = 3.58894\n",
      "epoch no.1 train no.122530  loss = 4.32554 avg_loss = 3.67228\n",
      "epoch no.1 train no.122540  loss = 3.31093 avg_loss = 3.69191\n",
      "epoch no.1 train no.122550  loss = 4.94155 avg_loss = 3.75744\n",
      "epoch no.1 train no.122560  loss = 2.70052 avg_loss = 3.77352\n",
      "epoch no.1 train no.122570  loss = 3.31474 avg_loss = 3.69525\n",
      "epoch no.1 train no.122580  loss = 2.92440 avg_loss = 3.65969\n",
      "epoch no.1 train no.122590  loss = 3.16033 avg_loss = 3.62268\n",
      "epoch no.1 train no.122600  loss = 4.44112 avg_loss = 3.64525\n",
      "epoch no.1 train no.122610  loss = 2.99277 avg_loss = 3.64378\n",
      "epoch no.1 train no.122620  loss = 4.05894 avg_loss = 3.68044\n",
      "epoch no.1 train no.122630  loss = 3.44958 avg_loss = 3.64942\n",
      "epoch no.1 train no.122640  loss = 5.61543 avg_loss = 3.61210\n",
      "epoch no.1 train no.122650  loss = 4.56650 avg_loss = 3.65887\n",
      "epoch no.1 train no.122660  loss = 4.44560 avg_loss = 3.65835\n",
      "epoch no.1 train no.122670  loss = 2.75123 avg_loss = 3.69327\n",
      "epoch no.1 train no.122680  loss = 3.95908 avg_loss = 3.73162\n",
      "epoch no.1 train no.122690  loss = 3.58885 avg_loss = 3.71818\n",
      "epoch no.1 train no.122700  loss = 5.20887 avg_loss = 3.73770\n",
      "epoch no.1 train no.122710  loss = 3.55736 avg_loss = 3.69337\n",
      "epoch no.1 train no.122720  loss = 5.33995 avg_loss = 3.65560\n",
      "epoch no.1 train no.122730  loss = 3.07106 avg_loss = 3.61090\n",
      "epoch no.1 train no.122740  loss = 4.50333 avg_loss = 3.62855\n",
      "epoch no.1 train no.122750  loss = 4.58611 avg_loss = 3.62814\n",
      "epoch no.1 train no.122760  loss = 4.33586 avg_loss = 3.66087\n",
      "epoch no.1 train no.122770  loss = 7.40334 avg_loss = 3.71728\n",
      "epoch no.1 train no.122780  loss = 4.36517 avg_loss = 3.71081\n",
      "epoch no.1 train no.122790  loss = 3.81653 avg_loss = 3.67079\n",
      "epoch no.1 train no.122800  loss = 4.78771 avg_loss = 3.62960\n",
      "epoch no.1 train no.122810  loss = 2.40514 avg_loss = 3.65239\n",
      "epoch no.1 train no.122820  loss = 6.02973 avg_loss = 3.67903\n",
      "epoch no.1 train no.122830  loss = 3.32053 avg_loss = 3.67965\n",
      "epoch no.1 train no.122840  loss = 4.74666 avg_loss = 3.67981\n",
      "epoch no.1 train no.122850  loss = 3.63434 avg_loss = 3.70272\n",
      "epoch no.1 train no.122860  loss = 4.45908 avg_loss = 3.64834\n",
      "epoch no.1 train no.122870  loss = 2.74411 avg_loss = 3.60554\n",
      "epoch no.1 train no.122880  loss = 2.49632 avg_loss = 3.61750\n",
      "epoch no.1 train no.122890  loss = 3.21057 avg_loss = 3.57776\n",
      "epoch no.1 train no.122900  loss = 2.78050 avg_loss = 3.55524\n",
      "epoch no.1 train no.122910  loss = 3.83848 avg_loss = 3.59993\n",
      "epoch no.1 train no.122920  loss = 3.94650 avg_loss = 3.58830\n",
      "epoch no.1 train no.122930  loss = 4.80763 avg_loss = 3.61714\n",
      "epoch no.1 train no.122940  loss = 5.56560 avg_loss = 3.60566\n",
      "epoch no.1 train no.122950  loss = 2.54060 avg_loss = 3.60848\n",
      "epoch no.1 train no.122960  loss = 2.49858 avg_loss = 3.59653\n",
      "epoch no.1 train no.122970  loss = 3.38153 avg_loss = 3.58915\n",
      "epoch no.1 train no.122980  loss = 3.61689 avg_loss = 3.60928\n",
      "epoch no.1 train no.122990  loss = 3.47028 avg_loss = 3.59919\n",
      "epoch no.1 train no.123000  loss = 2.96559 avg_loss = 3.60475\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓는', '▁노래', '▁음악', '</s>']\n",
      "여름밤을 수놓는 아름다운 음악</s>\n",
      "epoch no.1 train no.123010  loss = 7.11696 avg_loss = 3.65177\n",
      "epoch no.1 train no.123020  loss = 2.60825 avg_loss = 3.62891\n",
      "epoch no.1 train no.123030  loss = 4.52792 avg_loss = 3.63691\n",
      "epoch no.1 train no.123040  loss = 4.48775 avg_loss = 3.70080\n",
      "epoch no.1 train no.123050  loss = 3.07085 avg_loss = 3.73206\n",
      "epoch no.1 train no.123060  loss = 3.09105 avg_loss = 3.73897\n",
      "epoch no.1 train no.123070  loss = 2.27571 avg_loss = 3.71360\n",
      "epoch no.1 train no.123080  loss = 2.50715 avg_loss = 3.67445\n",
      "epoch no.1 train no.123090  loss = 4.37486 avg_loss = 3.71175\n",
      "epoch no.1 train no.123100  loss = 4.33252 avg_loss = 3.71848\n",
      "epoch no.1 train no.123110  loss = 4.47672 avg_loss = 3.77634\n",
      "epoch no.1 train no.123120  loss = 3.18123 avg_loss = 3.83298\n",
      "epoch no.1 train no.123130  loss = 3.78936 avg_loss = 3.78420\n",
      "epoch no.1 train no.123140  loss = 2.08195 avg_loss = 3.73692\n",
      "epoch no.1 train no.123150  loss = 4.52605 avg_loss = 3.75280\n",
      "epoch no.1 train no.123160  loss = 1.96258 avg_loss = 3.73750\n",
      "epoch no.1 train no.123170  loss = 4.86982 avg_loss = 3.77508\n",
      "epoch no.1 train no.123180  loss = 2.33957 avg_loss = 3.78211\n",
      "epoch no.1 train no.123190  loss = 3.21205 avg_loss = 3.77693\n",
      "epoch no.1 train no.123200  loss = 2.75751 avg_loss = 3.73418\n",
      "epoch no.1 train no.123210  loss = 1.84356 avg_loss = 3.71860\n",
      "epoch no.1 train no.123220  loss = 5.72582 avg_loss = 3.66990\n",
      "epoch no.1 train no.123230  loss = 2.27569 avg_loss = 3.66350\n",
      "epoch no.1 train no.123240  loss = 3.45901 avg_loss = 3.63570\n",
      "epoch no.1 train no.123250  loss = 2.65516 avg_loss = 3.62880\n",
      "epoch no.1 train no.123260  loss = 4.34956 avg_loss = 3.58977\n",
      "epoch no.1 train no.123270  loss = 2.10005 avg_loss = 3.54760\n",
      "epoch no.1 train no.123280  loss = 3.37214 avg_loss = 3.49530\n",
      "epoch no.1 train no.123290  loss = 8.75874 avg_loss = 3.52420\n",
      "epoch no.1 train no.123300  loss = 3.38340 avg_loss = 3.56576\n",
      "epoch no.1 train no.123310  loss = 2.37781 avg_loss = 3.56360\n",
      "epoch no.1 train no.123320  loss = 4.40318 avg_loss = 3.53527\n",
      "epoch no.1 train no.123330  loss = 2.95264 avg_loss = 3.50248\n",
      "epoch no.1 train no.123340  loss = 5.53002 avg_loss = 3.57193\n",
      "epoch no.1 train no.123350  loss = 3.36632 avg_loss = 3.55881\n",
      "epoch no.1 train no.123360  loss = 3.37454 avg_loss = 3.54449\n",
      "epoch no.1 train no.123370  loss = 4.29429 avg_loss = 3.53807\n",
      "epoch no.1 train no.123380  loss = 4.10078 avg_loss = 3.50903\n",
      "epoch no.1 train no.123390  loss = 2.04264 avg_loss = 3.48903\n",
      "epoch no.1 train no.123400  loss = 3.90271 avg_loss = 3.49684\n",
      "epoch no.1 train no.123410  loss = 3.15443 avg_loss = 3.49564\n",
      "epoch no.1 train no.123420  loss = 4.88667 avg_loss = 3.50425\n",
      "epoch no.1 train no.123430  loss = 3.23823 avg_loss = 3.59523\n",
      "epoch no.1 train no.123440  loss = 2.85908 avg_loss = 3.54621\n",
      "epoch no.1 train no.123450  loss = 4.30799 avg_loss = 3.56683\n",
      "epoch no.1 train no.123460  loss = 2.59602 avg_loss = 3.55857\n",
      "epoch no.1 train no.123470  loss = 4.04721 avg_loss = 3.54476\n",
      "epoch no.1 train no.123480  loss = 4.09726 avg_loss = 3.58242\n",
      "epoch no.1 train no.123490  loss = 3.27484 avg_loss = 3.56277\n",
      "epoch no.1 train no.123500  loss = 2.60061 avg_loss = 3.58124\n",
      "epoch no.1 train no.123510  loss = 3.49254 avg_loss = 3.63686\n",
      "epoch no.1 train no.123520  loss = 2.93623 avg_loss = 3.60104\n",
      "epoch no.1 train no.123530  loss = 2.53565 avg_loss = 3.61131\n",
      "epoch no.1 train no.123540  loss = 2.68297 avg_loss = 3.57327\n",
      "epoch no.1 train no.123550  loss = 4.56655 avg_loss = 3.60316\n",
      "epoch no.1 train no.123560  loss = 2.45651 avg_loss = 3.53569\n",
      "epoch no.1 train no.123570  loss = 5.84707 avg_loss = 3.55484\n",
      "epoch no.1 train no.123580  loss = 2.47581 avg_loss = 3.57397\n",
      "epoch no.1 train no.123590  loss = 2.72866 avg_loss = 3.55249\n",
      "epoch no.1 train no.123600  loss = 3.25995 avg_loss = 3.52546\n",
      "epoch no.1 train no.123610  loss = 3.67870 avg_loss = 3.54956\n",
      "epoch no.1 train no.123620  loss = 3.70742 avg_loss = 3.57088\n",
      "epoch no.1 train no.123630  loss = 3.14752 avg_loss = 3.55583\n",
      "epoch no.1 train no.123640  loss = 2.54282 avg_loss = 3.57067\n",
      "epoch no.1 train no.123650  loss = 3.01669 avg_loss = 3.62386\n",
      "epoch no.1 train no.123660  loss = 3.97166 avg_loss = 3.66186\n",
      "epoch no.1 train no.123670  loss = 2.67574 avg_loss = 3.65795\n",
      "epoch no.1 train no.123680  loss = 3.33652 avg_loss = 3.70660\n",
      "epoch no.1 train no.123690  loss = 4.46039 avg_loss = 3.67106\n",
      "epoch no.1 train no.123700  loss = 4.28825 avg_loss = 3.67589\n",
      "epoch no.1 train no.123710  loss = 3.77697 avg_loss = 3.69852\n",
      "epoch no.1 train no.123720  loss = 2.29867 avg_loss = 3.67204\n",
      "epoch no.1 train no.123730  loss = 3.43935 avg_loss = 3.68247\n",
      "epoch no.1 train no.123740  loss = 4.77044 avg_loss = 3.68790\n",
      "epoch no.1 train no.123750  loss = 3.94046 avg_loss = 3.72197\n",
      "epoch no.1 train no.123760  loss = 5.06268 avg_loss = 3.70127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.123770  loss = 2.39247 avg_loss = 3.71024\n",
      "epoch no.1 train no.123780  loss = 4.63154 avg_loss = 3.66169\n",
      "epoch no.1 train no.123790  loss = 5.37905 avg_loss = 3.70433\n",
      "epoch no.1 train no.123800  loss = 2.48593 avg_loss = 3.69223\n",
      "epoch no.1 train no.123810  loss = 3.88970 avg_loss = 3.69498\n",
      "epoch no.1 train no.123820  loss = 6.36629 avg_loss = 3.75045\n",
      "epoch no.1 train no.123830  loss = 2.67078 avg_loss = 3.72692\n",
      "epoch no.1 train no.123840  loss = 4.43486 avg_loss = 3.72941\n",
      "epoch no.1 train no.123850  loss = 3.11098 avg_loss = 3.70820\n",
      "epoch no.1 train no.123860  loss = 4.05019 avg_loss = 3.73484\n",
      "epoch no.1 train no.123870  loss = 2.98757 avg_loss = 3.76815\n",
      "epoch no.1 train no.123880  loss = 5.36298 avg_loss = 3.79374\n",
      "epoch no.1 train no.123890  loss = 3.05303 avg_loss = 3.73758\n",
      "epoch no.1 train no.123900  loss = 2.73628 avg_loss = 3.70714\n",
      "epoch no.1 train no.123910  loss = 2.51523 avg_loss = 3.68771\n",
      "epoch no.1 train no.123920  loss = 4.30280 avg_loss = 3.68320\n",
      "epoch no.1 train no.123930  loss = 2.38780 avg_loss = 3.60168\n",
      "epoch no.1 train no.123940  loss = 3.34488 avg_loss = 3.62703\n",
      "epoch no.1 train no.123950  loss = 3.87891 avg_loss = 3.65271\n",
      "epoch no.1 train no.123960  loss = 1.86663 avg_loss = 3.63604\n",
      "epoch no.1 train no.123970  loss = 3.19964 avg_loss = 3.63708\n",
      "epoch no.1 train no.123980  loss = 4.42005 avg_loss = 3.69805\n",
      "epoch no.1 train no.123990  loss = 6.05645 avg_loss = 3.75587\n",
      "epoch no.1 train no.124000  loss = 4.89382 avg_loss = 3.82708\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.1 train no.124010  loss = 3.28177 avg_loss = 3.84622\n",
      "epoch no.1 train no.124020  loss = 2.94836 avg_loss = 3.84473\n",
      "epoch no.1 train no.124030  loss = 3.15993 avg_loss = 3.82465\n",
      "epoch no.1 train no.124040  loss = 3.46742 avg_loss = 3.83217\n",
      "epoch no.1 train no.124050  loss = 4.04921 avg_loss = 3.83647\n",
      "epoch no.1 train no.124060  loss = 2.68609 avg_loss = 3.82521\n",
      "epoch no.1 train no.124070  loss = 3.32524 avg_loss = 3.87412\n",
      "epoch no.1 train no.124080  loss = 4.71201 avg_loss = 3.85176\n",
      "epoch no.1 train no.124090  loss = 5.48325 avg_loss = 3.82447\n",
      "epoch no.1 train no.124100  loss = 4.38407 avg_loss = 3.82769\n",
      "epoch no.1 train no.124110  loss = 5.14002 avg_loss = 3.81972\n",
      "epoch no.1 train no.124120  loss = 3.44574 avg_loss = 3.78278\n",
      "epoch no.1 train no.124130  loss = 4.19523 avg_loss = 3.82452\n",
      "epoch no.1 train no.124140  loss = 4.09641 avg_loss = 3.84109\n",
      "epoch no.1 train no.124150  loss = 3.39203 avg_loss = 3.82556\n",
      "epoch no.1 train no.124160  loss = 3.31892 avg_loss = 3.75538\n",
      "epoch no.1 train no.124170  loss = 4.12146 avg_loss = 3.73638\n",
      "epoch no.1 train no.124180  loss = 3.85546 avg_loss = 3.70769\n",
      "epoch no.1 train no.124190  loss = 2.86236 avg_loss = 3.66557\n",
      "epoch no.1 train no.124200  loss = 2.37678 avg_loss = 3.66065\n",
      "epoch no.1 train no.124210  loss = 4.50330 avg_loss = 3.67286\n",
      "epoch no.1 train no.124220  loss = 4.91169 avg_loss = 3.71172\n",
      "epoch no.1 train no.124230  loss = 4.81800 avg_loss = 3.71787\n",
      "epoch no.1 train no.124240  loss = 3.75569 avg_loss = 3.72596\n",
      "epoch no.1 train no.124250  loss = 2.36873 avg_loss = 3.73690\n",
      "epoch no.1 train no.124260  loss = 6.21705 avg_loss = 3.74213\n",
      "epoch no.1 train no.124270  loss = 4.65758 avg_loss = 3.74073\n",
      "epoch no.1 train no.124280  loss = 2.01998 avg_loss = 3.72191\n",
      "epoch no.1 train no.124290  loss = 3.76756 avg_loss = 3.74981\n",
      "epoch no.1 train no.124300  loss = 4.07210 avg_loss = 3.69309\n",
      "epoch no.1 train no.124310  loss = 4.62448 avg_loss = 3.70604\n",
      "epoch no.1 train no.124320  loss = 3.35666 avg_loss = 3.69705\n",
      "epoch no.1 train no.124330  loss = 4.96861 avg_loss = 3.75431\n",
      "epoch no.1 train no.124340  loss = 4.44034 avg_loss = 3.77487\n",
      "epoch no.1 train no.124350  loss = 4.20164 avg_loss = 3.78021\n",
      "epoch no.1 train no.124360  loss = 3.00141 avg_loss = 3.75170\n",
      "epoch no.1 train no.124370  loss = 3.38120 avg_loss = 3.66307\n",
      "epoch no.1 train no.124380  loss = 2.58603 avg_loss = 3.65652\n",
      "epoch no.1 train no.124390  loss = 3.05239 avg_loss = 3.65209\n",
      "epoch no.1 train no.124400  loss = 1.99898 avg_loss = 3.59372\n",
      "epoch no.1 train no.124410  loss = 4.21626 avg_loss = 3.61665\n",
      "epoch no.1 train no.124420  loss = 1.32272 avg_loss = 3.61799\n",
      "epoch no.1 train no.124430  loss = 3.50714 avg_loss = 3.64400\n",
      "epoch no.1 train no.124440  loss = 4.67972 avg_loss = 3.69874\n",
      "epoch no.1 train no.124450  loss = 2.41779 avg_loss = 3.71796\n",
      "epoch no.1 train no.124460  loss = 4.45847 avg_loss = 3.78844\n",
      "epoch no.1 train no.124470  loss = 5.96075 avg_loss = 3.77673\n",
      "epoch no.1 train no.124480  loss = 3.84067 avg_loss = 3.82955\n",
      "epoch no.1 train no.124490  loss = 2.52672 avg_loss = 3.86050\n",
      "epoch no.1 train no.124500  loss = 4.19708 avg_loss = 3.87575\n",
      "epoch no.1 train no.124510  loss = 2.78782 avg_loss = 3.86470\n",
      "epoch no.1 train no.124520  loss = 3.97174 avg_loss = 3.82394\n",
      "epoch no.1 train no.124530  loss = 4.00904 avg_loss = 3.78079\n",
      "epoch no.1 train no.124540  loss = 2.36684 avg_loss = 3.78723\n",
      "epoch no.1 train no.124550  loss = 4.89119 avg_loss = 3.73627\n",
      "epoch no.1 train no.124560  loss = 6.65469 avg_loss = 3.75769\n",
      "epoch no.1 train no.124570  loss = 4.23954 avg_loss = 3.75438\n",
      "epoch no.1 train no.124580  loss = 3.63683 avg_loss = 3.72373\n",
      "epoch no.1 train no.124590  loss = 2.37676 avg_loss = 3.72452\n",
      "epoch no.1 train no.124600  loss = 3.71010 avg_loss = 3.72417\n",
      "epoch no.1 train no.124610  loss = 4.06074 avg_loss = 3.69649\n",
      "epoch no.1 train no.124620  loss = 2.60633 avg_loss = 3.72251\n",
      "epoch no.1 train no.124630  loss = 6.45833 avg_loss = 3.85883\n",
      "epoch no.1 train no.124640  loss = 6.04360 avg_loss = 3.86365\n",
      "epoch no.1 train no.124650  loss = 1.90718 avg_loss = 3.83921\n",
      "epoch no.1 train no.124660  loss = 4.47387 avg_loss = 3.85877\n",
      "epoch no.1 train no.124670  loss = 4.24404 avg_loss = 3.83012\n",
      "epoch no.1 train no.124680  loss = 4.51847 avg_loss = 3.87168\n",
      "epoch no.1 train no.124690  loss = 3.25456 avg_loss = 3.90213\n",
      "epoch no.1 train no.124700  loss = 3.33055 avg_loss = 3.93380\n",
      "epoch no.1 train no.124710  loss = 4.06913 avg_loss = 3.88625\n",
      "epoch no.1 train no.124720  loss = 5.06661 avg_loss = 3.87736\n",
      "epoch no.1 train no.124730  loss = 4.95323 avg_loss = 3.83867\n",
      "epoch no.1 train no.124740  loss = 3.47983 avg_loss = 3.82475\n",
      "epoch no.1 train no.124750  loss = 5.48266 avg_loss = 3.79875\n",
      "epoch no.1 train no.124760  loss = 3.07972 avg_loss = 3.82300\n",
      "epoch no.1 train no.124770  loss = 5.39944 avg_loss = 3.83173\n",
      "epoch no.1 train no.124780  loss = 5.05141 avg_loss = 3.81624\n",
      "epoch no.1 train no.124790  loss = 3.26181 avg_loss = 3.86667\n",
      "epoch no.1 train no.124800  loss = 6.24264 avg_loss = 3.87801\n",
      "epoch no.1 train no.124810  loss = 2.43931 avg_loss = 3.80162\n",
      "epoch no.1 train no.124820  loss = 2.53727 avg_loss = 3.78277\n",
      "epoch no.1 train no.124830  loss = 5.54427 avg_loss = 3.75539\n",
      "epoch no.1 train no.124840  loss = 3.07278 avg_loss = 3.75007\n",
      "epoch no.1 train no.124850  loss = 3.99159 avg_loss = 3.78732\n",
      "epoch no.1 train no.124860  loss = 2.34000 avg_loss = 3.74420\n",
      "epoch no.1 train no.124870  loss = 3.32152 avg_loss = 3.69758\n",
      "epoch no.1 train no.124880  loss = 4.89358 avg_loss = 3.71588\n",
      "epoch no.1 train no.124890  loss = 5.57209 avg_loss = 3.70733\n",
      "epoch no.1 train no.124900  loss = 6.24866 avg_loss = 3.76215\n",
      "epoch no.1 train no.124910  loss = 2.85318 avg_loss = 3.75327\n",
      "epoch no.1 train no.124920  loss = 3.39135 avg_loss = 3.77772\n",
      "epoch no.1 train no.124930  loss = 6.24103 avg_loss = 3.83750\n",
      "epoch no.1 train no.124940  loss = 4.95528 avg_loss = 3.87487\n",
      "epoch no.1 train no.124950  loss = 4.08827 avg_loss = 3.84961\n",
      "epoch no.1 train no.124960  loss = 4.78774 avg_loss = 3.82416\n",
      "epoch no.1 train no.124970  loss = 5.61249 avg_loss = 3.82559\n",
      "epoch no.1 train no.124980  loss = 5.03412 avg_loss = 3.79885\n",
      "epoch no.1 train no.124990  loss = 2.96943 avg_loss = 3.77270\n",
      "epoch no.1 train no.125000  loss = 2.24914 avg_loss = 3.78001\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁청량', '적인', '드', '</s>']\n",
      "여름과 어울리는 감성 발라드</s>\n",
      "epoch no.1 train no.125010  loss = 3.15033 avg_loss = 3.73163\n",
      "epoch no.1 train no.125020  loss = 2.62669 avg_loss = 3.77993\n",
      "epoch no.1 train no.125030  loss = 4.35383 avg_loss = 3.78496\n",
      "epoch no.1 train no.125040  loss = 4.51163 avg_loss = 3.78930\n",
      "epoch no.1 train no.125050  loss = 2.17442 avg_loss = 3.68146\n",
      "epoch no.1 train no.125060  loss = 5.56563 avg_loss = 3.71024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.125070  loss = 1.91941 avg_loss = 3.67287\n",
      "epoch no.1 train no.125080  loss = 5.18314 avg_loss = 3.70652\n",
      "epoch no.1 train no.125090  loss = 2.44814 avg_loss = 3.71881\n",
      "epoch no.1 train no.125100  loss = 3.12308 avg_loss = 3.71068\n",
      "epoch no.1 train no.125110  loss = 2.71377 avg_loss = 3.63516\n",
      "epoch no.1 train no.125120  loss = 3.56307 avg_loss = 3.67600\n",
      "epoch no.1 train no.125130  loss = 4.64897 avg_loss = 3.70282\n",
      "epoch no.1 train no.125140  loss = 3.22674 avg_loss = 3.68726\n",
      "epoch no.1 train no.125150  loss = 3.81458 avg_loss = 3.72033\n",
      "epoch no.1 train no.125160  loss = 2.66810 avg_loss = 3.72336\n",
      "epoch no.1 train no.125170  loss = 3.86575 avg_loss = 3.74002\n",
      "epoch no.1 train no.125180  loss = 2.64384 avg_loss = 3.72309\n",
      "epoch no.1 train no.125190  loss = 2.66358 avg_loss = 3.68821\n",
      "epoch no.1 train no.125200  loss = 3.75631 avg_loss = 3.71057\n",
      "epoch no.1 train no.125210  loss = 3.83235 avg_loss = 3.70008\n",
      "epoch no.1 train no.125220  loss = 3.39779 avg_loss = 3.62770\n",
      "epoch no.1 train no.125230  loss = 2.50603 avg_loss = 3.59664\n",
      "epoch no.1 train no.125240  loss = 4.99425 avg_loss = 3.58379\n",
      "epoch no.1 train no.125250  loss = 2.03754 avg_loss = 3.52864\n",
      "epoch no.1 train no.125260  loss = 5.83496 avg_loss = 3.51085\n",
      "epoch no.1 train no.125270  loss = 3.08017 avg_loss = 3.51126\n",
      "epoch no.1 train no.125280  loss = 4.15440 avg_loss = 3.51386\n",
      "epoch no.1 train no.125290  loss = 2.40315 avg_loss = 3.46972\n",
      "epoch no.1 train no.125300  loss = 4.25820 avg_loss = 3.54451\n",
      "epoch no.1 train no.125310  loss = 3.80626 avg_loss = 3.53970\n",
      "epoch no.1 train no.125320  loss = 4.64245 avg_loss = 3.54895\n",
      "epoch no.1 train no.125330  loss = 2.25756 avg_loss = 3.51319\n",
      "epoch no.1 train no.125340  loss = 2.17881 avg_loss = 3.54892\n",
      "epoch no.1 train no.125350  loss = 4.65384 avg_loss = 3.52792\n",
      "epoch no.1 train no.125360  loss = 3.37487 avg_loss = 3.54175\n",
      "epoch no.1 train no.125370  loss = 5.09068 avg_loss = 3.55355\n",
      "epoch no.1 train no.125380  loss = 2.71639 avg_loss = 3.54840\n",
      "epoch no.1 train no.125390  loss = 3.89868 avg_loss = 3.55710\n",
      "epoch no.1 train no.125400  loss = 3.33488 avg_loss = 3.59127\n",
      "epoch no.1 train no.125410  loss = 3.27637 avg_loss = 3.61240\n",
      "epoch no.1 train no.125420  loss = 3.47398 avg_loss = 3.61909\n",
      "epoch no.1 train no.125430  loss = 5.75125 avg_loss = 3.66065\n",
      "epoch no.1 train no.125440  loss = 3.48072 avg_loss = 3.65230\n",
      "epoch no.1 train no.125450  loss = 4.18785 avg_loss = 3.59900\n",
      "epoch no.1 train no.125460  loss = 4.05854 avg_loss = 3.58492\n",
      "epoch no.1 train no.125470  loss = 5.58346 avg_loss = 3.58133\n",
      "epoch no.1 train no.125480  loss = 4.23982 avg_loss = 3.60176\n",
      "epoch no.1 train no.125490  loss = 2.64805 avg_loss = 3.58321\n",
      "epoch no.1 train no.125500  loss = 4.73671 avg_loss = 3.65432\n",
      "epoch no.1 train no.125510  loss = 4.85211 avg_loss = 3.66137\n",
      "epoch no.1 train no.125520  loss = 5.59498 avg_loss = 3.75749\n",
      "epoch no.1 train no.125530  loss = 3.20313 avg_loss = 3.74655\n",
      "epoch no.1 train no.125540  loss = 4.50388 avg_loss = 3.72218\n",
      "epoch no.1 train no.125550  loss = 4.25667 avg_loss = 3.70130\n",
      "epoch no.1 train no.125560  loss = 2.14736 avg_loss = 3.66845\n",
      "epoch no.1 train no.125570  loss = 5.37353 avg_loss = 3.69351\n",
      "epoch no.1 train no.125580  loss = 3.11157 avg_loss = 3.68389\n",
      "epoch no.1 train no.125590  loss = 2.19043 avg_loss = 3.64387\n",
      "epoch no.1 train no.125600  loss = 4.34959 avg_loss = 3.67202\n",
      "epoch no.1 train no.125610  loss = 5.74942 avg_loss = 3.67287\n",
      "epoch no.1 train no.125620  loss = 3.40854 avg_loss = 3.74052\n",
      "epoch no.1 train no.125630  loss = 2.16638 avg_loss = 3.72552\n",
      "epoch no.1 train no.125640  loss = 2.01734 avg_loss = 3.70235\n",
      "epoch no.1 train no.125650  loss = 3.15733 avg_loss = 3.70463\n",
      "epoch no.1 train no.125660  loss = 4.77802 avg_loss = 3.74223\n",
      "epoch no.1 train no.125670  loss = 4.71867 avg_loss = 3.80216\n",
      "epoch no.1 train no.125680  loss = 2.90170 avg_loss = 3.77256\n",
      "epoch no.1 train no.125690  loss = 2.79177 avg_loss = 3.75867\n",
      "epoch no.1 train no.125700  loss = 2.76593 avg_loss = 3.76434\n",
      "epoch no.1 train no.125710  loss = 3.46852 avg_loss = 3.73705\n",
      "epoch no.1 train no.125720  loss = 2.58497 avg_loss = 3.74515\n",
      "epoch no.1 train no.125730  loss = 5.06170 avg_loss = 3.77131\n",
      "epoch no.1 train no.125740  loss = 2.86590 avg_loss = 3.82873\n",
      "epoch no.1 train no.125750  loss = 2.41329 avg_loss = 3.78061\n",
      "epoch no.1 train no.125760  loss = 2.80845 avg_loss = 3.73440\n",
      "epoch no.1 train no.125770  loss = 3.34874 avg_loss = 3.75299\n",
      "epoch no.1 train no.125780  loss = 2.48922 avg_loss = 3.74753\n",
      "epoch no.1 train no.125790  loss = 4.14190 avg_loss = 3.72726\n",
      "epoch no.1 train no.125800  loss = 2.67692 avg_loss = 3.68970\n",
      "epoch no.1 train no.125810  loss = 5.62810 avg_loss = 3.69209\n",
      "epoch no.1 train no.125820  loss = 5.09054 avg_loss = 3.71036\n",
      "epoch no.1 train no.125830  loss = 3.31373 avg_loss = 3.64703\n",
      "epoch no.1 train no.125840  loss = 4.72238 avg_loss = 3.68475\n",
      "epoch no.1 train no.125850  loss = 4.82515 avg_loss = 3.61769\n",
      "epoch no.1 train no.125860  loss = 3.00282 avg_loss = 3.57473\n",
      "epoch no.1 train no.125870  loss = 4.41546 avg_loss = 3.61516\n",
      "epoch no.1 train no.125880  loss = 4.52118 avg_loss = 3.68020\n",
      "epoch no.1 train no.125890  loss = 3.93289 avg_loss = 3.62202\n",
      "epoch no.1 train no.125900  loss = 3.47191 avg_loss = 3.70323\n",
      "epoch no.1 train no.125910  loss = 4.48188 avg_loss = 3.69063\n",
      "epoch no.1 train no.125920  loss = 2.60104 avg_loss = 3.69495\n",
      "epoch no.1 train no.125930  loss = 4.29264 avg_loss = 3.68654\n",
      "epoch no.1 train no.125940  loss = 3.81605 avg_loss = 3.73961\n",
      "epoch no.1 train no.125950  loss = 5.58559 avg_loss = 3.74825\n",
      "epoch no.1 train no.125960  loss = 4.84240 avg_loss = 3.73863\n",
      "epoch no.1 train no.125970  loss = 2.58401 avg_loss = 3.69524\n",
      "epoch no.1 train no.125980  loss = 1.78088 avg_loss = 3.67504\n",
      "epoch no.1 train no.125990  loss = 2.82745 avg_loss = 3.63921\n",
      "epoch no.1 train no.126000  loss = 2.78606 avg_loss = 3.61707\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '보', '▁잔', '</s>']\n",
      "여름밤의 재즈 한 잔</s>\n",
      "epoch no.1 train no.126010  loss = 4.39244 avg_loss = 3.71196\n",
      "epoch no.1 train no.126020  loss = 3.17645 avg_loss = 3.68148\n",
      "epoch no.1 train no.126030  loss = 3.74200 avg_loss = 3.67985\n",
      "epoch no.1 train no.126040  loss = 2.54031 avg_loss = 3.68108\n",
      "epoch no.1 train no.126050  loss = 1.42210 avg_loss = 3.62349\n",
      "epoch no.1 train no.126060  loss = 3.46919 avg_loss = 3.59948\n",
      "epoch no.1 train no.126070  loss = 3.72551 avg_loss = 3.58968\n",
      "epoch no.1 train no.126080  loss = 5.06408 avg_loss = 3.59093\n",
      "epoch no.1 train no.126090  loss = 4.07715 avg_loss = 3.58601\n",
      "epoch no.1 train no.126100  loss = 3.94674 avg_loss = 3.58918\n",
      "epoch no.1 train no.126110  loss = 2.18778 avg_loss = 3.54387\n",
      "epoch no.1 train no.126120  loss = 2.71879 avg_loss = 3.58974\n",
      "epoch no.1 train no.126130  loss = 4.84603 avg_loss = 3.57997\n",
      "epoch no.1 train no.126140  loss = 2.55033 avg_loss = 3.59215\n",
      "epoch no.1 train no.126150  loss = 3.07576 avg_loss = 3.60819\n",
      "epoch no.1 train no.126160  loss = 3.51213 avg_loss = 3.65960\n",
      "epoch no.1 train no.126170  loss = 3.37804 avg_loss = 3.67745\n",
      "epoch no.1 train no.126180  loss = 4.59634 avg_loss = 3.75070\n",
      "epoch no.1 train no.126190  loss = 5.07289 avg_loss = 3.76734\n",
      "epoch no.1 train no.126200  loss = 3.97364 avg_loss = 3.73803\n",
      "epoch no.1 train no.126210  loss = 4.43063 avg_loss = 3.73160\n",
      "epoch no.1 train no.126220  loss = 4.23770 avg_loss = 3.74200\n",
      "epoch no.1 train no.126230  loss = 4.24590 avg_loss = 3.71838\n",
      "epoch no.1 train no.126240  loss = 1.83276 avg_loss = 3.71365\n",
      "epoch no.1 train no.126250  loss = 3.69794 avg_loss = 3.71151\n",
      "epoch no.1 train no.126260  loss = 4.06653 avg_loss = 3.69850\n",
      "epoch no.1 train no.126270  loss = 2.49110 avg_loss = 3.68177\n",
      "epoch no.1 train no.126280  loss = 3.23998 avg_loss = 3.65546\n",
      "epoch no.1 train no.126290  loss = 3.60986 avg_loss = 3.64357\n",
      "epoch no.1 train no.126300  loss = 4.78132 avg_loss = 3.64020\n",
      "epoch no.1 train no.126310  loss = 2.61490 avg_loss = 3.61899\n",
      "epoch no.1 train no.126320  loss = 3.02112 avg_loss = 3.61076\n",
      "epoch no.1 train no.126330  loss = 5.88227 avg_loss = 3.62669\n",
      "epoch no.1 train no.126340  loss = 1.97597 avg_loss = 3.61424\n",
      "epoch no.1 train no.126350  loss = 3.05824 avg_loss = 3.63477\n",
      "epoch no.1 train no.126360  loss = 3.10302 avg_loss = 3.65529\n",
      "epoch no.1 train no.126370  loss = 4.27073 avg_loss = 3.65683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.126380  loss = 2.92885 avg_loss = 3.65987\n",
      "epoch no.1 train no.126390  loss = 2.55290 avg_loss = 3.61429\n",
      "epoch no.1 train no.126400  loss = 2.73515 avg_loss = 3.61291\n",
      "epoch no.1 train no.126410  loss = 2.80147 avg_loss = 3.58531\n",
      "epoch no.1 train no.126420  loss = 4.92868 avg_loss = 3.60163\n",
      "epoch no.1 train no.126430  loss = 2.79225 avg_loss = 3.61546\n",
      "epoch no.1 train no.126440  loss = 4.45613 avg_loss = 3.63923\n",
      "epoch no.1 train no.126450  loss = 3.98312 avg_loss = 3.68796\n",
      "epoch no.1 train no.126460  loss = 5.24140 avg_loss = 3.66898\n",
      "epoch no.1 train no.126470  loss = 4.02069 avg_loss = 3.67312\n",
      "epoch no.1 train no.126480  loss = 1.72055 avg_loss = 3.65256\n",
      "epoch no.1 train no.126490  loss = 3.10743 avg_loss = 3.64050\n",
      "epoch no.1 train no.126500  loss = 3.17314 avg_loss = 3.59446\n",
      "epoch no.1 train no.126510  loss = 3.17230 avg_loss = 3.59092\n",
      "epoch no.1 train no.126520  loss = 3.91026 avg_loss = 3.65031\n",
      "epoch no.1 train no.126530  loss = 2.82032 avg_loss = 3.66556\n",
      "epoch no.1 train no.126540  loss = 2.71894 avg_loss = 3.65671\n",
      "epoch no.1 train no.126550  loss = 6.51097 avg_loss = 3.66364\n",
      "epoch no.1 train no.126560  loss = 4.20129 avg_loss = 3.66749\n",
      "epoch no.1 train no.126570  loss = 2.20108 avg_loss = 3.61775\n",
      "epoch no.1 train no.126580  loss = 3.81680 avg_loss = 3.65821\n",
      "epoch no.1 train no.126590  loss = 3.51770 avg_loss = 3.68545\n",
      "epoch no.1 train no.126600  loss = 6.45033 avg_loss = 3.71167\n",
      "epoch no.1 train no.126610  loss = 4.02476 avg_loss = 3.68063\n",
      "epoch no.1 train no.126620  loss = 2.11670 avg_loss = 3.70347\n",
      "epoch no.1 train no.126630  loss = 4.81937 avg_loss = 3.71237\n",
      "epoch no.1 train no.126640  loss = 2.58950 avg_loss = 3.68422\n",
      "epoch no.1 train no.126650  loss = 2.48861 avg_loss = 3.66513\n",
      "epoch no.1 train no.126660  loss = 3.12371 avg_loss = 3.67262\n",
      "epoch no.1 train no.126670  loss = 3.18069 avg_loss = 3.70548\n",
      "epoch no.1 train no.126680  loss = 4.90012 avg_loss = 3.71112\n",
      "epoch no.1 train no.126690  loss = 3.50536 avg_loss = 3.72538\n",
      "epoch no.1 train no.126700  loss = 2.61618 avg_loss = 3.73772\n",
      "epoch no.1 train no.126710  loss = 4.81036 avg_loss = 3.69196\n",
      "epoch no.1 train no.126720  loss = 3.98556 avg_loss = 3.72301\n",
      "epoch no.1 train no.126730  loss = 2.10604 avg_loss = 3.75011\n",
      "epoch no.1 train no.126740  loss = 5.05258 avg_loss = 3.76454\n",
      "epoch no.1 train no.126750  loss = 2.92197 avg_loss = 3.73675\n",
      "epoch no.1 train no.126760  loss = 3.14352 avg_loss = 3.76559\n",
      "epoch no.1 train no.126770  loss = 5.59965 avg_loss = 3.73661\n",
      "epoch no.1 train no.126780  loss = 2.89421 avg_loss = 3.74411\n",
      "epoch no.1 train no.126790  loss = 4.76591 avg_loss = 3.76305\n",
      "epoch no.1 train no.126800  loss = 3.25535 avg_loss = 3.82947\n",
      "epoch no.1 train no.126810  loss = 2.89553 avg_loss = 3.77245\n",
      "epoch no.1 train no.126820  loss = 5.61060 avg_loss = 3.78896\n",
      "epoch no.1 train no.126830  loss = 3.13756 avg_loss = 3.77303\n",
      "epoch no.1 train no.126840  loss = 5.25348 avg_loss = 3.77062\n",
      "epoch no.1 train no.126850  loss = 3.64080 avg_loss = 3.78421\n",
      "epoch no.1 train no.126860  loss = 4.27488 avg_loss = 3.75105\n",
      "epoch no.1 train no.126870  loss = 4.23865 avg_loss = 3.69039\n",
      "epoch no.1 train no.126880  loss = 3.56002 avg_loss = 3.66427\n",
      "epoch no.1 train no.126890  loss = 2.24539 avg_loss = 3.66527\n",
      "epoch no.1 train no.126900  loss = 3.94889 avg_loss = 3.72334\n",
      "epoch no.1 train no.126910  loss = 3.94817 avg_loss = 3.73677\n",
      "epoch no.1 train no.126920  loss = 4.27799 avg_loss = 3.68247\n",
      "epoch no.1 train no.126930  loss = 3.38697 avg_loss = 3.71929\n",
      "epoch no.1 train no.126940  loss = 3.06909 avg_loss = 3.68897\n",
      "epoch no.1 train no.126950  loss = 2.60201 avg_loss = 3.60923\n",
      "epoch no.1 train no.126960  loss = 2.66047 avg_loss = 3.67358\n",
      "epoch no.1 train no.126970  loss = 5.78416 avg_loss = 3.67707\n",
      "epoch no.1 train no.126980  loss = 3.46926 avg_loss = 3.63290\n",
      "epoch no.1 train no.126990  loss = 3.80265 avg_loss = 3.66934\n",
      "epoch no.1 train no.127000  loss = 4.84430 avg_loss = 3.72123\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '힙', '▁잔', '</s>']\n",
      "여름밤의 재즈 한 잔</s>\n",
      "epoch no.1 train no.127010  loss = 3.61802 avg_loss = 3.70158\n",
      "epoch no.1 train no.127020  loss = 5.46108 avg_loss = 3.69313\n",
      "epoch no.1 train no.127030  loss = 3.61545 avg_loss = 3.70711\n",
      "epoch no.1 train no.127040  loss = 3.75793 avg_loss = 3.71070\n",
      "epoch no.1 train no.127050  loss = 2.76666 avg_loss = 3.69019\n",
      "epoch no.1 train no.127060  loss = 2.36808 avg_loss = 3.69300\n",
      "epoch no.1 train no.127070  loss = 3.04961 avg_loss = 3.65791\n",
      "epoch no.1 train no.127080  loss = 2.36133 avg_loss = 3.59527\n",
      "epoch no.1 train no.127090  loss = 3.65876 avg_loss = 3.56733\n",
      "epoch no.1 train no.127100  loss = 4.05999 avg_loss = 3.60079\n",
      "epoch no.1 train no.127110  loss = 3.73884 avg_loss = 3.58345\n",
      "epoch no.1 train no.127120  loss = 5.57809 avg_loss = 3.64620\n",
      "epoch no.1 train no.127130  loss = 6.03498 avg_loss = 3.69965\n",
      "epoch no.1 train no.127140  loss = 3.79913 avg_loss = 3.75391\n",
      "epoch no.1 train no.127150  loss = 7.35131 avg_loss = 3.83448\n",
      "epoch no.1 train no.127160  loss = 2.10879 avg_loss = 3.76250\n",
      "epoch no.1 train no.127170  loss = 2.30439 avg_loss = 3.74149\n",
      "epoch no.1 train no.127180  loss = 4.89475 avg_loss = 3.79326\n",
      "epoch no.1 train no.127190  loss = 4.38501 avg_loss = 3.77601\n",
      "epoch no.1 train no.127200  loss = 3.54031 avg_loss = 3.70285\n",
      "epoch no.1 train no.127210  loss = 3.08882 avg_loss = 3.71918\n",
      "epoch no.1 train no.127220  loss = 4.91561 avg_loss = 3.72524\n",
      "epoch no.1 train no.127230  loss = 1.99269 avg_loss = 3.72548\n",
      "epoch no.1 train no.127240  loss = 2.63919 avg_loss = 3.71072\n",
      "epoch no.1 train no.127250  loss = 4.12731 avg_loss = 3.69395\n",
      "epoch no.1 train no.127260  loss = 1.38250 avg_loss = 3.68597\n",
      "epoch no.1 train no.127270  loss = 5.70326 avg_loss = 3.71380\n",
      "epoch no.1 train no.127280  loss = 3.86872 avg_loss = 3.67141\n",
      "epoch no.1 train no.127290  loss = 3.61300 avg_loss = 3.70824\n",
      "epoch no.1 train no.127300  loss = 5.84095 avg_loss = 3.72429\n",
      "epoch no.1 train no.127310  loss = 3.63674 avg_loss = 3.68957\n",
      "epoch no.1 train no.127320  loss = 2.69910 avg_loss = 3.71739\n",
      "epoch no.1 train no.127330  loss = 3.62832 avg_loss = 3.74188\n",
      "epoch no.1 train no.127340  loss = 1.29447 avg_loss = 3.72058\n",
      "epoch no.1 train no.127350  loss = 2.58098 avg_loss = 3.69047\n",
      "epoch no.1 train no.127360  loss = 2.39515 avg_loss = 3.65885\n",
      "epoch no.1 train no.127370  loss = 3.26480 avg_loss = 3.64177\n",
      "epoch no.1 train no.127380  loss = 1.54061 avg_loss = 3.64837\n",
      "epoch no.1 train no.127390  loss = 3.44625 avg_loss = 3.60145\n",
      "epoch no.1 train no.127400  loss = 4.78930 avg_loss = 3.63118\n",
      "epoch no.1 train no.127410  loss = 3.10105 avg_loss = 3.60860\n",
      "epoch no.1 train no.127420  loss = 5.80134 avg_loss = 3.67945\n",
      "epoch no.1 train no.127430  loss = 3.39197 avg_loss = 3.66792\n",
      "epoch no.1 train no.127440  loss = 2.26307 avg_loss = 3.62799\n",
      "epoch no.1 train no.127450  loss = 3.88861 avg_loss = 3.59700\n",
      "epoch no.1 train no.127460  loss = 5.20712 avg_loss = 3.62330\n",
      "epoch no.1 train no.127470  loss = 3.76262 avg_loss = 3.65693\n",
      "epoch no.1 train no.127480  loss = 3.90759 avg_loss = 3.66747\n",
      "epoch no.1 train no.127490  loss = 4.96293 avg_loss = 3.68247\n",
      "epoch no.1 train no.127500  loss = 2.39964 avg_loss = 3.72885\n",
      "epoch no.1 train no.127510  loss = 2.66890 avg_loss = 3.77851\n",
      "epoch no.1 train no.127520  loss = 4.78778 avg_loss = 3.76593\n",
      "epoch no.1 train no.127530  loss = 5.43710 avg_loss = 3.71337\n",
      "epoch no.1 train no.127540  loss = 4.79070 avg_loss = 3.69158\n",
      "epoch no.1 train no.127550  loss = 3.72546 avg_loss = 3.66687\n",
      "epoch no.1 train no.127560  loss = 3.61817 avg_loss = 3.69971\n",
      "epoch no.1 train no.127570  loss = 2.82311 avg_loss = 3.71666\n",
      "epoch no.1 train no.127580  loss = 5.43861 avg_loss = 3.72124\n",
      "epoch no.1 train no.127590  loss = 2.67505 avg_loss = 3.70192\n",
      "epoch no.1 train no.127600  loss = 2.09403 avg_loss = 3.64590\n",
      "epoch no.1 train no.127610  loss = 4.95773 avg_loss = 3.67732\n",
      "epoch no.1 train no.127620  loss = 3.83482 avg_loss = 3.64055\n",
      "epoch no.1 train no.127630  loss = 2.95369 avg_loss = 3.61432\n",
      "epoch no.1 train no.127640  loss = 3.58297 avg_loss = 3.64330\n",
      "epoch no.1 train no.127650  loss = 3.58264 avg_loss = 3.67739\n",
      "epoch no.1 train no.127660  loss = 1.96547 avg_loss = 3.64409\n",
      "epoch no.1 train no.127670  loss = 5.46401 avg_loss = 3.66529\n",
      "epoch no.1 train no.127680  loss = 2.81688 avg_loss = 3.73713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.127690  loss = 2.97642 avg_loss = 3.69750\n",
      "epoch no.1 train no.127700  loss = 3.73659 avg_loss = 3.67449\n",
      "epoch no.1 train no.127710  loss = 2.17604 avg_loss = 3.61460\n",
      "epoch no.1 train no.127720  loss = 5.25935 avg_loss = 3.67752\n",
      "epoch no.1 train no.127730  loss = 2.74401 avg_loss = 3.63359\n",
      "epoch no.1 train no.127740  loss = 3.07499 avg_loss = 3.60031\n",
      "epoch no.1 train no.127750  loss = 2.58453 avg_loss = 3.54963\n",
      "epoch no.1 train no.127760  loss = 4.78977 avg_loss = 3.61830\n",
      "epoch no.1 train no.127770  loss = 1.96652 avg_loss = 3.59305\n",
      "epoch no.1 train no.127780  loss = 3.32862 avg_loss = 3.56332\n",
      "epoch no.1 train no.127790  loss = 3.47635 avg_loss = 3.56536\n",
      "epoch no.1 train no.127800  loss = 4.50844 avg_loss = 3.59784\n",
      "epoch no.1 train no.127810  loss = 2.54181 avg_loss = 3.59753\n",
      "epoch no.1 train no.127820  loss = 3.43134 avg_loss = 3.57040\n",
      "epoch no.1 train no.127830  loss = 3.64019 avg_loss = 3.57478\n",
      "epoch no.1 train no.127840  loss = 3.94710 avg_loss = 3.61554\n",
      "epoch no.1 train no.127850  loss = 2.34045 avg_loss = 3.62086\n",
      "epoch no.1 train no.127860  loss = 3.28339 avg_loss = 3.64360\n",
      "epoch no.1 train no.127870  loss = 3.67977 avg_loss = 3.59771\n",
      "epoch no.1 train no.127880  loss = 3.75435 avg_loss = 3.63636\n",
      "epoch no.1 train no.127890  loss = 6.16809 avg_loss = 3.67717\n",
      "epoch no.1 train no.127900  loss = 3.57183 avg_loss = 3.74077\n",
      "epoch no.1 train no.127910  loss = 3.75589 avg_loss = 3.79407\n",
      "epoch no.1 train no.127920  loss = 2.66887 avg_loss = 3.73729\n",
      "epoch no.1 train no.127930  loss = 6.40349 avg_loss = 3.75793\n",
      "epoch no.1 train no.127940  loss = 4.25045 avg_loss = 3.75088\n",
      "epoch no.1 train no.127950  loss = 3.25572 avg_loss = 3.78940\n",
      "epoch no.1 train no.127960  loss = 2.33530 avg_loss = 3.75531\n",
      "epoch no.1 train no.127970  loss = 2.92633 avg_loss = 3.74342\n",
      "epoch no.1 train no.127980  loss = 5.42012 avg_loss = 3.76323\n",
      "epoch no.1 train no.127990  loss = 4.63355 avg_loss = 3.75617\n",
      "epoch no.1 train no.128000  loss = 4.23382 avg_loss = 3.75370\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓은', '▁감성', '적인', '</s>']\n",
      "여름밤을 수놓은  감성 음악</s>\n",
      "epoch no.1 train no.128010  loss = 2.86420 avg_loss = 3.78357\n",
      "epoch no.1 train no.128020  loss = 2.97791 avg_loss = 3.83898\n",
      "epoch no.1 train no.128030  loss = 3.51624 avg_loss = 3.83575\n",
      "epoch no.1 train no.128040  loss = 3.30324 avg_loss = 3.75892\n",
      "epoch no.1 train no.128050  loss = 2.71881 avg_loss = 3.70248\n",
      "epoch no.1 train no.128060  loss = 5.33251 avg_loss = 3.71792\n",
      "epoch no.1 train no.128070  loss = 2.69862 avg_loss = 3.69654\n",
      "epoch no.1 train no.128080  loss = 4.69226 avg_loss = 3.69792\n",
      "epoch no.1 train no.128090  loss = 3.78404 avg_loss = 3.70560\n",
      "epoch no.1 train no.128100  loss = 2.17412 avg_loss = 3.64961\n",
      "epoch no.1 train no.128110  loss = 2.17798 avg_loss = 3.61577\n",
      "epoch no.1 train no.128120  loss = 3.27182 avg_loss = 3.67488\n",
      "epoch no.1 train no.128130  loss = 4.27614 avg_loss = 3.66253\n",
      "epoch no.1 train no.128140  loss = 1.97217 avg_loss = 3.62155\n",
      "epoch no.1 train no.128150  loss = 3.13515 avg_loss = 3.61080\n",
      "epoch no.1 train no.128160  loss = 3.54754 avg_loss = 3.59189\n",
      "epoch no.1 train no.128170  loss = 3.33477 avg_loss = 3.57333\n",
      "epoch no.1 train no.128180  loss = 2.55520 avg_loss = 3.64557\n",
      "epoch no.1 train no.128190  loss = 4.94192 avg_loss = 3.66144\n",
      "epoch no.1 train no.128200  loss = 3.86726 avg_loss = 3.65158\n",
      "epoch no.1 train no.128210  loss = 3.14546 avg_loss = 3.64230\n",
      "epoch no.1 train no.128220  loss = 3.32739 avg_loss = 3.69143\n",
      "epoch no.1 train no.128230  loss = 3.54968 avg_loss = 3.72405\n",
      "epoch no.1 train no.128240  loss = 3.10756 avg_loss = 3.77134\n",
      "epoch no.1 train no.128250  loss = 4.26708 avg_loss = 3.71787\n",
      "epoch no.1 train no.128260  loss = 2.97085 avg_loss = 3.73866\n",
      "epoch no.1 train no.128270  loss = 2.37198 avg_loss = 3.69360\n",
      "epoch no.1 train no.128280  loss = 4.61629 avg_loss = 3.64705\n",
      "epoch no.1 train no.128290  loss = 3.72367 avg_loss = 3.69401\n",
      "epoch no.1 train no.128300  loss = 1.83591 avg_loss = 3.70910\n",
      "epoch no.1 train no.128310  loss = 3.76844 avg_loss = 3.73146\n",
      "epoch no.1 train no.128320  loss = 1.92700 avg_loss = 3.70519\n",
      "epoch no.1 train no.128330  loss = 2.89714 avg_loss = 3.71828\n",
      "epoch no.1 train no.128340  loss = 5.04694 avg_loss = 3.70528\n",
      "epoch no.1 train no.128350  loss = 4.19704 avg_loss = 3.66554\n",
      "epoch no.1 train no.128360  loss = 2.61175 avg_loss = 3.58935\n",
      "epoch no.1 train no.128370  loss = 6.19710 avg_loss = 3.60998\n",
      "epoch no.1 train no.128380  loss = 4.45376 avg_loss = 3.60175\n",
      "epoch no.1 train no.128390  loss = 2.35224 avg_loss = 3.61855\n",
      "epoch no.1 train no.128400  loss = 5.93387 avg_loss = 3.61485\n",
      "epoch no.1 train no.128410  loss = 4.92283 avg_loss = 3.60046\n",
      "epoch no.1 train no.128420  loss = 5.55061 avg_loss = 3.64174\n",
      "epoch no.1 train no.128430  loss = 3.96799 avg_loss = 3.61855\n",
      "epoch no.1 train no.128440  loss = 2.65095 avg_loss = 3.61335\n",
      "epoch no.1 train no.128450  loss = 3.60419 avg_loss = 3.67423\n",
      "epoch no.1 train no.128460  loss = 2.51623 avg_loss = 3.66500\n",
      "epoch no.1 train no.128470  loss = 3.12901 avg_loss = 3.66906\n",
      "epoch no.1 train no.128480  loss = 4.29441 avg_loss = 3.65813\n",
      "epoch no.1 train no.128490  loss = 1.00512 avg_loss = 3.64651\n",
      "epoch no.1 train no.128500  loss = 4.92019 avg_loss = 3.62579\n",
      "epoch no.1 train no.128510  loss = 3.21673 avg_loss = 3.57360\n",
      "epoch no.1 train no.128520  loss = 2.45181 avg_loss = 3.59608\n",
      "epoch no.1 train no.128530  loss = 2.33833 avg_loss = 3.60437\n",
      "epoch no.1 train no.128540  loss = 4.88589 avg_loss = 3.62760\n",
      "epoch no.1 train no.128550  loss = 6.58306 avg_loss = 3.64156\n",
      "epoch no.1 train no.128560  loss = 6.22027 avg_loss = 3.61614\n",
      "epoch no.1 train no.128570  loss = 3.97525 avg_loss = 3.61059\n",
      "epoch no.1 train no.128580  loss = 3.00144 avg_loss = 3.61314\n",
      "epoch no.1 train no.128590  loss = 2.57428 avg_loss = 3.63149\n",
      "epoch no.1 train no.128600  loss = 4.68001 avg_loss = 3.66776\n",
      "epoch no.1 train no.128610  loss = 5.53041 avg_loss = 3.73387\n",
      "epoch no.1 train no.128620  loss = 4.21362 avg_loss = 3.75403\n",
      "epoch no.1 train no.128630  loss = 3.48319 avg_loss = 3.80708\n",
      "epoch no.1 train no.128640  loss = 3.84165 avg_loss = 3.79061\n",
      "epoch no.1 train no.128650  loss = 2.42446 avg_loss = 3.77596\n",
      "epoch no.1 train no.128660  loss = 1.84002 avg_loss = 3.76163\n",
      "epoch no.1 train no.128670  loss = 2.62803 avg_loss = 3.70158\n",
      "epoch no.1 train no.128680  loss = 2.42361 avg_loss = 3.68517\n",
      "epoch no.1 train no.128690  loss = 2.12053 avg_loss = 3.69855\n",
      "epoch no.1 train no.128700  loss = 2.98509 avg_loss = 3.70983\n",
      "epoch no.1 train no.128710  loss = 2.77442 avg_loss = 3.72516\n",
      "epoch no.1 train no.128720  loss = 3.17121 avg_loss = 3.70959\n",
      "epoch no.1 train no.128730  loss = 3.73079 avg_loss = 3.73660\n",
      "epoch no.1 train no.128740  loss = 4.14839 avg_loss = 3.76944\n",
      "epoch no.1 train no.128750  loss = 2.75506 avg_loss = 3.73517\n",
      "epoch no.1 train no.128760  loss = 3.27498 avg_loss = 3.72925\n",
      "epoch no.1 train no.128770  loss = 5.96066 avg_loss = 3.73399\n",
      "epoch no.1 train no.128780  loss = 4.37285 avg_loss = 3.74959\n",
      "epoch no.1 train no.128790  loss = 3.81528 avg_loss = 3.71816\n",
      "epoch no.1 train no.128800  loss = 4.91669 avg_loss = 3.68665\n",
      "epoch no.1 train no.128810  loss = 3.59734 avg_loss = 3.72716\n",
      "epoch no.1 train no.128820  loss = 4.98074 avg_loss = 3.77034\n",
      "epoch no.1 train no.128830  loss = 2.13147 avg_loss = 3.73266\n",
      "epoch no.1 train no.128840  loss = 3.07630 avg_loss = 3.70429\n",
      "epoch no.1 train no.128850  loss = 4.56926 avg_loss = 3.71850\n",
      "epoch no.1 train no.128860  loss = 2.77739 avg_loss = 3.71141\n",
      "epoch no.1 train no.128870  loss = 3.13676 avg_loss = 3.69952\n",
      "epoch no.1 train no.128880  loss = 3.11319 avg_loss = 3.65198\n",
      "epoch no.1 train no.128890  loss = 2.64186 avg_loss = 3.60618\n",
      "epoch no.1 train no.128900  loss = 4.27974 avg_loss = 3.59614\n",
      "epoch no.1 train no.128910  loss = 5.16103 avg_loss = 3.59461\n",
      "epoch no.1 train no.128920  loss = 3.71878 avg_loss = 3.63416\n",
      "epoch no.1 train no.128930  loss = 4.31248 avg_loss = 3.63992\n",
      "epoch no.1 train no.128940  loss = 2.47449 avg_loss = 3.66900\n",
      "epoch no.1 train no.128950  loss = 4.26857 avg_loss = 3.67426\n",
      "epoch no.1 train no.128960  loss = 3.49657 avg_loss = 3.71178\n",
      "epoch no.1 train no.128970  loss = 3.18361 avg_loss = 3.71831\n",
      "epoch no.1 train no.128980  loss = 3.08328 avg_loss = 3.73669\n",
      "epoch no.1 train no.128990  loss = 4.60075 avg_loss = 3.70680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.129000  loss = 3.33104 avg_loss = 3.75118\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '힙', '</s>']\n",
      "여름밤에 듣는 감성곡</s>\n",
      "epoch no.1 train no.129010  loss = 3.84548 avg_loss = 3.73332\n",
      "epoch no.1 train no.129020  loss = 2.70663 avg_loss = 3.70228\n",
      "epoch no.1 train no.129030  loss = 2.62283 avg_loss = 3.65339\n",
      "epoch no.1 train no.129040  loss = 4.63298 avg_loss = 3.67188\n",
      "epoch no.1 train no.129050  loss = 3.65315 avg_loss = 3.63694\n",
      "epoch no.1 train no.129060  loss = 2.82757 avg_loss = 3.57551\n",
      "epoch no.1 train no.129070  loss = 1.78143 avg_loss = 3.56170\n",
      "epoch no.1 train no.129080  loss = 3.37566 avg_loss = 3.62236\n",
      "epoch no.1 train no.129090  loss = 3.04801 avg_loss = 3.61813\n",
      "epoch no.1 train no.129100  loss = 4.60575 avg_loss = 3.63248\n",
      "epoch no.1 train no.129110  loss = 3.25924 avg_loss = 3.66459\n",
      "epoch no.1 train no.129120  loss = 5.01810 avg_loss = 3.72025\n",
      "epoch no.1 train no.129130  loss = 3.41566 avg_loss = 3.73065\n",
      "epoch no.1 train no.129140  loss = 3.46490 avg_loss = 3.67013\n",
      "epoch no.1 train no.129150  loss = 5.68677 avg_loss = 3.65496\n",
      "epoch no.1 train no.129160  loss = 1.99307 avg_loss = 3.65874\n",
      "epoch no.1 train no.129170  loss = 3.16725 avg_loss = 3.63616\n",
      "epoch no.1 train no.129180  loss = 4.41927 avg_loss = 3.70489\n",
      "epoch no.1 train no.129190  loss = 5.14961 avg_loss = 3.75734\n",
      "epoch no.1 train no.129200  loss = 6.62571 avg_loss = 3.78199\n",
      "epoch no.1 train no.129210  loss = 2.77529 avg_loss = 3.77459\n",
      "epoch no.1 train no.129220  loss = 3.19075 avg_loss = 3.74861\n",
      "epoch no.1 train no.129230  loss = 1.89208 avg_loss = 3.69714\n",
      "epoch no.1 train no.129240  loss = 2.70650 avg_loss = 3.67396\n",
      "epoch no.1 train no.129250  loss = 3.17576 avg_loss = 3.67708\n",
      "epoch no.1 train no.129260  loss = 2.97126 avg_loss = 3.66338\n",
      "epoch no.1 train no.129270  loss = 4.29150 avg_loss = 3.65217\n",
      "epoch no.1 train no.129280  loss = 2.52073 avg_loss = 3.62905\n",
      "epoch no.1 train no.129290  loss = 4.47468 avg_loss = 3.65038\n",
      "epoch no.1 train no.129300  loss = 3.79371 avg_loss = 3.66562\n",
      "epoch no.1 train no.129310  loss = 2.82938 avg_loss = 3.61563\n",
      "epoch no.1 train no.129320  loss = 3.40796 avg_loss = 3.60028\n",
      "epoch no.1 train no.129330  loss = 3.95076 avg_loss = 3.60172\n",
      "epoch no.1 train no.129340  loss = 7.60281 avg_loss = 3.65707\n",
      "epoch no.1 train no.129350  loss = 2.11528 avg_loss = 3.65513\n",
      "epoch no.1 train no.129360  loss = 2.56856 avg_loss = 3.66171\n",
      "epoch no.1 train no.129370  loss = 3.36684 avg_loss = 3.61776\n",
      "epoch no.1 train no.129380  loss = 4.35054 avg_loss = 3.58088\n",
      "epoch no.1 train no.129390  loss = 3.67641 avg_loss = 3.53845\n",
      "epoch no.1 train no.129400  loss = 3.11616 avg_loss = 3.55694\n",
      "epoch no.1 train no.129410  loss = 3.27027 avg_loss = 3.55531\n",
      "epoch no.1 train no.129420  loss = 3.01167 avg_loss = 3.61290\n",
      "epoch no.1 train no.129430  loss = 3.87448 avg_loss = 3.59637\n",
      "epoch no.1 train no.129440  loss = 3.41226 avg_loss = 3.62157\n",
      "epoch no.1 train no.129450  loss = 3.27458 avg_loss = 3.57117\n",
      "epoch no.1 train no.129460  loss = 5.74101 avg_loss = 3.73711\n",
      "epoch no.1 train no.129470  loss = 3.82091 avg_loss = 3.77336\n",
      "epoch no.1 train no.129480  loss = 3.07109 avg_loss = 3.76681\n",
      "epoch no.1 train no.129490  loss = 1.99936 avg_loss = 3.75620\n",
      "epoch no.1 train no.129500  loss = 3.21894 avg_loss = 3.74251\n",
      "epoch no.1 train no.129510  loss = 4.67008 avg_loss = 3.80502\n",
      "epoch no.1 train no.129520  loss = 3.45007 avg_loss = 3.80574\n",
      "epoch no.1 train no.129530  loss = 4.51055 avg_loss = 3.76595\n",
      "epoch no.1 train no.129540  loss = 2.56670 avg_loss = 3.74053\n",
      "epoch no.1 train no.129550  loss = 4.93468 avg_loss = 3.72259\n",
      "epoch no.1 train no.129560  loss = 5.58711 avg_loss = 3.72581\n",
      "epoch no.1 train no.129570  loss = 2.73735 avg_loss = 3.75257\n",
      "epoch no.1 train no.129580  loss = 2.38725 avg_loss = 3.75283\n",
      "epoch no.1 train no.129590  loss = 1.90115 avg_loss = 3.79064\n",
      "epoch no.1 train no.129600  loss = 3.63098 avg_loss = 3.76016\n",
      "epoch no.1 train no.129610  loss = 3.57356 avg_loss = 3.73822\n",
      "epoch no.1 train no.129620  loss = 3.84978 avg_loss = 3.65791\n",
      "epoch no.1 train no.129630  loss = 2.36461 avg_loss = 3.62908\n",
      "epoch no.1 train no.129640  loss = 2.23119 avg_loss = 3.59689\n",
      "epoch no.1 train no.129650  loss = 2.34534 avg_loss = 3.58934\n",
      "epoch no.1 train no.129660  loss = 3.19377 avg_loss = 3.60760\n",
      "epoch no.1 train no.129670  loss = 3.02809 avg_loss = 3.63553\n",
      "epoch no.1 train no.129680  loss = 2.23082 avg_loss = 3.66405\n",
      "epoch no.1 train no.129690  loss = 5.16067 avg_loss = 3.66584\n",
      "epoch no.1 train no.129700  loss = 3.22207 avg_loss = 3.62387\n",
      "epoch no.1 train no.129710  loss = 2.84514 avg_loss = 3.63088\n",
      "epoch no.1 train no.129720  loss = 3.91586 avg_loss = 3.59314\n",
      "epoch no.1 train no.129730  loss = 4.19978 avg_loss = 3.57397\n",
      "epoch no.1 train no.129740  loss = 3.39294 avg_loss = 3.57115\n",
      "epoch no.1 train no.129750  loss = 2.36748 avg_loss = 3.52693\n",
      "epoch no.1 train no.129760  loss = 2.92333 avg_loss = 3.54229\n",
      "epoch no.1 train no.129770  loss = 5.23667 avg_loss = 3.52777\n",
      "epoch no.1 train no.129780  loss = 3.05114 avg_loss = 3.52330\n",
      "epoch no.1 train no.129790  loss = 2.71474 avg_loss = 3.48477\n",
      "epoch no.1 train no.129800  loss = 2.99864 avg_loss = 3.50189\n",
      "epoch no.1 train no.129810  loss = 3.56020 avg_loss = 3.47813\n",
      "epoch no.1 train no.129820  loss = 2.97888 avg_loss = 3.47909\n",
      "epoch no.1 train no.129830  loss = 3.83920 avg_loss = 3.46503\n",
      "epoch no.1 train no.129840  loss = 4.26581 avg_loss = 3.56809\n",
      "epoch no.1 train no.129850  loss = 2.38490 avg_loss = 3.56040\n",
      "epoch no.1 train no.129860  loss = 3.86810 avg_loss = 3.61268\n",
      "epoch no.1 train no.129870  loss = 3.79383 avg_loss = 3.63614\n",
      "epoch no.1 train no.129880  loss = 2.78845 avg_loss = 3.63287\n",
      "epoch no.1 train no.129890  loss = 3.46677 avg_loss = 3.61438\n",
      "epoch no.1 train no.129900  loss = 4.93849 avg_loss = 3.61954\n",
      "epoch no.1 train no.129910  loss = 3.77020 avg_loss = 3.62601\n",
      "epoch no.1 train no.129920  loss = 5.64530 avg_loss = 3.64224\n",
      "epoch no.1 train no.129930  loss = 2.00999 avg_loss = 3.64541\n",
      "epoch no.1 train no.129940  loss = 3.21326 avg_loss = 3.61474\n",
      "epoch no.1 train no.129950  loss = 3.51380 avg_loss = 3.63000\n",
      "epoch no.1 train no.129960  loss = 6.93449 avg_loss = 3.71374\n",
      "epoch no.1 train no.129970  loss = 3.73101 avg_loss = 3.75797\n",
      "epoch no.1 train no.129980  loss = 1.80647 avg_loss = 3.76242\n",
      "epoch no.1 train no.129990  loss = 4.34044 avg_loss = 3.75448\n",
      "epoch no.1 train no.130000  loss = 4.62516 avg_loss = 3.76249\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁좋은', '▁노래', '▁노래', 'op', '</s>']\n",
      "여름밤 듣기 좋은 시원한 pop</s>\n",
      "epoch no.1 train no.130010  loss = 4.83355 avg_loss = 3.76002\n",
      "epoch no.1 train no.130020  loss = 3.16600 avg_loss = 3.74634\n",
      "epoch no.1 train no.130030  loss = 6.92111 avg_loss = 3.76284\n",
      "epoch no.1 train no.130040  loss = 2.81341 avg_loss = 3.68462\n",
      "epoch no.1 train no.130050  loss = 2.96032 avg_loss = 3.70072\n",
      "epoch no.1 train no.130060  loss = 1.56340 avg_loss = 3.63304\n",
      "epoch no.1 train no.130070  loss = 2.80165 avg_loss = 3.63925\n",
      "epoch no.1 train no.130080  loss = 4.94926 avg_loss = 3.68785\n",
      "epoch no.1 train no.130090  loss = 4.74748 avg_loss = 3.72404\n",
      "epoch no.1 train no.130100  loss = 5.26348 avg_loss = 3.71071\n",
      "epoch no.1 train no.130110  loss = 3.11709 avg_loss = 3.67445\n",
      "epoch no.1 train no.130120  loss = 5.39030 avg_loss = 3.73699\n",
      "epoch no.1 train no.130130  loss = 2.27686 avg_loss = 3.63377\n",
      "epoch no.1 train no.130140  loss = 3.74267 avg_loss = 3.66072\n",
      "epoch no.1 train no.130150  loss = 2.95029 avg_loss = 3.65455\n",
      "epoch no.1 train no.130160  loss = 2.09776 avg_loss = 3.62932\n",
      "epoch no.1 train no.130170  loss = 2.48694 avg_loss = 3.66550\n",
      "epoch no.1 train no.130180  loss = 3.47311 avg_loss = 3.68219\n",
      "epoch no.1 train no.130190  loss = 4.49626 avg_loss = 3.72856\n",
      "epoch no.1 train no.130200  loss = 2.73204 avg_loss = 3.73094\n",
      "epoch no.1 train no.130210  loss = 5.82980 avg_loss = 3.70667\n",
      "epoch no.1 train no.130220  loss = 3.97710 avg_loss = 3.65240\n",
      "epoch no.1 train no.130230  loss = 2.83415 avg_loss = 3.61394\n",
      "epoch no.1 train no.130240  loss = 2.35805 avg_loss = 3.61477\n",
      "epoch no.1 train no.130250  loss = 4.82445 avg_loss = 3.59436\n",
      "epoch no.1 train no.130260  loss = 4.55537 avg_loss = 3.59772\n",
      "epoch no.1 train no.130270  loss = 4.64646 avg_loss = 3.56890\n",
      "epoch no.1 train no.130280  loss = 2.74469 avg_loss = 3.58053\n",
      "epoch no.1 train no.130290  loss = 3.36879 avg_loss = 3.58228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.130300  loss = 3.16070 avg_loss = 3.58613\n",
      "epoch no.1 train no.130310  loss = 1.08471 avg_loss = 3.54873\n",
      "epoch no.1 train no.130320  loss = 3.02025 avg_loss = 3.55358\n",
      "epoch no.1 train no.130330  loss = 4.38077 avg_loss = 3.52077\n",
      "epoch no.1 train no.130340  loss = 3.09698 avg_loss = 3.52566\n",
      "epoch no.1 train no.130350  loss = 6.10416 avg_loss = 3.59985\n",
      "epoch no.1 train no.130360  loss = 4.00512 avg_loss = 3.60927\n",
      "epoch no.1 train no.130370  loss = 4.44693 avg_loss = 3.61883\n",
      "epoch no.1 train no.130380  loss = 2.45434 avg_loss = 3.60931\n",
      "epoch no.1 train no.130390  loss = 3.94933 avg_loss = 3.67084\n",
      "epoch no.1 train no.130400  loss = 5.75961 avg_loss = 3.68425\n",
      "epoch no.1 train no.130410  loss = 2.96917 avg_loss = 3.64952\n",
      "epoch no.1 train no.130420  loss = 1.70495 avg_loss = 3.61915\n",
      "epoch no.1 train no.130430  loss = 6.82595 avg_loss = 3.63509\n",
      "epoch no.1 train no.130440  loss = 3.00089 avg_loss = 3.63421\n",
      "epoch no.1 train no.130450  loss = 2.77273 avg_loss = 3.59994\n",
      "epoch no.1 train no.130460  loss = 3.25200 avg_loss = 3.58307\n",
      "epoch no.1 train no.130470  loss = 3.25093 avg_loss = 3.58733\n",
      "epoch no.1 train no.130480  loss = 3.84021 avg_loss = 3.57380\n",
      "epoch no.1 train no.130490  loss = 4.62073 avg_loss = 3.68336\n",
      "epoch no.1 train no.130500  loss = 4.46499 avg_loss = 3.62986\n",
      "epoch no.1 train no.130510  loss = 2.68901 avg_loss = 3.62906\n",
      "epoch no.1 train no.130520  loss = 3.11760 avg_loss = 3.55600\n",
      "epoch no.1 train no.130530  loss = 3.97028 avg_loss = 3.57132\n",
      "epoch no.1 train no.130540  loss = 3.44162 avg_loss = 3.57124\n",
      "epoch no.1 train no.130550  loss = 4.95437 avg_loss = 3.54944\n",
      "epoch no.1 train no.130560  loss = 2.45339 avg_loss = 3.50825\n",
      "epoch no.1 train no.130570  loss = 3.48451 avg_loss = 3.62341\n",
      "epoch no.1 train no.130580  loss = 2.72394 avg_loss = 3.63631\n",
      "epoch no.1 train no.130590  loss = 4.41859 avg_loss = 3.66639\n",
      "epoch no.1 train no.130600  loss = 2.67061 avg_loss = 3.63197\n",
      "epoch no.1 train no.130610  loss = 2.89802 avg_loss = 3.64183\n",
      "epoch no.1 train no.130620  loss = 2.99685 avg_loss = 3.64974\n",
      "epoch no.1 train no.130630  loss = 2.56552 avg_loss = 3.59954\n",
      "epoch no.1 train no.130640  loss = 3.03031 avg_loss = 3.56439\n",
      "epoch no.1 train no.130650  loss = 2.93956 avg_loss = 3.53560\n",
      "epoch no.1 train no.130660  loss = 4.53984 avg_loss = 3.55381\n",
      "epoch no.1 train no.130670  loss = 3.14919 avg_loss = 3.54135\n",
      "epoch no.1 train no.130680  loss = 1.15335 avg_loss = 3.50273\n",
      "epoch no.1 train no.130690  loss = 4.41259 avg_loss = 3.50052\n",
      "epoch no.1 train no.130700  loss = 3.38371 avg_loss = 3.51223\n",
      "epoch no.1 train no.130710  loss = 3.58063 avg_loss = 3.50583\n",
      "epoch no.1 train no.130720  loss = 3.06615 avg_loss = 3.51414\n",
      "epoch no.1 train no.130730  loss = 3.93906 avg_loss = 3.55660\n",
      "epoch no.1 train no.130740  loss = 4.02173 avg_loss = 3.54393\n",
      "epoch no.1 train no.130750  loss = 1.81794 avg_loss = 3.49834\n",
      "epoch no.1 train no.130760  loss = 3.75366 avg_loss = 3.49961\n",
      "epoch no.1 train no.130770  loss = 4.41319 avg_loss = 3.56432\n",
      "epoch no.1 train no.130780  loss = 4.10516 avg_loss = 3.57179\n",
      "epoch no.1 train no.130790  loss = 6.75117 avg_loss = 3.65106\n",
      "epoch no.1 train no.130800  loss = 3.49419 avg_loss = 3.63970\n",
      "epoch no.1 train no.130810  loss = 4.70652 avg_loss = 3.65417\n",
      "epoch no.1 train no.130820  loss = 4.24877 avg_loss = 3.62109\n",
      "epoch no.1 train no.130830  loss = 3.12294 avg_loss = 3.59762\n",
      "epoch no.1 train no.130840  loss = 4.57942 avg_loss = 3.63059\n",
      "epoch no.1 train no.130850  loss = 3.75456 avg_loss = 3.62988\n",
      "epoch no.1 train no.130860  loss = 4.49644 avg_loss = 3.66035\n",
      "epoch no.1 train no.130870  loss = 3.65191 avg_loss = 3.68954\n",
      "epoch no.1 train no.130880  loss = 2.36411 avg_loss = 3.67653\n",
      "epoch no.1 train no.130890  loss = 3.94802 avg_loss = 3.64826\n",
      "epoch no.1 train no.130900  loss = 3.43651 avg_loss = 3.61361\n",
      "epoch no.1 train no.130910  loss = 3.17510 avg_loss = 3.57669\n",
      "epoch no.1 train no.130920  loss = 5.62938 avg_loss = 3.61605\n",
      "epoch no.1 train no.130930  loss = 3.16926 avg_loss = 3.62052\n",
      "epoch no.1 train no.130940  loss = 2.83756 avg_loss = 3.62024\n",
      "epoch no.1 train no.130950  loss = 5.22829 avg_loss = 3.61960\n",
      "epoch no.1 train no.130960  loss = 4.30463 avg_loss = 3.62173\n",
      "epoch no.1 train no.130970  loss = 4.40565 avg_loss = 3.63539\n",
      "epoch no.1 train no.130980  loss = 3.49734 avg_loss = 3.58608\n",
      "epoch no.1 train no.130990  loss = 4.35412 avg_loss = 3.57970\n",
      "epoch no.1 train no.131000  loss = 3.11041 avg_loss = 3.58890\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁책임질', '▁해줄', '▁감성', '적인', '드', '</s>']\n",
      "여름밤을 함께해줄 감성 발라드</s>\n",
      "epoch no.1 train no.131010  loss = 4.04534 avg_loss = 3.62310\n",
      "epoch no.1 train no.131020  loss = 3.36945 avg_loss = 3.60140\n",
      "epoch no.1 train no.131030  loss = 2.19833 avg_loss = 3.56906\n",
      "epoch no.1 train no.131040  loss = 3.55088 avg_loss = 3.54119\n",
      "epoch no.1 train no.131050  loss = 3.16984 avg_loss = 3.57148\n",
      "epoch no.1 train no.131060  loss = 2.94587 avg_loss = 3.51069\n",
      "epoch no.1 train no.131070  loss = 2.49554 avg_loss = 3.50747\n",
      "epoch no.1 train no.131080  loss = 1.96559 avg_loss = 3.50065\n",
      "epoch no.1 train no.131090  loss = 3.17293 avg_loss = 3.59342\n",
      "epoch no.1 train no.131100  loss = 3.98560 avg_loss = 3.51874\n",
      "epoch no.1 train no.131110  loss = 3.52260 avg_loss = 3.52696\n",
      "epoch no.1 train no.131120  loss = 3.07727 avg_loss = 3.54324\n",
      "epoch no.1 train no.131130  loss = 4.18197 avg_loss = 3.54410\n",
      "epoch no.1 train no.131140  loss = 1.74426 avg_loss = 3.53128\n",
      "epoch no.1 train no.131150  loss = 2.34054 avg_loss = 3.50084\n",
      "epoch no.1 train no.131160  loss = 2.75383 avg_loss = 3.48736\n",
      "epoch no.1 train no.131170  loss = 3.32398 avg_loss = 3.50502\n",
      "epoch no.1 train no.131180  loss = 5.13274 avg_loss = 3.54059\n",
      "epoch no.1 train no.131190  loss = 3.53024 avg_loss = 3.62054\n",
      "epoch no.1 train no.131200  loss = 2.43350 avg_loss = 3.61765\n",
      "epoch no.1 train no.131210  loss = 4.00859 avg_loss = 3.63535\n",
      "epoch no.1 train no.131220  loss = 2.84184 avg_loss = 3.59600\n",
      "epoch no.1 train no.131230  loss = 3.65995 avg_loss = 3.57071\n",
      "epoch no.1 train no.131240  loss = 3.89282 avg_loss = 3.55947\n",
      "epoch no.1 train no.131250  loss = 2.67059 avg_loss = 3.52049\n",
      "epoch no.1 train no.131260  loss = 4.50851 avg_loss = 3.57581\n",
      "epoch no.1 train no.131270  loss = 3.31488 avg_loss = 3.57491\n",
      "epoch no.1 train no.131280  loss = 2.33231 avg_loss = 3.65342\n",
      "epoch no.1 train no.131290  loss = 4.55929 avg_loss = 3.69949\n",
      "epoch no.1 train no.131300  loss = 5.76823 avg_loss = 3.78359\n",
      "epoch no.1 train no.131310  loss = 4.73592 avg_loss = 3.80628\n",
      "epoch no.1 train no.131320  loss = 3.49322 avg_loss = 3.77600\n",
      "epoch no.1 train no.131330  loss = 5.43395 avg_loss = 3.74293\n",
      "epoch no.1 train no.131340  loss = 5.37849 avg_loss = 3.76811\n",
      "epoch no.1 train no.131350  loss = 2.39581 avg_loss = 3.76543\n",
      "epoch no.1 train no.131360  loss = 3.82391 avg_loss = 3.75591\n",
      "epoch no.1 train no.131370  loss = 2.23593 avg_loss = 3.77683\n",
      "epoch no.1 train no.131380  loss = 3.28874 avg_loss = 3.79115\n",
      "epoch no.1 train no.131390  loss = 5.94131 avg_loss = 3.86601\n",
      "epoch no.1 train no.131400  loss = 4.47841 avg_loss = 3.88645\n",
      "epoch no.1 train no.131410  loss = 1.66308 avg_loss = 3.84867\n",
      "epoch no.1 train no.131420  loss = 4.63712 avg_loss = 3.83574\n",
      "epoch no.1 train no.131430  loss = 4.31194 avg_loss = 3.80777\n",
      "epoch no.1 train no.131440  loss = 6.02268 avg_loss = 3.82947\n",
      "epoch no.1 train no.131450  loss = 3.37708 avg_loss = 3.80609\n",
      "epoch no.1 train no.131460  loss = 2.73587 avg_loss = 3.76297\n",
      "epoch no.1 train no.131470  loss = 7.59558 avg_loss = 3.73932\n",
      "epoch no.1 train no.131480  loss = 2.56325 avg_loss = 3.72505\n",
      "epoch no.1 train no.131490  loss = 2.96771 avg_loss = 3.72885\n",
      "epoch no.1 train no.131500  loss = 3.24817 avg_loss = 3.68899\n",
      "epoch no.1 train no.131510  loss = 5.83373 avg_loss = 3.65940\n",
      "epoch no.1 train no.131520  loss = 6.42744 avg_loss = 3.69379\n",
      "epoch no.1 train no.131530  loss = 3.88543 avg_loss = 3.68224\n",
      "epoch no.1 train no.131540  loss = 3.37558 avg_loss = 3.71015\n",
      "epoch no.1 train no.131550  loss = 3.96071 avg_loss = 3.66789\n",
      "epoch no.1 train no.131560  loss = 2.96742 avg_loss = 3.67891\n",
      "epoch no.1 train no.131570  loss = 1.97690 avg_loss = 3.68761\n",
      "epoch no.1 train no.131580  loss = 1.50687 avg_loss = 3.60967\n",
      "epoch no.1 train no.131590  loss = 2.35029 avg_loss = 3.60179\n",
      "epoch no.1 train no.131600  loss = 2.54897 avg_loss = 3.58235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.131610  loss = 5.04028 avg_loss = 3.55404\n",
      "epoch no.1 train no.131620  loss = 4.30715 avg_loss = 3.54654\n",
      "epoch no.1 train no.131630  loss = 2.93092 avg_loss = 3.60688\n",
      "epoch no.1 train no.131640  loss = 3.54887 avg_loss = 3.68537\n",
      "epoch no.1 train no.131650  loss = 4.11058 avg_loss = 3.72818\n",
      "epoch no.1 train no.131660  loss = 2.61225 avg_loss = 3.74121\n",
      "epoch no.1 train no.131670  loss = 3.77625 avg_loss = 3.72131\n",
      "epoch no.1 train no.131680  loss = 2.84769 avg_loss = 3.70828\n",
      "epoch no.1 train no.131690  loss = 4.75803 avg_loss = 3.74178\n",
      "epoch no.1 train no.131700  loss = 5.12609 avg_loss = 3.76913\n",
      "epoch no.1 train no.131710  loss = 3.08115 avg_loss = 3.69197\n",
      "epoch no.1 train no.131720  loss = 4.21325 avg_loss = 3.73766\n",
      "epoch no.1 train no.131730  loss = 3.81858 avg_loss = 3.72775\n",
      "epoch no.1 train no.131740  loss = 2.46072 avg_loss = 3.71709\n",
      "epoch no.1 train no.131750  loss = 1.60096 avg_loss = 3.66273\n",
      "epoch no.1 train no.131760  loss = 5.90335 avg_loss = 3.77119\n",
      "epoch no.1 train no.131770  loss = 2.84328 avg_loss = 3.73084\n",
      "epoch no.1 train no.131780  loss = 3.52051 avg_loss = 3.71042\n",
      "epoch no.1 train no.131790  loss = 2.32717 avg_loss = 3.70964\n",
      "epoch no.1 train no.131800  loss = 6.51445 avg_loss = 3.76828\n",
      "epoch no.1 train no.131810  loss = 3.95400 avg_loss = 3.72992\n",
      "epoch no.1 train no.131820  loss = 5.61202 avg_loss = 3.72602\n",
      "epoch no.1 train no.131830  loss = 3.88888 avg_loss = 3.77977\n",
      "epoch no.1 train no.131840  loss = 4.51111 avg_loss = 3.73614\n",
      "epoch no.1 train no.131850  loss = 5.48485 avg_loss = 3.68099\n",
      "epoch no.1 train no.131860  loss = 5.06699 avg_loss = 3.71612\n",
      "epoch no.1 train no.131870  loss = 4.36895 avg_loss = 3.66545\n",
      "epoch no.1 train no.131880  loss = 2.78035 avg_loss = 3.67070\n",
      "epoch no.1 train no.131890  loss = 2.36680 avg_loss = 3.60319\n",
      "epoch no.1 train no.131900  loss = 3.41319 avg_loss = 3.66030\n",
      "epoch no.1 train no.131910  loss = 6.63612 avg_loss = 3.70476\n",
      "epoch no.1 train no.131920  loss = 2.28083 avg_loss = 3.67703\n",
      "epoch no.1 train no.131930  loss = 3.79203 avg_loss = 3.68151\n",
      "epoch no.1 train no.131940  loss = 6.09707 avg_loss = 3.68744\n",
      "epoch no.1 train no.131950  loss = 4.68979 avg_loss = 3.70697\n",
      "epoch no.1 train no.131960  loss = 1.88049 avg_loss = 3.65401\n",
      "epoch no.1 train no.131970  loss = 3.28383 avg_loss = 3.67344\n",
      "epoch no.1 train no.131980  loss = 3.62828 avg_loss = 3.61390\n",
      "epoch no.1 train no.131990  loss = 4.19098 avg_loss = 3.61001\n",
      "epoch no.1 train no.132000  loss = 4.68890 avg_loss = 3.63554\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓은', '▁감성', '</s>', '</s>']\n",
      "여름밤을 수놓을 노래들</s>\n",
      "epoch no.1 train no.132010  loss = 3.06477 avg_loss = 3.64432\n",
      "epoch no.1 train no.132020  loss = 2.72097 avg_loss = 3.69609\n",
      "epoch no.1 train no.132030  loss = 3.21417 avg_loss = 3.73158\n",
      "epoch no.1 train no.132040  loss = 5.62597 avg_loss = 3.67168\n",
      "epoch no.1 train no.132050  loss = 3.20045 avg_loss = 3.66045\n",
      "epoch no.1 train no.132060  loss = 4.42594 avg_loss = 3.68082\n",
      "epoch no.1 train no.132070  loss = 3.45500 avg_loss = 3.67670\n",
      "epoch no.1 train no.132080  loss = 3.39501 avg_loss = 3.65436\n",
      "epoch no.1 train no.132090  loss = 2.17033 avg_loss = 3.66798\n",
      "epoch no.1 train no.132100  loss = 2.16390 avg_loss = 3.62325\n",
      "epoch no.1 train no.132110  loss = 4.35225 avg_loss = 3.67347\n",
      "epoch no.1 train no.132120  loss = 5.26741 avg_loss = 3.69501\n",
      "epoch no.1 train no.132130  loss = 4.41076 avg_loss = 3.74668\n",
      "epoch no.1 train no.132140  loss = 3.83566 avg_loss = 3.78352\n",
      "epoch no.1 train no.132150  loss = 7.47259 avg_loss = 3.78838\n",
      "epoch no.1 train no.132160  loss = 3.03506 avg_loss = 3.74475\n",
      "epoch no.1 train no.132170  loss = 3.34146 avg_loss = 3.70561\n",
      "epoch no.1 train no.132180  loss = 4.22022 avg_loss = 3.74004\n",
      "epoch no.1 train no.132190  loss = 3.10399 avg_loss = 3.71585\n",
      "epoch no.1 train no.132200  loss = 3.41511 avg_loss = 3.71798\n",
      "epoch no.1 train no.132210  loss = 2.79316 avg_loss = 3.67180\n",
      "epoch no.1 train no.132220  loss = 1.60400 avg_loss = 3.64653\n",
      "epoch no.1 train no.132230  loss = 3.23069 avg_loss = 3.61447\n",
      "epoch no.1 train no.132240  loss = 3.98363 avg_loss = 3.58672\n",
      "epoch no.1 train no.132250  loss = 2.30858 avg_loss = 3.57173\n",
      "epoch no.1 train no.132260  loss = 4.98549 avg_loss = 3.64796\n",
      "epoch no.1 train no.132270  loss = 5.44413 avg_loss = 3.67927\n",
      "epoch no.1 train no.132280  loss = 3.19626 avg_loss = 3.70379\n",
      "epoch no.1 train no.132290  loss = 5.97919 avg_loss = 3.75303\n",
      "epoch no.1 train no.132300  loss = 2.64403 avg_loss = 3.67970\n",
      "epoch no.1 train no.132310  loss = 3.52192 avg_loss = 3.69517\n",
      "epoch no.1 train no.132320  loss = 4.32106 avg_loss = 3.66361\n",
      "epoch no.1 train no.132330  loss = 3.28608 avg_loss = 3.63701\n",
      "epoch no.1 train no.132340  loss = 2.81485 avg_loss = 3.62930\n",
      "epoch no.1 train no.132350  loss = 4.58788 avg_loss = 3.62376\n",
      "epoch no.1 train no.132360  loss = 2.72994 avg_loss = 3.58902\n",
      "epoch no.1 train no.132370  loss = 3.71936 avg_loss = 3.56900\n",
      "epoch no.1 train no.132380  loss = 4.02365 avg_loss = 3.55132\n",
      "epoch no.1 train no.132390  loss = 3.61402 avg_loss = 3.61492\n",
      "epoch no.1 train no.132400  loss = 2.98580 avg_loss = 3.62512\n",
      "epoch no.1 train no.132410  loss = 2.97389 avg_loss = 3.63555\n",
      "epoch no.1 train no.132420  loss = 3.76923 avg_loss = 3.67729\n",
      "epoch no.1 train no.132430  loss = 2.25560 avg_loss = 3.66339\n",
      "epoch no.1 train no.132440  loss = 3.58067 avg_loss = 3.65562\n",
      "epoch no.1 train no.132450  loss = 3.23060 avg_loss = 3.62609\n",
      "epoch no.1 train no.132460  loss = 2.49483 avg_loss = 3.64642\n",
      "epoch no.1 train no.132470  loss = 3.89438 avg_loss = 3.67398\n",
      "epoch no.1 train no.132480  loss = 6.52026 avg_loss = 3.69293\n",
      "epoch no.1 train no.132490  loss = 4.20050 avg_loss = 3.76375\n",
      "epoch no.1 train no.132500  loss = 3.68208 avg_loss = 3.78497\n",
      "epoch no.1 train no.132510  loss = 4.32319 avg_loss = 3.79834\n",
      "epoch no.1 train no.132520  loss = 2.88153 avg_loss = 3.74135\n",
      "epoch no.1 train no.132530  loss = 2.23352 avg_loss = 3.74497\n",
      "epoch no.1 train no.132540  loss = 2.24663 avg_loss = 3.75412\n",
      "epoch no.1 train no.132550  loss = 3.52991 avg_loss = 3.69345\n",
      "epoch no.1 train no.132560  loss = 4.99776 avg_loss = 3.72902\n",
      "epoch no.1 train no.132570  loss = 3.51698 avg_loss = 3.68613\n",
      "epoch no.1 train no.132580  loss = 2.79359 avg_loss = 3.65864\n",
      "epoch no.1 train no.132590  loss = 3.09204 avg_loss = 3.58844\n",
      "epoch no.1 train no.132600  loss = 3.68082 avg_loss = 3.56610\n",
      "epoch no.1 train no.132610  loss = 2.64409 avg_loss = 3.55395\n",
      "epoch no.1 train no.132620  loss = 3.09586 avg_loss = 3.56227\n",
      "epoch no.1 train no.132630  loss = 3.85569 avg_loss = 3.54767\n",
      "epoch no.1 train no.132640  loss = 2.86878 avg_loss = 3.56692\n",
      "epoch no.1 train no.132650  loss = 3.06123 avg_loss = 3.57259\n",
      "epoch no.1 train no.132660  loss = 1.95272 avg_loss = 3.58109\n",
      "epoch no.1 train no.132670  loss = 4.58187 avg_loss = 3.60110\n",
      "epoch no.1 train no.132680  loss = 3.65088 avg_loss = 3.59971\n",
      "epoch no.1 train no.132690  loss = 4.07603 avg_loss = 3.58028\n",
      "epoch no.1 train no.132700  loss = 2.35476 avg_loss = 3.59633\n",
      "epoch no.1 train no.132710  loss = 3.45446 avg_loss = 3.59698\n",
      "epoch no.1 train no.132720  loss = 2.67240 avg_loss = 3.58760\n",
      "epoch no.1 train no.132730  loss = 3.81970 avg_loss = 3.60323\n",
      "epoch no.1 train no.132740  loss = 3.99823 avg_loss = 3.61120\n",
      "epoch no.1 train no.132750  loss = 2.26104 avg_loss = 3.60633\n",
      "epoch no.1 train no.132760  loss = 4.46468 avg_loss = 3.59935\n",
      "epoch no.1 train no.132770  loss = 4.89557 avg_loss = 3.61745\n",
      "epoch no.1 train no.132780  loss = 4.09240 avg_loss = 3.63651\n",
      "epoch no.1 train no.132790  loss = 6.13792 avg_loss = 3.61936\n",
      "epoch no.1 train no.132800  loss = 4.99293 avg_loss = 3.61874\n",
      "epoch no.1 train no.132810  loss = 2.84511 avg_loss = 3.63814\n",
      "epoch no.1 train no.132820  loss = 3.05599 avg_loss = 3.60205\n",
      "epoch no.1 train no.132830  loss = 3.17327 avg_loss = 3.59642\n",
      "epoch no.1 train no.132840  loss = 4.26811 avg_loss = 3.64179\n",
      "epoch no.1 train no.132850  loss = 5.70361 avg_loss = 3.65004\n",
      "epoch no.1 train no.132860  loss = 3.56514 avg_loss = 3.63022\n",
      "epoch no.1 train no.132870  loss = 4.85676 avg_loss = 3.60644\n",
      "epoch no.1 train no.132880  loss = 6.55425 avg_loss = 3.61232\n",
      "epoch no.1 train no.132890  loss = 4.45202 avg_loss = 3.58693\n",
      "epoch no.1 train no.132900  loss = 3.56921 avg_loss = 3.60501\n",
      "epoch no.1 train no.132910  loss = 5.23465 avg_loss = 3.59113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.132920  loss = 3.25607 avg_loss = 3.60717\n",
      "epoch no.1 train no.132930  loss = 3.02091 avg_loss = 3.64862\n",
      "epoch no.1 train no.132940  loss = 3.98755 avg_loss = 3.62889\n",
      "epoch no.1 train no.132950  loss = 4.77886 avg_loss = 3.63331\n",
      "epoch no.1 train no.132960  loss = 4.18761 avg_loss = 3.65787\n",
      "epoch no.1 train no.132970  loss = 4.56206 avg_loss = 3.69211\n",
      "epoch no.1 train no.132980  loss = 4.11032 avg_loss = 3.68796\n",
      "epoch no.1 train no.132990  loss = 6.64011 avg_loss = 3.69077\n",
      "epoch no.1 train no.133000  loss = 5.95809 avg_loss = 3.75258\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁신나는', '▁노래', '</s>']\n",
      "여름엔 역시 이 노래</s>\n",
      "epoch no.1 train no.133010  loss = 4.58341 avg_loss = 3.76417\n",
      "epoch no.1 train no.133020  loss = 5.32586 avg_loss = 3.73174\n",
      "epoch no.1 train no.133030  loss = 2.78128 avg_loss = 3.72904\n",
      "epoch no.1 train no.133040  loss = 3.00840 avg_loss = 3.70477\n",
      "epoch no.1 train no.133050  loss = 2.64479 avg_loss = 3.69204\n",
      "epoch no.1 train no.133060  loss = 2.97087 avg_loss = 3.65648\n",
      "epoch no.1 train no.133070  loss = 3.89782 avg_loss = 3.68085\n",
      "epoch no.1 train no.133080  loss = 3.78459 avg_loss = 3.65772\n",
      "epoch no.1 train no.133090  loss = 1.69389 avg_loss = 3.66328\n",
      "epoch no.1 train no.133100  loss = 4.05621 avg_loss = 3.73939\n",
      "epoch no.1 train no.133110  loss = 4.98792 avg_loss = 3.73329\n",
      "epoch no.1 train no.133120  loss = 3.07749 avg_loss = 3.71150\n",
      "epoch no.1 train no.133130  loss = 5.59994 avg_loss = 3.75906\n",
      "epoch no.1 train no.133140  loss = 2.00586 avg_loss = 3.78994\n",
      "epoch no.1 train no.133150  loss = 4.44092 avg_loss = 3.74061\n",
      "epoch no.1 train no.133160  loss = 2.83340 avg_loss = 3.70261\n",
      "epoch no.1 train no.133170  loss = 5.83577 avg_loss = 3.69275\n",
      "epoch no.1 train no.133180  loss = 2.54180 avg_loss = 3.67969\n",
      "epoch no.1 train no.133190  loss = 5.06747 avg_loss = 3.71919\n",
      "epoch no.1 train no.133200  loss = 5.15957 avg_loss = 3.71200\n",
      "epoch no.1 train no.133210  loss = 3.69263 avg_loss = 3.62874\n",
      "epoch no.1 train no.133220  loss = 3.63906 avg_loss = 3.61527\n",
      "epoch no.1 train no.133230  loss = 4.90993 avg_loss = 3.62705\n",
      "epoch no.1 train no.133240  loss = 3.14934 avg_loss = 3.63437\n",
      "epoch no.1 train no.133250  loss = 3.58100 avg_loss = 3.65699\n",
      "epoch no.1 train no.133260  loss = 2.63232 avg_loss = 3.67470\n",
      "epoch no.1 train no.133270  loss = 1.93083 avg_loss = 3.65362\n",
      "epoch no.1 train no.133280  loss = 3.42869 avg_loss = 3.62624\n",
      "epoch no.1 train no.133290  loss = 4.11113 avg_loss = 3.56321\n",
      "epoch no.1 train no.133300  loss = 5.32280 avg_loss = 3.57372\n",
      "epoch no.1 train no.133310  loss = 3.99052 avg_loss = 3.59094\n",
      "epoch no.1 train no.133320  loss = 5.25296 avg_loss = 3.64287\n",
      "epoch no.1 train no.133330  loss = 4.20539 avg_loss = 3.66771\n",
      "epoch no.1 train no.133340  loss = 3.55360 avg_loss = 3.62311\n",
      "epoch no.1 train no.133350  loss = 3.38150 avg_loss = 3.59674\n",
      "epoch no.1 train no.133360  loss = 3.36588 avg_loss = 3.61439\n",
      "epoch no.1 train no.133370  loss = 3.34097 avg_loss = 3.59287\n",
      "epoch no.1 train no.133380  loss = 4.44186 avg_loss = 3.59730\n",
      "epoch no.1 train no.133390  loss = 5.77284 avg_loss = 3.60984\n",
      "epoch no.1 train no.133400  loss = 2.83403 avg_loss = 3.55907\n",
      "epoch no.1 train no.133410  loss = 2.19973 avg_loss = 3.61795\n",
      "epoch no.1 train no.133420  loss = 3.66008 avg_loss = 3.61150\n",
      "epoch no.1 train no.133430  loss = 3.35575 avg_loss = 3.68698\n",
      "epoch no.1 train no.133440  loss = 3.19694 avg_loss = 3.63306\n",
      "epoch no.1 train no.133450  loss = 2.36787 avg_loss = 3.61289\n",
      "epoch no.1 train no.133460  loss = 4.01184 avg_loss = 3.63884\n",
      "epoch no.1 train no.133470  loss = 4.18778 avg_loss = 3.71685\n",
      "epoch no.1 train no.133480  loss = 3.16882 avg_loss = 3.63495\n",
      "epoch no.1 train no.133490  loss = 2.32564 avg_loss = 3.59667\n",
      "epoch no.1 train no.133500  loss = 3.67018 avg_loss = 3.59216\n",
      "epoch no.1 train no.133510  loss = 4.20346 avg_loss = 3.59323\n",
      "epoch no.1 train no.133520  loss = 3.23237 avg_loss = 3.61711\n",
      "epoch no.1 train no.133530  loss = 5.03277 avg_loss = 3.62589\n",
      "epoch no.1 train no.133540  loss = 3.58306 avg_loss = 3.59083\n",
      "epoch no.1 train no.133550  loss = 3.47851 avg_loss = 3.56941\n",
      "epoch no.1 train no.133560  loss = 3.47372 avg_loss = 3.58726\n",
      "epoch no.1 train no.133570  loss = 3.20815 avg_loss = 3.57827\n",
      "epoch no.1 train no.133580  loss = 4.97504 avg_loss = 3.57635\n",
      "epoch no.1 train no.133590  loss = 3.13814 avg_loss = 3.58330\n",
      "epoch no.1 train no.133600  loss = 2.23977 avg_loss = 3.56853\n",
      "epoch no.1 train no.133610  loss = 3.78148 avg_loss = 3.53602\n",
      "epoch no.1 train no.133620  loss = 2.58566 avg_loss = 3.52479\n",
      "epoch no.1 train no.133630  loss = 3.40911 avg_loss = 3.53506\n",
      "epoch no.1 train no.133640  loss = 4.51442 avg_loss = 3.57229\n",
      "epoch no.1 train no.133650  loss = 3.85461 avg_loss = 3.53912\n",
      "epoch no.1 train no.133660  loss = 3.35903 avg_loss = 3.52589\n",
      "epoch no.1 train no.133670  loss = 4.59458 avg_loss = 3.51986\n",
      "epoch no.1 train no.133680  loss = 3.05466 avg_loss = 3.53082\n",
      "epoch no.1 train no.133690  loss = 2.08180 avg_loss = 3.50746\n",
      "epoch no.1 train no.133700  loss = 4.66361 avg_loss = 3.52651\n",
      "epoch no.1 train no.133710  loss = 5.79046 avg_loss = 3.61192\n",
      "epoch no.1 train no.133720  loss = 4.71729 avg_loss = 3.61906\n",
      "epoch no.1 train no.133730  loss = 2.96268 avg_loss = 3.57911\n",
      "epoch no.1 train no.133740  loss = 4.68713 avg_loss = 3.65077\n",
      "epoch no.1 train no.133750  loss = 2.18511 avg_loss = 3.60131\n",
      "epoch no.1 train no.133760  loss = 1.96722 avg_loss = 3.58196\n",
      "epoch no.1 train no.133770  loss = 3.90275 avg_loss = 3.59051\n",
      "epoch no.1 train no.133780  loss = 5.51995 avg_loss = 3.61513\n",
      "epoch no.1 train no.133790  loss = 3.78025 avg_loss = 3.61007\n",
      "epoch no.1 train no.133800  loss = 4.28374 avg_loss = 3.61585\n",
      "epoch no.1 train no.133810  loss = 1.36282 avg_loss = 3.57904\n",
      "epoch no.1 train no.133820  loss = 4.47901 avg_loss = 3.58921\n",
      "epoch no.1 train no.133830  loss = 3.06378 avg_loss = 3.61533\n",
      "epoch no.1 train no.133840  loss = 6.37034 avg_loss = 3.74090\n",
      "epoch no.1 train no.133850  loss = 3.92948 avg_loss = 3.71344\n",
      "epoch no.1 train no.133860  loss = 3.86013 avg_loss = 3.66769\n",
      "epoch no.1 train no.133870  loss = 6.69835 avg_loss = 3.66530\n",
      "epoch no.1 train no.133880  loss = 3.93404 avg_loss = 3.68839\n",
      "epoch no.1 train no.133890  loss = 5.54547 avg_loss = 3.65231\n",
      "epoch no.1 train no.133900  loss = 2.19521 avg_loss = 3.66529\n",
      "epoch no.1 train no.133910  loss = 4.20146 avg_loss = 3.64045\n",
      "epoch no.1 train no.133920  loss = 2.81715 avg_loss = 3.61146\n",
      "epoch no.1 train no.133930  loss = 3.09895 avg_loss = 3.64201\n",
      "epoch no.1 train no.133940  loss = 3.68665 avg_loss = 3.66222\n",
      "epoch no.1 train no.133950  loss = 4.22904 avg_loss = 3.66937\n",
      "epoch no.1 train no.133960  loss = 2.82252 avg_loss = 3.69402\n",
      "epoch no.1 train no.133970  loss = 4.43460 avg_loss = 3.75910\n",
      "epoch no.1 train no.133980  loss = 1.69461 avg_loss = 3.69820\n",
      "epoch no.1 train no.133990  loss = 3.05127 avg_loss = 3.70219\n",
      "epoch no.1 train no.134000  loss = 3.26799 avg_loss = 3.69169\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '▁역시', '한', '▁트로', '피', '컬', '▁음악']\n",
      "여름엔 청량한 트로피컬</s>\n",
      "epoch no.1 train no.134010  loss = 1.71722 avg_loss = 3.66905\n",
      "epoch no.1 train no.134020  loss = 4.56766 avg_loss = 3.71313\n",
      "epoch no.1 train no.134030  loss = 2.48840 avg_loss = 3.64403\n",
      "epoch no.1 train no.134040  loss = 4.17810 avg_loss = 3.61019\n",
      "epoch no.1 train no.134050  loss = 3.43618 avg_loss = 3.54890\n",
      "epoch no.1 train no.134060  loss = 5.24753 avg_loss = 3.58631\n",
      "epoch no.1 train no.134070  loss = 5.74229 avg_loss = 3.61809\n",
      "epoch no.1 train no.134080  loss = 4.37070 avg_loss = 3.62365\n",
      "epoch no.1 train no.134090  loss = 2.19678 avg_loss = 3.60701\n",
      "epoch no.1 train no.134100  loss = 4.14219 avg_loss = 3.57543\n",
      "epoch no.1 train no.134110  loss = 2.61638 avg_loss = 3.56409\n",
      "epoch no.1 train no.134120  loss = 3.47582 avg_loss = 3.56913\n",
      "epoch no.1 train no.134130  loss = 2.99311 avg_loss = 3.58041\n",
      "epoch no.1 train no.134140  loss = 5.75374 avg_loss = 3.65213\n",
      "epoch no.1 train no.134150  loss = 4.88287 avg_loss = 3.68999\n",
      "epoch no.1 train no.134160  loss = 2.62324 avg_loss = 3.66955\n",
      "epoch no.1 train no.134170  loss = 3.02958 avg_loss = 3.69608\n",
      "epoch no.1 train no.134180  loss = 4.23434 avg_loss = 3.67864\n",
      "epoch no.1 train no.134190  loss = 4.63168 avg_loss = 3.66840\n",
      "epoch no.1 train no.134200  loss = 2.56046 avg_loss = 3.68844\n",
      "epoch no.1 train no.134210  loss = 1.51276 avg_loss = 3.66296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.134220  loss = 3.70415 avg_loss = 3.64764\n",
      "epoch no.1 train no.134230  loss = 2.22353 avg_loss = 3.57493\n",
      "epoch no.1 train no.134240  loss = 4.09500 avg_loss = 3.55089\n",
      "epoch no.1 train no.134250  loss = 3.79234 avg_loss = 3.57971\n",
      "epoch no.1 train no.134260  loss = 2.01211 avg_loss = 3.58698\n",
      "epoch no.1 train no.134270  loss = 1.99390 avg_loss = 3.58022\n",
      "epoch no.1 train no.134280  loss = 4.81845 avg_loss = 3.58802\n",
      "epoch no.1 train no.134290  loss = 2.71063 avg_loss = 3.60360\n",
      "epoch no.1 train no.134300  loss = 3.11538 avg_loss = 3.63502\n",
      "epoch no.1 train no.134310  loss = 2.86317 avg_loss = 3.64041\n",
      "epoch no.1 train no.134320  loss = 1.70180 avg_loss = 3.62823\n",
      "epoch no.1 train no.134330  loss = 4.48737 avg_loss = 3.67782\n",
      "epoch no.1 train no.134340  loss = 3.84895 avg_loss = 3.66419\n",
      "epoch no.1 train no.134350  loss = 2.96275 avg_loss = 3.67734\n",
      "epoch no.1 train no.134360  loss = 2.42616 avg_loss = 3.68683\n",
      "epoch no.1 train no.134370  loss = 2.44292 avg_loss = 3.66101\n",
      "epoch no.1 train no.134380  loss = 4.15457 avg_loss = 3.66455\n",
      "epoch no.1 train no.134390  loss = 5.18451 avg_loss = 3.66004\n",
      "epoch no.1 train no.134400  loss = 2.38394 avg_loss = 3.68067\n",
      "epoch no.1 train no.134410  loss = 3.31551 avg_loss = 3.65221\n",
      "epoch no.1 train no.134420  loss = 3.30289 avg_loss = 3.64354\n",
      "epoch no.1 train no.134430  loss = 1.96465 avg_loss = 3.66117\n",
      "epoch no.1 train no.134440  loss = 4.61729 avg_loss = 3.71184\n",
      "epoch no.1 train no.134450  loss = 2.88631 avg_loss = 3.71043\n",
      "epoch no.1 train no.134460  loss = 3.80326 avg_loss = 3.70731\n",
      "epoch no.1 train no.134470  loss = 4.09282 avg_loss = 3.72178\n",
      "epoch no.1 train no.134480  loss = 3.58831 avg_loss = 3.70580\n",
      "epoch no.1 train no.134490  loss = 1.43760 avg_loss = 3.68063\n",
      "epoch no.1 train no.134500  loss = 2.59634 avg_loss = 3.73851\n",
      "epoch no.1 train no.134510  loss = 3.08168 avg_loss = 3.74052\n",
      "epoch no.1 train no.134520  loss = 4.27687 avg_loss = 3.71982\n",
      "epoch no.1 train no.134530  loss = 2.76466 avg_loss = 3.76523\n",
      "epoch no.1 train no.134540  loss = 3.01355 avg_loss = 3.78241\n",
      "epoch no.1 train no.134550  loss = 5.34265 avg_loss = 3.77756\n",
      "epoch no.1 train no.134560  loss = 2.46856 avg_loss = 3.77354\n",
      "epoch no.1 train no.134570  loss = 3.52424 avg_loss = 3.70882\n",
      "epoch no.1 train no.134580  loss = 4.14353 avg_loss = 3.67836\n",
      "epoch no.1 train no.134590  loss = 2.85148 avg_loss = 3.65619\n",
      "epoch no.1 train no.134600  loss = 3.81097 avg_loss = 3.61388\n",
      "epoch no.1 train no.134610  loss = 3.32145 avg_loss = 3.67938\n",
      "epoch no.1 train no.134620  loss = 3.20986 avg_loss = 3.66097\n",
      "epoch no.1 train no.134630  loss = 7.02470 avg_loss = 3.72194\n",
      "epoch no.1 train no.134640  loss = 2.69498 avg_loss = 3.68935\n",
      "epoch no.1 train no.134650  loss = 4.25810 avg_loss = 3.68034\n",
      "epoch no.1 train no.134660  loss = 5.89307 avg_loss = 3.74883\n",
      "epoch no.1 train no.134670  loss = 2.69511 avg_loss = 3.75333\n",
      "epoch no.1 train no.134680  loss = 4.65368 avg_loss = 3.74785\n",
      "epoch no.1 train no.134690  loss = 3.28198 avg_loss = 3.71335\n",
      "epoch no.1 train no.134700  loss = 3.33133 avg_loss = 3.70895\n",
      "epoch no.1 train no.134710  loss = 3.13410 avg_loss = 3.70593\n",
      "epoch no.1 train no.134720  loss = 2.27932 avg_loss = 3.65032\n",
      "epoch no.1 train no.134730  loss = 3.33118 avg_loss = 3.64485\n",
      "epoch no.1 train no.134740  loss = 1.72988 avg_loss = 3.64477\n",
      "epoch no.1 train no.134750  loss = 3.50483 avg_loss = 3.63193\n",
      "epoch no.1 train no.134760  loss = 5.57214 avg_loss = 3.68104\n",
      "epoch no.1 train no.134770  loss = 2.73827 avg_loss = 3.65614\n",
      "epoch no.1 train no.134780  loss = 4.83638 avg_loss = 3.71770\n",
      "epoch no.1 train no.134790  loss = 4.08157 avg_loss = 3.75338\n",
      "epoch no.1 train no.134800  loss = 3.10917 avg_loss = 3.73595\n",
      "epoch no.1 train no.134810  loss = 3.72523 avg_loss = 3.74676\n",
      "epoch no.1 train no.134820  loss = 3.06165 avg_loss = 3.74308\n",
      "epoch no.1 train no.134830  loss = 3.66159 avg_loss = 3.76126\n",
      "epoch no.1 train no.134840  loss = 3.47663 avg_loss = 3.80544\n",
      "epoch no.1 train no.134850  loss = 3.06193 avg_loss = 3.82366\n",
      "epoch no.1 train no.134860  loss = 4.16628 avg_loss = 3.82226\n",
      "epoch no.1 train no.134870  loss = 3.85634 avg_loss = 3.80060\n",
      "epoch no.1 train no.134880  loss = 4.89454 avg_loss = 3.87274\n",
      "epoch no.1 train no.134890  loss = 3.33018 avg_loss = 3.90401\n",
      "epoch no.1 train no.134900  loss = 6.80428 avg_loss = 3.97338\n",
      "epoch no.1 train no.134910  loss = 2.10150 avg_loss = 3.87230\n",
      "epoch no.1 train no.134920  loss = 3.84231 avg_loss = 3.86157\n",
      "epoch no.1 train no.134930  loss = 3.96614 avg_loss = 3.84474\n",
      "epoch no.1 train no.134940  loss = 3.93068 avg_loss = 3.83995\n",
      "epoch no.1 train no.134950  loss = 2.87786 avg_loss = 3.80797\n",
      "epoch no.1 train no.134960  loss = 4.30841 avg_loss = 3.70927\n",
      "epoch no.1 train no.134970  loss = 3.45522 avg_loss = 3.75747\n",
      "epoch no.1 train no.134980  loss = 3.21676 avg_loss = 3.68468\n",
      "epoch no.1 train no.134990  loss = 3.30609 avg_loss = 3.71029\n",
      "epoch no.1 train no.135000  loss = 2.89854 avg_loss = 3.69887\n",
      "3\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '적인']\n",
      "여름밤의 감성</s>\n",
      "epoch no.1 train no.135010  loss = 4.04095 avg_loss = 3.69470\n",
      "epoch no.1 train no.135020  loss = 7.30089 avg_loss = 3.68383\n",
      "epoch no.1 train no.135030  loss = 3.02083 avg_loss = 3.61875\n",
      "epoch no.1 train no.135040  loss = 3.18344 avg_loss = 3.59779\n",
      "epoch no.1 train no.135050  loss = 4.69397 avg_loss = 3.58362\n",
      "epoch no.1 train no.135060  loss = 2.67871 avg_loss = 3.58126\n",
      "epoch no.1 train no.135070  loss = 2.60574 avg_loss = 3.57017\n",
      "epoch no.1 train no.135080  loss = 3.85358 avg_loss = 3.62389\n",
      "epoch no.1 train no.135090  loss = 4.29262 avg_loss = 3.63587\n",
      "epoch no.1 train no.135100  loss = 3.99878 avg_loss = 3.65243\n",
      "epoch no.1 train no.135110  loss = 3.42580 avg_loss = 3.65175\n",
      "epoch no.1 train no.135120  loss = 4.32990 avg_loss = 3.64473\n",
      "epoch no.1 train no.135130  loss = 2.54733 avg_loss = 3.63223\n",
      "epoch no.1 train no.135140  loss = 2.47029 avg_loss = 3.61501\n",
      "epoch no.1 train no.135150  loss = 4.13125 avg_loss = 3.61293\n",
      "epoch no.1 train no.135160  loss = 2.27411 avg_loss = 3.58590\n",
      "epoch no.1 train no.135170  loss = 4.87270 avg_loss = 3.59411\n",
      "epoch no.1 train no.135180  loss = 3.59241 avg_loss = 3.57667\n",
      "epoch no.1 train no.135190  loss = 1.89187 avg_loss = 3.53120\n",
      "epoch no.1 train no.135200  loss = 3.75066 avg_loss = 3.52558\n",
      "epoch no.1 train no.135210  loss = 4.08118 avg_loss = 3.54318\n",
      "epoch no.1 train no.135220  loss = 3.79690 avg_loss = 3.53290\n",
      "epoch no.1 train no.135230  loss = 5.14160 avg_loss = 3.56657\n",
      "epoch no.1 train no.135240  loss = 3.45163 avg_loss = 3.57488\n",
      "epoch no.1 train no.135250  loss = 2.75504 avg_loss = 3.51217\n",
      "epoch no.1 train no.135260  loss = 3.22462 avg_loss = 3.53359\n",
      "epoch no.1 train no.135270  loss = 3.41263 avg_loss = 3.50422\n",
      "epoch no.1 train no.135280  loss = 2.18673 avg_loss = 3.48380\n",
      "epoch no.1 train no.135290  loss = 2.83721 avg_loss = 3.45597\n",
      "epoch no.1 train no.135300  loss = 4.35952 avg_loss = 3.49813\n",
      "epoch no.1 train no.135310  loss = 2.45598 avg_loss = 3.49132\n",
      "epoch no.1 train no.135320  loss = 3.25589 avg_loss = 3.47995\n",
      "epoch no.1 train no.135330  loss = 3.23932 avg_loss = 3.55965\n",
      "epoch no.1 train no.135340  loss = 5.38941 avg_loss = 3.63944\n",
      "epoch no.1 train no.135350  loss = 2.28506 avg_loss = 3.61325\n",
      "epoch no.1 train no.135360  loss = 3.03370 avg_loss = 3.55822\n",
      "epoch no.1 train no.135370  loss = 3.82645 avg_loss = 3.53824\n",
      "epoch no.1 train no.135380  loss = 2.16513 avg_loss = 3.54764\n",
      "epoch no.1 train no.135390  loss = 2.04083 avg_loss = 3.61491\n",
      "epoch no.1 train no.135400  loss = 1.52706 avg_loss = 3.66928\n",
      "epoch no.1 train no.135410  loss = 1.72781 avg_loss = 3.63329\n",
      "epoch no.1 train no.135420  loss = 2.88951 avg_loss = 3.67922\n",
      "epoch no.1 train no.135430  loss = 3.07698 avg_loss = 3.63425\n",
      "epoch no.1 train no.135440  loss = 2.96671 avg_loss = 3.64702\n",
      "epoch no.1 train no.135450  loss = 2.44502 avg_loss = 3.61193\n",
      "epoch no.1 train no.135460  loss = 2.71164 avg_loss = 3.65795\n",
      "epoch no.1 train no.135470  loss = 2.45119 avg_loss = 3.69150\n",
      "epoch no.1 train no.135480  loss = 4.66400 avg_loss = 3.70776\n",
      "epoch no.1 train no.135490  loss = 4.20858 avg_loss = 3.67068\n",
      "epoch no.1 train no.135500  loss = 3.99040 avg_loss = 3.70271\n",
      "epoch no.1 train no.135510  loss = 1.99625 avg_loss = 3.65707\n",
      "epoch no.1 train no.135520  loss = 3.62968 avg_loss = 3.67106\n",
      "epoch no.1 train no.135530  loss = 3.23460 avg_loss = 3.66532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.135540  loss = 3.05835 avg_loss = 3.60398\n",
      "epoch no.1 train no.135550  loss = 2.93564 avg_loss = 3.53475\n",
      "epoch no.1 train no.135560  loss = 5.82290 avg_loss = 3.55198\n",
      "epoch no.1 train no.135570  loss = 2.73778 avg_loss = 3.55902\n",
      "epoch no.1 train no.135580  loss = 3.25831 avg_loss = 3.59003\n",
      "epoch no.1 train no.135590  loss = 3.01251 avg_loss = 3.60557\n",
      "epoch no.1 train no.135600  loss = 3.20532 avg_loss = 3.59948\n",
      "epoch no.1 train no.135610  loss = 2.43857 avg_loss = 3.59934\n",
      "epoch no.1 train no.135620  loss = 3.02693 avg_loss = 3.66797\n",
      "epoch no.1 train no.135630  loss = 2.72220 avg_loss = 3.60900\n",
      "epoch no.1 train no.135640  loss = 5.25600 avg_loss = 3.66599\n",
      "epoch no.1 train no.135650  loss = 3.20124 avg_loss = 3.70644\n",
      "epoch no.1 train no.135660  loss = 3.10944 avg_loss = 3.73853\n",
      "epoch no.1 train no.135670  loss = 4.63821 avg_loss = 3.83803\n",
      "epoch no.1 train no.135680  loss = 3.38344 avg_loss = 3.81315\n",
      "epoch no.1 train no.135690  loss = 5.11481 avg_loss = 3.81806\n",
      "epoch no.1 train no.135700  loss = 3.06384 avg_loss = 3.79408\n",
      "epoch no.1 train no.135710  loss = 3.71955 avg_loss = 3.77419\n",
      "epoch no.1 train no.135720  loss = 3.90966 avg_loss = 3.78860\n",
      "epoch no.1 train no.135730  loss = 2.33461 avg_loss = 3.77106\n",
      "epoch no.1 train no.135740  loss = 4.29075 avg_loss = 3.73105\n",
      "epoch no.1 train no.135750  loss = 2.35823 avg_loss = 3.73540\n",
      "epoch no.1 train no.135760  loss = 3.12255 avg_loss = 3.73891\n",
      "epoch no.1 train no.135770  loss = 2.17718 avg_loss = 3.68357\n",
      "epoch no.1 train no.135780  loss = 2.80637 avg_loss = 3.70146\n",
      "epoch no.1 train no.135790  loss = 2.45891 avg_loss = 3.70450\n",
      "epoch no.1 train no.135800  loss = 2.71376 avg_loss = 3.67685\n",
      "epoch no.1 train no.135810  loss = 3.90824 avg_loss = 3.76145\n",
      "epoch no.1 train no.135820  loss = 4.44129 avg_loss = 3.71187\n",
      "epoch no.1 train no.135830  loss = 4.99608 avg_loss = 3.70596\n",
      "epoch no.1 train no.135840  loss = 2.86625 avg_loss = 3.69540\n",
      "epoch no.1 train no.135850  loss = 3.97225 avg_loss = 3.67789\n",
      "epoch no.1 train no.135860  loss = 3.59377 avg_loss = 3.64828\n",
      "epoch no.1 train no.135870  loss = 2.84310 avg_loss = 3.72476\n",
      "epoch no.1 train no.135880  loss = 4.96706 avg_loss = 3.70225\n",
      "epoch no.1 train no.135890  loss = 3.47528 avg_loss = 3.68765\n",
      "epoch no.1 train no.135900  loss = 2.40215 avg_loss = 3.65957\n",
      "epoch no.1 train no.135910  loss = 3.20730 avg_loss = 3.63824\n",
      "epoch no.1 train no.135920  loss = 3.97118 avg_loss = 3.65185\n",
      "epoch no.1 train no.135930  loss = 3.67166 avg_loss = 3.69727\n",
      "epoch no.1 train no.135940  loss = 4.06193 avg_loss = 3.76245\n",
      "epoch no.1 train no.135950  loss = 3.87970 avg_loss = 3.72726\n",
      "epoch no.1 train no.135960  loss = 2.96294 avg_loss = 3.67881\n",
      "epoch no.1 train no.135970  loss = 4.36805 avg_loss = 3.70927\n",
      "epoch no.1 train no.135980  loss = 4.18529 avg_loss = 3.67344\n",
      "epoch no.1 train no.135990  loss = 4.40191 avg_loss = 3.67461\n",
      "epoch no.1 train no.136000  loss = 2.76363 avg_loss = 3.67395\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '</s>']\n",
      "여름밤에 듣는 재즈</s>\n",
      "epoch no.1 train no.136010  loss = 3.87097 avg_loss = 3.66522\n",
      "epoch no.1 train no.136020  loss = 3.64711 avg_loss = 3.65403\n",
      "epoch no.1 train no.136030  loss = 3.70916 avg_loss = 3.67605\n",
      "epoch no.1 train no.136040  loss = 2.21336 avg_loss = 3.70669\n",
      "epoch no.1 train no.136050  loss = 2.70777 avg_loss = 3.69314\n",
      "epoch no.1 train no.136060  loss = 6.18654 avg_loss = 3.76214\n",
      "epoch no.1 train no.136070  loss = 1.98359 avg_loss = 3.69723\n",
      "epoch no.1 train no.136080  loss = 5.09389 avg_loss = 3.71217\n",
      "epoch no.1 train no.136090  loss = 2.73051 avg_loss = 3.73007\n",
      "epoch no.1 train no.136100  loss = 4.17745 avg_loss = 3.72663\n",
      "epoch no.1 train no.136110  loss = 3.63730 avg_loss = 3.72384\n",
      "epoch no.1 train no.136120  loss = 3.29392 avg_loss = 3.68612\n",
      "epoch no.1 train no.136130  loss = 3.46304 avg_loss = 3.64896\n",
      "epoch no.1 train no.136140  loss = 2.05334 avg_loss = 3.68414\n",
      "epoch no.1 train no.136150  loss = 4.26173 avg_loss = 3.71244\n",
      "epoch no.1 train no.136160  loss = 5.13706 avg_loss = 3.73176\n",
      "epoch no.1 train no.136170  loss = 2.35070 avg_loss = 3.70787\n",
      "epoch no.1 train no.136180  loss = 4.06173 avg_loss = 3.66536\n",
      "epoch no.1 train no.136190  loss = 3.29760 avg_loss = 3.60930\n",
      "epoch no.1 train no.136200  loss = 2.96190 avg_loss = 3.63627\n",
      "epoch no.1 train no.136210  loss = 5.44239 avg_loss = 3.67955\n",
      "epoch no.1 train no.136220  loss = 4.08628 avg_loss = 3.69465\n",
      "epoch no.1 train no.136230  loss = 5.77939 avg_loss = 3.73114\n",
      "epoch no.1 train no.136240  loss = 3.66975 avg_loss = 3.71722\n",
      "epoch no.1 train no.136250  loss = 5.35526 avg_loss = 3.79177\n",
      "epoch no.1 train no.136260  loss = 4.71828 avg_loss = 3.78779\n",
      "epoch no.1 train no.136270  loss = 5.05856 avg_loss = 3.83279\n",
      "epoch no.1 train no.136280  loss = 3.85652 avg_loss = 3.83880\n",
      "epoch no.1 train no.136290  loss = 3.75396 avg_loss = 3.78246\n",
      "epoch no.1 train no.136300  loss = 2.45273 avg_loss = 3.79343\n",
      "epoch no.1 train no.136310  loss = 3.21730 avg_loss = 3.77186\n",
      "epoch no.1 train no.136320  loss = 2.75684 avg_loss = 3.76249\n",
      "epoch no.1 train no.136330  loss = 3.30268 avg_loss = 3.67511\n",
      "epoch no.1 train no.136340  loss = 4.58564 avg_loss = 3.65446\n",
      "epoch no.1 train no.136350  loss = 4.15656 avg_loss = 3.67172\n",
      "epoch no.1 train no.136360  loss = 2.88488 avg_loss = 3.63382\n",
      "epoch no.1 train no.136370  loss = 4.04206 avg_loss = 3.64724\n",
      "epoch no.1 train no.136380  loss = 4.80396 avg_loss = 3.60439\n",
      "epoch no.1 train no.136390  loss = 2.51444 avg_loss = 3.60944\n",
      "epoch no.1 train no.136400  loss = 3.11221 avg_loss = 3.54868\n",
      "epoch no.1 train no.136410  loss = 4.51137 avg_loss = 3.55631\n",
      "epoch no.1 train no.136420  loss = 4.29589 avg_loss = 3.56985\n",
      "epoch no.1 train no.136430  loss = 3.97908 avg_loss = 3.54883\n",
      "epoch no.1 train no.136440  loss = 3.04948 avg_loss = 3.56314\n",
      "epoch no.1 train no.136450  loss = 1.97731 avg_loss = 3.57723\n",
      "epoch no.1 train no.136460  loss = 5.10745 avg_loss = 3.63178\n",
      "epoch no.1 train no.136470  loss = 2.85915 avg_loss = 3.64018\n",
      "epoch no.1 train no.136480  loss = 2.83541 avg_loss = 3.63435\n",
      "epoch no.1 train no.136490  loss = 3.26863 avg_loss = 3.61902\n",
      "epoch no.1 train no.136500  loss = 2.30568 avg_loss = 3.63185\n",
      "epoch no.1 train no.136510  loss = 3.40153 avg_loss = 3.57175\n",
      "epoch no.1 train no.136520  loss = 2.61596 avg_loss = 3.55581\n",
      "epoch no.1 train no.136530  loss = 3.15493 avg_loss = 3.49416\n",
      "epoch no.1 train no.136540  loss = 2.68222 avg_loss = 3.47395\n",
      "epoch no.1 train no.136550  loss = 2.36046 avg_loss = 3.47869\n",
      "epoch no.1 train no.136560  loss = 4.30874 avg_loss = 3.50707\n",
      "epoch no.1 train no.136570  loss = 2.52760 avg_loss = 3.52890\n",
      "epoch no.1 train no.136580  loss = 5.12972 avg_loss = 3.61199\n",
      "epoch no.1 train no.136590  loss = 3.80305 avg_loss = 3.60686\n",
      "epoch no.1 train no.136600  loss = 3.45056 avg_loss = 3.63307\n",
      "epoch no.1 train no.136610  loss = 2.54824 avg_loss = 3.63268\n",
      "epoch no.1 train no.136620  loss = 2.42351 avg_loss = 3.64881\n",
      "epoch no.1 train no.136630  loss = 4.41882 avg_loss = 3.70298\n",
      "epoch no.1 train no.136640  loss = 3.12138 avg_loss = 3.73013\n",
      "epoch no.1 train no.136650  loss = 2.65155 avg_loss = 3.72234\n",
      "epoch no.1 train no.136660  loss = 4.66359 avg_loss = 3.73231\n",
      "epoch no.1 train no.136670  loss = 3.69314 avg_loss = 3.71363\n",
      "epoch no.1 train no.136680  loss = 2.63991 avg_loss = 3.71352\n",
      "epoch no.1 train no.136690  loss = 4.17707 avg_loss = 3.70093\n",
      "epoch no.1 train no.136700  loss = 4.72003 avg_loss = 3.70991\n",
      "epoch no.1 train no.136710  loss = 4.00574 avg_loss = 3.73379\n",
      "epoch no.1 train no.136720  loss = 5.29419 avg_loss = 3.76814\n",
      "epoch no.1 train no.136730  loss = 4.67354 avg_loss = 3.78050\n",
      "epoch no.1 train no.136740  loss = 3.89055 avg_loss = 3.77296\n",
      "epoch no.1 train no.136750  loss = 3.22714 avg_loss = 3.79410\n",
      "epoch no.1 train no.136760  loss = 3.06679 avg_loss = 3.76906\n",
      "epoch no.1 train no.136770  loss = 2.90604 avg_loss = 3.73387\n",
      "epoch no.1 train no.136780  loss = 4.10829 avg_loss = 3.72014\n",
      "epoch no.1 train no.136790  loss = 2.58221 avg_loss = 3.74940\n",
      "epoch no.1 train no.136800  loss = 4.22843 avg_loss = 3.78203\n",
      "epoch no.1 train no.136810  loss = 2.54000 avg_loss = 3.69268\n",
      "epoch no.1 train no.136820  loss = 2.97468 avg_loss = 3.71310\n",
      "epoch no.1 train no.136830  loss = 4.06114 avg_loss = 3.70782\n",
      "epoch no.1 train no.136840  loss = 4.98219 avg_loss = 3.71147\n",
      "epoch no.1 train no.136850  loss = 2.94440 avg_loss = 3.73133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.136860  loss = 3.45928 avg_loss = 3.71581\n",
      "epoch no.1 train no.136870  loss = 3.04009 avg_loss = 3.68188\n",
      "epoch no.1 train no.136880  loss = 5.79485 avg_loss = 3.66748\n",
      "epoch no.1 train no.136890  loss = 3.00288 avg_loss = 3.69574\n",
      "epoch no.1 train no.136900  loss = 3.04578 avg_loss = 3.69103\n",
      "epoch no.1 train no.136910  loss = 2.42474 avg_loss = 3.72829\n",
      "epoch no.1 train no.136920  loss = 4.56745 avg_loss = 3.75027\n",
      "epoch no.1 train no.136930  loss = 3.56736 avg_loss = 3.72211\n",
      "epoch no.1 train no.136940  loss = 4.75233 avg_loss = 3.71224\n",
      "epoch no.1 train no.136950  loss = 3.71275 avg_loss = 3.71040\n",
      "epoch no.1 train no.136960  loss = 4.08139 avg_loss = 3.73902\n",
      "epoch no.1 train no.136970  loss = 3.68801 avg_loss = 3.72330\n",
      "epoch no.1 train no.136980  loss = 3.08746 avg_loss = 3.73341\n",
      "epoch no.1 train no.136990  loss = 4.91175 avg_loss = 3.77867\n",
      "epoch no.1 train no.137000  loss = 5.05346 avg_loss = 3.78477\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '▁끝', '자', '락에', '▁듣는', '▁노래', '음악', '</s>']\n",
      "여름의 끝자락에 듣는 인디음악</s>\n",
      "epoch no.1 train no.137010  loss = 2.76144 avg_loss = 3.71434\n",
      "epoch no.1 train no.137020  loss = 4.69454 avg_loss = 3.75615\n",
      "epoch no.1 train no.137030  loss = 2.40190 avg_loss = 3.80011\n",
      "epoch no.1 train no.137040  loss = 3.81294 avg_loss = 3.72118\n",
      "epoch no.1 train no.137050  loss = 4.73294 avg_loss = 3.76381\n",
      "epoch no.1 train no.137060  loss = 4.79056 avg_loss = 3.77139\n",
      "epoch no.1 train no.137070  loss = 2.84731 avg_loss = 3.71859\n",
      "epoch no.1 train no.137080  loss = 2.88734 avg_loss = 3.68402\n",
      "epoch no.1 train no.137090  loss = 4.36696 avg_loss = 3.70863\n",
      "epoch no.1 train no.137100  loss = 3.45014 avg_loss = 3.67731\n",
      "epoch no.1 train no.137110  loss = 4.93310 avg_loss = 3.66637\n",
      "epoch no.1 train no.137120  loss = 2.55203 avg_loss = 3.59939\n",
      "epoch no.1 train no.137130  loss = 2.42115 avg_loss = 3.57611\n",
      "epoch no.1 train no.137140  loss = 2.33810 avg_loss = 3.55661\n",
      "epoch no.1 train no.137150  loss = 5.02544 avg_loss = 3.60179\n",
      "epoch no.1 train no.137160  loss = 5.10611 avg_loss = 3.58704\n",
      "epoch no.1 train no.137170  loss = 2.77827 avg_loss = 3.58247\n",
      "epoch no.1 train no.137180  loss = 3.70038 avg_loss = 3.63964\n",
      "epoch no.1 train no.137190  loss = 5.73240 avg_loss = 3.63857\n",
      "epoch no.1 train no.137200  loss = 2.62879 avg_loss = 3.61543\n",
      "epoch no.1 train no.137210  loss = 3.90067 avg_loss = 3.61916\n",
      "epoch no.1 train no.137220  loss = 3.49946 avg_loss = 3.59441\n",
      "epoch no.1 train no.137230  loss = 2.33279 avg_loss = 3.53829\n",
      "epoch no.1 train no.137240  loss = 3.79133 avg_loss = 3.52771\n",
      "epoch no.1 train no.137250  loss = 5.37207 avg_loss = 3.60561\n",
      "epoch no.1 train no.137260  loss = 4.01245 avg_loss = 3.63312\n",
      "epoch no.1 train no.137270  loss = 4.74471 avg_loss = 3.64730\n",
      "epoch no.1 train no.137280  loss = 3.98139 avg_loss = 3.61658\n",
      "epoch no.1 train no.137290  loss = 2.25605 avg_loss = 3.59982\n",
      "epoch no.1 train no.137300  loss = 2.79042 avg_loss = 3.63209\n",
      "epoch no.1 train no.137310  loss = 4.22366 avg_loss = 3.70080\n",
      "epoch no.1 train no.137320  loss = 5.24239 avg_loss = 3.68512\n",
      "epoch no.1 train no.137330  loss = 3.21929 avg_loss = 3.71093\n",
      "epoch no.1 train no.137340  loss = 2.08886 avg_loss = 3.67505\n",
      "epoch no.1 train no.137350  loss = 3.78094 avg_loss = 3.63240\n",
      "epoch no.1 train no.137360  loss = 6.78163 avg_loss = 3.64160\n",
      "epoch no.1 train no.137370  loss = 3.09112 avg_loss = 3.63623\n",
      "epoch no.1 train no.137380  loss = 2.39570 avg_loss = 3.57462\n",
      "epoch no.1 train no.137390  loss = 3.38280 avg_loss = 3.60935\n",
      "epoch no.1 train no.137400  loss = 5.03273 avg_loss = 3.59909\n",
      "epoch no.1 train no.137410  loss = 3.01076 avg_loss = 3.64793\n",
      "epoch no.1 train no.137420  loss = 4.27903 avg_loss = 3.63015\n",
      "epoch no.1 train no.137430  loss = 2.32055 avg_loss = 3.62383\n",
      "epoch no.1 train no.137440  loss = 5.31997 avg_loss = 3.64907\n",
      "epoch no.1 train no.137450  loss = 4.61688 avg_loss = 3.66160\n",
      "epoch no.1 train no.137460  loss = 7.42398 avg_loss = 3.69566\n",
      "epoch no.1 train no.137470  loss = 3.28274 avg_loss = 3.64567\n",
      "epoch no.1 train no.137480  loss = 2.17508 avg_loss = 3.65982\n",
      "epoch no.1 train no.137490  loss = 3.31121 avg_loss = 3.67229\n",
      "epoch no.1 train no.137500  loss = 3.72126 avg_loss = 3.73161\n",
      "epoch no.1 train no.137510  loss = 3.46734 avg_loss = 3.69073\n",
      "epoch no.1 train no.137520  loss = 2.78975 avg_loss = 3.64933\n",
      "epoch no.1 train no.137530  loss = 3.97561 avg_loss = 3.70168\n",
      "epoch no.1 train no.137540  loss = 4.00786 avg_loss = 3.71028\n",
      "epoch no.1 train no.137550  loss = 4.41711 avg_loss = 3.65902\n",
      "epoch no.1 train no.137560  loss = 4.29039 avg_loss = 3.61799\n",
      "epoch no.1 train no.137570  loss = 5.73134 avg_loss = 3.67079\n",
      "epoch no.1 train no.137580  loss = 4.40779 avg_loss = 3.67224\n",
      "epoch no.1 train no.137590  loss = 3.16529 avg_loss = 3.62670\n",
      "epoch no.1 train no.137600  loss = 3.12618 avg_loss = 3.59298\n",
      "epoch no.1 train no.137610  loss = 2.39245 avg_loss = 3.52968\n",
      "epoch no.1 train no.137620  loss = 3.64751 avg_loss = 3.56881\n",
      "epoch no.1 train no.137630  loss = 3.08439 avg_loss = 3.57297\n",
      "epoch no.1 train no.137640  loss = 3.73483 avg_loss = 3.60823\n",
      "epoch no.1 train no.137650  loss = 3.82935 avg_loss = 3.61192\n",
      "epoch no.1 train no.137660  loss = 2.99339 avg_loss = 3.60317\n",
      "epoch no.1 train no.137670  loss = 3.23775 avg_loss = 3.59538\n",
      "epoch no.1 train no.137680  loss = 2.79431 avg_loss = 3.58939\n",
      "epoch no.1 train no.137690  loss = 5.03017 avg_loss = 3.62407\n",
      "epoch no.1 train no.137700  loss = 3.56205 avg_loss = 3.66124\n",
      "epoch no.1 train no.137710  loss = 3.66728 avg_loss = 3.68839\n",
      "epoch no.1 train no.137720  loss = 3.39593 avg_loss = 3.67089\n",
      "epoch no.1 train no.137730  loss = 4.06846 avg_loss = 3.71260\n",
      "epoch no.1 train no.137740  loss = 4.14880 avg_loss = 3.68138\n",
      "epoch no.1 train no.137750  loss = 3.57952 avg_loss = 3.68986\n",
      "epoch no.1 train no.137760  loss = 5.70093 avg_loss = 3.65748\n",
      "epoch no.1 train no.137770  loss = 5.00350 avg_loss = 3.69938\n",
      "epoch no.1 train no.137780  loss = 3.68445 avg_loss = 3.65265\n",
      "epoch no.1 train no.137790  loss = 2.45579 avg_loss = 3.64989\n",
      "epoch no.1 train no.137800  loss = 6.32522 avg_loss = 3.60412\n",
      "epoch no.1 train no.137810  loss = 2.82737 avg_loss = 3.60563\n",
      "epoch no.1 train no.137820  loss = 3.55263 avg_loss = 3.56467\n",
      "epoch no.1 train no.137830  loss = 4.59043 avg_loss = 3.56010\n",
      "epoch no.1 train no.137840  loss = 2.22068 avg_loss = 3.57744\n",
      "epoch no.1 train no.137850  loss = 2.56861 avg_loss = 3.52928\n",
      "epoch no.1 train no.137860  loss = 3.04154 avg_loss = 3.53395\n",
      "epoch no.1 train no.137870  loss = 4.21273 avg_loss = 3.55979\n",
      "epoch no.1 train no.137880  loss = 3.24299 avg_loss = 3.57288\n",
      "epoch no.1 train no.137890  loss = 3.27353 avg_loss = 3.58034\n",
      "epoch no.1 train no.137900  loss = 4.09978 avg_loss = 3.58065\n",
      "epoch no.1 train no.137910  loss = 4.55545 avg_loss = 3.65913\n",
      "epoch no.1 train no.137920  loss = 4.04465 avg_loss = 3.66094\n",
      "epoch no.1 train no.137930  loss = 4.94157 avg_loss = 3.66296\n",
      "epoch no.1 train no.137940  loss = 4.10008 avg_loss = 3.69856\n",
      "epoch no.1 train no.137950  loss = 3.79600 avg_loss = 3.64866\n",
      "epoch no.1 train no.137960  loss = 3.89081 avg_loss = 3.59194\n",
      "epoch no.1 train no.137970  loss = 3.50701 avg_loss = 3.55811\n",
      "epoch no.1 train no.137980  loss = 3.29585 avg_loss = 3.53741\n",
      "epoch no.1 train no.137990  loss = 4.74558 avg_loss = 3.51915\n",
      "epoch no.1 train no.138000  loss = 3.36275 avg_loss = 3.50132\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '적인', '</s>']\n",
      "여름밤의 감성 음악</s>\n",
      "epoch no.1 train no.138010  loss = 4.50529 avg_loss = 3.54328\n",
      "epoch no.1 train no.138020  loss = 3.74443 avg_loss = 3.55664\n",
      "epoch no.1 train no.138030  loss = 4.71229 avg_loss = 3.56024\n",
      "epoch no.1 train no.138040  loss = 3.75833 avg_loss = 3.63604\n",
      "epoch no.1 train no.138050  loss = 6.62558 avg_loss = 3.70787\n",
      "epoch no.1 train no.138060  loss = 5.24506 avg_loss = 3.74622\n",
      "epoch no.1 train no.138070  loss = 2.81267 avg_loss = 3.75801\n",
      "epoch no.1 train no.138080  loss = 4.95214 avg_loss = 3.76694\n",
      "epoch no.1 train no.138090  loss = 5.12374 avg_loss = 3.78556\n",
      "epoch no.1 train no.138100  loss = 3.07186 avg_loss = 3.73709\n",
      "epoch no.1 train no.138110  loss = 5.08516 avg_loss = 3.76893\n",
      "epoch no.1 train no.138120  loss = 4.72311 avg_loss = 3.73522\n",
      "epoch no.1 train no.138130  loss = 2.38284 avg_loss = 3.70372\n",
      "epoch no.1 train no.138140  loss = 4.31263 avg_loss = 3.71553\n",
      "epoch no.1 train no.138150  loss = 4.98003 avg_loss = 3.72282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.138160  loss = 3.30758 avg_loss = 3.70997\n",
      "epoch no.1 train no.138170  loss = 3.94662 avg_loss = 3.79511\n",
      "epoch no.1 train no.138180  loss = 3.52147 avg_loss = 3.82251\n",
      "epoch no.1 train no.138190  loss = 5.82380 avg_loss = 3.81857\n",
      "epoch no.1 train no.138200  loss = 3.43252 avg_loss = 3.73511\n",
      "epoch no.1 train no.138210  loss = 4.60278 avg_loss = 3.73548\n",
      "epoch no.1 train no.138220  loss = 3.15520 avg_loss = 3.77021\n",
      "epoch no.1 train no.138230  loss = 2.92435 avg_loss = 3.73268\n",
      "epoch no.1 train no.138240  loss = 3.36829 avg_loss = 3.67882\n",
      "epoch no.1 train no.138250  loss = 2.36303 avg_loss = 3.60979\n",
      "epoch no.1 train no.138260  loss = 3.22044 avg_loss = 3.56888\n",
      "epoch no.1 train no.138270  loss = 4.01834 avg_loss = 3.56906\n",
      "epoch no.1 train no.138280  loss = 3.18113 avg_loss = 3.53486\n",
      "epoch no.1 train no.138290  loss = 2.32307 avg_loss = 3.54962\n",
      "epoch no.1 train no.138300  loss = 5.64849 avg_loss = 3.59283\n",
      "epoch no.1 train no.138310  loss = 4.23078 avg_loss = 3.58505\n",
      "epoch no.1 train no.138320  loss = 1.75479 avg_loss = 3.59016\n",
      "epoch no.1 train no.138330  loss = 1.46858 avg_loss = 3.57394\n",
      "epoch no.1 train no.138340  loss = 4.16847 avg_loss = 3.58232\n",
      "epoch no.1 train no.138350  loss = 3.27752 avg_loss = 3.54025\n",
      "epoch no.1 train no.138360  loss = 4.63414 avg_loss = 3.55445\n",
      "epoch no.1 train no.138370  loss = 4.99069 avg_loss = 3.59483\n",
      "epoch no.1 train no.138380  loss = 2.05627 avg_loss = 3.64406\n",
      "epoch no.1 train no.138390  loss = 5.96288 avg_loss = 3.63649\n",
      "epoch no.1 train no.138400  loss = 2.84009 avg_loss = 3.65362\n",
      "epoch no.1 train no.138410  loss = 2.44771 avg_loss = 3.66855\n",
      "epoch no.1 train no.138420  loss = 4.28626 avg_loss = 3.62164\n",
      "epoch no.1 train no.138430  loss = 1.56505 avg_loss = 3.61025\n",
      "epoch no.1 train no.138440  loss = 3.70695 avg_loss = 3.72604\n",
      "epoch no.1 train no.138450  loss = 5.45049 avg_loss = 3.71461\n",
      "epoch no.1 train no.138460  loss = 3.09721 avg_loss = 3.71754\n",
      "epoch no.1 train no.138470  loss = 4.11799 avg_loss = 3.69272\n",
      "epoch no.1 train no.138480  loss = 2.76930 avg_loss = 3.69227\n",
      "epoch no.1 train no.138490  loss = 5.05559 avg_loss = 3.71320\n",
      "epoch no.1 train no.138500  loss = 2.41746 avg_loss = 3.75935\n",
      "epoch no.1 train no.138510  loss = 3.60140 avg_loss = 3.74098\n",
      "epoch no.1 train no.138520  loss = 2.04562 avg_loss = 3.70846\n",
      "epoch no.1 train no.138530  loss = 3.19093 avg_loss = 3.71831\n",
      "epoch no.1 train no.138540  loss = 5.27826 avg_loss = 3.77755\n",
      "epoch no.1 train no.138550  loss = 1.85339 avg_loss = 3.70857\n",
      "epoch no.1 train no.138560  loss = 4.03577 avg_loss = 3.64421\n",
      "epoch no.1 train no.138570  loss = 2.58774 avg_loss = 3.59175\n",
      "epoch no.1 train no.138580  loss = 2.46593 avg_loss = 3.59320\n",
      "epoch no.1 train no.138590  loss = 2.39651 avg_loss = 3.57683\n",
      "epoch no.1 train no.138600  loss = 4.14708 avg_loss = 3.59315\n",
      "epoch no.1 train no.138610  loss = 3.26278 avg_loss = 3.61653\n",
      "epoch no.1 train no.138620  loss = 3.60997 avg_loss = 3.60599\n",
      "epoch no.1 train no.138630  loss = 4.87899 avg_loss = 3.64759\n",
      "epoch no.1 train no.138640  loss = 5.14005 avg_loss = 3.71116\n",
      "epoch no.1 train no.138650  loss = 4.90553 avg_loss = 3.70121\n",
      "epoch no.1 train no.138660  loss = 2.28724 avg_loss = 3.65235\n",
      "epoch no.1 train no.138670  loss = 3.98593 avg_loss = 3.66204\n",
      "epoch no.1 train no.138680  loss = 6.13266 avg_loss = 3.69800\n",
      "epoch no.1 train no.138690  loss = 1.85573 avg_loss = 3.66709\n",
      "epoch no.1 train no.138700  loss = 3.46915 avg_loss = 3.70756\n",
      "epoch no.1 train no.138710  loss = 4.51369 avg_loss = 3.65818\n",
      "epoch no.1 train no.138720  loss = 2.59407 avg_loss = 3.71767\n",
      "epoch no.1 train no.138730  loss = 2.55309 avg_loss = 3.70924\n",
      "epoch no.1 train no.138740  loss = 3.84629 avg_loss = 3.71169\n",
      "epoch no.1 train no.138750  loss = 5.16727 avg_loss = 3.78787\n",
      "epoch no.1 train no.138760  loss = 3.99407 avg_loss = 3.76902\n",
      "epoch no.1 train no.138770  loss = 4.06531 avg_loss = 3.72624\n",
      "epoch no.1 train no.138780  loss = 3.00759 avg_loss = 3.70182\n",
      "epoch no.1 train no.138790  loss = 4.99561 avg_loss = 3.70000\n",
      "epoch no.1 train no.138800  loss = 2.94478 avg_loss = 3.69139\n",
      "epoch no.1 train no.138810  loss = 3.19480 avg_loss = 3.70344\n",
      "epoch no.1 train no.138820  loss = 3.76283 avg_loss = 3.70642\n",
      "epoch no.1 train no.138830  loss = 2.95742 avg_loss = 3.74618\n",
      "epoch no.1 train no.138840  loss = 3.64956 avg_loss = 3.77295\n",
      "epoch no.1 train no.138850  loss = 3.45990 avg_loss = 3.79820\n",
      "epoch no.1 train no.138860  loss = 3.60004 avg_loss = 3.78798\n",
      "epoch no.1 train no.138870  loss = 2.64349 avg_loss = 3.72413\n",
      "epoch no.1 train no.138880  loss = 2.28065 avg_loss = 3.65869\n",
      "epoch no.1 train no.138890  loss = 2.88875 avg_loss = 3.66108\n",
      "epoch no.1 train no.138900  loss = 2.02846 avg_loss = 3.60506\n",
      "epoch no.1 train no.138910  loss = 3.06512 avg_loss = 3.61364\n",
      "epoch no.1 train no.138920  loss = 2.35144 avg_loss = 3.59753\n",
      "epoch no.1 train no.138930  loss = 2.62084 avg_loss = 3.61964\n",
      "epoch no.1 train no.138940  loss = 4.56441 avg_loss = 3.61334\n",
      "epoch no.1 train no.138950  loss = 5.21353 avg_loss = 3.64248\n",
      "epoch no.1 train no.138960  loss = 4.38017 avg_loss = 3.64341\n",
      "epoch no.1 train no.138970  loss = 2.03072 avg_loss = 3.62804\n",
      "epoch no.1 train no.138980  loss = 2.11359 avg_loss = 3.63613\n",
      "epoch no.1 train no.138990  loss = 2.35527 avg_loss = 3.58854\n",
      "epoch no.1 train no.139000  loss = 4.96019 avg_loss = 3.57931\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁노래', '</s>']\n",
      "여름밤에 듣는 노래</s>\n",
      "epoch no.1 train no.139010  loss = 3.90504 avg_loss = 3.62555\n",
      "epoch no.1 train no.139020  loss = 1.94754 avg_loss = 3.62201\n",
      "epoch no.1 train no.139030  loss = 5.58459 avg_loss = 3.57874\n",
      "epoch no.1 train no.139040  loss = 3.79657 avg_loss = 3.61666\n",
      "epoch no.1 train no.139050  loss = 2.96765 avg_loss = 3.61849\n",
      "epoch no.1 train no.139060  loss = 4.48336 avg_loss = 3.60257\n",
      "epoch no.1 train no.139070  loss = 4.28253 avg_loss = 3.58803\n",
      "epoch no.1 train no.139080  loss = 3.92655 avg_loss = 3.61591\n",
      "epoch no.1 train no.139090  loss = 3.39027 avg_loss = 3.59319\n",
      "epoch no.1 train no.139100  loss = 3.26750 avg_loss = 3.58726\n",
      "epoch no.1 train no.139110  loss = 4.21641 avg_loss = 3.55017\n",
      "epoch no.1 train no.139120  loss = 6.26794 avg_loss = 3.57113\n",
      "epoch no.1 train no.139130  loss = 3.31615 avg_loss = 3.57347\n",
      "epoch no.1 train no.139140  loss = 2.54133 avg_loss = 3.59331\n",
      "epoch no.1 train no.139150  loss = 1.13993 avg_loss = 3.53115\n",
      "epoch no.1 train no.139160  loss = 3.66847 avg_loss = 3.61637\n",
      "epoch no.1 train no.139170  loss = 3.47186 avg_loss = 3.59022\n",
      "epoch no.1 train no.139180  loss = 2.76859 avg_loss = 3.61730\n",
      "epoch no.1 train no.139190  loss = 3.07076 avg_loss = 3.54814\n",
      "epoch no.1 train no.139200  loss = 6.06775 avg_loss = 3.54422\n",
      "epoch no.1 train no.139210  loss = 5.88387 avg_loss = 3.62292\n",
      "epoch no.1 train no.139220  loss = 3.35903 avg_loss = 3.62179\n",
      "epoch no.1 train no.139230  loss = 2.20223 avg_loss = 3.59147\n",
      "epoch no.1 train no.139240  loss = 4.24406 avg_loss = 3.55556\n",
      "epoch no.1 train no.139250  loss = 3.27365 avg_loss = 3.62048\n",
      "epoch no.1 train no.139260  loss = 2.39403 avg_loss = 3.61246\n",
      "epoch no.1 train no.139270  loss = 5.58491 avg_loss = 3.63071\n",
      "epoch no.1 train no.139280  loss = 5.74985 avg_loss = 3.61496\n",
      "epoch no.1 train no.139290  loss = 3.19326 avg_loss = 3.62978\n",
      "epoch no.1 train no.139300  loss = 5.60458 avg_loss = 3.67570\n",
      "epoch no.1 train no.139310  loss = 1.97038 avg_loss = 3.67767\n",
      "epoch no.1 train no.139320  loss = 3.84655 avg_loss = 3.68003\n",
      "epoch no.1 train no.139330  loss = 3.37020 avg_loss = 3.72060\n",
      "epoch no.1 train no.139340  loss = 2.69233 avg_loss = 3.72660\n",
      "epoch no.1 train no.139350  loss = 2.70505 avg_loss = 3.70006\n",
      "epoch no.1 train no.139360  loss = 2.52994 avg_loss = 3.67509\n",
      "epoch no.1 train no.139370  loss = 4.22739 avg_loss = 3.68505\n",
      "epoch no.1 train no.139380  loss = 4.69058 avg_loss = 3.74280\n",
      "epoch no.1 train no.139390  loss = 3.59215 avg_loss = 3.77236\n",
      "epoch no.1 train no.139400  loss = 4.63193 avg_loss = 3.74881\n",
      "epoch no.1 train no.139410  loss = 6.59328 avg_loss = 3.79127\n",
      "epoch no.1 train no.139420  loss = 5.09652 avg_loss = 3.81753\n",
      "epoch no.1 train no.139430  loss = 3.64339 avg_loss = 3.83179\n",
      "epoch no.1 train no.139440  loss = 1.47550 avg_loss = 3.75017\n",
      "epoch no.1 train no.139450  loss = 3.00754 avg_loss = 3.69108\n",
      "epoch no.1 train no.139460  loss = 4.03379 avg_loss = 3.68934\n",
      "epoch no.1 train no.139470  loss = 5.36983 avg_loss = 3.67922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.139480  loss = 5.43996 avg_loss = 3.71735\n",
      "epoch no.1 train no.139490  loss = 5.34083 avg_loss = 3.70359\n",
      "epoch no.1 train no.139500  loss = 2.73542 avg_loss = 3.67180\n",
      "epoch no.1 train no.139510  loss = 6.35970 avg_loss = 3.74529\n",
      "epoch no.1 train no.139520  loss = 4.42649 avg_loss = 3.78806\n",
      "epoch no.1 train no.139530  loss = 3.01097 avg_loss = 3.77023\n",
      "epoch no.1 train no.139540  loss = 2.26641 avg_loss = 3.72766\n",
      "epoch no.1 train no.139550  loss = 3.40220 avg_loss = 3.71072\n",
      "epoch no.1 train no.139560  loss = 6.21770 avg_loss = 3.76279\n",
      "epoch no.1 train no.139570  loss = 5.31126 avg_loss = 3.79155\n",
      "epoch no.1 train no.139580  loss = 6.23039 avg_loss = 3.77239\n",
      "epoch no.1 train no.139590  loss = 3.61337 avg_loss = 3.73774\n",
      "epoch no.1 train no.139600  loss = 2.80751 avg_loss = 3.66959\n",
      "epoch no.1 train no.139610  loss = 4.19578 avg_loss = 3.67651\n",
      "epoch no.1 train no.139620  loss = 5.42001 avg_loss = 3.66839\n",
      "epoch no.1 train no.139630  loss = 2.31599 avg_loss = 3.62153\n",
      "epoch no.1 train no.139640  loss = 2.50284 avg_loss = 3.66093\n",
      "epoch no.1 train no.139650  loss = 3.32848 avg_loss = 3.61936\n",
      "epoch no.1 train no.139660  loss = 4.78276 avg_loss = 3.58671\n",
      "epoch no.1 train no.139670  loss = 2.50257 avg_loss = 3.59161\n",
      "epoch no.1 train no.139680  loss = 3.66003 avg_loss = 3.56865\n",
      "epoch no.1 train no.139690  loss = 4.26064 avg_loss = 3.57438\n",
      "epoch no.1 train no.139700  loss = 2.40200 avg_loss = 3.54413\n",
      "epoch no.1 train no.139710  loss = 3.69387 avg_loss = 3.54678\n",
      "epoch no.1 train no.139720  loss = 2.85435 avg_loss = 3.53093\n",
      "epoch no.1 train no.139730  loss = 3.22386 avg_loss = 3.48783\n",
      "epoch no.1 train no.139740  loss = 3.55819 avg_loss = 3.51405\n",
      "epoch no.1 train no.139750  loss = 3.15163 avg_loss = 3.54386\n",
      "epoch no.1 train no.139760  loss = 3.66034 avg_loss = 3.55571\n",
      "epoch no.1 train no.139770  loss = 3.29627 avg_loss = 3.53979\n",
      "epoch no.1 train no.139780  loss = 5.56130 avg_loss = 3.59155\n",
      "epoch no.1 train no.139790  loss = 4.09286 avg_loss = 3.64894\n",
      "epoch no.1 train no.139800  loss = 2.29749 avg_loss = 3.72983\n",
      "epoch no.1 train no.139810  loss = 5.08150 avg_loss = 3.70453\n",
      "epoch no.1 train no.139820  loss = 1.93917 avg_loss = 3.68487\n",
      "epoch no.1 train no.139830  loss = 2.37495 avg_loss = 3.67919\n",
      "epoch no.1 train no.139840  loss = 2.83681 avg_loss = 3.68295\n",
      "epoch no.1 train no.139850  loss = 4.64828 avg_loss = 3.67818\n",
      "epoch no.1 train no.139860  loss = 2.75922 avg_loss = 3.63905\n",
      "epoch no.1 train no.139870  loss = 4.22405 avg_loss = 3.60296\n",
      "epoch no.1 train no.139880  loss = 1.94682 avg_loss = 3.61783\n",
      "epoch no.1 train no.139890  loss = 2.59159 avg_loss = 3.65961\n",
      "epoch no.1 train no.139900  loss = 4.49379 avg_loss = 3.69018\n",
      "epoch no.1 train no.139910  loss = 4.14298 avg_loss = 3.70010\n",
      "epoch no.1 train no.139920  loss = 3.70544 avg_loss = 3.72042\n",
      "epoch no.1 train no.139930  loss = 5.17334 avg_loss = 3.71050\n",
      "epoch no.1 train no.139940  loss = 3.11289 avg_loss = 3.70444\n",
      "epoch no.1 train no.139950  loss = 3.13403 avg_loss = 3.71399\n",
      "epoch no.1 train no.139960  loss = 3.38218 avg_loss = 3.74151\n",
      "epoch no.1 train no.139970  loss = 4.86045 avg_loss = 3.75549\n",
      "epoch no.1 train no.139980  loss = 5.78208 avg_loss = 3.74701\n",
      "epoch no.1 train no.139990  loss = 3.13788 avg_loss = 3.70205\n",
      "epoch no.1 train no.140000  loss = 3.68704 avg_loss = 3.75131\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 재즈</s>\n",
      "epoch no.1 train no.140010  loss = 6.36522 avg_loss = 3.73438\n",
      "epoch no.1 train no.140020  loss = 3.92803 avg_loss = 3.68136\n",
      "epoch no.1 train no.140030  loss = 6.37107 avg_loss = 3.69218\n",
      "epoch no.1 train no.140040  loss = 3.04704 avg_loss = 3.63575\n",
      "epoch no.1 train no.140050  loss = 2.85067 avg_loss = 3.57242\n",
      "epoch no.1 train no.140060  loss = 3.32738 avg_loss = 3.62455\n",
      "epoch no.1 train no.140070  loss = 2.42669 avg_loss = 3.67766\n",
      "epoch no.1 train no.140080  loss = 4.06915 avg_loss = 3.69372\n",
      "epoch no.1 train no.140090  loss = 4.68531 avg_loss = 3.65601\n",
      "epoch no.1 train no.140100  loss = 2.85928 avg_loss = 3.64512\n",
      "epoch no.1 train no.140110  loss = 4.31403 avg_loss = 3.63482\n",
      "epoch no.1 train no.140120  loss = 3.56015 avg_loss = 3.63657\n",
      "epoch no.1 train no.140130  loss = 4.38944 avg_loss = 3.63266\n",
      "epoch no.1 train no.140140  loss = 2.84991 avg_loss = 3.64502\n",
      "epoch no.1 train no.140150  loss = 3.50038 avg_loss = 3.62328\n",
      "epoch no.1 train no.140160  loss = 3.34580 avg_loss = 3.61630\n",
      "epoch no.1 train no.140170  loss = 3.25847 avg_loss = 3.61674\n",
      "epoch no.1 train no.140180  loss = 4.32593 avg_loss = 3.58886\n",
      "epoch no.1 train no.140190  loss = 2.40954 avg_loss = 3.56724\n",
      "epoch no.1 train no.140200  loss = 4.35705 avg_loss = 3.58862\n",
      "epoch no.1 train no.140210  loss = 2.92728 avg_loss = 3.60084\n",
      "epoch no.1 train no.140220  loss = 4.92528 avg_loss = 3.57492\n",
      "epoch no.1 train no.140230  loss = 4.03946 avg_loss = 3.57340\n",
      "epoch no.1 train no.140240  loss = 6.90377 avg_loss = 3.55315\n",
      "epoch no.1 train no.140250  loss = 3.92850 avg_loss = 3.55729\n",
      "epoch no.1 train no.140260  loss = 3.48892 avg_loss = 3.54531\n",
      "epoch no.1 train no.140270  loss = 7.20935 avg_loss = 3.57612\n",
      "epoch no.1 train no.140280  loss = 3.43454 avg_loss = 3.59834\n",
      "epoch no.1 train no.140290  loss = 5.27365 avg_loss = 3.60136\n",
      "epoch no.1 train no.140300  loss = 2.53967 avg_loss = 3.58235\n",
      "epoch no.1 train no.140310  loss = 4.47966 avg_loss = 3.64610\n",
      "epoch no.1 train no.140320  loss = 7.82654 avg_loss = 3.67781\n",
      "epoch no.1 train no.140330  loss = 3.36413 avg_loss = 3.64033\n",
      "epoch no.1 train no.140340  loss = 2.99035 avg_loss = 3.64171\n",
      "epoch no.1 train no.140350  loss = 3.92137 avg_loss = 3.59567\n",
      "epoch no.1 train no.140360  loss = 3.59807 avg_loss = 3.57696\n",
      "epoch no.1 train no.140370  loss = 4.15700 avg_loss = 3.56513\n",
      "epoch no.1 train no.140380  loss = 4.78421 avg_loss = 3.59930\n",
      "epoch no.1 train no.140390  loss = 5.79258 avg_loss = 3.62211\n",
      "epoch no.1 train no.140400  loss = 3.70503 avg_loss = 3.56918\n",
      "epoch no.1 train no.140410  loss = 2.60454 avg_loss = 3.59574\n",
      "epoch no.1 train no.140420  loss = 1.89470 avg_loss = 3.57275\n",
      "epoch no.1 train no.140430  loss = 3.39994 avg_loss = 3.56476\n",
      "epoch no.1 train no.140440  loss = 4.39849 avg_loss = 3.56953\n",
      "epoch no.1 train no.140450  loss = 5.12439 avg_loss = 3.58490\n",
      "epoch no.1 train no.140460  loss = 3.17953 avg_loss = 3.63100\n",
      "epoch no.1 train no.140470  loss = 2.38723 avg_loss = 3.66426\n",
      "epoch no.1 train no.140480  loss = 2.94843 avg_loss = 3.59954\n",
      "epoch no.1 train no.140490  loss = 2.31202 avg_loss = 3.51953\n",
      "epoch no.1 train no.140500  loss = 5.37645 avg_loss = 3.54116\n",
      "epoch no.1 train no.140510  loss = 3.64943 avg_loss = 3.58460\n",
      "epoch no.1 train no.140520  loss = 4.80557 avg_loss = 3.56573\n",
      "epoch no.1 train no.140530  loss = 3.06627 avg_loss = 3.60801\n",
      "epoch no.1 train no.140540  loss = 1.91013 avg_loss = 3.55758\n",
      "epoch no.1 train no.140550  loss = 4.08547 avg_loss = 3.53899\n",
      "epoch no.1 train no.140560  loss = 3.17930 avg_loss = 3.53034\n",
      "epoch no.1 train no.140570  loss = 3.04684 avg_loss = 3.58179\n",
      "epoch no.1 train no.140580  loss = 3.61471 avg_loss = 3.62636\n",
      "epoch no.1 train no.140590  loss = 3.03006 avg_loss = 3.62397\n",
      "epoch no.1 train no.140600  loss = 3.79328 avg_loss = 3.63837\n",
      "epoch no.1 train no.140610  loss = 3.20350 avg_loss = 3.61969\n",
      "epoch no.1 train no.140620  loss = 4.74538 avg_loss = 3.64187\n",
      "epoch no.1 train no.140630  loss = 5.13103 avg_loss = 3.66037\n",
      "epoch no.1 train no.140640  loss = 3.07858 avg_loss = 3.68836\n",
      "epoch no.1 train no.140650  loss = 4.61992 avg_loss = 3.68857\n",
      "epoch no.1 train no.140660  loss = 4.05506 avg_loss = 3.65917\n",
      "epoch no.1 train no.140670  loss = 2.90847 avg_loss = 3.67211\n",
      "epoch no.1 train no.140680  loss = 1.75875 avg_loss = 3.63826\n",
      "epoch no.1 train no.140690  loss = 4.54501 avg_loss = 3.67527\n",
      "epoch no.1 train no.140700  loss = 6.54613 avg_loss = 3.76362\n",
      "epoch no.1 train no.140710  loss = 6.19515 avg_loss = 3.81953\n",
      "epoch no.1 train no.140720  loss = 3.61000 avg_loss = 3.80063\n",
      "epoch no.1 train no.140730  loss = 2.90690 avg_loss = 3.77291\n",
      "epoch no.1 train no.140740  loss = 4.79091 avg_loss = 3.77328\n",
      "epoch no.1 train no.140750  loss = 5.56808 avg_loss = 3.76231\n",
      "epoch no.1 train no.140760  loss = 3.13915 avg_loss = 3.73141\n",
      "epoch no.1 train no.140770  loss = 4.53075 avg_loss = 3.78462\n",
      "epoch no.1 train no.140780  loss = 2.32909 avg_loss = 3.78450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.140790  loss = 3.35544 avg_loss = 3.79133\n",
      "epoch no.1 train no.140800  loss = 2.92340 avg_loss = 3.71823\n",
      "epoch no.1 train no.140810  loss = 3.29160 avg_loss = 3.71199\n",
      "epoch no.1 train no.140820  loss = 3.81274 avg_loss = 3.65833\n",
      "epoch no.1 train no.140830  loss = 2.66825 avg_loss = 3.63809\n",
      "epoch no.1 train no.140840  loss = 2.71521 avg_loss = 3.59583\n",
      "epoch no.1 train no.140850  loss = 2.69493 avg_loss = 3.63089\n",
      "epoch no.1 train no.140860  loss = 3.76933 avg_loss = 3.60526\n",
      "epoch no.1 train no.140870  loss = 3.83067 avg_loss = 3.58049\n",
      "epoch no.1 train no.140880  loss = 3.87826 avg_loss = 3.64199\n",
      "epoch no.1 train no.140890  loss = 4.67700 avg_loss = 3.62659\n",
      "epoch no.1 train no.140900  loss = 2.77711 avg_loss = 3.61472\n",
      "epoch no.1 train no.140910  loss = 2.03727 avg_loss = 3.65050\n",
      "epoch no.1 train no.140920  loss = 3.94997 avg_loss = 3.60748\n",
      "epoch no.1 train no.140930  loss = 3.28878 avg_loss = 3.59998\n",
      "epoch no.1 train no.140940  loss = 6.36036 avg_loss = 3.65283\n",
      "epoch no.1 train no.140950  loss = 3.64615 avg_loss = 3.61382\n",
      "epoch no.1 train no.140960  loss = 2.57306 avg_loss = 3.61610\n",
      "epoch no.1 train no.140970  loss = 2.75074 avg_loss = 3.60540\n",
      "epoch no.1 train no.140980  loss = 3.74409 avg_loss = 3.58544\n",
      "epoch no.1 train no.140990  loss = 3.77943 avg_loss = 3.58096\n",
      "epoch no.1 train no.141000  loss = 3.74105 avg_loss = 3.56635\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '한', '▁노래', '</s>']\n",
      "여름밤에 듣는 잔잔한 노래</s>\n",
      "epoch no.1 train no.141010  loss = 3.64500 avg_loss = 3.53824\n",
      "epoch no.1 train no.141020  loss = 4.15071 avg_loss = 3.56199\n",
      "epoch no.1 train no.141030  loss = 3.05199 avg_loss = 3.55636\n",
      "epoch no.1 train no.141040  loss = 4.95215 avg_loss = 3.58607\n",
      "epoch no.1 train no.141050  loss = 3.89390 avg_loss = 3.56392\n",
      "epoch no.1 train no.141060  loss = 3.76281 avg_loss = 3.56542\n",
      "epoch no.1 train no.141070  loss = 2.16115 avg_loss = 3.53760\n",
      "epoch no.1 train no.141080  loss = 2.56778 avg_loss = 3.51618\n",
      "epoch no.1 train no.141090  loss = 3.27969 avg_loss = 3.53407\n",
      "epoch no.1 train no.141100  loss = 5.54124 avg_loss = 3.56497\n",
      "epoch no.1 train no.141110  loss = 4.00155 avg_loss = 3.58167\n",
      "epoch no.1 train no.141120  loss = 2.42060 avg_loss = 3.58035\n",
      "epoch no.1 train no.141130  loss = 4.22555 avg_loss = 3.53845\n",
      "epoch no.1 train no.141140  loss = 4.54811 avg_loss = 3.55170\n",
      "epoch no.1 train no.141150  loss = 4.55849 avg_loss = 3.53453\n",
      "epoch no.1 train no.141160  loss = 3.93267 avg_loss = 3.58514\n",
      "epoch no.1 train no.141170  loss = 2.74193 avg_loss = 3.56134\n",
      "epoch no.1 train no.141180  loss = 4.30063 avg_loss = 3.56031\n",
      "epoch no.1 train no.141190  loss = 1.97007 avg_loss = 3.58429\n",
      "epoch no.1 train no.141200  loss = 5.39715 avg_loss = 3.62326\n",
      "epoch no.1 train no.141210  loss = 2.22997 avg_loss = 3.62930\n",
      "epoch no.1 train no.141220  loss = 4.39833 avg_loss = 3.62483\n",
      "epoch no.1 train no.141230  loss = 5.27960 avg_loss = 3.61702\n",
      "epoch no.1 train no.141240  loss = 3.78283 avg_loss = 3.65533\n",
      "epoch no.1 train no.141250  loss = 6.06008 avg_loss = 3.68644\n",
      "epoch no.1 train no.141260  loss = 6.29336 avg_loss = 3.70039\n",
      "epoch no.1 train no.141270  loss = 2.83338 avg_loss = 3.61746\n",
      "epoch no.1 train no.141280  loss = 2.98594 avg_loss = 3.57700\n",
      "epoch no.1 train no.141290  loss = 1.75341 avg_loss = 3.58580\n",
      "epoch no.1 train no.141300  loss = 3.38179 avg_loss = 3.60031\n",
      "epoch no.1 train no.141310  loss = 2.55926 avg_loss = 3.56413\n",
      "epoch no.1 train no.141320  loss = 3.91524 avg_loss = 3.58390\n",
      "epoch no.1 train no.141330  loss = 2.62725 avg_loss = 3.54875\n",
      "epoch no.1 train no.141340  loss = 4.88887 avg_loss = 3.55516\n",
      "epoch no.1 train no.141350  loss = 3.47775 avg_loss = 3.64443\n",
      "epoch no.1 train no.141360  loss = 2.84655 avg_loss = 3.59459\n",
      "epoch no.1 train no.141370  loss = 2.86082 avg_loss = 3.55691\n",
      "epoch no.1 train no.141380  loss = 4.39172 avg_loss = 3.55372\n",
      "epoch no.1 train no.141390  loss = 3.02043 avg_loss = 3.52804\n",
      "epoch no.1 train no.141400  loss = 3.59344 avg_loss = 3.54289\n",
      "epoch no.1 train no.141410  loss = 4.69920 avg_loss = 3.51866\n",
      "epoch no.1 train no.141420  loss = 3.45585 avg_loss = 3.53638\n",
      "epoch no.1 train no.141430  loss = 4.03889 avg_loss = 3.55783\n",
      "epoch no.1 train no.141440  loss = 4.04262 avg_loss = 3.56367\n",
      "epoch no.1 train no.141450  loss = 3.76470 avg_loss = 3.60353\n",
      "epoch no.1 train no.141460  loss = 5.23344 avg_loss = 3.59283\n",
      "epoch no.1 train no.141470  loss = 5.00141 avg_loss = 3.58986\n",
      "epoch no.1 train no.141480  loss = 5.30381 avg_loss = 3.60195\n",
      "epoch no.1 train no.141490  loss = 4.09290 avg_loss = 3.59869\n",
      "epoch no.1 train no.141500  loss = 3.67494 avg_loss = 3.62086\n",
      "epoch no.1 train no.141510  loss = 3.63279 avg_loss = 3.63473\n",
      "epoch no.1 train no.141520  loss = 2.95378 avg_loss = 3.64078\n",
      "epoch no.1 train no.141530  loss = 2.10810 avg_loss = 3.64632\n",
      "epoch no.1 train no.141540  loss = 5.16372 avg_loss = 3.69168\n",
      "epoch no.1 train no.141550  loss = 2.51313 avg_loss = 3.67128\n",
      "epoch no.1 train no.141560  loss = 3.22043 avg_loss = 3.72297\n",
      "epoch no.1 train no.141570  loss = 2.38715 avg_loss = 3.64306\n",
      "epoch no.1 train no.141580  loss = 3.32807 avg_loss = 3.64097\n",
      "epoch no.1 train no.141590  loss = 2.93847 avg_loss = 3.63572\n",
      "epoch no.1 train no.141600  loss = 4.27640 avg_loss = 3.63949\n",
      "epoch no.1 train no.141610  loss = 3.85599 avg_loss = 3.64824\n",
      "epoch no.1 train no.141620  loss = 3.34239 avg_loss = 3.71932\n",
      "epoch no.1 train no.141630  loss = 4.25921 avg_loss = 3.72389\n",
      "epoch no.1 train no.141640  loss = 4.14433 avg_loss = 3.73437\n",
      "epoch no.1 train no.141650  loss = 3.00747 avg_loss = 3.74418\n",
      "epoch no.1 train no.141660  loss = 4.85437 avg_loss = 3.77947\n",
      "epoch no.1 train no.141670  loss = 4.07597 avg_loss = 3.79631\n",
      "epoch no.1 train no.141680  loss = 3.05306 avg_loss = 3.82083\n",
      "epoch no.1 train no.141690  loss = 3.92150 avg_loss = 3.79736\n",
      "epoch no.1 train no.141700  loss = 3.05715 avg_loss = 3.83085\n",
      "epoch no.1 train no.141710  loss = 2.35036 avg_loss = 3.80575\n",
      "epoch no.1 train no.141720  loss = 4.10597 avg_loss = 3.82353\n",
      "epoch no.1 train no.141730  loss = 3.17180 avg_loss = 3.78228\n",
      "epoch no.1 train no.141740  loss = 3.65871 avg_loss = 3.74889\n",
      "epoch no.1 train no.141750  loss = 2.44676 avg_loss = 3.74930\n",
      "epoch no.1 train no.141760  loss = 2.86586 avg_loss = 3.73829\n",
      "epoch no.1 train no.141770  loss = 2.68383 avg_loss = 3.71356\n",
      "epoch no.1 train no.141780  loss = 2.81259 avg_loss = 3.67379\n",
      "epoch no.1 train no.141790  loss = 3.62780 avg_loss = 3.65283\n",
      "epoch no.1 train no.141800  loss = 1.91058 avg_loss = 3.69244\n",
      "epoch no.1 train no.141810  loss = 5.66897 avg_loss = 3.69870\n",
      "epoch no.1 train no.141820  loss = 3.47650 avg_loss = 3.65907\n",
      "epoch no.1 train no.141830  loss = 3.96673 avg_loss = 3.59351\n",
      "epoch no.1 train no.141840  loss = 3.38449 avg_loss = 3.55014\n",
      "epoch no.1 train no.141850  loss = 3.33485 avg_loss = 3.59500\n",
      "epoch no.1 train no.141860  loss = 2.51679 avg_loss = 3.58620\n",
      "epoch no.1 train no.141870  loss = 4.80391 avg_loss = 3.65301\n",
      "epoch no.1 train no.141880  loss = 4.93531 avg_loss = 3.65584\n",
      "epoch no.1 train no.141890  loss = 3.69551 avg_loss = 3.64260\n",
      "epoch no.1 train no.141900  loss = 3.70419 avg_loss = 3.64563\n",
      "epoch no.1 train no.141910  loss = 3.44496 avg_loss = 3.66284\n",
      "epoch no.1 train no.141920  loss = 3.38654 avg_loss = 3.68053\n",
      "epoch no.1 train no.141930  loss = 4.22530 avg_loss = 3.64338\n",
      "epoch no.1 train no.141940  loss = 3.99766 avg_loss = 3.64631\n",
      "epoch no.1 train no.141950  loss = 2.59541 avg_loss = 3.66417\n",
      "epoch no.1 train no.141960  loss = 3.46291 avg_loss = 3.66594\n",
      "epoch no.1 train no.141970  loss = 2.02643 avg_loss = 3.62110\n",
      "epoch no.1 train no.141980  loss = 2.56016 avg_loss = 3.61073\n",
      "epoch no.1 train no.141990  loss = 4.52353 avg_loss = 3.67730\n",
      "epoch no.1 train no.142000  loss = 2.97487 avg_loss = 3.64389\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁노래', '</s>']\n",
      "여름밤에 어울리는 노래</s>\n",
      "epoch no.1 train no.142010  loss = 3.78103 avg_loss = 3.64850\n",
      "epoch no.1 train no.142020  loss = 2.37269 avg_loss = 3.65614\n",
      "epoch no.1 train no.142030  loss = 5.69222 avg_loss = 3.67856\n",
      "epoch no.1 train no.142040  loss = 2.54158 avg_loss = 3.69521\n",
      "epoch no.1 train no.142050  loss = 3.09233 avg_loss = 3.67657\n",
      "epoch no.1 train no.142060  loss = 3.06704 avg_loss = 3.64560\n",
      "epoch no.1 train no.142070  loss = 5.80130 avg_loss = 3.66942\n",
      "epoch no.1 train no.142080  loss = 3.39402 avg_loss = 3.67327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.142090  loss = 3.10229 avg_loss = 3.66697\n",
      "epoch no.1 train no.142100  loss = 3.42685 avg_loss = 3.65293\n",
      "epoch no.1 train no.142110  loss = 4.55278 avg_loss = 3.64615\n",
      "epoch no.1 train no.142120  loss = 2.67954 avg_loss = 3.68408\n",
      "epoch no.1 train no.142130  loss = 3.56070 avg_loss = 3.73822\n",
      "epoch no.1 train no.142140  loss = 3.59992 avg_loss = 3.71714\n",
      "epoch no.1 train no.142150  loss = 2.53169 avg_loss = 3.69889\n",
      "epoch no.1 train no.142160  loss = 2.80150 avg_loss = 3.65947\n",
      "epoch no.1 train no.142170  loss = 5.03953 avg_loss = 3.74312\n",
      "epoch no.1 train no.142180  loss = 2.55476 avg_loss = 3.74397\n",
      "epoch no.1 train no.142190  loss = 2.62704 avg_loss = 3.69991\n",
      "epoch no.1 train no.142200  loss = 3.32730 avg_loss = 3.71249\n",
      "epoch no.1 train no.142210  loss = 2.40883 avg_loss = 3.70619\n",
      "epoch no.1 train no.142220  loss = 2.03784 avg_loss = 3.68874\n",
      "epoch no.1 train no.142230  loss = 1.91177 avg_loss = 3.69853\n",
      "epoch no.1 train no.142240  loss = 3.53789 avg_loss = 3.73166\n",
      "epoch no.1 train no.142250  loss = 3.31832 avg_loss = 3.75191\n",
      "epoch no.1 train no.142260  loss = 4.46948 avg_loss = 3.76648\n",
      "epoch no.1 train no.142270  loss = 3.52557 avg_loss = 3.75686\n",
      "epoch no.1 train no.142280  loss = 1.72345 avg_loss = 3.71911\n",
      "epoch no.1 train no.142290  loss = 5.54616 avg_loss = 3.75878\n",
      "epoch no.1 train no.142300  loss = 4.06189 avg_loss = 3.69288\n",
      "epoch no.1 train no.142310  loss = 4.62928 avg_loss = 3.68392\n",
      "epoch no.1 train no.142320  loss = 4.38341 avg_loss = 3.70738\n",
      "epoch no.1 train no.142330  loss = 2.61806 avg_loss = 3.67696\n",
      "epoch no.1 train no.142340  loss = 2.87168 avg_loss = 3.70387\n",
      "epoch no.1 train no.142350  loss = 4.09088 avg_loss = 3.67037\n",
      "epoch no.1 train no.142360  loss = 3.29642 avg_loss = 3.69780\n",
      "epoch no.1 train no.142370  loss = 2.98889 avg_loss = 3.67782\n",
      "epoch no.1 train no.142380  loss = 5.17463 avg_loss = 3.69382\n",
      "epoch no.1 train no.142390  loss = 3.10396 avg_loss = 3.70658\n",
      "epoch no.1 train no.142400  loss = 4.72317 avg_loss = 3.73079\n",
      "epoch no.1 train no.142410  loss = 5.62215 avg_loss = 3.70477\n",
      "epoch no.1 train no.142420  loss = 2.85063 avg_loss = 3.72128\n",
      "epoch no.1 train no.142430  loss = 3.23354 avg_loss = 3.72996\n",
      "epoch no.1 train no.142440  loss = 4.65219 avg_loss = 3.76556\n",
      "epoch no.1 train no.142450  loss = 3.28127 avg_loss = 3.77131\n",
      "epoch no.1 train no.142460  loss = 3.01380 avg_loss = 3.80971\n",
      "epoch no.1 train no.142470  loss = 7.22027 avg_loss = 3.82994\n",
      "epoch no.1 train no.142480  loss = 2.12308 avg_loss = 3.77737\n",
      "epoch no.1 train no.142490  loss = 3.43430 avg_loss = 3.75253\n",
      "epoch no.1 train no.142500  loss = 1.51838 avg_loss = 3.73345\n",
      "epoch no.1 train no.142510  loss = 3.21862 avg_loss = 3.67298\n",
      "epoch no.1 train no.142520  loss = 3.38132 avg_loss = 3.68835\n",
      "epoch no.1 train no.142530  loss = 2.11891 avg_loss = 3.68202\n",
      "epoch no.1 train no.142540  loss = 4.41747 avg_loss = 3.72660\n",
      "epoch no.1 train no.142550  loss = 4.50385 avg_loss = 3.71707\n",
      "epoch no.1 train no.142560  loss = 3.52034 avg_loss = 3.68441\n",
      "epoch no.1 train no.142570  loss = 3.71742 avg_loss = 3.63872\n",
      "epoch no.1 train no.142580  loss = 3.20127 avg_loss = 3.58452\n",
      "epoch no.1 train no.142590  loss = 4.40902 avg_loss = 3.57021\n",
      "epoch no.1 train no.142600  loss = 6.26422 avg_loss = 3.66084\n",
      "epoch no.1 train no.142610  loss = 5.75359 avg_loss = 3.71541\n",
      "epoch no.1 train no.142620  loss = 2.91400 avg_loss = 3.74124\n",
      "epoch no.1 train no.142630  loss = 3.89911 avg_loss = 3.74715\n",
      "epoch no.1 train no.142640  loss = 6.00026 avg_loss = 3.77302\n",
      "epoch no.1 train no.142650  loss = 3.00908 avg_loss = 3.74996\n",
      "epoch no.1 train no.142660  loss = 3.25930 avg_loss = 3.77482\n",
      "epoch no.1 train no.142670  loss = 6.20581 avg_loss = 3.78234\n",
      "epoch no.1 train no.142680  loss = 2.26665 avg_loss = 3.71357\n",
      "epoch no.1 train no.142690  loss = 3.49806 avg_loss = 3.68889\n",
      "epoch no.1 train no.142700  loss = 5.07848 avg_loss = 3.68112\n",
      "epoch no.1 train no.142710  loss = 4.09926 avg_loss = 3.64994\n",
      "epoch no.1 train no.142720  loss = 3.79917 avg_loss = 3.64591\n",
      "epoch no.1 train no.142730  loss = 4.35397 avg_loss = 3.66992\n",
      "epoch no.1 train no.142740  loss = 3.91053 avg_loss = 3.68154\n",
      "epoch no.1 train no.142750  loss = 5.96113 avg_loss = 3.70808\n",
      "epoch no.1 train no.142760  loss = 3.49023 avg_loss = 3.72567\n",
      "epoch no.1 train no.142770  loss = 2.64260 avg_loss = 3.73251\n",
      "epoch no.1 train no.142780  loss = 4.07896 avg_loss = 3.76697\n",
      "epoch no.1 train no.142790  loss = 3.34814 avg_loss = 3.72119\n",
      "epoch no.1 train no.142800  loss = 3.13139 avg_loss = 3.72352\n",
      "epoch no.1 train no.142810  loss = 2.34741 avg_loss = 3.69919\n",
      "epoch no.1 train no.142820  loss = 3.01675 avg_loss = 3.77229\n",
      "epoch no.1 train no.142830  loss = 2.07643 avg_loss = 3.66958\n",
      "epoch no.1 train no.142840  loss = 4.09178 avg_loss = 3.64447\n",
      "epoch no.1 train no.142850  loss = 2.11683 avg_loss = 3.62099\n",
      "epoch no.1 train no.142860  loss = 4.37592 avg_loss = 3.60472\n",
      "epoch no.1 train no.142870  loss = 2.55679 avg_loss = 3.64214\n",
      "epoch no.1 train no.142880  loss = 3.47436 avg_loss = 3.61315\n",
      "epoch no.1 train no.142890  loss = 3.06646 avg_loss = 3.65994\n",
      "epoch no.1 train no.142900  loss = 2.65489 avg_loss = 3.67319\n",
      "epoch no.1 train no.142910  loss = 3.22050 avg_loss = 3.67448\n",
      "epoch no.1 train no.142920  loss = 4.20169 avg_loss = 3.68021\n",
      "epoch no.1 train no.142930  loss = 3.98053 avg_loss = 3.62601\n",
      "epoch no.1 train no.142940  loss = 1.97944 avg_loss = 3.57819\n",
      "epoch no.1 train no.142950  loss = 2.09904 avg_loss = 3.56382\n",
      "epoch no.1 train no.142960  loss = 2.37545 avg_loss = 3.55532\n",
      "epoch no.1 train no.142970  loss = 2.66883 avg_loss = 3.54557\n",
      "epoch no.1 train no.142980  loss = 3.44771 avg_loss = 3.58953\n",
      "epoch no.1 train no.142990  loss = 5.59929 avg_loss = 3.59492\n",
      "epoch no.1 train no.143000  loss = 3.35346 avg_loss = 3.56453\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁가기', '전에', '▁듣는', '으면', '▁좋을', '▁노래', '</s>']\n",
      "여름이 가기전에 들으면 좋은 노래</s>\n",
      "epoch no.1 train no.143010  loss = 4.81512 avg_loss = 3.57120\n",
      "epoch no.1 train no.143020  loss = 4.55605 avg_loss = 3.61632\n",
      "epoch no.1 train no.143030  loss = 4.09350 avg_loss = 3.67742\n",
      "epoch no.1 train no.143040  loss = 2.56148 avg_loss = 3.63879\n",
      "epoch no.1 train no.143050  loss = 3.92979 avg_loss = 3.68860\n",
      "epoch no.1 train no.143060  loss = 3.36943 avg_loss = 3.67782\n",
      "epoch no.1 train no.143070  loss = 3.32788 avg_loss = 3.67868\n",
      "epoch no.1 train no.143080  loss = 3.62671 avg_loss = 3.68988\n",
      "epoch no.1 train no.143090  loss = 3.00282 avg_loss = 3.70520\n",
      "epoch no.1 train no.143100  loss = 4.63997 avg_loss = 3.75699\n",
      "epoch no.1 train no.143110  loss = 3.57995 avg_loss = 3.76643\n",
      "epoch no.1 train no.143120  loss = 1.68774 avg_loss = 3.73887\n",
      "epoch no.1 train no.143130  loss = 4.81299 avg_loss = 3.77225\n",
      "epoch no.1 train no.143140  loss = 4.12063 avg_loss = 3.77823\n",
      "epoch no.1 train no.143150  loss = 2.59330 avg_loss = 3.75136\n",
      "epoch no.1 train no.143160  loss = 4.56353 avg_loss = 3.71412\n",
      "epoch no.1 train no.143170  loss = 3.60047 avg_loss = 3.75380\n",
      "epoch no.1 train no.143180  loss = 3.04303 avg_loss = 3.76310\n",
      "epoch no.1 train no.143190  loss = 4.66750 avg_loss = 3.77243\n",
      "epoch no.1 train no.143200  loss = 2.71436 avg_loss = 3.69957\n",
      "epoch no.1 train no.143210  loss = 3.46094 avg_loss = 3.70309\n",
      "epoch no.1 train no.143220  loss = 2.62512 avg_loss = 3.71132\n",
      "epoch no.1 train no.143230  loss = 3.61354 avg_loss = 3.72984\n",
      "epoch no.1 train no.143240  loss = 5.39764 avg_loss = 3.74788\n",
      "epoch no.1 train no.143250  loss = 2.13325 avg_loss = 3.72646\n",
      "epoch no.1 train no.143260  loss = 4.42351 avg_loss = 3.72461\n",
      "epoch no.1 train no.143270  loss = 4.03153 avg_loss = 3.72670\n",
      "epoch no.1 train no.143280  loss = 3.41447 avg_loss = 3.72050\n",
      "epoch no.1 train no.143290  loss = 2.77339 avg_loss = 3.65887\n",
      "epoch no.1 train no.143300  loss = 6.52536 avg_loss = 3.64658\n",
      "epoch no.1 train no.143310  loss = 7.56197 avg_loss = 3.67170\n",
      "epoch no.1 train no.143320  loss = 3.14034 avg_loss = 3.67552\n",
      "epoch no.1 train no.143330  loss = 3.02370 avg_loss = 3.68392\n",
      "epoch no.1 train no.143340  loss = 3.90164 avg_loss = 3.74468\n",
      "epoch no.1 train no.143350  loss = 2.43820 avg_loss = 3.72788\n",
      "epoch no.1 train no.143360  loss = 3.98178 avg_loss = 3.75531\n",
      "epoch no.1 train no.143370  loss = 5.08973 avg_loss = 3.80308\n",
      "epoch no.1 train no.143380  loss = 2.05709 avg_loss = 3.79016\n",
      "epoch no.1 train no.143390  loss = 2.30688 avg_loss = 3.74965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.143400  loss = 3.58296 avg_loss = 3.76491\n",
      "epoch no.1 train no.143410  loss = 2.24025 avg_loss = 3.72580\n",
      "epoch no.1 train no.143420  loss = 3.89012 avg_loss = 3.70304\n",
      "epoch no.1 train no.143430  loss = 3.48784 avg_loss = 3.73834\n",
      "epoch no.1 train no.143440  loss = 5.08630 avg_loss = 3.72235\n",
      "epoch no.1 train no.143450  loss = 3.04242 avg_loss = 3.67904\n",
      "epoch no.1 train no.143460  loss = 3.90939 avg_loss = 3.67113\n",
      "epoch no.1 train no.143470  loss = 3.40712 avg_loss = 3.74006\n",
      "epoch no.1 train no.143480  loss = 4.95481 avg_loss = 3.78535\n",
      "epoch no.1 train no.143490  loss = 3.04248 avg_loss = 3.73400\n",
      "epoch no.1 train no.143500  loss = 6.09916 avg_loss = 3.78690\n",
      "epoch no.1 train no.143510  loss = 3.11306 avg_loss = 3.85885\n",
      "epoch no.1 train no.143520  loss = 3.08049 avg_loss = 3.80636\n",
      "epoch no.1 train no.143530  loss = 1.68582 avg_loss = 3.82567\n",
      "epoch no.1 train no.143540  loss = 4.78319 avg_loss = 3.84135\n",
      "epoch no.1 train no.143550  loss = 2.92720 avg_loss = 3.76938\n",
      "epoch no.1 train no.143560  loss = 2.67989 avg_loss = 3.70614\n",
      "epoch no.1 train no.143570  loss = 4.15583 avg_loss = 3.68864\n",
      "epoch no.1 train no.143580  loss = 2.40835 avg_loss = 3.70616\n",
      "epoch no.1 train no.143590  loss = 4.37685 avg_loss = 3.70510\n",
      "epoch no.1 train no.143600  loss = 2.99229 avg_loss = 3.69926\n",
      "epoch no.1 train no.143610  loss = 3.60088 avg_loss = 3.75016\n",
      "epoch no.1 train no.143620  loss = 4.05722 avg_loss = 3.75022\n",
      "epoch no.1 train no.143630  loss = 3.06907 avg_loss = 3.71912\n",
      "epoch no.1 train no.143640  loss = 3.15593 avg_loss = 3.69591\n",
      "epoch no.1 train no.143650  loss = 6.08295 avg_loss = 3.72924\n",
      "epoch no.1 train no.143660  loss = 2.54008 avg_loss = 3.69841\n",
      "epoch no.1 train no.143670  loss = 3.20478 avg_loss = 3.70982\n",
      "epoch no.1 train no.143680  loss = 5.46429 avg_loss = 3.69476\n",
      "epoch no.1 train no.143690  loss = 4.92293 avg_loss = 3.78172\n",
      "epoch no.1 train no.143700  loss = 4.61852 avg_loss = 3.75360\n",
      "epoch no.1 train no.143710  loss = 6.33290 avg_loss = 3.76074\n",
      "epoch no.1 train no.143720  loss = 2.55426 avg_loss = 3.71251\n",
      "epoch no.1 train no.143730  loss = 2.13545 avg_loss = 3.67395\n",
      "epoch no.1 train no.143740  loss = 3.01378 avg_loss = 3.70659\n",
      "epoch no.1 train no.143750  loss = 3.39123 avg_loss = 3.73082\n",
      "epoch no.1 train no.143760  loss = 5.18081 avg_loss = 3.69443\n",
      "epoch no.2 train no.143770  loss = 3.11731 avg_loss = 3.64627\n",
      "epoch no.2 train no.143780  loss = 2.83503 avg_loss = 3.61076\n",
      "epoch no.2 train no.143790  loss = 3.17022 avg_loss = 3.59565\n",
      "epoch no.2 train no.143800  loss = 2.83834 avg_loss = 3.54015\n",
      "epoch no.2 train no.143810  loss = 2.97961 avg_loss = 3.51939\n",
      "epoch no.2 train no.143820  loss = 1.76203 avg_loss = 3.51147\n",
      "epoch no.2 train no.143830  loss = 4.28090 avg_loss = 3.53231\n",
      "epoch no.2 train no.143840  loss = 2.73938 avg_loss = 3.50781\n",
      "epoch no.2 train no.143850  loss = 2.86112 avg_loss = 3.54426\n",
      "epoch no.2 train no.143860  loss = 2.97218 avg_loss = 3.56470\n",
      "epoch no.2 train no.143870  loss = 2.46603 avg_loss = 3.54212\n",
      "epoch no.2 train no.143880  loss = 2.91196 avg_loss = 3.50788\n",
      "epoch no.2 train no.143890  loss = 3.86264 avg_loss = 3.52627\n",
      "epoch no.2 train no.143900  loss = 3.85230 avg_loss = 3.49755\n",
      "epoch no.2 train no.143910  loss = 3.85956 avg_loss = 3.46828\n",
      "epoch no.2 train no.143920  loss = 3.08731 avg_loss = 3.45366\n",
      "epoch no.2 train no.143930  loss = 2.63215 avg_loss = 3.47447\n",
      "epoch no.2 train no.143940  loss = 4.32890 avg_loss = 3.44969\n",
      "epoch no.2 train no.143950  loss = 5.06382 avg_loss = 3.44818\n",
      "epoch no.2 train no.143960  loss = 2.84629 avg_loss = 3.45582\n",
      "epoch no.2 train no.143970  loss = 2.36397 avg_loss = 3.44958\n",
      "epoch no.2 train no.143980  loss = 1.66583 avg_loss = 3.41635\n",
      "epoch no.2 train no.143990  loss = 2.94394 avg_loss = 3.36655\n",
      "epoch no.2 train no.144000  loss = 4.66351 avg_loss = 3.37291\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '적인', '리스트', '</s>']\n",
      "여름밤에 듣는 감성 플레이리스트</s>\n",
      "epoch no.2 train no.144010  loss = 3.07781 avg_loss = 3.35478\n",
      "epoch no.2 train no.144020  loss = 4.56243 avg_loss = 3.34369\n",
      "epoch no.2 train no.144030  loss = 3.21054 avg_loss = 3.37992\n",
      "epoch no.2 train no.144040  loss = 6.31541 avg_loss = 3.39053\n",
      "epoch no.2 train no.144050  loss = 3.78620 avg_loss = 3.36468\n",
      "epoch no.2 train no.144060  loss = 4.05057 avg_loss = 3.32955\n",
      "epoch no.2 train no.144070  loss = 2.95146 avg_loss = 3.30919\n",
      "epoch no.2 train no.144080  loss = 5.70826 avg_loss = 3.33985\n",
      "epoch no.2 train no.144090  loss = 4.20302 avg_loss = 3.32081\n",
      "epoch no.2 train no.144100  loss = 3.16042 avg_loss = 3.30162\n",
      "epoch no.2 train no.144110  loss = 1.94756 avg_loss = 3.26608\n",
      "epoch no.2 train no.144120  loss = 3.83261 avg_loss = 3.30101\n",
      "epoch no.2 train no.144130  loss = 4.23807 avg_loss = 3.30977\n",
      "epoch no.2 train no.144140  loss = 2.91074 avg_loss = 3.34507\n",
      "epoch no.2 train no.144150  loss = 2.73407 avg_loss = 3.30813\n",
      "epoch no.2 train no.144160  loss = 3.78609 avg_loss = 3.28674\n",
      "epoch no.2 train no.144170  loss = 2.30179 avg_loss = 3.29133\n",
      "epoch no.2 train no.144180  loss = 3.46651 avg_loss = 3.35126\n",
      "epoch no.2 train no.144190  loss = 5.44345 avg_loss = 3.34194\n",
      "epoch no.2 train no.144200  loss = 4.12561 avg_loss = 3.32297\n",
      "epoch no.2 train no.144210  loss = 4.39166 avg_loss = 3.37868\n",
      "epoch no.2 train no.144220  loss = 4.00648 avg_loss = 3.39853\n",
      "epoch no.2 train no.144230  loss = 3.02384 avg_loss = 3.35337\n",
      "epoch no.2 train no.144240  loss = 2.74877 avg_loss = 3.37520\n",
      "epoch no.2 train no.144250  loss = 3.69244 avg_loss = 3.39019\n",
      "epoch no.2 train no.144260  loss = 7.15207 avg_loss = 3.42929\n",
      "epoch no.2 train no.144270  loss = 5.60842 avg_loss = 3.52196\n",
      "epoch no.2 train no.144280  loss = 4.31597 avg_loss = 3.50539\n",
      "epoch no.2 train no.144290  loss = 3.95579 avg_loss = 3.59673\n",
      "epoch no.2 train no.144300  loss = 2.78434 avg_loss = 3.60749\n",
      "epoch no.2 train no.144310  loss = 3.75106 avg_loss = 3.58970\n",
      "epoch no.2 train no.144320  loss = 3.60766 avg_loss = 3.53314\n",
      "epoch no.2 train no.144330  loss = 5.08774 avg_loss = 3.55983\n",
      "epoch no.2 train no.144340  loss = 3.99323 avg_loss = 3.50773\n",
      "epoch no.2 train no.144350  loss = 3.95450 avg_loss = 3.52941\n",
      "epoch no.2 train no.144360  loss = 3.20750 avg_loss = 3.48970\n",
      "epoch no.2 train no.144370  loss = 2.88976 avg_loss = 3.48707\n",
      "epoch no.2 train no.144380  loss = 4.54252 avg_loss = 3.47049\n",
      "epoch no.2 train no.144390  loss = 4.14886 avg_loss = 3.44907\n",
      "epoch no.2 train no.144400  loss = 4.42911 avg_loss = 3.44591\n",
      "epoch no.2 train no.144410  loss = 3.69556 avg_loss = 3.46314\n",
      "epoch no.2 train no.144420  loss = 3.76268 avg_loss = 3.47504\n",
      "epoch no.2 train no.144430  loss = 4.72325 avg_loss = 3.49403\n",
      "epoch no.2 train no.144440  loss = 3.03518 avg_loss = 3.49167\n",
      "epoch no.2 train no.144450  loss = 2.55225 avg_loss = 3.45650\n",
      "epoch no.2 train no.144460  loss = 3.25686 avg_loss = 3.40424\n",
      "epoch no.2 train no.144470  loss = 1.67481 avg_loss = 3.36309\n",
      "epoch no.2 train no.144480  loss = 3.34482 avg_loss = 3.35137\n",
      "epoch no.2 train no.144490  loss = 2.91461 avg_loss = 3.34596\n",
      "epoch no.2 train no.144500  loss = 2.35836 avg_loss = 3.35590\n",
      "epoch no.2 train no.144510  loss = 6.41887 avg_loss = 3.37847\n",
      "epoch no.2 train no.144520  loss = 4.00462 avg_loss = 3.37728\n",
      "epoch no.2 train no.144530  loss = 2.41894 avg_loss = 3.30231\n",
      "epoch no.2 train no.144540  loss = 2.54880 avg_loss = 3.30269\n",
      "epoch no.2 train no.144550  loss = 2.62637 avg_loss = 3.26465\n",
      "epoch no.2 train no.144560  loss = 6.39039 avg_loss = 3.30332\n",
      "epoch no.2 train no.144570  loss = 4.06761 avg_loss = 3.38061\n",
      "epoch no.2 train no.144580  loss = 3.31899 avg_loss = 3.36185\n",
      "epoch no.2 train no.144590  loss = 5.56506 avg_loss = 3.36570\n",
      "epoch no.2 train no.144600  loss = 3.31122 avg_loss = 3.33122\n",
      "epoch no.2 train no.144610  loss = 2.85225 avg_loss = 3.34303\n",
      "epoch no.2 train no.144620  loss = 3.92743 avg_loss = 3.37441\n",
      "epoch no.2 train no.144630  loss = 2.61407 avg_loss = 3.35538\n",
      "epoch no.2 train no.144640  loss = 3.95549 avg_loss = 3.36846\n",
      "epoch no.2 train no.144650  loss = 2.78908 avg_loss = 3.43335\n",
      "epoch no.2 train no.144660  loss = 2.32889 avg_loss = 3.46466\n",
      "epoch no.2 train no.144670  loss = 4.31499 avg_loss = 3.45756\n",
      "epoch no.2 train no.144680  loss = 2.86470 avg_loss = 3.44053\n",
      "epoch no.2 train no.144690  loss = 3.02705 avg_loss = 3.47607\n",
      "epoch no.2 train no.144700  loss = 3.86713 avg_loss = 3.47803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.144710  loss = 4.98671 avg_loss = 3.51763\n",
      "epoch no.2 train no.144720  loss = 3.51324 avg_loss = 3.50232\n",
      "epoch no.2 train no.144730  loss = 3.78630 avg_loss = 3.49048\n",
      "epoch no.2 train no.144740  loss = 2.88233 avg_loss = 3.48272\n",
      "epoch no.2 train no.144750  loss = 3.45204 avg_loss = 3.51988\n",
      "epoch no.2 train no.144760  loss = 2.53493 avg_loss = 3.52186\n",
      "epoch no.2 train no.144770  loss = 2.87169 avg_loss = 3.51011\n",
      "epoch no.2 train no.144780  loss = 1.83149 avg_loss = 3.48541\n",
      "epoch no.2 train no.144790  loss = 5.54535 avg_loss = 3.46501\n",
      "epoch no.2 train no.144800  loss = 3.57285 avg_loss = 3.51450\n",
      "epoch no.2 train no.144810  loss = 2.65913 avg_loss = 3.49691\n",
      "epoch no.2 train no.144820  loss = 3.18192 avg_loss = 3.49502\n",
      "epoch no.2 train no.144830  loss = 3.21434 avg_loss = 3.51479\n",
      "epoch no.2 train no.144840  loss = 3.19007 avg_loss = 3.48615\n",
      "epoch no.2 train no.144850  loss = 2.82754 avg_loss = 3.48824\n",
      "epoch no.2 train no.144860  loss = 3.38891 avg_loss = 3.45943\n",
      "epoch no.2 train no.144870  loss = 5.29657 avg_loss = 3.47275\n",
      "epoch no.2 train no.144880  loss = 4.33441 avg_loss = 3.48620\n",
      "epoch no.2 train no.144890  loss = 2.70619 avg_loss = 3.41042\n",
      "epoch no.2 train no.144900  loss = 2.76251 avg_loss = 3.40052\n",
      "epoch no.2 train no.144910  loss = 3.27127 avg_loss = 3.39060\n",
      "epoch no.2 train no.144920  loss = 4.05932 avg_loss = 3.43298\n",
      "epoch no.2 train no.144930  loss = 2.50649 avg_loss = 3.39263\n",
      "epoch no.2 train no.144940  loss = 2.63230 avg_loss = 3.37793\n",
      "epoch no.2 train no.144950  loss = 3.56228 avg_loss = 3.39961\n",
      "epoch no.2 train no.144960  loss = 2.27688 avg_loss = 3.37434\n",
      "epoch no.2 train no.144970  loss = 3.68391 avg_loss = 3.36672\n",
      "epoch no.2 train no.144980  loss = 4.03896 avg_loss = 3.39791\n",
      "epoch no.2 train no.144990  loss = 3.17782 avg_loss = 3.34423\n",
      "epoch no.2 train no.145000  loss = 2.51308 avg_loss = 3.32209\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '▁역시', '▁시원한', '▁댄스', '</s>']\n",
      "여름엔 역시 신나는 노래</s>\n",
      "epoch no.2 train no.145010  loss = 2.14166 avg_loss = 3.30044\n",
      "epoch no.2 train no.145020  loss = 3.15536 avg_loss = 3.24683\n",
      "epoch no.2 train no.145030  loss = 3.62366 avg_loss = 3.28467\n",
      "epoch no.2 train no.145040  loss = 3.36729 avg_loss = 3.38223\n",
      "epoch no.2 train no.145050  loss = 2.43302 avg_loss = 3.42550\n",
      "epoch no.2 train no.145060  loss = 4.17029 avg_loss = 3.44874\n",
      "epoch no.2 train no.145070  loss = 2.38782 avg_loss = 3.42896\n",
      "epoch no.2 train no.145080  loss = 4.71676 avg_loss = 3.47523\n",
      "epoch no.2 train no.145090  loss = 4.55325 avg_loss = 3.52508\n",
      "epoch no.2 train no.145100  loss = 2.71669 avg_loss = 3.48485\n",
      "epoch no.2 train no.145110  loss = 3.64371 avg_loss = 3.45163\n",
      "epoch no.2 train no.145120  loss = 3.83227 avg_loss = 3.46518\n",
      "epoch no.2 train no.145130  loss = 1.13674 avg_loss = 3.44255\n",
      "epoch no.2 train no.145140  loss = 4.13247 avg_loss = 3.46435\n",
      "epoch no.2 train no.145150  loss = 4.07373 avg_loss = 3.45214\n",
      "epoch no.2 train no.145160  loss = 2.41453 avg_loss = 3.42358\n",
      "epoch no.2 train no.145170  loss = 2.74242 avg_loss = 3.42497\n",
      "epoch no.2 train no.145180  loss = 1.98126 avg_loss = 3.43083\n",
      "epoch no.2 train no.145190  loss = 4.06382 avg_loss = 3.44221\n",
      "epoch no.2 train no.145200  loss = 3.28106 avg_loss = 3.45164\n",
      "epoch no.2 train no.145210  loss = 2.32532 avg_loss = 3.43964\n",
      "epoch no.2 train no.145220  loss = 3.21201 avg_loss = 3.42758\n",
      "epoch no.2 train no.145230  loss = 2.99407 avg_loss = 3.41034\n",
      "epoch no.2 train no.145240  loss = 4.30468 avg_loss = 3.39813\n",
      "epoch no.2 train no.145250  loss = 2.07331 avg_loss = 3.32781\n",
      "epoch no.2 train no.145260  loss = 1.95120 avg_loss = 3.32383\n",
      "epoch no.2 train no.145270  loss = 2.54483 avg_loss = 3.33213\n",
      "epoch no.2 train no.145280  loss = 2.80114 avg_loss = 3.25924\n",
      "epoch no.2 train no.145290  loss = 2.11118 avg_loss = 3.29443\n",
      "epoch no.2 train no.145300  loss = 2.64680 avg_loss = 3.29565\n",
      "epoch no.2 train no.145310  loss = 3.52328 avg_loss = 3.31512\n",
      "epoch no.2 train no.145320  loss = 2.89784 avg_loss = 3.32534\n",
      "epoch no.2 train no.145330  loss = 1.50588 avg_loss = 3.31486\n",
      "epoch no.2 train no.145340  loss = 2.41339 avg_loss = 3.29457\n",
      "epoch no.2 train no.145350  loss = 4.02121 avg_loss = 3.31460\n",
      "epoch no.2 train no.145360  loss = 2.92722 avg_loss = 3.33917\n",
      "epoch no.2 train no.145370  loss = 2.85784 avg_loss = 3.32569\n",
      "epoch no.2 train no.145380  loss = 4.34189 avg_loss = 3.30235\n",
      "epoch no.2 train no.145390  loss = 3.69859 avg_loss = 3.27646\n",
      "epoch no.2 train no.145400  loss = 3.10839 avg_loss = 3.27500\n",
      "epoch no.2 train no.145410  loss = 3.22525 avg_loss = 3.27370\n",
      "epoch no.2 train no.145420  loss = 3.81105 avg_loss = 3.27100\n",
      "epoch no.2 train no.145430  loss = 4.10078 avg_loss = 3.27185\n",
      "epoch no.2 train no.145440  loss = 1.90218 avg_loss = 3.25482\n",
      "epoch no.2 train no.145450  loss = 1.93381 avg_loss = 3.23156\n",
      "epoch no.2 train no.145460  loss = 3.33810 avg_loss = 3.25031\n",
      "epoch no.2 train no.145470  loss = 2.35225 avg_loss = 3.21387\n",
      "epoch no.2 train no.145480  loss = 2.57312 avg_loss = 3.20423\n",
      "epoch no.2 train no.145490  loss = 2.49592 avg_loss = 3.17454\n",
      "epoch no.2 train no.145500  loss = 4.58106 avg_loss = 3.20259\n",
      "epoch no.2 train no.145510  loss = 2.52144 avg_loss = 3.17853\n",
      "epoch no.2 train no.145520  loss = 3.14737 avg_loss = 3.20064\n",
      "epoch no.2 train no.145530  loss = 2.66701 avg_loss = 3.23282\n",
      "epoch no.2 train no.145540  loss = 4.36514 avg_loss = 3.25207\n",
      "epoch no.2 train no.145550  loss = 2.83855 avg_loss = 3.24847\n",
      "epoch no.2 train no.145560  loss = 3.34642 avg_loss = 3.27733\n",
      "epoch no.2 train no.145570  loss = 4.70919 avg_loss = 3.28901\n",
      "epoch no.2 train no.145580  loss = 4.15684 avg_loss = 3.33241\n",
      "epoch no.2 train no.145590  loss = 2.92880 avg_loss = 3.36450\n",
      "epoch no.2 train no.145600  loss = 2.10467 avg_loss = 3.35445\n",
      "epoch no.2 train no.145610  loss = 2.26019 avg_loss = 3.31941\n",
      "epoch no.2 train no.145620  loss = 3.10960 avg_loss = 3.33881\n",
      "epoch no.2 train no.145630  loss = 2.47413 avg_loss = 3.32029\n",
      "epoch no.2 train no.145640  loss = 2.23866 avg_loss = 3.27792\n",
      "epoch no.2 train no.145650  loss = 3.35177 avg_loss = 3.28390\n",
      "epoch no.2 train no.145660  loss = 2.47435 avg_loss = 3.26557\n",
      "epoch no.2 train no.145670  loss = 3.03253 avg_loss = 3.30742\n",
      "epoch no.2 train no.145680  loss = 3.72878 avg_loss = 3.28072\n",
      "epoch no.2 train no.145690  loss = 3.99011 avg_loss = 3.25906\n",
      "epoch no.2 train no.145700  loss = 3.68059 avg_loss = 3.28535\n",
      "epoch no.2 train no.145710  loss = 4.64652 avg_loss = 3.27784\n",
      "epoch no.2 train no.145720  loss = 3.97536 avg_loss = 3.25082\n",
      "epoch no.2 train no.145730  loss = 4.11180 avg_loss = 3.26207\n",
      "epoch no.2 train no.145740  loss = 3.61330 avg_loss = 3.33421\n",
      "epoch no.2 train no.145750  loss = 3.40590 avg_loss = 3.35885\n",
      "epoch no.2 train no.145760  loss = 3.09844 avg_loss = 3.37237\n",
      "epoch no.2 train no.145770  loss = 3.11013 avg_loss = 3.35986\n",
      "epoch no.2 train no.145780  loss = 1.84959 avg_loss = 3.32393\n",
      "epoch no.2 train no.145790  loss = 4.59559 avg_loss = 3.35767\n",
      "epoch no.2 train no.145800  loss = 2.95444 avg_loss = 3.35086\n",
      "epoch no.2 train no.145810  loss = 1.72981 avg_loss = 3.32894\n",
      "epoch no.2 train no.145820  loss = 3.22377 avg_loss = 3.34139\n",
      "epoch no.2 train no.145830  loss = 3.99549 avg_loss = 3.35412\n",
      "epoch no.2 train no.145840  loss = 6.37351 avg_loss = 3.40690\n",
      "epoch no.2 train no.145850  loss = 3.33862 avg_loss = 3.38491\n",
      "epoch no.2 train no.145860  loss = 3.85885 avg_loss = 3.41701\n",
      "epoch no.2 train no.145870  loss = 3.39117 avg_loss = 3.43704\n",
      "epoch no.2 train no.145880  loss = 3.17422 avg_loss = 3.44473\n",
      "epoch no.2 train no.145890  loss = 4.32263 avg_loss = 3.44998\n",
      "epoch no.2 train no.145900  loss = 3.99038 avg_loss = 3.38107\n",
      "epoch no.2 train no.145910  loss = 2.32690 avg_loss = 3.37625\n",
      "epoch no.2 train no.145920  loss = 4.02973 avg_loss = 3.36297\n",
      "epoch no.2 train no.145930  loss = 2.37897 avg_loss = 3.40195\n",
      "epoch no.2 train no.145940  loss = 5.14268 avg_loss = 3.38718\n",
      "epoch no.2 train no.145950  loss = 2.79056 avg_loss = 3.38005\n",
      "epoch no.2 train no.145960  loss = 4.32432 avg_loss = 3.37650\n",
      "epoch no.2 train no.145970  loss = 4.69132 avg_loss = 3.42838\n",
      "epoch no.2 train no.145980  loss = 3.82828 avg_loss = 3.46860\n",
      "epoch no.2 train no.145990  loss = 3.86874 avg_loss = 3.45374\n",
      "epoch no.2 train no.146000  loss = 1.97256 avg_loss = 3.45611\n",
      "4\n",
      "to_tokens: ['▁비', '엔', '▁역시', '▁시원한', '▁노래', '</s>']\n",
      "여름엔 역시 이 노래</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.146010  loss = 2.29670 avg_loss = 3.43578\n",
      "epoch no.2 train no.146020  loss = 3.76798 avg_loss = 3.45675\n",
      "epoch no.2 train no.146030  loss = 3.61344 avg_loss = 3.44488\n",
      "epoch no.2 train no.146040  loss = 3.78229 avg_loss = 3.42583\n",
      "epoch no.2 train no.146050  loss = 5.51655 avg_loss = 3.40692\n",
      "epoch no.2 train no.146060  loss = 1.76619 avg_loss = 3.43716\n",
      "epoch no.2 train no.146070  loss = 3.97827 avg_loss = 3.47442\n",
      "epoch no.2 train no.146080  loss = 3.94358 avg_loss = 3.47285\n",
      "epoch no.2 train no.146090  loss = 4.71806 avg_loss = 3.48269\n",
      "epoch no.2 train no.146100  loss = 3.43975 avg_loss = 3.47416\n",
      "epoch no.2 train no.146110  loss = 2.62820 avg_loss = 3.44854\n",
      "epoch no.2 train no.146120  loss = 5.00911 avg_loss = 3.45789\n",
      "epoch no.2 train no.146130  loss = 2.71233 avg_loss = 3.47627\n",
      "epoch no.2 train no.146140  loss = 5.32635 avg_loss = 3.48462\n",
      "epoch no.2 train no.146150  loss = 4.05381 avg_loss = 3.49114\n",
      "epoch no.2 train no.146160  loss = 4.30358 avg_loss = 3.49960\n",
      "epoch no.2 train no.146170  loss = 2.28801 avg_loss = 3.46349\n",
      "epoch no.2 train no.146180  loss = 4.11293 avg_loss = 3.48515\n",
      "epoch no.2 train no.146190  loss = 2.54505 avg_loss = 3.44790\n",
      "epoch no.2 train no.146200  loss = 2.16523 avg_loss = 3.46263\n",
      "epoch no.2 train no.146210  loss = 2.20413 avg_loss = 3.48413\n",
      "epoch no.2 train no.146220  loss = 3.52408 avg_loss = 3.42721\n",
      "epoch no.2 train no.146230  loss = 3.45353 avg_loss = 3.42453\n",
      "epoch no.2 train no.146240  loss = 2.90620 avg_loss = 3.42317\n",
      "epoch no.2 train no.146250  loss = 1.52940 avg_loss = 3.39068\n",
      "epoch no.2 train no.146260  loss = 3.99944 avg_loss = 3.38290\n",
      "epoch no.2 train no.146270  loss = 3.33624 avg_loss = 3.40463\n",
      "epoch no.2 train no.146280  loss = 1.94857 avg_loss = 3.41276\n",
      "epoch no.2 train no.146290  loss = 2.87091 avg_loss = 3.41381\n",
      "epoch no.2 train no.146300  loss = 3.83631 avg_loss = 3.39166\n",
      "epoch no.2 train no.146310  loss = 5.06401 avg_loss = 3.36176\n",
      "epoch no.2 train no.146320  loss = 3.15177 avg_loss = 3.37297\n",
      "epoch no.2 train no.146330  loss = 4.18714 avg_loss = 3.39122\n",
      "epoch no.2 train no.146340  loss = 1.99300 avg_loss = 3.36720\n",
      "epoch no.2 train no.146350  loss = 4.50807 avg_loss = 3.33932\n",
      "epoch no.2 train no.146360  loss = 2.00353 avg_loss = 3.29917\n",
      "epoch no.2 train no.146370  loss = 4.39256 avg_loss = 3.30959\n",
      "epoch no.2 train no.146380  loss = 3.37067 avg_loss = 3.34585\n",
      "epoch no.2 train no.146390  loss = 2.97201 avg_loss = 3.30860\n",
      "epoch no.2 train no.146400  loss = 1.92901 avg_loss = 3.26300\n",
      "epoch no.2 train no.146410  loss = 2.98347 avg_loss = 3.31123\n",
      "epoch no.2 train no.146420  loss = 2.87835 avg_loss = 3.29292\n",
      "epoch no.2 train no.146430  loss = 3.75460 avg_loss = 3.30332\n",
      "epoch no.2 train no.146440  loss = 2.30243 avg_loss = 3.32204\n",
      "epoch no.2 train no.146450  loss = 4.24234 avg_loss = 3.38518\n",
      "epoch no.2 train no.146460  loss = 3.63564 avg_loss = 3.38194\n",
      "epoch no.2 train no.146470  loss = 5.50161 avg_loss = 3.42308\n",
      "epoch no.2 train no.146480  loss = 3.83965 avg_loss = 3.42900\n",
      "epoch no.2 train no.146490  loss = 3.80512 avg_loss = 3.44455\n",
      "epoch no.2 train no.146500  loss = 2.77981 avg_loss = 3.40549\n",
      "epoch no.2 train no.146510  loss = 3.27973 avg_loss = 3.36164\n",
      "epoch no.2 train no.146520  loss = 3.28563 avg_loss = 3.33253\n",
      "epoch no.2 train no.146530  loss = 3.33504 avg_loss = 3.31900\n",
      "epoch no.2 train no.146540  loss = 2.42296 avg_loss = 3.29393\n",
      "epoch no.2 train no.146550  loss = 3.61587 avg_loss = 3.32719\n",
      "epoch no.2 train no.146560  loss = 3.77412 avg_loss = 3.35085\n",
      "epoch no.2 train no.146570  loss = 4.34788 avg_loss = 3.40123\n",
      "epoch no.2 train no.146580  loss = 1.67378 avg_loss = 3.43124\n",
      "epoch no.2 train no.146590  loss = 3.68095 avg_loss = 3.44467\n",
      "epoch no.2 train no.146600  loss = 5.18391 avg_loss = 3.45736\n",
      "epoch no.2 train no.146610  loss = 2.51780 avg_loss = 3.46339\n",
      "epoch no.2 train no.146620  loss = 2.34488 avg_loss = 3.46119\n",
      "epoch no.2 train no.146630  loss = 2.01087 avg_loss = 3.43634\n",
      "epoch no.2 train no.146640  loss = 1.39504 avg_loss = 3.36899\n",
      "epoch no.2 train no.146650  loss = 3.88480 avg_loss = 3.36986\n",
      "epoch no.2 train no.146660  loss = 2.90192 avg_loss = 3.41181\n",
      "epoch no.2 train no.146670  loss = 2.23095 avg_loss = 3.39934\n",
      "epoch no.2 train no.146680  loss = 2.26155 avg_loss = 3.35791\n",
      "epoch no.2 train no.146690  loss = 2.73662 avg_loss = 3.40234\n",
      "epoch no.2 train no.146700  loss = 1.70486 avg_loss = 3.43934\n",
      "epoch no.2 train no.146710  loss = 2.30541 avg_loss = 3.45093\n",
      "epoch no.2 train no.146720  loss = 4.32925 avg_loss = 3.45955\n",
      "epoch no.2 train no.146730  loss = 4.16876 avg_loss = 3.44791\n",
      "epoch no.2 train no.146740  loss = 3.00233 avg_loss = 3.37956\n",
      "epoch no.2 train no.146750  loss = 1.90906 avg_loss = 3.34905\n",
      "epoch no.2 train no.146760  loss = 2.69524 avg_loss = 3.34198\n",
      "epoch no.2 train no.146770  loss = 2.10001 avg_loss = 3.31134\n",
      "epoch no.2 train no.146780  loss = 2.49666 avg_loss = 3.32490\n",
      "epoch no.2 train no.146790  loss = 4.46098 avg_loss = 3.35411\n",
      "epoch no.2 train no.146800  loss = 4.30136 avg_loss = 3.36440\n",
      "epoch no.2 train no.146810  loss = 2.59444 avg_loss = 3.36101\n",
      "epoch no.2 train no.146820  loss = 5.02213 avg_loss = 3.42889\n",
      "epoch no.2 train no.146830  loss = 3.73797 avg_loss = 3.45811\n",
      "epoch no.2 train no.146840  loss = 3.66963 avg_loss = 3.44418\n",
      "epoch no.2 train no.146850  loss = 3.42731 avg_loss = 3.41161\n",
      "epoch no.2 train no.146860  loss = 3.01019 avg_loss = 3.38530\n",
      "epoch no.2 train no.146870  loss = 2.51671 avg_loss = 3.36451\n",
      "epoch no.2 train no.146880  loss = 1.95303 avg_loss = 3.32187\n",
      "epoch no.2 train no.146890  loss = 2.27250 avg_loss = 3.29997\n",
      "epoch no.2 train no.146900  loss = 2.64168 avg_loss = 3.27603\n",
      "epoch no.2 train no.146910  loss = 3.38546 avg_loss = 3.28886\n",
      "epoch no.2 train no.146920  loss = 4.22604 avg_loss = 3.30661\n",
      "epoch no.2 train no.146930  loss = 3.30708 avg_loss = 3.32841\n",
      "epoch no.2 train no.146940  loss = 2.99358 avg_loss = 3.35136\n",
      "epoch no.2 train no.146950  loss = 2.29883 avg_loss = 3.29258\n",
      "epoch no.2 train no.146960  loss = 3.47628 avg_loss = 3.27833\n",
      "epoch no.2 train no.146970  loss = 2.83241 avg_loss = 3.26301\n",
      "epoch no.2 train no.146980  loss = 2.12838 avg_loss = 3.25714\n",
      "epoch no.2 train no.146990  loss = 3.39613 avg_loss = 3.23758\n",
      "epoch no.2 train no.147000  loss = 3.38846 avg_loss = 3.28563\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '음악', '</s>']\n",
      "여름밤에 듣는 인디음악</s>\n",
      "epoch no.2 train no.147010  loss = 4.32713 avg_loss = 3.30716\n",
      "epoch no.2 train no.147020  loss = 5.74958 avg_loss = 3.30193\n",
      "epoch no.2 train no.147030  loss = 3.52043 avg_loss = 3.28936\n",
      "epoch no.2 train no.147040  loss = 5.41186 avg_loss = 3.33365\n",
      "epoch no.2 train no.147050  loss = 2.47193 avg_loss = 3.31436\n",
      "epoch no.2 train no.147060  loss = 3.93667 avg_loss = 3.34078\n",
      "epoch no.2 train no.147070  loss = 5.24483 avg_loss = 3.35867\n",
      "epoch no.2 train no.147080  loss = 3.04088 avg_loss = 3.36108\n",
      "epoch no.2 train no.147090  loss = 2.74305 avg_loss = 3.41118\n",
      "epoch no.2 train no.147100  loss = 3.54544 avg_loss = 3.39074\n",
      "epoch no.2 train no.147110  loss = 3.98784 avg_loss = 3.36145\n",
      "epoch no.2 train no.147120  loss = 2.46986 avg_loss = 3.32876\n",
      "epoch no.2 train no.147130  loss = 2.07268 avg_loss = 3.33432\n",
      "epoch no.2 train no.147140  loss = 3.59872 avg_loss = 3.31773\n",
      "epoch no.2 train no.147150  loss = 2.82694 avg_loss = 3.34904\n",
      "epoch no.2 train no.147160  loss = 3.59293 avg_loss = 3.37426\n",
      "epoch no.2 train no.147170  loss = 3.67205 avg_loss = 3.41611\n",
      "epoch no.2 train no.147180  loss = 1.72624 avg_loss = 3.40119\n",
      "epoch no.2 train no.147190  loss = 3.55265 avg_loss = 3.38685\n",
      "epoch no.2 train no.147200  loss = 3.27127 avg_loss = 3.42687\n",
      "epoch no.2 train no.147210  loss = 4.61993 avg_loss = 3.46800\n",
      "epoch no.2 train no.147220  loss = 3.79929 avg_loss = 3.45852\n",
      "epoch no.2 train no.147230  loss = 3.70279 avg_loss = 3.45140\n",
      "epoch no.2 train no.147240  loss = 3.17476 avg_loss = 3.44103\n",
      "epoch no.2 train no.147250  loss = 5.13852 avg_loss = 3.45841\n",
      "epoch no.2 train no.147260  loss = 2.61081 avg_loss = 3.42636\n",
      "epoch no.2 train no.147270  loss = 3.94959 avg_loss = 3.41045\n",
      "epoch no.2 train no.147280  loss = 5.64257 avg_loss = 3.40912\n",
      "epoch no.2 train no.147290  loss = 2.88371 avg_loss = 3.37843\n",
      "epoch no.2 train no.147300  loss = 2.67282 avg_loss = 3.36814\n",
      "epoch no.2 train no.147310  loss = 4.14439 avg_loss = 3.40883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.147320  loss = 5.46562 avg_loss = 3.41889\n",
      "epoch no.2 train no.147330  loss = 3.94344 avg_loss = 3.37425\n",
      "epoch no.2 train no.147340  loss = 1.55370 avg_loss = 3.34088\n",
      "epoch no.2 train no.147350  loss = 3.60688 avg_loss = 3.35321\n",
      "epoch no.2 train no.147360  loss = 4.10138 avg_loss = 3.33490\n",
      "epoch no.2 train no.147370  loss = 2.78569 avg_loss = 3.38154\n",
      "epoch no.2 train no.147380  loss = 4.01264 avg_loss = 3.33953\n",
      "epoch no.2 train no.147390  loss = 3.44465 avg_loss = 3.30818\n",
      "epoch no.2 train no.147400  loss = 3.46099 avg_loss = 3.34378\n",
      "epoch no.2 train no.147410  loss = 5.08171 avg_loss = 3.35444\n",
      "epoch no.2 train no.147420  loss = 3.30639 avg_loss = 3.32842\n",
      "epoch no.2 train no.147430  loss = 3.71581 avg_loss = 3.32242\n",
      "epoch no.2 train no.147440  loss = 2.04921 avg_loss = 3.30200\n",
      "epoch no.2 train no.147450  loss = 3.58593 avg_loss = 3.32626\n",
      "epoch no.2 train no.147460  loss = 3.99635 avg_loss = 3.33185\n",
      "epoch no.2 train no.147470  loss = 3.43629 avg_loss = 3.35353\n",
      "epoch no.2 train no.147480  loss = 4.73270 avg_loss = 3.40698\n",
      "epoch no.2 train no.147490  loss = 2.84291 avg_loss = 3.41252\n",
      "epoch no.2 train no.147500  loss = 4.72453 avg_loss = 3.40705\n",
      "epoch no.2 train no.147510  loss = 2.45718 avg_loss = 3.38778\n",
      "epoch no.2 train no.147520  loss = 4.84653 avg_loss = 3.41329\n",
      "epoch no.2 train no.147530  loss = 2.69309 avg_loss = 3.38475\n",
      "epoch no.2 train no.147540  loss = 6.12013 avg_loss = 3.41530\n",
      "epoch no.2 train no.147550  loss = 2.48410 avg_loss = 3.42288\n",
      "epoch no.2 train no.147560  loss = 3.45277 avg_loss = 3.45009\n",
      "epoch no.2 train no.147570  loss = 4.33407 avg_loss = 3.49972\n",
      "epoch no.2 train no.147580  loss = 3.52168 avg_loss = 3.50681\n",
      "epoch no.2 train no.147590  loss = 5.08813 avg_loss = 3.49594\n",
      "epoch no.2 train no.147600  loss = 4.72113 avg_loss = 3.46876\n",
      "epoch no.2 train no.147610  loss = 3.38270 avg_loss = 3.46564\n",
      "epoch no.2 train no.147620  loss = 2.55350 avg_loss = 3.43028\n",
      "epoch no.2 train no.147630  loss = 3.44703 avg_loss = 3.44812\n",
      "epoch no.2 train no.147640  loss = 3.85596 avg_loss = 3.42352\n",
      "epoch no.2 train no.147650  loss = 4.11266 avg_loss = 3.42073\n",
      "epoch no.2 train no.147660  loss = 2.86139 avg_loss = 3.44783\n",
      "epoch no.2 train no.147670  loss = 3.21343 avg_loss = 3.44531\n",
      "epoch no.2 train no.147680  loss = 3.11280 avg_loss = 3.41160\n",
      "epoch no.2 train no.147690  loss = 2.97861 avg_loss = 3.37440\n",
      "epoch no.2 train no.147700  loss = 2.29010 avg_loss = 3.30816\n",
      "epoch no.2 train no.147710  loss = 4.52212 avg_loss = 3.32815\n",
      "epoch no.2 train no.147720  loss = 4.37494 avg_loss = 3.38226\n",
      "epoch no.2 train no.147730  loss = 3.17225 avg_loss = 3.40536\n",
      "epoch no.2 train no.147740  loss = 2.21788 avg_loss = 3.35971\n",
      "epoch no.2 train no.147750  loss = 4.24549 avg_loss = 3.32776\n",
      "epoch no.2 train no.147760  loss = 4.84766 avg_loss = 3.35671\n",
      "epoch no.2 train no.147770  loss = 3.01447 avg_loss = 3.31439\n",
      "epoch no.2 train no.147780  loss = 2.81669 avg_loss = 3.35409\n",
      "epoch no.2 train no.147790  loss = 2.98861 avg_loss = 3.37964\n",
      "epoch no.2 train no.147800  loss = 4.44689 avg_loss = 3.38837\n",
      "epoch no.2 train no.147810  loss = 3.12707 avg_loss = 3.35474\n",
      "epoch no.2 train no.147820  loss = 3.78288 avg_loss = 3.34437\n",
      "epoch no.2 train no.147830  loss = 2.20739 avg_loss = 3.37024\n",
      "epoch no.2 train no.147840  loss = 2.54019 avg_loss = 3.35335\n",
      "epoch no.2 train no.147850  loss = 2.80335 avg_loss = 3.33650\n",
      "epoch no.2 train no.147860  loss = 2.17297 avg_loss = 3.38003\n",
      "epoch no.2 train no.147870  loss = 4.29843 avg_loss = 3.40416\n",
      "epoch no.2 train no.147880  loss = 3.44520 avg_loss = 3.44852\n",
      "epoch no.2 train no.147890  loss = 3.58678 avg_loss = 3.45893\n",
      "epoch no.2 train no.147900  loss = 3.96798 avg_loss = 3.46971\n",
      "epoch no.2 train no.147910  loss = 3.23097 avg_loss = 3.47390\n",
      "epoch no.2 train no.147920  loss = 2.74602 avg_loss = 3.44733\n",
      "epoch no.2 train no.147930  loss = 2.70185 avg_loss = 3.41095\n",
      "epoch no.2 train no.147940  loss = 2.55872 avg_loss = 3.34735\n",
      "epoch no.2 train no.147950  loss = 3.07520 avg_loss = 3.37593\n",
      "epoch no.2 train no.147960  loss = 2.95150 avg_loss = 3.38001\n",
      "epoch no.2 train no.147970  loss = 4.33436 avg_loss = 3.34104\n",
      "epoch no.2 train no.147980  loss = 3.16054 avg_loss = 3.32473\n",
      "epoch no.2 train no.147990  loss = 2.15195 avg_loss = 3.34244\n",
      "epoch no.2 train no.148000  loss = 3.34212 avg_loss = 3.36594\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁끝', '▁알리는', '▁시원한', '음악', '</s>']\n",
      "여름의 시작을 알리는 인디음악</s>\n",
      "epoch no.2 train no.148010  loss = 4.42343 avg_loss = 3.35806\n",
      "epoch no.2 train no.148020  loss = 4.83841 avg_loss = 3.37544\n",
      "epoch no.2 train no.148030  loss = 2.67150 avg_loss = 3.38750\n",
      "epoch no.2 train no.148040  loss = 3.80366 avg_loss = 3.45264\n",
      "epoch no.2 train no.148050  loss = 2.48555 avg_loss = 3.42875\n",
      "epoch no.2 train no.148060  loss = 2.42830 avg_loss = 3.46070\n",
      "epoch no.2 train no.148070  loss = 2.38379 avg_loss = 3.50950\n",
      "epoch no.2 train no.148080  loss = 3.42716 avg_loss = 3.49919\n",
      "epoch no.2 train no.148090  loss = 3.96351 avg_loss = 3.52170\n",
      "epoch no.2 train no.148100  loss = 2.27886 avg_loss = 3.47306\n",
      "epoch no.2 train no.148110  loss = 4.08717 avg_loss = 3.45697\n",
      "epoch no.2 train no.148120  loss = 2.52298 avg_loss = 3.45163\n",
      "epoch no.2 train no.148130  loss = 3.58411 avg_loss = 3.47109\n",
      "epoch no.2 train no.148140  loss = 4.38023 avg_loss = 3.47159\n",
      "epoch no.2 train no.148150  loss = 2.00077 avg_loss = 3.46226\n",
      "epoch no.2 train no.148160  loss = 2.91104 avg_loss = 3.44796\n",
      "epoch no.2 train no.148170  loss = 4.11086 avg_loss = 3.42548\n",
      "epoch no.2 train no.148180  loss = 2.23845 avg_loss = 3.42473\n",
      "epoch no.2 train no.148190  loss = 3.95573 avg_loss = 3.44725\n",
      "epoch no.2 train no.148200  loss = 2.55958 avg_loss = 3.41469\n",
      "epoch no.2 train no.148210  loss = 3.26407 avg_loss = 3.33436\n",
      "epoch no.2 train no.148220  loss = 3.39738 avg_loss = 3.30409\n",
      "epoch no.2 train no.148230  loss = 5.11235 avg_loss = 3.34652\n",
      "epoch no.2 train no.148240  loss = 2.83595 avg_loss = 3.34226\n",
      "epoch no.2 train no.148250  loss = 3.03833 avg_loss = 3.33623\n",
      "epoch no.2 train no.148260  loss = 6.76101 avg_loss = 3.40598\n",
      "epoch no.2 train no.148270  loss = 2.64227 avg_loss = 3.42843\n",
      "epoch no.2 train no.148280  loss = 2.93290 avg_loss = 3.40633\n",
      "epoch no.2 train no.148290  loss = 4.96158 avg_loss = 3.46491\n",
      "epoch no.2 train no.148300  loss = 4.48616 avg_loss = 3.48320\n",
      "epoch no.2 train no.148310  loss = 3.99604 avg_loss = 3.48605\n",
      "epoch no.2 train no.148320  loss = 3.03999 avg_loss = 3.45819\n",
      "epoch no.2 train no.148330  loss = 2.62618 avg_loss = 3.49507\n",
      "epoch no.2 train no.148340  loss = 3.07396 avg_loss = 3.44565\n",
      "epoch no.2 train no.148350  loss = 3.15725 avg_loss = 3.40942\n",
      "epoch no.2 train no.148360  loss = 3.32382 avg_loss = 3.38249\n",
      "epoch no.2 train no.148370  loss = 4.78426 avg_loss = 3.37224\n",
      "epoch no.2 train no.148380  loss = 3.31509 avg_loss = 3.43800\n",
      "epoch no.2 train no.148390  loss = 2.06900 avg_loss = 3.43190\n",
      "epoch no.2 train no.148400  loss = 2.20128 avg_loss = 3.39282\n",
      "epoch no.2 train no.148410  loss = 2.35597 avg_loss = 3.38039\n",
      "epoch no.2 train no.148420  loss = 2.43066 avg_loss = 3.39177\n",
      "epoch no.2 train no.148430  loss = 4.19135 avg_loss = 3.42654\n",
      "epoch no.2 train no.148440  loss = 2.73887 avg_loss = 3.38177\n",
      "epoch no.2 train no.148450  loss = 3.20087 avg_loss = 3.41013\n",
      "epoch no.2 train no.148460  loss = 2.14237 avg_loss = 3.44926\n",
      "epoch no.2 train no.148470  loss = 3.33179 avg_loss = 3.43722\n",
      "epoch no.2 train no.148480  loss = 3.88004 avg_loss = 3.43929\n",
      "epoch no.2 train no.148490  loss = 4.95979 avg_loss = 3.43319\n",
      "epoch no.2 train no.148500  loss = 3.06035 avg_loss = 3.41073\n",
      "epoch no.2 train no.148510  loss = 4.34624 avg_loss = 3.41136\n",
      "epoch no.2 train no.148520  loss = 3.53226 avg_loss = 3.40443\n",
      "epoch no.2 train no.148530  loss = 2.06009 avg_loss = 3.37079\n",
      "epoch no.2 train no.148540  loss = 4.18674 avg_loss = 3.40722\n",
      "epoch no.2 train no.148550  loss = 2.26978 avg_loss = 3.36811\n",
      "epoch no.2 train no.148560  loss = 5.94425 avg_loss = 3.34245\n",
      "epoch no.2 train no.148570  loss = 2.60221 avg_loss = 3.34588\n",
      "epoch no.2 train no.148580  loss = 4.10786 avg_loss = 3.33983\n",
      "epoch no.2 train no.148590  loss = 4.60606 avg_loss = 3.38261\n",
      "epoch no.2 train no.148600  loss = 2.86123 avg_loss = 3.36960\n",
      "epoch no.2 train no.148610  loss = 3.54596 avg_loss = 3.36972\n",
      "epoch no.2 train no.148620  loss = 5.11156 avg_loss = 3.39575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.148630  loss = 2.71209 avg_loss = 3.40400\n",
      "epoch no.2 train no.148640  loss = 3.96418 avg_loss = 3.40724\n",
      "epoch no.2 train no.148650  loss = 3.75019 avg_loss = 3.39753\n",
      "epoch no.2 train no.148660  loss = 2.82822 avg_loss = 3.37089\n",
      "epoch no.2 train no.148670  loss = 2.82271 avg_loss = 3.32862\n",
      "epoch no.2 train no.148680  loss = 2.63864 avg_loss = 3.32697\n",
      "epoch no.2 train no.148690  loss = 3.81391 avg_loss = 3.33068\n",
      "epoch no.2 train no.148700  loss = 3.36637 avg_loss = 3.31209\n",
      "epoch no.2 train no.148710  loss = 3.46450 avg_loss = 3.31047\n",
      "epoch no.2 train no.148720  loss = 4.73906 avg_loss = 3.30117\n",
      "epoch no.2 train no.148730  loss = 1.95948 avg_loss = 3.25500\n",
      "epoch no.2 train no.148740  loss = 4.65282 avg_loss = 3.26370\n",
      "epoch no.2 train no.148750  loss = 3.40728 avg_loss = 3.29583\n",
      "epoch no.2 train no.148760  loss = 2.63359 avg_loss = 3.28307\n",
      "epoch no.2 train no.148770  loss = 3.50246 avg_loss = 3.28224\n",
      "epoch no.2 train no.148780  loss = 3.26567 avg_loss = 3.25024\n",
      "epoch no.2 train no.148790  loss = 3.88208 avg_loss = 3.23313\n",
      "epoch no.2 train no.148800  loss = 2.53768 avg_loss = 3.24024\n",
      "epoch no.2 train no.148810  loss = 2.31649 avg_loss = 3.25152\n",
      "epoch no.2 train no.148820  loss = 2.28849 avg_loss = 3.29909\n",
      "epoch no.2 train no.148830  loss = 5.61171 avg_loss = 3.33313\n",
      "epoch no.2 train no.148840  loss = 5.07724 avg_loss = 3.34181\n",
      "epoch no.2 train no.148850  loss = 3.92660 avg_loss = 3.32454\n",
      "epoch no.2 train no.148860  loss = 2.59284 avg_loss = 3.32184\n",
      "epoch no.2 train no.148870  loss = 3.54421 avg_loss = 3.31990\n",
      "epoch no.2 train no.148880  loss = 3.84425 avg_loss = 3.31852\n",
      "epoch no.2 train no.148890  loss = 5.57700 avg_loss = 3.41792\n",
      "epoch no.2 train no.148900  loss = 3.67619 avg_loss = 3.42106\n",
      "epoch no.2 train no.148910  loss = 3.99332 avg_loss = 3.38670\n",
      "epoch no.2 train no.148920  loss = 3.67645 avg_loss = 3.36042\n",
      "epoch no.2 train no.148930  loss = 3.00748 avg_loss = 3.32732\n",
      "epoch no.2 train no.148940  loss = 5.13199 avg_loss = 3.35566\n",
      "epoch no.2 train no.148950  loss = 2.31219 avg_loss = 3.39885\n",
      "epoch no.2 train no.148960  loss = 2.93349 avg_loss = 3.37344\n",
      "epoch no.2 train no.148970  loss = 1.92502 avg_loss = 3.33405\n",
      "epoch no.2 train no.148980  loss = 2.91398 avg_loss = 3.33609\n",
      "epoch no.2 train no.148990  loss = 2.88616 avg_loss = 3.32780\n",
      "epoch no.2 train no.149000  loss = 4.46098 avg_loss = 3.34475\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁잔잔', '한', '▁노래', '</s>']\n",
      "여름밤에 듣는 잔잔한 팝</s>\n",
      "epoch no.2 train no.149010  loss = 3.47626 avg_loss = 3.36328\n",
      "epoch no.2 train no.149020  loss = 3.69316 avg_loss = 3.35323\n",
      "epoch no.2 train no.149030  loss = 4.03332 avg_loss = 3.40298\n",
      "epoch no.2 train no.149040  loss = 2.71377 avg_loss = 3.36921\n",
      "epoch no.2 train no.149050  loss = 2.28376 avg_loss = 3.37797\n",
      "epoch no.2 train no.149060  loss = 3.51065 avg_loss = 3.39484\n",
      "epoch no.2 train no.149070  loss = 4.51110 avg_loss = 3.36586\n",
      "epoch no.2 train no.149080  loss = 4.21804 avg_loss = 3.36781\n",
      "epoch no.2 train no.149090  loss = 3.91388 avg_loss = 3.42379\n",
      "epoch no.2 train no.149100  loss = 3.15031 avg_loss = 3.43508\n",
      "epoch no.2 train no.149110  loss = 2.52968 avg_loss = 3.44848\n",
      "epoch no.2 train no.149120  loss = 2.86737 avg_loss = 3.47583\n",
      "epoch no.2 train no.149130  loss = 2.92323 avg_loss = 3.45156\n",
      "epoch no.2 train no.149140  loss = 2.74136 avg_loss = 3.41172\n",
      "epoch no.2 train no.149150  loss = 4.02709 avg_loss = 3.38179\n",
      "epoch no.2 train no.149160  loss = 3.48336 avg_loss = 3.35681\n",
      "epoch no.2 train no.149170  loss = 2.51194 avg_loss = 3.33305\n",
      "epoch no.2 train no.149180  loss = 3.36495 avg_loss = 3.33860\n",
      "epoch no.2 train no.149190  loss = 3.18127 avg_loss = 3.36344\n",
      "epoch no.2 train no.149200  loss = 3.76698 avg_loss = 3.38996\n",
      "epoch no.2 train no.149210  loss = 2.13909 avg_loss = 3.38128\n",
      "epoch no.2 train no.149220  loss = 2.28358 avg_loss = 3.31152\n",
      "epoch no.2 train no.149230  loss = 3.04150 avg_loss = 3.29683\n",
      "epoch no.2 train no.149240  loss = 2.41534 avg_loss = 3.30219\n",
      "epoch no.2 train no.149250  loss = 2.53967 avg_loss = 3.31822\n",
      "epoch no.2 train no.149260  loss = 3.61421 avg_loss = 3.32658\n",
      "epoch no.2 train no.149270  loss = 5.57400 avg_loss = 3.32952\n",
      "epoch no.2 train no.149280  loss = 2.78437 avg_loss = 3.28299\n",
      "epoch no.2 train no.149290  loss = 3.04157 avg_loss = 3.31671\n",
      "epoch no.2 train no.149300  loss = 4.22987 avg_loss = 3.34137\n",
      "epoch no.2 train no.149310  loss = 2.84228 avg_loss = 3.35494\n",
      "epoch no.2 train no.149320  loss = 3.32991 avg_loss = 3.37397\n",
      "epoch no.2 train no.149330  loss = 4.25155 avg_loss = 3.36455\n",
      "epoch no.2 train no.149340  loss = 3.06410 avg_loss = 3.38552\n",
      "epoch no.2 train no.149350  loss = 2.57839 avg_loss = 3.40623\n",
      "epoch no.2 train no.149360  loss = 4.03745 avg_loss = 3.45734\n",
      "epoch no.2 train no.149370  loss = 3.20732 avg_loss = 3.41238\n",
      "epoch no.2 train no.149380  loss = 4.56748 avg_loss = 3.40162\n",
      "epoch no.2 train no.149390  loss = 2.45510 avg_loss = 3.34293\n",
      "epoch no.2 train no.149400  loss = 2.31982 avg_loss = 3.29137\n",
      "epoch no.2 train no.149410  loss = 2.34134 avg_loss = 3.26356\n",
      "epoch no.2 train no.149420  loss = 5.84518 avg_loss = 3.33508\n",
      "epoch no.2 train no.149430  loss = 3.23390 avg_loss = 3.35215\n",
      "epoch no.2 train no.149440  loss = 3.68688 avg_loss = 3.39582\n",
      "epoch no.2 train no.149450  loss = 4.40563 avg_loss = 3.41950\n",
      "epoch no.2 train no.149460  loss = 5.81554 avg_loss = 3.40235\n",
      "epoch no.2 train no.149470  loss = 3.83071 avg_loss = 3.34563\n",
      "epoch no.2 train no.149480  loss = 2.51885 avg_loss = 3.31372\n",
      "epoch no.2 train no.149490  loss = 2.66411 avg_loss = 3.31811\n",
      "epoch no.2 train no.149500  loss = 3.29959 avg_loss = 3.26274\n",
      "epoch no.2 train no.149510  loss = 3.73366 avg_loss = 3.23575\n",
      "epoch no.2 train no.149520  loss = 3.85979 avg_loss = 3.22188\n",
      "epoch no.2 train no.149530  loss = 3.24989 avg_loss = 3.25850\n",
      "epoch no.2 train no.149540  loss = 3.60076 avg_loss = 3.28418\n",
      "epoch no.2 train no.149550  loss = 2.29776 avg_loss = 3.29639\n",
      "epoch no.2 train no.149560  loss = 4.05407 avg_loss = 3.33333\n",
      "epoch no.2 train no.149570  loss = 3.74393 avg_loss = 3.33354\n",
      "epoch no.2 train no.149580  loss = 4.02274 avg_loss = 3.35685\n",
      "epoch no.2 train no.149590  loss = 2.58219 avg_loss = 3.36274\n",
      "epoch no.2 train no.149600  loss = 2.20543 avg_loss = 3.33359\n",
      "epoch no.2 train no.149610  loss = 2.63081 avg_loss = 3.35324\n",
      "epoch no.2 train no.149620  loss = 3.17202 avg_loss = 3.37449\n",
      "epoch no.2 train no.149630  loss = 2.05526 avg_loss = 3.36692\n",
      "epoch no.2 train no.149640  loss = 5.60398 avg_loss = 3.40043\n",
      "epoch no.2 train no.149650  loss = 3.56363 avg_loss = 3.44145\n",
      "epoch no.2 train no.149660  loss = 3.92368 avg_loss = 3.44650\n",
      "epoch no.2 train no.149670  loss = 3.29562 avg_loss = 3.44680\n",
      "epoch no.2 train no.149680  loss = 5.12499 avg_loss = 3.46837\n",
      "epoch no.2 train no.149690  loss = 2.69413 avg_loss = 3.48079\n",
      "epoch no.2 train no.149700  loss = 2.38930 avg_loss = 3.40060\n",
      "epoch no.2 train no.149710  loss = 5.34398 avg_loss = 3.40549\n",
      "epoch no.2 train no.149720  loss = 4.41974 avg_loss = 3.36574\n",
      "epoch no.2 train no.149730  loss = 4.50238 avg_loss = 3.37666\n",
      "epoch no.2 train no.149740  loss = 3.21288 avg_loss = 3.39418\n",
      "epoch no.2 train no.149750  loss = 3.13918 avg_loss = 3.40699\n",
      "epoch no.2 train no.149760  loss = 3.86775 avg_loss = 3.44973\n",
      "epoch no.2 train no.149770  loss = 4.79837 avg_loss = 3.43156\n",
      "epoch no.2 train no.149780  loss = 4.04085 avg_loss = 3.41240\n",
      "epoch no.2 train no.149790  loss = 3.35607 avg_loss = 3.44004\n",
      "epoch no.2 train no.149800  loss = 2.60199 avg_loss = 3.41747\n",
      "epoch no.2 train no.149810  loss = 2.21712 avg_loss = 3.41982\n",
      "epoch no.2 train no.149820  loss = 3.08020 avg_loss = 3.38285\n",
      "epoch no.2 train no.149830  loss = 2.02011 avg_loss = 3.32610\n",
      "epoch no.2 train no.149840  loss = 2.46561 avg_loss = 3.29643\n",
      "epoch no.2 train no.149850  loss = 2.18206 avg_loss = 3.26757\n",
      "epoch no.2 train no.149860  loss = 2.81968 avg_loss = 3.22211\n",
      "epoch no.2 train no.149870  loss = 3.10366 avg_loss = 3.24248\n",
      "epoch no.2 train no.149880  loss = 3.76006 avg_loss = 3.28084\n",
      "epoch no.2 train no.149890  loss = 4.27187 avg_loss = 3.26834\n",
      "epoch no.2 train no.149900  loss = 3.29715 avg_loss = 3.25886\n",
      "epoch no.2 train no.149910  loss = 4.06472 avg_loss = 3.26960\n",
      "epoch no.2 train no.149920  loss = 2.21175 avg_loss = 3.22583\n",
      "epoch no.2 train no.149930  loss = 3.67988 avg_loss = 3.27589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.149940  loss = 2.49815 avg_loss = 3.27276\n",
      "epoch no.2 train no.149950  loss = 4.39081 avg_loss = 3.29183\n",
      "epoch no.2 train no.149960  loss = 4.11840 avg_loss = 3.38388\n",
      "epoch no.2 train no.149970  loss = 3.42084 avg_loss = 3.38553\n",
      "epoch no.2 train no.149980  loss = 4.69177 avg_loss = 3.40385\n",
      "epoch no.2 train no.149990  loss = 5.68485 avg_loss = 3.37417\n",
      "epoch no.2 train no.150000  loss = 3.19736 avg_loss = 3.39760\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁시작을', '▁알리는', '▁재즈', '</s>']\n",
      "여름의 시작을 알리는 음악</s>\n",
      "epoch no.2 train no.150010  loss = 2.67398 avg_loss = 3.42075\n",
      "epoch no.2 train no.150020  loss = 4.45194 avg_loss = 3.42613\n",
      "epoch no.2 train no.150030  loss = 2.35478 avg_loss = 3.43110\n",
      "epoch no.2 train no.150040  loss = 3.50830 avg_loss = 3.35901\n",
      "epoch no.2 train no.150050  loss = 2.53443 avg_loss = 3.36955\n",
      "epoch no.2 train no.150060  loss = 3.86170 avg_loss = 3.38016\n",
      "epoch no.2 train no.150070  loss = 4.26379 avg_loss = 3.43541\n",
      "epoch no.2 train no.150080  loss = 3.58768 avg_loss = 3.40671\n",
      "epoch no.2 train no.150090  loss = 3.73416 avg_loss = 3.39137\n",
      "epoch no.2 train no.150100  loss = 2.50847 avg_loss = 3.38932\n",
      "epoch no.2 train no.150110  loss = 4.21182 avg_loss = 3.38445\n",
      "epoch no.2 train no.150120  loss = 2.75140 avg_loss = 3.43883\n",
      "epoch no.2 train no.150130  loss = 3.27285 avg_loss = 3.38720\n",
      "epoch no.2 train no.150140  loss = 2.06412 avg_loss = 3.39060\n",
      "epoch no.2 train no.150150  loss = 2.61394 avg_loss = 3.40968\n",
      "epoch no.2 train no.150160  loss = 4.57185 avg_loss = 3.41061\n",
      "epoch no.2 train no.150170  loss = 5.96196 avg_loss = 3.42826\n",
      "epoch no.2 train no.150180  loss = 2.00229 avg_loss = 3.45931\n",
      "epoch no.2 train no.150190  loss = 3.93741 avg_loss = 3.44173\n",
      "epoch no.2 train no.150200  loss = 3.08596 avg_loss = 3.44261\n",
      "epoch no.2 train no.150210  loss = 3.01209 avg_loss = 3.44074\n",
      "epoch no.2 train no.150220  loss = 2.38422 avg_loss = 3.42523\n",
      "epoch no.2 train no.150230  loss = 1.84454 avg_loss = 3.38235\n",
      "epoch no.2 train no.150240  loss = 5.59927 avg_loss = 3.37087\n",
      "epoch no.2 train no.150250  loss = 2.05534 avg_loss = 3.35010\n",
      "epoch no.2 train no.150260  loss = 5.66593 avg_loss = 3.36149\n",
      "epoch no.2 train no.150270  loss = 4.19229 avg_loss = 3.31254\n",
      "epoch no.2 train no.150280  loss = 3.66434 avg_loss = 3.30643\n",
      "epoch no.2 train no.150290  loss = 4.31759 avg_loss = 3.37692\n",
      "epoch no.2 train no.150300  loss = 2.76524 avg_loss = 3.35379\n",
      "epoch no.2 train no.150310  loss = 4.35868 avg_loss = 3.38203\n",
      "epoch no.2 train no.150320  loss = 3.53998 avg_loss = 3.40707\n",
      "epoch no.2 train no.150330  loss = 2.53444 avg_loss = 3.38773\n",
      "epoch no.2 train no.150340  loss = 3.27257 avg_loss = 3.38468\n",
      "epoch no.2 train no.150350  loss = 3.92756 avg_loss = 3.39777\n",
      "epoch no.2 train no.150360  loss = 1.60966 avg_loss = 3.38454\n",
      "epoch no.2 train no.150370  loss = 1.57283 avg_loss = 3.39416\n",
      "epoch no.2 train no.150380  loss = 3.26051 avg_loss = 3.38629\n",
      "epoch no.2 train no.150390  loss = 2.40472 avg_loss = 3.36685\n",
      "epoch no.2 train no.150400  loss = 1.84857 avg_loss = 3.35896\n",
      "epoch no.2 train no.150410  loss = 1.93918 avg_loss = 3.37111\n",
      "epoch no.2 train no.150420  loss = 2.31352 avg_loss = 3.38426\n",
      "epoch no.2 train no.150430  loss = 3.71730 avg_loss = 3.40882\n",
      "epoch no.2 train no.150440  loss = 2.98324 avg_loss = 3.37468\n",
      "epoch no.2 train no.150450  loss = 3.09079 avg_loss = 3.36730\n",
      "epoch no.2 train no.150460  loss = 3.91972 avg_loss = 3.34897\n",
      "epoch no.2 train no.150470  loss = 3.22635 avg_loss = 3.39662\n",
      "epoch no.2 train no.150480  loss = 3.66050 avg_loss = 3.43493\n",
      "epoch no.2 train no.150490  loss = 2.99486 avg_loss = 3.46175\n",
      "epoch no.2 train no.150500  loss = 2.96636 avg_loss = 3.42576\n",
      "epoch no.2 train no.150510  loss = 2.27163 avg_loss = 3.45067\n",
      "epoch no.2 train no.150520  loss = 2.99763 avg_loss = 3.42113\n",
      "epoch no.2 train no.150530  loss = 1.62734 avg_loss = 3.40559\n",
      "epoch no.2 train no.150540  loss = 4.38157 avg_loss = 3.37473\n",
      "epoch no.2 train no.150550  loss = 3.69730 avg_loss = 3.38115\n",
      "epoch no.2 train no.150560  loss = 3.32159 avg_loss = 3.37547\n",
      "epoch no.2 train no.150570  loss = 3.28205 avg_loss = 3.38928\n",
      "epoch no.2 train no.150580  loss = 3.60880 avg_loss = 3.37162\n",
      "epoch no.2 train no.150590  loss = 3.15766 avg_loss = 3.36357\n",
      "epoch no.2 train no.150600  loss = 3.49618 avg_loss = 3.37619\n",
      "epoch no.2 train no.150610  loss = 2.29881 avg_loss = 3.35351\n",
      "epoch no.2 train no.150620  loss = 2.52692 avg_loss = 3.34930\n",
      "epoch no.2 train no.150630  loss = 3.14381 avg_loss = 3.30118\n",
      "epoch no.2 train no.150640  loss = 2.16636 avg_loss = 3.24309\n",
      "epoch no.2 train no.150650  loss = 3.00372 avg_loss = 3.26794\n",
      "epoch no.2 train no.150660  loss = 2.50491 avg_loss = 3.25300\n",
      "epoch no.2 train no.150670  loss = 2.45932 avg_loss = 3.27738\n",
      "epoch no.2 train no.150680  loss = 3.24486 avg_loss = 3.31969\n",
      "epoch no.2 train no.150690  loss = 4.79361 avg_loss = 3.31308\n",
      "epoch no.2 train no.150700  loss = 2.81659 avg_loss = 3.35913\n",
      "epoch no.2 train no.150710  loss = 3.90595 avg_loss = 3.40757\n",
      "epoch no.2 train no.150720  loss = 4.51407 avg_loss = 3.40209\n",
      "epoch no.2 train no.150730  loss = 2.86903 avg_loss = 3.42305\n",
      "epoch no.2 train no.150740  loss = 3.23821 avg_loss = 3.46893\n",
      "epoch no.2 train no.150750  loss = 2.10884 avg_loss = 3.46416\n",
      "epoch no.2 train no.150760  loss = 3.38584 avg_loss = 3.49720\n",
      "epoch no.2 train no.150770  loss = 3.46658 avg_loss = 3.44710\n",
      "epoch no.2 train no.150780  loss = 3.57106 avg_loss = 3.43098\n",
      "epoch no.2 train no.150790  loss = 4.41276 avg_loss = 3.37463\n",
      "epoch no.2 train no.150800  loss = 3.28120 avg_loss = 3.39404\n",
      "epoch no.2 train no.150810  loss = 2.84633 avg_loss = 3.39300\n",
      "epoch no.2 train no.150820  loss = 3.99679 avg_loss = 3.37689\n",
      "epoch no.2 train no.150830  loss = 5.72852 avg_loss = 3.43981\n",
      "epoch no.2 train no.150840  loss = 2.92016 avg_loss = 3.40715\n",
      "epoch no.2 train no.150850  loss = 4.44572 avg_loss = 3.41006\n",
      "epoch no.2 train no.150860  loss = 1.97448 avg_loss = 3.38441\n",
      "epoch no.2 train no.150870  loss = 3.40697 avg_loss = 3.38741\n",
      "epoch no.2 train no.150880  loss = 5.59757 avg_loss = 3.37656\n",
      "epoch no.2 train no.150890  loss = 3.26384 avg_loss = 3.36394\n",
      "epoch no.2 train no.150900  loss = 2.85020 avg_loss = 3.39337\n",
      "epoch no.2 train no.150910  loss = 2.21270 avg_loss = 3.41579\n",
      "epoch no.2 train no.150920  loss = 2.94982 avg_loss = 3.37084\n",
      "epoch no.2 train no.150930  loss = 2.59696 avg_loss = 3.34133\n",
      "epoch no.2 train no.150940  loss = 2.59895 avg_loss = 3.31191\n",
      "epoch no.2 train no.150950  loss = 2.33343 avg_loss = 3.28981\n",
      "epoch no.2 train no.150960  loss = 2.40701 avg_loss = 3.27283\n",
      "epoch no.2 train no.150970  loss = 2.53430 avg_loss = 3.31820\n",
      "epoch no.2 train no.150980  loss = 1.85359 avg_loss = 3.38201\n",
      "epoch no.2 train no.150990  loss = 2.63183 avg_loss = 3.35207\n",
      "epoch no.2 train no.151000  loss = 2.95469 avg_loss = 3.32477\n",
      "5\n",
      "to_tokens: ['▁가을', '과', '▁어울리는', '▁청량', '한', '▁노래', '</s>']\n",
      "여름과 어울리는 청량한 노래</s>\n",
      "epoch no.2 train no.151010  loss = 3.54550 avg_loss = 3.32742\n",
      "epoch no.2 train no.151020  loss = 2.91119 avg_loss = 3.30696\n",
      "epoch no.2 train no.151030  loss = 3.13570 avg_loss = 3.25703\n",
      "epoch no.2 train no.151040  loss = 2.42627 avg_loss = 3.27574\n",
      "epoch no.2 train no.151050  loss = 2.36300 avg_loss = 3.27839\n",
      "epoch no.2 train no.151060  loss = 3.55567 avg_loss = 3.28313\n",
      "epoch no.2 train no.151070  loss = 5.00942 avg_loss = 3.33145\n",
      "epoch no.2 train no.151080  loss = 1.55748 avg_loss = 3.38043\n",
      "epoch no.2 train no.151090  loss = 3.61991 avg_loss = 3.38688\n",
      "epoch no.2 train no.151100  loss = 2.67944 avg_loss = 3.37702\n",
      "epoch no.2 train no.151110  loss = 5.07855 avg_loss = 3.36277\n",
      "epoch no.2 train no.151120  loss = 3.11348 avg_loss = 3.33562\n",
      "epoch no.2 train no.151130  loss = 2.74171 avg_loss = 3.37618\n",
      "epoch no.2 train no.151140  loss = 3.55270 avg_loss = 3.35989\n",
      "epoch no.2 train no.151150  loss = 4.96836 avg_loss = 3.37777\n",
      "epoch no.2 train no.151160  loss = 2.78937 avg_loss = 3.39906\n",
      "epoch no.2 train no.151170  loss = 1.72902 avg_loss = 3.37757\n",
      "epoch no.2 train no.151180  loss = 3.16968 avg_loss = 3.37531\n",
      "epoch no.2 train no.151190  loss = 3.62963 avg_loss = 3.35354\n",
      "epoch no.2 train no.151200  loss = 3.39638 avg_loss = 3.31283\n",
      "epoch no.2 train no.151210  loss = 2.67418 avg_loss = 3.31129\n",
      "epoch no.2 train no.151220  loss = 3.87684 avg_loss = 3.31438\n",
      "epoch no.2 train no.151230  loss = 3.23784 avg_loss = 3.29249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.151240  loss = 1.12663 avg_loss = 3.27454\n",
      "epoch no.2 train no.151250  loss = 5.45840 avg_loss = 3.29959\n",
      "epoch no.2 train no.151260  loss = 2.94201 avg_loss = 3.32112\n",
      "epoch no.2 train no.151270  loss = 3.79617 avg_loss = 3.31996\n",
      "epoch no.2 train no.151280  loss = 3.23199 avg_loss = 3.34907\n",
      "epoch no.2 train no.151290  loss = 3.25786 avg_loss = 3.31810\n",
      "epoch no.2 train no.151300  loss = 3.55121 avg_loss = 3.29965\n",
      "epoch no.2 train no.151310  loss = 5.32808 avg_loss = 3.32296\n",
      "epoch no.2 train no.151320  loss = 3.59485 avg_loss = 3.31605\n",
      "epoch no.2 train no.151330  loss = 2.58673 avg_loss = 3.28315\n",
      "epoch no.2 train no.151340  loss = 2.44161 avg_loss = 3.33988\n",
      "epoch no.2 train no.151350  loss = 2.78370 avg_loss = 3.31857\n",
      "epoch no.2 train no.151360  loss = 3.66422 avg_loss = 3.35677\n",
      "epoch no.2 train no.151370  loss = 3.56225 avg_loss = 3.34382\n",
      "epoch no.2 train no.151380  loss = 2.17973 avg_loss = 3.37242\n",
      "epoch no.2 train no.151390  loss = 3.65948 avg_loss = 3.42563\n",
      "epoch no.2 train no.151400  loss = 3.92067 avg_loss = 3.41434\n",
      "epoch no.2 train no.151410  loss = 3.89477 avg_loss = 3.39707\n",
      "epoch no.2 train no.151420  loss = 3.14236 avg_loss = 3.34191\n",
      "epoch no.2 train no.151430  loss = 2.62108 avg_loss = 3.31844\n",
      "epoch no.2 train no.151440  loss = 4.67459 avg_loss = 3.33717\n",
      "epoch no.2 train no.151450  loss = 1.80642 avg_loss = 3.36027\n",
      "epoch no.2 train no.151460  loss = 4.67048 avg_loss = 3.36184\n",
      "epoch no.2 train no.151470  loss = 2.55284 avg_loss = 3.35717\n",
      "epoch no.2 train no.151480  loss = 2.28283 avg_loss = 3.34366\n",
      "epoch no.2 train no.151490  loss = 4.13591 avg_loss = 3.36534\n",
      "epoch no.2 train no.151500  loss = 4.89600 avg_loss = 3.43023\n",
      "epoch no.2 train no.151510  loss = 5.05188 avg_loss = 3.41100\n",
      "epoch no.2 train no.151520  loss = 4.89551 avg_loss = 3.41244\n",
      "epoch no.2 train no.151530  loss = 4.47394 avg_loss = 3.37658\n",
      "epoch no.2 train no.151540  loss = 2.98933 avg_loss = 3.40306\n",
      "epoch no.2 train no.151550  loss = 4.52415 avg_loss = 3.40803\n",
      "epoch no.2 train no.151560  loss = 4.64365 avg_loss = 3.39619\n",
      "epoch no.2 train no.151570  loss = 2.83553 avg_loss = 3.38456\n",
      "epoch no.2 train no.151580  loss = 6.34157 avg_loss = 3.38680\n",
      "epoch no.2 train no.151590  loss = 3.94198 avg_loss = 3.36971\n",
      "epoch no.2 train no.151600  loss = 4.06002 avg_loss = 3.43450\n",
      "epoch no.2 train no.151610  loss = 2.72175 avg_loss = 3.42372\n",
      "epoch no.2 train no.151620  loss = 2.90946 avg_loss = 3.39168\n",
      "epoch no.2 train no.151630  loss = 3.22639 avg_loss = 3.39452\n",
      "epoch no.2 train no.151640  loss = 4.09744 avg_loss = 3.39469\n",
      "epoch no.2 train no.151650  loss = 2.98670 avg_loss = 3.37796\n",
      "epoch no.2 train no.151660  loss = 4.67442 avg_loss = 3.34758\n",
      "epoch no.2 train no.151670  loss = 3.70380 avg_loss = 3.35492\n",
      "epoch no.2 train no.151680  loss = 4.00438 avg_loss = 3.37091\n",
      "epoch no.2 train no.151690  loss = 2.24560 avg_loss = 3.32250\n",
      "epoch no.2 train no.151700  loss = 3.91158 avg_loss = 3.38661\n",
      "epoch no.2 train no.151710  loss = 2.28406 avg_loss = 3.40696\n",
      "epoch no.2 train no.151720  loss = 3.96315 avg_loss = 3.44176\n",
      "epoch no.2 train no.151730  loss = 2.49214 avg_loss = 3.41401\n",
      "epoch no.2 train no.151740  loss = 2.27781 avg_loss = 3.37866\n",
      "epoch no.2 train no.151750  loss = 2.74106 avg_loss = 3.38047\n",
      "epoch no.2 train no.151760  loss = 3.56297 avg_loss = 3.37340\n",
      "epoch no.2 train no.151770  loss = 2.71580 avg_loss = 3.37414\n",
      "epoch no.2 train no.151780  loss = 3.25649 avg_loss = 3.37746\n",
      "epoch no.2 train no.151790  loss = 2.13606 avg_loss = 3.38536\n",
      "epoch no.2 train no.151800  loss = 2.62882 avg_loss = 3.33483\n",
      "epoch no.2 train no.151810  loss = 3.05981 avg_loss = 3.32415\n",
      "epoch no.2 train no.151820  loss = 3.54984 avg_loss = 3.33599\n",
      "epoch no.2 train no.151830  loss = 4.48673 avg_loss = 3.35435\n",
      "epoch no.2 train no.151840  loss = 2.25484 avg_loss = 3.34699\n",
      "epoch no.2 train no.151850  loss = 2.37557 avg_loss = 3.33806\n",
      "epoch no.2 train no.151860  loss = 3.90176 avg_loss = 3.33059\n",
      "epoch no.2 train no.151870  loss = 3.14942 avg_loss = 3.34604\n",
      "epoch no.2 train no.151880  loss = 2.49457 avg_loss = 3.32057\n",
      "epoch no.2 train no.151890  loss = 3.41043 avg_loss = 3.39567\n",
      "epoch no.2 train no.151900  loss = 2.52976 avg_loss = 3.39490\n",
      "epoch no.2 train no.151910  loss = 1.95005 avg_loss = 3.35801\n",
      "epoch no.2 train no.151920  loss = 1.99507 avg_loss = 3.30639\n",
      "epoch no.2 train no.151930  loss = 2.75937 avg_loss = 3.30905\n",
      "epoch no.2 train no.151940  loss = 4.11074 avg_loss = 3.30923\n",
      "epoch no.2 train no.151950  loss = 2.60767 avg_loss = 3.29706\n",
      "epoch no.2 train no.151960  loss = 1.15351 avg_loss = 3.27411\n",
      "epoch no.2 train no.151970  loss = 2.54396 avg_loss = 3.31864\n",
      "epoch no.2 train no.151980  loss = 2.31339 avg_loss = 3.33542\n",
      "epoch no.2 train no.151990  loss = 2.66945 avg_loss = 3.30475\n",
      "epoch no.2 train no.152000  loss = 2.37904 avg_loss = 3.32987\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁재즈', '발', '</s>']\n",
      "여름밤에 듣는 감성음악</s>\n",
      "epoch no.2 train no.152010  loss = 2.60711 avg_loss = 3.33567\n",
      "epoch no.2 train no.152020  loss = 2.04504 avg_loss = 3.32239\n",
      "epoch no.2 train no.152030  loss = 4.82426 avg_loss = 3.38526\n",
      "epoch no.2 train no.152040  loss = 5.65032 avg_loss = 3.43681\n",
      "epoch no.2 train no.152050  loss = 2.02900 avg_loss = 3.39634\n",
      "epoch no.2 train no.152060  loss = 4.91257 avg_loss = 3.41423\n",
      "epoch no.2 train no.152070  loss = 3.76198 avg_loss = 3.42442\n",
      "epoch no.2 train no.152080  loss = 3.58797 avg_loss = 3.41596\n",
      "epoch no.2 train no.152090  loss = 2.91584 avg_loss = 3.42331\n",
      "epoch no.2 train no.152100  loss = 2.35881 avg_loss = 3.42232\n",
      "epoch no.2 train no.152110  loss = 3.33280 avg_loss = 3.42186\n",
      "epoch no.2 train no.152120  loss = 3.05912 avg_loss = 3.46579\n",
      "epoch no.2 train no.152130  loss = 3.50718 avg_loss = 3.42518\n",
      "epoch no.2 train no.152140  loss = 4.37023 avg_loss = 3.44559\n",
      "epoch no.2 train no.152150  loss = 3.76093 avg_loss = 3.41655\n",
      "epoch no.2 train no.152160  loss = 3.97311 avg_loss = 3.43875\n",
      "epoch no.2 train no.152170  loss = 2.64876 avg_loss = 3.39117\n",
      "epoch no.2 train no.152180  loss = 3.07539 avg_loss = 3.33053\n",
      "epoch no.2 train no.152190  loss = 3.61152 avg_loss = 3.39933\n",
      "epoch no.2 train no.152200  loss = 2.50092 avg_loss = 3.35001\n",
      "epoch no.2 train no.152210  loss = 4.47505 avg_loss = 3.40061\n",
      "epoch no.2 train no.152220  loss = 2.10791 avg_loss = 3.39392\n",
      "epoch no.2 train no.152230  loss = 2.53835 avg_loss = 3.43030\n",
      "epoch no.2 train no.152240  loss = 2.52667 avg_loss = 3.37672\n",
      "epoch no.2 train no.152250  loss = 4.65704 avg_loss = 3.37006\n",
      "epoch no.2 train no.152260  loss = 4.34747 avg_loss = 3.36719\n",
      "epoch no.2 train no.152270  loss = 3.72414 avg_loss = 3.40314\n",
      "epoch no.2 train no.152280  loss = 2.94594 avg_loss = 3.43942\n",
      "epoch no.2 train no.152290  loss = 3.58026 avg_loss = 3.43201\n",
      "epoch no.2 train no.152300  loss = 3.22891 avg_loss = 3.45383\n",
      "epoch no.2 train no.152310  loss = 3.67216 avg_loss = 3.48097\n",
      "epoch no.2 train no.152320  loss = 2.27654 avg_loss = 3.41287\n",
      "epoch no.2 train no.152330  loss = 5.33676 avg_loss = 3.45337\n",
      "epoch no.2 train no.152340  loss = 2.49238 avg_loss = 3.44024\n",
      "epoch no.2 train no.152350  loss = 2.20668 avg_loss = 3.46256\n",
      "epoch no.2 train no.152360  loss = 3.10240 avg_loss = 3.48887\n",
      "epoch no.2 train no.152370  loss = 3.35495 avg_loss = 3.46894\n",
      "epoch no.2 train no.152380  loss = 3.26679 avg_loss = 3.48482\n",
      "epoch no.2 train no.152390  loss = 2.91370 avg_loss = 3.47606\n",
      "epoch no.2 train no.152400  loss = 3.54267 avg_loss = 3.47259\n",
      "epoch no.2 train no.152410  loss = 2.55589 avg_loss = 3.45589\n",
      "epoch no.2 train no.152420  loss = 3.23222 avg_loss = 3.42261\n",
      "epoch no.2 train no.152430  loss = 2.99111 avg_loss = 3.41535\n",
      "epoch no.2 train no.152440  loss = 3.20325 avg_loss = 3.39609\n",
      "epoch no.2 train no.152450  loss = 2.53681 avg_loss = 3.41406\n",
      "epoch no.2 train no.152460  loss = 4.19330 avg_loss = 3.39827\n",
      "epoch no.2 train no.152470  loss = 3.58577 avg_loss = 3.33221\n",
      "epoch no.2 train no.152480  loss = 3.30853 avg_loss = 3.35746\n",
      "epoch no.2 train no.152490  loss = 1.97833 avg_loss = 3.39460\n",
      "epoch no.2 train no.152500  loss = 5.34324 avg_loss = 3.42607\n",
      "epoch no.2 train no.152510  loss = 3.04351 avg_loss = 3.44838\n",
      "epoch no.2 train no.152520  loss = 3.68194 avg_loss = 3.43472\n",
      "epoch no.2 train no.152530  loss = 1.89815 avg_loss = 3.44485\n",
      "epoch no.2 train no.152540  loss = 3.35747 avg_loss = 3.43576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.152550  loss = 3.58230 avg_loss = 3.45254\n",
      "epoch no.2 train no.152560  loss = 3.43091 avg_loss = 3.47460\n",
      "epoch no.2 train no.152570  loss = 7.12428 avg_loss = 3.55272\n",
      "epoch no.2 train no.152580  loss = 3.58821 avg_loss = 3.54746\n",
      "epoch no.2 train no.152590  loss = 2.46100 avg_loss = 3.51484\n",
      "epoch no.2 train no.152600  loss = 3.79513 avg_loss = 3.52829\n",
      "epoch no.2 train no.152610  loss = 4.12242 avg_loss = 3.51278\n",
      "epoch no.2 train no.152620  loss = 4.36625 avg_loss = 3.46167\n",
      "epoch no.2 train no.152630  loss = 3.63886 avg_loss = 3.48629\n",
      "epoch no.2 train no.152640  loss = 1.68081 avg_loss = 3.43974\n",
      "epoch no.2 train no.152650  loss = 3.15234 avg_loss = 3.40832\n",
      "epoch no.2 train no.152660  loss = 4.06281 avg_loss = 3.42100\n",
      "epoch no.2 train no.152670  loss = 3.16432 avg_loss = 3.43176\n",
      "epoch no.2 train no.152680  loss = 3.23982 avg_loss = 3.46338\n",
      "epoch no.2 train no.152690  loss = 3.40259 avg_loss = 3.47898\n",
      "epoch no.2 train no.152700  loss = 3.80205 avg_loss = 3.49563\n",
      "epoch no.2 train no.152710  loss = 3.05572 avg_loss = 3.52959\n",
      "epoch no.2 train no.152720  loss = 3.50268 avg_loss = 3.46248\n",
      "epoch no.2 train no.152730  loss = 3.36810 avg_loss = 3.47390\n",
      "epoch no.2 train no.152740  loss = 4.18623 avg_loss = 3.51803\n",
      "epoch no.2 train no.152750  loss = 3.11352 avg_loss = 3.51560\n",
      "epoch no.2 train no.152760  loss = 3.98015 avg_loss = 3.53888\n",
      "epoch no.2 train no.152770  loss = 2.39058 avg_loss = 3.52927\n",
      "epoch no.2 train no.152780  loss = 2.88471 avg_loss = 3.51007\n",
      "epoch no.2 train no.152790  loss = 2.81322 avg_loss = 3.52956\n",
      "epoch no.2 train no.152800  loss = 2.99468 avg_loss = 3.48226\n",
      "epoch no.2 train no.152810  loss = 2.42934 avg_loss = 3.44446\n",
      "epoch no.2 train no.152820  loss = 3.43612 avg_loss = 3.43331\n",
      "epoch no.2 train no.152830  loss = 3.21872 avg_loss = 3.40790\n",
      "epoch no.2 train no.152840  loss = 4.44296 avg_loss = 3.43593\n",
      "epoch no.2 train no.152850  loss = 2.81547 avg_loss = 3.43112\n",
      "epoch no.2 train no.152860  loss = 2.74407 avg_loss = 3.48780\n",
      "epoch no.2 train no.152870  loss = 2.44161 avg_loss = 3.49304\n",
      "epoch no.2 train no.152880  loss = 3.08031 avg_loss = 3.47429\n",
      "epoch no.2 train no.152890  loss = 2.74574 avg_loss = 3.46874\n",
      "epoch no.2 train no.152900  loss = 2.25713 avg_loss = 3.40731\n",
      "epoch no.2 train no.152910  loss = 2.36632 avg_loss = 3.32548\n",
      "epoch no.2 train no.152920  loss = 0.87635 avg_loss = 3.28941\n",
      "epoch no.2 train no.152930  loss = 2.99302 avg_loss = 3.29904\n",
      "epoch no.2 train no.152940  loss = 3.42513 avg_loss = 3.29588\n",
      "epoch no.2 train no.152950  loss = 3.58941 avg_loss = 3.29729\n",
      "epoch no.2 train no.152960  loss = 5.95218 avg_loss = 3.32983\n",
      "epoch no.2 train no.152970  loss = 3.74342 avg_loss = 3.32658\n",
      "epoch no.2 train no.152980  loss = 3.11654 avg_loss = 3.33903\n",
      "epoch no.2 train no.152990  loss = 2.54720 avg_loss = 3.31783\n",
      "epoch no.2 train no.153000  loss = 3.18751 avg_loss = 3.28205\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.153010  loss = 2.61056 avg_loss = 3.24993\n",
      "epoch no.2 train no.153020  loss = 2.06566 avg_loss = 3.28506\n",
      "epoch no.2 train no.153030  loss = 3.58147 avg_loss = 3.35605\n",
      "epoch no.2 train no.153040  loss = 4.32940 avg_loss = 3.37403\n",
      "epoch no.2 train no.153050  loss = 2.76804 avg_loss = 3.40659\n",
      "epoch no.2 train no.153060  loss = 3.90157 avg_loss = 3.37121\n",
      "epoch no.2 train no.153070  loss = 2.59485 avg_loss = 3.34235\n",
      "epoch no.2 train no.153080  loss = 4.05970 avg_loss = 3.38218\n",
      "epoch no.2 train no.153090  loss = 4.96517 avg_loss = 3.39047\n",
      "epoch no.2 train no.153100  loss = 5.71984 avg_loss = 3.38053\n",
      "epoch no.2 train no.153110  loss = 5.39452 avg_loss = 3.38843\n",
      "epoch no.2 train no.153120  loss = 4.63509 avg_loss = 3.42544\n",
      "epoch no.2 train no.153130  loss = 5.00779 avg_loss = 3.43432\n",
      "epoch no.2 train no.153140  loss = 2.02627 avg_loss = 3.37516\n",
      "epoch no.2 train no.153150  loss = 3.46669 avg_loss = 3.37170\n",
      "epoch no.2 train no.153160  loss = 3.52886 avg_loss = 3.38560\n",
      "epoch no.2 train no.153170  loss = 3.67758 avg_loss = 3.37817\n",
      "epoch no.2 train no.153180  loss = 2.22028 avg_loss = 3.30971\n",
      "epoch no.2 train no.153190  loss = 2.30263 avg_loss = 3.29834\n",
      "epoch no.2 train no.153200  loss = 2.51763 avg_loss = 3.30237\n",
      "epoch no.2 train no.153210  loss = 4.49318 avg_loss = 3.34483\n",
      "epoch no.2 train no.153220  loss = 4.01356 avg_loss = 3.40919\n",
      "epoch no.2 train no.153230  loss = 2.87191 avg_loss = 3.38289\n",
      "epoch no.2 train no.153240  loss = 3.01544 avg_loss = 3.36371\n",
      "epoch no.2 train no.153250  loss = 3.97666 avg_loss = 3.34906\n",
      "epoch no.2 train no.153260  loss = 2.78952 avg_loss = 3.34268\n",
      "epoch no.2 train no.153270  loss = 3.56185 avg_loss = 3.33010\n",
      "epoch no.2 train no.153280  loss = 2.43851 avg_loss = 3.34521\n",
      "epoch no.2 train no.153290  loss = 2.67870 avg_loss = 3.34923\n",
      "epoch no.2 train no.153300  loss = 3.38139 avg_loss = 3.32927\n",
      "epoch no.2 train no.153310  loss = 2.07756 avg_loss = 3.34877\n",
      "epoch no.2 train no.153320  loss = 4.57788 avg_loss = 3.36979\n",
      "epoch no.2 train no.153330  loss = 2.42544 avg_loss = 3.38967\n",
      "epoch no.2 train no.153340  loss = 4.60383 avg_loss = 3.35666\n",
      "epoch no.2 train no.153350  loss = 2.19575 avg_loss = 3.36062\n",
      "epoch no.2 train no.153360  loss = 4.09557 avg_loss = 3.39678\n",
      "epoch no.2 train no.153370  loss = 3.34620 avg_loss = 3.37457\n",
      "epoch no.2 train no.153380  loss = 2.80153 avg_loss = 3.34853\n",
      "epoch no.2 train no.153390  loss = 2.58329 avg_loss = 3.29704\n",
      "epoch no.2 train no.153400  loss = 3.75855 avg_loss = 3.28564\n",
      "epoch no.2 train no.153410  loss = 3.90695 avg_loss = 3.30199\n",
      "epoch no.2 train no.153420  loss = 3.48378 avg_loss = 3.32765\n",
      "epoch no.2 train no.153430  loss = 6.57259 avg_loss = 3.35053\n",
      "epoch no.2 train no.153440  loss = 3.64417 avg_loss = 3.40994\n",
      "epoch no.2 train no.153450  loss = 5.49076 avg_loss = 3.43811\n",
      "epoch no.2 train no.153460  loss = 2.29286 avg_loss = 3.49393\n",
      "epoch no.2 train no.153470  loss = 2.90273 avg_loss = 3.46829\n",
      "epoch no.2 train no.153480  loss = 2.30207 avg_loss = 3.48445\n",
      "epoch no.2 train no.153490  loss = 4.30313 avg_loss = 3.51081\n",
      "epoch no.2 train no.153500  loss = 2.77867 avg_loss = 3.50097\n",
      "epoch no.2 train no.153510  loss = 3.04498 avg_loss = 3.45541\n",
      "epoch no.2 train no.153520  loss = 3.75035 avg_loss = 3.49591\n",
      "epoch no.2 train no.153530  loss = 4.05470 avg_loss = 3.50261\n",
      "epoch no.2 train no.153540  loss = 3.74131 avg_loss = 3.46044\n",
      "epoch no.2 train no.153550  loss = 2.46439 avg_loss = 3.42274\n",
      "epoch no.2 train no.153560  loss = 2.49278 avg_loss = 3.42586\n",
      "epoch no.2 train no.153570  loss = 2.80151 avg_loss = 3.43600\n",
      "epoch no.2 train no.153580  loss = 3.02198 avg_loss = 3.48311\n",
      "epoch no.2 train no.153590  loss = 3.67445 avg_loss = 3.45170\n",
      "epoch no.2 train no.153600  loss = 3.19911 avg_loss = 3.45201\n",
      "epoch no.2 train no.153610  loss = 3.15836 avg_loss = 3.47020\n",
      "epoch no.2 train no.153620  loss = 2.47787 avg_loss = 3.43017\n",
      "epoch no.2 train no.153630  loss = 4.97360 avg_loss = 3.48305\n",
      "epoch no.2 train no.153640  loss = 2.70494 avg_loss = 3.47547\n",
      "epoch no.2 train no.153650  loss = 5.30391 avg_loss = 3.56297\n",
      "epoch no.2 train no.153660  loss = 2.20924 avg_loss = 3.54420\n",
      "epoch no.2 train no.153670  loss = 3.42314 avg_loss = 3.52619\n",
      "epoch no.2 train no.153680  loss = 3.00126 avg_loss = 3.49508\n",
      "epoch no.2 train no.153690  loss = 4.83640 avg_loss = 3.53887\n",
      "epoch no.2 train no.153700  loss = 2.12858 avg_loss = 3.45904\n",
      "epoch no.2 train no.153710  loss = 3.36153 avg_loss = 3.42264\n",
      "epoch no.2 train no.153720  loss = 2.08946 avg_loss = 3.39447\n",
      "epoch no.2 train no.153730  loss = 3.87088 avg_loss = 3.41072\n",
      "epoch no.2 train no.153740  loss = 1.86914 avg_loss = 3.37342\n",
      "epoch no.2 train no.153750  loss = 2.89677 avg_loss = 3.34274\n",
      "epoch no.2 train no.153760  loss = 3.34091 avg_loss = 3.38586\n",
      "epoch no.2 train no.153770  loss = 3.28160 avg_loss = 3.37418\n",
      "epoch no.2 train no.153780  loss = 3.37347 avg_loss = 3.40735\n",
      "epoch no.2 train no.153790  loss = 2.40583 avg_loss = 3.42443\n",
      "epoch no.2 train no.153800  loss = 4.12224 avg_loss = 3.42297\n",
      "epoch no.2 train no.153810  loss = 3.06891 avg_loss = 3.43933\n",
      "epoch no.2 train no.153820  loss = 5.95670 avg_loss = 3.39158\n",
      "epoch no.2 train no.153830  loss = 5.84724 avg_loss = 3.43909\n",
      "epoch no.2 train no.153840  loss = 3.32044 avg_loss = 3.41617\n",
      "epoch no.2 train no.153850  loss = 3.92623 avg_loss = 3.38922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.153860  loss = 3.62954 avg_loss = 3.37100\n",
      "epoch no.2 train no.153870  loss = 2.95025 avg_loss = 3.34071\n",
      "epoch no.2 train no.153880  loss = 3.32502 avg_loss = 3.37033\n",
      "epoch no.2 train no.153890  loss = 3.30437 avg_loss = 3.36338\n",
      "epoch no.2 train no.153900  loss = 2.30743 avg_loss = 3.35319\n",
      "epoch no.2 train no.153910  loss = 3.64306 avg_loss = 3.41495\n",
      "epoch no.2 train no.153920  loss = 3.09832 avg_loss = 3.46338\n",
      "epoch no.2 train no.153930  loss = 4.05539 avg_loss = 3.48581\n",
      "epoch no.2 train no.153940  loss = 3.94833 avg_loss = 3.49511\n",
      "epoch no.2 train no.153950  loss = 2.20814 avg_loss = 3.44442\n",
      "epoch no.2 train no.153960  loss = 2.19413 avg_loss = 3.44501\n",
      "epoch no.2 train no.153970  loss = 3.61752 avg_loss = 3.45430\n",
      "epoch no.2 train no.153980  loss = 4.97297 avg_loss = 3.45521\n",
      "epoch no.2 train no.153990  loss = 3.70018 avg_loss = 3.47169\n",
      "epoch no.2 train no.154000  loss = 2.59713 avg_loss = 3.46485\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '보', '▁힐링', '든', '▁시간', '</s>']\n",
      "여름밤의 재즈로 물든 시간</s>\n",
      "epoch no.2 train no.154010  loss = 3.32904 avg_loss = 3.47811\n",
      "epoch no.2 train no.154020  loss = 5.24038 avg_loss = 3.48746\n",
      "epoch no.2 train no.154030  loss = 2.41738 avg_loss = 3.45660\n",
      "epoch no.2 train no.154040  loss = 3.26034 avg_loss = 3.44271\n",
      "epoch no.2 train no.154050  loss = 3.77130 avg_loss = 3.44750\n",
      "epoch no.2 train no.154060  loss = 1.55545 avg_loss = 3.47602\n",
      "epoch no.2 train no.154070  loss = 6.38350 avg_loss = 3.52404\n",
      "epoch no.2 train no.154080  loss = 5.44305 avg_loss = 3.47744\n",
      "epoch no.2 train no.154090  loss = 3.59172 avg_loss = 3.50385\n",
      "epoch no.2 train no.154100  loss = 3.20199 avg_loss = 3.55082\n",
      "epoch no.2 train no.154110  loss = 2.51825 avg_loss = 3.54730\n",
      "epoch no.2 train no.154120  loss = 3.19861 avg_loss = 3.58545\n",
      "epoch no.2 train no.154130  loss = 3.98526 avg_loss = 3.58117\n",
      "epoch no.2 train no.154140  loss = 4.08224 avg_loss = 3.54659\n",
      "epoch no.2 train no.154150  loss = 3.29044 avg_loss = 3.52173\n",
      "epoch no.2 train no.154160  loss = 3.61081 avg_loss = 3.53772\n",
      "epoch no.2 train no.154170  loss = 2.93848 avg_loss = 3.50020\n",
      "epoch no.2 train no.154180  loss = 3.50388 avg_loss = 3.47684\n",
      "epoch no.2 train no.154190  loss = 5.62369 avg_loss = 3.49557\n",
      "epoch no.2 train no.154200  loss = 5.66694 avg_loss = 3.54617\n",
      "epoch no.2 train no.154210  loss = 2.98720 avg_loss = 3.50241\n",
      "epoch no.2 train no.154220  loss = 4.50024 avg_loss = 3.48358\n",
      "epoch no.2 train no.154230  loss = 5.79697 avg_loss = 3.51088\n",
      "epoch no.2 train no.154240  loss = 3.27072 avg_loss = 3.53913\n",
      "epoch no.2 train no.154250  loss = 2.56578 avg_loss = 3.57702\n",
      "epoch no.2 train no.154260  loss = 3.02126 avg_loss = 3.58390\n",
      "epoch no.2 train no.154270  loss = 3.79617 avg_loss = 3.53957\n",
      "epoch no.2 train no.154280  loss = 1.73731 avg_loss = 3.48706\n",
      "epoch no.2 train no.154290  loss = 3.16732 avg_loss = 3.46023\n",
      "epoch no.2 train no.154300  loss = 4.17013 avg_loss = 3.52099\n",
      "epoch no.2 train no.154310  loss = 3.55269 avg_loss = 3.52928\n",
      "epoch no.2 train no.154320  loss = 3.37661 avg_loss = 3.51491\n",
      "epoch no.2 train no.154330  loss = 4.33922 avg_loss = 3.48007\n",
      "epoch no.2 train no.154340  loss = 3.61822 avg_loss = 3.45242\n",
      "epoch no.2 train no.154350  loss = 4.49914 avg_loss = 3.42711\n",
      "epoch no.2 train no.154360  loss = 5.37508 avg_loss = 3.41856\n",
      "epoch no.2 train no.154370  loss = 3.22479 avg_loss = 3.38960\n",
      "epoch no.2 train no.154380  loss = 3.39059 avg_loss = 3.35870\n",
      "epoch no.2 train no.154390  loss = 2.80388 avg_loss = 3.30214\n",
      "epoch no.2 train no.154400  loss = 3.53891 avg_loss = 3.30033\n",
      "epoch no.2 train no.154410  loss = 2.76206 avg_loss = 3.31860\n",
      "epoch no.2 train no.154420  loss = 3.77772 avg_loss = 3.35507\n",
      "epoch no.2 train no.154430  loss = 3.07113 avg_loss = 3.27223\n",
      "epoch no.2 train no.154440  loss = 4.72720 avg_loss = 3.29748\n",
      "epoch no.2 train no.154450  loss = 3.84038 avg_loss = 3.33951\n",
      "epoch no.2 train no.154460  loss = 3.97605 avg_loss = 3.35380\n",
      "epoch no.2 train no.154470  loss = 3.12391 avg_loss = 3.37690\n",
      "epoch no.2 train no.154480  loss = 5.02328 avg_loss = 3.32222\n",
      "epoch no.2 train no.154490  loss = 2.93579 avg_loss = 3.33006\n",
      "epoch no.2 train no.154500  loss = 4.43714 avg_loss = 3.30884\n",
      "epoch no.2 train no.154510  loss = 1.35850 avg_loss = 3.33729\n",
      "epoch no.2 train no.154520  loss = 3.89689 avg_loss = 3.32857\n",
      "epoch no.2 train no.154530  loss = 4.11672 avg_loss = 3.34341\n",
      "epoch no.2 train no.154540  loss = 1.29325 avg_loss = 3.34498\n",
      "epoch no.2 train no.154550  loss = 2.50790 avg_loss = 3.31548\n",
      "epoch no.2 train no.154560  loss = 1.85900 avg_loss = 3.29279\n",
      "epoch no.2 train no.154570  loss = 3.29631 avg_loss = 3.28823\n",
      "epoch no.2 train no.154580  loss = 5.86619 avg_loss = 3.27699\n",
      "epoch no.2 train no.154590  loss = 4.29015 avg_loss = 3.26792\n",
      "epoch no.2 train no.154600  loss = 3.77568 avg_loss = 3.27192\n",
      "epoch no.2 train no.154610  loss = 6.03469 avg_loss = 3.31914\n",
      "epoch no.2 train no.154620  loss = 3.48040 avg_loss = 3.28983\n",
      "epoch no.2 train no.154630  loss = 2.47856 avg_loss = 3.31365\n",
      "epoch no.2 train no.154640  loss = 2.05319 avg_loss = 3.34000\n",
      "epoch no.2 train no.154650  loss = 2.63329 avg_loss = 3.32104\n",
      "epoch no.2 train no.154660  loss = 3.79904 avg_loss = 3.34033\n",
      "epoch no.2 train no.154670  loss = 3.15772 avg_loss = 3.34653\n",
      "epoch no.2 train no.154680  loss = 2.64258 avg_loss = 3.32553\n",
      "epoch no.2 train no.154690  loss = 2.72937 avg_loss = 3.31084\n",
      "epoch no.2 train no.154700  loss = 2.20753 avg_loss = 3.31491\n",
      "epoch no.2 train no.154710  loss = 4.91419 avg_loss = 3.40408\n",
      "epoch no.2 train no.154720  loss = 4.56560 avg_loss = 3.41947\n",
      "epoch no.2 train no.154730  loss = 2.71186 avg_loss = 3.44381\n",
      "epoch no.2 train no.154740  loss = 3.78331 avg_loss = 3.44682\n",
      "epoch no.2 train no.154750  loss = 3.02524 avg_loss = 3.45874\n",
      "epoch no.2 train no.154760  loss = 2.84631 avg_loss = 3.43132\n",
      "epoch no.2 train no.154770  loss = 4.16229 avg_loss = 3.40288\n",
      "epoch no.2 train no.154780  loss = 3.44376 avg_loss = 3.39889\n",
      "epoch no.2 train no.154790  loss = 3.86343 avg_loss = 3.39044\n",
      "epoch no.2 train no.154800  loss = 3.39882 avg_loss = 3.39917\n",
      "epoch no.2 train no.154810  loss = 4.52176 avg_loss = 3.45757\n",
      "epoch no.2 train no.154820  loss = 3.88027 avg_loss = 3.49159\n",
      "epoch no.2 train no.154830  loss = 3.88652 avg_loss = 3.49258\n",
      "epoch no.2 train no.154840  loss = 2.65168 avg_loss = 3.52616\n",
      "epoch no.2 train no.154850  loss = 2.45445 avg_loss = 3.51212\n",
      "epoch no.2 train no.154860  loss = 3.54269 avg_loss = 3.51845\n",
      "epoch no.2 train no.154870  loss = 3.70040 avg_loss = 3.52809\n",
      "epoch no.2 train no.154880  loss = 2.45893 avg_loss = 3.48299\n",
      "epoch no.2 train no.154890  loss = 4.53995 avg_loss = 3.45182\n",
      "epoch no.2 train no.154900  loss = 5.10832 avg_loss = 3.50927\n",
      "epoch no.2 train no.154910  loss = 2.13165 avg_loss = 3.44244\n",
      "epoch no.2 train no.154920  loss = 4.45716 avg_loss = 3.44268\n",
      "epoch no.2 train no.154930  loss = 2.84044 avg_loss = 3.40206\n",
      "epoch no.2 train no.154940  loss = 3.38591 avg_loss = 3.37170\n",
      "epoch no.2 train no.154950  loss = 3.74907 avg_loss = 3.37461\n",
      "epoch no.2 train no.154960  loss = 2.44205 avg_loss = 3.34007\n",
      "epoch no.2 train no.154970  loss = 2.81111 avg_loss = 3.35281\n",
      "epoch no.2 train no.154980  loss = 2.16962 avg_loss = 3.30904\n",
      "epoch no.2 train no.154990  loss = 2.13655 avg_loss = 3.31936\n",
      "epoch no.2 train no.155000  loss = 3.08304 avg_loss = 3.35437\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁카페', '로', '▁감성', '</s>']\n",
      "여름밤의 재즈로 힐링</s>\n",
      "epoch no.2 train no.155010  loss = 3.84116 avg_loss = 3.34354\n",
      "epoch no.2 train no.155020  loss = 4.09479 avg_loss = 3.33841\n",
      "epoch no.2 train no.155030  loss = 2.76873 avg_loss = 3.38414\n",
      "epoch no.2 train no.155040  loss = 3.94847 avg_loss = 3.42721\n",
      "epoch no.2 train no.155050  loss = 2.85437 avg_loss = 3.43625\n",
      "epoch no.2 train no.155060  loss = 2.21225 avg_loss = 3.44568\n",
      "epoch no.2 train no.155070  loss = 2.69786 avg_loss = 3.38935\n",
      "epoch no.2 train no.155080  loss = 4.91633 avg_loss = 3.42820\n",
      "epoch no.2 train no.155090  loss = 3.05159 avg_loss = 3.40750\n",
      "epoch no.2 train no.155100  loss = 4.24679 avg_loss = 3.39762\n",
      "epoch no.2 train no.155110  loss = 2.27178 avg_loss = 3.35394\n",
      "epoch no.2 train no.155120  loss = 4.13789 avg_loss = 3.36811\n",
      "epoch no.2 train no.155130  loss = 4.19433 avg_loss = 3.39242\n",
      "epoch no.2 train no.155140  loss = 2.17623 avg_loss = 3.42831\n",
      "epoch no.2 train no.155150  loss = 2.43073 avg_loss = 3.37138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.155160  loss = 5.05728 avg_loss = 3.38654\n",
      "epoch no.2 train no.155170  loss = 4.28980 avg_loss = 3.36027\n",
      "epoch no.2 train no.155180  loss = 3.54190 avg_loss = 3.40288\n",
      "epoch no.2 train no.155190  loss = 2.76021 avg_loss = 3.36766\n",
      "epoch no.2 train no.155200  loss = 3.67144 avg_loss = 3.35393\n",
      "epoch no.2 train no.155210  loss = 3.28645 avg_loss = 3.35834\n",
      "epoch no.2 train no.155220  loss = 3.10252 avg_loss = 3.37123\n",
      "epoch no.2 train no.155230  loss = 4.37243 avg_loss = 3.39210\n",
      "epoch no.2 train no.155240  loss = 2.64985 avg_loss = 3.39657\n",
      "epoch no.2 train no.155250  loss = 3.64655 avg_loss = 3.40771\n",
      "epoch no.2 train no.155260  loss = 4.31036 avg_loss = 3.39610\n",
      "epoch no.2 train no.155270  loss = 3.22640 avg_loss = 3.38649\n",
      "epoch no.2 train no.155280  loss = 2.48677 avg_loss = 3.34741\n",
      "epoch no.2 train no.155290  loss = 3.82851 avg_loss = 3.37769\n",
      "epoch no.2 train no.155300  loss = 4.49070 avg_loss = 3.40910\n",
      "epoch no.2 train no.155310  loss = 3.81493 avg_loss = 3.45063\n",
      "epoch no.2 train no.155320  loss = 5.63736 avg_loss = 3.47867\n",
      "epoch no.2 train no.155330  loss = 4.09761 avg_loss = 3.51191\n",
      "epoch no.2 train no.155340  loss = 1.76268 avg_loss = 3.48070\n",
      "epoch no.2 train no.155350  loss = 3.73218 avg_loss = 3.48607\n",
      "epoch no.2 train no.155360  loss = 3.77259 avg_loss = 3.48615\n",
      "epoch no.2 train no.155370  loss = 6.00148 avg_loss = 3.51319\n",
      "epoch no.2 train no.155380  loss = 3.13697 avg_loss = 3.53760\n",
      "epoch no.2 train no.155390  loss = 3.39932 avg_loss = 3.46623\n",
      "epoch no.2 train no.155400  loss = 2.37252 avg_loss = 3.45513\n",
      "epoch no.2 train no.155410  loss = 3.07823 avg_loss = 3.40524\n",
      "epoch no.2 train no.155420  loss = 4.40499 avg_loss = 3.45465\n",
      "epoch no.2 train no.155430  loss = 3.90354 avg_loss = 3.52487\n",
      "epoch no.2 train no.155440  loss = 2.74167 avg_loss = 3.57250\n",
      "epoch no.2 train no.155450  loss = 3.80447 avg_loss = 3.52680\n",
      "epoch no.2 train no.155460  loss = 2.95404 avg_loss = 3.57235\n",
      "epoch no.2 train no.155470  loss = 4.32319 avg_loss = 3.64945\n",
      "epoch no.2 train no.155480  loss = 2.65221 avg_loss = 3.62508\n",
      "epoch no.2 train no.155490  loss = 2.99917 avg_loss = 3.58075\n",
      "epoch no.2 train no.155500  loss = 2.36630 avg_loss = 3.54410\n",
      "epoch no.2 train no.155510  loss = 3.61051 avg_loss = 3.55021\n",
      "epoch no.2 train no.155520  loss = 3.74460 avg_loss = 3.51182\n",
      "epoch no.2 train no.155530  loss = 5.74156 avg_loss = 3.47305\n",
      "epoch no.2 train no.155540  loss = 4.61254 avg_loss = 3.56534\n",
      "epoch no.2 train no.155550  loss = 3.65347 avg_loss = 3.53763\n",
      "epoch no.2 train no.155560  loss = 3.62479 avg_loss = 3.52492\n",
      "epoch no.2 train no.155570  loss = 3.51494 avg_loss = 3.53310\n",
      "epoch no.2 train no.155580  loss = 5.09106 avg_loss = 3.48636\n",
      "epoch no.2 train no.155590  loss = 4.38739 avg_loss = 3.50498\n",
      "epoch no.2 train no.155600  loss = 3.97305 avg_loss = 3.47924\n",
      "epoch no.2 train no.155610  loss = 2.68315 avg_loss = 3.45259\n",
      "epoch no.2 train no.155620  loss = 2.23875 avg_loss = 3.47037\n",
      "epoch no.2 train no.155630  loss = 3.94627 avg_loss = 3.41664\n",
      "epoch no.2 train no.155640  loss = 4.07150 avg_loss = 3.41367\n",
      "epoch no.2 train no.155650  loss = 3.38599 avg_loss = 3.37149\n",
      "epoch no.2 train no.155660  loss = 3.73978 avg_loss = 3.39889\n",
      "epoch no.2 train no.155670  loss = 3.66069 avg_loss = 3.39034\n",
      "epoch no.2 train no.155680  loss = 1.96935 avg_loss = 3.40533\n",
      "epoch no.2 train no.155690  loss = 1.68479 avg_loss = 3.42287\n",
      "epoch no.2 train no.155700  loss = 2.29423 avg_loss = 3.36502\n",
      "epoch no.2 train no.155710  loss = 1.90675 avg_loss = 3.33432\n",
      "epoch no.2 train no.155720  loss = 4.04202 avg_loss = 3.42943\n",
      "epoch no.2 train no.155730  loss = 3.68258 avg_loss = 3.43194\n",
      "epoch no.2 train no.155740  loss = 3.19978 avg_loss = 3.39940\n",
      "epoch no.2 train no.155750  loss = 4.67915 avg_loss = 3.44699\n",
      "epoch no.2 train no.155760  loss = 4.49476 avg_loss = 3.46090\n",
      "epoch no.2 train no.155770  loss = 3.23525 avg_loss = 3.45602\n",
      "epoch no.2 train no.155780  loss = 2.69662 avg_loss = 3.41056\n",
      "epoch no.2 train no.155790  loss = 2.88049 avg_loss = 3.39504\n",
      "epoch no.2 train no.155800  loss = 4.01179 avg_loss = 3.39231\n",
      "epoch no.2 train no.155810  loss = 2.65755 avg_loss = 3.36120\n",
      "epoch no.2 train no.155820  loss = 3.34752 avg_loss = 3.34573\n",
      "epoch no.2 train no.155830  loss = 3.23981 avg_loss = 3.35483\n",
      "epoch no.2 train no.155840  loss = 2.74034 avg_loss = 3.33901\n",
      "epoch no.2 train no.155850  loss = 2.23310 avg_loss = 3.35696\n",
      "epoch no.2 train no.155860  loss = 2.41552 avg_loss = 3.36058\n",
      "epoch no.2 train no.155870  loss = 3.07913 avg_loss = 3.38098\n",
      "epoch no.2 train no.155880  loss = 2.79304 avg_loss = 3.36046\n",
      "epoch no.2 train no.155890  loss = 2.38474 avg_loss = 3.32168\n",
      "epoch no.2 train no.155900  loss = 2.70602 avg_loss = 3.31381\n",
      "epoch no.2 train no.155910  loss = 3.41750 avg_loss = 3.31511\n",
      "epoch no.2 train no.155920  loss = 2.86832 avg_loss = 3.28297\n",
      "epoch no.2 train no.155930  loss = 4.21776 avg_loss = 3.28299\n",
      "epoch no.2 train no.155940  loss = 3.14125 avg_loss = 3.29614\n",
      "epoch no.2 train no.155950  loss = 5.33880 avg_loss = 3.39338\n",
      "epoch no.2 train no.155960  loss = 3.20573 avg_loss = 3.37483\n",
      "epoch no.2 train no.155970  loss = 3.55381 avg_loss = 3.40149\n",
      "epoch no.2 train no.155980  loss = 2.12857 avg_loss = 3.39433\n",
      "epoch no.2 train no.155990  loss = 3.71567 avg_loss = 3.38911\n",
      "epoch no.2 train no.156000  loss = 3.89350 avg_loss = 3.40119\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기좋은 노래</s>\n",
      "epoch no.2 train no.156010  loss = 2.57292 avg_loss = 3.39987\n",
      "epoch no.2 train no.156020  loss = 2.42013 avg_loss = 3.37528\n",
      "epoch no.2 train no.156030  loss = 3.09292 avg_loss = 3.36881\n",
      "epoch no.2 train no.156040  loss = 2.54165 avg_loss = 3.34936\n",
      "epoch no.2 train no.156050  loss = 2.14989 avg_loss = 3.33281\n",
      "epoch no.2 train no.156060  loss = 3.06804 avg_loss = 3.35648\n",
      "epoch no.2 train no.156070  loss = 2.62053 avg_loss = 3.33895\n",
      "epoch no.2 train no.156080  loss = 3.09043 avg_loss = 3.30801\n",
      "epoch no.2 train no.156090  loss = 2.88433 avg_loss = 3.33603\n",
      "epoch no.2 train no.156100  loss = 2.51614 avg_loss = 3.31665\n",
      "epoch no.2 train no.156110  loss = 4.89661 avg_loss = 3.33405\n",
      "epoch no.2 train no.156120  loss = 4.69725 avg_loss = 3.35309\n",
      "epoch no.2 train no.156130  loss = 2.72243 avg_loss = 3.33995\n",
      "epoch no.2 train no.156140  loss = 3.86539 avg_loss = 3.30919\n",
      "epoch no.2 train no.156150  loss = 6.14575 avg_loss = 3.34481\n",
      "epoch no.2 train no.156160  loss = 2.46890 avg_loss = 3.33891\n",
      "epoch no.2 train no.156170  loss = 4.11046 avg_loss = 3.37496\n",
      "epoch no.2 train no.156180  loss = 2.53930 avg_loss = 3.34901\n",
      "epoch no.2 train no.156190  loss = 4.21632 avg_loss = 3.34057\n",
      "epoch no.2 train no.156200  loss = 3.93909 avg_loss = 3.35991\n",
      "epoch no.2 train no.156210  loss = 2.90178 avg_loss = 3.34269\n",
      "epoch no.2 train no.156220  loss = 2.84610 avg_loss = 3.35710\n",
      "epoch no.2 train no.156230  loss = 4.80104 avg_loss = 3.31179\n",
      "epoch no.2 train no.156240  loss = 3.62642 avg_loss = 3.30101\n",
      "epoch no.2 train no.156250  loss = 3.31975 avg_loss = 3.29599\n",
      "epoch no.2 train no.156260  loss = 3.10445 avg_loss = 3.24903\n",
      "epoch no.2 train no.156270  loss = 3.76394 avg_loss = 3.27941\n",
      "epoch no.2 train no.156280  loss = 3.69791 avg_loss = 3.26148\n",
      "epoch no.2 train no.156290  loss = 4.86719 avg_loss = 3.34678\n",
      "epoch no.2 train no.156300  loss = 2.60809 avg_loss = 3.35639\n",
      "epoch no.2 train no.156310  loss = 3.65670 avg_loss = 3.33564\n",
      "epoch no.2 train no.156320  loss = 2.52192 avg_loss = 3.34056\n",
      "epoch no.2 train no.156330  loss = 4.71484 avg_loss = 3.32020\n",
      "epoch no.2 train no.156340  loss = 4.87291 avg_loss = 3.36590\n",
      "epoch no.2 train no.156350  loss = 2.44627 avg_loss = 3.35876\n",
      "epoch no.2 train no.156360  loss = 3.33526 avg_loss = 3.40380\n",
      "epoch no.2 train no.156370  loss = 4.01118 avg_loss = 3.44898\n",
      "epoch no.2 train no.156380  loss = 5.86051 avg_loss = 3.50356\n",
      "epoch no.2 train no.156390  loss = 4.41610 avg_loss = 3.52925\n",
      "epoch no.2 train no.156400  loss = 3.20097 avg_loss = 3.53992\n",
      "epoch no.2 train no.156410  loss = 1.89672 avg_loss = 3.51219\n",
      "epoch no.2 train no.156420  loss = 4.79474 avg_loss = 3.56537\n",
      "epoch no.2 train no.156430  loss = 3.08587 avg_loss = 3.54653\n",
      "epoch no.2 train no.156440  loss = 3.66677 avg_loss = 3.55470\n",
      "epoch no.2 train no.156450  loss = 2.50055 avg_loss = 3.54416\n",
      "epoch no.2 train no.156460  loss = 3.34072 avg_loss = 3.50093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.156470  loss = 2.27521 avg_loss = 3.48577\n",
      "epoch no.2 train no.156480  loss = 3.08414 avg_loss = 3.46194\n",
      "epoch no.2 train no.156490  loss = 2.26667 avg_loss = 3.44204\n",
      "epoch no.2 train no.156500  loss = 4.76059 avg_loss = 3.39816\n",
      "epoch no.2 train no.156510  loss = 3.31524 avg_loss = 3.37563\n",
      "epoch no.2 train no.156520  loss = 3.43943 avg_loss = 3.40365\n",
      "epoch no.2 train no.156530  loss = 3.65611 avg_loss = 3.40556\n",
      "epoch no.2 train no.156540  loss = 3.80390 avg_loss = 3.40034\n",
      "epoch no.2 train no.156550  loss = 3.25550 avg_loss = 3.45398\n",
      "epoch no.2 train no.156560  loss = 2.55578 avg_loss = 3.40604\n",
      "epoch no.2 train no.156570  loss = 4.10180 avg_loss = 3.37859\n",
      "epoch no.2 train no.156580  loss = 3.69669 avg_loss = 3.39805\n",
      "epoch no.2 train no.156590  loss = 2.76397 avg_loss = 3.37486\n",
      "epoch no.2 train no.156600  loss = 3.84514 avg_loss = 3.40034\n",
      "epoch no.2 train no.156610  loss = 2.41579 avg_loss = 3.38958\n",
      "epoch no.2 train no.156620  loss = 4.36324 avg_loss = 3.37492\n",
      "epoch no.2 train no.156630  loss = 2.60606 avg_loss = 3.31174\n",
      "epoch no.2 train no.156640  loss = 2.28148 avg_loss = 3.33149\n",
      "epoch no.2 train no.156650  loss = 4.01264 avg_loss = 3.32053\n",
      "epoch no.2 train no.156660  loss = 4.85126 avg_loss = 3.35435\n",
      "epoch no.2 train no.156670  loss = 3.55126 avg_loss = 3.38827\n",
      "epoch no.2 train no.156680  loss = 3.50990 avg_loss = 3.39554\n",
      "epoch no.2 train no.156690  loss = 3.46042 avg_loss = 3.42161\n",
      "epoch no.2 train no.156700  loss = 4.43035 avg_loss = 3.40291\n",
      "epoch no.2 train no.156710  loss = 2.52939 avg_loss = 3.39783\n",
      "epoch no.2 train no.156720  loss = 3.40777 avg_loss = 3.39615\n",
      "epoch no.2 train no.156730  loss = 5.57947 avg_loss = 3.43676\n",
      "epoch no.2 train no.156740  loss = 2.96414 avg_loss = 3.42372\n",
      "epoch no.2 train no.156750  loss = 3.79189 avg_loss = 3.45580\n",
      "epoch no.2 train no.156760  loss = 4.03626 avg_loss = 3.43239\n",
      "epoch no.2 train no.156770  loss = 4.79127 avg_loss = 3.44702\n",
      "epoch no.2 train no.156780  loss = 2.54134 avg_loss = 3.44223\n",
      "epoch no.2 train no.156790  loss = 2.97002 avg_loss = 3.43279\n",
      "epoch no.2 train no.156800  loss = 3.55171 avg_loss = 3.45830\n",
      "epoch no.2 train no.156810  loss = 3.30720 avg_loss = 3.46126\n",
      "epoch no.2 train no.156820  loss = 4.61743 avg_loss = 3.44701\n",
      "epoch no.2 train no.156830  loss = 2.25012 avg_loss = 3.44075\n",
      "epoch no.2 train no.156840  loss = 2.05932 avg_loss = 3.43538\n",
      "epoch no.2 train no.156850  loss = 2.98792 avg_loss = 3.43733\n",
      "epoch no.2 train no.156860  loss = 2.41173 avg_loss = 3.38026\n",
      "epoch no.2 train no.156870  loss = 4.92357 avg_loss = 3.39561\n",
      "epoch no.2 train no.156880  loss = 3.23936 avg_loss = 3.39511\n",
      "epoch no.2 train no.156890  loss = 2.00446 avg_loss = 3.33820\n",
      "epoch no.2 train no.156900  loss = 4.54842 avg_loss = 3.37932\n",
      "epoch no.2 train no.156910  loss = 4.57085 avg_loss = 3.41543\n",
      "epoch no.2 train no.156920  loss = 3.50014 avg_loss = 3.46461\n",
      "epoch no.2 train no.156930  loss = 2.79497 avg_loss = 3.42363\n",
      "epoch no.2 train no.156940  loss = 3.16075 avg_loss = 3.41293\n",
      "epoch no.2 train no.156950  loss = 4.69676 avg_loss = 3.38807\n",
      "epoch no.2 train no.156960  loss = 2.70377 avg_loss = 3.36599\n",
      "epoch no.2 train no.156970  loss = 2.40591 avg_loss = 3.38320\n",
      "epoch no.2 train no.156980  loss = 4.17288 avg_loss = 3.39484\n",
      "epoch no.2 train no.156990  loss = 3.33817 avg_loss = 3.45398\n",
      "epoch no.2 train no.157000  loss = 4.09917 avg_loss = 3.44757\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.157010  loss = 3.91408 avg_loss = 3.46419\n",
      "epoch no.2 train no.157020  loss = 4.17512 avg_loss = 3.46060\n",
      "epoch no.2 train no.157030  loss = 3.54475 avg_loss = 3.46256\n",
      "epoch no.2 train no.157040  loss = 4.59219 avg_loss = 3.47046\n",
      "epoch no.2 train no.157050  loss = 3.42527 avg_loss = 3.49223\n",
      "epoch no.2 train no.157060  loss = 3.60493 avg_loss = 3.51160\n",
      "epoch no.2 train no.157070  loss = 3.93918 avg_loss = 3.52952\n",
      "epoch no.2 train no.157080  loss = 3.01413 avg_loss = 3.54040\n",
      "epoch no.2 train no.157090  loss = 2.35184 avg_loss = 3.48355\n",
      "epoch no.2 train no.157100  loss = 3.39005 avg_loss = 3.44562\n",
      "epoch no.2 train no.157110  loss = 5.24405 avg_loss = 3.44023\n",
      "epoch no.2 train no.157120  loss = 2.91939 avg_loss = 3.47670\n",
      "epoch no.2 train no.157130  loss = 3.40810 avg_loss = 3.52213\n",
      "epoch no.2 train no.157140  loss = 4.10365 avg_loss = 3.55106\n",
      "epoch no.2 train no.157150  loss = 3.50835 avg_loss = 3.54649\n",
      "epoch no.2 train no.157160  loss = 3.08391 avg_loss = 3.56453\n",
      "epoch no.2 train no.157170  loss = 1.98363 avg_loss = 3.50797\n",
      "epoch no.2 train no.157180  loss = 4.95603 avg_loss = 3.52494\n",
      "epoch no.2 train no.157190  loss = 3.78568 avg_loss = 3.50312\n",
      "epoch no.2 train no.157200  loss = 3.60552 avg_loss = 3.53686\n",
      "epoch no.2 train no.157210  loss = 5.17800 avg_loss = 3.52506\n",
      "epoch no.2 train no.157220  loss = 3.34417 avg_loss = 3.52134\n",
      "epoch no.2 train no.157230  loss = 3.62287 avg_loss = 3.57004\n",
      "epoch no.2 train no.157240  loss = 3.21494 avg_loss = 3.52351\n",
      "epoch no.2 train no.157250  loss = 2.29239 avg_loss = 3.49324\n",
      "epoch no.2 train no.157260  loss = 3.31557 avg_loss = 3.41906\n",
      "epoch no.2 train no.157270  loss = 4.92436 avg_loss = 3.44117\n",
      "epoch no.2 train no.157280  loss = 1.12535 avg_loss = 3.40697\n",
      "epoch no.2 train no.157290  loss = 3.20416 avg_loss = 3.44973\n",
      "epoch no.2 train no.157300  loss = 2.30657 avg_loss = 3.42246\n",
      "epoch no.2 train no.157310  loss = 3.00566 avg_loss = 3.41715\n",
      "epoch no.2 train no.157320  loss = 4.39168 avg_loss = 3.43752\n",
      "epoch no.2 train no.157330  loss = 2.38579 avg_loss = 3.48661\n",
      "epoch no.2 train no.157340  loss = 1.91750 avg_loss = 3.51308\n",
      "epoch no.2 train no.157350  loss = 3.23727 avg_loss = 3.61608\n",
      "epoch no.2 train no.157360  loss = 2.47235 avg_loss = 3.58683\n",
      "epoch no.2 train no.157370  loss = 2.72947 avg_loss = 3.61462\n",
      "epoch no.2 train no.157380  loss = 1.93217 avg_loss = 3.60859\n",
      "epoch no.2 train no.157390  loss = 4.77662 avg_loss = 3.60000\n",
      "epoch no.2 train no.157400  loss = 3.27098 avg_loss = 3.59663\n",
      "epoch no.2 train no.157410  loss = 3.73652 avg_loss = 3.61602\n",
      "epoch no.2 train no.157420  loss = 4.85914 avg_loss = 3.63644\n",
      "epoch no.2 train no.157430  loss = 3.21159 avg_loss = 3.60990\n",
      "epoch no.2 train no.157440  loss = 3.60632 avg_loss = 3.58085\n",
      "epoch no.2 train no.157450  loss = 2.45887 avg_loss = 3.51369\n",
      "epoch no.2 train no.157460  loss = 2.27013 avg_loss = 3.46582\n",
      "epoch no.2 train no.157470  loss = 3.53558 avg_loss = 3.49528\n",
      "epoch no.2 train no.157480  loss = 3.28432 avg_loss = 3.44609\n",
      "epoch no.2 train no.157490  loss = 2.54039 avg_loss = 3.46520\n",
      "epoch no.2 train no.157500  loss = 2.61250 avg_loss = 3.45229\n",
      "epoch no.2 train no.157510  loss = 2.39768 avg_loss = 3.37666\n",
      "epoch no.2 train no.157520  loss = 3.08024 avg_loss = 3.41642\n",
      "epoch no.2 train no.157530  loss = 4.96545 avg_loss = 3.41571\n",
      "epoch no.2 train no.157540  loss = 2.10314 avg_loss = 3.36343\n",
      "epoch no.2 train no.157550  loss = 3.77598 avg_loss = 3.35844\n",
      "epoch no.2 train no.157560  loss = 2.98239 avg_loss = 3.35718\n",
      "epoch no.2 train no.157570  loss = 4.38098 avg_loss = 3.38215\n",
      "epoch no.2 train no.157580  loss = 4.96833 avg_loss = 3.37860\n",
      "epoch no.2 train no.157590  loss = 3.22810 avg_loss = 3.37764\n",
      "epoch no.2 train no.157600  loss = 2.64205 avg_loss = 3.32409\n",
      "epoch no.2 train no.157610  loss = 3.36462 avg_loss = 3.38237\n",
      "epoch no.2 train no.157620  loss = 4.15107 avg_loss = 3.37727\n",
      "epoch no.2 train no.157630  loss = 2.88673 avg_loss = 3.39332\n",
      "epoch no.2 train no.157640  loss = 2.49990 avg_loss = 3.41625\n",
      "epoch no.2 train no.157650  loss = 2.59469 avg_loss = 3.39509\n",
      "epoch no.2 train no.157660  loss = 5.26635 avg_loss = 3.43086\n",
      "epoch no.2 train no.157670  loss = 2.65571 avg_loss = 3.42130\n",
      "epoch no.2 train no.157680  loss = 3.46458 avg_loss = 3.45217\n",
      "epoch no.2 train no.157690  loss = 5.03340 avg_loss = 3.51105\n",
      "epoch no.2 train no.157700  loss = 4.10097 avg_loss = 3.54693\n",
      "epoch no.2 train no.157710  loss = 2.52767 avg_loss = 3.52295\n",
      "epoch no.2 train no.157720  loss = 4.29837 avg_loss = 3.53135\n",
      "epoch no.2 train no.157730  loss = 4.14092 avg_loss = 3.56768\n",
      "epoch no.2 train no.157740  loss = 1.62339 avg_loss = 3.53247\n",
      "epoch no.2 train no.157750  loss = 4.01388 avg_loss = 3.55462\n",
      "epoch no.2 train no.157760  loss = 5.67355 avg_loss = 3.58895\n",
      "epoch no.2 train no.157770  loss = 2.91765 avg_loss = 3.55817\n",
      "epoch no.2 train no.157780  loss = 1.03486 avg_loss = 3.55117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.157790  loss = 2.39659 avg_loss = 3.51496\n",
      "epoch no.2 train no.157800  loss = 3.02318 avg_loss = 3.53581\n",
      "epoch no.2 train no.157810  loss = 3.36108 avg_loss = 3.49245\n",
      "epoch no.2 train no.157820  loss = 2.95217 avg_loss = 3.49625\n",
      "epoch no.2 train no.157830  loss = 4.02611 avg_loss = 3.54353\n",
      "epoch no.2 train no.157840  loss = 1.92762 avg_loss = 3.51704\n",
      "epoch no.2 train no.157850  loss = 4.01116 avg_loss = 3.60108\n",
      "epoch no.2 train no.157860  loss = 3.76300 avg_loss = 3.61823\n",
      "epoch no.2 train no.157870  loss = 2.93253 avg_loss = 3.56163\n",
      "epoch no.2 train no.157880  loss = 3.00724 avg_loss = 3.57511\n",
      "epoch no.2 train no.157890  loss = 2.40241 avg_loss = 3.58330\n",
      "epoch no.2 train no.157900  loss = 3.92427 avg_loss = 3.54851\n",
      "epoch no.2 train no.157910  loss = 2.29970 avg_loss = 3.51708\n",
      "epoch no.2 train no.157920  loss = 3.90953 avg_loss = 3.53107\n",
      "epoch no.2 train no.157930  loss = 3.49872 avg_loss = 3.55850\n",
      "epoch no.2 train no.157940  loss = 3.36539 avg_loss = 3.52246\n",
      "epoch no.2 train no.157950  loss = 2.77241 avg_loss = 3.54003\n",
      "epoch no.2 train no.157960  loss = 3.43587 avg_loss = 3.50479\n",
      "epoch no.2 train no.157970  loss = 4.04908 avg_loss = 3.54146\n",
      "epoch no.2 train no.157980  loss = 2.59415 avg_loss = 3.51189\n",
      "epoch no.2 train no.157990  loss = 3.26767 avg_loss = 3.50068\n",
      "epoch no.2 train no.158000  loss = 3.61941 avg_loss = 3.46096\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁어울리는', '▁재즈', '힙', '▁노래', '</s>']\n",
      "여름밤에 듣는 감성적인 노래</s>\n",
      "epoch no.2 train no.158010  loss = 3.86737 avg_loss = 3.47725\n",
      "epoch no.2 train no.158020  loss = 4.73802 avg_loss = 3.46555\n",
      "epoch no.2 train no.158030  loss = 2.89461 avg_loss = 3.44371\n",
      "epoch no.2 train no.158040  loss = 4.12468 avg_loss = 3.40285\n",
      "epoch no.2 train no.158050  loss = 2.21083 avg_loss = 3.37531\n",
      "epoch no.2 train no.158060  loss = 3.51863 avg_loss = 3.32864\n",
      "epoch no.2 train no.158070  loss = 1.93656 avg_loss = 3.34415\n",
      "epoch no.2 train no.158080  loss = 3.81175 avg_loss = 3.38753\n",
      "epoch no.2 train no.158090  loss = 3.99032 avg_loss = 3.40657\n",
      "epoch no.2 train no.158100  loss = 2.75421 avg_loss = 3.40112\n",
      "epoch no.2 train no.158110  loss = 4.66486 avg_loss = 3.38968\n",
      "epoch no.2 train no.158120  loss = 1.99549 avg_loss = 3.33956\n",
      "epoch no.2 train no.158130  loss = 2.38471 avg_loss = 3.36042\n",
      "epoch no.2 train no.158140  loss = 4.15941 avg_loss = 3.33950\n",
      "epoch no.2 train no.158150  loss = 3.12386 avg_loss = 3.35342\n",
      "epoch no.2 train no.158160  loss = 3.17663 avg_loss = 3.32008\n",
      "epoch no.2 train no.158170  loss = 3.64930 avg_loss = 3.29535\n",
      "epoch no.2 train no.158180  loss = 2.67502 avg_loss = 3.32550\n",
      "epoch no.2 train no.158190  loss = 3.90662 avg_loss = 3.35678\n",
      "epoch no.2 train no.158200  loss = 3.73379 avg_loss = 3.40558\n",
      "epoch no.2 train no.158210  loss = 2.63075 avg_loss = 3.41927\n",
      "epoch no.2 train no.158220  loss = 4.22531 avg_loss = 3.37670\n",
      "epoch no.2 train no.158230  loss = 5.08977 avg_loss = 3.37201\n",
      "epoch no.2 train no.158240  loss = 1.77009 avg_loss = 3.34961\n",
      "epoch no.2 train no.158250  loss = 3.59699 avg_loss = 3.38747\n",
      "epoch no.2 train no.158260  loss = 1.91054 avg_loss = 3.34788\n",
      "epoch no.2 train no.158270  loss = 4.25512 avg_loss = 3.37127\n",
      "epoch no.2 train no.158280  loss = 3.67415 avg_loss = 3.36831\n",
      "epoch no.2 train no.158290  loss = 1.42520 avg_loss = 3.36504\n",
      "epoch no.2 train no.158300  loss = 3.08870 avg_loss = 3.40246\n",
      "epoch no.2 train no.158310  loss = 1.89371 avg_loss = 3.38780\n",
      "epoch no.2 train no.158320  loss = 3.18745 avg_loss = 3.40706\n",
      "epoch no.2 train no.158330  loss = 3.12879 avg_loss = 3.40035\n",
      "epoch no.2 train no.158340  loss = 3.67173 avg_loss = 3.41719\n",
      "epoch no.2 train no.158350  loss = 2.56561 avg_loss = 3.39802\n",
      "epoch no.2 train no.158360  loss = 5.05844 avg_loss = 3.42241\n",
      "epoch no.2 train no.158370  loss = 3.08420 avg_loss = 3.39578\n",
      "epoch no.2 train no.158380  loss = 2.36937 avg_loss = 3.41656\n",
      "epoch no.2 train no.158390  loss = 2.88122 avg_loss = 3.43627\n",
      "epoch no.2 train no.158400  loss = 2.66454 avg_loss = 3.46581\n",
      "epoch no.2 train no.158410  loss = 3.55048 avg_loss = 3.50216\n",
      "epoch no.2 train no.158420  loss = 3.53697 avg_loss = 3.49157\n",
      "epoch no.2 train no.158430  loss = 2.83890 avg_loss = 3.44073\n",
      "epoch no.2 train no.158440  loss = 2.69635 avg_loss = 3.42363\n",
      "epoch no.2 train no.158450  loss = 3.84991 avg_loss = 3.42616\n",
      "epoch no.2 train no.158460  loss = 3.64363 avg_loss = 3.42806\n",
      "epoch no.2 train no.158470  loss = 2.70102 avg_loss = 3.40267\n",
      "epoch no.2 train no.158480  loss = 2.92480 avg_loss = 3.40945\n",
      "epoch no.2 train no.158490  loss = 4.41976 avg_loss = 3.44223\n",
      "epoch no.2 train no.158500  loss = 3.46579 avg_loss = 3.43806\n",
      "epoch no.2 train no.158510  loss = 2.80456 avg_loss = 3.45213\n",
      "epoch no.2 train no.158520  loss = 2.63702 avg_loss = 3.46913\n",
      "epoch no.2 train no.158530  loss = 3.61153 avg_loss = 3.49190\n",
      "epoch no.2 train no.158540  loss = 2.92197 avg_loss = 3.45649\n",
      "epoch no.2 train no.158550  loss = 2.02958 avg_loss = 3.44120\n",
      "epoch no.2 train no.158560  loss = 3.74382 avg_loss = 3.49179\n",
      "epoch no.2 train no.158570  loss = 3.32836 avg_loss = 3.49243\n",
      "epoch no.2 train no.158580  loss = 4.52675 avg_loss = 3.50517\n",
      "epoch no.2 train no.158590  loss = 5.37185 avg_loss = 3.49656\n",
      "epoch no.2 train no.158600  loss = 3.86801 avg_loss = 3.49190\n",
      "epoch no.2 train no.158610  loss = 3.09023 avg_loss = 3.49829\n",
      "epoch no.2 train no.158620  loss = 3.23358 avg_loss = 3.49421\n",
      "epoch no.2 train no.158630  loss = 3.79843 avg_loss = 3.52219\n",
      "epoch no.2 train no.158640  loss = 2.17332 avg_loss = 3.51933\n",
      "epoch no.2 train no.158650  loss = 3.61724 avg_loss = 3.47696\n",
      "epoch no.2 train no.158660  loss = 4.11937 avg_loss = 3.47136\n",
      "epoch no.2 train no.158670  loss = 3.79590 avg_loss = 3.48148\n",
      "epoch no.2 train no.158680  loss = 2.74385 avg_loss = 3.48924\n",
      "epoch no.2 train no.158690  loss = 4.99304 avg_loss = 3.54563\n",
      "epoch no.2 train no.158700  loss = 5.57435 avg_loss = 3.54462\n",
      "epoch no.2 train no.158710  loss = 4.17647 avg_loss = 3.55110\n",
      "epoch no.2 train no.158720  loss = 4.63508 avg_loss = 3.53909\n",
      "epoch no.2 train no.158730  loss = 3.24174 avg_loss = 3.49469\n",
      "epoch no.2 train no.158740  loss = 2.04044 avg_loss = 3.47119\n",
      "epoch no.2 train no.158750  loss = 3.22286 avg_loss = 3.46207\n",
      "epoch no.2 train no.158760  loss = 3.40775 avg_loss = 3.40766\n",
      "epoch no.2 train no.158770  loss = 2.09864 avg_loss = 3.37066\n",
      "epoch no.2 train no.158780  loss = 3.04378 avg_loss = 3.37423\n",
      "epoch no.2 train no.158790  loss = 2.75753 avg_loss = 3.36227\n",
      "epoch no.2 train no.158800  loss = 3.69309 avg_loss = 3.42153\n",
      "epoch no.2 train no.158810  loss = 3.22898 avg_loss = 3.39067\n",
      "epoch no.2 train no.158820  loss = 2.35287 avg_loss = 3.40813\n",
      "epoch no.2 train no.158830  loss = 1.79625 avg_loss = 3.36462\n",
      "epoch no.2 train no.158840  loss = 3.89431 avg_loss = 3.36359\n",
      "epoch no.2 train no.158850  loss = 2.57437 avg_loss = 3.37509\n",
      "epoch no.2 train no.158860  loss = 2.62321 avg_loss = 3.40884\n",
      "epoch no.2 train no.158870  loss = 2.86270 avg_loss = 3.41982\n",
      "epoch no.2 train no.158880  loss = 3.99372 avg_loss = 3.39474\n",
      "epoch no.2 train no.158890  loss = 5.98507 avg_loss = 3.49723\n",
      "epoch no.2 train no.158900  loss = 3.38936 avg_loss = 3.43398\n",
      "epoch no.2 train no.158910  loss = 5.19140 avg_loss = 3.48155\n",
      "epoch no.2 train no.158920  loss = 3.46553 avg_loss = 3.47883\n",
      "epoch no.2 train no.158930  loss = 3.10907 avg_loss = 3.51262\n",
      "epoch no.2 train no.158940  loss = 3.17212 avg_loss = 3.50882\n",
      "epoch no.2 train no.158950  loss = 2.48839 avg_loss = 3.53538\n",
      "epoch no.2 train no.158960  loss = 3.50480 avg_loss = 3.48092\n",
      "epoch no.2 train no.158970  loss = 2.97472 avg_loss = 3.43990\n",
      "epoch no.2 train no.158980  loss = 2.71640 avg_loss = 3.40692\n",
      "epoch no.2 train no.158990  loss = 3.97950 avg_loss = 3.46595\n",
      "epoch no.2 train no.159000  loss = 5.81240 avg_loss = 3.47811\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁좋은', '▁노래', '</s>']\n",
      "여름날 듣기좋은 노래</s>\n",
      "epoch no.2 train no.159010  loss = 4.23578 avg_loss = 3.47614\n",
      "epoch no.2 train no.159020  loss = 4.09289 avg_loss = 3.47765\n",
      "epoch no.2 train no.159030  loss = 2.43576 avg_loss = 3.42964\n",
      "epoch no.2 train no.159040  loss = 3.47382 avg_loss = 3.47868\n",
      "epoch no.2 train no.159050  loss = 3.50517 avg_loss = 3.46214\n",
      "epoch no.2 train no.159060  loss = 3.88332 avg_loss = 3.43512\n",
      "epoch no.2 train no.159070  loss = 2.99304 avg_loss = 3.38265\n",
      "epoch no.2 train no.159080  loss = 3.08822 avg_loss = 3.37742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.159090  loss = 3.93245 avg_loss = 3.41808\n",
      "epoch no.2 train no.159100  loss = 3.91397 avg_loss = 3.45455\n",
      "epoch no.2 train no.159110  loss = 3.93206 avg_loss = 3.43057\n",
      "epoch no.2 train no.159120  loss = 3.16917 avg_loss = 3.40446\n",
      "epoch no.2 train no.159130  loss = 2.80400 avg_loss = 3.38726\n",
      "epoch no.2 train no.159140  loss = 2.32644 avg_loss = 3.42025\n",
      "epoch no.2 train no.159150  loss = 3.68507 avg_loss = 3.43102\n",
      "epoch no.2 train no.159160  loss = 5.22383 avg_loss = 3.42075\n",
      "epoch no.2 train no.159170  loss = 4.75900 avg_loss = 3.43241\n",
      "epoch no.2 train no.159180  loss = 3.74775 avg_loss = 3.40644\n",
      "epoch no.2 train no.159190  loss = 4.87745 avg_loss = 3.39149\n",
      "epoch no.2 train no.159200  loss = 3.22465 avg_loss = 3.39505\n",
      "epoch no.2 train no.159210  loss = 2.92529 avg_loss = 3.33860\n",
      "epoch no.2 train no.159220  loss = 2.79329 avg_loss = 3.28583\n",
      "epoch no.2 train no.159230  loss = 3.42375 avg_loss = 3.25374\n",
      "epoch no.2 train no.159240  loss = 2.98595 avg_loss = 3.28692\n",
      "epoch no.2 train no.159250  loss = 4.76141 avg_loss = 3.30400\n",
      "epoch no.2 train no.159260  loss = 3.75676 avg_loss = 3.30657\n",
      "epoch no.2 train no.159270  loss = 4.56355 avg_loss = 3.33092\n",
      "epoch no.2 train no.159280  loss = 4.20686 avg_loss = 3.30658\n",
      "epoch no.2 train no.159290  loss = 2.49558 avg_loss = 3.32836\n",
      "epoch no.2 train no.159300  loss = 4.57764 avg_loss = 3.39636\n",
      "epoch no.2 train no.159310  loss = 4.03223 avg_loss = 3.41971\n",
      "epoch no.2 train no.159320  loss = 2.69374 avg_loss = 3.38794\n",
      "epoch no.2 train no.159330  loss = 2.90198 avg_loss = 3.39455\n",
      "epoch no.2 train no.159340  loss = 1.47149 avg_loss = 3.36577\n",
      "epoch no.2 train no.159350  loss = 2.68824 avg_loss = 3.32061\n",
      "epoch no.2 train no.159360  loss = 3.66174 avg_loss = 3.34639\n",
      "epoch no.2 train no.159370  loss = 2.46235 avg_loss = 3.32100\n",
      "epoch no.2 train no.159380  loss = 2.96233 avg_loss = 3.38650\n",
      "epoch no.2 train no.159390  loss = 4.74363 avg_loss = 3.39682\n",
      "epoch no.2 train no.159400  loss = 3.39728 avg_loss = 3.38115\n",
      "epoch no.2 train no.159410  loss = 3.41403 avg_loss = 3.40300\n",
      "epoch no.2 train no.159420  loss = 4.30114 avg_loss = 3.39297\n",
      "epoch no.2 train no.159430  loss = 2.49731 avg_loss = 3.39079\n",
      "epoch no.2 train no.159440  loss = 3.04490 avg_loss = 3.41469\n",
      "epoch no.2 train no.159450  loss = 4.29557 avg_loss = 3.44472\n",
      "epoch no.2 train no.159460  loss = 3.73728 avg_loss = 3.52204\n",
      "epoch no.2 train no.159470  loss = 2.60356 avg_loss = 3.49228\n",
      "epoch no.2 train no.159480  loss = 5.40634 avg_loss = 3.55523\n",
      "epoch no.2 train no.159490  loss = 3.75857 avg_loss = 3.57114\n",
      "epoch no.2 train no.159500  loss = 2.15084 avg_loss = 3.56260\n",
      "epoch no.2 train no.159510  loss = 3.47309 avg_loss = 3.57340\n",
      "epoch no.2 train no.159520  loss = 2.87800 avg_loss = 3.53754\n",
      "epoch no.2 train no.159530  loss = 2.92706 avg_loss = 3.54575\n",
      "epoch no.2 train no.159540  loss = 2.34055 avg_loss = 3.53067\n",
      "epoch no.2 train no.159550  loss = 3.73137 avg_loss = 3.50493\n",
      "epoch no.2 train no.159560  loss = 2.89751 avg_loss = 3.50814\n",
      "epoch no.2 train no.159570  loss = 2.98504 avg_loss = 3.51725\n",
      "epoch no.2 train no.159580  loss = 4.33900 avg_loss = 3.51821\n",
      "epoch no.2 train no.159590  loss = 2.85516 avg_loss = 3.56757\n",
      "epoch no.2 train no.159600  loss = 5.43876 avg_loss = 3.53365\n",
      "epoch no.2 train no.159610  loss = 2.14946 avg_loss = 3.48979\n",
      "epoch no.2 train no.159620  loss = 5.22317 avg_loss = 3.53389\n",
      "epoch no.2 train no.159630  loss = 2.52032 avg_loss = 3.51875\n",
      "epoch no.2 train no.159640  loss = 4.19877 avg_loss = 3.47114\n",
      "epoch no.2 train no.159650  loss = 3.79467 avg_loss = 3.50499\n",
      "epoch no.2 train no.159660  loss = 2.33168 avg_loss = 3.51541\n",
      "epoch no.2 train no.159670  loss = 4.50123 avg_loss = 3.52675\n",
      "epoch no.2 train no.159680  loss = 2.80123 avg_loss = 3.50995\n",
      "epoch no.2 train no.159690  loss = 4.64008 avg_loss = 3.48934\n",
      "epoch no.2 train no.159700  loss = 4.43731 avg_loss = 3.50485\n",
      "epoch no.2 train no.159710  loss = 4.13104 avg_loss = 3.47715\n",
      "epoch no.2 train no.159720  loss = 3.07321 avg_loss = 3.49666\n",
      "epoch no.2 train no.159730  loss = 2.89536 avg_loss = 3.50329\n",
      "epoch no.2 train no.159740  loss = 3.68932 avg_loss = 3.52553\n",
      "epoch no.2 train no.159750  loss = 3.75051 avg_loss = 3.53720\n",
      "epoch no.2 train no.159760  loss = 5.04279 avg_loss = 3.56558\n",
      "epoch no.2 train no.159770  loss = 5.64564 avg_loss = 3.55553\n",
      "epoch no.2 train no.159780  loss = 4.12392 avg_loss = 3.56153\n",
      "epoch no.2 train no.159790  loss = 2.09705 avg_loss = 3.51679\n",
      "epoch no.2 train no.159800  loss = 3.22205 avg_loss = 3.48358\n",
      "epoch no.2 train no.159810  loss = 3.27197 avg_loss = 3.43507\n",
      "epoch no.2 train no.159820  loss = 2.68379 avg_loss = 3.40325\n",
      "epoch no.2 train no.159830  loss = 2.93527 avg_loss = 3.39927\n",
      "epoch no.2 train no.159840  loss = 3.10874 avg_loss = 3.34423\n",
      "epoch no.2 train no.159850  loss = 2.76669 avg_loss = 3.35762\n",
      "epoch no.2 train no.159860  loss = 4.72783 avg_loss = 3.40771\n",
      "epoch no.2 train no.159870  loss = 1.95109 avg_loss = 3.35241\n",
      "epoch no.2 train no.159880  loss = 3.26103 avg_loss = 3.33532\n",
      "epoch no.2 train no.159890  loss = 3.64129 avg_loss = 3.35739\n",
      "epoch no.2 train no.159900  loss = 5.20596 avg_loss = 3.38070\n",
      "epoch no.2 train no.159910  loss = 3.61765 avg_loss = 3.36855\n",
      "epoch no.2 train no.159920  loss = 3.92484 avg_loss = 3.36875\n",
      "epoch no.2 train no.159930  loss = 4.96446 avg_loss = 3.38247\n",
      "epoch no.2 train no.159940  loss = 2.99924 avg_loss = 3.39601\n",
      "epoch no.2 train no.159950  loss = 4.30018 avg_loss = 3.39881\n",
      "epoch no.2 train no.159960  loss = 4.49131 avg_loss = 3.43881\n",
      "epoch no.2 train no.159970  loss = 4.12095 avg_loss = 3.43609\n",
      "epoch no.2 train no.159980  loss = 4.85753 avg_loss = 3.48454\n",
      "epoch no.2 train no.159990  loss = 2.82078 avg_loss = 3.47272\n",
      "epoch no.2 train no.160000  loss = 3.17509 avg_loss = 3.44664\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣기', '▁음악', '</s>']\n",
      "여름밤에 어울리는 음악</s>\n",
      "epoch no.2 train no.160010  loss = 3.69327 avg_loss = 3.43802\n",
      "epoch no.2 train no.160020  loss = 2.20127 avg_loss = 3.41656\n",
      "epoch no.2 train no.160030  loss = 3.37894 avg_loss = 3.45690\n",
      "epoch no.2 train no.160040  loss = 5.56890 avg_loss = 3.47352\n",
      "epoch no.2 train no.160050  loss = 4.23578 avg_loss = 3.47980\n",
      "epoch no.2 train no.160060  loss = 3.91667 avg_loss = 3.49260\n",
      "epoch no.2 train no.160070  loss = 4.19915 avg_loss = 3.51494\n",
      "epoch no.2 train no.160080  loss = 2.62400 avg_loss = 3.47161\n",
      "epoch no.2 train no.160090  loss = 1.89560 avg_loss = 3.41660\n",
      "epoch no.2 train no.160100  loss = 2.44236 avg_loss = 3.40883\n",
      "epoch no.2 train no.160110  loss = 4.37959 avg_loss = 3.40157\n",
      "epoch no.2 train no.160120  loss = 3.46416 avg_loss = 3.39721\n",
      "epoch no.2 train no.160130  loss = 2.83959 avg_loss = 3.38255\n",
      "epoch no.2 train no.160140  loss = 2.91826 avg_loss = 3.39778\n",
      "epoch no.2 train no.160150  loss = 2.17538 avg_loss = 3.37333\n",
      "epoch no.2 train no.160160  loss = 4.16584 avg_loss = 3.39980\n",
      "epoch no.2 train no.160170  loss = 3.51894 avg_loss = 3.42719\n",
      "epoch no.2 train no.160180  loss = 3.07413 avg_loss = 3.44679\n",
      "epoch no.2 train no.160190  loss = 4.80750 avg_loss = 3.47615\n",
      "epoch no.2 train no.160200  loss = 2.27891 avg_loss = 3.45031\n",
      "epoch no.2 train no.160210  loss = 4.52591 avg_loss = 3.44621\n",
      "epoch no.2 train no.160220  loss = 2.77963 avg_loss = 3.43411\n",
      "epoch no.2 train no.160230  loss = 3.37937 avg_loss = 3.38166\n",
      "epoch no.2 train no.160240  loss = 3.83006 avg_loss = 3.36876\n",
      "epoch no.2 train no.160250  loss = 3.04028 avg_loss = 3.33468\n",
      "epoch no.2 train no.160260  loss = 3.53653 avg_loss = 3.33598\n",
      "epoch no.2 train no.160270  loss = 2.75514 avg_loss = 3.36367\n",
      "epoch no.2 train no.160280  loss = 4.05669 avg_loss = 3.33937\n",
      "epoch no.2 train no.160290  loss = 2.70342 avg_loss = 3.28700\n",
      "epoch no.2 train no.160300  loss = 4.30734 avg_loss = 3.33384\n",
      "epoch no.2 train no.160310  loss = 2.74532 avg_loss = 3.32958\n",
      "epoch no.2 train no.160320  loss = 5.16586 avg_loss = 3.42539\n",
      "epoch no.2 train no.160330  loss = 4.20133 avg_loss = 3.42963\n",
      "epoch no.2 train no.160340  loss = 3.17306 avg_loss = 3.41003\n",
      "epoch no.2 train no.160350  loss = 3.18730 avg_loss = 3.41612\n",
      "epoch no.2 train no.160360  loss = 1.95794 avg_loss = 3.41233\n",
      "epoch no.2 train no.160370  loss = 5.16322 avg_loss = 3.45971\n",
      "epoch no.2 train no.160380  loss = 4.56548 avg_loss = 3.42297\n",
      "epoch no.2 train no.160390  loss = 3.32713 avg_loss = 3.45632\n",
      "epoch no.2 train no.160400  loss = 5.06392 avg_loss = 3.48050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.160410  loss = 2.82066 avg_loss = 3.43888\n",
      "epoch no.2 train no.160420  loss = 1.81883 avg_loss = 3.44368\n",
      "epoch no.2 train no.160430  loss = 3.36144 avg_loss = 3.44122\n",
      "epoch no.2 train no.160440  loss = 4.32611 avg_loss = 3.47903\n",
      "epoch no.2 train no.160450  loss = 3.04469 avg_loss = 3.47311\n",
      "epoch no.2 train no.160460  loss = 3.51690 avg_loss = 3.43486\n",
      "epoch no.2 train no.160470  loss = 4.65203 avg_loss = 3.40387\n",
      "epoch no.2 train no.160480  loss = 3.18694 avg_loss = 3.39685\n",
      "epoch no.2 train no.160490  loss = 2.57474 avg_loss = 3.42494\n",
      "epoch no.2 train no.160500  loss = 1.89665 avg_loss = 3.43022\n",
      "epoch no.2 train no.160510  loss = 4.08646 avg_loss = 3.44290\n",
      "epoch no.2 train no.160520  loss = 3.53210 avg_loss = 3.41251\n",
      "epoch no.2 train no.160530  loss = 4.75792 avg_loss = 3.42353\n",
      "epoch no.2 train no.160540  loss = 5.54423 avg_loss = 3.43144\n",
      "epoch no.2 train no.160550  loss = 2.51169 avg_loss = 3.45887\n",
      "epoch no.2 train no.160560  loss = 2.74889 avg_loss = 3.44707\n",
      "epoch no.2 train no.160570  loss = 4.44711 avg_loss = 3.39439\n",
      "epoch no.2 train no.160580  loss = 5.34189 avg_loss = 3.45142\n",
      "epoch no.2 train no.160590  loss = 3.33346 avg_loss = 3.44258\n",
      "epoch no.2 train no.160600  loss = 5.55862 avg_loss = 3.44528\n",
      "epoch no.2 train no.160610  loss = 3.67739 avg_loss = 3.43122\n",
      "epoch no.2 train no.160620  loss = 4.36639 avg_loss = 3.42767\n",
      "epoch no.2 train no.160630  loss = 3.70092 avg_loss = 3.42491\n",
      "epoch no.2 train no.160640  loss = 3.80426 avg_loss = 3.46276\n",
      "epoch no.2 train no.160650  loss = 4.21723 avg_loss = 3.39625\n",
      "epoch no.2 train no.160660  loss = 5.08447 avg_loss = 3.41142\n",
      "epoch no.2 train no.160670  loss = 3.55587 avg_loss = 3.41862\n",
      "epoch no.2 train no.160680  loss = 3.26707 avg_loss = 3.42967\n",
      "epoch no.2 train no.160690  loss = 3.35081 avg_loss = 3.43795\n",
      "epoch no.2 train no.160700  loss = 3.46130 avg_loss = 3.47086\n",
      "epoch no.2 train no.160710  loss = 3.81204 avg_loss = 3.44651\n",
      "epoch no.2 train no.160720  loss = 3.80880 avg_loss = 3.41435\n",
      "epoch no.2 train no.160730  loss = 3.92847 avg_loss = 3.42473\n",
      "epoch no.2 train no.160740  loss = 3.98391 avg_loss = 3.45515\n",
      "epoch no.2 train no.160750  loss = 3.24638 avg_loss = 3.46504\n",
      "epoch no.2 train no.160760  loss = 4.03740 avg_loss = 3.47648\n",
      "epoch no.2 train no.160770  loss = 2.09806 avg_loss = 3.48623\n",
      "epoch no.2 train no.160780  loss = 3.90343 avg_loss = 3.44446\n",
      "epoch no.2 train no.160790  loss = 3.98785 avg_loss = 3.42640\n",
      "epoch no.2 train no.160800  loss = 2.38761 avg_loss = 3.43028\n",
      "epoch no.2 train no.160810  loss = 3.31806 avg_loss = 3.44068\n",
      "epoch no.2 train no.160820  loss = 2.35286 avg_loss = 3.43728\n",
      "epoch no.2 train no.160830  loss = 1.52459 avg_loss = 3.45893\n",
      "epoch no.2 train no.160840  loss = 2.54773 avg_loss = 3.48123\n",
      "epoch no.2 train no.160850  loss = 3.96098 avg_loss = 3.44208\n",
      "epoch no.2 train no.160860  loss = 3.51113 avg_loss = 3.47047\n",
      "epoch no.2 train no.160870  loss = 2.86742 avg_loss = 3.44075\n",
      "epoch no.2 train no.160880  loss = 2.37912 avg_loss = 3.45617\n",
      "epoch no.2 train no.160890  loss = 2.09064 avg_loss = 3.40452\n",
      "epoch no.2 train no.160900  loss = 3.23179 avg_loss = 3.36303\n",
      "epoch no.2 train no.160910  loss = 5.77058 avg_loss = 3.46757\n",
      "epoch no.2 train no.160920  loss = 3.26753 avg_loss = 3.49425\n",
      "epoch no.2 train no.160930  loss = 3.75198 avg_loss = 3.49907\n",
      "epoch no.2 train no.160940  loss = 3.73655 avg_loss = 3.49296\n",
      "epoch no.2 train no.160950  loss = 4.66626 avg_loss = 3.46704\n",
      "epoch no.2 train no.160960  loss = 3.62814 avg_loss = 3.53464\n",
      "epoch no.2 train no.160970  loss = 4.50981 avg_loss = 3.56191\n",
      "epoch no.2 train no.160980  loss = 3.38290 avg_loss = 3.53818\n",
      "epoch no.2 train no.160990  loss = 2.55835 avg_loss = 3.50263\n",
      "epoch no.2 train no.161000  loss = 3.60892 avg_loss = 3.47863\n",
      "3\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '</s>']\n",
      "여름밤의 드라이브</s>\n",
      "epoch no.2 train no.161010  loss = 2.52766 avg_loss = 3.46850\n",
      "epoch no.2 train no.161020  loss = 1.86202 avg_loss = 3.49620\n",
      "epoch no.2 train no.161030  loss = 2.69263 avg_loss = 3.47229\n",
      "epoch no.2 train no.161040  loss = 4.32496 avg_loss = 3.48544\n",
      "epoch no.2 train no.161050  loss = 4.86119 avg_loss = 3.47663\n",
      "epoch no.2 train no.161060  loss = 3.21195 avg_loss = 3.45064\n",
      "epoch no.2 train no.161070  loss = 3.58638 avg_loss = 3.45249\n",
      "epoch no.2 train no.161080  loss = 4.97868 avg_loss = 3.46959\n",
      "epoch no.2 train no.161090  loss = 2.48070 avg_loss = 3.53441\n",
      "epoch no.2 train no.161100  loss = 2.18612 avg_loss = 3.49523\n",
      "epoch no.2 train no.161110  loss = 3.41039 avg_loss = 3.47856\n",
      "epoch no.2 train no.161120  loss = 4.29807 avg_loss = 3.46731\n",
      "epoch no.2 train no.161130  loss = 3.68602 avg_loss = 3.46230\n",
      "epoch no.2 train no.161140  loss = 2.95989 avg_loss = 3.45993\n",
      "epoch no.2 train no.161150  loss = 3.69264 avg_loss = 3.46486\n",
      "epoch no.2 train no.161160  loss = 4.53152 avg_loss = 3.46230\n",
      "epoch no.2 train no.161170  loss = 4.53217 avg_loss = 3.49718\n",
      "epoch no.2 train no.161180  loss = 3.58404 avg_loss = 3.46114\n",
      "epoch no.2 train no.161190  loss = 3.41235 avg_loss = 3.45383\n",
      "epoch no.2 train no.161200  loss = 4.11795 avg_loss = 3.44257\n",
      "epoch no.2 train no.161210  loss = 3.72668 avg_loss = 3.41424\n",
      "epoch no.2 train no.161220  loss = 2.03321 avg_loss = 3.43352\n",
      "epoch no.2 train no.161230  loss = 4.38985 avg_loss = 3.45167\n",
      "epoch no.2 train no.161240  loss = 3.68079 avg_loss = 3.43268\n",
      "epoch no.2 train no.161250  loss = 2.39560 avg_loss = 3.39944\n",
      "epoch no.2 train no.161260  loss = 3.47455 avg_loss = 3.44436\n",
      "epoch no.2 train no.161270  loss = 4.64430 avg_loss = 3.48238\n",
      "epoch no.2 train no.161280  loss = 2.45395 avg_loss = 3.48556\n",
      "epoch no.2 train no.161290  loss = 3.78694 avg_loss = 3.50376\n",
      "epoch no.2 train no.161300  loss = 2.90990 avg_loss = 3.47024\n",
      "epoch no.2 train no.161310  loss = 3.79622 avg_loss = 3.47809\n",
      "epoch no.2 train no.161320  loss = 5.27754 avg_loss = 3.49813\n",
      "epoch no.2 train no.161330  loss = 3.38105 avg_loss = 3.50392\n",
      "epoch no.2 train no.161340  loss = 4.50892 avg_loss = 3.53546\n",
      "epoch no.2 train no.161350  loss = 2.94249 avg_loss = 3.52667\n",
      "epoch no.2 train no.161360  loss = 2.05375 avg_loss = 3.53159\n",
      "epoch no.2 train no.161370  loss = 3.39521 avg_loss = 3.58288\n",
      "epoch no.2 train no.161380  loss = 2.91860 avg_loss = 3.58576\n",
      "epoch no.2 train no.161390  loss = 4.24573 avg_loss = 3.57454\n",
      "epoch no.2 train no.161400  loss = 3.65624 avg_loss = 3.57922\n",
      "epoch no.2 train no.161410  loss = 3.23971 avg_loss = 3.54923\n",
      "epoch no.2 train no.161420  loss = 3.95669 avg_loss = 3.52510\n",
      "epoch no.2 train no.161430  loss = 3.18954 avg_loss = 3.51352\n",
      "epoch no.2 train no.161440  loss = 2.39208 avg_loss = 3.47877\n",
      "epoch no.2 train no.161450  loss = 3.67883 avg_loss = 3.47031\n",
      "epoch no.2 train no.161460  loss = 2.50247 avg_loss = 3.49667\n",
      "epoch no.2 train no.161470  loss = 4.91041 avg_loss = 3.48643\n",
      "epoch no.2 train no.161480  loss = 3.16156 avg_loss = 3.46220\n",
      "epoch no.2 train no.161490  loss = 4.77379 avg_loss = 3.47126\n",
      "epoch no.2 train no.161500  loss = 3.98810 avg_loss = 3.44655\n",
      "epoch no.2 train no.161510  loss = 1.59819 avg_loss = 3.48130\n",
      "epoch no.2 train no.161520  loss = 2.44205 avg_loss = 3.47949\n",
      "epoch no.2 train no.161530  loss = 4.58351 avg_loss = 3.48762\n",
      "epoch no.2 train no.161540  loss = 3.14891 avg_loss = 3.47760\n",
      "epoch no.2 train no.161550  loss = 2.57450 avg_loss = 3.50876\n",
      "epoch no.2 train no.161560  loss = 2.81661 avg_loss = 3.49869\n",
      "epoch no.2 train no.161570  loss = 2.08641 avg_loss = 3.54027\n",
      "epoch no.2 train no.161580  loss = 5.26081 avg_loss = 3.51442\n",
      "epoch no.2 train no.161590  loss = 5.37118 avg_loss = 3.48394\n",
      "epoch no.2 train no.161600  loss = 3.53416 avg_loss = 3.51647\n",
      "epoch no.2 train no.161610  loss = 3.06210 avg_loss = 3.51664\n",
      "epoch no.2 train no.161620  loss = 2.77309 avg_loss = 3.46133\n",
      "epoch no.2 train no.161630  loss = 3.94874 avg_loss = 3.42457\n",
      "epoch no.2 train no.161640  loss = 4.32827 avg_loss = 3.46644\n",
      "epoch no.2 train no.161650  loss = 4.37129 avg_loss = 3.53351\n",
      "epoch no.2 train no.161660  loss = 3.55922 avg_loss = 3.50579\n",
      "epoch no.2 train no.161670  loss = 3.76866 avg_loss = 3.48902\n",
      "epoch no.2 train no.161680  loss = 2.58553 avg_loss = 3.43352\n",
      "epoch no.2 train no.161690  loss = 3.55299 avg_loss = 3.43398\n",
      "epoch no.2 train no.161700  loss = 2.87233 avg_loss = 3.43755\n",
      "epoch no.2 train no.161710  loss = 3.05829 avg_loss = 3.43339\n",
      "epoch no.2 train no.161720  loss = 4.45768 avg_loss = 3.42275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.161730  loss = 2.86044 avg_loss = 3.37455\n",
      "epoch no.2 train no.161740  loss = 3.84775 avg_loss = 3.37376\n",
      "epoch no.2 train no.161750  loss = 2.65271 avg_loss = 3.33945\n",
      "epoch no.2 train no.161760  loss = 3.39257 avg_loss = 3.38996\n",
      "epoch no.2 train no.161770  loss = 4.07346 avg_loss = 3.42662\n",
      "epoch no.2 train no.161780  loss = 2.90386 avg_loss = 3.39117\n",
      "epoch no.2 train no.161790  loss = 3.92637 avg_loss = 3.39141\n",
      "epoch no.2 train no.161800  loss = 3.34251 avg_loss = 3.44497\n",
      "epoch no.2 train no.161810  loss = 3.00390 avg_loss = 3.42315\n",
      "epoch no.2 train no.161820  loss = 3.58211 avg_loss = 3.41952\n",
      "epoch no.2 train no.161830  loss = 1.64449 avg_loss = 3.42474\n",
      "epoch no.2 train no.161840  loss = 2.82924 avg_loss = 3.44655\n",
      "epoch no.2 train no.161850  loss = 2.85791 avg_loss = 3.40926\n",
      "epoch no.2 train no.161860  loss = 4.69101 avg_loss = 3.39409\n",
      "epoch no.2 train no.161870  loss = 5.93116 avg_loss = 3.40189\n",
      "epoch no.2 train no.161880  loss = 3.10171 avg_loss = 3.40282\n",
      "epoch no.2 train no.161890  loss = 3.21343 avg_loss = 3.44669\n",
      "epoch no.2 train no.161900  loss = 3.98949 avg_loss = 3.42266\n",
      "epoch no.2 train no.161910  loss = 2.44397 avg_loss = 3.43041\n",
      "epoch no.2 train no.161920  loss = 3.74750 avg_loss = 3.43384\n",
      "epoch no.2 train no.161930  loss = 4.53104 avg_loss = 3.47979\n",
      "epoch no.2 train no.161940  loss = 3.55584 avg_loss = 3.50341\n",
      "epoch no.2 train no.161950  loss = 3.00931 avg_loss = 3.48883\n",
      "epoch no.2 train no.161960  loss = 2.95948 avg_loss = 3.48731\n",
      "epoch no.2 train no.161970  loss = 3.03638 avg_loss = 3.49969\n",
      "epoch no.2 train no.161980  loss = 2.43881 avg_loss = 3.45655\n",
      "epoch no.2 train no.161990  loss = 4.35487 avg_loss = 3.45770\n",
      "epoch no.2 train no.162000  loss = 2.93939 avg_loss = 3.44323\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁노래', '▁노래', '</s>']\n",
      "여름과 어울리는 신나는 노래</s>\n",
      "epoch no.2 train no.162010  loss = 3.46098 avg_loss = 3.43934\n",
      "epoch no.2 train no.162020  loss = 2.39905 avg_loss = 3.40449\n",
      "epoch no.2 train no.162030  loss = 2.49788 avg_loss = 3.41086\n",
      "epoch no.2 train no.162040  loss = 2.72077 avg_loss = 3.39177\n",
      "epoch no.2 train no.162050  loss = 4.76899 avg_loss = 3.37961\n",
      "epoch no.2 train no.162060  loss = 2.65637 avg_loss = 3.32069\n",
      "epoch no.2 train no.162070  loss = 2.09810 avg_loss = 3.35185\n",
      "epoch no.2 train no.162080  loss = 2.83372 avg_loss = 3.37379\n",
      "epoch no.2 train no.162090  loss = 2.77869 avg_loss = 3.38465\n",
      "epoch no.2 train no.162100  loss = 3.16354 avg_loss = 3.40922\n",
      "epoch no.2 train no.162110  loss = 4.25153 avg_loss = 3.45776\n",
      "epoch no.2 train no.162120  loss = 2.69155 avg_loss = 3.44073\n",
      "epoch no.2 train no.162130  loss = 2.70608 avg_loss = 3.41543\n",
      "epoch no.2 train no.162140  loss = 3.09193 avg_loss = 3.42477\n",
      "epoch no.2 train no.162150  loss = 2.78293 avg_loss = 3.42086\n",
      "epoch no.2 train no.162160  loss = 1.97582 avg_loss = 3.37338\n",
      "epoch no.2 train no.162170  loss = 2.80957 avg_loss = 3.36502\n",
      "epoch no.2 train no.162180  loss = 2.96424 avg_loss = 3.34851\n",
      "epoch no.2 train no.162190  loss = 2.86945 avg_loss = 3.33450\n",
      "epoch no.2 train no.162200  loss = 3.12699 avg_loss = 3.37501\n",
      "epoch no.2 train no.162210  loss = 2.55389 avg_loss = 3.35414\n",
      "epoch no.2 train no.162220  loss = 4.55589 avg_loss = 3.38869\n",
      "epoch no.2 train no.162230  loss = 2.04841 avg_loss = 3.37414\n",
      "epoch no.2 train no.162240  loss = 4.17335 avg_loss = 3.40102\n",
      "epoch no.2 train no.162250  loss = 3.34987 avg_loss = 3.37400\n",
      "epoch no.2 train no.162260  loss = 2.70383 avg_loss = 3.41160\n",
      "epoch no.2 train no.162270  loss = 4.32998 avg_loss = 3.46097\n",
      "epoch no.2 train no.162280  loss = 2.41386 avg_loss = 3.46933\n",
      "epoch no.2 train no.162290  loss = 3.20523 avg_loss = 3.51127\n",
      "epoch no.2 train no.162300  loss = 3.41930 avg_loss = 3.47650\n",
      "epoch no.2 train no.162310  loss = 3.36719 avg_loss = 3.48384\n",
      "epoch no.2 train no.162320  loss = 2.83158 avg_loss = 3.51172\n",
      "epoch no.2 train no.162330  loss = 6.38212 avg_loss = 3.52763\n",
      "epoch no.2 train no.162340  loss = 2.10238 avg_loss = 3.50741\n",
      "epoch no.2 train no.162350  loss = 1.59569 avg_loss = 3.44763\n",
      "epoch no.2 train no.162360  loss = 2.47337 avg_loss = 3.41908\n",
      "epoch no.2 train no.162370  loss = 3.74629 avg_loss = 3.39057\n",
      "epoch no.2 train no.162380  loss = 2.50777 avg_loss = 3.39973\n",
      "epoch no.2 train no.162390  loss = 1.97718 avg_loss = 3.39173\n",
      "epoch no.2 train no.162400  loss = 2.39704 avg_loss = 3.39907\n",
      "epoch no.2 train no.162410  loss = 3.70445 avg_loss = 3.43866\n",
      "epoch no.2 train no.162420  loss = 2.65489 avg_loss = 3.42837\n",
      "epoch no.2 train no.162430  loss = 2.11197 avg_loss = 3.37429\n",
      "epoch no.2 train no.162440  loss = 2.36090 avg_loss = 3.35364\n",
      "epoch no.2 train no.162450  loss = 3.12968 avg_loss = 3.33656\n",
      "epoch no.2 train no.162460  loss = 3.26843 avg_loss = 3.35349\n",
      "epoch no.2 train no.162470  loss = 4.32873 avg_loss = 3.38264\n",
      "epoch no.2 train no.162480  loss = 3.17611 avg_loss = 3.49188\n",
      "epoch no.2 train no.162490  loss = 2.17263 avg_loss = 3.46154\n",
      "epoch no.2 train no.162500  loss = 3.24230 avg_loss = 3.48756\n",
      "epoch no.2 train no.162510  loss = 3.45725 avg_loss = 3.46494\n",
      "epoch no.2 train no.162520  loss = 3.38609 avg_loss = 3.43553\n",
      "epoch no.2 train no.162530  loss = 2.81289 avg_loss = 3.41837\n",
      "epoch no.2 train no.162540  loss = 4.84217 avg_loss = 3.45633\n",
      "epoch no.2 train no.162550  loss = 3.09436 avg_loss = 3.43904\n",
      "epoch no.2 train no.162560  loss = 3.84489 avg_loss = 3.40671\n",
      "epoch no.2 train no.162570  loss = 2.99065 avg_loss = 3.39449\n",
      "epoch no.2 train no.162580  loss = 4.44295 avg_loss = 3.45345\n",
      "epoch no.2 train no.162590  loss = 3.04739 avg_loss = 3.40519\n",
      "epoch no.2 train no.162600  loss = 4.09320 avg_loss = 3.40959\n",
      "epoch no.2 train no.162610  loss = 4.63732 avg_loss = 3.39322\n",
      "epoch no.2 train no.162620  loss = 2.91537 avg_loss = 3.36675\n",
      "epoch no.2 train no.162630  loss = 4.01147 avg_loss = 3.31942\n",
      "epoch no.2 train no.162640  loss = 3.11462 avg_loss = 3.33027\n",
      "epoch no.2 train no.162650  loss = 3.76148 avg_loss = 3.29413\n",
      "epoch no.2 train no.162660  loss = 3.11882 avg_loss = 3.27680\n",
      "epoch no.2 train no.162670  loss = 3.31484 avg_loss = 3.30512\n",
      "epoch no.2 train no.162680  loss = 3.13240 avg_loss = 3.31600\n",
      "epoch no.2 train no.162690  loss = 2.39845 avg_loss = 3.27522\n",
      "epoch no.2 train no.162700  loss = 2.58239 avg_loss = 3.26610\n",
      "epoch no.2 train no.162710  loss = 3.25981 avg_loss = 3.24898\n",
      "epoch no.2 train no.162720  loss = 2.89136 avg_loss = 3.27274\n",
      "epoch no.2 train no.162730  loss = 2.70804 avg_loss = 3.25805\n",
      "epoch no.2 train no.162740  loss = 5.84026 avg_loss = 3.35384\n",
      "epoch no.2 train no.162750  loss = 1.69418 avg_loss = 3.35451\n",
      "epoch no.2 train no.162760  loss = 5.34570 avg_loss = 3.38515\n",
      "epoch no.2 train no.162770  loss = 2.65398 avg_loss = 3.43639\n",
      "epoch no.2 train no.162780  loss = 2.48156 avg_loss = 3.41245\n",
      "epoch no.2 train no.162790  loss = 4.00002 avg_loss = 3.41188\n",
      "epoch no.2 train no.162800  loss = 2.64674 avg_loss = 3.40235\n",
      "epoch no.2 train no.162810  loss = 4.59505 avg_loss = 3.40699\n",
      "epoch no.2 train no.162820  loss = 4.32030 avg_loss = 3.45038\n",
      "epoch no.2 train no.162830  loss = 2.88072 avg_loss = 3.44114\n",
      "epoch no.2 train no.162840  loss = 3.11716 avg_loss = 3.48878\n",
      "epoch no.2 train no.162850  loss = 3.63007 avg_loss = 3.50103\n",
      "epoch no.2 train no.162860  loss = 3.10594 avg_loss = 3.54191\n",
      "epoch no.2 train no.162870  loss = 2.38602 avg_loss = 3.48998\n",
      "epoch no.2 train no.162880  loss = 2.30841 avg_loss = 3.45992\n",
      "epoch no.2 train no.162890  loss = 2.99530 avg_loss = 3.43928\n",
      "epoch no.2 train no.162900  loss = 4.72024 avg_loss = 3.47703\n",
      "epoch no.2 train no.162910  loss = 3.10584 avg_loss = 3.44360\n",
      "epoch no.2 train no.162920  loss = 3.34130 avg_loss = 3.40466\n",
      "epoch no.2 train no.162930  loss = 3.85578 avg_loss = 3.38866\n",
      "epoch no.2 train no.162940  loss = 4.11239 avg_loss = 3.40417\n",
      "epoch no.2 train no.162950  loss = 3.84495 avg_loss = 3.39612\n",
      "epoch no.2 train no.162960  loss = 5.19925 avg_loss = 3.40896\n",
      "epoch no.2 train no.162970  loss = 4.36070 avg_loss = 3.40987\n",
      "epoch no.2 train no.162980  loss = 1.58447 avg_loss = 3.34602\n",
      "epoch no.2 train no.162990  loss = 4.03477 avg_loss = 3.36043\n",
      "epoch no.2 train no.163000  loss = 4.24798 avg_loss = 3.38997\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.163010  loss = 4.09327 avg_loss = 3.43562\n",
      "epoch no.2 train no.163020  loss = 5.06653 avg_loss = 3.45137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.163030  loss = 2.19305 avg_loss = 3.49203\n",
      "epoch no.2 train no.163040  loss = 2.49219 avg_loss = 3.45548\n",
      "epoch no.2 train no.163050  loss = 1.87992 avg_loss = 3.41880\n",
      "epoch no.2 train no.163060  loss = 2.74771 avg_loss = 3.38826\n",
      "epoch no.2 train no.163070  loss = 3.90924 avg_loss = 3.36760\n",
      "epoch no.2 train no.163080  loss = 4.87217 avg_loss = 3.37690\n",
      "epoch no.2 train no.163090  loss = 3.43763 avg_loss = 3.39074\n",
      "epoch no.2 train no.163100  loss = 2.82489 avg_loss = 3.36499\n",
      "epoch no.2 train no.163110  loss = 3.94238 avg_loss = 3.35137\n",
      "epoch no.2 train no.163120  loss = 2.25592 avg_loss = 3.32999\n",
      "epoch no.2 train no.163130  loss = 1.86062 avg_loss = 3.30978\n",
      "epoch no.2 train no.163140  loss = 3.33272 avg_loss = 3.29690\n",
      "epoch no.2 train no.163150  loss = 4.92112 avg_loss = 3.32707\n",
      "epoch no.2 train no.163160  loss = 1.85214 avg_loss = 3.29987\n",
      "epoch no.2 train no.163170  loss = 3.09390 avg_loss = 3.27898\n",
      "epoch no.2 train no.163180  loss = 3.18472 avg_loss = 3.30314\n",
      "epoch no.2 train no.163190  loss = 4.24418 avg_loss = 3.31235\n",
      "epoch no.2 train no.163200  loss = 4.82472 avg_loss = 3.26069\n",
      "epoch no.2 train no.163210  loss = 4.34924 avg_loss = 3.30745\n",
      "epoch no.2 train no.163220  loss = 3.92159 avg_loss = 3.31889\n",
      "epoch no.2 train no.163230  loss = 2.03323 avg_loss = 3.30212\n",
      "epoch no.2 train no.163240  loss = 3.65454 avg_loss = 3.29804\n",
      "epoch no.2 train no.163250  loss = 2.33548 avg_loss = 3.27960\n",
      "epoch no.2 train no.163260  loss = 2.80870 avg_loss = 3.26486\n",
      "epoch no.2 train no.163270  loss = 2.59414 avg_loss = 3.21816\n",
      "epoch no.2 train no.163280  loss = 2.10517 avg_loss = 3.21161\n",
      "epoch no.2 train no.163290  loss = 4.15374 avg_loss = 3.18850\n",
      "epoch no.2 train no.163300  loss = 4.01993 avg_loss = 3.21205\n",
      "epoch no.2 train no.163310  loss = 4.38494 avg_loss = 3.25763\n",
      "epoch no.2 train no.163320  loss = 2.64689 avg_loss = 3.30933\n",
      "epoch no.2 train no.163330  loss = 2.93519 avg_loss = 3.31863\n",
      "epoch no.2 train no.163340  loss = 2.68525 avg_loss = 3.37139\n",
      "epoch no.2 train no.163350  loss = 3.63274 avg_loss = 3.38578\n",
      "epoch no.2 train no.163360  loss = 4.15876 avg_loss = 3.37359\n",
      "epoch no.2 train no.163370  loss = 3.05486 avg_loss = 3.38563\n",
      "epoch no.2 train no.163380  loss = 3.41073 avg_loss = 3.38408\n",
      "epoch no.2 train no.163390  loss = 3.47796 avg_loss = 3.43312\n",
      "epoch no.2 train no.163400  loss = 2.44189 avg_loss = 3.37514\n",
      "epoch no.2 train no.163410  loss = 2.26923 avg_loss = 3.33603\n",
      "epoch no.2 train no.163420  loss = 3.60486 avg_loss = 3.32499\n",
      "epoch no.2 train no.163430  loss = 4.46886 avg_loss = 3.37718\n",
      "epoch no.2 train no.163440  loss = 2.16046 avg_loss = 3.38169\n",
      "epoch no.2 train no.163450  loss = 2.80317 avg_loss = 3.38749\n",
      "epoch no.2 train no.163460  loss = 1.84520 avg_loss = 3.42718\n",
      "epoch no.2 train no.163470  loss = 3.48068 avg_loss = 3.35029\n",
      "epoch no.2 train no.163480  loss = 1.75545 avg_loss = 3.34479\n",
      "epoch no.2 train no.163490  loss = 2.29028 avg_loss = 3.32396\n",
      "epoch no.2 train no.163500  loss = 3.18387 avg_loss = 3.37016\n",
      "epoch no.2 train no.163510  loss = 2.30322 avg_loss = 3.40499\n",
      "epoch no.2 train no.163520  loss = 1.83983 avg_loss = 3.43918\n",
      "epoch no.2 train no.163530  loss = 1.89690 avg_loss = 3.38744\n",
      "epoch no.2 train no.163540  loss = 3.97076 avg_loss = 3.40419\n",
      "epoch no.2 train no.163550  loss = 2.13901 avg_loss = 3.39938\n",
      "epoch no.2 train no.163560  loss = 3.51691 avg_loss = 3.45023\n",
      "epoch no.2 train no.163570  loss = 2.54403 avg_loss = 3.44547\n",
      "epoch no.2 train no.163580  loss = 2.52645 avg_loss = 3.43654\n",
      "epoch no.2 train no.163590  loss = 4.02425 avg_loss = 3.43627\n",
      "epoch no.2 train no.163600  loss = 3.08459 avg_loss = 3.43489\n",
      "epoch no.2 train no.163610  loss = 3.92169 avg_loss = 3.44872\n",
      "epoch no.2 train no.163620  loss = 5.07773 avg_loss = 3.45386\n",
      "epoch no.2 train no.163630  loss = 3.53609 avg_loss = 3.46338\n",
      "epoch no.2 train no.163640  loss = 2.68638 avg_loss = 3.51585\n",
      "epoch no.2 train no.163650  loss = 2.96425 avg_loss = 3.49021\n",
      "epoch no.2 train no.163660  loss = 1.73081 avg_loss = 3.52661\n",
      "epoch no.2 train no.163670  loss = 3.18618 avg_loss = 3.51348\n",
      "epoch no.2 train no.163680  loss = 3.84924 avg_loss = 3.52045\n",
      "epoch no.2 train no.163690  loss = 3.56839 avg_loss = 3.49842\n",
      "epoch no.2 train no.163700  loss = 2.38924 avg_loss = 3.49813\n",
      "epoch no.2 train no.163710  loss = 2.85801 avg_loss = 3.52158\n",
      "epoch no.2 train no.163720  loss = 6.93053 avg_loss = 3.57931\n",
      "epoch no.2 train no.163730  loss = 2.20923 avg_loss = 3.54127\n",
      "epoch no.2 train no.163740  loss = 3.28038 avg_loss = 3.52299\n",
      "epoch no.2 train no.163750  loss = 5.45767 avg_loss = 3.52037\n",
      "epoch no.2 train no.163760  loss = 3.59236 avg_loss = 3.51660\n",
      "epoch no.2 train no.163770  loss = 3.11026 avg_loss = 3.47968\n",
      "epoch no.2 train no.163780  loss = 2.57835 avg_loss = 3.45119\n",
      "epoch no.2 train no.163790  loss = 2.62886 avg_loss = 3.41903\n",
      "epoch no.2 train no.163800  loss = 2.89754 avg_loss = 3.41031\n",
      "epoch no.2 train no.163810  loss = 2.88783 avg_loss = 3.42784\n",
      "epoch no.2 train no.163820  loss = 3.12652 avg_loss = 3.44250\n",
      "epoch no.2 train no.163830  loss = 3.74594 avg_loss = 3.48894\n",
      "epoch no.2 train no.163840  loss = 2.16968 avg_loss = 3.46877\n",
      "epoch no.2 train no.163850  loss = 3.57566 avg_loss = 3.46482\n",
      "epoch no.2 train no.163860  loss = 3.72456 avg_loss = 3.49591\n",
      "epoch no.2 train no.163870  loss = 5.16427 avg_loss = 3.51946\n",
      "epoch no.2 train no.163880  loss = 2.88244 avg_loss = 3.46589\n",
      "epoch no.2 train no.163890  loss = 3.67035 avg_loss = 3.45917\n",
      "epoch no.2 train no.163900  loss = 3.80761 avg_loss = 3.50990\n",
      "epoch no.2 train no.163910  loss = 2.57871 avg_loss = 3.46799\n",
      "epoch no.2 train no.163920  loss = 6.34065 avg_loss = 3.47744\n",
      "epoch no.2 train no.163930  loss = 3.42569 avg_loss = 3.47883\n",
      "epoch no.2 train no.163940  loss = 3.38118 avg_loss = 3.49051\n",
      "epoch no.2 train no.163950  loss = 3.53008 avg_loss = 3.49446\n",
      "epoch no.2 train no.163960  loss = 4.96861 avg_loss = 3.47593\n",
      "epoch no.2 train no.163970  loss = 3.99997 avg_loss = 3.44471\n",
      "epoch no.2 train no.163980  loss = 2.01353 avg_loss = 3.38922\n",
      "epoch no.2 train no.163990  loss = 3.89126 avg_loss = 3.43617\n",
      "epoch no.2 train no.164000  loss = 4.88523 avg_loss = 3.44677\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁위한', '▁감성', '▁뉴', '리스트', '</s>']\n",
      "여름밤을 위한 감성 플레이리스트</s>\n",
      "epoch no.2 train no.164010  loss = 2.73217 avg_loss = 3.42340\n",
      "epoch no.2 train no.164020  loss = 3.01767 avg_loss = 3.44748\n",
      "epoch no.2 train no.164030  loss = 4.02261 avg_loss = 3.45184\n",
      "epoch no.2 train no.164040  loss = 3.74105 avg_loss = 3.41848\n",
      "epoch no.2 train no.164050  loss = 3.87218 avg_loss = 3.38478\n",
      "epoch no.2 train no.164060  loss = 5.08146 avg_loss = 3.38773\n",
      "epoch no.2 train no.164070  loss = 3.50504 avg_loss = 3.39722\n",
      "epoch no.2 train no.164080  loss = 3.99009 avg_loss = 3.42662\n",
      "epoch no.2 train no.164090  loss = 2.16949 avg_loss = 3.41984\n",
      "epoch no.2 train no.164100  loss = 3.81553 avg_loss = 3.42236\n",
      "epoch no.2 train no.164110  loss = 3.41158 avg_loss = 3.44426\n",
      "epoch no.2 train no.164120  loss = 2.84835 avg_loss = 3.41554\n",
      "epoch no.2 train no.164130  loss = 2.76375 avg_loss = 3.41622\n",
      "epoch no.2 train no.164140  loss = 2.80258 avg_loss = 3.43288\n",
      "epoch no.2 train no.164150  loss = 3.30978 avg_loss = 3.45844\n",
      "epoch no.2 train no.164160  loss = 3.43267 avg_loss = 3.47834\n",
      "epoch no.2 train no.164170  loss = 2.58594 avg_loss = 3.51518\n",
      "epoch no.2 train no.164180  loss = 2.23524 avg_loss = 3.53471\n",
      "epoch no.2 train no.164190  loss = 6.24788 avg_loss = 3.54649\n",
      "epoch no.2 train no.164200  loss = 4.24344 avg_loss = 3.53160\n",
      "epoch no.2 train no.164210  loss = 2.01794 avg_loss = 3.48341\n",
      "epoch no.2 train no.164220  loss = 2.52550 avg_loss = 3.46284\n",
      "epoch no.2 train no.164230  loss = 3.10856 avg_loss = 3.45011\n",
      "epoch no.2 train no.164240  loss = 2.17036 avg_loss = 3.51172\n",
      "epoch no.2 train no.164250  loss = 4.24623 avg_loss = 3.49757\n",
      "epoch no.2 train no.164260  loss = 2.57521 avg_loss = 3.46279\n",
      "epoch no.2 train no.164270  loss = 4.89851 avg_loss = 3.45352\n",
      "epoch no.2 train no.164280  loss = 6.55703 avg_loss = 3.44644\n",
      "epoch no.2 train no.164290  loss = 5.27490 avg_loss = 3.50925\n",
      "epoch no.2 train no.164300  loss = 4.33451 avg_loss = 3.45946\n",
      "epoch no.2 train no.164310  loss = 2.68224 avg_loss = 3.42835\n",
      "epoch no.2 train no.164320  loss = 4.51852 avg_loss = 3.45822\n",
      "epoch no.2 train no.164330  loss = 2.80470 avg_loss = 3.41149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.164340  loss = 4.71335 avg_loss = 3.44243\n",
      "epoch no.2 train no.164350  loss = 2.89814 avg_loss = 3.48044\n",
      "epoch no.2 train no.164360  loss = 4.01963 avg_loss = 3.42717\n",
      "epoch no.2 train no.164370  loss = 3.39554 avg_loss = 3.38708\n",
      "epoch no.2 train no.164380  loss = 3.77232 avg_loss = 3.42392\n",
      "epoch no.2 train no.164390  loss = 3.07034 avg_loss = 3.47159\n",
      "epoch no.2 train no.164400  loss = 2.20170 avg_loss = 3.47309\n",
      "epoch no.2 train no.164410  loss = 3.76538 avg_loss = 3.48093\n",
      "epoch no.2 train no.164420  loss = 2.54167 avg_loss = 3.44678\n",
      "epoch no.2 train no.164430  loss = 2.10662 avg_loss = 3.48299\n",
      "epoch no.2 train no.164440  loss = 2.33903 avg_loss = 3.47814\n",
      "epoch no.2 train no.164450  loss = 2.72973 avg_loss = 3.47908\n",
      "epoch no.2 train no.164460  loss = 5.84360 avg_loss = 3.47409\n",
      "epoch no.2 train no.164470  loss = 3.52452 avg_loss = 3.47027\n",
      "epoch no.2 train no.164480  loss = 2.22541 avg_loss = 3.44774\n",
      "epoch no.2 train no.164490  loss = 1.83677 avg_loss = 3.39601\n",
      "epoch no.2 train no.164500  loss = 2.92816 avg_loss = 3.39748\n",
      "epoch no.2 train no.164510  loss = 4.57914 avg_loss = 3.40393\n",
      "epoch no.2 train no.164520  loss = 4.45626 avg_loss = 3.41395\n",
      "epoch no.2 train no.164530  loss = 3.69246 avg_loss = 3.44441\n",
      "epoch no.2 train no.164540  loss = 3.30578 avg_loss = 3.41150\n",
      "epoch no.2 train no.164550  loss = 3.04132 avg_loss = 3.40829\n",
      "epoch no.2 train no.164560  loss = 3.30282 avg_loss = 3.44495\n",
      "epoch no.2 train no.164570  loss = 5.37099 avg_loss = 3.49231\n",
      "epoch no.2 train no.164580  loss = 5.34904 avg_loss = 3.46134\n",
      "epoch no.2 train no.164590  loss = 3.17360 avg_loss = 3.46863\n",
      "epoch no.2 train no.164600  loss = 2.76292 avg_loss = 3.42825\n",
      "epoch no.2 train no.164610  loss = 2.32990 avg_loss = 3.41116\n",
      "epoch no.2 train no.164620  loss = 3.37334 avg_loss = 3.42054\n",
      "epoch no.2 train no.164630  loss = 2.53930 avg_loss = 3.42235\n",
      "epoch no.2 train no.164640  loss = 3.27921 avg_loss = 3.43806\n",
      "epoch no.2 train no.164650  loss = 2.64712 avg_loss = 3.45387\n",
      "epoch no.2 train no.164660  loss = 2.81491 avg_loss = 3.48090\n",
      "epoch no.2 train no.164670  loss = 1.46385 avg_loss = 3.48361\n",
      "epoch no.2 train no.164680  loss = 5.16264 avg_loss = 3.53254\n",
      "epoch no.2 train no.164690  loss = 2.87001 avg_loss = 3.46530\n",
      "epoch no.2 train no.164700  loss = 3.31447 avg_loss = 3.41111\n",
      "epoch no.2 train no.164710  loss = 3.24399 avg_loss = 3.40839\n",
      "epoch no.2 train no.164720  loss = 3.66609 avg_loss = 3.43932\n",
      "epoch no.2 train no.164730  loss = 2.88889 avg_loss = 3.43830\n",
      "epoch no.2 train no.164740  loss = 4.18514 avg_loss = 3.46866\n",
      "epoch no.2 train no.164750  loss = 4.66754 avg_loss = 3.49443\n",
      "epoch no.2 train no.164760  loss = 2.59828 avg_loss = 3.47735\n",
      "epoch no.2 train no.164770  loss = 3.40305 avg_loss = 3.45062\n",
      "epoch no.2 train no.164780  loss = 1.82103 avg_loss = 3.43086\n",
      "epoch no.2 train no.164790  loss = 1.74037 avg_loss = 3.42038\n",
      "epoch no.2 train no.164800  loss = 4.56836 avg_loss = 3.43033\n",
      "epoch no.2 train no.164810  loss = 2.60422 avg_loss = 3.40826\n",
      "epoch no.2 train no.164820  loss = 3.88005 avg_loss = 3.44979\n",
      "epoch no.2 train no.164830  loss = 1.33431 avg_loss = 3.40880\n",
      "epoch no.2 train no.164840  loss = 3.82515 avg_loss = 3.40288\n",
      "epoch no.2 train no.164850  loss = 3.18523 avg_loss = 3.37103\n",
      "epoch no.2 train no.164860  loss = 1.95784 avg_loss = 3.32994\n",
      "epoch no.2 train no.164870  loss = 2.95159 avg_loss = 3.34459\n",
      "epoch no.2 train no.164880  loss = 2.73596 avg_loss = 3.29053\n",
      "epoch no.2 train no.164890  loss = 3.50378 avg_loss = 3.30009\n",
      "epoch no.2 train no.164900  loss = 6.07712 avg_loss = 3.33911\n",
      "epoch no.2 train no.164910  loss = 4.57902 avg_loss = 3.36137\n",
      "epoch no.2 train no.164920  loss = 3.14289 avg_loss = 3.35127\n",
      "epoch no.2 train no.164930  loss = 2.42613 avg_loss = 3.29897\n",
      "epoch no.2 train no.164940  loss = 2.33812 avg_loss = 3.29123\n",
      "epoch no.2 train no.164950  loss = 4.83996 avg_loss = 3.33069\n",
      "epoch no.2 train no.164960  loss = 4.35380 avg_loss = 3.32943\n",
      "epoch no.2 train no.164970  loss = 2.04530 avg_loss = 3.31116\n",
      "epoch no.2 train no.164980  loss = 4.00657 avg_loss = 3.40684\n",
      "epoch no.2 train no.164990  loss = 3.39954 avg_loss = 3.42752\n",
      "epoch no.2 train no.165000  loss = 3.09217 avg_loss = 3.48249\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁어울리는', '▁감성', '적인', '</s>']\n",
      "여름밤에 듣는 감성 피아노</s>\n",
      "epoch no.2 train no.165010  loss = 3.54558 avg_loss = 3.41474\n",
      "epoch no.2 train no.165020  loss = 2.94960 avg_loss = 3.45509\n",
      "epoch no.2 train no.165030  loss = 3.26509 avg_loss = 3.44587\n",
      "epoch no.2 train no.165040  loss = 3.41753 avg_loss = 3.51647\n",
      "epoch no.2 train no.165050  loss = 2.10404 avg_loss = 3.45204\n",
      "epoch no.2 train no.165060  loss = 1.73456 avg_loss = 3.43474\n",
      "epoch no.2 train no.165070  loss = 2.78107 avg_loss = 3.43215\n",
      "epoch no.2 train no.165080  loss = 3.21932 avg_loss = 3.44680\n",
      "epoch no.2 train no.165090  loss = 3.47989 avg_loss = 3.42981\n",
      "epoch no.2 train no.165100  loss = 2.18135 avg_loss = 3.38649\n",
      "epoch no.2 train no.165110  loss = 3.80104 avg_loss = 3.36718\n",
      "epoch no.2 train no.165120  loss = 2.96140 avg_loss = 3.34120\n",
      "epoch no.2 train no.165130  loss = 5.26033 avg_loss = 3.39043\n",
      "epoch no.2 train no.165140  loss = 4.93353 avg_loss = 3.44147\n",
      "epoch no.2 train no.165150  loss = 2.66734 avg_loss = 3.40477\n",
      "epoch no.2 train no.165160  loss = 3.02482 avg_loss = 3.37024\n",
      "epoch no.2 train no.165170  loss = 3.00425 avg_loss = 3.36472\n",
      "epoch no.2 train no.165180  loss = 3.28426 avg_loss = 3.39375\n",
      "epoch no.2 train no.165190  loss = 3.75545 avg_loss = 3.36733\n",
      "epoch no.2 train no.165200  loss = 3.27952 avg_loss = 3.32989\n",
      "epoch no.2 train no.165210  loss = 3.98247 avg_loss = 3.37929\n",
      "epoch no.2 train no.165220  loss = 3.83390 avg_loss = 3.37133\n",
      "epoch no.2 train no.165230  loss = 2.85707 avg_loss = 3.36103\n",
      "epoch no.2 train no.165240  loss = 4.79340 avg_loss = 3.37444\n",
      "epoch no.2 train no.165250  loss = 2.61934 avg_loss = 3.36324\n",
      "epoch no.2 train no.165260  loss = 3.19169 avg_loss = 3.42665\n",
      "epoch no.2 train no.165270  loss = 3.38598 avg_loss = 3.40253\n",
      "epoch no.2 train no.165280  loss = 4.24190 avg_loss = 3.40008\n",
      "epoch no.2 train no.165290  loss = 2.57482 avg_loss = 3.41146\n",
      "epoch no.2 train no.165300  loss = 4.27810 avg_loss = 3.43747\n",
      "epoch no.2 train no.165310  loss = 4.24853 avg_loss = 3.47193\n",
      "epoch no.2 train no.165320  loss = 2.47233 avg_loss = 3.46211\n",
      "epoch no.2 train no.165330  loss = 6.13669 avg_loss = 3.46038\n",
      "epoch no.2 train no.165340  loss = 2.67062 avg_loss = 3.45293\n",
      "epoch no.2 train no.165350  loss = 5.50824 avg_loss = 3.49197\n",
      "epoch no.2 train no.165360  loss = 2.59748 avg_loss = 3.47478\n",
      "epoch no.2 train no.165370  loss = 4.00649 avg_loss = 3.47038\n",
      "epoch no.2 train no.165380  loss = 3.33960 avg_loss = 3.45556\n",
      "epoch no.2 train no.165390  loss = 3.50936 avg_loss = 3.49149\n",
      "epoch no.2 train no.165400  loss = 4.31676 avg_loss = 3.49738\n",
      "epoch no.2 train no.165410  loss = 2.16530 avg_loss = 3.55811\n",
      "epoch no.2 train no.165420  loss = 4.31576 avg_loss = 3.56657\n",
      "epoch no.2 train no.165430  loss = 2.86955 avg_loss = 3.57065\n",
      "epoch no.2 train no.165440  loss = 3.23916 avg_loss = 3.51770\n",
      "epoch no.2 train no.165450  loss = 3.38214 avg_loss = 3.53829\n",
      "epoch no.2 train no.165460  loss = 2.72475 avg_loss = 3.50982\n",
      "epoch no.2 train no.165470  loss = 2.12103 avg_loss = 3.50167\n",
      "epoch no.2 train no.165480  loss = 2.82334 avg_loss = 3.52266\n",
      "epoch no.2 train no.165490  loss = 5.74386 avg_loss = 3.53116\n",
      "epoch no.2 train no.165500  loss = 3.68857 avg_loss = 3.50294\n",
      "epoch no.2 train no.165510  loss = 2.69322 avg_loss = 3.49475\n",
      "epoch no.2 train no.165520  loss = 2.96573 avg_loss = 3.47663\n",
      "epoch no.2 train no.165530  loss = 3.04512 avg_loss = 3.47770\n",
      "epoch no.2 train no.165540  loss = 2.23076 avg_loss = 3.48062\n",
      "epoch no.2 train no.165550  loss = 3.31472 avg_loss = 3.48608\n",
      "epoch no.2 train no.165560  loss = 4.12280 avg_loss = 3.51495\n",
      "epoch no.2 train no.165570  loss = 2.32182 avg_loss = 3.52375\n",
      "epoch no.2 train no.165580  loss = 4.00440 avg_loss = 3.54081\n",
      "epoch no.2 train no.165590  loss = 3.37174 avg_loss = 3.51481\n",
      "epoch no.2 train no.165600  loss = 4.98234 avg_loss = 3.48432\n",
      "epoch no.2 train no.165610  loss = 3.05804 avg_loss = 3.53422\n",
      "epoch no.2 train no.165620  loss = 2.70537 avg_loss = 3.52768\n",
      "epoch no.2 train no.165630  loss = 3.90807 avg_loss = 3.54978\n",
      "epoch no.2 train no.165640  loss = 2.63779 avg_loss = 3.56394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.165650  loss = 4.16598 avg_loss = 3.56943\n",
      "epoch no.2 train no.165660  loss = 3.27946 avg_loss = 3.53723\n",
      "epoch no.2 train no.165670  loss = 4.28862 avg_loss = 3.50993\n",
      "epoch no.2 train no.165680  loss = 2.89946 avg_loss = 3.51660\n",
      "epoch no.2 train no.165690  loss = 3.27462 avg_loss = 3.52917\n",
      "epoch no.2 train no.165700  loss = 2.96664 avg_loss = 3.57470\n",
      "epoch no.2 train no.165710  loss = 3.20190 avg_loss = 3.53432\n",
      "epoch no.2 train no.165720  loss = 3.49626 avg_loss = 3.52797\n",
      "epoch no.2 train no.165730  loss = 2.24910 avg_loss = 3.47742\n",
      "epoch no.2 train no.165740  loss = 2.90812 avg_loss = 3.48261\n",
      "epoch no.2 train no.165750  loss = 3.13670 avg_loss = 3.46568\n",
      "epoch no.2 train no.165760  loss = 2.45761 avg_loss = 3.41225\n",
      "epoch no.2 train no.165770  loss = 3.25098 avg_loss = 3.39068\n",
      "epoch no.2 train no.165780  loss = 2.47210 avg_loss = 3.38229\n",
      "epoch no.2 train no.165790  loss = 2.44129 avg_loss = 3.45872\n",
      "epoch no.2 train no.165800  loss = 5.83112 avg_loss = 3.49844\n",
      "epoch no.2 train no.165810  loss = 2.63916 avg_loss = 3.49113\n",
      "epoch no.2 train no.165820  loss = 2.50697 avg_loss = 3.50094\n",
      "epoch no.2 train no.165830  loss = 4.83190 avg_loss = 3.52360\n",
      "epoch no.2 train no.165840  loss = 3.63422 avg_loss = 3.54685\n",
      "epoch no.2 train no.165850  loss = 3.42312 avg_loss = 3.50170\n",
      "epoch no.2 train no.165860  loss = 2.30334 avg_loss = 3.51599\n",
      "epoch no.2 train no.165870  loss = 3.05472 avg_loss = 3.51681\n",
      "epoch no.2 train no.165880  loss = 3.98409 avg_loss = 3.52731\n",
      "epoch no.2 train no.165890  loss = 3.03887 avg_loss = 3.54076\n",
      "epoch no.2 train no.165900  loss = 4.16661 avg_loss = 3.55507\n",
      "epoch no.2 train no.165910  loss = 5.62370 avg_loss = 3.53827\n",
      "epoch no.2 train no.165920  loss = 3.22641 avg_loss = 3.50282\n",
      "epoch no.2 train no.165930  loss = 2.95536 avg_loss = 3.49223\n",
      "epoch no.2 train no.165940  loss = 2.50528 avg_loss = 3.47921\n",
      "epoch no.2 train no.165950  loss = 2.76839 avg_loss = 3.50209\n",
      "epoch no.2 train no.165960  loss = 4.39214 avg_loss = 3.52589\n",
      "epoch no.2 train no.165970  loss = 4.34452 avg_loss = 3.54468\n",
      "epoch no.2 train no.165980  loss = 2.80650 avg_loss = 3.55701\n",
      "epoch no.2 train no.165990  loss = 3.57460 avg_loss = 3.57728\n",
      "epoch no.2 train no.166000  loss = 4.57757 avg_loss = 3.51419\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁노래', '들', '</s>']\n",
      "여름과 어울리는 음악들</s>\n",
      "epoch no.2 train no.166010  loss = 3.47517 avg_loss = 3.48338\n",
      "epoch no.2 train no.166020  loss = 2.74379 avg_loss = 3.49483\n",
      "epoch no.2 train no.166030  loss = 4.83229 avg_loss = 3.52023\n",
      "epoch no.2 train no.166040  loss = 2.78216 avg_loss = 3.53297\n",
      "epoch no.2 train no.166050  loss = 2.53741 avg_loss = 3.53537\n",
      "epoch no.2 train no.166060  loss = 4.21352 avg_loss = 3.53449\n",
      "epoch no.2 train no.166070  loss = 2.24826 avg_loss = 3.58258\n",
      "epoch no.2 train no.166080  loss = 4.58738 avg_loss = 3.60639\n",
      "epoch no.2 train no.166090  loss = 4.44229 avg_loss = 3.58221\n",
      "epoch no.2 train no.166100  loss = 2.29238 avg_loss = 3.53584\n",
      "epoch no.2 train no.166110  loss = 3.18001 avg_loss = 3.53879\n",
      "epoch no.2 train no.166120  loss = 3.08905 avg_loss = 3.52670\n",
      "epoch no.2 train no.166130  loss = 2.46677 avg_loss = 3.55452\n",
      "epoch no.2 train no.166140  loss = 3.61764 avg_loss = 3.59478\n",
      "epoch no.2 train no.166150  loss = 3.56939 avg_loss = 3.59282\n",
      "epoch no.2 train no.166160  loss = 4.09727 avg_loss = 3.58062\n",
      "epoch no.2 train no.166170  loss = 2.58613 avg_loss = 3.57063\n",
      "epoch no.2 train no.166180  loss = 2.77278 avg_loss = 3.57804\n",
      "epoch no.2 train no.166190  loss = 2.48901 avg_loss = 3.56560\n",
      "epoch no.2 train no.166200  loss = 2.18340 avg_loss = 3.54565\n",
      "epoch no.2 train no.166210  loss = 5.11349 avg_loss = 3.54792\n",
      "epoch no.2 train no.166220  loss = 2.43182 avg_loss = 3.53101\n",
      "epoch no.2 train no.166230  loss = 2.35486 avg_loss = 3.51283\n",
      "epoch no.2 train no.166240  loss = 4.39631 avg_loss = 3.53449\n",
      "epoch no.2 train no.166250  loss = 2.99863 avg_loss = 3.53958\n",
      "epoch no.2 train no.166260  loss = 4.62085 avg_loss = 3.57138\n",
      "epoch no.2 train no.166270  loss = 2.72660 avg_loss = 3.59318\n",
      "epoch no.2 train no.166280  loss = 5.85742 avg_loss = 3.58881\n",
      "epoch no.2 train no.166290  loss = 3.15122 avg_loss = 3.55921\n",
      "epoch no.2 train no.166300  loss = 5.83045 avg_loss = 3.58679\n",
      "epoch no.2 train no.166310  loss = 2.97374 avg_loss = 3.58150\n",
      "epoch no.2 train no.166320  loss = 4.22857 avg_loss = 3.60001\n",
      "epoch no.2 train no.166330  loss = 4.29945 avg_loss = 3.58755\n",
      "epoch no.2 train no.166340  loss = 3.27048 avg_loss = 3.55659\n",
      "epoch no.2 train no.166350  loss = 3.01115 avg_loss = 3.51927\n",
      "epoch no.2 train no.166360  loss = 2.19013 avg_loss = 3.48767\n",
      "epoch no.2 train no.166370  loss = 4.03764 avg_loss = 3.44372\n",
      "epoch no.2 train no.166380  loss = 4.39494 avg_loss = 3.40985\n",
      "epoch no.2 train no.166390  loss = 2.55665 avg_loss = 3.44622\n",
      "epoch no.2 train no.166400  loss = 2.17293 avg_loss = 3.42000\n",
      "epoch no.2 train no.166410  loss = 4.37049 avg_loss = 3.45630\n",
      "epoch no.2 train no.166420  loss = 4.74229 avg_loss = 3.46529\n",
      "epoch no.2 train no.166430  loss = 3.83195 avg_loss = 3.50812\n",
      "epoch no.2 train no.166440  loss = 2.93901 avg_loss = 3.49764\n",
      "epoch no.2 train no.166450  loss = 3.03735 avg_loss = 3.44792\n",
      "epoch no.2 train no.166460  loss = 3.05168 avg_loss = 3.47110\n",
      "epoch no.2 train no.166470  loss = 3.54530 avg_loss = 3.40836\n",
      "epoch no.2 train no.166480  loss = 3.57564 avg_loss = 3.42276\n",
      "epoch no.2 train no.166490  loss = 2.75479 avg_loss = 3.42261\n",
      "epoch no.2 train no.166500  loss = 3.75936 avg_loss = 3.42027\n",
      "epoch no.2 train no.166510  loss = 2.73482 avg_loss = 3.36643\n",
      "epoch no.2 train no.166520  loss = 2.29574 avg_loss = 3.34219\n",
      "epoch no.2 train no.166530  loss = 4.15245 avg_loss = 3.37945\n",
      "epoch no.2 train no.166540  loss = 3.34394 avg_loss = 3.37860\n",
      "epoch no.2 train no.166550  loss = 2.76106 avg_loss = 3.35671\n",
      "epoch no.2 train no.166560  loss = 2.86682 avg_loss = 3.33463\n",
      "epoch no.2 train no.166570  loss = 2.78781 avg_loss = 3.36289\n",
      "epoch no.2 train no.166580  loss = 2.97610 avg_loss = 3.41661\n",
      "epoch no.2 train no.166590  loss = 1.48627 avg_loss = 3.40195\n",
      "epoch no.2 train no.166600  loss = 2.76435 avg_loss = 3.35305\n",
      "epoch no.2 train no.166610  loss = 3.28997 avg_loss = 3.42625\n",
      "epoch no.2 train no.166620  loss = 2.05177 avg_loss = 3.38767\n",
      "epoch no.2 train no.166630  loss = 6.78085 avg_loss = 3.47754\n",
      "epoch no.2 train no.166640  loss = 3.22077 avg_loss = 3.43566\n",
      "epoch no.2 train no.166650  loss = 3.63315 avg_loss = 3.45009\n",
      "epoch no.2 train no.166660  loss = 3.63063 avg_loss = 3.43818\n",
      "epoch no.2 train no.166670  loss = 2.88466 avg_loss = 3.41709\n",
      "epoch no.2 train no.166680  loss = 2.47510 avg_loss = 3.41010\n",
      "epoch no.2 train no.166690  loss = 3.36408 avg_loss = 3.48866\n",
      "epoch no.2 train no.166700  loss = 3.07599 avg_loss = 3.54316\n",
      "epoch no.2 train no.166710  loss = 2.58444 avg_loss = 3.50928\n",
      "epoch no.2 train no.166720  loss = 3.98861 avg_loss = 3.52042\n",
      "epoch no.2 train no.166730  loss = 4.14374 avg_loss = 3.57721\n",
      "epoch no.2 train no.166740  loss = 1.88820 avg_loss = 3.51044\n",
      "epoch no.2 train no.166750  loss = 3.74726 avg_loss = 3.47881\n",
      "epoch no.2 train no.166760  loss = 3.04015 avg_loss = 3.46146\n",
      "epoch no.2 train no.166770  loss = 3.88165 avg_loss = 3.45968\n",
      "epoch no.2 train no.166780  loss = 2.23081 avg_loss = 3.46055\n",
      "epoch no.2 train no.166790  loss = 4.26855 avg_loss = 3.46202\n",
      "epoch no.2 train no.166800  loss = 3.31278 avg_loss = 3.43668\n",
      "epoch no.2 train no.166810  loss = 2.22763 avg_loss = 3.44889\n",
      "epoch no.2 train no.166820  loss = 2.40478 avg_loss = 3.45150\n",
      "epoch no.2 train no.166830  loss = 3.36914 avg_loss = 3.44451\n",
      "epoch no.2 train no.166840  loss = 3.11155 avg_loss = 3.39969\n",
      "epoch no.2 train no.166850  loss = 3.85311 avg_loss = 3.45804\n",
      "epoch no.2 train no.166860  loss = 4.13802 avg_loss = 3.43716\n",
      "epoch no.2 train no.166870  loss = 4.57053 avg_loss = 3.47169\n",
      "epoch no.2 train no.166880  loss = 3.50023 avg_loss = 3.48484\n",
      "epoch no.2 train no.166890  loss = 4.53688 avg_loss = 3.50202\n",
      "epoch no.2 train no.166900  loss = 3.28199 avg_loss = 3.53252\n",
      "epoch no.2 train no.166910  loss = 2.90553 avg_loss = 3.55318\n",
      "epoch no.2 train no.166920  loss = 3.45274 avg_loss = 3.58011\n",
      "epoch no.2 train no.166930  loss = 3.05552 avg_loss = 3.58117\n",
      "epoch no.2 train no.166940  loss = 7.46305 avg_loss = 3.60244\n",
      "epoch no.2 train no.166950  loss = 4.27050 avg_loss = 3.56260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.166960  loss = 3.95243 avg_loss = 3.51312\n",
      "epoch no.2 train no.166970  loss = 2.68383 avg_loss = 3.49521\n",
      "epoch no.2 train no.166980  loss = 3.11954 avg_loss = 3.47885\n",
      "epoch no.2 train no.166990  loss = 4.08248 avg_loss = 3.45061\n",
      "epoch no.2 train no.167000  loss = 2.60643 avg_loss = 3.43292\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '발', '</s>', '</s>']\n",
      "여름밤에 듣는 감성 팝송</s>\n",
      "epoch no.2 train no.167010  loss = 2.71133 avg_loss = 3.45999\n",
      "epoch no.2 train no.167020  loss = 4.57260 avg_loss = 3.43323\n",
      "epoch no.2 train no.167030  loss = 3.43313 avg_loss = 3.42137\n",
      "epoch no.2 train no.167040  loss = 2.42000 avg_loss = 3.38032\n",
      "epoch no.2 train no.167050  loss = 2.24222 avg_loss = 3.39342\n",
      "epoch no.2 train no.167060  loss = 3.08626 avg_loss = 3.37580\n",
      "epoch no.2 train no.167070  loss = 4.93328 avg_loss = 3.40063\n",
      "epoch no.2 train no.167080  loss = 3.91259 avg_loss = 3.47768\n",
      "epoch no.2 train no.167090  loss = 3.98962 avg_loss = 3.53329\n",
      "epoch no.2 train no.167100  loss = 2.67745 avg_loss = 3.50003\n",
      "epoch no.2 train no.167110  loss = 3.44509 avg_loss = 3.49649\n",
      "epoch no.2 train no.167120  loss = 3.59108 avg_loss = 3.47221\n",
      "epoch no.2 train no.167130  loss = 3.09696 avg_loss = 3.47100\n",
      "epoch no.2 train no.167140  loss = 3.20198 avg_loss = 3.48674\n",
      "epoch no.2 train no.167150  loss = 3.19112 avg_loss = 3.47355\n",
      "epoch no.2 train no.167160  loss = 1.38137 avg_loss = 3.40248\n",
      "epoch no.2 train no.167170  loss = 4.79638 avg_loss = 3.44749\n",
      "epoch no.2 train no.167180  loss = 3.70050 avg_loss = 3.44679\n",
      "epoch no.2 train no.167190  loss = 2.25150 avg_loss = 3.40630\n",
      "epoch no.2 train no.167200  loss = 4.64593 avg_loss = 3.43211\n",
      "epoch no.2 train no.167210  loss = 2.99056 avg_loss = 3.47035\n",
      "epoch no.2 train no.167220  loss = 4.78184 avg_loss = 3.52210\n",
      "epoch no.2 train no.167230  loss = 2.97322 avg_loss = 3.53310\n",
      "epoch no.2 train no.167240  loss = 6.02265 avg_loss = 3.53051\n",
      "epoch no.2 train no.167250  loss = 3.77974 avg_loss = 3.53348\n",
      "epoch no.2 train no.167260  loss = 2.95834 avg_loss = 3.53041\n",
      "epoch no.2 train no.167270  loss = 3.47243 avg_loss = 3.52049\n",
      "epoch no.2 train no.167280  loss = 1.92462 avg_loss = 3.52091\n",
      "epoch no.2 train no.167290  loss = 3.38588 avg_loss = 3.50448\n",
      "epoch no.2 train no.167300  loss = 3.07593 avg_loss = 3.54062\n",
      "epoch no.2 train no.167310  loss = 3.71276 avg_loss = 3.52438\n",
      "epoch no.2 train no.167320  loss = 3.99715 avg_loss = 3.56418\n",
      "epoch no.2 train no.167330  loss = 2.62686 avg_loss = 3.53786\n",
      "epoch no.2 train no.167340  loss = 4.72492 avg_loss = 3.53862\n",
      "epoch no.2 train no.167350  loss = 2.88926 avg_loss = 3.50691\n",
      "epoch no.2 train no.167360  loss = 3.82646 avg_loss = 3.51665\n",
      "epoch no.2 train no.167370  loss = 1.99695 avg_loss = 3.47395\n",
      "epoch no.2 train no.167380  loss = 2.80784 avg_loss = 3.48960\n",
      "epoch no.2 train no.167390  loss = 2.46223 avg_loss = 3.49397\n",
      "epoch no.2 train no.167400  loss = 4.07776 avg_loss = 3.46896\n",
      "epoch no.2 train no.167410  loss = 4.21683 avg_loss = 3.53582\n",
      "epoch no.2 train no.167420  loss = 3.72240 avg_loss = 3.52148\n",
      "epoch no.2 train no.167430  loss = 4.12570 avg_loss = 3.52312\n",
      "epoch no.2 train no.167440  loss = 3.51269 avg_loss = 3.44739\n",
      "epoch no.2 train no.167450  loss = 3.58384 avg_loss = 3.44206\n",
      "epoch no.2 train no.167460  loss = 3.79246 avg_loss = 3.45312\n",
      "epoch no.2 train no.167470  loss = 4.09515 avg_loss = 3.45305\n",
      "epoch no.2 train no.167480  loss = 2.34370 avg_loss = 3.48647\n",
      "epoch no.2 train no.167490  loss = 2.66590 avg_loss = 3.43835\n",
      "epoch no.2 train no.167500  loss = 2.67249 avg_loss = 3.45337\n",
      "epoch no.2 train no.167510  loss = 3.36690 avg_loss = 3.41128\n",
      "epoch no.2 train no.167520  loss = 2.00406 avg_loss = 3.43057\n",
      "epoch no.2 train no.167530  loss = 2.98188 avg_loss = 3.47757\n",
      "epoch no.2 train no.167540  loss = 2.83446 avg_loss = 3.43513\n",
      "epoch no.2 train no.167550  loss = 2.51332 avg_loss = 3.47368\n",
      "epoch no.2 train no.167560  loss = 1.60584 avg_loss = 3.45372\n",
      "epoch no.2 train no.167570  loss = 4.23257 avg_loss = 3.42378\n",
      "epoch no.2 train no.167580  loss = 3.66163 avg_loss = 3.42524\n",
      "epoch no.2 train no.167590  loss = 2.89158 avg_loss = 3.44986\n",
      "epoch no.2 train no.167600  loss = 3.23298 avg_loss = 3.44438\n",
      "epoch no.2 train no.167610  loss = 1.75824 avg_loss = 3.41344\n",
      "epoch no.2 train no.167620  loss = 6.05357 avg_loss = 3.43253\n",
      "epoch no.2 train no.167630  loss = 2.92718 avg_loss = 3.41347\n",
      "epoch no.2 train no.167640  loss = 2.98887 avg_loss = 3.38803\n",
      "epoch no.2 train no.167650  loss = 2.83119 avg_loss = 3.33789\n",
      "epoch no.2 train no.167660  loss = 3.38674 avg_loss = 3.33037\n",
      "epoch no.2 train no.167670  loss = 2.17951 avg_loss = 3.33447\n",
      "epoch no.2 train no.167680  loss = 2.92825 avg_loss = 3.33025\n",
      "epoch no.2 train no.167690  loss = 4.98781 avg_loss = 3.37417\n",
      "epoch no.2 train no.167700  loss = 3.74725 avg_loss = 3.37941\n",
      "epoch no.2 train no.167710  loss = 2.50489 avg_loss = 3.34299\n",
      "epoch no.2 train no.167720  loss = 4.10921 avg_loss = 3.33364\n",
      "epoch no.2 train no.167730  loss = 3.99091 avg_loss = 3.34768\n",
      "epoch no.2 train no.167740  loss = 3.36348 avg_loss = 3.39942\n",
      "epoch no.2 train no.167750  loss = 3.57290 avg_loss = 3.37588\n",
      "epoch no.2 train no.167760  loss = 4.03205 avg_loss = 3.38363\n",
      "epoch no.2 train no.167770  loss = 4.09118 avg_loss = 3.36334\n",
      "epoch no.2 train no.167780  loss = 3.93098 avg_loss = 3.38199\n",
      "epoch no.2 train no.167790  loss = 3.43626 avg_loss = 3.37487\n",
      "epoch no.2 train no.167800  loss = 3.42113 avg_loss = 3.38404\n",
      "epoch no.2 train no.167810  loss = 4.94889 avg_loss = 3.41072\n",
      "epoch no.2 train no.167820  loss = 3.55793 avg_loss = 3.43357\n",
      "epoch no.2 train no.167830  loss = 3.64383 avg_loss = 3.43005\n",
      "epoch no.2 train no.167840  loss = 2.39934 avg_loss = 3.42733\n",
      "epoch no.2 train no.167850  loss = 2.64852 avg_loss = 3.40395\n",
      "epoch no.2 train no.167860  loss = 4.17402 avg_loss = 3.45720\n",
      "epoch no.2 train no.167870  loss = 2.52471 avg_loss = 3.39351\n",
      "epoch no.2 train no.167880  loss = 2.84722 avg_loss = 3.39784\n",
      "epoch no.2 train no.167890  loss = 1.84191 avg_loss = 3.43528\n",
      "epoch no.2 train no.167900  loss = 3.78674 avg_loss = 3.47917\n",
      "epoch no.2 train no.167910  loss = 4.16697 avg_loss = 3.48769\n",
      "epoch no.2 train no.167920  loss = 4.15790 avg_loss = 3.46331\n",
      "epoch no.2 train no.167930  loss = 5.24431 avg_loss = 3.46857\n",
      "epoch no.2 train no.167940  loss = 5.32964 avg_loss = 3.50569\n",
      "epoch no.2 train no.167950  loss = 3.27663 avg_loss = 3.50783\n",
      "epoch no.2 train no.167960  loss = 3.07714 avg_loss = 3.49537\n",
      "epoch no.2 train no.167970  loss = 3.89876 avg_loss = 3.48671\n",
      "epoch no.2 train no.167980  loss = 2.54949 avg_loss = 3.46867\n",
      "epoch no.2 train no.167990  loss = 3.59539 avg_loss = 3.50747\n",
      "epoch no.2 train no.168000  loss = 4.46927 avg_loss = 3.48514\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁드라이브', '와', '▁함께', '▁듣고', '▁싶은', '▁노래', '</s>']\n",
      "여름 밤 너와 함께 듣고 싶은 노래</s>\n",
      "epoch no.2 train no.168010  loss = 3.49042 avg_loss = 3.52249\n",
      "epoch no.2 train no.168020  loss = 3.88059 avg_loss = 3.54266\n",
      "epoch no.2 train no.168030  loss = 2.16397 avg_loss = 3.56297\n",
      "epoch no.2 train no.168040  loss = 3.54644 avg_loss = 3.53285\n",
      "epoch no.2 train no.168050  loss = 2.99628 avg_loss = 3.50443\n",
      "epoch no.2 train no.168060  loss = 4.17561 avg_loss = 3.51384\n",
      "epoch no.2 train no.168070  loss = 2.68900 avg_loss = 3.51427\n",
      "epoch no.2 train no.168080  loss = 3.06484 avg_loss = 3.49068\n",
      "epoch no.2 train no.168090  loss = 1.84348 avg_loss = 3.45534\n",
      "epoch no.2 train no.168100  loss = 4.43639 avg_loss = 3.45015\n",
      "epoch no.2 train no.168110  loss = 2.98604 avg_loss = 3.43331\n",
      "epoch no.2 train no.168120  loss = 2.32005 avg_loss = 3.39602\n",
      "epoch no.2 train no.168130  loss = 2.46629 avg_loss = 3.36980\n",
      "epoch no.2 train no.168140  loss = 3.28287 avg_loss = 3.36992\n",
      "epoch no.2 train no.168150  loss = 2.03818 avg_loss = 3.35293\n",
      "epoch no.2 train no.168160  loss = 4.32273 avg_loss = 3.35587\n",
      "epoch no.2 train no.168170  loss = 2.18346 avg_loss = 3.33069\n",
      "epoch no.2 train no.168180  loss = 4.66421 avg_loss = 3.30867\n",
      "epoch no.2 train no.168190  loss = 3.38291 avg_loss = 3.33556\n",
      "epoch no.2 train no.168200  loss = 2.61792 avg_loss = 3.31250\n",
      "epoch no.2 train no.168210  loss = 2.48512 avg_loss = 3.31637\n",
      "epoch no.2 train no.168220  loss = 6.67699 avg_loss = 3.33475\n",
      "epoch no.2 train no.168230  loss = 2.24270 avg_loss = 3.34823\n",
      "epoch no.2 train no.168240  loss = 3.30863 avg_loss = 3.35349\n",
      "epoch no.2 train no.168250  loss = 5.46490 avg_loss = 3.36368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.168260  loss = 3.12536 avg_loss = 3.41489\n",
      "epoch no.2 train no.168270  loss = 2.85839 avg_loss = 3.41860\n",
      "epoch no.2 train no.168280  loss = 3.00285 avg_loss = 3.44196\n",
      "epoch no.2 train no.168290  loss = 5.64407 avg_loss = 3.46350\n",
      "epoch no.2 train no.168300  loss = 3.36511 avg_loss = 3.44573\n",
      "epoch no.2 train no.168310  loss = 5.49950 avg_loss = 3.44251\n",
      "epoch no.2 train no.168320  loss = 3.56323 avg_loss = 3.42918\n",
      "epoch no.2 train no.168330  loss = 2.55536 avg_loss = 3.37843\n",
      "epoch no.2 train no.168340  loss = 5.21325 avg_loss = 3.39260\n",
      "epoch no.2 train no.168350  loss = 4.81937 avg_loss = 3.33772\n",
      "epoch no.2 train no.168360  loss = 4.03049 avg_loss = 3.36905\n",
      "epoch no.2 train no.168370  loss = 4.45090 avg_loss = 3.37479\n",
      "epoch no.2 train no.168380  loss = 3.45301 avg_loss = 3.39708\n",
      "epoch no.2 train no.168390  loss = 4.69191 avg_loss = 3.42750\n",
      "epoch no.2 train no.168400  loss = 3.36794 avg_loss = 3.47082\n",
      "epoch no.2 train no.168410  loss = 4.06702 avg_loss = 3.44273\n",
      "epoch no.2 train no.168420  loss = 2.49669 avg_loss = 3.38163\n",
      "epoch no.2 train no.168430  loss = 2.33600 avg_loss = 3.40865\n",
      "epoch no.2 train no.168440  loss = 3.49707 avg_loss = 3.39495\n",
      "epoch no.2 train no.168450  loss = 1.94397 avg_loss = 3.41366\n",
      "epoch no.2 train no.168460  loss = 4.53400 avg_loss = 3.42349\n",
      "epoch no.2 train no.168470  loss = 2.75802 avg_loss = 3.39190\n",
      "epoch no.2 train no.168480  loss = 3.26071 avg_loss = 3.42775\n",
      "epoch no.2 train no.168490  loss = 2.28113 avg_loss = 3.41902\n",
      "epoch no.2 train no.168500  loss = 3.39304 avg_loss = 3.45962\n",
      "epoch no.2 train no.168510  loss = 3.09078 avg_loss = 3.43415\n",
      "epoch no.2 train no.168520  loss = 3.68295 avg_loss = 3.41346\n",
      "epoch no.2 train no.168530  loss = 1.73004 avg_loss = 3.39938\n",
      "epoch no.2 train no.168540  loss = 3.18019 avg_loss = 3.36220\n",
      "epoch no.2 train no.168550  loss = 2.30941 avg_loss = 3.42000\n",
      "epoch no.2 train no.168560  loss = 2.78325 avg_loss = 3.43264\n",
      "epoch no.2 train no.168570  loss = 2.61716 avg_loss = 3.45790\n",
      "epoch no.2 train no.168580  loss = 2.69462 avg_loss = 3.47040\n",
      "epoch no.2 train no.168590  loss = 4.35606 avg_loss = 3.50979\n",
      "epoch no.2 train no.168600  loss = 2.84013 avg_loss = 3.48492\n",
      "epoch no.2 train no.168610  loss = 3.55924 avg_loss = 3.47104\n",
      "epoch no.2 train no.168620  loss = 3.82037 avg_loss = 3.43009\n",
      "epoch no.2 train no.168630  loss = 2.94170 avg_loss = 3.47834\n",
      "epoch no.2 train no.168640  loss = 3.22572 avg_loss = 3.46969\n",
      "epoch no.2 train no.168650  loss = 2.92665 avg_loss = 3.45862\n",
      "epoch no.2 train no.168660  loss = 3.63092 avg_loss = 3.46167\n",
      "epoch no.2 train no.168670  loss = 3.97388 avg_loss = 3.48096\n",
      "epoch no.2 train no.168680  loss = 5.78228 avg_loss = 3.49508\n",
      "epoch no.2 train no.168690  loss = 2.82263 avg_loss = 3.45391\n",
      "epoch no.2 train no.168700  loss = 3.42769 avg_loss = 3.50376\n",
      "epoch no.2 train no.168710  loss = 2.20262 avg_loss = 3.45969\n",
      "epoch no.2 train no.168720  loss = 3.72795 avg_loss = 3.46832\n",
      "epoch no.2 train no.168730  loss = 1.83235 avg_loss = 3.42441\n",
      "epoch no.2 train no.168740  loss = 3.26762 avg_loss = 3.44033\n",
      "epoch no.2 train no.168750  loss = 3.05497 avg_loss = 3.41315\n",
      "epoch no.2 train no.168760  loss = 5.23179 avg_loss = 3.43208\n",
      "epoch no.2 train no.168770  loss = 3.48025 avg_loss = 3.43588\n",
      "epoch no.2 train no.168780  loss = 4.39077 avg_loss = 3.40764\n",
      "epoch no.2 train no.168790  loss = 3.51921 avg_loss = 3.42146\n",
      "epoch no.2 train no.168800  loss = 3.46735 avg_loss = 3.40082\n",
      "epoch no.2 train no.168810  loss = 2.40816 avg_loss = 3.39671\n",
      "epoch no.2 train no.168820  loss = 4.24738 avg_loss = 3.39893\n",
      "epoch no.2 train no.168830  loss = 2.96229 avg_loss = 3.45710\n",
      "epoch no.2 train no.168840  loss = 3.17077 avg_loss = 3.43402\n",
      "epoch no.2 train no.168850  loss = 3.57509 avg_loss = 3.43419\n",
      "epoch no.2 train no.168860  loss = 3.00960 avg_loss = 3.42019\n",
      "epoch no.2 train no.168870  loss = 3.86908 avg_loss = 3.43286\n",
      "epoch no.2 train no.168880  loss = 5.20806 avg_loss = 3.43943\n",
      "epoch no.2 train no.168890  loss = 1.74438 avg_loss = 3.40511\n",
      "epoch no.2 train no.168900  loss = 2.31731 avg_loss = 3.39156\n",
      "epoch no.2 train no.168910  loss = 3.12274 avg_loss = 3.36729\n",
      "epoch no.2 train no.168920  loss = 3.85104 avg_loss = 3.37696\n",
      "epoch no.2 train no.168930  loss = 3.90104 avg_loss = 3.40179\n",
      "epoch no.2 train no.168940  loss = 3.22524 avg_loss = 3.46347\n",
      "epoch no.2 train no.168950  loss = 2.78666 avg_loss = 3.49743\n",
      "epoch no.2 train no.168960  loss = 5.99172 avg_loss = 3.47748\n",
      "epoch no.2 train no.168970  loss = 2.95113 avg_loss = 3.46543\n",
      "epoch no.2 train no.168980  loss = 4.85067 avg_loss = 3.45246\n",
      "epoch no.2 train no.168990  loss = 3.26786 avg_loss = 3.42739\n",
      "epoch no.2 train no.169000  loss = 2.59240 avg_loss = 3.47103\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁노래', '들', '</s>']\n",
      "여름과 어울리는 노래들</s>\n",
      "epoch no.2 train no.169010  loss = 2.41026 avg_loss = 3.46836\n",
      "epoch no.2 train no.169020  loss = 4.47160 avg_loss = 3.50014\n",
      "epoch no.2 train no.169030  loss = 4.45966 avg_loss = 3.52495\n",
      "epoch no.2 train no.169040  loss = 3.36841 avg_loss = 3.54214\n",
      "epoch no.2 train no.169050  loss = 2.39172 avg_loss = 3.47922\n",
      "epoch no.2 train no.169060  loss = 3.71264 avg_loss = 3.52444\n",
      "epoch no.2 train no.169070  loss = 2.67633 avg_loss = 3.52999\n",
      "epoch no.2 train no.169080  loss = 3.30956 avg_loss = 3.50176\n",
      "epoch no.2 train no.169090  loss = 2.62474 avg_loss = 3.49268\n",
      "epoch no.2 train no.169100  loss = 3.37578 avg_loss = 3.49244\n",
      "epoch no.2 train no.169110  loss = 5.20077 avg_loss = 3.51275\n",
      "epoch no.2 train no.169120  loss = 3.59181 avg_loss = 3.52031\n",
      "epoch no.2 train no.169130  loss = 3.28981 avg_loss = 3.54032\n",
      "epoch no.2 train no.169140  loss = 2.99525 avg_loss = 3.60130\n",
      "epoch no.2 train no.169150  loss = 5.61693 avg_loss = 3.62186\n",
      "epoch no.2 train no.169160  loss = 4.41348 avg_loss = 3.61335\n",
      "epoch no.2 train no.169170  loss = 3.47955 avg_loss = 3.55946\n",
      "epoch no.2 train no.169180  loss = 1.82189 avg_loss = 3.60354\n",
      "epoch no.2 train no.169190  loss = 2.74848 avg_loss = 3.57544\n",
      "epoch no.2 train no.169200  loss = 3.31864 avg_loss = 3.60305\n",
      "epoch no.2 train no.169210  loss = 3.68342 avg_loss = 3.59335\n",
      "epoch no.2 train no.169220  loss = 4.37869 avg_loss = 3.60431\n",
      "epoch no.2 train no.169230  loss = 3.58001 avg_loss = 3.56995\n",
      "epoch no.2 train no.169240  loss = 3.02908 avg_loss = 3.54768\n",
      "epoch no.2 train no.169250  loss = 2.53580 avg_loss = 3.51260\n",
      "epoch no.2 train no.169260  loss = 3.56913 avg_loss = 3.49611\n",
      "epoch no.2 train no.169270  loss = 4.32360 avg_loss = 3.50468\n",
      "epoch no.2 train no.169280  loss = 3.44398 avg_loss = 3.51862\n",
      "epoch no.2 train no.169290  loss = 5.45405 avg_loss = 3.51681\n",
      "epoch no.2 train no.169300  loss = 1.78714 avg_loss = 3.45941\n",
      "epoch no.2 train no.169310  loss = 2.49006 avg_loss = 3.42812\n",
      "epoch no.2 train no.169320  loss = 3.75663 avg_loss = 3.43105\n",
      "epoch no.2 train no.169330  loss = 3.01022 avg_loss = 3.42629\n",
      "epoch no.2 train no.169340  loss = 2.47668 avg_loss = 3.42646\n",
      "epoch no.2 train no.169350  loss = 2.08541 avg_loss = 3.48086\n",
      "epoch no.2 train no.169360  loss = 4.17112 avg_loss = 3.44650\n",
      "epoch no.2 train no.169370  loss = 2.42484 avg_loss = 3.43618\n",
      "epoch no.2 train no.169380  loss = 3.58892 avg_loss = 3.43315\n",
      "epoch no.2 train no.169390  loss = 2.95104 avg_loss = 3.43546\n",
      "epoch no.2 train no.169400  loss = 2.86725 avg_loss = 3.41054\n",
      "epoch no.2 train no.169410  loss = 1.45513 avg_loss = 3.39885\n",
      "epoch no.2 train no.169420  loss = 4.25148 avg_loss = 3.44222\n",
      "epoch no.2 train no.169430  loss = 3.18370 avg_loss = 3.43901\n",
      "epoch no.2 train no.169440  loss = 3.87575 avg_loss = 3.47275\n",
      "epoch no.2 train no.169450  loss = 3.36336 avg_loss = 3.48412\n",
      "epoch no.2 train no.169460  loss = 3.31941 avg_loss = 3.45966\n",
      "epoch no.2 train no.169470  loss = 2.92780 avg_loss = 3.44806\n",
      "epoch no.2 train no.169480  loss = 2.02727 avg_loss = 3.46077\n",
      "epoch no.2 train no.169490  loss = 2.33514 avg_loss = 3.46978\n",
      "epoch no.2 train no.169500  loss = 4.73015 avg_loss = 3.43527\n",
      "epoch no.2 train no.169510  loss = 3.35566 avg_loss = 3.40461\n",
      "epoch no.2 train no.169520  loss = 4.86662 avg_loss = 3.39441\n",
      "epoch no.2 train no.169530  loss = 6.02305 avg_loss = 3.39400\n",
      "epoch no.2 train no.169540  loss = 5.45161 avg_loss = 3.35923\n",
      "epoch no.2 train no.169550  loss = 1.81422 avg_loss = 3.35644\n",
      "epoch no.2 train no.169560  loss = 4.71799 avg_loss = 3.35746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.169570  loss = 3.17033 avg_loss = 3.33317\n",
      "epoch no.2 train no.169580  loss = 3.24279 avg_loss = 3.31389\n",
      "epoch no.2 train no.169590  loss = 3.18222 avg_loss = 3.35025\n",
      "epoch no.2 train no.169600  loss = 3.94733 avg_loss = 3.35844\n",
      "epoch no.2 train no.169610  loss = 2.30530 avg_loss = 3.32908\n",
      "epoch no.2 train no.169620  loss = 2.39239 avg_loss = 3.27709\n",
      "epoch no.2 train no.169630  loss = 3.25946 avg_loss = 3.30699\n",
      "epoch no.2 train no.169640  loss = 3.14236 avg_loss = 3.32923\n",
      "epoch no.2 train no.169650  loss = 2.30961 avg_loss = 3.32681\n",
      "epoch no.2 train no.169660  loss = 4.96203 avg_loss = 3.39430\n",
      "epoch no.2 train no.169670  loss = 3.97930 avg_loss = 3.38092\n",
      "epoch no.2 train no.169680  loss = 6.26738 avg_loss = 3.40446\n",
      "epoch no.2 train no.169690  loss = 5.57135 avg_loss = 3.43012\n",
      "epoch no.2 train no.169700  loss = 2.18713 avg_loss = 3.42799\n",
      "epoch no.2 train no.169710  loss = 3.73505 avg_loss = 3.41633\n",
      "epoch no.2 train no.169720  loss = 5.51202 avg_loss = 3.46585\n",
      "epoch no.2 train no.169730  loss = 1.66103 avg_loss = 3.40096\n",
      "epoch no.2 train no.169740  loss = 3.06443 avg_loss = 3.41843\n",
      "epoch no.2 train no.169750  loss = 3.33772 avg_loss = 3.37390\n",
      "epoch no.2 train no.169760  loss = 2.59611 avg_loss = 3.36990\n",
      "epoch no.2 train no.169770  loss = 2.44055 avg_loss = 3.38559\n",
      "epoch no.2 train no.169780  loss = 2.26779 avg_loss = 3.35994\n",
      "epoch no.2 train no.169790  loss = 2.73571 avg_loss = 3.37515\n",
      "epoch no.2 train no.169800  loss = 2.55093 avg_loss = 3.37026\n",
      "epoch no.2 train no.169810  loss = 3.06467 avg_loss = 3.38346\n",
      "epoch no.2 train no.169820  loss = 4.12410 avg_loss = 3.41086\n",
      "epoch no.2 train no.169830  loss = 4.56254 avg_loss = 3.43598\n",
      "epoch no.2 train no.169840  loss = 3.26850 avg_loss = 3.48301\n",
      "epoch no.2 train no.169850  loss = 3.33729 avg_loss = 3.46291\n",
      "epoch no.2 train no.169860  loss = 2.38426 avg_loss = 3.46641\n",
      "epoch no.2 train no.169870  loss = 5.44926 avg_loss = 3.45284\n",
      "epoch no.2 train no.169880  loss = 2.11105 avg_loss = 3.39948\n",
      "epoch no.2 train no.169890  loss = 3.87832 avg_loss = 3.38788\n",
      "epoch no.2 train no.169900  loss = 4.34611 avg_loss = 3.44562\n",
      "epoch no.2 train no.169910  loss = 4.46155 avg_loss = 3.49489\n",
      "epoch no.2 train no.169920  loss = 1.93954 avg_loss = 3.45888\n",
      "epoch no.2 train no.169930  loss = 3.39228 avg_loss = 3.49063\n",
      "epoch no.2 train no.169940  loss = 4.73791 avg_loss = 3.60904\n",
      "epoch no.2 train no.169950  loss = 5.31464 avg_loss = 3.63836\n",
      "epoch no.2 train no.169960  loss = 4.59333 avg_loss = 3.65071\n",
      "epoch no.2 train no.169970  loss = 2.96236 avg_loss = 3.64625\n",
      "epoch no.2 train no.169980  loss = 3.49974 avg_loss = 3.61052\n",
      "epoch no.2 train no.169990  loss = 4.00680 avg_loss = 3.61991\n",
      "epoch no.2 train no.170000  loss = 3.44381 avg_loss = 3.57882\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '▁듣기', '하며', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름 밤 산책하며 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.170010  loss = 2.86881 avg_loss = 3.55275\n",
      "epoch no.2 train no.170020  loss = 4.49634 avg_loss = 3.56846\n",
      "epoch no.2 train no.170030  loss = 3.40813 avg_loss = 3.53454\n",
      "epoch no.2 train no.170040  loss = 4.51390 avg_loss = 3.56784\n",
      "epoch no.2 train no.170050  loss = 5.26582 avg_loss = 3.56941\n",
      "epoch no.2 train no.170060  loss = 4.49536 avg_loss = 3.58923\n",
      "epoch no.2 train no.170070  loss = 2.03358 avg_loss = 3.60796\n",
      "epoch no.2 train no.170080  loss = 5.33800 avg_loss = 3.62030\n",
      "epoch no.2 train no.170090  loss = 2.41260 avg_loss = 3.61322\n",
      "epoch no.2 train no.170100  loss = 1.79098 avg_loss = 3.61282\n",
      "epoch no.2 train no.170110  loss = 3.65575 avg_loss = 3.60031\n",
      "epoch no.2 train no.170120  loss = 3.80889 avg_loss = 3.57009\n",
      "epoch no.2 train no.170130  loss = 5.31207 avg_loss = 3.60764\n",
      "epoch no.2 train no.170140  loss = 4.78789 avg_loss = 3.58760\n",
      "epoch no.2 train no.170150  loss = 2.92128 avg_loss = 3.61137\n",
      "epoch no.2 train no.170160  loss = 3.69338 avg_loss = 3.62047\n",
      "epoch no.2 train no.170170  loss = 2.88081 avg_loss = 3.57132\n",
      "epoch no.2 train no.170180  loss = 4.56628 avg_loss = 3.58262\n",
      "epoch no.2 train no.170190  loss = 4.31972 avg_loss = 3.56838\n",
      "epoch no.2 train no.170200  loss = 3.30995 avg_loss = 3.56418\n",
      "epoch no.2 train no.170210  loss = 2.77227 avg_loss = 3.52602\n",
      "epoch no.2 train no.170220  loss = 3.95188 avg_loss = 3.50958\n",
      "epoch no.2 train no.170230  loss = 4.78713 avg_loss = 3.48783\n",
      "epoch no.2 train no.170240  loss = 2.39565 avg_loss = 3.48879\n",
      "epoch no.2 train no.170250  loss = 2.58752 avg_loss = 3.44050\n",
      "epoch no.2 train no.170260  loss = 2.62414 avg_loss = 3.41886\n",
      "epoch no.2 train no.170270  loss = 2.91784 avg_loss = 3.41628\n",
      "epoch no.2 train no.170280  loss = 3.13254 avg_loss = 3.44187\n",
      "epoch no.2 train no.170290  loss = 2.27952 avg_loss = 3.50150\n",
      "epoch no.2 train no.170300  loss = 4.49491 avg_loss = 3.51564\n",
      "epoch no.2 train no.170310  loss = 1.27898 avg_loss = 3.49216\n",
      "epoch no.2 train no.170320  loss = 3.80655 avg_loss = 3.51400\n",
      "epoch no.2 train no.170330  loss = 3.86891 avg_loss = 3.50600\n",
      "epoch no.2 train no.170340  loss = 5.16358 avg_loss = 3.51715\n",
      "epoch no.2 train no.170350  loss = 3.47496 avg_loss = 3.52488\n",
      "epoch no.2 train no.170360  loss = 2.55682 avg_loss = 3.52419\n",
      "epoch no.2 train no.170370  loss = 2.43769 avg_loss = 3.54703\n",
      "epoch no.2 train no.170380  loss = 3.85405 avg_loss = 3.53007\n",
      "epoch no.2 train no.170390  loss = 4.03436 avg_loss = 3.50681\n",
      "epoch no.2 train no.170400  loss = 3.82956 avg_loss = 3.47745\n",
      "epoch no.2 train no.170410  loss = 2.06963 avg_loss = 3.42538\n",
      "epoch no.2 train no.170420  loss = 6.12679 avg_loss = 3.43075\n",
      "epoch no.2 train no.170430  loss = 2.20771 avg_loss = 3.45229\n",
      "epoch no.2 train no.170440  loss = 3.10564 avg_loss = 3.39710\n",
      "epoch no.2 train no.170450  loss = 3.47475 avg_loss = 3.44761\n",
      "epoch no.2 train no.170460  loss = 5.19272 avg_loss = 3.54371\n",
      "epoch no.2 train no.170470  loss = 2.14160 avg_loss = 3.62038\n",
      "epoch no.2 train no.170480  loss = 3.25727 avg_loss = 3.59849\n",
      "epoch no.2 train no.170490  loss = 2.54758 avg_loss = 3.53413\n",
      "epoch no.2 train no.170500  loss = 2.78516 avg_loss = 3.51849\n",
      "epoch no.2 train no.170510  loss = 3.18971 avg_loss = 3.51863\n",
      "epoch no.2 train no.170520  loss = 2.09578 avg_loss = 3.52506\n",
      "epoch no.2 train no.170530  loss = 1.93127 avg_loss = 3.49832\n",
      "epoch no.2 train no.170540  loss = 4.21026 avg_loss = 3.46767\n",
      "epoch no.2 train no.170550  loss = 2.98973 avg_loss = 3.49181\n",
      "epoch no.2 train no.170560  loss = 2.44456 avg_loss = 3.46490\n",
      "epoch no.2 train no.170570  loss = 3.35700 avg_loss = 3.45066\n",
      "epoch no.2 train no.170580  loss = 2.23501 avg_loss = 3.47325\n",
      "epoch no.2 train no.170590  loss = 1.87139 avg_loss = 3.44272\n",
      "epoch no.2 train no.170600  loss = 2.14809 avg_loss = 3.46267\n",
      "epoch no.2 train no.170610  loss = 2.81389 avg_loss = 3.45246\n",
      "epoch no.2 train no.170620  loss = 5.35479 avg_loss = 3.43378\n",
      "epoch no.2 train no.170630  loss = 2.59008 avg_loss = 3.41309\n",
      "epoch no.2 train no.170640  loss = 3.79979 avg_loss = 3.40875\n",
      "epoch no.2 train no.170650  loss = 4.40039 avg_loss = 3.41517\n",
      "epoch no.2 train no.170660  loss = 2.68179 avg_loss = 3.42763\n",
      "epoch no.2 train no.170670  loss = 4.37203 avg_loss = 3.42834\n",
      "epoch no.2 train no.170680  loss = 4.70443 avg_loss = 3.50229\n",
      "epoch no.2 train no.170690  loss = 1.54493 avg_loss = 3.51739\n",
      "epoch no.2 train no.170700  loss = 2.94525 avg_loss = 3.47603\n",
      "epoch no.2 train no.170710  loss = 3.36515 avg_loss = 3.45473\n",
      "epoch no.2 train no.170720  loss = 2.58479 avg_loss = 3.44068\n",
      "epoch no.2 train no.170730  loss = 2.50401 avg_loss = 3.40644\n",
      "epoch no.2 train no.170740  loss = 3.13709 avg_loss = 3.42687\n",
      "epoch no.2 train no.170750  loss = 3.65945 avg_loss = 3.41541\n",
      "epoch no.2 train no.170760  loss = 2.66864 avg_loss = 3.38542\n",
      "epoch no.2 train no.170770  loss = 4.52077 avg_loss = 3.39913\n",
      "epoch no.2 train no.170780  loss = 2.50491 avg_loss = 3.37496\n",
      "epoch no.2 train no.170790  loss = 2.13032 avg_loss = 3.38551\n",
      "epoch no.2 train no.170800  loss = 3.51737 avg_loss = 3.37032\n",
      "epoch no.2 train no.170810  loss = 3.64443 avg_loss = 3.43505\n",
      "epoch no.2 train no.170820  loss = 4.54050 avg_loss = 3.45501\n",
      "epoch no.2 train no.170830  loss = 3.25288 avg_loss = 3.48787\n",
      "epoch no.2 train no.170840  loss = 2.72533 avg_loss = 3.50780\n",
      "epoch no.2 train no.170850  loss = 3.04906 avg_loss = 3.51312\n",
      "epoch no.2 train no.170860  loss = 3.53367 avg_loss = 3.50567\n",
      "epoch no.2 train no.170870  loss = 3.18719 avg_loss = 3.50525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.170880  loss = 2.02974 avg_loss = 3.48307\n",
      "epoch no.2 train no.170890  loss = 3.42023 avg_loss = 3.46483\n",
      "epoch no.2 train no.170900  loss = 2.72548 avg_loss = 3.47003\n",
      "epoch no.2 train no.170910  loss = 4.79026 avg_loss = 3.40032\n",
      "epoch no.2 train no.170920  loss = 3.58504 avg_loss = 3.39978\n",
      "epoch no.2 train no.170930  loss = 4.82521 avg_loss = 3.44805\n",
      "epoch no.2 train no.170940  loss = 3.73511 avg_loss = 3.45564\n",
      "epoch no.2 train no.170950  loss = 3.22530 avg_loss = 3.41811\n",
      "epoch no.2 train no.170960  loss = 3.67940 avg_loss = 3.42486\n",
      "epoch no.2 train no.170970  loss = 3.34686 avg_loss = 3.37987\n",
      "epoch no.2 train no.170980  loss = 3.51475 avg_loss = 3.41617\n",
      "epoch no.2 train no.170990  loss = 2.40258 avg_loss = 3.39663\n",
      "epoch no.2 train no.171000  loss = 4.52202 avg_loss = 3.47758\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓을', '▁노래', '</s>']\n",
      "여름밤을 수놓은 재즈</s>\n",
      "epoch no.2 train no.171010  loss = 2.86826 avg_loss = 3.47672\n",
      "epoch no.2 train no.171020  loss = 4.06907 avg_loss = 3.51021\n",
      "epoch no.2 train no.171030  loss = 3.32205 avg_loss = 3.50333\n",
      "epoch no.2 train no.171040  loss = 3.03966 avg_loss = 3.48403\n",
      "epoch no.2 train no.171050  loss = 2.66472 avg_loss = 3.48206\n",
      "epoch no.2 train no.171060  loss = 3.91740 avg_loss = 3.49631\n",
      "epoch no.2 train no.171070  loss = 2.11657 avg_loss = 3.43471\n",
      "epoch no.2 train no.171080  loss = 4.21964 avg_loss = 3.42996\n",
      "epoch no.2 train no.171090  loss = 4.71461 avg_loss = 3.49697\n",
      "epoch no.2 train no.171100  loss = 3.03665 avg_loss = 3.49538\n",
      "epoch no.2 train no.171110  loss = 4.63133 avg_loss = 3.55342\n",
      "epoch no.2 train no.171120  loss = 3.21722 avg_loss = 3.53983\n",
      "epoch no.2 train no.171130  loss = 3.08115 avg_loss = 3.54581\n",
      "epoch no.2 train no.171140  loss = 2.92507 avg_loss = 3.56943\n",
      "epoch no.2 train no.171150  loss = 3.49902 avg_loss = 3.55367\n",
      "epoch no.2 train no.171160  loss = 1.70873 avg_loss = 3.49113\n",
      "epoch no.2 train no.171170  loss = 4.12250 avg_loss = 3.48433\n",
      "epoch no.2 train no.171180  loss = 3.15433 avg_loss = 3.49066\n",
      "epoch no.2 train no.171190  loss = 4.12817 avg_loss = 3.49537\n",
      "epoch no.2 train no.171200  loss = 4.93991 avg_loss = 3.52635\n",
      "epoch no.2 train no.171210  loss = 4.12095 avg_loss = 3.50109\n",
      "epoch no.2 train no.171220  loss = 2.26605 avg_loss = 3.51417\n",
      "epoch no.2 train no.171230  loss = 3.02338 avg_loss = 3.50214\n",
      "epoch no.2 train no.171240  loss = 3.34608 avg_loss = 3.48567\n",
      "epoch no.2 train no.171250  loss = 3.56315 avg_loss = 3.44703\n",
      "epoch no.2 train no.171260  loss = 2.20391 avg_loss = 3.41883\n",
      "epoch no.2 train no.171270  loss = 3.76213 avg_loss = 3.45148\n",
      "epoch no.2 train no.171280  loss = 2.97508 avg_loss = 3.49962\n",
      "epoch no.2 train no.171290  loss = 3.54131 avg_loss = 3.51493\n",
      "epoch no.2 train no.171300  loss = 5.11399 avg_loss = 3.59121\n",
      "epoch no.2 train no.171310  loss = 4.06083 avg_loss = 3.60016\n",
      "epoch no.2 train no.171320  loss = 2.90944 avg_loss = 3.56424\n",
      "epoch no.2 train no.171330  loss = 2.88459 avg_loss = 3.51342\n",
      "epoch no.2 train no.171340  loss = 3.46226 avg_loss = 3.49737\n",
      "epoch no.2 train no.171350  loss = 3.31425 avg_loss = 3.48534\n",
      "epoch no.2 train no.171360  loss = 3.21183 avg_loss = 3.47746\n",
      "epoch no.2 train no.171370  loss = 4.58856 avg_loss = 3.49018\n",
      "epoch no.2 train no.171380  loss = 3.38519 avg_loss = 3.50322\n",
      "epoch no.2 train no.171390  loss = 4.30940 avg_loss = 3.53570\n",
      "epoch no.2 train no.171400  loss = 2.73508 avg_loss = 3.46355\n",
      "epoch no.2 train no.171410  loss = 2.88936 avg_loss = 3.52278\n",
      "epoch no.2 train no.171420  loss = 2.41561 avg_loss = 3.48405\n",
      "epoch no.2 train no.171430  loss = 2.68421 avg_loss = 3.46442\n",
      "epoch no.2 train no.171440  loss = 3.12646 avg_loss = 3.37559\n",
      "epoch no.2 train no.171450  loss = 3.17073 avg_loss = 3.35545\n",
      "epoch no.2 train no.171460  loss = 2.74636 avg_loss = 3.33733\n",
      "epoch no.2 train no.171470  loss = 4.74914 avg_loss = 3.36559\n",
      "epoch no.2 train no.171480  loss = 4.53244 avg_loss = 3.37908\n",
      "epoch no.2 train no.171490  loss = 3.92143 avg_loss = 3.39522\n",
      "epoch no.2 train no.171500  loss = 3.32872 avg_loss = 3.38169\n",
      "epoch no.2 train no.171510  loss = 3.23248 avg_loss = 3.36548\n",
      "epoch no.2 train no.171520  loss = 4.35605 avg_loss = 3.41732\n",
      "epoch no.2 train no.171530  loss = 2.31030 avg_loss = 3.43522\n",
      "epoch no.2 train no.171540  loss = 4.52699 avg_loss = 3.44495\n",
      "epoch no.2 train no.171550  loss = 5.24852 avg_loss = 3.47682\n",
      "epoch no.2 train no.171560  loss = 3.81649 avg_loss = 3.48286\n",
      "epoch no.2 train no.171570  loss = 4.01924 avg_loss = 3.44286\n",
      "epoch no.2 train no.171580  loss = 5.62216 avg_loss = 3.48661\n",
      "epoch no.2 train no.171590  loss = 4.77069 avg_loss = 3.47607\n",
      "epoch no.2 train no.171600  loss = 3.49403 avg_loss = 3.41884\n",
      "epoch no.2 train no.171610  loss = 2.97271 avg_loss = 3.43674\n",
      "epoch no.2 train no.171620  loss = 5.84261 avg_loss = 3.45189\n",
      "epoch no.2 train no.171630  loss = 3.15009 avg_loss = 3.50424\n",
      "epoch no.2 train no.171640  loss = 2.01994 avg_loss = 3.43877\n",
      "epoch no.2 train no.171650  loss = 3.55958 avg_loss = 3.45711\n",
      "epoch no.2 train no.171660  loss = 2.47203 avg_loss = 3.48800\n",
      "epoch no.2 train no.171670  loss = 2.19076 avg_loss = 3.49406\n",
      "epoch no.2 train no.171680  loss = 3.14607 avg_loss = 3.48364\n",
      "epoch no.2 train no.171690  loss = 2.43028 avg_loss = 3.47001\n",
      "epoch no.2 train no.171700  loss = 2.74226 avg_loss = 3.44580\n",
      "epoch no.2 train no.171710  loss = 3.85575 avg_loss = 3.47646\n",
      "epoch no.2 train no.171720  loss = 4.74086 avg_loss = 3.51510\n",
      "epoch no.2 train no.171730  loss = 3.53355 avg_loss = 3.55567\n",
      "epoch no.2 train no.171740  loss = 4.57455 avg_loss = 3.56840\n",
      "epoch no.2 train no.171750  loss = 2.83913 avg_loss = 3.55742\n",
      "epoch no.2 train no.171760  loss = 2.36533 avg_loss = 3.56382\n",
      "epoch no.2 train no.171770  loss = 3.05341 avg_loss = 3.54240\n",
      "epoch no.2 train no.171780  loss = 3.55905 avg_loss = 3.52135\n",
      "epoch no.2 train no.171790  loss = 3.11450 avg_loss = 3.51358\n",
      "epoch no.2 train no.171800  loss = 2.86047 avg_loss = 3.53402\n",
      "epoch no.2 train no.171810  loss = 5.14672 avg_loss = 3.53302\n",
      "epoch no.2 train no.171820  loss = 3.69791 avg_loss = 3.52982\n",
      "epoch no.2 train no.171830  loss = 3.67310 avg_loss = 3.52799\n",
      "epoch no.2 train no.171840  loss = 2.14520 avg_loss = 3.46846\n",
      "epoch no.2 train no.171850  loss = 3.92577 avg_loss = 3.51303\n",
      "epoch no.2 train no.171860  loss = 3.14104 avg_loss = 3.51245\n",
      "epoch no.2 train no.171870  loss = 3.19721 avg_loss = 3.55944\n",
      "epoch no.2 train no.171880  loss = 3.65204 avg_loss = 3.54354\n",
      "epoch no.2 train no.171890  loss = 2.72401 avg_loss = 3.53477\n",
      "epoch no.2 train no.171900  loss = 4.90410 avg_loss = 3.60221\n",
      "epoch no.2 train no.171910  loss = 3.93447 avg_loss = 3.57835\n",
      "epoch no.2 train no.171920  loss = 2.77651 avg_loss = 3.56430\n",
      "epoch no.2 train no.171930  loss = 4.65800 avg_loss = 3.53852\n",
      "epoch no.2 train no.171940  loss = 3.89893 avg_loss = 3.48520\n",
      "epoch no.2 train no.171950  loss = 3.23917 avg_loss = 3.47865\n",
      "epoch no.2 train no.171960  loss = 2.12183 avg_loss = 3.46645\n",
      "epoch no.2 train no.171970  loss = 3.43031 avg_loss = 3.47043\n",
      "epoch no.2 train no.171980  loss = 2.83567 avg_loss = 3.47612\n",
      "epoch no.2 train no.171990  loss = 3.05820 avg_loss = 3.48055\n",
      "epoch no.2 train no.172000  loss = 2.89850 avg_loss = 3.44236\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '시는', '▁노래', '▁발라', '▁노래', '</s>']\n",
      "여름밤을 적시는 감성적인 노래</s>\n",
      "epoch no.2 train no.172010  loss = 3.26062 avg_loss = 3.43023\n",
      "epoch no.2 train no.172020  loss = 3.07296 avg_loss = 3.42683\n",
      "epoch no.2 train no.172030  loss = 3.81673 avg_loss = 3.49812\n",
      "epoch no.2 train no.172040  loss = 2.21999 avg_loss = 3.48511\n",
      "epoch no.2 train no.172050  loss = 4.62077 avg_loss = 3.55459\n",
      "epoch no.2 train no.172060  loss = 3.64799 avg_loss = 3.54163\n",
      "epoch no.2 train no.172070  loss = 3.02832 avg_loss = 3.53676\n",
      "epoch no.2 train no.172080  loss = 3.42137 avg_loss = 3.57713\n",
      "epoch no.2 train no.172090  loss = 5.06176 avg_loss = 3.60929\n",
      "epoch no.2 train no.172100  loss = 3.89461 avg_loss = 3.57803\n",
      "epoch no.2 train no.172110  loss = 3.37564 avg_loss = 3.54416\n",
      "epoch no.2 train no.172120  loss = 3.16546 avg_loss = 3.53818\n",
      "epoch no.2 train no.172130  loss = 4.98108 avg_loss = 3.53028\n",
      "epoch no.2 train no.172140  loss = 3.52868 avg_loss = 3.50913\n",
      "epoch no.2 train no.172150  loss = 3.41891 avg_loss = 3.51309\n",
      "epoch no.2 train no.172160  loss = 3.28557 avg_loss = 3.50654\n",
      "epoch no.2 train no.172170  loss = 4.11781 avg_loss = 3.46331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.172180  loss = 3.24536 avg_loss = 3.45309\n",
      "epoch no.2 train no.172190  loss = 3.98719 avg_loss = 3.43548\n",
      "epoch no.2 train no.172200  loss = 3.21819 avg_loss = 3.45566\n",
      "epoch no.2 train no.172210  loss = 4.01933 avg_loss = 3.46143\n",
      "epoch no.2 train no.172220  loss = 5.46205 avg_loss = 3.47732\n",
      "epoch no.2 train no.172230  loss = 2.18586 avg_loss = 3.49512\n",
      "epoch no.2 train no.172240  loss = 3.14810 avg_loss = 3.50637\n",
      "epoch no.2 train no.172250  loss = 2.92551 avg_loss = 3.49672\n",
      "epoch no.2 train no.172260  loss = 4.79708 avg_loss = 3.53181\n",
      "epoch no.2 train no.172270  loss = 2.84285 avg_loss = 3.48939\n",
      "epoch no.2 train no.172280  loss = 2.50151 avg_loss = 3.47255\n",
      "epoch no.2 train no.172290  loss = 2.79179 avg_loss = 3.51589\n",
      "epoch no.2 train no.172300  loss = 2.75831 avg_loss = 3.51103\n",
      "epoch no.2 train no.172310  loss = 2.14954 avg_loss = 3.50370\n",
      "epoch no.2 train no.172320  loss = 3.68691 avg_loss = 3.49739\n",
      "epoch no.2 train no.172330  loss = 3.32937 avg_loss = 3.48560\n",
      "epoch no.2 train no.172340  loss = 3.77982 avg_loss = 3.54188\n",
      "epoch no.2 train no.172350  loss = 2.99303 avg_loss = 3.52781\n",
      "epoch no.2 train no.172360  loss = 4.56904 avg_loss = 3.51235\n",
      "epoch no.2 train no.172370  loss = 4.09136 avg_loss = 3.53751\n",
      "epoch no.2 train no.172380  loss = 4.11667 avg_loss = 3.54297\n",
      "epoch no.2 train no.172390  loss = 4.88564 avg_loss = 3.56917\n",
      "epoch no.2 train no.172400  loss = 3.03599 avg_loss = 3.56303\n",
      "epoch no.2 train no.172410  loss = 2.43443 avg_loss = 3.57919\n",
      "epoch no.2 train no.172420  loss = 2.96930 avg_loss = 3.56927\n",
      "epoch no.2 train no.172430  loss = 3.15971 avg_loss = 3.54201\n",
      "epoch no.2 train no.172440  loss = 2.15253 avg_loss = 3.50572\n",
      "epoch no.2 train no.172450  loss = 1.61453 avg_loss = 3.49790\n",
      "epoch no.2 train no.172460  loss = 4.14921 avg_loss = 3.44964\n",
      "epoch no.2 train no.172470  loss = 2.49765 avg_loss = 3.42447\n",
      "epoch no.2 train no.172480  loss = 2.46760 avg_loss = 3.43586\n",
      "epoch no.2 train no.172490  loss = 2.07002 avg_loss = 3.44062\n",
      "epoch no.2 train no.172500  loss = 3.27127 avg_loss = 3.44989\n",
      "epoch no.2 train no.172510  loss = 4.71866 avg_loss = 3.46004\n",
      "epoch no.2 train no.172520  loss = 3.18356 avg_loss = 3.49120\n",
      "epoch no.2 train no.172530  loss = 3.45521 avg_loss = 3.55126\n",
      "epoch no.2 train no.172540  loss = 2.90604 avg_loss = 3.53791\n",
      "epoch no.2 train no.172550  loss = 2.00798 avg_loss = 3.51560\n",
      "epoch no.2 train no.172560  loss = 2.80256 avg_loss = 3.52793\n",
      "epoch no.2 train no.172570  loss = 4.45548 avg_loss = 3.52863\n",
      "epoch no.2 train no.172580  loss = 2.92464 avg_loss = 3.52496\n",
      "epoch no.2 train no.172590  loss = 3.22965 avg_loss = 3.48693\n",
      "epoch no.2 train no.172600  loss = 5.02890 avg_loss = 3.46558\n",
      "epoch no.2 train no.172610  loss = 4.52911 avg_loss = 3.45345\n",
      "epoch no.2 train no.172620  loss = 4.63498 avg_loss = 3.44851\n",
      "epoch no.2 train no.172630  loss = 3.00124 avg_loss = 3.38862\n",
      "epoch no.2 train no.172640  loss = 3.06548 avg_loss = 3.36191\n",
      "epoch no.2 train no.172650  loss = 3.83568 avg_loss = 3.33421\n",
      "epoch no.2 train no.172660  loss = 4.19846 avg_loss = 3.33505\n",
      "epoch no.2 train no.172670  loss = 4.35407 avg_loss = 3.37419\n",
      "epoch no.2 train no.172680  loss = 3.82113 avg_loss = 3.35759\n",
      "epoch no.2 train no.172690  loss = 4.14580 avg_loss = 3.39622\n",
      "epoch no.2 train no.172700  loss = 4.26356 avg_loss = 3.41319\n",
      "epoch no.2 train no.172710  loss = 5.14268 avg_loss = 3.47079\n",
      "epoch no.2 train no.172720  loss = 2.47600 avg_loss = 3.43857\n",
      "epoch no.2 train no.172730  loss = 3.74521 avg_loss = 3.44788\n",
      "epoch no.2 train no.172740  loss = 2.75435 avg_loss = 3.42977\n",
      "epoch no.2 train no.172750  loss = 4.12237 avg_loss = 3.49521\n",
      "epoch no.2 train no.172760  loss = 3.33631 avg_loss = 3.44870\n",
      "epoch no.2 train no.172770  loss = 3.09362 avg_loss = 3.44685\n",
      "epoch no.2 train no.172780  loss = 4.39994 avg_loss = 3.43797\n",
      "epoch no.2 train no.172790  loss = 3.23367 avg_loss = 3.49724\n",
      "epoch no.2 train no.172800  loss = 3.33855 avg_loss = 3.53553\n",
      "epoch no.2 train no.172810  loss = 2.59065 avg_loss = 3.51149\n",
      "epoch no.2 train no.172820  loss = 5.70068 avg_loss = 3.54243\n",
      "epoch no.2 train no.172830  loss = 2.36311 avg_loss = 3.52836\n",
      "epoch no.2 train no.172840  loss = 3.46964 avg_loss = 3.51249\n",
      "epoch no.2 train no.172850  loss = 3.09389 avg_loss = 3.53082\n",
      "epoch no.2 train no.172860  loss = 3.20573 avg_loss = 3.57534\n",
      "epoch no.2 train no.172870  loss = 3.92981 avg_loss = 3.60294\n",
      "epoch no.2 train no.172880  loss = 3.36236 avg_loss = 3.57046\n",
      "epoch no.2 train no.172890  loss = 4.49623 avg_loss = 3.55368\n",
      "epoch no.2 train no.172900  loss = 2.50370 avg_loss = 3.58328\n",
      "epoch no.2 train no.172910  loss = 3.43959 avg_loss = 3.62381\n",
      "epoch no.2 train no.172920  loss = 2.31127 avg_loss = 3.63033\n",
      "epoch no.2 train no.172930  loss = 4.91157 avg_loss = 3.67918\n",
      "epoch no.2 train no.172940  loss = 3.20499 avg_loss = 3.64945\n",
      "epoch no.2 train no.172950  loss = 2.46228 avg_loss = 3.62440\n",
      "epoch no.2 train no.172960  loss = 5.35536 avg_loss = 3.60351\n",
      "epoch no.2 train no.172970  loss = 3.07346 avg_loss = 3.57509\n",
      "epoch no.2 train no.172980  loss = 4.23374 avg_loss = 3.56529\n",
      "epoch no.2 train no.172990  loss = 4.72512 avg_loss = 3.57923\n",
      "epoch no.2 train no.173000  loss = 2.50498 avg_loss = 3.52631\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓은', '▁감성', '</s>', '</s>']\n",
      "여름밤을 수놓는 노래들</s>\n",
      "epoch no.2 train no.173010  loss = 4.09188 avg_loss = 3.56115\n",
      "epoch no.2 train no.173020  loss = 3.89470 avg_loss = 3.54272\n",
      "epoch no.2 train no.173030  loss = 4.40878 avg_loss = 3.50826\n",
      "epoch no.2 train no.173040  loss = 3.06007 avg_loss = 3.48044\n",
      "epoch no.2 train no.173050  loss = 4.20285 avg_loss = 3.50542\n",
      "epoch no.2 train no.173060  loss = 3.37404 avg_loss = 3.46107\n",
      "epoch no.2 train no.173070  loss = 3.64323 avg_loss = 3.47631\n",
      "epoch no.2 train no.173080  loss = 4.54635 avg_loss = 3.52746\n",
      "epoch no.2 train no.173090  loss = 2.64543 avg_loss = 3.47470\n",
      "epoch no.2 train no.173100  loss = 3.39042 avg_loss = 3.47450\n",
      "epoch no.2 train no.173110  loss = 3.10430 avg_loss = 3.45455\n",
      "epoch no.2 train no.173120  loss = 4.65477 avg_loss = 3.49955\n",
      "epoch no.2 train no.173130  loss = 2.69893 avg_loss = 3.54769\n",
      "epoch no.2 train no.173140  loss = 2.64794 avg_loss = 3.57080\n",
      "epoch no.2 train no.173150  loss = 2.69975 avg_loss = 3.51053\n",
      "epoch no.2 train no.173160  loss = 3.13182 avg_loss = 3.52967\n",
      "epoch no.2 train no.173170  loss = 2.97862 avg_loss = 3.48234\n",
      "epoch no.2 train no.173180  loss = 3.07581 avg_loss = 3.47494\n",
      "epoch no.2 train no.173190  loss = 3.65563 avg_loss = 3.44748\n",
      "epoch no.2 train no.173200  loss = 3.63219 avg_loss = 3.45679\n",
      "epoch no.2 train no.173210  loss = 3.70699 avg_loss = 3.44207\n",
      "epoch no.2 train no.173220  loss = 4.29048 avg_loss = 3.48722\n",
      "epoch no.2 train no.173230  loss = 5.25755 avg_loss = 3.48860\n",
      "epoch no.2 train no.173240  loss = 5.07025 avg_loss = 3.53013\n",
      "epoch no.2 train no.173250  loss = 3.44839 avg_loss = 3.53138\n",
      "epoch no.2 train no.173260  loss = 3.66991 avg_loss = 3.50413\n",
      "epoch no.2 train no.173270  loss = 2.88852 avg_loss = 3.52621\n",
      "epoch no.2 train no.173280  loss = 4.46167 avg_loss = 3.49769\n",
      "epoch no.2 train no.173290  loss = 3.02112 avg_loss = 3.50501\n",
      "epoch no.2 train no.173300  loss = 4.05076 avg_loss = 3.49642\n",
      "epoch no.2 train no.173310  loss = 4.97676 avg_loss = 3.51922\n",
      "epoch no.2 train no.173320  loss = 4.24841 avg_loss = 3.46314\n",
      "epoch no.2 train no.173330  loss = 3.64171 avg_loss = 3.44944\n",
      "epoch no.2 train no.173340  loss = 2.66176 avg_loss = 3.42746\n",
      "epoch no.2 train no.173350  loss = 1.97310 avg_loss = 3.50425\n",
      "epoch no.2 train no.173360  loss = 3.44184 avg_loss = 3.47005\n",
      "epoch no.2 train no.173370  loss = 3.72715 avg_loss = 3.43529\n",
      "epoch no.2 train no.173380  loss = 3.47370 avg_loss = 3.41962\n",
      "epoch no.2 train no.173390  loss = 2.73195 avg_loss = 3.39908\n",
      "epoch no.2 train no.173400  loss = 2.20857 avg_loss = 3.37293\n",
      "epoch no.2 train no.173410  loss = 4.45101 avg_loss = 3.37918\n",
      "epoch no.2 train no.173420  loss = 5.05944 avg_loss = 3.37992\n",
      "epoch no.2 train no.173430  loss = 4.91345 avg_loss = 3.39185\n",
      "epoch no.2 train no.173440  loss = 3.35263 avg_loss = 3.35075\n",
      "epoch no.2 train no.173450  loss = 3.81956 avg_loss = 3.37741\n",
      "epoch no.2 train no.173460  loss = 4.39273 avg_loss = 3.44079\n",
      "epoch no.2 train no.173470  loss = 3.17722 avg_loss = 3.43889\n",
      "epoch no.2 train no.173480  loss = 5.20742 avg_loss = 3.44883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.173490  loss = 5.48690 avg_loss = 3.46869\n",
      "epoch no.2 train no.173500  loss = 1.84649 avg_loss = 3.51362\n",
      "epoch no.2 train no.173510  loss = 4.43235 avg_loss = 3.50669\n",
      "epoch no.2 train no.173520  loss = 2.91599 avg_loss = 3.49526\n",
      "epoch no.2 train no.173530  loss = 3.57736 avg_loss = 3.50697\n",
      "epoch no.2 train no.173540  loss = 2.13186 avg_loss = 3.41206\n",
      "epoch no.2 train no.173550  loss = 1.68729 avg_loss = 3.43268\n",
      "epoch no.2 train no.173560  loss = 4.35758 avg_loss = 3.37997\n",
      "epoch no.2 train no.173570  loss = 3.56177 avg_loss = 3.43158\n",
      "epoch no.2 train no.173580  loss = 2.71867 avg_loss = 3.43835\n",
      "epoch no.2 train no.173590  loss = 2.85616 avg_loss = 3.41231\n",
      "epoch no.2 train no.173600  loss = 4.83205 avg_loss = 3.38977\n",
      "epoch no.2 train no.173610  loss = 3.38362 avg_loss = 3.42558\n",
      "epoch no.2 train no.173620  loss = 7.18839 avg_loss = 3.50919\n",
      "epoch no.2 train no.173630  loss = 3.69286 avg_loss = 3.50608\n",
      "epoch no.2 train no.173640  loss = 3.84169 avg_loss = 3.47981\n",
      "epoch no.2 train no.173650  loss = 3.54119 avg_loss = 3.44656\n",
      "epoch no.2 train no.173660  loss = 3.00575 avg_loss = 3.43205\n",
      "epoch no.2 train no.173670  loss = 2.35414 avg_loss = 3.44492\n",
      "epoch no.2 train no.173680  loss = 2.41910 avg_loss = 3.39719\n",
      "epoch no.2 train no.173690  loss = 2.78661 avg_loss = 3.45428\n",
      "epoch no.2 train no.173700  loss = 2.84113 avg_loss = 3.44249\n",
      "epoch no.2 train no.173710  loss = 3.04590 avg_loss = 3.45534\n",
      "epoch no.2 train no.173720  loss = 4.48427 avg_loss = 3.46504\n",
      "epoch no.2 train no.173730  loss = 4.22426 avg_loss = 3.50678\n",
      "epoch no.2 train no.173740  loss = 2.56056 avg_loss = 3.53669\n",
      "epoch no.2 train no.173750  loss = 2.95925 avg_loss = 3.52217\n",
      "epoch no.2 train no.173760  loss = 3.82503 avg_loss = 3.51030\n",
      "epoch no.2 train no.173770  loss = 2.80346 avg_loss = 3.53264\n",
      "epoch no.2 train no.173780  loss = 2.39120 avg_loss = 3.49622\n",
      "epoch no.2 train no.173790  loss = 2.97550 avg_loss = 3.52540\n",
      "epoch no.2 train no.173800  loss = 4.87854 avg_loss = 3.49190\n",
      "epoch no.2 train no.173810  loss = 4.05687 avg_loss = 3.54068\n",
      "epoch no.2 train no.173820  loss = 3.49729 avg_loss = 3.53850\n",
      "epoch no.2 train no.173830  loss = 3.76454 avg_loss = 3.54482\n",
      "epoch no.2 train no.173840  loss = 3.81318 avg_loss = 3.52933\n",
      "epoch no.2 train no.173850  loss = 2.21114 avg_loss = 3.50991\n",
      "epoch no.2 train no.173860  loss = 2.38067 avg_loss = 3.52611\n",
      "epoch no.2 train no.173870  loss = 3.32511 avg_loss = 3.55253\n",
      "epoch no.2 train no.173880  loss = 3.74332 avg_loss = 3.57119\n",
      "epoch no.2 train no.173890  loss = 3.99726 avg_loss = 3.57729\n",
      "epoch no.2 train no.173900  loss = 2.80103 avg_loss = 3.54257\n",
      "epoch no.2 train no.173910  loss = 3.22406 avg_loss = 3.50474\n",
      "epoch no.2 train no.173920  loss = 5.00390 avg_loss = 3.53147\n",
      "epoch no.2 train no.173930  loss = 2.56739 avg_loss = 3.50465\n",
      "epoch no.2 train no.173940  loss = 2.67796 avg_loss = 3.47196\n",
      "epoch no.2 train no.173950  loss = 5.37623 avg_loss = 3.48027\n",
      "epoch no.2 train no.173960  loss = 3.69881 avg_loss = 3.48852\n",
      "epoch no.2 train no.173970  loss = 3.14876 avg_loss = 3.47495\n",
      "epoch no.2 train no.173980  loss = 2.19102 avg_loss = 3.46162\n",
      "epoch no.2 train no.173990  loss = 4.31189 avg_loss = 3.45559\n",
      "epoch no.2 train no.174000  loss = 4.40207 avg_loss = 3.52945\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '▁한강', '▁재즈', '적인', '▁재즈', '</s>']\n",
      "여름밤의 낭만적인 재즈</s>\n",
      "epoch no.2 train no.174010  loss = 3.46849 avg_loss = 3.51084\n",
      "epoch no.2 train no.174020  loss = 3.38610 avg_loss = 3.52347\n",
      "epoch no.2 train no.174030  loss = 3.45087 avg_loss = 3.53814\n",
      "epoch no.2 train no.174040  loss = 2.42754 avg_loss = 3.57608\n",
      "epoch no.2 train no.174050  loss = 4.23711 avg_loss = 3.55407\n",
      "epoch no.2 train no.174060  loss = 2.47179 avg_loss = 3.55814\n",
      "epoch no.2 train no.174070  loss = 2.58694 avg_loss = 3.48894\n",
      "epoch no.2 train no.174080  loss = 3.03487 avg_loss = 3.46267\n",
      "epoch no.2 train no.174090  loss = 4.40407 avg_loss = 3.48477\n",
      "epoch no.2 train no.174100  loss = 3.93227 avg_loss = 3.51411\n",
      "epoch no.2 train no.174110  loss = 2.50559 avg_loss = 3.49703\n",
      "epoch no.2 train no.174120  loss = 2.15718 avg_loss = 3.54539\n",
      "epoch no.2 train no.174130  loss = 2.74684 avg_loss = 3.52439\n",
      "epoch no.2 train no.174140  loss = 3.91884 avg_loss = 3.54747\n",
      "epoch no.2 train no.174150  loss = 3.63680 avg_loss = 3.50363\n",
      "epoch no.2 train no.174160  loss = 3.92206 avg_loss = 3.53433\n",
      "epoch no.2 train no.174170  loss = 4.97594 avg_loss = 3.53629\n",
      "epoch no.2 train no.174180  loss = 6.87305 avg_loss = 3.61048\n",
      "epoch no.2 train no.174190  loss = 1.87433 avg_loss = 3.59993\n",
      "epoch no.2 train no.174200  loss = 1.70256 avg_loss = 3.53884\n",
      "epoch no.2 train no.174210  loss = 4.85746 avg_loss = 3.53366\n",
      "epoch no.2 train no.174220  loss = 2.43633 avg_loss = 3.52839\n",
      "epoch no.2 train no.174230  loss = 3.10796 avg_loss = 3.52022\n",
      "epoch no.2 train no.174240  loss = 3.72977 avg_loss = 3.49961\n",
      "epoch no.2 train no.174250  loss = 2.20030 avg_loss = 3.51970\n",
      "epoch no.2 train no.174260  loss = 3.39469 avg_loss = 3.55991\n",
      "epoch no.2 train no.174270  loss = 5.76116 avg_loss = 3.55853\n",
      "epoch no.2 train no.174280  loss = 3.07349 avg_loss = 3.51936\n",
      "epoch no.2 train no.174290  loss = 2.13065 avg_loss = 3.46880\n",
      "epoch no.2 train no.174300  loss = 3.25170 avg_loss = 3.46756\n",
      "epoch no.2 train no.174310  loss = 2.96796 avg_loss = 3.46615\n",
      "epoch no.2 train no.174320  loss = 2.22581 avg_loss = 3.41588\n",
      "epoch no.2 train no.174330  loss = 6.07312 avg_loss = 3.44795\n",
      "epoch no.2 train no.174340  loss = 3.39089 avg_loss = 3.43896\n",
      "epoch no.2 train no.174350  loss = 3.06845 avg_loss = 3.45866\n",
      "epoch no.2 train no.174360  loss = 2.75131 avg_loss = 3.48448\n",
      "epoch no.2 train no.174370  loss = 3.30440 avg_loss = 3.50874\n",
      "epoch no.2 train no.174380  loss = 3.60327 avg_loss = 3.47177\n",
      "epoch no.2 train no.174390  loss = 5.46326 avg_loss = 3.47157\n",
      "epoch no.2 train no.174400  loss = 4.72785 avg_loss = 3.46689\n",
      "epoch no.2 train no.174410  loss = 2.27607 avg_loss = 3.42210\n",
      "epoch no.2 train no.174420  loss = 3.21937 avg_loss = 3.35028\n",
      "epoch no.2 train no.174430  loss = 2.40239 avg_loss = 3.40636\n",
      "epoch no.2 train no.174440  loss = 3.39264 avg_loss = 3.36599\n",
      "epoch no.2 train no.174450  loss = 3.29408 avg_loss = 3.40529\n",
      "epoch no.2 train no.174460  loss = 2.89152 avg_loss = 3.43623\n",
      "epoch no.2 train no.174470  loss = 2.68300 avg_loss = 3.39062\n",
      "epoch no.2 train no.174480  loss = 4.73904 avg_loss = 3.41133\n",
      "epoch no.2 train no.174490  loss = 5.06236 avg_loss = 3.40713\n",
      "epoch no.2 train no.174500  loss = 2.62233 avg_loss = 3.42201\n",
      "epoch no.2 train no.174510  loss = 3.88708 avg_loss = 3.43578\n",
      "epoch no.2 train no.174520  loss = 2.51576 avg_loss = 3.41380\n",
      "epoch no.2 train no.174530  loss = 4.82221 avg_loss = 3.40813\n",
      "epoch no.2 train no.174540  loss = 3.31244 avg_loss = 3.46798\n",
      "epoch no.2 train no.174550  loss = 3.37088 avg_loss = 3.44734\n",
      "epoch no.2 train no.174560  loss = 2.96908 avg_loss = 3.45098\n",
      "epoch no.2 train no.174570  loss = 2.31849 avg_loss = 3.48641\n",
      "epoch no.2 train no.174580  loss = 2.97781 avg_loss = 3.51724\n",
      "epoch no.2 train no.174590  loss = 3.19508 avg_loss = 3.55968\n",
      "epoch no.2 train no.174600  loss = 3.65980 avg_loss = 3.54495\n",
      "epoch no.2 train no.174610  loss = 3.38882 avg_loss = 3.52239\n",
      "epoch no.2 train no.174620  loss = 2.70476 avg_loss = 3.50825\n",
      "epoch no.2 train no.174630  loss = 1.57923 avg_loss = 3.50640\n",
      "epoch no.2 train no.174640  loss = 2.10367 avg_loss = 3.48882\n",
      "epoch no.2 train no.174650  loss = 2.54142 avg_loss = 3.47652\n",
      "epoch no.2 train no.174660  loss = 5.10990 avg_loss = 3.51285\n",
      "epoch no.2 train no.174670  loss = 3.46606 avg_loss = 3.50378\n",
      "epoch no.2 train no.174680  loss = 2.46102 avg_loss = 3.52031\n",
      "epoch no.2 train no.174690  loss = 3.22265 avg_loss = 3.56048\n",
      "epoch no.2 train no.174700  loss = 3.50804 avg_loss = 3.55121\n",
      "epoch no.2 train no.174710  loss = 2.30313 avg_loss = 3.53641\n",
      "epoch no.2 train no.174720  loss = 2.91321 avg_loss = 3.55984\n",
      "epoch no.2 train no.174730  loss = 2.48235 avg_loss = 3.51933\n",
      "epoch no.2 train no.174740  loss = 3.04873 avg_loss = 3.54368\n",
      "epoch no.2 train no.174750  loss = 2.55620 avg_loss = 3.56709\n",
      "epoch no.2 train no.174760  loss = 4.68771 avg_loss = 3.57928\n",
      "epoch no.2 train no.174770  loss = 3.41777 avg_loss = 3.59800\n",
      "epoch no.2 train no.174780  loss = 3.35705 avg_loss = 3.62043\n",
      "epoch no.2 train no.174790  loss = 2.41084 avg_loss = 3.59365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.174800  loss = 2.24369 avg_loss = 3.59519\n",
      "epoch no.2 train no.174810  loss = 3.29898 avg_loss = 3.57157\n",
      "epoch no.2 train no.174820  loss = 2.40028 avg_loss = 3.51800\n",
      "epoch no.2 train no.174830  loss = 2.39900 avg_loss = 3.49633\n",
      "epoch no.2 train no.174840  loss = 2.66218 avg_loss = 3.51008\n",
      "epoch no.2 train no.174850  loss = 3.12326 avg_loss = 3.46398\n",
      "epoch no.2 train no.174860  loss = 3.31929 avg_loss = 3.48938\n",
      "epoch no.2 train no.174870  loss = 3.38880 avg_loss = 3.48892\n",
      "epoch no.2 train no.174880  loss = 3.42289 avg_loss = 3.49059\n",
      "epoch no.2 train no.174890  loss = 3.53864 avg_loss = 3.47858\n",
      "epoch no.2 train no.174900  loss = 2.59740 avg_loss = 3.45248\n",
      "epoch no.2 train no.174910  loss = 3.83314 avg_loss = 3.40921\n",
      "epoch no.2 train no.174920  loss = 2.71002 avg_loss = 3.38259\n",
      "epoch no.2 train no.174930  loss = 3.11489 avg_loss = 3.37462\n",
      "epoch no.2 train no.174940  loss = 3.35543 avg_loss = 3.42361\n",
      "epoch no.2 train no.174950  loss = 3.17367 avg_loss = 3.50076\n",
      "epoch no.2 train no.174960  loss = 3.10870 avg_loss = 3.48206\n",
      "epoch no.2 train no.174970  loss = 3.00961 avg_loss = 3.49235\n",
      "epoch no.2 train no.174980  loss = 3.50575 avg_loss = 3.46285\n",
      "epoch no.2 train no.174990  loss = 3.97081 avg_loss = 3.45096\n",
      "epoch no.2 train no.175000  loss = 2.01588 avg_loss = 3.48673\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '▁신나는', '▁시원한', '▁노래', '지', '</s>']\n",
      "여름엔 역시 이 노래지</s>\n",
      "epoch no.2 train no.175010  loss = 2.52296 avg_loss = 3.47073\n",
      "epoch no.2 train no.175020  loss = 3.69345 avg_loss = 3.46121\n",
      "epoch no.2 train no.175030  loss = 3.41672 avg_loss = 3.51060\n",
      "epoch no.2 train no.175040  loss = 3.85030 avg_loss = 3.49993\n",
      "epoch no.2 train no.175050  loss = 5.43803 avg_loss = 3.50415\n",
      "epoch no.2 train no.175060  loss = 2.64738 avg_loss = 3.48283\n",
      "epoch no.2 train no.175070  loss = 3.02743 avg_loss = 3.43801\n",
      "epoch no.2 train no.175080  loss = 3.13335 avg_loss = 3.44577\n",
      "epoch no.2 train no.175090  loss = 2.63257 avg_loss = 3.45770\n",
      "epoch no.2 train no.175100  loss = 1.30135 avg_loss = 3.44140\n",
      "epoch no.2 train no.175110  loss = 3.49527 avg_loss = 3.43204\n",
      "epoch no.2 train no.175120  loss = 3.09205 avg_loss = 3.48359\n",
      "epoch no.2 train no.175130  loss = 2.33135 avg_loss = 3.42296\n",
      "epoch no.2 train no.175140  loss = 3.70444 avg_loss = 3.46333\n",
      "epoch no.2 train no.175150  loss = 4.10604 avg_loss = 3.51135\n",
      "epoch no.2 train no.175160  loss = 3.14493 avg_loss = 3.53396\n",
      "epoch no.2 train no.175170  loss = 2.14669 avg_loss = 3.48138\n",
      "epoch no.2 train no.175180  loss = 2.56388 avg_loss = 3.47358\n",
      "epoch no.2 train no.175190  loss = 3.82853 avg_loss = 3.45674\n",
      "epoch no.2 train no.175200  loss = 2.95688 avg_loss = 3.40029\n",
      "epoch no.2 train no.175210  loss = 2.30552 avg_loss = 3.42332\n",
      "epoch no.2 train no.175220  loss = 3.89602 avg_loss = 3.44278\n",
      "epoch no.2 train no.175230  loss = 3.59098 avg_loss = 3.41526\n",
      "epoch no.2 train no.175240  loss = 2.38462 avg_loss = 3.42604\n",
      "epoch no.2 train no.175250  loss = 2.69183 avg_loss = 3.41978\n",
      "epoch no.2 train no.175260  loss = 3.57438 avg_loss = 3.41632\n",
      "epoch no.2 train no.175270  loss = 3.75053 avg_loss = 3.42503\n",
      "epoch no.2 train no.175280  loss = 3.20307 avg_loss = 3.44406\n",
      "epoch no.2 train no.175290  loss = 4.47092 avg_loss = 3.44212\n",
      "epoch no.2 train no.175300  loss = 2.56043 avg_loss = 3.45942\n",
      "epoch no.2 train no.175310  loss = 2.55700 avg_loss = 3.47251\n",
      "epoch no.2 train no.175320  loss = 2.83363 avg_loss = 3.45572\n",
      "epoch no.2 train no.175330  loss = 4.67604 avg_loss = 3.47825\n",
      "epoch no.2 train no.175340  loss = 2.77921 avg_loss = 3.45881\n",
      "epoch no.2 train no.175350  loss = 4.81137 avg_loss = 3.48521\n",
      "epoch no.2 train no.175360  loss = 4.23138 avg_loss = 3.45451\n",
      "epoch no.2 train no.175370  loss = 2.07475 avg_loss = 3.43628\n",
      "epoch no.2 train no.175380  loss = 3.08159 avg_loss = 3.41114\n",
      "epoch no.2 train no.175390  loss = 4.89409 avg_loss = 3.45188\n",
      "epoch no.2 train no.175400  loss = 1.88880 avg_loss = 3.45873\n",
      "epoch no.2 train no.175410  loss = 2.73417 avg_loss = 3.47618\n",
      "epoch no.2 train no.175420  loss = 3.02067 avg_loss = 3.48117\n",
      "epoch no.2 train no.175430  loss = 1.73457 avg_loss = 3.44439\n",
      "epoch no.2 train no.175440  loss = 6.13330 avg_loss = 3.49770\n",
      "epoch no.2 train no.175450  loss = 4.25336 avg_loss = 3.48277\n",
      "epoch no.2 train no.175460  loss = 2.65303 avg_loss = 3.43395\n",
      "epoch no.2 train no.175470  loss = 3.26275 avg_loss = 3.42022\n",
      "epoch no.2 train no.175480  loss = 3.00197 avg_loss = 3.48508\n",
      "epoch no.2 train no.175490  loss = 2.45374 avg_loss = 3.47465\n",
      "epoch no.2 train no.175500  loss = 2.28387 avg_loss = 3.48035\n",
      "epoch no.2 train no.175510  loss = 4.65166 avg_loss = 3.49893\n",
      "epoch no.2 train no.175520  loss = 3.37456 avg_loss = 3.48723\n",
      "epoch no.2 train no.175530  loss = 3.89181 avg_loss = 3.44675\n",
      "epoch no.2 train no.175540  loss = 3.16584 avg_loss = 3.46387\n",
      "epoch no.2 train no.175550  loss = 3.17435 avg_loss = 3.42653\n",
      "epoch no.2 train no.175560  loss = 3.73014 avg_loss = 3.42392\n",
      "epoch no.2 train no.175570  loss = 2.34432 avg_loss = 3.46680\n",
      "epoch no.2 train no.175580  loss = 4.58231 avg_loss = 3.47024\n",
      "epoch no.2 train no.175590  loss = 3.16377 avg_loss = 3.50833\n",
      "epoch no.2 train no.175600  loss = 4.83617 avg_loss = 3.56166\n",
      "epoch no.2 train no.175610  loss = 2.52401 avg_loss = 3.54295\n",
      "epoch no.2 train no.175620  loss = 3.54115 avg_loss = 3.54443\n",
      "epoch no.2 train no.175630  loss = 3.74881 avg_loss = 3.51853\n",
      "epoch no.2 train no.175640  loss = 2.87971 avg_loss = 3.49174\n",
      "epoch no.2 train no.175650  loss = 2.52558 avg_loss = 3.45540\n",
      "epoch no.2 train no.175660  loss = 7.02405 avg_loss = 3.53900\n",
      "epoch no.2 train no.175670  loss = 7.70197 avg_loss = 3.55406\n",
      "epoch no.2 train no.175680  loss = 3.70989 avg_loss = 3.55394\n",
      "epoch no.2 train no.175690  loss = 2.83701 avg_loss = 3.54831\n",
      "epoch no.2 train no.175700  loss = 2.46533 avg_loss = 3.53065\n",
      "epoch no.2 train no.175710  loss = 1.63846 avg_loss = 3.50327\n",
      "epoch no.2 train no.175720  loss = 4.31237 avg_loss = 3.51225\n",
      "epoch no.2 train no.175730  loss = 4.63374 avg_loss = 3.54034\n",
      "epoch no.2 train no.175740  loss = 4.40883 avg_loss = 3.52821\n",
      "epoch no.2 train no.175750  loss = 4.65745 avg_loss = 3.58407\n",
      "epoch no.2 train no.175760  loss = 2.34631 avg_loss = 3.58627\n",
      "epoch no.2 train no.175770  loss = 4.44124 avg_loss = 3.63939\n",
      "epoch no.2 train no.175780  loss = 2.53020 avg_loss = 3.63821\n",
      "epoch no.2 train no.175790  loss = 2.32192 avg_loss = 3.61251\n",
      "epoch no.2 train no.175800  loss = 2.70877 avg_loss = 3.61734\n",
      "epoch no.2 train no.175810  loss = 4.57603 avg_loss = 3.61302\n",
      "epoch no.2 train no.175820  loss = 4.16992 avg_loss = 3.57829\n",
      "epoch no.2 train no.175830  loss = 3.24357 avg_loss = 3.58816\n",
      "epoch no.2 train no.175840  loss = 2.07099 avg_loss = 3.50083\n",
      "epoch no.2 train no.175850  loss = 2.91324 avg_loss = 3.48765\n",
      "epoch no.2 train no.175860  loss = 3.96801 avg_loss = 3.45465\n",
      "epoch no.2 train no.175870  loss = 1.95040 avg_loss = 3.44320\n",
      "epoch no.2 train no.175880  loss = 4.23642 avg_loss = 3.50598\n",
      "epoch no.2 train no.175890  loss = 1.54102 avg_loss = 3.46617\n",
      "epoch no.2 train no.175900  loss = 5.36088 avg_loss = 3.47880\n",
      "epoch no.2 train no.175910  loss = 4.05310 avg_loss = 3.46612\n",
      "epoch no.2 train no.175920  loss = 4.98510 avg_loss = 3.46279\n",
      "epoch no.2 train no.175930  loss = 6.18687 avg_loss = 3.48936\n",
      "epoch no.2 train no.175940  loss = 2.58215 avg_loss = 3.46294\n",
      "epoch no.2 train no.175950  loss = 3.94164 avg_loss = 3.46665\n",
      "epoch no.2 train no.175960  loss = 3.06116 avg_loss = 3.43044\n",
      "epoch no.2 train no.175970  loss = 2.81108 avg_loss = 3.41088\n",
      "epoch no.2 train no.175980  loss = 3.43850 avg_loss = 3.40875\n",
      "epoch no.2 train no.175990  loss = 2.28594 avg_loss = 3.41679\n",
      "epoch no.2 train no.176000  loss = 2.70222 avg_loss = 3.36018\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓은', '▁노래', '</s>']\n",
      "여름밤을 수놓은 노래</s>\n",
      "epoch no.2 train no.176010  loss = 3.75405 avg_loss = 3.37436\n",
      "epoch no.2 train no.176020  loss = 3.00272 avg_loss = 3.39750\n",
      "epoch no.2 train no.176030  loss = 4.11381 avg_loss = 3.44564\n",
      "epoch no.2 train no.176040  loss = 2.13292 avg_loss = 3.41968\n",
      "epoch no.2 train no.176050  loss = 1.67472 avg_loss = 3.45072\n",
      "epoch no.2 train no.176060  loss = 1.90236 avg_loss = 3.41399\n",
      "epoch no.2 train no.176070  loss = 3.49047 avg_loss = 3.39081\n",
      "epoch no.2 train no.176080  loss = 6.13516 avg_loss = 3.43063\n",
      "epoch no.2 train no.176090  loss = 3.66841 avg_loss = 3.43986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.176100  loss = 3.69258 avg_loss = 3.46665\n",
      "epoch no.2 train no.176110  loss = 3.99246 avg_loss = 3.49576\n",
      "epoch no.2 train no.176120  loss = 4.00146 avg_loss = 3.46514\n",
      "epoch no.2 train no.176130  loss = 2.47859 avg_loss = 3.49437\n",
      "epoch no.2 train no.176140  loss = 2.60486 avg_loss = 3.50513\n",
      "epoch no.2 train no.176150  loss = 3.09016 avg_loss = 3.54097\n",
      "epoch no.2 train no.176160  loss = 1.92902 avg_loss = 3.48372\n",
      "epoch no.2 train no.176170  loss = 4.49063 avg_loss = 3.53302\n",
      "epoch no.2 train no.176180  loss = 4.74980 avg_loss = 3.50950\n",
      "epoch no.2 train no.176190  loss = 4.64168 avg_loss = 3.51899\n",
      "epoch no.2 train no.176200  loss = 5.12673 avg_loss = 3.50615\n",
      "epoch no.2 train no.176210  loss = 2.78175 avg_loss = 3.47279\n",
      "epoch no.2 train no.176220  loss = 4.48941 avg_loss = 3.55207\n",
      "epoch no.2 train no.176230  loss = 1.65955 avg_loss = 3.50295\n",
      "epoch no.2 train no.176240  loss = 2.23141 avg_loss = 3.46067\n",
      "epoch no.2 train no.176250  loss = 3.93005 avg_loss = 3.45432\n",
      "epoch no.2 train no.176260  loss = 2.62621 avg_loss = 3.42217\n",
      "epoch no.2 train no.176270  loss = 3.32054 avg_loss = 3.40153\n",
      "epoch no.2 train no.176280  loss = 3.05851 avg_loss = 3.45656\n",
      "epoch no.2 train no.176290  loss = 6.80796 avg_loss = 3.51040\n",
      "epoch no.2 train no.176300  loss = 3.12159 avg_loss = 3.47815\n",
      "epoch no.2 train no.176310  loss = 3.52033 avg_loss = 3.51088\n",
      "epoch no.2 train no.176320  loss = 3.22372 avg_loss = 3.51779\n",
      "epoch no.2 train no.176330  loss = 4.36763 avg_loss = 3.46412\n",
      "epoch no.2 train no.176340  loss = 2.82341 avg_loss = 3.43990\n",
      "epoch no.2 train no.176350  loss = 3.67896 avg_loss = 3.44289\n",
      "epoch no.2 train no.176360  loss = 2.88895 avg_loss = 3.41826\n",
      "epoch no.2 train no.176370  loss = 5.65842 avg_loss = 3.41874\n",
      "epoch no.2 train no.176380  loss = 4.52638 avg_loss = 3.46173\n",
      "epoch no.2 train no.176390  loss = 3.74670 avg_loss = 3.44484\n",
      "epoch no.2 train no.176400  loss = 3.56239 avg_loss = 3.44733\n",
      "epoch no.2 train no.176410  loss = 3.41778 avg_loss = 3.43324\n",
      "epoch no.2 train no.176420  loss = 4.33516 avg_loss = 3.42526\n",
      "epoch no.2 train no.176430  loss = 3.21797 avg_loss = 3.43341\n",
      "epoch no.2 train no.176440  loss = 4.36024 avg_loss = 3.42989\n",
      "epoch no.2 train no.176450  loss = 2.20579 avg_loss = 3.44768\n",
      "epoch no.2 train no.176460  loss = 2.54473 avg_loss = 3.40164\n",
      "epoch no.2 train no.176470  loss = 1.67795 avg_loss = 3.40164\n",
      "epoch no.2 train no.176480  loss = 3.10972 avg_loss = 3.42369\n",
      "epoch no.2 train no.176490  loss = 3.75489 avg_loss = 3.42100\n",
      "epoch no.2 train no.176500  loss = 2.27422 avg_loss = 3.41795\n",
      "epoch no.2 train no.176510  loss = 2.78265 avg_loss = 3.36347\n",
      "epoch no.2 train no.176520  loss = 5.01502 avg_loss = 3.43642\n",
      "epoch no.2 train no.176530  loss = 4.21362 avg_loss = 3.46359\n",
      "epoch no.2 train no.176540  loss = 3.32739 avg_loss = 3.46942\n",
      "epoch no.2 train no.176550  loss = 6.57735 avg_loss = 3.51167\n",
      "epoch no.2 train no.176560  loss = 3.37567 avg_loss = 3.48857\n",
      "epoch no.2 train no.176570  loss = 2.44881 avg_loss = 3.51104\n",
      "epoch no.2 train no.176580  loss = 2.79448 avg_loss = 3.52103\n",
      "epoch no.2 train no.176590  loss = 3.39971 avg_loss = 3.53442\n",
      "epoch no.2 train no.176600  loss = 3.19323 avg_loss = 3.52074\n",
      "epoch no.2 train no.176610  loss = 5.43721 avg_loss = 3.56927\n",
      "epoch no.2 train no.176620  loss = 2.99912 avg_loss = 3.55989\n",
      "epoch no.2 train no.176630  loss = 3.90198 avg_loss = 3.54707\n",
      "epoch no.2 train no.176640  loss = 3.54338 avg_loss = 3.51153\n",
      "epoch no.2 train no.176650  loss = 3.08108 avg_loss = 3.52159\n",
      "epoch no.2 train no.176660  loss = 2.06762 avg_loss = 3.50946\n",
      "epoch no.2 train no.176670  loss = 3.31307 avg_loss = 3.47800\n",
      "epoch no.2 train no.176680  loss = 2.52553 avg_loss = 3.45294\n",
      "epoch no.2 train no.176690  loss = 3.44705 avg_loss = 3.43431\n",
      "epoch no.2 train no.176700  loss = 2.34500 avg_loss = 3.42457\n",
      "epoch no.2 train no.176710  loss = 3.94672 avg_loss = 3.45477\n",
      "epoch no.2 train no.176720  loss = 3.97576 avg_loss = 3.44182\n",
      "epoch no.2 train no.176730  loss = 1.76478 avg_loss = 3.43985\n",
      "epoch no.2 train no.176740  loss = 5.78817 avg_loss = 3.43243\n",
      "epoch no.2 train no.176750  loss = 2.91803 avg_loss = 3.48478\n",
      "epoch no.2 train no.176760  loss = 2.41025 avg_loss = 3.46813\n",
      "epoch no.2 train no.176770  loss = 4.37915 avg_loss = 3.47195\n",
      "epoch no.2 train no.176780  loss = 1.71260 avg_loss = 3.42865\n",
      "epoch no.2 train no.176790  loss = 3.42064 avg_loss = 3.43425\n",
      "epoch no.2 train no.176800  loss = 4.35072 avg_loss = 3.46053\n",
      "epoch no.2 train no.176810  loss = 4.14995 avg_loss = 3.50608\n",
      "epoch no.2 train no.176820  loss = 3.98701 avg_loss = 3.48026\n",
      "epoch no.2 train no.176830  loss = 3.32947 avg_loss = 3.43108\n",
      "epoch no.2 train no.176840  loss = 4.50162 avg_loss = 3.41057\n",
      "epoch no.2 train no.176850  loss = 3.75884 avg_loss = 3.40427\n",
      "epoch no.2 train no.176860  loss = 5.42600 avg_loss = 3.42687\n",
      "epoch no.2 train no.176870  loss = 3.44354 avg_loss = 3.49270\n",
      "epoch no.2 train no.176880  loss = 1.86202 avg_loss = 3.43417\n",
      "epoch no.2 train no.176890  loss = 3.45840 avg_loss = 3.43288\n",
      "epoch no.2 train no.176900  loss = 2.04995 avg_loss = 3.45959\n",
      "epoch no.2 train no.176910  loss = 2.74150 avg_loss = 3.50097\n",
      "epoch no.2 train no.176920  loss = 5.18753 avg_loss = 3.48967\n",
      "epoch no.2 train no.176930  loss = 3.38563 avg_loss = 3.54044\n",
      "epoch no.2 train no.176940  loss = 2.50005 avg_loss = 3.48495\n",
      "epoch no.2 train no.176950  loss = 3.28500 avg_loss = 3.46596\n",
      "epoch no.2 train no.176960  loss = 3.88321 avg_loss = 3.50633\n",
      "epoch no.2 train no.176970  loss = 2.66153 avg_loss = 3.51968\n",
      "epoch no.2 train no.176980  loss = 3.03735 avg_loss = 3.52793\n",
      "epoch no.2 train no.176990  loss = 2.08933 avg_loss = 3.48155\n",
      "epoch no.2 train no.177000  loss = 3.88584 avg_loss = 3.46586\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '▁끝', '자', '락에', '에서', '▁듣는', '▁감성', '드', '</s>']\n",
      "여름의 끝자락에서 듣는 발라드</s>\n",
      "epoch no.2 train no.177010  loss = 3.15616 avg_loss = 3.42610\n",
      "epoch no.2 train no.177020  loss = 2.42380 avg_loss = 3.39337\n",
      "epoch no.2 train no.177030  loss = 3.76124 avg_loss = 3.37542\n",
      "epoch no.2 train no.177040  loss = 3.78855 avg_loss = 3.33735\n",
      "epoch no.2 train no.177050  loss = 5.99911 avg_loss = 3.42595\n",
      "epoch no.2 train no.177060  loss = 3.81390 avg_loss = 3.39935\n",
      "epoch no.2 train no.177070  loss = 4.20405 avg_loss = 3.40302\n",
      "epoch no.2 train no.177080  loss = 3.11264 avg_loss = 3.36494\n",
      "epoch no.2 train no.177090  loss = 2.09798 avg_loss = 3.40019\n",
      "epoch no.2 train no.177100  loss = 4.18884 avg_loss = 3.41150\n",
      "epoch no.2 train no.177110  loss = 3.52800 avg_loss = 3.46018\n",
      "epoch no.2 train no.177120  loss = 1.41553 avg_loss = 3.45656\n",
      "epoch no.2 train no.177130  loss = 2.60966 avg_loss = 3.44539\n",
      "epoch no.2 train no.177140  loss = 2.41954 avg_loss = 3.43870\n",
      "epoch no.2 train no.177150  loss = 3.62434 avg_loss = 3.46318\n",
      "epoch no.2 train no.177160  loss = 2.88284 avg_loss = 3.40842\n",
      "epoch no.2 train no.177170  loss = 5.22633 avg_loss = 3.43757\n",
      "epoch no.2 train no.177180  loss = 1.72576 avg_loss = 3.39362\n",
      "epoch no.2 train no.177190  loss = 4.40690 avg_loss = 3.39767\n",
      "epoch no.2 train no.177200  loss = 3.20911 avg_loss = 3.43011\n",
      "epoch no.2 train no.177210  loss = 1.98800 avg_loss = 3.37029\n",
      "epoch no.2 train no.177220  loss = 2.06470 avg_loss = 3.35756\n",
      "epoch no.2 train no.177230  loss = 2.21268 avg_loss = 3.37056\n",
      "epoch no.2 train no.177240  loss = 2.71515 avg_loss = 3.36873\n",
      "epoch no.2 train no.177250  loss = 3.37609 avg_loss = 3.37656\n",
      "epoch no.2 train no.177260  loss = 2.54632 avg_loss = 3.41611\n",
      "epoch no.2 train no.177270  loss = 3.34240 avg_loss = 3.37001\n",
      "epoch no.2 train no.177280  loss = 4.07436 avg_loss = 3.40044\n",
      "epoch no.2 train no.177290  loss = 2.01598 avg_loss = 3.37306\n",
      "epoch no.2 train no.177300  loss = 2.26614 avg_loss = 3.36375\n",
      "epoch no.2 train no.177310  loss = 2.62910 avg_loss = 3.35966\n",
      "epoch no.2 train no.177320  loss = 1.91898 avg_loss = 3.35214\n",
      "epoch no.2 train no.177330  loss = 3.88099 avg_loss = 3.39154\n",
      "epoch no.2 train no.177340  loss = 4.24945 avg_loss = 3.47585\n",
      "epoch no.2 train no.177350  loss = 2.37996 avg_loss = 3.43509\n",
      "epoch no.2 train no.177360  loss = 2.77179 avg_loss = 3.44782\n",
      "epoch no.2 train no.177370  loss = 2.80674 avg_loss = 3.47123\n",
      "epoch no.2 train no.177380  loss = 3.09941 avg_loss = 3.43038\n",
      "epoch no.2 train no.177390  loss = 3.51397 avg_loss = 3.46175\n",
      "epoch no.2 train no.177400  loss = 3.58452 avg_loss = 3.45794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.177410  loss = 3.01733 avg_loss = 3.45224\n",
      "epoch no.2 train no.177420  loss = 2.92178 avg_loss = 3.49254\n",
      "epoch no.2 train no.177430  loss = 3.35669 avg_loss = 3.48664\n",
      "epoch no.2 train no.177440  loss = 4.13597 avg_loss = 3.49534\n",
      "epoch no.2 train no.177450  loss = 2.61995 avg_loss = 3.45917\n",
      "epoch no.2 train no.177460  loss = 2.20173 avg_loss = 3.44440\n",
      "epoch no.2 train no.177470  loss = 4.23233 avg_loss = 3.42645\n",
      "epoch no.2 train no.177480  loss = 3.80616 avg_loss = 3.44418\n",
      "epoch no.2 train no.177490  loss = 3.13944 avg_loss = 3.43601\n",
      "epoch no.2 train no.177500  loss = 4.94013 avg_loss = 3.39746\n",
      "epoch no.2 train no.177510  loss = 3.92542 avg_loss = 3.37454\n",
      "epoch no.2 train no.177520  loss = 4.07865 avg_loss = 3.35171\n",
      "epoch no.2 train no.177530  loss = 1.51939 avg_loss = 3.33357\n",
      "epoch no.2 train no.177540  loss = 3.43390 avg_loss = 3.39001\n",
      "epoch no.2 train no.177550  loss = 3.20120 avg_loss = 3.34541\n",
      "epoch no.2 train no.177560  loss = 2.98762 avg_loss = 3.33985\n",
      "epoch no.2 train no.177570  loss = 3.05498 avg_loss = 3.31736\n",
      "epoch no.2 train no.177580  loss = 3.67855 avg_loss = 3.32515\n",
      "epoch no.2 train no.177590  loss = 3.45449 avg_loss = 3.34527\n",
      "epoch no.2 train no.177600  loss = 2.94951 avg_loss = 3.37075\n",
      "epoch no.2 train no.177610  loss = 3.99020 avg_loss = 3.39542\n",
      "epoch no.2 train no.177620  loss = 5.10742 avg_loss = 3.46670\n",
      "epoch no.2 train no.177630  loss = 3.49497 avg_loss = 3.43505\n",
      "epoch no.2 train no.177640  loss = 4.54689 avg_loss = 3.45804\n",
      "epoch no.2 train no.177650  loss = 2.24922 avg_loss = 3.45581\n",
      "epoch no.2 train no.177660  loss = 3.29399 avg_loss = 3.46463\n",
      "epoch no.2 train no.177670  loss = 2.47117 avg_loss = 3.43214\n",
      "epoch no.2 train no.177680  loss = 3.25888 avg_loss = 3.49032\n",
      "epoch no.2 train no.177690  loss = 3.07390 avg_loss = 3.49339\n",
      "epoch no.2 train no.177700  loss = 2.92372 avg_loss = 3.45397\n",
      "epoch no.2 train no.177710  loss = 3.58591 avg_loss = 3.47468\n",
      "epoch no.2 train no.177720  loss = 3.58097 avg_loss = 3.50549\n",
      "epoch no.2 train no.177730  loss = 2.70365 avg_loss = 3.44790\n",
      "epoch no.2 train no.177740  loss = 2.82537 avg_loss = 3.44195\n",
      "epoch no.2 train no.177750  loss = 3.78673 avg_loss = 3.46102\n",
      "epoch no.2 train no.177760  loss = 2.11071 avg_loss = 3.42445\n",
      "epoch no.2 train no.177770  loss = 2.59925 avg_loss = 3.41236\n",
      "epoch no.2 train no.177780  loss = 1.81068 avg_loss = 3.37718\n",
      "epoch no.2 train no.177790  loss = 3.95806 avg_loss = 3.37738\n",
      "epoch no.2 train no.177800  loss = 2.36002 avg_loss = 3.33908\n",
      "epoch no.2 train no.177810  loss = 2.90709 avg_loss = 3.35928\n",
      "epoch no.2 train no.177820  loss = 2.83708 avg_loss = 3.35507\n",
      "epoch no.2 train no.177830  loss = 3.00140 avg_loss = 3.34686\n",
      "epoch no.2 train no.177840  loss = 3.52057 avg_loss = 3.35490\n",
      "epoch no.2 train no.177850  loss = 3.90497 avg_loss = 3.38465\n",
      "epoch no.2 train no.177860  loss = 2.47106 avg_loss = 3.36246\n",
      "epoch no.2 train no.177870  loss = 5.78761 avg_loss = 3.39585\n",
      "epoch no.2 train no.177880  loss = 2.99548 avg_loss = 3.35442\n",
      "epoch no.2 train no.177890  loss = 2.44506 avg_loss = 3.34657\n",
      "epoch no.2 train no.177900  loss = 4.91718 avg_loss = 3.38283\n",
      "epoch no.2 train no.177910  loss = 3.65244 avg_loss = 3.36237\n",
      "epoch no.2 train no.177920  loss = 6.04575 avg_loss = 3.40651\n",
      "epoch no.2 train no.177930  loss = 2.26354 avg_loss = 3.38052\n",
      "epoch no.2 train no.177940  loss = 3.01276 avg_loss = 3.38633\n",
      "epoch no.2 train no.177950  loss = 3.61373 avg_loss = 3.39249\n",
      "epoch no.2 train no.177960  loss = 1.92141 avg_loss = 3.41437\n",
      "epoch no.2 train no.177970  loss = 3.00746 avg_loss = 3.41534\n",
      "epoch no.2 train no.177980  loss = 3.57708 avg_loss = 3.39931\n",
      "epoch no.2 train no.177990  loss = 1.49011 avg_loss = 3.42379\n",
      "epoch no.2 train no.178000  loss = 4.74895 avg_loss = 3.43000\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.178010  loss = 3.53618 avg_loss = 3.47924\n",
      "epoch no.2 train no.178020  loss = 3.20505 avg_loss = 3.42158\n",
      "epoch no.2 train no.178030  loss = 6.55341 avg_loss = 3.50435\n",
      "epoch no.2 train no.178040  loss = 5.02975 avg_loss = 3.50611\n",
      "epoch no.2 train no.178050  loss = 2.30398 avg_loss = 3.49525\n",
      "epoch no.2 train no.178060  loss = 3.39353 avg_loss = 3.46792\n",
      "epoch no.2 train no.178070  loss = 2.81784 avg_loss = 3.46976\n",
      "epoch no.2 train no.178080  loss = 4.75275 avg_loss = 3.45835\n",
      "epoch no.2 train no.178090  loss = 5.64578 avg_loss = 3.42943\n",
      "epoch no.2 train no.178100  loss = 2.97195 avg_loss = 3.42948\n",
      "epoch no.2 train no.178110  loss = 6.86836 avg_loss = 3.43942\n",
      "epoch no.2 train no.178120  loss = 4.00676 avg_loss = 3.43997\n",
      "epoch no.2 train no.178130  loss = 2.21904 avg_loss = 3.45000\n",
      "epoch no.2 train no.178140  loss = 2.17757 avg_loss = 3.44867\n",
      "epoch no.2 train no.178150  loss = 3.36032 avg_loss = 3.44133\n",
      "epoch no.2 train no.178160  loss = 3.93486 avg_loss = 3.49358\n",
      "epoch no.2 train no.178170  loss = 4.27429 avg_loss = 3.49970\n",
      "epoch no.2 train no.178180  loss = 3.37465 avg_loss = 3.49255\n",
      "epoch no.2 train no.178190  loss = 2.85901 avg_loss = 3.42591\n",
      "epoch no.2 train no.178200  loss = 3.83604 avg_loss = 3.42066\n",
      "epoch no.2 train no.178210  loss = 5.21935 avg_loss = 3.43048\n",
      "epoch no.2 train no.178220  loss = 6.59414 avg_loss = 3.43259\n",
      "epoch no.2 train no.178230  loss = 3.32396 avg_loss = 3.46168\n",
      "epoch no.2 train no.178240  loss = 4.21410 avg_loss = 3.46758\n",
      "epoch no.2 train no.178250  loss = 3.08305 avg_loss = 3.44981\n",
      "epoch no.2 train no.178260  loss = 3.46180 avg_loss = 3.43607\n",
      "epoch no.2 train no.178270  loss = 2.99118 avg_loss = 3.39185\n",
      "epoch no.2 train no.178280  loss = 4.10471 avg_loss = 3.44996\n",
      "epoch no.2 train no.178290  loss = 5.61635 avg_loss = 3.47972\n",
      "epoch no.2 train no.178300  loss = 5.98580 avg_loss = 3.48598\n",
      "epoch no.2 train no.178310  loss = 2.17368 avg_loss = 3.44965\n",
      "epoch no.2 train no.178320  loss = 5.56505 avg_loss = 3.47406\n",
      "epoch no.2 train no.178330  loss = 4.44662 avg_loss = 3.51630\n",
      "epoch no.2 train no.178340  loss = 2.99815 avg_loss = 3.45931\n",
      "epoch no.2 train no.178350  loss = 2.90296 avg_loss = 3.41030\n",
      "epoch no.2 train no.178360  loss = 2.44505 avg_loss = 3.40092\n",
      "epoch no.2 train no.178370  loss = 2.83319 avg_loss = 3.42533\n",
      "epoch no.2 train no.178380  loss = 2.83294 avg_loss = 3.43586\n",
      "epoch no.2 train no.178390  loss = 3.77355 avg_loss = 3.41611\n",
      "epoch no.2 train no.178400  loss = 2.51457 avg_loss = 3.39990\n",
      "epoch no.2 train no.178410  loss = 2.89743 avg_loss = 3.35225\n",
      "epoch no.2 train no.178420  loss = 4.52599 avg_loss = 3.39817\n",
      "epoch no.2 train no.178430  loss = 2.56604 avg_loss = 3.32542\n",
      "epoch no.2 train no.178440  loss = 4.59501 avg_loss = 3.30485\n",
      "epoch no.2 train no.178450  loss = 5.33729 avg_loss = 3.36148\n",
      "epoch no.2 train no.178460  loss = 1.76406 avg_loss = 3.31606\n",
      "epoch no.2 train no.178470  loss = 4.05685 avg_loss = 3.30614\n",
      "epoch no.2 train no.178480  loss = 1.94882 avg_loss = 3.33733\n",
      "epoch no.2 train no.178490  loss = 3.16353 avg_loss = 3.38130\n",
      "epoch no.2 train no.178500  loss = 3.64789 avg_loss = 3.42246\n",
      "epoch no.2 train no.178510  loss = 4.46898 avg_loss = 3.44046\n",
      "epoch no.2 train no.178520  loss = 2.56058 avg_loss = 3.42167\n",
      "epoch no.2 train no.178530  loss = 2.87570 avg_loss = 3.38966\n",
      "epoch no.2 train no.178540  loss = 4.38836 avg_loss = 3.42592\n",
      "epoch no.2 train no.178550  loss = 2.94846 avg_loss = 3.44411\n",
      "epoch no.2 train no.178560  loss = 3.34746 avg_loss = 3.49118\n",
      "epoch no.2 train no.178570  loss = 3.08331 avg_loss = 3.49060\n",
      "epoch no.2 train no.178580  loss = 3.02225 avg_loss = 3.47536\n",
      "epoch no.2 train no.178590  loss = 3.39215 avg_loss = 3.46984\n",
      "epoch no.2 train no.178600  loss = 3.27706 avg_loss = 3.46194\n",
      "epoch no.2 train no.178610  loss = 2.12567 avg_loss = 3.40365\n",
      "epoch no.2 train no.178620  loss = 2.93008 avg_loss = 3.37113\n",
      "epoch no.2 train no.178630  loss = 3.76657 avg_loss = 3.39884\n",
      "epoch no.2 train no.178640  loss = 2.62806 avg_loss = 3.43173\n",
      "epoch no.2 train no.178650  loss = 2.97513 avg_loss = 3.42567\n",
      "epoch no.2 train no.178660  loss = 3.24558 avg_loss = 3.42358\n",
      "epoch no.2 train no.178670  loss = 2.52427 avg_loss = 3.44293\n",
      "epoch no.2 train no.178680  loss = 4.04259 avg_loss = 3.51321\n",
      "epoch no.2 train no.178690  loss = 5.24485 avg_loss = 3.54426\n",
      "epoch no.2 train no.178700  loss = 3.73315 avg_loss = 3.51033\n",
      "epoch no.2 train no.178710  loss = 1.76904 avg_loss = 3.49982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.178720  loss = 4.36649 avg_loss = 3.50866\n",
      "epoch no.2 train no.178730  loss = 4.19591 avg_loss = 3.50148\n",
      "epoch no.2 train no.178740  loss = 3.44263 avg_loss = 3.55357\n",
      "epoch no.2 train no.178750  loss = 2.26660 avg_loss = 3.52058\n",
      "epoch no.2 train no.178760  loss = 5.17181 avg_loss = 3.56811\n",
      "epoch no.2 train no.178770  loss = 2.18414 avg_loss = 3.56320\n",
      "epoch no.2 train no.178780  loss = 2.48143 avg_loss = 3.54376\n",
      "epoch no.2 train no.178790  loss = 4.10036 avg_loss = 3.52603\n",
      "epoch no.2 train no.178800  loss = 3.26381 avg_loss = 3.54910\n",
      "epoch no.2 train no.178810  loss = 2.04535 avg_loss = 3.51893\n",
      "epoch no.2 train no.178820  loss = 3.31157 avg_loss = 3.51753\n",
      "epoch no.2 train no.178830  loss = 5.17525 avg_loss = 3.60527\n",
      "epoch no.2 train no.178840  loss = 4.30838 avg_loss = 3.55121\n",
      "epoch no.2 train no.178850  loss = 2.05233 avg_loss = 3.52566\n",
      "epoch no.2 train no.178860  loss = 2.88410 avg_loss = 3.58731\n",
      "epoch no.2 train no.178870  loss = 1.78452 avg_loss = 3.57288\n",
      "epoch no.2 train no.178880  loss = 3.60665 avg_loss = 3.59381\n",
      "epoch no.2 train no.178890  loss = 5.22852 avg_loss = 3.57054\n",
      "epoch no.2 train no.178900  loss = 2.65786 avg_loss = 3.52855\n",
      "epoch no.2 train no.178910  loss = 3.23373 avg_loss = 3.49070\n",
      "epoch no.2 train no.178920  loss = 6.24435 avg_loss = 3.49789\n",
      "epoch no.2 train no.178930  loss = 2.73858 avg_loss = 3.49960\n",
      "epoch no.2 train no.178940  loss = 2.38605 avg_loss = 3.45040\n",
      "epoch no.2 train no.178950  loss = 2.82111 avg_loss = 3.42170\n",
      "epoch no.2 train no.178960  loss = 2.10666 avg_loss = 3.41383\n",
      "epoch no.2 train no.178970  loss = 2.41511 avg_loss = 3.45290\n",
      "epoch no.2 train no.178980  loss = 3.07563 avg_loss = 3.48642\n",
      "epoch no.2 train no.178990  loss = 3.57578 avg_loss = 3.45404\n",
      "epoch no.2 train no.179000  loss = 4.77609 avg_loss = 3.46966\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '을', '를', '▁듣기', '▁노래', '</s>']\n",
      "여름밤 드라이브 하면서 듣는 노래</s>\n",
      "epoch no.2 train no.179010  loss = 2.34065 avg_loss = 3.42166\n",
      "epoch no.2 train no.179020  loss = 4.98759 avg_loss = 3.41031\n",
      "epoch no.2 train no.179030  loss = 3.25154 avg_loss = 3.46993\n",
      "epoch no.2 train no.179040  loss = 1.99327 avg_loss = 3.49432\n",
      "epoch no.2 train no.179050  loss = 3.93845 avg_loss = 3.51964\n",
      "epoch no.2 train no.179060  loss = 4.01384 avg_loss = 3.52075\n",
      "epoch no.2 train no.179070  loss = 2.43378 avg_loss = 3.49504\n",
      "epoch no.2 train no.179080  loss = 2.40737 avg_loss = 3.45659\n",
      "epoch no.2 train no.179090  loss = 5.03754 avg_loss = 3.45646\n",
      "epoch no.2 train no.179100  loss = 3.11355 avg_loss = 3.46447\n",
      "epoch no.2 train no.179110  loss = 2.41230 avg_loss = 3.44566\n",
      "epoch no.2 train no.179120  loss = 2.23843 avg_loss = 3.41179\n",
      "epoch no.2 train no.179130  loss = 2.18039 avg_loss = 3.35280\n",
      "epoch no.2 train no.179140  loss = 3.05480 avg_loss = 3.33289\n",
      "epoch no.2 train no.179150  loss = 4.38475 avg_loss = 3.31598\n",
      "epoch no.2 train no.179160  loss = 5.10864 avg_loss = 3.37727\n",
      "epoch no.2 train no.179170  loss = 3.33404 avg_loss = 3.33791\n",
      "epoch no.2 train no.179180  loss = 2.43038 avg_loss = 3.32272\n",
      "epoch no.2 train no.179190  loss = 4.19521 avg_loss = 3.32069\n",
      "epoch no.2 train no.179200  loss = 3.30851 avg_loss = 3.30838\n",
      "epoch no.2 train no.179210  loss = 2.90218 avg_loss = 3.29640\n",
      "epoch no.2 train no.179220  loss = 4.20397 avg_loss = 3.31042\n",
      "epoch no.2 train no.179230  loss = 3.93420 avg_loss = 3.33529\n",
      "epoch no.2 train no.179240  loss = 4.54069 avg_loss = 3.32888\n",
      "epoch no.2 train no.179250  loss = 5.56477 avg_loss = 3.36275\n",
      "epoch no.2 train no.179260  loss = 2.17759 avg_loss = 3.39670\n",
      "epoch no.2 train no.179270  loss = 3.12109 avg_loss = 3.47157\n",
      "epoch no.2 train no.179280  loss = 3.20411 avg_loss = 3.47408\n",
      "epoch no.2 train no.179290  loss = 3.08190 avg_loss = 3.41352\n",
      "epoch no.2 train no.179300  loss = 5.93109 avg_loss = 3.41504\n",
      "epoch no.2 train no.179310  loss = 2.80784 avg_loss = 3.40706\n",
      "epoch no.2 train no.179320  loss = 1.49239 avg_loss = 3.39792\n",
      "epoch no.2 train no.179330  loss = 1.93129 avg_loss = 3.38181\n",
      "epoch no.2 train no.179340  loss = 4.74430 avg_loss = 3.39602\n",
      "epoch no.2 train no.179350  loss = 3.63769 avg_loss = 3.39590\n",
      "epoch no.2 train no.179360  loss = 2.56999 avg_loss = 3.41095\n",
      "epoch no.2 train no.179370  loss = 4.55360 avg_loss = 3.42411\n",
      "epoch no.2 train no.179380  loss = 5.09671 avg_loss = 3.43486\n",
      "epoch no.2 train no.179390  loss = 3.96646 avg_loss = 3.44340\n",
      "epoch no.2 train no.179400  loss = 5.97136 avg_loss = 3.42450\n",
      "epoch no.2 train no.179410  loss = 4.11703 avg_loss = 3.41174\n",
      "epoch no.2 train no.179420  loss = 3.06474 avg_loss = 3.46967\n",
      "epoch no.2 train no.179430  loss = 3.87983 avg_loss = 3.43683\n",
      "epoch no.2 train no.179440  loss = 5.64198 avg_loss = 3.47234\n",
      "epoch no.2 train no.179450  loss = 6.08149 avg_loss = 3.52485\n",
      "epoch no.2 train no.179460  loss = 4.78126 avg_loss = 3.54104\n",
      "epoch no.2 train no.179470  loss = 2.96512 avg_loss = 3.54933\n",
      "epoch no.2 train no.179480  loss = 2.13965 avg_loss = 3.52943\n",
      "epoch no.2 train no.179490  loss = 3.87979 avg_loss = 3.56502\n",
      "epoch no.2 train no.179500  loss = 3.48911 avg_loss = 3.54866\n",
      "epoch no.2 train no.179510  loss = 2.68205 avg_loss = 3.55751\n",
      "epoch no.2 train no.179520  loss = 3.97661 avg_loss = 3.56576\n",
      "epoch no.2 train no.179530  loss = 3.00724 avg_loss = 3.57083\n",
      "epoch no.2 train no.179540  loss = 3.13319 avg_loss = 3.57128\n",
      "epoch no.2 train no.179550  loss = 3.23615 avg_loss = 3.56790\n",
      "epoch no.2 train no.179560  loss = 4.89259 avg_loss = 3.55151\n",
      "epoch no.2 train no.179570  loss = 4.04873 avg_loss = 3.53649\n",
      "epoch no.2 train no.179580  loss = 3.78779 avg_loss = 3.51320\n",
      "epoch no.2 train no.179590  loss = 3.08514 avg_loss = 3.53748\n",
      "epoch no.2 train no.179600  loss = 3.87747 avg_loss = 3.52462\n",
      "epoch no.2 train no.179610  loss = 3.21108 avg_loss = 3.54126\n",
      "epoch no.2 train no.179620  loss = 2.74689 avg_loss = 3.49849\n",
      "epoch no.2 train no.179630  loss = 4.24501 avg_loss = 3.49295\n",
      "epoch no.2 train no.179640  loss = 3.45408 avg_loss = 3.52250\n",
      "epoch no.2 train no.179650  loss = 3.72167 avg_loss = 3.49389\n",
      "epoch no.2 train no.179660  loss = 4.71011 avg_loss = 3.52693\n",
      "epoch no.2 train no.179670  loss = 3.67327 avg_loss = 3.51389\n",
      "epoch no.2 train no.179680  loss = 4.00403 avg_loss = 3.49808\n",
      "epoch no.2 train no.179690  loss = 3.03480 avg_loss = 3.49510\n",
      "epoch no.2 train no.179700  loss = 3.83883 avg_loss = 3.43632\n",
      "epoch no.2 train no.179710  loss = 2.12769 avg_loss = 3.42948\n",
      "epoch no.2 train no.179720  loss = 4.01478 avg_loss = 3.51014\n",
      "epoch no.2 train no.179730  loss = 2.22003 avg_loss = 3.45969\n",
      "epoch no.2 train no.179740  loss = 3.01216 avg_loss = 3.47170\n",
      "epoch no.2 train no.179750  loss = 4.10146 avg_loss = 3.48798\n",
      "epoch no.2 train no.179760  loss = 3.00646 avg_loss = 3.48127\n",
      "epoch no.2 train no.179770  loss = 4.37094 avg_loss = 3.49347\n",
      "epoch no.2 train no.179780  loss = 3.21541 avg_loss = 3.44974\n",
      "epoch no.2 train no.179790  loss = 3.19687 avg_loss = 3.44418\n",
      "epoch no.2 train no.179800  loss = 3.08917 avg_loss = 3.50200\n",
      "epoch no.2 train no.179810  loss = 2.83025 avg_loss = 3.48270\n",
      "epoch no.2 train no.179820  loss = 5.41546 avg_loss = 3.50498\n",
      "epoch no.2 train no.179830  loss = 4.07832 avg_loss = 3.48159\n",
      "epoch no.2 train no.179840  loss = 2.81107 avg_loss = 3.48120\n",
      "epoch no.2 train no.179850  loss = 3.66925 avg_loss = 3.48130\n",
      "epoch no.2 train no.179860  loss = 4.22961 avg_loss = 3.45298\n",
      "epoch no.2 train no.179870  loss = 3.94616 avg_loss = 3.42337\n",
      "epoch no.2 train no.179880  loss = 3.38242 avg_loss = 3.45664\n",
      "epoch no.2 train no.179890  loss = 4.90114 avg_loss = 3.50521\n",
      "epoch no.2 train no.179900  loss = 3.92057 avg_loss = 3.51602\n",
      "epoch no.2 train no.179910  loss = 4.75804 avg_loss = 3.54602\n",
      "epoch no.2 train no.179920  loss = 4.46767 avg_loss = 3.61656\n",
      "epoch no.2 train no.179930  loss = 3.54507 avg_loss = 3.61145\n",
      "epoch no.2 train no.179940  loss = 5.18233 avg_loss = 3.65557\n",
      "epoch no.2 train no.179950  loss = 3.44189 avg_loss = 3.62034\n",
      "epoch no.2 train no.179960  loss = 3.10070 avg_loss = 3.64273\n",
      "epoch no.2 train no.179970  loss = 2.61203 avg_loss = 3.62857\n",
      "epoch no.2 train no.179980  loss = 3.33888 avg_loss = 3.59870\n",
      "epoch no.2 train no.179990  loss = 4.23885 avg_loss = 3.53812\n",
      "epoch no.2 train no.180000  loss = 2.98598 avg_loss = 3.57516\n",
      "4\n",
      "to_tokens: ['▁비', '엔', '▁역시', '▁신나는', '▁노래', '</s>']\n",
      "여름엔 역시 신나는 노래</s>\n",
      "epoch no.2 train no.180010  loss = 1.86789 avg_loss = 3.57743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.180020  loss = 2.35209 avg_loss = 3.53866\n",
      "epoch no.2 train no.180030  loss = 2.44801 avg_loss = 3.52787\n",
      "epoch no.2 train no.180040  loss = 2.16855 avg_loss = 3.56846\n",
      "epoch no.2 train no.180050  loss = 3.50510 avg_loss = 3.54374\n",
      "epoch no.2 train no.180060  loss = 4.21855 avg_loss = 3.60640\n",
      "epoch no.2 train no.180070  loss = 5.05919 avg_loss = 3.63804\n",
      "epoch no.2 train no.180080  loss = 1.92089 avg_loss = 3.61192\n",
      "epoch no.2 train no.180090  loss = 2.14456 avg_loss = 3.60611\n",
      "epoch no.2 train no.180100  loss = 4.02766 avg_loss = 3.63177\n",
      "epoch no.2 train no.180110  loss = 4.65478 avg_loss = 3.61270\n",
      "epoch no.2 train no.180120  loss = 3.54171 avg_loss = 3.54772\n",
      "epoch no.2 train no.180130  loss = 3.50717 avg_loss = 3.49440\n",
      "epoch no.2 train no.180140  loss = 3.94443 avg_loss = 3.45187\n",
      "epoch no.2 train no.180150  loss = 3.94605 avg_loss = 3.49491\n",
      "epoch no.2 train no.180160  loss = 4.10961 avg_loss = 3.50082\n",
      "epoch no.2 train no.180170  loss = 4.21952 avg_loss = 3.50295\n",
      "epoch no.2 train no.180180  loss = 2.58846 avg_loss = 3.51050\n",
      "epoch no.2 train no.180190  loss = 3.56297 avg_loss = 3.50630\n",
      "epoch no.2 train no.180200  loss = 3.42993 avg_loss = 3.47743\n",
      "epoch no.2 train no.180210  loss = 2.50561 avg_loss = 3.45625\n",
      "epoch no.2 train no.180220  loss = 3.68000 avg_loss = 3.42599\n",
      "epoch no.2 train no.180230  loss = 2.00878 avg_loss = 3.46760\n",
      "epoch no.2 train no.180240  loss = 2.60380 avg_loss = 3.42841\n",
      "epoch no.2 train no.180250  loss = 3.56969 avg_loss = 3.45744\n",
      "epoch no.2 train no.180260  loss = 5.07764 avg_loss = 3.44406\n",
      "epoch no.2 train no.180270  loss = 4.13273 avg_loss = 3.48632\n",
      "epoch no.2 train no.180280  loss = 3.99028 avg_loss = 3.49088\n",
      "epoch no.2 train no.180290  loss = 3.03056 avg_loss = 3.50665\n",
      "epoch no.2 train no.180300  loss = 1.47314 avg_loss = 3.48145\n",
      "epoch no.2 train no.180310  loss = 4.20624 avg_loss = 3.48421\n",
      "epoch no.2 train no.180320  loss = 4.29611 avg_loss = 3.46730\n",
      "epoch no.2 train no.180330  loss = 4.51766 avg_loss = 3.53652\n",
      "epoch no.2 train no.180340  loss = 7.05184 avg_loss = 3.52460\n",
      "epoch no.2 train no.180350  loss = 2.90095 avg_loss = 3.53005\n",
      "epoch no.2 train no.180360  loss = 3.82757 avg_loss = 3.54448\n",
      "epoch no.2 train no.180370  loss = 4.25734 avg_loss = 3.54288\n",
      "epoch no.2 train no.180380  loss = 3.42551 avg_loss = 3.53702\n",
      "epoch no.2 train no.180390  loss = 2.39704 avg_loss = 3.53028\n",
      "epoch no.2 train no.180400  loss = 3.13708 avg_loss = 3.52155\n",
      "epoch no.2 train no.180410  loss = 3.22379 avg_loss = 3.53115\n",
      "epoch no.2 train no.180420  loss = 3.89442 avg_loss = 3.54060\n",
      "epoch no.2 train no.180430  loss = 4.03486 avg_loss = 3.53718\n",
      "epoch no.2 train no.180440  loss = 3.96852 avg_loss = 3.51517\n",
      "epoch no.2 train no.180450  loss = 3.00600 avg_loss = 3.54579\n",
      "epoch no.2 train no.180460  loss = 4.12498 avg_loss = 3.57999\n",
      "epoch no.2 train no.180470  loss = 5.77900 avg_loss = 3.63714\n",
      "epoch no.2 train no.180480  loss = 2.15933 avg_loss = 3.62215\n",
      "epoch no.2 train no.180490  loss = 5.10635 avg_loss = 3.63296\n",
      "epoch no.2 train no.180500  loss = 4.66433 avg_loss = 3.62019\n",
      "epoch no.2 train no.180510  loss = 3.36365 avg_loss = 3.62082\n",
      "epoch no.2 train no.180520  loss = 2.84869 avg_loss = 3.59891\n",
      "epoch no.2 train no.180530  loss = 3.03197 avg_loss = 3.52541\n",
      "epoch no.2 train no.180540  loss = 2.97697 avg_loss = 3.55343\n",
      "epoch no.2 train no.180550  loss = 2.27282 avg_loss = 3.50793\n",
      "epoch no.2 train no.180560  loss = 2.46981 avg_loss = 3.46290\n",
      "epoch no.2 train no.180570  loss = 4.69061 avg_loss = 3.45870\n",
      "epoch no.2 train no.180580  loss = 3.66511 avg_loss = 3.51755\n",
      "epoch no.2 train no.180590  loss = 2.96727 avg_loss = 3.52527\n",
      "epoch no.2 train no.180600  loss = 3.04421 avg_loss = 3.46284\n",
      "epoch no.2 train no.180610  loss = 5.16581 avg_loss = 3.47236\n",
      "epoch no.2 train no.180620  loss = 3.39044 avg_loss = 3.46161\n",
      "epoch no.2 train no.180630  loss = 4.30145 avg_loss = 3.45939\n",
      "epoch no.2 train no.180640  loss = 3.05773 avg_loss = 3.45191\n",
      "epoch no.2 train no.180650  loss = 2.54099 avg_loss = 3.37989\n",
      "epoch no.2 train no.180660  loss = 1.68919 avg_loss = 3.37937\n",
      "epoch no.2 train no.180670  loss = 5.44687 avg_loss = 3.41947\n",
      "epoch no.2 train no.180680  loss = 3.35073 avg_loss = 3.41370\n",
      "epoch no.2 train no.180690  loss = 3.48286 avg_loss = 3.44808\n",
      "epoch no.2 train no.180700  loss = 6.23061 avg_loss = 3.43127\n",
      "epoch no.2 train no.180710  loss = 4.25866 avg_loss = 3.44673\n",
      "epoch no.2 train no.180720  loss = 4.50448 avg_loss = 3.40184\n",
      "epoch no.2 train no.180730  loss = 2.21896 avg_loss = 3.39831\n",
      "epoch no.2 train no.180740  loss = 2.67773 avg_loss = 3.43119\n",
      "epoch no.2 train no.180750  loss = 3.42345 avg_loss = 3.42701\n",
      "epoch no.2 train no.180760  loss = 4.27061 avg_loss = 3.44555\n",
      "epoch no.2 train no.180770  loss = 2.47079 avg_loss = 3.39650\n",
      "epoch no.2 train no.180780  loss = 3.71010 avg_loss = 3.43869\n",
      "epoch no.2 train no.180790  loss = 2.38824 avg_loss = 3.39003\n",
      "epoch no.2 train no.180800  loss = 2.89977 avg_loss = 3.37814\n",
      "epoch no.2 train no.180810  loss = 4.34418 avg_loss = 3.36971\n",
      "epoch no.2 train no.180820  loss = 4.59651 avg_loss = 3.38434\n",
      "epoch no.2 train no.180830  loss = 4.25712 avg_loss = 3.39536\n",
      "epoch no.2 train no.180840  loss = 2.79174 avg_loss = 3.39602\n",
      "epoch no.2 train no.180850  loss = 2.55045 avg_loss = 3.42270\n",
      "epoch no.2 train no.180860  loss = 3.63018 avg_loss = 3.43934\n",
      "epoch no.2 train no.180870  loss = 2.37313 avg_loss = 3.39059\n",
      "epoch no.2 train no.180880  loss = 3.05433 avg_loss = 3.39666\n",
      "epoch no.2 train no.180890  loss = 3.18357 avg_loss = 3.42825\n",
      "epoch no.2 train no.180900  loss = 2.97231 avg_loss = 3.42445\n",
      "epoch no.2 train no.180910  loss = 3.63634 avg_loss = 3.46823\n",
      "epoch no.2 train no.180920  loss = 3.63997 avg_loss = 3.45268\n",
      "epoch no.2 train no.180930  loss = 4.39669 avg_loss = 3.47585\n",
      "epoch no.2 train no.180940  loss = 2.60932 avg_loss = 3.44266\n",
      "epoch no.2 train no.180950  loss = 2.21546 avg_loss = 3.45055\n",
      "epoch no.2 train no.180960  loss = 1.76584 avg_loss = 3.44258\n",
      "epoch no.2 train no.180970  loss = 2.89776 avg_loss = 3.44085\n",
      "epoch no.2 train no.180980  loss = 4.18851 avg_loss = 3.46507\n",
      "epoch no.2 train no.180990  loss = 1.69586 avg_loss = 3.45837\n",
      "epoch no.2 train no.181000  loss = 3.69695 avg_loss = 3.46724\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁신나는', '▁댄스', '</s>']\n",
      "여름엔 역시 신나는 노래</s>\n",
      "epoch no.2 train no.181010  loss = 3.00685 avg_loss = 3.46701\n",
      "epoch no.2 train no.181020  loss = 4.05736 avg_loss = 3.47494\n",
      "epoch no.2 train no.181030  loss = 3.09338 avg_loss = 3.50258\n",
      "epoch no.2 train no.181040  loss = 4.38694 avg_loss = 3.50843\n",
      "epoch no.2 train no.181050  loss = 4.06277 avg_loss = 3.52900\n",
      "epoch no.2 train no.181060  loss = 4.31435 avg_loss = 3.55115\n",
      "epoch no.2 train no.181070  loss = 2.50146 avg_loss = 3.57872\n",
      "epoch no.2 train no.181080  loss = 3.02781 avg_loss = 3.54751\n",
      "epoch no.2 train no.181090  loss = 3.06195 avg_loss = 3.59169\n",
      "epoch no.2 train no.181100  loss = 4.21935 avg_loss = 3.56586\n",
      "epoch no.2 train no.181110  loss = 4.16762 avg_loss = 3.55747\n",
      "epoch no.2 train no.181120  loss = 3.78647 avg_loss = 3.61082\n",
      "epoch no.2 train no.181130  loss = 3.41510 avg_loss = 3.57382\n",
      "epoch no.2 train no.181140  loss = 2.34268 avg_loss = 3.57862\n",
      "epoch no.2 train no.181150  loss = 1.79808 avg_loss = 3.56310\n",
      "epoch no.2 train no.181160  loss = 2.73473 avg_loss = 3.60706\n",
      "epoch no.2 train no.181170  loss = 4.13709 avg_loss = 3.62024\n",
      "epoch no.2 train no.181180  loss = 3.46277 avg_loss = 3.61227\n",
      "epoch no.2 train no.181190  loss = 2.20001 avg_loss = 3.60918\n",
      "epoch no.2 train no.181200  loss = 4.00910 avg_loss = 3.58227\n",
      "epoch no.2 train no.181210  loss = 3.42837 avg_loss = 3.58631\n",
      "epoch no.2 train no.181220  loss = 3.45359 avg_loss = 3.55012\n",
      "epoch no.2 train no.181230  loss = 1.54771 avg_loss = 3.50885\n",
      "epoch no.2 train no.181240  loss = 4.16542 avg_loss = 3.45645\n",
      "epoch no.2 train no.181250  loss = 4.54774 avg_loss = 3.44974\n",
      "epoch no.2 train no.181260  loss = 3.27818 avg_loss = 3.43976\n",
      "epoch no.2 train no.181270  loss = 3.58196 avg_loss = 3.45896\n",
      "epoch no.2 train no.181280  loss = 2.93587 avg_loss = 3.39913\n",
      "epoch no.2 train no.181290  loss = 1.90258 avg_loss = 3.40383\n",
      "epoch no.2 train no.181300  loss = 4.34020 avg_loss = 3.46560\n",
      "epoch no.2 train no.181310  loss = 4.17731 avg_loss = 3.47956\n",
      "epoch no.2 train no.181320  loss = 4.96903 avg_loss = 3.49078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.181330  loss = 3.15236 avg_loss = 3.53780\n",
      "epoch no.2 train no.181340  loss = 3.33580 avg_loss = 3.54408\n",
      "epoch no.2 train no.181350  loss = 4.58165 avg_loss = 3.52061\n",
      "epoch no.2 train no.181360  loss = 4.55803 avg_loss = 3.51128\n",
      "epoch no.2 train no.181370  loss = 1.97171 avg_loss = 3.48968\n",
      "epoch no.2 train no.181380  loss = 4.54326 avg_loss = 3.53288\n",
      "epoch no.2 train no.181390  loss = 3.15780 avg_loss = 3.52514\n",
      "epoch no.2 train no.181400  loss = 2.53113 avg_loss = 3.53572\n",
      "epoch no.2 train no.181410  loss = 3.71719 avg_loss = 3.49248\n",
      "epoch no.2 train no.181420  loss = 4.22529 avg_loss = 3.52410\n",
      "epoch no.2 train no.181430  loss = 4.47560 avg_loss = 3.53001\n",
      "epoch no.2 train no.181440  loss = 4.20196 avg_loss = 3.55262\n",
      "epoch no.2 train no.181450  loss = 2.80984 avg_loss = 3.58275\n",
      "epoch no.2 train no.181460  loss = 3.73006 avg_loss = 3.58516\n",
      "epoch no.2 train no.181470  loss = 2.49194 avg_loss = 3.54315\n",
      "epoch no.2 train no.181480  loss = 5.80592 avg_loss = 3.54306\n",
      "epoch no.2 train no.181490  loss = 4.08795 avg_loss = 3.48515\n",
      "epoch no.2 train no.181500  loss = 3.88865 avg_loss = 3.50402\n",
      "epoch no.2 train no.181510  loss = 3.19569 avg_loss = 3.45028\n",
      "epoch no.2 train no.181520  loss = 3.85461 avg_loss = 3.42908\n",
      "epoch no.2 train no.181530  loss = 2.53168 avg_loss = 3.45098\n",
      "epoch no.2 train no.181540  loss = 2.77122 avg_loss = 3.44982\n",
      "epoch no.2 train no.181550  loss = 2.17885 avg_loss = 3.42458\n",
      "epoch no.2 train no.181560  loss = 5.26914 avg_loss = 3.41552\n",
      "epoch no.2 train no.181570  loss = 3.16023 avg_loss = 3.38691\n",
      "epoch no.2 train no.181580  loss = 4.69142 avg_loss = 3.37397\n",
      "epoch no.2 train no.181590  loss = 2.98759 avg_loss = 3.35168\n",
      "epoch no.2 train no.181600  loss = 3.68032 avg_loss = 3.41891\n",
      "epoch no.2 train no.181610  loss = 4.49832 avg_loss = 3.36853\n",
      "epoch no.2 train no.181620  loss = 2.75541 avg_loss = 3.36084\n",
      "epoch no.2 train no.181630  loss = 2.99159 avg_loss = 3.39206\n",
      "epoch no.2 train no.181640  loss = 3.67940 avg_loss = 3.34550\n",
      "epoch no.2 train no.181650  loss = 2.43040 avg_loss = 3.34429\n",
      "epoch no.2 train no.181660  loss = 3.32905 avg_loss = 3.37596\n",
      "epoch no.2 train no.181670  loss = 2.18301 avg_loss = 3.35697\n",
      "epoch no.2 train no.181680  loss = 3.10792 avg_loss = 3.35846\n",
      "epoch no.2 train no.181690  loss = 2.03194 avg_loss = 3.34885\n",
      "epoch no.2 train no.181700  loss = 4.72059 avg_loss = 3.37637\n",
      "epoch no.2 train no.181710  loss = 3.28461 avg_loss = 3.39835\n",
      "epoch no.2 train no.181720  loss = 4.03007 avg_loss = 3.47459\n",
      "epoch no.2 train no.181730  loss = 2.70790 avg_loss = 3.52743\n",
      "epoch no.2 train no.181740  loss = 2.34097 avg_loss = 3.51315\n",
      "epoch no.2 train no.181750  loss = 3.17383 avg_loss = 3.52356\n",
      "epoch no.2 train no.181760  loss = 3.60559 avg_loss = 3.49235\n",
      "epoch no.2 train no.181770  loss = 1.98477 avg_loss = 3.47464\n",
      "epoch no.2 train no.181780  loss = 2.15899 avg_loss = 3.46987\n",
      "epoch no.2 train no.181790  loss = 4.28207 avg_loss = 3.49748\n",
      "epoch no.2 train no.181800  loss = 4.95251 avg_loss = 3.55292\n",
      "epoch no.2 train no.181810  loss = 5.02982 avg_loss = 3.52716\n",
      "epoch no.2 train no.181820  loss = 2.89942 avg_loss = 3.50773\n",
      "epoch no.2 train no.181830  loss = 2.77160 avg_loss = 3.48727\n",
      "epoch no.2 train no.181840  loss = 4.49342 avg_loss = 3.49052\n",
      "epoch no.2 train no.181850  loss = 3.90635 avg_loss = 3.50306\n",
      "epoch no.2 train no.181860  loss = 3.74815 avg_loss = 3.50642\n",
      "epoch no.2 train no.181870  loss = 2.87718 avg_loss = 3.52339\n",
      "epoch no.2 train no.181880  loss = 3.14772 avg_loss = 3.48661\n",
      "epoch no.2 train no.181890  loss = 4.75162 avg_loss = 3.55989\n",
      "epoch no.2 train no.181900  loss = 4.86757 avg_loss = 3.58015\n",
      "epoch no.2 train no.181910  loss = 3.74990 avg_loss = 3.57992\n",
      "epoch no.2 train no.181920  loss = 3.05961 avg_loss = 3.57909\n",
      "epoch no.2 train no.181930  loss = 4.11136 avg_loss = 3.57275\n",
      "epoch no.2 train no.181940  loss = 5.07036 avg_loss = 3.60799\n",
      "epoch no.2 train no.181950  loss = 4.51830 avg_loss = 3.64721\n",
      "epoch no.2 train no.181960  loss = 1.83962 avg_loss = 3.58625\n",
      "epoch no.2 train no.181970  loss = 3.14045 avg_loss = 3.58456\n",
      "epoch no.2 train no.181980  loss = 4.96890 avg_loss = 3.58881\n",
      "epoch no.2 train no.181990  loss = 2.83714 avg_loss = 3.58084\n",
      "epoch no.2 train no.182000  loss = 5.05570 avg_loss = 3.57786\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣는', '▁감성', '</s>']\n",
      "여름밤에 듣는 노래</s>\n",
      "epoch no.2 train no.182010  loss = 2.99799 avg_loss = 3.53966\n",
      "epoch no.2 train no.182020  loss = 4.14500 avg_loss = 3.55485\n",
      "epoch no.2 train no.182030  loss = 5.82087 avg_loss = 3.58996\n",
      "epoch no.2 train no.182040  loss = 2.71410 avg_loss = 3.55209\n",
      "epoch no.2 train no.182050  loss = 3.00646 avg_loss = 3.55274\n",
      "epoch no.2 train no.182060  loss = 2.99424 avg_loss = 3.55595\n",
      "epoch no.2 train no.182070  loss = 4.75167 avg_loss = 3.58263\n",
      "epoch no.2 train no.182080  loss = 3.28704 avg_loss = 3.59407\n",
      "epoch no.2 train no.182090  loss = 4.73805 avg_loss = 3.58347\n",
      "epoch no.2 train no.182100  loss = 4.40881 avg_loss = 3.57783\n",
      "epoch no.2 train no.182110  loss = 4.53606 avg_loss = 3.56298\n",
      "epoch no.2 train no.182120  loss = 2.94194 avg_loss = 3.58880\n",
      "epoch no.2 train no.182130  loss = 2.76552 avg_loss = 3.55126\n",
      "epoch no.2 train no.182140  loss = 2.09384 avg_loss = 3.53318\n",
      "epoch no.2 train no.182150  loss = 2.98267 avg_loss = 3.54061\n",
      "epoch no.2 train no.182160  loss = 3.73981 avg_loss = 3.55830\n",
      "epoch no.2 train no.182170  loss = 3.08816 avg_loss = 3.56519\n",
      "epoch no.2 train no.182180  loss = 3.24956 avg_loss = 3.61368\n",
      "epoch no.2 train no.182190  loss = 3.15662 avg_loss = 3.61685\n",
      "epoch no.2 train no.182200  loss = 2.81926 avg_loss = 3.60494\n",
      "epoch no.2 train no.182210  loss = 3.85570 avg_loss = 3.60000\n",
      "epoch no.2 train no.182220  loss = 2.63486 avg_loss = 3.64269\n",
      "epoch no.2 train no.182230  loss = 2.88654 avg_loss = 3.61892\n",
      "epoch no.2 train no.182240  loss = 2.64328 avg_loss = 3.59692\n",
      "epoch no.2 train no.182250  loss = 2.97345 avg_loss = 3.58999\n",
      "epoch no.2 train no.182260  loss = 2.99364 avg_loss = 3.58250\n",
      "epoch no.2 train no.182270  loss = 2.81326 avg_loss = 3.55424\n",
      "epoch no.2 train no.182280  loss = 3.27203 avg_loss = 3.52999\n",
      "epoch no.2 train no.182290  loss = 3.76074 avg_loss = 3.52875\n",
      "epoch no.2 train no.182300  loss = 5.15388 avg_loss = 3.51726\n",
      "epoch no.2 train no.182310  loss = 4.32125 avg_loss = 3.53493\n",
      "epoch no.2 train no.182320  loss = 4.39215 avg_loss = 3.51356\n",
      "epoch no.2 train no.182330  loss = 2.62493 avg_loss = 3.49155\n",
      "epoch no.2 train no.182340  loss = 4.75739 avg_loss = 3.54953\n",
      "epoch no.2 train no.182350  loss = 5.48688 avg_loss = 3.56512\n",
      "epoch no.2 train no.182360  loss = 3.65517 avg_loss = 3.57855\n",
      "epoch no.2 train no.182370  loss = 2.97093 avg_loss = 3.54537\n",
      "epoch no.2 train no.182380  loss = 2.86543 avg_loss = 3.52817\n",
      "epoch no.2 train no.182390  loss = 4.28033 avg_loss = 3.46811\n",
      "epoch no.2 train no.182400  loss = 2.88338 avg_loss = 3.48978\n",
      "epoch no.2 train no.182410  loss = 4.84202 avg_loss = 3.50915\n",
      "epoch no.2 train no.182420  loss = 4.59981 avg_loss = 3.50925\n",
      "epoch no.2 train no.182430  loss = 2.87535 avg_loss = 3.52823\n",
      "epoch no.2 train no.182440  loss = 4.34472 avg_loss = 3.55944\n",
      "epoch no.2 train no.182450  loss = 4.78240 avg_loss = 3.53062\n",
      "epoch no.2 train no.182460  loss = 4.42379 avg_loss = 3.55053\n",
      "epoch no.2 train no.182470  loss = 0.84386 avg_loss = 3.51207\n",
      "epoch no.2 train no.182480  loss = 2.34975 avg_loss = 3.50777\n",
      "epoch no.2 train no.182490  loss = 3.72117 avg_loss = 3.53601\n",
      "epoch no.2 train no.182500  loss = 2.86922 avg_loss = 3.51084\n",
      "epoch no.2 train no.182510  loss = 3.81816 avg_loss = 3.49225\n",
      "epoch no.2 train no.182520  loss = 5.00033 avg_loss = 3.53471\n",
      "epoch no.2 train no.182530  loss = 4.84538 avg_loss = 3.54551\n",
      "epoch no.2 train no.182540  loss = 1.87867 avg_loss = 3.54528\n",
      "epoch no.2 train no.182550  loss = 3.38039 avg_loss = 3.53393\n",
      "epoch no.2 train no.182560  loss = 4.82828 avg_loss = 3.61526\n",
      "epoch no.2 train no.182570  loss = 2.28816 avg_loss = 3.56545\n",
      "epoch no.2 train no.182580  loss = 3.29590 avg_loss = 3.52456\n",
      "epoch no.2 train no.182590  loss = 5.19867 avg_loss = 3.54223\n",
      "epoch no.2 train no.182600  loss = 3.71397 avg_loss = 3.55886\n",
      "epoch no.2 train no.182610  loss = 2.76605 avg_loss = 3.54767\n",
      "epoch no.2 train no.182620  loss = 3.65931 avg_loss = 3.57133\n",
      "epoch no.2 train no.182630  loss = 3.08927 avg_loss = 3.53722\n",
      "epoch no.2 train no.182640  loss = 3.56461 avg_loss = 3.49760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.182650  loss = 3.91687 avg_loss = 3.49821\n",
      "epoch no.2 train no.182660  loss = 1.94907 avg_loss = 3.45306\n",
      "epoch no.2 train no.182670  loss = 5.10775 avg_loss = 3.48769\n",
      "epoch no.2 train no.182680  loss = 2.73653 avg_loss = 3.48358\n",
      "epoch no.2 train no.182690  loss = 3.91430 avg_loss = 3.48458\n",
      "epoch no.2 train no.182700  loss = 4.24215 avg_loss = 3.48799\n",
      "epoch no.2 train no.182710  loss = 2.59627 avg_loss = 3.47084\n",
      "epoch no.2 train no.182720  loss = 3.37872 avg_loss = 3.48805\n",
      "epoch no.2 train no.182730  loss = 3.12652 avg_loss = 3.48414\n",
      "epoch no.2 train no.182740  loss = 2.85477 avg_loss = 3.49752\n",
      "epoch no.2 train no.182750  loss = 3.31351 avg_loss = 3.46429\n",
      "epoch no.2 train no.182760  loss = 2.40305 avg_loss = 3.41836\n",
      "epoch no.2 train no.182770  loss = 5.60727 avg_loss = 3.42736\n",
      "epoch no.2 train no.182780  loss = 2.76223 avg_loss = 3.42185\n",
      "epoch no.2 train no.182790  loss = 3.14835 avg_loss = 3.45586\n",
      "epoch no.2 train no.182800  loss = 2.61669 avg_loss = 3.47215\n",
      "epoch no.2 train no.182810  loss = 3.44370 avg_loss = 3.44613\n",
      "epoch no.2 train no.182820  loss = 2.98003 avg_loss = 3.40483\n",
      "epoch no.2 train no.182830  loss = 3.93327 avg_loss = 3.42020\n",
      "epoch no.2 train no.182840  loss = 3.67950 avg_loss = 3.42109\n",
      "epoch no.2 train no.182850  loss = 3.80531 avg_loss = 3.49660\n",
      "epoch no.2 train no.182860  loss = 3.80578 avg_loss = 3.53852\n",
      "epoch no.2 train no.182870  loss = 4.19617 avg_loss = 3.55769\n",
      "epoch no.2 train no.182880  loss = 3.25098 avg_loss = 3.52770\n",
      "epoch no.2 train no.182890  loss = 3.63578 avg_loss = 3.53973\n",
      "epoch no.2 train no.182900  loss = 2.85810 avg_loss = 3.53806\n",
      "epoch no.2 train no.182910  loss = 3.71904 avg_loss = 3.52767\n",
      "epoch no.2 train no.182920  loss = 2.43720 avg_loss = 3.52670\n",
      "epoch no.2 train no.182930  loss = 1.90876 avg_loss = 3.51549\n",
      "epoch no.2 train no.182940  loss = 2.92532 avg_loss = 3.49330\n",
      "epoch no.2 train no.182950  loss = 2.94959 avg_loss = 3.49077\n",
      "epoch no.2 train no.182960  loss = 3.76886 avg_loss = 3.48924\n",
      "epoch no.2 train no.182970  loss = 2.33653 avg_loss = 3.51455\n",
      "epoch no.2 train no.182980  loss = 1.97907 avg_loss = 3.45807\n",
      "epoch no.2 train no.182990  loss = 3.54003 avg_loss = 3.44879\n",
      "epoch no.2 train no.183000  loss = 3.68654 avg_loss = 3.49004\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '들', '</s>']\n",
      "여름밤에 어울리는 곡들</s>\n",
      "epoch no.2 train no.183010  loss = 2.29466 avg_loss = 3.49891\n",
      "epoch no.2 train no.183020  loss = 3.02653 avg_loss = 3.49866\n",
      "epoch no.2 train no.183030  loss = 4.34208 avg_loss = 3.45380\n",
      "epoch no.2 train no.183040  loss = 3.99695 avg_loss = 3.44331\n",
      "epoch no.2 train no.183050  loss = 4.05635 avg_loss = 3.50024\n",
      "epoch no.2 train no.183060  loss = 2.16680 avg_loss = 3.43585\n",
      "epoch no.2 train no.183070  loss = 3.99724 avg_loss = 3.46010\n",
      "epoch no.2 train no.183080  loss = 2.14230 avg_loss = 3.51207\n",
      "epoch no.2 train no.183090  loss = 3.46510 avg_loss = 3.47926\n",
      "epoch no.2 train no.183100  loss = 5.27560 avg_loss = 3.46289\n",
      "epoch no.2 train no.183110  loss = 3.57914 avg_loss = 3.47046\n",
      "epoch no.2 train no.183120  loss = 3.98521 avg_loss = 3.43449\n",
      "epoch no.2 train no.183130  loss = 3.44425 avg_loss = 3.43744\n",
      "epoch no.2 train no.183140  loss = 4.74636 avg_loss = 3.43705\n",
      "epoch no.2 train no.183150  loss = 5.70304 avg_loss = 3.43009\n",
      "epoch no.2 train no.183160  loss = 2.42388 avg_loss = 3.40901\n",
      "epoch no.2 train no.183170  loss = 2.00809 avg_loss = 3.36128\n",
      "epoch no.2 train no.183180  loss = 3.15278 avg_loss = 3.36629\n",
      "epoch no.2 train no.183190  loss = 3.14143 avg_loss = 3.35932\n",
      "epoch no.2 train no.183200  loss = 2.19527 avg_loss = 3.36989\n",
      "epoch no.2 train no.183210  loss = 4.03148 avg_loss = 3.40107\n",
      "epoch no.2 train no.183220  loss = 3.22359 avg_loss = 3.40313\n",
      "epoch no.2 train no.183230  loss = 3.35839 avg_loss = 3.40406\n",
      "epoch no.2 train no.183240  loss = 2.86963 avg_loss = 3.38786\n",
      "epoch no.2 train no.183250  loss = 3.54215 avg_loss = 3.41176\n",
      "epoch no.2 train no.183260  loss = 3.94729 avg_loss = 3.43253\n",
      "epoch no.2 train no.183270  loss = 3.36758 avg_loss = 3.40477\n",
      "epoch no.2 train no.183280  loss = 5.10624 avg_loss = 3.42463\n",
      "epoch no.2 train no.183290  loss = 2.98372 avg_loss = 3.40234\n",
      "epoch no.2 train no.183300  loss = 2.03451 avg_loss = 3.35845\n",
      "epoch no.2 train no.183310  loss = 4.47019 avg_loss = 3.35828\n",
      "epoch no.2 train no.183320  loss = 2.41119 avg_loss = 3.37720\n",
      "epoch no.2 train no.183330  loss = 2.63988 avg_loss = 3.38604\n",
      "epoch no.2 train no.183340  loss = 3.00202 avg_loss = 3.38946\n",
      "epoch no.2 train no.183350  loss = 3.83921 avg_loss = 3.42532\n",
      "epoch no.2 train no.183360  loss = 3.78029 avg_loss = 3.41860\n",
      "epoch no.2 train no.183370  loss = 3.84338 avg_loss = 3.40410\n",
      "epoch no.2 train no.183380  loss = 4.93157 avg_loss = 3.42677\n",
      "epoch no.2 train no.183390  loss = 2.76355 avg_loss = 3.44767\n",
      "epoch no.2 train no.183400  loss = 2.23428 avg_loss = 3.41686\n",
      "epoch no.2 train no.183410  loss = 2.74456 avg_loss = 3.39913\n",
      "epoch no.2 train no.183420  loss = 4.07869 avg_loss = 3.41136\n",
      "epoch no.2 train no.183430  loss = 4.61620 avg_loss = 3.40408\n",
      "epoch no.2 train no.183440  loss = 2.80663 avg_loss = 3.40771\n",
      "epoch no.2 train no.183450  loss = 2.80807 avg_loss = 3.35312\n",
      "epoch no.2 train no.183460  loss = 2.01158 avg_loss = 3.37370\n",
      "epoch no.2 train no.183470  loss = 2.46748 avg_loss = 3.35487\n",
      "epoch no.2 train no.183480  loss = 4.62219 avg_loss = 3.35003\n",
      "epoch no.2 train no.183490  loss = 4.74293 avg_loss = 3.35631\n",
      "epoch no.2 train no.183500  loss = 3.76718 avg_loss = 3.33802\n",
      "epoch no.2 train no.183510  loss = 4.31403 avg_loss = 3.35749\n",
      "epoch no.2 train no.183520  loss = 3.19422 avg_loss = 3.38486\n",
      "epoch no.2 train no.183530  loss = 3.26156 avg_loss = 3.40388\n",
      "epoch no.2 train no.183540  loss = 2.38825 avg_loss = 3.35412\n",
      "epoch no.2 train no.183550  loss = 4.75240 avg_loss = 3.48612\n",
      "epoch no.2 train no.183560  loss = 2.47469 avg_loss = 3.49508\n",
      "epoch no.2 train no.183570  loss = 3.96902 avg_loss = 3.44101\n",
      "epoch no.2 train no.183580  loss = 4.03429 avg_loss = 3.42002\n",
      "epoch no.2 train no.183590  loss = 5.17581 avg_loss = 3.41716\n",
      "epoch no.2 train no.183600  loss = 3.56602 avg_loss = 3.39760\n",
      "epoch no.2 train no.183610  loss = 3.00941 avg_loss = 3.41267\n",
      "epoch no.2 train no.183620  loss = 3.69490 avg_loss = 3.44176\n",
      "epoch no.2 train no.183630  loss = 4.62575 avg_loss = 3.46312\n",
      "epoch no.2 train no.183640  loss = 4.38013 avg_loss = 3.45522\n",
      "epoch no.2 train no.183650  loss = 1.63992 avg_loss = 3.45197\n",
      "epoch no.2 train no.183660  loss = 3.32192 avg_loss = 3.45625\n",
      "epoch no.2 train no.183670  loss = 2.61397 avg_loss = 3.45028\n",
      "epoch no.2 train no.183680  loss = 2.44729 avg_loss = 3.42740\n",
      "epoch no.2 train no.183690  loss = 3.32504 avg_loss = 3.43179\n",
      "epoch no.2 train no.183700  loss = 4.34995 avg_loss = 3.42589\n",
      "epoch no.2 train no.183710  loss = 3.41685 avg_loss = 3.42329\n",
      "epoch no.2 train no.183720  loss = 7.74435 avg_loss = 3.45512\n",
      "epoch no.2 train no.183730  loss = 2.32695 avg_loss = 3.48954\n",
      "epoch no.2 train no.183740  loss = 2.75749 avg_loss = 3.49935\n",
      "epoch no.2 train no.183750  loss = 3.46573 avg_loss = 3.50127\n",
      "epoch no.2 train no.183760  loss = 4.34318 avg_loss = 3.52950\n",
      "epoch no.2 train no.183770  loss = 3.61524 avg_loss = 3.51200\n",
      "epoch no.2 train no.183780  loss = 2.67042 avg_loss = 3.48349\n",
      "epoch no.2 train no.183790  loss = 4.60530 avg_loss = 3.50644\n",
      "epoch no.2 train no.183800  loss = 3.77209 avg_loss = 3.50163\n",
      "epoch no.2 train no.183810  loss = 2.42898 avg_loss = 3.46394\n",
      "epoch no.2 train no.183820  loss = 1.89781 avg_loss = 3.47285\n",
      "epoch no.2 train no.183830  loss = 3.58575 avg_loss = 3.46424\n",
      "epoch no.2 train no.183840  loss = 2.41020 avg_loss = 3.43680\n",
      "epoch no.2 train no.183850  loss = 2.79847 avg_loss = 3.38543\n",
      "epoch no.2 train no.183860  loss = 4.95480 avg_loss = 3.43044\n",
      "epoch no.2 train no.183870  loss = 2.35036 avg_loss = 3.41194\n",
      "epoch no.2 train no.183880  loss = 3.92838 avg_loss = 3.43845\n",
      "epoch no.2 train no.183890  loss = 2.19291 avg_loss = 3.43065\n",
      "epoch no.2 train no.183900  loss = 2.49642 avg_loss = 3.42039\n",
      "epoch no.2 train no.183910  loss = 4.29857 avg_loss = 3.45453\n",
      "epoch no.2 train no.183920  loss = 4.77511 avg_loss = 3.49983\n",
      "epoch no.2 train no.183930  loss = 4.41704 avg_loss = 3.49089\n",
      "epoch no.2 train no.183940  loss = 4.34238 avg_loss = 3.55069\n",
      "epoch no.2 train no.183950  loss = 1.94386 avg_loss = 3.48978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.183960  loss = 4.32682 avg_loss = 3.48543\n",
      "epoch no.2 train no.183970  loss = 5.06135 avg_loss = 3.52501\n",
      "epoch no.2 train no.183980  loss = 4.50485 avg_loss = 3.59148\n",
      "epoch no.2 train no.183990  loss = 2.95167 avg_loss = 3.53294\n",
      "epoch no.2 train no.184000  loss = 2.26925 avg_loss = 3.52799\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '한', '▁노래', '</s>']\n",
      "여름밤에 듣는 잔잔한 음악</s>\n",
      "epoch no.2 train no.184010  loss = 4.14539 avg_loss = 3.56734\n",
      "epoch no.2 train no.184020  loss = 4.76718 avg_loss = 3.57438\n",
      "epoch no.2 train no.184030  loss = 3.56082 avg_loss = 3.53705\n",
      "epoch no.2 train no.184040  loss = 1.71724 avg_loss = 3.52031\n",
      "epoch no.2 train no.184050  loss = 2.68525 avg_loss = 3.52745\n",
      "epoch no.2 train no.184060  loss = 3.19235 avg_loss = 3.47481\n",
      "epoch no.2 train no.184070  loss = 2.93640 avg_loss = 3.45988\n",
      "epoch no.2 train no.184080  loss = 3.66918 avg_loss = 3.48215\n",
      "epoch no.2 train no.184090  loss = 2.56884 avg_loss = 3.45065\n",
      "epoch no.2 train no.184100  loss = 4.73102 avg_loss = 3.44013\n",
      "epoch no.2 train no.184110  loss = 2.45441 avg_loss = 3.42367\n",
      "epoch no.2 train no.184120  loss = 3.22729 avg_loss = 3.47299\n",
      "epoch no.2 train no.184130  loss = 2.80454 avg_loss = 3.49058\n",
      "epoch no.2 train no.184140  loss = 2.00628 avg_loss = 3.47265\n",
      "epoch no.2 train no.184150  loss = 2.53269 avg_loss = 3.41841\n",
      "epoch no.2 train no.184160  loss = 5.01208 avg_loss = 3.46841\n",
      "epoch no.2 train no.184170  loss = 4.01559 avg_loss = 3.45969\n",
      "epoch no.2 train no.184180  loss = 2.55815 avg_loss = 3.44387\n",
      "epoch no.2 train no.184190  loss = 4.26501 avg_loss = 3.44925\n",
      "epoch no.2 train no.184200  loss = 5.00290 avg_loss = 3.44932\n",
      "epoch no.2 train no.184210  loss = 3.24514 avg_loss = 3.44559\n",
      "epoch no.2 train no.184220  loss = 3.73485 avg_loss = 3.45075\n",
      "epoch no.2 train no.184230  loss = 2.66685 avg_loss = 3.43962\n",
      "epoch no.2 train no.184240  loss = 3.86583 avg_loss = 3.47276\n",
      "epoch no.2 train no.184250  loss = 2.77792 avg_loss = 3.50087\n",
      "epoch no.2 train no.184260  loss = 2.66942 avg_loss = 3.48033\n",
      "epoch no.2 train no.184270  loss = 3.09163 avg_loss = 3.54030\n",
      "epoch no.2 train no.184280  loss = 3.38249 avg_loss = 3.57663\n",
      "epoch no.2 train no.184290  loss = 4.06169 avg_loss = 3.56388\n",
      "epoch no.2 train no.184300  loss = 2.58534 avg_loss = 3.56447\n",
      "epoch no.2 train no.184310  loss = 3.78358 avg_loss = 3.49417\n",
      "epoch no.2 train no.184320  loss = 4.46368 avg_loss = 3.50032\n",
      "epoch no.2 train no.184330  loss = 3.34124 avg_loss = 3.51463\n",
      "epoch no.2 train no.184340  loss = 4.00332 avg_loss = 3.55007\n",
      "epoch no.2 train no.184350  loss = 3.50084 avg_loss = 3.56712\n",
      "epoch no.2 train no.184360  loss = 2.99330 avg_loss = 3.50685\n",
      "epoch no.2 train no.184370  loss = 2.79424 avg_loss = 3.48950\n",
      "epoch no.2 train no.184380  loss = 2.44382 avg_loss = 3.43753\n",
      "epoch no.2 train no.184390  loss = 4.78924 avg_loss = 3.51643\n",
      "epoch no.2 train no.184400  loss = 2.17138 avg_loss = 3.52115\n",
      "epoch no.2 train no.184410  loss = 3.61744 avg_loss = 3.54983\n",
      "epoch no.2 train no.184420  loss = 4.13351 avg_loss = 3.54772\n",
      "epoch no.2 train no.184430  loss = 4.98402 avg_loss = 3.50444\n",
      "epoch no.2 train no.184440  loss = 3.27640 avg_loss = 3.48588\n",
      "epoch no.2 train no.184450  loss = 3.53013 avg_loss = 3.47976\n",
      "epoch no.2 train no.184460  loss = 2.04498 avg_loss = 3.45514\n",
      "epoch no.2 train no.184470  loss = 2.70098 avg_loss = 3.45208\n",
      "epoch no.2 train no.184480  loss = 4.75142 avg_loss = 3.46413\n",
      "epoch no.2 train no.184490  loss = 3.56262 avg_loss = 3.45401\n",
      "epoch no.2 train no.184500  loss = 3.71513 avg_loss = 3.47619\n",
      "epoch no.2 train no.184510  loss = 4.18943 avg_loss = 3.52098\n",
      "epoch no.2 train no.184520  loss = 3.84773 avg_loss = 3.50533\n",
      "epoch no.2 train no.184530  loss = 3.18117 avg_loss = 3.50564\n",
      "epoch no.2 train no.184540  loss = 3.48131 avg_loss = 3.55852\n",
      "epoch no.2 train no.184550  loss = 3.26379 avg_loss = 3.57590\n",
      "epoch no.2 train no.184560  loss = 2.09292 avg_loss = 3.52243\n",
      "epoch no.2 train no.184570  loss = 3.75910 avg_loss = 3.49067\n",
      "epoch no.2 train no.184580  loss = 2.63855 avg_loss = 3.51397\n",
      "epoch no.2 train no.184590  loss = 3.91960 avg_loss = 3.49472\n",
      "epoch no.2 train no.184600  loss = 3.14095 avg_loss = 3.51252\n",
      "epoch no.2 train no.184610  loss = 1.33381 avg_loss = 3.52990\n",
      "epoch no.2 train no.184620  loss = 2.69421 avg_loss = 3.56551\n",
      "epoch no.2 train no.184630  loss = 2.34770 avg_loss = 3.54172\n",
      "epoch no.2 train no.184640  loss = 6.13191 avg_loss = 3.60028\n",
      "epoch no.2 train no.184650  loss = 3.61072 avg_loss = 3.58154\n",
      "epoch no.2 train no.184660  loss = 3.00623 avg_loss = 3.56659\n",
      "epoch no.2 train no.184670  loss = 5.87842 avg_loss = 3.56566\n",
      "epoch no.2 train no.184680  loss = 4.30658 avg_loss = 3.52498\n",
      "epoch no.2 train no.184690  loss = 4.12729 avg_loss = 3.52369\n",
      "epoch no.2 train no.184700  loss = 3.55219 avg_loss = 3.53036\n",
      "epoch no.2 train no.184710  loss = 1.88885 avg_loss = 3.49957\n",
      "epoch no.2 train no.184720  loss = 3.61892 avg_loss = 3.50093\n",
      "epoch no.2 train no.184730  loss = 3.61102 avg_loss = 3.50374\n",
      "epoch no.2 train no.184740  loss = 4.14314 avg_loss = 3.51532\n",
      "epoch no.2 train no.184750  loss = 3.17374 avg_loss = 3.52435\n",
      "epoch no.2 train no.184760  loss = 4.17794 avg_loss = 3.50666\n",
      "epoch no.2 train no.184770  loss = 4.24978 avg_loss = 3.49794\n",
      "epoch no.2 train no.184780  loss = 3.54730 avg_loss = 3.48192\n",
      "epoch no.2 train no.184790  loss = 4.66198 avg_loss = 3.46940\n",
      "epoch no.2 train no.184800  loss = 3.54510 avg_loss = 3.48461\n",
      "epoch no.2 train no.184810  loss = 2.66462 avg_loss = 3.51536\n",
      "epoch no.2 train no.184820  loss = 2.95132 avg_loss = 3.49455\n",
      "epoch no.2 train no.184830  loss = 1.90929 avg_loss = 3.46058\n",
      "epoch no.2 train no.184840  loss = 6.16482 avg_loss = 3.45314\n",
      "epoch no.2 train no.184850  loss = 3.58978 avg_loss = 3.47569\n",
      "epoch no.2 train no.184860  loss = 2.17778 avg_loss = 3.45519\n",
      "epoch no.2 train no.184870  loss = 2.64770 avg_loss = 3.46376\n",
      "epoch no.2 train no.184880  loss = 2.59191 avg_loss = 3.47642\n",
      "epoch no.2 train no.184890  loss = 3.62551 avg_loss = 3.49557\n",
      "epoch no.2 train no.184900  loss = 2.19268 avg_loss = 3.52908\n",
      "epoch no.2 train no.184910  loss = 3.21852 avg_loss = 3.54431\n",
      "epoch no.2 train no.184920  loss = 2.18456 avg_loss = 3.53598\n",
      "epoch no.2 train no.184930  loss = 4.86468 avg_loss = 3.54756\n",
      "epoch no.2 train no.184940  loss = 3.25749 avg_loss = 3.50481\n",
      "epoch no.2 train no.184950  loss = 3.15745 avg_loss = 3.45498\n",
      "epoch no.2 train no.184960  loss = 6.60291 avg_loss = 3.48761\n",
      "epoch no.2 train no.184970  loss = 3.33655 avg_loss = 3.46125\n",
      "epoch no.2 train no.184980  loss = 4.08641 avg_loss = 3.47689\n",
      "epoch no.2 train no.184990  loss = 4.59439 avg_loss = 3.56241\n",
      "epoch no.2 train no.185000  loss = 2.51938 avg_loss = 3.54585\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁재즈', '적인', '▁노래', '</s>', '</s>']\n",
      "여름밤에 어울리는 감성적인 노래들</s>\n",
      "epoch no.2 train no.185010  loss = 3.98713 avg_loss = 3.54310\n",
      "epoch no.2 train no.185020  loss = 3.84325 avg_loss = 3.49714\n",
      "epoch no.2 train no.185030  loss = 2.82911 avg_loss = 3.49675\n",
      "epoch no.2 train no.185040  loss = 3.09242 avg_loss = 3.48431\n",
      "epoch no.2 train no.185050  loss = 2.63437 avg_loss = 3.49381\n",
      "epoch no.2 train no.185060  loss = 1.87375 avg_loss = 3.46543\n",
      "epoch no.2 train no.185070  loss = 3.06552 avg_loss = 3.46155\n",
      "epoch no.2 train no.185080  loss = 4.07675 avg_loss = 3.45426\n",
      "epoch no.2 train no.185090  loss = 3.76671 avg_loss = 3.45913\n",
      "epoch no.2 train no.185100  loss = 2.97376 avg_loss = 3.40794\n",
      "epoch no.2 train no.185110  loss = 3.25356 avg_loss = 3.39582\n",
      "epoch no.2 train no.185120  loss = 4.84662 avg_loss = 3.39544\n",
      "epoch no.2 train no.185130  loss = 2.82003 avg_loss = 3.39942\n",
      "epoch no.2 train no.185140  loss = 3.40883 avg_loss = 3.40731\n",
      "epoch no.2 train no.185150  loss = 4.06044 avg_loss = 3.41299\n",
      "epoch no.2 train no.185160  loss = 5.57909 avg_loss = 3.43990\n",
      "epoch no.2 train no.185170  loss = 5.23167 avg_loss = 3.45677\n",
      "epoch no.2 train no.185180  loss = 4.00202 avg_loss = 3.49340\n",
      "epoch no.2 train no.185190  loss = 2.35196 avg_loss = 3.46403\n",
      "epoch no.2 train no.185200  loss = 7.20402 avg_loss = 3.46404\n",
      "epoch no.2 train no.185210  loss = 3.90493 avg_loss = 3.49013\n",
      "epoch no.2 train no.185220  loss = 3.09048 avg_loss = 3.49981\n",
      "epoch no.2 train no.185230  loss = 3.93274 avg_loss = 3.50647\n",
      "epoch no.2 train no.185240  loss = 2.09485 avg_loss = 3.50958\n",
      "epoch no.2 train no.185250  loss = 3.15989 avg_loss = 3.45053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.185260  loss = 3.21347 avg_loss = 3.42314\n",
      "epoch no.2 train no.185270  loss = 3.73071 avg_loss = 3.42070\n",
      "epoch no.2 train no.185280  loss = 4.65260 avg_loss = 3.45648\n",
      "epoch no.2 train no.185290  loss = 3.07824 avg_loss = 3.45427\n",
      "epoch no.2 train no.185300  loss = 2.97147 avg_loss = 3.44077\n",
      "epoch no.2 train no.185310  loss = 4.67877 avg_loss = 3.45957\n",
      "epoch no.2 train no.185320  loss = 3.46703 avg_loss = 3.41847\n",
      "epoch no.2 train no.185330  loss = 2.57184 avg_loss = 3.38776\n",
      "epoch no.2 train no.185340  loss = 3.40794 avg_loss = 3.38419\n",
      "epoch no.2 train no.185350  loss = 5.53616 avg_loss = 3.38290\n",
      "epoch no.2 train no.185360  loss = 3.10811 avg_loss = 3.36529\n",
      "epoch no.2 train no.185370  loss = 4.05468 avg_loss = 3.38978\n",
      "epoch no.2 train no.185380  loss = 2.38945 avg_loss = 3.40611\n",
      "epoch no.2 train no.185390  loss = 2.11243 avg_loss = 3.36252\n",
      "epoch no.2 train no.185400  loss = 2.00019 avg_loss = 3.34272\n",
      "epoch no.2 train no.185410  loss = 3.61206 avg_loss = 3.35168\n",
      "epoch no.2 train no.185420  loss = 4.27235 avg_loss = 3.36451\n",
      "epoch no.2 train no.185430  loss = 3.06910 avg_loss = 3.36951\n",
      "epoch no.2 train no.185440  loss = 3.62520 avg_loss = 3.36390\n",
      "epoch no.2 train no.185450  loss = 3.85947 avg_loss = 3.41531\n",
      "epoch no.2 train no.185460  loss = 5.46364 avg_loss = 3.41690\n",
      "epoch no.2 train no.185470  loss = 2.83615 avg_loss = 3.42849\n",
      "epoch no.2 train no.185480  loss = 3.18725 avg_loss = 3.45386\n",
      "epoch no.2 train no.185490  loss = 4.95735 avg_loss = 3.46298\n",
      "epoch no.2 train no.185500  loss = 2.88045 avg_loss = 3.46390\n",
      "epoch no.2 train no.185510  loss = 3.88128 avg_loss = 3.44162\n",
      "epoch no.2 train no.185520  loss = 3.18964 avg_loss = 3.43834\n",
      "epoch no.2 train no.185530  loss = 2.22750 avg_loss = 3.45657\n",
      "epoch no.2 train no.185540  loss = 2.77619 avg_loss = 3.46410\n",
      "epoch no.2 train no.185550  loss = 2.70061 avg_loss = 3.43388\n",
      "epoch no.2 train no.185560  loss = 3.34333 avg_loss = 3.52097\n",
      "epoch no.2 train no.185570  loss = 2.04910 avg_loss = 3.54023\n",
      "epoch no.2 train no.185580  loss = 2.43455 avg_loss = 3.50783\n",
      "epoch no.2 train no.185590  loss = 1.72626 avg_loss = 3.48398\n",
      "epoch no.2 train no.185600  loss = 4.02840 avg_loss = 3.45104\n",
      "epoch no.2 train no.185610  loss = 5.01308 avg_loss = 3.43953\n",
      "epoch no.2 train no.185620  loss = 5.17906 avg_loss = 3.46346\n",
      "epoch no.2 train no.185630  loss = 1.80876 avg_loss = 3.50628\n",
      "epoch no.2 train no.185640  loss = 3.04159 avg_loss = 3.49818\n",
      "epoch no.2 train no.185650  loss = 2.75914 avg_loss = 3.55275\n",
      "epoch no.2 train no.185660  loss = 3.72708 avg_loss = 3.52622\n",
      "epoch no.2 train no.185670  loss = 3.40666 avg_loss = 3.56970\n",
      "epoch no.2 train no.185680  loss = 4.10258 avg_loss = 3.58827\n",
      "epoch no.2 train no.185690  loss = 1.84553 avg_loss = 3.53893\n",
      "epoch no.2 train no.185700  loss = 1.84674 avg_loss = 3.53695\n",
      "epoch no.2 train no.185710  loss = 4.12171 avg_loss = 3.54494\n",
      "epoch no.2 train no.185720  loss = 2.87405 avg_loss = 3.52252\n",
      "epoch no.2 train no.185730  loss = 3.94009 avg_loss = 3.57888\n",
      "epoch no.2 train no.185740  loss = 2.58041 avg_loss = 3.52575\n",
      "epoch no.2 train no.185750  loss = 3.71108 avg_loss = 3.49686\n",
      "epoch no.2 train no.185760  loss = 3.51525 avg_loss = 3.47417\n",
      "epoch no.2 train no.185770  loss = 3.96433 avg_loss = 3.43269\n",
      "epoch no.2 train no.185780  loss = 3.44553 avg_loss = 3.41607\n",
      "epoch no.2 train no.185790  loss = 3.64686 avg_loss = 3.44087\n",
      "epoch no.2 train no.185800  loss = 4.46461 avg_loss = 3.39534\n",
      "epoch no.2 train no.185810  loss = 3.05777 avg_loss = 3.39794\n",
      "epoch no.2 train no.185820  loss = 3.12450 avg_loss = 3.40990\n",
      "epoch no.2 train no.185830  loss = 2.87373 avg_loss = 3.38675\n",
      "epoch no.2 train no.185840  loss = 3.08085 avg_loss = 3.38440\n",
      "epoch no.2 train no.185850  loss = 4.16676 avg_loss = 3.40625\n",
      "epoch no.2 train no.185860  loss = 4.02213 avg_loss = 3.40450\n",
      "epoch no.2 train no.185870  loss = 2.56711 avg_loss = 3.36440\n",
      "epoch no.2 train no.185880  loss = 2.84608 avg_loss = 3.35388\n",
      "epoch no.2 train no.185890  loss = 4.21279 avg_loss = 3.39038\n",
      "epoch no.2 train no.185900  loss = 4.75725 avg_loss = 3.44526\n",
      "epoch no.2 train no.185910  loss = 1.99728 avg_loss = 3.45567\n",
      "epoch no.2 train no.185920  loss = 2.12841 avg_loss = 3.40474\n",
      "epoch no.2 train no.185930  loss = 6.18089 avg_loss = 3.43461\n",
      "epoch no.2 train no.185940  loss = 3.18308 avg_loss = 3.40750\n",
      "epoch no.2 train no.185950  loss = 4.51403 avg_loss = 3.38834\n",
      "epoch no.2 train no.185960  loss = 2.93496 avg_loss = 3.45978\n",
      "epoch no.2 train no.185970  loss = 4.15009 avg_loss = 3.53158\n",
      "epoch no.2 train no.185980  loss = 4.58349 avg_loss = 3.51510\n",
      "epoch no.2 train no.185990  loss = 2.92438 avg_loss = 3.48355\n",
      "epoch no.2 train no.186000  loss = 4.71381 avg_loss = 3.49470\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁잔잔', '한', '▁감성', '음악', '</s>']\n",
      "여름밤 잔잔한 인디노래</s>\n",
      "epoch no.2 train no.186010  loss = 4.01496 avg_loss = 3.50940\n",
      "epoch no.2 train no.186020  loss = 3.45070 avg_loss = 3.50242\n",
      "epoch no.2 train no.186030  loss = 5.28681 avg_loss = 3.53758\n",
      "epoch no.2 train no.186040  loss = 4.62356 avg_loss = 3.54537\n",
      "epoch no.2 train no.186050  loss = 2.58478 avg_loss = 3.48309\n",
      "epoch no.2 train no.186060  loss = 4.06351 avg_loss = 3.53388\n",
      "epoch no.2 train no.186070  loss = 3.68781 avg_loss = 3.48523\n",
      "epoch no.2 train no.186080  loss = 4.19977 avg_loss = 3.49203\n",
      "epoch no.2 train no.186090  loss = 4.16997 avg_loss = 3.48497\n",
      "epoch no.2 train no.186100  loss = 4.66421 avg_loss = 3.45704\n",
      "epoch no.2 train no.186110  loss = 4.42800 avg_loss = 3.45846\n",
      "epoch no.2 train no.186120  loss = 2.78687 avg_loss = 3.43816\n",
      "epoch no.2 train no.186130  loss = 5.79805 avg_loss = 3.41260\n",
      "epoch no.2 train no.186140  loss = 3.79140 avg_loss = 3.40019\n",
      "epoch no.2 train no.186150  loss = 3.15312 avg_loss = 3.39975\n",
      "epoch no.2 train no.186160  loss = 3.13886 avg_loss = 3.44890\n",
      "epoch no.2 train no.186170  loss = 2.48226 avg_loss = 3.42036\n",
      "epoch no.2 train no.186180  loss = 3.91940 avg_loss = 3.44706\n",
      "epoch no.2 train no.186190  loss = 1.95092 avg_loss = 3.42426\n",
      "epoch no.2 train no.186200  loss = 3.71488 avg_loss = 3.45755\n",
      "epoch no.2 train no.186210  loss = 3.69487 avg_loss = 3.42916\n",
      "epoch no.2 train no.186220  loss = 1.93449 avg_loss = 3.40780\n",
      "epoch no.2 train no.186230  loss = 3.67607 avg_loss = 3.39713\n",
      "epoch no.2 train no.186240  loss = 3.08988 avg_loss = 3.35295\n",
      "epoch no.2 train no.186250  loss = 2.84591 avg_loss = 3.33686\n",
      "epoch no.2 train no.186260  loss = 3.35453 avg_loss = 3.38978\n",
      "epoch no.2 train no.186270  loss = 4.81134 avg_loss = 3.39476\n",
      "epoch no.2 train no.186280  loss = 4.13723 avg_loss = 3.43807\n",
      "epoch no.2 train no.186290  loss = 2.93512 avg_loss = 3.43754\n",
      "epoch no.2 train no.186300  loss = 4.44769 avg_loss = 3.44676\n",
      "epoch no.2 train no.186310  loss = 2.84690 avg_loss = 3.39466\n",
      "epoch no.2 train no.186320  loss = 1.70601 avg_loss = 3.36606\n",
      "epoch no.2 train no.186330  loss = 1.93852 avg_loss = 3.35565\n",
      "epoch no.2 train no.186340  loss = 3.95601 avg_loss = 3.35223\n",
      "epoch no.2 train no.186350  loss = 4.58101 avg_loss = 3.37681\n",
      "epoch no.2 train no.186360  loss = 2.97946 avg_loss = 3.34019\n",
      "epoch no.2 train no.186370  loss = 3.44638 avg_loss = 3.36329\n",
      "epoch no.2 train no.186380  loss = 3.02472 avg_loss = 3.40850\n",
      "epoch no.2 train no.186390  loss = 2.39640 avg_loss = 3.39364\n",
      "epoch no.2 train no.186400  loss = 5.28413 avg_loss = 3.41359\n",
      "epoch no.2 train no.186410  loss = 2.13537 avg_loss = 3.40570\n",
      "epoch no.2 train no.186420  loss = 3.58293 avg_loss = 3.41164\n",
      "epoch no.2 train no.186430  loss = 2.06192 avg_loss = 3.43543\n",
      "epoch no.2 train no.186440  loss = 2.87466 avg_loss = 3.41219\n",
      "epoch no.2 train no.186450  loss = 5.76993 avg_loss = 3.48655\n",
      "epoch no.2 train no.186460  loss = 3.83312 avg_loss = 3.48944\n",
      "epoch no.2 train no.186470  loss = 4.27854 avg_loss = 3.46486\n",
      "epoch no.2 train no.186480  loss = 3.53597 avg_loss = 3.49695\n",
      "epoch no.2 train no.186490  loss = 3.73788 avg_loss = 3.44661\n",
      "epoch no.2 train no.186500  loss = 4.40305 avg_loss = 3.46401\n",
      "epoch no.2 train no.186510  loss = 3.92454 avg_loss = 3.45045\n",
      "epoch no.2 train no.186520  loss = 4.38868 avg_loss = 3.40135\n",
      "epoch no.2 train no.186530  loss = 2.25950 avg_loss = 3.44033\n",
      "epoch no.2 train no.186540  loss = 3.82842 avg_loss = 3.40896\n",
      "epoch no.2 train no.186550  loss = 2.98430 avg_loss = 3.44497\n",
      "epoch no.2 train no.186560  loss = 3.39832 avg_loss = 3.41622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.186570  loss = 5.28500 avg_loss = 3.44592\n",
      "epoch no.2 train no.186580  loss = 2.37792 avg_loss = 3.46448\n",
      "epoch no.2 train no.186590  loss = 2.62570 avg_loss = 3.42493\n",
      "epoch no.2 train no.186600  loss = 2.93541 avg_loss = 3.40929\n",
      "epoch no.2 train no.186610  loss = 2.72129 avg_loss = 3.42831\n",
      "epoch no.2 train no.186620  loss = 3.28797 avg_loss = 3.41102\n",
      "epoch no.2 train no.186630  loss = 4.57185 avg_loss = 3.50728\n",
      "epoch no.2 train no.186640  loss = 3.22270 avg_loss = 3.52305\n",
      "epoch no.2 train no.186650  loss = 4.06950 avg_loss = 3.49342\n",
      "epoch no.2 train no.186660  loss = 3.93907 avg_loss = 3.43821\n",
      "epoch no.2 train no.186670  loss = 3.06971 avg_loss = 3.45836\n",
      "epoch no.2 train no.186680  loss = 3.08264 avg_loss = 3.41894\n",
      "epoch no.2 train no.186690  loss = 2.36332 avg_loss = 3.40190\n",
      "epoch no.2 train no.186700  loss = 4.46926 avg_loss = 3.40947\n",
      "epoch no.2 train no.186710  loss = 3.61425 avg_loss = 3.44132\n",
      "epoch no.2 train no.186720  loss = 4.77606 avg_loss = 3.48664\n",
      "epoch no.2 train no.186730  loss = 3.47884 avg_loss = 3.50783\n",
      "epoch no.2 train no.186740  loss = 3.13953 avg_loss = 3.53549\n",
      "epoch no.2 train no.186750  loss = 2.08407 avg_loss = 3.49630\n",
      "epoch no.2 train no.186760  loss = 1.87428 avg_loss = 3.49307\n",
      "epoch no.2 train no.186770  loss = 4.51816 avg_loss = 3.53646\n",
      "epoch no.2 train no.186780  loss = 2.75729 avg_loss = 3.56569\n",
      "epoch no.2 train no.186790  loss = 5.92509 avg_loss = 3.62574\n",
      "epoch no.2 train no.186800  loss = 2.86329 avg_loss = 3.61021\n",
      "epoch no.2 train no.186810  loss = 3.96368 avg_loss = 3.60759\n",
      "epoch no.2 train no.186820  loss = 3.96128 avg_loss = 3.56409\n",
      "epoch no.2 train no.186830  loss = 2.21160 avg_loss = 3.53290\n",
      "epoch no.2 train no.186840  loss = 2.29571 avg_loss = 3.52041\n",
      "epoch no.2 train no.186850  loss = 4.88373 avg_loss = 3.61439\n",
      "epoch no.2 train no.186860  loss = 4.21602 avg_loss = 3.55683\n",
      "epoch no.2 train no.186870  loss = 3.06433 avg_loss = 3.55866\n",
      "epoch no.2 train no.186880  loss = 2.77921 avg_loss = 3.50797\n",
      "epoch no.2 train no.186890  loss = 2.23880 avg_loss = 3.50765\n",
      "epoch no.2 train no.186900  loss = 2.61127 avg_loss = 3.49370\n",
      "epoch no.2 train no.186910  loss = 4.48657 avg_loss = 3.47680\n",
      "epoch no.2 train no.186920  loss = 1.72891 avg_loss = 3.53251\n",
      "epoch no.2 train no.186930  loss = 2.18523 avg_loss = 3.51480\n",
      "epoch no.2 train no.186940  loss = 3.08046 avg_loss = 3.48126\n",
      "epoch no.2 train no.186950  loss = 3.48866 avg_loss = 3.49656\n",
      "epoch no.2 train no.186960  loss = 2.98923 avg_loss = 3.48928\n",
      "epoch no.2 train no.186970  loss = 3.16830 avg_loss = 3.49193\n",
      "epoch no.2 train no.186980  loss = 4.35150 avg_loss = 3.51893\n",
      "epoch no.2 train no.186990  loss = 2.08465 avg_loss = 3.47622\n",
      "epoch no.2 train no.187000  loss = 3.61236 avg_loss = 3.38301\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '을', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 재즈</s>\n",
      "epoch no.2 train no.187010  loss = 2.60818 avg_loss = 3.37924\n",
      "epoch no.2 train no.187020  loss = 3.55803 avg_loss = 3.37472\n",
      "epoch no.2 train no.187030  loss = 3.81169 avg_loss = 3.33057\n",
      "epoch no.2 train no.187040  loss = 6.93200 avg_loss = 3.37232\n",
      "epoch no.2 train no.187050  loss = 4.62975 avg_loss = 3.37395\n",
      "epoch no.2 train no.187060  loss = 2.40106 avg_loss = 3.36990\n",
      "epoch no.2 train no.187070  loss = 3.10900 avg_loss = 3.37560\n",
      "epoch no.2 train no.187080  loss = 5.11273 avg_loss = 3.39816\n",
      "epoch no.2 train no.187090  loss = 3.93158 avg_loss = 3.47688\n",
      "epoch no.2 train no.187100  loss = 3.93116 avg_loss = 3.46636\n",
      "epoch no.2 train no.187110  loss = 5.00337 avg_loss = 3.49342\n",
      "epoch no.2 train no.187120  loss = 4.18762 avg_loss = 3.47288\n",
      "epoch no.2 train no.187130  loss = 3.10527 avg_loss = 3.49810\n",
      "epoch no.2 train no.187140  loss = 2.40053 avg_loss = 3.52007\n",
      "epoch no.2 train no.187150  loss = 3.17119 avg_loss = 3.53890\n",
      "epoch no.2 train no.187160  loss = 4.83760 avg_loss = 3.52552\n",
      "epoch no.2 train no.187170  loss = 3.97313 avg_loss = 3.49154\n",
      "epoch no.2 train no.187180  loss = 1.43066 avg_loss = 3.45318\n",
      "epoch no.2 train no.187190  loss = 3.24582 avg_loss = 3.54965\n",
      "epoch no.2 train no.187200  loss = 3.67455 avg_loss = 3.52810\n",
      "epoch no.2 train no.187210  loss = 3.52434 avg_loss = 3.54824\n",
      "epoch no.2 train no.187220  loss = 3.25172 avg_loss = 3.54005\n",
      "epoch no.2 train no.187230  loss = 2.98401 avg_loss = 3.48668\n",
      "epoch no.2 train no.187240  loss = 3.20817 avg_loss = 3.51758\n",
      "epoch no.2 train no.187250  loss = 3.45545 avg_loss = 3.50236\n",
      "epoch no.2 train no.187260  loss = 3.75929 avg_loss = 3.49070\n",
      "epoch no.2 train no.187270  loss = 3.17527 avg_loss = 3.47974\n",
      "epoch no.2 train no.187280  loss = 2.87803 avg_loss = 3.49319\n",
      "epoch no.2 train no.187290  loss = 3.51744 avg_loss = 3.46923\n",
      "epoch no.2 train no.187300  loss = 2.76608 avg_loss = 3.45656\n",
      "epoch no.2 train no.187310  loss = 3.32887 avg_loss = 3.43578\n",
      "epoch no.2 train no.187320  loss = 2.27827 avg_loss = 3.37359\n",
      "epoch no.2 train no.187330  loss = 4.32394 avg_loss = 3.41036\n",
      "epoch no.2 train no.187340  loss = 2.55389 avg_loss = 3.41825\n",
      "epoch no.2 train no.187350  loss = 4.52284 avg_loss = 3.41597\n",
      "epoch no.2 train no.187360  loss = 3.28300 avg_loss = 3.43077\n",
      "epoch no.2 train no.187370  loss = 3.14650 avg_loss = 3.38427\n",
      "epoch no.2 train no.187380  loss = 4.20381 avg_loss = 3.41771\n",
      "epoch no.2 train no.187390  loss = 2.91095 avg_loss = 3.34964\n",
      "epoch no.2 train no.187400  loss = 2.93362 avg_loss = 3.35572\n",
      "epoch no.2 train no.187410  loss = 3.07404 avg_loss = 3.35599\n",
      "epoch no.2 train no.187420  loss = 2.84864 avg_loss = 3.33376\n",
      "epoch no.2 train no.187430  loss = 3.52334 avg_loss = 3.29910\n",
      "epoch no.2 train no.187440  loss = 2.00828 avg_loss = 3.24234\n",
      "epoch no.2 train no.187450  loss = 1.99733 avg_loss = 3.24392\n",
      "epoch no.2 train no.187460  loss = 5.63637 avg_loss = 3.27429\n",
      "epoch no.2 train no.187470  loss = 2.80013 avg_loss = 3.28921\n",
      "epoch no.2 train no.187480  loss = 3.84517 avg_loss = 3.33125\n",
      "epoch no.2 train no.187490  loss = 2.95386 avg_loss = 3.34647\n",
      "epoch no.2 train no.187500  loss = 1.16164 avg_loss = 3.29097\n",
      "epoch no.2 train no.187510  loss = 4.20760 avg_loss = 3.27074\n",
      "epoch no.2 train no.187520  loss = 2.46489 avg_loss = 3.25706\n",
      "epoch no.2 train no.187530  loss = 3.61092 avg_loss = 3.28646\n",
      "epoch no.2 train no.187540  loss = 6.93033 avg_loss = 3.33681\n",
      "epoch no.2 train no.187550  loss = 5.16915 avg_loss = 3.35299\n",
      "epoch no.2 train no.187560  loss = 2.45746 avg_loss = 3.32754\n",
      "epoch no.2 train no.187570  loss = 3.61063 avg_loss = 3.31079\n",
      "epoch no.2 train no.187580  loss = 2.48271 avg_loss = 3.27667\n",
      "epoch no.2 train no.187590  loss = 3.89566 avg_loss = 3.35341\n",
      "epoch no.2 train no.187600  loss = 3.35740 avg_loss = 3.33936\n",
      "epoch no.2 train no.187610  loss = 3.00742 avg_loss = 3.32949\n",
      "epoch no.2 train no.187620  loss = 4.77881 avg_loss = 3.33800\n",
      "epoch no.2 train no.187630  loss = 4.58186 avg_loss = 3.38834\n",
      "epoch no.2 train no.187640  loss = 3.42302 avg_loss = 3.37253\n",
      "epoch no.2 train no.187650  loss = 3.66860 avg_loss = 3.35455\n",
      "epoch no.2 train no.187660  loss = 4.65388 avg_loss = 3.34967\n",
      "epoch no.2 train no.187670  loss = 3.35616 avg_loss = 3.39551\n",
      "epoch no.2 train no.187680  loss = 2.27110 avg_loss = 3.40565\n",
      "epoch no.2 train no.187690  loss = 3.60974 avg_loss = 3.43711\n",
      "epoch no.2 train no.187700  loss = 3.91534 avg_loss = 3.40185\n",
      "epoch no.2 train no.187710  loss = 2.93552 avg_loss = 3.39738\n",
      "epoch no.2 train no.187720  loss = 2.71455 avg_loss = 3.36726\n",
      "epoch no.2 train no.187730  loss = 2.80363 avg_loss = 3.37619\n",
      "epoch no.2 train no.187740  loss = 1.96911 avg_loss = 3.34386\n",
      "epoch no.2 train no.187750  loss = 4.39274 avg_loss = 3.39658\n",
      "epoch no.2 train no.187760  loss = 2.05468 avg_loss = 3.39148\n",
      "epoch no.2 train no.187770  loss = 3.16336 avg_loss = 3.39893\n",
      "epoch no.2 train no.187780  loss = 3.31967 avg_loss = 3.39186\n",
      "epoch no.2 train no.187790  loss = 4.19586 avg_loss = 3.45322\n",
      "epoch no.2 train no.187800  loss = 2.23736 avg_loss = 3.44494\n",
      "epoch no.2 train no.187810  loss = 2.26190 avg_loss = 3.44730\n",
      "epoch no.2 train no.187820  loss = 2.34609 avg_loss = 3.48758\n",
      "epoch no.2 train no.187830  loss = 4.42637 avg_loss = 3.52848\n",
      "epoch no.2 train no.187840  loss = 2.95425 avg_loss = 3.57966\n",
      "epoch no.2 train no.187850  loss = 6.27374 avg_loss = 3.62180\n",
      "epoch no.2 train no.187860  loss = 3.13596 avg_loss = 3.59440\n",
      "epoch no.2 train no.187870  loss = 2.90705 avg_loss = 3.60941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.187880  loss = 4.79986 avg_loss = 3.59949\n",
      "epoch no.2 train no.187890  loss = 2.67713 avg_loss = 3.59940\n",
      "epoch no.2 train no.187900  loss = 3.86375 avg_loss = 3.60343\n",
      "epoch no.2 train no.187910  loss = 3.13373 avg_loss = 3.61919\n",
      "epoch no.2 train no.187920  loss = 4.05721 avg_loss = 3.60134\n",
      "epoch no.2 train no.187930  loss = 2.71594 avg_loss = 3.55123\n",
      "epoch no.2 train no.187940  loss = 4.58798 avg_loss = 3.57191\n",
      "epoch no.2 train no.187950  loss = 5.60800 avg_loss = 3.53661\n",
      "epoch no.2 train no.187960  loss = 3.39499 avg_loss = 3.51134\n",
      "epoch no.2 train no.187970  loss = 2.45023 avg_loss = 3.45666\n",
      "epoch no.2 train no.187980  loss = 2.08458 avg_loss = 3.46440\n",
      "epoch no.2 train no.187990  loss = 3.15082 avg_loss = 3.45784\n",
      "epoch no.2 train no.188000  loss = 4.44946 avg_loss = 3.50034\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.188010  loss = 3.85336 avg_loss = 3.49459\n",
      "epoch no.2 train no.188020  loss = 5.37240 avg_loss = 3.51031\n",
      "epoch no.2 train no.188030  loss = 5.10179 avg_loss = 3.49698\n",
      "epoch no.2 train no.188040  loss = 5.76933 avg_loss = 3.51648\n",
      "epoch no.2 train no.188050  loss = 4.29635 avg_loss = 3.53447\n",
      "epoch no.2 train no.188060  loss = 2.70089 avg_loss = 3.55643\n",
      "epoch no.2 train no.188070  loss = 2.76104 avg_loss = 3.50218\n",
      "epoch no.2 train no.188080  loss = 4.47813 avg_loss = 3.46058\n",
      "epoch no.2 train no.188090  loss = 5.95602 avg_loss = 3.48820\n",
      "epoch no.2 train no.188100  loss = 2.02546 avg_loss = 3.49670\n",
      "epoch no.2 train no.188110  loss = 2.28465 avg_loss = 3.45486\n",
      "epoch no.2 train no.188120  loss = 3.54335 avg_loss = 3.48662\n",
      "epoch no.2 train no.188130  loss = 2.93723 avg_loss = 3.45722\n",
      "epoch no.2 train no.188140  loss = 3.15718 avg_loss = 3.48344\n",
      "epoch no.2 train no.188150  loss = 2.70248 avg_loss = 3.45976\n",
      "epoch no.2 train no.188160  loss = 4.62242 avg_loss = 3.51514\n",
      "epoch no.2 train no.188170  loss = 1.75446 avg_loss = 3.45286\n",
      "epoch no.2 train no.188180  loss = 2.50056 avg_loss = 3.47156\n",
      "epoch no.2 train no.188190  loss = 2.51367 avg_loss = 3.41286\n",
      "epoch no.2 train no.188200  loss = 3.83350 avg_loss = 3.41282\n",
      "epoch no.2 train no.188210  loss = 2.41830 avg_loss = 3.41694\n",
      "epoch no.2 train no.188220  loss = 2.51152 avg_loss = 3.42630\n",
      "epoch no.2 train no.188230  loss = 2.66569 avg_loss = 3.41849\n",
      "epoch no.2 train no.188240  loss = 3.91011 avg_loss = 3.39960\n",
      "epoch no.2 train no.188250  loss = 4.70097 avg_loss = 3.51125\n",
      "epoch no.2 train no.188260  loss = 3.30678 avg_loss = 3.52210\n",
      "epoch no.2 train no.188270  loss = 4.49651 avg_loss = 3.52296\n",
      "epoch no.2 train no.188280  loss = 2.95714 avg_loss = 3.49805\n",
      "epoch no.2 train no.188290  loss = 4.62211 avg_loss = 3.47506\n",
      "epoch no.2 train no.188300  loss = 2.87561 avg_loss = 3.51517\n",
      "epoch no.2 train no.188310  loss = 2.85809 avg_loss = 3.48115\n",
      "epoch no.2 train no.188320  loss = 2.84615 avg_loss = 3.48962\n",
      "epoch no.2 train no.188330  loss = 2.85842 avg_loss = 3.45534\n",
      "epoch no.2 train no.188340  loss = 2.58572 avg_loss = 3.44181\n",
      "epoch no.2 train no.188350  loss = 5.08382 avg_loss = 3.46907\n",
      "epoch no.2 train no.188360  loss = 3.62519 avg_loss = 3.49658\n",
      "epoch no.2 train no.188370  loss = 5.44674 avg_loss = 3.51354\n",
      "epoch no.2 train no.188380  loss = 1.92933 avg_loss = 3.47148\n",
      "epoch no.2 train no.188390  loss = 4.14229 avg_loss = 3.47301\n",
      "epoch no.2 train no.188400  loss = 4.00018 avg_loss = 3.43677\n",
      "epoch no.2 train no.188410  loss = 3.01523 avg_loss = 3.40010\n",
      "epoch no.2 train no.188420  loss = 3.43157 avg_loss = 3.46775\n",
      "epoch no.2 train no.188430  loss = 4.08279 avg_loss = 3.49242\n",
      "epoch no.2 train no.188440  loss = 2.79231 avg_loss = 3.49170\n",
      "epoch no.2 train no.188450  loss = 3.73475 avg_loss = 3.52044\n",
      "epoch no.2 train no.188460  loss = 3.38860 avg_loss = 3.51189\n",
      "epoch no.2 train no.188470  loss = 4.88231 avg_loss = 3.48692\n",
      "epoch no.2 train no.188480  loss = 2.05134 avg_loss = 3.52367\n",
      "epoch no.2 train no.188490  loss = 3.56912 avg_loss = 3.56765\n",
      "epoch no.2 train no.188500  loss = 3.36027 avg_loss = 3.56015\n",
      "epoch no.2 train no.188510  loss = 3.67520 avg_loss = 3.61612\n",
      "epoch no.2 train no.188520  loss = 2.03173 avg_loss = 3.60652\n",
      "epoch no.2 train no.188530  loss = 2.36458 avg_loss = 3.63899\n",
      "epoch no.2 train no.188540  loss = 4.96559 avg_loss = 3.66837\n",
      "epoch no.2 train no.188550  loss = 4.23347 avg_loss = 3.63757\n",
      "epoch no.2 train no.188560  loss = 6.69265 avg_loss = 3.64401\n",
      "epoch no.2 train no.188570  loss = 6.25063 avg_loss = 3.69853\n",
      "epoch no.2 train no.188580  loss = 2.57034 avg_loss = 3.65261\n",
      "epoch no.2 train no.188590  loss = 3.29972 avg_loss = 3.61316\n",
      "epoch no.2 train no.188600  loss = 4.65240 avg_loss = 3.61273\n",
      "epoch no.2 train no.188610  loss = 3.64522 avg_loss = 3.62328\n",
      "epoch no.2 train no.188620  loss = 3.51671 avg_loss = 3.61990\n",
      "epoch no.2 train no.188630  loss = 4.35075 avg_loss = 3.59754\n",
      "epoch no.2 train no.188640  loss = 1.71356 avg_loss = 3.56846\n",
      "epoch no.2 train no.188650  loss = 3.11812 avg_loss = 3.56034\n",
      "epoch no.2 train no.188660  loss = 3.90539 avg_loss = 3.63608\n",
      "epoch no.2 train no.188670  loss = 2.13935 avg_loss = 3.56135\n",
      "epoch no.2 train no.188680  loss = 3.35958 avg_loss = 3.57960\n",
      "epoch no.2 train no.188690  loss = 3.58984 avg_loss = 3.56193\n",
      "epoch no.2 train no.188700  loss = 3.00796 avg_loss = 3.59558\n",
      "epoch no.2 train no.188710  loss = 2.67837 avg_loss = 3.54752\n",
      "epoch no.2 train no.188720  loss = 4.39395 avg_loss = 3.52970\n",
      "epoch no.2 train no.188730  loss = 5.27780 avg_loss = 3.49444\n",
      "epoch no.2 train no.188740  loss = 4.01330 avg_loss = 3.48696\n",
      "epoch no.2 train no.188750  loss = 4.46119 avg_loss = 3.51282\n",
      "epoch no.2 train no.188760  loss = 2.74720 avg_loss = 3.51689\n",
      "epoch no.2 train no.188770  loss = 2.61075 avg_loss = 3.49153\n",
      "epoch no.2 train no.188780  loss = 3.87611 avg_loss = 3.47591\n",
      "epoch no.2 train no.188790  loss = 3.89067 avg_loss = 3.47526\n",
      "epoch no.2 train no.188800  loss = 5.60039 avg_loss = 3.46476\n",
      "epoch no.2 train no.188810  loss = 2.70716 avg_loss = 3.50144\n",
      "epoch no.2 train no.188820  loss = 2.48564 avg_loss = 3.45853\n",
      "epoch no.2 train no.188830  loss = 3.86326 avg_loss = 3.53553\n",
      "epoch no.2 train no.188840  loss = 2.93685 avg_loss = 3.52890\n",
      "epoch no.2 train no.188850  loss = 4.85043 avg_loss = 3.49553\n",
      "epoch no.2 train no.188860  loss = 3.79018 avg_loss = 3.53624\n",
      "epoch no.2 train no.188870  loss = 2.20422 avg_loss = 3.49844\n",
      "epoch no.2 train no.188880  loss = 3.46607 avg_loss = 3.49107\n",
      "epoch no.2 train no.188890  loss = 3.80263 avg_loss = 3.52891\n",
      "epoch no.2 train no.188900  loss = 3.68361 avg_loss = 3.49491\n",
      "epoch no.2 train no.188910  loss = 4.48479 avg_loss = 3.48684\n",
      "epoch no.2 train no.188920  loss = 4.01367 avg_loss = 3.48381\n",
      "epoch no.2 train no.188930  loss = 3.25888 avg_loss = 3.46798\n",
      "epoch no.2 train no.188940  loss = 2.35411 avg_loss = 3.46320\n",
      "epoch no.2 train no.188950  loss = 1.57547 avg_loss = 3.40887\n",
      "epoch no.2 train no.188960  loss = 4.90600 avg_loss = 3.41828\n",
      "epoch no.2 train no.188970  loss = 3.88670 avg_loss = 3.45213\n",
      "epoch no.2 train no.188980  loss = 5.43682 avg_loss = 3.41007\n",
      "epoch no.2 train no.188990  loss = 3.37360 avg_loss = 3.43670\n",
      "epoch no.2 train no.189000  loss = 2.34914 avg_loss = 3.44284\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁위한', '해줄', '▁노래', '음악', '</s>']\n",
      "여름밤을 함께 해줄 인디음악</s>\n",
      "epoch no.2 train no.189010  loss = 3.77678 avg_loss = 3.43925\n",
      "epoch no.2 train no.189020  loss = 2.66105 avg_loss = 3.45647\n",
      "epoch no.2 train no.189030  loss = 3.93522 avg_loss = 3.44453\n",
      "epoch no.2 train no.189040  loss = 4.62140 avg_loss = 3.41499\n",
      "epoch no.2 train no.189050  loss = 3.37254 avg_loss = 3.45574\n",
      "epoch no.2 train no.189060  loss = 1.84074 avg_loss = 3.45664\n",
      "epoch no.2 train no.189070  loss = 4.40047 avg_loss = 3.47498\n",
      "epoch no.2 train no.189080  loss = 1.11310 avg_loss = 3.47230\n",
      "epoch no.2 train no.189090  loss = 3.68897 avg_loss = 3.46833\n",
      "epoch no.2 train no.189100  loss = 2.62526 avg_loss = 3.47174\n",
      "epoch no.2 train no.189110  loss = 2.17861 avg_loss = 3.48481\n",
      "epoch no.2 train no.189120  loss = 3.08621 avg_loss = 3.54943\n",
      "epoch no.2 train no.189130  loss = 1.80642 avg_loss = 3.48260\n",
      "epoch no.2 train no.189140  loss = 4.48661 avg_loss = 3.43907\n",
      "epoch no.2 train no.189150  loss = 3.04314 avg_loss = 3.39839\n",
      "epoch no.2 train no.189160  loss = 4.25678 avg_loss = 3.42715\n",
      "epoch no.2 train no.189170  loss = 2.85976 avg_loss = 3.41696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.189180  loss = 5.52860 avg_loss = 3.49080\n",
      "epoch no.2 train no.189190  loss = 3.46435 avg_loss = 3.49450\n",
      "epoch no.2 train no.189200  loss = 3.23266 avg_loss = 3.52630\n",
      "epoch no.2 train no.189210  loss = 2.56848 avg_loss = 3.52339\n",
      "epoch no.2 train no.189220  loss = 3.46624 avg_loss = 3.52063\n",
      "epoch no.2 train no.189230  loss = 5.58582 avg_loss = 3.53210\n",
      "epoch no.2 train no.189240  loss = 4.55755 avg_loss = 3.48104\n",
      "epoch no.2 train no.189250  loss = 3.54059 avg_loss = 3.46012\n",
      "epoch no.2 train no.189260  loss = 4.04347 avg_loss = 3.45536\n",
      "epoch no.2 train no.189270  loss = 3.32090 avg_loss = 3.39035\n",
      "epoch no.2 train no.189280  loss = 3.24779 avg_loss = 3.43878\n",
      "epoch no.2 train no.189290  loss = 3.89220 avg_loss = 3.45392\n",
      "epoch no.2 train no.189300  loss = 5.06858 avg_loss = 3.51464\n",
      "epoch no.2 train no.189310  loss = 3.10006 avg_loss = 3.50120\n",
      "epoch no.2 train no.189320  loss = 3.65973 avg_loss = 3.49687\n",
      "epoch no.2 train no.189330  loss = 2.85278 avg_loss = 3.49777\n",
      "epoch no.2 train no.189340  loss = 2.24222 avg_loss = 3.48038\n",
      "epoch no.2 train no.189350  loss = 3.12517 avg_loss = 3.47301\n",
      "epoch no.2 train no.189360  loss = 4.20493 avg_loss = 3.51451\n",
      "epoch no.2 train no.189370  loss = 3.82743 avg_loss = 3.55831\n",
      "epoch no.2 train no.189380  loss = 2.11020 avg_loss = 3.51612\n",
      "epoch no.2 train no.189390  loss = 3.22269 avg_loss = 3.55359\n",
      "epoch no.2 train no.189400  loss = 2.64179 avg_loss = 3.50049\n",
      "epoch no.2 train no.189410  loss = 4.52577 avg_loss = 3.57456\n",
      "epoch no.2 train no.189420  loss = 2.42074 avg_loss = 3.59022\n",
      "epoch no.2 train no.189430  loss = 4.58957 avg_loss = 3.60930\n",
      "epoch no.2 train no.189440  loss = 5.25008 avg_loss = 3.61677\n",
      "epoch no.2 train no.189450  loss = 2.91659 avg_loss = 3.58279\n",
      "epoch no.2 train no.189460  loss = 3.21741 avg_loss = 3.58961\n",
      "epoch no.2 train no.189470  loss = 4.64536 avg_loss = 3.61427\n",
      "epoch no.2 train no.189480  loss = 2.75269 avg_loss = 3.61568\n",
      "epoch no.2 train no.189490  loss = 5.87213 avg_loss = 3.62416\n",
      "epoch no.2 train no.189500  loss = 3.19664 avg_loss = 3.59116\n",
      "epoch no.2 train no.189510  loss = 2.00335 avg_loss = 3.56828\n",
      "epoch no.2 train no.189520  loss = 3.97624 avg_loss = 3.61736\n",
      "epoch no.2 train no.189530  loss = 3.52052 avg_loss = 3.60565\n",
      "epoch no.2 train no.189540  loss = 4.28295 avg_loss = 3.58173\n",
      "epoch no.2 train no.189550  loss = 4.78260 avg_loss = 3.55317\n",
      "epoch no.2 train no.189560  loss = 2.88797 avg_loss = 3.50502\n",
      "epoch no.2 train no.189570  loss = 3.24013 avg_loss = 3.49789\n",
      "epoch no.2 train no.189580  loss = 3.50334 avg_loss = 3.49414\n",
      "epoch no.2 train no.189590  loss = 5.72224 avg_loss = 3.52174\n",
      "epoch no.2 train no.189600  loss = 2.92569 avg_loss = 3.50434\n",
      "epoch no.2 train no.189610  loss = 2.33124 avg_loss = 3.49613\n",
      "epoch no.2 train no.189620  loss = 3.74420 avg_loss = 3.53248\n",
      "epoch no.2 train no.189630  loss = 4.67817 avg_loss = 3.51118\n",
      "epoch no.2 train no.189640  loss = 2.03207 avg_loss = 3.43387\n",
      "epoch no.2 train no.189650  loss = 4.61231 avg_loss = 3.44702\n",
      "epoch no.2 train no.189660  loss = 3.00082 avg_loss = 3.43830\n",
      "epoch no.2 train no.189670  loss = 2.71632 avg_loss = 3.47109\n",
      "epoch no.2 train no.189680  loss = 3.47214 avg_loss = 3.41413\n",
      "epoch no.2 train no.189690  loss = 2.69806 avg_loss = 3.46593\n",
      "epoch no.2 train no.189700  loss = 4.19895 avg_loss = 3.49962\n",
      "epoch no.2 train no.189710  loss = 3.39611 avg_loss = 3.50101\n",
      "epoch no.2 train no.189720  loss = 3.86642 avg_loss = 3.55041\n",
      "epoch no.2 train no.189730  loss = 2.09656 avg_loss = 3.53794\n",
      "epoch no.2 train no.189740  loss = 3.87970 avg_loss = 3.52591\n",
      "epoch no.2 train no.189750  loss = 4.04838 avg_loss = 3.51012\n",
      "epoch no.2 train no.189760  loss = 4.30141 avg_loss = 3.53928\n",
      "epoch no.2 train no.189770  loss = 3.62267 avg_loss = 3.54590\n",
      "epoch no.2 train no.189780  loss = 2.37042 avg_loss = 3.45987\n",
      "epoch no.2 train no.189790  loss = 2.84527 avg_loss = 3.45313\n",
      "epoch no.2 train no.189800  loss = 1.83066 avg_loss = 3.42733\n",
      "epoch no.2 train no.189810  loss = 2.34600 avg_loss = 3.43522\n",
      "epoch no.2 train no.189820  loss = 2.63178 avg_loss = 3.43257\n",
      "epoch no.2 train no.189830  loss = 2.16866 avg_loss = 3.40502\n",
      "epoch no.2 train no.189840  loss = 4.26012 avg_loss = 3.44153\n",
      "epoch no.2 train no.189850  loss = 4.25008 avg_loss = 3.44400\n",
      "epoch no.2 train no.189860  loss = 2.93148 avg_loss = 3.43286\n",
      "epoch no.2 train no.189870  loss = 5.09256 avg_loss = 3.45742\n",
      "epoch no.2 train no.189880  loss = 3.81389 avg_loss = 3.46789\n",
      "epoch no.2 train no.189890  loss = 3.63848 avg_loss = 3.51527\n",
      "epoch no.2 train no.189900  loss = 2.52622 avg_loss = 3.50896\n",
      "epoch no.2 train no.189910  loss = 5.11632 avg_loss = 3.53977\n",
      "epoch no.2 train no.189920  loss = 3.93990 avg_loss = 3.53714\n",
      "epoch no.2 train no.189930  loss = 2.67329 avg_loss = 3.49855\n",
      "epoch no.2 train no.189940  loss = 4.19706 avg_loss = 3.48844\n",
      "epoch no.2 train no.189950  loss = 2.42167 avg_loss = 3.45154\n",
      "epoch no.2 train no.189960  loss = 4.41951 avg_loss = 3.46652\n",
      "epoch no.2 train no.189970  loss = 2.87860 avg_loss = 3.49193\n",
      "epoch no.2 train no.189980  loss = 2.61382 avg_loss = 3.48528\n",
      "epoch no.2 train no.189990  loss = 4.27686 avg_loss = 3.46499\n",
      "epoch no.2 train no.190000  loss = 3.28248 avg_loss = 3.45768\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '을', '▁시원하게', '▁감성', '</s>']\n",
      "여름밤을 위한 재즈</s>\n",
      "epoch no.2 train no.190010  loss = 2.31342 avg_loss = 3.42164\n",
      "epoch no.2 train no.190020  loss = 4.50583 avg_loss = 3.43229\n",
      "epoch no.2 train no.190030  loss = 4.12056 avg_loss = 3.45895\n",
      "epoch no.2 train no.190040  loss = 3.75612 avg_loss = 3.46031\n",
      "epoch no.2 train no.190050  loss = 3.80520 avg_loss = 3.47701\n",
      "epoch no.2 train no.190060  loss = 3.58739 avg_loss = 3.43508\n",
      "epoch no.2 train no.190070  loss = 3.83965 avg_loss = 3.40240\n",
      "epoch no.2 train no.190080  loss = 3.63477 avg_loss = 3.40726\n",
      "epoch no.2 train no.190090  loss = 2.85013 avg_loss = 3.37084\n",
      "epoch no.2 train no.190100  loss = 6.29453 avg_loss = 3.39460\n",
      "epoch no.2 train no.190110  loss = 3.00342 avg_loss = 3.35780\n",
      "epoch no.2 train no.190120  loss = 3.75324 avg_loss = 3.33167\n",
      "epoch no.2 train no.190130  loss = 4.84559 avg_loss = 3.37362\n",
      "epoch no.2 train no.190140  loss = 3.90363 avg_loss = 3.39145\n",
      "epoch no.2 train no.190150  loss = 0.91468 avg_loss = 3.35953\n",
      "epoch no.2 train no.190160  loss = 5.78568 avg_loss = 3.43178\n",
      "epoch no.2 train no.190170  loss = 2.88379 avg_loss = 3.46649\n",
      "epoch no.2 train no.190180  loss = 3.97321 avg_loss = 3.45835\n",
      "epoch no.2 train no.190190  loss = 3.24377 avg_loss = 3.44793\n",
      "epoch no.2 train no.190200  loss = 3.05933 avg_loss = 3.43363\n",
      "epoch no.2 train no.190210  loss = 4.32083 avg_loss = 3.42336\n",
      "epoch no.2 train no.190220  loss = 2.27780 avg_loss = 3.39829\n",
      "epoch no.2 train no.190230  loss = 4.10084 avg_loss = 3.42921\n",
      "epoch no.2 train no.190240  loss = 3.73201 avg_loss = 3.44833\n",
      "epoch no.2 train no.190250  loss = 2.23074 avg_loss = 3.43490\n",
      "epoch no.2 train no.190260  loss = 3.05885 avg_loss = 3.49872\n",
      "epoch no.2 train no.190270  loss = 1.95992 avg_loss = 3.43272\n",
      "epoch no.2 train no.190280  loss = 5.06486 avg_loss = 3.43033\n",
      "epoch no.2 train no.190290  loss = 3.13456 avg_loss = 3.44909\n",
      "epoch no.2 train no.190300  loss = 4.36953 avg_loss = 3.42994\n",
      "epoch no.2 train no.190310  loss = 2.60543 avg_loss = 3.42497\n",
      "epoch no.2 train no.190320  loss = 3.70789 avg_loss = 3.39997\n",
      "epoch no.2 train no.190330  loss = 4.07664 avg_loss = 3.40505\n",
      "epoch no.2 train no.190340  loss = 2.48044 avg_loss = 3.34928\n",
      "epoch no.2 train no.190350  loss = 3.52360 avg_loss = 3.36604\n",
      "epoch no.2 train no.190360  loss = 1.80302 avg_loss = 3.37313\n",
      "epoch no.2 train no.190370  loss = 3.14473 avg_loss = 3.31733\n",
      "epoch no.2 train no.190380  loss = 3.05203 avg_loss = 3.38882\n",
      "epoch no.2 train no.190390  loss = 3.26238 avg_loss = 3.37835\n",
      "epoch no.2 train no.190400  loss = 2.53491 avg_loss = 3.37398\n",
      "epoch no.2 train no.190410  loss = 1.93600 avg_loss = 3.42546\n",
      "epoch no.2 train no.190420  loss = 3.43658 avg_loss = 3.48879\n",
      "epoch no.2 train no.190430  loss = 3.33688 avg_loss = 3.43457\n",
      "epoch no.2 train no.190440  loss = 2.36929 avg_loss = 3.35824\n",
      "epoch no.2 train no.190450  loss = 2.87882 avg_loss = 3.38828\n",
      "epoch no.2 train no.190460  loss = 3.15344 avg_loss = 3.38219\n",
      "epoch no.2 train no.190470  loss = 2.71787 avg_loss = 3.41519\n",
      "epoch no.2 train no.190480  loss = 2.75739 avg_loss = 3.40151\n",
      "epoch no.2 train no.190490  loss = 3.38030 avg_loss = 3.42635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.190500  loss = 2.11797 avg_loss = 3.34196\n",
      "epoch no.2 train no.190510  loss = 2.29240 avg_loss = 3.31321\n",
      "epoch no.2 train no.190520  loss = 3.47809 avg_loss = 3.33770\n",
      "epoch no.2 train no.190530  loss = 2.98343 avg_loss = 3.39970\n",
      "epoch no.2 train no.190540  loss = 3.68428 avg_loss = 3.39135\n",
      "epoch no.2 train no.190550  loss = 1.91578 avg_loss = 3.34117\n",
      "epoch no.2 train no.190560  loss = 4.70146 avg_loss = 3.33099\n",
      "epoch no.2 train no.190570  loss = 3.17063 avg_loss = 3.36424\n",
      "epoch no.2 train no.190580  loss = 4.19494 avg_loss = 3.33872\n",
      "epoch no.2 train no.190590  loss = 3.53086 avg_loss = 3.39600\n",
      "epoch no.2 train no.190600  loss = 5.89026 avg_loss = 3.41001\n",
      "epoch no.2 train no.190610  loss = 4.19279 avg_loss = 3.42597\n",
      "epoch no.2 train no.190620  loss = 4.04071 avg_loss = 3.46471\n",
      "epoch no.2 train no.190630  loss = 3.20560 avg_loss = 3.50044\n",
      "epoch no.2 train no.190640  loss = 2.31924 avg_loss = 3.45602\n",
      "epoch no.2 train no.190650  loss = 4.37123 avg_loss = 3.46102\n",
      "epoch no.2 train no.190660  loss = 2.39381 avg_loss = 3.42137\n",
      "epoch no.2 train no.190670  loss = 3.10100 avg_loss = 3.41953\n",
      "epoch no.2 train no.190680  loss = 2.95579 avg_loss = 3.38694\n",
      "epoch no.2 train no.190690  loss = 4.17448 avg_loss = 3.41549\n",
      "epoch no.2 train no.190700  loss = 5.25463 avg_loss = 3.42257\n",
      "epoch no.2 train no.190710  loss = 2.65411 avg_loss = 3.42754\n",
      "epoch no.2 train no.190720  loss = 4.15918 avg_loss = 3.49913\n",
      "epoch no.2 train no.190730  loss = 2.60662 avg_loss = 3.47790\n",
      "epoch no.2 train no.190740  loss = 2.97801 avg_loss = 3.47852\n",
      "epoch no.2 train no.190750  loss = 3.32123 avg_loss = 3.44658\n",
      "epoch no.2 train no.190760  loss = 4.26964 avg_loss = 3.48182\n",
      "epoch no.2 train no.190770  loss = 4.18516 avg_loss = 3.50367\n",
      "epoch no.2 train no.190780  loss = 3.78022 avg_loss = 3.48746\n",
      "epoch no.2 train no.190790  loss = 3.31489 avg_loss = 3.50099\n",
      "epoch no.2 train no.190800  loss = 3.57276 avg_loss = 3.45789\n",
      "epoch no.2 train no.190810  loss = 1.76334 avg_loss = 3.41469\n",
      "epoch no.2 train no.190820  loss = 4.62388 avg_loss = 3.41982\n",
      "epoch no.2 train no.190830  loss = 4.49189 avg_loss = 3.39718\n",
      "epoch no.2 train no.190840  loss = 4.34329 avg_loss = 3.40080\n",
      "epoch no.2 train no.190850  loss = 2.90319 avg_loss = 3.45519\n",
      "epoch no.2 train no.190860  loss = 3.07177 avg_loss = 3.45619\n",
      "epoch no.2 train no.190870  loss = 4.29256 avg_loss = 3.47977\n",
      "epoch no.2 train no.190880  loss = 2.62208 avg_loss = 3.43321\n",
      "epoch no.2 train no.190890  loss = 3.55059 avg_loss = 3.46241\n",
      "epoch no.2 train no.190900  loss = 4.54314 avg_loss = 3.45978\n",
      "epoch no.2 train no.190910  loss = 4.22823 avg_loss = 3.47166\n",
      "epoch no.2 train no.190920  loss = 2.28879 avg_loss = 3.52131\n",
      "epoch no.2 train no.190930  loss = 2.77040 avg_loss = 3.46052\n",
      "epoch no.2 train no.190940  loss = 3.99464 avg_loss = 3.46454\n",
      "epoch no.2 train no.190950  loss = 2.59189 avg_loss = 3.50010\n",
      "epoch no.2 train no.190960  loss = 2.47334 avg_loss = 3.48797\n",
      "epoch no.2 train no.190970  loss = 1.56179 avg_loss = 3.53009\n",
      "epoch no.2 train no.190980  loss = 2.82269 avg_loss = 3.46159\n",
      "epoch no.2 train no.190990  loss = 2.93254 avg_loss = 3.47560\n",
      "epoch no.2 train no.191000  loss = 4.86164 avg_loss = 3.47245\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '적인', '</s>']\n",
      "여름밤에 듣는 감성곡</s>\n",
      "epoch no.2 train no.191010  loss = 2.19518 avg_loss = 3.43212\n",
      "epoch no.2 train no.191020  loss = 3.66203 avg_loss = 3.40878\n",
      "epoch no.2 train no.191030  loss = 5.21255 avg_loss = 3.42929\n",
      "epoch no.2 train no.191040  loss = 4.97855 avg_loss = 3.47027\n",
      "epoch no.2 train no.191050  loss = 3.52097 avg_loss = 3.44209\n",
      "epoch no.2 train no.191060  loss = 3.16788 avg_loss = 3.44521\n",
      "epoch no.2 train no.191070  loss = 2.79499 avg_loss = 3.46616\n",
      "epoch no.2 train no.191080  loss = 2.72884 avg_loss = 3.40561\n",
      "epoch no.2 train no.191090  loss = 4.43672 avg_loss = 3.44047\n",
      "epoch no.2 train no.191100  loss = 3.35469 avg_loss = 3.41653\n",
      "epoch no.2 train no.191110  loss = 3.52715 avg_loss = 3.40971\n",
      "epoch no.2 train no.191120  loss = 3.70532 avg_loss = 3.40776\n",
      "epoch no.2 train no.191130  loss = 1.86036 avg_loss = 3.44047\n",
      "epoch no.2 train no.191140  loss = 4.26846 avg_loss = 3.43255\n",
      "epoch no.2 train no.191150  loss = 1.60432 avg_loss = 3.37276\n",
      "epoch no.2 train no.191160  loss = 2.30338 avg_loss = 3.34981\n",
      "epoch no.2 train no.191170  loss = 1.66622 avg_loss = 3.32800\n",
      "epoch no.2 train no.191180  loss = 2.81930 avg_loss = 3.40704\n",
      "epoch no.2 train no.191190  loss = 2.41326 avg_loss = 3.37037\n",
      "epoch no.2 train no.191200  loss = 3.67270 avg_loss = 3.39438\n",
      "epoch no.2 train no.191210  loss = 3.20898 avg_loss = 3.44796\n",
      "epoch no.2 train no.191220  loss = 3.65109 avg_loss = 3.41499\n",
      "epoch no.2 train no.191230  loss = 3.50640 avg_loss = 3.39976\n",
      "epoch no.2 train no.191240  loss = 3.75062 avg_loss = 3.41338\n",
      "epoch no.2 train no.191250  loss = 5.28349 avg_loss = 3.40690\n",
      "epoch no.2 train no.191260  loss = 5.51254 avg_loss = 3.43955\n",
      "epoch no.2 train no.191270  loss = 3.88491 avg_loss = 3.43283\n",
      "epoch no.2 train no.191280  loss = 2.37827 avg_loss = 3.43193\n",
      "epoch no.2 train no.191290  loss = 5.04124 avg_loss = 3.44168\n",
      "epoch no.2 train no.191300  loss = 1.82068 avg_loss = 3.48181\n",
      "epoch no.2 train no.191310  loss = 4.25596 avg_loss = 3.51655\n",
      "epoch no.2 train no.191320  loss = 3.85696 avg_loss = 3.52003\n",
      "epoch no.2 train no.191330  loss = 4.75322 avg_loss = 3.52573\n",
      "epoch no.2 train no.191340  loss = 4.15669 avg_loss = 3.53713\n",
      "epoch no.2 train no.191350  loss = 1.61883 avg_loss = 3.50177\n",
      "epoch no.2 train no.191360  loss = 3.16626 avg_loss = 3.50642\n",
      "epoch no.2 train no.191370  loss = 1.83852 avg_loss = 3.45135\n",
      "epoch no.2 train no.191380  loss = 3.10875 avg_loss = 3.42696\n",
      "epoch no.2 train no.191390  loss = 4.62107 avg_loss = 3.46229\n",
      "epoch no.2 train no.191400  loss = 3.09255 avg_loss = 3.41529\n",
      "epoch no.2 train no.191410  loss = 4.42914 avg_loss = 3.39676\n",
      "epoch no.2 train no.191420  loss = 5.55503 avg_loss = 3.39254\n",
      "epoch no.2 train no.191430  loss = 3.84298 avg_loss = 3.37572\n",
      "epoch no.2 train no.191440  loss = 2.17247 avg_loss = 3.34761\n",
      "epoch no.2 train no.191450  loss = 2.54557 avg_loss = 3.34172\n",
      "epoch no.2 train no.191460  loss = 2.89835 avg_loss = 3.34484\n",
      "epoch no.2 train no.191470  loss = 5.60932 avg_loss = 3.40403\n",
      "epoch no.2 train no.191480  loss = 2.96581 avg_loss = 3.44707\n",
      "epoch no.2 train no.191490  loss = 3.10188 avg_loss = 3.46867\n",
      "epoch no.2 train no.191500  loss = 2.46157 avg_loss = 3.45278\n",
      "epoch no.2 train no.191510  loss = 2.79077 avg_loss = 3.46242\n",
      "epoch no.2 train no.191520  loss = 3.29124 avg_loss = 3.48010\n",
      "epoch no.2 train no.191530  loss = 4.40924 avg_loss = 3.49177\n",
      "epoch no.2 train no.191540  loss = 4.17522 avg_loss = 3.49264\n",
      "epoch no.2 train no.191550  loss = 3.14500 avg_loss = 3.44591\n",
      "epoch no.2 train no.191560  loss = 4.57323 avg_loss = 3.49306\n",
      "epoch no.2 train no.191570  loss = 2.55344 avg_loss = 3.43818\n",
      "epoch no.2 train no.191580  loss = 5.36866 avg_loss = 3.49240\n",
      "epoch no.2 train no.191590  loss = 2.49981 avg_loss = 3.46414\n",
      "epoch no.2 train no.191600  loss = 3.94407 avg_loss = 3.48983\n",
      "epoch no.2 train no.191610  loss = 3.23811 avg_loss = 3.48152\n",
      "epoch no.2 train no.191620  loss = 2.37225 avg_loss = 3.48292\n",
      "epoch no.2 train no.191630  loss = 4.97156 avg_loss = 3.49139\n",
      "epoch no.2 train no.191640  loss = 5.29165 avg_loss = 3.49474\n",
      "epoch no.2 train no.191650  loss = 2.61458 avg_loss = 3.47849\n",
      "epoch no.2 train no.191660  loss = 4.01724 avg_loss = 3.43756\n",
      "epoch no.2 train no.191670  loss = 3.47029 avg_loss = 3.41046\n",
      "epoch no.2 train no.191680  loss = 4.30132 avg_loss = 3.45153\n",
      "epoch no.2 train no.191690  loss = 4.64992 avg_loss = 3.51501\n",
      "epoch no.2 train no.191700  loss = 4.02233 avg_loss = 3.53727\n",
      "epoch no.2 train no.191710  loss = 4.12525 avg_loss = 3.57510\n",
      "epoch no.2 train no.191720  loss = 2.77865 avg_loss = 3.57916\n",
      "epoch no.2 train no.191730  loss = 3.54659 avg_loss = 3.56179\n",
      "epoch no.2 train no.191740  loss = 2.76427 avg_loss = 3.52408\n",
      "epoch no.2 train no.191750  loss = 4.67593 avg_loss = 3.50485\n",
      "epoch no.2 train no.191760  loss = 4.40971 avg_loss = 3.50858\n",
      "epoch no.2 train no.191770  loss = 2.08028 avg_loss = 3.48395\n",
      "epoch no.2 train no.191780  loss = 4.52104 avg_loss = 3.50552\n",
      "epoch no.2 train no.191790  loss = 3.75838 avg_loss = 3.51837\n",
      "epoch no.2 train no.191800  loss = 2.42627 avg_loss = 3.52124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.191810  loss = 2.61498 avg_loss = 3.50765\n",
      "epoch no.2 train no.191820  loss = 2.93905 avg_loss = 3.48808\n",
      "epoch no.2 train no.191830  loss = 3.68394 avg_loss = 3.51243\n",
      "epoch no.2 train no.191840  loss = 2.96924 avg_loss = 3.53406\n",
      "epoch no.2 train no.191850  loss = 2.39773 avg_loss = 3.53836\n",
      "epoch no.2 train no.191860  loss = 2.84426 avg_loss = 3.49615\n",
      "epoch no.2 train no.191870  loss = 2.42555 avg_loss = 3.44924\n",
      "epoch no.2 train no.191880  loss = 2.98413 avg_loss = 3.42016\n",
      "epoch no.2 train no.191890  loss = 3.65144 avg_loss = 3.42600\n",
      "epoch no.2 train no.191900  loss = 3.54081 avg_loss = 3.41808\n",
      "epoch no.2 train no.191910  loss = 4.24102 avg_loss = 3.37561\n",
      "epoch no.2 train no.191920  loss = 3.10955 avg_loss = 3.32577\n",
      "epoch no.2 train no.191930  loss = 3.59074 avg_loss = 3.31147\n",
      "epoch no.2 train no.191940  loss = 4.45719 avg_loss = 3.43470\n",
      "epoch no.2 train no.191950  loss = 3.93666 avg_loss = 3.48355\n",
      "epoch no.2 train no.191960  loss = 3.27299 avg_loss = 3.43396\n",
      "epoch no.2 train no.191970  loss = 3.86714 avg_loss = 3.47780\n",
      "epoch no.2 train no.191980  loss = 4.28924 avg_loss = 3.48973\n",
      "epoch no.2 train no.191990  loss = 1.95927 avg_loss = 3.46286\n",
      "epoch no.2 train no.192000  loss = 4.23365 avg_loss = 3.49846\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '의', '▁위한', '▁노래', '음악', '</s>']\n",
      "여름밤을 위한 인디음악</s>\n",
      "epoch no.2 train no.192010  loss = 3.45048 avg_loss = 3.49119\n",
      "epoch no.2 train no.192020  loss = 3.65341 avg_loss = 3.49015\n",
      "epoch no.2 train no.192030  loss = 3.53805 avg_loss = 3.47997\n",
      "epoch no.2 train no.192040  loss = 1.07520 avg_loss = 3.42292\n",
      "epoch no.2 train no.192050  loss = 2.40695 avg_loss = 3.46713\n",
      "epoch no.2 train no.192060  loss = 3.52965 avg_loss = 3.49167\n",
      "epoch no.2 train no.192070  loss = 3.43132 avg_loss = 3.46421\n",
      "epoch no.2 train no.192080  loss = 4.47083 avg_loss = 3.45557\n",
      "epoch no.2 train no.192090  loss = 3.38627 avg_loss = 3.49473\n",
      "epoch no.2 train no.192100  loss = 2.80271 avg_loss = 3.49946\n",
      "epoch no.2 train no.192110  loss = 5.00055 avg_loss = 3.52231\n",
      "epoch no.2 train no.192120  loss = 3.44446 avg_loss = 3.49224\n",
      "epoch no.2 train no.192130  loss = 3.44290 avg_loss = 3.49483\n",
      "epoch no.2 train no.192140  loss = 5.19200 avg_loss = 3.51314\n",
      "epoch no.2 train no.192150  loss = 3.56163 avg_loss = 3.52351\n",
      "epoch no.2 train no.192160  loss = 3.18525 avg_loss = 3.54881\n",
      "epoch no.2 train no.192170  loss = 4.01214 avg_loss = 3.54559\n",
      "epoch no.2 train no.192180  loss = 2.56761 avg_loss = 3.51247\n",
      "epoch no.2 train no.192190  loss = 6.26548 avg_loss = 3.56988\n",
      "epoch no.2 train no.192200  loss = 3.58800 avg_loss = 3.59060\n",
      "epoch no.2 train no.192210  loss = 6.19858 avg_loss = 3.55984\n",
      "epoch no.2 train no.192220  loss = 3.46136 avg_loss = 3.56421\n",
      "epoch no.2 train no.192230  loss = 1.97381 avg_loss = 3.49217\n",
      "epoch no.2 train no.192240  loss = 3.06820 avg_loss = 3.46962\n",
      "epoch no.2 train no.192250  loss = 5.52835 avg_loss = 3.47366\n",
      "epoch no.2 train no.192260  loss = 4.18079 avg_loss = 3.48482\n",
      "epoch no.2 train no.192270  loss = 3.39492 avg_loss = 3.46793\n",
      "epoch no.2 train no.192280  loss = 2.52175 avg_loss = 3.49505\n",
      "epoch no.2 train no.192290  loss = 4.81755 avg_loss = 3.50824\n",
      "epoch no.2 train no.192300  loss = 5.32885 avg_loss = 3.44564\n",
      "epoch no.2 train no.192310  loss = 3.16093 avg_loss = 3.41285\n",
      "epoch no.2 train no.192320  loss = 3.10402 avg_loss = 3.45681\n",
      "epoch no.2 train no.192330  loss = 2.68115 avg_loss = 3.40459\n",
      "epoch no.2 train no.192340  loss = 3.75653 avg_loss = 3.39967\n",
      "epoch no.2 train no.192350  loss = 3.27729 avg_loss = 3.39684\n",
      "epoch no.2 train no.192360  loss = 3.06103 avg_loss = 3.41251\n",
      "epoch no.2 train no.192370  loss = 3.40972 avg_loss = 3.41643\n",
      "epoch no.2 train no.192380  loss = 2.19827 avg_loss = 3.39588\n",
      "epoch no.2 train no.192390  loss = 4.20188 avg_loss = 3.40303\n",
      "epoch no.2 train no.192400  loss = 4.98631 avg_loss = 3.45290\n",
      "epoch no.2 train no.192410  loss = 2.48855 avg_loss = 3.44503\n",
      "epoch no.2 train no.192420  loss = 2.25702 avg_loss = 3.43612\n",
      "epoch no.2 train no.192430  loss = 4.36480 avg_loss = 3.47715\n",
      "epoch no.2 train no.192440  loss = 6.57709 avg_loss = 3.48972\n",
      "epoch no.2 train no.192450  loss = 3.49526 avg_loss = 3.48719\n",
      "epoch no.2 train no.192460  loss = 5.12452 avg_loss = 3.50545\n",
      "epoch no.2 train no.192470  loss = 3.65303 avg_loss = 3.50087\n",
      "epoch no.2 train no.192480  loss = 4.15252 avg_loss = 3.49655\n",
      "epoch no.2 train no.192490  loss = 3.30098 avg_loss = 3.51365\n",
      "epoch no.2 train no.192500  loss = 2.51137 avg_loss = 3.46298\n",
      "epoch no.2 train no.192510  loss = 2.85562 avg_loss = 3.50324\n",
      "epoch no.2 train no.192520  loss = 3.49447 avg_loss = 3.55790\n",
      "epoch no.2 train no.192530  loss = 2.74212 avg_loss = 3.59038\n",
      "epoch no.2 train no.192540  loss = 2.09498 avg_loss = 3.58418\n",
      "epoch no.2 train no.192550  loss = 5.38044 avg_loss = 3.63549\n",
      "epoch no.2 train no.192560  loss = 2.47607 avg_loss = 3.58628\n",
      "epoch no.2 train no.192570  loss = 2.50751 avg_loss = 3.63278\n",
      "epoch no.2 train no.192580  loss = 3.41633 avg_loss = 3.61424\n",
      "epoch no.2 train no.192590  loss = 2.82486 avg_loss = 3.53662\n",
      "epoch no.2 train no.192600  loss = 2.54095 avg_loss = 3.57327\n",
      "epoch no.2 train no.192610  loss = 1.72925 avg_loss = 3.53950\n",
      "epoch no.2 train no.192620  loss = 3.58781 avg_loss = 3.58625\n",
      "epoch no.2 train no.192630  loss = 2.57283 avg_loss = 3.59803\n",
      "epoch no.2 train no.192640  loss = 2.97528 avg_loss = 3.57583\n",
      "epoch no.2 train no.192650  loss = 3.66306 avg_loss = 3.52749\n",
      "epoch no.2 train no.192660  loss = 2.07099 avg_loss = 3.52636\n",
      "epoch no.2 train no.192670  loss = 4.76296 avg_loss = 3.54107\n",
      "epoch no.2 train no.192680  loss = 2.96702 avg_loss = 3.45929\n",
      "epoch no.2 train no.192690  loss = 3.07028 avg_loss = 3.43775\n",
      "epoch no.2 train no.192700  loss = 2.53583 avg_loss = 3.47398\n",
      "epoch no.2 train no.192710  loss = 4.02021 avg_loss = 3.46129\n",
      "epoch no.2 train no.192720  loss = 4.70619 avg_loss = 3.47860\n",
      "epoch no.2 train no.192730  loss = 4.20138 avg_loss = 3.54721\n",
      "epoch no.2 train no.192740  loss = 4.04434 avg_loss = 3.59058\n",
      "epoch no.2 train no.192750  loss = 3.79083 avg_loss = 3.62989\n",
      "epoch no.2 train no.192760  loss = 2.11580 avg_loss = 3.62834\n",
      "epoch no.2 train no.192770  loss = 4.22691 avg_loss = 3.61620\n",
      "epoch no.2 train no.192780  loss = 3.03501 avg_loss = 3.66449\n",
      "epoch no.2 train no.192790  loss = 3.98528 avg_loss = 3.68702\n",
      "epoch no.2 train no.192800  loss = 4.10789 avg_loss = 3.67600\n",
      "epoch no.2 train no.192810  loss = 2.62927 avg_loss = 3.68891\n",
      "epoch no.2 train no.192820  loss = 3.72945 avg_loss = 3.64786\n",
      "epoch no.2 train no.192830  loss = 4.81539 avg_loss = 3.65269\n",
      "epoch no.2 train no.192840  loss = 4.45670 avg_loss = 3.65369\n",
      "epoch no.2 train no.192850  loss = 3.58636 avg_loss = 3.63102\n",
      "epoch no.2 train no.192860  loss = 3.37335 avg_loss = 3.64909\n",
      "epoch no.2 train no.192870  loss = 4.57075 avg_loss = 3.64632\n",
      "epoch no.2 train no.192880  loss = 3.25304 avg_loss = 3.60704\n",
      "epoch no.2 train no.192890  loss = 4.23916 avg_loss = 3.57378\n",
      "epoch no.2 train no.192900  loss = 2.55822 avg_loss = 3.58916\n",
      "epoch no.2 train no.192910  loss = 1.44170 avg_loss = 3.56108\n",
      "epoch no.2 train no.192920  loss = 4.83062 avg_loss = 3.53241\n",
      "epoch no.2 train no.192930  loss = 4.39761 avg_loss = 3.56528\n",
      "epoch no.2 train no.192940  loss = 2.92020 avg_loss = 3.58479\n",
      "epoch no.2 train no.192950  loss = 3.77642 avg_loss = 3.54259\n",
      "epoch no.2 train no.192960  loss = 2.65343 avg_loss = 3.57789\n",
      "epoch no.2 train no.192970  loss = 4.84029 avg_loss = 3.55685\n",
      "epoch no.2 train no.192980  loss = 3.67918 avg_loss = 3.58453\n",
      "epoch no.2 train no.192990  loss = 4.05229 avg_loss = 3.57540\n",
      "epoch no.2 train no.193000  loss = 3.93214 avg_loss = 3.56377\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '의', '▁수', '해주는', '▁노래', '발', '라드', '</s>']\n",
      "여름밤을 위로하는 감성발라드</s>\n",
      "epoch no.2 train no.193010  loss = 3.65514 avg_loss = 3.54160\n",
      "epoch no.2 train no.193020  loss = 3.50083 avg_loss = 3.49070\n",
      "epoch no.2 train no.193030  loss = 4.63816 avg_loss = 3.49469\n",
      "epoch no.2 train no.193040  loss = 2.59944 avg_loss = 3.53096\n",
      "epoch no.2 train no.193050  loss = 2.21346 avg_loss = 3.53429\n",
      "epoch no.2 train no.193060  loss = 1.98832 avg_loss = 3.51191\n",
      "epoch no.2 train no.193070  loss = 3.34495 avg_loss = 3.48478\n",
      "epoch no.2 train no.193080  loss = 3.07411 avg_loss = 3.43870\n",
      "epoch no.2 train no.193090  loss = 3.65867 avg_loss = 3.41746\n",
      "epoch no.2 train no.193100  loss = 3.95937 avg_loss = 3.47177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.193110  loss = 2.87156 avg_loss = 3.47381\n",
      "epoch no.2 train no.193120  loss = 3.00772 avg_loss = 3.51642\n",
      "epoch no.2 train no.193130  loss = 2.91430 avg_loss = 3.51469\n",
      "epoch no.2 train no.193140  loss = 3.13006 avg_loss = 3.52488\n",
      "epoch no.2 train no.193150  loss = 3.68609 avg_loss = 3.54219\n",
      "epoch no.2 train no.193160  loss = 4.72396 avg_loss = 3.54236\n",
      "epoch no.2 train no.193170  loss = 2.79826 avg_loss = 3.49778\n",
      "epoch no.2 train no.193180  loss = 2.47891 avg_loss = 3.47897\n",
      "epoch no.2 train no.193190  loss = 2.36248 avg_loss = 3.40968\n",
      "epoch no.2 train no.193200  loss = 3.43816 avg_loss = 3.42024\n",
      "epoch no.2 train no.193210  loss = 2.76009 avg_loss = 3.44258\n",
      "epoch no.2 train no.193220  loss = 3.10403 avg_loss = 3.50254\n",
      "epoch no.2 train no.193230  loss = 3.08546 avg_loss = 3.51067\n",
      "epoch no.2 train no.193240  loss = 3.03007 avg_loss = 3.51990\n",
      "epoch no.2 train no.193250  loss = 1.88598 avg_loss = 3.51470\n",
      "epoch no.2 train no.193260  loss = 2.08601 avg_loss = 3.55844\n",
      "epoch no.2 train no.193270  loss = 5.30329 avg_loss = 3.63183\n",
      "epoch no.2 train no.193280  loss = 3.19670 avg_loss = 3.64962\n",
      "epoch no.2 train no.193290  loss = 3.18016 avg_loss = 3.62728\n",
      "epoch no.2 train no.193300  loss = 4.26256 avg_loss = 3.62321\n",
      "epoch no.2 train no.193310  loss = 3.06733 avg_loss = 3.59621\n",
      "epoch no.2 train no.193320  loss = 4.59493 avg_loss = 3.60042\n",
      "epoch no.2 train no.193330  loss = 3.36830 avg_loss = 3.58528\n",
      "epoch no.2 train no.193340  loss = 4.21060 avg_loss = 3.58197\n",
      "epoch no.2 train no.193350  loss = 5.54992 avg_loss = 3.55400\n",
      "epoch no.2 train no.193360  loss = 2.37434 avg_loss = 3.51862\n",
      "epoch no.2 train no.193370  loss = 3.62341 avg_loss = 3.49822\n",
      "epoch no.2 train no.193380  loss = 4.46392 avg_loss = 3.53053\n",
      "epoch no.2 train no.193390  loss = 3.50297 avg_loss = 3.52118\n",
      "epoch no.2 train no.193400  loss = 3.36921 avg_loss = 3.50922\n",
      "epoch no.2 train no.193410  loss = 2.99435 avg_loss = 3.48242\n",
      "epoch no.2 train no.193420  loss = 2.82653 avg_loss = 3.49026\n",
      "epoch no.2 train no.193430  loss = 3.19468 avg_loss = 3.44546\n",
      "epoch no.2 train no.193440  loss = 3.36024 avg_loss = 3.44161\n",
      "epoch no.2 train no.193450  loss = 2.00169 avg_loss = 3.41876\n",
      "epoch no.2 train no.193460  loss = 4.41158 avg_loss = 3.40377\n",
      "epoch no.2 train no.193470  loss = 2.20029 avg_loss = 3.41531\n",
      "epoch no.2 train no.193480  loss = 3.23923 avg_loss = 3.44206\n",
      "epoch no.2 train no.193490  loss = 3.36261 avg_loss = 3.42180\n",
      "epoch no.2 train no.193500  loss = 3.37120 avg_loss = 3.40771\n",
      "epoch no.2 train no.193510  loss = 3.43916 avg_loss = 3.41090\n",
      "epoch no.2 train no.193520  loss = 2.78660 avg_loss = 3.41114\n",
      "epoch no.2 train no.193530  loss = 2.61845 avg_loss = 3.43418\n",
      "epoch no.2 train no.193540  loss = 1.18295 avg_loss = 3.40196\n",
      "epoch no.2 train no.193550  loss = 3.05121 avg_loss = 3.44876\n",
      "epoch no.2 train no.193560  loss = 3.46655 avg_loss = 3.44902\n",
      "epoch no.2 train no.193570  loss = 4.16786 avg_loss = 3.45870\n",
      "epoch no.2 train no.193580  loss = 3.66721 avg_loss = 3.48099\n",
      "epoch no.2 train no.193590  loss = 2.61973 avg_loss = 3.46292\n",
      "epoch no.2 train no.193600  loss = 4.23624 avg_loss = 3.45950\n",
      "epoch no.2 train no.193610  loss = 4.56383 avg_loss = 3.53685\n",
      "epoch no.2 train no.193620  loss = 3.76013 avg_loss = 3.51212\n",
      "epoch no.2 train no.193630  loss = 2.44485 avg_loss = 3.52303\n",
      "epoch no.2 train no.193640  loss = 4.80900 avg_loss = 3.51783\n",
      "epoch no.2 train no.193650  loss = 1.90472 avg_loss = 3.48181\n",
      "epoch no.2 train no.193660  loss = 4.45283 avg_loss = 3.49518\n",
      "epoch no.2 train no.193670  loss = 2.82533 avg_loss = 3.49711\n",
      "epoch no.2 train no.193680  loss = 4.98628 avg_loss = 3.49563\n",
      "epoch no.2 train no.193690  loss = 2.47820 avg_loss = 3.44357\n",
      "epoch no.2 train no.193700  loss = 4.93637 avg_loss = 3.38133\n",
      "epoch no.2 train no.193710  loss = 3.76585 avg_loss = 3.41170\n",
      "epoch no.2 train no.193720  loss = 3.77224 avg_loss = 3.49788\n",
      "epoch no.2 train no.193730  loss = 2.72915 avg_loss = 3.53978\n",
      "epoch no.2 train no.193740  loss = 3.55970 avg_loss = 3.47246\n",
      "epoch no.2 train no.193750  loss = 4.11318 avg_loss = 3.47223\n",
      "epoch no.2 train no.193760  loss = 4.50330 avg_loss = 3.45820\n",
      "epoch no.2 train no.193770  loss = 5.26587 avg_loss = 3.48989\n",
      "epoch no.2 train no.193780  loss = 4.82372 avg_loss = 3.55343\n",
      "epoch no.2 train no.193790  loss = 2.16063 avg_loss = 3.51117\n",
      "epoch no.2 train no.193800  loss = 3.29482 avg_loss = 3.49479\n",
      "epoch no.2 train no.193810  loss = 3.25945 avg_loss = 3.48976\n",
      "epoch no.2 train no.193820  loss = 3.08223 avg_loss = 3.50791\n",
      "epoch no.2 train no.193830  loss = 3.77785 avg_loss = 3.52976\n",
      "epoch no.2 train no.193840  loss = 3.38000 avg_loss = 3.55242\n",
      "epoch no.2 train no.193850  loss = 5.58042 avg_loss = 3.53325\n",
      "epoch no.2 train no.193860  loss = 3.22101 avg_loss = 3.53219\n",
      "epoch no.2 train no.193870  loss = 4.43907 avg_loss = 3.58818\n",
      "epoch no.2 train no.193880  loss = 3.27932 avg_loss = 3.58709\n",
      "epoch no.2 train no.193890  loss = 4.72102 avg_loss = 3.60014\n",
      "epoch no.2 train no.193900  loss = 2.92237 avg_loss = 3.59796\n",
      "epoch no.2 train no.193910  loss = 3.00551 avg_loss = 3.58883\n",
      "epoch no.2 train no.193920  loss = 4.23790 avg_loss = 3.56093\n",
      "epoch no.2 train no.193930  loss = 4.60124 avg_loss = 3.52802\n",
      "epoch no.2 train no.193940  loss = 2.61934 avg_loss = 3.53184\n",
      "epoch no.2 train no.193950  loss = 3.16270 avg_loss = 3.58126\n",
      "epoch no.2 train no.193960  loss = 1.31134 avg_loss = 3.57827\n",
      "epoch no.2 train no.193970  loss = 3.73870 avg_loss = 3.59606\n",
      "epoch no.2 train no.193980  loss = 4.21978 avg_loss = 3.61430\n",
      "epoch no.2 train no.193990  loss = 4.13071 avg_loss = 3.54342\n",
      "epoch no.2 train no.194000  loss = 4.37847 avg_loss = 3.52078\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '▁어울리는', '▁노래', '▁노래', '피', '컬', '▁하우스', '</s>']\n",
      "여름과 어울리는 신나는 트로피칼 하우스</s>\n",
      "epoch no.2 train no.194010  loss = 2.77773 avg_loss = 3.53162\n",
      "epoch no.2 train no.194020  loss = 4.68330 avg_loss = 3.55449\n",
      "epoch no.2 train no.194030  loss = 3.10925 avg_loss = 3.51546\n",
      "epoch no.2 train no.194040  loss = 4.67348 avg_loss = 3.49520\n",
      "epoch no.2 train no.194050  loss = 3.06031 avg_loss = 3.43923\n",
      "epoch no.2 train no.194060  loss = 4.83984 avg_loss = 3.42444\n",
      "epoch no.2 train no.194070  loss = 2.47539 avg_loss = 3.40798\n",
      "epoch no.2 train no.194080  loss = 4.10433 avg_loss = 3.44221\n",
      "epoch no.2 train no.194090  loss = 1.93698 avg_loss = 3.39658\n",
      "epoch no.2 train no.194100  loss = 4.68433 avg_loss = 3.40283\n",
      "epoch no.2 train no.194110  loss = 2.77741 avg_loss = 3.37590\n",
      "epoch no.2 train no.194120  loss = 2.43968 avg_loss = 3.36024\n",
      "epoch no.2 train no.194130  loss = 2.34596 avg_loss = 3.33553\n",
      "epoch no.2 train no.194140  loss = 3.79772 avg_loss = 3.31391\n",
      "epoch no.2 train no.194150  loss = 4.43275 avg_loss = 3.32618\n",
      "epoch no.2 train no.194160  loss = 3.17205 avg_loss = 3.36449\n",
      "epoch no.2 train no.194170  loss = 2.16223 avg_loss = 3.38710\n",
      "epoch no.2 train no.194180  loss = 4.22155 avg_loss = 3.38767\n",
      "epoch no.2 train no.194190  loss = 2.13293 avg_loss = 3.37671\n",
      "epoch no.2 train no.194200  loss = 1.60994 avg_loss = 3.39854\n",
      "epoch no.2 train no.194210  loss = 3.61765 avg_loss = 3.38976\n",
      "epoch no.2 train no.194220  loss = 5.57118 avg_loss = 3.46399\n",
      "epoch no.2 train no.194230  loss = 3.28740 avg_loss = 3.45022\n",
      "epoch no.2 train no.194240  loss = 3.75009 avg_loss = 3.48350\n",
      "epoch no.2 train no.194250  loss = 3.92469 avg_loss = 3.50614\n",
      "epoch no.2 train no.194260  loss = 1.76407 avg_loss = 3.48893\n",
      "epoch no.2 train no.194270  loss = 3.87818 avg_loss = 3.50621\n",
      "epoch no.2 train no.194280  loss = 4.42566 avg_loss = 3.47298\n",
      "epoch no.2 train no.194290  loss = 3.13325 avg_loss = 3.51000\n",
      "epoch no.2 train no.194300  loss = 4.77319 avg_loss = 3.54494\n",
      "epoch no.2 train no.194310  loss = 4.92664 avg_loss = 3.54302\n",
      "epoch no.2 train no.194320  loss = 3.67104 avg_loss = 3.54434\n",
      "epoch no.2 train no.194330  loss = 4.39497 avg_loss = 3.54108\n",
      "epoch no.2 train no.194340  loss = 3.09787 avg_loss = 3.55369\n",
      "epoch no.2 train no.194350  loss = 1.57812 avg_loss = 3.50376\n",
      "epoch no.2 train no.194360  loss = 4.09745 avg_loss = 3.47849\n",
      "epoch no.2 train no.194370  loss = 1.84882 avg_loss = 3.41760\n",
      "epoch no.2 train no.194380  loss = 4.30709 avg_loss = 3.45052\n",
      "epoch no.2 train no.194390  loss = 1.69449 avg_loss = 3.39828\n",
      "epoch no.2 train no.194400  loss = 4.62550 avg_loss = 3.43275\n",
      "epoch no.2 train no.194410  loss = 4.71495 avg_loss = 3.38240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.194420  loss = 2.78099 avg_loss = 3.36587\n",
      "epoch no.2 train no.194430  loss = 2.42927 avg_loss = 3.35968\n",
      "epoch no.2 train no.194440  loss = 3.05716 avg_loss = 3.33611\n",
      "epoch no.2 train no.194450  loss = 4.26672 avg_loss = 3.37781\n",
      "epoch no.2 train no.194460  loss = 4.78517 avg_loss = 3.48045\n",
      "epoch no.2 train no.194470  loss = 2.13285 avg_loss = 3.50695\n",
      "epoch no.2 train no.194480  loss = 3.16420 avg_loss = 3.45998\n",
      "epoch no.2 train no.194490  loss = 2.87199 avg_loss = 3.45539\n",
      "epoch no.2 train no.194500  loss = 5.78187 avg_loss = 3.49868\n",
      "epoch no.2 train no.194510  loss = 3.51508 avg_loss = 3.50828\n",
      "epoch no.2 train no.194520  loss = 3.81798 avg_loss = 3.53930\n",
      "epoch no.2 train no.194530  loss = 2.54157 avg_loss = 3.55385\n",
      "epoch no.2 train no.194540  loss = 2.36691 avg_loss = 3.55283\n",
      "epoch no.2 train no.194550  loss = 3.05369 avg_loss = 3.52877\n",
      "epoch no.2 train no.194560  loss = 1.73579 avg_loss = 3.49036\n",
      "epoch no.2 train no.194570  loss = 3.20665 avg_loss = 3.48946\n",
      "epoch no.2 train no.194580  loss = 2.77975 avg_loss = 3.46741\n",
      "epoch no.2 train no.194590  loss = 3.37181 avg_loss = 3.46796\n",
      "epoch no.2 train no.194600  loss = 3.16971 avg_loss = 3.44846\n",
      "epoch no.2 train no.194610  loss = 6.04083 avg_loss = 3.43548\n",
      "epoch no.2 train no.194620  loss = 3.31742 avg_loss = 3.45405\n",
      "epoch no.2 train no.194630  loss = 4.20692 avg_loss = 3.43552\n",
      "epoch no.2 train no.194640  loss = 2.17016 avg_loss = 3.41824\n",
      "epoch no.2 train no.194650  loss = 2.34226 avg_loss = 3.40982\n",
      "epoch no.2 train no.194660  loss = 2.77103 avg_loss = 3.43006\n",
      "epoch no.2 train no.194670  loss = 4.39018 avg_loss = 3.45841\n",
      "epoch no.2 train no.194680  loss = 2.51294 avg_loss = 3.45733\n",
      "epoch no.2 train no.194690  loss = 4.57572 avg_loss = 3.46043\n",
      "epoch no.2 train no.194700  loss = 4.23628 avg_loss = 3.49299\n",
      "epoch no.2 train no.194710  loss = 3.59982 avg_loss = 3.48189\n",
      "epoch no.2 train no.194720  loss = 2.86497 avg_loss = 3.46790\n",
      "epoch no.2 train no.194730  loss = 4.70477 avg_loss = 3.48259\n",
      "epoch no.2 train no.194740  loss = 2.83735 avg_loss = 3.49406\n",
      "epoch no.2 train no.194750  loss = 3.10223 avg_loss = 3.49687\n",
      "epoch no.2 train no.194760  loss = 3.64201 avg_loss = 3.53176\n",
      "epoch no.2 train no.194770  loss = 3.27370 avg_loss = 3.51600\n",
      "epoch no.2 train no.194780  loss = 4.61372 avg_loss = 3.50583\n",
      "epoch no.2 train no.194790  loss = 2.13940 avg_loss = 3.53020\n",
      "epoch no.2 train no.194800  loss = 3.13713 avg_loss = 3.48157\n",
      "epoch no.2 train no.194810  loss = 4.50499 avg_loss = 3.52595\n",
      "epoch no.2 train no.194820  loss = 4.81375 avg_loss = 3.48219\n",
      "epoch no.2 train no.194830  loss = 3.45697 avg_loss = 3.46118\n",
      "epoch no.2 train no.194840  loss = 3.95972 avg_loss = 3.44827\n",
      "epoch no.2 train no.194850  loss = 2.06786 avg_loss = 3.46247\n",
      "epoch no.2 train no.194860  loss = 3.69984 avg_loss = 3.42920\n",
      "epoch no.2 train no.194870  loss = 5.70444 avg_loss = 3.48440\n",
      "epoch no.2 train no.194880  loss = 2.66426 avg_loss = 3.52007\n",
      "epoch no.2 train no.194890  loss = 2.80804 avg_loss = 3.49079\n",
      "epoch no.2 train no.194900  loss = 3.92501 avg_loss = 3.46444\n",
      "epoch no.2 train no.194910  loss = 4.02631 avg_loss = 3.43567\n",
      "epoch no.2 train no.194920  loss = 3.39521 avg_loss = 3.42757\n",
      "epoch no.2 train no.194930  loss = 3.79860 avg_loss = 3.45507\n",
      "epoch no.2 train no.194940  loss = 4.63147 avg_loss = 3.49776\n",
      "epoch no.2 train no.194950  loss = 5.16827 avg_loss = 3.50678\n",
      "epoch no.2 train no.194960  loss = 4.46587 avg_loss = 3.47368\n",
      "epoch no.2 train no.194970  loss = 2.82689 avg_loss = 3.48867\n",
      "epoch no.2 train no.194980  loss = 2.92705 avg_loss = 3.48736\n",
      "epoch no.2 train no.194990  loss = 3.78303 avg_loss = 3.51999\n",
      "epoch no.2 train no.195000  loss = 3.15162 avg_loss = 3.53594\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '</s>']\n",
      "여름밤에 어울리는 노래</s>\n",
      "epoch no.2 train no.195010  loss = 2.67320 avg_loss = 3.50579\n",
      "epoch no.2 train no.195020  loss = 4.24809 avg_loss = 3.50277\n",
      "epoch no.2 train no.195030  loss = 3.16064 avg_loss = 3.48850\n",
      "epoch no.2 train no.195040  loss = 6.27999 avg_loss = 3.53712\n",
      "epoch no.2 train no.195050  loss = 2.84232 avg_loss = 3.54461\n",
      "epoch no.2 train no.195060  loss = 3.76237 avg_loss = 3.50560\n",
      "epoch no.2 train no.195070  loss = 4.13120 avg_loss = 3.55399\n",
      "epoch no.2 train no.195080  loss = 2.87916 avg_loss = 3.54625\n",
      "epoch no.2 train no.195090  loss = 3.12771 avg_loss = 3.51103\n",
      "epoch no.2 train no.195100  loss = 1.67535 avg_loss = 3.50273\n",
      "epoch no.2 train no.195110  loss = 5.35681 avg_loss = 3.51041\n",
      "epoch no.2 train no.195120  loss = 5.64729 avg_loss = 3.54935\n",
      "epoch no.2 train no.195130  loss = 4.68517 avg_loss = 3.55701\n",
      "epoch no.2 train no.195140  loss = 3.20397 avg_loss = 3.59648\n",
      "epoch no.2 train no.195150  loss = 3.38810 avg_loss = 3.61248\n",
      "epoch no.2 train no.195160  loss = 2.47012 avg_loss = 3.61131\n",
      "epoch no.2 train no.195170  loss = 3.02887 avg_loss = 3.57488\n",
      "epoch no.2 train no.195180  loss = 2.50896 avg_loss = 3.57338\n",
      "epoch no.2 train no.195190  loss = 6.08783 avg_loss = 3.58445\n",
      "epoch no.2 train no.195200  loss = 6.04579 avg_loss = 3.58359\n",
      "epoch no.2 train no.195210  loss = 2.85751 avg_loss = 3.50537\n",
      "epoch no.2 train no.195220  loss = 4.50916 avg_loss = 3.55975\n",
      "epoch no.2 train no.195230  loss = 4.02813 avg_loss = 3.50677\n",
      "epoch no.2 train no.195240  loss = 2.70437 avg_loss = 3.52591\n",
      "epoch no.2 train no.195250  loss = 5.34761 avg_loss = 3.57544\n",
      "epoch no.2 train no.195260  loss = 2.94244 avg_loss = 3.53411\n",
      "epoch no.2 train no.195270  loss = 3.82625 avg_loss = 3.50524\n",
      "epoch no.2 train no.195280  loss = 3.49882 avg_loss = 3.49174\n",
      "epoch no.2 train no.195290  loss = 4.28749 avg_loss = 3.53461\n",
      "epoch no.2 train no.195300  loss = 2.89079 avg_loss = 3.54977\n",
      "epoch no.2 train no.195310  loss = 1.68427 avg_loss = 3.49430\n",
      "epoch no.2 train no.195320  loss = 3.51669 avg_loss = 3.52553\n",
      "epoch no.2 train no.195330  loss = 2.88731 avg_loss = 3.51704\n",
      "epoch no.2 train no.195340  loss = 3.26273 avg_loss = 3.57159\n",
      "epoch no.2 train no.195350  loss = 2.06126 avg_loss = 3.48338\n",
      "epoch no.2 train no.195360  loss = 3.29433 avg_loss = 3.51134\n",
      "epoch no.2 train no.195370  loss = 4.07735 avg_loss = 3.54320\n",
      "epoch no.2 train no.195380  loss = 3.27380 avg_loss = 3.54630\n",
      "epoch no.2 train no.195390  loss = 4.85873 avg_loss = 3.51671\n",
      "epoch no.2 train no.195400  loss = 4.92329 avg_loss = 3.53018\n",
      "epoch no.2 train no.195410  loss = 2.52497 avg_loss = 3.50572\n",
      "epoch no.2 train no.195420  loss = 2.70823 avg_loss = 3.51671\n",
      "epoch no.2 train no.195430  loss = 2.06573 avg_loss = 3.48977\n",
      "epoch no.2 train no.195440  loss = 3.52178 avg_loss = 3.49299\n",
      "epoch no.2 train no.195450  loss = 4.16174 avg_loss = 3.52276\n",
      "epoch no.2 train no.195460  loss = 3.05657 avg_loss = 3.54563\n",
      "epoch no.2 train no.195470  loss = 3.34000 avg_loss = 3.53099\n",
      "epoch no.2 train no.195480  loss = 6.68102 avg_loss = 3.54789\n",
      "epoch no.2 train no.195490  loss = 3.14799 avg_loss = 3.59029\n",
      "epoch no.2 train no.195500  loss = 4.32280 avg_loss = 3.57213\n",
      "epoch no.2 train no.195510  loss = 3.29347 avg_loss = 3.60208\n",
      "epoch no.2 train no.195520  loss = 2.98476 avg_loss = 3.55012\n",
      "epoch no.2 train no.195530  loss = 4.12576 avg_loss = 3.53434\n",
      "epoch no.2 train no.195540  loss = 3.05915 avg_loss = 3.52176\n",
      "epoch no.2 train no.195550  loss = 2.54750 avg_loss = 3.57953\n",
      "epoch no.2 train no.195560  loss = 3.33102 avg_loss = 3.52853\n",
      "epoch no.2 train no.195570  loss = 2.01417 avg_loss = 3.48495\n",
      "epoch no.2 train no.195580  loss = 2.83163 avg_loss = 3.46650\n",
      "epoch no.2 train no.195590  loss = 3.82038 avg_loss = 3.51371\n",
      "epoch no.2 train no.195600  loss = 2.68098 avg_loss = 3.50557\n",
      "epoch no.2 train no.195610  loss = 2.56738 avg_loss = 3.47566\n",
      "epoch no.2 train no.195620  loss = 2.54225 avg_loss = 3.40504\n",
      "epoch no.2 train no.195630  loss = 4.66565 avg_loss = 3.42963\n",
      "epoch no.2 train no.195640  loss = 4.57367 avg_loss = 3.42087\n",
      "epoch no.2 train no.195650  loss = 2.68728 avg_loss = 3.43785\n",
      "epoch no.2 train no.195660  loss = 2.51504 avg_loss = 3.43482\n",
      "epoch no.2 train no.195670  loss = 3.30007 avg_loss = 3.46192\n",
      "epoch no.2 train no.195680  loss = 3.11460 avg_loss = 3.47797\n",
      "epoch no.2 train no.195690  loss = 2.69386 avg_loss = 3.49881\n",
      "epoch no.2 train no.195700  loss = 3.92020 avg_loss = 3.48116\n",
      "epoch no.2 train no.195710  loss = 3.51686 avg_loss = 3.45034\n",
      "epoch no.2 train no.195720  loss = 2.15984 avg_loss = 3.42103\n",
      "epoch no.2 train no.195730  loss = 2.81997 avg_loss = 3.42717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.195740  loss = 3.43886 avg_loss = 3.41701\n",
      "epoch no.2 train no.195750  loss = 2.89705 avg_loss = 3.44332\n",
      "epoch no.2 train no.195760  loss = 2.26981 avg_loss = 3.47441\n",
      "epoch no.2 train no.195770  loss = 5.02437 avg_loss = 3.47376\n",
      "epoch no.2 train no.195780  loss = 5.39472 avg_loss = 3.50871\n",
      "epoch no.2 train no.195790  loss = 3.19492 avg_loss = 3.51637\n",
      "epoch no.2 train no.195800  loss = 3.68710 avg_loss = 3.48144\n",
      "epoch no.2 train no.195810  loss = 3.44725 avg_loss = 3.47768\n",
      "epoch no.2 train no.195820  loss = 3.53813 avg_loss = 3.50132\n",
      "epoch no.2 train no.195830  loss = 4.01913 avg_loss = 3.48591\n",
      "epoch no.2 train no.195840  loss = 2.71519 avg_loss = 3.45531\n",
      "epoch no.2 train no.195850  loss = 3.98891 avg_loss = 3.48109\n",
      "epoch no.2 train no.195860  loss = 2.78432 avg_loss = 3.53794\n",
      "epoch no.2 train no.195870  loss = 4.00958 avg_loss = 3.51947\n",
      "epoch no.2 train no.195880  loss = 5.67962 avg_loss = 3.56106\n",
      "epoch no.2 train no.195890  loss = 2.47550 avg_loss = 3.54424\n",
      "epoch no.2 train no.195900  loss = 2.73397 avg_loss = 3.53206\n",
      "epoch no.2 train no.195910  loss = 5.08156 avg_loss = 3.54113\n",
      "epoch no.2 train no.195920  loss = 3.79254 avg_loss = 3.54384\n",
      "epoch no.2 train no.195930  loss = 3.97076 avg_loss = 3.62136\n",
      "epoch no.2 train no.195940  loss = 3.86135 avg_loss = 3.63574\n",
      "epoch no.2 train no.195950  loss = 4.63705 avg_loss = 3.65837\n",
      "epoch no.2 train no.195960  loss = 3.31423 avg_loss = 3.65278\n",
      "epoch no.2 train no.195970  loss = 1.62484 avg_loss = 3.60945\n",
      "epoch no.2 train no.195980  loss = 3.48590 avg_loss = 3.56278\n",
      "epoch no.2 train no.195990  loss = 3.61490 avg_loss = 3.56942\n",
      "epoch no.2 train no.196000  loss = 3.80252 avg_loss = 3.56425\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁플레이', '▁락', '▁음악', '</s>']\n",
      "여름밤의 시원한 드라이브 뮤직</s>\n",
      "epoch no.2 train no.196010  loss = 2.02986 avg_loss = 3.55295\n",
      "epoch no.2 train no.196020  loss = 4.01752 avg_loss = 3.54935\n",
      "epoch no.2 train no.196030  loss = 3.58898 avg_loss = 3.59024\n",
      "epoch no.2 train no.196040  loss = 2.74601 avg_loss = 3.56566\n",
      "epoch no.2 train no.196050  loss = 4.12344 avg_loss = 3.63164\n",
      "epoch no.2 train no.196060  loss = 2.03932 avg_loss = 3.59579\n",
      "epoch no.2 train no.196070  loss = 2.12188 avg_loss = 3.54960\n",
      "epoch no.2 train no.196080  loss = 4.36078 avg_loss = 3.50244\n",
      "epoch no.2 train no.196090  loss = 2.65346 avg_loss = 3.52866\n",
      "epoch no.2 train no.196100  loss = 3.63275 avg_loss = 3.53967\n",
      "epoch no.2 train no.196110  loss = 2.63632 avg_loss = 3.55978\n",
      "epoch no.2 train no.196120  loss = 4.18842 avg_loss = 3.52120\n",
      "epoch no.2 train no.196130  loss = 2.99732 avg_loss = 3.47271\n",
      "epoch no.2 train no.196140  loss = 3.47334 avg_loss = 3.44018\n",
      "epoch no.2 train no.196150  loss = 3.47826 avg_loss = 3.49868\n",
      "epoch no.2 train no.196160  loss = 3.37838 avg_loss = 3.54820\n",
      "epoch no.2 train no.196170  loss = 5.29601 avg_loss = 3.58138\n",
      "epoch no.2 train no.196180  loss = 3.96344 avg_loss = 3.56364\n",
      "epoch no.2 train no.196190  loss = 3.42862 avg_loss = 3.50575\n",
      "epoch no.2 train no.196200  loss = 2.64493 avg_loss = 3.44957\n",
      "epoch no.2 train no.196210  loss = 2.54154 avg_loss = 3.50561\n",
      "epoch no.2 train no.196220  loss = 3.60517 avg_loss = 3.46032\n",
      "epoch no.2 train no.196230  loss = 2.22077 avg_loss = 3.44604\n",
      "epoch no.2 train no.196240  loss = 3.25156 avg_loss = 3.45316\n",
      "epoch no.2 train no.196250  loss = 4.83801 avg_loss = 3.44373\n",
      "epoch no.2 train no.196260  loss = 3.23544 avg_loss = 3.40912\n",
      "epoch no.2 train no.196270  loss = 3.85499 avg_loss = 3.43508\n",
      "epoch no.2 train no.196280  loss = 3.06841 avg_loss = 3.43203\n",
      "epoch no.2 train no.196290  loss = 3.20284 avg_loss = 3.43773\n",
      "epoch no.2 train no.196300  loss = 1.59926 avg_loss = 3.40876\n",
      "epoch no.2 train no.196310  loss = 2.68117 avg_loss = 3.48957\n",
      "epoch no.2 train no.196320  loss = 4.70306 avg_loss = 3.48638\n",
      "epoch no.2 train no.196330  loss = 2.69965 avg_loss = 3.48085\n",
      "epoch no.2 train no.196340  loss = 4.86602 avg_loss = 3.50917\n",
      "epoch no.2 train no.196350  loss = 2.96737 avg_loss = 3.51767\n",
      "epoch no.2 train no.196360  loss = 4.61980 avg_loss = 3.48098\n",
      "epoch no.2 train no.196370  loss = 2.44165 avg_loss = 3.49910\n",
      "epoch no.2 train no.196380  loss = 3.58077 avg_loss = 3.48577\n",
      "epoch no.2 train no.196390  loss = 1.91174 avg_loss = 3.47404\n",
      "epoch no.2 train no.196400  loss = 2.50720 avg_loss = 3.42795\n",
      "epoch no.2 train no.196410  loss = 3.36533 avg_loss = 3.42846\n",
      "epoch no.2 train no.196420  loss = 4.18464 avg_loss = 3.47055\n",
      "epoch no.2 train no.196430  loss = 2.96930 avg_loss = 3.45511\n",
      "epoch no.2 train no.196440  loss = 2.79122 avg_loss = 3.43414\n",
      "epoch no.2 train no.196450  loss = 3.75250 avg_loss = 3.44073\n",
      "epoch no.2 train no.196460  loss = 2.79981 avg_loss = 3.44783\n",
      "epoch no.2 train no.196470  loss = 3.02099 avg_loss = 3.44028\n",
      "epoch no.2 train no.196480  loss = 3.73985 avg_loss = 3.40988\n",
      "epoch no.2 train no.196490  loss = 4.23259 avg_loss = 3.45166\n",
      "epoch no.2 train no.196500  loss = 3.19321 avg_loss = 3.44134\n",
      "epoch no.2 train no.196510  loss = 2.29289 avg_loss = 3.43958\n",
      "epoch no.2 train no.196520  loss = 5.22749 avg_loss = 3.45383\n",
      "epoch no.2 train no.196530  loss = 4.16832 avg_loss = 3.48251\n",
      "epoch no.2 train no.196540  loss = 3.01837 avg_loss = 3.46954\n",
      "epoch no.2 train no.196550  loss = 4.15596 avg_loss = 3.45189\n",
      "epoch no.2 train no.196560  loss = 4.33558 avg_loss = 3.44496\n",
      "epoch no.2 train no.196570  loss = 4.14395 avg_loss = 3.45715\n",
      "epoch no.2 train no.196580  loss = 4.18705 avg_loss = 3.48752\n",
      "epoch no.2 train no.196590  loss = 1.69684 avg_loss = 3.46320\n",
      "epoch no.2 train no.196600  loss = 3.39192 avg_loss = 3.47390\n",
      "epoch no.2 train no.196610  loss = 3.51723 avg_loss = 3.44855\n",
      "epoch no.2 train no.196620  loss = 4.23812 avg_loss = 3.44469\n",
      "epoch no.2 train no.196630  loss = 2.89859 avg_loss = 3.44005\n",
      "epoch no.2 train no.196640  loss = 3.05761 avg_loss = 3.47594\n",
      "epoch no.2 train no.196650  loss = 3.61843 avg_loss = 3.47124\n",
      "epoch no.2 train no.196660  loss = 2.53301 avg_loss = 3.42724\n",
      "epoch no.2 train no.196670  loss = 2.95107 avg_loss = 3.41606\n",
      "epoch no.2 train no.196680  loss = 3.74905 avg_loss = 3.46145\n",
      "epoch no.2 train no.196690  loss = 2.73812 avg_loss = 3.47591\n",
      "epoch no.2 train no.196700  loss = 3.42104 avg_loss = 3.50697\n",
      "epoch no.2 train no.196710  loss = 3.06834 avg_loss = 3.47728\n",
      "epoch no.2 train no.196720  loss = 4.83887 avg_loss = 3.51611\n",
      "epoch no.2 train no.196730  loss = 3.86247 avg_loss = 3.49345\n",
      "epoch no.2 train no.196740  loss = 2.56517 avg_loss = 3.47890\n",
      "epoch no.2 train no.196750  loss = 3.82348 avg_loss = 3.46733\n",
      "epoch no.2 train no.196760  loss = 3.44248 avg_loss = 3.46176\n",
      "epoch no.2 train no.196770  loss = 3.82844 avg_loss = 3.47844\n",
      "epoch no.2 train no.196780  loss = 4.84187 avg_loss = 3.47928\n",
      "epoch no.2 train no.196790  loss = 2.65999 avg_loss = 3.44810\n",
      "epoch no.2 train no.196800  loss = 4.60278 avg_loss = 3.48580\n",
      "epoch no.2 train no.196810  loss = 4.04842 avg_loss = 3.52580\n",
      "epoch no.2 train no.196820  loss = 2.82476 avg_loss = 3.53376\n",
      "epoch no.2 train no.196830  loss = 3.25406 avg_loss = 3.55649\n",
      "epoch no.2 train no.196840  loss = 4.87369 avg_loss = 3.51660\n",
      "epoch no.2 train no.196850  loss = 5.47973 avg_loss = 3.55831\n",
      "epoch no.2 train no.196860  loss = 3.05919 avg_loss = 3.54629\n",
      "epoch no.2 train no.196870  loss = 2.03743 avg_loss = 3.56056\n",
      "epoch no.2 train no.196880  loss = 4.48589 avg_loss = 3.57841\n",
      "epoch no.2 train no.196890  loss = 4.09744 avg_loss = 3.59375\n",
      "epoch no.2 train no.196900  loss = 5.34334 avg_loss = 3.60497\n",
      "epoch no.2 train no.196910  loss = 5.98568 avg_loss = 3.67440\n",
      "epoch no.2 train no.196920  loss = 2.66024 avg_loss = 3.64213\n",
      "epoch no.2 train no.196930  loss = 3.19487 avg_loss = 3.65339\n",
      "epoch no.2 train no.196940  loss = 2.75496 avg_loss = 3.60822\n",
      "epoch no.2 train no.196950  loss = 3.73465 avg_loss = 3.66770\n",
      "epoch no.2 train no.196960  loss = 4.09782 avg_loss = 3.67171\n",
      "epoch no.2 train no.196970  loss = 2.54871 avg_loss = 3.65832\n",
      "epoch no.2 train no.196980  loss = 3.35640 avg_loss = 3.65998\n",
      "epoch no.2 train no.196990  loss = 3.26202 avg_loss = 3.61749\n",
      "epoch no.2 train no.197000  loss = 2.11678 avg_loss = 3.57318\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '</s>']\n",
      "여름밤에 듣는 노래</s>\n",
      "epoch no.2 train no.197010  loss = 4.73644 avg_loss = 3.59261\n",
      "epoch no.2 train no.197020  loss = 5.33269 avg_loss = 3.62589\n",
      "epoch no.2 train no.197030  loss = 2.65561 avg_loss = 3.64496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.197040  loss = 3.20946 avg_loss = 3.65013\n",
      "epoch no.2 train no.197050  loss = 3.46792 avg_loss = 3.64206\n",
      "epoch no.2 train no.197060  loss = 3.69545 avg_loss = 3.61468\n",
      "epoch no.2 train no.197070  loss = 3.04501 avg_loss = 3.55299\n",
      "epoch no.2 train no.197080  loss = 3.98467 avg_loss = 3.52741\n",
      "epoch no.2 train no.197090  loss = 3.28346 avg_loss = 3.57481\n",
      "epoch no.2 train no.197100  loss = 1.01940 avg_loss = 3.53579\n",
      "epoch no.2 train no.197110  loss = 3.66410 avg_loss = 3.54206\n",
      "epoch no.2 train no.197120  loss = 4.78990 avg_loss = 3.57698\n",
      "epoch no.2 train no.197130  loss = 2.62891 avg_loss = 3.57455\n",
      "epoch no.2 train no.197140  loss = 3.05380 avg_loss = 3.57185\n",
      "epoch no.2 train no.197150  loss = 3.86263 avg_loss = 3.53346\n",
      "epoch no.2 train no.197160  loss = 4.34511 avg_loss = 3.57315\n",
      "epoch no.2 train no.197170  loss = 4.22182 avg_loss = 3.55331\n",
      "epoch no.2 train no.197180  loss = 1.98680 avg_loss = 3.53892\n",
      "epoch no.2 train no.197190  loss = 2.78963 avg_loss = 3.54145\n",
      "epoch no.2 train no.197200  loss = 2.79096 avg_loss = 3.49049\n",
      "epoch no.2 train no.197210  loss = 3.38650 avg_loss = 3.47234\n",
      "epoch no.2 train no.197220  loss = 2.86794 avg_loss = 3.50229\n",
      "epoch no.2 train no.197230  loss = 1.71866 avg_loss = 3.43975\n",
      "epoch no.2 train no.197240  loss = 2.88327 avg_loss = 3.39803\n",
      "epoch no.2 train no.197250  loss = 4.26679 avg_loss = 3.42287\n",
      "epoch no.2 train no.197260  loss = 3.03263 avg_loss = 3.46575\n",
      "epoch no.2 train no.197270  loss = 3.93395 avg_loss = 3.49277\n",
      "epoch no.2 train no.197280  loss = 2.20125 avg_loss = 3.49009\n",
      "epoch no.2 train no.197290  loss = 3.67718 avg_loss = 3.50804\n",
      "epoch no.2 train no.197300  loss = 4.97730 avg_loss = 3.52569\n",
      "epoch no.2 train no.197310  loss = 3.54912 avg_loss = 3.57147\n",
      "epoch no.2 train no.197320  loss = 4.27972 avg_loss = 3.58502\n",
      "epoch no.2 train no.197330  loss = 5.68184 avg_loss = 3.58997\n",
      "epoch no.2 train no.197340  loss = 2.27921 avg_loss = 3.57002\n",
      "epoch no.2 train no.197350  loss = 3.24112 avg_loss = 3.56490\n",
      "epoch no.2 train no.197360  loss = 2.78734 avg_loss = 3.51940\n",
      "epoch no.2 train no.197370  loss = 6.98096 avg_loss = 3.55563\n",
      "epoch no.2 train no.197380  loss = 4.98416 avg_loss = 3.60651\n",
      "epoch no.2 train no.197390  loss = 4.50552 avg_loss = 3.58421\n",
      "epoch no.2 train no.197400  loss = 2.33748 avg_loss = 3.55422\n",
      "epoch no.2 train no.197410  loss = 1.58061 avg_loss = 3.49568\n",
      "epoch no.2 train no.197420  loss = 3.18684 avg_loss = 3.50504\n",
      "epoch no.2 train no.197430  loss = 3.26999 avg_loss = 3.49062\n",
      "epoch no.2 train no.197440  loss = 4.00697 avg_loss = 3.48601\n",
      "epoch no.2 train no.197450  loss = 2.34567 avg_loss = 3.45632\n",
      "epoch no.2 train no.197460  loss = 2.56531 avg_loss = 3.48655\n",
      "epoch no.2 train no.197470  loss = 4.60797 avg_loss = 3.50354\n",
      "epoch no.2 train no.197480  loss = 1.69188 avg_loss = 3.45199\n",
      "epoch no.2 train no.197490  loss = 3.17594 avg_loss = 3.46955\n",
      "epoch no.2 train no.197500  loss = 5.40687 avg_loss = 3.47545\n",
      "epoch no.2 train no.197510  loss = 2.91553 avg_loss = 3.50760\n",
      "epoch no.2 train no.197520  loss = 2.93354 avg_loss = 3.55277\n",
      "epoch no.2 train no.197530  loss = 2.93111 avg_loss = 3.60529\n",
      "epoch no.2 train no.197540  loss = 4.22896 avg_loss = 3.58235\n",
      "epoch no.2 train no.197550  loss = 4.10519 avg_loss = 3.57362\n",
      "epoch no.2 train no.197560  loss = 1.83215 avg_loss = 3.52336\n",
      "epoch no.2 train no.197570  loss = 4.41885 avg_loss = 3.53297\n",
      "epoch no.2 train no.197580  loss = 3.50421 avg_loss = 3.52841\n",
      "epoch no.2 train no.197590  loss = 3.63670 avg_loss = 3.54697\n",
      "epoch no.2 train no.197600  loss = 5.07248 avg_loss = 3.52575\n",
      "epoch no.2 train no.197610  loss = 3.91696 avg_loss = 3.53690\n",
      "epoch no.2 train no.197620  loss = 3.18039 avg_loss = 3.50100\n",
      "epoch no.2 train no.197630  loss = 3.70755 avg_loss = 3.45601\n",
      "epoch no.2 train no.197640  loss = 2.70960 avg_loss = 3.48957\n",
      "epoch no.2 train no.197650  loss = 3.82000 avg_loss = 3.50613\n",
      "epoch no.2 train no.197660  loss = 4.17619 avg_loss = 3.50864\n",
      "epoch no.2 train no.197670  loss = 3.37046 avg_loss = 3.53629\n",
      "epoch no.2 train no.197680  loss = 3.09886 avg_loss = 3.51443\n",
      "epoch no.2 train no.197690  loss = 3.96032 avg_loss = 3.58370\n",
      "epoch no.2 train no.197700  loss = 2.23922 avg_loss = 3.56419\n",
      "epoch no.2 train no.197710  loss = 3.09130 avg_loss = 3.54151\n",
      "epoch no.2 train no.197720  loss = 3.83679 avg_loss = 3.51781\n",
      "epoch no.2 train no.197730  loss = 3.34310 avg_loss = 3.49768\n",
      "epoch no.2 train no.197740  loss = 3.37736 avg_loss = 3.52113\n",
      "epoch no.2 train no.197750  loss = 2.49750 avg_loss = 3.48836\n",
      "epoch no.2 train no.197760  loss = 2.91480 avg_loss = 3.48557\n",
      "epoch no.2 train no.197770  loss = 3.83334 avg_loss = 3.46344\n",
      "epoch no.2 train no.197780  loss = 4.14173 avg_loss = 3.47323\n",
      "epoch no.2 train no.197790  loss = 4.95174 avg_loss = 3.46654\n",
      "epoch no.2 train no.197800  loss = 3.01274 avg_loss = 3.49083\n",
      "epoch no.2 train no.197810  loss = 2.95607 avg_loss = 3.52356\n",
      "epoch no.2 train no.197820  loss = 2.72735 avg_loss = 3.49801\n",
      "epoch no.2 train no.197830  loss = 2.51795 avg_loss = 3.47755\n",
      "epoch no.2 train no.197840  loss = 4.73254 avg_loss = 3.49059\n",
      "epoch no.2 train no.197850  loss = 4.73328 avg_loss = 3.43692\n",
      "epoch no.2 train no.197860  loss = 5.91475 avg_loss = 3.48730\n",
      "epoch no.2 train no.197870  loss = 3.74398 avg_loss = 3.47668\n",
      "epoch no.2 train no.197880  loss = 2.96397 avg_loss = 3.43275\n",
      "epoch no.2 train no.197890  loss = 3.57208 avg_loss = 3.47201\n",
      "epoch no.2 train no.197900  loss = 5.24882 avg_loss = 3.54033\n",
      "epoch no.2 train no.197910  loss = 2.21094 avg_loss = 3.53098\n",
      "epoch no.2 train no.197920  loss = 4.10476 avg_loss = 3.51450\n",
      "epoch no.2 train no.197930  loss = 1.50926 avg_loss = 3.51590\n",
      "epoch no.2 train no.197940  loss = 3.69180 avg_loss = 3.52840\n",
      "epoch no.2 train no.197950  loss = 3.12417 avg_loss = 3.50097\n",
      "epoch no.2 train no.197960  loss = 4.38004 avg_loss = 3.46852\n",
      "epoch no.2 train no.197970  loss = 2.93276 avg_loss = 3.43114\n",
      "epoch no.2 train no.197980  loss = 4.25147 avg_loss = 3.43344\n",
      "epoch no.2 train no.197990  loss = 2.98106 avg_loss = 3.41286\n",
      "epoch no.2 train no.198000  loss = 3.04109 avg_loss = 3.44676\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 음악</s>\n",
      "epoch no.2 train no.198010  loss = 3.78815 avg_loss = 3.49544\n",
      "epoch no.2 train no.198020  loss = 2.64205 avg_loss = 3.43596\n",
      "epoch no.2 train no.198030  loss = 3.85462 avg_loss = 3.42338\n",
      "epoch no.2 train no.198040  loss = 4.59623 avg_loss = 3.42119\n",
      "epoch no.2 train no.198050  loss = 3.42301 avg_loss = 3.43105\n",
      "epoch no.2 train no.198060  loss = 4.05174 avg_loss = 3.44724\n",
      "epoch no.2 train no.198070  loss = 3.97809 avg_loss = 3.45537\n",
      "epoch no.2 train no.198080  loss = 2.26583 avg_loss = 3.40376\n",
      "epoch no.2 train no.198090  loss = 3.62861 avg_loss = 3.35917\n",
      "epoch no.2 train no.198100  loss = 3.28839 avg_loss = 3.34868\n",
      "epoch no.2 train no.198110  loss = 4.15443 avg_loss = 3.44083\n",
      "epoch no.2 train no.198120  loss = 2.21238 avg_loss = 3.40973\n",
      "epoch no.2 train no.198130  loss = 6.48538 avg_loss = 3.46349\n",
      "epoch no.2 train no.198140  loss = 7.15391 avg_loss = 3.47303\n",
      "epoch no.2 train no.198150  loss = 2.47120 avg_loss = 3.47573\n",
      "epoch no.2 train no.198160  loss = 5.92644 avg_loss = 3.46759\n",
      "epoch no.2 train no.198170  loss = 4.79189 avg_loss = 3.48562\n",
      "epoch no.2 train no.198180  loss = 2.86412 avg_loss = 3.48220\n",
      "epoch no.2 train no.198190  loss = 2.25050 avg_loss = 3.43530\n",
      "epoch no.2 train no.198200  loss = 3.80416 avg_loss = 3.44054\n",
      "epoch no.2 train no.198210  loss = 3.53621 avg_loss = 3.41839\n",
      "epoch no.2 train no.198220  loss = 5.90473 avg_loss = 3.43296\n",
      "epoch no.2 train no.198230  loss = 3.50736 avg_loss = 3.44767\n",
      "epoch no.2 train no.198240  loss = 1.96478 avg_loss = 3.49388\n",
      "epoch no.2 train no.198250  loss = 3.91860 avg_loss = 3.49546\n",
      "epoch no.2 train no.198260  loss = 3.21137 avg_loss = 3.51877\n",
      "epoch no.2 train no.198270  loss = 2.02207 avg_loss = 3.48705\n",
      "epoch no.2 train no.198280  loss = 2.99807 avg_loss = 3.50285\n",
      "epoch no.2 train no.198290  loss = 3.09064 avg_loss = 3.51333\n",
      "epoch no.2 train no.198300  loss = 3.27773 avg_loss = 3.52037\n",
      "epoch no.2 train no.198310  loss = 2.71908 avg_loss = 3.49908\n",
      "epoch no.2 train no.198320  loss = 3.47844 avg_loss = 3.52884\n",
      "epoch no.2 train no.198330  loss = 2.68371 avg_loss = 3.53112\n",
      "epoch no.2 train no.198340  loss = 1.97929 avg_loss = 3.49964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.198350  loss = 4.35939 avg_loss = 3.52712\n",
      "epoch no.2 train no.198360  loss = 4.90699 avg_loss = 3.57745\n",
      "epoch no.2 train no.198370  loss = 2.96395 avg_loss = 3.58600\n",
      "epoch no.2 train no.198380  loss = 2.82477 avg_loss = 3.56094\n",
      "epoch no.2 train no.198390  loss = 4.05255 avg_loss = 3.51974\n",
      "epoch no.2 train no.198400  loss = 4.07991 avg_loss = 3.54862\n",
      "epoch no.2 train no.198410  loss = 4.55343 avg_loss = 3.52463\n",
      "epoch no.2 train no.198420  loss = 3.80352 avg_loss = 3.52060\n",
      "epoch no.2 train no.198430  loss = 3.07491 avg_loss = 3.51310\n",
      "epoch no.2 train no.198440  loss = 5.59053 avg_loss = 3.52310\n",
      "epoch no.2 train no.198450  loss = 4.97180 avg_loss = 3.50442\n",
      "epoch no.2 train no.198460  loss = 2.50151 avg_loss = 3.51500\n",
      "epoch no.2 train no.198470  loss = 2.64554 avg_loss = 3.45802\n",
      "epoch no.2 train no.198480  loss = 3.96324 avg_loss = 3.53656\n",
      "epoch no.2 train no.198490  loss = 2.83562 avg_loss = 3.52264\n",
      "epoch no.2 train no.198500  loss = 6.33673 avg_loss = 3.56041\n",
      "epoch no.2 train no.198510  loss = 2.67986 avg_loss = 3.48751\n",
      "epoch no.2 train no.198520  loss = 4.00549 avg_loss = 3.48969\n",
      "epoch no.2 train no.198530  loss = 2.52704 avg_loss = 3.51407\n",
      "epoch no.2 train no.198540  loss = 2.98572 avg_loss = 3.52052\n",
      "epoch no.2 train no.198550  loss = 2.83158 avg_loss = 3.52685\n",
      "epoch no.2 train no.198560  loss = 3.33562 avg_loss = 3.51509\n",
      "epoch no.2 train no.198570  loss = 3.33115 avg_loss = 3.52399\n",
      "epoch no.2 train no.198580  loss = 2.76226 avg_loss = 3.51911\n",
      "epoch no.2 train no.198590  loss = 4.06546 avg_loss = 3.49186\n",
      "epoch no.2 train no.198600  loss = 2.82354 avg_loss = 3.48385\n",
      "epoch no.2 train no.198610  loss = 2.80331 avg_loss = 3.52658\n",
      "epoch no.2 train no.198620  loss = 3.49318 avg_loss = 3.51926\n",
      "epoch no.2 train no.198630  loss = 2.65699 avg_loss = 3.50072\n",
      "epoch no.2 train no.198640  loss = 3.09535 avg_loss = 3.48935\n",
      "epoch no.2 train no.198650  loss = 4.19985 avg_loss = 3.47849\n",
      "epoch no.2 train no.198660  loss = 3.18602 avg_loss = 3.48173\n",
      "epoch no.2 train no.198670  loss = 5.06332 avg_loss = 3.45607\n",
      "epoch no.2 train no.198680  loss = 4.49140 avg_loss = 3.48525\n",
      "epoch no.2 train no.198690  loss = 3.26978 avg_loss = 3.43269\n",
      "epoch no.2 train no.198700  loss = 2.18562 avg_loss = 3.51718\n",
      "epoch no.2 train no.198710  loss = 3.44997 avg_loss = 3.52484\n",
      "epoch no.2 train no.198720  loss = 4.89803 avg_loss = 3.51435\n",
      "epoch no.2 train no.198730  loss = 3.83351 avg_loss = 3.53779\n",
      "epoch no.2 train no.198740  loss = 3.82327 avg_loss = 3.50864\n",
      "epoch no.2 train no.198750  loss = 2.01742 avg_loss = 3.50572\n",
      "epoch no.2 train no.198760  loss = 1.17739 avg_loss = 3.52823\n",
      "epoch no.2 train no.198770  loss = 3.97784 avg_loss = 3.52036\n",
      "epoch no.2 train no.198780  loss = 5.18165 avg_loss = 3.54581\n",
      "epoch no.2 train no.198790  loss = 2.30436 avg_loss = 3.52853\n",
      "epoch no.2 train no.198800  loss = 2.61017 avg_loss = 3.51243\n",
      "epoch no.2 train no.198810  loss = 5.68744 avg_loss = 3.55561\n",
      "epoch no.2 train no.198820  loss = 2.53314 avg_loss = 3.55140\n",
      "epoch no.2 train no.198830  loss = 4.43142 avg_loss = 3.55217\n",
      "epoch no.2 train no.198840  loss = 4.97226 avg_loss = 3.57227\n",
      "epoch no.2 train no.198850  loss = 2.73802 avg_loss = 3.53012\n",
      "epoch no.2 train no.198860  loss = 1.85069 avg_loss = 3.48324\n",
      "epoch no.2 train no.198870  loss = 3.75746 avg_loss = 3.49872\n",
      "epoch no.2 train no.198880  loss = 3.31387 avg_loss = 3.52402\n",
      "epoch no.2 train no.198890  loss = 5.00934 avg_loss = 3.51975\n",
      "epoch no.2 train no.198900  loss = 2.95020 avg_loss = 3.51170\n",
      "epoch no.2 train no.198910  loss = 2.35546 avg_loss = 3.46660\n",
      "epoch no.2 train no.198920  loss = 5.08763 avg_loss = 3.48704\n",
      "epoch no.2 train no.198930  loss = 3.77403 avg_loss = 3.49137\n",
      "epoch no.2 train no.198940  loss = 3.25351 avg_loss = 3.48836\n",
      "epoch no.2 train no.198950  loss = 3.75580 avg_loss = 3.45680\n",
      "epoch no.2 train no.198960  loss = 2.42025 avg_loss = 3.39713\n",
      "epoch no.2 train no.198970  loss = 2.65339 avg_loss = 3.40831\n",
      "epoch no.2 train no.198980  loss = 3.44731 avg_loss = 3.39385\n",
      "epoch no.2 train no.198990  loss = 3.36379 avg_loss = 3.38019\n",
      "epoch no.2 train no.199000  loss = 6.09840 avg_loss = 3.47435\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁위한', '주는', '▁감성', '적인', '▁노래', '</s>']\n",
      "여름밤을 달래주는 감성적인 음악</s>\n",
      "epoch no.2 train no.199010  loss = 4.47618 avg_loss = 3.48333\n",
      "epoch no.2 train no.199020  loss = 2.65704 avg_loss = 3.47209\n",
      "epoch no.2 train no.199030  loss = 3.24726 avg_loss = 3.49438\n",
      "epoch no.2 train no.199040  loss = 3.68010 avg_loss = 3.48390\n",
      "epoch no.2 train no.199050  loss = 3.77670 avg_loss = 3.48217\n",
      "epoch no.2 train no.199060  loss = 3.43455 avg_loss = 3.47479\n",
      "epoch no.2 train no.199070  loss = 3.71742 avg_loss = 3.48933\n",
      "epoch no.2 train no.199080  loss = 2.71297 avg_loss = 3.50703\n",
      "epoch no.2 train no.199090  loss = 4.60219 avg_loss = 3.50944\n",
      "epoch no.2 train no.199100  loss = 3.81515 avg_loss = 3.51200\n",
      "epoch no.2 train no.199110  loss = 2.75834 avg_loss = 3.55001\n",
      "epoch no.2 train no.199120  loss = 3.11676 avg_loss = 3.53631\n",
      "epoch no.2 train no.199130  loss = 2.66874 avg_loss = 3.47356\n",
      "epoch no.2 train no.199140  loss = 3.42771 avg_loss = 3.46387\n",
      "epoch no.2 train no.199150  loss = 3.81407 avg_loss = 3.47301\n",
      "epoch no.2 train no.199160  loss = 2.36512 avg_loss = 3.42804\n",
      "epoch no.2 train no.199170  loss = 3.51418 avg_loss = 3.44511\n",
      "epoch no.2 train no.199180  loss = 2.90407 avg_loss = 3.49155\n",
      "epoch no.2 train no.199190  loss = 2.61497 avg_loss = 3.50622\n",
      "epoch no.2 train no.199200  loss = 2.96933 avg_loss = 3.50698\n",
      "epoch no.2 train no.199210  loss = 2.94898 avg_loss = 3.50507\n",
      "epoch no.2 train no.199220  loss = 2.00542 avg_loss = 3.48804\n",
      "epoch no.2 train no.199230  loss = 3.66275 avg_loss = 3.57693\n",
      "epoch no.2 train no.199240  loss = 2.39099 avg_loss = 3.53931\n",
      "epoch no.2 train no.199250  loss = 1.72793 avg_loss = 3.50010\n",
      "epoch no.2 train no.199260  loss = 3.99918 avg_loss = 3.55494\n",
      "epoch no.2 train no.199270  loss = 6.11374 avg_loss = 3.61668\n",
      "epoch no.2 train no.199280  loss = 1.86773 avg_loss = 3.53131\n",
      "epoch no.2 train no.199290  loss = 2.13754 avg_loss = 3.55600\n",
      "epoch no.2 train no.199300  loss = 1.82382 avg_loss = 3.51129\n",
      "epoch no.2 train no.199310  loss = 3.03807 avg_loss = 3.48915\n",
      "epoch no.2 train no.199320  loss = 3.27135 avg_loss = 3.46256\n",
      "epoch no.2 train no.199330  loss = 2.51198 avg_loss = 3.50995\n",
      "epoch no.2 train no.199340  loss = 3.71252 avg_loss = 3.54588\n",
      "epoch no.2 train no.199350  loss = 2.26271 avg_loss = 3.51725\n",
      "epoch no.2 train no.199360  loss = 2.42156 avg_loss = 3.50338\n",
      "epoch no.2 train no.199370  loss = 3.65997 avg_loss = 3.51098\n",
      "epoch no.2 train no.199380  loss = 2.77150 avg_loss = 3.54495\n",
      "epoch no.2 train no.199390  loss = 3.56209 avg_loss = 3.54708\n",
      "epoch no.2 train no.199400  loss = 4.34532 avg_loss = 3.53573\n",
      "epoch no.2 train no.199410  loss = 2.98960 avg_loss = 3.51284\n",
      "epoch no.2 train no.199420  loss = 3.65114 avg_loss = 3.48768\n",
      "epoch no.2 train no.199430  loss = 2.68077 avg_loss = 3.51496\n",
      "epoch no.2 train no.199440  loss = 2.39752 avg_loss = 3.51472\n",
      "epoch no.2 train no.199450  loss = 3.27352 avg_loss = 3.50594\n",
      "epoch no.2 train no.199460  loss = 2.71458 avg_loss = 3.53302\n",
      "epoch no.2 train no.199470  loss = 5.87021 avg_loss = 3.56364\n",
      "epoch no.2 train no.199480  loss = 2.40972 avg_loss = 3.47749\n",
      "epoch no.2 train no.199490  loss = 1.77342 avg_loss = 3.41604\n",
      "epoch no.2 train no.199500  loss = 3.02321 avg_loss = 3.40655\n",
      "epoch no.2 train no.199510  loss = 2.81520 avg_loss = 3.38589\n",
      "epoch no.2 train no.199520  loss = 3.31509 avg_loss = 3.40250\n",
      "epoch no.2 train no.199530  loss = 4.92587 avg_loss = 3.46707\n",
      "epoch no.2 train no.199540  loss = 1.80955 avg_loss = 3.41234\n",
      "epoch no.2 train no.199550  loss = 3.19977 avg_loss = 3.41382\n",
      "epoch no.2 train no.199560  loss = 2.17732 avg_loss = 3.37928\n",
      "epoch no.2 train no.199570  loss = 2.67632 avg_loss = 3.40197\n",
      "epoch no.2 train no.199580  loss = 4.90471 avg_loss = 3.46827\n",
      "epoch no.2 train no.199590  loss = 4.00435 avg_loss = 3.48050\n",
      "epoch no.2 train no.199600  loss = 3.18887 avg_loss = 3.46820\n",
      "epoch no.2 train no.199610  loss = 3.64188 avg_loss = 3.44935\n",
      "epoch no.2 train no.199620  loss = 3.56239 avg_loss = 3.43063\n",
      "epoch no.2 train no.199630  loss = 5.62821 avg_loss = 3.42623\n",
      "epoch no.2 train no.199640  loss = 5.88169 avg_loss = 3.49293\n",
      "epoch no.2 train no.199650  loss = 5.05146 avg_loss = 3.54861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.199660  loss = 2.70679 avg_loss = 3.53106\n",
      "epoch no.2 train no.199670  loss = 3.04153 avg_loss = 3.52210\n",
      "epoch no.2 train no.199680  loss = 3.70556 avg_loss = 3.53675\n",
      "epoch no.2 train no.199690  loss = 2.96313 avg_loss = 3.50315\n",
      "epoch no.2 train no.199700  loss = 2.21934 avg_loss = 3.46794\n",
      "epoch no.2 train no.199710  loss = 3.27496 avg_loss = 3.46903\n",
      "epoch no.2 train no.199720  loss = 1.74554 avg_loss = 3.47500\n",
      "epoch no.2 train no.199730  loss = 3.65794 avg_loss = 3.51605\n",
      "epoch no.2 train no.199740  loss = 2.63546 avg_loss = 3.50960\n",
      "epoch no.2 train no.199750  loss = 3.52767 avg_loss = 3.47262\n",
      "epoch no.2 train no.199760  loss = 3.66691 avg_loss = 3.47048\n",
      "epoch no.2 train no.199770  loss = 3.83622 avg_loss = 3.51840\n",
      "epoch no.2 train no.199780  loss = 3.53887 avg_loss = 3.50635\n",
      "epoch no.2 train no.199790  loss = 2.35131 avg_loss = 3.52897\n",
      "epoch no.2 train no.199800  loss = 3.40209 avg_loss = 3.48474\n",
      "epoch no.2 train no.199810  loss = 3.91828 avg_loss = 3.45681\n",
      "epoch no.2 train no.199820  loss = 1.91548 avg_loss = 3.44618\n",
      "epoch no.2 train no.199830  loss = 3.36694 avg_loss = 3.43945\n",
      "epoch no.2 train no.199840  loss = 3.52758 avg_loss = 3.43426\n",
      "epoch no.2 train no.199850  loss = 2.77848 avg_loss = 3.41934\n",
      "epoch no.2 train no.199860  loss = 4.12891 avg_loss = 3.40461\n",
      "epoch no.2 train no.199870  loss = 3.10261 avg_loss = 3.42499\n",
      "epoch no.2 train no.199880  loss = 5.89114 avg_loss = 3.48102\n",
      "epoch no.2 train no.199890  loss = 2.15114 avg_loss = 3.52508\n",
      "epoch no.2 train no.199900  loss = 2.06450 avg_loss = 3.51448\n",
      "epoch no.2 train no.199910  loss = 3.61851 avg_loss = 3.52430\n",
      "epoch no.2 train no.199920  loss = 2.18683 avg_loss = 3.51795\n",
      "epoch no.2 train no.199930  loss = 4.29592 avg_loss = 3.52151\n",
      "epoch no.2 train no.199940  loss = 4.07661 avg_loss = 3.51016\n",
      "epoch no.2 train no.199950  loss = 3.80025 avg_loss = 3.45861\n",
      "epoch no.2 train no.199960  loss = 3.31003 avg_loss = 3.46843\n",
      "epoch no.2 train no.199970  loss = 4.46896 avg_loss = 3.49813\n",
      "epoch no.2 train no.199980  loss = 3.95030 avg_loss = 3.47472\n",
      "epoch no.2 train no.199990  loss = 6.03537 avg_loss = 3.52787\n",
      "epoch no.2 train no.200000  loss = 3.45201 avg_loss = 3.48307\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.200010  loss = 4.20192 avg_loss = 3.48717\n",
      "epoch no.2 train no.200020  loss = 4.31760 avg_loss = 3.51486\n",
      "epoch no.2 train no.200030  loss = 3.86269 avg_loss = 3.57446\n",
      "epoch no.2 train no.200040  loss = 2.51759 avg_loss = 3.56568\n",
      "epoch no.2 train no.200050  loss = 5.62809 avg_loss = 3.59904\n",
      "epoch no.2 train no.200060  loss = 3.35055 avg_loss = 3.57477\n",
      "epoch no.2 train no.200070  loss = 2.32725 avg_loss = 3.52962\n",
      "epoch no.2 train no.200080  loss = 4.06435 avg_loss = 3.54292\n",
      "epoch no.2 train no.200090  loss = 3.47745 avg_loss = 3.53622\n",
      "epoch no.2 train no.200100  loss = 4.05239 avg_loss = 3.52762\n",
      "epoch no.2 train no.200110  loss = 3.19811 avg_loss = 3.51671\n",
      "epoch no.2 train no.200120  loss = 4.71330 avg_loss = 3.50939\n",
      "epoch no.2 train no.200130  loss = 1.06325 avg_loss = 3.46704\n",
      "epoch no.2 train no.200140  loss = 3.89477 avg_loss = 3.50272\n",
      "epoch no.2 train no.200150  loss = 4.62358 avg_loss = 3.49750\n",
      "epoch no.2 train no.200160  loss = 3.59401 avg_loss = 3.51081\n",
      "epoch no.2 train no.200170  loss = 3.34000 avg_loss = 3.49505\n",
      "epoch no.2 train no.200180  loss = 5.27534 avg_loss = 3.50795\n",
      "epoch no.2 train no.200190  loss = 3.30480 avg_loss = 3.55929\n",
      "epoch no.2 train no.200200  loss = 3.97845 avg_loss = 3.53951\n",
      "epoch no.2 train no.200210  loss = 3.76795 avg_loss = 3.53925\n",
      "epoch no.2 train no.200220  loss = 4.03800 avg_loss = 3.53013\n",
      "epoch no.2 train no.200230  loss = 2.54955 avg_loss = 3.50136\n",
      "epoch no.2 train no.200240  loss = 3.48100 avg_loss = 3.49524\n",
      "epoch no.2 train no.200250  loss = 4.01243 avg_loss = 3.49909\n",
      "epoch no.2 train no.200260  loss = 4.85323 avg_loss = 3.53112\n",
      "epoch no.2 train no.200270  loss = 1.96608 avg_loss = 3.55371\n",
      "epoch no.2 train no.200280  loss = 3.52193 avg_loss = 3.62199\n",
      "epoch no.2 train no.200290  loss = 4.25059 avg_loss = 3.60715\n",
      "epoch no.2 train no.200300  loss = 6.15726 avg_loss = 3.65090\n",
      "epoch no.2 train no.200310  loss = 5.20773 avg_loss = 3.64681\n",
      "epoch no.2 train no.200320  loss = 3.34781 avg_loss = 3.62089\n",
      "epoch no.2 train no.200330  loss = 3.64932 avg_loss = 3.60196\n",
      "epoch no.2 train no.200340  loss = 4.19374 avg_loss = 3.58724\n",
      "epoch no.2 train no.200350  loss = 3.77652 avg_loss = 3.56311\n",
      "epoch no.2 train no.200360  loss = 3.10822 avg_loss = 3.53053\n",
      "epoch no.2 train no.200370  loss = 3.81924 avg_loss = 3.55969\n",
      "epoch no.2 train no.200380  loss = 3.42346 avg_loss = 3.54488\n",
      "epoch no.2 train no.200390  loss = 5.78996 avg_loss = 3.52932\n",
      "epoch no.2 train no.200400  loss = 3.47753 avg_loss = 3.53581\n",
      "epoch no.2 train no.200410  loss = 3.01459 avg_loss = 3.49891\n",
      "epoch no.2 train no.200420  loss = 5.73389 avg_loss = 3.48988\n",
      "epoch no.2 train no.200430  loss = 3.71011 avg_loss = 3.46030\n",
      "epoch no.2 train no.200440  loss = 3.40536 avg_loss = 3.49363\n",
      "epoch no.2 train no.200450  loss = 3.10990 avg_loss = 3.46941\n",
      "epoch no.2 train no.200460  loss = 3.65647 avg_loss = 3.43669\n",
      "epoch no.2 train no.200470  loss = 5.12185 avg_loss = 3.45262\n",
      "epoch no.2 train no.200480  loss = 2.69187 avg_loss = 3.45240\n",
      "epoch no.2 train no.200490  loss = 2.84815 avg_loss = 3.43916\n",
      "epoch no.2 train no.200500  loss = 6.39076 avg_loss = 3.42790\n",
      "epoch no.2 train no.200510  loss = 2.79914 avg_loss = 3.40642\n",
      "epoch no.2 train no.200520  loss = 3.47245 avg_loss = 3.39637\n",
      "epoch no.2 train no.200530  loss = 3.26426 avg_loss = 3.43952\n",
      "epoch no.2 train no.200540  loss = 6.38408 avg_loss = 3.46750\n",
      "epoch no.2 train no.200550  loss = 5.55442 avg_loss = 3.47968\n",
      "epoch no.2 train no.200560  loss = 3.44562 avg_loss = 3.53132\n",
      "epoch no.2 train no.200570  loss = 2.60084 avg_loss = 3.54960\n",
      "epoch no.2 train no.200580  loss = 3.31038 avg_loss = 3.52507\n",
      "epoch no.2 train no.200590  loss = 2.33738 avg_loss = 3.52758\n",
      "epoch no.2 train no.200600  loss = 2.94522 avg_loss = 3.52329\n",
      "epoch no.2 train no.200610  loss = 4.20789 avg_loss = 3.52223\n",
      "epoch no.2 train no.200620  loss = 2.97709 avg_loss = 3.54695\n",
      "epoch no.2 train no.200630  loss = 2.71384 avg_loss = 3.55152\n",
      "epoch no.2 train no.200640  loss = 5.05570 avg_loss = 3.53984\n",
      "epoch no.2 train no.200650  loss = 1.43007 avg_loss = 3.52243\n",
      "epoch no.2 train no.200660  loss = 2.35278 avg_loss = 3.48969\n",
      "epoch no.2 train no.200670  loss = 3.22163 avg_loss = 3.51905\n",
      "epoch no.2 train no.200680  loss = 2.03487 avg_loss = 3.51758\n",
      "epoch no.2 train no.200690  loss = 3.49968 avg_loss = 3.49796\n",
      "epoch no.2 train no.200700  loss = 3.52414 avg_loss = 3.51276\n",
      "epoch no.2 train no.200710  loss = 2.53919 avg_loss = 3.54298\n",
      "epoch no.2 train no.200720  loss = 3.07025 avg_loss = 3.51162\n",
      "epoch no.2 train no.200730  loss = 4.59672 avg_loss = 3.54783\n",
      "epoch no.2 train no.200740  loss = 4.07611 avg_loss = 3.53790\n",
      "epoch no.2 train no.200750  loss = 2.49203 avg_loss = 3.56808\n",
      "epoch no.2 train no.200760  loss = 3.40279 avg_loss = 3.60543\n",
      "epoch no.2 train no.200770  loss = 4.03185 avg_loss = 3.57224\n",
      "epoch no.2 train no.200780  loss = 3.78117 avg_loss = 3.56168\n",
      "epoch no.2 train no.200790  loss = 4.21059 avg_loss = 3.59095\n",
      "epoch no.2 train no.200800  loss = 2.73633 avg_loss = 3.58583\n",
      "epoch no.2 train no.200810  loss = 2.61177 avg_loss = 3.57077\n",
      "epoch no.2 train no.200820  loss = 1.52827 avg_loss = 3.56591\n",
      "epoch no.2 train no.200830  loss = 2.52687 avg_loss = 3.58239\n",
      "epoch no.2 train no.200840  loss = 2.07998 avg_loss = 3.59086\n",
      "epoch no.2 train no.200850  loss = 3.32485 avg_loss = 3.51800\n",
      "epoch no.2 train no.200860  loss = 1.95226 avg_loss = 3.49759\n",
      "epoch no.2 train no.200870  loss = 4.08987 avg_loss = 3.49023\n",
      "epoch no.2 train no.200880  loss = 3.39292 avg_loss = 3.51235\n",
      "epoch no.2 train no.200890  loss = 1.62723 avg_loss = 3.48961\n",
      "epoch no.2 train no.200900  loss = 3.36338 avg_loss = 3.46056\n",
      "epoch no.2 train no.200910  loss = 1.84966 avg_loss = 3.44610\n",
      "epoch no.2 train no.200920  loss = 2.31162 avg_loss = 3.46280\n",
      "epoch no.2 train no.200930  loss = 4.40675 avg_loss = 3.47157\n",
      "epoch no.2 train no.200940  loss = 4.89908 avg_loss = 3.48433\n",
      "epoch no.2 train no.200950  loss = 2.37849 avg_loss = 3.51022\n",
      "epoch no.2 train no.200960  loss = 2.37984 avg_loss = 3.46440\n",
      "epoch no.2 train no.200970  loss = 3.35490 avg_loss = 3.41429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.200980  loss = 3.09689 avg_loss = 3.38886\n",
      "epoch no.2 train no.200990  loss = 5.52961 avg_loss = 3.38224\n",
      "epoch no.2 train no.201000  loss = 2.58181 avg_loss = 3.46186\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '의', '▁감성', '적인', '에이지', '</s>']\n",
      "여름밤의 감성 뉴에이지</s>\n",
      "epoch no.2 train no.201010  loss = 3.94922 avg_loss = 3.44076\n",
      "epoch no.2 train no.201020  loss = 4.65492 avg_loss = 3.45397\n",
      "epoch no.2 train no.201030  loss = 2.44425 avg_loss = 3.48773\n",
      "epoch no.2 train no.201040  loss = 6.06538 avg_loss = 3.58992\n",
      "epoch no.2 train no.201050  loss = 2.10521 avg_loss = 3.57010\n",
      "epoch no.2 train no.201060  loss = 3.66060 avg_loss = 3.52069\n",
      "epoch no.2 train no.201070  loss = 2.24173 avg_loss = 3.53818\n",
      "epoch no.2 train no.201080  loss = 3.76162 avg_loss = 3.51638\n",
      "epoch no.2 train no.201090  loss = 4.51657 avg_loss = 3.53023\n",
      "epoch no.2 train no.201100  loss = 2.26345 avg_loss = 3.55822\n",
      "epoch no.2 train no.201110  loss = 3.96586 avg_loss = 3.50924\n",
      "epoch no.2 train no.201120  loss = 4.76701 avg_loss = 3.52918\n",
      "epoch no.2 train no.201130  loss = 5.99872 avg_loss = 3.58370\n",
      "epoch no.2 train no.201140  loss = 3.39463 avg_loss = 3.57771\n",
      "epoch no.2 train no.201150  loss = 4.45438 avg_loss = 3.56146\n",
      "epoch no.2 train no.201160  loss = 4.08120 avg_loss = 3.61370\n",
      "epoch no.2 train no.201170  loss = 4.71514 avg_loss = 3.62435\n",
      "epoch no.2 train no.201180  loss = 4.34194 avg_loss = 3.60987\n",
      "epoch no.2 train no.201190  loss = 3.24909 avg_loss = 3.58270\n",
      "epoch no.2 train no.201200  loss = 2.46516 avg_loss = 3.54755\n",
      "epoch no.2 train no.201210  loss = 3.77535 avg_loss = 3.57077\n",
      "epoch no.2 train no.201220  loss = 4.04847 avg_loss = 3.55661\n",
      "epoch no.2 train no.201230  loss = 3.41136 avg_loss = 3.56469\n",
      "epoch no.2 train no.201240  loss = 2.01401 avg_loss = 3.56878\n",
      "epoch no.2 train no.201250  loss = 3.05023 avg_loss = 3.54579\n",
      "epoch no.2 train no.201260  loss = 3.43908 avg_loss = 3.52031\n",
      "epoch no.2 train no.201270  loss = 4.49062 avg_loss = 3.53798\n",
      "epoch no.2 train no.201280  loss = 4.52457 avg_loss = 3.49170\n",
      "epoch no.2 train no.201290  loss = 2.32552 avg_loss = 3.51506\n",
      "epoch no.2 train no.201300  loss = 5.13718 avg_loss = 3.54713\n",
      "epoch no.2 train no.201310  loss = 3.80062 avg_loss = 3.54587\n",
      "epoch no.2 train no.201320  loss = 4.70418 avg_loss = 3.58087\n",
      "epoch no.2 train no.201330  loss = 2.39679 avg_loss = 3.63152\n",
      "epoch no.2 train no.201340  loss = 2.33748 avg_loss = 3.56069\n",
      "epoch no.2 train no.201350  loss = 3.33717 avg_loss = 3.54767\n",
      "epoch no.2 train no.201360  loss = 3.48846 avg_loss = 3.52105\n",
      "epoch no.2 train no.201370  loss = 6.90378 avg_loss = 3.57147\n",
      "epoch no.2 train no.201380  loss = 4.67198 avg_loss = 3.58275\n",
      "epoch no.2 train no.201390  loss = 4.27555 avg_loss = 3.55292\n",
      "epoch no.2 train no.201400  loss = 3.37095 avg_loss = 3.59892\n",
      "epoch no.2 train no.201410  loss = 3.20042 avg_loss = 3.54926\n",
      "epoch no.2 train no.201420  loss = 2.51216 avg_loss = 3.55944\n",
      "epoch no.2 train no.201430  loss = 4.45965 avg_loss = 3.57682\n",
      "epoch no.2 train no.201440  loss = 4.72393 avg_loss = 3.58585\n",
      "epoch no.2 train no.201450  loss = 3.15895 avg_loss = 3.54644\n",
      "epoch no.2 train no.201460  loss = 2.86863 avg_loss = 3.51491\n",
      "epoch no.2 train no.201470  loss = 2.70019 avg_loss = 3.46438\n",
      "epoch no.2 train no.201480  loss = 1.69146 avg_loss = 3.42043\n",
      "epoch no.2 train no.201490  loss = 4.44327 avg_loss = 3.41207\n",
      "epoch no.2 train no.201500  loss = 3.78753 avg_loss = 3.42705\n",
      "epoch no.2 train no.201510  loss = 2.60339 avg_loss = 3.44323\n",
      "epoch no.2 train no.201520  loss = 3.72175 avg_loss = 3.45303\n",
      "epoch no.2 train no.201530  loss = 3.48243 avg_loss = 3.41638\n",
      "epoch no.2 train no.201540  loss = 2.54809 avg_loss = 3.37836\n",
      "epoch no.2 train no.201550  loss = 6.49440 avg_loss = 3.44085\n",
      "epoch no.2 train no.201560  loss = 3.12392 avg_loss = 3.47757\n",
      "epoch no.2 train no.201570  loss = 6.06681 avg_loss = 3.46557\n",
      "epoch no.2 train no.201580  loss = 3.50825 avg_loss = 3.47701\n",
      "epoch no.2 train no.201590  loss = 2.85691 avg_loss = 3.49844\n",
      "epoch no.2 train no.201600  loss = 3.33576 avg_loss = 3.54015\n",
      "epoch no.2 train no.201610  loss = 3.99862 avg_loss = 3.48136\n",
      "epoch no.2 train no.201620  loss = 2.57220 avg_loss = 3.52687\n",
      "epoch no.2 train no.201630  loss = 3.38344 avg_loss = 3.57201\n",
      "epoch no.2 train no.201640  loss = 2.26423 avg_loss = 3.54297\n",
      "epoch no.2 train no.201650  loss = 2.01067 avg_loss = 3.55568\n",
      "epoch no.2 train no.201660  loss = 3.08661 avg_loss = 3.53995\n",
      "epoch no.2 train no.201670  loss = 3.83261 avg_loss = 3.58599\n",
      "epoch no.2 train no.201680  loss = 4.38281 avg_loss = 3.61331\n",
      "epoch no.2 train no.201690  loss = 2.63512 avg_loss = 3.57015\n",
      "epoch no.2 train no.201700  loss = 4.37641 avg_loss = 3.52097\n",
      "epoch no.2 train no.201710  loss = 2.93538 avg_loss = 3.49252\n",
      "epoch no.2 train no.201720  loss = 3.31811 avg_loss = 3.53648\n",
      "epoch no.2 train no.201730  loss = 4.26692 avg_loss = 3.53724\n",
      "epoch no.2 train no.201740  loss = 3.70803 avg_loss = 3.52408\n",
      "epoch no.2 train no.201750  loss = 4.17410 avg_loss = 3.52013\n",
      "epoch no.2 train no.201760  loss = 4.10111 avg_loss = 3.52357\n",
      "epoch no.2 train no.201770  loss = 4.35817 avg_loss = 3.52651\n",
      "epoch no.2 train no.201780  loss = 5.54781 avg_loss = 3.60924\n",
      "epoch no.2 train no.201790  loss = 5.06492 avg_loss = 3.61903\n",
      "epoch no.2 train no.201800  loss = 3.10178 avg_loss = 3.59111\n",
      "epoch no.2 train no.201810  loss = 1.86732 avg_loss = 3.52202\n",
      "epoch no.2 train no.201820  loss = 4.83548 avg_loss = 3.53810\n",
      "epoch no.2 train no.201830  loss = 3.37258 avg_loss = 3.54680\n",
      "epoch no.2 train no.201840  loss = 1.94444 avg_loss = 3.52089\n",
      "epoch no.2 train no.201850  loss = 3.51742 avg_loss = 3.53374\n",
      "epoch no.2 train no.201860  loss = 2.20170 avg_loss = 3.53328\n",
      "epoch no.2 train no.201870  loss = 3.11614 avg_loss = 3.52120\n",
      "epoch no.2 train no.201880  loss = 2.76119 avg_loss = 3.49607\n",
      "epoch no.2 train no.201890  loss = 4.05329 avg_loss = 3.48307\n",
      "epoch no.2 train no.201900  loss = 3.14173 avg_loss = 3.50767\n",
      "epoch no.2 train no.201910  loss = 3.65349 avg_loss = 3.48308\n",
      "epoch no.2 train no.201920  loss = 3.18817 avg_loss = 3.46132\n",
      "epoch no.2 train no.201930  loss = 2.51115 avg_loss = 3.46490\n",
      "epoch no.2 train no.201940  loss = 3.83144 avg_loss = 3.49905\n",
      "epoch no.2 train no.201950  loss = 2.57579 avg_loss = 3.55042\n",
      "epoch no.2 train no.201960  loss = 4.24898 avg_loss = 3.58798\n",
      "epoch no.2 train no.201970  loss = 5.28533 avg_loss = 3.61883\n",
      "epoch no.2 train no.201980  loss = 6.08460 avg_loss = 3.57929\n",
      "epoch no.2 train no.201990  loss = 2.95910 avg_loss = 3.51262\n",
      "epoch no.2 train no.202000  loss = 2.75451 avg_loss = 3.52931\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁잔잔', '</s>']\n",
      "여름밤에 듣는 노래</s>\n",
      "epoch no.2 train no.202010  loss = 3.37599 avg_loss = 3.51677\n",
      "epoch no.2 train no.202020  loss = 2.56788 avg_loss = 3.50197\n",
      "epoch no.2 train no.202030  loss = 3.34636 avg_loss = 3.48596\n",
      "epoch no.2 train no.202040  loss = 1.97919 avg_loss = 3.47639\n",
      "epoch no.2 train no.202050  loss = 2.06158 avg_loss = 3.48874\n",
      "epoch no.2 train no.202060  loss = 3.21041 avg_loss = 3.51300\n",
      "epoch no.2 train no.202070  loss = 5.86368 avg_loss = 3.56193\n",
      "epoch no.2 train no.202080  loss = 3.25777 avg_loss = 3.56917\n",
      "epoch no.2 train no.202090  loss = 3.43962 avg_loss = 3.58493\n",
      "epoch no.2 train no.202100  loss = 3.09728 avg_loss = 3.56072\n",
      "epoch no.2 train no.202110  loss = 2.99232 avg_loss = 3.55722\n",
      "epoch no.2 train no.202120  loss = 2.82287 avg_loss = 3.53614\n",
      "epoch no.2 train no.202130  loss = 4.21602 avg_loss = 3.52359\n",
      "epoch no.2 train no.202140  loss = 3.52252 avg_loss = 3.51027\n",
      "epoch no.2 train no.202150  loss = 3.89164 avg_loss = 3.47480\n",
      "epoch no.2 train no.202160  loss = 3.53793 avg_loss = 3.50246\n",
      "epoch no.2 train no.202170  loss = 1.39867 avg_loss = 3.50010\n",
      "epoch no.2 train no.202180  loss = 3.56930 avg_loss = 3.48011\n",
      "epoch no.2 train no.202190  loss = 2.68228 avg_loss = 3.45843\n",
      "epoch no.2 train no.202200  loss = 4.70180 avg_loss = 3.51057\n",
      "epoch no.2 train no.202210  loss = 2.54583 avg_loss = 3.50155\n",
      "epoch no.2 train no.202220  loss = 5.00615 avg_loss = 3.48354\n",
      "epoch no.2 train no.202230  loss = 3.87174 avg_loss = 3.46764\n",
      "epoch no.2 train no.202240  loss = 4.44882 avg_loss = 3.48809\n",
      "epoch no.2 train no.202250  loss = 2.81115 avg_loss = 3.45162\n",
      "epoch no.2 train no.202260  loss = 3.01847 avg_loss = 3.46438\n",
      "epoch no.2 train no.202270  loss = 3.55018 avg_loss = 3.50307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.202280  loss = 3.17555 avg_loss = 3.51032\n",
      "epoch no.2 train no.202290  loss = 2.27611 avg_loss = 3.47920\n",
      "epoch no.2 train no.202300  loss = 5.17114 avg_loss = 3.46157\n",
      "epoch no.2 train no.202310  loss = 4.69325 avg_loss = 3.48186\n",
      "epoch no.2 train no.202320  loss = 3.82106 avg_loss = 3.51832\n",
      "epoch no.2 train no.202330  loss = 4.35892 avg_loss = 3.51069\n",
      "epoch no.2 train no.202340  loss = 3.42621 avg_loss = 3.53165\n",
      "epoch no.2 train no.202350  loss = 2.62446 avg_loss = 3.51439\n",
      "epoch no.2 train no.202360  loss = 4.49047 avg_loss = 3.53566\n",
      "epoch no.2 train no.202370  loss = 2.81676 avg_loss = 3.53962\n",
      "epoch no.2 train no.202380  loss = 4.75601 avg_loss = 3.56356\n",
      "epoch no.2 train no.202390  loss = 5.61653 avg_loss = 3.56742\n",
      "epoch no.2 train no.202400  loss = 2.99274 avg_loss = 3.60269\n",
      "epoch no.2 train no.202410  loss = 4.39055 avg_loss = 3.65678\n",
      "epoch no.2 train no.202420  loss = 3.27172 avg_loss = 3.63449\n",
      "epoch no.2 train no.202430  loss = 2.40091 avg_loss = 3.63170\n",
      "epoch no.2 train no.202440  loss = 4.97120 avg_loss = 3.61216\n",
      "epoch no.2 train no.202450  loss = 5.21311 avg_loss = 3.63574\n",
      "epoch no.2 train no.202460  loss = 3.50473 avg_loss = 3.65017\n",
      "epoch no.2 train no.202470  loss = 5.20494 avg_loss = 3.62159\n",
      "epoch no.2 train no.202480  loss = 3.50692 avg_loss = 3.59200\n",
      "epoch no.2 train no.202490  loss = 3.79306 avg_loss = 3.60079\n",
      "epoch no.2 train no.202500  loss = 3.78872 avg_loss = 3.59227\n",
      "epoch no.2 train no.202510  loss = 3.41725 avg_loss = 3.59699\n",
      "epoch no.2 train no.202520  loss = 3.46827 avg_loss = 3.59776\n",
      "epoch no.2 train no.202530  loss = 2.95038 avg_loss = 3.55827\n",
      "epoch no.2 train no.202540  loss = 4.20453 avg_loss = 3.59769\n",
      "epoch no.2 train no.202550  loss = 3.74487 avg_loss = 3.61480\n",
      "epoch no.2 train no.202560  loss = 3.38724 avg_loss = 3.64619\n",
      "epoch no.2 train no.202570  loss = 5.01490 avg_loss = 3.68714\n",
      "epoch no.2 train no.202580  loss = 6.66561 avg_loss = 3.69120\n",
      "epoch no.2 train no.202590  loss = 2.71195 avg_loss = 3.67387\n",
      "epoch no.2 train no.202600  loss = 3.63018 avg_loss = 3.70298\n",
      "epoch no.2 train no.202610  loss = 3.01050 avg_loss = 3.68627\n",
      "epoch no.2 train no.202620  loss = 3.30143 avg_loss = 3.72892\n",
      "epoch no.2 train no.202630  loss = 3.49217 avg_loss = 3.67222\n",
      "epoch no.2 train no.202640  loss = 4.68769 avg_loss = 3.64611\n",
      "epoch no.2 train no.202650  loss = 3.96903 avg_loss = 3.65665\n",
      "epoch no.2 train no.202660  loss = 2.94380 avg_loss = 3.58912\n",
      "epoch no.2 train no.202670  loss = 5.97827 avg_loss = 3.60886\n",
      "epoch no.2 train no.202680  loss = 2.62217 avg_loss = 3.62023\n",
      "epoch no.2 train no.202690  loss = 6.32976 avg_loss = 3.63605\n",
      "epoch no.2 train no.202700  loss = 2.90824 avg_loss = 3.61195\n",
      "epoch no.2 train no.202710  loss = 2.39008 avg_loss = 3.59580\n",
      "epoch no.2 train no.202720  loss = 4.05122 avg_loss = 3.56374\n",
      "epoch no.2 train no.202730  loss = 3.50945 avg_loss = 3.50210\n",
      "epoch no.2 train no.202740  loss = 4.22413 avg_loss = 3.54422\n",
      "epoch no.2 train no.202750  loss = 6.23570 avg_loss = 3.53302\n",
      "epoch no.2 train no.202760  loss = 2.94012 avg_loss = 3.51064\n",
      "epoch no.2 train no.202770  loss = 4.09735 avg_loss = 3.50427\n",
      "epoch no.2 train no.202780  loss = 2.82416 avg_loss = 3.47236\n",
      "epoch no.2 train no.202790  loss = 3.47692 avg_loss = 3.51327\n",
      "epoch no.2 train no.202800  loss = 4.41770 avg_loss = 3.56993\n",
      "epoch no.2 train no.202810  loss = 3.85187 avg_loss = 3.59342\n",
      "epoch no.2 train no.202820  loss = 4.49781 avg_loss = 3.59068\n",
      "epoch no.2 train no.202830  loss = 3.54322 avg_loss = 3.57121\n",
      "epoch no.2 train no.202840  loss = 3.16054 avg_loss = 3.52663\n",
      "epoch no.2 train no.202850  loss = 2.22506 avg_loss = 3.53382\n",
      "epoch no.2 train no.202860  loss = 3.27899 avg_loss = 3.52273\n",
      "epoch no.2 train no.202870  loss = 3.72884 avg_loss = 3.52398\n",
      "epoch no.2 train no.202880  loss = 2.92213 avg_loss = 3.54194\n",
      "epoch no.2 train no.202890  loss = 3.27364 avg_loss = 3.53537\n",
      "epoch no.2 train no.202900  loss = 5.34220 avg_loss = 3.50859\n",
      "epoch no.2 train no.202910  loss = 3.25983 avg_loss = 3.53490\n",
      "epoch no.2 train no.202920  loss = 4.00260 avg_loss = 3.55168\n",
      "epoch no.2 train no.202930  loss = 3.21591 avg_loss = 3.56142\n",
      "epoch no.2 train no.202940  loss = 2.65774 avg_loss = 3.53688\n",
      "epoch no.2 train no.202950  loss = 3.90622 avg_loss = 3.52610\n",
      "epoch no.2 train no.202960  loss = 2.25768 avg_loss = 3.52346\n",
      "epoch no.2 train no.202970  loss = 2.72104 avg_loss = 3.49241\n",
      "epoch no.2 train no.202980  loss = 5.77340 avg_loss = 3.57011\n",
      "epoch no.2 train no.202990  loss = 3.00441 avg_loss = 3.57116\n",
      "epoch no.2 train no.203000  loss = 2.78838 avg_loss = 3.57086\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '리스트', '</s>']\n",
      "여름밤의 플레이리스트</s>\n",
      "epoch no.2 train no.203010  loss = 3.95688 avg_loss = 3.51950\n",
      "epoch no.2 train no.203020  loss = 3.96199 avg_loss = 3.57722\n",
      "epoch no.2 train no.203030  loss = 5.28422 avg_loss = 3.60445\n",
      "epoch no.2 train no.203040  loss = 3.42103 avg_loss = 3.61776\n",
      "epoch no.2 train no.203050  loss = 3.65068 avg_loss = 3.62239\n",
      "epoch no.2 train no.203060  loss = 1.79929 avg_loss = 3.56195\n",
      "epoch no.2 train no.203070  loss = 4.03817 avg_loss = 3.55517\n",
      "epoch no.2 train no.203080  loss = 3.50713 avg_loss = 3.51395\n",
      "epoch no.2 train no.203090  loss = 2.81408 avg_loss = 3.47206\n",
      "epoch no.2 train no.203100  loss = 1.94078 avg_loss = 3.44562\n",
      "epoch no.2 train no.203110  loss = 3.36808 avg_loss = 3.39301\n",
      "epoch no.2 train no.203120  loss = 2.69974 avg_loss = 3.40328\n",
      "epoch no.2 train no.203130  loss = 2.93281 avg_loss = 3.38711\n",
      "epoch no.2 train no.203140  loss = 5.06155 avg_loss = 3.36089\n",
      "epoch no.2 train no.203150  loss = 4.93904 avg_loss = 3.43947\n",
      "epoch no.2 train no.203160  loss = 2.47107 avg_loss = 3.45443\n",
      "epoch no.2 train no.203170  loss = 3.34820 avg_loss = 3.45834\n",
      "epoch no.2 train no.203180  loss = 4.05342 avg_loss = 3.46896\n",
      "epoch no.2 train no.203190  loss = 3.16293 avg_loss = 3.44458\n",
      "epoch no.2 train no.203200  loss = 3.77294 avg_loss = 3.51202\n",
      "epoch no.2 train no.203210  loss = 3.77086 avg_loss = 3.53331\n",
      "epoch no.2 train no.203220  loss = 3.98167 avg_loss = 3.57409\n",
      "epoch no.2 train no.203230  loss = 3.53558 avg_loss = 3.55975\n",
      "epoch no.2 train no.203240  loss = 3.81382 avg_loss = 3.54624\n",
      "epoch no.2 train no.203250  loss = 2.41137 avg_loss = 3.50335\n",
      "epoch no.2 train no.203260  loss = 3.53627 avg_loss = 3.49647\n",
      "epoch no.2 train no.203270  loss = 5.12910 avg_loss = 3.49960\n",
      "epoch no.2 train no.203280  loss = 2.85484 avg_loss = 3.48839\n",
      "epoch no.2 train no.203290  loss = 2.89247 avg_loss = 3.52587\n",
      "epoch no.2 train no.203300  loss = 2.64436 avg_loss = 3.57758\n",
      "epoch no.2 train no.203310  loss = 2.06095 avg_loss = 3.56779\n",
      "epoch no.2 train no.203320  loss = 3.72909 avg_loss = 3.56941\n",
      "epoch no.2 train no.203330  loss = 3.46703 avg_loss = 3.58890\n",
      "epoch no.2 train no.203340  loss = 3.04875 avg_loss = 3.63056\n",
      "epoch no.2 train no.203350  loss = 3.04034 avg_loss = 3.59925\n",
      "epoch no.2 train no.203360  loss = 3.78362 avg_loss = 3.61049\n",
      "epoch no.2 train no.203370  loss = 2.97909 avg_loss = 3.62843\n",
      "epoch no.2 train no.203380  loss = 5.82615 avg_loss = 3.67059\n",
      "epoch no.2 train no.203390  loss = 1.78730 avg_loss = 3.62287\n",
      "epoch no.2 train no.203400  loss = 3.40714 avg_loss = 3.60754\n",
      "epoch no.2 train no.203410  loss = 6.58169 avg_loss = 3.58744\n",
      "epoch no.2 train no.203420  loss = 2.01080 avg_loss = 3.54682\n",
      "epoch no.2 train no.203430  loss = 3.93475 avg_loss = 3.54149\n",
      "epoch no.2 train no.203440  loss = 2.15950 avg_loss = 3.49000\n",
      "epoch no.2 train no.203450  loss = 2.94966 avg_loss = 3.51002\n",
      "epoch no.2 train no.203460  loss = 2.46346 avg_loss = 3.48655\n",
      "epoch no.2 train no.203470  loss = 4.38837 avg_loss = 3.51803\n",
      "epoch no.2 train no.203480  loss = 2.43270 avg_loss = 3.52985\n",
      "epoch no.2 train no.203490  loss = 2.22133 avg_loss = 3.52878\n",
      "epoch no.2 train no.203500  loss = 1.54150 avg_loss = 3.51418\n",
      "epoch no.2 train no.203510  loss = 2.57824 avg_loss = 3.50740\n",
      "epoch no.2 train no.203520  loss = 4.18299 avg_loss = 3.50357\n",
      "epoch no.2 train no.203530  loss = 3.37520 avg_loss = 3.49188\n",
      "epoch no.2 train no.203540  loss = 5.39067 avg_loss = 3.49582\n",
      "epoch no.2 train no.203550  loss = 3.30036 avg_loss = 3.52381\n",
      "epoch no.2 train no.203560  loss = 4.44399 avg_loss = 3.45836\n",
      "epoch no.2 train no.203570  loss = 3.33986 avg_loss = 3.44955\n",
      "epoch no.2 train no.203580  loss = 2.04751 avg_loss = 3.47617\n",
      "epoch no.2 train no.203590  loss = 2.66741 avg_loss = 3.54914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.203600  loss = 3.13085 avg_loss = 3.51342\n",
      "epoch no.2 train no.203610  loss = 3.84780 avg_loss = 3.58965\n",
      "epoch no.2 train no.203620  loss = 3.20682 avg_loss = 3.55827\n",
      "epoch no.2 train no.203630  loss = 2.61342 avg_loss = 3.47540\n",
      "epoch no.2 train no.203640  loss = 2.24244 avg_loss = 3.46042\n",
      "epoch no.2 train no.203650  loss = 2.57799 avg_loss = 3.48319\n",
      "epoch no.2 train no.203660  loss = 4.23365 avg_loss = 3.55263\n",
      "epoch no.2 train no.203670  loss = 4.65147 avg_loss = 3.53157\n",
      "epoch no.2 train no.203680  loss = 3.74731 avg_loss = 3.56417\n",
      "epoch no.2 train no.203690  loss = 3.84050 avg_loss = 3.63389\n",
      "epoch no.2 train no.203700  loss = 3.05129 avg_loss = 3.61440\n",
      "epoch no.2 train no.203710  loss = 2.75269 avg_loss = 3.56300\n",
      "epoch no.2 train no.203720  loss = 3.07645 avg_loss = 3.54937\n",
      "epoch no.2 train no.203730  loss = 3.98208 avg_loss = 3.53539\n",
      "epoch no.2 train no.203740  loss = 3.95113 avg_loss = 3.55948\n",
      "epoch no.2 train no.203750  loss = 3.23759 avg_loss = 3.52211\n",
      "epoch no.2 train no.203760  loss = 2.22165 avg_loss = 3.49903\n",
      "epoch no.2 train no.203770  loss = 2.77638 avg_loss = 3.53751\n",
      "epoch no.2 train no.203780  loss = 4.38591 avg_loss = 3.56211\n",
      "epoch no.2 train no.203790  loss = 3.33755 avg_loss = 3.62872\n",
      "epoch no.2 train no.203800  loss = 2.55328 avg_loss = 3.60512\n",
      "epoch no.2 train no.203810  loss = 2.55309 avg_loss = 3.61509\n",
      "epoch no.2 train no.203820  loss = 2.16498 avg_loss = 3.56113\n",
      "epoch no.2 train no.203830  loss = 3.49053 avg_loss = 3.54194\n",
      "epoch no.2 train no.203840  loss = 2.15440 avg_loss = 3.52131\n",
      "epoch no.2 train no.203850  loss = 3.15566 avg_loss = 3.54382\n",
      "epoch no.2 train no.203860  loss = 5.04179 avg_loss = 3.55950\n",
      "epoch no.2 train no.203870  loss = 2.56126 avg_loss = 3.50694\n",
      "epoch no.2 train no.203880  loss = 3.58976 avg_loss = 3.49789\n",
      "epoch no.2 train no.203890  loss = 2.51143 avg_loss = 3.49072\n",
      "epoch no.2 train no.203900  loss = 3.52986 avg_loss = 3.46312\n",
      "epoch no.2 train no.203910  loss = 5.53922 avg_loss = 3.50015\n",
      "epoch no.2 train no.203920  loss = 4.85135 avg_loss = 3.54588\n",
      "epoch no.2 train no.203930  loss = 1.86712 avg_loss = 3.49714\n",
      "epoch no.2 train no.203940  loss = 3.49038 avg_loss = 3.49555\n",
      "epoch no.2 train no.203950  loss = 3.10659 avg_loss = 3.45593\n",
      "epoch no.2 train no.203960  loss = 1.49827 avg_loss = 3.41586\n",
      "epoch no.2 train no.203970  loss = 3.41048 avg_loss = 3.43163\n",
      "epoch no.2 train no.203980  loss = 4.83503 avg_loss = 3.43484\n",
      "epoch no.2 train no.203990  loss = 2.14781 avg_loss = 3.40217\n",
      "epoch no.2 train no.204000  loss = 3.23435 avg_loss = 3.38562\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁신나는', '엔', '</s>', '</s>']\n",
      "여름엔 역시 여름노래지</s>\n",
      "epoch no.2 train no.204010  loss = 3.35287 avg_loss = 3.40876\n",
      "epoch no.2 train no.204020  loss = 2.48521 avg_loss = 3.38443\n",
      "epoch no.2 train no.204030  loss = 3.40857 avg_loss = 3.37450\n",
      "epoch no.2 train no.204040  loss = 3.59624 avg_loss = 3.34043\n",
      "epoch no.2 train no.204050  loss = 3.59901 avg_loss = 3.36387\n",
      "epoch no.2 train no.204060  loss = 3.28017 avg_loss = 3.39057\n",
      "epoch no.2 train no.204070  loss = 3.01258 avg_loss = 3.34559\n",
      "epoch no.2 train no.204080  loss = 4.59521 avg_loss = 3.42692\n",
      "epoch no.2 train no.204090  loss = 5.89138 avg_loss = 3.42073\n",
      "epoch no.2 train no.204100  loss = 3.53913 avg_loss = 3.40083\n",
      "epoch no.2 train no.204110  loss = 3.97153 avg_loss = 3.37625\n",
      "epoch no.2 train no.204120  loss = 3.81769 avg_loss = 3.39755\n",
      "epoch no.2 train no.204130  loss = 2.43733 avg_loss = 3.39159\n",
      "epoch no.2 train no.204140  loss = 2.69564 avg_loss = 3.41637\n",
      "epoch no.2 train no.204150  loss = 6.38995 avg_loss = 3.48965\n",
      "epoch no.2 train no.204160  loss = 3.12159 avg_loss = 3.46104\n",
      "epoch no.2 train no.204170  loss = 3.76016 avg_loss = 3.48615\n",
      "epoch no.2 train no.204180  loss = 2.45344 avg_loss = 3.45905\n",
      "epoch no.2 train no.204190  loss = 4.02295 avg_loss = 3.45380\n",
      "epoch no.2 train no.204200  loss = 2.04517 avg_loss = 3.48343\n",
      "epoch no.2 train no.204210  loss = 5.79225 avg_loss = 3.47989\n",
      "epoch no.2 train no.204220  loss = 1.95006 avg_loss = 3.43886\n",
      "epoch no.2 train no.204230  loss = 3.54472 avg_loss = 3.40099\n",
      "epoch no.2 train no.204240  loss = 2.74171 avg_loss = 3.36759\n",
      "epoch no.2 train no.204250  loss = 2.60385 avg_loss = 3.34675\n",
      "epoch no.2 train no.204260  loss = 3.59612 avg_loss = 3.36451\n",
      "epoch no.2 train no.204270  loss = 3.46860 avg_loss = 3.43688\n",
      "epoch no.2 train no.204280  loss = 3.77893 avg_loss = 3.44873\n",
      "epoch no.2 train no.204290  loss = 3.37803 avg_loss = 3.45527\n",
      "epoch no.2 train no.204300  loss = 4.81589 avg_loss = 3.55725\n",
      "epoch no.2 train no.204310  loss = 4.12040 avg_loss = 3.60172\n",
      "epoch no.2 train no.204320  loss = 3.44472 avg_loss = 3.55807\n",
      "epoch no.2 train no.204330  loss = 4.24341 avg_loss = 3.54021\n",
      "epoch no.2 train no.204340  loss = 3.77114 avg_loss = 3.53599\n",
      "epoch no.2 train no.204350  loss = 4.47679 avg_loss = 3.56454\n",
      "epoch no.2 train no.204360  loss = 4.62381 avg_loss = 3.55548\n",
      "epoch no.2 train no.204370  loss = 4.32939 avg_loss = 3.61891\n",
      "epoch no.2 train no.204380  loss = 2.66174 avg_loss = 3.52942\n",
      "epoch no.2 train no.204390  loss = 3.89043 avg_loss = 3.53479\n",
      "epoch no.2 train no.204400  loss = 1.63627 avg_loss = 3.50803\n",
      "epoch no.2 train no.204410  loss = 3.12057 avg_loss = 3.50483\n",
      "epoch no.2 train no.204420  loss = 2.87938 avg_loss = 3.47772\n",
      "epoch no.2 train no.204430  loss = 2.75267 avg_loss = 3.49405\n",
      "epoch no.2 train no.204440  loss = 3.17988 avg_loss = 3.46352\n",
      "epoch no.2 train no.204450  loss = 2.72401 avg_loss = 3.49531\n",
      "epoch no.2 train no.204460  loss = 4.37319 avg_loss = 3.49825\n",
      "epoch no.2 train no.204470  loss = 2.87043 avg_loss = 3.53011\n",
      "epoch no.2 train no.204480  loss = 3.52389 avg_loss = 3.51263\n",
      "epoch no.2 train no.204490  loss = 3.30007 avg_loss = 3.52280\n",
      "epoch no.2 train no.204500  loss = 2.86448 avg_loss = 3.44880\n",
      "epoch no.2 train no.204510  loss = 5.16924 avg_loss = 3.48074\n",
      "epoch no.2 train no.204520  loss = 3.94060 avg_loss = 3.47320\n",
      "epoch no.2 train no.204530  loss = 3.49559 avg_loss = 3.48092\n",
      "epoch no.2 train no.204540  loss = 3.05258 avg_loss = 3.46322\n",
      "epoch no.2 train no.204550  loss = 4.06450 avg_loss = 3.50102\n",
      "epoch no.2 train no.204560  loss = 4.78237 avg_loss = 3.48702\n",
      "epoch no.2 train no.204570  loss = 3.05181 avg_loss = 3.46748\n",
      "epoch no.2 train no.204580  loss = 3.95697 avg_loss = 3.43993\n",
      "epoch no.2 train no.204590  loss = 3.46592 avg_loss = 3.38907\n",
      "epoch no.2 train no.204600  loss = 2.92557 avg_loss = 3.43602\n",
      "epoch no.2 train no.204610  loss = 2.58431 avg_loss = 3.41766\n",
      "epoch no.2 train no.204620  loss = 1.48254 avg_loss = 3.46380\n",
      "epoch no.2 train no.204630  loss = 3.18755 avg_loss = 3.47671\n",
      "epoch no.2 train no.204640  loss = 5.10407 avg_loss = 3.50397\n",
      "epoch no.2 train no.204650  loss = 3.36655 avg_loss = 3.47749\n",
      "epoch no.2 train no.204660  loss = 1.73109 avg_loss = 3.52435\n",
      "epoch no.2 train no.204670  loss = 3.71864 avg_loss = 3.51829\n",
      "epoch no.2 train no.204680  loss = 2.12379 avg_loss = 3.53372\n",
      "epoch no.2 train no.204690  loss = 2.95887 avg_loss = 3.49214\n",
      "epoch no.2 train no.204700  loss = 3.65623 avg_loss = 3.53995\n",
      "epoch no.2 train no.204710  loss = 5.47093 avg_loss = 3.57299\n",
      "epoch no.2 train no.204720  loss = 5.20353 avg_loss = 3.59650\n",
      "epoch no.2 train no.204730  loss = 4.99044 avg_loss = 3.63075\n",
      "epoch no.2 train no.204740  loss = 3.24944 avg_loss = 3.58093\n",
      "epoch no.2 train no.204750  loss = 4.20949 avg_loss = 3.60504\n",
      "epoch no.2 train no.204760  loss = 5.21383 avg_loss = 3.57726\n",
      "epoch no.2 train no.204770  loss = 3.52374 avg_loss = 3.57939\n",
      "epoch no.2 train no.204780  loss = 2.31988 avg_loss = 3.56610\n",
      "epoch no.2 train no.204790  loss = 4.09939 avg_loss = 3.56439\n",
      "epoch no.2 train no.204800  loss = 2.99296 avg_loss = 3.52599\n",
      "epoch no.2 train no.204810  loss = 3.18880 avg_loss = 3.54867\n",
      "epoch no.2 train no.204820  loss = 3.58668 avg_loss = 3.59818\n",
      "epoch no.2 train no.204830  loss = 3.18962 avg_loss = 3.61471\n",
      "epoch no.2 train no.204840  loss = 4.24315 avg_loss = 3.60827\n",
      "epoch no.2 train no.204850  loss = 3.36910 avg_loss = 3.62738\n",
      "epoch no.2 train no.204860  loss = 3.52333 avg_loss = 3.58188\n",
      "epoch no.2 train no.204870  loss = 3.33491 avg_loss = 3.61820\n",
      "epoch no.2 train no.204880  loss = 2.45584 avg_loss = 3.60451\n",
      "epoch no.2 train no.204890  loss = 3.07914 avg_loss = 3.56182\n",
      "epoch no.2 train no.204900  loss = 3.01362 avg_loss = 3.51265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.204910  loss = 4.34043 avg_loss = 3.52280\n",
      "epoch no.2 train no.204920  loss = 3.79626 avg_loss = 3.52290\n",
      "epoch no.2 train no.204930  loss = 2.55038 avg_loss = 3.49393\n",
      "epoch no.2 train no.204940  loss = 3.87650 avg_loss = 3.50466\n",
      "epoch no.2 train no.204950  loss = 3.80363 avg_loss = 3.52093\n",
      "epoch no.2 train no.204960  loss = 2.33888 avg_loss = 3.52626\n",
      "epoch no.2 train no.204970  loss = 4.23353 avg_loss = 3.55832\n",
      "epoch no.2 train no.204980  loss = 5.44308 avg_loss = 3.56991\n",
      "epoch no.2 train no.204990  loss = 4.63858 avg_loss = 3.55299\n",
      "epoch no.2 train no.205000  loss = 3.38794 avg_loss = 3.55750\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁댄스', 'op', '</s>', '</s>']\n",
      "여름엔 신나는 pop음악</s>\n",
      "epoch no.2 train no.205010  loss = 4.77672 avg_loss = 3.55184\n",
      "epoch no.2 train no.205020  loss = 2.70508 avg_loss = 3.51781\n",
      "epoch no.2 train no.205030  loss = 4.01777 avg_loss = 3.53531\n",
      "epoch no.2 train no.205040  loss = 2.54940 avg_loss = 3.51944\n",
      "epoch no.2 train no.205050  loss = 4.48776 avg_loss = 3.48794\n",
      "epoch no.2 train no.205060  loss = 5.32894 avg_loss = 3.53968\n",
      "epoch no.2 train no.205070  loss = 2.43750 avg_loss = 3.58874\n",
      "epoch no.2 train no.205080  loss = 4.08141 avg_loss = 3.58809\n",
      "epoch no.2 train no.205090  loss = 3.03435 avg_loss = 3.63694\n",
      "epoch no.2 train no.205100  loss = 2.78984 avg_loss = 3.59937\n",
      "epoch no.2 train no.205110  loss = 4.92739 avg_loss = 3.57689\n",
      "epoch no.2 train no.205120  loss = 5.77590 avg_loss = 3.59411\n",
      "epoch no.2 train no.205130  loss = 3.63179 avg_loss = 3.59374\n",
      "epoch no.2 train no.205140  loss = 3.83985 avg_loss = 3.56660\n",
      "epoch no.2 train no.205150  loss = 2.98338 avg_loss = 3.58100\n",
      "epoch no.2 train no.205160  loss = 3.33789 avg_loss = 3.56835\n",
      "epoch no.2 train no.205170  loss = 3.56408 avg_loss = 3.53442\n",
      "epoch no.2 train no.205180  loss = 3.82813 avg_loss = 3.52875\n",
      "epoch no.2 train no.205190  loss = 4.92754 avg_loss = 3.52031\n",
      "epoch no.2 train no.205200  loss = 2.46380 avg_loss = 3.51014\n",
      "epoch no.2 train no.205210  loss = 3.26705 avg_loss = 3.49503\n",
      "epoch no.2 train no.205220  loss = 2.87823 avg_loss = 3.46334\n",
      "epoch no.2 train no.205230  loss = 3.83056 avg_loss = 3.49008\n",
      "epoch no.2 train no.205240  loss = 3.28357 avg_loss = 3.50344\n",
      "epoch no.2 train no.205250  loss = 3.94352 avg_loss = 3.52110\n",
      "epoch no.2 train no.205260  loss = 2.01659 avg_loss = 3.44812\n",
      "epoch no.2 train no.205270  loss = 3.00892 avg_loss = 3.44208\n",
      "epoch no.2 train no.205280  loss = 2.06229 avg_loss = 3.38940\n",
      "epoch no.2 train no.205290  loss = 3.60883 avg_loss = 3.38758\n",
      "epoch no.2 train no.205300  loss = 2.86746 avg_loss = 3.41643\n",
      "epoch no.2 train no.205310  loss = 5.15744 avg_loss = 3.49528\n",
      "epoch no.2 train no.205320  loss = 4.48207 avg_loss = 3.50058\n",
      "epoch no.2 train no.205330  loss = 3.81247 avg_loss = 3.46944\n",
      "epoch no.2 train no.205340  loss = 2.40396 avg_loss = 3.50440\n",
      "epoch no.2 train no.205350  loss = 3.80344 avg_loss = 3.49684\n",
      "epoch no.2 train no.205360  loss = 5.03619 avg_loss = 3.50809\n",
      "epoch no.2 train no.205370  loss = 2.79667 avg_loss = 3.50399\n",
      "epoch no.2 train no.205380  loss = 1.94903 avg_loss = 3.52056\n",
      "epoch no.2 train no.205390  loss = 3.43025 avg_loss = 3.51215\n",
      "epoch no.2 train no.205400  loss = 4.16105 avg_loss = 3.49461\n",
      "epoch no.2 train no.205410  loss = 2.70392 avg_loss = 3.48096\n",
      "epoch no.2 train no.205420  loss = 2.02510 avg_loss = 3.42727\n",
      "epoch no.2 train no.205430  loss = 2.92549 avg_loss = 3.41151\n",
      "epoch no.2 train no.205440  loss = 3.81785 avg_loss = 3.40058\n",
      "epoch no.2 train no.205450  loss = 4.11287 avg_loss = 3.44849\n",
      "epoch no.2 train no.205460  loss = 4.17511 avg_loss = 3.46997\n",
      "epoch no.2 train no.205470  loss = 4.89580 avg_loss = 3.45133\n",
      "epoch no.2 train no.205480  loss = 4.04277 avg_loss = 3.45805\n",
      "epoch no.2 train no.205490  loss = 3.20399 avg_loss = 3.44015\n",
      "epoch no.2 train no.205500  loss = 2.86657 avg_loss = 3.43479\n",
      "epoch no.2 train no.205510  loss = 2.38280 avg_loss = 3.42070\n",
      "epoch no.2 train no.205520  loss = 2.90972 avg_loss = 3.44813\n",
      "epoch no.2 train no.205530  loss = 4.13631 avg_loss = 3.43166\n",
      "epoch no.2 train no.205540  loss = 3.26492 avg_loss = 3.42093\n",
      "epoch no.2 train no.205550  loss = 3.35551 avg_loss = 3.41474\n",
      "epoch no.2 train no.205560  loss = 3.57455 avg_loss = 3.41085\n",
      "epoch no.2 train no.205570  loss = 1.92514 avg_loss = 3.39104\n",
      "epoch no.2 train no.205580  loss = 2.05983 avg_loss = 3.34797\n",
      "epoch no.2 train no.205590  loss = 2.27592 avg_loss = 3.36953\n",
      "epoch no.2 train no.205600  loss = 2.38730 avg_loss = 3.41215\n",
      "epoch no.2 train no.205610  loss = 2.14368 avg_loss = 3.42128\n",
      "epoch no.2 train no.205620  loss = 4.53320 avg_loss = 3.42682\n",
      "epoch no.2 train no.205630  loss = 4.00736 avg_loss = 3.43041\n",
      "epoch no.2 train no.205640  loss = 3.10822 avg_loss = 3.44207\n",
      "epoch no.2 train no.205650  loss = 4.30829 avg_loss = 3.46310\n",
      "epoch no.2 train no.205660  loss = 2.04218 avg_loss = 3.46723\n",
      "epoch no.2 train no.205670  loss = 2.52506 avg_loss = 3.42093\n",
      "epoch no.2 train no.205680  loss = 3.32284 avg_loss = 3.46109\n",
      "epoch no.2 train no.205690  loss = 3.48908 avg_loss = 3.47069\n",
      "epoch no.2 train no.205700  loss = 2.99022 avg_loss = 3.47427\n",
      "epoch no.2 train no.205710  loss = 3.36175 avg_loss = 3.46882\n",
      "epoch no.2 train no.205720  loss = 2.26855 avg_loss = 3.50528\n",
      "epoch no.2 train no.205730  loss = 3.43269 avg_loss = 3.50346\n",
      "epoch no.2 train no.205740  loss = 3.37993 avg_loss = 3.55349\n",
      "epoch no.2 train no.205750  loss = 2.97214 avg_loss = 3.52486\n",
      "epoch no.2 train no.205760  loss = 5.67150 avg_loss = 3.58279\n",
      "epoch no.2 train no.205770  loss = 7.65331 avg_loss = 3.67296\n",
      "epoch no.2 train no.205780  loss = 3.94390 avg_loss = 3.65854\n",
      "epoch no.2 train no.205790  loss = 4.69051 avg_loss = 3.67033\n",
      "epoch no.2 train no.205800  loss = 2.75078 avg_loss = 3.67795\n",
      "epoch no.2 train no.205810  loss = 5.20954 avg_loss = 3.65243\n",
      "epoch no.2 train no.205820  loss = 2.61253 avg_loss = 3.65799\n",
      "epoch no.2 train no.205830  loss = 3.44239 avg_loss = 3.65559\n",
      "epoch no.2 train no.205840  loss = 4.89801 avg_loss = 3.61273\n",
      "epoch no.2 train no.205850  loss = 2.80253 avg_loss = 3.61392\n",
      "epoch no.2 train no.205860  loss = 4.32652 avg_loss = 3.65529\n",
      "epoch no.2 train no.205870  loss = 3.60300 avg_loss = 3.61209\n",
      "epoch no.2 train no.205880  loss = 3.35404 avg_loss = 3.56924\n",
      "epoch no.2 train no.205890  loss = 2.99316 avg_loss = 3.56788\n",
      "epoch no.2 train no.205900  loss = 4.42597 avg_loss = 3.55014\n",
      "epoch no.2 train no.205910  loss = 4.88972 avg_loss = 3.51165\n",
      "epoch no.2 train no.205920  loss = 2.81450 avg_loss = 3.53708\n",
      "epoch no.2 train no.205930  loss = 4.25265 avg_loss = 3.55652\n",
      "epoch no.2 train no.205940  loss = 4.27116 avg_loss = 3.53282\n",
      "epoch no.2 train no.205950  loss = 2.15339 avg_loss = 3.51517\n",
      "epoch no.2 train no.205960  loss = 4.72684 avg_loss = 3.52516\n",
      "epoch no.2 train no.205970  loss = 3.56273 avg_loss = 3.51631\n",
      "epoch no.2 train no.205980  loss = 4.68744 avg_loss = 3.50525\n",
      "epoch no.2 train no.205990  loss = 5.38063 avg_loss = 3.51172\n",
      "epoch no.2 train no.206000  loss = 5.24414 avg_loss = 3.53986\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '▁오면', '▁생각', '나는', '▁노래', '</s>']\n",
      "여름이 오면 생각나는 노래</s>\n",
      "epoch no.2 train no.206010  loss = 3.31327 avg_loss = 3.54748\n",
      "epoch no.2 train no.206020  loss = 2.73422 avg_loss = 3.54844\n",
      "epoch no.2 train no.206030  loss = 4.18690 avg_loss = 3.55519\n",
      "epoch no.2 train no.206040  loss = 3.75474 avg_loss = 3.50844\n",
      "epoch no.2 train no.206050  loss = 3.76718 avg_loss = 3.50698\n",
      "epoch no.2 train no.206060  loss = 3.60438 avg_loss = 3.47579\n",
      "epoch no.2 train no.206070  loss = 2.47824 avg_loss = 3.43545\n",
      "epoch no.2 train no.206080  loss = 3.24069 avg_loss = 3.43817\n",
      "epoch no.2 train no.206090  loss = 7.77495 avg_loss = 3.47670\n",
      "epoch no.2 train no.206100  loss = 3.05501 avg_loss = 3.47911\n",
      "epoch no.2 train no.206110  loss = 3.60962 avg_loss = 3.48792\n",
      "epoch no.2 train no.206120  loss = 4.29034 avg_loss = 3.50479\n",
      "epoch no.2 train no.206130  loss = 3.53377 avg_loss = 3.50791\n",
      "epoch no.2 train no.206140  loss = 3.19161 avg_loss = 3.49724\n",
      "epoch no.2 train no.206150  loss = 4.59382 avg_loss = 3.47871\n",
      "epoch no.2 train no.206160  loss = 2.87241 avg_loss = 3.52232\n",
      "epoch no.2 train no.206170  loss = 4.58452 avg_loss = 3.51409\n",
      "epoch no.2 train no.206180  loss = 1.24517 avg_loss = 3.50506\n",
      "epoch no.2 train no.206190  loss = 3.46985 avg_loss = 3.54825\n",
      "epoch no.2 train no.206200  loss = 3.58928 avg_loss = 3.54458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.206210  loss = 2.90492 avg_loss = 3.52090\n",
      "epoch no.2 train no.206220  loss = 3.35184 avg_loss = 3.50294\n",
      "epoch no.2 train no.206230  loss = 3.11081 avg_loss = 3.51827\n",
      "epoch no.2 train no.206240  loss = 4.20888 avg_loss = 3.48011\n",
      "epoch no.2 train no.206250  loss = 3.22348 avg_loss = 3.49048\n",
      "epoch no.2 train no.206260  loss = 5.42438 avg_loss = 3.47160\n",
      "epoch no.2 train no.206270  loss = 4.90448 avg_loss = 3.47145\n",
      "epoch no.2 train no.206280  loss = 5.04203 avg_loss = 3.50175\n",
      "epoch no.2 train no.206290  loss = 4.56637 avg_loss = 3.53032\n",
      "epoch no.2 train no.206300  loss = 2.68829 avg_loss = 3.46824\n",
      "epoch no.2 train no.206310  loss = 3.01806 avg_loss = 3.47430\n",
      "epoch no.2 train no.206320  loss = 4.27306 avg_loss = 3.50135\n",
      "epoch no.2 train no.206330  loss = 2.86057 avg_loss = 3.50280\n",
      "epoch no.2 train no.206340  loss = 2.44356 avg_loss = 3.53036\n",
      "epoch no.2 train no.206350  loss = 2.25818 avg_loss = 3.50984\n",
      "epoch no.2 train no.206360  loss = 5.42619 avg_loss = 3.54061\n",
      "epoch no.2 train no.206370  loss = 3.51624 avg_loss = 3.54370\n",
      "epoch no.2 train no.206380  loss = 2.96376 avg_loss = 3.50912\n",
      "epoch no.2 train no.206390  loss = 2.26925 avg_loss = 3.52984\n",
      "epoch no.2 train no.206400  loss = 2.04315 avg_loss = 3.50407\n",
      "epoch no.2 train no.206410  loss = 1.85197 avg_loss = 3.49918\n",
      "epoch no.2 train no.206420  loss = 4.24465 avg_loss = 3.51429\n",
      "epoch no.2 train no.206430  loss = 2.81849 avg_loss = 3.50560\n",
      "epoch no.2 train no.206440  loss = 4.88191 avg_loss = 3.56020\n",
      "epoch no.2 train no.206450  loss = 3.86683 avg_loss = 3.54256\n",
      "epoch no.2 train no.206460  loss = 3.03056 avg_loss = 3.59291\n",
      "epoch no.2 train no.206470  loss = 4.12645 avg_loss = 3.59448\n",
      "epoch no.2 train no.206480  loss = 4.08587 avg_loss = 3.59716\n",
      "epoch no.2 train no.206490  loss = 3.15801 avg_loss = 3.59476\n",
      "epoch no.2 train no.206500  loss = 1.95264 avg_loss = 3.52568\n",
      "epoch no.2 train no.206510  loss = 3.38732 avg_loss = 3.47554\n",
      "epoch no.2 train no.206520  loss = 2.08558 avg_loss = 3.44355\n",
      "epoch no.2 train no.206530  loss = 2.91748 avg_loss = 3.46167\n",
      "epoch no.2 train no.206540  loss = 6.70285 avg_loss = 3.49932\n",
      "epoch no.2 train no.206550  loss = 2.89393 avg_loss = 3.50500\n",
      "epoch no.2 train no.206560  loss = 4.54383 avg_loss = 3.49412\n",
      "epoch no.2 train no.206570  loss = 3.52144 avg_loss = 3.50719\n",
      "epoch no.2 train no.206580  loss = 5.06821 avg_loss = 3.51577\n",
      "epoch no.2 train no.206590  loss = 3.33136 avg_loss = 3.55858\n",
      "epoch no.2 train no.206600  loss = 1.84844 avg_loss = 3.56022\n",
      "epoch no.2 train no.206610  loss = 4.16128 avg_loss = 3.58845\n",
      "epoch no.2 train no.206620  loss = 2.85986 avg_loss = 3.55181\n",
      "epoch no.2 train no.206630  loss = 2.50075 avg_loss = 3.53428\n",
      "epoch no.2 train no.206640  loss = 5.06059 avg_loss = 3.58408\n",
      "epoch no.2 train no.206650  loss = 2.02706 avg_loss = 3.53004\n",
      "epoch no.2 train no.206660  loss = 2.64906 avg_loss = 3.49097\n",
      "epoch no.2 train no.206670  loss = 3.16700 avg_loss = 3.46521\n",
      "epoch no.2 train no.206680  loss = 5.57736 avg_loss = 3.49992\n",
      "epoch no.2 train no.206690  loss = 3.04250 avg_loss = 3.49238\n",
      "epoch no.2 train no.206700  loss = 3.26778 avg_loss = 3.51307\n",
      "epoch no.2 train no.206710  loss = 3.67802 avg_loss = 3.56500\n",
      "epoch no.2 train no.206720  loss = 2.56481 avg_loss = 3.56704\n",
      "epoch no.2 train no.206730  loss = 5.16495 avg_loss = 3.56933\n",
      "epoch no.2 train no.206740  loss = 2.60867 avg_loss = 3.52649\n",
      "epoch no.2 train no.206750  loss = 3.85146 avg_loss = 3.51697\n",
      "epoch no.2 train no.206760  loss = 3.00250 avg_loss = 3.48808\n",
      "epoch no.2 train no.206770  loss = 1.25668 avg_loss = 3.47401\n",
      "epoch no.2 train no.206780  loss = 4.24207 avg_loss = 3.47745\n",
      "epoch no.2 train no.206790  loss = 2.95854 avg_loss = 3.55474\n",
      "epoch no.2 train no.206800  loss = 4.05853 avg_loss = 3.56283\n",
      "epoch no.2 train no.206810  loss = 3.13881 avg_loss = 3.59512\n",
      "epoch no.2 train no.206820  loss = 5.01187 avg_loss = 3.60950\n",
      "epoch no.2 train no.206830  loss = 3.72872 avg_loss = 3.61334\n",
      "epoch no.2 train no.206840  loss = 2.98563 avg_loss = 3.59649\n",
      "epoch no.2 train no.206850  loss = 3.97617 avg_loss = 3.54968\n",
      "epoch no.2 train no.206860  loss = 2.57119 avg_loss = 3.54975\n",
      "epoch no.2 train no.206870  loss = 2.45443 avg_loss = 3.55784\n",
      "epoch no.2 train no.206880  loss = 2.40154 avg_loss = 3.48841\n",
      "epoch no.2 train no.206890  loss = 2.42801 avg_loss = 3.54501\n",
      "epoch no.2 train no.206900  loss = 4.03082 avg_loss = 3.55359\n",
      "epoch no.2 train no.206910  loss = 4.85472 avg_loss = 3.56689\n",
      "epoch no.2 train no.206920  loss = 3.93933 avg_loss = 3.57601\n",
      "epoch no.2 train no.206930  loss = 2.66123 avg_loss = 3.60014\n",
      "epoch no.2 train no.206940  loss = 5.04415 avg_loss = 3.55081\n",
      "epoch no.2 train no.206950  loss = 2.53244 avg_loss = 3.52479\n",
      "epoch no.2 train no.206960  loss = 3.21215 avg_loss = 3.47529\n",
      "epoch no.2 train no.206970  loss = 2.86383 avg_loss = 3.47955\n",
      "epoch no.2 train no.206980  loss = 4.29881 avg_loss = 3.52662\n",
      "epoch no.2 train no.206990  loss = 3.73128 avg_loss = 3.53080\n",
      "epoch no.2 train no.207000  loss = 3.86081 avg_loss = 3.53898\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '▁오면', '▁전', '▁꼭', '자', '▁좋은', '▁노래', '</s>']\n",
      "여름이 가기 전 듣기에 좋은 노래</s>\n",
      "epoch no.2 train no.207010  loss = 2.62000 avg_loss = 3.57806\n",
      "epoch no.2 train no.207020  loss = 2.15918 avg_loss = 3.60965\n",
      "epoch no.2 train no.207030  loss = 2.55915 avg_loss = 3.56842\n",
      "epoch no.2 train no.207040  loss = 3.22992 avg_loss = 3.57756\n",
      "epoch no.2 train no.207050  loss = 4.78351 avg_loss = 3.61104\n",
      "epoch no.2 train no.207060  loss = 1.69279 avg_loss = 3.59022\n",
      "epoch no.2 train no.207070  loss = 2.80434 avg_loss = 3.55474\n",
      "epoch no.2 train no.207080  loss = 3.36679 avg_loss = 3.55609\n",
      "epoch no.2 train no.207090  loss = 3.64381 avg_loss = 3.54924\n",
      "epoch no.2 train no.207100  loss = 5.10116 avg_loss = 3.55776\n",
      "epoch no.2 train no.207110  loss = 2.76780 avg_loss = 3.55344\n",
      "epoch no.2 train no.207120  loss = 3.23930 avg_loss = 3.48780\n",
      "epoch no.2 train no.207130  loss = 4.16643 avg_loss = 3.52018\n",
      "epoch no.2 train no.207140  loss = 5.00974 avg_loss = 3.53226\n",
      "epoch no.2 train no.207150  loss = 2.49060 avg_loss = 3.50882\n",
      "epoch no.2 train no.207160  loss = 3.81014 avg_loss = 3.51342\n",
      "epoch no.2 train no.207170  loss = 2.42615 avg_loss = 3.46321\n",
      "epoch no.2 train no.207180  loss = 4.09999 avg_loss = 3.47460\n",
      "epoch no.2 train no.207190  loss = 2.54698 avg_loss = 3.48675\n",
      "epoch no.2 train no.207200  loss = 4.49838 avg_loss = 3.48904\n",
      "epoch no.2 train no.207210  loss = 2.12466 avg_loss = 3.43603\n",
      "epoch no.2 train no.207220  loss = 3.73397 avg_loss = 3.47101\n",
      "epoch no.2 train no.207230  loss = 6.67394 avg_loss = 3.46454\n",
      "epoch no.2 train no.207240  loss = 4.52132 avg_loss = 3.46107\n",
      "epoch no.2 train no.207250  loss = 4.42521 avg_loss = 3.49866\n",
      "epoch no.2 train no.207260  loss = 3.13505 avg_loss = 3.51023\n",
      "epoch no.2 train no.207270  loss = 5.56152 avg_loss = 3.54042\n",
      "epoch no.2 train no.207280  loss = 4.46063 avg_loss = 3.55762\n",
      "epoch no.2 train no.207290  loss = 2.87865 avg_loss = 3.54787\n",
      "epoch no.2 train no.207300  loss = 3.18645 avg_loss = 3.56250\n",
      "epoch no.2 train no.207310  loss = 4.79204 avg_loss = 3.57353\n",
      "epoch no.2 train no.207320  loss = 2.46920 avg_loss = 3.55980\n",
      "epoch no.2 train no.207330  loss = 3.20441 avg_loss = 3.54465\n",
      "epoch no.2 train no.207340  loss = 2.48895 avg_loss = 3.52971\n",
      "epoch no.2 train no.207350  loss = 2.67534 avg_loss = 3.52165\n",
      "epoch no.2 train no.207360  loss = 2.40873 avg_loss = 3.47228\n",
      "epoch no.2 train no.207370  loss = 4.95638 avg_loss = 3.51474\n",
      "epoch no.2 train no.207380  loss = 6.93303 avg_loss = 3.53832\n",
      "epoch no.2 train no.207390  loss = 3.83000 avg_loss = 3.48485\n",
      "epoch no.2 train no.207400  loss = 3.93847 avg_loss = 3.47510\n",
      "epoch no.2 train no.207410  loss = 2.45834 avg_loss = 3.41425\n",
      "epoch no.2 train no.207420  loss = 3.95148 avg_loss = 3.47687\n",
      "epoch no.2 train no.207430  loss = 3.85009 avg_loss = 3.50590\n",
      "epoch no.2 train no.207440  loss = 5.13878 avg_loss = 3.58619\n",
      "epoch no.2 train no.207450  loss = 4.20867 avg_loss = 3.61500\n",
      "epoch no.2 train no.207460  loss = 4.42199 avg_loss = 3.61301\n",
      "epoch no.2 train no.207470  loss = 2.83211 avg_loss = 3.59339\n",
      "epoch no.2 train no.207480  loss = 2.95798 avg_loss = 3.59193\n",
      "epoch no.2 train no.207490  loss = 2.13456 avg_loss = 3.59371\n",
      "epoch no.2 train no.207500  loss = 4.02840 avg_loss = 3.59942\n",
      "epoch no.2 train no.207510  loss = 2.44129 avg_loss = 3.62487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.207520  loss = 3.00411 avg_loss = 3.63556\n",
      "epoch no.2 train no.207530  loss = 3.46973 avg_loss = 3.62813\n",
      "epoch no.2 train no.207540  loss = 2.53839 avg_loss = 3.60974\n",
      "epoch no.2 train no.207550  loss = 4.86746 avg_loss = 3.63196\n",
      "epoch no.2 train no.207560  loss = 4.38221 avg_loss = 3.67840\n",
      "epoch no.2 train no.207570  loss = 2.63305 avg_loss = 3.67753\n",
      "epoch no.2 train no.207580  loss = 3.30256 avg_loss = 3.63570\n",
      "epoch no.2 train no.207590  loss = 4.25995 avg_loss = 3.60810\n",
      "epoch no.2 train no.207600  loss = 6.37723 avg_loss = 3.59744\n",
      "epoch no.2 train no.207610  loss = 3.94442 avg_loss = 3.62288\n",
      "epoch no.2 train no.207620  loss = 2.26583 avg_loss = 3.54419\n",
      "epoch no.2 train no.207630  loss = 2.46343 avg_loss = 3.52332\n",
      "epoch no.2 train no.207640  loss = 2.13151 avg_loss = 3.53854\n",
      "epoch no.2 train no.207650  loss = 3.21618 avg_loss = 3.49926\n",
      "epoch no.2 train no.207660  loss = 4.00632 avg_loss = 3.53380\n",
      "epoch no.2 train no.207670  loss = 2.81652 avg_loss = 3.53239\n",
      "epoch no.2 train no.207680  loss = 2.99044 avg_loss = 3.56163\n",
      "epoch no.2 train no.207690  loss = 3.97943 avg_loss = 3.56258\n",
      "epoch no.2 train no.207700  loss = 3.04090 avg_loss = 3.56056\n",
      "epoch no.2 train no.207710  loss = 4.70771 avg_loss = 3.58043\n",
      "epoch no.2 train no.207720  loss = 2.83898 avg_loss = 3.56601\n",
      "epoch no.2 train no.207730  loss = 4.40867 avg_loss = 3.55541\n",
      "epoch no.2 train no.207740  loss = 3.67337 avg_loss = 3.53781\n",
      "epoch no.2 train no.207750  loss = 1.72339 avg_loss = 3.54599\n",
      "epoch no.2 train no.207760  loss = 2.85550 avg_loss = 3.52922\n",
      "epoch no.2 train no.207770  loss = 4.09811 avg_loss = 3.60336\n",
      "epoch no.2 train no.207780  loss = 2.89581 avg_loss = 3.52821\n",
      "epoch no.2 train no.207790  loss = 2.08932 avg_loss = 3.52476\n",
      "epoch no.2 train no.207800  loss = 3.33624 avg_loss = 3.57787\n",
      "epoch no.2 train no.207810  loss = 4.37889 avg_loss = 3.56167\n",
      "epoch no.2 train no.207820  loss = 4.24372 avg_loss = 3.59328\n",
      "epoch no.2 train no.207830  loss = 4.74730 avg_loss = 3.58308\n",
      "epoch no.2 train no.207840  loss = 4.14623 avg_loss = 3.60687\n",
      "epoch no.2 train no.207850  loss = 3.10269 avg_loss = 3.55562\n",
      "epoch no.2 train no.207860  loss = 2.15269 avg_loss = 3.50784\n",
      "epoch no.2 train no.207870  loss = 3.57809 avg_loss = 3.50191\n",
      "epoch no.2 train no.207880  loss = 2.21622 avg_loss = 3.49214\n",
      "epoch no.2 train no.207890  loss = 1.81512 avg_loss = 3.51233\n",
      "epoch no.2 train no.207900  loss = 4.73201 avg_loss = 3.54274\n",
      "epoch no.2 train no.207910  loss = 4.88530 avg_loss = 3.53990\n",
      "epoch no.2 train no.207920  loss = 3.16941 avg_loss = 3.48920\n",
      "epoch no.2 train no.207930  loss = 3.11951 avg_loss = 3.46362\n",
      "epoch no.2 train no.207940  loss = 5.07792 avg_loss = 3.47700\n",
      "epoch no.2 train no.207950  loss = 4.84384 avg_loss = 3.45387\n",
      "epoch no.2 train no.207960  loss = 3.75304 avg_loss = 3.45107\n",
      "epoch no.2 train no.207970  loss = 4.33836 avg_loss = 3.48144\n",
      "epoch no.2 train no.207980  loss = 4.01083 avg_loss = 3.46240\n",
      "epoch no.2 train no.207990  loss = 4.23623 avg_loss = 3.49596\n",
      "epoch no.2 train no.208000  loss = 3.22555 avg_loss = 3.56017\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '한', '▁음악', '</s>']\n",
      "여름밤에 듣는 잔잔한 음악</s>\n",
      "epoch no.2 train no.208010  loss = 4.18417 avg_loss = 3.59957\n",
      "epoch no.2 train no.208020  loss = 3.03315 avg_loss = 3.55210\n",
      "epoch no.2 train no.208030  loss = 4.62301 avg_loss = 3.51712\n",
      "epoch no.2 train no.208040  loss = 1.95573 avg_loss = 3.46070\n",
      "epoch no.2 train no.208050  loss = 2.28447 avg_loss = 3.44162\n",
      "epoch no.2 train no.208060  loss = 3.24681 avg_loss = 3.45609\n",
      "epoch no.2 train no.208070  loss = 2.27656 avg_loss = 3.44832\n",
      "epoch no.2 train no.208080  loss = 5.23802 avg_loss = 3.49471\n",
      "epoch no.2 train no.208090  loss = 2.14336 avg_loss = 3.45990\n",
      "epoch no.2 train no.208100  loss = 1.91770 avg_loss = 3.43727\n",
      "epoch no.2 train no.208110  loss = 4.58168 avg_loss = 3.45709\n",
      "epoch no.2 train no.208120  loss = 3.73689 avg_loss = 3.47172\n",
      "epoch no.2 train no.208130  loss = 2.69957 avg_loss = 3.47756\n",
      "epoch no.2 train no.208140  loss = 1.82191 avg_loss = 3.43546\n",
      "epoch no.2 train no.208150  loss = 3.55940 avg_loss = 3.38602\n",
      "epoch no.2 train no.208160  loss = 3.09474 avg_loss = 3.39665\n",
      "epoch no.2 train no.208170  loss = 3.69963 avg_loss = 3.37710\n",
      "epoch no.2 train no.208180  loss = 4.77971 avg_loss = 3.41107\n",
      "epoch no.2 train no.208190  loss = 3.22055 avg_loss = 3.45394\n",
      "epoch no.2 train no.208200  loss = 4.90167 avg_loss = 3.45346\n",
      "epoch no.2 train no.208210  loss = 2.69866 avg_loss = 3.42676\n",
      "epoch no.2 train no.208220  loss = 4.00052 avg_loss = 3.43559\n",
      "epoch no.2 train no.208230  loss = 2.86093 avg_loss = 3.43336\n",
      "epoch no.2 train no.208240  loss = 5.94813 avg_loss = 3.42957\n",
      "epoch no.2 train no.208250  loss = 2.96616 avg_loss = 3.46676\n",
      "epoch no.2 train no.208260  loss = 4.58479 avg_loss = 3.51435\n",
      "epoch no.2 train no.208270  loss = 3.51861 avg_loss = 3.49721\n",
      "epoch no.2 train no.208280  loss = 6.84770 avg_loss = 3.48952\n",
      "epoch no.2 train no.208290  loss = 1.83356 avg_loss = 3.45090\n",
      "epoch no.2 train no.208300  loss = 4.31446 avg_loss = 3.44207\n",
      "epoch no.2 train no.208310  loss = 3.84354 avg_loss = 3.47929\n",
      "epoch no.2 train no.208320  loss = 2.90786 avg_loss = 3.47538\n",
      "epoch no.2 train no.208330  loss = 2.63028 avg_loss = 3.51272\n",
      "epoch no.2 train no.208340  loss = 6.14295 avg_loss = 3.51513\n",
      "epoch no.2 train no.208350  loss = 2.30212 avg_loss = 3.51574\n",
      "epoch no.2 train no.208360  loss = 3.39226 avg_loss = 3.53458\n",
      "epoch no.2 train no.208370  loss = 3.85969 avg_loss = 3.54755\n",
      "epoch no.2 train no.208380  loss = 3.23792 avg_loss = 3.56535\n",
      "epoch no.2 train no.208390  loss = 3.35205 avg_loss = 3.54259\n",
      "epoch no.2 train no.208400  loss = 5.45659 avg_loss = 3.54603\n",
      "epoch no.2 train no.208410  loss = 4.74171 avg_loss = 3.56546\n",
      "epoch no.2 train no.208420  loss = 6.57981 avg_loss = 3.61464\n",
      "epoch no.2 train no.208430  loss = 2.88796 avg_loss = 3.56839\n",
      "epoch no.2 train no.208440  loss = 1.92755 avg_loss = 3.58946\n",
      "epoch no.2 train no.208450  loss = 2.65347 avg_loss = 3.64725\n",
      "epoch no.2 train no.208460  loss = 4.19735 avg_loss = 3.64671\n",
      "epoch no.2 train no.208470  loss = 3.61424 avg_loss = 3.66583\n",
      "epoch no.2 train no.208480  loss = 3.19593 avg_loss = 3.61695\n",
      "epoch no.2 train no.208490  loss = 3.85859 avg_loss = 3.57849\n",
      "epoch no.2 train no.208500  loss = 2.90867 avg_loss = 3.54677\n",
      "epoch no.2 train no.208510  loss = 3.50468 avg_loss = 3.56678\n",
      "epoch no.2 train no.208520  loss = 3.54915 avg_loss = 3.52116\n",
      "epoch no.2 train no.208530  loss = 3.31711 avg_loss = 3.53046\n",
      "epoch no.2 train no.208540  loss = 4.27161 avg_loss = 3.51635\n",
      "epoch no.2 train no.208550  loss = 2.18254 avg_loss = 3.47308\n",
      "epoch no.2 train no.208560  loss = 2.52696 avg_loss = 3.47821\n",
      "epoch no.2 train no.208570  loss = 2.55502 avg_loss = 3.45483\n",
      "epoch no.2 train no.208580  loss = 3.74414 avg_loss = 3.42521\n",
      "epoch no.2 train no.208590  loss = 2.70955 avg_loss = 3.38427\n",
      "epoch no.2 train no.208600  loss = 6.77491 avg_loss = 3.46652\n",
      "epoch no.2 train no.208610  loss = 2.78025 avg_loss = 3.43662\n",
      "epoch no.2 train no.208620  loss = 2.11924 avg_loss = 3.43853\n",
      "epoch no.2 train no.208630  loss = 2.20972 avg_loss = 3.49727\n",
      "epoch no.2 train no.208640  loss = 2.67697 avg_loss = 3.51616\n",
      "epoch no.2 train no.208650  loss = 4.25339 avg_loss = 3.52181\n",
      "epoch no.2 train no.208660  loss = 5.91438 avg_loss = 3.55435\n",
      "epoch no.2 train no.208670  loss = 1.81160 avg_loss = 3.53142\n",
      "epoch no.2 train no.208680  loss = 4.51557 avg_loss = 3.52288\n",
      "epoch no.2 train no.208690  loss = 0.84343 avg_loss = 3.56624\n",
      "epoch no.2 train no.208700  loss = 2.46107 avg_loss = 3.56941\n",
      "epoch no.2 train no.208710  loss = 2.52519 avg_loss = 3.59844\n",
      "epoch no.2 train no.208720  loss = 6.17514 avg_loss = 3.63828\n",
      "epoch no.2 train no.208730  loss = 1.46628 avg_loss = 3.63944\n",
      "epoch no.2 train no.208740  loss = 4.42999 avg_loss = 3.66970\n",
      "epoch no.2 train no.208750  loss = 5.40237 avg_loss = 3.64975\n",
      "epoch no.2 train no.208760  loss = 2.27662 avg_loss = 3.57905\n",
      "epoch no.2 train no.208770  loss = 3.68956 avg_loss = 3.54847\n",
      "epoch no.2 train no.208780  loss = 4.44186 avg_loss = 3.52128\n",
      "epoch no.2 train no.208790  loss = 4.60109 avg_loss = 3.56024\n",
      "epoch no.2 train no.208800  loss = 4.74998 avg_loss = 3.60551\n",
      "epoch no.2 train no.208810  loss = 3.45202 avg_loss = 3.60346\n",
      "epoch no.2 train no.208820  loss = 4.67827 avg_loss = 3.67478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.208830  loss = 4.32258 avg_loss = 3.64986\n",
      "epoch no.2 train no.208840  loss = 2.12796 avg_loss = 3.64283\n",
      "epoch no.2 train no.208850  loss = 2.46926 avg_loss = 3.59282\n",
      "epoch no.2 train no.208860  loss = 3.42246 avg_loss = 3.58900\n",
      "epoch no.2 train no.208870  loss = 2.78315 avg_loss = 3.58943\n",
      "epoch no.2 train no.208880  loss = 3.39984 avg_loss = 3.61623\n",
      "epoch no.2 train no.208890  loss = 2.27885 avg_loss = 3.58891\n",
      "epoch no.2 train no.208900  loss = 2.29137 avg_loss = 3.55639\n",
      "epoch no.2 train no.208910  loss = 2.41238 avg_loss = 3.58899\n",
      "epoch no.2 train no.208920  loss = 4.61476 avg_loss = 3.56144\n",
      "epoch no.2 train no.208930  loss = 4.95238 avg_loss = 3.62333\n",
      "epoch no.2 train no.208940  loss = 4.77878 avg_loss = 3.62698\n",
      "epoch no.2 train no.208950  loss = 3.98254 avg_loss = 3.61980\n",
      "epoch no.2 train no.208960  loss = 2.69018 avg_loss = 3.55276\n",
      "epoch no.2 train no.208970  loss = 5.03611 avg_loss = 3.61864\n",
      "epoch no.2 train no.208980  loss = 2.39567 avg_loss = 3.63831\n",
      "epoch no.2 train no.208990  loss = 3.07569 avg_loss = 3.59879\n",
      "epoch no.2 train no.209000  loss = 2.47669 avg_loss = 3.58700\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '리스트', '</s>']\n",
      "여름밤의 플레이리스트</s>\n",
      "epoch no.2 train no.209010  loss = 3.26418 avg_loss = 3.51023\n",
      "epoch no.2 train no.209020  loss = 1.49451 avg_loss = 3.48166\n",
      "epoch no.2 train no.209030  loss = 3.00730 avg_loss = 3.50058\n",
      "epoch no.2 train no.209040  loss = 2.78872 avg_loss = 3.41688\n",
      "epoch no.2 train no.209050  loss = 2.66643 avg_loss = 3.35975\n",
      "epoch no.2 train no.209060  loss = 1.94049 avg_loss = 3.34607\n",
      "epoch no.2 train no.209070  loss = 2.75597 avg_loss = 3.36440\n",
      "epoch no.2 train no.209080  loss = 3.19757 avg_loss = 3.36684\n",
      "epoch no.2 train no.209090  loss = 3.79479 avg_loss = 3.36129\n",
      "epoch no.2 train no.209100  loss = 4.61905 avg_loss = 3.39683\n",
      "epoch no.2 train no.209110  loss = 2.98132 avg_loss = 3.39607\n",
      "epoch no.2 train no.209120  loss = 3.04437 avg_loss = 3.38667\n",
      "epoch no.2 train no.209130  loss = 2.86151 avg_loss = 3.34469\n",
      "epoch no.2 train no.209140  loss = 3.94513 avg_loss = 3.35018\n",
      "epoch no.2 train no.209150  loss = 3.55412 avg_loss = 3.32984\n",
      "epoch no.2 train no.209160  loss = 4.87764 avg_loss = 3.37936\n",
      "epoch no.2 train no.209170  loss = 3.20180 avg_loss = 3.38840\n",
      "epoch no.2 train no.209180  loss = 5.66270 avg_loss = 3.41519\n",
      "epoch no.2 train no.209190  loss = 3.47947 avg_loss = 3.47383\n",
      "epoch no.2 train no.209200  loss = 3.12856 avg_loss = 3.48404\n",
      "epoch no.2 train no.209210  loss = 2.35314 avg_loss = 3.51611\n",
      "epoch no.2 train no.209220  loss = 4.69182 avg_loss = 3.54198\n",
      "epoch no.2 train no.209230  loss = 3.34339 avg_loss = 3.57196\n",
      "epoch no.2 train no.209240  loss = 3.64262 avg_loss = 3.54614\n",
      "epoch no.2 train no.209250  loss = 3.39581 avg_loss = 3.51891\n",
      "epoch no.2 train no.209260  loss = 4.77237 avg_loss = 3.51446\n",
      "epoch no.2 train no.209270  loss = 4.19924 avg_loss = 3.58357\n",
      "epoch no.2 train no.209280  loss = 3.94042 avg_loss = 3.58402\n",
      "epoch no.2 train no.209290  loss = 2.99431 avg_loss = 3.58674\n",
      "epoch no.2 train no.209300  loss = 2.57902 avg_loss = 3.56312\n",
      "epoch no.2 train no.209310  loss = 2.64167 avg_loss = 3.54307\n",
      "epoch no.2 train no.209320  loss = 3.34373 avg_loss = 3.58231\n",
      "epoch no.2 train no.209330  loss = 4.12583 avg_loss = 3.60763\n",
      "epoch no.2 train no.209340  loss = 2.88816 avg_loss = 3.61532\n",
      "epoch no.2 train no.209350  loss = 4.30740 avg_loss = 3.57927\n",
      "epoch no.2 train no.209360  loss = 4.36008 avg_loss = 3.58940\n",
      "epoch no.2 train no.209370  loss = 5.67095 avg_loss = 3.57336\n",
      "epoch no.2 train no.209380  loss = 3.28928 avg_loss = 3.54083\n",
      "epoch no.2 train no.209390  loss = 2.25953 avg_loss = 3.56243\n",
      "epoch no.2 train no.209400  loss = 2.25333 avg_loss = 3.54632\n",
      "epoch no.2 train no.209410  loss = 4.71526 avg_loss = 3.53230\n",
      "epoch no.2 train no.209420  loss = 4.15074 avg_loss = 3.53917\n",
      "epoch no.2 train no.209430  loss = 5.54111 avg_loss = 3.54173\n",
      "epoch no.2 train no.209440  loss = 3.55738 avg_loss = 3.53626\n",
      "epoch no.2 train no.209450  loss = 2.47722 avg_loss = 3.52521\n",
      "epoch no.2 train no.209460  loss = 2.82029 avg_loss = 3.52026\n",
      "epoch no.2 train no.209470  loss = 4.14893 avg_loss = 3.52752\n",
      "epoch no.2 train no.209480  loss = 3.50462 avg_loss = 3.55010\n",
      "epoch no.2 train no.209490  loss = 6.56812 avg_loss = 3.56914\n",
      "epoch no.2 train no.209500  loss = 2.90416 avg_loss = 3.56779\n",
      "epoch no.2 train no.209510  loss = 4.71489 avg_loss = 3.57597\n",
      "epoch no.2 train no.209520  loss = 2.49240 avg_loss = 3.54473\n",
      "epoch no.2 train no.209530  loss = 2.00827 avg_loss = 3.51024\n",
      "epoch no.2 train no.209540  loss = 2.28041 avg_loss = 3.45906\n",
      "epoch no.2 train no.209550  loss = 2.30159 avg_loss = 3.46630\n",
      "epoch no.2 train no.209560  loss = 3.25537 avg_loss = 3.46982\n",
      "epoch no.2 train no.209570  loss = 2.67756 avg_loss = 3.47609\n",
      "epoch no.2 train no.209580  loss = 3.16975 avg_loss = 3.44320\n",
      "epoch no.2 train no.209590  loss = 3.00396 avg_loss = 3.44666\n",
      "epoch no.2 train no.209600  loss = 2.75816 avg_loss = 3.41792\n",
      "epoch no.2 train no.209610  loss = 2.60653 avg_loss = 3.36975\n",
      "epoch no.2 train no.209620  loss = 5.85232 avg_loss = 3.39865\n",
      "epoch no.2 train no.209630  loss = 2.53694 avg_loss = 3.40589\n",
      "epoch no.2 train no.209640  loss = 2.75515 avg_loss = 3.43087\n",
      "epoch no.2 train no.209650  loss = 4.32350 avg_loss = 3.47857\n",
      "epoch no.2 train no.209660  loss = 2.79808 avg_loss = 3.48031\n",
      "epoch no.2 train no.209670  loss = 4.01427 avg_loss = 3.48105\n",
      "epoch no.2 train no.209680  loss = 3.53203 avg_loss = 3.50685\n",
      "epoch no.2 train no.209690  loss = 2.68899 avg_loss = 3.49586\n",
      "epoch no.2 train no.209700  loss = 3.99664 avg_loss = 3.54752\n",
      "epoch no.2 train no.209710  loss = 2.61542 avg_loss = 3.52042\n",
      "epoch no.2 train no.209720  loss = 4.08121 avg_loss = 3.52618\n",
      "epoch no.2 train no.209730  loss = 3.57429 avg_loss = 3.51255\n",
      "epoch no.2 train no.209740  loss = 4.46215 avg_loss = 3.54648\n",
      "epoch no.2 train no.209750  loss = 2.59336 avg_loss = 3.52830\n",
      "epoch no.2 train no.209760  loss = 2.98032 avg_loss = 3.52676\n",
      "epoch no.2 train no.209770  loss = 1.95516 avg_loss = 3.56542\n",
      "epoch no.2 train no.209780  loss = 4.79245 avg_loss = 3.58045\n",
      "epoch no.2 train no.209790  loss = 3.88484 avg_loss = 3.59167\n",
      "epoch no.2 train no.209800  loss = 3.13538 avg_loss = 3.59148\n",
      "epoch no.2 train no.209810  loss = 2.40208 avg_loss = 3.59464\n",
      "epoch no.2 train no.209820  loss = 2.71804 avg_loss = 3.57878\n",
      "epoch no.2 train no.209830  loss = 3.71205 avg_loss = 3.64506\n",
      "epoch no.2 train no.209840  loss = 4.01371 avg_loss = 3.68265\n",
      "epoch no.2 train no.209850  loss = 3.69329 avg_loss = 3.61952\n",
      "epoch no.2 train no.209860  loss = 4.99158 avg_loss = 3.58753\n",
      "epoch no.2 train no.209870  loss = 4.74268 avg_loss = 3.61764\n",
      "epoch no.2 train no.209880  loss = 3.48450 avg_loss = 3.65284\n",
      "epoch no.2 train no.209890  loss = 5.30309 avg_loss = 3.61598\n",
      "epoch no.2 train no.209900  loss = 2.77904 avg_loss = 3.57315\n",
      "epoch no.2 train no.209910  loss = 3.10572 avg_loss = 3.61692\n",
      "epoch no.2 train no.209920  loss = 2.61333 avg_loss = 3.59246\n",
      "epoch no.2 train no.209930  loss = 3.52539 avg_loss = 3.58342\n",
      "epoch no.2 train no.209940  loss = 3.24938 avg_loss = 3.56968\n",
      "epoch no.2 train no.209950  loss = 4.27673 avg_loss = 3.60326\n",
      "epoch no.2 train no.209960  loss = 3.72073 avg_loss = 3.60494\n",
      "epoch no.2 train no.209970  loss = 3.71235 avg_loss = 3.59290\n",
      "epoch no.2 train no.209980  loss = 1.79512 avg_loss = 3.60726\n",
      "epoch no.2 train no.209990  loss = 2.96524 avg_loss = 3.56490\n",
      "epoch no.2 train no.210000  loss = 3.49019 avg_loss = 3.54294\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '▁끝', '자', '락', '▁잡고', '</s>']\n",
      "여름의 끝자락을 잡고</s>\n",
      "epoch no.2 train no.210010  loss = 1.99278 avg_loss = 3.46838\n",
      "epoch no.2 train no.210020  loss = 4.32013 avg_loss = 3.47682\n",
      "epoch no.2 train no.210030  loss = 2.18167 avg_loss = 3.50089\n",
      "epoch no.2 train no.210040  loss = 2.83960 avg_loss = 3.51612\n",
      "epoch no.2 train no.210050  loss = 4.43363 avg_loss = 3.56021\n",
      "epoch no.2 train no.210060  loss = 2.01752 avg_loss = 3.52311\n",
      "epoch no.2 train no.210070  loss = 3.18766 avg_loss = 3.47317\n",
      "epoch no.2 train no.210080  loss = 1.51381 avg_loss = 3.51034\n",
      "epoch no.2 train no.210090  loss = 5.33045 avg_loss = 3.55124\n",
      "epoch no.2 train no.210100  loss = 3.63318 avg_loss = 3.53985\n",
      "epoch no.2 train no.210110  loss = 4.76373 avg_loss = 3.60895\n",
      "epoch no.2 train no.210120  loss = 6.20994 avg_loss = 3.61788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.210130  loss = 1.53601 avg_loss = 3.56895\n",
      "epoch no.2 train no.210140  loss = 2.30540 avg_loss = 3.56726\n",
      "epoch no.2 train no.210150  loss = 6.19504 avg_loss = 3.57013\n",
      "epoch no.2 train no.210160  loss = 2.86249 avg_loss = 3.53208\n",
      "epoch no.2 train no.210170  loss = 4.41348 avg_loss = 3.55114\n",
      "epoch no.2 train no.210180  loss = 2.74157 avg_loss = 3.53186\n",
      "epoch no.2 train no.210190  loss = 2.88845 avg_loss = 3.58470\n",
      "epoch no.2 train no.210200  loss = 1.39028 avg_loss = 3.56334\n",
      "epoch no.2 train no.210210  loss = 2.84021 avg_loss = 3.55843\n",
      "epoch no.2 train no.210220  loss = 3.94411 avg_loss = 3.55516\n",
      "epoch no.2 train no.210230  loss = 5.78854 avg_loss = 3.57995\n",
      "epoch no.2 train no.210240  loss = 5.77704 avg_loss = 3.56901\n",
      "epoch no.2 train no.210250  loss = 3.99915 avg_loss = 3.53186\n",
      "epoch no.2 train no.210260  loss = 2.59024 avg_loss = 3.51347\n",
      "epoch no.2 train no.210270  loss = 3.62781 avg_loss = 3.54285\n",
      "epoch no.2 train no.210280  loss = 1.05807 avg_loss = 3.51267\n",
      "epoch no.2 train no.210290  loss = 1.18777 avg_loss = 3.45636\n",
      "epoch no.2 train no.210300  loss = 1.98408 avg_loss = 3.46299\n",
      "epoch no.2 train no.210310  loss = 5.09073 avg_loss = 3.49000\n",
      "epoch no.2 train no.210320  loss = 4.57583 avg_loss = 3.53221\n",
      "epoch no.2 train no.210330  loss = 2.41718 avg_loss = 3.51633\n",
      "epoch no.2 train no.210340  loss = 2.87814 avg_loss = 3.51598\n",
      "epoch no.2 train no.210350  loss = 6.50501 avg_loss = 3.53803\n",
      "epoch no.2 train no.210360  loss = 3.69393 avg_loss = 3.54313\n",
      "epoch no.2 train no.210370  loss = 2.07135 avg_loss = 3.57683\n",
      "epoch no.2 train no.210380  loss = 2.22416 avg_loss = 3.54738\n",
      "epoch no.2 train no.210390  loss = 3.03738 avg_loss = 3.52988\n",
      "epoch no.2 train no.210400  loss = 2.83286 avg_loss = 3.49570\n",
      "epoch no.2 train no.210410  loss = 4.35455 avg_loss = 3.51322\n",
      "epoch no.2 train no.210420  loss = 3.68787 avg_loss = 3.50099\n",
      "epoch no.2 train no.210430  loss = 3.09740 avg_loss = 3.53924\n",
      "epoch no.2 train no.210440  loss = 3.34830 avg_loss = 3.51978\n",
      "epoch no.2 train no.210450  loss = 2.98450 avg_loss = 3.51534\n",
      "epoch no.2 train no.210460  loss = 2.30216 avg_loss = 3.49709\n",
      "epoch no.2 train no.210470  loss = 4.44534 avg_loss = 3.45007\n",
      "epoch no.2 train no.210480  loss = 2.12704 avg_loss = 3.42674\n",
      "epoch no.2 train no.210490  loss = 5.97214 avg_loss = 3.44411\n",
      "epoch no.2 train no.210500  loss = 3.98138 avg_loss = 3.46076\n",
      "epoch no.2 train no.210510  loss = 3.67319 avg_loss = 3.48333\n",
      "epoch no.2 train no.210520  loss = 3.86739 avg_loss = 3.46318\n",
      "epoch no.2 train no.210530  loss = 3.93812 avg_loss = 3.46758\n",
      "epoch no.2 train no.210540  loss = 5.34000 avg_loss = 3.47516\n",
      "epoch no.2 train no.210550  loss = 3.29239 avg_loss = 3.47555\n",
      "epoch no.2 train no.210560  loss = 6.13269 avg_loss = 3.48568\n",
      "epoch no.2 train no.210570  loss = 4.04403 avg_loss = 3.48879\n",
      "epoch no.2 train no.210580  loss = 2.16159 avg_loss = 3.44449\n",
      "epoch no.2 train no.210590  loss = 2.95683 avg_loss = 3.47934\n",
      "epoch no.2 train no.210600  loss = 3.93388 avg_loss = 3.46867\n",
      "epoch no.2 train no.210610  loss = 4.96871 avg_loss = 3.46782\n",
      "epoch no.2 train no.210620  loss = 2.79246 avg_loss = 3.49384\n",
      "epoch no.2 train no.210630  loss = 4.23664 avg_loss = 3.49163\n",
      "epoch no.2 train no.210640  loss = 5.64683 avg_loss = 3.52539\n",
      "epoch no.2 train no.210650  loss = 2.03941 avg_loss = 3.47778\n",
      "epoch no.2 train no.210660  loss = 4.39117 avg_loss = 3.49410\n",
      "epoch no.2 train no.210670  loss = 4.37668 avg_loss = 3.46169\n",
      "epoch no.2 train no.210680  loss = 3.88071 avg_loss = 3.45833\n",
      "epoch no.2 train no.210690  loss = 1.74870 avg_loss = 3.43590\n",
      "epoch no.2 train no.210700  loss = 3.26759 avg_loss = 3.43972\n",
      "epoch no.2 train no.210710  loss = 3.46081 avg_loss = 3.41219\n",
      "epoch no.2 train no.210720  loss = 4.76177 avg_loss = 3.42246\n",
      "epoch no.2 train no.210730  loss = 3.11139 avg_loss = 3.46623\n",
      "epoch no.2 train no.210740  loss = 3.21745 avg_loss = 3.45332\n",
      "epoch no.2 train no.210750  loss = 4.38688 avg_loss = 3.48575\n",
      "epoch no.2 train no.210760  loss = 3.66388 avg_loss = 3.44002\n",
      "epoch no.2 train no.210770  loss = 1.99306 avg_loss = 3.49180\n",
      "epoch no.2 train no.210780  loss = 4.08314 avg_loss = 3.54093\n",
      "epoch no.2 train no.210790  loss = 4.14735 avg_loss = 3.50967\n",
      "epoch no.2 train no.210800  loss = 1.66344 avg_loss = 3.50992\n",
      "epoch no.2 train no.210810  loss = 3.16906 avg_loss = 3.53630\n",
      "epoch no.2 train no.210820  loss = 3.37891 avg_loss = 3.51710\n",
      "epoch no.2 train no.210830  loss = 3.52496 avg_loss = 3.48162\n",
      "epoch no.2 train no.210840  loss = 3.31329 avg_loss = 3.47650\n",
      "epoch no.2 train no.210850  loss = 3.08596 avg_loss = 3.45427\n",
      "epoch no.2 train no.210860  loss = 2.56068 avg_loss = 3.43199\n",
      "epoch no.2 train no.210870  loss = 2.94994 avg_loss = 3.44051\n",
      "epoch no.2 train no.210880  loss = 3.95463 avg_loss = 3.45821\n",
      "epoch no.2 train no.210890  loss = 4.11870 avg_loss = 3.53074\n",
      "epoch no.2 train no.210900  loss = 3.98123 avg_loss = 3.53252\n",
      "epoch no.2 train no.210910  loss = 3.64090 avg_loss = 3.49103\n",
      "epoch no.2 train no.210920  loss = 4.84621 avg_loss = 3.54284\n",
      "epoch no.2 train no.210930  loss = 2.11295 avg_loss = 3.52169\n",
      "epoch no.2 train no.210940  loss = 4.26573 avg_loss = 3.53169\n",
      "epoch no.2 train no.210950  loss = 2.68558 avg_loss = 3.53610\n",
      "epoch no.2 train no.210960  loss = 4.36841 avg_loss = 3.60746\n",
      "epoch no.2 train no.210970  loss = 2.33742 avg_loss = 3.60860\n",
      "epoch no.2 train no.210980  loss = 3.43427 avg_loss = 3.59767\n",
      "epoch no.2 train no.210990  loss = 2.72155 avg_loss = 3.63001\n",
      "epoch no.2 train no.211000  loss = 4.00889 avg_loss = 3.59097\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 음악</s>\n",
      "epoch no.2 train no.211010  loss = 6.27256 avg_loss = 3.61606\n",
      "epoch no.2 train no.211020  loss = 6.28624 avg_loss = 3.63494\n",
      "epoch no.2 train no.211030  loss = 2.53471 avg_loss = 3.60584\n",
      "epoch no.2 train no.211040  loss = 2.39148 avg_loss = 3.53848\n",
      "epoch no.2 train no.211050  loss = 4.07476 avg_loss = 3.54241\n",
      "epoch no.2 train no.211060  loss = 2.98937 avg_loss = 3.52278\n",
      "epoch no.2 train no.211070  loss = 2.94714 avg_loss = 3.49246\n",
      "epoch no.2 train no.211080  loss = 5.40333 avg_loss = 3.52633\n",
      "epoch no.2 train no.211090  loss = 3.34144 avg_loss = 3.52806\n",
      "epoch no.2 train no.211100  loss = 2.72323 avg_loss = 3.52181\n",
      "epoch no.2 train no.211110  loss = 2.32559 avg_loss = 3.60093\n",
      "epoch no.2 train no.211120  loss = 3.10996 avg_loss = 3.55724\n",
      "epoch no.2 train no.211130  loss = 3.38212 avg_loss = 3.51730\n",
      "epoch no.2 train no.211140  loss = 2.58441 avg_loss = 3.45877\n",
      "epoch no.2 train no.211150  loss = 3.79121 avg_loss = 3.42830\n",
      "epoch no.2 train no.211160  loss = 4.77887 avg_loss = 3.45291\n",
      "epoch no.2 train no.211170  loss = 2.80999 avg_loss = 3.50170\n",
      "epoch no.2 train no.211180  loss = 3.34138 avg_loss = 3.54647\n",
      "epoch no.2 train no.211190  loss = 2.79777 avg_loss = 3.49579\n",
      "epoch no.2 train no.211200  loss = 5.57360 avg_loss = 3.46308\n",
      "epoch no.2 train no.211210  loss = 3.24058 avg_loss = 3.42785\n",
      "epoch no.2 train no.211220  loss = 3.11699 avg_loss = 3.44628\n",
      "epoch no.2 train no.211230  loss = 5.67406 avg_loss = 3.51271\n",
      "epoch no.2 train no.211240  loss = 2.73289 avg_loss = 3.54833\n",
      "epoch no.2 train no.211250  loss = 3.17889 avg_loss = 3.52499\n",
      "epoch no.2 train no.211260  loss = 2.61755 avg_loss = 3.47662\n",
      "epoch no.2 train no.211270  loss = 4.33371 avg_loss = 3.43187\n",
      "epoch no.2 train no.211280  loss = 2.28481 avg_loss = 3.39780\n",
      "epoch no.2 train no.211290  loss = 3.49411 avg_loss = 3.37382\n",
      "epoch no.2 train no.211300  loss = 3.63108 avg_loss = 3.36174\n",
      "epoch no.2 train no.211310  loss = 4.78743 avg_loss = 3.38199\n",
      "epoch no.2 train no.211320  loss = 4.17465 avg_loss = 3.39990\n",
      "epoch no.2 train no.211330  loss = 4.01400 avg_loss = 3.37635\n",
      "epoch no.2 train no.211340  loss = 4.67324 avg_loss = 3.32628\n",
      "epoch no.2 train no.211350  loss = 3.67082 avg_loss = 3.31868\n",
      "epoch no.2 train no.211360  loss = 4.22462 avg_loss = 3.35396\n",
      "epoch no.2 train no.211370  loss = 3.92239 avg_loss = 3.41053\n",
      "epoch no.2 train no.211380  loss = 3.53474 avg_loss = 3.40506\n",
      "epoch no.2 train no.211390  loss = 2.80475 avg_loss = 3.43122\n",
      "epoch no.2 train no.211400  loss = 3.55447 avg_loss = 3.41249\n",
      "epoch no.2 train no.211410  loss = 4.12043 avg_loss = 3.46285\n",
      "epoch no.2 train no.211420  loss = 3.64334 avg_loss = 3.45645\n",
      "epoch no.2 train no.211430  loss = 2.23793 avg_loss = 3.44746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.211440  loss = 4.07066 avg_loss = 3.44824\n",
      "epoch no.2 train no.211450  loss = 2.78070 avg_loss = 3.47366\n",
      "epoch no.2 train no.211460  loss = 2.73670 avg_loss = 3.42722\n",
      "epoch no.2 train no.211470  loss = 2.17551 avg_loss = 3.38305\n",
      "epoch no.2 train no.211480  loss = 2.59376 avg_loss = 3.37490\n",
      "epoch no.2 train no.211490  loss = 4.10011 avg_loss = 3.40704\n",
      "epoch no.2 train no.211500  loss = 3.43473 avg_loss = 3.47477\n",
      "epoch no.2 train no.211510  loss = 2.60624 avg_loss = 3.46364\n",
      "epoch no.2 train no.211520  loss = 4.92143 avg_loss = 3.46700\n",
      "epoch no.2 train no.211530  loss = 3.83599 avg_loss = 3.42804\n",
      "epoch no.2 train no.211540  loss = 4.42068 avg_loss = 3.43793\n",
      "epoch no.2 train no.211550  loss = 2.30992 avg_loss = 3.41591\n",
      "epoch no.2 train no.211560  loss = 2.89405 avg_loss = 3.46605\n",
      "epoch no.2 train no.211570  loss = 6.55638 avg_loss = 3.44406\n",
      "epoch no.2 train no.211580  loss = 4.12575 avg_loss = 3.45104\n",
      "epoch no.2 train no.211590  loss = 1.99230 avg_loss = 3.43235\n",
      "epoch no.2 train no.211600  loss = 5.70336 avg_loss = 3.45204\n",
      "epoch no.2 train no.211610  loss = 4.23263 avg_loss = 3.50313\n",
      "epoch no.2 train no.211620  loss = 4.15329 avg_loss = 3.53062\n",
      "epoch no.2 train no.211630  loss = 2.39473 avg_loss = 3.53797\n",
      "epoch no.2 train no.211640  loss = 3.80212 avg_loss = 3.59538\n",
      "epoch no.2 train no.211650  loss = 1.82785 avg_loss = 3.55263\n",
      "epoch no.2 train no.211660  loss = 3.93819 avg_loss = 3.53446\n",
      "epoch no.2 train no.211670  loss = 3.60747 avg_loss = 3.51710\n",
      "epoch no.2 train no.211680  loss = 3.02150 avg_loss = 3.47702\n",
      "epoch no.2 train no.211690  loss = 3.54065 avg_loss = 3.46660\n",
      "epoch no.2 train no.211700  loss = 3.89622 avg_loss = 3.49158\n",
      "epoch no.2 train no.211710  loss = 4.74204 avg_loss = 3.51665\n",
      "epoch no.2 train no.211720  loss = 4.12049 avg_loss = 3.50583\n",
      "epoch no.2 train no.211730  loss = 2.63358 avg_loss = 3.49044\n",
      "epoch no.2 train no.211740  loss = 3.37585 avg_loss = 3.52635\n",
      "epoch no.2 train no.211750  loss = 3.08079 avg_loss = 3.48846\n",
      "epoch no.2 train no.211760  loss = 4.11898 avg_loss = 3.48587\n",
      "epoch no.2 train no.211770  loss = 4.30908 avg_loss = 3.45021\n",
      "epoch no.2 train no.211780  loss = 4.39551 avg_loss = 3.47838\n",
      "epoch no.2 train no.211790  loss = 2.00917 avg_loss = 3.47205\n",
      "epoch no.2 train no.211800  loss = 3.31917 avg_loss = 3.45682\n",
      "epoch no.2 train no.211810  loss = 4.15402 avg_loss = 3.42713\n",
      "epoch no.2 train no.211820  loss = 2.99645 avg_loss = 3.45776\n",
      "epoch no.2 train no.211830  loss = 3.85223 avg_loss = 3.50047\n",
      "epoch no.2 train no.211840  loss = 3.77727 avg_loss = 3.53517\n",
      "epoch no.2 train no.211850  loss = 4.03770 avg_loss = 3.58023\n",
      "epoch no.2 train no.211860  loss = 2.21440 avg_loss = 3.57398\n",
      "epoch no.2 train no.211870  loss = 2.91579 avg_loss = 3.57898\n",
      "epoch no.2 train no.211880  loss = 4.45441 avg_loss = 3.57749\n",
      "epoch no.2 train no.211890  loss = 4.27413 avg_loss = 3.55512\n",
      "epoch no.2 train no.211900  loss = 4.98877 avg_loss = 3.61579\n",
      "epoch no.2 train no.211910  loss = 2.67703 avg_loss = 3.57386\n",
      "epoch no.2 train no.211920  loss = 3.96199 avg_loss = 3.57889\n",
      "epoch no.2 train no.211930  loss = 4.30562 avg_loss = 3.61199\n",
      "epoch no.2 train no.211940  loss = 4.26640 avg_loss = 3.60395\n",
      "epoch no.2 train no.211950  loss = 3.36787 avg_loss = 3.57237\n",
      "epoch no.2 train no.211960  loss = 5.13593 avg_loss = 3.59015\n",
      "epoch no.2 train no.211970  loss = 2.85862 avg_loss = 3.54371\n",
      "epoch no.2 train no.211980  loss = 3.17665 avg_loss = 3.49491\n",
      "epoch no.2 train no.211990  loss = 3.56607 avg_loss = 3.48755\n",
      "epoch no.2 train no.212000  loss = 3.44329 avg_loss = 3.50171\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.2 train no.212010  loss = 3.27533 avg_loss = 3.48099\n",
      "epoch no.2 train no.212020  loss = 3.80173 avg_loss = 3.50832\n",
      "epoch no.2 train no.212030  loss = 1.77175 avg_loss = 3.55134\n",
      "epoch no.2 train no.212040  loss = 4.04857 avg_loss = 3.56872\n",
      "epoch no.2 train no.212050  loss = 1.78302 avg_loss = 3.56787\n",
      "epoch no.2 train no.212060  loss = 3.06095 avg_loss = 3.58417\n",
      "epoch no.2 train no.212070  loss = 5.14810 avg_loss = 3.55505\n",
      "epoch no.2 train no.212080  loss = 2.65647 avg_loss = 3.55395\n",
      "epoch no.2 train no.212090  loss = 3.48795 avg_loss = 3.53456\n",
      "epoch no.2 train no.212100  loss = 4.16520 avg_loss = 3.52733\n",
      "epoch no.2 train no.212110  loss = 2.73619 avg_loss = 3.47999\n",
      "epoch no.2 train no.212120  loss = 4.72636 avg_loss = 3.47937\n",
      "epoch no.2 train no.212130  loss = 2.31862 avg_loss = 3.45941\n",
      "epoch no.2 train no.212140  loss = 3.63110 avg_loss = 3.52067\n",
      "epoch no.2 train no.212150  loss = 2.54821 avg_loss = 3.50791\n",
      "epoch no.2 train no.212160  loss = 2.61400 avg_loss = 3.55717\n",
      "epoch no.2 train no.212170  loss = 2.38849 avg_loss = 3.55395\n",
      "epoch no.2 train no.212180  loss = 3.11712 avg_loss = 3.53613\n",
      "epoch no.2 train no.212190  loss = 2.84337 avg_loss = 3.50690\n",
      "epoch no.2 train no.212200  loss = 2.51777 avg_loss = 3.50856\n",
      "epoch no.2 train no.212210  loss = 4.67511 avg_loss = 3.49352\n",
      "epoch no.2 train no.212220  loss = 2.76216 avg_loss = 3.53760\n",
      "epoch no.2 train no.212230  loss = 3.64725 avg_loss = 3.50167\n",
      "epoch no.2 train no.212240  loss = 3.97438 avg_loss = 3.45025\n",
      "epoch no.2 train no.212250  loss = 1.67105 avg_loss = 3.46027\n",
      "epoch no.2 train no.212260  loss = 2.92730 avg_loss = 3.41653\n",
      "epoch no.2 train no.212270  loss = 3.77636 avg_loss = 3.40454\n",
      "epoch no.2 train no.212280  loss = 5.42933 avg_loss = 3.41325\n",
      "epoch no.2 train no.212290  loss = 3.30174 avg_loss = 3.38700\n",
      "epoch no.2 train no.212300  loss = 4.27905 avg_loss = 3.39928\n",
      "epoch no.2 train no.212310  loss = 2.12908 avg_loss = 3.39971\n",
      "epoch no.2 train no.212320  loss = 3.05462 avg_loss = 3.40438\n",
      "epoch no.2 train no.212330  loss = 3.77242 avg_loss = 3.40099\n",
      "epoch no.2 train no.212340  loss = 3.57681 avg_loss = 3.44408\n",
      "epoch no.2 train no.212350  loss = 4.87412 avg_loss = 3.48013\n",
      "epoch no.2 train no.212360  loss = 4.99954 avg_loss = 3.47849\n",
      "epoch no.2 train no.212370  loss = 2.58981 avg_loss = 3.47178\n",
      "epoch no.2 train no.212380  loss = 2.05900 avg_loss = 3.43229\n",
      "epoch no.2 train no.212390  loss = 5.16882 avg_loss = 3.41303\n",
      "epoch no.2 train no.212400  loss = 2.46895 avg_loss = 3.40832\n",
      "epoch no.2 train no.212410  loss = 3.08588 avg_loss = 3.37398\n",
      "epoch no.2 train no.212420  loss = 1.08138 avg_loss = 3.34906\n",
      "epoch no.2 train no.212430  loss = 2.75236 avg_loss = 3.31253\n",
      "epoch no.2 train no.212440  loss = 3.10507 avg_loss = 3.30777\n",
      "epoch no.2 train no.212450  loss = 2.43869 avg_loss = 3.27085\n",
      "epoch no.2 train no.212460  loss = 4.24485 avg_loss = 3.27153\n",
      "epoch no.2 train no.212470  loss = 3.19970 avg_loss = 3.30513\n",
      "epoch no.2 train no.212480  loss = 3.62864 avg_loss = 3.32524\n",
      "epoch no.2 train no.212490  loss = 2.95948 avg_loss = 3.32559\n",
      "epoch no.2 train no.212500  loss = 2.53047 avg_loss = 3.38554\n",
      "epoch no.2 train no.212510  loss = 2.41669 avg_loss = 3.36172\n",
      "epoch no.2 train no.212520  loss = 3.90528 avg_loss = 3.42379\n",
      "epoch no.2 train no.212530  loss = 3.74096 avg_loss = 3.49286\n",
      "epoch no.2 train no.212540  loss = 5.39118 avg_loss = 3.62453\n",
      "epoch no.2 train no.212550  loss = 2.94617 avg_loss = 3.63223\n",
      "epoch no.2 train no.212560  loss = 1.97164 avg_loss = 3.61645\n",
      "epoch no.2 train no.212570  loss = 3.25318 avg_loss = 3.61323\n",
      "epoch no.2 train no.212580  loss = 2.54628 avg_loss = 3.63061\n",
      "epoch no.2 train no.212590  loss = 2.71356 avg_loss = 3.65108\n",
      "epoch no.2 train no.212600  loss = 2.84427 avg_loss = 3.59686\n",
      "epoch no.2 train no.212610  loss = 2.08066 avg_loss = 3.54815\n",
      "epoch no.2 train no.212620  loss = 3.81688 avg_loss = 3.55769\n",
      "epoch no.2 train no.212630  loss = 1.86193 avg_loss = 3.53635\n",
      "epoch no.2 train no.212640  loss = 2.50347 avg_loss = 3.48935\n",
      "epoch no.2 train no.212650  loss = 3.34305 avg_loss = 3.46902\n",
      "epoch no.2 train no.212660  loss = 2.72334 avg_loss = 3.45522\n",
      "epoch no.2 train no.212670  loss = 4.02267 avg_loss = 3.48213\n",
      "epoch no.2 train no.212680  loss = 3.83875 avg_loss = 3.48848\n",
      "epoch no.2 train no.212690  loss = 2.97878 avg_loss = 3.50482\n",
      "epoch no.2 train no.212700  loss = 3.01841 avg_loss = 3.54372\n",
      "epoch no.2 train no.212710  loss = 3.45047 avg_loss = 3.50535\n",
      "epoch no.2 train no.212720  loss = 4.30085 avg_loss = 3.53662\n",
      "epoch no.2 train no.212730  loss = 3.58740 avg_loss = 3.52613\n",
      "epoch no.2 train no.212740  loss = 3.12621 avg_loss = 3.49736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.212750  loss = 4.23606 avg_loss = 3.49107\n",
      "epoch no.2 train no.212760  loss = 3.17675 avg_loss = 3.44738\n",
      "epoch no.2 train no.212770  loss = 2.08983 avg_loss = 3.48586\n",
      "epoch no.2 train no.212780  loss = 2.00728 avg_loss = 3.47843\n",
      "epoch no.2 train no.212790  loss = 5.80499 avg_loss = 3.52328\n",
      "epoch no.2 train no.212800  loss = 3.74805 avg_loss = 3.50972\n",
      "epoch no.2 train no.212810  loss = 2.11220 avg_loss = 3.43357\n",
      "epoch no.2 train no.212820  loss = 3.44205 avg_loss = 3.41176\n",
      "epoch no.2 train no.212830  loss = 5.46643 avg_loss = 3.45782\n",
      "epoch no.2 train no.212840  loss = 5.08742 avg_loss = 3.46112\n",
      "epoch no.2 train no.212850  loss = 4.19357 avg_loss = 3.46397\n",
      "epoch no.2 train no.212860  loss = 3.41013 avg_loss = 3.49204\n",
      "epoch no.2 train no.212870  loss = 3.90857 avg_loss = 3.41808\n",
      "epoch no.2 train no.212880  loss = 2.71821 avg_loss = 3.43712\n",
      "epoch no.2 train no.212890  loss = 4.14107 avg_loss = 3.48119\n",
      "epoch no.2 train no.212900  loss = 2.94870 avg_loss = 3.43892\n",
      "epoch no.2 train no.212910  loss = 2.98009 avg_loss = 3.45865\n",
      "epoch no.2 train no.212920  loss = 3.52856 avg_loss = 3.45944\n",
      "epoch no.2 train no.212930  loss = 2.86681 avg_loss = 3.42823\n",
      "epoch no.2 train no.212940  loss = 3.14118 avg_loss = 3.44501\n",
      "epoch no.2 train no.212950  loss = 5.23963 avg_loss = 3.49661\n",
      "epoch no.2 train no.212960  loss = 3.52386 avg_loss = 3.51503\n",
      "epoch no.2 train no.212970  loss = 3.94396 avg_loss = 3.50921\n",
      "epoch no.2 train no.212980  loss = 2.68474 avg_loss = 3.49170\n",
      "epoch no.2 train no.212990  loss = 4.64689 avg_loss = 3.55919\n",
      "epoch no.2 train no.213000  loss = 2.41245 avg_loss = 3.56629\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '과', '▁함께', '</s>']\n",
      "여름밤의 추억과 함께</s>\n",
      "epoch no.2 train no.213010  loss = 3.37943 avg_loss = 3.58241\n",
      "epoch no.2 train no.213020  loss = 3.18068 avg_loss = 3.54714\n",
      "epoch no.2 train no.213030  loss = 2.71967 avg_loss = 3.52506\n",
      "epoch no.2 train no.213040  loss = 3.46581 avg_loss = 3.54211\n",
      "epoch no.2 train no.213050  loss = 3.83537 avg_loss = 3.54353\n",
      "epoch no.2 train no.213060  loss = 3.91619 avg_loss = 3.56190\n",
      "epoch no.2 train no.213070  loss = 5.89223 avg_loss = 3.51063\n",
      "epoch no.2 train no.213080  loss = 5.44109 avg_loss = 3.53027\n",
      "epoch no.2 train no.213090  loss = 6.82478 avg_loss = 3.56514\n",
      "epoch no.2 train no.213100  loss = 3.07008 avg_loss = 3.56346\n",
      "epoch no.2 train no.213110  loss = 5.47535 avg_loss = 3.59288\n",
      "epoch no.2 train no.213120  loss = 3.63816 avg_loss = 3.60652\n",
      "epoch no.2 train no.213130  loss = 2.65230 avg_loss = 3.51162\n",
      "epoch no.2 train no.213140  loss = 2.40451 avg_loss = 3.53097\n",
      "epoch no.2 train no.213150  loss = 3.69527 avg_loss = 3.59278\n",
      "epoch no.2 train no.213160  loss = 4.07509 avg_loss = 3.60457\n",
      "epoch no.2 train no.213170  loss = 4.94856 avg_loss = 3.65687\n",
      "epoch no.2 train no.213180  loss = 3.99395 avg_loss = 3.68754\n",
      "epoch no.2 train no.213190  loss = 2.91091 avg_loss = 3.62870\n",
      "epoch no.2 train no.213200  loss = 2.26370 avg_loss = 3.60885\n",
      "epoch no.2 train no.213210  loss = 2.58583 avg_loss = 3.58922\n",
      "epoch no.2 train no.213220  loss = 4.47003 avg_loss = 3.56181\n",
      "epoch no.2 train no.213230  loss = 4.18770 avg_loss = 3.56407\n",
      "epoch no.2 train no.213240  loss = 2.78165 avg_loss = 3.53619\n",
      "epoch no.2 train no.213250  loss = 4.26808 avg_loss = 3.48003\n",
      "epoch no.2 train no.213260  loss = 3.09252 avg_loss = 3.46725\n",
      "epoch no.2 train no.213270  loss = 3.49969 avg_loss = 3.46412\n",
      "epoch no.2 train no.213280  loss = 3.28945 avg_loss = 3.46367\n",
      "epoch no.2 train no.213290  loss = 2.61551 avg_loss = 3.44468\n",
      "epoch no.2 train no.213300  loss = 2.88809 avg_loss = 3.44559\n",
      "epoch no.2 train no.213310  loss = 4.97333 avg_loss = 3.45322\n",
      "epoch no.2 train no.213320  loss = 3.30567 avg_loss = 3.44278\n",
      "epoch no.2 train no.213330  loss = 3.26018 avg_loss = 3.39866\n",
      "epoch no.2 train no.213340  loss = 4.04660 avg_loss = 3.38692\n",
      "epoch no.2 train no.213350  loss = 3.72266 avg_loss = 3.35692\n",
      "epoch no.2 train no.213360  loss = 1.97763 avg_loss = 3.35894\n",
      "epoch no.2 train no.213370  loss = 2.75817 avg_loss = 3.39074\n",
      "epoch no.2 train no.213380  loss = 2.59933 avg_loss = 3.37196\n",
      "epoch no.2 train no.213390  loss = 2.69729 avg_loss = 3.43814\n",
      "epoch no.2 train no.213400  loss = 3.65308 avg_loss = 3.45798\n",
      "epoch no.2 train no.213410  loss = 3.93138 avg_loss = 3.41991\n",
      "epoch no.2 train no.213420  loss = 4.28918 avg_loss = 3.49444\n",
      "epoch no.2 train no.213430  loss = 3.13554 avg_loss = 3.45915\n",
      "epoch no.2 train no.213440  loss = 3.68542 avg_loss = 3.44250\n",
      "epoch no.2 train no.213450  loss = 5.26969 avg_loss = 3.48633\n",
      "epoch no.2 train no.213460  loss = 4.45735 avg_loss = 3.51106\n",
      "epoch no.2 train no.213470  loss = 4.85993 avg_loss = 3.50692\n",
      "epoch no.2 train no.213480  loss = 3.19230 avg_loss = 3.49609\n",
      "epoch no.2 train no.213490  loss = 3.17501 avg_loss = 3.47978\n",
      "epoch no.2 train no.213500  loss = 1.75371 avg_loss = 3.44212\n",
      "epoch no.2 train no.213510  loss = 4.04683 avg_loss = 3.52548\n",
      "epoch no.2 train no.213520  loss = 3.95625 avg_loss = 3.55739\n",
      "epoch no.2 train no.213530  loss = 3.68293 avg_loss = 3.54862\n",
      "epoch no.2 train no.213540  loss = 4.38715 avg_loss = 3.55091\n",
      "epoch no.2 train no.213550  loss = 3.18523 avg_loss = 3.53035\n",
      "epoch no.2 train no.213560  loss = 4.44072 avg_loss = 3.49663\n",
      "epoch no.2 train no.213570  loss = 3.33222 avg_loss = 3.48853\n",
      "epoch no.2 train no.213580  loss = 2.53396 avg_loss = 3.52801\n",
      "epoch no.2 train no.213590  loss = 3.80510 avg_loss = 3.49107\n",
      "epoch no.2 train no.213600  loss = 3.14634 avg_loss = 3.47359\n",
      "epoch no.2 train no.213610  loss = 4.34136 avg_loss = 3.43756\n",
      "epoch no.2 train no.213620  loss = 2.02315 avg_loss = 3.40359\n",
      "epoch no.2 train no.213630  loss = 4.91077 avg_loss = 3.45083\n",
      "epoch no.2 train no.213640  loss = 3.65406 avg_loss = 3.41164\n",
      "epoch no.2 train no.213650  loss = 3.31877 avg_loss = 3.45414\n",
      "epoch no.2 train no.213660  loss = 6.06360 avg_loss = 3.51904\n",
      "epoch no.2 train no.213670  loss = 3.42432 avg_loss = 3.54317\n",
      "epoch no.2 train no.213680  loss = 2.36565 avg_loss = 3.50723\n",
      "epoch no.2 train no.213690  loss = 3.40816 avg_loss = 3.46551\n",
      "epoch no.2 train no.213700  loss = 4.33185 avg_loss = 3.45240\n",
      "epoch no.2 train no.213710  loss = 3.44647 avg_loss = 3.44868\n",
      "epoch no.2 train no.213720  loss = 3.97401 avg_loss = 3.42848\n",
      "epoch no.2 train no.213730  loss = 4.33933 avg_loss = 3.48697\n",
      "epoch no.2 train no.213740  loss = 3.00347 avg_loss = 3.47733\n",
      "epoch no.2 train no.213750  loss = 3.40841 avg_loss = 3.48875\n",
      "epoch no.2 train no.213760  loss = 2.69879 avg_loss = 3.53231\n",
      "epoch no.2 train no.213770  loss = 3.78898 avg_loss = 3.48997\n",
      "epoch no.2 train no.213780  loss = 4.21504 avg_loss = 3.52727\n",
      "epoch no.2 train no.213790  loss = 4.42943 avg_loss = 3.49845\n",
      "epoch no.2 train no.213800  loss = 3.99202 avg_loss = 3.62493\n",
      "epoch no.2 train no.213810  loss = 4.99843 avg_loss = 3.60773\n",
      "epoch no.2 train no.213820  loss = 4.23200 avg_loss = 3.61858\n",
      "epoch no.2 train no.213830  loss = 2.85162 avg_loss = 3.61984\n",
      "epoch no.2 train no.213840  loss = 3.52652 avg_loss = 3.58167\n",
      "epoch no.2 train no.213850  loss = 3.90724 avg_loss = 3.54405\n",
      "epoch no.2 train no.213860  loss = 2.32348 avg_loss = 3.58208\n",
      "epoch no.2 train no.213870  loss = 3.34015 avg_loss = 3.60441\n",
      "epoch no.2 train no.213880  loss = 3.28801 avg_loss = 3.58821\n",
      "epoch no.2 train no.213890  loss = 2.93708 avg_loss = 3.59093\n",
      "epoch no.2 train no.213900  loss = 2.35738 avg_loss = 3.53369\n",
      "epoch no.2 train no.213910  loss = 4.33863 avg_loss = 3.55572\n",
      "epoch no.2 train no.213920  loss = 2.82572 avg_loss = 3.52260\n",
      "epoch no.2 train no.213930  loss = 3.15412 avg_loss = 3.53813\n",
      "epoch no.2 train no.213940  loss = 3.64560 avg_loss = 3.51742\n",
      "epoch no.2 train no.213950  loss = 4.93884 avg_loss = 3.55599\n",
      "epoch no.2 train no.213960  loss = 3.79196 avg_loss = 3.58132\n",
      "epoch no.2 train no.213970  loss = 2.63432 avg_loss = 3.57477\n",
      "epoch no.2 train no.213980  loss = 4.13794 avg_loss = 3.50891\n",
      "epoch no.2 train no.213990  loss = 2.84469 avg_loss = 3.47868\n",
      "epoch no.2 train no.214000  loss = 3.40511 avg_loss = 3.44802\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '의', '▁어울리는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 음악</s>\n",
      "epoch no.2 train no.214010  loss = 2.90506 avg_loss = 3.45565\n",
      "epoch no.2 train no.214020  loss = 2.95683 avg_loss = 3.44569\n",
      "epoch no.2 train no.214030  loss = 3.24139 avg_loss = 3.42085\n",
      "epoch no.2 train no.214040  loss = 3.16718 avg_loss = 3.44718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.214050  loss = 3.61158 avg_loss = 3.41064\n",
      "epoch no.2 train no.214060  loss = 3.07492 avg_loss = 3.46858\n",
      "epoch no.2 train no.214070  loss = 1.83788 avg_loss = 3.47097\n",
      "epoch no.2 train no.214080  loss = 3.57490 avg_loss = 3.44915\n",
      "epoch no.2 train no.214090  loss = 6.22348 avg_loss = 3.43317\n",
      "epoch no.2 train no.214100  loss = 3.34266 avg_loss = 3.43924\n",
      "epoch no.2 train no.214110  loss = 4.71473 avg_loss = 3.45504\n",
      "epoch no.2 train no.214120  loss = 2.95223 avg_loss = 3.47101\n",
      "epoch no.2 train no.214130  loss = 2.16132 avg_loss = 3.44345\n",
      "epoch no.2 train no.214140  loss = 2.31542 avg_loss = 3.42304\n",
      "epoch no.2 train no.214150  loss = 3.25133 avg_loss = 3.41082\n",
      "epoch no.2 train no.214160  loss = 3.93865 avg_loss = 3.43288\n",
      "epoch no.2 train no.214170  loss = 4.85476 avg_loss = 3.43691\n",
      "epoch no.2 train no.214180  loss = 3.39194 avg_loss = 3.41082\n",
      "epoch no.2 train no.214190  loss = 2.45594 avg_loss = 3.48623\n",
      "epoch no.2 train no.214200  loss = 2.17839 avg_loss = 3.43424\n",
      "epoch no.2 train no.214210  loss = 4.85797 avg_loss = 3.42377\n",
      "epoch no.2 train no.214220  loss = 4.11807 avg_loss = 3.45993\n",
      "epoch no.2 train no.214230  loss = 5.19903 avg_loss = 3.56664\n",
      "epoch no.2 train no.214240  loss = 3.51422 avg_loss = 3.57426\n",
      "epoch no.2 train no.214250  loss = 3.23828 avg_loss = 3.60757\n",
      "epoch no.2 train no.214260  loss = 3.23292 avg_loss = 3.54224\n",
      "epoch no.2 train no.214270  loss = 2.85839 avg_loss = 3.58099\n",
      "epoch no.2 train no.214280  loss = 2.06077 avg_loss = 3.55190\n",
      "epoch no.2 train no.214290  loss = 3.94300 avg_loss = 3.57962\n",
      "epoch no.2 train no.214300  loss = 2.62416 avg_loss = 3.57134\n",
      "epoch no.2 train no.214310  loss = 2.63696 avg_loss = 3.56322\n",
      "epoch no.2 train no.214320  loss = 3.88416 avg_loss = 3.56883\n",
      "epoch no.2 train no.214330  loss = 1.71232 avg_loss = 3.51142\n",
      "epoch no.2 train no.214340  loss = 2.39900 avg_loss = 3.52515\n",
      "epoch no.2 train no.214350  loss = 2.97008 avg_loss = 3.46480\n",
      "epoch no.2 train no.214360  loss = 2.74924 avg_loss = 3.46587\n",
      "epoch no.2 train no.214370  loss = 3.77799 avg_loss = 3.51273\n",
      "epoch no.2 train no.214380  loss = 4.09207 avg_loss = 3.51924\n",
      "epoch no.2 train no.214390  loss = 2.35141 avg_loss = 3.50000\n",
      "epoch no.2 train no.214400  loss = 5.91694 avg_loss = 3.51088\n",
      "epoch no.2 train no.214410  loss = 1.95439 avg_loss = 3.55420\n",
      "epoch no.2 train no.214420  loss = 6.58087 avg_loss = 3.62417\n",
      "epoch no.2 train no.214430  loss = 3.97562 avg_loss = 3.62606\n",
      "epoch no.2 train no.214440  loss = 3.82229 avg_loss = 3.63748\n",
      "epoch no.2 train no.214450  loss = 2.57645 avg_loss = 3.61423\n",
      "epoch no.2 train no.214460  loss = 3.99154 avg_loss = 3.57309\n",
      "epoch no.2 train no.214470  loss = 2.23418 avg_loss = 3.55356\n",
      "epoch no.2 train no.214480  loss = 5.37790 avg_loss = 3.53806\n",
      "epoch no.2 train no.214490  loss = 2.11109 avg_loss = 3.51503\n",
      "epoch no.2 train no.214500  loss = 1.40353 avg_loss = 3.47447\n",
      "epoch no.2 train no.214510  loss = 4.11971 avg_loss = 3.46960\n",
      "epoch no.2 train no.214520  loss = 5.01662 avg_loss = 3.49428\n",
      "epoch no.2 train no.214530  loss = 3.72671 avg_loss = 3.49745\n",
      "epoch no.2 train no.214540  loss = 3.86927 avg_loss = 3.45248\n",
      "epoch no.2 train no.214550  loss = 3.62991 avg_loss = 3.49522\n",
      "epoch no.2 train no.214560  loss = 2.93150 avg_loss = 3.48567\n",
      "epoch no.2 train no.214570  loss = 4.49612 avg_loss = 3.48589\n",
      "epoch no.2 train no.214580  loss = 2.91719 avg_loss = 3.48873\n",
      "epoch no.2 train no.214590  loss = 3.15324 avg_loss = 3.44619\n",
      "epoch no.2 train no.214600  loss = 2.38890 avg_loss = 3.43626\n",
      "epoch no.2 train no.214610  loss = 3.69995 avg_loss = 3.41381\n",
      "epoch no.2 train no.214620  loss = 4.26158 avg_loss = 3.45808\n",
      "epoch no.2 train no.214630  loss = 2.90168 avg_loss = 3.43965\n",
      "epoch no.2 train no.214640  loss = 3.55836 avg_loss = 3.43127\n",
      "epoch no.2 train no.214650  loss = 4.80792 avg_loss = 3.46212\n",
      "epoch no.2 train no.214660  loss = 3.86832 avg_loss = 3.44619\n",
      "epoch no.2 train no.214670  loss = 1.44236 avg_loss = 3.47846\n",
      "epoch no.2 train no.214680  loss = 2.45811 avg_loss = 3.49171\n",
      "epoch no.2 train no.214690  loss = 4.73061 avg_loss = 3.43347\n",
      "epoch no.2 train no.214700  loss = 4.11595 avg_loss = 3.45066\n",
      "epoch no.2 train no.214710  loss = 4.47575 avg_loss = 3.51003\n",
      "epoch no.2 train no.214720  loss = 3.72006 avg_loss = 3.50015\n",
      "epoch no.2 train no.214730  loss = 3.28297 avg_loss = 3.47738\n",
      "epoch no.2 train no.214740  loss = 3.34413 avg_loss = 3.48631\n",
      "epoch no.2 train no.214750  loss = 2.84548 avg_loss = 3.48825\n",
      "epoch no.2 train no.214760  loss = 3.32849 avg_loss = 3.49475\n",
      "epoch no.2 train no.214770  loss = 2.62976 avg_loss = 3.51124\n",
      "epoch no.2 train no.214780  loss = 2.21578 avg_loss = 3.48387\n",
      "epoch no.2 train no.214790  loss = 5.28207 avg_loss = 3.47397\n",
      "epoch no.2 train no.214800  loss = 4.37168 avg_loss = 3.48052\n",
      "epoch no.2 train no.214810  loss = 3.43908 avg_loss = 3.48161\n",
      "epoch no.2 train no.214820  loss = 3.20888 avg_loss = 3.44176\n",
      "epoch no.2 train no.214830  loss = 6.25881 avg_loss = 3.45641\n",
      "epoch no.2 train no.214840  loss = 2.79494 avg_loss = 3.47942\n",
      "epoch no.2 train no.214850  loss = 2.86957 avg_loss = 3.49064\n",
      "epoch no.2 train no.214860  loss = 2.51439 avg_loss = 3.47847\n",
      "epoch no.2 train no.214870  loss = 2.78477 avg_loss = 3.49752\n",
      "epoch no.2 train no.214880  loss = 4.16242 avg_loss = 3.50710\n",
      "epoch no.2 train no.214890  loss = 2.02096 avg_loss = 3.51309\n",
      "epoch no.2 train no.214900  loss = 2.05677 avg_loss = 3.49276\n",
      "epoch no.2 train no.214910  loss = 2.17294 avg_loss = 3.50399\n",
      "epoch no.2 train no.214920  loss = 5.56956 avg_loss = 3.56714\n",
      "epoch no.2 train no.214930  loss = 3.97092 avg_loss = 3.56724\n",
      "epoch no.2 train no.214940  loss = 3.37862 avg_loss = 3.57323\n",
      "epoch no.2 train no.214950  loss = 3.85708 avg_loss = 3.56254\n",
      "epoch no.2 train no.214960  loss = 4.25080 avg_loss = 3.57615\n",
      "epoch no.2 train no.214970  loss = 6.09481 avg_loss = 3.58662\n",
      "epoch no.2 train no.214980  loss = 2.60627 avg_loss = 3.61398\n",
      "epoch no.2 train no.214990  loss = 2.55429 avg_loss = 3.58385\n",
      "epoch no.2 train no.215000  loss = 4.29079 avg_loss = 3.60245\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '의', '▁재즈', '음악', '▁음악', '</s>', '</s>']\n",
      "여름밤의 감성적인 팝송</s>\n",
      "epoch no.2 train no.215010  loss = 3.48458 avg_loss = 3.60225\n",
      "epoch no.2 train no.215020  loss = 2.50121 avg_loss = 3.53899\n",
      "epoch no.2 train no.215030  loss = 5.57232 avg_loss = 3.57306\n",
      "epoch no.2 train no.215040  loss = 4.10363 avg_loss = 3.52262\n",
      "epoch no.2 train no.215050  loss = 6.26546 avg_loss = 3.55472\n",
      "epoch no.2 train no.215060  loss = 2.88741 avg_loss = 3.57225\n",
      "epoch no.2 train no.215070  loss = 2.70950 avg_loss = 3.56869\n",
      "epoch no.2 train no.215080  loss = 3.54578 avg_loss = 3.54709\n",
      "epoch no.2 train no.215090  loss = 2.02676 avg_loss = 3.52023\n",
      "epoch no.2 train no.215100  loss = 2.77106 avg_loss = 3.52293\n",
      "epoch no.2 train no.215110  loss = 2.20999 avg_loss = 3.51432\n",
      "epoch no.2 train no.215120  loss = 4.13966 avg_loss = 3.49033\n",
      "epoch no.2 train no.215130  loss = 5.64883 avg_loss = 3.48837\n",
      "epoch no.2 train no.215140  loss = 4.01090 avg_loss = 3.50454\n",
      "epoch no.2 train no.215150  loss = 1.93569 avg_loss = 3.46544\n",
      "epoch no.2 train no.215160  loss = 2.52384 avg_loss = 3.51441\n",
      "epoch no.2 train no.215170  loss = 4.07443 avg_loss = 3.50976\n",
      "epoch no.2 train no.215180  loss = 3.79272 avg_loss = 3.55480\n",
      "epoch no.2 train no.215190  loss = 3.74715 avg_loss = 3.49588\n",
      "epoch no.2 train no.215200  loss = 4.16411 avg_loss = 3.50131\n",
      "epoch no.2 train no.215210  loss = 3.66041 avg_loss = 3.46539\n",
      "epoch no.2 train no.215220  loss = 2.56311 avg_loss = 3.48225\n",
      "epoch no.2 train no.215230  loss = 1.74782 avg_loss = 3.49607\n",
      "epoch no.2 train no.215240  loss = 5.90166 avg_loss = 3.44040\n",
      "epoch no.2 train no.215250  loss = 3.47057 avg_loss = 3.41604\n",
      "epoch no.2 train no.215260  loss = 4.60890 avg_loss = 3.41155\n",
      "epoch no.2 train no.215270  loss = 3.40542 avg_loss = 3.45266\n",
      "epoch no.2 train no.215280  loss = 3.62870 avg_loss = 3.48605\n",
      "epoch no.2 train no.215290  loss = 2.72775 avg_loss = 3.45028\n",
      "epoch no.2 train no.215300  loss = 2.63548 avg_loss = 3.41625\n",
      "epoch no.2 train no.215310  loss = 2.64312 avg_loss = 3.40095\n",
      "epoch no.2 train no.215320  loss = 1.05898 avg_loss = 3.43726\n",
      "epoch no.2 train no.215330  loss = 2.47146 avg_loss = 3.46141\n",
      "epoch no.2 train no.215340  loss = 4.74964 avg_loss = 3.50128\n",
      "epoch no.2 train no.215350  loss = 3.58373 avg_loss = 3.52818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.215360  loss = 3.86608 avg_loss = 3.52243\n",
      "epoch no.2 train no.215370  loss = 2.46250 avg_loss = 3.43617\n",
      "epoch no.2 train no.215380  loss = 4.50623 avg_loss = 3.46160\n",
      "epoch no.2 train no.215390  loss = 2.87255 avg_loss = 3.45136\n",
      "epoch no.2 train no.215400  loss = 3.31455 avg_loss = 3.51408\n",
      "epoch no.2 train no.215410  loss = 3.46935 avg_loss = 3.48536\n",
      "epoch no.2 train no.215420  loss = 3.36839 avg_loss = 3.48547\n",
      "epoch no.2 train no.215430  loss = 2.70511 avg_loss = 3.45061\n",
      "epoch no.2 train no.215440  loss = 2.89584 avg_loss = 3.43615\n",
      "epoch no.2 train no.215450  loss = 2.34044 avg_loss = 3.39987\n",
      "epoch no.2 train no.215460  loss = 4.01203 avg_loss = 3.44144\n",
      "epoch no.2 train no.215470  loss = 2.87086 avg_loss = 3.45404\n",
      "epoch no.2 train no.215480  loss = 5.75087 avg_loss = 3.52080\n",
      "epoch no.2 train no.215490  loss = 4.88680 avg_loss = 3.57222\n",
      "epoch no.2 train no.215500  loss = 3.61977 avg_loss = 3.61198\n",
      "epoch no.2 train no.215510  loss = 3.38158 avg_loss = 3.60591\n",
      "epoch no.2 train no.215520  loss = 3.11730 avg_loss = 3.54074\n",
      "epoch no.2 train no.215530  loss = 4.95875 avg_loss = 3.55405\n",
      "epoch no.2 train no.215540  loss = 5.28703 avg_loss = 3.56240\n",
      "epoch no.2 train no.215550  loss = 2.76189 avg_loss = 3.56150\n",
      "epoch no.2 train no.215560  loss = 4.16563 avg_loss = 3.54110\n",
      "epoch no.2 train no.215570  loss = 3.91647 avg_loss = 3.52101\n",
      "epoch no.2 train no.215580  loss = 4.93164 avg_loss = 3.52699\n",
      "epoch no.2 train no.215590  loss = 3.23941 avg_loss = 3.51263\n",
      "epoch no.2 train no.215600  loss = 3.18707 avg_loss = 3.52116\n",
      "epoch no.2 train no.215610  loss = 2.46153 avg_loss = 3.51269\n",
      "epoch no.2 train no.215620  loss = 5.10921 avg_loss = 3.60705\n",
      "epoch no.2 train no.215630  loss = 3.56928 avg_loss = 3.56437\n",
      "epoch no.2 train no.215640  loss = 3.87144 avg_loss = 3.53382\n",
      "epoch no.2 train no.215650  loss = 3.95965 avg_loss = 3.54943\n",
      "epoch no.3 train no.215660  loss = 3.22443 avg_loss = 3.52954\n",
      "epoch no.3 train no.215670  loss = 2.75702 avg_loss = 3.52700\n",
      "epoch no.3 train no.215680  loss = 2.49398 avg_loss = 3.46248\n",
      "epoch no.3 train no.215690  loss = 2.52747 avg_loss = 3.46097\n",
      "epoch no.3 train no.215700  loss = 3.52674 avg_loss = 3.45297\n",
      "epoch no.3 train no.215710  loss = 2.03208 avg_loss = 3.37093\n",
      "epoch no.3 train no.215720  loss = 3.09108 avg_loss = 3.35554\n",
      "epoch no.3 train no.215730  loss = 3.67464 avg_loss = 3.33813\n",
      "epoch no.3 train no.215740  loss = 4.01972 avg_loss = 3.33828\n",
      "epoch no.3 train no.215750  loss = 3.60920 avg_loss = 3.30060\n",
      "epoch no.3 train no.215760  loss = 2.63399 avg_loss = 3.26349\n",
      "epoch no.3 train no.215770  loss = 2.14825 avg_loss = 3.21157\n",
      "epoch no.3 train no.215780  loss = 3.08509 avg_loss = 3.21315\n",
      "epoch no.3 train no.215790  loss = 4.01584 avg_loss = 3.22412\n",
      "epoch no.3 train no.215800  loss = 2.51395 avg_loss = 3.18867\n",
      "epoch no.3 train no.215810  loss = 3.56665 avg_loss = 3.18647\n",
      "epoch no.3 train no.215820  loss = 2.84518 avg_loss = 3.18137\n",
      "epoch no.3 train no.215830  loss = 2.43160 avg_loss = 3.18202\n",
      "epoch no.3 train no.215840  loss = 2.02508 avg_loss = 3.20152\n",
      "epoch no.3 train no.215850  loss = 2.25339 avg_loss = 3.20270\n",
      "epoch no.3 train no.215860  loss = 3.70570 avg_loss = 3.19983\n",
      "epoch no.3 train no.215870  loss = 2.68707 avg_loss = 3.20430\n",
      "epoch no.3 train no.215880  loss = 3.44552 avg_loss = 3.22988\n",
      "epoch no.3 train no.215890  loss = 2.05850 avg_loss = 3.21376\n",
      "epoch no.3 train no.215900  loss = 3.28400 avg_loss = 3.18894\n",
      "epoch no.3 train no.215910  loss = 4.34927 avg_loss = 3.19041\n",
      "epoch no.3 train no.215920  loss = 1.99437 avg_loss = 3.19333\n",
      "epoch no.3 train no.215930  loss = 2.90274 avg_loss = 3.17079\n",
      "epoch no.3 train no.215940  loss = 3.67075 avg_loss = 3.24355\n",
      "epoch no.3 train no.215950  loss = 3.02440 avg_loss = 3.26876\n",
      "epoch no.3 train no.215960  loss = 1.92604 avg_loss = 3.26476\n",
      "epoch no.3 train no.215970  loss = 3.49969 avg_loss = 3.28910\n",
      "epoch no.3 train no.215980  loss = 3.01626 avg_loss = 3.27997\n",
      "epoch no.3 train no.215990  loss = 2.53420 avg_loss = 3.25765\n",
      "epoch no.3 train no.216000  loss = 1.48222 avg_loss = 3.22920\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '한', '▁피아노', '선', '</s>', '</s>']\n",
      "여름밤 잔잔한 피아노곡들</s>\n",
      "epoch no.3 train no.216010  loss = 2.71670 avg_loss = 3.23542\n",
      "epoch no.3 train no.216020  loss = 4.24177 avg_loss = 3.26232\n",
      "epoch no.3 train no.216030  loss = 2.46631 avg_loss = 3.24787\n",
      "epoch no.3 train no.216040  loss = 3.46355 avg_loss = 3.24501\n",
      "epoch no.3 train no.216050  loss = 3.53389 avg_loss = 3.20629\n",
      "epoch no.3 train no.216060  loss = 2.47329 avg_loss = 3.18854\n",
      "epoch no.3 train no.216070  loss = 2.09064 avg_loss = 3.13373\n",
      "epoch no.3 train no.216080  loss = 5.75923 avg_loss = 3.20108\n",
      "epoch no.3 train no.216090  loss = 3.08950 avg_loss = 3.20300\n",
      "epoch no.3 train no.216100  loss = 4.12967 avg_loss = 3.23122\n",
      "epoch no.3 train no.216110  loss = 2.81353 avg_loss = 3.20982\n",
      "epoch no.3 train no.216120  loss = 3.57401 avg_loss = 3.25571\n",
      "epoch no.3 train no.216130  loss = 3.70753 avg_loss = 3.27742\n",
      "epoch no.3 train no.216140  loss = 3.35418 avg_loss = 3.25827\n",
      "epoch no.3 train no.216150  loss = 2.34129 avg_loss = 3.25968\n",
      "epoch no.3 train no.216160  loss = 2.35798 avg_loss = 3.31779\n",
      "epoch no.3 train no.216170  loss = 2.13510 avg_loss = 3.29040\n",
      "epoch no.3 train no.216180  loss = 4.65002 avg_loss = 3.27162\n",
      "epoch no.3 train no.216190  loss = 2.34648 avg_loss = 3.23881\n",
      "epoch no.3 train no.216200  loss = 3.56716 avg_loss = 3.27239\n",
      "epoch no.3 train no.216210  loss = 2.41851 avg_loss = 3.24892\n",
      "epoch no.3 train no.216220  loss = 2.06951 avg_loss = 3.19224\n",
      "epoch no.3 train no.216230  loss = 3.08591 avg_loss = 3.20543\n",
      "epoch no.3 train no.216240  loss = 1.98939 avg_loss = 3.15922\n",
      "epoch no.3 train no.216250  loss = 4.63405 avg_loss = 3.18052\n",
      "epoch no.3 train no.216260  loss = 4.58800 avg_loss = 3.21531\n",
      "epoch no.3 train no.216270  loss = 2.11365 avg_loss = 3.17649\n",
      "epoch no.3 train no.216280  loss = 4.83766 avg_loss = 3.19453\n",
      "epoch no.3 train no.216290  loss = 4.31369 avg_loss = 3.22118\n",
      "epoch no.3 train no.216300  loss = 2.02478 avg_loss = 3.22588\n",
      "epoch no.3 train no.216310  loss = 3.73768 avg_loss = 3.21272\n",
      "epoch no.3 train no.216320  loss = 3.75215 avg_loss = 3.23841\n",
      "epoch no.3 train no.216330  loss = 2.80674 avg_loss = 3.24445\n",
      "epoch no.3 train no.216340  loss = 2.88673 avg_loss = 3.19518\n",
      "epoch no.3 train no.216350  loss = 2.60244 avg_loss = 3.14115\n",
      "epoch no.3 train no.216360  loss = 2.30174 avg_loss = 3.15342\n",
      "epoch no.3 train no.216370  loss = 2.27387 avg_loss = 3.13021\n",
      "epoch no.3 train no.216380  loss = 1.71650 avg_loss = 3.09065\n",
      "epoch no.3 train no.216390  loss = 4.41388 avg_loss = 3.10215\n",
      "epoch no.3 train no.216400  loss = 4.14156 avg_loss = 3.12985\n",
      "epoch no.3 train no.216410  loss = 2.70601 avg_loss = 3.11988\n",
      "epoch no.3 train no.216420  loss = 3.67441 avg_loss = 3.11456\n",
      "epoch no.3 train no.216430  loss = 1.74044 avg_loss = 3.12247\n",
      "epoch no.3 train no.216440  loss = 3.46759 avg_loss = 3.14895\n",
      "epoch no.3 train no.216450  loss = 4.27466 avg_loss = 3.15393\n",
      "epoch no.3 train no.216460  loss = 3.53222 avg_loss = 3.15944\n",
      "epoch no.3 train no.216470  loss = 2.31925 avg_loss = 3.17242\n",
      "epoch no.3 train no.216480  loss = 1.28923 avg_loss = 3.14485\n",
      "epoch no.3 train no.216490  loss = 1.63388 avg_loss = 3.16765\n",
      "epoch no.3 train no.216500  loss = 3.50951 avg_loss = 3.15016\n",
      "epoch no.3 train no.216510  loss = 2.99985 avg_loss = 3.14979\n",
      "epoch no.3 train no.216520  loss = 4.49596 avg_loss = 3.13810\n",
      "epoch no.3 train no.216530  loss = 3.86988 avg_loss = 3.15778\n",
      "epoch no.3 train no.216540  loss = 4.04914 avg_loss = 3.16161\n",
      "epoch no.3 train no.216550  loss = 3.86451 avg_loss = 3.15129\n",
      "epoch no.3 train no.216560  loss = 3.28895 avg_loss = 3.17165\n",
      "epoch no.3 train no.216570  loss = 1.65200 avg_loss = 3.20247\n",
      "epoch no.3 train no.216580  loss = 4.78236 avg_loss = 3.22604\n",
      "epoch no.3 train no.216590  loss = 3.60133 avg_loss = 3.20282\n",
      "epoch no.3 train no.216600  loss = 3.16357 avg_loss = 3.19288\n",
      "epoch no.3 train no.216610  loss = 2.46730 avg_loss = 3.22215\n",
      "epoch no.3 train no.216620  loss = 2.70486 avg_loss = 3.16782\n",
      "epoch no.3 train no.216630  loss = 3.25439 avg_loss = 3.25672\n",
      "epoch no.3 train no.216640  loss = 4.70403 avg_loss = 3.25606\n",
      "epoch no.3 train no.216650  loss = 2.16449 avg_loss = 3.26220\n",
      "epoch no.3 train no.216660  loss = 2.05863 avg_loss = 3.26383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.216670  loss = 2.53998 avg_loss = 3.24510\n",
      "epoch no.3 train no.216680  loss = 3.01216 avg_loss = 3.31146\n",
      "epoch no.3 train no.216690  loss = 1.68393 avg_loss = 3.24456\n",
      "epoch no.3 train no.216700  loss = 2.39754 avg_loss = 3.24780\n",
      "epoch no.3 train no.216710  loss = 3.63750 avg_loss = 3.25466\n",
      "epoch no.3 train no.216720  loss = 1.58745 avg_loss = 3.26315\n",
      "epoch no.3 train no.216730  loss = 2.46508 avg_loss = 3.24822\n",
      "epoch no.3 train no.216740  loss = 2.85074 avg_loss = 3.29551\n",
      "epoch no.3 train no.216750  loss = 2.70069 avg_loss = 3.24707\n",
      "epoch no.3 train no.216760  loss = 4.48272 avg_loss = 3.27495\n",
      "epoch no.3 train no.216770  loss = 2.59492 avg_loss = 3.27050\n",
      "epoch no.3 train no.216780  loss = 2.52329 avg_loss = 3.23697\n",
      "epoch no.3 train no.216790  loss = 3.85333 avg_loss = 3.23377\n",
      "epoch no.3 train no.216800  loss = 2.62330 avg_loss = 3.19049\n",
      "epoch no.3 train no.216810  loss = 2.69959 avg_loss = 3.18978\n",
      "epoch no.3 train no.216820  loss = 3.69043 avg_loss = 3.19510\n",
      "epoch no.3 train no.216830  loss = 3.78411 avg_loss = 3.20228\n",
      "epoch no.3 train no.216840  loss = 3.26532 avg_loss = 3.25188\n",
      "epoch no.3 train no.216850  loss = 2.66792 avg_loss = 3.22976\n",
      "epoch no.3 train no.216860  loss = 3.34414 avg_loss = 3.20983\n",
      "epoch no.3 train no.216870  loss = 2.58859 avg_loss = 3.22259\n",
      "epoch no.3 train no.216880  loss = 3.21545 avg_loss = 3.19100\n",
      "epoch no.3 train no.216890  loss = 3.46650 avg_loss = 3.19670\n",
      "epoch no.3 train no.216900  loss = 3.21199 avg_loss = 3.14102\n",
      "epoch no.3 train no.216910  loss = 2.69163 avg_loss = 3.12854\n",
      "epoch no.3 train no.216920  loss = 4.68771 avg_loss = 3.12657\n",
      "epoch no.3 train no.216930  loss = 1.23484 avg_loss = 3.10107\n",
      "epoch no.3 train no.216940  loss = 2.88356 avg_loss = 3.04698\n",
      "epoch no.3 train no.216950  loss = 2.58566 avg_loss = 3.05639\n",
      "epoch no.3 train no.216960  loss = 4.65959 avg_loss = 3.06828\n",
      "epoch no.3 train no.216970  loss = 2.00453 avg_loss = 3.08128\n",
      "epoch no.3 train no.216980  loss = 2.01477 avg_loss = 3.05710\n",
      "epoch no.3 train no.216990  loss = 2.78124 avg_loss = 3.09294\n",
      "epoch no.3 train no.217000  loss = 3.73137 avg_loss = 3.10857\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁끝', '▁su', '쾌한', '▁아침', '</s>']\n",
      "여름의 시작 상쾌한 음악</s>\n",
      "epoch no.3 train no.217010  loss = 2.52239 avg_loss = 3.10502\n",
      "epoch no.3 train no.217020  loss = 2.77102 avg_loss = 3.05875\n",
      "epoch no.3 train no.217030  loss = 2.99185 avg_loss = 3.10670\n",
      "epoch no.3 train no.217040  loss = 3.90786 avg_loss = 3.11246\n",
      "epoch no.3 train no.217050  loss = 3.63982 avg_loss = 3.14130\n",
      "epoch no.3 train no.217060  loss = 3.74577 avg_loss = 3.18154\n",
      "epoch no.3 train no.217070  loss = 3.35332 avg_loss = 3.18693\n",
      "epoch no.3 train no.217080  loss = 3.69015 avg_loss = 3.17480\n",
      "epoch no.3 train no.217090  loss = 2.37045 avg_loss = 3.14415\n",
      "epoch no.3 train no.217100  loss = 5.85294 avg_loss = 3.15460\n",
      "epoch no.3 train no.217110  loss = 3.45175 avg_loss = 3.18080\n",
      "epoch no.3 train no.217120  loss = 3.78134 avg_loss = 3.16718\n",
      "epoch no.3 train no.217130  loss = 5.87317 avg_loss = 3.18768\n",
      "epoch no.3 train no.217140  loss = 2.61799 avg_loss = 3.15089\n",
      "epoch no.3 train no.217150  loss = 4.39016 avg_loss = 3.15893\n",
      "epoch no.3 train no.217160  loss = 3.93233 avg_loss = 3.17463\n",
      "epoch no.3 train no.217170  loss = 3.51223 avg_loss = 3.15436\n",
      "epoch no.3 train no.217180  loss = 2.37132 avg_loss = 3.17548\n",
      "epoch no.3 train no.217190  loss = 2.90227 avg_loss = 3.17651\n",
      "epoch no.3 train no.217200  loss = 4.20432 avg_loss = 3.20779\n",
      "epoch no.3 train no.217210  loss = 4.51688 avg_loss = 3.22244\n",
      "epoch no.3 train no.217220  loss = 3.43943 avg_loss = 3.25471\n",
      "epoch no.3 train no.217230  loss = 2.52538 avg_loss = 3.28526\n",
      "epoch no.3 train no.217240  loss = 2.64911 avg_loss = 3.34550\n",
      "epoch no.3 train no.217250  loss = 3.57065 avg_loss = 3.29564\n",
      "epoch no.3 train no.217260  loss = 2.97486 avg_loss = 3.30566\n",
      "epoch no.3 train no.217270  loss = 2.47970 avg_loss = 3.28048\n",
      "epoch no.3 train no.217280  loss = 2.45610 avg_loss = 3.22472\n",
      "epoch no.3 train no.217290  loss = 3.51177 avg_loss = 3.25986\n",
      "epoch no.3 train no.217300  loss = 2.74382 avg_loss = 3.23841\n",
      "epoch no.3 train no.217310  loss = 2.98229 avg_loss = 3.19954\n",
      "epoch no.3 train no.217320  loss = 1.81140 avg_loss = 3.21172\n",
      "epoch no.3 train no.217330  loss = 3.13285 avg_loss = 3.27364\n",
      "epoch no.3 train no.217340  loss = 2.42303 avg_loss = 3.27955\n",
      "epoch no.3 train no.217350  loss = 4.08969 avg_loss = 3.28747\n",
      "epoch no.3 train no.217360  loss = 2.30029 avg_loss = 3.26287\n",
      "epoch no.3 train no.217370  loss = 5.02236 avg_loss = 3.24539\n",
      "epoch no.3 train no.217380  loss = 2.44211 avg_loss = 3.21382\n",
      "epoch no.3 train no.217390  loss = 4.22260 avg_loss = 3.17562\n",
      "epoch no.3 train no.217400  loss = 3.53126 avg_loss = 3.19230\n",
      "epoch no.3 train no.217410  loss = 4.20793 avg_loss = 3.20683\n",
      "epoch no.3 train no.217420  loss = 3.18872 avg_loss = 3.24413\n",
      "epoch no.3 train no.217430  loss = 3.22034 avg_loss = 3.23116\n",
      "epoch no.3 train no.217440  loss = 3.61650 avg_loss = 3.24767\n",
      "epoch no.3 train no.217450  loss = 1.85127 avg_loss = 3.27283\n",
      "epoch no.3 train no.217460  loss = 2.66948 avg_loss = 3.30464\n",
      "epoch no.3 train no.217470  loss = 3.42029 avg_loss = 3.28980\n",
      "epoch no.3 train no.217480  loss = 3.40017 avg_loss = 3.25032\n",
      "epoch no.3 train no.217490  loss = 3.50347 avg_loss = 3.26588\n",
      "epoch no.3 train no.217500  loss = 3.46699 avg_loss = 3.25937\n",
      "epoch no.3 train no.217510  loss = 3.91196 avg_loss = 3.26986\n",
      "epoch no.3 train no.217520  loss = 2.38641 avg_loss = 3.27092\n",
      "epoch no.3 train no.217530  loss = 4.26973 avg_loss = 3.26804\n",
      "epoch no.3 train no.217540  loss = 3.10258 avg_loss = 3.25561\n",
      "epoch no.3 train no.217550  loss = 3.32581 avg_loss = 3.24775\n",
      "epoch no.3 train no.217560  loss = 3.51874 avg_loss = 3.27572\n",
      "epoch no.3 train no.217570  loss = 3.22009 avg_loss = 3.23442\n",
      "epoch no.3 train no.217580  loss = 2.69761 avg_loss = 3.20113\n",
      "epoch no.3 train no.217590  loss = 3.19916 avg_loss = 3.20507\n",
      "epoch no.3 train no.217600  loss = 3.27610 avg_loss = 3.21188\n",
      "epoch no.3 train no.217610  loss = 2.67256 avg_loss = 3.17980\n",
      "epoch no.3 train no.217620  loss = 2.80824 avg_loss = 3.21722\n",
      "epoch no.3 train no.217630  loss = 4.01511 avg_loss = 3.25777\n",
      "epoch no.3 train no.217640  loss = 3.64355 avg_loss = 3.28515\n",
      "epoch no.3 train no.217650  loss = 3.42952 avg_loss = 3.25905\n",
      "epoch no.3 train no.217660  loss = 2.58943 avg_loss = 3.23180\n",
      "epoch no.3 train no.217670  loss = 4.80934 avg_loss = 3.24454\n",
      "epoch no.3 train no.217680  loss = 3.85299 avg_loss = 3.24332\n",
      "epoch no.3 train no.217690  loss = 5.11552 avg_loss = 3.23357\n",
      "epoch no.3 train no.217700  loss = 3.65321 avg_loss = 3.24925\n",
      "epoch no.3 train no.217710  loss = 5.99250 avg_loss = 3.23761\n",
      "epoch no.3 train no.217720  loss = 5.81257 avg_loss = 3.27528\n",
      "epoch no.3 train no.217730  loss = 3.40266 avg_loss = 3.24755\n",
      "epoch no.3 train no.217740  loss = 2.37861 avg_loss = 3.21150\n",
      "epoch no.3 train no.217750  loss = 3.05493 avg_loss = 3.22020\n",
      "epoch no.3 train no.217760  loss = 2.96665 avg_loss = 3.21448\n",
      "epoch no.3 train no.217770  loss = 4.08233 avg_loss = 3.20850\n",
      "epoch no.3 train no.217780  loss = 2.16120 avg_loss = 3.18036\n",
      "epoch no.3 train no.217790  loss = 4.83399 avg_loss = 3.18213\n",
      "epoch no.3 train no.217800  loss = 4.23459 avg_loss = 3.17154\n",
      "epoch no.3 train no.217810  loss = 2.95399 avg_loss = 3.17771\n",
      "epoch no.3 train no.217820  loss = 5.33062 avg_loss = 3.20942\n",
      "epoch no.3 train no.217830  loss = 2.77423 avg_loss = 3.21333\n",
      "epoch no.3 train no.217840  loss = 2.25456 avg_loss = 3.23207\n",
      "epoch no.3 train no.217850  loss = 2.26008 avg_loss = 3.21178\n",
      "epoch no.3 train no.217860  loss = 3.36015 avg_loss = 3.23014\n",
      "epoch no.3 train no.217870  loss = 2.48145 avg_loss = 3.17663\n",
      "epoch no.3 train no.217880  loss = 1.91851 avg_loss = 3.18914\n",
      "epoch no.3 train no.217890  loss = 4.06271 avg_loss = 3.25103\n",
      "epoch no.3 train no.217900  loss = 3.69935 avg_loss = 3.24333\n",
      "epoch no.3 train no.217910  loss = 3.23743 avg_loss = 3.26031\n",
      "epoch no.3 train no.217920  loss = 2.43828 avg_loss = 3.21732\n",
      "epoch no.3 train no.217930  loss = 3.59510 avg_loss = 3.25521\n",
      "epoch no.3 train no.217940  loss = 3.10283 avg_loss = 3.21851\n",
      "epoch no.3 train no.217950  loss = 1.93358 avg_loss = 3.20028\n",
      "epoch no.3 train no.217960  loss = 4.06107 avg_loss = 3.26411\n",
      "epoch no.3 train no.217970  loss = 3.36595 avg_loss = 3.28118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.217980  loss = 1.97151 avg_loss = 3.27221\n",
      "epoch no.3 train no.217990  loss = 4.76666 avg_loss = 3.31888\n",
      "epoch no.3 train no.218000  loss = 2.61834 avg_loss = 3.29218\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '하며', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 산책하며 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.218010  loss = 3.28528 avg_loss = 3.23983\n",
      "epoch no.3 train no.218020  loss = 3.48736 avg_loss = 3.25849\n",
      "epoch no.3 train no.218030  loss = 2.16902 avg_loss = 3.21469\n",
      "epoch no.3 train no.218040  loss = 3.54586 avg_loss = 3.19252\n",
      "epoch no.3 train no.218050  loss = 3.70438 avg_loss = 3.18179\n",
      "epoch no.3 train no.218060  loss = 1.57570 avg_loss = 3.21140\n",
      "epoch no.3 train no.218070  loss = 2.06423 avg_loss = 3.19035\n",
      "epoch no.3 train no.218080  loss = 2.78693 avg_loss = 3.17637\n",
      "epoch no.3 train no.218090  loss = 3.49724 avg_loss = 3.16814\n",
      "epoch no.3 train no.218100  loss = 2.68928 avg_loss = 3.16935\n",
      "epoch no.3 train no.218110  loss = 3.28289 avg_loss = 3.14075\n",
      "epoch no.3 train no.218120  loss = 3.00107 avg_loss = 3.17998\n",
      "epoch no.3 train no.218130  loss = 4.52129 avg_loss = 3.19258\n",
      "epoch no.3 train no.218140  loss = 3.32289 avg_loss = 3.21605\n",
      "epoch no.3 train no.218150  loss = 2.46124 avg_loss = 3.16344\n",
      "epoch no.3 train no.218160  loss = 2.10273 avg_loss = 3.17891\n",
      "epoch no.3 train no.218170  loss = 2.72208 avg_loss = 3.19122\n",
      "epoch no.3 train no.218180  loss = 1.93070 avg_loss = 3.15974\n",
      "epoch no.3 train no.218190  loss = 3.04298 avg_loss = 3.16360\n",
      "epoch no.3 train no.218200  loss = 2.65398 avg_loss = 3.16366\n",
      "epoch no.3 train no.218210  loss = 2.61251 avg_loss = 3.17589\n",
      "epoch no.3 train no.218220  loss = 2.07127 avg_loss = 3.16740\n",
      "epoch no.3 train no.218230  loss = 2.69704 avg_loss = 3.19377\n",
      "epoch no.3 train no.218240  loss = 4.34616 avg_loss = 3.19427\n",
      "epoch no.3 train no.218250  loss = 2.52302 avg_loss = 3.17316\n",
      "epoch no.3 train no.218260  loss = 2.92271 avg_loss = 3.18750\n",
      "epoch no.3 train no.218270  loss = 2.56630 avg_loss = 3.17110\n",
      "epoch no.3 train no.218280  loss = 3.47802 avg_loss = 3.19234\n",
      "epoch no.3 train no.218290  loss = 2.87741 avg_loss = 3.25381\n",
      "epoch no.3 train no.218300  loss = 1.97535 avg_loss = 3.29091\n",
      "epoch no.3 train no.218310  loss = 2.80018 avg_loss = 3.30413\n",
      "epoch no.3 train no.218320  loss = 3.30465 avg_loss = 3.31590\n",
      "epoch no.3 train no.218330  loss = 1.87287 avg_loss = 3.31206\n",
      "epoch no.3 train no.218340  loss = 3.80137 avg_loss = 3.31778\n",
      "epoch no.3 train no.218350  loss = 2.26069 avg_loss = 3.32296\n",
      "epoch no.3 train no.218360  loss = 3.50443 avg_loss = 3.25421\n",
      "epoch no.3 train no.218370  loss = 3.50792 avg_loss = 3.23877\n",
      "epoch no.3 train no.218380  loss = 3.73484 avg_loss = 3.26240\n",
      "epoch no.3 train no.218390  loss = 4.61713 avg_loss = 3.23074\n",
      "epoch no.3 train no.218400  loss = 3.85461 avg_loss = 3.24144\n",
      "epoch no.3 train no.218410  loss = 2.89245 avg_loss = 3.21841\n",
      "epoch no.3 train no.218420  loss = 2.23787 avg_loss = 3.25048\n",
      "epoch no.3 train no.218430  loss = 3.64750 avg_loss = 3.30600\n",
      "epoch no.3 train no.218440  loss = 1.45643 avg_loss = 3.28655\n",
      "epoch no.3 train no.218450  loss = 2.99265 avg_loss = 3.31928\n",
      "epoch no.3 train no.218460  loss = 4.37424 avg_loss = 3.39634\n",
      "epoch no.3 train no.218470  loss = 4.32845 avg_loss = 3.40444\n",
      "epoch no.3 train no.218480  loss = 3.55843 avg_loss = 3.34579\n",
      "epoch no.3 train no.218490  loss = 3.43735 avg_loss = 3.37324\n",
      "epoch no.3 train no.218500  loss = 2.51722 avg_loss = 3.35851\n",
      "epoch no.3 train no.218510  loss = 3.04461 avg_loss = 3.30205\n",
      "epoch no.3 train no.218520  loss = 2.30532 avg_loss = 3.31003\n",
      "epoch no.3 train no.218530  loss = 2.85788 avg_loss = 3.29156\n",
      "epoch no.3 train no.218540  loss = 4.16176 avg_loss = 3.27172\n",
      "epoch no.3 train no.218550  loss = 5.67522 avg_loss = 3.31241\n",
      "epoch no.3 train no.218560  loss = 3.00091 avg_loss = 3.27382\n",
      "epoch no.3 train no.218570  loss = 3.11548 avg_loss = 3.25176\n",
      "epoch no.3 train no.218580  loss = 3.47671 avg_loss = 3.27986\n",
      "epoch no.3 train no.218590  loss = 2.45093 avg_loss = 3.27133\n",
      "epoch no.3 train no.218600  loss = 3.23261 avg_loss = 3.24561\n",
      "epoch no.3 train no.218610  loss = 3.10869 avg_loss = 3.24197\n",
      "epoch no.3 train no.218620  loss = 5.44989 avg_loss = 3.24341\n",
      "epoch no.3 train no.218630  loss = 3.24982 avg_loss = 3.23975\n",
      "epoch no.3 train no.218640  loss = 2.72141 avg_loss = 3.24959\n",
      "epoch no.3 train no.218650  loss = 3.24042 avg_loss = 3.25159\n",
      "epoch no.3 train no.218660  loss = 1.36361 avg_loss = 3.24406\n",
      "epoch no.3 train no.218670  loss = 3.16098 avg_loss = 3.30790\n",
      "epoch no.3 train no.218680  loss = 1.93315 avg_loss = 3.29626\n",
      "epoch no.3 train no.218690  loss = 2.94081 avg_loss = 3.29327\n",
      "epoch no.3 train no.218700  loss = 3.50906 avg_loss = 3.29133\n",
      "epoch no.3 train no.218710  loss = 2.33160 avg_loss = 3.26180\n",
      "epoch no.3 train no.218720  loss = 3.42218 avg_loss = 3.24790\n",
      "epoch no.3 train no.218730  loss = 3.68409 avg_loss = 3.23011\n",
      "epoch no.3 train no.218740  loss = 2.02022 avg_loss = 3.16204\n",
      "epoch no.3 train no.218750  loss = 2.27645 avg_loss = 3.22370\n",
      "epoch no.3 train no.218760  loss = 4.52489 avg_loss = 3.27889\n",
      "epoch no.3 train no.218770  loss = 3.24831 avg_loss = 3.28842\n",
      "epoch no.3 train no.218780  loss = 3.98381 avg_loss = 3.28474\n",
      "epoch no.3 train no.218790  loss = 2.55104 avg_loss = 3.28495\n",
      "epoch no.3 train no.218800  loss = 3.26058 avg_loss = 3.29677\n",
      "epoch no.3 train no.218810  loss = 3.85307 avg_loss = 3.30724\n",
      "epoch no.3 train no.218820  loss = 2.68565 avg_loss = 3.31729\n",
      "epoch no.3 train no.218830  loss = 4.15285 avg_loss = 3.33379\n",
      "epoch no.3 train no.218840  loss = 4.27051 avg_loss = 3.32796\n",
      "epoch no.3 train no.218850  loss = 3.62533 avg_loss = 3.32289\n",
      "epoch no.3 train no.218860  loss = 3.25625 avg_loss = 3.36213\n",
      "epoch no.3 train no.218870  loss = 3.06784 avg_loss = 3.34785\n",
      "epoch no.3 train no.218880  loss = 2.33528 avg_loss = 3.31887\n",
      "epoch no.3 train no.218890  loss = 2.34773 avg_loss = 3.30353\n",
      "epoch no.3 train no.218900  loss = 2.79763 avg_loss = 3.24177\n",
      "epoch no.3 train no.218910  loss = 3.61406 avg_loss = 3.22301\n",
      "epoch no.3 train no.218920  loss = 2.13903 avg_loss = 3.19757\n",
      "epoch no.3 train no.218930  loss = 2.58987 avg_loss = 3.18052\n",
      "epoch no.3 train no.218940  loss = 6.52924 avg_loss = 3.24428\n",
      "epoch no.3 train no.218950  loss = 2.56928 avg_loss = 3.27604\n",
      "epoch no.3 train no.218960  loss = 2.58616 avg_loss = 3.25870\n",
      "epoch no.3 train no.218970  loss = 3.06214 avg_loss = 3.25011\n",
      "epoch no.3 train no.218980  loss = 3.61275 avg_loss = 3.21201\n",
      "epoch no.3 train no.218990  loss = 2.30745 avg_loss = 3.19306\n",
      "epoch no.3 train no.219000  loss = 3.93430 avg_loss = 3.19870\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓을', '▁감성', '디', '</s>']\n",
      "여름밤을 수놓을 멜로디</s>\n",
      "epoch no.3 train no.219010  loss = 3.83602 avg_loss = 3.19014\n",
      "epoch no.3 train no.219020  loss = 4.24895 avg_loss = 3.18429\n",
      "epoch no.3 train no.219030  loss = 4.15122 avg_loss = 3.24263\n",
      "epoch no.3 train no.219040  loss = 3.23293 avg_loss = 3.27901\n",
      "epoch no.3 train no.219050  loss = 4.77382 avg_loss = 3.30996\n",
      "epoch no.3 train no.219060  loss = 2.45907 avg_loss = 3.24883\n",
      "epoch no.3 train no.219070  loss = 2.72317 avg_loss = 3.31356\n",
      "epoch no.3 train no.219080  loss = 4.71961 avg_loss = 3.32930\n",
      "epoch no.3 train no.219090  loss = 2.55400 avg_loss = 3.26736\n",
      "epoch no.3 train no.219100  loss = 2.25389 avg_loss = 3.24656\n",
      "epoch no.3 train no.219110  loss = 4.42752 avg_loss = 3.24542\n",
      "epoch no.3 train no.219120  loss = 1.92229 avg_loss = 3.22711\n",
      "epoch no.3 train no.219130  loss = 2.00147 avg_loss = 3.19408\n",
      "epoch no.3 train no.219140  loss = 5.50988 avg_loss = 3.21833\n",
      "epoch no.3 train no.219150  loss = 2.73707 avg_loss = 3.27219\n",
      "epoch no.3 train no.219160  loss = 3.77949 avg_loss = 3.26794\n",
      "epoch no.3 train no.219170  loss = 4.93499 avg_loss = 3.32283\n",
      "epoch no.3 train no.219180  loss = 3.08529 avg_loss = 3.30624\n",
      "epoch no.3 train no.219190  loss = 4.06180 avg_loss = 3.27874\n",
      "epoch no.3 train no.219200  loss = 3.94553 avg_loss = 3.29355\n",
      "epoch no.3 train no.219210  loss = 4.29615 avg_loss = 3.26371\n",
      "epoch no.3 train no.219220  loss = 2.56477 avg_loss = 3.31722\n",
      "epoch no.3 train no.219230  loss = 4.22015 avg_loss = 3.31814\n",
      "epoch no.3 train no.219240  loss = 3.20742 avg_loss = 3.28705\n",
      "epoch no.3 train no.219250  loss = 2.77897 avg_loss = 3.28955\n",
      "epoch no.3 train no.219260  loss = 2.72067 avg_loss = 3.30363\n",
      "epoch no.3 train no.219270  loss = 5.03252 avg_loss = 3.32454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.219280  loss = 4.23006 avg_loss = 3.32970\n",
      "epoch no.3 train no.219290  loss = 3.37432 avg_loss = 3.29298\n",
      "epoch no.3 train no.219300  loss = 2.91335 avg_loss = 3.26984\n",
      "epoch no.3 train no.219310  loss = 1.95382 avg_loss = 3.24504\n",
      "epoch no.3 train no.219320  loss = 2.11135 avg_loss = 3.21440\n",
      "epoch no.3 train no.219330  loss = 4.16965 avg_loss = 3.24892\n",
      "epoch no.3 train no.219340  loss = 2.53984 avg_loss = 3.24423\n",
      "epoch no.3 train no.219350  loss = 2.14419 avg_loss = 3.20418\n",
      "epoch no.3 train no.219360  loss = 2.72503 avg_loss = 3.24844\n",
      "epoch no.3 train no.219370  loss = 3.38099 avg_loss = 3.25180\n",
      "epoch no.3 train no.219380  loss = 3.67072 avg_loss = 3.20916\n",
      "epoch no.3 train no.219390  loss = 2.77824 avg_loss = 3.17415\n",
      "epoch no.3 train no.219400  loss = 3.55473 avg_loss = 3.18953\n",
      "epoch no.3 train no.219410  loss = 3.34670 avg_loss = 3.19547\n",
      "epoch no.3 train no.219420  loss = 2.92288 avg_loss = 3.18132\n",
      "epoch no.3 train no.219430  loss = 4.95314 avg_loss = 3.24723\n",
      "epoch no.3 train no.219440  loss = 4.09644 avg_loss = 3.21941\n",
      "epoch no.3 train no.219450  loss = 2.79404 avg_loss = 3.23415\n",
      "epoch no.3 train no.219460  loss = 3.24333 avg_loss = 3.23002\n",
      "epoch no.3 train no.219470  loss = 2.58796 avg_loss = 3.20726\n",
      "epoch no.3 train no.219480  loss = 1.81909 avg_loss = 3.15772\n",
      "epoch no.3 train no.219490  loss = 1.97630 avg_loss = 3.14683\n",
      "epoch no.3 train no.219500  loss = 3.48238 avg_loss = 3.15457\n",
      "epoch no.3 train no.219510  loss = 3.76912 avg_loss = 3.16804\n",
      "epoch no.3 train no.219520  loss = 4.88717 avg_loss = 3.23895\n",
      "epoch no.3 train no.219530  loss = 3.05287 avg_loss = 3.21922\n",
      "epoch no.3 train no.219540  loss = 2.46508 avg_loss = 3.20384\n",
      "epoch no.3 train no.219550  loss = 2.76067 avg_loss = 3.17780\n",
      "epoch no.3 train no.219560  loss = 2.20073 avg_loss = 3.20357\n",
      "epoch no.3 train no.219570  loss = 2.16108 avg_loss = 3.25183\n",
      "epoch no.3 train no.219580  loss = 4.95672 avg_loss = 3.26561\n",
      "epoch no.3 train no.219590  loss = 5.57192 avg_loss = 3.30150\n",
      "epoch no.3 train no.219600  loss = 2.94639 avg_loss = 3.25453\n",
      "epoch no.3 train no.219610  loss = 3.31700 avg_loss = 3.26170\n",
      "epoch no.3 train no.219620  loss = 3.23265 avg_loss = 3.27278\n",
      "epoch no.3 train no.219630  loss = 3.00922 avg_loss = 3.26658\n",
      "epoch no.3 train no.219640  loss = 2.59599 avg_loss = 3.28576\n",
      "epoch no.3 train no.219650  loss = 4.32553 avg_loss = 3.28545\n",
      "epoch no.3 train no.219660  loss = 3.44805 avg_loss = 3.26647\n",
      "epoch no.3 train no.219670  loss = 4.79267 avg_loss = 3.30517\n",
      "epoch no.3 train no.219680  loss = 2.48293 avg_loss = 3.28361\n",
      "epoch no.3 train no.219690  loss = 1.94934 avg_loss = 3.22695\n",
      "epoch no.3 train no.219700  loss = 3.11537 avg_loss = 3.24624\n",
      "epoch no.3 train no.219710  loss = 4.30517 avg_loss = 3.22457\n",
      "epoch no.3 train no.219720  loss = 3.71656 avg_loss = 3.18029\n",
      "epoch no.3 train no.219730  loss = 4.12284 avg_loss = 3.24863\n",
      "epoch no.3 train no.219740  loss = 3.39746 avg_loss = 3.24023\n",
      "epoch no.3 train no.219750  loss = 2.72457 avg_loss = 3.24245\n",
      "epoch no.3 train no.219760  loss = 4.47708 avg_loss = 3.23821\n",
      "epoch no.3 train no.219770  loss = 3.80215 avg_loss = 3.20921\n",
      "epoch no.3 train no.219780  loss = 2.60371 avg_loss = 3.20969\n",
      "epoch no.3 train no.219790  loss = 3.05149 avg_loss = 3.18088\n",
      "epoch no.3 train no.219800  loss = 4.27094 avg_loss = 3.18376\n",
      "epoch no.3 train no.219810  loss = 3.46099 avg_loss = 3.18087\n",
      "epoch no.3 train no.219820  loss = 3.02691 avg_loss = 3.18596\n",
      "epoch no.3 train no.219830  loss = 2.87751 avg_loss = 3.19128\n",
      "epoch no.3 train no.219840  loss = 2.83167 avg_loss = 3.16717\n",
      "epoch no.3 train no.219850  loss = 2.96711 avg_loss = 3.13301\n",
      "epoch no.3 train no.219860  loss = 2.20859 avg_loss = 3.11127\n",
      "epoch no.3 train no.219870  loss = 2.97822 avg_loss = 3.16681\n",
      "epoch no.3 train no.219880  loss = 2.43611 avg_loss = 3.16994\n",
      "epoch no.3 train no.219890  loss = 3.99019 avg_loss = 3.18445\n",
      "epoch no.3 train no.219900  loss = 4.14651 avg_loss = 3.19024\n",
      "epoch no.3 train no.219910  loss = 2.44062 avg_loss = 3.18037\n",
      "epoch no.3 train no.219920  loss = 3.51327 avg_loss = 3.13565\n",
      "epoch no.3 train no.219930  loss = 2.09425 avg_loss = 3.11129\n",
      "epoch no.3 train no.219940  loss = 2.66398 avg_loss = 3.13214\n",
      "epoch no.3 train no.219950  loss = 3.99705 avg_loss = 3.15070\n",
      "epoch no.3 train no.219960  loss = 2.17815 avg_loss = 3.20860\n",
      "epoch no.3 train no.219970  loss = 2.56820 avg_loss = 3.25160\n",
      "epoch no.3 train no.219980  loss = 3.00193 avg_loss = 3.22274\n",
      "epoch no.3 train no.219990  loss = 3.70732 avg_loss = 3.21659\n",
      "epoch no.3 train no.220000  loss = 4.54096 avg_loss = 3.20325\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '을', '▁수', '놓을', '▁감성', '▁음악', '</s>']\n",
      "여름밤을 수놓을 감각적인 재즈</s>\n",
      "epoch no.3 train no.220010  loss = 2.93275 avg_loss = 3.23075\n",
      "epoch no.3 train no.220020  loss = 3.63579 avg_loss = 3.23763\n",
      "epoch no.3 train no.220030  loss = 1.95047 avg_loss = 3.26226\n",
      "epoch no.3 train no.220040  loss = 2.35759 avg_loss = 3.22295\n",
      "epoch no.3 train no.220050  loss = 5.57948 avg_loss = 3.23114\n",
      "epoch no.3 train no.220060  loss = 3.62619 avg_loss = 3.24689\n",
      "epoch no.3 train no.220070  loss = 1.77812 avg_loss = 3.23677\n",
      "epoch no.3 train no.220080  loss = 3.98739 avg_loss = 3.26944\n",
      "epoch no.3 train no.220090  loss = 2.64462 avg_loss = 3.24113\n",
      "epoch no.3 train no.220100  loss = 4.12620 avg_loss = 3.30530\n",
      "epoch no.3 train no.220110  loss = 2.19053 avg_loss = 3.30607\n",
      "epoch no.3 train no.220120  loss = 2.40079 avg_loss = 3.25388\n",
      "epoch no.3 train no.220130  loss = 1.97007 avg_loss = 3.23542\n",
      "epoch no.3 train no.220140  loss = 3.39819 avg_loss = 3.24574\n",
      "epoch no.3 train no.220150  loss = 1.61201 avg_loss = 3.23884\n",
      "epoch no.3 train no.220160  loss = 2.84436 avg_loss = 3.21943\n",
      "epoch no.3 train no.220170  loss = 1.38908 avg_loss = 3.17654\n",
      "epoch no.3 train no.220180  loss = 4.07875 avg_loss = 3.13683\n",
      "epoch no.3 train no.220190  loss = 3.66766 avg_loss = 3.19803\n",
      "epoch no.3 train no.220200  loss = 2.18873 avg_loss = 3.16127\n",
      "epoch no.3 train no.220210  loss = 2.67588 avg_loss = 3.23962\n",
      "epoch no.3 train no.220220  loss = 3.41844 avg_loss = 3.25852\n",
      "epoch no.3 train no.220230  loss = 2.53670 avg_loss = 3.20606\n",
      "epoch no.3 train no.220240  loss = 2.43638 avg_loss = 3.20790\n",
      "epoch no.3 train no.220250  loss = 3.80135 avg_loss = 3.19533\n",
      "epoch no.3 train no.220260  loss = 4.10562 avg_loss = 3.21711\n",
      "epoch no.3 train no.220270  loss = 3.57043 avg_loss = 3.20249\n",
      "epoch no.3 train no.220280  loss = 3.21995 avg_loss = 3.18824\n",
      "epoch no.3 train no.220290  loss = 4.02202 avg_loss = 3.18943\n",
      "epoch no.3 train no.220300  loss = 3.52200 avg_loss = 3.26010\n",
      "epoch no.3 train no.220310  loss = 4.36611 avg_loss = 3.27506\n",
      "epoch no.3 train no.220320  loss = 3.89625 avg_loss = 3.27207\n",
      "epoch no.3 train no.220330  loss = 4.18093 avg_loss = 3.28370\n",
      "epoch no.3 train no.220340  loss = 2.93521 avg_loss = 3.28621\n",
      "epoch no.3 train no.220350  loss = 2.65643 avg_loss = 3.30749\n",
      "epoch no.3 train no.220360  loss = 3.09063 avg_loss = 3.28869\n",
      "epoch no.3 train no.220370  loss = 4.65160 avg_loss = 3.26803\n",
      "epoch no.3 train no.220380  loss = 3.97116 avg_loss = 3.30597\n",
      "epoch no.3 train no.220390  loss = 2.35621 avg_loss = 3.28505\n",
      "epoch no.3 train no.220400  loss = 2.63686 avg_loss = 3.26781\n",
      "epoch no.3 train no.220410  loss = 0.86439 avg_loss = 3.19035\n",
      "epoch no.3 train no.220420  loss = 2.67523 avg_loss = 3.18784\n",
      "epoch no.3 train no.220430  loss = 2.51341 avg_loss = 3.25032\n",
      "epoch no.3 train no.220440  loss = 3.29224 avg_loss = 3.23498\n",
      "epoch no.3 train no.220450  loss = 2.76437 avg_loss = 3.24264\n",
      "epoch no.3 train no.220460  loss = 3.59749 avg_loss = 3.22462\n",
      "epoch no.3 train no.220470  loss = 4.56649 avg_loss = 3.20116\n",
      "epoch no.3 train no.220480  loss = 3.01889 avg_loss = 3.22058\n",
      "epoch no.3 train no.220490  loss = 3.15079 avg_loss = 3.24084\n",
      "epoch no.3 train no.220500  loss = 3.55460 avg_loss = 3.24911\n",
      "epoch no.3 train no.220510  loss = 2.34195 avg_loss = 3.30601\n",
      "epoch no.3 train no.220520  loss = 2.66187 avg_loss = 3.31069\n",
      "epoch no.3 train no.220530  loss = 5.20718 avg_loss = 3.32486\n",
      "epoch no.3 train no.220540  loss = 2.57266 avg_loss = 3.30010\n",
      "epoch no.3 train no.220550  loss = 2.80499 avg_loss = 3.26645\n",
      "epoch no.3 train no.220560  loss = 3.03292 avg_loss = 3.23544\n",
      "epoch no.3 train no.220570  loss = 1.56356 avg_loss = 3.23098\n",
      "epoch no.3 train no.220580  loss = 4.22469 avg_loss = 3.22008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.220590  loss = 3.76125 avg_loss = 3.23466\n",
      "epoch no.3 train no.220600  loss = 4.05092 avg_loss = 3.27715\n",
      "epoch no.3 train no.220610  loss = 3.52702 avg_loss = 3.26006\n",
      "epoch no.3 train no.220620  loss = 5.46788 avg_loss = 3.26261\n",
      "epoch no.3 train no.220630  loss = 4.24317 avg_loss = 3.28243\n",
      "epoch no.3 train no.220640  loss = 2.47948 avg_loss = 3.20850\n",
      "epoch no.3 train no.220650  loss = 2.26037 avg_loss = 3.17849\n",
      "epoch no.3 train no.220660  loss = 2.48856 avg_loss = 3.15672\n",
      "epoch no.3 train no.220670  loss = 3.08814 avg_loss = 3.15308\n",
      "epoch no.3 train no.220680  loss = 2.46297 avg_loss = 3.14099\n",
      "epoch no.3 train no.220690  loss = 3.30995 avg_loss = 3.15172\n",
      "epoch no.3 train no.220700  loss = 2.66819 avg_loss = 3.17120\n",
      "epoch no.3 train no.220710  loss = 4.63733 avg_loss = 3.17630\n",
      "epoch no.3 train no.220720  loss = 3.22495 avg_loss = 3.15687\n",
      "epoch no.3 train no.220730  loss = 2.97703 avg_loss = 3.17290\n",
      "epoch no.3 train no.220740  loss = 2.76444 avg_loss = 3.18978\n",
      "epoch no.3 train no.220750  loss = 3.20647 avg_loss = 3.15603\n",
      "epoch no.3 train no.220760  loss = 5.40322 avg_loss = 3.21013\n",
      "epoch no.3 train no.220770  loss = 3.16045 avg_loss = 3.22203\n",
      "epoch no.3 train no.220780  loss = 1.92833 avg_loss = 3.22512\n",
      "epoch no.3 train no.220790  loss = 2.05624 avg_loss = 3.19503\n",
      "epoch no.3 train no.220800  loss = 2.51627 avg_loss = 3.20908\n",
      "epoch no.3 train no.220810  loss = 3.74981 avg_loss = 3.21511\n",
      "epoch no.3 train no.220820  loss = 2.67825 avg_loss = 3.19896\n",
      "epoch no.3 train no.220830  loss = 3.06292 avg_loss = 3.16768\n",
      "epoch no.3 train no.220840  loss = 2.14030 avg_loss = 3.19372\n",
      "epoch no.3 train no.220850  loss = 2.07631 avg_loss = 3.15827\n",
      "epoch no.3 train no.220860  loss = 5.21799 avg_loss = 3.21189\n",
      "epoch no.3 train no.220870  loss = 3.57595 avg_loss = 3.21567\n",
      "epoch no.3 train no.220880  loss = 3.20621 avg_loss = 3.17013\n",
      "epoch no.3 train no.220890  loss = 3.04709 avg_loss = 3.17762\n",
      "epoch no.3 train no.220900  loss = 3.53939 avg_loss = 3.18709\n",
      "epoch no.3 train no.220910  loss = 3.23390 avg_loss = 3.18478\n",
      "epoch no.3 train no.220920  loss = 3.20512 avg_loss = 3.17947\n",
      "epoch no.3 train no.220930  loss = 2.99322 avg_loss = 3.20622\n",
      "epoch no.3 train no.220940  loss = 2.09256 avg_loss = 3.20318\n",
      "epoch no.3 train no.220950  loss = 3.16338 avg_loss = 3.15484\n",
      "epoch no.3 train no.220960  loss = 3.87308 avg_loss = 3.14795\n",
      "epoch no.3 train no.220970  loss = 2.59402 avg_loss = 3.12946\n",
      "epoch no.3 train no.220980  loss = 3.05449 avg_loss = 3.09550\n",
      "epoch no.3 train no.220990  loss = 5.78425 avg_loss = 3.08909\n",
      "epoch no.3 train no.221000  loss = 4.66139 avg_loss = 3.13733\n",
      "9\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '셔', '줄', '▁노래', '미', '로운', '▁p', '</s>']\n",
      "여름밤을 적셔줄 감미로운 재즈</s>\n",
      "epoch no.3 train no.221010  loss = 4.28888 avg_loss = 3.17269\n",
      "epoch no.3 train no.221020  loss = 4.23567 avg_loss = 3.12933\n",
      "epoch no.3 train no.221030  loss = 5.05624 avg_loss = 3.12524\n",
      "epoch no.3 train no.221040  loss = 4.19295 avg_loss = 3.12759\n",
      "epoch no.3 train no.221050  loss = 3.37783 avg_loss = 3.21858\n",
      "epoch no.3 train no.221060  loss = 1.65486 avg_loss = 3.20295\n",
      "epoch no.3 train no.221070  loss = 2.99593 avg_loss = 3.25191\n",
      "epoch no.3 train no.221080  loss = 3.52952 avg_loss = 3.26785\n",
      "epoch no.3 train no.221090  loss = 4.30996 avg_loss = 3.31546\n",
      "epoch no.3 train no.221100  loss = 3.27651 avg_loss = 3.35140\n",
      "epoch no.3 train no.221110  loss = 3.02200 avg_loss = 3.35319\n",
      "epoch no.3 train no.221120  loss = 1.96107 avg_loss = 3.33737\n",
      "epoch no.3 train no.221130  loss = 3.44861 avg_loss = 3.28993\n",
      "epoch no.3 train no.221140  loss = 3.27506 avg_loss = 3.31547\n",
      "epoch no.3 train no.221150  loss = 1.72934 avg_loss = 3.25031\n",
      "epoch no.3 train no.221160  loss = 3.93267 avg_loss = 3.24778\n",
      "epoch no.3 train no.221170  loss = 2.15520 avg_loss = 3.21285\n",
      "epoch no.3 train no.221180  loss = 3.48648 avg_loss = 3.27484\n",
      "epoch no.3 train no.221190  loss = 2.99079 avg_loss = 3.30403\n",
      "epoch no.3 train no.221200  loss = 2.58981 avg_loss = 3.32366\n",
      "epoch no.3 train no.221210  loss = 3.09008 avg_loss = 3.29695\n",
      "epoch no.3 train no.221220  loss = 3.48226 avg_loss = 3.31562\n",
      "epoch no.3 train no.221230  loss = 2.96125 avg_loss = 3.31308\n",
      "epoch no.3 train no.221240  loss = 4.50276 avg_loss = 3.30721\n",
      "epoch no.3 train no.221250  loss = 2.79998 avg_loss = 3.26271\n",
      "epoch no.3 train no.221260  loss = 3.06627 avg_loss = 3.26702\n",
      "epoch no.3 train no.221270  loss = 4.27492 avg_loss = 3.27335\n",
      "epoch no.3 train no.221280  loss = 5.05987 avg_loss = 3.26455\n",
      "epoch no.3 train no.221290  loss = 3.22792 avg_loss = 3.23373\n",
      "epoch no.3 train no.221300  loss = 2.90298 avg_loss = 3.19291\n",
      "epoch no.3 train no.221310  loss = 5.59300 avg_loss = 3.22558\n",
      "epoch no.3 train no.221320  loss = 2.47333 avg_loss = 3.21635\n",
      "epoch no.3 train no.221330  loss = 2.12135 avg_loss = 3.21901\n",
      "epoch no.3 train no.221340  loss = 4.02851 avg_loss = 3.25428\n",
      "epoch no.3 train no.221350  loss = 3.51177 avg_loss = 3.23748\n",
      "epoch no.3 train no.221360  loss = 1.55374 avg_loss = 3.25091\n",
      "epoch no.3 train no.221370  loss = 3.38396 avg_loss = 3.22875\n",
      "epoch no.3 train no.221380  loss = 1.36498 avg_loss = 3.18068\n",
      "epoch no.3 train no.221390  loss = 3.03049 avg_loss = 3.15524\n",
      "epoch no.3 train no.221400  loss = 3.04241 avg_loss = 3.14697\n",
      "epoch no.3 train no.221410  loss = 4.78550 avg_loss = 3.15088\n",
      "epoch no.3 train no.221420  loss = 5.05206 avg_loss = 3.18297\n",
      "epoch no.3 train no.221430  loss = 2.02269 avg_loss = 3.19812\n",
      "epoch no.3 train no.221440  loss = 1.86744 avg_loss = 3.18767\n",
      "epoch no.3 train no.221450  loss = 2.79907 avg_loss = 3.19459\n",
      "epoch no.3 train no.221460  loss = 2.14611 avg_loss = 3.14319\n",
      "epoch no.3 train no.221470  loss = 2.93482 avg_loss = 3.12822\n",
      "epoch no.3 train no.221480  loss = 3.37689 avg_loss = 3.16961\n",
      "epoch no.3 train no.221490  loss = 2.77295 avg_loss = 3.16781\n",
      "epoch no.3 train no.221500  loss = 5.17583 avg_loss = 3.17197\n",
      "epoch no.3 train no.221510  loss = 0.91904 avg_loss = 3.12202\n",
      "epoch no.3 train no.221520  loss = 4.67920 avg_loss = 3.15058\n",
      "epoch no.3 train no.221530  loss = 2.84796 avg_loss = 3.15035\n",
      "epoch no.3 train no.221540  loss = 2.56079 avg_loss = 3.16534\n",
      "epoch no.3 train no.221550  loss = 2.18735 avg_loss = 3.15198\n",
      "epoch no.3 train no.221560  loss = 3.47332 avg_loss = 3.18999\n",
      "epoch no.3 train no.221570  loss = 3.64445 avg_loss = 3.25294\n",
      "epoch no.3 train no.221580  loss = 2.22851 avg_loss = 3.20696\n",
      "epoch no.3 train no.221590  loss = 2.97622 avg_loss = 3.20138\n",
      "epoch no.3 train no.221600  loss = 3.95588 avg_loss = 3.16384\n",
      "epoch no.3 train no.221610  loss = 2.81320 avg_loss = 3.17758\n",
      "epoch no.3 train no.221620  loss = 4.29460 avg_loss = 3.18546\n",
      "epoch no.3 train no.221630  loss = 2.43858 avg_loss = 3.20342\n",
      "epoch no.3 train no.221640  loss = 2.51041 avg_loss = 3.16645\n",
      "epoch no.3 train no.221650  loss = 4.00289 avg_loss = 3.20811\n",
      "epoch no.3 train no.221660  loss = 3.01550 avg_loss = 3.21101\n",
      "epoch no.3 train no.221670  loss = 4.18456 avg_loss = 3.24032\n",
      "epoch no.3 train no.221680  loss = 3.41890 avg_loss = 3.20309\n",
      "epoch no.3 train no.221690  loss = 3.17993 avg_loss = 3.21829\n",
      "epoch no.3 train no.221700  loss = 4.96731 avg_loss = 3.25551\n",
      "epoch no.3 train no.221710  loss = 2.13256 avg_loss = 3.24729\n",
      "epoch no.3 train no.221720  loss = 2.85517 avg_loss = 3.22051\n",
      "epoch no.3 train no.221730  loss = 1.72861 avg_loss = 3.22872\n",
      "epoch no.3 train no.221740  loss = 2.99881 avg_loss = 3.26452\n",
      "epoch no.3 train no.221750  loss = 2.27907 avg_loss = 3.28371\n",
      "epoch no.3 train no.221760  loss = 5.28403 avg_loss = 3.27713\n",
      "epoch no.3 train no.221770  loss = 3.63290 avg_loss = 3.27352\n",
      "epoch no.3 train no.221780  loss = 3.15786 avg_loss = 3.29923\n",
      "epoch no.3 train no.221790  loss = 3.21585 avg_loss = 3.24698\n",
      "epoch no.3 train no.221800  loss = 3.68440 avg_loss = 3.26681\n",
      "epoch no.3 train no.221810  loss = 2.15553 avg_loss = 3.22044\n",
      "epoch no.3 train no.221820  loss = 5.13147 avg_loss = 3.23808\n",
      "epoch no.3 train no.221830  loss = 5.68789 avg_loss = 3.28307\n",
      "epoch no.3 train no.221840  loss = 2.79989 avg_loss = 3.28463\n",
      "epoch no.3 train no.221850  loss = 2.11928 avg_loss = 3.22628\n",
      "epoch no.3 train no.221860  loss = 3.17416 avg_loss = 3.20943\n",
      "epoch no.3 train no.221870  loss = 2.24938 avg_loss = 3.16584\n",
      "epoch no.3 train no.221880  loss = 3.85720 avg_loss = 3.19007\n",
      "epoch no.3 train no.221890  loss = 3.78702 avg_loss = 3.18335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.221900  loss = 2.86089 avg_loss = 3.21976\n",
      "epoch no.3 train no.221910  loss = 3.28676 avg_loss = 3.20176\n",
      "epoch no.3 train no.221920  loss = 2.36513 avg_loss = 3.18872\n",
      "epoch no.3 train no.221930  loss = 4.61797 avg_loss = 3.22188\n",
      "epoch no.3 train no.221940  loss = 3.62147 avg_loss = 3.22517\n",
      "epoch no.3 train no.221950  loss = 3.43773 avg_loss = 3.19696\n",
      "epoch no.3 train no.221960  loss = 2.18030 avg_loss = 3.18848\n",
      "epoch no.3 train no.221970  loss = 2.99006 avg_loss = 3.16598\n",
      "epoch no.3 train no.221980  loss = 4.93807 avg_loss = 3.14241\n",
      "epoch no.3 train no.221990  loss = 2.28185 avg_loss = 3.11223\n",
      "epoch no.3 train no.222000  loss = 4.52447 avg_loss = 3.15333\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 재즈</s>\n",
      "epoch no.3 train no.222010  loss = 1.64437 avg_loss = 3.09963\n",
      "epoch no.3 train no.222020  loss = 4.75497 avg_loss = 3.15335\n",
      "epoch no.3 train no.222030  loss = 3.41673 avg_loss = 3.13479\n",
      "epoch no.3 train no.222040  loss = 3.23650 avg_loss = 3.15440\n",
      "epoch no.3 train no.222050  loss = 6.16545 avg_loss = 3.20511\n",
      "epoch no.3 train no.222060  loss = 2.92066 avg_loss = 3.23202\n",
      "epoch no.3 train no.222070  loss = 2.88816 avg_loss = 3.22647\n",
      "epoch no.3 train no.222080  loss = 2.91557 avg_loss = 3.23291\n",
      "epoch no.3 train no.222090  loss = 4.80847 avg_loss = 3.22930\n",
      "epoch no.3 train no.222100  loss = 1.75177 avg_loss = 3.21086\n",
      "epoch no.3 train no.222110  loss = 2.59678 avg_loss = 3.18516\n",
      "epoch no.3 train no.222120  loss = 4.25037 avg_loss = 3.21817\n",
      "epoch no.3 train no.222130  loss = 1.73991 avg_loss = 3.18620\n",
      "epoch no.3 train no.222140  loss = 3.18296 avg_loss = 3.18030\n",
      "epoch no.3 train no.222150  loss = 1.76703 avg_loss = 3.15088\n",
      "epoch no.3 train no.222160  loss = 2.25587 avg_loss = 3.13507\n",
      "epoch no.3 train no.222170  loss = 3.34514 avg_loss = 3.13389\n",
      "epoch no.3 train no.222180  loss = 6.62467 avg_loss = 3.20018\n",
      "epoch no.3 train no.222190  loss = 4.31029 avg_loss = 3.18489\n",
      "epoch no.3 train no.222200  loss = 3.91816 avg_loss = 3.22272\n",
      "epoch no.3 train no.222210  loss = 2.53322 avg_loss = 3.21671\n",
      "epoch no.3 train no.222220  loss = 2.78411 avg_loss = 3.23598\n",
      "epoch no.3 train no.222230  loss = 5.47295 avg_loss = 3.25819\n",
      "epoch no.3 train no.222240  loss = 2.65373 avg_loss = 3.25274\n",
      "epoch no.3 train no.222250  loss = 2.43932 avg_loss = 3.22796\n",
      "epoch no.3 train no.222260  loss = 2.35657 avg_loss = 3.24449\n",
      "epoch no.3 train no.222270  loss = 3.01901 avg_loss = 3.23892\n",
      "epoch no.3 train no.222280  loss = 2.85191 avg_loss = 3.25950\n",
      "epoch no.3 train no.222290  loss = 2.96820 avg_loss = 3.30741\n",
      "epoch no.3 train no.222300  loss = 2.67321 avg_loss = 3.28326\n",
      "epoch no.3 train no.222310  loss = 2.71633 avg_loss = 3.24256\n",
      "epoch no.3 train no.222320  loss = 2.57032 avg_loss = 3.21326\n",
      "epoch no.3 train no.222330  loss = 3.36350 avg_loss = 3.19531\n",
      "epoch no.3 train no.222340  loss = 3.33373 avg_loss = 3.23593\n",
      "epoch no.3 train no.222350  loss = 4.48206 avg_loss = 3.21313\n",
      "epoch no.3 train no.222360  loss = 2.28325 avg_loss = 3.23136\n",
      "epoch no.3 train no.222370  loss = 3.54679 avg_loss = 3.25185\n",
      "epoch no.3 train no.222380  loss = 2.93968 avg_loss = 3.22867\n",
      "epoch no.3 train no.222390  loss = 3.92406 avg_loss = 3.22203\n",
      "epoch no.3 train no.222400  loss = 3.85176 avg_loss = 3.25703\n",
      "epoch no.3 train no.222410  loss = 2.80292 avg_loss = 3.28688\n",
      "epoch no.3 train no.222420  loss = 4.50041 avg_loss = 3.31383\n",
      "epoch no.3 train no.222430  loss = 3.51815 avg_loss = 3.29096\n",
      "epoch no.3 train no.222440  loss = 2.23226 avg_loss = 3.30272\n",
      "epoch no.3 train no.222450  loss = 1.84454 avg_loss = 3.29275\n",
      "epoch no.3 train no.222460  loss = 2.13967 avg_loss = 3.28669\n",
      "epoch no.3 train no.222470  loss = 3.79713 avg_loss = 3.25576\n",
      "epoch no.3 train no.222480  loss = 4.30963 avg_loss = 3.26053\n",
      "epoch no.3 train no.222490  loss = 2.09693 avg_loss = 3.22673\n",
      "epoch no.3 train no.222500  loss = 2.47353 avg_loss = 3.20662\n",
      "epoch no.3 train no.222510  loss = 3.31320 avg_loss = 3.23167\n",
      "epoch no.3 train no.222520  loss = 4.15025 avg_loss = 3.25169\n",
      "epoch no.3 train no.222530  loss = 3.11566 avg_loss = 3.24814\n",
      "epoch no.3 train no.222540  loss = 2.55158 avg_loss = 3.22295\n",
      "epoch no.3 train no.222550  loss = 2.24484 avg_loss = 3.21467\n",
      "epoch no.3 train no.222560  loss = 2.20848 avg_loss = 3.21235\n",
      "epoch no.3 train no.222570  loss = 3.40806 avg_loss = 3.19191\n",
      "epoch no.3 train no.222580  loss = 2.19208 avg_loss = 3.19631\n",
      "epoch no.3 train no.222590  loss = 3.15039 avg_loss = 3.20965\n",
      "epoch no.3 train no.222600  loss = 4.74174 avg_loss = 3.20089\n",
      "epoch no.3 train no.222610  loss = 5.41636 avg_loss = 3.21865\n",
      "epoch no.3 train no.222620  loss = 2.94931 avg_loss = 3.23175\n",
      "epoch no.3 train no.222630  loss = 2.51626 avg_loss = 3.18855\n",
      "epoch no.3 train no.222640  loss = 4.10731 avg_loss = 3.20800\n",
      "epoch no.3 train no.222650  loss = 2.15862 avg_loss = 3.15827\n",
      "epoch no.3 train no.222660  loss = 2.32222 avg_loss = 3.12655\n",
      "epoch no.3 train no.222670  loss = 3.12312 avg_loss = 3.11606\n",
      "epoch no.3 train no.222680  loss = 2.60546 avg_loss = 3.16425\n",
      "epoch no.3 train no.222690  loss = 1.73132 avg_loss = 3.16233\n",
      "epoch no.3 train no.222700  loss = 2.18527 avg_loss = 3.21886\n",
      "epoch no.3 train no.222710  loss = 3.67437 avg_loss = 3.19753\n",
      "epoch no.3 train no.222720  loss = 2.21555 avg_loss = 3.24271\n",
      "epoch no.3 train no.222730  loss = 2.36500 avg_loss = 3.21214\n",
      "epoch no.3 train no.222740  loss = 4.32517 avg_loss = 3.21678\n",
      "epoch no.3 train no.222750  loss = 2.79425 avg_loss = 3.20158\n",
      "epoch no.3 train no.222760  loss = 2.18077 avg_loss = 3.20567\n",
      "epoch no.3 train no.222770  loss = 4.13858 avg_loss = 3.21005\n",
      "epoch no.3 train no.222780  loss = 3.63409 avg_loss = 3.22361\n",
      "epoch no.3 train no.222790  loss = 3.84154 avg_loss = 3.20439\n",
      "epoch no.3 train no.222800  loss = 2.51540 avg_loss = 3.24030\n",
      "epoch no.3 train no.222810  loss = 4.38685 avg_loss = 3.20393\n",
      "epoch no.3 train no.222820  loss = 3.22125 avg_loss = 3.23646\n",
      "epoch no.3 train no.222830  loss = 3.46438 avg_loss = 3.20967\n",
      "epoch no.3 train no.222840  loss = 4.19864 avg_loss = 3.26378\n",
      "epoch no.3 train no.222850  loss = 4.07095 avg_loss = 3.28255\n",
      "epoch no.3 train no.222860  loss = 2.73902 avg_loss = 3.27604\n",
      "epoch no.3 train no.222870  loss = 3.56423 avg_loss = 3.25849\n",
      "epoch no.3 train no.222880  loss = 3.08248 avg_loss = 3.21976\n",
      "epoch no.3 train no.222890  loss = 3.44179 avg_loss = 3.21613\n",
      "epoch no.3 train no.222900  loss = 4.00107 avg_loss = 3.18300\n",
      "epoch no.3 train no.222910  loss = 5.05866 avg_loss = 3.21490\n",
      "epoch no.3 train no.222920  loss = 3.23477 avg_loss = 3.20965\n",
      "epoch no.3 train no.222930  loss = 2.30513 avg_loss = 3.19098\n",
      "epoch no.3 train no.222940  loss = 1.96421 avg_loss = 3.17272\n",
      "epoch no.3 train no.222950  loss = 2.73310 avg_loss = 3.14818\n",
      "epoch no.3 train no.222960  loss = 3.08798 avg_loss = 3.16964\n",
      "epoch no.3 train no.222970  loss = 3.34442 avg_loss = 3.16987\n",
      "epoch no.3 train no.222980  loss = 2.36778 avg_loss = 3.20358\n",
      "epoch no.3 train no.222990  loss = 3.74765 avg_loss = 3.22596\n",
      "epoch no.3 train no.223000  loss = 4.58589 avg_loss = 3.25806\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁함께', '놓을', '▁감성', '▁음악', '음악']\n",
      "여름밤을 수놓을 감각적인 인디</s>\n",
      "epoch no.3 train no.223010  loss = 2.03686 avg_loss = 3.23689\n",
      "epoch no.3 train no.223020  loss = 3.87911 avg_loss = 3.20167\n",
      "epoch no.3 train no.223030  loss = 3.55192 avg_loss = 3.24899\n",
      "epoch no.3 train no.223040  loss = 5.03213 avg_loss = 3.25596\n",
      "epoch no.3 train no.223050  loss = 2.52366 avg_loss = 3.24596\n",
      "epoch no.3 train no.223060  loss = 6.66335 avg_loss = 3.26966\n",
      "epoch no.3 train no.223070  loss = 2.83143 avg_loss = 3.24168\n",
      "epoch no.3 train no.223080  loss = 2.75589 avg_loss = 3.24493\n",
      "epoch no.3 train no.223090  loss = 2.76497 avg_loss = 3.23909\n",
      "epoch no.3 train no.223100  loss = 2.67903 avg_loss = 3.23855\n",
      "epoch no.3 train no.223110  loss = 2.48442 avg_loss = 3.21108\n",
      "epoch no.3 train no.223120  loss = 4.10978 avg_loss = 3.25407\n",
      "epoch no.3 train no.223130  loss = 2.71339 avg_loss = 3.23484\n",
      "epoch no.3 train no.223140  loss = 3.94992 avg_loss = 3.26763\n",
      "epoch no.3 train no.223150  loss = 3.26776 avg_loss = 3.28955\n",
      "epoch no.3 train no.223160  loss = 3.70997 avg_loss = 3.29965\n",
      "epoch no.3 train no.223170  loss = 2.64076 avg_loss = 3.27817\n",
      "epoch no.3 train no.223180  loss = 3.04972 avg_loss = 3.30032\n",
      "epoch no.3 train no.223190  loss = 5.19244 avg_loss = 3.32741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.223200  loss = 3.30824 avg_loss = 3.31170\n",
      "epoch no.3 train no.223210  loss = 2.69971 avg_loss = 3.26574\n",
      "epoch no.3 train no.223220  loss = 3.43401 avg_loss = 3.25572\n",
      "epoch no.3 train no.223230  loss = 4.21426 avg_loss = 3.24552\n",
      "epoch no.3 train no.223240  loss = 2.67953 avg_loss = 3.23531\n",
      "epoch no.3 train no.223250  loss = 4.30030 avg_loss = 3.28161\n",
      "epoch no.3 train no.223260  loss = 3.23609 avg_loss = 3.25313\n",
      "epoch no.3 train no.223270  loss = 4.24224 avg_loss = 3.26516\n",
      "epoch no.3 train no.223280  loss = 3.84205 avg_loss = 3.23458\n",
      "epoch no.3 train no.223290  loss = 3.22765 avg_loss = 3.25691\n",
      "epoch no.3 train no.223300  loss = 2.94814 avg_loss = 3.24040\n",
      "epoch no.3 train no.223310  loss = 3.52264 avg_loss = 3.25128\n",
      "epoch no.3 train no.223320  loss = 3.43673 avg_loss = 3.26162\n",
      "epoch no.3 train no.223330  loss = 2.27829 avg_loss = 3.19723\n",
      "epoch no.3 train no.223340  loss = 2.49971 avg_loss = 3.21311\n",
      "epoch no.3 train no.223350  loss = 2.25109 avg_loss = 3.16055\n",
      "epoch no.3 train no.223360  loss = 3.15461 avg_loss = 3.13330\n",
      "epoch no.3 train no.223370  loss = 3.73243 avg_loss = 3.14487\n",
      "epoch no.3 train no.223380  loss = 2.81785 avg_loss = 3.12116\n",
      "epoch no.3 train no.223390  loss = 4.41818 avg_loss = 3.11110\n",
      "epoch no.3 train no.223400  loss = 2.72735 avg_loss = 3.17525\n",
      "epoch no.3 train no.223410  loss = 2.29155 avg_loss = 3.18437\n",
      "epoch no.3 train no.223420  loss = 3.86862 avg_loss = 3.16578\n",
      "epoch no.3 train no.223430  loss = 3.64263 avg_loss = 3.17425\n",
      "epoch no.3 train no.223440  loss = 4.29154 avg_loss = 3.15671\n",
      "epoch no.3 train no.223450  loss = 4.00180 avg_loss = 3.17177\n",
      "epoch no.3 train no.223460  loss = 2.83677 avg_loss = 3.15346\n",
      "epoch no.3 train no.223470  loss = 2.66313 avg_loss = 3.15507\n",
      "epoch no.3 train no.223480  loss = 2.19416 avg_loss = 3.15350\n",
      "epoch no.3 train no.223490  loss = 3.70624 avg_loss = 3.15801\n",
      "epoch no.3 train no.223500  loss = 3.32293 avg_loss = 3.17298\n",
      "epoch no.3 train no.223510  loss = 2.54302 avg_loss = 3.18725\n",
      "epoch no.3 train no.223520  loss = 1.46094 avg_loss = 3.17200\n",
      "epoch no.3 train no.223530  loss = 4.65187 avg_loss = 3.17251\n",
      "epoch no.3 train no.223540  loss = 2.79769 avg_loss = 3.16021\n",
      "epoch no.3 train no.223550  loss = 1.83567 avg_loss = 3.13893\n",
      "epoch no.3 train no.223560  loss = 3.21059 avg_loss = 3.15629\n",
      "epoch no.3 train no.223570  loss = 2.32998 avg_loss = 3.14434\n",
      "epoch no.3 train no.223580  loss = 3.56409 avg_loss = 3.17455\n",
      "epoch no.3 train no.223590  loss = 2.06850 avg_loss = 3.12889\n",
      "epoch no.3 train no.223600  loss = 2.71591 avg_loss = 3.14854\n",
      "epoch no.3 train no.223610  loss = 2.51161 avg_loss = 3.14187\n",
      "epoch no.3 train no.223620  loss = 2.29534 avg_loss = 3.13977\n",
      "epoch no.3 train no.223630  loss = 4.04869 avg_loss = 3.15024\n",
      "epoch no.3 train no.223640  loss = 1.53308 avg_loss = 3.10393\n",
      "epoch no.3 train no.223650  loss = 4.93301 avg_loss = 3.10809\n",
      "epoch no.3 train no.223660  loss = 1.41394 avg_loss = 3.11146\n",
      "epoch no.3 train no.223670  loss = 2.08033 avg_loss = 3.10776\n",
      "epoch no.3 train no.223680  loss = 2.69425 avg_loss = 3.08926\n",
      "epoch no.3 train no.223690  loss = 3.43738 avg_loss = 3.12875\n",
      "epoch no.3 train no.223700  loss = 1.42589 avg_loss = 3.15829\n",
      "epoch no.3 train no.223710  loss = 4.02696 avg_loss = 3.13503\n",
      "epoch no.3 train no.223720  loss = 1.59294 avg_loss = 3.13069\n",
      "epoch no.3 train no.223730  loss = 4.10171 avg_loss = 3.18485\n",
      "epoch no.3 train no.223740  loss = 5.02229 avg_loss = 3.23438\n",
      "epoch no.3 train no.223750  loss = 4.28046 avg_loss = 3.23836\n",
      "epoch no.3 train no.223760  loss = 2.19623 avg_loss = 3.20289\n",
      "epoch no.3 train no.223770  loss = 4.98622 avg_loss = 3.23325\n",
      "epoch no.3 train no.223780  loss = 4.58005 avg_loss = 3.26357\n",
      "epoch no.3 train no.223790  loss = 2.77531 avg_loss = 3.26796\n",
      "epoch no.3 train no.223800  loss = 4.55893 avg_loss = 3.32038\n",
      "epoch no.3 train no.223810  loss = 3.39230 avg_loss = 3.29774\n",
      "epoch no.3 train no.223820  loss = 7.77697 avg_loss = 3.33719\n",
      "epoch no.3 train no.223830  loss = 2.58164 avg_loss = 3.30619\n",
      "epoch no.3 train no.223840  loss = 3.63387 avg_loss = 3.29293\n",
      "epoch no.3 train no.223850  loss = 3.95703 avg_loss = 3.26719\n",
      "epoch no.3 train no.223860  loss = 2.91308 avg_loss = 3.22088\n",
      "epoch no.3 train no.223870  loss = 2.49501 avg_loss = 3.19023\n",
      "epoch no.3 train no.223880  loss = 2.52324 avg_loss = 3.16756\n",
      "epoch no.3 train no.223890  loss = 2.70474 avg_loss = 3.17476\n",
      "epoch no.3 train no.223900  loss = 2.69958 avg_loss = 3.16795\n",
      "epoch no.3 train no.223910  loss = 3.27243 avg_loss = 3.21795\n",
      "epoch no.3 train no.223920  loss = 2.56167 avg_loss = 3.19594\n",
      "epoch no.3 train no.223930  loss = 2.67883 avg_loss = 3.15058\n",
      "epoch no.3 train no.223940  loss = 4.71470 avg_loss = 3.19844\n",
      "epoch no.3 train no.223950  loss = 3.02909 avg_loss = 3.27614\n",
      "epoch no.3 train no.223960  loss = 3.44614 avg_loss = 3.31438\n",
      "epoch no.3 train no.223970  loss = 2.38067 avg_loss = 3.30053\n",
      "epoch no.3 train no.223980  loss = 2.57421 avg_loss = 3.23368\n",
      "epoch no.3 train no.223990  loss = 5.29879 avg_loss = 3.26867\n",
      "epoch no.3 train no.224000  loss = 3.02332 avg_loss = 3.27932\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁감성', '속으로', '▁되는', '나는', '▁노래', '</s>']\n",
      "여름밤의 추억이 생각나는 노래</s>\n",
      "epoch no.3 train no.224010  loss = 5.09711 avg_loss = 3.27551\n",
      "epoch no.3 train no.224020  loss = 2.46461 avg_loss = 3.27638\n",
      "epoch no.3 train no.224030  loss = 3.17836 avg_loss = 3.26545\n",
      "epoch no.3 train no.224040  loss = 2.02252 avg_loss = 3.22726\n",
      "epoch no.3 train no.224050  loss = 2.35632 avg_loss = 3.20300\n",
      "epoch no.3 train no.224060  loss = 3.31814 avg_loss = 3.21671\n",
      "epoch no.3 train no.224070  loss = 3.95789 avg_loss = 3.18522\n",
      "epoch no.3 train no.224080  loss = 4.67128 avg_loss = 3.19887\n",
      "epoch no.3 train no.224090  loss = 3.11410 avg_loss = 3.24611\n",
      "epoch no.3 train no.224100  loss = 3.82758 avg_loss = 3.26501\n",
      "epoch no.3 train no.224110  loss = 2.59252 avg_loss = 3.24152\n",
      "epoch no.3 train no.224120  loss = 2.19675 avg_loss = 3.23390\n",
      "epoch no.3 train no.224130  loss = 3.80493 avg_loss = 3.23086\n",
      "epoch no.3 train no.224140  loss = 4.45821 avg_loss = 3.24020\n",
      "epoch no.3 train no.224150  loss = 3.77293 avg_loss = 3.22653\n",
      "epoch no.3 train no.224160  loss = 5.30523 avg_loss = 3.26537\n",
      "epoch no.3 train no.224170  loss = 4.73665 avg_loss = 3.25650\n",
      "epoch no.3 train no.224180  loss = 2.66441 avg_loss = 3.24873\n",
      "epoch no.3 train no.224190  loss = 3.81285 avg_loss = 3.24658\n",
      "epoch no.3 train no.224200  loss = 2.88187 avg_loss = 3.24814\n",
      "epoch no.3 train no.224210  loss = 3.36588 avg_loss = 3.21139\n",
      "epoch no.3 train no.224220  loss = 4.54785 avg_loss = 3.19971\n",
      "epoch no.3 train no.224230  loss = 3.10078 avg_loss = 3.20896\n",
      "epoch no.3 train no.224240  loss = 1.23458 avg_loss = 3.17504\n",
      "epoch no.3 train no.224250  loss = 4.31954 avg_loss = 3.18071\n",
      "epoch no.3 train no.224260  loss = 3.48947 avg_loss = 3.22197\n",
      "epoch no.3 train no.224270  loss = 6.10668 avg_loss = 3.27389\n",
      "epoch no.3 train no.224280  loss = 2.85222 avg_loss = 3.29995\n",
      "epoch no.3 train no.224290  loss = 3.17319 avg_loss = 3.24736\n",
      "epoch no.3 train no.224300  loss = 2.50935 avg_loss = 3.27935\n",
      "epoch no.3 train no.224310  loss = 3.29580 avg_loss = 3.28979\n",
      "epoch no.3 train no.224320  loss = 3.44487 avg_loss = 3.27799\n",
      "epoch no.3 train no.224330  loss = 3.66548 avg_loss = 3.28435\n",
      "epoch no.3 train no.224340  loss = 2.93343 avg_loss = 3.30287\n",
      "epoch no.3 train no.224350  loss = 3.37601 avg_loss = 3.27549\n",
      "epoch no.3 train no.224360  loss = 2.30947 avg_loss = 3.21411\n",
      "epoch no.3 train no.224370  loss = 2.14259 avg_loss = 3.20874\n",
      "epoch no.3 train no.224380  loss = 4.95735 avg_loss = 3.24035\n",
      "epoch no.3 train no.224390  loss = 2.74111 avg_loss = 3.24982\n",
      "epoch no.3 train no.224400  loss = 2.12215 avg_loss = 3.28033\n",
      "epoch no.3 train no.224410  loss = 3.75967 avg_loss = 3.28613\n",
      "epoch no.3 train no.224420  loss = 3.76043 avg_loss = 3.31184\n",
      "epoch no.3 train no.224430  loss = 2.44126 avg_loss = 3.28390\n",
      "epoch no.3 train no.224440  loss = 3.52046 avg_loss = 3.33164\n",
      "epoch no.3 train no.224450  loss = 2.55692 avg_loss = 3.33628\n",
      "epoch no.3 train no.224460  loss = 2.79903 avg_loss = 3.33591\n",
      "epoch no.3 train no.224470  loss = 2.82525 avg_loss = 3.34810\n",
      "epoch no.3 train no.224480  loss = 4.16548 avg_loss = 3.33369\n",
      "epoch no.3 train no.224490  loss = 3.42909 avg_loss = 3.31072\n",
      "epoch no.3 train no.224500  loss = 2.23546 avg_loss = 3.32704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.224510  loss = 2.64861 avg_loss = 3.34432\n",
      "epoch no.3 train no.224520  loss = 2.82186 avg_loss = 3.34377\n",
      "epoch no.3 train no.224530  loss = 3.81981 avg_loss = 3.32962\n",
      "epoch no.3 train no.224540  loss = 4.06005 avg_loss = 3.34355\n",
      "epoch no.3 train no.224550  loss = 3.20936 avg_loss = 3.32704\n",
      "epoch no.3 train no.224560  loss = 2.09196 avg_loss = 3.31037\n",
      "epoch no.3 train no.224570  loss = 2.38247 avg_loss = 3.31665\n",
      "epoch no.3 train no.224580  loss = 4.11436 avg_loss = 3.31503\n",
      "epoch no.3 train no.224590  loss = 3.26233 avg_loss = 3.32094\n",
      "epoch no.3 train no.224600  loss = 3.64138 avg_loss = 3.27362\n",
      "epoch no.3 train no.224610  loss = 2.32030 avg_loss = 3.25838\n",
      "epoch no.3 train no.224620  loss = 2.49628 avg_loss = 3.20544\n",
      "epoch no.3 train no.224630  loss = 3.04784 avg_loss = 3.19553\n",
      "epoch no.3 train no.224640  loss = 5.25784 avg_loss = 3.21889\n",
      "epoch no.3 train no.224650  loss = 1.85061 avg_loss = 3.22945\n",
      "epoch no.3 train no.224660  loss = 2.42214 avg_loss = 3.19437\n",
      "epoch no.3 train no.224670  loss = 2.55735 avg_loss = 3.16800\n",
      "epoch no.3 train no.224680  loss = 2.90239 avg_loss = 3.09509\n",
      "epoch no.3 train no.224690  loss = 3.16932 avg_loss = 3.16685\n",
      "epoch no.3 train no.224700  loss = 2.23283 avg_loss = 3.14233\n",
      "epoch no.3 train no.224710  loss = 1.44746 avg_loss = 3.17764\n",
      "epoch no.3 train no.224720  loss = 4.10504 avg_loss = 3.20573\n",
      "epoch no.3 train no.224730  loss = 2.30247 avg_loss = 3.18810\n",
      "epoch no.3 train no.224740  loss = 3.64900 avg_loss = 3.13337\n",
      "epoch no.3 train no.224750  loss = 3.13080 avg_loss = 3.15056\n",
      "epoch no.3 train no.224760  loss = 2.89458 avg_loss = 3.17020\n",
      "epoch no.3 train no.224770  loss = 2.93157 avg_loss = 3.12629\n",
      "epoch no.3 train no.224780  loss = 2.98421 avg_loss = 3.14850\n",
      "epoch no.3 train no.224790  loss = 2.85658 avg_loss = 3.12118\n",
      "epoch no.3 train no.224800  loss = 3.38883 avg_loss = 3.09089\n",
      "epoch no.3 train no.224810  loss = 3.33684 avg_loss = 3.13695\n",
      "epoch no.3 train no.224820  loss = 2.51984 avg_loss = 3.14302\n",
      "epoch no.3 train no.224830  loss = 2.32558 avg_loss = 3.10961\n",
      "epoch no.3 train no.224840  loss = 3.88054 avg_loss = 3.13911\n",
      "epoch no.3 train no.224850  loss = 4.62315 avg_loss = 3.16111\n",
      "epoch no.3 train no.224860  loss = 3.03588 avg_loss = 3.17302\n",
      "epoch no.3 train no.224870  loss = 2.21006 avg_loss = 3.13680\n",
      "epoch no.3 train no.224880  loss = 3.54792 avg_loss = 3.17761\n",
      "epoch no.3 train no.224890  loss = 2.76693 avg_loss = 3.17742\n",
      "epoch no.3 train no.224900  loss = 3.05892 avg_loss = 3.22653\n",
      "epoch no.3 train no.224910  loss = 5.65895 avg_loss = 3.28841\n",
      "epoch no.3 train no.224920  loss = 2.62824 avg_loss = 3.28985\n",
      "epoch no.3 train no.224930  loss = 3.01325 avg_loss = 3.27153\n",
      "epoch no.3 train no.224940  loss = 2.04765 avg_loss = 3.26239\n",
      "epoch no.3 train no.224950  loss = 5.12208 avg_loss = 3.26198\n",
      "epoch no.3 train no.224960  loss = 2.32980 avg_loss = 3.22081\n",
      "epoch no.3 train no.224970  loss = 4.26213 avg_loss = 3.26504\n",
      "epoch no.3 train no.224980  loss = 2.26095 avg_loss = 3.25024\n",
      "epoch no.3 train no.224990  loss = 3.43599 avg_loss = 3.24108\n",
      "epoch no.3 train no.225000  loss = 4.34301 avg_loss = 3.25063\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁락', '한', '▁아이돌', '같은', '▁노래', '</s>', '</s>']\n",
      "여름엔 시원한 청량한 사이다같은 노래들</s>\n",
      "epoch no.3 train no.225010  loss = 1.82519 avg_loss = 3.23403\n",
      "epoch no.3 train no.225020  loss = 4.53622 avg_loss = 3.27166\n",
      "epoch no.3 train no.225030  loss = 2.16955 avg_loss = 3.27120\n",
      "epoch no.3 train no.225040  loss = 3.37726 avg_loss = 3.26302\n",
      "epoch no.3 train no.225050  loss = 3.51220 avg_loss = 3.26615\n",
      "epoch no.3 train no.225060  loss = 2.54135 avg_loss = 3.23489\n",
      "epoch no.3 train no.225070  loss = 2.74718 avg_loss = 3.20608\n",
      "epoch no.3 train no.225080  loss = 5.05152 avg_loss = 3.22320\n",
      "epoch no.3 train no.225090  loss = 2.89261 avg_loss = 3.22329\n",
      "epoch no.3 train no.225100  loss = 3.03168 avg_loss = 3.24207\n",
      "epoch no.3 train no.225110  loss = 3.33255 avg_loss = 3.20469\n",
      "epoch no.3 train no.225120  loss = 1.53946 avg_loss = 3.19394\n",
      "epoch no.3 train no.225130  loss = 3.23039 avg_loss = 3.20142\n",
      "epoch no.3 train no.225140  loss = 4.87673 avg_loss = 3.24894\n",
      "epoch no.3 train no.225150  loss = 2.79514 avg_loss = 3.19316\n",
      "epoch no.3 train no.225160  loss = 4.80388 avg_loss = 3.26095\n",
      "epoch no.3 train no.225170  loss = 4.36043 avg_loss = 3.27541\n",
      "epoch no.3 train no.225180  loss = 1.80503 avg_loss = 3.23329\n",
      "epoch no.3 train no.225190  loss = 2.94181 avg_loss = 3.22301\n",
      "epoch no.3 train no.225200  loss = 3.92598 avg_loss = 3.23660\n",
      "epoch no.3 train no.225210  loss = 2.69630 avg_loss = 3.19929\n",
      "epoch no.3 train no.225220  loss = 4.28571 avg_loss = 3.24094\n",
      "epoch no.3 train no.225230  loss = 2.40881 avg_loss = 3.27131\n",
      "epoch no.3 train no.225240  loss = 2.66923 avg_loss = 3.31100\n",
      "epoch no.3 train no.225250  loss = 3.35583 avg_loss = 3.26923\n",
      "epoch no.3 train no.225260  loss = 4.23470 avg_loss = 3.28183\n",
      "epoch no.3 train no.225270  loss = 1.94784 avg_loss = 3.26769\n",
      "epoch no.3 train no.225280  loss = 2.80958 avg_loss = 3.26222\n",
      "epoch no.3 train no.225290  loss = 2.52070 avg_loss = 3.20561\n",
      "epoch no.3 train no.225300  loss = 2.87799 avg_loss = 3.19219\n",
      "epoch no.3 train no.225310  loss = 5.67469 avg_loss = 3.16861\n",
      "epoch no.3 train no.225320  loss = 1.90368 avg_loss = 3.17570\n",
      "epoch no.3 train no.225330  loss = 2.41850 avg_loss = 3.20676\n",
      "epoch no.3 train no.225340  loss = 3.04461 avg_loss = 3.19076\n",
      "epoch no.3 train no.225350  loss = 4.85324 avg_loss = 3.21212\n",
      "epoch no.3 train no.225360  loss = 2.69978 avg_loss = 3.21814\n",
      "epoch no.3 train no.225370  loss = 3.29180 avg_loss = 3.22743\n",
      "epoch no.3 train no.225380  loss = 2.06610 avg_loss = 3.22248\n",
      "epoch no.3 train no.225390  loss = 3.67869 avg_loss = 3.19875\n",
      "epoch no.3 train no.225400  loss = 3.84184 avg_loss = 3.16552\n",
      "epoch no.3 train no.225410  loss = 1.15907 avg_loss = 3.12993\n",
      "epoch no.3 train no.225420  loss = 4.30696 avg_loss = 3.11254\n",
      "epoch no.3 train no.225430  loss = 3.06258 avg_loss = 3.13365\n",
      "epoch no.3 train no.225440  loss = 3.56622 avg_loss = 3.13858\n",
      "epoch no.3 train no.225450  loss = 3.11075 avg_loss = 3.13542\n",
      "epoch no.3 train no.225460  loss = 2.25040 avg_loss = 3.11887\n",
      "epoch no.3 train no.225470  loss = 2.93036 avg_loss = 3.10785\n",
      "epoch no.3 train no.225480  loss = 4.37206 avg_loss = 3.13050\n",
      "epoch no.3 train no.225490  loss = 1.70051 avg_loss = 3.17455\n",
      "epoch no.3 train no.225500  loss = 2.67760 avg_loss = 3.21301\n",
      "epoch no.3 train no.225510  loss = 3.40942 avg_loss = 3.21385\n",
      "epoch no.3 train no.225520  loss = 2.76253 avg_loss = 3.23447\n",
      "epoch no.3 train no.225530  loss = 4.25488 avg_loss = 3.24889\n",
      "epoch no.3 train no.225540  loss = 2.67285 avg_loss = 3.27686\n",
      "epoch no.3 train no.225550  loss = 2.71557 avg_loss = 3.22410\n",
      "epoch no.3 train no.225560  loss = 2.44153 avg_loss = 3.18248\n",
      "epoch no.3 train no.225570  loss = 3.20642 avg_loss = 3.16533\n",
      "epoch no.3 train no.225580  loss = 3.18032 avg_loss = 3.18004\n",
      "epoch no.3 train no.225590  loss = 3.16091 avg_loss = 3.12780\n",
      "epoch no.3 train no.225600  loss = 5.19777 avg_loss = 3.13224\n",
      "epoch no.3 train no.225610  loss = 3.88223 avg_loss = 3.19728\n",
      "epoch no.3 train no.225620  loss = 4.00945 avg_loss = 3.20941\n",
      "epoch no.3 train no.225630  loss = 3.07153 avg_loss = 3.16871\n",
      "epoch no.3 train no.225640  loss = 3.26161 avg_loss = 3.17763\n",
      "epoch no.3 train no.225650  loss = 3.26121 avg_loss = 3.21505\n",
      "epoch no.3 train no.225660  loss = 3.46829 avg_loss = 3.21159\n",
      "epoch no.3 train no.225670  loss = 2.10368 avg_loss = 3.20844\n",
      "epoch no.3 train no.225680  loss = 3.07772 avg_loss = 3.20996\n",
      "epoch no.3 train no.225690  loss = 4.48016 avg_loss = 3.20231\n",
      "epoch no.3 train no.225700  loss = 2.82217 avg_loss = 3.12066\n",
      "epoch no.3 train no.225710  loss = 3.44328 avg_loss = 3.16281\n",
      "epoch no.3 train no.225720  loss = 2.57390 avg_loss = 3.13764\n",
      "epoch no.3 train no.225730  loss = 1.75225 avg_loss = 3.10161\n",
      "epoch no.3 train no.225740  loss = 2.64307 avg_loss = 3.12806\n",
      "epoch no.3 train no.225750  loss = 2.54481 avg_loss = 3.13705\n",
      "epoch no.3 train no.225760  loss = 3.62936 avg_loss = 3.16659\n",
      "epoch no.3 train no.225770  loss = 3.72047 avg_loss = 3.21566\n",
      "epoch no.3 train no.225780  loss = 5.38190 avg_loss = 3.26643\n",
      "epoch no.3 train no.225790  loss = 2.45499 avg_loss = 3.24484\n",
      "epoch no.3 train no.225800  loss = 3.52373 avg_loss = 3.28050\n",
      "epoch no.3 train no.225810  loss = 4.52566 avg_loss = 3.26200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.225820  loss = 3.48555 avg_loss = 3.25871\n",
      "epoch no.3 train no.225830  loss = 4.11226 avg_loss = 3.24136\n",
      "epoch no.3 train no.225840  loss = 2.99441 avg_loss = 3.23172\n",
      "epoch no.3 train no.225850  loss = 2.46938 avg_loss = 3.29638\n",
      "epoch no.3 train no.225860  loss = 3.84492 avg_loss = 3.31314\n",
      "epoch no.3 train no.225870  loss = 4.73898 avg_loss = 3.34150\n",
      "epoch no.3 train no.225880  loss = 2.44868 avg_loss = 3.31512\n",
      "epoch no.3 train no.225890  loss = 1.78481 avg_loss = 3.32684\n",
      "epoch no.3 train no.225900  loss = 1.94026 avg_loss = 3.27184\n",
      "epoch no.3 train no.225910  loss = 2.60050 avg_loss = 3.28142\n",
      "epoch no.3 train no.225920  loss = 2.85060 avg_loss = 3.24342\n",
      "epoch no.3 train no.225930  loss = 2.68884 avg_loss = 3.19490\n",
      "epoch no.3 train no.225940  loss = 3.15976 avg_loss = 3.19201\n",
      "epoch no.3 train no.225950  loss = 1.74999 avg_loss = 3.17212\n",
      "epoch no.3 train no.225960  loss = 2.81205 avg_loss = 3.17287\n",
      "epoch no.3 train no.225970  loss = 4.80889 avg_loss = 3.22447\n",
      "epoch no.3 train no.225980  loss = 2.17781 avg_loss = 3.20553\n",
      "epoch no.3 train no.225990  loss = 3.42555 avg_loss = 3.22212\n",
      "epoch no.3 train no.226000  loss = 2.94716 avg_loss = 3.25141\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁잔잔', '리스트', '</s>']\n",
      "여름밤의 플레이리스트</s>\n",
      "epoch no.3 train no.226010  loss = 2.79361 avg_loss = 3.24600\n",
      "epoch no.3 train no.226020  loss = 3.75460 avg_loss = 3.25798\n",
      "epoch no.3 train no.226030  loss = 3.02716 avg_loss = 3.20438\n",
      "epoch no.3 train no.226040  loss = 4.40299 avg_loss = 3.18928\n",
      "epoch no.3 train no.226050  loss = 3.66608 avg_loss = 3.22638\n",
      "epoch no.3 train no.226060  loss = 2.93676 avg_loss = 3.17074\n",
      "epoch no.3 train no.226070  loss = 2.30992 avg_loss = 3.14268\n",
      "epoch no.3 train no.226080  loss = 2.44669 avg_loss = 3.14936\n",
      "epoch no.3 train no.226090  loss = 1.59220 avg_loss = 3.11626\n",
      "epoch no.3 train no.226100  loss = 3.73706 avg_loss = 3.10937\n",
      "epoch no.3 train no.226110  loss = 1.98243 avg_loss = 3.09539\n",
      "epoch no.3 train no.226120  loss = 2.86769 avg_loss = 3.12483\n",
      "epoch no.3 train no.226130  loss = 2.27619 avg_loss = 3.09977\n",
      "epoch no.3 train no.226140  loss = 2.47661 avg_loss = 3.08136\n",
      "epoch no.3 train no.226150  loss = 2.03175 avg_loss = 3.08316\n",
      "epoch no.3 train no.226160  loss = 2.61930 avg_loss = 3.09141\n",
      "epoch no.3 train no.226170  loss = 5.55559 avg_loss = 3.13316\n",
      "epoch no.3 train no.226180  loss = 3.44852 avg_loss = 3.12213\n",
      "epoch no.3 train no.226190  loss = 1.94412 avg_loss = 3.14337\n",
      "epoch no.3 train no.226200  loss = 4.74634 avg_loss = 3.13226\n",
      "epoch no.3 train no.226210  loss = 2.48242 avg_loss = 3.14071\n",
      "epoch no.3 train no.226220  loss = 4.24867 avg_loss = 3.10307\n",
      "epoch no.3 train no.226230  loss = 2.85401 avg_loss = 3.11195\n",
      "epoch no.3 train no.226240  loss = 2.48648 avg_loss = 3.13432\n",
      "epoch no.3 train no.226250  loss = 2.60631 avg_loss = 3.13784\n",
      "epoch no.3 train no.226260  loss = 1.41078 avg_loss = 3.08845\n",
      "epoch no.3 train no.226270  loss = 2.69168 avg_loss = 3.10432\n",
      "epoch no.3 train no.226280  loss = 2.85463 avg_loss = 3.10645\n",
      "epoch no.3 train no.226290  loss = 3.66342 avg_loss = 3.15034\n",
      "epoch no.3 train no.226300  loss = 4.05962 avg_loss = 3.15869\n",
      "epoch no.3 train no.226310  loss = 3.11083 avg_loss = 3.12593\n",
      "epoch no.3 train no.226320  loss = 4.41175 avg_loss = 3.14313\n",
      "epoch no.3 train no.226330  loss = 3.33661 avg_loss = 3.11982\n",
      "epoch no.3 train no.226340  loss = 2.32583 avg_loss = 3.12252\n",
      "epoch no.3 train no.226350  loss = 2.66271 avg_loss = 3.18201\n",
      "epoch no.3 train no.226360  loss = 3.15218 avg_loss = 3.16609\n",
      "epoch no.3 train no.226370  loss = 4.30180 avg_loss = 3.19683\n",
      "epoch no.3 train no.226380  loss = 2.45858 avg_loss = 3.16572\n",
      "epoch no.3 train no.226390  loss = 4.53317 avg_loss = 3.17378\n",
      "epoch no.3 train no.226400  loss = 2.84518 avg_loss = 3.17994\n",
      "epoch no.3 train no.226410  loss = 3.83473 avg_loss = 3.21247\n",
      "epoch no.3 train no.226420  loss = 1.64508 avg_loss = 3.19048\n",
      "epoch no.3 train no.226430  loss = 3.28960 avg_loss = 3.19778\n",
      "epoch no.3 train no.226440  loss = 3.00769 avg_loss = 3.18532\n",
      "epoch no.3 train no.226450  loss = 4.15559 avg_loss = 3.24007\n",
      "epoch no.3 train no.226460  loss = 1.80631 avg_loss = 3.19865\n",
      "epoch no.3 train no.226470  loss = 2.86715 avg_loss = 3.15767\n",
      "epoch no.3 train no.226480  loss = 3.74619 avg_loss = 3.20442\n",
      "epoch no.3 train no.226490  loss = 4.39323 avg_loss = 3.21078\n",
      "epoch no.3 train no.226500  loss = 2.67707 avg_loss = 3.21343\n",
      "epoch no.3 train no.226510  loss = 4.48922 avg_loss = 3.23048\n",
      "epoch no.3 train no.226520  loss = 3.29864 avg_loss = 3.28284\n",
      "epoch no.3 train no.226530  loss = 3.10651 avg_loss = 3.31628\n",
      "epoch no.3 train no.226540  loss = 2.44526 avg_loss = 3.30215\n",
      "epoch no.3 train no.226550  loss = 2.95900 avg_loss = 3.28173\n",
      "epoch no.3 train no.226560  loss = 4.36721 avg_loss = 3.28459\n",
      "epoch no.3 train no.226570  loss = 3.62412 avg_loss = 3.23367\n",
      "epoch no.3 train no.226580  loss = 2.98362 avg_loss = 3.20509\n",
      "epoch no.3 train no.226590  loss = 2.84924 avg_loss = 3.21760\n",
      "epoch no.3 train no.226600  loss = 2.74084 avg_loss = 3.17644\n",
      "epoch no.3 train no.226610  loss = 3.50554 avg_loss = 3.12219\n",
      "epoch no.3 train no.226620  loss = 2.24575 avg_loss = 3.12292\n",
      "epoch no.3 train no.226630  loss = 4.98812 avg_loss = 3.12714\n",
      "epoch no.3 train no.226640  loss = 3.31603 avg_loss = 3.12992\n",
      "epoch no.3 train no.226650  loss = 3.53885 avg_loss = 3.16360\n",
      "epoch no.3 train no.226660  loss = 2.41937 avg_loss = 3.16476\n",
      "epoch no.3 train no.226670  loss = 2.82173 avg_loss = 3.16958\n",
      "epoch no.3 train no.226680  loss = 2.26892 avg_loss = 3.18839\n",
      "epoch no.3 train no.226690  loss = 2.33630 avg_loss = 3.17740\n",
      "epoch no.3 train no.226700  loss = 2.77631 avg_loss = 3.20032\n",
      "epoch no.3 train no.226710  loss = 3.33746 avg_loss = 3.24816\n",
      "epoch no.3 train no.226720  loss = 3.34233 avg_loss = 3.26281\n",
      "epoch no.3 train no.226730  loss = 2.80250 avg_loss = 3.29011\n",
      "epoch no.3 train no.226740  loss = 2.45562 avg_loss = 3.26582\n",
      "epoch no.3 train no.226750  loss = 2.15585 avg_loss = 3.24234\n",
      "epoch no.3 train no.226760  loss = 3.00152 avg_loss = 3.25664\n",
      "epoch no.3 train no.226770  loss = 3.17054 avg_loss = 3.31661\n",
      "epoch no.3 train no.226780  loss = 4.40337 avg_loss = 3.31207\n",
      "epoch no.3 train no.226790  loss = 2.43623 avg_loss = 3.32118\n",
      "epoch no.3 train no.226800  loss = 3.61336 avg_loss = 3.29052\n",
      "epoch no.3 train no.226810  loss = 2.47865 avg_loss = 3.29364\n",
      "epoch no.3 train no.226820  loss = 2.26731 avg_loss = 3.28803\n",
      "epoch no.3 train no.226830  loss = 3.53493 avg_loss = 3.27572\n",
      "epoch no.3 train no.226840  loss = 3.10379 avg_loss = 3.23078\n",
      "epoch no.3 train no.226850  loss = 2.91167 avg_loss = 3.22566\n",
      "epoch no.3 train no.226860  loss = 3.64031 avg_loss = 3.28143\n",
      "epoch no.3 train no.226870  loss = 2.78080 avg_loss = 3.26321\n",
      "epoch no.3 train no.226880  loss = 2.77474 avg_loss = 3.22803\n",
      "epoch no.3 train no.226890  loss = 2.89333 avg_loss = 3.24912\n",
      "epoch no.3 train no.226900  loss = 4.42127 avg_loss = 3.30894\n",
      "epoch no.3 train no.226910  loss = 1.14369 avg_loss = 3.28948\n",
      "epoch no.3 train no.226920  loss = 4.47668 avg_loss = 3.30488\n",
      "epoch no.3 train no.226930  loss = 3.62721 avg_loss = 3.29803\n",
      "epoch no.3 train no.226940  loss = 3.30346 avg_loss = 3.28248\n",
      "epoch no.3 train no.226950  loss = 3.50690 avg_loss = 3.26459\n",
      "epoch no.3 train no.226960  loss = 2.79527 avg_loss = 3.24884\n",
      "epoch no.3 train no.226970  loss = 3.10815 avg_loss = 3.25628\n",
      "epoch no.3 train no.226980  loss = 4.37805 avg_loss = 3.25489\n",
      "epoch no.3 train no.226990  loss = 4.45744 avg_loss = 3.31532\n",
      "epoch no.3 train no.227000  loss = 3.85421 avg_loss = 3.35384\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁잔잔', '에이지', '▁피아노']\n",
      "여름밤에 듣는 뉴에이지</s>\n",
      "epoch no.3 train no.227010  loss = 2.62430 avg_loss = 3.36127\n",
      "epoch no.3 train no.227020  loss = 2.83968 avg_loss = 3.39500\n",
      "epoch no.3 train no.227030  loss = 1.96942 avg_loss = 3.38291\n",
      "epoch no.3 train no.227040  loss = 2.49336 avg_loss = 3.35347\n",
      "epoch no.3 train no.227050  loss = 3.89655 avg_loss = 3.31869\n",
      "epoch no.3 train no.227060  loss = 3.40502 avg_loss = 3.33417\n",
      "epoch no.3 train no.227070  loss = 3.88276 avg_loss = 3.30262\n",
      "epoch no.3 train no.227080  loss = 2.55478 avg_loss = 3.28755\n",
      "epoch no.3 train no.227090  loss = 2.59428 avg_loss = 3.30981\n",
      "epoch no.3 train no.227100  loss = 2.54308 avg_loss = 3.28975\n",
      "epoch no.3 train no.227110  loss = 2.91565 avg_loss = 3.24588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.227120  loss = 2.98324 avg_loss = 3.26832\n",
      "epoch no.3 train no.227130  loss = 2.95026 avg_loss = 3.28986\n",
      "epoch no.3 train no.227140  loss = 2.82184 avg_loss = 3.27912\n",
      "epoch no.3 train no.227150  loss = 5.84711 avg_loss = 3.29019\n",
      "epoch no.3 train no.227160  loss = 2.96995 avg_loss = 3.29362\n",
      "epoch no.3 train no.227170  loss = 3.11574 avg_loss = 3.31503\n",
      "epoch no.3 train no.227180  loss = 3.99889 avg_loss = 3.34369\n",
      "epoch no.3 train no.227190  loss = 1.94500 avg_loss = 3.33626\n",
      "epoch no.3 train no.227200  loss = 3.26150 avg_loss = 3.32884\n",
      "epoch no.3 train no.227210  loss = 2.47931 avg_loss = 3.30168\n",
      "epoch no.3 train no.227220  loss = 2.50229 avg_loss = 3.33872\n",
      "epoch no.3 train no.227230  loss = 2.79856 avg_loss = 3.30796\n",
      "epoch no.3 train no.227240  loss = 3.39596 avg_loss = 3.35443\n",
      "epoch no.3 train no.227250  loss = 3.17098 avg_loss = 3.34556\n",
      "epoch no.3 train no.227260  loss = 5.06838 avg_loss = 3.33317\n",
      "epoch no.3 train no.227270  loss = 4.90108 avg_loss = 3.36397\n",
      "epoch no.3 train no.227280  loss = 3.70474 avg_loss = 3.37212\n",
      "epoch no.3 train no.227290  loss = 3.77895 avg_loss = 3.37089\n",
      "epoch no.3 train no.227300  loss = 2.22691 avg_loss = 3.32331\n",
      "epoch no.3 train no.227310  loss = 3.00577 avg_loss = 3.32474\n",
      "epoch no.3 train no.227320  loss = 3.82603 avg_loss = 3.35711\n",
      "epoch no.3 train no.227330  loss = 4.38270 avg_loss = 3.35571\n",
      "epoch no.3 train no.227340  loss = 3.58761 avg_loss = 3.34230\n",
      "epoch no.3 train no.227350  loss = 5.40413 avg_loss = 3.32036\n",
      "epoch no.3 train no.227360  loss = 1.93155 avg_loss = 3.33082\n",
      "epoch no.3 train no.227370  loss = 2.27114 avg_loss = 3.32234\n",
      "epoch no.3 train no.227380  loss = 3.92186 avg_loss = 3.29773\n",
      "epoch no.3 train no.227390  loss = 2.73090 avg_loss = 3.23920\n",
      "epoch no.3 train no.227400  loss = 4.77993 avg_loss = 3.27627\n",
      "epoch no.3 train no.227410  loss = 2.26953 avg_loss = 3.25863\n",
      "epoch no.3 train no.227420  loss = 2.01990 avg_loss = 3.24959\n",
      "epoch no.3 train no.227430  loss = 3.27046 avg_loss = 3.24312\n",
      "epoch no.3 train no.227440  loss = 3.37166 avg_loss = 3.25351\n",
      "epoch no.3 train no.227450  loss = 2.41494 avg_loss = 3.26817\n",
      "epoch no.3 train no.227460  loss = 2.54857 avg_loss = 3.27709\n",
      "epoch no.3 train no.227470  loss = 2.86036 avg_loss = 3.31143\n",
      "epoch no.3 train no.227480  loss = 2.64256 avg_loss = 3.30289\n",
      "epoch no.3 train no.227490  loss = 2.35657 avg_loss = 3.29130\n",
      "epoch no.3 train no.227500  loss = 3.08076 avg_loss = 3.26822\n",
      "epoch no.3 train no.227510  loss = 2.83089 avg_loss = 3.24518\n",
      "epoch no.3 train no.227520  loss = 3.67180 avg_loss = 3.24755\n",
      "epoch no.3 train no.227530  loss = 2.88487 avg_loss = 3.24516\n",
      "epoch no.3 train no.227540  loss = 3.27407 avg_loss = 3.30502\n",
      "epoch no.3 train no.227550  loss = 4.38292 avg_loss = 3.34944\n",
      "epoch no.3 train no.227560  loss = 4.37823 avg_loss = 3.37740\n",
      "epoch no.3 train no.227570  loss = 2.61483 avg_loss = 3.34391\n",
      "epoch no.3 train no.227580  loss = 2.49191 avg_loss = 3.33982\n",
      "epoch no.3 train no.227590  loss = 3.14633 avg_loss = 3.34824\n",
      "epoch no.3 train no.227600  loss = 3.54499 avg_loss = 3.33488\n",
      "epoch no.3 train no.227610  loss = 3.62058 avg_loss = 3.32862\n",
      "epoch no.3 train no.227620  loss = 1.60431 avg_loss = 3.29348\n",
      "epoch no.3 train no.227630  loss = 3.29218 avg_loss = 3.31772\n",
      "epoch no.3 train no.227640  loss = 1.84649 avg_loss = 3.32737\n",
      "epoch no.3 train no.227650  loss = 4.58805 avg_loss = 3.31801\n",
      "epoch no.3 train no.227660  loss = 4.41105 avg_loss = 3.31742\n",
      "epoch no.3 train no.227670  loss = 2.96136 avg_loss = 3.33787\n",
      "epoch no.3 train no.227680  loss = 4.58522 avg_loss = 3.33568\n",
      "epoch no.3 train no.227690  loss = 2.98314 avg_loss = 3.34673\n",
      "epoch no.3 train no.227700  loss = 4.28541 avg_loss = 3.36325\n",
      "epoch no.3 train no.227710  loss = 3.91464 avg_loss = 3.33419\n",
      "epoch no.3 train no.227720  loss = 2.80124 avg_loss = 3.37258\n",
      "epoch no.3 train no.227730  loss = 2.29683 avg_loss = 3.31499\n",
      "epoch no.3 train no.227740  loss = 5.21976 avg_loss = 3.33759\n",
      "epoch no.3 train no.227750  loss = 2.32472 avg_loss = 3.31471\n",
      "epoch no.3 train no.227760  loss = 4.64846 avg_loss = 3.31178\n",
      "epoch no.3 train no.227770  loss = 4.46446 avg_loss = 3.31564\n",
      "epoch no.3 train no.227780  loss = 3.96049 avg_loss = 3.33113\n",
      "epoch no.3 train no.227790  loss = 3.39937 avg_loss = 3.35689\n",
      "epoch no.3 train no.227800  loss = 2.86477 avg_loss = 3.36229\n",
      "epoch no.3 train no.227810  loss = 3.34697 avg_loss = 3.38867\n",
      "epoch no.3 train no.227820  loss = 2.01027 avg_loss = 3.41491\n",
      "epoch no.3 train no.227830  loss = 2.70096 avg_loss = 3.41222\n",
      "epoch no.3 train no.227840  loss = 2.12419 avg_loss = 3.36350\n",
      "epoch no.3 train no.227850  loss = 3.33447 avg_loss = 3.35698\n",
      "epoch no.3 train no.227860  loss = 3.13548 avg_loss = 3.33737\n",
      "epoch no.3 train no.227870  loss = 2.67655 avg_loss = 3.31807\n",
      "epoch no.3 train no.227880  loss = 2.85474 avg_loss = 3.27196\n",
      "epoch no.3 train no.227890  loss = 4.71169 avg_loss = 3.33651\n",
      "epoch no.3 train no.227900  loss = 2.56890 avg_loss = 3.30845\n",
      "epoch no.3 train no.227910  loss = 2.59323 avg_loss = 3.26818\n",
      "epoch no.3 train no.227920  loss = 2.41023 avg_loss = 3.32100\n",
      "epoch no.3 train no.227930  loss = 5.18652 avg_loss = 3.35104\n",
      "epoch no.3 train no.227940  loss = 4.09413 avg_loss = 3.33104\n",
      "epoch no.3 train no.227950  loss = 3.30970 avg_loss = 3.31864\n",
      "epoch no.3 train no.227960  loss = 2.32376 avg_loss = 3.28534\n",
      "epoch no.3 train no.227970  loss = 3.46716 avg_loss = 3.24237\n",
      "epoch no.3 train no.227980  loss = 3.18881 avg_loss = 3.25062\n",
      "epoch no.3 train no.227990  loss = 4.05183 avg_loss = 3.28856\n",
      "epoch no.3 train no.228000  loss = 3.59484 avg_loss = 3.27160\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁재즈', '이', '▁함께', '</s>', '▁음악', '</s>']\n",
      "여름밤의 추억과 함께 하는 노래</s>\n",
      "epoch no.3 train no.228010  loss = 4.39426 avg_loss = 3.31477\n",
      "epoch no.3 train no.228020  loss = 4.35211 avg_loss = 3.35333\n",
      "epoch no.3 train no.228030  loss = 3.84322 avg_loss = 3.35436\n",
      "epoch no.3 train no.228040  loss = 3.37938 avg_loss = 3.34889\n",
      "epoch no.3 train no.228050  loss = 4.38003 avg_loss = 3.30959\n",
      "epoch no.3 train no.228060  loss = 2.27554 avg_loss = 3.28586\n",
      "epoch no.3 train no.228070  loss = 4.64010 avg_loss = 3.27433\n",
      "epoch no.3 train no.228080  loss = 2.41717 avg_loss = 3.19164\n",
      "epoch no.3 train no.228090  loss = 2.90779 avg_loss = 3.22873\n",
      "epoch no.3 train no.228100  loss = 2.60729 avg_loss = 3.23483\n",
      "epoch no.3 train no.228110  loss = 3.22075 avg_loss = 3.29596\n",
      "epoch no.3 train no.228120  loss = 4.50705 avg_loss = 3.29706\n",
      "epoch no.3 train no.228130  loss = 2.36250 avg_loss = 3.28584\n",
      "epoch no.3 train no.228140  loss = 1.61893 avg_loss = 3.24947\n",
      "epoch no.3 train no.228150  loss = 3.10558 avg_loss = 3.24091\n",
      "epoch no.3 train no.228160  loss = 2.12582 avg_loss = 3.25081\n",
      "epoch no.3 train no.228170  loss = 2.48528 avg_loss = 3.21190\n",
      "epoch no.3 train no.228180  loss = 3.81701 avg_loss = 3.19061\n",
      "epoch no.3 train no.228190  loss = 4.66733 avg_loss = 3.22014\n",
      "epoch no.3 train no.228200  loss = 2.52185 avg_loss = 3.20243\n",
      "epoch no.3 train no.228210  loss = 3.15303 avg_loss = 3.20097\n",
      "epoch no.3 train no.228220  loss = 2.25024 avg_loss = 3.17849\n",
      "epoch no.3 train no.228230  loss = 4.02795 avg_loss = 3.23789\n",
      "epoch no.3 train no.228240  loss = 3.27564 avg_loss = 3.20771\n",
      "epoch no.3 train no.228250  loss = 1.97682 avg_loss = 3.17967\n",
      "epoch no.3 train no.228260  loss = 2.34231 avg_loss = 3.14914\n",
      "epoch no.3 train no.228270  loss = 3.71724 avg_loss = 3.13004\n",
      "epoch no.3 train no.228280  loss = 4.34222 avg_loss = 3.16653\n",
      "epoch no.3 train no.228290  loss = 2.30664 avg_loss = 3.15989\n",
      "epoch no.3 train no.228300  loss = 2.36327 avg_loss = 3.23645\n",
      "epoch no.3 train no.228310  loss = 4.36410 avg_loss = 3.26149\n",
      "epoch no.3 train no.228320  loss = 1.75235 avg_loss = 3.25570\n",
      "epoch no.3 train no.228330  loss = 2.64575 avg_loss = 3.24662\n",
      "epoch no.3 train no.228340  loss = 3.73436 avg_loss = 3.24040\n",
      "epoch no.3 train no.228350  loss = 3.21305 avg_loss = 3.22326\n",
      "epoch no.3 train no.228360  loss = 4.96185 avg_loss = 3.23081\n",
      "epoch no.3 train no.228370  loss = 1.92513 avg_loss = 3.23605\n",
      "epoch no.3 train no.228380  loss = 2.33511 avg_loss = 3.23291\n",
      "epoch no.3 train no.228390  loss = 3.89587 avg_loss = 3.20569\n",
      "epoch no.3 train no.228400  loss = 2.16625 avg_loss = 3.19962\n",
      "epoch no.3 train no.228410  loss = 2.80010 avg_loss = 3.22558\n",
      "epoch no.3 train no.228420  loss = 2.92537 avg_loss = 3.23320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.228430  loss = 4.43075 avg_loss = 3.28039\n",
      "epoch no.3 train no.228440  loss = 2.33477 avg_loss = 3.23500\n",
      "epoch no.3 train no.228450  loss = 4.07796 avg_loss = 3.23154\n",
      "epoch no.3 train no.228460  loss = 3.25494 avg_loss = 3.29801\n",
      "epoch no.3 train no.228470  loss = 4.16852 avg_loss = 3.29783\n",
      "epoch no.3 train no.228480  loss = 3.09609 avg_loss = 3.32531\n",
      "epoch no.3 train no.228490  loss = 2.15665 avg_loss = 3.28454\n",
      "epoch no.3 train no.228500  loss = 3.72551 avg_loss = 3.24018\n",
      "epoch no.3 train no.228510  loss = 3.98510 avg_loss = 3.26638\n",
      "epoch no.3 train no.228520  loss = 2.02268 avg_loss = 3.24856\n",
      "epoch no.3 train no.228530  loss = 2.17074 avg_loss = 3.23179\n",
      "epoch no.3 train no.228540  loss = 2.49156 avg_loss = 3.19952\n",
      "epoch no.3 train no.228550  loss = 2.87933 avg_loss = 3.25315\n",
      "epoch no.3 train no.228560  loss = 2.90146 avg_loss = 3.26053\n",
      "epoch no.3 train no.228570  loss = 3.43394 avg_loss = 3.28681\n",
      "epoch no.3 train no.228580  loss = 3.54618 avg_loss = 3.34051\n",
      "epoch no.3 train no.228590  loss = 3.50128 avg_loss = 3.34437\n",
      "epoch no.3 train no.228600  loss = 5.28293 avg_loss = 3.34504\n",
      "epoch no.3 train no.228610  loss = 2.47213 avg_loss = 3.34872\n",
      "epoch no.3 train no.228620  loss = 4.70567 avg_loss = 3.36649\n",
      "epoch no.3 train no.228630  loss = 3.34505 avg_loss = 3.35340\n",
      "epoch no.3 train no.228640  loss = 4.29985 avg_loss = 3.33833\n",
      "epoch no.3 train no.228650  loss = 5.37555 avg_loss = 3.40707\n",
      "epoch no.3 train no.228660  loss = 2.14344 avg_loss = 3.35668\n",
      "epoch no.3 train no.228670  loss = 3.24025 avg_loss = 3.34356\n",
      "epoch no.3 train no.228680  loss = 2.58261 avg_loss = 3.31196\n",
      "epoch no.3 train no.228690  loss = 1.62134 avg_loss = 3.30037\n",
      "epoch no.3 train no.228700  loss = 3.28209 avg_loss = 3.32472\n",
      "epoch no.3 train no.228710  loss = 2.23327 avg_loss = 3.27428\n",
      "epoch no.3 train no.228720  loss = 5.92997 avg_loss = 3.27615\n",
      "epoch no.3 train no.228730  loss = 3.10419 avg_loss = 3.24243\n",
      "epoch no.3 train no.228740  loss = 3.51963 avg_loss = 3.23784\n",
      "epoch no.3 train no.228750  loss = 4.00635 avg_loss = 3.28815\n",
      "epoch no.3 train no.228760  loss = 2.70316 avg_loss = 3.28163\n",
      "epoch no.3 train no.228770  loss = 3.11962 avg_loss = 3.25604\n",
      "epoch no.3 train no.228780  loss = 1.53552 avg_loss = 3.24047\n",
      "epoch no.3 train no.228790  loss = 3.27081 avg_loss = 3.23980\n",
      "epoch no.3 train no.228800  loss = 2.00486 avg_loss = 3.18810\n",
      "epoch no.3 train no.228810  loss = 2.30703 avg_loss = 3.17130\n",
      "epoch no.3 train no.228820  loss = 4.28777 avg_loss = 3.18075\n",
      "epoch no.3 train no.228830  loss = 4.21451 avg_loss = 3.22963\n",
      "epoch no.3 train no.228840  loss = 4.21132 avg_loss = 3.26638\n",
      "epoch no.3 train no.228850  loss = 3.21530 avg_loss = 3.31579\n",
      "epoch no.3 train no.228860  loss = 2.29508 avg_loss = 3.28512\n",
      "epoch no.3 train no.228870  loss = 3.35240 avg_loss = 3.26623\n",
      "epoch no.3 train no.228880  loss = 3.53686 avg_loss = 3.22859\n",
      "epoch no.3 train no.228890  loss = 4.63219 avg_loss = 3.26953\n",
      "epoch no.3 train no.228900  loss = 3.68312 avg_loss = 3.26007\n",
      "epoch no.3 train no.228910  loss = 3.50772 avg_loss = 3.27087\n",
      "epoch no.3 train no.228920  loss = 2.26035 avg_loss = 3.30443\n",
      "epoch no.3 train no.228930  loss = 2.30087 avg_loss = 3.32009\n",
      "epoch no.3 train no.228940  loss = 4.25538 avg_loss = 3.32164\n",
      "epoch no.3 train no.228950  loss = 2.37176 avg_loss = 3.28157\n",
      "epoch no.3 train no.228960  loss = 3.71747 avg_loss = 3.30384\n",
      "epoch no.3 train no.228970  loss = 2.40620 avg_loss = 3.26259\n",
      "epoch no.3 train no.228980  loss = 1.39451 avg_loss = 3.29942\n",
      "epoch no.3 train no.228990  loss = 3.77559 avg_loss = 3.30135\n",
      "epoch no.3 train no.229000  loss = 3.20633 avg_loss = 3.26591\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '▁다', '전에', '▁꼭', '이', '▁가기', '으면', '</s>']\n",
      "여름이 가기전에 여름이 왔어요</s>\n",
      "epoch no.3 train no.229010  loss = 2.95087 avg_loss = 3.20107\n",
      "epoch no.3 train no.229020  loss = 2.01588 avg_loss = 3.19866\n",
      "epoch no.3 train no.229030  loss = 3.76335 avg_loss = 3.21914\n",
      "epoch no.3 train no.229040  loss = 2.30549 avg_loss = 3.17956\n",
      "epoch no.3 train no.229050  loss = 4.48352 avg_loss = 3.18512\n",
      "epoch no.3 train no.229060  loss = 3.00324 avg_loss = 3.18258\n",
      "epoch no.3 train no.229070  loss = 2.23875 avg_loss = 3.23356\n",
      "epoch no.3 train no.229080  loss = 5.09193 avg_loss = 3.30777\n",
      "epoch no.3 train no.229090  loss = 3.22901 avg_loss = 3.30982\n",
      "epoch no.3 train no.229100  loss = 3.41794 avg_loss = 3.26309\n",
      "epoch no.3 train no.229110  loss = 2.04663 avg_loss = 3.23740\n",
      "epoch no.3 train no.229120  loss = 3.53715 avg_loss = 3.26365\n",
      "epoch no.3 train no.229130  loss = 2.77255 avg_loss = 3.26832\n",
      "epoch no.3 train no.229140  loss = 2.72808 avg_loss = 3.28377\n",
      "epoch no.3 train no.229150  loss = 2.18386 avg_loss = 3.25042\n",
      "epoch no.3 train no.229160  loss = 4.50520 avg_loss = 3.24775\n",
      "epoch no.3 train no.229170  loss = 2.39878 avg_loss = 3.23151\n",
      "epoch no.3 train no.229180  loss = 2.74187 avg_loss = 3.24904\n",
      "epoch no.3 train no.229190  loss = 4.01465 avg_loss = 3.26288\n",
      "epoch no.3 train no.229200  loss = 3.76582 avg_loss = 3.25658\n",
      "epoch no.3 train no.229210  loss = 2.60790 avg_loss = 3.21319\n",
      "epoch no.3 train no.229220  loss = 2.78100 avg_loss = 3.23441\n",
      "epoch no.3 train no.229230  loss = 3.77976 avg_loss = 3.20931\n",
      "epoch no.3 train no.229240  loss = 3.27357 avg_loss = 3.19274\n",
      "epoch no.3 train no.229250  loss = 3.40016 avg_loss = 3.30028\n",
      "epoch no.3 train no.229260  loss = 3.15253 avg_loss = 3.28323\n",
      "epoch no.3 train no.229270  loss = 3.06514 avg_loss = 3.30357\n",
      "epoch no.3 train no.229280  loss = 2.15170 avg_loss = 3.26875\n",
      "epoch no.3 train no.229290  loss = 2.33158 avg_loss = 3.28044\n",
      "epoch no.3 train no.229300  loss = 2.46603 avg_loss = 3.26990\n",
      "epoch no.3 train no.229310  loss = 2.73253 avg_loss = 3.28569\n",
      "epoch no.3 train no.229320  loss = 2.81879 avg_loss = 3.26546\n",
      "epoch no.3 train no.229330  loss = 2.90874 avg_loss = 3.20830\n",
      "epoch no.3 train no.229340  loss = 2.69645 avg_loss = 3.16469\n",
      "epoch no.3 train no.229350  loss = 3.36545 avg_loss = 3.15933\n",
      "epoch no.3 train no.229360  loss = 1.63600 avg_loss = 3.13631\n",
      "epoch no.3 train no.229370  loss = 3.44776 avg_loss = 3.11343\n",
      "epoch no.3 train no.229380  loss = 2.07823 avg_loss = 3.15615\n",
      "epoch no.3 train no.229390  loss = 2.58050 avg_loss = 3.14184\n",
      "epoch no.3 train no.229400  loss = 3.57455 avg_loss = 3.12598\n",
      "epoch no.3 train no.229410  loss = 2.44559 avg_loss = 3.16783\n",
      "epoch no.3 train no.229420  loss = 3.52740 avg_loss = 3.15094\n",
      "epoch no.3 train no.229430  loss = 1.87712 avg_loss = 3.16909\n",
      "epoch no.3 train no.229440  loss = 5.67344 avg_loss = 3.18944\n",
      "epoch no.3 train no.229450  loss = 3.28042 avg_loss = 3.21777\n",
      "epoch no.3 train no.229460  loss = 5.33770 avg_loss = 3.23731\n",
      "epoch no.3 train no.229470  loss = 5.00510 avg_loss = 3.28298\n",
      "epoch no.3 train no.229480  loss = 2.54310 avg_loss = 3.31171\n",
      "epoch no.3 train no.229490  loss = 2.91103 avg_loss = 3.31715\n",
      "epoch no.3 train no.229500  loss = 1.15091 avg_loss = 3.29717\n",
      "epoch no.3 train no.229510  loss = 3.49949 avg_loss = 3.26761\n",
      "epoch no.3 train no.229520  loss = 2.17857 avg_loss = 3.25103\n",
      "epoch no.3 train no.229530  loss = 2.05716 avg_loss = 3.24120\n",
      "epoch no.3 train no.229540  loss = 3.12286 avg_loss = 3.22437\n",
      "epoch no.3 train no.229550  loss = 3.24546 avg_loss = 3.19175\n",
      "epoch no.3 train no.229560  loss = 2.15806 avg_loss = 3.17510\n",
      "epoch no.3 train no.229570  loss = 4.02251 avg_loss = 3.20647\n",
      "epoch no.3 train no.229580  loss = 3.08257 avg_loss = 3.20490\n",
      "epoch no.3 train no.229590  loss = 2.72824 avg_loss = 3.15830\n",
      "epoch no.3 train no.229600  loss = 1.90184 avg_loss = 3.15241\n",
      "epoch no.3 train no.229610  loss = 3.11212 avg_loss = 3.18251\n",
      "epoch no.3 train no.229620  loss = 2.57591 avg_loss = 3.15913\n",
      "epoch no.3 train no.229630  loss = 2.60227 avg_loss = 3.10738\n",
      "epoch no.3 train no.229640  loss = 2.95535 avg_loss = 3.10111\n",
      "epoch no.3 train no.229650  loss = 4.88040 avg_loss = 3.10257\n",
      "epoch no.3 train no.229660  loss = 2.02857 avg_loss = 3.08338\n",
      "epoch no.3 train no.229670  loss = 4.47599 avg_loss = 3.08566\n",
      "epoch no.3 train no.229680  loss = 3.14683 avg_loss = 3.07586\n",
      "epoch no.3 train no.229690  loss = 2.79357 avg_loss = 3.04748\n",
      "epoch no.3 train no.229700  loss = 2.94166 avg_loss = 3.10031\n",
      "epoch no.3 train no.229710  loss = 2.25304 avg_loss = 3.10406\n",
      "epoch no.3 train no.229720  loss = 3.12161 avg_loss = 3.08305\n",
      "epoch no.3 train no.229730  loss = 4.22341 avg_loss = 3.11762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.229740  loss = 4.03120 avg_loss = 3.11251\n",
      "epoch no.3 train no.229750  loss = 3.69526 avg_loss = 3.12083\n",
      "epoch no.3 train no.229760  loss = 4.29936 avg_loss = 3.19573\n",
      "epoch no.3 train no.229770  loss = 2.03360 avg_loss = 3.19739\n",
      "epoch no.3 train no.229780  loss = 3.64369 avg_loss = 3.14936\n",
      "epoch no.3 train no.229790  loss = 4.62479 avg_loss = 3.15031\n",
      "epoch no.3 train no.229800  loss = 4.05519 avg_loss = 3.15000\n",
      "epoch no.3 train no.229810  loss = 3.46487 avg_loss = 3.20445\n",
      "epoch no.3 train no.229820  loss = 4.45871 avg_loss = 3.20864\n",
      "epoch no.3 train no.229830  loss = 2.73652 avg_loss = 3.17787\n",
      "epoch no.3 train no.229840  loss = 1.99801 avg_loss = 3.17431\n",
      "epoch no.3 train no.229850  loss = 4.67573 avg_loss = 3.17794\n",
      "epoch no.3 train no.229860  loss = 4.39488 avg_loss = 3.20522\n",
      "epoch no.3 train no.229870  loss = 2.55732 avg_loss = 3.23606\n",
      "epoch no.3 train no.229880  loss = 2.78627 avg_loss = 3.20662\n",
      "epoch no.3 train no.229890  loss = 2.87060 avg_loss = 3.18923\n",
      "epoch no.3 train no.229900  loss = 3.08751 avg_loss = 3.26123\n",
      "epoch no.3 train no.229910  loss = 2.25393 avg_loss = 3.28012\n",
      "epoch no.3 train no.229920  loss = 3.59039 avg_loss = 3.30892\n",
      "epoch no.3 train no.229930  loss = 3.01860 avg_loss = 3.29065\n",
      "epoch no.3 train no.229940  loss = 2.54207 avg_loss = 3.30076\n",
      "epoch no.3 train no.229950  loss = 4.20359 avg_loss = 3.29126\n",
      "epoch no.3 train no.229960  loss = 2.84244 avg_loss = 3.30473\n",
      "epoch no.3 train no.229970  loss = 3.15117 avg_loss = 3.29993\n",
      "epoch no.3 train no.229980  loss = 1.95614 avg_loss = 3.26496\n",
      "epoch no.3 train no.229990  loss = 3.66373 avg_loss = 3.25610\n",
      "epoch no.3 train no.230000  loss = 1.98890 avg_loss = 3.27193\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.230010  loss = 3.16836 avg_loss = 3.25047\n",
      "epoch no.3 train no.230020  loss = 2.86353 avg_loss = 3.27122\n",
      "epoch no.3 train no.230030  loss = 2.39407 avg_loss = 3.20426\n",
      "epoch no.3 train no.230040  loss = 4.37194 avg_loss = 3.19230\n",
      "epoch no.3 train no.230050  loss = 4.58635 avg_loss = 3.18374\n",
      "epoch no.3 train no.230060  loss = 4.63515 avg_loss = 3.20185\n",
      "epoch no.3 train no.230070  loss = 3.36556 avg_loss = 3.20470\n",
      "epoch no.3 train no.230080  loss = 5.61075 avg_loss = 3.21481\n",
      "epoch no.3 train no.230090  loss = 2.56288 avg_loss = 3.22154\n",
      "epoch no.3 train no.230100  loss = 2.51197 avg_loss = 3.20090\n",
      "epoch no.3 train no.230110  loss = 2.29758 avg_loss = 3.21450\n",
      "epoch no.3 train no.230120  loss = 3.23166 avg_loss = 3.20346\n",
      "epoch no.3 train no.230130  loss = 2.34511 avg_loss = 3.18584\n",
      "epoch no.3 train no.230140  loss = 2.21292 avg_loss = 3.18136\n",
      "epoch no.3 train no.230150  loss = 3.11862 avg_loss = 3.15205\n",
      "epoch no.3 train no.230160  loss = 3.60254 avg_loss = 3.18000\n",
      "epoch no.3 train no.230170  loss = 2.68992 avg_loss = 3.20557\n",
      "epoch no.3 train no.230180  loss = 3.30158 avg_loss = 3.24546\n",
      "epoch no.3 train no.230190  loss = 1.56494 avg_loss = 3.21647\n",
      "epoch no.3 train no.230200  loss = 3.00059 avg_loss = 3.24131\n",
      "epoch no.3 train no.230210  loss = 3.13672 avg_loss = 3.22895\n",
      "epoch no.3 train no.230220  loss = 3.71299 avg_loss = 3.31317\n",
      "epoch no.3 train no.230230  loss = 2.71379 avg_loss = 3.30352\n",
      "epoch no.3 train no.230240  loss = 2.97630 avg_loss = 3.30860\n",
      "epoch no.3 train no.230250  loss = 2.10890 avg_loss = 3.28353\n",
      "epoch no.3 train no.230260  loss = 3.52969 avg_loss = 3.27976\n",
      "epoch no.3 train no.230270  loss = 2.70058 avg_loss = 3.31251\n",
      "epoch no.3 train no.230280  loss = 2.83373 avg_loss = 3.34853\n",
      "epoch no.3 train no.230290  loss = 2.77016 avg_loss = 3.29325\n",
      "epoch no.3 train no.230300  loss = 3.52889 avg_loss = 3.33936\n",
      "epoch no.3 train no.230310  loss = 3.89082 avg_loss = 3.35239\n",
      "epoch no.3 train no.230320  loss = 3.38158 avg_loss = 3.34726\n",
      "epoch no.3 train no.230330  loss = 3.62084 avg_loss = 3.32309\n",
      "epoch no.3 train no.230340  loss = 2.07955 avg_loss = 3.29620\n",
      "epoch no.3 train no.230350  loss = 2.57965 avg_loss = 3.25107\n",
      "epoch no.3 train no.230360  loss = 3.91799 avg_loss = 3.23876\n",
      "epoch no.3 train no.230370  loss = 4.78347 avg_loss = 3.27520\n",
      "epoch no.3 train no.230380  loss = 2.24420 avg_loss = 3.28044\n",
      "epoch no.3 train no.230390  loss = 3.04537 avg_loss = 3.25202\n",
      "epoch no.3 train no.230400  loss = 2.71847 avg_loss = 3.23947\n",
      "epoch no.3 train no.230410  loss = 4.14209 avg_loss = 3.26641\n",
      "epoch no.3 train no.230420  loss = 3.47877 avg_loss = 3.21587\n",
      "epoch no.3 train no.230430  loss = 3.17275 avg_loss = 3.21062\n",
      "epoch no.3 train no.230440  loss = 3.38064 avg_loss = 3.17269\n",
      "epoch no.3 train no.230450  loss = 2.49590 avg_loss = 3.15842\n",
      "epoch no.3 train no.230460  loss = 2.95177 avg_loss = 3.17600\n",
      "epoch no.3 train no.230470  loss = 1.95082 avg_loss = 3.18460\n",
      "epoch no.3 train no.230480  loss = 3.89081 avg_loss = 3.25068\n",
      "epoch no.3 train no.230490  loss = 3.19387 avg_loss = 3.19675\n",
      "epoch no.3 train no.230500  loss = 2.58769 avg_loss = 3.15784\n",
      "epoch no.3 train no.230510  loss = 2.96567 avg_loss = 3.21036\n",
      "epoch no.3 train no.230520  loss = 2.51532 avg_loss = 3.19268\n",
      "epoch no.3 train no.230530  loss = 2.19121 avg_loss = 3.23058\n",
      "epoch no.3 train no.230540  loss = 2.51191 avg_loss = 3.26789\n",
      "epoch no.3 train no.230550  loss = 3.99719 avg_loss = 3.29613\n",
      "epoch no.3 train no.230560  loss = 4.15562 avg_loss = 3.31939\n",
      "epoch no.3 train no.230570  loss = 4.77017 avg_loss = 3.32875\n",
      "epoch no.3 train no.230580  loss = 4.07777 avg_loss = 3.33164\n",
      "epoch no.3 train no.230590  loss = 3.27772 avg_loss = 3.30143\n",
      "epoch no.3 train no.230600  loss = 2.56354 avg_loss = 3.26694\n",
      "epoch no.3 train no.230610  loss = 2.62917 avg_loss = 3.24063\n",
      "epoch no.3 train no.230620  loss = 3.28706 avg_loss = 3.24064\n",
      "epoch no.3 train no.230630  loss = 2.68595 avg_loss = 3.25096\n",
      "epoch no.3 train no.230640  loss = 3.44988 avg_loss = 3.24264\n",
      "epoch no.3 train no.230650  loss = 3.89230 avg_loss = 3.24056\n",
      "epoch no.3 train no.230660  loss = 2.04291 avg_loss = 3.20112\n",
      "epoch no.3 train no.230670  loss = 3.04173 avg_loss = 3.23612\n",
      "epoch no.3 train no.230680  loss = 2.39392 avg_loss = 3.21146\n",
      "epoch no.3 train no.230690  loss = 3.25787 avg_loss = 3.22573\n",
      "epoch no.3 train no.230700  loss = 3.85468 avg_loss = 3.22375\n",
      "epoch no.3 train no.230710  loss = 2.48510 avg_loss = 3.24451\n",
      "epoch no.3 train no.230720  loss = 4.44056 avg_loss = 3.27075\n",
      "epoch no.3 train no.230730  loss = 4.90593 avg_loss = 3.25391\n",
      "epoch no.3 train no.230740  loss = 2.94125 avg_loss = 3.26217\n",
      "epoch no.3 train no.230750  loss = 2.41861 avg_loss = 3.24984\n",
      "epoch no.3 train no.230760  loss = 2.14633 avg_loss = 3.23945\n",
      "epoch no.3 train no.230770  loss = 3.14770 avg_loss = 3.24148\n",
      "epoch no.3 train no.230780  loss = 2.52837 avg_loss = 3.26091\n",
      "epoch no.3 train no.230790  loss = 2.34926 avg_loss = 3.24141\n",
      "epoch no.3 train no.230800  loss = 3.09583 avg_loss = 3.26114\n",
      "epoch no.3 train no.230810  loss = 4.17361 avg_loss = 3.25650\n",
      "epoch no.3 train no.230820  loss = 2.05321 avg_loss = 3.19754\n",
      "epoch no.3 train no.230830  loss = 4.28942 avg_loss = 3.21423\n",
      "epoch no.3 train no.230840  loss = 3.57481 avg_loss = 3.16585\n",
      "epoch no.3 train no.230850  loss = 2.61081 avg_loss = 3.15717\n",
      "epoch no.3 train no.230860  loss = 3.59618 avg_loss = 3.17854\n",
      "epoch no.3 train no.230870  loss = 2.10435 avg_loss = 3.18473\n",
      "epoch no.3 train no.230880  loss = 1.78241 avg_loss = 3.17288\n",
      "epoch no.3 train no.230890  loss = 2.36495 avg_loss = 3.17174\n",
      "epoch no.3 train no.230900  loss = 5.44091 avg_loss = 3.20442\n",
      "epoch no.3 train no.230910  loss = 2.08807 avg_loss = 3.24015\n",
      "epoch no.3 train no.230920  loss = 2.63200 avg_loss = 3.20517\n",
      "epoch no.3 train no.230930  loss = 3.82773 avg_loss = 3.17987\n",
      "epoch no.3 train no.230940  loss = 2.28713 avg_loss = 3.18233\n",
      "epoch no.3 train no.230950  loss = 4.33113 avg_loss = 3.18927\n",
      "epoch no.3 train no.230960  loss = 1.32332 avg_loss = 3.12891\n",
      "epoch no.3 train no.230970  loss = 3.76503 avg_loss = 3.10846\n",
      "epoch no.3 train no.230980  loss = 2.61964 avg_loss = 3.08385\n",
      "epoch no.3 train no.230990  loss = 2.62711 avg_loss = 3.10331\n",
      "epoch no.3 train no.231000  loss = 3.68825 avg_loss = 3.15045\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣기', '▁감성', '한', '▁노래', '송', '</s>']\n",
      "여름밤에 듣는 잔잔한 팝송</s>\n",
      "epoch no.3 train no.231010  loss = 3.71474 avg_loss = 3.20571\n",
      "epoch no.3 train no.231020  loss = 1.83500 avg_loss = 3.18915\n",
      "epoch no.3 train no.231030  loss = 5.41896 avg_loss = 3.20479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.231040  loss = 2.57850 avg_loss = 3.19607\n",
      "epoch no.3 train no.231050  loss = 3.90704 avg_loss = 3.20983\n",
      "epoch no.3 train no.231060  loss = 3.48615 avg_loss = 3.25807\n",
      "epoch no.3 train no.231070  loss = 3.42043 avg_loss = 3.24498\n",
      "epoch no.3 train no.231080  loss = 3.06997 avg_loss = 3.27154\n",
      "epoch no.3 train no.231090  loss = 3.70816 avg_loss = 3.30110\n",
      "epoch no.3 train no.231100  loss = 2.84877 avg_loss = 3.30791\n",
      "epoch no.3 train no.231110  loss = 3.62133 avg_loss = 3.30993\n",
      "epoch no.3 train no.231120  loss = 3.24427 avg_loss = 3.29862\n",
      "epoch no.3 train no.231130  loss = 2.15705 avg_loss = 3.26782\n",
      "epoch no.3 train no.231140  loss = 3.13792 avg_loss = 3.22229\n",
      "epoch no.3 train no.231150  loss = 3.88551 avg_loss = 3.22673\n",
      "epoch no.3 train no.231160  loss = 1.88467 avg_loss = 3.18601\n",
      "epoch no.3 train no.231170  loss = 5.03460 avg_loss = 3.25156\n",
      "epoch no.3 train no.231180  loss = 4.22280 avg_loss = 3.27579\n",
      "epoch no.3 train no.231190  loss = 2.82576 avg_loss = 3.27558\n",
      "epoch no.3 train no.231200  loss = 3.81887 avg_loss = 3.27847\n",
      "epoch no.3 train no.231210  loss = 3.63774 avg_loss = 3.29743\n",
      "epoch no.3 train no.231220  loss = 2.77024 avg_loss = 3.28708\n",
      "epoch no.3 train no.231230  loss = 2.17901 avg_loss = 3.22710\n",
      "epoch no.3 train no.231240  loss = 3.36137 avg_loss = 3.22937\n",
      "epoch no.3 train no.231250  loss = 4.33518 avg_loss = 3.22333\n",
      "epoch no.3 train no.231260  loss = 3.73914 avg_loss = 3.18500\n",
      "epoch no.3 train no.231270  loss = 4.03534 avg_loss = 3.20666\n",
      "epoch no.3 train no.231280  loss = 2.05822 avg_loss = 3.20161\n",
      "epoch no.3 train no.231290  loss = 2.18264 avg_loss = 3.21363\n",
      "epoch no.3 train no.231300  loss = 2.67133 avg_loss = 3.20758\n",
      "epoch no.3 train no.231310  loss = 3.46269 avg_loss = 3.24173\n",
      "epoch no.3 train no.231320  loss = 2.17763 avg_loss = 3.24154\n",
      "epoch no.3 train no.231330  loss = 2.30674 avg_loss = 3.24325\n",
      "epoch no.3 train no.231340  loss = 3.35472 avg_loss = 3.23657\n",
      "epoch no.3 train no.231350  loss = 3.69396 avg_loss = 3.25371\n",
      "epoch no.3 train no.231360  loss = 2.72524 avg_loss = 3.24028\n",
      "epoch no.3 train no.231370  loss = 2.86262 avg_loss = 3.23960\n",
      "epoch no.3 train no.231380  loss = 2.06368 avg_loss = 3.24544\n",
      "epoch no.3 train no.231390  loss = 4.99509 avg_loss = 3.29457\n",
      "epoch no.3 train no.231400  loss = 2.52171 avg_loss = 3.33501\n",
      "epoch no.3 train no.231410  loss = 2.72561 avg_loss = 3.29601\n",
      "epoch no.3 train no.231420  loss = 4.61900 avg_loss = 3.33823\n",
      "epoch no.3 train no.231430  loss = 2.45781 avg_loss = 3.36400\n",
      "epoch no.3 train no.231440  loss = 3.19398 avg_loss = 3.35428\n",
      "epoch no.3 train no.231450  loss = 3.37943 avg_loss = 3.33519\n",
      "epoch no.3 train no.231460  loss = 2.04641 avg_loss = 3.30600\n",
      "epoch no.3 train no.231470  loss = 3.60690 avg_loss = 3.33379\n",
      "epoch no.3 train no.231480  loss = 2.64553 avg_loss = 3.26148\n",
      "epoch no.3 train no.231490  loss = 3.49991 avg_loss = 3.28509\n",
      "epoch no.3 train no.231500  loss = 4.08475 avg_loss = 3.29685\n",
      "epoch no.3 train no.231510  loss = 3.59861 avg_loss = 3.30198\n",
      "epoch no.3 train no.231520  loss = 2.21815 avg_loss = 3.31661\n",
      "epoch no.3 train no.231530  loss = 2.33149 avg_loss = 3.28526\n",
      "epoch no.3 train no.231540  loss = 4.32869 avg_loss = 3.24901\n",
      "epoch no.3 train no.231550  loss = 2.08138 avg_loss = 3.19640\n",
      "epoch no.3 train no.231560  loss = 2.70798 avg_loss = 3.21960\n",
      "epoch no.3 train no.231570  loss = 2.81848 avg_loss = 3.24001\n",
      "epoch no.3 train no.231580  loss = 3.15395 avg_loss = 3.18832\n",
      "epoch no.3 train no.231590  loss = 1.88121 avg_loss = 3.21896\n",
      "epoch no.3 train no.231600  loss = 3.81666 avg_loss = 3.25287\n",
      "epoch no.3 train no.231610  loss = 3.14042 avg_loss = 3.24835\n",
      "epoch no.3 train no.231620  loss = 2.53274 avg_loss = 3.25454\n",
      "epoch no.3 train no.231630  loss = 2.86324 avg_loss = 3.25144\n",
      "epoch no.3 train no.231640  loss = 2.76196 avg_loss = 3.26111\n",
      "epoch no.3 train no.231650  loss = 3.35838 avg_loss = 3.23167\n",
      "epoch no.3 train no.231660  loss = 4.84929 avg_loss = 3.27208\n",
      "epoch no.3 train no.231670  loss = 2.10463 avg_loss = 3.25749\n",
      "epoch no.3 train no.231680  loss = 3.00279 avg_loss = 3.20481\n",
      "epoch no.3 train no.231690  loss = 2.16070 avg_loss = 3.18864\n",
      "epoch no.3 train no.231700  loss = 3.33027 avg_loss = 3.21538\n",
      "epoch no.3 train no.231710  loss = 3.25560 avg_loss = 3.19208\n",
      "epoch no.3 train no.231720  loss = 3.05777 avg_loss = 3.21214\n",
      "epoch no.3 train no.231730  loss = 3.50907 avg_loss = 3.24869\n",
      "epoch no.3 train no.231740  loss = 3.33111 avg_loss = 3.24326\n",
      "epoch no.3 train no.231750  loss = 4.01801 avg_loss = 3.28709\n",
      "epoch no.3 train no.231760  loss = 5.22471 avg_loss = 3.27086\n",
      "epoch no.3 train no.231770  loss = 3.18679 avg_loss = 3.28024\n",
      "epoch no.3 train no.231780  loss = 2.70537 avg_loss = 3.26591\n",
      "epoch no.3 train no.231790  loss = 2.96623 avg_loss = 3.27471\n",
      "epoch no.3 train no.231800  loss = 4.63754 avg_loss = 3.29673\n",
      "epoch no.3 train no.231810  loss = 4.06629 avg_loss = 3.32541\n",
      "epoch no.3 train no.231820  loss = 4.35628 avg_loss = 3.33693\n",
      "epoch no.3 train no.231830  loss = 2.83538 avg_loss = 3.30055\n",
      "epoch no.3 train no.231840  loss = 3.32260 avg_loss = 3.32203\n",
      "epoch no.3 train no.231850  loss = 3.91855 avg_loss = 3.29145\n",
      "epoch no.3 train no.231860  loss = 2.52210 avg_loss = 3.30288\n",
      "epoch no.3 train no.231870  loss = 6.25669 avg_loss = 3.28488\n",
      "epoch no.3 train no.231880  loss = 4.01375 avg_loss = 3.31205\n",
      "epoch no.3 train no.231890  loss = 3.17113 avg_loss = 3.32043\n",
      "epoch no.3 train no.231900  loss = 3.14870 avg_loss = 3.31921\n",
      "epoch no.3 train no.231910  loss = 3.00218 avg_loss = 3.26122\n",
      "epoch no.3 train no.231920  loss = 4.66988 avg_loss = 3.23879\n",
      "epoch no.3 train no.231930  loss = 3.71442 avg_loss = 3.20134\n",
      "epoch no.3 train no.231940  loss = 4.11740 avg_loss = 3.26147\n",
      "epoch no.3 train no.231950  loss = 5.18804 avg_loss = 3.26540\n",
      "epoch no.3 train no.231960  loss = 2.56746 avg_loss = 3.24750\n",
      "epoch no.3 train no.231970  loss = 2.46440 avg_loss = 3.22008\n",
      "epoch no.3 train no.231980  loss = 4.36711 avg_loss = 3.23796\n",
      "epoch no.3 train no.231990  loss = 3.33840 avg_loss = 3.20897\n",
      "epoch no.3 train no.232000  loss = 2.30902 avg_loss = 3.14249\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁잔잔', '</s>']\n",
      "여름밤에 듣는 재즈</s>\n",
      "epoch no.3 train no.232010  loss = 3.03821 avg_loss = 3.15657\n",
      "epoch no.3 train no.232020  loss = 2.51063 avg_loss = 3.14819\n",
      "epoch no.3 train no.232030  loss = 3.27756 avg_loss = 3.10999\n",
      "epoch no.3 train no.232040  loss = 2.63486 avg_loss = 3.14602\n",
      "epoch no.3 train no.232050  loss = 2.29431 avg_loss = 3.14134\n",
      "epoch no.3 train no.232060  loss = 2.39523 avg_loss = 3.08558\n",
      "epoch no.3 train no.232070  loss = 3.33401 avg_loss = 3.08274\n",
      "epoch no.3 train no.232080  loss = 2.94202 avg_loss = 3.13201\n",
      "epoch no.3 train no.232090  loss = 3.08577 avg_loss = 3.15093\n",
      "epoch no.3 train no.232100  loss = 4.03356 avg_loss = 3.18758\n",
      "epoch no.3 train no.232110  loss = 3.20126 avg_loss = 3.22980\n",
      "epoch no.3 train no.232120  loss = 2.82438 avg_loss = 3.26668\n",
      "epoch no.3 train no.232130  loss = 2.40258 avg_loss = 3.21982\n",
      "epoch no.3 train no.232140  loss = 2.03376 avg_loss = 3.19218\n",
      "epoch no.3 train no.232150  loss = 4.07942 avg_loss = 3.21920\n",
      "epoch no.3 train no.232160  loss = 3.20252 avg_loss = 3.21254\n",
      "epoch no.3 train no.232170  loss = 2.92869 avg_loss = 3.22456\n",
      "epoch no.3 train no.232180  loss = 4.89193 avg_loss = 3.27649\n",
      "epoch no.3 train no.232190  loss = 4.29016 avg_loss = 3.29778\n",
      "epoch no.3 train no.232200  loss = 3.51414 avg_loss = 3.31378\n",
      "epoch no.3 train no.232210  loss = 4.25519 avg_loss = 3.27018\n",
      "epoch no.3 train no.232220  loss = 1.97955 avg_loss = 3.26646\n",
      "epoch no.3 train no.232230  loss = 1.97899 avg_loss = 3.24158\n",
      "epoch no.3 train no.232240  loss = 3.45814 avg_loss = 3.29110\n",
      "epoch no.3 train no.232250  loss = 3.77763 avg_loss = 3.27744\n",
      "epoch no.3 train no.232260  loss = 3.24123 avg_loss = 3.29044\n",
      "epoch no.3 train no.232270  loss = 3.31816 avg_loss = 3.30791\n",
      "epoch no.3 train no.232280  loss = 6.17299 avg_loss = 3.30708\n",
      "epoch no.3 train no.232290  loss = 3.55761 avg_loss = 3.35046\n",
      "epoch no.3 train no.232300  loss = 3.08736 avg_loss = 3.32617\n",
      "epoch no.3 train no.232310  loss = 2.94501 avg_loss = 3.32730\n",
      "epoch no.3 train no.232320  loss = 3.17095 avg_loss = 3.29774\n",
      "epoch no.3 train no.232330  loss = 4.38688 avg_loss = 3.29420\n",
      "epoch no.3 train no.232340  loss = 5.18853 avg_loss = 3.31617\n",
      "epoch no.3 train no.232350  loss = 3.88975 avg_loss = 3.31981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.232360  loss = 3.83884 avg_loss = 3.32679\n",
      "epoch no.3 train no.232370  loss = 4.46960 avg_loss = 3.33700\n",
      "epoch no.3 train no.232380  loss = 4.76185 avg_loss = 3.33898\n",
      "epoch no.3 train no.232390  loss = 4.42619 avg_loss = 3.35847\n",
      "epoch no.3 train no.232400  loss = 2.64200 avg_loss = 3.31450\n",
      "epoch no.3 train no.232410  loss = 3.61242 avg_loss = 3.28857\n",
      "epoch no.3 train no.232420  loss = 5.50333 avg_loss = 3.31377\n",
      "epoch no.3 train no.232430  loss = 4.00959 avg_loss = 3.30802\n",
      "epoch no.3 train no.232440  loss = 2.06096 avg_loss = 3.32350\n",
      "epoch no.3 train no.232450  loss = 3.17290 avg_loss = 3.36730\n",
      "epoch no.3 train no.232460  loss = 3.02828 avg_loss = 3.34573\n",
      "epoch no.3 train no.232470  loss = 4.32582 avg_loss = 3.38723\n",
      "epoch no.3 train no.232480  loss = 4.02023 avg_loss = 3.41647\n",
      "epoch no.3 train no.232490  loss = 3.28068 avg_loss = 3.38612\n",
      "epoch no.3 train no.232500  loss = 3.88586 avg_loss = 3.39807\n",
      "epoch no.3 train no.232510  loss = 2.82272 avg_loss = 3.36075\n",
      "epoch no.3 train no.232520  loss = 2.24781 avg_loss = 3.29304\n",
      "epoch no.3 train no.232530  loss = 5.24191 avg_loss = 3.28793\n",
      "epoch no.3 train no.232540  loss = 3.42477 avg_loss = 3.26846\n",
      "epoch no.3 train no.232550  loss = 4.14228 avg_loss = 3.29151\n",
      "epoch no.3 train no.232560  loss = 0.93936 avg_loss = 3.22281\n",
      "epoch no.3 train no.232570  loss = 3.13210 avg_loss = 3.26660\n",
      "epoch no.3 train no.232580  loss = 4.41586 avg_loss = 3.27252\n",
      "epoch no.3 train no.232590  loss = 2.74754 avg_loss = 3.26532\n",
      "epoch no.3 train no.232600  loss = 1.80662 avg_loss = 3.27016\n",
      "epoch no.3 train no.232610  loss = 1.90045 avg_loss = 3.24524\n",
      "epoch no.3 train no.232620  loss = 2.78021 avg_loss = 3.23931\n",
      "epoch no.3 train no.232630  loss = 2.68181 avg_loss = 3.25441\n",
      "epoch no.3 train no.232640  loss = 2.54766 avg_loss = 3.23568\n",
      "epoch no.3 train no.232650  loss = 2.21969 avg_loss = 3.23058\n",
      "epoch no.3 train no.232660  loss = 3.54660 avg_loss = 3.22617\n",
      "epoch no.3 train no.232670  loss = 4.59760 avg_loss = 3.24209\n",
      "epoch no.3 train no.232680  loss = 2.73449 avg_loss = 3.30916\n",
      "epoch no.3 train no.232690  loss = 2.81682 avg_loss = 3.27959\n",
      "epoch no.3 train no.232700  loss = 3.06266 avg_loss = 3.25558\n",
      "epoch no.3 train no.232710  loss = 3.68528 avg_loss = 3.25746\n",
      "epoch no.3 train no.232720  loss = 4.31899 avg_loss = 3.29275\n",
      "epoch no.3 train no.232730  loss = 2.79225 avg_loss = 3.27072\n",
      "epoch no.3 train no.232740  loss = 3.30432 avg_loss = 3.24996\n",
      "epoch no.3 train no.232750  loss = 4.42349 avg_loss = 3.27828\n",
      "epoch no.3 train no.232760  loss = 3.64650 avg_loss = 3.31206\n",
      "epoch no.3 train no.232770  loss = 3.63282 avg_loss = 3.32878\n",
      "epoch no.3 train no.232780  loss = 4.00825 avg_loss = 3.29041\n",
      "epoch no.3 train no.232790  loss = 2.15991 avg_loss = 3.29150\n",
      "epoch no.3 train no.232800  loss = 1.94124 avg_loss = 3.23690\n",
      "epoch no.3 train no.232810  loss = 2.86493 avg_loss = 3.26029\n",
      "epoch no.3 train no.232820  loss = 3.26642 avg_loss = 3.27050\n",
      "epoch no.3 train no.232830  loss = 3.63125 avg_loss = 3.21975\n",
      "epoch no.3 train no.232840  loss = 4.04905 avg_loss = 3.19768\n",
      "epoch no.3 train no.232850  loss = 2.57167 avg_loss = 3.16845\n",
      "epoch no.3 train no.232860  loss = 4.59772 avg_loss = 3.15768\n",
      "epoch no.3 train no.232870  loss = 3.64132 avg_loss = 3.17467\n",
      "epoch no.3 train no.232880  loss = 4.71681 avg_loss = 3.23337\n",
      "epoch no.3 train no.232890  loss = 2.59268 avg_loss = 3.24950\n",
      "epoch no.3 train no.232900  loss = 2.55120 avg_loss = 3.28102\n",
      "epoch no.3 train no.232910  loss = 2.31424 avg_loss = 3.29958\n",
      "epoch no.3 train no.232920  loss = 3.02910 avg_loss = 3.34802\n",
      "epoch no.3 train no.232930  loss = 3.39867 avg_loss = 3.35868\n",
      "epoch no.3 train no.232940  loss = 3.25972 avg_loss = 3.33894\n",
      "epoch no.3 train no.232950  loss = 2.29736 avg_loss = 3.35154\n",
      "epoch no.3 train no.232960  loss = 3.52599 avg_loss = 3.31753\n",
      "epoch no.3 train no.232970  loss = 2.48941 avg_loss = 3.39173\n",
      "epoch no.3 train no.232980  loss = 2.74536 avg_loss = 3.42720\n",
      "epoch no.3 train no.232990  loss = 2.92904 avg_loss = 3.41988\n",
      "epoch no.3 train no.233000  loss = 2.67688 avg_loss = 3.39078\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '▁듣기', '하며', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름 밤 산책하며 듣기 좋은 음악</s>\n",
      "epoch no.3 train no.233010  loss = 2.36550 avg_loss = 3.33259\n",
      "epoch no.3 train no.233020  loss = 3.04620 avg_loss = 3.32174\n",
      "epoch no.3 train no.233030  loss = 3.40082 avg_loss = 3.30466\n",
      "epoch no.3 train no.233040  loss = 2.81149 avg_loss = 3.25790\n",
      "epoch no.3 train no.233050  loss = 3.00297 avg_loss = 3.24143\n",
      "epoch no.3 train no.233060  loss = 2.56006 avg_loss = 3.24735\n",
      "epoch no.3 train no.233070  loss = 2.82950 avg_loss = 3.21800\n",
      "epoch no.3 train no.233080  loss = 4.37171 avg_loss = 3.25547\n",
      "epoch no.3 train no.233090  loss = 4.63293 avg_loss = 3.27772\n",
      "epoch no.3 train no.233100  loss = 2.27778 avg_loss = 3.20573\n",
      "epoch no.3 train no.233110  loss = 2.21864 avg_loss = 3.20367\n",
      "epoch no.3 train no.233120  loss = 2.60821 avg_loss = 3.18799\n",
      "epoch no.3 train no.233130  loss = 3.59215 avg_loss = 3.16438\n",
      "epoch no.3 train no.233140  loss = 4.96935 avg_loss = 3.18566\n",
      "epoch no.3 train no.233150  loss = 2.86846 avg_loss = 3.17451\n",
      "epoch no.3 train no.233160  loss = 3.11750 avg_loss = 3.17336\n",
      "epoch no.3 train no.233170  loss = 1.97430 avg_loss = 3.20940\n",
      "epoch no.3 train no.233180  loss = 4.01371 avg_loss = 3.23603\n",
      "epoch no.3 train no.233190  loss = 3.25814 avg_loss = 3.23668\n",
      "epoch no.3 train no.233200  loss = 2.41408 avg_loss = 3.25633\n",
      "epoch no.3 train no.233210  loss = 2.63577 avg_loss = 3.16990\n",
      "epoch no.3 train no.233220  loss = 5.92528 avg_loss = 3.15167\n",
      "epoch no.3 train no.233230  loss = 4.28542 avg_loss = 3.15179\n",
      "epoch no.3 train no.233240  loss = 2.13730 avg_loss = 3.11530\n",
      "epoch no.3 train no.233250  loss = 2.73443 avg_loss = 3.10989\n",
      "epoch no.3 train no.233260  loss = 4.17882 avg_loss = 3.12288\n",
      "epoch no.3 train no.233270  loss = 2.39689 avg_loss = 3.14039\n",
      "epoch no.3 train no.233280  loss = 3.08767 avg_loss = 3.16811\n",
      "epoch no.3 train no.233290  loss = 2.77126 avg_loss = 3.22697\n",
      "epoch no.3 train no.233300  loss = 3.10265 avg_loss = 3.25621\n",
      "epoch no.3 train no.233310  loss = 5.98054 avg_loss = 3.26512\n",
      "epoch no.3 train no.233320  loss = 2.18144 avg_loss = 3.23675\n",
      "epoch no.3 train no.233330  loss = 3.00979 avg_loss = 3.20872\n",
      "epoch no.3 train no.233340  loss = 2.81311 avg_loss = 3.23005\n",
      "epoch no.3 train no.233350  loss = 2.00290 avg_loss = 3.22449\n",
      "epoch no.3 train no.233360  loss = 2.17160 avg_loss = 3.24918\n",
      "epoch no.3 train no.233370  loss = 3.11778 avg_loss = 3.25886\n",
      "epoch no.3 train no.233380  loss = 1.85453 avg_loss = 3.19882\n",
      "epoch no.3 train no.233390  loss = 4.27675 avg_loss = 3.25217\n",
      "epoch no.3 train no.233400  loss = 2.71618 avg_loss = 3.22760\n",
      "epoch no.3 train no.233410  loss = 4.69280 avg_loss = 3.23936\n",
      "epoch no.3 train no.233420  loss = 2.79999 avg_loss = 3.20936\n",
      "epoch no.3 train no.233430  loss = 3.24687 avg_loss = 3.19279\n",
      "epoch no.3 train no.233440  loss = 3.90670 avg_loss = 3.19876\n",
      "epoch no.3 train no.233450  loss = 2.56214 avg_loss = 3.16365\n",
      "epoch no.3 train no.233460  loss = 5.41119 avg_loss = 3.18466\n",
      "epoch no.3 train no.233470  loss = 3.88231 avg_loss = 3.21680\n",
      "epoch no.3 train no.233480  loss = 2.28220 avg_loss = 3.22175\n",
      "epoch no.3 train no.233490  loss = 2.74234 avg_loss = 3.24291\n",
      "epoch no.3 train no.233500  loss = 2.42005 avg_loss = 3.20419\n",
      "epoch no.3 train no.233510  loss = 2.62106 avg_loss = 3.19367\n",
      "epoch no.3 train no.233520  loss = 3.02364 avg_loss = 3.19605\n",
      "epoch no.3 train no.233530  loss = 4.05031 avg_loss = 3.17884\n",
      "epoch no.3 train no.233540  loss = 3.48009 avg_loss = 3.17575\n",
      "epoch no.3 train no.233550  loss = 2.02163 avg_loss = 3.21141\n",
      "epoch no.3 train no.233560  loss = 3.61621 avg_loss = 3.21230\n",
      "epoch no.3 train no.233570  loss = 2.14417 avg_loss = 3.16794\n",
      "epoch no.3 train no.233580  loss = 4.03330 avg_loss = 3.20025\n",
      "epoch no.3 train no.233590  loss = 2.68276 avg_loss = 3.17252\n",
      "epoch no.3 train no.233600  loss = 3.75209 avg_loss = 3.20421\n",
      "epoch no.3 train no.233610  loss = 3.01618 avg_loss = 3.22973\n",
      "epoch no.3 train no.233620  loss = 1.72315 avg_loss = 3.23050\n",
      "epoch no.3 train no.233630  loss = 4.26344 avg_loss = 3.27275\n",
      "epoch no.3 train no.233640  loss = 3.23854 avg_loss = 3.29490\n",
      "epoch no.3 train no.233650  loss = 4.55499 avg_loss = 3.34942\n",
      "epoch no.3 train no.233660  loss = 3.03401 avg_loss = 3.33515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.233670  loss = 4.37370 avg_loss = 3.31196\n",
      "epoch no.3 train no.233680  loss = 2.21150 avg_loss = 3.28567\n",
      "epoch no.3 train no.233690  loss = 2.07059 avg_loss = 3.28235\n",
      "epoch no.3 train no.233700  loss = 3.20031 avg_loss = 3.30659\n",
      "epoch no.3 train no.233710  loss = 3.61529 avg_loss = 3.31336\n",
      "epoch no.3 train no.233720  loss = 3.38366 avg_loss = 3.29886\n",
      "epoch no.3 train no.233730  loss = 3.93909 avg_loss = 3.30914\n",
      "epoch no.3 train no.233740  loss = 2.49749 avg_loss = 3.25576\n",
      "epoch no.3 train no.233750  loss = 3.14328 avg_loss = 3.21395\n",
      "epoch no.3 train no.233760  loss = 4.41099 avg_loss = 3.20924\n",
      "epoch no.3 train no.233770  loss = 2.74319 avg_loss = 3.18951\n",
      "epoch no.3 train no.233780  loss = 3.42029 avg_loss = 3.21860\n",
      "epoch no.3 train no.233790  loss = 4.15713 avg_loss = 3.21785\n",
      "epoch no.3 train no.233800  loss = 4.52194 avg_loss = 3.25282\n",
      "epoch no.3 train no.233810  loss = 2.37336 avg_loss = 3.25301\n",
      "epoch no.3 train no.233820  loss = 3.27033 avg_loss = 3.23765\n",
      "epoch no.3 train no.233830  loss = 1.64407 avg_loss = 3.21632\n",
      "epoch no.3 train no.233840  loss = 3.41306 avg_loss = 3.24451\n",
      "epoch no.3 train no.233850  loss = 4.01060 avg_loss = 3.26337\n",
      "epoch no.3 train no.233860  loss = 4.25938 avg_loss = 3.27243\n",
      "epoch no.3 train no.233870  loss = 1.72325 avg_loss = 3.27206\n",
      "epoch no.3 train no.233880  loss = 3.94736 avg_loss = 3.27717\n",
      "epoch no.3 train no.233890  loss = 3.18114 avg_loss = 3.29418\n",
      "epoch no.3 train no.233900  loss = 3.02829 avg_loss = 3.28889\n",
      "epoch no.3 train no.233910  loss = 1.24827 avg_loss = 3.31597\n",
      "epoch no.3 train no.233920  loss = 3.11963 avg_loss = 3.30789\n",
      "epoch no.3 train no.233930  loss = 2.81579 avg_loss = 3.28905\n",
      "epoch no.3 train no.233940  loss = 2.22083 avg_loss = 3.26825\n",
      "epoch no.3 train no.233950  loss = 3.79361 avg_loss = 3.26958\n",
      "epoch no.3 train no.233960  loss = 3.54988 avg_loss = 3.28859\n",
      "epoch no.3 train no.233970  loss = 2.59066 avg_loss = 3.33730\n",
      "epoch no.3 train no.233980  loss = 2.87708 avg_loss = 3.35394\n",
      "epoch no.3 train no.233990  loss = 4.95191 avg_loss = 3.39025\n",
      "epoch no.3 train no.234000  loss = 2.40451 avg_loss = 3.35447\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁이', '피', '컬', '▁하우스', '</s>']\n",
      "여름엔 트로피컬 하우스</s>\n",
      "epoch no.3 train no.234010  loss = 4.55921 avg_loss = 3.37125\n",
      "epoch no.3 train no.234020  loss = 3.15497 avg_loss = 3.37312\n",
      "epoch no.3 train no.234030  loss = 4.85840 avg_loss = 3.37222\n",
      "epoch no.3 train no.234040  loss = 1.95747 avg_loss = 3.40221\n",
      "epoch no.3 train no.234050  loss = 3.26697 avg_loss = 3.38797\n",
      "epoch no.3 train no.234060  loss = 3.03140 avg_loss = 3.38044\n",
      "epoch no.3 train no.234070  loss = 3.66037 avg_loss = 3.38031\n",
      "epoch no.3 train no.234080  loss = 2.50910 avg_loss = 3.34023\n",
      "epoch no.3 train no.234090  loss = 3.93720 avg_loss = 3.35213\n",
      "epoch no.3 train no.234100  loss = 2.13907 avg_loss = 3.40348\n",
      "epoch no.3 train no.234110  loss = 3.33049 avg_loss = 3.38134\n",
      "epoch no.3 train no.234120  loss = 3.62019 avg_loss = 3.37007\n",
      "epoch no.3 train no.234130  loss = 2.87025 avg_loss = 3.33789\n",
      "epoch no.3 train no.234140  loss = 2.82268 avg_loss = 3.29104\n",
      "epoch no.3 train no.234150  loss = 3.05827 avg_loss = 3.21686\n",
      "epoch no.3 train no.234160  loss = 2.77996 avg_loss = 3.24387\n",
      "epoch no.3 train no.234170  loss = 3.94803 avg_loss = 3.27640\n",
      "epoch no.3 train no.234180  loss = 4.69161 avg_loss = 3.31228\n",
      "epoch no.3 train no.234190  loss = 3.57635 avg_loss = 3.34142\n",
      "epoch no.3 train no.234200  loss = 2.27478 avg_loss = 3.27779\n",
      "epoch no.3 train no.234210  loss = 4.87050 avg_loss = 3.29764\n",
      "epoch no.3 train no.234220  loss = 2.71689 avg_loss = 3.29273\n",
      "epoch no.3 train no.234230  loss = 3.96313 avg_loss = 3.27874\n",
      "epoch no.3 train no.234240  loss = 3.70074 avg_loss = 3.30517\n",
      "epoch no.3 train no.234250  loss = 3.76707 avg_loss = 3.28334\n",
      "epoch no.3 train no.234260  loss = 2.56290 avg_loss = 3.33133\n",
      "epoch no.3 train no.234270  loss = 2.42025 avg_loss = 3.35906\n",
      "epoch no.3 train no.234280  loss = 2.69603 avg_loss = 3.34607\n",
      "epoch no.3 train no.234290  loss = 3.70417 avg_loss = 3.35374\n",
      "epoch no.3 train no.234300  loss = 4.14027 avg_loss = 3.37229\n",
      "epoch no.3 train no.234310  loss = 2.60743 avg_loss = 3.37788\n",
      "epoch no.3 train no.234320  loss = 1.83452 avg_loss = 3.35771\n",
      "epoch no.3 train no.234330  loss = 3.63383 avg_loss = 3.34662\n",
      "epoch no.3 train no.234340  loss = 2.29401 avg_loss = 3.36007\n",
      "epoch no.3 train no.234350  loss = 2.97854 avg_loss = 3.34670\n",
      "epoch no.3 train no.234360  loss = 4.06041 avg_loss = 3.33775\n",
      "epoch no.3 train no.234370  loss = 3.57283 avg_loss = 3.32677\n",
      "epoch no.3 train no.234380  loss = 3.67780 avg_loss = 3.32596\n",
      "epoch no.3 train no.234390  loss = 3.75178 avg_loss = 3.29688\n",
      "epoch no.3 train no.234400  loss = 4.67447 avg_loss = 3.32912\n",
      "epoch no.3 train no.234410  loss = 3.13822 avg_loss = 3.26456\n",
      "epoch no.3 train no.234420  loss = 1.98125 avg_loss = 3.22281\n",
      "epoch no.3 train no.234430  loss = 4.64955 avg_loss = 3.21775\n",
      "epoch no.3 train no.234440  loss = 3.39460 avg_loss = 3.20458\n",
      "epoch no.3 train no.234450  loss = 4.14180 avg_loss = 3.25011\n",
      "epoch no.3 train no.234460  loss = 4.04171 avg_loss = 3.29843\n",
      "epoch no.3 train no.234470  loss = 2.91614 avg_loss = 3.29010\n",
      "epoch no.3 train no.234480  loss = 3.89951 avg_loss = 3.31690\n",
      "epoch no.3 train no.234490  loss = 3.63361 avg_loss = 3.31608\n",
      "epoch no.3 train no.234500  loss = 2.23148 avg_loss = 3.31240\n",
      "epoch no.3 train no.234510  loss = 2.54678 avg_loss = 3.29366\n",
      "epoch no.3 train no.234520  loss = 3.62264 avg_loss = 3.25759\n",
      "epoch no.3 train no.234530  loss = 1.65180 avg_loss = 3.25334\n",
      "epoch no.3 train no.234540  loss = 2.50458 avg_loss = 3.20801\n",
      "epoch no.3 train no.234550  loss = 2.36791 avg_loss = 3.17484\n",
      "epoch no.3 train no.234560  loss = 3.42069 avg_loss = 3.21586\n",
      "epoch no.3 train no.234570  loss = 2.86716 avg_loss = 3.20083\n",
      "epoch no.3 train no.234580  loss = 2.56226 avg_loss = 3.17837\n",
      "epoch no.3 train no.234590  loss = 2.74261 avg_loss = 3.23483\n",
      "epoch no.3 train no.234600  loss = 2.89115 avg_loss = 3.19103\n",
      "epoch no.3 train no.234610  loss = 4.04188 avg_loss = 3.22621\n",
      "epoch no.3 train no.234620  loss = 2.59141 avg_loss = 3.22561\n",
      "epoch no.3 train no.234630  loss = 1.95345 avg_loss = 3.23152\n",
      "epoch no.3 train no.234640  loss = 3.12281 avg_loss = 3.23000\n",
      "epoch no.3 train no.234650  loss = 4.97783 avg_loss = 3.23896\n",
      "epoch no.3 train no.234660  loss = 3.28372 avg_loss = 3.23998\n",
      "epoch no.3 train no.234670  loss = 4.12536 avg_loss = 3.28547\n",
      "epoch no.3 train no.234680  loss = 3.49123 avg_loss = 3.31687\n",
      "epoch no.3 train no.234690  loss = 3.90917 avg_loss = 3.34914\n",
      "epoch no.3 train no.234700  loss = 2.92920 avg_loss = 3.35897\n",
      "epoch no.3 train no.234710  loss = 5.75690 avg_loss = 3.39279\n",
      "epoch no.3 train no.234720  loss = 1.75334 avg_loss = 3.38211\n",
      "epoch no.3 train no.234730  loss = 3.25266 avg_loss = 3.35104\n",
      "epoch no.3 train no.234740  loss = 2.89208 avg_loss = 3.32545\n",
      "epoch no.3 train no.234750  loss = 2.90030 avg_loss = 3.28889\n",
      "epoch no.3 train no.234760  loss = 2.50725 avg_loss = 3.29075\n",
      "epoch no.3 train no.234770  loss = 4.35954 avg_loss = 3.30052\n",
      "epoch no.3 train no.234780  loss = 2.73485 avg_loss = 3.27906\n",
      "epoch no.3 train no.234790  loss = 2.69433 avg_loss = 3.23522\n",
      "epoch no.3 train no.234800  loss = 4.02037 avg_loss = 3.25080\n",
      "epoch no.3 train no.234810  loss = 2.83457 avg_loss = 3.23781\n",
      "epoch no.3 train no.234820  loss = 3.18899 avg_loss = 3.25746\n",
      "epoch no.3 train no.234830  loss = 3.69192 avg_loss = 3.25135\n",
      "epoch no.3 train no.234840  loss = 3.15509 avg_loss = 3.23106\n",
      "epoch no.3 train no.234850  loss = 2.35719 avg_loss = 3.22981\n",
      "epoch no.3 train no.234860  loss = 2.33249 avg_loss = 3.26211\n",
      "epoch no.3 train no.234870  loss = 3.95179 avg_loss = 3.28267\n",
      "epoch no.3 train no.234880  loss = 3.77514 avg_loss = 3.25771\n",
      "epoch no.3 train no.234890  loss = 3.64107 avg_loss = 3.31053\n",
      "epoch no.3 train no.234900  loss = 2.34601 avg_loss = 3.27062\n",
      "epoch no.3 train no.234910  loss = 2.17322 avg_loss = 3.30185\n",
      "epoch no.3 train no.234920  loss = 2.87873 avg_loss = 3.32237\n",
      "epoch no.3 train no.234930  loss = 2.04195 avg_loss = 3.25795\n",
      "epoch no.3 train no.234940  loss = 3.58060 avg_loss = 3.24127\n",
      "epoch no.3 train no.234950  loss = 2.63171 avg_loss = 3.25324\n",
      "epoch no.3 train no.234960  loss = 2.80518 avg_loss = 3.22973\n",
      "epoch no.3 train no.234970  loss = 1.87679 avg_loss = 3.20709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.234980  loss = 2.77127 avg_loss = 3.20748\n",
      "epoch no.3 train no.234990  loss = 2.05350 avg_loss = 3.15428\n",
      "epoch no.3 train no.235000  loss = 3.53411 avg_loss = 3.13633\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁듣기', '▁듣는', '▁잔잔', '브', '한', '▁음악', '</s>']\n",
      "여름밤에 듣는 그루브한 음악</s>\n",
      "epoch no.3 train no.235010  loss = 3.64729 avg_loss = 3.16849\n",
      "epoch no.3 train no.235020  loss = 3.64553 avg_loss = 3.22497\n",
      "epoch no.3 train no.235030  loss = 4.13399 avg_loss = 3.20606\n",
      "epoch no.3 train no.235040  loss = 4.00579 avg_loss = 3.19183\n",
      "epoch no.3 train no.235050  loss = 4.28939 avg_loss = 3.19024\n",
      "epoch no.3 train no.235060  loss = 4.35837 avg_loss = 3.22452\n",
      "epoch no.3 train no.235070  loss = 2.09209 avg_loss = 3.20451\n",
      "epoch no.3 train no.235080  loss = 2.86240 avg_loss = 3.22473\n",
      "epoch no.3 train no.235090  loss = 3.52169 avg_loss = 3.26146\n",
      "epoch no.3 train no.235100  loss = 3.69932 avg_loss = 3.24896\n",
      "epoch no.3 train no.235110  loss = 2.49330 avg_loss = 3.28173\n",
      "epoch no.3 train no.235120  loss = 3.85351 avg_loss = 3.27860\n",
      "epoch no.3 train no.235130  loss = 0.95314 avg_loss = 3.23840\n",
      "epoch no.3 train no.235140  loss = 5.60324 avg_loss = 3.25074\n",
      "epoch no.3 train no.235150  loss = 2.36879 avg_loss = 3.23716\n",
      "epoch no.3 train no.235160  loss = 1.94071 avg_loss = 3.28234\n",
      "epoch no.3 train no.235170  loss = 2.39954 avg_loss = 3.27697\n",
      "epoch no.3 train no.235180  loss = 2.95028 avg_loss = 3.25395\n",
      "epoch no.3 train no.235190  loss = 2.71452 avg_loss = 3.27068\n",
      "epoch no.3 train no.235200  loss = 2.14919 avg_loss = 3.25101\n",
      "epoch no.3 train no.235210  loss = 3.46265 avg_loss = 3.29759\n",
      "epoch no.3 train no.235220  loss = 5.32443 avg_loss = 3.29723\n",
      "epoch no.3 train no.235230  loss = 2.96306 avg_loss = 3.30493\n",
      "epoch no.3 train no.235240  loss = 3.10907 avg_loss = 3.25418\n",
      "epoch no.3 train no.235250  loss = 3.19523 avg_loss = 3.25728\n",
      "epoch no.3 train no.235260  loss = 2.52860 avg_loss = 3.21663\n",
      "epoch no.3 train no.235270  loss = 2.08570 avg_loss = 3.25999\n",
      "epoch no.3 train no.235280  loss = 2.93584 avg_loss = 3.23361\n",
      "epoch no.3 train no.235290  loss = 3.30695 avg_loss = 3.23361\n",
      "epoch no.3 train no.235300  loss = 2.07376 avg_loss = 3.21967\n",
      "epoch no.3 train no.235310  loss = 2.93208 avg_loss = 3.22889\n",
      "epoch no.3 train no.235320  loss = 4.94375 avg_loss = 3.23702\n",
      "epoch no.3 train no.235330  loss = 3.22946 avg_loss = 3.22249\n",
      "epoch no.3 train no.235340  loss = 2.91156 avg_loss = 3.23769\n",
      "epoch no.3 train no.235350  loss = 2.52128 avg_loss = 3.22812\n",
      "epoch no.3 train no.235360  loss = 3.14897 avg_loss = 3.20623\n",
      "epoch no.3 train no.235370  loss = 1.96525 avg_loss = 3.16024\n",
      "epoch no.3 train no.235380  loss = 2.01304 avg_loss = 3.15334\n",
      "epoch no.3 train no.235390  loss = 3.64441 avg_loss = 3.17217\n",
      "epoch no.3 train no.235400  loss = 3.94255 avg_loss = 3.20544\n",
      "epoch no.3 train no.235410  loss = 4.82586 avg_loss = 3.23606\n",
      "epoch no.3 train no.235420  loss = 2.58427 avg_loss = 3.22768\n",
      "epoch no.3 train no.235430  loss = 4.48425 avg_loss = 3.21091\n",
      "epoch no.3 train no.235440  loss = 3.24121 avg_loss = 3.21539\n",
      "epoch no.3 train no.235450  loss = 3.21561 avg_loss = 3.23644\n",
      "epoch no.3 train no.235460  loss = 4.36536 avg_loss = 3.25142\n",
      "epoch no.3 train no.235470  loss = 2.59264 avg_loss = 3.27797\n",
      "epoch no.3 train no.235480  loss = 3.92597 avg_loss = 3.26150\n",
      "epoch no.3 train no.235490  loss = 3.17640 avg_loss = 3.27458\n",
      "epoch no.3 train no.235500  loss = 3.14138 avg_loss = 3.23919\n",
      "epoch no.3 train no.235510  loss = 3.05174 avg_loss = 3.21512\n",
      "epoch no.3 train no.235520  loss = 5.25556 avg_loss = 3.23305\n",
      "epoch no.3 train no.235530  loss = 2.16665 avg_loss = 3.24464\n",
      "epoch no.3 train no.235540  loss = 4.17254 avg_loss = 3.26391\n",
      "epoch no.3 train no.235550  loss = 5.29368 avg_loss = 3.26711\n",
      "epoch no.3 train no.235560  loss = 3.01810 avg_loss = 3.29384\n",
      "epoch no.3 train no.235570  loss = 3.79752 avg_loss = 3.25959\n",
      "epoch no.3 train no.235580  loss = 2.99977 avg_loss = 3.24058\n",
      "epoch no.3 train no.235590  loss = 3.05290 avg_loss = 3.20837\n",
      "epoch no.3 train no.235600  loss = 3.76790 avg_loss = 3.23966\n",
      "epoch no.3 train no.235610  loss = 3.37685 avg_loss = 3.20010\n",
      "epoch no.3 train no.235620  loss = 3.62989 avg_loss = 3.23563\n",
      "epoch no.3 train no.235630  loss = 3.49057 avg_loss = 3.20350\n",
      "epoch no.3 train no.235640  loss = 2.18629 avg_loss = 3.18972\n",
      "epoch no.3 train no.235650  loss = 2.81453 avg_loss = 3.12626\n",
      "epoch no.3 train no.235660  loss = 3.14935 avg_loss = 3.13449\n",
      "epoch no.3 train no.235670  loss = 3.04598 avg_loss = 3.16603\n",
      "epoch no.3 train no.235680  loss = 2.32931 avg_loss = 3.16814\n",
      "epoch no.3 train no.235690  loss = 2.71356 avg_loss = 3.16359\n",
      "epoch no.3 train no.235700  loss = 2.67911 avg_loss = 3.17719\n",
      "epoch no.3 train no.235710  loss = 4.07388 avg_loss = 3.17885\n",
      "epoch no.3 train no.235720  loss = 5.55129 avg_loss = 3.19885\n",
      "epoch no.3 train no.235730  loss = 3.00491 avg_loss = 3.25404\n",
      "epoch no.3 train no.235740  loss = 2.72716 avg_loss = 3.23789\n",
      "epoch no.3 train no.235750  loss = 2.61828 avg_loss = 3.19897\n",
      "epoch no.3 train no.235760  loss = 3.15810 avg_loss = 3.18844\n",
      "epoch no.3 train no.235770  loss = 2.58722 avg_loss = 3.20279\n",
      "epoch no.3 train no.235780  loss = 4.20351 avg_loss = 3.23048\n",
      "epoch no.3 train no.235790  loss = 2.74075 avg_loss = 3.19330\n",
      "epoch no.3 train no.235800  loss = 2.88223 avg_loss = 3.23233\n",
      "epoch no.3 train no.235810  loss = 3.98374 avg_loss = 3.22693\n",
      "epoch no.3 train no.235820  loss = 3.68841 avg_loss = 3.25003\n",
      "epoch no.3 train no.235830  loss = 3.52901 avg_loss = 3.26716\n",
      "epoch no.3 train no.235840  loss = 3.89043 avg_loss = 3.24894\n",
      "epoch no.3 train no.235850  loss = 4.11501 avg_loss = 3.22402\n",
      "epoch no.3 train no.235860  loss = 3.26145 avg_loss = 3.24794\n",
      "epoch no.3 train no.235870  loss = 3.35285 avg_loss = 3.23582\n",
      "epoch no.3 train no.235880  loss = 3.17759 avg_loss = 3.22447\n",
      "epoch no.3 train no.235890  loss = 3.79444 avg_loss = 3.23528\n",
      "epoch no.3 train no.235900  loss = 4.74098 avg_loss = 3.26673\n",
      "epoch no.3 train no.235910  loss = 2.76600 avg_loss = 3.29854\n",
      "epoch no.3 train no.235920  loss = 2.41528 avg_loss = 3.27637\n",
      "epoch no.3 train no.235930  loss = 3.02947 avg_loss = 3.29783\n",
      "epoch no.3 train no.235940  loss = 2.72739 avg_loss = 3.28067\n",
      "epoch no.3 train no.235950  loss = 3.25334 avg_loss = 3.29929\n",
      "epoch no.3 train no.235960  loss = 3.12291 avg_loss = 3.29145\n",
      "epoch no.3 train no.235970  loss = 2.29362 avg_loss = 3.26578\n",
      "epoch no.3 train no.235980  loss = 3.89995 avg_loss = 3.28400\n",
      "epoch no.3 train no.235990  loss = 3.10330 avg_loss = 3.26777\n",
      "epoch no.3 train no.236000  loss = 4.31540 avg_loss = 3.25864\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁듣기', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.236010  loss = 3.11101 avg_loss = 3.27060\n",
      "epoch no.3 train no.236020  loss = 2.95278 avg_loss = 3.24411\n",
      "epoch no.3 train no.236030  loss = 2.38890 avg_loss = 3.25914\n",
      "epoch no.3 train no.236040  loss = 4.21371 avg_loss = 3.33727\n",
      "epoch no.3 train no.236050  loss = 4.40445 avg_loss = 3.37100\n",
      "epoch no.3 train no.236060  loss = 1.86914 avg_loss = 3.32995\n",
      "epoch no.3 train no.236070  loss = 2.27085 avg_loss = 3.34752\n",
      "epoch no.3 train no.236080  loss = 3.48059 avg_loss = 3.32305\n",
      "epoch no.3 train no.236090  loss = 3.13135 avg_loss = 3.25313\n",
      "epoch no.3 train no.236100  loss = 3.61695 avg_loss = 3.26355\n",
      "epoch no.3 train no.236110  loss = 3.28955 avg_loss = 3.24518\n",
      "epoch no.3 train no.236120  loss = 2.43275 avg_loss = 3.25519\n",
      "epoch no.3 train no.236130  loss = 4.81950 avg_loss = 3.26651\n",
      "epoch no.3 train no.236140  loss = 3.90696 avg_loss = 3.29674\n",
      "epoch no.3 train no.236150  loss = 1.60137 avg_loss = 3.27624\n",
      "epoch no.3 train no.236160  loss = 1.88650 avg_loss = 3.24420\n",
      "epoch no.3 train no.236170  loss = 2.54347 avg_loss = 3.24446\n",
      "epoch no.3 train no.236180  loss = 2.03919 avg_loss = 3.25179\n",
      "epoch no.3 train no.236190  loss = 2.56628 avg_loss = 3.24062\n",
      "epoch no.3 train no.236200  loss = 2.94769 avg_loss = 3.29685\n",
      "epoch no.3 train no.236210  loss = 4.18039 avg_loss = 3.30788\n",
      "epoch no.3 train no.236220  loss = 3.99843 avg_loss = 3.34556\n",
      "epoch no.3 train no.236230  loss = 2.90837 avg_loss = 3.32246\n",
      "epoch no.3 train no.236240  loss = 3.05367 avg_loss = 3.30934\n",
      "epoch no.3 train no.236250  loss = 2.99754 avg_loss = 3.30415\n",
      "epoch no.3 train no.236260  loss = 3.69235 avg_loss = 3.35244\n",
      "epoch no.3 train no.236270  loss = 4.75193 avg_loss = 3.37070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.236280  loss = 3.63596 avg_loss = 3.37649\n",
      "epoch no.3 train no.236290  loss = 3.30505 avg_loss = 3.35899\n",
      "epoch no.3 train no.236300  loss = 3.64115 avg_loss = 3.39614\n",
      "epoch no.3 train no.236310  loss = 2.80113 avg_loss = 3.41682\n",
      "epoch no.3 train no.236320  loss = 1.79870 avg_loss = 3.37210\n",
      "epoch no.3 train no.236330  loss = 1.54943 avg_loss = 3.36164\n",
      "epoch no.3 train no.236340  loss = 4.39920 avg_loss = 3.33632\n",
      "epoch no.3 train no.236350  loss = 2.56290 avg_loss = 3.35618\n",
      "epoch no.3 train no.236360  loss = 3.28778 avg_loss = 3.34754\n",
      "epoch no.3 train no.236370  loss = 2.10744 avg_loss = 3.32480\n",
      "epoch no.3 train no.236380  loss = 3.08149 avg_loss = 3.27450\n",
      "epoch no.3 train no.236390  loss = 2.85338 avg_loss = 3.27984\n",
      "epoch no.3 train no.236400  loss = 2.48801 avg_loss = 3.30387\n",
      "epoch no.3 train no.236410  loss = 1.94497 avg_loss = 3.28491\n",
      "epoch no.3 train no.236420  loss = 3.22138 avg_loss = 3.27368\n",
      "epoch no.3 train no.236430  loss = 3.79063 avg_loss = 3.31884\n",
      "epoch no.3 train no.236440  loss = 2.11780 avg_loss = 3.32492\n",
      "epoch no.3 train no.236450  loss = 2.37873 avg_loss = 3.26895\n",
      "epoch no.3 train no.236460  loss = 3.20718 avg_loss = 3.24883\n",
      "epoch no.3 train no.236470  loss = 2.45108 avg_loss = 3.23842\n",
      "epoch no.3 train no.236480  loss = 2.93918 avg_loss = 3.23309\n",
      "epoch no.3 train no.236490  loss = 3.28476 avg_loss = 3.16809\n",
      "epoch no.3 train no.236500  loss = 3.67831 avg_loss = 3.12064\n",
      "epoch no.3 train no.236510  loss = 4.09764 avg_loss = 3.13400\n",
      "epoch no.3 train no.236520  loss = 3.58116 avg_loss = 3.13586\n",
      "epoch no.3 train no.236530  loss = 2.26755 avg_loss = 3.09915\n",
      "epoch no.3 train no.236540  loss = 4.38840 avg_loss = 3.17064\n",
      "epoch no.3 train no.236550  loss = 2.52999 avg_loss = 3.17279\n",
      "epoch no.3 train no.236560  loss = 3.72924 avg_loss = 3.18975\n",
      "epoch no.3 train no.236570  loss = 5.34858 avg_loss = 3.22326\n",
      "epoch no.3 train no.236580  loss = 2.61899 avg_loss = 3.22503\n",
      "epoch no.3 train no.236590  loss = 2.59480 avg_loss = 3.24451\n",
      "epoch no.3 train no.236600  loss = 3.39506 avg_loss = 3.21683\n",
      "epoch no.3 train no.236610  loss = 2.28378 avg_loss = 3.17785\n",
      "epoch no.3 train no.236620  loss = 3.06925 avg_loss = 3.18503\n",
      "epoch no.3 train no.236630  loss = 3.51432 avg_loss = 3.19580\n",
      "epoch no.3 train no.236640  loss = 4.25129 avg_loss = 3.17050\n",
      "epoch no.3 train no.236650  loss = 2.33171 avg_loss = 3.16674\n",
      "epoch no.3 train no.236660  loss = 4.15401 avg_loss = 3.16942\n",
      "epoch no.3 train no.236670  loss = 5.29032 avg_loss = 3.19878\n",
      "epoch no.3 train no.236680  loss = 4.25744 avg_loss = 3.20829\n",
      "epoch no.3 train no.236690  loss = 3.40814 avg_loss = 3.25678\n",
      "epoch no.3 train no.236700  loss = 4.87680 avg_loss = 3.21943\n",
      "epoch no.3 train no.236710  loss = 4.94937 avg_loss = 3.25635\n",
      "epoch no.3 train no.236720  loss = 3.70411 avg_loss = 3.26177\n",
      "epoch no.3 train no.236730  loss = 5.14590 avg_loss = 3.26859\n",
      "epoch no.3 train no.236740  loss = 3.62149 avg_loss = 3.27488\n",
      "epoch no.3 train no.236750  loss = 3.58492 avg_loss = 3.29683\n",
      "epoch no.3 train no.236760  loss = 2.94908 avg_loss = 3.24355\n",
      "epoch no.3 train no.236770  loss = 3.94256 avg_loss = 3.29131\n",
      "epoch no.3 train no.236780  loss = 2.80584 avg_loss = 3.28826\n",
      "epoch no.3 train no.236790  loss = 2.64379 avg_loss = 3.27724\n",
      "epoch no.3 train no.236800  loss = 2.69731 avg_loss = 3.29708\n",
      "epoch no.3 train no.236810  loss = 4.39328 avg_loss = 3.28587\n",
      "epoch no.3 train no.236820  loss = 3.88577 avg_loss = 3.28671\n",
      "epoch no.3 train no.236830  loss = 2.49119 avg_loss = 3.25349\n",
      "epoch no.3 train no.236840  loss = 2.58325 avg_loss = 3.24097\n",
      "epoch no.3 train no.236850  loss = 4.50591 avg_loss = 3.21046\n",
      "epoch no.3 train no.236860  loss = 2.50100 avg_loss = 3.18931\n",
      "epoch no.3 train no.236870  loss = 3.91247 avg_loss = 3.26504\n",
      "epoch no.3 train no.236880  loss = 5.54847 avg_loss = 3.28091\n",
      "epoch no.3 train no.236890  loss = 5.62283 avg_loss = 3.30266\n",
      "epoch no.3 train no.236900  loss = 2.91672 avg_loss = 3.33535\n",
      "epoch no.3 train no.236910  loss = 4.96656 avg_loss = 3.33373\n",
      "epoch no.3 train no.236920  loss = 2.12354 avg_loss = 3.31188\n",
      "epoch no.3 train no.236930  loss = 3.40955 avg_loss = 3.32804\n",
      "epoch no.3 train no.236940  loss = 3.06239 avg_loss = 3.31257\n",
      "epoch no.3 train no.236950  loss = 2.76053 avg_loss = 3.31540\n",
      "epoch no.3 train no.236960  loss = 2.74124 avg_loss = 3.34230\n",
      "epoch no.3 train no.236970  loss = 2.40008 avg_loss = 3.32498\n",
      "epoch no.3 train no.236980  loss = 1.40274 avg_loss = 3.26522\n",
      "epoch no.3 train no.236990  loss = 4.14796 avg_loss = 3.30184\n",
      "epoch no.3 train no.237000  loss = 3.56002 avg_loss = 3.30665\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '한', '▁노래', '음악']\n",
      "여름밤에 듣는 잔잔한 인디</s>\n",
      "epoch no.3 train no.237010  loss = 1.99646 avg_loss = 3.25105\n",
      "epoch no.3 train no.237020  loss = 4.90380 avg_loss = 3.29692\n",
      "epoch no.3 train no.237030  loss = 2.82827 avg_loss = 3.24199\n",
      "epoch no.3 train no.237040  loss = 2.32053 avg_loss = 3.20294\n",
      "epoch no.3 train no.237050  loss = 2.03898 avg_loss = 3.17467\n",
      "epoch no.3 train no.237060  loss = 2.35645 avg_loss = 3.21936\n",
      "epoch no.3 train no.237070  loss = 2.42185 avg_loss = 3.18677\n",
      "epoch no.3 train no.237080  loss = 3.17414 avg_loss = 3.18925\n",
      "epoch no.3 train no.237090  loss = 2.41954 avg_loss = 3.21914\n",
      "epoch no.3 train no.237100  loss = 1.89200 avg_loss = 3.19433\n",
      "epoch no.3 train no.237110  loss = 2.62140 avg_loss = 3.25262\n",
      "epoch no.3 train no.237120  loss = 2.87199 avg_loss = 3.24101\n",
      "epoch no.3 train no.237130  loss = 2.22229 avg_loss = 3.26209\n",
      "epoch no.3 train no.237140  loss = 2.39759 avg_loss = 3.26085\n",
      "epoch no.3 train no.237150  loss = 3.91459 avg_loss = 3.30907\n",
      "epoch no.3 train no.237160  loss = 2.37945 avg_loss = 3.36717\n",
      "epoch no.3 train no.237170  loss = 2.24170 avg_loss = 3.35059\n",
      "epoch no.3 train no.237180  loss = 4.09225 avg_loss = 3.34038\n",
      "epoch no.3 train no.237190  loss = 1.84820 avg_loss = 3.32191\n",
      "epoch no.3 train no.237200  loss = 4.24108 avg_loss = 3.29264\n",
      "epoch no.3 train no.237210  loss = 3.77421 avg_loss = 3.27347\n",
      "epoch no.3 train no.237220  loss = 3.82404 avg_loss = 3.22107\n",
      "epoch no.3 train no.237230  loss = 3.50557 avg_loss = 3.22637\n",
      "epoch no.3 train no.237240  loss = 2.43840 avg_loss = 3.22919\n",
      "epoch no.3 train no.237250  loss = 4.22867 avg_loss = 3.23170\n",
      "epoch no.3 train no.237260  loss = 3.65180 avg_loss = 3.24287\n",
      "epoch no.3 train no.237270  loss = 5.11226 avg_loss = 3.21639\n",
      "epoch no.3 train no.237280  loss = 2.36848 avg_loss = 3.21344\n",
      "epoch no.3 train no.237290  loss = 2.80524 avg_loss = 3.21852\n",
      "epoch no.3 train no.237300  loss = 2.41890 avg_loss = 3.22923\n",
      "epoch no.3 train no.237310  loss = 2.68844 avg_loss = 3.23256\n",
      "epoch no.3 train no.237320  loss = 4.25042 avg_loss = 3.24798\n",
      "epoch no.3 train no.237330  loss = 3.46036 avg_loss = 3.21311\n",
      "epoch no.3 train no.237340  loss = 3.68128 avg_loss = 3.24517\n",
      "epoch no.3 train no.237350  loss = 3.41292 avg_loss = 3.27344\n",
      "epoch no.3 train no.237360  loss = 3.89057 avg_loss = 3.32656\n",
      "epoch no.3 train no.237370  loss = 2.64455 avg_loss = 3.30738\n",
      "epoch no.3 train no.237380  loss = 5.50510 avg_loss = 3.31828\n",
      "epoch no.3 train no.237390  loss = 4.66933 avg_loss = 3.31849\n",
      "epoch no.3 train no.237400  loss = 2.08514 avg_loss = 3.26664\n",
      "epoch no.3 train no.237410  loss = 5.29902 avg_loss = 3.24521\n",
      "epoch no.3 train no.237420  loss = 3.16971 avg_loss = 3.19056\n",
      "epoch no.3 train no.237430  loss = 2.78303 avg_loss = 3.17963\n",
      "epoch no.3 train no.237440  loss = 2.35761 avg_loss = 3.17009\n",
      "epoch no.3 train no.237450  loss = 3.45505 avg_loss = 3.16305\n",
      "epoch no.3 train no.237460  loss = 2.16439 avg_loss = 3.14697\n",
      "epoch no.3 train no.237470  loss = 3.78781 avg_loss = 3.18765\n",
      "epoch no.3 train no.237480  loss = 3.81704 avg_loss = 3.19219\n",
      "epoch no.3 train no.237490  loss = 3.69077 avg_loss = 3.23189\n",
      "epoch no.3 train no.237500  loss = 2.32104 avg_loss = 3.24151\n",
      "epoch no.3 train no.237510  loss = 2.88654 avg_loss = 3.22831\n",
      "epoch no.3 train no.237520  loss = 4.21122 avg_loss = 3.24969\n",
      "epoch no.3 train no.237530  loss = 2.56027 avg_loss = 3.21912\n",
      "epoch no.3 train no.237540  loss = 3.03394 avg_loss = 3.22711\n",
      "epoch no.3 train no.237550  loss = 2.86517 avg_loss = 3.18236\n",
      "epoch no.3 train no.237560  loss = 3.50040 avg_loss = 3.18270\n",
      "epoch no.3 train no.237570  loss = 3.19492 avg_loss = 3.19849\n",
      "epoch no.3 train no.237580  loss = 2.50140 avg_loss = 3.22441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.237590  loss = 3.72048 avg_loss = 3.22506\n",
      "epoch no.3 train no.237600  loss = 3.14246 avg_loss = 3.23930\n",
      "epoch no.3 train no.237610  loss = 4.37147 avg_loss = 3.22157\n",
      "epoch no.3 train no.237620  loss = 3.03120 avg_loss = 3.19255\n",
      "epoch no.3 train no.237630  loss = 4.56896 avg_loss = 3.20235\n",
      "epoch no.3 train no.237640  loss = 3.02975 avg_loss = 3.18297\n",
      "epoch no.3 train no.237650  loss = 4.67638 avg_loss = 3.22999\n",
      "epoch no.3 train no.237660  loss = 2.42208 avg_loss = 3.28140\n",
      "epoch no.3 train no.237670  loss = 1.89861 avg_loss = 3.28109\n",
      "epoch no.3 train no.237680  loss = 3.14516 avg_loss = 3.27191\n",
      "epoch no.3 train no.237690  loss = 3.07376 avg_loss = 3.26659\n",
      "epoch no.3 train no.237700  loss = 1.99104 avg_loss = 3.24269\n",
      "epoch no.3 train no.237710  loss = 2.13990 avg_loss = 3.22687\n",
      "epoch no.3 train no.237720  loss = 1.96795 avg_loss = 3.21374\n",
      "epoch no.3 train no.237730  loss = 4.27829 avg_loss = 3.24849\n",
      "epoch no.3 train no.237740  loss = 3.32349 avg_loss = 3.23728\n",
      "epoch no.3 train no.237750  loss = 3.50408 avg_loss = 3.22504\n",
      "epoch no.3 train no.237760  loss = 3.71795 avg_loss = 3.29077\n",
      "epoch no.3 train no.237770  loss = 4.46957 avg_loss = 3.37197\n",
      "epoch no.3 train no.237780  loss = 3.21716 avg_loss = 3.33411\n",
      "epoch no.3 train no.237790  loss = 2.71747 avg_loss = 3.33897\n",
      "epoch no.3 train no.237800  loss = 2.27285 avg_loss = 3.31545\n",
      "epoch no.3 train no.237810  loss = 2.50992 avg_loss = 3.28809\n",
      "epoch no.3 train no.237820  loss = 3.69050 avg_loss = 3.30212\n",
      "epoch no.3 train no.237830  loss = 2.90530 avg_loss = 3.28705\n",
      "epoch no.3 train no.237840  loss = 3.05051 avg_loss = 3.24790\n",
      "epoch no.3 train no.237850  loss = 2.55323 avg_loss = 3.23844\n",
      "epoch no.3 train no.237860  loss = 2.57182 avg_loss = 3.28072\n",
      "epoch no.3 train no.237870  loss = 2.23029 avg_loss = 3.28568\n",
      "epoch no.3 train no.237880  loss = 3.01693 avg_loss = 3.29262\n",
      "epoch no.3 train no.237890  loss = 1.99174 avg_loss = 3.24722\n",
      "epoch no.3 train no.237900  loss = 2.90308 avg_loss = 3.27507\n",
      "epoch no.3 train no.237910  loss = 3.94554 avg_loss = 3.26461\n",
      "epoch no.3 train no.237920  loss = 5.70911 avg_loss = 3.27304\n",
      "epoch no.3 train no.237930  loss = 3.16063 avg_loss = 3.30860\n",
      "epoch no.3 train no.237940  loss = 3.58431 avg_loss = 3.33163\n",
      "epoch no.3 train no.237950  loss = 2.21294 avg_loss = 3.34108\n",
      "epoch no.3 train no.237960  loss = 0.91535 avg_loss = 3.28207\n",
      "epoch no.3 train no.237970  loss = 4.13219 avg_loss = 3.27532\n",
      "epoch no.3 train no.237980  loss = 2.27089 avg_loss = 3.28318\n",
      "epoch no.3 train no.237990  loss = 2.52620 avg_loss = 3.27839\n",
      "epoch no.3 train no.238000  loss = 3.33569 avg_loss = 3.25956\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁좋은', '▁노래', '▁노래', '</s>']\n",
      "여름날 듣기좋은 신나는 노래</s>\n",
      "epoch no.3 train no.238010  loss = 3.13668 avg_loss = 3.26286\n",
      "epoch no.3 train no.238020  loss = 2.62074 avg_loss = 3.27907\n",
      "epoch no.3 train no.238030  loss = 3.69414 avg_loss = 3.25546\n",
      "epoch no.3 train no.238040  loss = 4.64175 avg_loss = 3.27068\n",
      "epoch no.3 train no.238050  loss = 2.17659 avg_loss = 3.24314\n",
      "epoch no.3 train no.238060  loss = 4.18324 avg_loss = 3.25387\n",
      "epoch no.3 train no.238070  loss = 2.32186 avg_loss = 3.29299\n",
      "epoch no.3 train no.238080  loss = 3.48225 avg_loss = 3.29774\n",
      "epoch no.3 train no.238090  loss = 2.53019 avg_loss = 3.25848\n",
      "epoch no.3 train no.238100  loss = 2.98178 avg_loss = 3.30300\n",
      "epoch no.3 train no.238110  loss = 3.19026 avg_loss = 3.28446\n",
      "epoch no.3 train no.238120  loss = 2.58329 avg_loss = 3.27791\n",
      "epoch no.3 train no.238130  loss = 2.80758 avg_loss = 3.24772\n",
      "epoch no.3 train no.238140  loss = 2.09936 avg_loss = 3.24716\n",
      "epoch no.3 train no.238150  loss = 2.38458 avg_loss = 3.22344\n",
      "epoch no.3 train no.238160  loss = 2.27170 avg_loss = 3.28141\n",
      "epoch no.3 train no.238170  loss = 3.07337 avg_loss = 3.25436\n",
      "epoch no.3 train no.238180  loss = 2.03932 avg_loss = 3.23237\n",
      "epoch no.3 train no.238190  loss = 4.23937 avg_loss = 3.27720\n",
      "epoch no.3 train no.238200  loss = 2.20770 avg_loss = 3.27447\n",
      "epoch no.3 train no.238210  loss = 3.83937 avg_loss = 3.26605\n",
      "epoch no.3 train no.238220  loss = 3.88230 avg_loss = 3.28792\n",
      "epoch no.3 train no.238230  loss = 2.82352 avg_loss = 3.29675\n",
      "epoch no.3 train no.238240  loss = 4.05036 avg_loss = 3.30128\n",
      "epoch no.3 train no.238250  loss = 4.35110 avg_loss = 3.29694\n",
      "epoch no.3 train no.238260  loss = 2.75728 avg_loss = 3.29863\n",
      "epoch no.3 train no.238270  loss = 4.83121 avg_loss = 3.32987\n",
      "epoch no.3 train no.238280  loss = 5.89830 avg_loss = 3.35955\n",
      "epoch no.3 train no.238290  loss = 2.18672 avg_loss = 3.38093\n",
      "epoch no.3 train no.238300  loss = 5.30325 avg_loss = 3.36390\n",
      "epoch no.3 train no.238310  loss = 4.25322 avg_loss = 3.34936\n",
      "epoch no.3 train no.238320  loss = 3.61769 avg_loss = 3.35118\n",
      "epoch no.3 train no.238330  loss = 2.86242 avg_loss = 3.32235\n",
      "epoch no.3 train no.238340  loss = 2.62833 avg_loss = 3.32640\n",
      "epoch no.3 train no.238350  loss = 3.59203 avg_loss = 3.36564\n",
      "epoch no.3 train no.238360  loss = 2.34998 avg_loss = 3.38573\n",
      "epoch no.3 train no.238370  loss = 4.73187 avg_loss = 3.37288\n",
      "epoch no.3 train no.238380  loss = 2.32052 avg_loss = 3.34892\n",
      "epoch no.3 train no.238390  loss = 3.59773 avg_loss = 3.38927\n",
      "epoch no.3 train no.238400  loss = 7.12518 avg_loss = 3.43944\n",
      "epoch no.3 train no.238410  loss = 3.55739 avg_loss = 3.44959\n",
      "epoch no.3 train no.238420  loss = 2.96394 avg_loss = 3.41544\n",
      "epoch no.3 train no.238430  loss = 3.98418 avg_loss = 3.39883\n",
      "epoch no.3 train no.238440  loss = 1.93892 avg_loss = 3.37640\n",
      "epoch no.3 train no.238450  loss = 2.52729 avg_loss = 3.33211\n",
      "epoch no.3 train no.238460  loss = 3.26544 avg_loss = 3.32527\n",
      "epoch no.3 train no.238470  loss = 4.70952 avg_loss = 3.31985\n",
      "epoch no.3 train no.238480  loss = 2.06374 avg_loss = 3.30567\n",
      "epoch no.3 train no.238490  loss = 2.50819 avg_loss = 3.28029\n",
      "epoch no.3 train no.238500  loss = 3.84175 avg_loss = 3.25778\n",
      "epoch no.3 train no.238510  loss = 3.46857 avg_loss = 3.27486\n",
      "epoch no.3 train no.238520  loss = 2.07700 avg_loss = 3.25630\n",
      "epoch no.3 train no.238530  loss = 3.18201 avg_loss = 3.22134\n",
      "epoch no.3 train no.238540  loss = 2.93595 avg_loss = 3.23798\n",
      "epoch no.3 train no.238550  loss = 2.70532 avg_loss = 3.21312\n",
      "epoch no.3 train no.238560  loss = 2.57705 avg_loss = 3.24031\n",
      "epoch no.3 train no.238570  loss = 3.80670 avg_loss = 3.26001\n",
      "epoch no.3 train no.238580  loss = 2.35418 avg_loss = 3.25965\n",
      "epoch no.3 train no.238590  loss = 3.89864 avg_loss = 3.26604\n",
      "epoch no.3 train no.238600  loss = 3.63098 avg_loss = 3.29921\n",
      "epoch no.3 train no.238610  loss = 4.30057 avg_loss = 3.30372\n",
      "epoch no.3 train no.238620  loss = 3.83308 avg_loss = 3.29950\n",
      "epoch no.3 train no.238630  loss = 4.11794 avg_loss = 3.25567\n",
      "epoch no.3 train no.238640  loss = 1.41742 avg_loss = 3.24784\n",
      "epoch no.3 train no.238650  loss = 3.23582 avg_loss = 3.25905\n",
      "epoch no.3 train no.238660  loss = 3.82625 avg_loss = 3.21492\n",
      "epoch no.3 train no.238670  loss = 2.14435 avg_loss = 3.18341\n",
      "epoch no.3 train no.238680  loss = 3.41087 avg_loss = 3.21435\n",
      "epoch no.3 train no.238690  loss = 3.13996 avg_loss = 3.25323\n",
      "epoch no.3 train no.238700  loss = 1.82313 avg_loss = 3.24324\n",
      "epoch no.3 train no.238710  loss = 2.05820 avg_loss = 3.24000\n",
      "epoch no.3 train no.238720  loss = 3.15808 avg_loss = 3.23569\n",
      "epoch no.3 train no.238730  loss = 3.56311 avg_loss = 3.25247\n",
      "epoch no.3 train no.238740  loss = 3.32537 avg_loss = 3.21796\n",
      "epoch no.3 train no.238750  loss = 2.26966 avg_loss = 3.22625\n",
      "epoch no.3 train no.238760  loss = 3.77886 avg_loss = 3.27080\n",
      "epoch no.3 train no.238770  loss = 3.84670 avg_loss = 3.24276\n",
      "epoch no.3 train no.238780  loss = 1.92856 avg_loss = 3.24951\n",
      "epoch no.3 train no.238790  loss = 3.16341 avg_loss = 3.22326\n",
      "epoch no.3 train no.238800  loss = 2.44587 avg_loss = 3.22116\n",
      "epoch no.3 train no.238810  loss = 3.44200 avg_loss = 3.22389\n",
      "epoch no.3 train no.238820  loss = 3.72549 avg_loss = 3.21263\n",
      "epoch no.3 train no.238830  loss = 3.64244 avg_loss = 3.25979\n",
      "epoch no.3 train no.238840  loss = 2.91941 avg_loss = 3.24363\n",
      "epoch no.3 train no.238850  loss = 1.93482 avg_loss = 3.24512\n",
      "epoch no.3 train no.238860  loss = 3.31183 avg_loss = 3.30127\n",
      "epoch no.3 train no.238870  loss = 2.61371 avg_loss = 3.30368\n",
      "epoch no.3 train no.238880  loss = 3.76444 avg_loss = 3.35885\n",
      "epoch no.3 train no.238890  loss = 5.48580 avg_loss = 3.38487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.238900  loss = 4.05825 avg_loss = 3.35642\n",
      "epoch no.3 train no.238910  loss = 3.29794 avg_loss = 3.33947\n",
      "epoch no.3 train no.238920  loss = 3.22892 avg_loss = 3.35406\n",
      "epoch no.3 train no.238930  loss = 2.72250 avg_loss = 3.31695\n",
      "epoch no.3 train no.238940  loss = 2.67243 avg_loss = 3.27737\n",
      "epoch no.3 train no.238950  loss = 3.47821 avg_loss = 3.31971\n",
      "epoch no.3 train no.238960  loss = 3.76705 avg_loss = 3.31575\n",
      "epoch no.3 train no.238970  loss = 2.98600 avg_loss = 3.23911\n",
      "epoch no.3 train no.238980  loss = 2.95478 avg_loss = 3.25891\n",
      "epoch no.3 train no.238990  loss = 2.76048 avg_loss = 3.27949\n",
      "epoch no.3 train no.239000  loss = 3.75674 avg_loss = 3.38228\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '에서', '▁듣기', '▁좋은', '▁음악', '</s>']\n",
      "여름밤 한강에서 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.239010  loss = 2.65294 avg_loss = 3.36341\n",
      "epoch no.3 train no.239020  loss = 2.88188 avg_loss = 3.29988\n",
      "epoch no.3 train no.239030  loss = 4.84018 avg_loss = 3.33126\n",
      "epoch no.3 train no.239040  loss = 3.91684 avg_loss = 3.39303\n",
      "epoch no.3 train no.239050  loss = 3.45261 avg_loss = 3.43489\n",
      "epoch no.3 train no.239060  loss = 4.00127 avg_loss = 3.41134\n",
      "epoch no.3 train no.239070  loss = 3.14606 avg_loss = 3.43194\n",
      "epoch no.3 train no.239080  loss = 2.49295 avg_loss = 3.39829\n",
      "epoch no.3 train no.239090  loss = 3.14721 avg_loss = 3.36078\n",
      "epoch no.3 train no.239100  loss = 2.49673 avg_loss = 3.37463\n",
      "epoch no.3 train no.239110  loss = 1.69229 avg_loss = 3.39724\n",
      "epoch no.3 train no.239120  loss = 1.95295 avg_loss = 3.35209\n",
      "epoch no.3 train no.239130  loss = 2.79155 avg_loss = 3.33823\n",
      "epoch no.3 train no.239140  loss = 2.83152 avg_loss = 3.32682\n",
      "epoch no.3 train no.239150  loss = 3.36479 avg_loss = 3.32162\n",
      "epoch no.3 train no.239160  loss = 3.88485 avg_loss = 3.32050\n",
      "epoch no.3 train no.239170  loss = 2.35008 avg_loss = 3.27637\n",
      "epoch no.3 train no.239180  loss = 1.92616 avg_loss = 3.28995\n",
      "epoch no.3 train no.239190  loss = 3.81272 avg_loss = 3.30437\n",
      "epoch no.3 train no.239200  loss = 2.66454 avg_loss = 3.32353\n",
      "epoch no.3 train no.239210  loss = 2.39676 avg_loss = 3.28558\n",
      "epoch no.3 train no.239220  loss = 3.40964 avg_loss = 3.25342\n",
      "epoch no.3 train no.239230  loss = 4.75028 avg_loss = 3.23672\n",
      "epoch no.3 train no.239240  loss = 2.19183 avg_loss = 3.19107\n",
      "epoch no.3 train no.239250  loss = 2.78581 avg_loss = 3.16571\n",
      "epoch no.3 train no.239260  loss = 4.73715 avg_loss = 3.19740\n",
      "epoch no.3 train no.239270  loss = 3.25654 avg_loss = 3.13754\n",
      "epoch no.3 train no.239280  loss = 2.25258 avg_loss = 3.11207\n",
      "epoch no.3 train no.239290  loss = 3.32560 avg_loss = 3.15563\n",
      "epoch no.3 train no.239300  loss = 4.64988 avg_loss = 3.18396\n",
      "epoch no.3 train no.239310  loss = 4.12842 avg_loss = 3.18555\n",
      "epoch no.3 train no.239320  loss = 2.33388 avg_loss = 3.17582\n",
      "epoch no.3 train no.239330  loss = 4.39645 avg_loss = 3.18285\n",
      "epoch no.3 train no.239340  loss = 3.70132 avg_loss = 3.17033\n",
      "epoch no.3 train no.239350  loss = 5.45954 avg_loss = 3.20170\n",
      "epoch no.3 train no.239360  loss = 1.87338 avg_loss = 3.20861\n",
      "epoch no.3 train no.239370  loss = 3.34547 avg_loss = 3.21202\n",
      "epoch no.3 train no.239380  loss = 2.31143 avg_loss = 3.20193\n",
      "epoch no.3 train no.239390  loss = 2.76563 avg_loss = 3.23059\n",
      "epoch no.3 train no.239400  loss = 3.16247 avg_loss = 3.27944\n",
      "epoch no.3 train no.239410  loss = 2.31692 avg_loss = 3.28115\n",
      "epoch no.3 train no.239420  loss = 3.31943 avg_loss = 3.29369\n",
      "epoch no.3 train no.239430  loss = 5.82120 avg_loss = 3.34893\n",
      "epoch no.3 train no.239440  loss = 4.29983 avg_loss = 3.31083\n",
      "epoch no.3 train no.239450  loss = 5.49896 avg_loss = 3.33136\n",
      "epoch no.3 train no.239460  loss = 2.88738 avg_loss = 3.30828\n",
      "epoch no.3 train no.239470  loss = 2.65314 avg_loss = 3.28812\n",
      "epoch no.3 train no.239480  loss = 3.21456 avg_loss = 3.26503\n",
      "epoch no.3 train no.239490  loss = 3.18148 avg_loss = 3.28825\n",
      "epoch no.3 train no.239500  loss = 3.10237 avg_loss = 3.25112\n",
      "epoch no.3 train no.239510  loss = 2.60699 avg_loss = 3.29953\n",
      "epoch no.3 train no.239520  loss = 5.00450 avg_loss = 3.31158\n",
      "epoch no.3 train no.239530  loss = 3.27039 avg_loss = 3.30119\n",
      "epoch no.3 train no.239540  loss = 4.04626 avg_loss = 3.31418\n",
      "epoch no.3 train no.239550  loss = 3.94797 avg_loss = 3.27090\n",
      "epoch no.3 train no.239560  loss = 2.94589 avg_loss = 3.22975\n",
      "epoch no.3 train no.239570  loss = 3.33790 avg_loss = 3.21821\n",
      "epoch no.3 train no.239580  loss = 2.27487 avg_loss = 3.19130\n",
      "epoch no.3 train no.239590  loss = 3.39187 avg_loss = 3.24647\n",
      "epoch no.3 train no.239600  loss = 3.47093 avg_loss = 3.20690\n",
      "epoch no.3 train no.239610  loss = 2.64660 avg_loss = 3.23430\n",
      "epoch no.3 train no.239620  loss = 2.65306 avg_loss = 3.18939\n",
      "epoch no.3 train no.239630  loss = 2.39977 avg_loss = 3.18595\n",
      "epoch no.3 train no.239640  loss = 3.74402 avg_loss = 3.19349\n",
      "epoch no.3 train no.239650  loss = 1.21457 avg_loss = 3.19311\n",
      "epoch no.3 train no.239660  loss = 3.36878 avg_loss = 3.18264\n",
      "epoch no.3 train no.239670  loss = 2.69233 avg_loss = 3.21998\n",
      "epoch no.3 train no.239680  loss = 2.52103 avg_loss = 3.18401\n",
      "epoch no.3 train no.239690  loss = 2.70766 avg_loss = 3.22372\n",
      "epoch no.3 train no.239700  loss = 4.55168 avg_loss = 3.22007\n",
      "epoch no.3 train no.239710  loss = 2.00029 avg_loss = 3.21602\n",
      "epoch no.3 train no.239720  loss = 3.11966 avg_loss = 3.21286\n",
      "epoch no.3 train no.239730  loss = 3.67657 avg_loss = 3.20402\n",
      "epoch no.3 train no.239740  loss = 1.99137 avg_loss = 3.21522\n",
      "epoch no.3 train no.239750  loss = 4.37340 avg_loss = 3.25723\n",
      "epoch no.3 train no.239760  loss = 1.48972 avg_loss = 3.26462\n",
      "epoch no.3 train no.239770  loss = 2.52665 avg_loss = 3.23813\n",
      "epoch no.3 train no.239780  loss = 3.55324 avg_loss = 3.27459\n",
      "epoch no.3 train no.239790  loss = 3.66759 avg_loss = 3.26776\n",
      "epoch no.3 train no.239800  loss = 4.00094 avg_loss = 3.27112\n",
      "epoch no.3 train no.239810  loss = 2.89755 avg_loss = 3.25914\n",
      "epoch no.3 train no.239820  loss = 7.31880 avg_loss = 3.29965\n",
      "epoch no.3 train no.239830  loss = 3.33540 avg_loss = 3.29833\n",
      "epoch no.3 train no.239840  loss = 3.40123 avg_loss = 3.27826\n",
      "epoch no.3 train no.239850  loss = 2.98056 avg_loss = 3.31129\n",
      "epoch no.3 train no.239860  loss = 2.57052 avg_loss = 3.31151\n",
      "epoch no.3 train no.239870  loss = 3.05528 avg_loss = 3.32114\n",
      "epoch no.3 train no.239880  loss = 4.05853 avg_loss = 3.34166\n",
      "epoch no.3 train no.239890  loss = 2.58437 avg_loss = 3.35282\n",
      "epoch no.3 train no.239900  loss = 2.31954 avg_loss = 3.34626\n",
      "epoch no.3 train no.239910  loss = 4.44637 avg_loss = 3.40336\n",
      "epoch no.3 train no.239920  loss = 3.45109 avg_loss = 3.39707\n",
      "epoch no.3 train no.239930  loss = 2.47872 avg_loss = 3.31191\n",
      "epoch no.3 train no.239940  loss = 3.16445 avg_loss = 3.32318\n",
      "epoch no.3 train no.239950  loss = 3.44551 avg_loss = 3.31688\n",
      "epoch no.3 train no.239960  loss = 4.44059 avg_loss = 3.31727\n",
      "epoch no.3 train no.239970  loss = 3.52810 avg_loss = 3.30763\n",
      "epoch no.3 train no.239980  loss = 2.64053 avg_loss = 3.28344\n",
      "epoch no.3 train no.239990  loss = 3.46095 avg_loss = 3.29689\n",
      "epoch no.3 train no.240000  loss = 4.60163 avg_loss = 3.34813\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁잔잔', '보', '</s>', '</s>']\n",
      "여름밤의 재즈바에서</s>\n",
      "epoch no.3 train no.240010  loss = 3.23261 avg_loss = 3.37283\n",
      "epoch no.3 train no.240020  loss = 2.90844 avg_loss = 3.35219\n",
      "epoch no.3 train no.240030  loss = 3.71758 avg_loss = 3.32587\n",
      "epoch no.3 train no.240040  loss = 4.25866 avg_loss = 3.33428\n",
      "epoch no.3 train no.240050  loss = 3.63044 avg_loss = 3.28920\n",
      "epoch no.3 train no.240060  loss = 2.49417 avg_loss = 3.29118\n",
      "epoch no.3 train no.240070  loss = 2.54810 avg_loss = 3.25784\n",
      "epoch no.3 train no.240080  loss = 2.39311 avg_loss = 3.27434\n",
      "epoch no.3 train no.240090  loss = 3.99536 avg_loss = 3.27335\n",
      "epoch no.3 train no.240100  loss = 2.66511 avg_loss = 3.24804\n",
      "epoch no.3 train no.240110  loss = 3.62112 avg_loss = 3.27592\n",
      "epoch no.3 train no.240120  loss = 2.55091 avg_loss = 3.26885\n",
      "epoch no.3 train no.240130  loss = 2.16039 avg_loss = 3.21154\n",
      "epoch no.3 train no.240140  loss = 3.03218 avg_loss = 3.15642\n",
      "epoch no.3 train no.240150  loss = 3.28788 avg_loss = 3.16118\n",
      "epoch no.3 train no.240160  loss = 3.51650 avg_loss = 3.13535\n",
      "epoch no.3 train no.240170  loss = 3.87339 avg_loss = 3.14056\n",
      "epoch no.3 train no.240180  loss = 2.56326 avg_loss = 3.15286\n",
      "epoch no.3 train no.240190  loss = 3.38890 avg_loss = 3.16493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.240200  loss = 4.84428 avg_loss = 3.16058\n",
      "epoch no.3 train no.240210  loss = 6.63033 avg_loss = 3.18837\n",
      "epoch no.3 train no.240220  loss = 3.01946 avg_loss = 3.16433\n",
      "epoch no.3 train no.240230  loss = 2.91779 avg_loss = 3.17961\n",
      "epoch no.3 train no.240240  loss = 2.83862 avg_loss = 3.16077\n",
      "epoch no.3 train no.240250  loss = 4.31999 avg_loss = 3.21527\n",
      "epoch no.3 train no.240260  loss = 3.33113 avg_loss = 3.22768\n",
      "epoch no.3 train no.240270  loss = 3.63763 avg_loss = 3.22717\n",
      "epoch no.3 train no.240280  loss = 5.72588 avg_loss = 3.24036\n",
      "epoch no.3 train no.240290  loss = 2.29478 avg_loss = 3.22962\n",
      "epoch no.3 train no.240300  loss = 3.59896 avg_loss = 3.25914\n",
      "epoch no.3 train no.240310  loss = 3.24574 avg_loss = 3.24861\n",
      "epoch no.3 train no.240320  loss = 2.96035 avg_loss = 3.25191\n",
      "epoch no.3 train no.240330  loss = 4.28984 avg_loss = 3.29032\n",
      "epoch no.3 train no.240340  loss = 1.40962 avg_loss = 3.24872\n",
      "epoch no.3 train no.240350  loss = 2.15352 avg_loss = 3.22355\n",
      "epoch no.3 train no.240360  loss = 2.50275 avg_loss = 3.21673\n",
      "epoch no.3 train no.240370  loss = 3.75555 avg_loss = 3.20199\n",
      "epoch no.3 train no.240380  loss = 4.34954 avg_loss = 3.19553\n",
      "epoch no.3 train no.240390  loss = 2.58569 avg_loss = 3.23014\n",
      "epoch no.3 train no.240400  loss = 3.81223 avg_loss = 3.21687\n",
      "epoch no.3 train no.240410  loss = 3.66523 avg_loss = 3.23310\n",
      "epoch no.3 train no.240420  loss = 2.81840 avg_loss = 3.21816\n",
      "epoch no.3 train no.240430  loss = 4.02777 avg_loss = 3.28821\n",
      "epoch no.3 train no.240440  loss = 2.84386 avg_loss = 3.31264\n",
      "epoch no.3 train no.240450  loss = 3.93087 avg_loss = 3.32363\n",
      "epoch no.3 train no.240460  loss = 2.50478 avg_loss = 3.31695\n",
      "epoch no.3 train no.240470  loss = 3.40376 avg_loss = 3.31837\n",
      "epoch no.3 train no.240480  loss = 2.45941 avg_loss = 3.31608\n",
      "epoch no.3 train no.240490  loss = 4.98709 avg_loss = 3.36463\n",
      "epoch no.3 train no.240500  loss = 2.34872 avg_loss = 3.35461\n",
      "epoch no.3 train no.240510  loss = 2.99936 avg_loss = 3.33883\n",
      "epoch no.3 train no.240520  loss = 3.47163 avg_loss = 3.33058\n",
      "epoch no.3 train no.240530  loss = 4.08296 avg_loss = 3.33190\n",
      "epoch no.3 train no.240540  loss = 3.87597 avg_loss = 3.32740\n",
      "epoch no.3 train no.240550  loss = 1.68278 avg_loss = 3.32504\n",
      "epoch no.3 train no.240560  loss = 3.39372 avg_loss = 3.33846\n",
      "epoch no.3 train no.240570  loss = 3.16333 avg_loss = 3.36303\n",
      "epoch no.3 train no.240580  loss = 3.57904 avg_loss = 3.34923\n",
      "epoch no.3 train no.240590  loss = 3.81674 avg_loss = 3.36669\n",
      "epoch no.3 train no.240600  loss = 3.27927 avg_loss = 3.33188\n",
      "epoch no.3 train no.240610  loss = 1.46909 avg_loss = 3.33970\n",
      "epoch no.3 train no.240620  loss = 2.70366 avg_loss = 3.32707\n",
      "epoch no.3 train no.240630  loss = 2.73339 avg_loss = 3.35813\n",
      "epoch no.3 train no.240640  loss = 3.44888 avg_loss = 3.35777\n",
      "epoch no.3 train no.240650  loss = 7.17791 avg_loss = 3.39654\n",
      "epoch no.3 train no.240660  loss = 2.36786 avg_loss = 3.37410\n",
      "epoch no.3 train no.240670  loss = 2.32438 avg_loss = 3.34038\n",
      "epoch no.3 train no.240680  loss = 2.82117 avg_loss = 3.33447\n",
      "epoch no.3 train no.240690  loss = 3.60083 avg_loss = 3.35756\n",
      "epoch no.3 train no.240700  loss = 2.90932 avg_loss = 3.31745\n",
      "epoch no.3 train no.240710  loss = 2.77250 avg_loss = 3.32364\n",
      "epoch no.3 train no.240720  loss = 2.08941 avg_loss = 3.30690\n",
      "epoch no.3 train no.240730  loss = 2.14550 avg_loss = 3.26724\n",
      "epoch no.3 train no.240740  loss = 3.03885 avg_loss = 3.23096\n",
      "epoch no.3 train no.240750  loss = 2.57160 avg_loss = 3.21021\n",
      "epoch no.3 train no.240760  loss = 2.21380 avg_loss = 3.19030\n",
      "epoch no.3 train no.240770  loss = 3.93967 avg_loss = 3.21130\n",
      "epoch no.3 train no.240780  loss = 3.61929 avg_loss = 3.19211\n",
      "epoch no.3 train no.240790  loss = 2.73420 avg_loss = 3.20478\n",
      "epoch no.3 train no.240800  loss = 2.57134 avg_loss = 3.21125\n",
      "epoch no.3 train no.240810  loss = 1.35681 avg_loss = 3.19488\n",
      "epoch no.3 train no.240820  loss = 3.31696 avg_loss = 3.20044\n",
      "epoch no.3 train no.240830  loss = 4.57899 avg_loss = 3.20026\n",
      "epoch no.3 train no.240840  loss = 3.32103 avg_loss = 3.17983\n",
      "epoch no.3 train no.240850  loss = 2.20963 avg_loss = 3.17979\n",
      "epoch no.3 train no.240860  loss = 3.24204 avg_loss = 3.23158\n",
      "epoch no.3 train no.240870  loss = 3.43119 avg_loss = 3.28540\n",
      "epoch no.3 train no.240880  loss = 2.86081 avg_loss = 3.27912\n",
      "epoch no.3 train no.240890  loss = 6.22922 avg_loss = 3.29017\n",
      "epoch no.3 train no.240900  loss = 3.74488 avg_loss = 3.28767\n",
      "epoch no.3 train no.240910  loss = 4.54581 avg_loss = 3.27263\n",
      "epoch no.3 train no.240920  loss = 3.32207 avg_loss = 3.30284\n",
      "epoch no.3 train no.240930  loss = 2.53008 avg_loss = 3.31705\n",
      "epoch no.3 train no.240940  loss = 1.98560 avg_loss = 3.33601\n",
      "epoch no.3 train no.240950  loss = 1.89031 avg_loss = 3.31053\n",
      "epoch no.3 train no.240960  loss = 3.61876 avg_loss = 3.23835\n",
      "epoch no.3 train no.240970  loss = 3.08248 avg_loss = 3.25001\n",
      "epoch no.3 train no.240980  loss = 2.93392 avg_loss = 3.28074\n",
      "epoch no.3 train no.240990  loss = 2.35002 avg_loss = 3.29089\n",
      "epoch no.3 train no.241000  loss = 3.82734 avg_loss = 3.26382\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁잔잔', '에서', '▁듣기', '▁싶은', '은', '▁노래', '</s>']\n",
      "여름밤 한강에서 듣고싶은 노래</s>\n",
      "epoch no.3 train no.241010  loss = 3.51101 avg_loss = 3.28459\n",
      "epoch no.3 train no.241020  loss = 3.16704 avg_loss = 3.33418\n",
      "epoch no.3 train no.241030  loss = 3.37316 avg_loss = 3.30175\n",
      "epoch no.3 train no.241040  loss = 3.35881 avg_loss = 3.35441\n",
      "epoch no.3 train no.241050  loss = 3.59254 avg_loss = 3.35649\n",
      "epoch no.3 train no.241060  loss = 4.55649 avg_loss = 3.45784\n",
      "epoch no.3 train no.241070  loss = 1.56068 avg_loss = 3.41406\n",
      "epoch no.3 train no.241080  loss = 2.96970 avg_loss = 3.42302\n",
      "epoch no.3 train no.241090  loss = 4.00656 avg_loss = 3.38494\n",
      "epoch no.3 train no.241100  loss = 4.28953 avg_loss = 3.43125\n",
      "epoch no.3 train no.241110  loss = 4.08375 avg_loss = 3.44293\n",
      "epoch no.3 train no.241120  loss = 2.71196 avg_loss = 3.44546\n",
      "epoch no.3 train no.241130  loss = 3.15272 avg_loss = 3.41926\n",
      "epoch no.3 train no.241140  loss = 1.37100 avg_loss = 3.35754\n",
      "epoch no.3 train no.241150  loss = 3.90233 avg_loss = 3.39914\n",
      "epoch no.3 train no.241160  loss = 3.55662 avg_loss = 3.44581\n",
      "epoch no.3 train no.241170  loss = 2.51407 avg_loss = 3.43610\n",
      "epoch no.3 train no.241180  loss = 3.17080 avg_loss = 3.45230\n",
      "epoch no.3 train no.241190  loss = 4.32267 avg_loss = 3.42493\n",
      "epoch no.3 train no.241200  loss = 2.47128 avg_loss = 3.42595\n",
      "epoch no.3 train no.241210  loss = 3.26820 avg_loss = 3.43304\n",
      "epoch no.3 train no.241220  loss = 3.14006 avg_loss = 3.44324\n",
      "epoch no.3 train no.241230  loss = 3.56058 avg_loss = 3.47837\n",
      "epoch no.3 train no.241240  loss = 2.71247 avg_loss = 3.44192\n",
      "epoch no.3 train no.241250  loss = 2.06069 avg_loss = 3.42065\n",
      "epoch no.3 train no.241260  loss = 1.91755 avg_loss = 3.43642\n",
      "epoch no.3 train no.241270  loss = 3.15126 avg_loss = 3.38837\n",
      "epoch no.3 train no.241280  loss = 3.24878 avg_loss = 3.41037\n",
      "epoch no.3 train no.241290  loss = 3.57491 avg_loss = 3.37030\n",
      "epoch no.3 train no.241300  loss = 2.13894 avg_loss = 3.36987\n",
      "epoch no.3 train no.241310  loss = 5.08883 avg_loss = 3.38217\n",
      "epoch no.3 train no.241320  loss = 5.13581 avg_loss = 3.40013\n",
      "epoch no.3 train no.241330  loss = 3.10153 avg_loss = 3.44105\n",
      "epoch no.3 train no.241340  loss = 1.89914 avg_loss = 3.42085\n",
      "epoch no.3 train no.241350  loss = 2.72034 avg_loss = 3.41426\n",
      "epoch no.3 train no.241360  loss = 2.17657 avg_loss = 3.39594\n",
      "epoch no.3 train no.241370  loss = 2.44756 avg_loss = 3.36192\n",
      "epoch no.3 train no.241380  loss = 4.57441 avg_loss = 3.36340\n",
      "epoch no.3 train no.241390  loss = 3.98812 avg_loss = 3.37936\n",
      "epoch no.3 train no.241400  loss = 2.80684 avg_loss = 3.37311\n",
      "epoch no.3 train no.241410  loss = 3.20737 avg_loss = 3.32291\n",
      "epoch no.3 train no.241420  loss = 2.62328 avg_loss = 3.31725\n",
      "epoch no.3 train no.241430  loss = 3.89458 avg_loss = 3.27717\n",
      "epoch no.3 train no.241440  loss = 3.47732 avg_loss = 3.29737\n",
      "epoch no.3 train no.241450  loss = 2.58072 avg_loss = 3.33033\n",
      "epoch no.3 train no.241460  loss = 3.52536 avg_loss = 3.38312\n",
      "epoch no.3 train no.241470  loss = 3.50653 avg_loss = 3.38258\n",
      "epoch no.3 train no.241480  loss = 4.32871 avg_loss = 3.43301\n",
      "epoch no.3 train no.241490  loss = 2.80223 avg_loss = 3.42705\n",
      "epoch no.3 train no.241500  loss = 2.99894 avg_loss = 3.44251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.241510  loss = 2.47153 avg_loss = 3.40239\n",
      "epoch no.3 train no.241520  loss = 4.68451 avg_loss = 3.37940\n",
      "epoch no.3 train no.241530  loss = 2.15084 avg_loss = 3.32801\n",
      "epoch no.3 train no.241540  loss = 5.39447 avg_loss = 3.31627\n",
      "epoch no.3 train no.241550  loss = 2.01794 avg_loss = 3.27505\n",
      "epoch no.3 train no.241560  loss = 4.72637 avg_loss = 3.35900\n",
      "epoch no.3 train no.241570  loss = 3.89625 avg_loss = 3.36744\n",
      "epoch no.3 train no.241580  loss = 3.16208 avg_loss = 3.34323\n",
      "epoch no.3 train no.241590  loss = 5.23705 avg_loss = 3.34013\n",
      "epoch no.3 train no.241600  loss = 2.74404 avg_loss = 3.34442\n",
      "epoch no.3 train no.241610  loss = 3.54014 avg_loss = 3.39292\n",
      "epoch no.3 train no.241620  loss = 3.56999 avg_loss = 3.36325\n",
      "epoch no.3 train no.241630  loss = 3.81484 avg_loss = 3.32479\n",
      "epoch no.3 train no.241640  loss = 3.66914 avg_loss = 3.36571\n",
      "epoch no.3 train no.241650  loss = 3.01961 avg_loss = 3.36536\n",
      "epoch no.3 train no.241660  loss = 3.48462 avg_loss = 3.32787\n",
      "epoch no.3 train no.241670  loss = 2.78179 avg_loss = 3.30921\n",
      "epoch no.3 train no.241680  loss = 3.16006 avg_loss = 3.31019\n",
      "epoch no.3 train no.241690  loss = 2.70378 avg_loss = 3.30319\n",
      "epoch no.3 train no.241700  loss = 4.00143 avg_loss = 3.29394\n",
      "epoch no.3 train no.241710  loss = 2.94720 avg_loss = 3.26967\n",
      "epoch no.3 train no.241720  loss = 3.14879 avg_loss = 3.25986\n",
      "epoch no.3 train no.241730  loss = 3.06152 avg_loss = 3.27021\n",
      "epoch no.3 train no.241740  loss = 5.28410 avg_loss = 3.26689\n",
      "epoch no.3 train no.241750  loss = 2.92454 avg_loss = 3.25540\n",
      "epoch no.3 train no.241760  loss = 3.50475 avg_loss = 3.23698\n",
      "epoch no.3 train no.241770  loss = 2.88107 avg_loss = 3.19993\n",
      "epoch no.3 train no.241780  loss = 3.89725 avg_loss = 3.22449\n",
      "epoch no.3 train no.241790  loss = 3.68542 avg_loss = 3.26655\n",
      "epoch no.3 train no.241800  loss = 3.36717 avg_loss = 3.26058\n",
      "epoch no.3 train no.241810  loss = 5.02402 avg_loss = 3.28341\n",
      "epoch no.3 train no.241820  loss = 4.65137 avg_loss = 3.29344\n",
      "epoch no.3 train no.241830  loss = 3.72958 avg_loss = 3.29656\n",
      "epoch no.3 train no.241840  loss = 2.45777 avg_loss = 3.29429\n",
      "epoch no.3 train no.241850  loss = 2.17886 avg_loss = 3.30422\n",
      "epoch no.3 train no.241860  loss = 5.73068 avg_loss = 3.31133\n",
      "epoch no.3 train no.241870  loss = 3.38878 avg_loss = 3.27110\n",
      "epoch no.3 train no.241880  loss = 2.34234 avg_loss = 3.27203\n",
      "epoch no.3 train no.241890  loss = 4.38751 avg_loss = 3.30442\n",
      "epoch no.3 train no.241900  loss = 5.33986 avg_loss = 3.32676\n",
      "epoch no.3 train no.241910  loss = 3.48649 avg_loss = 3.33656\n",
      "epoch no.3 train no.241920  loss = 3.46970 avg_loss = 3.29066\n",
      "epoch no.3 train no.241930  loss = 4.16486 avg_loss = 3.27882\n",
      "epoch no.3 train no.241940  loss = 3.85490 avg_loss = 3.26155\n",
      "epoch no.3 train no.241950  loss = 4.19091 avg_loss = 3.29224\n",
      "epoch no.3 train no.241960  loss = 2.75640 avg_loss = 3.29629\n",
      "epoch no.3 train no.241970  loss = 3.34381 avg_loss = 3.29998\n",
      "epoch no.3 train no.241980  loss = 4.04682 avg_loss = 3.34253\n",
      "epoch no.3 train no.241990  loss = 2.68092 avg_loss = 3.38786\n",
      "epoch no.3 train no.242000  loss = 3.75307 avg_loss = 3.37352\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣는', '▁잔잔', '적인', '음악', '</s>']\n",
      "여름밤에 듣는 감성 인디음악</s>\n",
      "epoch no.3 train no.242010  loss = 3.04154 avg_loss = 3.41450\n",
      "epoch no.3 train no.242020  loss = 4.05658 avg_loss = 3.37588\n",
      "epoch no.3 train no.242030  loss = 3.23552 avg_loss = 3.35727\n",
      "epoch no.3 train no.242040  loss = 3.44396 avg_loss = 3.28249\n",
      "epoch no.3 train no.242050  loss = 2.40914 avg_loss = 3.32190\n",
      "epoch no.3 train no.242060  loss = 2.63613 avg_loss = 3.28664\n",
      "epoch no.3 train no.242070  loss = 2.45524 avg_loss = 3.32975\n",
      "epoch no.3 train no.242080  loss = 1.53368 avg_loss = 3.32991\n",
      "epoch no.3 train no.242090  loss = 3.29046 avg_loss = 3.30026\n",
      "epoch no.3 train no.242100  loss = 4.24430 avg_loss = 3.33015\n",
      "epoch no.3 train no.242110  loss = 1.77821 avg_loss = 3.33763\n",
      "epoch no.3 train no.242120  loss = 3.18641 avg_loss = 3.33962\n",
      "epoch no.3 train no.242130  loss = 4.69905 avg_loss = 3.36330\n",
      "epoch no.3 train no.242140  loss = 3.47572 avg_loss = 3.38850\n",
      "epoch no.3 train no.242150  loss = 2.34566 avg_loss = 3.42206\n",
      "epoch no.3 train no.242160  loss = 1.97481 avg_loss = 3.39897\n",
      "epoch no.3 train no.242170  loss = 1.66114 avg_loss = 3.40306\n",
      "epoch no.3 train no.242180  loss = 2.28012 avg_loss = 3.38159\n",
      "epoch no.3 train no.242190  loss = 3.55452 avg_loss = 3.38540\n",
      "epoch no.3 train no.242200  loss = 2.51235 avg_loss = 3.36720\n",
      "epoch no.3 train no.242210  loss = 4.31278 avg_loss = 3.37812\n",
      "epoch no.3 train no.242220  loss = 3.04134 avg_loss = 3.33893\n",
      "epoch no.3 train no.242230  loss = 2.88371 avg_loss = 3.29669\n",
      "epoch no.3 train no.242240  loss = 2.56148 avg_loss = 3.28804\n",
      "epoch no.3 train no.242250  loss = 4.17914 avg_loss = 3.29421\n",
      "epoch no.3 train no.242260  loss = 1.36925 avg_loss = 3.27590\n",
      "epoch no.3 train no.242270  loss = 2.00487 avg_loss = 3.25373\n",
      "epoch no.3 train no.242280  loss = 2.59621 avg_loss = 3.25535\n",
      "epoch no.3 train no.242290  loss = 2.80758 avg_loss = 3.31430\n",
      "epoch no.3 train no.242300  loss = 2.58075 avg_loss = 3.28556\n",
      "epoch no.3 train no.242310  loss = 3.22841 avg_loss = 3.28625\n",
      "epoch no.3 train no.242320  loss = 4.47029 avg_loss = 3.26921\n",
      "epoch no.3 train no.242330  loss = 2.04553 avg_loss = 3.26951\n",
      "epoch no.3 train no.242340  loss = 2.30353 avg_loss = 3.30652\n",
      "epoch no.3 train no.242350  loss = 2.87688 avg_loss = 3.28708\n",
      "epoch no.3 train no.242360  loss = 2.25020 avg_loss = 3.29178\n",
      "epoch no.3 train no.242370  loss = 3.44786 avg_loss = 3.33705\n",
      "epoch no.3 train no.242380  loss = 4.89128 avg_loss = 3.40265\n",
      "epoch no.3 train no.242390  loss = 6.51368 avg_loss = 3.38755\n",
      "epoch no.3 train no.242400  loss = 2.33883 avg_loss = 3.43550\n",
      "epoch no.3 train no.242410  loss = 2.15772 avg_loss = 3.44068\n",
      "epoch no.3 train no.242420  loss = 2.29222 avg_loss = 3.41665\n",
      "epoch no.3 train no.242430  loss = 3.11583 avg_loss = 3.46520\n",
      "epoch no.3 train no.242440  loss = 1.77023 avg_loss = 3.50868\n",
      "epoch no.3 train no.242450  loss = 4.31229 avg_loss = 3.49276\n",
      "epoch no.3 train no.242460  loss = 3.35164 avg_loss = 3.45288\n",
      "epoch no.3 train no.242470  loss = 2.54431 avg_loss = 3.43754\n",
      "epoch no.3 train no.242480  loss = 2.82298 avg_loss = 3.42382\n",
      "epoch no.3 train no.242490  loss = 3.79702 avg_loss = 3.37637\n",
      "epoch no.3 train no.242500  loss = 2.28659 avg_loss = 3.36085\n",
      "epoch no.3 train no.242510  loss = 3.28352 avg_loss = 3.33861\n",
      "epoch no.3 train no.242520  loss = 3.33480 avg_loss = 3.33246\n",
      "epoch no.3 train no.242530  loss = 4.29247 avg_loss = 3.31085\n",
      "epoch no.3 train no.242540  loss = 1.93606 avg_loss = 3.35474\n",
      "epoch no.3 train no.242550  loss = 2.94255 avg_loss = 3.35688\n",
      "epoch no.3 train no.242560  loss = 1.79718 avg_loss = 3.31938\n",
      "epoch no.3 train no.242570  loss = 3.41091 avg_loss = 3.28900\n",
      "epoch no.3 train no.242580  loss = 3.72215 avg_loss = 3.32674\n",
      "epoch no.3 train no.242590  loss = 4.54443 avg_loss = 3.38217\n",
      "epoch no.3 train no.242600  loss = 3.13413 avg_loss = 3.38154\n",
      "epoch no.3 train no.242610  loss = 2.13432 avg_loss = 3.33269\n",
      "epoch no.3 train no.242620  loss = 3.05608 avg_loss = 3.35474\n",
      "epoch no.3 train no.242630  loss = 3.94755 avg_loss = 3.36861\n",
      "epoch no.3 train no.242640  loss = 2.98783 avg_loss = 3.36159\n",
      "epoch no.3 train no.242650  loss = 2.57819 avg_loss = 3.34990\n",
      "epoch no.3 train no.242660  loss = 2.09627 avg_loss = 3.30589\n",
      "epoch no.3 train no.242670  loss = 4.60714 avg_loss = 3.30988\n",
      "epoch no.3 train no.242680  loss = 3.99858 avg_loss = 3.26348\n",
      "epoch no.3 train no.242690  loss = 2.19587 avg_loss = 3.25223\n",
      "epoch no.3 train no.242700  loss = 4.59910 avg_loss = 3.26941\n",
      "epoch no.3 train no.242710  loss = 2.83611 avg_loss = 3.29780\n",
      "epoch no.3 train no.242720  loss = 2.94544 avg_loss = 3.23998\n",
      "epoch no.3 train no.242730  loss = 4.32690 avg_loss = 3.28701\n",
      "epoch no.3 train no.242740  loss = 3.05561 avg_loss = 3.27933\n",
      "epoch no.3 train no.242750  loss = 3.76777 avg_loss = 3.31783\n",
      "epoch no.3 train no.242760  loss = 2.85761 avg_loss = 3.38433\n",
      "epoch no.3 train no.242770  loss = 3.08546 avg_loss = 3.38188\n",
      "epoch no.3 train no.242780  loss = 3.12843 avg_loss = 3.39915\n",
      "epoch no.3 train no.242790  loss = 3.32746 avg_loss = 3.36910\n",
      "epoch no.3 train no.242800  loss = 2.84291 avg_loss = 3.33464\n",
      "epoch no.3 train no.242810  loss = 3.47092 avg_loss = 3.32383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.242820  loss = 2.98752 avg_loss = 3.32846\n",
      "epoch no.3 train no.242830  loss = 2.16780 avg_loss = 3.31097\n",
      "epoch no.3 train no.242840  loss = 2.01661 avg_loss = 3.30891\n",
      "epoch no.3 train no.242850  loss = 3.48075 avg_loss = 3.26408\n",
      "epoch no.3 train no.242860  loss = 4.18473 avg_loss = 3.27391\n",
      "epoch no.3 train no.242870  loss = 4.11763 avg_loss = 3.27366\n",
      "epoch no.3 train no.242880  loss = 1.57066 avg_loss = 3.25577\n",
      "epoch no.3 train no.242890  loss = 2.15789 avg_loss = 3.26570\n",
      "epoch no.3 train no.242900  loss = 3.35973 avg_loss = 3.31781\n",
      "epoch no.3 train no.242910  loss = 2.31865 avg_loss = 3.30977\n",
      "epoch no.3 train no.242920  loss = 2.55485 avg_loss = 3.29294\n",
      "epoch no.3 train no.242930  loss = 4.96375 avg_loss = 3.30976\n",
      "epoch no.3 train no.242940  loss = 3.36866 avg_loss = 3.26307\n",
      "epoch no.3 train no.242950  loss = 3.57847 avg_loss = 3.29196\n",
      "epoch no.3 train no.242960  loss = 2.95081 avg_loss = 3.30774\n",
      "epoch no.3 train no.242970  loss = 2.44919 avg_loss = 3.26184\n",
      "epoch no.3 train no.242980  loss = 5.86130 avg_loss = 3.27431\n",
      "epoch no.3 train no.242990  loss = 5.34052 avg_loss = 3.31326\n",
      "epoch no.3 train no.243000  loss = 1.38655 avg_loss = 3.26000\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '한', '▁감성', '</s>']\n",
      "여름밤 잔잔한 클래식</s>\n",
      "epoch no.3 train no.243010  loss = 2.73449 avg_loss = 3.26083\n",
      "epoch no.3 train no.243020  loss = 3.97172 avg_loss = 3.28590\n",
      "epoch no.3 train no.243030  loss = 3.12198 avg_loss = 3.31040\n",
      "epoch no.3 train no.243040  loss = 4.35609 avg_loss = 3.31372\n",
      "epoch no.3 train no.243050  loss = 2.36289 avg_loss = 3.28033\n",
      "epoch no.3 train no.243060  loss = 1.98254 avg_loss = 3.28962\n",
      "epoch no.3 train no.243070  loss = 3.32286 avg_loss = 3.33183\n",
      "epoch no.3 train no.243080  loss = 4.73302 avg_loss = 3.33855\n",
      "epoch no.3 train no.243090  loss = 2.32139 avg_loss = 3.29896\n",
      "epoch no.3 train no.243100  loss = 3.03614 avg_loss = 3.27594\n",
      "epoch no.3 train no.243110  loss = 3.38155 avg_loss = 3.31066\n",
      "epoch no.3 train no.243120  loss = 4.99869 avg_loss = 3.36105\n",
      "epoch no.3 train no.243130  loss = 2.85966 avg_loss = 3.35888\n",
      "epoch no.3 train no.243140  loss = 4.33316 avg_loss = 3.38497\n",
      "epoch no.3 train no.243150  loss = 2.96204 avg_loss = 3.33489\n",
      "epoch no.3 train no.243160  loss = 3.00950 avg_loss = 3.32223\n",
      "epoch no.3 train no.243170  loss = 2.57980 avg_loss = 3.30790\n",
      "epoch no.3 train no.243180  loss = 2.23850 avg_loss = 3.33230\n",
      "epoch no.3 train no.243190  loss = 4.30214 avg_loss = 3.37966\n",
      "epoch no.3 train no.243200  loss = 3.43373 avg_loss = 3.36854\n",
      "epoch no.3 train no.243210  loss = 2.63987 avg_loss = 3.37299\n",
      "epoch no.3 train no.243220  loss = 2.81971 avg_loss = 3.36085\n",
      "epoch no.3 train no.243230  loss = 2.35831 avg_loss = 3.32439\n",
      "epoch no.3 train no.243240  loss = 1.80302 avg_loss = 3.28883\n",
      "epoch no.3 train no.243250  loss = 3.88395 avg_loss = 3.26196\n",
      "epoch no.3 train no.243260  loss = 4.13327 avg_loss = 3.26590\n",
      "epoch no.3 train no.243270  loss = 2.85110 avg_loss = 3.27807\n",
      "epoch no.3 train no.243280  loss = 2.82325 avg_loss = 3.25155\n",
      "epoch no.3 train no.243290  loss = 2.54059 avg_loss = 3.24795\n",
      "epoch no.3 train no.243300  loss = 2.70342 avg_loss = 3.26266\n",
      "epoch no.3 train no.243310  loss = 4.12119 avg_loss = 3.26510\n",
      "epoch no.3 train no.243320  loss = 3.08469 avg_loss = 3.26734\n",
      "epoch no.3 train no.243330  loss = 3.60082 avg_loss = 3.28337\n",
      "epoch no.3 train no.243340  loss = 4.15236 avg_loss = 3.26766\n",
      "epoch no.3 train no.243350  loss = 3.50756 avg_loss = 3.24444\n",
      "epoch no.3 train no.243360  loss = 2.95294 avg_loss = 3.23162\n",
      "epoch no.3 train no.243370  loss = 1.91439 avg_loss = 3.20588\n",
      "epoch no.3 train no.243380  loss = 2.33051 avg_loss = 3.18737\n",
      "epoch no.3 train no.243390  loss = 2.76613 avg_loss = 3.19833\n",
      "epoch no.3 train no.243400  loss = 2.56732 avg_loss = 3.20122\n",
      "epoch no.3 train no.243410  loss = 4.09843 avg_loss = 3.22383\n",
      "epoch no.3 train no.243420  loss = 3.81655 avg_loss = 3.23044\n",
      "epoch no.3 train no.243430  loss = 4.17816 avg_loss = 3.24395\n",
      "epoch no.3 train no.243440  loss = 4.01182 avg_loss = 3.26943\n",
      "epoch no.3 train no.243450  loss = 2.46592 avg_loss = 3.26575\n",
      "epoch no.3 train no.243460  loss = 4.59588 avg_loss = 3.29802\n",
      "epoch no.3 train no.243470  loss = 2.53085 avg_loss = 3.27734\n",
      "epoch no.3 train no.243480  loss = 4.21746 avg_loss = 3.26544\n",
      "epoch no.3 train no.243490  loss = 3.48028 avg_loss = 3.29722\n",
      "epoch no.3 train no.243500  loss = 2.05509 avg_loss = 3.29491\n",
      "epoch no.3 train no.243510  loss = 2.65979 avg_loss = 3.25242\n",
      "epoch no.3 train no.243520  loss = 2.80941 avg_loss = 3.27013\n",
      "epoch no.3 train no.243530  loss = 1.83751 avg_loss = 3.26013\n",
      "epoch no.3 train no.243540  loss = 2.13981 avg_loss = 3.22452\n",
      "epoch no.3 train no.243550  loss = 2.41310 avg_loss = 3.16743\n",
      "epoch no.3 train no.243560  loss = 3.23744 avg_loss = 3.15739\n",
      "epoch no.3 train no.243570  loss = 3.65233 avg_loss = 3.18880\n",
      "epoch no.3 train no.243580  loss = 2.25705 avg_loss = 3.16829\n",
      "epoch no.3 train no.243590  loss = 4.03894 avg_loss = 3.18204\n",
      "epoch no.3 train no.243600  loss = 2.86125 avg_loss = 3.13205\n",
      "epoch no.3 train no.243610  loss = 2.13354 avg_loss = 3.05981\n",
      "epoch no.3 train no.243620  loss = 3.96285 avg_loss = 3.08031\n",
      "epoch no.3 train no.243630  loss = 4.01246 avg_loss = 3.11639\n",
      "epoch no.3 train no.243640  loss = 4.10088 avg_loss = 3.12396\n",
      "epoch no.3 train no.243650  loss = 3.26448 avg_loss = 3.17823\n",
      "epoch no.3 train no.243660  loss = 1.29293 avg_loss = 3.17561\n",
      "epoch no.3 train no.243670  loss = 5.93144 avg_loss = 3.20200\n",
      "epoch no.3 train no.243680  loss = 3.12284 avg_loss = 3.17423\n",
      "epoch no.3 train no.243690  loss = 3.13511 avg_loss = 3.19039\n",
      "epoch no.3 train no.243700  loss = 5.92711 avg_loss = 3.19128\n",
      "epoch no.3 train no.243710  loss = 5.75014 avg_loss = 3.21700\n",
      "epoch no.3 train no.243720  loss = 3.17605 avg_loss = 3.20294\n",
      "epoch no.3 train no.243730  loss = 4.60420 avg_loss = 3.22458\n",
      "epoch no.3 train no.243740  loss = 2.64505 avg_loss = 3.19320\n",
      "epoch no.3 train no.243750  loss = 3.77997 avg_loss = 3.21127\n",
      "epoch no.3 train no.243760  loss = 3.51458 avg_loss = 3.22286\n",
      "epoch no.3 train no.243770  loss = 2.62434 avg_loss = 3.27116\n",
      "epoch no.3 train no.243780  loss = 2.66560 avg_loss = 3.24768\n",
      "epoch no.3 train no.243790  loss = 3.83009 avg_loss = 3.21144\n",
      "epoch no.3 train no.243800  loss = 2.86594 avg_loss = 3.24449\n",
      "epoch no.3 train no.243810  loss = 3.79700 avg_loss = 3.28414\n",
      "epoch no.3 train no.243820  loss = 4.29496 avg_loss = 3.34106\n",
      "epoch no.3 train no.243830  loss = 1.29640 avg_loss = 3.31781\n",
      "epoch no.3 train no.243840  loss = 1.96842 avg_loss = 3.30612\n",
      "epoch no.3 train no.243850  loss = 3.84077 avg_loss = 3.31778\n",
      "epoch no.3 train no.243860  loss = 2.90118 avg_loss = 3.36572\n",
      "epoch no.3 train no.243870  loss = 2.99604 avg_loss = 3.31351\n",
      "epoch no.3 train no.243880  loss = 2.98027 avg_loss = 3.29806\n",
      "epoch no.3 train no.243890  loss = 4.17196 avg_loss = 3.25322\n",
      "epoch no.3 train no.243900  loss = 3.28198 avg_loss = 3.24290\n",
      "epoch no.3 train no.243910  loss = 4.69950 avg_loss = 3.19938\n",
      "epoch no.3 train no.243920  loss = 4.28641 avg_loss = 3.18415\n",
      "epoch no.3 train no.243930  loss = 3.26620 avg_loss = 3.20627\n",
      "epoch no.3 train no.243940  loss = 5.98021 avg_loss = 3.22630\n",
      "epoch no.3 train no.243950  loss = 4.07272 avg_loss = 3.21465\n",
      "epoch no.3 train no.243960  loss = 4.35724 avg_loss = 3.25210\n",
      "epoch no.3 train no.243970  loss = 3.14949 avg_loss = 3.25395\n",
      "epoch no.3 train no.243980  loss = 1.80592 avg_loss = 3.24591\n",
      "epoch no.3 train no.243990  loss = 4.29944 avg_loss = 3.29725\n",
      "epoch no.3 train no.244000  loss = 5.04903 avg_loss = 3.31385\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣기', '▁재즈', '한', '▁음악', '</s>', '</s>']\n",
      "여름밤에 듣는 잔잔한 노래들</s>\n",
      "epoch no.3 train no.244010  loss = 4.05457 avg_loss = 3.29342\n",
      "epoch no.3 train no.244020  loss = 4.99969 avg_loss = 3.31195\n",
      "epoch no.3 train no.244030  loss = 4.43146 avg_loss = 3.30407\n",
      "epoch no.3 train no.244040  loss = 5.24225 avg_loss = 3.28604\n",
      "epoch no.3 train no.244050  loss = 2.42614 avg_loss = 3.25598\n",
      "epoch no.3 train no.244060  loss = 3.94081 avg_loss = 3.24621\n",
      "epoch no.3 train no.244070  loss = 2.53183 avg_loss = 3.26123\n",
      "epoch no.3 train no.244080  loss = 2.57805 avg_loss = 3.25779\n",
      "epoch no.3 train no.244090  loss = 2.45049 avg_loss = 3.21874\n",
      "epoch no.3 train no.244100  loss = 4.36732 avg_loss = 3.22936\n",
      "epoch no.3 train no.244110  loss = 2.36943 avg_loss = 3.23335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.244120  loss = 4.57645 avg_loss = 3.23904\n",
      "epoch no.3 train no.244130  loss = 2.73656 avg_loss = 3.24796\n",
      "epoch no.3 train no.244140  loss = 2.26142 avg_loss = 3.23411\n",
      "epoch no.3 train no.244150  loss = 4.72679 avg_loss = 3.24358\n",
      "epoch no.3 train no.244160  loss = 2.29946 avg_loss = 3.23471\n",
      "epoch no.3 train no.244170  loss = 3.51514 avg_loss = 3.21301\n",
      "epoch no.3 train no.244180  loss = 2.68020 avg_loss = 3.23551\n",
      "epoch no.3 train no.244190  loss = 3.14675 avg_loss = 3.26272\n",
      "epoch no.3 train no.244200  loss = 3.75055 avg_loss = 3.26735\n",
      "epoch no.3 train no.244210  loss = 2.73094 avg_loss = 3.27320\n",
      "epoch no.3 train no.244220  loss = 2.58209 avg_loss = 3.29615\n",
      "epoch no.3 train no.244230  loss = 2.29210 avg_loss = 3.32205\n",
      "epoch no.3 train no.244240  loss = 3.62947 avg_loss = 3.30658\n",
      "epoch no.3 train no.244250  loss = 1.88739 avg_loss = 3.35112\n",
      "epoch no.3 train no.244260  loss = 3.17300 avg_loss = 3.34707\n",
      "epoch no.3 train no.244270  loss = 2.87639 avg_loss = 3.33645\n",
      "epoch no.3 train no.244280  loss = 2.91347 avg_loss = 3.34839\n",
      "epoch no.3 train no.244290  loss = 5.51813 avg_loss = 3.39248\n",
      "epoch no.3 train no.244300  loss = 3.36302 avg_loss = 3.38993\n",
      "epoch no.3 train no.244310  loss = 3.66133 avg_loss = 3.39084\n",
      "epoch no.3 train no.244320  loss = 4.52996 avg_loss = 3.41780\n",
      "epoch no.3 train no.244330  loss = 2.29670 avg_loss = 3.41094\n",
      "epoch no.3 train no.244340  loss = 3.97020 avg_loss = 3.38360\n",
      "epoch no.3 train no.244350  loss = 4.10264 avg_loss = 3.34247\n",
      "epoch no.3 train no.244360  loss = 2.30377 avg_loss = 3.28970\n",
      "epoch no.3 train no.244370  loss = 2.17029 avg_loss = 3.26145\n",
      "epoch no.3 train no.244380  loss = 4.08209 avg_loss = 3.25876\n",
      "epoch no.3 train no.244390  loss = 2.97894 avg_loss = 3.25570\n",
      "epoch no.3 train no.244400  loss = 2.51361 avg_loss = 3.22017\n",
      "epoch no.3 train no.244410  loss = 4.99196 avg_loss = 3.20896\n",
      "epoch no.3 train no.244420  loss = 2.94115 avg_loss = 3.21517\n",
      "epoch no.3 train no.244430  loss = 3.59316 avg_loss = 3.24296\n",
      "epoch no.3 train no.244440  loss = 2.47570 avg_loss = 3.23701\n",
      "epoch no.3 train no.244450  loss = 2.82612 avg_loss = 3.28063\n",
      "epoch no.3 train no.244460  loss = 3.31125 avg_loss = 3.31028\n",
      "epoch no.3 train no.244470  loss = 2.77227 avg_loss = 3.32110\n",
      "epoch no.3 train no.244480  loss = 4.89805 avg_loss = 3.37762\n",
      "epoch no.3 train no.244490  loss = 4.46148 avg_loss = 3.41099\n",
      "epoch no.3 train no.244500  loss = 2.23184 avg_loss = 3.37899\n",
      "epoch no.3 train no.244510  loss = 3.56536 avg_loss = 3.43454\n",
      "epoch no.3 train no.244520  loss = 3.66517 avg_loss = 3.46414\n",
      "epoch no.3 train no.244530  loss = 3.85245 avg_loss = 3.47786\n",
      "epoch no.3 train no.244540  loss = 3.06123 avg_loss = 3.45483\n",
      "epoch no.3 train no.244550  loss = 3.37895 avg_loss = 3.43969\n",
      "epoch no.3 train no.244560  loss = 3.32899 avg_loss = 3.42790\n",
      "epoch no.3 train no.244570  loss = 3.80203 avg_loss = 3.41353\n",
      "epoch no.3 train no.244580  loss = 5.41860 avg_loss = 3.45667\n",
      "epoch no.3 train no.244590  loss = 2.13850 avg_loss = 3.42297\n",
      "epoch no.3 train no.244600  loss = 2.92386 avg_loss = 3.39062\n",
      "epoch no.3 train no.244610  loss = 2.90904 avg_loss = 3.41886\n",
      "epoch no.3 train no.244620  loss = 2.83027 avg_loss = 3.40689\n",
      "epoch no.3 train no.244630  loss = 4.38711 avg_loss = 3.45311\n",
      "epoch no.3 train no.244640  loss = 2.00066 avg_loss = 3.42745\n",
      "epoch no.3 train no.244650  loss = 2.50367 avg_loss = 3.41485\n",
      "epoch no.3 train no.244660  loss = 2.96054 avg_loss = 3.41530\n",
      "epoch no.3 train no.244670  loss = 2.55994 avg_loss = 3.40214\n",
      "epoch no.3 train no.244680  loss = 2.86461 avg_loss = 3.40416\n",
      "epoch no.3 train no.244690  loss = 3.48272 avg_loss = 3.37052\n",
      "epoch no.3 train no.244700  loss = 3.54611 avg_loss = 3.42181\n",
      "epoch no.3 train no.244710  loss = 2.62792 avg_loss = 3.39290\n",
      "epoch no.3 train no.244720  loss = 2.18276 avg_loss = 3.38441\n",
      "epoch no.3 train no.244730  loss = 4.88970 avg_loss = 3.38515\n",
      "epoch no.3 train no.244740  loss = 2.91939 avg_loss = 3.36863\n",
      "epoch no.3 train no.244750  loss = 3.43678 avg_loss = 3.31432\n",
      "epoch no.3 train no.244760  loss = 3.19506 avg_loss = 3.29980\n",
      "epoch no.3 train no.244770  loss = 3.96726 avg_loss = 3.27047\n",
      "epoch no.3 train no.244780  loss = 3.42435 avg_loss = 3.24432\n",
      "epoch no.3 train no.244790  loss = 6.45062 avg_loss = 3.25947\n",
      "epoch no.3 train no.244800  loss = 2.34974 avg_loss = 3.28270\n",
      "epoch no.3 train no.244810  loss = 3.90400 avg_loss = 3.34815\n",
      "epoch no.3 train no.244820  loss = 4.50609 avg_loss = 3.37222\n",
      "epoch no.3 train no.244830  loss = 2.30712 avg_loss = 3.32201\n",
      "epoch no.3 train no.244840  loss = 3.23782 avg_loss = 3.31857\n",
      "epoch no.3 train no.244850  loss = 2.16224 avg_loss = 3.32778\n",
      "epoch no.3 train no.244860  loss = 5.66837 avg_loss = 3.35865\n",
      "epoch no.3 train no.244870  loss = 2.79455 avg_loss = 3.33815\n",
      "epoch no.3 train no.244880  loss = 2.68145 avg_loss = 3.32270\n",
      "epoch no.3 train no.244890  loss = 4.33489 avg_loss = 3.31836\n",
      "epoch no.3 train no.244900  loss = 4.05970 avg_loss = 3.31078\n",
      "epoch no.3 train no.244910  loss = 3.57285 avg_loss = 3.27948\n",
      "epoch no.3 train no.244920  loss = 5.17894 avg_loss = 3.28231\n",
      "epoch no.3 train no.244930  loss = 2.11110 avg_loss = 3.31815\n",
      "epoch no.3 train no.244940  loss = 3.76198 avg_loss = 3.32889\n",
      "epoch no.3 train no.244950  loss = 5.07704 avg_loss = 3.29068\n",
      "epoch no.3 train no.244960  loss = 3.26181 avg_loss = 3.32818\n",
      "epoch no.3 train no.244970  loss = 2.35421 avg_loss = 3.35299\n",
      "epoch no.3 train no.244980  loss = 3.82807 avg_loss = 3.34096\n",
      "epoch no.3 train no.244990  loss = 1.80228 avg_loss = 3.35799\n",
      "epoch no.3 train no.245000  loss = 2.75423 avg_loss = 3.34740\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '의', '▁감성', '를', '▁위한', '▁음악', '</s>', '</s>']\n",
      "여름밤의 드라이브를 위한 팝송</s>\n",
      "epoch no.3 train no.245010  loss = 3.97447 avg_loss = 3.36127\n",
      "epoch no.3 train no.245020  loss = 2.81309 avg_loss = 3.34750\n",
      "epoch no.3 train no.245030  loss = 3.07485 avg_loss = 3.31375\n",
      "epoch no.3 train no.245040  loss = 4.92060 avg_loss = 3.35027\n",
      "epoch no.3 train no.245050  loss = 2.53856 avg_loss = 3.30140\n",
      "epoch no.3 train no.245060  loss = 2.73468 avg_loss = 3.33082\n",
      "epoch no.3 train no.245070  loss = 3.19540 avg_loss = 3.33418\n",
      "epoch no.3 train no.245080  loss = 4.22522 avg_loss = 3.37956\n",
      "epoch no.3 train no.245090  loss = 2.98270 avg_loss = 3.35341\n",
      "epoch no.3 train no.245100  loss = 2.23754 avg_loss = 3.30594\n",
      "epoch no.3 train no.245110  loss = 3.84131 avg_loss = 3.28986\n",
      "epoch no.3 train no.245120  loss = 3.01569 avg_loss = 3.28395\n",
      "epoch no.3 train no.245130  loss = 3.92047 avg_loss = 3.32074\n",
      "epoch no.3 train no.245140  loss = 2.69013 avg_loss = 3.30016\n",
      "epoch no.3 train no.245150  loss = 3.95453 avg_loss = 3.29380\n",
      "epoch no.3 train no.245160  loss = 2.32786 avg_loss = 3.29479\n",
      "epoch no.3 train no.245170  loss = 2.35122 avg_loss = 3.30277\n",
      "epoch no.3 train no.245180  loss = 2.45051 avg_loss = 3.29813\n",
      "epoch no.3 train no.245190  loss = 3.31697 avg_loss = 3.28469\n",
      "epoch no.3 train no.245200  loss = 2.95969 avg_loss = 3.25670\n",
      "epoch no.3 train no.245210  loss = 3.52534 avg_loss = 3.21608\n",
      "epoch no.3 train no.245220  loss = 3.12089 avg_loss = 3.29070\n",
      "epoch no.3 train no.245230  loss = 4.75319 avg_loss = 3.35806\n",
      "epoch no.3 train no.245240  loss = 5.13177 avg_loss = 3.42899\n",
      "epoch no.3 train no.245250  loss = 2.72136 avg_loss = 3.38994\n",
      "epoch no.3 train no.245260  loss = 2.58723 avg_loss = 3.36821\n",
      "epoch no.3 train no.245270  loss = 3.96609 avg_loss = 3.33574\n",
      "epoch no.3 train no.245280  loss = 2.90895 avg_loss = 3.34974\n",
      "epoch no.3 train no.245290  loss = 3.61056 avg_loss = 3.28808\n",
      "epoch no.3 train no.245300  loss = 2.40544 avg_loss = 3.26797\n",
      "epoch no.3 train no.245310  loss = 3.32236 avg_loss = 3.25327\n",
      "epoch no.3 train no.245320  loss = 3.30605 avg_loss = 3.29054\n",
      "epoch no.3 train no.245330  loss = 3.29968 avg_loss = 3.27667\n",
      "epoch no.3 train no.245340  loss = 2.92042 avg_loss = 3.29162\n",
      "epoch no.3 train no.245350  loss = 4.15785 avg_loss = 3.28602\n",
      "epoch no.3 train no.245360  loss = 2.84479 avg_loss = 3.26408\n",
      "epoch no.3 train no.245370  loss = 3.28962 avg_loss = 3.27959\n",
      "epoch no.3 train no.245380  loss = 2.82548 avg_loss = 3.30738\n",
      "epoch no.3 train no.245390  loss = 4.87063 avg_loss = 3.32596\n",
      "epoch no.3 train no.245400  loss = 3.59709 avg_loss = 3.29351\n",
      "epoch no.3 train no.245410  loss = 2.00019 avg_loss = 3.30698\n",
      "epoch no.3 train no.245420  loss = 2.60217 avg_loss = 3.30947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.245430  loss = 2.04719 avg_loss = 3.27282\n",
      "epoch no.3 train no.245440  loss = 1.96026 avg_loss = 3.28426\n",
      "epoch no.3 train no.245450  loss = 3.47357 avg_loss = 3.32458\n",
      "epoch no.3 train no.245460  loss = 3.50936 avg_loss = 3.34560\n",
      "epoch no.3 train no.245470  loss = 4.35972 avg_loss = 3.38744\n",
      "epoch no.3 train no.245480  loss = 2.13746 avg_loss = 3.36068\n",
      "epoch no.3 train no.245490  loss = 2.63823 avg_loss = 3.35504\n",
      "epoch no.3 train no.245500  loss = 1.66600 avg_loss = 3.35150\n",
      "epoch no.3 train no.245510  loss = 2.78310 avg_loss = 3.30398\n",
      "epoch no.3 train no.245520  loss = 1.93464 avg_loss = 3.26289\n",
      "epoch no.3 train no.245530  loss = 3.38492 avg_loss = 3.26082\n",
      "epoch no.3 train no.245540  loss = 5.00352 avg_loss = 3.29400\n",
      "epoch no.3 train no.245550  loss = 3.53694 avg_loss = 3.26369\n",
      "epoch no.3 train no.245560  loss = 4.04492 avg_loss = 3.28467\n",
      "epoch no.3 train no.245570  loss = 3.01150 avg_loss = 3.35006\n",
      "epoch no.3 train no.245580  loss = 2.12573 avg_loss = 3.29415\n",
      "epoch no.3 train no.245590  loss = 2.86070 avg_loss = 3.31019\n",
      "epoch no.3 train no.245600  loss = 4.33244 avg_loss = 3.31191\n",
      "epoch no.3 train no.245610  loss = 2.47144 avg_loss = 3.28007\n",
      "epoch no.3 train no.245620  loss = 3.19964 avg_loss = 3.28782\n",
      "epoch no.3 train no.245630  loss = 2.57712 avg_loss = 3.27532\n",
      "epoch no.3 train no.245640  loss = 3.95698 avg_loss = 3.31623\n",
      "epoch no.3 train no.245650  loss = 3.34354 avg_loss = 3.35118\n",
      "epoch no.3 train no.245660  loss = 2.37524 avg_loss = 3.32372\n",
      "epoch no.3 train no.245670  loss = 4.17113 avg_loss = 3.31800\n",
      "epoch no.3 train no.245680  loss = 4.96569 avg_loss = 3.29596\n",
      "epoch no.3 train no.245690  loss = 1.49205 avg_loss = 3.27175\n",
      "epoch no.3 train no.245700  loss = 2.16808 avg_loss = 3.23554\n",
      "epoch no.3 train no.245710  loss = 3.59916 avg_loss = 3.26105\n",
      "epoch no.3 train no.245720  loss = 3.77921 avg_loss = 3.29383\n",
      "epoch no.3 train no.245730  loss = 2.90509 avg_loss = 3.32692\n",
      "epoch no.3 train no.245740  loss = 3.78314 avg_loss = 3.33244\n",
      "epoch no.3 train no.245750  loss = 3.56376 avg_loss = 3.34608\n",
      "epoch no.3 train no.245760  loss = 3.55799 avg_loss = 3.35107\n",
      "epoch no.3 train no.245770  loss = 3.66115 avg_loss = 3.35728\n",
      "epoch no.3 train no.245780  loss = 3.21961 avg_loss = 3.36110\n",
      "epoch no.3 train no.245790  loss = 3.22185 avg_loss = 3.34339\n",
      "epoch no.3 train no.245800  loss = 3.84819 avg_loss = 3.35346\n",
      "epoch no.3 train no.245810  loss = 3.27145 avg_loss = 3.33168\n",
      "epoch no.3 train no.245820  loss = 4.26647 avg_loss = 3.35594\n",
      "epoch no.3 train no.245830  loss = 2.47432 avg_loss = 3.32648\n",
      "epoch no.3 train no.245840  loss = 3.21800 avg_loss = 3.29231\n",
      "epoch no.3 train no.245850  loss = 2.38260 avg_loss = 3.30135\n",
      "epoch no.3 train no.245860  loss = 2.41417 avg_loss = 3.24817\n",
      "epoch no.3 train no.245870  loss = 2.11303 avg_loss = 3.25019\n",
      "epoch no.3 train no.245880  loss = 3.68923 avg_loss = 3.30603\n",
      "epoch no.3 train no.245890  loss = 2.80982 avg_loss = 3.27229\n",
      "epoch no.3 train no.245900  loss = 4.21829 avg_loss = 3.27486\n",
      "epoch no.3 train no.245910  loss = 3.48236 avg_loss = 3.31246\n",
      "epoch no.3 train no.245920  loss = 3.15680 avg_loss = 3.36710\n",
      "epoch no.3 train no.245930  loss = 4.06433 avg_loss = 3.35920\n",
      "epoch no.3 train no.245940  loss = 3.60313 avg_loss = 3.31024\n",
      "epoch no.3 train no.245950  loss = 3.00935 avg_loss = 3.32184\n",
      "epoch no.3 train no.245960  loss = 3.11499 avg_loss = 3.31897\n",
      "epoch no.3 train no.245970  loss = 3.81006 avg_loss = 3.36522\n",
      "epoch no.3 train no.245980  loss = 3.01729 avg_loss = 3.34125\n",
      "epoch no.3 train no.245990  loss = 2.03587 avg_loss = 3.31483\n",
      "epoch no.3 train no.246000  loss = 2.04610 avg_loss = 3.31912\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '한', '▁피아노', '곡', '</s>']\n",
      "여름밤 잔잔한 연주곡</s>\n",
      "epoch no.3 train no.246010  loss = 2.03916 avg_loss = 3.28098\n",
      "epoch no.3 train no.246020  loss = 3.54497 avg_loss = 3.30337\n",
      "epoch no.3 train no.246030  loss = 4.15391 avg_loss = 3.32925\n",
      "epoch no.3 train no.246040  loss = 3.90206 avg_loss = 3.31795\n",
      "epoch no.3 train no.246050  loss = 3.50709 avg_loss = 3.34339\n",
      "epoch no.3 train no.246060  loss = 3.80088 avg_loss = 3.34076\n",
      "epoch no.3 train no.246070  loss = 4.10380 avg_loss = 3.32236\n",
      "epoch no.3 train no.246080  loss = 4.27007 avg_loss = 3.34979\n",
      "epoch no.3 train no.246090  loss = 3.81507 avg_loss = 3.35380\n",
      "epoch no.3 train no.246100  loss = 3.68260 avg_loss = 3.36158\n",
      "epoch no.3 train no.246110  loss = 2.98375 avg_loss = 3.32297\n",
      "epoch no.3 train no.246120  loss = 3.46951 avg_loss = 3.29159\n",
      "epoch no.3 train no.246130  loss = 4.59616 avg_loss = 3.29054\n",
      "epoch no.3 train no.246140  loss = 3.38687 avg_loss = 3.30472\n",
      "epoch no.3 train no.246150  loss = 2.58510 avg_loss = 3.29665\n",
      "epoch no.3 train no.246160  loss = 3.25061 avg_loss = 3.31044\n",
      "epoch no.3 train no.246170  loss = 3.20332 avg_loss = 3.32112\n",
      "epoch no.3 train no.246180  loss = 3.89610 avg_loss = 3.33018\n",
      "epoch no.3 train no.246190  loss = 3.28235 avg_loss = 3.34661\n",
      "epoch no.3 train no.246200  loss = 4.30995 avg_loss = 3.39189\n",
      "epoch no.3 train no.246210  loss = 3.48525 avg_loss = 3.36434\n",
      "epoch no.3 train no.246220  loss = 1.57257 avg_loss = 3.32867\n",
      "epoch no.3 train no.246230  loss = 2.17479 avg_loss = 3.31174\n",
      "epoch no.3 train no.246240  loss = 2.55734 avg_loss = 3.27075\n",
      "epoch no.3 train no.246250  loss = 2.61782 avg_loss = 3.22840\n",
      "epoch no.3 train no.246260  loss = 3.08433 avg_loss = 3.19773\n",
      "epoch no.3 train no.246270  loss = 2.60333 avg_loss = 3.14799\n",
      "epoch no.3 train no.246280  loss = 4.75115 avg_loss = 3.19613\n",
      "epoch no.3 train no.246290  loss = 2.36217 avg_loss = 3.21615\n",
      "epoch no.3 train no.246300  loss = 3.22537 avg_loss = 3.24914\n",
      "epoch no.3 train no.246310  loss = 3.65093 avg_loss = 3.25830\n",
      "epoch no.3 train no.246320  loss = 3.37091 avg_loss = 3.24247\n",
      "epoch no.3 train no.246330  loss = 2.50583 avg_loss = 3.24538\n",
      "epoch no.3 train no.246340  loss = 2.24693 avg_loss = 3.22200\n",
      "epoch no.3 train no.246350  loss = 2.27987 avg_loss = 3.25235\n",
      "epoch no.3 train no.246360  loss = 4.93666 avg_loss = 3.27326\n",
      "epoch no.3 train no.246370  loss = 3.13869 avg_loss = 3.27145\n",
      "epoch no.3 train no.246380  loss = 4.15355 avg_loss = 3.29046\n",
      "epoch no.3 train no.246390  loss = 1.96886 avg_loss = 3.24324\n",
      "epoch no.3 train no.246400  loss = 3.75093 avg_loss = 3.21769\n",
      "epoch no.3 train no.246410  loss = 1.74298 avg_loss = 3.18385\n",
      "epoch no.3 train no.246420  loss = 3.27578 avg_loss = 3.17827\n",
      "epoch no.3 train no.246430  loss = 4.12549 avg_loss = 3.22832\n",
      "epoch no.3 train no.246440  loss = 4.82109 avg_loss = 3.24349\n",
      "epoch no.3 train no.246450  loss = 2.52621 avg_loss = 3.27342\n",
      "epoch no.3 train no.246460  loss = 1.68732 avg_loss = 3.27085\n",
      "epoch no.3 train no.246470  loss = 2.98524 avg_loss = 3.27577\n",
      "epoch no.3 train no.246480  loss = 3.24387 avg_loss = 3.26458\n",
      "epoch no.3 train no.246490  loss = 3.89062 avg_loss = 3.23885\n",
      "epoch no.3 train no.246500  loss = 2.98604 avg_loss = 3.23246\n",
      "epoch no.3 train no.246510  loss = 2.21909 avg_loss = 3.21546\n",
      "epoch no.3 train no.246520  loss = 5.36162 avg_loss = 3.26472\n",
      "epoch no.3 train no.246530  loss = 2.66231 avg_loss = 3.20131\n",
      "epoch no.3 train no.246540  loss = 3.56492 avg_loss = 3.23942\n",
      "epoch no.3 train no.246550  loss = 1.94232 avg_loss = 3.23704\n",
      "epoch no.3 train no.246560  loss = 3.21750 avg_loss = 3.23265\n",
      "epoch no.3 train no.246570  loss = 3.84882 avg_loss = 3.29898\n",
      "epoch no.3 train no.246580  loss = 3.39010 avg_loss = 3.34862\n",
      "epoch no.3 train no.246590  loss = 2.18058 avg_loss = 3.30023\n",
      "epoch no.3 train no.246600  loss = 3.89229 avg_loss = 3.27322\n",
      "epoch no.3 train no.246610  loss = 3.23874 avg_loss = 3.23764\n",
      "epoch no.3 train no.246620  loss = 2.22645 avg_loss = 3.22239\n",
      "epoch no.3 train no.246630  loss = 2.98660 avg_loss = 3.28200\n",
      "epoch no.3 train no.246640  loss = 2.70465 avg_loss = 3.26643\n",
      "epoch no.3 train no.246650  loss = 2.74973 avg_loss = 3.25533\n",
      "epoch no.3 train no.246660  loss = 2.38668 avg_loss = 3.20033\n",
      "epoch no.3 train no.246670  loss = 2.29864 avg_loss = 3.21385\n",
      "epoch no.3 train no.246680  loss = 4.85373 avg_loss = 3.19923\n",
      "epoch no.3 train no.246690  loss = 2.56410 avg_loss = 3.18915\n",
      "epoch no.3 train no.246700  loss = 2.48900 avg_loss = 3.16826\n",
      "epoch no.3 train no.246710  loss = 3.06031 avg_loss = 3.18941\n",
      "epoch no.3 train no.246720  loss = 3.43064 avg_loss = 3.22124\n",
      "epoch no.3 train no.246730  loss = 3.84860 avg_loss = 3.24716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.246740  loss = 1.95505 avg_loss = 3.26317\n",
      "epoch no.3 train no.246750  loss = 2.70321 avg_loss = 3.23636\n",
      "epoch no.3 train no.246760  loss = 2.68692 avg_loss = 3.25462\n",
      "epoch no.3 train no.246770  loss = 3.14072 avg_loss = 3.25798\n",
      "epoch no.3 train no.246780  loss = 2.10089 avg_loss = 3.22598\n",
      "epoch no.3 train no.246790  loss = 2.85259 avg_loss = 3.22551\n",
      "epoch no.3 train no.246800  loss = 2.46505 avg_loss = 3.22340\n",
      "epoch no.3 train no.246810  loss = 3.26206 avg_loss = 3.23344\n",
      "epoch no.3 train no.246820  loss = 3.05326 avg_loss = 3.24312\n",
      "epoch no.3 train no.246830  loss = 3.17384 avg_loss = 3.24418\n",
      "epoch no.3 train no.246840  loss = 4.42846 avg_loss = 3.23723\n",
      "epoch no.3 train no.246850  loss = 2.11028 avg_loss = 3.20344\n",
      "epoch no.3 train no.246860  loss = 3.95105 avg_loss = 3.22192\n",
      "epoch no.3 train no.246870  loss = 2.96825 avg_loss = 3.19180\n",
      "epoch no.3 train no.246880  loss = 3.43303 avg_loss = 3.20741\n",
      "epoch no.3 train no.246890  loss = 3.13477 avg_loss = 3.25801\n",
      "epoch no.3 train no.246900  loss = 3.29389 avg_loss = 3.26231\n",
      "epoch no.3 train no.246910  loss = 2.16899 avg_loss = 3.27402\n",
      "epoch no.3 train no.246920  loss = 3.91713 avg_loss = 3.30715\n",
      "epoch no.3 train no.246930  loss = 3.05182 avg_loss = 3.29603\n",
      "epoch no.3 train no.246940  loss = 2.81634 avg_loss = 3.29917\n",
      "epoch no.3 train no.246950  loss = 3.52352 avg_loss = 3.30263\n",
      "epoch no.3 train no.246960  loss = 4.66108 avg_loss = 3.29605\n",
      "epoch no.3 train no.246970  loss = 3.69646 avg_loss = 3.30782\n",
      "epoch no.3 train no.246980  loss = 2.52844 avg_loss = 3.29754\n",
      "epoch no.3 train no.246990  loss = 4.85985 avg_loss = 3.32386\n",
      "epoch no.3 train no.247000  loss = 4.50706 avg_loss = 3.34338\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '좋은', '▁노래', '</s>']\n",
      "여름밤 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.247010  loss = 2.09590 avg_loss = 3.34119\n",
      "epoch no.3 train no.247020  loss = 4.18732 avg_loss = 3.35868\n",
      "epoch no.3 train no.247030  loss = 5.76239 avg_loss = 3.41373\n",
      "epoch no.3 train no.247040  loss = 3.61768 avg_loss = 3.39309\n",
      "epoch no.3 train no.247050  loss = 3.75014 avg_loss = 3.38260\n",
      "epoch no.3 train no.247060  loss = 4.59809 avg_loss = 3.34982\n",
      "epoch no.3 train no.247070  loss = 3.84726 avg_loss = 3.39093\n",
      "epoch no.3 train no.247080  loss = 4.03062 avg_loss = 3.36676\n",
      "epoch no.3 train no.247090  loss = 2.68197 avg_loss = 3.33862\n",
      "epoch no.3 train no.247100  loss = 1.37839 avg_loss = 3.31742\n",
      "epoch no.3 train no.247110  loss = 2.52032 avg_loss = 3.24583\n",
      "epoch no.3 train no.247120  loss = 1.81445 avg_loss = 3.24915\n",
      "epoch no.3 train no.247130  loss = 5.02014 avg_loss = 3.27298\n",
      "epoch no.3 train no.247140  loss = 3.40633 avg_loss = 3.26873\n",
      "epoch no.3 train no.247150  loss = 1.93605 avg_loss = 3.24047\n",
      "epoch no.3 train no.247160  loss = 3.73528 avg_loss = 3.29678\n",
      "epoch no.3 train no.247170  loss = 2.85953 avg_loss = 3.31213\n",
      "epoch no.3 train no.247180  loss = 4.64680 avg_loss = 3.32970\n",
      "epoch no.3 train no.247190  loss = 3.21708 avg_loss = 3.32648\n",
      "epoch no.3 train no.247200  loss = 3.66096 avg_loss = 3.32539\n",
      "epoch no.3 train no.247210  loss = 4.43243 avg_loss = 3.30296\n",
      "epoch no.3 train no.247220  loss = 3.83185 avg_loss = 3.33396\n",
      "epoch no.3 train no.247230  loss = 4.18479 avg_loss = 3.36113\n",
      "epoch no.3 train no.247240  loss = 1.72142 avg_loss = 3.33682\n",
      "epoch no.3 train no.247250  loss = 3.28615 avg_loss = 3.29857\n",
      "epoch no.3 train no.247260  loss = 2.02901 avg_loss = 3.26848\n",
      "epoch no.3 train no.247270  loss = 3.18287 avg_loss = 3.26384\n",
      "epoch no.3 train no.247280  loss = 4.28981 avg_loss = 3.24473\n",
      "epoch no.3 train no.247290  loss = 2.84825 avg_loss = 3.21411\n",
      "epoch no.3 train no.247300  loss = 3.16400 avg_loss = 3.22754\n",
      "epoch no.3 train no.247310  loss = 2.11261 avg_loss = 3.19660\n",
      "epoch no.3 train no.247320  loss = 4.16077 avg_loss = 3.21224\n",
      "epoch no.3 train no.247330  loss = 5.63961 avg_loss = 3.26130\n",
      "epoch no.3 train no.247340  loss = 2.29313 avg_loss = 3.27506\n",
      "epoch no.3 train no.247350  loss = 2.66076 avg_loss = 3.26664\n",
      "epoch no.3 train no.247360  loss = 5.41038 avg_loss = 3.31115\n",
      "epoch no.3 train no.247370  loss = 3.72126 avg_loss = 3.28048\n",
      "epoch no.3 train no.247380  loss = 3.15272 avg_loss = 3.30983\n",
      "epoch no.3 train no.247390  loss = 4.08553 avg_loss = 3.27231\n",
      "epoch no.3 train no.247400  loss = 3.48498 avg_loss = 3.30054\n",
      "epoch no.3 train no.247410  loss = 3.92859 avg_loss = 3.34060\n",
      "epoch no.3 train no.247420  loss = 3.02941 avg_loss = 3.35556\n",
      "epoch no.3 train no.247430  loss = 3.89443 avg_loss = 3.34953\n",
      "epoch no.3 train no.247440  loss = 2.00879 avg_loss = 3.33700\n",
      "epoch no.3 train no.247450  loss = 5.74117 avg_loss = 3.37792\n",
      "epoch no.3 train no.247460  loss = 3.11519 avg_loss = 3.37946\n",
      "epoch no.3 train no.247470  loss = 3.40353 avg_loss = 3.38549\n",
      "epoch no.3 train no.247480  loss = 3.02173 avg_loss = 3.39645\n",
      "epoch no.3 train no.247490  loss = 4.91006 avg_loss = 3.38738\n",
      "epoch no.3 train no.247500  loss = 3.74519 avg_loss = 3.42391\n",
      "epoch no.3 train no.247510  loss = 3.03058 avg_loss = 3.39008\n",
      "epoch no.3 train no.247520  loss = 2.30985 avg_loss = 3.35412\n",
      "epoch no.3 train no.247530  loss = 3.45239 avg_loss = 3.34628\n",
      "epoch no.3 train no.247540  loss = 1.82473 avg_loss = 3.31212\n",
      "epoch no.3 train no.247550  loss = 3.19657 avg_loss = 3.34273\n",
      "epoch no.3 train no.247560  loss = 3.98072 avg_loss = 3.35233\n",
      "epoch no.3 train no.247570  loss = 2.66346 avg_loss = 3.38343\n",
      "epoch no.3 train no.247580  loss = 3.89922 avg_loss = 3.38608\n",
      "epoch no.3 train no.247590  loss = 4.80935 avg_loss = 3.38558\n",
      "epoch no.3 train no.247600  loss = 3.13064 avg_loss = 3.36368\n",
      "epoch no.3 train no.247610  loss = 3.56533 avg_loss = 3.38377\n",
      "epoch no.3 train no.247620  loss = 3.45638 avg_loss = 3.33438\n",
      "epoch no.3 train no.247630  loss = 3.20920 avg_loss = 3.30225\n",
      "epoch no.3 train no.247640  loss = 4.14394 avg_loss = 3.30502\n",
      "epoch no.3 train no.247650  loss = 1.53559 avg_loss = 3.27323\n",
      "epoch no.3 train no.247660  loss = 5.01655 avg_loss = 3.34062\n",
      "epoch no.3 train no.247670  loss = 4.56802 avg_loss = 3.36079\n",
      "epoch no.3 train no.247680  loss = 2.57825 avg_loss = 3.40297\n",
      "epoch no.3 train no.247690  loss = 2.67607 avg_loss = 3.37964\n",
      "epoch no.3 train no.247700  loss = 5.57785 avg_loss = 3.42839\n",
      "epoch no.3 train no.247710  loss = 3.68477 avg_loss = 3.45374\n",
      "epoch no.3 train no.247720  loss = 4.02429 avg_loss = 3.45773\n",
      "epoch no.3 train no.247730  loss = 2.19089 avg_loss = 3.39997\n",
      "epoch no.3 train no.247740  loss = 4.96269 avg_loss = 3.43169\n",
      "epoch no.3 train no.247750  loss = 4.18871 avg_loss = 3.38329\n",
      "epoch no.3 train no.247760  loss = 3.07179 avg_loss = 3.38384\n",
      "epoch no.3 train no.247770  loss = 2.53254 avg_loss = 3.35957\n",
      "epoch no.3 train no.247780  loss = 2.89295 avg_loss = 3.38679\n",
      "epoch no.3 train no.247790  loss = 4.03515 avg_loss = 3.38204\n",
      "epoch no.3 train no.247800  loss = 4.07925 avg_loss = 3.42691\n",
      "epoch no.3 train no.247810  loss = 3.33483 avg_loss = 3.41285\n",
      "epoch no.3 train no.247820  loss = 4.72111 avg_loss = 3.39766\n",
      "epoch no.3 train no.247830  loss = 2.39618 avg_loss = 3.37537\n",
      "epoch no.3 train no.247840  loss = 3.48528 avg_loss = 3.36563\n",
      "epoch no.3 train no.247850  loss = 4.81256 avg_loss = 3.37211\n",
      "epoch no.3 train no.247860  loss = 2.59087 avg_loss = 3.33773\n",
      "epoch no.3 train no.247870  loss = 2.56316 avg_loss = 3.33667\n",
      "epoch no.3 train no.247880  loss = 2.13877 avg_loss = 3.36695\n",
      "epoch no.3 train no.247890  loss = 3.04094 avg_loss = 3.37142\n",
      "epoch no.3 train no.247900  loss = 4.21443 avg_loss = 3.39195\n",
      "epoch no.3 train no.247910  loss = 2.60526 avg_loss = 3.37786\n",
      "epoch no.3 train no.247920  loss = 4.15303 avg_loss = 3.37872\n",
      "epoch no.3 train no.247930  loss = 4.88586 avg_loss = 3.40626\n",
      "epoch no.3 train no.247940  loss = 2.76364 avg_loss = 3.40227\n",
      "epoch no.3 train no.247950  loss = 2.70736 avg_loss = 3.44117\n",
      "epoch no.3 train no.247960  loss = 2.00514 avg_loss = 3.34618\n",
      "epoch no.3 train no.247970  loss = 2.33189 avg_loss = 3.34743\n",
      "epoch no.3 train no.247980  loss = 3.80691 avg_loss = 3.31474\n",
      "epoch no.3 train no.247990  loss = 4.06038 avg_loss = 3.34674\n",
      "epoch no.3 train no.248000  loss = 2.15916 avg_loss = 3.32374\n",
      "3\n",
      "to_tokens: ['▁비', '밤', '의', '▁잔잔', '</s>']\n",
      "여름밤의 재즈</s>\n",
      "epoch no.3 train no.248010  loss = 2.59683 avg_loss = 3.29955\n",
      "epoch no.3 train no.248020  loss = 3.15247 avg_loss = 3.31370\n",
      "epoch no.3 train no.248030  loss = 4.53774 avg_loss = 3.28476\n",
      "epoch no.3 train no.248040  loss = 3.56060 avg_loss = 3.26189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.248050  loss = 3.13042 avg_loss = 3.24537\n",
      "epoch no.3 train no.248060  loss = 2.93285 avg_loss = 3.31258\n",
      "epoch no.3 train no.248070  loss = 2.99018 avg_loss = 3.27323\n",
      "epoch no.3 train no.248080  loss = 3.08748 avg_loss = 3.32035\n",
      "epoch no.3 train no.248090  loss = 4.53196 avg_loss = 3.32037\n",
      "epoch no.3 train no.248100  loss = 5.57902 avg_loss = 3.29210\n",
      "epoch no.3 train no.248110  loss = 4.44751 avg_loss = 3.34705\n",
      "epoch no.3 train no.248120  loss = 4.07783 avg_loss = 3.31828\n",
      "epoch no.3 train no.248130  loss = 3.60425 avg_loss = 3.34568\n",
      "epoch no.3 train no.248140  loss = 2.43505 avg_loss = 3.37368\n",
      "epoch no.3 train no.248150  loss = 2.35130 avg_loss = 3.32432\n",
      "epoch no.3 train no.248160  loss = 3.50983 avg_loss = 3.33021\n",
      "epoch no.3 train no.248170  loss = 2.54783 avg_loss = 3.31975\n",
      "epoch no.3 train no.248180  loss = 2.04647 avg_loss = 3.27365\n",
      "epoch no.3 train no.248190  loss = 2.12356 avg_loss = 3.30755\n",
      "epoch no.3 train no.248200  loss = 2.24758 avg_loss = 3.32731\n",
      "epoch no.3 train no.248210  loss = 1.86795 avg_loss = 3.34041\n",
      "epoch no.3 train no.248220  loss = 4.03740 avg_loss = 3.32995\n",
      "epoch no.3 train no.248230  loss = 4.25378 avg_loss = 3.31292\n",
      "epoch no.3 train no.248240  loss = 3.54188 avg_loss = 3.31444\n",
      "epoch no.3 train no.248250  loss = 3.23033 avg_loss = 3.27592\n",
      "epoch no.3 train no.248260  loss = 2.16196 avg_loss = 3.29549\n",
      "epoch no.3 train no.248270  loss = 3.52548 avg_loss = 3.26322\n",
      "epoch no.3 train no.248280  loss = 3.40011 avg_loss = 3.25311\n",
      "epoch no.3 train no.248290  loss = 3.03395 avg_loss = 3.23183\n",
      "epoch no.3 train no.248300  loss = 3.16442 avg_loss = 3.19396\n",
      "epoch no.3 train no.248310  loss = 3.64678 avg_loss = 3.20433\n",
      "epoch no.3 train no.248320  loss = 3.28393 avg_loss = 3.21204\n",
      "epoch no.3 train no.248330  loss = 3.54782 avg_loss = 3.25083\n",
      "epoch no.3 train no.248340  loss = 1.94257 avg_loss = 3.23392\n",
      "epoch no.3 train no.248350  loss = 1.82623 avg_loss = 3.21647\n",
      "epoch no.3 train no.248360  loss = 3.09453 avg_loss = 3.22097\n",
      "epoch no.3 train no.248370  loss = 4.10902 avg_loss = 3.25775\n",
      "epoch no.3 train no.248380  loss = 2.13441 avg_loss = 3.25565\n",
      "epoch no.3 train no.248390  loss = 4.87170 avg_loss = 3.25766\n",
      "epoch no.3 train no.248400  loss = 3.94999 avg_loss = 3.24594\n",
      "epoch no.3 train no.248410  loss = 3.33626 avg_loss = 3.28624\n",
      "epoch no.3 train no.248420  loss = 3.69803 avg_loss = 3.33352\n",
      "epoch no.3 train no.248430  loss = 2.45318 avg_loss = 3.30377\n",
      "epoch no.3 train no.248440  loss = 3.66614 avg_loss = 3.29949\n",
      "epoch no.3 train no.248450  loss = 3.80337 avg_loss = 3.32141\n",
      "epoch no.3 train no.248460  loss = 2.30619 avg_loss = 3.37685\n",
      "epoch no.3 train no.248470  loss = 3.68360 avg_loss = 3.39002\n",
      "epoch no.3 train no.248480  loss = 3.98344 avg_loss = 3.40486\n",
      "epoch no.3 train no.248490  loss = 4.68951 avg_loss = 3.39025\n",
      "epoch no.3 train no.248500  loss = 3.76341 avg_loss = 3.38974\n",
      "epoch no.3 train no.248510  loss = 3.89590 avg_loss = 3.39028\n",
      "epoch no.3 train no.248520  loss = 5.10225 avg_loss = 3.36752\n",
      "epoch no.3 train no.248530  loss = 2.16453 avg_loss = 3.31360\n",
      "epoch no.3 train no.248540  loss = 3.62085 avg_loss = 3.35421\n",
      "epoch no.3 train no.248550  loss = 2.50655 avg_loss = 3.34353\n",
      "epoch no.3 train no.248560  loss = 2.22316 avg_loss = 3.36845\n",
      "epoch no.3 train no.248570  loss = 2.40147 avg_loss = 3.38291\n",
      "epoch no.3 train no.248580  loss = 2.52991 avg_loss = 3.40546\n",
      "epoch no.3 train no.248590  loss = 1.81145 avg_loss = 3.39045\n",
      "epoch no.3 train no.248600  loss = 4.87756 avg_loss = 3.37125\n",
      "epoch no.3 train no.248610  loss = 2.17823 avg_loss = 3.34784\n",
      "epoch no.3 train no.248620  loss = 3.17861 avg_loss = 3.33143\n",
      "epoch no.3 train no.248630  loss = 2.38576 avg_loss = 3.34232\n",
      "epoch no.3 train no.248640  loss = 3.44301 avg_loss = 3.33548\n",
      "epoch no.3 train no.248650  loss = 1.91676 avg_loss = 3.30978\n",
      "epoch no.3 train no.248660  loss = 2.97702 avg_loss = 3.30185\n",
      "epoch no.3 train no.248670  loss = 3.82394 avg_loss = 3.27214\n",
      "epoch no.3 train no.248680  loss = 3.69924 avg_loss = 3.27430\n",
      "epoch no.3 train no.248690  loss = 3.14811 avg_loss = 3.24732\n",
      "epoch no.3 train no.248700  loss = 2.19997 avg_loss = 3.23237\n",
      "epoch no.3 train no.248710  loss = 5.29028 avg_loss = 3.20825\n",
      "epoch no.3 train no.248720  loss = 3.64048 avg_loss = 3.22464\n",
      "epoch no.3 train no.248730  loss = 3.39815 avg_loss = 3.19686\n",
      "epoch no.3 train no.248740  loss = 5.91805 avg_loss = 3.24640\n",
      "epoch no.3 train no.248750  loss = 3.76623 avg_loss = 3.21488\n",
      "epoch no.3 train no.248760  loss = 3.17054 avg_loss = 3.21234\n",
      "epoch no.3 train no.248770  loss = 2.16068 avg_loss = 3.21981\n",
      "epoch no.3 train no.248780  loss = 2.15491 avg_loss = 3.21666\n",
      "epoch no.3 train no.248790  loss = 3.67755 avg_loss = 3.15130\n",
      "epoch no.3 train no.248800  loss = 2.57160 avg_loss = 3.17362\n",
      "epoch no.3 train no.248810  loss = 4.27161 avg_loss = 3.17979\n",
      "epoch no.3 train no.248820  loss = 2.61264 avg_loss = 3.16628\n",
      "epoch no.3 train no.248830  loss = 3.36845 avg_loss = 3.24077\n",
      "epoch no.3 train no.248840  loss = 2.26070 avg_loss = 3.23612\n",
      "epoch no.3 train no.248850  loss = 2.77498 avg_loss = 3.25077\n",
      "epoch no.3 train no.248860  loss = 2.75795 avg_loss = 3.28562\n",
      "epoch no.3 train no.248870  loss = 3.63440 avg_loss = 3.27344\n",
      "epoch no.3 train no.248880  loss = 4.02851 avg_loss = 3.29360\n",
      "epoch no.3 train no.248890  loss = 3.31119 avg_loss = 3.29535\n",
      "epoch no.3 train no.248900  loss = 3.59414 avg_loss = 3.28775\n",
      "epoch no.3 train no.248910  loss = 2.89441 avg_loss = 3.29241\n",
      "epoch no.3 train no.248920  loss = 3.29512 avg_loss = 3.32129\n",
      "epoch no.3 train no.248930  loss = 2.47866 avg_loss = 3.32601\n",
      "epoch no.3 train no.248940  loss = 2.73473 avg_loss = 3.32767\n",
      "epoch no.3 train no.248950  loss = 4.30166 avg_loss = 3.34395\n",
      "epoch no.3 train no.248960  loss = 2.78872 avg_loss = 3.34426\n",
      "epoch no.3 train no.248970  loss = 2.58949 avg_loss = 3.29148\n",
      "epoch no.3 train no.248980  loss = 2.51207 avg_loss = 3.26460\n",
      "epoch no.3 train no.248990  loss = 2.37560 avg_loss = 3.27189\n",
      "epoch no.3 train no.249000  loss = 3.81033 avg_loss = 3.25233\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '뮤직', '</s>']\n",
      "여름밤의 인디뮤직</s>\n",
      "epoch no.3 train no.249010  loss = 3.68865 avg_loss = 3.21530\n",
      "epoch no.3 train no.249020  loss = 2.38638 avg_loss = 3.20207\n",
      "epoch no.3 train no.249030  loss = 3.34313 avg_loss = 3.19719\n",
      "epoch no.3 train no.249040  loss = 4.67214 avg_loss = 3.23694\n",
      "epoch no.3 train no.249050  loss = 4.64514 avg_loss = 3.24575\n",
      "epoch no.3 train no.249060  loss = 2.34473 avg_loss = 3.23966\n",
      "epoch no.3 train no.249070  loss = 2.21109 avg_loss = 3.28618\n",
      "epoch no.3 train no.249080  loss = 3.15136 avg_loss = 3.26973\n",
      "epoch no.3 train no.249090  loss = 2.92277 avg_loss = 3.28042\n",
      "epoch no.3 train no.249100  loss = 3.55432 avg_loss = 3.29370\n",
      "epoch no.3 train no.249110  loss = 2.67660 avg_loss = 3.30070\n",
      "epoch no.3 train no.249120  loss = 3.95291 avg_loss = 3.26493\n",
      "epoch no.3 train no.249130  loss = 2.01192 avg_loss = 3.21364\n",
      "epoch no.3 train no.249140  loss = 2.65933 avg_loss = 3.24420\n",
      "epoch no.3 train no.249150  loss = 3.55632 avg_loss = 3.24639\n",
      "epoch no.3 train no.249160  loss = 3.50455 avg_loss = 3.24557\n",
      "epoch no.3 train no.249170  loss = 2.76597 avg_loss = 3.25437\n",
      "epoch no.3 train no.249180  loss = 2.57397 avg_loss = 3.20867\n",
      "epoch no.3 train no.249190  loss = 2.47246 avg_loss = 3.23321\n",
      "epoch no.3 train no.249200  loss = 2.56607 avg_loss = 3.23354\n",
      "epoch no.3 train no.249210  loss = 4.30817 avg_loss = 3.26160\n",
      "epoch no.3 train no.249220  loss = 3.72791 avg_loss = 3.28610\n",
      "epoch no.3 train no.249230  loss = 2.13726 avg_loss = 3.28064\n",
      "epoch no.3 train no.249240  loss = 3.93589 avg_loss = 3.29520\n",
      "epoch no.3 train no.249250  loss = 4.24195 avg_loss = 3.31541\n",
      "epoch no.3 train no.249260  loss = 2.50025 avg_loss = 3.33349\n",
      "epoch no.3 train no.249270  loss = 4.49071 avg_loss = 3.37557\n",
      "epoch no.3 train no.249280  loss = 3.97223 avg_loss = 3.35088\n",
      "epoch no.3 train no.249290  loss = 3.80281 avg_loss = 3.36229\n",
      "epoch no.3 train no.249300  loss = 2.75288 avg_loss = 3.36168\n",
      "epoch no.3 train no.249310  loss = 4.59726 avg_loss = 3.33811\n",
      "epoch no.3 train no.249320  loss = 2.39245 avg_loss = 3.33075\n",
      "epoch no.3 train no.249330  loss = 3.69406 avg_loss = 3.33560\n",
      "epoch no.3 train no.249340  loss = 3.42303 avg_loss = 3.31699\n",
      "epoch no.3 train no.249350  loss = 3.94346 avg_loss = 3.33480\n",
      "epoch no.3 train no.249360  loss = 2.83063 avg_loss = 3.34430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.249370  loss = 2.14046 avg_loss = 3.28609\n",
      "epoch no.3 train no.249380  loss = 3.96689 avg_loss = 3.35678\n",
      "epoch no.3 train no.249390  loss = 3.88636 avg_loss = 3.34962\n",
      "epoch no.3 train no.249400  loss = 3.11618 avg_loss = 3.34990\n",
      "epoch no.3 train no.249410  loss = 2.38770 avg_loss = 3.32915\n",
      "epoch no.3 train no.249420  loss = 4.03837 avg_loss = 3.31474\n",
      "epoch no.3 train no.249430  loss = 2.73760 avg_loss = 3.31307\n",
      "epoch no.3 train no.249440  loss = 3.02435 avg_loss = 3.32105\n",
      "epoch no.3 train no.249450  loss = 4.31633 avg_loss = 3.32515\n",
      "epoch no.3 train no.249460  loss = 2.62746 avg_loss = 3.29331\n",
      "epoch no.3 train no.249470  loss = 2.87270 avg_loss = 3.31968\n",
      "epoch no.3 train no.249480  loss = 2.50932 avg_loss = 3.28074\n",
      "epoch no.3 train no.249490  loss = 2.28166 avg_loss = 3.28817\n",
      "epoch no.3 train no.249500  loss = 2.35306 avg_loss = 3.30367\n",
      "epoch no.3 train no.249510  loss = 3.06924 avg_loss = 3.31189\n",
      "epoch no.3 train no.249520  loss = 3.25634 avg_loss = 3.31732\n",
      "epoch no.3 train no.249530  loss = 2.77037 avg_loss = 3.30373\n",
      "epoch no.3 train no.249540  loss = 3.30057 avg_loss = 3.29375\n",
      "epoch no.3 train no.249550  loss = 3.07952 avg_loss = 3.26406\n",
      "epoch no.3 train no.249560  loss = 3.30184 avg_loss = 3.25146\n",
      "epoch no.3 train no.249570  loss = 5.31255 avg_loss = 3.28984\n",
      "epoch no.3 train no.249580  loss = 3.31739 avg_loss = 3.28703\n",
      "epoch no.3 train no.249590  loss = 3.70696 avg_loss = 3.25643\n",
      "epoch no.3 train no.249600  loss = 2.31325 avg_loss = 3.24412\n",
      "epoch no.3 train no.249610  loss = 3.61743 avg_loss = 3.29854\n",
      "epoch no.3 train no.249620  loss = 3.95738 avg_loss = 3.33648\n",
      "epoch no.3 train no.249630  loss = 3.51783 avg_loss = 3.30103\n",
      "epoch no.3 train no.249640  loss = 2.33565 avg_loss = 3.34012\n",
      "epoch no.3 train no.249650  loss = 4.91676 avg_loss = 3.35098\n",
      "epoch no.3 train no.249660  loss = 2.25912 avg_loss = 3.34518\n",
      "epoch no.3 train no.249670  loss = 3.61370 avg_loss = 3.34164\n",
      "epoch no.3 train no.249680  loss = 3.71281 avg_loss = 3.34703\n",
      "epoch no.3 train no.249690  loss = 2.31412 avg_loss = 3.32090\n",
      "epoch no.3 train no.249700  loss = 3.14015 avg_loss = 3.32958\n",
      "epoch no.3 train no.249710  loss = 3.32549 avg_loss = 3.30639\n",
      "epoch no.3 train no.249720  loss = 5.13033 avg_loss = 3.31436\n",
      "epoch no.3 train no.249730  loss = 2.80378 avg_loss = 3.28071\n",
      "epoch no.3 train no.249740  loss = 4.17443 avg_loss = 3.30711\n",
      "epoch no.3 train no.249750  loss = 2.09003 avg_loss = 3.25070\n",
      "epoch no.3 train no.249760  loss = 2.40009 avg_loss = 3.27697\n",
      "epoch no.3 train no.249770  loss = 3.48250 avg_loss = 3.31012\n",
      "epoch no.3 train no.249780  loss = 2.41679 avg_loss = 3.26284\n",
      "epoch no.3 train no.249790  loss = 2.41837 avg_loss = 3.22224\n",
      "epoch no.3 train no.249800  loss = 2.09689 avg_loss = 3.21664\n",
      "epoch no.3 train no.249810  loss = 3.89246 avg_loss = 3.27669\n",
      "epoch no.3 train no.249820  loss = 2.34837 avg_loss = 3.30173\n",
      "epoch no.3 train no.249830  loss = 5.17786 avg_loss = 3.30753\n",
      "epoch no.3 train no.249840  loss = 2.88126 avg_loss = 3.25675\n",
      "epoch no.3 train no.249850  loss = 3.00706 avg_loss = 3.27537\n",
      "epoch no.3 train no.249860  loss = 2.57033 avg_loss = 3.28023\n",
      "epoch no.3 train no.249870  loss = 3.71530 avg_loss = 3.30389\n",
      "epoch no.3 train no.249880  loss = 4.60254 avg_loss = 3.31693\n",
      "epoch no.3 train no.249890  loss = 2.96817 avg_loss = 3.28402\n",
      "epoch no.3 train no.249900  loss = 4.82443 avg_loss = 3.30864\n",
      "epoch no.3 train no.249910  loss = 4.22244 avg_loss = 3.31643\n",
      "epoch no.3 train no.249920  loss = 2.89110 avg_loss = 3.33368\n",
      "epoch no.3 train no.249930  loss = 5.38891 avg_loss = 3.30077\n",
      "epoch no.3 train no.249940  loss = 2.14384 avg_loss = 3.25673\n",
      "epoch no.3 train no.249950  loss = 1.93209 avg_loss = 3.27289\n",
      "epoch no.3 train no.249960  loss = 2.55499 avg_loss = 3.28887\n",
      "epoch no.3 train no.249970  loss = 3.80477 avg_loss = 3.29538\n",
      "epoch no.3 train no.249980  loss = 2.16212 avg_loss = 3.34112\n",
      "epoch no.3 train no.249990  loss = 2.84283 avg_loss = 3.33451\n",
      "epoch no.3 train no.250000  loss = 4.17614 avg_loss = 3.30548\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁플레이', '보', '리스트', '</s>']\n",
      "여름밤의 재즈 플레이리스트</s>\n",
      "epoch no.3 train no.250010  loss = 2.16456 avg_loss = 3.30232\n",
      "epoch no.3 train no.250020  loss = 3.11020 avg_loss = 3.34102\n",
      "epoch no.3 train no.250030  loss = 3.30565 avg_loss = 3.33085\n",
      "epoch no.3 train no.250040  loss = 1.82096 avg_loss = 3.29004\n",
      "epoch no.3 train no.250050  loss = 4.66851 avg_loss = 3.27684\n",
      "epoch no.3 train no.250060  loss = 3.02132 avg_loss = 3.27466\n",
      "epoch no.3 train no.250070  loss = 2.29052 avg_loss = 3.33307\n",
      "epoch no.3 train no.250080  loss = 2.86132 avg_loss = 3.32970\n",
      "epoch no.3 train no.250090  loss = 2.90362 avg_loss = 3.31164\n",
      "epoch no.3 train no.250100  loss = 3.84841 avg_loss = 3.33044\n",
      "epoch no.3 train no.250110  loss = 4.78728 avg_loss = 3.29880\n",
      "epoch no.3 train no.250120  loss = 5.36795 avg_loss = 3.31059\n",
      "epoch no.3 train no.250130  loss = 3.27524 avg_loss = 3.36391\n",
      "epoch no.3 train no.250140  loss = 3.00435 avg_loss = 3.36491\n",
      "epoch no.3 train no.250150  loss = 3.10005 avg_loss = 3.36258\n",
      "epoch no.3 train no.250160  loss = 3.01835 avg_loss = 3.39132\n",
      "epoch no.3 train no.250170  loss = 2.98839 avg_loss = 3.38892\n",
      "epoch no.3 train no.250180  loss = 2.63967 avg_loss = 3.38894\n",
      "epoch no.3 train no.250190  loss = 3.70596 avg_loss = 3.38994\n",
      "epoch no.3 train no.250200  loss = 1.98639 avg_loss = 3.35708\n",
      "epoch no.3 train no.250210  loss = 3.60134 avg_loss = 3.37171\n",
      "epoch no.3 train no.250220  loss = 4.22071 avg_loss = 3.40995\n",
      "epoch no.3 train no.250230  loss = 3.28313 avg_loss = 3.37329\n",
      "epoch no.3 train no.250240  loss = 2.17954 avg_loss = 3.36610\n",
      "epoch no.3 train no.250250  loss = 3.87148 avg_loss = 3.38715\n",
      "epoch no.3 train no.250260  loss = 3.81297 avg_loss = 3.40424\n",
      "epoch no.3 train no.250270  loss = 4.35923 avg_loss = 3.46794\n",
      "epoch no.3 train no.250280  loss = 3.57869 avg_loss = 3.42599\n",
      "epoch no.3 train no.250290  loss = 3.56140 avg_loss = 3.44071\n",
      "epoch no.3 train no.250300  loss = 2.40323 avg_loss = 3.38985\n",
      "epoch no.3 train no.250310  loss = 3.79029 avg_loss = 3.41825\n",
      "epoch no.3 train no.250320  loss = 2.97350 avg_loss = 3.37822\n",
      "epoch no.3 train no.250330  loss = 3.27453 avg_loss = 3.40951\n",
      "epoch no.3 train no.250340  loss = 4.18753 avg_loss = 3.39415\n",
      "epoch no.3 train no.250350  loss = 1.69298 avg_loss = 3.36768\n",
      "epoch no.3 train no.250360  loss = 2.13049 avg_loss = 3.36850\n",
      "epoch no.3 train no.250370  loss = 4.62556 avg_loss = 3.36017\n",
      "epoch no.3 train no.250380  loss = 1.87918 avg_loss = 3.32912\n",
      "epoch no.3 train no.250390  loss = 3.42682 avg_loss = 3.32272\n",
      "epoch no.3 train no.250400  loss = 2.68989 avg_loss = 3.34895\n",
      "epoch no.3 train no.250410  loss = 3.03415 avg_loss = 3.28928\n",
      "epoch no.3 train no.250420  loss = 2.26787 avg_loss = 3.24785\n",
      "epoch no.3 train no.250430  loss = 4.56705 avg_loss = 3.31408\n",
      "epoch no.3 train no.250440  loss = 2.16453 avg_loss = 3.29235\n",
      "epoch no.3 train no.250450  loss = 3.50251 avg_loss = 3.29049\n",
      "epoch no.3 train no.250460  loss = 3.35548 avg_loss = 3.23383\n",
      "epoch no.3 train no.250470  loss = 3.69288 avg_loss = 3.24842\n",
      "epoch no.3 train no.250480  loss = 3.04219 avg_loss = 3.26366\n",
      "epoch no.3 train no.250490  loss = 5.81854 avg_loss = 3.32060\n",
      "epoch no.3 train no.250500  loss = 3.18779 avg_loss = 3.32995\n",
      "epoch no.3 train no.250510  loss = 2.80300 avg_loss = 3.31811\n",
      "epoch no.3 train no.250520  loss = 5.89911 avg_loss = 3.35540\n",
      "epoch no.3 train no.250530  loss = 2.82680 avg_loss = 3.35185\n",
      "epoch no.3 train no.250540  loss = 5.69125 avg_loss = 3.41376\n",
      "epoch no.3 train no.250550  loss = 2.99937 avg_loss = 3.39302\n",
      "epoch no.3 train no.250560  loss = 3.43028 avg_loss = 3.35586\n",
      "epoch no.3 train no.250570  loss = 2.94014 avg_loss = 3.36142\n",
      "epoch no.3 train no.250580  loss = 3.72735 avg_loss = 3.36318\n",
      "epoch no.3 train no.250590  loss = 5.96256 avg_loss = 3.36521\n",
      "epoch no.3 train no.250600  loss = 5.87354 avg_loss = 3.38051\n",
      "epoch no.3 train no.250610  loss = 3.15426 avg_loss = 3.40336\n",
      "epoch no.3 train no.250620  loss = 5.99831 avg_loss = 3.40613\n",
      "epoch no.3 train no.250630  loss = 2.29567 avg_loss = 3.40098\n",
      "epoch no.3 train no.250640  loss = 5.86033 avg_loss = 3.44565\n",
      "epoch no.3 train no.250650  loss = 3.27751 avg_loss = 3.39996\n",
      "epoch no.3 train no.250660  loss = 3.03505 avg_loss = 3.41068\n",
      "epoch no.3 train no.250670  loss = 2.82229 avg_loss = 3.41574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.250680  loss = 4.61976 avg_loss = 3.45428\n",
      "epoch no.3 train no.250690  loss = 4.79629 avg_loss = 3.51658\n",
      "epoch no.3 train no.250700  loss = 3.02126 avg_loss = 3.48985\n",
      "epoch no.3 train no.250710  loss = 3.63959 avg_loss = 3.51203\n",
      "epoch no.3 train no.250720  loss = 2.87955 avg_loss = 3.44374\n",
      "epoch no.3 train no.250730  loss = 4.50130 avg_loss = 3.48785\n",
      "epoch no.3 train no.250740  loss = 2.49997 avg_loss = 3.48320\n",
      "epoch no.3 train no.250750  loss = 2.85893 avg_loss = 3.47356\n",
      "epoch no.3 train no.250760  loss = 3.13056 avg_loss = 3.46255\n",
      "epoch no.3 train no.250770  loss = 3.74562 avg_loss = 3.47801\n",
      "epoch no.3 train no.250780  loss = 3.13395 avg_loss = 3.45389\n",
      "epoch no.3 train no.250790  loss = 2.57649 avg_loss = 3.45462\n",
      "epoch no.3 train no.250800  loss = 4.11138 avg_loss = 3.45924\n",
      "epoch no.3 train no.250810  loss = 4.35484 avg_loss = 3.41062\n",
      "epoch no.3 train no.250820  loss = 3.10542 avg_loss = 3.39574\n",
      "epoch no.3 train no.250830  loss = 3.01214 avg_loss = 3.35556\n",
      "epoch no.3 train no.250840  loss = 2.54046 avg_loss = 3.35089\n",
      "epoch no.3 train no.250850  loss = 3.11342 avg_loss = 3.36269\n",
      "epoch no.3 train no.250860  loss = 2.85577 avg_loss = 3.38607\n",
      "epoch no.3 train no.250870  loss = 2.88493 avg_loss = 3.33539\n",
      "epoch no.3 train no.250880  loss = 5.54027 avg_loss = 3.38138\n",
      "epoch no.3 train no.250890  loss = 5.17600 avg_loss = 3.37864\n",
      "epoch no.3 train no.250900  loss = 3.41944 avg_loss = 3.36411\n",
      "epoch no.3 train no.250910  loss = 3.62125 avg_loss = 3.35830\n",
      "epoch no.3 train no.250920  loss = 3.48228 avg_loss = 3.36086\n",
      "epoch no.3 train no.250930  loss = 1.99719 avg_loss = 3.37233\n",
      "epoch no.3 train no.250940  loss = 2.79542 avg_loss = 3.42779\n",
      "epoch no.3 train no.250950  loss = 2.54805 avg_loss = 3.40095\n",
      "epoch no.3 train no.250960  loss = 5.02350 avg_loss = 3.40137\n",
      "epoch no.3 train no.250970  loss = 2.48781 avg_loss = 3.40118\n",
      "epoch no.3 train no.250980  loss = 4.24455 avg_loss = 3.42743\n",
      "epoch no.3 train no.250990  loss = 1.99313 avg_loss = 3.42714\n",
      "epoch no.3 train no.251000  loss = 2.26563 avg_loss = 3.34922\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '니까', '▁신나는', '▁노래', '</s>']\n",
      "여름이니까 신나는 노래</s>\n",
      "epoch no.3 train no.251010  loss = 2.71630 avg_loss = 3.32764\n",
      "epoch no.3 train no.251020  loss = 5.20119 avg_loss = 3.35131\n",
      "epoch no.3 train no.251030  loss = 2.20607 avg_loss = 3.33341\n",
      "epoch no.3 train no.251040  loss = 4.39603 avg_loss = 3.33092\n",
      "epoch no.3 train no.251050  loss = 3.18047 avg_loss = 3.36368\n",
      "epoch no.3 train no.251060  loss = 2.95040 avg_loss = 3.37359\n",
      "epoch no.3 train no.251070  loss = 2.27421 avg_loss = 3.38404\n",
      "epoch no.3 train no.251080  loss = 4.77620 avg_loss = 3.35995\n",
      "epoch no.3 train no.251090  loss = 3.65434 avg_loss = 3.35207\n",
      "epoch no.3 train no.251100  loss = 4.67798 avg_loss = 3.33918\n",
      "epoch no.3 train no.251110  loss = 5.75467 avg_loss = 3.39314\n",
      "epoch no.3 train no.251120  loss = 3.99875 avg_loss = 3.38595\n",
      "epoch no.3 train no.251130  loss = 2.48565 avg_loss = 3.32968\n",
      "epoch no.3 train no.251140  loss = 2.47258 avg_loss = 3.31702\n",
      "epoch no.3 train no.251150  loss = 2.92650 avg_loss = 3.34760\n",
      "epoch no.3 train no.251160  loss = 5.05350 avg_loss = 3.33995\n",
      "epoch no.3 train no.251170  loss = 2.66848 avg_loss = 3.34064\n",
      "epoch no.3 train no.251180  loss = 3.59974 avg_loss = 3.33731\n",
      "epoch no.3 train no.251190  loss = 2.04815 avg_loss = 3.29556\n",
      "epoch no.3 train no.251200  loss = 1.90928 avg_loss = 3.28975\n",
      "epoch no.3 train no.251210  loss = 5.12776 avg_loss = 3.29103\n",
      "epoch no.3 train no.251220  loss = 3.84237 avg_loss = 3.31362\n",
      "epoch no.3 train no.251230  loss = 4.53144 avg_loss = 3.30936\n",
      "epoch no.3 train no.251240  loss = 3.52959 avg_loss = 3.32925\n",
      "epoch no.3 train no.251250  loss = 4.90415 avg_loss = 3.33610\n",
      "epoch no.3 train no.251260  loss = 4.58065 avg_loss = 3.34390\n",
      "epoch no.3 train no.251270  loss = 3.95071 avg_loss = 3.37710\n",
      "epoch no.3 train no.251280  loss = 1.88636 avg_loss = 3.31623\n",
      "epoch no.3 train no.251290  loss = 2.79022 avg_loss = 3.35006\n",
      "epoch no.3 train no.251300  loss = 4.64435 avg_loss = 3.38479\n",
      "epoch no.3 train no.251310  loss = 4.11522 avg_loss = 3.38606\n",
      "epoch no.3 train no.251320  loss = 3.20845 avg_loss = 3.39541\n",
      "epoch no.3 train no.251330  loss = 4.42711 avg_loss = 3.40741\n",
      "epoch no.3 train no.251340  loss = 2.52968 avg_loss = 3.43131\n",
      "epoch no.3 train no.251350  loss = 2.75038 avg_loss = 3.39163\n",
      "epoch no.3 train no.251360  loss = 3.32479 avg_loss = 3.37502\n",
      "epoch no.3 train no.251370  loss = 2.60139 avg_loss = 3.37464\n",
      "epoch no.3 train no.251380  loss = 2.55843 avg_loss = 3.30935\n",
      "epoch no.3 train no.251390  loss = 2.60511 avg_loss = 3.27982\n",
      "epoch no.3 train no.251400  loss = 2.95676 avg_loss = 3.25337\n",
      "epoch no.3 train no.251410  loss = 3.34977 avg_loss = 3.29693\n",
      "epoch no.3 train no.251420  loss = 2.85262 avg_loss = 3.32092\n",
      "epoch no.3 train no.251430  loss = 2.02626 avg_loss = 3.28666\n",
      "epoch no.3 train no.251440  loss = 3.62388 avg_loss = 3.25173\n",
      "epoch no.3 train no.251450  loss = 4.61348 avg_loss = 3.27366\n",
      "epoch no.3 train no.251460  loss = 2.75198 avg_loss = 3.30316\n",
      "epoch no.3 train no.251470  loss = 4.13306 avg_loss = 3.32426\n",
      "epoch no.3 train no.251480  loss = 4.89058 avg_loss = 3.36394\n",
      "epoch no.3 train no.251490  loss = 3.09925 avg_loss = 3.34038\n",
      "epoch no.3 train no.251500  loss = 2.78725 avg_loss = 3.32325\n",
      "epoch no.3 train no.251510  loss = 4.15076 avg_loss = 3.36002\n",
      "epoch no.3 train no.251520  loss = 2.96558 avg_loss = 3.34063\n",
      "epoch no.3 train no.251530  loss = 2.96182 avg_loss = 3.33603\n",
      "epoch no.3 train no.251540  loss = 3.48007 avg_loss = 3.35487\n",
      "epoch no.3 train no.251550  loss = 2.43805 avg_loss = 3.35634\n",
      "epoch no.3 train no.251560  loss = 3.66347 avg_loss = 3.35359\n",
      "epoch no.3 train no.251570  loss = 3.10473 avg_loss = 3.36036\n",
      "epoch no.3 train no.251580  loss = 3.89474 avg_loss = 3.32054\n",
      "epoch no.3 train no.251590  loss = 3.02377 avg_loss = 3.34166\n",
      "epoch no.3 train no.251600  loss = 3.38601 avg_loss = 3.32563\n",
      "epoch no.3 train no.251610  loss = 3.59167 avg_loss = 3.36409\n",
      "epoch no.3 train no.251620  loss = 4.00503 avg_loss = 3.41534\n",
      "epoch no.3 train no.251630  loss = 3.22340 avg_loss = 3.35152\n",
      "epoch no.3 train no.251640  loss = 2.06628 avg_loss = 3.35275\n",
      "epoch no.3 train no.251650  loss = 2.78786 avg_loss = 3.31614\n",
      "epoch no.3 train no.251660  loss = 3.43920 avg_loss = 3.28608\n",
      "epoch no.3 train no.251670  loss = 3.17753 avg_loss = 3.29094\n",
      "epoch no.3 train no.251680  loss = 2.67670 avg_loss = 3.27191\n",
      "epoch no.3 train no.251690  loss = 2.62035 avg_loss = 3.19468\n",
      "epoch no.3 train no.251700  loss = 3.82596 avg_loss = 3.19496\n",
      "epoch no.3 train no.251710  loss = 4.50211 avg_loss = 3.21998\n",
      "epoch no.3 train no.251720  loss = 2.91962 avg_loss = 3.25357\n",
      "epoch no.3 train no.251730  loss = 3.03540 avg_loss = 3.24207\n",
      "epoch no.3 train no.251740  loss = 2.99985 avg_loss = 3.26484\n",
      "epoch no.3 train no.251750  loss = 4.65619 avg_loss = 3.30416\n",
      "epoch no.3 train no.251760  loss = 3.13274 avg_loss = 3.35180\n",
      "epoch no.3 train no.251770  loss = 2.26122 avg_loss = 3.32973\n",
      "epoch no.3 train no.251780  loss = 3.20164 avg_loss = 3.34335\n",
      "epoch no.3 train no.251790  loss = 3.00482 avg_loss = 3.34448\n",
      "epoch no.3 train no.251800  loss = 3.21652 avg_loss = 3.35761\n",
      "epoch no.3 train no.251810  loss = 2.64138 avg_loss = 3.34476\n",
      "epoch no.3 train no.251820  loss = 2.87186 avg_loss = 3.34538\n",
      "epoch no.3 train no.251830  loss = 3.20614 avg_loss = 3.30589\n",
      "epoch no.3 train no.251840  loss = 3.02717 avg_loss = 3.29720\n",
      "epoch no.3 train no.251850  loss = 3.77643 avg_loss = 3.34504\n",
      "epoch no.3 train no.251860  loss = 3.90852 avg_loss = 3.31500\n",
      "epoch no.3 train no.251870  loss = 2.96607 avg_loss = 3.26953\n",
      "epoch no.3 train no.251880  loss = 4.09864 avg_loss = 3.27632\n",
      "epoch no.3 train no.251890  loss = 5.96307 avg_loss = 3.26322\n",
      "epoch no.3 train no.251900  loss = 2.07414 avg_loss = 3.26150\n",
      "epoch no.3 train no.251910  loss = 2.52284 avg_loss = 3.27502\n",
      "epoch no.3 train no.251920  loss = 3.91330 avg_loss = 3.23517\n",
      "epoch no.3 train no.251930  loss = 2.72475 avg_loss = 3.21808\n",
      "epoch no.3 train no.251940  loss = 3.50671 avg_loss = 3.24285\n",
      "epoch no.3 train no.251950  loss = 4.48644 avg_loss = 3.33710\n",
      "epoch no.3 train no.251960  loss = 5.26997 avg_loss = 3.33635\n",
      "epoch no.3 train no.251970  loss = 4.07286 avg_loss = 3.33354\n",
      "epoch no.3 train no.251980  loss = 3.10351 avg_loss = 3.34517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.251990  loss = 3.59192 avg_loss = 3.29836\n",
      "epoch no.3 train no.252000  loss = 1.83048 avg_loss = 3.28548\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁재즈', '보', '▁시작하는', '하기', '</s>']\n",
      "여름밤의 재즈로 힐링하기</s>\n",
      "epoch no.3 train no.252010  loss = 2.06035 avg_loss = 3.25511\n",
      "epoch no.3 train no.252020  loss = 5.06463 avg_loss = 3.31040\n",
      "epoch no.3 train no.252030  loss = 2.25200 avg_loss = 3.32615\n",
      "epoch no.3 train no.252040  loss = 1.74330 avg_loss = 3.32469\n",
      "epoch no.3 train no.252050  loss = 3.16993 avg_loss = 3.31089\n",
      "epoch no.3 train no.252060  loss = 3.75054 avg_loss = 3.39805\n",
      "epoch no.3 train no.252070  loss = 3.90954 avg_loss = 3.38271\n",
      "epoch no.3 train no.252080  loss = 3.31187 avg_loss = 3.34828\n",
      "epoch no.3 train no.252090  loss = 3.73689 avg_loss = 3.30384\n",
      "epoch no.3 train no.252100  loss = 3.97263 avg_loss = 3.27275\n",
      "epoch no.3 train no.252110  loss = 3.16468 avg_loss = 3.28842\n",
      "epoch no.3 train no.252120  loss = 4.08408 avg_loss = 3.25939\n",
      "epoch no.3 train no.252130  loss = 2.37651 avg_loss = 3.26386\n",
      "epoch no.3 train no.252140  loss = 3.06411 avg_loss = 3.23080\n",
      "epoch no.3 train no.252150  loss = 3.53949 avg_loss = 3.21233\n",
      "epoch no.3 train no.252160  loss = 3.15038 avg_loss = 3.20684\n",
      "epoch no.3 train no.252170  loss = 3.72766 avg_loss = 3.18439\n",
      "epoch no.3 train no.252180  loss = 4.24606 avg_loss = 3.16515\n",
      "epoch no.3 train no.252190  loss = 2.63611 avg_loss = 3.18564\n",
      "epoch no.3 train no.252200  loss = 5.13719 avg_loss = 3.24129\n",
      "epoch no.3 train no.252210  loss = 2.20123 avg_loss = 3.21687\n",
      "epoch no.3 train no.252220  loss = 2.23422 avg_loss = 3.22720\n",
      "epoch no.3 train no.252230  loss = 4.13354 avg_loss = 3.27014\n",
      "epoch no.3 train no.252240  loss = 2.61386 avg_loss = 3.25697\n",
      "epoch no.3 train no.252250  loss = 2.89575 avg_loss = 3.22922\n",
      "epoch no.3 train no.252260  loss = 4.35914 avg_loss = 3.28660\n",
      "epoch no.3 train no.252270  loss = 2.84980 avg_loss = 3.25994\n",
      "epoch no.3 train no.252280  loss = 4.32687 avg_loss = 3.26055\n",
      "epoch no.3 train no.252290  loss = 2.41808 avg_loss = 3.25150\n",
      "epoch no.3 train no.252300  loss = 2.53301 avg_loss = 3.23649\n",
      "epoch no.3 train no.252310  loss = 1.72593 avg_loss = 3.20692\n",
      "epoch no.3 train no.252320  loss = 2.68960 avg_loss = 3.21369\n",
      "epoch no.3 train no.252330  loss = 2.42249 avg_loss = 3.20436\n",
      "epoch no.3 train no.252340  loss = 2.69981 avg_loss = 3.20463\n",
      "epoch no.3 train no.252350  loss = 3.04385 avg_loss = 3.20673\n",
      "epoch no.3 train no.252360  loss = 2.75964 avg_loss = 3.25230\n",
      "epoch no.3 train no.252370  loss = 2.55278 avg_loss = 3.19505\n",
      "epoch no.3 train no.252380  loss = 2.87241 avg_loss = 3.19413\n",
      "epoch no.3 train no.252390  loss = 3.34689 avg_loss = 3.20801\n",
      "epoch no.3 train no.252400  loss = 2.87765 avg_loss = 3.21749\n",
      "epoch no.3 train no.252410  loss = 2.27188 avg_loss = 3.17467\n",
      "epoch no.3 train no.252420  loss = 4.08492 avg_loss = 3.20539\n",
      "epoch no.3 train no.252430  loss = 4.56781 avg_loss = 3.24609\n",
      "epoch no.3 train no.252440  loss = 3.59809 avg_loss = 3.31589\n",
      "epoch no.3 train no.252450  loss = 3.34989 avg_loss = 3.36402\n",
      "epoch no.3 train no.252460  loss = 2.57215 avg_loss = 3.33969\n",
      "epoch no.3 train no.252470  loss = 3.30731 avg_loss = 3.32825\n",
      "epoch no.3 train no.252480  loss = 3.48055 avg_loss = 3.37520\n",
      "epoch no.3 train no.252490  loss = 4.32573 avg_loss = 3.36450\n",
      "epoch no.3 train no.252500  loss = 2.68422 avg_loss = 3.34159\n",
      "epoch no.3 train no.252510  loss = 2.95999 avg_loss = 3.30643\n",
      "epoch no.3 train no.252520  loss = 2.92825 avg_loss = 3.29363\n",
      "epoch no.3 train no.252530  loss = 4.22956 avg_loss = 3.31401\n",
      "epoch no.3 train no.252540  loss = 3.24430 avg_loss = 3.33051\n",
      "epoch no.3 train no.252550  loss = 2.90790 avg_loss = 3.32188\n",
      "epoch no.3 train no.252560  loss = 2.53582 avg_loss = 3.34920\n",
      "epoch no.3 train no.252570  loss = 2.27966 avg_loss = 3.34805\n",
      "epoch no.3 train no.252580  loss = 3.19595 avg_loss = 3.28828\n",
      "epoch no.3 train no.252590  loss = 3.02620 avg_loss = 3.29914\n",
      "epoch no.3 train no.252600  loss = 3.40464 avg_loss = 3.27437\n",
      "epoch no.3 train no.252610  loss = 3.00299 avg_loss = 3.28384\n",
      "epoch no.3 train no.252620  loss = 5.75325 avg_loss = 3.31292\n",
      "epoch no.3 train no.252630  loss = 1.91141 avg_loss = 3.32576\n",
      "epoch no.3 train no.252640  loss = 2.22352 avg_loss = 3.29115\n",
      "epoch no.3 train no.252650  loss = 3.22614 avg_loss = 3.28384\n",
      "epoch no.3 train no.252660  loss = 4.73830 avg_loss = 3.29806\n",
      "epoch no.3 train no.252670  loss = 3.97286 avg_loss = 3.27791\n",
      "epoch no.3 train no.252680  loss = 4.38285 avg_loss = 3.28249\n",
      "epoch no.3 train no.252690  loss = 2.87869 avg_loss = 3.29344\n",
      "epoch no.3 train no.252700  loss = 2.45032 avg_loss = 3.28163\n",
      "epoch no.3 train no.252710  loss = 3.67830 avg_loss = 3.26736\n",
      "epoch no.3 train no.252720  loss = 3.42295 avg_loss = 3.22497\n",
      "epoch no.3 train no.252730  loss = 2.00679 avg_loss = 3.22464\n",
      "epoch no.3 train no.252740  loss = 4.08055 avg_loss = 3.23727\n",
      "epoch no.3 train no.252750  loss = 1.77693 avg_loss = 3.18642\n",
      "epoch no.3 train no.252760  loss = 2.45878 avg_loss = 3.18814\n",
      "epoch no.3 train no.252770  loss = 2.89655 avg_loss = 3.18582\n",
      "epoch no.3 train no.252780  loss = 3.97250 avg_loss = 3.21155\n",
      "epoch no.3 train no.252790  loss = 4.29576 avg_loss = 3.27105\n",
      "epoch no.3 train no.252800  loss = 7.48024 avg_loss = 3.31423\n",
      "epoch no.3 train no.252810  loss = 6.35535 avg_loss = 3.35038\n",
      "epoch no.3 train no.252820  loss = 2.70013 avg_loss = 3.32961\n",
      "epoch no.3 train no.252830  loss = 2.38248 avg_loss = 3.29321\n",
      "epoch no.3 train no.252840  loss = 2.82479 avg_loss = 3.24171\n",
      "epoch no.3 train no.252850  loss = 2.61726 avg_loss = 3.20960\n",
      "epoch no.3 train no.252860  loss = 2.97736 avg_loss = 3.24615\n",
      "epoch no.3 train no.252870  loss = 2.40005 avg_loss = 3.22846\n",
      "epoch no.3 train no.252880  loss = 1.85459 avg_loss = 3.20082\n",
      "epoch no.3 train no.252890  loss = 4.32929 avg_loss = 3.22117\n",
      "epoch no.3 train no.252900  loss = 3.25427 avg_loss = 3.19769\n",
      "epoch no.3 train no.252910  loss = 2.12451 avg_loss = 3.18587\n",
      "epoch no.3 train no.252920  loss = 3.58790 avg_loss = 3.16647\n",
      "epoch no.3 train no.252930  loss = 2.66852 avg_loss = 3.21053\n",
      "epoch no.3 train no.252940  loss = 4.61352 avg_loss = 3.23142\n",
      "epoch no.3 train no.252950  loss = 4.22335 avg_loss = 3.22829\n",
      "epoch no.3 train no.252960  loss = 2.93109 avg_loss = 3.21329\n",
      "epoch no.3 train no.252970  loss = 2.37617 avg_loss = 3.19165\n",
      "epoch no.3 train no.252980  loss = 3.08962 avg_loss = 3.17896\n",
      "epoch no.3 train no.252990  loss = 4.12366 avg_loss = 3.20147\n",
      "epoch no.3 train no.253000  loss = 2.47439 avg_loss = 3.20642\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁산책', '들기', '▁드는', '▁당신을', '</s>']\n",
      "여름 밤 잠 못 이루는 밤</s>\n",
      "epoch no.3 train no.253010  loss = 3.17406 avg_loss = 3.24112\n",
      "epoch no.3 train no.253020  loss = 5.16121 avg_loss = 3.24928\n",
      "epoch no.3 train no.253030  loss = 3.46254 avg_loss = 3.23165\n",
      "epoch no.3 train no.253040  loss = 2.95132 avg_loss = 3.22407\n",
      "epoch no.3 train no.253050  loss = 4.48830 avg_loss = 3.23023\n",
      "epoch no.3 train no.253060  loss = 3.63429 avg_loss = 3.26329\n",
      "epoch no.3 train no.253070  loss = 2.37355 avg_loss = 3.23412\n",
      "epoch no.3 train no.253080  loss = 2.34211 avg_loss = 3.22994\n",
      "epoch no.3 train no.253090  loss = 2.66880 avg_loss = 3.20945\n",
      "epoch no.3 train no.253100  loss = 3.66784 avg_loss = 3.21298\n",
      "epoch no.3 train no.253110  loss = 4.12142 avg_loss = 3.16423\n",
      "epoch no.3 train no.253120  loss = 2.34153 avg_loss = 3.13656\n",
      "epoch no.3 train no.253130  loss = 3.79904 avg_loss = 3.17437\n",
      "epoch no.3 train no.253140  loss = 3.48474 avg_loss = 3.18576\n",
      "epoch no.3 train no.253150  loss = 4.15430 avg_loss = 3.15529\n",
      "epoch no.3 train no.253160  loss = 5.08039 avg_loss = 3.19467\n",
      "epoch no.3 train no.253170  loss = 2.51534 avg_loss = 3.23614\n",
      "epoch no.3 train no.253180  loss = 1.95391 avg_loss = 3.22384\n",
      "epoch no.3 train no.253190  loss = 3.41803 avg_loss = 3.25535\n",
      "epoch no.3 train no.253200  loss = 4.57804 avg_loss = 3.29304\n",
      "epoch no.3 train no.253210  loss = 5.23672 avg_loss = 3.30774\n",
      "epoch no.3 train no.253220  loss = 1.24888 avg_loss = 3.30082\n",
      "epoch no.3 train no.253230  loss = 2.32079 avg_loss = 3.32596\n",
      "epoch no.3 train no.253240  loss = 5.07343 avg_loss = 3.33498\n",
      "epoch no.3 train no.253250  loss = 3.08012 avg_loss = 3.31498\n",
      "epoch no.3 train no.253260  loss = 1.58637 avg_loss = 3.27379\n",
      "epoch no.3 train no.253270  loss = 3.14843 avg_loss = 3.26391\n",
      "epoch no.3 train no.253280  loss = 3.39302 avg_loss = 3.25143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.253290  loss = 4.87254 avg_loss = 3.32145\n",
      "epoch no.3 train no.253300  loss = 3.91871 avg_loss = 3.28548\n",
      "epoch no.3 train no.253310  loss = 3.05275 avg_loss = 3.27710\n",
      "epoch no.3 train no.253320  loss = 3.60914 avg_loss = 3.33528\n",
      "epoch no.3 train no.253330  loss = 2.55606 avg_loss = 3.32811\n",
      "epoch no.3 train no.253340  loss = 2.47901 avg_loss = 3.30999\n",
      "epoch no.3 train no.253350  loss = 2.72031 avg_loss = 3.32736\n",
      "epoch no.3 train no.253360  loss = 2.60708 avg_loss = 3.30755\n",
      "epoch no.3 train no.253370  loss = 2.03874 avg_loss = 3.33783\n",
      "epoch no.3 train no.253380  loss = 2.90576 avg_loss = 3.32093\n",
      "epoch no.3 train no.253390  loss = 2.21202 avg_loss = 3.32708\n",
      "epoch no.3 train no.253400  loss = 6.24339 avg_loss = 3.34793\n",
      "epoch no.3 train no.253410  loss = 2.79964 avg_loss = 3.28939\n",
      "epoch no.3 train no.253420  loss = 2.86676 avg_loss = 3.28498\n",
      "epoch no.3 train no.253430  loss = 2.05051 avg_loss = 3.27355\n",
      "epoch no.3 train no.253440  loss = 2.86729 avg_loss = 3.30750\n",
      "epoch no.3 train no.253450  loss = 5.00329 avg_loss = 3.34883\n",
      "epoch no.3 train no.253460  loss = 3.02085 avg_loss = 3.32137\n",
      "epoch no.3 train no.253470  loss = 2.97877 avg_loss = 3.37939\n",
      "epoch no.3 train no.253480  loss = 1.78482 avg_loss = 3.34950\n",
      "epoch no.3 train no.253490  loss = 3.62740 avg_loss = 3.31267\n",
      "epoch no.3 train no.253500  loss = 2.99683 avg_loss = 3.31037\n",
      "epoch no.3 train no.253510  loss = 3.21833 avg_loss = 3.29913\n",
      "epoch no.3 train no.253520  loss = 2.14209 avg_loss = 3.29135\n",
      "epoch no.3 train no.253530  loss = 3.60757 avg_loss = 3.28512\n",
      "epoch no.3 train no.253540  loss = 2.57371 avg_loss = 3.26844\n",
      "epoch no.3 train no.253550  loss = 2.12537 avg_loss = 3.29644\n",
      "epoch no.3 train no.253560  loss = 3.97456 avg_loss = 3.27599\n",
      "epoch no.3 train no.253570  loss = 3.42638 avg_loss = 3.28653\n",
      "epoch no.3 train no.253580  loss = 2.59856 avg_loss = 3.24686\n",
      "epoch no.3 train no.253590  loss = 2.22707 avg_loss = 3.20733\n",
      "epoch no.3 train no.253600  loss = 3.52481 avg_loss = 3.22058\n",
      "epoch no.3 train no.253610  loss = 3.35733 avg_loss = 3.20725\n",
      "epoch no.3 train no.253620  loss = 1.85700 avg_loss = 3.23544\n",
      "epoch no.3 train no.253630  loss = 2.62927 avg_loss = 3.29312\n",
      "epoch no.3 train no.253640  loss = 4.05112 avg_loss = 3.30685\n",
      "epoch no.3 train no.253650  loss = 2.86256 avg_loss = 3.29610\n",
      "epoch no.3 train no.253660  loss = 2.58203 avg_loss = 3.30476\n",
      "epoch no.3 train no.253670  loss = 4.78936 avg_loss = 3.31323\n",
      "epoch no.3 train no.253680  loss = 3.95414 avg_loss = 3.34716\n",
      "epoch no.3 train no.253690  loss = 3.88756 avg_loss = 3.38342\n",
      "epoch no.3 train no.253700  loss = 2.41375 avg_loss = 3.39229\n",
      "epoch no.3 train no.253710  loss = 5.14743 avg_loss = 3.39462\n",
      "epoch no.3 train no.253720  loss = 2.20881 avg_loss = 3.36703\n",
      "epoch no.3 train no.253730  loss = 1.96240 avg_loss = 3.35984\n",
      "epoch no.3 train no.253740  loss = 3.73674 avg_loss = 3.34150\n",
      "epoch no.3 train no.253750  loss = 2.90506 avg_loss = 3.30494\n",
      "epoch no.3 train no.253760  loss = 2.13248 avg_loss = 3.27946\n",
      "epoch no.3 train no.253770  loss = 2.90995 avg_loss = 3.28833\n",
      "epoch no.3 train no.253780  loss = 4.30163 avg_loss = 3.29679\n",
      "epoch no.3 train no.253790  loss = 2.55956 avg_loss = 3.30396\n",
      "epoch no.3 train no.253800  loss = 4.15595 avg_loss = 3.27786\n",
      "epoch no.3 train no.253810  loss = 5.35766 avg_loss = 3.28501\n",
      "epoch no.3 train no.253820  loss = 3.00450 avg_loss = 3.30728\n",
      "epoch no.3 train no.253830  loss = 3.69796 avg_loss = 3.30284\n",
      "epoch no.3 train no.253840  loss = 2.33864 avg_loss = 3.23715\n",
      "epoch no.3 train no.253850  loss = 1.75016 avg_loss = 3.23518\n",
      "epoch no.3 train no.253860  loss = 3.27344 avg_loss = 3.23659\n",
      "epoch no.3 train no.253870  loss = 3.57827 avg_loss = 3.26006\n",
      "epoch no.3 train no.253880  loss = 1.96807 avg_loss = 3.25667\n",
      "epoch no.3 train no.253890  loss = 3.72716 avg_loss = 3.27326\n",
      "epoch no.3 train no.253900  loss = 3.12420 avg_loss = 3.29966\n",
      "epoch no.3 train no.253910  loss = 3.59579 avg_loss = 3.26053\n",
      "epoch no.3 train no.253920  loss = 4.02957 avg_loss = 3.28064\n",
      "epoch no.3 train no.253930  loss = 4.58985 avg_loss = 3.30465\n",
      "epoch no.3 train no.253940  loss = 5.31547 avg_loss = 3.28224\n",
      "epoch no.3 train no.253950  loss = 4.90090 avg_loss = 3.35896\n",
      "epoch no.3 train no.253960  loss = 3.53050 avg_loss = 3.35666\n",
      "epoch no.3 train no.253970  loss = 3.35557 avg_loss = 3.37668\n",
      "epoch no.3 train no.253980  loss = 3.89055 avg_loss = 3.36066\n",
      "epoch no.3 train no.253990  loss = 4.25336 avg_loss = 3.38114\n",
      "epoch no.3 train no.254000  loss = 3.30299 avg_loss = 3.35799\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '니까', '▁신나는', '▁노래', '▁들어']\n",
      "여름이니까 시원한 노래를</s>\n",
      "epoch no.3 train no.254010  loss = 4.05590 avg_loss = 3.43999\n",
      "epoch no.3 train no.254020  loss = 2.63438 avg_loss = 3.39074\n",
      "epoch no.3 train no.254030  loss = 3.71527 avg_loss = 3.36098\n",
      "epoch no.3 train no.254040  loss = 4.02731 avg_loss = 3.37751\n",
      "epoch no.3 train no.254050  loss = 4.08964 avg_loss = 3.44808\n",
      "epoch no.3 train no.254060  loss = 3.66035 avg_loss = 3.46488\n",
      "epoch no.3 train no.254070  loss = 3.92977 avg_loss = 3.49269\n",
      "epoch no.3 train no.254080  loss = 4.36290 avg_loss = 3.47487\n",
      "epoch no.3 train no.254090  loss = 3.34086 avg_loss = 3.57207\n",
      "epoch no.3 train no.254100  loss = 2.34270 avg_loss = 3.52771\n",
      "epoch no.3 train no.254110  loss = 4.66021 avg_loss = 3.51171\n",
      "epoch no.3 train no.254120  loss = 4.96417 avg_loss = 3.50618\n",
      "epoch no.3 train no.254130  loss = 2.64491 avg_loss = 3.46118\n",
      "epoch no.3 train no.254140  loss = 1.63512 avg_loss = 3.41668\n",
      "epoch no.3 train no.254150  loss = 2.93505 avg_loss = 3.43720\n",
      "epoch no.3 train no.254160  loss = 2.77998 avg_loss = 3.44086\n",
      "epoch no.3 train no.254170  loss = 2.97542 avg_loss = 3.37154\n",
      "epoch no.3 train no.254180  loss = 2.62091 avg_loss = 3.39015\n",
      "epoch no.3 train no.254190  loss = 3.16439 avg_loss = 3.44048\n",
      "epoch no.3 train no.254200  loss = 3.89226 avg_loss = 3.40699\n",
      "epoch no.3 train no.254210  loss = 3.45572 avg_loss = 3.38070\n",
      "epoch no.3 train no.254220  loss = 2.12405 avg_loss = 3.37466\n",
      "epoch no.3 train no.254230  loss = 2.79247 avg_loss = 3.38789\n",
      "epoch no.3 train no.254240  loss = 2.87574 avg_loss = 3.44333\n",
      "epoch no.3 train no.254250  loss = 2.32875 avg_loss = 3.45915\n",
      "epoch no.3 train no.254260  loss = 4.19838 avg_loss = 3.49358\n",
      "epoch no.3 train no.254270  loss = 3.57862 avg_loss = 3.50826\n",
      "epoch no.3 train no.254280  loss = 4.34343 avg_loss = 3.47759\n",
      "epoch no.3 train no.254290  loss = 3.01337 avg_loss = 3.48266\n",
      "epoch no.3 train no.254300  loss = 3.63870 avg_loss = 3.44845\n",
      "epoch no.3 train no.254310  loss = 2.63820 avg_loss = 3.44544\n",
      "epoch no.3 train no.254320  loss = 3.58442 avg_loss = 3.46577\n",
      "epoch no.3 train no.254330  loss = 2.30721 avg_loss = 3.45794\n",
      "epoch no.3 train no.254340  loss = 3.65624 avg_loss = 3.45692\n",
      "epoch no.3 train no.254350  loss = 2.88444 avg_loss = 3.43587\n",
      "epoch no.3 train no.254360  loss = 2.05681 avg_loss = 3.43508\n",
      "epoch no.3 train no.254370  loss = 4.99025 avg_loss = 3.46854\n",
      "epoch no.3 train no.254380  loss = 2.31133 avg_loss = 3.42180\n",
      "epoch no.3 train no.254390  loss = 2.29747 avg_loss = 3.37053\n",
      "epoch no.3 train no.254400  loss = 3.49551 avg_loss = 3.35549\n",
      "epoch no.3 train no.254410  loss = 3.38342 avg_loss = 3.32087\n",
      "epoch no.3 train no.254420  loss = 2.71829 avg_loss = 3.22961\n",
      "epoch no.3 train no.254430  loss = 3.57661 avg_loss = 3.22241\n",
      "epoch no.3 train no.254440  loss = 4.06222 avg_loss = 3.23206\n",
      "epoch no.3 train no.254450  loss = 2.98887 avg_loss = 3.22028\n",
      "epoch no.3 train no.254460  loss = 3.46146 avg_loss = 3.16723\n",
      "epoch no.3 train no.254470  loss = 4.46456 avg_loss = 3.15069\n",
      "epoch no.3 train no.254480  loss = 3.43130 avg_loss = 3.14844\n",
      "epoch no.3 train no.254490  loss = 2.57160 avg_loss = 3.16708\n",
      "epoch no.3 train no.254500  loss = 3.58110 avg_loss = 3.12107\n",
      "epoch no.3 train no.254510  loss = 3.13859 avg_loss = 3.13356\n",
      "epoch no.3 train no.254520  loss = 1.92632 avg_loss = 3.11918\n",
      "epoch no.3 train no.254530  loss = 3.41720 avg_loss = 3.15596\n",
      "epoch no.3 train no.254540  loss = 3.88191 avg_loss = 3.16621\n",
      "epoch no.3 train no.254550  loss = 3.53773 avg_loss = 3.15330\n",
      "epoch no.3 train no.254560  loss = 3.87218 avg_loss = 3.17167\n",
      "epoch no.3 train no.254570  loss = 1.15774 avg_loss = 3.15874\n",
      "epoch no.3 train no.254580  loss = 2.61892 avg_loss = 3.19793\n",
      "epoch no.3 train no.254590  loss = 3.17425 avg_loss = 3.25877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.254600  loss = 3.00037 avg_loss = 3.26897\n",
      "epoch no.3 train no.254610  loss = 3.46627 avg_loss = 3.26219\n",
      "epoch no.3 train no.254620  loss = 3.87544 avg_loss = 3.25744\n",
      "epoch no.3 train no.254630  loss = 3.61877 avg_loss = 3.32755\n",
      "epoch no.3 train no.254640  loss = 1.92281 avg_loss = 3.31246\n",
      "epoch no.3 train no.254650  loss = 4.90665 avg_loss = 3.32191\n",
      "epoch no.3 train no.254660  loss = 4.49554 avg_loss = 3.32864\n",
      "epoch no.3 train no.254670  loss = 3.01600 avg_loss = 3.33124\n",
      "epoch no.3 train no.254680  loss = 4.62590 avg_loss = 3.32616\n",
      "epoch no.3 train no.254690  loss = 2.63561 avg_loss = 3.31034\n",
      "epoch no.3 train no.254700  loss = 3.37031 avg_loss = 3.30114\n",
      "epoch no.3 train no.254710  loss = 2.21511 avg_loss = 3.27570\n",
      "epoch no.3 train no.254720  loss = 2.31369 avg_loss = 3.25112\n",
      "epoch no.3 train no.254730  loss = 4.91249 avg_loss = 3.22819\n",
      "epoch no.3 train no.254740  loss = 3.74568 avg_loss = 3.18910\n",
      "epoch no.3 train no.254750  loss = 2.37933 avg_loss = 3.26467\n",
      "epoch no.3 train no.254760  loss = 3.75631 avg_loss = 3.25694\n",
      "epoch no.3 train no.254770  loss = 3.20184 avg_loss = 3.23460\n",
      "epoch no.3 train no.254780  loss = 3.00741 avg_loss = 3.24045\n",
      "epoch no.3 train no.254790  loss = 2.43876 avg_loss = 3.25366\n",
      "epoch no.3 train no.254800  loss = 3.25336 avg_loss = 3.29889\n",
      "epoch no.3 train no.254810  loss = 2.70648 avg_loss = 3.28142\n",
      "epoch no.3 train no.254820  loss = 2.11515 avg_loss = 3.28952\n",
      "epoch no.3 train no.254830  loss = 3.40529 avg_loss = 3.24078\n",
      "epoch no.3 train no.254840  loss = 2.62204 avg_loss = 3.22912\n",
      "epoch no.3 train no.254850  loss = 4.40970 avg_loss = 3.20924\n",
      "epoch no.3 train no.254860  loss = 3.98835 avg_loss = 3.23321\n",
      "epoch no.3 train no.254870  loss = 4.05841 avg_loss = 3.24032\n",
      "epoch no.3 train no.254880  loss = 2.37345 avg_loss = 3.20510\n",
      "epoch no.3 train no.254890  loss = 3.14511 avg_loss = 3.19528\n",
      "epoch no.3 train no.254900  loss = 2.91423 avg_loss = 3.23502\n",
      "epoch no.3 train no.254910  loss = 4.77720 avg_loss = 3.26076\n",
      "epoch no.3 train no.254920  loss = 5.16330 avg_loss = 3.26669\n",
      "epoch no.3 train no.254930  loss = 4.73425 avg_loss = 3.26329\n",
      "epoch no.3 train no.254940  loss = 3.64910 avg_loss = 3.25932\n",
      "epoch no.3 train no.254950  loss = 2.15534 avg_loss = 3.24578\n",
      "epoch no.3 train no.254960  loss = 2.06409 avg_loss = 3.25436\n",
      "epoch no.3 train no.254970  loss = 3.27197 avg_loss = 3.28968\n",
      "epoch no.3 train no.254980  loss = 3.13612 avg_loss = 3.28095\n",
      "epoch no.3 train no.254990  loss = 3.07642 avg_loss = 3.28791\n",
      "epoch no.3 train no.255000  loss = 3.89608 avg_loss = 3.31041\n",
      "8\n",
      "to_tokens: ['▁가을', '밤', '▁끝', '자', '락', '</s>', '▁가을', '의', '▁끝', '</s>']\n",
      "여름의 끝자락에서 가을의 끝까지</s>\n",
      "epoch no.3 train no.255010  loss = 4.27598 avg_loss = 3.31651\n",
      "epoch no.3 train no.255020  loss = 1.10319 avg_loss = 3.27172\n",
      "epoch no.3 train no.255030  loss = 2.02454 avg_loss = 3.26027\n",
      "epoch no.3 train no.255040  loss = 2.25331 avg_loss = 3.24118\n",
      "epoch no.3 train no.255050  loss = 2.86975 avg_loss = 3.17973\n",
      "epoch no.3 train no.255060  loss = 3.96243 avg_loss = 3.21833\n",
      "epoch no.3 train no.255070  loss = 4.21415 avg_loss = 3.21717\n",
      "epoch no.3 train no.255080  loss = 4.33461 avg_loss = 3.21657\n",
      "epoch no.3 train no.255090  loss = 2.27634 avg_loss = 3.20473\n",
      "epoch no.3 train no.255100  loss = 2.87090 avg_loss = 3.17286\n",
      "epoch no.3 train no.255110  loss = 3.45396 avg_loss = 3.14660\n",
      "epoch no.3 train no.255120  loss = 3.76682 avg_loss = 3.14024\n",
      "epoch no.3 train no.255130  loss = 3.73185 avg_loss = 3.16511\n",
      "epoch no.3 train no.255140  loss = 3.32649 avg_loss = 3.21499\n",
      "epoch no.3 train no.255150  loss = 4.27133 avg_loss = 3.24039\n",
      "epoch no.3 train no.255160  loss = 3.15827 avg_loss = 3.23013\n",
      "epoch no.3 train no.255170  loss = 3.42330 avg_loss = 3.26730\n",
      "epoch no.3 train no.255180  loss = 3.11316 avg_loss = 3.33474\n",
      "epoch no.3 train no.255190  loss = 4.42455 avg_loss = 3.37049\n",
      "epoch no.3 train no.255200  loss = 1.96949 avg_loss = 3.35430\n",
      "epoch no.3 train no.255210  loss = 4.04106 avg_loss = 3.37058\n",
      "epoch no.3 train no.255220  loss = 2.44149 avg_loss = 3.38999\n",
      "epoch no.3 train no.255230  loss = 4.73480 avg_loss = 3.42060\n",
      "epoch no.3 train no.255240  loss = 3.76080 avg_loss = 3.41080\n",
      "epoch no.3 train no.255250  loss = 3.96672 avg_loss = 3.43283\n",
      "epoch no.3 train no.255260  loss = 2.70955 avg_loss = 3.38017\n",
      "epoch no.3 train no.255270  loss = 3.06875 avg_loss = 3.39195\n",
      "epoch no.3 train no.255280  loss = 2.73616 avg_loss = 3.38613\n",
      "epoch no.3 train no.255290  loss = 3.98335 avg_loss = 3.35893\n",
      "epoch no.3 train no.255300  loss = 2.65011 avg_loss = 3.34877\n",
      "epoch no.3 train no.255310  loss = 2.02706 avg_loss = 3.39384\n",
      "epoch no.3 train no.255320  loss = 2.77152 avg_loss = 3.37902\n",
      "epoch no.3 train no.255330  loss = 4.27083 avg_loss = 3.37377\n",
      "epoch no.3 train no.255340  loss = 3.92768 avg_loss = 3.40655\n",
      "epoch no.3 train no.255350  loss = 2.80638 avg_loss = 3.39976\n",
      "epoch no.3 train no.255360  loss = 2.55276 avg_loss = 3.39214\n",
      "epoch no.3 train no.255370  loss = 3.40324 avg_loss = 3.37191\n",
      "epoch no.3 train no.255380  loss = 3.61831 avg_loss = 3.34263\n",
      "epoch no.3 train no.255390  loss = 4.28931 avg_loss = 3.31185\n",
      "epoch no.3 train no.255400  loss = 5.91902 avg_loss = 3.36692\n",
      "epoch no.3 train no.255410  loss = 3.63273 avg_loss = 3.35135\n",
      "epoch no.3 train no.255420  loss = 3.29313 avg_loss = 3.33189\n",
      "epoch no.3 train no.255430  loss = 4.15357 avg_loss = 3.39381\n",
      "epoch no.3 train no.255440  loss = 2.86043 avg_loss = 3.38971\n",
      "epoch no.3 train no.255450  loss = 4.01042 avg_loss = 3.38618\n",
      "epoch no.3 train no.255460  loss = 2.78500 avg_loss = 3.43426\n",
      "epoch no.3 train no.255470  loss = 2.79835 avg_loss = 3.41798\n",
      "epoch no.3 train no.255480  loss = 1.84900 avg_loss = 3.44606\n",
      "epoch no.3 train no.255490  loss = 3.58444 avg_loss = 3.37527\n",
      "epoch no.3 train no.255500  loss = 3.88953 avg_loss = 3.35041\n",
      "epoch no.3 train no.255510  loss = 2.65886 avg_loss = 3.33972\n",
      "epoch no.3 train no.255520  loss = 2.73752 avg_loss = 3.30141\n",
      "epoch no.3 train no.255530  loss = 2.40176 avg_loss = 3.30463\n",
      "epoch no.3 train no.255540  loss = 3.31835 avg_loss = 3.29027\n",
      "epoch no.3 train no.255550  loss = 1.93228 avg_loss = 3.29117\n",
      "epoch no.3 train no.255560  loss = 1.95933 avg_loss = 3.25212\n",
      "epoch no.3 train no.255570  loss = 5.08823 avg_loss = 3.32713\n",
      "epoch no.3 train no.255580  loss = 1.76136 avg_loss = 3.31782\n",
      "epoch no.3 train no.255590  loss = 3.48185 avg_loss = 3.33973\n",
      "epoch no.3 train no.255600  loss = 3.14390 avg_loss = 3.32609\n",
      "epoch no.3 train no.255610  loss = 4.16935 avg_loss = 3.33047\n",
      "epoch no.3 train no.255620  loss = 3.23868 avg_loss = 3.29057\n",
      "epoch no.3 train no.255630  loss = 3.23846 avg_loss = 3.28822\n",
      "epoch no.3 train no.255640  loss = 4.54137 avg_loss = 3.33464\n",
      "epoch no.3 train no.255650  loss = 3.63498 avg_loss = 3.36509\n",
      "epoch no.3 train no.255660  loss = 4.11058 avg_loss = 3.44626\n",
      "epoch no.3 train no.255670  loss = 2.53072 avg_loss = 3.40971\n",
      "epoch no.3 train no.255680  loss = 2.99091 avg_loss = 3.37308\n",
      "epoch no.3 train no.255690  loss = 4.32932 avg_loss = 3.38611\n",
      "epoch no.3 train no.255700  loss = 2.92285 avg_loss = 3.36631\n",
      "epoch no.3 train no.255710  loss = 4.34383 avg_loss = 3.38634\n",
      "epoch no.3 train no.255720  loss = 3.87362 avg_loss = 3.35615\n",
      "epoch no.3 train no.255730  loss = 3.43650 avg_loss = 3.36554\n",
      "epoch no.3 train no.255740  loss = 3.14711 avg_loss = 3.33854\n",
      "epoch no.3 train no.255750  loss = 3.43556 avg_loss = 3.33188\n",
      "epoch no.3 train no.255760  loss = 4.04204 avg_loss = 3.31397\n",
      "epoch no.3 train no.255770  loss = 3.02123 avg_loss = 3.30500\n",
      "epoch no.3 train no.255780  loss = 3.55246 avg_loss = 3.31707\n",
      "epoch no.3 train no.255790  loss = 2.00841 avg_loss = 3.25942\n",
      "epoch no.3 train no.255800  loss = 3.52942 avg_loss = 3.31103\n",
      "epoch no.3 train no.255810  loss = 4.90208 avg_loss = 3.32828\n",
      "epoch no.3 train no.255820  loss = 3.94623 avg_loss = 3.31534\n",
      "epoch no.3 train no.255830  loss = 3.50638 avg_loss = 3.31153\n",
      "epoch no.3 train no.255840  loss = 1.79271 avg_loss = 3.30621\n",
      "epoch no.3 train no.255850  loss = 2.83378 avg_loss = 3.28781\n",
      "epoch no.3 train no.255860  loss = 1.95153 avg_loss = 3.26422\n",
      "epoch no.3 train no.255870  loss = 3.07084 avg_loss = 3.28916\n",
      "epoch no.3 train no.255880  loss = 3.91023 avg_loss = 3.38396\n",
      "epoch no.3 train no.255890  loss = 3.03292 avg_loss = 3.33887\n",
      "epoch no.3 train no.255900  loss = 3.11091 avg_loss = 3.32985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.255910  loss = 3.95181 avg_loss = 3.34553\n",
      "epoch no.3 train no.255920  loss = 2.56146 avg_loss = 3.32794\n",
      "epoch no.3 train no.255930  loss = 2.56762 avg_loss = 3.32723\n",
      "epoch no.3 train no.255940  loss = 2.87370 avg_loss = 3.30814\n",
      "epoch no.3 train no.255950  loss = 2.87727 avg_loss = 3.26427\n",
      "epoch no.3 train no.255960  loss = 2.67836 avg_loss = 3.30475\n",
      "epoch no.3 train no.255970  loss = 4.44422 avg_loss = 3.31167\n",
      "epoch no.3 train no.255980  loss = 4.39679 avg_loss = 3.38382\n",
      "epoch no.3 train no.255990  loss = 2.26735 avg_loss = 3.39108\n",
      "epoch no.3 train no.256000  loss = 3.42380 avg_loss = 3.43462\n",
      "9\n",
      "to_tokens: ['▁비', '밤', '▁끝', '자', '락', '</s>', '▁가을', '의', '▁문턱', '자', '</s>']\n",
      "여름의 끝자락에서 가을의 끝에서</s>\n",
      "epoch no.3 train no.256010  loss = 2.66435 avg_loss = 3.49380\n",
      "epoch no.3 train no.256020  loss = 5.66754 avg_loss = 3.49696\n",
      "epoch no.3 train no.256030  loss = 4.41237 avg_loss = 3.51306\n",
      "epoch no.3 train no.256040  loss = 3.67420 avg_loss = 3.50596\n",
      "epoch no.3 train no.256050  loss = 3.18360 avg_loss = 3.49081\n",
      "epoch no.3 train no.256060  loss = 2.35987 avg_loss = 3.46794\n",
      "epoch no.3 train no.256070  loss = 4.62742 avg_loss = 3.45780\n",
      "epoch no.3 train no.256080  loss = 2.18441 avg_loss = 3.39596\n",
      "epoch no.3 train no.256090  loss = 3.49236 avg_loss = 3.37301\n",
      "epoch no.3 train no.256100  loss = 4.80516 avg_loss = 3.40532\n",
      "epoch no.3 train no.256110  loss = 2.01114 avg_loss = 3.41434\n",
      "epoch no.3 train no.256120  loss = 2.20895 avg_loss = 3.41587\n",
      "epoch no.3 train no.256130  loss = 2.33881 avg_loss = 3.39625\n",
      "epoch no.3 train no.256140  loss = 2.42074 avg_loss = 3.37708\n",
      "epoch no.3 train no.256150  loss = 3.93101 avg_loss = 3.36967\n",
      "epoch no.3 train no.256160  loss = 3.85570 avg_loss = 3.36164\n",
      "epoch no.3 train no.256170  loss = 3.48004 avg_loss = 3.34272\n",
      "epoch no.3 train no.256180  loss = 3.63104 avg_loss = 3.34551\n",
      "epoch no.3 train no.256190  loss = 2.61381 avg_loss = 3.27411\n",
      "epoch no.3 train no.256200  loss = 2.81586 avg_loss = 3.23783\n",
      "epoch no.3 train no.256210  loss = 3.46461 avg_loss = 3.21930\n",
      "epoch no.3 train no.256220  loss = 5.12023 avg_loss = 3.24631\n",
      "epoch no.3 train no.256230  loss = 2.62260 avg_loss = 3.24530\n",
      "epoch no.3 train no.256240  loss = 2.47677 avg_loss = 3.25047\n",
      "epoch no.3 train no.256250  loss = 4.66722 avg_loss = 3.28151\n",
      "epoch no.3 train no.256260  loss = 2.00377 avg_loss = 3.22022\n",
      "epoch no.3 train no.256270  loss = 3.10813 avg_loss = 3.22977\n",
      "epoch no.3 train no.256280  loss = 3.99706 avg_loss = 3.24539\n",
      "epoch no.3 train no.256290  loss = 2.41384 avg_loss = 3.28582\n",
      "epoch no.3 train no.256300  loss = 2.32335 avg_loss = 3.32594\n",
      "epoch no.3 train no.256310  loss = 4.02658 avg_loss = 3.29486\n",
      "epoch no.3 train no.256320  loss = 2.02058 avg_loss = 3.31995\n",
      "epoch no.3 train no.256330  loss = 3.22864 avg_loss = 3.35889\n",
      "epoch no.3 train no.256340  loss = 2.80634 avg_loss = 3.32468\n",
      "epoch no.3 train no.256350  loss = 2.14126 avg_loss = 3.33718\n",
      "epoch no.3 train no.256360  loss = 2.25770 avg_loss = 3.36409\n",
      "epoch no.3 train no.256370  loss = 3.60932 avg_loss = 3.33595\n",
      "epoch no.3 train no.256380  loss = 3.88629 avg_loss = 3.34446\n",
      "epoch no.3 train no.256390  loss = 3.90592 avg_loss = 3.33711\n",
      "epoch no.3 train no.256400  loss = 2.05239 avg_loss = 3.32580\n",
      "epoch no.3 train no.256410  loss = 3.54933 avg_loss = 3.27158\n",
      "epoch no.3 train no.256420  loss = 2.10852 avg_loss = 3.21055\n",
      "epoch no.3 train no.256430  loss = 3.31806 avg_loss = 3.25407\n",
      "epoch no.3 train no.256440  loss = 4.05390 avg_loss = 3.31793\n",
      "epoch no.3 train no.256450  loss = 2.79713 avg_loss = 3.30359\n",
      "epoch no.3 train no.256460  loss = 1.88134 avg_loss = 3.27460\n",
      "epoch no.3 train no.256470  loss = 2.86424 avg_loss = 3.27032\n",
      "epoch no.3 train no.256480  loss = 4.45438 avg_loss = 3.28813\n",
      "epoch no.3 train no.256490  loss = 5.60345 avg_loss = 3.31447\n",
      "epoch no.3 train no.256500  loss = 2.28310 avg_loss = 3.24409\n",
      "epoch no.3 train no.256510  loss = 3.12876 avg_loss = 3.28059\n",
      "epoch no.3 train no.256520  loss = 3.07202 avg_loss = 3.27830\n",
      "epoch no.3 train no.256530  loss = 2.67398 avg_loss = 3.24903\n",
      "epoch no.3 train no.256540  loss = 3.64757 avg_loss = 3.25487\n",
      "epoch no.3 train no.256550  loss = 2.79773 avg_loss = 3.20833\n",
      "epoch no.3 train no.256560  loss = 4.68115 avg_loss = 3.20418\n",
      "epoch no.3 train no.256570  loss = 3.42569 avg_loss = 3.24843\n",
      "epoch no.3 train no.256580  loss = 6.08784 avg_loss = 3.23051\n",
      "epoch no.3 train no.256590  loss = 2.40046 avg_loss = 3.25487\n",
      "epoch no.3 train no.256600  loss = 3.29000 avg_loss = 3.26239\n",
      "epoch no.3 train no.256610  loss = 3.79570 avg_loss = 3.26450\n",
      "epoch no.3 train no.256620  loss = 2.80928 avg_loss = 3.23352\n",
      "epoch no.3 train no.256630  loss = 2.94096 avg_loss = 3.27841\n",
      "epoch no.3 train no.256640  loss = 3.96630 avg_loss = 3.30821\n",
      "epoch no.3 train no.256650  loss = 2.56673 avg_loss = 3.35176\n",
      "epoch no.3 train no.256660  loss = 1.94992 avg_loss = 3.32809\n",
      "epoch no.3 train no.256670  loss = 2.81205 avg_loss = 3.33048\n",
      "epoch no.3 train no.256680  loss = 4.08658 avg_loss = 3.35992\n",
      "epoch no.3 train no.256690  loss = 1.87171 avg_loss = 3.40827\n",
      "epoch no.3 train no.256700  loss = 3.22533 avg_loss = 3.37789\n",
      "epoch no.3 train no.256710  loss = 2.88959 avg_loss = 3.37317\n",
      "epoch no.3 train no.256720  loss = 3.23346 avg_loss = 3.37541\n",
      "epoch no.3 train no.256730  loss = 3.00518 avg_loss = 3.35833\n",
      "epoch no.3 train no.256740  loss = 2.63040 avg_loss = 3.34943\n",
      "epoch no.3 train no.256750  loss = 2.25074 avg_loss = 3.29875\n",
      "epoch no.3 train no.256760  loss = 3.17519 avg_loss = 3.28368\n",
      "epoch no.3 train no.256770  loss = 3.66367 avg_loss = 3.29299\n",
      "epoch no.3 train no.256780  loss = 1.88486 avg_loss = 3.25073\n",
      "epoch no.3 train no.256790  loss = 2.33076 avg_loss = 3.23131\n",
      "epoch no.3 train no.256800  loss = 4.44712 avg_loss = 3.29004\n",
      "epoch no.3 train no.256810  loss = 1.99686 avg_loss = 3.26068\n",
      "epoch no.3 train no.256820  loss = 4.08668 avg_loss = 3.23084\n",
      "epoch no.3 train no.256830  loss = 5.32042 avg_loss = 3.24865\n",
      "epoch no.3 train no.256840  loss = 2.43606 avg_loss = 3.25438\n",
      "epoch no.3 train no.256850  loss = 2.44968 avg_loss = 3.20919\n",
      "epoch no.3 train no.256860  loss = 3.30411 avg_loss = 3.23608\n",
      "epoch no.3 train no.256870  loss = 3.51402 avg_loss = 3.24211\n",
      "epoch no.3 train no.256880  loss = 2.75951 avg_loss = 3.23936\n",
      "epoch no.3 train no.256890  loss = 3.32155 avg_loss = 3.22808\n",
      "epoch no.3 train no.256900  loss = 6.20692 avg_loss = 3.29175\n",
      "epoch no.3 train no.256910  loss = 3.20273 avg_loss = 3.28775\n",
      "epoch no.3 train no.256920  loss = 4.49297 avg_loss = 3.30432\n",
      "epoch no.3 train no.256930  loss = 2.04487 avg_loss = 3.26833\n",
      "epoch no.3 train no.256940  loss = 3.55878 avg_loss = 3.27108\n",
      "epoch no.3 train no.256950  loss = 3.94788 avg_loss = 3.29564\n",
      "epoch no.3 train no.256960  loss = 1.61049 avg_loss = 3.29686\n",
      "epoch no.3 train no.256970  loss = 3.33731 avg_loss = 3.33121\n",
      "epoch no.3 train no.256980  loss = 2.57624 avg_loss = 3.35833\n",
      "epoch no.3 train no.256990  loss = 2.44193 avg_loss = 3.32908\n",
      "epoch no.3 train no.257000  loss = 3.40817 avg_loss = 3.33073\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '힙', '컬', '</s>']\n",
      "여름밤의 재즈보컬</s>\n",
      "epoch no.3 train no.257010  loss = 3.55562 avg_loss = 3.32090\n",
      "epoch no.3 train no.257020  loss = 4.97322 avg_loss = 3.32899\n",
      "epoch no.3 train no.257030  loss = 4.09469 avg_loss = 3.31641\n",
      "epoch no.3 train no.257040  loss = 4.46298 avg_loss = 3.31916\n",
      "epoch no.3 train no.257050  loss = 7.07888 avg_loss = 3.34176\n",
      "epoch no.3 train no.257060  loss = 2.91459 avg_loss = 3.31649\n",
      "epoch no.3 train no.257070  loss = 3.71956 avg_loss = 3.32552\n",
      "epoch no.3 train no.257080  loss = 3.29533 avg_loss = 3.31281\n",
      "epoch no.3 train no.257090  loss = 4.84459 avg_loss = 3.32431\n",
      "epoch no.3 train no.257100  loss = 3.95062 avg_loss = 3.31835\n",
      "epoch no.3 train no.257110  loss = 4.12831 avg_loss = 3.31124\n",
      "epoch no.3 train no.257120  loss = 3.20608 avg_loss = 3.31091\n",
      "epoch no.3 train no.257130  loss = 3.39537 avg_loss = 3.30276\n",
      "epoch no.3 train no.257140  loss = 2.41943 avg_loss = 3.25058\n",
      "epoch no.3 train no.257150  loss = 5.35053 avg_loss = 3.27634\n",
      "epoch no.3 train no.257160  loss = 4.48226 avg_loss = 3.27009\n",
      "epoch no.3 train no.257170  loss = 3.30145 avg_loss = 3.30880\n",
      "epoch no.3 train no.257180  loss = 3.87506 avg_loss = 3.27110\n",
      "epoch no.3 train no.257190  loss = 3.40918 avg_loss = 3.32242\n",
      "epoch no.3 train no.257200  loss = 2.96620 avg_loss = 3.33558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.257210  loss = 3.70319 avg_loss = 3.33097\n",
      "epoch no.3 train no.257220  loss = 4.04459 avg_loss = 3.35331\n",
      "epoch no.3 train no.257230  loss = 3.43195 avg_loss = 3.34256\n",
      "epoch no.3 train no.257240  loss = 2.40104 avg_loss = 3.32033\n",
      "epoch no.3 train no.257250  loss = 3.44174 avg_loss = 3.28729\n",
      "epoch no.3 train no.257260  loss = 4.53878 avg_loss = 3.26443\n",
      "epoch no.3 train no.257270  loss = 3.54975 avg_loss = 3.24286\n",
      "epoch no.3 train no.257280  loss = 2.07539 avg_loss = 3.24365\n",
      "epoch no.3 train no.257290  loss = 3.17877 avg_loss = 3.25944\n",
      "epoch no.3 train no.257300  loss = 3.26521 avg_loss = 3.26385\n",
      "epoch no.3 train no.257310  loss = 2.53005 avg_loss = 3.25957\n",
      "epoch no.3 train no.257320  loss = 3.32510 avg_loss = 3.24707\n",
      "epoch no.3 train no.257330  loss = 2.69407 avg_loss = 3.21879\n",
      "epoch no.3 train no.257340  loss = 2.52014 avg_loss = 3.24720\n",
      "epoch no.3 train no.257350  loss = 3.35175 avg_loss = 3.25778\n",
      "epoch no.3 train no.257360  loss = 2.74513 avg_loss = 3.26132\n",
      "epoch no.3 train no.257370  loss = 3.21826 avg_loss = 3.27406\n",
      "epoch no.3 train no.257380  loss = 2.25857 avg_loss = 3.26614\n",
      "epoch no.3 train no.257390  loss = 3.75151 avg_loss = 3.27429\n",
      "epoch no.3 train no.257400  loss = 3.22130 avg_loss = 3.27369\n",
      "epoch no.3 train no.257410  loss = 3.21129 avg_loss = 3.30036\n",
      "epoch no.3 train no.257420  loss = 2.59297 avg_loss = 3.29106\n",
      "epoch no.3 train no.257430  loss = 2.65100 avg_loss = 3.33822\n",
      "epoch no.3 train no.257440  loss = 4.65343 avg_loss = 3.34162\n",
      "epoch no.3 train no.257450  loss = 2.15244 avg_loss = 3.35729\n",
      "epoch no.3 train no.257460  loss = 1.61875 avg_loss = 3.30083\n",
      "epoch no.3 train no.257470  loss = 2.30472 avg_loss = 3.33405\n",
      "epoch no.3 train no.257480  loss = 2.93216 avg_loss = 3.29842\n",
      "epoch no.3 train no.257490  loss = 1.43529 avg_loss = 3.28651\n",
      "epoch no.3 train no.257500  loss = 3.43445 avg_loss = 3.27531\n",
      "epoch no.3 train no.257510  loss = 2.33581 avg_loss = 3.32417\n",
      "epoch no.3 train no.257520  loss = 2.87117 avg_loss = 3.29716\n",
      "epoch no.3 train no.257530  loss = 3.45834 avg_loss = 3.34132\n",
      "epoch no.3 train no.257540  loss = 3.14552 avg_loss = 3.35051\n",
      "epoch no.3 train no.257550  loss = 2.06516 avg_loss = 3.30600\n",
      "epoch no.3 train no.257560  loss = 2.46804 avg_loss = 3.27448\n",
      "epoch no.3 train no.257570  loss = 1.86304 avg_loss = 3.26710\n",
      "epoch no.3 train no.257580  loss = 4.17362 avg_loss = 3.28486\n",
      "epoch no.3 train no.257590  loss = 2.75276 avg_loss = 3.29253\n",
      "epoch no.3 train no.257600  loss = 2.24452 avg_loss = 3.28832\n",
      "epoch no.3 train no.257610  loss = 4.08774 avg_loss = 3.30041\n",
      "epoch no.3 train no.257620  loss = 3.05239 avg_loss = 3.38377\n",
      "epoch no.3 train no.257630  loss = 3.68794 avg_loss = 3.36050\n",
      "epoch no.3 train no.257640  loss = 5.16178 avg_loss = 3.40889\n",
      "epoch no.3 train no.257650  loss = 4.04261 avg_loss = 3.40422\n",
      "epoch no.3 train no.257660  loss = 3.75572 avg_loss = 3.35504\n",
      "epoch no.3 train no.257670  loss = 3.14786 avg_loss = 3.33543\n",
      "epoch no.3 train no.257680  loss = 1.75765 avg_loss = 3.32760\n",
      "epoch no.3 train no.257690  loss = 3.81070 avg_loss = 3.32572\n",
      "epoch no.3 train no.257700  loss = 3.39380 avg_loss = 3.36026\n",
      "epoch no.3 train no.257710  loss = 4.68026 avg_loss = 3.37861\n",
      "epoch no.3 train no.257720  loss = 3.34801 avg_loss = 3.37813\n",
      "epoch no.3 train no.257730  loss = 3.99459 avg_loss = 3.37696\n",
      "epoch no.3 train no.257740  loss = 2.42322 avg_loss = 3.39016\n",
      "epoch no.3 train no.257750  loss = 5.41066 avg_loss = 3.39363\n",
      "epoch no.3 train no.257760  loss = 2.67895 avg_loss = 3.41614\n",
      "epoch no.3 train no.257770  loss = 3.47127 avg_loss = 3.44393\n",
      "epoch no.3 train no.257780  loss = 2.34656 avg_loss = 3.39700\n",
      "epoch no.3 train no.257790  loss = 3.06788 avg_loss = 3.39249\n",
      "epoch no.3 train no.257800  loss = 3.50514 avg_loss = 3.41480\n",
      "epoch no.3 train no.257810  loss = 3.29912 avg_loss = 3.38316\n",
      "epoch no.3 train no.257820  loss = 2.40569 avg_loss = 3.34231\n",
      "epoch no.3 train no.257830  loss = 2.76316 avg_loss = 3.28822\n",
      "epoch no.3 train no.257840  loss = 1.51247 avg_loss = 3.28098\n",
      "epoch no.3 train no.257850  loss = 3.24388 avg_loss = 3.29157\n",
      "epoch no.3 train no.257860  loss = 2.92301 avg_loss = 3.26907\n",
      "epoch no.3 train no.257870  loss = 3.70303 avg_loss = 3.26150\n",
      "epoch no.3 train no.257880  loss = 3.26137 avg_loss = 3.24215\n",
      "epoch no.3 train no.257890  loss = 2.46646 avg_loss = 3.26510\n",
      "epoch no.3 train no.257900  loss = 3.75721 avg_loss = 3.26968\n",
      "epoch no.3 train no.257910  loss = 3.97311 avg_loss = 3.28653\n",
      "epoch no.3 train no.257920  loss = 1.84943 avg_loss = 3.26989\n",
      "epoch no.3 train no.257930  loss = 3.22548 avg_loss = 3.30970\n",
      "epoch no.3 train no.257940  loss = 2.22925 avg_loss = 3.27880\n",
      "epoch no.3 train no.257950  loss = 4.61879 avg_loss = 3.37213\n",
      "epoch no.3 train no.257960  loss = 5.55230 avg_loss = 3.42658\n",
      "epoch no.3 train no.257970  loss = 3.18274 avg_loss = 3.40745\n",
      "epoch no.3 train no.257980  loss = 2.37303 avg_loss = 3.41018\n",
      "epoch no.3 train no.257990  loss = 2.06424 avg_loss = 3.37425\n",
      "epoch no.3 train no.258000  loss = 4.92741 avg_loss = 3.38149\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁오면', '▁듣고', '하는', '▁노래', '한', '▁노래', '</s>']\n",
      "여름이 오면 들어야하는 청량한 노래</s>\n",
      "epoch no.3 train no.258010  loss = 2.99552 avg_loss = 3.36188\n",
      "epoch no.3 train no.258020  loss = 3.06929 avg_loss = 3.40026\n",
      "epoch no.3 train no.258030  loss = 2.79130 avg_loss = 3.38184\n",
      "epoch no.3 train no.258040  loss = 6.62078 avg_loss = 3.37318\n",
      "epoch no.3 train no.258050  loss = 3.79799 avg_loss = 3.35494\n",
      "epoch no.3 train no.258060  loss = 3.69361 avg_loss = 3.37051\n",
      "epoch no.3 train no.258070  loss = 3.38869 avg_loss = 3.33385\n",
      "epoch no.3 train no.258080  loss = 2.44335 avg_loss = 3.33276\n",
      "epoch no.3 train no.258090  loss = 3.72872 avg_loss = 3.32858\n",
      "epoch no.3 train no.258100  loss = 2.71391 avg_loss = 3.36443\n",
      "epoch no.3 train no.258110  loss = 2.52205 avg_loss = 3.32874\n",
      "epoch no.3 train no.258120  loss = 3.69787 avg_loss = 3.32417\n",
      "epoch no.3 train no.258130  loss = 2.16503 avg_loss = 3.32270\n",
      "epoch no.3 train no.258140  loss = 4.38674 avg_loss = 3.31580\n",
      "epoch no.3 train no.258150  loss = 3.52865 avg_loss = 3.30432\n",
      "epoch no.3 train no.258160  loss = 2.14391 avg_loss = 3.33143\n",
      "epoch no.3 train no.258170  loss = 4.46553 avg_loss = 3.36154\n",
      "epoch no.3 train no.258180  loss = 3.01805 avg_loss = 3.32455\n",
      "epoch no.3 train no.258190  loss = 3.98883 avg_loss = 3.35429\n",
      "epoch no.3 train no.258200  loss = 2.95816 avg_loss = 3.32606\n",
      "epoch no.3 train no.258210  loss = 7.14555 avg_loss = 3.36617\n",
      "epoch no.3 train no.258220  loss = 2.81940 avg_loss = 3.35766\n",
      "epoch no.3 train no.258230  loss = 2.44653 avg_loss = 3.34996\n",
      "epoch no.3 train no.258240  loss = 3.99336 avg_loss = 3.33688\n",
      "epoch no.3 train no.258250  loss = 3.16009 avg_loss = 3.35217\n",
      "epoch no.3 train no.258260  loss = 2.43869 avg_loss = 3.31527\n",
      "epoch no.3 train no.258270  loss = 3.91097 avg_loss = 3.34109\n",
      "epoch no.3 train no.258280  loss = 3.35410 avg_loss = 3.38459\n",
      "epoch no.3 train no.258290  loss = 2.48990 avg_loss = 3.32989\n",
      "epoch no.3 train no.258300  loss = 2.34083 avg_loss = 3.34332\n",
      "epoch no.3 train no.258310  loss = 2.07208 avg_loss = 3.34251\n",
      "epoch no.3 train no.258320  loss = 3.32521 avg_loss = 3.36909\n",
      "epoch no.3 train no.258330  loss = 1.82715 avg_loss = 3.35139\n",
      "epoch no.3 train no.258340  loss = 3.44987 avg_loss = 3.32385\n",
      "epoch no.3 train no.258350  loss = 4.31185 avg_loss = 3.28568\n",
      "epoch no.3 train no.258360  loss = 2.36866 avg_loss = 3.26449\n",
      "epoch no.3 train no.258370  loss = 2.20042 avg_loss = 3.20990\n",
      "epoch no.3 train no.258380  loss = 3.01297 avg_loss = 3.19677\n",
      "epoch no.3 train no.258390  loss = 1.57946 avg_loss = 3.18071\n",
      "epoch no.3 train no.258400  loss = 2.51161 avg_loss = 3.19925\n",
      "epoch no.3 train no.258410  loss = 3.79063 avg_loss = 3.21675\n",
      "epoch no.3 train no.258420  loss = 3.09202 avg_loss = 3.21709\n",
      "epoch no.3 train no.258430  loss = 2.55326 avg_loss = 3.19472\n",
      "epoch no.3 train no.258440  loss = 2.13700 avg_loss = 3.17042\n",
      "epoch no.3 train no.258450  loss = 5.09920 avg_loss = 3.22983\n",
      "epoch no.3 train no.258460  loss = 4.48780 avg_loss = 3.26074\n",
      "epoch no.3 train no.258470  loss = 3.49361 avg_loss = 3.19274\n",
      "epoch no.3 train no.258480  loss = 3.17744 avg_loss = 3.22188\n",
      "epoch no.3 train no.258490  loss = 4.89769 avg_loss = 3.28256\n",
      "epoch no.3 train no.258500  loss = 3.60775 avg_loss = 3.27636\n",
      "epoch no.3 train no.258510  loss = 3.78779 avg_loss = 3.28170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.258520  loss = 3.58382 avg_loss = 3.31751\n",
      "epoch no.3 train no.258530  loss = 4.28871 avg_loss = 3.29168\n",
      "epoch no.3 train no.258540  loss = 3.39474 avg_loss = 3.31610\n",
      "epoch no.3 train no.258550  loss = 4.30026 avg_loss = 3.34807\n",
      "epoch no.3 train no.258560  loss = 4.19419 avg_loss = 3.39213\n",
      "epoch no.3 train no.258570  loss = 1.67075 avg_loss = 3.34797\n",
      "epoch no.3 train no.258580  loss = 3.92267 avg_loss = 3.34317\n",
      "epoch no.3 train no.258590  loss = 3.02612 avg_loss = 3.30435\n",
      "epoch no.3 train no.258600  loss = 3.49361 avg_loss = 3.36422\n",
      "epoch no.3 train no.258610  loss = 2.66427 avg_loss = 3.37772\n",
      "epoch no.3 train no.258620  loss = 2.45740 avg_loss = 3.32855\n",
      "epoch no.3 train no.258630  loss = 3.15681 avg_loss = 3.31335\n",
      "epoch no.3 train no.258640  loss = 3.59757 avg_loss = 3.32698\n",
      "epoch no.3 train no.258650  loss = 3.15607 avg_loss = 3.33596\n",
      "epoch no.3 train no.258660  loss = 5.67222 avg_loss = 3.40257\n",
      "epoch no.3 train no.258670  loss = 3.45590 avg_loss = 3.35745\n",
      "epoch no.3 train no.258680  loss = 2.82824 avg_loss = 3.34428\n",
      "epoch no.3 train no.258690  loss = 4.13901 avg_loss = 3.34872\n",
      "epoch no.3 train no.258700  loss = 3.76553 avg_loss = 3.33491\n",
      "epoch no.3 train no.258710  loss = 4.69518 avg_loss = 3.34195\n",
      "epoch no.3 train no.258720  loss = 3.21384 avg_loss = 3.34553\n",
      "epoch no.3 train no.258730  loss = 4.20849 avg_loss = 3.36236\n",
      "epoch no.3 train no.258740  loss = 3.84654 avg_loss = 3.32932\n",
      "epoch no.3 train no.258750  loss = 2.41998 avg_loss = 3.33002\n",
      "epoch no.3 train no.258760  loss = 2.99941 avg_loss = 3.30578\n",
      "epoch no.3 train no.258770  loss = 4.95829 avg_loss = 3.33414\n",
      "epoch no.3 train no.258780  loss = 2.69172 avg_loss = 3.33839\n",
      "epoch no.3 train no.258790  loss = 3.59038 avg_loss = 3.31909\n",
      "epoch no.3 train no.258800  loss = 2.52197 avg_loss = 3.32870\n",
      "epoch no.3 train no.258810  loss = 2.68985 avg_loss = 3.34946\n",
      "epoch no.3 train no.258820  loss = 2.58829 avg_loss = 3.28689\n",
      "epoch no.3 train no.258830  loss = 3.49008 avg_loss = 3.28300\n",
      "epoch no.3 train no.258840  loss = 2.78034 avg_loss = 3.30008\n",
      "epoch no.3 train no.258850  loss = 2.78956 avg_loss = 3.35035\n",
      "epoch no.3 train no.258860  loss = 3.14146 avg_loss = 3.32680\n",
      "epoch no.3 train no.258870  loss = 4.51119 avg_loss = 3.33062\n",
      "epoch no.3 train no.258880  loss = 2.79437 avg_loss = 3.33465\n",
      "epoch no.3 train no.258890  loss = 3.61385 avg_loss = 3.35477\n",
      "epoch no.3 train no.258900  loss = 5.30469 avg_loss = 3.37190\n",
      "epoch no.3 train no.258910  loss = 2.79307 avg_loss = 3.33218\n",
      "epoch no.3 train no.258920  loss = 3.18888 avg_loss = 3.34132\n",
      "epoch no.3 train no.258930  loss = 2.88645 avg_loss = 3.36108\n",
      "epoch no.3 train no.258940  loss = 3.09634 avg_loss = 3.34452\n",
      "epoch no.3 train no.258950  loss = 4.50651 avg_loss = 3.43102\n",
      "epoch no.3 train no.258960  loss = 3.97042 avg_loss = 3.43776\n",
      "epoch no.3 train no.258970  loss = 3.78299 avg_loss = 3.43872\n",
      "epoch no.3 train no.258980  loss = 4.53778 avg_loss = 3.44950\n",
      "epoch no.3 train no.258990  loss = 4.10610 avg_loss = 3.42544\n",
      "epoch no.3 train no.259000  loss = 3.71217 avg_loss = 3.36654\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '을', '▁감성', '과', '▁재즈', '율', '</s>', '</s>']\n",
      "여름밤의 낭만적인 선율들</s>\n",
      "epoch no.3 train no.259010  loss = 3.83756 avg_loss = 3.35315\n",
      "epoch no.3 train no.259020  loss = 2.55566 avg_loss = 3.29888\n",
      "epoch no.3 train no.259030  loss = 1.83315 avg_loss = 3.27003\n",
      "epoch no.3 train no.259040  loss = 2.26414 avg_loss = 3.31735\n",
      "epoch no.3 train no.259050  loss = 2.49877 avg_loss = 3.27438\n",
      "epoch no.3 train no.259060  loss = 2.17702 avg_loss = 3.31303\n",
      "epoch no.3 train no.259070  loss = 4.49586 avg_loss = 3.34332\n",
      "epoch no.3 train no.259080  loss = 6.06054 avg_loss = 3.40001\n",
      "epoch no.3 train no.259090  loss = 3.82743 avg_loss = 3.39151\n",
      "epoch no.3 train no.259100  loss = 2.11425 avg_loss = 3.33458\n",
      "epoch no.3 train no.259110  loss = 3.29073 avg_loss = 3.33814\n",
      "epoch no.3 train no.259120  loss = 2.57770 avg_loss = 3.39455\n",
      "epoch no.3 train no.259130  loss = 3.22614 avg_loss = 3.33463\n",
      "epoch no.3 train no.259140  loss = 3.84307 avg_loss = 3.33902\n",
      "epoch no.3 train no.259150  loss = 2.75664 avg_loss = 3.36034\n",
      "epoch no.3 train no.259160  loss = 2.93605 avg_loss = 3.33718\n",
      "epoch no.3 train no.259170  loss = 3.78538 avg_loss = 3.33744\n",
      "epoch no.3 train no.259180  loss = 3.06330 avg_loss = 3.33678\n",
      "epoch no.3 train no.259190  loss = 4.60960 avg_loss = 3.38495\n",
      "epoch no.3 train no.259200  loss = 4.13512 avg_loss = 3.36554\n",
      "epoch no.3 train no.259210  loss = 4.28589 avg_loss = 3.31768\n",
      "epoch no.3 train no.259220  loss = 3.89706 avg_loss = 3.37114\n",
      "epoch no.3 train no.259230  loss = 4.41985 avg_loss = 3.35941\n",
      "epoch no.3 train no.259240  loss = 3.55057 avg_loss = 3.31009\n",
      "epoch no.3 train no.259250  loss = 3.41838 avg_loss = 3.30394\n",
      "epoch no.3 train no.259260  loss = 4.34508 avg_loss = 3.28580\n",
      "epoch no.3 train no.259270  loss = 6.10275 avg_loss = 3.29507\n",
      "epoch no.3 train no.259280  loss = 3.63650 avg_loss = 3.32098\n",
      "epoch no.3 train no.259290  loss = 1.73639 avg_loss = 3.31963\n",
      "epoch no.3 train no.259300  loss = 3.48129 avg_loss = 3.35574\n",
      "epoch no.3 train no.259310  loss = 3.86976 avg_loss = 3.36810\n",
      "epoch no.3 train no.259320  loss = 2.81314 avg_loss = 3.35875\n",
      "epoch no.3 train no.259330  loss = 1.95587 avg_loss = 3.32120\n",
      "epoch no.3 train no.259340  loss = 3.30476 avg_loss = 3.30719\n",
      "epoch no.3 train no.259350  loss = 3.06042 avg_loss = 3.32089\n",
      "epoch no.3 train no.259360  loss = 1.13933 avg_loss = 3.29753\n",
      "epoch no.3 train no.259370  loss = 3.02516 avg_loss = 3.28604\n",
      "epoch no.3 train no.259380  loss = 1.85172 avg_loss = 3.26165\n",
      "epoch no.3 train no.259390  loss = 3.24749 avg_loss = 3.27759\n",
      "epoch no.3 train no.259400  loss = 3.22430 avg_loss = 3.26029\n",
      "epoch no.3 train no.259410  loss = 2.54770 avg_loss = 3.28543\n",
      "epoch no.3 train no.259420  loss = 3.17060 avg_loss = 3.29456\n",
      "epoch no.3 train no.259430  loss = 2.00555 avg_loss = 3.29894\n",
      "epoch no.3 train no.259440  loss = 1.99552 avg_loss = 3.26012\n",
      "epoch no.3 train no.259450  loss = 5.18470 avg_loss = 3.29161\n",
      "epoch no.3 train no.259460  loss = 2.62462 avg_loss = 3.28642\n",
      "epoch no.3 train no.259470  loss = 2.25753 avg_loss = 3.26906\n",
      "epoch no.3 train no.259480  loss = 2.31085 avg_loss = 3.26716\n",
      "epoch no.3 train no.259490  loss = 2.95611 avg_loss = 3.25413\n",
      "epoch no.3 train no.259500  loss = 2.34333 avg_loss = 3.24912\n",
      "epoch no.3 train no.259510  loss = 2.73835 avg_loss = 3.33380\n",
      "epoch no.3 train no.259520  loss = 3.58832 avg_loss = 3.33496\n",
      "epoch no.3 train no.259530  loss = 2.47300 avg_loss = 3.32844\n",
      "epoch no.3 train no.259540  loss = 3.54309 avg_loss = 3.32034\n",
      "epoch no.3 train no.259550  loss = 3.01641 avg_loss = 3.32358\n",
      "epoch no.3 train no.259560  loss = 3.47889 avg_loss = 3.35406\n",
      "epoch no.3 train no.259570  loss = 3.36108 avg_loss = 3.33943\n",
      "epoch no.3 train no.259580  loss = 2.16376 avg_loss = 3.35895\n",
      "epoch no.3 train no.259590  loss = 4.51672 avg_loss = 3.39001\n",
      "epoch no.3 train no.259600  loss = 2.81519 avg_loss = 3.35694\n",
      "epoch no.3 train no.259610  loss = 3.90040 avg_loss = 3.39777\n",
      "epoch no.3 train no.259620  loss = 2.51030 avg_loss = 3.37015\n",
      "epoch no.3 train no.259630  loss = 6.05220 avg_loss = 3.37872\n",
      "epoch no.3 train no.259640  loss = 4.16235 avg_loss = 3.36636\n",
      "epoch no.3 train no.259650  loss = 2.22121 avg_loss = 3.34802\n",
      "epoch no.3 train no.259660  loss = 6.07161 avg_loss = 3.37374\n",
      "epoch no.3 train no.259670  loss = 3.64692 avg_loss = 3.34265\n",
      "epoch no.3 train no.259680  loss = 4.94429 avg_loss = 3.31693\n",
      "epoch no.3 train no.259690  loss = 4.26902 avg_loss = 3.33492\n",
      "epoch no.3 train no.259700  loss = 3.08661 avg_loss = 3.36827\n",
      "epoch no.3 train no.259710  loss = 2.99684 avg_loss = 3.37358\n",
      "epoch no.3 train no.259720  loss = 3.68701 avg_loss = 3.34156\n",
      "epoch no.3 train no.259730  loss = 3.41887 avg_loss = 3.34045\n",
      "epoch no.3 train no.259740  loss = 3.58342 avg_loss = 3.32954\n",
      "epoch no.3 train no.259750  loss = 4.88944 avg_loss = 3.37030\n",
      "epoch no.3 train no.259760  loss = 3.85751 avg_loss = 3.37372\n",
      "epoch no.3 train no.259770  loss = 4.20831 avg_loss = 3.38064\n",
      "epoch no.3 train no.259780  loss = 2.30404 avg_loss = 3.34319\n",
      "epoch no.3 train no.259790  loss = 5.05038 avg_loss = 3.37560\n",
      "epoch no.3 train no.259800  loss = 3.66921 avg_loss = 3.37040\n",
      "epoch no.3 train no.259810  loss = 3.72886 avg_loss = 3.34454\n",
      "epoch no.3 train no.259820  loss = 2.68345 avg_loss = 3.34458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.259830  loss = 2.68279 avg_loss = 3.34937\n",
      "epoch no.3 train no.259840  loss = 3.46928 avg_loss = 3.31137\n",
      "epoch no.3 train no.259850  loss = 4.84272 avg_loss = 3.33676\n",
      "epoch no.3 train no.259860  loss = 4.54661 avg_loss = 3.30592\n",
      "epoch no.3 train no.259870  loss = 2.99887 avg_loss = 3.31441\n",
      "epoch no.3 train no.259880  loss = 3.86747 avg_loss = 3.32149\n",
      "epoch no.3 train no.259890  loss = 3.49981 avg_loss = 3.36364\n",
      "epoch no.3 train no.259900  loss = 3.21951 avg_loss = 3.37355\n",
      "epoch no.3 train no.259910  loss = 6.05156 avg_loss = 3.40303\n",
      "epoch no.3 train no.259920  loss = 3.06560 avg_loss = 3.35705\n",
      "epoch no.3 train no.259930  loss = 2.49208 avg_loss = 3.34724\n",
      "epoch no.3 train no.259940  loss = 3.78566 avg_loss = 3.39300\n",
      "epoch no.3 train no.259950  loss = 3.93468 avg_loss = 3.37881\n",
      "epoch no.3 train no.259960  loss = 2.53863 avg_loss = 3.38769\n",
      "epoch no.3 train no.259970  loss = 2.50833 avg_loss = 3.36979\n",
      "epoch no.3 train no.259980  loss = 3.30986 avg_loss = 3.36488\n",
      "epoch no.3 train no.259990  loss = 2.75867 avg_loss = 3.35421\n",
      "epoch no.3 train no.260000  loss = 1.88583 avg_loss = 3.29169\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣는', '▁노래', '</s>']\n",
      "여름밤에 어울리는 노래</s>\n",
      "epoch no.3 train no.260010  loss = 2.08788 avg_loss = 3.29662\n",
      "epoch no.3 train no.260020  loss = 2.32179 avg_loss = 3.28620\n",
      "epoch no.3 train no.260030  loss = 4.49692 avg_loss = 3.27178\n",
      "epoch no.3 train no.260040  loss = 2.57020 avg_loss = 3.28550\n",
      "epoch no.3 train no.260050  loss = 4.02769 avg_loss = 3.27642\n",
      "epoch no.3 train no.260060  loss = 4.24914 avg_loss = 3.24386\n",
      "epoch no.3 train no.260070  loss = 3.96230 avg_loss = 3.26946\n",
      "epoch no.3 train no.260080  loss = 3.78548 avg_loss = 3.24042\n",
      "epoch no.3 train no.260090  loss = 4.86233 avg_loss = 3.24065\n",
      "epoch no.3 train no.260100  loss = 3.76749 avg_loss = 3.25746\n",
      "epoch no.3 train no.260110  loss = 4.33310 avg_loss = 3.25752\n",
      "epoch no.3 train no.260120  loss = 3.01396 avg_loss = 3.27032\n",
      "epoch no.3 train no.260130  loss = 2.93138 avg_loss = 3.30771\n",
      "epoch no.3 train no.260140  loss = 3.33284 avg_loss = 3.29890\n",
      "epoch no.3 train no.260150  loss = 2.45222 avg_loss = 3.25950\n",
      "epoch no.3 train no.260160  loss = 3.21741 avg_loss = 3.25161\n",
      "epoch no.3 train no.260170  loss = 3.46705 avg_loss = 3.25906\n",
      "epoch no.3 train no.260180  loss = 2.89572 avg_loss = 3.26170\n",
      "epoch no.3 train no.260190  loss = 3.65703 avg_loss = 3.25208\n",
      "epoch no.3 train no.260200  loss = 4.79638 avg_loss = 3.26198\n",
      "epoch no.3 train no.260210  loss = 3.54333 avg_loss = 3.32506\n",
      "epoch no.3 train no.260220  loss = 2.82913 avg_loss = 3.30691\n",
      "epoch no.3 train no.260230  loss = 2.66530 avg_loss = 3.35384\n",
      "epoch no.3 train no.260240  loss = 3.25394 avg_loss = 3.30939\n",
      "epoch no.3 train no.260250  loss = 3.72425 avg_loss = 3.33594\n",
      "epoch no.3 train no.260260  loss = 3.13752 avg_loss = 3.30160\n",
      "epoch no.3 train no.260270  loss = 3.40216 avg_loss = 3.32155\n",
      "epoch no.3 train no.260280  loss = 2.40676 avg_loss = 3.29862\n",
      "epoch no.3 train no.260290  loss = 2.48787 avg_loss = 3.27236\n",
      "epoch no.3 train no.260300  loss = 3.57589 avg_loss = 3.26432\n",
      "epoch no.3 train no.260310  loss = 1.67304 avg_loss = 3.28581\n",
      "epoch no.3 train no.260320  loss = 3.80891 avg_loss = 3.29048\n",
      "epoch no.3 train no.260330  loss = 3.61508 avg_loss = 3.33662\n",
      "epoch no.3 train no.260340  loss = 2.36531 avg_loss = 3.31286\n",
      "epoch no.3 train no.260350  loss = 4.34166 avg_loss = 3.34825\n",
      "epoch no.3 train no.260360  loss = 2.35410 avg_loss = 3.32623\n",
      "epoch no.3 train no.260370  loss = 1.70729 avg_loss = 3.26284\n",
      "epoch no.3 train no.260380  loss = 3.38163 avg_loss = 3.25615\n",
      "epoch no.3 train no.260390  loss = 3.63599 avg_loss = 3.24336\n",
      "epoch no.3 train no.260400  loss = 2.73216 avg_loss = 3.24537\n",
      "epoch no.3 train no.260410  loss = 2.59857 avg_loss = 3.27785\n",
      "epoch no.3 train no.260420  loss = 1.64080 avg_loss = 3.28020\n",
      "epoch no.3 train no.260430  loss = 4.09546 avg_loss = 3.28459\n",
      "epoch no.3 train no.260440  loss = 2.50241 avg_loss = 3.24929\n",
      "epoch no.3 train no.260450  loss = 4.32315 avg_loss = 3.27286\n",
      "epoch no.3 train no.260460  loss = 4.29189 avg_loss = 3.26872\n",
      "epoch no.3 train no.260470  loss = 2.08610 avg_loss = 3.26331\n",
      "epoch no.3 train no.260480  loss = 5.02700 avg_loss = 3.31409\n",
      "epoch no.3 train no.260490  loss = 4.10845 avg_loss = 3.34550\n",
      "epoch no.3 train no.260500  loss = 3.68290 avg_loss = 3.36472\n",
      "epoch no.3 train no.260510  loss = 4.58452 avg_loss = 3.32389\n",
      "epoch no.3 train no.260520  loss = 4.72134 avg_loss = 3.30705\n",
      "epoch no.3 train no.260530  loss = 3.75931 avg_loss = 3.32237\n",
      "epoch no.3 train no.260540  loss = 2.11230 avg_loss = 3.33392\n",
      "epoch no.3 train no.260550  loss = 4.72746 avg_loss = 3.33414\n",
      "epoch no.3 train no.260560  loss = 4.33317 avg_loss = 3.37839\n",
      "epoch no.3 train no.260570  loss = 5.10130 avg_loss = 3.43519\n",
      "epoch no.3 train no.260580  loss = 2.04071 avg_loss = 3.40833\n",
      "epoch no.3 train no.260590  loss = 3.58013 avg_loss = 3.43829\n",
      "epoch no.3 train no.260600  loss = 4.51999 avg_loss = 3.43645\n",
      "epoch no.3 train no.260610  loss = 2.95225 avg_loss = 3.42823\n",
      "epoch no.3 train no.260620  loss = 6.07892 avg_loss = 3.39606\n",
      "epoch no.3 train no.260630  loss = 4.42459 avg_loss = 3.40977\n",
      "epoch no.3 train no.260640  loss = 2.59539 avg_loss = 3.40969\n",
      "epoch no.3 train no.260650  loss = 3.84876 avg_loss = 3.39704\n",
      "epoch no.3 train no.260660  loss = 2.37388 avg_loss = 3.34307\n",
      "epoch no.3 train no.260670  loss = 3.60756 avg_loss = 3.37177\n",
      "epoch no.3 train no.260680  loss = 2.80484 avg_loss = 3.31136\n",
      "epoch no.3 train no.260690  loss = 3.60570 avg_loss = 3.30729\n",
      "epoch no.3 train no.260700  loss = 4.20919 avg_loss = 3.27836\n",
      "epoch no.3 train no.260710  loss = 3.49003 avg_loss = 3.29868\n",
      "epoch no.3 train no.260720  loss = 5.76160 avg_loss = 3.36820\n",
      "epoch no.3 train no.260730  loss = 4.26598 avg_loss = 3.34034\n",
      "epoch no.3 train no.260740  loss = 2.78502 avg_loss = 3.33832\n",
      "epoch no.3 train no.260750  loss = 3.71029 avg_loss = 3.35243\n",
      "epoch no.3 train no.260760  loss = 3.41248 avg_loss = 3.36219\n",
      "epoch no.3 train no.260770  loss = 3.05045 avg_loss = 3.36317\n",
      "epoch no.3 train no.260780  loss = 4.40277 avg_loss = 3.35260\n",
      "epoch no.3 train no.260790  loss = 3.58520 avg_loss = 3.32086\n",
      "epoch no.3 train no.260800  loss = 3.18709 avg_loss = 3.31353\n",
      "epoch no.3 train no.260810  loss = 3.54237 avg_loss = 3.30007\n",
      "epoch no.3 train no.260820  loss = 3.53564 avg_loss = 3.34343\n",
      "epoch no.3 train no.260830  loss = 3.79810 avg_loss = 3.32983\n",
      "epoch no.3 train no.260840  loss = 3.88486 avg_loss = 3.30710\n",
      "epoch no.3 train no.260850  loss = 4.31539 avg_loss = 3.31028\n",
      "epoch no.3 train no.260860  loss = 3.78747 avg_loss = 3.31808\n",
      "epoch no.3 train no.260870  loss = 2.34311 avg_loss = 3.30625\n",
      "epoch no.3 train no.260880  loss = 4.05742 avg_loss = 3.32115\n",
      "epoch no.3 train no.260890  loss = 4.61574 avg_loss = 3.32543\n",
      "epoch no.3 train no.260900  loss = 3.11014 avg_loss = 3.35559\n",
      "epoch no.3 train no.260910  loss = 2.62576 avg_loss = 3.35904\n",
      "epoch no.3 train no.260920  loss = 4.07079 avg_loss = 3.34514\n",
      "epoch no.3 train no.260930  loss = 1.88672 avg_loss = 3.33640\n",
      "epoch no.3 train no.260940  loss = 4.82323 avg_loss = 3.36503\n",
      "epoch no.3 train no.260950  loss = 2.37512 avg_loss = 3.37989\n",
      "epoch no.3 train no.260960  loss = 2.38197 avg_loss = 3.35701\n",
      "epoch no.3 train no.260970  loss = 3.49082 avg_loss = 3.35538\n",
      "epoch no.3 train no.260980  loss = 2.70743 avg_loss = 3.38590\n",
      "epoch no.3 train no.260990  loss = 4.03607 avg_loss = 3.35239\n",
      "epoch no.3 train no.261000  loss = 3.14627 avg_loss = 3.36385\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '의', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 음악</s>\n",
      "epoch no.3 train no.261010  loss = 3.14917 avg_loss = 3.39733\n",
      "epoch no.3 train no.261020  loss = 2.43005 avg_loss = 3.39540\n",
      "epoch no.3 train no.261030  loss = 4.27689 avg_loss = 3.37519\n",
      "epoch no.3 train no.261040  loss = 4.39610 avg_loss = 3.35561\n",
      "epoch no.3 train no.261050  loss = 2.92059 avg_loss = 3.35587\n",
      "epoch no.3 train no.261060  loss = 3.13150 avg_loss = 3.35517\n",
      "epoch no.3 train no.261070  loss = 3.68890 avg_loss = 3.30548\n",
      "epoch no.3 train no.261080  loss = 3.88198 avg_loss = 3.30965\n",
      "epoch no.3 train no.261090  loss = 5.13525 avg_loss = 3.36590\n",
      "epoch no.3 train no.261100  loss = 3.10403 avg_loss = 3.36852\n",
      "epoch no.3 train no.261110  loss = 4.07433 avg_loss = 3.32024\n",
      "epoch no.3 train no.261120  loss = 3.09239 avg_loss = 3.36592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.261130  loss = 2.88326 avg_loss = 3.39739\n",
      "epoch no.3 train no.261140  loss = 2.69028 avg_loss = 3.42624\n",
      "epoch no.3 train no.261150  loss = 2.82967 avg_loss = 3.42944\n",
      "epoch no.3 train no.261160  loss = 2.82786 avg_loss = 3.39694\n",
      "epoch no.3 train no.261170  loss = 3.92814 avg_loss = 3.40398\n",
      "epoch no.3 train no.261180  loss = 3.09912 avg_loss = 3.42278\n",
      "epoch no.3 train no.261190  loss = 4.22781 avg_loss = 3.35852\n",
      "epoch no.3 train no.261200  loss = 3.16567 avg_loss = 3.32585\n",
      "epoch no.3 train no.261210  loss = 2.40653 avg_loss = 3.30040\n",
      "epoch no.3 train no.261220  loss = 1.16589 avg_loss = 3.28567\n",
      "epoch no.3 train no.261230  loss = 2.62930 avg_loss = 3.27765\n",
      "epoch no.3 train no.261240  loss = 3.61814 avg_loss = 3.29314\n",
      "epoch no.3 train no.261250  loss = 4.40444 avg_loss = 3.34260\n",
      "epoch no.3 train no.261260  loss = 3.12209 avg_loss = 3.35662\n",
      "epoch no.3 train no.261270  loss = 3.58277 avg_loss = 3.43325\n",
      "epoch no.3 train no.261280  loss = 2.82760 avg_loss = 3.44285\n",
      "epoch no.3 train no.261290  loss = 3.64819 avg_loss = 3.41295\n",
      "epoch no.3 train no.261300  loss = 1.52131 avg_loss = 3.39787\n",
      "epoch no.3 train no.261310  loss = 2.90613 avg_loss = 3.39309\n",
      "epoch no.3 train no.261320  loss = 2.69703 avg_loss = 3.40202\n",
      "epoch no.3 train no.261330  loss = 5.64377 avg_loss = 3.39862\n",
      "epoch no.3 train no.261340  loss = 5.46370 avg_loss = 3.40963\n",
      "epoch no.3 train no.261350  loss = 3.59053 avg_loss = 3.33717\n",
      "epoch no.3 train no.261360  loss = 3.20905 avg_loss = 3.33325\n",
      "epoch no.3 train no.261370  loss = 3.01353 avg_loss = 3.34365\n",
      "epoch no.3 train no.261380  loss = 3.23635 avg_loss = 3.38449\n",
      "epoch no.3 train no.261390  loss = 3.75554 avg_loss = 3.36401\n",
      "epoch no.3 train no.261400  loss = 2.26014 avg_loss = 3.28716\n",
      "epoch no.3 train no.261410  loss = 2.40094 avg_loss = 3.28421\n",
      "epoch no.3 train no.261420  loss = 6.43248 avg_loss = 3.35498\n",
      "epoch no.3 train no.261430  loss = 4.20935 avg_loss = 3.35940\n",
      "epoch no.3 train no.261440  loss = 3.49342 avg_loss = 3.38274\n",
      "epoch no.3 train no.261450  loss = 2.76704 avg_loss = 3.37854\n",
      "epoch no.3 train no.261460  loss = 1.22541 avg_loss = 3.33833\n",
      "epoch no.3 train no.261470  loss = 2.67478 avg_loss = 3.33272\n",
      "epoch no.3 train no.261480  loss = 3.58445 avg_loss = 3.30749\n",
      "epoch no.3 train no.261490  loss = 3.12013 avg_loss = 3.31817\n",
      "epoch no.3 train no.261500  loss = 3.20711 avg_loss = 3.30889\n",
      "epoch no.3 train no.261510  loss = 3.44601 avg_loss = 3.35448\n",
      "epoch no.3 train no.261520  loss = 2.98258 avg_loss = 3.30798\n",
      "epoch no.3 train no.261530  loss = 4.57658 avg_loss = 3.33460\n",
      "epoch no.3 train no.261540  loss = 2.32007 avg_loss = 3.33186\n",
      "epoch no.3 train no.261550  loss = 2.88007 avg_loss = 3.31229\n",
      "epoch no.3 train no.261560  loss = 2.74296 avg_loss = 3.34019\n",
      "epoch no.3 train no.261570  loss = 2.80267 avg_loss = 3.37475\n",
      "epoch no.3 train no.261580  loss = 2.23530 avg_loss = 3.39528\n",
      "epoch no.3 train no.261590  loss = 3.64773 avg_loss = 3.42097\n",
      "epoch no.3 train no.261600  loss = 3.01117 avg_loss = 3.43780\n",
      "epoch no.3 train no.261610  loss = 1.61154 avg_loss = 3.44030\n",
      "epoch no.3 train no.261620  loss = 1.84745 avg_loss = 3.39318\n",
      "epoch no.3 train no.261630  loss = 3.18064 avg_loss = 3.35966\n",
      "epoch no.3 train no.261640  loss = 2.76754 avg_loss = 3.37302\n",
      "epoch no.3 train no.261650  loss = 4.08699 avg_loss = 3.38558\n",
      "epoch no.3 train no.261660  loss = 3.74492 avg_loss = 3.36239\n",
      "epoch no.3 train no.261670  loss = 2.93674 avg_loss = 3.37777\n",
      "epoch no.3 train no.261680  loss = 4.07381 avg_loss = 3.41741\n",
      "epoch no.3 train no.261690  loss = 2.68238 avg_loss = 3.37656\n",
      "epoch no.3 train no.261700  loss = 3.49348 avg_loss = 3.36802\n",
      "epoch no.3 train no.261710  loss = 2.09894 avg_loss = 3.35111\n",
      "epoch no.3 train no.261720  loss = 3.05680 avg_loss = 3.32570\n",
      "epoch no.3 train no.261730  loss = 2.59153 avg_loss = 3.30398\n",
      "epoch no.3 train no.261740  loss = 3.90878 avg_loss = 3.33289\n",
      "epoch no.3 train no.261750  loss = 4.86838 avg_loss = 3.36407\n",
      "epoch no.3 train no.261760  loss = 2.61473 avg_loss = 3.36810\n",
      "epoch no.3 train no.261770  loss = 2.19748 avg_loss = 3.36417\n",
      "epoch no.3 train no.261780  loss = 3.58414 avg_loss = 3.34520\n",
      "epoch no.3 train no.261790  loss = 2.81070 avg_loss = 3.33273\n",
      "epoch no.3 train no.261800  loss = 3.97118 avg_loss = 3.33934\n",
      "epoch no.3 train no.261810  loss = 3.06906 avg_loss = 3.29847\n",
      "epoch no.3 train no.261820  loss = 3.20528 avg_loss = 3.32768\n",
      "epoch no.3 train no.261830  loss = 3.45024 avg_loss = 3.32839\n",
      "epoch no.3 train no.261840  loss = 2.85126 avg_loss = 3.32524\n",
      "epoch no.3 train no.261850  loss = 3.91468 avg_loss = 3.34174\n",
      "epoch no.3 train no.261860  loss = 3.19748 avg_loss = 3.31911\n",
      "epoch no.3 train no.261870  loss = 2.34683 avg_loss = 3.33666\n",
      "epoch no.3 train no.261880  loss = 3.60391 avg_loss = 3.30403\n",
      "epoch no.3 train no.261890  loss = 3.57658 avg_loss = 3.30278\n",
      "epoch no.3 train no.261900  loss = 3.84696 avg_loss = 3.30250\n",
      "epoch no.3 train no.261910  loss = 4.27583 avg_loss = 3.29767\n",
      "epoch no.3 train no.261920  loss = 5.80782 avg_loss = 3.32520\n",
      "epoch no.3 train no.261930  loss = 3.66524 avg_loss = 3.35473\n",
      "epoch no.3 train no.261940  loss = 4.94445 avg_loss = 3.36673\n",
      "epoch no.3 train no.261950  loss = 2.68156 avg_loss = 3.36080\n",
      "epoch no.3 train no.261960  loss = 2.25221 avg_loss = 3.37375\n",
      "epoch no.3 train no.261970  loss = 2.36378 avg_loss = 3.35416\n",
      "epoch no.3 train no.261980  loss = 3.12995 avg_loss = 3.32983\n",
      "epoch no.3 train no.261990  loss = 2.85131 avg_loss = 3.30200\n",
      "epoch no.3 train no.262000  loss = 3.01439 avg_loss = 3.29470\n",
      "10\n",
      "to_tokens: ['▁가을', '밤', '▁역시', '▁댄스', '음악', '과', '</s>', '▁날려', '내', '</s>']\n",
      "여름엔 신나는\n",
      "\n",
      "댄스음악으로 더위를 이겨요</s>\n",
      "epoch no.3 train no.262010  loss = 2.91674 avg_loss = 3.30031\n",
      "epoch no.3 train no.262020  loss = 3.13535 avg_loss = 3.26608\n",
      "epoch no.3 train no.262030  loss = 2.02676 avg_loss = 3.26503\n",
      "epoch no.3 train no.262040  loss = 4.50956 avg_loss = 3.32566\n",
      "epoch no.3 train no.262050  loss = 2.98499 avg_loss = 3.34710\n",
      "epoch no.3 train no.262060  loss = 3.71264 avg_loss = 3.34324\n",
      "epoch no.3 train no.262070  loss = 2.72826 avg_loss = 3.30689\n",
      "epoch no.3 train no.262080  loss = 2.40551 avg_loss = 3.30801\n",
      "epoch no.3 train no.262090  loss = 4.21435 avg_loss = 3.29825\n",
      "epoch no.3 train no.262100  loss = 6.10674 avg_loss = 3.32016\n",
      "epoch no.3 train no.262110  loss = 3.08682 avg_loss = 3.33966\n",
      "epoch no.3 train no.262120  loss = 2.18189 avg_loss = 3.34145\n",
      "epoch no.3 train no.262130  loss = 3.77859 avg_loss = 3.32376\n",
      "epoch no.3 train no.262140  loss = 4.16969 avg_loss = 3.31259\n",
      "epoch no.3 train no.262150  loss = 3.98226 avg_loss = 3.28158\n",
      "epoch no.3 train no.262160  loss = 6.21363 avg_loss = 3.29451\n",
      "epoch no.3 train no.262170  loss = 4.18297 avg_loss = 3.29273\n",
      "epoch no.3 train no.262180  loss = 4.73719 avg_loss = 3.30917\n",
      "epoch no.3 train no.262190  loss = 2.37980 avg_loss = 3.29537\n",
      "epoch no.3 train no.262200  loss = 2.99339 avg_loss = 3.33168\n",
      "epoch no.3 train no.262210  loss = 3.12378 avg_loss = 3.32422\n",
      "epoch no.3 train no.262220  loss = 2.96123 avg_loss = 3.33053\n",
      "epoch no.3 train no.262230  loss = 3.46398 avg_loss = 3.30662\n",
      "epoch no.3 train no.262240  loss = 1.89096 avg_loss = 3.32837\n",
      "epoch no.3 train no.262250  loss = 2.88565 avg_loss = 3.33392\n",
      "epoch no.3 train no.262260  loss = 3.71221 avg_loss = 3.34545\n",
      "epoch no.3 train no.262270  loss = 3.96594 avg_loss = 3.32720\n",
      "epoch no.3 train no.262280  loss = 4.34702 avg_loss = 3.35670\n",
      "epoch no.3 train no.262290  loss = 3.37372 avg_loss = 3.36203\n",
      "epoch no.3 train no.262300  loss = 2.75545 avg_loss = 3.29882\n",
      "epoch no.3 train no.262310  loss = 2.38581 avg_loss = 3.29763\n",
      "epoch no.3 train no.262320  loss = 2.47737 avg_loss = 3.33198\n",
      "epoch no.3 train no.262330  loss = 3.99382 avg_loss = 3.34393\n",
      "epoch no.3 train no.262340  loss = 3.21131 avg_loss = 3.29587\n",
      "epoch no.3 train no.262350  loss = 2.30655 avg_loss = 3.26600\n",
      "epoch no.3 train no.262360  loss = 3.66205 avg_loss = 3.29511\n",
      "epoch no.3 train no.262370  loss = 3.56994 avg_loss = 3.29917\n",
      "epoch no.3 train no.262380  loss = 3.57661 avg_loss = 3.31600\n",
      "epoch no.3 train no.262390  loss = 2.47984 avg_loss = 3.24782\n",
      "epoch no.3 train no.262400  loss = 3.09257 avg_loss = 3.29137\n",
      "epoch no.3 train no.262410  loss = 3.53875 avg_loss = 3.26590\n",
      "epoch no.3 train no.262420  loss = 3.83871 avg_loss = 3.31484\n",
      "epoch no.3 train no.262430  loss = 4.54129 avg_loss = 3.39011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.262440  loss = 5.07417 avg_loss = 3.41919\n",
      "epoch no.3 train no.262450  loss = 2.99629 avg_loss = 3.37097\n",
      "epoch no.3 train no.262460  loss = 3.40255 avg_loss = 3.37377\n",
      "epoch no.3 train no.262470  loss = 4.89695 avg_loss = 3.43502\n",
      "epoch no.3 train no.262480  loss = 3.94011 avg_loss = 3.43568\n",
      "epoch no.3 train no.262490  loss = 2.89292 avg_loss = 3.44466\n",
      "epoch no.3 train no.262500  loss = 3.23818 avg_loss = 3.39646\n",
      "epoch no.3 train no.262510  loss = 4.34784 avg_loss = 3.34799\n",
      "epoch no.3 train no.262520  loss = 5.98184 avg_loss = 3.30525\n",
      "epoch no.3 train no.262530  loss = 1.98585 avg_loss = 3.30860\n",
      "epoch no.3 train no.262540  loss = 2.86177 avg_loss = 3.24935\n",
      "epoch no.3 train no.262550  loss = 3.12619 avg_loss = 3.25112\n",
      "epoch no.3 train no.262560  loss = 3.66694 avg_loss = 3.29337\n",
      "epoch no.3 train no.262570  loss = 4.59863 avg_loss = 3.29177\n",
      "epoch no.3 train no.262580  loss = 3.58928 avg_loss = 3.30536\n",
      "epoch no.3 train no.262590  loss = 2.81792 avg_loss = 3.29490\n",
      "epoch no.3 train no.262600  loss = 2.72336 avg_loss = 3.28596\n",
      "epoch no.3 train no.262610  loss = 3.61444 avg_loss = 3.30730\n",
      "epoch no.3 train no.262620  loss = 2.55388 avg_loss = 3.31440\n",
      "epoch no.3 train no.262630  loss = 3.58894 avg_loss = 3.30456\n",
      "epoch no.3 train no.262640  loss = 2.19034 avg_loss = 3.28932\n",
      "epoch no.3 train no.262650  loss = 6.85566 avg_loss = 3.29545\n",
      "epoch no.3 train no.262660  loss = 2.42841 avg_loss = 3.31871\n",
      "epoch no.3 train no.262670  loss = 2.85164 avg_loss = 3.33541\n",
      "epoch no.3 train no.262680  loss = 3.46399 avg_loss = 3.36762\n",
      "epoch no.3 train no.262690  loss = 2.44354 avg_loss = 3.33590\n",
      "epoch no.3 train no.262700  loss = 2.48730 avg_loss = 3.33289\n",
      "epoch no.3 train no.262710  loss = 4.48823 avg_loss = 3.37945\n",
      "epoch no.3 train no.262720  loss = 2.54198 avg_loss = 3.42457\n",
      "epoch no.3 train no.262730  loss = 2.99130 avg_loss = 3.44211\n",
      "epoch no.3 train no.262740  loss = 3.37591 avg_loss = 3.40039\n",
      "epoch no.3 train no.262750  loss = 3.52530 avg_loss = 3.40890\n",
      "epoch no.3 train no.262760  loss = 3.42010 avg_loss = 3.41797\n",
      "epoch no.3 train no.262770  loss = 2.66137 avg_loss = 3.50031\n",
      "epoch no.3 train no.262780  loss = 1.65429 avg_loss = 3.51111\n",
      "epoch no.3 train no.262790  loss = 3.45653 avg_loss = 3.45616\n",
      "epoch no.3 train no.262800  loss = 3.93344 avg_loss = 3.43133\n",
      "epoch no.3 train no.262810  loss = 3.76664 avg_loss = 3.46644\n",
      "epoch no.3 train no.262820  loss = 1.00890 avg_loss = 3.44558\n",
      "epoch no.3 train no.262830  loss = 4.05400 avg_loss = 3.40859\n",
      "epoch no.3 train no.262840  loss = 2.18348 avg_loss = 3.40984\n",
      "epoch no.3 train no.262850  loss = 2.90571 avg_loss = 3.39845\n",
      "epoch no.3 train no.262860  loss = 3.09151 avg_loss = 3.44401\n",
      "epoch no.3 train no.262870  loss = 3.10157 avg_loss = 3.46440\n",
      "epoch no.3 train no.262880  loss = 2.46666 avg_loss = 3.48653\n",
      "epoch no.3 train no.262890  loss = 3.38125 avg_loss = 3.52210\n",
      "epoch no.3 train no.262900  loss = 5.44357 avg_loss = 3.50354\n",
      "epoch no.3 train no.262910  loss = 3.35016 avg_loss = 3.47611\n",
      "epoch no.3 train no.262920  loss = 3.10071 avg_loss = 3.46424\n",
      "epoch no.3 train no.262930  loss = 2.59377 avg_loss = 3.44506\n",
      "epoch no.3 train no.262940  loss = 2.93716 avg_loss = 3.44283\n",
      "epoch no.3 train no.262950  loss = 4.57127 avg_loss = 3.47460\n",
      "epoch no.3 train no.262960  loss = 2.12091 avg_loss = 3.44954\n",
      "epoch no.3 train no.262970  loss = 2.38736 avg_loss = 3.44099\n",
      "epoch no.3 train no.262980  loss = 3.75525 avg_loss = 3.41650\n",
      "epoch no.3 train no.262990  loss = 4.92843 avg_loss = 3.46838\n",
      "epoch no.3 train no.263000  loss = 5.01360 avg_loss = 3.54436\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '을', '▁듣는', '▁감성', '음악', '</s>']\n",
      "여름밤에 듣는 감성곡</s>\n",
      "epoch no.3 train no.263010  loss = 3.85601 avg_loss = 3.52949\n",
      "epoch no.3 train no.263020  loss = 3.09820 avg_loss = 3.53372\n",
      "epoch no.3 train no.263030  loss = 2.67253 avg_loss = 3.52699\n",
      "epoch no.3 train no.263040  loss = 3.08304 avg_loss = 3.53004\n",
      "epoch no.3 train no.263050  loss = 2.53758 avg_loss = 3.52165\n",
      "epoch no.3 train no.263060  loss = 2.71224 avg_loss = 3.47612\n",
      "epoch no.3 train no.263070  loss = 2.73760 avg_loss = 3.50613\n",
      "epoch no.3 train no.263080  loss = 2.28261 avg_loss = 3.46174\n",
      "epoch no.3 train no.263090  loss = 3.27259 avg_loss = 3.44859\n",
      "epoch no.3 train no.263100  loss = 3.10374 avg_loss = 3.43871\n",
      "epoch no.3 train no.263110  loss = 1.78007 avg_loss = 3.36531\n",
      "epoch no.3 train no.263120  loss = 3.34051 avg_loss = 3.41529\n",
      "epoch no.3 train no.263130  loss = 3.37376 avg_loss = 3.38716\n",
      "epoch no.3 train no.263140  loss = 4.60010 avg_loss = 3.36578\n",
      "epoch no.3 train no.263150  loss = 1.45579 avg_loss = 3.37415\n",
      "epoch no.3 train no.263160  loss = 3.70359 avg_loss = 3.36635\n",
      "epoch no.3 train no.263170  loss = 4.62027 avg_loss = 3.36930\n",
      "epoch no.3 train no.263180  loss = 4.18513 avg_loss = 3.37734\n",
      "epoch no.3 train no.263190  loss = 3.18740 avg_loss = 3.34021\n",
      "epoch no.3 train no.263200  loss = 2.28879 avg_loss = 3.30419\n",
      "epoch no.3 train no.263210  loss = 4.28712 avg_loss = 3.32413\n",
      "epoch no.3 train no.263220  loss = 4.54359 avg_loss = 3.37783\n",
      "epoch no.3 train no.263230  loss = 2.18367 avg_loss = 3.40576\n",
      "epoch no.3 train no.263240  loss = 3.29887 avg_loss = 3.42520\n",
      "epoch no.3 train no.263250  loss = 1.97367 avg_loss = 3.38454\n",
      "epoch no.3 train no.263260  loss = 3.72361 avg_loss = 3.38992\n",
      "epoch no.3 train no.263270  loss = 2.73284 avg_loss = 3.39622\n",
      "epoch no.3 train no.263280  loss = 4.07116 avg_loss = 3.37530\n",
      "epoch no.3 train no.263290  loss = 4.01190 avg_loss = 3.37109\n",
      "epoch no.3 train no.263300  loss = 2.28394 avg_loss = 3.39682\n",
      "epoch no.3 train no.263310  loss = 3.77769 avg_loss = 3.38877\n",
      "epoch no.3 train no.263320  loss = 3.31582 avg_loss = 3.37316\n",
      "epoch no.3 train no.263330  loss = 4.21984 avg_loss = 3.36206\n",
      "epoch no.3 train no.263340  loss = 2.97965 avg_loss = 3.36416\n",
      "epoch no.3 train no.263350  loss = 2.97083 avg_loss = 3.36007\n",
      "epoch no.3 train no.263360  loss = 2.23743 avg_loss = 3.35548\n",
      "epoch no.3 train no.263370  loss = 2.50717 avg_loss = 3.34373\n",
      "epoch no.3 train no.263380  loss = 4.40555 avg_loss = 3.34911\n",
      "epoch no.3 train no.263390  loss = 2.77833 avg_loss = 3.31494\n",
      "epoch no.3 train no.263400  loss = 2.43889 avg_loss = 3.30688\n",
      "epoch no.3 train no.263410  loss = 2.78654 avg_loss = 3.32450\n",
      "epoch no.3 train no.263420  loss = 3.12683 avg_loss = 3.30048\n",
      "epoch no.3 train no.263430  loss = 4.41517 avg_loss = 3.32277\n",
      "epoch no.3 train no.263440  loss = 2.74608 avg_loss = 3.28813\n",
      "epoch no.3 train no.263450  loss = 5.59060 avg_loss = 3.33022\n",
      "epoch no.3 train no.263460  loss = 2.77089 avg_loss = 3.29048\n",
      "epoch no.3 train no.263470  loss = 2.15044 avg_loss = 3.27302\n",
      "epoch no.3 train no.263480  loss = 2.51256 avg_loss = 3.25081\n",
      "epoch no.3 train no.263490  loss = 2.86437 avg_loss = 3.24431\n",
      "epoch no.3 train no.263500  loss = 2.56027 avg_loss = 3.23694\n",
      "epoch no.3 train no.263510  loss = 2.03378 avg_loss = 3.21654\n",
      "epoch no.3 train no.263520  loss = 3.76021 avg_loss = 3.25924\n",
      "epoch no.3 train no.263530  loss = 3.91192 avg_loss = 3.26650\n",
      "epoch no.3 train no.263540  loss = 3.16936 avg_loss = 3.24764\n",
      "epoch no.3 train no.263550  loss = 3.65446 avg_loss = 3.27768\n",
      "epoch no.3 train no.263560  loss = 1.91761 avg_loss = 3.28165\n",
      "epoch no.3 train no.263570  loss = 4.02472 avg_loss = 3.27944\n",
      "epoch no.3 train no.263580  loss = 2.92157 avg_loss = 3.31660\n",
      "epoch no.3 train no.263590  loss = 4.47179 avg_loss = 3.34636\n",
      "epoch no.3 train no.263600  loss = 2.29462 avg_loss = 3.30586\n",
      "epoch no.3 train no.263610  loss = 3.65526 avg_loss = 3.34370\n",
      "epoch no.3 train no.263620  loss = 3.25140 avg_loss = 3.36873\n",
      "epoch no.3 train no.263630  loss = 3.58505 avg_loss = 3.38942\n",
      "epoch no.3 train no.263640  loss = 1.91341 avg_loss = 3.35188\n",
      "epoch no.3 train no.263650  loss = 2.41442 avg_loss = 3.31994\n",
      "epoch no.3 train no.263660  loss = 4.88412 avg_loss = 3.29824\n",
      "epoch no.3 train no.263670  loss = 3.19694 avg_loss = 3.30478\n",
      "epoch no.3 train no.263680  loss = 2.16280 avg_loss = 3.30030\n",
      "epoch no.3 train no.263690  loss = 3.82508 avg_loss = 3.33745\n",
      "epoch no.3 train no.263700  loss = 3.52813 avg_loss = 3.33849\n",
      "epoch no.3 train no.263710  loss = 5.06692 avg_loss = 3.36398\n",
      "epoch no.3 train no.263720  loss = 4.45591 avg_loss = 3.38393\n",
      "epoch no.3 train no.263730  loss = 4.72837 avg_loss = 3.35845\n",
      "epoch no.3 train no.263740  loss = 2.57281 avg_loss = 3.37024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.263750  loss = 4.82162 avg_loss = 3.37686\n",
      "epoch no.3 train no.263760  loss = 2.70614 avg_loss = 3.34021\n",
      "epoch no.3 train no.263770  loss = 2.49967 avg_loss = 3.37005\n",
      "epoch no.3 train no.263780  loss = 2.23064 avg_loss = 3.35476\n",
      "epoch no.3 train no.263790  loss = 2.87980 avg_loss = 3.33498\n",
      "epoch no.3 train no.263800  loss = 3.60345 avg_loss = 3.32851\n",
      "epoch no.3 train no.263810  loss = 2.88395 avg_loss = 3.33221\n",
      "epoch no.3 train no.263820  loss = 4.03340 avg_loss = 3.35882\n",
      "epoch no.3 train no.263830  loss = 3.49238 avg_loss = 3.34281\n",
      "epoch no.3 train no.263840  loss = 2.50207 avg_loss = 3.32266\n",
      "epoch no.3 train no.263850  loss = 2.46916 avg_loss = 3.30956\n",
      "epoch no.3 train no.263860  loss = 2.03817 avg_loss = 3.27997\n",
      "epoch no.3 train no.263870  loss = 3.58951 avg_loss = 3.28907\n",
      "epoch no.3 train no.263880  loss = 2.48671 avg_loss = 3.23984\n",
      "epoch no.3 train no.263890  loss = 4.73532 avg_loss = 3.24686\n",
      "epoch no.3 train no.263900  loss = 2.13190 avg_loss = 3.24870\n",
      "epoch no.3 train no.263910  loss = 2.71656 avg_loss = 3.31208\n",
      "epoch no.3 train no.263920  loss = 3.53237 avg_loss = 3.27729\n",
      "epoch no.3 train no.263930  loss = 2.58709 avg_loss = 3.30456\n",
      "epoch no.3 train no.263940  loss = 3.26101 avg_loss = 3.32973\n",
      "epoch no.3 train no.263950  loss = 4.04754 avg_loss = 3.29357\n",
      "epoch no.3 train no.263960  loss = 3.97164 avg_loss = 3.30821\n",
      "epoch no.3 train no.263970  loss = 2.01649 avg_loss = 3.33567\n",
      "epoch no.3 train no.263980  loss = 2.49024 avg_loss = 3.34150\n",
      "epoch no.3 train no.263990  loss = 4.38598 avg_loss = 3.37491\n",
      "epoch no.3 train no.264000  loss = 3.53473 avg_loss = 3.36541\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '을', '▁수', '놓을', '▁감성', '적인', '디', '</s>']\n",
      "여름밤을 수놓을 감성인디</s>\n",
      "epoch no.3 train no.264010  loss = 2.91858 avg_loss = 3.46641\n",
      "epoch no.3 train no.264020  loss = 2.65125 avg_loss = 3.40767\n",
      "epoch no.3 train no.264030  loss = 4.17981 avg_loss = 3.38922\n",
      "epoch no.3 train no.264040  loss = 3.55372 avg_loss = 3.35443\n",
      "epoch no.3 train no.264050  loss = 3.12544 avg_loss = 3.33037\n",
      "epoch no.3 train no.264060  loss = 4.49730 avg_loss = 3.37366\n",
      "epoch no.3 train no.264070  loss = 4.80015 avg_loss = 3.41550\n",
      "epoch no.3 train no.264080  loss = 4.25940 avg_loss = 3.36856\n",
      "epoch no.3 train no.264090  loss = 3.69983 avg_loss = 3.35249\n",
      "epoch no.3 train no.264100  loss = 3.02296 avg_loss = 3.35449\n",
      "epoch no.3 train no.264110  loss = 4.04555 avg_loss = 3.38164\n",
      "epoch no.3 train no.264120  loss = 1.97900 avg_loss = 3.35693\n",
      "epoch no.3 train no.264130  loss = 3.40160 avg_loss = 3.34233\n",
      "epoch no.3 train no.264140  loss = 2.59369 avg_loss = 3.32055\n",
      "epoch no.3 train no.264150  loss = 2.73961 avg_loss = 3.35024\n",
      "epoch no.3 train no.264160  loss = 2.28287 avg_loss = 3.32634\n",
      "epoch no.3 train no.264170  loss = 3.12625 avg_loss = 3.28723\n",
      "epoch no.3 train no.264180  loss = 2.29944 avg_loss = 3.29424\n",
      "epoch no.3 train no.264190  loss = 3.37303 avg_loss = 3.25537\n",
      "epoch no.3 train no.264200  loss = 2.79650 avg_loss = 3.23687\n",
      "epoch no.3 train no.264210  loss = 4.03807 avg_loss = 3.27224\n",
      "epoch no.3 train no.264220  loss = 1.91901 avg_loss = 3.27136\n",
      "epoch no.3 train no.264230  loss = 4.45876 avg_loss = 3.27298\n",
      "epoch no.3 train no.264240  loss = 2.41989 avg_loss = 3.26866\n",
      "epoch no.3 train no.264250  loss = 5.10933 avg_loss = 3.32005\n",
      "epoch no.3 train no.264260  loss = 2.17922 avg_loss = 3.30450\n",
      "epoch no.3 train no.264270  loss = 4.73581 avg_loss = 3.35201\n",
      "epoch no.3 train no.264280  loss = 2.62053 avg_loss = 3.34619\n",
      "epoch no.3 train no.264290  loss = 4.17358 avg_loss = 3.37255\n",
      "epoch no.3 train no.264300  loss = 3.73723 avg_loss = 3.40050\n",
      "epoch no.3 train no.264310  loss = 2.93448 avg_loss = 3.35977\n",
      "epoch no.3 train no.264320  loss = 2.37267 avg_loss = 3.37268\n",
      "epoch no.3 train no.264330  loss = 2.16820 avg_loss = 3.35019\n",
      "epoch no.3 train no.264340  loss = 1.67943 avg_loss = 3.37309\n",
      "epoch no.3 train no.264350  loss = 6.27945 avg_loss = 3.42147\n",
      "epoch no.3 train no.264360  loss = 2.49191 avg_loss = 3.35988\n",
      "epoch no.3 train no.264370  loss = 3.71908 avg_loss = 3.37044\n",
      "epoch no.3 train no.264380  loss = 6.67772 avg_loss = 3.42811\n",
      "epoch no.3 train no.264390  loss = 3.72194 avg_loss = 3.41250\n",
      "epoch no.3 train no.264400  loss = 2.19913 avg_loss = 3.41948\n",
      "epoch no.3 train no.264410  loss = 2.16822 avg_loss = 3.39636\n",
      "epoch no.3 train no.264420  loss = 1.90978 avg_loss = 3.41461\n",
      "epoch no.3 train no.264430  loss = 3.81133 avg_loss = 3.41678\n",
      "epoch no.3 train no.264440  loss = 4.39663 avg_loss = 3.44515\n",
      "epoch no.3 train no.264450  loss = 1.93360 avg_loss = 3.39998\n",
      "epoch no.3 train no.264460  loss = 3.52601 avg_loss = 3.39681\n",
      "epoch no.3 train no.264470  loss = 3.05544 avg_loss = 3.39424\n",
      "epoch no.3 train no.264480  loss = 3.44693 avg_loss = 3.38541\n",
      "epoch no.3 train no.264490  loss = 2.37221 avg_loss = 3.34422\n",
      "epoch no.3 train no.264500  loss = 2.41676 avg_loss = 3.28537\n",
      "epoch no.3 train no.264510  loss = 4.65016 avg_loss = 3.31399\n",
      "epoch no.3 train no.264520  loss = 2.02601 avg_loss = 3.28568\n",
      "epoch no.3 train no.264530  loss = 2.98246 avg_loss = 3.27003\n",
      "epoch no.3 train no.264540  loss = 3.18115 avg_loss = 3.28060\n",
      "epoch no.3 train no.264550  loss = 2.08736 avg_loss = 3.34748\n",
      "epoch no.3 train no.264560  loss = 2.20103 avg_loss = 3.32665\n",
      "epoch no.3 train no.264570  loss = 5.75325 avg_loss = 3.37914\n",
      "epoch no.3 train no.264580  loss = 3.15052 avg_loss = 3.36857\n",
      "epoch no.3 train no.264590  loss = 2.67154 avg_loss = 3.33072\n",
      "epoch no.3 train no.264600  loss = 2.48003 avg_loss = 3.31322\n",
      "epoch no.3 train no.264610  loss = 2.59217 avg_loss = 3.30106\n",
      "epoch no.3 train no.264620  loss = 4.33868 avg_loss = 3.38513\n",
      "epoch no.3 train no.264630  loss = 4.17976 avg_loss = 3.39119\n",
      "epoch no.3 train no.264640  loss = 3.09422 avg_loss = 3.44255\n",
      "epoch no.3 train no.264650  loss = 2.75486 avg_loss = 3.43783\n",
      "epoch no.3 train no.264660  loss = 3.93665 avg_loss = 3.45632\n",
      "epoch no.3 train no.264670  loss = 3.00437 avg_loss = 3.39775\n",
      "epoch no.3 train no.264680  loss = 2.29663 avg_loss = 3.44108\n",
      "epoch no.3 train no.264690  loss = 4.02838 avg_loss = 3.42953\n",
      "epoch no.3 train no.264700  loss = 3.04184 avg_loss = 3.42093\n",
      "epoch no.3 train no.264710  loss = 4.08863 avg_loss = 3.38946\n",
      "epoch no.3 train no.264720  loss = 3.82279 avg_loss = 3.35987\n",
      "epoch no.3 train no.264730  loss = 5.13748 avg_loss = 3.41955\n",
      "epoch no.3 train no.264740  loss = 3.46569 avg_loss = 3.39350\n",
      "epoch no.3 train no.264750  loss = 3.06175 avg_loss = 3.40281\n",
      "epoch no.3 train no.264760  loss = 3.36274 avg_loss = 3.38531\n",
      "epoch no.3 train no.264770  loss = 2.11322 avg_loss = 3.38252\n",
      "epoch no.3 train no.264780  loss = 2.58079 avg_loss = 3.35218\n",
      "epoch no.3 train no.264790  loss = 4.57746 avg_loss = 3.37846\n",
      "epoch no.3 train no.264800  loss = 4.65624 avg_loss = 3.42235\n",
      "epoch no.3 train no.264810  loss = 3.09600 avg_loss = 3.45313\n",
      "epoch no.3 train no.264820  loss = 2.79517 avg_loss = 3.44913\n",
      "epoch no.3 train no.264830  loss = 1.63346 avg_loss = 3.43340\n",
      "epoch no.3 train no.264840  loss = 3.81546 avg_loss = 3.42487\n",
      "epoch no.3 train no.264850  loss = 4.46658 avg_loss = 3.38309\n",
      "epoch no.3 train no.264860  loss = 3.84468 avg_loss = 3.41622\n",
      "epoch no.3 train no.264870  loss = 2.62668 avg_loss = 3.39305\n",
      "epoch no.3 train no.264880  loss = 4.29058 avg_loss = 3.40751\n",
      "epoch no.3 train no.264890  loss = 2.32927 avg_loss = 3.40858\n",
      "epoch no.3 train no.264900  loss = 3.32337 avg_loss = 3.34314\n",
      "epoch no.3 train no.264910  loss = 3.99091 avg_loss = 3.37003\n",
      "epoch no.3 train no.264920  loss = 3.53760 avg_loss = 3.40433\n",
      "epoch no.3 train no.264930  loss = 4.88671 avg_loss = 3.45376\n",
      "epoch no.3 train no.264940  loss = 2.79522 avg_loss = 3.45365\n",
      "epoch no.3 train no.264950  loss = 4.04718 avg_loss = 3.44697\n",
      "epoch no.3 train no.264960  loss = 3.30031 avg_loss = 3.44955\n",
      "epoch no.3 train no.264970  loss = 3.70166 avg_loss = 3.45836\n",
      "epoch no.3 train no.264980  loss = 2.91616 avg_loss = 3.42334\n",
      "epoch no.3 train no.264990  loss = 3.44937 avg_loss = 3.48878\n",
      "epoch no.3 train no.265000  loss = 3.53734 avg_loss = 3.51614\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '음악', '</s>']\n",
      "여름밤에 듣기 좋은 인디음악</s>\n",
      "epoch no.3 train no.265010  loss = 4.71183 avg_loss = 3.54092\n",
      "epoch no.3 train no.265020  loss = 2.52635 avg_loss = 3.53922\n",
      "epoch no.3 train no.265030  loss = 2.59298 avg_loss = 3.48855\n",
      "epoch no.3 train no.265040  loss = 3.37662 avg_loss = 3.47472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.265050  loss = 4.12330 avg_loss = 3.43805\n",
      "epoch no.3 train no.265060  loss = 2.97202 avg_loss = 3.43173\n",
      "epoch no.3 train no.265070  loss = 3.54407 avg_loss = 3.42204\n",
      "epoch no.3 train no.265080  loss = 3.43903 avg_loss = 3.41086\n",
      "epoch no.3 train no.265090  loss = 3.68884 avg_loss = 3.40011\n",
      "epoch no.3 train no.265100  loss = 3.64171 avg_loss = 3.39720\n",
      "epoch no.3 train no.265110  loss = 3.49669 avg_loss = 3.39126\n",
      "epoch no.3 train no.265120  loss = 2.05032 avg_loss = 3.36371\n",
      "epoch no.3 train no.265130  loss = 2.53757 avg_loss = 3.37619\n",
      "epoch no.3 train no.265140  loss = 3.08183 avg_loss = 3.32218\n",
      "epoch no.3 train no.265150  loss = 2.96453 avg_loss = 3.31442\n",
      "epoch no.3 train no.265160  loss = 3.52553 avg_loss = 3.31118\n",
      "epoch no.3 train no.265170  loss = 1.91612 avg_loss = 3.29639\n",
      "epoch no.3 train no.265180  loss = 3.03298 avg_loss = 3.30203\n",
      "epoch no.3 train no.265190  loss = 2.65062 avg_loss = 3.29566\n",
      "epoch no.3 train no.265200  loss = 2.81032 avg_loss = 3.32881\n",
      "epoch no.3 train no.265210  loss = 3.31997 avg_loss = 3.35285\n",
      "epoch no.3 train no.265220  loss = 4.00412 avg_loss = 3.35348\n",
      "epoch no.3 train no.265230  loss = 2.02543 avg_loss = 3.37841\n",
      "epoch no.3 train no.265240  loss = 3.22184 avg_loss = 3.39200\n",
      "epoch no.3 train no.265250  loss = 2.66554 avg_loss = 3.35946\n",
      "epoch no.3 train no.265260  loss = 3.52989 avg_loss = 3.31379\n",
      "epoch no.3 train no.265270  loss = 4.83092 avg_loss = 3.31177\n",
      "epoch no.3 train no.265280  loss = 4.63769 avg_loss = 3.29994\n",
      "epoch no.3 train no.265290  loss = 4.30648 avg_loss = 3.31229\n",
      "epoch no.3 train no.265300  loss = 3.21775 avg_loss = 3.33022\n",
      "epoch no.3 train no.265310  loss = 2.71322 avg_loss = 3.31642\n",
      "epoch no.3 train no.265320  loss = 5.07525 avg_loss = 3.36479\n",
      "epoch no.3 train no.265330  loss = 3.54357 avg_loss = 3.34526\n",
      "epoch no.3 train no.265340  loss = 3.16646 avg_loss = 3.30721\n",
      "epoch no.3 train no.265350  loss = 4.69897 avg_loss = 3.30596\n",
      "epoch no.3 train no.265360  loss = 4.04245 avg_loss = 3.30828\n",
      "epoch no.3 train no.265370  loss = 4.00294 avg_loss = 3.32237\n",
      "epoch no.3 train no.265380  loss = 2.37618 avg_loss = 3.36222\n",
      "epoch no.3 train no.265390  loss = 2.33444 avg_loss = 3.35032\n",
      "epoch no.3 train no.265400  loss = 3.83965 avg_loss = 3.34594\n",
      "epoch no.3 train no.265410  loss = 2.66700 avg_loss = 3.37849\n",
      "epoch no.3 train no.265420  loss = 2.75595 avg_loss = 3.35430\n",
      "epoch no.3 train no.265430  loss = 1.78283 avg_loss = 3.34709\n",
      "epoch no.3 train no.265440  loss = 3.26206 avg_loss = 3.33965\n",
      "epoch no.3 train no.265450  loss = 3.29347 avg_loss = 3.28293\n",
      "epoch no.3 train no.265460  loss = 3.90322 avg_loss = 3.33353\n",
      "epoch no.3 train no.265470  loss = 6.23267 avg_loss = 3.36678\n",
      "epoch no.3 train no.265480  loss = 2.69553 avg_loss = 3.35905\n",
      "epoch no.3 train no.265490  loss = 4.32945 avg_loss = 3.35261\n",
      "epoch no.3 train no.265500  loss = 2.50399 avg_loss = 3.33317\n",
      "epoch no.3 train no.265510  loss = 2.66440 avg_loss = 3.32262\n",
      "epoch no.3 train no.265520  loss = 3.14601 avg_loss = 3.31888\n",
      "epoch no.3 train no.265530  loss = 3.10395 avg_loss = 3.32660\n",
      "epoch no.3 train no.265540  loss = 3.84839 avg_loss = 3.35043\n",
      "epoch no.3 train no.265550  loss = 2.95022 avg_loss = 3.34264\n",
      "epoch no.3 train no.265560  loss = 1.59270 avg_loss = 3.32363\n",
      "epoch no.3 train no.265570  loss = 2.83280 avg_loss = 3.28016\n",
      "epoch no.3 train no.265580  loss = 4.38020 avg_loss = 3.27806\n",
      "epoch no.3 train no.265590  loss = 3.30416 avg_loss = 3.32344\n",
      "epoch no.3 train no.265600  loss = 2.13944 avg_loss = 3.32285\n",
      "epoch no.3 train no.265610  loss = 4.63107 avg_loss = 3.32551\n",
      "epoch no.3 train no.265620  loss = 2.95828 avg_loss = 3.32032\n",
      "epoch no.3 train no.265630  loss = 2.36421 avg_loss = 3.29295\n",
      "epoch no.3 train no.265640  loss = 2.98596 avg_loss = 3.32222\n",
      "epoch no.3 train no.265650  loss = 2.54988 avg_loss = 3.32297\n",
      "epoch no.3 train no.265660  loss = 3.10752 avg_loss = 3.32442\n",
      "epoch no.3 train no.265670  loss = 3.43168 avg_loss = 3.38390\n",
      "epoch no.3 train no.265680  loss = 2.76514 avg_loss = 3.40176\n",
      "epoch no.3 train no.265690  loss = 2.69855 avg_loss = 3.40750\n",
      "epoch no.3 train no.265700  loss = 2.96077 avg_loss = 3.36999\n",
      "epoch no.3 train no.265710  loss = 4.40362 avg_loss = 3.43141\n",
      "epoch no.3 train no.265720  loss = 5.03089 avg_loss = 3.38992\n",
      "epoch no.3 train no.265730  loss = 4.07019 avg_loss = 3.37818\n",
      "epoch no.3 train no.265740  loss = 2.94959 avg_loss = 3.34614\n",
      "epoch no.3 train no.265750  loss = 4.15601 avg_loss = 3.39307\n",
      "epoch no.3 train no.265760  loss = 4.97304 avg_loss = 3.46557\n",
      "epoch no.3 train no.265770  loss = 2.71740 avg_loss = 3.44925\n",
      "epoch no.3 train no.265780  loss = 2.95379 avg_loss = 3.40922\n",
      "epoch no.3 train no.265790  loss = 3.71211 avg_loss = 3.38449\n",
      "epoch no.3 train no.265800  loss = 2.17227 avg_loss = 3.34276\n",
      "epoch no.3 train no.265810  loss = 3.53201 avg_loss = 3.31905\n",
      "epoch no.3 train no.265820  loss = 3.19756 avg_loss = 3.30097\n",
      "epoch no.3 train no.265830  loss = 1.50693 avg_loss = 3.34575\n",
      "epoch no.3 train no.265840  loss = 4.46791 avg_loss = 3.34239\n",
      "epoch no.3 train no.265850  loss = 2.83739 avg_loss = 3.34058\n",
      "epoch no.3 train no.265860  loss = 3.05097 avg_loss = 3.35888\n",
      "epoch no.3 train no.265870  loss = 2.46767 avg_loss = 3.31830\n",
      "epoch no.3 train no.265880  loss = 3.07407 avg_loss = 3.33574\n",
      "epoch no.3 train no.265890  loss = 3.08386 avg_loss = 3.33802\n",
      "epoch no.3 train no.265900  loss = 2.19446 avg_loss = 3.30285\n",
      "epoch no.3 train no.265910  loss = 4.33749 avg_loss = 3.31160\n",
      "epoch no.3 train no.265920  loss = 1.98594 avg_loss = 3.28783\n",
      "epoch no.3 train no.265930  loss = 3.52370 avg_loss = 3.29534\n",
      "epoch no.3 train no.265940  loss = 3.03969 avg_loss = 3.24527\n",
      "epoch no.3 train no.265950  loss = 2.87167 avg_loss = 3.24545\n",
      "epoch no.3 train no.265960  loss = 2.39082 avg_loss = 3.30127\n",
      "epoch no.3 train no.265970  loss = 3.50508 avg_loss = 3.29633\n",
      "epoch no.3 train no.265980  loss = 2.98804 avg_loss = 3.26410\n",
      "epoch no.3 train no.265990  loss = 3.35080 avg_loss = 3.27659\n",
      "epoch no.3 train no.266000  loss = 4.28934 avg_loss = 3.30751\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '▁어울리는', '▁노래', '</s>']\n",
      "여름밤에 듣는 음악</s>\n",
      "epoch no.3 train no.266010  loss = 3.06863 avg_loss = 3.27790\n",
      "epoch no.3 train no.266020  loss = 3.21083 avg_loss = 3.24405\n",
      "epoch no.3 train no.266030  loss = 4.35529 avg_loss = 3.25951\n",
      "epoch no.3 train no.266040  loss = 4.52835 avg_loss = 3.28567\n",
      "epoch no.3 train no.266050  loss = 4.85450 avg_loss = 3.28879\n",
      "epoch no.3 train no.266060  loss = 3.73597 avg_loss = 3.31569\n",
      "epoch no.3 train no.266070  loss = 3.92997 avg_loss = 3.33949\n",
      "epoch no.3 train no.266080  loss = 3.10242 avg_loss = 3.37721\n",
      "epoch no.3 train no.266090  loss = 4.75630 avg_loss = 3.41409\n",
      "epoch no.3 train no.266100  loss = 3.63527 avg_loss = 3.37351\n",
      "epoch no.3 train no.266110  loss = 2.01077 avg_loss = 3.39215\n",
      "epoch no.3 train no.266120  loss = 3.85543 avg_loss = 3.37933\n",
      "epoch no.3 train no.266130  loss = 2.32922 avg_loss = 3.37193\n",
      "epoch no.3 train no.266140  loss = 3.21122 avg_loss = 3.38170\n",
      "epoch no.3 train no.266150  loss = 4.39721 avg_loss = 3.43827\n",
      "epoch no.3 train no.266160  loss = 3.21935 avg_loss = 3.41377\n",
      "epoch no.3 train no.266170  loss = 2.74101 avg_loss = 3.42798\n",
      "epoch no.3 train no.266180  loss = 4.95331 avg_loss = 3.44804\n",
      "epoch no.3 train no.266190  loss = 4.56155 avg_loss = 3.43341\n",
      "epoch no.3 train no.266200  loss = 3.26088 avg_loss = 3.43584\n",
      "epoch no.3 train no.266210  loss = 2.73146 avg_loss = 3.42414\n",
      "epoch no.3 train no.266220  loss = 3.11513 avg_loss = 3.43017\n",
      "epoch no.3 train no.266230  loss = 3.01390 avg_loss = 3.40309\n",
      "epoch no.3 train no.266240  loss = 3.19703 avg_loss = 3.40440\n",
      "epoch no.3 train no.266250  loss = 1.98719 avg_loss = 3.40570\n",
      "epoch no.3 train no.266260  loss = 3.37250 avg_loss = 3.42395\n",
      "epoch no.3 train no.266270  loss = 2.68085 avg_loss = 3.37814\n",
      "epoch no.3 train no.266280  loss = 3.01708 avg_loss = 3.34713\n",
      "epoch no.3 train no.266290  loss = 2.97881 avg_loss = 3.34185\n",
      "epoch no.3 train no.266300  loss = 2.37002 avg_loss = 3.32345\n",
      "epoch no.3 train no.266310  loss = 4.89521 avg_loss = 3.31759\n",
      "epoch no.3 train no.266320  loss = 4.09131 avg_loss = 3.36197\n",
      "epoch no.3 train no.266330  loss = 3.06000 avg_loss = 3.33137\n",
      "epoch no.3 train no.266340  loss = 3.24541 avg_loss = 3.32048\n",
      "epoch no.3 train no.266350  loss = 2.91188 avg_loss = 3.33033\n",
      "epoch no.3 train no.266360  loss = 3.26813 avg_loss = 3.38071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.266370  loss = 3.55787 avg_loss = 3.36055\n",
      "epoch no.3 train no.266380  loss = 3.02453 avg_loss = 3.30525\n",
      "epoch no.3 train no.266390  loss = 4.36255 avg_loss = 3.32404\n",
      "epoch no.3 train no.266400  loss = 4.36122 avg_loss = 3.36488\n",
      "epoch no.3 train no.266410  loss = 3.28529 avg_loss = 3.34397\n",
      "epoch no.3 train no.266420  loss = 2.22089 avg_loss = 3.32002\n",
      "epoch no.3 train no.266430  loss = 3.68984 avg_loss = 3.34751\n",
      "epoch no.3 train no.266440  loss = 4.09676 avg_loss = 3.35350\n",
      "epoch no.3 train no.266450  loss = 3.85140 avg_loss = 3.35250\n",
      "epoch no.3 train no.266460  loss = 2.78108 avg_loss = 3.35862\n",
      "epoch no.3 train no.266470  loss = 3.90564 avg_loss = 3.35662\n",
      "epoch no.3 train no.266480  loss = 3.18539 avg_loss = 3.32574\n",
      "epoch no.3 train no.266490  loss = 2.49070 avg_loss = 3.32151\n",
      "epoch no.3 train no.266500  loss = 4.27504 avg_loss = 3.34710\n",
      "epoch no.3 train no.266510  loss = 4.90489 avg_loss = 3.38885\n",
      "epoch no.3 train no.266520  loss = 3.75769 avg_loss = 3.41721\n",
      "epoch no.3 train no.266530  loss = 3.35815 avg_loss = 3.39086\n",
      "epoch no.3 train no.266540  loss = 4.38437 avg_loss = 3.38678\n",
      "epoch no.3 train no.266550  loss = 3.28291 avg_loss = 3.35745\n",
      "epoch no.3 train no.266560  loss = 1.47112 avg_loss = 3.31904\n",
      "epoch no.3 train no.266570  loss = 4.67909 avg_loss = 3.33945\n",
      "epoch no.3 train no.266580  loss = 1.94106 avg_loss = 3.30299\n",
      "epoch no.3 train no.266590  loss = 2.95933 avg_loss = 3.27538\n",
      "epoch no.3 train no.266600  loss = 3.39528 avg_loss = 3.29883\n",
      "epoch no.3 train no.266610  loss = 4.22893 avg_loss = 3.36646\n",
      "epoch no.3 train no.266620  loss = 3.55430 avg_loss = 3.36701\n",
      "epoch no.3 train no.266630  loss = 1.89442 avg_loss = 3.29404\n",
      "epoch no.3 train no.266640  loss = 1.74092 avg_loss = 3.26441\n",
      "epoch no.3 train no.266650  loss = 3.29726 avg_loss = 3.31428\n",
      "epoch no.3 train no.266660  loss = 3.30020 avg_loss = 3.31264\n",
      "epoch no.3 train no.266670  loss = 4.85574 avg_loss = 3.35377\n",
      "epoch no.3 train no.266680  loss = 4.32545 avg_loss = 3.40795\n",
      "epoch no.3 train no.266690  loss = 2.57384 avg_loss = 3.38547\n",
      "epoch no.3 train no.266700  loss = 5.19304 avg_loss = 3.38161\n",
      "epoch no.3 train no.266710  loss = 3.47360 avg_loss = 3.37713\n",
      "epoch no.3 train no.266720  loss = 3.29021 avg_loss = 3.37438\n",
      "epoch no.3 train no.266730  loss = 3.95319 avg_loss = 3.35396\n",
      "epoch no.3 train no.266740  loss = 5.83315 avg_loss = 3.40354\n",
      "epoch no.3 train no.266750  loss = 2.63206 avg_loss = 3.36319\n",
      "epoch no.3 train no.266760  loss = 3.02255 avg_loss = 3.35072\n",
      "epoch no.3 train no.266770  loss = 2.88429 avg_loss = 3.34087\n",
      "epoch no.3 train no.266780  loss = 2.45431 avg_loss = 3.32631\n",
      "epoch no.3 train no.266790  loss = 3.64822 avg_loss = 3.34177\n",
      "epoch no.3 train no.266800  loss = 4.35750 avg_loss = 3.34771\n",
      "epoch no.3 train no.266810  loss = 3.11149 avg_loss = 3.34895\n",
      "epoch no.3 train no.266820  loss = 2.41416 avg_loss = 3.33469\n",
      "epoch no.3 train no.266830  loss = 3.32345 avg_loss = 3.34412\n",
      "epoch no.3 train no.266840  loss = 3.21621 avg_loss = 3.36223\n",
      "epoch no.3 train no.266850  loss = 2.38361 avg_loss = 3.35669\n",
      "epoch no.3 train no.266860  loss = 2.86376 avg_loss = 3.36354\n",
      "epoch no.3 train no.266870  loss = 3.79780 avg_loss = 3.36971\n",
      "epoch no.3 train no.266880  loss = 5.14919 avg_loss = 3.34607\n",
      "epoch no.3 train no.266890  loss = 3.43969 avg_loss = 3.31095\n",
      "epoch no.3 train no.266900  loss = 2.93317 avg_loss = 3.32319\n",
      "epoch no.3 train no.266910  loss = 2.54254 avg_loss = 3.34499\n",
      "epoch no.3 train no.266920  loss = 6.58124 avg_loss = 3.38184\n",
      "epoch no.3 train no.266930  loss = 2.79475 avg_loss = 3.35451\n",
      "epoch no.3 train no.266940  loss = 2.61402 avg_loss = 3.30477\n",
      "epoch no.3 train no.266950  loss = 3.44149 avg_loss = 3.31357\n",
      "epoch no.3 train no.266960  loss = 3.17960 avg_loss = 3.28632\n",
      "epoch no.3 train no.266970  loss = 4.26234 avg_loss = 3.24107\n",
      "epoch no.3 train no.266980  loss = 2.84490 avg_loss = 3.25876\n",
      "epoch no.3 train no.266990  loss = 2.13857 avg_loss = 3.20901\n",
      "epoch no.3 train no.267000  loss = 2.86235 avg_loss = 3.25650\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '의', '▁재즈', '음악', '</s>']\n",
      "여름밤의 인디음악</s>\n",
      "epoch no.3 train no.267010  loss = 4.04087 avg_loss = 3.25721\n",
      "epoch no.3 train no.267020  loss = 5.54680 avg_loss = 3.35213\n",
      "epoch no.3 train no.267030  loss = 3.47255 avg_loss = 3.32265\n",
      "epoch no.3 train no.267040  loss = 3.27127 avg_loss = 3.34509\n",
      "epoch no.3 train no.267050  loss = 4.15011 avg_loss = 3.37071\n",
      "epoch no.3 train no.267060  loss = 4.39545 avg_loss = 3.35818\n",
      "epoch no.3 train no.267070  loss = 3.42555 avg_loss = 3.32472\n",
      "epoch no.3 train no.267080  loss = 2.12753 avg_loss = 3.33747\n",
      "epoch no.3 train no.267090  loss = 2.35892 avg_loss = 3.29292\n",
      "epoch no.3 train no.267100  loss = 3.25890 avg_loss = 3.29185\n",
      "epoch no.3 train no.267110  loss = 3.56098 avg_loss = 3.28872\n",
      "epoch no.3 train no.267120  loss = 3.75440 avg_loss = 3.32509\n",
      "epoch no.3 train no.267130  loss = 3.81207 avg_loss = 3.37437\n",
      "epoch no.3 train no.267140  loss = 4.21362 avg_loss = 3.38132\n",
      "epoch no.3 train no.267150  loss = 4.76931 avg_loss = 3.40351\n",
      "epoch no.3 train no.267160  loss = 4.83579 avg_loss = 3.40854\n",
      "epoch no.3 train no.267170  loss = 3.04841 avg_loss = 3.42515\n",
      "epoch no.3 train no.267180  loss = 4.80875 avg_loss = 3.44016\n",
      "epoch no.3 train no.267190  loss = 2.93303 avg_loss = 3.41735\n",
      "epoch no.3 train no.267200  loss = 3.95456 avg_loss = 3.37984\n",
      "epoch no.3 train no.267210  loss = 2.61414 avg_loss = 3.39454\n",
      "epoch no.3 train no.267220  loss = 3.74921 avg_loss = 3.36082\n",
      "epoch no.3 train no.267230  loss = 2.64826 avg_loss = 3.34710\n",
      "epoch no.3 train no.267240  loss = 2.87650 avg_loss = 3.35798\n",
      "epoch no.3 train no.267250  loss = 2.40249 avg_loss = 3.33734\n",
      "epoch no.3 train no.267260  loss = 2.14048 avg_loss = 3.30714\n",
      "epoch no.3 train no.267270  loss = 3.17932 avg_loss = 3.30056\n",
      "epoch no.3 train no.267280  loss = 2.60619 avg_loss = 3.28920\n",
      "epoch no.3 train no.267290  loss = 2.95186 avg_loss = 3.32927\n",
      "epoch no.3 train no.267300  loss = 4.15234 avg_loss = 3.32247\n",
      "epoch no.3 train no.267310  loss = 2.89360 avg_loss = 3.27012\n",
      "epoch no.3 train no.267320  loss = 2.61357 avg_loss = 3.29181\n",
      "epoch no.3 train no.267330  loss = 4.11808 avg_loss = 3.30214\n",
      "epoch no.3 train no.267340  loss = 2.79879 avg_loss = 3.30019\n",
      "epoch no.3 train no.267350  loss = 2.73188 avg_loss = 3.23147\n",
      "epoch no.3 train no.267360  loss = 1.32904 avg_loss = 3.22338\n",
      "epoch no.3 train no.267370  loss = 3.44631 avg_loss = 3.24046\n",
      "epoch no.3 train no.267380  loss = 1.71467 avg_loss = 3.28235\n",
      "epoch no.3 train no.267390  loss = 2.99268 avg_loss = 3.36243\n",
      "epoch no.3 train no.267400  loss = 4.93713 avg_loss = 3.40156\n",
      "epoch no.3 train no.267410  loss = 4.26392 avg_loss = 3.38345\n",
      "epoch no.3 train no.267420  loss = 4.08386 avg_loss = 3.36627\n",
      "epoch no.3 train no.267430  loss = 3.38701 avg_loss = 3.38018\n",
      "epoch no.3 train no.267440  loss = 2.85479 avg_loss = 3.36038\n",
      "epoch no.3 train no.267450  loss = 3.18250 avg_loss = 3.37650\n",
      "epoch no.3 train no.267460  loss = 2.35559 avg_loss = 3.38272\n",
      "epoch no.3 train no.267470  loss = 3.67990 avg_loss = 3.37340\n",
      "epoch no.3 train no.267480  loss = 4.78895 avg_loss = 3.41417\n",
      "epoch no.3 train no.267490  loss = 4.92356 avg_loss = 3.42364\n",
      "epoch no.3 train no.267500  loss = 3.19191 avg_loss = 3.46788\n",
      "epoch no.3 train no.267510  loss = 3.22832 avg_loss = 3.45550\n",
      "epoch no.3 train no.267520  loss = 3.10439 avg_loss = 3.41988\n",
      "epoch no.3 train no.267530  loss = 2.57470 avg_loss = 3.41152\n",
      "epoch no.3 train no.267540  loss = 4.36586 avg_loss = 3.45819\n",
      "epoch no.3 train no.267550  loss = 4.50972 avg_loss = 3.46685\n",
      "epoch no.3 train no.267560  loss = 2.63588 avg_loss = 3.41993\n",
      "epoch no.3 train no.267570  loss = 4.95530 avg_loss = 3.41988\n",
      "epoch no.3 train no.267580  loss = 2.87712 avg_loss = 3.38914\n",
      "epoch no.3 train no.267590  loss = 3.84255 avg_loss = 3.35750\n",
      "epoch no.3 train no.267600  loss = 2.19530 avg_loss = 3.31458\n",
      "epoch no.3 train no.267610  loss = 2.25119 avg_loss = 3.29109\n",
      "epoch no.3 train no.267620  loss = 1.44840 avg_loss = 3.26526\n",
      "epoch no.3 train no.267630  loss = 4.15594 avg_loss = 3.26292\n",
      "epoch no.3 train no.267640  loss = 3.47451 avg_loss = 3.23545\n",
      "epoch no.3 train no.267650  loss = 3.33649 avg_loss = 3.20391\n",
      "epoch no.3 train no.267660  loss = 3.69937 avg_loss = 3.22115\n",
      "epoch no.3 train no.267670  loss = 2.79320 avg_loss = 3.24454\n",
      "epoch no.3 train no.267680  loss = 2.48460 avg_loss = 3.31050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.267690  loss = 4.29081 avg_loss = 3.31723\n",
      "epoch no.3 train no.267700  loss = 3.14296 avg_loss = 3.28175\n",
      "epoch no.3 train no.267710  loss = 4.34480 avg_loss = 3.29883\n",
      "epoch no.3 train no.267720  loss = 5.39633 avg_loss = 3.31031\n",
      "epoch no.3 train no.267730  loss = 4.42405 avg_loss = 3.29234\n",
      "epoch no.3 train no.267740  loss = 2.25782 avg_loss = 3.29217\n",
      "epoch no.3 train no.267750  loss = 1.03396 avg_loss = 3.30948\n",
      "epoch no.3 train no.267760  loss = 3.35721 avg_loss = 3.33547\n",
      "epoch no.3 train no.267770  loss = 3.23601 avg_loss = 3.32119\n",
      "epoch no.3 train no.267780  loss = 1.21090 avg_loss = 3.27396\n",
      "epoch no.3 train no.267790  loss = 3.69416 avg_loss = 3.26677\n",
      "epoch no.3 train no.267800  loss = 3.21616 avg_loss = 3.26784\n",
      "epoch no.3 train no.267810  loss = 4.12537 avg_loss = 3.27616\n",
      "epoch no.3 train no.267820  loss = 2.95731 avg_loss = 3.28683\n",
      "epoch no.3 train no.267830  loss = 2.45302 avg_loss = 3.32689\n",
      "epoch no.3 train no.267840  loss = 2.42174 avg_loss = 3.30106\n",
      "epoch no.3 train no.267850  loss = 2.47417 avg_loss = 3.29085\n",
      "epoch no.3 train no.267860  loss = 2.85152 avg_loss = 3.25505\n",
      "epoch no.3 train no.267870  loss = 4.17096 avg_loss = 3.26415\n",
      "epoch no.3 train no.267880  loss = 4.29273 avg_loss = 3.28163\n",
      "epoch no.3 train no.267890  loss = 2.94893 avg_loss = 3.27297\n",
      "epoch no.3 train no.267900  loss = 2.67192 avg_loss = 3.28037\n",
      "epoch no.3 train no.267910  loss = 2.23364 avg_loss = 3.27481\n",
      "epoch no.3 train no.267920  loss = 2.58871 avg_loss = 3.24758\n",
      "epoch no.3 train no.267930  loss = 4.43678 avg_loss = 3.26001\n",
      "epoch no.3 train no.267940  loss = 3.88667 avg_loss = 3.23785\n",
      "epoch no.3 train no.267950  loss = 4.32099 avg_loss = 3.28896\n",
      "epoch no.3 train no.267960  loss = 3.00634 avg_loss = 3.34100\n",
      "epoch no.3 train no.267970  loss = 6.37731 avg_loss = 3.37077\n",
      "epoch no.3 train no.267980  loss = 4.68539 avg_loss = 3.38323\n",
      "epoch no.3 train no.267990  loss = 2.09856 avg_loss = 3.33533\n",
      "epoch no.3 train no.268000  loss = 2.35957 avg_loss = 3.35790\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣는', '▁재즈', '</s>']\n",
      "여름밤에 어울리는 음악</s>\n",
      "epoch no.3 train no.268010  loss = 4.17385 avg_loss = 3.38778\n",
      "epoch no.3 train no.268020  loss = 3.58086 avg_loss = 3.42554\n",
      "epoch no.3 train no.268030  loss = 2.62005 avg_loss = 3.40684\n",
      "epoch no.3 train no.268040  loss = 5.32186 avg_loss = 3.39069\n",
      "epoch no.3 train no.268050  loss = 3.85408 avg_loss = 3.38378\n",
      "epoch no.3 train no.268060  loss = 3.72441 avg_loss = 3.42211\n",
      "epoch no.3 train no.268070  loss = 4.24874 avg_loss = 3.41656\n",
      "epoch no.3 train no.268080  loss = 3.04855 avg_loss = 3.44152\n",
      "epoch no.3 train no.268090  loss = 3.69241 avg_loss = 3.40072\n",
      "epoch no.3 train no.268100  loss = 2.96739 avg_loss = 3.44285\n",
      "epoch no.3 train no.268110  loss = 2.45317 avg_loss = 3.38113\n",
      "epoch no.3 train no.268120  loss = 5.00786 avg_loss = 3.43040\n",
      "epoch no.3 train no.268130  loss = 2.17125 avg_loss = 3.42388\n",
      "epoch no.3 train no.268140  loss = 3.70381 avg_loss = 3.39996\n",
      "epoch no.3 train no.268150  loss = 3.77277 avg_loss = 3.46520\n",
      "epoch no.3 train no.268160  loss = 2.45589 avg_loss = 3.47760\n",
      "epoch no.3 train no.268170  loss = 2.82651 avg_loss = 3.39895\n",
      "epoch no.3 train no.268180  loss = 4.14459 avg_loss = 3.38387\n",
      "epoch no.3 train no.268190  loss = 3.98012 avg_loss = 3.36997\n",
      "epoch no.3 train no.268200  loss = 3.27307 avg_loss = 3.41276\n",
      "epoch no.3 train no.268210  loss = 2.15861 avg_loss = 3.41569\n",
      "epoch no.3 train no.268220  loss = 4.28501 avg_loss = 3.43032\n",
      "epoch no.3 train no.268230  loss = 3.25792 avg_loss = 3.43319\n",
      "epoch no.3 train no.268240  loss = 2.69150 avg_loss = 3.41441\n",
      "epoch no.3 train no.268250  loss = 5.76599 avg_loss = 3.39230\n",
      "epoch no.3 train no.268260  loss = 2.78740 avg_loss = 3.36649\n",
      "epoch no.3 train no.268270  loss = 2.73373 avg_loss = 3.36329\n",
      "epoch no.3 train no.268280  loss = 2.22879 avg_loss = 3.34309\n",
      "epoch no.3 train no.268290  loss = 3.64249 avg_loss = 3.31690\n",
      "epoch no.3 train no.268300  loss = 4.53627 avg_loss = 3.34521\n",
      "epoch no.3 train no.268310  loss = 3.53959 avg_loss = 3.32469\n",
      "epoch no.3 train no.268320  loss = 4.20250 avg_loss = 3.30705\n",
      "epoch no.3 train no.268330  loss = 4.33889 avg_loss = 3.31703\n",
      "epoch no.3 train no.268340  loss = 2.33296 avg_loss = 3.32031\n",
      "epoch no.3 train no.268350  loss = 2.45970 avg_loss = 3.32647\n",
      "epoch no.3 train no.268360  loss = 3.83941 avg_loss = 3.34587\n",
      "epoch no.3 train no.268370  loss = 2.77799 avg_loss = 3.29384\n",
      "epoch no.3 train no.268380  loss = 2.72019 avg_loss = 3.24979\n",
      "epoch no.3 train no.268390  loss = 2.49892 avg_loss = 3.24379\n",
      "epoch no.3 train no.268400  loss = 3.45474 avg_loss = 3.26535\n",
      "epoch no.3 train no.268410  loss = 2.94492 avg_loss = 3.28150\n",
      "epoch no.3 train no.268420  loss = 3.04538 avg_loss = 3.26868\n",
      "epoch no.3 train no.268430  loss = 2.99888 avg_loss = 3.26227\n",
      "epoch no.3 train no.268440  loss = 3.55986 avg_loss = 3.26904\n",
      "epoch no.3 train no.268450  loss = 2.93703 avg_loss = 3.26889\n",
      "epoch no.3 train no.268460  loss = 4.54814 avg_loss = 3.29972\n",
      "epoch no.3 train no.268470  loss = 2.90324 avg_loss = 3.37012\n",
      "epoch no.3 train no.268480  loss = 3.89712 avg_loss = 3.37542\n",
      "epoch no.3 train no.268490  loss = 5.09020 avg_loss = 3.40004\n",
      "epoch no.3 train no.268500  loss = 3.28935 avg_loss = 3.39945\n",
      "epoch no.3 train no.268510  loss = 3.07984 avg_loss = 3.40246\n",
      "epoch no.3 train no.268520  loss = 4.49627 avg_loss = 3.41592\n",
      "epoch no.3 train no.268530  loss = 2.86611 avg_loss = 3.41788\n",
      "epoch no.3 train no.268540  loss = 2.21052 avg_loss = 3.43350\n",
      "epoch no.3 train no.268550  loss = 2.42236 avg_loss = 3.39613\n",
      "epoch no.3 train no.268560  loss = 3.07932 avg_loss = 3.35675\n",
      "epoch no.3 train no.268570  loss = 2.28266 avg_loss = 3.34968\n",
      "epoch no.3 train no.268580  loss = 2.99006 avg_loss = 3.32253\n",
      "epoch no.3 train no.268590  loss = 3.60446 avg_loss = 3.32828\n",
      "epoch no.3 train no.268600  loss = 2.43400 avg_loss = 3.32245\n",
      "epoch no.3 train no.268610  loss = 2.38068 avg_loss = 3.28342\n",
      "epoch no.3 train no.268620  loss = 3.98823 avg_loss = 3.28109\n",
      "epoch no.3 train no.268630  loss = 2.54004 avg_loss = 3.29257\n",
      "epoch no.3 train no.268640  loss = 4.08391 avg_loss = 3.27115\n",
      "epoch no.3 train no.268650  loss = 2.24788 avg_loss = 3.25021\n",
      "epoch no.3 train no.268660  loss = 2.19590 avg_loss = 3.26904\n",
      "epoch no.3 train no.268670  loss = 1.79989 avg_loss = 3.32082\n",
      "epoch no.3 train no.268680  loss = 3.11706 avg_loss = 3.31037\n",
      "epoch no.3 train no.268690  loss = 3.93402 avg_loss = 3.33098\n",
      "epoch no.3 train no.268700  loss = 2.16170 avg_loss = 3.30871\n",
      "epoch no.3 train no.268710  loss = 5.31951 avg_loss = 3.32699\n",
      "epoch no.3 train no.268720  loss = 2.71730 avg_loss = 3.31711\n",
      "epoch no.3 train no.268730  loss = 3.75655 avg_loss = 3.37506\n",
      "epoch no.3 train no.268740  loss = 3.30269 avg_loss = 3.34394\n",
      "epoch no.3 train no.268750  loss = 3.04833 avg_loss = 3.37424\n",
      "epoch no.3 train no.268760  loss = 5.07556 avg_loss = 3.35596\n",
      "epoch no.3 train no.268770  loss = 2.88038 avg_loss = 3.34797\n",
      "epoch no.3 train no.268780  loss = 2.91326 avg_loss = 3.32521\n",
      "epoch no.3 train no.268790  loss = 4.12418 avg_loss = 3.32401\n",
      "epoch no.3 train no.268800  loss = 3.35326 avg_loss = 3.39802\n",
      "epoch no.3 train no.268810  loss = 2.22403 avg_loss = 3.36785\n",
      "epoch no.3 train no.268820  loss = 3.98875 avg_loss = 3.37261\n",
      "epoch no.3 train no.268830  loss = 4.45884 avg_loss = 3.37373\n",
      "epoch no.3 train no.268840  loss = 3.91515 avg_loss = 3.40595\n",
      "epoch no.3 train no.268850  loss = 2.60413 avg_loss = 3.41022\n",
      "epoch no.3 train no.268860  loss = 4.63116 avg_loss = 3.38605\n",
      "epoch no.3 train no.268870  loss = 3.32705 avg_loss = 3.36142\n",
      "epoch no.3 train no.268880  loss = 2.55615 avg_loss = 3.34496\n",
      "epoch no.3 train no.268890  loss = 2.37753 avg_loss = 3.36240\n",
      "epoch no.3 train no.268900  loss = 3.72578 avg_loss = 3.41090\n",
      "epoch no.3 train no.268910  loss = 1.88202 avg_loss = 3.38763\n",
      "epoch no.3 train no.268920  loss = 2.19809 avg_loss = 3.38679\n",
      "epoch no.3 train no.268930  loss = 3.02984 avg_loss = 3.37932\n",
      "epoch no.3 train no.268940  loss = 3.35518 avg_loss = 3.35089\n",
      "epoch no.3 train no.268950  loss = 2.23041 avg_loss = 3.31363\n",
      "epoch no.3 train no.268960  loss = 2.61658 avg_loss = 3.30408\n",
      "epoch no.3 train no.268970  loss = 2.96924 avg_loss = 3.33006\n",
      "epoch no.3 train no.268980  loss = 2.80893 avg_loss = 3.32341\n",
      "epoch no.3 train no.268990  loss = 2.43708 avg_loss = 3.31343\n",
      "epoch no.3 train no.269000  loss = 1.73140 avg_loss = 3.28907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "to_tokens: ['▁비', '밤', '▁끝', '에서', '▁여름', '의', '▁끝', '에']\n",
      "여름의 끝에서 가을의 문턱</s>\n",
      "epoch no.3 train no.269010  loss = 2.81832 avg_loss = 3.29368\n",
      "epoch no.3 train no.269020  loss = 3.79188 avg_loss = 3.30419\n",
      "epoch no.3 train no.269030  loss = 4.00570 avg_loss = 3.33183\n",
      "epoch no.3 train no.269040  loss = 2.72504 avg_loss = 3.41473\n",
      "epoch no.3 train no.269050  loss = 2.86114 avg_loss = 3.42856\n",
      "epoch no.3 train no.269060  loss = 2.76695 avg_loss = 3.40927\n",
      "epoch no.3 train no.269070  loss = 5.16827 avg_loss = 3.42972\n",
      "epoch no.3 train no.269080  loss = 2.59031 avg_loss = 3.43556\n",
      "epoch no.3 train no.269090  loss = 3.39706 avg_loss = 3.44042\n",
      "epoch no.3 train no.269100  loss = 4.29299 avg_loss = 3.44917\n",
      "epoch no.3 train no.269110  loss = 2.19425 avg_loss = 3.42218\n",
      "epoch no.3 train no.269120  loss = 4.76236 avg_loss = 3.42954\n",
      "epoch no.3 train no.269130  loss = 4.01528 avg_loss = 3.44488\n",
      "epoch no.3 train no.269140  loss = 3.55847 avg_loss = 3.43873\n",
      "epoch no.3 train no.269150  loss = 3.91292 avg_loss = 3.41115\n",
      "epoch no.3 train no.269160  loss = 2.96900 avg_loss = 3.39342\n",
      "epoch no.3 train no.269170  loss = 3.60389 avg_loss = 3.44075\n",
      "epoch no.3 train no.269180  loss = 2.83715 avg_loss = 3.45841\n",
      "epoch no.3 train no.269190  loss = 3.09237 avg_loss = 3.46299\n",
      "epoch no.3 train no.269200  loss = 2.89699 avg_loss = 3.42551\n",
      "epoch no.3 train no.269210  loss = 1.96051 avg_loss = 3.42544\n",
      "epoch no.3 train no.269220  loss = 2.74572 avg_loss = 3.39573\n",
      "epoch no.3 train no.269230  loss = 2.23634 avg_loss = 3.41232\n",
      "epoch no.3 train no.269240  loss = 4.00996 avg_loss = 3.43957\n",
      "epoch no.3 train no.269250  loss = 2.21483 avg_loss = 3.41465\n",
      "epoch no.3 train no.269260  loss = 1.64013 avg_loss = 3.39665\n",
      "epoch no.3 train no.269270  loss = 4.15566 avg_loss = 3.42490\n",
      "epoch no.3 train no.269280  loss = 2.50571 avg_loss = 3.39483\n",
      "epoch no.3 train no.269290  loss = 4.01805 avg_loss = 3.43653\n",
      "epoch no.3 train no.269300  loss = 5.00137 avg_loss = 3.42002\n",
      "epoch no.3 train no.269310  loss = 2.92013 avg_loss = 3.39294\n",
      "epoch no.3 train no.269320  loss = 5.48093 avg_loss = 3.35805\n",
      "epoch no.3 train no.269330  loss = 3.03653 avg_loss = 3.36301\n",
      "epoch no.3 train no.269340  loss = 5.44876 avg_loss = 3.33637\n",
      "epoch no.3 train no.269350  loss = 4.59449 avg_loss = 3.32219\n",
      "epoch no.3 train no.269360  loss = 3.33390 avg_loss = 3.29869\n",
      "epoch no.3 train no.269370  loss = 3.62849 avg_loss = 3.27349\n",
      "epoch no.3 train no.269380  loss = 2.64383 avg_loss = 3.25872\n",
      "epoch no.3 train no.269390  loss = 2.26825 avg_loss = 3.23773\n",
      "epoch no.3 train no.269400  loss = 2.08122 avg_loss = 3.24360\n",
      "epoch no.3 train no.269410  loss = 5.43455 avg_loss = 3.34252\n",
      "epoch no.3 train no.269420  loss = 3.62668 avg_loss = 3.39573\n",
      "epoch no.3 train no.269430  loss = 2.18347 avg_loss = 3.41974\n",
      "epoch no.3 train no.269440  loss = 3.32561 avg_loss = 3.41701\n",
      "epoch no.3 train no.269450  loss = 2.82054 avg_loss = 3.41389\n",
      "epoch no.3 train no.269460  loss = 2.32590 avg_loss = 3.42998\n",
      "epoch no.3 train no.269470  loss = 2.54073 avg_loss = 3.38285\n",
      "epoch no.3 train no.269480  loss = 2.58295 avg_loss = 3.35271\n",
      "epoch no.3 train no.269490  loss = 3.53520 avg_loss = 3.40787\n",
      "epoch no.3 train no.269500  loss = 4.31493 avg_loss = 3.42768\n",
      "epoch no.3 train no.269510  loss = 4.69852 avg_loss = 3.40613\n",
      "epoch no.3 train no.269520  loss = 3.25177 avg_loss = 3.37432\n",
      "epoch no.3 train no.269530  loss = 3.04581 avg_loss = 3.36916\n",
      "epoch no.3 train no.269540  loss = 3.42744 avg_loss = 3.38713\n",
      "epoch no.3 train no.269550  loss = 1.95469 avg_loss = 3.36678\n",
      "epoch no.3 train no.269560  loss = 5.33070 avg_loss = 3.37470\n",
      "epoch no.3 train no.269570  loss = 2.29747 avg_loss = 3.38979\n",
      "epoch no.3 train no.269580  loss = 2.91791 avg_loss = 3.37501\n",
      "epoch no.3 train no.269590  loss = 4.04889 avg_loss = 3.38455\n",
      "epoch no.3 train no.269600  loss = 1.72263 avg_loss = 3.34406\n",
      "epoch no.3 train no.269610  loss = 2.89096 avg_loss = 3.33828\n",
      "epoch no.3 train no.269620  loss = 3.78997 avg_loss = 3.30781\n",
      "epoch no.3 train no.269630  loss = 3.51639 avg_loss = 3.30051\n",
      "epoch no.3 train no.269640  loss = 2.68678 avg_loss = 3.29623\n",
      "epoch no.3 train no.269650  loss = 2.48497 avg_loss = 3.29166\n",
      "epoch no.3 train no.269660  loss = 2.25668 avg_loss = 3.29060\n",
      "epoch no.3 train no.269670  loss = 2.90907 avg_loss = 3.32358\n",
      "epoch no.3 train no.269680  loss = 3.14213 avg_loss = 3.33990\n",
      "epoch no.3 train no.269690  loss = 3.90777 avg_loss = 3.36261\n",
      "epoch no.3 train no.269700  loss = 2.89145 avg_loss = 3.40665\n",
      "epoch no.3 train no.269710  loss = 5.06721 avg_loss = 3.42020\n",
      "epoch no.3 train no.269720  loss = 5.39359 avg_loss = 3.45367\n",
      "epoch no.3 train no.269730  loss = 4.42691 avg_loss = 3.46928\n",
      "epoch no.3 train no.269740  loss = 2.09060 avg_loss = 3.47011\n",
      "epoch no.3 train no.269750  loss = 3.98058 avg_loss = 3.50080\n",
      "epoch no.3 train no.269760  loss = 3.02925 avg_loss = 3.53338\n",
      "epoch no.3 train no.269770  loss = 2.67696 avg_loss = 3.49007\n",
      "epoch no.3 train no.269780  loss = 2.88125 avg_loss = 3.45985\n",
      "epoch no.3 train no.269790  loss = 6.30786 avg_loss = 3.46801\n",
      "epoch no.3 train no.269800  loss = 2.62302 avg_loss = 3.49185\n",
      "epoch no.3 train no.269810  loss = 3.47999 avg_loss = 3.48241\n",
      "epoch no.3 train no.269820  loss = 1.71030 avg_loss = 3.42329\n",
      "epoch no.3 train no.269830  loss = 2.94087 avg_loss = 3.43081\n",
      "epoch no.3 train no.269840  loss = 3.98604 avg_loss = 3.45845\n",
      "epoch no.3 train no.269850  loss = 2.10122 avg_loss = 3.46284\n",
      "epoch no.3 train no.269860  loss = 3.44861 avg_loss = 3.44813\n",
      "epoch no.3 train no.269870  loss = 3.52264 avg_loss = 3.44680\n",
      "epoch no.3 train no.269880  loss = 4.47433 avg_loss = 3.45578\n",
      "epoch no.3 train no.269890  loss = 3.52915 avg_loss = 3.49167\n",
      "epoch no.3 train no.269900  loss = 4.56986 avg_loss = 3.46053\n",
      "epoch no.3 train no.269910  loss = 4.06358 avg_loss = 3.43776\n",
      "epoch no.3 train no.269920  loss = 4.02325 avg_loss = 3.44864\n",
      "epoch no.3 train no.269930  loss = 3.40974 avg_loss = 3.42965\n",
      "epoch no.3 train no.269940  loss = 1.40437 avg_loss = 3.37486\n",
      "epoch no.3 train no.269950  loss = 3.03855 avg_loss = 3.36297\n",
      "epoch no.3 train no.269960  loss = 2.62913 avg_loss = 3.36246\n",
      "epoch no.3 train no.269970  loss = 1.25258 avg_loss = 3.30861\n",
      "epoch no.3 train no.269980  loss = 3.92213 avg_loss = 3.30146\n",
      "epoch no.3 train no.269990  loss = 5.81841 avg_loss = 3.37772\n",
      "epoch no.3 train no.270000  loss = 3.49519 avg_loss = 3.34810\n",
      "4\n",
      "to_tokens: ['▁비', '엔', '에', '▁꿀', '▁재즈', '</s>']\n",
      "여름밤의 로맨틱 재즈</s>\n",
      "epoch no.3 train no.270010  loss = 2.94728 avg_loss = 3.30153\n",
      "epoch no.3 train no.270020  loss = 3.19841 avg_loss = 3.27553\n",
      "epoch no.3 train no.270030  loss = 2.69672 avg_loss = 3.28130\n",
      "epoch no.3 train no.270040  loss = 4.14384 avg_loss = 3.28373\n",
      "epoch no.3 train no.270050  loss = 4.62982 avg_loss = 3.34490\n",
      "epoch no.3 train no.270060  loss = 2.85599 avg_loss = 3.33801\n",
      "epoch no.3 train no.270070  loss = 3.45411 avg_loss = 3.36537\n",
      "epoch no.3 train no.270080  loss = 4.69852 avg_loss = 3.37666\n",
      "epoch no.3 train no.270090  loss = 2.91183 avg_loss = 3.35912\n",
      "epoch no.3 train no.270100  loss = 3.82333 avg_loss = 3.33949\n",
      "epoch no.3 train no.270110  loss = 2.46617 avg_loss = 3.31028\n",
      "epoch no.3 train no.270120  loss = 4.47367 avg_loss = 3.30049\n",
      "epoch no.3 train no.270130  loss = 1.77215 avg_loss = 3.28732\n",
      "epoch no.3 train no.270140  loss = 5.10877 avg_loss = 3.31767\n",
      "epoch no.3 train no.270150  loss = 2.94991 avg_loss = 3.30264\n",
      "epoch no.3 train no.270160  loss = 1.68370 avg_loss = 3.36409\n",
      "epoch no.3 train no.270170  loss = 4.68950 avg_loss = 3.37604\n",
      "epoch no.3 train no.270180  loss = 2.48260 avg_loss = 3.32986\n",
      "epoch no.3 train no.270190  loss = 4.08309 avg_loss = 3.31554\n",
      "epoch no.3 train no.270200  loss = 5.68308 avg_loss = 3.35929\n",
      "epoch no.3 train no.270210  loss = 3.15472 avg_loss = 3.38710\n",
      "epoch no.3 train no.270220  loss = 3.46602 avg_loss = 3.40911\n",
      "epoch no.3 train no.270230  loss = 2.90898 avg_loss = 3.37836\n",
      "epoch no.3 train no.270240  loss = 3.71223 avg_loss = 3.44637\n",
      "epoch no.3 train no.270250  loss = 5.54655 avg_loss = 3.48381\n",
      "epoch no.3 train no.270260  loss = 2.81716 avg_loss = 3.45288\n",
      "epoch no.3 train no.270270  loss = 1.46446 avg_loss = 3.44107\n",
      "epoch no.3 train no.270280  loss = 2.87454 avg_loss = 3.43563\n",
      "epoch no.3 train no.270290  loss = 2.44210 avg_loss = 3.43391\n",
      "epoch no.3 train no.270300  loss = 3.63471 avg_loss = 3.44921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.270310  loss = 2.26427 avg_loss = 3.39862\n",
      "epoch no.3 train no.270320  loss = 3.88919 avg_loss = 3.35517\n",
      "epoch no.3 train no.270330  loss = 2.81519 avg_loss = 3.32039\n",
      "epoch no.3 train no.270340  loss = 2.98800 avg_loss = 3.39188\n",
      "epoch no.3 train no.270350  loss = 1.82311 avg_loss = 3.38097\n",
      "epoch no.3 train no.270360  loss = 2.98658 avg_loss = 3.39227\n",
      "epoch no.3 train no.270370  loss = 3.92227 avg_loss = 3.35508\n",
      "epoch no.3 train no.270380  loss = 3.74716 avg_loss = 3.34705\n",
      "epoch no.3 train no.270390  loss = 6.89769 avg_loss = 3.36287\n",
      "epoch no.3 train no.270400  loss = 2.55424 avg_loss = 3.29356\n",
      "epoch no.3 train no.270410  loss = 2.34292 avg_loss = 3.31817\n",
      "epoch no.3 train no.270420  loss = 5.39160 avg_loss = 3.36242\n",
      "epoch no.3 train no.270430  loss = 2.17095 avg_loss = 3.35812\n",
      "epoch no.3 train no.270440  loss = 4.19008 avg_loss = 3.35411\n",
      "epoch no.3 train no.270450  loss = 4.21167 avg_loss = 3.37186\n",
      "epoch no.3 train no.270460  loss = 3.87911 avg_loss = 3.39664\n",
      "epoch no.3 train no.270470  loss = 3.04003 avg_loss = 3.41638\n",
      "epoch no.3 train no.270480  loss = 2.78539 avg_loss = 3.36091\n",
      "epoch no.3 train no.270490  loss = 4.79519 avg_loss = 3.36962\n",
      "epoch no.3 train no.270500  loss = 2.72106 avg_loss = 3.37236\n",
      "epoch no.3 train no.270510  loss = 2.40296 avg_loss = 3.32649\n",
      "epoch no.3 train no.270520  loss = 2.50199 avg_loss = 3.30022\n",
      "epoch no.3 train no.270530  loss = 3.99915 avg_loss = 3.27026\n",
      "epoch no.3 train no.270540  loss = 2.55059 avg_loss = 3.22491\n",
      "epoch no.3 train no.270550  loss = 3.47961 avg_loss = 3.19953\n",
      "epoch no.3 train no.270560  loss = 4.39768 avg_loss = 3.18532\n",
      "epoch no.3 train no.270570  loss = 3.62194 avg_loss = 3.15547\n",
      "epoch no.3 train no.270580  loss = 4.12343 avg_loss = 3.18477\n",
      "epoch no.3 train no.270590  loss = 4.09905 avg_loss = 3.18756\n",
      "epoch no.3 train no.270600  loss = 2.06265 avg_loss = 3.20813\n",
      "epoch no.3 train no.270610  loss = 2.02345 avg_loss = 3.19763\n",
      "epoch no.3 train no.270620  loss = 2.77928 avg_loss = 3.21210\n",
      "epoch no.3 train no.270630  loss = 3.48978 avg_loss = 3.25586\n",
      "epoch no.3 train no.270640  loss = 3.84298 avg_loss = 3.31331\n",
      "epoch no.3 train no.270650  loss = 3.93875 avg_loss = 3.31568\n",
      "epoch no.3 train no.270660  loss = 3.19975 avg_loss = 3.32258\n",
      "epoch no.3 train no.270670  loss = 3.30223 avg_loss = 3.33222\n",
      "epoch no.3 train no.270680  loss = 3.88751 avg_loss = 3.29566\n",
      "epoch no.3 train no.270690  loss = 4.14419 avg_loss = 3.34833\n",
      "epoch no.3 train no.270700  loss = 4.13517 avg_loss = 3.36661\n",
      "epoch no.3 train no.270710  loss = 3.81389 avg_loss = 3.35145\n",
      "epoch no.3 train no.270720  loss = 1.69759 avg_loss = 3.34161\n",
      "epoch no.3 train no.270730  loss = 2.54924 avg_loss = 3.29439\n",
      "epoch no.3 train no.270740  loss = 2.94995 avg_loss = 3.28128\n",
      "epoch no.3 train no.270750  loss = 1.86369 avg_loss = 3.28564\n",
      "epoch no.3 train no.270760  loss = 2.93525 avg_loss = 3.30207\n",
      "epoch no.3 train no.270770  loss = 4.67186 avg_loss = 3.33315\n",
      "epoch no.3 train no.270780  loss = 3.12440 avg_loss = 3.31819\n",
      "epoch no.3 train no.270790  loss = 4.00516 avg_loss = 3.30710\n",
      "epoch no.3 train no.270800  loss = 3.44079 avg_loss = 3.27568\n",
      "epoch no.3 train no.270810  loss = 2.87230 avg_loss = 3.25732\n",
      "epoch no.3 train no.270820  loss = 4.07256 avg_loss = 3.24713\n",
      "epoch no.3 train no.270830  loss = 4.05997 avg_loss = 3.24766\n",
      "epoch no.3 train no.270840  loss = 4.45432 avg_loss = 3.25558\n",
      "epoch no.3 train no.270850  loss = 3.65772 avg_loss = 3.27817\n",
      "epoch no.3 train no.270860  loss = 2.23365 avg_loss = 3.31420\n",
      "epoch no.3 train no.270870  loss = 4.09446 avg_loss = 3.29470\n",
      "epoch no.3 train no.270880  loss = 2.95567 avg_loss = 3.25279\n",
      "epoch no.3 train no.270890  loss = 3.59015 avg_loss = 3.28411\n",
      "epoch no.3 train no.270900  loss = 4.08940 avg_loss = 3.28275\n",
      "epoch no.3 train no.270910  loss = 3.90429 avg_loss = 3.33340\n",
      "epoch no.3 train no.270920  loss = 2.48424 avg_loss = 3.35758\n",
      "epoch no.3 train no.270930  loss = 3.30726 avg_loss = 3.35811\n",
      "epoch no.3 train no.270940  loss = 2.72868 avg_loss = 3.32832\n",
      "epoch no.3 train no.270950  loss = 4.99063 avg_loss = 3.35968\n",
      "epoch no.3 train no.270960  loss = 3.45516 avg_loss = 3.38957\n",
      "epoch no.3 train no.270970  loss = 2.82151 avg_loss = 3.39556\n",
      "epoch no.3 train no.270980  loss = 3.49088 avg_loss = 3.40758\n",
      "epoch no.3 train no.270990  loss = 5.03338 avg_loss = 3.40262\n",
      "epoch no.3 train no.271000  loss = 1.88311 avg_loss = 3.39212\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '을', '▁어울리는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.271010  loss = 4.57617 avg_loss = 3.42266\n",
      "epoch no.3 train no.271020  loss = 5.13610 avg_loss = 3.40872\n",
      "epoch no.3 train no.271030  loss = 2.99512 avg_loss = 3.42288\n",
      "epoch no.3 train no.271040  loss = 2.89000 avg_loss = 3.42807\n",
      "epoch no.3 train no.271050  loss = 3.44050 avg_loss = 3.41868\n",
      "epoch no.3 train no.271060  loss = 3.08267 avg_loss = 3.40594\n",
      "epoch no.3 train no.271070  loss = 3.32027 avg_loss = 3.38495\n",
      "epoch no.3 train no.271080  loss = 3.47581 avg_loss = 3.38519\n",
      "epoch no.3 train no.271090  loss = 2.45617 avg_loss = 3.38951\n",
      "epoch no.3 train no.271100  loss = 2.27903 avg_loss = 3.36877\n",
      "epoch no.3 train no.271110  loss = 2.72304 avg_loss = 3.31974\n",
      "epoch no.3 train no.271120  loss = 4.35664 avg_loss = 3.36006\n",
      "epoch no.3 train no.271130  loss = 2.84880 avg_loss = 3.39940\n",
      "epoch no.3 train no.271140  loss = 3.34546 avg_loss = 3.38630\n",
      "epoch no.3 train no.271150  loss = 3.83624 avg_loss = 3.38796\n",
      "epoch no.3 train no.271160  loss = 3.55061 avg_loss = 3.39711\n",
      "epoch no.3 train no.271170  loss = 4.03563 avg_loss = 3.41383\n",
      "epoch no.3 train no.271180  loss = 3.38816 avg_loss = 3.40509\n",
      "epoch no.3 train no.271190  loss = 2.28483 avg_loss = 3.40480\n",
      "epoch no.3 train no.271200  loss = 4.67479 avg_loss = 3.41081\n",
      "epoch no.3 train no.271210  loss = 3.00924 avg_loss = 3.37363\n",
      "epoch no.3 train no.271220  loss = 3.53422 avg_loss = 3.38411\n",
      "epoch no.3 train no.271230  loss = 4.85612 avg_loss = 3.37983\n",
      "epoch no.3 train no.271240  loss = 5.06510 avg_loss = 3.40132\n",
      "epoch no.3 train no.271250  loss = 5.18488 avg_loss = 3.40415\n",
      "epoch no.3 train no.271260  loss = 4.11893 avg_loss = 3.38416\n",
      "epoch no.3 train no.271270  loss = 4.18762 avg_loss = 3.38869\n",
      "epoch no.3 train no.271280  loss = 3.10653 avg_loss = 3.37784\n",
      "epoch no.3 train no.271290  loss = 1.66597 avg_loss = 3.32718\n",
      "epoch no.3 train no.271300  loss = 5.16365 avg_loss = 3.35406\n",
      "epoch no.3 train no.271310  loss = 4.31918 avg_loss = 3.34102\n",
      "epoch no.3 train no.271320  loss = 3.96093 avg_loss = 3.38653\n",
      "epoch no.3 train no.271330  loss = 2.55465 avg_loss = 3.38272\n",
      "epoch no.3 train no.271340  loss = 3.81127 avg_loss = 3.40092\n",
      "epoch no.3 train no.271350  loss = 3.99997 avg_loss = 3.39178\n",
      "epoch no.3 train no.271360  loss = 6.22883 avg_loss = 3.42715\n",
      "epoch no.3 train no.271370  loss = 5.49168 avg_loss = 3.42469\n",
      "epoch no.3 train no.271380  loss = 3.49925 avg_loss = 3.45134\n",
      "epoch no.3 train no.271390  loss = 3.12970 avg_loss = 3.44490\n",
      "epoch no.3 train no.271400  loss = 3.15331 avg_loss = 3.46974\n",
      "epoch no.3 train no.271410  loss = 2.63786 avg_loss = 3.47611\n",
      "epoch no.3 train no.271420  loss = 4.30278 avg_loss = 3.51515\n",
      "epoch no.3 train no.271430  loss = 3.26084 avg_loss = 3.46526\n",
      "epoch no.3 train no.271440  loss = 3.07029 avg_loss = 3.41991\n",
      "epoch no.3 train no.271450  loss = 1.53452 avg_loss = 3.43969\n",
      "epoch no.3 train no.271460  loss = 3.55553 avg_loss = 3.41198\n",
      "epoch no.3 train no.271470  loss = 2.80100 avg_loss = 3.38456\n",
      "epoch no.3 train no.271480  loss = 5.05280 avg_loss = 3.39340\n",
      "epoch no.3 train no.271490  loss = 1.91339 avg_loss = 3.38523\n",
      "epoch no.3 train no.271500  loss = 3.85469 avg_loss = 3.42367\n",
      "epoch no.3 train no.271510  loss = 3.00646 avg_loss = 3.39843\n",
      "epoch no.3 train no.271520  loss = 4.72187 avg_loss = 3.37029\n",
      "epoch no.3 train no.271530  loss = 3.43925 avg_loss = 3.32491\n",
      "epoch no.3 train no.271540  loss = 3.13674 avg_loss = 3.31045\n",
      "epoch no.3 train no.271550  loss = 3.41129 avg_loss = 3.34172\n",
      "epoch no.3 train no.271560  loss = 3.07517 avg_loss = 3.35168\n",
      "epoch no.3 train no.271570  loss = 5.42097 avg_loss = 3.38651\n",
      "epoch no.3 train no.271580  loss = 3.28934 avg_loss = 3.45366\n",
      "epoch no.3 train no.271590  loss = 2.98581 avg_loss = 3.36754\n",
      "epoch no.3 train no.271600  loss = 4.16228 avg_loss = 3.38309\n",
      "epoch no.3 train no.271610  loss = 2.18882 avg_loss = 3.37455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.271620  loss = 3.57541 avg_loss = 3.39050\n",
      "epoch no.3 train no.271630  loss = 4.07173 avg_loss = 3.38289\n",
      "epoch no.3 train no.271640  loss = 2.37738 avg_loss = 3.37499\n",
      "epoch no.3 train no.271650  loss = 5.03071 avg_loss = 3.37695\n",
      "epoch no.3 train no.271660  loss = 3.52774 avg_loss = 3.36427\n",
      "epoch no.3 train no.271670  loss = 3.21801 avg_loss = 3.33743\n",
      "epoch no.3 train no.271680  loss = 3.72833 avg_loss = 3.32867\n",
      "epoch no.3 train no.271690  loss = 2.90478 avg_loss = 3.31762\n",
      "epoch no.3 train no.271700  loss = 3.49079 avg_loss = 3.32204\n",
      "epoch no.3 train no.271710  loss = 2.96386 avg_loss = 3.29363\n",
      "epoch no.3 train no.271720  loss = 3.38352 avg_loss = 3.34362\n",
      "epoch no.3 train no.271730  loss = 3.01067 avg_loss = 3.28822\n",
      "epoch no.3 train no.271740  loss = 2.12329 avg_loss = 3.31323\n",
      "epoch no.3 train no.271750  loss = 2.02018 avg_loss = 3.31128\n",
      "epoch no.3 train no.271760  loss = 2.98339 avg_loss = 3.29511\n",
      "epoch no.3 train no.271770  loss = 3.82668 avg_loss = 3.28778\n",
      "epoch no.3 train no.271780  loss = 2.93415 avg_loss = 3.32127\n",
      "epoch no.3 train no.271790  loss = 2.96576 avg_loss = 3.28561\n",
      "epoch no.3 train no.271800  loss = 2.84274 avg_loss = 3.32410\n",
      "epoch no.3 train no.271810  loss = 4.13071 avg_loss = 3.31830\n",
      "epoch no.3 train no.271820  loss = 4.56747 avg_loss = 3.37253\n",
      "epoch no.3 train no.271830  loss = 3.05392 avg_loss = 3.37334\n",
      "epoch no.3 train no.271840  loss = 2.14452 avg_loss = 3.34709\n",
      "epoch no.3 train no.271850  loss = 3.87085 avg_loss = 3.38330\n",
      "epoch no.3 train no.271860  loss = 3.72391 avg_loss = 3.35095\n",
      "epoch no.3 train no.271870  loss = 1.45399 avg_loss = 3.31074\n",
      "epoch no.3 train no.271880  loss = 3.68935 avg_loss = 3.25780\n",
      "epoch no.3 train no.271890  loss = 4.32927 avg_loss = 3.27654\n",
      "epoch no.3 train no.271900  loss = 3.15976 avg_loss = 3.26767\n",
      "epoch no.3 train no.271910  loss = 4.22330 avg_loss = 3.27301\n",
      "epoch no.3 train no.271920  loss = 3.31792 avg_loss = 3.31027\n",
      "epoch no.3 train no.271930  loss = 4.17626 avg_loss = 3.35549\n",
      "epoch no.3 train no.271940  loss = 2.83941 avg_loss = 3.38825\n",
      "epoch no.3 train no.271950  loss = 2.99547 avg_loss = 3.40089\n",
      "epoch no.3 train no.271960  loss = 2.99238 avg_loss = 3.36903\n",
      "epoch no.3 train no.271970  loss = 4.76245 avg_loss = 3.40934\n",
      "epoch no.3 train no.271980  loss = 2.99032 avg_loss = 3.36312\n",
      "epoch no.3 train no.271990  loss = 4.33732 avg_loss = 3.35535\n",
      "epoch no.3 train no.272000  loss = 2.73197 avg_loss = 3.34728\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.272010  loss = 2.79905 avg_loss = 3.31024\n",
      "epoch no.3 train no.272020  loss = 3.42059 avg_loss = 3.30053\n",
      "epoch no.3 train no.272030  loss = 2.88404 avg_loss = 3.35556\n",
      "epoch no.3 train no.272040  loss = 4.25251 avg_loss = 3.37256\n",
      "epoch no.3 train no.272050  loss = 2.80289 avg_loss = 3.35148\n",
      "epoch no.3 train no.272060  loss = 3.08464 avg_loss = 3.34373\n",
      "epoch no.3 train no.272070  loss = 6.51864 avg_loss = 3.38526\n",
      "epoch no.3 train no.272080  loss = 2.91554 avg_loss = 3.38663\n",
      "epoch no.3 train no.272090  loss = 3.74864 avg_loss = 3.34128\n",
      "epoch no.3 train no.272100  loss = 1.95657 avg_loss = 3.31605\n",
      "epoch no.3 train no.272110  loss = 2.85663 avg_loss = 3.31386\n",
      "epoch no.3 train no.272120  loss = 2.57261 avg_loss = 3.31282\n",
      "epoch no.3 train no.272130  loss = 2.84501 avg_loss = 3.29703\n",
      "epoch no.3 train no.272140  loss = 4.03672 avg_loss = 3.37277\n",
      "epoch no.3 train no.272150  loss = 3.80372 avg_loss = 3.34777\n",
      "epoch no.3 train no.272160  loss = 3.26257 avg_loss = 3.35405\n",
      "epoch no.3 train no.272170  loss = 3.70608 avg_loss = 3.35039\n",
      "epoch no.3 train no.272180  loss = 2.09980 avg_loss = 3.29651\n",
      "epoch no.3 train no.272190  loss = 1.79292 avg_loss = 3.26430\n",
      "epoch no.3 train no.272200  loss = 4.07874 avg_loss = 3.29976\n",
      "epoch no.3 train no.272210  loss = 3.56379 avg_loss = 3.34569\n",
      "epoch no.3 train no.272220  loss = 3.78125 avg_loss = 3.32534\n",
      "epoch no.3 train no.272230  loss = 3.39076 avg_loss = 3.31480\n",
      "epoch no.3 train no.272240  loss = 3.21973 avg_loss = 3.31863\n",
      "epoch no.3 train no.272250  loss = 2.50920 avg_loss = 3.26011\n",
      "epoch no.3 train no.272260  loss = 3.35922 avg_loss = 3.21293\n",
      "epoch no.3 train no.272270  loss = 3.72091 avg_loss = 3.22687\n",
      "epoch no.3 train no.272280  loss = 3.40801 avg_loss = 3.22267\n",
      "epoch no.3 train no.272290  loss = 3.22205 avg_loss = 3.26541\n",
      "epoch no.3 train no.272300  loss = 1.53432 avg_loss = 3.23792\n",
      "epoch no.3 train no.272310  loss = 2.80719 avg_loss = 3.26614\n",
      "epoch no.3 train no.272320  loss = 2.68861 avg_loss = 3.23182\n",
      "epoch no.3 train no.272330  loss = 2.67862 avg_loss = 3.29954\n",
      "epoch no.3 train no.272340  loss = 3.56958 avg_loss = 3.30611\n",
      "epoch no.3 train no.272350  loss = 4.09263 avg_loss = 3.30269\n",
      "epoch no.3 train no.272360  loss = 2.90120 avg_loss = 3.28235\n",
      "epoch no.3 train no.272370  loss = 3.16461 avg_loss = 3.24755\n",
      "epoch no.3 train no.272380  loss = 1.67508 avg_loss = 3.26894\n",
      "epoch no.3 train no.272390  loss = 1.96480 avg_loss = 3.26419\n",
      "epoch no.3 train no.272400  loss = 3.85429 avg_loss = 3.23279\n",
      "epoch no.3 train no.272410  loss = 4.15481 avg_loss = 3.24476\n",
      "epoch no.3 train no.272420  loss = 2.69968 avg_loss = 3.26353\n",
      "epoch no.3 train no.272430  loss = 4.90643 avg_loss = 3.30298\n",
      "epoch no.3 train no.272440  loss = 3.46792 avg_loss = 3.34305\n",
      "epoch no.3 train no.272450  loss = 2.97393 avg_loss = 3.33345\n",
      "epoch no.3 train no.272460  loss = 3.86263 avg_loss = 3.35584\n",
      "epoch no.3 train no.272470  loss = 4.69676 avg_loss = 3.34869\n",
      "epoch no.3 train no.272480  loss = 5.42507 avg_loss = 3.35924\n",
      "epoch no.3 train no.272490  loss = 2.11947 avg_loss = 3.32532\n",
      "epoch no.3 train no.272500  loss = 3.72963 avg_loss = 3.31235\n",
      "epoch no.3 train no.272510  loss = 3.01498 avg_loss = 3.30571\n",
      "epoch no.3 train no.272520  loss = 4.25895 avg_loss = 3.37487\n",
      "epoch no.3 train no.272530  loss = 2.42491 avg_loss = 3.35518\n",
      "epoch no.3 train no.272540  loss = 3.36193 avg_loss = 3.36165\n",
      "epoch no.3 train no.272550  loss = 2.32071 avg_loss = 3.37136\n",
      "epoch no.3 train no.272560  loss = 2.54429 avg_loss = 3.35027\n",
      "epoch no.3 train no.272570  loss = 3.07718 avg_loss = 3.34943\n",
      "epoch no.3 train no.272580  loss = 4.54934 avg_loss = 3.35383\n",
      "epoch no.3 train no.272590  loss = 2.63181 avg_loss = 3.32855\n",
      "epoch no.3 train no.272600  loss = 2.70428 avg_loss = 3.32603\n",
      "epoch no.3 train no.272610  loss = 3.42965 avg_loss = 3.34707\n",
      "epoch no.3 train no.272620  loss = 2.88010 avg_loss = 3.35858\n",
      "epoch no.3 train no.272630  loss = 4.77127 avg_loss = 3.37469\n",
      "epoch no.3 train no.272640  loss = 4.12742 avg_loss = 3.35344\n",
      "epoch no.3 train no.272650  loss = 4.34172 avg_loss = 3.38195\n",
      "epoch no.3 train no.272660  loss = 3.73056 avg_loss = 3.38073\n",
      "epoch no.3 train no.272670  loss = 2.56832 avg_loss = 3.34703\n",
      "epoch no.3 train no.272680  loss = 5.18429 avg_loss = 3.31665\n",
      "epoch no.3 train no.272690  loss = 2.40765 avg_loss = 3.32157\n",
      "epoch no.3 train no.272700  loss = 3.66559 avg_loss = 3.34628\n",
      "epoch no.3 train no.272710  loss = 3.63604 avg_loss = 3.37671\n",
      "epoch no.3 train no.272720  loss = 3.16731 avg_loss = 3.35042\n",
      "epoch no.3 train no.272730  loss = 2.48226 avg_loss = 3.32788\n",
      "epoch no.3 train no.272740  loss = 4.19050 avg_loss = 3.30185\n",
      "epoch no.3 train no.272750  loss = 2.40975 avg_loss = 3.26507\n",
      "epoch no.3 train no.272760  loss = 2.27800 avg_loss = 3.29366\n",
      "epoch no.3 train no.272770  loss = 2.27040 avg_loss = 3.27283\n",
      "epoch no.3 train no.272780  loss = 3.46158 avg_loss = 3.21823\n",
      "epoch no.3 train no.272790  loss = 1.38688 avg_loss = 3.17107\n",
      "epoch no.3 train no.272800  loss = 2.83756 avg_loss = 3.18370\n",
      "epoch no.3 train no.272810  loss = 3.82631 avg_loss = 3.24174\n",
      "epoch no.3 train no.272820  loss = 2.60764 avg_loss = 3.23890\n",
      "epoch no.3 train no.272830  loss = 3.61073 avg_loss = 3.24209\n",
      "epoch no.3 train no.272840  loss = 2.04135 avg_loss = 3.23220\n",
      "epoch no.3 train no.272850  loss = 2.92305 avg_loss = 3.23548\n",
      "epoch no.3 train no.272860  loss = 4.49119 avg_loss = 3.23656\n",
      "epoch no.3 train no.272870  loss = 3.42200 avg_loss = 3.24512\n",
      "epoch no.3 train no.272880  loss = 1.63914 avg_loss = 3.22686\n",
      "epoch no.3 train no.272890  loss = 2.73298 avg_loss = 3.22276\n",
      "epoch no.3 train no.272900  loss = 2.57874 avg_loss = 3.25948\n",
      "epoch no.3 train no.272910  loss = 2.28251 avg_loss = 3.23995\n",
      "epoch no.3 train no.272920  loss = 3.54072 avg_loss = 3.24646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.272930  loss = 3.12261 avg_loss = 3.27885\n",
      "epoch no.3 train no.272940  loss = 1.79802 avg_loss = 3.24254\n",
      "epoch no.3 train no.272950  loss = 4.19153 avg_loss = 3.24758\n",
      "epoch no.3 train no.272960  loss = 3.26885 avg_loss = 3.20459\n",
      "epoch no.3 train no.272970  loss = 2.38303 avg_loss = 3.17718\n",
      "epoch no.3 train no.272980  loss = 2.52831 avg_loss = 3.20401\n",
      "epoch no.3 train no.272990  loss = 2.62724 avg_loss = 3.18927\n",
      "epoch no.3 train no.273000  loss = 1.93220 avg_loss = 3.16384\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁신나는', '▁노래', '곡', '</s>']\n",
      "여름엔 역시 신나는 댄스곡</s>\n",
      "epoch no.3 train no.273010  loss = 2.57110 avg_loss = 3.11896\n",
      "epoch no.3 train no.273020  loss = 4.47318 avg_loss = 3.15024\n",
      "epoch no.3 train no.273030  loss = 4.17396 avg_loss = 3.14738\n",
      "epoch no.3 train no.273040  loss = 3.39246 avg_loss = 3.14075\n",
      "epoch no.3 train no.273050  loss = 2.02867 avg_loss = 3.16916\n",
      "epoch no.3 train no.273060  loss = 2.36483 avg_loss = 3.15357\n",
      "epoch no.3 train no.273070  loss = 2.59510 avg_loss = 3.14759\n",
      "epoch no.3 train no.273080  loss = 5.06406 avg_loss = 3.16991\n",
      "epoch no.3 train no.273090  loss = 2.80225 avg_loss = 3.16628\n",
      "epoch no.3 train no.273100  loss = 3.75289 avg_loss = 3.18063\n",
      "epoch no.3 train no.273110  loss = 2.41371 avg_loss = 3.17405\n",
      "epoch no.3 train no.273120  loss = 3.20359 avg_loss = 3.18810\n",
      "epoch no.3 train no.273130  loss = 1.66205 avg_loss = 3.18978\n",
      "epoch no.3 train no.273140  loss = 2.64241 avg_loss = 3.21887\n",
      "epoch no.3 train no.273150  loss = 1.97348 avg_loss = 3.23167\n",
      "epoch no.3 train no.273160  loss = 2.52298 avg_loss = 3.23861\n",
      "epoch no.3 train no.273170  loss = 5.75760 avg_loss = 3.28502\n",
      "epoch no.3 train no.273180  loss = 3.32228 avg_loss = 3.27248\n",
      "epoch no.3 train no.273190  loss = 2.71527 avg_loss = 3.27875\n",
      "epoch no.3 train no.273200  loss = 2.63887 avg_loss = 3.29068\n",
      "epoch no.3 train no.273210  loss = 4.43624 avg_loss = 3.31452\n",
      "epoch no.3 train no.273220  loss = 4.20577 avg_loss = 3.29474\n",
      "epoch no.3 train no.273230  loss = 6.14719 avg_loss = 3.28794\n",
      "epoch no.3 train no.273240  loss = 2.49644 avg_loss = 3.31789\n",
      "epoch no.3 train no.273250  loss = 2.27633 avg_loss = 3.28662\n",
      "epoch no.3 train no.273260  loss = 2.53966 avg_loss = 3.29722\n",
      "epoch no.3 train no.273270  loss = 2.79115 avg_loss = 3.32596\n",
      "epoch no.3 train no.273280  loss = 2.74622 avg_loss = 3.32880\n",
      "epoch no.3 train no.273290  loss = 2.87352 avg_loss = 3.28886\n",
      "epoch no.3 train no.273300  loss = 2.54457 avg_loss = 3.27623\n",
      "epoch no.3 train no.273310  loss = 2.84083 avg_loss = 3.30617\n",
      "epoch no.3 train no.273320  loss = 3.57541 avg_loss = 3.31576\n",
      "epoch no.3 train no.273330  loss = 3.41094 avg_loss = 3.31375\n",
      "epoch no.3 train no.273340  loss = 3.97212 avg_loss = 3.29834\n",
      "epoch no.3 train no.273350  loss = 2.94538 avg_loss = 3.26962\n",
      "epoch no.3 train no.273360  loss = 2.69153 avg_loss = 3.26938\n",
      "epoch no.3 train no.273370  loss = 5.10793 avg_loss = 3.30547\n",
      "epoch no.3 train no.273380  loss = 4.13595 avg_loss = 3.34020\n",
      "epoch no.3 train no.273390  loss = 4.35182 avg_loss = 3.35145\n",
      "epoch no.3 train no.273400  loss = 3.42313 avg_loss = 3.30478\n",
      "epoch no.3 train no.273410  loss = 2.96527 avg_loss = 3.37161\n",
      "epoch no.3 train no.273420  loss = 3.06189 avg_loss = 3.37323\n",
      "epoch no.3 train no.273430  loss = 2.59560 avg_loss = 3.39545\n",
      "epoch no.3 train no.273440  loss = 3.25558 avg_loss = 3.37622\n",
      "epoch no.3 train no.273450  loss = 1.82987 avg_loss = 3.36913\n",
      "epoch no.3 train no.273460  loss = 1.92191 avg_loss = 3.34400\n",
      "epoch no.3 train no.273470  loss = 3.08922 avg_loss = 3.33644\n",
      "epoch no.3 train no.273480  loss = 4.47805 avg_loss = 3.32400\n",
      "epoch no.3 train no.273490  loss = 2.77023 avg_loss = 3.33415\n",
      "epoch no.3 train no.273500  loss = 3.09055 avg_loss = 3.30159\n",
      "epoch no.3 train no.273510  loss = 3.15570 avg_loss = 3.31828\n",
      "epoch no.3 train no.273520  loss = 3.57068 avg_loss = 3.32341\n",
      "epoch no.3 train no.273530  loss = 2.49811 avg_loss = 3.29193\n",
      "epoch no.3 train no.273540  loss = 1.60019 avg_loss = 3.30119\n",
      "epoch no.3 train no.273550  loss = 4.34511 avg_loss = 3.36759\n",
      "epoch no.3 train no.273560  loss = 2.69851 avg_loss = 3.36136\n",
      "epoch no.3 train no.273570  loss = 5.85634 avg_loss = 3.40336\n",
      "epoch no.3 train no.273580  loss = 3.59303 avg_loss = 3.37930\n",
      "epoch no.3 train no.273590  loss = 2.70383 avg_loss = 3.34411\n",
      "epoch no.3 train no.273600  loss = 3.70508 avg_loss = 3.36495\n",
      "epoch no.3 train no.273610  loss = 2.82824 avg_loss = 3.31073\n",
      "epoch no.3 train no.273620  loss = 3.00455 avg_loss = 3.33069\n",
      "epoch no.3 train no.273630  loss = 2.45722 avg_loss = 3.32845\n",
      "epoch no.3 train no.273640  loss = 3.60636 avg_loss = 3.31629\n",
      "epoch no.3 train no.273650  loss = 3.10771 avg_loss = 3.35491\n",
      "epoch no.3 train no.273660  loss = 5.38353 avg_loss = 3.40944\n",
      "epoch no.3 train no.273670  loss = 3.68809 avg_loss = 3.44152\n",
      "epoch no.3 train no.273680  loss = 2.80037 avg_loss = 3.42679\n",
      "epoch no.3 train no.273690  loss = 3.60553 avg_loss = 3.40902\n",
      "epoch no.3 train no.273700  loss = 3.54890 avg_loss = 3.42087\n",
      "epoch no.3 train no.273710  loss = 3.95443 avg_loss = 3.42256\n",
      "epoch no.3 train no.273720  loss = 3.06937 avg_loss = 3.38581\n",
      "epoch no.3 train no.273730  loss = 1.99137 avg_loss = 3.36126\n",
      "epoch no.3 train no.273740  loss = 3.38276 avg_loss = 3.40467\n",
      "epoch no.3 train no.273750  loss = 4.22672 avg_loss = 3.46122\n",
      "epoch no.3 train no.273760  loss = 4.12176 avg_loss = 3.45737\n",
      "epoch no.3 train no.273770  loss = 2.59304 avg_loss = 3.43589\n",
      "epoch no.3 train no.273780  loss = 2.95127 avg_loss = 3.42042\n",
      "epoch no.3 train no.273790  loss = 1.12438 avg_loss = 3.37308\n",
      "epoch no.3 train no.273800  loss = 3.34696 avg_loss = 3.34001\n",
      "epoch no.3 train no.273810  loss = 3.42774 avg_loss = 3.30730\n",
      "epoch no.3 train no.273820  loss = 3.08452 avg_loss = 3.27219\n",
      "epoch no.3 train no.273830  loss = 4.05056 avg_loss = 3.30453\n",
      "epoch no.3 train no.273840  loss = 1.98762 avg_loss = 3.26453\n",
      "epoch no.3 train no.273850  loss = 2.28521 avg_loss = 3.26971\n",
      "epoch no.3 train no.273860  loss = 2.83730 avg_loss = 3.23907\n",
      "epoch no.3 train no.273870  loss = 5.00512 avg_loss = 3.27293\n",
      "epoch no.3 train no.273880  loss = 2.79124 avg_loss = 3.25383\n",
      "epoch no.3 train no.273890  loss = 1.88660 avg_loss = 3.22212\n",
      "epoch no.3 train no.273900  loss = 2.54818 avg_loss = 3.24045\n",
      "epoch no.3 train no.273910  loss = 2.70750 avg_loss = 3.21236\n",
      "epoch no.3 train no.273920  loss = 2.74694 avg_loss = 3.25081\n",
      "epoch no.3 train no.273930  loss = 2.37640 avg_loss = 3.26663\n",
      "epoch no.3 train no.273940  loss = 3.49975 avg_loss = 3.27046\n",
      "epoch no.3 train no.273950  loss = 4.23652 avg_loss = 3.29608\n",
      "epoch no.3 train no.273960  loss = 5.04221 avg_loss = 3.31999\n",
      "epoch no.3 train no.273970  loss = 2.49780 avg_loss = 3.36409\n",
      "epoch no.3 train no.273980  loss = 4.66734 avg_loss = 3.37965\n",
      "epoch no.3 train no.273990  loss = 4.63447 avg_loss = 3.36406\n",
      "epoch no.3 train no.274000  loss = 5.49831 avg_loss = 3.40238\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁이', '▁노래', '지', '</s>']\n",
      "여름엔 이 노래지</s>\n",
      "epoch no.3 train no.274010  loss = 2.88432 avg_loss = 3.37341\n",
      "epoch no.3 train no.274020  loss = 2.26904 avg_loss = 3.35207\n",
      "epoch no.3 train no.274030  loss = 4.58864 avg_loss = 3.35598\n",
      "epoch no.3 train no.274040  loss = 3.05291 avg_loss = 3.35428\n",
      "epoch no.3 train no.274050  loss = 2.78905 avg_loss = 3.34122\n",
      "epoch no.3 train no.274060  loss = 1.72305 avg_loss = 3.34155\n",
      "epoch no.3 train no.274070  loss = 4.18319 avg_loss = 3.29736\n",
      "epoch no.3 train no.274080  loss = 3.37497 avg_loss = 3.34073\n",
      "epoch no.3 train no.274090  loss = 2.63510 avg_loss = 3.31561\n",
      "epoch no.3 train no.274100  loss = 3.34866 avg_loss = 3.32774\n",
      "epoch no.3 train no.274110  loss = 3.24834 avg_loss = 3.36148\n",
      "epoch no.3 train no.274120  loss = 2.21467 avg_loss = 3.34111\n",
      "epoch no.3 train no.274130  loss = 4.69108 avg_loss = 3.39025\n",
      "epoch no.3 train no.274140  loss = 3.99443 avg_loss = 3.39079\n",
      "epoch no.3 train no.274150  loss = 3.33479 avg_loss = 3.36065\n",
      "epoch no.3 train no.274160  loss = 4.36591 avg_loss = 3.37781\n",
      "epoch no.3 train no.274170  loss = 3.87267 avg_loss = 3.34654\n",
      "epoch no.3 train no.274180  loss = 3.16064 avg_loss = 3.32342\n",
      "epoch no.3 train no.274190  loss = 4.60901 avg_loss = 3.42851\n",
      "epoch no.3 train no.274200  loss = 3.86281 avg_loss = 3.39065\n",
      "epoch no.3 train no.274210  loss = 4.82065 avg_loss = 3.41293\n",
      "epoch no.3 train no.274220  loss = 3.95369 avg_loss = 3.41929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.274230  loss = 3.33665 avg_loss = 3.44875\n",
      "epoch no.3 train no.274240  loss = 3.21939 avg_loss = 3.41791\n",
      "epoch no.3 train no.274250  loss = 3.51939 avg_loss = 3.41244\n",
      "epoch no.3 train no.274260  loss = 4.35048 avg_loss = 3.40991\n",
      "epoch no.3 train no.274270  loss = 3.40358 avg_loss = 3.43833\n",
      "epoch no.3 train no.274280  loss = 5.49017 avg_loss = 3.45108\n",
      "epoch no.3 train no.274290  loss = 2.24631 avg_loss = 3.41560\n",
      "epoch no.3 train no.274300  loss = 2.69787 avg_loss = 3.37744\n",
      "epoch no.3 train no.274310  loss = 3.63159 avg_loss = 3.42202\n",
      "epoch no.3 train no.274320  loss = 3.53254 avg_loss = 3.36693\n",
      "epoch no.3 train no.274330  loss = 5.11866 avg_loss = 3.45063\n",
      "epoch no.3 train no.274340  loss = 5.32030 avg_loss = 3.46848\n",
      "epoch no.3 train no.274350  loss = 4.22610 avg_loss = 3.42899\n",
      "epoch no.3 train no.274360  loss = 2.05890 avg_loss = 3.45219\n",
      "epoch no.3 train no.274370  loss = 3.80997 avg_loss = 3.42861\n",
      "epoch no.3 train no.274380  loss = 3.67510 avg_loss = 3.42708\n",
      "epoch no.3 train no.274390  loss = 4.08641 avg_loss = 3.43303\n",
      "epoch no.3 train no.274400  loss = 3.46063 avg_loss = 3.42451\n",
      "epoch no.3 train no.274410  loss = 2.87433 avg_loss = 3.45834\n",
      "epoch no.3 train no.274420  loss = 4.19750 avg_loss = 3.42836\n",
      "epoch no.3 train no.274430  loss = 2.64803 avg_loss = 3.38973\n",
      "epoch no.3 train no.274440  loss = 4.69519 avg_loss = 3.39609\n",
      "epoch no.3 train no.274450  loss = 2.57701 avg_loss = 3.31544\n",
      "epoch no.3 train no.274460  loss = 2.29958 avg_loss = 3.30389\n",
      "epoch no.3 train no.274470  loss = 1.74776 avg_loss = 3.32972\n",
      "epoch no.3 train no.274480  loss = 6.14067 avg_loss = 3.35332\n",
      "epoch no.3 train no.274490  loss = 4.55468 avg_loss = 3.38273\n",
      "epoch no.3 train no.274500  loss = 2.10329 avg_loss = 3.37243\n",
      "epoch no.3 train no.274510  loss = 4.20706 avg_loss = 3.38510\n",
      "epoch no.3 train no.274520  loss = 4.51840 avg_loss = 3.37454\n",
      "epoch no.3 train no.274530  loss = 2.21805 avg_loss = 3.38778\n",
      "epoch no.3 train no.274540  loss = 2.33444 avg_loss = 3.37603\n",
      "epoch no.3 train no.274550  loss = 3.32593 avg_loss = 3.40596\n",
      "epoch no.3 train no.274560  loss = 3.34727 avg_loss = 3.44299\n",
      "epoch no.3 train no.274570  loss = 3.79630 avg_loss = 3.45539\n",
      "epoch no.3 train no.274580  loss = 2.23781 avg_loss = 3.43870\n",
      "epoch no.3 train no.274590  loss = 3.34493 avg_loss = 3.47409\n",
      "epoch no.3 train no.274600  loss = 2.97577 avg_loss = 3.44807\n",
      "epoch no.3 train no.274610  loss = 4.88477 avg_loss = 3.45307\n",
      "epoch no.3 train no.274620  loss = 2.18233 avg_loss = 3.41551\n",
      "epoch no.3 train no.274630  loss = 2.12247 avg_loss = 3.37282\n",
      "epoch no.3 train no.274640  loss = 3.37475 avg_loss = 3.35871\n",
      "epoch no.3 train no.274650  loss = 3.21451 avg_loss = 3.34641\n",
      "epoch no.3 train no.274660  loss = 6.77829 avg_loss = 3.34697\n",
      "epoch no.3 train no.274670  loss = 2.15930 avg_loss = 3.32379\n",
      "epoch no.3 train no.274680  loss = 3.84217 avg_loss = 3.32454\n",
      "epoch no.3 train no.274690  loss = 3.44573 avg_loss = 3.32088\n",
      "epoch no.3 train no.274700  loss = 4.92953 avg_loss = 3.32878\n",
      "epoch no.3 train no.274710  loss = 3.09924 avg_loss = 3.37202\n",
      "epoch no.3 train no.274720  loss = 5.05292 avg_loss = 3.41549\n",
      "epoch no.3 train no.274730  loss = 2.45315 avg_loss = 3.38529\n",
      "epoch no.3 train no.274740  loss = 3.59189 avg_loss = 3.40334\n",
      "epoch no.3 train no.274750  loss = 2.13584 avg_loss = 3.36442\n",
      "epoch no.3 train no.274760  loss = 1.86004 avg_loss = 3.35500\n",
      "epoch no.3 train no.274770  loss = 3.49768 avg_loss = 3.37352\n",
      "epoch no.3 train no.274780  loss = 2.93989 avg_loss = 3.35459\n",
      "epoch no.3 train no.274790  loss = 4.14551 avg_loss = 3.39242\n",
      "epoch no.3 train no.274800  loss = 3.95290 avg_loss = 3.38732\n",
      "epoch no.3 train no.274810  loss = 1.53331 avg_loss = 3.37919\n",
      "epoch no.3 train no.274820  loss = 2.96624 avg_loss = 3.40253\n",
      "epoch no.3 train no.274830  loss = 2.08050 avg_loss = 3.36948\n",
      "epoch no.3 train no.274840  loss = 4.33396 avg_loss = 3.38659\n",
      "epoch no.3 train no.274850  loss = 3.21597 avg_loss = 3.35234\n",
      "epoch no.3 train no.274860  loss = 3.55202 avg_loss = 3.40993\n",
      "epoch no.3 train no.274870  loss = 3.25133 avg_loss = 3.45591\n",
      "epoch no.3 train no.274880  loss = 4.51994 avg_loss = 3.44388\n",
      "epoch no.3 train no.274890  loss = 2.78449 avg_loss = 3.44299\n",
      "epoch no.3 train no.274900  loss = 3.59379 avg_loss = 3.43765\n",
      "epoch no.3 train no.274910  loss = 3.53937 avg_loss = 3.40769\n",
      "epoch no.3 train no.274920  loss = 4.59321 avg_loss = 3.42729\n",
      "epoch no.3 train no.274930  loss = 3.80613 avg_loss = 3.42593\n",
      "epoch no.3 train no.274940  loss = 4.01456 avg_loss = 3.39095\n",
      "epoch no.3 train no.274950  loss = 2.19228 avg_loss = 3.35554\n",
      "epoch no.3 train no.274960  loss = 3.82052 avg_loss = 3.42204\n",
      "epoch no.3 train no.274970  loss = 4.73193 avg_loss = 3.46825\n",
      "epoch no.3 train no.274980  loss = 3.11854 avg_loss = 3.49094\n",
      "epoch no.3 train no.274990  loss = 4.77310 avg_loss = 3.46187\n",
      "epoch no.3 train no.275000  loss = 3.46793 avg_loss = 3.43324\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁꿀', '▁플레이', '컬', '</s>']\n",
      "여름밤의 재즈보컬</s>\n",
      "epoch no.3 train no.275010  loss = 3.81419 avg_loss = 3.45143\n",
      "epoch no.3 train no.275020  loss = 4.10612 avg_loss = 3.45224\n",
      "epoch no.3 train no.275030  loss = 3.95532 avg_loss = 3.42839\n",
      "epoch no.3 train no.275040  loss = 3.49159 avg_loss = 3.41409\n",
      "epoch no.3 train no.275050  loss = 3.59437 avg_loss = 3.37063\n",
      "epoch no.3 train no.275060  loss = 3.44885 avg_loss = 3.36644\n",
      "epoch no.3 train no.275070  loss = 2.79329 avg_loss = 3.36286\n",
      "epoch no.3 train no.275080  loss = 2.87950 avg_loss = 3.36243\n",
      "epoch no.3 train no.275090  loss = 3.04664 avg_loss = 3.36694\n",
      "epoch no.3 train no.275100  loss = 2.88246 avg_loss = 3.45409\n",
      "epoch no.3 train no.275110  loss = 2.15603 avg_loss = 3.43726\n",
      "epoch no.3 train no.275120  loss = 3.08129 avg_loss = 3.43514\n",
      "epoch no.3 train no.275130  loss = 1.91660 avg_loss = 3.42530\n",
      "epoch no.3 train no.275140  loss = 3.13252 avg_loss = 3.39492\n",
      "epoch no.3 train no.275150  loss = 3.05598 avg_loss = 3.44807\n",
      "epoch no.3 train no.275160  loss = 1.30237 avg_loss = 3.45302\n",
      "epoch no.3 train no.275170  loss = 2.34670 avg_loss = 3.43620\n",
      "epoch no.3 train no.275180  loss = 2.13445 avg_loss = 3.45435\n",
      "epoch no.3 train no.275190  loss = 2.07264 avg_loss = 3.37333\n",
      "epoch no.3 train no.275200  loss = 2.87433 avg_loss = 3.37199\n",
      "epoch no.3 train no.275210  loss = 4.36545 avg_loss = 3.35715\n",
      "epoch no.3 train no.275220  loss = 3.05470 avg_loss = 3.38106\n",
      "epoch no.3 train no.275230  loss = 2.28869 avg_loss = 3.36919\n",
      "epoch no.3 train no.275240  loss = 3.66031 avg_loss = 3.36161\n",
      "epoch no.3 train no.275250  loss = 4.84312 avg_loss = 3.37199\n",
      "epoch no.3 train no.275260  loss = 4.52175 avg_loss = 3.36877\n",
      "epoch no.3 train no.275270  loss = 4.66339 avg_loss = 3.39048\n",
      "epoch no.3 train no.275280  loss = 2.72450 avg_loss = 3.35331\n",
      "epoch no.3 train no.275290  loss = 2.50278 avg_loss = 3.34965\n",
      "epoch no.3 train no.275300  loss = 4.04784 avg_loss = 3.34307\n",
      "epoch no.3 train no.275310  loss = 1.83733 avg_loss = 3.35659\n",
      "epoch no.3 train no.275320  loss = 4.14874 avg_loss = 3.39089\n",
      "epoch no.3 train no.275330  loss = 3.09884 avg_loss = 3.35612\n",
      "epoch no.3 train no.275340  loss = 3.66750 avg_loss = 3.31167\n",
      "epoch no.3 train no.275350  loss = 3.76691 avg_loss = 3.32274\n",
      "epoch no.3 train no.275360  loss = 3.25194 avg_loss = 3.33439\n",
      "epoch no.3 train no.275370  loss = 4.38798 avg_loss = 3.34988\n",
      "epoch no.3 train no.275380  loss = 5.60878 avg_loss = 3.35466\n",
      "epoch no.3 train no.275390  loss = 3.17578 avg_loss = 3.35979\n",
      "epoch no.3 train no.275400  loss = 2.84537 avg_loss = 3.37157\n",
      "epoch no.3 train no.275410  loss = 3.93555 avg_loss = 3.37508\n",
      "epoch no.3 train no.275420  loss = 3.47576 avg_loss = 3.40296\n",
      "epoch no.3 train no.275430  loss = 2.88776 avg_loss = 3.44455\n",
      "epoch no.3 train no.275440  loss = 2.65203 avg_loss = 3.42484\n",
      "epoch no.3 train no.275450  loss = 3.13849 avg_loss = 3.41099\n",
      "epoch no.3 train no.275460  loss = 3.55710 avg_loss = 3.41866\n",
      "epoch no.3 train no.275470  loss = 2.65982 avg_loss = 3.41872\n",
      "epoch no.3 train no.275480  loss = 4.10574 avg_loss = 3.39929\n",
      "epoch no.3 train no.275490  loss = 1.85902 avg_loss = 3.35713\n",
      "epoch no.3 train no.275500  loss = 4.09021 avg_loss = 3.42243\n",
      "epoch no.3 train no.275510  loss = 2.27238 avg_loss = 3.40785\n",
      "epoch no.3 train no.275520  loss = 4.37883 avg_loss = 3.42212\n",
      "epoch no.3 train no.275530  loss = 3.82828 avg_loss = 3.41931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.275540  loss = 5.65956 avg_loss = 3.47395\n",
      "epoch no.3 train no.275550  loss = 3.10807 avg_loss = 3.46063\n",
      "epoch no.3 train no.275560  loss = 1.78981 avg_loss = 3.39074\n",
      "epoch no.3 train no.275570  loss = 4.56792 avg_loss = 3.43508\n",
      "epoch no.3 train no.275580  loss = 2.02521 avg_loss = 3.41741\n",
      "epoch no.3 train no.275590  loss = 6.77620 avg_loss = 3.45817\n",
      "epoch no.3 train no.275600  loss = 3.55517 avg_loss = 3.45150\n",
      "epoch no.3 train no.275610  loss = 5.75102 avg_loss = 3.49244\n",
      "epoch no.3 train no.275620  loss = 3.09332 avg_loss = 3.49177\n",
      "epoch no.3 train no.275630  loss = 2.98576 avg_loss = 3.47754\n",
      "epoch no.3 train no.275640  loss = 2.65862 avg_loss = 3.43006\n",
      "epoch no.3 train no.275650  loss = 4.25874 avg_loss = 3.45294\n",
      "epoch no.3 train no.275660  loss = 3.62087 avg_loss = 3.44711\n",
      "epoch no.3 train no.275670  loss = 2.18315 avg_loss = 3.43530\n",
      "epoch no.3 train no.275680  loss = 3.61691 avg_loss = 3.44481\n",
      "epoch no.3 train no.275690  loss = 2.79782 avg_loss = 3.42164\n",
      "epoch no.3 train no.275700  loss = 3.80357 avg_loss = 3.37843\n",
      "epoch no.3 train no.275710  loss = 2.34543 avg_loss = 3.37677\n",
      "epoch no.3 train no.275720  loss = 4.91080 avg_loss = 3.44279\n",
      "epoch no.3 train no.275730  loss = 2.80835 avg_loss = 3.40700\n",
      "epoch no.3 train no.275740  loss = 2.73537 avg_loss = 3.37155\n",
      "epoch no.3 train no.275750  loss = 2.94666 avg_loss = 3.38428\n",
      "epoch no.3 train no.275760  loss = 3.27527 avg_loss = 3.41953\n",
      "epoch no.3 train no.275770  loss = 2.49936 avg_loss = 3.38748\n",
      "epoch no.3 train no.275780  loss = 1.80993 avg_loss = 3.38537\n",
      "epoch no.3 train no.275790  loss = 4.25392 avg_loss = 3.37481\n",
      "epoch no.3 train no.275800  loss = 2.82740 avg_loss = 3.38579\n",
      "epoch no.3 train no.275810  loss = 2.96077 avg_loss = 3.40587\n",
      "epoch no.3 train no.275820  loss = 5.24212 avg_loss = 3.39255\n",
      "epoch no.3 train no.275830  loss = 3.97197 avg_loss = 3.38523\n",
      "epoch no.3 train no.275840  loss = 3.61922 avg_loss = 3.36692\n",
      "epoch no.3 train no.275850  loss = 2.42838 avg_loss = 3.38146\n",
      "epoch no.3 train no.275860  loss = 3.64653 avg_loss = 3.36730\n",
      "epoch no.3 train no.275870  loss = 3.42076 avg_loss = 3.36104\n",
      "epoch no.3 train no.275880  loss = 2.52830 avg_loss = 3.34204\n",
      "epoch no.3 train no.275890  loss = 3.98939 avg_loss = 3.39234\n",
      "epoch no.3 train no.275900  loss = 2.97065 avg_loss = 3.47597\n",
      "epoch no.3 train no.275910  loss = 2.35183 avg_loss = 3.43697\n",
      "epoch no.3 train no.275920  loss = 4.21910 avg_loss = 3.45875\n",
      "epoch no.3 train no.275930  loss = 3.64335 avg_loss = 3.46994\n",
      "epoch no.3 train no.275940  loss = 2.93837 avg_loss = 3.42855\n",
      "epoch no.3 train no.275950  loss = 3.38900 avg_loss = 3.42042\n",
      "epoch no.3 train no.275960  loss = 3.50088 avg_loss = 3.41728\n",
      "epoch no.3 train no.275970  loss = 3.77282 avg_loss = 3.36706\n",
      "epoch no.3 train no.275980  loss = 3.77994 avg_loss = 3.35283\n",
      "epoch no.3 train no.275990  loss = 2.55459 avg_loss = 3.28127\n",
      "epoch no.3 train no.276000  loss = 4.54285 avg_loss = 3.30440\n",
      "4\n",
      "to_tokens: ['▁비', '엔', '에', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.276010  loss = 2.42647 avg_loss = 3.30236\n",
      "epoch no.3 train no.276020  loss = 4.53004 avg_loss = 3.30082\n",
      "epoch no.3 train no.276030  loss = 2.83164 avg_loss = 3.29802\n",
      "epoch no.3 train no.276040  loss = 3.02499 avg_loss = 3.29306\n",
      "epoch no.3 train no.276050  loss = 2.18818 avg_loss = 3.29579\n",
      "epoch no.3 train no.276060  loss = 3.71265 avg_loss = 3.27389\n",
      "epoch no.3 train no.276070  loss = 3.14272 avg_loss = 3.26674\n",
      "epoch no.3 train no.276080  loss = 3.75876 avg_loss = 3.26093\n",
      "epoch no.3 train no.276090  loss = 3.84113 avg_loss = 3.28578\n",
      "epoch no.3 train no.276100  loss = 3.22086 avg_loss = 3.34956\n",
      "epoch no.3 train no.276110  loss = 3.44864 avg_loss = 3.32655\n",
      "epoch no.3 train no.276120  loss = 4.41946 avg_loss = 3.29912\n",
      "epoch no.3 train no.276130  loss = 2.95121 avg_loss = 3.29107\n",
      "epoch no.3 train no.276140  loss = 3.20194 avg_loss = 3.31509\n",
      "epoch no.3 train no.276150  loss = 2.46692 avg_loss = 3.26270\n",
      "epoch no.3 train no.276160  loss = 2.37940 avg_loss = 3.30252\n",
      "epoch no.3 train no.276170  loss = 2.35074 avg_loss = 3.28430\n",
      "epoch no.3 train no.276180  loss = 2.93245 avg_loss = 3.29845\n",
      "epoch no.3 train no.276190  loss = 2.95672 avg_loss = 3.25068\n",
      "epoch no.3 train no.276200  loss = 3.10261 avg_loss = 3.31626\n",
      "epoch no.3 train no.276210  loss = 4.23992 avg_loss = 3.30018\n",
      "epoch no.3 train no.276220  loss = 4.77253 avg_loss = 3.31090\n",
      "epoch no.3 train no.276230  loss = 2.10422 avg_loss = 3.27112\n",
      "epoch no.3 train no.276240  loss = 4.92546 avg_loss = 3.28991\n",
      "epoch no.3 train no.276250  loss = 4.57643 avg_loss = 3.33511\n",
      "epoch no.3 train no.276260  loss = 1.51496 avg_loss = 3.32347\n",
      "epoch no.3 train no.276270  loss = 3.41005 avg_loss = 3.34175\n",
      "epoch no.3 train no.276280  loss = 5.08996 avg_loss = 3.34843\n",
      "epoch no.3 train no.276290  loss = 1.61023 avg_loss = 3.32280\n",
      "epoch no.3 train no.276300  loss = 5.09273 avg_loss = 3.33855\n",
      "epoch no.3 train no.276310  loss = 3.48909 avg_loss = 3.40991\n",
      "epoch no.3 train no.276320  loss = 3.72160 avg_loss = 3.40437\n",
      "epoch no.3 train no.276330  loss = 4.24389 avg_loss = 3.37564\n",
      "epoch no.3 train no.276340  loss = 3.33302 avg_loss = 3.40877\n",
      "epoch no.3 train no.276350  loss = 5.91342 avg_loss = 3.41639\n",
      "epoch no.3 train no.276360  loss = 5.25712 avg_loss = 3.41117\n",
      "epoch no.3 train no.276370  loss = 5.29519 avg_loss = 3.42404\n",
      "epoch no.3 train no.276380  loss = 2.57408 avg_loss = 3.43149\n",
      "epoch no.3 train no.276390  loss = 2.92167 avg_loss = 3.37146\n",
      "epoch no.3 train no.276400  loss = 4.22584 avg_loss = 3.41289\n",
      "epoch no.3 train no.276410  loss = 1.29845 avg_loss = 3.36849\n",
      "epoch no.3 train no.276420  loss = 3.23233 avg_loss = 3.37503\n",
      "epoch no.3 train no.276430  loss = 4.16230 avg_loss = 3.36446\n",
      "epoch no.3 train no.276440  loss = 2.90199 avg_loss = 3.29757\n",
      "epoch no.3 train no.276450  loss = 2.59850 avg_loss = 3.28224\n",
      "epoch no.3 train no.276460  loss = 2.25005 avg_loss = 3.27445\n",
      "epoch no.3 train no.276470  loss = 2.33323 avg_loss = 3.27502\n",
      "epoch no.3 train no.276480  loss = 3.49902 avg_loss = 3.30709\n",
      "epoch no.3 train no.276490  loss = 3.31219 avg_loss = 3.31305\n",
      "epoch no.3 train no.276500  loss = 2.04710 avg_loss = 3.31141\n",
      "epoch no.3 train no.276510  loss = 4.32289 avg_loss = 3.32137\n",
      "epoch no.3 train no.276520  loss = 2.79601 avg_loss = 3.30448\n",
      "epoch no.3 train no.276530  loss = 2.75664 avg_loss = 3.32680\n",
      "epoch no.3 train no.276540  loss = 2.91919 avg_loss = 3.31152\n",
      "epoch no.3 train no.276550  loss = 3.28415 avg_loss = 3.29702\n",
      "epoch no.3 train no.276560  loss = 2.92676 avg_loss = 3.33206\n",
      "epoch no.3 train no.276570  loss = 3.55412 avg_loss = 3.32993\n",
      "epoch no.3 train no.276580  loss = 2.07017 avg_loss = 3.31221\n",
      "epoch no.3 train no.276590  loss = 2.97897 avg_loss = 3.32299\n",
      "epoch no.3 train no.276600  loss = 3.05659 avg_loss = 3.31128\n",
      "epoch no.3 train no.276610  loss = 2.55118 avg_loss = 3.29796\n",
      "epoch no.3 train no.276620  loss = 2.70250 avg_loss = 3.26687\n",
      "epoch no.3 train no.276630  loss = 4.19705 avg_loss = 3.26917\n",
      "epoch no.3 train no.276640  loss = 2.78283 avg_loss = 3.34577\n",
      "epoch no.3 train no.276650  loss = 3.34322 avg_loss = 3.37882\n",
      "epoch no.3 train no.276660  loss = 3.71673 avg_loss = 3.38617\n",
      "epoch no.3 train no.276670  loss = 2.95467 avg_loss = 3.35377\n",
      "epoch no.3 train no.276680  loss = 3.19619 avg_loss = 3.40109\n",
      "epoch no.3 train no.276690  loss = 3.83815 avg_loss = 3.45241\n",
      "epoch no.3 train no.276700  loss = 4.25810 avg_loss = 3.44005\n",
      "epoch no.3 train no.276710  loss = 3.06772 avg_loss = 3.34609\n",
      "epoch no.3 train no.276720  loss = 3.39387 avg_loss = 3.34567\n",
      "epoch no.3 train no.276730  loss = 2.63563 avg_loss = 3.32578\n",
      "epoch no.3 train no.276740  loss = 5.48458 avg_loss = 3.40983\n",
      "epoch no.3 train no.276750  loss = 4.62551 avg_loss = 3.49926\n",
      "epoch no.3 train no.276760  loss = 3.41221 avg_loss = 3.47886\n",
      "epoch no.3 train no.276770  loss = 2.53490 avg_loss = 3.44344\n",
      "epoch no.3 train no.276780  loss = 4.19454 avg_loss = 3.42701\n",
      "epoch no.3 train no.276790  loss = 3.96672 avg_loss = 3.41718\n",
      "epoch no.3 train no.276800  loss = 2.08578 avg_loss = 3.45591\n",
      "epoch no.3 train no.276810  loss = 2.07458 avg_loss = 3.44534\n",
      "epoch no.3 train no.276820  loss = 1.83588 avg_loss = 3.39266\n",
      "epoch no.3 train no.276830  loss = 3.52257 avg_loss = 3.39216\n",
      "epoch no.3 train no.276840  loss = 1.94650 avg_loss = 3.36037\n",
      "epoch no.3 train no.276850  loss = 3.61572 avg_loss = 3.37461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.276860  loss = 2.68608 avg_loss = 3.35615\n",
      "epoch no.3 train no.276870  loss = 5.40315 avg_loss = 3.38648\n",
      "epoch no.3 train no.276880  loss = 3.10796 avg_loss = 3.38318\n",
      "epoch no.3 train no.276890  loss = 4.06696 avg_loss = 3.34395\n",
      "epoch no.3 train no.276900  loss = 3.92471 avg_loss = 3.31356\n",
      "epoch no.3 train no.276910  loss = 2.24702 avg_loss = 3.26875\n",
      "epoch no.3 train no.276920  loss = 3.55858 avg_loss = 3.28376\n",
      "epoch no.3 train no.276930  loss = 3.30151 avg_loss = 3.28456\n",
      "epoch no.3 train no.276940  loss = 2.94343 avg_loss = 3.28934\n",
      "epoch no.3 train no.276950  loss = 3.66727 avg_loss = 3.27605\n",
      "epoch no.3 train no.276960  loss = 2.91735 avg_loss = 3.36035\n",
      "epoch no.3 train no.276970  loss = 4.01639 avg_loss = 3.36247\n",
      "epoch no.3 train no.276980  loss = 2.98641 avg_loss = 3.38501\n",
      "epoch no.3 train no.276990  loss = 3.70696 avg_loss = 3.41181\n",
      "epoch no.3 train no.277000  loss = 3.49989 avg_loss = 3.43144\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁어울리는', '으면', '▁더', '▁노래', '</s>']\n",
      "여름밤에 들으면 좋은 노래</s>\n",
      "epoch no.3 train no.277010  loss = 2.76867 avg_loss = 3.42197\n",
      "epoch no.3 train no.277020  loss = 3.08766 avg_loss = 3.42405\n",
      "epoch no.3 train no.277030  loss = 3.09041 avg_loss = 3.41133\n",
      "epoch no.3 train no.277040  loss = 3.32782 avg_loss = 3.41266\n",
      "epoch no.3 train no.277050  loss = 2.99030 avg_loss = 3.38311\n",
      "epoch no.3 train no.277060  loss = 3.03502 avg_loss = 3.39589\n",
      "epoch no.3 train no.277070  loss = 2.82164 avg_loss = 3.39748\n",
      "epoch no.3 train no.277080  loss = 3.78550 avg_loss = 3.40810\n",
      "epoch no.3 train no.277090  loss = 3.13050 avg_loss = 3.37937\n",
      "epoch no.3 train no.277100  loss = 4.74501 avg_loss = 3.40601\n",
      "epoch no.3 train no.277110  loss = 3.30307 avg_loss = 3.43371\n",
      "epoch no.3 train no.277120  loss = 3.11089 avg_loss = 3.41518\n",
      "epoch no.3 train no.277130  loss = 3.01178 avg_loss = 3.37007\n",
      "epoch no.3 train no.277140  loss = 2.22240 avg_loss = 3.34106\n",
      "epoch no.3 train no.277150  loss = 3.29421 avg_loss = 3.34168\n",
      "epoch no.3 train no.277160  loss = 4.85173 avg_loss = 3.34594\n",
      "epoch no.3 train no.277170  loss = 2.87659 avg_loss = 3.34983\n",
      "epoch no.3 train no.277180  loss = 3.08993 avg_loss = 3.33896\n",
      "epoch no.3 train no.277190  loss = 2.85328 avg_loss = 3.31928\n",
      "epoch no.3 train no.277200  loss = 3.71508 avg_loss = 3.34704\n",
      "epoch no.3 train no.277210  loss = 2.93471 avg_loss = 3.34698\n",
      "epoch no.3 train no.277220  loss = 3.31576 avg_loss = 3.33630\n",
      "epoch no.3 train no.277230  loss = 2.50524 avg_loss = 3.36405\n",
      "epoch no.3 train no.277240  loss = 1.93101 avg_loss = 3.30176\n",
      "epoch no.3 train no.277250  loss = 5.99945 avg_loss = 3.35516\n",
      "epoch no.3 train no.277260  loss = 3.68811 avg_loss = 3.32617\n",
      "epoch no.3 train no.277270  loss = 5.42059 avg_loss = 3.32977\n",
      "epoch no.3 train no.277280  loss = 3.22159 avg_loss = 3.35770\n",
      "epoch no.3 train no.277290  loss = 3.24676 avg_loss = 3.33457\n",
      "epoch no.3 train no.277300  loss = 3.91955 avg_loss = 3.33031\n",
      "epoch no.3 train no.277310  loss = 1.97158 avg_loss = 3.33880\n",
      "epoch no.3 train no.277320  loss = 3.87920 avg_loss = 3.37611\n",
      "epoch no.3 train no.277330  loss = 5.37790 avg_loss = 3.39465\n",
      "epoch no.3 train no.277340  loss = 4.52018 avg_loss = 3.38966\n",
      "epoch no.3 train no.277350  loss = 2.85098 avg_loss = 3.37247\n",
      "epoch no.3 train no.277360  loss = 4.51157 avg_loss = 3.39388\n",
      "epoch no.3 train no.277370  loss = 3.73977 avg_loss = 3.44104\n",
      "epoch no.3 train no.277380  loss = 2.49909 avg_loss = 3.41023\n",
      "epoch no.3 train no.277390  loss = 4.65304 avg_loss = 3.42771\n",
      "epoch no.3 train no.277400  loss = 2.90487 avg_loss = 3.43069\n",
      "epoch no.3 train no.277410  loss = 3.94445 avg_loss = 3.44608\n",
      "epoch no.3 train no.277420  loss = 2.81129 avg_loss = 3.41685\n",
      "epoch no.3 train no.277430  loss = 2.70668 avg_loss = 3.40294\n",
      "epoch no.3 train no.277440  loss = 3.36230 avg_loss = 3.39499\n",
      "epoch no.3 train no.277450  loss = 5.55633 avg_loss = 3.39974\n",
      "epoch no.3 train no.277460  loss = 5.71968 avg_loss = 3.44252\n",
      "epoch no.3 train no.277470  loss = 3.98179 avg_loss = 3.45799\n",
      "epoch no.3 train no.277480  loss = 1.58140 avg_loss = 3.45877\n",
      "epoch no.3 train no.277490  loss = 3.10047 avg_loss = 3.44794\n",
      "epoch no.3 train no.277500  loss = 5.47309 avg_loss = 3.45940\n",
      "epoch no.3 train no.277510  loss = 4.39590 avg_loss = 3.46289\n",
      "epoch no.3 train no.277520  loss = 3.00081 avg_loss = 3.44669\n",
      "epoch no.3 train no.277530  loss = 3.27913 avg_loss = 3.43194\n",
      "epoch no.3 train no.277540  loss = 5.55205 avg_loss = 3.42808\n",
      "epoch no.3 train no.277550  loss = 1.81402 avg_loss = 3.40952\n",
      "epoch no.3 train no.277560  loss = 3.44172 avg_loss = 3.37559\n",
      "epoch no.3 train no.277570  loss = 2.82499 avg_loss = 3.37683\n",
      "epoch no.3 train no.277580  loss = 3.86812 avg_loss = 3.34567\n",
      "epoch no.3 train no.277590  loss = 2.35091 avg_loss = 3.37576\n",
      "epoch no.3 train no.277600  loss = 4.05200 avg_loss = 3.38435\n",
      "epoch no.3 train no.277610  loss = 4.72923 avg_loss = 3.40352\n",
      "epoch no.3 train no.277620  loss = 3.12969 avg_loss = 3.39990\n",
      "epoch no.3 train no.277630  loss = 2.96537 avg_loss = 3.35568\n",
      "epoch no.3 train no.277640  loss = 2.31169 avg_loss = 3.30629\n",
      "epoch no.3 train no.277650  loss = 4.36069 avg_loss = 3.33031\n",
      "epoch no.3 train no.277660  loss = 1.71280 avg_loss = 3.29312\n",
      "epoch no.3 train no.277670  loss = 6.03046 avg_loss = 3.33384\n",
      "epoch no.3 train no.277680  loss = 6.08776 avg_loss = 3.36410\n",
      "epoch no.3 train no.277690  loss = 3.37009 avg_loss = 3.35248\n",
      "epoch no.3 train no.277700  loss = 2.73516 avg_loss = 3.36759\n",
      "epoch no.3 train no.277710  loss = 3.37063 avg_loss = 3.35725\n",
      "epoch no.3 train no.277720  loss = 3.52096 avg_loss = 3.35141\n",
      "epoch no.3 train no.277730  loss = 3.96304 avg_loss = 3.39611\n",
      "epoch no.3 train no.277740  loss = 3.88036 avg_loss = 3.40512\n",
      "epoch no.3 train no.277750  loss = 2.50108 avg_loss = 3.30969\n",
      "epoch no.3 train no.277760  loss = 2.86875 avg_loss = 3.32767\n",
      "epoch no.3 train no.277770  loss = 2.93549 avg_loss = 3.31290\n",
      "epoch no.3 train no.277780  loss = 4.58034 avg_loss = 3.31492\n",
      "epoch no.3 train no.277790  loss = 3.03499 avg_loss = 3.31185\n",
      "epoch no.3 train no.277800  loss = 4.89329 avg_loss = 3.27892\n",
      "epoch no.3 train no.277810  loss = 2.74928 avg_loss = 3.26084\n",
      "epoch no.3 train no.277820  loss = 2.99402 avg_loss = 3.24896\n",
      "epoch no.3 train no.277830  loss = 4.00670 avg_loss = 3.22971\n",
      "epoch no.3 train no.277840  loss = 5.27488 avg_loss = 3.24968\n",
      "epoch no.3 train no.277850  loss = 2.50888 avg_loss = 3.19010\n",
      "epoch no.3 train no.277860  loss = 3.73775 avg_loss = 3.19018\n",
      "epoch no.3 train no.277870  loss = 3.30448 avg_loss = 3.19929\n",
      "epoch no.3 train no.277880  loss = 2.69970 avg_loss = 3.21792\n",
      "epoch no.3 train no.277890  loss = 4.31123 avg_loss = 3.20900\n",
      "epoch no.3 train no.277900  loss = 2.76486 avg_loss = 3.20163\n",
      "epoch no.3 train no.277910  loss = 3.49533 avg_loss = 3.26874\n",
      "epoch no.3 train no.277920  loss = 2.53242 avg_loss = 3.24492\n",
      "epoch no.3 train no.277930  loss = 3.52735 avg_loss = 3.24539\n",
      "epoch no.3 train no.277940  loss = 2.18573 avg_loss = 3.24688\n",
      "epoch no.3 train no.277950  loss = 3.14016 avg_loss = 3.25823\n",
      "epoch no.3 train no.277960  loss = 4.00504 avg_loss = 3.24056\n",
      "epoch no.3 train no.277970  loss = 3.09471 avg_loss = 3.20162\n",
      "epoch no.3 train no.277980  loss = 3.25513 avg_loss = 3.23524\n",
      "epoch no.3 train no.277990  loss = 1.99847 avg_loss = 3.22160\n",
      "epoch no.3 train no.278000  loss = 2.12658 avg_loss = 3.24119\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.3 train no.278010  loss = 3.23402 avg_loss = 3.29270\n",
      "epoch no.3 train no.278020  loss = 2.88971 avg_loss = 3.28399\n",
      "epoch no.3 train no.278030  loss = 3.50607 avg_loss = 3.29099\n",
      "epoch no.3 train no.278040  loss = 5.55139 avg_loss = 3.27491\n",
      "epoch no.3 train no.278050  loss = 4.04137 avg_loss = 3.23442\n",
      "epoch no.3 train no.278060  loss = 3.57299 avg_loss = 3.25769\n",
      "epoch no.3 train no.278070  loss = 4.10779 avg_loss = 3.24704\n",
      "epoch no.3 train no.278080  loss = 2.84426 avg_loss = 3.20541\n",
      "epoch no.3 train no.278090  loss = 3.88290 avg_loss = 3.21296\n",
      "epoch no.3 train no.278100  loss = 3.01787 avg_loss = 3.25946\n",
      "epoch no.3 train no.278110  loss = 3.43759 avg_loss = 3.24595\n",
      "epoch no.3 train no.278120  loss = 3.19043 avg_loss = 3.27083\n",
      "epoch no.3 train no.278130  loss = 2.64452 avg_loss = 3.24539\n",
      "epoch no.3 train no.278140  loss = 3.31738 avg_loss = 3.25983\n",
      "epoch no.3 train no.278150  loss = 4.82419 avg_loss = 3.30296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.278160  loss = 2.46631 avg_loss = 3.37422\n",
      "epoch no.3 train no.278170  loss = 1.89720 avg_loss = 3.35530\n",
      "epoch no.3 train no.278180  loss = 2.75621 avg_loss = 3.34073\n",
      "epoch no.3 train no.278190  loss = 1.41815 avg_loss = 3.25429\n",
      "epoch no.3 train no.278200  loss = 4.08232 avg_loss = 3.28648\n",
      "epoch no.3 train no.278210  loss = 4.01449 avg_loss = 3.27692\n",
      "epoch no.3 train no.278220  loss = 3.67276 avg_loss = 3.30186\n",
      "epoch no.3 train no.278230  loss = 2.71277 avg_loss = 3.34061\n",
      "epoch no.3 train no.278240  loss = 3.65280 avg_loss = 3.33067\n",
      "epoch no.3 train no.278250  loss = 2.56996 avg_loss = 3.31446\n",
      "epoch no.3 train no.278260  loss = 2.64618 avg_loss = 3.29758\n",
      "epoch no.3 train no.278270  loss = 3.77588 avg_loss = 3.34140\n",
      "epoch no.3 train no.278280  loss = 6.58237 avg_loss = 3.38897\n",
      "epoch no.3 train no.278290  loss = 2.77925 avg_loss = 3.37678\n",
      "epoch no.3 train no.278300  loss = 2.38408 avg_loss = 3.36324\n",
      "epoch no.3 train no.278310  loss = 3.62105 avg_loss = 3.34029\n",
      "epoch no.3 train no.278320  loss = 5.08223 avg_loss = 3.33384\n",
      "epoch no.3 train no.278330  loss = 4.62827 avg_loss = 3.36449\n",
      "epoch no.3 train no.278340  loss = 3.55760 avg_loss = 3.33394\n",
      "epoch no.3 train no.278350  loss = 2.98516 avg_loss = 3.35680\n",
      "epoch no.3 train no.278360  loss = 2.91462 avg_loss = 3.34568\n",
      "epoch no.3 train no.278370  loss = 2.77470 avg_loss = 3.37918\n",
      "epoch no.3 train no.278380  loss = 2.45229 avg_loss = 3.33986\n",
      "epoch no.3 train no.278390  loss = 3.31716 avg_loss = 3.37013\n",
      "epoch no.3 train no.278400  loss = 3.43011 avg_loss = 3.34090\n",
      "epoch no.3 train no.278410  loss = 3.20854 avg_loss = 3.40357\n",
      "epoch no.3 train no.278420  loss = 4.62125 avg_loss = 3.36310\n",
      "epoch no.3 train no.278430  loss = 2.16826 avg_loss = 3.33843\n",
      "epoch no.3 train no.278440  loss = 3.02781 avg_loss = 3.34757\n",
      "epoch no.3 train no.278450  loss = 2.27999 avg_loss = 3.31920\n",
      "epoch no.3 train no.278460  loss = 3.96509 avg_loss = 3.35931\n",
      "epoch no.3 train no.278470  loss = 3.65432 avg_loss = 3.38085\n",
      "epoch no.3 train no.278480  loss = 5.22162 avg_loss = 3.40288\n",
      "epoch no.3 train no.278490  loss = 2.89208 avg_loss = 3.45276\n",
      "epoch no.3 train no.278500  loss = 4.03960 avg_loss = 3.45743\n",
      "epoch no.3 train no.278510  loss = 3.05190 avg_loss = 3.41300\n",
      "epoch no.3 train no.278520  loss = 2.05713 avg_loss = 3.40914\n",
      "epoch no.3 train no.278530  loss = 1.82022 avg_loss = 3.34611\n",
      "epoch no.3 train no.278540  loss = 6.09999 avg_loss = 3.34355\n",
      "epoch no.3 train no.278550  loss = 3.01017 avg_loss = 3.36009\n",
      "epoch no.3 train no.278560  loss = 3.27055 avg_loss = 3.35277\n",
      "epoch no.3 train no.278570  loss = 3.45032 avg_loss = 3.39978\n",
      "epoch no.3 train no.278580  loss = 4.71934 avg_loss = 3.43198\n",
      "epoch no.3 train no.278590  loss = 3.42145 avg_loss = 3.38124\n",
      "epoch no.3 train no.278600  loss = 3.08400 avg_loss = 3.40601\n",
      "epoch no.3 train no.278610  loss = 3.01346 avg_loss = 3.43597\n",
      "epoch no.3 train no.278620  loss = 5.05844 avg_loss = 3.48024\n",
      "epoch no.3 train no.278630  loss = 4.78259 avg_loss = 3.47944\n",
      "epoch no.3 train no.278640  loss = 3.87465 avg_loss = 3.47903\n",
      "epoch no.3 train no.278650  loss = 4.30920 avg_loss = 3.44866\n",
      "epoch no.3 train no.278660  loss = 4.18587 avg_loss = 3.43929\n",
      "epoch no.3 train no.278670  loss = 3.42694 avg_loss = 3.41194\n",
      "epoch no.3 train no.278680  loss = 3.93577 avg_loss = 3.43490\n",
      "epoch no.3 train no.278690  loss = 2.61684 avg_loss = 3.40197\n",
      "epoch no.3 train no.278700  loss = 3.22829 avg_loss = 3.38439\n",
      "epoch no.3 train no.278710  loss = 2.62959 avg_loss = 3.36773\n",
      "epoch no.3 train no.278720  loss = 1.93472 avg_loss = 3.38930\n",
      "epoch no.3 train no.278730  loss = 2.05950 avg_loss = 3.37163\n",
      "epoch no.3 train no.278740  loss = 4.52454 avg_loss = 3.38877\n",
      "epoch no.3 train no.278750  loss = 6.32932 avg_loss = 3.46038\n",
      "epoch no.3 train no.278760  loss = 3.03645 avg_loss = 3.41970\n",
      "epoch no.3 train no.278770  loss = 3.27653 avg_loss = 3.41313\n",
      "epoch no.3 train no.278780  loss = 3.08005 avg_loss = 3.39005\n",
      "epoch no.3 train no.278790  loss = 3.44526 avg_loss = 3.41017\n",
      "epoch no.3 train no.278800  loss = 2.32215 avg_loss = 3.42444\n",
      "epoch no.3 train no.278810  loss = 2.44357 avg_loss = 3.44349\n",
      "epoch no.3 train no.278820  loss = 2.72881 avg_loss = 3.39654\n",
      "epoch no.3 train no.278830  loss = 2.91620 avg_loss = 3.34750\n",
      "epoch no.3 train no.278840  loss = 2.75004 avg_loss = 3.30158\n",
      "epoch no.3 train no.278850  loss = 5.38844 avg_loss = 3.24837\n",
      "epoch no.3 train no.278860  loss = 4.00013 avg_loss = 3.26189\n",
      "epoch no.3 train no.278870  loss = 4.77288 avg_loss = 3.26264\n",
      "epoch no.3 train no.278880  loss = 3.98380 avg_loss = 3.28352\n",
      "epoch no.3 train no.278890  loss = 2.09747 avg_loss = 3.23369\n",
      "epoch no.3 train no.278900  loss = 5.04375 avg_loss = 3.22513\n",
      "epoch no.3 train no.278910  loss = 5.00227 avg_loss = 3.23190\n",
      "epoch no.3 train no.278920  loss = 2.03081 avg_loss = 3.23682\n",
      "epoch no.3 train no.278930  loss = 3.81371 avg_loss = 3.26622\n",
      "epoch no.3 train no.278940  loss = 2.35977 avg_loss = 3.25311\n",
      "epoch no.3 train no.278950  loss = 3.56743 avg_loss = 3.26734\n",
      "epoch no.3 train no.278960  loss = 4.74856 avg_loss = 3.26531\n",
      "epoch no.3 train no.278970  loss = 2.92249 avg_loss = 3.29919\n",
      "epoch no.3 train no.278980  loss = 3.50461 avg_loss = 3.35669\n",
      "epoch no.3 train no.278990  loss = 3.43604 avg_loss = 3.34044\n",
      "epoch no.3 train no.279000  loss = 2.55119 avg_loss = 3.36364\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '적인', '▁노래', '</s>', '</s>']\n",
      "여름밤에 듣는 감성적인 노래들</s>\n",
      "epoch no.3 train no.279010  loss = 4.29961 avg_loss = 3.40922\n",
      "epoch no.3 train no.279020  loss = 3.48314 avg_loss = 3.39310\n",
      "epoch no.3 train no.279030  loss = 4.10260 avg_loss = 3.38273\n",
      "epoch no.3 train no.279040  loss = 3.98490 avg_loss = 3.36784\n",
      "epoch no.3 train no.279050  loss = 1.88807 avg_loss = 3.36136\n",
      "epoch no.3 train no.279060  loss = 4.59138 avg_loss = 3.35277\n",
      "epoch no.3 train no.279070  loss = 3.99591 avg_loss = 3.36174\n",
      "epoch no.3 train no.279080  loss = 1.79809 avg_loss = 3.35103\n",
      "epoch no.3 train no.279090  loss = 3.19556 avg_loss = 3.37866\n",
      "epoch no.3 train no.279100  loss = 2.78784 avg_loss = 3.36942\n",
      "epoch no.3 train no.279110  loss = 3.85044 avg_loss = 3.34381\n",
      "epoch no.3 train no.279120  loss = 4.26821 avg_loss = 3.36888\n",
      "epoch no.3 train no.279130  loss = 3.61915 avg_loss = 3.38446\n",
      "epoch no.3 train no.279140  loss = 3.00206 avg_loss = 3.41884\n",
      "epoch no.3 train no.279150  loss = 5.14563 avg_loss = 3.42210\n",
      "epoch no.3 train no.279160  loss = 2.17299 avg_loss = 3.41803\n",
      "epoch no.3 train no.279170  loss = 3.29398 avg_loss = 3.46569\n",
      "epoch no.3 train no.279180  loss = 3.39575 avg_loss = 3.47253\n",
      "epoch no.3 train no.279190  loss = 3.24942 avg_loss = 3.43152\n",
      "epoch no.3 train no.279200  loss = 3.10515 avg_loss = 3.39409\n",
      "epoch no.3 train no.279210  loss = 2.42448 avg_loss = 3.37589\n",
      "epoch no.3 train no.279220  loss = 4.11981 avg_loss = 3.42319\n",
      "epoch no.3 train no.279230  loss = 3.32623 avg_loss = 3.40250\n",
      "epoch no.3 train no.279240  loss = 3.89300 avg_loss = 3.42551\n",
      "epoch no.3 train no.279250  loss = 2.90457 avg_loss = 3.40848\n",
      "epoch no.3 train no.279260  loss = 3.20448 avg_loss = 3.37364\n",
      "epoch no.3 train no.279270  loss = 2.69797 avg_loss = 3.37632\n",
      "epoch no.3 train no.279280  loss = 2.98003 avg_loss = 3.34511\n",
      "epoch no.3 train no.279290  loss = 4.07523 avg_loss = 3.32155\n",
      "epoch no.3 train no.279300  loss = 2.53065 avg_loss = 3.27308\n",
      "epoch no.3 train no.279310  loss = 2.75455 avg_loss = 3.26805\n",
      "epoch no.3 train no.279320  loss = 2.58150 avg_loss = 3.33224\n",
      "epoch no.3 train no.279330  loss = 3.53210 avg_loss = 3.29614\n",
      "epoch no.3 train no.279340  loss = 3.75042 avg_loss = 3.31548\n",
      "epoch no.3 train no.279350  loss = 3.23407 avg_loss = 3.26020\n",
      "epoch no.3 train no.279360  loss = 3.57910 avg_loss = 3.30072\n",
      "epoch no.3 train no.279370  loss = 3.48079 avg_loss = 3.30729\n",
      "epoch no.3 train no.279380  loss = 2.97324 avg_loss = 3.29059\n",
      "epoch no.3 train no.279390  loss = 2.18577 avg_loss = 3.30865\n",
      "epoch no.3 train no.279400  loss = 3.01702 avg_loss = 3.30293\n",
      "epoch no.3 train no.279410  loss = 2.45877 avg_loss = 3.31274\n",
      "epoch no.3 train no.279420  loss = 3.83516 avg_loss = 3.27259\n",
      "epoch no.3 train no.279430  loss = 2.20628 avg_loss = 3.25240\n",
      "epoch no.3 train no.279440  loss = 3.30276 avg_loss = 3.26866\n",
      "epoch no.3 train no.279450  loss = 1.80120 avg_loss = 3.22043\n",
      "epoch no.3 train no.279460  loss = 2.38802 avg_loss = 3.29961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.279470  loss = 5.23346 avg_loss = 3.35595\n",
      "epoch no.3 train no.279480  loss = 2.49731 avg_loss = 3.36439\n",
      "epoch no.3 train no.279490  loss = 2.73000 avg_loss = 3.35407\n",
      "epoch no.3 train no.279500  loss = 3.70918 avg_loss = 3.37902\n",
      "epoch no.3 train no.279510  loss = 2.21054 avg_loss = 3.44885\n",
      "epoch no.3 train no.279520  loss = 2.93124 avg_loss = 3.41771\n",
      "epoch no.3 train no.279530  loss = 3.89553 avg_loss = 3.42791\n",
      "epoch no.3 train no.279540  loss = 3.96946 avg_loss = 3.39268\n",
      "epoch no.3 train no.279550  loss = 3.24227 avg_loss = 3.37033\n",
      "epoch no.3 train no.279560  loss = 2.36099 avg_loss = 3.32998\n",
      "epoch no.3 train no.279570  loss = 3.40644 avg_loss = 3.28649\n",
      "epoch no.3 train no.279580  loss = 3.39676 avg_loss = 3.30275\n",
      "epoch no.3 train no.279590  loss = 2.87417 avg_loss = 3.35771\n",
      "epoch no.3 train no.279600  loss = 2.24402 avg_loss = 3.38376\n",
      "epoch no.3 train no.279610  loss = 3.60067 avg_loss = 3.45697\n",
      "epoch no.3 train no.279620  loss = 5.48320 avg_loss = 3.45334\n",
      "epoch no.3 train no.279630  loss = 2.81621 avg_loss = 3.36842\n",
      "epoch no.3 train no.279640  loss = 5.27664 avg_loss = 3.33510\n",
      "epoch no.3 train no.279650  loss = 1.51169 avg_loss = 3.35417\n",
      "epoch no.3 train no.279660  loss = 2.14409 avg_loss = 3.32432\n",
      "epoch no.3 train no.279670  loss = 4.02403 avg_loss = 3.38272\n",
      "epoch no.3 train no.279680  loss = 2.40946 avg_loss = 3.38811\n",
      "epoch no.3 train no.279690  loss = 2.67772 avg_loss = 3.37645\n",
      "epoch no.3 train no.279700  loss = 3.86604 avg_loss = 3.39496\n",
      "epoch no.3 train no.279710  loss = 3.38810 avg_loss = 3.45264\n",
      "epoch no.3 train no.279720  loss = 1.83286 avg_loss = 3.43667\n",
      "epoch no.3 train no.279730  loss = 2.87401 avg_loss = 3.36010\n",
      "epoch no.3 train no.279740  loss = 3.06671 avg_loss = 3.41644\n",
      "epoch no.3 train no.279750  loss = 3.31322 avg_loss = 3.40057\n",
      "epoch no.3 train no.279760  loss = 2.17673 avg_loss = 3.39920\n",
      "epoch no.3 train no.279770  loss = 2.97660 avg_loss = 3.43102\n",
      "epoch no.3 train no.279780  loss = 4.48043 avg_loss = 3.45083\n",
      "epoch no.3 train no.279790  loss = 2.67071 avg_loss = 3.42947\n",
      "epoch no.3 train no.279800  loss = 4.99160 avg_loss = 3.43769\n",
      "epoch no.3 train no.279810  loss = 1.60714 avg_loss = 3.45543\n",
      "epoch no.3 train no.279820  loss = 5.06128 avg_loss = 3.44524\n",
      "epoch no.3 train no.279830  loss = 2.03092 avg_loss = 3.45518\n",
      "epoch no.3 train no.279840  loss = 3.08555 avg_loss = 3.43599\n",
      "epoch no.3 train no.279850  loss = 2.67369 avg_loss = 3.42593\n",
      "epoch no.3 train no.279860  loss = 3.27395 avg_loss = 3.40838\n",
      "epoch no.3 train no.279870  loss = 2.81279 avg_loss = 3.43511\n",
      "epoch no.3 train no.279880  loss = 4.43279 avg_loss = 3.42777\n",
      "epoch no.3 train no.279890  loss = 2.98729 avg_loss = 3.40871\n",
      "epoch no.3 train no.279900  loss = 2.45223 avg_loss = 3.38054\n",
      "epoch no.3 train no.279910  loss = 3.80726 avg_loss = 3.39586\n",
      "epoch no.3 train no.279920  loss = 4.89739 avg_loss = 3.44518\n",
      "epoch no.3 train no.279930  loss = 4.70876 avg_loss = 3.42416\n",
      "epoch no.3 train no.279940  loss = 4.44337 avg_loss = 3.40549\n",
      "epoch no.3 train no.279950  loss = 3.90888 avg_loss = 3.39614\n",
      "epoch no.3 train no.279960  loss = 3.58637 avg_loss = 3.40420\n",
      "epoch no.3 train no.279970  loss = 3.68752 avg_loss = 3.36609\n",
      "epoch no.3 train no.279980  loss = 3.60189 avg_loss = 3.35541\n",
      "epoch no.3 train no.279990  loss = 2.64619 avg_loss = 3.36202\n",
      "epoch no.3 train no.280000  loss = 2.35711 avg_loss = 3.36230\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁꿀', '적인', '▁노래', '송', '</s>']\n",
      "여름밤의 감성적인 팝송</s>\n",
      "epoch no.3 train no.280010  loss = 2.78013 avg_loss = 3.37055\n",
      "epoch no.3 train no.280020  loss = 2.29720 avg_loss = 3.32702\n",
      "epoch no.3 train no.280030  loss = 3.61366 avg_loss = 3.33626\n",
      "epoch no.3 train no.280040  loss = 2.11390 avg_loss = 3.33642\n",
      "epoch no.3 train no.280050  loss = 3.18937 avg_loss = 3.31867\n",
      "epoch no.3 train no.280060  loss = 4.15001 avg_loss = 3.35394\n",
      "epoch no.3 train no.280070  loss = 2.71366 avg_loss = 3.33950\n",
      "epoch no.3 train no.280080  loss = 3.11694 avg_loss = 3.31807\n",
      "epoch no.3 train no.280090  loss = 1.98578 avg_loss = 3.29987\n",
      "epoch no.3 train no.280100  loss = 2.03293 avg_loss = 3.22069\n",
      "epoch no.3 train no.280110  loss = 2.00130 avg_loss = 3.18849\n",
      "epoch no.3 train no.280120  loss = 3.54472 avg_loss = 3.17533\n",
      "epoch no.3 train no.280130  loss = 4.63031 avg_loss = 3.19720\n",
      "epoch no.3 train no.280140  loss = 3.58795 avg_loss = 3.24852\n",
      "epoch no.3 train no.280150  loss = 4.24680 avg_loss = 3.26783\n",
      "epoch no.3 train no.280160  loss = 4.15323 avg_loss = 3.23209\n",
      "epoch no.3 train no.280170  loss = 2.52283 avg_loss = 3.19325\n",
      "epoch no.3 train no.280180  loss = 3.33922 avg_loss = 3.18059\n",
      "epoch no.3 train no.280190  loss = 3.65668 avg_loss = 3.17098\n",
      "epoch no.3 train no.280200  loss = 2.20455 avg_loss = 3.19528\n",
      "epoch no.3 train no.280210  loss = 2.75029 avg_loss = 3.18032\n",
      "epoch no.3 train no.280220  loss = 2.33464 avg_loss = 3.22368\n",
      "epoch no.3 train no.280230  loss = 3.11124 avg_loss = 3.18757\n",
      "epoch no.3 train no.280240  loss = 2.54635 avg_loss = 3.19681\n",
      "epoch no.3 train no.280250  loss = 2.56374 avg_loss = 3.19642\n",
      "epoch no.3 train no.280260  loss = 5.55960 avg_loss = 3.21799\n",
      "epoch no.3 train no.280270  loss = 5.51672 avg_loss = 3.32558\n",
      "epoch no.3 train no.280280  loss = 3.29198 avg_loss = 3.34733\n",
      "epoch no.3 train no.280290  loss = 1.77490 avg_loss = 3.32990\n",
      "epoch no.3 train no.280300  loss = 3.97211 avg_loss = 3.37194\n",
      "epoch no.3 train no.280310  loss = 5.49030 avg_loss = 3.37872\n",
      "epoch no.3 train no.280320  loss = 3.23714 avg_loss = 3.38779\n",
      "epoch no.3 train no.280330  loss = 4.30230 avg_loss = 3.41307\n",
      "epoch no.3 train no.280340  loss = 3.99818 avg_loss = 3.43458\n",
      "epoch no.3 train no.280350  loss = 2.04216 avg_loss = 3.37125\n",
      "epoch no.3 train no.280360  loss = 5.11842 avg_loss = 3.36290\n",
      "epoch no.3 train no.280370  loss = 2.38345 avg_loss = 3.32230\n",
      "epoch no.3 train no.280380  loss = 3.30192 avg_loss = 3.31658\n",
      "epoch no.3 train no.280390  loss = 3.33789 avg_loss = 3.31480\n",
      "epoch no.3 train no.280400  loss = 3.87156 avg_loss = 3.32928\n",
      "epoch no.3 train no.280410  loss = 3.86159 avg_loss = 3.35961\n",
      "epoch no.3 train no.280420  loss = 3.05383 avg_loss = 3.39114\n",
      "epoch no.3 train no.280430  loss = 2.06296 avg_loss = 3.35336\n",
      "epoch no.3 train no.280440  loss = 3.67589 avg_loss = 3.39142\n",
      "epoch no.3 train no.280450  loss = 5.36365 avg_loss = 3.41385\n",
      "epoch no.3 train no.280460  loss = 1.28713 avg_loss = 3.38153\n",
      "epoch no.3 train no.280470  loss = 2.71108 avg_loss = 3.37478\n",
      "epoch no.3 train no.280480  loss = 3.07257 avg_loss = 3.38245\n",
      "epoch no.3 train no.280490  loss = 3.23476 avg_loss = 3.40767\n",
      "epoch no.3 train no.280500  loss = 2.56873 avg_loss = 3.37767\n",
      "epoch no.3 train no.280510  loss = 2.54653 avg_loss = 3.37890\n",
      "epoch no.3 train no.280520  loss = 2.22843 avg_loss = 3.39490\n",
      "epoch no.3 train no.280530  loss = 3.34521 avg_loss = 3.37047\n",
      "epoch no.3 train no.280540  loss = 1.70204 avg_loss = 3.35161\n",
      "epoch no.3 train no.280550  loss = 3.57367 avg_loss = 3.35989\n",
      "epoch no.3 train no.280560  loss = 1.89471 avg_loss = 3.32765\n",
      "epoch no.3 train no.280570  loss = 2.62008 avg_loss = 3.30643\n",
      "epoch no.3 train no.280580  loss = 2.97414 avg_loss = 3.30968\n",
      "epoch no.3 train no.280590  loss = 2.28651 avg_loss = 3.29995\n",
      "epoch no.3 train no.280600  loss = 2.68145 avg_loss = 3.34065\n",
      "epoch no.3 train no.280610  loss = 3.45708 avg_loss = 3.36901\n",
      "epoch no.3 train no.280620  loss = 3.98222 avg_loss = 3.36352\n",
      "epoch no.3 train no.280630  loss = 2.44440 avg_loss = 3.31582\n",
      "epoch no.3 train no.280640  loss = 3.01411 avg_loss = 3.31891\n",
      "epoch no.3 train no.280650  loss = 3.29139 avg_loss = 3.32959\n",
      "epoch no.3 train no.280660  loss = 2.26995 avg_loss = 3.30706\n",
      "epoch no.3 train no.280670  loss = 4.39156 avg_loss = 3.29413\n",
      "epoch no.3 train no.280680  loss = 4.52757 avg_loss = 3.31399\n",
      "epoch no.3 train no.280690  loss = 2.85000 avg_loss = 3.29136\n",
      "epoch no.3 train no.280700  loss = 3.14587 avg_loss = 3.30493\n",
      "epoch no.3 train no.280710  loss = 3.50612 avg_loss = 3.34404\n",
      "epoch no.3 train no.280720  loss = 4.56412 avg_loss = 3.36885\n",
      "epoch no.3 train no.280730  loss = 2.13435 avg_loss = 3.37562\n",
      "epoch no.3 train no.280740  loss = 2.55463 avg_loss = 3.31367\n",
      "epoch no.3 train no.280750  loss = 3.04148 avg_loss = 3.31444\n",
      "epoch no.3 train no.280760  loss = 3.64142 avg_loss = 3.27305\n",
      "epoch no.3 train no.280770  loss = 4.08506 avg_loss = 3.25838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.280780  loss = 3.60140 avg_loss = 3.27113\n",
      "epoch no.3 train no.280790  loss = 3.16968 avg_loss = 3.25598\n",
      "epoch no.3 train no.280800  loss = 3.20300 avg_loss = 3.28049\n",
      "epoch no.3 train no.280810  loss = 3.68882 avg_loss = 3.25877\n",
      "epoch no.3 train no.280820  loss = 4.25517 avg_loss = 3.27235\n",
      "epoch no.3 train no.280830  loss = 3.47339 avg_loss = 3.37137\n",
      "epoch no.3 train no.280840  loss = 1.98061 avg_loss = 3.31007\n",
      "epoch no.3 train no.280850  loss = 1.83017 avg_loss = 3.31741\n",
      "epoch no.3 train no.280860  loss = 3.44200 avg_loss = 3.32859\n",
      "epoch no.3 train no.280870  loss = 3.62213 avg_loss = 3.33274\n",
      "epoch no.3 train no.280880  loss = 2.95002 avg_loss = 3.31476\n",
      "epoch no.3 train no.280890  loss = 2.99746 avg_loss = 3.38610\n",
      "epoch no.3 train no.280900  loss = 4.03817 avg_loss = 3.38271\n",
      "epoch no.3 train no.280910  loss = 2.07421 avg_loss = 3.34771\n",
      "epoch no.3 train no.280920  loss = 2.42380 avg_loss = 3.33747\n",
      "epoch no.3 train no.280930  loss = 4.19752 avg_loss = 3.36851\n",
      "epoch no.3 train no.280940  loss = 3.02064 avg_loss = 3.39495\n",
      "epoch no.3 train no.280950  loss = 3.58040 avg_loss = 3.38174\n",
      "epoch no.3 train no.280960  loss = 2.92005 avg_loss = 3.34879\n",
      "epoch no.3 train no.280970  loss = 3.49147 avg_loss = 3.31115\n",
      "epoch no.3 train no.280980  loss = 4.02601 avg_loss = 3.36319\n",
      "epoch no.3 train no.280990  loss = 3.66026 avg_loss = 3.37343\n",
      "epoch no.3 train no.281000  loss = 2.96812 avg_loss = 3.36203\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁수', '랠', '▁노래', '적인', '▁음악', '</s>']\n",
      "여름밤을 달굴 감성적인 음악</s>\n",
      "epoch no.3 train no.281010  loss = 3.37395 avg_loss = 3.39383\n",
      "epoch no.3 train no.281020  loss = 4.33189 avg_loss = 3.40555\n",
      "epoch no.3 train no.281030  loss = 3.32192 avg_loss = 3.37829\n",
      "epoch no.3 train no.281040  loss = 3.27704 avg_loss = 3.40011\n",
      "epoch no.3 train no.281050  loss = 4.66902 avg_loss = 3.39747\n",
      "epoch no.3 train no.281060  loss = 2.29435 avg_loss = 3.36038\n",
      "epoch no.3 train no.281070  loss = 1.94336 avg_loss = 3.30286\n",
      "epoch no.3 train no.281080  loss = 2.78770 avg_loss = 3.31635\n",
      "epoch no.3 train no.281090  loss = 3.02365 avg_loss = 3.35037\n",
      "epoch no.3 train no.281100  loss = 2.48928 avg_loss = 3.34141\n",
      "epoch no.3 train no.281110  loss = 3.65690 avg_loss = 3.33359\n",
      "epoch no.3 train no.281120  loss = 3.40962 avg_loss = 3.30235\n",
      "epoch no.3 train no.281130  loss = 3.38968 avg_loss = 3.28551\n",
      "epoch no.3 train no.281140  loss = 4.19454 avg_loss = 3.32038\n",
      "epoch no.3 train no.281150  loss = 1.38527 avg_loss = 3.34392\n",
      "epoch no.3 train no.281160  loss = 3.21786 avg_loss = 3.32204\n",
      "epoch no.3 train no.281170  loss = 3.64455 avg_loss = 3.31356\n",
      "epoch no.3 train no.281180  loss = 2.36692 avg_loss = 3.30858\n",
      "epoch no.3 train no.281190  loss = 3.17667 avg_loss = 3.36734\n",
      "epoch no.3 train no.281200  loss = 3.24017 avg_loss = 3.35838\n",
      "epoch no.3 train no.281210  loss = 4.95739 avg_loss = 3.37610\n",
      "epoch no.3 train no.281220  loss = 2.74150 avg_loss = 3.37418\n",
      "epoch no.3 train no.281230  loss = 4.35280 avg_loss = 3.36334\n",
      "epoch no.3 train no.281240  loss = 3.93579 avg_loss = 3.31794\n",
      "epoch no.3 train no.281250  loss = 2.78229 avg_loss = 3.32752\n",
      "epoch no.3 train no.281260  loss = 4.02142 avg_loss = 3.29902\n",
      "epoch no.3 train no.281270  loss = 2.31263 avg_loss = 3.27668\n",
      "epoch no.3 train no.281280  loss = 4.22093 avg_loss = 3.26695\n",
      "epoch no.3 train no.281290  loss = 4.06000 avg_loss = 3.30801\n",
      "epoch no.3 train no.281300  loss = 2.09658 avg_loss = 3.28573\n",
      "epoch no.3 train no.281310  loss = 4.03396 avg_loss = 3.27873\n",
      "epoch no.3 train no.281320  loss = 2.49265 avg_loss = 3.28036\n",
      "epoch no.3 train no.281330  loss = 2.26476 avg_loss = 3.27151\n",
      "epoch no.3 train no.281340  loss = 5.05582 avg_loss = 3.28916\n",
      "epoch no.3 train no.281350  loss = 3.29157 avg_loss = 3.32507\n",
      "epoch no.3 train no.281360  loss = 2.69547 avg_loss = 3.31279\n",
      "epoch no.3 train no.281370  loss = 3.02660 avg_loss = 3.32578\n",
      "epoch no.3 train no.281380  loss = 4.49845 avg_loss = 3.33909\n",
      "epoch no.3 train no.281390  loss = 6.98653 avg_loss = 3.34715\n",
      "epoch no.3 train no.281400  loss = 3.03520 avg_loss = 3.30740\n",
      "epoch no.3 train no.281410  loss = 3.93352 avg_loss = 3.33279\n",
      "epoch no.3 train no.281420  loss = 3.27268 avg_loss = 3.28017\n",
      "epoch no.3 train no.281430  loss = 2.14510 avg_loss = 3.28215\n",
      "epoch no.3 train no.281440  loss = 4.53911 avg_loss = 3.36890\n",
      "epoch no.3 train no.281450  loss = 3.28935 avg_loss = 3.34311\n",
      "epoch no.3 train no.281460  loss = 2.49556 avg_loss = 3.32676\n",
      "epoch no.3 train no.281470  loss = 3.34661 avg_loss = 3.33605\n",
      "epoch no.3 train no.281480  loss = 4.04591 avg_loss = 3.29736\n",
      "epoch no.3 train no.281490  loss = 3.90369 avg_loss = 3.35188\n",
      "epoch no.3 train no.281500  loss = 3.26392 avg_loss = 3.41191\n",
      "epoch no.3 train no.281510  loss = 3.74688 avg_loss = 3.44158\n",
      "epoch no.3 train no.281520  loss = 3.80578 avg_loss = 3.48306\n",
      "epoch no.3 train no.281530  loss = 3.43327 avg_loss = 3.48008\n",
      "epoch no.3 train no.281540  loss = 4.29773 avg_loss = 3.48083\n",
      "epoch no.3 train no.281550  loss = 3.97048 avg_loss = 3.52919\n",
      "epoch no.3 train no.281560  loss = 4.30408 avg_loss = 3.57971\n",
      "epoch no.3 train no.281570  loss = 3.50225 avg_loss = 3.58064\n",
      "epoch no.3 train no.281580  loss = 2.75868 avg_loss = 3.56882\n",
      "epoch no.3 train no.281590  loss = 1.84941 avg_loss = 3.48003\n",
      "epoch no.3 train no.281600  loss = 4.67812 avg_loss = 3.45255\n",
      "epoch no.3 train no.281610  loss = 2.76873 avg_loss = 3.43758\n",
      "epoch no.3 train no.281620  loss = 3.96897 avg_loss = 3.39944\n",
      "epoch no.3 train no.281630  loss = 2.22306 avg_loss = 3.37907\n",
      "epoch no.3 train no.281640  loss = 2.50003 avg_loss = 3.38882\n",
      "epoch no.3 train no.281650  loss = 3.26498 avg_loss = 3.42820\n",
      "epoch no.3 train no.281660  loss = 2.90144 avg_loss = 3.46381\n",
      "epoch no.3 train no.281670  loss = 2.99200 avg_loss = 3.50191\n",
      "epoch no.3 train no.281680  loss = 3.50221 avg_loss = 3.48499\n",
      "epoch no.3 train no.281690  loss = 2.66764 avg_loss = 3.49387\n",
      "epoch no.3 train no.281700  loss = 2.47813 avg_loss = 3.46211\n",
      "epoch no.3 train no.281710  loss = 3.26227 avg_loss = 3.42727\n",
      "epoch no.3 train no.281720  loss = 3.13865 avg_loss = 3.41529\n",
      "epoch no.3 train no.281730  loss = 1.49829 avg_loss = 3.37006\n",
      "epoch no.3 train no.281740  loss = 3.55591 avg_loss = 3.36555\n",
      "epoch no.3 train no.281750  loss = 4.31957 avg_loss = 3.37587\n",
      "epoch no.3 train no.281760  loss = 3.43120 avg_loss = 3.31854\n",
      "epoch no.3 train no.281770  loss = 3.42270 avg_loss = 3.38296\n",
      "epoch no.3 train no.281780  loss = 2.47590 avg_loss = 3.34824\n",
      "epoch no.3 train no.281790  loss = 6.04995 avg_loss = 3.43854\n",
      "epoch no.3 train no.281800  loss = 4.43730 avg_loss = 3.41205\n",
      "epoch no.3 train no.281810  loss = 3.14380 avg_loss = 3.44597\n",
      "epoch no.3 train no.281820  loss = 2.64721 avg_loss = 3.43730\n",
      "epoch no.3 train no.281830  loss = 1.53493 avg_loss = 3.38755\n",
      "epoch no.3 train no.281840  loss = 3.95279 avg_loss = 3.39508\n",
      "epoch no.3 train no.281850  loss = 3.29812 avg_loss = 3.36776\n",
      "epoch no.3 train no.281860  loss = 2.45722 avg_loss = 3.30554\n",
      "epoch no.3 train no.281870  loss = 2.80414 avg_loss = 3.27436\n",
      "epoch no.3 train no.281880  loss = 3.82962 avg_loss = 3.31092\n",
      "epoch no.3 train no.281890  loss = 3.49649 avg_loss = 3.27076\n",
      "epoch no.3 train no.281900  loss = 4.92940 avg_loss = 3.29339\n",
      "epoch no.3 train no.281910  loss = 2.51532 avg_loss = 3.26777\n",
      "epoch no.3 train no.281920  loss = 2.98741 avg_loss = 3.24169\n",
      "epoch no.3 train no.281930  loss = 1.76359 avg_loss = 3.19471\n",
      "epoch no.3 train no.281940  loss = 4.14377 avg_loss = 3.19102\n",
      "epoch no.3 train no.281950  loss = 3.39666 avg_loss = 3.22220\n",
      "epoch no.3 train no.281960  loss = 5.44579 avg_loss = 3.26774\n",
      "epoch no.3 train no.281970  loss = 1.97282 avg_loss = 3.20178\n",
      "epoch no.3 train no.281980  loss = 3.97216 avg_loss = 3.20898\n",
      "epoch no.3 train no.281990  loss = 2.45599 avg_loss = 3.18529\n",
      "epoch no.3 train no.282000  loss = 2.61612 avg_loss = 3.16198\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '의', '▁그루', '을', '</s>']\n",
      "여름밤의 힐링송</s>\n",
      "epoch no.3 train no.282010  loss = 3.38738 avg_loss = 3.17358\n",
      "epoch no.3 train no.282020  loss = 2.16956 avg_loss = 3.16668\n",
      "epoch no.3 train no.282030  loss = 3.10182 avg_loss = 3.13901\n",
      "epoch no.3 train no.282040  loss = 2.28914 avg_loss = 3.11536\n",
      "epoch no.3 train no.282050  loss = 2.46964 avg_loss = 3.12474\n",
      "epoch no.3 train no.282060  loss = 3.83440 avg_loss = 3.13097\n",
      "epoch no.3 train no.282070  loss = 5.34478 avg_loss = 3.14958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.282080  loss = 2.99269 avg_loss = 3.17490\n",
      "epoch no.3 train no.282090  loss = 2.12364 avg_loss = 3.17187\n",
      "epoch no.3 train no.282100  loss = 3.53400 avg_loss = 3.17648\n",
      "epoch no.3 train no.282110  loss = 5.31452 avg_loss = 3.21103\n",
      "epoch no.3 train no.282120  loss = 2.19435 avg_loss = 3.16511\n",
      "epoch no.3 train no.282130  loss = 4.37629 avg_loss = 3.26729\n",
      "epoch no.3 train no.282140  loss = 3.14470 avg_loss = 3.29389\n",
      "epoch no.3 train no.282150  loss = 3.33789 avg_loss = 3.28965\n",
      "epoch no.3 train no.282160  loss = 4.20460 avg_loss = 3.32631\n",
      "epoch no.3 train no.282170  loss = 2.18091 avg_loss = 3.32484\n",
      "epoch no.3 train no.282180  loss = 4.59727 avg_loss = 3.33561\n",
      "epoch no.3 train no.282190  loss = 4.32420 avg_loss = 3.33765\n",
      "epoch no.3 train no.282200  loss = 2.11704 avg_loss = 3.32590\n",
      "epoch no.3 train no.282210  loss = 2.79218 avg_loss = 3.30305\n",
      "epoch no.3 train no.282220  loss = 3.43826 avg_loss = 3.31057\n",
      "epoch no.3 train no.282230  loss = 4.22090 avg_loss = 3.33525\n",
      "epoch no.3 train no.282240  loss = 3.30042 avg_loss = 3.33388\n",
      "epoch no.3 train no.282250  loss = 2.88918 avg_loss = 3.28942\n",
      "epoch no.3 train no.282260  loss = 3.45738 avg_loss = 3.29233\n",
      "epoch no.3 train no.282270  loss = 3.10004 avg_loss = 3.29409\n",
      "epoch no.3 train no.282280  loss = 4.78804 avg_loss = 3.31592\n",
      "epoch no.3 train no.282290  loss = 2.94512 avg_loss = 3.29637\n",
      "epoch no.3 train no.282300  loss = 3.18464 avg_loss = 3.30672\n",
      "epoch no.3 train no.282310  loss = 3.81656 avg_loss = 3.31348\n",
      "epoch no.3 train no.282320  loss = 2.29423 avg_loss = 3.33405\n",
      "epoch no.3 train no.282330  loss = 1.98062 avg_loss = 3.30400\n",
      "epoch no.3 train no.282340  loss = 2.80244 avg_loss = 3.30964\n",
      "epoch no.3 train no.282350  loss = 3.32882 avg_loss = 3.33585\n",
      "epoch no.3 train no.282360  loss = 3.34585 avg_loss = 3.32350\n",
      "epoch no.3 train no.282370  loss = 4.58283 avg_loss = 3.32413\n",
      "epoch no.3 train no.282380  loss = 2.86702 avg_loss = 3.34978\n",
      "epoch no.3 train no.282390  loss = 2.56653 avg_loss = 3.33881\n",
      "epoch no.3 train no.282400  loss = 2.08967 avg_loss = 3.28953\n",
      "epoch no.3 train no.282410  loss = 3.09880 avg_loss = 3.27216\n",
      "epoch no.3 train no.282420  loss = 5.10034 avg_loss = 3.33984\n",
      "epoch no.3 train no.282430  loss = 3.58630 avg_loss = 3.34056\n",
      "epoch no.3 train no.282440  loss = 2.77908 avg_loss = 3.36765\n",
      "epoch no.3 train no.282450  loss = 2.63239 avg_loss = 3.30514\n",
      "epoch no.3 train no.282460  loss = 2.50789 avg_loss = 3.24709\n",
      "epoch no.3 train no.282470  loss = 2.72525 avg_loss = 3.24447\n",
      "epoch no.3 train no.282480  loss = 2.47503 avg_loss = 3.26337\n",
      "epoch no.3 train no.282490  loss = 3.71781 avg_loss = 3.26150\n",
      "epoch no.3 train no.282500  loss = 1.65573 avg_loss = 3.25927\n",
      "epoch no.3 train no.282510  loss = 4.19803 avg_loss = 3.28693\n",
      "epoch no.3 train no.282520  loss = 3.05160 avg_loss = 3.22864\n",
      "epoch no.3 train no.282530  loss = 2.96423 avg_loss = 3.26310\n",
      "epoch no.3 train no.282540  loss = 2.05971 avg_loss = 3.27145\n",
      "epoch no.3 train no.282550  loss = 2.86197 avg_loss = 3.31819\n",
      "epoch no.3 train no.282560  loss = 2.02321 avg_loss = 3.30730\n",
      "epoch no.3 train no.282570  loss = 2.90159 avg_loss = 3.38587\n",
      "epoch no.3 train no.282580  loss = 2.79861 avg_loss = 3.35538\n",
      "epoch no.3 train no.282590  loss = 3.67724 avg_loss = 3.36509\n",
      "epoch no.3 train no.282600  loss = 2.72481 avg_loss = 3.35875\n",
      "epoch no.3 train no.282610  loss = 2.51753 avg_loss = 3.30768\n",
      "epoch no.3 train no.282620  loss = 2.23210 avg_loss = 3.32722\n",
      "epoch no.3 train no.282630  loss = 2.65127 avg_loss = 3.27649\n",
      "epoch no.3 train no.282640  loss = 2.28833 avg_loss = 3.28636\n",
      "epoch no.3 train no.282650  loss = 4.19452 avg_loss = 3.29650\n",
      "epoch no.3 train no.282660  loss = 3.87851 avg_loss = 3.28168\n",
      "epoch no.3 train no.282670  loss = 4.57316 avg_loss = 3.30938\n",
      "epoch no.3 train no.282680  loss = 2.17631 avg_loss = 3.29528\n",
      "epoch no.3 train no.282690  loss = 2.40219 avg_loss = 3.35416\n",
      "epoch no.3 train no.282700  loss = 4.57291 avg_loss = 3.38201\n",
      "epoch no.3 train no.282710  loss = 3.50725 avg_loss = 3.37510\n",
      "epoch no.3 train no.282720  loss = 2.95411 avg_loss = 3.35498\n",
      "epoch no.3 train no.282730  loss = 1.81370 avg_loss = 3.35232\n",
      "epoch no.3 train no.282740  loss = 3.30872 avg_loss = 3.33481\n",
      "epoch no.3 train no.282750  loss = 2.72169 avg_loss = 3.33205\n",
      "epoch no.3 train no.282760  loss = 4.01998 avg_loss = 3.31808\n",
      "epoch no.3 train no.282770  loss = 3.05935 avg_loss = 3.29354\n",
      "epoch no.3 train no.282780  loss = 2.83767 avg_loss = 3.38494\n",
      "epoch no.3 train no.282790  loss = 1.99394 avg_loss = 3.35422\n",
      "epoch no.3 train no.282800  loss = 3.64417 avg_loss = 3.36890\n",
      "epoch no.3 train no.282810  loss = 2.94325 avg_loss = 3.32050\n",
      "epoch no.3 train no.282820  loss = 4.54780 avg_loss = 3.32594\n",
      "epoch no.3 train no.282830  loss = 4.94526 avg_loss = 3.35688\n",
      "epoch no.3 train no.282840  loss = 2.21345 avg_loss = 3.35898\n",
      "epoch no.3 train no.282850  loss = 2.76118 avg_loss = 3.35184\n",
      "epoch no.3 train no.282860  loss = 2.66453 avg_loss = 3.34247\n",
      "epoch no.3 train no.282870  loss = 6.71221 avg_loss = 3.34734\n",
      "epoch no.3 train no.282880  loss = 4.84103 avg_loss = 3.43213\n",
      "epoch no.3 train no.282890  loss = 3.50235 avg_loss = 3.39545\n",
      "epoch no.3 train no.282900  loss = 5.31776 avg_loss = 3.40496\n",
      "epoch no.3 train no.282910  loss = 3.68479 avg_loss = 3.37175\n",
      "epoch no.3 train no.282920  loss = 3.41193 avg_loss = 3.39192\n",
      "epoch no.3 train no.282930  loss = 4.32080 avg_loss = 3.36168\n",
      "epoch no.3 train no.282940  loss = 2.85730 avg_loss = 3.40252\n",
      "epoch no.3 train no.282950  loss = 3.20777 avg_loss = 3.37724\n",
      "epoch no.3 train no.282960  loss = 4.50651 avg_loss = 3.39689\n",
      "epoch no.3 train no.282970  loss = 2.62005 avg_loss = 3.37541\n",
      "epoch no.3 train no.282980  loss = 2.12212 avg_loss = 3.39040\n",
      "epoch no.3 train no.282990  loss = 3.60144 avg_loss = 3.33557\n",
      "epoch no.3 train no.283000  loss = 3.16386 avg_loss = 3.30421\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '적인', '에이지', '</s>']\n",
      "여름밤의 감성 뉴에이지</s>\n",
      "epoch no.3 train no.283010  loss = 1.73045 avg_loss = 3.27696\n",
      "epoch no.3 train no.283020  loss = 3.92807 avg_loss = 3.27817\n",
      "epoch no.3 train no.283030  loss = 3.01936 avg_loss = 3.23826\n",
      "epoch no.3 train no.283040  loss = 4.05227 avg_loss = 3.27722\n",
      "epoch no.3 train no.283050  loss = 3.20866 avg_loss = 3.29513\n",
      "epoch no.3 train no.283060  loss = 5.45321 avg_loss = 3.38251\n",
      "epoch no.3 train no.283070  loss = 3.49341 avg_loss = 3.37228\n",
      "epoch no.3 train no.283080  loss = 4.98699 avg_loss = 3.38905\n",
      "epoch no.3 train no.283090  loss = 4.90937 avg_loss = 3.35924\n",
      "epoch no.3 train no.283100  loss = 2.72911 avg_loss = 3.35825\n",
      "epoch no.3 train no.283110  loss = 2.70339 avg_loss = 3.35756\n",
      "epoch no.3 train no.283120  loss = 5.20038 avg_loss = 3.41799\n",
      "epoch no.3 train no.283130  loss = 3.40336 avg_loss = 3.35830\n",
      "epoch no.3 train no.283140  loss = 2.09674 avg_loss = 3.37549\n",
      "epoch no.3 train no.283150  loss = 4.04490 avg_loss = 3.38054\n",
      "epoch no.3 train no.283160  loss = 2.35011 avg_loss = 3.33493\n",
      "epoch no.3 train no.283170  loss = 3.27825 avg_loss = 3.35388\n",
      "epoch no.3 train no.283180  loss = 3.76414 avg_loss = 3.39735\n",
      "epoch no.3 train no.283190  loss = 2.83644 avg_loss = 3.38654\n",
      "epoch no.3 train no.283200  loss = 2.59914 avg_loss = 3.40895\n",
      "epoch no.3 train no.283210  loss = 3.26051 avg_loss = 3.44214\n",
      "epoch no.3 train no.283220  loss = 2.89561 avg_loss = 3.40735\n",
      "epoch no.3 train no.283230  loss = 3.66735 avg_loss = 3.45750\n",
      "epoch no.3 train no.283240  loss = 3.33920 avg_loss = 3.53955\n",
      "epoch no.3 train no.283250  loss = 2.29268 avg_loss = 3.49524\n",
      "epoch no.3 train no.283260  loss = 3.16073 avg_loss = 3.45397\n",
      "epoch no.3 train no.283270  loss = 2.92166 avg_loss = 3.42672\n",
      "epoch no.3 train no.283280  loss = 4.26379 avg_loss = 3.46242\n",
      "epoch no.3 train no.283290  loss = 4.57414 avg_loss = 3.43007\n",
      "epoch no.3 train no.283300  loss = 3.94654 avg_loss = 3.40670\n",
      "epoch no.3 train no.283310  loss = 3.90851 avg_loss = 3.40507\n",
      "epoch no.3 train no.283320  loss = 2.75293 avg_loss = 3.40490\n",
      "epoch no.3 train no.283330  loss = 2.61855 avg_loss = 3.34564\n",
      "epoch no.3 train no.283340  loss = 4.37043 avg_loss = 3.35054\n",
      "epoch no.3 train no.283350  loss = 2.88495 avg_loss = 3.31997\n",
      "epoch no.3 train no.283360  loss = 2.90691 avg_loss = 3.31262\n",
      "epoch no.3 train no.283370  loss = 3.95231 avg_loss = 3.31608\n",
      "epoch no.3 train no.283380  loss = 2.43818 avg_loss = 3.27356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.283390  loss = 5.16015 avg_loss = 3.28346\n",
      "epoch no.3 train no.283400  loss = 4.80681 avg_loss = 3.35139\n",
      "epoch no.3 train no.283410  loss = 3.95393 avg_loss = 3.35586\n",
      "epoch no.3 train no.283420  loss = 3.01761 avg_loss = 3.32813\n",
      "epoch no.3 train no.283430  loss = 4.61305 avg_loss = 3.31521\n",
      "epoch no.3 train no.283440  loss = 2.66028 avg_loss = 3.30378\n",
      "epoch no.3 train no.283450  loss = 4.03038 avg_loss = 3.34072\n",
      "epoch no.3 train no.283460  loss = 4.21995 avg_loss = 3.33278\n",
      "epoch no.3 train no.283470  loss = 5.20291 avg_loss = 3.42642\n",
      "epoch no.3 train no.283480  loss = 2.49255 avg_loss = 3.39339\n",
      "epoch no.3 train no.283490  loss = 2.32515 avg_loss = 3.36850\n",
      "epoch no.3 train no.283500  loss = 2.28305 avg_loss = 3.32302\n",
      "epoch no.3 train no.283510  loss = 3.13059 avg_loss = 3.24937\n",
      "epoch no.3 train no.283520  loss = 2.94350 avg_loss = 3.27565\n",
      "epoch no.3 train no.283530  loss = 3.33321 avg_loss = 3.31182\n",
      "epoch no.3 train no.283540  loss = 2.31041 avg_loss = 3.30847\n",
      "epoch no.3 train no.283550  loss = 3.07313 avg_loss = 3.27238\n",
      "epoch no.3 train no.283560  loss = 4.95464 avg_loss = 3.30352\n",
      "epoch no.3 train no.283570  loss = 4.04352 avg_loss = 3.29299\n",
      "epoch no.3 train no.283580  loss = 4.51813 avg_loss = 3.31316\n",
      "epoch no.3 train no.283590  loss = 3.26167 avg_loss = 3.33635\n",
      "epoch no.3 train no.283600  loss = 4.77702 avg_loss = 3.38634\n",
      "epoch no.3 train no.283610  loss = 2.39286 avg_loss = 3.35106\n",
      "epoch no.3 train no.283620  loss = 4.61856 avg_loss = 3.34854\n",
      "epoch no.3 train no.283630  loss = 2.73457 avg_loss = 3.35410\n",
      "epoch no.3 train no.283640  loss = 3.62351 avg_loss = 3.33493\n",
      "epoch no.3 train no.283650  loss = 3.07912 avg_loss = 3.36943\n",
      "epoch no.3 train no.283660  loss = 2.79924 avg_loss = 3.34779\n",
      "epoch no.3 train no.283670  loss = 2.35938 avg_loss = 3.33797\n",
      "epoch no.3 train no.283680  loss = 3.36219 avg_loss = 3.29707\n",
      "epoch no.3 train no.283690  loss = 4.21770 avg_loss = 3.31872\n",
      "epoch no.3 train no.283700  loss = 3.27037 avg_loss = 3.29674\n",
      "epoch no.3 train no.283710  loss = 3.40410 avg_loss = 3.31576\n",
      "epoch no.3 train no.283720  loss = 3.62495 avg_loss = 3.32667\n",
      "epoch no.3 train no.283730  loss = 3.73399 avg_loss = 3.38061\n",
      "epoch no.3 train no.283740  loss = 2.30112 avg_loss = 3.43110\n",
      "epoch no.3 train no.283750  loss = 2.33200 avg_loss = 3.39467\n",
      "epoch no.3 train no.283760  loss = 4.54836 avg_loss = 3.41796\n",
      "epoch no.3 train no.283770  loss = 3.31328 avg_loss = 3.41946\n",
      "epoch no.3 train no.283780  loss = 5.26639 avg_loss = 3.41651\n",
      "epoch no.3 train no.283790  loss = 3.42761 avg_loss = 3.45134\n",
      "epoch no.3 train no.283800  loss = 2.65416 avg_loss = 3.44144\n",
      "epoch no.3 train no.283810  loss = 3.22763 avg_loss = 3.42798\n",
      "epoch no.3 train no.283820  loss = 4.29975 avg_loss = 3.43353\n",
      "epoch no.3 train no.283830  loss = 4.32192 avg_loss = 3.40163\n",
      "epoch no.3 train no.283840  loss = 2.78583 avg_loss = 3.39811\n",
      "epoch no.3 train no.283850  loss = 1.83305 avg_loss = 3.38989\n",
      "epoch no.3 train no.283860  loss = 3.82516 avg_loss = 3.39476\n",
      "epoch no.3 train no.283870  loss = 4.14649 avg_loss = 3.42487\n",
      "epoch no.3 train no.283880  loss = 4.89673 avg_loss = 3.44201\n",
      "epoch no.3 train no.283890  loss = 4.01717 avg_loss = 3.39169\n",
      "epoch no.3 train no.283900  loss = 4.03701 avg_loss = 3.46081\n",
      "epoch no.3 train no.283910  loss = 2.46555 avg_loss = 3.45646\n",
      "epoch no.3 train no.283920  loss = 3.59981 avg_loss = 3.45451\n",
      "epoch no.3 train no.283930  loss = 3.13062 avg_loss = 3.42888\n",
      "epoch no.3 train no.283940  loss = 3.92213 avg_loss = 3.40703\n",
      "epoch no.3 train no.283950  loss = 2.49153 avg_loss = 3.41100\n",
      "epoch no.3 train no.283960  loss = 2.28071 avg_loss = 3.35889\n",
      "epoch no.3 train no.283970  loss = 5.04283 avg_loss = 3.37426\n",
      "epoch no.3 train no.283980  loss = 3.10218 avg_loss = 3.39901\n",
      "epoch no.3 train no.283990  loss = 3.57111 avg_loss = 3.39720\n",
      "epoch no.3 train no.284000  loss = 3.18009 avg_loss = 3.43230\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '</s>']\n",
      "여름밤에 어울리는 재즈</s>\n",
      "epoch no.3 train no.284010  loss = 5.33554 avg_loss = 3.44202\n",
      "epoch no.3 train no.284020  loss = 2.94472 avg_loss = 3.42292\n",
      "epoch no.3 train no.284030  loss = 4.41431 avg_loss = 3.45308\n",
      "epoch no.3 train no.284040  loss = 2.67475 avg_loss = 3.37973\n",
      "epoch no.3 train no.284050  loss = 3.00738 avg_loss = 3.40467\n",
      "epoch no.3 train no.284060  loss = 5.50423 avg_loss = 3.37443\n",
      "epoch no.3 train no.284070  loss = 1.72456 avg_loss = 3.36805\n",
      "epoch no.3 train no.284080  loss = 3.87309 avg_loss = 3.41927\n",
      "epoch no.3 train no.284090  loss = 4.53764 avg_loss = 3.39805\n",
      "epoch no.3 train no.284100  loss = 2.84715 avg_loss = 3.37009\n",
      "epoch no.3 train no.284110  loss = 2.33374 avg_loss = 3.37267\n",
      "epoch no.3 train no.284120  loss = 1.92660 avg_loss = 3.35697\n",
      "epoch no.3 train no.284130  loss = 2.29843 avg_loss = 3.34645\n",
      "epoch no.3 train no.284140  loss = 1.84959 avg_loss = 3.34604\n",
      "epoch no.3 train no.284150  loss = 4.37832 avg_loss = 3.42513\n",
      "epoch no.3 train no.284160  loss = 4.88366 avg_loss = 3.47107\n",
      "epoch no.3 train no.284170  loss = 3.01430 avg_loss = 3.51942\n",
      "epoch no.3 train no.284180  loss = 5.61781 avg_loss = 3.54022\n",
      "epoch no.3 train no.284190  loss = 3.41792 avg_loss = 3.52543\n",
      "epoch no.3 train no.284200  loss = 2.86641 avg_loss = 3.48743\n",
      "epoch no.3 train no.284210  loss = 4.26570 avg_loss = 3.52561\n",
      "epoch no.3 train no.284220  loss = 2.61954 avg_loss = 3.53507\n",
      "epoch no.3 train no.284230  loss = 4.75633 avg_loss = 3.57179\n",
      "epoch no.3 train no.284240  loss = 3.88069 avg_loss = 3.54809\n",
      "epoch no.3 train no.284250  loss = 3.53882 avg_loss = 3.55349\n",
      "epoch no.3 train no.284260  loss = 1.57632 avg_loss = 3.56686\n",
      "epoch no.3 train no.284270  loss = 3.14916 avg_loss = 3.54044\n",
      "epoch no.3 train no.284280  loss = 2.01263 avg_loss = 3.52487\n",
      "epoch no.3 train no.284290  loss = 3.09774 avg_loss = 3.52652\n",
      "epoch no.3 train no.284300  loss = 2.73408 avg_loss = 3.49568\n",
      "epoch no.3 train no.284310  loss = 2.71457 avg_loss = 3.50213\n",
      "epoch no.3 train no.284320  loss = 3.56526 avg_loss = 3.53434\n",
      "epoch no.3 train no.284330  loss = 1.22313 avg_loss = 3.51998\n",
      "epoch no.3 train no.284340  loss = 4.10498 avg_loss = 3.51324\n",
      "epoch no.3 train no.284350  loss = 4.99603 avg_loss = 3.53218\n",
      "epoch no.3 train no.284360  loss = 3.47583 avg_loss = 3.52544\n",
      "epoch no.3 train no.284370  loss = 2.14518 avg_loss = 3.52870\n",
      "epoch no.3 train no.284380  loss = 4.29733 avg_loss = 3.53514\n",
      "epoch no.3 train no.284390  loss = 2.22422 avg_loss = 3.46288\n",
      "epoch no.3 train no.284400  loss = 2.79631 avg_loss = 3.43882\n",
      "epoch no.3 train no.284410  loss = 3.55335 avg_loss = 3.44761\n",
      "epoch no.3 train no.284420  loss = 4.24433 avg_loss = 3.47968\n",
      "epoch no.3 train no.284430  loss = 4.49309 avg_loss = 3.51047\n",
      "epoch no.3 train no.284440  loss = 4.77088 avg_loss = 3.50892\n",
      "epoch no.3 train no.284450  loss = 4.78933 avg_loss = 3.55809\n",
      "epoch no.3 train no.284460  loss = 3.89866 avg_loss = 3.58587\n",
      "epoch no.3 train no.284470  loss = 3.35403 avg_loss = 3.54169\n",
      "epoch no.3 train no.284480  loss = 3.47473 avg_loss = 3.52763\n",
      "epoch no.3 train no.284490  loss = 3.15709 avg_loss = 3.52280\n",
      "epoch no.3 train no.284500  loss = 3.41165 avg_loss = 3.56926\n",
      "epoch no.3 train no.284510  loss = 2.75177 avg_loss = 3.50244\n",
      "epoch no.3 train no.284520  loss = 4.35303 avg_loss = 3.46771\n",
      "epoch no.3 train no.284530  loss = 2.40601 avg_loss = 3.51300\n",
      "epoch no.3 train no.284540  loss = 3.76645 avg_loss = 3.52684\n",
      "epoch no.3 train no.284550  loss = 2.09854 avg_loss = 3.47054\n",
      "epoch no.3 train no.284560  loss = 2.70767 avg_loss = 3.45524\n",
      "epoch no.3 train no.284570  loss = 4.04132 avg_loss = 3.46946\n",
      "epoch no.3 train no.284580  loss = 3.42656 avg_loss = 3.43156\n",
      "epoch no.3 train no.284590  loss = 3.57541 avg_loss = 3.44153\n",
      "epoch no.3 train no.284600  loss = 5.15539 avg_loss = 3.44320\n",
      "epoch no.3 train no.284610  loss = 2.20981 avg_loss = 3.39141\n",
      "epoch no.3 train no.284620  loss = 2.28174 avg_loss = 3.35216\n",
      "epoch no.3 train no.284630  loss = 5.17613 avg_loss = 3.37845\n",
      "epoch no.3 train no.284640  loss = 4.31135 avg_loss = 3.37980\n",
      "epoch no.3 train no.284650  loss = 2.33745 avg_loss = 3.34515\n",
      "epoch no.3 train no.284660  loss = 2.96123 avg_loss = 3.31037\n",
      "epoch no.3 train no.284670  loss = 4.90927 avg_loss = 3.33499\n",
      "epoch no.3 train no.284680  loss = 4.98326 avg_loss = 3.34161\n",
      "epoch no.3 train no.284690  loss = 5.63700 avg_loss = 3.36537\n",
      "epoch no.3 train no.284700  loss = 3.60210 avg_loss = 3.33941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.284710  loss = 3.82243 avg_loss = 3.37565\n",
      "epoch no.3 train no.284720  loss = 3.37321 avg_loss = 3.34045\n",
      "epoch no.3 train no.284730  loss = 5.88389 avg_loss = 3.34673\n",
      "epoch no.3 train no.284740  loss = 3.68055 avg_loss = 3.33249\n",
      "epoch no.3 train no.284750  loss = 2.91413 avg_loss = 3.34162\n",
      "epoch no.3 train no.284760  loss = 3.61377 avg_loss = 3.37884\n",
      "epoch no.3 train no.284770  loss = 2.80409 avg_loss = 3.36960\n",
      "epoch no.3 train no.284780  loss = 4.09036 avg_loss = 3.34020\n",
      "epoch no.3 train no.284790  loss = 3.14161 avg_loss = 3.37347\n",
      "epoch no.3 train no.284800  loss = 4.65967 avg_loss = 3.39632\n",
      "epoch no.3 train no.284810  loss = 4.31595 avg_loss = 3.36020\n",
      "epoch no.3 train no.284820  loss = 4.12525 avg_loss = 3.37183\n",
      "epoch no.3 train no.284830  loss = 2.48181 avg_loss = 3.38023\n",
      "epoch no.3 train no.284840  loss = 3.53909 avg_loss = 3.35838\n",
      "epoch no.3 train no.284850  loss = 3.13091 avg_loss = 3.31893\n",
      "epoch no.3 train no.284860  loss = 4.57635 avg_loss = 3.33527\n",
      "epoch no.3 train no.284870  loss = 2.90915 avg_loss = 3.31174\n",
      "epoch no.3 train no.284880  loss = 3.89782 avg_loss = 3.31955\n",
      "epoch no.3 train no.284890  loss = 4.41845 avg_loss = 3.34496\n",
      "epoch no.3 train no.284900  loss = 1.57413 avg_loss = 3.36274\n",
      "epoch no.3 train no.284910  loss = 4.06022 avg_loss = 3.41196\n",
      "epoch no.3 train no.284920  loss = 5.53435 avg_loss = 3.40229\n",
      "epoch no.3 train no.284930  loss = 3.10731 avg_loss = 3.43921\n",
      "epoch no.3 train no.284940  loss = 3.98925 avg_loss = 3.41001\n",
      "epoch no.3 train no.284950  loss = 2.29012 avg_loss = 3.36988\n",
      "epoch no.3 train no.284960  loss = 3.56122 avg_loss = 3.35962\n",
      "epoch no.3 train no.284970  loss = 2.13730 avg_loss = 3.35278\n",
      "epoch no.3 train no.284980  loss = 2.05025 avg_loss = 3.29276\n",
      "epoch no.3 train no.284990  loss = 3.42406 avg_loss = 3.30223\n",
      "epoch no.3 train no.285000  loss = 4.93723 avg_loss = 3.31280\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁수', '놓을', '▁감성', '적인', '▁노래', '</s>']\n",
      "여름밤을 수놓을 감성적인 음악</s>\n",
      "epoch no.3 train no.285010  loss = 2.67020 avg_loss = 3.28625\n",
      "epoch no.3 train no.285020  loss = 2.04039 avg_loss = 3.21885\n",
      "epoch no.3 train no.285030  loss = 2.27484 avg_loss = 3.21095\n",
      "epoch no.3 train no.285040  loss = 3.70138 avg_loss = 3.18465\n",
      "epoch no.3 train no.285050  loss = 3.36771 avg_loss = 3.14893\n",
      "epoch no.3 train no.285060  loss = 2.13253 avg_loss = 3.20616\n",
      "epoch no.3 train no.285070  loss = 3.20101 avg_loss = 3.24502\n",
      "epoch no.3 train no.285080  loss = 1.93363 avg_loss = 3.21174\n",
      "epoch no.3 train no.285090  loss = 3.57524 avg_loss = 3.23163\n",
      "epoch no.3 train no.285100  loss = 1.55334 avg_loss = 3.23154\n",
      "epoch no.3 train no.285110  loss = 4.66058 avg_loss = 3.23487\n",
      "epoch no.3 train no.285120  loss = 2.79175 avg_loss = 3.24179\n",
      "epoch no.3 train no.285130  loss = 4.17711 avg_loss = 3.25724\n",
      "epoch no.3 train no.285140  loss = 3.34803 avg_loss = 3.27415\n",
      "epoch no.3 train no.285150  loss = 3.09985 avg_loss = 3.26581\n",
      "epoch no.3 train no.285160  loss = 2.41188 avg_loss = 3.26300\n",
      "epoch no.3 train no.285170  loss = 3.40674 avg_loss = 3.21054\n",
      "epoch no.3 train no.285180  loss = 2.57853 avg_loss = 3.23568\n",
      "epoch no.3 train no.285190  loss = 3.17009 avg_loss = 3.25079\n",
      "epoch no.3 train no.285200  loss = 2.20032 avg_loss = 3.29210\n",
      "epoch no.3 train no.285210  loss = 4.38392 avg_loss = 3.31904\n",
      "epoch no.3 train no.285220  loss = 4.46945 avg_loss = 3.30983\n",
      "epoch no.3 train no.285230  loss = 3.20891 avg_loss = 3.31710\n",
      "epoch no.3 train no.285240  loss = 3.60250 avg_loss = 3.33379\n",
      "epoch no.3 train no.285250  loss = 2.81530 avg_loss = 3.31709\n",
      "epoch no.3 train no.285260  loss = 2.88164 avg_loss = 3.26020\n",
      "epoch no.3 train no.285270  loss = 2.99092 avg_loss = 3.27939\n",
      "epoch no.3 train no.285280  loss = 2.29035 avg_loss = 3.24371\n",
      "epoch no.3 train no.285290  loss = 3.77453 avg_loss = 3.23568\n",
      "epoch no.3 train no.285300  loss = 3.19615 avg_loss = 3.23391\n",
      "epoch no.3 train no.285310  loss = 3.62727 avg_loss = 3.22907\n",
      "epoch no.3 train no.285320  loss = 4.55323 avg_loss = 3.24629\n",
      "epoch no.3 train no.285330  loss = 2.44960 avg_loss = 3.25717\n",
      "epoch no.3 train no.285340  loss = 2.68820 avg_loss = 3.25550\n",
      "epoch no.3 train no.285350  loss = 2.63284 avg_loss = 3.27901\n",
      "epoch no.3 train no.285360  loss = 2.39328 avg_loss = 3.24444\n",
      "epoch no.3 train no.285370  loss = 1.57611 avg_loss = 3.22166\n",
      "epoch no.3 train no.285380  loss = 3.37873 avg_loss = 3.21871\n",
      "epoch no.3 train no.285390  loss = 4.66148 avg_loss = 3.25206\n",
      "epoch no.3 train no.285400  loss = 2.39375 avg_loss = 3.23416\n",
      "epoch no.3 train no.285410  loss = 3.36563 avg_loss = 3.31081\n",
      "epoch no.3 train no.285420  loss = 1.96569 avg_loss = 3.31360\n",
      "epoch no.3 train no.285430  loss = 3.46300 avg_loss = 3.27187\n",
      "epoch no.3 train no.285440  loss = 3.43761 avg_loss = 3.27947\n",
      "epoch no.3 train no.285450  loss = 4.81803 avg_loss = 3.33505\n",
      "epoch no.3 train no.285460  loss = 3.95547 avg_loss = 3.28346\n",
      "epoch no.3 train no.285470  loss = 4.13893 avg_loss = 3.27303\n",
      "epoch no.3 train no.285480  loss = 2.54738 avg_loss = 3.25952\n",
      "epoch no.3 train no.285490  loss = 3.65743 avg_loss = 3.28190\n",
      "epoch no.3 train no.285500  loss = 5.63628 avg_loss = 3.31864\n",
      "epoch no.3 train no.285510  loss = 4.27869 avg_loss = 3.34508\n",
      "epoch no.3 train no.285520  loss = 4.09343 avg_loss = 3.31976\n",
      "epoch no.3 train no.285530  loss = 5.08043 avg_loss = 3.32383\n",
      "epoch no.3 train no.285540  loss = 5.20159 avg_loss = 3.35208\n",
      "epoch no.3 train no.285550  loss = 2.91931 avg_loss = 3.35526\n",
      "epoch no.3 train no.285560  loss = 3.39009 avg_loss = 3.34417\n",
      "epoch no.3 train no.285570  loss = 3.72953 avg_loss = 3.37198\n",
      "epoch no.3 train no.285580  loss = 5.78618 avg_loss = 3.34929\n",
      "epoch no.3 train no.285590  loss = 3.46755 avg_loss = 3.32261\n",
      "epoch no.3 train no.285600  loss = 2.35623 avg_loss = 3.33169\n",
      "epoch no.3 train no.285610  loss = 3.56141 avg_loss = 3.31875\n",
      "epoch no.3 train no.285620  loss = 4.04088 avg_loss = 3.38301\n",
      "epoch no.3 train no.285630  loss = 3.67146 avg_loss = 3.38668\n",
      "epoch no.3 train no.285640  loss = 5.33598 avg_loss = 3.36630\n",
      "epoch no.3 train no.285650  loss = 3.69521 avg_loss = 3.39159\n",
      "epoch no.3 train no.285660  loss = 3.56543 avg_loss = 3.38510\n",
      "epoch no.3 train no.285670  loss = 5.52594 avg_loss = 3.39628\n",
      "epoch no.3 train no.285680  loss = 4.82868 avg_loss = 3.42703\n",
      "epoch no.3 train no.285690  loss = 2.92024 avg_loss = 3.41075\n",
      "epoch no.3 train no.285700  loss = 2.31100 avg_loss = 3.39613\n",
      "epoch no.3 train no.285710  loss = 4.98296 avg_loss = 3.40755\n",
      "epoch no.3 train no.285720  loss = 2.16437 avg_loss = 3.35949\n",
      "epoch no.3 train no.285730  loss = 2.79533 avg_loss = 3.34243\n",
      "epoch no.3 train no.285740  loss = 2.71398 avg_loss = 3.30187\n",
      "epoch no.3 train no.285750  loss = 2.60614 avg_loss = 3.32503\n",
      "epoch no.3 train no.285760  loss = 3.35780 avg_loss = 3.31019\n",
      "epoch no.3 train no.285770  loss = 1.77060 avg_loss = 3.32054\n",
      "epoch no.3 train no.285780  loss = 2.04115 avg_loss = 3.28663\n",
      "epoch no.3 train no.285790  loss = 5.04600 avg_loss = 3.28682\n",
      "epoch no.3 train no.285800  loss = 2.56572 avg_loss = 3.36403\n",
      "epoch no.3 train no.285810  loss = 2.05822 avg_loss = 3.34398\n",
      "epoch no.3 train no.285820  loss = 2.40006 avg_loss = 3.35824\n",
      "epoch no.3 train no.285830  loss = 2.81812 avg_loss = 3.35412\n",
      "epoch no.3 train no.285840  loss = 1.72657 avg_loss = 3.36352\n",
      "epoch no.3 train no.285850  loss = 3.78587 avg_loss = 3.33490\n",
      "epoch no.3 train no.285860  loss = 3.97982 avg_loss = 3.30176\n",
      "epoch no.3 train no.285870  loss = 3.07125 avg_loss = 3.33720\n",
      "epoch no.3 train no.285880  loss = 2.67425 avg_loss = 3.32752\n",
      "epoch no.3 train no.285890  loss = 3.88021 avg_loss = 3.35540\n",
      "epoch no.3 train no.285900  loss = 3.95168 avg_loss = 3.36310\n",
      "epoch no.3 train no.285910  loss = 2.46958 avg_loss = 3.37188\n",
      "epoch no.3 train no.285920  loss = 3.11499 avg_loss = 3.35750\n",
      "epoch no.3 train no.285930  loss = 2.42705 avg_loss = 3.38948\n",
      "epoch no.3 train no.285940  loss = 6.24410 avg_loss = 3.41790\n",
      "epoch no.3 train no.285950  loss = 4.33934 avg_loss = 3.36857\n",
      "epoch no.3 train no.285960  loss = 3.00002 avg_loss = 3.42561\n",
      "epoch no.3 train no.285970  loss = 3.83948 avg_loss = 3.40072\n",
      "epoch no.3 train no.285980  loss = 1.74437 avg_loss = 3.41492\n",
      "epoch no.3 train no.285990  loss = 5.02227 avg_loss = 3.40903\n",
      "epoch no.3 train no.286000  loss = 3.09468 avg_loss = 3.38304\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '에', '▁꿀', '렘', '을', '▁담은', '▁재즈', '리스트', '</s>']\n",
      "여름밤의 설렘을 담은 플레이리스트</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.286010  loss = 3.68909 avg_loss = 3.42068\n",
      "epoch no.3 train no.286020  loss = 2.74877 avg_loss = 3.40753\n",
      "epoch no.3 train no.286030  loss = 1.73692 avg_loss = 3.39428\n",
      "epoch no.3 train no.286040  loss = 1.48636 avg_loss = 3.37777\n",
      "epoch no.3 train no.286050  loss = 2.63319 avg_loss = 3.37068\n",
      "epoch no.3 train no.286060  loss = 2.94591 avg_loss = 3.33587\n",
      "epoch no.3 train no.286070  loss = 2.60555 avg_loss = 3.30555\n",
      "epoch no.3 train no.286080  loss = 2.79077 avg_loss = 3.30205\n",
      "epoch no.3 train no.286090  loss = 2.77418 avg_loss = 3.31709\n",
      "epoch no.3 train no.286100  loss = 2.42241 avg_loss = 3.32304\n",
      "epoch no.3 train no.286110  loss = 2.73901 avg_loss = 3.29632\n",
      "epoch no.3 train no.286120  loss = 2.88814 avg_loss = 3.27676\n",
      "epoch no.3 train no.286130  loss = 6.01272 avg_loss = 3.27680\n",
      "epoch no.3 train no.286140  loss = 5.12796 avg_loss = 3.32092\n",
      "epoch no.3 train no.286150  loss = 2.57100 avg_loss = 3.29503\n",
      "epoch no.3 train no.286160  loss = 4.08980 avg_loss = 3.29096\n",
      "epoch no.3 train no.286170  loss = 2.50843 avg_loss = 3.27135\n",
      "epoch no.3 train no.286180  loss = 2.76817 avg_loss = 3.30002\n",
      "epoch no.3 train no.286190  loss = 2.41131 avg_loss = 3.36099\n",
      "epoch no.3 train no.286200  loss = 3.79601 avg_loss = 3.33503\n",
      "epoch no.3 train no.286210  loss = 4.54184 avg_loss = 3.35023\n",
      "epoch no.3 train no.286220  loss = 3.87844 avg_loss = 3.37122\n",
      "epoch no.3 train no.286230  loss = 3.15384 avg_loss = 3.35209\n",
      "epoch no.3 train no.286240  loss = 2.15146 avg_loss = 3.33471\n",
      "epoch no.3 train no.286250  loss = 3.88143 avg_loss = 3.33982\n",
      "epoch no.3 train no.286260  loss = 3.08916 avg_loss = 3.35349\n",
      "epoch no.3 train no.286270  loss = 3.06192 avg_loss = 3.35609\n",
      "epoch no.3 train no.286280  loss = 2.94368 avg_loss = 3.35239\n",
      "epoch no.3 train no.286290  loss = 4.14015 avg_loss = 3.36749\n",
      "epoch no.3 train no.286300  loss = 4.64450 avg_loss = 3.42285\n",
      "epoch no.3 train no.286310  loss = 2.92857 avg_loss = 3.39081\n",
      "epoch no.3 train no.286320  loss = 4.66508 avg_loss = 3.40826\n",
      "epoch no.3 train no.286330  loss = 2.21393 avg_loss = 3.37403\n",
      "epoch no.3 train no.286340  loss = 2.76038 avg_loss = 3.35464\n",
      "epoch no.3 train no.286350  loss = 2.97863 avg_loss = 3.33742\n",
      "epoch no.3 train no.286360  loss = 1.12350 avg_loss = 3.27755\n",
      "epoch no.3 train no.286370  loss = 2.92915 avg_loss = 3.30527\n",
      "epoch no.3 train no.286380  loss = 3.49343 avg_loss = 3.32510\n",
      "epoch no.3 train no.286390  loss = 4.54740 avg_loss = 3.40215\n",
      "epoch no.3 train no.286400  loss = 2.91171 avg_loss = 3.40750\n",
      "epoch no.3 train no.286410  loss = 3.53193 avg_loss = 3.41049\n",
      "epoch no.3 train no.286420  loss = 4.79354 avg_loss = 3.39823\n",
      "epoch no.3 train no.286430  loss = 2.11206 avg_loss = 3.39998\n",
      "epoch no.3 train no.286440  loss = 2.84599 avg_loss = 3.39023\n",
      "epoch no.3 train no.286450  loss = 5.44154 avg_loss = 3.38296\n",
      "epoch no.3 train no.286460  loss = 2.58890 avg_loss = 3.37380\n",
      "epoch no.3 train no.286470  loss = 3.80723 avg_loss = 3.41154\n",
      "epoch no.3 train no.286480  loss = 2.17865 avg_loss = 3.42109\n",
      "epoch no.3 train no.286490  loss = 3.05842 avg_loss = 3.38139\n",
      "epoch no.3 train no.286500  loss = 3.78945 avg_loss = 3.36333\n",
      "epoch no.3 train no.286510  loss = 4.78278 avg_loss = 3.32716\n",
      "epoch no.3 train no.286520  loss = 4.76011 avg_loss = 3.31826\n",
      "epoch no.3 train no.286530  loss = 3.20033 avg_loss = 3.34352\n",
      "epoch no.3 train no.286540  loss = 3.47502 avg_loss = 3.39068\n",
      "epoch no.3 train no.286550  loss = 3.50884 avg_loss = 3.37286\n",
      "epoch no.3 train no.286560  loss = 3.73334 avg_loss = 3.40217\n",
      "epoch no.3 train no.286570  loss = 3.87895 avg_loss = 3.42987\n",
      "epoch no.3 train no.286580  loss = 5.34079 avg_loss = 3.45034\n",
      "epoch no.3 train no.286590  loss = 2.18209 avg_loss = 3.46048\n",
      "epoch no.3 train no.286600  loss = 2.45211 avg_loss = 3.47201\n",
      "epoch no.3 train no.286610  loss = 4.90330 avg_loss = 3.45192\n",
      "epoch no.3 train no.286620  loss = 3.30763 avg_loss = 3.43975\n",
      "epoch no.3 train no.286630  loss = 3.53716 avg_loss = 3.45735\n",
      "epoch no.3 train no.286640  loss = 4.00475 avg_loss = 3.45422\n",
      "epoch no.3 train no.286650  loss = 4.30116 avg_loss = 3.42369\n",
      "epoch no.3 train no.286660  loss = 3.73072 avg_loss = 3.42098\n",
      "epoch no.3 train no.286670  loss = 3.64356 avg_loss = 3.40520\n",
      "epoch no.3 train no.286680  loss = 3.20832 avg_loss = 3.41313\n",
      "epoch no.3 train no.286690  loss = 3.52917 avg_loss = 3.40951\n",
      "epoch no.3 train no.286700  loss = 4.14424 avg_loss = 3.38939\n",
      "epoch no.3 train no.286710  loss = 5.01623 avg_loss = 3.39623\n",
      "epoch no.3 train no.286720  loss = 2.76638 avg_loss = 3.39433\n",
      "epoch no.3 train no.286730  loss = 5.95587 avg_loss = 3.43112\n",
      "epoch no.3 train no.286740  loss = 3.14901 avg_loss = 3.44129\n",
      "epoch no.3 train no.286750  loss = 5.30410 avg_loss = 3.44724\n",
      "epoch no.3 train no.286760  loss = 3.19737 avg_loss = 3.44123\n",
      "epoch no.3 train no.286770  loss = 1.52312 avg_loss = 3.44080\n",
      "epoch no.3 train no.286780  loss = 2.44186 avg_loss = 3.40299\n",
      "epoch no.3 train no.286790  loss = 3.65398 avg_loss = 3.43745\n",
      "epoch no.3 train no.286800  loss = 4.29989 avg_loss = 3.43498\n",
      "epoch no.3 train no.286810  loss = 2.97324 avg_loss = 3.40335\n",
      "epoch no.3 train no.286820  loss = 1.81665 avg_loss = 3.37842\n",
      "epoch no.3 train no.286830  loss = 1.94844 avg_loss = 3.33810\n",
      "epoch no.3 train no.286840  loss = 2.68897 avg_loss = 3.31373\n",
      "epoch no.3 train no.286850  loss = 3.22839 avg_loss = 3.28079\n",
      "epoch no.3 train no.286860  loss = 2.26357 avg_loss = 3.24597\n",
      "epoch no.3 train no.286870  loss = 3.31497 avg_loss = 3.18893\n",
      "epoch no.3 train no.286880  loss = 3.86320 avg_loss = 3.22902\n",
      "epoch no.3 train no.286890  loss = 6.68728 avg_loss = 3.27684\n",
      "epoch no.3 train no.286900  loss = 4.00220 avg_loss = 3.34016\n",
      "epoch no.3 train no.286910  loss = 2.33941 avg_loss = 3.31011\n",
      "epoch no.3 train no.286920  loss = 3.22394 avg_loss = 3.32552\n",
      "epoch no.3 train no.286930  loss = 3.70707 avg_loss = 3.28932\n",
      "epoch no.3 train no.286940  loss = 2.19209 avg_loss = 3.29699\n",
      "epoch no.3 train no.286950  loss = 3.95215 avg_loss = 3.33497\n",
      "epoch no.3 train no.286960  loss = 1.81880 avg_loss = 3.41118\n",
      "epoch no.3 train no.286970  loss = 2.80938 avg_loss = 3.43466\n",
      "epoch no.3 train no.286980  loss = 2.29278 avg_loss = 3.37869\n",
      "epoch no.3 train no.286990  loss = 4.62436 avg_loss = 3.37888\n",
      "epoch no.3 train no.287000  loss = 3.84540 avg_loss = 3.39897\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁채워', '놓은', '▁별', '들', '</s>']\n",
      "여름밤을 수놓은 음악들</s>\n",
      "epoch no.3 train no.287010  loss = 4.17334 avg_loss = 3.41087\n",
      "epoch no.3 train no.287020  loss = 3.34099 avg_loss = 3.38690\n",
      "epoch no.3 train no.287030  loss = 3.20416 avg_loss = 3.38418\n",
      "epoch no.3 train no.287040  loss = 2.70047 avg_loss = 3.36499\n",
      "epoch no.3 train no.287050  loss = 3.06120 avg_loss = 3.37102\n",
      "epoch no.3 train no.287060  loss = 4.27216 avg_loss = 3.37466\n",
      "epoch no.3 train no.287070  loss = 4.70347 avg_loss = 3.43166\n",
      "epoch no.3 train no.287080  loss = 2.74750 avg_loss = 3.45886\n",
      "epoch no.3 train no.287090  loss = 2.69883 avg_loss = 3.51297\n",
      "epoch no.3 train no.287100  loss = 4.83296 avg_loss = 3.54983\n",
      "epoch no.3 train no.287110  loss = 2.41300 avg_loss = 3.51875\n",
      "epoch no.3 train no.287120  loss = 3.81546 avg_loss = 3.49269\n",
      "epoch no.3 train no.287130  loss = 4.30746 avg_loss = 3.51942\n",
      "epoch no.3 train no.287140  loss = 3.28808 avg_loss = 3.48179\n",
      "epoch no.3 train no.287150  loss = 3.97505 avg_loss = 3.39475\n",
      "epoch no.3 train no.287160  loss = 3.96224 avg_loss = 3.41840\n",
      "epoch no.3 train no.287170  loss = 4.08722 avg_loss = 3.46959\n",
      "epoch no.3 train no.287180  loss = 3.12723 avg_loss = 3.42182\n",
      "epoch no.3 train no.287190  loss = 3.39246 avg_loss = 3.42633\n",
      "epoch no.3 train no.287200  loss = 3.61775 avg_loss = 3.43059\n",
      "epoch no.3 train no.287210  loss = 4.12589 avg_loss = 3.45467\n",
      "epoch no.3 train no.287220  loss = 4.22609 avg_loss = 3.48885\n",
      "epoch no.3 train no.287230  loss = 2.41882 avg_loss = 3.47879\n",
      "epoch no.3 train no.287240  loss = 3.32291 avg_loss = 3.52766\n",
      "epoch no.3 train no.287250  loss = 4.33761 avg_loss = 3.56263\n",
      "epoch no.3 train no.287260  loss = 2.45788 avg_loss = 3.59182\n",
      "epoch no.3 train no.287270  loss = 5.21348 avg_loss = 3.55459\n",
      "epoch no.3 train no.287280  loss = 3.38746 avg_loss = 3.54483\n",
      "epoch no.3 train no.287290  loss = 4.11196 avg_loss = 3.51217\n",
      "epoch no.3 train no.287300  loss = 4.20951 avg_loss = 3.48763\n",
      "epoch no.3 train no.287310  loss = 2.95925 avg_loss = 3.50720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.287320  loss = 4.51390 avg_loss = 3.46366\n",
      "epoch no.3 train no.287330  loss = 2.73658 avg_loss = 3.43140\n",
      "epoch no.3 train no.287340  loss = 2.99084 avg_loss = 3.46392\n",
      "epoch no.3 train no.287350  loss = 4.67961 avg_loss = 3.48602\n",
      "epoch no.3 train no.287360  loss = 2.53843 avg_loss = 3.47288\n",
      "epoch no.3 train no.287370  loss = 1.86190 avg_loss = 3.48734\n",
      "epoch no.3 train no.287380  loss = 3.14556 avg_loss = 3.46721\n",
      "epoch no.3 train no.287390  loss = 4.10942 avg_loss = 3.43732\n",
      "epoch no.3 train no.287400  loss = 3.99141 avg_loss = 3.48795\n",
      "epoch no.3 train no.287410  loss = 3.56940 avg_loss = 3.44182\n",
      "epoch no.3 train no.287420  loss = 4.90155 avg_loss = 3.46658\n",
      "epoch no.3 train no.287430  loss = 2.78349 avg_loss = 3.50164\n",
      "epoch no.3 train no.287440  loss = 3.66257 avg_loss = 3.44735\n",
      "epoch no.3 train no.287450  loss = 2.90056 avg_loss = 3.41141\n",
      "epoch no.3 train no.287460  loss = 1.81511 avg_loss = 3.39274\n",
      "epoch no.3 train no.287470  loss = 3.61978 avg_loss = 3.39496\n",
      "epoch no.3 train no.287480  loss = 3.73264 avg_loss = 3.37200\n",
      "epoch no.3 train no.287490  loss = 3.93513 avg_loss = 3.41319\n",
      "epoch no.3 train no.287500  loss = 4.77488 avg_loss = 3.41047\n",
      "epoch no.3 train no.287510  loss = 4.21199 avg_loss = 3.42671\n",
      "epoch no.3 train no.287520  loss = 4.30815 avg_loss = 3.43526\n",
      "epoch no.3 train no.287530  loss = 2.51473 avg_loss = 3.47046\n",
      "epoch no.4 train no.287540  loss = 3.45826 avg_loss = 3.43564\n",
      "epoch no.4 train no.287550  loss = 1.92725 avg_loss = 3.37462\n",
      "epoch no.4 train no.287560  loss = 2.63818 avg_loss = 3.32424\n",
      "epoch no.4 train no.287570  loss = 2.47599 avg_loss = 3.26287\n",
      "epoch no.4 train no.287580  loss = 2.53290 avg_loss = 3.23177\n",
      "epoch no.4 train no.287590  loss = 3.71767 avg_loss = 3.24094\n",
      "epoch no.4 train no.287600  loss = 2.22218 avg_loss = 3.17329\n",
      "epoch no.4 train no.287610  loss = 3.37184 avg_loss = 3.11099\n",
      "epoch no.4 train no.287620  loss = 3.78960 avg_loss = 3.11410\n",
      "epoch no.4 train no.287630  loss = 2.19590 avg_loss = 3.11228\n",
      "epoch no.4 train no.287640  loss = 3.36986 avg_loss = 3.09954\n",
      "epoch no.4 train no.287650  loss = 3.92224 avg_loss = 3.08846\n",
      "epoch no.4 train no.287660  loss = 2.38728 avg_loss = 3.09510\n",
      "epoch no.4 train no.287670  loss = 3.31991 avg_loss = 3.09987\n",
      "epoch no.4 train no.287680  loss = 4.05535 avg_loss = 3.08183\n",
      "epoch no.4 train no.287690  loss = 3.61334 avg_loss = 3.08353\n",
      "epoch no.4 train no.287700  loss = 1.77873 avg_loss = 3.06117\n",
      "epoch no.4 train no.287710  loss = 2.57718 avg_loss = 3.02686\n",
      "epoch no.4 train no.287720  loss = 2.98548 avg_loss = 3.09472\n",
      "epoch no.4 train no.287730  loss = 3.87934 avg_loss = 3.11068\n",
      "epoch no.4 train no.287740  loss = 2.15510 avg_loss = 3.07092\n",
      "epoch no.4 train no.287750  loss = 2.33644 avg_loss = 3.06092\n",
      "epoch no.4 train no.287760  loss = 2.72448 avg_loss = 3.09478\n",
      "epoch no.4 train no.287770  loss = 3.67309 avg_loss = 3.10886\n",
      "epoch no.4 train no.287780  loss = 4.52408 avg_loss = 3.12756\n",
      "epoch no.4 train no.287790  loss = 3.62658 avg_loss = 3.15126\n",
      "epoch no.4 train no.287800  loss = 3.13638 avg_loss = 3.11196\n",
      "epoch no.4 train no.287810  loss = 2.79934 avg_loss = 3.07112\n",
      "epoch no.4 train no.287820  loss = 4.09898 avg_loss = 3.05864\n",
      "epoch no.4 train no.287830  loss = 2.65821 avg_loss = 3.03807\n",
      "epoch no.4 train no.287840  loss = 2.38867 avg_loss = 3.01866\n",
      "epoch no.4 train no.287850  loss = 2.68032 avg_loss = 2.99242\n",
      "epoch no.4 train no.287860  loss = 3.42540 avg_loss = 2.99571\n",
      "epoch no.4 train no.287870  loss = 1.75398 avg_loss = 2.99004\n",
      "epoch no.4 train no.287880  loss = 2.51264 avg_loss = 3.01358\n",
      "epoch no.4 train no.287890  loss = 2.78421 avg_loss = 3.04600\n",
      "epoch no.4 train no.287900  loss = 2.44755 avg_loss = 3.04268\n",
      "epoch no.4 train no.287910  loss = 3.57608 avg_loss = 3.06930\n",
      "epoch no.4 train no.287920  loss = 2.84846 avg_loss = 3.02828\n",
      "epoch no.4 train no.287930  loss = 2.91633 avg_loss = 3.00523\n",
      "epoch no.4 train no.287940  loss = 3.67188 avg_loss = 3.03665\n",
      "epoch no.4 train no.287950  loss = 2.42082 avg_loss = 3.00451\n",
      "epoch no.4 train no.287960  loss = 2.24529 avg_loss = 3.00346\n",
      "epoch no.4 train no.287970  loss = 3.12017 avg_loss = 3.00988\n",
      "epoch no.4 train no.287980  loss = 3.67053 avg_loss = 2.99918\n",
      "epoch no.4 train no.287990  loss = 4.57205 avg_loss = 3.00288\n",
      "epoch no.4 train no.288000  loss = 4.35741 avg_loss = 2.97736\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁꿀', '▁로맨틱', '</s>', '</s>']\n",
      "여름밤의 달콤한 음악들</s>\n",
      "epoch no.4 train no.288010  loss = 2.82772 avg_loss = 2.96154\n",
      "epoch no.4 train no.288020  loss = 5.37852 avg_loss = 3.00426\n",
      "epoch no.4 train no.288030  loss = 2.08719 avg_loss = 3.01233\n",
      "epoch no.4 train no.288040  loss = 1.97047 avg_loss = 3.01115\n",
      "epoch no.4 train no.288050  loss = 3.83575 avg_loss = 3.05647\n",
      "epoch no.4 train no.288060  loss = 2.44977 avg_loss = 3.02395\n",
      "epoch no.4 train no.288070  loss = 4.33594 avg_loss = 3.01090\n",
      "epoch no.4 train no.288080  loss = 3.99099 avg_loss = 3.02329\n",
      "epoch no.4 train no.288090  loss = 4.02634 avg_loss = 3.03882\n",
      "epoch no.4 train no.288100  loss = 1.79328 avg_loss = 2.98954\n",
      "epoch no.4 train no.288110  loss = 1.20289 avg_loss = 2.96743\n",
      "epoch no.4 train no.288120  loss = 3.72217 avg_loss = 2.98076\n",
      "epoch no.4 train no.288130  loss = 2.23853 avg_loss = 2.97590\n",
      "epoch no.4 train no.288140  loss = 2.51188 avg_loss = 3.04482\n",
      "epoch no.4 train no.288150  loss = 2.59422 avg_loss = 3.08055\n",
      "epoch no.4 train no.288160  loss = 2.10003 avg_loss = 3.06183\n",
      "epoch no.4 train no.288170  loss = 4.34223 avg_loss = 3.04704\n",
      "epoch no.4 train no.288180  loss = 2.02973 avg_loss = 3.03606\n",
      "epoch no.4 train no.288190  loss = 2.82729 avg_loss = 3.01993\n",
      "epoch no.4 train no.288200  loss = 3.21556 avg_loss = 3.03903\n",
      "epoch no.4 train no.288210  loss = 2.71943 avg_loss = 3.06595\n",
      "epoch no.4 train no.288220  loss = 2.68010 avg_loss = 3.04481\n",
      "epoch no.4 train no.288230  loss = 3.67945 avg_loss = 3.08251\n",
      "epoch no.4 train no.288240  loss = 3.34816 avg_loss = 3.08014\n",
      "epoch no.4 train no.288250  loss = 1.36552 avg_loss = 3.09762\n",
      "epoch no.4 train no.288260  loss = 2.90003 avg_loss = 3.08350\n",
      "epoch no.4 train no.288270  loss = 2.30558 avg_loss = 3.10866\n",
      "epoch no.4 train no.288280  loss = 2.27936 avg_loss = 3.13274\n",
      "epoch no.4 train no.288290  loss = 1.83883 avg_loss = 3.11711\n",
      "epoch no.4 train no.288300  loss = 4.33842 avg_loss = 3.10184\n",
      "epoch no.4 train no.288310  loss = 3.01913 avg_loss = 3.10289\n",
      "epoch no.4 train no.288320  loss = 2.97731 avg_loss = 3.06088\n",
      "epoch no.4 train no.288330  loss = 2.91247 avg_loss = 3.08948\n",
      "epoch no.4 train no.288340  loss = 2.89580 avg_loss = 3.05913\n",
      "epoch no.4 train no.288350  loss = 2.78829 avg_loss = 3.03265\n",
      "epoch no.4 train no.288360  loss = 1.98319 avg_loss = 2.98643\n",
      "epoch no.4 train no.288370  loss = 2.48794 avg_loss = 2.92722\n",
      "epoch no.4 train no.288380  loss = 2.08643 avg_loss = 2.95308\n",
      "epoch no.4 train no.288390  loss = 3.77845 avg_loss = 2.94536\n",
      "epoch no.4 train no.288400  loss = 2.98965 avg_loss = 2.98517\n",
      "epoch no.4 train no.288410  loss = 3.24926 avg_loss = 2.98650\n",
      "epoch no.4 train no.288420  loss = 1.59878 avg_loss = 2.96324\n",
      "epoch no.4 train no.288430  loss = 2.99478 avg_loss = 2.95514\n",
      "epoch no.4 train no.288440  loss = 2.68546 avg_loss = 2.93847\n",
      "epoch no.4 train no.288450  loss = 2.49155 avg_loss = 2.93778\n",
      "epoch no.4 train no.288460  loss = 0.98187 avg_loss = 2.91647\n",
      "epoch no.4 train no.288470  loss = 1.72412 avg_loss = 2.94676\n",
      "epoch no.4 train no.288480  loss = 1.98099 avg_loss = 2.90358\n",
      "epoch no.4 train no.288490  loss = 2.10260 avg_loss = 2.92194\n",
      "epoch no.4 train no.288500  loss = 2.84280 avg_loss = 2.94803\n",
      "epoch no.4 train no.288510  loss = 2.58104 avg_loss = 2.97449\n",
      "epoch no.4 train no.288520  loss = 2.68216 avg_loss = 2.95498\n",
      "epoch no.4 train no.288530  loss = 3.40136 avg_loss = 2.94761\n",
      "epoch no.4 train no.288540  loss = 4.03550 avg_loss = 2.97472\n",
      "epoch no.4 train no.288550  loss = 4.07175 avg_loss = 2.98223\n",
      "epoch no.4 train no.288560  loss = 2.62260 avg_loss = 3.01538\n",
      "epoch no.4 train no.288570  loss = 2.37592 avg_loss = 3.02832\n",
      "epoch no.4 train no.288580  loss = 3.78028 avg_loss = 3.03181\n",
      "epoch no.4 train no.288590  loss = 4.17124 avg_loss = 3.02919\n",
      "epoch no.4 train no.288600  loss = 3.28194 avg_loss = 3.04544\n",
      "epoch no.4 train no.288610  loss = 2.64505 avg_loss = 3.02554\n",
      "epoch no.4 train no.288620  loss = 2.83299 avg_loss = 2.97155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.288630  loss = 2.08378 avg_loss = 2.96924\n",
      "epoch no.4 train no.288640  loss = 4.71901 avg_loss = 3.03756\n",
      "epoch no.4 train no.288650  loss = 2.98823 avg_loss = 3.02131\n",
      "epoch no.4 train no.288660  loss = 1.82087 avg_loss = 2.99157\n",
      "epoch no.4 train no.288670  loss = 2.91308 avg_loss = 3.00940\n",
      "epoch no.4 train no.288680  loss = 4.19377 avg_loss = 3.02122\n",
      "epoch no.4 train no.288690  loss = 2.35186 avg_loss = 2.99756\n",
      "epoch no.4 train no.288700  loss = 2.38894 avg_loss = 3.00226\n",
      "epoch no.4 train no.288710  loss = 3.29364 avg_loss = 3.07082\n",
      "epoch no.4 train no.288720  loss = 2.63957 avg_loss = 3.05621\n",
      "epoch no.4 train no.288730  loss = 4.34259 avg_loss = 3.06770\n",
      "epoch no.4 train no.288740  loss = 3.54759 avg_loss = 3.04869\n",
      "epoch no.4 train no.288750  loss = 3.29023 avg_loss = 3.06710\n",
      "epoch no.4 train no.288760  loss = 1.67750 avg_loss = 3.06231\n",
      "epoch no.4 train no.288770  loss = 2.74256 avg_loss = 3.05976\n",
      "epoch no.4 train no.288780  loss = 3.19056 avg_loss = 3.07016\n",
      "epoch no.4 train no.288790  loss = 4.00960 avg_loss = 3.04306\n",
      "epoch no.4 train no.288800  loss = 4.57271 avg_loss = 3.06738\n",
      "epoch no.4 train no.288810  loss = 2.75127 avg_loss = 3.07353\n",
      "epoch no.4 train no.288820  loss = 3.78090 avg_loss = 3.07866\n",
      "epoch no.4 train no.288830  loss = 2.71674 avg_loss = 3.06376\n",
      "epoch no.4 train no.288840  loss = 3.25208 avg_loss = 3.07151\n",
      "epoch no.4 train no.288850  loss = 1.98981 avg_loss = 3.03437\n",
      "epoch no.4 train no.288860  loss = 1.99903 avg_loss = 3.03840\n",
      "epoch no.4 train no.288870  loss = 2.25571 avg_loss = 3.01268\n",
      "epoch no.4 train no.288880  loss = 2.45402 avg_loss = 3.00181\n",
      "epoch no.4 train no.288890  loss = 4.13791 avg_loss = 2.97541\n",
      "epoch no.4 train no.288900  loss = 2.35855 avg_loss = 2.95380\n",
      "epoch no.4 train no.288910  loss = 2.51020 avg_loss = 2.95021\n",
      "epoch no.4 train no.288920  loss = 1.76664 avg_loss = 2.95632\n",
      "epoch no.4 train no.288930  loss = 1.99865 avg_loss = 2.97859\n",
      "epoch no.4 train no.288940  loss = 3.34435 avg_loss = 2.97108\n",
      "epoch no.4 train no.288950  loss = 1.95048 avg_loss = 2.99057\n",
      "epoch no.4 train no.288960  loss = 4.70660 avg_loss = 3.00682\n",
      "epoch no.4 train no.288970  loss = 3.41938 avg_loss = 3.03942\n",
      "epoch no.4 train no.288980  loss = 3.33942 avg_loss = 3.03389\n",
      "epoch no.4 train no.288990  loss = 3.09792 avg_loss = 3.06766\n",
      "epoch no.4 train no.289000  loss = 3.37294 avg_loss = 3.07105\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁재즈', '적인', '▁노래', '</s>']\n",
      "여름밤에 어울리는 감성적인 팝</s>\n",
      "epoch no.4 train no.289010  loss = 2.38704 avg_loss = 3.02195\n",
      "epoch no.4 train no.289020  loss = 2.85797 avg_loss = 2.99545\n",
      "epoch no.4 train no.289030  loss = 1.38728 avg_loss = 2.95928\n",
      "epoch no.4 train no.289040  loss = 3.14689 avg_loss = 2.96338\n",
      "epoch no.4 train no.289050  loss = 2.87533 avg_loss = 2.97381\n",
      "epoch no.4 train no.289060  loss = 2.72598 avg_loss = 2.99556\n",
      "epoch no.4 train no.289070  loss = 2.86012 avg_loss = 2.96211\n",
      "epoch no.4 train no.289080  loss = 3.30293 avg_loss = 2.99840\n",
      "epoch no.4 train no.289090  loss = 3.54853 avg_loss = 3.02417\n",
      "epoch no.4 train no.289100  loss = 3.52772 avg_loss = 3.04590\n",
      "epoch no.4 train no.289110  loss = 1.59678 avg_loss = 3.02754\n",
      "epoch no.4 train no.289120  loss = 3.22256 avg_loss = 3.05185\n",
      "epoch no.4 train no.289130  loss = 2.24049 avg_loss = 3.07466\n",
      "epoch no.4 train no.289140  loss = 3.37654 avg_loss = 3.07910\n",
      "epoch no.4 train no.289150  loss = 4.54178 avg_loss = 3.07943\n",
      "epoch no.4 train no.289160  loss = 2.06266 avg_loss = 3.09909\n",
      "epoch no.4 train no.289170  loss = 2.69381 avg_loss = 3.07387\n",
      "epoch no.4 train no.289180  loss = 2.11482 avg_loss = 3.07927\n",
      "epoch no.4 train no.289190  loss = 2.95051 avg_loss = 3.04892\n",
      "epoch no.4 train no.289200  loss = 2.66418 avg_loss = 3.01053\n",
      "epoch no.4 train no.289210  loss = 3.18253 avg_loss = 3.01169\n",
      "epoch no.4 train no.289220  loss = 2.17429 avg_loss = 2.99383\n",
      "epoch no.4 train no.289230  loss = 2.34780 avg_loss = 3.00170\n",
      "epoch no.4 train no.289240  loss = 3.02184 avg_loss = 3.02476\n",
      "epoch no.4 train no.289250  loss = 3.32413 avg_loss = 3.01737\n",
      "epoch no.4 train no.289260  loss = 2.16265 avg_loss = 2.97523\n",
      "epoch no.4 train no.289270  loss = 2.62108 avg_loss = 2.98057\n",
      "epoch no.4 train no.289280  loss = 1.69250 avg_loss = 2.98465\n",
      "epoch no.4 train no.289290  loss = 1.71624 avg_loss = 2.95183\n",
      "epoch no.4 train no.289300  loss = 2.56148 avg_loss = 2.95926\n",
      "epoch no.4 train no.289310  loss = 4.68405 avg_loss = 2.98050\n",
      "epoch no.4 train no.289320  loss = 3.63972 avg_loss = 2.98179\n",
      "epoch no.4 train no.289330  loss = 1.26300 avg_loss = 2.97789\n",
      "epoch no.4 train no.289340  loss = 4.89328 avg_loss = 2.96229\n",
      "epoch no.4 train no.289350  loss = 3.12388 avg_loss = 2.99129\n",
      "epoch no.4 train no.289360  loss = 2.84419 avg_loss = 3.01070\n",
      "epoch no.4 train no.289370  loss = 2.66510 avg_loss = 3.02362\n",
      "epoch no.4 train no.289380  loss = 4.18758 avg_loss = 3.01245\n",
      "epoch no.4 train no.289390  loss = 3.41837 avg_loss = 3.01486\n",
      "epoch no.4 train no.289400  loss = 3.17478 avg_loss = 3.01246\n",
      "epoch no.4 train no.289410  loss = 4.28794 avg_loss = 3.04274\n",
      "epoch no.4 train no.289420  loss = 4.38879 avg_loss = 3.05326\n",
      "epoch no.4 train no.289430  loss = 2.59159 avg_loss = 3.06787\n",
      "epoch no.4 train no.289440  loss = 2.65343 avg_loss = 3.01986\n",
      "epoch no.4 train no.289450  loss = 6.25316 avg_loss = 3.07855\n",
      "epoch no.4 train no.289460  loss = 2.92628 avg_loss = 3.06002\n",
      "epoch no.4 train no.289470  loss = 3.64693 avg_loss = 3.03016\n",
      "epoch no.4 train no.289480  loss = 2.98180 avg_loss = 2.98096\n",
      "epoch no.4 train no.289490  loss = 2.12861 avg_loss = 3.02236\n",
      "epoch no.4 train no.289500  loss = 4.90520 avg_loss = 3.04920\n",
      "epoch no.4 train no.289510  loss = 3.67976 avg_loss = 3.11479\n",
      "epoch no.4 train no.289520  loss = 4.32813 avg_loss = 3.11859\n",
      "epoch no.4 train no.289530  loss = 3.74878 avg_loss = 3.13393\n",
      "epoch no.4 train no.289540  loss = 3.38838 avg_loss = 3.14916\n",
      "epoch no.4 train no.289550  loss = 2.77595 avg_loss = 3.14415\n",
      "epoch no.4 train no.289560  loss = 4.17672 avg_loss = 3.17340\n",
      "epoch no.4 train no.289570  loss = 3.41002 avg_loss = 3.13504\n",
      "epoch no.4 train no.289580  loss = 3.27682 avg_loss = 3.11017\n",
      "epoch no.4 train no.289590  loss = 3.72187 avg_loss = 3.14636\n",
      "epoch no.4 train no.289600  loss = 2.35784 avg_loss = 3.20520\n",
      "epoch no.4 train no.289610  loss = 2.71423 avg_loss = 3.20451\n",
      "epoch no.4 train no.289620  loss = 5.65147 avg_loss = 3.20704\n",
      "epoch no.4 train no.289630  loss = 3.56878 avg_loss = 3.18899\n",
      "epoch no.4 train no.289640  loss = 2.89198 avg_loss = 3.15488\n",
      "epoch no.4 train no.289650  loss = 4.51517 avg_loss = 3.13611\n",
      "epoch no.4 train no.289660  loss = 3.06553 avg_loss = 3.13480\n",
      "epoch no.4 train no.289670  loss = 4.88213 avg_loss = 3.15059\n",
      "epoch no.4 train no.289680  loss = 2.92898 avg_loss = 3.12651\n",
      "epoch no.4 train no.289690  loss = 3.80946 avg_loss = 3.13536\n",
      "epoch no.4 train no.289700  loss = 4.19709 avg_loss = 3.12766\n",
      "epoch no.4 train no.289710  loss = 2.21588 avg_loss = 3.15413\n",
      "epoch no.4 train no.289720  loss = 2.53203 avg_loss = 3.11781\n",
      "epoch no.4 train no.289730  loss = 2.11490 avg_loss = 3.13014\n",
      "epoch no.4 train no.289740  loss = 2.97657 avg_loss = 3.09919\n",
      "epoch no.4 train no.289750  loss = 3.97433 avg_loss = 3.09130\n",
      "epoch no.4 train no.289760  loss = 2.76399 avg_loss = 3.11010\n",
      "epoch no.4 train no.289770  loss = 2.42362 avg_loss = 3.07642\n",
      "epoch no.4 train no.289780  loss = 2.98631 avg_loss = 3.08516\n",
      "epoch no.4 train no.289790  loss = 3.69014 avg_loss = 3.08371\n",
      "epoch no.4 train no.289800  loss = 4.25484 avg_loss = 3.06896\n",
      "epoch no.4 train no.289810  loss = 3.98162 avg_loss = 3.09541\n",
      "epoch no.4 train no.289820  loss = 3.80223 avg_loss = 3.09833\n",
      "epoch no.4 train no.289830  loss = 3.39743 avg_loss = 3.12856\n",
      "epoch no.4 train no.289840  loss = 3.74580 avg_loss = 3.14029\n",
      "epoch no.4 train no.289850  loss = 1.79711 avg_loss = 3.11953\n",
      "epoch no.4 train no.289860  loss = 3.19316 avg_loss = 3.15978\n",
      "epoch no.4 train no.289870  loss = 3.26823 avg_loss = 3.14047\n",
      "epoch no.4 train no.289880  loss = 4.38911 avg_loss = 3.13025\n",
      "epoch no.4 train no.289890  loss = 3.13038 avg_loss = 3.13761\n",
      "epoch no.4 train no.289900  loss = 3.34721 avg_loss = 3.14149\n",
      "epoch no.4 train no.289910  loss = 3.15560 avg_loss = 3.08405\n",
      "epoch no.4 train no.289920  loss = 2.17847 avg_loss = 3.07763\n",
      "epoch no.4 train no.289930  loss = 3.20023 avg_loss = 3.06606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.289940  loss = 2.30819 avg_loss = 3.06061\n",
      "epoch no.4 train no.289950  loss = 3.47276 avg_loss = 3.05189\n",
      "epoch no.4 train no.289960  loss = 3.36219 avg_loss = 3.11097\n",
      "epoch no.4 train no.289970  loss = 4.13295 avg_loss = 3.11173\n",
      "epoch no.4 train no.289980  loss = 2.38958 avg_loss = 3.07595\n",
      "epoch no.4 train no.289990  loss = 2.15974 avg_loss = 3.05777\n",
      "epoch no.4 train no.290000  loss = 3.34795 avg_loss = 3.05205\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '▁음악', '</s>']\n",
      "여름밤에 듣는 아름다운 음악</s>\n",
      "epoch no.4 train no.290010  loss = 5.69842 avg_loss = 3.07501\n",
      "epoch no.4 train no.290020  loss = 2.36406 avg_loss = 3.08059\n",
      "epoch no.4 train no.290030  loss = 3.34530 avg_loss = 3.08609\n",
      "epoch no.4 train no.290040  loss = 2.06828 avg_loss = 3.08933\n",
      "epoch no.4 train no.290050  loss = 3.30690 avg_loss = 3.12283\n",
      "epoch no.4 train no.290060  loss = 2.31906 avg_loss = 3.09650\n",
      "epoch no.4 train no.290070  loss = 4.21997 avg_loss = 3.07144\n",
      "epoch no.4 train no.290080  loss = 2.67966 avg_loss = 3.06226\n",
      "epoch no.4 train no.290090  loss = 1.37891 avg_loss = 3.01568\n",
      "epoch no.4 train no.290100  loss = 1.31720 avg_loss = 3.03661\n",
      "epoch no.4 train no.290110  loss = 3.47252 avg_loss = 2.98475\n",
      "epoch no.4 train no.290120  loss = 2.68489 avg_loss = 3.00859\n",
      "epoch no.4 train no.290130  loss = 3.60401 avg_loss = 2.99713\n",
      "epoch no.4 train no.290140  loss = 4.28516 avg_loss = 2.98275\n",
      "epoch no.4 train no.290150  loss = 3.23532 avg_loss = 3.02647\n",
      "epoch no.4 train no.290160  loss = 3.29345 avg_loss = 2.99499\n",
      "epoch no.4 train no.290170  loss = 2.89238 avg_loss = 2.98960\n",
      "epoch no.4 train no.290180  loss = 3.21265 avg_loss = 2.96551\n",
      "epoch no.4 train no.290190  loss = 3.50293 avg_loss = 2.96931\n",
      "epoch no.4 train no.290200  loss = 3.86211 avg_loss = 2.98147\n",
      "epoch no.4 train no.290210  loss = 3.14689 avg_loss = 2.97973\n",
      "epoch no.4 train no.290220  loss = 1.90983 avg_loss = 2.98398\n",
      "epoch no.4 train no.290230  loss = 3.99635 avg_loss = 2.98775\n",
      "epoch no.4 train no.290240  loss = 3.36183 avg_loss = 2.99101\n",
      "epoch no.4 train no.290250  loss = 1.12278 avg_loss = 3.02350\n",
      "epoch no.4 train no.290260  loss = 4.08804 avg_loss = 3.04969\n",
      "epoch no.4 train no.290270  loss = 3.06741 avg_loss = 3.05816\n",
      "epoch no.4 train no.290280  loss = 3.10129 avg_loss = 3.03562\n",
      "epoch no.4 train no.290290  loss = 2.92601 avg_loss = 3.00826\n",
      "epoch no.4 train no.290300  loss = 3.21354 avg_loss = 3.01580\n",
      "epoch no.4 train no.290310  loss = 4.95205 avg_loss = 3.04394\n",
      "epoch no.4 train no.290320  loss = 2.04733 avg_loss = 3.04947\n",
      "epoch no.4 train no.290330  loss = 4.31949 avg_loss = 3.05341\n",
      "epoch no.4 train no.290340  loss = 2.92040 avg_loss = 3.08498\n",
      "epoch no.4 train no.290350  loss = 2.81229 avg_loss = 3.03627\n",
      "epoch no.4 train no.290360  loss = 3.14473 avg_loss = 3.04208\n",
      "epoch no.4 train no.290370  loss = 4.16344 avg_loss = 3.06834\n",
      "epoch no.4 train no.290380  loss = 4.12029 avg_loss = 3.06664\n",
      "epoch no.4 train no.290390  loss = 4.07265 avg_loss = 3.07293\n",
      "epoch no.4 train no.290400  loss = 2.50302 avg_loss = 3.05761\n",
      "epoch no.4 train no.290410  loss = 2.97780 avg_loss = 3.05338\n",
      "epoch no.4 train no.290420  loss = 2.37290 avg_loss = 3.09396\n",
      "epoch no.4 train no.290430  loss = 2.69117 avg_loss = 3.06948\n",
      "epoch no.4 train no.290440  loss = 2.65298 avg_loss = 3.05508\n",
      "epoch no.4 train no.290450  loss = 2.25769 avg_loss = 3.07073\n",
      "epoch no.4 train no.290460  loss = 3.43407 avg_loss = 3.03505\n",
      "epoch no.4 train no.290470  loss = 3.28916 avg_loss = 3.03579\n",
      "epoch no.4 train no.290480  loss = 2.65466 avg_loss = 3.03533\n",
      "epoch no.4 train no.290490  loss = 2.18995 avg_loss = 2.99280\n",
      "epoch no.4 train no.290500  loss = 2.65970 avg_loss = 2.98750\n",
      "epoch no.4 train no.290510  loss = 3.45916 avg_loss = 3.00605\n",
      "epoch no.4 train no.290520  loss = 4.77452 avg_loss = 2.98978\n",
      "epoch no.4 train no.290530  loss = 3.32967 avg_loss = 2.99994\n",
      "epoch no.4 train no.290540  loss = 2.34010 avg_loss = 2.99840\n",
      "epoch no.4 train no.290550  loss = 2.28439 avg_loss = 2.96938\n",
      "epoch no.4 train no.290560  loss = 4.09699 avg_loss = 2.95890\n",
      "epoch no.4 train no.290570  loss = 2.38706 avg_loss = 2.95859\n",
      "epoch no.4 train no.290580  loss = 3.14682 avg_loss = 2.94834\n",
      "epoch no.4 train no.290590  loss = 2.64999 avg_loss = 2.93401\n",
      "epoch no.4 train no.290600  loss = 3.58852 avg_loss = 2.91632\n",
      "epoch no.4 train no.290610  loss = 2.96789 avg_loss = 2.93380\n",
      "epoch no.4 train no.290620  loss = 2.77682 avg_loss = 2.93031\n",
      "epoch no.4 train no.290630  loss = 4.45999 avg_loss = 2.95332\n",
      "epoch no.4 train no.290640  loss = 3.85464 avg_loss = 2.96429\n",
      "epoch no.4 train no.290650  loss = 3.00509 avg_loss = 3.01257\n",
      "epoch no.4 train no.290660  loss = 3.77416 avg_loss = 3.02478\n",
      "epoch no.4 train no.290670  loss = 2.79719 avg_loss = 2.96892\n",
      "epoch no.4 train no.290680  loss = 3.02613 avg_loss = 3.01502\n",
      "epoch no.4 train no.290690  loss = 3.73928 avg_loss = 3.01574\n",
      "epoch no.4 train no.290700  loss = 4.57363 avg_loss = 3.03771\n",
      "epoch no.4 train no.290710  loss = 3.09760 avg_loss = 3.05991\n",
      "epoch no.4 train no.290720  loss = 3.95750 avg_loss = 3.04112\n",
      "epoch no.4 train no.290730  loss = 2.11465 avg_loss = 3.04404\n",
      "epoch no.4 train no.290740  loss = 2.10184 avg_loss = 3.04408\n",
      "epoch no.4 train no.290750  loss = 2.36299 avg_loss = 3.01403\n",
      "epoch no.4 train no.290760  loss = 3.35015 avg_loss = 3.04883\n",
      "epoch no.4 train no.290770  loss = 1.87753 avg_loss = 3.02677\n",
      "epoch no.4 train no.290780  loss = 3.59297 avg_loss = 3.01832\n",
      "epoch no.4 train no.290790  loss = 3.76375 avg_loss = 3.03626\n",
      "epoch no.4 train no.290800  loss = 2.58089 avg_loss = 3.03704\n",
      "epoch no.4 train no.290810  loss = 3.45240 avg_loss = 2.98648\n",
      "epoch no.4 train no.290820  loss = 4.68830 avg_loss = 2.99921\n",
      "epoch no.4 train no.290830  loss = 2.56292 avg_loss = 3.05917\n",
      "epoch no.4 train no.290840  loss = 3.04747 avg_loss = 3.07797\n",
      "epoch no.4 train no.290850  loss = 4.24216 avg_loss = 3.10285\n",
      "epoch no.4 train no.290860  loss = 2.15565 avg_loss = 3.06325\n",
      "epoch no.4 train no.290870  loss = 3.87312 avg_loss = 3.08224\n",
      "epoch no.4 train no.290880  loss = 3.06658 avg_loss = 3.07258\n",
      "epoch no.4 train no.290890  loss = 2.95371 avg_loss = 3.04732\n",
      "epoch no.4 train no.290900  loss = 2.68933 avg_loss = 3.05804\n",
      "epoch no.4 train no.290910  loss = 2.95439 avg_loss = 3.07462\n",
      "epoch no.4 train no.290920  loss = 2.02407 avg_loss = 3.03885\n",
      "epoch no.4 train no.290930  loss = 2.72699 avg_loss = 3.01760\n",
      "epoch no.4 train no.290940  loss = 2.87819 avg_loss = 2.99342\n",
      "epoch no.4 train no.290950  loss = 2.42761 avg_loss = 2.99221\n",
      "epoch no.4 train no.290960  loss = 2.33842 avg_loss = 3.01636\n",
      "epoch no.4 train no.290970  loss = 1.62826 avg_loss = 2.99655\n",
      "epoch no.4 train no.290980  loss = 2.90256 avg_loss = 3.02436\n",
      "epoch no.4 train no.290990  loss = 3.69241 avg_loss = 3.01500\n",
      "epoch no.4 train no.291000  loss = 4.76628 avg_loss = 2.99601\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '팝', '</s>']\n",
      "여름밤에 듣는 감성 음악</s>\n",
      "epoch no.4 train no.291010  loss = 2.91805 avg_loss = 3.00658\n",
      "epoch no.4 train no.291020  loss = 2.93013 avg_loss = 3.02912\n",
      "epoch no.4 train no.291030  loss = 3.71909 avg_loss = 3.07982\n",
      "epoch no.4 train no.291040  loss = 2.32669 avg_loss = 3.06415\n",
      "epoch no.4 train no.291050  loss = 3.33509 avg_loss = 3.03759\n",
      "epoch no.4 train no.291060  loss = 2.21525 avg_loss = 3.00779\n",
      "epoch no.4 train no.291070  loss = 1.62262 avg_loss = 3.00491\n",
      "epoch no.4 train no.291080  loss = 3.18415 avg_loss = 2.99023\n",
      "epoch no.4 train no.291090  loss = 2.17151 avg_loss = 3.01515\n",
      "epoch no.4 train no.291100  loss = 2.82997 avg_loss = 3.00595\n",
      "epoch no.4 train no.291110  loss = 4.45222 avg_loss = 2.99919\n",
      "epoch no.4 train no.291120  loss = 3.31115 avg_loss = 2.99070\n",
      "epoch no.4 train no.291130  loss = 3.72197 avg_loss = 2.99847\n",
      "epoch no.4 train no.291140  loss = 2.05615 avg_loss = 3.01113\n",
      "epoch no.4 train no.291150  loss = 2.17566 avg_loss = 2.97871\n",
      "epoch no.4 train no.291160  loss = 2.21280 avg_loss = 2.95017\n",
      "epoch no.4 train no.291170  loss = 2.84350 avg_loss = 2.96734\n",
      "epoch no.4 train no.291180  loss = 3.75667 avg_loss = 2.99285\n",
      "epoch no.4 train no.291190  loss = 2.48075 avg_loss = 3.02227\n",
      "epoch no.4 train no.291200  loss = 1.73497 avg_loss = 3.04157\n",
      "epoch no.4 train no.291210  loss = 3.10290 avg_loss = 3.07147\n",
      "epoch no.4 train no.291220  loss = 3.21012 avg_loss = 3.09563\n",
      "epoch no.4 train no.291230  loss = 4.80200 avg_loss = 3.11558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.291240  loss = 3.14178 avg_loss = 3.14637\n",
      "epoch no.4 train no.291250  loss = 2.64596 avg_loss = 3.18216\n",
      "epoch no.4 train no.291260  loss = 3.59500 avg_loss = 3.19902\n",
      "epoch no.4 train no.291270  loss = 4.35769 avg_loss = 3.20012\n",
      "epoch no.4 train no.291280  loss = 1.75369 avg_loss = 3.21439\n",
      "epoch no.4 train no.291290  loss = 1.67080 avg_loss = 3.17519\n",
      "epoch no.4 train no.291300  loss = 1.83183 avg_loss = 3.14283\n",
      "epoch no.4 train no.291310  loss = 2.25608 avg_loss = 3.09041\n",
      "epoch no.4 train no.291320  loss = 3.61219 avg_loss = 3.05418\n",
      "epoch no.4 train no.291330  loss = 2.29437 avg_loss = 3.04614\n",
      "epoch no.4 train no.291340  loss = 5.22041 avg_loss = 3.04230\n",
      "epoch no.4 train no.291350  loss = 2.41426 avg_loss = 3.02914\n",
      "epoch no.4 train no.291360  loss = 3.79682 avg_loss = 3.01030\n",
      "epoch no.4 train no.291370  loss = 1.62502 avg_loss = 2.98184\n",
      "epoch no.4 train no.291380  loss = 2.55285 avg_loss = 2.95911\n",
      "epoch no.4 train no.291390  loss = 5.45392 avg_loss = 2.95457\n",
      "epoch no.4 train no.291400  loss = 2.57242 avg_loss = 2.95656\n",
      "epoch no.4 train no.291410  loss = 2.66274 avg_loss = 2.97076\n",
      "epoch no.4 train no.291420  loss = 3.08017 avg_loss = 2.95060\n",
      "epoch no.4 train no.291430  loss = 3.11859 avg_loss = 2.94907\n",
      "epoch no.4 train no.291440  loss = 3.17266 avg_loss = 2.95993\n",
      "epoch no.4 train no.291450  loss = 2.98599 avg_loss = 2.95220\n",
      "epoch no.4 train no.291460  loss = 1.94024 avg_loss = 2.91879\n",
      "epoch no.4 train no.291470  loss = 2.80779 avg_loss = 2.94927\n",
      "epoch no.4 train no.291480  loss = 1.56697 avg_loss = 2.89846\n",
      "epoch no.4 train no.291490  loss = 3.82125 avg_loss = 2.92041\n",
      "epoch no.4 train no.291500  loss = 5.63946 avg_loss = 2.93850\n",
      "epoch no.4 train no.291510  loss = 2.98831 avg_loss = 2.96012\n",
      "epoch no.4 train no.291520  loss = 2.73815 avg_loss = 2.97405\n",
      "epoch no.4 train no.291530  loss = 3.70481 avg_loss = 3.01323\n",
      "epoch no.4 train no.291540  loss = 3.23471 avg_loss = 2.98843\n",
      "epoch no.4 train no.291550  loss = 1.68917 avg_loss = 2.97761\n",
      "epoch no.4 train no.291560  loss = 2.43589 avg_loss = 2.95113\n",
      "epoch no.4 train no.291570  loss = 2.36206 avg_loss = 2.94302\n",
      "epoch no.4 train no.291580  loss = 2.71959 avg_loss = 2.90841\n",
      "epoch no.4 train no.291590  loss = 2.66643 avg_loss = 2.93454\n",
      "epoch no.4 train no.291600  loss = 1.78265 avg_loss = 2.90890\n",
      "epoch no.4 train no.291610  loss = 2.24629 avg_loss = 2.89937\n",
      "epoch no.4 train no.291620  loss = 2.99422 avg_loss = 2.90067\n",
      "epoch no.4 train no.291630  loss = 2.59622 avg_loss = 2.93149\n",
      "epoch no.4 train no.291640  loss = 2.16690 avg_loss = 2.93141\n",
      "epoch no.4 train no.291650  loss = 2.92217 avg_loss = 2.93545\n",
      "epoch no.4 train no.291660  loss = 4.03822 avg_loss = 2.93169\n",
      "epoch no.4 train no.291670  loss = 2.57337 avg_loss = 2.94104\n",
      "epoch no.4 train no.291680  loss = 2.09251 avg_loss = 2.95044\n",
      "epoch no.4 train no.291690  loss = 4.49684 avg_loss = 2.98652\n",
      "epoch no.4 train no.291700  loss = 2.64536 avg_loss = 2.99898\n",
      "epoch no.4 train no.291710  loss = 1.64983 avg_loss = 3.01301\n",
      "epoch no.4 train no.291720  loss = 4.43733 avg_loss = 3.02988\n",
      "epoch no.4 train no.291730  loss = 1.89869 avg_loss = 2.99596\n",
      "epoch no.4 train no.291740  loss = 2.57319 avg_loss = 3.00590\n",
      "epoch no.4 train no.291750  loss = 3.14699 avg_loss = 2.99851\n",
      "epoch no.4 train no.291760  loss = 3.57041 avg_loss = 3.03107\n",
      "epoch no.4 train no.291770  loss = 2.80932 avg_loss = 2.98652\n",
      "epoch no.4 train no.291780  loss = 3.26446 avg_loss = 2.95908\n",
      "epoch no.4 train no.291790  loss = 1.85260 avg_loss = 2.95209\n",
      "epoch no.4 train no.291800  loss = 2.60174 avg_loss = 2.97505\n",
      "epoch no.4 train no.291810  loss = 3.06205 avg_loss = 2.99813\n",
      "epoch no.4 train no.291820  loss = 4.34165 avg_loss = 3.00849\n",
      "epoch no.4 train no.291830  loss = 3.21297 avg_loss = 3.01207\n",
      "epoch no.4 train no.291840  loss = 3.69041 avg_loss = 3.07635\n",
      "epoch no.4 train no.291850  loss = 2.19941 avg_loss = 3.07982\n",
      "epoch no.4 train no.291860  loss = 4.56823 avg_loss = 3.08721\n",
      "epoch no.4 train no.291870  loss = 2.53560 avg_loss = 3.07554\n",
      "epoch no.4 train no.291880  loss = 4.45018 avg_loss = 3.07938\n",
      "epoch no.4 train no.291890  loss = 3.92912 avg_loss = 3.07921\n",
      "epoch no.4 train no.291900  loss = 3.51253 avg_loss = 3.06613\n",
      "epoch no.4 train no.291910  loss = 3.96836 avg_loss = 3.07163\n",
      "epoch no.4 train no.291920  loss = 3.46436 avg_loss = 3.06794\n",
      "epoch no.4 train no.291930  loss = 3.63586 avg_loss = 3.11731\n",
      "epoch no.4 train no.291940  loss = 3.17272 avg_loss = 3.12000\n",
      "epoch no.4 train no.291950  loss = 3.01120 avg_loss = 3.12266\n",
      "epoch no.4 train no.291960  loss = 3.68094 avg_loss = 3.10964\n",
      "epoch no.4 train no.291970  loss = 2.36091 avg_loss = 3.08225\n",
      "epoch no.4 train no.291980  loss = 4.49307 avg_loss = 3.10487\n",
      "epoch no.4 train no.291990  loss = 2.64518 avg_loss = 3.06748\n",
      "epoch no.4 train no.292000  loss = 4.61258 avg_loss = 3.11813\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.292010  loss = 4.14181 avg_loss = 3.12855\n",
      "epoch no.4 train no.292020  loss = 2.98460 avg_loss = 3.12415\n",
      "epoch no.4 train no.292030  loss = 2.41098 avg_loss = 3.10037\n",
      "epoch no.4 train no.292040  loss = 3.37586 avg_loss = 3.09940\n",
      "epoch no.4 train no.292050  loss = 3.17740 avg_loss = 3.09265\n",
      "epoch no.4 train no.292060  loss = 2.52954 avg_loss = 3.07614\n",
      "epoch no.4 train no.292070  loss = 1.87400 avg_loss = 3.04944\n",
      "epoch no.4 train no.292080  loss = 1.99829 avg_loss = 3.03606\n",
      "epoch no.4 train no.292090  loss = 2.22275 avg_loss = 2.98987\n",
      "epoch no.4 train no.292100  loss = 2.78412 avg_loss = 3.01594\n",
      "epoch no.4 train no.292110  loss = 3.77263 avg_loss = 3.00071\n",
      "epoch no.4 train no.292120  loss = 1.92893 avg_loss = 2.97859\n",
      "epoch no.4 train no.292130  loss = 4.02912 avg_loss = 2.97145\n",
      "epoch no.4 train no.292140  loss = 3.56000 avg_loss = 2.96613\n",
      "epoch no.4 train no.292150  loss = 3.40232 avg_loss = 2.98608\n",
      "epoch no.4 train no.292160  loss = 2.21922 avg_loss = 2.95786\n",
      "epoch no.4 train no.292170  loss = 2.39675 avg_loss = 2.94931\n",
      "epoch no.4 train no.292180  loss = 2.17430 avg_loss = 2.96259\n",
      "epoch no.4 train no.292190  loss = 2.24672 avg_loss = 2.96085\n",
      "epoch no.4 train no.292200  loss = 2.90398 avg_loss = 2.96442\n",
      "epoch no.4 train no.292210  loss = 3.14675 avg_loss = 3.00612\n",
      "epoch no.4 train no.292220  loss = 2.97697 avg_loss = 3.01883\n",
      "epoch no.4 train no.292230  loss = 3.95853 avg_loss = 3.01891\n",
      "epoch no.4 train no.292240  loss = 4.10513 avg_loss = 3.09126\n",
      "epoch no.4 train no.292250  loss = 2.08149 avg_loss = 3.08174\n",
      "epoch no.4 train no.292260  loss = 2.36474 avg_loss = 3.06403\n",
      "epoch no.4 train no.292270  loss = 2.59117 avg_loss = 3.08604\n",
      "epoch no.4 train no.292280  loss = 2.75588 avg_loss = 3.07850\n",
      "epoch no.4 train no.292290  loss = 3.61128 avg_loss = 3.06915\n",
      "epoch no.4 train no.292300  loss = 1.85196 avg_loss = 3.08377\n",
      "epoch no.4 train no.292310  loss = 3.60911 avg_loss = 3.06701\n",
      "epoch no.4 train no.292320  loss = 2.56193 avg_loss = 3.04722\n",
      "epoch no.4 train no.292330  loss = 4.04820 avg_loss = 3.05264\n",
      "epoch no.4 train no.292340  loss = 3.81295 avg_loss = 3.05041\n",
      "epoch no.4 train no.292350  loss = 2.32673 avg_loss = 3.02121\n",
      "epoch no.4 train no.292360  loss = 1.96101 avg_loss = 3.06595\n",
      "epoch no.4 train no.292370  loss = 3.69573 avg_loss = 3.08251\n",
      "epoch no.4 train no.292380  loss = 2.64471 avg_loss = 3.06528\n",
      "epoch no.4 train no.292390  loss = 2.77484 avg_loss = 3.04284\n",
      "epoch no.4 train no.292400  loss = 3.21140 avg_loss = 3.03398\n",
      "epoch no.4 train no.292410  loss = 3.96550 avg_loss = 3.04419\n",
      "epoch no.4 train no.292420  loss = 2.45652 avg_loss = 3.01793\n",
      "epoch no.4 train no.292430  loss = 2.39856 avg_loss = 3.00505\n",
      "epoch no.4 train no.292440  loss = 2.04355 avg_loss = 3.02264\n",
      "epoch no.4 train no.292450  loss = 2.60320 avg_loss = 3.01211\n",
      "epoch no.4 train no.292460  loss = 3.09351 avg_loss = 3.02089\n",
      "epoch no.4 train no.292470  loss = 2.45032 avg_loss = 3.03671\n",
      "epoch no.4 train no.292480  loss = 1.88568 avg_loss = 3.00542\n",
      "epoch no.4 train no.292490  loss = 2.98193 avg_loss = 3.05582\n",
      "epoch no.4 train no.292500  loss = 2.77529 avg_loss = 3.07949\n",
      "epoch no.4 train no.292510  loss = 4.47952 avg_loss = 3.11500\n",
      "epoch no.4 train no.292520  loss = 2.14435 avg_loss = 3.12250\n",
      "epoch no.4 train no.292530  loss = 2.13662 avg_loss = 3.10820\n",
      "epoch no.4 train no.292540  loss = 2.56309 avg_loss = 3.11503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.292550  loss = 4.55312 avg_loss = 3.11580\n",
      "epoch no.4 train no.292560  loss = 2.21658 avg_loss = 3.12296\n",
      "epoch no.4 train no.292570  loss = 3.39156 avg_loss = 3.10385\n",
      "epoch no.4 train no.292580  loss = 3.63035 avg_loss = 3.10624\n",
      "epoch no.4 train no.292590  loss = 2.73851 avg_loss = 3.09159\n",
      "epoch no.4 train no.292600  loss = 2.51124 avg_loss = 3.05610\n",
      "epoch no.4 train no.292610  loss = 3.39596 avg_loss = 3.03782\n",
      "epoch no.4 train no.292620  loss = 2.49656 avg_loss = 3.04822\n",
      "epoch no.4 train no.292630  loss = 2.32118 avg_loss = 3.04398\n",
      "epoch no.4 train no.292640  loss = 3.19113 avg_loss = 3.00229\n",
      "epoch no.4 train no.292650  loss = 3.70650 avg_loss = 3.02737\n",
      "epoch no.4 train no.292660  loss = 2.13105 avg_loss = 3.02793\n",
      "epoch no.4 train no.292670  loss = 2.82786 avg_loss = 2.98367\n",
      "epoch no.4 train no.292680  loss = 4.37782 avg_loss = 3.01428\n",
      "epoch no.4 train no.292690  loss = 2.43844 avg_loss = 2.99006\n",
      "epoch no.4 train no.292700  loss = 3.10266 avg_loss = 3.00989\n",
      "epoch no.4 train no.292710  loss = 2.31205 avg_loss = 3.01344\n",
      "epoch no.4 train no.292720  loss = 2.87042 avg_loss = 2.97713\n",
      "epoch no.4 train no.292730  loss = 4.33202 avg_loss = 2.99545\n",
      "epoch no.4 train no.292740  loss = 3.34488 avg_loss = 2.97896\n",
      "epoch no.4 train no.292750  loss = 4.47688 avg_loss = 3.04326\n",
      "epoch no.4 train no.292760  loss = 6.41387 avg_loss = 3.09134\n",
      "epoch no.4 train no.292770  loss = 2.41073 avg_loss = 3.08739\n",
      "epoch no.4 train no.292780  loss = 2.76124 avg_loss = 3.06930\n",
      "epoch no.4 train no.292790  loss = 3.82979 avg_loss = 3.09696\n",
      "epoch no.4 train no.292800  loss = 3.04022 avg_loss = 3.08063\n",
      "epoch no.4 train no.292810  loss = 3.43905 avg_loss = 3.10244\n",
      "epoch no.4 train no.292820  loss = 3.02253 avg_loss = 3.06847\n",
      "epoch no.4 train no.292830  loss = 3.97872 avg_loss = 3.07597\n",
      "epoch no.4 train no.292840  loss = 4.17467 avg_loss = 3.11115\n",
      "epoch no.4 train no.292850  loss = 2.68947 avg_loss = 3.10243\n",
      "epoch no.4 train no.292860  loss = 2.63426 avg_loss = 3.14095\n",
      "epoch no.4 train no.292870  loss = 3.61542 avg_loss = 3.09555\n",
      "epoch no.4 train no.292880  loss = 4.56128 avg_loss = 3.09359\n",
      "epoch no.4 train no.292890  loss = 2.73620 avg_loss = 3.07243\n",
      "epoch no.4 train no.292900  loss = 2.32989 avg_loss = 3.08648\n",
      "epoch no.4 train no.292910  loss = 3.63359 avg_loss = 3.05761\n",
      "epoch no.4 train no.292920  loss = 3.99744 avg_loss = 3.05584\n",
      "epoch no.4 train no.292930  loss = 3.54334 avg_loss = 3.10537\n",
      "epoch no.4 train no.292940  loss = 1.68411 avg_loss = 3.10252\n",
      "epoch no.4 train no.292950  loss = 3.76859 avg_loss = 3.10950\n",
      "epoch no.4 train no.292960  loss = 2.97178 avg_loss = 3.12729\n",
      "epoch no.4 train no.292970  loss = 5.44345 avg_loss = 3.17461\n",
      "epoch no.4 train no.292980  loss = 2.08526 avg_loss = 3.12825\n",
      "epoch no.4 train no.292990  loss = 1.88032 avg_loss = 3.14240\n",
      "epoch no.4 train no.293000  loss = 3.71065 avg_loss = 3.14186\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '▁해주는', '▁노래', '</s>']\n",
      "여름밤을 시원하게 해줄 노래</s>\n",
      "epoch no.4 train no.293010  loss = 2.35587 avg_loss = 3.14715\n",
      "epoch no.4 train no.293020  loss = 3.20553 avg_loss = 3.13793\n",
      "epoch no.4 train no.293030  loss = 3.26447 avg_loss = 3.12616\n",
      "epoch no.4 train no.293040  loss = 1.85554 avg_loss = 3.11261\n",
      "epoch no.4 train no.293050  loss = 3.77395 avg_loss = 3.08965\n",
      "epoch no.4 train no.293060  loss = 2.06373 avg_loss = 3.05868\n",
      "epoch no.4 train no.293070  loss = 3.16392 avg_loss = 3.06586\n",
      "epoch no.4 train no.293080  loss = 2.19701 avg_loss = 3.04170\n",
      "epoch no.4 train no.293090  loss = 4.70108 avg_loss = 3.10447\n",
      "epoch no.4 train no.293100  loss = 3.33349 avg_loss = 3.08188\n",
      "epoch no.4 train no.293110  loss = 2.23800 avg_loss = 3.05242\n",
      "epoch no.4 train no.293120  loss = 4.09615 avg_loss = 3.01604\n",
      "epoch no.4 train no.293130  loss = 1.93504 avg_loss = 3.04759\n",
      "epoch no.4 train no.293140  loss = 3.93917 avg_loss = 3.05979\n",
      "epoch no.4 train no.293150  loss = 2.69257 avg_loss = 3.02658\n",
      "epoch no.4 train no.293160  loss = 4.14484 avg_loss = 3.02843\n",
      "epoch no.4 train no.293170  loss = 3.22562 avg_loss = 3.04528\n",
      "epoch no.4 train no.293180  loss = 4.62815 avg_loss = 3.06898\n",
      "epoch no.4 train no.293190  loss = 2.33564 avg_loss = 3.04236\n",
      "epoch no.4 train no.293200  loss = 2.39368 avg_loss = 3.04812\n",
      "epoch no.4 train no.293210  loss = 2.57432 avg_loss = 3.08809\n",
      "epoch no.4 train no.293220  loss = 4.03094 avg_loss = 3.10550\n",
      "epoch no.4 train no.293230  loss = 3.37903 avg_loss = 3.11281\n",
      "epoch no.4 train no.293240  loss = 2.32755 avg_loss = 3.08039\n",
      "epoch no.4 train no.293250  loss = 2.77369 avg_loss = 3.09968\n",
      "epoch no.4 train no.293260  loss = 2.80568 avg_loss = 3.10269\n",
      "epoch no.4 train no.293270  loss = 2.42175 avg_loss = 3.09299\n",
      "epoch no.4 train no.293280  loss = 2.45725 avg_loss = 3.06609\n",
      "epoch no.4 train no.293290  loss = 2.61396 avg_loss = 3.06955\n",
      "epoch no.4 train no.293300  loss = 1.91253 avg_loss = 3.08190\n",
      "epoch no.4 train no.293310  loss = 2.94187 avg_loss = 3.09527\n",
      "epoch no.4 train no.293320  loss = 3.33946 avg_loss = 3.07482\n",
      "epoch no.4 train no.293330  loss = 3.27259 avg_loss = 3.09159\n",
      "epoch no.4 train no.293340  loss = 3.58244 avg_loss = 3.09064\n",
      "epoch no.4 train no.293350  loss = 2.25533 avg_loss = 3.08489\n",
      "epoch no.4 train no.293360  loss = 1.57918 avg_loss = 3.05275\n",
      "epoch no.4 train no.293370  loss = 3.06381 avg_loss = 3.03510\n",
      "epoch no.4 train no.293380  loss = 2.73520 avg_loss = 3.03200\n",
      "epoch no.4 train no.293390  loss = 4.41563 avg_loss = 3.07576\n",
      "epoch no.4 train no.293400  loss = 3.01005 avg_loss = 3.11455\n",
      "epoch no.4 train no.293410  loss = 3.84497 avg_loss = 3.09398\n",
      "epoch no.4 train no.293420  loss = 3.06794 avg_loss = 3.10847\n",
      "epoch no.4 train no.293430  loss = 3.39372 avg_loss = 3.08676\n",
      "epoch no.4 train no.293440  loss = 4.40652 avg_loss = 3.06771\n",
      "epoch no.4 train no.293450  loss = 2.64578 avg_loss = 3.04874\n",
      "epoch no.4 train no.293460  loss = 1.44236 avg_loss = 3.02607\n",
      "epoch no.4 train no.293470  loss = 3.33653 avg_loss = 3.09092\n",
      "epoch no.4 train no.293480  loss = 2.43775 avg_loss = 3.06825\n",
      "epoch no.4 train no.293490  loss = 2.52674 avg_loss = 3.00635\n",
      "epoch no.4 train no.293500  loss = 3.24683 avg_loss = 3.01302\n",
      "epoch no.4 train no.293510  loss = 3.82609 avg_loss = 3.02020\n",
      "epoch no.4 train no.293520  loss = 4.22756 avg_loss = 3.02833\n",
      "epoch no.4 train no.293530  loss = 2.73837 avg_loss = 3.01649\n",
      "epoch no.4 train no.293540  loss = 2.91342 avg_loss = 2.96637\n",
      "epoch no.4 train no.293550  loss = 2.42663 avg_loss = 2.96037\n",
      "epoch no.4 train no.293560  loss = 4.37819 avg_loss = 2.94200\n",
      "epoch no.4 train no.293570  loss = 4.38174 avg_loss = 2.93634\n",
      "epoch no.4 train no.293580  loss = 2.71337 avg_loss = 2.97170\n",
      "epoch no.4 train no.293590  loss = 3.41035 avg_loss = 2.97381\n",
      "epoch no.4 train no.293600  loss = 4.82539 avg_loss = 3.00646\n",
      "epoch no.4 train no.293610  loss = 2.61955 avg_loss = 3.00511\n",
      "epoch no.4 train no.293620  loss = 3.13013 avg_loss = 3.01818\n",
      "epoch no.4 train no.293630  loss = 1.17052 avg_loss = 2.96270\n",
      "epoch no.4 train no.293640  loss = 4.13060 avg_loss = 2.98215\n",
      "epoch no.4 train no.293650  loss = 3.03117 avg_loss = 3.00619\n",
      "epoch no.4 train no.293660  loss = 2.26302 avg_loss = 3.03184\n",
      "epoch no.4 train no.293670  loss = 4.01296 avg_loss = 3.05868\n",
      "epoch no.4 train no.293680  loss = 1.45333 avg_loss = 3.05329\n",
      "epoch no.4 train no.293690  loss = 2.08971 avg_loss = 3.08095\n",
      "epoch no.4 train no.293700  loss = 2.96332 avg_loss = 3.09134\n",
      "epoch no.4 train no.293710  loss = 2.38465 avg_loss = 3.07756\n",
      "epoch no.4 train no.293720  loss = 2.87153 avg_loss = 3.11899\n",
      "epoch no.4 train no.293730  loss = 2.29866 avg_loss = 3.15003\n",
      "epoch no.4 train no.293740  loss = 6.04193 avg_loss = 3.17404\n",
      "epoch no.4 train no.293750  loss = 3.44025 avg_loss = 3.14882\n",
      "epoch no.4 train no.293760  loss = 2.61948 avg_loss = 3.09760\n",
      "epoch no.4 train no.293770  loss = 3.33886 avg_loss = 3.08588\n",
      "epoch no.4 train no.293780  loss = 2.34131 avg_loss = 3.06374\n",
      "epoch no.4 train no.293790  loss = 3.05342 avg_loss = 3.04768\n",
      "epoch no.4 train no.293800  loss = 3.01263 avg_loss = 3.00876\n",
      "epoch no.4 train no.293810  loss = 4.11443 avg_loss = 3.02727\n",
      "epoch no.4 train no.293820  loss = 3.83901 avg_loss = 3.07691\n",
      "epoch no.4 train no.293830  loss = 3.25932 avg_loss = 3.04368\n",
      "epoch no.4 train no.293840  loss = 2.33928 avg_loss = 3.08371\n",
      "epoch no.4 train no.293850  loss = 4.43890 avg_loss = 3.08279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.293860  loss = 3.67126 avg_loss = 3.07005\n",
      "epoch no.4 train no.293870  loss = 2.66880 avg_loss = 3.09896\n",
      "epoch no.4 train no.293880  loss = 3.88197 avg_loss = 3.10997\n",
      "epoch no.4 train no.293890  loss = 2.99603 avg_loss = 3.12359\n",
      "epoch no.4 train no.293900  loss = 3.10906 avg_loss = 3.13374\n",
      "epoch no.4 train no.293910  loss = 2.08807 avg_loss = 3.12371\n",
      "epoch no.4 train no.293920  loss = 2.84818 avg_loss = 3.11812\n",
      "epoch no.4 train no.293930  loss = 2.71272 avg_loss = 3.10435\n",
      "epoch no.4 train no.293940  loss = 2.56089 avg_loss = 3.06844\n",
      "epoch no.4 train no.293950  loss = 2.42505 avg_loss = 3.05926\n",
      "epoch no.4 train no.293960  loss = 2.01612 avg_loss = 3.05464\n",
      "epoch no.4 train no.293970  loss = 3.32006 avg_loss = 3.04776\n",
      "epoch no.4 train no.293980  loss = 2.56016 avg_loss = 3.04003\n",
      "epoch no.4 train no.293990  loss = 2.86901 avg_loss = 3.03687\n",
      "epoch no.4 train no.294000  loss = 4.88124 avg_loss = 3.03209\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.294010  loss = 2.81699 avg_loss = 3.01187\n",
      "epoch no.4 train no.294020  loss = 4.35279 avg_loss = 3.07143\n",
      "epoch no.4 train no.294030  loss = 3.21295 avg_loss = 3.08392\n",
      "epoch no.4 train no.294040  loss = 3.36785 avg_loss = 3.09593\n",
      "epoch no.4 train no.294050  loss = 3.75391 avg_loss = 3.12209\n",
      "epoch no.4 train no.294060  loss = 4.43475 avg_loss = 3.15088\n",
      "epoch no.4 train no.294070  loss = 3.01420 avg_loss = 3.16845\n",
      "epoch no.4 train no.294080  loss = 2.33195 avg_loss = 3.16848\n",
      "epoch no.4 train no.294090  loss = 2.53225 avg_loss = 3.14479\n",
      "epoch no.4 train no.294100  loss = 3.13209 avg_loss = 3.12547\n",
      "epoch no.4 train no.294110  loss = 2.98206 avg_loss = 3.14098\n",
      "epoch no.4 train no.294120  loss = 3.54246 avg_loss = 3.10131\n",
      "epoch no.4 train no.294130  loss = 4.12160 avg_loss = 3.09740\n",
      "epoch no.4 train no.294140  loss = 3.05483 avg_loss = 3.07238\n",
      "epoch no.4 train no.294150  loss = 5.56716 avg_loss = 3.05608\n",
      "epoch no.4 train no.294160  loss = 2.80127 avg_loss = 3.00089\n",
      "epoch no.4 train no.294170  loss = 2.30305 avg_loss = 2.97916\n",
      "epoch no.4 train no.294180  loss = 1.84787 avg_loss = 3.00583\n",
      "epoch no.4 train no.294190  loss = 1.74599 avg_loss = 2.96801\n",
      "epoch no.4 train no.294200  loss = 2.77589 avg_loss = 2.97978\n",
      "epoch no.4 train no.294210  loss = 2.99129 avg_loss = 2.98398\n",
      "epoch no.4 train no.294220  loss = 2.31828 avg_loss = 2.96814\n",
      "epoch no.4 train no.294230  loss = 3.26833 avg_loss = 2.94414\n",
      "epoch no.4 train no.294240  loss = 4.22451 avg_loss = 2.96826\n",
      "epoch no.4 train no.294250  loss = 3.98193 avg_loss = 2.97957\n",
      "epoch no.4 train no.294260  loss = 3.03110 avg_loss = 2.97585\n",
      "epoch no.4 train no.294270  loss = 3.01357 avg_loss = 3.00367\n",
      "epoch no.4 train no.294280  loss = 2.53277 avg_loss = 2.97835\n",
      "epoch no.4 train no.294290  loss = 2.45953 avg_loss = 3.00546\n",
      "epoch no.4 train no.294300  loss = 3.49466 avg_loss = 3.00183\n",
      "epoch no.4 train no.294310  loss = 2.30629 avg_loss = 2.96029\n",
      "epoch no.4 train no.294320  loss = 1.93320 avg_loss = 2.95222\n",
      "epoch no.4 train no.294330  loss = 3.63871 avg_loss = 2.96921\n",
      "epoch no.4 train no.294340  loss = 1.86869 avg_loss = 2.98070\n",
      "epoch no.4 train no.294350  loss = 4.19666 avg_loss = 2.99655\n",
      "epoch no.4 train no.294360  loss = 2.36755 avg_loss = 3.03841\n",
      "epoch no.4 train no.294370  loss = 2.54558 avg_loss = 3.04426\n",
      "epoch no.4 train no.294380  loss = 2.52042 avg_loss = 3.01204\n",
      "epoch no.4 train no.294390  loss = 2.17992 avg_loss = 3.01379\n",
      "epoch no.4 train no.294400  loss = 2.77945 avg_loss = 3.02096\n",
      "epoch no.4 train no.294410  loss = 3.83461 avg_loss = 3.05582\n",
      "epoch no.4 train no.294420  loss = 2.36303 avg_loss = 3.05309\n",
      "epoch no.4 train no.294430  loss = 3.58455 avg_loss = 3.07448\n",
      "epoch no.4 train no.294440  loss = 4.39353 avg_loss = 3.07777\n",
      "epoch no.4 train no.294450  loss = 3.26011 avg_loss = 3.13156\n",
      "epoch no.4 train no.294460  loss = 3.06965 avg_loss = 3.13102\n",
      "epoch no.4 train no.294470  loss = 3.05979 avg_loss = 3.08650\n",
      "epoch no.4 train no.294480  loss = 3.47221 avg_loss = 3.10516\n",
      "epoch no.4 train no.294490  loss = 2.77215 avg_loss = 3.08223\n",
      "epoch no.4 train no.294500  loss = 3.55392 avg_loss = 3.07776\n",
      "epoch no.4 train no.294510  loss = 4.25352 avg_loss = 3.09390\n",
      "epoch no.4 train no.294520  loss = 1.97330 avg_loss = 3.10279\n",
      "epoch no.4 train no.294530  loss = 4.45952 avg_loss = 3.11735\n",
      "epoch no.4 train no.294540  loss = 2.89327 avg_loss = 3.09861\n",
      "epoch no.4 train no.294550  loss = 3.79430 avg_loss = 3.07866\n",
      "epoch no.4 train no.294560  loss = 1.55356 avg_loss = 3.03856\n",
      "epoch no.4 train no.294570  loss = 3.01086 avg_loss = 3.01284\n",
      "epoch no.4 train no.294580  loss = 3.25603 avg_loss = 2.98997\n",
      "epoch no.4 train no.294590  loss = 4.63012 avg_loss = 2.98697\n",
      "epoch no.4 train no.294600  loss = 3.72358 avg_loss = 2.94653\n",
      "epoch no.4 train no.294610  loss = 2.15781 avg_loss = 2.91892\n",
      "epoch no.4 train no.294620  loss = 3.90611 avg_loss = 2.94554\n",
      "epoch no.4 train no.294630  loss = 4.41108 avg_loss = 2.97258\n",
      "epoch no.4 train no.294640  loss = 2.25079 avg_loss = 3.00719\n",
      "epoch no.4 train no.294650  loss = 3.89852 avg_loss = 3.02474\n",
      "epoch no.4 train no.294660  loss = 3.54052 avg_loss = 3.05496\n",
      "epoch no.4 train no.294670  loss = 3.64628 avg_loss = 3.04939\n",
      "epoch no.4 train no.294680  loss = 3.23062 avg_loss = 3.04733\n",
      "epoch no.4 train no.294690  loss = 3.87854 avg_loss = 3.08165\n",
      "epoch no.4 train no.294700  loss = 1.97867 avg_loss = 3.05846\n",
      "epoch no.4 train no.294710  loss = 1.97367 avg_loss = 3.02631\n",
      "epoch no.4 train no.294720  loss = 2.39389 avg_loss = 2.98256\n",
      "epoch no.4 train no.294730  loss = 2.47997 avg_loss = 2.98198\n",
      "epoch no.4 train no.294740  loss = 3.61928 avg_loss = 3.03285\n",
      "epoch no.4 train no.294750  loss = 4.44137 avg_loss = 3.07114\n",
      "epoch no.4 train no.294760  loss = 2.32119 avg_loss = 3.07656\n",
      "epoch no.4 train no.294770  loss = 3.62996 avg_loss = 3.06776\n",
      "epoch no.4 train no.294780  loss = 4.01163 avg_loss = 3.08942\n",
      "epoch no.4 train no.294790  loss = 3.37298 avg_loss = 3.08017\n",
      "epoch no.4 train no.294800  loss = 3.85652 avg_loss = 3.06853\n",
      "epoch no.4 train no.294810  loss = 2.62060 avg_loss = 3.07866\n",
      "epoch no.4 train no.294820  loss = 1.61295 avg_loss = 3.07261\n",
      "epoch no.4 train no.294830  loss = 3.41020 avg_loss = 3.09250\n",
      "epoch no.4 train no.294840  loss = 3.62382 avg_loss = 3.13513\n",
      "epoch no.4 train no.294850  loss = 2.49802 avg_loss = 3.13479\n",
      "epoch no.4 train no.294860  loss = 1.15525 avg_loss = 3.08270\n",
      "epoch no.4 train no.294870  loss = 3.36284 avg_loss = 3.14255\n",
      "epoch no.4 train no.294880  loss = 3.80352 avg_loss = 3.15659\n",
      "epoch no.4 train no.294890  loss = 3.30732 avg_loss = 3.15706\n",
      "epoch no.4 train no.294900  loss = 3.25545 avg_loss = 3.13986\n",
      "epoch no.4 train no.294910  loss = 2.77324 avg_loss = 3.16075\n",
      "epoch no.4 train no.294920  loss = 2.63532 avg_loss = 3.14869\n",
      "epoch no.4 train no.294930  loss = 4.18340 avg_loss = 3.15181\n",
      "epoch no.4 train no.294940  loss = 2.60149 avg_loss = 3.16220\n",
      "epoch no.4 train no.294950  loss = 1.71511 avg_loss = 3.12803\n",
      "epoch no.4 train no.294960  loss = 3.70314 avg_loss = 3.14482\n",
      "epoch no.4 train no.294970  loss = 3.78557 avg_loss = 3.12866\n",
      "epoch no.4 train no.294980  loss = 3.42714 avg_loss = 3.15645\n",
      "epoch no.4 train no.294990  loss = 2.75572 avg_loss = 3.12151\n",
      "epoch no.4 train no.295000  loss = 2.42563 avg_loss = 3.12392\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '에', '▁재즈', '같은', '▁노래', '들']\n",
      "여름밤의 꿈같은 노래</s>\n",
      "epoch no.4 train no.295010  loss = 2.80806 avg_loss = 3.08779\n",
      "epoch no.4 train no.295020  loss = 2.94223 avg_loss = 3.08593\n",
      "epoch no.4 train no.295030  loss = 2.85406 avg_loss = 3.07302\n",
      "epoch no.4 train no.295040  loss = 1.45182 avg_loss = 3.08153\n",
      "epoch no.4 train no.295050  loss = 4.36073 avg_loss = 3.08347\n",
      "epoch no.4 train no.295060  loss = 3.32116 avg_loss = 3.08155\n",
      "epoch no.4 train no.295070  loss = 1.50192 avg_loss = 3.10345\n",
      "epoch no.4 train no.295080  loss = 3.28615 avg_loss = 3.09078\n",
      "epoch no.4 train no.295090  loss = 1.67124 avg_loss = 3.06706\n",
      "epoch no.4 train no.295100  loss = 2.15666 avg_loss = 3.07076\n",
      "epoch no.4 train no.295110  loss = 2.71759 avg_loss = 3.12019\n",
      "epoch no.4 train no.295120  loss = 1.69065 avg_loss = 3.05577\n",
      "epoch no.4 train no.295130  loss = 3.21892 avg_loss = 3.09939\n",
      "epoch no.4 train no.295140  loss = 2.26245 avg_loss = 3.06681\n",
      "epoch no.4 train no.295150  loss = 4.99953 avg_loss = 3.07454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.295160  loss = 3.29373 avg_loss = 3.04558\n",
      "epoch no.4 train no.295170  loss = 2.53273 avg_loss = 3.08720\n",
      "epoch no.4 train no.295180  loss = 1.71272 avg_loss = 3.07420\n",
      "epoch no.4 train no.295190  loss = 4.22054 avg_loss = 3.05788\n",
      "epoch no.4 train no.295200  loss = 3.37522 avg_loss = 3.03214\n",
      "epoch no.4 train no.295210  loss = 1.90164 avg_loss = 2.98190\n",
      "epoch no.4 train no.295220  loss = 2.12026 avg_loss = 2.99640\n",
      "epoch no.4 train no.295230  loss = 1.82730 avg_loss = 3.01351\n",
      "epoch no.4 train no.295240  loss = 3.95429 avg_loss = 3.02224\n",
      "epoch no.4 train no.295250  loss = 3.66169 avg_loss = 3.06743\n",
      "epoch no.4 train no.295260  loss = 3.78378 avg_loss = 3.09550\n",
      "epoch no.4 train no.295270  loss = 3.84752 avg_loss = 3.08755\n",
      "epoch no.4 train no.295280  loss = 2.66445 avg_loss = 3.00344\n",
      "epoch no.4 train no.295290  loss = 3.21186 avg_loss = 3.00322\n",
      "epoch no.4 train no.295300  loss = 2.08720 avg_loss = 2.97527\n",
      "epoch no.4 train no.295310  loss = 1.99421 avg_loss = 2.93460\n",
      "epoch no.4 train no.295320  loss = 3.30413 avg_loss = 2.92709\n",
      "epoch no.4 train no.295330  loss = 3.87750 avg_loss = 2.96126\n",
      "epoch no.4 train no.295340  loss = 2.84109 avg_loss = 2.98101\n",
      "epoch no.4 train no.295350  loss = 2.41333 avg_loss = 2.97882\n",
      "epoch no.4 train no.295360  loss = 2.05190 avg_loss = 2.92085\n",
      "epoch no.4 train no.295370  loss = 3.09501 avg_loss = 2.91713\n",
      "epoch no.4 train no.295380  loss = 2.84970 avg_loss = 2.92443\n",
      "epoch no.4 train no.295390  loss = 2.38436 avg_loss = 2.94472\n",
      "epoch no.4 train no.295400  loss = 3.25405 avg_loss = 2.95598\n",
      "epoch no.4 train no.295410  loss = 2.47094 avg_loss = 2.91771\n",
      "epoch no.4 train no.295420  loss = 4.23074 avg_loss = 2.90957\n",
      "epoch no.4 train no.295430  loss = 2.36497 avg_loss = 2.93576\n",
      "epoch no.4 train no.295440  loss = 2.64867 avg_loss = 2.98771\n",
      "epoch no.4 train no.295450  loss = 2.82731 avg_loss = 3.00474\n",
      "epoch no.4 train no.295460  loss = 2.94266 avg_loss = 3.04054\n",
      "epoch no.4 train no.295470  loss = 4.21246 avg_loss = 3.07640\n",
      "epoch no.4 train no.295480  loss = 3.21218 avg_loss = 3.09807\n",
      "epoch no.4 train no.295490  loss = 4.06244 avg_loss = 3.10172\n",
      "epoch no.4 train no.295500  loss = 5.56774 avg_loss = 3.13912\n",
      "epoch no.4 train no.295510  loss = 3.81341 avg_loss = 3.12194\n",
      "epoch no.4 train no.295520  loss = 3.46973 avg_loss = 3.09145\n",
      "epoch no.4 train no.295530  loss = 2.38700 avg_loss = 3.04270\n",
      "epoch no.4 train no.295540  loss = 2.83512 avg_loss = 3.03127\n",
      "epoch no.4 train no.295550  loss = 3.79090 avg_loss = 3.06840\n",
      "epoch no.4 train no.295560  loss = 3.35288 avg_loss = 3.10734\n",
      "epoch no.4 train no.295570  loss = 3.37197 avg_loss = 3.12880\n",
      "epoch no.4 train no.295580  loss = 3.11539 avg_loss = 3.06558\n",
      "epoch no.4 train no.295590  loss = 2.28866 avg_loss = 3.03141\n",
      "epoch no.4 train no.295600  loss = 2.83039 avg_loss = 3.02191\n",
      "epoch no.4 train no.295610  loss = 2.25530 avg_loss = 2.99720\n",
      "epoch no.4 train no.295620  loss = 2.94468 avg_loss = 3.00839\n",
      "epoch no.4 train no.295630  loss = 2.70145 avg_loss = 3.03393\n",
      "epoch no.4 train no.295640  loss = 3.28791 avg_loss = 3.03282\n",
      "epoch no.4 train no.295650  loss = 5.92025 avg_loss = 3.10865\n",
      "epoch no.4 train no.295660  loss = 3.77015 avg_loss = 3.14065\n",
      "epoch no.4 train no.295670  loss = 3.47582 avg_loss = 3.15773\n",
      "epoch no.4 train no.295680  loss = 2.67570 avg_loss = 3.14689\n",
      "epoch no.4 train no.295690  loss = 3.96031 avg_loss = 3.13031\n",
      "epoch no.4 train no.295700  loss = 3.77287 avg_loss = 3.10575\n",
      "epoch no.4 train no.295710  loss = 3.86911 avg_loss = 3.10883\n",
      "epoch no.4 train no.295720  loss = 2.73788 avg_loss = 3.12323\n",
      "epoch no.4 train no.295730  loss = 2.39318 avg_loss = 3.09469\n",
      "epoch no.4 train no.295740  loss = 4.52944 avg_loss = 3.09527\n",
      "epoch no.4 train no.295750  loss = 1.94159 avg_loss = 3.09778\n",
      "epoch no.4 train no.295760  loss = 3.49867 avg_loss = 3.10780\n",
      "epoch no.4 train no.295770  loss = 3.30562 avg_loss = 3.12674\n",
      "epoch no.4 train no.295780  loss = 2.83526 avg_loss = 3.15507\n",
      "epoch no.4 train no.295790  loss = 2.64813 avg_loss = 3.15390\n",
      "epoch no.4 train no.295800  loss = 2.05370 avg_loss = 3.16120\n",
      "epoch no.4 train no.295810  loss = 2.73840 avg_loss = 3.15892\n",
      "epoch no.4 train no.295820  loss = 3.44819 avg_loss = 3.16086\n",
      "epoch no.4 train no.295830  loss = 2.97487 avg_loss = 3.15410\n",
      "epoch no.4 train no.295840  loss = 2.15145 avg_loss = 3.11281\n",
      "epoch no.4 train no.295850  loss = 2.07478 avg_loss = 3.10738\n",
      "epoch no.4 train no.295860  loss = 2.78944 avg_loss = 3.06948\n",
      "epoch no.4 train no.295870  loss = 2.81660 avg_loss = 3.03634\n",
      "epoch no.4 train no.295880  loss = 3.91218 avg_loss = 3.01884\n",
      "epoch no.4 train no.295890  loss = 3.33264 avg_loss = 3.01744\n",
      "epoch no.4 train no.295900  loss = 2.57424 avg_loss = 3.07210\n",
      "epoch no.4 train no.295910  loss = 2.82775 avg_loss = 3.07535\n",
      "epoch no.4 train no.295920  loss = 3.37792 avg_loss = 3.12505\n",
      "epoch no.4 train no.295930  loss = 2.46908 avg_loss = 3.11329\n",
      "epoch no.4 train no.295940  loss = 2.70384 avg_loss = 3.12503\n",
      "epoch no.4 train no.295950  loss = 2.72376 avg_loss = 3.09583\n",
      "epoch no.4 train no.295960  loss = 4.34500 avg_loss = 3.10405\n",
      "epoch no.4 train no.295970  loss = 3.22902 avg_loss = 3.10992\n",
      "epoch no.4 train no.295980  loss = 1.37509 avg_loss = 3.07605\n",
      "epoch no.4 train no.295990  loss = 3.94167 avg_loss = 3.08370\n",
      "epoch no.4 train no.296000  loss = 2.44800 avg_loss = 3.09174\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.296010  loss = 3.47126 avg_loss = 3.06144\n",
      "epoch no.4 train no.296020  loss = 2.70102 avg_loss = 3.08841\n",
      "epoch no.4 train no.296030  loss = 4.16741 avg_loss = 3.10335\n",
      "epoch no.4 train no.296040  loss = 2.58358 avg_loss = 3.06406\n",
      "epoch no.4 train no.296050  loss = 1.89180 avg_loss = 3.07142\n",
      "epoch no.4 train no.296060  loss = 3.68673 avg_loss = 3.05740\n",
      "epoch no.4 train no.296070  loss = 5.70115 avg_loss = 3.10118\n",
      "epoch no.4 train no.296080  loss = 2.35301 avg_loss = 3.08856\n",
      "epoch no.4 train no.296090  loss = 2.24414 avg_loss = 3.08839\n",
      "epoch no.4 train no.296100  loss = 3.09264 avg_loss = 3.09481\n",
      "epoch no.4 train no.296110  loss = 3.00752 avg_loss = 3.09918\n",
      "epoch no.4 train no.296120  loss = 4.79685 avg_loss = 3.13136\n",
      "epoch no.4 train no.296130  loss = 2.83502 avg_loss = 3.10320\n",
      "epoch no.4 train no.296140  loss = 3.23892 avg_loss = 3.13109\n",
      "epoch no.4 train no.296150  loss = 2.74465 avg_loss = 3.14209\n",
      "epoch no.4 train no.296160  loss = 3.65054 avg_loss = 3.19001\n",
      "epoch no.4 train no.296170  loss = 2.99137 avg_loss = 3.16475\n",
      "epoch no.4 train no.296180  loss = 4.26349 avg_loss = 3.18746\n",
      "epoch no.4 train no.296190  loss = 1.86801 avg_loss = 3.15136\n",
      "epoch no.4 train no.296200  loss = 2.97668 avg_loss = 3.13634\n",
      "epoch no.4 train no.296210  loss = 3.06588 avg_loss = 3.16226\n",
      "epoch no.4 train no.296220  loss = 2.49779 avg_loss = 3.18282\n",
      "epoch no.4 train no.296230  loss = 3.33407 avg_loss = 3.20517\n",
      "epoch no.4 train no.296240  loss = 2.51972 avg_loss = 3.18579\n",
      "epoch no.4 train no.296250  loss = 2.44008 avg_loss = 3.16224\n",
      "epoch no.4 train no.296260  loss = 3.20151 avg_loss = 3.13630\n",
      "epoch no.4 train no.296270  loss = 3.60719 avg_loss = 3.16691\n",
      "epoch no.4 train no.296280  loss = 2.41664 avg_loss = 3.12810\n",
      "epoch no.4 train no.296290  loss = 2.17630 avg_loss = 3.11545\n",
      "epoch no.4 train no.296300  loss = 3.22196 avg_loss = 3.14256\n",
      "epoch no.4 train no.296310  loss = 3.38372 avg_loss = 3.10347\n",
      "epoch no.4 train no.296320  loss = 3.10926 avg_loss = 3.09105\n",
      "epoch no.4 train no.296330  loss = 3.75491 avg_loss = 3.11540\n",
      "epoch no.4 train no.296340  loss = 2.09063 avg_loss = 3.15049\n",
      "epoch no.4 train no.296350  loss = 3.03070 avg_loss = 3.16563\n",
      "epoch no.4 train no.296360  loss = 2.91317 avg_loss = 3.16002\n",
      "epoch no.4 train no.296370  loss = 3.93190 avg_loss = 3.18738\n",
      "epoch no.4 train no.296380  loss = 2.65917 avg_loss = 3.18213\n",
      "epoch no.4 train no.296390  loss = 4.69479 avg_loss = 3.13822\n",
      "epoch no.4 train no.296400  loss = 3.48418 avg_loss = 3.12376\n",
      "epoch no.4 train no.296410  loss = 2.66591 avg_loss = 3.07755\n",
      "epoch no.4 train no.296420  loss = 1.60354 avg_loss = 3.06313\n",
      "epoch no.4 train no.296430  loss = 2.36103 avg_loss = 3.03020\n",
      "epoch no.4 train no.296440  loss = 3.13021 avg_loss = 3.02781\n",
      "epoch no.4 train no.296450  loss = 2.15397 avg_loss = 3.02375\n",
      "epoch no.4 train no.296460  loss = 2.54470 avg_loss = 3.01317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.296470  loss = 2.83250 avg_loss = 3.02546\n",
      "epoch no.4 train no.296480  loss = 4.33049 avg_loss = 3.00357\n",
      "epoch no.4 train no.296490  loss = 2.40861 avg_loss = 3.05508\n",
      "epoch no.4 train no.296500  loss = 2.77215 avg_loss = 3.02600\n",
      "epoch no.4 train no.296510  loss = 2.93247 avg_loss = 3.00033\n",
      "epoch no.4 train no.296520  loss = 4.22130 avg_loss = 2.99323\n",
      "epoch no.4 train no.296530  loss = 2.98734 avg_loss = 2.99670\n",
      "epoch no.4 train no.296540  loss = 2.57591 avg_loss = 2.97613\n",
      "epoch no.4 train no.296550  loss = 3.81630 avg_loss = 3.01483\n",
      "epoch no.4 train no.296560  loss = 1.97055 avg_loss = 2.96766\n",
      "epoch no.4 train no.296570  loss = 1.67062 avg_loss = 2.97370\n",
      "epoch no.4 train no.296580  loss = 4.91652 avg_loss = 2.98903\n",
      "epoch no.4 train no.296590  loss = 3.48557 avg_loss = 3.03824\n",
      "epoch no.4 train no.296600  loss = 2.44689 avg_loss = 3.02216\n",
      "epoch no.4 train no.296610  loss = 3.52473 avg_loss = 3.02526\n",
      "epoch no.4 train no.296620  loss = 2.66083 avg_loss = 3.03351\n",
      "epoch no.4 train no.296630  loss = 1.50227 avg_loss = 2.99516\n",
      "epoch no.4 train no.296640  loss = 2.97406 avg_loss = 2.99715\n",
      "epoch no.4 train no.296650  loss = 3.46517 avg_loss = 2.99209\n",
      "epoch no.4 train no.296660  loss = 2.61058 avg_loss = 2.96948\n",
      "epoch no.4 train no.296670  loss = 4.50286 avg_loss = 3.02154\n",
      "epoch no.4 train no.296680  loss = 3.53956 avg_loss = 3.03660\n",
      "epoch no.4 train no.296690  loss = 2.93813 avg_loss = 3.06290\n",
      "epoch no.4 train no.296700  loss = 3.70045 avg_loss = 3.08021\n",
      "epoch no.4 train no.296710  loss = 2.64437 avg_loss = 3.04238\n",
      "epoch no.4 train no.296720  loss = 3.21633 avg_loss = 3.07726\n",
      "epoch no.4 train no.296730  loss = 2.58420 avg_loss = 3.09101\n",
      "epoch no.4 train no.296740  loss = 3.24470 avg_loss = 3.10206\n",
      "epoch no.4 train no.296750  loss = 2.85810 avg_loss = 3.13507\n",
      "epoch no.4 train no.296760  loss = 3.19345 avg_loss = 3.14366\n",
      "epoch no.4 train no.296770  loss = 3.10585 avg_loss = 3.19393\n",
      "epoch no.4 train no.296780  loss = 2.36313 avg_loss = 3.17922\n",
      "epoch no.4 train no.296790  loss = 3.17621 avg_loss = 3.17107\n",
      "epoch no.4 train no.296800  loss = 2.15091 avg_loss = 3.13679\n",
      "epoch no.4 train no.296810  loss = 2.92149 avg_loss = 3.12219\n",
      "epoch no.4 train no.296820  loss = 1.83595 avg_loss = 3.09338\n",
      "epoch no.4 train no.296830  loss = 2.91672 avg_loss = 3.13276\n",
      "epoch no.4 train no.296840  loss = 2.23887 avg_loss = 3.12297\n",
      "epoch no.4 train no.296850  loss = 3.09154 avg_loss = 3.11651\n",
      "epoch no.4 train no.296860  loss = 1.83341 avg_loss = 3.11883\n",
      "epoch no.4 train no.296870  loss = 3.43145 avg_loss = 3.06432\n",
      "epoch no.4 train no.296880  loss = 4.06114 avg_loss = 3.08689\n",
      "epoch no.4 train no.296890  loss = 1.52146 avg_loss = 3.05146\n",
      "epoch no.4 train no.296900  loss = 3.95229 avg_loss = 3.01418\n",
      "epoch no.4 train no.296910  loss = 3.73534 avg_loss = 3.00112\n",
      "epoch no.4 train no.296920  loss = 4.30403 avg_loss = 3.00768\n",
      "epoch no.4 train no.296930  loss = 2.89483 avg_loss = 2.99798\n",
      "epoch no.4 train no.296940  loss = 1.65909 avg_loss = 3.02322\n",
      "epoch no.4 train no.296950  loss = 3.24634 avg_loss = 3.05215\n",
      "epoch no.4 train no.296960  loss = 1.18185 avg_loss = 3.04356\n",
      "epoch no.4 train no.296970  loss = 2.90227 avg_loss = 3.06892\n",
      "epoch no.4 train no.296980  loss = 3.63847 avg_loss = 3.11285\n",
      "epoch no.4 train no.296990  loss = 2.75277 avg_loss = 3.09182\n",
      "epoch no.4 train no.297000  loss = 2.74541 avg_loss = 3.07760\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁어울리는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.297010  loss = 2.57356 avg_loss = 3.05519\n",
      "epoch no.4 train no.297020  loss = 4.87560 avg_loss = 3.06495\n",
      "epoch no.4 train no.297030  loss = 3.39530 avg_loss = 3.06010\n",
      "epoch no.4 train no.297040  loss = 3.01744 avg_loss = 3.05312\n",
      "epoch no.4 train no.297050  loss = 3.14143 avg_loss = 3.02653\n",
      "epoch no.4 train no.297060  loss = 1.43291 avg_loss = 3.00643\n",
      "epoch no.4 train no.297070  loss = 2.35568 avg_loss = 2.94528\n",
      "epoch no.4 train no.297080  loss = 1.84219 avg_loss = 2.96268\n",
      "epoch no.4 train no.297090  loss = 2.29466 avg_loss = 2.96040\n",
      "epoch no.4 train no.297100  loss = 5.11281 avg_loss = 3.02345\n",
      "epoch no.4 train no.297110  loss = 2.27116 avg_loss = 2.98849\n",
      "epoch no.4 train no.297120  loss = 2.77688 avg_loss = 3.01084\n",
      "epoch no.4 train no.297130  loss = 3.14177 avg_loss = 3.04921\n",
      "epoch no.4 train no.297140  loss = 2.35115 avg_loss = 3.02265\n",
      "epoch no.4 train no.297150  loss = 3.03054 avg_loss = 3.05787\n",
      "epoch no.4 train no.297160  loss = 1.95325 avg_loss = 3.09022\n",
      "epoch no.4 train no.297170  loss = 3.07844 avg_loss = 3.12551\n",
      "epoch no.4 train no.297180  loss = 3.07213 avg_loss = 3.15971\n",
      "epoch no.4 train no.297190  loss = 4.53705 avg_loss = 3.15855\n",
      "epoch no.4 train no.297200  loss = 2.37501 avg_loss = 3.10679\n",
      "epoch no.4 train no.297210  loss = 3.87255 avg_loss = 3.11549\n",
      "epoch no.4 train no.297220  loss = 2.34937 avg_loss = 3.13985\n",
      "epoch no.4 train no.297230  loss = 3.16951 avg_loss = 3.11008\n",
      "epoch no.4 train no.297240  loss = 2.28835 avg_loss = 3.07883\n",
      "epoch no.4 train no.297250  loss = 4.12135 avg_loss = 3.11646\n",
      "epoch no.4 train no.297260  loss = 3.83984 avg_loss = 3.12904\n",
      "epoch no.4 train no.297270  loss = 3.84412 avg_loss = 3.14882\n",
      "epoch no.4 train no.297280  loss = 3.73472 avg_loss = 3.10976\n",
      "epoch no.4 train no.297290  loss = 2.62307 avg_loss = 3.07553\n",
      "epoch no.4 train no.297300  loss = 2.91685 avg_loss = 3.09328\n",
      "epoch no.4 train no.297310  loss = 2.01698 avg_loss = 3.06321\n",
      "epoch no.4 train no.297320  loss = 2.73583 avg_loss = 3.07787\n",
      "epoch no.4 train no.297330  loss = 3.55017 avg_loss = 3.06186\n",
      "epoch no.4 train no.297340  loss = 2.62510 avg_loss = 3.03966\n",
      "epoch no.4 train no.297350  loss = 3.24948 avg_loss = 3.10876\n",
      "epoch no.4 train no.297360  loss = 3.42690 avg_loss = 3.14252\n",
      "epoch no.4 train no.297370  loss = 2.94819 avg_loss = 3.15205\n",
      "epoch no.4 train no.297380  loss = 3.14336 avg_loss = 3.14056\n",
      "epoch no.4 train no.297390  loss = 3.06713 avg_loss = 3.15051\n",
      "epoch no.4 train no.297400  loss = 2.49950 avg_loss = 3.09881\n",
      "epoch no.4 train no.297410  loss = 2.98393 avg_loss = 3.06957\n",
      "epoch no.4 train no.297420  loss = 3.04569 avg_loss = 3.06262\n",
      "epoch no.4 train no.297430  loss = 4.30400 avg_loss = 3.07214\n",
      "epoch no.4 train no.297440  loss = 3.44369 avg_loss = 3.04082\n",
      "epoch no.4 train no.297450  loss = 3.92475 avg_loss = 3.03318\n",
      "epoch no.4 train no.297460  loss = 2.55295 avg_loss = 3.05441\n",
      "epoch no.4 train no.297470  loss = 2.34153 avg_loss = 3.04157\n",
      "epoch no.4 train no.297480  loss = 2.15339 avg_loss = 3.03090\n",
      "epoch no.4 train no.297490  loss = 4.27787 avg_loss = 3.02465\n",
      "epoch no.4 train no.297500  loss = 2.17436 avg_loss = 2.99337\n",
      "epoch no.4 train no.297510  loss = 3.89646 avg_loss = 3.00065\n",
      "epoch no.4 train no.297520  loss = 4.88610 avg_loss = 3.05163\n",
      "epoch no.4 train no.297530  loss = 5.53822 avg_loss = 3.06901\n",
      "epoch no.4 train no.297540  loss = 3.77491 avg_loss = 3.07944\n",
      "epoch no.4 train no.297550  loss = 2.50818 avg_loss = 3.10318\n",
      "epoch no.4 train no.297560  loss = 2.16163 avg_loss = 3.10877\n",
      "epoch no.4 train no.297570  loss = 2.09111 avg_loss = 3.11054\n",
      "epoch no.4 train no.297580  loss = 3.48677 avg_loss = 3.11712\n",
      "epoch no.4 train no.297590  loss = 2.30114 avg_loss = 3.10211\n",
      "epoch no.4 train no.297600  loss = 2.41761 avg_loss = 3.08620\n",
      "epoch no.4 train no.297610  loss = 1.94615 avg_loss = 3.09207\n",
      "epoch no.4 train no.297620  loss = 3.54334 avg_loss = 3.15212\n",
      "epoch no.4 train no.297630  loss = 3.40519 avg_loss = 3.12961\n",
      "epoch no.4 train no.297640  loss = 2.19518 avg_loss = 3.12251\n",
      "epoch no.4 train no.297650  loss = 2.78879 avg_loss = 3.10516\n",
      "epoch no.4 train no.297660  loss = 4.68953 avg_loss = 3.10630\n",
      "epoch no.4 train no.297670  loss = 3.21233 avg_loss = 3.11637\n",
      "epoch no.4 train no.297680  loss = 3.73446 avg_loss = 3.14028\n",
      "epoch no.4 train no.297690  loss = 4.04299 avg_loss = 3.14816\n",
      "epoch no.4 train no.297700  loss = 2.07987 avg_loss = 3.15757\n",
      "epoch no.4 train no.297710  loss = 3.40065 avg_loss = 3.15877\n",
      "epoch no.4 train no.297720  loss = 2.83752 avg_loss = 3.12989\n",
      "epoch no.4 train no.297730  loss = 3.58672 avg_loss = 3.11538\n",
      "epoch no.4 train no.297740  loss = 2.47548 avg_loss = 3.10101\n",
      "epoch no.4 train no.297750  loss = 2.92813 avg_loss = 3.11675\n",
      "epoch no.4 train no.297760  loss = 3.00512 avg_loss = 3.12054\n",
      "epoch no.4 train no.297770  loss = 3.58867 avg_loss = 3.14078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.297780  loss = 3.88654 avg_loss = 3.15800\n",
      "epoch no.4 train no.297790  loss = 3.45565 avg_loss = 3.13572\n",
      "epoch no.4 train no.297800  loss = 4.14707 avg_loss = 3.18662\n",
      "epoch no.4 train no.297810  loss = 2.19246 avg_loss = 3.15289\n",
      "epoch no.4 train no.297820  loss = 3.02239 avg_loss = 3.15194\n",
      "epoch no.4 train no.297830  loss = 2.44152 avg_loss = 3.15066\n",
      "epoch no.4 train no.297840  loss = 2.97588 avg_loss = 3.14339\n",
      "epoch no.4 train no.297850  loss = 2.57816 avg_loss = 3.07015\n",
      "epoch no.4 train no.297860  loss = 3.60889 avg_loss = 3.05809\n",
      "epoch no.4 train no.297870  loss = 3.62848 avg_loss = 3.08259\n",
      "epoch no.4 train no.297880  loss = 2.30742 avg_loss = 3.05629\n",
      "epoch no.4 train no.297890  loss = 5.03264 avg_loss = 3.06404\n",
      "epoch no.4 train no.297900  loss = 1.76025 avg_loss = 3.09399\n",
      "epoch no.4 train no.297910  loss = 2.53780 avg_loss = 3.08609\n",
      "epoch no.4 train no.297920  loss = 3.80474 avg_loss = 3.08995\n",
      "epoch no.4 train no.297930  loss = 2.36671 avg_loss = 3.10407\n",
      "epoch no.4 train no.297940  loss = 2.56041 avg_loss = 3.08382\n",
      "epoch no.4 train no.297950  loss = 3.35049 avg_loss = 3.10998\n",
      "epoch no.4 train no.297960  loss = 3.61217 avg_loss = 3.07816\n",
      "epoch no.4 train no.297970  loss = 3.54650 avg_loss = 3.06937\n",
      "epoch no.4 train no.297980  loss = 3.58170 avg_loss = 3.06696\n",
      "epoch no.4 train no.297990  loss = 1.70893 avg_loss = 3.09464\n",
      "epoch no.4 train no.298000  loss = 2.28071 avg_loss = 3.12964\n",
      "9\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓을', '▁감성', '달', '한', '▁인디', '</s>', '</s>']\n",
      "여름밤을 수놓을 달달한 음악들</s>\n",
      "epoch no.4 train no.298010  loss = 1.52069 avg_loss = 3.09640\n",
      "epoch no.4 train no.298020  loss = 2.58039 avg_loss = 3.09443\n",
      "epoch no.4 train no.298030  loss = 3.29400 avg_loss = 3.12966\n",
      "epoch no.4 train no.298040  loss = 3.53181 avg_loss = 3.10887\n",
      "epoch no.4 train no.298050  loss = 4.05665 avg_loss = 3.10632\n",
      "epoch no.4 train no.298060  loss = 1.98972 avg_loss = 3.05489\n",
      "epoch no.4 train no.298070  loss = 2.71618 avg_loss = 3.04753\n",
      "epoch no.4 train no.298080  loss = 4.02955 avg_loss = 3.06411\n",
      "epoch no.4 train no.298090  loss = 2.71285 avg_loss = 3.04972\n",
      "epoch no.4 train no.298100  loss = 2.65071 avg_loss = 3.04727\n",
      "epoch no.4 train no.298110  loss = 2.82484 avg_loss = 3.04494\n",
      "epoch no.4 train no.298120  loss = 2.63726 avg_loss = 3.03095\n",
      "epoch no.4 train no.298130  loss = 3.75056 avg_loss = 3.02688\n",
      "epoch no.4 train no.298140  loss = 1.61638 avg_loss = 3.05426\n",
      "epoch no.4 train no.298150  loss = 2.16146 avg_loss = 3.00902\n",
      "epoch no.4 train no.298160  loss = 2.51060 avg_loss = 3.02224\n",
      "epoch no.4 train no.298170  loss = 2.85265 avg_loss = 3.05765\n",
      "epoch no.4 train no.298180  loss = 2.94666 avg_loss = 3.08151\n",
      "epoch no.4 train no.298190  loss = 2.51004 avg_loss = 3.09129\n",
      "epoch no.4 train no.298200  loss = 2.69727 avg_loss = 3.07540\n",
      "epoch no.4 train no.298210  loss = 3.11240 avg_loss = 3.06413\n",
      "epoch no.4 train no.298220  loss = 1.93858 avg_loss = 3.07052\n",
      "epoch no.4 train no.298230  loss = 1.84834 avg_loss = 3.04864\n",
      "epoch no.4 train no.298240  loss = 3.74194 avg_loss = 3.06583\n",
      "epoch no.4 train no.298250  loss = 2.46450 avg_loss = 3.06465\n",
      "epoch no.4 train no.298260  loss = 4.02366 avg_loss = 3.06638\n",
      "epoch no.4 train no.298270  loss = 2.70415 avg_loss = 3.05406\n",
      "epoch no.4 train no.298280  loss = 4.46462 avg_loss = 3.10675\n",
      "epoch no.4 train no.298290  loss = 3.39760 avg_loss = 3.09211\n",
      "epoch no.4 train no.298300  loss = 3.66791 avg_loss = 3.11805\n",
      "epoch no.4 train no.298310  loss = 2.51287 avg_loss = 3.07642\n",
      "epoch no.4 train no.298320  loss = 2.95194 avg_loss = 3.03789\n",
      "epoch no.4 train no.298330  loss = 2.83903 avg_loss = 3.07303\n",
      "epoch no.4 train no.298340  loss = 3.82027 avg_loss = 3.05719\n",
      "epoch no.4 train no.298350  loss = 2.21883 avg_loss = 3.01203\n",
      "epoch no.4 train no.298360  loss = 2.31770 avg_loss = 3.01374\n",
      "epoch no.4 train no.298370  loss = 3.37358 avg_loss = 3.00404\n",
      "epoch no.4 train no.298380  loss = 1.70425 avg_loss = 2.99847\n",
      "epoch no.4 train no.298390  loss = 3.31416 avg_loss = 2.99184\n",
      "epoch no.4 train no.298400  loss = 2.43747 avg_loss = 3.10482\n",
      "epoch no.4 train no.298410  loss = 2.48928 avg_loss = 3.09942\n",
      "epoch no.4 train no.298420  loss = 3.52635 avg_loss = 3.10717\n",
      "epoch no.4 train no.298430  loss = 3.25472 avg_loss = 3.15375\n",
      "epoch no.4 train no.298440  loss = 3.83196 avg_loss = 3.12990\n",
      "epoch no.4 train no.298450  loss = 3.54281 avg_loss = 3.13072\n",
      "epoch no.4 train no.298460  loss = 2.54254 avg_loss = 3.09984\n",
      "epoch no.4 train no.298470  loss = 4.00229 avg_loss = 3.11197\n",
      "epoch no.4 train no.298480  loss = 2.29102 avg_loss = 3.12513\n",
      "epoch no.4 train no.298490  loss = 4.16781 avg_loss = 3.16328\n",
      "epoch no.4 train no.298500  loss = 4.58085 avg_loss = 3.21928\n",
      "epoch no.4 train no.298510  loss = 1.88153 avg_loss = 3.18034\n",
      "epoch no.4 train no.298520  loss = 3.94699 avg_loss = 3.14303\n",
      "epoch no.4 train no.298530  loss = 3.20551 avg_loss = 3.13349\n",
      "epoch no.4 train no.298540  loss = 2.99829 avg_loss = 3.10995\n",
      "epoch no.4 train no.298550  loss = 2.64497 avg_loss = 3.09723\n",
      "epoch no.4 train no.298560  loss = 4.87317 avg_loss = 3.09582\n",
      "epoch no.4 train no.298570  loss = 2.82309 avg_loss = 3.06591\n",
      "epoch no.4 train no.298580  loss = 2.42408 avg_loss = 3.07727\n",
      "epoch no.4 train no.298590  loss = 3.86529 avg_loss = 3.10705\n",
      "epoch no.4 train no.298600  loss = 2.77337 avg_loss = 3.11988\n",
      "epoch no.4 train no.298610  loss = 3.53507 avg_loss = 3.11941\n",
      "epoch no.4 train no.298620  loss = 4.34354 avg_loss = 3.17584\n",
      "epoch no.4 train no.298630  loss = 1.65819 avg_loss = 3.13297\n",
      "epoch no.4 train no.298640  loss = 2.09995 avg_loss = 3.09559\n",
      "epoch no.4 train no.298650  loss = 4.60663 avg_loss = 3.10509\n",
      "epoch no.4 train no.298660  loss = 3.06055 avg_loss = 3.11624\n",
      "epoch no.4 train no.298670  loss = 3.13755 avg_loss = 3.10869\n",
      "epoch no.4 train no.298680  loss = 3.79722 avg_loss = 3.09740\n",
      "epoch no.4 train no.298690  loss = 2.82818 avg_loss = 3.07959\n",
      "epoch no.4 train no.298700  loss = 2.20423 avg_loss = 3.05286\n",
      "epoch no.4 train no.298710  loss = 5.86183 avg_loss = 3.10772\n",
      "epoch no.4 train no.298720  loss = 2.86189 avg_loss = 3.10179\n",
      "epoch no.4 train no.298730  loss = 1.20564 avg_loss = 3.12660\n",
      "epoch no.4 train no.298740  loss = 3.38619 avg_loss = 3.10088\n",
      "epoch no.4 train no.298750  loss = 3.83791 avg_loss = 3.06986\n",
      "epoch no.4 train no.298760  loss = 3.53647 avg_loss = 3.05720\n",
      "epoch no.4 train no.298770  loss = 2.31396 avg_loss = 3.04824\n",
      "epoch no.4 train no.298780  loss = 2.18998 avg_loss = 3.03867\n",
      "epoch no.4 train no.298790  loss = 2.82894 avg_loss = 3.05449\n",
      "epoch no.4 train no.298800  loss = 2.76301 avg_loss = 3.06740\n",
      "epoch no.4 train no.298810  loss = 2.91621 avg_loss = 3.05266\n",
      "epoch no.4 train no.298820  loss = 2.86182 avg_loss = 3.07303\n",
      "epoch no.4 train no.298830  loss = 3.34277 avg_loss = 3.07825\n",
      "epoch no.4 train no.298840  loss = 4.07009 avg_loss = 3.12022\n",
      "epoch no.4 train no.298850  loss = 3.62275 avg_loss = 3.17103\n",
      "epoch no.4 train no.298860  loss = 2.95260 avg_loss = 3.17717\n",
      "epoch no.4 train no.298870  loss = 3.24813 avg_loss = 3.18072\n",
      "epoch no.4 train no.298880  loss = 3.46903 avg_loss = 3.17770\n",
      "epoch no.4 train no.298890  loss = 2.90443 avg_loss = 3.16981\n",
      "epoch no.4 train no.298900  loss = 2.47457 avg_loss = 3.14650\n",
      "epoch no.4 train no.298910  loss = 2.35226 avg_loss = 3.11338\n",
      "epoch no.4 train no.298920  loss = 2.77159 avg_loss = 3.09302\n",
      "epoch no.4 train no.298930  loss = 3.34372 avg_loss = 3.08926\n",
      "epoch no.4 train no.298940  loss = 3.44047 avg_loss = 3.11432\n",
      "epoch no.4 train no.298950  loss = 3.15785 avg_loss = 3.12573\n",
      "epoch no.4 train no.298960  loss = 2.16059 avg_loss = 3.08482\n",
      "epoch no.4 train no.298970  loss = 2.93406 avg_loss = 3.05811\n",
      "epoch no.4 train no.298980  loss = 2.88693 avg_loss = 3.04421\n",
      "epoch no.4 train no.298990  loss = 3.01713 avg_loss = 3.05409\n",
      "epoch no.4 train no.299000  loss = 2.56078 avg_loss = 3.03408\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '을', '▁수', '줄', '▁감성', '발', '▁노래', '</s>']\n",
      "여름밤을 달래줄 감성적인 팝</s>\n",
      "epoch no.4 train no.299010  loss = 3.03903 avg_loss = 3.04153\n",
      "epoch no.4 train no.299020  loss = 3.37851 avg_loss = 3.09730\n",
      "epoch no.4 train no.299030  loss = 2.80129 avg_loss = 3.08353\n",
      "epoch no.4 train no.299040  loss = 3.38285 avg_loss = 3.13205\n",
      "epoch no.4 train no.299050  loss = 1.97312 avg_loss = 3.09552\n",
      "epoch no.4 train no.299060  loss = 1.95198 avg_loss = 3.07227\n",
      "epoch no.4 train no.299070  loss = 2.54938 avg_loss = 3.07149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.299080  loss = 2.31545 avg_loss = 3.03868\n",
      "epoch no.4 train no.299090  loss = 2.10929 avg_loss = 3.01878\n",
      "epoch no.4 train no.299100  loss = 2.22669 avg_loss = 3.00574\n",
      "epoch no.4 train no.299110  loss = 3.03865 avg_loss = 3.04116\n",
      "epoch no.4 train no.299120  loss = 1.71011 avg_loss = 2.99832\n",
      "epoch no.4 train no.299130  loss = 3.25711 avg_loss = 3.00865\n",
      "epoch no.4 train no.299140  loss = 2.74275 avg_loss = 3.05156\n",
      "epoch no.4 train no.299150  loss = 2.47440 avg_loss = 3.11522\n",
      "epoch no.4 train no.299160  loss = 2.33768 avg_loss = 3.09686\n",
      "epoch no.4 train no.299170  loss = 2.25675 avg_loss = 3.10119\n",
      "epoch no.4 train no.299180  loss = 3.23051 avg_loss = 3.08399\n",
      "epoch no.4 train no.299190  loss = 2.78654 avg_loss = 3.04267\n",
      "epoch no.4 train no.299200  loss = 2.71078 avg_loss = 3.05469\n",
      "epoch no.4 train no.299210  loss = 2.83029 avg_loss = 3.04355\n",
      "epoch no.4 train no.299220  loss = 3.13443 avg_loss = 3.03064\n",
      "epoch no.4 train no.299230  loss = 1.13085 avg_loss = 3.01099\n",
      "epoch no.4 train no.299240  loss = 2.35095 avg_loss = 2.99055\n",
      "epoch no.4 train no.299250  loss = 1.01941 avg_loss = 2.95087\n",
      "epoch no.4 train no.299260  loss = 4.37111 avg_loss = 2.96117\n",
      "epoch no.4 train no.299270  loss = 2.23953 avg_loss = 2.93364\n",
      "epoch no.4 train no.299280  loss = 1.96347 avg_loss = 2.92750\n",
      "epoch no.4 train no.299290  loss = 2.75726 avg_loss = 2.95260\n",
      "epoch no.4 train no.299300  loss = 4.68168 avg_loss = 2.98431\n",
      "epoch no.4 train no.299310  loss = 2.63525 avg_loss = 3.02319\n",
      "epoch no.4 train no.299320  loss = 3.65332 avg_loss = 3.07881\n",
      "epoch no.4 train no.299330  loss = 3.32066 avg_loss = 3.09638\n",
      "epoch no.4 train no.299340  loss = 3.95474 avg_loss = 3.11079\n",
      "epoch no.4 train no.299350  loss = 3.43624 avg_loss = 3.08275\n",
      "epoch no.4 train no.299360  loss = 3.17023 avg_loss = 3.04708\n",
      "epoch no.4 train no.299370  loss = 2.51219 avg_loss = 3.04154\n",
      "epoch no.4 train no.299380  loss = 2.75397 avg_loss = 3.06029\n",
      "epoch no.4 train no.299390  loss = 2.35841 avg_loss = 3.05433\n",
      "epoch no.4 train no.299400  loss = 4.56482 avg_loss = 3.04527\n",
      "epoch no.4 train no.299410  loss = 2.62722 avg_loss = 3.07565\n",
      "epoch no.4 train no.299420  loss = 2.06387 avg_loss = 3.12072\n",
      "epoch no.4 train no.299430  loss = 2.50165 avg_loss = 3.04924\n",
      "epoch no.4 train no.299440  loss = 1.75588 avg_loss = 3.01817\n",
      "epoch no.4 train no.299450  loss = 3.62516 avg_loss = 3.03450\n",
      "epoch no.4 train no.299460  loss = 2.91140 avg_loss = 3.05213\n",
      "epoch no.4 train no.299470  loss = 4.95636 avg_loss = 3.08225\n",
      "epoch no.4 train no.299480  loss = 4.46816 avg_loss = 3.07952\n",
      "epoch no.4 train no.299490  loss = 2.96818 avg_loss = 3.07576\n",
      "epoch no.4 train no.299500  loss = 2.40731 avg_loss = 3.10554\n",
      "epoch no.4 train no.299510  loss = 2.69213 avg_loss = 3.09815\n",
      "epoch no.4 train no.299520  loss = 2.18590 avg_loss = 3.03824\n",
      "epoch no.4 train no.299530  loss = 3.02974 avg_loss = 3.04590\n",
      "epoch no.4 train no.299540  loss = 2.23959 avg_loss = 3.09557\n",
      "epoch no.4 train no.299550  loss = 3.42803 avg_loss = 3.10042\n",
      "epoch no.4 train no.299560  loss = 2.76460 avg_loss = 3.08483\n",
      "epoch no.4 train no.299570  loss = 2.76512 avg_loss = 3.05355\n",
      "epoch no.4 train no.299580  loss = 4.02563 avg_loss = 3.03488\n",
      "epoch no.4 train no.299590  loss = 3.20844 avg_loss = 3.04160\n",
      "epoch no.4 train no.299600  loss = 1.81379 avg_loss = 3.02387\n",
      "epoch no.4 train no.299610  loss = 2.01256 avg_loss = 3.00007\n",
      "epoch no.4 train no.299620  loss = 2.73645 avg_loss = 3.00636\n",
      "epoch no.4 train no.299630  loss = 4.12202 avg_loss = 2.99816\n",
      "epoch no.4 train no.299640  loss = 2.86094 avg_loss = 3.01414\n",
      "epoch no.4 train no.299650  loss = 3.72434 avg_loss = 3.05014\n",
      "epoch no.4 train no.299660  loss = 3.50634 avg_loss = 3.07641\n",
      "epoch no.4 train no.299670  loss = 3.32977 avg_loss = 3.04593\n",
      "epoch no.4 train no.299680  loss = 2.58670 avg_loss = 3.04735\n",
      "epoch no.4 train no.299690  loss = 2.13823 avg_loss = 3.00703\n",
      "epoch no.4 train no.299700  loss = 3.57429 avg_loss = 3.01139\n",
      "epoch no.4 train no.299710  loss = 3.46153 avg_loss = 3.01220\n",
      "epoch no.4 train no.299720  loss = 3.69366 avg_loss = 3.02523\n",
      "epoch no.4 train no.299730  loss = 2.89847 avg_loss = 3.02476\n",
      "epoch no.4 train no.299740  loss = 3.03390 avg_loss = 3.01160\n",
      "epoch no.4 train no.299750  loss = 4.49143 avg_loss = 3.08460\n",
      "epoch no.4 train no.299760  loss = 3.06310 avg_loss = 3.08762\n",
      "epoch no.4 train no.299770  loss = 1.99949 avg_loss = 3.08118\n",
      "epoch no.4 train no.299780  loss = 3.18841 avg_loss = 3.07983\n",
      "epoch no.4 train no.299790  loss = 1.99919 avg_loss = 3.00482\n",
      "epoch no.4 train no.299800  loss = 1.19023 avg_loss = 2.94545\n",
      "epoch no.4 train no.299810  loss = 2.86740 avg_loss = 2.97824\n",
      "epoch no.4 train no.299820  loss = 3.19925 avg_loss = 2.95975\n",
      "epoch no.4 train no.299830  loss = 3.86190 avg_loss = 3.01327\n",
      "epoch no.4 train no.299840  loss = 2.57714 avg_loss = 3.00159\n",
      "epoch no.4 train no.299850  loss = 3.27906 avg_loss = 3.02064\n",
      "epoch no.4 train no.299860  loss = 4.79686 avg_loss = 3.04480\n",
      "epoch no.4 train no.299870  loss = 2.74699 avg_loss = 3.05631\n",
      "epoch no.4 train no.299880  loss = 3.96390 avg_loss = 3.08955\n",
      "epoch no.4 train no.299890  loss = 4.50647 avg_loss = 3.09344\n",
      "epoch no.4 train no.299900  loss = 2.14084 avg_loss = 3.08764\n",
      "epoch no.4 train no.299910  loss = 4.03951 avg_loss = 3.10464\n",
      "epoch no.4 train no.299920  loss = 2.84735 avg_loss = 3.11695\n",
      "epoch no.4 train no.299930  loss = 2.95950 avg_loss = 3.10723\n",
      "epoch no.4 train no.299940  loss = 2.13231 avg_loss = 3.07856\n",
      "epoch no.4 train no.299950  loss = 2.96996 avg_loss = 3.12739\n",
      "epoch no.4 train no.299960  loss = 1.87931 avg_loss = 3.10968\n",
      "epoch no.4 train no.299970  loss = 3.14188 avg_loss = 3.08497\n",
      "epoch no.4 train no.299980  loss = 2.52110 avg_loss = 3.08560\n",
      "epoch no.4 train no.299990  loss = 2.36287 avg_loss = 3.06137\n",
      "epoch no.4 train no.300000  loss = 3.62854 avg_loss = 3.03566\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '▁청량', '▁댄스', '곡', '과', '▁함께', '</s>']\n",
      "여름엔 신나는  댄스음악과 함께</s>\n",
      "epoch no.4 train no.300010  loss = 3.26499 avg_loss = 3.05734\n",
      "epoch no.4 train no.300020  loss = 3.82354 avg_loss = 3.10130\n",
      "epoch no.4 train no.300030  loss = 4.46527 avg_loss = 3.14739\n",
      "epoch no.4 train no.300040  loss = 3.27657 avg_loss = 3.14298\n",
      "epoch no.4 train no.300050  loss = 2.66936 avg_loss = 3.12876\n",
      "epoch no.4 train no.300060  loss = 1.43911 avg_loss = 3.10147\n",
      "epoch no.4 train no.300070  loss = 3.18334 avg_loss = 3.09108\n",
      "epoch no.4 train no.300080  loss = 3.60476 avg_loss = 3.09853\n",
      "epoch no.4 train no.300090  loss = 3.46353 avg_loss = 3.10506\n",
      "epoch no.4 train no.300100  loss = 2.70347 avg_loss = 3.13920\n",
      "epoch no.4 train no.300110  loss = 3.26581 avg_loss = 3.11039\n",
      "epoch no.4 train no.300120  loss = 3.16261 avg_loss = 3.10022\n",
      "epoch no.4 train no.300130  loss = 2.53116 avg_loss = 3.08673\n",
      "epoch no.4 train no.300140  loss = 1.50172 avg_loss = 3.09334\n",
      "epoch no.4 train no.300150  loss = 2.77029 avg_loss = 3.08211\n",
      "epoch no.4 train no.300160  loss = 2.51385 avg_loss = 3.09303\n",
      "epoch no.4 train no.300170  loss = 2.56082 avg_loss = 3.09997\n",
      "epoch no.4 train no.300180  loss = 2.15774 avg_loss = 3.07131\n",
      "epoch no.4 train no.300190  loss = 2.59183 avg_loss = 3.09717\n",
      "epoch no.4 train no.300200  loss = 3.16074 avg_loss = 3.08867\n",
      "epoch no.4 train no.300210  loss = 2.70582 avg_loss = 3.05952\n",
      "epoch no.4 train no.300220  loss = 2.27891 avg_loss = 3.03890\n",
      "epoch no.4 train no.300230  loss = 2.95503 avg_loss = 2.99564\n",
      "epoch no.4 train no.300240  loss = 3.17130 avg_loss = 3.02331\n",
      "epoch no.4 train no.300250  loss = 2.68924 avg_loss = 3.03050\n",
      "epoch no.4 train no.300260  loss = 1.44882 avg_loss = 2.97538\n",
      "epoch no.4 train no.300270  loss = 2.66556 avg_loss = 2.99021\n",
      "epoch no.4 train no.300280  loss = 3.64080 avg_loss = 2.96624\n",
      "epoch no.4 train no.300290  loss = 1.02751 avg_loss = 2.94002\n",
      "epoch no.4 train no.300300  loss = 3.77470 avg_loss = 2.99360\n",
      "epoch no.4 train no.300310  loss = 2.61531 avg_loss = 2.96158\n",
      "epoch no.4 train no.300320  loss = 2.34445 avg_loss = 2.99552\n",
      "epoch no.4 train no.300330  loss = 3.55825 avg_loss = 3.02727\n",
      "epoch no.4 train no.300340  loss = 3.18510 avg_loss = 3.00618\n",
      "epoch no.4 train no.300350  loss = 4.68682 avg_loss = 3.04492\n",
      "epoch no.4 train no.300360  loss = 4.12317 avg_loss = 3.04475\n",
      "epoch no.4 train no.300370  loss = 2.12870 avg_loss = 3.02581\n",
      "epoch no.4 train no.300380  loss = 1.91376 avg_loss = 3.02396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.300390  loss = 2.33487 avg_loss = 3.03805\n",
      "epoch no.4 train no.300400  loss = 2.75517 avg_loss = 3.08243\n",
      "epoch no.4 train no.300410  loss = 4.76264 avg_loss = 3.07054\n",
      "epoch no.4 train no.300420  loss = 3.76195 avg_loss = 3.11751\n",
      "epoch no.4 train no.300430  loss = 2.74122 avg_loss = 3.10374\n",
      "epoch no.4 train no.300440  loss = 2.79435 avg_loss = 3.08609\n",
      "epoch no.4 train no.300450  loss = 3.93862 avg_loss = 3.06779\n",
      "epoch no.4 train no.300460  loss = 2.53951 avg_loss = 3.06598\n",
      "epoch no.4 train no.300470  loss = 2.10769 avg_loss = 3.06784\n",
      "epoch no.4 train no.300480  loss = 3.01054 avg_loss = 3.05097\n",
      "epoch no.4 train no.300490  loss = 2.20557 avg_loss = 3.02823\n",
      "epoch no.4 train no.300500  loss = 2.10092 avg_loss = 3.03813\n",
      "epoch no.4 train no.300510  loss = 2.26987 avg_loss = 3.03784\n",
      "epoch no.4 train no.300520  loss = 2.60703 avg_loss = 3.02126\n",
      "epoch no.4 train no.300530  loss = 2.09768 avg_loss = 3.02990\n",
      "epoch no.4 train no.300540  loss = 2.91770 avg_loss = 3.02307\n",
      "epoch no.4 train no.300550  loss = 2.96930 avg_loss = 3.01461\n",
      "epoch no.4 train no.300560  loss = 3.06870 avg_loss = 2.99010\n",
      "epoch no.4 train no.300570  loss = 2.12969 avg_loss = 2.98016\n",
      "epoch no.4 train no.300580  loss = 4.09707 avg_loss = 3.04262\n",
      "epoch no.4 train no.300590  loss = 4.61050 avg_loss = 3.06658\n",
      "epoch no.4 train no.300600  loss = 2.14330 avg_loss = 3.06156\n",
      "epoch no.4 train no.300610  loss = 2.81049 avg_loss = 3.07725\n",
      "epoch no.4 train no.300620  loss = 2.94454 avg_loss = 3.05201\n",
      "epoch no.4 train no.300630  loss = 1.83543 avg_loss = 3.06657\n",
      "epoch no.4 train no.300640  loss = 3.72285 avg_loss = 3.09142\n",
      "epoch no.4 train no.300650  loss = 3.61720 avg_loss = 3.06894\n",
      "epoch no.4 train no.300660  loss = 3.70915 avg_loss = 3.07887\n",
      "epoch no.4 train no.300670  loss = 2.92704 avg_loss = 3.07339\n",
      "epoch no.4 train no.300680  loss = 4.20585 avg_loss = 3.09359\n",
      "epoch no.4 train no.300690  loss = 3.71594 avg_loss = 3.13366\n",
      "epoch no.4 train no.300700  loss = 3.45042 avg_loss = 3.16280\n",
      "epoch no.4 train no.300710  loss = 4.33298 avg_loss = 3.20205\n",
      "epoch no.4 train no.300720  loss = 2.83088 avg_loss = 3.23192\n",
      "epoch no.4 train no.300730  loss = 2.86995 avg_loss = 3.18814\n",
      "epoch no.4 train no.300740  loss = 2.85865 avg_loss = 3.17060\n",
      "epoch no.4 train no.300750  loss = 4.33438 avg_loss = 3.14920\n",
      "epoch no.4 train no.300760  loss = 3.73773 avg_loss = 3.16479\n",
      "epoch no.4 train no.300770  loss = 2.17864 avg_loss = 3.16882\n",
      "epoch no.4 train no.300780  loss = 2.70757 avg_loss = 3.19808\n",
      "epoch no.4 train no.300790  loss = 4.61958 avg_loss = 3.20232\n",
      "epoch no.4 train no.300800  loss = 3.23698 avg_loss = 3.19526\n",
      "epoch no.4 train no.300810  loss = 2.31312 avg_loss = 3.15488\n",
      "epoch no.4 train no.300820  loss = 2.37087 avg_loss = 3.13742\n",
      "epoch no.4 train no.300830  loss = 2.39612 avg_loss = 3.10696\n",
      "epoch no.4 train no.300840  loss = 2.59713 avg_loss = 3.09492\n",
      "epoch no.4 train no.300850  loss = 3.82896 avg_loss = 3.11020\n",
      "epoch no.4 train no.300860  loss = 2.47906 avg_loss = 3.10063\n",
      "epoch no.4 train no.300870  loss = 3.25638 avg_loss = 3.11558\n",
      "epoch no.4 train no.300880  loss = 4.94231 avg_loss = 3.15834\n",
      "epoch no.4 train no.300890  loss = 2.71638 avg_loss = 3.12783\n",
      "epoch no.4 train no.300900  loss = 3.60060 avg_loss = 3.12296\n",
      "epoch no.4 train no.300910  loss = 3.18393 avg_loss = 3.14074\n",
      "epoch no.4 train no.300920  loss = 3.09366 avg_loss = 3.13667\n",
      "epoch no.4 train no.300930  loss = 2.10600 avg_loss = 3.12281\n",
      "epoch no.4 train no.300940  loss = 3.08853 avg_loss = 3.10642\n",
      "epoch no.4 train no.300950  loss = 4.64187 avg_loss = 3.09857\n",
      "epoch no.4 train no.300960  loss = 1.71393 avg_loss = 3.12427\n",
      "epoch no.4 train no.300970  loss = 2.60726 avg_loss = 3.12473\n",
      "epoch no.4 train no.300980  loss = 3.45276 avg_loss = 3.11320\n",
      "epoch no.4 train no.300990  loss = 3.13790 avg_loss = 3.12957\n",
      "epoch no.4 train no.301000  loss = 3.45350 avg_loss = 3.08956\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '을', '▁듣기', '▁좋은', '▁노래', '적인', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 감성적인 노래</s>\n",
      "epoch no.4 train no.301010  loss = 2.35678 avg_loss = 3.06884\n",
      "epoch no.4 train no.301020  loss = 2.51172 avg_loss = 3.04929\n",
      "epoch no.4 train no.301030  loss = 2.91985 avg_loss = 3.05110\n",
      "epoch no.4 train no.301040  loss = 3.38513 avg_loss = 3.08660\n",
      "epoch no.4 train no.301050  loss = 2.39720 avg_loss = 3.09130\n",
      "epoch no.4 train no.301060  loss = 2.67907 avg_loss = 3.07297\n",
      "epoch no.4 train no.301070  loss = 4.74079 avg_loss = 3.12500\n",
      "epoch no.4 train no.301080  loss = 3.91174 avg_loss = 3.12406\n",
      "epoch no.4 train no.301090  loss = 4.38422 avg_loss = 3.16913\n",
      "epoch no.4 train no.301100  loss = 1.78262 avg_loss = 3.09031\n",
      "epoch no.4 train no.301110  loss = 4.01194 avg_loss = 3.13057\n",
      "epoch no.4 train no.301120  loss = 3.17871 avg_loss = 3.12694\n",
      "epoch no.4 train no.301130  loss = 2.65306 avg_loss = 3.10677\n",
      "epoch no.4 train no.301140  loss = 1.97636 avg_loss = 3.08398\n",
      "epoch no.4 train no.301150  loss = 2.64557 avg_loss = 3.06953\n",
      "epoch no.4 train no.301160  loss = 2.77471 avg_loss = 3.09481\n",
      "epoch no.4 train no.301170  loss = 1.26678 avg_loss = 3.09406\n",
      "epoch no.4 train no.301180  loss = 3.09909 avg_loss = 3.10989\n",
      "epoch no.4 train no.301190  loss = 3.78830 avg_loss = 3.12178\n",
      "epoch no.4 train no.301200  loss = 2.97938 avg_loss = 3.16643\n",
      "epoch no.4 train no.301210  loss = 2.24840 avg_loss = 3.15874\n",
      "epoch no.4 train no.301220  loss = 4.48713 avg_loss = 3.16088\n",
      "epoch no.4 train no.301230  loss = 1.63005 avg_loss = 3.13884\n",
      "epoch no.4 train no.301240  loss = 2.66963 avg_loss = 3.11433\n",
      "epoch no.4 train no.301250  loss = 4.65796 avg_loss = 3.11631\n",
      "epoch no.4 train no.301260  loss = 4.52130 avg_loss = 3.13527\n",
      "epoch no.4 train no.301270  loss = 4.99228 avg_loss = 3.16338\n",
      "epoch no.4 train no.301280  loss = 2.39822 avg_loss = 3.17720\n",
      "epoch no.4 train no.301290  loss = 3.56458 avg_loss = 3.16986\n",
      "epoch no.4 train no.301300  loss = 5.17358 avg_loss = 3.13221\n",
      "epoch no.4 train no.301310  loss = 2.55946 avg_loss = 3.12262\n",
      "epoch no.4 train no.301320  loss = 2.29613 avg_loss = 3.10222\n",
      "epoch no.4 train no.301330  loss = 4.09933 avg_loss = 3.15509\n",
      "epoch no.4 train no.301340  loss = 2.80208 avg_loss = 3.14277\n",
      "epoch no.4 train no.301350  loss = 3.40403 avg_loss = 3.11876\n",
      "epoch no.4 train no.301360  loss = 3.05632 avg_loss = 3.13027\n",
      "epoch no.4 train no.301370  loss = 2.82649 avg_loss = 3.14553\n",
      "epoch no.4 train no.301380  loss = 3.85740 avg_loss = 3.18047\n",
      "epoch no.4 train no.301390  loss = 3.67285 avg_loss = 3.16850\n",
      "epoch no.4 train no.301400  loss = 3.16645 avg_loss = 3.17337\n",
      "epoch no.4 train no.301410  loss = 2.78011 avg_loss = 3.11973\n",
      "epoch no.4 train no.301420  loss = 2.91118 avg_loss = 3.08401\n",
      "epoch no.4 train no.301430  loss = 2.39746 avg_loss = 3.05770\n",
      "epoch no.4 train no.301440  loss = 2.80597 avg_loss = 3.07183\n",
      "epoch no.4 train no.301450  loss = 2.16846 avg_loss = 3.05659\n",
      "epoch no.4 train no.301460  loss = 2.38992 avg_loss = 3.06047\n",
      "epoch no.4 train no.301470  loss = 3.10880 avg_loss = 3.07480\n",
      "epoch no.4 train no.301480  loss = 2.77658 avg_loss = 3.06782\n",
      "epoch no.4 train no.301490  loss = 4.43608 avg_loss = 3.09013\n",
      "epoch no.4 train no.301500  loss = 3.08494 avg_loss = 3.10103\n",
      "epoch no.4 train no.301510  loss = 1.41216 avg_loss = 3.10048\n",
      "epoch no.4 train no.301520  loss = 3.18053 avg_loss = 3.14323\n",
      "epoch no.4 train no.301530  loss = 4.47871 avg_loss = 3.11752\n",
      "epoch no.4 train no.301540  loss = 3.21411 avg_loss = 3.13903\n",
      "epoch no.4 train no.301550  loss = 2.69887 avg_loss = 3.13809\n",
      "epoch no.4 train no.301560  loss = 3.51928 avg_loss = 3.13942\n",
      "epoch no.4 train no.301570  loss = 3.33942 avg_loss = 3.10630\n",
      "epoch no.4 train no.301580  loss = 3.48414 avg_loss = 3.14072\n",
      "epoch no.4 train no.301590  loss = 2.37706 avg_loss = 3.11906\n",
      "epoch no.4 train no.301600  loss = 3.64189 avg_loss = 3.20173\n",
      "epoch no.4 train no.301610  loss = 4.94960 avg_loss = 3.24332\n",
      "epoch no.4 train no.301620  loss = 2.94428 avg_loss = 3.19895\n",
      "epoch no.4 train no.301630  loss = 1.98033 avg_loss = 3.20842\n",
      "epoch no.4 train no.301640  loss = 1.56462 avg_loss = 3.21245\n",
      "epoch no.4 train no.301650  loss = 4.44337 avg_loss = 3.24847\n",
      "epoch no.4 train no.301660  loss = 3.16135 avg_loss = 3.25418\n",
      "epoch no.4 train no.301670  loss = 4.03036 avg_loss = 3.21362\n",
      "epoch no.4 train no.301680  loss = 2.99998 avg_loss = 3.18918\n",
      "epoch no.4 train no.301690  loss = 5.34786 avg_loss = 3.20372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.301700  loss = 3.20983 avg_loss = 3.22188\n",
      "epoch no.4 train no.301710  loss = 2.81732 avg_loss = 3.23973\n",
      "epoch no.4 train no.301720  loss = 2.07779 avg_loss = 3.20852\n",
      "epoch no.4 train no.301730  loss = 2.46972 avg_loss = 3.19591\n",
      "epoch no.4 train no.301740  loss = 2.90638 avg_loss = 3.21644\n",
      "epoch no.4 train no.301750  loss = 2.18561 avg_loss = 3.19988\n",
      "epoch no.4 train no.301760  loss = 2.45577 avg_loss = 3.16825\n",
      "epoch no.4 train no.301770  loss = 3.12260 avg_loss = 3.17588\n",
      "epoch no.4 train no.301780  loss = 3.39047 avg_loss = 3.15494\n",
      "epoch no.4 train no.301790  loss = 3.00629 avg_loss = 3.12731\n",
      "epoch no.4 train no.301800  loss = 2.43158 avg_loss = 3.11426\n",
      "epoch no.4 train no.301810  loss = 3.54572 avg_loss = 3.20029\n",
      "epoch no.4 train no.301820  loss = 2.94584 avg_loss = 3.24122\n",
      "epoch no.4 train no.301830  loss = 3.68313 avg_loss = 3.28571\n",
      "epoch no.4 train no.301840  loss = 2.32093 avg_loss = 3.25531\n",
      "epoch no.4 train no.301850  loss = 2.15659 avg_loss = 3.19147\n",
      "epoch no.4 train no.301860  loss = 2.80560 avg_loss = 3.19109\n",
      "epoch no.4 train no.301870  loss = 3.79356 avg_loss = 3.17483\n",
      "epoch no.4 train no.301880  loss = 4.38086 avg_loss = 3.17197\n",
      "epoch no.4 train no.301890  loss = 2.75305 avg_loss = 3.14922\n",
      "epoch no.4 train no.301900  loss = 4.02810 avg_loss = 3.18575\n",
      "epoch no.4 train no.301910  loss = 3.86911 avg_loss = 3.16464\n",
      "epoch no.4 train no.301920  loss = 3.53750 avg_loss = 3.13466\n",
      "epoch no.4 train no.301930  loss = 3.79957 avg_loss = 3.15651\n",
      "epoch no.4 train no.301940  loss = 1.79944 avg_loss = 3.13412\n",
      "epoch no.4 train no.301950  loss = 3.34990 avg_loss = 3.14995\n",
      "epoch no.4 train no.301960  loss = 2.41256 avg_loss = 3.12961\n",
      "epoch no.4 train no.301970  loss = 2.66037 avg_loss = 3.14305\n",
      "epoch no.4 train no.301980  loss = 4.10377 avg_loss = 3.14244\n",
      "epoch no.4 train no.301990  loss = 2.12402 avg_loss = 3.13660\n",
      "epoch no.4 train no.302000  loss = 1.80653 avg_loss = 3.08430\n",
      "10\n",
      "to_tokens: ['▁비', '밤', '에', '▁로맨틱', '막', '남', '친', '과', '랑', '줄', '▁노래', '</s>']\n",
      "여름밤의 고막남친이 되어줄 노래</s>\n",
      "epoch no.4 train no.302010  loss = 2.30815 avg_loss = 3.06980\n",
      "epoch no.4 train no.302020  loss = 3.80242 avg_loss = 3.04177\n",
      "epoch no.4 train no.302030  loss = 2.62486 avg_loss = 3.00085\n",
      "epoch no.4 train no.302040  loss = 2.64654 avg_loss = 2.99592\n",
      "epoch no.4 train no.302050  loss = 2.31332 avg_loss = 2.96181\n",
      "epoch no.4 train no.302060  loss = 3.08961 avg_loss = 3.01847\n",
      "epoch no.4 train no.302070  loss = 2.70773 avg_loss = 3.02726\n",
      "epoch no.4 train no.302080  loss = 3.73985 avg_loss = 3.04250\n",
      "epoch no.4 train no.302090  loss = 4.69787 avg_loss = 3.03529\n",
      "epoch no.4 train no.302100  loss = 1.90052 avg_loss = 3.05555\n",
      "epoch no.4 train no.302110  loss = 3.22236 avg_loss = 3.05914\n",
      "epoch no.4 train no.302120  loss = 2.16701 avg_loss = 3.11845\n",
      "epoch no.4 train no.302130  loss = 2.47442 avg_loss = 3.12382\n",
      "epoch no.4 train no.302140  loss = 4.64586 avg_loss = 3.14875\n",
      "epoch no.4 train no.302150  loss = 1.83466 avg_loss = 3.11605\n",
      "epoch no.4 train no.302160  loss = 3.81477 avg_loss = 3.09381\n",
      "epoch no.4 train no.302170  loss = 5.01703 avg_loss = 3.10623\n",
      "epoch no.4 train no.302180  loss = 3.06298 avg_loss = 3.12620\n",
      "epoch no.4 train no.302190  loss = 2.91289 avg_loss = 3.11322\n",
      "epoch no.4 train no.302200  loss = 3.16281 avg_loss = 3.07217\n",
      "epoch no.4 train no.302210  loss = 3.56404 avg_loss = 3.03971\n",
      "epoch no.4 train no.302220  loss = 2.78855 avg_loss = 3.03291\n",
      "epoch no.4 train no.302230  loss = 2.18470 avg_loss = 2.99615\n",
      "epoch no.4 train no.302240  loss = 3.32625 avg_loss = 3.02311\n",
      "epoch no.4 train no.302250  loss = 2.13068 avg_loss = 3.03628\n",
      "epoch no.4 train no.302260  loss = 2.59499 avg_loss = 3.07825\n",
      "epoch no.4 train no.302270  loss = 2.87627 avg_loss = 3.04973\n",
      "epoch no.4 train no.302280  loss = 2.95432 avg_loss = 2.99597\n",
      "epoch no.4 train no.302290  loss = 2.74519 avg_loss = 2.98391\n",
      "epoch no.4 train no.302300  loss = 4.35155 avg_loss = 3.01013\n",
      "epoch no.4 train no.302310  loss = 2.57225 avg_loss = 3.01247\n",
      "epoch no.4 train no.302320  loss = 2.43726 avg_loss = 3.03333\n",
      "epoch no.4 train no.302330  loss = 3.41604 avg_loss = 3.04689\n",
      "epoch no.4 train no.302340  loss = 3.20444 avg_loss = 3.03877\n",
      "epoch no.4 train no.302350  loss = 2.85525 avg_loss = 3.02161\n",
      "epoch no.4 train no.302360  loss = 2.88644 avg_loss = 3.01514\n",
      "epoch no.4 train no.302370  loss = 3.06797 avg_loss = 3.02064\n",
      "epoch no.4 train no.302380  loss = 2.72081 avg_loss = 3.02691\n",
      "epoch no.4 train no.302390  loss = 2.47587 avg_loss = 3.00820\n",
      "epoch no.4 train no.302400  loss = 5.17448 avg_loss = 3.02381\n",
      "epoch no.4 train no.302410  loss = 4.10479 avg_loss = 3.03516\n",
      "epoch no.4 train no.302420  loss = 3.89174 avg_loss = 3.07455\n",
      "epoch no.4 train no.302430  loss = 4.09122 avg_loss = 3.06723\n",
      "epoch no.4 train no.302440  loss = 2.87007 avg_loss = 3.07237\n",
      "epoch no.4 train no.302450  loss = 3.70316 avg_loss = 3.08208\n",
      "epoch no.4 train no.302460  loss = 2.72963 avg_loss = 3.08992\n",
      "epoch no.4 train no.302470  loss = 3.33128 avg_loss = 3.10225\n",
      "epoch no.4 train no.302480  loss = 4.24257 avg_loss = 3.08106\n",
      "epoch no.4 train no.302490  loss = 4.12497 avg_loss = 3.05486\n",
      "epoch no.4 train no.302500  loss = 4.71222 avg_loss = 3.08905\n",
      "epoch no.4 train no.302510  loss = 2.91611 avg_loss = 3.07006\n",
      "epoch no.4 train no.302520  loss = 4.76269 avg_loss = 3.10065\n",
      "epoch no.4 train no.302530  loss = 2.78566 avg_loss = 3.11706\n",
      "epoch no.4 train no.302540  loss = 2.39652 avg_loss = 3.10708\n",
      "epoch no.4 train no.302550  loss = 3.20090 avg_loss = 3.08391\n",
      "epoch no.4 train no.302560  loss = 3.04420 avg_loss = 3.06913\n",
      "epoch no.4 train no.302570  loss = 2.78024 avg_loss = 3.07999\n",
      "epoch no.4 train no.302580  loss = 2.36475 avg_loss = 3.04006\n",
      "epoch no.4 train no.302590  loss = 3.70870 avg_loss = 3.06525\n",
      "epoch no.4 train no.302600  loss = 2.94178 avg_loss = 3.07424\n",
      "epoch no.4 train no.302610  loss = 3.53937 avg_loss = 3.07736\n",
      "epoch no.4 train no.302620  loss = 2.36891 avg_loss = 3.08593\n",
      "epoch no.4 train no.302630  loss = 2.49724 avg_loss = 3.15875\n",
      "epoch no.4 train no.302640  loss = 2.94032 avg_loss = 3.17014\n",
      "epoch no.4 train no.302650  loss = 3.12340 avg_loss = 3.16777\n",
      "epoch no.4 train no.302660  loss = 2.53638 avg_loss = 3.14226\n",
      "epoch no.4 train no.302670  loss = 2.84338 avg_loss = 3.12588\n",
      "epoch no.4 train no.302680  loss = 2.65109 avg_loss = 3.09885\n",
      "epoch no.4 train no.302690  loss = 2.65774 avg_loss = 3.08365\n",
      "epoch no.4 train no.302700  loss = 3.82901 avg_loss = 3.06473\n",
      "epoch no.4 train no.302710  loss = 2.16456 avg_loss = 3.04889\n",
      "epoch no.4 train no.302720  loss = 3.28806 avg_loss = 3.05404\n",
      "epoch no.4 train no.302730  loss = 1.85447 avg_loss = 3.04396\n",
      "epoch no.4 train no.302740  loss = 3.42838 avg_loss = 3.03678\n",
      "epoch no.4 train no.302750  loss = 3.07149 avg_loss = 3.03989\n",
      "epoch no.4 train no.302760  loss = 2.51323 avg_loss = 3.03619\n",
      "epoch no.4 train no.302770  loss = 2.97310 avg_loss = 3.08399\n",
      "epoch no.4 train no.302780  loss = 2.70860 avg_loss = 3.07799\n",
      "epoch no.4 train no.302790  loss = 3.32329 avg_loss = 3.08374\n",
      "epoch no.4 train no.302800  loss = 2.45800 avg_loss = 3.06046\n",
      "epoch no.4 train no.302810  loss = 3.49978 avg_loss = 3.04282\n",
      "epoch no.4 train no.302820  loss = 2.50517 avg_loss = 3.03554\n",
      "epoch no.4 train no.302830  loss = 1.84225 avg_loss = 3.02010\n",
      "epoch no.4 train no.302840  loss = 4.03517 avg_loss = 3.01824\n",
      "epoch no.4 train no.302850  loss = 2.63363 avg_loss = 3.03256\n",
      "epoch no.4 train no.302860  loss = 2.76123 avg_loss = 3.03888\n",
      "epoch no.4 train no.302870  loss = 2.35373 avg_loss = 3.04956\n",
      "epoch no.4 train no.302880  loss = 3.52439 avg_loss = 3.05464\n",
      "epoch no.4 train no.302890  loss = 2.72970 avg_loss = 3.06223\n",
      "epoch no.4 train no.302900  loss = 1.98173 avg_loss = 3.01975\n",
      "epoch no.4 train no.302910  loss = 2.32970 avg_loss = 3.09134\n",
      "epoch no.4 train no.302920  loss = 3.16707 avg_loss = 3.08176\n",
      "epoch no.4 train no.302930  loss = 1.78769 avg_loss = 3.06377\n",
      "epoch no.4 train no.302940  loss = 2.13251 avg_loss = 3.05229\n",
      "epoch no.4 train no.302950  loss = 2.71214 avg_loss = 3.08419\n",
      "epoch no.4 train no.302960  loss = 3.11330 avg_loss = 3.13920\n",
      "epoch no.4 train no.302970  loss = 2.40542 avg_loss = 3.10569\n",
      "epoch no.4 train no.302980  loss = 1.99937 avg_loss = 3.09127\n",
      "epoch no.4 train no.302990  loss = 2.60706 avg_loss = 3.09103\n",
      "epoch no.4 train no.303000  loss = 3.34214 avg_loss = 3.04691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '한', '▁노래', '</s>']\n",
      "여름밤에 듣는 잔잔한 노래</s>\n",
      "epoch no.4 train no.303010  loss = 2.58106 avg_loss = 3.06682\n",
      "epoch no.4 train no.303020  loss = 4.79119 avg_loss = 3.08172\n",
      "epoch no.4 train no.303030  loss = 3.72701 avg_loss = 3.10495\n",
      "epoch no.4 train no.303040  loss = 3.18481 avg_loss = 3.11774\n",
      "epoch no.4 train no.303050  loss = 2.58177 avg_loss = 3.10522\n",
      "epoch no.4 train no.303060  loss = 2.58256 avg_loss = 3.10263\n",
      "epoch no.4 train no.303070  loss = 1.86705 avg_loss = 3.05644\n",
      "epoch no.4 train no.303080  loss = 4.01826 avg_loss = 3.05368\n",
      "epoch no.4 train no.303090  loss = 3.84226 avg_loss = 3.05438\n",
      "epoch no.4 train no.303100  loss = 1.70048 avg_loss = 3.04376\n",
      "epoch no.4 train no.303110  loss = 2.63891 avg_loss = 3.04736\n",
      "epoch no.4 train no.303120  loss = 2.42063 avg_loss = 3.07722\n",
      "epoch no.4 train no.303130  loss = 2.51482 avg_loss = 3.09689\n",
      "epoch no.4 train no.303140  loss = 2.12164 avg_loss = 3.08681\n",
      "epoch no.4 train no.303150  loss = 4.63201 avg_loss = 3.09647\n",
      "epoch no.4 train no.303160  loss = 2.77002 avg_loss = 3.09435\n",
      "epoch no.4 train no.303170  loss = 2.64978 avg_loss = 3.11827\n",
      "epoch no.4 train no.303180  loss = 2.99290 avg_loss = 3.11509\n",
      "epoch no.4 train no.303190  loss = 0.77387 avg_loss = 3.04791\n",
      "epoch no.4 train no.303200  loss = 3.40794 avg_loss = 3.00254\n",
      "epoch no.4 train no.303210  loss = 3.22471 avg_loss = 3.06208\n",
      "epoch no.4 train no.303220  loss = 4.67343 avg_loss = 3.11071\n",
      "epoch no.4 train no.303230  loss = 2.34604 avg_loss = 3.09324\n",
      "epoch no.4 train no.303240  loss = 2.51666 avg_loss = 3.06806\n",
      "epoch no.4 train no.303250  loss = 2.63805 avg_loss = 3.09733\n",
      "epoch no.4 train no.303260  loss = 2.45377 avg_loss = 3.07833\n",
      "epoch no.4 train no.303270  loss = 3.01622 avg_loss = 3.05852\n",
      "epoch no.4 train no.303280  loss = 2.33272 avg_loss = 3.03305\n",
      "epoch no.4 train no.303290  loss = 2.45162 avg_loss = 3.05278\n",
      "epoch no.4 train no.303300  loss = 1.61652 avg_loss = 3.01487\n",
      "epoch no.4 train no.303310  loss = 2.36636 avg_loss = 3.05123\n",
      "epoch no.4 train no.303320  loss = 1.65048 avg_loss = 3.05231\n",
      "epoch no.4 train no.303330  loss = 2.78595 avg_loss = 3.02968\n",
      "epoch no.4 train no.303340  loss = 2.88135 avg_loss = 3.05905\n",
      "epoch no.4 train no.303350  loss = 1.85583 avg_loss = 3.05664\n",
      "epoch no.4 train no.303360  loss = 3.37318 avg_loss = 3.05861\n",
      "epoch no.4 train no.303370  loss = 3.09321 avg_loss = 3.09699\n",
      "epoch no.4 train no.303380  loss = 3.09654 avg_loss = 3.04338\n",
      "epoch no.4 train no.303390  loss = 2.30139 avg_loss = 3.03302\n",
      "epoch no.4 train no.303400  loss = 2.70495 avg_loss = 3.00454\n",
      "epoch no.4 train no.303410  loss = 1.62101 avg_loss = 3.03560\n",
      "epoch no.4 train no.303420  loss = 2.90281 avg_loss = 3.08001\n",
      "epoch no.4 train no.303430  loss = 3.59867 avg_loss = 3.09434\n",
      "epoch no.4 train no.303440  loss = 3.93631 avg_loss = 3.08606\n",
      "epoch no.4 train no.303450  loss = 3.57929 avg_loss = 3.09941\n",
      "epoch no.4 train no.303460  loss = 2.90253 avg_loss = 3.11991\n",
      "epoch no.4 train no.303470  loss = 2.37275 avg_loss = 3.12163\n",
      "epoch no.4 train no.303480  loss = 3.21178 avg_loss = 3.17242\n",
      "epoch no.4 train no.303490  loss = 2.20066 avg_loss = 3.18204\n",
      "epoch no.4 train no.303500  loss = 3.40118 avg_loss = 3.17784\n",
      "epoch no.4 train no.303510  loss = 2.52265 avg_loss = 3.15851\n",
      "epoch no.4 train no.303520  loss = 2.05369 avg_loss = 3.14579\n",
      "epoch no.4 train no.303530  loss = 3.22363 avg_loss = 3.13924\n",
      "epoch no.4 train no.303540  loss = 2.44332 avg_loss = 3.12011\n",
      "epoch no.4 train no.303550  loss = 2.52613 avg_loss = 3.10858\n",
      "epoch no.4 train no.303560  loss = 2.93180 avg_loss = 3.14144\n",
      "epoch no.4 train no.303570  loss = 2.27428 avg_loss = 3.12567\n",
      "epoch no.4 train no.303580  loss = 6.04483 avg_loss = 3.15898\n",
      "epoch no.4 train no.303590  loss = 2.83626 avg_loss = 3.15225\n",
      "epoch no.4 train no.303600  loss = 3.31708 avg_loss = 3.18200\n",
      "epoch no.4 train no.303610  loss = 2.59108 avg_loss = 3.19935\n",
      "epoch no.4 train no.303620  loss = 3.80611 avg_loss = 3.23104\n",
      "epoch no.4 train no.303630  loss = 3.77989 avg_loss = 3.20265\n",
      "epoch no.4 train no.303640  loss = 2.03405 avg_loss = 3.16184\n",
      "epoch no.4 train no.303650  loss = 3.00519 avg_loss = 3.17273\n",
      "epoch no.4 train no.303660  loss = 4.19570 avg_loss = 3.18436\n",
      "epoch no.4 train no.303670  loss = 2.87171 avg_loss = 3.16364\n",
      "epoch no.4 train no.303680  loss = 2.54807 avg_loss = 3.14961\n",
      "epoch no.4 train no.303690  loss = 2.92040 avg_loss = 3.16621\n",
      "epoch no.4 train no.303700  loss = 4.26293 avg_loss = 3.18120\n",
      "epoch no.4 train no.303710  loss = 2.26293 avg_loss = 3.13909\n",
      "epoch no.4 train no.303720  loss = 2.87322 avg_loss = 3.15043\n",
      "epoch no.4 train no.303730  loss = 3.14214 avg_loss = 3.15718\n",
      "epoch no.4 train no.303740  loss = 2.46803 avg_loss = 3.15319\n",
      "epoch no.4 train no.303750  loss = 3.00458 avg_loss = 3.12171\n",
      "epoch no.4 train no.303760  loss = 2.49952 avg_loss = 3.14598\n",
      "epoch no.4 train no.303770  loss = 3.71173 avg_loss = 3.15467\n",
      "epoch no.4 train no.303780  loss = 1.78075 avg_loss = 3.18155\n",
      "epoch no.4 train no.303790  loss = 3.43100 avg_loss = 3.13807\n",
      "epoch no.4 train no.303800  loss = 2.24978 avg_loss = 3.13910\n",
      "epoch no.4 train no.303810  loss = 3.43655 avg_loss = 3.11181\n",
      "epoch no.4 train no.303820  loss = 3.15665 avg_loss = 3.12164\n",
      "epoch no.4 train no.303830  loss = 3.64993 avg_loss = 3.18103\n",
      "epoch no.4 train no.303840  loss = 2.88309 avg_loss = 3.17028\n",
      "epoch no.4 train no.303850  loss = 2.74525 avg_loss = 3.14106\n",
      "epoch no.4 train no.303860  loss = 3.00918 avg_loss = 3.12500\n",
      "epoch no.4 train no.303870  loss = 2.29435 avg_loss = 3.13056\n",
      "epoch no.4 train no.303880  loss = 3.11725 avg_loss = 3.16223\n",
      "epoch no.4 train no.303890  loss = 4.15891 avg_loss = 3.14417\n",
      "epoch no.4 train no.303900  loss = 2.73848 avg_loss = 3.13468\n",
      "epoch no.4 train no.303910  loss = 2.35651 avg_loss = 3.10488\n",
      "epoch no.4 train no.303920  loss = 4.13800 avg_loss = 3.07680\n",
      "epoch no.4 train no.303930  loss = 2.99250 avg_loss = 3.10921\n",
      "epoch no.4 train no.303940  loss = 3.43702 avg_loss = 3.14821\n",
      "epoch no.4 train no.303950  loss = 3.70155 avg_loss = 3.12366\n",
      "epoch no.4 train no.303960  loss = 3.30602 avg_loss = 3.11350\n",
      "epoch no.4 train no.303970  loss = 2.70535 avg_loss = 3.10296\n",
      "epoch no.4 train no.303980  loss = 2.95313 avg_loss = 3.13631\n",
      "epoch no.4 train no.303990  loss = 2.64519 avg_loss = 3.12372\n",
      "epoch no.4 train no.304000  loss = 5.60922 avg_loss = 3.16219\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '▁잘', '▁청량', '한', '▁p', 'op', '</s>']\n",
      "여름과 어울리는 청량한 pop</s>\n",
      "epoch no.4 train no.304010  loss = 4.64863 avg_loss = 3.14096\n",
      "epoch no.4 train no.304020  loss = 2.86273 avg_loss = 3.16034\n",
      "epoch no.4 train no.304030  loss = 2.84464 avg_loss = 3.15014\n",
      "epoch no.4 train no.304040  loss = 2.90570 avg_loss = 3.15226\n",
      "epoch no.4 train no.304050  loss = 4.14288 avg_loss = 3.15999\n",
      "epoch no.4 train no.304060  loss = 2.24096 avg_loss = 3.13930\n",
      "epoch no.4 train no.304070  loss = 4.29823 avg_loss = 3.21139\n",
      "epoch no.4 train no.304080  loss = 2.70226 avg_loss = 3.19008\n",
      "epoch no.4 train no.304090  loss = 2.16094 avg_loss = 3.17049\n",
      "epoch no.4 train no.304100  loss = 3.66817 avg_loss = 3.17751\n",
      "epoch no.4 train no.304110  loss = 3.18827 avg_loss = 3.17204\n",
      "epoch no.4 train no.304120  loss = 2.16635 avg_loss = 3.16154\n",
      "epoch no.4 train no.304130  loss = 3.57403 avg_loss = 3.16266\n",
      "epoch no.4 train no.304140  loss = 3.99035 avg_loss = 3.15789\n",
      "epoch no.4 train no.304150  loss = 4.34481 avg_loss = 3.15317\n",
      "epoch no.4 train no.304160  loss = 2.86295 avg_loss = 3.10852\n",
      "epoch no.4 train no.304170  loss = 2.20691 avg_loss = 3.11031\n",
      "epoch no.4 train no.304180  loss = 4.48266 avg_loss = 3.14798\n",
      "epoch no.4 train no.304190  loss = 2.85312 avg_loss = 3.14050\n",
      "epoch no.4 train no.304200  loss = 4.71916 avg_loss = 3.15967\n",
      "epoch no.4 train no.304210  loss = 2.62777 avg_loss = 3.15308\n",
      "epoch no.4 train no.304220  loss = 3.59115 avg_loss = 3.15279\n",
      "epoch no.4 train no.304230  loss = 3.11767 avg_loss = 3.15699\n",
      "epoch no.4 train no.304240  loss = 3.70228 avg_loss = 3.17034\n",
      "epoch no.4 train no.304250  loss = 2.56465 avg_loss = 3.13127\n",
      "epoch no.4 train no.304260  loss = 3.32512 avg_loss = 3.15212\n",
      "epoch no.4 train no.304270  loss = 3.63583 avg_loss = 3.17603\n",
      "epoch no.4 train no.304280  loss = 1.80177 avg_loss = 3.16896\n",
      "epoch no.4 train no.304290  loss = 3.87171 avg_loss = 3.15301\n",
      "epoch no.4 train no.304300  loss = 2.33353 avg_loss = 3.12310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.304310  loss = 4.06469 avg_loss = 3.10037\n",
      "epoch no.4 train no.304320  loss = 3.11370 avg_loss = 3.14354\n",
      "epoch no.4 train no.304330  loss = 3.96638 avg_loss = 3.15599\n",
      "epoch no.4 train no.304340  loss = 3.22399 avg_loss = 3.16958\n",
      "epoch no.4 train no.304350  loss = 4.25882 avg_loss = 3.19054\n",
      "epoch no.4 train no.304360  loss = 3.55016 avg_loss = 3.18784\n",
      "epoch no.4 train no.304370  loss = 3.65795 avg_loss = 3.17992\n",
      "epoch no.4 train no.304380  loss = 3.11755 avg_loss = 3.14827\n",
      "epoch no.4 train no.304390  loss = 4.48380 avg_loss = 3.18466\n",
      "epoch no.4 train no.304400  loss = 3.98454 avg_loss = 3.17188\n",
      "epoch no.4 train no.304410  loss = 1.55808 avg_loss = 3.19101\n",
      "epoch no.4 train no.304420  loss = 2.64885 avg_loss = 3.18366\n",
      "epoch no.4 train no.304430  loss = 3.32031 avg_loss = 3.18055\n",
      "epoch no.4 train no.304440  loss = 4.21804 avg_loss = 3.19632\n",
      "epoch no.4 train no.304450  loss = 2.42857 avg_loss = 3.23832\n",
      "epoch no.4 train no.304460  loss = 1.14632 avg_loss = 3.21139\n",
      "epoch no.4 train no.304470  loss = 2.96161 avg_loss = 3.20412\n",
      "epoch no.4 train no.304480  loss = 3.24755 avg_loss = 3.18075\n",
      "epoch no.4 train no.304490  loss = 2.62794 avg_loss = 3.13905\n",
      "epoch no.4 train no.304500  loss = 5.19341 avg_loss = 3.16974\n",
      "epoch no.4 train no.304510  loss = 3.11628 avg_loss = 3.13870\n",
      "epoch no.4 train no.304520  loss = 3.67582 avg_loss = 3.12980\n",
      "epoch no.4 train no.304530  loss = 3.17818 avg_loss = 3.12813\n",
      "epoch no.4 train no.304540  loss = 1.77542 avg_loss = 3.11770\n",
      "epoch no.4 train no.304550  loss = 3.33057 avg_loss = 3.09643\n",
      "epoch no.4 train no.304560  loss = 1.78270 avg_loss = 3.07905\n",
      "epoch no.4 train no.304570  loss = 2.37277 avg_loss = 3.08316\n",
      "epoch no.4 train no.304580  loss = 4.70007 avg_loss = 3.10670\n",
      "epoch no.4 train no.304590  loss = 3.26820 avg_loss = 3.08333\n",
      "epoch no.4 train no.304600  loss = 3.62140 avg_loss = 3.08859\n",
      "epoch no.4 train no.304610  loss = 3.83804 avg_loss = 3.07549\n",
      "epoch no.4 train no.304620  loss = 2.19069 avg_loss = 3.07788\n",
      "epoch no.4 train no.304630  loss = 1.96655 avg_loss = 3.07474\n",
      "epoch no.4 train no.304640  loss = 3.09687 avg_loss = 3.06194\n",
      "epoch no.4 train no.304650  loss = 3.12389 avg_loss = 3.05496\n",
      "epoch no.4 train no.304660  loss = 3.30685 avg_loss = 3.04301\n",
      "epoch no.4 train no.304670  loss = 3.13367 avg_loss = 3.06994\n",
      "epoch no.4 train no.304680  loss = 2.57995 avg_loss = 3.05936\n",
      "epoch no.4 train no.304690  loss = 3.26130 avg_loss = 3.12122\n",
      "epoch no.4 train no.304700  loss = 3.25948 avg_loss = 3.14338\n",
      "epoch no.4 train no.304710  loss = 3.01028 avg_loss = 3.12848\n",
      "epoch no.4 train no.304720  loss = 3.36623 avg_loss = 3.09538\n",
      "epoch no.4 train no.304730  loss = 2.38022 avg_loss = 3.10291\n",
      "epoch no.4 train no.304740  loss = 2.05101 avg_loss = 3.12511\n",
      "epoch no.4 train no.304750  loss = 1.89605 avg_loss = 3.12639\n",
      "epoch no.4 train no.304760  loss = 3.65696 avg_loss = 3.14543\n",
      "epoch no.4 train no.304770  loss = 2.82370 avg_loss = 3.15422\n",
      "epoch no.4 train no.304780  loss = 2.49060 avg_loss = 3.16140\n",
      "epoch no.4 train no.304790  loss = 2.84509 avg_loss = 3.18640\n",
      "epoch no.4 train no.304800  loss = 2.70569 avg_loss = 3.17627\n",
      "epoch no.4 train no.304810  loss = 3.33299 avg_loss = 3.12286\n",
      "epoch no.4 train no.304820  loss = 3.19250 avg_loss = 3.09923\n",
      "epoch no.4 train no.304830  loss = 3.70379 avg_loss = 3.10059\n",
      "epoch no.4 train no.304840  loss = 3.41052 avg_loss = 3.13176\n",
      "epoch no.4 train no.304850  loss = 2.99965 avg_loss = 3.14214\n",
      "epoch no.4 train no.304860  loss = 3.23928 avg_loss = 3.15146\n",
      "epoch no.4 train no.304870  loss = 2.61734 avg_loss = 3.14692\n",
      "epoch no.4 train no.304880  loss = 2.67340 avg_loss = 3.10609\n",
      "epoch no.4 train no.304890  loss = 2.67318 avg_loss = 3.12919\n",
      "epoch no.4 train no.304900  loss = 4.54403 avg_loss = 3.10313\n",
      "epoch no.4 train no.304910  loss = 2.07968 avg_loss = 3.10478\n",
      "epoch no.4 train no.304920  loss = 2.24330 avg_loss = 3.08972\n",
      "epoch no.4 train no.304930  loss = 2.26562 avg_loss = 3.05601\n",
      "epoch no.4 train no.304940  loss = 2.97034 avg_loss = 3.07553\n",
      "epoch no.4 train no.304950  loss = 2.22931 avg_loss = 3.08764\n",
      "epoch no.4 train no.304960  loss = 3.37747 avg_loss = 3.10396\n",
      "epoch no.4 train no.304970  loss = 2.81392 avg_loss = 3.07838\n",
      "epoch no.4 train no.304980  loss = 2.37398 avg_loss = 3.09652\n",
      "epoch no.4 train no.304990  loss = 3.76093 avg_loss = 3.11781\n",
      "epoch no.4 train no.305000  loss = 2.98322 avg_loss = 3.11642\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '니까', '나는', '▁노래', '</s>']\n",
      "여름이 생각나는 노래</s>\n",
      "epoch no.4 train no.305010  loss = 2.93203 avg_loss = 3.12825\n",
      "epoch no.4 train no.305020  loss = 3.66608 avg_loss = 3.17385\n",
      "epoch no.4 train no.305030  loss = 4.40736 avg_loss = 3.14996\n",
      "epoch no.4 train no.305040  loss = 5.09886 avg_loss = 3.19659\n",
      "epoch no.4 train no.305050  loss = 2.63499 avg_loss = 3.18161\n",
      "epoch no.4 train no.305060  loss = 2.90091 avg_loss = 3.14958\n",
      "epoch no.4 train no.305070  loss = 2.49576 avg_loss = 3.17899\n",
      "epoch no.4 train no.305080  loss = 2.82530 avg_loss = 3.17554\n",
      "epoch no.4 train no.305090  loss = 5.19214 avg_loss = 3.19841\n",
      "epoch no.4 train no.305100  loss = 2.71798 avg_loss = 3.21450\n",
      "epoch no.4 train no.305110  loss = 4.16330 avg_loss = 3.20990\n",
      "epoch no.4 train no.305120  loss = 3.10470 avg_loss = 3.20317\n",
      "epoch no.4 train no.305130  loss = 3.11926 avg_loss = 3.19399\n",
      "epoch no.4 train no.305140  loss = 2.01631 avg_loss = 3.16364\n",
      "epoch no.4 train no.305150  loss = 3.24042 avg_loss = 3.17814\n",
      "epoch no.4 train no.305160  loss = 3.63298 avg_loss = 3.14678\n",
      "epoch no.4 train no.305170  loss = 2.96735 avg_loss = 3.11262\n",
      "epoch no.4 train no.305180  loss = 4.15792 avg_loss = 3.13334\n",
      "epoch no.4 train no.305190  loss = 2.96065 avg_loss = 3.12848\n",
      "epoch no.4 train no.305200  loss = 3.30238 avg_loss = 3.15159\n",
      "epoch no.4 train no.305210  loss = 3.48814 avg_loss = 3.15694\n",
      "epoch no.4 train no.305220  loss = 4.34171 avg_loss = 3.18195\n",
      "epoch no.4 train no.305230  loss = 3.99172 avg_loss = 3.17188\n",
      "epoch no.4 train no.305240  loss = 2.64581 avg_loss = 3.24241\n",
      "epoch no.4 train no.305250  loss = 2.81954 avg_loss = 3.26914\n",
      "epoch no.4 train no.305260  loss = 2.22668 avg_loss = 3.20793\n",
      "epoch no.4 train no.305270  loss = 2.17634 avg_loss = 3.20666\n",
      "epoch no.4 train no.305280  loss = 4.95229 avg_loss = 3.26711\n",
      "epoch no.4 train no.305290  loss = 2.39467 avg_loss = 3.28187\n",
      "epoch no.4 train no.305300  loss = 2.74473 avg_loss = 3.30180\n",
      "epoch no.4 train no.305310  loss = 4.78098 avg_loss = 3.31321\n",
      "epoch no.4 train no.305320  loss = 3.66221 avg_loss = 3.28608\n",
      "epoch no.4 train no.305330  loss = 3.07424 avg_loss = 3.25375\n",
      "epoch no.4 train no.305340  loss = 4.03945 avg_loss = 3.24100\n",
      "epoch no.4 train no.305350  loss = 3.07664 avg_loss = 3.26828\n",
      "epoch no.4 train no.305360  loss = 3.80508 avg_loss = 3.27054\n",
      "epoch no.4 train no.305370  loss = 3.27927 avg_loss = 3.22221\n",
      "epoch no.4 train no.305380  loss = 2.95079 avg_loss = 3.17249\n",
      "epoch no.4 train no.305390  loss = 2.93789 avg_loss = 3.12000\n",
      "epoch no.4 train no.305400  loss = 2.14587 avg_loss = 3.10917\n",
      "epoch no.4 train no.305410  loss = 3.00784 avg_loss = 3.10147\n",
      "epoch no.4 train no.305420  loss = 2.16582 avg_loss = 3.11055\n",
      "epoch no.4 train no.305430  loss = 3.68686 avg_loss = 3.12817\n",
      "epoch no.4 train no.305440  loss = 4.10376 avg_loss = 3.10346\n",
      "epoch no.4 train no.305450  loss = 3.27166 avg_loss = 3.11361\n",
      "epoch no.4 train no.305460  loss = 3.42631 avg_loss = 3.12595\n",
      "epoch no.4 train no.305470  loss = 3.07433 avg_loss = 3.11466\n",
      "epoch no.4 train no.305480  loss = 3.29293 avg_loss = 3.13362\n",
      "epoch no.4 train no.305490  loss = 5.94141 avg_loss = 3.13110\n",
      "epoch no.4 train no.305500  loss = 2.48316 avg_loss = 3.14068\n",
      "epoch no.4 train no.305510  loss = 4.53911 avg_loss = 3.15334\n",
      "epoch no.4 train no.305520  loss = 2.70257 avg_loss = 3.14960\n",
      "epoch no.4 train no.305530  loss = 3.92295 avg_loss = 3.13877\n",
      "epoch no.4 train no.305540  loss = 2.97982 avg_loss = 3.14688\n",
      "epoch no.4 train no.305550  loss = 2.92662 avg_loss = 3.16850\n",
      "epoch no.4 train no.305560  loss = 2.00729 avg_loss = 3.12504\n",
      "epoch no.4 train no.305570  loss = 4.05882 avg_loss = 3.10503\n",
      "epoch no.4 train no.305580  loss = 3.00130 avg_loss = 3.12985\n",
      "epoch no.4 train no.305590  loss = 2.36087 avg_loss = 3.12636\n",
      "epoch no.4 train no.305600  loss = 3.64166 avg_loss = 3.12952\n",
      "epoch no.4 train no.305610  loss = 3.28826 avg_loss = 3.12212\n",
      "epoch no.4 train no.305620  loss = 2.77458 avg_loss = 3.10875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.305630  loss = 4.83353 avg_loss = 3.13078\n",
      "epoch no.4 train no.305640  loss = 2.30202 avg_loss = 3.10059\n",
      "epoch no.4 train no.305650  loss = 2.71170 avg_loss = 3.10219\n",
      "epoch no.4 train no.305660  loss = 3.08615 avg_loss = 3.12028\n",
      "epoch no.4 train no.305670  loss = 2.57999 avg_loss = 3.11387\n",
      "epoch no.4 train no.305680  loss = 2.98352 avg_loss = 3.09803\n",
      "epoch no.4 train no.305690  loss = 3.17903 avg_loss = 3.10783\n",
      "epoch no.4 train no.305700  loss = 3.07618 avg_loss = 3.09433\n",
      "epoch no.4 train no.305710  loss = 2.27404 avg_loss = 3.10014\n",
      "epoch no.4 train no.305720  loss = 3.69232 avg_loss = 3.11427\n",
      "epoch no.4 train no.305730  loss = 4.11247 avg_loss = 3.11529\n",
      "epoch no.4 train no.305740  loss = 3.20429 avg_loss = 3.09264\n",
      "epoch no.4 train no.305750  loss = 1.79075 avg_loss = 3.06007\n",
      "epoch no.4 train no.305760  loss = 3.49926 avg_loss = 3.11422\n",
      "epoch no.4 train no.305770  loss = 3.40337 avg_loss = 3.15151\n",
      "epoch no.4 train no.305780  loss = 4.58044 avg_loss = 3.17198\n",
      "epoch no.4 train no.305790  loss = 3.94129 avg_loss = 3.15781\n",
      "epoch no.4 train no.305800  loss = 3.64807 avg_loss = 3.13288\n",
      "epoch no.4 train no.305810  loss = 2.17296 avg_loss = 3.14758\n",
      "epoch no.4 train no.305820  loss = 3.66368 avg_loss = 3.14031\n",
      "epoch no.4 train no.305830  loss = 3.32269 avg_loss = 3.16440\n",
      "epoch no.4 train no.305840  loss = 3.24497 avg_loss = 3.18152\n",
      "epoch no.4 train no.305850  loss = 2.19186 avg_loss = 3.14808\n",
      "epoch no.4 train no.305860  loss = 3.22604 avg_loss = 3.11238\n",
      "epoch no.4 train no.305870  loss = 2.24918 avg_loss = 3.09067\n",
      "epoch no.4 train no.305880  loss = 2.95538 avg_loss = 3.11387\n",
      "epoch no.4 train no.305890  loss = 3.30255 avg_loss = 3.13055\n",
      "epoch no.4 train no.305900  loss = 3.34446 avg_loss = 3.11033\n",
      "epoch no.4 train no.305910  loss = 3.70336 avg_loss = 3.13180\n",
      "epoch no.4 train no.305920  loss = 4.24265 avg_loss = 3.13648\n",
      "epoch no.4 train no.305930  loss = 3.03924 avg_loss = 3.18671\n",
      "epoch no.4 train no.305940  loss = 3.79410 avg_loss = 3.15787\n",
      "epoch no.4 train no.305950  loss = 2.37980 avg_loss = 3.13531\n",
      "epoch no.4 train no.305960  loss = 2.85414 avg_loss = 3.13505\n",
      "epoch no.4 train no.305970  loss = 3.15124 avg_loss = 3.14892\n",
      "epoch no.4 train no.305980  loss = 2.33238 avg_loss = 3.14640\n",
      "epoch no.4 train no.305990  loss = 2.03007 avg_loss = 3.09739\n",
      "epoch no.4 train no.306000  loss = 3.68637 avg_loss = 3.09573\n",
      "6\n",
      "to_tokens: ['▁비', '엔', '에', '▁수', '주는', '▁감성', '음악', '</s>']\n",
      "여름밤을 달래주는 인디음악</s>\n",
      "epoch no.4 train no.306010  loss = 3.57016 avg_loss = 3.10906\n",
      "epoch no.4 train no.306020  loss = 3.93420 avg_loss = 3.13355\n",
      "epoch no.4 train no.306030  loss = 2.14807 avg_loss = 3.09310\n",
      "epoch no.4 train no.306040  loss = 3.73267 avg_loss = 3.13740\n",
      "epoch no.4 train no.306050  loss = 2.11435 avg_loss = 3.11727\n",
      "epoch no.4 train no.306060  loss = 3.53370 avg_loss = 3.11824\n",
      "epoch no.4 train no.306070  loss = 2.99841 avg_loss = 3.11089\n",
      "epoch no.4 train no.306080  loss = 3.10681 avg_loss = 3.11314\n",
      "epoch no.4 train no.306090  loss = 3.43651 avg_loss = 3.12540\n",
      "epoch no.4 train no.306100  loss = 2.77177 avg_loss = 3.09064\n",
      "epoch no.4 train no.306110  loss = 3.11707 avg_loss = 3.10428\n",
      "epoch no.4 train no.306120  loss = 3.38906 avg_loss = 3.09809\n",
      "epoch no.4 train no.306130  loss = 3.50695 avg_loss = 3.11037\n",
      "epoch no.4 train no.306140  loss = 2.12017 avg_loss = 3.09627\n",
      "epoch no.4 train no.306150  loss = 3.29200 avg_loss = 3.10343\n",
      "epoch no.4 train no.306160  loss = 2.87263 avg_loss = 3.09586\n",
      "epoch no.4 train no.306170  loss = 2.68301 avg_loss = 3.07726\n",
      "epoch no.4 train no.306180  loss = 4.18297 avg_loss = 3.11233\n",
      "epoch no.4 train no.306190  loss = 2.04105 avg_loss = 3.12188\n",
      "epoch no.4 train no.306200  loss = 2.79395 avg_loss = 3.16720\n",
      "epoch no.4 train no.306210  loss = 2.98067 avg_loss = 3.18419\n",
      "epoch no.4 train no.306220  loss = 3.26717 avg_loss = 3.20011\n",
      "epoch no.4 train no.306230  loss = 2.24933 avg_loss = 3.20101\n",
      "epoch no.4 train no.306240  loss = 2.40569 avg_loss = 3.18528\n",
      "epoch no.4 train no.306250  loss = 3.91437 avg_loss = 3.16330\n",
      "epoch no.4 train no.306260  loss = 4.87632 avg_loss = 3.19294\n",
      "epoch no.4 train no.306270  loss = 8.20077 avg_loss = 3.22735\n",
      "epoch no.4 train no.306280  loss = 2.20804 avg_loss = 3.22260\n",
      "epoch no.4 train no.306290  loss = 3.31693 avg_loss = 3.24848\n",
      "epoch no.4 train no.306300  loss = 2.74973 avg_loss = 3.20896\n",
      "epoch no.4 train no.306310  loss = 2.38291 avg_loss = 3.18395\n",
      "epoch no.4 train no.306320  loss = 2.59307 avg_loss = 3.19779\n",
      "epoch no.4 train no.306330  loss = 3.17217 avg_loss = 3.20896\n",
      "epoch no.4 train no.306340  loss = 4.81451 avg_loss = 3.19633\n",
      "epoch no.4 train no.306350  loss = 4.08279 avg_loss = 3.20343\n",
      "epoch no.4 train no.306360  loss = 3.54608 avg_loss = 3.22832\n",
      "epoch no.4 train no.306370  loss = 2.93019 avg_loss = 3.16100\n",
      "epoch no.4 train no.306380  loss = 2.80093 avg_loss = 3.14128\n",
      "epoch no.4 train no.306390  loss = 2.33552 avg_loss = 3.13147\n",
      "epoch no.4 train no.306400  loss = 3.37018 avg_loss = 3.16175\n",
      "epoch no.4 train no.306410  loss = 2.92802 avg_loss = 3.13114\n",
      "epoch no.4 train no.306420  loss = 3.12051 avg_loss = 3.16402\n",
      "epoch no.4 train no.306430  loss = 2.61312 avg_loss = 3.15561\n",
      "epoch no.4 train no.306440  loss = 4.38098 avg_loss = 3.17283\n",
      "epoch no.4 train no.306450  loss = 2.79207 avg_loss = 3.17110\n",
      "epoch no.4 train no.306460  loss = 3.22020 avg_loss = 3.22257\n",
      "epoch no.4 train no.306470  loss = 3.25639 avg_loss = 3.24484\n",
      "epoch no.4 train no.306480  loss = 2.39450 avg_loss = 3.21087\n",
      "epoch no.4 train no.306490  loss = 2.34419 avg_loss = 3.19272\n",
      "epoch no.4 train no.306500  loss = 3.35101 avg_loss = 3.17103\n",
      "epoch no.4 train no.306510  loss = 2.76672 avg_loss = 3.14670\n",
      "epoch no.4 train no.306520  loss = 3.80054 avg_loss = 3.12638\n",
      "epoch no.4 train no.306530  loss = 2.71408 avg_loss = 3.14355\n",
      "epoch no.4 train no.306540  loss = 2.56845 avg_loss = 3.15509\n",
      "epoch no.4 train no.306550  loss = 4.27943 avg_loss = 3.18255\n",
      "epoch no.4 train no.306560  loss = 3.00195 avg_loss = 3.17855\n",
      "epoch no.4 train no.306570  loss = 3.31479 avg_loss = 3.18343\n",
      "epoch no.4 train no.306580  loss = 3.14121 avg_loss = 3.15868\n",
      "epoch no.4 train no.306590  loss = 1.99738 avg_loss = 3.12906\n",
      "epoch no.4 train no.306600  loss = 4.43896 avg_loss = 3.14486\n",
      "epoch no.4 train no.306610  loss = 1.56809 avg_loss = 3.13794\n",
      "epoch no.4 train no.306620  loss = 3.07883 avg_loss = 3.12364\n",
      "epoch no.4 train no.306630  loss = 2.51555 avg_loss = 3.14930\n",
      "epoch no.4 train no.306640  loss = 2.38531 avg_loss = 3.17870\n",
      "epoch no.4 train no.306650  loss = 3.06665 avg_loss = 3.19398\n",
      "epoch no.4 train no.306660  loss = 3.96610 avg_loss = 3.16197\n",
      "epoch no.4 train no.306670  loss = 3.95985 avg_loss = 3.19290\n",
      "epoch no.4 train no.306680  loss = 2.42692 avg_loss = 3.22627\n",
      "epoch no.4 train no.306690  loss = 4.44781 avg_loss = 3.19801\n",
      "epoch no.4 train no.306700  loss = 2.34030 avg_loss = 3.16050\n",
      "epoch no.4 train no.306710  loss = 1.41824 avg_loss = 3.19036\n",
      "epoch no.4 train no.306720  loss = 3.18319 avg_loss = 3.18566\n",
      "epoch no.4 train no.306730  loss = 3.08910 avg_loss = 3.19256\n",
      "epoch no.4 train no.306740  loss = 3.42039 avg_loss = 3.14885\n",
      "epoch no.4 train no.306750  loss = 2.22828 avg_loss = 3.16305\n",
      "epoch no.4 train no.306760  loss = 4.10745 avg_loss = 3.17040\n",
      "epoch no.4 train no.306770  loss = 2.93427 avg_loss = 3.16633\n",
      "epoch no.4 train no.306780  loss = 2.40152 avg_loss = 3.15167\n",
      "epoch no.4 train no.306790  loss = 3.79449 avg_loss = 3.13025\n",
      "epoch no.4 train no.306800  loss = 3.73958 avg_loss = 3.16418\n",
      "epoch no.4 train no.306810  loss = 3.36008 avg_loss = 3.20839\n",
      "epoch no.4 train no.306820  loss = 2.32796 avg_loss = 3.19164\n",
      "epoch no.4 train no.306830  loss = 4.00050 avg_loss = 3.17466\n",
      "epoch no.4 train no.306840  loss = 2.42220 avg_loss = 3.19709\n",
      "epoch no.4 train no.306850  loss = 4.09923 avg_loss = 3.22711\n",
      "epoch no.4 train no.306860  loss = 4.32848 avg_loss = 3.21569\n",
      "epoch no.4 train no.306870  loss = 2.56361 avg_loss = 3.19143\n",
      "epoch no.4 train no.306880  loss = 2.28678 avg_loss = 3.13818\n",
      "epoch no.4 train no.306890  loss = 4.46774 avg_loss = 3.11642\n",
      "epoch no.4 train no.306900  loss = 3.16445 avg_loss = 3.07622\n",
      "epoch no.4 train no.306910  loss = 3.17346 avg_loss = 3.09643\n",
      "epoch no.4 train no.306920  loss = 4.42911 avg_loss = 3.14583\n",
      "epoch no.4 train no.306930  loss = 2.14916 avg_loss = 3.12126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.306940  loss = 3.41881 avg_loss = 3.10017\n",
      "epoch no.4 train no.306950  loss = 3.25636 avg_loss = 3.08791\n",
      "epoch no.4 train no.306960  loss = 2.24526 avg_loss = 3.09811\n",
      "epoch no.4 train no.306970  loss = 2.57858 avg_loss = 3.09581\n",
      "epoch no.4 train no.306980  loss = 2.43487 avg_loss = 3.12096\n",
      "epoch no.4 train no.306990  loss = 3.40160 avg_loss = 3.13378\n",
      "epoch no.4 train no.307000  loss = 2.83134 avg_loss = 3.10243\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁노래', '한', '▁음악', '</s>', '</s>']\n",
      "여름과 어울리는 청량한 음악들</s>\n",
      "epoch no.4 train no.307010  loss = 3.43572 avg_loss = 3.07588\n",
      "epoch no.4 train no.307020  loss = 2.99398 avg_loss = 3.09580\n",
      "epoch no.4 train no.307030  loss = 2.38968 avg_loss = 3.13195\n",
      "epoch no.4 train no.307040  loss = 4.43442 avg_loss = 3.12976\n",
      "epoch no.4 train no.307050  loss = 2.25762 avg_loss = 3.08326\n",
      "epoch no.4 train no.307060  loss = 4.18560 avg_loss = 3.13792\n",
      "epoch no.4 train no.307070  loss = 2.42441 avg_loss = 3.12804\n",
      "epoch no.4 train no.307080  loss = 3.13967 avg_loss = 3.09100\n",
      "epoch no.4 train no.307090  loss = 2.86595 avg_loss = 3.12789\n",
      "epoch no.4 train no.307100  loss = 3.26751 avg_loss = 3.12975\n",
      "epoch no.4 train no.307110  loss = 5.57422 avg_loss = 3.12596\n",
      "epoch no.4 train no.307120  loss = 2.65296 avg_loss = 3.09253\n",
      "epoch no.4 train no.307130  loss = 2.86229 avg_loss = 3.07621\n",
      "epoch no.4 train no.307140  loss = 4.35198 avg_loss = 3.08996\n",
      "epoch no.4 train no.307150  loss = 4.52150 avg_loss = 3.10550\n",
      "epoch no.4 train no.307160  loss = 3.84821 avg_loss = 3.17054\n",
      "epoch no.4 train no.307170  loss = 2.94311 avg_loss = 3.14221\n",
      "epoch no.4 train no.307180  loss = 3.22139 avg_loss = 3.13304\n",
      "epoch no.4 train no.307190  loss = 2.50984 avg_loss = 3.16189\n",
      "epoch no.4 train no.307200  loss = 2.50736 avg_loss = 3.16718\n",
      "epoch no.4 train no.307210  loss = 3.02590 avg_loss = 3.12890\n",
      "epoch no.4 train no.307220  loss = 2.13649 avg_loss = 3.11978\n",
      "epoch no.4 train no.307230  loss = 2.79119 avg_loss = 3.11194\n",
      "epoch no.4 train no.307240  loss = 3.12761 avg_loss = 3.07105\n",
      "epoch no.4 train no.307250  loss = 2.79558 avg_loss = 3.05958\n",
      "epoch no.4 train no.307260  loss = 3.07842 avg_loss = 3.04771\n",
      "epoch no.4 train no.307270  loss = 2.22650 avg_loss = 3.07481\n",
      "epoch no.4 train no.307280  loss = 2.84695 avg_loss = 3.10027\n",
      "epoch no.4 train no.307290  loss = 3.45736 avg_loss = 3.11366\n",
      "epoch no.4 train no.307300  loss = 2.47134 avg_loss = 3.13666\n",
      "epoch no.4 train no.307310  loss = 2.89611 avg_loss = 3.13961\n",
      "epoch no.4 train no.307320  loss = 4.22176 avg_loss = 3.17500\n",
      "epoch no.4 train no.307330  loss = 2.86866 avg_loss = 3.18587\n",
      "epoch no.4 train no.307340  loss = 2.96787 avg_loss = 3.14322\n",
      "epoch no.4 train no.307350  loss = 2.92035 avg_loss = 3.09853\n",
      "epoch no.4 train no.307360  loss = 3.49128 avg_loss = 3.10254\n",
      "epoch no.4 train no.307370  loss = 4.50092 avg_loss = 3.08114\n",
      "epoch no.4 train no.307380  loss = 4.32377 avg_loss = 3.09257\n",
      "epoch no.4 train no.307390  loss = 2.81682 avg_loss = 3.08437\n",
      "epoch no.4 train no.307400  loss = 3.62521 avg_loss = 3.06506\n",
      "epoch no.4 train no.307410  loss = 2.51625 avg_loss = 3.06947\n",
      "epoch no.4 train no.307420  loss = 2.70547 avg_loss = 3.06873\n",
      "epoch no.4 train no.307430  loss = 2.93069 avg_loss = 3.04116\n",
      "epoch no.4 train no.307440  loss = 3.28218 avg_loss = 3.04716\n",
      "epoch no.4 train no.307450  loss = 1.75888 avg_loss = 3.03745\n",
      "epoch no.4 train no.307460  loss = 4.78083 avg_loss = 3.07081\n",
      "epoch no.4 train no.307470  loss = 2.51988 avg_loss = 3.04899\n",
      "epoch no.4 train no.307480  loss = 2.88928 avg_loss = 3.03377\n",
      "epoch no.4 train no.307490  loss = 2.61559 avg_loss = 3.03548\n",
      "epoch no.4 train no.307500  loss = 1.41027 avg_loss = 3.02393\n",
      "epoch no.4 train no.307510  loss = 2.05408 avg_loss = 3.00148\n",
      "epoch no.4 train no.307520  loss = 3.25007 avg_loss = 3.05654\n",
      "epoch no.4 train no.307530  loss = 3.60349 avg_loss = 3.09623\n",
      "epoch no.4 train no.307540  loss = 3.42502 avg_loss = 3.17002\n",
      "epoch no.4 train no.307550  loss = 4.86013 avg_loss = 3.17545\n",
      "epoch no.4 train no.307560  loss = 2.83461 avg_loss = 3.14708\n",
      "epoch no.4 train no.307570  loss = 5.88001 avg_loss = 3.22161\n",
      "epoch no.4 train no.307580  loss = 2.24326 avg_loss = 3.22543\n",
      "epoch no.4 train no.307590  loss = 2.11990 avg_loss = 3.23945\n",
      "epoch no.4 train no.307600  loss = 4.97617 avg_loss = 3.21269\n",
      "epoch no.4 train no.307610  loss = 2.39077 avg_loss = 3.18096\n",
      "epoch no.4 train no.307620  loss = 3.67788 avg_loss = 3.17328\n",
      "epoch no.4 train no.307630  loss = 4.10254 avg_loss = 3.16744\n",
      "epoch no.4 train no.307640  loss = 2.33300 avg_loss = 3.12382\n",
      "epoch no.4 train no.307650  loss = 2.79320 avg_loss = 3.13298\n",
      "epoch no.4 train no.307660  loss = 2.98869 avg_loss = 3.09259\n",
      "epoch no.4 train no.307670  loss = 2.58603 avg_loss = 3.08125\n",
      "epoch no.4 train no.307680  loss = 3.43310 avg_loss = 3.12668\n",
      "epoch no.4 train no.307690  loss = 2.02797 avg_loss = 3.12154\n",
      "epoch no.4 train no.307700  loss = 2.66614 avg_loss = 3.12828\n",
      "epoch no.4 train no.307710  loss = 1.63144 avg_loss = 3.09242\n",
      "epoch no.4 train no.307720  loss = 3.41486 avg_loss = 3.08921\n",
      "epoch no.4 train no.307730  loss = 2.66376 avg_loss = 3.09497\n",
      "epoch no.4 train no.307740  loss = 2.79765 avg_loss = 3.09372\n",
      "epoch no.4 train no.307750  loss = 4.16364 avg_loss = 3.09935\n",
      "epoch no.4 train no.307760  loss = 2.22916 avg_loss = 3.09539\n",
      "epoch no.4 train no.307770  loss = 1.80328 avg_loss = 3.12127\n",
      "epoch no.4 train no.307780  loss = 3.52918 avg_loss = 3.09412\n",
      "epoch no.4 train no.307790  loss = 2.55897 avg_loss = 3.08999\n",
      "epoch no.4 train no.307800  loss = 3.80792 avg_loss = 3.07417\n",
      "epoch no.4 train no.307810  loss = 4.07272 avg_loss = 3.12882\n",
      "epoch no.4 train no.307820  loss = 2.85781 avg_loss = 3.15591\n",
      "epoch no.4 train no.307830  loss = 2.46744 avg_loss = 3.15020\n",
      "epoch no.4 train no.307840  loss = 4.38072 avg_loss = 3.11313\n",
      "epoch no.4 train no.307850  loss = 2.83650 avg_loss = 3.11228\n",
      "epoch no.4 train no.307860  loss = 3.39620 avg_loss = 3.10210\n",
      "epoch no.4 train no.307870  loss = 3.46285 avg_loss = 3.13115\n",
      "epoch no.4 train no.307880  loss = 3.57351 avg_loss = 3.12100\n",
      "epoch no.4 train no.307890  loss = 2.98921 avg_loss = 3.15150\n",
      "epoch no.4 train no.307900  loss = 4.01875 avg_loss = 3.15961\n",
      "epoch no.4 train no.307910  loss = 3.41445 avg_loss = 3.18112\n",
      "epoch no.4 train no.307920  loss = 2.84723 avg_loss = 3.18335\n",
      "epoch no.4 train no.307930  loss = 4.32271 avg_loss = 3.22922\n",
      "epoch no.4 train no.307940  loss = 3.07770 avg_loss = 3.18356\n",
      "epoch no.4 train no.307950  loss = 2.54084 avg_loss = 3.17897\n",
      "epoch no.4 train no.307960  loss = 4.04326 avg_loss = 3.17887\n",
      "epoch no.4 train no.307970  loss = 3.38027 avg_loss = 3.21430\n",
      "epoch no.4 train no.307980  loss = 1.58233 avg_loss = 3.18143\n",
      "epoch no.4 train no.307990  loss = 3.02208 avg_loss = 3.17474\n",
      "epoch no.4 train no.308000  loss = 2.47219 avg_loss = 3.18472\n",
      "4\n",
      "to_tokens: ['▁가을', '이', '▁역시', '▁락', '과', '</s>']\n",
      "여름엔 시원한 락으로</s>\n",
      "epoch no.4 train no.308010  loss = 1.49179 avg_loss = 3.14060\n",
      "epoch no.4 train no.308020  loss = 3.83171 avg_loss = 3.18255\n",
      "epoch no.4 train no.308030  loss = 3.96303 avg_loss = 3.16616\n",
      "epoch no.4 train no.308040  loss = 3.12320 avg_loss = 3.13224\n",
      "epoch no.4 train no.308050  loss = 4.31973 avg_loss = 3.11141\n",
      "epoch no.4 train no.308060  loss = 1.74209 avg_loss = 3.12649\n",
      "epoch no.4 train no.308070  loss = 4.62637 avg_loss = 3.13554\n",
      "epoch no.4 train no.308080  loss = 2.27928 avg_loss = 3.14117\n",
      "epoch no.4 train no.308090  loss = 3.07566 avg_loss = 3.11849\n",
      "epoch no.4 train no.308100  loss = 2.64091 avg_loss = 3.08944\n",
      "epoch no.4 train no.308110  loss = 2.42525 avg_loss = 3.10293\n",
      "epoch no.4 train no.308120  loss = 2.30844 avg_loss = 3.13682\n",
      "epoch no.4 train no.308130  loss = 5.67714 avg_loss = 3.20570\n",
      "epoch no.4 train no.308140  loss = 3.28248 avg_loss = 3.20733\n",
      "epoch no.4 train no.308150  loss = 2.58936 avg_loss = 3.18621\n",
      "epoch no.4 train no.308160  loss = 1.83586 avg_loss = 3.14621\n",
      "epoch no.4 train no.308170  loss = 2.62696 avg_loss = 3.14734\n",
      "epoch no.4 train no.308180  loss = 3.14302 avg_loss = 3.14646\n",
      "epoch no.4 train no.308190  loss = 4.20155 avg_loss = 3.17379\n",
      "epoch no.4 train no.308200  loss = 2.98224 avg_loss = 3.19674\n",
      "epoch no.4 train no.308210  loss = 1.25852 avg_loss = 3.16771\n",
      "epoch no.4 train no.308220  loss = 4.60549 avg_loss = 3.17022\n",
      "epoch no.4 train no.308230  loss = 1.61019 avg_loss = 3.13513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.308240  loss = 2.17855 avg_loss = 3.11605\n",
      "epoch no.4 train no.308250  loss = 1.40957 avg_loss = 3.08129\n",
      "epoch no.4 train no.308260  loss = 2.63111 avg_loss = 3.05474\n",
      "epoch no.4 train no.308270  loss = 1.53343 avg_loss = 3.07299\n",
      "epoch no.4 train no.308280  loss = 3.81644 avg_loss = 3.06626\n",
      "epoch no.4 train no.308290  loss = 4.21134 avg_loss = 3.07119\n",
      "epoch no.4 train no.308300  loss = 2.95746 avg_loss = 3.13471\n",
      "epoch no.4 train no.308310  loss = 2.83587 avg_loss = 3.11182\n",
      "epoch no.4 train no.308320  loss = 2.40896 avg_loss = 3.07884\n",
      "epoch no.4 train no.308330  loss = 2.21954 avg_loss = 3.06840\n",
      "epoch no.4 train no.308340  loss = 2.57201 avg_loss = 3.09115\n",
      "epoch no.4 train no.308350  loss = 2.63523 avg_loss = 3.14726\n",
      "epoch no.4 train no.308360  loss = 3.49687 avg_loss = 3.13750\n",
      "epoch no.4 train no.308370  loss = 4.17081 avg_loss = 3.17971\n",
      "epoch no.4 train no.308380  loss = 4.29137 avg_loss = 3.19637\n",
      "epoch no.4 train no.308390  loss = 2.67026 avg_loss = 3.18267\n",
      "epoch no.4 train no.308400  loss = 2.91599 avg_loss = 3.18395\n",
      "epoch no.4 train no.308410  loss = 2.54565 avg_loss = 3.15752\n",
      "epoch no.4 train no.308420  loss = 2.10301 avg_loss = 3.12359\n",
      "epoch no.4 train no.308430  loss = 2.68009 avg_loss = 3.09342\n",
      "epoch no.4 train no.308440  loss = 3.43783 avg_loss = 3.09897\n",
      "epoch no.4 train no.308450  loss = 3.30328 avg_loss = 3.10995\n",
      "epoch no.4 train no.308460  loss = 4.21115 avg_loss = 3.15875\n",
      "epoch no.4 train no.308470  loss = 3.70424 avg_loss = 3.15925\n",
      "epoch no.4 train no.308480  loss = 2.80756 avg_loss = 3.16982\n",
      "epoch no.4 train no.308490  loss = 3.25773 avg_loss = 3.18129\n",
      "epoch no.4 train no.308500  loss = 2.47527 avg_loss = 3.18298\n",
      "epoch no.4 train no.308510  loss = 1.57579 avg_loss = 3.17406\n",
      "epoch no.4 train no.308520  loss = 3.27625 avg_loss = 3.16686\n",
      "epoch no.4 train no.308530  loss = 3.57233 avg_loss = 3.16409\n",
      "epoch no.4 train no.308540  loss = 2.28568 avg_loss = 3.10623\n",
      "epoch no.4 train no.308550  loss = 4.34293 avg_loss = 3.11936\n",
      "epoch no.4 train no.308560  loss = 4.36265 avg_loss = 3.13288\n",
      "epoch no.4 train no.308570  loss = 2.58951 avg_loss = 3.09368\n",
      "epoch no.4 train no.308580  loss = 2.35091 avg_loss = 3.08875\n",
      "epoch no.4 train no.308590  loss = 2.13664 avg_loss = 3.08128\n",
      "epoch no.4 train no.308600  loss = 2.98916 avg_loss = 3.08421\n",
      "epoch no.4 train no.308610  loss = 2.39788 avg_loss = 3.07425\n",
      "epoch no.4 train no.308620  loss = 2.69752 avg_loss = 3.12003\n",
      "epoch no.4 train no.308630  loss = 4.14819 avg_loss = 3.09810\n",
      "epoch no.4 train no.308640  loss = 5.03936 avg_loss = 3.11689\n",
      "epoch no.4 train no.308650  loss = 3.67418 avg_loss = 3.11808\n",
      "epoch no.4 train no.308660  loss = 2.08101 avg_loss = 3.06910\n",
      "epoch no.4 train no.308670  loss = 2.12524 avg_loss = 3.04189\n",
      "epoch no.4 train no.308680  loss = 3.45721 avg_loss = 3.07421\n",
      "epoch no.4 train no.308690  loss = 2.70515 avg_loss = 3.06300\n",
      "epoch no.4 train no.308700  loss = 2.47057 avg_loss = 3.05798\n",
      "epoch no.4 train no.308710  loss = 2.79563 avg_loss = 3.08570\n",
      "epoch no.4 train no.308720  loss = 3.69329 avg_loss = 3.11860\n",
      "epoch no.4 train no.308730  loss = 3.80869 avg_loss = 3.08687\n",
      "epoch no.4 train no.308740  loss = 2.39254 avg_loss = 3.08834\n",
      "epoch no.4 train no.308750  loss = 5.02277 avg_loss = 3.13968\n",
      "epoch no.4 train no.308760  loss = 2.73830 avg_loss = 3.13157\n",
      "epoch no.4 train no.308770  loss = 3.83880 avg_loss = 3.13695\n",
      "epoch no.4 train no.308780  loss = 2.14488 avg_loss = 3.15647\n",
      "epoch no.4 train no.308790  loss = 2.46891 avg_loss = 3.16341\n",
      "epoch no.4 train no.308800  loss = 2.91220 avg_loss = 3.14790\n",
      "epoch no.4 train no.308810  loss = 2.81475 avg_loss = 3.13136\n",
      "epoch no.4 train no.308820  loss = 2.95393 avg_loss = 3.12997\n",
      "epoch no.4 train no.308830  loss = 2.07385 avg_loss = 3.14232\n",
      "epoch no.4 train no.308840  loss = 4.48044 avg_loss = 3.14664\n",
      "epoch no.4 train no.308850  loss = 2.71457 avg_loss = 3.15315\n",
      "epoch no.4 train no.308860  loss = 2.24347 avg_loss = 3.14797\n",
      "epoch no.4 train no.308870  loss = 2.59060 avg_loss = 3.13747\n",
      "epoch no.4 train no.308880  loss = 3.84641 avg_loss = 3.21241\n",
      "epoch no.4 train no.308890  loss = 2.26180 avg_loss = 3.17679\n",
      "epoch no.4 train no.308900  loss = 2.52855 avg_loss = 3.14108\n",
      "epoch no.4 train no.308910  loss = 2.81900 avg_loss = 3.11873\n",
      "epoch no.4 train no.308920  loss = 2.80314 avg_loss = 3.12843\n",
      "epoch no.4 train no.308930  loss = 3.62970 avg_loss = 3.09446\n",
      "epoch no.4 train no.308940  loss = 2.15695 avg_loss = 3.11073\n",
      "epoch no.4 train no.308950  loss = 2.57876 avg_loss = 3.10777\n",
      "epoch no.4 train no.308960  loss = 2.62142 avg_loss = 3.11261\n",
      "epoch no.4 train no.308970  loss = 2.32659 avg_loss = 3.10389\n",
      "epoch no.4 train no.308980  loss = 2.58830 avg_loss = 3.09270\n",
      "epoch no.4 train no.308990  loss = 2.09333 avg_loss = 3.08410\n",
      "epoch no.4 train no.309000  loss = 2.49600 avg_loss = 3.03637\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '한', '▁노래', '</s>', '</s>']\n",
      "여름밤에 듣는 잔잔한 음악들</s>\n",
      "epoch no.4 train no.309010  loss = 2.59715 avg_loss = 3.06629\n",
      "epoch no.4 train no.309020  loss = 2.38046 avg_loss = 3.08036\n",
      "epoch no.4 train no.309030  loss = 4.28120 avg_loss = 3.10847\n",
      "epoch no.4 train no.309040  loss = 1.47022 avg_loss = 3.11062\n",
      "epoch no.4 train no.309050  loss = 2.38437 avg_loss = 3.13087\n",
      "epoch no.4 train no.309060  loss = 2.39723 avg_loss = 3.11903\n",
      "epoch no.4 train no.309070  loss = 2.26822 avg_loss = 3.15150\n",
      "epoch no.4 train no.309080  loss = 2.72089 avg_loss = 3.15815\n",
      "epoch no.4 train no.309090  loss = 3.31370 avg_loss = 3.20962\n",
      "epoch no.4 train no.309100  loss = 3.98585 avg_loss = 3.23500\n",
      "epoch no.4 train no.309110  loss = 4.37536 avg_loss = 3.20320\n",
      "epoch no.4 train no.309120  loss = 4.06294 avg_loss = 3.20923\n",
      "epoch no.4 train no.309130  loss = 2.27342 avg_loss = 3.22565\n",
      "epoch no.4 train no.309140  loss = 1.90760 avg_loss = 3.20019\n",
      "epoch no.4 train no.309150  loss = 2.86763 avg_loss = 3.18693\n",
      "epoch no.4 train no.309160  loss = 3.02103 avg_loss = 3.18564\n",
      "epoch no.4 train no.309170  loss = 3.06920 avg_loss = 3.16059\n",
      "epoch no.4 train no.309180  loss = 3.07053 avg_loss = 3.15667\n",
      "epoch no.4 train no.309190  loss = 3.70189 avg_loss = 3.16596\n",
      "epoch no.4 train no.309200  loss = 2.85044 avg_loss = 3.13580\n",
      "epoch no.4 train no.309210  loss = 2.39869 avg_loss = 3.15915\n",
      "epoch no.4 train no.309220  loss = 3.64472 avg_loss = 3.12226\n",
      "epoch no.4 train no.309230  loss = 3.40552 avg_loss = 3.10040\n",
      "epoch no.4 train no.309240  loss = 2.78382 avg_loss = 3.10910\n",
      "epoch no.4 train no.309250  loss = 2.36147 avg_loss = 3.10844\n",
      "epoch no.4 train no.309260  loss = 2.39111 avg_loss = 3.07728\n",
      "epoch no.4 train no.309270  loss = 3.52709 avg_loss = 3.10063\n",
      "epoch no.4 train no.309280  loss = 2.61962 avg_loss = 3.07881\n",
      "epoch no.4 train no.309290  loss = 3.18408 avg_loss = 3.12648\n",
      "epoch no.4 train no.309300  loss = 3.65404 avg_loss = 3.12197\n",
      "epoch no.4 train no.309310  loss = 2.27789 avg_loss = 3.08024\n",
      "epoch no.4 train no.309320  loss = 3.06168 avg_loss = 3.02940\n",
      "epoch no.4 train no.309330  loss = 3.55471 avg_loss = 2.98662\n",
      "epoch no.4 train no.309340  loss = 2.63756 avg_loss = 3.00721\n",
      "epoch no.4 train no.309350  loss = 2.91470 avg_loss = 3.00752\n",
      "epoch no.4 train no.309360  loss = 6.36073 avg_loss = 3.03124\n",
      "epoch no.4 train no.309370  loss = 2.71036 avg_loss = 3.01466\n",
      "epoch no.4 train no.309380  loss = 2.16523 avg_loss = 2.98009\n",
      "epoch no.4 train no.309390  loss = 3.14339 avg_loss = 2.96707\n",
      "epoch no.4 train no.309400  loss = 2.58177 avg_loss = 2.99998\n",
      "epoch no.4 train no.309410  loss = 1.78639 avg_loss = 3.00149\n",
      "epoch no.4 train no.309420  loss = 1.50132 avg_loss = 3.01401\n",
      "epoch no.4 train no.309430  loss = 3.33147 avg_loss = 3.08574\n",
      "epoch no.4 train no.309440  loss = 3.21962 avg_loss = 3.12342\n",
      "epoch no.4 train no.309450  loss = 2.94725 avg_loss = 3.12075\n",
      "epoch no.4 train no.309460  loss = 2.89580 avg_loss = 3.09623\n",
      "epoch no.4 train no.309470  loss = 2.55172 avg_loss = 3.11346\n",
      "epoch no.4 train no.309480  loss = 3.30382 avg_loss = 3.15619\n",
      "epoch no.4 train no.309490  loss = 2.36893 avg_loss = 3.14235\n",
      "epoch no.4 train no.309500  loss = 3.94011 avg_loss = 3.11669\n",
      "epoch no.4 train no.309510  loss = 5.19278 avg_loss = 3.13250\n",
      "epoch no.4 train no.309520  loss = 3.00022 avg_loss = 3.16046\n",
      "epoch no.4 train no.309530  loss = 2.65908 avg_loss = 3.14245\n",
      "epoch no.4 train no.309540  loss = 2.46206 avg_loss = 3.16756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.309550  loss = 2.41423 avg_loss = 3.14010\n",
      "epoch no.4 train no.309560  loss = 3.26529 avg_loss = 3.11932\n",
      "epoch no.4 train no.309570  loss = 4.02550 avg_loss = 3.10177\n",
      "epoch no.4 train no.309580  loss = 3.37565 avg_loss = 3.08556\n",
      "epoch no.4 train no.309590  loss = 3.84824 avg_loss = 3.08776\n",
      "epoch no.4 train no.309600  loss = 2.97485 avg_loss = 3.08023\n",
      "epoch no.4 train no.309610  loss = 2.47129 avg_loss = 3.05453\n",
      "epoch no.4 train no.309620  loss = 2.40280 avg_loss = 3.05755\n",
      "epoch no.4 train no.309630  loss = 4.57964 avg_loss = 3.06966\n",
      "epoch no.4 train no.309640  loss = 4.18835 avg_loss = 3.09960\n",
      "epoch no.4 train no.309650  loss = 3.17861 avg_loss = 3.09173\n",
      "epoch no.4 train no.309660  loss = 2.63152 avg_loss = 3.07112\n",
      "epoch no.4 train no.309670  loss = 2.60447 avg_loss = 3.14975\n",
      "epoch no.4 train no.309680  loss = 2.34312 avg_loss = 3.10724\n",
      "epoch no.4 train no.309690  loss = 3.65001 avg_loss = 3.14360\n",
      "epoch no.4 train no.309700  loss = 4.38191 avg_loss = 3.16427\n",
      "epoch no.4 train no.309710  loss = 4.42856 avg_loss = 3.16078\n",
      "epoch no.4 train no.309720  loss = 1.91140 avg_loss = 3.15585\n",
      "epoch no.4 train no.309730  loss = 4.45808 avg_loss = 3.16588\n",
      "epoch no.4 train no.309740  loss = 4.04748 avg_loss = 3.14937\n",
      "epoch no.4 train no.309750  loss = 1.88686 avg_loss = 3.12504\n",
      "epoch no.4 train no.309760  loss = 2.66058 avg_loss = 3.08837\n",
      "epoch no.4 train no.309770  loss = 3.02608 avg_loss = 3.08679\n",
      "epoch no.4 train no.309780  loss = 3.12248 avg_loss = 3.07296\n",
      "epoch no.4 train no.309790  loss = 2.39830 avg_loss = 3.07731\n",
      "epoch no.4 train no.309800  loss = 3.37120 avg_loss = 3.07149\n",
      "epoch no.4 train no.309810  loss = 2.71936 avg_loss = 3.09106\n",
      "epoch no.4 train no.309820  loss = 4.09968 avg_loss = 3.07184\n",
      "epoch no.4 train no.309830  loss = 2.44609 avg_loss = 3.08939\n",
      "epoch no.4 train no.309840  loss = 3.71416 avg_loss = 3.10957\n",
      "epoch no.4 train no.309850  loss = 5.04668 avg_loss = 3.15999\n",
      "epoch no.4 train no.309860  loss = 3.82086 avg_loss = 3.16708\n",
      "epoch no.4 train no.309870  loss = 4.38715 avg_loss = 3.16061\n",
      "epoch no.4 train no.309880  loss = 2.84893 avg_loss = 3.14694\n",
      "epoch no.4 train no.309890  loss = 4.15744 avg_loss = 3.13516\n",
      "epoch no.4 train no.309900  loss = 4.45414 avg_loss = 3.14279\n",
      "epoch no.4 train no.309910  loss = 3.81549 avg_loss = 3.16366\n",
      "epoch no.4 train no.309920  loss = 2.47684 avg_loss = 3.16368\n",
      "epoch no.4 train no.309930  loss = 3.05134 avg_loss = 3.13946\n",
      "epoch no.4 train no.309940  loss = 3.57833 avg_loss = 3.14378\n",
      "epoch no.4 train no.309950  loss = 3.24329 avg_loss = 3.14143\n",
      "epoch no.4 train no.309960  loss = 4.07096 avg_loss = 3.13142\n",
      "epoch no.4 train no.309970  loss = 4.15141 avg_loss = 3.12581\n",
      "epoch no.4 train no.309980  loss = 2.62987 avg_loss = 3.18630\n",
      "epoch no.4 train no.309990  loss = 3.33347 avg_loss = 3.16213\n",
      "epoch no.4 train no.310000  loss = 3.77046 avg_loss = 3.14538\n",
      "5\n",
      "to_tokens: ['▁비', '과', '▁여행', '갈', '▁플레이', '리스트', '</s>']\n",
      "여름맞이 여행용 플레이리스트</s>\n",
      "epoch no.4 train no.310010  loss = 1.83103 avg_loss = 3.11415\n",
      "epoch no.4 train no.310020  loss = 2.74441 avg_loss = 3.09907\n",
      "epoch no.4 train no.310030  loss = 5.03859 avg_loss = 3.13940\n",
      "epoch no.4 train no.310040  loss = 1.95364 avg_loss = 3.16132\n",
      "epoch no.4 train no.310050  loss = 4.12836 avg_loss = 3.16647\n",
      "epoch no.4 train no.310060  loss = 2.19017 avg_loss = 3.13778\n",
      "epoch no.4 train no.310070  loss = 3.14081 avg_loss = 3.13796\n",
      "epoch no.4 train no.310080  loss = 0.97797 avg_loss = 3.09415\n",
      "epoch no.4 train no.310090  loss = 3.39486 avg_loss = 3.07378\n",
      "epoch no.4 train no.310100  loss = 3.61020 avg_loss = 3.10208\n",
      "epoch no.4 train no.310110  loss = 2.19451 avg_loss = 3.10065\n",
      "epoch no.4 train no.310120  loss = 4.50916 avg_loss = 3.11726\n",
      "epoch no.4 train no.310130  loss = 2.57293 avg_loss = 3.09277\n",
      "epoch no.4 train no.310140  loss = 3.24767 avg_loss = 3.09446\n",
      "epoch no.4 train no.310150  loss = 1.92909 avg_loss = 3.04264\n",
      "epoch no.4 train no.310160  loss = 2.22991 avg_loss = 3.06178\n",
      "epoch no.4 train no.310170  loss = 2.34571 avg_loss = 3.10265\n",
      "epoch no.4 train no.310180  loss = 3.36513 avg_loss = 3.08518\n",
      "epoch no.4 train no.310190  loss = 4.01784 avg_loss = 3.10303\n",
      "epoch no.4 train no.310200  loss = 3.74111 avg_loss = 3.13010\n",
      "epoch no.4 train no.310210  loss = 2.33222 avg_loss = 3.11684\n",
      "epoch no.4 train no.310220  loss = 2.97862 avg_loss = 3.11779\n",
      "epoch no.4 train no.310230  loss = 2.86233 avg_loss = 3.14196\n",
      "epoch no.4 train no.310240  loss = 3.97042 avg_loss = 3.12066\n",
      "epoch no.4 train no.310250  loss = 4.62693 avg_loss = 3.11098\n",
      "epoch no.4 train no.310260  loss = 2.68426 avg_loss = 3.11765\n",
      "epoch no.4 train no.310270  loss = 2.94114 avg_loss = 3.10243\n",
      "epoch no.4 train no.310280  loss = 2.32768 avg_loss = 3.13479\n",
      "epoch no.4 train no.310290  loss = 3.05434 avg_loss = 3.12030\n",
      "epoch no.4 train no.310300  loss = 2.18110 avg_loss = 3.12439\n",
      "epoch no.4 train no.310310  loss = 2.82631 avg_loss = 3.12231\n",
      "epoch no.4 train no.310320  loss = 2.85142 avg_loss = 3.11508\n",
      "epoch no.4 train no.310330  loss = 2.72370 avg_loss = 3.11315\n",
      "epoch no.4 train no.310340  loss = 4.54993 avg_loss = 3.15248\n",
      "epoch no.4 train no.310350  loss = 2.69325 avg_loss = 3.18971\n",
      "epoch no.4 train no.310360  loss = 5.11730 avg_loss = 3.17286\n",
      "epoch no.4 train no.310370  loss = 3.15386 avg_loss = 3.23733\n",
      "epoch no.4 train no.310380  loss = 2.64463 avg_loss = 3.21075\n",
      "epoch no.4 train no.310390  loss = 4.21959 avg_loss = 3.20443\n",
      "epoch no.4 train no.310400  loss = 3.99304 avg_loss = 3.20573\n",
      "epoch no.4 train no.310410  loss = 2.86421 avg_loss = 3.20429\n",
      "epoch no.4 train no.310420  loss = 3.77503 avg_loss = 3.18303\n",
      "epoch no.4 train no.310430  loss = 3.00539 avg_loss = 3.17191\n",
      "epoch no.4 train no.310440  loss = 3.00743 avg_loss = 3.17669\n",
      "epoch no.4 train no.310450  loss = 3.52766 avg_loss = 3.18055\n",
      "epoch no.4 train no.310460  loss = 3.30856 avg_loss = 3.15479\n",
      "epoch no.4 train no.310470  loss = 2.59113 avg_loss = 3.18586\n",
      "epoch no.4 train no.310480  loss = 4.50009 avg_loss = 3.17940\n",
      "epoch no.4 train no.310490  loss = 3.32589 avg_loss = 3.17667\n",
      "epoch no.4 train no.310500  loss = 2.49508 avg_loss = 3.18887\n",
      "epoch no.4 train no.310510  loss = 4.79982 avg_loss = 3.19704\n",
      "epoch no.4 train no.310520  loss = 2.15382 avg_loss = 3.16109\n",
      "epoch no.4 train no.310530  loss = 3.65829 avg_loss = 3.10915\n",
      "epoch no.4 train no.310540  loss = 1.26176 avg_loss = 3.12108\n",
      "epoch no.4 train no.310550  loss = 2.74749 avg_loss = 3.14321\n",
      "epoch no.4 train no.310560  loss = 2.51033 avg_loss = 3.08876\n",
      "epoch no.4 train no.310570  loss = 3.45581 avg_loss = 3.09754\n",
      "epoch no.4 train no.310580  loss = 4.47289 avg_loss = 3.10966\n",
      "epoch no.4 train no.310590  loss = 3.24179 avg_loss = 3.11150\n",
      "epoch no.4 train no.310600  loss = 3.72145 avg_loss = 3.10071\n",
      "epoch no.4 train no.310610  loss = 3.14731 avg_loss = 3.10848\n",
      "epoch no.4 train no.310620  loss = 3.07386 avg_loss = 3.10402\n",
      "epoch no.4 train no.310630  loss = 3.33313 avg_loss = 3.12300\n",
      "epoch no.4 train no.310640  loss = 2.72201 avg_loss = 3.09486\n",
      "epoch no.4 train no.310650  loss = 2.86953 avg_loss = 3.10900\n",
      "epoch no.4 train no.310660  loss = 1.65489 avg_loss = 3.10545\n",
      "epoch no.4 train no.310670  loss = 4.43062 avg_loss = 3.11344\n",
      "epoch no.4 train no.310680  loss = 3.56978 avg_loss = 3.11063\n",
      "epoch no.4 train no.310690  loss = 3.06274 avg_loss = 3.07671\n",
      "epoch no.4 train no.310700  loss = 2.81294 avg_loss = 3.07576\n",
      "epoch no.4 train no.310710  loss = 4.09773 avg_loss = 3.10960\n",
      "epoch no.4 train no.310720  loss = 5.24930 avg_loss = 3.10570\n",
      "epoch no.4 train no.310730  loss = 3.51109 avg_loss = 3.09164\n",
      "epoch no.4 train no.310740  loss = 2.86454 avg_loss = 3.08246\n",
      "epoch no.4 train no.310750  loss = 2.41131 avg_loss = 3.07021\n",
      "epoch no.4 train no.310760  loss = 2.20242 avg_loss = 3.10977\n",
      "epoch no.4 train no.310770  loss = 2.50368 avg_loss = 3.14715\n",
      "epoch no.4 train no.310780  loss = 2.48248 avg_loss = 3.16662\n",
      "epoch no.4 train no.310790  loss = 2.37307 avg_loss = 3.14225\n",
      "epoch no.4 train no.310800  loss = 2.08126 avg_loss = 3.13290\n",
      "epoch no.4 train no.310810  loss = 2.57365 avg_loss = 3.16315\n",
      "epoch no.4 train no.310820  loss = 2.24194 avg_loss = 3.11209\n",
      "epoch no.4 train no.310830  loss = 3.11174 avg_loss = 3.12879\n",
      "epoch no.4 train no.310840  loss = 4.06677 avg_loss = 3.17193\n",
      "epoch no.4 train no.310850  loss = 3.28342 avg_loss = 3.19160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.310860  loss = 4.01587 avg_loss = 3.18191\n",
      "epoch no.4 train no.310870  loss = 3.31403 avg_loss = 3.17039\n",
      "epoch no.4 train no.310880  loss = 1.97602 avg_loss = 3.18367\n",
      "epoch no.4 train no.310890  loss = 4.33737 avg_loss = 3.23676\n",
      "epoch no.4 train no.310900  loss = 2.67400 avg_loss = 3.22490\n",
      "epoch no.4 train no.310910  loss = 2.29758 avg_loss = 3.22387\n",
      "epoch no.4 train no.310920  loss = 3.03222 avg_loss = 3.27844\n",
      "epoch no.4 train no.310930  loss = 2.85248 avg_loss = 3.26563\n",
      "epoch no.4 train no.310940  loss = 2.96254 avg_loss = 3.23511\n",
      "epoch no.4 train no.310950  loss = 2.82930 avg_loss = 3.23528\n",
      "epoch no.4 train no.310960  loss = 3.72543 avg_loss = 3.24207\n",
      "epoch no.4 train no.310970  loss = 3.19936 avg_loss = 3.20725\n",
      "epoch no.4 train no.310980  loss = 3.22872 avg_loss = 3.18656\n",
      "epoch no.4 train no.310990  loss = 2.86398 avg_loss = 3.16884\n",
      "epoch no.4 train no.311000  loss = 4.70104 avg_loss = 3.17512\n",
      "4\n",
      "to_tokens: ['▁비', '▁여름', '▁여름', '▁여름', '▁여름', '</s>']\n",
      "여름 여름 여름 여름 여름</s>\n",
      "epoch no.4 train no.311010  loss = 3.09095 avg_loss = 3.13987\n",
      "epoch no.4 train no.311020  loss = 3.15319 avg_loss = 3.17960\n",
      "epoch no.4 train no.311030  loss = 3.58737 avg_loss = 3.17660\n",
      "epoch no.4 train no.311040  loss = 3.00725 avg_loss = 3.18349\n",
      "epoch no.4 train no.311050  loss = 3.16425 avg_loss = 3.18134\n",
      "epoch no.4 train no.311060  loss = 3.00060 avg_loss = 3.13576\n",
      "epoch no.4 train no.311070  loss = 5.12879 avg_loss = 3.15102\n",
      "epoch no.4 train no.311080  loss = 1.53794 avg_loss = 3.12149\n",
      "epoch no.4 train no.311090  loss = 4.28442 avg_loss = 3.14111\n",
      "epoch no.4 train no.311100  loss = 5.34008 avg_loss = 3.14879\n",
      "epoch no.4 train no.311110  loss = 4.01700 avg_loss = 3.18952\n",
      "epoch no.4 train no.311120  loss = 3.63793 avg_loss = 3.21213\n",
      "epoch no.4 train no.311130  loss = 2.46444 avg_loss = 3.16825\n",
      "epoch no.4 train no.311140  loss = 2.14485 avg_loss = 3.13959\n",
      "epoch no.4 train no.311150  loss = 2.23926 avg_loss = 3.14562\n",
      "epoch no.4 train no.311160  loss = 2.31727 avg_loss = 3.14213\n",
      "epoch no.4 train no.311170  loss = 3.30025 avg_loss = 3.11862\n",
      "epoch no.4 train no.311180  loss = 2.29224 avg_loss = 3.16354\n",
      "epoch no.4 train no.311190  loss = 2.64594 avg_loss = 3.17497\n",
      "epoch no.4 train no.311200  loss = 2.76972 avg_loss = 3.15949\n",
      "epoch no.4 train no.311210  loss = 2.45687 avg_loss = 3.16787\n",
      "epoch no.4 train no.311220  loss = 3.55786 avg_loss = 3.12535\n",
      "epoch no.4 train no.311230  loss = 2.44701 avg_loss = 3.12316\n",
      "epoch no.4 train no.311240  loss = 2.35865 avg_loss = 3.09258\n",
      "epoch no.4 train no.311250  loss = 2.81586 avg_loss = 3.04308\n",
      "epoch no.4 train no.311260  loss = 3.07915 avg_loss = 3.06495\n",
      "epoch no.4 train no.311270  loss = 4.63510 avg_loss = 3.09236\n",
      "epoch no.4 train no.311280  loss = 2.62787 avg_loss = 3.11191\n",
      "epoch no.4 train no.311290  loss = 4.61213 avg_loss = 3.13915\n",
      "epoch no.4 train no.311300  loss = 3.15494 avg_loss = 3.09793\n",
      "epoch no.4 train no.311310  loss = 3.22445 avg_loss = 3.12138\n",
      "epoch no.4 train no.311320  loss = 3.53169 avg_loss = 3.15287\n",
      "epoch no.4 train no.311330  loss = 4.72128 avg_loss = 3.20813\n",
      "epoch no.4 train no.311340  loss = 2.55822 avg_loss = 3.17930\n",
      "epoch no.4 train no.311350  loss = 3.13353 avg_loss = 3.16318\n",
      "epoch no.4 train no.311360  loss = 3.51623 avg_loss = 3.15551\n",
      "epoch no.4 train no.311370  loss = 4.64380 avg_loss = 3.15917\n",
      "epoch no.4 train no.311380  loss = 2.25635 avg_loss = 3.13089\n",
      "epoch no.4 train no.311390  loss = 2.93436 avg_loss = 3.12763\n",
      "epoch no.4 train no.311400  loss = 2.92557 avg_loss = 3.12923\n",
      "epoch no.4 train no.311410  loss = 5.91707 avg_loss = 3.15716\n",
      "epoch no.4 train no.311420  loss = 3.33628 avg_loss = 3.16779\n",
      "epoch no.4 train no.311430  loss = 3.30151 avg_loss = 3.20060\n",
      "epoch no.4 train no.311440  loss = 1.67674 avg_loss = 3.19895\n",
      "epoch no.4 train no.311450  loss = 3.94961 avg_loss = 3.27282\n",
      "epoch no.4 train no.311460  loss = 3.28776 avg_loss = 3.25696\n",
      "epoch no.4 train no.311470  loss = 2.20801 avg_loss = 3.23649\n",
      "epoch no.4 train no.311480  loss = 2.44445 avg_loss = 3.17939\n",
      "epoch no.4 train no.311490  loss = 2.83886 avg_loss = 3.11812\n",
      "epoch no.4 train no.311500  loss = 3.71286 avg_loss = 3.13272\n",
      "epoch no.4 train no.311510  loss = 2.70514 avg_loss = 3.14530\n",
      "epoch no.4 train no.311520  loss = 3.27407 avg_loss = 3.14479\n",
      "epoch no.4 train no.311530  loss = 3.05524 avg_loss = 3.12762\n",
      "epoch no.4 train no.311540  loss = 4.33262 avg_loss = 3.16371\n",
      "epoch no.4 train no.311550  loss = 3.35162 avg_loss = 3.12199\n",
      "epoch no.4 train no.311560  loss = 2.01444 avg_loss = 3.14566\n",
      "epoch no.4 train no.311570  loss = 3.48218 avg_loss = 3.13571\n",
      "epoch no.4 train no.311580  loss = 1.74250 avg_loss = 3.11551\n",
      "epoch no.4 train no.311590  loss = 3.23215 avg_loss = 3.11393\n",
      "epoch no.4 train no.311600  loss = 3.12097 avg_loss = 3.09235\n",
      "epoch no.4 train no.311610  loss = 2.78179 avg_loss = 3.07139\n",
      "epoch no.4 train no.311620  loss = 4.43711 avg_loss = 3.09106\n",
      "epoch no.4 train no.311630  loss = 5.09149 avg_loss = 3.17977\n",
      "epoch no.4 train no.311640  loss = 3.02086 avg_loss = 3.15306\n",
      "epoch no.4 train no.311650  loss = 3.08996 avg_loss = 3.14468\n",
      "epoch no.4 train no.311660  loss = 3.36753 avg_loss = 3.15465\n",
      "epoch no.4 train no.311670  loss = 4.01951 avg_loss = 3.15488\n",
      "epoch no.4 train no.311680  loss = 3.97954 avg_loss = 3.18168\n",
      "epoch no.4 train no.311690  loss = 2.93539 avg_loss = 3.17841\n",
      "epoch no.4 train no.311700  loss = 3.29739 avg_loss = 3.19077\n",
      "epoch no.4 train no.311710  loss = 2.35665 avg_loss = 3.21936\n",
      "epoch no.4 train no.311720  loss = 2.09549 avg_loss = 3.18893\n",
      "epoch no.4 train no.311730  loss = 3.56213 avg_loss = 3.17427\n",
      "epoch no.4 train no.311740  loss = 3.16701 avg_loss = 3.12810\n",
      "epoch no.4 train no.311750  loss = 2.86791 avg_loss = 3.12463\n",
      "epoch no.4 train no.311760  loss = 3.91013 avg_loss = 3.10868\n",
      "epoch no.4 train no.311770  loss = 2.48859 avg_loss = 3.10414\n",
      "epoch no.4 train no.311780  loss = 4.17867 avg_loss = 3.04794\n",
      "epoch no.4 train no.311790  loss = 3.41561 avg_loss = 3.07565\n",
      "epoch no.4 train no.311800  loss = 2.50247 avg_loss = 3.06838\n",
      "epoch no.4 train no.311810  loss = 3.01074 avg_loss = 3.11179\n",
      "epoch no.4 train no.311820  loss = 4.04044 avg_loss = 3.13967\n",
      "epoch no.4 train no.311830  loss = 3.95215 avg_loss = 3.15664\n",
      "epoch no.4 train no.311840  loss = 3.74974 avg_loss = 3.15154\n",
      "epoch no.4 train no.311850  loss = 3.14908 avg_loss = 3.16378\n",
      "epoch no.4 train no.311860  loss = 2.86164 avg_loss = 3.22194\n",
      "epoch no.4 train no.311870  loss = 2.41937 avg_loss = 3.18155\n",
      "epoch no.4 train no.311880  loss = 4.17120 avg_loss = 3.15275\n",
      "epoch no.4 train no.311890  loss = 2.85251 avg_loss = 3.17335\n",
      "epoch no.4 train no.311900  loss = 2.04420 avg_loss = 3.17721\n",
      "epoch no.4 train no.311910  loss = 2.83923 avg_loss = 3.19822\n",
      "epoch no.4 train no.311920  loss = 3.60773 avg_loss = 3.21607\n",
      "epoch no.4 train no.311930  loss = 2.85040 avg_loss = 3.24812\n",
      "epoch no.4 train no.311940  loss = 3.04360 avg_loss = 3.28177\n",
      "epoch no.4 train no.311950  loss = 2.74012 avg_loss = 3.25046\n",
      "epoch no.4 train no.311960  loss = 3.21184 avg_loss = 3.24618\n",
      "epoch no.4 train no.311970  loss = 3.34990 avg_loss = 3.26257\n",
      "epoch no.4 train no.311980  loss = 3.28323 avg_loss = 3.29455\n",
      "epoch no.4 train no.311990  loss = 3.34952 avg_loss = 3.28623\n",
      "epoch no.4 train no.312000  loss = 2.66212 avg_loss = 3.27541\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '▁오면', '▁생각', '나는', '▁노래', '</s>']\n",
      "여름이 오면 생각나는 노래</s>\n",
      "epoch no.4 train no.312010  loss = 1.66240 avg_loss = 3.32028\n",
      "epoch no.4 train no.312020  loss = 1.91785 avg_loss = 3.30801\n",
      "epoch no.4 train no.312030  loss = 2.95183 avg_loss = 3.32367\n",
      "epoch no.4 train no.312040  loss = 2.99872 avg_loss = 3.35006\n",
      "epoch no.4 train no.312050  loss = 4.61086 avg_loss = 3.35304\n",
      "epoch no.4 train no.312060  loss = 2.93278 avg_loss = 3.32721\n",
      "epoch no.4 train no.312070  loss = 3.20198 avg_loss = 3.27915\n",
      "epoch no.4 train no.312080  loss = 4.10072 avg_loss = 3.27308\n",
      "epoch no.4 train no.312090  loss = 4.04605 avg_loss = 3.34174\n",
      "epoch no.4 train no.312100  loss = 2.77164 avg_loss = 3.29840\n",
      "epoch no.4 train no.312110  loss = 2.84201 avg_loss = 3.28371\n",
      "epoch no.4 train no.312120  loss = 2.04561 avg_loss = 3.25350\n",
      "epoch no.4 train no.312130  loss = 2.01744 avg_loss = 3.23769\n",
      "epoch no.4 train no.312140  loss = 3.35535 avg_loss = 3.24821\n",
      "epoch no.4 train no.312150  loss = 3.19546 avg_loss = 3.26442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.312160  loss = 2.59529 avg_loss = 3.26672\n",
      "epoch no.4 train no.312170  loss = 3.74455 avg_loss = 3.24781\n",
      "epoch no.4 train no.312180  loss = 5.12183 avg_loss = 3.27486\n",
      "epoch no.4 train no.312190  loss = 2.19836 avg_loss = 3.26597\n",
      "epoch no.4 train no.312200  loss = 3.13823 avg_loss = 3.21722\n",
      "epoch no.4 train no.312210  loss = 2.31237 avg_loss = 3.16530\n",
      "epoch no.4 train no.312220  loss = 3.02526 avg_loss = 3.13563\n",
      "epoch no.4 train no.312230  loss = 2.56567 avg_loss = 3.09816\n",
      "epoch no.4 train no.312240  loss = 3.39183 avg_loss = 3.10306\n",
      "epoch no.4 train no.312250  loss = 4.39712 avg_loss = 3.08824\n",
      "epoch no.4 train no.312260  loss = 4.00062 avg_loss = 3.08346\n",
      "epoch no.4 train no.312270  loss = 1.88713 avg_loss = 3.11664\n",
      "epoch no.4 train no.312280  loss = 2.68928 avg_loss = 3.11940\n",
      "epoch no.4 train no.312290  loss = 3.19058 avg_loss = 3.12593\n",
      "epoch no.4 train no.312300  loss = 2.76012 avg_loss = 3.08723\n",
      "epoch no.4 train no.312310  loss = 4.18113 avg_loss = 3.09443\n",
      "epoch no.4 train no.312320  loss = 3.17495 avg_loss = 3.03520\n",
      "epoch no.4 train no.312330  loss = 2.32016 avg_loss = 3.00237\n",
      "epoch no.4 train no.312340  loss = 3.58814 avg_loss = 2.99192\n",
      "epoch no.4 train no.312350  loss = 3.11243 avg_loss = 3.04219\n",
      "epoch no.4 train no.312360  loss = 4.09246 avg_loss = 3.07730\n",
      "epoch no.4 train no.312370  loss = 2.61564 avg_loss = 3.08829\n",
      "epoch no.4 train no.312380  loss = 2.45239 avg_loss = 3.05736\n",
      "epoch no.4 train no.312390  loss = 3.92978 avg_loss = 3.09057\n",
      "epoch no.4 train no.312400  loss = 2.18192 avg_loss = 3.12195\n",
      "epoch no.4 train no.312410  loss = 3.87923 avg_loss = 3.12283\n",
      "epoch no.4 train no.312420  loss = 2.39250 avg_loss = 3.09611\n",
      "epoch no.4 train no.312430  loss = 2.81641 avg_loss = 3.10003\n",
      "epoch no.4 train no.312440  loss = 3.19158 avg_loss = 3.08947\n",
      "epoch no.4 train no.312450  loss = 4.01302 avg_loss = 3.06955\n",
      "epoch no.4 train no.312460  loss = 1.97988 avg_loss = 3.07021\n",
      "epoch no.4 train no.312470  loss = 3.99632 avg_loss = 3.06786\n",
      "epoch no.4 train no.312480  loss = 3.44282 avg_loss = 3.07291\n",
      "epoch no.4 train no.312490  loss = 1.76745 avg_loss = 3.04303\n",
      "epoch no.4 train no.312500  loss = 4.61259 avg_loss = 3.08732\n",
      "epoch no.4 train no.312510  loss = 2.45135 avg_loss = 3.11604\n",
      "epoch no.4 train no.312520  loss = 2.43561 avg_loss = 3.10711\n",
      "epoch no.4 train no.312530  loss = 2.26995 avg_loss = 3.08647\n",
      "epoch no.4 train no.312540  loss = 3.90454 avg_loss = 3.11100\n",
      "epoch no.4 train no.312550  loss = 2.14834 avg_loss = 3.10358\n",
      "epoch no.4 train no.312560  loss = 2.00739 avg_loss = 3.10349\n",
      "epoch no.4 train no.312570  loss = 3.83288 avg_loss = 3.14472\n",
      "epoch no.4 train no.312580  loss = 2.58874 avg_loss = 3.09920\n",
      "epoch no.4 train no.312590  loss = 1.58970 avg_loss = 3.09044\n",
      "epoch no.4 train no.312600  loss = 3.67077 avg_loss = 3.11600\n",
      "epoch no.4 train no.312610  loss = 2.99912 avg_loss = 3.14798\n",
      "epoch no.4 train no.312620  loss = 2.29802 avg_loss = 3.14674\n",
      "epoch no.4 train no.312630  loss = 2.31213 avg_loss = 3.13134\n",
      "epoch no.4 train no.312640  loss = 2.92576 avg_loss = 3.13537\n",
      "epoch no.4 train no.312650  loss = 3.05988 avg_loss = 3.13640\n",
      "epoch no.4 train no.312660  loss = 2.82455 avg_loss = 3.15139\n",
      "epoch no.4 train no.312670  loss = 3.04084 avg_loss = 3.15803\n",
      "epoch no.4 train no.312680  loss = 2.35930 avg_loss = 3.13805\n",
      "epoch no.4 train no.312690  loss = 3.14380 avg_loss = 3.14202\n",
      "epoch no.4 train no.312700  loss = 2.09081 avg_loss = 3.14672\n",
      "epoch no.4 train no.312710  loss = 4.03427 avg_loss = 3.10941\n",
      "epoch no.4 train no.312720  loss = 2.71776 avg_loss = 3.12668\n",
      "epoch no.4 train no.312730  loss = 2.27239 avg_loss = 3.11188\n",
      "epoch no.4 train no.312740  loss = 2.70198 avg_loss = 3.14377\n",
      "epoch no.4 train no.312750  loss = 2.48406 avg_loss = 3.11207\n",
      "epoch no.4 train no.312760  loss = 1.58776 avg_loss = 3.09976\n",
      "epoch no.4 train no.312770  loss = 3.16220 avg_loss = 3.09216\n",
      "epoch no.4 train no.312780  loss = 3.15404 avg_loss = 3.07865\n",
      "epoch no.4 train no.312790  loss = 4.65578 avg_loss = 3.07772\n",
      "epoch no.4 train no.312800  loss = 3.25228 avg_loss = 3.06582\n",
      "epoch no.4 train no.312810  loss = 3.12996 avg_loss = 3.08742\n",
      "epoch no.4 train no.312820  loss = 3.07074 avg_loss = 3.08844\n",
      "epoch no.4 train no.312830  loss = 3.39261 avg_loss = 3.11127\n",
      "epoch no.4 train no.312840  loss = 2.08605 avg_loss = 3.10796\n",
      "epoch no.4 train no.312850  loss = 2.92726 avg_loss = 3.15505\n",
      "epoch no.4 train no.312860  loss = 2.85108 avg_loss = 3.12933\n",
      "epoch no.4 train no.312870  loss = 4.47887 avg_loss = 3.16083\n",
      "epoch no.4 train no.312880  loss = 4.54509 avg_loss = 3.17102\n",
      "epoch no.4 train no.312890  loss = 3.90064 avg_loss = 3.18850\n",
      "epoch no.4 train no.312900  loss = 4.08016 avg_loss = 3.13788\n",
      "epoch no.4 train no.312910  loss = 2.02890 avg_loss = 3.12627\n",
      "epoch no.4 train no.312920  loss = 2.29577 avg_loss = 3.10793\n",
      "epoch no.4 train no.312930  loss = 3.95062 avg_loss = 3.16191\n",
      "epoch no.4 train no.312940  loss = 4.48438 avg_loss = 3.14948\n",
      "epoch no.4 train no.312950  loss = 2.79462 avg_loss = 3.15080\n",
      "epoch no.4 train no.312960  loss = 3.01930 avg_loss = 3.14496\n",
      "epoch no.4 train no.312970  loss = 4.80309 avg_loss = 3.17573\n",
      "epoch no.4 train no.312980  loss = 2.30856 avg_loss = 3.17070\n",
      "epoch no.4 train no.312990  loss = 4.59687 avg_loss = 3.18092\n",
      "epoch no.4 train no.313000  loss = 2.00086 avg_loss = 3.19281\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '같은', '▁노래', 'nb', '</s>']\n",
      "여름밤의 꿀같은 rnb</s>\n",
      "epoch no.4 train no.313010  loss = 3.17885 avg_loss = 3.20455\n",
      "epoch no.4 train no.313020  loss = 2.63533 avg_loss = 3.17943\n",
      "epoch no.4 train no.313030  loss = 2.91969 avg_loss = 3.14576\n",
      "epoch no.4 train no.313040  loss = 2.77648 avg_loss = 3.16473\n",
      "epoch no.4 train no.313050  loss = 4.15307 avg_loss = 3.16835\n",
      "epoch no.4 train no.313060  loss = 2.73295 avg_loss = 3.15348\n",
      "epoch no.4 train no.313070  loss = 5.85736 avg_loss = 3.17065\n",
      "epoch no.4 train no.313080  loss = 1.80384 avg_loss = 3.14862\n",
      "epoch no.4 train no.313090  loss = 2.15077 avg_loss = 3.15088\n",
      "epoch no.4 train no.313100  loss = 3.07177 avg_loss = 3.13131\n",
      "epoch no.4 train no.313110  loss = 2.40906 avg_loss = 3.10770\n",
      "epoch no.4 train no.313120  loss = 3.09767 avg_loss = 3.11129\n",
      "epoch no.4 train no.313130  loss = 2.39843 avg_loss = 3.13140\n",
      "epoch no.4 train no.313140  loss = 2.18228 avg_loss = 3.13727\n",
      "epoch no.4 train no.313150  loss = 2.83182 avg_loss = 3.15246\n",
      "epoch no.4 train no.313160  loss = 3.30676 avg_loss = 3.14516\n",
      "epoch no.4 train no.313170  loss = 2.32586 avg_loss = 3.13158\n",
      "epoch no.4 train no.313180  loss = 3.69033 avg_loss = 3.16412\n",
      "epoch no.4 train no.313190  loss = 2.06750 avg_loss = 3.16123\n",
      "epoch no.4 train no.313200  loss = 3.43508 avg_loss = 3.12386\n",
      "epoch no.4 train no.313210  loss = 4.19031 avg_loss = 3.11545\n",
      "epoch no.4 train no.313220  loss = 2.94860 avg_loss = 3.15824\n",
      "epoch no.4 train no.313230  loss = 3.65537 avg_loss = 3.13333\n",
      "epoch no.4 train no.313240  loss = 4.35055 avg_loss = 3.13342\n",
      "epoch no.4 train no.313250  loss = 3.31166 avg_loss = 3.13437\n",
      "epoch no.4 train no.313260  loss = 2.50049 avg_loss = 3.09811\n",
      "epoch no.4 train no.313270  loss = 2.38442 avg_loss = 3.14641\n",
      "epoch no.4 train no.313280  loss = 2.38828 avg_loss = 3.17443\n",
      "epoch no.4 train no.313290  loss = 1.86089 avg_loss = 3.13461\n",
      "epoch no.4 train no.313300  loss = 1.50166 avg_loss = 3.07929\n",
      "epoch no.4 train no.313310  loss = 2.71218 avg_loss = 3.11984\n",
      "epoch no.4 train no.313320  loss = 2.69133 avg_loss = 3.12667\n",
      "epoch no.4 train no.313330  loss = 3.02935 avg_loss = 3.10709\n",
      "epoch no.4 train no.313340  loss = 2.61146 avg_loss = 3.12371\n",
      "epoch no.4 train no.313350  loss = 4.42719 avg_loss = 3.13845\n",
      "epoch no.4 train no.313360  loss = 1.61211 avg_loss = 3.15313\n",
      "epoch no.4 train no.313370  loss = 1.58077 avg_loss = 3.14181\n",
      "epoch no.4 train no.313380  loss = 2.61451 avg_loss = 3.10820\n",
      "epoch no.4 train no.313390  loss = 2.73900 avg_loss = 3.12346\n",
      "epoch no.4 train no.313400  loss = 2.76545 avg_loss = 3.14216\n",
      "epoch no.4 train no.313410  loss = 4.09255 avg_loss = 3.20956\n",
      "epoch no.4 train no.313420  loss = 3.23656 avg_loss = 3.20429\n",
      "epoch no.4 train no.313430  loss = 2.01668 avg_loss = 3.15497\n",
      "epoch no.4 train no.313440  loss = 2.44066 avg_loss = 3.11726\n",
      "epoch no.4 train no.313450  loss = 2.43362 avg_loss = 3.06107\n",
      "epoch no.4 train no.313460  loss = 3.10366 avg_loss = 3.04282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.313470  loss = 2.96267 avg_loss = 3.02979\n",
      "epoch no.4 train no.313480  loss = 3.44643 avg_loss = 3.02375\n",
      "epoch no.4 train no.313490  loss = 5.39499 avg_loss = 3.06147\n",
      "epoch no.4 train no.313500  loss = 2.15740 avg_loss = 3.08370\n",
      "epoch no.4 train no.313510  loss = 4.09652 avg_loss = 3.08557\n",
      "epoch no.4 train no.313520  loss = 2.75483 avg_loss = 3.07000\n",
      "epoch no.4 train no.313530  loss = 3.58819 avg_loss = 3.06710\n",
      "epoch no.4 train no.313540  loss = 2.23290 avg_loss = 3.02505\n",
      "epoch no.4 train no.313550  loss = 2.85558 avg_loss = 3.10732\n",
      "epoch no.4 train no.313560  loss = 2.93966 avg_loss = 3.10548\n",
      "epoch no.4 train no.313570  loss = 1.98750 avg_loss = 3.08464\n",
      "epoch no.4 train no.313580  loss = 2.22509 avg_loss = 3.05432\n",
      "epoch no.4 train no.313590  loss = 3.58668 avg_loss = 3.06116\n",
      "epoch no.4 train no.313600  loss = 3.86598 avg_loss = 3.08128\n",
      "epoch no.4 train no.313610  loss = 3.22422 avg_loss = 3.07184\n",
      "epoch no.4 train no.313620  loss = 3.88917 avg_loss = 3.09362\n",
      "epoch no.4 train no.313630  loss = 4.39301 avg_loss = 3.11211\n",
      "epoch no.4 train no.313640  loss = 2.43100 avg_loss = 3.11812\n",
      "epoch no.4 train no.313650  loss = 2.96226 avg_loss = 3.09085\n",
      "epoch no.4 train no.313660  loss = 4.20411 avg_loss = 3.16828\n",
      "epoch no.4 train no.313670  loss = 4.68619 avg_loss = 3.20691\n",
      "epoch no.4 train no.313680  loss = 2.58459 avg_loss = 3.22714\n",
      "epoch no.4 train no.313690  loss = 3.76232 avg_loss = 3.21754\n",
      "epoch no.4 train no.313700  loss = 2.13077 avg_loss = 3.20280\n",
      "epoch no.4 train no.313710  loss = 2.21746 avg_loss = 3.18021\n",
      "epoch no.4 train no.313720  loss = 4.95250 avg_loss = 3.22469\n",
      "epoch no.4 train no.313730  loss = 1.62348 avg_loss = 3.20866\n",
      "epoch no.4 train no.313740  loss = 2.34142 avg_loss = 3.19941\n",
      "epoch no.4 train no.313750  loss = 2.33347 avg_loss = 3.21528\n",
      "epoch no.4 train no.313760  loss = 2.15519 avg_loss = 3.18178\n",
      "epoch no.4 train no.313770  loss = 4.29960 avg_loss = 3.21112\n",
      "epoch no.4 train no.313780  loss = 2.59118 avg_loss = 3.20331\n",
      "epoch no.4 train no.313790  loss = 3.70385 avg_loss = 3.23835\n",
      "epoch no.4 train no.313800  loss = 2.97572 avg_loss = 3.22560\n",
      "epoch no.4 train no.313810  loss = 2.25735 avg_loss = 3.22174\n",
      "epoch no.4 train no.313820  loss = 5.04899 avg_loss = 3.26941\n",
      "epoch no.4 train no.313830  loss = 2.57505 avg_loss = 3.22290\n",
      "epoch no.4 train no.313840  loss = 2.85528 avg_loss = 3.17807\n",
      "epoch no.4 train no.313850  loss = 2.85253 avg_loss = 3.16024\n",
      "epoch no.4 train no.313860  loss = 2.74022 avg_loss = 3.17365\n",
      "epoch no.4 train no.313870  loss = 2.90117 avg_loss = 3.15037\n",
      "epoch no.4 train no.313880  loss = 2.03805 avg_loss = 3.14753\n",
      "epoch no.4 train no.313890  loss = 1.89666 avg_loss = 3.12302\n",
      "epoch no.4 train no.313900  loss = 4.26181 avg_loss = 3.09352\n",
      "epoch no.4 train no.313910  loss = 2.46208 avg_loss = 3.05215\n",
      "epoch no.4 train no.313920  loss = 4.35035 avg_loss = 3.06023\n",
      "epoch no.4 train no.313930  loss = 3.85531 avg_loss = 3.09294\n",
      "epoch no.4 train no.313940  loss = 3.30334 avg_loss = 3.09488\n",
      "epoch no.4 train no.313950  loss = 2.98591 avg_loss = 3.09111\n",
      "epoch no.4 train no.313960  loss = 4.50672 avg_loss = 3.08941\n",
      "epoch no.4 train no.313970  loss = 3.37025 avg_loss = 3.10185\n",
      "epoch no.4 train no.313980  loss = 2.64510 avg_loss = 3.12824\n",
      "epoch no.4 train no.313990  loss = 4.25278 avg_loss = 3.12580\n",
      "epoch no.4 train no.314000  loss = 3.81503 avg_loss = 3.09207\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁청량', '한', '▁노래', '</s>']\n",
      "여름과 어울리는 청량한 노래</s>\n",
      "epoch no.4 train no.314010  loss = 2.38151 avg_loss = 3.04949\n",
      "epoch no.4 train no.314020  loss = 2.73658 avg_loss = 3.04079\n",
      "epoch no.4 train no.314030  loss = 1.82928 avg_loss = 3.02313\n",
      "epoch no.4 train no.314040  loss = 3.05723 avg_loss = 3.02474\n",
      "epoch no.4 train no.314050  loss = 2.83075 avg_loss = 3.05245\n",
      "epoch no.4 train no.314060  loss = 3.18484 avg_loss = 3.04142\n",
      "epoch no.4 train no.314070  loss = 3.14560 avg_loss = 3.06233\n",
      "epoch no.4 train no.314080  loss = 3.26208 avg_loss = 3.10130\n",
      "epoch no.4 train no.314090  loss = 2.57614 avg_loss = 3.12378\n",
      "epoch no.4 train no.314100  loss = 3.03793 avg_loss = 3.16282\n",
      "epoch no.4 train no.314110  loss = 2.97182 avg_loss = 3.14802\n",
      "epoch no.4 train no.314120  loss = 2.47695 avg_loss = 3.18263\n",
      "epoch no.4 train no.314130  loss = 4.13242 avg_loss = 3.22704\n",
      "epoch no.4 train no.314140  loss = 3.23690 avg_loss = 3.20348\n",
      "epoch no.4 train no.314150  loss = 3.24782 avg_loss = 3.25253\n",
      "epoch no.4 train no.314160  loss = 4.44456 avg_loss = 3.30641\n",
      "epoch no.4 train no.314170  loss = 3.40755 avg_loss = 3.35027\n",
      "epoch no.4 train no.314180  loss = 2.53671 avg_loss = 3.30467\n",
      "epoch no.4 train no.314190  loss = 1.95575 avg_loss = 3.27706\n",
      "epoch no.4 train no.314200  loss = 2.78929 avg_loss = 3.23043\n",
      "epoch no.4 train no.314210  loss = 3.67456 avg_loss = 3.24278\n",
      "epoch no.4 train no.314220  loss = 3.12486 avg_loss = 3.25243\n",
      "epoch no.4 train no.314230  loss = 2.51995 avg_loss = 3.25212\n",
      "epoch no.4 train no.314240  loss = 3.65717 avg_loss = 3.25781\n",
      "epoch no.4 train no.314250  loss = 1.63784 avg_loss = 3.21207\n",
      "epoch no.4 train no.314260  loss = 4.21217 avg_loss = 3.26903\n",
      "epoch no.4 train no.314270  loss = 3.21268 avg_loss = 3.22852\n",
      "epoch no.4 train no.314280  loss = 1.78292 avg_loss = 3.19118\n",
      "epoch no.4 train no.314290  loss = 1.49570 avg_loss = 3.16202\n",
      "epoch no.4 train no.314300  loss = 4.27470 avg_loss = 3.14770\n",
      "epoch no.4 train no.314310  loss = 1.50685 avg_loss = 3.09358\n",
      "epoch no.4 train no.314320  loss = 3.48223 avg_loss = 3.10819\n",
      "epoch no.4 train no.314330  loss = 2.04527 avg_loss = 3.11853\n",
      "epoch no.4 train no.314340  loss = 3.69857 avg_loss = 3.14765\n",
      "epoch no.4 train no.314350  loss = 3.19557 avg_loss = 3.19232\n",
      "epoch no.4 train no.314360  loss = 2.97807 avg_loss = 3.17405\n",
      "epoch no.4 train no.314370  loss = 2.98468 avg_loss = 3.16831\n",
      "epoch no.4 train no.314380  loss = 2.86827 avg_loss = 3.20047\n",
      "epoch no.4 train no.314390  loss = 3.78092 avg_loss = 3.25969\n",
      "epoch no.4 train no.314400  loss = 2.33733 avg_loss = 3.20027\n",
      "epoch no.4 train no.314410  loss = 2.39240 avg_loss = 3.20436\n",
      "epoch no.4 train no.314420  loss = 2.48346 avg_loss = 3.18875\n",
      "epoch no.4 train no.314430  loss = 3.27917 avg_loss = 3.17909\n",
      "epoch no.4 train no.314440  loss = 3.76845 avg_loss = 3.24506\n",
      "epoch no.4 train no.314450  loss = 2.41785 avg_loss = 3.20324\n",
      "epoch no.4 train no.314460  loss = 1.92157 avg_loss = 3.21323\n",
      "epoch no.4 train no.314470  loss = 2.72625 avg_loss = 3.22449\n",
      "epoch no.4 train no.314480  loss = 5.09443 avg_loss = 3.23328\n",
      "epoch no.4 train no.314490  loss = 3.46120 avg_loss = 3.25696\n",
      "epoch no.4 train no.314500  loss = 2.53578 avg_loss = 3.25946\n",
      "epoch no.4 train no.314510  loss = 2.27637 avg_loss = 3.24513\n",
      "epoch no.4 train no.314520  loss = 3.91363 avg_loss = 3.25050\n",
      "epoch no.4 train no.314530  loss = 5.63329 avg_loss = 3.28746\n",
      "epoch no.4 train no.314540  loss = 5.07817 avg_loss = 3.25378\n",
      "epoch no.4 train no.314550  loss = 3.47304 avg_loss = 3.23245\n",
      "epoch no.4 train no.314560  loss = 4.03902 avg_loss = 3.25035\n",
      "epoch no.4 train no.314570  loss = 3.71201 avg_loss = 3.25173\n",
      "epoch no.4 train no.314580  loss = 3.18015 avg_loss = 3.23288\n",
      "epoch no.4 train no.314590  loss = 2.52413 avg_loss = 3.22913\n",
      "epoch no.4 train no.314600  loss = 5.11984 avg_loss = 3.23467\n",
      "epoch no.4 train no.314610  loss = 3.74114 avg_loss = 3.21431\n",
      "epoch no.4 train no.314620  loss = 3.20796 avg_loss = 3.19197\n",
      "epoch no.4 train no.314630  loss = 2.39442 avg_loss = 3.16486\n",
      "epoch no.4 train no.314640  loss = 2.63806 avg_loss = 3.18711\n",
      "epoch no.4 train no.314650  loss = 2.51904 avg_loss = 3.20923\n",
      "epoch no.4 train no.314660  loss = 3.00376 avg_loss = 3.22469\n",
      "epoch no.4 train no.314670  loss = 4.37932 avg_loss = 3.26152\n",
      "epoch no.4 train no.314680  loss = 1.79950 avg_loss = 3.21606\n",
      "epoch no.4 train no.314690  loss = 2.87335 avg_loss = 3.21680\n",
      "epoch no.4 train no.314700  loss = 2.96625 avg_loss = 3.24380\n",
      "epoch no.4 train no.314710  loss = 2.52830 avg_loss = 3.26485\n",
      "epoch no.4 train no.314720  loss = 4.64143 avg_loss = 3.29796\n",
      "epoch no.4 train no.314730  loss = 3.02134 avg_loss = 3.32622\n",
      "epoch no.4 train no.314740  loss = 3.06450 avg_loss = 3.34453\n",
      "epoch no.4 train no.314750  loss = 3.17331 avg_loss = 3.30089\n",
      "epoch no.4 train no.314760  loss = 3.01156 avg_loss = 3.33204\n",
      "epoch no.4 train no.314770  loss = 2.36883 avg_loss = 3.30163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.314780  loss = 2.76316 avg_loss = 3.30343\n",
      "epoch no.4 train no.314790  loss = 1.74869 avg_loss = 3.31813\n",
      "epoch no.4 train no.314800  loss = 3.81335 avg_loss = 3.32701\n",
      "epoch no.4 train no.314810  loss = 3.92153 avg_loss = 3.29781\n",
      "epoch no.4 train no.314820  loss = 2.28291 avg_loss = 3.25107\n",
      "epoch no.4 train no.314830  loss = 2.52324 avg_loss = 3.24333\n",
      "epoch no.4 train no.314840  loss = 2.44368 avg_loss = 3.25490\n",
      "epoch no.4 train no.314850  loss = 3.77914 avg_loss = 3.21947\n",
      "epoch no.4 train no.314860  loss = 3.86082 avg_loss = 3.23153\n",
      "epoch no.4 train no.314870  loss = 4.48961 avg_loss = 3.22567\n",
      "epoch no.4 train no.314880  loss = 2.45157 avg_loss = 3.20460\n",
      "epoch no.4 train no.314890  loss = 2.77389 avg_loss = 3.21449\n",
      "epoch no.4 train no.314900  loss = 2.47698 avg_loss = 3.21245\n",
      "epoch no.4 train no.314910  loss = 3.66631 avg_loss = 3.24048\n",
      "epoch no.4 train no.314920  loss = 3.42571 avg_loss = 3.21356\n",
      "epoch no.4 train no.314930  loss = 3.82958 avg_loss = 3.20233\n",
      "epoch no.4 train no.314940  loss = 3.44879 avg_loss = 3.22047\n",
      "epoch no.4 train no.314950  loss = 4.18854 avg_loss = 3.21885\n",
      "epoch no.4 train no.314960  loss = 2.60448 avg_loss = 3.21602\n",
      "epoch no.4 train no.314970  loss = 2.54747 avg_loss = 3.22725\n",
      "epoch no.4 train no.314980  loss = 3.11797 avg_loss = 3.19441\n",
      "epoch no.4 train no.314990  loss = 4.26135 avg_loss = 3.20600\n",
      "epoch no.4 train no.315000  loss = 4.48416 avg_loss = 3.22275\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓을', '▁노래', '한', '▁노래', '</s>', '</s>']\n",
      "여름밤을 수놓을 청량한 노래들</s>\n",
      "epoch no.4 train no.315010  loss = 3.01343 avg_loss = 3.21418\n",
      "epoch no.4 train no.315020  loss = 2.62170 avg_loss = 3.21096\n",
      "epoch no.4 train no.315030  loss = 3.47544 avg_loss = 3.23272\n",
      "epoch no.4 train no.315040  loss = 3.19066 avg_loss = 3.24738\n",
      "epoch no.4 train no.315050  loss = 3.43119 avg_loss = 3.25584\n",
      "epoch no.4 train no.315060  loss = 3.50569 avg_loss = 3.22490\n",
      "epoch no.4 train no.315070  loss = 2.65352 avg_loss = 3.27211\n",
      "epoch no.4 train no.315080  loss = 3.33190 avg_loss = 3.26678\n",
      "epoch no.4 train no.315090  loss = 1.77404 avg_loss = 3.26893\n",
      "epoch no.4 train no.315100  loss = 2.70614 avg_loss = 3.29028\n",
      "epoch no.4 train no.315110  loss = 3.24278 avg_loss = 3.30024\n",
      "epoch no.4 train no.315120  loss = 3.45326 avg_loss = 3.30661\n",
      "epoch no.4 train no.315130  loss = 3.34137 avg_loss = 3.29364\n",
      "epoch no.4 train no.315140  loss = 3.26269 avg_loss = 3.27738\n",
      "epoch no.4 train no.315150  loss = 3.48631 avg_loss = 3.27097\n",
      "epoch no.4 train no.315160  loss = 3.41731 avg_loss = 3.27492\n",
      "epoch no.4 train no.315170  loss = 3.35303 avg_loss = 3.23643\n",
      "epoch no.4 train no.315180  loss = 2.73129 avg_loss = 3.20345\n",
      "epoch no.4 train no.315190  loss = 2.63784 avg_loss = 3.18844\n",
      "epoch no.4 train no.315200  loss = 3.41969 avg_loss = 3.18236\n",
      "epoch no.4 train no.315210  loss = 2.86657 avg_loss = 3.19753\n",
      "epoch no.4 train no.315220  loss = 2.80863 avg_loss = 3.22564\n",
      "epoch no.4 train no.315230  loss = 3.61949 avg_loss = 3.19302\n",
      "epoch no.4 train no.315240  loss = 3.16897 avg_loss = 3.14013\n",
      "epoch no.4 train no.315250  loss = 2.30910 avg_loss = 3.12198\n",
      "epoch no.4 train no.315260  loss = 3.32967 avg_loss = 3.12300\n",
      "epoch no.4 train no.315270  loss = 3.50197 avg_loss = 3.21046\n",
      "epoch no.4 train no.315280  loss = 3.15850 avg_loss = 3.23238\n",
      "epoch no.4 train no.315290  loss = 1.89954 avg_loss = 3.20904\n",
      "epoch no.4 train no.315300  loss = 4.00425 avg_loss = 3.22550\n",
      "epoch no.4 train no.315310  loss = 2.70645 avg_loss = 3.18021\n",
      "epoch no.4 train no.315320  loss = 2.89941 avg_loss = 3.14667\n",
      "epoch no.4 train no.315330  loss = 2.96826 avg_loss = 3.14025\n",
      "epoch no.4 train no.315340  loss = 3.06830 avg_loss = 3.10611\n",
      "epoch no.4 train no.315350  loss = 3.99508 avg_loss = 3.13223\n",
      "epoch no.4 train no.315360  loss = 2.04426 avg_loss = 3.09564\n",
      "epoch no.4 train no.315370  loss = 2.50804 avg_loss = 3.11770\n",
      "epoch no.4 train no.315380  loss = 2.07939 avg_loss = 3.09585\n",
      "epoch no.4 train no.315390  loss = 2.74175 avg_loss = 3.07771\n",
      "epoch no.4 train no.315400  loss = 6.07833 avg_loss = 3.12951\n",
      "epoch no.4 train no.315410  loss = 4.40782 avg_loss = 3.22046\n",
      "epoch no.4 train no.315420  loss = 2.89797 avg_loss = 3.21104\n",
      "epoch no.4 train no.315430  loss = 2.99094 avg_loss = 3.19019\n",
      "epoch no.4 train no.315440  loss = 3.35630 avg_loss = 3.19239\n",
      "epoch no.4 train no.315450  loss = 5.86789 avg_loss = 3.25115\n",
      "epoch no.4 train no.315460  loss = 3.11378 avg_loss = 3.24210\n",
      "epoch no.4 train no.315470  loss = 1.35485 avg_loss = 3.20892\n",
      "epoch no.4 train no.315480  loss = 3.88327 avg_loss = 3.18930\n",
      "epoch no.4 train no.315490  loss = 3.25045 avg_loss = 3.22218\n",
      "epoch no.4 train no.315500  loss = 3.69708 avg_loss = 3.20468\n",
      "epoch no.4 train no.315510  loss = 2.45728 avg_loss = 3.25917\n",
      "epoch no.4 train no.315520  loss = 3.01786 avg_loss = 3.23498\n",
      "epoch no.4 train no.315530  loss = 2.97502 avg_loss = 3.22811\n",
      "epoch no.4 train no.315540  loss = 2.99442 avg_loss = 3.20977\n",
      "epoch no.4 train no.315550  loss = 3.67569 avg_loss = 3.22234\n",
      "epoch no.4 train no.315560  loss = 2.96756 avg_loss = 3.20834\n",
      "epoch no.4 train no.315570  loss = 4.57724 avg_loss = 3.25084\n",
      "epoch no.4 train no.315580  loss = 4.04694 avg_loss = 3.23963\n",
      "epoch no.4 train no.315590  loss = 1.87834 avg_loss = 3.24650\n",
      "epoch no.4 train no.315600  loss = 2.59119 avg_loss = 3.24306\n",
      "epoch no.4 train no.315610  loss = 2.63025 avg_loss = 3.19723\n",
      "epoch no.4 train no.315620  loss = 2.43226 avg_loss = 3.16554\n",
      "epoch no.4 train no.315630  loss = 3.30048 avg_loss = 3.19802\n",
      "epoch no.4 train no.315640  loss = 2.16241 avg_loss = 3.17392\n",
      "epoch no.4 train no.315650  loss = 3.88008 avg_loss = 3.21295\n",
      "epoch no.4 train no.315660  loss = 2.07932 avg_loss = 3.18190\n",
      "epoch no.4 train no.315670  loss = 2.74294 avg_loss = 3.16332\n",
      "epoch no.4 train no.315680  loss = 3.30476 avg_loss = 3.20413\n",
      "epoch no.4 train no.315690  loss = 3.94211 avg_loss = 3.17421\n",
      "epoch no.4 train no.315700  loss = 1.73507 avg_loss = 3.15913\n",
      "epoch no.4 train no.315710  loss = 3.03968 avg_loss = 3.09162\n",
      "epoch no.4 train no.315720  loss = 2.38166 avg_loss = 3.07305\n",
      "epoch no.4 train no.315730  loss = 2.02345 avg_loss = 3.04848\n",
      "epoch no.4 train no.315740  loss = 3.14195 avg_loss = 3.10093\n",
      "epoch no.4 train no.315750  loss = 2.14811 avg_loss = 3.11751\n",
      "epoch no.4 train no.315760  loss = 4.58084 avg_loss = 3.17850\n",
      "epoch no.4 train no.315770  loss = 3.38405 avg_loss = 3.16492\n",
      "epoch no.4 train no.315780  loss = 3.13876 avg_loss = 3.18350\n",
      "epoch no.4 train no.315790  loss = 3.86238 avg_loss = 3.15096\n",
      "epoch no.4 train no.315800  loss = 4.15190 avg_loss = 3.18000\n",
      "epoch no.4 train no.315810  loss = 2.06301 avg_loss = 3.13576\n",
      "epoch no.4 train no.315820  loss = 2.71621 avg_loss = 3.08885\n",
      "epoch no.4 train no.315830  loss = 3.05721 avg_loss = 3.11332\n",
      "epoch no.4 train no.315840  loss = 3.06985 avg_loss = 3.12443\n",
      "epoch no.4 train no.315850  loss = 3.89095 avg_loss = 3.13660\n",
      "epoch no.4 train no.315860  loss = 3.71188 avg_loss = 3.13520\n",
      "epoch no.4 train no.315870  loss = 3.28347 avg_loss = 3.17875\n",
      "epoch no.4 train no.315880  loss = 4.00185 avg_loss = 3.22031\n",
      "epoch no.4 train no.315890  loss = 3.00656 avg_loss = 3.21070\n",
      "epoch no.4 train no.315900  loss = 2.35964 avg_loss = 3.19933\n",
      "epoch no.4 train no.315910  loss = 2.25748 avg_loss = 3.22105\n",
      "epoch no.4 train no.315920  loss = 3.08075 avg_loss = 3.24279\n",
      "epoch no.4 train no.315930  loss = 1.72684 avg_loss = 3.22539\n",
      "epoch no.4 train no.315940  loss = 3.69048 avg_loss = 3.21320\n",
      "epoch no.4 train no.315950  loss = 3.38782 avg_loss = 3.24218\n",
      "epoch no.4 train no.315960  loss = 3.81239 avg_loss = 3.26205\n",
      "epoch no.4 train no.315970  loss = 3.73403 avg_loss = 3.27366\n",
      "epoch no.4 train no.315980  loss = 2.28347 avg_loss = 3.23752\n",
      "epoch no.4 train no.315990  loss = 3.48084 avg_loss = 3.20410\n",
      "epoch no.4 train no.316000  loss = 3.80660 avg_loss = 3.17014\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁어울리는', '▁청량', '</s>']\n",
      "여름과 잘 어울리는 음악</s>\n",
      "epoch no.4 train no.316010  loss = 2.32445 avg_loss = 3.19615\n",
      "epoch no.4 train no.316020  loss = 3.37902 avg_loss = 3.19407\n",
      "epoch no.4 train no.316030  loss = 3.06591 avg_loss = 3.19668\n",
      "epoch no.4 train no.316040  loss = 3.00151 avg_loss = 3.16674\n",
      "epoch no.4 train no.316050  loss = 3.75245 avg_loss = 3.13290\n",
      "epoch no.4 train no.316060  loss = 3.78722 avg_loss = 3.15165\n",
      "epoch no.4 train no.316070  loss = 3.50413 avg_loss = 3.15807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.316080  loss = 3.64956 avg_loss = 3.20027\n",
      "epoch no.4 train no.316090  loss = 4.73351 avg_loss = 3.17885\n",
      "epoch no.4 train no.316100  loss = 3.92180 avg_loss = 3.16800\n",
      "epoch no.4 train no.316110  loss = 2.96494 avg_loss = 3.16668\n",
      "epoch no.4 train no.316120  loss = 4.18981 avg_loss = 3.18286\n",
      "epoch no.4 train no.316130  loss = 2.41510 avg_loss = 3.13506\n",
      "epoch no.4 train no.316140  loss = 3.78397 avg_loss = 3.14468\n",
      "epoch no.4 train no.316150  loss = 1.79074 avg_loss = 3.06133\n",
      "epoch no.4 train no.316160  loss = 1.99415 avg_loss = 3.04209\n",
      "epoch no.4 train no.316170  loss = 3.73466 avg_loss = 3.04714\n",
      "epoch no.4 train no.316180  loss = 2.63325 avg_loss = 3.06135\n",
      "epoch no.4 train no.316190  loss = 3.36443 avg_loss = 3.06664\n",
      "epoch no.4 train no.316200  loss = 2.71357 avg_loss = 3.04183\n",
      "epoch no.4 train no.316210  loss = 2.18848 avg_loss = 3.01165\n",
      "epoch no.4 train no.316220  loss = 2.26978 avg_loss = 2.98472\n",
      "epoch no.4 train no.316230  loss = 3.79637 avg_loss = 3.04262\n",
      "epoch no.4 train no.316240  loss = 2.73257 avg_loss = 3.03404\n",
      "epoch no.4 train no.316250  loss = 3.38021 avg_loss = 3.06735\n",
      "epoch no.4 train no.316260  loss = 1.99957 avg_loss = 3.05875\n",
      "epoch no.4 train no.316270  loss = 3.63590 avg_loss = 3.06163\n",
      "epoch no.4 train no.316280  loss = 1.82073 avg_loss = 3.06949\n",
      "epoch no.4 train no.316290  loss = 2.74004 avg_loss = 3.08136\n",
      "epoch no.4 train no.316300  loss = 4.15438 avg_loss = 3.06666\n",
      "epoch no.4 train no.316310  loss = 3.60864 avg_loss = 3.10235\n",
      "epoch no.4 train no.316320  loss = 3.00382 avg_loss = 3.09968\n",
      "epoch no.4 train no.316330  loss = 3.84857 avg_loss = 3.13895\n",
      "epoch no.4 train no.316340  loss = 4.02758 avg_loss = 3.13207\n",
      "epoch no.4 train no.316350  loss = 5.47165 avg_loss = 3.13175\n",
      "epoch no.4 train no.316360  loss = 3.11322 avg_loss = 3.13423\n",
      "epoch no.4 train no.316370  loss = 2.44974 avg_loss = 3.14337\n",
      "epoch no.4 train no.316380  loss = 3.09120 avg_loss = 3.14650\n",
      "epoch no.4 train no.316390  loss = 6.41596 avg_loss = 3.20273\n",
      "epoch no.4 train no.316400  loss = 3.03429 avg_loss = 3.18324\n",
      "epoch no.4 train no.316410  loss = 2.87149 avg_loss = 3.15641\n",
      "epoch no.4 train no.316420  loss = 2.40406 avg_loss = 3.15061\n",
      "epoch no.4 train no.316430  loss = 2.15182 avg_loss = 3.15767\n",
      "epoch no.4 train no.316440  loss = 2.56253 avg_loss = 3.16508\n",
      "epoch no.4 train no.316450  loss = 2.53909 avg_loss = 3.20916\n",
      "epoch no.4 train no.316460  loss = 3.81009 avg_loss = 3.18691\n",
      "epoch no.4 train no.316470  loss = 1.98342 avg_loss = 3.20662\n",
      "epoch no.4 train no.316480  loss = 2.57682 avg_loss = 3.22678\n",
      "epoch no.4 train no.316490  loss = 3.30548 avg_loss = 3.26487\n",
      "epoch no.4 train no.316500  loss = 2.89725 avg_loss = 3.24842\n",
      "epoch no.4 train no.316510  loss = 3.08816 avg_loss = 3.22919\n",
      "epoch no.4 train no.316520  loss = 2.51327 avg_loss = 3.22019\n",
      "epoch no.4 train no.316530  loss = 4.62583 avg_loss = 3.21457\n",
      "epoch no.4 train no.316540  loss = 2.73384 avg_loss = 3.19289\n",
      "epoch no.4 train no.316550  loss = 4.22766 avg_loss = 3.18434\n",
      "epoch no.4 train no.316560  loss = 4.51620 avg_loss = 3.19537\n",
      "epoch no.4 train no.316570  loss = 2.83940 avg_loss = 3.15977\n",
      "epoch no.4 train no.316580  loss = 2.52238 avg_loss = 3.16204\n",
      "epoch no.4 train no.316590  loss = 3.69631 avg_loss = 3.17972\n",
      "epoch no.4 train no.316600  loss = 2.42944 avg_loss = 3.15363\n",
      "epoch no.4 train no.316610  loss = 3.38959 avg_loss = 3.15329\n",
      "epoch no.4 train no.316620  loss = 4.82843 avg_loss = 3.16757\n",
      "epoch no.4 train no.316630  loss = 3.41831 avg_loss = 3.17822\n",
      "epoch no.4 train no.316640  loss = 2.26604 avg_loss = 3.12660\n",
      "epoch no.4 train no.316650  loss = 3.25544 avg_loss = 3.13212\n",
      "epoch no.4 train no.316660  loss = 3.52687 avg_loss = 3.13493\n",
      "epoch no.4 train no.316670  loss = 3.89324 avg_loss = 3.16695\n",
      "epoch no.4 train no.316680  loss = 3.33211 avg_loss = 3.12719\n",
      "epoch no.4 train no.316690  loss = 4.00685 avg_loss = 3.12287\n",
      "epoch no.4 train no.316700  loss = 2.32956 avg_loss = 3.14378\n",
      "epoch no.4 train no.316710  loss = 2.74707 avg_loss = 3.11976\n",
      "epoch no.4 train no.316720  loss = 2.82776 avg_loss = 3.10625\n",
      "epoch no.4 train no.316730  loss = 2.50444 avg_loss = 3.10214\n",
      "epoch no.4 train no.316740  loss = 3.62497 avg_loss = 3.09725\n",
      "epoch no.4 train no.316750  loss = 2.16889 avg_loss = 3.08787\n",
      "epoch no.4 train no.316760  loss = 2.70215 avg_loss = 3.06034\n",
      "epoch no.4 train no.316770  loss = 2.95004 avg_loss = 3.05014\n",
      "epoch no.4 train no.316780  loss = 4.32935 avg_loss = 3.06796\n",
      "epoch no.4 train no.316790  loss = 4.17246 avg_loss = 3.10335\n",
      "epoch no.4 train no.316800  loss = 2.65681 avg_loss = 3.04610\n",
      "epoch no.4 train no.316810  loss = 5.80662 avg_loss = 3.08276\n",
      "epoch no.4 train no.316820  loss = 2.76381 avg_loss = 3.07332\n",
      "epoch no.4 train no.316830  loss = 2.42568 avg_loss = 3.07623\n",
      "epoch no.4 train no.316840  loss = 3.88331 avg_loss = 3.10632\n",
      "epoch no.4 train no.316850  loss = 2.62145 avg_loss = 3.09679\n",
      "epoch no.4 train no.316860  loss = 3.51771 avg_loss = 3.13596\n",
      "epoch no.4 train no.316870  loss = 3.05396 avg_loss = 3.18543\n",
      "epoch no.4 train no.316880  loss = 2.70420 avg_loss = 3.15033\n",
      "epoch no.4 train no.316890  loss = 2.74502 avg_loss = 3.17003\n",
      "epoch no.4 train no.316900  loss = 3.52330 avg_loss = 3.17623\n",
      "epoch no.4 train no.316910  loss = 2.53182 avg_loss = 3.18433\n",
      "epoch no.4 train no.316920  loss = 2.75214 avg_loss = 3.16681\n",
      "epoch no.4 train no.316930  loss = 3.25520 avg_loss = 3.15231\n",
      "epoch no.4 train no.316940  loss = 3.78366 avg_loss = 3.15064\n",
      "epoch no.4 train no.316950  loss = 2.88709 avg_loss = 3.13956\n",
      "epoch no.4 train no.316960  loss = 2.89567 avg_loss = 3.12500\n",
      "epoch no.4 train no.316970  loss = 3.21909 avg_loss = 3.11310\n",
      "epoch no.4 train no.316980  loss = 2.07736 avg_loss = 3.12013\n",
      "epoch no.4 train no.316990  loss = 2.11194 avg_loss = 3.11391\n",
      "epoch no.4 train no.317000  loss = 4.88946 avg_loss = 3.16151\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁청량', '한', '▁노래', '</s>']\n",
      "여름과 어울리는 청량한 재즈</s>\n",
      "epoch no.4 train no.317010  loss = 3.27966 avg_loss = 3.17852\n",
      "epoch no.4 train no.317020  loss = 2.82551 avg_loss = 3.14016\n",
      "epoch no.4 train no.317030  loss = 2.67113 avg_loss = 3.12825\n",
      "epoch no.4 train no.317040  loss = 2.81018 avg_loss = 3.11044\n",
      "epoch no.4 train no.317050  loss = 2.33976 avg_loss = 3.10778\n",
      "epoch no.4 train no.317060  loss = 1.89117 avg_loss = 3.11703\n",
      "epoch no.4 train no.317070  loss = 4.39632 avg_loss = 3.14554\n",
      "epoch no.4 train no.317080  loss = 2.58164 avg_loss = 3.10124\n",
      "epoch no.4 train no.317090  loss = 3.39747 avg_loss = 3.13129\n",
      "epoch no.4 train no.317100  loss = 3.68828 avg_loss = 3.14180\n",
      "epoch no.4 train no.317110  loss = 3.09552 avg_loss = 3.12022\n",
      "epoch no.4 train no.317120  loss = 2.57037 avg_loss = 3.14042\n",
      "epoch no.4 train no.317130  loss = 3.29358 avg_loss = 3.14258\n",
      "epoch no.4 train no.317140  loss = 4.75874 avg_loss = 3.13858\n",
      "epoch no.4 train no.317150  loss = 4.11612 avg_loss = 3.11132\n",
      "epoch no.4 train no.317160  loss = 2.34164 avg_loss = 3.10509\n",
      "epoch no.4 train no.317170  loss = 3.80795 avg_loss = 3.08888\n",
      "epoch no.4 train no.317180  loss = 3.01083 avg_loss = 3.07303\n",
      "epoch no.4 train no.317190  loss = 2.69333 avg_loss = 3.06810\n",
      "epoch no.4 train no.317200  loss = 2.68576 avg_loss = 3.08516\n",
      "epoch no.4 train no.317210  loss = 3.16620 avg_loss = 3.10180\n",
      "epoch no.4 train no.317220  loss = 2.40028 avg_loss = 3.10412\n",
      "epoch no.4 train no.317230  loss = 2.19961 avg_loss = 3.08624\n",
      "epoch no.4 train no.317240  loss = 4.07247 avg_loss = 3.09417\n",
      "epoch no.4 train no.317250  loss = 2.39614 avg_loss = 3.06054\n",
      "epoch no.4 train no.317260  loss = 2.20460 avg_loss = 3.05287\n",
      "epoch no.4 train no.317270  loss = 2.79389 avg_loss = 3.07802\n",
      "epoch no.4 train no.317280  loss = 2.65326 avg_loss = 3.06293\n",
      "epoch no.4 train no.317290  loss = 2.84477 avg_loss = 3.09847\n",
      "epoch no.4 train no.317300  loss = 3.71057 avg_loss = 3.14267\n",
      "epoch no.4 train no.317310  loss = 3.90283 avg_loss = 3.13937\n",
      "epoch no.4 train no.317320  loss = 3.20516 avg_loss = 3.14908\n",
      "epoch no.4 train no.317330  loss = 3.80515 avg_loss = 3.15323\n",
      "epoch no.4 train no.317340  loss = 3.62395 avg_loss = 3.16497\n",
      "epoch no.4 train no.317350  loss = 3.41641 avg_loss = 3.15992\n",
      "epoch no.4 train no.317360  loss = 4.92962 avg_loss = 3.17396\n",
      "epoch no.4 train no.317370  loss = 2.53298 avg_loss = 3.17415\n",
      "epoch no.4 train no.317380  loss = 3.48361 avg_loss = 3.15517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.317390  loss = 3.23020 avg_loss = 3.14666\n",
      "epoch no.4 train no.317400  loss = 2.60154 avg_loss = 3.14466\n",
      "epoch no.4 train no.317410  loss = 2.62939 avg_loss = 3.17523\n",
      "epoch no.4 train no.317420  loss = 3.14090 avg_loss = 3.18115\n",
      "epoch no.4 train no.317430  loss = 3.33289 avg_loss = 3.18551\n",
      "epoch no.4 train no.317440  loss = 1.55689 avg_loss = 3.12688\n",
      "epoch no.4 train no.317450  loss = 2.84325 avg_loss = 3.13196\n",
      "epoch no.4 train no.317460  loss = 2.26871 avg_loss = 3.11971\n",
      "epoch no.4 train no.317470  loss = 2.88268 avg_loss = 3.10057\n",
      "epoch no.4 train no.317480  loss = 4.69790 avg_loss = 3.11576\n",
      "epoch no.4 train no.317490  loss = 2.35373 avg_loss = 3.09518\n",
      "epoch no.4 train no.317500  loss = 3.99384 avg_loss = 3.11896\n",
      "epoch no.4 train no.317510  loss = 1.69620 avg_loss = 3.09082\n",
      "epoch no.4 train no.317520  loss = 3.13861 avg_loss = 3.10689\n",
      "epoch no.4 train no.317530  loss = 3.98067 avg_loss = 3.13721\n",
      "epoch no.4 train no.317540  loss = 3.17571 avg_loss = 3.13245\n",
      "epoch no.4 train no.317550  loss = 3.69930 avg_loss = 3.12343\n",
      "epoch no.4 train no.317560  loss = 3.24452 avg_loss = 3.12117\n",
      "epoch no.4 train no.317570  loss = 3.69871 avg_loss = 3.15945\n",
      "epoch no.4 train no.317580  loss = 4.82516 avg_loss = 3.14896\n",
      "epoch no.4 train no.317590  loss = 4.67839 avg_loss = 3.16547\n",
      "epoch no.4 train no.317600  loss = 3.06748 avg_loss = 3.12408\n",
      "epoch no.4 train no.317610  loss = 2.69048 avg_loss = 3.08587\n",
      "epoch no.4 train no.317620  loss = 3.16790 avg_loss = 3.11345\n",
      "epoch no.4 train no.317630  loss = 3.51619 avg_loss = 3.09868\n",
      "epoch no.4 train no.317640  loss = 3.34445 avg_loss = 3.11838\n",
      "epoch no.4 train no.317650  loss = 4.60678 avg_loss = 3.20083\n",
      "epoch no.4 train no.317660  loss = 5.52888 avg_loss = 3.24966\n",
      "epoch no.4 train no.317670  loss = 3.68904 avg_loss = 3.21012\n",
      "epoch no.4 train no.317680  loss = 2.70670 avg_loss = 3.17794\n",
      "epoch no.4 train no.317690  loss = 4.20657 avg_loss = 3.19443\n",
      "epoch no.4 train no.317700  loss = 3.79627 avg_loss = 3.19741\n",
      "epoch no.4 train no.317710  loss = 4.69060 avg_loss = 3.19612\n",
      "epoch no.4 train no.317720  loss = 4.47829 avg_loss = 3.26341\n",
      "epoch no.4 train no.317730  loss = 2.48847 avg_loss = 3.27672\n",
      "epoch no.4 train no.317740  loss = 2.51685 avg_loss = 3.26108\n",
      "epoch no.4 train no.317750  loss = 1.81914 avg_loss = 3.26867\n",
      "epoch no.4 train no.317760  loss = 2.32605 avg_loss = 3.22057\n",
      "epoch no.4 train no.317770  loss = 4.22290 avg_loss = 3.22459\n",
      "epoch no.4 train no.317780  loss = 2.27230 avg_loss = 3.17484\n",
      "epoch no.4 train no.317790  loss = 3.58890 avg_loss = 3.12550\n",
      "epoch no.4 train no.317800  loss = 5.30050 avg_loss = 3.13290\n",
      "epoch no.4 train no.317810  loss = 2.21066 avg_loss = 3.15522\n",
      "epoch no.4 train no.317820  loss = 2.69652 avg_loss = 3.13528\n",
      "epoch no.4 train no.317830  loss = 3.53701 avg_loss = 3.14699\n",
      "epoch no.4 train no.317840  loss = 3.36848 avg_loss = 3.12698\n",
      "epoch no.4 train no.317850  loss = 3.28170 avg_loss = 3.09494\n",
      "epoch no.4 train no.317860  loss = 4.14947 avg_loss = 3.10566\n",
      "epoch no.4 train no.317870  loss = 4.44468 avg_loss = 3.13068\n",
      "epoch no.4 train no.317880  loss = 4.69524 avg_loss = 3.15067\n",
      "epoch no.4 train no.317890  loss = 3.13897 avg_loss = 3.16642\n",
      "epoch no.4 train no.317900  loss = 1.98852 avg_loss = 3.16239\n",
      "epoch no.4 train no.317910  loss = 5.35164 avg_loss = 3.15862\n",
      "epoch no.4 train no.317920  loss = 1.96830 avg_loss = 3.15026\n",
      "epoch no.4 train no.317930  loss = 2.19927 avg_loss = 3.14510\n",
      "epoch no.4 train no.317940  loss = 3.11870 avg_loss = 3.12178\n",
      "epoch no.4 train no.317950  loss = 2.11093 avg_loss = 3.12744\n",
      "epoch no.4 train no.317960  loss = 3.70731 avg_loss = 3.10595\n",
      "epoch no.4 train no.317970  loss = 2.17222 avg_loss = 3.10447\n",
      "epoch no.4 train no.317980  loss = 4.01307 avg_loss = 3.17414\n",
      "epoch no.4 train no.317990  loss = 2.69545 avg_loss = 3.16651\n",
      "epoch no.4 train no.318000  loss = 3.17567 avg_loss = 3.16852\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '▁끝', '자', '락', '에서', '▁가을', '▁노래', '노래', '</s>']\n",
      "여름의 끝자락에서 듣는 여름노래</s>\n",
      "epoch no.4 train no.318010  loss = 3.11762 avg_loss = 3.16662\n",
      "epoch no.4 train no.318020  loss = 2.24698 avg_loss = 3.11109\n",
      "epoch no.4 train no.318030  loss = 2.42802 avg_loss = 3.12952\n",
      "epoch no.4 train no.318040  loss = 5.00687 avg_loss = 3.11735\n",
      "epoch no.4 train no.318050  loss = 4.14125 avg_loss = 3.12913\n",
      "epoch no.4 train no.318060  loss = 2.49826 avg_loss = 3.11979\n",
      "epoch no.4 train no.318070  loss = 2.23282 avg_loss = 3.08990\n",
      "epoch no.4 train no.318080  loss = 2.50493 avg_loss = 3.07846\n",
      "epoch no.4 train no.318090  loss = 3.69313 avg_loss = 3.12058\n",
      "epoch no.4 train no.318100  loss = 2.83836 avg_loss = 3.17653\n",
      "epoch no.4 train no.318110  loss = 5.27468 avg_loss = 3.19411\n",
      "epoch no.4 train no.318120  loss = 3.85925 avg_loss = 3.18079\n",
      "epoch no.4 train no.318130  loss = 2.89365 avg_loss = 3.20106\n",
      "epoch no.4 train no.318140  loss = 2.73009 avg_loss = 3.18291\n",
      "epoch no.4 train no.318150  loss = 3.39924 avg_loss = 3.17721\n",
      "epoch no.4 train no.318160  loss = 3.20998 avg_loss = 3.16689\n",
      "epoch no.4 train no.318170  loss = 3.52037 avg_loss = 3.14278\n",
      "epoch no.4 train no.318180  loss = 2.93202 avg_loss = 3.16927\n",
      "epoch no.4 train no.318190  loss = 3.19561 avg_loss = 3.16002\n",
      "epoch no.4 train no.318200  loss = 3.74114 avg_loss = 3.09473\n",
      "epoch no.4 train no.318210  loss = 3.97087 avg_loss = 3.17360\n",
      "epoch no.4 train no.318220  loss = 3.51525 avg_loss = 3.18827\n",
      "epoch no.4 train no.318230  loss = 3.41456 avg_loss = 3.15594\n",
      "epoch no.4 train no.318240  loss = 3.21174 avg_loss = 3.16095\n",
      "epoch no.4 train no.318250  loss = 2.75704 avg_loss = 3.12073\n",
      "epoch no.4 train no.318260  loss = 2.26984 avg_loss = 3.06199\n",
      "epoch no.4 train no.318270  loss = 3.44599 avg_loss = 3.03945\n",
      "epoch no.4 train no.318280  loss = 2.02757 avg_loss = 3.04851\n",
      "epoch no.4 train no.318290  loss = 3.09832 avg_loss = 3.05771\n",
      "epoch no.4 train no.318300  loss = 3.33806 avg_loss = 3.09896\n",
      "epoch no.4 train no.318310  loss = 5.41336 avg_loss = 3.11173\n",
      "epoch no.4 train no.318320  loss = 2.41841 avg_loss = 3.10165\n",
      "epoch no.4 train no.318330  loss = 3.33370 avg_loss = 3.12020\n",
      "epoch no.4 train no.318340  loss = 4.49469 avg_loss = 3.12330\n",
      "epoch no.4 train no.318350  loss = 3.55422 avg_loss = 3.13717\n",
      "epoch no.4 train no.318360  loss = 3.68496 avg_loss = 3.16337\n",
      "epoch no.4 train no.318370  loss = 2.25201 avg_loss = 3.14491\n",
      "epoch no.4 train no.318380  loss = 1.52688 avg_loss = 3.12485\n",
      "epoch no.4 train no.318390  loss = 2.82529 avg_loss = 3.09605\n",
      "epoch no.4 train no.318400  loss = 1.85447 avg_loss = 3.08090\n",
      "epoch no.4 train no.318410  loss = 3.18894 avg_loss = 3.10763\n",
      "epoch no.4 train no.318420  loss = 3.55616 avg_loss = 3.08582\n",
      "epoch no.4 train no.318430  loss = 3.19916 avg_loss = 3.09794\n",
      "epoch no.4 train no.318440  loss = 2.41409 avg_loss = 3.08894\n",
      "epoch no.4 train no.318450  loss = 3.29408 avg_loss = 3.11270\n",
      "epoch no.4 train no.318460  loss = 2.95216 avg_loss = 3.10179\n",
      "epoch no.4 train no.318470  loss = 1.72164 avg_loss = 3.14332\n",
      "epoch no.4 train no.318480  loss = 4.16328 avg_loss = 3.18664\n",
      "epoch no.4 train no.318490  loss = 4.02693 avg_loss = 3.21187\n",
      "epoch no.4 train no.318500  loss = 3.13155 avg_loss = 3.21633\n",
      "epoch no.4 train no.318510  loss = 2.86665 avg_loss = 3.18386\n",
      "epoch no.4 train no.318520  loss = 2.93369 avg_loss = 3.17931\n",
      "epoch no.4 train no.318530  loss = 2.50832 avg_loss = 3.18511\n",
      "epoch no.4 train no.318540  loss = 3.48243 avg_loss = 3.19445\n",
      "epoch no.4 train no.318550  loss = 3.28235 avg_loss = 3.21211\n",
      "epoch no.4 train no.318560  loss = 1.68476 avg_loss = 3.18408\n",
      "epoch no.4 train no.318570  loss = 3.03351 avg_loss = 3.16559\n",
      "epoch no.4 train no.318580  loss = 4.01565 avg_loss = 3.14275\n",
      "epoch no.4 train no.318590  loss = 2.06227 avg_loss = 3.11789\n",
      "epoch no.4 train no.318600  loss = 2.93526 avg_loss = 3.13201\n",
      "epoch no.4 train no.318610  loss = 3.07527 avg_loss = 3.11863\n",
      "epoch no.4 train no.318620  loss = 1.97215 avg_loss = 3.10280\n",
      "epoch no.4 train no.318630  loss = 2.94290 avg_loss = 3.09318\n",
      "epoch no.4 train no.318640  loss = 2.55455 avg_loss = 3.11565\n",
      "epoch no.4 train no.318650  loss = 2.38229 avg_loss = 3.11579\n",
      "epoch no.4 train no.318660  loss = 3.13608 avg_loss = 3.10985\n",
      "epoch no.4 train no.318670  loss = 2.66824 avg_loss = 3.12801\n",
      "epoch no.4 train no.318680  loss = 1.88715 avg_loss = 3.16866\n",
      "epoch no.4 train no.318690  loss = 4.39754 avg_loss = 3.15098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.318700  loss = 4.02796 avg_loss = 3.15411\n",
      "epoch no.4 train no.318710  loss = 4.52177 avg_loss = 3.14551\n",
      "epoch no.4 train no.318720  loss = 3.91404 avg_loss = 3.17067\n",
      "epoch no.4 train no.318730  loss = 2.25788 avg_loss = 3.13058\n",
      "epoch no.4 train no.318740  loss = 2.49714 avg_loss = 3.11613\n",
      "epoch no.4 train no.318750  loss = 2.63275 avg_loss = 3.10076\n",
      "epoch no.4 train no.318760  loss = 2.98363 avg_loss = 3.08062\n",
      "epoch no.4 train no.318770  loss = 3.37554 avg_loss = 3.06027\n",
      "epoch no.4 train no.318780  loss = 3.86800 avg_loss = 3.09736\n",
      "epoch no.4 train no.318790  loss = 3.32834 avg_loss = 3.08906\n",
      "epoch no.4 train no.318800  loss = 3.02874 avg_loss = 3.14145\n",
      "epoch no.4 train no.318810  loss = 3.90071 avg_loss = 3.15479\n",
      "epoch no.4 train no.318820  loss = 3.07558 avg_loss = 3.16581\n",
      "epoch no.4 train no.318830  loss = 5.42699 avg_loss = 3.16956\n",
      "epoch no.4 train no.318840  loss = 4.38018 avg_loss = 3.19538\n",
      "epoch no.4 train no.318850  loss = 3.54517 avg_loss = 3.17448\n",
      "epoch no.4 train no.318860  loss = 5.26585 avg_loss = 3.23121\n",
      "epoch no.4 train no.318870  loss = 3.51791 avg_loss = 3.22762\n",
      "epoch no.4 train no.318880  loss = 2.83936 avg_loss = 3.20682\n",
      "epoch no.4 train no.318890  loss = 3.71732 avg_loss = 3.19227\n",
      "epoch no.4 train no.318900  loss = 3.09528 avg_loss = 3.16693\n",
      "epoch no.4 train no.318910  loss = 3.38756 avg_loss = 3.15174\n",
      "epoch no.4 train no.318920  loss = 3.60632 avg_loss = 3.10323\n",
      "epoch no.4 train no.318930  loss = 2.33957 avg_loss = 3.11765\n",
      "epoch no.4 train no.318940  loss = 2.60200 avg_loss = 3.11001\n",
      "epoch no.4 train no.318950  loss = 4.91501 avg_loss = 3.13146\n",
      "epoch no.4 train no.318960  loss = 2.74425 avg_loss = 3.10708\n",
      "epoch no.4 train no.318970  loss = 1.89277 avg_loss = 3.10384\n",
      "epoch no.4 train no.318980  loss = 3.47098 avg_loss = 3.10266\n",
      "epoch no.4 train no.318990  loss = 1.61920 avg_loss = 3.05594\n",
      "epoch no.4 train no.319000  loss = 3.27709 avg_loss = 3.03364\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '의', '하며', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤 산책하며 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.319010  loss = 4.71814 avg_loss = 3.05229\n",
      "epoch no.4 train no.319020  loss = 2.09546 avg_loss = 3.04885\n",
      "epoch no.4 train no.319030  loss = 2.55054 avg_loss = 3.04686\n",
      "epoch no.4 train no.319040  loss = 3.87286 avg_loss = 3.07512\n",
      "epoch no.4 train no.319050  loss = 3.48897 avg_loss = 3.09110\n",
      "epoch no.4 train no.319060  loss = 3.32155 avg_loss = 3.12670\n",
      "epoch no.4 train no.319070  loss = 4.02093 avg_loss = 3.09376\n",
      "epoch no.4 train no.319080  loss = 3.11883 avg_loss = 3.09358\n",
      "epoch no.4 train no.319090  loss = 3.08293 avg_loss = 3.06511\n",
      "epoch no.4 train no.319100  loss = 3.06822 avg_loss = 3.04512\n",
      "epoch no.4 train no.319110  loss = 4.18334 avg_loss = 3.06360\n",
      "epoch no.4 train no.319120  loss = 3.10244 avg_loss = 3.08943\n",
      "epoch no.4 train no.319130  loss = 3.25763 avg_loss = 3.10029\n",
      "epoch no.4 train no.319140  loss = 2.47135 avg_loss = 3.07247\n",
      "epoch no.4 train no.319150  loss = 2.67862 avg_loss = 3.08721\n",
      "epoch no.4 train no.319160  loss = 3.57682 avg_loss = 3.07971\n",
      "epoch no.4 train no.319170  loss = 1.09486 avg_loss = 3.07469\n",
      "epoch no.4 train no.319180  loss = 2.84167 avg_loss = 3.05772\n",
      "epoch no.4 train no.319190  loss = 3.54852 avg_loss = 3.05387\n",
      "epoch no.4 train no.319200  loss = 2.91201 avg_loss = 3.05175\n",
      "epoch no.4 train no.319210  loss = 3.14289 avg_loss = 3.08203\n",
      "epoch no.4 train no.319220  loss = 3.60044 avg_loss = 3.09967\n",
      "epoch no.4 train no.319230  loss = 3.86023 avg_loss = 3.11931\n",
      "epoch no.4 train no.319240  loss = 2.38433 avg_loss = 3.15682\n",
      "epoch no.4 train no.319250  loss = 1.62967 avg_loss = 3.19119\n",
      "epoch no.4 train no.319260  loss = 2.82536 avg_loss = 3.22229\n",
      "epoch no.4 train no.319270  loss = 2.47644 avg_loss = 3.23180\n",
      "epoch no.4 train no.319280  loss = 4.62728 avg_loss = 3.21645\n",
      "epoch no.4 train no.319290  loss = 3.04398 avg_loss = 3.17411\n",
      "epoch no.4 train no.319300  loss = 2.06941 avg_loss = 3.12956\n",
      "epoch no.4 train no.319310  loss = 3.15598 avg_loss = 3.11709\n",
      "epoch no.4 train no.319320  loss = 3.95960 avg_loss = 3.15303\n",
      "epoch no.4 train no.319330  loss = 2.00008 avg_loss = 3.14248\n",
      "epoch no.4 train no.319340  loss = 3.26513 avg_loss = 3.12778\n",
      "epoch no.4 train no.319350  loss = 4.03876 avg_loss = 3.14380\n",
      "epoch no.4 train no.319360  loss = 3.10717 avg_loss = 3.11130\n",
      "epoch no.4 train no.319370  loss = 2.07654 avg_loss = 3.09261\n",
      "epoch no.4 train no.319380  loss = 2.31440 avg_loss = 3.10079\n",
      "epoch no.4 train no.319390  loss = 3.21211 avg_loss = 3.09856\n",
      "epoch no.4 train no.319400  loss = 2.66117 avg_loss = 3.09809\n",
      "epoch no.4 train no.319410  loss = 2.71301 avg_loss = 3.07798\n",
      "epoch no.4 train no.319420  loss = 2.98047 avg_loss = 3.06496\n",
      "epoch no.4 train no.319430  loss = 2.83482 avg_loss = 3.06932\n",
      "epoch no.4 train no.319440  loss = 3.71062 avg_loss = 3.05830\n",
      "epoch no.4 train no.319450  loss = 2.84710 avg_loss = 3.06023\n",
      "epoch no.4 train no.319460  loss = 2.01305 avg_loss = 3.03421\n",
      "epoch no.4 train no.319470  loss = 2.65090 avg_loss = 3.02074\n",
      "epoch no.4 train no.319480  loss = 3.24435 avg_loss = 3.11258\n",
      "epoch no.4 train no.319490  loss = 3.00129 avg_loss = 3.13641\n",
      "epoch no.4 train no.319500  loss = 3.44135 avg_loss = 3.15551\n",
      "epoch no.4 train no.319510  loss = 2.90405 avg_loss = 3.14558\n",
      "epoch no.4 train no.319520  loss = 1.58925 avg_loss = 3.15413\n",
      "epoch no.4 train no.319530  loss = 2.85361 avg_loss = 3.15741\n",
      "epoch no.4 train no.319540  loss = 3.71564 avg_loss = 3.13445\n",
      "epoch no.4 train no.319550  loss = 6.66888 avg_loss = 3.16894\n",
      "epoch no.4 train no.319560  loss = 2.82943 avg_loss = 3.15354\n",
      "epoch no.4 train no.319570  loss = 5.23724 avg_loss = 3.16533\n",
      "epoch no.4 train no.319580  loss = 4.83230 avg_loss = 3.21052\n",
      "epoch no.4 train no.319590  loss = 2.73322 avg_loss = 3.14230\n",
      "epoch no.4 train no.319600  loss = 2.13007 avg_loss = 3.16711\n",
      "epoch no.4 train no.319610  loss = 2.19929 avg_loss = 3.14175\n",
      "epoch no.4 train no.319620  loss = 3.60282 avg_loss = 3.14090\n",
      "epoch no.4 train no.319630  loss = 2.29963 avg_loss = 3.11436\n",
      "epoch no.4 train no.319640  loss = 1.84399 avg_loss = 3.10368\n",
      "epoch no.4 train no.319650  loss = 2.04556 avg_loss = 3.08670\n",
      "epoch no.4 train no.319660  loss = 3.23229 avg_loss = 3.10445\n",
      "epoch no.4 train no.319670  loss = 2.07352 avg_loss = 3.09385\n",
      "epoch no.4 train no.319680  loss = 3.66031 avg_loss = 3.11329\n",
      "epoch no.4 train no.319690  loss = 1.86800 avg_loss = 3.09317\n",
      "epoch no.4 train no.319700  loss = 1.63896 avg_loss = 3.09141\n",
      "epoch no.4 train no.319710  loss = 3.70043 avg_loss = 3.06373\n",
      "epoch no.4 train no.319720  loss = 2.84213 avg_loss = 3.06273\n",
      "epoch no.4 train no.319730  loss = 3.00299 avg_loss = 3.09668\n",
      "epoch no.4 train no.319740  loss = 2.15346 avg_loss = 3.05084\n",
      "epoch no.4 train no.319750  loss = 3.41875 avg_loss = 3.02817\n",
      "epoch no.4 train no.319760  loss = 1.99617 avg_loss = 2.98919\n",
      "epoch no.4 train no.319770  loss = 3.06297 avg_loss = 2.99762\n",
      "epoch no.4 train no.319780  loss = 3.34141 avg_loss = 3.04774\n",
      "epoch no.4 train no.319790  loss = 3.36698 avg_loss = 3.08079\n",
      "epoch no.4 train no.319800  loss = 2.75957 avg_loss = 3.09649\n",
      "epoch no.4 train no.319810  loss = 3.31478 avg_loss = 3.08188\n",
      "epoch no.4 train no.319820  loss = 4.08477 avg_loss = 3.11736\n",
      "epoch no.4 train no.319830  loss = 2.90556 avg_loss = 3.14009\n",
      "epoch no.4 train no.319840  loss = 3.76811 avg_loss = 3.10175\n",
      "epoch no.4 train no.319850  loss = 2.10064 avg_loss = 3.11334\n",
      "epoch no.4 train no.319860  loss = 2.44636 avg_loss = 3.09926\n",
      "epoch no.4 train no.319870  loss = 3.45006 avg_loss = 3.16667\n",
      "epoch no.4 train no.319880  loss = 2.82217 avg_loss = 3.13642\n",
      "epoch no.4 train no.319890  loss = 3.08055 avg_loss = 3.15400\n",
      "epoch no.4 train no.319900  loss = 3.39832 avg_loss = 3.14705\n",
      "epoch no.4 train no.319910  loss = 3.06059 avg_loss = 3.12634\n",
      "epoch no.4 train no.319920  loss = 4.09369 avg_loss = 3.10547\n",
      "epoch no.4 train no.319930  loss = 4.83378 avg_loss = 3.09436\n",
      "epoch no.4 train no.319940  loss = 1.99528 avg_loss = 3.09890\n",
      "epoch no.4 train no.319950  loss = 2.85851 avg_loss = 3.11518\n",
      "epoch no.4 train no.319960  loss = 4.16469 avg_loss = 3.11697\n",
      "epoch no.4 train no.319970  loss = 4.00719 avg_loss = 3.13839\n",
      "epoch no.4 train no.319980  loss = 3.43331 avg_loss = 3.14341\n",
      "epoch no.4 train no.319990  loss = 2.99711 avg_loss = 3.17789\n",
      "epoch no.4 train no.320000  loss = 4.49866 avg_loss = 3.17729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '브', '한', '▁음악', 'op', '</s>']\n",
      "여름밤에 듣는  그루브한 pop</s>\n",
      "epoch no.4 train no.320010  loss = 1.79593 avg_loss = 3.16828\n",
      "epoch no.4 train no.320020  loss = 2.37518 avg_loss = 3.13520\n",
      "epoch no.4 train no.320030  loss = 4.06548 avg_loss = 3.14774\n",
      "epoch no.4 train no.320040  loss = 2.86014 avg_loss = 3.15028\n",
      "epoch no.4 train no.320050  loss = 5.35845 avg_loss = 3.16766\n",
      "epoch no.4 train no.320060  loss = 3.21421 avg_loss = 3.13281\n",
      "epoch no.4 train no.320070  loss = 2.33510 avg_loss = 3.12477\n",
      "epoch no.4 train no.320080  loss = 3.31999 avg_loss = 3.12921\n",
      "epoch no.4 train no.320090  loss = 3.55422 avg_loss = 3.13847\n",
      "epoch no.4 train no.320100  loss = 4.18782 avg_loss = 3.13073\n",
      "epoch no.4 train no.320110  loss = 2.53618 avg_loss = 3.13074\n",
      "epoch no.4 train no.320120  loss = 2.15017 avg_loss = 3.11366\n",
      "epoch no.4 train no.320130  loss = 3.69403 avg_loss = 3.15298\n",
      "epoch no.4 train no.320140  loss = 2.83140 avg_loss = 3.18040\n",
      "epoch no.4 train no.320150  loss = 3.05177 avg_loss = 3.17250\n",
      "epoch no.4 train no.320160  loss = 2.65334 avg_loss = 3.17874\n",
      "epoch no.4 train no.320170  loss = 3.08185 avg_loss = 3.19123\n",
      "epoch no.4 train no.320180  loss = 2.29788 avg_loss = 3.15381\n",
      "epoch no.4 train no.320190  loss = 2.90168 avg_loss = 3.16180\n",
      "epoch no.4 train no.320200  loss = 3.98534 avg_loss = 3.20290\n",
      "epoch no.4 train no.320210  loss = 3.02599 avg_loss = 3.20802\n",
      "epoch no.4 train no.320220  loss = 4.04197 avg_loss = 3.22517\n",
      "epoch no.4 train no.320230  loss = 3.28512 avg_loss = 3.23273\n",
      "epoch no.4 train no.320240  loss = 3.83833 avg_loss = 3.24160\n",
      "epoch no.4 train no.320250  loss = 3.00312 avg_loss = 3.28813\n",
      "epoch no.4 train no.320260  loss = 3.17879 avg_loss = 3.28480\n",
      "epoch no.4 train no.320270  loss = 1.55142 avg_loss = 3.25292\n",
      "epoch no.4 train no.320280  loss = 2.89122 avg_loss = 3.21722\n",
      "epoch no.4 train no.320290  loss = 2.18801 avg_loss = 3.19795\n",
      "epoch no.4 train no.320300  loss = 3.32774 avg_loss = 3.19959\n",
      "epoch no.4 train no.320310  loss = 1.64989 avg_loss = 3.16252\n",
      "epoch no.4 train no.320320  loss = 5.14725 avg_loss = 3.16477\n",
      "epoch no.4 train no.320330  loss = 3.74194 avg_loss = 3.19326\n",
      "epoch no.4 train no.320340  loss = 3.23468 avg_loss = 3.16770\n",
      "epoch no.4 train no.320350  loss = 2.70010 avg_loss = 3.13371\n",
      "epoch no.4 train no.320360  loss = 2.31259 avg_loss = 3.09331\n",
      "epoch no.4 train no.320370  loss = 1.94117 avg_loss = 3.12426\n",
      "epoch no.4 train no.320380  loss = 3.57775 avg_loss = 3.12182\n",
      "epoch no.4 train no.320390  loss = 1.82909 avg_loss = 3.18124\n",
      "epoch no.4 train no.320400  loss = 1.33569 avg_loss = 3.19406\n",
      "epoch no.4 train no.320410  loss = 2.42866 avg_loss = 3.23911\n",
      "epoch no.4 train no.320420  loss = 3.01037 avg_loss = 3.22771\n",
      "epoch no.4 train no.320430  loss = 3.95574 avg_loss = 3.19920\n",
      "epoch no.4 train no.320440  loss = 1.92622 avg_loss = 3.20736\n",
      "epoch no.4 train no.320450  loss = 1.93611 avg_loss = 3.17136\n",
      "epoch no.4 train no.320460  loss = 3.83334 avg_loss = 3.21479\n",
      "epoch no.4 train no.320470  loss = 3.82504 avg_loss = 3.20450\n",
      "epoch no.4 train no.320480  loss = 2.81400 avg_loss = 3.18163\n",
      "epoch no.4 train no.320490  loss = 4.57764 avg_loss = 3.21719\n",
      "epoch no.4 train no.320500  loss = 3.78383 avg_loss = 3.19794\n",
      "epoch no.4 train no.320510  loss = 1.89617 avg_loss = 3.11539\n",
      "epoch no.4 train no.320520  loss = 1.98213 avg_loss = 3.04786\n",
      "epoch no.4 train no.320530  loss = 3.49720 avg_loss = 3.04710\n",
      "epoch no.4 train no.320540  loss = 1.92225 avg_loss = 3.07408\n",
      "epoch no.4 train no.320550  loss = 2.48762 avg_loss = 3.05024\n",
      "epoch no.4 train no.320560  loss = 3.58184 avg_loss = 3.05064\n",
      "epoch no.4 train no.320570  loss = 2.40876 avg_loss = 3.01702\n",
      "epoch no.4 train no.320580  loss = 3.02584 avg_loss = 3.04384\n",
      "epoch no.4 train no.320590  loss = 2.17645 avg_loss = 3.00091\n",
      "epoch no.4 train no.320600  loss = 1.82973 avg_loss = 2.99052\n",
      "epoch no.4 train no.320610  loss = 1.35137 avg_loss = 2.95850\n",
      "epoch no.4 train no.320620  loss = 4.98473 avg_loss = 2.96428\n",
      "epoch no.4 train no.320630  loss = 3.45707 avg_loss = 3.00666\n",
      "epoch no.4 train no.320640  loss = 4.30250 avg_loss = 3.00676\n",
      "epoch no.4 train no.320650  loss = 2.12578 avg_loss = 3.01868\n",
      "epoch no.4 train no.320660  loss = 3.86322 avg_loss = 3.02127\n",
      "epoch no.4 train no.320670  loss = 2.68987 avg_loss = 2.98968\n",
      "epoch no.4 train no.320680  loss = 3.17893 avg_loss = 3.00182\n",
      "epoch no.4 train no.320690  loss = 1.83374 avg_loss = 3.01766\n",
      "epoch no.4 train no.320700  loss = 2.79942 avg_loss = 2.98792\n",
      "epoch no.4 train no.320710  loss = 3.07166 avg_loss = 3.00490\n",
      "epoch no.4 train no.320720  loss = 3.29200 avg_loss = 3.04912\n",
      "epoch no.4 train no.320730  loss = 3.74315 avg_loss = 3.09582\n",
      "epoch no.4 train no.320740  loss = 3.59186 avg_loss = 3.10458\n",
      "epoch no.4 train no.320750  loss = 1.95570 avg_loss = 3.10831\n",
      "epoch no.4 train no.320760  loss = 3.01445 avg_loss = 3.12195\n",
      "epoch no.4 train no.320770  loss = 2.97389 avg_loss = 3.17253\n",
      "epoch no.4 train no.320780  loss = 4.15004 avg_loss = 3.21476\n",
      "epoch no.4 train no.320790  loss = 2.15602 avg_loss = 3.21024\n",
      "epoch no.4 train no.320800  loss = 2.50987 avg_loss = 3.17241\n",
      "epoch no.4 train no.320810  loss = 2.11739 avg_loss = 3.11889\n",
      "epoch no.4 train no.320820  loss = 2.14288 avg_loss = 3.10615\n",
      "epoch no.4 train no.320830  loss = 3.36815 avg_loss = 3.09980\n",
      "epoch no.4 train no.320840  loss = 2.86455 avg_loss = 3.13925\n",
      "epoch no.4 train no.320850  loss = 3.45696 avg_loss = 3.10115\n",
      "epoch no.4 train no.320860  loss = 3.36497 avg_loss = 3.12829\n",
      "epoch no.4 train no.320870  loss = 3.83164 avg_loss = 3.15791\n",
      "epoch no.4 train no.320880  loss = 3.41723 avg_loss = 3.18473\n",
      "epoch no.4 train no.320890  loss = 2.86746 avg_loss = 3.16338\n",
      "epoch no.4 train no.320900  loss = 2.95018 avg_loss = 3.13998\n",
      "epoch no.4 train no.320910  loss = 3.05592 avg_loss = 3.16680\n",
      "epoch no.4 train no.320920  loss = 3.72925 avg_loss = 3.16363\n",
      "epoch no.4 train no.320930  loss = 3.72305 avg_loss = 3.14127\n",
      "epoch no.4 train no.320940  loss = 2.43475 avg_loss = 3.11327\n",
      "epoch no.4 train no.320950  loss = 3.52815 avg_loss = 3.11088\n",
      "epoch no.4 train no.320960  loss = 4.50083 avg_loss = 3.15414\n",
      "epoch no.4 train no.320970  loss = 3.05663 avg_loss = 3.11945\n",
      "epoch no.4 train no.320980  loss = 3.71698 avg_loss = 3.10789\n",
      "epoch no.4 train no.320990  loss = 2.39208 avg_loss = 3.15244\n",
      "epoch no.4 train no.321000  loss = 3.54903 avg_loss = 3.15760\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁채우는', '주는', '▁감성', '음악', '</s>']\n",
      "여름밤을 달래는 인디음악</s>\n",
      "epoch no.4 train no.321010  loss = 3.61527 avg_loss = 3.18514\n",
      "epoch no.4 train no.321020  loss = 3.06679 avg_loss = 3.16144\n",
      "epoch no.4 train no.321030  loss = 4.63645 avg_loss = 3.17372\n",
      "epoch no.4 train no.321040  loss = 3.45126 avg_loss = 3.20733\n",
      "epoch no.4 train no.321050  loss = 3.69128 avg_loss = 3.22311\n",
      "epoch no.4 train no.321060  loss = 2.73300 avg_loss = 3.18534\n",
      "epoch no.4 train no.321070  loss = 3.30009 avg_loss = 3.21644\n",
      "epoch no.4 train no.321080  loss = 4.10233 avg_loss = 3.21374\n",
      "epoch no.4 train no.321090  loss = 3.83877 avg_loss = 3.24090\n",
      "epoch no.4 train no.321100  loss = 2.67110 avg_loss = 3.25566\n",
      "epoch no.4 train no.321110  loss = 3.92272 avg_loss = 3.28089\n",
      "epoch no.4 train no.321120  loss = 2.80027 avg_loss = 3.22678\n",
      "epoch no.4 train no.321130  loss = 5.05849 avg_loss = 3.21209\n",
      "epoch no.4 train no.321140  loss = 2.53978 avg_loss = 3.20385\n",
      "epoch no.4 train no.321150  loss = 1.81042 avg_loss = 3.22944\n",
      "epoch no.4 train no.321160  loss = 1.83289 avg_loss = 3.19461\n",
      "epoch no.4 train no.321170  loss = 2.28126 avg_loss = 3.15834\n",
      "epoch no.4 train no.321180  loss = 3.60567 avg_loss = 3.13532\n",
      "epoch no.4 train no.321190  loss = 2.89584 avg_loss = 3.14334\n",
      "epoch no.4 train no.321200  loss = 2.96476 avg_loss = 3.16791\n",
      "epoch no.4 train no.321210  loss = 2.43773 avg_loss = 3.15977\n",
      "epoch no.4 train no.321220  loss = 2.23398 avg_loss = 3.13233\n",
      "epoch no.4 train no.321230  loss = 3.56743 avg_loss = 3.12328\n",
      "epoch no.4 train no.321240  loss = 2.82066 avg_loss = 3.10602\n",
      "epoch no.4 train no.321250  loss = 5.33220 avg_loss = 3.16280\n",
      "epoch no.4 train no.321260  loss = 2.79199 avg_loss = 3.15642\n",
      "epoch no.4 train no.321270  loss = 2.36451 avg_loss = 3.10723\n",
      "epoch no.4 train no.321280  loss = 3.77660 avg_loss = 3.15873\n",
      "epoch no.4 train no.321290  loss = 4.15563 avg_loss = 3.15839\n",
      "epoch no.4 train no.321300  loss = 1.94165 avg_loss = 3.12576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.321310  loss = 2.06822 avg_loss = 3.11019\n",
      "epoch no.4 train no.321320  loss = 4.35290 avg_loss = 3.13132\n",
      "epoch no.4 train no.321330  loss = 2.28489 avg_loss = 3.09771\n",
      "epoch no.4 train no.321340  loss = 3.22511 avg_loss = 3.13634\n",
      "epoch no.4 train no.321350  loss = 3.14707 avg_loss = 3.12113\n",
      "epoch no.4 train no.321360  loss = 3.97311 avg_loss = 3.13716\n",
      "epoch no.4 train no.321370  loss = 3.35632 avg_loss = 3.14831\n",
      "epoch no.4 train no.321380  loss = 2.35643 avg_loss = 3.15968\n",
      "epoch no.4 train no.321390  loss = 3.35052 avg_loss = 3.16485\n",
      "epoch no.4 train no.321400  loss = 2.40530 avg_loss = 3.13736\n",
      "epoch no.4 train no.321410  loss = 4.18644 avg_loss = 3.14705\n",
      "epoch no.4 train no.321420  loss = 3.80998 avg_loss = 3.14418\n",
      "epoch no.4 train no.321430  loss = 2.88998 avg_loss = 3.14019\n",
      "epoch no.4 train no.321440  loss = 2.10690 avg_loss = 3.13501\n",
      "epoch no.4 train no.321450  loss = 3.29006 avg_loss = 3.13030\n",
      "epoch no.4 train no.321460  loss = 3.52433 avg_loss = 3.10954\n",
      "epoch no.4 train no.321470  loss = 3.18769 avg_loss = 3.11597\n",
      "epoch no.4 train no.321480  loss = 2.94688 avg_loss = 3.11929\n",
      "epoch no.4 train no.321490  loss = 2.11908 avg_loss = 3.07315\n",
      "epoch no.4 train no.321500  loss = 1.74667 avg_loss = 3.08348\n",
      "epoch no.4 train no.321510  loss = 4.54152 avg_loss = 3.09630\n",
      "epoch no.4 train no.321520  loss = 2.43862 avg_loss = 3.07058\n",
      "epoch no.4 train no.321530  loss = 5.19731 avg_loss = 3.07964\n",
      "epoch no.4 train no.321540  loss = 3.16001 avg_loss = 3.06013\n",
      "epoch no.4 train no.321550  loss = 3.54069 avg_loss = 3.08453\n",
      "epoch no.4 train no.321560  loss = 3.32013 avg_loss = 3.09578\n",
      "epoch no.4 train no.321570  loss = 2.89320 avg_loss = 3.11438\n",
      "epoch no.4 train no.321580  loss = 2.67579 avg_loss = 3.12510\n",
      "epoch no.4 train no.321590  loss = 2.68012 avg_loss = 3.13250\n",
      "epoch no.4 train no.321600  loss = 3.49012 avg_loss = 3.15071\n",
      "epoch no.4 train no.321610  loss = 2.25357 avg_loss = 3.17308\n",
      "epoch no.4 train no.321620  loss = 3.14806 avg_loss = 3.17498\n",
      "epoch no.4 train no.321630  loss = 3.49265 avg_loss = 3.18716\n",
      "epoch no.4 train no.321640  loss = 3.06307 avg_loss = 3.14376\n",
      "epoch no.4 train no.321650  loss = 2.94602 avg_loss = 3.17837\n",
      "epoch no.4 train no.321660  loss = 3.57286 avg_loss = 3.15107\n",
      "epoch no.4 train no.321670  loss = 2.97462 avg_loss = 3.20646\n",
      "epoch no.4 train no.321680  loss = 2.84567 avg_loss = 3.17975\n",
      "epoch no.4 train no.321690  loss = 2.21393 avg_loss = 3.13104\n",
      "epoch no.4 train no.321700  loss = 4.59731 avg_loss = 3.13452\n",
      "epoch no.4 train no.321710  loss = 2.64942 avg_loss = 3.16315\n",
      "epoch no.4 train no.321720  loss = 2.57123 avg_loss = 3.11836\n",
      "epoch no.4 train no.321730  loss = 4.74571 avg_loss = 3.17563\n",
      "epoch no.4 train no.321740  loss = 2.87709 avg_loss = 3.18440\n",
      "epoch no.4 train no.321750  loss = 2.37806 avg_loss = 3.16610\n",
      "epoch no.4 train no.321760  loss = 2.50752 avg_loss = 3.16106\n",
      "epoch no.4 train no.321770  loss = 4.21793 avg_loss = 3.14852\n",
      "epoch no.4 train no.321780  loss = 3.13548 avg_loss = 3.13239\n",
      "epoch no.4 train no.321790  loss = 3.36116 avg_loss = 3.12682\n",
      "epoch no.4 train no.321800  loss = 2.47243 avg_loss = 3.13834\n",
      "epoch no.4 train no.321810  loss = 4.46429 avg_loss = 3.15492\n",
      "epoch no.4 train no.321820  loss = 3.02310 avg_loss = 3.17458\n",
      "epoch no.4 train no.321830  loss = 2.64185 avg_loss = 3.15919\n",
      "epoch no.4 train no.321840  loss = 3.47269 avg_loss = 3.16488\n",
      "epoch no.4 train no.321850  loss = 3.98879 avg_loss = 3.16961\n",
      "epoch no.4 train no.321860  loss = 2.33289 avg_loss = 3.12626\n",
      "epoch no.4 train no.321870  loss = 2.70706 avg_loss = 3.16153\n",
      "epoch no.4 train no.321880  loss = 2.06374 avg_loss = 3.15964\n",
      "epoch no.4 train no.321890  loss = 4.03394 avg_loss = 3.14713\n",
      "epoch no.4 train no.321900  loss = 2.14302 avg_loss = 3.13055\n",
      "epoch no.4 train no.321910  loss = 5.06071 avg_loss = 3.15981\n",
      "epoch no.4 train no.321920  loss = 1.82183 avg_loss = 3.13671\n",
      "epoch no.4 train no.321930  loss = 2.57531 avg_loss = 3.11853\n",
      "epoch no.4 train no.321940  loss = 3.91116 avg_loss = 3.17138\n",
      "epoch no.4 train no.321950  loss = 1.66626 avg_loss = 3.13054\n",
      "epoch no.4 train no.321960  loss = 3.23545 avg_loss = 3.13324\n",
      "epoch no.4 train no.321970  loss = 4.58279 avg_loss = 3.11827\n",
      "epoch no.4 train no.321980  loss = 1.90647 avg_loss = 3.10165\n",
      "epoch no.4 train no.321990  loss = 2.41554 avg_loss = 3.11802\n",
      "epoch no.4 train no.322000  loss = 4.23211 avg_loss = 3.13507\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁재즈', '▁뉴', '</s>']\n",
      "여름밤에 듣는 감성 팝</s>\n",
      "epoch no.4 train no.322010  loss = 2.98176 avg_loss = 3.13162\n",
      "epoch no.4 train no.322020  loss = 1.87263 avg_loss = 3.14864\n",
      "epoch no.4 train no.322030  loss = 2.50175 avg_loss = 3.15429\n",
      "epoch no.4 train no.322040  loss = 3.61867 avg_loss = 3.17833\n",
      "epoch no.4 train no.322050  loss = 3.02607 avg_loss = 3.15455\n",
      "epoch no.4 train no.322060  loss = 2.70702 avg_loss = 3.12841\n",
      "epoch no.4 train no.322070  loss = 4.36157 avg_loss = 3.17512\n",
      "epoch no.4 train no.322080  loss = 2.12498 avg_loss = 3.11051\n",
      "epoch no.4 train no.322090  loss = 3.98878 avg_loss = 3.15451\n",
      "epoch no.4 train no.322100  loss = 3.09706 avg_loss = 3.16158\n",
      "epoch no.4 train no.322110  loss = 4.19037 avg_loss = 3.15552\n",
      "epoch no.4 train no.322120  loss = 1.42412 avg_loss = 3.11137\n",
      "epoch no.4 train no.322130  loss = 3.09035 avg_loss = 3.10326\n",
      "epoch no.4 train no.322140  loss = 3.00729 avg_loss = 3.10608\n",
      "epoch no.4 train no.322150  loss = 4.15774 avg_loss = 3.12873\n",
      "epoch no.4 train no.322160  loss = 3.47239 avg_loss = 3.14841\n",
      "epoch no.4 train no.322170  loss = 4.18541 avg_loss = 3.18589\n",
      "epoch no.4 train no.322180  loss = 4.43481 avg_loss = 3.17686\n",
      "epoch no.4 train no.322190  loss = 2.56576 avg_loss = 3.17416\n",
      "epoch no.4 train no.322200  loss = 2.68501 avg_loss = 3.18635\n",
      "epoch no.4 train no.322210  loss = 2.94274 avg_loss = 3.19791\n",
      "epoch no.4 train no.322220  loss = 3.46988 avg_loss = 3.17465\n",
      "epoch no.4 train no.322230  loss = 3.31889 avg_loss = 3.19419\n",
      "epoch no.4 train no.322240  loss = 3.22467 avg_loss = 3.17276\n",
      "epoch no.4 train no.322250  loss = 5.13635 avg_loss = 3.21141\n",
      "epoch no.4 train no.322260  loss = 2.67842 avg_loss = 3.18336\n",
      "epoch no.4 train no.322270  loss = 2.69415 avg_loss = 3.19575\n",
      "epoch no.4 train no.322280  loss = 3.76394 avg_loss = 3.19979\n",
      "epoch no.4 train no.322290  loss = 1.91510 avg_loss = 3.18036\n",
      "epoch no.4 train no.322300  loss = 2.50684 avg_loss = 3.17970\n",
      "epoch no.4 train no.322310  loss = 3.20502 avg_loss = 3.14042\n",
      "epoch no.4 train no.322320  loss = 3.42817 avg_loss = 3.14025\n",
      "epoch no.4 train no.322330  loss = 3.74067 avg_loss = 3.15092\n",
      "epoch no.4 train no.322340  loss = 2.89231 avg_loss = 3.12148\n",
      "epoch no.4 train no.322350  loss = 4.38656 avg_loss = 3.16096\n",
      "epoch no.4 train no.322360  loss = 2.39763 avg_loss = 3.16235\n",
      "epoch no.4 train no.322370  loss = 3.25203 avg_loss = 3.17411\n",
      "epoch no.4 train no.322380  loss = 2.02018 avg_loss = 3.13824\n",
      "epoch no.4 train no.322390  loss = 3.11107 avg_loss = 3.17781\n",
      "epoch no.4 train no.322400  loss = 2.49902 avg_loss = 3.14616\n",
      "epoch no.4 train no.322410  loss = 3.88757 avg_loss = 3.16453\n",
      "epoch no.4 train no.322420  loss = 3.62933 avg_loss = 3.17865\n",
      "epoch no.4 train no.322430  loss = 3.19691 avg_loss = 3.18441\n",
      "epoch no.4 train no.322440  loss = 2.44584 avg_loss = 3.19063\n",
      "epoch no.4 train no.322450  loss = 4.05877 avg_loss = 3.15799\n",
      "epoch no.4 train no.322460  loss = 2.68632 avg_loss = 3.14624\n",
      "epoch no.4 train no.322470  loss = 2.99124 avg_loss = 3.14592\n",
      "epoch no.4 train no.322480  loss = 2.19617 avg_loss = 3.13600\n",
      "epoch no.4 train no.322490  loss = 3.10167 avg_loss = 3.12969\n",
      "epoch no.4 train no.322500  loss = 3.77277 avg_loss = 3.11903\n",
      "epoch no.4 train no.322510  loss = 4.60976 avg_loss = 3.16025\n",
      "epoch no.4 train no.322520  loss = 3.05976 avg_loss = 3.15319\n",
      "epoch no.4 train no.322530  loss = 3.63101 avg_loss = 3.15087\n",
      "epoch no.4 train no.322540  loss = 2.26217 avg_loss = 3.14248\n",
      "epoch no.4 train no.322550  loss = 4.95763 avg_loss = 3.15685\n",
      "epoch no.4 train no.322560  loss = 3.19797 avg_loss = 3.17517\n",
      "epoch no.4 train no.322570  loss = 3.09978 avg_loss = 3.18818\n",
      "epoch no.4 train no.322580  loss = 2.79785 avg_loss = 3.18878\n",
      "epoch no.4 train no.322590  loss = 1.50110 avg_loss = 3.14139\n",
      "epoch no.4 train no.322600  loss = 2.37190 avg_loss = 3.14974\n",
      "epoch no.4 train no.322610  loss = 2.66003 avg_loss = 3.13637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.322620  loss = 2.87568 avg_loss = 3.12268\n",
      "epoch no.4 train no.322630  loss = 3.17831 avg_loss = 3.13916\n",
      "epoch no.4 train no.322640  loss = 4.09212 avg_loss = 3.13843\n",
      "epoch no.4 train no.322650  loss = 2.37697 avg_loss = 3.12729\n",
      "epoch no.4 train no.322660  loss = 4.38702 avg_loss = 3.14779\n",
      "epoch no.4 train no.322670  loss = 2.29558 avg_loss = 3.19213\n",
      "epoch no.4 train no.322680  loss = 3.06271 avg_loss = 3.13523\n",
      "epoch no.4 train no.322690  loss = 5.37697 avg_loss = 3.15123\n",
      "epoch no.4 train no.322700  loss = 2.17503 avg_loss = 3.14890\n",
      "epoch no.4 train no.322710  loss = 2.92509 avg_loss = 3.13256\n",
      "epoch no.4 train no.322720  loss = 2.75939 avg_loss = 3.16642\n",
      "epoch no.4 train no.322730  loss = 2.17237 avg_loss = 3.13424\n",
      "epoch no.4 train no.322740  loss = 4.82635 avg_loss = 3.16774\n",
      "epoch no.4 train no.322750  loss = 3.79534 avg_loss = 3.17807\n",
      "epoch no.4 train no.322760  loss = 2.01233 avg_loss = 3.12119\n",
      "epoch no.4 train no.322770  loss = 3.20145 avg_loss = 3.11844\n",
      "epoch no.4 train no.322780  loss = 3.29943 avg_loss = 3.10865\n",
      "epoch no.4 train no.322790  loss = 3.21757 avg_loss = 3.11787\n",
      "epoch no.4 train no.322800  loss = 3.69024 avg_loss = 3.12824\n",
      "epoch no.4 train no.322810  loss = 3.02524 avg_loss = 3.12098\n",
      "epoch no.4 train no.322820  loss = 1.92226 avg_loss = 3.19436\n",
      "epoch no.4 train no.322830  loss = 2.71014 avg_loss = 3.15619\n",
      "epoch no.4 train no.322840  loss = 5.20873 avg_loss = 3.12481\n",
      "epoch no.4 train no.322850  loss = 2.26426 avg_loss = 3.12257\n",
      "epoch no.4 train no.322860  loss = 3.29875 avg_loss = 3.12037\n",
      "epoch no.4 train no.322870  loss = 3.60927 avg_loss = 3.10631\n",
      "epoch no.4 train no.322880  loss = 4.21322 avg_loss = 3.14237\n",
      "epoch no.4 train no.322890  loss = 2.61837 avg_loss = 3.14027\n",
      "epoch no.4 train no.322900  loss = 3.50108 avg_loss = 3.15034\n",
      "epoch no.4 train no.322910  loss = 3.24912 avg_loss = 3.18648\n",
      "epoch no.4 train no.322920  loss = 3.61919 avg_loss = 3.15632\n",
      "epoch no.4 train no.322930  loss = 4.68018 avg_loss = 3.18885\n",
      "epoch no.4 train no.322940  loss = 3.21063 avg_loss = 3.14462\n",
      "epoch no.4 train no.322950  loss = 4.64572 avg_loss = 3.15370\n",
      "epoch no.4 train no.322960  loss = 4.96717 avg_loss = 3.16356\n",
      "epoch no.4 train no.322970  loss = 2.50247 avg_loss = 3.13147\n",
      "epoch no.4 train no.322980  loss = 3.11223 avg_loss = 3.11413\n",
      "epoch no.4 train no.322990  loss = 3.54890 avg_loss = 3.18700\n",
      "epoch no.4 train no.323000  loss = 3.26312 avg_loss = 3.21341\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '▁뉴', '</s>']\n",
      "여름밤에 듣는 감성 피아노</s>\n",
      "epoch no.4 train no.323010  loss = 1.59033 avg_loss = 3.17427\n",
      "epoch no.4 train no.323020  loss = 1.52272 avg_loss = 3.17917\n",
      "epoch no.4 train no.323030  loss = 3.44760 avg_loss = 3.22268\n",
      "epoch no.4 train no.323040  loss = 2.46793 avg_loss = 3.19300\n",
      "epoch no.4 train no.323050  loss = 2.33117 avg_loss = 3.21585\n",
      "epoch no.4 train no.323060  loss = 2.56897 avg_loss = 3.21683\n",
      "epoch no.4 train no.323070  loss = 4.35867 avg_loss = 3.21572\n",
      "epoch no.4 train no.323080  loss = 3.55468 avg_loss = 3.18372\n",
      "epoch no.4 train no.323090  loss = 3.06090 avg_loss = 3.20992\n",
      "epoch no.4 train no.323100  loss = 3.35036 avg_loss = 3.21479\n",
      "epoch no.4 train no.323110  loss = 3.45708 avg_loss = 3.21540\n",
      "epoch no.4 train no.323120  loss = 3.28173 avg_loss = 3.21197\n",
      "epoch no.4 train no.323130  loss = 3.73381 avg_loss = 3.20316\n",
      "epoch no.4 train no.323140  loss = 2.71006 avg_loss = 3.19776\n",
      "epoch no.4 train no.323150  loss = 3.73156 avg_loss = 3.22272\n",
      "epoch no.4 train no.323160  loss = 3.17978 avg_loss = 3.20335\n",
      "epoch no.4 train no.323170  loss = 2.96606 avg_loss = 3.19866\n",
      "epoch no.4 train no.323180  loss = 2.90413 avg_loss = 3.21185\n",
      "epoch no.4 train no.323190  loss = 1.95822 avg_loss = 3.18578\n",
      "epoch no.4 train no.323200  loss = 2.60939 avg_loss = 3.23158\n",
      "epoch no.4 train no.323210  loss = 3.00969 avg_loss = 3.20978\n",
      "epoch no.4 train no.323220  loss = 2.34955 avg_loss = 3.18972\n",
      "epoch no.4 train no.323230  loss = 3.90635 avg_loss = 3.18012\n",
      "epoch no.4 train no.323240  loss = 2.74138 avg_loss = 3.17268\n",
      "epoch no.4 train no.323250  loss = 3.19792 avg_loss = 3.17286\n",
      "epoch no.4 train no.323260  loss = 1.94967 avg_loss = 3.16074\n",
      "epoch no.4 train no.323270  loss = 2.69640 avg_loss = 3.14420\n",
      "epoch no.4 train no.323280  loss = 5.13927 avg_loss = 3.17740\n",
      "epoch no.4 train no.323290  loss = 2.73191 avg_loss = 3.15224\n",
      "epoch no.4 train no.323300  loss = 2.57207 avg_loss = 3.13644\n",
      "epoch no.4 train no.323310  loss = 3.32852 avg_loss = 3.13636\n",
      "epoch no.4 train no.323320  loss = 3.73277 avg_loss = 3.13232\n",
      "epoch no.4 train no.323330  loss = 2.88546 avg_loss = 3.11854\n",
      "epoch no.4 train no.323340  loss = 3.29080 avg_loss = 3.11031\n",
      "epoch no.4 train no.323350  loss = 4.98807 avg_loss = 3.13841\n",
      "epoch no.4 train no.323360  loss = 3.24465 avg_loss = 3.15195\n",
      "epoch no.4 train no.323370  loss = 3.54240 avg_loss = 3.20383\n",
      "epoch no.4 train no.323380  loss = 3.28947 avg_loss = 3.18309\n",
      "epoch no.4 train no.323390  loss = 2.01855 avg_loss = 3.17201\n",
      "epoch no.4 train no.323400  loss = 3.58383 avg_loss = 3.19732\n",
      "epoch no.4 train no.323410  loss = 3.09748 avg_loss = 3.22346\n",
      "epoch no.4 train no.323420  loss = 3.02506 avg_loss = 3.25173\n",
      "epoch no.4 train no.323430  loss = 3.17176 avg_loss = 3.24482\n",
      "epoch no.4 train no.323440  loss = 2.53518 avg_loss = 3.22611\n",
      "epoch no.4 train no.323450  loss = 3.26842 avg_loss = 3.21760\n",
      "epoch no.4 train no.323460  loss = 2.86743 avg_loss = 3.21247\n",
      "epoch no.4 train no.323470  loss = 2.54446 avg_loss = 3.17561\n",
      "epoch no.4 train no.323480  loss = 2.32080 avg_loss = 3.12889\n",
      "epoch no.4 train no.323490  loss = 2.99245 avg_loss = 3.16299\n",
      "epoch no.4 train no.323500  loss = 3.34635 avg_loss = 3.19746\n",
      "epoch no.4 train no.323510  loss = 2.73627 avg_loss = 3.18021\n",
      "epoch no.4 train no.323520  loss = 6.39096 avg_loss = 3.19691\n",
      "epoch no.4 train no.323530  loss = 3.64611 avg_loss = 3.20504\n",
      "epoch no.4 train no.323540  loss = 4.35601 avg_loss = 3.22705\n",
      "epoch no.4 train no.323550  loss = 3.36245 avg_loss = 3.19817\n",
      "epoch no.4 train no.323560  loss = 2.35424 avg_loss = 3.18976\n",
      "epoch no.4 train no.323570  loss = 1.99790 avg_loss = 3.17345\n",
      "epoch no.4 train no.323580  loss = 3.01287 avg_loss = 3.15034\n",
      "epoch no.4 train no.323590  loss = 2.78377 avg_loss = 3.11429\n",
      "epoch no.4 train no.323600  loss = 3.88238 avg_loss = 3.10777\n",
      "epoch no.4 train no.323610  loss = 3.63851 avg_loss = 3.12208\n",
      "epoch no.4 train no.323620  loss = 2.24629 avg_loss = 3.08956\n",
      "epoch no.4 train no.323630  loss = 3.90939 avg_loss = 3.10844\n",
      "epoch no.4 train no.323640  loss = 2.53265 avg_loss = 3.11167\n",
      "epoch no.4 train no.323650  loss = 2.74970 avg_loss = 3.09446\n",
      "epoch no.4 train no.323660  loss = 2.66714 avg_loss = 3.09115\n",
      "epoch no.4 train no.323670  loss = 2.91277 avg_loss = 3.12224\n",
      "epoch no.4 train no.323680  loss = 3.46447 avg_loss = 3.15690\n",
      "epoch no.4 train no.323690  loss = 3.58208 avg_loss = 3.18049\n",
      "epoch no.4 train no.323700  loss = 3.34894 avg_loss = 3.12951\n",
      "epoch no.4 train no.323710  loss = 3.78704 avg_loss = 3.10827\n",
      "epoch no.4 train no.323720  loss = 2.55478 avg_loss = 3.10444\n",
      "epoch no.4 train no.323730  loss = 2.29967 avg_loss = 3.15166\n",
      "epoch no.4 train no.323740  loss = 2.39333 avg_loss = 3.15406\n",
      "epoch no.4 train no.323750  loss = 1.76946 avg_loss = 3.17200\n",
      "epoch no.4 train no.323760  loss = 3.00678 avg_loss = 3.21445\n",
      "epoch no.4 train no.323770  loss = 2.34604 avg_loss = 3.21167\n",
      "epoch no.4 train no.323780  loss = 2.51439 avg_loss = 3.18944\n",
      "epoch no.4 train no.323790  loss = 2.61191 avg_loss = 3.13421\n",
      "epoch no.4 train no.323800  loss = 2.27337 avg_loss = 3.13444\n",
      "epoch no.4 train no.323810  loss = 2.90510 avg_loss = 3.18672\n",
      "epoch no.4 train no.323820  loss = 2.56477 avg_loss = 3.19185\n",
      "epoch no.4 train no.323830  loss = 3.45181 avg_loss = 3.23433\n",
      "epoch no.4 train no.323840  loss = 2.71956 avg_loss = 3.25658\n",
      "epoch no.4 train no.323850  loss = 4.75720 avg_loss = 3.23143\n",
      "epoch no.4 train no.323860  loss = 1.80904 avg_loss = 3.22324\n",
      "epoch no.4 train no.323870  loss = 3.08835 avg_loss = 3.19484\n",
      "epoch no.4 train no.323880  loss = 4.71064 avg_loss = 3.19907\n",
      "epoch no.4 train no.323890  loss = 2.48423 avg_loss = 3.18965\n",
      "epoch no.4 train no.323900  loss = 4.29853 avg_loss = 3.21340\n",
      "epoch no.4 train no.323910  loss = 1.78123 avg_loss = 3.17112\n",
      "epoch no.4 train no.323920  loss = 3.21595 avg_loss = 3.20014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.323930  loss = 3.02219 avg_loss = 3.21250\n",
      "epoch no.4 train no.323940  loss = 3.41616 avg_loss = 3.20319\n",
      "epoch no.4 train no.323950  loss = 4.11722 avg_loss = 3.20735\n",
      "epoch no.4 train no.323960  loss = 2.78198 avg_loss = 3.20363\n",
      "epoch no.4 train no.323970  loss = 3.64202 avg_loss = 3.18951\n",
      "epoch no.4 train no.323980  loss = 1.65854 avg_loss = 3.17224\n",
      "epoch no.4 train no.323990  loss = 3.76950 avg_loss = 3.13597\n",
      "epoch no.4 train no.324000  loss = 4.67717 avg_loss = 3.16674\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '같은', '색', '</s>']\n",
      "여름밤의 꿀음색</s>\n",
      "epoch no.4 train no.324010  loss = 3.52510 avg_loss = 3.18235\n",
      "epoch no.4 train no.324020  loss = 3.00398 avg_loss = 3.16607\n",
      "epoch no.4 train no.324030  loss = 4.69584 avg_loss = 3.17914\n",
      "epoch no.4 train no.324040  loss = 1.19777 avg_loss = 3.16043\n",
      "epoch no.4 train no.324050  loss = 2.04615 avg_loss = 3.18371\n",
      "epoch no.4 train no.324060  loss = 4.19231 avg_loss = 3.16802\n",
      "epoch no.4 train no.324070  loss = 2.88023 avg_loss = 3.19084\n",
      "epoch no.4 train no.324080  loss = 3.71025 avg_loss = 3.21770\n",
      "epoch no.4 train no.324090  loss = 2.34442 avg_loss = 3.20271\n",
      "epoch no.4 train no.324100  loss = 3.15127 avg_loss = 3.19329\n",
      "epoch no.4 train no.324110  loss = 1.80394 avg_loss = 3.19607\n",
      "epoch no.4 train no.324120  loss = 4.41308 avg_loss = 3.18065\n",
      "epoch no.4 train no.324130  loss = 2.36966 avg_loss = 3.18463\n",
      "epoch no.4 train no.324140  loss = 3.61056 avg_loss = 3.17083\n",
      "epoch no.4 train no.324150  loss = 3.04180 avg_loss = 3.16797\n",
      "epoch no.4 train no.324160  loss = 2.97642 avg_loss = 3.18189\n",
      "epoch no.4 train no.324170  loss = 3.25034 avg_loss = 3.16395\n",
      "epoch no.4 train no.324180  loss = 3.38504 avg_loss = 3.11415\n",
      "epoch no.4 train no.324190  loss = 4.04798 avg_loss = 3.11682\n",
      "epoch no.4 train no.324200  loss = 2.95738 avg_loss = 3.11387\n",
      "epoch no.4 train no.324210  loss = 3.65737 avg_loss = 3.14319\n",
      "epoch no.4 train no.324220  loss = 2.38760 avg_loss = 3.12320\n",
      "epoch no.4 train no.324230  loss = 2.48477 avg_loss = 3.13962\n",
      "epoch no.4 train no.324240  loss = 2.44827 avg_loss = 3.14394\n",
      "epoch no.4 train no.324250  loss = 2.31484 avg_loss = 3.15763\n",
      "epoch no.4 train no.324260  loss = 3.64988 avg_loss = 3.19496\n",
      "epoch no.4 train no.324270  loss = 2.27295 avg_loss = 3.20287\n",
      "epoch no.4 train no.324280  loss = 3.35436 avg_loss = 3.17749\n",
      "epoch no.4 train no.324290  loss = 2.02875 avg_loss = 3.16592\n",
      "epoch no.4 train no.324300  loss = 2.90001 avg_loss = 3.10911\n",
      "epoch no.4 train no.324310  loss = 3.92138 avg_loss = 3.18124\n",
      "epoch no.4 train no.324320  loss = 3.40971 avg_loss = 3.17140\n",
      "epoch no.4 train no.324330  loss = 3.88548 avg_loss = 3.13337\n",
      "epoch no.4 train no.324340  loss = 2.40767 avg_loss = 3.13761\n",
      "epoch no.4 train no.324350  loss = 3.72548 avg_loss = 3.14388\n",
      "epoch no.4 train no.324360  loss = 2.69318 avg_loss = 3.12921\n",
      "epoch no.4 train no.324370  loss = 4.00953 avg_loss = 3.13949\n",
      "epoch no.4 train no.324380  loss = 3.05150 avg_loss = 3.14714\n",
      "epoch no.4 train no.324390  loss = 3.48462 avg_loss = 3.14454\n",
      "epoch no.4 train no.324400  loss = 2.14425 avg_loss = 3.10701\n",
      "epoch no.4 train no.324410  loss = 2.89534 avg_loss = 3.11800\n",
      "epoch no.4 train no.324420  loss = 3.05600 avg_loss = 3.11443\n",
      "epoch no.4 train no.324430  loss = 3.35598 avg_loss = 3.09780\n",
      "epoch no.4 train no.324440  loss = 3.29773 avg_loss = 3.09413\n",
      "epoch no.4 train no.324450  loss = 4.14572 avg_loss = 3.12382\n",
      "epoch no.4 train no.324460  loss = 2.53925 avg_loss = 3.11747\n",
      "epoch no.4 train no.324470  loss = 2.01001 avg_loss = 3.13001\n",
      "epoch no.4 train no.324480  loss = 3.44050 avg_loss = 3.12566\n",
      "epoch no.4 train no.324490  loss = 2.69181 avg_loss = 3.12625\n",
      "epoch no.4 train no.324500  loss = 3.37487 avg_loss = 3.19041\n",
      "epoch no.4 train no.324510  loss = 3.59903 avg_loss = 3.21134\n",
      "epoch no.4 train no.324520  loss = 3.84725 avg_loss = 3.19202\n",
      "epoch no.4 train no.324530  loss = 2.99678 avg_loss = 3.14209\n",
      "epoch no.4 train no.324540  loss = 2.05586 avg_loss = 3.08537\n",
      "epoch no.4 train no.324550  loss = 3.45086 avg_loss = 3.08144\n",
      "epoch no.4 train no.324560  loss = 2.81204 avg_loss = 3.09955\n",
      "epoch no.4 train no.324570  loss = 1.93628 avg_loss = 3.03668\n",
      "epoch no.4 train no.324580  loss = 3.96109 avg_loss = 3.05434\n",
      "epoch no.4 train no.324590  loss = 1.70486 avg_loss = 3.02520\n",
      "epoch no.4 train no.324600  loss = 3.72246 avg_loss = 3.01959\n",
      "epoch no.4 train no.324610  loss = 3.54765 avg_loss = 3.05951\n",
      "epoch no.4 train no.324620  loss = 1.82707 avg_loss = 3.03087\n",
      "epoch no.4 train no.324630  loss = 3.76137 avg_loss = 3.09169\n",
      "epoch no.4 train no.324640  loss = 2.52382 avg_loss = 3.04199\n",
      "epoch no.4 train no.324650  loss = 4.43262 avg_loss = 3.09086\n",
      "epoch no.4 train no.324660  loss = 2.90160 avg_loss = 3.06423\n",
      "epoch no.4 train no.324670  loss = 4.30111 avg_loss = 3.11958\n",
      "epoch no.4 train no.324680  loss = 2.52250 avg_loss = 3.11107\n",
      "epoch no.4 train no.324690  loss = 3.21764 avg_loss = 3.12464\n",
      "epoch no.4 train no.324700  loss = 2.01459 avg_loss = 3.13486\n",
      "epoch no.4 train no.324710  loss = 3.08979 avg_loss = 3.09215\n",
      "epoch no.4 train no.324720  loss = 2.06361 avg_loss = 3.13380\n",
      "epoch no.4 train no.324730  loss = 2.84786 avg_loss = 3.11384\n",
      "epoch no.4 train no.324740  loss = 4.02169 avg_loss = 3.18317\n",
      "epoch no.4 train no.324750  loss = 3.14011 avg_loss = 3.16307\n",
      "epoch no.4 train no.324760  loss = 3.15719 avg_loss = 3.14948\n",
      "epoch no.4 train no.324770  loss = 4.21549 avg_loss = 3.10965\n",
      "epoch no.4 train no.324780  loss = 3.09467 avg_loss = 3.08434\n",
      "epoch no.4 train no.324790  loss = 3.53849 avg_loss = 3.08834\n",
      "epoch no.4 train no.324800  loss = 5.80533 avg_loss = 3.10849\n",
      "epoch no.4 train no.324810  loss = 2.95651 avg_loss = 3.12377\n",
      "epoch no.4 train no.324820  loss = 4.29873 avg_loss = 3.12030\n",
      "epoch no.4 train no.324830  loss = 4.10784 avg_loss = 3.12727\n",
      "epoch no.4 train no.324840  loss = 3.06084 avg_loss = 3.13497\n",
      "epoch no.4 train no.324850  loss = 2.85075 avg_loss = 3.13255\n",
      "epoch no.4 train no.324860  loss = 3.55134 avg_loss = 3.11177\n",
      "epoch no.4 train no.324870  loss = 2.71232 avg_loss = 3.11691\n",
      "epoch no.4 train no.324880  loss = 3.33447 avg_loss = 3.12587\n",
      "epoch no.4 train no.324890  loss = 2.40448 avg_loss = 3.14221\n",
      "epoch no.4 train no.324900  loss = 4.84754 avg_loss = 3.18947\n",
      "epoch no.4 train no.324910  loss = 2.11980 avg_loss = 3.13767\n",
      "epoch no.4 train no.324920  loss = 2.68098 avg_loss = 3.20118\n",
      "epoch no.4 train no.324930  loss = 2.58834 avg_loss = 3.18032\n",
      "epoch no.4 train no.324940  loss = 3.05238 avg_loss = 3.17971\n",
      "epoch no.4 train no.324950  loss = 2.14717 avg_loss = 3.21509\n",
      "epoch no.4 train no.324960  loss = 2.05128 avg_loss = 3.24408\n",
      "epoch no.4 train no.324970  loss = 3.74022 avg_loss = 3.21540\n",
      "epoch no.4 train no.324980  loss = 2.95386 avg_loss = 3.20548\n",
      "epoch no.4 train no.324990  loss = 3.14215 avg_loss = 3.20259\n",
      "epoch no.4 train no.325000  loss = 3.68548 avg_loss = 3.20803\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '▁담은', '▁뉴', '</s>', '</s>']\n",
      "여름밤의 감성을 담은 음악들</s>\n",
      "epoch no.4 train no.325010  loss = 3.03424 avg_loss = 3.19083\n",
      "epoch no.4 train no.325020  loss = 3.24198 avg_loss = 3.14917\n",
      "epoch no.4 train no.325030  loss = 3.74415 avg_loss = 3.18364\n",
      "epoch no.4 train no.325040  loss = 4.05332 avg_loss = 3.16879\n",
      "epoch no.4 train no.325050  loss = 2.27900 avg_loss = 3.13317\n",
      "epoch no.4 train no.325060  loss = 3.91766 avg_loss = 3.16687\n",
      "epoch no.4 train no.325070  loss = 2.81168 avg_loss = 3.15094\n",
      "epoch no.4 train no.325080  loss = 4.54033 avg_loss = 3.17266\n",
      "epoch no.4 train no.325090  loss = 1.94852 avg_loss = 3.17149\n",
      "epoch no.4 train no.325100  loss = 5.30539 avg_loss = 3.18535\n",
      "epoch no.4 train no.325110  loss = 2.32615 avg_loss = 3.18179\n",
      "epoch no.4 train no.325120  loss = 4.16104 avg_loss = 3.19304\n",
      "epoch no.4 train no.325130  loss = 2.74707 avg_loss = 3.19213\n",
      "epoch no.4 train no.325140  loss = 2.73238 avg_loss = 3.17183\n",
      "epoch no.4 train no.325150  loss = 3.44822 avg_loss = 3.19669\n",
      "epoch no.4 train no.325160  loss = 4.00653 avg_loss = 3.17206\n",
      "epoch no.4 train no.325170  loss = 3.03499 avg_loss = 3.19927\n",
      "epoch no.4 train no.325180  loss = 2.77311 avg_loss = 3.22184\n",
      "epoch no.4 train no.325190  loss = 2.76334 avg_loss = 3.21566\n",
      "epoch no.4 train no.325200  loss = 4.93875 avg_loss = 3.22523\n",
      "epoch no.4 train no.325210  loss = 3.55807 avg_loss = 3.20584\n",
      "epoch no.4 train no.325220  loss = 2.72232 avg_loss = 3.22680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.325230  loss = 3.31375 avg_loss = 3.20827\n",
      "epoch no.4 train no.325240  loss = 2.72736 avg_loss = 3.18457\n",
      "epoch no.4 train no.325250  loss = 2.15257 avg_loss = 3.19541\n",
      "epoch no.4 train no.325260  loss = 3.15418 avg_loss = 3.20025\n",
      "epoch no.4 train no.325270  loss = 3.23501 avg_loss = 3.19349\n",
      "epoch no.4 train no.325280  loss = 5.83512 avg_loss = 3.15391\n",
      "epoch no.4 train no.325290  loss = 2.13525 avg_loss = 3.16516\n",
      "epoch no.4 train no.325300  loss = 2.63521 avg_loss = 3.14529\n",
      "epoch no.4 train no.325310  loss = 2.93929 avg_loss = 3.13492\n",
      "epoch no.4 train no.325320  loss = 2.30413 avg_loss = 3.14606\n",
      "epoch no.4 train no.325330  loss = 3.55196 avg_loss = 3.17301\n",
      "epoch no.4 train no.325340  loss = 3.95380 avg_loss = 3.21095\n",
      "epoch no.4 train no.325350  loss = 3.59630 avg_loss = 3.21695\n",
      "epoch no.4 train no.325360  loss = 2.24146 avg_loss = 3.18937\n",
      "epoch no.4 train no.325370  loss = 2.65134 avg_loss = 3.13617\n",
      "epoch no.4 train no.325380  loss = 4.47378 avg_loss = 3.13847\n",
      "epoch no.4 train no.325390  loss = 2.76291 avg_loss = 3.12331\n",
      "epoch no.4 train no.325400  loss = 3.54405 avg_loss = 3.12362\n",
      "epoch no.4 train no.325410  loss = 3.73427 avg_loss = 3.14964\n",
      "epoch no.4 train no.325420  loss = 3.08465 avg_loss = 3.18341\n",
      "epoch no.4 train no.325430  loss = 3.85147 avg_loss = 3.20560\n",
      "epoch no.4 train no.325440  loss = 2.49440 avg_loss = 3.19298\n",
      "epoch no.4 train no.325450  loss = 3.34161 avg_loss = 3.21641\n",
      "epoch no.4 train no.325460  loss = 2.70968 avg_loss = 3.17600\n",
      "epoch no.4 train no.325470  loss = 2.34922 avg_loss = 3.17236\n",
      "epoch no.4 train no.325480  loss = 2.48255 avg_loss = 3.14990\n",
      "epoch no.4 train no.325490  loss = 3.39373 avg_loss = 3.16286\n",
      "epoch no.4 train no.325500  loss = 2.89105 avg_loss = 3.18008\n",
      "epoch no.4 train no.325510  loss = 5.11491 avg_loss = 3.20960\n",
      "epoch no.4 train no.325520  loss = 3.34528 avg_loss = 3.17757\n",
      "epoch no.4 train no.325530  loss = 4.72701 avg_loss = 3.18131\n",
      "epoch no.4 train no.325540  loss = 3.42449 avg_loss = 3.18559\n",
      "epoch no.4 train no.325550  loss = 3.30113 avg_loss = 3.19754\n",
      "epoch no.4 train no.325560  loss = 3.21514 avg_loss = 3.21678\n",
      "epoch no.4 train no.325570  loss = 3.28263 avg_loss = 3.20654\n",
      "epoch no.4 train no.325580  loss = 2.30157 avg_loss = 3.20902\n",
      "epoch no.4 train no.325590  loss = 3.65223 avg_loss = 3.22561\n",
      "epoch no.4 train no.325600  loss = 4.47250 avg_loss = 3.20338\n",
      "epoch no.4 train no.325610  loss = 2.44976 avg_loss = 3.17941\n",
      "epoch no.4 train no.325620  loss = 3.28194 avg_loss = 3.20872\n",
      "epoch no.4 train no.325630  loss = 3.89049 avg_loss = 3.22008\n",
      "epoch no.4 train no.325640  loss = 4.73041 avg_loss = 3.23327\n",
      "epoch no.4 train no.325650  loss = 3.14778 avg_loss = 3.20411\n",
      "epoch no.4 train no.325660  loss = 3.45966 avg_loss = 3.19408\n",
      "epoch no.4 train no.325670  loss = 3.28559 avg_loss = 3.19732\n",
      "epoch no.4 train no.325680  loss = 2.34120 avg_loss = 3.17389\n",
      "epoch no.4 train no.325690  loss = 3.27415 avg_loss = 3.19019\n",
      "epoch no.4 train no.325700  loss = 1.71827 avg_loss = 3.16582\n",
      "epoch no.4 train no.325710  loss = 4.56781 avg_loss = 3.22378\n",
      "epoch no.4 train no.325720  loss = 2.06827 avg_loss = 3.18664\n",
      "epoch no.4 train no.325730  loss = 3.38194 avg_loss = 3.17227\n",
      "epoch no.4 train no.325740  loss = 4.66654 avg_loss = 3.21596\n",
      "epoch no.4 train no.325750  loss = 3.70756 avg_loss = 3.18859\n",
      "epoch no.4 train no.325760  loss = 2.75869 avg_loss = 3.21025\n",
      "epoch no.4 train no.325770  loss = 3.05574 avg_loss = 3.20023\n",
      "epoch no.4 train no.325780  loss = 2.28733 avg_loss = 3.19048\n",
      "epoch no.4 train no.325790  loss = 3.12780 avg_loss = 3.19697\n",
      "epoch no.4 train no.325800  loss = 2.57842 avg_loss = 3.18612\n",
      "epoch no.4 train no.325810  loss = 1.62431 avg_loss = 3.15061\n",
      "epoch no.4 train no.325820  loss = 4.48567 avg_loss = 3.14366\n",
      "epoch no.4 train no.325830  loss = 2.73735 avg_loss = 3.14833\n",
      "epoch no.4 train no.325840  loss = 4.25225 avg_loss = 3.15592\n",
      "epoch no.4 train no.325850  loss = 3.32262 avg_loss = 3.17607\n",
      "epoch no.4 train no.325860  loss = 3.50601 avg_loss = 3.22529\n",
      "epoch no.4 train no.325870  loss = 2.36191 avg_loss = 3.22702\n",
      "epoch no.4 train no.325880  loss = 2.98438 avg_loss = 3.25040\n",
      "epoch no.4 train no.325890  loss = 4.12015 avg_loss = 3.22661\n",
      "epoch no.4 train no.325900  loss = 2.21623 avg_loss = 3.22500\n",
      "epoch no.4 train no.325910  loss = 2.49641 avg_loss = 3.23607\n",
      "epoch no.4 train no.325920  loss = 2.77297 avg_loss = 3.24885\n",
      "epoch no.4 train no.325930  loss = 3.65836 avg_loss = 3.24696\n",
      "epoch no.4 train no.325940  loss = 4.36537 avg_loss = 3.26452\n",
      "epoch no.4 train no.325950  loss = 3.78903 avg_loss = 3.27893\n",
      "epoch no.4 train no.325960  loss = 2.66417 avg_loss = 3.25125\n",
      "epoch no.4 train no.325970  loss = 2.81403 avg_loss = 3.26499\n",
      "epoch no.4 train no.325980  loss = 3.53480 avg_loss = 3.23715\n",
      "epoch no.4 train no.325990  loss = 2.59151 avg_loss = 3.20566\n",
      "epoch no.4 train no.326000  loss = 4.09524 avg_loss = 3.20106\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '</s>', '사', '</s>']\n",
      "여름밤에 듣는 재즈보사</s>\n",
      "epoch no.4 train no.326010  loss = 2.93047 avg_loss = 3.19511\n",
      "epoch no.4 train no.326020  loss = 3.57335 avg_loss = 3.22725\n",
      "epoch no.4 train no.326030  loss = 3.13072 avg_loss = 3.21572\n",
      "epoch no.4 train no.326040  loss = 4.65006 avg_loss = 3.19989\n",
      "epoch no.4 train no.326050  loss = 4.36544 avg_loss = 3.19155\n",
      "epoch no.4 train no.326060  loss = 4.19984 avg_loss = 3.15348\n",
      "epoch no.4 train no.326070  loss = 2.34550 avg_loss = 3.14266\n",
      "epoch no.4 train no.326080  loss = 3.51847 avg_loss = 3.16020\n",
      "epoch no.4 train no.326090  loss = 2.98467 avg_loss = 3.13339\n",
      "epoch no.4 train no.326100  loss = 3.11032 avg_loss = 3.09697\n",
      "epoch no.4 train no.326110  loss = 3.28491 avg_loss = 3.06874\n",
      "epoch no.4 train no.326120  loss = 2.80965 avg_loss = 3.12206\n",
      "epoch no.4 train no.326130  loss = 4.56182 avg_loss = 3.16008\n",
      "epoch no.4 train no.326140  loss = 5.05212 avg_loss = 3.17734\n",
      "epoch no.4 train no.326150  loss = 2.44143 avg_loss = 3.21567\n",
      "epoch no.4 train no.326160  loss = 3.60628 avg_loss = 3.24752\n",
      "epoch no.4 train no.326170  loss = 4.61743 avg_loss = 3.26273\n",
      "epoch no.4 train no.326180  loss = 4.67525 avg_loss = 3.27725\n",
      "epoch no.4 train no.326190  loss = 2.94796 avg_loss = 3.22004\n",
      "epoch no.4 train no.326200  loss = 2.87267 avg_loss = 3.21208\n",
      "epoch no.4 train no.326210  loss = 3.35454 avg_loss = 3.21667\n",
      "epoch no.4 train no.326220  loss = 3.66873 avg_loss = 3.22484\n",
      "epoch no.4 train no.326230  loss = 2.41348 avg_loss = 3.20840\n",
      "epoch no.4 train no.326240  loss = 3.28320 avg_loss = 3.24581\n",
      "epoch no.4 train no.326250  loss = 2.39174 avg_loss = 3.23641\n",
      "epoch no.4 train no.326260  loss = 2.78019 avg_loss = 3.23053\n",
      "epoch no.4 train no.326270  loss = 2.75201 avg_loss = 3.22796\n",
      "epoch no.4 train no.326280  loss = 4.89415 avg_loss = 3.26024\n",
      "epoch no.4 train no.326290  loss = 4.52119 avg_loss = 3.25069\n",
      "epoch no.4 train no.326300  loss = 1.73730 avg_loss = 3.27694\n",
      "epoch no.4 train no.326310  loss = 3.79267 avg_loss = 3.30183\n",
      "epoch no.4 train no.326320  loss = 3.77658 avg_loss = 3.28838\n",
      "epoch no.4 train no.326330  loss = 2.30071 avg_loss = 3.25614\n",
      "epoch no.4 train no.326340  loss = 3.53703 avg_loss = 3.22773\n",
      "epoch no.4 train no.326350  loss = 4.10944 avg_loss = 3.19177\n",
      "epoch no.4 train no.326360  loss = 2.50713 avg_loss = 3.16179\n",
      "epoch no.4 train no.326370  loss = 3.20070 avg_loss = 3.16273\n",
      "epoch no.4 train no.326380  loss = 4.19316 avg_loss = 3.16954\n",
      "epoch no.4 train no.326390  loss = 2.10911 avg_loss = 3.15515\n",
      "epoch no.4 train no.326400  loss = 3.71746 avg_loss = 3.19775\n",
      "epoch no.4 train no.326410  loss = 2.66759 avg_loss = 3.21566\n",
      "epoch no.4 train no.326420  loss = 3.85182 avg_loss = 3.19024\n",
      "epoch no.4 train no.326430  loss = 3.70449 avg_loss = 3.20515\n",
      "epoch no.4 train no.326440  loss = 3.96267 avg_loss = 3.23421\n",
      "epoch no.4 train no.326450  loss = 2.17535 avg_loss = 3.20337\n",
      "epoch no.4 train no.326460  loss = 4.46649 avg_loss = 3.24750\n",
      "epoch no.4 train no.326470  loss = 1.94075 avg_loss = 3.23136\n",
      "epoch no.4 train no.326480  loss = 2.63510 avg_loss = 3.18716\n",
      "epoch no.4 train no.326490  loss = 2.47299 avg_loss = 3.19162\n",
      "epoch no.4 train no.326500  loss = 2.71026 avg_loss = 3.13242\n",
      "epoch no.4 train no.326510  loss = 3.34795 avg_loss = 3.12383\n",
      "epoch no.4 train no.326520  loss = 2.38416 avg_loss = 3.16598\n",
      "epoch no.4 train no.326530  loss = 2.24961 avg_loss = 3.13597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.326540  loss = 3.46508 avg_loss = 3.16255\n",
      "epoch no.4 train no.326550  loss = 3.39683 avg_loss = 3.17811\n",
      "epoch no.4 train no.326560  loss = 3.65050 avg_loss = 3.15140\n",
      "epoch no.4 train no.326570  loss = 3.58019 avg_loss = 3.16888\n",
      "epoch no.4 train no.326580  loss = 3.80747 avg_loss = 3.16120\n",
      "epoch no.4 train no.326590  loss = 3.14327 avg_loss = 3.12631\n",
      "epoch no.4 train no.326600  loss = 1.97039 avg_loss = 3.11277\n",
      "epoch no.4 train no.326610  loss = 2.75290 avg_loss = 3.10120\n",
      "epoch no.4 train no.326620  loss = 3.88323 avg_loss = 3.08769\n",
      "epoch no.4 train no.326630  loss = 3.86993 avg_loss = 3.09927\n",
      "epoch no.4 train no.326640  loss = 2.55968 avg_loss = 3.05621\n",
      "epoch no.4 train no.326650  loss = 3.39553 avg_loss = 3.07732\n",
      "epoch no.4 train no.326660  loss = 1.33250 avg_loss = 3.06020\n",
      "epoch no.4 train no.326670  loss = 3.53286 avg_loss = 3.05936\n",
      "epoch no.4 train no.326680  loss = 3.70089 avg_loss = 3.04633\n",
      "epoch no.4 train no.326690  loss = 3.21001 avg_loss = 3.06146\n",
      "epoch no.4 train no.326700  loss = 3.29786 avg_loss = 3.04263\n",
      "epoch no.4 train no.326710  loss = 3.31237 avg_loss = 3.06062\n",
      "epoch no.4 train no.326720  loss = 2.03593 avg_loss = 3.05578\n",
      "epoch no.4 train no.326730  loss = 4.69078 avg_loss = 3.07660\n",
      "epoch no.4 train no.326740  loss = 2.65898 avg_loss = 3.06708\n",
      "epoch no.4 train no.326750  loss = 4.61968 avg_loss = 3.09106\n",
      "epoch no.4 train no.326760  loss = 3.39317 avg_loss = 3.08552\n",
      "epoch no.4 train no.326770  loss = 3.18790 avg_loss = 3.12510\n",
      "epoch no.4 train no.326780  loss = 2.00450 avg_loss = 3.13945\n",
      "epoch no.4 train no.326790  loss = 2.19334 avg_loss = 3.11165\n",
      "epoch no.4 train no.326800  loss = 5.02737 avg_loss = 3.10072\n",
      "epoch no.4 train no.326810  loss = 3.76952 avg_loss = 3.07974\n",
      "epoch no.4 train no.326820  loss = 4.70601 avg_loss = 3.12212\n",
      "epoch no.4 train no.326830  loss = 2.14893 avg_loss = 3.14574\n",
      "epoch no.4 train no.326840  loss = 3.15887 avg_loss = 3.16370\n",
      "epoch no.4 train no.326850  loss = 2.84305 avg_loss = 3.16038\n",
      "epoch no.4 train no.326860  loss = 4.54390 avg_loss = 3.20743\n",
      "epoch no.4 train no.326870  loss = 3.71579 avg_loss = 3.18840\n",
      "epoch no.4 train no.326880  loss = 4.71924 avg_loss = 3.17840\n",
      "epoch no.4 train no.326890  loss = 1.68198 avg_loss = 3.13459\n",
      "epoch no.4 train no.326900  loss = 1.92917 avg_loss = 3.11798\n",
      "epoch no.4 train no.326910  loss = 2.45937 avg_loss = 3.07905\n",
      "epoch no.4 train no.326920  loss = 2.50209 avg_loss = 3.08896\n",
      "epoch no.4 train no.326930  loss = 3.57696 avg_loss = 3.08511\n",
      "epoch no.4 train no.326940  loss = 2.99228 avg_loss = 3.06796\n",
      "epoch no.4 train no.326950  loss = 1.58969 avg_loss = 3.08871\n",
      "epoch no.4 train no.326960  loss = 3.66322 avg_loss = 3.09811\n",
      "epoch no.4 train no.326970  loss = 2.59276 avg_loss = 3.11069\n",
      "epoch no.4 train no.326980  loss = 3.01204 avg_loss = 3.12654\n",
      "epoch no.4 train no.326990  loss = 3.24756 avg_loss = 3.16863\n",
      "epoch no.4 train no.327000  loss = 3.90671 avg_loss = 3.20923\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁신나는', '한', '▁노래', '</s>']\n",
      "여름과 어울리는 청량한 음악</s>\n",
      "epoch no.4 train no.327010  loss = 2.22716 avg_loss = 3.20013\n",
      "epoch no.4 train no.327020  loss = 4.54087 avg_loss = 3.28522\n",
      "epoch no.4 train no.327030  loss = 3.74283 avg_loss = 3.24647\n",
      "epoch no.4 train no.327040  loss = 2.81627 avg_loss = 3.26686\n",
      "epoch no.4 train no.327050  loss = 4.13605 avg_loss = 3.29823\n",
      "epoch no.4 train no.327060  loss = 1.60887 avg_loss = 3.25144\n",
      "epoch no.4 train no.327070  loss = 3.58644 avg_loss = 3.28458\n",
      "epoch no.4 train no.327080  loss = 2.58947 avg_loss = 3.28438\n",
      "epoch no.4 train no.327090  loss = 2.65400 avg_loss = 3.24938\n",
      "epoch no.4 train no.327100  loss = 2.85073 avg_loss = 3.23076\n",
      "epoch no.4 train no.327110  loss = 3.26574 avg_loss = 3.21034\n",
      "epoch no.4 train no.327120  loss = 3.19220 avg_loss = 3.15049\n",
      "epoch no.4 train no.327130  loss = 4.66042 avg_loss = 3.16350\n",
      "epoch no.4 train no.327140  loss = 3.50702 avg_loss = 3.15218\n",
      "epoch no.4 train no.327150  loss = 3.11363 avg_loss = 3.16551\n",
      "epoch no.4 train no.327160  loss = 3.32434 avg_loss = 3.11881\n",
      "epoch no.4 train no.327170  loss = 2.96099 avg_loss = 3.14513\n",
      "epoch no.4 train no.327180  loss = 2.79511 avg_loss = 3.14063\n",
      "epoch no.4 train no.327190  loss = 3.77416 avg_loss = 3.14041\n",
      "epoch no.4 train no.327200  loss = 3.39754 avg_loss = 3.14469\n",
      "epoch no.4 train no.327210  loss = 4.45737 avg_loss = 3.17137\n",
      "epoch no.4 train no.327220  loss = 3.98534 avg_loss = 3.17380\n",
      "epoch no.4 train no.327230  loss = 3.41066 avg_loss = 3.19170\n",
      "epoch no.4 train no.327240  loss = 2.73882 avg_loss = 3.18298\n",
      "epoch no.4 train no.327250  loss = 2.29587 avg_loss = 3.19054\n",
      "epoch no.4 train no.327260  loss = 4.07483 avg_loss = 3.18580\n",
      "epoch no.4 train no.327270  loss = 3.33823 avg_loss = 3.19095\n",
      "epoch no.4 train no.327280  loss = 2.90029 avg_loss = 3.20471\n",
      "epoch no.4 train no.327290  loss = 3.02859 avg_loss = 3.22193\n",
      "epoch no.4 train no.327300  loss = 4.10860 avg_loss = 3.22136\n",
      "epoch no.4 train no.327310  loss = 3.20577 avg_loss = 3.21816\n",
      "epoch no.4 train no.327320  loss = 3.10921 avg_loss = 3.20637\n",
      "epoch no.4 train no.327330  loss = 1.65975 avg_loss = 3.17136\n",
      "epoch no.4 train no.327340  loss = 4.74469 avg_loss = 3.20194\n",
      "epoch no.4 train no.327350  loss = 2.39149 avg_loss = 3.20319\n",
      "epoch no.4 train no.327360  loss = 2.14825 avg_loss = 3.17623\n",
      "epoch no.4 train no.327370  loss = 3.77336 avg_loss = 3.17271\n",
      "epoch no.4 train no.327380  loss = 2.32241 avg_loss = 3.18328\n",
      "epoch no.4 train no.327390  loss = 2.77114 avg_loss = 3.18300\n",
      "epoch no.4 train no.327400  loss = 2.51289 avg_loss = 3.17219\n",
      "epoch no.4 train no.327410  loss = 2.74764 avg_loss = 3.16683\n",
      "epoch no.4 train no.327420  loss = 2.28168 avg_loss = 3.17122\n",
      "epoch no.4 train no.327430  loss = 3.37571 avg_loss = 3.15139\n",
      "epoch no.4 train no.327440  loss = 3.55909 avg_loss = 3.15405\n",
      "epoch no.4 train no.327450  loss = 5.53027 avg_loss = 3.15610\n",
      "epoch no.4 train no.327460  loss = 3.01806 avg_loss = 3.18942\n",
      "epoch no.4 train no.327470  loss = 2.57744 avg_loss = 3.22079\n",
      "epoch no.4 train no.327480  loss = 2.13275 avg_loss = 3.19128\n",
      "epoch no.4 train no.327490  loss = 2.74378 avg_loss = 3.16031\n",
      "epoch no.4 train no.327500  loss = 3.01601 avg_loss = 3.18915\n",
      "epoch no.4 train no.327510  loss = 2.99380 avg_loss = 3.17189\n",
      "epoch no.4 train no.327520  loss = 2.02235 avg_loss = 3.12874\n",
      "epoch no.4 train no.327530  loss = 4.69455 avg_loss = 3.19697\n",
      "epoch no.4 train no.327540  loss = 1.74929 avg_loss = 3.16279\n",
      "epoch no.4 train no.327550  loss = 3.77373 avg_loss = 3.17551\n",
      "epoch no.4 train no.327560  loss = 2.89933 avg_loss = 3.16371\n",
      "epoch no.4 train no.327570  loss = 4.06578 avg_loss = 3.17357\n",
      "epoch no.4 train no.327580  loss = 3.17139 avg_loss = 3.21689\n",
      "epoch no.4 train no.327590  loss = 4.37024 avg_loss = 3.23680\n",
      "epoch no.4 train no.327600  loss = 1.80563 avg_loss = 3.21262\n",
      "epoch no.4 train no.327610  loss = 3.09177 avg_loss = 3.23027\n",
      "epoch no.4 train no.327620  loss = 5.20842 avg_loss = 3.26562\n",
      "epoch no.4 train no.327630  loss = 2.98632 avg_loss = 3.26446\n",
      "epoch no.4 train no.327640  loss = 3.23369 avg_loss = 3.25444\n",
      "epoch no.4 train no.327650  loss = 2.53945 avg_loss = 3.23164\n",
      "epoch no.4 train no.327660  loss = 4.63565 avg_loss = 3.22590\n",
      "epoch no.4 train no.327670  loss = 2.55260 avg_loss = 3.20565\n",
      "epoch no.4 train no.327680  loss = 2.87033 avg_loss = 3.18839\n",
      "epoch no.4 train no.327690  loss = 4.23245 avg_loss = 3.21582\n",
      "epoch no.4 train no.327700  loss = 3.58406 avg_loss = 3.22112\n",
      "epoch no.4 train no.327710  loss = 2.99375 avg_loss = 3.22094\n",
      "epoch no.4 train no.327720  loss = 4.43560 avg_loss = 3.28290\n",
      "epoch no.4 train no.327730  loss = 3.86602 avg_loss = 3.27703\n",
      "epoch no.4 train no.327740  loss = 3.06202 avg_loss = 3.23040\n",
      "epoch no.4 train no.327750  loss = 4.16382 avg_loss = 3.22284\n",
      "epoch no.4 train no.327760  loss = 3.67852 avg_loss = 3.20060\n",
      "epoch no.4 train no.327770  loss = 1.72289 avg_loss = 3.16971\n",
      "epoch no.4 train no.327780  loss = 3.59553 avg_loss = 3.12971\n",
      "epoch no.4 train no.327790  loss = 4.12074 avg_loss = 3.14564\n",
      "epoch no.4 train no.327800  loss = 2.49208 avg_loss = 3.13832\n",
      "epoch no.4 train no.327810  loss = 3.04845 avg_loss = 3.16506\n",
      "epoch no.4 train no.327820  loss = 6.21220 avg_loss = 3.19003\n",
      "epoch no.4 train no.327830  loss = 2.04194 avg_loss = 3.18515\n",
      "epoch no.4 train no.327840  loss = 2.51599 avg_loss = 3.17480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.327850  loss = 2.31801 avg_loss = 3.17444\n",
      "epoch no.4 train no.327860  loss = 4.49969 avg_loss = 3.17715\n",
      "epoch no.4 train no.327870  loss = 3.60208 avg_loss = 3.19940\n",
      "epoch no.4 train no.327880  loss = 4.11941 avg_loss = 3.21556\n",
      "epoch no.4 train no.327890  loss = 2.76267 avg_loss = 3.23748\n",
      "epoch no.4 train no.327900  loss = 4.05882 avg_loss = 3.20097\n",
      "epoch no.4 train no.327910  loss = 2.65556 avg_loss = 3.21959\n",
      "epoch no.4 train no.327920  loss = 4.25606 avg_loss = 3.25306\n",
      "epoch no.4 train no.327930  loss = 2.62124 avg_loss = 3.26802\n",
      "epoch no.4 train no.327940  loss = 2.77930 avg_loss = 3.28274\n",
      "epoch no.4 train no.327950  loss = 3.29276 avg_loss = 3.26438\n",
      "epoch no.4 train no.327960  loss = 2.89952 avg_loss = 3.23746\n",
      "epoch no.4 train no.327970  loss = 3.43827 avg_loss = 3.23486\n",
      "epoch no.4 train no.327980  loss = 3.53371 avg_loss = 3.29912\n",
      "epoch no.4 train no.327990  loss = 2.66173 avg_loss = 3.29621\n",
      "epoch no.4 train no.328000  loss = 2.59335 avg_loss = 3.28685\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁채워', '놓을', '▁감성', '</s>']\n",
      "여름밤을 수놓는 음악</s>\n",
      "epoch no.4 train no.328010  loss = 3.09380 avg_loss = 3.27043\n",
      "epoch no.4 train no.328020  loss = 3.24167 avg_loss = 3.23859\n",
      "epoch no.4 train no.328030  loss = 5.32489 avg_loss = 3.24980\n",
      "epoch no.4 train no.328040  loss = 3.40847 avg_loss = 3.22181\n",
      "epoch no.4 train no.328050  loss = 2.46103 avg_loss = 3.20805\n",
      "epoch no.4 train no.328060  loss = 4.57226 avg_loss = 3.21543\n",
      "epoch no.4 train no.328070  loss = 2.74514 avg_loss = 3.20617\n",
      "epoch no.4 train no.328080  loss = 2.98141 avg_loss = 3.19645\n",
      "epoch no.4 train no.328090  loss = 3.17074 avg_loss = 3.20873\n",
      "epoch no.4 train no.328100  loss = 2.90477 avg_loss = 3.18896\n",
      "epoch no.4 train no.328110  loss = 2.25093 avg_loss = 3.21008\n",
      "epoch no.4 train no.328120  loss = 3.49093 avg_loss = 3.19399\n",
      "epoch no.4 train no.328130  loss = 2.56587 avg_loss = 3.21806\n",
      "epoch no.4 train no.328140  loss = 3.77340 avg_loss = 3.25874\n",
      "epoch no.4 train no.328150  loss = 3.41855 avg_loss = 3.20100\n",
      "epoch no.4 train no.328160  loss = 3.49034 avg_loss = 3.22667\n",
      "epoch no.4 train no.328170  loss = 5.18349 avg_loss = 3.24832\n",
      "epoch no.4 train no.328180  loss = 1.72672 avg_loss = 3.19512\n",
      "epoch no.4 train no.328190  loss = 3.25318 avg_loss = 3.20645\n",
      "epoch no.4 train no.328200  loss = 3.18440 avg_loss = 3.19285\n",
      "epoch no.4 train no.328210  loss = 2.22561 avg_loss = 3.15808\n",
      "epoch no.4 train no.328220  loss = 3.51156 avg_loss = 3.13074\n",
      "epoch no.4 train no.328230  loss = 2.50058 avg_loss = 3.10834\n",
      "epoch no.4 train no.328240  loss = 2.62487 avg_loss = 3.12839\n",
      "epoch no.4 train no.328250  loss = 2.81199 avg_loss = 3.13760\n",
      "epoch no.4 train no.328260  loss = 1.88204 avg_loss = 3.21334\n",
      "epoch no.4 train no.328270  loss = 2.43561 avg_loss = 3.20149\n",
      "epoch no.4 train no.328280  loss = 2.96999 avg_loss = 3.24109\n",
      "epoch no.4 train no.328290  loss = 4.01605 avg_loss = 3.28026\n",
      "epoch no.4 train no.328300  loss = 4.09922 avg_loss = 3.29998\n",
      "epoch no.4 train no.328310  loss = 1.75938 avg_loss = 3.31866\n",
      "epoch no.4 train no.328320  loss = 3.90840 avg_loss = 3.37861\n",
      "epoch no.4 train no.328330  loss = 3.30279 avg_loss = 3.35845\n",
      "epoch no.4 train no.328340  loss = 3.17596 avg_loss = 3.37489\n",
      "epoch no.4 train no.328350  loss = 2.88689 avg_loss = 3.37089\n",
      "epoch no.4 train no.328360  loss = 3.62885 avg_loss = 3.33926\n",
      "epoch no.4 train no.328370  loss = 2.72192 avg_loss = 3.27544\n",
      "epoch no.4 train no.328380  loss = 3.86297 avg_loss = 3.22516\n",
      "epoch no.4 train no.328390  loss = 1.57604 avg_loss = 3.18083\n",
      "epoch no.4 train no.328400  loss = 3.86184 avg_loss = 3.16605\n",
      "epoch no.4 train no.328410  loss = 4.84875 avg_loss = 3.18426\n",
      "epoch no.4 train no.328420  loss = 2.43725 avg_loss = 3.19314\n",
      "epoch no.4 train no.328430  loss = 2.25816 avg_loss = 3.16854\n",
      "epoch no.4 train no.328440  loss = 2.86357 avg_loss = 3.17040\n",
      "epoch no.4 train no.328450  loss = 1.83050 avg_loss = 3.15154\n",
      "epoch no.4 train no.328460  loss = 3.06987 avg_loss = 3.13854\n",
      "epoch no.4 train no.328470  loss = 1.97413 avg_loss = 3.15747\n",
      "epoch no.4 train no.328480  loss = 2.69114 avg_loss = 3.15662\n",
      "epoch no.4 train no.328490  loss = 3.76192 avg_loss = 3.15128\n",
      "epoch no.4 train no.328500  loss = 3.70279 avg_loss = 3.16975\n",
      "epoch no.4 train no.328510  loss = 3.42263 avg_loss = 3.16511\n",
      "epoch no.4 train no.328520  loss = 2.85881 avg_loss = 3.21721\n",
      "epoch no.4 train no.328530  loss = 2.61109 avg_loss = 3.18255\n",
      "epoch no.4 train no.328540  loss = 5.08809 avg_loss = 3.24114\n",
      "epoch no.4 train no.328550  loss = 4.76959 avg_loss = 3.24680\n",
      "epoch no.4 train no.328560  loss = 1.72351 avg_loss = 3.21799\n",
      "epoch no.4 train no.328570  loss = 2.63369 avg_loss = 3.22238\n",
      "epoch no.4 train no.328580  loss = 2.06874 avg_loss = 3.21889\n",
      "epoch no.4 train no.328590  loss = 3.06677 avg_loss = 3.19842\n",
      "epoch no.4 train no.328600  loss = 2.26732 avg_loss = 3.19902\n",
      "epoch no.4 train no.328610  loss = 2.11061 avg_loss = 3.22631\n",
      "epoch no.4 train no.328620  loss = 4.19616 avg_loss = 3.18787\n",
      "epoch no.4 train no.328630  loss = 3.96883 avg_loss = 3.21876\n",
      "epoch no.4 train no.328640  loss = 5.45973 avg_loss = 3.25186\n",
      "epoch no.4 train no.328650  loss = 4.34663 avg_loss = 3.24336\n",
      "epoch no.4 train no.328660  loss = 4.11827 avg_loss = 3.24165\n",
      "epoch no.4 train no.328670  loss = 2.70445 avg_loss = 3.24460\n",
      "epoch no.4 train no.328680  loss = 2.71881 avg_loss = 3.22058\n",
      "epoch no.4 train no.328690  loss = 4.33221 avg_loss = 3.20930\n",
      "epoch no.4 train no.328700  loss = 3.32849 avg_loss = 3.21863\n",
      "epoch no.4 train no.328710  loss = 3.21587 avg_loss = 3.23377\n",
      "epoch no.4 train no.328720  loss = 3.51446 avg_loss = 3.25651\n",
      "epoch no.4 train no.328730  loss = 3.18013 avg_loss = 3.22010\n",
      "epoch no.4 train no.328740  loss = 3.45262 avg_loss = 3.26162\n",
      "epoch no.4 train no.328750  loss = 3.40261 avg_loss = 3.25653\n",
      "epoch no.4 train no.328760  loss = 3.17386 avg_loss = 3.26209\n",
      "epoch no.4 train no.328770  loss = 2.44549 avg_loss = 3.25069\n",
      "epoch no.4 train no.328780  loss = 2.73892 avg_loss = 3.23074\n",
      "epoch no.4 train no.328790  loss = 2.71841 avg_loss = 3.23508\n",
      "epoch no.4 train no.328800  loss = 3.57024 avg_loss = 3.21854\n",
      "epoch no.4 train no.328810  loss = 2.15460 avg_loss = 3.17697\n",
      "epoch no.4 train no.328820  loss = 2.72442 avg_loss = 3.24103\n",
      "epoch no.4 train no.328830  loss = 2.53967 avg_loss = 3.20850\n",
      "epoch no.4 train no.328840  loss = 4.01163 avg_loss = 3.21962\n",
      "epoch no.4 train no.328850  loss = 2.70654 avg_loss = 3.20987\n",
      "epoch no.4 train no.328860  loss = 2.67326 avg_loss = 3.16223\n",
      "epoch no.4 train no.328870  loss = 2.47761 avg_loss = 3.16499\n",
      "epoch no.4 train no.328880  loss = 1.69188 avg_loss = 3.11544\n",
      "epoch no.4 train no.328890  loss = 3.60291 avg_loss = 3.11677\n",
      "epoch no.4 train no.328900  loss = 3.78800 avg_loss = 3.15292\n",
      "epoch no.4 train no.328910  loss = 3.95545 avg_loss = 3.20171\n",
      "epoch no.4 train no.328920  loss = 2.14127 avg_loss = 3.18874\n",
      "epoch no.4 train no.328930  loss = 2.25990 avg_loss = 3.22004\n",
      "epoch no.4 train no.328940  loss = 3.89741 avg_loss = 3.24633\n",
      "epoch no.4 train no.328950  loss = 3.05213 avg_loss = 3.20518\n",
      "epoch no.4 train no.328960  loss = 2.55675 avg_loss = 3.18832\n",
      "epoch no.4 train no.328970  loss = 2.56666 avg_loss = 3.21564\n",
      "epoch no.4 train no.328980  loss = 2.72365 avg_loss = 3.19346\n",
      "epoch no.4 train no.328990  loss = 2.68079 avg_loss = 3.16271\n",
      "epoch no.4 train no.329000  loss = 2.89451 avg_loss = 3.11456\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', '한', '▁노래', '곡', '</s>']\n",
      "여름밤에 듣는 잔잔한 연주곡</s>\n",
      "epoch no.4 train no.329010  loss = 2.98169 avg_loss = 3.12860\n",
      "epoch no.4 train no.329020  loss = 3.10305 avg_loss = 3.12327\n",
      "epoch no.4 train no.329030  loss = 3.31418 avg_loss = 3.12763\n",
      "epoch no.4 train no.329040  loss = 3.14079 avg_loss = 3.14700\n",
      "epoch no.4 train no.329050  loss = 5.97302 avg_loss = 3.16376\n",
      "epoch no.4 train no.329060  loss = 3.59956 avg_loss = 3.16304\n",
      "epoch no.4 train no.329070  loss = 4.60566 avg_loss = 3.17283\n",
      "epoch no.4 train no.329080  loss = 3.81698 avg_loss = 3.21429\n",
      "epoch no.4 train no.329090  loss = 3.04109 avg_loss = 3.19968\n",
      "epoch no.4 train no.329100  loss = 2.49486 avg_loss = 3.18248\n",
      "epoch no.4 train no.329110  loss = 1.91504 avg_loss = 3.15381\n",
      "epoch no.4 train no.329120  loss = 2.49547 avg_loss = 3.12539\n",
      "epoch no.4 train no.329130  loss = 3.12692 avg_loss = 3.14401\n",
      "epoch no.4 train no.329140  loss = 3.72789 avg_loss = 3.16967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.329150  loss = 3.06616 avg_loss = 3.15198\n",
      "epoch no.4 train no.329160  loss = 4.32672 avg_loss = 3.16549\n",
      "epoch no.4 train no.329170  loss = 3.00160 avg_loss = 3.16617\n",
      "epoch no.4 train no.329180  loss = 3.18783 avg_loss = 3.13416\n",
      "epoch no.4 train no.329190  loss = 0.91190 avg_loss = 3.15313\n",
      "epoch no.4 train no.329200  loss = 2.44273 avg_loss = 3.11269\n",
      "epoch no.4 train no.329210  loss = 2.36575 avg_loss = 3.10421\n",
      "epoch no.4 train no.329220  loss = 3.11582 avg_loss = 3.10429\n",
      "epoch no.4 train no.329230  loss = 4.77939 avg_loss = 3.18978\n",
      "epoch no.4 train no.329240  loss = 2.23001 avg_loss = 3.17456\n",
      "epoch no.4 train no.329250  loss = 4.52521 avg_loss = 3.17805\n",
      "epoch no.4 train no.329260  loss = 4.11740 avg_loss = 3.19292\n",
      "epoch no.4 train no.329270  loss = 2.76401 avg_loss = 3.21836\n",
      "epoch no.4 train no.329280  loss = 2.95824 avg_loss = 3.20343\n",
      "epoch no.4 train no.329290  loss = 4.22603 avg_loss = 3.18487\n",
      "epoch no.4 train no.329300  loss = 3.02492 avg_loss = 3.19316\n",
      "epoch no.4 train no.329310  loss = 2.92865 avg_loss = 3.22909\n",
      "epoch no.4 train no.329320  loss = 2.97562 avg_loss = 3.21483\n",
      "epoch no.4 train no.329330  loss = 3.94849 avg_loss = 3.18359\n",
      "epoch no.4 train no.329340  loss = 2.70179 avg_loss = 3.18087\n",
      "epoch no.4 train no.329350  loss = 4.07509 avg_loss = 3.20447\n",
      "epoch no.4 train no.329360  loss = 3.42627 avg_loss = 3.17710\n",
      "epoch no.4 train no.329370  loss = 2.93237 avg_loss = 3.20858\n",
      "epoch no.4 train no.329380  loss = 3.81463 avg_loss = 3.21791\n",
      "epoch no.4 train no.329390  loss = 3.03893 avg_loss = 3.21402\n",
      "epoch no.4 train no.329400  loss = 3.57912 avg_loss = 3.23127\n",
      "epoch no.4 train no.329410  loss = 1.41681 avg_loss = 3.20575\n",
      "epoch no.4 train no.329420  loss = 3.09209 avg_loss = 3.24139\n",
      "epoch no.4 train no.329430  loss = 2.97871 avg_loss = 3.21376\n",
      "epoch no.4 train no.329440  loss = 3.70705 avg_loss = 3.21796\n",
      "epoch no.4 train no.329450  loss = 2.90015 avg_loss = 3.19646\n",
      "epoch no.4 train no.329460  loss = 3.33569 avg_loss = 3.19034\n",
      "epoch no.4 train no.329470  loss = 3.79073 avg_loss = 3.19257\n",
      "epoch no.4 train no.329480  loss = 2.26671 avg_loss = 3.18659\n",
      "epoch no.4 train no.329490  loss = 2.97408 avg_loss = 3.19257\n",
      "epoch no.4 train no.329500  loss = 2.03221 avg_loss = 3.18677\n",
      "epoch no.4 train no.329510  loss = 2.96701 avg_loss = 3.20996\n",
      "epoch no.4 train no.329520  loss = 4.22340 avg_loss = 3.20951\n",
      "epoch no.4 train no.329530  loss = 2.92382 avg_loss = 3.16567\n",
      "epoch no.4 train no.329540  loss = 3.68062 avg_loss = 3.22138\n",
      "epoch no.4 train no.329550  loss = 3.26749 avg_loss = 3.20670\n",
      "epoch no.4 train no.329560  loss = 1.25420 avg_loss = 3.17385\n",
      "epoch no.4 train no.329570  loss = 2.70157 avg_loss = 3.15130\n",
      "epoch no.4 train no.329580  loss = 3.79131 avg_loss = 3.10344\n",
      "epoch no.4 train no.329590  loss = 3.29679 avg_loss = 3.11806\n",
      "epoch no.4 train no.329600  loss = 1.63795 avg_loss = 3.07758\n",
      "epoch no.4 train no.329610  loss = 2.90338 avg_loss = 3.09335\n",
      "epoch no.4 train no.329620  loss = 2.31149 avg_loss = 3.12516\n",
      "epoch no.4 train no.329630  loss = 3.09351 avg_loss = 3.11842\n",
      "epoch no.4 train no.329640  loss = 3.23650 avg_loss = 3.09009\n",
      "epoch no.4 train no.329650  loss = 4.15587 avg_loss = 3.09025\n",
      "epoch no.4 train no.329660  loss = 2.80563 avg_loss = 3.14320\n",
      "epoch no.4 train no.329670  loss = 2.53815 avg_loss = 3.17009\n",
      "epoch no.4 train no.329680  loss = 4.28841 avg_loss = 3.19066\n",
      "epoch no.4 train no.329690  loss = 3.18967 avg_loss = 3.18053\n",
      "epoch no.4 train no.329700  loss = 3.06010 avg_loss = 3.17640\n",
      "epoch no.4 train no.329710  loss = 2.76840 avg_loss = 3.13145\n",
      "epoch no.4 train no.329720  loss = 2.43715 avg_loss = 3.12145\n",
      "epoch no.4 train no.329730  loss = 3.18296 avg_loss = 3.14427\n",
      "epoch no.4 train no.329740  loss = 2.37836 avg_loss = 3.15058\n",
      "epoch no.4 train no.329750  loss = 2.48313 avg_loss = 3.16908\n",
      "epoch no.4 train no.329760  loss = 1.99257 avg_loss = 3.13605\n",
      "epoch no.4 train no.329770  loss = 2.56111 avg_loss = 3.17621\n",
      "epoch no.4 train no.329780  loss = 2.77554 avg_loss = 3.17362\n",
      "epoch no.4 train no.329790  loss = 3.06055 avg_loss = 3.20003\n",
      "epoch no.4 train no.329800  loss = 3.96343 avg_loss = 3.23670\n",
      "epoch no.4 train no.329810  loss = 2.89244 avg_loss = 3.20282\n",
      "epoch no.4 train no.329820  loss = 2.84396 avg_loss = 3.20768\n",
      "epoch no.4 train no.329830  loss = 1.15375 avg_loss = 3.17092\n",
      "epoch no.4 train no.329840  loss = 1.85926 avg_loss = 3.13293\n",
      "epoch no.4 train no.329850  loss = 3.58860 avg_loss = 3.11189\n",
      "epoch no.4 train no.329860  loss = 3.33978 avg_loss = 3.12976\n",
      "epoch no.4 train no.329870  loss = 2.34619 avg_loss = 3.14327\n",
      "epoch no.4 train no.329880  loss = 2.56445 avg_loss = 3.16836\n",
      "epoch no.4 train no.329890  loss = 2.14827 avg_loss = 3.15490\n",
      "epoch no.4 train no.329900  loss = 4.50480 avg_loss = 3.14095\n",
      "epoch no.4 train no.329910  loss = 2.27332 avg_loss = 3.12217\n",
      "epoch no.4 train no.329920  loss = 2.90759 avg_loss = 3.08748\n",
      "epoch no.4 train no.329930  loss = 3.23527 avg_loss = 3.09866\n",
      "epoch no.4 train no.329940  loss = 2.84080 avg_loss = 3.11496\n",
      "epoch no.4 train no.329950  loss = 2.32881 avg_loss = 3.17488\n",
      "epoch no.4 train no.329960  loss = 2.56467 avg_loss = 3.22483\n",
      "epoch no.4 train no.329970  loss = 2.17619 avg_loss = 3.18665\n",
      "epoch no.4 train no.329980  loss = 3.13321 avg_loss = 3.17740\n",
      "epoch no.4 train no.329990  loss = 3.35932 avg_loss = 3.19289\n",
      "epoch no.4 train no.330000  loss = 2.70792 avg_loss = 3.17225\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁시원한', '▁재즈', '</s>']\n",
      "여름밤의 로맨틱 재즈</s>\n",
      "epoch no.4 train no.330010  loss = 1.68311 avg_loss = 3.17187\n",
      "epoch no.4 train no.330020  loss = 3.40447 avg_loss = 3.15416\n",
      "epoch no.4 train no.330030  loss = 2.43225 avg_loss = 3.16671\n",
      "epoch no.4 train no.330040  loss = 2.06905 avg_loss = 3.14566\n",
      "epoch no.4 train no.330050  loss = 3.73890 avg_loss = 3.14575\n",
      "epoch no.4 train no.330060  loss = 2.31564 avg_loss = 3.12080\n",
      "epoch no.4 train no.330070  loss = 3.25393 avg_loss = 3.13984\n",
      "epoch no.4 train no.330080  loss = 2.48549 avg_loss = 3.13986\n",
      "epoch no.4 train no.330090  loss = 4.15814 avg_loss = 3.13006\n",
      "epoch no.4 train no.330100  loss = 5.07297 avg_loss = 3.17910\n",
      "epoch no.4 train no.330110  loss = 3.28216 avg_loss = 3.12680\n",
      "epoch no.4 train no.330120  loss = 3.24610 avg_loss = 3.11754\n",
      "epoch no.4 train no.330130  loss = 3.19711 avg_loss = 3.12280\n",
      "epoch no.4 train no.330140  loss = 4.71389 avg_loss = 3.16765\n",
      "epoch no.4 train no.330150  loss = 4.33599 avg_loss = 3.14758\n",
      "epoch no.4 train no.330160  loss = 3.66344 avg_loss = 3.12579\n",
      "epoch no.4 train no.330170  loss = 3.61271 avg_loss = 3.08143\n",
      "epoch no.4 train no.330180  loss = 3.43010 avg_loss = 3.11142\n",
      "epoch no.4 train no.330190  loss = 2.69005 avg_loss = 3.12437\n",
      "epoch no.4 train no.330200  loss = 5.37380 avg_loss = 3.21138\n",
      "epoch no.4 train no.330210  loss = 3.76212 avg_loss = 3.19314\n",
      "epoch no.4 train no.330220  loss = 3.06308 avg_loss = 3.18602\n",
      "epoch no.4 train no.330230  loss = 3.70916 avg_loss = 3.20669\n",
      "epoch no.4 train no.330240  loss = 5.58516 avg_loss = 3.23874\n",
      "epoch no.4 train no.330250  loss = 2.36715 avg_loss = 3.20255\n",
      "epoch no.4 train no.330260  loss = 3.23930 avg_loss = 3.16798\n",
      "epoch no.4 train no.330270  loss = 3.99566 avg_loss = 3.15158\n",
      "epoch no.4 train no.330280  loss = 2.89181 avg_loss = 3.15344\n",
      "epoch no.4 train no.330290  loss = 1.99001 avg_loss = 3.12410\n",
      "epoch no.4 train no.330300  loss = 3.31060 avg_loss = 3.13231\n",
      "epoch no.4 train no.330310  loss = 3.46042 avg_loss = 3.12186\n",
      "epoch no.4 train no.330320  loss = 2.52622 avg_loss = 3.15600\n",
      "epoch no.4 train no.330330  loss = 3.40065 avg_loss = 3.12799\n",
      "epoch no.4 train no.330340  loss = 2.63707 avg_loss = 3.14981\n",
      "epoch no.4 train no.330350  loss = 2.99543 avg_loss = 3.14384\n",
      "epoch no.4 train no.330360  loss = 3.83749 avg_loss = 3.16889\n",
      "epoch no.4 train no.330370  loss = 2.18537 avg_loss = 3.15694\n",
      "epoch no.4 train no.330380  loss = 5.31502 avg_loss = 3.19792\n",
      "epoch no.4 train no.330390  loss = 3.29731 avg_loss = 3.19960\n",
      "epoch no.4 train no.330400  loss = 3.74194 avg_loss = 3.22028\n",
      "epoch no.4 train no.330410  loss = 3.66026 avg_loss = 3.28496\n",
      "epoch no.4 train no.330420  loss = 2.68736 avg_loss = 3.22934\n",
      "epoch no.4 train no.330430  loss = 2.64817 avg_loss = 3.22068\n",
      "epoch no.4 train no.330440  loss = 3.27747 avg_loss = 3.21172\n",
      "epoch no.4 train no.330450  loss = 3.42210 avg_loss = 3.23829\n",
      "epoch no.4 train no.330460  loss = 2.85422 avg_loss = 3.22697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.330470  loss = 4.42598 avg_loss = 3.25403\n",
      "epoch no.4 train no.330480  loss = 4.70744 avg_loss = 3.29793\n",
      "epoch no.4 train no.330490  loss = 3.15227 avg_loss = 3.29946\n",
      "epoch no.4 train no.330500  loss = 4.25009 avg_loss = 3.26953\n",
      "epoch no.4 train no.330510  loss = 3.71427 avg_loss = 3.22379\n",
      "epoch no.4 train no.330520  loss = 3.33864 avg_loss = 3.24449\n",
      "epoch no.4 train no.330530  loss = 2.46483 avg_loss = 3.22751\n",
      "epoch no.4 train no.330540  loss = 3.29623 avg_loss = 3.21939\n",
      "epoch no.4 train no.330550  loss = 2.90668 avg_loss = 3.20258\n",
      "epoch no.4 train no.330560  loss = 3.01077 avg_loss = 3.19503\n",
      "epoch no.4 train no.330570  loss = 2.82287 avg_loss = 3.14018\n",
      "epoch no.4 train no.330580  loss = 1.77361 avg_loss = 3.10650\n",
      "epoch no.4 train no.330590  loss = 2.04257 avg_loss = 3.06191\n",
      "epoch no.4 train no.330600  loss = 3.25108 avg_loss = 3.08551\n",
      "epoch no.4 train no.330610  loss = 3.15482 avg_loss = 3.07187\n",
      "epoch no.4 train no.330620  loss = 1.87291 avg_loss = 3.08337\n",
      "epoch no.4 train no.330630  loss = 3.70481 avg_loss = 3.12013\n",
      "epoch no.4 train no.330640  loss = 2.24265 avg_loss = 3.14795\n",
      "epoch no.4 train no.330650  loss = 4.13140 avg_loss = 3.16208\n",
      "epoch no.4 train no.330660  loss = 3.83510 avg_loss = 3.12378\n",
      "epoch no.4 train no.330670  loss = 1.41812 avg_loss = 3.11829\n",
      "epoch no.4 train no.330680  loss = 4.42794 avg_loss = 3.12208\n",
      "epoch no.4 train no.330690  loss = 2.27303 avg_loss = 3.10865\n",
      "epoch no.4 train no.330700  loss = 3.63190 avg_loss = 3.08890\n",
      "epoch no.4 train no.330710  loss = 1.91935 avg_loss = 3.08354\n",
      "epoch no.4 train no.330720  loss = 2.61547 avg_loss = 3.11036\n",
      "epoch no.4 train no.330730  loss = 3.09285 avg_loss = 3.15042\n",
      "epoch no.4 train no.330740  loss = 2.69188 avg_loss = 3.14397\n",
      "epoch no.4 train no.330750  loss = 2.46995 avg_loss = 3.12674\n",
      "epoch no.4 train no.330760  loss = 3.47687 avg_loss = 3.09120\n",
      "epoch no.4 train no.330770  loss = 1.85838 avg_loss = 3.06112\n",
      "epoch no.4 train no.330780  loss = 2.60353 avg_loss = 3.07549\n",
      "epoch no.4 train no.330790  loss = 2.89500 avg_loss = 3.09518\n",
      "epoch no.4 train no.330800  loss = 1.32427 avg_loss = 3.05979\n",
      "epoch no.4 train no.330810  loss = 2.93432 avg_loss = 3.02651\n",
      "epoch no.4 train no.330820  loss = 4.82737 avg_loss = 3.07702\n",
      "epoch no.4 train no.330830  loss = 3.49305 avg_loss = 3.07320\n",
      "epoch no.4 train no.330840  loss = 3.40692 avg_loss = 3.09257\n",
      "epoch no.4 train no.330850  loss = 2.46064 avg_loss = 3.03985\n",
      "epoch no.4 train no.330860  loss = 3.01584 avg_loss = 3.02176\n",
      "epoch no.4 train no.330870  loss = 2.09918 avg_loss = 3.05049\n",
      "epoch no.4 train no.330880  loss = 2.79764 avg_loss = 3.03031\n",
      "epoch no.4 train no.330890  loss = 2.86143 avg_loss = 3.02655\n",
      "epoch no.4 train no.330900  loss = 1.83054 avg_loss = 3.02104\n",
      "epoch no.4 train no.330910  loss = 3.00733 avg_loss = 3.02149\n",
      "epoch no.4 train no.330920  loss = 2.76749 avg_loss = 3.01647\n",
      "epoch no.4 train no.330930  loss = 3.87105 avg_loss = 3.05523\n",
      "epoch no.4 train no.330940  loss = 3.75053 avg_loss = 3.06864\n",
      "epoch no.4 train no.330950  loss = 5.05297 avg_loss = 3.13901\n",
      "epoch no.4 train no.330960  loss = 3.04245 avg_loss = 3.17210\n",
      "epoch no.4 train no.330970  loss = 2.55703 avg_loss = 3.16103\n",
      "epoch no.4 train no.330980  loss = 3.38091 avg_loss = 3.15226\n",
      "epoch no.4 train no.330990  loss = 2.56498 avg_loss = 3.14114\n",
      "epoch no.4 train no.331000  loss = 3.27650 avg_loss = 3.14354\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁노래', '힙', '</s>']\n",
      "여름밤에 어울리는 재즈음악</s>\n",
      "epoch no.4 train no.331010  loss = 3.24082 avg_loss = 3.15848\n",
      "epoch no.4 train no.331020  loss = 3.45359 avg_loss = 3.17881\n",
      "epoch no.4 train no.331030  loss = 2.62247 avg_loss = 3.15309\n",
      "epoch no.4 train no.331040  loss = 2.70523 avg_loss = 3.14748\n",
      "epoch no.4 train no.331050  loss = 1.59973 avg_loss = 3.13292\n",
      "epoch no.4 train no.331060  loss = 2.45104 avg_loss = 3.16805\n",
      "epoch no.4 train no.331070  loss = 3.90838 avg_loss = 3.16980\n",
      "epoch no.4 train no.331080  loss = 2.26970 avg_loss = 3.12501\n",
      "epoch no.4 train no.331090  loss = 2.17004 avg_loss = 3.16715\n",
      "epoch no.4 train no.331100  loss = 2.46298 avg_loss = 3.20632\n",
      "epoch no.4 train no.331110  loss = 2.11105 avg_loss = 3.25921\n",
      "epoch no.4 train no.331120  loss = 2.97829 avg_loss = 3.26349\n",
      "epoch no.4 train no.331130  loss = 3.67835 avg_loss = 3.26936\n",
      "epoch no.4 train no.331140  loss = 3.01971 avg_loss = 3.30373\n",
      "epoch no.4 train no.331150  loss = 4.48361 avg_loss = 3.30940\n",
      "epoch no.4 train no.331160  loss = 2.91244 avg_loss = 3.28030\n",
      "epoch no.4 train no.331170  loss = 3.11566 avg_loss = 3.23324\n",
      "epoch no.4 train no.331180  loss = 3.85199 avg_loss = 3.20131\n",
      "epoch no.4 train no.331190  loss = 3.71430 avg_loss = 3.25713\n",
      "epoch no.4 train no.331200  loss = 3.54288 avg_loss = 3.22525\n",
      "epoch no.4 train no.331210  loss = 2.35619 avg_loss = 3.18916\n",
      "epoch no.4 train no.331220  loss = 3.32948 avg_loss = 3.18556\n",
      "epoch no.4 train no.331230  loss = 2.74235 avg_loss = 3.17120\n",
      "epoch no.4 train no.331240  loss = 3.31287 avg_loss = 3.17587\n",
      "epoch no.4 train no.331250  loss = 2.73766 avg_loss = 3.13561\n",
      "epoch no.4 train no.331260  loss = 2.52576 avg_loss = 3.10928\n",
      "epoch no.4 train no.331270  loss = 4.40113 avg_loss = 3.11715\n",
      "epoch no.4 train no.331280  loss = 4.04852 avg_loss = 3.16107\n",
      "epoch no.4 train no.331290  loss = 5.10824 avg_loss = 3.17193\n",
      "epoch no.4 train no.331300  loss = 2.34413 avg_loss = 3.14842\n",
      "epoch no.4 train no.331310  loss = 3.62535 avg_loss = 3.15030\n",
      "epoch no.4 train no.331320  loss = 3.23573 avg_loss = 3.09882\n",
      "epoch no.4 train no.331330  loss = 2.70634 avg_loss = 3.13795\n",
      "epoch no.4 train no.331340  loss = 2.32031 avg_loss = 3.14520\n",
      "epoch no.4 train no.331350  loss = 2.91380 avg_loss = 3.10891\n",
      "epoch no.4 train no.331360  loss = 2.41380 avg_loss = 3.10074\n",
      "epoch no.4 train no.331370  loss = 4.75519 avg_loss = 3.10318\n",
      "epoch no.4 train no.331380  loss = 2.65919 avg_loss = 3.08000\n",
      "epoch no.4 train no.331390  loss = 2.54087 avg_loss = 3.04732\n",
      "epoch no.4 train no.331400  loss = 4.50843 avg_loss = 3.05061\n",
      "epoch no.4 train no.331410  loss = 3.47265 avg_loss = 3.07724\n",
      "epoch no.4 train no.331420  loss = 4.14873 avg_loss = 3.16403\n",
      "epoch no.4 train no.331430  loss = 5.16004 avg_loss = 3.17547\n",
      "epoch no.4 train no.331440  loss = 3.28200 avg_loss = 3.18533\n",
      "epoch no.4 train no.331450  loss = 4.93901 avg_loss = 3.16242\n",
      "epoch no.4 train no.331460  loss = 3.28657 avg_loss = 3.17465\n",
      "epoch no.4 train no.331470  loss = 3.69841 avg_loss = 3.15338\n",
      "epoch no.4 train no.331480  loss = 2.46024 avg_loss = 3.11835\n",
      "epoch no.4 train no.331490  loss = 3.73198 avg_loss = 3.09860\n",
      "epoch no.4 train no.331500  loss = 3.09971 avg_loss = 3.05008\n",
      "epoch no.4 train no.331510  loss = 1.72029 avg_loss = 3.01891\n",
      "epoch no.4 train no.331520  loss = 3.00917 avg_loss = 3.01413\n",
      "epoch no.4 train no.331530  loss = 2.12134 avg_loss = 2.96075\n",
      "epoch no.4 train no.331540  loss = 5.90930 avg_loss = 2.96209\n",
      "epoch no.4 train no.331550  loss = 2.22662 avg_loss = 2.95412\n",
      "epoch no.4 train no.331560  loss = 3.27536 avg_loss = 2.98909\n",
      "epoch no.4 train no.331570  loss = 2.61829 avg_loss = 2.97927\n",
      "epoch no.4 train no.331580  loss = 3.13261 avg_loss = 3.00835\n",
      "epoch no.4 train no.331590  loss = 3.02032 avg_loss = 3.06934\n",
      "epoch no.4 train no.331600  loss = 1.92049 avg_loss = 3.10047\n",
      "epoch no.4 train no.331610  loss = 4.30619 avg_loss = 3.14342\n",
      "epoch no.4 train no.331620  loss = 2.09475 avg_loss = 3.10411\n",
      "epoch no.4 train no.331630  loss = 4.64267 avg_loss = 3.14487\n",
      "epoch no.4 train no.331640  loss = 2.47385 avg_loss = 3.11634\n",
      "epoch no.4 train no.331650  loss = 5.11039 avg_loss = 3.16575\n",
      "epoch no.4 train no.331660  loss = 2.52960 avg_loss = 3.13192\n",
      "epoch no.4 train no.331670  loss = 3.98006 avg_loss = 3.14102\n",
      "epoch no.4 train no.331680  loss = 1.99270 avg_loss = 3.10618\n",
      "epoch no.4 train no.331690  loss = 1.88284 avg_loss = 3.15167\n",
      "epoch no.4 train no.331700  loss = 3.82356 avg_loss = 3.11729\n",
      "epoch no.4 train no.331710  loss = 2.27519 avg_loss = 3.12843\n",
      "epoch no.4 train no.331720  loss = 4.02310 avg_loss = 3.15682\n",
      "epoch no.4 train no.331730  loss = 2.49035 avg_loss = 3.12837\n",
      "epoch no.4 train no.331740  loss = 5.26809 avg_loss = 3.18585\n",
      "epoch no.4 train no.331750  loss = 2.62039 avg_loss = 3.13065\n",
      "epoch no.4 train no.331760  loss = 2.30300 avg_loss = 3.10160\n",
      "epoch no.4 train no.331770  loss = 2.95906 avg_loss = 3.11940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.331780  loss = 4.21111 avg_loss = 3.12423\n",
      "epoch no.4 train no.331790  loss = 2.84494 avg_loss = 3.17070\n",
      "epoch no.4 train no.331800  loss = 3.94693 avg_loss = 3.16806\n",
      "epoch no.4 train no.331810  loss = 3.37394 avg_loss = 3.21645\n",
      "epoch no.4 train no.331820  loss = 2.69972 avg_loss = 3.17765\n",
      "epoch no.4 train no.331830  loss = 4.42385 avg_loss = 3.19862\n",
      "epoch no.4 train no.331840  loss = 2.52960 avg_loss = 3.24921\n",
      "epoch no.4 train no.331850  loss = 4.00657 avg_loss = 3.25848\n",
      "epoch no.4 train no.331860  loss = 2.01238 avg_loss = 3.23360\n",
      "epoch no.4 train no.331870  loss = 2.96411 avg_loss = 3.19510\n",
      "epoch no.4 train no.331880  loss = 3.57772 avg_loss = 3.24592\n",
      "epoch no.4 train no.331890  loss = 3.09444 avg_loss = 3.20199\n",
      "epoch no.4 train no.331900  loss = 2.46221 avg_loss = 3.19223\n",
      "epoch no.4 train no.331910  loss = 3.64643 avg_loss = 3.20701\n",
      "epoch no.4 train no.331920  loss = 3.47801 avg_loss = 3.15167\n",
      "epoch no.4 train no.331930  loss = 2.92733 avg_loss = 3.09838\n",
      "epoch no.4 train no.331940  loss = 2.47546 avg_loss = 3.12088\n",
      "epoch no.4 train no.331950  loss = 3.25131 avg_loss = 3.09224\n",
      "epoch no.4 train no.331960  loss = 1.92665 avg_loss = 3.11344\n",
      "epoch no.4 train no.331970  loss = 3.73343 avg_loss = 3.12538\n",
      "epoch no.4 train no.331980  loss = 2.56329 avg_loss = 3.13596\n",
      "epoch no.4 train no.331990  loss = 3.53460 avg_loss = 3.15560\n",
      "epoch no.4 train no.332000  loss = 2.39951 avg_loss = 3.18286\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '▁시원하게', '▁만들어', '줄', '▁노래', '</s>']\n",
      "여름밤을 더욱 시원하게 만들어줄 노래</s>\n",
      "epoch no.4 train no.332010  loss = 2.26430 avg_loss = 3.17759\n",
      "epoch no.4 train no.332020  loss = 2.88141 avg_loss = 3.21310\n",
      "epoch no.4 train no.332030  loss = 3.44454 avg_loss = 3.23230\n",
      "epoch no.4 train no.332040  loss = 3.89979 avg_loss = 3.21849\n",
      "epoch no.4 train no.332050  loss = 1.65156 avg_loss = 3.18507\n",
      "epoch no.4 train no.332060  loss = 3.85795 avg_loss = 3.17766\n",
      "epoch no.4 train no.332070  loss = 3.26400 avg_loss = 3.17368\n",
      "epoch no.4 train no.332080  loss = 4.29708 avg_loss = 3.22555\n",
      "epoch no.4 train no.332090  loss = 2.73577 avg_loss = 3.21467\n",
      "epoch no.4 train no.332100  loss = 2.94143 avg_loss = 3.20159\n",
      "epoch no.4 train no.332110  loss = 3.09345 avg_loss = 3.19869\n",
      "epoch no.4 train no.332120  loss = 3.12402 avg_loss = 3.18276\n",
      "epoch no.4 train no.332130  loss = 3.96910 avg_loss = 3.16661\n",
      "epoch no.4 train no.332140  loss = 1.64694 avg_loss = 3.15709\n",
      "epoch no.4 train no.332150  loss = 3.01025 avg_loss = 3.16322\n",
      "epoch no.4 train no.332160  loss = 2.69122 avg_loss = 3.19324\n",
      "epoch no.4 train no.332170  loss = 4.59422 avg_loss = 3.17979\n",
      "epoch no.4 train no.332180  loss = 3.01211 avg_loss = 3.21785\n",
      "epoch no.4 train no.332190  loss = 2.85296 avg_loss = 3.23236\n",
      "epoch no.4 train no.332200  loss = 4.51710 avg_loss = 3.29492\n",
      "epoch no.4 train no.332210  loss = 3.00200 avg_loss = 3.31062\n",
      "epoch no.4 train no.332220  loss = 3.54127 avg_loss = 3.33199\n",
      "epoch no.4 train no.332230  loss = 2.90799 avg_loss = 3.29676\n",
      "epoch no.4 train no.332240  loss = 2.27786 avg_loss = 3.28173\n",
      "epoch no.4 train no.332250  loss = 2.59661 avg_loss = 3.30922\n",
      "epoch no.4 train no.332260  loss = 4.14093 avg_loss = 3.30887\n",
      "epoch no.4 train no.332270  loss = 3.83641 avg_loss = 3.30106\n",
      "epoch no.4 train no.332280  loss = 2.73922 avg_loss = 3.27622\n",
      "epoch no.4 train no.332290  loss = 3.52922 avg_loss = 3.27313\n",
      "epoch no.4 train no.332300  loss = 3.84355 avg_loss = 3.23043\n",
      "epoch no.4 train no.332310  loss = 2.68487 avg_loss = 3.17903\n",
      "epoch no.4 train no.332320  loss = 2.61092 avg_loss = 3.13748\n",
      "epoch no.4 train no.332330  loss = 3.39621 avg_loss = 3.15928\n",
      "epoch no.4 train no.332340  loss = 4.27834 avg_loss = 3.14015\n",
      "epoch no.4 train no.332350  loss = 2.89469 avg_loss = 3.15695\n",
      "epoch no.4 train no.332360  loss = 3.44683 avg_loss = 3.16593\n",
      "epoch no.4 train no.332370  loss = 3.02733 avg_loss = 3.15776\n",
      "epoch no.4 train no.332380  loss = 2.48673 avg_loss = 3.15178\n",
      "epoch no.4 train no.332390  loss = 2.09992 avg_loss = 3.16245\n",
      "epoch no.4 train no.332400  loss = 2.87792 avg_loss = 3.18793\n",
      "epoch no.4 train no.332410  loss = 3.62449 avg_loss = 3.13975\n",
      "epoch no.4 train no.332420  loss = 3.81869 avg_loss = 3.18022\n",
      "epoch no.4 train no.332430  loss = 2.41687 avg_loss = 3.17188\n",
      "epoch no.4 train no.332440  loss = 4.21444 avg_loss = 3.22293\n",
      "epoch no.4 train no.332450  loss = 2.03494 avg_loss = 3.22734\n",
      "epoch no.4 train no.332460  loss = 2.58267 avg_loss = 3.20540\n",
      "epoch no.4 train no.332470  loss = 3.01661 avg_loss = 3.20987\n",
      "epoch no.4 train no.332480  loss = 2.14310 avg_loss = 3.17246\n",
      "epoch no.4 train no.332490  loss = 2.80335 avg_loss = 3.17413\n",
      "epoch no.4 train no.332500  loss = 1.60564 avg_loss = 3.15196\n",
      "epoch no.4 train no.332510  loss = 3.31913 avg_loss = 3.21533\n",
      "epoch no.4 train no.332520  loss = 3.90464 avg_loss = 3.28741\n",
      "epoch no.4 train no.332530  loss = 3.87710 avg_loss = 3.26262\n",
      "epoch no.4 train no.332540  loss = 3.12178 avg_loss = 3.25480\n",
      "epoch no.4 train no.332550  loss = 2.90347 avg_loss = 3.23937\n",
      "epoch no.4 train no.332560  loss = 5.56284 avg_loss = 3.24728\n",
      "epoch no.4 train no.332570  loss = 2.74251 avg_loss = 3.20274\n",
      "epoch no.4 train no.332580  loss = 3.41272 avg_loss = 3.20319\n",
      "epoch no.4 train no.332590  loss = 3.50099 avg_loss = 3.18884\n",
      "epoch no.4 train no.332600  loss = 2.06777 avg_loss = 3.16751\n",
      "epoch no.4 train no.332610  loss = 3.94479 avg_loss = 3.20590\n",
      "epoch no.4 train no.332620  loss = 3.65158 avg_loss = 3.23945\n",
      "epoch no.4 train no.332630  loss = 4.73821 avg_loss = 3.29783\n",
      "epoch no.4 train no.332640  loss = 3.83054 avg_loss = 3.26218\n",
      "epoch no.4 train no.332650  loss = 3.91625 avg_loss = 3.26334\n",
      "epoch no.4 train no.332660  loss = 3.48981 avg_loss = 3.26324\n",
      "epoch no.4 train no.332670  loss = 2.74804 avg_loss = 3.23019\n",
      "epoch no.4 train no.332680  loss = 3.04194 avg_loss = 3.25622\n",
      "epoch no.4 train no.332690  loss = 2.79166 avg_loss = 3.21530\n",
      "epoch no.4 train no.332700  loss = 3.09983 avg_loss = 3.17411\n",
      "epoch no.4 train no.332710  loss = 3.78870 avg_loss = 3.18580\n",
      "epoch no.4 train no.332720  loss = 2.98876 avg_loss = 3.15712\n",
      "epoch no.4 train no.332730  loss = 2.19752 avg_loss = 3.13062\n",
      "epoch no.4 train no.332740  loss = 3.33972 avg_loss = 3.16838\n",
      "epoch no.4 train no.332750  loss = 4.23194 avg_loss = 3.17424\n",
      "epoch no.4 train no.332760  loss = 1.95100 avg_loss = 3.14282\n",
      "epoch no.4 train no.332770  loss = 3.28228 avg_loss = 3.13645\n",
      "epoch no.4 train no.332780  loss = 3.40790 avg_loss = 3.20892\n",
      "epoch no.4 train no.332790  loss = 3.31682 avg_loss = 3.20596\n",
      "epoch no.4 train no.332800  loss = 3.28813 avg_loss = 3.16126\n",
      "epoch no.4 train no.332810  loss = 3.84455 avg_loss = 3.14974\n",
      "epoch no.4 train no.332820  loss = 3.56457 avg_loss = 3.12484\n",
      "epoch no.4 train no.332830  loss = 4.58097 avg_loss = 3.12275\n",
      "epoch no.4 train no.332840  loss = 5.92090 avg_loss = 3.18000\n",
      "epoch no.4 train no.332850  loss = 2.26452 avg_loss = 3.14950\n",
      "epoch no.4 train no.332860  loss = 1.84375 avg_loss = 3.18398\n",
      "epoch no.4 train no.332870  loss = 3.63451 avg_loss = 3.22277\n",
      "epoch no.4 train no.332880  loss = 3.13491 avg_loss = 3.19829\n",
      "epoch no.4 train no.332890  loss = 4.40585 avg_loss = 3.26039\n",
      "epoch no.4 train no.332900  loss = 2.92353 avg_loss = 3.25277\n",
      "epoch no.4 train no.332910  loss = 2.97240 avg_loss = 3.27646\n",
      "epoch no.4 train no.332920  loss = 2.16042 avg_loss = 3.24551\n",
      "epoch no.4 train no.332930  loss = 2.79091 avg_loss = 3.27855\n",
      "epoch no.4 train no.332940  loss = 4.87000 avg_loss = 3.29630\n",
      "epoch no.4 train no.332950  loss = 2.35848 avg_loss = 3.26612\n",
      "epoch no.4 train no.332960  loss = 1.93748 avg_loss = 3.23570\n",
      "epoch no.4 train no.332970  loss = 3.96266 avg_loss = 3.24476\n",
      "epoch no.4 train no.332980  loss = 3.51033 avg_loss = 3.22223\n",
      "epoch no.4 train no.332990  loss = 5.25607 avg_loss = 3.26203\n",
      "epoch no.4 train no.333000  loss = 3.19259 avg_loss = 3.24301\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름 밤 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.333010  loss = 3.31646 avg_loss = 3.24808\n",
      "epoch no.4 train no.333020  loss = 4.41818 avg_loss = 3.21750\n",
      "epoch no.4 train no.333030  loss = 2.68002 avg_loss = 3.22902\n",
      "epoch no.4 train no.333040  loss = 6.45245 avg_loss = 3.28587\n",
      "epoch no.4 train no.333050  loss = 1.90540 avg_loss = 3.23807\n",
      "epoch no.4 train no.333060  loss = 3.67494 avg_loss = 3.24678\n",
      "epoch no.4 train no.333070  loss = 2.56452 avg_loss = 3.20247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.333080  loss = 2.34454 avg_loss = 3.24253\n",
      "epoch no.4 train no.333090  loss = 2.16317 avg_loss = 3.23850\n",
      "epoch no.4 train no.333100  loss = 2.49456 avg_loss = 3.26486\n",
      "epoch no.4 train no.333110  loss = 4.44393 avg_loss = 3.23785\n",
      "epoch no.4 train no.333120  loss = 2.76251 avg_loss = 3.24785\n",
      "epoch no.4 train no.333130  loss = 3.31817 avg_loss = 3.23821\n",
      "epoch no.4 train no.333140  loss = 4.16613 avg_loss = 3.22532\n",
      "epoch no.4 train no.333150  loss = 3.10186 avg_loss = 3.24485\n",
      "epoch no.4 train no.333160  loss = 3.65262 avg_loss = 3.22563\n",
      "epoch no.4 train no.333170  loss = 3.99417 avg_loss = 3.19270\n",
      "epoch no.4 train no.333180  loss = 4.08983 avg_loss = 3.18656\n",
      "epoch no.4 train no.333190  loss = 3.66267 avg_loss = 3.20298\n",
      "epoch no.4 train no.333200  loss = 2.34545 avg_loss = 3.19355\n",
      "epoch no.4 train no.333210  loss = 2.50666 avg_loss = 3.17009\n",
      "epoch no.4 train no.333220  loss = 3.43764 avg_loss = 3.14100\n",
      "epoch no.4 train no.333230  loss = 2.92899 avg_loss = 3.18570\n",
      "epoch no.4 train no.333240  loss = 5.43158 avg_loss = 3.24873\n",
      "epoch no.4 train no.333250  loss = 4.09405 avg_loss = 3.23651\n",
      "epoch no.4 train no.333260  loss = 2.75260 avg_loss = 3.25452\n",
      "epoch no.4 train no.333270  loss = 3.92567 avg_loss = 3.27332\n",
      "epoch no.4 train no.333280  loss = 2.81211 avg_loss = 3.23336\n",
      "epoch no.4 train no.333290  loss = 1.65898 avg_loss = 3.18913\n",
      "epoch no.4 train no.333300  loss = 2.52522 avg_loss = 3.14969\n",
      "epoch no.4 train no.333310  loss = 3.80549 avg_loss = 3.15425\n",
      "epoch no.4 train no.333320  loss = 1.52730 avg_loss = 3.16901\n",
      "epoch no.4 train no.333330  loss = 3.28044 avg_loss = 3.17479\n",
      "epoch no.4 train no.333340  loss = 3.00763 avg_loss = 3.19007\n",
      "epoch no.4 train no.333350  loss = 2.24299 avg_loss = 3.13241\n",
      "epoch no.4 train no.333360  loss = 2.47662 avg_loss = 3.14097\n",
      "epoch no.4 train no.333370  loss = 2.08556 avg_loss = 3.13547\n",
      "epoch no.4 train no.333380  loss = 2.22832 avg_loss = 3.13585\n",
      "epoch no.4 train no.333390  loss = 1.95760 avg_loss = 3.09069\n",
      "epoch no.4 train no.333400  loss = 1.65045 avg_loss = 3.08748\n",
      "epoch no.4 train no.333410  loss = 2.90821 avg_loss = 3.06535\n",
      "epoch no.4 train no.333420  loss = 3.01079 avg_loss = 3.07361\n",
      "epoch no.4 train no.333430  loss = 2.26497 avg_loss = 3.10651\n",
      "epoch no.4 train no.333440  loss = 3.01165 avg_loss = 3.07602\n",
      "epoch no.4 train no.333450  loss = 3.01437 avg_loss = 3.05677\n",
      "epoch no.4 train no.333460  loss = 3.02902 avg_loss = 3.05883\n",
      "epoch no.4 train no.333470  loss = 2.66571 avg_loss = 3.06679\n",
      "epoch no.4 train no.333480  loss = 3.18366 avg_loss = 3.01985\n",
      "epoch no.4 train no.333490  loss = 2.68813 avg_loss = 3.01174\n",
      "epoch no.4 train no.333500  loss = 3.41783 avg_loss = 2.99613\n",
      "epoch no.4 train no.333510  loss = 3.44266 avg_loss = 3.02508\n",
      "epoch no.4 train no.333520  loss = 3.14323 avg_loss = 3.00807\n",
      "epoch no.4 train no.333530  loss = 2.83048 avg_loss = 2.97327\n",
      "epoch no.4 train no.333540  loss = 3.01771 avg_loss = 2.99326\n",
      "epoch no.4 train no.333550  loss = 2.73474 avg_loss = 2.99465\n",
      "epoch no.4 train no.333560  loss = 2.26791 avg_loss = 2.99546\n",
      "epoch no.4 train no.333570  loss = 2.78161 avg_loss = 2.99586\n",
      "epoch no.4 train no.333580  loss = 4.31269 avg_loss = 3.00596\n",
      "epoch no.4 train no.333590  loss = 3.60135 avg_loss = 3.05056\n",
      "epoch no.4 train no.333600  loss = 1.85518 avg_loss = 3.08808\n",
      "epoch no.4 train no.333610  loss = 2.09404 avg_loss = 3.13325\n",
      "epoch no.4 train no.333620  loss = 2.49333 avg_loss = 3.14027\n",
      "epoch no.4 train no.333630  loss = 2.42365 avg_loss = 3.13472\n",
      "epoch no.4 train no.333640  loss = 3.21996 avg_loss = 3.13806\n",
      "epoch no.4 train no.333650  loss = 3.14734 avg_loss = 3.14847\n",
      "epoch no.4 train no.333660  loss = 4.40895 avg_loss = 3.15741\n",
      "epoch no.4 train no.333670  loss = 2.10570 avg_loss = 3.17406\n",
      "epoch no.4 train no.333680  loss = 3.83979 avg_loss = 3.18351\n",
      "epoch no.4 train no.333690  loss = 3.57656 avg_loss = 3.15629\n",
      "epoch no.4 train no.333700  loss = 3.80687 avg_loss = 3.19952\n",
      "epoch no.4 train no.333710  loss = 3.31224 avg_loss = 3.21399\n",
      "epoch no.4 train no.333720  loss = 3.86082 avg_loss = 3.17632\n",
      "epoch no.4 train no.333730  loss = 2.14586 avg_loss = 3.19785\n",
      "epoch no.4 train no.333740  loss = 3.70497 avg_loss = 3.18677\n",
      "epoch no.4 train no.333750  loss = 1.78891 avg_loss = 3.17381\n",
      "epoch no.4 train no.333760  loss = 1.01522 avg_loss = 3.17900\n",
      "epoch no.4 train no.333770  loss = 3.09622 avg_loss = 3.19543\n",
      "epoch no.4 train no.333780  loss = 1.68197 avg_loss = 3.16058\n",
      "epoch no.4 train no.333790  loss = 2.14416 avg_loss = 3.19489\n",
      "epoch no.4 train no.333800  loss = 1.55505 avg_loss = 3.15285\n",
      "epoch no.4 train no.333810  loss = 2.66793 avg_loss = 3.24580\n",
      "epoch no.4 train no.333820  loss = 2.72690 avg_loss = 3.22341\n",
      "epoch no.4 train no.333830  loss = 5.29293 avg_loss = 3.23256\n",
      "epoch no.4 train no.333840  loss = 2.97557 avg_loss = 3.20867\n",
      "epoch no.4 train no.333850  loss = 2.75621 avg_loss = 3.22097\n",
      "epoch no.4 train no.333860  loss = 4.60397 avg_loss = 3.27724\n",
      "epoch no.4 train no.333870  loss = 3.09239 avg_loss = 3.28698\n",
      "epoch no.4 train no.333880  loss = 1.88940 avg_loss = 3.29082\n",
      "epoch no.4 train no.333890  loss = 3.22208 avg_loss = 3.27394\n",
      "epoch no.4 train no.333900  loss = 3.27056 avg_loss = 3.29913\n",
      "epoch no.4 train no.333910  loss = 3.48967 avg_loss = 3.27171\n",
      "epoch no.4 train no.333920  loss = 3.34643 avg_loss = 3.25823\n",
      "epoch no.4 train no.333930  loss = 4.49165 avg_loss = 3.27933\n",
      "epoch no.4 train no.333940  loss = 2.06842 avg_loss = 3.24415\n",
      "epoch no.4 train no.333950  loss = 4.74234 avg_loss = 3.22154\n",
      "epoch no.4 train no.333960  loss = 2.50790 avg_loss = 3.23859\n",
      "epoch no.4 train no.333970  loss = 2.66850 avg_loss = 3.26127\n",
      "epoch no.4 train no.333980  loss = 3.41260 avg_loss = 3.28185\n",
      "epoch no.4 train no.333990  loss = 2.55567 avg_loss = 3.26912\n",
      "epoch no.4 train no.334000  loss = 2.29407 avg_loss = 3.21576\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓을', '▁감성', '달', '한', '▁노래', '</s>']\n",
      "여름밤을 수놓을 달달한 노래</s>\n",
      "epoch no.4 train no.334010  loss = 2.51838 avg_loss = 3.23124\n",
      "epoch no.4 train no.334020  loss = 2.52412 avg_loss = 3.21004\n",
      "epoch no.4 train no.334030  loss = 2.71287 avg_loss = 3.17448\n",
      "epoch no.4 train no.334040  loss = 2.48754 avg_loss = 3.15776\n",
      "epoch no.4 train no.334050  loss = 2.95260 avg_loss = 3.20309\n",
      "epoch no.4 train no.334060  loss = 4.01399 avg_loss = 3.16741\n",
      "epoch no.4 train no.334070  loss = 2.29470 avg_loss = 3.16269\n",
      "epoch no.4 train no.334080  loss = 3.04190 avg_loss = 3.18508\n",
      "epoch no.4 train no.334090  loss = 1.29314 avg_loss = 3.16555\n",
      "epoch no.4 train no.334100  loss = 3.46860 avg_loss = 3.16692\n",
      "epoch no.4 train no.334110  loss = 4.19017 avg_loss = 3.19303\n",
      "epoch no.4 train no.334120  loss = 4.13505 avg_loss = 3.20030\n",
      "epoch no.4 train no.334130  loss = 4.74379 avg_loss = 3.21053\n",
      "epoch no.4 train no.334140  loss = 2.24133 avg_loss = 3.21946\n",
      "epoch no.4 train no.334150  loss = 3.11745 avg_loss = 3.25912\n",
      "epoch no.4 train no.334160  loss = 3.50374 avg_loss = 3.22602\n",
      "epoch no.4 train no.334170  loss = 3.66443 avg_loss = 3.17747\n",
      "epoch no.4 train no.334180  loss = 2.56379 avg_loss = 3.15975\n",
      "epoch no.4 train no.334190  loss = 1.54956 avg_loss = 3.10836\n",
      "epoch no.4 train no.334200  loss = 4.12190 avg_loss = 3.14955\n",
      "epoch no.4 train no.334210  loss = 2.73777 avg_loss = 3.15198\n",
      "epoch no.4 train no.334220  loss = 2.30416 avg_loss = 3.11024\n",
      "epoch no.4 train no.334230  loss = 3.53794 avg_loss = 3.15025\n",
      "epoch no.4 train no.334240  loss = 1.19307 avg_loss = 3.10662\n",
      "epoch no.4 train no.334250  loss = 2.60530 avg_loss = 3.11954\n",
      "epoch no.4 train no.334260  loss = 4.07146 avg_loss = 3.14033\n",
      "epoch no.4 train no.334270  loss = 3.51112 avg_loss = 3.15699\n",
      "epoch no.4 train no.334280  loss = 1.83563 avg_loss = 3.11928\n",
      "epoch no.4 train no.334290  loss = 3.34080 avg_loss = 3.14163\n",
      "epoch no.4 train no.334300  loss = 1.99249 avg_loss = 3.11060\n",
      "epoch no.4 train no.334310  loss = 2.86151 avg_loss = 3.09140\n",
      "epoch no.4 train no.334320  loss = 2.46954 avg_loss = 3.11011\n",
      "epoch no.4 train no.334330  loss = 3.36806 avg_loss = 3.08067\n",
      "epoch no.4 train no.334340  loss = 3.84839 avg_loss = 3.08606\n",
      "epoch no.4 train no.334350  loss = 3.00409 avg_loss = 3.13061\n",
      "epoch no.4 train no.334360  loss = 3.22430 avg_loss = 3.16353\n",
      "epoch no.4 train no.334370  loss = 2.00433 avg_loss = 3.18876\n",
      "epoch no.4 train no.334380  loss = 2.33255 avg_loss = 3.22412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.334390  loss = 3.87632 avg_loss = 3.22172\n",
      "epoch no.4 train no.334400  loss = 2.84384 avg_loss = 3.27310\n",
      "epoch no.4 train no.334410  loss = 2.85631 avg_loss = 3.25017\n",
      "epoch no.4 train no.334420  loss = 2.92625 avg_loss = 3.24593\n",
      "epoch no.4 train no.334430  loss = 4.01461 avg_loss = 3.23944\n",
      "epoch no.4 train no.334440  loss = 2.38379 avg_loss = 3.24198\n",
      "epoch no.4 train no.334450  loss = 2.66377 avg_loss = 3.20640\n",
      "epoch no.4 train no.334460  loss = 2.87735 avg_loss = 3.20521\n",
      "epoch no.4 train no.334470  loss = 3.22131 avg_loss = 3.21641\n",
      "epoch no.4 train no.334480  loss = 2.71824 avg_loss = 3.21585\n",
      "epoch no.4 train no.334490  loss = 4.03376 avg_loss = 3.18305\n",
      "epoch no.4 train no.334500  loss = 2.54084 avg_loss = 3.14843\n",
      "epoch no.4 train no.334510  loss = 3.02131 avg_loss = 3.16792\n",
      "epoch no.4 train no.334520  loss = 4.38111 avg_loss = 3.17680\n",
      "epoch no.4 train no.334530  loss = 4.44262 avg_loss = 3.22847\n",
      "epoch no.4 train no.334540  loss = 3.04643 avg_loss = 3.22054\n",
      "epoch no.4 train no.334550  loss = 2.97351 avg_loss = 3.21293\n",
      "epoch no.4 train no.334560  loss = 3.54360 avg_loss = 3.20364\n",
      "epoch no.4 train no.334570  loss = 4.05232 avg_loss = 3.18303\n",
      "epoch no.4 train no.334580  loss = 2.86405 avg_loss = 3.16877\n",
      "epoch no.4 train no.334590  loss = 3.73058 avg_loss = 3.16803\n",
      "epoch no.4 train no.334600  loss = 2.92644 avg_loss = 3.14248\n",
      "epoch no.4 train no.334610  loss = 2.64331 avg_loss = 3.11479\n",
      "epoch no.4 train no.334620  loss = 3.05242 avg_loss = 3.06512\n",
      "epoch no.4 train no.334630  loss = 3.21467 avg_loss = 3.07944\n",
      "epoch no.4 train no.334640  loss = 3.46602 avg_loss = 3.09627\n",
      "epoch no.4 train no.334650  loss = 4.38121 avg_loss = 3.10329\n",
      "epoch no.4 train no.334660  loss = 4.02926 avg_loss = 3.13412\n",
      "epoch no.4 train no.334670  loss = 2.75116 avg_loss = 3.12191\n",
      "epoch no.4 train no.334680  loss = 2.13616 avg_loss = 3.12592\n",
      "epoch no.4 train no.334690  loss = 2.45442 avg_loss = 3.15731\n",
      "epoch no.4 train no.334700  loss = 3.86850 avg_loss = 3.16287\n",
      "epoch no.4 train no.334710  loss = 3.88521 avg_loss = 3.17845\n",
      "epoch no.4 train no.334720  loss = 3.27923 avg_loss = 3.10513\n",
      "epoch no.4 train no.334730  loss = 3.20026 avg_loss = 3.12466\n",
      "epoch no.4 train no.334740  loss = 3.25209 avg_loss = 3.15446\n",
      "epoch no.4 train no.334750  loss = 4.14592 avg_loss = 3.15164\n",
      "epoch no.4 train no.334760  loss = 2.85991 avg_loss = 3.12206\n",
      "epoch no.4 train no.334770  loss = 3.64738 avg_loss = 3.09229\n",
      "epoch no.4 train no.334780  loss = 3.39080 avg_loss = 3.10259\n",
      "epoch no.4 train no.334790  loss = 2.81286 avg_loss = 3.09637\n",
      "epoch no.4 train no.334800  loss = 2.88357 avg_loss = 3.09275\n",
      "epoch no.4 train no.334810  loss = 3.91073 avg_loss = 3.09983\n",
      "epoch no.4 train no.334820  loss = 4.56603 avg_loss = 3.12189\n",
      "epoch no.4 train no.334830  loss = 4.34498 avg_loss = 3.15484\n",
      "epoch no.4 train no.334840  loss = 2.73319 avg_loss = 3.19951\n",
      "epoch no.4 train no.334850  loss = 2.64611 avg_loss = 3.16193\n",
      "epoch no.4 train no.334860  loss = 2.34033 avg_loss = 3.13380\n",
      "epoch no.4 train no.334870  loss = 1.89849 avg_loss = 3.12119\n",
      "epoch no.4 train no.334880  loss = 2.64689 avg_loss = 3.08529\n",
      "epoch no.4 train no.334890  loss = 3.39899 avg_loss = 3.08725\n",
      "epoch no.4 train no.334900  loss = 3.00725 avg_loss = 3.06355\n",
      "epoch no.4 train no.334910  loss = 4.39710 avg_loss = 3.06004\n",
      "epoch no.4 train no.334920  loss = 4.36794 avg_loss = 3.09354\n",
      "epoch no.4 train no.334930  loss = 2.46145 avg_loss = 3.12865\n",
      "epoch no.4 train no.334940  loss = 2.25369 avg_loss = 3.17324\n",
      "epoch no.4 train no.334950  loss = 3.66159 avg_loss = 3.20605\n",
      "epoch no.4 train no.334960  loss = 2.11290 avg_loss = 3.13919\n",
      "epoch no.4 train no.334970  loss = 3.80394 avg_loss = 3.18963\n",
      "epoch no.4 train no.334980  loss = 3.33028 avg_loss = 3.19959\n",
      "epoch no.4 train no.334990  loss = 2.86586 avg_loss = 3.17859\n",
      "epoch no.4 train no.335000  loss = 3.18769 avg_loss = 3.18267\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '음악', '</s>']\n",
      "여름밤에 듣는 감성곡</s>\n",
      "epoch no.4 train no.335010  loss = 2.54419 avg_loss = 3.18493\n",
      "epoch no.4 train no.335020  loss = 3.40346 avg_loss = 3.20323\n",
      "epoch no.4 train no.335030  loss = 3.09741 avg_loss = 3.19748\n",
      "epoch no.4 train no.335040  loss = 3.16065 avg_loss = 3.16886\n",
      "epoch no.4 train no.335050  loss = 2.92695 avg_loss = 3.13930\n",
      "epoch no.4 train no.335060  loss = 2.74754 avg_loss = 3.15213\n",
      "epoch no.4 train no.335070  loss = 3.12799 avg_loss = 3.16208\n",
      "epoch no.4 train no.335080  loss = 3.57324 avg_loss = 3.17804\n",
      "epoch no.4 train no.335090  loss = 2.91358 avg_loss = 3.18727\n",
      "epoch no.4 train no.335100  loss = 3.89740 avg_loss = 3.22186\n",
      "epoch no.4 train no.335110  loss = 3.89773 avg_loss = 3.17122\n",
      "epoch no.4 train no.335120  loss = 3.19573 avg_loss = 3.14716\n",
      "epoch no.4 train no.335130  loss = 3.03781 avg_loss = 3.16642\n",
      "epoch no.4 train no.335140  loss = 4.31899 avg_loss = 3.17273\n",
      "epoch no.4 train no.335150  loss = 2.03644 avg_loss = 3.18503\n",
      "epoch no.4 train no.335160  loss = 3.34653 avg_loss = 3.16996\n",
      "epoch no.4 train no.335170  loss = 4.04343 avg_loss = 3.20110\n",
      "epoch no.4 train no.335180  loss = 3.28794 avg_loss = 3.20175\n",
      "epoch no.4 train no.335190  loss = 2.42133 avg_loss = 3.22020\n",
      "epoch no.4 train no.335200  loss = 3.83268 avg_loss = 3.19784\n",
      "epoch no.4 train no.335210  loss = 7.15500 avg_loss = 3.27444\n",
      "epoch no.4 train no.335220  loss = 2.87501 avg_loss = 3.23884\n",
      "epoch no.4 train no.335230  loss = 3.47430 avg_loss = 3.23603\n",
      "epoch no.4 train no.335240  loss = 3.67568 avg_loss = 3.24432\n",
      "epoch no.4 train no.335250  loss = 3.17677 avg_loss = 3.22341\n",
      "epoch no.4 train no.335260  loss = 3.16060 avg_loss = 3.23759\n",
      "epoch no.4 train no.335270  loss = 4.22320 avg_loss = 3.23687\n",
      "epoch no.4 train no.335280  loss = 2.24002 avg_loss = 3.20125\n",
      "epoch no.4 train no.335290  loss = 2.75469 avg_loss = 3.19219\n",
      "epoch no.4 train no.335300  loss = 2.29708 avg_loss = 3.19987\n",
      "epoch no.4 train no.335310  loss = 2.74751 avg_loss = 3.17760\n",
      "epoch no.4 train no.335320  loss = 1.61008 avg_loss = 3.17710\n",
      "epoch no.4 train no.335330  loss = 2.86327 avg_loss = 3.17959\n",
      "epoch no.4 train no.335340  loss = 2.97134 avg_loss = 3.17931\n",
      "epoch no.4 train no.335350  loss = 3.63051 avg_loss = 3.18074\n",
      "epoch no.4 train no.335360  loss = 3.51019 avg_loss = 3.16264\n",
      "epoch no.4 train no.335370  loss = 3.56273 avg_loss = 3.17526\n",
      "epoch no.4 train no.335380  loss = 3.87390 avg_loss = 3.17300\n",
      "epoch no.4 train no.335390  loss = 3.10609 avg_loss = 3.21336\n",
      "epoch no.4 train no.335400  loss = 3.06257 avg_loss = 3.21462\n",
      "epoch no.4 train no.335410  loss = 3.39959 avg_loss = 3.20741\n",
      "epoch no.4 train no.335420  loss = 3.42873 avg_loss = 3.20957\n",
      "epoch no.4 train no.335430  loss = 3.73086 avg_loss = 3.21763\n",
      "epoch no.4 train no.335440  loss = 2.51307 avg_loss = 3.22210\n",
      "epoch no.4 train no.335450  loss = 3.58739 avg_loss = 3.22061\n",
      "epoch no.4 train no.335460  loss = 2.73407 avg_loss = 3.19206\n",
      "epoch no.4 train no.335470  loss = 4.03582 avg_loss = 3.22844\n",
      "epoch no.4 train no.335480  loss = 3.00207 avg_loss = 3.17975\n",
      "epoch no.4 train no.335490  loss = 2.55833 avg_loss = 3.20378\n",
      "epoch no.4 train no.335500  loss = 3.19027 avg_loss = 3.21006\n",
      "epoch no.4 train no.335510  loss = 5.05343 avg_loss = 3.21250\n",
      "epoch no.4 train no.335520  loss = 2.67692 avg_loss = 3.20131\n",
      "epoch no.4 train no.335530  loss = 3.95331 avg_loss = 3.22249\n",
      "epoch no.4 train no.335540  loss = 3.62527 avg_loss = 3.23437\n",
      "epoch no.4 train no.335550  loss = 3.95038 avg_loss = 3.23110\n",
      "epoch no.4 train no.335560  loss = 4.33193 avg_loss = 3.25715\n",
      "epoch no.4 train no.335570  loss = 4.93348 avg_loss = 3.27524\n",
      "epoch no.4 train no.335580  loss = 3.59541 avg_loss = 3.27269\n",
      "epoch no.4 train no.335590  loss = 3.15984 avg_loss = 3.29288\n",
      "epoch no.4 train no.335600  loss = 1.70569 avg_loss = 3.27983\n",
      "epoch no.4 train no.335610  loss = 3.41727 avg_loss = 3.26137\n",
      "epoch no.4 train no.335620  loss = 4.73699 avg_loss = 3.27231\n",
      "epoch no.4 train no.335630  loss = 1.93337 avg_loss = 3.25201\n",
      "epoch no.4 train no.335640  loss = 4.82555 avg_loss = 3.26876\n",
      "epoch no.4 train no.335650  loss = 2.84592 avg_loss = 3.24929\n",
      "epoch no.4 train no.335660  loss = 2.39992 avg_loss = 3.24452\n",
      "epoch no.4 train no.335670  loss = 3.29258 avg_loss = 3.22899\n",
      "epoch no.4 train no.335680  loss = 4.31754 avg_loss = 3.23065\n",
      "epoch no.4 train no.335690  loss = 1.85002 avg_loss = 3.18553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.335700  loss = 3.95774 avg_loss = 3.16866\n",
      "epoch no.4 train no.335710  loss = 2.34783 avg_loss = 3.19435\n",
      "epoch no.4 train no.335720  loss = 2.29131 avg_loss = 3.17470\n",
      "epoch no.4 train no.335730  loss = 3.58793 avg_loss = 3.13995\n",
      "epoch no.4 train no.335740  loss = 4.47245 avg_loss = 3.16956\n",
      "epoch no.4 train no.335750  loss = 4.53458 avg_loss = 3.22112\n",
      "epoch no.4 train no.335760  loss = 0.76938 avg_loss = 3.20668\n",
      "epoch no.4 train no.335770  loss = 2.94121 avg_loss = 3.20709\n",
      "epoch no.4 train no.335780  loss = 2.59058 avg_loss = 3.18281\n",
      "epoch no.4 train no.335790  loss = 2.96916 avg_loss = 3.15001\n",
      "epoch no.4 train no.335800  loss = 1.76414 avg_loss = 3.18810\n",
      "epoch no.4 train no.335810  loss = 1.83296 avg_loss = 3.26276\n",
      "epoch no.4 train no.335820  loss = 3.87552 avg_loss = 3.24259\n",
      "epoch no.4 train no.335830  loss = 3.28747 avg_loss = 3.23766\n",
      "epoch no.4 train no.335840  loss = 2.92919 avg_loss = 3.21497\n",
      "epoch no.4 train no.335850  loss = 3.48978 avg_loss = 3.22344\n",
      "epoch no.4 train no.335860  loss = 1.97086 avg_loss = 3.21268\n",
      "epoch no.4 train no.335870  loss = 3.97614 avg_loss = 3.18119\n",
      "epoch no.4 train no.335880  loss = 2.69940 avg_loss = 3.16622\n",
      "epoch no.4 train no.335890  loss = 2.71668 avg_loss = 3.12596\n",
      "epoch no.4 train no.335900  loss = 3.73510 avg_loss = 3.13933\n",
      "epoch no.4 train no.335910  loss = 3.77345 avg_loss = 3.12672\n",
      "epoch no.4 train no.335920  loss = 2.99758 avg_loss = 3.10641\n",
      "epoch no.4 train no.335930  loss = 2.50013 avg_loss = 3.09162\n",
      "epoch no.4 train no.335940  loss = 3.30024 avg_loss = 3.09200\n",
      "epoch no.4 train no.335950  loss = 3.55626 avg_loss = 3.10560\n",
      "epoch no.4 train no.335960  loss = 3.13887 avg_loss = 3.09976\n",
      "epoch no.4 train no.335970  loss = 2.70836 avg_loss = 3.08165\n",
      "epoch no.4 train no.335980  loss = 2.24557 avg_loss = 3.11876\n",
      "epoch no.4 train no.335990  loss = 3.66604 avg_loss = 3.16790\n",
      "epoch no.4 train no.336000  loss = 2.83002 avg_loss = 3.16106\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.336010  loss = 2.88779 avg_loss = 3.15998\n",
      "epoch no.4 train no.336020  loss = 3.94789 avg_loss = 3.23989\n",
      "epoch no.4 train no.336030  loss = 3.69544 avg_loss = 3.25342\n",
      "epoch no.4 train no.336040  loss = 3.47890 avg_loss = 3.24828\n",
      "epoch no.4 train no.336050  loss = 3.66258 avg_loss = 3.24548\n",
      "epoch no.4 train no.336060  loss = 2.68170 avg_loss = 3.19487\n",
      "epoch no.4 train no.336070  loss = 2.44175 avg_loss = 3.20053\n",
      "epoch no.4 train no.336080  loss = 2.57416 avg_loss = 3.22649\n",
      "epoch no.4 train no.336090  loss = 2.53635 avg_loss = 3.23593\n",
      "epoch no.4 train no.336100  loss = 2.88933 avg_loss = 3.22843\n",
      "epoch no.4 train no.336110  loss = 2.37614 avg_loss = 3.23604\n",
      "epoch no.4 train no.336120  loss = 2.38797 avg_loss = 3.27318\n",
      "epoch no.4 train no.336130  loss = 4.13783 avg_loss = 3.26395\n",
      "epoch no.4 train no.336140  loss = 5.67238 avg_loss = 3.30553\n",
      "epoch no.4 train no.336150  loss = 3.68860 avg_loss = 3.24637\n",
      "epoch no.4 train no.336160  loss = 3.95029 avg_loss = 3.25319\n",
      "epoch no.4 train no.336170  loss = 2.71916 avg_loss = 3.27350\n",
      "epoch no.4 train no.336180  loss = 2.32328 avg_loss = 3.27746\n",
      "epoch no.4 train no.336190  loss = 2.59012 avg_loss = 3.27844\n",
      "epoch no.4 train no.336200  loss = 3.66995 avg_loss = 3.25989\n",
      "epoch no.4 train no.336210  loss = 3.05137 avg_loss = 3.26111\n",
      "epoch no.4 train no.336220  loss = 2.52629 avg_loss = 3.27472\n",
      "epoch no.4 train no.336230  loss = 2.74078 avg_loss = 3.20531\n",
      "epoch no.4 train no.336240  loss = 2.86759 avg_loss = 3.21213\n",
      "epoch no.4 train no.336250  loss = 4.16560 avg_loss = 3.23938\n",
      "epoch no.4 train no.336260  loss = 3.21770 avg_loss = 3.26529\n",
      "epoch no.4 train no.336270  loss = 3.49227 avg_loss = 3.32587\n",
      "epoch no.4 train no.336280  loss = 4.63952 avg_loss = 3.32284\n",
      "epoch no.4 train no.336290  loss = 2.72749 avg_loss = 3.30885\n",
      "epoch no.4 train no.336300  loss = 2.96677 avg_loss = 3.30393\n",
      "epoch no.4 train no.336310  loss = 2.42861 avg_loss = 3.22514\n",
      "epoch no.4 train no.336320  loss = 3.97714 avg_loss = 3.26684\n",
      "epoch no.4 train no.336330  loss = 2.81435 avg_loss = 3.29780\n",
      "epoch no.4 train no.336340  loss = 2.72347 avg_loss = 3.29081\n",
      "epoch no.4 train no.336350  loss = 2.71052 avg_loss = 3.26522\n",
      "epoch no.4 train no.336360  loss = 2.64011 avg_loss = 3.28840\n",
      "epoch no.4 train no.336370  loss = 3.57849 avg_loss = 3.30554\n",
      "epoch no.4 train no.336380  loss = 3.39128 avg_loss = 3.32648\n",
      "epoch no.4 train no.336390  loss = 3.68693 avg_loss = 3.30618\n",
      "epoch no.4 train no.336400  loss = 3.12975 avg_loss = 3.27925\n",
      "epoch no.4 train no.336410  loss = 3.56673 avg_loss = 3.26507\n",
      "epoch no.4 train no.336420  loss = 3.58642 avg_loss = 3.31557\n",
      "epoch no.4 train no.336430  loss = 2.40160 avg_loss = 3.27756\n",
      "epoch no.4 train no.336440  loss = 2.82344 avg_loss = 3.27084\n",
      "epoch no.4 train no.336450  loss = 2.03155 avg_loss = 3.24825\n",
      "epoch no.4 train no.336460  loss = 5.14536 avg_loss = 3.22279\n",
      "epoch no.4 train no.336470  loss = 2.89398 avg_loss = 3.20209\n",
      "epoch no.4 train no.336480  loss = 2.46464 avg_loss = 3.22310\n",
      "epoch no.4 train no.336490  loss = 3.98912 avg_loss = 3.21788\n",
      "epoch no.4 train no.336500  loss = 4.22085 avg_loss = 3.21273\n",
      "epoch no.4 train no.336510  loss = 4.68483 avg_loss = 3.28142\n",
      "epoch no.4 train no.336520  loss = 4.86776 avg_loss = 3.30593\n",
      "epoch no.4 train no.336530  loss = 3.74618 avg_loss = 3.26255\n",
      "epoch no.4 train no.336540  loss = 3.96843 avg_loss = 3.30206\n",
      "epoch no.4 train no.336550  loss = 2.89335 avg_loss = 3.28595\n",
      "epoch no.4 train no.336560  loss = 3.11348 avg_loss = 3.27242\n",
      "epoch no.4 train no.336570  loss = 2.59381 avg_loss = 3.22974\n",
      "epoch no.4 train no.336580  loss = 3.01978 avg_loss = 3.22248\n",
      "epoch no.4 train no.336590  loss = 4.75943 avg_loss = 3.22901\n",
      "epoch no.4 train no.336600  loss = 2.85833 avg_loss = 3.22396\n",
      "epoch no.4 train no.336610  loss = 3.43443 avg_loss = 3.20818\n",
      "epoch no.4 train no.336620  loss = 2.47076 avg_loss = 3.21080\n",
      "epoch no.4 train no.336630  loss = 3.99497 avg_loss = 3.20775\n",
      "epoch no.4 train no.336640  loss = 2.92372 avg_loss = 3.22585\n",
      "epoch no.4 train no.336650  loss = 4.85104 avg_loss = 3.28904\n",
      "epoch no.4 train no.336660  loss = 2.21029 avg_loss = 3.28895\n",
      "epoch no.4 train no.336670  loss = 4.09256 avg_loss = 3.34083\n",
      "epoch no.4 train no.336680  loss = 3.88733 avg_loss = 3.35011\n",
      "epoch no.4 train no.336690  loss = 5.51367 avg_loss = 3.33177\n",
      "epoch no.4 train no.336700  loss = 2.51094 avg_loss = 3.34599\n",
      "epoch no.4 train no.336710  loss = 4.12044 avg_loss = 3.33135\n",
      "epoch no.4 train no.336720  loss = 1.93734 avg_loss = 3.26867\n",
      "epoch no.4 train no.336730  loss = 3.89244 avg_loss = 3.26461\n",
      "epoch no.4 train no.336740  loss = 3.13684 avg_loss = 3.26832\n",
      "epoch no.4 train no.336750  loss = 3.27204 avg_loss = 3.22146\n",
      "epoch no.4 train no.336760  loss = 2.26290 avg_loss = 3.16452\n",
      "epoch no.4 train no.336770  loss = 3.16817 avg_loss = 3.13068\n",
      "epoch no.4 train no.336780  loss = 4.10209 avg_loss = 3.21293\n",
      "epoch no.4 train no.336790  loss = 2.80341 avg_loss = 3.20284\n",
      "epoch no.4 train no.336800  loss = 1.82640 avg_loss = 3.19784\n",
      "epoch no.4 train no.336810  loss = 2.44399 avg_loss = 3.23077\n",
      "epoch no.4 train no.336820  loss = 2.18405 avg_loss = 3.17909\n",
      "epoch no.4 train no.336830  loss = 3.40359 avg_loss = 3.16092\n",
      "epoch no.4 train no.336840  loss = 3.07480 avg_loss = 3.15017\n",
      "epoch no.4 train no.336850  loss = 2.23059 avg_loss = 3.12756\n",
      "epoch no.4 train no.336860  loss = 2.43295 avg_loss = 3.08161\n",
      "epoch no.4 train no.336870  loss = 4.38320 avg_loss = 3.10065\n",
      "epoch no.4 train no.336880  loss = 3.06044 avg_loss = 3.10237\n",
      "epoch no.4 train no.336890  loss = 2.73945 avg_loss = 3.04884\n",
      "epoch no.4 train no.336900  loss = 2.47150 avg_loss = 3.02109\n",
      "epoch no.4 train no.336910  loss = 3.06988 avg_loss = 3.04995\n",
      "epoch no.4 train no.336920  loss = 3.03578 avg_loss = 3.06950\n",
      "epoch no.4 train no.336930  loss = 3.01131 avg_loss = 3.03314\n",
      "epoch no.4 train no.336940  loss = 5.07074 avg_loss = 3.06014\n",
      "epoch no.4 train no.336950  loss = 2.42342 avg_loss = 3.05224\n",
      "epoch no.4 train no.336960  loss = 3.96667 avg_loss = 3.07743\n",
      "epoch no.4 train no.336970  loss = 1.83549 avg_loss = 3.02956\n",
      "epoch no.4 train no.336980  loss = 3.72286 avg_loss = 3.05567\n",
      "epoch no.4 train no.336990  loss = 2.62375 avg_loss = 3.06039\n",
      "epoch no.4 train no.337000  loss = 3.49916 avg_loss = 3.08845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁신나는', 'mmer', '</s>']\n",
      "여름엔 역시 summer</s>\n",
      "epoch no.4 train no.337010  loss = 3.34805 avg_loss = 3.11115\n",
      "epoch no.4 train no.337020  loss = 3.63700 avg_loss = 3.12498\n",
      "epoch no.4 train no.337030  loss = 4.13153 avg_loss = 3.18474\n",
      "epoch no.4 train no.337040  loss = 1.97341 avg_loss = 3.18760\n",
      "epoch no.4 train no.337050  loss = 3.64848 avg_loss = 3.15286\n",
      "epoch no.4 train no.337060  loss = 3.96167 avg_loss = 3.15954\n",
      "epoch no.4 train no.337070  loss = 4.19560 avg_loss = 3.16744\n",
      "epoch no.4 train no.337080  loss = 4.74363 avg_loss = 3.20534\n",
      "epoch no.4 train no.337090  loss = 3.11684 avg_loss = 3.18110\n",
      "epoch no.4 train no.337100  loss = 2.86423 avg_loss = 3.17796\n",
      "epoch no.4 train no.337110  loss = 2.06860 avg_loss = 3.14625\n",
      "epoch no.4 train no.337120  loss = 2.28609 avg_loss = 3.11812\n",
      "epoch no.4 train no.337130  loss = 3.61791 avg_loss = 3.16454\n",
      "epoch no.4 train no.337140  loss = 3.34556 avg_loss = 3.16743\n",
      "epoch no.4 train no.337150  loss = 3.42963 avg_loss = 3.19139\n",
      "epoch no.4 train no.337160  loss = 2.89169 avg_loss = 3.20323\n",
      "epoch no.4 train no.337170  loss = 3.19933 avg_loss = 3.19477\n",
      "epoch no.4 train no.337180  loss = 4.43664 avg_loss = 3.19123\n",
      "epoch no.4 train no.337190  loss = 4.05526 avg_loss = 3.19536\n",
      "epoch no.4 train no.337200  loss = 4.11851 avg_loss = 3.18734\n",
      "epoch no.4 train no.337210  loss = 3.70315 avg_loss = 3.18422\n",
      "epoch no.4 train no.337220  loss = 3.34399 avg_loss = 3.18397\n",
      "epoch no.4 train no.337230  loss = 2.32357 avg_loss = 3.15153\n",
      "epoch no.4 train no.337240  loss = 2.06400 avg_loss = 3.15632\n",
      "epoch no.4 train no.337250  loss = 3.60920 avg_loss = 3.18403\n",
      "epoch no.4 train no.337260  loss = 4.42215 avg_loss = 3.19729\n",
      "epoch no.4 train no.337270  loss = 1.97935 avg_loss = 3.18382\n",
      "epoch no.4 train no.337280  loss = 1.63756 avg_loss = 3.13327\n",
      "epoch no.4 train no.337290  loss = 3.06041 avg_loss = 3.15835\n",
      "epoch no.4 train no.337300  loss = 2.18169 avg_loss = 3.19497\n",
      "epoch no.4 train no.337310  loss = 2.36704 avg_loss = 3.19671\n",
      "epoch no.4 train no.337320  loss = 2.47781 avg_loss = 3.18029\n",
      "epoch no.4 train no.337330  loss = 3.18676 avg_loss = 3.15341\n",
      "epoch no.4 train no.337340  loss = 3.77844 avg_loss = 3.15275\n",
      "epoch no.4 train no.337350  loss = 4.57730 avg_loss = 3.22175\n",
      "epoch no.4 train no.337360  loss = 2.11953 avg_loss = 3.19016\n",
      "epoch no.4 train no.337370  loss = 1.90011 avg_loss = 3.17159\n",
      "epoch no.4 train no.337380  loss = 3.16326 avg_loss = 3.18156\n",
      "epoch no.4 train no.337390  loss = 2.38968 avg_loss = 3.16404\n",
      "epoch no.4 train no.337400  loss = 3.00957 avg_loss = 3.17274\n",
      "epoch no.4 train no.337410  loss = 2.56145 avg_loss = 3.19935\n",
      "epoch no.4 train no.337420  loss = 2.93076 avg_loss = 3.21900\n",
      "epoch no.4 train no.337430  loss = 2.14436 avg_loss = 3.21610\n",
      "epoch no.4 train no.337440  loss = 4.31602 avg_loss = 3.23204\n",
      "epoch no.4 train no.337450  loss = 2.36454 avg_loss = 3.21548\n",
      "epoch no.4 train no.337460  loss = 2.01018 avg_loss = 3.19948\n",
      "epoch no.4 train no.337470  loss = 2.95771 avg_loss = 3.14991\n",
      "epoch no.4 train no.337480  loss = 2.92988 avg_loss = 3.18776\n",
      "epoch no.4 train no.337490  loss = 1.88952 avg_loss = 3.16235\n",
      "epoch no.4 train no.337500  loss = 2.37030 avg_loss = 3.13078\n",
      "epoch no.4 train no.337510  loss = 4.38440 avg_loss = 3.12146\n",
      "epoch no.4 train no.337520  loss = 3.07444 avg_loss = 3.16030\n",
      "epoch no.4 train no.337530  loss = 2.79913 avg_loss = 3.11298\n",
      "epoch no.4 train no.337540  loss = 4.67142 avg_loss = 3.16324\n",
      "epoch no.4 train no.337550  loss = 2.66142 avg_loss = 3.15591\n",
      "epoch no.4 train no.337560  loss = 3.95854 avg_loss = 3.15659\n",
      "epoch no.4 train no.337570  loss = 2.96064 avg_loss = 3.15545\n",
      "epoch no.4 train no.337580  loss = 3.75558 avg_loss = 3.14327\n",
      "epoch no.4 train no.337590  loss = 2.40964 avg_loss = 3.13425\n",
      "epoch no.4 train no.337600  loss = 2.43526 avg_loss = 3.12051\n",
      "epoch no.4 train no.337610  loss = 3.19347 avg_loss = 3.08565\n",
      "epoch no.4 train no.337620  loss = 3.02305 avg_loss = 3.11371\n",
      "epoch no.4 train no.337630  loss = 3.23428 avg_loss = 3.15067\n",
      "epoch no.4 train no.337640  loss = 3.87158 avg_loss = 3.11712\n",
      "epoch no.4 train no.337650  loss = 3.22155 avg_loss = 3.15449\n",
      "epoch no.4 train no.337660  loss = 4.64272 avg_loss = 3.15012\n",
      "epoch no.4 train no.337670  loss = 3.75794 avg_loss = 3.13207\n",
      "epoch no.4 train no.337680  loss = 4.31016 avg_loss = 3.12021\n",
      "epoch no.4 train no.337690  loss = 3.52902 avg_loss = 3.10505\n",
      "epoch no.4 train no.337700  loss = 4.15410 avg_loss = 3.16892\n",
      "epoch no.4 train no.337710  loss = 4.59539 avg_loss = 3.16955\n",
      "epoch no.4 train no.337720  loss = 4.16949 avg_loss = 3.16318\n",
      "epoch no.4 train no.337730  loss = 4.13294 avg_loss = 3.16988\n",
      "epoch no.4 train no.337740  loss = 3.18981 avg_loss = 3.18190\n",
      "epoch no.4 train no.337750  loss = 3.07792 avg_loss = 3.14597\n",
      "epoch no.4 train no.337760  loss = 3.93556 avg_loss = 3.18819\n",
      "epoch no.4 train no.337770  loss = 2.00861 avg_loss = 3.16763\n",
      "epoch no.4 train no.337780  loss = 1.89838 avg_loss = 3.12886\n",
      "epoch no.4 train no.337790  loss = 3.34057 avg_loss = 3.15932\n",
      "epoch no.4 train no.337800  loss = 3.38491 avg_loss = 3.16902\n",
      "epoch no.4 train no.337810  loss = 3.96314 avg_loss = 3.16281\n",
      "epoch no.4 train no.337820  loss = 2.89366 avg_loss = 3.12486\n",
      "epoch no.4 train no.337830  loss = 3.60439 avg_loss = 3.12101\n",
      "epoch no.4 train no.337840  loss = 1.65085 avg_loss = 3.09774\n",
      "epoch no.4 train no.337850  loss = 3.44253 avg_loss = 3.12011\n",
      "epoch no.4 train no.337860  loss = 3.62566 avg_loss = 3.16029\n",
      "epoch no.4 train no.337870  loss = 2.70297 avg_loss = 3.18732\n",
      "epoch no.4 train no.337880  loss = 2.33938 avg_loss = 3.18513\n",
      "epoch no.4 train no.337890  loss = 2.69279 avg_loss = 3.16913\n",
      "epoch no.4 train no.337900  loss = 3.18007 avg_loss = 3.18058\n",
      "epoch no.4 train no.337910  loss = 2.17761 avg_loss = 3.16366\n",
      "epoch no.4 train no.337920  loss = 4.46787 avg_loss = 3.18956\n",
      "epoch no.4 train no.337930  loss = 2.46344 avg_loss = 3.20219\n",
      "epoch no.4 train no.337940  loss = 2.01567 avg_loss = 3.21854\n",
      "epoch no.4 train no.337950  loss = 3.18836 avg_loss = 3.19394\n",
      "epoch no.4 train no.337960  loss = 3.48179 avg_loss = 3.15261\n",
      "epoch no.4 train no.337970  loss = 3.88525 avg_loss = 3.16725\n",
      "epoch no.4 train no.337980  loss = 2.21118 avg_loss = 3.15404\n",
      "epoch no.4 train no.337990  loss = 4.49774 avg_loss = 3.16452\n",
      "epoch no.4 train no.338000  loss = 1.92723 avg_loss = 3.15126\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '</s>']\n",
      "여름밤에 듣는 재즈</s>\n",
      "epoch no.4 train no.338010  loss = 2.94158 avg_loss = 3.15044\n",
      "epoch no.4 train no.338020  loss = 2.34420 avg_loss = 3.14562\n",
      "epoch no.4 train no.338030  loss = 1.99767 avg_loss = 3.14220\n",
      "epoch no.4 train no.338040  loss = 2.88917 avg_loss = 3.13305\n",
      "epoch no.4 train no.338050  loss = 3.70987 avg_loss = 3.17668\n",
      "epoch no.4 train no.338060  loss = 3.61639 avg_loss = 3.22844\n",
      "epoch no.4 train no.338070  loss = 3.88282 avg_loss = 3.20396\n",
      "epoch no.4 train no.338080  loss = 2.87314 avg_loss = 3.21947\n",
      "epoch no.4 train no.338090  loss = 2.82295 avg_loss = 3.16076\n",
      "epoch no.4 train no.338100  loss = 1.95085 avg_loss = 3.15268\n",
      "epoch no.4 train no.338110  loss = 3.41911 avg_loss = 3.19257\n",
      "epoch no.4 train no.338120  loss = 2.38721 avg_loss = 3.16625\n",
      "epoch no.4 train no.338130  loss = 2.11495 avg_loss = 3.18700\n",
      "epoch no.4 train no.338140  loss = 4.02500 avg_loss = 3.18869\n",
      "epoch no.4 train no.338150  loss = 2.79211 avg_loss = 3.20960\n",
      "epoch no.4 train no.338160  loss = 1.91740 avg_loss = 3.19121\n",
      "epoch no.4 train no.338170  loss = 2.42093 avg_loss = 3.19068\n",
      "epoch no.4 train no.338180  loss = 2.25926 avg_loss = 3.16805\n",
      "epoch no.4 train no.338190  loss = 2.16738 avg_loss = 3.16475\n",
      "epoch no.4 train no.338200  loss = 4.84349 avg_loss = 3.15698\n",
      "epoch no.4 train no.338210  loss = 2.58297 avg_loss = 3.12738\n",
      "epoch no.4 train no.338220  loss = 3.31974 avg_loss = 3.15715\n",
      "epoch no.4 train no.338230  loss = 5.12627 avg_loss = 3.17625\n",
      "epoch no.4 train no.338240  loss = 3.63341 avg_loss = 3.17282\n",
      "epoch no.4 train no.338250  loss = 2.26050 avg_loss = 3.14461\n",
      "epoch no.4 train no.338260  loss = 2.60848 avg_loss = 3.10964\n",
      "epoch no.4 train no.338270  loss = 3.79873 avg_loss = 3.08889\n",
      "epoch no.4 train no.338280  loss = 5.63979 avg_loss = 3.12385\n",
      "epoch no.4 train no.338290  loss = 3.17192 avg_loss = 3.11017\n",
      "epoch no.4 train no.338300  loss = 2.64839 avg_loss = 3.11087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.338310  loss = 2.91465 avg_loss = 3.14137\n",
      "epoch no.4 train no.338320  loss = 3.64575 avg_loss = 3.19780\n",
      "epoch no.4 train no.338330  loss = 4.01764 avg_loss = 3.16177\n",
      "epoch no.4 train no.338340  loss = 2.63109 avg_loss = 3.20838\n",
      "epoch no.4 train no.338350  loss = 3.27399 avg_loss = 3.17628\n",
      "epoch no.4 train no.338360  loss = 2.37959 avg_loss = 3.22195\n",
      "epoch no.4 train no.338370  loss = 2.22576 avg_loss = 3.26410\n",
      "epoch no.4 train no.338380  loss = 2.11575 avg_loss = 3.23558\n",
      "epoch no.4 train no.338390  loss = 2.42803 avg_loss = 3.23909\n",
      "epoch no.4 train no.338400  loss = 3.55806 avg_loss = 3.25894\n",
      "epoch no.4 train no.338410  loss = 3.18255 avg_loss = 3.27065\n",
      "epoch no.4 train no.338420  loss = 4.93193 avg_loss = 3.27892\n",
      "epoch no.4 train no.338430  loss = 4.63465 avg_loss = 3.32195\n",
      "epoch no.4 train no.338440  loss = 2.30869 avg_loss = 3.25976\n",
      "epoch no.4 train no.338450  loss = 3.60409 avg_loss = 3.26254\n",
      "epoch no.4 train no.338460  loss = 2.60882 avg_loss = 3.27487\n",
      "epoch no.4 train no.338470  loss = 5.21180 avg_loss = 3.27909\n",
      "epoch no.4 train no.338480  loss = 2.89329 avg_loss = 3.22892\n",
      "epoch no.4 train no.338490  loss = 1.30505 avg_loss = 3.24631\n",
      "epoch no.4 train no.338500  loss = 3.68948 avg_loss = 3.27802\n",
      "epoch no.4 train no.338510  loss = 3.06168 avg_loss = 3.29730\n",
      "epoch no.4 train no.338520  loss = 2.33864 avg_loss = 3.30830\n",
      "epoch no.4 train no.338530  loss = 4.17451 avg_loss = 3.30180\n",
      "epoch no.4 train no.338540  loss = 3.10906 avg_loss = 3.35942\n",
      "epoch no.4 train no.338550  loss = 2.64230 avg_loss = 3.37213\n",
      "epoch no.4 train no.338560  loss = 1.17631 avg_loss = 3.34021\n",
      "epoch no.4 train no.338570  loss = 3.57032 avg_loss = 3.32264\n",
      "epoch no.4 train no.338580  loss = 4.52003 avg_loss = 3.35724\n",
      "epoch no.4 train no.338590  loss = 3.01003 avg_loss = 3.33120\n",
      "epoch no.4 train no.338600  loss = 1.82674 avg_loss = 3.27643\n",
      "epoch no.4 train no.338610  loss = 2.74562 avg_loss = 3.27165\n",
      "epoch no.4 train no.338620  loss = 3.45727 avg_loss = 3.30237\n",
      "epoch no.4 train no.338630  loss = 3.65870 avg_loss = 3.31801\n",
      "epoch no.4 train no.338640  loss = 4.61751 avg_loss = 3.32246\n",
      "epoch no.4 train no.338650  loss = 3.48532 avg_loss = 3.29828\n",
      "epoch no.4 train no.338660  loss = 2.72576 avg_loss = 3.26586\n",
      "epoch no.4 train no.338670  loss = 2.81049 avg_loss = 3.32174\n",
      "epoch no.4 train no.338680  loss = 3.37716 avg_loss = 3.26641\n",
      "epoch no.4 train no.338690  loss = 3.42592 avg_loss = 3.26556\n",
      "epoch no.4 train no.338700  loss = 3.74142 avg_loss = 3.28388\n",
      "epoch no.4 train no.338710  loss = 4.00387 avg_loss = 3.29457\n",
      "epoch no.4 train no.338720  loss = 2.83188 avg_loss = 3.28323\n",
      "epoch no.4 train no.338730  loss = 3.53158 avg_loss = 3.24627\n",
      "epoch no.4 train no.338740  loss = 3.08720 avg_loss = 3.23937\n",
      "epoch no.4 train no.338750  loss = 2.80969 avg_loss = 3.26597\n",
      "epoch no.4 train no.338760  loss = 2.68119 avg_loss = 3.25926\n",
      "epoch no.4 train no.338770  loss = 2.69329 avg_loss = 3.24383\n",
      "epoch no.4 train no.338780  loss = 2.53563 avg_loss = 3.21932\n",
      "epoch no.4 train no.338790  loss = 2.18092 avg_loss = 3.20381\n",
      "epoch no.4 train no.338800  loss = 3.39421 avg_loss = 3.15995\n",
      "epoch no.4 train no.338810  loss = 2.82198 avg_loss = 3.13717\n",
      "epoch no.4 train no.338820  loss = 3.08542 avg_loss = 3.15048\n",
      "epoch no.4 train no.338830  loss = 2.69495 avg_loss = 3.15945\n",
      "epoch no.4 train no.338840  loss = 2.46799 avg_loss = 3.19159\n",
      "epoch no.4 train no.338850  loss = 3.64138 avg_loss = 3.18752\n",
      "epoch no.4 train no.338860  loss = 3.75405 avg_loss = 3.21306\n",
      "epoch no.4 train no.338870  loss = 2.62200 avg_loss = 3.21647\n",
      "epoch no.4 train no.338880  loss = 4.24865 avg_loss = 3.23068\n",
      "epoch no.4 train no.338890  loss = 2.33429 avg_loss = 3.24674\n",
      "epoch no.4 train no.338900  loss = 2.14701 avg_loss = 3.23015\n",
      "epoch no.4 train no.338910  loss = 5.14670 avg_loss = 3.25814\n",
      "epoch no.4 train no.338920  loss = 2.12948 avg_loss = 3.24968\n",
      "epoch no.4 train no.338930  loss = 1.77890 avg_loss = 3.20116\n",
      "epoch no.4 train no.338940  loss = 2.01178 avg_loss = 3.15353\n",
      "epoch no.4 train no.338950  loss = 3.35473 avg_loss = 3.15791\n",
      "epoch no.4 train no.338960  loss = 1.71927 avg_loss = 3.15720\n",
      "epoch no.4 train no.338970  loss = 2.63605 avg_loss = 3.12434\n",
      "epoch no.4 train no.338980  loss = 3.57595 avg_loss = 3.09627\n",
      "epoch no.4 train no.338990  loss = 3.75478 avg_loss = 3.07096\n",
      "epoch no.4 train no.339000  loss = 3.71179 avg_loss = 3.08069\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '</s>']\n",
      "여름밤에 듣는 노래</s>\n",
      "epoch no.4 train no.339010  loss = 1.96430 avg_loss = 3.09306\n",
      "epoch no.4 train no.339020  loss = 2.50480 avg_loss = 3.10500\n",
      "epoch no.4 train no.339030  loss = 2.60873 avg_loss = 3.14302\n",
      "epoch no.4 train no.339040  loss = 2.62382 avg_loss = 3.15492\n",
      "epoch no.4 train no.339050  loss = 2.54317 avg_loss = 3.15519\n",
      "epoch no.4 train no.339060  loss = 2.96269 avg_loss = 3.13693\n",
      "epoch no.4 train no.339070  loss = 3.70872 avg_loss = 3.14645\n",
      "epoch no.4 train no.339080  loss = 5.17197 avg_loss = 3.18013\n",
      "epoch no.4 train no.339090  loss = 2.33568 avg_loss = 3.13897\n",
      "epoch no.4 train no.339100  loss = 2.54073 avg_loss = 3.17038\n",
      "epoch no.4 train no.339110  loss = 4.55445 avg_loss = 3.20366\n",
      "epoch no.4 train no.339120  loss = 4.53145 avg_loss = 3.17833\n",
      "epoch no.4 train no.339130  loss = 3.10870 avg_loss = 3.16735\n",
      "epoch no.4 train no.339140  loss = 2.88487 avg_loss = 3.18709\n",
      "epoch no.4 train no.339150  loss = 4.54683 avg_loss = 3.22480\n",
      "epoch no.4 train no.339160  loss = 2.32317 avg_loss = 3.21067\n",
      "epoch no.4 train no.339170  loss = 2.24811 avg_loss = 3.21400\n",
      "epoch no.4 train no.339180  loss = 5.01428 avg_loss = 3.26053\n",
      "epoch no.4 train no.339190  loss = 2.27850 avg_loss = 3.22763\n",
      "epoch no.4 train no.339200  loss = 3.86017 avg_loss = 3.23059\n",
      "epoch no.4 train no.339210  loss = 2.09381 avg_loss = 3.21305\n",
      "epoch no.4 train no.339220  loss = 2.78268 avg_loss = 3.21196\n",
      "epoch no.4 train no.339230  loss = 2.86353 avg_loss = 3.22919\n",
      "epoch no.4 train no.339240  loss = 3.88124 avg_loss = 3.25324\n",
      "epoch no.4 train no.339250  loss = 2.03602 avg_loss = 3.23419\n",
      "epoch no.4 train no.339260  loss = 2.31878 avg_loss = 3.16173\n",
      "epoch no.4 train no.339270  loss = 3.23835 avg_loss = 3.17294\n",
      "epoch no.4 train no.339280  loss = 5.52637 avg_loss = 3.18076\n",
      "epoch no.4 train no.339290  loss = 2.87323 avg_loss = 3.16615\n",
      "epoch no.4 train no.339300  loss = 2.04269 avg_loss = 3.15451\n",
      "epoch no.4 train no.339310  loss = 2.48703 avg_loss = 3.15508\n",
      "epoch no.4 train no.339320  loss = 4.35441 avg_loss = 3.20141\n",
      "epoch no.4 train no.339330  loss = 2.66273 avg_loss = 3.21158\n",
      "epoch no.4 train no.339340  loss = 2.34879 avg_loss = 3.19289\n",
      "epoch no.4 train no.339350  loss = 2.06894 avg_loss = 3.19067\n",
      "epoch no.4 train no.339360  loss = 4.82946 avg_loss = 3.21741\n",
      "epoch no.4 train no.339370  loss = 1.90408 avg_loss = 3.22405\n",
      "epoch no.4 train no.339380  loss = 4.17737 avg_loss = 3.20719\n",
      "epoch no.4 train no.339390  loss = 4.83203 avg_loss = 3.23519\n",
      "epoch no.4 train no.339400  loss = 3.46607 avg_loss = 3.26143\n",
      "epoch no.4 train no.339410  loss = 3.57047 avg_loss = 3.25765\n",
      "epoch no.4 train no.339420  loss = 2.63690 avg_loss = 3.27429\n",
      "epoch no.4 train no.339430  loss = 3.81923 avg_loss = 3.28570\n",
      "epoch no.4 train no.339440  loss = 3.82731 avg_loss = 3.23931\n",
      "epoch no.4 train no.339450  loss = 1.03649 avg_loss = 3.21189\n",
      "epoch no.4 train no.339460  loss = 4.10235 avg_loss = 3.18616\n",
      "epoch no.4 train no.339470  loss = 3.49958 avg_loss = 3.18468\n",
      "epoch no.4 train no.339480  loss = 4.52954 avg_loss = 3.20652\n",
      "epoch no.4 train no.339490  loss = 3.50755 avg_loss = 3.18262\n",
      "epoch no.4 train no.339500  loss = 3.48188 avg_loss = 3.16952\n",
      "epoch no.4 train no.339510  loss = 4.90563 avg_loss = 3.20038\n",
      "epoch no.4 train no.339520  loss = 2.71645 avg_loss = 3.19952\n",
      "epoch no.4 train no.339530  loss = 3.91211 avg_loss = 3.20507\n",
      "epoch no.4 train no.339540  loss = 2.77589 avg_loss = 3.19541\n",
      "epoch no.4 train no.339550  loss = 2.59146 avg_loss = 3.17678\n",
      "epoch no.4 train no.339560  loss = 2.17492 avg_loss = 3.18680\n",
      "epoch no.4 train no.339570  loss = 3.78661 avg_loss = 3.15919\n",
      "epoch no.4 train no.339580  loss = 2.82658 avg_loss = 3.16325\n",
      "epoch no.4 train no.339590  loss = 4.39930 avg_loss = 3.25874\n",
      "epoch no.4 train no.339600  loss = 2.54843 avg_loss = 3.24152\n",
      "epoch no.4 train no.339610  loss = 4.39223 avg_loss = 3.22309\n",
      "epoch no.4 train no.339620  loss = 2.28624 avg_loss = 3.21003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.339630  loss = 2.73172 avg_loss = 3.25480\n",
      "epoch no.4 train no.339640  loss = 3.32275 avg_loss = 3.23160\n",
      "epoch no.4 train no.339650  loss = 3.31255 avg_loss = 3.21015\n",
      "epoch no.4 train no.339660  loss = 2.39273 avg_loss = 3.20057\n",
      "epoch no.4 train no.339670  loss = 1.61040 avg_loss = 3.17531\n",
      "epoch no.4 train no.339680  loss = 4.13919 avg_loss = 3.17586\n",
      "epoch no.4 train no.339690  loss = 2.61413 avg_loss = 3.14441\n",
      "epoch no.4 train no.339700  loss = 2.21844 avg_loss = 3.14721\n",
      "epoch no.4 train no.339710  loss = 2.97484 avg_loss = 3.13936\n",
      "epoch no.4 train no.339720  loss = 2.28342 avg_loss = 3.10486\n",
      "epoch no.4 train no.339730  loss = 3.18128 avg_loss = 3.07670\n",
      "epoch no.4 train no.339740  loss = 1.94681 avg_loss = 3.11829\n",
      "epoch no.4 train no.339750  loss = 1.87362 avg_loss = 3.12591\n",
      "epoch no.4 train no.339760  loss = 2.00765 avg_loss = 3.11753\n",
      "epoch no.4 train no.339770  loss = 4.21394 avg_loss = 3.16059\n",
      "epoch no.4 train no.339780  loss = 3.45561 avg_loss = 3.19887\n",
      "epoch no.4 train no.339790  loss = 4.16017 avg_loss = 3.21764\n",
      "epoch no.4 train no.339800  loss = 3.85445 avg_loss = 3.17493\n",
      "epoch no.4 train no.339810  loss = 3.40853 avg_loss = 3.24780\n",
      "epoch no.4 train no.339820  loss = 4.23388 avg_loss = 3.28372\n",
      "epoch no.4 train no.339830  loss = 4.67596 avg_loss = 3.30551\n",
      "epoch no.4 train no.339840  loss = 4.08409 avg_loss = 3.36724\n",
      "epoch no.4 train no.339850  loss = 3.31573 avg_loss = 3.32724\n",
      "epoch no.4 train no.339860  loss = 3.76135 avg_loss = 3.32711\n",
      "epoch no.4 train no.339870  loss = 2.84764 avg_loss = 3.30390\n",
      "epoch no.4 train no.339880  loss = 2.73980 avg_loss = 3.28930\n",
      "epoch no.4 train no.339890  loss = 3.34938 avg_loss = 3.31930\n",
      "epoch no.4 train no.339900  loss = 4.30743 avg_loss = 3.27957\n",
      "epoch no.4 train no.339910  loss = 1.81420 avg_loss = 3.24363\n",
      "epoch no.4 train no.339920  loss = 2.43274 avg_loss = 3.25157\n",
      "epoch no.4 train no.339930  loss = 2.55613 avg_loss = 3.23837\n",
      "epoch no.4 train no.339940  loss = 2.94946 avg_loss = 3.24472\n",
      "epoch no.4 train no.339950  loss = 1.88868 avg_loss = 3.29085\n",
      "epoch no.4 train no.339960  loss = 4.11859 avg_loss = 3.32783\n",
      "epoch no.4 train no.339970  loss = 3.06338 avg_loss = 3.31420\n",
      "epoch no.4 train no.339980  loss = 3.34217 avg_loss = 3.32327\n",
      "epoch no.4 train no.339990  loss = 2.64488 avg_loss = 3.34097\n",
      "epoch no.4 train no.340000  loss = 3.99746 avg_loss = 3.32469\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁그루', '한', '▁인디', '율', '</s>']\n",
      "여름밤의 잔잔한 선율</s>\n",
      "epoch no.4 train no.340010  loss = 2.29850 avg_loss = 3.31466\n",
      "epoch no.4 train no.340020  loss = 2.81740 avg_loss = 3.27267\n",
      "epoch no.4 train no.340030  loss = 3.82472 avg_loss = 3.23894\n",
      "epoch no.4 train no.340040  loss = 3.03942 avg_loss = 3.19022\n",
      "epoch no.4 train no.340050  loss = 4.27036 avg_loss = 3.21149\n",
      "epoch no.4 train no.340060  loss = 2.79294 avg_loss = 3.24265\n",
      "epoch no.4 train no.340070  loss = 3.15783 avg_loss = 3.21105\n",
      "epoch no.4 train no.340080  loss = 4.61061 avg_loss = 3.23616\n",
      "epoch no.4 train no.340090  loss = 2.68765 avg_loss = 3.27215\n",
      "epoch no.4 train no.340100  loss = 2.78124 avg_loss = 3.26420\n",
      "epoch no.4 train no.340110  loss = 2.73718 avg_loss = 3.24291\n",
      "epoch no.4 train no.340120  loss = 2.32082 avg_loss = 3.29690\n",
      "epoch no.4 train no.340130  loss = 3.57098 avg_loss = 3.29819\n",
      "epoch no.4 train no.340140  loss = 3.71805 avg_loss = 3.32326\n",
      "epoch no.4 train no.340150  loss = 2.06445 avg_loss = 3.31978\n",
      "epoch no.4 train no.340160  loss = 3.59325 avg_loss = 3.29698\n",
      "epoch no.4 train no.340170  loss = 2.28649 avg_loss = 3.29005\n",
      "epoch no.4 train no.340180  loss = 4.51871 avg_loss = 3.30754\n",
      "epoch no.4 train no.340190  loss = 2.36849 avg_loss = 3.26972\n",
      "epoch no.4 train no.340200  loss = 2.25629 avg_loss = 3.26370\n",
      "epoch no.4 train no.340210  loss = 3.49220 avg_loss = 3.26232\n",
      "epoch no.4 train no.340220  loss = 2.75747 avg_loss = 3.26230\n",
      "epoch no.4 train no.340230  loss = 5.38213 avg_loss = 3.28611\n",
      "epoch no.4 train no.340240  loss = 4.83214 avg_loss = 3.27813\n",
      "epoch no.4 train no.340250  loss = 2.90575 avg_loss = 3.26901\n",
      "epoch no.4 train no.340260  loss = 4.75292 avg_loss = 3.29403\n",
      "epoch no.4 train no.340270  loss = 2.83027 avg_loss = 3.25708\n",
      "epoch no.4 train no.340280  loss = 2.10571 avg_loss = 3.21344\n",
      "epoch no.4 train no.340290  loss = 3.83006 avg_loss = 3.27375\n",
      "epoch no.4 train no.340300  loss = 2.12370 avg_loss = 3.22972\n",
      "epoch no.4 train no.340310  loss = 2.16689 avg_loss = 3.21547\n",
      "epoch no.4 train no.340320  loss = 1.12866 avg_loss = 3.20329\n",
      "epoch no.4 train no.340330  loss = 2.68266 avg_loss = 3.20245\n",
      "epoch no.4 train no.340340  loss = 2.91555 avg_loss = 3.18483\n",
      "epoch no.4 train no.340350  loss = 2.94022 avg_loss = 3.18584\n",
      "epoch no.4 train no.340360  loss = 2.89245 avg_loss = 3.20045\n",
      "epoch no.4 train no.340370  loss = 0.79371 avg_loss = 3.23129\n",
      "epoch no.4 train no.340380  loss = 3.70235 avg_loss = 3.21288\n",
      "epoch no.4 train no.340390  loss = 4.04556 avg_loss = 3.22872\n",
      "epoch no.4 train no.340400  loss = 4.24550 avg_loss = 3.27347\n",
      "epoch no.4 train no.340410  loss = 3.43707 avg_loss = 3.22337\n",
      "epoch no.4 train no.340420  loss = 3.31045 avg_loss = 3.24790\n",
      "epoch no.4 train no.340430  loss = 3.39039 avg_loss = 3.23754\n",
      "epoch no.4 train no.340440  loss = 2.83098 avg_loss = 3.21573\n",
      "epoch no.4 train no.340450  loss = 2.62852 avg_loss = 3.17614\n",
      "epoch no.4 train no.340460  loss = 2.95272 avg_loss = 3.21157\n",
      "epoch no.4 train no.340470  loss = 2.92932 avg_loss = 3.18594\n",
      "epoch no.4 train no.340480  loss = 4.39864 avg_loss = 3.18199\n",
      "epoch no.4 train no.340490  loss = 2.45609 avg_loss = 3.15027\n",
      "epoch no.4 train no.340500  loss = 2.97575 avg_loss = 3.12447\n",
      "epoch no.4 train no.340510  loss = 2.23109 avg_loss = 3.12714\n",
      "epoch no.4 train no.340520  loss = 3.70306 avg_loss = 3.13950\n",
      "epoch no.4 train no.340530  loss = 3.69058 avg_loss = 3.13988\n",
      "epoch no.4 train no.340540  loss = 2.68036 avg_loss = 3.16949\n",
      "epoch no.4 train no.340550  loss = 2.77047 avg_loss = 3.20104\n",
      "epoch no.4 train no.340560  loss = 2.80465 avg_loss = 3.18509\n",
      "epoch no.4 train no.340570  loss = 3.97134 avg_loss = 3.15393\n",
      "epoch no.4 train no.340580  loss = 2.17157 avg_loss = 3.14636\n",
      "epoch no.4 train no.340590  loss = 4.07256 avg_loss = 3.15363\n",
      "epoch no.4 train no.340600  loss = 3.12768 avg_loss = 3.16766\n",
      "epoch no.4 train no.340610  loss = 3.14550 avg_loss = 3.14352\n",
      "epoch no.4 train no.340620  loss = 4.66455 avg_loss = 3.15652\n",
      "epoch no.4 train no.340630  loss = 4.76607 avg_loss = 3.18876\n",
      "epoch no.4 train no.340640  loss = 3.83171 avg_loss = 3.16600\n",
      "epoch no.4 train no.340650  loss = 3.17916 avg_loss = 3.13687\n",
      "epoch no.4 train no.340660  loss = 1.42556 avg_loss = 3.08966\n",
      "epoch no.4 train no.340670  loss = 2.87951 avg_loss = 3.06227\n",
      "epoch no.4 train no.340680  loss = 3.59695 avg_loss = 3.07572\n",
      "epoch no.4 train no.340690  loss = 3.50049 avg_loss = 3.13046\n",
      "epoch no.4 train no.340700  loss = 2.70359 avg_loss = 3.11245\n",
      "epoch no.4 train no.340710  loss = 3.38214 avg_loss = 3.14228\n",
      "epoch no.4 train no.340720  loss = 3.08238 avg_loss = 3.17360\n",
      "epoch no.4 train no.340730  loss = 4.24789 avg_loss = 3.20159\n",
      "epoch no.4 train no.340740  loss = 3.46267 avg_loss = 3.24903\n",
      "epoch no.4 train no.340750  loss = 2.65917 avg_loss = 3.24011\n",
      "epoch no.4 train no.340760  loss = 3.31524 avg_loss = 3.25017\n",
      "epoch no.4 train no.340770  loss = 3.34019 avg_loss = 3.25872\n",
      "epoch no.4 train no.340780  loss = 4.04477 avg_loss = 3.24729\n",
      "epoch no.4 train no.340790  loss = 3.03835 avg_loss = 3.21765\n",
      "epoch no.4 train no.340800  loss = 2.74149 avg_loss = 3.15553\n",
      "epoch no.4 train no.340810  loss = 3.51210 avg_loss = 3.18394\n",
      "epoch no.4 train no.340820  loss = 2.39958 avg_loss = 3.14039\n",
      "epoch no.4 train no.340830  loss = 3.42624 avg_loss = 3.19137\n",
      "epoch no.4 train no.340840  loss = 4.97729 avg_loss = 3.15875\n",
      "epoch no.4 train no.340850  loss = 3.84506 avg_loss = 3.17268\n",
      "epoch no.4 train no.340860  loss = 3.25433 avg_loss = 3.16558\n",
      "epoch no.4 train no.340870  loss = 3.18862 avg_loss = 3.21197\n",
      "epoch no.4 train no.340880  loss = 3.58351 avg_loss = 3.19414\n",
      "epoch no.4 train no.340890  loss = 2.75966 avg_loss = 3.19234\n",
      "epoch no.4 train no.340900  loss = 3.10880 avg_loss = 3.23006\n",
      "epoch no.4 train no.340910  loss = 4.26378 avg_loss = 3.24893\n",
      "epoch no.4 train no.340920  loss = 3.37884 avg_loss = 3.22982\n",
      "epoch no.4 train no.340930  loss = 4.08741 avg_loss = 3.21056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.340940  loss = 3.04828 avg_loss = 3.22609\n",
      "epoch no.4 train no.340950  loss = 4.12303 avg_loss = 3.24487\n",
      "epoch no.4 train no.340960  loss = 3.52111 avg_loss = 3.22899\n",
      "epoch no.4 train no.340970  loss = 1.78637 avg_loss = 3.23320\n",
      "epoch no.4 train no.340980  loss = 3.08434 avg_loss = 3.26478\n",
      "epoch no.4 train no.340990  loss = 3.05080 avg_loss = 3.24751\n",
      "epoch no.4 train no.341000  loss = 2.67121 avg_loss = 3.23541\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '을', '▁듣는', '▁감성', '▁음악', '</s>']\n",
      "여름밤에 듣는 힐링음악</s>\n",
      "epoch no.4 train no.341010  loss = 2.43837 avg_loss = 3.22489\n",
      "epoch no.4 train no.341020  loss = 2.39031 avg_loss = 3.28436\n",
      "epoch no.4 train no.341030  loss = 4.74583 avg_loss = 3.32393\n",
      "epoch no.4 train no.341040  loss = 4.12539 avg_loss = 3.31389\n",
      "epoch no.4 train no.341050  loss = 4.26407 avg_loss = 3.29829\n",
      "epoch no.4 train no.341060  loss = 3.36483 avg_loss = 3.32379\n",
      "epoch no.4 train no.341070  loss = 4.24480 avg_loss = 3.32284\n",
      "epoch no.4 train no.341080  loss = 2.65840 avg_loss = 3.32250\n",
      "epoch no.4 train no.341090  loss = 2.72777 avg_loss = 3.29853\n",
      "epoch no.4 train no.341100  loss = 2.41678 avg_loss = 3.31675\n",
      "epoch no.4 train no.341110  loss = 3.46328 avg_loss = 3.34638\n",
      "epoch no.4 train no.341120  loss = 3.34942 avg_loss = 3.36540\n",
      "epoch no.4 train no.341130  loss = 2.64585 avg_loss = 3.39376\n",
      "epoch no.4 train no.341140  loss = 2.28813 avg_loss = 3.42252\n",
      "epoch no.4 train no.341150  loss = 2.42410 avg_loss = 3.37523\n",
      "epoch no.4 train no.341160  loss = 3.03332 avg_loss = 3.33098\n",
      "epoch no.4 train no.341170  loss = 3.02203 avg_loss = 3.30820\n",
      "epoch no.4 train no.341180  loss = 3.72021 avg_loss = 3.30510\n",
      "epoch no.4 train no.341190  loss = 4.27212 avg_loss = 3.30778\n",
      "epoch no.4 train no.341200  loss = 2.25601 avg_loss = 3.32893\n",
      "epoch no.4 train no.341210  loss = 6.17114 avg_loss = 3.41027\n",
      "epoch no.4 train no.341220  loss = 2.13579 avg_loss = 3.37834\n",
      "epoch no.4 train no.341230  loss = 2.19656 avg_loss = 3.33953\n",
      "epoch no.4 train no.341240  loss = 3.91656 avg_loss = 3.30719\n",
      "epoch no.4 train no.341250  loss = 4.83939 avg_loss = 3.33370\n",
      "epoch no.4 train no.341260  loss = 2.13552 avg_loss = 3.30390\n",
      "epoch no.4 train no.341270  loss = 3.19151 avg_loss = 3.26060\n",
      "epoch no.4 train no.341280  loss = 4.00875 avg_loss = 3.27291\n",
      "epoch no.4 train no.341290  loss = 4.63297 avg_loss = 3.26971\n",
      "epoch no.4 train no.341300  loss = 2.22403 avg_loss = 3.24806\n",
      "epoch no.4 train no.341310  loss = 2.22624 avg_loss = 3.29749\n",
      "epoch no.4 train no.341320  loss = 2.61846 avg_loss = 3.26269\n",
      "epoch no.4 train no.341330  loss = 3.70212 avg_loss = 3.29125\n",
      "epoch no.4 train no.341340  loss = 3.33046 avg_loss = 3.30232\n",
      "epoch no.4 train no.341350  loss = 3.41613 avg_loss = 3.31678\n",
      "epoch no.4 train no.341360  loss = 4.02845 avg_loss = 3.33547\n",
      "epoch no.4 train no.341370  loss = 3.13463 avg_loss = 3.35075\n",
      "epoch no.4 train no.341380  loss = 3.53764 avg_loss = 3.35826\n",
      "epoch no.4 train no.341390  loss = 3.12063 avg_loss = 3.36221\n",
      "epoch no.4 train no.341400  loss = 3.42178 avg_loss = 3.33755\n",
      "epoch no.4 train no.341410  loss = 2.74915 avg_loss = 3.36117\n",
      "epoch no.4 train no.341420  loss = 5.17947 avg_loss = 3.37704\n",
      "epoch no.4 train no.341430  loss = 2.80856 avg_loss = 3.35523\n",
      "epoch no.4 train no.341440  loss = 1.78515 avg_loss = 3.33381\n",
      "epoch no.4 train no.341450  loss = 2.27122 avg_loss = 3.30907\n",
      "epoch no.4 train no.341460  loss = 1.73346 avg_loss = 3.25284\n",
      "epoch no.4 train no.341470  loss = 2.13235 avg_loss = 3.23981\n",
      "epoch no.4 train no.341480  loss = 2.69381 avg_loss = 3.28930\n",
      "epoch no.4 train no.341490  loss = 2.73127 avg_loss = 3.27445\n",
      "epoch no.4 train no.341500  loss = 2.22497 avg_loss = 3.27947\n",
      "epoch no.4 train no.341510  loss = 3.79098 avg_loss = 3.30498\n",
      "epoch no.4 train no.341520  loss = 2.55728 avg_loss = 3.26429\n",
      "epoch no.4 train no.341530  loss = 4.44795 avg_loss = 3.30359\n",
      "epoch no.4 train no.341540  loss = 2.62599 avg_loss = 3.36599\n",
      "epoch no.4 train no.341550  loss = 3.80795 avg_loss = 3.33474\n",
      "epoch no.4 train no.341560  loss = 2.45480 avg_loss = 3.29890\n",
      "epoch no.4 train no.341570  loss = 2.85477 avg_loss = 3.26672\n",
      "epoch no.4 train no.341580  loss = 3.26933 avg_loss = 3.25755\n",
      "epoch no.4 train no.341590  loss = 2.87267 avg_loss = 3.26699\n",
      "epoch no.4 train no.341600  loss = 2.35390 avg_loss = 3.26219\n",
      "epoch no.4 train no.341610  loss = 4.47426 avg_loss = 3.30701\n",
      "epoch no.4 train no.341620  loss = 2.12577 avg_loss = 3.28872\n",
      "epoch no.4 train no.341630  loss = 2.87272 avg_loss = 3.29666\n",
      "epoch no.4 train no.341640  loss = 2.73475 avg_loss = 3.23774\n",
      "epoch no.4 train no.341650  loss = 4.02714 avg_loss = 3.18818\n",
      "epoch no.4 train no.341660  loss = 3.16215 avg_loss = 3.18823\n",
      "epoch no.4 train no.341670  loss = 2.63986 avg_loss = 3.18902\n",
      "epoch no.4 train no.341680  loss = 2.34382 avg_loss = 3.14854\n",
      "epoch no.4 train no.341690  loss = 2.31359 avg_loss = 3.21525\n",
      "epoch no.4 train no.341700  loss = 1.99027 avg_loss = 3.20950\n",
      "epoch no.4 train no.341710  loss = 3.05663 avg_loss = 3.20072\n",
      "epoch no.4 train no.341720  loss = 3.18303 avg_loss = 3.21049\n",
      "epoch no.4 train no.341730  loss = 2.99640 avg_loss = 3.20583\n",
      "epoch no.4 train no.341740  loss = 3.34373 avg_loss = 3.19445\n",
      "epoch no.4 train no.341750  loss = 2.58300 avg_loss = 3.17235\n",
      "epoch no.4 train no.341760  loss = 3.58440 avg_loss = 3.20776\n",
      "epoch no.4 train no.341770  loss = 2.86487 avg_loss = 3.21765\n",
      "epoch no.4 train no.341780  loss = 3.54304 avg_loss = 3.18149\n",
      "epoch no.4 train no.341790  loss = 2.76239 avg_loss = 3.17433\n",
      "epoch no.4 train no.341800  loss = 2.55654 avg_loss = 3.15831\n",
      "epoch no.4 train no.341810  loss = 3.97445 avg_loss = 3.20139\n",
      "epoch no.4 train no.341820  loss = 3.22930 avg_loss = 3.21013\n",
      "epoch no.4 train no.341830  loss = 3.68201 avg_loss = 3.23830\n",
      "epoch no.4 train no.341840  loss = 3.45440 avg_loss = 3.25649\n",
      "epoch no.4 train no.341850  loss = 2.68733 avg_loss = 3.24246\n",
      "epoch no.4 train no.341860  loss = 4.87096 avg_loss = 3.23865\n",
      "epoch no.4 train no.341870  loss = 4.08388 avg_loss = 3.25428\n",
      "epoch no.4 train no.341880  loss = 2.79842 avg_loss = 3.26438\n",
      "epoch no.4 train no.341890  loss = 3.52313 avg_loss = 3.25214\n",
      "epoch no.4 train no.341900  loss = 2.79972 avg_loss = 3.28641\n",
      "epoch no.4 train no.341910  loss = 2.36364 avg_loss = 3.30641\n",
      "epoch no.4 train no.341920  loss = 2.99885 avg_loss = 3.29666\n",
      "epoch no.4 train no.341930  loss = 3.03371 avg_loss = 3.30309\n",
      "epoch no.4 train no.341940  loss = 1.94370 avg_loss = 3.27298\n",
      "epoch no.4 train no.341950  loss = 2.58597 avg_loss = 3.30944\n",
      "epoch no.4 train no.341960  loss = 4.11522 avg_loss = 3.29934\n",
      "epoch no.4 train no.341970  loss = 2.49059 avg_loss = 3.27585\n",
      "epoch no.4 train no.341980  loss = 2.05484 avg_loss = 3.22329\n",
      "epoch no.4 train no.341990  loss = 2.07876 avg_loss = 3.19104\n",
      "epoch no.4 train no.342000  loss = 3.06769 avg_loss = 3.20047\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '적인', '▁노래', '</s>', '</s>']\n",
      "여름밤에 어울리는 감성적인 노래들</s>\n",
      "epoch no.4 train no.342010  loss = 3.12808 avg_loss = 3.18519\n",
      "epoch no.4 train no.342020  loss = 4.38872 avg_loss = 3.21095\n",
      "epoch no.4 train no.342030  loss = 2.45452 avg_loss = 3.17193\n",
      "epoch no.4 train no.342040  loss = 3.35602 avg_loss = 3.14216\n",
      "epoch no.4 train no.342050  loss = 3.03531 avg_loss = 3.12752\n",
      "epoch no.4 train no.342060  loss = 4.05895 avg_loss = 3.17142\n",
      "epoch no.4 train no.342070  loss = 2.80554 avg_loss = 3.19015\n",
      "epoch no.4 train no.342080  loss = 2.50336 avg_loss = 3.16603\n",
      "epoch no.4 train no.342090  loss = 2.70545 avg_loss = 3.10770\n",
      "epoch no.4 train no.342100  loss = 3.82386 avg_loss = 3.13460\n",
      "epoch no.4 train no.342110  loss = 2.80208 avg_loss = 3.17768\n",
      "epoch no.4 train no.342120  loss = 4.19222 avg_loss = 3.18577\n",
      "epoch no.4 train no.342130  loss = 2.08118 avg_loss = 3.12631\n",
      "epoch no.4 train no.342140  loss = 3.72139 avg_loss = 3.12177\n",
      "epoch no.4 train no.342150  loss = 1.64580 avg_loss = 3.08929\n",
      "epoch no.4 train no.342160  loss = 3.89638 avg_loss = 3.08084\n",
      "epoch no.4 train no.342170  loss = 1.51933 avg_loss = 3.03678\n",
      "epoch no.4 train no.342180  loss = 2.70117 avg_loss = 3.04306\n",
      "epoch no.4 train no.342190  loss = 3.14977 avg_loss = 3.04504\n",
      "epoch no.4 train no.342200  loss = 3.20531 avg_loss = 3.04479\n",
      "epoch no.4 train no.342210  loss = 2.25324 avg_loss = 3.05224\n",
      "epoch no.4 train no.342220  loss = 4.31171 avg_loss = 3.02045\n",
      "epoch no.4 train no.342230  loss = 3.99102 avg_loss = 3.10411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.342240  loss = 2.48205 avg_loss = 3.08354\n",
      "epoch no.4 train no.342250  loss = 2.43906 avg_loss = 3.11268\n",
      "epoch no.4 train no.342260  loss = 3.77666 avg_loss = 3.11961\n",
      "epoch no.4 train no.342270  loss = 2.04320 avg_loss = 3.10178\n",
      "epoch no.4 train no.342280  loss = 3.16106 avg_loss = 3.14176\n",
      "epoch no.4 train no.342290  loss = 3.33459 avg_loss = 3.17728\n",
      "epoch no.4 train no.342300  loss = 4.19530 avg_loss = 3.22271\n",
      "epoch no.4 train no.342310  loss = 3.59969 avg_loss = 3.31885\n",
      "epoch no.4 train no.342320  loss = 2.39682 avg_loss = 3.27902\n",
      "epoch no.4 train no.342330  loss = 2.65375 avg_loss = 3.32741\n",
      "epoch no.4 train no.342340  loss = 3.70730 avg_loss = 3.36018\n",
      "epoch no.4 train no.342350  loss = 2.57374 avg_loss = 3.30680\n",
      "epoch no.4 train no.342360  loss = 4.47370 avg_loss = 3.31513\n",
      "epoch no.4 train no.342370  loss = 2.16839 avg_loss = 3.30351\n",
      "epoch no.4 train no.342380  loss = 2.35581 avg_loss = 3.27695\n",
      "epoch no.4 train no.342390  loss = 2.42204 avg_loss = 3.28776\n",
      "epoch no.4 train no.342400  loss = 4.66486 avg_loss = 3.25880\n",
      "epoch no.4 train no.342410  loss = 1.94803 avg_loss = 3.22184\n",
      "epoch no.4 train no.342420  loss = 1.91223 avg_loss = 3.17306\n",
      "epoch no.4 train no.342430  loss = 1.33666 avg_loss = 3.22859\n",
      "epoch no.4 train no.342440  loss = 3.57470 avg_loss = 3.24309\n",
      "epoch no.4 train no.342450  loss = 3.08470 avg_loss = 3.24803\n",
      "epoch no.4 train no.342460  loss = 4.45765 avg_loss = 3.23834\n",
      "epoch no.4 train no.342470  loss = 4.39455 avg_loss = 3.28384\n",
      "epoch no.4 train no.342480  loss = 3.36017 avg_loss = 3.30296\n",
      "epoch no.4 train no.342490  loss = 3.84490 avg_loss = 3.30836\n",
      "epoch no.4 train no.342500  loss = 2.83995 avg_loss = 3.26027\n",
      "epoch no.4 train no.342510  loss = 3.84711 avg_loss = 3.20209\n",
      "epoch no.4 train no.342520  loss = 3.22075 avg_loss = 3.19616\n",
      "epoch no.4 train no.342530  loss = 5.07639 avg_loss = 3.23631\n",
      "epoch no.4 train no.342540  loss = 3.16944 avg_loss = 3.26145\n",
      "epoch no.4 train no.342550  loss = 4.62866 avg_loss = 3.25504\n",
      "epoch no.4 train no.342560  loss = 2.70041 avg_loss = 3.21475\n",
      "epoch no.4 train no.342570  loss = 2.79811 avg_loss = 3.18480\n",
      "epoch no.4 train no.342580  loss = 3.18687 avg_loss = 3.17361\n",
      "epoch no.4 train no.342590  loss = 2.81645 avg_loss = 3.14400\n",
      "epoch no.4 train no.342600  loss = 2.34425 avg_loss = 3.11402\n",
      "epoch no.4 train no.342610  loss = 4.34951 avg_loss = 3.10267\n",
      "epoch no.4 train no.342620  loss = 4.34820 avg_loss = 3.14574\n",
      "epoch no.4 train no.342630  loss = 2.56502 avg_loss = 3.13842\n",
      "epoch no.4 train no.342640  loss = 3.95365 avg_loss = 3.16820\n",
      "epoch no.4 train no.342650  loss = 3.64810 avg_loss = 3.16155\n",
      "epoch no.4 train no.342660  loss = 2.67895 avg_loss = 3.12106\n",
      "epoch no.4 train no.342670  loss = 3.28518 avg_loss = 3.12570\n",
      "epoch no.4 train no.342680  loss = 2.54698 avg_loss = 3.10431\n",
      "epoch no.4 train no.342690  loss = 2.25010 avg_loss = 3.15002\n",
      "epoch no.4 train no.342700  loss = 3.19572 avg_loss = 3.17313\n",
      "epoch no.4 train no.342710  loss = 2.73068 avg_loss = 3.14935\n",
      "epoch no.4 train no.342720  loss = 2.29542 avg_loss = 3.11741\n",
      "epoch no.4 train no.342730  loss = 3.35563 avg_loss = 3.15243\n",
      "epoch no.4 train no.342740  loss = 4.61706 avg_loss = 3.21584\n",
      "epoch no.4 train no.342750  loss = 3.26362 avg_loss = 3.26867\n",
      "epoch no.4 train no.342760  loss = 2.72652 avg_loss = 3.22387\n",
      "epoch no.4 train no.342770  loss = 2.50697 avg_loss = 3.22367\n",
      "epoch no.4 train no.342780  loss = 2.04269 avg_loss = 3.20818\n",
      "epoch no.4 train no.342790  loss = 2.02628 avg_loss = 3.19081\n",
      "epoch no.4 train no.342800  loss = 3.73557 avg_loss = 3.18744\n",
      "epoch no.4 train no.342810  loss = 1.54102 avg_loss = 3.19288\n",
      "epoch no.4 train no.342820  loss = 2.50694 avg_loss = 3.19544\n",
      "epoch no.4 train no.342830  loss = 2.44266 avg_loss = 3.16504\n",
      "epoch no.4 train no.342840  loss = 3.71056 avg_loss = 3.16385\n",
      "epoch no.4 train no.342850  loss = 3.43751 avg_loss = 3.23771\n",
      "epoch no.4 train no.342860  loss = 3.14268 avg_loss = 3.20052\n",
      "epoch no.4 train no.342870  loss = 2.75371 avg_loss = 3.23935\n",
      "epoch no.4 train no.342880  loss = 3.27361 avg_loss = 3.29415\n",
      "epoch no.4 train no.342890  loss = 3.18961 avg_loss = 3.33578\n",
      "epoch no.4 train no.342900  loss = 1.92307 avg_loss = 3.31947\n",
      "epoch no.4 train no.342910  loss = 2.99740 avg_loss = 3.30715\n",
      "epoch no.4 train no.342920  loss = 3.11298 avg_loss = 3.28898\n",
      "epoch no.4 train no.342930  loss = 4.52722 avg_loss = 3.30190\n",
      "epoch no.4 train no.342940  loss = 3.62381 avg_loss = 3.35856\n",
      "epoch no.4 train no.342950  loss = 3.15567 avg_loss = 3.39575\n",
      "epoch no.4 train no.342960  loss = 3.42384 avg_loss = 3.32622\n",
      "epoch no.4 train no.342970  loss = 2.55966 avg_loss = 3.29784\n",
      "epoch no.4 train no.342980  loss = 3.48444 avg_loss = 3.26941\n",
      "epoch no.4 train no.342990  loss = 3.75075 avg_loss = 3.30210\n",
      "epoch no.4 train no.343000  loss = 1.89921 avg_loss = 3.23029\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '에', '▁듣는', '▁감성', '들']\n",
      "여름밤에 어울리는 음악</s>\n",
      "epoch no.4 train no.343010  loss = 2.03206 avg_loss = 3.22627\n",
      "epoch no.4 train no.343020  loss = 3.12126 avg_loss = 3.22276\n",
      "epoch no.4 train no.343030  loss = 2.69111 avg_loss = 3.21234\n",
      "epoch no.4 train no.343040  loss = 4.84275 avg_loss = 3.26630\n",
      "epoch no.4 train no.343050  loss = 4.11077 avg_loss = 3.29647\n",
      "epoch no.4 train no.343060  loss = 4.27750 avg_loss = 3.29424\n",
      "epoch no.4 train no.343070  loss = 2.99908 avg_loss = 3.27513\n",
      "epoch no.4 train no.343080  loss = 3.22913 avg_loss = 3.25870\n",
      "epoch no.4 train no.343090  loss = 4.69156 avg_loss = 3.26418\n",
      "epoch no.4 train no.343100  loss = 3.96923 avg_loss = 3.29102\n",
      "epoch no.4 train no.343110  loss = 2.83105 avg_loss = 3.26014\n",
      "epoch no.4 train no.343120  loss = 2.14596 avg_loss = 3.26112\n",
      "epoch no.4 train no.343130  loss = 2.44904 avg_loss = 3.23543\n",
      "epoch no.4 train no.343140  loss = 2.87058 avg_loss = 3.22872\n",
      "epoch no.4 train no.343150  loss = 3.24402 avg_loss = 3.23288\n",
      "epoch no.4 train no.343160  loss = 2.25517 avg_loss = 3.21578\n",
      "epoch no.4 train no.343170  loss = 3.29893 avg_loss = 3.27665\n",
      "epoch no.4 train no.343180  loss = 2.35720 avg_loss = 3.25925\n",
      "epoch no.4 train no.343190  loss = 2.64171 avg_loss = 3.24962\n",
      "epoch no.4 train no.343200  loss = 4.11705 avg_loss = 3.21891\n",
      "epoch no.4 train no.343210  loss = 4.30310 avg_loss = 3.25386\n",
      "epoch no.4 train no.343220  loss = 1.70415 avg_loss = 3.22428\n",
      "epoch no.4 train no.343230  loss = 3.10770 avg_loss = 3.21595\n",
      "epoch no.4 train no.343240  loss = 3.13778 avg_loss = 3.18809\n",
      "epoch no.4 train no.343250  loss = 2.29500 avg_loss = 3.18599\n",
      "epoch no.4 train no.343260  loss = 3.85140 avg_loss = 3.17882\n",
      "epoch no.4 train no.343270  loss = 2.48421 avg_loss = 3.14556\n",
      "epoch no.4 train no.343280  loss = 2.60715 avg_loss = 3.11950\n",
      "epoch no.4 train no.343290  loss = 2.37886 avg_loss = 3.14899\n",
      "epoch no.4 train no.343300  loss = 3.13301 avg_loss = 3.12233\n",
      "epoch no.4 train no.343310  loss = 2.74772 avg_loss = 3.10954\n",
      "epoch no.4 train no.343320  loss = 2.94762 avg_loss = 3.14114\n",
      "epoch no.4 train no.343330  loss = 4.20086 avg_loss = 3.16633\n",
      "epoch no.4 train no.343340  loss = 3.50369 avg_loss = 3.14782\n",
      "epoch no.4 train no.343350  loss = 2.99719 avg_loss = 3.17224\n",
      "epoch no.4 train no.343360  loss = 4.28825 avg_loss = 3.15548\n",
      "epoch no.4 train no.343370  loss = 4.39951 avg_loss = 3.14031\n",
      "epoch no.4 train no.343380  loss = 3.06481 avg_loss = 3.15156\n",
      "epoch no.4 train no.343390  loss = 3.59112 avg_loss = 3.14366\n",
      "epoch no.4 train no.343400  loss = 4.01437 avg_loss = 3.19110\n",
      "epoch no.4 train no.343410  loss = 4.27910 avg_loss = 3.18640\n",
      "epoch no.4 train no.343420  loss = 2.95593 avg_loss = 3.15746\n",
      "epoch no.4 train no.343430  loss = 2.42411 avg_loss = 3.16988\n",
      "epoch no.4 train no.343440  loss = 2.41435 avg_loss = 3.18280\n",
      "epoch no.4 train no.343450  loss = 4.34784 avg_loss = 3.19804\n",
      "epoch no.4 train no.343460  loss = 3.13317 avg_loss = 3.19397\n",
      "epoch no.4 train no.343470  loss = 2.72780 avg_loss = 3.19672\n",
      "epoch no.4 train no.343480  loss = 2.89337 avg_loss = 3.17877\n",
      "epoch no.4 train no.343490  loss = 2.78693 avg_loss = 3.15411\n",
      "epoch no.4 train no.343500  loss = 3.87477 avg_loss = 3.14832\n",
      "epoch no.4 train no.343510  loss = 2.46726 avg_loss = 3.10897\n",
      "epoch no.4 train no.343520  loss = 3.96417 avg_loss = 3.13245\n",
      "epoch no.4 train no.343530  loss = 3.40305 avg_loss = 3.20821\n",
      "epoch no.4 train no.343540  loss = 1.17641 avg_loss = 3.17899\n",
      "epoch no.4 train no.343550  loss = 3.54287 avg_loss = 3.20160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.343560  loss = 3.33612 avg_loss = 3.23156\n",
      "epoch no.4 train no.343570  loss = 3.78667 avg_loss = 3.24729\n",
      "epoch no.4 train no.343580  loss = 3.67280 avg_loss = 3.24677\n",
      "epoch no.4 train no.343590  loss = 5.47602 avg_loss = 3.31415\n",
      "epoch no.4 train no.343600  loss = 2.60709 avg_loss = 3.27025\n",
      "epoch no.4 train no.343610  loss = 3.54920 avg_loss = 3.27000\n",
      "epoch no.4 train no.343620  loss = 3.93755 avg_loss = 3.30696\n",
      "epoch no.4 train no.343630  loss = 2.34975 avg_loss = 3.34183\n",
      "epoch no.4 train no.343640  loss = 2.61404 avg_loss = 3.29346\n",
      "epoch no.4 train no.343650  loss = 3.13045 avg_loss = 3.29573\n",
      "epoch no.4 train no.343660  loss = 1.87921 avg_loss = 3.28679\n",
      "epoch no.4 train no.343670  loss = 3.09684 avg_loss = 3.31964\n",
      "epoch no.4 train no.343680  loss = 2.27459 avg_loss = 3.26325\n",
      "epoch no.4 train no.343690  loss = 4.62468 avg_loss = 3.25638\n",
      "epoch no.4 train no.343700  loss = 2.90983 avg_loss = 3.26958\n",
      "epoch no.4 train no.343710  loss = 2.01643 avg_loss = 3.23995\n",
      "epoch no.4 train no.343720  loss = 2.71018 avg_loss = 3.25920\n",
      "epoch no.4 train no.343730  loss = 3.27609 avg_loss = 3.27209\n",
      "epoch no.4 train no.343740  loss = 4.11119 avg_loss = 3.32294\n",
      "epoch no.4 train no.343750  loss = 2.77217 avg_loss = 3.28681\n",
      "epoch no.4 train no.343760  loss = 3.30036 avg_loss = 3.29225\n",
      "epoch no.4 train no.343770  loss = 2.99726 avg_loss = 3.28168\n",
      "epoch no.4 train no.343780  loss = 2.32871 avg_loss = 3.32317\n",
      "epoch no.4 train no.343790  loss = 3.80583 avg_loss = 3.30466\n",
      "epoch no.4 train no.343800  loss = 3.36227 avg_loss = 3.30232\n",
      "epoch no.4 train no.343810  loss = 1.89591 avg_loss = 3.28532\n",
      "epoch no.4 train no.343820  loss = 2.15814 avg_loss = 3.25088\n",
      "epoch no.4 train no.343830  loss = 2.98706 avg_loss = 3.23168\n",
      "epoch no.4 train no.343840  loss = 3.24551 avg_loss = 3.21909\n",
      "epoch no.4 train no.343850  loss = 3.29823 avg_loss = 3.15358\n",
      "epoch no.4 train no.343860  loss = 3.38115 avg_loss = 3.15608\n",
      "epoch no.4 train no.343870  loss = 2.98460 avg_loss = 3.16911\n",
      "epoch no.4 train no.343880  loss = 4.00067 avg_loss = 3.15997\n",
      "epoch no.4 train no.343890  loss = 3.95129 avg_loss = 3.20177\n",
      "epoch no.4 train no.343900  loss = 2.72511 avg_loss = 3.24677\n",
      "epoch no.4 train no.343910  loss = 2.43069 avg_loss = 3.24430\n",
      "epoch no.4 train no.343920  loss = 1.93718 avg_loss = 3.22658\n",
      "epoch no.4 train no.343930  loss = 2.49235 avg_loss = 3.27063\n",
      "epoch no.4 train no.343940  loss = 3.18689 avg_loss = 3.24737\n",
      "epoch no.4 train no.343950  loss = 4.94984 avg_loss = 3.28212\n",
      "epoch no.4 train no.343960  loss = 2.91400 avg_loss = 3.25663\n",
      "epoch no.4 train no.343970  loss = 2.85229 avg_loss = 3.20663\n",
      "epoch no.4 train no.343980  loss = 2.00043 avg_loss = 3.16077\n",
      "epoch no.4 train no.343990  loss = 2.04266 avg_loss = 3.17874\n",
      "epoch no.4 train no.344000  loss = 3.86370 avg_loss = 3.17824\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '적인', '리스트', '</s>']\n",
      "여름밤에 어울리는 감성 플레이리스트</s>\n",
      "epoch no.4 train no.344010  loss = 3.11529 avg_loss = 3.20858\n",
      "epoch no.4 train no.344020  loss = 2.61263 avg_loss = 3.20023\n",
      "epoch no.4 train no.344030  loss = 3.93457 avg_loss = 3.17186\n",
      "epoch no.4 train no.344040  loss = 4.66730 avg_loss = 3.17384\n",
      "epoch no.4 train no.344050  loss = 2.79666 avg_loss = 3.19616\n",
      "epoch no.4 train no.344060  loss = 1.76352 avg_loss = 3.17120\n",
      "epoch no.4 train no.344070  loss = 3.72249 avg_loss = 3.20387\n",
      "epoch no.4 train no.344080  loss = 2.99892 avg_loss = 3.24695\n",
      "epoch no.4 train no.344090  loss = 5.63237 avg_loss = 3.29496\n",
      "epoch no.4 train no.344100  loss = 1.61307 avg_loss = 3.25721\n",
      "epoch no.4 train no.344110  loss = 3.50077 avg_loss = 3.26244\n",
      "epoch no.4 train no.344120  loss = 3.82973 avg_loss = 3.28765\n",
      "epoch no.4 train no.344130  loss = 3.24349 avg_loss = 3.25230\n",
      "epoch no.4 train no.344140  loss = 2.68110 avg_loss = 3.24444\n",
      "epoch no.4 train no.344150  loss = 2.86661 avg_loss = 3.21774\n",
      "epoch no.4 train no.344160  loss = 2.40426 avg_loss = 3.19368\n",
      "epoch no.4 train no.344170  loss = 1.89687 avg_loss = 3.20521\n",
      "epoch no.4 train no.344180  loss = 3.20457 avg_loss = 3.19908\n",
      "epoch no.4 train no.344190  loss = 4.20460 avg_loss = 3.21529\n",
      "epoch no.4 train no.344200  loss = 5.57301 avg_loss = 3.23933\n",
      "epoch no.4 train no.344210  loss = 3.06179 avg_loss = 3.20500\n",
      "epoch no.4 train no.344220  loss = 3.22946 avg_loss = 3.19433\n",
      "epoch no.4 train no.344230  loss = 4.03623 avg_loss = 3.17975\n",
      "epoch no.4 train no.344240  loss = 1.70623 avg_loss = 3.16446\n",
      "epoch no.4 train no.344250  loss = 3.64342 avg_loss = 3.14430\n",
      "epoch no.4 train no.344260  loss = 4.36920 avg_loss = 3.16875\n",
      "epoch no.4 train no.344270  loss = 3.33703 avg_loss = 3.16145\n",
      "epoch no.4 train no.344280  loss = 2.40283 avg_loss = 3.17276\n",
      "epoch no.4 train no.344290  loss = 1.07586 avg_loss = 3.18078\n",
      "epoch no.4 train no.344300  loss = 2.67397 avg_loss = 3.16423\n",
      "epoch no.4 train no.344310  loss = 1.71743 avg_loss = 3.15142\n",
      "epoch no.4 train no.344320  loss = 3.68473 avg_loss = 3.21378\n",
      "epoch no.4 train no.344330  loss = 2.67368 avg_loss = 3.19343\n",
      "epoch no.4 train no.344340  loss = 2.20398 avg_loss = 3.23580\n",
      "epoch no.4 train no.344350  loss = 2.69341 avg_loss = 3.19983\n",
      "epoch no.4 train no.344360  loss = 2.49509 avg_loss = 3.22271\n",
      "epoch no.4 train no.344370  loss = 2.05835 avg_loss = 3.24618\n",
      "epoch no.4 train no.344380  loss = 2.59939 avg_loss = 3.23132\n",
      "epoch no.4 train no.344390  loss = 3.68126 avg_loss = 3.24409\n",
      "epoch no.4 train no.344400  loss = 2.50174 avg_loss = 3.21921\n",
      "epoch no.4 train no.344410  loss = 2.58419 avg_loss = 3.20246\n",
      "epoch no.4 train no.344420  loss = 4.90676 avg_loss = 3.22681\n",
      "epoch no.4 train no.344430  loss = 2.67525 avg_loss = 3.25540\n",
      "epoch no.4 train no.344440  loss = 2.95421 avg_loss = 3.25537\n",
      "epoch no.4 train no.344450  loss = 1.59999 avg_loss = 3.24849\n",
      "epoch no.4 train no.344460  loss = 1.63870 avg_loss = 3.23928\n",
      "epoch no.4 train no.344470  loss = 3.57669 avg_loss = 3.22935\n",
      "epoch no.4 train no.344480  loss = 2.70170 avg_loss = 3.21087\n",
      "epoch no.4 train no.344490  loss = 2.48535 avg_loss = 3.15364\n",
      "epoch no.4 train no.344500  loss = 2.49830 avg_loss = 3.16538\n",
      "epoch no.4 train no.344510  loss = 2.79955 avg_loss = 3.12450\n",
      "epoch no.4 train no.344520  loss = 2.88090 avg_loss = 3.16056\n",
      "epoch no.4 train no.344530  loss = 2.49360 avg_loss = 3.16328\n",
      "epoch no.4 train no.344540  loss = 3.54237 avg_loss = 3.13342\n",
      "epoch no.4 train no.344550  loss = 2.88771 avg_loss = 3.16448\n",
      "epoch no.4 train no.344560  loss = 2.71036 avg_loss = 3.16047\n",
      "epoch no.4 train no.344570  loss = 2.91645 avg_loss = 3.15137\n",
      "epoch no.4 train no.344580  loss = 4.18742 avg_loss = 3.12379\n",
      "epoch no.4 train no.344590  loss = 2.89674 avg_loss = 3.17634\n",
      "epoch no.4 train no.344600  loss = 2.47071 avg_loss = 3.17756\n",
      "epoch no.4 train no.344610  loss = 3.39302 avg_loss = 3.16944\n",
      "epoch no.4 train no.344620  loss = 5.08525 avg_loss = 3.20240\n",
      "epoch no.4 train no.344630  loss = 3.54871 avg_loss = 3.21532\n",
      "epoch no.4 train no.344640  loss = 3.58321 avg_loss = 3.22783\n",
      "epoch no.4 train no.344650  loss = 3.29999 avg_loss = 3.25588\n",
      "epoch no.4 train no.344660  loss = 4.65118 avg_loss = 3.25804\n",
      "epoch no.4 train no.344670  loss = 1.45780 avg_loss = 3.23871\n",
      "epoch no.4 train no.344680  loss = 3.43023 avg_loss = 3.25323\n",
      "epoch no.4 train no.344690  loss = 2.10360 avg_loss = 3.19986\n",
      "epoch no.4 train no.344700  loss = 3.41333 avg_loss = 3.18586\n",
      "epoch no.4 train no.344710  loss = 2.81930 avg_loss = 3.10172\n",
      "epoch no.4 train no.344720  loss = 3.28284 avg_loss = 3.12246\n",
      "epoch no.4 train no.344730  loss = 2.76824 avg_loss = 3.10354\n",
      "epoch no.4 train no.344740  loss = 5.77653 avg_loss = 3.13560\n",
      "epoch no.4 train no.344750  loss = 2.57826 avg_loss = 3.14795\n",
      "epoch no.4 train no.344760  loss = 4.81734 avg_loss = 3.18362\n",
      "epoch no.4 train no.344770  loss = 4.30234 avg_loss = 3.16276\n",
      "epoch no.4 train no.344780  loss = 2.56749 avg_loss = 3.17638\n",
      "epoch no.4 train no.344790  loss = 3.61280 avg_loss = 3.22238\n",
      "epoch no.4 train no.344800  loss = 2.87463 avg_loss = 3.20235\n",
      "epoch no.4 train no.344810  loss = 3.74755 avg_loss = 3.16196\n",
      "epoch no.4 train no.344820  loss = 3.27810 avg_loss = 3.13823\n",
      "epoch no.4 train no.344830  loss = 3.58730 avg_loss = 3.14637\n",
      "epoch no.4 train no.344840  loss = 2.56475 avg_loss = 3.14779\n",
      "epoch no.4 train no.344850  loss = 3.49837 avg_loss = 3.09244\n",
      "epoch no.4 train no.344860  loss = 4.57131 avg_loss = 3.10225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.344870  loss = 2.67471 avg_loss = 3.08128\n",
      "epoch no.4 train no.344880  loss = 2.42064 avg_loss = 3.06775\n",
      "epoch no.4 train no.344890  loss = 1.78529 avg_loss = 3.08162\n",
      "epoch no.4 train no.344900  loss = 3.52458 avg_loss = 3.11856\n",
      "epoch no.4 train no.344910  loss = 2.97001 avg_loss = 3.11626\n",
      "epoch no.4 train no.344920  loss = 3.15535 avg_loss = 3.09124\n",
      "epoch no.4 train no.344930  loss = 2.70370 avg_loss = 3.15771\n",
      "epoch no.4 train no.344940  loss = 3.09115 avg_loss = 3.16444\n",
      "epoch no.4 train no.344950  loss = 3.09862 avg_loss = 3.09572\n",
      "epoch no.4 train no.344960  loss = 1.60597 avg_loss = 3.09190\n",
      "epoch no.4 train no.344970  loss = 3.22183 avg_loss = 3.12036\n",
      "epoch no.4 train no.344980  loss = 1.92813 avg_loss = 3.13873\n",
      "epoch no.4 train no.344990  loss = 4.20144 avg_loss = 3.18514\n",
      "epoch no.4 train no.345000  loss = 2.66639 avg_loss = 3.19851\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.345010  loss = 2.47783 avg_loss = 3.21181\n",
      "epoch no.4 train no.345020  loss = 3.69604 avg_loss = 3.20619\n",
      "epoch no.4 train no.345030  loss = 4.44417 avg_loss = 3.21730\n",
      "epoch no.4 train no.345040  loss = 2.60788 avg_loss = 3.16783\n",
      "epoch no.4 train no.345050  loss = 2.11719 avg_loss = 3.15006\n",
      "epoch no.4 train no.345060  loss = 2.86378 avg_loss = 3.14265\n",
      "epoch no.4 train no.345070  loss = 1.51916 avg_loss = 3.13648\n",
      "epoch no.4 train no.345080  loss = 3.79658 avg_loss = 3.12391\n",
      "epoch no.4 train no.345090  loss = 4.54850 avg_loss = 3.17638\n",
      "epoch no.4 train no.345100  loss = 2.63327 avg_loss = 3.18794\n",
      "epoch no.4 train no.345110  loss = 3.61163 avg_loss = 3.19127\n",
      "epoch no.4 train no.345120  loss = 2.72701 avg_loss = 3.18264\n",
      "epoch no.4 train no.345130  loss = 3.79917 avg_loss = 3.20720\n",
      "epoch no.4 train no.345140  loss = 2.43850 avg_loss = 3.19073\n",
      "epoch no.4 train no.345150  loss = 2.28078 avg_loss = 3.15823\n",
      "epoch no.4 train no.345160  loss = 2.18733 avg_loss = 3.11234\n",
      "epoch no.4 train no.345170  loss = 2.87343 avg_loss = 3.09840\n",
      "epoch no.4 train no.345180  loss = 3.73411 avg_loss = 3.16372\n",
      "epoch no.4 train no.345190  loss = 2.88844 avg_loss = 3.12729\n",
      "epoch no.4 train no.345200  loss = 2.48985 avg_loss = 3.15376\n",
      "epoch no.4 train no.345210  loss = 2.89107 avg_loss = 3.15792\n",
      "epoch no.4 train no.345220  loss = 4.85865 avg_loss = 3.18949\n",
      "epoch no.4 train no.345230  loss = 0.95131 avg_loss = 3.18353\n",
      "epoch no.4 train no.345240  loss = 2.85647 avg_loss = 3.17861\n",
      "epoch no.4 train no.345250  loss = 3.31032 avg_loss = 3.15856\n",
      "epoch no.4 train no.345260  loss = 3.18469 avg_loss = 3.12655\n",
      "epoch no.4 train no.345270  loss = 2.65713 avg_loss = 3.15245\n",
      "epoch no.4 train no.345280  loss = 2.55502 avg_loss = 3.14854\n",
      "epoch no.4 train no.345290  loss = 2.63953 avg_loss = 3.15386\n",
      "epoch no.4 train no.345300  loss = 2.29647 avg_loss = 3.19688\n",
      "epoch no.4 train no.345310  loss = 3.54442 avg_loss = 3.24117\n",
      "epoch no.4 train no.345320  loss = 2.87817 avg_loss = 3.22382\n",
      "epoch no.4 train no.345330  loss = 3.62473 avg_loss = 3.25735\n",
      "epoch no.4 train no.345340  loss = 3.12179 avg_loss = 3.24057\n",
      "epoch no.4 train no.345350  loss = 5.89641 avg_loss = 3.28889\n",
      "epoch no.4 train no.345360  loss = 2.25654 avg_loss = 3.22707\n",
      "epoch no.4 train no.345370  loss = 2.95446 avg_loss = 3.19151\n",
      "epoch no.4 train no.345380  loss = 3.15839 avg_loss = 3.19429\n",
      "epoch no.4 train no.345390  loss = 3.46184 avg_loss = 3.16778\n",
      "epoch no.4 train no.345400  loss = 3.40675 avg_loss = 3.18308\n",
      "epoch no.4 train no.345410  loss = 4.19814 avg_loss = 3.26074\n",
      "epoch no.4 train no.345420  loss = 3.32382 avg_loss = 3.24806\n",
      "epoch no.4 train no.345430  loss = 2.66328 avg_loss = 3.24406\n",
      "epoch no.4 train no.345440  loss = 2.48116 avg_loss = 3.20268\n",
      "epoch no.4 train no.345450  loss = 4.64579 avg_loss = 3.22022\n",
      "epoch no.4 train no.345460  loss = 2.82607 avg_loss = 3.20417\n",
      "epoch no.4 train no.345470  loss = 3.55022 avg_loss = 3.19117\n",
      "epoch no.4 train no.345480  loss = 4.44204 avg_loss = 3.19640\n",
      "epoch no.4 train no.345490  loss = 3.22978 avg_loss = 3.20749\n",
      "epoch no.4 train no.345500  loss = 3.36633 avg_loss = 3.21732\n",
      "epoch no.4 train no.345510  loss = 2.48984 avg_loss = 3.20236\n",
      "epoch no.4 train no.345520  loss = 3.37263 avg_loss = 3.22933\n",
      "epoch no.4 train no.345530  loss = 4.54483 avg_loss = 3.26298\n",
      "epoch no.4 train no.345540  loss = 3.57785 avg_loss = 3.26272\n",
      "epoch no.4 train no.345550  loss = 2.74904 avg_loss = 3.27014\n",
      "epoch no.4 train no.345560  loss = 3.44252 avg_loss = 3.30148\n",
      "epoch no.4 train no.345570  loss = 3.33591 avg_loss = 3.28755\n",
      "epoch no.4 train no.345580  loss = 2.44006 avg_loss = 3.28878\n",
      "epoch no.4 train no.345590  loss = 2.15355 avg_loss = 3.26533\n",
      "epoch no.4 train no.345600  loss = 3.31240 avg_loss = 3.26211\n",
      "epoch no.4 train no.345610  loss = 2.65380 avg_loss = 3.27370\n",
      "epoch no.4 train no.345620  loss = 3.22921 avg_loss = 3.28179\n",
      "epoch no.4 train no.345630  loss = 2.90761 avg_loss = 3.27857\n",
      "epoch no.4 train no.345640  loss = 2.38042 avg_loss = 3.27807\n",
      "epoch no.4 train no.345650  loss = 2.92947 avg_loss = 3.23605\n",
      "epoch no.4 train no.345660  loss = 3.20912 avg_loss = 3.21688\n",
      "epoch no.4 train no.345670  loss = 3.69673 avg_loss = 3.27282\n",
      "epoch no.4 train no.345680  loss = 4.84981 avg_loss = 3.32838\n",
      "epoch no.4 train no.345690  loss = 3.84898 avg_loss = 3.32977\n",
      "epoch no.4 train no.345700  loss = 4.66152 avg_loss = 3.27634\n",
      "epoch no.4 train no.345710  loss = 3.42712 avg_loss = 3.26710\n",
      "epoch no.4 train no.345720  loss = 2.81043 avg_loss = 3.23394\n",
      "epoch no.4 train no.345730  loss = 2.50356 avg_loss = 3.19683\n",
      "epoch no.4 train no.345740  loss = 3.38320 avg_loss = 3.23376\n",
      "epoch no.4 train no.345750  loss = 4.11410 avg_loss = 3.27339\n",
      "epoch no.4 train no.345760  loss = 3.91237 avg_loss = 3.29713\n",
      "epoch no.4 train no.345770  loss = 3.97374 avg_loss = 3.28613\n",
      "epoch no.4 train no.345780  loss = 1.90573 avg_loss = 3.29482\n",
      "epoch no.4 train no.345790  loss = 3.30236 avg_loss = 3.26139\n",
      "epoch no.4 train no.345800  loss = 2.21978 avg_loss = 3.24551\n",
      "epoch no.4 train no.345810  loss = 4.28318 avg_loss = 3.20953\n",
      "epoch no.4 train no.345820  loss = 2.61954 avg_loss = 3.17106\n",
      "epoch no.4 train no.345830  loss = 2.18467 avg_loss = 3.16108\n",
      "epoch no.4 train no.345840  loss = 3.67975 avg_loss = 3.13437\n",
      "epoch no.4 train no.345850  loss = 3.95881 avg_loss = 3.18813\n",
      "epoch no.4 train no.345860  loss = 5.32743 avg_loss = 3.17902\n",
      "epoch no.4 train no.345870  loss = 2.77479 avg_loss = 3.14772\n",
      "epoch no.4 train no.345880  loss = 4.40339 avg_loss = 3.15627\n",
      "epoch no.4 train no.345890  loss = 4.56425 avg_loss = 3.18869\n",
      "epoch no.4 train no.345900  loss = 2.15152 avg_loss = 3.17490\n",
      "epoch no.4 train no.345910  loss = 2.37196 avg_loss = 3.11929\n",
      "epoch no.4 train no.345920  loss = 3.35161 avg_loss = 3.15006\n",
      "epoch no.4 train no.345930  loss = 3.28423 avg_loss = 3.19234\n",
      "epoch no.4 train no.345940  loss = 3.85448 avg_loss = 3.22061\n",
      "epoch no.4 train no.345950  loss = 3.88701 avg_loss = 3.22405\n",
      "epoch no.4 train no.345960  loss = 3.91708 avg_loss = 3.20450\n",
      "epoch no.4 train no.345970  loss = 2.56980 avg_loss = 3.18320\n",
      "epoch no.4 train no.345980  loss = 3.72184 avg_loss = 3.14886\n",
      "epoch no.4 train no.345990  loss = 2.63622 avg_loss = 3.14558\n",
      "epoch no.4 train no.346000  loss = 4.11219 avg_loss = 3.16636\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁노래', '</s>', '</s>']\n",
      "여름밤에 어울리는 클래식 음악</s>\n",
      "epoch no.4 train no.346010  loss = 2.59948 avg_loss = 3.17349\n",
      "epoch no.4 train no.346020  loss = 1.88087 avg_loss = 3.17115\n",
      "epoch no.4 train no.346030  loss = 4.00812 avg_loss = 3.18295\n",
      "epoch no.4 train no.346040  loss = 2.27372 avg_loss = 3.21687\n",
      "epoch no.4 train no.346050  loss = 2.28133 avg_loss = 3.16867\n",
      "epoch no.4 train no.346060  loss = 2.95896 avg_loss = 3.17785\n",
      "epoch no.4 train no.346070  loss = 5.44379 avg_loss = 3.19184\n",
      "epoch no.4 train no.346080  loss = 5.29606 avg_loss = 3.22605\n",
      "epoch no.4 train no.346090  loss = 1.72833 avg_loss = 3.24045\n",
      "epoch no.4 train no.346100  loss = 4.25837 avg_loss = 3.24470\n",
      "epoch no.4 train no.346110  loss = 4.22498 avg_loss = 3.26064\n",
      "epoch no.4 train no.346120  loss = 4.80270 avg_loss = 3.26739\n",
      "epoch no.4 train no.346130  loss = 2.15086 avg_loss = 3.24615\n",
      "epoch no.4 train no.346140  loss = 1.96639 avg_loss = 3.21124\n",
      "epoch no.4 train no.346150  loss = 2.85265 avg_loss = 3.21331\n",
      "epoch no.4 train no.346160  loss = 4.71953 avg_loss = 3.19290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.346170  loss = 2.47269 avg_loss = 3.15625\n",
      "epoch no.4 train no.346180  loss = 3.08840 avg_loss = 3.18417\n",
      "epoch no.4 train no.346190  loss = 2.83923 avg_loss = 3.20424\n",
      "epoch no.4 train no.346200  loss = 2.94601 avg_loss = 3.23056\n",
      "epoch no.4 train no.346210  loss = 2.88007 avg_loss = 3.23589\n",
      "epoch no.4 train no.346220  loss = 3.06494 avg_loss = 3.22081\n",
      "epoch no.4 train no.346230  loss = 2.94486 avg_loss = 3.19273\n",
      "epoch no.4 train no.346240  loss = 4.38561 avg_loss = 3.20244\n",
      "epoch no.4 train no.346250  loss = 2.69100 avg_loss = 3.22106\n",
      "epoch no.4 train no.346260  loss = 4.19413 avg_loss = 3.24541\n",
      "epoch no.4 train no.346270  loss = 3.37887 avg_loss = 3.31374\n",
      "epoch no.4 train no.346280  loss = 3.27775 avg_loss = 3.35209\n",
      "epoch no.4 train no.346290  loss = 3.87702 avg_loss = 3.38491\n",
      "epoch no.4 train no.346300  loss = 2.98173 avg_loss = 3.37765\n",
      "epoch no.4 train no.346310  loss = 4.77496 avg_loss = 3.37259\n",
      "epoch no.4 train no.346320  loss = 2.87742 avg_loss = 3.33163\n",
      "epoch no.4 train no.346330  loss = 2.25241 avg_loss = 3.33054\n",
      "epoch no.4 train no.346340  loss = 2.90868 avg_loss = 3.33777\n",
      "epoch no.4 train no.346350  loss = 5.81710 avg_loss = 3.31249\n",
      "epoch no.4 train no.346360  loss = 3.35268 avg_loss = 3.29157\n",
      "epoch no.4 train no.346370  loss = 2.59442 avg_loss = 3.26198\n",
      "epoch no.4 train no.346380  loss = 1.97841 avg_loss = 3.23715\n",
      "epoch no.4 train no.346390  loss = 2.62305 avg_loss = 3.21550\n",
      "epoch no.4 train no.346400  loss = 2.73280 avg_loss = 3.22286\n",
      "epoch no.4 train no.346410  loss = 2.93913 avg_loss = 3.22340\n",
      "epoch no.4 train no.346420  loss = 2.80910 avg_loss = 3.19894\n",
      "epoch no.4 train no.346430  loss = 3.61285 avg_loss = 3.19700\n",
      "epoch no.4 train no.346440  loss = 2.94447 avg_loss = 3.18818\n",
      "epoch no.4 train no.346450  loss = 2.79856 avg_loss = 3.14217\n",
      "epoch no.4 train no.346460  loss = 6.07589 avg_loss = 3.16377\n",
      "epoch no.4 train no.346470  loss = 2.40524 avg_loss = 3.18208\n",
      "epoch no.4 train no.346480  loss = 3.47708 avg_loss = 3.16599\n",
      "epoch no.4 train no.346490  loss = 2.37295 avg_loss = 3.20471\n",
      "epoch no.4 train no.346500  loss = 3.31528 avg_loss = 3.19720\n",
      "epoch no.4 train no.346510  loss = 4.55372 avg_loss = 3.18130\n",
      "epoch no.4 train no.346520  loss = 4.48699 avg_loss = 3.20980\n",
      "epoch no.4 train no.346530  loss = 2.39182 avg_loss = 3.19397\n",
      "epoch no.4 train no.346540  loss = 3.95792 avg_loss = 3.22481\n",
      "epoch no.4 train no.346550  loss = 2.48970 avg_loss = 3.20822\n",
      "epoch no.4 train no.346560  loss = 2.65088 avg_loss = 3.18567\n",
      "epoch no.4 train no.346570  loss = 2.91823 avg_loss = 3.18899\n",
      "epoch no.4 train no.346580  loss = 3.84575 avg_loss = 3.20866\n",
      "epoch no.4 train no.346590  loss = 5.14608 avg_loss = 3.23009\n",
      "epoch no.4 train no.346600  loss = 3.45157 avg_loss = 3.22632\n",
      "epoch no.4 train no.346610  loss = 3.12214 avg_loss = 3.22246\n",
      "epoch no.4 train no.346620  loss = 4.96928 avg_loss = 3.23071\n",
      "epoch no.4 train no.346630  loss = 4.08737 avg_loss = 3.24632\n",
      "epoch no.4 train no.346640  loss = 2.51567 avg_loss = 3.24847\n",
      "epoch no.4 train no.346650  loss = 3.59708 avg_loss = 3.25176\n",
      "epoch no.4 train no.346660  loss = 2.70174 avg_loss = 3.23545\n",
      "epoch no.4 train no.346670  loss = 4.45847 avg_loss = 3.25500\n",
      "epoch no.4 train no.346680  loss = 3.13515 avg_loss = 3.26923\n",
      "epoch no.4 train no.346690  loss = 3.33018 avg_loss = 3.27647\n",
      "epoch no.4 train no.346700  loss = 2.58778 avg_loss = 3.26235\n",
      "epoch no.4 train no.346710  loss = 5.90445 avg_loss = 3.27439\n",
      "epoch no.4 train no.346720  loss = 2.75550 avg_loss = 3.23869\n",
      "epoch no.4 train no.346730  loss = 4.37404 avg_loss = 3.21164\n",
      "epoch no.4 train no.346740  loss = 2.83875 avg_loss = 3.23029\n",
      "epoch no.4 train no.346750  loss = 3.60943 avg_loss = 3.23415\n",
      "epoch no.4 train no.346760  loss = 4.66229 avg_loss = 3.19976\n",
      "epoch no.4 train no.346770  loss = 3.11882 avg_loss = 3.22033\n",
      "epoch no.4 train no.346780  loss = 3.67120 avg_loss = 3.19592\n",
      "epoch no.4 train no.346790  loss = 3.59490 avg_loss = 3.20825\n",
      "epoch no.4 train no.346800  loss = 4.85651 avg_loss = 3.21301\n",
      "epoch no.4 train no.346810  loss = 3.22017 avg_loss = 3.22821\n",
      "epoch no.4 train no.346820  loss = 2.89919 avg_loss = 3.24696\n",
      "epoch no.4 train no.346830  loss = 2.52656 avg_loss = 3.25769\n",
      "epoch no.4 train no.346840  loss = 4.39782 avg_loss = 3.30218\n",
      "epoch no.4 train no.346850  loss = 2.64587 avg_loss = 3.28903\n",
      "epoch no.4 train no.346860  loss = 2.96737 avg_loss = 3.26902\n",
      "epoch no.4 train no.346870  loss = 2.97168 avg_loss = 3.23285\n",
      "epoch no.4 train no.346880  loss = 2.46627 avg_loss = 3.23836\n",
      "epoch no.4 train no.346890  loss = 2.12085 avg_loss = 3.27523\n",
      "epoch no.4 train no.346900  loss = 2.99040 avg_loss = 3.30849\n",
      "epoch no.4 train no.346910  loss = 4.05725 avg_loss = 3.33575\n",
      "epoch no.4 train no.346920  loss = 3.73102 avg_loss = 3.34495\n",
      "epoch no.4 train no.346930  loss = 4.42876 avg_loss = 3.36817\n",
      "epoch no.4 train no.346940  loss = 2.14523 avg_loss = 3.37056\n",
      "epoch no.4 train no.346950  loss = 2.92697 avg_loss = 3.33663\n",
      "epoch no.4 train no.346960  loss = 2.80511 avg_loss = 3.34995\n",
      "epoch no.4 train no.346970  loss = 2.79753 avg_loss = 3.40090\n",
      "epoch no.4 train no.346980  loss = 2.75260 avg_loss = 3.37820\n",
      "epoch no.4 train no.346990  loss = 1.19651 avg_loss = 3.34835\n",
      "epoch no.4 train no.347000  loss = 1.81166 avg_loss = 3.33068\n",
      "9\n",
      "to_tokens: ['▁비', '밤', '에', '▁적', '놓을', '▁별', '처럼', '▁반짝', '이는', '▁노래', '들']\n",
      "여름밤을 수놓은 별처럼 반짝이는 노래</s>\n",
      "epoch no.4 train no.347010  loss = 2.75768 avg_loss = 3.33242\n",
      "epoch no.4 train no.347020  loss = 2.88744 avg_loss = 3.31062\n",
      "epoch no.4 train no.347030  loss = 3.21692 avg_loss = 3.34151\n",
      "epoch no.4 train no.347040  loss = 3.47160 avg_loss = 3.32687\n",
      "epoch no.4 train no.347050  loss = 2.26748 avg_loss = 3.30814\n",
      "epoch no.4 train no.347060  loss = 3.40223 avg_loss = 3.31814\n",
      "epoch no.4 train no.347070  loss = 4.27363 avg_loss = 3.33499\n",
      "epoch no.4 train no.347080  loss = 3.61206 avg_loss = 3.34012\n",
      "epoch no.4 train no.347090  loss = 3.55937 avg_loss = 3.32428\n",
      "epoch no.4 train no.347100  loss = 3.09374 avg_loss = 3.26317\n",
      "epoch no.4 train no.347110  loss = 2.15313 avg_loss = 3.21649\n",
      "epoch no.4 train no.347120  loss = 2.47644 avg_loss = 3.19429\n",
      "epoch no.4 train no.347130  loss = 2.12922 avg_loss = 3.17824\n",
      "epoch no.4 train no.347140  loss = 2.47848 avg_loss = 3.16380\n",
      "epoch no.4 train no.347150  loss = 3.28969 avg_loss = 3.17577\n",
      "epoch no.4 train no.347160  loss = 2.17954 avg_loss = 3.15710\n",
      "epoch no.4 train no.347170  loss = 2.66565 avg_loss = 3.15457\n",
      "epoch no.4 train no.347180  loss = 3.36857 avg_loss = 3.20014\n",
      "epoch no.4 train no.347190  loss = 2.77856 avg_loss = 3.18345\n",
      "epoch no.4 train no.347200  loss = 3.32481 avg_loss = 3.15551\n",
      "epoch no.4 train no.347210  loss = 4.46509 avg_loss = 3.21039\n",
      "epoch no.4 train no.347220  loss = 2.91966 avg_loss = 3.23016\n",
      "epoch no.4 train no.347230  loss = 4.42357 avg_loss = 3.21350\n",
      "epoch no.4 train no.347240  loss = 3.70871 avg_loss = 3.24107\n",
      "epoch no.4 train no.347250  loss = 4.44931 avg_loss = 3.26094\n",
      "epoch no.4 train no.347260  loss = 3.79391 avg_loss = 3.26574\n",
      "epoch no.4 train no.347270  loss = 2.61936 avg_loss = 3.23041\n",
      "epoch no.4 train no.347280  loss = 5.01915 avg_loss = 3.20302\n",
      "epoch no.4 train no.347290  loss = 2.91708 avg_loss = 3.27394\n",
      "epoch no.4 train no.347300  loss = 3.76563 avg_loss = 3.25915\n",
      "epoch no.4 train no.347310  loss = 2.19361 avg_loss = 3.25474\n",
      "epoch no.4 train no.347320  loss = 2.83009 avg_loss = 3.24247\n",
      "epoch no.4 train no.347330  loss = 1.74747 avg_loss = 3.23777\n",
      "epoch no.4 train no.347340  loss = 1.80441 avg_loss = 3.24614\n",
      "epoch no.4 train no.347350  loss = 3.18507 avg_loss = 3.23848\n",
      "epoch no.4 train no.347360  loss = 2.93589 avg_loss = 3.21341\n",
      "epoch no.4 train no.347370  loss = 4.93025 avg_loss = 3.27117\n",
      "epoch no.4 train no.347380  loss = 3.65057 avg_loss = 3.28074\n",
      "epoch no.4 train no.347390  loss = 2.50082 avg_loss = 3.26423\n",
      "epoch no.4 train no.347400  loss = 2.28911 avg_loss = 3.24694\n",
      "epoch no.4 train no.347410  loss = 4.42571 avg_loss = 3.26203\n",
      "epoch no.4 train no.347420  loss = 4.27013 avg_loss = 3.20473\n",
      "epoch no.4 train no.347430  loss = 3.19808 avg_loss = 3.20218\n",
      "epoch no.4 train no.347440  loss = 4.95766 avg_loss = 3.24691\n",
      "epoch no.4 train no.347450  loss = 2.74049 avg_loss = 3.24826\n",
      "epoch no.4 train no.347460  loss = 2.73635 avg_loss = 3.23441\n",
      "epoch no.4 train no.347470  loss = 2.75947 avg_loss = 3.23723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.347480  loss = 3.51366 avg_loss = 3.19670\n",
      "epoch no.4 train no.347490  loss = 3.73487 avg_loss = 3.22780\n",
      "epoch no.4 train no.347500  loss = 2.28531 avg_loss = 3.21210\n",
      "epoch no.4 train no.347510  loss = 3.70326 avg_loss = 3.19970\n",
      "epoch no.4 train no.347520  loss = 5.00040 avg_loss = 3.22534\n",
      "epoch no.4 train no.347530  loss = 3.90691 avg_loss = 3.18462\n",
      "epoch no.4 train no.347540  loss = 2.06173 avg_loss = 3.23610\n",
      "epoch no.4 train no.347550  loss = 3.52649 avg_loss = 3.28135\n",
      "epoch no.4 train no.347560  loss = 2.68361 avg_loss = 3.30765\n",
      "epoch no.4 train no.347570  loss = 2.85623 avg_loss = 3.30653\n",
      "epoch no.4 train no.347580  loss = 3.35694 avg_loss = 3.30750\n",
      "epoch no.4 train no.347590  loss = 5.72373 avg_loss = 3.29208\n",
      "epoch no.4 train no.347600  loss = 2.37028 avg_loss = 3.27815\n",
      "epoch no.4 train no.347610  loss = 2.20491 avg_loss = 3.23546\n",
      "epoch no.4 train no.347620  loss = 2.97898 avg_loss = 3.18520\n",
      "epoch no.4 train no.347630  loss = 4.19517 avg_loss = 3.17414\n",
      "epoch no.4 train no.347640  loss = 4.48472 avg_loss = 3.17747\n",
      "epoch no.4 train no.347650  loss = 1.35376 avg_loss = 3.10486\n",
      "epoch no.4 train no.347660  loss = 1.75024 avg_loss = 3.08816\n",
      "epoch no.4 train no.347670  loss = 2.17007 avg_loss = 3.02197\n",
      "epoch no.4 train no.347680  loss = 3.45215 avg_loss = 3.02637\n",
      "epoch no.4 train no.347690  loss = 2.33887 avg_loss = 3.04122\n",
      "epoch no.4 train no.347700  loss = 3.54043 avg_loss = 3.03444\n",
      "epoch no.4 train no.347710  loss = 2.20874 avg_loss = 3.05609\n",
      "epoch no.4 train no.347720  loss = 2.29466 avg_loss = 3.07705\n",
      "epoch no.4 train no.347730  loss = 3.95244 avg_loss = 3.10374\n",
      "epoch no.4 train no.347740  loss = 2.34312 avg_loss = 3.08860\n",
      "epoch no.4 train no.347750  loss = 3.03670 avg_loss = 3.10273\n",
      "epoch no.4 train no.347760  loss = 2.07705 avg_loss = 3.08255\n",
      "epoch no.4 train no.347770  loss = 1.48799 avg_loss = 3.06325\n",
      "epoch no.4 train no.347780  loss = 3.30168 avg_loss = 3.08683\n",
      "epoch no.4 train no.347790  loss = 3.04624 avg_loss = 3.17191\n",
      "epoch no.4 train no.347800  loss = 2.03494 avg_loss = 3.16608\n",
      "epoch no.4 train no.347810  loss = 3.14801 avg_loss = 3.18857\n",
      "epoch no.4 train no.347820  loss = 1.75552 avg_loss = 3.17215\n",
      "epoch no.4 train no.347830  loss = 1.90599 avg_loss = 3.19492\n",
      "epoch no.4 train no.347840  loss = 4.28974 avg_loss = 3.17618\n",
      "epoch no.4 train no.347850  loss = 2.87838 avg_loss = 3.16960\n",
      "epoch no.4 train no.347860  loss = 5.18579 avg_loss = 3.20843\n",
      "epoch no.4 train no.347870  loss = 3.41854 avg_loss = 3.21568\n",
      "epoch no.4 train no.347880  loss = 1.84134 avg_loss = 3.18009\n",
      "epoch no.4 train no.347890  loss = 5.86449 avg_loss = 3.20468\n",
      "epoch no.4 train no.347900  loss = 3.62617 avg_loss = 3.20283\n",
      "epoch no.4 train no.347910  loss = 4.37411 avg_loss = 3.25384\n",
      "epoch no.4 train no.347920  loss = 1.95262 avg_loss = 3.24171\n",
      "epoch no.4 train no.347930  loss = 3.39884 avg_loss = 3.27189\n",
      "epoch no.4 train no.347940  loss = 2.42781 avg_loss = 3.25914\n",
      "epoch no.4 train no.347950  loss = 2.77557 avg_loss = 3.33601\n",
      "epoch no.4 train no.347960  loss = 2.85338 avg_loss = 3.32432\n",
      "epoch no.4 train no.347970  loss = 2.39552 avg_loss = 3.34925\n",
      "epoch no.4 train no.347980  loss = 2.40672 avg_loss = 3.36442\n",
      "epoch no.4 train no.347990  loss = 3.10066 avg_loss = 3.34649\n",
      "epoch no.4 train no.348000  loss = 4.38745 avg_loss = 3.38918\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '한', '▁듣는', '▁좋은', '▁노래', '곡', '</s>']\n",
      "여름밤 잔잔하게 듣기좋은 감성곡</s>\n",
      "epoch no.4 train no.348010  loss = 3.73412 avg_loss = 3.40392\n",
      "epoch no.4 train no.348020  loss = 3.83780 avg_loss = 3.39591\n",
      "epoch no.4 train no.348030  loss = 4.85778 avg_loss = 3.38463\n",
      "epoch no.4 train no.348040  loss = 2.26140 avg_loss = 3.36522\n",
      "epoch no.4 train no.348050  loss = 4.48695 avg_loss = 3.37235\n",
      "epoch no.4 train no.348060  loss = 4.06614 avg_loss = 3.39129\n",
      "epoch no.4 train no.348070  loss = 2.03152 avg_loss = 3.36020\n",
      "epoch no.4 train no.348080  loss = 2.07780 avg_loss = 3.36840\n",
      "epoch no.4 train no.348090  loss = 3.95010 avg_loss = 3.32029\n",
      "epoch no.4 train no.348100  loss = 2.79660 avg_loss = 3.29587\n",
      "epoch no.4 train no.348110  loss = 3.35833 avg_loss = 3.32834\n",
      "epoch no.4 train no.348120  loss = 2.13410 avg_loss = 3.31471\n",
      "epoch no.4 train no.348130  loss = 3.94394 avg_loss = 3.27154\n",
      "epoch no.4 train no.348140  loss = 3.09169 avg_loss = 3.24879\n",
      "epoch no.4 train no.348150  loss = 3.07397 avg_loss = 3.23597\n",
      "epoch no.4 train no.348160  loss = 1.83616 avg_loss = 3.20995\n",
      "epoch no.4 train no.348170  loss = 2.41855 avg_loss = 3.23645\n",
      "epoch no.4 train no.348180  loss = 2.68766 avg_loss = 3.21477\n",
      "epoch no.4 train no.348190  loss = 2.93965 avg_loss = 3.21196\n",
      "epoch no.4 train no.348200  loss = 1.16081 avg_loss = 3.15920\n",
      "epoch no.4 train no.348210  loss = 2.90218 avg_loss = 3.09634\n",
      "epoch no.4 train no.348220  loss = 3.10602 avg_loss = 3.12166\n",
      "epoch no.4 train no.348230  loss = 2.51526 avg_loss = 3.13256\n",
      "epoch no.4 train no.348240  loss = 2.10796 avg_loss = 3.13062\n",
      "epoch no.4 train no.348250  loss = 4.31260 avg_loss = 3.16630\n",
      "epoch no.4 train no.348260  loss = 2.85200 avg_loss = 3.15825\n",
      "epoch no.4 train no.348270  loss = 3.42749 avg_loss = 3.17772\n",
      "epoch no.4 train no.348280  loss = 3.22891 avg_loss = 3.20937\n",
      "epoch no.4 train no.348290  loss = 3.51312 avg_loss = 3.22528\n",
      "epoch no.4 train no.348300  loss = 1.61373 avg_loss = 3.19210\n",
      "epoch no.4 train no.348310  loss = 2.81220 avg_loss = 3.18766\n",
      "epoch no.4 train no.348320  loss = 2.42962 avg_loss = 3.16160\n",
      "epoch no.4 train no.348330  loss = 4.66217 avg_loss = 3.15534\n",
      "epoch no.4 train no.348340  loss = 2.32844 avg_loss = 3.16886\n",
      "epoch no.4 train no.348350  loss = 3.26811 avg_loss = 3.12912\n",
      "epoch no.4 train no.348360  loss = 3.04809 avg_loss = 3.13258\n",
      "epoch no.4 train no.348370  loss = 3.56844 avg_loss = 3.08200\n",
      "epoch no.4 train no.348380  loss = 3.24176 avg_loss = 3.11545\n",
      "epoch no.4 train no.348390  loss = 2.12373 avg_loss = 3.10048\n",
      "epoch no.4 train no.348400  loss = 2.02337 avg_loss = 3.13572\n",
      "epoch no.4 train no.348410  loss = 3.29206 avg_loss = 3.13515\n",
      "epoch no.4 train no.348420  loss = 3.12508 avg_loss = 3.13817\n",
      "epoch no.4 train no.348430  loss = 3.65172 avg_loss = 3.17218\n",
      "epoch no.4 train no.348440  loss = 2.34138 avg_loss = 3.19060\n",
      "epoch no.4 train no.348450  loss = 2.27804 avg_loss = 3.18138\n",
      "epoch no.4 train no.348460  loss = 4.20952 avg_loss = 3.19838\n",
      "epoch no.4 train no.348470  loss = 3.31629 avg_loss = 3.17680\n",
      "epoch no.4 train no.348480  loss = 3.58531 avg_loss = 3.16644\n",
      "epoch no.4 train no.348490  loss = 1.61973 avg_loss = 3.18877\n",
      "epoch no.4 train no.348500  loss = 3.02394 avg_loss = 3.17467\n",
      "epoch no.4 train no.348510  loss = 3.06580 avg_loss = 3.14425\n",
      "epoch no.4 train no.348520  loss = 1.83089 avg_loss = 3.10087\n",
      "epoch no.4 train no.348530  loss = 2.25618 avg_loss = 3.08399\n",
      "epoch no.4 train no.348540  loss = 4.29006 avg_loss = 3.13266\n",
      "epoch no.4 train no.348550  loss = 3.14508 avg_loss = 3.13739\n",
      "epoch no.4 train no.348560  loss = 2.00190 avg_loss = 3.07343\n",
      "epoch no.4 train no.348570  loss = 3.40093 avg_loss = 3.08378\n",
      "epoch no.4 train no.348580  loss = 2.97905 avg_loss = 3.10250\n",
      "epoch no.4 train no.348590  loss = 2.16604 avg_loss = 3.08430\n",
      "epoch no.4 train no.348600  loss = 4.18508 avg_loss = 3.08069\n",
      "epoch no.4 train no.348610  loss = 3.99649 avg_loss = 3.12173\n",
      "epoch no.4 train no.348620  loss = 4.23129 avg_loss = 3.11188\n",
      "epoch no.4 train no.348630  loss = 4.40383 avg_loss = 3.14407\n",
      "epoch no.4 train no.348640  loss = 2.45023 avg_loss = 3.14321\n",
      "epoch no.4 train no.348650  loss = 2.27791 avg_loss = 3.12002\n",
      "epoch no.4 train no.348660  loss = 3.74452 avg_loss = 3.14306\n",
      "epoch no.4 train no.348670  loss = 2.29137 avg_loss = 3.12825\n",
      "epoch no.4 train no.348680  loss = 2.39605 avg_loss = 3.15860\n",
      "epoch no.4 train no.348690  loss = 2.92787 avg_loss = 3.14006\n",
      "epoch no.4 train no.348700  loss = 2.82091 avg_loss = 3.15012\n",
      "epoch no.4 train no.348710  loss = 2.35777 avg_loss = 3.12941\n",
      "epoch no.4 train no.348720  loss = 3.69350 avg_loss = 3.15785\n",
      "epoch no.4 train no.348730  loss = 2.42315 avg_loss = 3.11160\n",
      "epoch no.4 train no.348740  loss = 2.78875 avg_loss = 3.09139\n",
      "epoch no.4 train no.348750  loss = 2.65761 avg_loss = 3.08486\n",
      "epoch no.4 train no.348760  loss = 2.88081 avg_loss = 3.10265\n",
      "epoch no.4 train no.348770  loss = 2.78934 avg_loss = 3.10496\n",
      "epoch no.4 train no.348780  loss = 5.02705 avg_loss = 3.10457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.348790  loss = 2.25172 avg_loss = 3.07938\n",
      "epoch no.4 train no.348800  loss = 4.60672 avg_loss = 3.11821\n",
      "epoch no.4 train no.348810  loss = 4.25833 avg_loss = 3.13081\n",
      "epoch no.4 train no.348820  loss = 2.41326 avg_loss = 3.08552\n",
      "epoch no.4 train no.348830  loss = 3.81061 avg_loss = 3.15094\n",
      "epoch no.4 train no.348840  loss = 3.00710 avg_loss = 3.14484\n",
      "epoch no.4 train no.348850  loss = 3.85425 avg_loss = 3.18575\n",
      "epoch no.4 train no.348860  loss = 3.04061 avg_loss = 3.20075\n",
      "epoch no.4 train no.348870  loss = 3.07356 avg_loss = 3.22749\n",
      "epoch no.4 train no.348880  loss = 2.26701 avg_loss = 3.22409\n",
      "epoch no.4 train no.348890  loss = 2.92600 avg_loss = 3.26200\n",
      "epoch no.4 train no.348900  loss = 2.12941 avg_loss = 3.23235\n",
      "epoch no.4 train no.348910  loss = 4.62080 avg_loss = 3.26316\n",
      "epoch no.4 train no.348920  loss = 2.76898 avg_loss = 3.30019\n",
      "epoch no.4 train no.348930  loss = 2.85470 avg_loss = 3.27993\n",
      "epoch no.4 train no.348940  loss = 3.19749 avg_loss = 3.24699\n",
      "epoch no.4 train no.348950  loss = 3.39778 avg_loss = 3.24654\n",
      "epoch no.4 train no.348960  loss = 3.19161 avg_loss = 3.24217\n",
      "epoch no.4 train no.348970  loss = 3.68554 avg_loss = 3.25796\n",
      "epoch no.4 train no.348980  loss = 2.38052 avg_loss = 3.30185\n",
      "epoch no.4 train no.348990  loss = 3.19938 avg_loss = 3.30240\n",
      "epoch no.4 train no.349000  loss = 2.92778 avg_loss = 3.31718\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '에', '하며', '▁때', '▁듣기', '▁싶은', '은', '▁노래', '</s>']\n",
      "여름밤 산책할 때 듣고싶은 음악</s>\n",
      "epoch no.4 train no.349010  loss = 3.49315 avg_loss = 3.27777\n",
      "epoch no.4 train no.349020  loss = 4.64482 avg_loss = 3.26773\n",
      "epoch no.4 train no.349030  loss = 4.15959 avg_loss = 3.27545\n",
      "epoch no.4 train no.349040  loss = 2.30543 avg_loss = 3.26568\n",
      "epoch no.4 train no.349050  loss = 2.68535 avg_loss = 3.24173\n",
      "epoch no.4 train no.349060  loss = 3.83969 avg_loss = 3.26879\n",
      "epoch no.4 train no.349070  loss = 2.20852 avg_loss = 3.24977\n",
      "epoch no.4 train no.349080  loss = 1.88430 avg_loss = 3.27681\n",
      "epoch no.4 train no.349090  loss = 3.03007 avg_loss = 3.28839\n",
      "epoch no.4 train no.349100  loss = 3.71249 avg_loss = 3.28058\n",
      "epoch no.4 train no.349110  loss = 1.61888 avg_loss = 3.23626\n",
      "epoch no.4 train no.349120  loss = 2.66114 avg_loss = 3.22553\n",
      "epoch no.4 train no.349130  loss = 4.24742 avg_loss = 3.23331\n",
      "epoch no.4 train no.349140  loss = 5.08135 avg_loss = 3.22332\n",
      "epoch no.4 train no.349150  loss = 3.82844 avg_loss = 3.25406\n",
      "epoch no.4 train no.349160  loss = 3.53149 avg_loss = 3.27629\n",
      "epoch no.4 train no.349170  loss = 2.94313 avg_loss = 3.27062\n",
      "epoch no.4 train no.349180  loss = 2.05556 avg_loss = 3.21042\n",
      "epoch no.4 train no.349190  loss = 3.10584 avg_loss = 3.18735\n",
      "epoch no.4 train no.349200  loss = 3.29356 avg_loss = 3.17773\n",
      "epoch no.4 train no.349210  loss = 3.48709 avg_loss = 3.19715\n",
      "epoch no.4 train no.349220  loss = 4.01024 avg_loss = 3.26158\n",
      "epoch no.4 train no.349230  loss = 2.15765 avg_loss = 3.30137\n",
      "epoch no.4 train no.349240  loss = 3.29228 avg_loss = 3.32133\n",
      "epoch no.4 train no.349250  loss = 3.98841 avg_loss = 3.34270\n",
      "epoch no.4 train no.349260  loss = 2.51497 avg_loss = 3.32918\n",
      "epoch no.4 train no.349270  loss = 2.18282 avg_loss = 3.30502\n",
      "epoch no.4 train no.349280  loss = 3.86850 avg_loss = 3.29501\n",
      "epoch no.4 train no.349290  loss = 3.65162 avg_loss = 3.30324\n",
      "epoch no.4 train no.349300  loss = 3.83293 avg_loss = 3.26934\n",
      "epoch no.4 train no.349310  loss = 2.52360 avg_loss = 3.26153\n",
      "epoch no.4 train no.349320  loss = 3.54800 avg_loss = 3.26132\n",
      "epoch no.4 train no.349330  loss = 2.50059 avg_loss = 3.27301\n",
      "epoch no.4 train no.349340  loss = 3.03024 avg_loss = 3.21910\n",
      "epoch no.4 train no.349350  loss = 3.08835 avg_loss = 3.25078\n",
      "epoch no.4 train no.349360  loss = 3.77896 avg_loss = 3.24610\n",
      "epoch no.4 train no.349370  loss = 3.39951 avg_loss = 3.24608\n",
      "epoch no.4 train no.349380  loss = 2.38464 avg_loss = 3.21475\n",
      "epoch no.4 train no.349390  loss = 2.50684 avg_loss = 3.18869\n",
      "epoch no.4 train no.349400  loss = 2.81634 avg_loss = 3.14207\n",
      "epoch no.4 train no.349410  loss = 3.23640 avg_loss = 3.12900\n",
      "epoch no.4 train no.349420  loss = 3.50817 avg_loss = 3.13373\n",
      "epoch no.4 train no.349430  loss = 2.41309 avg_loss = 3.10906\n",
      "epoch no.4 train no.349440  loss = 2.65003 avg_loss = 3.11514\n",
      "epoch no.4 train no.349450  loss = 4.80013 avg_loss = 3.13751\n",
      "epoch no.4 train no.349460  loss = 3.31598 avg_loss = 3.10483\n",
      "epoch no.4 train no.349470  loss = 2.51739 avg_loss = 3.13519\n",
      "epoch no.4 train no.349480  loss = 2.94966 avg_loss = 3.11150\n",
      "epoch no.4 train no.349490  loss = 3.04990 avg_loss = 3.13212\n",
      "epoch no.4 train no.349500  loss = 2.87996 avg_loss = 3.12885\n",
      "epoch no.4 train no.349510  loss = 4.68706 avg_loss = 3.15077\n",
      "epoch no.4 train no.349520  loss = 1.88606 avg_loss = 3.13452\n",
      "epoch no.4 train no.349530  loss = 4.41142 avg_loss = 3.19163\n",
      "epoch no.4 train no.349540  loss = 2.55722 avg_loss = 3.20621\n",
      "epoch no.4 train no.349550  loss = 2.07304 avg_loss = 3.20035\n",
      "epoch no.4 train no.349560  loss = 2.12818 avg_loss = 3.20639\n",
      "epoch no.4 train no.349570  loss = 2.56386 avg_loss = 3.20077\n",
      "epoch no.4 train no.349580  loss = 3.38513 avg_loss = 3.18501\n",
      "epoch no.4 train no.349590  loss = 3.14554 avg_loss = 3.17306\n",
      "epoch no.4 train no.349600  loss = 3.82698 avg_loss = 3.20303\n",
      "epoch no.4 train no.349610  loss = 5.13867 avg_loss = 3.22035\n",
      "epoch no.4 train no.349620  loss = 4.05876 avg_loss = 3.21716\n",
      "epoch no.4 train no.349630  loss = 2.99599 avg_loss = 3.24321\n",
      "epoch no.4 train no.349640  loss = 3.49174 avg_loss = 3.21852\n",
      "epoch no.4 train no.349650  loss = 3.28078 avg_loss = 3.20510\n",
      "epoch no.4 train no.349660  loss = 4.69242 avg_loss = 3.20802\n",
      "epoch no.4 train no.349670  loss = 3.70571 avg_loss = 3.25461\n",
      "epoch no.4 train no.349680  loss = 3.71954 avg_loss = 3.21466\n",
      "epoch no.4 train no.349690  loss = 3.46395 avg_loss = 3.27906\n",
      "epoch no.4 train no.349700  loss = 5.16334 avg_loss = 3.26484\n",
      "epoch no.4 train no.349710  loss = 3.60849 avg_loss = 3.25815\n",
      "epoch no.4 train no.349720  loss = 3.65180 avg_loss = 3.27528\n",
      "epoch no.4 train no.349730  loss = 5.58199 avg_loss = 3.33593\n",
      "epoch no.4 train no.349740  loss = 1.04740 avg_loss = 3.27675\n",
      "epoch no.4 train no.349750  loss = 2.79602 avg_loss = 3.26880\n",
      "epoch no.4 train no.349760  loss = 3.49620 avg_loss = 3.27046\n",
      "epoch no.4 train no.349770  loss = 3.39796 avg_loss = 3.26201\n",
      "epoch no.4 train no.349780  loss = 3.41367 avg_loss = 3.27203\n",
      "epoch no.4 train no.349790  loss = 2.97692 avg_loss = 3.23172\n",
      "epoch no.4 train no.349800  loss = 3.16487 avg_loss = 3.22776\n",
      "epoch no.4 train no.349810  loss = 1.95436 avg_loss = 3.25447\n",
      "epoch no.4 train no.349820  loss = 3.08944 avg_loss = 3.25002\n",
      "epoch no.4 train no.349830  loss = 2.24507 avg_loss = 3.22633\n",
      "epoch no.4 train no.349840  loss = 2.78786 avg_loss = 3.25999\n",
      "epoch no.4 train no.349850  loss = 3.27244 avg_loss = 3.28112\n",
      "epoch no.4 train no.349860  loss = 3.47892 avg_loss = 3.27192\n",
      "epoch no.4 train no.349870  loss = 3.88942 avg_loss = 3.28263\n",
      "epoch no.4 train no.349880  loss = 3.72807 avg_loss = 3.23904\n",
      "epoch no.4 train no.349890  loss = 2.33639 avg_loss = 3.19708\n",
      "epoch no.4 train no.349900  loss = 2.27213 avg_loss = 3.20798\n",
      "epoch no.4 train no.349910  loss = 3.82239 avg_loss = 3.23512\n",
      "epoch no.4 train no.349920  loss = 2.28064 avg_loss = 3.23525\n",
      "epoch no.4 train no.349930  loss = 2.69817 avg_loss = 3.18194\n",
      "epoch no.4 train no.349940  loss = 3.55686 avg_loss = 3.21083\n",
      "epoch no.4 train no.349950  loss = 3.78399 avg_loss = 3.20945\n",
      "epoch no.4 train no.349960  loss = 2.14633 avg_loss = 3.25461\n",
      "epoch no.4 train no.349970  loss = 3.38420 avg_loss = 3.26704\n",
      "epoch no.4 train no.349980  loss = 2.50471 avg_loss = 3.23758\n",
      "epoch no.4 train no.349990  loss = 3.17540 avg_loss = 3.21699\n",
      "epoch no.4 train no.350000  loss = 2.39210 avg_loss = 3.19928\n",
      "10\n",
      "to_tokens: ['▁비', '밤', '을', '▁위한', '놓은', '▁별', '처럼', '▁반짝', '이는', '▁노래', '처럼', '</s>']\n",
      "여름밤을 수놓은 별처럼 반짝이는 별처럼</s>\n",
      "epoch no.4 train no.350010  loss = 3.31309 avg_loss = 3.17619\n",
      "epoch no.4 train no.350020  loss = 3.71276 avg_loss = 3.17093\n",
      "epoch no.4 train no.350030  loss = 4.32390 avg_loss = 3.19874\n",
      "epoch no.4 train no.350040  loss = 4.94855 avg_loss = 3.21900\n",
      "epoch no.4 train no.350050  loss = 3.84780 avg_loss = 3.22516\n",
      "epoch no.4 train no.350060  loss = 3.19344 avg_loss = 3.21805\n",
      "epoch no.4 train no.350070  loss = 2.09202 avg_loss = 3.20308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.350080  loss = 4.16808 avg_loss = 3.18484\n",
      "epoch no.4 train no.350090  loss = 3.05376 avg_loss = 3.19100\n",
      "epoch no.4 train no.350100  loss = 2.26493 avg_loss = 3.22223\n",
      "epoch no.4 train no.350110  loss = 2.41864 avg_loss = 3.17506\n",
      "epoch no.4 train no.350120  loss = 3.99596 avg_loss = 3.21788\n",
      "epoch no.4 train no.350130  loss = 4.01176 avg_loss = 3.27265\n",
      "epoch no.4 train no.350140  loss = 3.09310 avg_loss = 3.26059\n",
      "epoch no.4 train no.350150  loss = 4.92408 avg_loss = 3.29525\n",
      "epoch no.4 train no.350160  loss = 4.78385 avg_loss = 3.32027\n",
      "epoch no.4 train no.350170  loss = 5.67696 avg_loss = 3.38253\n",
      "epoch no.4 train no.350180  loss = 3.19664 avg_loss = 3.34773\n",
      "epoch no.4 train no.350190  loss = 2.88445 avg_loss = 3.34088\n",
      "epoch no.4 train no.350200  loss = 4.19579 avg_loss = 3.37048\n",
      "epoch no.4 train no.350210  loss = 2.55555 avg_loss = 3.34906\n",
      "epoch no.4 train no.350220  loss = 2.41437 avg_loss = 3.32889\n",
      "epoch no.4 train no.350230  loss = 2.62203 avg_loss = 3.29279\n",
      "epoch no.4 train no.350240  loss = 2.53864 avg_loss = 3.30752\n",
      "epoch no.4 train no.350250  loss = 2.54244 avg_loss = 3.32389\n",
      "epoch no.4 train no.350260  loss = 2.93078 avg_loss = 3.26195\n",
      "epoch no.4 train no.350270  loss = 2.76358 avg_loss = 3.25037\n",
      "epoch no.4 train no.350280  loss = 4.38393 avg_loss = 3.23685\n",
      "epoch no.4 train no.350290  loss = 3.11995 avg_loss = 3.26517\n",
      "epoch no.4 train no.350300  loss = 2.05796 avg_loss = 3.27372\n",
      "epoch no.4 train no.350310  loss = 2.44831 avg_loss = 3.25279\n",
      "epoch no.4 train no.350320  loss = 1.93102 avg_loss = 3.22953\n",
      "epoch no.4 train no.350330  loss = 1.83401 avg_loss = 3.19140\n",
      "epoch no.4 train no.350340  loss = 4.07174 avg_loss = 3.20998\n",
      "epoch no.4 train no.350350  loss = 2.70669 avg_loss = 3.21940\n",
      "epoch no.4 train no.350360  loss = 1.94383 avg_loss = 3.24531\n",
      "epoch no.4 train no.350370  loss = 4.18803 avg_loss = 3.25681\n",
      "epoch no.4 train no.350380  loss = 4.89759 avg_loss = 3.27921\n",
      "epoch no.4 train no.350390  loss = 3.48028 avg_loss = 3.26433\n",
      "epoch no.4 train no.350400  loss = 3.98622 avg_loss = 3.29720\n",
      "epoch no.4 train no.350410  loss = 2.68844 avg_loss = 3.26882\n",
      "epoch no.4 train no.350420  loss = 4.24682 avg_loss = 3.26030\n",
      "epoch no.4 train no.350430  loss = 2.47586 avg_loss = 3.24571\n",
      "epoch no.4 train no.350440  loss = 4.29102 avg_loss = 3.27008\n",
      "epoch no.4 train no.350450  loss = 2.20249 avg_loss = 3.25221\n",
      "epoch no.4 train no.350460  loss = 3.51579 avg_loss = 3.24799\n",
      "epoch no.4 train no.350470  loss = 4.47127 avg_loss = 3.26612\n",
      "epoch no.4 train no.350480  loss = 2.67685 avg_loss = 3.26483\n",
      "epoch no.4 train no.350490  loss = 2.62216 avg_loss = 3.27031\n",
      "epoch no.4 train no.350500  loss = 2.09196 avg_loss = 3.24236\n",
      "epoch no.4 train no.350510  loss = 4.22956 avg_loss = 3.28200\n",
      "epoch no.4 train no.350520  loss = 3.69484 avg_loss = 3.32252\n",
      "epoch no.4 train no.350530  loss = 2.90634 avg_loss = 3.33589\n",
      "epoch no.4 train no.350540  loss = 4.48011 avg_loss = 3.30531\n",
      "epoch no.4 train no.350550  loss = 4.25184 avg_loss = 3.37135\n",
      "epoch no.4 train no.350560  loss = 2.25982 avg_loss = 3.32056\n",
      "epoch no.4 train no.350570  loss = 2.79658 avg_loss = 3.26683\n",
      "epoch no.4 train no.350580  loss = 3.08849 avg_loss = 3.31452\n",
      "epoch no.4 train no.350590  loss = 2.38235 avg_loss = 3.30068\n",
      "epoch no.4 train no.350600  loss = 4.58992 avg_loss = 3.26393\n",
      "epoch no.4 train no.350610  loss = 2.69411 avg_loss = 3.25650\n",
      "epoch no.4 train no.350620  loss = 1.78177 avg_loss = 3.25328\n",
      "epoch no.4 train no.350630  loss = 2.16011 avg_loss = 3.22414\n",
      "epoch no.4 train no.350640  loss = 4.13710 avg_loss = 3.21357\n",
      "epoch no.4 train no.350650  loss = 2.54790 avg_loss = 3.16212\n",
      "epoch no.4 train no.350660  loss = 1.99323 avg_loss = 3.13791\n",
      "epoch no.4 train no.350670  loss = 2.40918 avg_loss = 3.16265\n",
      "epoch no.4 train no.350680  loss = 3.55160 avg_loss = 3.18516\n",
      "epoch no.4 train no.350690  loss = 4.55980 avg_loss = 3.16613\n",
      "epoch no.4 train no.350700  loss = 3.17012 avg_loss = 3.16603\n",
      "epoch no.4 train no.350710  loss = 3.32159 avg_loss = 3.11649\n",
      "epoch no.4 train no.350720  loss = 3.91602 avg_loss = 3.13773\n",
      "epoch no.4 train no.350730  loss = 2.60623 avg_loss = 3.12337\n",
      "epoch no.4 train no.350740  loss = 2.74944 avg_loss = 3.10855\n",
      "epoch no.4 train no.350750  loss = 3.13253 avg_loss = 3.12463\n",
      "epoch no.4 train no.350760  loss = 3.15147 avg_loss = 3.11903\n",
      "epoch no.4 train no.350770  loss = 3.04631 avg_loss = 3.09832\n",
      "epoch no.4 train no.350780  loss = 3.22847 avg_loss = 3.12525\n",
      "epoch no.4 train no.350790  loss = 2.41603 avg_loss = 3.13638\n",
      "epoch no.4 train no.350800  loss = 6.45387 avg_loss = 3.18307\n",
      "epoch no.4 train no.350810  loss = 2.18987 avg_loss = 3.15198\n",
      "epoch no.4 train no.350820  loss = 4.09720 avg_loss = 3.14471\n",
      "epoch no.4 train no.350830  loss = 3.21118 avg_loss = 3.13851\n",
      "epoch no.4 train no.350840  loss = 2.78957 avg_loss = 3.16937\n",
      "epoch no.4 train no.350850  loss = 3.02655 avg_loss = 3.16258\n",
      "epoch no.4 train no.350860  loss = 4.11816 avg_loss = 3.12016\n",
      "epoch no.4 train no.350870  loss = 3.64863 avg_loss = 3.16436\n",
      "epoch no.4 train no.350880  loss = 3.15467 avg_loss = 3.20996\n",
      "epoch no.4 train no.350890  loss = 2.61457 avg_loss = 3.19136\n",
      "epoch no.4 train no.350900  loss = 1.93916 avg_loss = 3.12884\n",
      "epoch no.4 train no.350910  loss = 3.15449 avg_loss = 3.16958\n",
      "epoch no.4 train no.350920  loss = 1.73575 avg_loss = 3.20982\n",
      "epoch no.4 train no.350930  loss = 2.98564 avg_loss = 3.22675\n",
      "epoch no.4 train no.350940  loss = 3.88231 avg_loss = 3.19241\n",
      "epoch no.4 train no.350950  loss = 3.07759 avg_loss = 3.23234\n",
      "epoch no.4 train no.350960  loss = 3.26086 avg_loss = 3.25470\n",
      "epoch no.4 train no.350970  loss = 2.62219 avg_loss = 3.24865\n",
      "epoch no.4 train no.350980  loss = 4.27308 avg_loss = 3.26033\n",
      "epoch no.4 train no.350990  loss = 1.83767 avg_loss = 3.22312\n",
      "epoch no.4 train no.351000  loss = 2.55158 avg_loss = 3.20690\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '의', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.351010  loss = 3.47011 avg_loss = 3.26351\n",
      "epoch no.4 train no.351020  loss = 4.12650 avg_loss = 3.24141\n",
      "epoch no.4 train no.351030  loss = 3.31948 avg_loss = 3.24505\n",
      "epoch no.4 train no.351040  loss = 3.79825 avg_loss = 3.27034\n",
      "epoch no.4 train no.351050  loss = 2.61944 avg_loss = 3.26819\n",
      "epoch no.4 train no.351060  loss = 3.81772 avg_loss = 3.28517\n",
      "epoch no.4 train no.351070  loss = 4.26867 avg_loss = 3.31137\n",
      "epoch no.4 train no.351080  loss = 2.78980 avg_loss = 3.31869\n",
      "epoch no.4 train no.351090  loss = 4.36554 avg_loss = 3.31935\n",
      "epoch no.4 train no.351100  loss = 1.80961 avg_loss = 3.31324\n",
      "epoch no.4 train no.351110  loss = 2.89753 avg_loss = 3.24779\n",
      "epoch no.4 train no.351120  loss = 3.82769 avg_loss = 3.22222\n",
      "epoch no.4 train no.351130  loss = 2.61905 avg_loss = 3.24954\n",
      "epoch no.4 train no.351140  loss = 3.84483 avg_loss = 3.26533\n",
      "epoch no.4 train no.351150  loss = 4.76447 avg_loss = 3.31053\n",
      "epoch no.4 train no.351160  loss = 3.52310 avg_loss = 3.35214\n",
      "epoch no.4 train no.351170  loss = 3.35855 avg_loss = 3.35929\n",
      "epoch no.4 train no.351180  loss = 3.43529 avg_loss = 3.33661\n",
      "epoch no.4 train no.351190  loss = 6.14893 avg_loss = 3.34143\n",
      "epoch no.4 train no.351200  loss = 3.21336 avg_loss = 3.32041\n",
      "epoch no.4 train no.351210  loss = 2.25782 avg_loss = 3.31387\n",
      "epoch no.4 train no.351220  loss = 2.50843 avg_loss = 3.28048\n",
      "epoch no.4 train no.351230  loss = 4.02996 avg_loss = 3.31206\n",
      "epoch no.4 train no.351240  loss = 4.30825 avg_loss = 3.33780\n",
      "epoch no.4 train no.351250  loss = 3.32177 avg_loss = 3.31993\n",
      "epoch no.4 train no.351260  loss = 3.01087 avg_loss = 3.30850\n",
      "epoch no.4 train no.351270  loss = 2.59399 avg_loss = 3.36530\n",
      "epoch no.4 train no.351280  loss = 4.60274 avg_loss = 3.38717\n",
      "epoch no.4 train no.351290  loss = 2.61016 avg_loss = 3.32869\n",
      "epoch no.4 train no.351300  loss = 2.58248 avg_loss = 3.28477\n",
      "epoch no.4 train no.351310  loss = 3.18365 avg_loss = 3.30021\n",
      "epoch no.4 train no.351320  loss = 2.73806 avg_loss = 3.27361\n",
      "epoch no.4 train no.351330  loss = 3.51211 avg_loss = 3.25293\n",
      "epoch no.4 train no.351340  loss = 3.64676 avg_loss = 3.25206\n",
      "epoch no.4 train no.351350  loss = 3.40883 avg_loss = 3.24246\n",
      "epoch no.4 train no.351360  loss = 2.10365 avg_loss = 3.26399\n",
      "epoch no.4 train no.351370  loss = 3.49482 avg_loss = 3.23851\n",
      "epoch no.4 train no.351380  loss = 2.04445 avg_loss = 3.20874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.351390  loss = 3.54612 avg_loss = 3.23765\n",
      "epoch no.4 train no.351400  loss = 2.44460 avg_loss = 3.27193\n",
      "epoch no.4 train no.351410  loss = 1.94528 avg_loss = 3.27926\n",
      "epoch no.4 train no.351420  loss = 3.46324 avg_loss = 3.28811\n",
      "epoch no.4 train no.351430  loss = 4.52108 avg_loss = 3.29512\n",
      "epoch no.4 train no.351440  loss = 2.11709 avg_loss = 3.30586\n",
      "epoch no.4 train no.351450  loss = 3.69334 avg_loss = 3.30085\n",
      "epoch no.4 train no.351460  loss = 2.44441 avg_loss = 3.28163\n",
      "epoch no.4 train no.351470  loss = 1.85829 avg_loss = 3.25487\n",
      "epoch no.4 train no.351480  loss = 3.07564 avg_loss = 3.25202\n",
      "epoch no.4 train no.351490  loss = 5.62037 avg_loss = 3.27992\n",
      "epoch no.4 train no.351500  loss = 4.87990 avg_loss = 3.31635\n",
      "epoch no.4 train no.351510  loss = 2.06447 avg_loss = 3.32841\n",
      "epoch no.4 train no.351520  loss = 5.24474 avg_loss = 3.32354\n",
      "epoch no.4 train no.351530  loss = 2.99989 avg_loss = 3.31316\n",
      "epoch no.4 train no.351540  loss = 2.64025 avg_loss = 3.25539\n",
      "epoch no.4 train no.351550  loss = 5.47416 avg_loss = 3.25570\n",
      "epoch no.4 train no.351560  loss = 3.48860 avg_loss = 3.20544\n",
      "epoch no.4 train no.351570  loss = 4.76993 avg_loss = 3.24324\n",
      "epoch no.4 train no.351580  loss = 2.84827 avg_loss = 3.26895\n",
      "epoch no.4 train no.351590  loss = 2.34249 avg_loss = 3.24398\n",
      "epoch no.4 train no.351600  loss = 4.98898 avg_loss = 3.26561\n",
      "epoch no.4 train no.351610  loss = 2.95371 avg_loss = 3.28909\n",
      "epoch no.4 train no.351620  loss = 2.81181 avg_loss = 3.24738\n",
      "epoch no.4 train no.351630  loss = 4.50077 avg_loss = 3.26632\n",
      "epoch no.4 train no.351640  loss = 1.79764 avg_loss = 3.28213\n",
      "epoch no.4 train no.351650  loss = 3.01982 avg_loss = 3.25102\n",
      "epoch no.4 train no.351660  loss = 4.43029 avg_loss = 3.29624\n",
      "epoch no.4 train no.351670  loss = 2.91877 avg_loss = 3.27649\n",
      "epoch no.4 train no.351680  loss = 3.36559 avg_loss = 3.31369\n",
      "epoch no.4 train no.351690  loss = 3.24079 avg_loss = 3.30486\n",
      "epoch no.4 train no.351700  loss = 1.50945 avg_loss = 3.28328\n",
      "epoch no.4 train no.351710  loss = 4.26021 avg_loss = 3.31337\n",
      "epoch no.4 train no.351720  loss = 4.26378 avg_loss = 3.29844\n",
      "epoch no.4 train no.351730  loss = 4.34295 avg_loss = 3.29614\n",
      "epoch no.4 train no.351740  loss = 4.47690 avg_loss = 3.35656\n",
      "epoch no.4 train no.351750  loss = 2.62299 avg_loss = 3.31902\n",
      "epoch no.4 train no.351760  loss = 2.23706 avg_loss = 3.31908\n",
      "epoch no.4 train no.351770  loss = 2.85641 avg_loss = 3.28665\n",
      "epoch no.4 train no.351780  loss = 2.93738 avg_loss = 3.30782\n",
      "epoch no.4 train no.351790  loss = 2.06434 avg_loss = 3.29326\n",
      "epoch no.4 train no.351800  loss = 2.86809 avg_loss = 3.31011\n",
      "epoch no.4 train no.351810  loss = 5.11124 avg_loss = 3.30773\n",
      "epoch no.4 train no.351820  loss = 5.74647 avg_loss = 3.31703\n",
      "epoch no.4 train no.351830  loss = 2.50225 avg_loss = 3.31508\n",
      "epoch no.4 train no.351840  loss = 4.20843 avg_loss = 3.34444\n",
      "epoch no.4 train no.351850  loss = 2.32293 avg_loss = 3.34270\n",
      "epoch no.4 train no.351860  loss = 4.39823 avg_loss = 3.35320\n",
      "epoch no.4 train no.351870  loss = 2.78803 avg_loss = 3.38312\n",
      "epoch no.4 train no.351880  loss = 2.21709 avg_loss = 3.35408\n",
      "epoch no.4 train no.351890  loss = 2.36720 avg_loss = 3.34496\n",
      "epoch no.4 train no.351900  loss = 4.08791 avg_loss = 3.32723\n",
      "epoch no.4 train no.351910  loss = 3.57634 avg_loss = 3.34477\n",
      "epoch no.4 train no.351920  loss = 3.72232 avg_loss = 3.36207\n",
      "epoch no.4 train no.351930  loss = 2.82176 avg_loss = 3.34185\n",
      "epoch no.4 train no.351940  loss = 4.07472 avg_loss = 3.34536\n",
      "epoch no.4 train no.351950  loss = 2.69295 avg_loss = 3.34132\n",
      "epoch no.4 train no.351960  loss = 2.75452 avg_loss = 3.30911\n",
      "epoch no.4 train no.351970  loss = 3.96634 avg_loss = 3.33245\n",
      "epoch no.4 train no.351980  loss = 1.79006 avg_loss = 3.28419\n",
      "epoch no.4 train no.351990  loss = 4.30531 avg_loss = 3.28761\n",
      "epoch no.4 train no.352000  loss = 2.52148 avg_loss = 3.28159\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '의', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.4 train no.352010  loss = 2.60923 avg_loss = 3.23511\n",
      "epoch no.4 train no.352020  loss = 2.84397 avg_loss = 3.20969\n",
      "epoch no.4 train no.352030  loss = 3.36498 avg_loss = 3.17291\n",
      "epoch no.4 train no.352040  loss = 3.30147 avg_loss = 3.18732\n",
      "epoch no.4 train no.352050  loss = 2.57162 avg_loss = 3.19099\n",
      "epoch no.4 train no.352060  loss = 2.40159 avg_loss = 3.24829\n",
      "epoch no.4 train no.352070  loss = 2.90117 avg_loss = 3.22824\n",
      "epoch no.4 train no.352080  loss = 3.46643 avg_loss = 3.24927\n",
      "epoch no.4 train no.352090  loss = 1.88340 avg_loss = 3.20783\n",
      "epoch no.4 train no.352100  loss = 3.75411 avg_loss = 3.23686\n",
      "epoch no.4 train no.352110  loss = 2.18170 avg_loss = 3.16252\n",
      "epoch no.4 train no.352120  loss = 2.59648 avg_loss = 3.15414\n",
      "epoch no.4 train no.352130  loss = 1.35417 avg_loss = 3.14242\n",
      "epoch no.4 train no.352140  loss = 2.76059 avg_loss = 3.16904\n",
      "epoch no.4 train no.352150  loss = 2.71421 avg_loss = 3.19273\n",
      "epoch no.4 train no.352160  loss = 1.90609 avg_loss = 3.14740\n",
      "epoch no.4 train no.352170  loss = 3.59608 avg_loss = 3.14721\n",
      "epoch no.4 train no.352180  loss = 2.82426 avg_loss = 3.12741\n",
      "epoch no.4 train no.352190  loss = 1.87700 avg_loss = 3.09672\n",
      "epoch no.4 train no.352200  loss = 2.96265 avg_loss = 3.12747\n",
      "epoch no.4 train no.352210  loss = 3.86811 avg_loss = 3.14888\n",
      "epoch no.4 train no.352220  loss = 3.65431 avg_loss = 3.16235\n",
      "epoch no.4 train no.352230  loss = 2.77336 avg_loss = 3.17288\n",
      "epoch no.4 train no.352240  loss = 2.14039 avg_loss = 3.16000\n",
      "epoch no.4 train no.352250  loss = 3.06557 avg_loss = 3.16272\n",
      "epoch no.4 train no.352260  loss = 3.07237 avg_loss = 3.13432\n",
      "epoch no.4 train no.352270  loss = 3.01309 avg_loss = 3.10196\n",
      "epoch no.4 train no.352280  loss = 2.58899 avg_loss = 3.17284\n",
      "epoch no.4 train no.352290  loss = 5.78660 avg_loss = 3.18820\n",
      "epoch no.4 train no.352300  loss = 4.15292 avg_loss = 3.22927\n",
      "epoch no.4 train no.352310  loss = 4.16325 avg_loss = 3.24279\n",
      "epoch no.4 train no.352320  loss = 4.42494 avg_loss = 3.24991\n",
      "epoch no.4 train no.352330  loss = 3.63228 avg_loss = 3.27785\n",
      "epoch no.4 train no.352340  loss = 1.62562 avg_loss = 3.21656\n",
      "epoch no.4 train no.352350  loss = 3.26396 avg_loss = 3.19756\n",
      "epoch no.4 train no.352360  loss = 2.57718 avg_loss = 3.21513\n",
      "epoch no.4 train no.352370  loss = 2.25534 avg_loss = 3.21303\n",
      "epoch no.4 train no.352380  loss = 3.32805 avg_loss = 3.19350\n",
      "epoch no.4 train no.352390  loss = 3.94256 avg_loss = 3.21044\n",
      "epoch no.4 train no.352400  loss = 3.46863 avg_loss = 3.14228\n",
      "epoch no.4 train no.352410  loss = 3.68827 avg_loss = 3.16496\n",
      "epoch no.4 train no.352420  loss = 3.25842 avg_loss = 3.16499\n",
      "epoch no.4 train no.352430  loss = 4.98434 avg_loss = 3.21445\n",
      "epoch no.4 train no.352440  loss = 2.35845 avg_loss = 3.19382\n",
      "epoch no.4 train no.352450  loss = 3.66678 avg_loss = 3.20783\n",
      "epoch no.4 train no.352460  loss = 2.07642 avg_loss = 3.18859\n",
      "epoch no.4 train no.352470  loss = 3.25503 avg_loss = 3.20811\n",
      "epoch no.4 train no.352480  loss = 2.94546 avg_loss = 3.19941\n",
      "epoch no.4 train no.352490  loss = 4.15929 avg_loss = 3.21839\n",
      "epoch no.4 train no.352500  loss = 2.86740 avg_loss = 3.21499\n",
      "epoch no.4 train no.352510  loss = 4.55274 avg_loss = 3.23072\n",
      "epoch no.4 train no.352520  loss = 4.15686 avg_loss = 3.22103\n",
      "epoch no.4 train no.352530  loss = 2.17300 avg_loss = 3.20663\n",
      "epoch no.4 train no.352540  loss = 1.99661 avg_loss = 3.18533\n",
      "epoch no.4 train no.352550  loss = 2.84371 avg_loss = 3.17007\n",
      "epoch no.4 train no.352560  loss = 3.93288 avg_loss = 3.16874\n",
      "epoch no.4 train no.352570  loss = 5.60489 avg_loss = 3.17958\n",
      "epoch no.4 train no.352580  loss = 3.19375 avg_loss = 3.17664\n",
      "epoch no.4 train no.352590  loss = 2.70142 avg_loss = 3.16205\n",
      "epoch no.4 train no.352600  loss = 3.49025 avg_loss = 3.14183\n",
      "epoch no.4 train no.352610  loss = 3.96941 avg_loss = 3.16687\n",
      "epoch no.4 train no.352620  loss = 3.63964 avg_loss = 3.14430\n",
      "epoch no.4 train no.352630  loss = 2.76327 avg_loss = 3.16489\n",
      "epoch no.4 train no.352640  loss = 3.40090 avg_loss = 3.18293\n",
      "epoch no.4 train no.352650  loss = 2.47803 avg_loss = 3.20152\n",
      "epoch no.4 train no.352660  loss = 2.84985 avg_loss = 3.18581\n",
      "epoch no.4 train no.352670  loss = 4.00510 avg_loss = 3.18011\n",
      "epoch no.4 train no.352680  loss = 2.43301 avg_loss = 3.19471\n",
      "epoch no.4 train no.352690  loss = 3.50333 avg_loss = 3.21655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.352700  loss = 4.27786 avg_loss = 3.23483\n",
      "epoch no.4 train no.352710  loss = 3.93886 avg_loss = 3.22373\n",
      "epoch no.4 train no.352720  loss = 2.16190 avg_loss = 3.20079\n",
      "epoch no.4 train no.352730  loss = 3.03526 avg_loss = 3.19926\n",
      "epoch no.4 train no.352740  loss = 3.54439 avg_loss = 3.20733\n",
      "epoch no.4 train no.352750  loss = 2.79903 avg_loss = 3.22005\n",
      "epoch no.4 train no.352760  loss = 2.64671 avg_loss = 3.22819\n",
      "epoch no.4 train no.352770  loss = 2.19008 avg_loss = 3.22670\n",
      "epoch no.4 train no.352780  loss = 2.57016 avg_loss = 3.23382\n",
      "epoch no.4 train no.352790  loss = 3.50086 avg_loss = 3.25416\n",
      "epoch no.4 train no.352800  loss = 3.14795 avg_loss = 3.25361\n",
      "epoch no.4 train no.352810  loss = 3.47296 avg_loss = 3.26128\n",
      "epoch no.4 train no.352820  loss = 2.61226 avg_loss = 3.26859\n",
      "epoch no.4 train no.352830  loss = 2.79750 avg_loss = 3.33723\n",
      "epoch no.4 train no.352840  loss = 3.12590 avg_loss = 3.31547\n",
      "epoch no.4 train no.352850  loss = 2.96248 avg_loss = 3.29622\n",
      "epoch no.4 train no.352860  loss = 4.31075 avg_loss = 3.32334\n",
      "epoch no.4 train no.352870  loss = 3.23986 avg_loss = 3.30723\n",
      "epoch no.4 train no.352880  loss = 2.46328 avg_loss = 3.31472\n",
      "epoch no.4 train no.352890  loss = 2.72725 avg_loss = 3.27258\n",
      "epoch no.4 train no.352900  loss = 2.72136 avg_loss = 3.26888\n",
      "epoch no.4 train no.352910  loss = 3.86066 avg_loss = 3.33849\n",
      "epoch no.4 train no.352920  loss = 3.05507 avg_loss = 3.35521\n",
      "epoch no.4 train no.352930  loss = 2.23485 avg_loss = 3.33446\n",
      "epoch no.4 train no.352940  loss = 2.40354 avg_loss = 3.32943\n",
      "epoch no.4 train no.352950  loss = 3.10612 avg_loss = 3.30932\n",
      "epoch no.4 train no.352960  loss = 3.36009 avg_loss = 3.29605\n",
      "epoch no.4 train no.352970  loss = 2.26116 avg_loss = 3.25313\n",
      "epoch no.4 train no.352980  loss = 2.83163 avg_loss = 3.25721\n",
      "epoch no.4 train no.352990  loss = 3.98656 avg_loss = 3.26271\n",
      "epoch no.4 train no.353000  loss = 2.81188 avg_loss = 3.23576\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '▁가기', '전에', '▁이별', '</s>', '▁듣는', '▁때', '</s>']\n",
      "여름이 가기 전에 이별하고 싶을 때</s>\n",
      "epoch no.4 train no.353010  loss = 2.86230 avg_loss = 3.24959\n",
      "epoch no.4 train no.353020  loss = 3.92012 avg_loss = 3.21444\n",
      "epoch no.4 train no.353030  loss = 3.36100 avg_loss = 3.22374\n",
      "epoch no.4 train no.353040  loss = 3.32713 avg_loss = 3.20101\n",
      "epoch no.4 train no.353050  loss = 4.06641 avg_loss = 3.19852\n",
      "epoch no.4 train no.353060  loss = 2.79274 avg_loss = 3.18652\n",
      "epoch no.4 train no.353070  loss = 4.16040 avg_loss = 3.17979\n",
      "epoch no.4 train no.353080  loss = 4.14477 avg_loss = 3.18168\n",
      "epoch no.4 train no.353090  loss = 3.11033 avg_loss = 3.20901\n",
      "epoch no.4 train no.353100  loss = 3.22953 avg_loss = 3.22606\n",
      "epoch no.4 train no.353110  loss = 2.16160 avg_loss = 3.20818\n",
      "epoch no.4 train no.353120  loss = 3.82813 avg_loss = 3.22368\n",
      "epoch no.4 train no.353130  loss = 3.06925 avg_loss = 3.24015\n",
      "epoch no.4 train no.353140  loss = 2.98813 avg_loss = 3.26230\n",
      "epoch no.4 train no.353150  loss = 4.69464 avg_loss = 3.24623\n",
      "epoch no.4 train no.353160  loss = 3.12579 avg_loss = 3.28550\n",
      "epoch no.4 train no.353170  loss = 3.94183 avg_loss = 3.26531\n",
      "epoch no.4 train no.353180  loss = 3.27020 avg_loss = 3.28807\n",
      "epoch no.4 train no.353190  loss = 4.18801 avg_loss = 3.27742\n",
      "epoch no.4 train no.353200  loss = 5.32098 avg_loss = 3.29018\n",
      "epoch no.4 train no.353210  loss = 2.68000 avg_loss = 3.23507\n",
      "epoch no.4 train no.353220  loss = 2.70224 avg_loss = 3.23199\n",
      "epoch no.4 train no.353230  loss = 4.48163 avg_loss = 3.24321\n",
      "epoch no.4 train no.353240  loss = 3.12479 avg_loss = 3.21752\n",
      "epoch no.4 train no.353250  loss = 3.26302 avg_loss = 3.22993\n",
      "epoch no.4 train no.353260  loss = 3.72752 avg_loss = 3.24021\n",
      "epoch no.4 train no.353270  loss = 3.84615 avg_loss = 3.25816\n",
      "epoch no.4 train no.353280  loss = 2.45805 avg_loss = 3.25638\n",
      "epoch no.4 train no.353290  loss = 3.11812 avg_loss = 3.25147\n",
      "epoch no.4 train no.353300  loss = 2.79631 avg_loss = 3.23928\n",
      "epoch no.4 train no.353310  loss = 4.38390 avg_loss = 3.27268\n",
      "epoch no.4 train no.353320  loss = 3.98376 avg_loss = 3.23456\n",
      "epoch no.4 train no.353330  loss = 3.03969 avg_loss = 3.18517\n",
      "epoch no.4 train no.353340  loss = 1.98200 avg_loss = 3.22578\n",
      "epoch no.4 train no.353350  loss = 3.34714 avg_loss = 3.19662\n",
      "epoch no.4 train no.353360  loss = 2.96019 avg_loss = 3.21053\n",
      "epoch no.4 train no.353370  loss = 4.34380 avg_loss = 3.23430\n",
      "epoch no.4 train no.353380  loss = 2.54887 avg_loss = 3.24435\n",
      "epoch no.4 train no.353390  loss = 3.24832 avg_loss = 3.22955\n",
      "epoch no.4 train no.353400  loss = 2.78215 avg_loss = 3.25350\n",
      "epoch no.4 train no.353410  loss = 2.81799 avg_loss = 3.26868\n",
      "epoch no.4 train no.353420  loss = 3.37712 avg_loss = 3.27939\n",
      "epoch no.4 train no.353430  loss = 4.11282 avg_loss = 3.31379\n",
      "epoch no.4 train no.353440  loss = 4.12222 avg_loss = 3.29893\n",
      "epoch no.4 train no.353450  loss = 3.06733 avg_loss = 3.33189\n",
      "epoch no.4 train no.353460  loss = 2.98544 avg_loss = 3.31764\n",
      "epoch no.4 train no.353470  loss = 3.56774 avg_loss = 3.32366\n",
      "epoch no.4 train no.353480  loss = 5.14148 avg_loss = 3.31323\n",
      "epoch no.4 train no.353490  loss = 1.99571 avg_loss = 3.35278\n",
      "epoch no.4 train no.353500  loss = 3.08220 avg_loss = 3.38802\n",
      "epoch no.4 train no.353510  loss = 3.27208 avg_loss = 3.39196\n",
      "epoch no.4 train no.353520  loss = 4.45963 avg_loss = 3.38534\n",
      "epoch no.4 train no.353530  loss = 2.64407 avg_loss = 3.35467\n",
      "epoch no.4 train no.353540  loss = 3.22333 avg_loss = 3.31348\n",
      "epoch no.4 train no.353550  loss = 2.16382 avg_loss = 3.27841\n",
      "epoch no.4 train no.353560  loss = 2.21493 avg_loss = 3.26480\n",
      "epoch no.4 train no.353570  loss = 3.83019 avg_loss = 3.27993\n",
      "epoch no.4 train no.353580  loss = 1.91764 avg_loss = 3.23998\n",
      "epoch no.4 train no.353590  loss = 2.17652 avg_loss = 3.19591\n",
      "epoch no.4 train no.353600  loss = 2.35095 avg_loss = 3.16612\n",
      "epoch no.4 train no.353610  loss = 4.92058 avg_loss = 3.19205\n",
      "epoch no.4 train no.353620  loss = 3.05705 avg_loss = 3.23364\n",
      "epoch no.4 train no.353630  loss = 4.13814 avg_loss = 3.26955\n",
      "epoch no.4 train no.353640  loss = 2.50155 avg_loss = 3.29855\n",
      "epoch no.4 train no.353650  loss = 2.68150 avg_loss = 3.31003\n",
      "epoch no.4 train no.353660  loss = 3.16852 avg_loss = 3.30271\n",
      "epoch no.4 train no.353670  loss = 3.36169 avg_loss = 3.30843\n",
      "epoch no.4 train no.353680  loss = 4.02759 avg_loss = 3.28626\n",
      "epoch no.4 train no.353690  loss = 2.57904 avg_loss = 3.28441\n",
      "epoch no.4 train no.353700  loss = 4.30667 avg_loss = 3.24004\n",
      "epoch no.4 train no.353710  loss = 4.94317 avg_loss = 3.24324\n",
      "epoch no.4 train no.353720  loss = 2.32322 avg_loss = 3.22466\n",
      "epoch no.4 train no.353730  loss = 4.07998 avg_loss = 3.23188\n",
      "epoch no.4 train no.353740  loss = 3.84291 avg_loss = 3.20998\n",
      "epoch no.4 train no.353750  loss = 2.80383 avg_loss = 3.15751\n",
      "epoch no.4 train no.353760  loss = 2.86035 avg_loss = 3.21716\n",
      "epoch no.4 train no.353770  loss = 3.58229 avg_loss = 3.21545\n",
      "epoch no.4 train no.353780  loss = 3.34440 avg_loss = 3.24140\n",
      "epoch no.4 train no.353790  loss = 1.77826 avg_loss = 3.27672\n",
      "epoch no.4 train no.353800  loss = 4.61469 avg_loss = 3.26773\n",
      "epoch no.4 train no.353810  loss = 5.37135 avg_loss = 3.32292\n",
      "epoch no.4 train no.353820  loss = 3.11831 avg_loss = 3.33839\n",
      "epoch no.4 train no.353830  loss = 2.17265 avg_loss = 3.31048\n",
      "epoch no.4 train no.353840  loss = 3.02822 avg_loss = 3.31083\n",
      "epoch no.4 train no.353850  loss = 2.97082 avg_loss = 3.28137\n",
      "epoch no.4 train no.353860  loss = 2.58862 avg_loss = 3.26663\n",
      "epoch no.4 train no.353870  loss = 3.28197 avg_loss = 3.28025\n",
      "epoch no.4 train no.353880  loss = 2.69266 avg_loss = 3.25780\n",
      "epoch no.4 train no.353890  loss = 4.49074 avg_loss = 3.26753\n",
      "epoch no.4 train no.353900  loss = 3.73745 avg_loss = 3.31267\n",
      "epoch no.4 train no.353910  loss = 3.29451 avg_loss = 3.30678\n",
      "epoch no.4 train no.353920  loss = 5.86783 avg_loss = 3.34399\n",
      "epoch no.4 train no.353930  loss = 3.76389 avg_loss = 3.32343\n",
      "epoch no.4 train no.353940  loss = 1.58319 avg_loss = 3.27906\n",
      "epoch no.4 train no.353950  loss = 2.25478 avg_loss = 3.28252\n",
      "epoch no.4 train no.353960  loss = 4.22214 avg_loss = 3.30350\n",
      "epoch no.4 train no.353970  loss = 1.38720 avg_loss = 3.26934\n",
      "epoch no.4 train no.353980  loss = 3.64129 avg_loss = 3.24202\n",
      "epoch no.4 train no.353990  loss = 2.53102 avg_loss = 3.27357\n",
      "epoch no.4 train no.354000  loss = 2.71095 avg_loss = 3.27974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "to_tokens: ['▁가을', '밤', '▁이', '▁', '▁락', '</s>']\n",
      "여름엔 역시 시원한 락</s>\n",
      "epoch no.4 train no.354010  loss = 2.31858 avg_loss = 3.27382\n",
      "epoch no.4 train no.354020  loss = 3.61243 avg_loss = 3.27807\n",
      "epoch no.4 train no.354030  loss = 2.20779 avg_loss = 3.26722\n",
      "epoch no.4 train no.354040  loss = 1.94997 avg_loss = 3.27433\n",
      "epoch no.4 train no.354050  loss = 3.33044 avg_loss = 3.26389\n",
      "epoch no.4 train no.354060  loss = 3.14914 avg_loss = 3.24467\n",
      "epoch no.4 train no.354070  loss = 2.83640 avg_loss = 3.25012\n",
      "epoch no.4 train no.354080  loss = 2.82656 avg_loss = 3.25405\n",
      "epoch no.4 train no.354090  loss = 2.57918 avg_loss = 3.25509\n",
      "epoch no.4 train no.354100  loss = 4.14431 avg_loss = 3.28181\n",
      "epoch no.4 train no.354110  loss = 4.52240 avg_loss = 3.35958\n",
      "epoch no.4 train no.354120  loss = 3.38037 avg_loss = 3.36017\n",
      "epoch no.4 train no.354130  loss = 3.73053 avg_loss = 3.31153\n",
      "epoch no.4 train no.354140  loss = 2.07979 avg_loss = 3.31570\n",
      "epoch no.4 train no.354150  loss = 2.46913 avg_loss = 3.31478\n",
      "epoch no.4 train no.354160  loss = 3.79042 avg_loss = 3.28140\n",
      "epoch no.4 train no.354170  loss = 2.70353 avg_loss = 3.25180\n",
      "epoch no.4 train no.354180  loss = 2.69092 avg_loss = 3.24329\n",
      "epoch no.4 train no.354190  loss = 3.46010 avg_loss = 3.22957\n",
      "epoch no.4 train no.354200  loss = 4.35868 avg_loss = 3.23400\n",
      "epoch no.4 train no.354210  loss = 3.08765 avg_loss = 3.23828\n",
      "epoch no.4 train no.354220  loss = 2.19651 avg_loss = 3.20986\n",
      "epoch no.4 train no.354230  loss = 3.06764 avg_loss = 3.24923\n",
      "epoch no.4 train no.354240  loss = 3.64650 avg_loss = 3.21685\n",
      "epoch no.4 train no.354250  loss = 3.13040 avg_loss = 3.22165\n",
      "epoch no.4 train no.354260  loss = 1.57165 avg_loss = 3.15676\n",
      "epoch no.4 train no.354270  loss = 4.39427 avg_loss = 3.22550\n",
      "epoch no.4 train no.354280  loss = 3.77312 avg_loss = 3.21509\n",
      "epoch no.4 train no.354290  loss = 3.14056 avg_loss = 3.22122\n",
      "epoch no.4 train no.354300  loss = 3.38580 avg_loss = 3.26218\n",
      "epoch no.4 train no.354310  loss = 2.73880 avg_loss = 3.23062\n",
      "epoch no.4 train no.354320  loss = 4.20283 avg_loss = 3.22533\n",
      "epoch no.4 train no.354330  loss = 3.63997 avg_loss = 3.25321\n",
      "epoch no.4 train no.354340  loss = 2.74849 avg_loss = 3.25420\n",
      "epoch no.4 train no.354350  loss = 2.97441 avg_loss = 3.27689\n",
      "epoch no.4 train no.354360  loss = 3.09911 avg_loss = 3.29202\n",
      "epoch no.4 train no.354370  loss = 3.12709 avg_loss = 3.28761\n",
      "epoch no.4 train no.354380  loss = 1.89411 avg_loss = 3.27532\n",
      "epoch no.4 train no.354390  loss = 2.56615 avg_loss = 3.25970\n",
      "epoch no.4 train no.354400  loss = 2.63047 avg_loss = 3.25547\n",
      "epoch no.4 train no.354410  loss = 3.18316 avg_loss = 3.21708\n",
      "epoch no.4 train no.354420  loss = 3.20723 avg_loss = 3.21856\n",
      "epoch no.4 train no.354430  loss = 2.17743 avg_loss = 3.26292\n",
      "epoch no.4 train no.354440  loss = 3.26268 avg_loss = 3.27439\n",
      "epoch no.4 train no.354450  loss = 2.74458 avg_loss = 3.29271\n",
      "epoch no.4 train no.354460  loss = 3.39188 avg_loss = 3.29201\n",
      "epoch no.4 train no.354470  loss = 1.79851 avg_loss = 3.25130\n",
      "epoch no.4 train no.354480  loss = 3.23500 avg_loss = 3.22301\n",
      "epoch no.4 train no.354490  loss = 3.46194 avg_loss = 3.28694\n",
      "epoch no.4 train no.354500  loss = 2.06337 avg_loss = 3.26231\n",
      "epoch no.4 train no.354510  loss = 4.01310 avg_loss = 3.22197\n",
      "epoch no.4 train no.354520  loss = 3.32925 avg_loss = 3.22459\n",
      "epoch no.4 train no.354530  loss = 2.60451 avg_loss = 3.20126\n",
      "epoch no.4 train no.354540  loss = 3.63935 avg_loss = 3.17820\n",
      "epoch no.4 train no.354550  loss = 3.53389 avg_loss = 3.20546\n",
      "epoch no.4 train no.354560  loss = 4.23488 avg_loss = 3.22709\n",
      "epoch no.4 train no.354570  loss = 3.13519 avg_loss = 3.22431\n",
      "epoch no.4 train no.354580  loss = 2.40657 avg_loss = 3.21885\n",
      "epoch no.4 train no.354590  loss = 3.73768 avg_loss = 3.20817\n",
      "epoch no.4 train no.354600  loss = 4.12790 avg_loss = 3.17950\n",
      "epoch no.4 train no.354610  loss = 3.11851 avg_loss = 3.18390\n",
      "epoch no.4 train no.354620  loss = 3.19571 avg_loss = 3.20905\n",
      "epoch no.4 train no.354630  loss = 3.00458 avg_loss = 3.24332\n",
      "epoch no.4 train no.354640  loss = 1.66197 avg_loss = 3.20783\n",
      "epoch no.4 train no.354650  loss = 3.29524 avg_loss = 3.24314\n",
      "epoch no.4 train no.354660  loss = 2.54598 avg_loss = 3.28864\n",
      "epoch no.4 train no.354670  loss = 5.49197 avg_loss = 3.29291\n",
      "epoch no.4 train no.354680  loss = 3.06862 avg_loss = 3.28833\n",
      "epoch no.4 train no.354690  loss = 2.91173 avg_loss = 3.24767\n",
      "epoch no.4 train no.354700  loss = 2.87619 avg_loss = 3.23224\n",
      "epoch no.4 train no.354710  loss = 2.32788 avg_loss = 3.20193\n",
      "epoch no.4 train no.354720  loss = 4.57897 avg_loss = 3.24005\n",
      "epoch no.4 train no.354730  loss = 2.92229 avg_loss = 3.22882\n",
      "epoch no.4 train no.354740  loss = 3.25668 avg_loss = 3.21021\n",
      "epoch no.4 train no.354750  loss = 2.60041 avg_loss = 3.23602\n",
      "epoch no.4 train no.354760  loss = 3.12898 avg_loss = 3.26136\n",
      "epoch no.4 train no.354770  loss = 2.59406 avg_loss = 3.28592\n",
      "epoch no.4 train no.354780  loss = 1.78276 avg_loss = 3.22954\n",
      "epoch no.4 train no.354790  loss = 4.06681 avg_loss = 3.20108\n",
      "epoch no.4 train no.354800  loss = 4.16439 avg_loss = 3.19922\n",
      "epoch no.4 train no.354810  loss = 3.43973 avg_loss = 3.19997\n",
      "epoch no.4 train no.354820  loss = 3.34168 avg_loss = 3.20358\n",
      "epoch no.4 train no.354830  loss = 2.81444 avg_loss = 3.17377\n",
      "epoch no.4 train no.354840  loss = 2.61566 avg_loss = 3.13147\n",
      "epoch no.4 train no.354850  loss = 1.97338 avg_loss = 3.12972\n",
      "epoch no.4 train no.354860  loss = 2.97505 avg_loss = 3.13642\n",
      "epoch no.4 train no.354870  loss = 2.47051 avg_loss = 3.10195\n",
      "epoch no.4 train no.354880  loss = 2.97998 avg_loss = 3.15110\n",
      "epoch no.4 train no.354890  loss = 2.33502 avg_loss = 3.19677\n",
      "epoch no.4 train no.354900  loss = 3.22129 avg_loss = 3.19973\n",
      "epoch no.4 train no.354910  loss = 5.04670 avg_loss = 3.22898\n",
      "epoch no.4 train no.354920  loss = 3.89497 avg_loss = 3.23485\n",
      "epoch no.4 train no.354930  loss = 3.37770 avg_loss = 3.20294\n",
      "epoch no.4 train no.354940  loss = 2.61822 avg_loss = 3.20786\n",
      "epoch no.4 train no.354950  loss = 3.12211 avg_loss = 3.18413\n",
      "epoch no.4 train no.354960  loss = 2.44718 avg_loss = 3.14739\n",
      "epoch no.4 train no.354970  loss = 3.57228 avg_loss = 3.16400\n",
      "epoch no.4 train no.354980  loss = 2.40516 avg_loss = 3.17909\n",
      "epoch no.4 train no.354990  loss = 2.92894 avg_loss = 3.19013\n",
      "epoch no.4 train no.355000  loss = 3.23764 avg_loss = 3.17543\n",
      "4\n",
      "to_tokens: ['▁가을', '밤', '의', '▁꿀', '브', '</s>']\n",
      "여름밤의 그루브</s>\n",
      "epoch no.4 train no.355010  loss = 4.20384 avg_loss = 3.19626\n",
      "epoch no.4 train no.355020  loss = 4.15364 avg_loss = 3.20451\n",
      "epoch no.4 train no.355030  loss = 2.18006 avg_loss = 3.14516\n",
      "epoch no.4 train no.355040  loss = 3.20076 avg_loss = 3.18711\n",
      "epoch no.4 train no.355050  loss = 2.60969 avg_loss = 3.20336\n",
      "epoch no.4 train no.355060  loss = 1.96083 avg_loss = 3.19696\n",
      "epoch no.4 train no.355070  loss = 3.21246 avg_loss = 3.17401\n",
      "epoch no.4 train no.355080  loss = 2.35866 avg_loss = 3.18750\n",
      "epoch no.4 train no.355090  loss = 2.35100 avg_loss = 3.23806\n",
      "epoch no.4 train no.355100  loss = 2.96047 avg_loss = 3.24345\n",
      "epoch no.4 train no.355110  loss = 3.66587 avg_loss = 3.22921\n",
      "epoch no.4 train no.355120  loss = 4.92559 avg_loss = 3.23020\n",
      "epoch no.4 train no.355130  loss = 2.31573 avg_loss = 3.26309\n",
      "epoch no.4 train no.355140  loss = 4.14406 avg_loss = 3.25934\n",
      "epoch no.4 train no.355150  loss = 2.59838 avg_loss = 3.25277\n",
      "epoch no.4 train no.355160  loss = 3.68115 avg_loss = 3.22384\n",
      "epoch no.4 train no.355170  loss = 3.33143 avg_loss = 3.27367\n",
      "epoch no.4 train no.355180  loss = 4.58100 avg_loss = 3.25379\n",
      "epoch no.4 train no.355190  loss = 2.98579 avg_loss = 3.24404\n",
      "epoch no.4 train no.355200  loss = 3.55219 avg_loss = 3.26094\n",
      "epoch no.4 train no.355210  loss = 2.51532 avg_loss = 3.24685\n",
      "epoch no.4 train no.355220  loss = 4.75538 avg_loss = 3.24374\n",
      "epoch no.4 train no.355230  loss = 3.83660 avg_loss = 3.20609\n",
      "epoch no.4 train no.355240  loss = 3.50263 avg_loss = 3.21874\n",
      "epoch no.4 train no.355250  loss = 5.79147 avg_loss = 3.21878\n",
      "epoch no.4 train no.355260  loss = 3.43350 avg_loss = 3.22036\n",
      "epoch no.4 train no.355270  loss = 2.96498 avg_loss = 3.24959\n",
      "epoch no.4 train no.355280  loss = 4.17923 avg_loss = 3.29876\n",
      "epoch no.4 train no.355290  loss = 3.34218 avg_loss = 3.32985\n",
      "epoch no.4 train no.355300  loss = 3.38420 avg_loss = 3.32234\n",
      "epoch no.4 train no.355310  loss = 3.04690 avg_loss = 3.31698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.355320  loss = 6.58181 avg_loss = 3.29230\n",
      "epoch no.4 train no.355330  loss = 4.34709 avg_loss = 3.28901\n",
      "epoch no.4 train no.355340  loss = 2.16678 avg_loss = 3.25268\n",
      "epoch no.4 train no.355350  loss = 2.93058 avg_loss = 3.26778\n",
      "epoch no.4 train no.355360  loss = 3.50059 avg_loss = 3.27669\n",
      "epoch no.4 train no.355370  loss = 2.75843 avg_loss = 3.31833\n",
      "epoch no.4 train no.355380  loss = 2.96594 avg_loss = 3.30964\n",
      "epoch no.4 train no.355390  loss = 3.39551 avg_loss = 3.30109\n",
      "epoch no.4 train no.355400  loss = 2.78038 avg_loss = 3.31983\n",
      "epoch no.4 train no.355410  loss = 1.09438 avg_loss = 3.32851\n",
      "epoch no.4 train no.355420  loss = 2.52807 avg_loss = 3.28545\n",
      "epoch no.4 train no.355430  loss = 2.95292 avg_loss = 3.29681\n",
      "epoch no.4 train no.355440  loss = 2.91925 avg_loss = 3.30975\n",
      "epoch no.4 train no.355450  loss = 4.84976 avg_loss = 3.32858\n",
      "epoch no.4 train no.355460  loss = 2.80697 avg_loss = 3.30449\n",
      "epoch no.4 train no.355470  loss = 3.34899 avg_loss = 3.27191\n",
      "epoch no.4 train no.355480  loss = 2.25306 avg_loss = 3.30232\n",
      "epoch no.4 train no.355490  loss = 2.95639 avg_loss = 3.30094\n",
      "epoch no.4 train no.355500  loss = 2.87763 avg_loss = 3.31462\n",
      "epoch no.4 train no.355510  loss = 3.92971 avg_loss = 3.32108\n",
      "epoch no.4 train no.355520  loss = 2.71359 avg_loss = 3.27326\n",
      "epoch no.4 train no.355530  loss = 4.78557 avg_loss = 3.27840\n",
      "epoch no.4 train no.355540  loss = 3.02839 avg_loss = 3.28136\n",
      "epoch no.4 train no.355550  loss = 3.20643 avg_loss = 3.30393\n",
      "epoch no.4 train no.355560  loss = 5.39763 avg_loss = 3.27253\n",
      "epoch no.4 train no.355570  loss = 4.17070 avg_loss = 3.26102\n",
      "epoch no.4 train no.355580  loss = 3.25553 avg_loss = 3.24689\n",
      "epoch no.4 train no.355590  loss = 2.97875 avg_loss = 3.27981\n",
      "epoch no.4 train no.355600  loss = 2.66886 avg_loss = 3.26026\n",
      "epoch no.4 train no.355610  loss = 2.64396 avg_loss = 3.28879\n",
      "epoch no.4 train no.355620  loss = 3.18248 avg_loss = 3.28843\n",
      "epoch no.4 train no.355630  loss = 3.60226 avg_loss = 3.27083\n",
      "epoch no.4 train no.355640  loss = 2.39813 avg_loss = 3.25953\n",
      "epoch no.4 train no.355650  loss = 4.34465 avg_loss = 3.29557\n",
      "epoch no.4 train no.355660  loss = 1.44578 avg_loss = 3.28478\n",
      "epoch no.4 train no.355670  loss = 2.59098 avg_loss = 3.25686\n",
      "epoch no.4 train no.355680  loss = 3.91102 avg_loss = 3.27265\n",
      "epoch no.4 train no.355690  loss = 2.45131 avg_loss = 3.25394\n",
      "epoch no.4 train no.355700  loss = 2.37493 avg_loss = 3.21640\n",
      "epoch no.4 train no.355710  loss = 3.12127 avg_loss = 3.23857\n",
      "epoch no.4 train no.355720  loss = 3.35771 avg_loss = 3.21654\n",
      "epoch no.4 train no.355730  loss = 2.56126 avg_loss = 3.19618\n",
      "epoch no.4 train no.355740  loss = 2.15825 avg_loss = 3.20280\n",
      "epoch no.4 train no.355750  loss = 2.16978 avg_loss = 3.17858\n",
      "epoch no.4 train no.355760  loss = 3.12863 avg_loss = 3.18099\n",
      "epoch no.4 train no.355770  loss = 3.45111 avg_loss = 3.20293\n",
      "epoch no.4 train no.355780  loss = 1.87181 avg_loss = 3.20245\n",
      "epoch no.4 train no.355790  loss = 3.66947 avg_loss = 3.16263\n",
      "epoch no.4 train no.355800  loss = 1.81378 avg_loss = 3.18016\n",
      "epoch no.4 train no.355810  loss = 4.25501 avg_loss = 3.20685\n",
      "epoch no.4 train no.355820  loss = 3.78374 avg_loss = 3.18565\n",
      "epoch no.4 train no.355830  loss = 2.68415 avg_loss = 3.17321\n",
      "epoch no.4 train no.355840  loss = 2.75223 avg_loss = 3.14026\n",
      "epoch no.4 train no.355850  loss = 3.91473 avg_loss = 3.16107\n",
      "epoch no.4 train no.355860  loss = 4.76369 avg_loss = 3.14364\n",
      "epoch no.4 train no.355870  loss = 2.27973 avg_loss = 3.14522\n",
      "epoch no.4 train no.355880  loss = 2.75398 avg_loss = 3.15919\n",
      "epoch no.4 train no.355890  loss = 3.18049 avg_loss = 3.16562\n",
      "epoch no.4 train no.355900  loss = 2.71015 avg_loss = 3.15319\n",
      "epoch no.4 train no.355910  loss = 3.38721 avg_loss = 3.18412\n",
      "epoch no.4 train no.355920  loss = 3.13139 avg_loss = 3.15104\n",
      "epoch no.4 train no.355930  loss = 2.29134 avg_loss = 3.15006\n",
      "epoch no.4 train no.355940  loss = 2.05061 avg_loss = 3.10556\n",
      "epoch no.4 train no.355950  loss = 2.32670 avg_loss = 3.12498\n",
      "epoch no.4 train no.355960  loss = 2.73449 avg_loss = 3.17431\n",
      "epoch no.4 train no.355970  loss = 2.98322 avg_loss = 3.15405\n",
      "epoch no.4 train no.355980  loss = 2.73934 avg_loss = 3.17093\n",
      "epoch no.4 train no.355990  loss = 3.33333 avg_loss = 3.18194\n",
      "epoch no.4 train no.356000  loss = 2.80805 avg_loss = 3.16938\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '의', '▁감성', '▁있는', '▁노래', '</s>']\n",
      "여름밤의 분위기 있는 음악</s>\n",
      "epoch no.4 train no.356010  loss = 4.36610 avg_loss = 3.26112\n",
      "epoch no.4 train no.356020  loss = 3.90607 avg_loss = 3.25609\n",
      "epoch no.4 train no.356030  loss = 2.91593 avg_loss = 3.24735\n",
      "epoch no.4 train no.356040  loss = 1.90969 avg_loss = 3.19720\n",
      "epoch no.4 train no.356050  loss = 1.91105 avg_loss = 3.15429\n",
      "epoch no.4 train no.356060  loss = 3.26549 avg_loss = 3.16188\n",
      "epoch no.4 train no.356070  loss = 4.14771 avg_loss = 3.17131\n",
      "epoch no.4 train no.356080  loss = 2.33350 avg_loss = 3.15351\n",
      "epoch no.4 train no.356090  loss = 5.51253 avg_loss = 3.23563\n",
      "epoch no.4 train no.356100  loss = 1.96286 avg_loss = 3.23116\n",
      "epoch no.4 train no.356110  loss = 4.10351 avg_loss = 3.22968\n",
      "epoch no.4 train no.356120  loss = 3.18324 avg_loss = 3.21749\n",
      "epoch no.4 train no.356130  loss = 2.63164 avg_loss = 3.20423\n",
      "epoch no.4 train no.356140  loss = 3.02118 avg_loss = 3.22690\n",
      "epoch no.4 train no.356150  loss = 3.80796 avg_loss = 3.22025\n",
      "epoch no.4 train no.356160  loss = 2.77903 avg_loss = 3.20103\n",
      "epoch no.4 train no.356170  loss = 2.46358 avg_loss = 3.18379\n",
      "epoch no.4 train no.356180  loss = 1.39307 avg_loss = 3.18227\n",
      "epoch no.4 train no.356190  loss = 3.20695 avg_loss = 3.16838\n",
      "epoch no.4 train no.356200  loss = 1.28866 avg_loss = 3.21905\n",
      "epoch no.4 train no.356210  loss = 4.37048 avg_loss = 3.25702\n",
      "epoch no.4 train no.356220  loss = 4.68374 avg_loss = 3.28683\n",
      "epoch no.4 train no.356230  loss = 2.90874 avg_loss = 3.26321\n",
      "epoch no.4 train no.356240  loss = 2.33111 avg_loss = 3.26040\n",
      "epoch no.4 train no.356250  loss = 3.46210 avg_loss = 3.25218\n",
      "epoch no.4 train no.356260  loss = 3.97315 avg_loss = 3.25995\n",
      "epoch no.4 train no.356270  loss = 2.73834 avg_loss = 3.25960\n",
      "epoch no.4 train no.356280  loss = 3.30498 avg_loss = 3.25021\n",
      "epoch no.4 train no.356290  loss = 2.77521 avg_loss = 3.22984\n",
      "epoch no.4 train no.356300  loss = 3.90502 avg_loss = 3.25132\n",
      "epoch no.4 train no.356310  loss = 2.87233 avg_loss = 3.19753\n",
      "epoch no.4 train no.356320  loss = 1.76670 avg_loss = 3.18264\n",
      "epoch no.4 train no.356330  loss = 2.79583 avg_loss = 3.16226\n",
      "epoch no.4 train no.356340  loss = 3.87545 avg_loss = 3.16642\n",
      "epoch no.4 train no.356350  loss = 3.19883 avg_loss = 3.15721\n",
      "epoch no.4 train no.356360  loss = 4.52095 avg_loss = 3.15393\n",
      "epoch no.4 train no.356370  loss = 2.93048 avg_loss = 3.14202\n",
      "epoch no.4 train no.356380  loss = 3.06583 avg_loss = 3.16084\n",
      "epoch no.4 train no.356390  loss = 3.51819 avg_loss = 3.19263\n",
      "epoch no.4 train no.356400  loss = 3.49986 avg_loss = 3.19245\n",
      "epoch no.4 train no.356410  loss = 2.93797 avg_loss = 3.20423\n",
      "epoch no.4 train no.356420  loss = 3.43535 avg_loss = 3.20626\n",
      "epoch no.4 train no.356430  loss = 2.79311 avg_loss = 3.23594\n",
      "epoch no.4 train no.356440  loss = 4.02185 avg_loss = 3.22923\n",
      "epoch no.4 train no.356450  loss = 3.19396 avg_loss = 3.18933\n",
      "epoch no.4 train no.356460  loss = 3.23859 avg_loss = 3.14018\n",
      "epoch no.4 train no.356470  loss = 3.42103 avg_loss = 3.12772\n",
      "epoch no.4 train no.356480  loss = 2.39371 avg_loss = 3.10398\n",
      "epoch no.4 train no.356490  loss = 2.89647 avg_loss = 3.08833\n",
      "epoch no.4 train no.356500  loss = 3.53004 avg_loss = 3.08499\n",
      "epoch no.4 train no.356510  loss = 4.69330 avg_loss = 3.09660\n",
      "epoch no.4 train no.356520  loss = 2.77728 avg_loss = 3.11146\n",
      "epoch no.4 train no.356530  loss = 3.61245 avg_loss = 3.13781\n",
      "epoch no.4 train no.356540  loss = 4.22166 avg_loss = 3.13356\n",
      "epoch no.4 train no.356550  loss = 4.54390 avg_loss = 3.13968\n",
      "epoch no.4 train no.356560  loss = 2.19345 avg_loss = 3.11625\n",
      "epoch no.4 train no.356570  loss = 5.74150 avg_loss = 3.18008\n",
      "epoch no.4 train no.356580  loss = 3.77445 avg_loss = 3.21948\n",
      "epoch no.4 train no.356590  loss = 4.13112 avg_loss = 3.21927\n",
      "epoch no.4 train no.356600  loss = 3.07848 avg_loss = 3.22661\n",
      "epoch no.4 train no.356610  loss = 2.93544 avg_loss = 3.22438\n",
      "epoch no.4 train no.356620  loss = 3.65742 avg_loss = 3.20342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.356630  loss = 3.26053 avg_loss = 3.23868\n",
      "epoch no.4 train no.356640  loss = 3.40491 avg_loss = 3.26012\n",
      "epoch no.4 train no.356650  loss = 3.21921 avg_loss = 3.22106\n",
      "epoch no.4 train no.356660  loss = 3.39276 avg_loss = 3.19589\n",
      "epoch no.4 train no.356670  loss = 2.98955 avg_loss = 3.21132\n",
      "epoch no.4 train no.356680  loss = 5.16836 avg_loss = 3.23242\n",
      "epoch no.4 train no.356690  loss = 4.04388 avg_loss = 3.22522\n",
      "epoch no.4 train no.356700  loss = 2.55628 avg_loss = 3.26081\n",
      "epoch no.4 train no.356710  loss = 2.41964 avg_loss = 3.21984\n",
      "epoch no.4 train no.356720  loss = 3.71831 avg_loss = 3.23078\n",
      "epoch no.4 train no.356730  loss = 2.98919 avg_loss = 3.22377\n",
      "epoch no.4 train no.356740  loss = 3.77738 avg_loss = 3.26110\n",
      "epoch no.4 train no.356750  loss = 4.55908 avg_loss = 3.25495\n",
      "epoch no.4 train no.356760  loss = 4.22907 avg_loss = 3.23114\n",
      "epoch no.4 train no.356770  loss = 2.01925 avg_loss = 3.24994\n",
      "epoch no.4 train no.356780  loss = 3.27652 avg_loss = 3.24570\n",
      "epoch no.4 train no.356790  loss = 2.71826 avg_loss = 3.23288\n",
      "epoch no.4 train no.356800  loss = 3.20173 avg_loss = 3.21896\n",
      "epoch no.4 train no.356810  loss = 4.53366 avg_loss = 3.24354\n",
      "epoch no.4 train no.356820  loss = 2.72542 avg_loss = 3.26881\n",
      "epoch no.4 train no.356830  loss = 2.97343 avg_loss = 3.28831\n",
      "epoch no.4 train no.356840  loss = 6.14656 avg_loss = 3.31426\n",
      "epoch no.4 train no.356850  loss = 2.91668 avg_loss = 3.26690\n",
      "epoch no.4 train no.356860  loss = 3.30990 avg_loss = 3.25734\n",
      "epoch no.4 train no.356870  loss = 4.17927 avg_loss = 3.31669\n",
      "epoch no.4 train no.356880  loss = 3.93606 avg_loss = 3.29622\n",
      "epoch no.4 train no.356890  loss = 3.59787 avg_loss = 3.30456\n",
      "epoch no.4 train no.356900  loss = 4.79169 avg_loss = 3.33245\n",
      "epoch no.4 train no.356910  loss = 2.31842 avg_loss = 3.33188\n",
      "epoch no.4 train no.356920  loss = 5.18101 avg_loss = 3.33063\n",
      "epoch no.4 train no.356930  loss = 2.44992 avg_loss = 3.30490\n",
      "epoch no.4 train no.356940  loss = 3.31165 avg_loss = 3.25973\n",
      "epoch no.4 train no.356950  loss = 2.72470 avg_loss = 3.21553\n",
      "epoch no.4 train no.356960  loss = 2.94683 avg_loss = 3.18405\n",
      "epoch no.4 train no.356970  loss = 2.38023 avg_loss = 3.15159\n",
      "epoch no.4 train no.356980  loss = 3.91116 avg_loss = 3.17470\n",
      "epoch no.4 train no.356990  loss = 2.74079 avg_loss = 3.16855\n",
      "epoch no.4 train no.357000  loss = 2.95159 avg_loss = 3.19442\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '을', '▁수', '놓을', '▁감성', '</s>', '</s>']\n",
      "여름밤을 수놓을 음악들</s>\n",
      "epoch no.4 train no.357010  loss = 4.62580 avg_loss = 3.16505\n",
      "epoch no.4 train no.357020  loss = 3.95018 avg_loss = 3.16390\n",
      "epoch no.4 train no.357030  loss = 3.73700 avg_loss = 3.12081\n",
      "epoch no.4 train no.357040  loss = 3.42981 avg_loss = 3.14499\n",
      "epoch no.4 train no.357050  loss = 3.07929 avg_loss = 3.14459\n",
      "epoch no.4 train no.357060  loss = 3.59258 avg_loss = 3.12927\n",
      "epoch no.4 train no.357070  loss = 2.71213 avg_loss = 3.12223\n",
      "epoch no.4 train no.357080  loss = 3.31918 avg_loss = 3.15145\n",
      "epoch no.4 train no.357090  loss = 2.59312 avg_loss = 3.16339\n",
      "epoch no.4 train no.357100  loss = 2.58510 avg_loss = 3.16447\n",
      "epoch no.4 train no.357110  loss = 2.91875 avg_loss = 3.18749\n",
      "epoch no.4 train no.357120  loss = 2.72560 avg_loss = 3.22252\n",
      "epoch no.4 train no.357130  loss = 2.47076 avg_loss = 3.26724\n",
      "epoch no.4 train no.357140  loss = 4.02139 avg_loss = 3.31977\n",
      "epoch no.4 train no.357150  loss = 3.52354 avg_loss = 3.31897\n",
      "epoch no.4 train no.357160  loss = 3.55391 avg_loss = 3.35766\n",
      "epoch no.4 train no.357170  loss = 3.04423 avg_loss = 3.32214\n",
      "epoch no.4 train no.357180  loss = 2.93722 avg_loss = 3.32510\n",
      "epoch no.4 train no.357190  loss = 2.84134 avg_loss = 3.31886\n",
      "epoch no.4 train no.357200  loss = 2.61855 avg_loss = 3.28089\n",
      "epoch no.4 train no.357210  loss = 3.23751 avg_loss = 3.26622\n",
      "epoch no.4 train no.357220  loss = 2.88141 avg_loss = 3.23594\n",
      "epoch no.4 train no.357230  loss = 5.28836 avg_loss = 3.27426\n",
      "epoch no.4 train no.357240  loss = 2.73664 avg_loss = 3.24931\n",
      "epoch no.4 train no.357250  loss = 3.51477 avg_loss = 3.30066\n",
      "epoch no.4 train no.357260  loss = 1.51010 avg_loss = 3.31941\n",
      "epoch no.4 train no.357270  loss = 2.02418 avg_loss = 3.34915\n",
      "epoch no.4 train no.357280  loss = 2.29203 avg_loss = 3.34159\n",
      "epoch no.4 train no.357290  loss = 3.07653 avg_loss = 3.30169\n",
      "epoch no.4 train no.357300  loss = 3.41217 avg_loss = 3.31761\n",
      "epoch no.4 train no.357310  loss = 2.83183 avg_loss = 3.32528\n",
      "epoch no.4 train no.357320  loss = 3.56361 avg_loss = 3.31814\n",
      "epoch no.4 train no.357330  loss = 3.80318 avg_loss = 3.28149\n",
      "epoch no.4 train no.357340  loss = 3.88008 avg_loss = 3.27173\n",
      "epoch no.4 train no.357350  loss = 3.71303 avg_loss = 3.28842\n",
      "epoch no.4 train no.357360  loss = 3.61178 avg_loss = 3.25935\n",
      "epoch no.4 train no.357370  loss = 2.71051 avg_loss = 3.25773\n",
      "epoch no.4 train no.357380  loss = 4.53802 avg_loss = 3.27408\n",
      "epoch no.4 train no.357390  loss = 4.54655 avg_loss = 3.29753\n",
      "epoch no.4 train no.357400  loss = 3.44362 avg_loss = 3.31444\n",
      "epoch no.4 train no.357410  loss = 3.79947 avg_loss = 3.31583\n",
      "epoch no.4 train no.357420  loss = 3.91371 avg_loss = 3.32085\n",
      "epoch no.4 train no.357430  loss = 3.25529 avg_loss = 3.33950\n",
      "epoch no.4 train no.357440  loss = 3.86067 avg_loss = 3.31648\n",
      "epoch no.4 train no.357450  loss = 2.82479 avg_loss = 3.29862\n",
      "epoch no.4 train no.357460  loss = 2.62199 avg_loss = 3.32753\n",
      "epoch no.4 train no.357470  loss = 3.90166 avg_loss = 3.32781\n",
      "epoch no.4 train no.357480  loss = 3.36364 avg_loss = 3.33253\n",
      "epoch no.4 train no.357490  loss = 4.50008 avg_loss = 3.32536\n",
      "epoch no.4 train no.357500  loss = 3.26771 avg_loss = 3.31249\n",
      "epoch no.4 train no.357510  loss = 2.61977 avg_loss = 3.24117\n",
      "epoch no.4 train no.357520  loss = 3.80985 avg_loss = 3.24093\n",
      "epoch no.4 train no.357530  loss = 3.66775 avg_loss = 3.24940\n",
      "epoch no.4 train no.357540  loss = 3.26025 avg_loss = 3.26997\n",
      "epoch no.4 train no.357550  loss = 6.00628 avg_loss = 3.28329\n",
      "epoch no.4 train no.357560  loss = 2.74636 avg_loss = 3.28883\n",
      "epoch no.4 train no.357570  loss = 3.36985 avg_loss = 3.27128\n",
      "epoch no.4 train no.357580  loss = 3.06137 avg_loss = 3.21246\n",
      "epoch no.4 train no.357590  loss = 4.49765 avg_loss = 3.22987\n",
      "epoch no.4 train no.357600  loss = 2.03264 avg_loss = 3.15561\n",
      "epoch no.4 train no.357610  loss = 3.58173 avg_loss = 3.14854\n",
      "epoch no.4 train no.357620  loss = 2.62227 avg_loss = 3.13323\n",
      "epoch no.4 train no.357630  loss = 2.75125 avg_loss = 3.16232\n",
      "epoch no.4 train no.357640  loss = 3.27998 avg_loss = 3.21303\n",
      "epoch no.4 train no.357650  loss = 3.43727 avg_loss = 3.21962\n",
      "epoch no.4 train no.357660  loss = 3.46700 avg_loss = 3.22199\n",
      "epoch no.4 train no.357670  loss = 2.12628 avg_loss = 3.20995\n",
      "epoch no.4 train no.357680  loss = 2.95881 avg_loss = 3.20413\n",
      "epoch no.4 train no.357690  loss = 2.23058 avg_loss = 3.16221\n",
      "epoch no.4 train no.357700  loss = 3.07565 avg_loss = 3.18244\n",
      "epoch no.4 train no.357710  loss = 4.61228 avg_loss = 3.19316\n",
      "epoch no.4 train no.357720  loss = 3.17100 avg_loss = 3.15600\n",
      "epoch no.4 train no.357730  loss = 5.12873 avg_loss = 3.15905\n",
      "epoch no.4 train no.357740  loss = 2.80244 avg_loss = 3.16061\n",
      "epoch no.4 train no.357750  loss = 2.80879 avg_loss = 3.17454\n",
      "epoch no.4 train no.357760  loss = 3.64707 avg_loss = 3.21452\n",
      "epoch no.4 train no.357770  loss = 2.60964 avg_loss = 3.19388\n",
      "epoch no.4 train no.357780  loss = 4.39844 avg_loss = 3.15614\n",
      "epoch no.4 train no.357790  loss = 3.15983 avg_loss = 3.12877\n",
      "epoch no.4 train no.357800  loss = 3.30059 avg_loss = 3.09353\n",
      "epoch no.4 train no.357810  loss = 4.24896 avg_loss = 3.07380\n",
      "epoch no.4 train no.357820  loss = 3.08353 avg_loss = 3.04674\n",
      "epoch no.4 train no.357830  loss = 4.61873 avg_loss = 3.10014\n",
      "epoch no.4 train no.357840  loss = 2.13718 avg_loss = 3.10163\n",
      "epoch no.4 train no.357850  loss = 4.24219 avg_loss = 3.12123\n",
      "epoch no.4 train no.357860  loss = 4.81029 avg_loss = 3.17532\n",
      "epoch no.4 train no.357870  loss = 3.60368 avg_loss = 3.22719\n",
      "epoch no.4 train no.357880  loss = 3.78863 avg_loss = 3.25376\n",
      "epoch no.4 train no.357890  loss = 4.22557 avg_loss = 3.25742\n",
      "epoch no.4 train no.357900  loss = 1.93309 avg_loss = 3.30770\n",
      "epoch no.4 train no.357910  loss = 3.26765 avg_loss = 3.31549\n",
      "epoch no.4 train no.357920  loss = 2.34756 avg_loss = 3.28721\n",
      "epoch no.4 train no.357930  loss = 4.16017 avg_loss = 3.32455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.357940  loss = 2.94399 avg_loss = 3.30543\n",
      "epoch no.4 train no.357950  loss = 3.09895 avg_loss = 3.31758\n",
      "epoch no.4 train no.357960  loss = 3.51921 avg_loss = 3.28402\n",
      "epoch no.4 train no.357970  loss = 2.15789 avg_loss = 3.29904\n",
      "epoch no.4 train no.357980  loss = 3.03074 avg_loss = 3.36913\n",
      "epoch no.4 train no.357990  loss = 2.30018 avg_loss = 3.36961\n",
      "epoch no.4 train no.358000  loss = 3.24886 avg_loss = 3.35249\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '적인', '</s>']\n",
      "여름밤에 듣기 좋은 감성곡</s>\n",
      "epoch no.4 train no.358010  loss = 3.69474 avg_loss = 3.36193\n",
      "epoch no.4 train no.358020  loss = 2.77048 avg_loss = 3.35618\n",
      "epoch no.4 train no.358030  loss = 2.36418 avg_loss = 3.36758\n",
      "epoch no.4 train no.358040  loss = 3.13793 avg_loss = 3.37982\n",
      "epoch no.4 train no.358050  loss = 2.14256 avg_loss = 3.38201\n",
      "epoch no.4 train no.358060  loss = 3.47047 avg_loss = 3.40850\n",
      "epoch no.4 train no.358070  loss = 3.73394 avg_loss = 3.39948\n",
      "epoch no.4 train no.358080  loss = 2.93405 avg_loss = 3.38049\n",
      "epoch no.4 train no.358090  loss = 3.86461 avg_loss = 3.36373\n",
      "epoch no.4 train no.358100  loss = 5.75187 avg_loss = 3.41308\n",
      "epoch no.4 train no.358110  loss = 3.45581 avg_loss = 3.39468\n",
      "epoch no.4 train no.358120  loss = 2.25961 avg_loss = 3.42746\n",
      "epoch no.4 train no.358130  loss = 3.45580 avg_loss = 3.39554\n",
      "epoch no.4 train no.358140  loss = 1.89584 avg_loss = 3.36267\n",
      "epoch no.4 train no.358150  loss = 2.39822 avg_loss = 3.39039\n",
      "epoch no.4 train no.358160  loss = 4.30659 avg_loss = 3.41732\n",
      "epoch no.4 train no.358170  loss = 2.87636 avg_loss = 3.43165\n",
      "epoch no.4 train no.358180  loss = 2.66206 avg_loss = 3.42328\n",
      "epoch no.4 train no.358190  loss = 4.59858 avg_loss = 3.37863\n",
      "epoch no.4 train no.358200  loss = 2.93737 avg_loss = 3.36374\n",
      "epoch no.4 train no.358210  loss = 1.87061 avg_loss = 3.33016\n",
      "epoch no.4 train no.358220  loss = 3.42773 avg_loss = 3.31577\n",
      "epoch no.4 train no.358230  loss = 2.62124 avg_loss = 3.32725\n",
      "epoch no.4 train no.358240  loss = 2.60640 avg_loss = 3.35530\n",
      "epoch no.4 train no.358250  loss = 2.90454 avg_loss = 3.34229\n",
      "epoch no.4 train no.358260  loss = 3.73336 avg_loss = 3.37555\n",
      "epoch no.4 train no.358270  loss = 2.01422 avg_loss = 3.36663\n",
      "epoch no.4 train no.358280  loss = 3.84045 avg_loss = 3.38633\n",
      "epoch no.4 train no.358290  loss = 3.91329 avg_loss = 3.39518\n",
      "epoch no.4 train no.358300  loss = 3.79073 avg_loss = 3.40036\n",
      "epoch no.4 train no.358310  loss = 2.34259 avg_loss = 3.33500\n",
      "epoch no.4 train no.358320  loss = 2.44788 avg_loss = 3.28802\n",
      "epoch no.4 train no.358330  loss = 4.24562 avg_loss = 3.31236\n",
      "epoch no.4 train no.358340  loss = 3.31426 avg_loss = 3.31705\n",
      "epoch no.4 train no.358350  loss = 3.36815 avg_loss = 3.29759\n",
      "epoch no.4 train no.358360  loss = 2.86729 avg_loss = 3.30047\n",
      "epoch no.4 train no.358370  loss = 3.03215 avg_loss = 3.31172\n",
      "epoch no.4 train no.358380  loss = 2.76479 avg_loss = 3.25046\n",
      "epoch no.4 train no.358390  loss = 3.72683 avg_loss = 3.26107\n",
      "epoch no.4 train no.358400  loss = 4.45797 avg_loss = 3.25971\n",
      "epoch no.4 train no.358410  loss = 2.77174 avg_loss = 3.30000\n",
      "epoch no.4 train no.358420  loss = 2.84202 avg_loss = 3.29535\n",
      "epoch no.4 train no.358430  loss = 3.26116 avg_loss = 3.22776\n",
      "epoch no.4 train no.358440  loss = 2.73916 avg_loss = 3.19388\n",
      "epoch no.4 train no.358450  loss = 2.55358 avg_loss = 3.21372\n",
      "epoch no.4 train no.358460  loss = 2.87368 avg_loss = 3.21384\n",
      "epoch no.4 train no.358470  loss = 2.20522 avg_loss = 3.22042\n",
      "epoch no.4 train no.358480  loss = 4.03999 avg_loss = 3.22773\n",
      "epoch no.4 train no.358490  loss = 3.04157 avg_loss = 3.21378\n",
      "epoch no.4 train no.358500  loss = 4.71181 avg_loss = 3.23035\n",
      "epoch no.4 train no.358510  loss = 2.59873 avg_loss = 3.19430\n",
      "epoch no.4 train no.358520  loss = 3.01895 avg_loss = 3.24807\n",
      "epoch no.4 train no.358530  loss = 2.68046 avg_loss = 3.27227\n",
      "epoch no.4 train no.358540  loss = 4.12058 avg_loss = 3.30087\n",
      "epoch no.4 train no.358550  loss = 3.87197 avg_loss = 3.31275\n",
      "epoch no.4 train no.358560  loss = 3.12431 avg_loss = 3.33961\n",
      "epoch no.4 train no.358570  loss = 3.35500 avg_loss = 3.34143\n",
      "epoch no.4 train no.358580  loss = 2.47432 avg_loss = 3.31778\n",
      "epoch no.4 train no.358590  loss = 2.96818 avg_loss = 3.33561\n",
      "epoch no.4 train no.358600  loss = 4.46613 avg_loss = 3.39072\n",
      "epoch no.4 train no.358610  loss = 3.85855 avg_loss = 3.37368\n",
      "epoch no.4 train no.358620  loss = 3.41197 avg_loss = 3.38032\n",
      "epoch no.4 train no.358630  loss = 2.68179 avg_loss = 3.38967\n",
      "epoch no.4 train no.358640  loss = 4.04108 avg_loss = 3.39169\n",
      "epoch no.4 train no.358650  loss = 4.29012 avg_loss = 3.39718\n",
      "epoch no.4 train no.358660  loss = 2.45548 avg_loss = 3.42477\n",
      "epoch no.4 train no.358670  loss = 2.19194 avg_loss = 3.39058\n",
      "epoch no.4 train no.358680  loss = 3.20499 avg_loss = 3.36273\n",
      "epoch no.4 train no.358690  loss = 4.10867 avg_loss = 3.31710\n",
      "epoch no.4 train no.358700  loss = 3.90926 avg_loss = 3.31817\n",
      "epoch no.4 train no.358710  loss = 2.86266 avg_loss = 3.27506\n",
      "epoch no.4 train no.358720  loss = 2.42525 avg_loss = 3.27676\n",
      "epoch no.4 train no.358730  loss = 3.68675 avg_loss = 3.30419\n",
      "epoch no.4 train no.358740  loss = 3.35540 avg_loss = 3.29249\n",
      "epoch no.4 train no.358750  loss = 3.19744 avg_loss = 3.30002\n",
      "epoch no.4 train no.358760  loss = 2.71092 avg_loss = 3.31859\n",
      "epoch no.4 train no.358770  loss = 3.92062 avg_loss = 3.27519\n",
      "epoch no.4 train no.358780  loss = 2.45314 avg_loss = 3.27725\n",
      "epoch no.4 train no.358790  loss = 2.64054 avg_loss = 3.28642\n",
      "epoch no.4 train no.358800  loss = 3.89904 avg_loss = 3.28781\n",
      "epoch no.4 train no.358810  loss = 2.23368 avg_loss = 3.32932\n",
      "epoch no.4 train no.358820  loss = 2.59969 avg_loss = 3.32383\n",
      "epoch no.4 train no.358830  loss = 2.82620 avg_loss = 3.29271\n",
      "epoch no.4 train no.358840  loss = 2.26972 avg_loss = 3.29598\n",
      "epoch no.4 train no.358850  loss = 4.06922 avg_loss = 3.30394\n",
      "epoch no.4 train no.358860  loss = 3.04104 avg_loss = 3.30856\n",
      "epoch no.4 train no.358870  loss = 2.47095 avg_loss = 3.31369\n",
      "epoch no.4 train no.358880  loss = 2.91735 avg_loss = 3.29473\n",
      "epoch no.4 train no.358890  loss = 4.51623 avg_loss = 3.33049\n",
      "epoch no.4 train no.358900  loss = 2.10096 avg_loss = 3.29423\n",
      "epoch no.4 train no.358910  loss = 2.75353 avg_loss = 3.25058\n",
      "epoch no.4 train no.358920  loss = 2.04770 avg_loss = 3.18398\n",
      "epoch no.4 train no.358930  loss = 4.78983 avg_loss = 3.23981\n",
      "epoch no.4 train no.358940  loss = 1.64892 avg_loss = 3.23876\n",
      "epoch no.4 train no.358950  loss = 1.25563 avg_loss = 3.21277\n",
      "epoch no.4 train no.358960  loss = 2.71430 avg_loss = 3.22155\n",
      "epoch no.4 train no.358970  loss = 2.46843 avg_loss = 3.20420\n",
      "epoch no.4 train no.358980  loss = 2.67199 avg_loss = 3.18616\n",
      "epoch no.4 train no.358990  loss = 1.86817 avg_loss = 3.17492\n",
      "epoch no.4 train no.359000  loss = 2.72088 avg_loss = 3.19282\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '을', '▁수', '놓을', '▁감성', '</s>']\n",
      "여름밤을 수놓을 재즈</s>\n",
      "epoch no.4 train no.359010  loss = 2.57183 avg_loss = 3.17278\n",
      "epoch no.4 train no.359020  loss = 1.82635 avg_loss = 3.14886\n",
      "epoch no.4 train no.359030  loss = 2.92755 avg_loss = 3.15882\n",
      "epoch no.4 train no.359040  loss = 4.49047 avg_loss = 3.18512\n",
      "epoch no.4 train no.359050  loss = 2.43073 avg_loss = 3.20062\n",
      "epoch no.4 train no.359060  loss = 4.04459 avg_loss = 3.22472\n",
      "epoch no.4 train no.359070  loss = 4.33621 avg_loss = 3.27227\n",
      "epoch no.4 train no.359080  loss = 4.40363 avg_loss = 3.31503\n",
      "epoch no.4 train no.359090  loss = 2.58387 avg_loss = 3.31697\n",
      "epoch no.4 train no.359100  loss = 5.01958 avg_loss = 3.32540\n",
      "epoch no.4 train no.359110  loss = 3.81570 avg_loss = 3.31312\n",
      "epoch no.4 train no.359120  loss = 3.54111 avg_loss = 3.30457\n",
      "epoch no.4 train no.359130  loss = 3.47555 avg_loss = 3.30072\n",
      "epoch no.4 train no.359140  loss = 1.87363 avg_loss = 3.28762\n",
      "epoch no.4 train no.359150  loss = 2.82446 avg_loss = 3.27408\n",
      "epoch no.4 train no.359160  loss = 2.34520 avg_loss = 3.28559\n",
      "epoch no.4 train no.359170  loss = 2.72618 avg_loss = 3.26183\n",
      "epoch no.4 train no.359180  loss = 2.66894 avg_loss = 3.21174\n",
      "epoch no.4 train no.359190  loss = 3.46768 avg_loss = 3.20881\n",
      "epoch no.4 train no.359200  loss = 4.11249 avg_loss = 3.25618\n",
      "epoch no.4 train no.359210  loss = 3.65052 avg_loss = 3.27005\n",
      "epoch no.4 train no.359220  loss = 2.66253 avg_loss = 3.22102\n",
      "epoch no.4 train no.359230  loss = 2.71135 avg_loss = 3.24710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.359240  loss = 5.76315 avg_loss = 3.27667\n",
      "epoch no.4 train no.359250  loss = 4.68853 avg_loss = 3.25279\n",
      "epoch no.4 train no.359260  loss = 3.03373 avg_loss = 3.24691\n",
      "epoch no.4 train no.359270  loss = 1.67798 avg_loss = 3.24391\n",
      "epoch no.4 train no.359280  loss = 2.26229 avg_loss = 3.21122\n",
      "epoch no.4 train no.359290  loss = 2.48627 avg_loss = 3.19149\n",
      "epoch no.4 train no.359300  loss = 3.67186 avg_loss = 3.17358\n",
      "epoch no.4 train no.359310  loss = 2.77735 avg_loss = 3.13527\n",
      "epoch no.4 train no.359320  loss = 3.71657 avg_loss = 3.16096\n",
      "epoch no.4 train no.359330  loss = 3.05609 avg_loss = 3.13724\n",
      "epoch no.4 train no.359340  loss = 4.50973 avg_loss = 3.16070\n",
      "epoch no.4 train no.359350  loss = 4.26271 avg_loss = 3.13074\n",
      "epoch no.4 train no.359360  loss = 5.47709 avg_loss = 3.19221\n",
      "epoch no.4 train no.359370  loss = 3.44227 avg_loss = 3.23341\n",
      "epoch no.4 train no.359380  loss = 2.46609 avg_loss = 3.21636\n",
      "epoch no.4 train no.359390  loss = 2.11758 avg_loss = 3.18270\n",
      "epoch no.4 train no.359400  loss = 4.25162 avg_loss = 3.18513\n",
      "epoch no.4 train no.359410  loss = 3.68033 avg_loss = 3.24537\n",
      "epoch no.4 train no.359420  loss = 3.67755 avg_loss = 3.24413\n",
      "epoch no.5 train no.359430  loss = 4.02736 avg_loss = 3.25965\n",
      "epoch no.5 train no.359440  loss = 3.43415 avg_loss = 3.23449\n",
      "epoch no.5 train no.359450  loss = 1.83034 avg_loss = 3.18840\n",
      "epoch no.5 train no.359460  loss = 2.42357 avg_loss = 3.14029\n",
      "epoch no.5 train no.359470  loss = 3.36654 avg_loss = 3.14294\n",
      "epoch no.5 train no.359480  loss = 2.73714 avg_loss = 3.10809\n",
      "epoch no.5 train no.359490  loss = 3.68090 avg_loss = 3.13481\n",
      "epoch no.5 train no.359500  loss = 4.78074 avg_loss = 3.13591\n",
      "epoch no.5 train no.359510  loss = 3.47284 avg_loss = 3.07287\n",
      "epoch no.5 train no.359520  loss = 3.36285 avg_loss = 3.09057\n",
      "epoch no.5 train no.359530  loss = 3.79806 avg_loss = 3.09857\n",
      "epoch no.5 train no.359540  loss = 4.48583 avg_loss = 3.10453\n",
      "epoch no.5 train no.359550  loss = 3.22991 avg_loss = 3.10263\n",
      "epoch no.5 train no.359560  loss = 2.59302 avg_loss = 3.08270\n",
      "epoch no.5 train no.359570  loss = 3.18897 avg_loss = 3.07547\n",
      "epoch no.5 train no.359580  loss = 2.15475 avg_loss = 3.06002\n",
      "epoch no.5 train no.359590  loss = 2.70388 avg_loss = 3.03411\n",
      "epoch no.5 train no.359600  loss = 3.94161 avg_loss = 3.02968\n",
      "epoch no.5 train no.359610  loss = 2.23892 avg_loss = 3.02231\n",
      "epoch no.5 train no.359620  loss = 2.92352 avg_loss = 2.97928\n",
      "epoch no.5 train no.359630  loss = 4.19362 avg_loss = 2.96123\n",
      "epoch no.5 train no.359640  loss = 3.94756 avg_loss = 2.95986\n",
      "epoch no.5 train no.359650  loss = 2.68581 avg_loss = 2.92157\n",
      "epoch no.5 train no.359660  loss = 3.00979 avg_loss = 2.90817\n",
      "epoch no.5 train no.359670  loss = 2.91940 avg_loss = 2.87492\n",
      "epoch no.5 train no.359680  loss = 3.18108 avg_loss = 2.88101\n",
      "epoch no.5 train no.359690  loss = 4.10737 avg_loss = 2.90476\n",
      "epoch no.5 train no.359700  loss = 3.31750 avg_loss = 2.94340\n",
      "epoch no.5 train no.359710  loss = 3.75138 avg_loss = 2.94661\n",
      "epoch no.5 train no.359720  loss = 2.18502 avg_loss = 2.97222\n",
      "epoch no.5 train no.359730  loss = 2.21499 avg_loss = 2.93415\n",
      "epoch no.5 train no.359740  loss = 1.97962 avg_loss = 2.89960\n",
      "epoch no.5 train no.359750  loss = 2.52447 avg_loss = 2.92980\n",
      "epoch no.5 train no.359760  loss = 3.25319 avg_loss = 2.93855\n",
      "epoch no.5 train no.359770  loss = 1.81235 avg_loss = 2.91880\n",
      "epoch no.5 train no.359780  loss = 3.49093 avg_loss = 2.92800\n",
      "epoch no.5 train no.359790  loss = 3.30206 avg_loss = 2.93543\n",
      "epoch no.5 train no.359800  loss = 1.86287 avg_loss = 2.91200\n",
      "epoch no.5 train no.359810  loss = 3.11315 avg_loss = 2.91417\n",
      "epoch no.5 train no.359820  loss = 3.70732 avg_loss = 2.94519\n",
      "epoch no.5 train no.359830  loss = 4.65686 avg_loss = 2.98106\n",
      "epoch no.5 train no.359840  loss = 2.17621 avg_loss = 2.98192\n",
      "epoch no.5 train no.359850  loss = 4.14786 avg_loss = 2.99477\n",
      "epoch no.5 train no.359860  loss = 2.87530 avg_loss = 2.98884\n",
      "epoch no.5 train no.359870  loss = 2.72310 avg_loss = 2.98921\n",
      "epoch no.5 train no.359880  loss = 3.21276 avg_loss = 2.97487\n",
      "epoch no.5 train no.359890  loss = 4.02551 avg_loss = 2.98403\n",
      "epoch no.5 train no.359900  loss = 3.51891 avg_loss = 3.01570\n",
      "epoch no.5 train no.359910  loss = 3.26249 avg_loss = 2.98909\n",
      "epoch no.5 train no.359920  loss = 4.41865 avg_loss = 3.00238\n",
      "epoch no.5 train no.359930  loss = 3.75225 avg_loss = 2.99780\n",
      "epoch no.5 train no.359940  loss = 4.10276 avg_loss = 2.99993\n",
      "epoch no.5 train no.359950  loss = 2.97970 avg_loss = 2.98943\n",
      "epoch no.5 train no.359960  loss = 2.49648 avg_loss = 3.01071\n",
      "epoch no.5 train no.359970  loss = 2.57887 avg_loss = 3.05073\n",
      "epoch no.5 train no.359980  loss = 2.36622 avg_loss = 3.01150\n",
      "epoch no.5 train no.359990  loss = 2.74948 avg_loss = 2.95413\n",
      "epoch no.5 train no.360000  loss = 2.41543 avg_loss = 2.95864\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '을', '▁듣기', '▁잔잔', 'op', '</s>']\n",
      "여름밤에 듣는 pop</s>\n",
      "epoch no.5 train no.360010  loss = 3.18441 avg_loss = 2.96048\n",
      "epoch no.5 train no.360020  loss = 2.61583 avg_loss = 2.97537\n",
      "epoch no.5 train no.360030  loss = 2.14233 avg_loss = 2.94312\n",
      "epoch no.5 train no.360040  loss = 2.71172 avg_loss = 2.92427\n",
      "epoch no.5 train no.360050  loss = 2.49078 avg_loss = 2.88777\n",
      "epoch no.5 train no.360060  loss = 3.61400 avg_loss = 2.89377\n",
      "epoch no.5 train no.360070  loss = 2.46086 avg_loss = 2.95355\n",
      "epoch no.5 train no.360080  loss = 4.13660 avg_loss = 2.96534\n",
      "epoch no.5 train no.360090  loss = 3.44820 avg_loss = 2.99068\n",
      "epoch no.5 train no.360100  loss = 3.46879 avg_loss = 2.97145\n",
      "epoch no.5 train no.360110  loss = 3.53981 avg_loss = 2.98054\n",
      "epoch no.5 train no.360120  loss = 1.69036 avg_loss = 2.96021\n",
      "epoch no.5 train no.360130  loss = 2.12330 avg_loss = 2.95628\n",
      "epoch no.5 train no.360140  loss = 2.76028 avg_loss = 2.95107\n",
      "epoch no.5 train no.360150  loss = 2.46118 avg_loss = 2.94319\n",
      "epoch no.5 train no.360160  loss = 2.72837 avg_loss = 2.91124\n",
      "epoch no.5 train no.360170  loss = 3.47707 avg_loss = 2.93096\n",
      "epoch no.5 train no.360180  loss = 1.73678 avg_loss = 2.92795\n",
      "epoch no.5 train no.360190  loss = 3.14478 avg_loss = 2.90454\n",
      "epoch no.5 train no.360200  loss = 5.04781 avg_loss = 2.95219\n",
      "epoch no.5 train no.360210  loss = 3.81238 avg_loss = 2.96275\n",
      "epoch no.5 train no.360220  loss = 2.71036 avg_loss = 2.94260\n",
      "epoch no.5 train no.360230  loss = 2.23965 avg_loss = 2.93549\n",
      "epoch no.5 train no.360240  loss = 1.95454 avg_loss = 2.87968\n",
      "epoch no.5 train no.360250  loss = 2.74486 avg_loss = 2.89091\n",
      "epoch no.5 train no.360260  loss = 3.12454 avg_loss = 2.88985\n",
      "epoch no.5 train no.360270  loss = 2.97312 avg_loss = 2.87705\n",
      "epoch no.5 train no.360280  loss = 2.83688 avg_loss = 2.89890\n",
      "epoch no.5 train no.360290  loss = 3.15563 avg_loss = 2.92250\n",
      "epoch no.5 train no.360300  loss = 3.77840 avg_loss = 2.89768\n",
      "epoch no.5 train no.360310  loss = 4.96425 avg_loss = 2.92000\n",
      "epoch no.5 train no.360320  loss = 3.38059 avg_loss = 2.90137\n",
      "epoch no.5 train no.360330  loss = 2.46362 avg_loss = 2.89318\n",
      "epoch no.5 train no.360340  loss = 4.28530 avg_loss = 2.88536\n",
      "epoch no.5 train no.360350  loss = 4.86469 avg_loss = 2.88601\n",
      "epoch no.5 train no.360360  loss = 2.49073 avg_loss = 2.86458\n",
      "epoch no.5 train no.360370  loss = 2.10490 avg_loss = 2.84999\n",
      "epoch no.5 train no.360380  loss = 2.49668 avg_loss = 2.87005\n",
      "epoch no.5 train no.360390  loss = 3.25569 avg_loss = 2.88124\n",
      "epoch no.5 train no.360400  loss = 1.72291 avg_loss = 2.88363\n",
      "epoch no.5 train no.360410  loss = 2.08657 avg_loss = 2.93380\n",
      "epoch no.5 train no.360420  loss = 2.33577 avg_loss = 2.95439\n",
      "epoch no.5 train no.360430  loss = 2.96251 avg_loss = 2.98906\n",
      "epoch no.5 train no.360440  loss = 2.81447 avg_loss = 2.94896\n",
      "epoch no.5 train no.360450  loss = 3.47333 avg_loss = 2.96851\n",
      "epoch no.5 train no.360460  loss = 1.22945 avg_loss = 2.93958\n",
      "epoch no.5 train no.360470  loss = 2.53817 avg_loss = 2.95884\n",
      "epoch no.5 train no.360480  loss = 2.07745 avg_loss = 2.92472\n",
      "epoch no.5 train no.360490  loss = 2.58592 avg_loss = 2.92153\n",
      "epoch no.5 train no.360500  loss = 2.63994 avg_loss = 2.93701\n",
      "epoch no.5 train no.360510  loss = 2.95986 avg_loss = 2.94910\n",
      "epoch no.5 train no.360520  loss = 2.82413 avg_loss = 2.93241\n",
      "epoch no.5 train no.360530  loss = 3.22049 avg_loss = 2.92000\n",
      "epoch no.5 train no.360540  loss = 2.33228 avg_loss = 2.90310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.360550  loss = 2.39573 avg_loss = 2.91967\n",
      "epoch no.5 train no.360560  loss = 5.15855 avg_loss = 2.91036\n",
      "epoch no.5 train no.360570  loss = 3.06835 avg_loss = 2.91651\n",
      "epoch no.5 train no.360580  loss = 3.46688 avg_loss = 2.88187\n",
      "epoch no.5 train no.360590  loss = 5.05770 avg_loss = 2.90568\n",
      "epoch no.5 train no.360600  loss = 2.67905 avg_loss = 2.92944\n",
      "epoch no.5 train no.360610  loss = 2.78794 avg_loss = 2.91284\n",
      "epoch no.5 train no.360620  loss = 2.17739 avg_loss = 2.91145\n",
      "epoch no.5 train no.360630  loss = 3.99194 avg_loss = 2.93247\n",
      "epoch no.5 train no.360640  loss = 2.22972 avg_loss = 2.94315\n",
      "epoch no.5 train no.360650  loss = 2.11191 avg_loss = 2.91594\n",
      "epoch no.5 train no.360660  loss = 2.26420 avg_loss = 2.89826\n",
      "epoch no.5 train no.360670  loss = 2.04769 avg_loss = 2.87924\n",
      "epoch no.5 train no.360680  loss = 3.11984 avg_loss = 2.88286\n",
      "epoch no.5 train no.360690  loss = 2.64399 avg_loss = 2.87474\n",
      "epoch no.5 train no.360700  loss = 3.19834 avg_loss = 2.90905\n",
      "epoch no.5 train no.360710  loss = 3.45928 avg_loss = 2.92955\n",
      "epoch no.5 train no.360720  loss = 3.04363 avg_loss = 2.93127\n",
      "epoch no.5 train no.360730  loss = 3.39401 avg_loss = 2.91352\n",
      "epoch no.5 train no.360740  loss = 2.59779 avg_loss = 2.89850\n",
      "epoch no.5 train no.360750  loss = 4.96523 avg_loss = 2.92120\n",
      "epoch no.5 train no.360760  loss = 1.55877 avg_loss = 2.92892\n",
      "epoch no.5 train no.360770  loss = 3.76228 avg_loss = 2.91903\n",
      "epoch no.5 train no.360780  loss = 2.95609 avg_loss = 2.91919\n",
      "epoch no.5 train no.360790  loss = 3.51613 avg_loss = 2.88666\n",
      "epoch no.5 train no.360800  loss = 2.73757 avg_loss = 2.94160\n",
      "epoch no.5 train no.360810  loss = 2.19148 avg_loss = 2.93644\n",
      "epoch no.5 train no.360820  loss = 2.83342 avg_loss = 2.90729\n",
      "epoch no.5 train no.360830  loss = 3.21206 avg_loss = 2.93297\n",
      "epoch no.5 train no.360840  loss = 2.61445 avg_loss = 2.94192\n",
      "epoch no.5 train no.360850  loss = 2.96332 avg_loss = 2.92936\n",
      "epoch no.5 train no.360860  loss = 2.32060 avg_loss = 2.90340\n",
      "epoch no.5 train no.360870  loss = 2.06479 avg_loss = 2.90006\n",
      "epoch no.5 train no.360880  loss = 3.61456 avg_loss = 2.90575\n",
      "epoch no.5 train no.360890  loss = 3.20431 avg_loss = 2.91455\n",
      "epoch no.5 train no.360900  loss = 2.12041 avg_loss = 2.96291\n",
      "epoch no.5 train no.360910  loss = 2.46241 avg_loss = 2.92472\n",
      "epoch no.5 train no.360920  loss = 3.19932 avg_loss = 2.93506\n",
      "epoch no.5 train no.360930  loss = 2.40394 avg_loss = 2.91394\n",
      "epoch no.5 train no.360940  loss = 3.34884 avg_loss = 2.94412\n",
      "epoch no.5 train no.360950  loss = 3.64161 avg_loss = 2.92972\n",
      "epoch no.5 train no.360960  loss = 2.99377 avg_loss = 2.92030\n",
      "epoch no.5 train no.360970  loss = 2.96010 avg_loss = 2.94147\n",
      "epoch no.5 train no.360980  loss = 3.81282 avg_loss = 2.93684\n",
      "epoch no.5 train no.360990  loss = 2.65577 avg_loss = 2.88333\n",
      "epoch no.5 train no.361000  loss = 1.86035 avg_loss = 2.86442\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '음악', '</s>']\n",
      "여름밤에 듣기 좋은 인디음악</s>\n",
      "epoch no.5 train no.361010  loss = 4.05338 avg_loss = 2.87531\n",
      "epoch no.5 train no.361020  loss = 1.65672 avg_loss = 2.88849\n",
      "epoch no.5 train no.361030  loss = 2.69549 avg_loss = 2.86375\n",
      "epoch no.5 train no.361040  loss = 2.56090 avg_loss = 2.83952\n",
      "epoch no.5 train no.361050  loss = 1.83494 avg_loss = 2.82653\n",
      "epoch no.5 train no.361060  loss = 2.61729 avg_loss = 2.84447\n",
      "epoch no.5 train no.361070  loss = 3.17290 avg_loss = 2.86389\n",
      "epoch no.5 train no.361080  loss = 2.79540 avg_loss = 2.84761\n",
      "epoch no.5 train no.361090  loss = 2.63849 avg_loss = 2.84301\n",
      "epoch no.5 train no.361100  loss = 2.76898 avg_loss = 2.83797\n",
      "epoch no.5 train no.361110  loss = 2.29297 avg_loss = 2.82446\n",
      "epoch no.5 train no.361120  loss = 2.40464 avg_loss = 2.82373\n",
      "epoch no.5 train no.361130  loss = 2.89504 avg_loss = 2.82595\n",
      "epoch no.5 train no.361140  loss = 3.88757 avg_loss = 2.81986\n",
      "epoch no.5 train no.361150  loss = 3.38686 avg_loss = 2.80751\n",
      "epoch no.5 train no.361160  loss = 2.73780 avg_loss = 2.78289\n",
      "epoch no.5 train no.361170  loss = 2.35739 avg_loss = 2.81249\n",
      "epoch no.5 train no.361180  loss = 4.36066 avg_loss = 2.83148\n",
      "epoch no.5 train no.361190  loss = 3.28922 avg_loss = 2.80674\n",
      "epoch no.5 train no.361200  loss = 2.34030 avg_loss = 2.80366\n",
      "epoch no.5 train no.361210  loss = 1.96393 avg_loss = 2.77193\n",
      "epoch no.5 train no.361220  loss = 2.47265 avg_loss = 2.80017\n",
      "epoch no.5 train no.361230  loss = 2.54060 avg_loss = 2.80214\n",
      "epoch no.5 train no.361240  loss = 2.41920 avg_loss = 2.79445\n",
      "epoch no.5 train no.361250  loss = 2.43343 avg_loss = 2.79733\n",
      "epoch no.5 train no.361260  loss = 2.65175 avg_loss = 2.79732\n",
      "epoch no.5 train no.361270  loss = 2.05960 avg_loss = 2.78238\n",
      "epoch no.5 train no.361280  loss = 2.46770 avg_loss = 2.78295\n",
      "epoch no.5 train no.361290  loss = 3.02476 avg_loss = 2.82175\n",
      "epoch no.5 train no.361300  loss = 2.25788 avg_loss = 2.84973\n",
      "epoch no.5 train no.361310  loss = 2.29651 avg_loss = 2.84904\n",
      "epoch no.5 train no.361320  loss = 4.22364 avg_loss = 2.84953\n",
      "epoch no.5 train no.361330  loss = 2.25709 avg_loss = 2.90115\n",
      "epoch no.5 train no.361340  loss = 3.01244 avg_loss = 2.91957\n",
      "epoch no.5 train no.361350  loss = 2.31986 avg_loss = 2.91689\n",
      "epoch no.5 train no.361360  loss = 2.04579 avg_loss = 2.87487\n",
      "epoch no.5 train no.361370  loss = 1.75002 avg_loss = 2.85095\n",
      "epoch no.5 train no.361380  loss = 2.88888 avg_loss = 2.84140\n",
      "epoch no.5 train no.361390  loss = 1.77735 avg_loss = 2.82993\n",
      "epoch no.5 train no.361400  loss = 3.16114 avg_loss = 2.83595\n",
      "epoch no.5 train no.361410  loss = 2.80051 avg_loss = 2.84217\n",
      "epoch no.5 train no.361420  loss = 1.89058 avg_loss = 2.84381\n",
      "epoch no.5 train no.361430  loss = 3.22912 avg_loss = 2.86164\n",
      "epoch no.5 train no.361440  loss = 2.30194 avg_loss = 2.88771\n",
      "epoch no.5 train no.361450  loss = 2.24104 avg_loss = 2.91880\n",
      "epoch no.5 train no.361460  loss = 3.73622 avg_loss = 2.92279\n",
      "epoch no.5 train no.361470  loss = 2.62897 avg_loss = 2.90221\n",
      "epoch no.5 train no.361480  loss = 2.56561 avg_loss = 2.94078\n",
      "epoch no.5 train no.361490  loss = 2.58910 avg_loss = 2.89196\n",
      "epoch no.5 train no.361500  loss = 2.98500 avg_loss = 2.86819\n",
      "epoch no.5 train no.361510  loss = 2.58217 avg_loss = 2.83295\n",
      "epoch no.5 train no.361520  loss = 2.39525 avg_loss = 2.80379\n",
      "epoch no.5 train no.361530  loss = 3.79356 avg_loss = 2.81666\n",
      "epoch no.5 train no.361540  loss = 2.94319 avg_loss = 2.84572\n",
      "epoch no.5 train no.361550  loss = 3.74606 avg_loss = 2.83848\n",
      "epoch no.5 train no.361560  loss = 2.36456 avg_loss = 2.84133\n",
      "epoch no.5 train no.361570  loss = 2.33997 avg_loss = 2.82115\n",
      "epoch no.5 train no.361580  loss = 2.25907 avg_loss = 2.80701\n",
      "epoch no.5 train no.361590  loss = 2.97675 avg_loss = 2.79742\n",
      "epoch no.5 train no.361600  loss = 2.77165 avg_loss = 2.84229\n",
      "epoch no.5 train no.361610  loss = 4.46636 avg_loss = 2.86357\n",
      "epoch no.5 train no.361620  loss = 3.92917 avg_loss = 2.87836\n",
      "epoch no.5 train no.361630  loss = 3.59003 avg_loss = 2.88916\n",
      "epoch no.5 train no.361640  loss = 2.85418 avg_loss = 2.89421\n",
      "epoch no.5 train no.361650  loss = 3.59525 avg_loss = 2.86562\n",
      "epoch no.5 train no.361660  loss = 2.72913 avg_loss = 2.81807\n",
      "epoch no.5 train no.361670  loss = 3.05719 avg_loss = 2.81836\n",
      "epoch no.5 train no.361680  loss = 1.64348 avg_loss = 2.77633\n",
      "epoch no.5 train no.361690  loss = 2.95706 avg_loss = 2.77984\n",
      "epoch no.5 train no.361700  loss = 2.15654 avg_loss = 2.75944\n",
      "epoch no.5 train no.361710  loss = 2.36792 avg_loss = 2.76586\n",
      "epoch no.5 train no.361720  loss = 3.22432 avg_loss = 2.77589\n",
      "epoch no.5 train no.361730  loss = 1.65793 avg_loss = 2.73760\n",
      "epoch no.5 train no.361740  loss = 1.79661 avg_loss = 2.70436\n",
      "epoch no.5 train no.361750  loss = 3.33338 avg_loss = 2.72083\n",
      "epoch no.5 train no.361760  loss = 2.35494 avg_loss = 2.77208\n",
      "epoch no.5 train no.361770  loss = 2.06129 avg_loss = 2.79353\n",
      "epoch no.5 train no.361780  loss = 2.12067 avg_loss = 2.79367\n",
      "epoch no.5 train no.361790  loss = 3.72455 avg_loss = 2.82602\n",
      "epoch no.5 train no.361800  loss = 1.42024 avg_loss = 2.80912\n",
      "epoch no.5 train no.361810  loss = 3.03982 avg_loss = 2.80078\n",
      "epoch no.5 train no.361820  loss = 3.52497 avg_loss = 2.83888\n",
      "epoch no.5 train no.361830  loss = 2.04178 avg_loss = 2.82151\n",
      "epoch no.5 train no.361840  loss = 3.52085 avg_loss = 2.85489\n",
      "epoch no.5 train no.361850  loss = 2.99566 avg_loss = 2.84107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.361860  loss = 2.48159 avg_loss = 2.86565\n",
      "epoch no.5 train no.361870  loss = 2.93829 avg_loss = 2.86753\n",
      "epoch no.5 train no.361880  loss = 2.88536 avg_loss = 2.83059\n",
      "epoch no.5 train no.361890  loss = 2.35516 avg_loss = 2.85629\n",
      "epoch no.5 train no.361900  loss = 3.27795 avg_loss = 2.87617\n",
      "epoch no.5 train no.361910  loss = 3.78027 avg_loss = 2.88585\n",
      "epoch no.5 train no.361920  loss = 4.03830 avg_loss = 2.89789\n",
      "epoch no.5 train no.361930  loss = 2.87347 avg_loss = 2.88014\n",
      "epoch no.5 train no.361940  loss = 2.18137 avg_loss = 2.88875\n",
      "epoch no.5 train no.361950  loss = 3.03324 avg_loss = 2.86539\n",
      "epoch no.5 train no.361960  loss = 2.36202 avg_loss = 2.83015\n",
      "epoch no.5 train no.361970  loss = 2.49989 avg_loss = 2.82127\n",
      "epoch no.5 train no.361980  loss = 2.39346 avg_loss = 2.83558\n",
      "epoch no.5 train no.361990  loss = 1.79666 avg_loss = 2.81230\n",
      "epoch no.5 train no.362000  loss = 3.21727 avg_loss = 2.83941\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.5 train no.362010  loss = 1.09634 avg_loss = 2.82335\n",
      "epoch no.5 train no.362020  loss = 3.40688 avg_loss = 2.79339\n",
      "epoch no.5 train no.362030  loss = 3.11702 avg_loss = 2.83896\n",
      "epoch no.5 train no.362040  loss = 2.72456 avg_loss = 2.85217\n",
      "epoch no.5 train no.362050  loss = 3.13759 avg_loss = 2.84114\n",
      "epoch no.5 train no.362060  loss = 2.77340 avg_loss = 2.90623\n",
      "epoch no.5 train no.362070  loss = 2.48808 avg_loss = 2.91595\n",
      "epoch no.5 train no.362080  loss = 3.13513 avg_loss = 2.93436\n",
      "epoch no.5 train no.362090  loss = 1.91984 avg_loss = 2.90804\n",
      "epoch no.5 train no.362100  loss = 3.02927 avg_loss = 2.91566\n",
      "epoch no.5 train no.362110  loss = 1.92878 avg_loss = 2.89559\n",
      "epoch no.5 train no.362120  loss = 3.66525 avg_loss = 2.91144\n",
      "epoch no.5 train no.362130  loss = 3.24487 avg_loss = 2.91907\n",
      "epoch no.5 train no.362140  loss = 2.54405 avg_loss = 2.92638\n",
      "epoch no.5 train no.362150  loss = 2.62893 avg_loss = 2.93885\n",
      "epoch no.5 train no.362160  loss = 4.38379 avg_loss = 2.93088\n",
      "epoch no.5 train no.362170  loss = 2.33566 avg_loss = 2.91478\n",
      "epoch no.5 train no.362180  loss = 2.60095 avg_loss = 2.90081\n",
      "epoch no.5 train no.362190  loss = 3.10264 avg_loss = 2.91097\n",
      "epoch no.5 train no.362200  loss = 4.19100 avg_loss = 2.91025\n",
      "epoch no.5 train no.362210  loss = 2.23311 avg_loss = 2.87108\n",
      "epoch no.5 train no.362220  loss = 2.06104 avg_loss = 2.84656\n",
      "epoch no.5 train no.362230  loss = 3.12571 avg_loss = 2.83582\n",
      "epoch no.5 train no.362240  loss = 2.01207 avg_loss = 2.79160\n",
      "epoch no.5 train no.362250  loss = 3.13583 avg_loss = 2.83363\n",
      "epoch no.5 train no.362260  loss = 1.88474 avg_loss = 2.85024\n",
      "epoch no.5 train no.362270  loss = 3.79046 avg_loss = 2.85664\n",
      "epoch no.5 train no.362280  loss = 4.63740 avg_loss = 2.90035\n",
      "epoch no.5 train no.362290  loss = 3.02979 avg_loss = 2.88254\n",
      "epoch no.5 train no.362300  loss = 3.12806 avg_loss = 2.89087\n",
      "epoch no.5 train no.362310  loss = 1.87038 avg_loss = 2.89889\n",
      "epoch no.5 train no.362320  loss = 2.52616 avg_loss = 2.90082\n",
      "epoch no.5 train no.362330  loss = 4.12656 avg_loss = 2.88290\n",
      "epoch no.5 train no.362340  loss = 2.51790 avg_loss = 2.89293\n",
      "epoch no.5 train no.362350  loss = 2.57374 avg_loss = 2.89795\n",
      "epoch no.5 train no.362360  loss = 3.05538 avg_loss = 2.86966\n",
      "epoch no.5 train no.362370  loss = 2.53723 avg_loss = 2.84206\n",
      "epoch no.5 train no.362380  loss = 6.26110 avg_loss = 2.87654\n",
      "epoch no.5 train no.362390  loss = 2.41545 avg_loss = 2.87656\n",
      "epoch no.5 train no.362400  loss = 4.03711 avg_loss = 2.90160\n",
      "epoch no.5 train no.362410  loss = 1.73479 avg_loss = 2.85886\n",
      "epoch no.5 train no.362420  loss = 2.58579 avg_loss = 2.86474\n",
      "epoch no.5 train no.362430  loss = 3.83530 avg_loss = 2.87957\n",
      "epoch no.5 train no.362440  loss = 3.18339 avg_loss = 2.88735\n",
      "epoch no.5 train no.362450  loss = 3.25593 avg_loss = 2.87341\n",
      "epoch no.5 train no.362460  loss = 4.13009 avg_loss = 2.93100\n",
      "epoch no.5 train no.362470  loss = 3.49408 avg_loss = 2.94028\n",
      "epoch no.5 train no.362480  loss = 2.31688 avg_loss = 2.93454\n",
      "epoch no.5 train no.362490  loss = 2.51784 avg_loss = 2.90534\n",
      "epoch no.5 train no.362500  loss = 3.62524 avg_loss = 2.90045\n",
      "epoch no.5 train no.362510  loss = 2.59631 avg_loss = 2.88004\n",
      "epoch no.5 train no.362520  loss = 3.43266 avg_loss = 2.91301\n",
      "epoch no.5 train no.362530  loss = 3.57591 avg_loss = 2.92393\n",
      "epoch no.5 train no.362540  loss = 4.23669 avg_loss = 2.91325\n",
      "epoch no.5 train no.362550  loss = 3.48271 avg_loss = 2.88496\n",
      "epoch no.5 train no.362560  loss = 3.37605 avg_loss = 2.85890\n",
      "epoch no.5 train no.362570  loss = 3.26878 avg_loss = 2.84865\n",
      "epoch no.5 train no.362580  loss = 2.72390 avg_loss = 2.83968\n",
      "epoch no.5 train no.362590  loss = 4.27631 avg_loss = 2.85100\n",
      "epoch no.5 train no.362600  loss = 2.74991 avg_loss = 2.87190\n",
      "epoch no.5 train no.362610  loss = 2.33316 avg_loss = 2.89060\n",
      "epoch no.5 train no.362620  loss = 1.84627 avg_loss = 2.84613\n",
      "epoch no.5 train no.362630  loss = 2.20157 avg_loss = 2.86112\n",
      "epoch no.5 train no.362640  loss = 2.25978 avg_loss = 2.83791\n",
      "epoch no.5 train no.362650  loss = 2.57094 avg_loss = 2.81818\n",
      "epoch no.5 train no.362660  loss = 2.75962 avg_loss = 2.82425\n",
      "epoch no.5 train no.362670  loss = 4.15686 avg_loss = 2.84751\n",
      "epoch no.5 train no.362680  loss = 2.57717 avg_loss = 2.84401\n",
      "epoch no.5 train no.362690  loss = 2.51763 avg_loss = 2.83521\n",
      "epoch no.5 train no.362700  loss = 2.10972 avg_loss = 2.81550\n",
      "epoch no.5 train no.362710  loss = 3.19633 avg_loss = 2.84795\n",
      "epoch no.5 train no.362720  loss = 2.78190 avg_loss = 2.83248\n",
      "epoch no.5 train no.362730  loss = 2.12365 avg_loss = 2.80037\n",
      "epoch no.5 train no.362740  loss = 3.41359 avg_loss = 2.83186\n",
      "epoch no.5 train no.362750  loss = 2.29335 avg_loss = 2.88547\n",
      "epoch no.5 train no.362760  loss = 3.10848 avg_loss = 2.84578\n",
      "epoch no.5 train no.362770  loss = 3.17374 avg_loss = 2.83458\n",
      "epoch no.5 train no.362780  loss = 2.47596 avg_loss = 2.82528\n",
      "epoch no.5 train no.362790  loss = 1.94323 avg_loss = 2.80474\n",
      "epoch no.5 train no.362800  loss = 2.86510 avg_loss = 2.82226\n",
      "epoch no.5 train no.362810  loss = 3.17303 avg_loss = 2.81897\n",
      "epoch no.5 train no.362820  loss = 3.71198 avg_loss = 2.84244\n",
      "epoch no.5 train no.362830  loss = 2.11383 avg_loss = 2.84115\n",
      "epoch no.5 train no.362840  loss = 2.98956 avg_loss = 2.86227\n",
      "epoch no.5 train no.362850  loss = 2.73340 avg_loss = 2.85628\n",
      "epoch no.5 train no.362860  loss = 2.77500 avg_loss = 2.90004\n",
      "epoch no.5 train no.362870  loss = 2.20403 avg_loss = 2.88568\n",
      "epoch no.5 train no.362880  loss = 2.96340 avg_loss = 2.88106\n",
      "epoch no.5 train no.362890  loss = 3.89841 avg_loss = 2.90214\n",
      "epoch no.5 train no.362900  loss = 2.95758 avg_loss = 2.89843\n",
      "epoch no.5 train no.362910  loss = 4.36358 avg_loss = 2.89099\n",
      "epoch no.5 train no.362920  loss = 2.52627 avg_loss = 2.88999\n",
      "epoch no.5 train no.362930  loss = 3.88770 avg_loss = 2.90611\n",
      "epoch no.5 train no.362940  loss = 3.11142 avg_loss = 2.92359\n",
      "epoch no.5 train no.362950  loss = 1.72159 avg_loss = 2.94980\n",
      "epoch no.5 train no.362960  loss = 2.52655 avg_loss = 2.92798\n",
      "epoch no.5 train no.362970  loss = 2.03948 avg_loss = 2.92930\n",
      "epoch no.5 train no.362980  loss = 3.79650 avg_loss = 2.94846\n",
      "epoch no.5 train no.362990  loss = 3.09712 avg_loss = 2.99847\n",
      "epoch no.5 train no.363000  loss = 3.83596 avg_loss = 2.99204\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '▁역시', '한', '▁터지는', '▁노래', '피', '컬', '▁팝', '</s>']\n",
      "여름엔 청량감 넘치는 트로피컬 팝</s>\n",
      "epoch no.5 train no.363010  loss = 2.46848 avg_loss = 2.98060\n",
      "epoch no.5 train no.363020  loss = 1.59470 avg_loss = 2.95319\n",
      "epoch no.5 train no.363030  loss = 2.17001 avg_loss = 2.94342\n",
      "epoch no.5 train no.363040  loss = 1.55953 avg_loss = 2.95465\n",
      "epoch no.5 train no.363050  loss = 2.57676 avg_loss = 2.96193\n",
      "epoch no.5 train no.363060  loss = 2.68210 avg_loss = 2.96984\n",
      "epoch no.5 train no.363070  loss = 2.82805 avg_loss = 2.95775\n",
      "epoch no.5 train no.363080  loss = 3.12165 avg_loss = 2.91112\n",
      "epoch no.5 train no.363090  loss = 2.26781 avg_loss = 2.88916\n",
      "epoch no.5 train no.363100  loss = 2.53539 avg_loss = 2.89998\n",
      "epoch no.5 train no.363110  loss = 2.28333 avg_loss = 2.89529\n",
      "epoch no.5 train no.363120  loss = 3.24351 avg_loss = 2.89951\n",
      "epoch no.5 train no.363130  loss = 3.35183 avg_loss = 2.91471\n",
      "epoch no.5 train no.363140  loss = 2.37371 avg_loss = 2.90245\n",
      "epoch no.5 train no.363150  loss = 3.09780 avg_loss = 2.87773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.363160  loss = 2.30425 avg_loss = 2.89283\n",
      "epoch no.5 train no.363170  loss = 3.98883 avg_loss = 2.89393\n",
      "epoch no.5 train no.363180  loss = 2.50218 avg_loss = 2.90784\n",
      "epoch no.5 train no.363190  loss = 2.04602 avg_loss = 2.92520\n",
      "epoch no.5 train no.363200  loss = 3.48606 avg_loss = 2.94241\n",
      "epoch no.5 train no.363210  loss = 2.82401 avg_loss = 2.92380\n",
      "epoch no.5 train no.363220  loss = 3.09643 avg_loss = 2.88648\n",
      "epoch no.5 train no.363230  loss = 4.03918 avg_loss = 2.91445\n",
      "epoch no.5 train no.363240  loss = 4.81062 avg_loss = 2.94764\n",
      "epoch no.5 train no.363250  loss = 3.34722 avg_loss = 2.94285\n",
      "epoch no.5 train no.363260  loss = 2.35167 avg_loss = 2.93548\n",
      "epoch no.5 train no.363270  loss = 2.58489 avg_loss = 2.94339\n",
      "epoch no.5 train no.363280  loss = 1.73607 avg_loss = 2.93738\n",
      "epoch no.5 train no.363290  loss = 1.89667 avg_loss = 2.93174\n",
      "epoch no.5 train no.363300  loss = 2.16172 avg_loss = 2.90809\n",
      "epoch no.5 train no.363310  loss = 2.77707 avg_loss = 2.88741\n",
      "epoch no.5 train no.363320  loss = 2.14003 avg_loss = 2.87319\n",
      "epoch no.5 train no.363330  loss = 2.28065 avg_loss = 2.87807\n",
      "epoch no.5 train no.363340  loss = 1.88088 avg_loss = 2.85619\n",
      "epoch no.5 train no.363350  loss = 3.32792 avg_loss = 2.85832\n",
      "epoch no.5 train no.363360  loss = 2.92731 avg_loss = 2.88721\n",
      "epoch no.5 train no.363370  loss = 2.14638 avg_loss = 2.88578\n",
      "epoch no.5 train no.363380  loss = 2.48773 avg_loss = 2.86485\n",
      "epoch no.5 train no.363390  loss = 2.52586 avg_loss = 2.87139\n",
      "epoch no.5 train no.363400  loss = 2.63105 avg_loss = 2.91676\n",
      "epoch no.5 train no.363410  loss = 2.40297 avg_loss = 2.92255\n",
      "epoch no.5 train no.363420  loss = 2.97207 avg_loss = 2.90812\n",
      "epoch no.5 train no.363430  loss = 2.39310 avg_loss = 2.90692\n",
      "epoch no.5 train no.363440  loss = 3.79092 avg_loss = 2.91104\n",
      "epoch no.5 train no.363450  loss = 2.85526 avg_loss = 2.92782\n",
      "epoch no.5 train no.363460  loss = 2.65468 avg_loss = 2.92528\n",
      "epoch no.5 train no.363470  loss = 3.00947 avg_loss = 2.90739\n",
      "epoch no.5 train no.363480  loss = 2.62976 avg_loss = 2.95576\n",
      "epoch no.5 train no.363490  loss = 2.74375 avg_loss = 2.94015\n",
      "epoch no.5 train no.363500  loss = 1.94231 avg_loss = 2.90431\n",
      "epoch no.5 train no.363510  loss = 3.34118 avg_loss = 2.86172\n",
      "epoch no.5 train no.363520  loss = 3.87598 avg_loss = 2.85471\n",
      "epoch no.5 train no.363530  loss = 1.90065 avg_loss = 2.83859\n",
      "epoch no.5 train no.363540  loss = 2.73686 avg_loss = 2.84101\n",
      "epoch no.5 train no.363550  loss = 4.72773 avg_loss = 2.87853\n",
      "epoch no.5 train no.363560  loss = 2.70582 avg_loss = 2.91231\n",
      "epoch no.5 train no.363570  loss = 3.87119 avg_loss = 2.91308\n",
      "epoch no.5 train no.363580  loss = 1.79388 avg_loss = 2.93738\n",
      "epoch no.5 train no.363590  loss = 2.35354 avg_loss = 2.92363\n",
      "epoch no.5 train no.363600  loss = 2.89380 avg_loss = 2.92573\n",
      "epoch no.5 train no.363610  loss = 3.51023 avg_loss = 2.94122\n",
      "epoch no.5 train no.363620  loss = 3.04338 avg_loss = 2.95338\n",
      "epoch no.5 train no.363630  loss = 2.43463 avg_loss = 2.94738\n",
      "epoch no.5 train no.363640  loss = 2.85141 avg_loss = 2.93301\n",
      "epoch no.5 train no.363650  loss = 2.32536 avg_loss = 2.95000\n",
      "epoch no.5 train no.363660  loss = 2.66992 avg_loss = 2.96928\n",
      "epoch no.5 train no.363670  loss = 2.63019 avg_loss = 2.90558\n",
      "epoch no.5 train no.363680  loss = 3.88485 avg_loss = 2.90218\n",
      "epoch no.5 train no.363690  loss = 3.18931 avg_loss = 2.96845\n",
      "epoch no.5 train no.363700  loss = 2.47439 avg_loss = 2.93816\n",
      "epoch no.5 train no.363710  loss = 2.03345 avg_loss = 2.94604\n",
      "epoch no.5 train no.363720  loss = 4.72625 avg_loss = 2.99346\n",
      "epoch no.5 train no.363730  loss = 2.20087 avg_loss = 2.97254\n",
      "epoch no.5 train no.363740  loss = 2.45664 avg_loss = 2.95481\n",
      "epoch no.5 train no.363750  loss = 3.40505 avg_loss = 2.94524\n",
      "epoch no.5 train no.363760  loss = 2.37500 avg_loss = 2.91650\n",
      "epoch no.5 train no.363770  loss = 2.90086 avg_loss = 2.94173\n",
      "epoch no.5 train no.363780  loss = 3.20345 avg_loss = 2.93232\n",
      "epoch no.5 train no.363790  loss = 2.33219 avg_loss = 2.91426\n",
      "epoch no.5 train no.363800  loss = 2.51752 avg_loss = 2.89429\n",
      "epoch no.5 train no.363810  loss = 2.09123 avg_loss = 2.89989\n",
      "epoch no.5 train no.363820  loss = 3.34494 avg_loss = 2.88467\n",
      "epoch no.5 train no.363830  loss = 1.93624 avg_loss = 2.88458\n",
      "epoch no.5 train no.363840  loss = 3.77745 avg_loss = 2.90340\n",
      "epoch no.5 train no.363850  loss = 3.95011 avg_loss = 2.91723\n",
      "epoch no.5 train no.363860  loss = 3.66150 avg_loss = 2.93397\n",
      "epoch no.5 train no.363870  loss = 2.50792 avg_loss = 2.96666\n",
      "epoch no.5 train no.363880  loss = 2.51259 avg_loss = 2.97128\n",
      "epoch no.5 train no.363890  loss = 2.49080 avg_loss = 2.94891\n",
      "epoch no.5 train no.363900  loss = 2.57165 avg_loss = 2.96071\n",
      "epoch no.5 train no.363910  loss = 3.12523 avg_loss = 2.96946\n",
      "epoch no.5 train no.363920  loss = 2.90264 avg_loss = 2.97430\n",
      "epoch no.5 train no.363930  loss = 2.48979 avg_loss = 2.94091\n",
      "epoch no.5 train no.363940  loss = 1.76051 avg_loss = 2.97079\n",
      "epoch no.5 train no.363950  loss = 1.85458 avg_loss = 2.96493\n",
      "epoch no.5 train no.363960  loss = 2.53199 avg_loss = 2.95547\n",
      "epoch no.5 train no.363970  loss = 1.69953 avg_loss = 2.97573\n",
      "epoch no.5 train no.363980  loss = 1.76748 avg_loss = 2.94707\n",
      "epoch no.5 train no.363990  loss = 2.86735 avg_loss = 2.93907\n",
      "epoch no.5 train no.364000  loss = 3.39376 avg_loss = 2.97918\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁감성', 'op', '</s>']\n",
      "여름밤에 듣는 pop</s>\n",
      "epoch no.5 train no.364010  loss = 2.21877 avg_loss = 2.92820\n",
      "epoch no.5 train no.364020  loss = 3.32953 avg_loss = 2.93056\n",
      "epoch no.5 train no.364030  loss = 2.80489 avg_loss = 2.94773\n",
      "epoch no.5 train no.364040  loss = 4.14223 avg_loss = 2.96847\n",
      "epoch no.5 train no.364050  loss = 3.02688 avg_loss = 2.98439\n",
      "epoch no.5 train no.364060  loss = 2.95788 avg_loss = 2.96876\n",
      "epoch no.5 train no.364070  loss = 2.11149 avg_loss = 2.92413\n",
      "epoch no.5 train no.364080  loss = 2.79510 avg_loss = 2.88692\n",
      "epoch no.5 train no.364090  loss = 2.68848 avg_loss = 2.91317\n",
      "epoch no.5 train no.364100  loss = 4.83700 avg_loss = 2.93866\n",
      "epoch no.5 train no.364110  loss = 4.05433 avg_loss = 2.98975\n",
      "epoch no.5 train no.364120  loss = 4.08853 avg_loss = 2.96528\n",
      "epoch no.5 train no.364130  loss = 1.56995 avg_loss = 2.97525\n",
      "epoch no.5 train no.364140  loss = 2.89947 avg_loss = 2.95378\n",
      "epoch no.5 train no.364150  loss = 4.52167 avg_loss = 2.96423\n",
      "epoch no.5 train no.364160  loss = 2.75295 avg_loss = 2.95227\n",
      "epoch no.5 train no.364170  loss = 2.81578 avg_loss = 2.90599\n",
      "epoch no.5 train no.364180  loss = 4.09699 avg_loss = 2.91258\n",
      "epoch no.5 train no.364190  loss = 2.38439 avg_loss = 2.90789\n",
      "epoch no.5 train no.364200  loss = 3.24180 avg_loss = 2.90226\n",
      "epoch no.5 train no.364210  loss = 4.50178 avg_loss = 2.89949\n",
      "epoch no.5 train no.364220  loss = 1.76498 avg_loss = 2.85383\n",
      "epoch no.5 train no.364230  loss = 1.94933 avg_loss = 2.86488\n",
      "epoch no.5 train no.364240  loss = 2.77532 avg_loss = 2.87175\n",
      "epoch no.5 train no.364250  loss = 3.82013 avg_loss = 2.87452\n",
      "epoch no.5 train no.364260  loss = 2.23304 avg_loss = 2.88621\n",
      "epoch no.5 train no.364270  loss = 3.44929 avg_loss = 2.88907\n",
      "epoch no.5 train no.364280  loss = 3.50244 avg_loss = 2.92411\n",
      "epoch no.5 train no.364290  loss = 3.33439 avg_loss = 2.92585\n",
      "epoch no.5 train no.364300  loss = 3.07742 avg_loss = 2.92959\n",
      "epoch no.5 train no.364310  loss = 3.84644 avg_loss = 2.89241\n",
      "epoch no.5 train no.364320  loss = 2.44258 avg_loss = 2.86684\n",
      "epoch no.5 train no.364330  loss = 3.57978 avg_loss = 2.88907\n",
      "epoch no.5 train no.364340  loss = 2.63247 avg_loss = 2.83589\n",
      "epoch no.5 train no.364350  loss = 2.71247 avg_loss = 2.85761\n",
      "epoch no.5 train no.364360  loss = 3.40679 avg_loss = 2.84416\n",
      "epoch no.5 train no.364370  loss = 4.17573 avg_loss = 2.84627\n",
      "epoch no.5 train no.364380  loss = 3.58598 avg_loss = 2.85605\n",
      "epoch no.5 train no.364390  loss = 2.53110 avg_loss = 2.89479\n",
      "epoch no.5 train no.364400  loss = 3.16408 avg_loss = 2.91365\n",
      "epoch no.5 train no.364410  loss = 3.10428 avg_loss = 2.90295\n",
      "epoch no.5 train no.364420  loss = 4.23463 avg_loss = 2.95106\n",
      "epoch no.5 train no.364430  loss = 3.80694 avg_loss = 2.90886\n",
      "epoch no.5 train no.364440  loss = 3.96530 avg_loss = 2.93034\n",
      "epoch no.5 train no.364450  loss = 2.63184 avg_loss = 2.89617\n",
      "epoch no.5 train no.364460  loss = 1.95780 avg_loss = 2.92455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.364470  loss = 2.13368 avg_loss = 2.93912\n",
      "epoch no.5 train no.364480  loss = 3.65888 avg_loss = 2.93472\n",
      "epoch no.5 train no.364490  loss = 2.85387 avg_loss = 2.94857\n",
      "epoch no.5 train no.364500  loss = 2.52444 avg_loss = 2.96052\n",
      "epoch no.5 train no.364510  loss = 3.39329 avg_loss = 2.96979\n",
      "epoch no.5 train no.364520  loss = 3.43998 avg_loss = 2.97777\n",
      "epoch no.5 train no.364530  loss = 2.02835 avg_loss = 2.96067\n",
      "epoch no.5 train no.364540  loss = 4.04564 avg_loss = 2.96720\n",
      "epoch no.5 train no.364550  loss = 2.35736 avg_loss = 2.96014\n",
      "epoch no.5 train no.364560  loss = 4.41300 avg_loss = 2.96267\n",
      "epoch no.5 train no.364570  loss = 3.98083 avg_loss = 2.99194\n",
      "epoch no.5 train no.364580  loss = 1.93312 avg_loss = 2.98915\n",
      "epoch no.5 train no.364590  loss = 5.08318 avg_loss = 3.01399\n",
      "epoch no.5 train no.364600  loss = 4.09579 avg_loss = 3.02287\n",
      "epoch no.5 train no.364610  loss = 2.86504 avg_loss = 3.00812\n",
      "epoch no.5 train no.364620  loss = 3.03619 avg_loss = 3.01219\n",
      "epoch no.5 train no.364630  loss = 2.25731 avg_loss = 3.01826\n",
      "epoch no.5 train no.364640  loss = 3.23094 avg_loss = 2.99058\n",
      "epoch no.5 train no.364650  loss = 2.43987 avg_loss = 2.95998\n",
      "epoch no.5 train no.364660  loss = 3.86571 avg_loss = 3.00809\n",
      "epoch no.5 train no.364670  loss = 2.94659 avg_loss = 2.99823\n",
      "epoch no.5 train no.364680  loss = 3.01194 avg_loss = 2.95249\n",
      "epoch no.5 train no.364690  loss = 2.76214 avg_loss = 2.95029\n",
      "epoch no.5 train no.364700  loss = 4.16967 avg_loss = 2.93603\n",
      "epoch no.5 train no.364710  loss = 1.22512 avg_loss = 2.94448\n",
      "epoch no.5 train no.364720  loss = 2.75845 avg_loss = 2.93233\n",
      "epoch no.5 train no.364730  loss = 2.32657 avg_loss = 2.95089\n",
      "epoch no.5 train no.364740  loss = 3.15646 avg_loss = 2.95806\n",
      "epoch no.5 train no.364750  loss = 2.28665 avg_loss = 2.94621\n",
      "epoch no.5 train no.364760  loss = 3.40822 avg_loss = 2.92978\n",
      "epoch no.5 train no.364770  loss = 4.20512 avg_loss = 2.92476\n",
      "epoch no.5 train no.364780  loss = 1.72717 avg_loss = 2.89819\n",
      "epoch no.5 train no.364790  loss = 3.11348 avg_loss = 2.90911\n",
      "epoch no.5 train no.364800  loss = 2.75871 avg_loss = 2.90873\n",
      "epoch no.5 train no.364810  loss = 2.18055 avg_loss = 2.90803\n",
      "epoch no.5 train no.364820  loss = 4.23974 avg_loss = 2.92921\n",
      "epoch no.5 train no.364830  loss = 2.85910 avg_loss = 2.91628\n",
      "epoch no.5 train no.364840  loss = 3.79898 avg_loss = 2.92447\n",
      "epoch no.5 train no.364850  loss = 5.38075 avg_loss = 2.92995\n",
      "epoch no.5 train no.364860  loss = 3.19175 avg_loss = 2.90784\n",
      "epoch no.5 train no.364870  loss = 2.93773 avg_loss = 2.91435\n",
      "epoch no.5 train no.364880  loss = 4.20836 avg_loss = 2.89591\n",
      "epoch no.5 train no.364890  loss = 1.99673 avg_loss = 2.89172\n",
      "epoch no.5 train no.364900  loss = 3.71859 avg_loss = 2.94582\n",
      "epoch no.5 train no.364910  loss = 2.69633 avg_loss = 2.91306\n",
      "epoch no.5 train no.364920  loss = 3.32968 avg_loss = 2.90400\n",
      "epoch no.5 train no.364930  loss = 4.22008 avg_loss = 2.90009\n",
      "epoch no.5 train no.364940  loss = 2.79930 avg_loss = 2.89609\n",
      "epoch no.5 train no.364950  loss = 2.54717 avg_loss = 2.88567\n",
      "epoch no.5 train no.364960  loss = 3.02124 avg_loss = 2.87282\n",
      "epoch no.5 train no.364970  loss = 2.36117 avg_loss = 2.86095\n",
      "epoch no.5 train no.364980  loss = 3.53070 avg_loss = 2.85551\n",
      "epoch no.5 train no.364990  loss = 2.37662 avg_loss = 2.84826\n",
      "epoch no.5 train no.365000  loss = 2.66390 avg_loss = 2.89757\n",
      "7\n",
      "to_tokens: ['▁가을', '밤', '에', '▁어울리는', '▁잔잔', '미', '로운', '▁클래식', '</s>']\n",
      "여름밤에 듣는 감미로운 팝</s>\n",
      "epoch no.5 train no.365010  loss = 3.07117 avg_loss = 2.86514\n",
      "epoch no.5 train no.365020  loss = 2.48157 avg_loss = 2.90661\n",
      "epoch no.5 train no.365030  loss = 3.60627 avg_loss = 2.88534\n",
      "epoch no.5 train no.365040  loss = 3.53416 avg_loss = 2.94904\n",
      "epoch no.5 train no.365050  loss = 2.32759 avg_loss = 2.94631\n",
      "epoch no.5 train no.365060  loss = 2.91643 avg_loss = 2.94554\n",
      "epoch no.5 train no.365070  loss = 2.43174 avg_loss = 2.95203\n",
      "epoch no.5 train no.365080  loss = 4.03512 avg_loss = 2.95092\n",
      "epoch no.5 train no.365090  loss = 4.65148 avg_loss = 2.97159\n",
      "epoch no.5 train no.365100  loss = 2.46784 avg_loss = 2.96031\n",
      "epoch no.5 train no.365110  loss = 2.03198 avg_loss = 2.97550\n",
      "epoch no.5 train no.365120  loss = 4.13503 avg_loss = 2.94551\n",
      "epoch no.5 train no.365130  loss = 2.35593 avg_loss = 2.92196\n",
      "epoch no.5 train no.365140  loss = 3.09795 avg_loss = 2.94538\n",
      "epoch no.5 train no.365150  loss = 2.03581 avg_loss = 2.95961\n",
      "epoch no.5 train no.365160  loss = 2.71848 avg_loss = 2.97285\n",
      "epoch no.5 train no.365170  loss = 2.49638 avg_loss = 2.97452\n",
      "epoch no.5 train no.365180  loss = 2.14436 avg_loss = 2.95313\n",
      "epoch no.5 train no.365190  loss = 2.75635 avg_loss = 2.96301\n",
      "epoch no.5 train no.365200  loss = 3.86414 avg_loss = 2.99070\n",
      "epoch no.5 train no.365210  loss = 2.56534 avg_loss = 2.97947\n",
      "epoch no.5 train no.365220  loss = 2.58097 avg_loss = 2.99325\n",
      "epoch no.5 train no.365230  loss = 3.14499 avg_loss = 2.98867\n",
      "epoch no.5 train no.365240  loss = 3.81521 avg_loss = 2.97599\n",
      "epoch no.5 train no.365250  loss = 2.65699 avg_loss = 2.96539\n",
      "epoch no.5 train no.365260  loss = 4.18810 avg_loss = 2.96175\n",
      "epoch no.5 train no.365270  loss = 2.36213 avg_loss = 2.93616\n",
      "epoch no.5 train no.365280  loss = 1.80242 avg_loss = 2.93098\n",
      "epoch no.5 train no.365290  loss = 3.47828 avg_loss = 2.97079\n",
      "epoch no.5 train no.365300  loss = 3.22203 avg_loss = 2.98914\n",
      "epoch no.5 train no.365310  loss = 2.38108 avg_loss = 2.95562\n",
      "epoch no.5 train no.365320  loss = 2.43688 avg_loss = 2.98524\n",
      "epoch no.5 train no.365330  loss = 3.34369 avg_loss = 2.97881\n",
      "epoch no.5 train no.365340  loss = 4.61337 avg_loss = 2.97770\n",
      "epoch no.5 train no.365350  loss = 2.57439 avg_loss = 2.95633\n",
      "epoch no.5 train no.365360  loss = 2.96050 avg_loss = 2.93881\n",
      "epoch no.5 train no.365370  loss = 2.56288 avg_loss = 2.88007\n",
      "epoch no.5 train no.365380  loss = 3.08905 avg_loss = 2.89630\n",
      "epoch no.5 train no.365390  loss = 3.27273 avg_loss = 2.93823\n",
      "epoch no.5 train no.365400  loss = 2.47273 avg_loss = 2.94424\n",
      "epoch no.5 train no.365410  loss = 3.40742 avg_loss = 2.96816\n",
      "epoch no.5 train no.365420  loss = 4.10931 avg_loss = 2.97530\n",
      "epoch no.5 train no.365430  loss = 2.57536 avg_loss = 2.99903\n",
      "epoch no.5 train no.365440  loss = 3.14512 avg_loss = 3.00032\n",
      "epoch no.5 train no.365450  loss = 2.86384 avg_loss = 3.01457\n",
      "epoch no.5 train no.365460  loss = 3.22735 avg_loss = 3.03433\n",
      "epoch no.5 train no.365470  loss = 4.11079 avg_loss = 3.03020\n",
      "epoch no.5 train no.365480  loss = 2.21207 avg_loss = 2.99476\n",
      "epoch no.5 train no.365490  loss = 2.24120 avg_loss = 2.96077\n",
      "epoch no.5 train no.365500  loss = 2.70241 avg_loss = 2.95423\n",
      "epoch no.5 train no.365510  loss = 2.46374 avg_loss = 2.92539\n",
      "epoch no.5 train no.365520  loss = 4.38517 avg_loss = 2.92965\n",
      "epoch no.5 train no.365530  loss = 1.93162 avg_loss = 2.90481\n",
      "epoch no.5 train no.365540  loss = 2.75952 avg_loss = 2.93270\n",
      "epoch no.5 train no.365550  loss = 2.39453 avg_loss = 2.94573\n",
      "epoch no.5 train no.365560  loss = 3.00434 avg_loss = 2.91847\n",
      "epoch no.5 train no.365570  loss = 2.52041 avg_loss = 2.88658\n",
      "epoch no.5 train no.365580  loss = 2.56581 avg_loss = 2.89599\n",
      "epoch no.5 train no.365590  loss = 2.02921 avg_loss = 2.90404\n",
      "epoch no.5 train no.365600  loss = 4.03949 avg_loss = 2.91072\n",
      "epoch no.5 train no.365610  loss = 3.29379 avg_loss = 2.89082\n",
      "epoch no.5 train no.365620  loss = 2.47912 avg_loss = 2.89963\n",
      "epoch no.5 train no.365630  loss = 1.94435 avg_loss = 2.89779\n",
      "epoch no.5 train no.365640  loss = 2.09389 avg_loss = 2.88185\n",
      "epoch no.5 train no.365650  loss = 4.48072 avg_loss = 2.89769\n",
      "epoch no.5 train no.365660  loss = 2.64144 avg_loss = 2.85998\n",
      "epoch no.5 train no.365670  loss = 1.94057 avg_loss = 2.84724\n",
      "epoch no.5 train no.365680  loss = 2.83466 avg_loss = 2.82750\n",
      "epoch no.5 train no.365690  loss = 2.20992 avg_loss = 2.84848\n",
      "epoch no.5 train no.365700  loss = 3.22091 avg_loss = 2.86513\n",
      "epoch no.5 train no.365710  loss = 3.33870 avg_loss = 2.84330\n",
      "epoch no.5 train no.365720  loss = 2.93590 avg_loss = 2.83729\n",
      "epoch no.5 train no.365730  loss = 2.29778 avg_loss = 2.86707\n",
      "epoch no.5 train no.365740  loss = 3.01839 avg_loss = 2.85327\n",
      "epoch no.5 train no.365750  loss = 4.01808 avg_loss = 2.87217\n",
      "epoch no.5 train no.365760  loss = 3.58908 avg_loss = 2.88289\n",
      "epoch no.5 train no.365770  loss = 2.90234 avg_loss = 2.91621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.365780  loss = 3.09410 avg_loss = 2.94070\n",
      "epoch no.5 train no.365790  loss = 1.52974 avg_loss = 2.94001\n",
      "epoch no.5 train no.365800  loss = 3.61413 avg_loss = 2.95624\n",
      "epoch no.5 train no.365810  loss = 2.17379 avg_loss = 2.93894\n",
      "epoch no.5 train no.365820  loss = 3.28057 avg_loss = 2.94235\n",
      "epoch no.5 train no.365830  loss = 2.27786 avg_loss = 2.91953\n",
      "epoch no.5 train no.365840  loss = 2.65187 avg_loss = 2.92303\n",
      "epoch no.5 train no.365850  loss = 3.24200 avg_loss = 2.93162\n",
      "epoch no.5 train no.365860  loss = 2.72393 avg_loss = 2.94734\n",
      "epoch no.5 train no.365870  loss = 3.60217 avg_loss = 2.93977\n",
      "epoch no.5 train no.365880  loss = 2.98695 avg_loss = 2.89264\n",
      "epoch no.5 train no.365890  loss = 3.29413 avg_loss = 2.93806\n",
      "epoch no.5 train no.365900  loss = 2.96377 avg_loss = 2.98457\n",
      "epoch no.5 train no.365910  loss = 3.14472 avg_loss = 2.95730\n",
      "epoch no.5 train no.365920  loss = 2.18942 avg_loss = 2.94735\n",
      "epoch no.5 train no.365930  loss = 2.66612 avg_loss = 2.90025\n",
      "epoch no.5 train no.365940  loss = 1.68821 avg_loss = 2.95397\n",
      "epoch no.5 train no.365950  loss = 2.10286 avg_loss = 2.92642\n",
      "epoch no.5 train no.365960  loss = 2.62856 avg_loss = 2.91700\n",
      "epoch no.5 train no.365970  loss = 2.31683 avg_loss = 2.91794\n",
      "epoch no.5 train no.365980  loss = 3.84745 avg_loss = 2.90480\n",
      "epoch no.5 train no.365990  loss = 4.00208 avg_loss = 2.92033\n",
      "epoch no.5 train no.366000  loss = 3.61869 avg_loss = 2.90708\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '▁잔잔', '▁수', '줄', '▁감성', '▁뉴', '</s>']\n",
      "여름밤을 채워줄 아름다운 재즈</s>\n",
      "epoch no.5 train no.366010  loss = 4.09310 avg_loss = 2.91730\n",
      "epoch no.5 train no.366020  loss = 2.63381 avg_loss = 2.92677\n",
      "epoch no.5 train no.366030  loss = 2.05878 avg_loss = 2.92002\n",
      "epoch no.5 train no.366040  loss = 3.58535 avg_loss = 2.92528\n",
      "epoch no.5 train no.366050  loss = 3.66598 avg_loss = 2.90883\n",
      "epoch no.5 train no.366060  loss = 2.39355 avg_loss = 2.88516\n",
      "epoch no.5 train no.366070  loss = 2.85604 avg_loss = 2.93029\n",
      "epoch no.5 train no.366080  loss = 3.55107 avg_loss = 2.93992\n",
      "epoch no.5 train no.366090  loss = 2.08707 avg_loss = 2.93172\n",
      "epoch no.5 train no.366100  loss = 3.66787 avg_loss = 2.90346\n",
      "epoch no.5 train no.366110  loss = 3.25207 avg_loss = 2.86680\n",
      "epoch no.5 train no.366120  loss = 4.31512 avg_loss = 2.83253\n",
      "epoch no.5 train no.366130  loss = 2.99086 avg_loss = 2.86425\n",
      "epoch no.5 train no.366140  loss = 4.08983 avg_loss = 2.94658\n",
      "epoch no.5 train no.366150  loss = 4.37694 avg_loss = 2.96285\n",
      "epoch no.5 train no.366160  loss = 2.52314 avg_loss = 2.98060\n",
      "epoch no.5 train no.366170  loss = 3.27818 avg_loss = 3.03947\n",
      "epoch no.5 train no.366180  loss = 3.13946 avg_loss = 3.01474\n",
      "epoch no.5 train no.366190  loss = 1.83288 avg_loss = 3.01091\n",
      "epoch no.5 train no.366200  loss = 4.21553 avg_loss = 3.02680\n",
      "epoch no.5 train no.366210  loss = 3.16103 avg_loss = 3.03574\n",
      "epoch no.5 train no.366220  loss = 3.48088 avg_loss = 3.03543\n",
      "epoch no.5 train no.366230  loss = 2.30952 avg_loss = 3.01280\n",
      "epoch no.5 train no.366240  loss = 3.78271 avg_loss = 3.02431\n",
      "epoch no.5 train no.366250  loss = 5.06262 avg_loss = 3.01103\n",
      "epoch no.5 train no.366260  loss = 2.93957 avg_loss = 3.02758\n",
      "epoch no.5 train no.366270  loss = 2.92438 avg_loss = 3.01467\n",
      "epoch no.5 train no.366280  loss = 5.22766 avg_loss = 2.99343\n",
      "epoch no.5 train no.366290  loss = 2.32375 avg_loss = 2.97970\n",
      "epoch no.5 train no.366300  loss = 3.18806 avg_loss = 2.97524\n",
      "epoch no.5 train no.366310  loss = 2.14036 avg_loss = 2.94917\n",
      "epoch no.5 train no.366320  loss = 2.64856 avg_loss = 2.92859\n",
      "epoch no.5 train no.366330  loss = 3.11419 avg_loss = 2.92262\n",
      "epoch no.5 train no.366340  loss = 2.45282 avg_loss = 2.86344\n",
      "epoch no.5 train no.366350  loss = 3.38374 avg_loss = 2.86165\n",
      "epoch no.5 train no.366360  loss = 5.28900 avg_loss = 2.89044\n",
      "epoch no.5 train no.366370  loss = 2.61222 avg_loss = 2.89051\n",
      "epoch no.5 train no.366380  loss = 1.53391 avg_loss = 2.86857\n",
      "epoch no.5 train no.366390  loss = 2.56787 avg_loss = 2.86523\n",
      "epoch no.5 train no.366400  loss = 2.51575 avg_loss = 2.84601\n",
      "epoch no.5 train no.366410  loss = 3.01482 avg_loss = 2.87295\n",
      "epoch no.5 train no.366420  loss = 3.20993 avg_loss = 2.88815\n",
      "epoch no.5 train no.366430  loss = 1.92428 avg_loss = 2.85659\n",
      "epoch no.5 train no.366440  loss = 1.74940 avg_loss = 2.83110\n",
      "epoch no.5 train no.366450  loss = 1.98708 avg_loss = 2.78856\n",
      "epoch no.5 train no.366460  loss = 1.97629 avg_loss = 2.79219\n",
      "epoch no.5 train no.366470  loss = 3.25489 avg_loss = 2.79164\n",
      "epoch no.5 train no.366480  loss = 3.03810 avg_loss = 2.81635\n",
      "epoch no.5 train no.366490  loss = 3.91899 avg_loss = 2.81870\n",
      "epoch no.5 train no.366500  loss = 3.96841 avg_loss = 2.83917\n",
      "epoch no.5 train no.366510  loss = 2.55411 avg_loss = 2.79015\n",
      "epoch no.5 train no.366520  loss = 2.59703 avg_loss = 2.80854\n",
      "epoch no.5 train no.366530  loss = 3.42220 avg_loss = 2.84304\n",
      "epoch no.5 train no.366540  loss = 1.79742 avg_loss = 2.85915\n",
      "epoch no.5 train no.366550  loss = 3.34699 avg_loss = 2.86109\n",
      "epoch no.5 train no.366560  loss = 3.88707 avg_loss = 2.87350\n",
      "epoch no.5 train no.366570  loss = 1.25923 avg_loss = 2.84963\n",
      "epoch no.5 train no.366580  loss = 3.06363 avg_loss = 2.79561\n",
      "epoch no.5 train no.366590  loss = 2.22936 avg_loss = 2.84208\n",
      "epoch no.5 train no.366600  loss = 2.65034 avg_loss = 2.83689\n",
      "epoch no.5 train no.366610  loss = 2.11080 avg_loss = 2.81265\n",
      "epoch no.5 train no.366620  loss = 3.30556 avg_loss = 2.86456\n",
      "epoch no.5 train no.366630  loss = 3.45595 avg_loss = 2.85490\n",
      "epoch no.5 train no.366640  loss = 2.51218 avg_loss = 2.83424\n",
      "epoch no.5 train no.366650  loss = 3.71215 avg_loss = 2.85336\n",
      "epoch no.5 train no.366660  loss = 3.14933 avg_loss = 2.85254\n",
      "epoch no.5 train no.366670  loss = 3.60793 avg_loss = 2.89038\n",
      "epoch no.5 train no.366680  loss = 2.70886 avg_loss = 2.92348\n",
      "epoch no.5 train no.366690  loss = 4.19077 avg_loss = 2.93356\n",
      "epoch no.5 train no.366700  loss = 3.97548 avg_loss = 2.98227\n",
      "epoch no.5 train no.366710  loss = 2.97537 avg_loss = 2.96535\n",
      "epoch no.5 train no.366720  loss = 2.52196 avg_loss = 2.98153\n",
      "epoch no.5 train no.366730  loss = 2.85360 avg_loss = 2.95823\n",
      "epoch no.5 train no.366740  loss = 2.39543 avg_loss = 3.00358\n",
      "epoch no.5 train no.366750  loss = 2.37457 avg_loss = 2.96869\n",
      "epoch no.5 train no.366760  loss = 3.23108 avg_loss = 2.98142\n",
      "epoch no.5 train no.366770  loss = 2.32274 avg_loss = 2.96849\n",
      "epoch no.5 train no.366780  loss = 3.78002 avg_loss = 2.97751\n",
      "epoch no.5 train no.366790  loss = 2.44456 avg_loss = 2.93809\n",
      "epoch no.5 train no.366800  loss = 2.97833 avg_loss = 2.94493\n",
      "epoch no.5 train no.366810  loss = 2.90666 avg_loss = 2.95435\n",
      "epoch no.5 train no.366820  loss = 2.91332 avg_loss = 2.89739\n",
      "epoch no.5 train no.366830  loss = 3.39269 avg_loss = 2.90809\n",
      "epoch no.5 train no.366840  loss = 2.66527 avg_loss = 2.92148\n",
      "epoch no.5 train no.366850  loss = 3.01910 avg_loss = 2.94595\n",
      "epoch no.5 train no.366860  loss = 4.05308 avg_loss = 2.92171\n",
      "epoch no.5 train no.366870  loss = 3.84273 avg_loss = 2.95126\n",
      "epoch no.5 train no.366880  loss = 2.84768 avg_loss = 2.96108\n",
      "epoch no.5 train no.366890  loss = 2.87647 avg_loss = 2.95926\n",
      "epoch no.5 train no.366900  loss = 3.11990 avg_loss = 2.96291\n",
      "epoch no.5 train no.366910  loss = 2.41563 avg_loss = 2.98874\n",
      "epoch no.5 train no.366920  loss = 2.88635 avg_loss = 3.00190\n",
      "epoch no.5 train no.366930  loss = 2.72493 avg_loss = 2.96844\n",
      "epoch no.5 train no.366940  loss = 1.49580 avg_loss = 2.95069\n",
      "epoch no.5 train no.366950  loss = 2.54538 avg_loss = 2.94951\n",
      "epoch no.5 train no.366960  loss = 2.59893 avg_loss = 2.90378\n",
      "epoch no.5 train no.366970  loss = 1.89658 avg_loss = 2.86136\n",
      "epoch no.5 train no.366980  loss = 2.66930 avg_loss = 2.84593\n",
      "epoch no.5 train no.366990  loss = 3.07257 avg_loss = 2.88127\n",
      "epoch no.5 train no.367000  loss = 2.75173 avg_loss = 2.90154\n",
      "9\n",
      "to_tokens: ['▁비', '밤', '에', '▁더욱', '놓을', '▁별', '빛', '</s>', '▁노래', '들', '</s>']\n",
      "여름밤을 수놓은 별빛같은 노래들</s>\n",
      "epoch no.5 train no.367010  loss = 3.82237 avg_loss = 2.89794\n",
      "epoch no.5 train no.367020  loss = 2.19393 avg_loss = 2.87114\n",
      "epoch no.5 train no.367030  loss = 2.34430 avg_loss = 2.86290\n",
      "epoch no.5 train no.367040  loss = 2.36420 avg_loss = 2.86970\n",
      "epoch no.5 train no.367050  loss = 3.67989 avg_loss = 2.89036\n",
      "epoch no.5 train no.367060  loss = 2.10793 avg_loss = 2.90969\n",
      "epoch no.5 train no.367070  loss = 1.86749 avg_loss = 2.89570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.367080  loss = 2.91052 avg_loss = 2.85989\n",
      "epoch no.5 train no.367090  loss = 4.12454 avg_loss = 2.89431\n",
      "epoch no.5 train no.367100  loss = 2.26215 avg_loss = 2.84662\n",
      "epoch no.5 train no.367110  loss = 2.04803 avg_loss = 2.83036\n",
      "epoch no.5 train no.367120  loss = 1.89387 avg_loss = 2.84738\n",
      "epoch no.5 train no.367130  loss = 2.49653 avg_loss = 2.87274\n",
      "epoch no.5 train no.367140  loss = 3.63038 avg_loss = 2.91217\n",
      "epoch no.5 train no.367150  loss = 1.92030 avg_loss = 2.93255\n",
      "epoch no.5 train no.367160  loss = 2.52447 avg_loss = 2.91580\n",
      "epoch no.5 train no.367170  loss = 2.96270 avg_loss = 2.89993\n",
      "epoch no.5 train no.367180  loss = 2.81626 avg_loss = 2.87537\n",
      "epoch no.5 train no.367190  loss = 3.33469 avg_loss = 2.89606\n",
      "epoch no.5 train no.367200  loss = 2.93888 avg_loss = 2.88340\n",
      "epoch no.5 train no.367210  loss = 2.22619 avg_loss = 2.85846\n",
      "epoch no.5 train no.367220  loss = 4.08313 avg_loss = 2.83405\n",
      "epoch no.5 train no.367230  loss = 2.36073 avg_loss = 2.84658\n",
      "epoch no.5 train no.367240  loss = 2.32870 avg_loss = 2.82628\n",
      "epoch no.5 train no.367250  loss = 3.97525 avg_loss = 2.82834\n",
      "epoch no.5 train no.367260  loss = 2.11544 avg_loss = 2.85787\n",
      "epoch no.5 train no.367270  loss = 2.68539 avg_loss = 2.89313\n",
      "epoch no.5 train no.367280  loss = 2.57792 avg_loss = 2.89385\n",
      "epoch no.5 train no.367290  loss = 3.06408 avg_loss = 2.87008\n",
      "epoch no.5 train no.367300  loss = 3.88397 avg_loss = 2.88772\n",
      "epoch no.5 train no.367310  loss = 3.59164 avg_loss = 2.91030\n",
      "epoch no.5 train no.367320  loss = 3.20884 avg_loss = 2.91748\n",
      "epoch no.5 train no.367330  loss = 2.86718 avg_loss = 2.93061\n",
      "epoch no.5 train no.367340  loss = 2.14182 avg_loss = 2.90774\n",
      "epoch no.5 train no.367350  loss = 2.93423 avg_loss = 2.91241\n",
      "epoch no.5 train no.367360  loss = 2.32667 avg_loss = 2.90890\n",
      "epoch no.5 train no.367370  loss = 2.84475 avg_loss = 2.94130\n",
      "epoch no.5 train no.367380  loss = 2.73015 avg_loss = 2.92927\n",
      "epoch no.5 train no.367390  loss = 3.23840 avg_loss = 2.91146\n",
      "epoch no.5 train no.367400  loss = 2.47229 avg_loss = 2.90769\n",
      "epoch no.5 train no.367410  loss = 3.44727 avg_loss = 2.89319\n",
      "epoch no.5 train no.367420  loss = 5.53741 avg_loss = 2.87968\n",
      "epoch no.5 train no.367430  loss = 2.86980 avg_loss = 2.86647\n",
      "epoch no.5 train no.367440  loss = 3.35517 avg_loss = 2.88186\n",
      "epoch no.5 train no.367450  loss = 3.53129 avg_loss = 2.89859\n",
      "epoch no.5 train no.367460  loss = 3.01066 avg_loss = 2.91012\n",
      "epoch no.5 train no.367470  loss = 2.23121 avg_loss = 2.94086\n",
      "epoch no.5 train no.367480  loss = 2.21128 avg_loss = 2.94105\n",
      "epoch no.5 train no.367490  loss = 3.35724 avg_loss = 2.95882\n",
      "epoch no.5 train no.367500  loss = 2.69182 avg_loss = 2.91383\n",
      "epoch no.5 train no.367510  loss = 3.49120 avg_loss = 2.92188\n",
      "epoch no.5 train no.367520  loss = 4.33249 avg_loss = 2.95368\n",
      "epoch no.5 train no.367530  loss = 2.42155 avg_loss = 2.97029\n",
      "epoch no.5 train no.367540  loss = 2.31605 avg_loss = 2.96428\n",
      "epoch no.5 train no.367550  loss = 2.77977 avg_loss = 2.91679\n",
      "epoch no.5 train no.367560  loss = 4.13091 avg_loss = 2.93293\n",
      "epoch no.5 train no.367570  loss = 1.80809 avg_loss = 2.92380\n",
      "epoch no.5 train no.367580  loss = 2.55784 avg_loss = 2.87935\n",
      "epoch no.5 train no.367590  loss = 4.06346 avg_loss = 2.89181\n",
      "epoch no.5 train no.367600  loss = 3.46123 avg_loss = 2.88873\n",
      "epoch no.5 train no.367610  loss = 4.25682 avg_loss = 2.91189\n",
      "epoch no.5 train no.367620  loss = 2.81356 avg_loss = 2.94762\n",
      "epoch no.5 train no.367630  loss = 3.45710 avg_loss = 2.91034\n",
      "epoch no.5 train no.367640  loss = 4.44391 avg_loss = 2.94935\n",
      "epoch no.5 train no.367650  loss = 4.12304 avg_loss = 2.95827\n",
      "epoch no.5 train no.367660  loss = 2.68468 avg_loss = 2.94650\n",
      "epoch no.5 train no.367670  loss = 3.36337 avg_loss = 2.92872\n",
      "epoch no.5 train no.367680  loss = 2.98918 avg_loss = 2.95953\n",
      "epoch no.5 train no.367690  loss = 3.25760 avg_loss = 2.98267\n",
      "epoch no.5 train no.367700  loss = 4.36173 avg_loss = 3.01775\n",
      "epoch no.5 train no.367710  loss = 2.07844 avg_loss = 3.01437\n",
      "epoch no.5 train no.367720  loss = 3.39323 avg_loss = 2.99803\n",
      "epoch no.5 train no.367730  loss = 4.17839 avg_loss = 2.97224\n",
      "epoch no.5 train no.367740  loss = 3.25566 avg_loss = 2.98735\n",
      "epoch no.5 train no.367750  loss = 3.11976 avg_loss = 2.98907\n",
      "epoch no.5 train no.367760  loss = 3.06307 avg_loss = 2.92988\n",
      "epoch no.5 train no.367770  loss = 1.94588 avg_loss = 2.95869\n",
      "epoch no.5 train no.367780  loss = 2.66226 avg_loss = 2.96014\n",
      "epoch no.5 train no.367790  loss = 2.69506 avg_loss = 2.97647\n",
      "epoch no.5 train no.367800  loss = 3.63415 avg_loss = 2.97974\n",
      "epoch no.5 train no.367810  loss = 2.63782 avg_loss = 2.98139\n",
      "epoch no.5 train no.367820  loss = 3.38027 avg_loss = 3.00027\n",
      "epoch no.5 train no.367830  loss = 1.89400 avg_loss = 2.96589\n",
      "epoch no.5 train no.367840  loss = 2.61927 avg_loss = 2.94501\n",
      "epoch no.5 train no.367850  loss = 2.59912 avg_loss = 2.98891\n",
      "epoch no.5 train no.367860  loss = 2.90610 avg_loss = 2.96985\n",
      "epoch no.5 train no.367870  loss = 1.67916 avg_loss = 2.98530\n",
      "epoch no.5 train no.367880  loss = 3.00286 avg_loss = 2.97088\n",
      "epoch no.5 train no.367890  loss = 3.35945 avg_loss = 2.98175\n",
      "epoch no.5 train no.367900  loss = 3.09786 avg_loss = 2.96312\n",
      "epoch no.5 train no.367910  loss = 1.81399 avg_loss = 2.93322\n",
      "epoch no.5 train no.367920  loss = 3.90055 avg_loss = 2.95027\n",
      "epoch no.5 train no.367930  loss = 3.70154 avg_loss = 2.94605\n",
      "epoch no.5 train no.367940  loss = 3.29268 avg_loss = 2.91725\n",
      "epoch no.5 train no.367950  loss = 3.25798 avg_loss = 2.92776\n",
      "epoch no.5 train no.367960  loss = 1.90869 avg_loss = 2.92066\n",
      "epoch no.5 train no.367970  loss = 3.66387 avg_loss = 2.89821\n",
      "epoch no.5 train no.367980  loss = 4.69264 avg_loss = 2.90340\n",
      "epoch no.5 train no.367990  loss = 2.87286 avg_loss = 2.92382\n",
      "epoch no.5 train no.368000  loss = 3.03567 avg_loss = 2.95282\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '을', '▁플레이', '맛', '▁같은', '▁노래', '</s>']\n",
      "여름밤의 꿀맛 같은 노래</s>\n",
      "epoch no.5 train no.368010  loss = 2.45215 avg_loss = 2.95967\n",
      "epoch no.5 train no.368020  loss = 3.49577 avg_loss = 2.95669\n",
      "epoch no.5 train no.368030  loss = 2.54347 avg_loss = 2.93956\n",
      "epoch no.5 train no.368040  loss = 2.51225 avg_loss = 2.95009\n",
      "epoch no.5 train no.368050  loss = 3.00604 avg_loss = 2.91446\n",
      "epoch no.5 train no.368060  loss = 1.92131 avg_loss = 2.89819\n",
      "epoch no.5 train no.368070  loss = 2.16516 avg_loss = 2.90494\n",
      "epoch no.5 train no.368080  loss = 3.39427 avg_loss = 2.89792\n",
      "epoch no.5 train no.368090  loss = 1.91129 avg_loss = 2.90477\n",
      "epoch no.5 train no.368100  loss = 2.52254 avg_loss = 2.87437\n",
      "epoch no.5 train no.368110  loss = 2.73702 avg_loss = 2.86471\n",
      "epoch no.5 train no.368120  loss = 3.03688 avg_loss = 2.89395\n",
      "epoch no.5 train no.368130  loss = 2.40220 avg_loss = 2.88703\n",
      "epoch no.5 train no.368140  loss = 3.77198 avg_loss = 2.90844\n",
      "epoch no.5 train no.368150  loss = 3.41668 avg_loss = 2.92380\n",
      "epoch no.5 train no.368160  loss = 1.95884 avg_loss = 2.86786\n",
      "epoch no.5 train no.368170  loss = 3.23565 avg_loss = 2.86872\n",
      "epoch no.5 train no.368180  loss = 3.35546 avg_loss = 2.86759\n",
      "epoch no.5 train no.368190  loss = 2.82879 avg_loss = 2.84719\n",
      "epoch no.5 train no.368200  loss = 3.11480 avg_loss = 2.86602\n",
      "epoch no.5 train no.368210  loss = 2.45602 avg_loss = 2.82169\n",
      "epoch no.5 train no.368220  loss = 2.32592 avg_loss = 2.79819\n",
      "epoch no.5 train no.368230  loss = 2.24161 avg_loss = 2.81864\n",
      "epoch no.5 train no.368240  loss = 2.84630 avg_loss = 2.83265\n",
      "epoch no.5 train no.368250  loss = 3.21967 avg_loss = 2.83415\n",
      "epoch no.5 train no.368260  loss = 3.61502 avg_loss = 2.87050\n",
      "epoch no.5 train no.368270  loss = 2.55623 avg_loss = 2.89047\n",
      "epoch no.5 train no.368280  loss = 2.13005 avg_loss = 2.88898\n",
      "epoch no.5 train no.368290  loss = 3.38619 avg_loss = 2.91912\n",
      "epoch no.5 train no.368300  loss = 3.25913 avg_loss = 2.90141\n",
      "epoch no.5 train no.368310  loss = 2.65472 avg_loss = 2.92598\n",
      "epoch no.5 train no.368320  loss = 3.12787 avg_loss = 2.91356\n",
      "epoch no.5 train no.368330  loss = 3.00854 avg_loss = 2.91301\n",
      "epoch no.5 train no.368340  loss = 2.49186 avg_loss = 2.93793\n",
      "epoch no.5 train no.368350  loss = 3.46001 avg_loss = 2.90763\n",
      "epoch no.5 train no.368360  loss = 4.74595 avg_loss = 2.93587\n",
      "epoch no.5 train no.368370  loss = 2.28302 avg_loss = 2.91902\n",
      "epoch no.5 train no.368380  loss = 2.28425 avg_loss = 2.92452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.368390  loss = 2.12240 avg_loss = 2.87470\n",
      "epoch no.5 train no.368400  loss = 3.62173 avg_loss = 2.89276\n",
      "epoch no.5 train no.368410  loss = 2.57306 avg_loss = 2.85072\n",
      "epoch no.5 train no.368420  loss = 3.64608 avg_loss = 2.88675\n",
      "epoch no.5 train no.368430  loss = 2.59368 avg_loss = 2.87372\n",
      "epoch no.5 train no.368440  loss = 3.80177 avg_loss = 2.87927\n",
      "epoch no.5 train no.368450  loss = 2.82746 avg_loss = 2.86690\n",
      "epoch no.5 train no.368460  loss = 3.78627 avg_loss = 2.91229\n",
      "epoch no.5 train no.368470  loss = 2.52557 avg_loss = 2.90227\n",
      "epoch no.5 train no.368480  loss = 2.22564 avg_loss = 2.92319\n",
      "epoch no.5 train no.368490  loss = 3.75000 avg_loss = 2.90825\n",
      "epoch no.5 train no.368500  loss = 3.53020 avg_loss = 2.94073\n",
      "epoch no.5 train no.368510  loss = 3.77241 avg_loss = 2.98234\n",
      "epoch no.5 train no.368520  loss = 3.31047 avg_loss = 2.97657\n",
      "epoch no.5 train no.368530  loss = 4.74253 avg_loss = 3.02066\n",
      "epoch no.5 train no.368540  loss = 3.65683 avg_loss = 3.02144\n",
      "epoch no.5 train no.368550  loss = 2.18551 avg_loss = 3.02594\n",
      "epoch no.5 train no.368560  loss = 3.47643 avg_loss = 3.05722\n",
      "epoch no.5 train no.368570  loss = 2.29937 avg_loss = 3.06548\n",
      "epoch no.5 train no.368580  loss = 2.24020 avg_loss = 3.02512\n",
      "epoch no.5 train no.368590  loss = 5.11554 avg_loss = 3.06277\n",
      "epoch no.5 train no.368600  loss = 2.69188 avg_loss = 3.01139\n",
      "epoch no.5 train no.368610  loss = 2.54923 avg_loss = 3.00200\n",
      "epoch no.5 train no.368620  loss = 2.24087 avg_loss = 2.98390\n",
      "epoch no.5 train no.368630  loss = 2.92172 avg_loss = 2.99850\n",
      "epoch no.5 train no.368640  loss = 2.54268 avg_loss = 2.97158\n",
      "epoch no.5 train no.368650  loss = 1.65320 avg_loss = 2.98050\n",
      "epoch no.5 train no.368660  loss = 2.19139 avg_loss = 2.99112\n",
      "epoch no.5 train no.368670  loss = 2.72815 avg_loss = 3.01914\n",
      "epoch no.5 train no.368680  loss = 2.79437 avg_loss = 3.01357\n",
      "epoch no.5 train no.368690  loss = 2.52164 avg_loss = 3.01393\n",
      "epoch no.5 train no.368700  loss = 2.58581 avg_loss = 2.97899\n",
      "epoch no.5 train no.368710  loss = 3.60529 avg_loss = 2.95686\n",
      "epoch no.5 train no.368720  loss = 3.84686 avg_loss = 2.97697\n",
      "epoch no.5 train no.368730  loss = 2.07741 avg_loss = 2.99670\n",
      "epoch no.5 train no.368740  loss = 3.41312 avg_loss = 3.00757\n",
      "epoch no.5 train no.368750  loss = 2.61318 avg_loss = 2.95018\n",
      "epoch no.5 train no.368760  loss = 3.20932 avg_loss = 2.93728\n",
      "epoch no.5 train no.368770  loss = 4.33548 avg_loss = 2.95874\n",
      "epoch no.5 train no.368780  loss = 2.59060 avg_loss = 2.95717\n",
      "epoch no.5 train no.368790  loss = 2.30137 avg_loss = 2.95311\n",
      "epoch no.5 train no.368800  loss = 2.50633 avg_loss = 2.92093\n",
      "epoch no.5 train no.368810  loss = 1.90667 avg_loss = 2.89669\n",
      "epoch no.5 train no.368820  loss = 2.38356 avg_loss = 2.89281\n",
      "epoch no.5 train no.368830  loss = 3.08541 avg_loss = 2.91148\n",
      "epoch no.5 train no.368840  loss = 2.69328 avg_loss = 2.90073\n",
      "epoch no.5 train no.368850  loss = 3.26078 avg_loss = 2.90911\n",
      "epoch no.5 train no.368860  loss = 4.17185 avg_loss = 2.91744\n",
      "epoch no.5 train no.368870  loss = 2.79785 avg_loss = 2.93908\n",
      "epoch no.5 train no.368880  loss = 4.17516 avg_loss = 2.94132\n",
      "epoch no.5 train no.368890  loss = 2.78660 avg_loss = 2.97203\n",
      "epoch no.5 train no.368900  loss = 1.95506 avg_loss = 2.96929\n",
      "epoch no.5 train no.368910  loss = 2.16607 avg_loss = 2.91258\n",
      "epoch no.5 train no.368920  loss = 2.77443 avg_loss = 2.90794\n",
      "epoch no.5 train no.368930  loss = 3.27123 avg_loss = 2.92217\n",
      "epoch no.5 train no.368940  loss = 3.29342 avg_loss = 2.90400\n",
      "epoch no.5 train no.368950  loss = 2.39447 avg_loss = 2.93119\n",
      "epoch no.5 train no.368960  loss = 2.74351 avg_loss = 2.92442\n",
      "epoch no.5 train no.368970  loss = 3.31270 avg_loss = 2.93394\n",
      "epoch no.5 train no.368980  loss = 2.54766 avg_loss = 2.94823\n",
      "epoch no.5 train no.368990  loss = 2.45366 avg_loss = 2.94349\n",
      "epoch no.5 train no.369000  loss = 2.39301 avg_loss = 2.93558\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '의', '▁감성', '▁감성', '</s>', 'op', '</s>']\n",
      "여름밤의 트렌디 감성 pop</s>\n",
      "epoch no.5 train no.369010  loss = 2.72902 avg_loss = 2.92977\n",
      "epoch no.5 train no.369020  loss = 2.57257 avg_loss = 2.91484\n",
      "epoch no.5 train no.369030  loss = 3.53381 avg_loss = 2.91670\n",
      "epoch no.5 train no.369040  loss = 3.40255 avg_loss = 2.91860\n",
      "epoch no.5 train no.369050  loss = 1.80685 avg_loss = 2.88269\n",
      "epoch no.5 train no.369060  loss = 3.56486 avg_loss = 2.87805\n",
      "epoch no.5 train no.369070  loss = 2.29331 avg_loss = 2.84688\n",
      "epoch no.5 train no.369080  loss = 2.87345 avg_loss = 2.85583\n",
      "epoch no.5 train no.369090  loss = 3.21390 avg_loss = 2.87325\n",
      "epoch no.5 train no.369100  loss = 3.38460 avg_loss = 2.87382\n",
      "epoch no.5 train no.369110  loss = 3.74621 avg_loss = 2.88677\n",
      "epoch no.5 train no.369120  loss = 2.97930 avg_loss = 2.89460\n",
      "epoch no.5 train no.369130  loss = 2.87188 avg_loss = 2.89568\n",
      "epoch no.5 train no.369140  loss = 2.42567 avg_loss = 2.88102\n",
      "epoch no.5 train no.369150  loss = 2.22657 avg_loss = 2.88731\n",
      "epoch no.5 train no.369160  loss = 2.74124 avg_loss = 2.91808\n",
      "epoch no.5 train no.369170  loss = 2.94594 avg_loss = 2.89764\n",
      "epoch no.5 train no.369180  loss = 1.92282 avg_loss = 2.88034\n",
      "epoch no.5 train no.369190  loss = 3.94333 avg_loss = 2.91324\n",
      "epoch no.5 train no.369200  loss = 1.70667 avg_loss = 2.84177\n",
      "epoch no.5 train no.369210  loss = 2.52862 avg_loss = 2.85912\n",
      "epoch no.5 train no.369220  loss = 2.33788 avg_loss = 2.84443\n",
      "epoch no.5 train no.369230  loss = 1.59689 avg_loss = 2.87657\n",
      "epoch no.5 train no.369240  loss = 2.23086 avg_loss = 2.88892\n",
      "epoch no.5 train no.369250  loss = 2.02370 avg_loss = 2.90782\n",
      "epoch no.5 train no.369260  loss = 2.53389 avg_loss = 2.87991\n",
      "epoch no.5 train no.369270  loss = 3.28968 avg_loss = 2.83088\n",
      "epoch no.5 train no.369280  loss = 3.26447 avg_loss = 2.84802\n",
      "epoch no.5 train no.369290  loss = 2.39798 avg_loss = 2.85043\n",
      "epoch no.5 train no.369300  loss = 2.59055 avg_loss = 2.84990\n",
      "epoch no.5 train no.369310  loss = 3.10762 avg_loss = 2.87166\n",
      "epoch no.5 train no.369320  loss = 2.56094 avg_loss = 2.90415\n",
      "epoch no.5 train no.369330  loss = 2.92765 avg_loss = 2.91949\n",
      "epoch no.5 train no.369340  loss = 2.88320 avg_loss = 2.92485\n",
      "epoch no.5 train no.369350  loss = 3.69438 avg_loss = 2.91373\n",
      "epoch no.5 train no.369360  loss = 2.88856 avg_loss = 2.90363\n",
      "epoch no.5 train no.369370  loss = 2.66562 avg_loss = 2.90454\n",
      "epoch no.5 train no.369380  loss = 2.83996 avg_loss = 2.90987\n",
      "epoch no.5 train no.369390  loss = 2.25513 avg_loss = 2.90842\n",
      "epoch no.5 train no.369400  loss = 2.55395 avg_loss = 2.89040\n",
      "epoch no.5 train no.369410  loss = 3.54556 avg_loss = 2.90259\n",
      "epoch no.5 train no.369420  loss = 2.74116 avg_loss = 2.86201\n",
      "epoch no.5 train no.369430  loss = 2.78596 avg_loss = 2.82405\n",
      "epoch no.5 train no.369440  loss = 3.47129 avg_loss = 2.82430\n",
      "epoch no.5 train no.369450  loss = 4.75811 avg_loss = 2.90818\n",
      "epoch no.5 train no.369460  loss = 2.21127 avg_loss = 2.90959\n",
      "epoch no.5 train no.369470  loss = 3.50820 avg_loss = 2.90768\n",
      "epoch no.5 train no.369480  loss = 2.74449 avg_loss = 2.84877\n",
      "epoch no.5 train no.369490  loss = 3.47558 avg_loss = 2.84873\n",
      "epoch no.5 train no.369500  loss = 3.05244 avg_loss = 2.89139\n",
      "epoch no.5 train no.369510  loss = 2.72865 avg_loss = 2.91045\n",
      "epoch no.5 train no.369520  loss = 4.72943 avg_loss = 2.89181\n",
      "epoch no.5 train no.369530  loss = 2.77341 avg_loss = 2.86357\n",
      "epoch no.5 train no.369540  loss = 3.23729 avg_loss = 2.86945\n",
      "epoch no.5 train no.369550  loss = 1.49070 avg_loss = 2.88087\n",
      "epoch no.5 train no.369560  loss = 2.35157 avg_loss = 2.88118\n",
      "epoch no.5 train no.369570  loss = 1.63373 avg_loss = 2.92106\n",
      "epoch no.5 train no.369580  loss = 3.02930 avg_loss = 2.88130\n",
      "epoch no.5 train no.369590  loss = 2.24027 avg_loss = 2.85621\n",
      "epoch no.5 train no.369600  loss = 2.19792 avg_loss = 2.86785\n",
      "epoch no.5 train no.369610  loss = 3.36577 avg_loss = 2.88080\n",
      "epoch no.5 train no.369620  loss = 3.17863 avg_loss = 2.87320\n",
      "epoch no.5 train no.369630  loss = 3.33159 avg_loss = 2.88744\n",
      "epoch no.5 train no.369640  loss = 3.52070 avg_loss = 2.89809\n",
      "epoch no.5 train no.369650  loss = 4.02762 avg_loss = 2.92342\n",
      "epoch no.5 train no.369660  loss = 2.15821 avg_loss = 2.96622\n",
      "epoch no.5 train no.369670  loss = 2.92819 avg_loss = 2.97437\n",
      "epoch no.5 train no.369680  loss = 3.98828 avg_loss = 2.96838\n",
      "epoch no.5 train no.369690  loss = 2.74573 avg_loss = 2.95012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.369700  loss = 3.17533 avg_loss = 2.96172\n",
      "epoch no.5 train no.369710  loss = 3.23910 avg_loss = 2.94609\n",
      "epoch no.5 train no.369720  loss = 3.01179 avg_loss = 2.95898\n",
      "epoch no.5 train no.369730  loss = 2.30830 avg_loss = 2.98534\n",
      "epoch no.5 train no.369740  loss = 2.62255 avg_loss = 3.02735\n",
      "epoch no.5 train no.369750  loss = 3.31204 avg_loss = 3.00328\n",
      "epoch no.5 train no.369760  loss = 5.69739 avg_loss = 3.01654\n",
      "epoch no.5 train no.369770  loss = 2.59414 avg_loss = 2.99487\n",
      "epoch no.5 train no.369780  loss = 1.80820 avg_loss = 2.97248\n",
      "epoch no.5 train no.369790  loss = 3.36516 avg_loss = 2.94756\n",
      "epoch no.5 train no.369800  loss = 2.36541 avg_loss = 2.91383\n",
      "epoch no.5 train no.369810  loss = 2.14937 avg_loss = 2.91051\n",
      "epoch no.5 train no.369820  loss = 2.73858 avg_loss = 2.93755\n",
      "epoch no.5 train no.369830  loss = 2.94344 avg_loss = 2.91813\n",
      "epoch no.5 train no.369840  loss = 3.22576 avg_loss = 2.95417\n",
      "epoch no.5 train no.369850  loss = 3.66490 avg_loss = 2.95613\n",
      "epoch no.5 train no.369860  loss = 0.97445 avg_loss = 2.93287\n",
      "epoch no.5 train no.369870  loss = 4.23163 avg_loss = 2.95841\n",
      "epoch no.5 train no.369880  loss = 2.81072 avg_loss = 2.93480\n",
      "epoch no.5 train no.369890  loss = 2.37483 avg_loss = 2.88618\n",
      "epoch no.5 train no.369900  loss = 2.40789 avg_loss = 2.91824\n",
      "epoch no.5 train no.369910  loss = 2.40303 avg_loss = 2.93811\n",
      "epoch no.5 train no.369920  loss = 3.74417 avg_loss = 2.94593\n",
      "epoch no.5 train no.369930  loss = 2.82960 avg_loss = 2.92810\n",
      "epoch no.5 train no.369940  loss = 4.05403 avg_loss = 2.95852\n",
      "epoch no.5 train no.369950  loss = 4.00283 avg_loss = 2.97683\n",
      "epoch no.5 train no.369960  loss = 2.24688 avg_loss = 2.95687\n",
      "epoch no.5 train no.369970  loss = 2.38163 avg_loss = 2.94669\n",
      "epoch no.5 train no.369980  loss = 3.69986 avg_loss = 2.90789\n",
      "epoch no.5 train no.369990  loss = 2.04368 avg_loss = 2.89936\n",
      "epoch no.5 train no.370000  loss = 4.07317 avg_loss = 2.92295\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '에서', '▁바라보며', '닐', '면서', '▁듣고']\n",
      "여름밤 한강을 거닐면서</s>\n",
      "epoch no.5 train no.370010  loss = 2.65961 avg_loss = 2.91120\n",
      "epoch no.5 train no.370020  loss = 2.75897 avg_loss = 2.91110\n",
      "epoch no.5 train no.370030  loss = 2.03185 avg_loss = 2.90617\n",
      "epoch no.5 train no.370040  loss = 3.04120 avg_loss = 2.93144\n",
      "epoch no.5 train no.370050  loss = 3.77206 avg_loss = 2.93018\n",
      "epoch no.5 train no.370060  loss = 2.44892 avg_loss = 2.95304\n",
      "epoch no.5 train no.370070  loss = 2.01182 avg_loss = 2.96459\n",
      "epoch no.5 train no.370080  loss = 2.45345 avg_loss = 3.01153\n",
      "epoch no.5 train no.370090  loss = 2.35549 avg_loss = 2.97895\n",
      "epoch no.5 train no.370100  loss = 2.82114 avg_loss = 2.97165\n",
      "epoch no.5 train no.370110  loss = 2.68480 avg_loss = 2.94946\n",
      "epoch no.5 train no.370120  loss = 3.24353 avg_loss = 2.92446\n",
      "epoch no.5 train no.370130  loss = 2.42862 avg_loss = 2.91534\n",
      "epoch no.5 train no.370140  loss = 3.21202 avg_loss = 2.90862\n",
      "epoch no.5 train no.370150  loss = 2.07210 avg_loss = 2.90718\n",
      "epoch no.5 train no.370160  loss = 2.72966 avg_loss = 2.87202\n",
      "epoch no.5 train no.370170  loss = 1.72887 avg_loss = 2.91521\n",
      "epoch no.5 train no.370180  loss = 3.85785 avg_loss = 2.93869\n",
      "epoch no.5 train no.370190  loss = 3.93717 avg_loss = 2.91647\n",
      "epoch no.5 train no.370200  loss = 3.86826 avg_loss = 2.91633\n",
      "epoch no.5 train no.370210  loss = 5.63653 avg_loss = 2.93616\n",
      "epoch no.5 train no.370220  loss = 2.81146 avg_loss = 2.94587\n",
      "epoch no.5 train no.370230  loss = 2.80528 avg_loss = 2.99848\n",
      "epoch no.5 train no.370240  loss = 3.21568 avg_loss = 2.99950\n",
      "epoch no.5 train no.370250  loss = 2.62518 avg_loss = 2.96047\n",
      "epoch no.5 train no.370260  loss = 2.25890 avg_loss = 2.95683\n",
      "epoch no.5 train no.370270  loss = 2.50069 avg_loss = 2.98443\n",
      "epoch no.5 train no.370280  loss = 3.59192 avg_loss = 2.96999\n",
      "epoch no.5 train no.370290  loss = 2.62138 avg_loss = 2.97889\n",
      "epoch no.5 train no.370300  loss = 1.54857 avg_loss = 2.95121\n",
      "epoch no.5 train no.370310  loss = 1.72375 avg_loss = 2.92588\n",
      "epoch no.5 train no.370320  loss = 3.41913 avg_loss = 2.89608\n",
      "epoch no.5 train no.370330  loss = 1.80026 avg_loss = 2.86917\n",
      "epoch no.5 train no.370340  loss = 3.15962 avg_loss = 2.90388\n",
      "epoch no.5 train no.370350  loss = 2.69074 avg_loss = 2.93365\n",
      "epoch no.5 train no.370360  loss = 2.24029 avg_loss = 2.91130\n",
      "epoch no.5 train no.370370  loss = 2.39751 avg_loss = 2.92763\n",
      "epoch no.5 train no.370380  loss = 2.09177 avg_loss = 2.90118\n",
      "epoch no.5 train no.370390  loss = 3.12943 avg_loss = 2.91902\n",
      "epoch no.5 train no.370400  loss = 4.59598 avg_loss = 2.95726\n",
      "epoch no.5 train no.370410  loss = 4.57554 avg_loss = 2.96584\n",
      "epoch no.5 train no.370420  loss = 2.86248 avg_loss = 2.95037\n",
      "epoch no.5 train no.370430  loss = 2.80723 avg_loss = 2.95744\n",
      "epoch no.5 train no.370440  loss = 2.44737 avg_loss = 2.92624\n",
      "epoch no.5 train no.370450  loss = 2.53110 avg_loss = 2.92183\n",
      "epoch no.5 train no.370460  loss = 2.77567 avg_loss = 2.94187\n",
      "epoch no.5 train no.370470  loss = 4.14020 avg_loss = 2.94945\n",
      "epoch no.5 train no.370480  loss = 3.29332 avg_loss = 2.97284\n",
      "epoch no.5 train no.370490  loss = 2.20953 avg_loss = 2.95197\n",
      "epoch no.5 train no.370500  loss = 3.28278 avg_loss = 2.96904\n",
      "epoch no.5 train no.370510  loss = 2.54094 avg_loss = 2.98400\n",
      "epoch no.5 train no.370520  loss = 3.98412 avg_loss = 2.97627\n",
      "epoch no.5 train no.370530  loss = 5.06402 avg_loss = 2.97337\n",
      "epoch no.5 train no.370540  loss = 2.11109 avg_loss = 3.02757\n",
      "epoch no.5 train no.370550  loss = 3.29194 avg_loss = 2.99236\n",
      "epoch no.5 train no.370560  loss = 2.93228 avg_loss = 2.96817\n",
      "epoch no.5 train no.370570  loss = 2.90552 avg_loss = 2.93760\n",
      "epoch no.5 train no.370580  loss = 5.30571 avg_loss = 2.96464\n",
      "epoch no.5 train no.370590  loss = 2.31372 avg_loss = 2.95791\n",
      "epoch no.5 train no.370600  loss = 3.52659 avg_loss = 2.98618\n",
      "epoch no.5 train no.370610  loss = 3.01850 avg_loss = 3.01213\n",
      "epoch no.5 train no.370620  loss = 2.58823 avg_loss = 2.99825\n",
      "epoch no.5 train no.370630  loss = 2.03683 avg_loss = 3.00481\n",
      "epoch no.5 train no.370640  loss = 3.92734 avg_loss = 3.00553\n",
      "epoch no.5 train no.370650  loss = 2.16887 avg_loss = 3.02349\n",
      "epoch no.5 train no.370660  loss = 3.54781 avg_loss = 3.06063\n",
      "epoch no.5 train no.370670  loss = 2.48749 avg_loss = 3.04876\n",
      "epoch no.5 train no.370680  loss = 3.76497 avg_loss = 3.05401\n",
      "epoch no.5 train no.370690  loss = 2.34695 avg_loss = 3.06480\n",
      "epoch no.5 train no.370700  loss = 2.05455 avg_loss = 3.04630\n",
      "epoch no.5 train no.370710  loss = 3.65075 avg_loss = 3.02814\n",
      "epoch no.5 train no.370720  loss = 3.24389 avg_loss = 3.03999\n",
      "epoch no.5 train no.370730  loss = 4.89660 avg_loss = 3.07296\n",
      "epoch no.5 train no.370740  loss = 2.40141 avg_loss = 3.07431\n",
      "epoch no.5 train no.370750  loss = 3.02814 avg_loss = 3.04829\n",
      "epoch no.5 train no.370760  loss = 2.09002 avg_loss = 3.04940\n",
      "epoch no.5 train no.370770  loss = 2.51578 avg_loss = 3.02658\n",
      "epoch no.5 train no.370780  loss = 3.10467 avg_loss = 3.00520\n",
      "epoch no.5 train no.370790  loss = 2.25536 avg_loss = 2.97290\n",
      "epoch no.5 train no.370800  loss = 3.65321 avg_loss = 2.98613\n",
      "epoch no.5 train no.370810  loss = 3.48024 avg_loss = 2.99088\n",
      "epoch no.5 train no.370820  loss = 3.13394 avg_loss = 2.99257\n",
      "epoch no.5 train no.370830  loss = 3.22070 avg_loss = 2.99990\n",
      "epoch no.5 train no.370840  loss = 2.31387 avg_loss = 3.02305\n",
      "epoch no.5 train no.370850  loss = 2.35214 avg_loss = 2.98790\n",
      "epoch no.5 train no.370860  loss = 2.96661 avg_loss = 2.98727\n",
      "epoch no.5 train no.370870  loss = 2.53353 avg_loss = 2.99345\n",
      "epoch no.5 train no.370880  loss = 3.16079 avg_loss = 2.95141\n",
      "epoch no.5 train no.370890  loss = 3.63574 avg_loss = 2.95194\n",
      "epoch no.5 train no.370900  loss = 3.06993 avg_loss = 2.93660\n",
      "epoch no.5 train no.370910  loss = 4.90122 avg_loss = 2.93371\n",
      "epoch no.5 train no.370920  loss = 2.14985 avg_loss = 2.92299\n",
      "epoch no.5 train no.370930  loss = 3.55854 avg_loss = 2.94678\n",
      "epoch no.5 train no.370940  loss = 2.52396 avg_loss = 2.93325\n",
      "epoch no.5 train no.370950  loss = 3.42583 avg_loss = 3.00298\n",
      "epoch no.5 train no.370960  loss = 2.77649 avg_loss = 3.04264\n",
      "epoch no.5 train no.370970  loss = 2.93808 avg_loss = 3.07080\n",
      "epoch no.5 train no.370980  loss = 2.53550 avg_loss = 3.11100\n",
      "epoch no.5 train no.370990  loss = 2.12704 avg_loss = 3.07862\n",
      "epoch no.5 train no.371000  loss = 2.61585 avg_loss = 3.07171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "to_tokens: ['▁비', '밤', '▁신나는', '▁노래', '지', '</s>']\n",
      "여름엔 이 노래지</s>\n",
      "epoch no.5 train no.371010  loss = 2.86842 avg_loss = 3.03024\n",
      "epoch no.5 train no.371020  loss = 2.30951 avg_loss = 3.00261\n",
      "epoch no.5 train no.371030  loss = 4.14939 avg_loss = 2.99418\n",
      "epoch no.5 train no.371040  loss = 2.70507 avg_loss = 2.99768\n",
      "epoch no.5 train no.371050  loss = 2.25180 avg_loss = 2.99765\n",
      "epoch no.5 train no.371060  loss = 5.25833 avg_loss = 3.00670\n",
      "epoch no.5 train no.371070  loss = 3.35626 avg_loss = 2.99745\n",
      "epoch no.5 train no.371080  loss = 3.95419 avg_loss = 3.00183\n",
      "epoch no.5 train no.371090  loss = 2.80850 avg_loss = 3.00885\n",
      "epoch no.5 train no.371100  loss = 3.45073 avg_loss = 3.03150\n",
      "epoch no.5 train no.371110  loss = 2.15039 avg_loss = 3.00081\n",
      "epoch no.5 train no.371120  loss = 3.95594 avg_loss = 2.99629\n",
      "epoch no.5 train no.371130  loss = 4.30104 avg_loss = 3.00869\n",
      "epoch no.5 train no.371140  loss = 3.94698 avg_loss = 3.01453\n",
      "epoch no.5 train no.371150  loss = 3.83659 avg_loss = 3.04073\n",
      "epoch no.5 train no.371160  loss = 1.88572 avg_loss = 3.05753\n",
      "epoch no.5 train no.371170  loss = 4.93549 avg_loss = 3.04350\n",
      "epoch no.5 train no.371180  loss = 2.20835 avg_loss = 3.00318\n",
      "epoch no.5 train no.371190  loss = 2.09748 avg_loss = 2.96519\n",
      "epoch no.5 train no.371200  loss = 5.29799 avg_loss = 2.98637\n",
      "epoch no.5 train no.371210  loss = 2.60160 avg_loss = 3.01145\n",
      "epoch no.5 train no.371220  loss = 2.29023 avg_loss = 2.99340\n",
      "epoch no.5 train no.371230  loss = 4.10483 avg_loss = 3.00430\n",
      "epoch no.5 train no.371240  loss = 3.21734 avg_loss = 3.02994\n",
      "epoch no.5 train no.371250  loss = 3.25151 avg_loss = 3.01661\n",
      "epoch no.5 train no.371260  loss = 2.25074 avg_loss = 3.00203\n",
      "epoch no.5 train no.371270  loss = 2.89259 avg_loss = 3.01739\n",
      "epoch no.5 train no.371280  loss = 5.21238 avg_loss = 3.03494\n",
      "epoch no.5 train no.371290  loss = 4.58750 avg_loss = 3.03922\n",
      "epoch no.5 train no.371300  loss = 1.95052 avg_loss = 3.00751\n",
      "epoch no.5 train no.371310  loss = 2.48264 avg_loss = 3.03717\n",
      "epoch no.5 train no.371320  loss = 3.25478 avg_loss = 3.04770\n",
      "epoch no.5 train no.371330  loss = 2.35639 avg_loss = 3.04711\n",
      "epoch no.5 train no.371340  loss = 5.17717 avg_loss = 3.08770\n",
      "epoch no.5 train no.371350  loss = 3.38636 avg_loss = 3.06894\n",
      "epoch no.5 train no.371360  loss = 3.43780 avg_loss = 3.04710\n",
      "epoch no.5 train no.371370  loss = 3.58153 avg_loss = 3.04384\n",
      "epoch no.5 train no.371380  loss = 3.04123 avg_loss = 3.05008\n",
      "epoch no.5 train no.371390  loss = 4.18389 avg_loss = 3.04181\n",
      "epoch no.5 train no.371400  loss = 5.25986 avg_loss = 3.03848\n",
      "epoch no.5 train no.371410  loss = 1.78521 avg_loss = 3.02534\n",
      "epoch no.5 train no.371420  loss = 2.10474 avg_loss = 3.03018\n",
      "epoch no.5 train no.371430  loss = 3.60529 avg_loss = 3.03740\n",
      "epoch no.5 train no.371440  loss = 3.21178 avg_loss = 3.04385\n",
      "epoch no.5 train no.371450  loss = 1.83908 avg_loss = 3.02580\n",
      "epoch no.5 train no.371460  loss = 3.66232 avg_loss = 3.01316\n",
      "epoch no.5 train no.371470  loss = 4.02525 avg_loss = 3.00718\n",
      "epoch no.5 train no.371480  loss = 5.13884 avg_loss = 3.02227\n",
      "epoch no.5 train no.371490  loss = 2.20494 avg_loss = 3.00183\n",
      "epoch no.5 train no.371500  loss = 3.61639 avg_loss = 2.99867\n",
      "epoch no.5 train no.371510  loss = 2.91570 avg_loss = 3.05189\n",
      "epoch no.5 train no.371520  loss = 2.39171 avg_loss = 3.02268\n",
      "epoch no.5 train no.371530  loss = 4.14631 avg_loss = 3.01016\n",
      "epoch no.5 train no.371540  loss = 3.29167 avg_loss = 2.97739\n",
      "epoch no.5 train no.371550  loss = 2.15165 avg_loss = 2.97419\n",
      "epoch no.5 train no.371560  loss = 2.50807 avg_loss = 2.94230\n",
      "epoch no.5 train no.371570  loss = 5.56236 avg_loss = 3.02052\n",
      "epoch no.5 train no.371580  loss = 3.15492 avg_loss = 3.01733\n",
      "epoch no.5 train no.371590  loss = 4.08118 avg_loss = 3.00937\n",
      "epoch no.5 train no.371600  loss = 3.53470 avg_loss = 3.00236\n",
      "epoch no.5 train no.371610  loss = 2.24423 avg_loss = 2.96031\n",
      "epoch no.5 train no.371620  loss = 3.00509 avg_loss = 2.95880\n",
      "epoch no.5 train no.371630  loss = 3.47017 avg_loss = 2.98650\n",
      "epoch no.5 train no.371640  loss = 3.07989 avg_loss = 2.98212\n",
      "epoch no.5 train no.371650  loss = 3.64773 avg_loss = 2.96091\n",
      "epoch no.5 train no.371660  loss = 2.46219 avg_loss = 2.94088\n",
      "epoch no.5 train no.371670  loss = 4.71044 avg_loss = 2.94798\n",
      "epoch no.5 train no.371680  loss = 2.30096 avg_loss = 2.94153\n",
      "epoch no.5 train no.371690  loss = 5.14633 avg_loss = 2.96046\n",
      "epoch no.5 train no.371700  loss = 4.55606 avg_loss = 2.97313\n",
      "epoch no.5 train no.371710  loss = 2.69360 avg_loss = 2.98867\n",
      "epoch no.5 train no.371720  loss = 3.40864 avg_loss = 3.02287\n",
      "epoch no.5 train no.371730  loss = 3.86919 avg_loss = 3.06256\n",
      "epoch no.5 train no.371740  loss = 2.09179 avg_loss = 3.03525\n",
      "epoch no.5 train no.371750  loss = 3.20893 avg_loss = 3.02327\n",
      "epoch no.5 train no.371760  loss = 2.96142 avg_loss = 3.03473\n",
      "epoch no.5 train no.371770  loss = 2.83209 avg_loss = 3.01500\n",
      "epoch no.5 train no.371780  loss = 3.26685 avg_loss = 3.01244\n",
      "epoch no.5 train no.371790  loss = 2.95452 avg_loss = 3.00081\n",
      "epoch no.5 train no.371800  loss = 2.03752 avg_loss = 2.97381\n",
      "epoch no.5 train no.371810  loss = 2.74365 avg_loss = 2.98855\n",
      "epoch no.5 train no.371820  loss = 2.21410 avg_loss = 3.00271\n",
      "epoch no.5 train no.371830  loss = 1.41635 avg_loss = 2.95679\n",
      "epoch no.5 train no.371840  loss = 2.56610 avg_loss = 2.99263\n",
      "epoch no.5 train no.371850  loss = 2.85620 avg_loss = 2.96259\n",
      "epoch no.5 train no.371860  loss = 2.58338 avg_loss = 2.98118\n",
      "epoch no.5 train no.371870  loss = 3.75428 avg_loss = 2.97216\n",
      "epoch no.5 train no.371880  loss = 3.69375 avg_loss = 2.96683\n",
      "epoch no.5 train no.371890  loss = 2.64381 avg_loss = 2.91510\n",
      "epoch no.5 train no.371900  loss = 4.34933 avg_loss = 2.94602\n",
      "epoch no.5 train no.371910  loss = 3.05918 avg_loss = 2.95756\n",
      "epoch no.5 train no.371920  loss = 2.04326 avg_loss = 2.93960\n",
      "epoch no.5 train no.371930  loss = 4.83979 avg_loss = 2.93261\n",
      "epoch no.5 train no.371940  loss = 3.63956 avg_loss = 2.94140\n",
      "epoch no.5 train no.371950  loss = 2.70811 avg_loss = 2.89694\n",
      "epoch no.5 train no.371960  loss = 2.66470 avg_loss = 2.91437\n",
      "epoch no.5 train no.371970  loss = 3.07436 avg_loss = 2.93956\n",
      "epoch no.5 train no.371980  loss = 1.72943 avg_loss = 2.90910\n",
      "epoch no.5 train no.371990  loss = 2.76995 avg_loss = 2.90862\n",
      "epoch no.5 train no.372000  loss = 2.41491 avg_loss = 2.94943\n",
      "7\n",
      "to_tokens: ['▁비', '엔', '을', '▁수', '놓을', '▁청량', '한', '▁노래', '</s>']\n",
      "여름밤을 수놓을 청량한 노래</s>\n",
      "epoch no.5 train no.372010  loss = 2.73790 avg_loss = 2.91148\n",
      "epoch no.5 train no.372020  loss = 2.25848 avg_loss = 2.88292\n",
      "epoch no.5 train no.372030  loss = 2.12784 avg_loss = 2.91373\n",
      "epoch no.5 train no.372040  loss = 2.46312 avg_loss = 2.90276\n",
      "epoch no.5 train no.372050  loss = 2.26484 avg_loss = 2.91176\n",
      "epoch no.5 train no.372060  loss = 4.21271 avg_loss = 2.91919\n",
      "epoch no.5 train no.372070  loss = 4.34815 avg_loss = 2.95618\n",
      "epoch no.5 train no.372080  loss = 3.95567 avg_loss = 2.97667\n",
      "epoch no.5 train no.372090  loss = 3.38613 avg_loss = 2.97837\n",
      "epoch no.5 train no.372100  loss = 2.35510 avg_loss = 2.98534\n",
      "epoch no.5 train no.372110  loss = 4.11387 avg_loss = 2.97887\n",
      "epoch no.5 train no.372120  loss = 5.78124 avg_loss = 2.98149\n",
      "epoch no.5 train no.372130  loss = 2.07007 avg_loss = 2.93578\n",
      "epoch no.5 train no.372140  loss = 3.58259 avg_loss = 2.92556\n",
      "epoch no.5 train no.372150  loss = 2.17675 avg_loss = 2.91342\n",
      "epoch no.5 train no.372160  loss = 2.60193 avg_loss = 2.91739\n",
      "epoch no.5 train no.372170  loss = 2.68188 avg_loss = 2.89741\n",
      "epoch no.5 train no.372180  loss = 3.32188 avg_loss = 2.87108\n",
      "epoch no.5 train no.372190  loss = 3.13422 avg_loss = 2.91054\n",
      "epoch no.5 train no.372200  loss = 2.17880 avg_loss = 2.92459\n",
      "epoch no.5 train no.372210  loss = 3.56791 avg_loss = 2.93523\n",
      "epoch no.5 train no.372220  loss = 4.81728 avg_loss = 2.95365\n",
      "epoch no.5 train no.372230  loss = 5.12700 avg_loss = 2.96576\n",
      "epoch no.5 train no.372240  loss = 1.90011 avg_loss = 2.96102\n",
      "epoch no.5 train no.372250  loss = 3.71384 avg_loss = 2.96662\n",
      "epoch no.5 train no.372260  loss = 3.89372 avg_loss = 2.97756\n",
      "epoch no.5 train no.372270  loss = 1.95650 avg_loss = 3.00731\n",
      "epoch no.5 train no.372280  loss = 2.33462 avg_loss = 3.00797\n",
      "epoch no.5 train no.372290  loss = 4.42213 avg_loss = 3.05127\n",
      "epoch no.5 train no.372300  loss = 2.67342 avg_loss = 3.02522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.372310  loss = 2.74480 avg_loss = 2.99546\n",
      "epoch no.5 train no.372320  loss = 2.02090 avg_loss = 2.96473\n",
      "epoch no.5 train no.372330  loss = 4.20591 avg_loss = 2.94220\n",
      "epoch no.5 train no.372340  loss = 2.64161 avg_loss = 2.94377\n",
      "epoch no.5 train no.372350  loss = 1.60471 avg_loss = 2.90913\n",
      "epoch no.5 train no.372360  loss = 2.21025 avg_loss = 2.90865\n",
      "epoch no.5 train no.372370  loss = 3.50828 avg_loss = 2.89638\n",
      "epoch no.5 train no.372380  loss = 4.30591 avg_loss = 2.92927\n",
      "epoch no.5 train no.372390  loss = 2.32679 avg_loss = 2.92863\n",
      "epoch no.5 train no.372400  loss = 2.95419 avg_loss = 2.93845\n",
      "epoch no.5 train no.372410  loss = 3.01314 avg_loss = 2.95881\n",
      "epoch no.5 train no.372420  loss = 4.93270 avg_loss = 2.96688\n",
      "epoch no.5 train no.372430  loss = 3.12875 avg_loss = 2.97038\n",
      "epoch no.5 train no.372440  loss = 3.15392 avg_loss = 3.01519\n",
      "epoch no.5 train no.372450  loss = 2.49443 avg_loss = 3.04181\n",
      "epoch no.5 train no.372460  loss = 2.68008 avg_loss = 3.00905\n",
      "epoch no.5 train no.372470  loss = 2.07549 avg_loss = 2.97450\n",
      "epoch no.5 train no.372480  loss = 2.08888 avg_loss = 2.99486\n",
      "epoch no.5 train no.372490  loss = 3.22250 avg_loss = 2.97084\n",
      "epoch no.5 train no.372500  loss = 2.87538 avg_loss = 2.95884\n",
      "epoch no.5 train no.372510  loss = 3.00137 avg_loss = 2.98422\n",
      "epoch no.5 train no.372520  loss = 1.78012 avg_loss = 2.93521\n",
      "epoch no.5 train no.372530  loss = 2.03739 avg_loss = 2.91897\n",
      "epoch no.5 train no.372540  loss = 2.36235 avg_loss = 2.89677\n",
      "epoch no.5 train no.372550  loss = 2.60756 avg_loss = 2.87410\n",
      "epoch no.5 train no.372560  loss = 1.78906 avg_loss = 2.86204\n",
      "epoch no.5 train no.372570  loss = 1.77938 avg_loss = 2.85418\n",
      "epoch no.5 train no.372580  loss = 2.86440 avg_loss = 2.87474\n",
      "epoch no.5 train no.372590  loss = 3.43766 avg_loss = 2.87479\n",
      "epoch no.5 train no.372600  loss = 4.12491 avg_loss = 2.88238\n",
      "epoch no.5 train no.372610  loss = 2.28144 avg_loss = 2.87504\n",
      "epoch no.5 train no.372620  loss = 5.16979 avg_loss = 2.91321\n",
      "epoch no.5 train no.372630  loss = 3.11240 avg_loss = 2.91687\n",
      "epoch no.5 train no.372640  loss = 3.28285 avg_loss = 2.91248\n",
      "epoch no.5 train no.372650  loss = 3.09710 avg_loss = 2.93790\n",
      "epoch no.5 train no.372660  loss = 2.33702 avg_loss = 2.93263\n",
      "epoch no.5 train no.372670  loss = 4.04413 avg_loss = 2.96078\n",
      "epoch no.5 train no.372680  loss = 3.61548 avg_loss = 2.97520\n",
      "epoch no.5 train no.372690  loss = 3.78938 avg_loss = 3.00685\n",
      "epoch no.5 train no.372700  loss = 2.45690 avg_loss = 3.03149\n",
      "epoch no.5 train no.372710  loss = 2.12521 avg_loss = 3.03605\n",
      "epoch no.5 train no.372720  loss = 3.42541 avg_loss = 3.00915\n",
      "epoch no.5 train no.372730  loss = 4.46815 avg_loss = 2.99220\n",
      "epoch no.5 train no.372740  loss = 3.83899 avg_loss = 2.95981\n",
      "epoch no.5 train no.372750  loss = 2.17200 avg_loss = 2.94078\n",
      "epoch no.5 train no.372760  loss = 4.03401 avg_loss = 2.95988\n",
      "epoch no.5 train no.372770  loss = 2.18601 avg_loss = 2.95972\n",
      "epoch no.5 train no.372780  loss = 2.50192 avg_loss = 2.97942\n",
      "epoch no.5 train no.372790  loss = 3.69682 avg_loss = 2.96448\n",
      "epoch no.5 train no.372800  loss = 2.21841 avg_loss = 2.96945\n",
      "epoch no.5 train no.372810  loss = 3.01811 avg_loss = 2.95164\n",
      "epoch no.5 train no.372820  loss = 3.10456 avg_loss = 2.96442\n",
      "epoch no.5 train no.372830  loss = 2.62329 avg_loss = 2.96425\n",
      "epoch no.5 train no.372840  loss = 3.55700 avg_loss = 2.96662\n",
      "epoch no.5 train no.372850  loss = 3.05028 avg_loss = 2.96345\n",
      "epoch no.5 train no.372860  loss = 2.92403 avg_loss = 2.98197\n",
      "epoch no.5 train no.372870  loss = 2.24375 avg_loss = 3.02119\n",
      "epoch no.5 train no.372880  loss = 2.60887 avg_loss = 3.01708\n",
      "epoch no.5 train no.372890  loss = 2.80298 avg_loss = 2.99584\n",
      "epoch no.5 train no.372900  loss = 2.12943 avg_loss = 2.97538\n",
      "epoch no.5 train no.372910  loss = 2.31254 avg_loss = 2.93262\n",
      "epoch no.5 train no.372920  loss = 1.87418 avg_loss = 2.91969\n",
      "epoch no.5 train no.372930  loss = 2.64468 avg_loss = 2.91197\n",
      "epoch no.5 train no.372940  loss = 2.20544 avg_loss = 2.88103\n",
      "epoch no.5 train no.372950  loss = 3.10724 avg_loss = 2.84710\n",
      "epoch no.5 train no.372960  loss = 5.03944 avg_loss = 2.89975\n",
      "epoch no.5 train no.372970  loss = 4.01707 avg_loss = 2.94451\n",
      "epoch no.5 train no.372980  loss = 3.05609 avg_loss = 2.93706\n",
      "epoch no.5 train no.372990  loss = 3.13388 avg_loss = 2.94045\n",
      "epoch no.5 train no.373000  loss = 2.68220 avg_loss = 2.94516\n",
      "5\n",
      "to_tokens: ['▁가을', '밤', '▁산책', '하며', '▁듣기', '▁재즈', '</s>']\n",
      "여름 밤 산책하며 듣는 음악</s>\n",
      "epoch no.5 train no.373010  loss = 3.40659 avg_loss = 2.93312\n",
      "epoch no.5 train no.373020  loss = 2.74905 avg_loss = 2.92257\n",
      "epoch no.5 train no.373030  loss = 2.50972 avg_loss = 2.93053\n",
      "epoch no.5 train no.373040  loss = 2.88573 avg_loss = 2.91740\n",
      "epoch no.5 train no.373050  loss = 3.22128 avg_loss = 2.96371\n",
      "epoch no.5 train no.373060  loss = 1.40826 avg_loss = 2.92441\n",
      "epoch no.5 train no.373070  loss = 2.60210 avg_loss = 2.91936\n",
      "epoch no.5 train no.373080  loss = 2.11925 avg_loss = 2.90610\n",
      "epoch no.5 train no.373090  loss = 2.47397 avg_loss = 2.93504\n",
      "epoch no.5 train no.373100  loss = 2.56150 avg_loss = 2.92844\n",
      "epoch no.5 train no.373110  loss = 3.06511 avg_loss = 2.92499\n",
      "epoch no.5 train no.373120  loss = 4.15036 avg_loss = 3.00069\n",
      "epoch no.5 train no.373130  loss = 3.99989 avg_loss = 2.99446\n",
      "epoch no.5 train no.373140  loss = 2.76541 avg_loss = 3.00330\n",
      "epoch no.5 train no.373150  loss = 3.47885 avg_loss = 3.01379\n",
      "epoch no.5 train no.373160  loss = 3.35924 avg_loss = 3.01482\n",
      "epoch no.5 train no.373170  loss = 2.92016 avg_loss = 3.00657\n",
      "epoch no.5 train no.373180  loss = 4.66210 avg_loss = 3.03674\n",
      "epoch no.5 train no.373190  loss = 3.57204 avg_loss = 3.04749\n",
      "epoch no.5 train no.373200  loss = 4.02903 avg_loss = 3.04757\n",
      "epoch no.5 train no.373210  loss = 3.38245 avg_loss = 3.06333\n",
      "epoch no.5 train no.373220  loss = 5.22327 avg_loss = 3.06999\n",
      "epoch no.5 train no.373230  loss = 2.75798 avg_loss = 3.05269\n",
      "epoch no.5 train no.373240  loss = 3.77444 avg_loss = 3.12274\n",
      "epoch no.5 train no.373250  loss = 3.44938 avg_loss = 3.10413\n",
      "epoch no.5 train no.373260  loss = 3.31993 avg_loss = 3.13916\n",
      "epoch no.5 train no.373270  loss = 1.54879 avg_loss = 3.09915\n",
      "epoch no.5 train no.373280  loss = 3.59781 avg_loss = 3.15320\n",
      "epoch no.5 train no.373290  loss = 1.88395 avg_loss = 3.09300\n",
      "epoch no.5 train no.373300  loss = 3.60108 avg_loss = 3.10797\n",
      "epoch no.5 train no.373310  loss = 4.42148 avg_loss = 3.10385\n",
      "epoch no.5 train no.373320  loss = 3.90303 avg_loss = 3.10141\n",
      "epoch no.5 train no.373330  loss = 5.25004 avg_loss = 3.11524\n",
      "epoch no.5 train no.373340  loss = 4.08521 avg_loss = 3.11217\n",
      "epoch no.5 train no.373350  loss = 3.26781 avg_loss = 3.08706\n",
      "epoch no.5 train no.373360  loss = 1.71011 avg_loss = 3.09621\n",
      "epoch no.5 train no.373370  loss = 3.00015 avg_loss = 3.06151\n",
      "epoch no.5 train no.373380  loss = 4.77872 avg_loss = 3.05558\n",
      "epoch no.5 train no.373390  loss = 1.70101 avg_loss = 3.00950\n",
      "epoch no.5 train no.373400  loss = 2.15874 avg_loss = 2.99328\n",
      "epoch no.5 train no.373410  loss = 2.89280 avg_loss = 3.00025\n",
      "epoch no.5 train no.373420  loss = 3.76584 avg_loss = 2.99688\n",
      "epoch no.5 train no.373430  loss = 2.84071 avg_loss = 3.00084\n",
      "epoch no.5 train no.373440  loss = 1.56039 avg_loss = 2.95399\n",
      "epoch no.5 train no.373450  loss = 2.52114 avg_loss = 2.94538\n",
      "epoch no.5 train no.373460  loss = 2.09221 avg_loss = 2.93792\n",
      "epoch no.5 train no.373470  loss = 2.85286 avg_loss = 2.90085\n",
      "epoch no.5 train no.373480  loss = 1.46703 avg_loss = 2.89096\n",
      "epoch no.5 train no.373490  loss = 2.32614 avg_loss = 2.91277\n",
      "epoch no.5 train no.373500  loss = 2.79445 avg_loss = 2.90041\n",
      "epoch no.5 train no.373510  loss = 2.58001 avg_loss = 2.93935\n",
      "epoch no.5 train no.373520  loss = 1.96292 avg_loss = 2.93316\n",
      "epoch no.5 train no.373530  loss = 4.59231 avg_loss = 2.99207\n",
      "epoch no.5 train no.373540  loss = 2.01064 avg_loss = 2.96542\n",
      "epoch no.5 train no.373550  loss = 4.02185 avg_loss = 2.97873\n",
      "epoch no.5 train no.373560  loss = 6.35229 avg_loss = 2.98211\n",
      "epoch no.5 train no.373570  loss = 1.99259 avg_loss = 2.95983\n",
      "epoch no.5 train no.373580  loss = 3.74746 avg_loss = 3.01235\n",
      "epoch no.5 train no.373590  loss = 3.01897 avg_loss = 2.97473\n",
      "epoch no.5 train no.373600  loss = 1.96847 avg_loss = 2.99206\n",
      "epoch no.5 train no.373610  loss = 3.62226 avg_loss = 3.04453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.373620  loss = 2.41884 avg_loss = 3.08798\n",
      "epoch no.5 train no.373630  loss = 3.33927 avg_loss = 3.08355\n",
      "epoch no.5 train no.373640  loss = 3.52029 avg_loss = 3.10327\n",
      "epoch no.5 train no.373650  loss = 2.91407 avg_loss = 3.09622\n",
      "epoch no.5 train no.373660  loss = 2.69691 avg_loss = 3.09318\n",
      "epoch no.5 train no.373670  loss = 2.62092 avg_loss = 3.05449\n",
      "epoch no.5 train no.373680  loss = 2.71631 avg_loss = 3.03581\n",
      "epoch no.5 train no.373690  loss = 2.59540 avg_loss = 3.01079\n",
      "epoch no.5 train no.373700  loss = 2.77586 avg_loss = 2.97601\n",
      "epoch no.5 train no.373710  loss = 3.23936 avg_loss = 2.96772\n",
      "epoch no.5 train no.373720  loss = 1.87040 avg_loss = 2.97162\n",
      "epoch no.5 train no.373730  loss = 2.33323 avg_loss = 2.93583\n",
      "epoch no.5 train no.373740  loss = 2.28460 avg_loss = 2.93194\n",
      "epoch no.5 train no.373750  loss = 3.04527 avg_loss = 2.93070\n",
      "epoch no.5 train no.373760  loss = 4.21772 avg_loss = 2.95957\n",
      "epoch no.5 train no.373770  loss = 2.69569 avg_loss = 2.95892\n",
      "epoch no.5 train no.373780  loss = 3.94409 avg_loss = 2.97491\n",
      "epoch no.5 train no.373790  loss = 1.62908 avg_loss = 2.96735\n",
      "epoch no.5 train no.373800  loss = 3.07358 avg_loss = 2.93391\n",
      "epoch no.5 train no.373810  loss = 2.68311 avg_loss = 2.91108\n",
      "epoch no.5 train no.373820  loss = 2.72988 avg_loss = 2.87616\n",
      "epoch no.5 train no.373830  loss = 3.19688 avg_loss = 2.90127\n",
      "epoch no.5 train no.373840  loss = 4.79154 avg_loss = 2.89728\n",
      "epoch no.5 train no.373850  loss = 3.68709 avg_loss = 2.90843\n",
      "epoch no.5 train no.373860  loss = 2.08077 avg_loss = 2.89377\n",
      "epoch no.5 train no.373870  loss = 2.09950 avg_loss = 2.89271\n",
      "epoch no.5 train no.373880  loss = 2.90154 avg_loss = 2.90365\n",
      "epoch no.5 train no.373890  loss = 3.01494 avg_loss = 2.87050\n",
      "epoch no.5 train no.373900  loss = 3.43063 avg_loss = 2.88015\n",
      "epoch no.5 train no.373910  loss = 2.70647 avg_loss = 2.90749\n",
      "epoch no.5 train no.373920  loss = 3.81283 avg_loss = 2.92334\n",
      "epoch no.5 train no.373930  loss = 2.50302 avg_loss = 2.92243\n",
      "epoch no.5 train no.373940  loss = 3.75549 avg_loss = 2.95117\n",
      "epoch no.5 train no.373950  loss = 2.75146 avg_loss = 2.93076\n",
      "epoch no.5 train no.373960  loss = 1.90213 avg_loss = 2.92749\n",
      "epoch no.5 train no.373970  loss = 2.26381 avg_loss = 2.92895\n",
      "epoch no.5 train no.373980  loss = 3.47134 avg_loss = 2.97231\n",
      "epoch no.5 train no.373990  loss = 3.63150 avg_loss = 3.01777\n",
      "epoch no.5 train no.374000  loss = 2.53243 avg_loss = 2.99488\n",
      "5\n",
      "to_tokens: ['▁', '밤', '▁역시', '▁p', '▁노래', '음악', '</s>']\n",
      "여름엔 신나는 걸그룹 댄스곡</s>\n",
      "epoch no.5 train no.374010  loss = 2.33034 avg_loss = 2.95868\n",
      "epoch no.5 train no.374020  loss = 2.87750 avg_loss = 2.99808\n",
      "epoch no.5 train no.374030  loss = 2.25075 avg_loss = 2.99802\n",
      "epoch no.5 train no.374040  loss = 2.50145 avg_loss = 2.98198\n",
      "epoch no.5 train no.374050  loss = 2.14753 avg_loss = 2.95792\n",
      "epoch no.5 train no.374060  loss = 2.69810 avg_loss = 2.94761\n",
      "epoch no.5 train no.374070  loss = 3.96019 avg_loss = 2.95438\n",
      "epoch no.5 train no.374080  loss = 1.81678 avg_loss = 2.95473\n",
      "epoch no.5 train no.374090  loss = 3.67661 avg_loss = 2.94442\n",
      "epoch no.5 train no.374100  loss = 4.27133 avg_loss = 2.91684\n",
      "epoch no.5 train no.374110  loss = 1.83968 avg_loss = 2.94328\n",
      "epoch no.5 train no.374120  loss = 2.60415 avg_loss = 2.93325\n",
      "epoch no.5 train no.374130  loss = 2.18840 avg_loss = 2.94056\n",
      "epoch no.5 train no.374140  loss = 2.14765 avg_loss = 2.95482\n",
      "epoch no.5 train no.374150  loss = 2.97897 avg_loss = 2.93080\n",
      "epoch no.5 train no.374160  loss = 3.47242 avg_loss = 2.90613\n",
      "epoch no.5 train no.374170  loss = 3.00051 avg_loss = 2.91175\n",
      "epoch no.5 train no.374180  loss = 1.02695 avg_loss = 2.90825\n",
      "epoch no.5 train no.374190  loss = 2.10887 avg_loss = 2.93051\n",
      "epoch no.5 train no.374200  loss = 2.83950 avg_loss = 2.95132\n",
      "epoch no.5 train no.374210  loss = 2.46994 avg_loss = 2.94804\n",
      "epoch no.5 train no.374220  loss = 3.43340 avg_loss = 2.94329\n",
      "epoch no.5 train no.374230  loss = 2.46766 avg_loss = 2.95633\n",
      "epoch no.5 train no.374240  loss = 3.05356 avg_loss = 2.98677\n",
      "epoch no.5 train no.374250  loss = 2.55000 avg_loss = 2.98969\n",
      "epoch no.5 train no.374260  loss = 2.13207 avg_loss = 2.96874\n",
      "epoch no.5 train no.374270  loss = 2.62780 avg_loss = 2.93495\n",
      "epoch no.5 train no.374280  loss = 1.77863 avg_loss = 2.93950\n",
      "epoch no.5 train no.374290  loss = 3.09770 avg_loss = 2.94370\n",
      "epoch no.5 train no.374300  loss = 3.05384 avg_loss = 2.94053\n",
      "epoch no.5 train no.374310  loss = 2.45702 avg_loss = 2.93182\n",
      "epoch no.5 train no.374320  loss = 3.11128 avg_loss = 2.91342\n",
      "epoch no.5 train no.374330  loss = 2.42497 avg_loss = 2.91606\n",
      "epoch no.5 train no.374340  loss = 2.70250 avg_loss = 2.92752\n",
      "epoch no.5 train no.374350  loss = 2.47134 avg_loss = 2.94026\n",
      "epoch no.5 train no.374360  loss = 3.45480 avg_loss = 2.92370\n",
      "epoch no.5 train no.374370  loss = 3.38476 avg_loss = 2.91950\n",
      "epoch no.5 train no.374380  loss = 2.75881 avg_loss = 2.90256\n",
      "epoch no.5 train no.374390  loss = 2.46307 avg_loss = 2.90520\n",
      "epoch no.5 train no.374400  loss = 2.56687 avg_loss = 2.93956\n",
      "epoch no.5 train no.374410  loss = 3.36926 avg_loss = 2.94811\n",
      "epoch no.5 train no.374420  loss = 3.61728 avg_loss = 2.94007\n",
      "epoch no.5 train no.374430  loss = 3.57501 avg_loss = 2.98948\n",
      "epoch no.5 train no.374440  loss = 2.80746 avg_loss = 3.04117\n",
      "epoch no.5 train no.374450  loss = 2.71315 avg_loss = 3.02087\n",
      "epoch no.5 train no.374460  loss = 3.01523 avg_loss = 3.00611\n",
      "epoch no.5 train no.374470  loss = 1.77098 avg_loss = 3.01786\n",
      "epoch no.5 train no.374480  loss = 3.21908 avg_loss = 2.99151\n",
      "epoch no.5 train no.374490  loss = 2.86239 avg_loss = 2.99701\n",
      "epoch no.5 train no.374500  loss = 3.80647 avg_loss = 3.02747\n",
      "epoch no.5 train no.374510  loss = 2.35790 avg_loss = 3.00943\n",
      "epoch no.5 train no.374520  loss = 2.21374 avg_loss = 2.98434\n",
      "epoch no.5 train no.374530  loss = 2.63500 avg_loss = 3.00557\n",
      "epoch no.5 train no.374540  loss = 3.08357 avg_loss = 3.02580\n",
      "epoch no.5 train no.374550  loss = 1.94897 avg_loss = 2.98904\n",
      "epoch no.5 train no.374560  loss = 2.63558 avg_loss = 3.04021\n",
      "epoch no.5 train no.374570  loss = 3.28167 avg_loss = 3.01046\n",
      "epoch no.5 train no.374580  loss = 3.60303 avg_loss = 2.97016\n",
      "epoch no.5 train no.374590  loss = 3.77096 avg_loss = 3.00416\n",
      "epoch no.5 train no.374600  loss = 2.95825 avg_loss = 3.03523\n",
      "epoch no.5 train no.374610  loss = 4.64807 avg_loss = 3.07560\n",
      "epoch no.5 train no.374620  loss = 3.24916 avg_loss = 3.09504\n",
      "epoch no.5 train no.374630  loss = 3.12193 avg_loss = 3.06432\n",
      "epoch no.5 train no.374640  loss = 2.82505 avg_loss = 3.08383\n",
      "epoch no.5 train no.374650  loss = 3.53099 avg_loss = 3.06854\n",
      "epoch no.5 train no.374660  loss = 3.04163 avg_loss = 3.08333\n",
      "epoch no.5 train no.374670  loss = 2.49055 avg_loss = 3.03324\n",
      "epoch no.5 train no.374680  loss = 1.98772 avg_loss = 3.03870\n",
      "epoch no.5 train no.374690  loss = 4.72671 avg_loss = 3.06126\n",
      "epoch no.5 train no.374700  loss = 3.26164 avg_loss = 3.03098\n",
      "epoch no.5 train no.374710  loss = 2.14971 avg_loss = 2.98265\n",
      "epoch no.5 train no.374720  loss = 3.31368 avg_loss = 2.96615\n",
      "epoch no.5 train no.374730  loss = 2.78317 avg_loss = 2.96878\n",
      "epoch no.5 train no.374740  loss = 2.14717 avg_loss = 2.96965\n",
      "epoch no.5 train no.374750  loss = 2.46455 avg_loss = 2.94232\n",
      "epoch no.5 train no.374760  loss = 2.84988 avg_loss = 2.99090\n",
      "epoch no.5 train no.374770  loss = 2.82414 avg_loss = 3.04416\n",
      "epoch no.5 train no.374780  loss = 2.63526 avg_loss = 3.04622\n",
      "epoch no.5 train no.374790  loss = 2.80987 avg_loss = 2.98303\n",
      "epoch no.5 train no.374800  loss = 3.17368 avg_loss = 3.01758\n",
      "epoch no.5 train no.374810  loss = 2.82595 avg_loss = 3.02916\n",
      "epoch no.5 train no.374820  loss = 3.04984 avg_loss = 3.03507\n",
      "epoch no.5 train no.374830  loss = 2.56985 avg_loss = 3.00071\n",
      "epoch no.5 train no.374840  loss = 5.06394 avg_loss = 3.00088\n",
      "epoch no.5 train no.374850  loss = 2.59204 avg_loss = 2.96580\n",
      "epoch no.5 train no.374860  loss = 3.17266 avg_loss = 2.99512\n",
      "epoch no.5 train no.374870  loss = 4.09408 avg_loss = 2.97343\n",
      "epoch no.5 train no.374880  loss = 2.05917 avg_loss = 2.99864\n",
      "epoch no.5 train no.374890  loss = 2.72267 avg_loss = 3.00300\n",
      "epoch no.5 train no.374900  loss = 3.53400 avg_loss = 3.00909\n",
      "epoch no.5 train no.374910  loss = 2.35499 avg_loss = 3.06455\n",
      "epoch no.5 train no.374920  loss = 2.63230 avg_loss = 3.05895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.374930  loss = 2.80092 avg_loss = 3.04079\n",
      "epoch no.5 train no.374940  loss = 2.18749 avg_loss = 3.02164\n",
      "epoch no.5 train no.374950  loss = 2.59826 avg_loss = 3.01429\n",
      "epoch no.5 train no.374960  loss = 2.35411 avg_loss = 3.01600\n",
      "epoch no.5 train no.374970  loss = 3.23046 avg_loss = 3.03135\n",
      "epoch no.5 train no.374980  loss = 3.38354 avg_loss = 3.01493\n",
      "epoch no.5 train no.374990  loss = 3.03240 avg_loss = 3.06608\n",
      "epoch no.5 train no.375000  loss = 2.99032 avg_loss = 3.02602\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '▁한강', '▁감성', '같은', '▁노래', '</s>', '</s>']\n",
      "여름밤의 꿀같은 노래들</s>\n",
      "epoch no.5 train no.375010  loss = 3.81252 avg_loss = 3.06951\n",
      "epoch no.5 train no.375020  loss = 2.30363 avg_loss = 3.03749\n",
      "epoch no.5 train no.375030  loss = 3.15529 avg_loss = 3.04893\n",
      "epoch no.5 train no.375040  loss = 2.71300 avg_loss = 3.06136\n",
      "epoch no.5 train no.375050  loss = 3.88983 avg_loss = 3.04364\n",
      "epoch no.5 train no.375060  loss = 3.95741 avg_loss = 3.04792\n",
      "epoch no.5 train no.375070  loss = 2.59582 avg_loss = 3.02850\n",
      "epoch no.5 train no.375080  loss = 2.50358 avg_loss = 3.02595\n",
      "epoch no.5 train no.375090  loss = 3.26799 avg_loss = 3.06217\n",
      "epoch no.5 train no.375100  loss = 2.00188 avg_loss = 3.07556\n",
      "epoch no.5 train no.375110  loss = 2.34063 avg_loss = 3.04362\n",
      "epoch no.5 train no.375120  loss = 2.54952 avg_loss = 3.00683\n",
      "epoch no.5 train no.375130  loss = 2.19626 avg_loss = 2.99118\n",
      "epoch no.5 train no.375140  loss = 3.04437 avg_loss = 2.96922\n",
      "epoch no.5 train no.375150  loss = 2.73343 avg_loss = 2.95744\n",
      "epoch no.5 train no.375160  loss = 3.17523 avg_loss = 2.96932\n",
      "epoch no.5 train no.375170  loss = 2.62773 avg_loss = 2.93597\n",
      "epoch no.5 train no.375180  loss = 1.82324 avg_loss = 2.91880\n",
      "epoch no.5 train no.375190  loss = 3.94080 avg_loss = 2.93044\n",
      "epoch no.5 train no.375200  loss = 3.78853 avg_loss = 2.94195\n",
      "epoch no.5 train no.375210  loss = 2.97474 avg_loss = 2.93884\n",
      "epoch no.5 train no.375220  loss = 3.12003 avg_loss = 2.91473\n",
      "epoch no.5 train no.375230  loss = 4.20887 avg_loss = 2.91555\n",
      "epoch no.5 train no.375240  loss = 3.97178 avg_loss = 2.90598\n",
      "epoch no.5 train no.375250  loss = 3.48895 avg_loss = 2.91483\n",
      "epoch no.5 train no.375260  loss = 2.02173 avg_loss = 2.93431\n",
      "epoch no.5 train no.375270  loss = 2.26361 avg_loss = 2.88254\n",
      "epoch no.5 train no.375280  loss = 3.15094 avg_loss = 2.90970\n",
      "epoch no.5 train no.375290  loss = 3.91750 avg_loss = 2.89931\n",
      "epoch no.5 train no.375300  loss = 3.32919 avg_loss = 2.92636\n",
      "epoch no.5 train no.375310  loss = 4.64678 avg_loss = 2.96857\n",
      "epoch no.5 train no.375320  loss = 1.72045 avg_loss = 2.97350\n",
      "epoch no.5 train no.375330  loss = 2.93626 avg_loss = 2.95281\n",
      "epoch no.5 train no.375340  loss = 2.34032 avg_loss = 2.91726\n",
      "epoch no.5 train no.375350  loss = 2.44027 avg_loss = 2.94755\n",
      "epoch no.5 train no.375360  loss = 3.91202 avg_loss = 2.95034\n",
      "epoch no.5 train no.375370  loss = 3.12456 avg_loss = 2.92386\n",
      "epoch no.5 train no.375380  loss = 2.53645 avg_loss = 2.89802\n",
      "epoch no.5 train no.375390  loss = 1.94265 avg_loss = 2.85227\n",
      "epoch no.5 train no.375400  loss = 2.21996 avg_loss = 2.87456\n",
      "epoch no.5 train no.375410  loss = 2.92190 avg_loss = 2.87973\n",
      "epoch no.5 train no.375420  loss = 1.76736 avg_loss = 2.86223\n",
      "epoch no.5 train no.375430  loss = 1.73544 avg_loss = 2.86435\n",
      "epoch no.5 train no.375440  loss = 2.37263 avg_loss = 2.91323\n",
      "epoch no.5 train no.375450  loss = 2.68697 avg_loss = 2.97803\n",
      "epoch no.5 train no.375460  loss = 3.21008 avg_loss = 2.96863\n",
      "epoch no.5 train no.375470  loss = 3.41509 avg_loss = 2.94010\n",
      "epoch no.5 train no.375480  loss = 4.11728 avg_loss = 2.96839\n",
      "epoch no.5 train no.375490  loss = 2.08245 avg_loss = 2.96492\n",
      "epoch no.5 train no.375500  loss = 3.50097 avg_loss = 2.97469\n",
      "epoch no.5 train no.375510  loss = 3.50370 avg_loss = 2.97912\n",
      "epoch no.5 train no.375520  loss = 4.47668 avg_loss = 3.01600\n",
      "epoch no.5 train no.375530  loss = 2.61701 avg_loss = 2.99243\n",
      "epoch no.5 train no.375540  loss = 3.41395 avg_loss = 2.96612\n",
      "epoch no.5 train no.375550  loss = 4.23497 avg_loss = 2.97128\n",
      "epoch no.5 train no.375560  loss = 2.61062 avg_loss = 2.93985\n",
      "epoch no.5 train no.375570  loss = 2.79634 avg_loss = 2.92792\n",
      "epoch no.5 train no.375580  loss = 2.64620 avg_loss = 2.91489\n",
      "epoch no.5 train no.375590  loss = 3.23208 avg_loss = 2.89974\n",
      "epoch no.5 train no.375600  loss = 3.20738 avg_loss = 2.90011\n",
      "epoch no.5 train no.375610  loss = 2.12860 avg_loss = 2.85533\n",
      "epoch no.5 train no.375620  loss = 1.01921 avg_loss = 2.86125\n",
      "epoch no.5 train no.375630  loss = 3.35511 avg_loss = 2.89492\n",
      "epoch no.5 train no.375640  loss = 2.33282 avg_loss = 2.88235\n",
      "epoch no.5 train no.375650  loss = 2.65864 avg_loss = 2.87574\n",
      "epoch no.5 train no.375660  loss = 2.36201 avg_loss = 2.85125\n",
      "epoch no.5 train no.375670  loss = 3.09673 avg_loss = 2.85757\n",
      "epoch no.5 train no.375680  loss = 4.86379 avg_loss = 2.90256\n",
      "epoch no.5 train no.375690  loss = 3.77377 avg_loss = 2.87656\n",
      "epoch no.5 train no.375700  loss = 2.60536 avg_loss = 2.83160\n",
      "epoch no.5 train no.375710  loss = 2.83423 avg_loss = 2.83192\n",
      "epoch no.5 train no.375720  loss = 2.82656 avg_loss = 2.86138\n",
      "epoch no.5 train no.375730  loss = 3.09610 avg_loss = 2.90864\n",
      "epoch no.5 train no.375740  loss = 2.65087 avg_loss = 2.91732\n",
      "epoch no.5 train no.375750  loss = 2.94449 avg_loss = 2.94005\n",
      "epoch no.5 train no.375760  loss = 3.22900 avg_loss = 2.92177\n",
      "epoch no.5 train no.375770  loss = 2.53618 avg_loss = 2.90990\n",
      "epoch no.5 train no.375780  loss = 3.23264 avg_loss = 2.91562\n",
      "epoch no.5 train no.375790  loss = 3.45141 avg_loss = 2.91944\n",
      "epoch no.5 train no.375800  loss = 1.96120 avg_loss = 2.88000\n",
      "epoch no.5 train no.375810  loss = 2.92547 avg_loss = 2.88737\n",
      "epoch no.5 train no.375820  loss = 2.26506 avg_loss = 2.86604\n",
      "epoch no.5 train no.375830  loss = 3.82678 avg_loss = 2.86485\n",
      "epoch no.5 train no.375840  loss = 1.80597 avg_loss = 2.83917\n",
      "epoch no.5 train no.375850  loss = 3.67228 avg_loss = 2.87017\n",
      "epoch no.5 train no.375860  loss = 5.18715 avg_loss = 2.93077\n",
      "epoch no.5 train no.375870  loss = 2.42526 avg_loss = 2.90633\n",
      "epoch no.5 train no.375880  loss = 2.77874 avg_loss = 2.96610\n",
      "epoch no.5 train no.375890  loss = 2.23692 avg_loss = 2.95659\n",
      "epoch no.5 train no.375900  loss = 2.43325 avg_loss = 2.99122\n",
      "epoch no.5 train no.375910  loss = 3.68603 avg_loss = 3.01885\n",
      "epoch no.5 train no.375920  loss = 4.77898 avg_loss = 3.04169\n",
      "epoch no.5 train no.375930  loss = 3.76010 avg_loss = 3.07221\n",
      "epoch no.5 train no.375940  loss = 3.25804 avg_loss = 3.04044\n",
      "epoch no.5 train no.375950  loss = 3.03725 avg_loss = 3.04423\n",
      "epoch no.5 train no.375960  loss = 2.55401 avg_loss = 3.06264\n",
      "epoch no.5 train no.375970  loss = 2.15486 avg_loss = 3.06602\n",
      "epoch no.5 train no.375980  loss = 2.65000 avg_loss = 3.02407\n",
      "epoch no.5 train no.375990  loss = 2.61184 avg_loss = 3.00351\n",
      "epoch no.5 train no.376000  loss = 3.63576 avg_loss = 3.01992\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁신나는', '들', '</s>']\n",
      "여름과 어울리는 노래들</s>\n",
      "epoch no.5 train no.376010  loss = 4.28723 avg_loss = 2.98409\n",
      "epoch no.5 train no.376020  loss = 3.49273 avg_loss = 3.03836\n",
      "epoch no.5 train no.376030  loss = 4.59978 avg_loss = 3.05357\n",
      "epoch no.5 train no.376040  loss = 1.29637 avg_loss = 3.01607\n",
      "epoch no.5 train no.376050  loss = 2.38195 avg_loss = 3.00090\n",
      "epoch no.5 train no.376060  loss = 3.00986 avg_loss = 2.99488\n",
      "epoch no.5 train no.376070  loss = 3.20311 avg_loss = 3.01605\n",
      "epoch no.5 train no.376080  loss = 3.98533 avg_loss = 3.00745\n",
      "epoch no.5 train no.376090  loss = 3.65505 avg_loss = 3.00187\n",
      "epoch no.5 train no.376100  loss = 3.61014 avg_loss = 3.02819\n",
      "epoch no.5 train no.376110  loss = 2.83908 avg_loss = 3.03089\n",
      "epoch no.5 train no.376120  loss = 2.05908 avg_loss = 3.02046\n",
      "epoch no.5 train no.376130  loss = 2.30224 avg_loss = 3.01707\n",
      "epoch no.5 train no.376140  loss = 3.56002 avg_loss = 2.99956\n",
      "epoch no.5 train no.376150  loss = 3.34434 avg_loss = 3.04892\n",
      "epoch no.5 train no.376160  loss = 2.11287 avg_loss = 3.04874\n",
      "epoch no.5 train no.376170  loss = 2.06306 avg_loss = 2.99841\n",
      "epoch no.5 train no.376180  loss = 2.97060 avg_loss = 2.95504\n",
      "epoch no.5 train no.376190  loss = 4.19642 avg_loss = 2.98796\n",
      "epoch no.5 train no.376200  loss = 3.66353 avg_loss = 2.97415\n",
      "epoch no.5 train no.376210  loss = 2.79260 avg_loss = 2.98187\n",
      "epoch no.5 train no.376220  loss = 2.78403 avg_loss = 2.95261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.376230  loss = 2.49655 avg_loss = 2.98148\n",
      "epoch no.5 train no.376240  loss = 2.94682 avg_loss = 2.99099\n",
      "epoch no.5 train no.376250  loss = 2.52503 avg_loss = 2.98127\n",
      "epoch no.5 train no.376260  loss = 5.37552 avg_loss = 3.03792\n",
      "epoch no.5 train no.376270  loss = 1.66110 avg_loss = 2.99213\n",
      "epoch no.5 train no.376280  loss = 2.23260 avg_loss = 3.00604\n",
      "epoch no.5 train no.376290  loss = 2.85388 avg_loss = 2.96571\n",
      "epoch no.5 train no.376300  loss = 2.89996 avg_loss = 2.99599\n",
      "epoch no.5 train no.376310  loss = 2.33275 avg_loss = 2.97268\n",
      "epoch no.5 train no.376320  loss = 3.06258 avg_loss = 2.95565\n",
      "epoch no.5 train no.376330  loss = 3.08726 avg_loss = 2.95253\n",
      "epoch no.5 train no.376340  loss = 4.59388 avg_loss = 3.01703\n",
      "epoch no.5 train no.376350  loss = 2.47755 avg_loss = 2.98564\n",
      "epoch no.5 train no.376360  loss = 3.30958 avg_loss = 2.95411\n",
      "epoch no.5 train no.376370  loss = 1.99000 avg_loss = 2.92378\n",
      "epoch no.5 train no.376380  loss = 2.30754 avg_loss = 2.90072\n",
      "epoch no.5 train no.376390  loss = 3.19130 avg_loss = 2.89453\n",
      "epoch no.5 train no.376400  loss = 3.12650 avg_loss = 2.88552\n",
      "epoch no.5 train no.376410  loss = 3.51087 avg_loss = 2.88375\n",
      "epoch no.5 train no.376420  loss = 3.66745 avg_loss = 2.89244\n",
      "epoch no.5 train no.376430  loss = 1.97622 avg_loss = 2.85439\n",
      "epoch no.5 train no.376440  loss = 3.83805 avg_loss = 2.87803\n",
      "epoch no.5 train no.376450  loss = 3.08674 avg_loss = 2.89084\n",
      "epoch no.5 train no.376460  loss = 1.43367 avg_loss = 2.89229\n",
      "epoch no.5 train no.376470  loss = 3.66199 avg_loss = 2.95381\n",
      "epoch no.5 train no.376480  loss = 3.28387 avg_loss = 2.99695\n",
      "epoch no.5 train no.376490  loss = 2.59072 avg_loss = 2.99259\n",
      "epoch no.5 train no.376500  loss = 2.71270 avg_loss = 2.98435\n",
      "epoch no.5 train no.376510  loss = 2.87847 avg_loss = 2.96304\n",
      "epoch no.5 train no.376520  loss = 3.38013 avg_loss = 2.97205\n",
      "epoch no.5 train no.376530  loss = 2.65087 avg_loss = 2.96605\n",
      "epoch no.5 train no.376540  loss = 3.10109 avg_loss = 2.93811\n",
      "epoch no.5 train no.376550  loss = 2.54102 avg_loss = 2.96296\n",
      "epoch no.5 train no.376560  loss = 1.94157 avg_loss = 2.94686\n",
      "epoch no.5 train no.376570  loss = 1.25867 avg_loss = 2.91520\n",
      "epoch no.5 train no.376580  loss = 3.73697 avg_loss = 2.94423\n",
      "epoch no.5 train no.376590  loss = 1.84011 avg_loss = 2.91704\n",
      "epoch no.5 train no.376600  loss = 4.65852 avg_loss = 2.93398\n",
      "epoch no.5 train no.376610  loss = 2.44051 avg_loss = 2.92037\n",
      "epoch no.5 train no.376620  loss = 2.54057 avg_loss = 2.91697\n",
      "epoch no.5 train no.376630  loss = 2.34561 avg_loss = 2.90301\n",
      "epoch no.5 train no.376640  loss = 3.79284 avg_loss = 2.92677\n",
      "epoch no.5 train no.376650  loss = 3.87308 avg_loss = 2.90173\n",
      "epoch no.5 train no.376660  loss = 3.48403 avg_loss = 2.87389\n",
      "epoch no.5 train no.376670  loss = 2.46007 avg_loss = 2.85896\n",
      "epoch no.5 train no.376680  loss = 2.94552 avg_loss = 2.84082\n",
      "epoch no.5 train no.376690  loss = 2.69967 avg_loss = 2.79949\n",
      "epoch no.5 train no.376700  loss = 2.62139 avg_loss = 2.81769\n",
      "epoch no.5 train no.376710  loss = 3.35922 avg_loss = 2.87731\n",
      "epoch no.5 train no.376720  loss = 2.25621 avg_loss = 2.89273\n",
      "epoch no.5 train no.376730  loss = 2.26235 avg_loss = 2.89559\n",
      "epoch no.5 train no.376740  loss = 2.48119 avg_loss = 2.88271\n",
      "epoch no.5 train no.376750  loss = 3.41442 avg_loss = 2.90642\n",
      "epoch no.5 train no.376760  loss = 2.42251 avg_loss = 2.92475\n",
      "epoch no.5 train no.376770  loss = 3.72967 avg_loss = 2.91737\n",
      "epoch no.5 train no.376780  loss = 3.15508 avg_loss = 2.93601\n",
      "epoch no.5 train no.376790  loss = 3.74678 avg_loss = 2.90321\n",
      "epoch no.5 train no.376800  loss = 3.58759 avg_loss = 2.95935\n",
      "epoch no.5 train no.376810  loss = 2.75064 avg_loss = 2.94075\n",
      "epoch no.5 train no.376820  loss = 2.40218 avg_loss = 2.93450\n",
      "epoch no.5 train no.376830  loss = 3.42061 avg_loss = 2.90211\n",
      "epoch no.5 train no.376840  loss = 3.31996 avg_loss = 2.91459\n",
      "epoch no.5 train no.376850  loss = 4.67904 avg_loss = 2.93340\n",
      "epoch no.5 train no.376860  loss = 3.57947 avg_loss = 2.93258\n",
      "epoch no.5 train no.376870  loss = 2.81953 avg_loss = 2.95695\n",
      "epoch no.5 train no.376880  loss = 2.57009 avg_loss = 2.95166\n",
      "epoch no.5 train no.376890  loss = 2.57776 avg_loss = 2.96709\n",
      "epoch no.5 train no.376900  loss = 3.33791 avg_loss = 2.98826\n",
      "epoch no.5 train no.376910  loss = 2.80023 avg_loss = 2.97727\n",
      "epoch no.5 train no.376920  loss = 2.24216 avg_loss = 2.98095\n",
      "epoch no.5 train no.376930  loss = 2.12355 avg_loss = 2.99396\n",
      "epoch no.5 train no.376940  loss = 2.73533 avg_loss = 2.99116\n",
      "epoch no.5 train no.376950  loss = 3.30162 avg_loss = 3.00266\n",
      "epoch no.5 train no.376960  loss = 2.15174 avg_loss = 2.99326\n",
      "epoch no.5 train no.376970  loss = 2.63702 avg_loss = 2.99033\n",
      "epoch no.5 train no.376980  loss = 2.69392 avg_loss = 2.96167\n",
      "epoch no.5 train no.376990  loss = 3.38957 avg_loss = 2.96918\n",
      "epoch no.5 train no.377000  loss = 4.37528 avg_loss = 2.99790\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁잔잔', '과', '▁감성을', '자', '</s>']\n",
      "여름밤의 추억과 감성노래</s>\n",
      "epoch no.5 train no.377010  loss = 2.10410 avg_loss = 3.00692\n",
      "epoch no.5 train no.377020  loss = 3.83822 avg_loss = 2.98732\n",
      "epoch no.5 train no.377030  loss = 2.43745 avg_loss = 3.05050\n",
      "epoch no.5 train no.377040  loss = 2.01338 avg_loss = 3.05043\n",
      "epoch no.5 train no.377050  loss = 3.09911 avg_loss = 3.04792\n",
      "epoch no.5 train no.377060  loss = 1.72891 avg_loss = 3.03390\n",
      "epoch no.5 train no.377070  loss = 3.04836 avg_loss = 2.99552\n",
      "epoch no.5 train no.377080  loss = 1.92611 avg_loss = 2.98950\n",
      "epoch no.5 train no.377090  loss = 3.01146 avg_loss = 2.96735\n",
      "epoch no.5 train no.377100  loss = 3.62788 avg_loss = 2.95160\n",
      "epoch no.5 train no.377110  loss = 2.62368 avg_loss = 2.94536\n",
      "epoch no.5 train no.377120  loss = 4.43185 avg_loss = 2.94766\n",
      "epoch no.5 train no.377130  loss = 2.14237 avg_loss = 2.92134\n",
      "epoch no.5 train no.377140  loss = 3.20751 avg_loss = 2.94528\n",
      "epoch no.5 train no.377150  loss = 1.49642 avg_loss = 2.91813\n",
      "epoch no.5 train no.377160  loss = 2.20017 avg_loss = 2.92412\n",
      "epoch no.5 train no.377170  loss = 2.49190 avg_loss = 2.96018\n",
      "epoch no.5 train no.377180  loss = 2.78108 avg_loss = 2.98757\n",
      "epoch no.5 train no.377190  loss = 3.35761 avg_loss = 2.97929\n",
      "epoch no.5 train no.377200  loss = 2.78325 avg_loss = 2.99449\n",
      "epoch no.5 train no.377210  loss = 3.56633 avg_loss = 2.98438\n",
      "epoch no.5 train no.377220  loss = 3.76299 avg_loss = 2.97253\n",
      "epoch no.5 train no.377230  loss = 1.82767 avg_loss = 2.93707\n",
      "epoch no.5 train no.377240  loss = 2.50167 avg_loss = 2.92888\n",
      "epoch no.5 train no.377250  loss = 2.05985 avg_loss = 2.91606\n",
      "epoch no.5 train no.377260  loss = 1.78498 avg_loss = 2.91638\n",
      "epoch no.5 train no.377270  loss = 1.97110 avg_loss = 2.91657\n",
      "epoch no.5 train no.377280  loss = 4.06318 avg_loss = 2.89866\n",
      "epoch no.5 train no.377290  loss = 3.48251 avg_loss = 2.90804\n",
      "epoch no.5 train no.377300  loss = 2.68469 avg_loss = 2.89593\n",
      "epoch no.5 train no.377310  loss = 2.38948 avg_loss = 2.89826\n",
      "epoch no.5 train no.377320  loss = 4.46379 avg_loss = 2.95434\n",
      "epoch no.5 train no.377330  loss = 3.68851 avg_loss = 2.96887\n",
      "epoch no.5 train no.377340  loss = 2.34860 avg_loss = 2.95003\n",
      "epoch no.5 train no.377350  loss = 3.84504 avg_loss = 2.91392\n",
      "epoch no.5 train no.377360  loss = 4.09489 avg_loss = 2.89398\n",
      "epoch no.5 train no.377370  loss = 3.90652 avg_loss = 2.91450\n",
      "epoch no.5 train no.377380  loss = 3.44424 avg_loss = 2.97454\n",
      "epoch no.5 train no.377390  loss = 3.07926 avg_loss = 2.99068\n",
      "epoch no.5 train no.377400  loss = 2.85040 avg_loss = 3.00013\n",
      "epoch no.5 train no.377410  loss = 2.39610 avg_loss = 3.00766\n",
      "epoch no.5 train no.377420  loss = 4.10012 avg_loss = 3.01206\n",
      "epoch no.5 train no.377430  loss = 1.96728 avg_loss = 2.99421\n",
      "epoch no.5 train no.377440  loss = 3.58091 avg_loss = 3.02021\n",
      "epoch no.5 train no.377450  loss = 3.63483 avg_loss = 2.99829\n",
      "epoch no.5 train no.377460  loss = 2.03375 avg_loss = 2.94220\n",
      "epoch no.5 train no.377470  loss = 2.74257 avg_loss = 2.94353\n",
      "epoch no.5 train no.377480  loss = 2.73280 avg_loss = 2.95918\n",
      "epoch no.5 train no.377490  loss = 2.38994 avg_loss = 2.95207\n",
      "epoch no.5 train no.377500  loss = 3.12772 avg_loss = 2.94767\n",
      "epoch no.5 train no.377510  loss = 3.27067 avg_loss = 2.97658\n",
      "epoch no.5 train no.377520  loss = 3.38352 avg_loss = 2.96199\n",
      "epoch no.5 train no.377530  loss = 3.19717 avg_loss = 2.94858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.377540  loss = 2.13981 avg_loss = 2.93408\n",
      "epoch no.5 train no.377550  loss = 4.31979 avg_loss = 2.95031\n",
      "epoch no.5 train no.377560  loss = 2.95649 avg_loss = 2.91689\n",
      "epoch no.5 train no.377570  loss = 2.25985 avg_loss = 2.91482\n",
      "epoch no.5 train no.377580  loss = 2.70921 avg_loss = 2.90302\n",
      "epoch no.5 train no.377590  loss = 2.14720 avg_loss = 2.91729\n",
      "epoch no.5 train no.377600  loss = 3.25530 avg_loss = 2.92364\n",
      "epoch no.5 train no.377610  loss = 1.19065 avg_loss = 2.91114\n",
      "epoch no.5 train no.377620  loss = 3.56246 avg_loss = 2.91897\n",
      "epoch no.5 train no.377630  loss = 1.85676 avg_loss = 2.89554\n",
      "epoch no.5 train no.377640  loss = 4.43780 avg_loss = 2.92786\n",
      "epoch no.5 train no.377650  loss = 1.38841 avg_loss = 2.92280\n",
      "epoch no.5 train no.377660  loss = 4.68552 avg_loss = 2.94593\n",
      "epoch no.5 train no.377670  loss = 2.77446 avg_loss = 2.95253\n",
      "epoch no.5 train no.377680  loss = 3.21029 avg_loss = 2.95431\n",
      "epoch no.5 train no.377690  loss = 2.19809 avg_loss = 2.92946\n",
      "epoch no.5 train no.377700  loss = 2.33121 avg_loss = 2.93745\n",
      "epoch no.5 train no.377710  loss = 1.92260 avg_loss = 2.97182\n",
      "epoch no.5 train no.377720  loss = 3.79301 avg_loss = 2.98260\n",
      "epoch no.5 train no.377730  loss = 2.91228 avg_loss = 2.95791\n",
      "epoch no.5 train no.377740  loss = 3.14741 avg_loss = 2.94012\n",
      "epoch no.5 train no.377750  loss = 2.76989 avg_loss = 2.92990\n",
      "epoch no.5 train no.377760  loss = 3.62822 avg_loss = 2.95074\n",
      "epoch no.5 train no.377770  loss = 2.77728 avg_loss = 2.98331\n",
      "epoch no.5 train no.377780  loss = 1.98762 avg_loss = 2.91588\n",
      "epoch no.5 train no.377790  loss = 2.39929 avg_loss = 2.88250\n",
      "epoch no.5 train no.377800  loss = 2.08316 avg_loss = 2.90170\n",
      "epoch no.5 train no.377810  loss = 2.60370 avg_loss = 2.91827\n",
      "epoch no.5 train no.377820  loss = 2.45947 avg_loss = 2.93253\n",
      "epoch no.5 train no.377830  loss = 2.88915 avg_loss = 2.92137\n",
      "epoch no.5 train no.377840  loss = 4.38518 avg_loss = 2.90893\n",
      "epoch no.5 train no.377850  loss = 5.90993 avg_loss = 2.94243\n",
      "epoch no.5 train no.377860  loss = 2.14354 avg_loss = 2.92747\n",
      "epoch no.5 train no.377870  loss = 3.38176 avg_loss = 2.94126\n",
      "epoch no.5 train no.377880  loss = 3.20794 avg_loss = 2.93268\n",
      "epoch no.5 train no.377890  loss = 2.03098 avg_loss = 2.94929\n",
      "epoch no.5 train no.377900  loss = 2.11531 avg_loss = 2.94949\n",
      "epoch no.5 train no.377910  loss = 2.55709 avg_loss = 2.93014\n",
      "epoch no.5 train no.377920  loss = 4.39354 avg_loss = 2.94596\n",
      "epoch no.5 train no.377930  loss = 3.08172 avg_loss = 2.93566\n",
      "epoch no.5 train no.377940  loss = 3.10585 avg_loss = 2.94345\n",
      "epoch no.5 train no.377950  loss = 3.74289 avg_loss = 2.90771\n",
      "epoch no.5 train no.377960  loss = 2.59684 avg_loss = 2.89982\n",
      "epoch no.5 train no.377970  loss = 2.74169 avg_loss = 2.90752\n",
      "epoch no.5 train no.377980  loss = 2.55443 avg_loss = 2.90087\n",
      "epoch no.5 train no.377990  loss = 3.39665 avg_loss = 2.89738\n",
      "epoch no.5 train no.378000  loss = 2.16275 avg_loss = 2.87823\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.5 train no.378010  loss = 2.41518 avg_loss = 2.85645\n",
      "epoch no.5 train no.378020  loss = 3.49798 avg_loss = 2.87954\n",
      "epoch no.5 train no.378030  loss = 4.28622 avg_loss = 2.93205\n",
      "epoch no.5 train no.378040  loss = 2.84733 avg_loss = 2.95058\n",
      "epoch no.5 train no.378050  loss = 2.60352 avg_loss = 2.96360\n",
      "epoch no.5 train no.378060  loss = 3.77906 avg_loss = 3.01171\n",
      "epoch no.5 train no.378070  loss = 3.20378 avg_loss = 2.96782\n",
      "epoch no.5 train no.378080  loss = 2.56955 avg_loss = 2.93258\n",
      "epoch no.5 train no.378090  loss = 3.57078 avg_loss = 2.98305\n",
      "epoch no.5 train no.378100  loss = 2.48948 avg_loss = 2.95198\n",
      "epoch no.5 train no.378110  loss = 2.83357 avg_loss = 2.94955\n",
      "epoch no.5 train no.378120  loss = 2.74534 avg_loss = 3.01413\n",
      "epoch no.5 train no.378130  loss = 4.13653 avg_loss = 3.00901\n",
      "epoch no.5 train no.378140  loss = 2.12911 avg_loss = 2.94729\n",
      "epoch no.5 train no.378150  loss = 2.99677 avg_loss = 2.96137\n",
      "epoch no.5 train no.378160  loss = 2.66801 avg_loss = 2.96557\n",
      "epoch no.5 train no.378170  loss = 1.40519 avg_loss = 2.95093\n",
      "epoch no.5 train no.378180  loss = 2.82623 avg_loss = 2.92131\n",
      "epoch no.5 train no.378190  loss = 2.74698 avg_loss = 2.91218\n",
      "epoch no.5 train no.378200  loss = 3.98250 avg_loss = 2.98315\n",
      "epoch no.5 train no.378210  loss = 2.23600 avg_loss = 2.92589\n",
      "epoch no.5 train no.378220  loss = 4.09134 avg_loss = 2.92170\n",
      "epoch no.5 train no.378230  loss = 2.44204 avg_loss = 2.90419\n",
      "epoch no.5 train no.378240  loss = 3.78342 avg_loss = 2.95218\n",
      "epoch no.5 train no.378250  loss = 3.20208 avg_loss = 2.97263\n",
      "epoch no.5 train no.378260  loss = 3.23031 avg_loss = 2.95733\n",
      "epoch no.5 train no.378270  loss = 3.80931 avg_loss = 2.93450\n",
      "epoch no.5 train no.378280  loss = 2.86613 avg_loss = 2.94017\n",
      "epoch no.5 train no.378290  loss = 4.73756 avg_loss = 2.98388\n",
      "epoch no.5 train no.378300  loss = 2.78883 avg_loss = 3.04382\n",
      "epoch no.5 train no.378310  loss = 3.61287 avg_loss = 3.00422\n",
      "epoch no.5 train no.378320  loss = 3.38708 avg_loss = 2.98003\n",
      "epoch no.5 train no.378330  loss = 2.97046 avg_loss = 2.98160\n",
      "epoch no.5 train no.378340  loss = 3.37457 avg_loss = 3.01232\n",
      "epoch no.5 train no.378350  loss = 3.33898 avg_loss = 2.99674\n",
      "epoch no.5 train no.378360  loss = 3.38494 avg_loss = 3.02895\n",
      "epoch no.5 train no.378370  loss = 4.14622 avg_loss = 3.06067\n",
      "epoch no.5 train no.378380  loss = 2.69145 avg_loss = 3.09274\n",
      "epoch no.5 train no.378390  loss = 3.87714 avg_loss = 3.09489\n",
      "epoch no.5 train no.378400  loss = 3.14970 avg_loss = 3.08462\n",
      "epoch no.5 train no.378410  loss = 2.73141 avg_loss = 3.07196\n",
      "epoch no.5 train no.378420  loss = 2.91450 avg_loss = 3.08441\n",
      "epoch no.5 train no.378430  loss = 3.66508 avg_loss = 3.07585\n",
      "epoch no.5 train no.378440  loss = 2.86911 avg_loss = 3.07337\n",
      "epoch no.5 train no.378450  loss = 2.95743 avg_loss = 3.05919\n",
      "epoch no.5 train no.378460  loss = 2.67076 avg_loss = 3.06697\n",
      "epoch no.5 train no.378470  loss = 1.89004 avg_loss = 3.02471\n",
      "epoch no.5 train no.378480  loss = 1.78727 avg_loss = 3.00485\n",
      "epoch no.5 train no.378490  loss = 3.24490 avg_loss = 2.98850\n",
      "epoch no.5 train no.378500  loss = 3.54066 avg_loss = 3.02248\n",
      "epoch no.5 train no.378510  loss = 3.50778 avg_loss = 3.02825\n",
      "epoch no.5 train no.378520  loss = 2.51613 avg_loss = 3.02608\n",
      "epoch no.5 train no.378530  loss = 3.14009 avg_loss = 2.98254\n",
      "epoch no.5 train no.378540  loss = 1.37905 avg_loss = 2.99014\n",
      "epoch no.5 train no.378550  loss = 3.36386 avg_loss = 3.02880\n",
      "epoch no.5 train no.378560  loss = 3.19806 avg_loss = 3.04311\n",
      "epoch no.5 train no.378570  loss = 2.83340 avg_loss = 3.06444\n",
      "epoch no.5 train no.378580  loss = 2.66938 avg_loss = 3.03306\n",
      "epoch no.5 train no.378590  loss = 1.93047 avg_loss = 3.04210\n",
      "epoch no.5 train no.378600  loss = 2.89235 avg_loss = 3.03541\n",
      "epoch no.5 train no.378610  loss = 2.31075 avg_loss = 3.00693\n",
      "epoch no.5 train no.378620  loss = 2.15492 avg_loss = 2.98536\n",
      "epoch no.5 train no.378630  loss = 3.65158 avg_loss = 2.98300\n",
      "epoch no.5 train no.378640  loss = 2.76826 avg_loss = 2.98155\n",
      "epoch no.5 train no.378650  loss = 2.56696 avg_loss = 2.96259\n",
      "epoch no.5 train no.378660  loss = 2.41430 avg_loss = 2.93761\n",
      "epoch no.5 train no.378670  loss = 3.45568 avg_loss = 2.92121\n",
      "epoch no.5 train no.378680  loss = 2.86779 avg_loss = 2.99669\n",
      "epoch no.5 train no.378690  loss = 2.43868 avg_loss = 3.03926\n",
      "epoch no.5 train no.378700  loss = 2.01328 avg_loss = 3.00361\n",
      "epoch no.5 train no.378710  loss = 3.33394 avg_loss = 2.99474\n",
      "epoch no.5 train no.378720  loss = 2.11548 avg_loss = 2.97841\n",
      "epoch no.5 train no.378730  loss = 5.03896 avg_loss = 2.97855\n",
      "epoch no.5 train no.378740  loss = 2.72670 avg_loss = 2.98905\n",
      "epoch no.5 train no.378750  loss = 3.84091 avg_loss = 3.03390\n",
      "epoch no.5 train no.378760  loss = 2.76972 avg_loss = 3.02591\n",
      "epoch no.5 train no.378770  loss = 2.36395 avg_loss = 3.01575\n",
      "epoch no.5 train no.378780  loss = 4.49899 avg_loss = 3.03294\n",
      "epoch no.5 train no.378790  loss = 2.26951 avg_loss = 3.01219\n",
      "epoch no.5 train no.378800  loss = 2.29236 avg_loss = 2.96951\n",
      "epoch no.5 train no.378810  loss = 2.98568 avg_loss = 2.97951\n",
      "epoch no.5 train no.378820  loss = 2.47449 avg_loss = 2.99184\n",
      "epoch no.5 train no.378830  loss = 2.10110 avg_loss = 3.00270\n",
      "epoch no.5 train no.378840  loss = 2.96392 avg_loss = 2.97505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.378850  loss = 2.87275 avg_loss = 2.95254\n",
      "epoch no.5 train no.378860  loss = 2.69750 avg_loss = 2.95310\n",
      "epoch no.5 train no.378870  loss = 3.16895 avg_loss = 2.97245\n",
      "epoch no.5 train no.378880  loss = 2.57720 avg_loss = 2.98398\n",
      "epoch no.5 train no.378890  loss = 2.12103 avg_loss = 3.02722\n",
      "epoch no.5 train no.378900  loss = 2.09073 avg_loss = 3.03076\n",
      "epoch no.5 train no.378910  loss = 3.28830 avg_loss = 2.98681\n",
      "epoch no.5 train no.378920  loss = 2.46757 avg_loss = 3.00036\n",
      "epoch no.5 train no.378930  loss = 2.50100 avg_loss = 3.01959\n",
      "epoch no.5 train no.378940  loss = 3.60013 avg_loss = 3.04650\n",
      "epoch no.5 train no.378950  loss = 2.30368 avg_loss = 3.04267\n",
      "epoch no.5 train no.378960  loss = 3.29462 avg_loss = 3.06710\n",
      "epoch no.5 train no.378970  loss = 3.91708 avg_loss = 3.05861\n",
      "epoch no.5 train no.378980  loss = 2.75488 avg_loss = 3.02825\n",
      "epoch no.5 train no.378990  loss = 2.44370 avg_loss = 3.03856\n",
      "epoch no.5 train no.379000  loss = 3.78793 avg_loss = 3.06149\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁산책', '▁좋은', '▁노래', '</s>']\n",
      "여름 밤 듣기 좋은 노래</s>\n",
      "epoch no.5 train no.379010  loss = 3.19924 avg_loss = 3.03313\n",
      "epoch no.5 train no.379020  loss = 1.85580 avg_loss = 3.03657\n",
      "epoch no.5 train no.379030  loss = 3.10786 avg_loss = 3.01072\n",
      "epoch no.5 train no.379040  loss = 3.38146 avg_loss = 3.01824\n",
      "epoch no.5 train no.379050  loss = 4.72977 avg_loss = 3.07223\n",
      "epoch no.5 train no.379060  loss = 2.63083 avg_loss = 3.07061\n",
      "epoch no.5 train no.379070  loss = 2.35500 avg_loss = 3.04472\n",
      "epoch no.5 train no.379080  loss = 2.67744 avg_loss = 2.99672\n",
      "epoch no.5 train no.379090  loss = 2.47991 avg_loss = 2.97382\n",
      "epoch no.5 train no.379100  loss = 2.72634 avg_loss = 2.94531\n",
      "epoch no.5 train no.379110  loss = 2.48892 avg_loss = 2.94766\n",
      "epoch no.5 train no.379120  loss = 4.85906 avg_loss = 3.00635\n",
      "epoch no.5 train no.379130  loss = 2.40604 avg_loss = 3.00185\n",
      "epoch no.5 train no.379140  loss = 2.43209 avg_loss = 3.01926\n",
      "epoch no.5 train no.379150  loss = 3.00540 avg_loss = 3.04875\n",
      "epoch no.5 train no.379160  loss = 4.54551 avg_loss = 3.06227\n",
      "epoch no.5 train no.379170  loss = 3.31286 avg_loss = 3.06050\n",
      "epoch no.5 train no.379180  loss = 2.20948 avg_loss = 3.05989\n",
      "epoch no.5 train no.379190  loss = 2.33377 avg_loss = 3.04834\n",
      "epoch no.5 train no.379200  loss = 3.01624 avg_loss = 3.04532\n",
      "epoch no.5 train no.379210  loss = 2.39049 avg_loss = 3.00957\n",
      "epoch no.5 train no.379220  loss = 2.70907 avg_loss = 2.95445\n",
      "epoch no.5 train no.379230  loss = 3.37261 avg_loss = 2.95988\n",
      "epoch no.5 train no.379240  loss = 2.67045 avg_loss = 2.91748\n",
      "epoch no.5 train no.379250  loss = 1.86609 avg_loss = 2.93195\n",
      "epoch no.5 train no.379260  loss = 2.50638 avg_loss = 2.92273\n",
      "epoch no.5 train no.379270  loss = 3.40988 avg_loss = 2.94969\n",
      "epoch no.5 train no.379280  loss = 2.02571 avg_loss = 2.99988\n",
      "epoch no.5 train no.379290  loss = 3.66367 avg_loss = 2.97176\n",
      "epoch no.5 train no.379300  loss = 3.59405 avg_loss = 2.99258\n",
      "epoch no.5 train no.379310  loss = 3.11052 avg_loss = 2.98906\n",
      "epoch no.5 train no.379320  loss = 3.08125 avg_loss = 2.99903\n",
      "epoch no.5 train no.379330  loss = 2.63608 avg_loss = 2.96020\n",
      "epoch no.5 train no.379340  loss = 3.85107 avg_loss = 2.94856\n",
      "epoch no.5 train no.379350  loss = 3.22313 avg_loss = 2.97095\n",
      "epoch no.5 train no.379360  loss = 2.91073 avg_loss = 2.97791\n",
      "epoch no.5 train no.379370  loss = 3.87833 avg_loss = 2.97835\n",
      "epoch no.5 train no.379380  loss = 3.31253 avg_loss = 3.00482\n",
      "epoch no.5 train no.379390  loss = 2.18551 avg_loss = 2.97248\n",
      "epoch no.5 train no.379400  loss = 2.99382 avg_loss = 2.99597\n",
      "epoch no.5 train no.379410  loss = 4.42113 avg_loss = 3.00653\n",
      "epoch no.5 train no.379420  loss = 3.00827 avg_loss = 3.02319\n",
      "epoch no.5 train no.379430  loss = 3.67951 avg_loss = 3.05578\n",
      "epoch no.5 train no.379440  loss = 2.12493 avg_loss = 3.02989\n",
      "epoch no.5 train no.379450  loss = 3.33273 avg_loss = 3.04621\n",
      "epoch no.5 train no.379460  loss = 1.95270 avg_loss = 3.02482\n",
      "epoch no.5 train no.379470  loss = 1.77183 avg_loss = 3.01752\n",
      "epoch no.5 train no.379480  loss = 2.58659 avg_loss = 2.99758\n",
      "epoch no.5 train no.379490  loss = 2.97884 avg_loss = 2.99731\n",
      "epoch no.5 train no.379500  loss = 2.47984 avg_loss = 2.96050\n",
      "epoch no.5 train no.379510  loss = 2.29756 avg_loss = 2.98090\n",
      "epoch no.5 train no.379520  loss = 3.71335 avg_loss = 2.99161\n",
      "epoch no.5 train no.379530  loss = 3.06176 avg_loss = 3.03932\n",
      "epoch no.5 train no.379540  loss = 3.38613 avg_loss = 3.04208\n",
      "epoch no.5 train no.379550  loss = 3.81915 avg_loss = 3.02872\n",
      "epoch no.5 train no.379560  loss = 2.39067 avg_loss = 3.03996\n",
      "epoch no.5 train no.379570  loss = 1.97578 avg_loss = 3.02231\n",
      "epoch no.5 train no.379580  loss = 1.46800 avg_loss = 2.96976\n",
      "epoch no.5 train no.379590  loss = 3.29828 avg_loss = 2.94599\n",
      "epoch no.5 train no.379600  loss = 2.98834 avg_loss = 2.93522\n",
      "epoch no.5 train no.379610  loss = 3.76550 avg_loss = 2.94622\n",
      "epoch no.5 train no.379620  loss = 2.44696 avg_loss = 2.91510\n",
      "epoch no.5 train no.379630  loss = 3.03688 avg_loss = 2.90729\n",
      "epoch no.5 train no.379640  loss = 1.56457 avg_loss = 2.89894\n",
      "epoch no.5 train no.379650  loss = 2.41439 avg_loss = 2.90949\n",
      "epoch no.5 train no.379660  loss = 1.91937 avg_loss = 2.90333\n",
      "epoch no.5 train no.379670  loss = 2.88450 avg_loss = 2.86623\n",
      "epoch no.5 train no.379680  loss = 3.50457 avg_loss = 2.87382\n",
      "epoch no.5 train no.379690  loss = 2.49461 avg_loss = 2.88167\n",
      "epoch no.5 train no.379700  loss = 2.38504 avg_loss = 2.90036\n",
      "epoch no.5 train no.379710  loss = 3.58893 avg_loss = 2.92210\n",
      "epoch no.5 train no.379720  loss = 2.24208 avg_loss = 2.89834\n",
      "epoch no.5 train no.379730  loss = 2.83090 avg_loss = 2.93632\n",
      "epoch no.5 train no.379740  loss = 2.81734 avg_loss = 2.94816\n",
      "epoch no.5 train no.379750  loss = 3.37959 avg_loss = 2.98047\n",
      "epoch no.5 train no.379760  loss = 2.96849 avg_loss = 3.04431\n",
      "epoch no.5 train no.379770  loss = 2.79524 avg_loss = 3.02485\n",
      "epoch no.5 train no.379780  loss = 2.44524 avg_loss = 3.02855\n",
      "epoch no.5 train no.379790  loss = 2.92398 avg_loss = 3.00320\n",
      "epoch no.5 train no.379800  loss = 3.93754 avg_loss = 3.00300\n",
      "epoch no.5 train no.379810  loss = 2.81918 avg_loss = 3.01233\n",
      "epoch no.5 train no.379820  loss = 2.38288 avg_loss = 2.97149\n",
      "epoch no.5 train no.379830  loss = 2.07294 avg_loss = 2.97358\n",
      "epoch no.5 train no.379840  loss = 3.07639 avg_loss = 2.94678\n",
      "epoch no.5 train no.379850  loss = 3.25576 avg_loss = 2.98628\n",
      "epoch no.5 train no.379860  loss = 2.76673 avg_loss = 2.99107\n",
      "epoch no.5 train no.379870  loss = 1.83580 avg_loss = 2.96389\n",
      "epoch no.5 train no.379880  loss = 2.16228 avg_loss = 2.96060\n",
      "epoch no.5 train no.379890  loss = 2.91841 avg_loss = 2.93985\n",
      "epoch no.5 train no.379900  loss = 1.69020 avg_loss = 2.90444\n",
      "epoch no.5 train no.379910  loss = 3.84720 avg_loss = 2.92892\n",
      "epoch no.5 train no.379920  loss = 2.41441 avg_loss = 2.90225\n",
      "epoch no.5 train no.379930  loss = 5.29907 avg_loss = 2.97996\n",
      "epoch no.5 train no.379940  loss = 2.78026 avg_loss = 3.01418\n",
      "epoch no.5 train no.379950  loss = 2.80898 avg_loss = 3.02743\n",
      "epoch no.5 train no.379960  loss = 2.26355 avg_loss = 3.05294\n",
      "epoch no.5 train no.379970  loss = 2.08133 avg_loss = 3.01768\n",
      "epoch no.5 train no.379980  loss = 1.33767 avg_loss = 3.05807\n",
      "epoch no.5 train no.379990  loss = 2.48708 avg_loss = 3.02275\n",
      "epoch no.5 train no.380000  loss = 3.87672 avg_loss = 3.03501\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁가기', '▁전에', '</s>', '을', '▁하고', '▁때', '</s>']\n",
      "여름이 가기 전에 이별을 할 때</s>\n",
      "epoch no.5 train no.380010  loss = 2.77721 avg_loss = 3.00812\n",
      "epoch no.5 train no.380020  loss = 3.33435 avg_loss = 2.96156\n",
      "epoch no.5 train no.380030  loss = 3.14715 avg_loss = 2.96672\n",
      "epoch no.5 train no.380040  loss = 3.40369 avg_loss = 2.96100\n",
      "epoch no.5 train no.380050  loss = 2.68904 avg_loss = 2.98919\n",
      "epoch no.5 train no.380060  loss = 2.78547 avg_loss = 2.98651\n",
      "epoch no.5 train no.380070  loss = 3.07204 avg_loss = 2.97332\n",
      "epoch no.5 train no.380080  loss = 3.59357 avg_loss = 3.00424\n",
      "epoch no.5 train no.380090  loss = 4.43621 avg_loss = 3.00078\n",
      "epoch no.5 train no.380100  loss = 2.53287 avg_loss = 2.97290\n",
      "epoch no.5 train no.380110  loss = 3.38955 avg_loss = 2.92405\n",
      "epoch no.5 train no.380120  loss = 2.31437 avg_loss = 2.94583\n",
      "epoch no.5 train no.380130  loss = 2.23714 avg_loss = 2.96521\n",
      "epoch no.5 train no.380140  loss = 4.44039 avg_loss = 3.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.380150  loss = 4.36704 avg_loss = 2.98516\n",
      "epoch no.5 train no.380160  loss = 2.32462 avg_loss = 3.01427\n",
      "epoch no.5 train no.380170  loss = 3.55449 avg_loss = 3.04120\n",
      "epoch no.5 train no.380180  loss = 2.31533 avg_loss = 3.04034\n",
      "epoch no.5 train no.380190  loss = 2.67429 avg_loss = 2.99124\n",
      "epoch no.5 train no.380200  loss = 3.21392 avg_loss = 2.99384\n",
      "epoch no.5 train no.380210  loss = 2.62880 avg_loss = 2.97355\n",
      "epoch no.5 train no.380220  loss = 2.65652 avg_loss = 2.97777\n",
      "epoch no.5 train no.380230  loss = 4.01817 avg_loss = 2.99040\n",
      "epoch no.5 train no.380240  loss = 3.04135 avg_loss = 3.00649\n",
      "epoch no.5 train no.380250  loss = 3.33065 avg_loss = 3.02295\n",
      "epoch no.5 train no.380260  loss = 2.64685 avg_loss = 2.99942\n",
      "epoch no.5 train no.380270  loss = 2.99726 avg_loss = 2.98291\n",
      "epoch no.5 train no.380280  loss = 3.14898 avg_loss = 2.94835\n",
      "epoch no.5 train no.380290  loss = 2.85899 avg_loss = 2.97748\n",
      "epoch no.5 train no.380300  loss = 3.19906 avg_loss = 2.94850\n",
      "epoch no.5 train no.380310  loss = 2.39899 avg_loss = 2.91325\n",
      "epoch no.5 train no.380320  loss = 1.74536 avg_loss = 2.93372\n",
      "epoch no.5 train no.380330  loss = 2.76712 avg_loss = 2.92421\n",
      "epoch no.5 train no.380340  loss = 3.69144 avg_loss = 2.91693\n",
      "epoch no.5 train no.380350  loss = 2.82163 avg_loss = 2.90190\n",
      "epoch no.5 train no.380360  loss = 2.31522 avg_loss = 2.88284\n",
      "epoch no.5 train no.380370  loss = 3.91628 avg_loss = 2.86301\n",
      "epoch no.5 train no.380380  loss = 4.04553 avg_loss = 2.90859\n",
      "epoch no.5 train no.380390  loss = 3.54486 avg_loss = 2.95972\n",
      "epoch no.5 train no.380400  loss = 2.48638 avg_loss = 2.96115\n",
      "epoch no.5 train no.380410  loss = 2.01664 avg_loss = 2.94525\n",
      "epoch no.5 train no.380420  loss = 3.31907 avg_loss = 2.97026\n",
      "epoch no.5 train no.380430  loss = 2.13017 avg_loss = 2.96039\n",
      "epoch no.5 train no.380440  loss = 2.48272 avg_loss = 2.94270\n",
      "epoch no.5 train no.380450  loss = 2.01262 avg_loss = 2.97700\n",
      "epoch no.5 train no.380460  loss = 1.56439 avg_loss = 2.93341\n",
      "epoch no.5 train no.380470  loss = 3.14460 avg_loss = 2.93170\n",
      "epoch no.5 train no.380480  loss = 1.93277 avg_loss = 2.94987\n",
      "epoch no.5 train no.380490  loss = 2.40368 avg_loss = 2.95514\n",
      "epoch no.5 train no.380500  loss = 1.98261 avg_loss = 2.93700\n",
      "epoch no.5 train no.380510  loss = 2.95287 avg_loss = 2.93660\n",
      "epoch no.5 train no.380520  loss = 2.69805 avg_loss = 2.94278\n",
      "epoch no.5 train no.380530  loss = 3.45713 avg_loss = 2.93377\n",
      "epoch no.5 train no.380540  loss = 1.99608 avg_loss = 2.90068\n",
      "epoch no.5 train no.380550  loss = 2.80678 avg_loss = 2.89268\n",
      "epoch no.5 train no.380560  loss = 3.40857 avg_loss = 2.88009\n",
      "epoch no.5 train no.380570  loss = 2.92894 avg_loss = 2.92404\n",
      "epoch no.5 train no.380580  loss = 3.43375 avg_loss = 2.93468\n",
      "epoch no.5 train no.380590  loss = 2.84638 avg_loss = 2.92085\n",
      "epoch no.5 train no.380600  loss = 4.02675 avg_loss = 2.95512\n",
      "epoch no.5 train no.380610  loss = 3.26287 avg_loss = 2.98092\n",
      "epoch no.5 train no.380620  loss = 3.53008 avg_loss = 2.96644\n",
      "epoch no.5 train no.380630  loss = 2.52150 avg_loss = 2.95854\n",
      "epoch no.5 train no.380640  loss = 2.13304 avg_loss = 2.96519\n",
      "epoch no.5 train no.380650  loss = 2.22767 avg_loss = 2.98609\n",
      "epoch no.5 train no.380660  loss = 4.04190 avg_loss = 2.97549\n",
      "epoch no.5 train no.380670  loss = 2.54927 avg_loss = 2.98353\n",
      "epoch no.5 train no.380680  loss = 2.74193 avg_loss = 2.97228\n",
      "epoch no.5 train no.380690  loss = 2.41034 avg_loss = 2.98348\n",
      "epoch no.5 train no.380700  loss = 3.94321 avg_loss = 2.96960\n",
      "epoch no.5 train no.380710  loss = 3.70458 avg_loss = 2.97659\n",
      "epoch no.5 train no.380720  loss = 2.72809 avg_loss = 2.98274\n",
      "epoch no.5 train no.380730  loss = 2.77945 avg_loss = 2.97170\n",
      "epoch no.5 train no.380740  loss = 1.50612 avg_loss = 2.93664\n",
      "epoch no.5 train no.380750  loss = 3.13004 avg_loss = 2.92643\n",
      "epoch no.5 train no.380760  loss = 3.22340 avg_loss = 2.96848\n",
      "epoch no.5 train no.380770  loss = 4.22254 avg_loss = 2.98710\n",
      "epoch no.5 train no.380780  loss = 3.09547 avg_loss = 3.03595\n",
      "epoch no.5 train no.380790  loss = 3.59089 avg_loss = 3.05961\n",
      "epoch no.5 train no.380800  loss = 3.27976 avg_loss = 3.05912\n",
      "epoch no.5 train no.380810  loss = 3.08334 avg_loss = 3.05025\n",
      "epoch no.5 train no.380820  loss = 3.30769 avg_loss = 3.04848\n",
      "epoch no.5 train no.380830  loss = 3.11221 avg_loss = 3.06500\n",
      "epoch no.5 train no.380840  loss = 3.50613 avg_loss = 3.08557\n",
      "epoch no.5 train no.380850  loss = 2.78236 avg_loss = 3.09608\n",
      "epoch no.5 train no.380860  loss = 3.17117 avg_loss = 3.06175\n",
      "epoch no.5 train no.380870  loss = 2.64085 avg_loss = 3.05315\n",
      "epoch no.5 train no.380880  loss = 2.95996 avg_loss = 3.04188\n",
      "epoch no.5 train no.380890  loss = 3.48317 avg_loss = 3.06154\n",
      "epoch no.5 train no.380900  loss = 4.21268 avg_loss = 3.07723\n",
      "epoch no.5 train no.380910  loss = 3.33612 avg_loss = 3.06311\n",
      "epoch no.5 train no.380920  loss = 3.26138 avg_loss = 3.06520\n",
      "epoch no.5 train no.380930  loss = 4.33446 avg_loss = 3.04594\n",
      "epoch no.5 train no.380940  loss = 1.92008 avg_loss = 3.02448\n",
      "epoch no.5 train no.380950  loss = 2.62436 avg_loss = 3.01378\n",
      "epoch no.5 train no.380960  loss = 4.06946 avg_loss = 3.03685\n",
      "epoch no.5 train no.380970  loss = 2.29674 avg_loss = 3.01719\n",
      "epoch no.5 train no.380980  loss = 3.35130 avg_loss = 2.98117\n",
      "epoch no.5 train no.380990  loss = 2.95713 avg_loss = 2.98941\n",
      "epoch no.5 train no.381000  loss = 4.00875 avg_loss = 2.98748\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁꿀', '▁감성', '</s>']\n",
      "여름밤의 포근한 음악</s>\n",
      "epoch no.5 train no.381010  loss = 2.52663 avg_loss = 2.97850\n",
      "epoch no.5 train no.381020  loss = 2.70005 avg_loss = 2.96005\n",
      "epoch no.5 train no.381030  loss = 3.50224 avg_loss = 2.95255\n",
      "epoch no.5 train no.381040  loss = 3.11077 avg_loss = 2.94081\n",
      "epoch no.5 train no.381050  loss = 3.00827 avg_loss = 2.95042\n",
      "epoch no.5 train no.381060  loss = 2.65731 avg_loss = 2.94426\n",
      "epoch no.5 train no.381070  loss = 2.37859 avg_loss = 2.94698\n",
      "epoch no.5 train no.381080  loss = 3.71211 avg_loss = 2.98272\n",
      "epoch no.5 train no.381090  loss = 3.03867 avg_loss = 2.98495\n",
      "epoch no.5 train no.381100  loss = 4.57114 avg_loss = 3.01870\n",
      "epoch no.5 train no.381110  loss = 2.30592 avg_loss = 3.04194\n",
      "epoch no.5 train no.381120  loss = 3.29034 avg_loss = 3.10145\n",
      "epoch no.5 train no.381130  loss = 2.60595 avg_loss = 3.10174\n",
      "epoch no.5 train no.381140  loss = 3.21712 avg_loss = 3.09782\n",
      "epoch no.5 train no.381150  loss = 2.78881 avg_loss = 3.06283\n",
      "epoch no.5 train no.381160  loss = 3.95748 avg_loss = 3.09439\n",
      "epoch no.5 train no.381170  loss = 2.66265 avg_loss = 3.07778\n",
      "epoch no.5 train no.381180  loss = 2.46327 avg_loss = 3.05448\n",
      "epoch no.5 train no.381190  loss = 3.09187 avg_loss = 3.06893\n",
      "epoch no.5 train no.381200  loss = 2.61921 avg_loss = 3.10140\n",
      "epoch no.5 train no.381210  loss = 2.30561 avg_loss = 3.07682\n",
      "epoch no.5 train no.381220  loss = 2.54232 avg_loss = 3.04216\n",
      "epoch no.5 train no.381230  loss = 2.91681 avg_loss = 3.07579\n",
      "epoch no.5 train no.381240  loss = 3.35452 avg_loss = 3.07861\n",
      "epoch no.5 train no.381250  loss = 3.04604 avg_loss = 3.09109\n",
      "epoch no.5 train no.381260  loss = 2.58377 avg_loss = 3.06409\n",
      "epoch no.5 train no.381270  loss = 2.30017 avg_loss = 3.06993\n",
      "epoch no.5 train no.381280  loss = 3.50683 avg_loss = 3.05752\n",
      "epoch no.5 train no.381290  loss = 2.38184 avg_loss = 3.00416\n",
      "epoch no.5 train no.381300  loss = 3.07319 avg_loss = 2.99292\n",
      "epoch no.5 train no.381310  loss = 3.13882 avg_loss = 3.03048\n",
      "epoch no.5 train no.381320  loss = 3.43421 avg_loss = 3.07581\n",
      "epoch no.5 train no.381330  loss = 2.96386 avg_loss = 3.04976\n",
      "epoch no.5 train no.381340  loss = 4.16276 avg_loss = 3.09084\n",
      "epoch no.5 train no.381350  loss = 3.03546 avg_loss = 3.03163\n",
      "epoch no.5 train no.381360  loss = 4.39282 avg_loss = 3.03128\n",
      "epoch no.5 train no.381370  loss = 2.06309 avg_loss = 3.03027\n",
      "epoch no.5 train no.381380  loss = 2.98081 avg_loss = 3.00001\n",
      "epoch no.5 train no.381390  loss = 3.72764 avg_loss = 3.05175\n",
      "epoch no.5 train no.381400  loss = 4.87991 avg_loss = 3.10192\n",
      "epoch no.5 train no.381410  loss = 2.42174 avg_loss = 3.11359\n",
      "epoch no.5 train no.381420  loss = 3.10434 avg_loss = 3.09614\n",
      "epoch no.5 train no.381430  loss = 4.78240 avg_loss = 3.10708\n",
      "epoch no.5 train no.381440  loss = 2.59011 avg_loss = 3.07553\n",
      "epoch no.5 train no.381450  loss = 5.36788 avg_loss = 3.09244\n",
      "epoch no.5 train no.381460  loss = 2.56808 avg_loss = 3.09659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.381470  loss = 1.97996 avg_loss = 3.09468\n",
      "epoch no.5 train no.381480  loss = 2.15762 avg_loss = 3.10116\n",
      "epoch no.5 train no.381490  loss = 2.96381 avg_loss = 3.12572\n",
      "epoch no.5 train no.381500  loss = 3.11043 avg_loss = 3.08562\n",
      "epoch no.5 train no.381510  loss = 2.64884 avg_loss = 3.09487\n",
      "epoch no.5 train no.381520  loss = 4.69345 avg_loss = 3.07051\n",
      "epoch no.5 train no.381530  loss = 2.04796 avg_loss = 3.03627\n",
      "epoch no.5 train no.381540  loss = 3.90565 avg_loss = 3.07203\n",
      "epoch no.5 train no.381550  loss = 3.12521 avg_loss = 3.05711\n",
      "epoch no.5 train no.381560  loss = 3.03122 avg_loss = 3.02060\n",
      "epoch no.5 train no.381570  loss = 1.72947 avg_loss = 3.00475\n",
      "epoch no.5 train no.381580  loss = 2.25675 avg_loss = 3.01863\n",
      "epoch no.5 train no.381590  loss = 2.99219 avg_loss = 3.05710\n",
      "epoch no.5 train no.381600  loss = 4.38255 avg_loss = 3.03964\n",
      "epoch no.5 train no.381610  loss = 2.09502 avg_loss = 2.99758\n",
      "epoch no.5 train no.381620  loss = 2.63036 avg_loss = 3.03234\n",
      "epoch no.5 train no.381630  loss = 2.77036 avg_loss = 3.02637\n",
      "epoch no.5 train no.381640  loss = 2.86850 avg_loss = 3.00186\n",
      "epoch no.5 train no.381650  loss = 3.47898 avg_loss = 3.03062\n",
      "epoch no.5 train no.381660  loss = 3.72474 avg_loss = 3.02013\n",
      "epoch no.5 train no.381670  loss = 2.51947 avg_loss = 2.98550\n",
      "epoch no.5 train no.381680  loss = 2.99507 avg_loss = 2.95872\n",
      "epoch no.5 train no.381690  loss = 3.58264 avg_loss = 2.94899\n",
      "epoch no.5 train no.381700  loss = 4.43185 avg_loss = 2.95251\n",
      "epoch no.5 train no.381710  loss = 1.87162 avg_loss = 2.95114\n",
      "epoch no.5 train no.381720  loss = 3.29059 avg_loss = 2.96705\n",
      "epoch no.5 train no.381730  loss = 2.19893 avg_loss = 2.99708\n",
      "epoch no.5 train no.381740  loss = 5.05713 avg_loss = 2.97250\n",
      "epoch no.5 train no.381750  loss = 2.52529 avg_loss = 2.96098\n",
      "epoch no.5 train no.381760  loss = 4.26273 avg_loss = 2.96690\n",
      "epoch no.5 train no.381770  loss = 3.64597 avg_loss = 3.00723\n",
      "epoch no.5 train no.381780  loss = 3.05843 avg_loss = 2.96709\n",
      "epoch no.5 train no.381790  loss = 2.21409 avg_loss = 2.95642\n",
      "epoch no.5 train no.381800  loss = 4.00437 avg_loss = 2.95010\n",
      "epoch no.5 train no.381810  loss = 2.57879 avg_loss = 2.92941\n",
      "epoch no.5 train no.381820  loss = 3.99470 avg_loss = 2.90204\n",
      "epoch no.5 train no.381830  loss = 2.11550 avg_loss = 2.92100\n",
      "epoch no.5 train no.381840  loss = 2.13271 avg_loss = 2.90753\n",
      "epoch no.5 train no.381850  loss = 3.49213 avg_loss = 2.91911\n",
      "epoch no.5 train no.381860  loss = 2.59651 avg_loss = 2.91065\n",
      "epoch no.5 train no.381870  loss = 2.75434 avg_loss = 2.90997\n",
      "epoch no.5 train no.381880  loss = 2.86474 avg_loss = 2.88953\n",
      "epoch no.5 train no.381890  loss = 2.89641 avg_loss = 2.90501\n",
      "epoch no.5 train no.381900  loss = 2.17555 avg_loss = 2.88133\n",
      "epoch no.5 train no.381910  loss = 3.44487 avg_loss = 2.91544\n",
      "epoch no.5 train no.381920  loss = 1.16463 avg_loss = 2.89250\n",
      "epoch no.5 train no.381930  loss = 2.82294 avg_loss = 2.92045\n",
      "epoch no.5 train no.381940  loss = 2.80798 avg_loss = 2.88410\n",
      "epoch no.5 train no.381950  loss = 3.31368 avg_loss = 2.91990\n",
      "epoch no.5 train no.381960  loss = 2.89106 avg_loss = 2.93463\n",
      "epoch no.5 train no.381970  loss = 3.97547 avg_loss = 2.95152\n",
      "epoch no.5 train no.381980  loss = 1.99979 avg_loss = 2.94833\n",
      "epoch no.5 train no.381990  loss = 3.43182 avg_loss = 2.96064\n",
      "epoch no.5 train no.382000  loss = 4.00745 avg_loss = 2.92189\n",
      "10\n",
      "to_tokens: ['▁비', '밤', '니까', '▁가기', '도', '▁록', '▁생각', '버', '▁시원한', '노래', '리스트', '</s>']\n",
      "여름이 다가와도구가 되어줄 여름 플레이리스트</s>\n",
      "epoch no.5 train no.382010  loss = 2.33283 avg_loss = 2.93952\n",
      "epoch no.5 train no.382020  loss = 2.58524 avg_loss = 2.94376\n",
      "epoch no.5 train no.382030  loss = 3.17914 avg_loss = 2.97387\n",
      "epoch no.5 train no.382040  loss = 3.22520 avg_loss = 2.96441\n",
      "epoch no.5 train no.382050  loss = 3.04956 avg_loss = 3.00374\n",
      "epoch no.5 train no.382060  loss = 2.80893 avg_loss = 3.03598\n",
      "epoch no.5 train no.382070  loss = 2.42206 avg_loss = 3.05254\n",
      "epoch no.5 train no.382080  loss = 4.08310 avg_loss = 3.07141\n",
      "epoch no.5 train no.382090  loss = 2.50623 avg_loss = 3.09935\n",
      "epoch no.5 train no.382100  loss = 3.72680 avg_loss = 3.09703\n",
      "epoch no.5 train no.382110  loss = 2.44015 avg_loss = 3.11236\n",
      "epoch no.5 train no.382120  loss = 2.25969 avg_loss = 3.07205\n",
      "epoch no.5 train no.382130  loss = 2.88336 avg_loss = 3.07140\n",
      "epoch no.5 train no.382140  loss = 2.14665 avg_loss = 3.04918\n",
      "epoch no.5 train no.382150  loss = 2.92450 avg_loss = 3.04213\n",
      "epoch no.5 train no.382160  loss = 3.17454 avg_loss = 3.06381\n",
      "epoch no.5 train no.382170  loss = 2.75239 avg_loss = 3.02769\n",
      "epoch no.5 train no.382180  loss = 2.38189 avg_loss = 3.01169\n",
      "epoch no.5 train no.382190  loss = 2.85238 avg_loss = 3.03470\n",
      "epoch no.5 train no.382200  loss = 3.86905 avg_loss = 3.04880\n",
      "epoch no.5 train no.382210  loss = 2.18506 avg_loss = 2.99789\n",
      "epoch no.5 train no.382220  loss = 1.67272 avg_loss = 2.95074\n",
      "epoch no.5 train no.382230  loss = 2.56003 avg_loss = 2.96926\n",
      "epoch no.5 train no.382240  loss = 3.66600 avg_loss = 2.95543\n",
      "epoch no.5 train no.382250  loss = 3.07296 avg_loss = 2.95564\n",
      "epoch no.5 train no.382260  loss = 2.45608 avg_loss = 2.97563\n",
      "epoch no.5 train no.382270  loss = 3.68051 avg_loss = 2.97964\n",
      "epoch no.5 train no.382280  loss = 2.43624 avg_loss = 2.97303\n",
      "epoch no.5 train no.382290  loss = 2.59113 avg_loss = 2.99278\n",
      "epoch no.5 train no.382300  loss = 3.05578 avg_loss = 3.01517\n",
      "epoch no.5 train no.382310  loss = 4.85206 avg_loss = 3.03331\n",
      "epoch no.5 train no.382320  loss = 2.63626 avg_loss = 3.03203\n",
      "epoch no.5 train no.382330  loss = 3.83036 avg_loss = 3.10483\n",
      "epoch no.5 train no.382340  loss = 3.13081 avg_loss = 3.12640\n",
      "epoch no.5 train no.382350  loss = 2.61407 avg_loss = 3.09286\n",
      "epoch no.5 train no.382360  loss = 3.76873 avg_loss = 3.05793\n",
      "epoch no.5 train no.382370  loss = 2.61203 avg_loss = 3.06480\n",
      "epoch no.5 train no.382380  loss = 1.94342 avg_loss = 3.04605\n",
      "epoch no.5 train no.382390  loss = 3.98812 avg_loss = 3.02758\n",
      "epoch no.5 train no.382400  loss = 2.56777 avg_loss = 3.00002\n",
      "epoch no.5 train no.382410  loss = 3.28216 avg_loss = 3.00429\n",
      "epoch no.5 train no.382420  loss = 2.63411 avg_loss = 2.98659\n",
      "epoch no.5 train no.382430  loss = 1.94297 avg_loss = 2.95736\n",
      "epoch no.5 train no.382440  loss = 2.17922 avg_loss = 3.02157\n",
      "epoch no.5 train no.382450  loss = 2.44560 avg_loss = 2.99238\n",
      "epoch no.5 train no.382460  loss = 2.20726 avg_loss = 2.96222\n",
      "epoch no.5 train no.382470  loss = 2.99804 avg_loss = 2.98171\n",
      "epoch no.5 train no.382480  loss = 2.86864 avg_loss = 2.99200\n",
      "epoch no.5 train no.382490  loss = 4.11352 avg_loss = 2.97699\n",
      "epoch no.5 train no.382500  loss = 2.36284 avg_loss = 2.97755\n",
      "epoch no.5 train no.382510  loss = 2.30699 avg_loss = 2.96456\n",
      "epoch no.5 train no.382520  loss = 1.80369 avg_loss = 2.93535\n",
      "epoch no.5 train no.382530  loss = 2.69773 avg_loss = 2.95373\n",
      "epoch no.5 train no.382540  loss = 4.27732 avg_loss = 3.00822\n",
      "epoch no.5 train no.382550  loss = 2.74988 avg_loss = 3.01239\n",
      "epoch no.5 train no.382560  loss = 1.29742 avg_loss = 3.00731\n",
      "epoch no.5 train no.382570  loss = 3.50626 avg_loss = 3.02306\n",
      "epoch no.5 train no.382580  loss = 2.33625 avg_loss = 2.98916\n",
      "epoch no.5 train no.382590  loss = 1.45769 avg_loss = 2.96185\n",
      "epoch no.5 train no.382600  loss = 3.82426 avg_loss = 2.95689\n",
      "epoch no.5 train no.382610  loss = 3.29379 avg_loss = 2.94562\n",
      "epoch no.5 train no.382620  loss = 2.61145 avg_loss = 2.92440\n",
      "epoch no.5 train no.382630  loss = 1.92013 avg_loss = 2.90745\n",
      "epoch no.5 train no.382640  loss = 1.96922 avg_loss = 2.90129\n",
      "epoch no.5 train no.382650  loss = 2.34372 avg_loss = 2.90066\n",
      "epoch no.5 train no.382660  loss = 3.16081 avg_loss = 2.92477\n",
      "epoch no.5 train no.382670  loss = 2.39368 avg_loss = 2.91107\n",
      "epoch no.5 train no.382680  loss = 2.22953 avg_loss = 2.90608\n",
      "epoch no.5 train no.382690  loss = 3.82191 avg_loss = 2.92423\n",
      "epoch no.5 train no.382700  loss = 2.11995 avg_loss = 2.88428\n",
      "epoch no.5 train no.382710  loss = 2.72974 avg_loss = 2.87650\n",
      "epoch no.5 train no.382720  loss = 2.47927 avg_loss = 2.84732\n",
      "epoch no.5 train no.382730  loss = 2.55164 avg_loss = 2.86004\n",
      "epoch no.5 train no.382740  loss = 4.70228 avg_loss = 2.94847\n",
      "epoch no.5 train no.382750  loss = 4.31312 avg_loss = 2.92797\n",
      "epoch no.5 train no.382760  loss = 2.31851 avg_loss = 2.93451\n",
      "epoch no.5 train no.382770  loss = 2.08291 avg_loss = 2.90344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.382780  loss = 3.28105 avg_loss = 2.94983\n",
      "epoch no.5 train no.382790  loss = 2.48984 avg_loss = 2.95802\n",
      "epoch no.5 train no.382800  loss = 3.49750 avg_loss = 2.94663\n",
      "epoch no.5 train no.382810  loss = 2.82115 avg_loss = 2.93948\n",
      "epoch no.5 train no.382820  loss = 3.29511 avg_loss = 2.96047\n",
      "epoch no.5 train no.382830  loss = 4.01465 avg_loss = 2.97157\n",
      "epoch no.5 train no.382840  loss = 2.54061 avg_loss = 2.99257\n",
      "epoch no.5 train no.382850  loss = 2.72304 avg_loss = 2.98740\n",
      "epoch no.5 train no.382860  loss = 2.00673 avg_loss = 3.01585\n",
      "epoch no.5 train no.382870  loss = 2.52152 avg_loss = 2.97917\n",
      "epoch no.5 train no.382880  loss = 4.18101 avg_loss = 2.98859\n",
      "epoch no.5 train no.382890  loss = 2.86584 avg_loss = 2.99515\n",
      "epoch no.5 train no.382900  loss = 3.38686 avg_loss = 2.95757\n",
      "epoch no.5 train no.382910  loss = 2.85949 avg_loss = 2.94924\n",
      "epoch no.5 train no.382920  loss = 2.59965 avg_loss = 2.96069\n",
      "epoch no.5 train no.382930  loss = 3.64016 avg_loss = 2.94507\n",
      "epoch no.5 train no.382940  loss = 3.55630 avg_loss = 2.96727\n",
      "epoch no.5 train no.382950  loss = 3.88641 avg_loss = 2.99327\n",
      "epoch no.5 train no.382960  loss = 2.56980 avg_loss = 2.98160\n",
      "epoch no.5 train no.382970  loss = 3.78440 avg_loss = 3.03209\n",
      "epoch no.5 train no.382980  loss = 4.60507 avg_loss = 3.04559\n",
      "epoch no.5 train no.382990  loss = 4.51829 avg_loss = 3.08959\n",
      "epoch no.5 train no.383000  loss = 3.15887 avg_loss = 3.08535\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁더욱', '셔', '줄', '▁감성', '</s>']\n",
      "여름밤을 적셔줄 노래</s>\n",
      "epoch no.5 train no.383010  loss = 3.15151 avg_loss = 3.10866\n",
      "epoch no.5 train no.383020  loss = 1.93419 avg_loss = 3.06418\n",
      "epoch no.5 train no.383030  loss = 2.76754 avg_loss = 3.03349\n",
      "epoch no.5 train no.383040  loss = 3.73021 avg_loss = 3.02811\n",
      "epoch no.5 train no.383050  loss = 2.94354 avg_loss = 3.02949\n",
      "epoch no.5 train no.383060  loss = 2.84950 avg_loss = 3.03617\n",
      "epoch no.5 train no.383070  loss = 3.52777 avg_loss = 3.00046\n",
      "epoch no.5 train no.383080  loss = 3.03458 avg_loss = 3.00161\n",
      "epoch no.5 train no.383090  loss = 3.50155 avg_loss = 3.01864\n",
      "epoch no.5 train no.383100  loss = 3.80280 avg_loss = 3.01009\n",
      "epoch no.5 train no.383110  loss = 1.93031 avg_loss = 2.98540\n",
      "epoch no.5 train no.383120  loss = 3.28876 avg_loss = 3.03248\n",
      "epoch no.5 train no.383130  loss = 2.43549 avg_loss = 3.06967\n",
      "epoch no.5 train no.383140  loss = 4.16864 avg_loss = 3.11263\n",
      "epoch no.5 train no.383150  loss = 2.65082 avg_loss = 3.12503\n",
      "epoch no.5 train no.383160  loss = 3.14990 avg_loss = 3.08531\n",
      "epoch no.5 train no.383170  loss = 4.34866 avg_loss = 3.08317\n",
      "epoch no.5 train no.383180  loss = 4.14115 avg_loss = 3.05068\n",
      "epoch no.5 train no.383190  loss = 3.60139 avg_loss = 3.07219\n",
      "epoch no.5 train no.383200  loss = 2.98377 avg_loss = 3.08543\n",
      "epoch no.5 train no.383210  loss = 2.99376 avg_loss = 3.07592\n",
      "epoch no.5 train no.383220  loss = 2.51629 avg_loss = 3.07930\n",
      "epoch no.5 train no.383230  loss = 2.63644 avg_loss = 3.06665\n",
      "epoch no.5 train no.383240  loss = 4.56215 avg_loss = 3.06332\n",
      "epoch no.5 train no.383250  loss = 2.85253 avg_loss = 3.10423\n",
      "epoch no.5 train no.383260  loss = 3.56247 avg_loss = 3.08232\n",
      "epoch no.5 train no.383270  loss = 2.57261 avg_loss = 3.07829\n",
      "epoch no.5 train no.383280  loss = 2.99812 avg_loss = 3.10233\n",
      "epoch no.5 train no.383290  loss = 3.86358 avg_loss = 3.09082\n",
      "epoch no.5 train no.383300  loss = 3.04911 avg_loss = 3.11573\n",
      "epoch no.5 train no.383310  loss = 2.23064 avg_loss = 3.08203\n",
      "epoch no.5 train no.383320  loss = 4.34307 avg_loss = 3.10659\n",
      "epoch no.5 train no.383330  loss = 2.61582 avg_loss = 3.06544\n",
      "epoch no.5 train no.383340  loss = 2.63657 avg_loss = 3.01904\n",
      "epoch no.5 train no.383350  loss = 4.24554 avg_loss = 3.02830\n",
      "epoch no.5 train no.383360  loss = 2.66530 avg_loss = 2.99767\n",
      "epoch no.5 train no.383370  loss = 3.30912 avg_loss = 2.99057\n",
      "epoch no.5 train no.383380  loss = 3.02458 avg_loss = 3.01412\n",
      "epoch no.5 train no.383390  loss = 2.16266 avg_loss = 3.06487\n",
      "epoch no.5 train no.383400  loss = 2.47077 avg_loss = 3.10937\n",
      "epoch no.5 train no.383410  loss = 2.63801 avg_loss = 3.11832\n",
      "epoch no.5 train no.383420  loss = 3.03168 avg_loss = 3.08500\n",
      "epoch no.5 train no.383430  loss = 4.47072 avg_loss = 3.09020\n",
      "epoch no.5 train no.383440  loss = 1.60309 avg_loss = 3.08607\n",
      "epoch no.5 train no.383450  loss = 2.59193 avg_loss = 3.06734\n",
      "epoch no.5 train no.383460  loss = 2.26190 avg_loss = 3.06254\n",
      "epoch no.5 train no.383470  loss = 3.21694 avg_loss = 3.04290\n",
      "epoch no.5 train no.383480  loss = 3.16274 avg_loss = 3.04058\n",
      "epoch no.5 train no.383490  loss = 2.98225 avg_loss = 3.06439\n",
      "epoch no.5 train no.383500  loss = 2.91010 avg_loss = 3.04586\n",
      "epoch no.5 train no.383510  loss = 1.99603 avg_loss = 3.00724\n",
      "epoch no.5 train no.383520  loss = 4.39412 avg_loss = 3.02701\n",
      "epoch no.5 train no.383530  loss = 2.77492 avg_loss = 3.02923\n",
      "epoch no.5 train no.383540  loss = 2.82306 avg_loss = 3.01413\n",
      "epoch no.5 train no.383550  loss = 2.23686 avg_loss = 3.03094\n",
      "epoch no.5 train no.383560  loss = 2.63422 avg_loss = 3.03376\n",
      "epoch no.5 train no.383570  loss = 3.62982 avg_loss = 3.03420\n",
      "epoch no.5 train no.383580  loss = 2.89692 avg_loss = 3.04205\n",
      "epoch no.5 train no.383590  loss = 4.08144 avg_loss = 3.03238\n",
      "epoch no.5 train no.383600  loss = 2.77978 avg_loss = 3.08991\n",
      "epoch no.5 train no.383610  loss = 3.59322 avg_loss = 3.10282\n",
      "epoch no.5 train no.383620  loss = 2.70195 avg_loss = 3.10896\n",
      "epoch no.5 train no.383630  loss = 2.67618 avg_loss = 3.11396\n",
      "epoch no.5 train no.383640  loss = 2.35991 avg_loss = 3.08620\n",
      "epoch no.5 train no.383650  loss = 3.26383 avg_loss = 3.10144\n",
      "epoch no.5 train no.383660  loss = 2.98601 avg_loss = 3.10406\n",
      "epoch no.5 train no.383670  loss = 1.98356 avg_loss = 3.10898\n",
      "epoch no.5 train no.383680  loss = 2.22596 avg_loss = 3.09715\n",
      "epoch no.5 train no.383690  loss = 4.35077 avg_loss = 3.10873\n",
      "epoch no.5 train no.383700  loss = 4.10103 avg_loss = 3.11393\n",
      "epoch no.5 train no.383710  loss = 2.32258 avg_loss = 3.09702\n",
      "epoch no.5 train no.383720  loss = 3.57047 avg_loss = 3.09804\n",
      "epoch no.5 train no.383730  loss = 3.59694 avg_loss = 3.10681\n",
      "epoch no.5 train no.383740  loss = 2.46571 avg_loss = 3.07279\n",
      "epoch no.5 train no.383750  loss = 2.40031 avg_loss = 3.04297\n",
      "epoch no.5 train no.383760  loss = 2.26343 avg_loss = 3.02537\n",
      "epoch no.5 train no.383770  loss = 1.98793 avg_loss = 3.03387\n",
      "epoch no.5 train no.383780  loss = 3.40533 avg_loss = 3.06902\n",
      "epoch no.5 train no.383790  loss = 2.49201 avg_loss = 3.00746\n",
      "epoch no.5 train no.383800  loss = 2.78609 avg_loss = 3.00144\n",
      "epoch no.5 train no.383810  loss = 2.90788 avg_loss = 2.95237\n",
      "epoch no.5 train no.383820  loss = 2.69251 avg_loss = 2.97211\n",
      "epoch no.5 train no.383830  loss = 2.20197 avg_loss = 2.97412\n",
      "epoch no.5 train no.383840  loss = 2.10302 avg_loss = 2.97363\n",
      "epoch no.5 train no.383850  loss = 4.14230 avg_loss = 3.02947\n",
      "epoch no.5 train no.383860  loss = 3.42128 avg_loss = 3.02053\n",
      "epoch no.5 train no.383870  loss = 3.39813 avg_loss = 3.05733\n",
      "epoch no.5 train no.383880  loss = 2.35909 avg_loss = 3.06195\n",
      "epoch no.5 train no.383890  loss = 2.13774 avg_loss = 3.05062\n",
      "epoch no.5 train no.383900  loss = 1.40355 avg_loss = 3.00301\n",
      "epoch no.5 train no.383910  loss = 2.36205 avg_loss = 3.01966\n",
      "epoch no.5 train no.383920  loss = 2.45942 avg_loss = 3.01188\n",
      "epoch no.5 train no.383930  loss = 2.54298 avg_loss = 3.00746\n",
      "epoch no.5 train no.383940  loss = 2.73045 avg_loss = 3.01257\n",
      "epoch no.5 train no.383950  loss = 3.56172 avg_loss = 3.04577\n",
      "epoch no.5 train no.383960  loss = 3.46002 avg_loss = 3.01242\n",
      "epoch no.5 train no.383970  loss = 2.95582 avg_loss = 3.04755\n",
      "epoch no.5 train no.383980  loss = 3.08432 avg_loss = 3.03881\n",
      "epoch no.5 train no.383990  loss = 3.34925 avg_loss = 3.00203\n",
      "epoch no.5 train no.384000  loss = 3.77746 avg_loss = 2.98290\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁잔잔', '▁노래', '</s>']\n",
      "여름밤에 듣는 시원한 팝</s>\n",
      "epoch no.5 train no.384010  loss = 3.67081 avg_loss = 2.99813\n",
      "epoch no.5 train no.384020  loss = 2.84217 avg_loss = 2.96967\n",
      "epoch no.5 train no.384030  loss = 3.69195 avg_loss = 2.99067\n",
      "epoch no.5 train no.384040  loss = 2.66900 avg_loss = 2.98343\n",
      "epoch no.5 train no.384050  loss = 1.96002 avg_loss = 3.04840\n",
      "epoch no.5 train no.384060  loss = 2.19136 avg_loss = 3.02755\n",
      "epoch no.5 train no.384070  loss = 3.42665 avg_loss = 3.05836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.384080  loss = 3.66391 avg_loss = 3.04219\n",
      "epoch no.5 train no.384090  loss = 4.32841 avg_loss = 3.07510\n",
      "epoch no.5 train no.384100  loss = 3.55547 avg_loss = 3.03258\n",
      "epoch no.5 train no.384110  loss = 5.45387 avg_loss = 3.06253\n",
      "epoch no.5 train no.384120  loss = 3.44819 avg_loss = 3.05601\n",
      "epoch no.5 train no.384130  loss = 3.19106 avg_loss = 3.02269\n",
      "epoch no.5 train no.384140  loss = 4.62505 avg_loss = 3.05123\n",
      "epoch no.5 train no.384150  loss = 3.26199 avg_loss = 3.05126\n",
      "epoch no.5 train no.384160  loss = 3.20733 avg_loss = 3.01993\n",
      "epoch no.5 train no.384170  loss = 3.52866 avg_loss = 3.02445\n",
      "epoch no.5 train no.384180  loss = 3.72645 avg_loss = 3.02056\n",
      "epoch no.5 train no.384190  loss = 4.68231 avg_loss = 3.06388\n",
      "epoch no.5 train no.384200  loss = 3.11372 avg_loss = 3.07547\n",
      "epoch no.5 train no.384210  loss = 4.14622 avg_loss = 3.06362\n",
      "epoch no.5 train no.384220  loss = 4.85985 avg_loss = 3.11918\n",
      "epoch no.5 train no.384230  loss = 2.49510 avg_loss = 3.09430\n",
      "epoch no.5 train no.384240  loss = 1.82905 avg_loss = 3.09702\n",
      "epoch no.5 train no.384250  loss = 4.25154 avg_loss = 3.08720\n",
      "epoch no.5 train no.384260  loss = 3.69132 avg_loss = 3.09939\n",
      "epoch no.5 train no.384270  loss = 3.09327 avg_loss = 3.08769\n",
      "epoch no.5 train no.384280  loss = 3.50145 avg_loss = 3.07194\n",
      "epoch no.5 train no.384290  loss = 2.66849 avg_loss = 3.02954\n",
      "epoch no.5 train no.384300  loss = 1.58526 avg_loss = 3.00004\n",
      "epoch no.5 train no.384310  loss = 1.49881 avg_loss = 2.97821\n",
      "epoch no.5 train no.384320  loss = 2.82176 avg_loss = 3.01793\n",
      "epoch no.5 train no.384330  loss = 3.13062 avg_loss = 3.01150\n",
      "epoch no.5 train no.384340  loss = 1.97040 avg_loss = 2.98629\n",
      "epoch no.5 train no.384350  loss = 2.68459 avg_loss = 3.02469\n",
      "epoch no.5 train no.384360  loss = 2.52052 avg_loss = 3.04880\n",
      "epoch no.5 train no.384370  loss = 2.11951 avg_loss = 3.00332\n",
      "epoch no.5 train no.384380  loss = 2.53689 avg_loss = 3.05582\n",
      "epoch no.5 train no.384390  loss = 1.62210 avg_loss = 3.02562\n",
      "epoch no.5 train no.384400  loss = 2.69944 avg_loss = 3.06243\n",
      "epoch no.5 train no.384410  loss = 2.22369 avg_loss = 3.05445\n",
      "epoch no.5 train no.384420  loss = 2.41655 avg_loss = 3.08151\n",
      "epoch no.5 train no.384430  loss = 2.95599 avg_loss = 3.09709\n",
      "epoch no.5 train no.384440  loss = 3.27892 avg_loss = 3.08115\n",
      "epoch no.5 train no.384450  loss = 2.15142 avg_loss = 3.05866\n",
      "epoch no.5 train no.384460  loss = 1.81709 avg_loss = 3.01029\n",
      "epoch no.5 train no.384470  loss = 2.58291 avg_loss = 3.05618\n",
      "epoch no.5 train no.384480  loss = 3.44341 avg_loss = 3.04150\n",
      "epoch no.5 train no.384490  loss = 3.56286 avg_loss = 3.04698\n",
      "epoch no.5 train no.384500  loss = 3.19063 avg_loss = 3.03392\n",
      "epoch no.5 train no.384510  loss = 3.30263 avg_loss = 3.04577\n",
      "epoch no.5 train no.384520  loss = 2.92969 avg_loss = 3.02805\n",
      "epoch no.5 train no.384530  loss = 2.75800 avg_loss = 3.01324\n",
      "epoch no.5 train no.384540  loss = 4.20233 avg_loss = 3.01098\n",
      "epoch no.5 train no.384550  loss = 2.75685 avg_loss = 3.02779\n",
      "epoch no.5 train no.384560  loss = 3.47699 avg_loss = 3.02590\n",
      "epoch no.5 train no.384570  loss = 2.96150 avg_loss = 2.97381\n",
      "epoch no.5 train no.384580  loss = 2.97341 avg_loss = 2.98399\n",
      "epoch no.5 train no.384590  loss = 3.55807 avg_loss = 2.98230\n",
      "epoch no.5 train no.384600  loss = 2.49909 avg_loss = 2.96221\n",
      "epoch no.5 train no.384610  loss = 2.73380 avg_loss = 2.95812\n",
      "epoch no.5 train no.384620  loss = 2.66487 avg_loss = 2.97432\n",
      "epoch no.5 train no.384630  loss = 3.40153 avg_loss = 3.02604\n",
      "epoch no.5 train no.384640  loss = 2.58873 avg_loss = 2.99852\n",
      "epoch no.5 train no.384650  loss = 2.16575 avg_loss = 2.95739\n",
      "epoch no.5 train no.384660  loss = 3.46425 avg_loss = 2.93728\n",
      "epoch no.5 train no.384670  loss = 3.35205 avg_loss = 2.96129\n",
      "epoch no.5 train no.384680  loss = 2.50860 avg_loss = 2.96480\n",
      "epoch no.5 train no.384690  loss = 2.87572 avg_loss = 2.93901\n",
      "epoch no.5 train no.384700  loss = 2.90101 avg_loss = 2.95283\n",
      "epoch no.5 train no.384710  loss = 3.52730 avg_loss = 2.95425\n",
      "epoch no.5 train no.384720  loss = 3.21713 avg_loss = 2.93271\n",
      "epoch no.5 train no.384730  loss = 3.11436 avg_loss = 2.96193\n",
      "epoch no.5 train no.384740  loss = 2.87810 avg_loss = 2.97267\n",
      "epoch no.5 train no.384750  loss = 2.89447 avg_loss = 3.01694\n",
      "epoch no.5 train no.384760  loss = 3.40099 avg_loss = 3.02586\n",
      "epoch no.5 train no.384770  loss = 3.21865 avg_loss = 3.00898\n",
      "epoch no.5 train no.384780  loss = 2.81112 avg_loss = 3.00581\n",
      "epoch no.5 train no.384790  loss = 2.30752 avg_loss = 3.03321\n",
      "epoch no.5 train no.384800  loss = 3.80475 avg_loss = 3.03763\n",
      "epoch no.5 train no.384810  loss = 2.77195 avg_loss = 3.05223\n",
      "epoch no.5 train no.384820  loss = 2.75837 avg_loss = 3.04665\n",
      "epoch no.5 train no.384830  loss = 3.98622 avg_loss = 3.08743\n",
      "epoch no.5 train no.384840  loss = 3.18931 avg_loss = 3.13270\n",
      "epoch no.5 train no.384850  loss = 2.40838 avg_loss = 3.15294\n",
      "epoch no.5 train no.384860  loss = 4.50285 avg_loss = 3.12701\n",
      "epoch no.5 train no.384870  loss = 3.85949 avg_loss = 3.07960\n",
      "epoch no.5 train no.384880  loss = 3.53336 avg_loss = 3.10173\n",
      "epoch no.5 train no.384890  loss = 2.58422 avg_loss = 3.07754\n",
      "epoch no.5 train no.384900  loss = 2.93188 avg_loss = 3.07471\n",
      "epoch no.5 train no.384910  loss = 2.93295 avg_loss = 3.07914\n",
      "epoch no.5 train no.384920  loss = 2.25359 avg_loss = 3.07739\n",
      "epoch no.5 train no.384930  loss = 2.08266 avg_loss = 3.02035\n",
      "epoch no.5 train no.384940  loss = 2.58371 avg_loss = 3.00265\n",
      "epoch no.5 train no.384950  loss = 6.12771 avg_loss = 2.98784\n",
      "epoch no.5 train no.384960  loss = 3.14962 avg_loss = 3.00975\n",
      "epoch no.5 train no.384970  loss = 2.79741 avg_loss = 2.96661\n",
      "epoch no.5 train no.384980  loss = 1.73485 avg_loss = 2.95664\n",
      "epoch no.5 train no.384990  loss = 4.12358 avg_loss = 2.98604\n",
      "epoch no.5 train no.385000  loss = 2.97939 avg_loss = 2.95240\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '좋은', '▁인디', '</s>']\n",
      "여름밤 듣기 좋은 노래</s>\n",
      "epoch no.5 train no.385010  loss = 3.54782 avg_loss = 2.96339\n",
      "epoch no.5 train no.385020  loss = 5.17903 avg_loss = 3.02449\n",
      "epoch no.5 train no.385030  loss = 2.59562 avg_loss = 3.02077\n",
      "epoch no.5 train no.385040  loss = 3.81765 avg_loss = 3.04344\n",
      "epoch no.5 train no.385050  loss = 2.35442 avg_loss = 3.07233\n",
      "epoch no.5 train no.385060  loss = 2.97732 avg_loss = 3.11352\n",
      "epoch no.5 train no.385070  loss = 2.55322 avg_loss = 3.11823\n",
      "epoch no.5 train no.385080  loss = 2.72087 avg_loss = 3.07855\n",
      "epoch no.5 train no.385090  loss = 3.29754 avg_loss = 3.07908\n",
      "epoch no.5 train no.385100  loss = 2.97660 avg_loss = 3.05178\n",
      "epoch no.5 train no.385110  loss = 2.84273 avg_loss = 3.09923\n",
      "epoch no.5 train no.385120  loss = 3.04295 avg_loss = 3.09100\n",
      "epoch no.5 train no.385130  loss = 2.07083 avg_loss = 3.07656\n",
      "epoch no.5 train no.385140  loss = 2.17196 avg_loss = 3.05468\n",
      "epoch no.5 train no.385150  loss = 3.47180 avg_loss = 3.06740\n",
      "epoch no.5 train no.385160  loss = 2.83664 avg_loss = 3.04920\n",
      "epoch no.5 train no.385170  loss = 2.64455 avg_loss = 3.01199\n",
      "epoch no.5 train no.385180  loss = 2.02703 avg_loss = 2.97013\n",
      "epoch no.5 train no.385190  loss = 2.24022 avg_loss = 2.96688\n",
      "epoch no.5 train no.385200  loss = 3.34589 avg_loss = 2.99206\n",
      "epoch no.5 train no.385210  loss = 3.39362 avg_loss = 3.02469\n",
      "epoch no.5 train no.385220  loss = 3.28725 avg_loss = 3.06734\n",
      "epoch no.5 train no.385230  loss = 2.31132 avg_loss = 3.04898\n",
      "epoch no.5 train no.385240  loss = 3.18293 avg_loss = 3.02782\n",
      "epoch no.5 train no.385250  loss = 2.37783 avg_loss = 2.99052\n",
      "epoch no.5 train no.385260  loss = 2.09370 avg_loss = 2.97322\n",
      "epoch no.5 train no.385270  loss = 3.97144 avg_loss = 3.00766\n",
      "epoch no.5 train no.385280  loss = 2.43024 avg_loss = 3.00320\n",
      "epoch no.5 train no.385290  loss = 2.85359 avg_loss = 3.00755\n",
      "epoch no.5 train no.385300  loss = 3.67958 avg_loss = 2.98840\n",
      "epoch no.5 train no.385310  loss = 2.32770 avg_loss = 2.98306\n",
      "epoch no.5 train no.385320  loss = 2.30977 avg_loss = 2.99737\n",
      "epoch no.5 train no.385330  loss = 3.57152 avg_loss = 2.99003\n",
      "epoch no.5 train no.385340  loss = 4.81318 avg_loss = 3.00037\n",
      "epoch no.5 train no.385350  loss = 2.11344 avg_loss = 2.99789\n",
      "epoch no.5 train no.385360  loss = 2.43348 avg_loss = 2.98495\n",
      "epoch no.5 train no.385370  loss = 2.32963 avg_loss = 2.99870\n",
      "epoch no.5 train no.385380  loss = 2.75180 avg_loss = 3.02298\n",
      "epoch no.5 train no.385390  loss = 2.43447 avg_loss = 2.98860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.385400  loss = 3.60257 avg_loss = 3.02304\n",
      "epoch no.5 train no.385410  loss = 1.83040 avg_loss = 3.02195\n",
      "epoch no.5 train no.385420  loss = 3.34188 avg_loss = 3.01346\n",
      "epoch no.5 train no.385430  loss = 3.30536 avg_loss = 3.00370\n",
      "epoch no.5 train no.385440  loss = 3.53846 avg_loss = 2.99994\n",
      "epoch no.5 train no.385450  loss = 2.13799 avg_loss = 2.99436\n",
      "epoch no.5 train no.385460  loss = 3.31897 avg_loss = 2.98012\n",
      "epoch no.5 train no.385470  loss = 2.39972 avg_loss = 2.94437\n",
      "epoch no.5 train no.385480  loss = 2.30438 avg_loss = 2.91560\n",
      "epoch no.5 train no.385490  loss = 1.55310 avg_loss = 2.86861\n",
      "epoch no.5 train no.385500  loss = 1.99587 avg_loss = 2.86496\n",
      "epoch no.5 train no.385510  loss = 3.20592 avg_loss = 2.88653\n",
      "epoch no.5 train no.385520  loss = 2.41326 avg_loss = 2.88225\n",
      "epoch no.5 train no.385530  loss = 2.30655 avg_loss = 2.90554\n",
      "epoch no.5 train no.385540  loss = 3.33731 avg_loss = 2.92405\n",
      "epoch no.5 train no.385550  loss = 1.76087 avg_loss = 2.92667\n",
      "epoch no.5 train no.385560  loss = 3.09202 avg_loss = 2.97204\n",
      "epoch no.5 train no.385570  loss = 2.11489 avg_loss = 2.96177\n",
      "epoch no.5 train no.385580  loss = 4.45363 avg_loss = 2.97360\n",
      "epoch no.5 train no.385590  loss = 2.99570 avg_loss = 2.95239\n",
      "epoch no.5 train no.385600  loss = 2.96849 avg_loss = 2.94620\n",
      "epoch no.5 train no.385610  loss = 2.81332 avg_loss = 2.92620\n",
      "epoch no.5 train no.385620  loss = 1.70329 avg_loss = 2.93485\n",
      "epoch no.5 train no.385630  loss = 2.75211 avg_loss = 2.97135\n",
      "epoch no.5 train no.385640  loss = 2.59512 avg_loss = 2.96953\n",
      "epoch no.5 train no.385650  loss = 2.70592 avg_loss = 2.93906\n",
      "epoch no.5 train no.385660  loss = 3.12670 avg_loss = 2.95933\n",
      "epoch no.5 train no.385670  loss = 3.17347 avg_loss = 2.94578\n",
      "epoch no.5 train no.385680  loss = 3.48267 avg_loss = 2.93680\n",
      "epoch no.5 train no.385690  loss = 2.85431 avg_loss = 2.92550\n",
      "epoch no.5 train no.385700  loss = 2.41061 avg_loss = 2.90593\n",
      "epoch no.5 train no.385710  loss = 1.73880 avg_loss = 2.86230\n",
      "epoch no.5 train no.385720  loss = 3.27710 avg_loss = 2.89058\n",
      "epoch no.5 train no.385730  loss = 2.56605 avg_loss = 2.86943\n",
      "epoch no.5 train no.385740  loss = 2.01809 avg_loss = 2.85671\n",
      "epoch no.5 train no.385750  loss = 3.27598 avg_loss = 2.88826\n",
      "epoch no.5 train no.385760  loss = 2.36925 avg_loss = 2.87427\n",
      "epoch no.5 train no.385770  loss = 3.89774 avg_loss = 2.88253\n",
      "epoch no.5 train no.385780  loss = 4.32215 avg_loss = 2.89995\n",
      "epoch no.5 train no.385790  loss = 2.77209 avg_loss = 2.91596\n",
      "epoch no.5 train no.385800  loss = 4.28807 avg_loss = 2.92136\n",
      "epoch no.5 train no.385810  loss = 4.17360 avg_loss = 2.94849\n",
      "epoch no.5 train no.385820  loss = 3.92179 avg_loss = 2.96335\n",
      "epoch no.5 train no.385830  loss = 3.11820 avg_loss = 2.92013\n",
      "epoch no.5 train no.385840  loss = 4.31865 avg_loss = 2.95191\n",
      "epoch no.5 train no.385850  loss = 2.91396 avg_loss = 2.95924\n",
      "epoch no.5 train no.385860  loss = 3.84070 avg_loss = 2.96148\n",
      "epoch no.5 train no.385870  loss = 2.59542 avg_loss = 2.94531\n",
      "epoch no.5 train no.385880  loss = 1.15780 avg_loss = 2.90939\n",
      "epoch no.5 train no.385890  loss = 2.47230 avg_loss = 2.91113\n",
      "epoch no.5 train no.385900  loss = 3.54848 avg_loss = 2.92456\n",
      "epoch no.5 train no.385910  loss = 3.92845 avg_loss = 2.97940\n",
      "epoch no.5 train no.385920  loss = 2.19010 avg_loss = 2.98145\n",
      "epoch no.5 train no.385930  loss = 4.01884 avg_loss = 2.97291\n",
      "epoch no.5 train no.385940  loss = 2.39426 avg_loss = 2.95626\n",
      "epoch no.5 train no.385950  loss = 2.49192 avg_loss = 2.92386\n",
      "epoch no.5 train no.385960  loss = 3.09152 avg_loss = 2.92209\n",
      "epoch no.5 train no.385970  loss = 3.53788 avg_loss = 2.90324\n",
      "epoch no.5 train no.385980  loss = 2.02935 avg_loss = 2.86641\n",
      "epoch no.5 train no.385990  loss = 3.53230 avg_loss = 2.90597\n",
      "epoch no.5 train no.386000  loss = 2.51321 avg_loss = 2.92392\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '▁오면', '▁생각', '래', '가', '▁생각', '나', '</s>']\n",
      "여름이 오면 이노래가 생각나</s>\n",
      "epoch no.5 train no.386010  loss = 2.53360 avg_loss = 2.91760\n",
      "epoch no.5 train no.386020  loss = 4.02726 avg_loss = 2.94852\n",
      "epoch no.5 train no.386030  loss = 3.15458 avg_loss = 2.93887\n",
      "epoch no.5 train no.386040  loss = 4.31696 avg_loss = 2.92389\n",
      "epoch no.5 train no.386050  loss = 3.28204 avg_loss = 2.97109\n",
      "epoch no.5 train no.386060  loss = 2.46186 avg_loss = 2.98075\n",
      "epoch no.5 train no.386070  loss = 3.91011 avg_loss = 2.96802\n",
      "epoch no.5 train no.386080  loss = 3.14610 avg_loss = 2.99433\n",
      "epoch no.5 train no.386090  loss = 2.68655 avg_loss = 2.99853\n",
      "epoch no.5 train no.386100  loss = 3.14390 avg_loss = 3.03625\n",
      "epoch no.5 train no.386110  loss = 2.18392 avg_loss = 3.01603\n",
      "epoch no.5 train no.386120  loss = 1.84543 avg_loss = 2.96144\n",
      "epoch no.5 train no.386130  loss = 2.54432 avg_loss = 2.95862\n",
      "epoch no.5 train no.386140  loss = 3.32820 avg_loss = 2.95293\n",
      "epoch no.5 train no.386150  loss = 3.71743 avg_loss = 2.96919\n",
      "epoch no.5 train no.386160  loss = 2.03493 avg_loss = 3.00078\n",
      "epoch no.5 train no.386170  loss = 3.70095 avg_loss = 2.98972\n",
      "epoch no.5 train no.386180  loss = 2.78900 avg_loss = 3.00640\n",
      "epoch no.5 train no.386190  loss = 3.31283 avg_loss = 3.01941\n",
      "epoch no.5 train no.386200  loss = 4.36405 avg_loss = 3.04988\n",
      "epoch no.5 train no.386210  loss = 2.38119 avg_loss = 3.06097\n",
      "epoch no.5 train no.386220  loss = 3.29435 avg_loss = 3.05616\n",
      "epoch no.5 train no.386230  loss = 2.10804 avg_loss = 3.10390\n",
      "epoch no.5 train no.386240  loss = 2.47400 avg_loss = 3.08203\n",
      "epoch no.5 train no.386250  loss = 1.56193 avg_loss = 3.04417\n",
      "epoch no.5 train no.386260  loss = 2.86285 avg_loss = 3.03243\n",
      "epoch no.5 train no.386270  loss = 5.42562 avg_loss = 3.04837\n",
      "epoch no.5 train no.386280  loss = 3.90954 avg_loss = 3.02895\n",
      "epoch no.5 train no.386290  loss = 2.72526 avg_loss = 3.03180\n",
      "epoch no.5 train no.386300  loss = 3.12514 avg_loss = 3.03371\n",
      "epoch no.5 train no.386310  loss = 3.02219 avg_loss = 2.98263\n",
      "epoch no.5 train no.386320  loss = 4.58707 avg_loss = 3.06443\n",
      "epoch no.5 train no.386330  loss = 2.66292 avg_loss = 3.06807\n",
      "epoch no.5 train no.386340  loss = 1.90607 avg_loss = 3.05405\n",
      "epoch no.5 train no.386350  loss = 3.40653 avg_loss = 3.06024\n",
      "epoch no.5 train no.386360  loss = 2.58114 avg_loss = 3.02885\n",
      "epoch no.5 train no.386370  loss = 4.03490 avg_loss = 3.05749\n",
      "epoch no.5 train no.386380  loss = 2.01243 avg_loss = 3.02667\n",
      "epoch no.5 train no.386390  loss = 2.46333 avg_loss = 3.03114\n",
      "epoch no.5 train no.386400  loss = 1.77495 avg_loss = 3.05623\n",
      "epoch no.5 train no.386410  loss = 3.42237 avg_loss = 3.04735\n",
      "epoch no.5 train no.386420  loss = 2.77848 avg_loss = 3.04171\n",
      "epoch no.5 train no.386430  loss = 2.98502 avg_loss = 3.04956\n",
      "epoch no.5 train no.386440  loss = 4.15439 avg_loss = 3.05199\n",
      "epoch no.5 train no.386450  loss = 3.48389 avg_loss = 3.08832\n",
      "epoch no.5 train no.386460  loss = 4.48516 avg_loss = 3.10032\n",
      "epoch no.5 train no.386470  loss = 3.48668 avg_loss = 3.12558\n",
      "epoch no.5 train no.386480  loss = 2.43398 avg_loss = 3.10970\n",
      "epoch no.5 train no.386490  loss = 2.13792 avg_loss = 3.10972\n",
      "epoch no.5 train no.386500  loss = 4.26189 avg_loss = 3.13064\n",
      "epoch no.5 train no.386510  loss = 3.27350 avg_loss = 3.14099\n",
      "epoch no.5 train no.386520  loss = 3.15639 avg_loss = 3.13048\n",
      "epoch no.5 train no.386530  loss = 3.06088 avg_loss = 3.10650\n",
      "epoch no.5 train no.386540  loss = 3.25379 avg_loss = 3.07012\n",
      "epoch no.5 train no.386550  loss = 2.45917 avg_loss = 3.04017\n",
      "epoch no.5 train no.386560  loss = 3.69843 avg_loss = 3.09011\n",
      "epoch no.5 train no.386570  loss = 2.74032 avg_loss = 3.09627\n",
      "epoch no.5 train no.386580  loss = 1.71547 avg_loss = 3.11329\n",
      "epoch no.5 train no.386590  loss = 2.55996 avg_loss = 3.10331\n",
      "epoch no.5 train no.386600  loss = 2.61791 avg_loss = 3.10761\n",
      "epoch no.5 train no.386610  loss = 3.22241 avg_loss = 3.11737\n",
      "epoch no.5 train no.386620  loss = 3.47160 avg_loss = 3.09327\n",
      "epoch no.5 train no.386630  loss = 2.68163 avg_loss = 3.05608\n",
      "epoch no.5 train no.386640  loss = 2.97539 avg_loss = 2.99096\n",
      "epoch no.5 train no.386650  loss = 2.45633 avg_loss = 2.98854\n",
      "epoch no.5 train no.386660  loss = 3.67416 avg_loss = 3.01067\n",
      "epoch no.5 train no.386670  loss = 2.06724 avg_loss = 3.00744\n",
      "epoch no.5 train no.386680  loss = 2.32709 avg_loss = 2.99790\n",
      "epoch no.5 train no.386690  loss = 3.89970 avg_loss = 2.98633\n",
      "epoch no.5 train no.386700  loss = 3.04589 avg_loss = 2.96637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.386710  loss = 3.67996 avg_loss = 2.96677\n",
      "epoch no.5 train no.386720  loss = 3.01381 avg_loss = 2.92357\n",
      "epoch no.5 train no.386730  loss = 3.41378 avg_loss = 2.97284\n",
      "epoch no.5 train no.386740  loss = 2.27910 avg_loss = 2.96504\n",
      "epoch no.5 train no.386750  loss = 2.47213 avg_loss = 2.96213\n",
      "epoch no.5 train no.386760  loss = 2.64995 avg_loss = 2.98763\n",
      "epoch no.5 train no.386770  loss = 3.81749 avg_loss = 3.03854\n",
      "epoch no.5 train no.386780  loss = 3.02929 avg_loss = 3.03047\n",
      "epoch no.5 train no.386790  loss = 3.54082 avg_loss = 3.00561\n",
      "epoch no.5 train no.386800  loss = 4.32759 avg_loss = 3.00474\n",
      "epoch no.5 train no.386810  loss = 5.24857 avg_loss = 3.06677\n",
      "epoch no.5 train no.386820  loss = 3.62817 avg_loss = 3.08298\n",
      "epoch no.5 train no.386830  loss = 2.28490 avg_loss = 3.08989\n",
      "epoch no.5 train no.386840  loss = 2.33446 avg_loss = 3.06571\n",
      "epoch no.5 train no.386850  loss = 4.09684 avg_loss = 3.05739\n",
      "epoch no.5 train no.386860  loss = 2.47159 avg_loss = 3.05874\n",
      "epoch no.5 train no.386870  loss = 4.22576 avg_loss = 3.12289\n",
      "epoch no.5 train no.386880  loss = 2.27217 avg_loss = 3.14386\n",
      "epoch no.5 train no.386890  loss = 2.62105 avg_loss = 3.13754\n",
      "epoch no.5 train no.386900  loss = 1.64969 avg_loss = 3.11543\n",
      "epoch no.5 train no.386910  loss = 2.22751 avg_loss = 3.07275\n",
      "epoch no.5 train no.386920  loss = 2.64469 avg_loss = 3.08237\n",
      "epoch no.5 train no.386930  loss = 1.99856 avg_loss = 3.05271\n",
      "epoch no.5 train no.386940  loss = 2.67921 avg_loss = 3.04899\n",
      "epoch no.5 train no.386950  loss = 3.85121 avg_loss = 3.02135\n",
      "epoch no.5 train no.386960  loss = 2.20067 avg_loss = 3.02553\n",
      "epoch no.5 train no.386970  loss = 3.27252 avg_loss = 3.06405\n",
      "epoch no.5 train no.386980  loss = 2.80818 avg_loss = 3.06188\n",
      "epoch no.5 train no.386990  loss = 3.36124 avg_loss = 3.02590\n",
      "epoch no.5 train no.387000  loss = 2.81228 avg_loss = 3.03659\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁잔잔', '팝', '▁노래', '</s>']\n",
      "여름밤에 듣는 감성적인 노래</s>\n",
      "epoch no.5 train no.387010  loss = 2.74012 avg_loss = 3.03381\n",
      "epoch no.5 train no.387020  loss = 3.46484 avg_loss = 3.05572\n",
      "epoch no.5 train no.387030  loss = 2.50663 avg_loss = 3.05717\n",
      "epoch no.5 train no.387040  loss = 2.55488 avg_loss = 3.03948\n",
      "epoch no.5 train no.387050  loss = 3.91765 avg_loss = 3.05111\n",
      "epoch no.5 train no.387060  loss = 3.46508 avg_loss = 3.05078\n",
      "epoch no.5 train no.387070  loss = 2.68409 avg_loss = 3.03798\n",
      "epoch no.5 train no.387080  loss = 3.27781 avg_loss = 3.04113\n",
      "epoch no.5 train no.387090  loss = 2.93077 avg_loss = 3.02195\n",
      "epoch no.5 train no.387100  loss = 3.29080 avg_loss = 3.00885\n",
      "epoch no.5 train no.387110  loss = 1.95697 avg_loss = 2.98164\n",
      "epoch no.5 train no.387120  loss = 2.05650 avg_loss = 3.01392\n",
      "epoch no.5 train no.387130  loss = 3.91847 avg_loss = 3.00977\n",
      "epoch no.5 train no.387140  loss = 2.50193 avg_loss = 2.99791\n",
      "epoch no.5 train no.387150  loss = 3.17232 avg_loss = 2.96488\n",
      "epoch no.5 train no.387160  loss = 3.75919 avg_loss = 3.02310\n",
      "epoch no.5 train no.387170  loss = 3.15672 avg_loss = 2.99120\n",
      "epoch no.5 train no.387180  loss = 3.44097 avg_loss = 2.98309\n",
      "epoch no.5 train no.387190  loss = 3.19144 avg_loss = 2.98464\n",
      "epoch no.5 train no.387200  loss = 3.08309 avg_loss = 3.00687\n",
      "epoch no.5 train no.387210  loss = 3.58966 avg_loss = 3.04904\n",
      "epoch no.5 train no.387220  loss = 2.81796 avg_loss = 3.05158\n",
      "epoch no.5 train no.387230  loss = 2.29031 avg_loss = 3.07080\n",
      "epoch no.5 train no.387240  loss = 3.71586 avg_loss = 3.13044\n",
      "epoch no.5 train no.387250  loss = 3.18678 avg_loss = 3.18038\n",
      "epoch no.5 train no.387260  loss = 4.06513 avg_loss = 3.18847\n",
      "epoch no.5 train no.387270  loss = 2.16938 avg_loss = 3.16810\n",
      "epoch no.5 train no.387280  loss = 3.02200 avg_loss = 3.14181\n",
      "epoch no.5 train no.387290  loss = 2.78719 avg_loss = 3.10413\n",
      "epoch no.5 train no.387300  loss = 2.02602 avg_loss = 3.06586\n",
      "epoch no.5 train no.387310  loss = 3.39638 avg_loss = 3.06164\n",
      "epoch no.5 train no.387320  loss = 2.80784 avg_loss = 3.06519\n",
      "epoch no.5 train no.387330  loss = 2.39163 avg_loss = 3.02249\n",
      "epoch no.5 train no.387340  loss = 2.96238 avg_loss = 3.02119\n",
      "epoch no.5 train no.387350  loss = 2.38827 avg_loss = 3.01852\n",
      "epoch no.5 train no.387360  loss = 3.50761 avg_loss = 3.03769\n",
      "epoch no.5 train no.387370  loss = 2.54683 avg_loss = 3.03056\n",
      "epoch no.5 train no.387380  loss = 2.78107 avg_loss = 3.01030\n",
      "epoch no.5 train no.387390  loss = 2.46033 avg_loss = 2.99049\n",
      "epoch no.5 train no.387400  loss = 2.88535 avg_loss = 2.95732\n",
      "epoch no.5 train no.387410  loss = 2.38015 avg_loss = 2.93338\n",
      "epoch no.5 train no.387420  loss = 3.56242 avg_loss = 2.98730\n",
      "epoch no.5 train no.387430  loss = 3.44419 avg_loss = 2.99022\n",
      "epoch no.5 train no.387440  loss = 2.26942 avg_loss = 2.98866\n",
      "epoch no.5 train no.387450  loss = 3.91088 avg_loss = 2.99299\n",
      "epoch no.5 train no.387460  loss = 2.47022 avg_loss = 2.97213\n",
      "epoch no.5 train no.387470  loss = 2.92965 avg_loss = 2.94863\n",
      "epoch no.5 train no.387480  loss = 2.02391 avg_loss = 2.94913\n",
      "epoch no.5 train no.387490  loss = 2.43050 avg_loss = 2.97811\n",
      "epoch no.5 train no.387500  loss = 2.71279 avg_loss = 2.95281\n",
      "epoch no.5 train no.387510  loss = 1.70138 avg_loss = 2.93865\n",
      "epoch no.5 train no.387520  loss = 3.19619 avg_loss = 2.91699\n",
      "epoch no.5 train no.387530  loss = 3.38271 avg_loss = 2.93448\n",
      "epoch no.5 train no.387540  loss = 2.10623 avg_loss = 2.92498\n",
      "epoch no.5 train no.387550  loss = 1.81547 avg_loss = 2.94115\n",
      "epoch no.5 train no.387560  loss = 2.61117 avg_loss = 2.97619\n",
      "epoch no.5 train no.387570  loss = 4.39635 avg_loss = 2.99604\n",
      "epoch no.5 train no.387580  loss = 2.22944 avg_loss = 3.01469\n",
      "epoch no.5 train no.387590  loss = 1.81990 avg_loss = 3.02922\n",
      "epoch no.5 train no.387600  loss = 2.40207 avg_loss = 3.08767\n",
      "epoch no.5 train no.387610  loss = 3.60033 avg_loss = 3.08804\n",
      "epoch no.5 train no.387620  loss = 2.89921 avg_loss = 3.09618\n",
      "epoch no.5 train no.387630  loss = 3.53809 avg_loss = 3.08909\n",
      "epoch no.5 train no.387640  loss = 3.30452 avg_loss = 3.07777\n",
      "epoch no.5 train no.387650  loss = 2.28884 avg_loss = 3.08326\n",
      "epoch no.5 train no.387660  loss = 2.19665 avg_loss = 3.11351\n",
      "epoch no.5 train no.387670  loss = 2.85897 avg_loss = 3.11637\n",
      "epoch no.5 train no.387680  loss = 4.38901 avg_loss = 3.11602\n",
      "epoch no.5 train no.387690  loss = 2.25363 avg_loss = 3.08868\n",
      "epoch no.5 train no.387700  loss = 2.57971 avg_loss = 3.05103\n",
      "epoch no.5 train no.387710  loss = 1.85025 avg_loss = 3.02336\n",
      "epoch no.5 train no.387720  loss = 3.36933 avg_loss = 3.00911\n",
      "epoch no.5 train no.387730  loss = 2.80553 avg_loss = 3.00535\n",
      "epoch no.5 train no.387740  loss = 1.84106 avg_loss = 3.00979\n",
      "epoch no.5 train no.387750  loss = 3.50266 avg_loss = 3.03807\n",
      "epoch no.5 train no.387760  loss = 2.74162 avg_loss = 3.04207\n",
      "epoch no.5 train no.387770  loss = 1.87967 avg_loss = 3.00998\n",
      "epoch no.5 train no.387780  loss = 3.20043 avg_loss = 3.01005\n",
      "epoch no.5 train no.387790  loss = 3.31473 avg_loss = 2.99514\n",
      "epoch no.5 train no.387800  loss = 3.19401 avg_loss = 2.99822\n",
      "epoch no.5 train no.387810  loss = 2.89991 avg_loss = 3.01988\n",
      "epoch no.5 train no.387820  loss = 3.46333 avg_loss = 3.04073\n",
      "epoch no.5 train no.387830  loss = 2.59550 avg_loss = 3.06826\n",
      "epoch no.5 train no.387840  loss = 2.77056 avg_loss = 3.05490\n",
      "epoch no.5 train no.387850  loss = 1.98271 avg_loss = 3.05763\n",
      "epoch no.5 train no.387860  loss = 3.69753 avg_loss = 3.10090\n",
      "epoch no.5 train no.387870  loss = 4.12752 avg_loss = 3.15487\n",
      "epoch no.5 train no.387880  loss = 3.35886 avg_loss = 3.15516\n",
      "epoch no.5 train no.387890  loss = 3.89205 avg_loss = 3.15976\n",
      "epoch no.5 train no.387900  loss = 2.01213 avg_loss = 3.13542\n",
      "epoch no.5 train no.387910  loss = 2.58142 avg_loss = 3.07350\n",
      "epoch no.5 train no.387920  loss = 2.39552 avg_loss = 3.06260\n",
      "epoch no.5 train no.387930  loss = 1.59197 avg_loss = 3.06967\n",
      "epoch no.5 train no.387940  loss = 2.73372 avg_loss = 3.03603\n",
      "epoch no.5 train no.387950  loss = 2.69333 avg_loss = 3.01109\n",
      "epoch no.5 train no.387960  loss = 2.48073 avg_loss = 3.00959\n",
      "epoch no.5 train no.387970  loss = 2.30578 avg_loss = 2.98716\n",
      "epoch no.5 train no.387980  loss = 3.58137 avg_loss = 2.98451\n",
      "epoch no.5 train no.387990  loss = 3.66363 avg_loss = 2.96530\n",
      "epoch no.5 train no.388000  loss = 4.11870 avg_loss = 2.98465\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁산책', '한', '▁듣는', '▁노래', '</s>']\n",
      "여름 밤 잔잔하게 듣는 노래</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.388010  loss = 2.93227 avg_loss = 2.94648\n",
      "epoch no.5 train no.388020  loss = 2.33700 avg_loss = 2.94693\n",
      "epoch no.5 train no.388030  loss = 2.83287 avg_loss = 2.95181\n",
      "epoch no.5 train no.388040  loss = 2.89991 avg_loss = 2.92996\n",
      "epoch no.5 train no.388050  loss = 1.87357 avg_loss = 2.93576\n",
      "epoch no.5 train no.388060  loss = 2.87095 avg_loss = 2.93170\n",
      "epoch no.5 train no.388070  loss = 4.54732 avg_loss = 2.95557\n",
      "epoch no.5 train no.388080  loss = 3.14392 avg_loss = 2.98841\n",
      "epoch no.5 train no.388090  loss = 3.31208 avg_loss = 3.01387\n",
      "epoch no.5 train no.388100  loss = 3.39414 avg_loss = 2.97985\n",
      "epoch no.5 train no.388110  loss = 3.68219 avg_loss = 2.96423\n",
      "epoch no.5 train no.388120  loss = 2.68601 avg_loss = 2.97917\n",
      "epoch no.5 train no.388130  loss = 2.09056 avg_loss = 2.97366\n",
      "epoch no.5 train no.388140  loss = 2.56667 avg_loss = 2.91852\n",
      "epoch no.5 train no.388150  loss = 5.26106 avg_loss = 2.97989\n",
      "epoch no.5 train no.388160  loss = 2.42675 avg_loss = 2.98781\n",
      "epoch no.5 train no.388170  loss = 2.73113 avg_loss = 3.02381\n",
      "epoch no.5 train no.388180  loss = 2.39613 avg_loss = 3.05472\n",
      "epoch no.5 train no.388190  loss = 3.74512 avg_loss = 3.01861\n",
      "epoch no.5 train no.388200  loss = 3.25659 avg_loss = 3.02880\n",
      "epoch no.5 train no.388210  loss = 3.97878 avg_loss = 3.03329\n",
      "epoch no.5 train no.388220  loss = 3.39735 avg_loss = 3.02402\n",
      "epoch no.5 train no.388230  loss = 2.58786 avg_loss = 3.02333\n",
      "epoch no.5 train no.388240  loss = 3.53178 avg_loss = 3.00080\n",
      "epoch no.5 train no.388250  loss = 3.16681 avg_loss = 3.03120\n",
      "epoch no.5 train no.388260  loss = 4.80223 avg_loss = 3.06011\n",
      "epoch no.5 train no.388270  loss = 4.59367 avg_loss = 3.05745\n",
      "epoch no.5 train no.388280  loss = 2.47613 avg_loss = 3.02968\n",
      "epoch no.5 train no.388290  loss = 4.04336 avg_loss = 3.07073\n",
      "epoch no.5 train no.388300  loss = 2.91745 avg_loss = 3.08064\n",
      "epoch no.5 train no.388310  loss = 2.99896 avg_loss = 3.12607\n",
      "epoch no.5 train no.388320  loss = 2.15936 avg_loss = 3.06657\n",
      "epoch no.5 train no.388330  loss = 4.01908 avg_loss = 3.05494\n",
      "epoch no.5 train no.388340  loss = 2.74641 avg_loss = 3.08112\n",
      "epoch no.5 train no.388350  loss = 3.59220 avg_loss = 3.05774\n",
      "epoch no.5 train no.388360  loss = 3.41543 avg_loss = 3.10453\n",
      "epoch no.5 train no.388370  loss = 3.32326 avg_loss = 3.07614\n",
      "epoch no.5 train no.388380  loss = 3.11380 avg_loss = 3.07209\n",
      "epoch no.5 train no.388390  loss = 2.08856 avg_loss = 3.05869\n",
      "epoch no.5 train no.388400  loss = 3.33548 avg_loss = 3.06219\n",
      "epoch no.5 train no.388410  loss = 2.80364 avg_loss = 3.03148\n",
      "epoch no.5 train no.388420  loss = 3.78388 avg_loss = 3.00758\n",
      "epoch no.5 train no.388430  loss = 2.73981 avg_loss = 2.98862\n",
      "epoch no.5 train no.388440  loss = 3.41161 avg_loss = 3.04554\n",
      "epoch no.5 train no.388450  loss = 2.23313 avg_loss = 3.03644\n",
      "epoch no.5 train no.388460  loss = 3.13439 avg_loss = 3.06480\n",
      "epoch no.5 train no.388470  loss = 2.94589 avg_loss = 3.06358\n",
      "epoch no.5 train no.388480  loss = 2.11706 avg_loss = 3.02763\n",
      "epoch no.5 train no.388490  loss = 2.82937 avg_loss = 2.98481\n",
      "epoch no.5 train no.388500  loss = 4.02723 avg_loss = 2.97756\n",
      "epoch no.5 train no.388510  loss = 2.54796 avg_loss = 2.97297\n",
      "epoch no.5 train no.388520  loss = 4.67321 avg_loss = 3.00022\n",
      "epoch no.5 train no.388530  loss = 2.21981 avg_loss = 3.02233\n",
      "epoch no.5 train no.388540  loss = 2.43252 avg_loss = 2.99351\n",
      "epoch no.5 train no.388550  loss = 4.37405 avg_loss = 3.01677\n",
      "epoch no.5 train no.388560  loss = 1.92376 avg_loss = 3.00289\n",
      "epoch no.5 train no.388570  loss = 3.74175 avg_loss = 3.00672\n",
      "epoch no.5 train no.388580  loss = 3.02192 avg_loss = 2.97336\n",
      "epoch no.5 train no.388590  loss = 3.87350 avg_loss = 2.96218\n",
      "epoch no.5 train no.388600  loss = 3.11552 avg_loss = 2.95218\n",
      "epoch no.5 train no.388610  loss = 3.15795 avg_loss = 2.94995\n",
      "epoch no.5 train no.388620  loss = 4.06219 avg_loss = 2.98916\n",
      "epoch no.5 train no.388630  loss = 2.33669 avg_loss = 2.96115\n",
      "epoch no.5 train no.388640  loss = 4.35024 avg_loss = 3.01988\n",
      "epoch no.5 train no.388650  loss = 2.16521 avg_loss = 3.00879\n",
      "epoch no.5 train no.388660  loss = 2.71621 avg_loss = 3.01774\n",
      "epoch no.5 train no.388670  loss = 2.65743 avg_loss = 3.00724\n",
      "epoch no.5 train no.388680  loss = 3.85197 avg_loss = 3.02981\n",
      "epoch no.5 train no.388690  loss = 3.68437 avg_loss = 3.04359\n",
      "epoch no.5 train no.388700  loss = 3.03932 avg_loss = 3.08254\n",
      "epoch no.5 train no.388710  loss = 3.24934 avg_loss = 3.07165\n",
      "epoch no.5 train no.388720  loss = 2.78295 avg_loss = 3.03725\n",
      "epoch no.5 train no.388730  loss = 2.52638 avg_loss = 3.03655\n",
      "epoch no.5 train no.388740  loss = 4.48730 avg_loss = 3.05656\n",
      "epoch no.5 train no.388750  loss = 3.72301 avg_loss = 3.12290\n",
      "epoch no.5 train no.388760  loss = 3.69840 avg_loss = 3.12130\n",
      "epoch no.5 train no.388770  loss = 3.99838 avg_loss = 3.10633\n",
      "epoch no.5 train no.388780  loss = 2.42818 avg_loss = 3.06019\n",
      "epoch no.5 train no.388790  loss = 2.10757 avg_loss = 3.04287\n",
      "epoch no.5 train no.388800  loss = 2.67156 avg_loss = 3.03925\n",
      "epoch no.5 train no.388810  loss = 2.29736 avg_loss = 3.03008\n",
      "epoch no.5 train no.388820  loss = 3.14250 avg_loss = 3.05746\n",
      "epoch no.5 train no.388830  loss = 2.11559 avg_loss = 3.03963\n",
      "epoch no.5 train no.388840  loss = 2.36144 avg_loss = 3.05440\n",
      "epoch no.5 train no.388850  loss = 4.47961 avg_loss = 3.06172\n",
      "epoch no.5 train no.388860  loss = 3.54448 avg_loss = 3.06017\n",
      "epoch no.5 train no.388870  loss = 3.00391 avg_loss = 3.05668\n",
      "epoch no.5 train no.388880  loss = 2.09783 avg_loss = 3.06393\n",
      "epoch no.5 train no.388890  loss = 1.60595 avg_loss = 3.01834\n",
      "epoch no.5 train no.388900  loss = 3.55588 avg_loss = 2.99311\n",
      "epoch no.5 train no.388910  loss = 2.58662 avg_loss = 3.00048\n",
      "epoch no.5 train no.388920  loss = 3.35499 avg_loss = 2.98146\n",
      "epoch no.5 train no.388930  loss = 6.44176 avg_loss = 2.99095\n",
      "epoch no.5 train no.388940  loss = 1.53490 avg_loss = 2.99432\n",
      "epoch no.5 train no.388950  loss = 2.62370 avg_loss = 3.02189\n",
      "epoch no.5 train no.388960  loss = 2.24144 avg_loss = 3.03076\n",
      "epoch no.5 train no.388970  loss = 1.85906 avg_loss = 2.98206\n",
      "epoch no.5 train no.388980  loss = 3.25719 avg_loss = 2.95133\n",
      "epoch no.5 train no.388990  loss = 2.34808 avg_loss = 2.98518\n",
      "epoch no.5 train no.389000  loss = 2.83636 avg_loss = 2.94887\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁잔잔', '한', '▁감성', '</s>']\n",
      "여름밤의 잔잔한 클래식</s>\n",
      "epoch no.5 train no.389010  loss = 2.38356 avg_loss = 2.90237\n",
      "epoch no.5 train no.389020  loss = 2.10356 avg_loss = 2.91544\n",
      "epoch no.5 train no.389030  loss = 2.80755 avg_loss = 2.95793\n",
      "epoch no.5 train no.389040  loss = 3.06655 avg_loss = 2.97045\n",
      "epoch no.5 train no.389050  loss = 3.15487 avg_loss = 2.98881\n",
      "epoch no.5 train no.389060  loss = 2.95067 avg_loss = 2.97832\n",
      "epoch no.5 train no.389070  loss = 2.32698 avg_loss = 2.99778\n",
      "epoch no.5 train no.389080  loss = 2.81357 avg_loss = 2.99486\n",
      "epoch no.5 train no.389090  loss = 4.07627 avg_loss = 2.99311\n",
      "epoch no.5 train no.389100  loss = 1.75313 avg_loss = 2.97619\n",
      "epoch no.5 train no.389110  loss = 3.84218 avg_loss = 2.98812\n",
      "epoch no.5 train no.389120  loss = 2.57136 avg_loss = 2.93845\n",
      "epoch no.5 train no.389130  loss = 3.93780 avg_loss = 2.91452\n",
      "epoch no.5 train no.389140  loss = 3.56967 avg_loss = 2.91832\n",
      "epoch no.5 train no.389150  loss = 3.72908 avg_loss = 2.92344\n",
      "epoch no.5 train no.389160  loss = 3.28725 avg_loss = 2.97074\n",
      "epoch no.5 train no.389170  loss = 2.57910 avg_loss = 2.99014\n",
      "epoch no.5 train no.389180  loss = 2.77053 avg_loss = 2.95914\n",
      "epoch no.5 train no.389190  loss = 2.89389 avg_loss = 2.96752\n",
      "epoch no.5 train no.389200  loss = 2.15794 avg_loss = 2.94903\n",
      "epoch no.5 train no.389210  loss = 2.20852 avg_loss = 2.94574\n",
      "epoch no.5 train no.389220  loss = 2.86134 avg_loss = 2.96427\n",
      "epoch no.5 train no.389230  loss = 2.21584 avg_loss = 2.93357\n",
      "epoch no.5 train no.389240  loss = 2.98479 avg_loss = 2.96886\n",
      "epoch no.5 train no.389250  loss = 5.08748 avg_loss = 2.98837\n",
      "epoch no.5 train no.389260  loss = 2.99497 avg_loss = 2.99211\n",
      "epoch no.5 train no.389270  loss = 3.09474 avg_loss = 3.00245\n",
      "epoch no.5 train no.389280  loss = 2.14002 avg_loss = 2.99244\n",
      "epoch no.5 train no.389290  loss = 6.03585 avg_loss = 2.98942\n",
      "epoch no.5 train no.389300  loss = 2.49292 avg_loss = 2.97386\n",
      "epoch no.5 train no.389310  loss = 4.06075 avg_loss = 2.96411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.389320  loss = 2.01522 avg_loss = 2.98667\n",
      "epoch no.5 train no.389330  loss = 2.35587 avg_loss = 3.01240\n",
      "epoch no.5 train no.389340  loss = 2.46431 avg_loss = 2.97470\n",
      "epoch no.5 train no.389350  loss = 3.30976 avg_loss = 2.99350\n",
      "epoch no.5 train no.389360  loss = 1.55494 avg_loss = 2.98404\n",
      "epoch no.5 train no.389370  loss = 4.23414 avg_loss = 2.98993\n",
      "epoch no.5 train no.389380  loss = 2.87597 avg_loss = 2.97285\n",
      "epoch no.5 train no.389390  loss = 3.73731 avg_loss = 2.97832\n",
      "epoch no.5 train no.389400  loss = 3.46308 avg_loss = 3.01085\n",
      "epoch no.5 train no.389410  loss = 4.65842 avg_loss = 3.02643\n",
      "epoch no.5 train no.389420  loss = 2.87990 avg_loss = 3.05175\n",
      "epoch no.5 train no.389430  loss = 0.98165 avg_loss = 3.04509\n",
      "epoch no.5 train no.389440  loss = 3.37799 avg_loss = 3.04487\n",
      "epoch no.5 train no.389450  loss = 3.54792 avg_loss = 3.04106\n",
      "epoch no.5 train no.389460  loss = 3.56088 avg_loss = 3.04817\n",
      "epoch no.5 train no.389470  loss = 2.46412 avg_loss = 3.02572\n",
      "epoch no.5 train no.389480  loss = 2.68901 avg_loss = 2.99940\n",
      "epoch no.5 train no.389490  loss = 3.47533 avg_loss = 3.01828\n",
      "epoch no.5 train no.389500  loss = 2.90479 avg_loss = 3.06321\n",
      "epoch no.5 train no.389510  loss = 2.99574 avg_loss = 3.08878\n",
      "epoch no.5 train no.389520  loss = 3.34392 avg_loss = 3.07205\n",
      "epoch no.5 train no.389530  loss = 5.44579 avg_loss = 3.06895\n",
      "epoch no.5 train no.389540  loss = 2.69618 avg_loss = 3.06137\n",
      "epoch no.5 train no.389550  loss = 3.17521 avg_loss = 3.04944\n",
      "epoch no.5 train no.389560  loss = 3.32335 avg_loss = 3.04963\n",
      "epoch no.5 train no.389570  loss = 2.55604 avg_loss = 3.07612\n",
      "epoch no.5 train no.389580  loss = 3.35856 avg_loss = 3.04552\n",
      "epoch no.5 train no.389590  loss = 4.96268 avg_loss = 3.08600\n",
      "epoch no.5 train no.389600  loss = 3.13733 avg_loss = 3.04014\n",
      "epoch no.5 train no.389610  loss = 3.09602 avg_loss = 3.04085\n",
      "epoch no.5 train no.389620  loss = 1.81133 avg_loss = 3.06151\n",
      "epoch no.5 train no.389630  loss = 3.35137 avg_loss = 3.05851\n",
      "epoch no.5 train no.389640  loss = 3.10716 avg_loss = 3.01447\n",
      "epoch no.5 train no.389650  loss = 3.88350 avg_loss = 3.01031\n",
      "epoch no.5 train no.389660  loss = 3.27187 avg_loss = 3.01823\n",
      "epoch no.5 train no.389670  loss = 2.50671 avg_loss = 2.96818\n",
      "epoch no.5 train no.389680  loss = 2.17274 avg_loss = 2.92632\n",
      "epoch no.5 train no.389690  loss = 1.66594 avg_loss = 2.91491\n",
      "epoch no.5 train no.389700  loss = 4.81209 avg_loss = 2.95438\n",
      "epoch no.5 train no.389710  loss = 2.50967 avg_loss = 2.95925\n",
      "epoch no.5 train no.389720  loss = 2.76021 avg_loss = 2.93836\n",
      "epoch no.5 train no.389730  loss = 3.73569 avg_loss = 2.93101\n",
      "epoch no.5 train no.389740  loss = 2.11107 avg_loss = 2.95394\n",
      "epoch no.5 train no.389750  loss = 3.40111 avg_loss = 2.96383\n",
      "epoch no.5 train no.389760  loss = 2.01326 avg_loss = 2.95745\n",
      "epoch no.5 train no.389770  loss = 3.34647 avg_loss = 3.00229\n",
      "epoch no.5 train no.389780  loss = 2.79605 avg_loss = 3.02092\n",
      "epoch no.5 train no.389790  loss = 2.50793 avg_loss = 3.00594\n",
      "epoch no.5 train no.389800  loss = 4.55226 avg_loss = 3.01815\n",
      "epoch no.5 train no.389810  loss = 4.45809 avg_loss = 3.01531\n",
      "epoch no.5 train no.389820  loss = 3.50136 avg_loss = 3.01309\n",
      "epoch no.5 train no.389830  loss = 1.69850 avg_loss = 2.97890\n",
      "epoch no.5 train no.389840  loss = 2.17027 avg_loss = 2.98403\n",
      "epoch no.5 train no.389850  loss = 3.17916 avg_loss = 3.00509\n",
      "epoch no.5 train no.389860  loss = 3.27506 avg_loss = 3.01277\n",
      "epoch no.5 train no.389870  loss = 2.81469 avg_loss = 3.01663\n",
      "epoch no.5 train no.389880  loss = 2.63791 avg_loss = 3.02688\n",
      "epoch no.5 train no.389890  loss = 3.88322 avg_loss = 3.03353\n",
      "epoch no.5 train no.389900  loss = 3.32963 avg_loss = 3.07526\n",
      "epoch no.5 train no.389910  loss = 3.36703 avg_loss = 3.11118\n",
      "epoch no.5 train no.389920  loss = 2.17052 avg_loss = 3.08694\n",
      "epoch no.5 train no.389930  loss = 3.04929 avg_loss = 3.06431\n",
      "epoch no.5 train no.389940  loss = 3.23731 avg_loss = 3.06171\n",
      "epoch no.5 train no.389950  loss = 3.06237 avg_loss = 3.05368\n",
      "epoch no.5 train no.389960  loss = 2.31637 avg_loss = 3.05639\n",
      "epoch no.5 train no.389970  loss = 3.12239 avg_loss = 3.07839\n",
      "epoch no.5 train no.389980  loss = 3.50835 avg_loss = 3.07264\n",
      "epoch no.5 train no.389990  loss = 1.84936 avg_loss = 3.04974\n",
      "epoch no.5 train no.390000  loss = 2.28084 avg_loss = 3.01256\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '음악', '</s>']\n",
      "여름밤에 듣기 좋은 인디음악</s>\n",
      "epoch no.5 train no.390010  loss = 2.15974 avg_loss = 3.05614\n",
      "epoch no.5 train no.390020  loss = 2.21356 avg_loss = 3.04272\n",
      "epoch no.5 train no.390030  loss = 4.29433 avg_loss = 3.03747\n",
      "epoch no.5 train no.390040  loss = 3.83927 avg_loss = 3.07251\n",
      "epoch no.5 train no.390050  loss = 4.06729 avg_loss = 3.05166\n",
      "epoch no.5 train no.390060  loss = 4.08330 avg_loss = 3.01967\n",
      "epoch no.5 train no.390070  loss = 3.50676 avg_loss = 3.03445\n",
      "epoch no.5 train no.390080  loss = 3.20462 avg_loss = 3.03714\n",
      "epoch no.5 train no.390090  loss = 3.16076 avg_loss = 3.04423\n",
      "epoch no.5 train no.390100  loss = 2.47474 avg_loss = 3.09030\n",
      "epoch no.5 train no.390110  loss = 1.47950 avg_loss = 3.12671\n",
      "epoch no.5 train no.390120  loss = 3.81908 avg_loss = 3.08792\n",
      "epoch no.5 train no.390130  loss = 3.04331 avg_loss = 3.11495\n",
      "epoch no.5 train no.390140  loss = 2.34048 avg_loss = 3.07785\n",
      "epoch no.5 train no.390150  loss = 2.36206 avg_loss = 3.07962\n",
      "epoch no.5 train no.390160  loss = 2.58244 avg_loss = 3.07094\n",
      "epoch no.5 train no.390170  loss = 2.74891 avg_loss = 3.05182\n",
      "epoch no.5 train no.390180  loss = 1.86137 avg_loss = 3.02962\n",
      "epoch no.5 train no.390190  loss = 1.96921 avg_loss = 3.01651\n",
      "epoch no.5 train no.390200  loss = 2.76358 avg_loss = 3.00021\n",
      "epoch no.5 train no.390210  loss = 3.15556 avg_loss = 3.01600\n",
      "epoch no.5 train no.390220  loss = 3.73117 avg_loss = 3.02696\n",
      "epoch no.5 train no.390230  loss = 1.86020 avg_loss = 3.05430\n",
      "epoch no.5 train no.390240  loss = 3.19958 avg_loss = 3.04343\n",
      "epoch no.5 train no.390250  loss = 3.45634 avg_loss = 3.06044\n",
      "epoch no.5 train no.390260  loss = 2.16732 avg_loss = 3.03037\n",
      "epoch no.5 train no.390270  loss = 2.51135 avg_loss = 3.04726\n",
      "epoch no.5 train no.390280  loss = 3.24917 avg_loss = 3.02213\n",
      "epoch no.5 train no.390290  loss = 2.01587 avg_loss = 3.01495\n",
      "epoch no.5 train no.390300  loss = 1.87431 avg_loss = 2.97385\n",
      "epoch no.5 train no.390310  loss = 3.07158 avg_loss = 3.00669\n",
      "epoch no.5 train no.390320  loss = 2.62814 avg_loss = 3.02097\n",
      "epoch no.5 train no.390330  loss = 2.12242 avg_loss = 3.04558\n",
      "epoch no.5 train no.390340  loss = 2.46774 avg_loss = 3.03626\n",
      "epoch no.5 train no.390350  loss = 3.11057 avg_loss = 3.02969\n",
      "epoch no.5 train no.390360  loss = 2.22059 avg_loss = 3.00057\n",
      "epoch no.5 train no.390370  loss = 2.54521 avg_loss = 2.97888\n",
      "epoch no.5 train no.390380  loss = 3.89883 avg_loss = 2.93830\n",
      "epoch no.5 train no.390390  loss = 2.17158 avg_loss = 2.94600\n",
      "epoch no.5 train no.390400  loss = 2.66074 avg_loss = 2.95714\n",
      "epoch no.5 train no.390410  loss = 3.52141 avg_loss = 2.95245\n",
      "epoch no.5 train no.390420  loss = 2.98757 avg_loss = 2.96840\n",
      "epoch no.5 train no.390430  loss = 2.26132 avg_loss = 2.96284\n",
      "epoch no.5 train no.390440  loss = 3.43938 avg_loss = 2.97515\n",
      "epoch no.5 train no.390450  loss = 2.94022 avg_loss = 2.98689\n",
      "epoch no.5 train no.390460  loss = 3.17883 avg_loss = 2.95820\n",
      "epoch no.5 train no.390470  loss = 3.11543 avg_loss = 2.97625\n",
      "epoch no.5 train no.390480  loss = 2.50532 avg_loss = 2.95311\n",
      "epoch no.5 train no.390490  loss = 2.60291 avg_loss = 2.98444\n",
      "epoch no.5 train no.390500  loss = 2.77782 avg_loss = 2.98747\n",
      "epoch no.5 train no.390510  loss = 3.04883 avg_loss = 2.96762\n",
      "epoch no.5 train no.390520  loss = 3.33524 avg_loss = 2.98444\n",
      "epoch no.5 train no.390530  loss = 3.66100 avg_loss = 2.97838\n",
      "epoch no.5 train no.390540  loss = 1.71032 avg_loss = 2.99864\n",
      "epoch no.5 train no.390550  loss = 4.80802 avg_loss = 3.06193\n",
      "epoch no.5 train no.390560  loss = 2.76861 avg_loss = 3.07756\n",
      "epoch no.5 train no.390570  loss = 3.01841 avg_loss = 3.07320\n",
      "epoch no.5 train no.390580  loss = 2.85046 avg_loss = 3.04668\n",
      "epoch no.5 train no.390590  loss = 3.75961 avg_loss = 3.08022\n",
      "epoch no.5 train no.390600  loss = 2.50656 avg_loss = 3.10197\n",
      "epoch no.5 train no.390610  loss = 2.22321 avg_loss = 3.07009\n",
      "epoch no.5 train no.390620  loss = 4.37951 avg_loss = 3.06025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.390630  loss = 1.64713 avg_loss = 3.05635\n",
      "epoch no.5 train no.390640  loss = 2.85441 avg_loss = 3.03118\n",
      "epoch no.5 train no.390650  loss = 3.93905 avg_loss = 2.99575\n",
      "epoch no.5 train no.390660  loss = 3.65921 avg_loss = 2.99777\n",
      "epoch no.5 train no.390670  loss = 2.35694 avg_loss = 3.00398\n",
      "epoch no.5 train no.390680  loss = 3.59998 avg_loss = 3.00490\n",
      "epoch no.5 train no.390690  loss = 3.73562 avg_loss = 3.02824\n",
      "epoch no.5 train no.390700  loss = 4.42759 avg_loss = 3.01270\n",
      "epoch no.5 train no.390710  loss = 2.82228 avg_loss = 2.99984\n",
      "epoch no.5 train no.390720  loss = 3.96400 avg_loss = 3.00791\n",
      "epoch no.5 train no.390730  loss = 3.16749 avg_loss = 3.00848\n",
      "epoch no.5 train no.390740  loss = 3.82270 avg_loss = 3.04242\n",
      "epoch no.5 train no.390750  loss = 2.19359 avg_loss = 3.04998\n",
      "epoch no.5 train no.390760  loss = 3.45155 avg_loss = 3.05705\n",
      "epoch no.5 train no.390770  loss = 2.76137 avg_loss = 3.06713\n",
      "epoch no.5 train no.390780  loss = 3.18487 avg_loss = 3.09546\n",
      "epoch no.5 train no.390790  loss = 2.87727 avg_loss = 3.08932\n",
      "epoch no.5 train no.390800  loss = 2.85324 avg_loss = 3.16149\n",
      "epoch no.5 train no.390810  loss = 5.04865 avg_loss = 3.17088\n",
      "epoch no.5 train no.390820  loss = 2.81552 avg_loss = 3.18789\n",
      "epoch no.5 train no.390830  loss = 3.61478 avg_loss = 3.17240\n",
      "epoch no.5 train no.390840  loss = 2.88249 avg_loss = 3.14386\n",
      "epoch no.5 train no.390850  loss = 2.54066 avg_loss = 3.12821\n",
      "epoch no.5 train no.390860  loss = 2.08687 avg_loss = 3.11446\n",
      "epoch no.5 train no.390870  loss = 4.13040 avg_loss = 3.13027\n",
      "epoch no.5 train no.390880  loss = 2.58377 avg_loss = 3.10043\n",
      "epoch no.5 train no.390890  loss = 1.97550 avg_loss = 3.11113\n",
      "epoch no.5 train no.390900  loss = 3.87073 avg_loss = 3.14132\n",
      "epoch no.5 train no.390910  loss = 4.45840 avg_loss = 3.14276\n",
      "epoch no.5 train no.390920  loss = 2.35958 avg_loss = 3.15129\n",
      "epoch no.5 train no.390930  loss = 2.79197 avg_loss = 3.13817\n",
      "epoch no.5 train no.390940  loss = 4.74469 avg_loss = 3.17434\n",
      "epoch no.5 train no.390950  loss = 2.96001 avg_loss = 3.18667\n",
      "epoch no.5 train no.390960  loss = 2.35639 avg_loss = 3.16361\n",
      "epoch no.5 train no.390970  loss = 2.24748 avg_loss = 3.14739\n",
      "epoch no.5 train no.390980  loss = 2.53195 avg_loss = 3.12586\n",
      "epoch no.5 train no.390990  loss = 2.88018 avg_loss = 3.10124\n",
      "epoch no.5 train no.391000  loss = 3.43889 avg_loss = 3.09312\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣기', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.5 train no.391010  loss = 2.52682 avg_loss = 3.09133\n",
      "epoch no.5 train no.391020  loss = 1.75312 avg_loss = 3.04863\n",
      "epoch no.5 train no.391030  loss = 3.10094 avg_loss = 3.03034\n",
      "epoch no.5 train no.391040  loss = 3.82570 avg_loss = 3.07006\n",
      "epoch no.5 train no.391050  loss = 5.86176 avg_loss = 3.07889\n",
      "epoch no.5 train no.391060  loss = 3.81027 avg_loss = 3.11208\n",
      "epoch no.5 train no.391070  loss = 3.61080 avg_loss = 3.12053\n",
      "epoch no.5 train no.391080  loss = 2.41576 avg_loss = 3.13803\n",
      "epoch no.5 train no.391090  loss = 2.31803 avg_loss = 3.12037\n",
      "epoch no.5 train no.391100  loss = 2.22856 avg_loss = 3.09894\n",
      "epoch no.5 train no.391110  loss = 2.65263 avg_loss = 3.09058\n",
      "epoch no.5 train no.391120  loss = 2.79818 avg_loss = 3.08342\n",
      "epoch no.5 train no.391130  loss = 1.76267 avg_loss = 3.06766\n",
      "epoch no.5 train no.391140  loss = 2.76962 avg_loss = 3.08639\n",
      "epoch no.5 train no.391150  loss = 3.14675 avg_loss = 3.08046\n",
      "epoch no.5 train no.391160  loss = 3.23709 avg_loss = 3.10648\n",
      "epoch no.5 train no.391170  loss = 2.22598 avg_loss = 3.07321\n",
      "epoch no.5 train no.391180  loss = 2.89897 avg_loss = 3.03677\n",
      "epoch no.5 train no.391190  loss = 2.86086 avg_loss = 3.06834\n",
      "epoch no.5 train no.391200  loss = 3.06177 avg_loss = 3.06194\n",
      "epoch no.5 train no.391210  loss = 2.99688 avg_loss = 3.07341\n",
      "epoch no.5 train no.391220  loss = 3.60454 avg_loss = 3.06275\n",
      "epoch no.5 train no.391230  loss = 4.08041 avg_loss = 3.06140\n",
      "epoch no.5 train no.391240  loss = 2.89994 avg_loss = 3.05639\n",
      "epoch no.5 train no.391250  loss = 3.60663 avg_loss = 3.05104\n",
      "epoch no.5 train no.391260  loss = 3.67742 avg_loss = 3.03398\n",
      "epoch no.5 train no.391270  loss = 2.91421 avg_loss = 3.04359\n",
      "epoch no.5 train no.391280  loss = 3.85691 avg_loss = 3.10771\n",
      "epoch no.5 train no.391290  loss = 2.93569 avg_loss = 3.08559\n",
      "epoch no.5 train no.391300  loss = 3.08258 avg_loss = 3.09908\n",
      "epoch no.5 train no.391310  loss = 4.37256 avg_loss = 3.07133\n",
      "epoch no.5 train no.391320  loss = 2.03249 avg_loss = 3.10164\n",
      "epoch no.5 train no.391330  loss = 3.27912 avg_loss = 3.09463\n",
      "epoch no.5 train no.391340  loss = 3.21526 avg_loss = 3.07148\n",
      "epoch no.5 train no.391350  loss = 4.94800 avg_loss = 3.05152\n",
      "epoch no.5 train no.391360  loss = 2.58483 avg_loss = 3.05042\n",
      "epoch no.5 train no.391370  loss = 2.45759 avg_loss = 3.11931\n",
      "epoch no.5 train no.391380  loss = 2.56967 avg_loss = 3.10100\n",
      "epoch no.5 train no.391390  loss = 2.68455 avg_loss = 3.09804\n",
      "epoch no.5 train no.391400  loss = 2.41347 avg_loss = 3.08166\n",
      "epoch no.5 train no.391410  loss = 2.87374 avg_loss = 3.04747\n",
      "epoch no.5 train no.391420  loss = 4.54612 avg_loss = 3.04824\n",
      "epoch no.5 train no.391430  loss = 4.30607 avg_loss = 3.07419\n",
      "epoch no.5 train no.391440  loss = 3.31569 avg_loss = 3.08091\n",
      "epoch no.5 train no.391450  loss = 4.15525 avg_loss = 3.11304\n",
      "epoch no.5 train no.391460  loss = 4.78852 avg_loss = 3.13620\n",
      "epoch no.5 train no.391470  loss = 2.41911 avg_loss = 3.10820\n",
      "epoch no.5 train no.391480  loss = 3.23551 avg_loss = 3.10303\n",
      "epoch no.5 train no.391490  loss = 2.67864 avg_loss = 3.08990\n",
      "epoch no.5 train no.391500  loss = 3.71833 avg_loss = 3.12788\n",
      "epoch no.5 train no.391510  loss = 3.30856 avg_loss = 3.10379\n",
      "epoch no.5 train no.391520  loss = 3.75189 avg_loss = 3.11844\n",
      "epoch no.5 train no.391530  loss = 3.02248 avg_loss = 3.12066\n",
      "epoch no.5 train no.391540  loss = 3.20400 avg_loss = 3.10491\n",
      "epoch no.5 train no.391550  loss = 2.44412 avg_loss = 3.10000\n",
      "epoch no.5 train no.391560  loss = 2.77092 avg_loss = 3.09919\n",
      "epoch no.5 train no.391570  loss = 3.77649 avg_loss = 3.10469\n",
      "epoch no.5 train no.391580  loss = 4.41320 avg_loss = 3.09991\n",
      "epoch no.5 train no.391590  loss = 2.23832 avg_loss = 3.05194\n",
      "epoch no.5 train no.391600  loss = 3.05934 avg_loss = 3.03139\n",
      "epoch no.5 train no.391610  loss = 4.48774 avg_loss = 3.06561\n",
      "epoch no.5 train no.391620  loss = 3.07875 avg_loss = 3.05726\n",
      "epoch no.5 train no.391630  loss = 3.49542 avg_loss = 3.11877\n",
      "epoch no.5 train no.391640  loss = 2.50340 avg_loss = 3.08729\n",
      "epoch no.5 train no.391650  loss = 3.87682 avg_loss = 3.07234\n",
      "epoch no.5 train no.391660  loss = 3.23102 avg_loss = 3.06179\n",
      "epoch no.5 train no.391670  loss = 3.51551 avg_loss = 3.01452\n",
      "epoch no.5 train no.391680  loss = 3.02945 avg_loss = 3.02187\n",
      "epoch no.5 train no.391690  loss = 2.56043 avg_loss = 3.02087\n",
      "epoch no.5 train no.391700  loss = 2.99908 avg_loss = 3.02946\n",
      "epoch no.5 train no.391710  loss = 2.71203 avg_loss = 3.01992\n",
      "epoch no.5 train no.391720  loss = 2.54638 avg_loss = 3.01320\n",
      "epoch no.5 train no.391730  loss = 2.63198 avg_loss = 2.99271\n",
      "epoch no.5 train no.391740  loss = 1.90866 avg_loss = 3.01733\n",
      "epoch no.5 train no.391750  loss = 3.36913 avg_loss = 3.02148\n",
      "epoch no.5 train no.391760  loss = 3.78163 avg_loss = 3.03200\n",
      "epoch no.5 train no.391770  loss = 3.01741 avg_loss = 3.05693\n",
      "epoch no.5 train no.391780  loss = 3.05380 avg_loss = 3.01032\n",
      "epoch no.5 train no.391790  loss = 2.13983 avg_loss = 3.02291\n",
      "epoch no.5 train no.391800  loss = 3.01594 avg_loss = 3.02243\n",
      "epoch no.5 train no.391810  loss = 2.22239 avg_loss = 3.01416\n",
      "epoch no.5 train no.391820  loss = 4.27212 avg_loss = 2.97889\n",
      "epoch no.5 train no.391830  loss = 2.91289 avg_loss = 2.97704\n",
      "epoch no.5 train no.391840  loss = 4.08446 avg_loss = 2.97416\n",
      "epoch no.5 train no.391850  loss = 2.01314 avg_loss = 2.92940\n",
      "epoch no.5 train no.391860  loss = 2.78078 avg_loss = 2.95758\n",
      "epoch no.5 train no.391870  loss = 3.21423 avg_loss = 2.94076\n",
      "epoch no.5 train no.391880  loss = 2.94209 avg_loss = 2.92629\n",
      "epoch no.5 train no.391890  loss = 3.82477 avg_loss = 2.94603\n",
      "epoch no.5 train no.391900  loss = 3.17588 avg_loss = 2.97521\n",
      "epoch no.5 train no.391910  loss = 2.74415 avg_loss = 2.99654\n",
      "epoch no.5 train no.391920  loss = 3.12695 avg_loss = 2.98088\n",
      "epoch no.5 train no.391930  loss = 2.74987 avg_loss = 2.96644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.391940  loss = 2.51762 avg_loss = 2.93125\n",
      "epoch no.5 train no.391950  loss = 1.81670 avg_loss = 2.92855\n",
      "epoch no.5 train no.391960  loss = 4.34001 avg_loss = 2.97352\n",
      "epoch no.5 train no.391970  loss = 3.67023 avg_loss = 2.97205\n",
      "epoch no.5 train no.391980  loss = 2.21932 avg_loss = 2.98990\n",
      "epoch no.5 train no.391990  loss = 2.80193 avg_loss = 3.00424\n",
      "epoch no.5 train no.392000  loss = 2.03897 avg_loss = 2.97720\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁꿀', '적인', '▁노래', '음악', '</s>']\n",
      "여름밤의 감성적인 인디음악</s>\n",
      "epoch no.5 train no.392010  loss = 2.70565 avg_loss = 2.95711\n",
      "epoch no.5 train no.392020  loss = 4.68360 avg_loss = 2.98579\n",
      "epoch no.5 train no.392030  loss = 2.70393 avg_loss = 2.95321\n",
      "epoch no.5 train no.392040  loss = 3.38593 avg_loss = 2.96419\n",
      "epoch no.5 train no.392050  loss = 3.87201 avg_loss = 3.00091\n",
      "epoch no.5 train no.392060  loss = 2.10941 avg_loss = 3.00937\n",
      "epoch no.5 train no.392070  loss = 3.48337 avg_loss = 2.99218\n",
      "epoch no.5 train no.392080  loss = 3.07313 avg_loss = 3.02340\n",
      "epoch no.5 train no.392090  loss = 3.91066 avg_loss = 3.05949\n",
      "epoch no.5 train no.392100  loss = 3.78657 avg_loss = 3.07601\n",
      "epoch no.5 train no.392110  loss = 2.16181 avg_loss = 3.06935\n",
      "epoch no.5 train no.392120  loss = 3.11006 avg_loss = 3.04108\n",
      "epoch no.5 train no.392130  loss = 2.44910 avg_loss = 3.06566\n",
      "epoch no.5 train no.392140  loss = 2.25005 avg_loss = 3.03550\n",
      "epoch no.5 train no.392150  loss = 2.83656 avg_loss = 3.06607\n",
      "epoch no.5 train no.392160  loss = 3.04881 avg_loss = 3.09826\n",
      "epoch no.5 train no.392170  loss = 3.00293 avg_loss = 3.13649\n",
      "epoch no.5 train no.392180  loss = 4.01371 avg_loss = 3.14750\n",
      "epoch no.5 train no.392190  loss = 5.61669 avg_loss = 3.16515\n",
      "epoch no.5 train no.392200  loss = 3.75098 avg_loss = 3.17161\n",
      "epoch no.5 train no.392210  loss = 2.04737 avg_loss = 3.16375\n",
      "epoch no.5 train no.392220  loss = 2.38381 avg_loss = 3.13068\n",
      "epoch no.5 train no.392230  loss = 3.76564 avg_loss = 3.18321\n",
      "epoch no.5 train no.392240  loss = 2.30841 avg_loss = 3.17493\n",
      "epoch no.5 train no.392250  loss = 2.66411 avg_loss = 3.14808\n",
      "epoch no.5 train no.392260  loss = 3.56028 avg_loss = 3.12743\n",
      "epoch no.5 train no.392270  loss = 5.23260 avg_loss = 3.11080\n",
      "epoch no.5 train no.392280  loss = 3.09363 avg_loss = 3.10537\n",
      "epoch no.5 train no.392290  loss = 2.44256 avg_loss = 3.08495\n",
      "epoch no.5 train no.392300  loss = 2.28314 avg_loss = 3.05264\n",
      "epoch no.5 train no.392310  loss = 3.03500 avg_loss = 3.07219\n",
      "epoch no.5 train no.392320  loss = 4.09609 avg_loss = 3.09681\n",
      "epoch no.5 train no.392330  loss = 3.19113 avg_loss = 3.07518\n",
      "epoch no.5 train no.392340  loss = 4.34433 avg_loss = 3.06437\n",
      "epoch no.5 train no.392350  loss = 2.90632 avg_loss = 3.07297\n",
      "epoch no.5 train no.392360  loss = 2.73843 avg_loss = 3.07425\n",
      "epoch no.5 train no.392370  loss = 2.50803 avg_loss = 3.03216\n",
      "epoch no.5 train no.392380  loss = 2.52349 avg_loss = 3.04702\n",
      "epoch no.5 train no.392390  loss = 2.88178 avg_loss = 3.04720\n",
      "epoch no.5 train no.392400  loss = 4.51589 avg_loss = 3.06339\n",
      "epoch no.5 train no.392410  loss = 2.88777 avg_loss = 3.08962\n",
      "epoch no.5 train no.392420  loss = 2.50092 avg_loss = 3.06773\n",
      "epoch no.5 train no.392430  loss = 3.39057 avg_loss = 3.08934\n",
      "epoch no.5 train no.392440  loss = 3.48944 avg_loss = 3.09857\n",
      "epoch no.5 train no.392450  loss = 2.12695 avg_loss = 3.10077\n",
      "epoch no.5 train no.392460  loss = 2.64307 avg_loss = 3.10006\n",
      "epoch no.5 train no.392470  loss = 3.29206 avg_loss = 3.07677\n",
      "epoch no.5 train no.392480  loss = 3.77004 avg_loss = 3.11685\n",
      "epoch no.5 train no.392490  loss = 3.15694 avg_loss = 3.12169\n",
      "epoch no.5 train no.392500  loss = 3.24199 avg_loss = 3.11721\n",
      "epoch no.5 train no.392510  loss = 3.16143 avg_loss = 3.09066\n",
      "epoch no.5 train no.392520  loss = 2.76944 avg_loss = 3.08513\n",
      "epoch no.5 train no.392530  loss = 2.07749 avg_loss = 3.08335\n",
      "epoch no.5 train no.392540  loss = 4.29408 avg_loss = 3.08853\n",
      "epoch no.5 train no.392550  loss = 2.72732 avg_loss = 3.04263\n",
      "epoch no.5 train no.392560  loss = 2.36264 avg_loss = 3.02658\n",
      "epoch no.5 train no.392570  loss = 4.55753 avg_loss = 3.02522\n",
      "epoch no.5 train no.392580  loss = 2.88763 avg_loss = 3.07261\n",
      "epoch no.5 train no.392590  loss = 2.12762 avg_loss = 3.05828\n",
      "epoch no.5 train no.392600  loss = 1.76298 avg_loss = 3.06047\n",
      "epoch no.5 train no.392610  loss = 3.35489 avg_loss = 3.06327\n",
      "epoch no.5 train no.392620  loss = 3.16010 avg_loss = 3.02125\n",
      "epoch no.5 train no.392630  loss = 3.05551 avg_loss = 3.01456\n",
      "epoch no.5 train no.392640  loss = 3.22095 avg_loss = 2.99933\n",
      "epoch no.5 train no.392650  loss = 1.32888 avg_loss = 2.97877\n",
      "epoch no.5 train no.392660  loss = 4.58382 avg_loss = 3.01372\n",
      "epoch no.5 train no.392670  loss = 2.96903 avg_loss = 3.02666\n",
      "epoch no.5 train no.392680  loss = 3.32246 avg_loss = 3.04210\n",
      "epoch no.5 train no.392690  loss = 2.77276 avg_loss = 3.02233\n",
      "epoch no.5 train no.392700  loss = 3.67491 avg_loss = 3.03284\n",
      "epoch no.5 train no.392710  loss = 2.43510 avg_loss = 3.05373\n",
      "epoch no.5 train no.392720  loss = 3.17111 avg_loss = 3.03841\n",
      "epoch no.5 train no.392730  loss = 1.42278 avg_loss = 2.99299\n",
      "epoch no.5 train no.392740  loss = 2.64811 avg_loss = 3.02993\n",
      "epoch no.5 train no.392750  loss = 3.33257 avg_loss = 3.03442\n",
      "epoch no.5 train no.392760  loss = 2.19661 avg_loss = 2.99838\n",
      "epoch no.5 train no.392770  loss = 2.11869 avg_loss = 2.99380\n",
      "epoch no.5 train no.392780  loss = 4.63752 avg_loss = 2.99842\n",
      "epoch no.5 train no.392790  loss = 2.47636 avg_loss = 3.01088\n",
      "epoch no.5 train no.392800  loss = 2.55410 avg_loss = 2.99297\n",
      "epoch no.5 train no.392810  loss = 3.02603 avg_loss = 2.99292\n",
      "epoch no.5 train no.392820  loss = 3.28753 avg_loss = 2.99866\n",
      "epoch no.5 train no.392830  loss = 1.98849 avg_loss = 2.99379\n",
      "epoch no.5 train no.392840  loss = 2.87612 avg_loss = 2.95868\n",
      "epoch no.5 train no.392850  loss = 3.46772 avg_loss = 2.97977\n",
      "epoch no.5 train no.392860  loss = 2.89908 avg_loss = 2.94427\n",
      "epoch no.5 train no.392870  loss = 2.65712 avg_loss = 2.96150\n",
      "epoch no.5 train no.392880  loss = 2.89454 avg_loss = 2.96889\n",
      "epoch no.5 train no.392890  loss = 2.62476 avg_loss = 2.94589\n",
      "epoch no.5 train no.392900  loss = 2.82142 avg_loss = 2.94790\n",
      "epoch no.5 train no.392910  loss = 2.16249 avg_loss = 2.95503\n",
      "epoch no.5 train no.392920  loss = 3.29481 avg_loss = 2.94381\n",
      "epoch no.5 train no.392930  loss = 4.93332 avg_loss = 2.98191\n",
      "epoch no.5 train no.392940  loss = 2.85515 avg_loss = 3.01615\n",
      "epoch no.5 train no.392950  loss = 2.44806 avg_loss = 2.99390\n",
      "epoch no.5 train no.392960  loss = 2.53243 avg_loss = 2.99237\n",
      "epoch no.5 train no.392970  loss = 2.58503 avg_loss = 2.97731\n",
      "epoch no.5 train no.392980  loss = 3.89953 avg_loss = 2.96480\n",
      "epoch no.5 train no.392990  loss = 2.51132 avg_loss = 2.95770\n",
      "epoch no.5 train no.393000  loss = 4.06369 avg_loss = 2.94567\n",
      "6\n",
      "to_tokens: ['▁비', '날', '▁잘', '▁하는', '▁좋은', '▁음악', '</s>', '</s>']\n",
      "여름과 함께하면 좋은 음악들</s>\n",
      "epoch no.5 train no.393010  loss = 2.91750 avg_loss = 2.95538\n",
      "epoch no.5 train no.393020  loss = 2.14817 avg_loss = 2.95149\n",
      "epoch no.5 train no.393030  loss = 2.50084 avg_loss = 2.95958\n",
      "epoch no.5 train no.393040  loss = 5.02893 avg_loss = 2.99341\n",
      "epoch no.5 train no.393050  loss = 2.39074 avg_loss = 2.99635\n",
      "epoch no.5 train no.393060  loss = 2.00078 avg_loss = 2.99391\n",
      "epoch no.5 train no.393070  loss = 3.26702 avg_loss = 3.02017\n",
      "epoch no.5 train no.393080  loss = 2.56981 avg_loss = 2.98679\n",
      "epoch no.5 train no.393090  loss = 3.52719 avg_loss = 3.02451\n",
      "epoch no.5 train no.393100  loss = 3.54772 avg_loss = 3.05815\n",
      "epoch no.5 train no.393110  loss = 2.35378 avg_loss = 3.01623\n",
      "epoch no.5 train no.393120  loss = 2.11238 avg_loss = 3.01447\n",
      "epoch no.5 train no.393130  loss = 2.30076 avg_loss = 3.03221\n",
      "epoch no.5 train no.393140  loss = 2.16666 avg_loss = 3.00768\n",
      "epoch no.5 train no.393150  loss = 1.89188 avg_loss = 3.00974\n",
      "epoch no.5 train no.393160  loss = 3.41875 avg_loss = 2.99056\n",
      "epoch no.5 train no.393170  loss = 2.55390 avg_loss = 2.95011\n",
      "epoch no.5 train no.393180  loss = 4.48975 avg_loss = 3.01109\n",
      "epoch no.5 train no.393190  loss = 2.42121 avg_loss = 2.99067\n",
      "epoch no.5 train no.393200  loss = 4.60013 avg_loss = 2.99571\n",
      "epoch no.5 train no.393210  loss = 2.43898 avg_loss = 2.97422\n",
      "epoch no.5 train no.393220  loss = 1.95186 avg_loss = 2.96849\n",
      "epoch no.5 train no.393230  loss = 3.48597 avg_loss = 2.93122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.393240  loss = 2.34922 avg_loss = 2.91296\n",
      "epoch no.5 train no.393250  loss = 2.75829 avg_loss = 2.92642\n",
      "epoch no.5 train no.393260  loss = 2.30195 avg_loss = 2.95299\n",
      "epoch no.5 train no.393270  loss = 2.49785 avg_loss = 2.95451\n",
      "epoch no.5 train no.393280  loss = 2.31933 avg_loss = 2.94582\n",
      "epoch no.5 train no.393290  loss = 2.10218 avg_loss = 2.94617\n",
      "epoch no.5 train no.393300  loss = 2.12790 avg_loss = 2.98284\n",
      "epoch no.5 train no.393310  loss = 2.81483 avg_loss = 2.96691\n",
      "epoch no.5 train no.393320  loss = 3.61870 avg_loss = 3.00755\n",
      "epoch no.5 train no.393330  loss = 2.65916 avg_loss = 3.02966\n",
      "epoch no.5 train no.393340  loss = 2.56336 avg_loss = 3.03340\n",
      "epoch no.5 train no.393350  loss = 2.07170 avg_loss = 2.99451\n",
      "epoch no.5 train no.393360  loss = 2.91414 avg_loss = 2.94576\n",
      "epoch no.5 train no.393370  loss = 3.55454 avg_loss = 2.94427\n",
      "epoch no.5 train no.393380  loss = 2.57156 avg_loss = 2.94746\n",
      "epoch no.5 train no.393390  loss = 3.67022 avg_loss = 2.96562\n",
      "epoch no.5 train no.393400  loss = 2.65705 avg_loss = 2.94914\n",
      "epoch no.5 train no.393410  loss = 2.37035 avg_loss = 2.90294\n",
      "epoch no.5 train no.393420  loss = 2.29828 avg_loss = 2.94086\n",
      "epoch no.5 train no.393430  loss = 2.49839 avg_loss = 2.93714\n",
      "epoch no.5 train no.393440  loss = 5.15226 avg_loss = 2.93130\n",
      "epoch no.5 train no.393450  loss = 4.24266 avg_loss = 2.93635\n",
      "epoch no.5 train no.393460  loss = 3.49858 avg_loss = 2.97896\n",
      "epoch no.5 train no.393470  loss = 3.23565 avg_loss = 2.98942\n",
      "epoch no.5 train no.393480  loss = 4.13600 avg_loss = 3.00589\n",
      "epoch no.5 train no.393490  loss = 2.76905 avg_loss = 3.03877\n",
      "epoch no.5 train no.393500  loss = 5.94949 avg_loss = 3.02628\n",
      "epoch no.5 train no.393510  loss = 4.18762 avg_loss = 3.04846\n",
      "epoch no.5 train no.393520  loss = 4.06685 avg_loss = 3.01799\n",
      "epoch no.5 train no.393530  loss = 4.75596 avg_loss = 3.02249\n",
      "epoch no.5 train no.393540  loss = 3.59504 avg_loss = 2.97774\n",
      "epoch no.5 train no.393550  loss = 2.22185 avg_loss = 2.96110\n",
      "epoch no.5 train no.393560  loss = 2.20482 avg_loss = 2.95446\n",
      "epoch no.5 train no.393570  loss = 3.01071 avg_loss = 2.96445\n",
      "epoch no.5 train no.393580  loss = 2.95146 avg_loss = 2.98772\n",
      "epoch no.5 train no.393590  loss = 4.11094 avg_loss = 3.00807\n",
      "epoch no.5 train no.393600  loss = 2.12341 avg_loss = 2.97863\n",
      "epoch no.5 train no.393610  loss = 4.64330 avg_loss = 3.01388\n",
      "epoch no.5 train no.393620  loss = 2.62411 avg_loss = 3.05062\n",
      "epoch no.5 train no.393630  loss = 3.87330 avg_loss = 3.01408\n",
      "epoch no.5 train no.393640  loss = 2.21987 avg_loss = 3.02606\n",
      "epoch no.5 train no.393650  loss = 3.85695 avg_loss = 3.00347\n",
      "epoch no.5 train no.393660  loss = 2.12038 avg_loss = 3.00054\n",
      "epoch no.5 train no.393670  loss = 3.28590 avg_loss = 2.98194\n",
      "epoch no.5 train no.393680  loss = 2.94566 avg_loss = 3.00697\n",
      "epoch no.5 train no.393690  loss = 3.08432 avg_loss = 3.00180\n",
      "epoch no.5 train no.393700  loss = 2.42076 avg_loss = 3.02736\n",
      "epoch no.5 train no.393710  loss = 2.52389 avg_loss = 3.00479\n",
      "epoch no.5 train no.393720  loss = 2.36553 avg_loss = 3.01499\n",
      "epoch no.5 train no.393730  loss = 2.03506 avg_loss = 3.00710\n",
      "epoch no.5 train no.393740  loss = 3.79552 avg_loss = 2.97593\n",
      "epoch no.5 train no.393750  loss = 2.61573 avg_loss = 2.94830\n",
      "epoch no.5 train no.393760  loss = 3.85376 avg_loss = 2.99208\n",
      "epoch no.5 train no.393770  loss = 4.10128 avg_loss = 2.97980\n",
      "epoch no.5 train no.393780  loss = 3.14351 avg_loss = 2.97453\n",
      "epoch no.5 train no.393790  loss = 2.46181 avg_loss = 2.98190\n",
      "epoch no.5 train no.393800  loss = 2.44084 avg_loss = 2.92695\n",
      "epoch no.5 train no.393810  loss = 2.91824 avg_loss = 2.95425\n",
      "epoch no.5 train no.393820  loss = 2.85863 avg_loss = 2.93776\n",
      "epoch no.5 train no.393830  loss = 2.42874 avg_loss = 2.95130\n",
      "epoch no.5 train no.393840  loss = 2.58692 avg_loss = 2.94113\n",
      "epoch no.5 train no.393850  loss = 2.43367 avg_loss = 2.93230\n",
      "epoch no.5 train no.393860  loss = 3.06341 avg_loss = 2.92458\n",
      "epoch no.5 train no.393870  loss = 2.31308 avg_loss = 2.90827\n",
      "epoch no.5 train no.393880  loss = 2.51221 avg_loss = 2.91895\n",
      "epoch no.5 train no.393890  loss = 2.82888 avg_loss = 2.92235\n",
      "epoch no.5 train no.393900  loss = 5.60927 avg_loss = 2.95813\n",
      "epoch no.5 train no.393910  loss = 2.67239 avg_loss = 3.02165\n",
      "epoch no.5 train no.393920  loss = 4.23589 avg_loss = 3.01538\n",
      "epoch no.5 train no.393930  loss = 3.19873 avg_loss = 3.03539\n",
      "epoch no.5 train no.393940  loss = 2.93873 avg_loss = 3.07112\n",
      "epoch no.5 train no.393950  loss = 2.61839 avg_loss = 3.01280\n",
      "epoch no.5 train no.393960  loss = 4.59712 avg_loss = 3.08345\n",
      "epoch no.5 train no.393970  loss = 3.22160 avg_loss = 3.07219\n",
      "epoch no.5 train no.393980  loss = 3.82171 avg_loss = 3.02682\n",
      "epoch no.5 train no.393990  loss = 2.72503 avg_loss = 2.99284\n",
      "epoch no.5 train no.394000  loss = 2.50988 avg_loss = 2.99216\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '한', '여름', '▁여름', '</s>']\n",
      "여름여름 여름여름 여름</s>\n",
      "epoch no.5 train no.394010  loss = 2.72891 avg_loss = 3.00963\n",
      "epoch no.5 train no.394020  loss = 2.27492 avg_loss = 3.02527\n",
      "epoch no.5 train no.394030  loss = 3.69088 avg_loss = 3.03488\n",
      "epoch no.5 train no.394040  loss = 2.58199 avg_loss = 3.00296\n",
      "epoch no.5 train no.394050  loss = 3.49809 avg_loss = 3.04569\n",
      "epoch no.5 train no.394060  loss = 3.62706 avg_loss = 3.02287\n",
      "epoch no.5 train no.394070  loss = 3.10030 avg_loss = 3.00802\n",
      "epoch no.5 train no.394080  loss = 2.69745 avg_loss = 2.98547\n",
      "epoch no.5 train no.394090  loss = 3.95893 avg_loss = 3.00610\n",
      "epoch no.5 train no.394100  loss = 1.53827 avg_loss = 3.01921\n",
      "epoch no.5 train no.394110  loss = 2.77604 avg_loss = 2.99389\n",
      "epoch no.5 train no.394120  loss = 2.87259 avg_loss = 3.03096\n",
      "epoch no.5 train no.394130  loss = 3.92977 avg_loss = 3.05921\n",
      "epoch no.5 train no.394140  loss = 3.66932 avg_loss = 3.05568\n",
      "epoch no.5 train no.394150  loss = 2.21244 avg_loss = 3.06030\n",
      "epoch no.5 train no.394160  loss = 2.99415 avg_loss = 3.08224\n",
      "epoch no.5 train no.394170  loss = 3.50167 avg_loss = 3.06317\n",
      "epoch no.5 train no.394180  loss = 5.19978 avg_loss = 3.07506\n",
      "epoch no.5 train no.394190  loss = 2.34769 avg_loss = 3.08021\n",
      "epoch no.5 train no.394200  loss = 3.38822 avg_loss = 3.10723\n",
      "epoch no.5 train no.394210  loss = 2.62246 avg_loss = 3.07182\n",
      "epoch no.5 train no.394220  loss = 1.30896 avg_loss = 3.02042\n",
      "epoch no.5 train no.394230  loss = 2.74246 avg_loss = 3.02056\n",
      "epoch no.5 train no.394240  loss = 1.93575 avg_loss = 3.01316\n",
      "epoch no.5 train no.394250  loss = 2.85176 avg_loss = 3.01043\n",
      "epoch no.5 train no.394260  loss = 1.67152 avg_loss = 2.99445\n",
      "epoch no.5 train no.394270  loss = 2.25626 avg_loss = 3.01418\n",
      "epoch no.5 train no.394280  loss = 4.32900 avg_loss = 3.02572\n",
      "epoch no.5 train no.394290  loss = 2.40544 avg_loss = 2.97002\n",
      "epoch no.5 train no.394300  loss = 2.15106 avg_loss = 2.94640\n",
      "epoch no.5 train no.394310  loss = 2.19122 avg_loss = 2.97290\n",
      "epoch no.5 train no.394320  loss = 3.06298 avg_loss = 2.97865\n",
      "epoch no.5 train no.394330  loss = 3.01232 avg_loss = 2.97870\n",
      "epoch no.5 train no.394340  loss = 3.31713 avg_loss = 2.97876\n",
      "epoch no.5 train no.394350  loss = 2.18228 avg_loss = 2.98965\n",
      "epoch no.5 train no.394360  loss = 5.04160 avg_loss = 3.00978\n",
      "epoch no.5 train no.394370  loss = 4.08334 avg_loss = 3.00293\n",
      "epoch no.5 train no.394380  loss = 3.02513 avg_loss = 3.00402\n",
      "epoch no.5 train no.394390  loss = 2.53014 avg_loss = 3.01474\n",
      "epoch no.5 train no.394400  loss = 2.11726 avg_loss = 3.01284\n",
      "epoch no.5 train no.394410  loss = 2.25147 avg_loss = 3.04334\n",
      "epoch no.5 train no.394420  loss = 2.04050 avg_loss = 3.05886\n",
      "epoch no.5 train no.394430  loss = 2.39322 avg_loss = 3.03617\n",
      "epoch no.5 train no.394440  loss = 3.02661 avg_loss = 2.99018\n",
      "epoch no.5 train no.394450  loss = 3.47660 avg_loss = 2.99820\n",
      "epoch no.5 train no.394460  loss = 3.66949 avg_loss = 3.02195\n",
      "epoch no.5 train no.394470  loss = 2.10795 avg_loss = 2.99336\n",
      "epoch no.5 train no.394480  loss = 2.63264 avg_loss = 2.96522\n",
      "epoch no.5 train no.394490  loss = 3.51353 avg_loss = 2.96164\n",
      "epoch no.5 train no.394500  loss = 2.65315 avg_loss = 2.98618\n",
      "epoch no.5 train no.394510  loss = 1.97376 avg_loss = 3.00935\n",
      "epoch no.5 train no.394520  loss = 3.33297 avg_loss = 3.03754\n",
      "epoch no.5 train no.394530  loss = 3.99947 avg_loss = 3.02501\n",
      "epoch no.5 train no.394540  loss = 3.61608 avg_loss = 2.97512\n",
      "epoch no.5 train no.394550  loss = 2.86952 avg_loss = 2.99336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.394560  loss = 3.06990 avg_loss = 2.98376\n",
      "epoch no.5 train no.394570  loss = 2.97519 avg_loss = 3.00157\n",
      "epoch no.5 train no.394580  loss = 2.46079 avg_loss = 3.03225\n",
      "epoch no.5 train no.394590  loss = 3.10172 avg_loss = 3.03716\n",
      "epoch no.5 train no.394600  loss = 3.20671 avg_loss = 3.02273\n",
      "epoch no.5 train no.394610  loss = 3.02663 avg_loss = 3.05094\n",
      "epoch no.5 train no.394620  loss = 3.99732 avg_loss = 3.03133\n",
      "epoch no.5 train no.394630  loss = 3.89690 avg_loss = 3.04428\n",
      "epoch no.5 train no.394640  loss = 2.07476 avg_loss = 3.01979\n",
      "epoch no.5 train no.394650  loss = 2.96057 avg_loss = 2.97815\n",
      "epoch no.5 train no.394660  loss = 3.09234 avg_loss = 3.01023\n",
      "epoch no.5 train no.394670  loss = 1.85190 avg_loss = 2.97180\n",
      "epoch no.5 train no.394680  loss = 2.13468 avg_loss = 2.95675\n",
      "epoch no.5 train no.394690  loss = 4.09228 avg_loss = 2.97919\n",
      "epoch no.5 train no.394700  loss = 3.21502 avg_loss = 2.94812\n",
      "epoch no.5 train no.394710  loss = 2.18646 avg_loss = 2.97108\n",
      "epoch no.5 train no.394720  loss = 3.02928 avg_loss = 2.97661\n",
      "epoch no.5 train no.394730  loss = 3.01092 avg_loss = 3.00152\n",
      "epoch no.5 train no.394740  loss = 2.33085 avg_loss = 3.01617\n",
      "epoch no.5 train no.394750  loss = 1.26574 avg_loss = 3.00248\n",
      "epoch no.5 train no.394760  loss = 2.42233 avg_loss = 2.99206\n",
      "epoch no.5 train no.394770  loss = 2.56449 avg_loss = 2.97744\n",
      "epoch no.5 train no.394780  loss = 2.31232 avg_loss = 3.01655\n",
      "epoch no.5 train no.394790  loss = 2.35814 avg_loss = 3.02117\n",
      "epoch no.5 train no.394800  loss = 2.60931 avg_loss = 3.01240\n",
      "epoch no.5 train no.394810  loss = 3.23470 avg_loss = 3.02055\n",
      "epoch no.5 train no.394820  loss = 3.24755 avg_loss = 3.04144\n",
      "epoch no.5 train no.394830  loss = 3.48990 avg_loss = 3.04812\n",
      "epoch no.5 train no.394840  loss = 2.22372 avg_loss = 3.04771\n",
      "epoch no.5 train no.394850  loss = 4.63738 avg_loss = 3.09786\n",
      "epoch no.5 train no.394860  loss = 3.35221 avg_loss = 3.10031\n",
      "epoch no.5 train no.394870  loss = 3.14733 avg_loss = 3.10502\n",
      "epoch no.5 train no.394880  loss = 3.70834 avg_loss = 3.09227\n",
      "epoch no.5 train no.394890  loss = 3.05274 avg_loss = 3.09292\n",
      "epoch no.5 train no.394900  loss = 3.69888 avg_loss = 3.09781\n",
      "epoch no.5 train no.394910  loss = 2.16806 avg_loss = 3.06746\n",
      "epoch no.5 train no.394920  loss = 3.32540 avg_loss = 3.03455\n",
      "epoch no.5 train no.394930  loss = 2.62882 avg_loss = 3.03526\n",
      "epoch no.5 train no.394940  loss = 1.98763 avg_loss = 3.03129\n",
      "epoch no.5 train no.394950  loss = 2.68930 avg_loss = 3.06447\n",
      "epoch no.5 train no.394960  loss = 1.53753 avg_loss = 3.03363\n",
      "epoch no.5 train no.394970  loss = 3.01551 avg_loss = 3.07212\n",
      "epoch no.5 train no.394980  loss = 3.19926 avg_loss = 3.11587\n",
      "epoch no.5 train no.394990  loss = 2.28017 avg_loss = 3.13175\n",
      "epoch no.5 train no.395000  loss = 4.49935 avg_loss = 3.11407\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁잔잔', '▁뉴', '▁인디', '송', '</s>', '</s>']\n",
      "여름밤의 감성적인 팝송들</s>\n",
      "epoch no.5 train no.395010  loss = 6.14921 avg_loss = 3.15097\n",
      "epoch no.5 train no.395020  loss = 3.17119 avg_loss = 3.13175\n",
      "epoch no.5 train no.395030  loss = 3.04600 avg_loss = 3.12198\n",
      "epoch no.5 train no.395040  loss = 2.22206 avg_loss = 3.09601\n",
      "epoch no.5 train no.395050  loss = 4.37573 avg_loss = 3.12762\n",
      "epoch no.5 train no.395060  loss = 3.29404 avg_loss = 3.12373\n",
      "epoch no.5 train no.395070  loss = 3.22600 avg_loss = 3.10973\n",
      "epoch no.5 train no.395080  loss = 3.31799 avg_loss = 3.09806\n",
      "epoch no.5 train no.395090  loss = 2.49796 avg_loss = 3.07590\n",
      "epoch no.5 train no.395100  loss = 4.68509 avg_loss = 3.06575\n",
      "epoch no.5 train no.395110  loss = 3.50696 avg_loss = 3.09012\n",
      "epoch no.5 train no.395120  loss = 2.52800 avg_loss = 3.07247\n",
      "epoch no.5 train no.395130  loss = 3.10922 avg_loss = 3.09445\n",
      "epoch no.5 train no.395140  loss = 3.00620 avg_loss = 3.11353\n",
      "epoch no.5 train no.395150  loss = 2.64332 avg_loss = 3.08185\n",
      "epoch no.5 train no.395160  loss = 5.08851 avg_loss = 3.10991\n",
      "epoch no.5 train no.395170  loss = 4.13437 avg_loss = 3.10110\n",
      "epoch no.5 train no.395180  loss = 2.06275 avg_loss = 3.10644\n",
      "epoch no.5 train no.395190  loss = 2.80338 avg_loss = 3.10468\n",
      "epoch no.5 train no.395200  loss = 3.63915 avg_loss = 3.13394\n",
      "epoch no.5 train no.395210  loss = 2.96106 avg_loss = 3.11038\n",
      "epoch no.5 train no.395220  loss = 3.07549 avg_loss = 3.07620\n",
      "epoch no.5 train no.395230  loss = 3.60366 avg_loss = 3.04988\n",
      "epoch no.5 train no.395240  loss = 4.08702 avg_loss = 3.08345\n",
      "epoch no.5 train no.395250  loss = 2.43700 avg_loss = 3.09666\n",
      "epoch no.5 train no.395260  loss = 3.28378 avg_loss = 3.13855\n",
      "epoch no.5 train no.395270  loss = 3.30794 avg_loss = 3.10077\n",
      "epoch no.5 train no.395280  loss = 2.81682 avg_loss = 3.10022\n",
      "epoch no.5 train no.395290  loss = 2.69771 avg_loss = 3.04557\n",
      "epoch no.5 train no.395300  loss = 3.79704 avg_loss = 3.06996\n",
      "epoch no.5 train no.395310  loss = 3.81832 avg_loss = 3.06324\n",
      "epoch no.5 train no.395320  loss = 2.84497 avg_loss = 3.02688\n",
      "epoch no.5 train no.395330  loss = 2.45226 avg_loss = 3.00641\n",
      "epoch no.5 train no.395340  loss = 2.56208 avg_loss = 2.97666\n",
      "epoch no.5 train no.395350  loss = 3.91319 avg_loss = 3.00359\n",
      "epoch no.5 train no.395360  loss = 2.75182 avg_loss = 2.97396\n",
      "epoch no.5 train no.395370  loss = 3.48818 avg_loss = 2.96470\n",
      "epoch no.5 train no.395380  loss = 2.80370 avg_loss = 2.94989\n",
      "epoch no.5 train no.395390  loss = 4.32002 avg_loss = 2.98177\n",
      "epoch no.5 train no.395400  loss = 2.91517 avg_loss = 3.00679\n",
      "epoch no.5 train no.395410  loss = 4.27074 avg_loss = 3.03009\n",
      "epoch no.5 train no.395420  loss = 2.83618 avg_loss = 3.02132\n",
      "epoch no.5 train no.395430  loss = 3.91502 avg_loss = 3.00576\n",
      "epoch no.5 train no.395440  loss = 4.58909 avg_loss = 3.02766\n",
      "epoch no.5 train no.395450  loss = 2.73998 avg_loss = 3.05753\n",
      "epoch no.5 train no.395460  loss = 2.30825 avg_loss = 3.07146\n",
      "epoch no.5 train no.395470  loss = 1.59757 avg_loss = 3.03259\n",
      "epoch no.5 train no.395480  loss = 2.40465 avg_loss = 3.00430\n",
      "epoch no.5 train no.395490  loss = 3.21667 avg_loss = 3.01101\n",
      "epoch no.5 train no.395500  loss = 3.29152 avg_loss = 3.04955\n",
      "epoch no.5 train no.395510  loss = 2.09217 avg_loss = 3.03338\n",
      "epoch no.5 train no.395520  loss = 3.48035 avg_loss = 3.01292\n",
      "epoch no.5 train no.395530  loss = 2.30838 avg_loss = 3.00816\n",
      "epoch no.5 train no.395540  loss = 2.40819 avg_loss = 3.05463\n",
      "epoch no.5 train no.395550  loss = 4.52163 avg_loss = 3.07702\n",
      "epoch no.5 train no.395560  loss = 3.08402 avg_loss = 3.06948\n",
      "epoch no.5 train no.395570  loss = 3.88357 avg_loss = 3.05496\n",
      "epoch no.5 train no.395580  loss = 3.57315 avg_loss = 3.08133\n",
      "epoch no.5 train no.395590  loss = 2.19920 avg_loss = 3.10024\n",
      "epoch no.5 train no.395600  loss = 3.45658 avg_loss = 3.07828\n",
      "epoch no.5 train no.395610  loss = 2.58747 avg_loss = 3.04608\n",
      "epoch no.5 train no.395620  loss = 4.39933 avg_loss = 3.07504\n",
      "epoch no.5 train no.395630  loss = 2.67915 avg_loss = 3.06277\n",
      "epoch no.5 train no.395640  loss = 2.69588 avg_loss = 3.03683\n",
      "epoch no.5 train no.395650  loss = 3.63268 avg_loss = 3.02755\n",
      "epoch no.5 train no.395660  loss = 2.10265 avg_loss = 2.99092\n",
      "epoch no.5 train no.395670  loss = 1.78252 avg_loss = 2.96170\n",
      "epoch no.5 train no.395680  loss = 1.98084 avg_loss = 2.95951\n",
      "epoch no.5 train no.395690  loss = 2.44185 avg_loss = 2.97797\n",
      "epoch no.5 train no.395700  loss = 2.94207 avg_loss = 2.95196\n",
      "epoch no.5 train no.395710  loss = 2.16494 avg_loss = 2.95612\n",
      "epoch no.5 train no.395720  loss = 3.16253 avg_loss = 2.96269\n",
      "epoch no.5 train no.395730  loss = 3.44741 avg_loss = 2.95490\n",
      "epoch no.5 train no.395740  loss = 3.98707 avg_loss = 2.96868\n",
      "epoch no.5 train no.395750  loss = 2.42019 avg_loss = 2.94285\n",
      "epoch no.5 train no.395760  loss = 2.54073 avg_loss = 2.98073\n",
      "epoch no.5 train no.395770  loss = 3.11528 avg_loss = 2.96390\n",
      "epoch no.5 train no.395780  loss = 1.91181 avg_loss = 2.95584\n",
      "epoch no.5 train no.395790  loss = 4.30828 avg_loss = 2.93907\n",
      "epoch no.5 train no.395800  loss = 3.91491 avg_loss = 2.93796\n",
      "epoch no.5 train no.395810  loss = 2.29078 avg_loss = 2.94150\n",
      "epoch no.5 train no.395820  loss = 4.48782 avg_loss = 2.91680\n",
      "epoch no.5 train no.395830  loss = 2.37075 avg_loss = 2.95787\n",
      "epoch no.5 train no.395840  loss = 3.28941 avg_loss = 2.98470\n",
      "epoch no.5 train no.395850  loss = 3.88207 avg_loss = 2.98892\n",
      "epoch no.5 train no.395860  loss = 2.85418 avg_loss = 3.00842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.395870  loss = 2.51483 avg_loss = 3.01970\n",
      "epoch no.5 train no.395880  loss = 2.91199 avg_loss = 3.02813\n",
      "epoch no.5 train no.395890  loss = 2.66665 avg_loss = 3.03053\n",
      "epoch no.5 train no.395900  loss = 2.65869 avg_loss = 3.03930\n",
      "epoch no.5 train no.395910  loss = 3.47887 avg_loss = 3.03174\n",
      "epoch no.5 train no.395920  loss = 2.79127 avg_loss = 2.99985\n",
      "epoch no.5 train no.395930  loss = 2.59846 avg_loss = 2.97801\n",
      "epoch no.5 train no.395940  loss = 3.71756 avg_loss = 3.01612\n",
      "epoch no.5 train no.395950  loss = 5.10643 avg_loss = 3.06611\n",
      "epoch no.5 train no.395960  loss = 2.33474 avg_loss = 3.05267\n",
      "epoch no.5 train no.395970  loss = 3.06699 avg_loss = 3.02229\n",
      "epoch no.5 train no.395980  loss = 3.09533 avg_loss = 3.02238\n",
      "epoch no.5 train no.395990  loss = 4.17332 avg_loss = 3.04636\n",
      "epoch no.5 train no.396000  loss = 3.82214 avg_loss = 3.07291\n",
      "6\n",
      "to_tokens: ['▁비', '여름', '▁역시', '▁신나는', '▁트로', '곡', '</s>']\n",
      "여름엔 역시 신나는  댄스곡</s>\n",
      "epoch no.5 train no.396010  loss = 3.90318 avg_loss = 3.07810\n",
      "epoch no.5 train no.396020  loss = 3.34715 avg_loss = 3.08797\n",
      "epoch no.5 train no.396030  loss = 4.18394 avg_loss = 3.08684\n",
      "epoch no.5 train no.396040  loss = 2.68675 avg_loss = 3.04564\n",
      "epoch no.5 train no.396050  loss = 2.68845 avg_loss = 3.05947\n",
      "epoch no.5 train no.396060  loss = 2.78580 avg_loss = 3.03712\n",
      "epoch no.5 train no.396070  loss = 3.16162 avg_loss = 3.05344\n",
      "epoch no.5 train no.396080  loss = 2.80349 avg_loss = 3.07884\n",
      "epoch no.5 train no.396090  loss = 1.93931 avg_loss = 3.03728\n",
      "epoch no.5 train no.396100  loss = 3.26331 avg_loss = 3.08041\n",
      "epoch no.5 train no.396110  loss = 3.71824 avg_loss = 3.05664\n",
      "epoch no.5 train no.396120  loss = 2.92912 avg_loss = 3.02478\n",
      "epoch no.5 train no.396130  loss = 3.14950 avg_loss = 3.05072\n",
      "epoch no.5 train no.396140  loss = 2.16480 avg_loss = 3.05882\n",
      "epoch no.5 train no.396150  loss = 2.59938 avg_loss = 3.05041\n",
      "epoch no.5 train no.396160  loss = 2.22084 avg_loss = 3.02164\n",
      "epoch no.5 train no.396170  loss = 3.45100 avg_loss = 3.01726\n",
      "epoch no.5 train no.396180  loss = 2.37603 avg_loss = 3.01204\n",
      "epoch no.5 train no.396190  loss = 3.10823 avg_loss = 3.06013\n",
      "epoch no.5 train no.396200  loss = 3.37471 avg_loss = 3.05159\n",
      "epoch no.5 train no.396210  loss = 1.93845 avg_loss = 3.00001\n",
      "epoch no.5 train no.396220  loss = 2.08602 avg_loss = 2.99768\n",
      "epoch no.5 train no.396230  loss = 3.55617 avg_loss = 2.99468\n",
      "epoch no.5 train no.396240  loss = 2.00666 avg_loss = 2.98718\n",
      "epoch no.5 train no.396250  loss = 4.37750 avg_loss = 3.02507\n",
      "epoch no.5 train no.396260  loss = 3.37540 avg_loss = 2.99920\n",
      "epoch no.5 train no.396270  loss = 2.22143 avg_loss = 2.97929\n",
      "epoch no.5 train no.396280  loss = 1.84120 avg_loss = 2.98021\n",
      "epoch no.5 train no.396290  loss = 3.55921 avg_loss = 3.04864\n",
      "epoch no.5 train no.396300  loss = 2.69939 avg_loss = 3.04703\n",
      "epoch no.5 train no.396310  loss = 2.16919 avg_loss = 3.04118\n",
      "epoch no.5 train no.396320  loss = 1.87969 avg_loss = 3.00332\n",
      "epoch no.5 train no.396330  loss = 2.27432 avg_loss = 2.99621\n",
      "epoch no.5 train no.396340  loss = 2.64996 avg_loss = 2.96012\n",
      "epoch no.5 train no.396350  loss = 2.57368 avg_loss = 2.96536\n",
      "epoch no.5 train no.396360  loss = 3.44050 avg_loss = 2.98083\n",
      "epoch no.5 train no.396370  loss = 3.54744 avg_loss = 2.97008\n",
      "epoch no.5 train no.396380  loss = 3.17188 avg_loss = 2.95495\n",
      "epoch no.5 train no.396390  loss = 2.24476 avg_loss = 2.93451\n",
      "epoch no.5 train no.396400  loss = 3.32873 avg_loss = 2.92341\n",
      "epoch no.5 train no.396410  loss = 4.80621 avg_loss = 2.97974\n",
      "epoch no.5 train no.396420  loss = 2.25210 avg_loss = 2.97225\n",
      "epoch no.5 train no.396430  loss = 3.89215 avg_loss = 2.97349\n",
      "epoch no.5 train no.396440  loss = 2.67250 avg_loss = 2.98217\n",
      "epoch no.5 train no.396450  loss = 3.39009 avg_loss = 2.98223\n",
      "epoch no.5 train no.396460  loss = 2.69273 avg_loss = 2.98118\n",
      "epoch no.5 train no.396470  loss = 3.46561 avg_loss = 2.97922\n",
      "epoch no.5 train no.396480  loss = 4.11858 avg_loss = 2.95529\n",
      "epoch no.5 train no.396490  loss = 4.41145 avg_loss = 2.96683\n",
      "epoch no.5 train no.396500  loss = 1.74749 avg_loss = 2.98068\n",
      "epoch no.5 train no.396510  loss = 2.41777 avg_loss = 2.96422\n",
      "epoch no.5 train no.396520  loss = 3.22010 avg_loss = 2.99915\n",
      "epoch no.5 train no.396530  loss = 1.99907 avg_loss = 2.98727\n",
      "epoch no.5 train no.396540  loss = 5.11404 avg_loss = 3.02321\n",
      "epoch no.5 train no.396550  loss = 1.67986 avg_loss = 3.01907\n",
      "epoch no.5 train no.396560  loss = 2.84698 avg_loss = 3.05015\n",
      "epoch no.5 train no.396570  loss = 3.94548 avg_loss = 3.06041\n",
      "epoch no.5 train no.396580  loss = 2.30893 avg_loss = 3.05659\n",
      "epoch no.5 train no.396590  loss = 3.49277 avg_loss = 3.08128\n",
      "epoch no.5 train no.396600  loss = 4.03024 avg_loss = 3.08704\n",
      "epoch no.5 train no.396610  loss = 3.76840 avg_loss = 3.13595\n",
      "epoch no.5 train no.396620  loss = 2.20026 avg_loss = 3.12769\n",
      "epoch no.5 train no.396630  loss = 3.21544 avg_loss = 3.12246\n",
      "epoch no.5 train no.396640  loss = 3.28337 avg_loss = 3.10762\n",
      "epoch no.5 train no.396650  loss = 3.55532 avg_loss = 3.10419\n",
      "epoch no.5 train no.396660  loss = 3.63289 avg_loss = 3.07632\n",
      "epoch no.5 train no.396670  loss = 4.17128 avg_loss = 3.09937\n",
      "epoch no.5 train no.396680  loss = 2.52031 avg_loss = 3.06152\n",
      "epoch no.5 train no.396690  loss = 2.81986 avg_loss = 3.05880\n",
      "epoch no.5 train no.396700  loss = 3.39755 avg_loss = 3.05758\n",
      "epoch no.5 train no.396710  loss = 2.13742 avg_loss = 3.00137\n",
      "epoch no.5 train no.396720  loss = 2.98987 avg_loss = 3.03563\n",
      "epoch no.5 train no.396730  loss = 3.14121 avg_loss = 3.03060\n",
      "epoch no.5 train no.396740  loss = 3.20478 avg_loss = 3.02910\n",
      "epoch no.5 train no.396750  loss = 2.96771 avg_loss = 2.99984\n",
      "epoch no.5 train no.396760  loss = 2.48319 avg_loss = 3.02342\n",
      "epoch no.5 train no.396770  loss = 2.19829 avg_loss = 3.02333\n",
      "epoch no.5 train no.396780  loss = 1.42454 avg_loss = 3.00544\n",
      "epoch no.5 train no.396790  loss = 2.88896 avg_loss = 3.05517\n",
      "epoch no.5 train no.396800  loss = 2.55452 avg_loss = 3.04881\n",
      "epoch no.5 train no.396810  loss = 3.78777 avg_loss = 3.01931\n",
      "epoch no.5 train no.396820  loss = 1.53790 avg_loss = 3.00955\n",
      "epoch no.5 train no.396830  loss = 2.19135 avg_loss = 2.98783\n",
      "epoch no.5 train no.396840  loss = 3.99327 avg_loss = 3.01515\n",
      "epoch no.5 train no.396850  loss = 3.05468 avg_loss = 3.01291\n",
      "epoch no.5 train no.396860  loss = 2.30799 avg_loss = 3.02267\n",
      "epoch no.5 train no.396870  loss = 1.69321 avg_loss = 3.00989\n",
      "epoch no.5 train no.396880  loss = 3.19315 avg_loss = 2.99142\n",
      "epoch no.5 train no.396890  loss = 2.16902 avg_loss = 2.96431\n",
      "epoch no.5 train no.396900  loss = 6.24337 avg_loss = 3.00871\n",
      "epoch no.5 train no.396910  loss = 3.52978 avg_loss = 2.99133\n",
      "epoch no.5 train no.396920  loss = 2.82128 avg_loss = 2.98904\n",
      "epoch no.5 train no.396930  loss = 4.82562 avg_loss = 3.00580\n",
      "epoch no.5 train no.396940  loss = 4.38209 avg_loss = 3.04531\n",
      "epoch no.5 train no.396950  loss = 2.97931 avg_loss = 3.08084\n",
      "epoch no.5 train no.396960  loss = 1.94582 avg_loss = 3.06442\n",
      "epoch no.5 train no.396970  loss = 5.36595 avg_loss = 3.08094\n",
      "epoch no.5 train no.396980  loss = 4.32937 avg_loss = 3.07874\n",
      "epoch no.5 train no.396990  loss = 4.71988 avg_loss = 3.11515\n",
      "epoch no.5 train no.397000  loss = 2.06384 avg_loss = 3.10924\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '▁어울리는', '▁청량', '▁노래', '</s>']\n",
      "여름과 어울리는 신나는 음악</s>\n",
      "epoch no.5 train no.397010  loss = 3.05794 avg_loss = 3.10620\n",
      "epoch no.5 train no.397020  loss = 3.82733 avg_loss = 3.09325\n",
      "epoch no.5 train no.397030  loss = 4.45187 avg_loss = 3.08683\n",
      "epoch no.5 train no.397040  loss = 3.06092 avg_loss = 3.10798\n",
      "epoch no.5 train no.397050  loss = 4.11368 avg_loss = 3.13346\n",
      "epoch no.5 train no.397060  loss = 3.00527 avg_loss = 3.12419\n",
      "epoch no.5 train no.397070  loss = 2.27702 avg_loss = 3.16372\n",
      "epoch no.5 train no.397080  loss = 2.96008 avg_loss = 3.19352\n",
      "epoch no.5 train no.397090  loss = 1.85033 avg_loss = 3.19102\n",
      "epoch no.5 train no.397100  loss = 3.16779 avg_loss = 3.17722\n",
      "epoch no.5 train no.397110  loss = 2.17619 avg_loss = 3.14936\n",
      "epoch no.5 train no.397120  loss = 3.98963 avg_loss = 3.13115\n",
      "epoch no.5 train no.397130  loss = 2.40618 avg_loss = 3.11103\n",
      "epoch no.5 train no.397140  loss = 2.54989 avg_loss = 3.09945\n",
      "epoch no.5 train no.397150  loss = 2.49350 avg_loss = 3.06162\n",
      "epoch no.5 train no.397160  loss = 2.61585 avg_loss = 3.07378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.397170  loss = 2.20245 avg_loss = 3.04355\n",
      "epoch no.5 train no.397180  loss = 2.69859 avg_loss = 3.06984\n",
      "epoch no.5 train no.397190  loss = 1.75879 avg_loss = 3.04858\n",
      "epoch no.5 train no.397200  loss = 2.88597 avg_loss = 3.08248\n",
      "epoch no.5 train no.397210  loss = 2.47031 avg_loss = 3.09155\n",
      "epoch no.5 train no.397220  loss = 2.95323 avg_loss = 3.08947\n",
      "epoch no.5 train no.397230  loss = 4.47637 avg_loss = 3.08528\n",
      "epoch no.5 train no.397240  loss = 2.71772 avg_loss = 3.06953\n",
      "epoch no.5 train no.397250  loss = 2.42149 avg_loss = 3.03761\n",
      "epoch no.5 train no.397260  loss = 4.12918 avg_loss = 3.04985\n",
      "epoch no.5 train no.397270  loss = 2.54338 avg_loss = 3.04429\n",
      "epoch no.5 train no.397280  loss = 3.82669 avg_loss = 3.03862\n",
      "epoch no.5 train no.397290  loss = 2.97800 avg_loss = 3.03724\n",
      "epoch no.5 train no.397300  loss = 4.15826 avg_loss = 3.04725\n",
      "epoch no.5 train no.397310  loss = 4.35314 avg_loss = 3.09396\n",
      "epoch no.5 train no.397320  loss = 2.42084 avg_loss = 3.08657\n",
      "epoch no.5 train no.397330  loss = 3.00287 avg_loss = 3.10345\n",
      "epoch no.5 train no.397340  loss = 1.96577 avg_loss = 3.10588\n",
      "epoch no.5 train no.397350  loss = 3.43994 avg_loss = 3.07040\n",
      "epoch no.5 train no.397360  loss = 3.47036 avg_loss = 3.08179\n",
      "epoch no.5 train no.397370  loss = 2.87762 avg_loss = 3.09448\n",
      "epoch no.5 train no.397380  loss = 2.44504 avg_loss = 3.07686\n",
      "epoch no.5 train no.397390  loss = 3.85350 avg_loss = 3.10887\n",
      "epoch no.5 train no.397400  loss = 1.85309 avg_loss = 3.08481\n",
      "epoch no.5 train no.397410  loss = 3.71123 avg_loss = 3.06632\n",
      "epoch no.5 train no.397420  loss = 2.18322 avg_loss = 3.04147\n",
      "epoch no.5 train no.397430  loss = 2.85362 avg_loss = 3.03978\n",
      "epoch no.5 train no.397440  loss = 2.61425 avg_loss = 3.06213\n",
      "epoch no.5 train no.397450  loss = 3.49237 avg_loss = 3.08005\n",
      "epoch no.5 train no.397460  loss = 2.02118 avg_loss = 3.03709\n",
      "epoch no.5 train no.397470  loss = 4.22636 avg_loss = 3.05180\n",
      "epoch no.5 train no.397480  loss = 3.40982 avg_loss = 3.05814\n",
      "epoch no.5 train no.397490  loss = 2.01916 avg_loss = 3.04713\n",
      "epoch no.5 train no.397500  loss = 3.42663 avg_loss = 3.04098\n",
      "epoch no.5 train no.397510  loss = 2.71074 avg_loss = 3.01909\n",
      "epoch no.5 train no.397520  loss = 5.30683 avg_loss = 3.03926\n",
      "epoch no.5 train no.397530  loss = 2.98031 avg_loss = 3.03142\n",
      "epoch no.5 train no.397540  loss = 2.55884 avg_loss = 3.03522\n",
      "epoch no.5 train no.397550  loss = 4.13304 avg_loss = 3.07987\n",
      "epoch no.5 train no.397560  loss = 3.79383 avg_loss = 3.07635\n",
      "epoch no.5 train no.397570  loss = 3.87314 avg_loss = 3.07525\n",
      "epoch no.5 train no.397580  loss = 3.54736 avg_loss = 3.04409\n",
      "epoch no.5 train no.397590  loss = 2.74101 avg_loss = 3.03929\n",
      "epoch no.5 train no.397600  loss = 3.51278 avg_loss = 3.05369\n",
      "epoch no.5 train no.397610  loss = 3.20033 avg_loss = 3.08560\n",
      "epoch no.5 train no.397620  loss = 2.20084 avg_loss = 3.10201\n",
      "epoch no.5 train no.397630  loss = 3.19852 avg_loss = 3.08079\n",
      "epoch no.5 train no.397640  loss = 4.46199 avg_loss = 3.03849\n",
      "epoch no.5 train no.397650  loss = 4.57017 avg_loss = 3.02164\n",
      "epoch no.5 train no.397660  loss = 2.48068 avg_loss = 3.04721\n",
      "epoch no.5 train no.397670  loss = 2.76204 avg_loss = 3.05481\n",
      "epoch no.5 train no.397680  loss = 1.68025 avg_loss = 3.06908\n",
      "epoch no.5 train no.397690  loss = 3.70896 avg_loss = 3.07200\n",
      "epoch no.5 train no.397700  loss = 2.95506 avg_loss = 3.09464\n",
      "epoch no.5 train no.397710  loss = 3.67608 avg_loss = 3.10818\n",
      "epoch no.5 train no.397720  loss = 4.90759 avg_loss = 3.10829\n",
      "epoch no.5 train no.397730  loss = 2.99588 avg_loss = 3.10155\n",
      "epoch no.5 train no.397740  loss = 2.92552 avg_loss = 3.10916\n",
      "epoch no.5 train no.397750  loss = 2.37757 avg_loss = 3.08590\n",
      "epoch no.5 train no.397760  loss = 3.91744 avg_loss = 3.12340\n",
      "epoch no.5 train no.397770  loss = 3.13608 avg_loss = 3.12702\n",
      "epoch no.5 train no.397780  loss = 3.30381 avg_loss = 3.13224\n",
      "epoch no.5 train no.397790  loss = 3.23462 avg_loss = 3.15424\n",
      "epoch no.5 train no.397800  loss = 3.38269 avg_loss = 3.11750\n",
      "epoch no.5 train no.397810  loss = 3.93748 avg_loss = 3.16016\n",
      "epoch no.5 train no.397820  loss = 3.57421 avg_loss = 3.14454\n",
      "epoch no.5 train no.397830  loss = 3.52881 avg_loss = 3.14298\n",
      "epoch no.5 train no.397840  loss = 5.90378 avg_loss = 3.18988\n",
      "epoch no.5 train no.397850  loss = 3.46410 avg_loss = 3.17762\n",
      "epoch no.5 train no.397860  loss = 3.31889 avg_loss = 3.13317\n",
      "epoch no.5 train no.397870  loss = 3.09678 avg_loss = 3.12153\n",
      "epoch no.5 train no.397880  loss = 2.68170 avg_loss = 3.12081\n",
      "epoch no.5 train no.397890  loss = 3.03158 avg_loss = 3.11068\n",
      "epoch no.5 train no.397900  loss = 2.54269 avg_loss = 3.07703\n",
      "epoch no.5 train no.397910  loss = 3.81460 avg_loss = 3.05222\n",
      "epoch no.5 train no.397920  loss = 2.45373 avg_loss = 3.04285\n",
      "epoch no.5 train no.397930  loss = 3.16924 avg_loss = 3.03377\n",
      "epoch no.5 train no.397940  loss = 1.85440 avg_loss = 3.04029\n",
      "epoch no.5 train no.397950  loss = 3.78120 avg_loss = 3.06846\n",
      "epoch no.5 train no.397960  loss = 2.19927 avg_loss = 3.07180\n",
      "epoch no.5 train no.397970  loss = 3.28963 avg_loss = 3.03994\n",
      "epoch no.5 train no.397980  loss = 2.16688 avg_loss = 3.01232\n",
      "epoch no.5 train no.397990  loss = 4.09102 avg_loss = 3.06224\n",
      "epoch no.5 train no.398000  loss = 3.31148 avg_loss = 3.07266\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '에서', '▁듣기', '▁좋은', '</s>']\n",
      "여름밤 한강에서 듣기 딱</s>\n",
      "epoch no.5 train no.398010  loss = 3.01558 avg_loss = 3.03931\n",
      "epoch no.5 train no.398020  loss = 3.82063 avg_loss = 3.01252\n",
      "epoch no.5 train no.398030  loss = 2.52057 avg_loss = 3.00412\n",
      "epoch no.5 train no.398040  loss = 2.82280 avg_loss = 3.01003\n",
      "epoch no.5 train no.398050  loss = 2.45802 avg_loss = 2.99973\n",
      "epoch no.5 train no.398060  loss = 2.05802 avg_loss = 3.01460\n",
      "epoch no.5 train no.398070  loss = 2.33645 avg_loss = 3.02970\n",
      "epoch no.5 train no.398080  loss = 1.69973 avg_loss = 3.02951\n",
      "epoch no.5 train no.398090  loss = 3.63224 avg_loss = 3.05027\n",
      "epoch no.5 train no.398100  loss = 2.39944 avg_loss = 3.00998\n",
      "epoch no.5 train no.398110  loss = 4.64437 avg_loss = 3.05087\n",
      "epoch no.5 train no.398120  loss = 4.58328 avg_loss = 3.07763\n",
      "epoch no.5 train no.398130  loss = 3.27649 avg_loss = 3.11583\n",
      "epoch no.5 train no.398140  loss = 3.54750 avg_loss = 3.14031\n",
      "epoch no.5 train no.398150  loss = 1.87932 avg_loss = 3.08630\n",
      "epoch no.5 train no.398160  loss = 2.75740 avg_loss = 3.10784\n",
      "epoch no.5 train no.398170  loss = 2.10284 avg_loss = 3.13001\n",
      "epoch no.5 train no.398180  loss = 2.27153 avg_loss = 3.08719\n",
      "epoch no.5 train no.398190  loss = 2.36734 avg_loss = 3.07170\n",
      "epoch no.5 train no.398200  loss = 3.35184 avg_loss = 3.08151\n",
      "epoch no.5 train no.398210  loss = 3.20254 avg_loss = 3.08192\n",
      "epoch no.5 train no.398220  loss = 3.00651 avg_loss = 3.04653\n",
      "epoch no.5 train no.398230  loss = 3.00343 avg_loss = 3.08534\n",
      "epoch no.5 train no.398240  loss = 3.05003 avg_loss = 3.10735\n",
      "epoch no.5 train no.398250  loss = 2.44454 avg_loss = 3.12125\n",
      "epoch no.5 train no.398260  loss = 1.09548 avg_loss = 3.09404\n",
      "epoch no.5 train no.398270  loss = 4.03629 avg_loss = 3.12830\n",
      "epoch no.5 train no.398280  loss = 2.74624 avg_loss = 3.13010\n",
      "epoch no.5 train no.398290  loss = 3.65583 avg_loss = 3.12080\n",
      "epoch no.5 train no.398300  loss = 4.60302 avg_loss = 3.12297\n",
      "epoch no.5 train no.398310  loss = 3.82255 avg_loss = 3.13697\n",
      "epoch no.5 train no.398320  loss = 3.86845 avg_loss = 3.11347\n",
      "epoch no.5 train no.398330  loss = 4.54335 avg_loss = 3.11604\n",
      "epoch no.5 train no.398340  loss = 2.79362 avg_loss = 3.15440\n",
      "epoch no.5 train no.398350  loss = 2.32342 avg_loss = 3.15281\n",
      "epoch no.5 train no.398360  loss = 2.24395 avg_loss = 3.13292\n",
      "epoch no.5 train no.398370  loss = 3.15916 avg_loss = 3.14861\n",
      "epoch no.5 train no.398380  loss = 4.46679 avg_loss = 3.14788\n",
      "epoch no.5 train no.398390  loss = 2.02249 avg_loss = 3.12489\n",
      "epoch no.5 train no.398400  loss = 4.10353 avg_loss = 3.09390\n",
      "epoch no.5 train no.398410  loss = 2.58283 avg_loss = 3.08794\n",
      "epoch no.5 train no.398420  loss = 3.38832 avg_loss = 3.09068\n",
      "epoch no.5 train no.398430  loss = 3.65978 avg_loss = 3.08794\n",
      "epoch no.5 train no.398440  loss = 2.79396 avg_loss = 3.08227\n",
      "epoch no.5 train no.398450  loss = 2.69348 avg_loss = 3.07575\n",
      "epoch no.5 train no.398460  loss = 2.48791 avg_loss = 3.07899\n",
      "epoch no.5 train no.398470  loss = 3.69276 avg_loss = 3.05823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.398480  loss = 2.39346 avg_loss = 3.07909\n",
      "epoch no.5 train no.398490  loss = 3.38723 avg_loss = 3.09407\n",
      "epoch no.5 train no.398500  loss = 3.60968 avg_loss = 3.05599\n",
      "epoch no.5 train no.398510  loss = 2.01871 avg_loss = 3.04472\n",
      "epoch no.5 train no.398520  loss = 5.03071 avg_loss = 3.04382\n",
      "epoch no.5 train no.398530  loss = 2.16953 avg_loss = 3.02404\n",
      "epoch no.5 train no.398540  loss = 2.95162 avg_loss = 3.05691\n",
      "epoch no.5 train no.398550  loss = 3.18364 avg_loss = 3.06722\n",
      "epoch no.5 train no.398560  loss = 4.79898 avg_loss = 3.08718\n",
      "epoch no.5 train no.398570  loss = 3.63974 avg_loss = 3.06256\n",
      "epoch no.5 train no.398580  loss = 3.33740 avg_loss = 3.08687\n",
      "epoch no.5 train no.398590  loss = 3.39907 avg_loss = 3.07413\n",
      "epoch no.5 train no.398600  loss = 4.54134 avg_loss = 3.08970\n",
      "epoch no.5 train no.398610  loss = 3.61067 avg_loss = 3.04659\n",
      "epoch no.5 train no.398620  loss = 3.71363 avg_loss = 3.02858\n",
      "epoch no.5 train no.398630  loss = 3.65548 avg_loss = 3.02218\n",
      "epoch no.5 train no.398640  loss = 3.51056 avg_loss = 3.01412\n",
      "epoch no.5 train no.398650  loss = 1.76321 avg_loss = 3.02958\n",
      "epoch no.5 train no.398660  loss = 2.25391 avg_loss = 3.03286\n",
      "epoch no.5 train no.398670  loss = 4.73442 avg_loss = 3.02697\n",
      "epoch no.5 train no.398680  loss = 3.21440 avg_loss = 3.02637\n",
      "epoch no.5 train no.398690  loss = 2.38896 avg_loss = 3.02871\n",
      "epoch no.5 train no.398700  loss = 2.35714 avg_loss = 3.04776\n",
      "epoch no.5 train no.398710  loss = 1.77165 avg_loss = 3.02896\n",
      "epoch no.5 train no.398720  loss = 2.85982 avg_loss = 2.98975\n",
      "epoch no.5 train no.398730  loss = 2.66600 avg_loss = 3.01269\n",
      "epoch no.5 train no.398740  loss = 2.95184 avg_loss = 3.03222\n",
      "epoch no.5 train no.398750  loss = 2.77780 avg_loss = 3.03609\n",
      "epoch no.5 train no.398760  loss = 2.96805 avg_loss = 3.01998\n",
      "epoch no.5 train no.398770  loss = 4.78147 avg_loss = 3.05034\n",
      "epoch no.5 train no.398780  loss = 3.88612 avg_loss = 3.03300\n",
      "epoch no.5 train no.398790  loss = 2.27051 avg_loss = 3.03962\n",
      "epoch no.5 train no.398800  loss = 4.10105 avg_loss = 3.02608\n",
      "epoch no.5 train no.398810  loss = 2.26450 avg_loss = 3.05723\n",
      "epoch no.5 train no.398820  loss = 2.86719 avg_loss = 3.05639\n",
      "epoch no.5 train no.398830  loss = 4.13255 avg_loss = 3.09836\n",
      "epoch no.5 train no.398840  loss = 3.79160 avg_loss = 3.12175\n",
      "epoch no.5 train no.398850  loss = 2.82189 avg_loss = 3.09744\n",
      "epoch no.5 train no.398860  loss = 1.20200 avg_loss = 3.07624\n",
      "epoch no.5 train no.398870  loss = 4.99278 avg_loss = 3.09379\n",
      "epoch no.5 train no.398880  loss = 2.57852 avg_loss = 3.08715\n",
      "epoch no.5 train no.398890  loss = 3.92182 avg_loss = 3.06466\n",
      "epoch no.5 train no.398900  loss = 2.66560 avg_loss = 3.05380\n",
      "epoch no.5 train no.398910  loss = 2.95974 avg_loss = 3.01959\n",
      "epoch no.5 train no.398920  loss = 3.33290 avg_loss = 2.99303\n",
      "epoch no.5 train no.398930  loss = 3.04366 avg_loss = 3.00142\n",
      "epoch no.5 train no.398940  loss = 2.21211 avg_loss = 2.99017\n",
      "epoch no.5 train no.398950  loss = 2.08529 avg_loss = 2.97336\n",
      "epoch no.5 train no.398960  loss = 3.26244 avg_loss = 2.93682\n",
      "epoch no.5 train no.398970  loss = 2.14257 avg_loss = 2.91418\n",
      "epoch no.5 train no.398980  loss = 3.04947 avg_loss = 2.91554\n",
      "epoch no.5 train no.398990  loss = 3.43077 avg_loss = 2.99255\n",
      "epoch no.5 train no.399000  loss = 4.41050 avg_loss = 2.98446\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '에', '▁수', '놓을', '▁감성', '디', '</s>']\n",
      "여름밤을 수놓을 멜로디</s>\n",
      "epoch no.5 train no.399010  loss = 2.89894 avg_loss = 2.95731\n",
      "epoch no.5 train no.399020  loss = 3.91843 avg_loss = 2.97622\n",
      "epoch no.5 train no.399030  loss = 4.08321 avg_loss = 3.00093\n",
      "epoch no.5 train no.399040  loss = 3.02549 avg_loss = 2.96543\n",
      "epoch no.5 train no.399050  loss = 3.12451 avg_loss = 2.96506\n",
      "epoch no.5 train no.399060  loss = 2.94351 avg_loss = 2.95904\n",
      "epoch no.5 train no.399070  loss = 2.97983 avg_loss = 2.94697\n",
      "epoch no.5 train no.399080  loss = 2.96569 avg_loss = 2.91318\n",
      "epoch no.5 train no.399090  loss = 3.35461 avg_loss = 2.89269\n",
      "epoch no.5 train no.399100  loss = 2.55181 avg_loss = 2.86377\n",
      "epoch no.5 train no.399110  loss = 4.55273 avg_loss = 2.88403\n",
      "epoch no.5 train no.399120  loss = 4.96026 avg_loss = 2.93438\n",
      "epoch no.5 train no.399130  loss = 3.92180 avg_loss = 2.93604\n",
      "epoch no.5 train no.399140  loss = 3.90542 avg_loss = 2.94204\n",
      "epoch no.5 train no.399150  loss = 3.31484 avg_loss = 2.98247\n",
      "epoch no.5 train no.399160  loss = 2.78986 avg_loss = 3.02663\n",
      "epoch no.5 train no.399170  loss = 2.95963 avg_loss = 3.00102\n",
      "epoch no.5 train no.399180  loss = 2.71177 avg_loss = 3.03060\n",
      "epoch no.5 train no.399190  loss = 1.45422 avg_loss = 3.03083\n",
      "epoch no.5 train no.399200  loss = 2.19105 avg_loss = 3.05801\n",
      "epoch no.5 train no.399210  loss = 3.13221 avg_loss = 3.07943\n",
      "epoch no.5 train no.399220  loss = 2.43106 avg_loss = 3.16092\n",
      "epoch no.5 train no.399230  loss = 3.36764 avg_loss = 3.16188\n",
      "epoch no.5 train no.399240  loss = 3.69113 avg_loss = 3.18770\n",
      "epoch no.5 train no.399250  loss = 1.87372 avg_loss = 3.20514\n",
      "epoch no.5 train no.399260  loss = 1.93232 avg_loss = 3.19536\n",
      "epoch no.5 train no.399270  loss = 3.76675 avg_loss = 3.16183\n",
      "epoch no.5 train no.399280  loss = 1.82652 avg_loss = 3.14530\n",
      "epoch no.5 train no.399290  loss = 3.05200 avg_loss = 3.14358\n",
      "epoch no.5 train no.399300  loss = 3.21153 avg_loss = 3.16500\n",
      "epoch no.5 train no.399310  loss = 4.24282 avg_loss = 3.16110\n",
      "epoch no.5 train no.399320  loss = 2.42091 avg_loss = 3.15678\n",
      "epoch no.5 train no.399330  loss = 3.68446 avg_loss = 3.17669\n",
      "epoch no.5 train no.399340  loss = 2.94836 avg_loss = 3.18881\n",
      "epoch no.5 train no.399350  loss = 3.03452 avg_loss = 3.17559\n",
      "epoch no.5 train no.399360  loss = 2.76075 avg_loss = 3.12860\n",
      "epoch no.5 train no.399370  loss = 2.93546 avg_loss = 3.16511\n",
      "epoch no.5 train no.399380  loss = 4.58082 avg_loss = 3.16179\n",
      "epoch no.5 train no.399390  loss = 2.87121 avg_loss = 3.15067\n",
      "epoch no.5 train no.399400  loss = 2.68273 avg_loss = 3.10581\n",
      "epoch no.5 train no.399410  loss = 2.72778 avg_loss = 3.11297\n",
      "epoch no.5 train no.399420  loss = 2.67221 avg_loss = 3.10274\n",
      "epoch no.5 train no.399430  loss = 3.80290 avg_loss = 3.12873\n",
      "epoch no.5 train no.399440  loss = 3.40659 avg_loss = 3.15132\n",
      "epoch no.5 train no.399450  loss = 3.00537 avg_loss = 3.15714\n",
      "epoch no.5 train no.399460  loss = 3.16260 avg_loss = 3.15443\n",
      "epoch no.5 train no.399470  loss = 3.09379 avg_loss = 3.12805\n",
      "epoch no.5 train no.399480  loss = 2.80596 avg_loss = 3.12063\n",
      "epoch no.5 train no.399490  loss = 3.30607 avg_loss = 3.11791\n",
      "epoch no.5 train no.399500  loss = 4.00923 avg_loss = 3.12476\n",
      "epoch no.5 train no.399510  loss = 2.49365 avg_loss = 3.12288\n",
      "epoch no.5 train no.399520  loss = 4.27598 avg_loss = 3.13884\n",
      "epoch no.5 train no.399530  loss = 1.62000 avg_loss = 3.14495\n",
      "epoch no.5 train no.399540  loss = 3.89896 avg_loss = 3.18069\n",
      "epoch no.5 train no.399550  loss = 2.44015 avg_loss = 3.18894\n",
      "epoch no.5 train no.399560  loss = 2.12267 avg_loss = 3.16203\n",
      "epoch no.5 train no.399570  loss = 4.06888 avg_loss = 3.12107\n",
      "epoch no.5 train no.399580  loss = 2.03662 avg_loss = 3.09980\n",
      "epoch no.5 train no.399590  loss = 3.48146 avg_loss = 3.08850\n",
      "epoch no.5 train no.399600  loss = 3.50483 avg_loss = 3.09871\n",
      "epoch no.5 train no.399610  loss = 2.16419 avg_loss = 3.11746\n",
      "epoch no.5 train no.399620  loss = 3.00966 avg_loss = 3.11742\n",
      "epoch no.5 train no.399630  loss = 3.55011 avg_loss = 3.11511\n",
      "epoch no.5 train no.399640  loss = 4.23521 avg_loss = 3.10333\n",
      "epoch no.5 train no.399650  loss = 3.05992 avg_loss = 3.09921\n",
      "epoch no.5 train no.399660  loss = 2.27381 avg_loss = 3.06882\n",
      "epoch no.5 train no.399670  loss = 4.99486 avg_loss = 3.06612\n",
      "epoch no.5 train no.399680  loss = 2.14874 avg_loss = 3.04593\n",
      "epoch no.5 train no.399690  loss = 3.79972 avg_loss = 3.07122\n",
      "epoch no.5 train no.399700  loss = 2.31367 avg_loss = 3.05295\n",
      "epoch no.5 train no.399710  loss = 1.51093 avg_loss = 3.02576\n",
      "epoch no.5 train no.399720  loss = 2.67740 avg_loss = 3.03302\n",
      "epoch no.5 train no.399730  loss = 3.81009 avg_loss = 3.03987\n",
      "epoch no.5 train no.399740  loss = 3.01338 avg_loss = 3.06708\n",
      "epoch no.5 train no.399750  loss = 2.26405 avg_loss = 3.04727\n",
      "epoch no.5 train no.399760  loss = 3.46247 avg_loss = 3.05310\n",
      "epoch no.5 train no.399770  loss = 2.94153 avg_loss = 3.05088\n",
      "epoch no.5 train no.399780  loss = 1.80603 avg_loss = 3.02658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.399790  loss = 2.18809 avg_loss = 3.01572\n",
      "epoch no.5 train no.399800  loss = 2.29343 avg_loss = 2.98829\n",
      "epoch no.5 train no.399810  loss = 3.44076 avg_loss = 3.00432\n",
      "epoch no.5 train no.399820  loss = 4.00356 avg_loss = 3.01409\n",
      "epoch no.5 train no.399830  loss = 2.27497 avg_loss = 3.04325\n",
      "epoch no.5 train no.399840  loss = 2.97898 avg_loss = 3.01241\n",
      "epoch no.5 train no.399850  loss = 3.83690 avg_loss = 2.99473\n",
      "epoch no.5 train no.399860  loss = 3.15440 avg_loss = 3.01757\n",
      "epoch no.5 train no.399870  loss = 2.59012 avg_loss = 3.03225\n",
      "epoch no.5 train no.399880  loss = 3.43496 avg_loss = 3.03278\n",
      "epoch no.5 train no.399890  loss = 3.52018 avg_loss = 3.06849\n",
      "epoch no.5 train no.399900  loss = 2.69240 avg_loss = 3.05893\n",
      "epoch no.5 train no.399910  loss = 3.92972 avg_loss = 3.05925\n",
      "epoch no.5 train no.399920  loss = 2.68350 avg_loss = 3.05018\n",
      "epoch no.5 train no.399930  loss = 1.92900 avg_loss = 3.05088\n",
      "epoch no.5 train no.399940  loss = 3.12683 avg_loss = 3.08122\n",
      "epoch no.5 train no.399950  loss = 3.08038 avg_loss = 3.05670\n",
      "epoch no.5 train no.399960  loss = 3.05733 avg_loss = 3.07362\n",
      "epoch no.5 train no.399970  loss = 3.26285 avg_loss = 3.09583\n",
      "epoch no.5 train no.399980  loss = 3.65274 avg_loss = 3.12039\n",
      "epoch no.5 train no.399990  loss = 2.80468 avg_loss = 3.16383\n",
      "epoch no.5 train no.400000  loss = 3.67579 avg_loss = 3.16774\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '▁역시', '▁노래', '들', '죠', '</s>']\n",
      "여름엔 이 노래들이지</s>\n",
      "epoch no.5 train no.400010  loss = 3.16933 avg_loss = 3.14211\n",
      "epoch no.5 train no.400020  loss = 3.97745 avg_loss = 3.13973\n",
      "epoch no.5 train no.400030  loss = 3.39088 avg_loss = 3.15455\n",
      "epoch no.5 train no.400040  loss = 2.91925 avg_loss = 3.12422\n",
      "epoch no.5 train no.400050  loss = 2.81700 avg_loss = 3.10775\n",
      "epoch no.5 train no.400060  loss = 2.58196 avg_loss = 3.10899\n",
      "epoch no.5 train no.400070  loss = 2.94991 avg_loss = 3.07258\n",
      "epoch no.5 train no.400080  loss = 4.39279 avg_loss = 3.11986\n",
      "epoch no.5 train no.400090  loss = 3.53158 avg_loss = 3.13004\n",
      "epoch no.5 train no.400100  loss = 4.76557 avg_loss = 3.13919\n",
      "epoch no.5 train no.400110  loss = 2.01045 avg_loss = 3.12548\n",
      "epoch no.5 train no.400120  loss = 3.74284 avg_loss = 3.12584\n",
      "epoch no.5 train no.400130  loss = 1.51071 avg_loss = 3.12824\n",
      "epoch no.5 train no.400140  loss = 2.88668 avg_loss = 3.11688\n",
      "epoch no.5 train no.400150  loss = 2.68276 avg_loss = 3.09918\n",
      "epoch no.5 train no.400160  loss = 3.48202 avg_loss = 3.09879\n",
      "epoch no.5 train no.400170  loss = 3.49613 avg_loss = 3.12214\n",
      "epoch no.5 train no.400180  loss = 3.67633 avg_loss = 3.14651\n",
      "epoch no.5 train no.400190  loss = 2.53157 avg_loss = 3.13849\n",
      "epoch no.5 train no.400200  loss = 3.62608 avg_loss = 3.21692\n",
      "epoch no.5 train no.400210  loss = 4.30096 avg_loss = 3.22380\n",
      "epoch no.5 train no.400220  loss = 3.15278 avg_loss = 3.22286\n",
      "epoch no.5 train no.400230  loss = 3.66497 avg_loss = 3.21566\n",
      "epoch no.5 train no.400240  loss = 1.78579 avg_loss = 3.18621\n",
      "epoch no.5 train no.400250  loss = 4.10253 avg_loss = 3.22661\n",
      "epoch no.5 train no.400260  loss = 4.83700 avg_loss = 3.23402\n",
      "epoch no.5 train no.400270  loss = 3.63736 avg_loss = 3.21936\n",
      "epoch no.5 train no.400280  loss = 3.31076 avg_loss = 3.19864\n",
      "epoch no.5 train no.400290  loss = 3.78487 avg_loss = 3.20335\n",
      "epoch no.5 train no.400300  loss = 4.32703 avg_loss = 3.18685\n",
      "epoch no.5 train no.400310  loss = 2.43164 avg_loss = 3.12513\n",
      "epoch no.5 train no.400320  loss = 5.33294 avg_loss = 3.11987\n",
      "epoch no.5 train no.400330  loss = 3.11211 avg_loss = 3.11526\n",
      "epoch no.5 train no.400340  loss = 2.72698 avg_loss = 3.07868\n",
      "epoch no.5 train no.400350  loss = 3.52404 avg_loss = 3.08937\n",
      "epoch no.5 train no.400360  loss = 1.56542 avg_loss = 3.06117\n",
      "epoch no.5 train no.400370  loss = 1.40687 avg_loss = 3.06061\n",
      "epoch no.5 train no.400380  loss = 4.28995 avg_loss = 3.05648\n",
      "epoch no.5 train no.400390  loss = 2.10027 avg_loss = 3.03653\n",
      "epoch no.5 train no.400400  loss = 2.97009 avg_loss = 3.04045\n",
      "epoch no.5 train no.400410  loss = 2.90520 avg_loss = 3.01457\n",
      "epoch no.5 train no.400420  loss = 2.41044 avg_loss = 2.98347\n",
      "epoch no.5 train no.400430  loss = 2.29498 avg_loss = 3.02315\n",
      "epoch no.5 train no.400440  loss = 1.98549 avg_loss = 3.01061\n",
      "epoch no.5 train no.400450  loss = 2.76959 avg_loss = 3.00158\n",
      "epoch no.5 train no.400460  loss = 2.65062 avg_loss = 3.02994\n",
      "epoch no.5 train no.400470  loss = 1.79408 avg_loss = 3.06932\n",
      "epoch no.5 train no.400480  loss = 2.00942 avg_loss = 3.04929\n",
      "epoch no.5 train no.400490  loss = 2.95017 avg_loss = 3.08881\n",
      "epoch no.5 train no.400500  loss = 3.41676 avg_loss = 3.09869\n",
      "epoch no.5 train no.400510  loss = 2.13131 avg_loss = 3.07854\n",
      "epoch no.5 train no.400520  loss = 4.24242 avg_loss = 3.08391\n",
      "epoch no.5 train no.400530  loss = 3.58478 avg_loss = 3.14721\n",
      "epoch no.5 train no.400540  loss = 2.50924 avg_loss = 3.16144\n",
      "epoch no.5 train no.400550  loss = 1.98406 avg_loss = 3.13566\n",
      "epoch no.5 train no.400560  loss = 3.04813 avg_loss = 3.12651\n",
      "epoch no.5 train no.400570  loss = 2.49010 avg_loss = 3.10474\n",
      "epoch no.5 train no.400580  loss = 2.47376 avg_loss = 3.08293\n",
      "epoch no.5 train no.400590  loss = 3.30022 avg_loss = 3.11265\n",
      "epoch no.5 train no.400600  loss = 2.24203 avg_loss = 3.12062\n",
      "epoch no.5 train no.400610  loss = 4.17975 avg_loss = 3.09993\n",
      "epoch no.5 train no.400620  loss = 2.54857 avg_loss = 3.14810\n",
      "epoch no.5 train no.400630  loss = 2.64914 avg_loss = 3.12226\n",
      "epoch no.5 train no.400640  loss = 4.35018 avg_loss = 3.12837\n",
      "epoch no.5 train no.400650  loss = 2.04420 avg_loss = 3.10889\n",
      "epoch no.5 train no.400660  loss = 3.86714 avg_loss = 3.09775\n",
      "epoch no.5 train no.400670  loss = 2.30769 avg_loss = 3.07693\n",
      "epoch no.5 train no.400680  loss = 3.05685 avg_loss = 3.04366\n",
      "epoch no.5 train no.400690  loss = 3.14099 avg_loss = 3.05544\n",
      "epoch no.5 train no.400700  loss = 4.11795 avg_loss = 3.05604\n",
      "epoch no.5 train no.400710  loss = 2.29339 avg_loss = 3.04393\n",
      "epoch no.5 train no.400720  loss = 3.80437 avg_loss = 3.05904\n",
      "epoch no.5 train no.400730  loss = 2.90200 avg_loss = 3.04698\n",
      "epoch no.5 train no.400740  loss = 3.52387 avg_loss = 3.02313\n",
      "epoch no.5 train no.400750  loss = 3.36194 avg_loss = 3.04396\n",
      "epoch no.5 train no.400760  loss = 2.53690 avg_loss = 3.04785\n",
      "epoch no.5 train no.400770  loss = 3.39291 avg_loss = 3.06423\n",
      "epoch no.5 train no.400780  loss = 3.46799 avg_loss = 3.12498\n",
      "epoch no.5 train no.400790  loss = 2.80768 avg_loss = 3.12097\n",
      "epoch no.5 train no.400800  loss = 2.90905 avg_loss = 3.11131\n",
      "epoch no.5 train no.400810  loss = 3.27880 avg_loss = 3.12304\n",
      "epoch no.5 train no.400820  loss = 2.90631 avg_loss = 3.10494\n",
      "epoch no.5 train no.400830  loss = 2.25589 avg_loss = 3.12185\n",
      "epoch no.5 train no.400840  loss = 4.72952 avg_loss = 3.12186\n",
      "epoch no.5 train no.400850  loss = 1.38870 avg_loss = 3.06683\n",
      "epoch no.5 train no.400860  loss = 2.96094 avg_loss = 3.05118\n",
      "epoch no.5 train no.400870  loss = 3.80639 avg_loss = 3.06338\n",
      "epoch no.5 train no.400880  loss = 4.48949 avg_loss = 3.07711\n",
      "epoch no.5 train no.400890  loss = 2.39991 avg_loss = 3.09316\n",
      "epoch no.5 train no.400900  loss = 2.84015 avg_loss = 3.08611\n",
      "epoch no.5 train no.400910  loss = 3.11011 avg_loss = 3.08352\n",
      "epoch no.5 train no.400920  loss = 2.77248 avg_loss = 3.06684\n",
      "epoch no.5 train no.400930  loss = 3.30061 avg_loss = 3.03421\n",
      "epoch no.5 train no.400940  loss = 2.13988 avg_loss = 3.01115\n",
      "epoch no.5 train no.400950  loss = 5.32067 avg_loss = 3.04538\n",
      "epoch no.5 train no.400960  loss = 4.18647 avg_loss = 3.06289\n",
      "epoch no.5 train no.400970  loss = 2.61746 avg_loss = 3.08316\n",
      "epoch no.5 train no.400980  loss = 2.81391 avg_loss = 3.09914\n",
      "epoch no.5 train no.400990  loss = 2.35957 avg_loss = 3.08365\n",
      "epoch no.5 train no.401000  loss = 4.63716 avg_loss = 3.10646\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "여름밤에 듣기 좋은 노래</s>\n",
      "epoch no.5 train no.401010  loss = 4.20011 avg_loss = 3.11691\n",
      "epoch no.5 train no.401020  loss = 2.48152 avg_loss = 3.09653\n",
      "epoch no.5 train no.401030  loss = 2.27714 avg_loss = 3.09833\n",
      "epoch no.5 train no.401040  loss = 2.55278 avg_loss = 3.06949\n",
      "epoch no.5 train no.401050  loss = 2.55509 avg_loss = 3.06044\n",
      "epoch no.5 train no.401060  loss = 3.21382 avg_loss = 3.06528\n",
      "epoch no.5 train no.401070  loss = 2.27463 avg_loss = 3.04680\n",
      "epoch no.5 train no.401080  loss = 2.99077 avg_loss = 3.08047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.401090  loss = 2.20232 avg_loss = 3.08395\n",
      "epoch no.5 train no.401100  loss = 1.90782 avg_loss = 3.08524\n",
      "epoch no.5 train no.401110  loss = 3.69145 avg_loss = 3.05664\n",
      "epoch no.5 train no.401120  loss = 3.56750 avg_loss = 3.04312\n",
      "epoch no.5 train no.401130  loss = 2.76770 avg_loss = 3.02857\n",
      "epoch no.5 train no.401140  loss = 2.37538 avg_loss = 3.01372\n",
      "epoch no.5 train no.401150  loss = 2.95789 avg_loss = 3.04154\n",
      "epoch no.5 train no.401160  loss = 1.96156 avg_loss = 3.07038\n",
      "epoch no.5 train no.401170  loss = 3.31274 avg_loss = 3.04998\n",
      "epoch no.5 train no.401180  loss = 1.89829 avg_loss = 3.02232\n",
      "epoch no.5 train no.401190  loss = 2.18273 avg_loss = 2.99946\n",
      "epoch no.5 train no.401200  loss = 1.72874 avg_loss = 2.95857\n",
      "epoch no.5 train no.401210  loss = 3.92909 avg_loss = 2.98253\n",
      "epoch no.5 train no.401220  loss = 2.90395 avg_loss = 2.98155\n",
      "epoch no.5 train no.401230  loss = 2.22816 avg_loss = 2.93535\n",
      "epoch no.5 train no.401240  loss = 2.81426 avg_loss = 2.95346\n",
      "epoch no.5 train no.401250  loss = 2.97669 avg_loss = 2.97117\n",
      "epoch no.5 train no.401260  loss = 2.55684 avg_loss = 2.97386\n",
      "epoch no.5 train no.401270  loss = 2.65446 avg_loss = 2.97622\n",
      "epoch no.5 train no.401280  loss = 2.08643 avg_loss = 2.95991\n",
      "epoch no.5 train no.401290  loss = 2.71613 avg_loss = 2.98592\n",
      "epoch no.5 train no.401300  loss = 2.39826 avg_loss = 2.94813\n",
      "epoch no.5 train no.401310  loss = 2.42252 avg_loss = 2.95457\n",
      "epoch no.5 train no.401320  loss = 2.77672 avg_loss = 2.97824\n",
      "epoch no.5 train no.401330  loss = 3.48437 avg_loss = 2.95202\n",
      "epoch no.5 train no.401340  loss = 2.31983 avg_loss = 2.93291\n",
      "epoch no.5 train no.401350  loss = 2.60352 avg_loss = 2.90449\n",
      "epoch no.5 train no.401360  loss = 4.00790 avg_loss = 2.89075\n",
      "epoch no.5 train no.401370  loss = 2.32782 avg_loss = 2.89818\n",
      "epoch no.5 train no.401380  loss = 2.55921 avg_loss = 2.90033\n",
      "epoch no.5 train no.401390  loss = 2.91619 avg_loss = 2.95934\n",
      "epoch no.5 train no.401400  loss = 2.05449 avg_loss = 2.97324\n",
      "epoch no.5 train no.401410  loss = 2.74439 avg_loss = 2.97683\n",
      "epoch no.5 train no.401420  loss = 3.27471 avg_loss = 2.95127\n",
      "epoch no.5 train no.401430  loss = 1.99237 avg_loss = 2.95160\n",
      "epoch no.5 train no.401440  loss = 2.00314 avg_loss = 2.96121\n",
      "epoch no.5 train no.401450  loss = 2.50516 avg_loss = 3.03311\n",
      "epoch no.5 train no.401460  loss = 2.95220 avg_loss = 3.06148\n",
      "epoch no.5 train no.401470  loss = 2.36409 avg_loss = 3.09494\n",
      "epoch no.5 train no.401480  loss = 3.25166 avg_loss = 3.06971\n",
      "epoch no.5 train no.401490  loss = 2.50229 avg_loss = 3.05597\n",
      "epoch no.5 train no.401500  loss = 2.87403 avg_loss = 3.07023\n",
      "epoch no.5 train no.401510  loss = 3.52058 avg_loss = 3.07589\n",
      "epoch no.5 train no.401520  loss = 1.85936 avg_loss = 3.02822\n",
      "epoch no.5 train no.401530  loss = 2.86380 avg_loss = 3.05118\n",
      "epoch no.5 train no.401540  loss = 5.05970 avg_loss = 3.06774\n",
      "epoch no.5 train no.401550  loss = 2.41016 avg_loss = 3.08536\n",
      "epoch no.5 train no.401560  loss = 3.33496 avg_loss = 3.05038\n",
      "epoch no.5 train no.401570  loss = 2.32459 avg_loss = 3.05006\n",
      "epoch no.5 train no.401580  loss = 2.25246 avg_loss = 3.06042\n",
      "epoch no.5 train no.401590  loss = 2.15156 avg_loss = 3.06424\n",
      "epoch no.5 train no.401600  loss = 3.17447 avg_loss = 3.07177\n",
      "epoch no.5 train no.401610  loss = 2.28321 avg_loss = 3.07282\n",
      "epoch no.5 train no.401620  loss = 4.64616 avg_loss = 3.09635\n",
      "epoch no.5 train no.401630  loss = 2.32708 avg_loss = 3.06436\n",
      "epoch no.5 train no.401640  loss = 3.13703 avg_loss = 3.06899\n",
      "epoch no.5 train no.401650  loss = 3.62478 avg_loss = 3.07972\n",
      "epoch no.5 train no.401660  loss = 3.65608 avg_loss = 3.12825\n",
      "epoch no.5 train no.401670  loss = 3.18696 avg_loss = 3.10929\n",
      "epoch no.5 train no.401680  loss = 3.02217 avg_loss = 3.06916\n",
      "epoch no.5 train no.401690  loss = 2.76214 avg_loss = 3.11043\n",
      "epoch no.5 train no.401700  loss = 2.39731 avg_loss = 3.03221\n",
      "epoch no.5 train no.401710  loss = 5.05467 avg_loss = 3.06314\n",
      "epoch no.5 train no.401720  loss = 3.56853 avg_loss = 3.05478\n",
      "epoch no.5 train no.401730  loss = 2.60659 avg_loss = 3.05159\n",
      "epoch no.5 train no.401740  loss = 2.52521 avg_loss = 3.05589\n",
      "epoch no.5 train no.401750  loss = 3.25416 avg_loss = 3.05812\n",
      "epoch no.5 train no.401760  loss = 4.21865 avg_loss = 3.04696\n",
      "epoch no.5 train no.401770  loss = 4.45476 avg_loss = 3.04448\n",
      "epoch no.5 train no.401780  loss = 4.00908 avg_loss = 3.01537\n",
      "epoch no.5 train no.401790  loss = 3.33303 avg_loss = 3.00255\n",
      "epoch no.5 train no.401800  loss = 3.97431 avg_loss = 3.01462\n",
      "epoch no.5 train no.401810  loss = 2.50765 avg_loss = 3.01814\n",
      "epoch no.5 train no.401820  loss = 2.63760 avg_loss = 3.02892\n",
      "epoch no.5 train no.401830  loss = 4.51468 avg_loss = 3.03751\n",
      "epoch no.5 train no.401840  loss = 2.92923 avg_loss = 3.03348\n",
      "epoch no.5 train no.401850  loss = 1.99119 avg_loss = 3.02672\n",
      "epoch no.5 train no.401860  loss = 3.17205 avg_loss = 3.00495\n",
      "epoch no.5 train no.401870  loss = 3.16090 avg_loss = 2.96988\n",
      "epoch no.5 train no.401880  loss = 4.01607 avg_loss = 2.98550\n",
      "epoch no.5 train no.401890  loss = 3.59255 avg_loss = 3.02146\n",
      "epoch no.5 train no.401900  loss = 3.98609 avg_loss = 3.04028\n",
      "epoch no.5 train no.401910  loss = 1.82107 avg_loss = 3.03604\n",
      "epoch no.5 train no.401920  loss = 4.34119 avg_loss = 3.06467\n",
      "epoch no.5 train no.401930  loss = 2.43166 avg_loss = 3.03259\n",
      "epoch no.5 train no.401940  loss = 3.01135 avg_loss = 3.00846\n",
      "epoch no.5 train no.401950  loss = 2.31303 avg_loss = 3.03573\n",
      "epoch no.5 train no.401960  loss = 2.94346 avg_loss = 3.02342\n",
      "epoch no.5 train no.401970  loss = 2.86979 avg_loss = 3.07750\n",
      "epoch no.5 train no.401980  loss = 2.33626 avg_loss = 3.08823\n",
      "epoch no.5 train no.401990  loss = 1.96183 avg_loss = 3.06823\n",
      "epoch no.5 train no.402000  loss = 4.01332 avg_loss = 3.07331\n",
      "8\n",
      "to_tokens: ['▁비', '밤', '▁끝', '자', '락', '▁감성', '보는', '▁좋은', '▁노래', '</s>']\n",
      "여름의 끝자락 들어도 좋은 노래</s>\n",
      "epoch no.5 train no.402010  loss = 4.16625 avg_loss = 3.06961\n",
      "epoch no.5 train no.402020  loss = 3.08535 avg_loss = 3.05881\n",
      "epoch no.5 train no.402030  loss = 3.08501 avg_loss = 3.08052\n",
      "epoch no.5 train no.402040  loss = 2.68707 avg_loss = 3.05808\n",
      "epoch no.5 train no.402050  loss = 3.04308 avg_loss = 3.03502\n",
      "epoch no.5 train no.402060  loss = 3.25482 avg_loss = 3.03942\n",
      "epoch no.5 train no.402070  loss = 4.51878 avg_loss = 3.09882\n",
      "epoch no.5 train no.402080  loss = 2.18471 avg_loss = 3.09749\n",
      "epoch no.5 train no.402090  loss = 3.53054 avg_loss = 3.06118\n",
      "epoch no.5 train no.402100  loss = 3.25417 avg_loss = 3.05802\n",
      "epoch no.5 train no.402110  loss = 2.49211 avg_loss = 3.05362\n",
      "epoch no.5 train no.402120  loss = 3.23947 avg_loss = 3.11802\n",
      "epoch no.5 train no.402130  loss = 2.57133 avg_loss = 3.12083\n",
      "epoch no.5 train no.402140  loss = 2.94880 avg_loss = 3.06959\n",
      "epoch no.5 train no.402150  loss = 2.09047 avg_loss = 3.03984\n",
      "epoch no.5 train no.402160  loss = 3.76439 avg_loss = 3.07918\n",
      "epoch no.5 train no.402170  loss = 2.68466 avg_loss = 3.07329\n",
      "epoch no.5 train no.402180  loss = 2.72283 avg_loss = 3.05991\n",
      "epoch no.5 train no.402190  loss = 3.00799 avg_loss = 3.03171\n",
      "epoch no.5 train no.402200  loss = 3.30635 avg_loss = 3.02230\n",
      "epoch no.5 train no.402210  loss = 2.91886 avg_loss = 3.06508\n",
      "epoch no.5 train no.402220  loss = 2.61731 avg_loss = 3.05619\n",
      "epoch no.5 train no.402230  loss = 2.95498 avg_loss = 3.07539\n",
      "epoch no.5 train no.402240  loss = 3.26664 avg_loss = 3.05598\n",
      "epoch no.5 train no.402250  loss = 4.40277 avg_loss = 3.05367\n",
      "epoch no.5 train no.402260  loss = 3.37223 avg_loss = 3.08399\n",
      "epoch no.5 train no.402270  loss = 3.12653 avg_loss = 3.07387\n",
      "epoch no.5 train no.402280  loss = 3.42030 avg_loss = 3.06056\n",
      "epoch no.5 train no.402290  loss = 2.67727 avg_loss = 3.04844\n",
      "epoch no.5 train no.402300  loss = 4.07251 avg_loss = 3.07131\n",
      "epoch no.5 train no.402310  loss = 2.24041 avg_loss = 3.05388\n",
      "epoch no.5 train no.402320  loss = 2.20463 avg_loss = 3.08646\n",
      "epoch no.5 train no.402330  loss = 3.05500 avg_loss = 3.09322\n",
      "epoch no.5 train no.402340  loss = 2.32780 avg_loss = 3.02972\n",
      "epoch no.5 train no.402350  loss = 3.17018 avg_loss = 2.96539\n",
      "epoch no.5 train no.402360  loss = 3.81848 avg_loss = 2.95485\n",
      "epoch no.5 train no.402370  loss = 2.36684 avg_loss = 2.93779\n",
      "epoch no.5 train no.402380  loss = 1.73359 avg_loss = 2.99949\n",
      "epoch no.5 train no.402390  loss = 2.59535 avg_loss = 3.01297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.402400  loss = 3.23450 avg_loss = 3.01054\n",
      "epoch no.5 train no.402410  loss = 2.82536 avg_loss = 3.03201\n",
      "epoch no.5 train no.402420  loss = 4.52960 avg_loss = 3.06775\n",
      "epoch no.5 train no.402430  loss = 4.72975 avg_loss = 3.10637\n",
      "epoch no.5 train no.402440  loss = 3.38328 avg_loss = 3.09643\n",
      "epoch no.5 train no.402450  loss = 2.41943 avg_loss = 3.08091\n",
      "epoch no.5 train no.402460  loss = 3.35428 avg_loss = 3.10090\n",
      "epoch no.5 train no.402470  loss = 2.47297 avg_loss = 3.11420\n",
      "epoch no.5 train no.402480  loss = 2.25033 avg_loss = 3.12703\n",
      "epoch no.5 train no.402490  loss = 2.78705 avg_loss = 3.12040\n",
      "epoch no.5 train no.402500  loss = 5.27505 avg_loss = 3.10489\n",
      "epoch no.5 train no.402510  loss = 3.67459 avg_loss = 3.07936\n",
      "epoch no.5 train no.402520  loss = 4.13545 avg_loss = 3.09889\n",
      "epoch no.5 train no.402530  loss = 3.52186 avg_loss = 3.07595\n",
      "epoch no.5 train no.402540  loss = 2.70599 avg_loss = 3.03078\n",
      "epoch no.5 train no.402550  loss = 3.49781 avg_loss = 3.07084\n",
      "epoch no.5 train no.402560  loss = 3.88619 avg_loss = 3.04622\n",
      "epoch no.5 train no.402570  loss = 3.86702 avg_loss = 3.08589\n",
      "epoch no.5 train no.402580  loss = 4.56972 avg_loss = 3.11296\n",
      "epoch no.5 train no.402590  loss = 3.39632 avg_loss = 3.06923\n",
      "epoch no.5 train no.402600  loss = 3.65658 avg_loss = 3.09520\n",
      "epoch no.5 train no.402610  loss = 2.96238 avg_loss = 3.13343\n",
      "epoch no.5 train no.402620  loss = 4.38058 avg_loss = 3.14916\n",
      "epoch no.5 train no.402630  loss = 2.96157 avg_loss = 3.11161\n",
      "epoch no.5 train no.402640  loss = 2.01569 avg_loss = 3.08162\n",
      "epoch no.5 train no.402650  loss = 1.70051 avg_loss = 3.06084\n",
      "epoch no.5 train no.402660  loss = 3.88466 avg_loss = 3.02413\n",
      "epoch no.5 train no.402670  loss = 3.72486 avg_loss = 3.05749\n",
      "epoch no.5 train no.402680  loss = 4.19592 avg_loss = 3.08507\n",
      "epoch no.5 train no.402690  loss = 3.69040 avg_loss = 3.06433\n",
      "epoch no.5 train no.402700  loss = 2.40516 avg_loss = 3.00464\n",
      "epoch no.5 train no.402710  loss = 2.77230 avg_loss = 2.96622\n",
      "epoch no.5 train no.402720  loss = 2.93062 avg_loss = 2.94613\n",
      "epoch no.5 train no.402730  loss = 2.04488 avg_loss = 2.95442\n",
      "epoch no.5 train no.402740  loss = 2.79797 avg_loss = 2.98456\n",
      "epoch no.5 train no.402750  loss = 3.24999 avg_loss = 2.99318\n",
      "epoch no.5 train no.402760  loss = 1.35212 avg_loss = 2.98371\n",
      "epoch no.5 train no.402770  loss = 3.67074 avg_loss = 3.00655\n",
      "epoch no.5 train no.402780  loss = 2.60126 avg_loss = 3.01448\n",
      "epoch no.5 train no.402790  loss = 3.64157 avg_loss = 3.04693\n",
      "epoch no.5 train no.402800  loss = 2.68862 avg_loss = 2.99398\n",
      "epoch no.5 train no.402810  loss = 3.10545 avg_loss = 3.02482\n",
      "epoch no.5 train no.402820  loss = 2.34275 avg_loss = 3.02302\n",
      "epoch no.5 train no.402830  loss = 3.86386 avg_loss = 3.01790\n",
      "epoch no.5 train no.402840  loss = 3.00501 avg_loss = 3.05449\n",
      "epoch no.5 train no.402850  loss = 2.70408 avg_loss = 3.03489\n",
      "epoch no.5 train no.402860  loss = 2.47798 avg_loss = 3.06307\n",
      "epoch no.5 train no.402870  loss = 2.46201 avg_loss = 3.07405\n",
      "epoch no.5 train no.402880  loss = 2.93822 avg_loss = 3.06557\n",
      "epoch no.5 train no.402890  loss = 2.38405 avg_loss = 3.11983\n",
      "epoch no.5 train no.402900  loss = 3.81520 avg_loss = 3.16966\n",
      "epoch no.5 train no.402910  loss = 3.33945 avg_loss = 3.21089\n",
      "epoch no.5 train no.402920  loss = 2.54242 avg_loss = 3.21214\n",
      "epoch no.5 train no.402930  loss = 2.73683 avg_loss = 3.18166\n",
      "epoch no.5 train no.402940  loss = 2.47714 avg_loss = 3.13991\n",
      "epoch no.5 train no.402950  loss = 2.92416 avg_loss = 3.12001\n",
      "epoch no.5 train no.402960  loss = 2.98061 avg_loss = 3.15117\n",
      "epoch no.5 train no.402970  loss = 3.78792 avg_loss = 3.13125\n",
      "epoch no.5 train no.402980  loss = 3.64383 avg_loss = 3.15133\n",
      "epoch no.5 train no.402990  loss = 4.46424 avg_loss = 3.16835\n",
      "epoch no.5 train no.403000  loss = 3.44347 avg_loss = 3.14478\n",
      "7\n",
      "to_tokens: ['▁비', '밤', '에', '▁재즈', '힙', '에서', '▁들', '보', '</s>']\n",
      "여름밤의 재즈바에서 재즈 한잔</s>\n",
      "epoch no.5 train no.403010  loss = 4.31507 avg_loss = 3.16846\n",
      "epoch no.5 train no.403020  loss = 2.83505 avg_loss = 3.17567\n",
      "epoch no.5 train no.403030  loss = 1.06917 avg_loss = 3.13580\n",
      "epoch no.5 train no.403040  loss = 4.00450 avg_loss = 3.13083\n",
      "epoch no.5 train no.403050  loss = 3.01815 avg_loss = 3.10955\n",
      "epoch no.5 train no.403060  loss = 2.67160 avg_loss = 3.09556\n",
      "epoch no.5 train no.403070  loss = 3.93899 avg_loss = 3.08466\n",
      "epoch no.5 train no.403080  loss = 3.48961 avg_loss = 3.09169\n",
      "epoch no.5 train no.403090  loss = 2.49773 avg_loss = 3.11322\n",
      "epoch no.5 train no.403100  loss = 2.34414 avg_loss = 3.16074\n",
      "epoch no.5 train no.403110  loss = 3.22941 avg_loss = 3.14870\n",
      "epoch no.5 train no.403120  loss = 3.25171 avg_loss = 3.11523\n",
      "epoch no.5 train no.403130  loss = 1.77764 avg_loss = 3.14435\n",
      "epoch no.5 train no.403140  loss = 3.49751 avg_loss = 3.12893\n",
      "epoch no.5 train no.403150  loss = 3.60862 avg_loss = 3.15611\n",
      "epoch no.5 train no.403160  loss = 3.10352 avg_loss = 3.14622\n",
      "epoch no.5 train no.403170  loss = 4.14333 avg_loss = 3.15261\n",
      "epoch no.5 train no.403180  loss = 2.13991 avg_loss = 3.16437\n",
      "epoch no.5 train no.403190  loss = 3.59146 avg_loss = 3.18532\n",
      "epoch no.5 train no.403200  loss = 3.85101 avg_loss = 3.20682\n",
      "epoch no.5 train no.403210  loss = 3.13277 avg_loss = 3.16583\n",
      "epoch no.5 train no.403220  loss = 4.56663 avg_loss = 3.19991\n",
      "epoch no.5 train no.403230  loss = 3.00018 avg_loss = 3.20194\n",
      "epoch no.5 train no.403240  loss = 2.43804 avg_loss = 3.18930\n",
      "epoch no.5 train no.403250  loss = 2.37852 avg_loss = 3.15912\n",
      "epoch no.5 train no.403260  loss = 3.36350 avg_loss = 3.14555\n",
      "epoch no.5 train no.403270  loss = 3.32876 avg_loss = 3.14475\n",
      "epoch no.5 train no.403280  loss = 3.15038 avg_loss = 3.13553\n",
      "epoch no.5 train no.403290  loss = 3.10137 avg_loss = 3.09811\n",
      "epoch no.5 train no.403300  loss = 2.54165 avg_loss = 3.11581\n",
      "epoch no.5 train no.403310  loss = 3.13272 avg_loss = 3.13155\n",
      "epoch no.5 train no.403320  loss = 2.05404 avg_loss = 3.11129\n",
      "epoch no.5 train no.403330  loss = 3.57708 avg_loss = 3.10419\n",
      "epoch no.5 train no.403340  loss = 2.69245 avg_loss = 3.15100\n",
      "epoch no.5 train no.403350  loss = 2.07474 avg_loss = 3.17066\n",
      "epoch no.5 train no.403360  loss = 3.58457 avg_loss = 3.14641\n",
      "epoch no.5 train no.403370  loss = 2.38764 avg_loss = 3.08306\n",
      "epoch no.5 train no.403380  loss = 4.52468 avg_loss = 3.10382\n",
      "epoch no.5 train no.403390  loss = 3.25286 avg_loss = 3.11226\n",
      "epoch no.5 train no.403400  loss = 2.46837 avg_loss = 3.11161\n",
      "epoch no.5 train no.403410  loss = 2.63141 avg_loss = 3.12143\n",
      "epoch no.5 train no.403420  loss = 3.08191 avg_loss = 3.12496\n",
      "epoch no.5 train no.403430  loss = 2.93055 avg_loss = 3.13077\n",
      "epoch no.5 train no.403440  loss = 2.58389 avg_loss = 3.07530\n",
      "epoch no.5 train no.403450  loss = 1.96543 avg_loss = 3.03512\n",
      "epoch no.5 train no.403460  loss = 2.61220 avg_loss = 3.02742\n",
      "epoch no.5 train no.403470  loss = 1.44663 avg_loss = 3.03667\n",
      "epoch no.5 train no.403480  loss = 2.40649 avg_loss = 2.98977\n",
      "epoch no.5 train no.403490  loss = 2.84337 avg_loss = 3.00293\n",
      "epoch no.5 train no.403500  loss = 2.76120 avg_loss = 2.97376\n",
      "epoch no.5 train no.403510  loss = 3.19001 avg_loss = 2.98788\n",
      "epoch no.5 train no.403520  loss = 3.88850 avg_loss = 3.02316\n",
      "epoch no.5 train no.403530  loss = 2.27108 avg_loss = 2.99780\n",
      "epoch no.5 train no.403540  loss = 2.57629 avg_loss = 3.02019\n",
      "epoch no.5 train no.403550  loss = 3.20953 avg_loss = 3.05333\n",
      "epoch no.5 train no.403560  loss = 2.37046 avg_loss = 3.03120\n",
      "epoch no.5 train no.403570  loss = 3.35352 avg_loss = 3.00240\n",
      "epoch no.5 train no.403580  loss = 1.98169 avg_loss = 3.02669\n",
      "epoch no.5 train no.403590  loss = 2.67167 avg_loss = 3.03062\n",
      "epoch no.5 train no.403600  loss = 2.98780 avg_loss = 3.02113\n",
      "epoch no.5 train no.403610  loss = 2.58138 avg_loss = 2.98519\n",
      "epoch no.5 train no.403620  loss = 3.16603 avg_loss = 2.98031\n",
      "epoch no.5 train no.403630  loss = 6.05579 avg_loss = 3.05444\n",
      "epoch no.5 train no.403640  loss = 3.98919 avg_loss = 3.02816\n",
      "epoch no.5 train no.403650  loss = 4.23651 avg_loss = 3.03675\n",
      "epoch no.5 train no.403660  loss = 2.82803 avg_loss = 3.05570\n",
      "epoch no.5 train no.403670  loss = 3.29162 avg_loss = 3.04518\n",
      "epoch no.5 train no.403680  loss = 4.32116 avg_loss = 3.06819\n",
      "epoch no.5 train no.403690  loss = 3.06363 avg_loss = 3.11555\n",
      "epoch no.5 train no.403700  loss = 3.11906 avg_loss = 3.08130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.403710  loss = 2.31673 avg_loss = 3.12630\n",
      "epoch no.5 train no.403720  loss = 2.13533 avg_loss = 3.10301\n",
      "epoch no.5 train no.403730  loss = 3.18282 avg_loss = 3.08274\n",
      "epoch no.5 train no.403740  loss = 2.89469 avg_loss = 3.07974\n",
      "epoch no.5 train no.403750  loss = 3.48675 avg_loss = 3.09465\n",
      "epoch no.5 train no.403760  loss = 3.55257 avg_loss = 3.12106\n",
      "epoch no.5 train no.403770  loss = 3.55397 avg_loss = 3.10743\n",
      "epoch no.5 train no.403780  loss = 4.13403 avg_loss = 3.11328\n",
      "epoch no.5 train no.403790  loss = 2.99569 avg_loss = 3.08708\n",
      "epoch no.5 train no.403800  loss = 3.50065 avg_loss = 3.05406\n",
      "epoch no.5 train no.403810  loss = 2.16117 avg_loss = 3.10668\n",
      "epoch no.5 train no.403820  loss = 2.84153 avg_loss = 3.12634\n",
      "epoch no.5 train no.403830  loss = 2.87310 avg_loss = 3.11492\n",
      "epoch no.5 train no.403840  loss = 2.85919 avg_loss = 3.15884\n",
      "epoch no.5 train no.403850  loss = 3.44371 avg_loss = 3.13711\n",
      "epoch no.5 train no.403860  loss = 3.42589 avg_loss = 3.11350\n",
      "epoch no.5 train no.403870  loss = 2.82898 avg_loss = 3.07565\n",
      "epoch no.5 train no.403880  loss = 2.47428 avg_loss = 3.08215\n",
      "epoch no.5 train no.403890  loss = 5.07006 avg_loss = 3.08890\n",
      "epoch no.5 train no.403900  loss = 1.87054 avg_loss = 3.05840\n",
      "epoch no.5 train no.403910  loss = 3.80025 avg_loss = 3.06433\n",
      "epoch no.5 train no.403920  loss = 2.55223 avg_loss = 3.06518\n",
      "epoch no.5 train no.403930  loss = 3.38858 avg_loss = 3.02527\n",
      "epoch no.5 train no.403940  loss = 4.80950 avg_loss = 2.99623\n",
      "epoch no.5 train no.403950  loss = 3.93117 avg_loss = 3.01817\n",
      "epoch no.5 train no.403960  loss = 2.48383 avg_loss = 2.97995\n",
      "epoch no.5 train no.403970  loss = 3.15854 avg_loss = 2.97276\n",
      "epoch no.5 train no.403980  loss = 3.54222 avg_loss = 2.97520\n",
      "epoch no.5 train no.403990  loss = 2.33582 avg_loss = 2.97935\n",
      "epoch no.5 train no.404000  loss = 2.08265 avg_loss = 2.97702\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁그루', '보', '스', '</s>']\n",
      "여름밤의 재즈 블루스</s>\n",
      "epoch no.5 train no.404010  loss = 3.18483 avg_loss = 2.95368\n",
      "epoch no.5 train no.404020  loss = 2.24154 avg_loss = 2.91994\n",
      "epoch no.5 train no.404030  loss = 3.53501 avg_loss = 2.98646\n",
      "epoch no.5 train no.404040  loss = 2.18610 avg_loss = 2.97531\n",
      "epoch no.5 train no.404050  loss = 2.75161 avg_loss = 2.99183\n",
      "epoch no.5 train no.404060  loss = 2.66120 avg_loss = 2.96707\n",
      "epoch no.5 train no.404070  loss = 3.88947 avg_loss = 3.00604\n",
      "epoch no.5 train no.404080  loss = 3.95511 avg_loss = 2.98228\n",
      "epoch no.5 train no.404090  loss = 2.24987 avg_loss = 2.94457\n",
      "epoch no.5 train no.404100  loss = 4.20497 avg_loss = 2.96666\n",
      "epoch no.5 train no.404110  loss = 3.89050 avg_loss = 2.98205\n",
      "epoch no.5 train no.404120  loss = 1.92867 avg_loss = 2.96217\n",
      "epoch no.5 train no.404130  loss = 3.36626 avg_loss = 3.01209\n",
      "epoch no.5 train no.404140  loss = 2.51179 avg_loss = 3.02347\n",
      "epoch no.5 train no.404150  loss = 2.69668 avg_loss = 3.02541\n",
      "epoch no.5 train no.404160  loss = 2.80768 avg_loss = 3.05818\n",
      "epoch no.5 train no.404170  loss = 1.96803 avg_loss = 3.02572\n",
      "epoch no.5 train no.404180  loss = 3.27132 avg_loss = 3.00745\n",
      "epoch no.5 train no.404190  loss = 2.41340 avg_loss = 3.00714\n",
      "epoch no.5 train no.404200  loss = 3.21590 avg_loss = 3.02361\n",
      "epoch no.5 train no.404210  loss = 3.33055 avg_loss = 3.01473\n",
      "epoch no.5 train no.404220  loss = 3.28532 avg_loss = 3.05265\n",
      "epoch no.5 train no.404230  loss = 2.61889 avg_loss = 3.05878\n",
      "epoch no.5 train no.404240  loss = 3.09402 avg_loss = 3.03017\n",
      "epoch no.5 train no.404250  loss = 2.59124 avg_loss = 3.02446\n",
      "epoch no.5 train no.404260  loss = 2.60630 avg_loss = 3.01057\n",
      "epoch no.5 train no.404270  loss = 2.94179 avg_loss = 2.99699\n",
      "epoch no.5 train no.404280  loss = 3.28562 avg_loss = 3.00894\n",
      "epoch no.5 train no.404290  loss = 2.54236 avg_loss = 3.01475\n",
      "epoch no.5 train no.404300  loss = 3.08793 avg_loss = 3.03537\n",
      "epoch no.5 train no.404310  loss = 2.39999 avg_loss = 3.03665\n",
      "epoch no.5 train no.404320  loss = 2.59128 avg_loss = 3.00748\n",
      "epoch no.5 train no.404330  loss = 3.24809 avg_loss = 3.00965\n",
      "epoch no.5 train no.404340  loss = 2.03402 avg_loss = 2.97410\n",
      "epoch no.5 train no.404350  loss = 2.66049 avg_loss = 2.95974\n",
      "epoch no.5 train no.404360  loss = 1.90045 avg_loss = 2.97361\n",
      "epoch no.5 train no.404370  loss = 4.03141 avg_loss = 2.98496\n",
      "epoch no.5 train no.404380  loss = 2.99875 avg_loss = 2.99090\n",
      "epoch no.5 train no.404390  loss = 2.38904 avg_loss = 2.96996\n",
      "epoch no.5 train no.404400  loss = 3.08726 avg_loss = 2.99838\n",
      "epoch no.5 train no.404410  loss = 3.50723 avg_loss = 2.99438\n",
      "epoch no.5 train no.404420  loss = 4.17323 avg_loss = 3.01970\n",
      "epoch no.5 train no.404430  loss = 2.83608 avg_loss = 3.00453\n",
      "epoch no.5 train no.404440  loss = 2.44954 avg_loss = 2.99395\n",
      "epoch no.5 train no.404450  loss = 2.62030 avg_loss = 3.00056\n",
      "epoch no.5 train no.404460  loss = 2.34170 avg_loss = 2.97123\n",
      "epoch no.5 train no.404470  loss = 3.30666 avg_loss = 2.97412\n",
      "epoch no.5 train no.404480  loss = 2.22170 avg_loss = 3.01605\n",
      "epoch no.5 train no.404490  loss = 2.50473 avg_loss = 3.05282\n",
      "epoch no.5 train no.404500  loss = 1.66148 avg_loss = 3.03326\n",
      "epoch no.5 train no.404510  loss = 2.96674 avg_loss = 3.04461\n",
      "epoch no.5 train no.404520  loss = 2.69998 avg_loss = 3.03724\n",
      "epoch no.5 train no.404530  loss = 3.94305 avg_loss = 3.02621\n",
      "epoch no.5 train no.404540  loss = 4.43483 avg_loss = 3.01351\n",
      "epoch no.5 train no.404550  loss = 2.80435 avg_loss = 3.00183\n",
      "epoch no.5 train no.404560  loss = 3.57071 avg_loss = 3.00755\n",
      "epoch no.5 train no.404570  loss = 2.60296 avg_loss = 3.00660\n",
      "epoch no.5 train no.404580  loss = 3.91669 avg_loss = 3.01269\n",
      "epoch no.5 train no.404590  loss = 4.11511 avg_loss = 3.02867\n",
      "epoch no.5 train no.404600  loss = 3.71499 avg_loss = 3.04653\n",
      "epoch no.5 train no.404610  loss = 4.00465 avg_loss = 3.07264\n",
      "epoch no.5 train no.404620  loss = 3.00751 avg_loss = 3.03417\n",
      "epoch no.5 train no.404630  loss = 4.91544 avg_loss = 3.05316\n",
      "epoch no.5 train no.404640  loss = 2.68645 avg_loss = 3.06281\n",
      "epoch no.5 train no.404650  loss = 2.45724 avg_loss = 3.05461\n",
      "epoch no.5 train no.404660  loss = 2.48436 avg_loss = 3.05515\n",
      "epoch no.5 train no.404670  loss = 3.31948 avg_loss = 3.04893\n",
      "epoch no.5 train no.404680  loss = 3.28786 avg_loss = 3.03804\n",
      "epoch no.5 train no.404690  loss = 3.57155 avg_loss = 3.05079\n",
      "epoch no.5 train no.404700  loss = 4.24194 avg_loss = 3.08638\n",
      "epoch no.5 train no.404710  loss = 2.97794 avg_loss = 3.06647\n",
      "epoch no.5 train no.404720  loss = 4.07723 avg_loss = 3.06080\n",
      "epoch no.5 train no.404730  loss = 2.61608 avg_loss = 3.03654\n",
      "epoch no.5 train no.404740  loss = 3.20577 avg_loss = 3.05565\n",
      "epoch no.5 train no.404750  loss = 1.44476 avg_loss = 3.02905\n",
      "epoch no.5 train no.404760  loss = 3.46415 avg_loss = 3.04715\n",
      "epoch no.5 train no.404770  loss = 3.28860 avg_loss = 3.07362\n",
      "epoch no.5 train no.404780  loss = 3.11876 avg_loss = 3.04195\n",
      "epoch no.5 train no.404790  loss = 2.69241 avg_loss = 3.05690\n",
      "epoch no.5 train no.404800  loss = 3.36266 avg_loss = 3.05486\n",
      "epoch no.5 train no.404810  loss = 2.42457 avg_loss = 3.06811\n",
      "epoch no.5 train no.404820  loss = 2.17710 avg_loss = 3.02147\n",
      "epoch no.5 train no.404830  loss = 2.32116 avg_loss = 2.99886\n",
      "epoch no.5 train no.404840  loss = 2.48620 avg_loss = 3.02455\n",
      "epoch no.5 train no.404850  loss = 3.09409 avg_loss = 3.02106\n",
      "epoch no.5 train no.404860  loss = 3.36762 avg_loss = 3.02912\n",
      "epoch no.5 train no.404870  loss = 3.18324 avg_loss = 3.04639\n",
      "epoch no.5 train no.404880  loss = 1.95183 avg_loss = 3.03950\n",
      "epoch no.5 train no.404890  loss = 4.47972 avg_loss = 3.05749\n",
      "epoch no.5 train no.404900  loss = 2.45605 avg_loss = 3.05662\n",
      "epoch no.5 train no.404910  loss = 3.94087 avg_loss = 3.11472\n",
      "epoch no.5 train no.404920  loss = 1.64750 avg_loss = 3.10781\n",
      "epoch no.5 train no.404930  loss = 4.68051 avg_loss = 3.11576\n",
      "epoch no.5 train no.404940  loss = 4.13640 avg_loss = 3.14656\n",
      "epoch no.5 train no.404950  loss = 2.56498 avg_loss = 3.13456\n",
      "epoch no.5 train no.404960  loss = 2.58736 avg_loss = 3.13345\n",
      "epoch no.5 train no.404970  loss = 5.19663 avg_loss = 3.16546\n",
      "epoch no.5 train no.404980  loss = 1.99630 avg_loss = 3.10932\n",
      "epoch no.5 train no.404990  loss = 5.20815 avg_loss = 3.10003\n",
      "epoch no.5 train no.405000  loss = 4.37438 avg_loss = 3.12790\n",
      "8\n",
      "to_tokens: ['▁가을', '밤', '을', '▁드라이브', '바', '에서', '</s>', '▁재즈', '음악', '</s>']\n",
      "여름밤의 재즈바에서 어울리는 재즈음악</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.405010  loss = 3.31002 avg_loss = 3.09463\n",
      "epoch no.5 train no.405020  loss = 2.25463 avg_loss = 3.06962\n",
      "epoch no.5 train no.405030  loss = 2.33434 avg_loss = 3.04307\n",
      "epoch no.5 train no.405040  loss = 3.06227 avg_loss = 3.05673\n",
      "epoch no.5 train no.405050  loss = 2.11455 avg_loss = 3.09544\n",
      "epoch no.5 train no.405060  loss = 2.68769 avg_loss = 3.09993\n",
      "epoch no.5 train no.405070  loss = 3.68105 avg_loss = 3.08830\n",
      "epoch no.5 train no.405080  loss = 2.21413 avg_loss = 3.03916\n",
      "epoch no.5 train no.405090  loss = 2.28346 avg_loss = 3.05621\n",
      "epoch no.5 train no.405100  loss = 2.70867 avg_loss = 3.07216\n",
      "epoch no.5 train no.405110  loss = 3.36279 avg_loss = 3.05418\n",
      "epoch no.5 train no.405120  loss = 2.70288 avg_loss = 3.03508\n",
      "epoch no.5 train no.405130  loss = 2.81897 avg_loss = 3.04049\n",
      "epoch no.5 train no.405140  loss = 4.36881 avg_loss = 3.06823\n",
      "epoch no.5 train no.405150  loss = 2.05553 avg_loss = 3.05311\n",
      "epoch no.5 train no.405160  loss = 2.46475 avg_loss = 3.02015\n",
      "epoch no.5 train no.405170  loss = 3.48281 avg_loss = 3.02729\n",
      "epoch no.5 train no.405180  loss = 4.63137 avg_loss = 3.03401\n",
      "epoch no.5 train no.405190  loss = 2.16553 avg_loss = 3.03205\n",
      "epoch no.5 train no.405200  loss = 1.75818 avg_loss = 3.01382\n",
      "epoch no.5 train no.405210  loss = 3.17275 avg_loss = 3.05995\n",
      "epoch no.5 train no.405220  loss = 2.33197 avg_loss = 3.06447\n",
      "epoch no.5 train no.405230  loss = 2.48051 avg_loss = 3.06996\n",
      "epoch no.5 train no.405240  loss = 2.44967 avg_loss = 3.08453\n",
      "epoch no.5 train no.405250  loss = 2.32745 avg_loss = 3.11308\n",
      "epoch no.5 train no.405260  loss = 2.77294 avg_loss = 3.10326\n",
      "epoch no.5 train no.405270  loss = 2.77515 avg_loss = 3.14101\n",
      "epoch no.5 train no.405280  loss = 2.71454 avg_loss = 3.11122\n",
      "epoch no.5 train no.405290  loss = 2.99640 avg_loss = 3.11594\n",
      "epoch no.5 train no.405300  loss = 3.46620 avg_loss = 3.09986\n",
      "epoch no.5 train no.405310  loss = 2.19950 avg_loss = 3.06573\n",
      "epoch no.5 train no.405320  loss = 2.10231 avg_loss = 3.09120\n",
      "epoch no.5 train no.405330  loss = 4.13019 avg_loss = 3.09842\n",
      "epoch no.5 train no.405340  loss = 3.02069 avg_loss = 3.09349\n",
      "epoch no.5 train no.405350  loss = 2.98322 avg_loss = 3.10803\n",
      "epoch no.5 train no.405360  loss = 1.76384 avg_loss = 3.12417\n",
      "epoch no.5 train no.405370  loss = 2.44612 avg_loss = 3.09438\n",
      "epoch no.5 train no.405380  loss = 4.12788 avg_loss = 3.08576\n",
      "epoch no.5 train no.405390  loss = 1.77145 avg_loss = 3.07377\n",
      "epoch no.5 train no.405400  loss = 3.97983 avg_loss = 3.08877\n",
      "epoch no.5 train no.405410  loss = 2.70840 avg_loss = 3.08765\n",
      "epoch no.5 train no.405420  loss = 3.05446 avg_loss = 3.10141\n",
      "epoch no.5 train no.405430  loss = 2.43363 avg_loss = 3.12451\n",
      "epoch no.5 train no.405440  loss = 3.44498 avg_loss = 3.11020\n",
      "epoch no.5 train no.405450  loss = 3.69710 avg_loss = 3.11960\n",
      "epoch no.5 train no.405460  loss = 3.65274 avg_loss = 3.14681\n",
      "epoch no.5 train no.405470  loss = 2.32953 avg_loss = 3.14038\n",
      "epoch no.5 train no.405480  loss = 3.97824 avg_loss = 3.18075\n",
      "epoch no.5 train no.405490  loss = 2.11981 avg_loss = 3.14839\n",
      "epoch no.5 train no.405500  loss = 2.86256 avg_loss = 3.14400\n",
      "epoch no.5 train no.405510  loss = 3.55877 avg_loss = 3.12112\n",
      "epoch no.5 train no.405520  loss = 2.76072 avg_loss = 3.10412\n",
      "epoch no.5 train no.405530  loss = 3.59506 avg_loss = 3.09464\n",
      "epoch no.5 train no.405540  loss = 2.69253 avg_loss = 3.09424\n",
      "epoch no.5 train no.405550  loss = 2.84787 avg_loss = 3.04969\n",
      "epoch no.5 train no.405560  loss = 2.14836 avg_loss = 3.04389\n",
      "epoch no.5 train no.405570  loss = 3.45209 avg_loss = 3.05280\n",
      "epoch no.5 train no.405580  loss = 2.85769 avg_loss = 3.04422\n",
      "epoch no.5 train no.405590  loss = 2.11454 avg_loss = 3.03272\n",
      "epoch no.5 train no.405600  loss = 2.39923 avg_loss = 3.04649\n",
      "epoch no.5 train no.405610  loss = 2.72998 avg_loss = 2.99315\n",
      "epoch no.5 train no.405620  loss = 2.83004 avg_loss = 2.98038\n",
      "epoch no.5 train no.405630  loss = 3.36699 avg_loss = 2.97574\n",
      "epoch no.5 train no.405640  loss = 3.67814 avg_loss = 2.98081\n",
      "epoch no.5 train no.405650  loss = 2.51782 avg_loss = 2.94112\n",
      "epoch no.5 train no.405660  loss = 2.66624 avg_loss = 2.99946\n",
      "epoch no.5 train no.405670  loss = 1.63127 avg_loss = 2.97025\n",
      "epoch no.5 train no.405680  loss = 2.97750 avg_loss = 3.00548\n",
      "epoch no.5 train no.405690  loss = 2.28331 avg_loss = 3.01317\n",
      "epoch no.5 train no.405700  loss = 4.19879 avg_loss = 3.02143\n",
      "epoch no.5 train no.405710  loss = 2.94524 avg_loss = 3.02222\n",
      "epoch no.5 train no.405720  loss = 3.01272 avg_loss = 3.04051\n",
      "epoch no.5 train no.405730  loss = 3.70258 avg_loss = 3.06266\n",
      "epoch no.5 train no.405740  loss = 2.83843 avg_loss = 3.05971\n",
      "epoch no.5 train no.405750  loss = 2.70570 avg_loss = 3.05946\n",
      "epoch no.5 train no.405760  loss = 1.64100 avg_loss = 3.05600\n",
      "epoch no.5 train no.405770  loss = 4.08602 avg_loss = 3.05571\n",
      "epoch no.5 train no.405780  loss = 2.26812 avg_loss = 3.07180\n",
      "epoch no.5 train no.405790  loss = 2.65518 avg_loss = 3.08523\n",
      "epoch no.5 train no.405800  loss = 3.79847 avg_loss = 3.08930\n",
      "epoch no.5 train no.405810  loss = 3.53812 avg_loss = 3.10081\n",
      "epoch no.5 train no.405820  loss = 3.99525 avg_loss = 3.12098\n",
      "epoch no.5 train no.405830  loss = 2.76576 avg_loss = 3.12809\n",
      "epoch no.5 train no.405840  loss = 3.56400 avg_loss = 3.15162\n",
      "epoch no.5 train no.405850  loss = 3.89601 avg_loss = 3.11802\n",
      "epoch no.5 train no.405860  loss = 3.00701 avg_loss = 3.11928\n",
      "epoch no.5 train no.405870  loss = 2.53726 avg_loss = 3.09172\n",
      "epoch no.5 train no.405880  loss = 3.27607 avg_loss = 3.09640\n",
      "epoch no.5 train no.405890  loss = 2.71195 avg_loss = 3.07285\n",
      "epoch no.5 train no.405900  loss = 3.19933 avg_loss = 3.09954\n",
      "epoch no.5 train no.405910  loss = 3.17644 avg_loss = 3.07075\n",
      "epoch no.5 train no.405920  loss = 4.60077 avg_loss = 3.10847\n",
      "epoch no.5 train no.405930  loss = 2.04628 avg_loss = 3.11823\n",
      "epoch no.5 train no.405940  loss = 3.05364 avg_loss = 3.10954\n",
      "epoch no.5 train no.405950  loss = 3.37130 avg_loss = 3.07704\n",
      "epoch no.5 train no.405960  loss = 2.39211 avg_loss = 3.04746\n",
      "epoch no.5 train no.405970  loss = 2.97212 avg_loss = 3.05322\n",
      "epoch no.5 train no.405980  loss = 2.66449 avg_loss = 3.03070\n",
      "epoch no.5 train no.405990  loss = 3.37970 avg_loss = 3.03396\n",
      "epoch no.5 train no.406000  loss = 3.70532 avg_loss = 3.05654\n",
      "6\n",
      "to_tokens: ['▁가을', '밤', '에', '▁수', '놓을', '▁노래', '</s>', '</s>']\n",
      "여름밤을 수놓을 노래들</s>\n",
      "epoch no.5 train no.406010  loss = 3.85320 avg_loss = 3.07889\n",
      "epoch no.5 train no.406020  loss = 7.50222 avg_loss = 3.13553\n",
      "epoch no.5 train no.406030  loss = 2.63795 avg_loss = 3.13660\n",
      "epoch no.5 train no.406040  loss = 3.36011 avg_loss = 3.12115\n",
      "epoch no.5 train no.406050  loss = 1.77379 avg_loss = 3.06926\n",
      "epoch no.5 train no.406060  loss = 5.25844 avg_loss = 3.10511\n",
      "epoch no.5 train no.406070  loss = 3.16220 avg_loss = 3.07532\n",
      "epoch no.5 train no.406080  loss = 2.31596 avg_loss = 3.06140\n",
      "epoch no.5 train no.406090  loss = 2.77144 avg_loss = 3.05086\n",
      "epoch no.5 train no.406100  loss = 3.56802 avg_loss = 3.02310\n",
      "epoch no.5 train no.406110  loss = 3.50387 avg_loss = 3.02513\n",
      "epoch no.5 train no.406120  loss = 2.30983 avg_loss = 3.00215\n",
      "epoch no.5 train no.406130  loss = 3.38763 avg_loss = 2.99083\n",
      "epoch no.5 train no.406140  loss = 4.14480 avg_loss = 2.98118\n",
      "epoch no.5 train no.406150  loss = 2.18852 avg_loss = 2.94700\n",
      "epoch no.5 train no.406160  loss = 3.16702 avg_loss = 2.96057\n",
      "epoch no.5 train no.406170  loss = 3.11643 avg_loss = 2.97861\n",
      "epoch no.5 train no.406180  loss = 4.11588 avg_loss = 3.00223\n",
      "epoch no.5 train no.406190  loss = 3.46260 avg_loss = 3.03335\n",
      "epoch no.5 train no.406200  loss = 4.18344 avg_loss = 3.02421\n",
      "epoch no.5 train no.406210  loss = 4.46245 avg_loss = 3.09239\n",
      "epoch no.5 train no.406220  loss = 2.60069 avg_loss = 3.04392\n",
      "epoch no.5 train no.406230  loss = 3.20866 avg_loss = 3.08011\n",
      "epoch no.5 train no.406240  loss = 5.19747 avg_loss = 3.09267\n",
      "epoch no.5 train no.406250  loss = 2.32104 avg_loss = 3.07781\n",
      "epoch no.5 train no.406260  loss = 1.84230 avg_loss = 3.10099\n",
      "epoch no.5 train no.406270  loss = 3.53034 avg_loss = 3.10548\n",
      "epoch no.5 train no.406280  loss = 3.86165 avg_loss = 3.14517\n",
      "epoch no.5 train no.406290  loss = 3.01315 avg_loss = 3.08285\n",
      "epoch no.5 train no.406300  loss = 2.78068 avg_loss = 3.06907\n",
      "epoch no.5 train no.406310  loss = 3.55960 avg_loss = 3.09902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.406320  loss = 3.86321 avg_loss = 3.08538\n",
      "epoch no.5 train no.406330  loss = 1.62682 avg_loss = 3.08975\n",
      "epoch no.5 train no.406340  loss = 3.48291 avg_loss = 3.12713\n",
      "epoch no.5 train no.406350  loss = 2.39105 avg_loss = 3.13109\n",
      "epoch no.5 train no.406360  loss = 4.28876 avg_loss = 3.15680\n",
      "epoch no.5 train no.406370  loss = 3.67207 avg_loss = 3.16073\n",
      "epoch no.5 train no.406380  loss = 3.31096 avg_loss = 3.14380\n",
      "epoch no.5 train no.406390  loss = 2.87632 avg_loss = 3.15252\n",
      "epoch no.5 train no.406400  loss = 2.59772 avg_loss = 3.11570\n",
      "epoch no.5 train no.406410  loss = 2.86685 avg_loss = 3.05492\n",
      "epoch no.5 train no.406420  loss = 3.03103 avg_loss = 3.10813\n",
      "epoch no.5 train no.406430  loss = 2.59080 avg_loss = 3.06328\n",
      "epoch no.5 train no.406440  loss = 4.77111 avg_loss = 3.09176\n",
      "epoch no.5 train no.406450  loss = 3.66758 avg_loss = 3.11832\n",
      "epoch no.5 train no.406460  loss = 3.49550 avg_loss = 3.10003\n",
      "epoch no.5 train no.406470  loss = 3.89700 avg_loss = 3.09331\n",
      "epoch no.5 train no.406480  loss = 2.90511 avg_loss = 3.10402\n",
      "epoch no.5 train no.406490  loss = 2.21570 avg_loss = 3.06479\n",
      "epoch no.5 train no.406500  loss = 1.55588 avg_loss = 3.06833\n",
      "epoch no.5 train no.406510  loss = 4.06853 avg_loss = 3.07423\n",
      "epoch no.5 train no.406520  loss = 3.30357 avg_loss = 3.07224\n",
      "epoch no.5 train no.406530  loss = 3.66992 avg_loss = 3.06607\n",
      "epoch no.5 train no.406540  loss = 2.45220 avg_loss = 3.06958\n",
      "epoch no.5 train no.406550  loss = 1.16278 avg_loss = 2.99995\n",
      "epoch no.5 train no.406560  loss = 4.17510 avg_loss = 3.00008\n",
      "epoch no.5 train no.406570  loss = 2.84640 avg_loss = 3.01135\n",
      "epoch no.5 train no.406580  loss = 2.97478 avg_loss = 3.09182\n",
      "epoch no.5 train no.406590  loss = 3.89548 avg_loss = 3.09583\n",
      "epoch no.5 train no.406600  loss = 1.98022 avg_loss = 3.07607\n",
      "epoch no.5 train no.406610  loss = 2.25531 avg_loss = 3.08057\n",
      "epoch no.5 train no.406620  loss = 3.96260 avg_loss = 3.03753\n",
      "epoch no.5 train no.406630  loss = 1.77021 avg_loss = 3.03288\n",
      "epoch no.5 train no.406640  loss = 2.78229 avg_loss = 3.03560\n",
      "epoch no.5 train no.406650  loss = 3.03542 avg_loss = 3.06900\n",
      "epoch no.5 train no.406660  loss = 4.91946 avg_loss = 3.06336\n",
      "epoch no.5 train no.406670  loss = 2.13908 avg_loss = 3.04967\n",
      "epoch no.5 train no.406680  loss = 5.46325 avg_loss = 3.09878\n",
      "epoch no.5 train no.406690  loss = 3.06213 avg_loss = 3.08809\n",
      "epoch no.5 train no.406700  loss = 5.17761 avg_loss = 3.10512\n",
      "epoch no.5 train no.406710  loss = 4.65298 avg_loss = 3.10662\n",
      "epoch no.5 train no.406720  loss = 3.14985 avg_loss = 3.10252\n",
      "epoch no.5 train no.406730  loss = 2.39223 avg_loss = 3.11642\n",
      "epoch no.5 train no.406740  loss = 3.30931 avg_loss = 3.09816\n",
      "epoch no.5 train no.406750  loss = 1.77786 avg_loss = 3.08243\n",
      "epoch no.5 train no.406760  loss = 2.22976 avg_loss = 3.08533\n",
      "epoch no.5 train no.406770  loss = 3.03235 avg_loss = 3.07782\n",
      "epoch no.5 train no.406780  loss = 2.16946 avg_loss = 3.06697\n",
      "epoch no.5 train no.406790  loss = 2.70115 avg_loss = 3.03906\n",
      "epoch no.5 train no.406800  loss = 2.85593 avg_loss = 3.05416\n",
      "epoch no.5 train no.406810  loss = 4.06148 avg_loss = 3.06284\n",
      "epoch no.5 train no.406820  loss = 3.32444 avg_loss = 3.07144\n",
      "epoch no.5 train no.406830  loss = 2.71451 avg_loss = 3.11294\n",
      "epoch no.5 train no.406840  loss = 4.38781 avg_loss = 3.13307\n",
      "epoch no.5 train no.406850  loss = 2.24379 avg_loss = 3.09986\n",
      "epoch no.5 train no.406860  loss = 2.56381 avg_loss = 3.08211\n",
      "epoch no.5 train no.406870  loss = 2.68459 avg_loss = 3.05534\n",
      "epoch no.5 train no.406880  loss = 3.33729 avg_loss = 3.01123\n",
      "epoch no.5 train no.406890  loss = 2.66463 avg_loss = 3.01272\n",
      "epoch no.5 train no.406900  loss = 3.01161 avg_loss = 2.96461\n",
      "epoch no.5 train no.406910  loss = 3.33371 avg_loss = 2.98811\n",
      "epoch no.5 train no.406920  loss = 3.13790 avg_loss = 3.02281\n",
      "epoch no.5 train no.406930  loss = 1.83917 avg_loss = 3.02543\n",
      "epoch no.5 train no.406940  loss = 2.32648 avg_loss = 3.00815\n",
      "epoch no.5 train no.406950  loss = 1.84631 avg_loss = 3.00143\n",
      "epoch no.5 train no.406960  loss = 3.04129 avg_loss = 3.00563\n",
      "epoch no.5 train no.406970  loss = 2.49790 avg_loss = 3.01043\n",
      "epoch no.5 train no.406980  loss = 2.50463 avg_loss = 2.98681\n",
      "epoch no.5 train no.406990  loss = 3.20346 avg_loss = 2.95930\n",
      "epoch no.5 train no.407000  loss = 3.73241 avg_loss = 2.96073\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁그루', '▁있는', '▁재즈', '</s>']\n",
      "여름밤의 분위기 있는 재즈</s>\n",
      "epoch no.5 train no.407010  loss = 4.85213 avg_loss = 3.05458\n",
      "epoch no.5 train no.407020  loss = 3.31849 avg_loss = 3.04319\n",
      "epoch no.5 train no.407030  loss = 2.74345 avg_loss = 3.03843\n",
      "epoch no.5 train no.407040  loss = 3.27022 avg_loss = 3.02688\n",
      "epoch no.5 train no.407050  loss = 2.55550 avg_loss = 3.03405\n",
      "epoch no.5 train no.407060  loss = 2.65779 avg_loss = 3.03056\n",
      "epoch no.5 train no.407070  loss = 2.49856 avg_loss = 3.05661\n",
      "epoch no.5 train no.407080  loss = 2.37873 avg_loss = 3.07581\n",
      "epoch no.5 train no.407090  loss = 2.84778 avg_loss = 3.04021\n",
      "epoch no.5 train no.407100  loss = 3.29660 avg_loss = 3.03359\n",
      "epoch no.5 train no.407110  loss = 2.68552 avg_loss = 3.02381\n",
      "epoch no.5 train no.407120  loss = 3.73263 avg_loss = 3.03977\n",
      "epoch no.5 train no.407130  loss = 2.62774 avg_loss = 3.04233\n",
      "epoch no.5 train no.407140  loss = 3.82694 avg_loss = 3.04094\n",
      "epoch no.5 train no.407150  loss = 2.35236 avg_loss = 3.06008\n",
      "epoch no.5 train no.407160  loss = 3.29064 avg_loss = 3.07177\n",
      "epoch no.5 train no.407170  loss = 3.10091 avg_loss = 3.07820\n",
      "epoch no.5 train no.407180  loss = 1.78478 avg_loss = 3.03181\n",
      "epoch no.5 train no.407190  loss = 3.51217 avg_loss = 3.02755\n",
      "epoch no.5 train no.407200  loss = 3.01250 avg_loss = 3.03209\n",
      "epoch no.5 train no.407210  loss = 4.42786 avg_loss = 3.04513\n",
      "epoch no.5 train no.407220  loss = 2.67304 avg_loss = 3.02623\n",
      "epoch no.5 train no.407230  loss = 2.89033 avg_loss = 3.02281\n",
      "epoch no.5 train no.407240  loss = 3.81257 avg_loss = 3.00257\n",
      "epoch no.5 train no.407250  loss = 2.22648 avg_loss = 2.98156\n",
      "epoch no.5 train no.407260  loss = 2.95814 avg_loss = 2.99579\n",
      "epoch no.5 train no.407270  loss = 2.61638 avg_loss = 2.97331\n",
      "epoch no.5 train no.407280  loss = 4.32721 avg_loss = 2.98126\n",
      "epoch no.5 train no.407290  loss = 4.75239 avg_loss = 2.98311\n",
      "epoch no.5 train no.407300  loss = 1.96968 avg_loss = 3.01069\n",
      "epoch no.5 train no.407310  loss = 2.77605 avg_loss = 2.97870\n",
      "epoch no.5 train no.407320  loss = 3.36417 avg_loss = 2.97435\n",
      "epoch no.5 train no.407330  loss = 2.58067 avg_loss = 2.97019\n",
      "epoch no.5 train no.407340  loss = 3.03641 avg_loss = 2.97289\n",
      "epoch no.5 train no.407350  loss = 3.12804 avg_loss = 2.99165\n",
      "epoch no.5 train no.407360  loss = 3.06450 avg_loss = 3.02599\n",
      "epoch no.5 train no.407370  loss = 2.13912 avg_loss = 3.01140\n",
      "epoch no.5 train no.407380  loss = 2.05259 avg_loss = 3.03412\n",
      "epoch no.5 train no.407390  loss = 2.91306 avg_loss = 3.04574\n",
      "epoch no.5 train no.407400  loss = 2.69854 avg_loss = 3.02494\n",
      "epoch no.5 train no.407410  loss = 3.69960 avg_loss = 3.04765\n",
      "epoch no.5 train no.407420  loss = 3.63242 avg_loss = 3.07611\n",
      "epoch no.5 train no.407430  loss = 4.03554 avg_loss = 3.12812\n",
      "epoch no.5 train no.407440  loss = 3.28544 avg_loss = 3.16230\n",
      "epoch no.5 train no.407450  loss = 3.27496 avg_loss = 3.10946\n",
      "epoch no.5 train no.407460  loss = 2.28193 avg_loss = 3.11548\n",
      "epoch no.5 train no.407470  loss = 2.08665 avg_loss = 3.10452\n",
      "epoch no.5 train no.407480  loss = 3.41349 avg_loss = 3.10127\n",
      "epoch no.5 train no.407490  loss = 2.27046 avg_loss = 3.08580\n",
      "epoch no.5 train no.407500  loss = 2.55511 avg_loss = 3.09537\n",
      "epoch no.5 train no.407510  loss = 2.21609 avg_loss = 3.08404\n",
      "epoch no.5 train no.407520  loss = 2.53257 avg_loss = 3.08947\n",
      "epoch no.5 train no.407530  loss = 1.74679 avg_loss = 3.06770\n",
      "epoch no.5 train no.407540  loss = 3.95059 avg_loss = 3.07214\n",
      "epoch no.5 train no.407550  loss = 2.02275 avg_loss = 3.03485\n",
      "epoch no.5 train no.407560  loss = 4.49617 avg_loss = 3.05048\n",
      "epoch no.5 train no.407570  loss = 3.13526 avg_loss = 3.04525\n",
      "epoch no.5 train no.407580  loss = 3.10535 avg_loss = 3.06657\n",
      "epoch no.5 train no.407590  loss = 2.86688 avg_loss = 3.08593\n",
      "epoch no.5 train no.407600  loss = 2.82699 avg_loss = 3.07217\n",
      "epoch no.5 train no.407610  loss = 3.28000 avg_loss = 3.09578\n",
      "epoch no.5 train no.407620  loss = 3.08668 avg_loss = 3.06918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.407630  loss = 3.38098 avg_loss = 3.07796\n",
      "epoch no.5 train no.407640  loss = 2.72410 avg_loss = 3.04724\n",
      "epoch no.5 train no.407650  loss = 5.10335 avg_loss = 3.09484\n",
      "epoch no.5 train no.407660  loss = 2.40637 avg_loss = 3.09190\n",
      "epoch no.5 train no.407670  loss = 3.15128 avg_loss = 3.13158\n",
      "epoch no.5 train no.407680  loss = 2.77282 avg_loss = 3.12566\n",
      "epoch no.5 train no.407690  loss = 2.56136 avg_loss = 3.08596\n",
      "epoch no.5 train no.407700  loss = 4.78706 avg_loss = 3.13849\n",
      "epoch no.5 train no.407710  loss = 3.99661 avg_loss = 3.12611\n",
      "epoch no.5 train no.407720  loss = 4.24294 avg_loss = 3.13167\n",
      "epoch no.5 train no.407730  loss = 4.13309 avg_loss = 3.09864\n",
      "epoch no.5 train no.407740  loss = 2.10611 avg_loss = 3.07525\n",
      "epoch no.5 train no.407750  loss = 4.06754 avg_loss = 3.13328\n",
      "epoch no.5 train no.407760  loss = 2.88412 avg_loss = 3.10346\n",
      "epoch no.5 train no.407770  loss = 3.38093 avg_loss = 3.10348\n",
      "epoch no.5 train no.407780  loss = 3.41560 avg_loss = 3.05947\n",
      "epoch no.5 train no.407790  loss = 2.46957 avg_loss = 3.02321\n",
      "epoch no.5 train no.407800  loss = 4.32577 avg_loss = 3.06582\n",
      "epoch no.5 train no.407810  loss = 2.14114 avg_loss = 3.02335\n",
      "epoch no.5 train no.407820  loss = 4.70056 avg_loss = 3.03130\n",
      "epoch no.5 train no.407830  loss = 2.25760 avg_loss = 3.05516\n",
      "epoch no.5 train no.407840  loss = 2.97363 avg_loss = 3.06919\n",
      "epoch no.5 train no.407850  loss = 2.06773 avg_loss = 3.07681\n",
      "epoch no.5 train no.407860  loss = 2.02504 avg_loss = 3.07240\n",
      "epoch no.5 train no.407870  loss = 2.65418 avg_loss = 3.02526\n",
      "epoch no.5 train no.407880  loss = 3.28924 avg_loss = 3.06494\n",
      "epoch no.5 train no.407890  loss = 2.72920 avg_loss = 3.04485\n",
      "epoch no.5 train no.407900  loss = 2.83122 avg_loss = 3.03364\n",
      "epoch no.5 train no.407910  loss = 4.61501 avg_loss = 3.07891\n",
      "epoch no.5 train no.407920  loss = 2.45282 avg_loss = 3.06500\n",
      "epoch no.5 train no.407930  loss = 2.40416 avg_loss = 3.02711\n",
      "epoch no.5 train no.407940  loss = 3.40991 avg_loss = 3.03522\n",
      "epoch no.5 train no.407950  loss = 4.83116 avg_loss = 3.05724\n",
      "epoch no.5 train no.407960  loss = 4.24078 avg_loss = 3.03200\n",
      "epoch no.5 train no.407970  loss = 3.30302 avg_loss = 3.06555\n",
      "epoch no.5 train no.407980  loss = 2.90212 avg_loss = 3.02198\n",
      "epoch no.5 train no.407990  loss = 2.83488 avg_loss = 3.01827\n",
      "epoch no.5 train no.408000  loss = 2.38672 avg_loss = 3.00820\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '니까', '나는', '▁노래', '</s>']\n",
      "여름이 생각나는 노래</s>\n",
      "epoch no.5 train no.408010  loss = 3.77559 avg_loss = 3.06983\n",
      "epoch no.5 train no.408020  loss = 3.10995 avg_loss = 3.03692\n",
      "epoch no.5 train no.408030  loss = 2.76878 avg_loss = 3.04832\n",
      "epoch no.5 train no.408040  loss = 3.21736 avg_loss = 3.11767\n",
      "epoch no.5 train no.408050  loss = 1.72566 avg_loss = 3.09642\n",
      "epoch no.5 train no.408060  loss = 2.77661 avg_loss = 3.08197\n",
      "epoch no.5 train no.408070  loss = 4.05822 avg_loss = 3.10031\n",
      "epoch no.5 train no.408080  loss = 2.27027 avg_loss = 3.07930\n",
      "epoch no.5 train no.408090  loss = 3.46570 avg_loss = 3.06912\n",
      "epoch no.5 train no.408100  loss = 3.17052 avg_loss = 3.09509\n",
      "epoch no.5 train no.408110  loss = 2.94334 avg_loss = 3.10087\n",
      "epoch no.5 train no.408120  loss = 2.51453 avg_loss = 3.08174\n",
      "epoch no.5 train no.408130  loss = 3.43022 avg_loss = 3.09832\n",
      "epoch no.5 train no.408140  loss = 3.02140 avg_loss = 3.09567\n",
      "epoch no.5 train no.408150  loss = 2.52413 avg_loss = 3.09442\n",
      "epoch no.5 train no.408160  loss = 3.54272 avg_loss = 3.13006\n",
      "epoch no.5 train no.408170  loss = 3.83343 avg_loss = 3.14038\n",
      "epoch no.5 train no.408180  loss = 2.91376 avg_loss = 3.14118\n",
      "epoch no.5 train no.408190  loss = 1.88166 avg_loss = 3.09459\n",
      "epoch no.5 train no.408200  loss = 2.37894 avg_loss = 3.09399\n",
      "epoch no.5 train no.408210  loss = 4.09141 avg_loss = 3.09841\n",
      "epoch no.5 train no.408220  loss = 4.40563 avg_loss = 3.08816\n",
      "epoch no.5 train no.408230  loss = 3.96757 avg_loss = 3.09577\n",
      "epoch no.5 train no.408240  loss = 2.25749 avg_loss = 3.10451\n",
      "epoch no.5 train no.408250  loss = 3.31928 avg_loss = 3.05455\n",
      "epoch no.5 train no.408260  loss = 2.81122 avg_loss = 3.04266\n",
      "epoch no.5 train no.408270  loss = 3.48324 avg_loss = 3.07458\n",
      "epoch no.5 train no.408280  loss = 3.86793 avg_loss = 3.06810\n",
      "epoch no.5 train no.408290  loss = 2.38287 avg_loss = 3.04906\n",
      "epoch no.5 train no.408300  loss = 1.81644 avg_loss = 3.06742\n",
      "epoch no.5 train no.408310  loss = 3.25019 avg_loss = 3.05643\n",
      "epoch no.5 train no.408320  loss = 3.03829 avg_loss = 3.04286\n",
      "epoch no.5 train no.408330  loss = 3.41586 avg_loss = 3.08611\n",
      "epoch no.5 train no.408340  loss = 2.52190 avg_loss = 3.02960\n",
      "epoch no.5 train no.408350  loss = 4.01269 avg_loss = 3.05414\n",
      "epoch no.5 train no.408360  loss = 3.12015 avg_loss = 3.04861\n",
      "epoch no.5 train no.408370  loss = 2.39211 avg_loss = 3.04052\n",
      "epoch no.5 train no.408380  loss = 3.77509 avg_loss = 3.00891\n",
      "epoch no.5 train no.408390  loss = 3.84912 avg_loss = 3.01873\n",
      "epoch no.5 train no.408400  loss = 2.94117 avg_loss = 3.06730\n",
      "epoch no.5 train no.408410  loss = 4.07360 avg_loss = 3.04296\n",
      "epoch no.5 train no.408420  loss = 3.41820 avg_loss = 3.03500\n",
      "epoch no.5 train no.408430  loss = 2.38287 avg_loss = 2.99673\n",
      "epoch no.5 train no.408440  loss = 2.89201 avg_loss = 2.97987\n",
      "epoch no.5 train no.408450  loss = 3.96199 avg_loss = 2.98883\n",
      "epoch no.5 train no.408460  loss = 2.06743 avg_loss = 3.03211\n",
      "epoch no.5 train no.408470  loss = 4.11360 avg_loss = 3.02433\n",
      "epoch no.5 train no.408480  loss = 3.84880 avg_loss = 3.02267\n",
      "epoch no.5 train no.408490  loss = 3.13188 avg_loss = 3.02247\n",
      "epoch no.5 train no.408500  loss = 3.10705 avg_loss = 3.05513\n",
      "epoch no.5 train no.408510  loss = 3.20772 avg_loss = 3.03340\n",
      "epoch no.5 train no.408520  loss = 2.76856 avg_loss = 3.07561\n",
      "epoch no.5 train no.408530  loss = 2.54299 avg_loss = 3.04398\n",
      "epoch no.5 train no.408540  loss = 2.25900 avg_loss = 3.04215\n",
      "epoch no.5 train no.408550  loss = 2.75123 avg_loss = 3.09734\n",
      "epoch no.5 train no.408560  loss = 1.60860 avg_loss = 3.08064\n",
      "epoch no.5 train no.408570  loss = 3.64061 avg_loss = 3.06662\n",
      "epoch no.5 train no.408580  loss = 3.29464 avg_loss = 3.04457\n",
      "epoch no.5 train no.408590  loss = 3.28629 avg_loss = 3.06361\n",
      "epoch no.5 train no.408600  loss = 3.59626 avg_loss = 3.06637\n",
      "epoch no.5 train no.408610  loss = 2.40053 avg_loss = 3.08804\n",
      "epoch no.5 train no.408620  loss = 3.77375 avg_loss = 3.04887\n",
      "epoch no.5 train no.408630  loss = 2.85514 avg_loss = 3.06540\n",
      "epoch no.5 train no.408640  loss = 1.99872 avg_loss = 3.09092\n",
      "epoch no.5 train no.408650  loss = 2.66653 avg_loss = 3.09008\n",
      "epoch no.5 train no.408660  loss = 4.90369 avg_loss = 3.09938\n",
      "epoch no.5 train no.408670  loss = 2.75376 avg_loss = 3.10391\n",
      "epoch no.5 train no.408680  loss = 2.01420 avg_loss = 3.06975\n",
      "epoch no.5 train no.408690  loss = 1.73353 avg_loss = 3.02570\n",
      "epoch no.5 train no.408700  loss = 2.49993 avg_loss = 3.03071\n",
      "epoch no.5 train no.408710  loss = 2.93869 avg_loss = 3.01214\n",
      "epoch no.5 train no.408720  loss = 2.23450 avg_loss = 2.98553\n",
      "epoch no.5 train no.408730  loss = 3.23030 avg_loss = 3.03480\n",
      "epoch no.5 train no.408740  loss = 1.47509 avg_loss = 3.01103\n",
      "epoch no.5 train no.408750  loss = 1.97818 avg_loss = 2.97153\n",
      "epoch no.5 train no.408760  loss = 2.84786 avg_loss = 3.01954\n",
      "epoch no.5 train no.408770  loss = 2.51816 avg_loss = 3.02422\n",
      "epoch no.5 train no.408780  loss = 3.98406 avg_loss = 3.05127\n",
      "epoch no.5 train no.408790  loss = 3.90803 avg_loss = 3.04460\n",
      "epoch no.5 train no.408800  loss = 2.55379 avg_loss = 3.08331\n",
      "epoch no.5 train no.408810  loss = 2.06839 avg_loss = 3.10690\n",
      "epoch no.5 train no.408820  loss = 4.11002 avg_loss = 3.08745\n",
      "epoch no.5 train no.408830  loss = 3.33067 avg_loss = 3.04855\n",
      "epoch no.5 train no.408840  loss = 2.47944 avg_loss = 3.05146\n",
      "epoch no.5 train no.408850  loss = 2.36918 avg_loss = 3.03364\n",
      "epoch no.5 train no.408860  loss = 2.82347 avg_loss = 3.05420\n",
      "epoch no.5 train no.408870  loss = 2.30138 avg_loss = 3.01312\n",
      "epoch no.5 train no.408880  loss = 4.14672 avg_loss = 3.02076\n",
      "epoch no.5 train no.408890  loss = 3.15575 avg_loss = 3.01857\n",
      "epoch no.5 train no.408900  loss = 4.90647 avg_loss = 3.04957\n",
      "epoch no.5 train no.408910  loss = 2.76058 avg_loss = 3.03075\n",
      "epoch no.5 train no.408920  loss = 3.40452 avg_loss = 3.08578\n",
      "epoch no.5 train no.408930  loss = 3.33219 avg_loss = 3.05772\n",
      "epoch no.5 train no.408940  loss = 4.05854 avg_loss = 3.05568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.408950  loss = 3.71177 avg_loss = 3.10103\n",
      "epoch no.5 train no.408960  loss = 3.33537 avg_loss = 3.11194\n",
      "epoch no.5 train no.408970  loss = 2.74290 avg_loss = 3.09165\n",
      "epoch no.5 train no.408980  loss = 1.58821 avg_loss = 3.09222\n",
      "epoch no.5 train no.408990  loss = 2.25580 avg_loss = 3.09218\n",
      "epoch no.5 train no.409000  loss = 2.78936 avg_loss = 3.04172\n",
      "4\n",
      "to_tokens: ['▁비', '밤', '에', '▁감성', '보', '</s>']\n",
      "여름밤의 재즈바</s>\n",
      "epoch no.5 train no.409010  loss = 5.24320 avg_loss = 3.06747\n",
      "epoch no.5 train no.409020  loss = 2.96416 avg_loss = 3.06743\n",
      "epoch no.5 train no.409030  loss = 4.15234 avg_loss = 3.06113\n",
      "epoch no.5 train no.409040  loss = 1.95673 avg_loss = 3.07128\n",
      "epoch no.5 train no.409050  loss = 3.28943 avg_loss = 3.05081\n",
      "epoch no.5 train no.409060  loss = 3.35559 avg_loss = 3.07354\n",
      "epoch no.5 train no.409070  loss = 1.26810 avg_loss = 3.04810\n",
      "epoch no.5 train no.409080  loss = 2.38073 avg_loss = 3.02574\n",
      "epoch no.5 train no.409090  loss = 2.90597 avg_loss = 3.02225\n",
      "epoch no.5 train no.409100  loss = 2.60080 avg_loss = 3.03501\n",
      "epoch no.5 train no.409110  loss = 3.37502 avg_loss = 3.03886\n",
      "epoch no.5 train no.409120  loss = 3.36269 avg_loss = 3.03846\n",
      "epoch no.5 train no.409130  loss = 3.18738 avg_loss = 3.08124\n",
      "epoch no.5 train no.409140  loss = 1.93579 avg_loss = 3.10301\n",
      "epoch no.5 train no.409150  loss = 3.40697 avg_loss = 3.09263\n",
      "epoch no.5 train no.409160  loss = 1.92774 avg_loss = 3.09436\n",
      "epoch no.5 train no.409170  loss = 2.46269 avg_loss = 3.10448\n",
      "epoch no.5 train no.409180  loss = 2.63233 avg_loss = 3.10382\n",
      "epoch no.5 train no.409190  loss = 2.99442 avg_loss = 3.11545\n",
      "epoch no.5 train no.409200  loss = 2.82588 avg_loss = 3.09872\n",
      "epoch no.5 train no.409210  loss = 2.67542 avg_loss = 3.11919\n",
      "epoch no.5 train no.409220  loss = 3.60577 avg_loss = 3.12515\n",
      "epoch no.5 train no.409230  loss = 2.25999 avg_loss = 3.12074\n",
      "epoch no.5 train no.409240  loss = 2.25138 avg_loss = 3.08917\n",
      "epoch no.5 train no.409250  loss = 2.69718 avg_loss = 3.07464\n",
      "epoch no.5 train no.409260  loss = 2.65221 avg_loss = 3.08437\n",
      "epoch no.5 train no.409270  loss = 2.63418 avg_loss = 3.09752\n",
      "epoch no.5 train no.409280  loss = 1.90928 avg_loss = 3.09251\n",
      "epoch no.5 train no.409290  loss = 1.51465 avg_loss = 3.07072\n",
      "epoch no.5 train no.409300  loss = 5.10521 avg_loss = 3.06107\n",
      "epoch no.5 train no.409310  loss = 2.66376 avg_loss = 3.05672\n",
      "epoch no.5 train no.409320  loss = 2.87984 avg_loss = 3.06750\n",
      "epoch no.5 train no.409330  loss = 2.30480 avg_loss = 3.08098\n",
      "epoch no.5 train no.409340  loss = 5.33881 avg_loss = 3.12978\n",
      "epoch no.5 train no.409350  loss = 2.28693 avg_loss = 3.10252\n",
      "epoch no.5 train no.409360  loss = 2.15657 avg_loss = 3.06910\n",
      "epoch no.5 train no.409370  loss = 3.50340 avg_loss = 3.05802\n",
      "epoch no.5 train no.409380  loss = 3.36388 avg_loss = 3.06094\n",
      "epoch no.5 train no.409390  loss = 3.38163 avg_loss = 3.07154\n",
      "epoch no.5 train no.409400  loss = 4.35509 avg_loss = 3.05711\n",
      "epoch no.5 train no.409410  loss = 2.67487 avg_loss = 3.08074\n",
      "epoch no.5 train no.409420  loss = 3.09418 avg_loss = 3.06226\n",
      "epoch no.5 train no.409430  loss = 2.76405 avg_loss = 3.04817\n",
      "epoch no.5 train no.409440  loss = 3.43899 avg_loss = 3.04446\n",
      "epoch no.5 train no.409450  loss = 2.98724 avg_loss = 3.06454\n",
      "epoch no.5 train no.409460  loss = 3.39224 avg_loss = 3.07646\n",
      "epoch no.5 train no.409470  loss = 3.63227 avg_loss = 3.06863\n",
      "epoch no.5 train no.409480  loss = 2.46189 avg_loss = 3.07592\n",
      "epoch no.5 train no.409490  loss = 2.22870 avg_loss = 3.07221\n",
      "epoch no.5 train no.409500  loss = 4.44101 avg_loss = 3.07926\n",
      "epoch no.5 train no.409510  loss = 2.98624 avg_loss = 3.11537\n",
      "epoch no.5 train no.409520  loss = 2.33043 avg_loss = 3.09736\n",
      "epoch no.5 train no.409530  loss = 2.31755 avg_loss = 3.12006\n",
      "epoch no.5 train no.409540  loss = 3.64550 avg_loss = 3.14557\n",
      "epoch no.5 train no.409550  loss = 2.88266 avg_loss = 3.10710\n",
      "epoch no.5 train no.409560  loss = 1.53296 avg_loss = 3.05981\n",
      "epoch no.5 train no.409570  loss = 2.80377 avg_loss = 3.05706\n",
      "epoch no.5 train no.409580  loss = 3.23552 avg_loss = 3.04441\n",
      "epoch no.5 train no.409590  loss = 2.63955 avg_loss = 3.08107\n",
      "epoch no.5 train no.409600  loss = 2.28346 avg_loss = 3.06666\n",
      "epoch no.5 train no.409610  loss = 1.78571 avg_loss = 3.05146\n",
      "epoch no.5 train no.409620  loss = 2.36830 avg_loss = 3.05032\n",
      "epoch no.5 train no.409630  loss = 3.49603 avg_loss = 3.05057\n",
      "epoch no.5 train no.409640  loss = 2.89025 avg_loss = 3.07548\n",
      "epoch no.5 train no.409650  loss = 3.52898 avg_loss = 3.03437\n",
      "epoch no.5 train no.409660  loss = 3.37835 avg_loss = 3.02927\n",
      "epoch no.5 train no.409670  loss = 2.67558 avg_loss = 3.03987\n",
      "epoch no.5 train no.409680  loss = 5.13401 avg_loss = 3.10952\n",
      "epoch no.5 train no.409690  loss = 3.73712 avg_loss = 3.12116\n",
      "epoch no.5 train no.409700  loss = 3.87675 avg_loss = 3.16754\n",
      "epoch no.5 train no.409710  loss = 3.35197 avg_loss = 3.14551\n",
      "epoch no.5 train no.409720  loss = 3.33484 avg_loss = 3.19282\n",
      "epoch no.5 train no.409730  loss = 3.03114 avg_loss = 3.18657\n",
      "epoch no.5 train no.409740  loss = 3.90859 avg_loss = 3.16485\n",
      "epoch no.5 train no.409750  loss = 5.26293 avg_loss = 3.20275\n",
      "epoch no.5 train no.409760  loss = 3.70437 avg_loss = 3.19239\n",
      "epoch no.5 train no.409770  loss = 2.74444 avg_loss = 3.18796\n",
      "epoch no.5 train no.409780  loss = 4.86260 avg_loss = 3.22398\n",
      "epoch no.5 train no.409790  loss = 3.03695 avg_loss = 3.23090\n",
      "epoch no.5 train no.409800  loss = 3.22511 avg_loss = 3.21869\n",
      "epoch no.5 train no.409810  loss = 1.62349 avg_loss = 3.20239\n",
      "epoch no.5 train no.409820  loss = 3.75026 avg_loss = 3.23178\n",
      "epoch no.5 train no.409830  loss = 2.87605 avg_loss = 3.20043\n",
      "epoch no.5 train no.409840  loss = 3.57171 avg_loss = 3.19743\n",
      "epoch no.5 train no.409850  loss = 2.89664 avg_loss = 3.12253\n",
      "epoch no.5 train no.409860  loss = 3.12117 avg_loss = 3.07781\n",
      "epoch no.5 train no.409870  loss = 1.58539 avg_loss = 3.06136\n",
      "epoch no.5 train no.409880  loss = 3.32815 avg_loss = 3.06259\n",
      "epoch no.5 train no.409890  loss = 2.49943 avg_loss = 3.03803\n",
      "epoch no.5 train no.409900  loss = 2.72797 avg_loss = 3.04648\n",
      "epoch no.5 train no.409910  loss = 2.64238 avg_loss = 3.03372\n",
      "epoch no.5 train no.409920  loss = 2.72379 avg_loss = 3.03474\n",
      "epoch no.5 train no.409930  loss = 2.84946 avg_loss = 3.02830\n",
      "epoch no.5 train no.409940  loss = 3.50283 avg_loss = 3.01480\n",
      "epoch no.5 train no.409950  loss = 2.47921 avg_loss = 2.98356\n",
      "epoch no.5 train no.409960  loss = 2.45220 avg_loss = 2.98572\n",
      "epoch no.5 train no.409970  loss = 2.45885 avg_loss = 2.98220\n",
      "epoch no.5 train no.409980  loss = 2.71065 avg_loss = 3.00269\n",
      "epoch no.5 train no.409990  loss = 2.34124 avg_loss = 2.98260\n",
      "epoch no.5 train no.410000  loss = 2.10590 avg_loss = 3.00319\n",
      "6\n",
      "to_tokens: ['▁비', '밤', '을', '▁수', '줄', '▁감성', '음악', '</s>']\n",
      "여름밤을 달래줄 인디음악</s>\n",
      "epoch no.5 train no.410010  loss = 3.64876 avg_loss = 3.02718\n",
      "epoch no.5 train no.410020  loss = 3.07978 avg_loss = 3.00297\n",
      "epoch no.5 train no.410030  loss = 4.08547 avg_loss = 3.07622\n",
      "epoch no.5 train no.410040  loss = 2.54211 avg_loss = 3.05568\n",
      "epoch no.5 train no.410050  loss = 3.37819 avg_loss = 3.04675\n",
      "epoch no.5 train no.410060  loss = 3.36036 avg_loss = 3.03127\n",
      "epoch no.5 train no.410070  loss = 3.70144 avg_loss = 3.03251\n",
      "epoch no.5 train no.410080  loss = 2.45434 avg_loss = 2.98785\n",
      "epoch no.5 train no.410090  loss = 4.54211 avg_loss = 2.97734\n",
      "epoch no.5 train no.410100  loss = 2.79126 avg_loss = 3.04756\n",
      "epoch no.5 train no.410110  loss = 2.83350 avg_loss = 3.01902\n",
      "epoch no.5 train no.410120  loss = 2.00177 avg_loss = 3.01721\n",
      "epoch no.5 train no.410130  loss = 2.26283 avg_loss = 3.06320\n",
      "epoch no.5 train no.410140  loss = 2.95071 avg_loss = 3.08060\n",
      "epoch no.5 train no.410150  loss = 2.11868 avg_loss = 3.05998\n",
      "epoch no.5 train no.410160  loss = 2.52299 avg_loss = 3.07786\n",
      "epoch no.5 train no.410170  loss = 5.13754 avg_loss = 3.11204\n",
      "epoch no.5 train no.410180  loss = 3.49802 avg_loss = 3.13749\n",
      "epoch no.5 train no.410190  loss = 3.41924 avg_loss = 3.12176\n",
      "epoch no.5 train no.410200  loss = 1.90928 avg_loss = 3.12643\n",
      "epoch no.5 train no.410210  loss = 2.14624 avg_loss = 3.12809\n",
      "epoch no.5 train no.410220  loss = 2.03912 avg_loss = 3.11980\n",
      "epoch no.5 train no.410230  loss = 3.84187 avg_loss = 3.14186\n",
      "epoch no.5 train no.410240  loss = 2.42534 avg_loss = 3.13641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.410250  loss = 2.74614 avg_loss = 3.10339\n",
      "epoch no.5 train no.410260  loss = 2.11141 avg_loss = 3.05753\n",
      "epoch no.5 train no.410270  loss = 2.10233 avg_loss = 3.05281\n",
      "epoch no.5 train no.410280  loss = 3.14838 avg_loss = 3.05032\n",
      "epoch no.5 train no.410290  loss = 1.81876 avg_loss = 3.08649\n",
      "epoch no.5 train no.410300  loss = 2.38600 avg_loss = 3.05081\n",
      "epoch no.5 train no.410310  loss = 2.42079 avg_loss = 3.02008\n",
      "epoch no.5 train no.410320  loss = 2.29925 avg_loss = 3.02825\n",
      "epoch no.5 train no.410330  loss = 2.81929 avg_loss = 3.01469\n",
      "epoch no.5 train no.410340  loss = 2.38145 avg_loss = 3.02929\n",
      "epoch no.5 train no.410350  loss = 2.34909 avg_loss = 3.02892\n",
      "epoch no.5 train no.410360  loss = 2.29083 avg_loss = 2.99826\n",
      "epoch no.5 train no.410370  loss = 3.41407 avg_loss = 2.99834\n",
      "epoch no.5 train no.410380  loss = 2.96195 avg_loss = 3.05356\n",
      "epoch no.5 train no.410390  loss = 3.46487 avg_loss = 3.06188\n",
      "epoch no.5 train no.410400  loss = 3.31956 avg_loss = 3.04574\n",
      "epoch no.5 train no.410410  loss = 2.77591 avg_loss = 3.05753\n",
      "epoch no.5 train no.410420  loss = 3.39152 avg_loss = 3.05673\n",
      "epoch no.5 train no.410430  loss = 2.55806 avg_loss = 3.01964\n",
      "epoch no.5 train no.410440  loss = 3.43576 avg_loss = 2.99910\n",
      "epoch no.5 train no.410450  loss = 3.29334 avg_loss = 2.98627\n",
      "epoch no.5 train no.410460  loss = 2.74309 avg_loss = 3.01236\n",
      "epoch no.5 train no.410470  loss = 2.32042 avg_loss = 3.02748\n",
      "epoch no.5 train no.410480  loss = 3.08209 avg_loss = 3.04066\n",
      "epoch no.5 train no.410490  loss = 2.21697 avg_loss = 3.06787\n",
      "epoch no.5 train no.410500  loss = 1.92412 avg_loss = 3.03171\n",
      "epoch no.5 train no.410510  loss = 2.51661 avg_loss = 3.07388\n",
      "epoch no.5 train no.410520  loss = 3.99815 avg_loss = 3.11511\n",
      "epoch no.5 train no.410530  loss = 4.11590 avg_loss = 3.08208\n",
      "epoch no.5 train no.410540  loss = 2.79069 avg_loss = 3.09572\n",
      "epoch no.5 train no.410550  loss = 2.67782 avg_loss = 3.06545\n",
      "epoch no.5 train no.410560  loss = 1.54971 avg_loss = 3.01711\n",
      "epoch no.5 train no.410570  loss = 3.42918 avg_loss = 3.02824\n",
      "epoch no.5 train no.410580  loss = 2.23535 avg_loss = 3.00276\n",
      "epoch no.5 train no.410590  loss = 4.08866 avg_loss = 3.01699\n",
      "epoch no.5 train no.410600  loss = 4.36275 avg_loss = 3.07857\n",
      "epoch no.5 train no.410610  loss = 3.74830 avg_loss = 3.11086\n",
      "epoch no.5 train no.410620  loss = 2.59523 avg_loss = 3.11066\n",
      "epoch no.5 train no.410630  loss = 2.74743 avg_loss = 3.09590\n",
      "epoch no.5 train no.410640  loss = 3.47089 avg_loss = 3.10558\n",
      "epoch no.5 train no.410650  loss = 2.54606 avg_loss = 3.10690\n",
      "epoch no.5 train no.410660  loss = 2.14605 avg_loss = 3.10467\n",
      "epoch no.5 train no.410670  loss = 3.16891 avg_loss = 3.04852\n",
      "epoch no.5 train no.410680  loss = 2.81248 avg_loss = 3.07915\n",
      "epoch no.5 train no.410690  loss = 1.80491 avg_loss = 3.09821\n",
      "epoch no.5 train no.410700  loss = 3.16146 avg_loss = 3.08443\n",
      "epoch no.5 train no.410710  loss = 2.54564 avg_loss = 3.08464\n",
      "epoch no.5 train no.410720  loss = 2.91602 avg_loss = 3.08768\n",
      "epoch no.5 train no.410730  loss = 2.69432 avg_loss = 3.07718\n",
      "epoch no.5 train no.410740  loss = 2.89170 avg_loss = 3.09044\n",
      "epoch no.5 train no.410750  loss = 3.13696 avg_loss = 3.12500\n",
      "epoch no.5 train no.410760  loss = 2.81615 avg_loss = 3.13768\n",
      "epoch no.5 train no.410770  loss = 2.99809 avg_loss = 3.13780\n",
      "epoch no.5 train no.410780  loss = 3.23883 avg_loss = 3.14087\n",
      "epoch no.5 train no.410790  loss = 2.37088 avg_loss = 3.10307\n",
      "epoch no.5 train no.410800  loss = 1.69198 avg_loss = 3.05291\n",
      "epoch no.5 train no.410810  loss = 3.24852 avg_loss = 3.04475\n",
      "epoch no.5 train no.410820  loss = 2.82560 avg_loss = 3.00517\n",
      "epoch no.5 train no.410830  loss = 3.36343 avg_loss = 3.01559\n",
      "epoch no.5 train no.410840  loss = 3.17384 avg_loss = 3.10901\n",
      "epoch no.5 train no.410850  loss = 2.67511 avg_loss = 3.11468\n",
      "epoch no.5 train no.410860  loss = 4.17402 avg_loss = 3.13497\n",
      "epoch no.5 train no.410870  loss = 2.86484 avg_loss = 3.14456\n",
      "epoch no.5 train no.410880  loss = 1.54475 avg_loss = 3.11740\n",
      "epoch no.5 train no.410890  loss = 4.41680 avg_loss = 3.15648\n",
      "epoch no.5 train no.410900  loss = 3.23521 avg_loss = 3.15362\n",
      "epoch no.5 train no.410910  loss = 2.36562 avg_loss = 3.14720\n",
      "epoch no.5 train no.410920  loss = 3.56863 avg_loss = 3.12176\n",
      "epoch no.5 train no.410930  loss = 2.23353 avg_loss = 3.09697\n",
      "epoch no.5 train no.410940  loss = 2.04318 avg_loss = 3.07626\n",
      "epoch no.5 train no.410950  loss = 3.44674 avg_loss = 3.06990\n",
      "epoch no.5 train no.410960  loss = 2.99917 avg_loss = 3.08285\n",
      "epoch no.5 train no.410970  loss = 1.77771 avg_loss = 3.09317\n",
      "epoch no.5 train no.410980  loss = 4.34504 avg_loss = 3.12374\n",
      "epoch no.5 train no.410990  loss = 2.93411 avg_loss = 3.09226\n",
      "epoch no.5 train no.411000  loss = 2.94632 avg_loss = 3.08797\n",
      "5\n",
      "to_tokens: ['▁비', '밤', '에', '▁듣는', '▁감성', '적인', '</s>']\n",
      "여름밤에 듣는 감성음악</s>\n",
      "epoch no.5 train no.411010  loss = 3.44863 avg_loss = 3.11852\n",
      "epoch no.5 train no.411020  loss = 2.52596 avg_loss = 3.10633\n",
      "epoch no.5 train no.411030  loss = 2.62050 avg_loss = 3.08911\n",
      "epoch no.5 train no.411040  loss = 3.75099 avg_loss = 3.09097\n",
      "epoch no.5 train no.411050  loss = 2.18245 avg_loss = 3.06459\n",
      "epoch no.5 train no.411060  loss = 6.79954 avg_loss = 3.13575\n",
      "epoch no.5 train no.411070  loss = 3.11838 avg_loss = 3.11653\n",
      "epoch no.5 train no.411080  loss = 4.42289 avg_loss = 3.06861\n",
      "epoch no.5 train no.411090  loss = 3.54368 avg_loss = 3.07245\n",
      "epoch no.5 train no.411100  loss = 2.34026 avg_loss = 3.06769\n",
      "epoch no.5 train no.411110  loss = 2.24368 avg_loss = 3.06010\n",
      "epoch no.5 train no.411120  loss = 2.12375 avg_loss = 3.06812\n",
      "epoch no.5 train no.411130  loss = 2.54349 avg_loss = 3.07143\n",
      "epoch no.5 train no.411140  loss = 3.09491 avg_loss = 3.06134\n",
      "epoch no.5 train no.411150  loss = 4.10347 avg_loss = 3.09124\n",
      "epoch no.5 train no.411160  loss = 2.77038 avg_loss = 3.10083\n",
      "epoch no.5 train no.411170  loss = 2.07170 avg_loss = 3.12937\n",
      "epoch no.5 train no.411180  loss = 3.70186 avg_loss = 3.14013\n",
      "epoch no.5 train no.411190  loss = 2.46180 avg_loss = 3.13881\n",
      "epoch no.5 train no.411200  loss = 1.68779 avg_loss = 3.13503\n",
      "epoch no.5 train no.411210  loss = 4.24189 avg_loss = 3.10370\n",
      "epoch no.5 train no.411220  loss = 2.96666 avg_loss = 3.09644\n",
      "epoch no.5 train no.411230  loss = 3.34169 avg_loss = 3.10487\n",
      "epoch no.5 train no.411240  loss = 2.25135 avg_loss = 3.12347\n",
      "epoch no.5 train no.411250  loss = 2.89787 avg_loss = 3.11432\n",
      "epoch no.5 train no.411260  loss = 2.94212 avg_loss = 3.11170\n",
      "epoch no.5 train no.411270  loss = 2.92745 avg_loss = 3.12016\n",
      "epoch no.5 train no.411280  loss = 3.21082 avg_loss = 3.13804\n",
      "epoch no.5 train no.411290  loss = 4.12248 avg_loss = 3.08006\n",
      "epoch no.5 train no.411300  loss = 4.69412 avg_loss = 3.13492\n",
      "epoch no.5 train no.411310  loss = 2.87924 avg_loss = 3.13100\n",
      "epoch no.5 train no.411320  loss = 3.28989 avg_loss = 3.12996\n",
      "epoch no.5 train no.411330  loss = 3.23206 avg_loss = 3.12956\n",
      "epoch no.5 train no.411340  loss = 4.61564 avg_loss = 3.13410\n",
      "epoch no.5 train no.411350  loss = 4.71353 avg_loss = 3.14627\n",
      "epoch no.5 train no.411360  loss = 1.55062 avg_loss = 3.08661\n",
      "epoch no.5 train no.411370  loss = 4.05483 avg_loss = 3.07641\n",
      "epoch no.5 train no.411380  loss = 4.26100 avg_loss = 3.05723\n",
      "epoch no.5 train no.411390  loss = 3.72164 avg_loss = 3.05921\n",
      "epoch no.5 train no.411400  loss = 4.01650 avg_loss = 3.10775\n",
      "epoch no.5 train no.411410  loss = 2.67802 avg_loss = 3.06464\n",
      "epoch no.5 train no.411420  loss = 3.80651 avg_loss = 3.08508\n",
      "epoch no.5 train no.411430  loss = 3.62589 avg_loss = 3.09191\n",
      "epoch no.5 train no.411440  loss = 2.62737 avg_loss = 3.10154\n",
      "epoch no.5 train no.411450  loss = 2.91927 avg_loss = 3.11282\n",
      "epoch no.5 train no.411460  loss = 2.35462 avg_loss = 3.16224\n",
      "epoch no.5 train no.411470  loss = 2.74724 avg_loss = 3.10818\n",
      "epoch no.5 train no.411480  loss = 3.31448 avg_loss = 3.08890\n",
      "epoch no.5 train no.411490  loss = 3.09874 avg_loss = 3.08690\n",
      "epoch no.5 train no.411500  loss = 2.49255 avg_loss = 3.07685\n",
      "epoch no.5 train no.411510  loss = 3.01284 avg_loss = 3.08868\n",
      "epoch no.5 train no.411520  loss = 2.05574 avg_loss = 3.08529\n",
      "epoch no.5 train no.411530  loss = 2.84205 avg_loss = 3.06303\n",
      "epoch no.5 train no.411540  loss = 2.39652 avg_loss = 3.03085\n",
      "epoch no.5 train no.411550  loss = 3.58825 avg_loss = 3.05469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.411560  loss = 2.43979 avg_loss = 3.05098\n",
      "epoch no.5 train no.411570  loss = 3.76056 avg_loss = 3.06330\n",
      "epoch no.5 train no.411580  loss = 2.76086 avg_loss = 3.03625\n",
      "epoch no.5 train no.411590  loss = 3.52071 avg_loss = 3.05860\n",
      "epoch no.5 train no.411600  loss = 2.63432 avg_loss = 3.01165\n",
      "epoch no.5 train no.411610  loss = 2.29799 avg_loss = 3.01546\n",
      "epoch no.5 train no.411620  loss = 5.28839 avg_loss = 3.03931\n",
      "epoch no.5 train no.411630  loss = 3.12578 avg_loss = 3.10914\n",
      "epoch no.5 train no.411640  loss = 2.77491 avg_loss = 3.14410\n",
      "epoch no.5 train no.411650  loss = 3.81334 avg_loss = 3.14516\n",
      "epoch no.5 train no.411660  loss = 2.30390 avg_loss = 3.15695\n",
      "epoch no.5 train no.411670  loss = 3.66792 avg_loss = 3.10768\n",
      "epoch no.5 train no.411680  loss = 2.91048 avg_loss = 3.11310\n",
      "epoch no.5 train no.411690  loss = 1.91752 avg_loss = 3.09769\n",
      "epoch no.5 train no.411700  loss = 2.50429 avg_loss = 3.12790\n",
      "epoch no.5 train no.411710  loss = 4.24716 avg_loss = 3.14205\n",
      "epoch no.5 train no.411720  loss = 2.88591 avg_loss = 3.11273\n",
      "epoch no.5 train no.411730  loss = 3.53111 avg_loss = 3.12947\n",
      "epoch no.5 train no.411740  loss = 4.41595 avg_loss = 3.14537\n",
      "epoch no.5 train no.411750  loss = 3.75478 avg_loss = 3.15412\n",
      "epoch no.5 train no.411760  loss = 2.55914 avg_loss = 3.16271\n",
      "epoch no.5 train no.411770  loss = 2.32701 avg_loss = 3.09403\n",
      "epoch no.5 train no.411780  loss = 2.28594 avg_loss = 3.05522\n",
      "epoch no.5 train no.411790  loss = 1.13850 avg_loss = 3.01762\n",
      "epoch no.5 train no.411800  loss = 3.96388 avg_loss = 3.07409\n",
      "epoch no.5 train no.411810  loss = 3.29836 avg_loss = 3.11379\n",
      "epoch no.5 train no.411820  loss = 3.17442 avg_loss = 3.08987\n",
      "epoch no.5 train no.411830  loss = 2.80494 avg_loss = 3.05656\n",
      "epoch no.5 train no.411840  loss = 2.29346 avg_loss = 3.04699\n",
      "epoch no.5 train no.411850  loss = 1.98879 avg_loss = 3.05743\n",
      "epoch no.5 train no.411860  loss = 1.67268 avg_loss = 3.04921\n",
      "epoch no.5 train no.411870  loss = 2.83462 avg_loss = 3.01409\n",
      "epoch no.5 train no.411880  loss = 3.11258 avg_loss = 2.98858\n",
      "epoch no.5 train no.411890  loss = 3.22616 avg_loss = 3.03130\n",
      "epoch no.5 train no.411900  loss = 2.45316 avg_loss = 2.97912\n",
      "epoch no.5 train no.411910  loss = 2.87727 avg_loss = 2.99009\n",
      "epoch no.5 train no.411920  loss = 4.61580 avg_loss = 3.03185\n",
      "epoch no.5 train no.411930  loss = 2.44215 avg_loss = 3.04219\n",
      "epoch no.5 train no.411940  loss = 3.50196 avg_loss = 3.04155\n",
      "epoch no.5 train no.411950  loss = 3.33354 avg_loss = 3.02652\n",
      "epoch no.5 train no.411960  loss = 3.61648 avg_loss = 3.02282\n",
      "epoch no.5 train no.411970  loss = 2.75533 avg_loss = 3.02169\n",
      "epoch no.5 train no.411980  loss = 1.78323 avg_loss = 3.02676\n",
      "epoch no.5 train no.411990  loss = 3.07808 avg_loss = 3.03059\n",
      "epoch no.5 train no.412000  loss = 2.47574 avg_loss = 3.00931\n",
      "Killed\n",
      "time : 24165.549039125443\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "!python3 main.py --epoch=200 --data_file_path=./data/train_2_space.csv --save_path=./checkpoint/1113cp/ --load_path=./checkpoint/KoGPT2_checkpoint_80000.tar --batch_size=1\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/minji/.local/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n",
      "2020-11-13 01:40:06.403787: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n",
      "2020-11-13 01:40:07.990429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2020-11-13 01:40:08.091754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:40:08.093747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:40:08.093821: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-13 01:40:08.096078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-13 01:40:08.098025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-13 01:40:08.098414: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-13 01:40:08.100797: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-13 01:40:08.102015: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-13 01:40:08.106916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-13 01:40:08.114991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-13 01:40:08.115416: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-13 01:40:08.143589: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3499910000 Hz\n",
      "2020-11-13 01:40:08.144358: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8f03110 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-13 01:40:08.144390: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-13 01:40:08.551375: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8f6ec60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-13 01:40:08.551417: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-13 01:40:08.551426: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-13 01:40:08.552754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:40:08.553957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-13 01:40:08.554058: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-13 01:40:08.554101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-13 01:40:08.554122: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-13 01:40:08.554142: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-13 01:40:08.554162: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-13 01:40:08.554181: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-13 01:40:08.554200: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-13 01:40:08.558125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-13 01:40:08.558223: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-13 01:40:10.220829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-11-13 01:40:10.220889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \n",
      "2020-11-13 01:40:10.220901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y \n",
      "2020-11-13 01:40:10.220908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N \n",
      "2020-11-13 01:40:10.224400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22418 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:65:00.0, compute capability: 7.5)\n",
      "2020-11-13 01:40:10.225984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22466 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:b3:00.0, compute capability: 7.5)\n",
      "using cached model\n",
      "using cached model\n",
      "0\n",
      "using cached model\n",
      "tokenizer ending\n",
      "(71885,)\n",
      "Read_Dataset ok\n",
      "KoGPT-2 Transfer Learning Start\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 218, in <module>\n",
      "    main(args.epoch, args.save_path, args.load_path, args.samples, args.data_file_path, args.batch_size)\n",
      "  File \"main.py\", line 158, in main\n",
      "    for data in data_loader:\n",
      "  File \"/home/minji/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/minji/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/minji/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/minji/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 82, in default_collate\n",
      "    raise RuntimeError('each element in list of batch should be of equal size')\n",
      "RuntimeError: each element in list of batch should be of equal size\n",
      "time : 35.0991415977478\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "!python3 main.py --epoch=200 --data_file_path=./data/train_2_space.csv --save_path=./checkpoint/1113cp/ --load_path=./checkpoint/KoGPT2_checkpoint_80000.tar --batch_size=8\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
