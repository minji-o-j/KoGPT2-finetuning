{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/minji/.local/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n",
      "2020-11-12 13:43:28.072011: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n",
      "2020-11-12 13:43:29.652679: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2020-11-12 13:43:29.754549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-12 13:43:29.756560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-12 13:43:29.756639: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-12 13:43:29.758960: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-12 13:43:29.760924: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-12 13:43:29.761320: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-12 13:43:29.763711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-12 13:43:29.764959: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-12 13:43:29.769941: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-12 13:43:29.777927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-12 13:43:29.778358: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-12 13:43:29.803587: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3499910000 Hz\n",
      "2020-11-12 13:43:29.804384: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x72dd3b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-12 13:43:29.804416: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-12 13:43:30.219572: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7c45520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-12 13:43:30.219614: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-12 13:43:30.219624: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-12 13:43:30.220952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-12 13:43:30.222025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-12 13:43:30.222108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-12 13:43:30.222144: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-12 13:43:30.222161: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-12 13:43:30.222178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-12 13:43:30.222193: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-12 13:43:30.222208: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-12 13:43:30.222224: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-12 13:43:30.226161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-12 13:43:30.226249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-12 13:43:31.897679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-11-12 13:43:31.897744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \n",
      "2020-11-12 13:43:31.897755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y \n",
      "2020-11-12 13:43:31.897763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N \n",
      "2020-11-12 13:43:31.901249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22418 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:65:00.0, compute capability: 7.5)\n",
      "2020-11-12 13:43:31.902812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22466 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:b3:00.0, compute capability: 7.5)\n",
      "using cached model\n",
      "using cached model\n",
      "0\n",
      "using cached model\n",
      "tokenizer ending\n",
      "(83903,)\n",
      "Read_Dataset ok\n",
      "KoGPT-2 Transfer Learning Start\n",
      "epoch no.0 train no.0  loss = 7.11098 avg_loss = 7.11098\n",
      "epoch no.0 train no.10  loss = 5.83350 avg_loss = 6.02656\n",
      "epoch no.0 train no.20  loss = 6.48071 avg_loss = 5.96396\n",
      "epoch no.0 train no.30  loss = 5.60156 avg_loss = 5.67268\n",
      "epoch no.0 train no.40  loss = 5.45818 avg_loss = 5.75543\n",
      "epoch no.0 train no.50  loss = 5.80392 avg_loss = 5.67761\n",
      "epoch no.0 train no.60  loss = 6.45861 avg_loss = 5.54374\n",
      "epoch no.0 train no.70  loss = 5.04110 avg_loss = 5.53974\n",
      "epoch no.0 train no.80  loss = 4.56555 avg_loss = 5.46144\n",
      "epoch no.0 train no.90  loss = 4.82395 avg_loss = 5.55680\n",
      "epoch no.0 train no.100  loss = 4.24839 avg_loss = 5.54274\n",
      "epoch no.0 train no.110  loss = 4.45276 avg_loss = 5.46946\n",
      "epoch no.0 train no.120  loss = 3.61804 avg_loss = 5.38465\n",
      "epoch no.0 train no.130  loss = 6.64005 avg_loss = 5.37792\n",
      "epoch no.0 train no.140  loss = 5.33033 avg_loss = 5.34904\n",
      "epoch no.0 train no.150  loss = 4.63507 avg_loss = 5.31581\n",
      "epoch no.0 train no.160  loss = 5.33985 avg_loss = 5.26127\n",
      "epoch no.0 train no.170  loss = 4.79297 avg_loss = 5.29699\n",
      "epoch no.0 train no.180  loss = 6.65156 avg_loss = 5.30320\n",
      "epoch no.0 train no.190  loss = 4.73799 avg_loss = 5.28692\n",
      "epoch no.0 train no.200  loss = 3.96576 avg_loss = 5.22387\n",
      "epoch no.0 train no.210  loss = 5.01177 avg_loss = 5.17335\n",
      "epoch no.0 train no.220  loss = 4.53276 avg_loss = 5.15017\n",
      "epoch no.0 train no.230  loss = 4.71025 avg_loss = 5.14702\n",
      "epoch no.0 train no.240  loss = 7.46988 avg_loss = 5.10639\n",
      "epoch no.0 train no.250  loss = 5.56819 avg_loss = 5.18154\n",
      "epoch no.0 train no.260  loss = 4.35965 avg_loss = 5.16773\n",
      "epoch no.0 train no.270  loss = 3.00256 avg_loss = 5.14275\n",
      "epoch no.0 train no.280  loss = 4.20382 avg_loss = 5.06719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.290  loss = 4.98073 avg_loss = 5.10146\n",
      "epoch no.0 train no.300  loss = 2.79118 avg_loss = 5.11241\n",
      "epoch no.0 train no.310  loss = 6.09574 avg_loss = 5.06033\n",
      "epoch no.0 train no.320  loss = 6.80649 avg_loss = 5.10248\n",
      "epoch no.0 train no.330  loss = 5.15263 avg_loss = 5.06898\n",
      "epoch no.0 train no.340  loss = 6.01551 avg_loss = 5.07881\n",
      "epoch no.0 train no.350  loss = 5.04903 avg_loss = 5.05102\n",
      "epoch no.0 train no.360  loss = 3.70390 avg_loss = 4.98844\n",
      "epoch no.0 train no.370  loss = 4.85695 avg_loss = 4.96366\n",
      "epoch no.0 train no.380  loss = 4.90877 avg_loss = 4.98079\n",
      "epoch no.0 train no.390  loss = 6.95880 avg_loss = 4.94161\n",
      "epoch no.0 train no.400  loss = 5.25512 avg_loss = 4.94645\n",
      "epoch no.0 train no.410  loss = 5.68550 avg_loss = 4.97818\n",
      "epoch no.0 train no.420  loss = 3.48788 avg_loss = 4.94302\n",
      "epoch no.0 train no.430  loss = 4.47479 avg_loss = 4.87143\n",
      "epoch no.0 train no.440  loss = 4.60490 avg_loss = 4.88414\n",
      "epoch no.0 train no.450  loss = 5.57057 avg_loss = 4.91728\n",
      "epoch no.0 train no.460  loss = 5.29624 avg_loss = 4.88084\n",
      "epoch no.0 train no.470  loss = 4.49554 avg_loss = 4.87567\n",
      "epoch no.0 train no.480  loss = 2.84036 avg_loss = 4.87452\n",
      "epoch no.0 train no.490  loss = 4.94841 avg_loss = 4.87187\n",
      "epoch no.0 train no.500  loss = 7.28497 avg_loss = 4.84874\n",
      "epoch no.0 train no.510  loss = 4.50957 avg_loss = 4.85294\n",
      "epoch no.0 train no.520  loss = 3.15187 avg_loss = 4.83637\n",
      "epoch no.0 train no.530  loss = 5.63984 avg_loss = 4.78772\n",
      "epoch no.0 train no.540  loss = 5.36780 avg_loss = 4.79501\n",
      "epoch no.0 train no.550  loss = 5.31451 avg_loss = 4.82013\n",
      "epoch no.0 train no.560  loss = 4.17870 avg_loss = 4.85677\n",
      "epoch no.0 train no.570  loss = 4.73252 avg_loss = 4.85170\n",
      "epoch no.0 train no.580  loss = 3.84702 avg_loss = 4.82679\n",
      "epoch no.0 train no.590  loss = 3.45481 avg_loss = 4.79640\n",
      "epoch no.0 train no.600  loss = 5.65567 avg_loss = 4.75803\n",
      "epoch no.0 train no.610  loss = 3.06646 avg_loss = 4.77625\n",
      "epoch no.0 train no.620  loss = 3.49927 avg_loss = 4.77368\n",
      "epoch no.0 train no.630  loss = 4.58453 avg_loss = 4.80037\n",
      "epoch no.0 train no.640  loss = 6.43857 avg_loss = 4.78797\n",
      "epoch no.0 train no.650  loss = 6.58065 avg_loss = 4.78508\n",
      "epoch no.0 train no.660  loss = 4.52682 avg_loss = 4.82276\n",
      "epoch no.0 train no.670  loss = 3.96961 avg_loss = 4.79911\n",
      "epoch no.0 train no.680  loss = 3.55463 avg_loss = 4.73413\n",
      "epoch no.0 train no.690  loss = 3.15867 avg_loss = 4.68309\n",
      "epoch no.0 train no.700  loss = 6.36995 avg_loss = 4.67928\n",
      "epoch no.0 train no.710  loss = 4.83949 avg_loss = 4.68420\n",
      "epoch no.0 train no.720  loss = 4.42253 avg_loss = 4.70494\n",
      "epoch no.0 train no.730  loss = 5.60759 avg_loss = 4.66515\n",
      "epoch no.0 train no.740  loss = 6.36062 avg_loss = 4.73875\n",
      "epoch no.0 train no.750  loss = 4.87311 avg_loss = 4.78907\n",
      "epoch no.0 train no.760  loss = 5.21404 avg_loss = 4.75498\n",
      "epoch no.0 train no.770  loss = 4.67232 avg_loss = 4.68567\n",
      "epoch no.0 train no.780  loss = 4.73202 avg_loss = 4.68957\n",
      "epoch no.0 train no.790  loss = 5.20624 avg_loss = 4.64176\n",
      "epoch no.0 train no.800  loss = 4.38964 avg_loss = 4.59635\n",
      "epoch no.0 train no.810  loss = 5.58126 avg_loss = 4.60839\n",
      "epoch no.0 train no.820  loss = 3.77480 avg_loss = 4.62190\n",
      "epoch no.0 train no.830  loss = 5.98967 avg_loss = 4.63088\n",
      "epoch no.0 train no.840  loss = 5.37437 avg_loss = 4.59313\n",
      "epoch no.0 train no.850  loss = 7.96033 avg_loss = 4.62888\n",
      "epoch no.0 train no.860  loss = 5.14241 avg_loss = 4.58071\n",
      "epoch no.0 train no.870  loss = 4.79706 avg_loss = 4.61655\n",
      "epoch no.0 train no.880  loss = 4.44931 avg_loss = 4.58959\n",
      "epoch no.0 train no.890  loss = 3.85554 avg_loss = 4.58914\n",
      "epoch no.0 train no.900  loss = 5.27465 avg_loss = 4.55536\n",
      "epoch no.0 train no.910  loss = 4.32181 avg_loss = 4.53660\n",
      "epoch no.0 train no.920  loss = 3.66641 avg_loss = 4.53995\n",
      "epoch no.0 train no.930  loss = 6.95640 avg_loss = 4.58858\n",
      "epoch no.0 train no.940  loss = 4.20188 avg_loss = 4.60780\n",
      "epoch no.0 train no.950  loss = 4.32455 avg_loss = 4.60378\n",
      "epoch no.0 train no.960  loss = 5.22252 avg_loss = 4.67152\n",
      "epoch no.0 train no.970  loss = 5.47532 avg_loss = 4.67744\n",
      "epoch no.0 train no.980  loss = 5.46904 avg_loss = 4.71113\n",
      "epoch no.0 train no.990  loss = 4.43226 avg_loss = 4.71850\n",
      "epoch no.0 train no.1000  loss = 5.27020 avg_loss = 4.71574\n",
      "1\n",
      "to_tokens: ['▁힐링', '▁좋은', '용', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.0 train no.1010  loss = 5.20052 avg_loss = 4.73355\n",
      "epoch no.0 train no.1020  loss = 4.16055 avg_loss = 4.70885\n",
      "epoch no.0 train no.1030  loss = 2.15980 avg_loss = 4.66806\n",
      "epoch no.0 train no.1040  loss = 3.57843 avg_loss = 4.61835\n",
      "epoch no.0 train no.1050  loss = 5.48992 avg_loss = 4.63774\n",
      "epoch no.0 train no.1060  loss = 4.72912 avg_loss = 4.63061\n",
      "epoch no.0 train no.1070  loss = 4.87197 avg_loss = 4.65447\n",
      "epoch no.0 train no.1080  loss = 6.08674 avg_loss = 4.69252\n",
      "epoch no.0 train no.1090  loss = 5.53428 avg_loss = 4.71436\n",
      "epoch no.0 train no.1100  loss = 3.59926 avg_loss = 4.73743\n",
      "epoch no.0 train no.1110  loss = 4.25222 avg_loss = 4.77100\n",
      "epoch no.0 train no.1120  loss = 6.30657 avg_loss = 4.81319\n",
      "epoch no.0 train no.1130  loss = 4.53012 avg_loss = 4.81288\n",
      "epoch no.0 train no.1140  loss = 4.93646 avg_loss = 4.77567\n",
      "epoch no.0 train no.1150  loss = 4.49912 avg_loss = 4.75504\n",
      "epoch no.0 train no.1160  loss = 4.04093 avg_loss = 4.80736\n",
      "epoch no.0 train no.1170  loss = 6.31629 avg_loss = 4.79779\n",
      "epoch no.0 train no.1180  loss = 4.86576 avg_loss = 4.75416\n",
      "epoch no.0 train no.1190  loss = 5.81440 avg_loss = 4.70823\n",
      "epoch no.0 train no.1200  loss = 5.94234 avg_loss = 4.70110\n",
      "epoch no.0 train no.1210  loss = 6.16677 avg_loss = 4.72117\n",
      "epoch no.0 train no.1220  loss = 4.21596 avg_loss = 4.77068\n",
      "epoch no.0 train no.1230  loss = 3.03144 avg_loss = 4.70247\n",
      "epoch no.0 train no.1240  loss = 5.83215 avg_loss = 4.69546\n",
      "epoch no.0 train no.1250  loss = 3.37754 avg_loss = 4.62718\n",
      "epoch no.0 train no.1260  loss = 3.05115 avg_loss = 4.61715\n",
      "epoch no.0 train no.1270  loss = 7.01721 avg_loss = 4.62921\n",
      "epoch no.0 train no.1280  loss = 5.83110 avg_loss = 4.69974\n",
      "epoch no.0 train no.1290  loss = 4.20940 avg_loss = 4.65385\n",
      "epoch no.0 train no.1300  loss = 5.97968 avg_loss = 4.68189\n",
      "epoch no.0 train no.1310  loss = 4.80736 avg_loss = 4.61633\n",
      "epoch no.0 train no.1320  loss = 6.20286 avg_loss = 4.65317\n",
      "epoch no.0 train no.1330  loss = 4.92754 avg_loss = 4.69572\n",
      "epoch no.0 train no.1340  loss = 3.66222 avg_loss = 4.71204\n",
      "epoch no.0 train no.1350  loss = 4.56304 avg_loss = 4.65110\n",
      "epoch no.0 train no.1360  loss = 3.49898 avg_loss = 4.56868\n",
      "epoch no.0 train no.1370  loss = 5.38491 avg_loss = 4.53914\n",
      "epoch no.0 train no.1380  loss = 4.87318 avg_loss = 4.57551\n",
      "epoch no.0 train no.1390  loss = 3.82266 avg_loss = 4.59898\n",
      "epoch no.0 train no.1400  loss = 3.60071 avg_loss = 4.64352\n",
      "epoch no.0 train no.1410  loss = 3.32210 avg_loss = 4.66314\n",
      "epoch no.0 train no.1420  loss = 3.77136 avg_loss = 4.63566\n",
      "epoch no.0 train no.1430  loss = 4.78184 avg_loss = 4.64979\n",
      "epoch no.0 train no.1440  loss = 4.22113 avg_loss = 4.66978\n",
      "epoch no.0 train no.1450  loss = 4.13149 avg_loss = 4.63206\n",
      "epoch no.0 train no.1460  loss = 3.73162 avg_loss = 4.60002\n",
      "epoch no.0 train no.1470  loss = 3.46871 avg_loss = 4.55958\n",
      "epoch no.0 train no.1480  loss = 2.90237 avg_loss = 4.53028\n",
      "epoch no.0 train no.1490  loss = 4.61277 avg_loss = 4.55669\n",
      "epoch no.0 train no.1500  loss = 4.39259 avg_loss = 4.51719\n",
      "epoch no.0 train no.1510  loss = 5.05657 avg_loss = 4.54971\n",
      "epoch no.0 train no.1520  loss = 3.23126 avg_loss = 4.53000\n",
      "epoch no.0 train no.1530  loss = 5.24653 avg_loss = 4.49836\n",
      "epoch no.0 train no.1540  loss = 3.86205 avg_loss = 4.47857\n",
      "epoch no.0 train no.1550  loss = 5.17164 avg_loss = 4.47976\n",
      "epoch no.0 train no.1560  loss = 3.34469 avg_loss = 4.43321\n",
      "epoch no.0 train no.1570  loss = 5.09201 avg_loss = 4.43762\n",
      "epoch no.0 train no.1580  loss = 3.84875 avg_loss = 4.41293\n",
      "epoch no.0 train no.1590  loss = 4.16862 avg_loss = 4.41367\n",
      "epoch no.0 train no.1600  loss = 4.05019 avg_loss = 4.44260\n",
      "epoch no.0 train no.1610  loss = 4.62692 avg_loss = 4.42520\n",
      "epoch no.0 train no.1620  loss = 4.95542 avg_loss = 4.44162\n",
      "epoch no.0 train no.1630  loss = 4.66572 avg_loss = 4.41320\n",
      "epoch no.0 train no.1640  loss = 2.93702 avg_loss = 4.37800\n",
      "epoch no.0 train no.1650  loss = 5.18005 avg_loss = 4.40156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.1660  loss = 3.45879 avg_loss = 4.42210\n",
      "epoch no.0 train no.1670  loss = 6.76188 avg_loss = 4.49738\n",
      "epoch no.0 train no.1680  loss = 2.85037 avg_loss = 4.48431\n",
      "epoch no.0 train no.1690  loss = 4.77307 avg_loss = 4.50737\n",
      "epoch no.0 train no.1700  loss = 6.97401 avg_loss = 4.54738\n",
      "epoch no.0 train no.1710  loss = 3.89009 avg_loss = 4.57972\n",
      "epoch no.0 train no.1720  loss = 3.82505 avg_loss = 4.60082\n",
      "epoch no.0 train no.1730  loss = 4.25378 avg_loss = 4.57573\n",
      "epoch no.0 train no.1740  loss = 3.99206 avg_loss = 4.55139\n",
      "epoch no.0 train no.1750  loss = 3.27837 avg_loss = 4.51621\n",
      "epoch no.0 train no.1760  loss = 1.83188 avg_loss = 4.50291\n",
      "epoch no.0 train no.1770  loss = 3.42507 avg_loss = 4.46791\n",
      "epoch no.0 train no.1780  loss = 5.16997 avg_loss = 4.44839\n",
      "epoch no.0 train no.1790  loss = 4.27837 avg_loss = 4.46262\n",
      "epoch no.0 train no.1800  loss = 3.31370 avg_loss = 4.51637\n",
      "epoch no.0 train no.1810  loss = 4.38494 avg_loss = 4.46567\n",
      "epoch no.0 train no.1820  loss = 4.24331 avg_loss = 4.44406\n",
      "epoch no.0 train no.1830  loss = 6.26698 avg_loss = 4.48782\n",
      "epoch no.0 train no.1840  loss = 4.14437 avg_loss = 4.44809\n",
      "epoch no.0 train no.1850  loss = 4.89399 avg_loss = 4.39865\n",
      "epoch no.0 train no.1860  loss = 4.99502 avg_loss = 4.38805\n",
      "epoch no.0 train no.1870  loss = 3.78097 avg_loss = 4.39570\n",
      "epoch no.0 train no.1880  loss = 3.60128 avg_loss = 4.39460\n",
      "epoch no.0 train no.1890  loss = 3.97024 avg_loss = 4.41879\n",
      "epoch no.0 train no.1900  loss = 5.28532 avg_loss = 4.45931\n",
      "epoch no.0 train no.1910  loss = 3.49619 avg_loss = 4.46980\n",
      "epoch no.0 train no.1920  loss = 4.28742 avg_loss = 4.43749\n",
      "epoch no.0 train no.1930  loss = 4.64057 avg_loss = 4.46523\n",
      "epoch no.0 train no.1940  loss = 4.91389 avg_loss = 4.50752\n",
      "epoch no.0 train no.1950  loss = 3.57938 avg_loss = 4.48506\n",
      "epoch no.0 train no.1960  loss = 4.39382 avg_loss = 4.44202\n",
      "epoch no.0 train no.1970  loss = 5.08140 avg_loss = 4.41940\n",
      "epoch no.0 train no.1980  loss = 7.24546 avg_loss = 4.50489\n",
      "epoch no.0 train no.1990  loss = 4.27202 avg_loss = 4.56802\n",
      "epoch no.0 train no.2000  loss = 4.87795 avg_loss = 4.60525\n",
      "3\n",
      "to_tokens: ['▁', '전환', '이', '▁위한', '▁노래', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.0 train no.2010  loss = 6.17513 avg_loss = 4.57889\n",
      "epoch no.0 train no.2020  loss = 7.07665 avg_loss = 4.57899\n",
      "epoch no.0 train no.2030  loss = 4.83633 avg_loss = 4.55951\n",
      "epoch no.0 train no.2040  loss = 2.30043 avg_loss = 4.55464\n",
      "epoch no.0 train no.2050  loss = 6.53813 avg_loss = 4.57170\n",
      "epoch no.0 train no.2060  loss = 4.63380 avg_loss = 4.55722\n",
      "epoch no.0 train no.2070  loss = 8.31541 avg_loss = 4.52003\n",
      "epoch no.0 train no.2080  loss = 3.32624 avg_loss = 4.46136\n",
      "epoch no.0 train no.2090  loss = 4.75074 avg_loss = 4.45164\n",
      "epoch no.0 train no.2100  loss = 2.90162 avg_loss = 4.39646\n",
      "epoch no.0 train no.2110  loss = 3.83175 avg_loss = 4.42280\n",
      "epoch no.0 train no.2120  loss = 3.23647 avg_loss = 4.37538\n",
      "epoch no.0 train no.2130  loss = 2.54430 avg_loss = 4.37750\n",
      "epoch no.0 train no.2140  loss = 2.73579 avg_loss = 4.33371\n",
      "epoch no.0 train no.2150  loss = 5.52944 avg_loss = 4.39787\n",
      "epoch no.0 train no.2160  loss = 3.80346 avg_loss = 4.39048\n",
      "epoch no.0 train no.2170  loss = 5.55419 avg_loss = 4.34486\n",
      "epoch no.0 train no.2180  loss = 5.60064 avg_loss = 4.37673\n",
      "epoch no.0 train no.2190  loss = 3.59936 avg_loss = 4.36802\n",
      "epoch no.0 train no.2200  loss = 4.94740 avg_loss = 4.38585\n",
      "epoch no.0 train no.2210  loss = 2.88553 avg_loss = 4.39257\n",
      "epoch no.0 train no.2220  loss = 5.91368 avg_loss = 4.45581\n",
      "epoch no.0 train no.2230  loss = 4.51148 avg_loss = 4.49455\n",
      "epoch no.0 train no.2240  loss = 2.72287 avg_loss = 4.43317\n",
      "epoch no.0 train no.2250  loss = 4.94807 avg_loss = 4.43234\n",
      "epoch no.0 train no.2260  loss = 6.19159 avg_loss = 4.47860\n",
      "epoch no.0 train no.2270  loss = 2.75613 avg_loss = 4.44665\n",
      "epoch no.0 train no.2280  loss = 4.51889 avg_loss = 4.43346\n",
      "epoch no.0 train no.2290  loss = 5.81190 avg_loss = 4.41565\n",
      "epoch no.0 train no.2300  loss = 5.62460 avg_loss = 4.42900\n",
      "epoch no.0 train no.2310  loss = 5.48661 avg_loss = 4.50401\n",
      "epoch no.0 train no.2320  loss = 4.43435 avg_loss = 4.47665\n",
      "epoch no.0 train no.2330  loss = 6.29747 avg_loss = 4.57235\n",
      "epoch no.0 train no.2340  loss = 2.54425 avg_loss = 4.48156\n",
      "epoch no.0 train no.2350  loss = 4.27859 avg_loss = 4.52543\n",
      "epoch no.0 train no.2360  loss = 3.03710 avg_loss = 4.49826\n",
      "epoch no.0 train no.2370  loss = 5.83662 avg_loss = 4.49878\n",
      "epoch no.0 train no.2380  loss = 5.32166 avg_loss = 4.47195\n",
      "epoch no.0 train no.2390  loss = 5.75073 avg_loss = 4.51057\n",
      "epoch no.0 train no.2400  loss = 5.42141 avg_loss = 4.49765\n",
      "epoch no.0 train no.2410  loss = 3.73558 avg_loss = 4.50034\n",
      "epoch no.0 train no.2420  loss = 4.60551 avg_loss = 4.54641\n",
      "epoch no.0 train no.2430  loss = 4.32977 avg_loss = 4.51301\n",
      "epoch no.0 train no.2440  loss = 6.16970 avg_loss = 4.50166\n",
      "epoch no.0 train no.2450  loss = 4.18757 avg_loss = 4.54493\n",
      "epoch no.0 train no.2460  loss = 5.44660 avg_loss = 4.54007\n",
      "epoch no.0 train no.2470  loss = 4.81070 avg_loss = 4.47706\n",
      "epoch no.0 train no.2480  loss = 4.22648 avg_loss = 4.46887\n",
      "epoch no.0 train no.2490  loss = 2.82598 avg_loss = 4.45452\n",
      "epoch no.0 train no.2500  loss = 2.79250 avg_loss = 4.45144\n",
      "epoch no.0 train no.2510  loss = 3.61202 avg_loss = 4.39939\n",
      "epoch no.0 train no.2520  loss = 7.96801 avg_loss = 4.44732\n",
      "epoch no.0 train no.2530  loss = 4.98487 avg_loss = 4.43579\n",
      "epoch no.0 train no.2540  loss = 2.51796 avg_loss = 4.38398\n",
      "epoch no.0 train no.2550  loss = 4.05591 avg_loss = 4.38953\n",
      "epoch no.0 train no.2560  loss = 5.88294 avg_loss = 4.38825\n",
      "epoch no.0 train no.2570  loss = 4.89663 avg_loss = 4.37076\n",
      "epoch no.0 train no.2580  loss = 4.18496 avg_loss = 4.37384\n",
      "epoch no.0 train no.2590  loss = 4.95376 avg_loss = 4.44661\n",
      "epoch no.0 train no.2600  loss = 4.13058 avg_loss = 4.40137\n",
      "epoch no.0 train no.2610  loss = 4.58870 avg_loss = 4.43214\n",
      "epoch no.0 train no.2620  loss = 2.64569 avg_loss = 4.41656\n",
      "epoch no.0 train no.2630  loss = 4.95873 avg_loss = 4.45585\n",
      "epoch no.0 train no.2640  loss = 3.98259 avg_loss = 4.41686\n",
      "epoch no.0 train no.2650  loss = 5.11079 avg_loss = 4.42562\n",
      "epoch no.0 train no.2660  loss = 6.00254 avg_loss = 4.45495\n",
      "epoch no.0 train no.2670  loss = 2.87185 avg_loss = 4.44384\n",
      "epoch no.0 train no.2680  loss = 4.06303 avg_loss = 4.49226\n",
      "epoch no.0 train no.2690  loss = 5.52437 avg_loss = 4.46440\n",
      "epoch no.0 train no.2700  loss = 5.26986 avg_loss = 4.46258\n",
      "epoch no.0 train no.2710  loss = 4.58000 avg_loss = 4.46297\n",
      "epoch no.0 train no.2720  loss = 3.50935 avg_loss = 4.45884\n",
      "epoch no.0 train no.2730  loss = 5.87982 avg_loss = 4.41808\n",
      "epoch no.0 train no.2740  loss = 3.52265 avg_loss = 4.39131\n",
      "epoch no.0 train no.2750  loss = 5.96969 avg_loss = 4.42870\n",
      "epoch no.0 train no.2760  loss = 4.11594 avg_loss = 4.46709\n",
      "epoch no.0 train no.2770  loss = 6.22459 avg_loss = 4.49508\n",
      "epoch no.0 train no.2780  loss = 5.81864 avg_loss = 4.50606\n",
      "epoch no.0 train no.2790  loss = 4.01315 avg_loss = 4.50383\n",
      "epoch no.0 train no.2800  loss = 4.49138 avg_loss = 4.47548\n",
      "epoch no.0 train no.2810  loss = 5.43890 avg_loss = 4.50595\n",
      "epoch no.0 train no.2820  loss = 3.24614 avg_loss = 4.48922\n",
      "epoch no.0 train no.2830  loss = 3.82942 avg_loss = 4.53176\n",
      "epoch no.0 train no.2840  loss = 3.59029 avg_loss = 4.52311\n",
      "epoch no.0 train no.2850  loss = 4.01334 avg_loss = 4.54616\n",
      "epoch no.0 train no.2860  loss = 5.61967 avg_loss = 4.55801\n",
      "epoch no.0 train no.2870  loss = 3.51291 avg_loss = 4.53343\n",
      "epoch no.0 train no.2880  loss = 3.68229 avg_loss = 4.54672\n",
      "epoch no.0 train no.2890  loss = 4.89039 avg_loss = 4.55718\n",
      "epoch no.0 train no.2900  loss = 4.93687 avg_loss = 4.54829\n",
      "epoch no.0 train no.2910  loss = 3.77088 avg_loss = 4.51240\n",
      "epoch no.0 train no.2920  loss = 6.21219 avg_loss = 4.51463\n",
      "epoch no.0 train no.2930  loss = 5.56279 avg_loss = 4.53252\n",
      "epoch no.0 train no.2940  loss = 5.72646 avg_loss = 4.51814\n",
      "epoch no.0 train no.2950  loss = 4.90971 avg_loss = 4.53396\n",
      "epoch no.0 train no.2960  loss = 4.24575 avg_loss = 4.52571\n",
      "epoch no.0 train no.2970  loss = 3.30025 avg_loss = 4.51124\n",
      "epoch no.0 train no.2980  loss = 4.40267 avg_loss = 4.53449\n",
      "epoch no.0 train no.2990  loss = 3.25281 avg_loss = 4.53329\n",
      "epoch no.0 train no.3000  loss = 4.63994 avg_loss = 4.50930\n",
      "4\n",
      "to_tokens: ['▁내가', '전환', '을', '▁좋은', '▁노래', '</s>', '</s>']\n",
      "기분전환에 좋은 클래식 모음</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.3010  loss = 2.40546 avg_loss = 4.49054\n",
      "epoch no.0 train no.3020  loss = 4.86923 avg_loss = 4.48309\n",
      "epoch no.0 train no.3030  loss = 3.93888 avg_loss = 4.51171\n",
      "epoch no.0 train no.3040  loss = 4.12686 avg_loss = 4.48898\n",
      "epoch no.0 train no.3050  loss = 3.96357 avg_loss = 4.45493\n",
      "epoch no.0 train no.3060  loss = 5.20901 avg_loss = 4.43210\n",
      "epoch no.0 train no.3070  loss = 2.19081 avg_loss = 4.40560\n",
      "epoch no.0 train no.3080  loss = 4.71322 avg_loss = 4.35673\n",
      "epoch no.0 train no.3090  loss = 7.65131 avg_loss = 4.42991\n",
      "epoch no.0 train no.3100  loss = 5.66713 avg_loss = 4.40836\n",
      "epoch no.0 train no.3110  loss = 4.29950 avg_loss = 4.39760\n",
      "epoch no.0 train no.3120  loss = 7.30962 avg_loss = 4.45183\n",
      "epoch no.0 train no.3130  loss = 3.39864 avg_loss = 4.39455\n",
      "epoch no.0 train no.3140  loss = 5.07854 avg_loss = 4.44664\n",
      "epoch no.0 train no.3150  loss = 4.27320 avg_loss = 4.46126\n",
      "epoch no.0 train no.3160  loss = 2.99804 avg_loss = 4.47490\n",
      "epoch no.0 train no.3170  loss = 3.70425 avg_loss = 4.47044\n",
      "epoch no.0 train no.3180  loss = 5.64037 avg_loss = 4.50965\n",
      "epoch no.0 train no.3190  loss = 6.67115 avg_loss = 4.51401\n",
      "epoch no.0 train no.3200  loss = 3.84816 avg_loss = 4.51377\n",
      "epoch no.0 train no.3210  loss = 3.82389 avg_loss = 4.57056\n",
      "epoch no.0 train no.3220  loss = 5.40924 avg_loss = 4.57225\n",
      "epoch no.0 train no.3230  loss = 4.66835 avg_loss = 4.67095\n",
      "epoch no.0 train no.3240  loss = 3.49074 avg_loss = 4.65041\n",
      "epoch no.0 train no.3250  loss = 6.36325 avg_loss = 4.70052\n",
      "epoch no.0 train no.3260  loss = 4.89849 avg_loss = 4.72974\n",
      "epoch no.0 train no.3270  loss = 5.04514 avg_loss = 4.68475\n",
      "epoch no.0 train no.3280  loss = 5.89827 avg_loss = 4.62697\n",
      "epoch no.0 train no.3290  loss = 2.19181 avg_loss = 4.57855\n",
      "epoch no.0 train no.3300  loss = 3.15037 avg_loss = 4.59491\n",
      "epoch no.0 train no.3310  loss = 4.29109 avg_loss = 4.54525\n",
      "epoch no.0 train no.3320  loss = 3.49422 avg_loss = 4.56510\n",
      "epoch no.0 train no.3330  loss = 4.11712 avg_loss = 4.60023\n",
      "epoch no.0 train no.3340  loss = 4.27389 avg_loss = 4.59923\n",
      "epoch no.0 train no.3350  loss = 5.37563 avg_loss = 4.58881\n",
      "epoch no.0 train no.3360  loss = 5.23758 avg_loss = 4.58663\n",
      "epoch no.0 train no.3370  loss = 2.87321 avg_loss = 4.54585\n",
      "epoch no.0 train no.3380  loss = 4.27842 avg_loss = 4.57819\n",
      "epoch no.0 train no.3390  loss = 3.47279 avg_loss = 4.54265\n",
      "epoch no.0 train no.3400  loss = 2.82607 avg_loss = 4.50710\n",
      "epoch no.0 train no.3410  loss = 4.59526 avg_loss = 4.51045\n",
      "epoch no.0 train no.3420  loss = 3.55047 avg_loss = 4.53928\n",
      "epoch no.0 train no.3430  loss = 3.11615 avg_loss = 4.46566\n",
      "epoch no.0 train no.3440  loss = 3.01557 avg_loss = 4.44273\n",
      "epoch no.0 train no.3450  loss = 3.63880 avg_loss = 4.38019\n",
      "epoch no.0 train no.3460  loss = 4.00275 avg_loss = 4.41801\n",
      "epoch no.0 train no.3470  loss = 4.57121 avg_loss = 4.40634\n",
      "epoch no.0 train no.3480  loss = 3.52869 avg_loss = 4.38853\n",
      "epoch no.0 train no.3490  loss = 5.51977 avg_loss = 4.38820\n",
      "epoch no.0 train no.3500  loss = 5.71705 avg_loss = 4.35766\n",
      "epoch no.0 train no.3510  loss = 3.30276 avg_loss = 4.37341\n",
      "epoch no.0 train no.3520  loss = 4.31746 avg_loss = 4.33846\n",
      "epoch no.0 train no.3530  loss = 2.80808 avg_loss = 4.31195\n",
      "epoch no.0 train no.3540  loss = 5.17886 avg_loss = 4.33624\n",
      "epoch no.0 train no.3550  loss = 4.64643 avg_loss = 4.28834\n",
      "epoch no.0 train no.3560  loss = 4.53190 avg_loss = 4.33058\n",
      "epoch no.0 train no.3570  loss = 3.17241 avg_loss = 4.33508\n",
      "epoch no.0 train no.3580  loss = 5.04724 avg_loss = 4.32157\n",
      "epoch no.0 train no.3590  loss = 5.39971 avg_loss = 4.34895\n",
      "epoch no.0 train no.3600  loss = 4.07541 avg_loss = 4.39587\n",
      "epoch no.0 train no.3610  loss = 4.31985 avg_loss = 4.40447\n",
      "epoch no.0 train no.3620  loss = 4.15158 avg_loss = 4.39822\n",
      "epoch no.0 train no.3630  loss = 5.46728 avg_loss = 4.39542\n",
      "epoch no.0 train no.3640  loss = 2.62600 avg_loss = 4.35413\n",
      "epoch no.0 train no.3650  loss = 4.58109 avg_loss = 4.36604\n",
      "epoch no.0 train no.3660  loss = 2.48561 avg_loss = 4.41731\n",
      "epoch no.0 train no.3670  loss = 5.48748 avg_loss = 4.42616\n",
      "epoch no.0 train no.3680  loss = 4.05824 avg_loss = 4.38829\n",
      "epoch no.0 train no.3690  loss = 5.75292 avg_loss = 4.51136\n",
      "epoch no.0 train no.3700  loss = 3.44454 avg_loss = 4.52502\n",
      "epoch no.0 train no.3710  loss = 3.40730 avg_loss = 4.51346\n",
      "epoch no.0 train no.3720  loss = 5.81598 avg_loss = 4.52520\n",
      "epoch no.0 train no.3730  loss = 2.25093 avg_loss = 4.50379\n",
      "epoch no.0 train no.3740  loss = 4.01937 avg_loss = 4.54389\n",
      "epoch no.0 train no.3750  loss = 4.75668 avg_loss = 4.53718\n",
      "epoch no.0 train no.3760  loss = 4.89339 avg_loss = 4.48990\n",
      "epoch no.0 train no.3770  loss = 4.37688 avg_loss = 4.47931\n",
      "epoch no.0 train no.3780  loss = 4.75326 avg_loss = 4.45973\n",
      "epoch no.0 train no.3790  loss = 3.76893 avg_loss = 4.40886\n",
      "epoch no.0 train no.3800  loss = 5.93315 avg_loss = 4.44397\n",
      "epoch no.0 train no.3810  loss = 4.17235 avg_loss = 4.47787\n",
      "epoch no.0 train no.3820  loss = 4.80349 avg_loss = 4.47457\n",
      "epoch no.0 train no.3830  loss = 4.76801 avg_loss = 4.51627\n",
      "epoch no.0 train no.3840  loss = 2.84220 avg_loss = 4.46328\n",
      "epoch no.0 train no.3850  loss = 4.21467 avg_loss = 4.46115\n",
      "epoch no.0 train no.3860  loss = 6.19776 avg_loss = 4.46732\n",
      "epoch no.0 train no.3870  loss = 4.48498 avg_loss = 4.52408\n",
      "epoch no.0 train no.3880  loss = 4.76087 avg_loss = 4.51617\n",
      "epoch no.0 train no.3890  loss = 3.66271 avg_loss = 4.52508\n",
      "epoch no.0 train no.3900  loss = 5.84550 avg_loss = 4.49648\n",
      "epoch no.0 train no.3910  loss = 5.59372 avg_loss = 4.52126\n",
      "epoch no.0 train no.3920  loss = 3.62616 avg_loss = 4.50833\n",
      "epoch no.0 train no.3930  loss = 4.15616 avg_loss = 4.55893\n",
      "epoch no.0 train no.3940  loss = 6.53741 avg_loss = 4.56573\n",
      "epoch no.0 train no.3950  loss = 5.93406 avg_loss = 4.55513\n",
      "epoch no.0 train no.3960  loss = 4.07973 avg_loss = 4.54711\n",
      "epoch no.0 train no.3970  loss = 5.44908 avg_loss = 4.56150\n",
      "epoch no.0 train no.3980  loss = 4.65562 avg_loss = 4.61605\n",
      "epoch no.0 train no.3990  loss = 2.41296 avg_loss = 4.55750\n",
      "epoch no.0 train no.4000  loss = 2.27673 avg_loss = 4.46094\n",
      "2\n",
      "to_tokens: ['▁', '전환', '이', '▁필요할', '</s>']\n",
      "기분전환이 필요해</s>\n",
      "epoch no.0 train no.4010  loss = 4.67337 avg_loss = 4.49853\n",
      "epoch no.0 train no.4020  loss = 4.06247 avg_loss = 4.54405\n",
      "epoch no.0 train no.4030  loss = 7.74162 avg_loss = 4.56379\n",
      "epoch no.0 train no.4040  loss = 3.34131 avg_loss = 4.56006\n",
      "epoch no.0 train no.4050  loss = 3.17636 avg_loss = 4.49034\n",
      "epoch no.0 train no.4060  loss = 4.57687 avg_loss = 4.43386\n",
      "epoch no.0 train no.4070  loss = 2.80071 avg_loss = 4.36993\n",
      "epoch no.0 train no.4080  loss = 5.35152 avg_loss = 4.35591\n",
      "epoch no.0 train no.4090  loss = 4.90831 avg_loss = 4.38872\n",
      "epoch no.0 train no.4100  loss = 3.90160 avg_loss = 4.37329\n",
      "epoch no.0 train no.4110  loss = 5.36126 avg_loss = 4.35614\n",
      "epoch no.0 train no.4120  loss = 4.09770 avg_loss = 4.35909\n",
      "epoch no.0 train no.4130  loss = 6.44464 avg_loss = 4.35090\n",
      "epoch no.0 train no.4140  loss = 2.97021 avg_loss = 4.33343\n",
      "epoch no.0 train no.4150  loss = 5.88863 avg_loss = 4.34091\n",
      "epoch no.0 train no.4160  loss = 3.98981 avg_loss = 4.42313\n",
      "epoch no.0 train no.4170  loss = 5.77221 avg_loss = 4.44383\n",
      "epoch no.0 train no.4180  loss = 3.53177 avg_loss = 4.41779\n",
      "epoch no.0 train no.4190  loss = 3.83615 avg_loss = 4.39321\n",
      "epoch no.0 train no.4200  loss = 5.92844 avg_loss = 4.44790\n",
      "epoch no.0 train no.4210  loss = 3.25741 avg_loss = 4.45220\n",
      "epoch no.0 train no.4220  loss = 3.48008 avg_loss = 4.41405\n",
      "epoch no.0 train no.4230  loss = 4.53488 avg_loss = 4.39149\n",
      "epoch no.0 train no.4240  loss = 6.34116 avg_loss = 4.41419\n",
      "epoch no.0 train no.4250  loss = 4.53255 avg_loss = 4.35402\n",
      "epoch no.0 train no.4260  loss = 2.75688 avg_loss = 4.31921\n",
      "epoch no.0 train no.4270  loss = 2.19533 avg_loss = 4.23869\n",
      "epoch no.0 train no.4280  loss = 4.34734 avg_loss = 4.22919\n",
      "epoch no.0 train no.4290  loss = 5.49943 avg_loss = 4.25178\n",
      "epoch no.0 train no.4300  loss = 3.38701 avg_loss = 4.24905\n",
      "epoch no.0 train no.4310  loss = 4.19386 avg_loss = 4.25016\n",
      "epoch no.0 train no.4320  loss = 6.75275 avg_loss = 4.28384\n",
      "epoch no.0 train no.4330  loss = 4.00442 avg_loss = 4.29902\n",
      "epoch no.0 train no.4340  loss = 4.17772 avg_loss = 4.28745\n",
      "epoch no.0 train no.4350  loss = 2.34241 avg_loss = 4.28844\n",
      "epoch no.0 train no.4360  loss = 2.83300 avg_loss = 4.23843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.4370  loss = 3.81912 avg_loss = 4.25470\n",
      "epoch no.0 train no.4380  loss = 4.58933 avg_loss = 4.28304\n",
      "epoch no.0 train no.4390  loss = 4.79971 avg_loss = 4.34942\n",
      "epoch no.0 train no.4400  loss = 3.30949 avg_loss = 4.30576\n",
      "epoch no.0 train no.4410  loss = 3.72601 avg_loss = 4.34455\n",
      "epoch no.0 train no.4420  loss = 4.22744 avg_loss = 4.36156\n",
      "epoch no.0 train no.4430  loss = 6.37962 avg_loss = 4.42008\n",
      "epoch no.0 train no.4440  loss = 5.07603 avg_loss = 4.42922\n",
      "epoch no.0 train no.4450  loss = 3.26413 avg_loss = 4.43727\n",
      "epoch no.0 train no.4460  loss = 4.56811 avg_loss = 4.41054\n",
      "epoch no.0 train no.4470  loss = 5.06733 avg_loss = 4.41607\n",
      "epoch no.0 train no.4480  loss = 5.91852 avg_loss = 4.33909\n",
      "epoch no.0 train no.4490  loss = 5.44480 avg_loss = 4.36599\n",
      "epoch no.0 train no.4500  loss = 6.46828 avg_loss = 4.35653\n",
      "epoch no.0 train no.4510  loss = 3.62885 avg_loss = 4.36937\n",
      "epoch no.0 train no.4520  loss = 3.66619 avg_loss = 4.35118\n",
      "epoch no.0 train no.4530  loss = 5.14460 avg_loss = 4.34552\n",
      "epoch no.0 train no.4540  loss = 2.91883 avg_loss = 4.33208\n",
      "epoch no.0 train no.4550  loss = 5.29750 avg_loss = 4.39239\n",
      "epoch no.0 train no.4560  loss = 2.83037 avg_loss = 4.44419\n",
      "epoch no.0 train no.4570  loss = 3.22789 avg_loss = 4.42861\n",
      "epoch no.0 train no.4580  loss = 4.11805 avg_loss = 4.40463\n",
      "epoch no.0 train no.4590  loss = 5.02238 avg_loss = 4.38828\n",
      "epoch no.0 train no.4600  loss = 4.26338 avg_loss = 4.37217\n",
      "epoch no.0 train no.4610  loss = 5.56944 avg_loss = 4.39018\n",
      "epoch no.0 train no.4620  loss = 6.88109 avg_loss = 4.34929\n",
      "epoch no.0 train no.4630  loss = 5.61315 avg_loss = 4.39601\n",
      "epoch no.0 train no.4640  loss = 3.06943 avg_loss = 4.36110\n",
      "epoch no.0 train no.4650  loss = 4.85653 avg_loss = 4.40304\n",
      "epoch no.0 train no.4660  loss = 3.45484 avg_loss = 4.37200\n",
      "epoch no.0 train no.4670  loss = 6.35376 avg_loss = 4.36044\n",
      "epoch no.0 train no.4680  loss = 3.76897 avg_loss = 4.38801\n",
      "epoch no.0 train no.4690  loss = 5.54820 avg_loss = 4.35707\n",
      "epoch no.0 train no.4700  loss = 5.32650 avg_loss = 4.33187\n",
      "epoch no.0 train no.4710  loss = 6.63994 avg_loss = 4.33144\n",
      "epoch no.0 train no.4720  loss = 4.80624 avg_loss = 4.35244\n",
      "epoch no.0 train no.4730  loss = 4.59609 avg_loss = 4.33633\n",
      "epoch no.0 train no.4740  loss = 3.05238 avg_loss = 4.29384\n",
      "epoch no.0 train no.4750  loss = 3.11774 avg_loss = 4.24979\n",
      "epoch no.0 train no.4760  loss = 6.85722 avg_loss = 4.21917\n",
      "epoch no.0 train no.4770  loss = 3.58073 avg_loss = 4.17996\n",
      "epoch no.0 train no.4780  loss = 5.02018 avg_loss = 4.25601\n",
      "epoch no.0 train no.4790  loss = 5.54483 avg_loss = 4.25902\n",
      "epoch no.0 train no.4800  loss = 4.59211 avg_loss = 4.26866\n",
      "epoch no.0 train no.4810  loss = 3.68211 avg_loss = 4.25610\n",
      "epoch no.0 train no.4820  loss = 7.27232 avg_loss = 4.30512\n",
      "epoch no.0 train no.4830  loss = 4.81496 avg_loss = 4.30627\n",
      "epoch no.0 train no.4840  loss = 5.00161 avg_loss = 4.29481\n",
      "epoch no.0 train no.4850  loss = 4.06699 avg_loss = 4.32266\n",
      "epoch no.0 train no.4860  loss = 3.68000 avg_loss = 4.32387\n",
      "epoch no.0 train no.4870  loss = 6.43550 avg_loss = 4.38337\n",
      "epoch no.0 train no.4880  loss = 3.32139 avg_loss = 4.36276\n",
      "epoch no.0 train no.4890  loss = 4.48400 avg_loss = 4.35593\n",
      "epoch no.0 train no.4900  loss = 6.73767 avg_loss = 4.41479\n",
      "epoch no.0 train no.4910  loss = 4.63815 avg_loss = 4.43526\n",
      "epoch no.0 train no.4920  loss = 4.62177 avg_loss = 4.41652\n",
      "epoch no.0 train no.4930  loss = 4.76823 avg_loss = 4.41139\n",
      "epoch no.0 train no.4940  loss = 5.61846 avg_loss = 4.41667\n",
      "epoch no.0 train no.4950  loss = 4.14043 avg_loss = 4.42013\n",
      "epoch no.0 train no.4960  loss = 6.87902 avg_loss = 4.43991\n",
      "epoch no.0 train no.4970  loss = 4.16305 avg_loss = 4.41458\n",
      "epoch no.0 train no.4980  loss = 4.61515 avg_loss = 4.39594\n",
      "epoch no.0 train no.4990  loss = 3.77625 avg_loss = 4.34082\n",
      "epoch no.0 train no.5000  loss = 5.19206 avg_loss = 4.33498\n",
      "4\n",
      "to_tokens: ['▁가을', '좋은', '이', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환할때 듣는 힙합</s>\n",
      "epoch no.0 train no.5010  loss = 5.71929 avg_loss = 4.35317\n",
      "epoch no.0 train no.5020  loss = 3.49766 avg_loss = 4.29988\n",
      "epoch no.0 train no.5030  loss = 2.37778 avg_loss = 4.25831\n",
      "epoch no.0 train no.5040  loss = 6.62585 avg_loss = 4.26762\n",
      "epoch no.0 train no.5050  loss = 4.19019 avg_loss = 4.27031\n",
      "epoch no.0 train no.5060  loss = 3.12434 avg_loss = 4.24522\n",
      "epoch no.0 train no.5070  loss = 5.90895 avg_loss = 4.31635\n",
      "epoch no.0 train no.5080  loss = 3.75187 avg_loss = 4.26956\n",
      "epoch no.0 train no.5090  loss = 4.67748 avg_loss = 4.27853\n",
      "epoch no.0 train no.5100  loss = 3.79860 avg_loss = 4.30175\n",
      "epoch no.0 train no.5110  loss = 6.40843 avg_loss = 4.31425\n",
      "epoch no.0 train no.5120  loss = 2.20963 avg_loss = 4.27444\n",
      "epoch no.0 train no.5130  loss = 4.14071 avg_loss = 4.31097\n",
      "epoch no.0 train no.5140  loss = 6.20901 avg_loss = 4.35512\n",
      "epoch no.0 train no.5150  loss = 3.63565 avg_loss = 4.33939\n",
      "epoch no.0 train no.5160  loss = 4.00073 avg_loss = 4.31255\n",
      "epoch no.0 train no.5170  loss = 3.45527 avg_loss = 4.28363\n",
      "epoch no.0 train no.5180  loss = 2.91999 avg_loss = 4.21886\n",
      "epoch no.0 train no.5190  loss = 5.48774 avg_loss = 4.21909\n",
      "epoch no.0 train no.5200  loss = 6.37457 avg_loss = 4.20825\n",
      "epoch no.0 train no.5210  loss = 3.10482 avg_loss = 4.19002\n",
      "epoch no.0 train no.5220  loss = 3.62536 avg_loss = 4.20757\n",
      "epoch no.0 train no.5230  loss = 4.46525 avg_loss = 4.22088\n",
      "epoch no.0 train no.5240  loss = 4.29296 avg_loss = 4.23611\n",
      "epoch no.0 train no.5250  loss = 7.08824 avg_loss = 4.32996\n",
      "epoch no.0 train no.5260  loss = 2.88680 avg_loss = 4.37525\n",
      "epoch no.0 train no.5270  loss = 3.98183 avg_loss = 4.36337\n",
      "epoch no.0 train no.5280  loss = 5.83905 avg_loss = 4.38329\n",
      "epoch no.0 train no.5290  loss = 4.56333 avg_loss = 4.34234\n",
      "epoch no.0 train no.5300  loss = 2.33930 avg_loss = 4.31683\n",
      "epoch no.0 train no.5310  loss = 2.82472 avg_loss = 4.32943\n",
      "epoch no.0 train no.5320  loss = 2.60184 avg_loss = 4.31691\n",
      "epoch no.0 train no.5330  loss = 4.44380 avg_loss = 4.34953\n",
      "epoch no.0 train no.5340  loss = 2.35520 avg_loss = 4.35306\n",
      "epoch no.0 train no.5350  loss = 6.35683 avg_loss = 4.40079\n",
      "epoch no.0 train no.5360  loss = 3.29306 avg_loss = 4.38071\n",
      "epoch no.0 train no.5370  loss = 2.72439 avg_loss = 4.37908\n",
      "epoch no.0 train no.5380  loss = 4.54459 avg_loss = 4.42477\n",
      "epoch no.0 train no.5390  loss = 4.73336 avg_loss = 4.43636\n",
      "epoch no.0 train no.5400  loss = 5.57204 avg_loss = 4.42961\n",
      "epoch no.0 train no.5410  loss = 4.21551 avg_loss = 4.42195\n",
      "epoch no.0 train no.5420  loss = 6.53195 avg_loss = 4.47433\n",
      "epoch no.0 train no.5430  loss = 5.68645 avg_loss = 4.41344\n",
      "epoch no.0 train no.5440  loss = 4.97240 avg_loss = 4.39834\n",
      "epoch no.0 train no.5450  loss = 4.42380 avg_loss = 4.38957\n",
      "epoch no.0 train no.5460  loss = 3.51801 avg_loss = 4.42770\n",
      "epoch no.0 train no.5470  loss = 3.96618 avg_loss = 4.42848\n",
      "epoch no.0 train no.5480  loss = 6.32874 avg_loss = 4.41877\n",
      "epoch no.0 train no.5490  loss = 4.41982 avg_loss = 4.46062\n",
      "epoch no.0 train no.5500  loss = 4.44952 avg_loss = 4.46491\n",
      "epoch no.0 train no.5510  loss = 3.92921 avg_loss = 4.45602\n",
      "epoch no.0 train no.5520  loss = 4.66250 avg_loss = 4.41104\n",
      "epoch no.0 train no.5530  loss = 3.05083 avg_loss = 4.34970\n",
      "epoch no.0 train no.5540  loss = 3.61624 avg_loss = 4.30514\n",
      "epoch no.0 train no.5550  loss = 2.61932 avg_loss = 4.33670\n",
      "epoch no.0 train no.5560  loss = 5.27313 avg_loss = 4.31283\n",
      "epoch no.0 train no.5570  loss = 2.94916 avg_loss = 4.27166\n",
      "epoch no.0 train no.5580  loss = 6.60500 avg_loss = 4.31044\n",
      "epoch no.0 train no.5590  loss = 4.12524 avg_loss = 4.30563\n",
      "epoch no.0 train no.5600  loss = 2.54219 avg_loss = 4.30454\n",
      "epoch no.0 train no.5610  loss = 2.61408 avg_loss = 4.30005\n",
      "epoch no.0 train no.5620  loss = 3.27063 avg_loss = 4.30480\n",
      "epoch no.0 train no.5630  loss = 5.78102 avg_loss = 4.41921\n",
      "epoch no.0 train no.5640  loss = 3.88721 avg_loss = 4.37171\n",
      "epoch no.0 train no.5650  loss = 7.04751 avg_loss = 4.33525\n",
      "epoch no.0 train no.5660  loss = 5.32864 avg_loss = 4.30897\n",
      "epoch no.0 train no.5670  loss = 3.07457 avg_loss = 4.30882\n",
      "epoch no.0 train no.5680  loss = 4.52977 avg_loss = 4.27572\n",
      "epoch no.0 train no.5690  loss = 4.66037 avg_loss = 4.32847\n",
      "epoch no.0 train no.5700  loss = 4.78789 avg_loss = 4.32387\n",
      "epoch no.0 train no.5710  loss = 3.13876 avg_loss = 4.33262\n",
      "epoch no.0 train no.5720  loss = 3.62538 avg_loss = 4.33126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.5730  loss = 4.12054 avg_loss = 4.29149\n",
      "epoch no.0 train no.5740  loss = 6.10224 avg_loss = 4.32832\n",
      "epoch no.0 train no.5750  loss = 4.28982 avg_loss = 4.35278\n",
      "epoch no.0 train no.5760  loss = 6.54117 avg_loss = 4.35387\n",
      "epoch no.0 train no.5770  loss = 3.60064 avg_loss = 4.34698\n",
      "epoch no.0 train no.5780  loss = 6.89929 avg_loss = 4.35197\n",
      "epoch no.0 train no.5790  loss = 5.85588 avg_loss = 4.35913\n",
      "epoch no.0 train no.5800  loss = 3.26773 avg_loss = 4.39038\n",
      "epoch no.0 train no.5810  loss = 3.48776 avg_loss = 4.35818\n",
      "epoch no.0 train no.5820  loss = 2.04624 avg_loss = 4.36171\n",
      "epoch no.0 train no.5830  loss = 5.31212 avg_loss = 4.36689\n",
      "epoch no.0 train no.5840  loss = 5.75627 avg_loss = 4.47662\n",
      "epoch no.0 train no.5850  loss = 4.18461 avg_loss = 4.44675\n",
      "epoch no.0 train no.5860  loss = 3.08394 avg_loss = 4.49474\n",
      "epoch no.0 train no.5870  loss = 6.60553 avg_loss = 4.44980\n",
      "epoch no.0 train no.5880  loss = 7.26958 avg_loss = 4.52925\n",
      "epoch no.0 train no.5890  loss = 3.75167 avg_loss = 4.48942\n",
      "epoch no.0 train no.5900  loss = 4.86446 avg_loss = 4.47841\n",
      "epoch no.0 train no.5910  loss = 2.62081 avg_loss = 4.47396\n",
      "epoch no.0 train no.5920  loss = 5.84497 avg_loss = 4.49937\n",
      "epoch no.0 train no.5930  loss = 5.06232 avg_loss = 4.48514\n",
      "epoch no.0 train no.5940  loss = 4.57446 avg_loss = 4.46388\n",
      "epoch no.0 train no.5950  loss = 4.27622 avg_loss = 4.43429\n",
      "epoch no.0 train no.5960  loss = 3.34945 avg_loss = 4.41428\n",
      "epoch no.0 train no.5970  loss = 5.48657 avg_loss = 4.40192\n",
      "epoch no.0 train no.5980  loss = 8.32323 avg_loss = 4.49713\n",
      "epoch no.0 train no.5990  loss = 2.30726 avg_loss = 4.45601\n",
      "epoch no.0 train no.6000  loss = 4.59650 avg_loss = 4.46215\n",
      "3\n",
      "to_tokens: ['▁', '전환', '용', '▁위한', '▁노래', '</s>']\n",
      "기분전환을 위한 재즈</s>\n",
      "epoch no.0 train no.6010  loss = 3.23733 avg_loss = 4.49968\n",
      "epoch no.0 train no.6020  loss = 3.95416 avg_loss = 4.49679\n",
      "epoch no.0 train no.6030  loss = 5.28764 avg_loss = 4.43258\n",
      "epoch no.0 train no.6040  loss = 3.92928 avg_loss = 4.38787\n",
      "epoch no.0 train no.6050  loss = 5.63610 avg_loss = 4.35245\n",
      "epoch no.0 train no.6060  loss = 4.37937 avg_loss = 4.32884\n",
      "epoch no.0 train no.6070  loss = 4.74155 avg_loss = 4.33399\n",
      "epoch no.0 train no.6080  loss = 4.22645 avg_loss = 4.31526\n",
      "epoch no.0 train no.6090  loss = 4.81734 avg_loss = 4.30109\n",
      "epoch no.0 train no.6100  loss = 3.30422 avg_loss = 4.29362\n",
      "epoch no.0 train no.6110  loss = 4.74636 avg_loss = 4.23660\n",
      "epoch no.0 train no.6120  loss = 5.17642 avg_loss = 4.27348\n",
      "epoch no.0 train no.6130  loss = 1.77046 avg_loss = 4.30256\n",
      "epoch no.0 train no.6140  loss = 4.16264 avg_loss = 4.31959\n",
      "epoch no.0 train no.6150  loss = 3.09701 avg_loss = 4.31589\n",
      "epoch no.0 train no.6160  loss = 4.56450 avg_loss = 4.29631\n",
      "epoch no.0 train no.6170  loss = 4.10191 avg_loss = 4.34427\n",
      "epoch no.0 train no.6180  loss = 3.53126 avg_loss = 4.34611\n",
      "epoch no.0 train no.6190  loss = 4.38913 avg_loss = 4.34567\n",
      "epoch no.0 train no.6200  loss = 4.00608 avg_loss = 4.40331\n",
      "epoch no.0 train no.6210  loss = 3.04088 avg_loss = 4.36935\n",
      "epoch no.0 train no.6220  loss = 4.31790 avg_loss = 4.33887\n",
      "epoch no.0 train no.6230  loss = 5.00774 avg_loss = 4.36265\n",
      "epoch no.0 train no.6240  loss = 5.53114 avg_loss = 4.38851\n",
      "epoch no.0 train no.6250  loss = 3.34911 avg_loss = 4.43595\n",
      "epoch no.0 train no.6260  loss = 2.34664 avg_loss = 4.35803\n",
      "epoch no.0 train no.6270  loss = 3.24163 avg_loss = 4.33976\n",
      "epoch no.0 train no.6280  loss = 3.49183 avg_loss = 4.29338\n",
      "epoch no.0 train no.6290  loss = 3.10184 avg_loss = 4.27624\n",
      "epoch no.0 train no.6300  loss = 4.72132 avg_loss = 4.27687\n",
      "epoch no.0 train no.6310  loss = 4.42001 avg_loss = 4.31487\n",
      "epoch no.0 train no.6320  loss = 3.45151 avg_loss = 4.26715\n",
      "epoch no.0 train no.6330  loss = 2.20828 avg_loss = 4.24129\n",
      "epoch no.0 train no.6340  loss = 3.92426 avg_loss = 4.28259\n",
      "epoch no.0 train no.6350  loss = 3.07719 avg_loss = 4.30377\n",
      "epoch no.0 train no.6360  loss = 4.61928 avg_loss = 4.31501\n",
      "epoch no.0 train no.6370  loss = 3.44556 avg_loss = 4.29919\n",
      "epoch no.0 train no.6380  loss = 4.83867 avg_loss = 4.27785\n",
      "epoch no.0 train no.6390  loss = 2.47677 avg_loss = 4.17969\n",
      "epoch no.0 train no.6400  loss = 3.60406 avg_loss = 4.16769\n",
      "epoch no.0 train no.6410  loss = 2.62929 avg_loss = 4.15983\n",
      "epoch no.0 train no.6420  loss = 3.12215 avg_loss = 4.17089\n",
      "epoch no.0 train no.6430  loss = 3.62274 avg_loss = 4.12837\n",
      "epoch no.0 train no.6440  loss = 4.51409 avg_loss = 4.09337\n",
      "epoch no.0 train no.6450  loss = 2.57136 avg_loss = 4.12163\n",
      "epoch no.0 train no.6460  loss = 3.47073 avg_loss = 4.09430\n",
      "epoch no.0 train no.6470  loss = 4.66757 avg_loss = 4.09788\n",
      "epoch no.0 train no.6480  loss = 2.54074 avg_loss = 4.13969\n",
      "epoch no.0 train no.6490  loss = 4.41256 avg_loss = 4.20447\n",
      "epoch no.0 train no.6500  loss = 3.86011 avg_loss = 4.10450\n",
      "epoch no.0 train no.6510  loss = 2.70506 avg_loss = 4.10794\n",
      "epoch no.0 train no.6520  loss = 5.27075 avg_loss = 4.14152\n",
      "epoch no.0 train no.6530  loss = 5.73201 avg_loss = 4.21707\n",
      "epoch no.0 train no.6540  loss = 3.22565 avg_loss = 4.26907\n",
      "epoch no.0 train no.6550  loss = 3.90017 avg_loss = 4.25587\n",
      "epoch no.0 train no.6560  loss = 4.51704 avg_loss = 4.26920\n",
      "epoch no.0 train no.6570  loss = 3.28660 avg_loss = 4.23948\n",
      "epoch no.0 train no.6580  loss = 5.01619 avg_loss = 4.30687\n",
      "epoch no.0 train no.6590  loss = 4.06095 avg_loss = 4.29952\n",
      "epoch no.0 train no.6600  loss = 5.02246 avg_loss = 4.27649\n",
      "epoch no.0 train no.6610  loss = 1.51724 avg_loss = 4.25237\n",
      "epoch no.0 train no.6620  loss = 3.58628 avg_loss = 4.21243\n",
      "epoch no.0 train no.6630  loss = 3.96997 avg_loss = 4.19331\n",
      "epoch no.0 train no.6640  loss = 2.80882 avg_loss = 4.19752\n",
      "epoch no.0 train no.6650  loss = 3.92319 avg_loss = 4.21893\n",
      "epoch no.0 train no.6660  loss = 2.72558 avg_loss = 4.25011\n",
      "epoch no.0 train no.6670  loss = 2.39085 avg_loss = 4.24445\n",
      "epoch no.0 train no.6680  loss = 3.01898 avg_loss = 4.26342\n",
      "epoch no.0 train no.6690  loss = 3.82163 avg_loss = 4.28117\n",
      "epoch no.0 train no.6700  loss = 4.98170 avg_loss = 4.30002\n",
      "epoch no.0 train no.6710  loss = 3.70890 avg_loss = 4.32676\n",
      "epoch no.0 train no.6720  loss = 4.23687 avg_loss = 4.35548\n",
      "epoch no.0 train no.6730  loss = 3.41120 avg_loss = 4.29650\n",
      "epoch no.0 train no.6740  loss = 3.88631 avg_loss = 4.33556\n",
      "epoch no.0 train no.6750  loss = 3.99454 avg_loss = 4.31130\n",
      "epoch no.0 train no.6760  loss = 4.48797 avg_loss = 4.26076\n",
      "epoch no.0 train no.6770  loss = 4.70333 avg_loss = 4.26666\n",
      "epoch no.0 train no.6780  loss = 3.35944 avg_loss = 4.19970\n",
      "epoch no.0 train no.6790  loss = 4.70056 avg_loss = 4.16209\n",
      "epoch no.0 train no.6800  loss = 2.96422 avg_loss = 4.21176\n",
      "epoch no.0 train no.6810  loss = 6.26178 avg_loss = 4.25123\n",
      "epoch no.0 train no.6820  loss = 4.10825 avg_loss = 4.21820\n",
      "epoch no.0 train no.6830  loss = 3.47523 avg_loss = 4.19673\n",
      "epoch no.0 train no.6840  loss = 4.55672 avg_loss = 4.30228\n",
      "epoch no.0 train no.6850  loss = 3.52650 avg_loss = 4.26427\n",
      "epoch no.0 train no.6860  loss = 4.21262 avg_loss = 4.29064\n",
      "epoch no.0 train no.6870  loss = 4.53456 avg_loss = 4.24973\n",
      "epoch no.0 train no.6880  loss = 3.81764 avg_loss = 4.32847\n",
      "epoch no.0 train no.6890  loss = 4.06010 avg_loss = 4.33310\n",
      "epoch no.0 train no.6900  loss = 3.43324 avg_loss = 4.26650\n",
      "epoch no.0 train no.6910  loss = 2.77598 avg_loss = 4.31253\n",
      "epoch no.0 train no.6920  loss = 4.65466 avg_loss = 4.26853\n",
      "epoch no.0 train no.6930  loss = 6.66362 avg_loss = 4.27913\n",
      "epoch no.0 train no.6940  loss = 6.98888 avg_loss = 4.33657\n",
      "epoch no.0 train no.6950  loss = 4.61426 avg_loss = 4.26859\n",
      "epoch no.0 train no.6960  loss = 4.30101 avg_loss = 4.31472\n",
      "epoch no.0 train no.6970  loss = 5.10246 avg_loss = 4.28727\n",
      "epoch no.0 train no.6980  loss = 2.16404 avg_loss = 4.20004\n",
      "epoch no.0 train no.6990  loss = 5.86068 avg_loss = 4.20749\n",
      "epoch no.0 train no.7000  loss = 5.59573 avg_loss = 4.20175\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁up', '하기', '▁위한', '▁노래', '송']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.0 train no.7010  loss = 3.43707 avg_loss = 4.20398\n",
      "epoch no.0 train no.7020  loss = 2.74612 avg_loss = 4.23682\n",
      "epoch no.0 train no.7030  loss = 3.54653 avg_loss = 4.23240\n",
      "epoch no.0 train no.7040  loss = 4.23795 avg_loss = 4.21865\n",
      "epoch no.0 train no.7050  loss = 3.02232 avg_loss = 4.19104\n",
      "epoch no.0 train no.7060  loss = 4.38591 avg_loss = 4.20855\n",
      "epoch no.0 train no.7070  loss = 3.42954 avg_loss = 4.22591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.7080  loss = 4.01985 avg_loss = 4.20564\n",
      "epoch no.0 train no.7090  loss = 4.28483 avg_loss = 4.27355\n",
      "epoch no.0 train no.7100  loss = 3.09233 avg_loss = 4.28898\n",
      "epoch no.0 train no.7110  loss = 4.56375 avg_loss = 4.28614\n",
      "epoch no.0 train no.7120  loss = 6.04093 avg_loss = 4.39006\n",
      "epoch no.0 train no.7130  loss = 4.21925 avg_loss = 4.37431\n",
      "epoch no.0 train no.7140  loss = 3.75389 avg_loss = 4.43386\n",
      "epoch no.0 train no.7150  loss = 4.65973 avg_loss = 4.49208\n",
      "epoch no.0 train no.7160  loss = 4.06905 avg_loss = 4.46100\n",
      "epoch no.0 train no.7170  loss = 4.96680 avg_loss = 4.48661\n",
      "epoch no.0 train no.7180  loss = 4.72850 avg_loss = 4.45789\n",
      "epoch no.0 train no.7190  loss = 4.46778 avg_loss = 4.37035\n",
      "epoch no.0 train no.7200  loss = 6.36158 avg_loss = 4.44217\n",
      "epoch no.0 train no.7210  loss = 7.89233 avg_loss = 4.53979\n",
      "epoch no.0 train no.7220  loss = 5.45934 avg_loss = 4.55022\n",
      "epoch no.0 train no.7230  loss = 3.72672 avg_loss = 4.47812\n",
      "epoch no.0 train no.7240  loss = 4.70171 avg_loss = 4.41908\n",
      "epoch no.0 train no.7250  loss = 3.37536 avg_loss = 4.35874\n",
      "epoch no.0 train no.7260  loss = 5.97092 avg_loss = 4.37628\n",
      "epoch no.0 train no.7270  loss = 4.89013 avg_loss = 4.35874\n",
      "epoch no.0 train no.7280  loss = 4.17985 avg_loss = 4.42680\n",
      "epoch no.0 train no.7290  loss = 4.41521 avg_loss = 4.42503\n",
      "epoch no.0 train no.7300  loss = 4.97709 avg_loss = 4.45091\n",
      "epoch no.0 train no.7310  loss = 7.11445 avg_loss = 4.43561\n",
      "epoch no.0 train no.7320  loss = 4.79378 avg_loss = 4.39581\n",
      "epoch no.0 train no.7330  loss = 4.70803 avg_loss = 4.38827\n",
      "epoch no.0 train no.7340  loss = 3.74092 avg_loss = 4.41746\n",
      "epoch no.0 train no.7350  loss = 7.12864 avg_loss = 4.40030\n",
      "epoch no.0 train no.7360  loss = 4.01050 avg_loss = 4.38268\n",
      "epoch no.0 train no.7370  loss = 3.82208 avg_loss = 4.38146\n",
      "epoch no.0 train no.7380  loss = 4.98970 avg_loss = 4.35514\n",
      "epoch no.0 train no.7390  loss = 2.30944 avg_loss = 4.32822\n",
      "epoch no.0 train no.7400  loss = 4.59636 avg_loss = 4.27495\n",
      "epoch no.0 train no.7410  loss = 4.89195 avg_loss = 4.25903\n",
      "epoch no.0 train no.7420  loss = 4.62978 avg_loss = 4.29314\n",
      "epoch no.0 train no.7430  loss = 5.75440 avg_loss = 4.32433\n",
      "epoch no.0 train no.7440  loss = 2.81273 avg_loss = 4.30071\n",
      "epoch no.0 train no.7450  loss = 3.70293 avg_loss = 4.33111\n",
      "epoch no.0 train no.7460  loss = 4.82887 avg_loss = 4.33477\n",
      "epoch no.0 train no.7470  loss = 2.30236 avg_loss = 4.29283\n",
      "epoch no.0 train no.7480  loss = 3.86858 avg_loss = 4.37267\n",
      "epoch no.0 train no.7490  loss = 3.29577 avg_loss = 4.38384\n",
      "epoch no.0 train no.7500  loss = 3.65055 avg_loss = 4.34901\n",
      "epoch no.0 train no.7510  loss = 4.81066 avg_loss = 4.32917\n",
      "epoch no.0 train no.7520  loss = 4.86778 avg_loss = 4.31043\n",
      "epoch no.0 train no.7530  loss = 4.07763 avg_loss = 4.33087\n",
      "epoch no.0 train no.7540  loss = 4.20218 avg_loss = 4.32561\n",
      "epoch no.0 train no.7550  loss = 5.42905 avg_loss = 4.30891\n",
      "epoch no.0 train no.7560  loss = 4.27282 avg_loss = 4.31820\n",
      "epoch no.0 train no.7570  loss = 5.13042 avg_loss = 4.30372\n",
      "epoch no.0 train no.7580  loss = 3.87295 avg_loss = 4.30275\n",
      "epoch no.0 train no.7590  loss = 4.03390 avg_loss = 4.31302\n",
      "epoch no.0 train no.7600  loss = 5.57402 avg_loss = 4.35697\n",
      "epoch no.0 train no.7610  loss = 4.91219 avg_loss = 4.37187\n",
      "epoch no.0 train no.7620  loss = 4.05782 avg_loss = 4.33014\n",
      "epoch no.0 train no.7630  loss = 5.41875 avg_loss = 4.30869\n",
      "epoch no.0 train no.7640  loss = 4.59387 avg_loss = 4.21713\n",
      "epoch no.0 train no.7650  loss = 4.33251 avg_loss = 4.24881\n",
      "epoch no.0 train no.7660  loss = 3.98519 avg_loss = 4.22860\n",
      "epoch no.0 train no.7670  loss = 5.48844 avg_loss = 4.22747\n",
      "epoch no.0 train no.7680  loss = 3.29368 avg_loss = 4.20932\n",
      "epoch no.0 train no.7690  loss = 2.80113 avg_loss = 4.17484\n",
      "epoch no.0 train no.7700  loss = 3.65496 avg_loss = 4.16171\n",
      "epoch no.0 train no.7710  loss = 3.47100 avg_loss = 4.20470\n",
      "epoch no.0 train no.7720  loss = 3.33566 avg_loss = 4.26090\n",
      "epoch no.0 train no.7730  loss = 3.78243 avg_loss = 4.25879\n",
      "epoch no.0 train no.7740  loss = 2.91359 avg_loss = 4.27295\n",
      "epoch no.0 train no.7750  loss = 3.30172 avg_loss = 4.22424\n",
      "epoch no.0 train no.7760  loss = 3.89499 avg_loss = 4.27116\n",
      "epoch no.0 train no.7770  loss = 2.65946 avg_loss = 4.20904\n",
      "epoch no.0 train no.7780  loss = 2.88349 avg_loss = 4.20276\n",
      "epoch no.0 train no.7790  loss = 4.83436 avg_loss = 4.24073\n",
      "epoch no.0 train no.7800  loss = 4.09012 avg_loss = 4.22424\n",
      "epoch no.0 train no.7810  loss = 5.88065 avg_loss = 4.23209\n",
      "epoch no.0 train no.7820  loss = 3.50338 avg_loss = 4.23182\n",
      "epoch no.0 train no.7830  loss = 5.61582 avg_loss = 4.20139\n",
      "epoch no.0 train no.7840  loss = 3.47916 avg_loss = 4.24879\n",
      "epoch no.0 train no.7850  loss = 3.77280 avg_loss = 4.23534\n",
      "epoch no.0 train no.7860  loss = 3.47122 avg_loss = 4.27484\n",
      "epoch no.0 train no.7870  loss = 4.67354 avg_loss = 4.31906\n",
      "epoch no.0 train no.7880  loss = 5.30439 avg_loss = 4.32336\n",
      "epoch no.0 train no.7890  loss = 2.79367 avg_loss = 4.30769\n",
      "epoch no.0 train no.7900  loss = 5.10092 avg_loss = 4.32295\n",
      "epoch no.0 train no.7910  loss = 3.92546 avg_loss = 4.28015\n",
      "epoch no.0 train no.7920  loss = 4.96309 avg_loss = 4.27057\n",
      "epoch no.0 train no.7930  loss = 3.47094 avg_loss = 4.19442\n",
      "epoch no.0 train no.7940  loss = 3.83929 avg_loss = 4.21216\n",
      "epoch no.0 train no.7950  loss = 3.87377 avg_loss = 4.20381\n",
      "epoch no.0 train no.7960  loss = 3.31816 avg_loss = 4.18605\n",
      "epoch no.0 train no.7970  loss = 5.84497 avg_loss = 4.22634\n",
      "epoch no.0 train no.7980  loss = 5.17119 avg_loss = 4.24626\n",
      "epoch no.0 train no.7990  loss = 3.82963 avg_loss = 4.23000\n",
      "epoch no.0 train no.8000  loss = 4.61783 avg_loss = 4.21450\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '용', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환 할때 듣는 노래</s>\n",
      "epoch no.0 train no.8010  loss = 2.51865 avg_loss = 4.16816\n",
      "epoch no.0 train no.8020  loss = 1.79584 avg_loss = 4.15436\n",
      "epoch no.0 train no.8030  loss = 3.33791 avg_loss = 4.10831\n",
      "epoch no.0 train no.8040  loss = 4.92656 avg_loss = 4.15922\n",
      "epoch no.0 train no.8050  loss = 3.99935 avg_loss = 4.18021\n",
      "epoch no.0 train no.8060  loss = 6.08106 avg_loss = 4.22298\n",
      "epoch no.0 train no.8070  loss = 3.32087 avg_loss = 4.18529\n",
      "epoch no.0 train no.8080  loss = 3.75375 avg_loss = 4.26204\n",
      "epoch no.0 train no.8090  loss = 2.11445 avg_loss = 4.24831\n",
      "epoch no.0 train no.8100  loss = 3.90791 avg_loss = 4.27913\n",
      "epoch no.0 train no.8110  loss = 4.83292 avg_loss = 4.26145\n",
      "epoch no.0 train no.8120  loss = 3.32151 avg_loss = 4.25185\n",
      "epoch no.0 train no.8130  loss = 3.16334 avg_loss = 4.29215\n",
      "epoch no.0 train no.8140  loss = 3.14924 avg_loss = 4.28613\n",
      "epoch no.0 train no.8150  loss = 3.49647 avg_loss = 4.30981\n",
      "epoch no.0 train no.8160  loss = 4.16478 avg_loss = 4.30841\n",
      "epoch no.0 train no.8170  loss = 5.64799 avg_loss = 4.25319\n",
      "epoch no.0 train no.8180  loss = 5.57919 avg_loss = 4.25297\n",
      "epoch no.0 train no.8190  loss = 4.16158 avg_loss = 4.28761\n",
      "epoch no.0 train no.8200  loss = 4.30207 avg_loss = 4.28139\n",
      "epoch no.0 train no.8210  loss = 3.30220 avg_loss = 4.27551\n",
      "epoch no.0 train no.8220  loss = 4.21519 avg_loss = 4.29836\n",
      "epoch no.0 train no.8230  loss = 2.87073 avg_loss = 4.25206\n",
      "epoch no.0 train no.8240  loss = 3.27473 avg_loss = 4.23596\n",
      "epoch no.0 train no.8250  loss = 2.93682 avg_loss = 4.18784\n",
      "epoch no.0 train no.8260  loss = 4.84149 avg_loss = 4.22501\n",
      "epoch no.0 train no.8270  loss = 6.17713 avg_loss = 4.22684\n",
      "epoch no.0 train no.8280  loss = 4.85997 avg_loss = 4.23168\n",
      "epoch no.0 train no.8290  loss = 3.46184 avg_loss = 4.23697\n",
      "epoch no.0 train no.8300  loss = 5.09607 avg_loss = 4.22177\n",
      "epoch no.0 train no.8310  loss = 2.81529 avg_loss = 4.15040\n",
      "epoch no.0 train no.8320  loss = 2.69209 avg_loss = 4.15694\n",
      "epoch no.0 train no.8330  loss = 5.01819 avg_loss = 4.16320\n",
      "epoch no.0 train no.8340  loss = 3.90865 avg_loss = 4.15313\n",
      "epoch no.0 train no.8350  loss = 3.36251 avg_loss = 4.14039\n",
      "epoch no.0 train no.8360  loss = 3.27572 avg_loss = 4.13939\n",
      "epoch no.0 train no.8370  loss = 3.75182 avg_loss = 4.15407\n",
      "epoch no.0 train no.8380  loss = 2.81493 avg_loss = 4.13881\n",
      "epoch no.0 train no.8390  loss = 2.65156 avg_loss = 4.07962\n",
      "epoch no.0 train no.8400  loss = 7.00455 avg_loss = 4.14787\n",
      "epoch no.0 train no.8410  loss = 4.76401 avg_loss = 4.08567\n",
      "epoch no.0 train no.8420  loss = 3.02329 avg_loss = 4.08830\n",
      "epoch no.0 train no.8430  loss = 3.92708 avg_loss = 4.15661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.8440  loss = 3.51818 avg_loss = 4.15373\n",
      "epoch no.0 train no.8450  loss = 3.33213 avg_loss = 4.18211\n",
      "epoch no.0 train no.8460  loss = 2.94363 avg_loss = 4.14498\n",
      "epoch no.0 train no.8470  loss = 4.10891 avg_loss = 4.20839\n",
      "epoch no.0 train no.8480  loss = 4.07486 avg_loss = 4.23346\n",
      "epoch no.0 train no.8490  loss = 3.85028 avg_loss = 4.27287\n",
      "epoch no.0 train no.8500  loss = 4.59488 avg_loss = 4.21634\n",
      "epoch no.0 train no.8510  loss = 4.21052 avg_loss = 4.25993\n",
      "epoch no.0 train no.8520  loss = 4.97223 avg_loss = 4.33220\n",
      "epoch no.0 train no.8530  loss = 2.72204 avg_loss = 4.29889\n",
      "epoch no.0 train no.8540  loss = 3.65037 avg_loss = 4.30547\n",
      "epoch no.0 train no.8550  loss = 3.25565 avg_loss = 4.27353\n",
      "epoch no.0 train no.8560  loss = 2.75420 avg_loss = 4.21380\n",
      "epoch no.0 train no.8570  loss = 4.83495 avg_loss = 4.20364\n",
      "epoch no.0 train no.8580  loss = 2.02229 avg_loss = 4.26953\n",
      "epoch no.0 train no.8590  loss = 2.93966 avg_loss = 4.23954\n",
      "epoch no.0 train no.8600  loss = 9.65804 avg_loss = 4.33136\n",
      "epoch no.0 train no.8610  loss = 5.05382 avg_loss = 4.34271\n",
      "epoch no.0 train no.8620  loss = 3.04676 avg_loss = 4.32146\n",
      "epoch no.0 train no.8630  loss = 3.81477 avg_loss = 4.29239\n",
      "epoch no.0 train no.8640  loss = 5.23899 avg_loss = 4.30705\n",
      "epoch no.0 train no.8650  loss = 4.13788 avg_loss = 4.29707\n",
      "epoch no.0 train no.8660  loss = 4.74867 avg_loss = 4.29679\n",
      "epoch no.0 train no.8670  loss = 5.52711 avg_loss = 4.25181\n",
      "epoch no.0 train no.8680  loss = 6.35323 avg_loss = 4.30190\n",
      "epoch no.0 train no.8690  loss = 5.45783 avg_loss = 4.24832\n",
      "epoch no.0 train no.8700  loss = 4.30575 avg_loss = 4.20143\n",
      "epoch no.0 train no.8710  loss = 2.60513 avg_loss = 4.23744\n",
      "epoch no.0 train no.8720  loss = 2.80361 avg_loss = 4.18922\n",
      "epoch no.0 train no.8730  loss = 4.14045 avg_loss = 4.19115\n",
      "epoch no.0 train no.8740  loss = 2.59495 avg_loss = 4.16549\n",
      "epoch no.0 train no.8750  loss = 5.41379 avg_loss = 4.22114\n",
      "epoch no.0 train no.8760  loss = 4.17800 avg_loss = 4.27156\n",
      "epoch no.0 train no.8770  loss = 5.52761 avg_loss = 4.30109\n",
      "epoch no.0 train no.8780  loss = 2.25046 avg_loss = 4.26943\n",
      "epoch no.0 train no.8790  loss = 2.98214 avg_loss = 4.24074\n",
      "epoch no.0 train no.8800  loss = 2.89948 avg_loss = 4.28029\n",
      "epoch no.0 train no.8810  loss = 3.82497 avg_loss = 4.25036\n",
      "epoch no.0 train no.8820  loss = 3.17306 avg_loss = 4.30437\n",
      "epoch no.0 train no.8830  loss = 5.46917 avg_loss = 4.30008\n",
      "epoch no.0 train no.8840  loss = 4.35595 avg_loss = 4.30640\n",
      "epoch no.0 train no.8850  loss = 3.71829 avg_loss = 4.30122\n",
      "epoch no.0 train no.8860  loss = 4.38406 avg_loss = 4.30373\n",
      "epoch no.0 train no.8870  loss = 2.64798 avg_loss = 4.26101\n",
      "epoch no.0 train no.8880  loss = 4.47313 avg_loss = 4.22463\n",
      "epoch no.0 train no.8890  loss = 3.86669 avg_loss = 4.22090\n",
      "epoch no.0 train no.8900  loss = 2.58707 avg_loss = 4.21900\n",
      "epoch no.0 train no.8910  loss = 3.84402 avg_loss = 4.20514\n",
      "epoch no.0 train no.8920  loss = 3.37261 avg_loss = 4.20106\n",
      "epoch no.0 train no.8930  loss = 5.24955 avg_loss = 4.24977\n",
      "epoch no.0 train no.8940  loss = 3.99964 avg_loss = 4.22943\n",
      "epoch no.0 train no.8950  loss = 5.63927 avg_loss = 4.25518\n",
      "epoch no.0 train no.8960  loss = 3.73553 avg_loss = 4.28840\n",
      "epoch no.0 train no.8970  loss = 3.95099 avg_loss = 4.25365\n",
      "epoch no.0 train no.8980  loss = 3.22226 avg_loss = 4.21184\n",
      "epoch no.0 train no.8990  loss = 3.56497 avg_loss = 4.23136\n",
      "epoch no.0 train no.9000  loss = 3.03214 avg_loss = 4.22289\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.0 train no.9010  loss = 6.86467 avg_loss = 4.24609\n",
      "epoch no.0 train no.9020  loss = 5.84304 avg_loss = 4.23316\n",
      "epoch no.0 train no.9030  loss = 2.78968 avg_loss = 4.19100\n",
      "epoch no.0 train no.9040  loss = 4.59960 avg_loss = 4.17593\n",
      "epoch no.0 train no.9050  loss = 4.62782 avg_loss = 4.16273\n",
      "epoch no.0 train no.9060  loss = 4.20751 avg_loss = 4.15778\n",
      "epoch no.0 train no.9070  loss = 4.44363 avg_loss = 4.16393\n",
      "epoch no.0 train no.9080  loss = 3.57937 avg_loss = 4.24955\n",
      "epoch no.0 train no.9090  loss = 3.15183 avg_loss = 4.18293\n",
      "epoch no.0 train no.9100  loss = 2.30326 avg_loss = 4.18609\n",
      "epoch no.0 train no.9110  loss = 5.44891 avg_loss = 4.17848\n",
      "epoch no.0 train no.9120  loss = 3.75319 avg_loss = 4.23739\n",
      "epoch no.0 train no.9130  loss = 1.94583 avg_loss = 4.23540\n",
      "epoch no.0 train no.9140  loss = 3.68838 avg_loss = 4.20220\n",
      "epoch no.0 train no.9150  loss = 5.88578 avg_loss = 4.18008\n",
      "epoch no.0 train no.9160  loss = 4.44370 avg_loss = 4.24906\n",
      "epoch no.0 train no.9170  loss = 1.51315 avg_loss = 4.21393\n",
      "epoch no.0 train no.9180  loss = 3.86657 avg_loss = 4.26426\n",
      "epoch no.0 train no.9190  loss = 3.31622 avg_loss = 4.26274\n",
      "epoch no.0 train no.9200  loss = 7.31223 avg_loss = 4.27062\n",
      "epoch no.0 train no.9210  loss = 3.98919 avg_loss = 4.24221\n",
      "epoch no.0 train no.9220  loss = 4.09002 avg_loss = 4.18512\n",
      "epoch no.0 train no.9230  loss = 4.87796 avg_loss = 4.17940\n",
      "epoch no.0 train no.9240  loss = 4.35443 avg_loss = 4.16824\n",
      "epoch no.0 train no.9250  loss = 4.66698 avg_loss = 4.21231\n",
      "epoch no.0 train no.9260  loss = 5.16221 avg_loss = 4.27163\n",
      "epoch no.0 train no.9270  loss = 5.67546 avg_loss = 4.25858\n",
      "epoch no.0 train no.9280  loss = 3.59236 avg_loss = 4.28583\n",
      "epoch no.0 train no.9290  loss = 6.58722 avg_loss = 4.29755\n",
      "epoch no.0 train no.9300  loss = 3.36411 avg_loss = 4.29774\n",
      "epoch no.0 train no.9310  loss = 4.09975 avg_loss = 4.31996\n",
      "epoch no.0 train no.9320  loss = 2.77014 avg_loss = 4.32261\n",
      "epoch no.0 train no.9330  loss = 4.31888 avg_loss = 4.39425\n",
      "epoch no.0 train no.9340  loss = 3.55304 avg_loss = 4.36965\n",
      "epoch no.0 train no.9350  loss = 3.80808 avg_loss = 4.32875\n",
      "epoch no.0 train no.9360  loss = 2.57225 avg_loss = 4.29475\n",
      "epoch no.0 train no.9370  loss = 4.82376 avg_loss = 4.28980\n",
      "epoch no.0 train no.9380  loss = 2.89269 avg_loss = 4.31784\n",
      "epoch no.0 train no.9390  loss = 4.56193 avg_loss = 4.32443\n",
      "epoch no.0 train no.9400  loss = 4.96743 avg_loss = 4.33341\n",
      "epoch no.0 train no.9410  loss = 7.72997 avg_loss = 4.39698\n",
      "epoch no.0 train no.9420  loss = 4.33817 avg_loss = 4.34619\n",
      "epoch no.0 train no.9430  loss = 4.07731 avg_loss = 4.35852\n",
      "epoch no.0 train no.9440  loss = 4.25855 avg_loss = 4.37437\n",
      "epoch no.0 train no.9450  loss = 3.51254 avg_loss = 4.38337\n",
      "epoch no.0 train no.9460  loss = 3.06527 avg_loss = 4.35172\n",
      "epoch no.0 train no.9470  loss = 5.45696 avg_loss = 4.36622\n",
      "epoch no.0 train no.9480  loss = 4.74014 avg_loss = 4.44322\n",
      "epoch no.0 train no.9490  loss = 3.65008 avg_loss = 4.39378\n",
      "epoch no.0 train no.9500  loss = 3.62886 avg_loss = 4.33331\n",
      "epoch no.0 train no.9510  loss = 3.34203 avg_loss = 4.34889\n",
      "epoch no.0 train no.9520  loss = 4.03737 avg_loss = 4.36832\n",
      "epoch no.0 train no.9530  loss = 3.42262 avg_loss = 4.31886\n",
      "epoch no.0 train no.9540  loss = 3.84201 avg_loss = 4.27733\n",
      "epoch no.0 train no.9550  loss = 5.46099 avg_loss = 4.28310\n",
      "epoch no.0 train no.9560  loss = 2.74994 avg_loss = 4.27079\n",
      "epoch no.0 train no.9570  loss = 4.70357 avg_loss = 4.28675\n",
      "epoch no.0 train no.9580  loss = 3.13617 avg_loss = 4.31140\n",
      "epoch no.0 train no.9590  loss = 4.68027 avg_loss = 4.30529\n",
      "epoch no.0 train no.9600  loss = 3.08543 avg_loss = 4.34896\n",
      "epoch no.0 train no.9610  loss = 4.61105 avg_loss = 4.37373\n",
      "epoch no.0 train no.9620  loss = 3.55651 avg_loss = 4.35554\n",
      "epoch no.0 train no.9630  loss = 3.33949 avg_loss = 4.37444\n",
      "epoch no.0 train no.9640  loss = 4.43994 avg_loss = 4.37855\n",
      "epoch no.0 train no.9650  loss = 5.18451 avg_loss = 4.39012\n",
      "epoch no.0 train no.9660  loss = 5.19476 avg_loss = 4.33387\n",
      "epoch no.0 train no.9670  loss = 3.83852 avg_loss = 4.33299\n",
      "epoch no.0 train no.9680  loss = 4.73048 avg_loss = 4.33523\n",
      "epoch no.0 train no.9690  loss = 6.88167 avg_loss = 4.36236\n",
      "epoch no.0 train no.9700  loss = 3.36822 avg_loss = 4.34476\n",
      "epoch no.0 train no.9710  loss = 4.85648 avg_loss = 4.32189\n",
      "epoch no.0 train no.9720  loss = 6.31862 avg_loss = 4.32991\n",
      "epoch no.0 train no.9730  loss = 5.17051 avg_loss = 4.29981\n",
      "epoch no.0 train no.9740  loss = 4.00107 avg_loss = 4.32912\n",
      "epoch no.0 train no.9750  loss = 3.94460 avg_loss = 4.29620\n",
      "epoch no.0 train no.9760  loss = 6.72613 avg_loss = 4.31356\n",
      "epoch no.0 train no.9770  loss = 3.88355 avg_loss = 4.31657\n",
      "epoch no.0 train no.9780  loss = 2.18059 avg_loss = 4.26052\n",
      "epoch no.0 train no.9790  loss = 4.13907 avg_loss = 4.23423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.9800  loss = 2.44790 avg_loss = 4.20646\n",
      "epoch no.0 train no.9810  loss = 4.90876 avg_loss = 4.19658\n",
      "epoch no.0 train no.9820  loss = 3.69364 avg_loss = 4.19867\n",
      "epoch no.0 train no.9830  loss = 4.99459 avg_loss = 4.24537\n",
      "epoch no.0 train no.9840  loss = 4.80901 avg_loss = 4.25404\n",
      "epoch no.0 train no.9850  loss = 3.67298 avg_loss = 4.24029\n",
      "epoch no.0 train no.9860  loss = 5.27351 avg_loss = 4.25192\n",
      "epoch no.0 train no.9870  loss = 2.32452 avg_loss = 4.24553\n",
      "epoch no.0 train no.9880  loss = 4.46936 avg_loss = 4.27169\n",
      "epoch no.0 train no.9890  loss = 2.83489 avg_loss = 4.31211\n",
      "epoch no.0 train no.9900  loss = 3.68447 avg_loss = 4.30911\n",
      "epoch no.0 train no.9910  loss = 3.69048 avg_loss = 4.30394\n",
      "epoch no.0 train no.9920  loss = 5.14807 avg_loss = 4.25753\n",
      "epoch no.0 train no.9930  loss = 2.96540 avg_loss = 4.24981\n",
      "epoch no.0 train no.9940  loss = 5.07545 avg_loss = 4.20722\n",
      "epoch no.0 train no.9950  loss = 3.66902 avg_loss = 4.17161\n",
      "epoch no.0 train no.9960  loss = 4.20990 avg_loss = 4.14984\n",
      "epoch no.0 train no.9970  loss = 5.52160 avg_loss = 4.19865\n",
      "epoch no.0 train no.9980  loss = 3.97439 avg_loss = 4.22129\n",
      "epoch no.0 train no.9990  loss = 3.79282 avg_loss = 4.24309\n",
      "epoch no.0 train no.10000  loss = 4.46411 avg_loss = 4.27378\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 좋은 음악</s>\n",
      "epoch no.0 train no.10010  loss = 4.45479 avg_loss = 4.24521\n",
      "epoch no.0 train no.10020  loss = 6.36596 avg_loss = 4.28750\n",
      "epoch no.0 train no.10030  loss = 3.53169 avg_loss = 4.28126\n",
      "epoch no.0 train no.10040  loss = 2.82358 avg_loss = 4.22702\n",
      "epoch no.0 train no.10050  loss = 3.20543 avg_loss = 4.19880\n",
      "epoch no.0 train no.10060  loss = 4.64042 avg_loss = 4.21986\n",
      "epoch no.0 train no.10070  loss = 3.43149 avg_loss = 4.23693\n",
      "epoch no.0 train no.10080  loss = 4.97638 avg_loss = 4.21979\n",
      "epoch no.0 train no.10090  loss = 3.16807 avg_loss = 4.22126\n",
      "epoch no.0 train no.10100  loss = 5.23249 avg_loss = 4.19444\n",
      "epoch no.0 train no.10110  loss = 3.67405 avg_loss = 4.14992\n",
      "epoch no.0 train no.10120  loss = 2.72809 avg_loss = 4.09237\n",
      "epoch no.0 train no.10130  loss = 2.94429 avg_loss = 4.09485\n",
      "epoch no.0 train no.10140  loss = 3.05108 avg_loss = 4.17795\n",
      "epoch no.0 train no.10150  loss = 5.10013 avg_loss = 4.17182\n",
      "epoch no.0 train no.10160  loss = 3.51279 avg_loss = 4.14769\n",
      "epoch no.0 train no.10170  loss = 4.52281 avg_loss = 4.18123\n",
      "epoch no.0 train no.10180  loss = 3.69987 avg_loss = 4.19681\n",
      "epoch no.0 train no.10190  loss = 5.47741 avg_loss = 4.23417\n",
      "epoch no.0 train no.10200  loss = 7.37726 avg_loss = 4.25827\n",
      "epoch no.0 train no.10210  loss = 6.71632 avg_loss = 4.36006\n",
      "epoch no.0 train no.10220  loss = 2.30587 avg_loss = 4.28595\n",
      "epoch no.0 train no.10230  loss = 3.82411 avg_loss = 4.35895\n",
      "epoch no.0 train no.10240  loss = 5.28005 avg_loss = 4.43526\n",
      "epoch no.0 train no.10250  loss = 4.43642 avg_loss = 4.50796\n",
      "epoch no.0 train no.10260  loss = 5.02083 avg_loss = 4.52447\n",
      "epoch no.0 train no.10270  loss = 4.12346 avg_loss = 4.45627\n",
      "epoch no.0 train no.10280  loss = 4.64765 avg_loss = 4.43497\n",
      "epoch no.0 train no.10290  loss = 3.14618 avg_loss = 4.38969\n",
      "epoch no.0 train no.10300  loss = 4.54541 avg_loss = 4.39737\n",
      "epoch no.0 train no.10310  loss = 5.10061 avg_loss = 4.38396\n",
      "epoch no.0 train no.10320  loss = 4.53757 avg_loss = 4.41589\n",
      "epoch no.0 train no.10330  loss = 2.81920 avg_loss = 4.36223\n",
      "epoch no.0 train no.10340  loss = 5.54427 avg_loss = 4.41004\n",
      "epoch no.0 train no.10350  loss = 3.38157 avg_loss = 4.36636\n",
      "epoch no.0 train no.10360  loss = 3.09050 avg_loss = 4.35985\n",
      "epoch no.0 train no.10370  loss = 2.35098 avg_loss = 4.32862\n",
      "epoch no.0 train no.10380  loss = 3.25653 avg_loss = 4.34368\n",
      "epoch no.0 train no.10390  loss = 4.98786 avg_loss = 4.39481\n",
      "epoch no.0 train no.10400  loss = 5.26168 avg_loss = 4.44095\n",
      "epoch no.0 train no.10410  loss = 4.28142 avg_loss = 4.46695\n",
      "epoch no.0 train no.10420  loss = 3.27784 avg_loss = 4.49357\n",
      "epoch no.0 train no.10430  loss = 4.44764 avg_loss = 4.44272\n",
      "epoch no.0 train no.10440  loss = 3.79193 avg_loss = 4.40619\n",
      "epoch no.0 train no.10450  loss = 4.67538 avg_loss = 4.35716\n",
      "epoch no.0 train no.10460  loss = 4.57152 avg_loss = 4.37377\n",
      "epoch no.0 train no.10470  loss = 4.37239 avg_loss = 4.34298\n",
      "epoch no.0 train no.10480  loss = 5.52486 avg_loss = 4.34662\n",
      "epoch no.0 train no.10490  loss = 4.66597 avg_loss = 4.28934\n",
      "epoch no.0 train no.10500  loss = 4.15690 avg_loss = 4.28720\n",
      "epoch no.0 train no.10510  loss = 6.72395 avg_loss = 4.31419\n",
      "epoch no.0 train no.10520  loss = 2.97165 avg_loss = 4.26883\n",
      "epoch no.0 train no.10530  loss = 5.64387 avg_loss = 4.27675\n",
      "epoch no.0 train no.10540  loss = 4.87402 avg_loss = 4.29492\n",
      "epoch no.0 train no.10550  loss = 4.93040 avg_loss = 4.27595\n",
      "epoch no.0 train no.10560  loss = 3.37492 avg_loss = 4.26828\n",
      "epoch no.0 train no.10570  loss = 2.69884 avg_loss = 4.21675\n",
      "epoch no.0 train no.10580  loss = 4.76270 avg_loss = 4.23205\n",
      "epoch no.0 train no.10590  loss = 3.78852 avg_loss = 4.15252\n",
      "epoch no.0 train no.10600  loss = 5.53535 avg_loss = 4.14637\n",
      "epoch no.0 train no.10610  loss = 6.02677 avg_loss = 4.09209\n",
      "epoch no.0 train no.10620  loss = 4.21261 avg_loss = 4.10184\n",
      "epoch no.0 train no.10630  loss = 3.56674 avg_loss = 4.12342\n",
      "epoch no.0 train no.10640  loss = 3.17408 avg_loss = 4.12248\n",
      "epoch no.0 train no.10650  loss = 4.30195 avg_loss = 4.13020\n",
      "epoch no.0 train no.10660  loss = 2.93934 avg_loss = 4.14243\n",
      "epoch no.0 train no.10670  loss = 4.36239 avg_loss = 4.19136\n",
      "epoch no.0 train no.10680  loss = 2.41667 avg_loss = 4.21403\n",
      "epoch no.0 train no.10690  loss = 6.57702 avg_loss = 4.21685\n",
      "epoch no.0 train no.10700  loss = 2.00028 avg_loss = 4.19915\n",
      "epoch no.0 train no.10710  loss = 4.56215 avg_loss = 4.21572\n",
      "epoch no.0 train no.10720  loss = 3.12779 avg_loss = 4.19009\n",
      "epoch no.0 train no.10730  loss = 3.29011 avg_loss = 4.20942\n",
      "epoch no.0 train no.10740  loss = 3.89925 avg_loss = 4.18081\n",
      "epoch no.0 train no.10750  loss = 4.11012 avg_loss = 4.20790\n",
      "epoch no.0 train no.10760  loss = 4.40907 avg_loss = 4.23084\n",
      "epoch no.0 train no.10770  loss = 2.61967 avg_loss = 4.20543\n",
      "epoch no.0 train no.10780  loss = 7.45557 avg_loss = 4.26569\n",
      "epoch no.0 train no.10790  loss = 3.76569 avg_loss = 4.27551\n",
      "epoch no.0 train no.10800  loss = 3.64895 avg_loss = 4.24135\n",
      "epoch no.0 train no.10810  loss = 2.35485 avg_loss = 4.24622\n",
      "epoch no.0 train no.10820  loss = 6.21749 avg_loss = 4.24186\n",
      "epoch no.0 train no.10830  loss = 5.55168 avg_loss = 4.23380\n",
      "epoch no.0 train no.10840  loss = 4.89555 avg_loss = 4.28186\n",
      "epoch no.0 train no.10850  loss = 3.47331 avg_loss = 4.25521\n",
      "epoch no.0 train no.10860  loss = 3.26672 avg_loss = 4.24628\n",
      "epoch no.0 train no.10870  loss = 2.52419 avg_loss = 4.20906\n",
      "epoch no.0 train no.10880  loss = 1.62976 avg_loss = 4.22960\n",
      "epoch no.0 train no.10890  loss = 7.47063 avg_loss = 4.23295\n",
      "epoch no.0 train no.10900  loss = 3.32444 avg_loss = 4.21856\n",
      "epoch no.0 train no.10910  loss = 4.60121 avg_loss = 4.22155\n",
      "epoch no.0 train no.10920  loss = 4.09818 avg_loss = 4.25227\n",
      "epoch no.0 train no.10930  loss = 2.60925 avg_loss = 4.23101\n",
      "epoch no.0 train no.10940  loss = 5.03934 avg_loss = 4.19893\n",
      "epoch no.0 train no.10950  loss = 5.25131 avg_loss = 4.21491\n",
      "epoch no.0 train no.10960  loss = 6.53453 avg_loss = 4.19873\n",
      "epoch no.0 train no.10970  loss = 3.22481 avg_loss = 4.18194\n",
      "epoch no.0 train no.10980  loss = 5.48340 avg_loss = 4.16528\n",
      "epoch no.0 train no.10990  loss = 3.28538 avg_loss = 4.17914\n",
      "epoch no.0 train no.11000  loss = 3.58571 avg_loss = 4.10128\n",
      "3\n",
      "to_tokens: ['▁가을', '▁좋은', '을', '▁필요할', '▁날', '</s>']\n",
      "기분전환이 필요한날</s>\n",
      "epoch no.0 train no.11010  loss = 4.05919 avg_loss = 4.12427\n",
      "epoch no.0 train no.11020  loss = 4.25389 avg_loss = 4.08204\n",
      "epoch no.0 train no.11030  loss = 3.37855 avg_loss = 4.10524\n",
      "epoch no.0 train no.11040  loss = 2.72564 avg_loss = 4.10134\n",
      "epoch no.0 train no.11050  loss = 2.89403 avg_loss = 4.06413\n",
      "epoch no.0 train no.11060  loss = 5.92354 avg_loss = 4.04609\n",
      "epoch no.0 train no.11070  loss = 3.46843 avg_loss = 4.02130\n",
      "epoch no.0 train no.11080  loss = 7.32782 avg_loss = 4.04777\n",
      "epoch no.0 train no.11090  loss = 4.41292 avg_loss = 4.06955\n",
      "epoch no.0 train no.11100  loss = 5.50612 avg_loss = 4.04331\n",
      "epoch no.0 train no.11110  loss = 3.13257 avg_loss = 4.05414\n",
      "epoch no.0 train no.11120  loss = 3.51442 avg_loss = 4.01738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.11130  loss = 4.43083 avg_loss = 4.01867\n",
      "epoch no.0 train no.11140  loss = 3.63115 avg_loss = 4.06869\n",
      "epoch no.0 train no.11150  loss = 3.26262 avg_loss = 4.04440\n",
      "epoch no.0 train no.11160  loss = 2.51605 avg_loss = 4.07465\n",
      "epoch no.0 train no.11170  loss = 4.52334 avg_loss = 4.10585\n",
      "epoch no.0 train no.11180  loss = 2.46212 avg_loss = 4.16580\n",
      "epoch no.0 train no.11190  loss = 4.98633 avg_loss = 4.18736\n",
      "epoch no.0 train no.11200  loss = 3.38504 avg_loss = 4.15910\n",
      "epoch no.0 train no.11210  loss = 5.48163 avg_loss = 4.18736\n",
      "epoch no.0 train no.11220  loss = 4.70106 avg_loss = 4.21697\n",
      "epoch no.0 train no.11230  loss = 4.50922 avg_loss = 4.18716\n",
      "epoch no.0 train no.11240  loss = 2.92138 avg_loss = 4.18214\n",
      "epoch no.0 train no.11250  loss = 3.95258 avg_loss = 4.20390\n",
      "epoch no.0 train no.11260  loss = 3.51418 avg_loss = 4.14343\n",
      "epoch no.0 train no.11270  loss = 5.45326 avg_loss = 4.14216\n",
      "epoch no.0 train no.11280  loss = 3.56174 avg_loss = 4.13800\n",
      "epoch no.0 train no.11290  loss = 6.92187 avg_loss = 4.17937\n",
      "epoch no.0 train no.11300  loss = 3.04048 avg_loss = 4.17404\n",
      "epoch no.0 train no.11310  loss = 4.29864 avg_loss = 4.21986\n",
      "epoch no.0 train no.11320  loss = 5.49956 avg_loss = 4.22031\n",
      "epoch no.0 train no.11330  loss = 7.44227 avg_loss = 4.27907\n",
      "epoch no.0 train no.11340  loss = 5.66775 avg_loss = 4.24070\n",
      "epoch no.0 train no.11350  loss = 3.52374 avg_loss = 4.28527\n",
      "epoch no.0 train no.11360  loss = 5.59885 avg_loss = 4.26784\n",
      "epoch no.0 train no.11370  loss = 3.40602 avg_loss = 4.21821\n",
      "epoch no.0 train no.11380  loss = 5.51968 avg_loss = 4.17725\n",
      "epoch no.0 train no.11390  loss = 3.42277 avg_loss = 4.20685\n",
      "epoch no.0 train no.11400  loss = 2.92576 avg_loss = 4.14597\n",
      "epoch no.0 train no.11410  loss = 4.80929 avg_loss = 4.12532\n",
      "epoch no.0 train no.11420  loss = 2.51032 avg_loss = 4.14859\n",
      "epoch no.0 train no.11430  loss = 4.93028 avg_loss = 4.14600\n",
      "epoch no.0 train no.11440  loss = 5.02486 avg_loss = 4.12821\n",
      "epoch no.0 train no.11450  loss = 6.60652 avg_loss = 4.16280\n",
      "epoch no.0 train no.11460  loss = 5.70049 avg_loss = 4.20431\n",
      "epoch no.0 train no.11470  loss = 3.74453 avg_loss = 4.16355\n",
      "epoch no.0 train no.11480  loss = 3.68640 avg_loss = 4.18972\n",
      "epoch no.0 train no.11490  loss = 4.61788 avg_loss = 4.18821\n",
      "epoch no.0 train no.11500  loss = 2.94948 avg_loss = 4.18518\n",
      "epoch no.0 train no.11510  loss = 4.19453 avg_loss = 4.22331\n",
      "epoch no.0 train no.11520  loss = 3.76970 avg_loss = 4.21949\n",
      "epoch no.0 train no.11530  loss = 4.63128 avg_loss = 4.23853\n",
      "epoch no.0 train no.11540  loss = 2.54730 avg_loss = 4.16609\n",
      "epoch no.0 train no.11550  loss = 4.90276 avg_loss = 4.19467\n",
      "epoch no.0 train no.11560  loss = 5.89420 avg_loss = 4.26742\n",
      "epoch no.0 train no.11570  loss = 5.83235 avg_loss = 4.31886\n",
      "epoch no.0 train no.11580  loss = 6.84237 avg_loss = 4.36622\n",
      "epoch no.0 train no.11590  loss = 5.32815 avg_loss = 4.34049\n",
      "epoch no.0 train no.11600  loss = 4.88281 avg_loss = 4.34009\n",
      "epoch no.0 train no.11610  loss = 5.17459 avg_loss = 4.38701\n",
      "epoch no.0 train no.11620  loss = 4.09381 avg_loss = 4.38342\n",
      "epoch no.0 train no.11630  loss = 2.95697 avg_loss = 4.36144\n",
      "epoch no.0 train no.11640  loss = 2.61275 avg_loss = 4.31982\n",
      "epoch no.0 train no.11650  loss = 3.54911 avg_loss = 4.30887\n",
      "epoch no.0 train no.11660  loss = 2.75204 avg_loss = 4.30609\n",
      "epoch no.0 train no.11670  loss = 2.60718 avg_loss = 4.25976\n",
      "epoch no.0 train no.11680  loss = 2.83537 avg_loss = 4.21080\n",
      "epoch no.0 train no.11690  loss = 4.03586 avg_loss = 4.23821\n",
      "epoch no.0 train no.11700  loss = 5.68348 avg_loss = 4.25396\n",
      "epoch no.0 train no.11710  loss = 4.33054 avg_loss = 4.20601\n",
      "epoch no.0 train no.11720  loss = 5.86463 avg_loss = 4.20430\n",
      "epoch no.0 train no.11730  loss = 5.29588 avg_loss = 4.16993\n",
      "epoch no.0 train no.11740  loss = 3.11294 avg_loss = 4.18295\n",
      "epoch no.0 train no.11750  loss = 2.83236 avg_loss = 4.19781\n",
      "epoch no.0 train no.11760  loss = 4.48049 avg_loss = 4.25284\n",
      "epoch no.0 train no.11770  loss = 4.06865 avg_loss = 4.25558\n",
      "epoch no.0 train no.11780  loss = 3.29544 avg_loss = 4.24325\n",
      "epoch no.0 train no.11790  loss = 3.75618 avg_loss = 4.27949\n",
      "epoch no.0 train no.11800  loss = 2.44942 avg_loss = 4.28082\n",
      "epoch no.0 train no.11810  loss = 3.61585 avg_loss = 4.27410\n",
      "epoch no.0 train no.11820  loss = 4.09410 avg_loss = 4.29752\n",
      "epoch no.0 train no.11830  loss = 4.80819 avg_loss = 4.27815\n",
      "epoch no.0 train no.11840  loss = 2.44079 avg_loss = 4.23041\n",
      "epoch no.0 train no.11850  loss = 3.43510 avg_loss = 4.23925\n",
      "epoch no.0 train no.11860  loss = 4.36711 avg_loss = 4.24330\n",
      "epoch no.0 train no.11870  loss = 5.35515 avg_loss = 4.28787\n",
      "epoch no.0 train no.11880  loss = 5.28674 avg_loss = 4.27407\n",
      "epoch no.0 train no.11890  loss = 4.29944 avg_loss = 4.29024\n",
      "epoch no.0 train no.11900  loss = 3.26540 avg_loss = 4.33209\n",
      "epoch no.0 train no.11910  loss = 4.37200 avg_loss = 4.28538\n",
      "epoch no.0 train no.11920  loss = 5.12577 avg_loss = 4.30055\n",
      "epoch no.0 train no.11930  loss = 3.71152 avg_loss = 4.24997\n",
      "epoch no.0 train no.11940  loss = 5.93928 avg_loss = 4.26229\n",
      "epoch no.0 train no.11950  loss = 3.24027 avg_loss = 4.20026\n",
      "epoch no.0 train no.11960  loss = 5.82242 avg_loss = 4.19588\n",
      "epoch no.0 train no.11970  loss = 4.12993 avg_loss = 4.22925\n",
      "epoch no.0 train no.11980  loss = 3.78687 avg_loss = 4.25262\n",
      "epoch no.0 train no.11990  loss = 2.91370 avg_loss = 4.26644\n",
      "epoch no.0 train no.12000  loss = 5.95428 avg_loss = 4.27024\n",
      "4\n",
      "to_tokens: ['▁라디오', '▁좋은', '에', '▁필요할', '▁날', '에게', '</s>']\n",
      "기분전환이 필요한 당신에게</s>\n",
      "epoch no.0 train no.12010  loss = 3.34061 avg_loss = 4.22236\n",
      "epoch no.0 train no.12020  loss = 3.25444 avg_loss = 4.20277\n",
      "epoch no.0 train no.12030  loss = 4.29043 avg_loss = 4.20632\n",
      "epoch no.0 train no.12040  loss = 3.79456 avg_loss = 4.21430\n",
      "epoch no.0 train no.12050  loss = 4.01440 avg_loss = 4.19126\n",
      "epoch no.0 train no.12060  loss = 3.71744 avg_loss = 4.14993\n",
      "epoch no.0 train no.12070  loss = 4.10299 avg_loss = 4.17954\n",
      "epoch no.0 train no.12080  loss = 5.34020 avg_loss = 4.14960\n",
      "epoch no.0 train no.12090  loss = 2.37375 avg_loss = 4.17765\n",
      "epoch no.0 train no.12100  loss = 5.22892 avg_loss = 4.15071\n",
      "epoch no.0 train no.12110  loss = 3.76390 avg_loss = 4.14895\n",
      "epoch no.0 train no.12120  loss = 6.95319 avg_loss = 4.16669\n",
      "epoch no.0 train no.12130  loss = 2.37229 avg_loss = 4.16063\n",
      "epoch no.0 train no.12140  loss = 5.45149 avg_loss = 4.12691\n",
      "epoch no.0 train no.12150  loss = 4.12859 avg_loss = 4.19198\n",
      "epoch no.0 train no.12160  loss = 4.18222 avg_loss = 4.15996\n",
      "epoch no.0 train no.12170  loss = 3.86704 avg_loss = 4.18987\n",
      "epoch no.0 train no.12180  loss = 4.78904 avg_loss = 4.21365\n",
      "epoch no.0 train no.12190  loss = 3.25436 avg_loss = 4.17371\n",
      "epoch no.0 train no.12200  loss = 5.06929 avg_loss = 4.22395\n",
      "epoch no.0 train no.12210  loss = 4.58128 avg_loss = 4.21464\n",
      "epoch no.0 train no.12220  loss = 5.48495 avg_loss = 4.22564\n",
      "epoch no.0 train no.12230  loss = 3.38244 avg_loss = 4.30191\n",
      "epoch no.0 train no.12240  loss = 3.89437 avg_loss = 4.25597\n",
      "epoch no.0 train no.12250  loss = 3.22870 avg_loss = 4.25196\n",
      "epoch no.0 train no.12260  loss = 5.13753 avg_loss = 4.26816\n",
      "epoch no.0 train no.12270  loss = 6.21400 avg_loss = 4.23429\n",
      "epoch no.0 train no.12280  loss = 5.78136 avg_loss = 4.31640\n",
      "epoch no.0 train no.12290  loss = 7.40159 avg_loss = 4.36110\n",
      "epoch no.0 train no.12300  loss = 3.89505 avg_loss = 4.39621\n",
      "epoch no.0 train no.12310  loss = 4.40330 avg_loss = 4.39390\n",
      "epoch no.0 train no.12320  loss = 5.41642 avg_loss = 4.36089\n",
      "epoch no.0 train no.12330  loss = 2.72446 avg_loss = 4.32828\n",
      "epoch no.0 train no.12340  loss = 5.81567 avg_loss = 4.33843\n",
      "epoch no.0 train no.12350  loss = 5.91637 avg_loss = 4.32659\n",
      "epoch no.0 train no.12360  loss = 2.31009 avg_loss = 4.28210\n",
      "epoch no.0 train no.12370  loss = 5.52930 avg_loss = 4.27653\n",
      "epoch no.0 train no.12380  loss = 3.04195 avg_loss = 4.27467\n",
      "epoch no.0 train no.12390  loss = 5.32487 avg_loss = 4.26152\n",
      "epoch no.0 train no.12400  loss = 2.91316 avg_loss = 4.23525\n",
      "epoch no.0 train no.12410  loss = 4.73596 avg_loss = 4.22691\n",
      "epoch no.0 train no.12420  loss = 4.70772 avg_loss = 4.19515\n",
      "epoch no.0 train no.12430  loss = 4.18653 avg_loss = 4.16901\n",
      "epoch no.0 train no.12440  loss = 5.72619 avg_loss = 4.24144\n",
      "epoch no.0 train no.12450  loss = 4.66989 avg_loss = 4.28661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.12460  loss = 3.05345 avg_loss = 4.28419\n",
      "epoch no.0 train no.12470  loss = 3.20701 avg_loss = 4.26641\n",
      "epoch no.0 train no.12480  loss = 4.70034 avg_loss = 4.24889\n",
      "epoch no.0 train no.12490  loss = 4.09852 avg_loss = 4.25117\n",
      "epoch no.0 train no.12500  loss = 2.54287 avg_loss = 4.24893\n",
      "epoch no.0 train no.12510  loss = 3.36874 avg_loss = 4.20267\n",
      "epoch no.0 train no.12520  loss = 3.94276 avg_loss = 4.24392\n",
      "epoch no.0 train no.12530  loss = 4.48871 avg_loss = 4.27107\n",
      "epoch no.0 train no.12540  loss = 2.62251 avg_loss = 4.21676\n",
      "epoch no.0 train no.12550  loss = 5.97822 avg_loss = 4.22399\n",
      "epoch no.0 train no.12560  loss = 4.26478 avg_loss = 4.26256\n",
      "epoch no.0 train no.12570  loss = 3.37417 avg_loss = 4.23415\n",
      "epoch no.0 train no.12580  loss = 6.68700 avg_loss = 4.30288\n",
      "epoch no.0 train no.12590  loss = 3.24514 avg_loss = 4.27383\n",
      "epoch no.0 train no.12600  loss = 2.70459 avg_loss = 4.24331\n",
      "epoch no.0 train no.12610  loss = 4.36181 avg_loss = 4.21232\n",
      "epoch no.0 train no.12620  loss = 3.40043 avg_loss = 4.17909\n",
      "epoch no.0 train no.12630  loss = 4.38343 avg_loss = 4.17948\n",
      "epoch no.0 train no.12640  loss = 4.68726 avg_loss = 4.20264\n",
      "epoch no.0 train no.12650  loss = 2.85192 avg_loss = 4.20514\n",
      "epoch no.0 train no.12660  loss = 3.40056 avg_loss = 4.16988\n",
      "epoch no.0 train no.12670  loss = 4.43559 avg_loss = 4.23199\n",
      "epoch no.0 train no.12680  loss = 4.89889 avg_loss = 4.25236\n",
      "epoch no.0 train no.12690  loss = 4.20239 avg_loss = 4.17403\n",
      "epoch no.0 train no.12700  loss = 5.81695 avg_loss = 4.21219\n",
      "epoch no.0 train no.12710  loss = 4.79149 avg_loss = 4.10631\n",
      "epoch no.0 train no.12720  loss = 3.71012 avg_loss = 4.13385\n",
      "epoch no.0 train no.12730  loss = 4.70777 avg_loss = 4.19139\n",
      "epoch no.0 train no.12740  loss = 4.26540 avg_loss = 4.18674\n",
      "epoch no.0 train no.12750  loss = 5.12833 avg_loss = 4.19211\n",
      "epoch no.0 train no.12760  loss = 3.58004 avg_loss = 4.16882\n",
      "epoch no.0 train no.12770  loss = 5.68813 avg_loss = 4.16704\n",
      "epoch no.0 train no.12780  loss = 4.06138 avg_loss = 4.10503\n",
      "epoch no.0 train no.12790  loss = 4.11379 avg_loss = 4.11329\n",
      "epoch no.0 train no.12800  loss = 4.15414 avg_loss = 4.09546\n",
      "epoch no.0 train no.12810  loss = 5.82164 avg_loss = 4.17837\n",
      "epoch no.0 train no.12820  loss = 4.91220 avg_loss = 4.13778\n",
      "epoch no.0 train no.12830  loss = 6.07385 avg_loss = 4.12281\n",
      "epoch no.0 train no.12840  loss = 4.89537 avg_loss = 4.13546\n",
      "epoch no.0 train no.12850  loss = 2.56224 avg_loss = 4.13843\n",
      "epoch no.0 train no.12860  loss = 3.47906 avg_loss = 4.17693\n",
      "epoch no.0 train no.12870  loss = 5.81435 avg_loss = 4.12902\n",
      "epoch no.0 train no.12880  loss = 3.08211 avg_loss = 4.19038\n",
      "epoch no.0 train no.12890  loss = 6.89956 avg_loss = 4.22694\n",
      "epoch no.0 train no.12900  loss = 4.89413 avg_loss = 4.22659\n",
      "epoch no.0 train no.12910  loss = 4.30968 avg_loss = 4.20408\n",
      "epoch no.0 train no.12920  loss = 2.71241 avg_loss = 4.28850\n",
      "epoch no.0 train no.12930  loss = 3.97291 avg_loss = 4.28418\n",
      "epoch no.0 train no.12940  loss = 3.24334 avg_loss = 4.31234\n",
      "epoch no.0 train no.12950  loss = 3.27053 avg_loss = 4.28151\n",
      "epoch no.0 train no.12960  loss = 2.31337 avg_loss = 4.30899\n",
      "epoch no.0 train no.12970  loss = 3.54847 avg_loss = 4.29526\n",
      "epoch no.0 train no.12980  loss = 4.89970 avg_loss = 4.32618\n",
      "epoch no.0 train no.12990  loss = 4.07421 avg_loss = 4.30168\n",
      "epoch no.0 train no.13000  loss = 4.28114 avg_loss = 4.29104\n",
      "0\n",
      "to_tokens: ['▁', '전환', '에']\n",
      "기분전환</s>\n",
      "epoch no.0 train no.13010  loss = 3.02808 avg_loss = 4.25707\n",
      "epoch no.0 train no.13020  loss = 2.11922 avg_loss = 4.23082\n",
      "epoch no.0 train no.13030  loss = 2.73535 avg_loss = 4.15665\n",
      "epoch no.0 train no.13040  loss = 3.27653 avg_loss = 4.19529\n",
      "epoch no.0 train no.13050  loss = 2.90091 avg_loss = 4.19854\n",
      "epoch no.0 train no.13060  loss = 3.05250 avg_loss = 4.22614\n",
      "epoch no.0 train no.13070  loss = 3.80444 avg_loss = 4.19672\n",
      "epoch no.0 train no.13080  loss = 6.03283 avg_loss = 4.18613\n",
      "epoch no.0 train no.13090  loss = 4.14988 avg_loss = 4.20153\n",
      "epoch no.0 train no.13100  loss = 3.90647 avg_loss = 4.25175\n",
      "epoch no.0 train no.13110  loss = 3.15360 avg_loss = 4.21176\n",
      "epoch no.0 train no.13120  loss = 4.93832 avg_loss = 4.15788\n",
      "epoch no.0 train no.13130  loss = 3.78510 avg_loss = 4.19142\n",
      "epoch no.0 train no.13140  loss = 4.65897 avg_loss = 4.19641\n",
      "epoch no.0 train no.13150  loss = 5.56068 avg_loss = 4.21100\n",
      "epoch no.0 train no.13160  loss = 5.41094 avg_loss = 4.19110\n",
      "epoch no.0 train no.13170  loss = 3.36928 avg_loss = 4.16435\n",
      "epoch no.0 train no.13180  loss = 6.09035 avg_loss = 4.14085\n",
      "epoch no.0 train no.13190  loss = 5.29272 avg_loss = 4.16223\n",
      "epoch no.0 train no.13200  loss = 4.78288 avg_loss = 4.10845\n",
      "epoch no.0 train no.13210  loss = 4.49532 avg_loss = 4.12346\n",
      "epoch no.0 train no.13220  loss = 3.81014 avg_loss = 4.14995\n",
      "epoch no.0 train no.13230  loss = 3.91630 avg_loss = 4.16374\n",
      "epoch no.0 train no.13240  loss = 5.30498 avg_loss = 4.21918\n",
      "epoch no.0 train no.13250  loss = 4.23154 avg_loss = 4.16932\n",
      "epoch no.0 train no.13260  loss = 4.09395 avg_loss = 4.19149\n",
      "epoch no.0 train no.13270  loss = 4.47783 avg_loss = 4.15915\n",
      "epoch no.0 train no.13280  loss = 3.39814 avg_loss = 4.20716\n",
      "epoch no.0 train no.13290  loss = 1.96949 avg_loss = 4.22958\n",
      "epoch no.0 train no.13300  loss = 3.13903 avg_loss = 4.28786\n",
      "epoch no.0 train no.13310  loss = 2.95467 avg_loss = 4.32705\n",
      "epoch no.0 train no.13320  loss = 6.78222 avg_loss = 4.42380\n",
      "epoch no.0 train no.13330  loss = 3.82672 avg_loss = 4.39028\n",
      "epoch no.0 train no.13340  loss = 5.60713 avg_loss = 4.38598\n",
      "epoch no.0 train no.13350  loss = 4.16462 avg_loss = 4.30308\n",
      "epoch no.0 train no.13360  loss = 4.11349 avg_loss = 4.35514\n",
      "epoch no.0 train no.13370  loss = 3.84258 avg_loss = 4.37278\n",
      "epoch no.0 train no.13380  loss = 2.66752 avg_loss = 4.33139\n",
      "epoch no.0 train no.13390  loss = 4.97489 avg_loss = 4.36794\n",
      "epoch no.0 train no.13400  loss = 3.52091 avg_loss = 4.37751\n",
      "epoch no.0 train no.13410  loss = 5.51499 avg_loss = 4.37807\n",
      "epoch no.0 train no.13420  loss = 5.59963 avg_loss = 4.33473\n",
      "epoch no.0 train no.13430  loss = 4.03536 avg_loss = 4.32702\n",
      "epoch no.0 train no.13440  loss = 6.33313 avg_loss = 4.37306\n",
      "epoch no.0 train no.13450  loss = 5.99550 avg_loss = 4.35978\n",
      "epoch no.0 train no.13460  loss = 3.43819 avg_loss = 4.36642\n",
      "epoch no.0 train no.13470  loss = 3.81576 avg_loss = 4.33100\n",
      "epoch no.0 train no.13480  loss = 5.29382 avg_loss = 4.34440\n",
      "epoch no.0 train no.13490  loss = 2.64577 avg_loss = 4.32244\n",
      "epoch no.0 train no.13500  loss = 3.02927 avg_loss = 4.29248\n",
      "epoch no.0 train no.13510  loss = 4.61165 avg_loss = 4.27632\n",
      "epoch no.0 train no.13520  loss = 3.14863 avg_loss = 4.24341\n",
      "epoch no.0 train no.13530  loss = 7.26419 avg_loss = 4.27688\n",
      "epoch no.0 train no.13540  loss = 3.64393 avg_loss = 4.27919\n",
      "epoch no.0 train no.13550  loss = 3.08698 avg_loss = 4.30008\n",
      "epoch no.0 train no.13560  loss = 3.62716 avg_loss = 4.27115\n",
      "epoch no.0 train no.13570  loss = 3.96014 avg_loss = 4.22568\n",
      "epoch no.0 train no.13580  loss = 3.12357 avg_loss = 4.22161\n",
      "epoch no.0 train no.13590  loss = 2.95278 avg_loss = 4.28362\n",
      "epoch no.0 train no.13600  loss = 3.55254 avg_loss = 4.22323\n",
      "epoch no.0 train no.13610  loss = 3.58033 avg_loss = 4.20363\n",
      "epoch no.0 train no.13620  loss = 2.63494 avg_loss = 4.18027\n",
      "epoch no.0 train no.13630  loss = 4.68501 avg_loss = 4.15217\n",
      "epoch no.0 train no.13640  loss = 3.69944 avg_loss = 4.19815\n",
      "epoch no.0 train no.13650  loss = 4.91233 avg_loss = 4.20734\n",
      "epoch no.0 train no.13660  loss = 7.13967 avg_loss = 4.23808\n",
      "epoch no.0 train no.13670  loss = 4.19416 avg_loss = 4.24161\n",
      "epoch no.0 train no.13680  loss = 3.32370 avg_loss = 4.22817\n",
      "epoch no.0 train no.13690  loss = 2.96769 avg_loss = 4.20928\n",
      "epoch no.0 train no.13700  loss = 3.95324 avg_loss = 4.23185\n",
      "epoch no.0 train no.13710  loss = 4.91371 avg_loss = 4.22419\n",
      "epoch no.0 train no.13720  loss = 4.78219 avg_loss = 4.19974\n",
      "epoch no.0 train no.13730  loss = 4.39647 avg_loss = 4.22975\n",
      "epoch no.0 train no.13740  loss = 5.99938 avg_loss = 4.29410\n",
      "epoch no.0 train no.13750  loss = 4.27862 avg_loss = 4.28824\n",
      "epoch no.0 train no.13760  loss = 2.31269 avg_loss = 4.23573\n",
      "epoch no.0 train no.13770  loss = 3.63521 avg_loss = 4.25748\n",
      "epoch no.0 train no.13780  loss = 3.24667 avg_loss = 4.23348\n",
      "epoch no.0 train no.13790  loss = 3.10740 avg_loss = 4.28707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.13800  loss = 3.12155 avg_loss = 4.32754\n",
      "epoch no.0 train no.13810  loss = 4.24398 avg_loss = 4.38167\n",
      "epoch no.0 train no.13820  loss = 4.73412 avg_loss = 4.36486\n",
      "epoch no.0 train no.13830  loss = 3.69562 avg_loss = 4.34005\n",
      "epoch no.0 train no.13840  loss = 5.83904 avg_loss = 4.40688\n",
      "epoch no.0 train no.13850  loss = 5.23295 avg_loss = 4.37893\n",
      "epoch no.0 train no.13860  loss = 4.32299 avg_loss = 4.30715\n",
      "epoch no.0 train no.13870  loss = 2.90561 avg_loss = 4.27899\n",
      "epoch no.0 train no.13880  loss = 5.65336 avg_loss = 4.27918\n",
      "epoch no.0 train no.13890  loss = 3.77563 avg_loss = 4.28677\n",
      "epoch no.0 train no.13900  loss = 4.93873 avg_loss = 4.35997\n",
      "epoch no.0 train no.13910  loss = 5.46908 avg_loss = 4.41318\n",
      "epoch no.0 train no.13920  loss = 4.10423 avg_loss = 4.39814\n",
      "epoch no.0 train no.13930  loss = 7.15761 avg_loss = 4.40450\n",
      "epoch no.0 train no.13940  loss = 3.26496 avg_loss = 4.33748\n",
      "epoch no.0 train no.13950  loss = 4.09012 avg_loss = 4.32514\n",
      "epoch no.0 train no.13960  loss = 5.09496 avg_loss = 4.33239\n",
      "epoch no.0 train no.13970  loss = 3.52692 avg_loss = 4.32754\n",
      "epoch no.0 train no.13980  loss = 4.95789 avg_loss = 4.34895\n",
      "epoch no.0 train no.13990  loss = 1.95126 avg_loss = 4.31308\n",
      "epoch no.0 train no.14000  loss = 4.77335 avg_loss = 4.31135\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.0 train no.14010  loss = 6.59301 avg_loss = 4.39789\n",
      "epoch no.0 train no.14020  loss = 3.75002 avg_loss = 4.34821\n",
      "epoch no.0 train no.14030  loss = 2.61306 avg_loss = 4.34411\n",
      "epoch no.0 train no.14040  loss = 5.50297 avg_loss = 4.36349\n",
      "epoch no.0 train no.14050  loss = 2.95991 avg_loss = 4.36245\n",
      "epoch no.0 train no.14060  loss = 3.03718 avg_loss = 4.35398\n",
      "epoch no.0 train no.14070  loss = 4.09825 avg_loss = 4.32218\n",
      "epoch no.0 train no.14080  loss = 2.35978 avg_loss = 4.29337\n",
      "epoch no.0 train no.14090  loss = 4.83927 avg_loss = 4.29134\n",
      "epoch no.0 train no.14100  loss = 4.42701 avg_loss = 4.29699\n",
      "epoch no.0 train no.14110  loss = 2.61272 avg_loss = 4.25351\n",
      "epoch no.0 train no.14120  loss = 5.90960 avg_loss = 4.25532\n",
      "epoch no.0 train no.14130  loss = 3.77632 avg_loss = 4.19731\n",
      "epoch no.0 train no.14140  loss = 2.94552 avg_loss = 4.14518\n",
      "epoch no.0 train no.14150  loss = 4.03097 avg_loss = 4.12935\n",
      "epoch no.0 train no.14160  loss = 3.91831 avg_loss = 4.11441\n",
      "epoch no.0 train no.14170  loss = 6.04682 avg_loss = 4.11027\n",
      "epoch no.0 train no.14180  loss = 3.49075 avg_loss = 4.12003\n",
      "epoch no.0 train no.14190  loss = 4.96638 avg_loss = 4.12082\n",
      "epoch no.0 train no.14200  loss = 2.22909 avg_loss = 4.07478\n",
      "epoch no.0 train no.14210  loss = 5.37255 avg_loss = 4.09434\n",
      "epoch no.0 train no.14220  loss = 3.25310 avg_loss = 4.07136\n",
      "epoch no.0 train no.14230  loss = 5.34465 avg_loss = 4.08777\n",
      "epoch no.0 train no.14240  loss = 3.59938 avg_loss = 4.14539\n",
      "epoch no.0 train no.14250  loss = 3.12000 avg_loss = 4.14995\n",
      "epoch no.0 train no.14260  loss = 3.95131 avg_loss = 4.14083\n",
      "epoch no.0 train no.14270  loss = 3.37146 avg_loss = 4.15825\n",
      "epoch no.0 train no.14280  loss = 4.68776 avg_loss = 4.17940\n",
      "epoch no.0 train no.14290  loss = 4.12657 avg_loss = 4.28054\n",
      "epoch no.0 train no.14300  loss = 4.72606 avg_loss = 4.28859\n",
      "epoch no.0 train no.14310  loss = 3.01949 avg_loss = 4.23139\n",
      "epoch no.0 train no.14320  loss = 4.91328 avg_loss = 4.22654\n",
      "epoch no.0 train no.14330  loss = 5.14982 avg_loss = 4.21963\n",
      "epoch no.0 train no.14340  loss = 4.80170 avg_loss = 4.26159\n",
      "epoch no.0 train no.14350  loss = 3.14798 avg_loss = 4.23718\n",
      "epoch no.0 train no.14360  loss = 3.01132 avg_loss = 4.25316\n",
      "epoch no.0 train no.14370  loss = 5.09852 avg_loss = 4.30059\n",
      "epoch no.0 train no.14380  loss = 3.93706 avg_loss = 4.24384\n",
      "epoch no.0 train no.14390  loss = 1.96341 avg_loss = 4.24022\n",
      "epoch no.0 train no.14400  loss = 3.08722 avg_loss = 4.17778\n",
      "epoch no.0 train no.14410  loss = 4.90663 avg_loss = 4.21787\n",
      "epoch no.0 train no.14420  loss = 2.82347 avg_loss = 4.20623\n",
      "epoch no.0 train no.14430  loss = 5.65548 avg_loss = 4.22193\n",
      "epoch no.0 train no.14440  loss = 4.19952 avg_loss = 4.17176\n",
      "epoch no.0 train no.14450  loss = 2.17184 avg_loss = 4.12097\n",
      "epoch no.0 train no.14460  loss = 2.38473 avg_loss = 4.11108\n",
      "epoch no.0 train no.14470  loss = 2.83924 avg_loss = 4.13342\n",
      "epoch no.0 train no.14480  loss = 3.64873 avg_loss = 4.19861\n",
      "epoch no.0 train no.14490  loss = 4.43508 avg_loss = 4.18255\n",
      "epoch no.0 train no.14500  loss = 5.96811 avg_loss = 4.22765\n",
      "epoch no.0 train no.14510  loss = 5.15488 avg_loss = 4.26034\n",
      "epoch no.0 train no.14520  loss = 3.78332 avg_loss = 4.26541\n",
      "epoch no.0 train no.14530  loss = 3.46150 avg_loss = 4.22073\n",
      "epoch no.0 train no.14540  loss = 4.49302 avg_loss = 4.22433\n",
      "epoch no.0 train no.14550  loss = 4.27860 avg_loss = 4.21983\n",
      "epoch no.0 train no.14560  loss = 3.21542 avg_loss = 4.24836\n",
      "epoch no.0 train no.14570  loss = 4.03348 avg_loss = 4.27223\n",
      "epoch no.0 train no.14580  loss = 4.85664 avg_loss = 4.26520\n",
      "epoch no.0 train no.14590  loss = 3.20271 avg_loss = 4.24681\n",
      "epoch no.0 train no.14600  loss = 1.84985 avg_loss = 4.20215\n",
      "epoch no.0 train no.14610  loss = 3.62716 avg_loss = 4.17833\n",
      "epoch no.0 train no.14620  loss = 3.34184 avg_loss = 4.14674\n",
      "epoch no.0 train no.14630  loss = 5.98175 avg_loss = 4.11792\n",
      "epoch no.0 train no.14640  loss = 3.29052 avg_loss = 4.10161\n",
      "epoch no.0 train no.14650  loss = 1.86421 avg_loss = 4.08260\n",
      "epoch no.0 train no.14660  loss = 2.76808 avg_loss = 4.13041\n",
      "epoch no.0 train no.14670  loss = 3.16603 avg_loss = 4.10379\n",
      "epoch no.0 train no.14680  loss = 1.90359 avg_loss = 4.07057\n",
      "epoch no.0 train no.14690  loss = 3.11369 avg_loss = 3.97714\n",
      "epoch no.0 train no.14700  loss = 3.38866 avg_loss = 3.91784\n",
      "epoch no.0 train no.14710  loss = 4.25131 avg_loss = 3.93927\n",
      "epoch no.0 train no.14720  loss = 2.82327 avg_loss = 3.93330\n",
      "epoch no.0 train no.14730  loss = 5.04941 avg_loss = 3.99778\n",
      "epoch no.0 train no.14740  loss = 7.36427 avg_loss = 4.00399\n",
      "epoch no.0 train no.14750  loss = 3.37407 avg_loss = 3.96152\n",
      "epoch no.0 train no.14760  loss = 4.78180 avg_loss = 4.09710\n",
      "epoch no.0 train no.14770  loss = 4.78365 avg_loss = 4.03906\n",
      "epoch no.0 train no.14780  loss = 2.96657 avg_loss = 4.05932\n",
      "epoch no.0 train no.14790  loss = 8.74164 avg_loss = 4.10815\n",
      "epoch no.0 train no.14800  loss = 4.43553 avg_loss = 4.13107\n",
      "epoch no.0 train no.14810  loss = 3.81268 avg_loss = 4.18178\n",
      "epoch no.0 train no.14820  loss = 6.88720 avg_loss = 4.18083\n",
      "epoch no.0 train no.14830  loss = 3.21616 avg_loss = 4.14972\n",
      "epoch no.0 train no.14840  loss = 4.76513 avg_loss = 4.16872\n",
      "epoch no.0 train no.14850  loss = 4.83561 avg_loss = 4.13911\n",
      "epoch no.0 train no.14860  loss = 4.73759 avg_loss = 4.18091\n",
      "epoch no.0 train no.14870  loss = 3.81618 avg_loss = 4.18747\n",
      "epoch no.0 train no.14880  loss = 6.43346 avg_loss = 4.22316\n",
      "epoch no.0 train no.14890  loss = 2.30074 avg_loss = 4.15281\n",
      "epoch no.0 train no.14900  loss = 6.36940 avg_loss = 4.19733\n",
      "epoch no.0 train no.14910  loss = 5.74592 avg_loss = 4.21547\n",
      "epoch no.0 train no.14920  loss = 1.94312 avg_loss = 4.16849\n",
      "epoch no.0 train no.14930  loss = 4.20145 avg_loss = 4.21267\n",
      "epoch no.0 train no.14940  loss = 4.27153 avg_loss = 4.18048\n",
      "epoch no.0 train no.14950  loss = 4.26513 avg_loss = 4.12509\n",
      "epoch no.0 train no.14960  loss = 3.40857 avg_loss = 4.12211\n",
      "epoch no.0 train no.14970  loss = 3.94788 avg_loss = 4.22798\n",
      "epoch no.0 train no.14980  loss = 3.31425 avg_loss = 4.21598\n",
      "epoch no.0 train no.14990  loss = 2.58371 avg_loss = 4.14982\n",
      "epoch no.0 train no.15000  loss = 4.58717 avg_loss = 4.11088\n",
      "7\n",
      "to_tokens: ['▁라디오', '▁좋은', '용', '▁필요할', '때', '▁듣는', '▁노래', '힙', '합', '</s>']\n",
      "기분전환이 필요할 때 듣는 감성힙합</s>\n",
      "epoch no.0 train no.15010  loss = 4.01598 avg_loss = 4.10155\n",
      "epoch no.0 train no.15020  loss = 7.24989 avg_loss = 4.11501\n",
      "epoch no.0 train no.15030  loss = 5.88359 avg_loss = 4.12493\n",
      "epoch no.0 train no.15040  loss = 4.15117 avg_loss = 4.16189\n",
      "epoch no.0 train no.15050  loss = 4.70436 avg_loss = 4.15442\n",
      "epoch no.0 train no.15060  loss = 2.08872 avg_loss = 4.14016\n",
      "epoch no.0 train no.15070  loss = 3.92432 avg_loss = 4.11593\n",
      "epoch no.0 train no.15080  loss = 3.70386 avg_loss = 4.07317\n",
      "epoch no.0 train no.15090  loss = 6.35955 avg_loss = 4.06916\n",
      "epoch no.0 train no.15100  loss = 2.24427 avg_loss = 4.01563\n",
      "epoch no.0 train no.15110  loss = 2.78083 avg_loss = 4.06282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.15120  loss = 3.46429 avg_loss = 4.05663\n",
      "epoch no.0 train no.15130  loss = 8.68820 avg_loss = 4.13389\n",
      "epoch no.0 train no.15140  loss = 2.78779 avg_loss = 4.05536\n",
      "epoch no.0 train no.15150  loss = 5.15700 avg_loss = 4.09839\n",
      "epoch no.0 train no.15160  loss = 3.16885 avg_loss = 4.14076\n",
      "epoch no.0 train no.15170  loss = 4.20556 avg_loss = 4.15202\n",
      "epoch no.0 train no.15180  loss = 5.52101 avg_loss = 4.10852\n",
      "epoch no.0 train no.15190  loss = 3.72747 avg_loss = 4.17015\n",
      "epoch no.0 train no.15200  loss = 5.47691 avg_loss = 4.15729\n",
      "epoch no.0 train no.15210  loss = 3.34737 avg_loss = 4.16503\n",
      "epoch no.0 train no.15220  loss = 5.73369 avg_loss = 4.19113\n",
      "epoch no.0 train no.15230  loss = 4.19145 avg_loss = 4.11507\n",
      "epoch no.0 train no.15240  loss = 4.63021 avg_loss = 4.19834\n",
      "epoch no.0 train no.15250  loss = 3.99225 avg_loss = 4.17069\n",
      "epoch no.0 train no.15260  loss = 5.16240 avg_loss = 4.21011\n",
      "epoch no.0 train no.15270  loss = 5.65891 avg_loss = 4.20015\n",
      "epoch no.0 train no.15280  loss = 3.16470 avg_loss = 4.17448\n",
      "epoch no.0 train no.15290  loss = 6.45974 avg_loss = 4.14048\n",
      "epoch no.0 train no.15300  loss = 4.84709 avg_loss = 4.08801\n",
      "epoch no.0 train no.15310  loss = 3.04138 avg_loss = 4.10430\n",
      "epoch no.0 train no.15320  loss = 1.67927 avg_loss = 4.04803\n",
      "epoch no.0 train no.15330  loss = 3.45022 avg_loss = 4.06889\n",
      "epoch no.0 train no.15340  loss = 4.11927 avg_loss = 4.08304\n",
      "epoch no.0 train no.15350  loss = 2.62167 avg_loss = 4.07729\n",
      "epoch no.0 train no.15360  loss = 4.83770 avg_loss = 4.06951\n",
      "epoch no.0 train no.15370  loss = 3.46247 avg_loss = 4.12085\n",
      "epoch no.0 train no.15380  loss = 4.48519 avg_loss = 4.05790\n",
      "epoch no.0 train no.15390  loss = 1.85678 avg_loss = 4.10708\n",
      "epoch no.0 train no.15400  loss = 6.40758 avg_loss = 4.15050\n",
      "epoch no.0 train no.15410  loss = 4.52494 avg_loss = 4.07795\n",
      "epoch no.0 train no.15420  loss = 2.11208 avg_loss = 4.06345\n",
      "epoch no.0 train no.15430  loss = 4.01307 avg_loss = 4.07851\n",
      "epoch no.0 train no.15440  loss = 4.72887 avg_loss = 4.12197\n",
      "epoch no.0 train no.15450  loss = 4.11475 avg_loss = 4.09103\n",
      "epoch no.0 train no.15460  loss = 4.42755 avg_loss = 4.12375\n",
      "epoch no.0 train no.15470  loss = 4.16752 avg_loss = 4.14368\n",
      "epoch no.0 train no.15480  loss = 6.16432 avg_loss = 4.13773\n",
      "epoch no.0 train no.15490  loss = 4.75814 avg_loss = 4.15510\n",
      "epoch no.0 train no.15500  loss = 3.22432 avg_loss = 4.14853\n",
      "epoch no.0 train no.15510  loss = 6.85845 avg_loss = 4.14108\n",
      "epoch no.0 train no.15520  loss = 5.16954 avg_loss = 4.13537\n",
      "epoch no.0 train no.15530  loss = 3.78700 avg_loss = 4.13661\n",
      "epoch no.0 train no.15540  loss = 4.08543 avg_loss = 4.16533\n",
      "epoch no.0 train no.15550  loss = 3.50661 avg_loss = 4.19022\n",
      "epoch no.0 train no.15560  loss = 3.88598 avg_loss = 4.24259\n",
      "epoch no.0 train no.15570  loss = 4.61041 avg_loss = 4.20463\n",
      "epoch no.0 train no.15580  loss = 1.57403 avg_loss = 4.20449\n",
      "epoch no.0 train no.15590  loss = 5.63422 avg_loss = 4.20452\n",
      "epoch no.0 train no.15600  loss = 4.01971 avg_loss = 4.19550\n",
      "epoch no.0 train no.15610  loss = 4.94098 avg_loss = 4.21261\n",
      "epoch no.0 train no.15620  loss = 5.61892 avg_loss = 4.21248\n",
      "epoch no.0 train no.15630  loss = 2.10477 avg_loss = 4.16130\n",
      "epoch no.0 train no.15640  loss = 6.02131 avg_loss = 4.25881\n",
      "epoch no.0 train no.15650  loss = 4.60470 avg_loss = 4.20110\n",
      "epoch no.0 train no.15660  loss = 2.21236 avg_loss = 4.19846\n",
      "epoch no.0 train no.15670  loss = 4.39566 avg_loss = 4.21060\n",
      "epoch no.0 train no.15680  loss = 3.30754 avg_loss = 4.14953\n",
      "epoch no.0 train no.15690  loss = 4.44843 avg_loss = 4.16644\n",
      "epoch no.0 train no.15700  loss = 3.39907 avg_loss = 4.08456\n",
      "epoch no.0 train no.15710  loss = 7.34294 avg_loss = 4.13876\n",
      "epoch no.0 train no.15720  loss = 3.28267 avg_loss = 4.13270\n",
      "epoch no.0 train no.15730  loss = 3.28524 avg_loss = 4.10148\n",
      "epoch no.0 train no.15740  loss = 2.57760 avg_loss = 4.07006\n",
      "epoch no.0 train no.15750  loss = 7.66502 avg_loss = 4.13265\n",
      "epoch no.0 train no.15760  loss = 7.97916 avg_loss = 4.18365\n",
      "epoch no.0 train no.15770  loss = 4.20743 avg_loss = 4.20917\n",
      "epoch no.0 train no.15780  loss = 4.96054 avg_loss = 4.18752\n",
      "epoch no.0 train no.15790  loss = 5.19509 avg_loss = 4.22495\n",
      "epoch no.0 train no.15800  loss = 6.60069 avg_loss = 4.27430\n",
      "epoch no.0 train no.15810  loss = 3.69104 avg_loss = 4.25235\n",
      "epoch no.0 train no.15820  loss = 4.68453 avg_loss = 4.25713\n",
      "epoch no.0 train no.15830  loss = 2.78101 avg_loss = 4.23594\n",
      "epoch no.0 train no.15840  loss = 2.51778 avg_loss = 4.19965\n",
      "epoch no.0 train no.15850  loss = 8.72333 avg_loss = 4.22537\n",
      "epoch no.0 train no.15860  loss = 2.60856 avg_loss = 4.18694\n",
      "epoch no.0 train no.15870  loss = 5.32061 avg_loss = 4.21623\n",
      "epoch no.0 train no.15880  loss = 3.69863 avg_loss = 4.24384\n",
      "epoch no.0 train no.15890  loss = 2.51108 avg_loss = 4.23674\n",
      "epoch no.0 train no.15900  loss = 3.21055 avg_loss = 4.26323\n",
      "epoch no.0 train no.15910  loss = 5.86971 avg_loss = 4.25743\n",
      "epoch no.0 train no.15920  loss = 4.60596 avg_loss = 4.25427\n",
      "epoch no.0 train no.15930  loss = 4.00707 avg_loss = 4.26273\n",
      "epoch no.0 train no.15940  loss = 2.76613 avg_loss = 4.21276\n",
      "epoch no.0 train no.15950  loss = 4.99986 avg_loss = 4.21092\n",
      "epoch no.0 train no.15960  loss = 6.82897 avg_loss = 4.23253\n",
      "epoch no.0 train no.15970  loss = 2.97893 avg_loss = 4.18082\n",
      "epoch no.0 train no.15980  loss = 5.20005 avg_loss = 4.20985\n",
      "epoch no.0 train no.15990  loss = 5.54394 avg_loss = 4.23114\n",
      "epoch no.0 train no.16000  loss = 3.88363 avg_loss = 4.22428\n",
      "3\n",
      "to_tokens: ['▁가을', '좋', '이', '▁필요할', '▁날', '</s>']\n",
      "기분전환이 필요한 날</s>\n",
      "epoch no.0 train no.16010  loss = 8.82895 avg_loss = 4.21026\n",
      "epoch no.0 train no.16020  loss = 6.21566 avg_loss = 4.25560\n",
      "epoch no.0 train no.16030  loss = 2.88147 avg_loss = 4.20133\n",
      "epoch no.0 train no.16040  loss = 4.72638 avg_loss = 4.21209\n",
      "epoch no.0 train no.16050  loss = 3.88773 avg_loss = 4.17219\n",
      "epoch no.0 train no.16060  loss = 5.43788 avg_loss = 4.19390\n",
      "epoch no.0 train no.16070  loss = 4.58279 avg_loss = 4.21930\n",
      "epoch no.0 train no.16080  loss = 2.82314 avg_loss = 4.20138\n",
      "epoch no.0 train no.16090  loss = 3.20641 avg_loss = 4.18820\n",
      "epoch no.0 train no.16100  loss = 3.66164 avg_loss = 4.20123\n",
      "epoch no.0 train no.16110  loss = 5.19527 avg_loss = 4.19206\n",
      "epoch no.0 train no.16120  loss = 4.08704 avg_loss = 4.17123\n",
      "epoch no.0 train no.16130  loss = 3.84642 avg_loss = 4.16792\n",
      "epoch no.0 train no.16140  loss = 4.11706 avg_loss = 4.13699\n",
      "epoch no.0 train no.16150  loss = 2.69315 avg_loss = 4.11892\n",
      "epoch no.0 train no.16160  loss = 2.46489 avg_loss = 4.13922\n",
      "epoch no.0 train no.16170  loss = 7.07198 avg_loss = 4.23684\n",
      "epoch no.0 train no.16180  loss = 9.20813 avg_loss = 4.29124\n",
      "epoch no.0 train no.16190  loss = 3.81883 avg_loss = 4.29413\n",
      "epoch no.0 train no.16200  loss = 4.18762 avg_loss = 4.33286\n",
      "epoch no.0 train no.16210  loss = 4.96264 avg_loss = 4.32846\n",
      "epoch no.0 train no.16220  loss = 3.33627 avg_loss = 4.35832\n",
      "epoch no.0 train no.16230  loss = 2.57911 avg_loss = 4.30062\n",
      "epoch no.0 train no.16240  loss = 6.60045 avg_loss = 4.27558\n",
      "epoch no.0 train no.16250  loss = 3.22173 avg_loss = 4.21241\n",
      "epoch no.0 train no.16260  loss = 3.84971 avg_loss = 4.22132\n",
      "epoch no.0 train no.16270  loss = 4.84967 avg_loss = 4.24620\n",
      "epoch no.0 train no.16280  loss = 5.22366 avg_loss = 4.24353\n",
      "epoch no.0 train no.16290  loss = 2.86308 avg_loss = 4.21931\n",
      "epoch no.0 train no.16300  loss = 2.83272 avg_loss = 4.17877\n",
      "epoch no.0 train no.16310  loss = 2.68834 avg_loss = 4.19539\n",
      "epoch no.0 train no.16320  loss = 4.10015 avg_loss = 4.18983\n",
      "epoch no.0 train no.16330  loss = 3.33768 avg_loss = 4.13776\n",
      "epoch no.0 train no.16340  loss = 5.43189 avg_loss = 4.22401\n",
      "epoch no.0 train no.16350  loss = 4.55094 avg_loss = 4.21832\n",
      "epoch no.0 train no.16360  loss = 2.99410 avg_loss = 4.24481\n",
      "epoch no.0 train no.16370  loss = 3.77254 avg_loss = 4.20932\n",
      "epoch no.0 train no.16380  loss = 4.44260 avg_loss = 4.25142\n",
      "epoch no.0 train no.16390  loss = 6.37354 avg_loss = 4.25185\n",
      "epoch no.0 train no.16400  loss = 4.10060 avg_loss = 4.32161\n",
      "epoch no.0 train no.16410  loss = 5.46818 avg_loss = 4.31857\n",
      "epoch no.0 train no.16420  loss = 3.31886 avg_loss = 4.31893\n",
      "epoch no.0 train no.16430  loss = 3.13861 avg_loss = 4.29826\n",
      "epoch no.0 train no.16440  loss = 4.07531 avg_loss = 4.26863\n",
      "epoch no.0 train no.16450  loss = 2.79769 avg_loss = 4.25102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.16460  loss = 3.36245 avg_loss = 4.25456\n",
      "epoch no.0 train no.16470  loss = 4.71316 avg_loss = 4.26968\n",
      "epoch no.0 train no.16480  loss = 1.44858 avg_loss = 4.21332\n",
      "epoch no.0 train no.16490  loss = 6.68534 avg_loss = 4.21272\n",
      "epoch no.0 train no.16500  loss = 4.53923 avg_loss = 4.18456\n",
      "epoch no.0 train no.16510  loss = 3.00819 avg_loss = 4.16707\n",
      "epoch no.0 train no.16520  loss = 2.88204 avg_loss = 4.14863\n",
      "epoch no.0 train no.16530  loss = 3.54169 avg_loss = 4.11486\n",
      "epoch no.0 train no.16540  loss = 5.56970 avg_loss = 4.18333\n",
      "epoch no.0 train no.16550  loss = 4.11045 avg_loss = 4.24309\n",
      "epoch no.0 train no.16560  loss = 2.65536 avg_loss = 4.20118\n",
      "epoch no.0 train no.16570  loss = 4.31512 avg_loss = 4.22501\n",
      "epoch no.0 train no.16580  loss = 4.25865 avg_loss = 4.21701\n",
      "epoch no.0 train no.16590  loss = 5.83189 avg_loss = 4.23401\n",
      "epoch no.0 train no.16600  loss = 6.48894 avg_loss = 4.27659\n",
      "epoch no.0 train no.16610  loss = 3.95673 avg_loss = 4.23526\n",
      "epoch no.0 train no.16620  loss = 5.57159 avg_loss = 4.23111\n",
      "epoch no.0 train no.16630  loss = 3.11226 avg_loss = 4.18483\n",
      "epoch no.0 train no.16640  loss = 6.96260 avg_loss = 4.26349\n",
      "epoch no.0 train no.16650  loss = 4.16092 avg_loss = 4.22739\n",
      "epoch no.0 train no.16660  loss = 5.91063 avg_loss = 4.26913\n",
      "epoch no.0 train no.16670  loss = 3.80267 avg_loss = 4.24596\n",
      "epoch no.0 train no.16680  loss = 4.83623 avg_loss = 4.24356\n",
      "epoch no.0 train no.16690  loss = 3.51192 avg_loss = 4.34012\n",
      "epoch no.0 train no.16700  loss = 2.44255 avg_loss = 4.26999\n",
      "epoch no.0 train no.16710  loss = 3.38628 avg_loss = 4.22670\n",
      "epoch no.0 train no.16720  loss = 4.48808 avg_loss = 4.23725\n",
      "epoch no.0 train no.16730  loss = 2.92408 avg_loss = 4.19084\n",
      "epoch no.0 train no.16740  loss = 2.90704 avg_loss = 4.16962\n",
      "epoch no.0 train no.16750  loss = 3.18197 avg_loss = 4.23109\n",
      "epoch no.0 train no.16760  loss = 2.58197 avg_loss = 4.18356\n",
      "epoch no.0 train no.16770  loss = 2.61882 avg_loss = 4.17306\n",
      "epoch no.0 train no.16780  loss = 6.10613 avg_loss = 4.17823\n",
      "epoch no.0 train no.16790  loss = 6.43244 avg_loss = 4.18863\n",
      "epoch no.0 train no.16800  loss = 4.25395 avg_loss = 4.17284\n",
      "epoch no.0 train no.16810  loss = 2.83261 avg_loss = 4.16067\n",
      "epoch no.0 train no.16820  loss = 2.09476 avg_loss = 4.13051\n",
      "epoch no.0 train no.16830  loss = 4.43771 avg_loss = 4.21883\n",
      "epoch no.0 train no.16840  loss = 3.74402 avg_loss = 4.23063\n",
      "epoch no.0 train no.16850  loss = 4.71574 avg_loss = 4.28378\n",
      "epoch no.0 train no.16860  loss = 4.97175 avg_loss = 4.32204\n",
      "epoch no.0 train no.16870  loss = 3.46946 avg_loss = 4.34573\n",
      "epoch no.0 train no.16880  loss = 4.75212 avg_loss = 4.33956\n",
      "epoch no.0 train no.16890  loss = 3.45942 avg_loss = 4.28405\n",
      "epoch no.0 train no.16900  loss = 2.48441 avg_loss = 4.20349\n",
      "epoch no.0 train no.16910  loss = 6.65358 avg_loss = 4.14997\n",
      "epoch no.0 train no.16920  loss = 4.47009 avg_loss = 4.14490\n",
      "epoch no.0 train no.16930  loss = 4.45198 avg_loss = 4.13989\n",
      "epoch no.0 train no.16940  loss = 6.26525 avg_loss = 4.21330\n",
      "epoch no.0 train no.16950  loss = 5.19057 avg_loss = 4.25590\n",
      "epoch no.0 train no.16960  loss = 2.66188 avg_loss = 4.26373\n",
      "epoch no.0 train no.16970  loss = 4.19687 avg_loss = 4.25525\n",
      "epoch no.0 train no.16980  loss = 4.32646 avg_loss = 4.24018\n",
      "epoch no.0 train no.16990  loss = 5.83718 avg_loss = 4.25779\n",
      "epoch no.0 train no.17000  loss = 5.82860 avg_loss = 4.25600\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁플레이', '</s>']\n",
      "기분전환을 위한 재즈</s>\n",
      "epoch no.0 train no.17010  loss = 3.68963 avg_loss = 4.29088\n",
      "epoch no.0 train no.17020  loss = 3.49573 avg_loss = 4.29371\n",
      "epoch no.0 train no.17030  loss = 3.36959 avg_loss = 4.30806\n",
      "epoch no.0 train no.17040  loss = 3.02674 avg_loss = 4.27116\n",
      "epoch no.0 train no.17050  loss = 5.18724 avg_loss = 4.25539\n",
      "epoch no.0 train no.17060  loss = 4.95452 avg_loss = 4.20592\n",
      "epoch no.0 train no.17070  loss = 4.31014 avg_loss = 4.16049\n",
      "epoch no.0 train no.17080  loss = 2.69455 avg_loss = 4.15926\n",
      "epoch no.0 train no.17090  loss = 3.63382 avg_loss = 4.14544\n",
      "epoch no.0 train no.17100  loss = 5.35125 avg_loss = 4.15952\n",
      "epoch no.0 train no.17110  loss = 2.58261 avg_loss = 4.08470\n",
      "epoch no.0 train no.17120  loss = 3.41566 avg_loss = 4.05138\n",
      "epoch no.0 train no.17130  loss = 4.62858 avg_loss = 4.06168\n",
      "epoch no.0 train no.17140  loss = 4.60049 avg_loss = 4.06380\n",
      "epoch no.0 train no.17150  loss = 3.74537 avg_loss = 4.05497\n",
      "epoch no.0 train no.17160  loss = 6.55491 avg_loss = 4.05931\n",
      "epoch no.0 train no.17170  loss = 2.80267 avg_loss = 4.02717\n",
      "epoch no.0 train no.17180  loss = 7.87869 avg_loss = 4.16141\n",
      "epoch no.0 train no.17190  loss = 3.17029 avg_loss = 4.15878\n",
      "epoch no.0 train no.17200  loss = 5.65717 avg_loss = 4.21077\n",
      "epoch no.0 train no.17210  loss = 3.31394 avg_loss = 4.22483\n",
      "epoch no.0 train no.17220  loss = 4.64771 avg_loss = 4.23498\n",
      "epoch no.0 train no.17230  loss = 3.67014 avg_loss = 4.24144\n",
      "epoch no.0 train no.17240  loss = 2.93357 avg_loss = 4.18562\n",
      "epoch no.0 train no.17250  loss = 2.64589 avg_loss = 4.11565\n",
      "epoch no.0 train no.17260  loss = 2.36697 avg_loss = 4.07792\n",
      "epoch no.0 train no.17270  loss = 2.51163 avg_loss = 4.02551\n",
      "epoch no.0 train no.17280  loss = 2.27905 avg_loss = 4.01452\n",
      "epoch no.0 train no.17290  loss = 3.65405 avg_loss = 4.01257\n",
      "epoch no.0 train no.17300  loss = 6.96674 avg_loss = 4.02874\n",
      "epoch no.0 train no.17310  loss = 4.50568 avg_loss = 4.03534\n",
      "epoch no.0 train no.17320  loss = 5.22612 avg_loss = 4.03160\n",
      "epoch no.0 train no.17330  loss = 3.01977 avg_loss = 4.02187\n",
      "epoch no.0 train no.17340  loss = 3.70627 avg_loss = 3.97495\n",
      "epoch no.0 train no.17350  loss = 3.48439 avg_loss = 3.90948\n",
      "epoch no.0 train no.17360  loss = 4.56614 avg_loss = 3.95434\n",
      "epoch no.0 train no.17370  loss = 2.63713 avg_loss = 3.93742\n",
      "epoch no.0 train no.17380  loss = 2.49863 avg_loss = 3.91544\n",
      "epoch no.0 train no.17390  loss = 3.13769 avg_loss = 3.94324\n",
      "epoch no.0 train no.17400  loss = 4.50196 avg_loss = 3.89622\n",
      "epoch no.0 train no.17410  loss = 2.28856 avg_loss = 3.87695\n",
      "epoch no.0 train no.17420  loss = 3.24799 avg_loss = 3.87463\n",
      "epoch no.0 train no.17430  loss = 3.58027 avg_loss = 3.89111\n",
      "epoch no.0 train no.17440  loss = 4.84748 avg_loss = 3.89910\n",
      "epoch no.0 train no.17450  loss = 3.86067 avg_loss = 3.89749\n",
      "epoch no.0 train no.17460  loss = 7.36832 avg_loss = 4.00066\n",
      "epoch no.0 train no.17470  loss = 3.35839 avg_loss = 4.00273\n",
      "epoch no.0 train no.17480  loss = 2.63889 avg_loss = 4.04460\n",
      "epoch no.0 train no.17490  loss = 2.54121 avg_loss = 4.07240\n",
      "epoch no.0 train no.17500  loss = 5.00856 avg_loss = 4.10824\n",
      "epoch no.0 train no.17510  loss = 3.55599 avg_loss = 4.07083\n",
      "epoch no.0 train no.17520  loss = 4.16893 avg_loss = 4.06582\n",
      "epoch no.0 train no.17530  loss = 4.70690 avg_loss = 4.12391\n",
      "epoch no.0 train no.17540  loss = 5.53311 avg_loss = 4.15100\n",
      "epoch no.0 train no.17550  loss = 3.90071 avg_loss = 4.12913\n",
      "epoch no.0 train no.17560  loss = 2.43094 avg_loss = 4.10464\n",
      "epoch no.0 train no.17570  loss = 3.75736 avg_loss = 4.13636\n",
      "epoch no.0 train no.17580  loss = 4.92244 avg_loss = 4.13198\n",
      "epoch no.0 train no.17590  loss = 3.78024 avg_loss = 4.18513\n",
      "epoch no.0 train no.17600  loss = 5.42009 avg_loss = 4.10089\n",
      "epoch no.0 train no.17610  loss = 3.16269 avg_loss = 4.10450\n",
      "epoch no.0 train no.17620  loss = 4.05972 avg_loss = 4.08868\n",
      "epoch no.0 train no.17630  loss = 4.69488 avg_loss = 4.10251\n",
      "epoch no.0 train no.17640  loss = 3.58452 avg_loss = 4.10796\n",
      "epoch no.0 train no.17650  loss = 5.85841 avg_loss = 4.18662\n",
      "epoch no.0 train no.17660  loss = 3.80148 avg_loss = 4.20906\n",
      "epoch no.0 train no.17670  loss = 4.22076 avg_loss = 4.18689\n",
      "epoch no.0 train no.17680  loss = 3.78463 avg_loss = 4.15989\n",
      "epoch no.0 train no.17690  loss = 3.74761 avg_loss = 4.12900\n",
      "epoch no.0 train no.17700  loss = 3.30997 avg_loss = 4.09502\n",
      "epoch no.0 train no.17710  loss = 5.20803 avg_loss = 4.10675\n",
      "epoch no.0 train no.17720  loss = 4.24243 avg_loss = 4.10410\n",
      "epoch no.0 train no.17730  loss = 2.67211 avg_loss = 4.05040\n",
      "epoch no.0 train no.17740  loss = 3.21181 avg_loss = 4.04078\n",
      "epoch no.0 train no.17750  loss = 3.98009 avg_loss = 4.04821\n",
      "epoch no.0 train no.17760  loss = 4.98174 avg_loss = 4.03437\n",
      "epoch no.0 train no.17770  loss = 6.11127 avg_loss = 4.10608\n",
      "epoch no.0 train no.17780  loss = 3.09370 avg_loss = 4.13470\n",
      "epoch no.0 train no.17790  loss = 3.87931 avg_loss = 4.13339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.17800  loss = 3.36797 avg_loss = 4.18488\n",
      "epoch no.0 train no.17810  loss = 7.16644 avg_loss = 4.27227\n",
      "epoch no.0 train no.17820  loss = 2.74235 avg_loss = 4.25689\n",
      "epoch no.0 train no.17830  loss = 4.73369 avg_loss = 4.26908\n",
      "epoch no.0 train no.17840  loss = 5.29190 avg_loss = 4.27385\n",
      "epoch no.0 train no.17850  loss = 4.22948 avg_loss = 4.30715\n",
      "epoch no.0 train no.17860  loss = 3.57143 avg_loss = 4.24732\n",
      "epoch no.0 train no.17870  loss = 5.42262 avg_loss = 4.23574\n",
      "epoch no.0 train no.17880  loss = 3.12554 avg_loss = 4.23627\n",
      "epoch no.0 train no.17890  loss = 4.51819 avg_loss = 4.25080\n",
      "epoch no.0 train no.17900  loss = 5.71317 avg_loss = 4.28088\n",
      "epoch no.0 train no.17910  loss = 3.99787 avg_loss = 4.22874\n",
      "epoch no.0 train no.17920  loss = 2.84595 avg_loss = 4.20259\n",
      "epoch no.0 train no.17930  loss = 3.15183 avg_loss = 4.16068\n",
      "epoch no.0 train no.17940  loss = 3.36834 avg_loss = 4.10078\n",
      "epoch no.0 train no.17950  loss = 2.67003 avg_loss = 4.06793\n",
      "epoch no.0 train no.17960  loss = 3.01423 avg_loss = 4.12979\n",
      "epoch no.0 train no.17970  loss = 4.67479 avg_loss = 4.11720\n",
      "epoch no.0 train no.17980  loss = 3.41903 avg_loss = 4.08555\n",
      "epoch no.0 train no.17990  loss = 6.17465 avg_loss = 4.15727\n",
      "epoch no.0 train no.18000  loss = 3.18213 avg_loss = 4.11168\n",
      "7\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요한', '▁당신을', '</s>', '▁감성', '</s>', '합', '</s>']\n",
      "기분전환이 필요한 오후의 감성힙합</s>\n",
      "epoch no.0 train no.18010  loss = 3.86644 avg_loss = 4.06335\n",
      "epoch no.0 train no.18020  loss = 4.21063 avg_loss = 4.13228\n",
      "epoch no.0 train no.18030  loss = 3.52789 avg_loss = 4.14148\n",
      "epoch no.0 train no.18040  loss = 2.04225 avg_loss = 4.05150\n",
      "epoch no.0 train no.18050  loss = 2.48272 avg_loss = 4.01547\n",
      "epoch no.0 train no.18060  loss = 3.16880 avg_loss = 4.03500\n",
      "epoch no.0 train no.18070  loss = 3.27579 avg_loss = 4.02779\n",
      "epoch no.0 train no.18080  loss = 4.26816 avg_loss = 4.00453\n",
      "epoch no.0 train no.18090  loss = 2.93563 avg_loss = 4.04541\n",
      "epoch no.0 train no.18100  loss = 2.37641 avg_loss = 4.03417\n",
      "epoch no.0 train no.18110  loss = 5.96568 avg_loss = 4.11542\n",
      "epoch no.0 train no.18120  loss = 4.58641 avg_loss = 4.10690\n",
      "epoch no.0 train no.18130  loss = 3.48248 avg_loss = 4.15570\n",
      "epoch no.0 train no.18140  loss = 3.96472 avg_loss = 4.21086\n",
      "epoch no.0 train no.18150  loss = 4.20998 avg_loss = 4.20825\n",
      "epoch no.0 train no.18160  loss = 3.71039 avg_loss = 4.20040\n",
      "epoch no.0 train no.18170  loss = 2.77650 avg_loss = 4.19891\n",
      "epoch no.0 train no.18180  loss = 3.12449 avg_loss = 4.17511\n",
      "epoch no.0 train no.18190  loss = 3.14283 avg_loss = 4.14251\n",
      "epoch no.0 train no.18200  loss = 5.49533 avg_loss = 4.16187\n",
      "epoch no.0 train no.18210  loss = 5.36728 avg_loss = 4.21762\n",
      "epoch no.0 train no.18220  loss = 5.74374 avg_loss = 4.21733\n",
      "epoch no.0 train no.18230  loss = 3.24055 avg_loss = 4.20670\n",
      "epoch no.0 train no.18240  loss = 2.92686 avg_loss = 4.20114\n",
      "epoch no.0 train no.18250  loss = 3.03912 avg_loss = 4.20954\n",
      "epoch no.0 train no.18260  loss = 5.92430 avg_loss = 4.21510\n",
      "epoch no.0 train no.18270  loss = 3.17930 avg_loss = 4.18324\n",
      "epoch no.0 train no.18280  loss = 5.88491 avg_loss = 4.18677\n",
      "epoch no.0 train no.18290  loss = 6.03701 avg_loss = 4.14536\n",
      "epoch no.0 train no.18300  loss = 3.29989 avg_loss = 4.12957\n",
      "epoch no.0 train no.18310  loss = 5.03330 avg_loss = 4.12967\n",
      "epoch no.0 train no.18320  loss = 3.98452 avg_loss = 4.16833\n",
      "epoch no.0 train no.18330  loss = 4.50029 avg_loss = 4.17312\n",
      "epoch no.0 train no.18340  loss = 4.90020 avg_loss = 4.20353\n",
      "epoch no.0 train no.18350  loss = 3.31678 avg_loss = 4.16085\n",
      "epoch no.0 train no.18360  loss = 2.17406 avg_loss = 4.19220\n",
      "epoch no.0 train no.18370  loss = 4.31795 avg_loss = 4.15022\n",
      "epoch no.0 train no.18380  loss = 5.57626 avg_loss = 4.19533\n",
      "epoch no.0 train no.18390  loss = 6.22288 avg_loss = 4.20092\n",
      "epoch no.0 train no.18400  loss = 2.52887 avg_loss = 4.17322\n",
      "epoch no.0 train no.18410  loss = 3.39694 avg_loss = 4.20927\n",
      "epoch no.0 train no.18420  loss = 4.25115 avg_loss = 4.21063\n",
      "epoch no.0 train no.18430  loss = 4.72044 avg_loss = 4.12882\n",
      "epoch no.0 train no.18440  loss = 5.45175 avg_loss = 4.20095\n",
      "epoch no.0 train no.18450  loss = 4.19111 avg_loss = 4.19011\n",
      "epoch no.0 train no.18460  loss = 2.60270 avg_loss = 4.14612\n",
      "epoch no.0 train no.18470  loss = 3.02072 avg_loss = 4.15467\n",
      "epoch no.0 train no.18480  loss = 3.04249 avg_loss = 4.13536\n",
      "epoch no.0 train no.18490  loss = 7.24094 avg_loss = 4.17571\n",
      "epoch no.0 train no.18500  loss = 3.99311 avg_loss = 4.20429\n",
      "epoch no.0 train no.18510  loss = 4.83688 avg_loss = 4.14072\n",
      "epoch no.0 train no.18520  loss = 4.34059 avg_loss = 4.11231\n",
      "epoch no.0 train no.18530  loss = 2.86812 avg_loss = 4.05641\n",
      "epoch no.0 train no.18540  loss = 5.16181 avg_loss = 4.05399\n",
      "epoch no.0 train no.18550  loss = 3.54951 avg_loss = 4.11592\n",
      "epoch no.0 train no.18560  loss = 5.53200 avg_loss = 4.12158\n",
      "epoch no.0 train no.18570  loss = 3.96973 avg_loss = 4.12050\n",
      "epoch no.0 train no.18580  loss = 4.64259 avg_loss = 4.09622\n",
      "epoch no.0 train no.18590  loss = 3.83183 avg_loss = 4.12628\n",
      "epoch no.0 train no.18600  loss = 4.68427 avg_loss = 4.09757\n",
      "epoch no.0 train no.18610  loss = 2.81744 avg_loss = 4.09355\n",
      "epoch no.0 train no.18620  loss = 4.51618 avg_loss = 4.07669\n",
      "epoch no.0 train no.18630  loss = 5.37815 avg_loss = 4.06356\n",
      "epoch no.0 train no.18640  loss = 3.51610 avg_loss = 4.09620\n",
      "epoch no.0 train no.18650  loss = 3.53386 avg_loss = 4.06814\n",
      "epoch no.0 train no.18660  loss = 5.18002 avg_loss = 4.08315\n",
      "epoch no.0 train no.18670  loss = 3.27578 avg_loss = 4.11561\n",
      "epoch no.0 train no.18680  loss = 2.21454 avg_loss = 4.08800\n",
      "epoch no.0 train no.18690  loss = 4.01385 avg_loss = 4.04446\n",
      "epoch no.0 train no.18700  loss = 4.12187 avg_loss = 4.07826\n",
      "epoch no.0 train no.18710  loss = 5.12842 avg_loss = 4.08491\n",
      "epoch no.0 train no.18720  loss = 3.42496 avg_loss = 4.09084\n",
      "epoch no.0 train no.18730  loss = 6.84903 avg_loss = 4.08785\n",
      "epoch no.0 train no.18740  loss = 4.08366 avg_loss = 4.11785\n",
      "epoch no.0 train no.18750  loss = 4.63557 avg_loss = 4.15396\n",
      "epoch no.0 train no.18760  loss = 2.82385 avg_loss = 4.11107\n",
      "epoch no.0 train no.18770  loss = 2.78217 avg_loss = 4.13371\n",
      "epoch no.0 train no.18780  loss = 3.33661 avg_loss = 4.13989\n",
      "epoch no.0 train no.18790  loss = 3.00336 avg_loss = 4.14421\n",
      "epoch no.0 train no.18800  loss = 3.63628 avg_loss = 4.14997\n",
      "epoch no.0 train no.18810  loss = 5.97838 avg_loss = 4.15845\n",
      "epoch no.0 train no.18820  loss = 2.91190 avg_loss = 4.14465\n",
      "epoch no.0 train no.18830  loss = 6.35529 avg_loss = 4.20137\n",
      "epoch no.0 train no.18840  loss = 2.61737 avg_loss = 4.18631\n",
      "epoch no.0 train no.18850  loss = 4.95846 avg_loss = 4.17267\n",
      "epoch no.0 train no.18860  loss = 4.12949 avg_loss = 4.14256\n",
      "epoch no.0 train no.18870  loss = 1.80932 avg_loss = 4.10627\n",
      "epoch no.0 train no.18880  loss = 4.43031 avg_loss = 4.10429\n",
      "epoch no.0 train no.18890  loss = 3.22698 avg_loss = 4.05487\n",
      "epoch no.0 train no.18900  loss = 2.72479 avg_loss = 4.07254\n",
      "epoch no.0 train no.18910  loss = 2.78212 avg_loss = 4.02745\n",
      "epoch no.0 train no.18920  loss = 3.38929 avg_loss = 4.02074\n",
      "epoch no.0 train no.18930  loss = 4.72382 avg_loss = 4.11867\n",
      "epoch no.0 train no.18940  loss = 5.06657 avg_loss = 4.14585\n",
      "epoch no.0 train no.18950  loss = 1.59444 avg_loss = 4.12106\n",
      "epoch no.0 train no.18960  loss = 4.84430 avg_loss = 4.17211\n",
      "epoch no.0 train no.18970  loss = 3.72047 avg_loss = 4.20059\n",
      "epoch no.0 train no.18980  loss = 4.52481 avg_loss = 4.16694\n",
      "epoch no.0 train no.18990  loss = 2.32106 avg_loss = 4.16426\n",
      "epoch no.0 train no.19000  loss = 5.41509 avg_loss = 4.12133\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁위한', '▁노래', '송']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.0 train no.19010  loss = 4.27328 avg_loss = 4.11738\n",
      "epoch no.0 train no.19020  loss = 2.73181 avg_loss = 4.17254\n",
      "epoch no.0 train no.19030  loss = 6.05396 avg_loss = 4.18653\n",
      "epoch no.0 train no.19040  loss = 4.00415 avg_loss = 4.11940\n",
      "epoch no.0 train no.19050  loss = 4.52486 avg_loss = 4.09129\n",
      "epoch no.0 train no.19060  loss = 4.38464 avg_loss = 4.07608\n",
      "epoch no.0 train no.19070  loss = 5.50778 avg_loss = 4.08832\n",
      "epoch no.0 train no.19080  loss = 4.44938 avg_loss = 4.10829\n",
      "epoch no.0 train no.19090  loss = 5.19069 avg_loss = 4.10566\n",
      "epoch no.0 train no.19100  loss = 3.87830 avg_loss = 4.06743\n",
      "epoch no.0 train no.19110  loss = 3.46006 avg_loss = 4.06533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.19120  loss = 2.60201 avg_loss = 4.10132\n",
      "epoch no.0 train no.19130  loss = 3.60488 avg_loss = 4.03821\n",
      "epoch no.0 train no.19140  loss = 3.49675 avg_loss = 4.10062\n",
      "epoch no.0 train no.19150  loss = 4.39808 avg_loss = 4.13750\n",
      "epoch no.0 train no.19160  loss = 2.68196 avg_loss = 4.10666\n",
      "epoch no.0 train no.19170  loss = 3.17315 avg_loss = 4.09496\n",
      "epoch no.0 train no.19180  loss = 2.51529 avg_loss = 4.08451\n",
      "epoch no.0 train no.19190  loss = 4.02605 avg_loss = 4.07097\n",
      "epoch no.0 train no.19200  loss = 6.01550 avg_loss = 4.07850\n",
      "epoch no.0 train no.19210  loss = 3.08189 avg_loss = 4.04971\n",
      "epoch no.0 train no.19220  loss = 4.49176 avg_loss = 4.04202\n",
      "epoch no.0 train no.19230  loss = 4.91513 avg_loss = 4.07129\n",
      "epoch no.0 train no.19240  loss = 3.98349 avg_loss = 4.05848\n",
      "epoch no.0 train no.19250  loss = 2.67229 avg_loss = 4.02693\n",
      "epoch no.0 train no.19260  loss = 3.06758 avg_loss = 4.03526\n",
      "epoch no.0 train no.19270  loss = 5.78557 avg_loss = 4.05640\n",
      "epoch no.0 train no.19280  loss = 4.42220 avg_loss = 4.06132\n",
      "epoch no.0 train no.19290  loss = 3.93484 avg_loss = 4.08127\n",
      "epoch no.0 train no.19300  loss = 3.81532 avg_loss = 4.07455\n",
      "epoch no.0 train no.19310  loss = 4.94419 avg_loss = 4.07099\n",
      "epoch no.0 train no.19320  loss = 3.61046 avg_loss = 4.06861\n",
      "epoch no.0 train no.19330  loss = 2.71852 avg_loss = 4.04204\n",
      "epoch no.0 train no.19340  loss = 3.02947 avg_loss = 4.06869\n",
      "epoch no.0 train no.19350  loss = 3.32272 avg_loss = 4.09475\n",
      "epoch no.0 train no.19360  loss = 5.07897 avg_loss = 4.13189\n",
      "epoch no.0 train no.19370  loss = 3.85730 avg_loss = 4.13922\n",
      "epoch no.0 train no.19380  loss = 3.22256 avg_loss = 4.12476\n",
      "epoch no.0 train no.19390  loss = 2.72361 avg_loss = 4.07152\n",
      "epoch no.0 train no.19400  loss = 4.87476 avg_loss = 4.09532\n",
      "epoch no.0 train no.19410  loss = 4.98296 avg_loss = 4.12770\n",
      "epoch no.0 train no.19420  loss = 3.60150 avg_loss = 4.09581\n",
      "epoch no.0 train no.19430  loss = 3.14546 avg_loss = 4.09323\n",
      "epoch no.0 train no.19440  loss = 3.53011 avg_loss = 4.09389\n",
      "epoch no.0 train no.19450  loss = 6.13924 avg_loss = 4.13691\n",
      "epoch no.0 train no.19460  loss = 5.23445 avg_loss = 4.08814\n",
      "epoch no.0 train no.19470  loss = 2.85382 avg_loss = 4.09398\n",
      "epoch no.0 train no.19480  loss = 5.26708 avg_loss = 4.09769\n",
      "epoch no.0 train no.19490  loss = 3.94788 avg_loss = 4.04239\n",
      "epoch no.0 train no.19500  loss = 4.21188 avg_loss = 4.16341\n",
      "epoch no.0 train no.19510  loss = 3.36067 avg_loss = 4.15740\n",
      "epoch no.0 train no.19520  loss = 4.93422 avg_loss = 4.16405\n",
      "epoch no.0 train no.19530  loss = 4.18382 avg_loss = 4.15212\n",
      "epoch no.0 train no.19540  loss = 5.70930 avg_loss = 4.17083\n",
      "epoch no.0 train no.19550  loss = 4.29443 avg_loss = 4.15741\n",
      "epoch no.0 train no.19560  loss = 4.62740 avg_loss = 4.14593\n",
      "epoch no.0 train no.19570  loss = 3.23574 avg_loss = 4.23711\n",
      "epoch no.0 train no.19580  loss = 5.71012 avg_loss = 4.21084\n",
      "epoch no.0 train no.19590  loss = 4.25082 avg_loss = 4.20344\n",
      "epoch no.0 train no.19600  loss = 3.62635 avg_loss = 4.17937\n",
      "epoch no.0 train no.19610  loss = 3.06872 avg_loss = 4.25048\n",
      "epoch no.0 train no.19620  loss = 5.01767 avg_loss = 4.35791\n",
      "epoch no.0 train no.19630  loss = 5.76451 avg_loss = 4.33902\n",
      "epoch no.0 train no.19640  loss = 4.03371 avg_loss = 4.33115\n",
      "epoch no.0 train no.19650  loss = 6.16083 avg_loss = 4.33381\n",
      "epoch no.0 train no.19660  loss = 6.23567 avg_loss = 4.32399\n",
      "epoch no.0 train no.19670  loss = 3.12962 avg_loss = 4.31368\n",
      "epoch no.0 train no.19680  loss = 4.03126 avg_loss = 4.32027\n",
      "epoch no.0 train no.19690  loss = 2.60481 avg_loss = 4.29859\n",
      "epoch no.0 train no.19700  loss = 2.89597 avg_loss = 4.25332\n",
      "epoch no.0 train no.19710  loss = 3.36086 avg_loss = 4.22600\n",
      "epoch no.0 train no.19720  loss = 5.31989 avg_loss = 4.21128\n",
      "epoch no.0 train no.19730  loss = 2.12459 avg_loss = 4.28173\n",
      "epoch no.0 train no.19740  loss = 4.97654 avg_loss = 4.27168\n",
      "epoch no.0 train no.19750  loss = 3.97190 avg_loss = 4.25985\n",
      "epoch no.0 train no.19760  loss = 2.99745 avg_loss = 4.25198\n",
      "epoch no.0 train no.19770  loss = 2.85454 avg_loss = 4.21985\n",
      "epoch no.0 train no.19780  loss = 2.59704 avg_loss = 4.16991\n",
      "epoch no.0 train no.19790  loss = 4.53974 avg_loss = 4.22607\n",
      "epoch no.0 train no.19800  loss = 4.00289 avg_loss = 4.25240\n",
      "epoch no.0 train no.19810  loss = 4.13511 avg_loss = 4.25274\n",
      "epoch no.0 train no.19820  loss = 2.57506 avg_loss = 4.20220\n",
      "epoch no.0 train no.19830  loss = 3.28840 avg_loss = 4.20067\n",
      "epoch no.0 train no.19840  loss = 3.38635 avg_loss = 4.23174\n",
      "epoch no.0 train no.19850  loss = 4.41073 avg_loss = 4.26546\n",
      "epoch no.0 train no.19860  loss = 6.48337 avg_loss = 4.23052\n",
      "epoch no.0 train no.19870  loss = 4.00986 avg_loss = 4.25065\n",
      "epoch no.0 train no.19880  loss = 3.40721 avg_loss = 4.26685\n",
      "epoch no.0 train no.19890  loss = 3.70327 avg_loss = 4.23688\n",
      "epoch no.0 train no.19900  loss = 4.61675 avg_loss = 4.21247\n",
      "epoch no.0 train no.19910  loss = 4.23038 avg_loss = 4.19410\n",
      "epoch no.0 train no.19920  loss = 4.08765 avg_loss = 4.14955\n",
      "epoch no.0 train no.19930  loss = 3.15950 avg_loss = 4.13622\n",
      "epoch no.0 train no.19940  loss = 4.75349 avg_loss = 4.11124\n",
      "epoch no.0 train no.19950  loss = 2.61902 avg_loss = 4.08570\n",
      "epoch no.0 train no.19960  loss = 2.24557 avg_loss = 4.11745\n",
      "epoch no.0 train no.19970  loss = 3.33383 avg_loss = 4.11870\n",
      "epoch no.0 train no.19980  loss = 3.97235 avg_loss = 4.12633\n",
      "epoch no.0 train no.19990  loss = 3.95668 avg_loss = 4.08867\n",
      "epoch no.0 train no.20000  loss = 3.80297 avg_loss = 4.07761\n",
      "3\n",
      "to_tokens: ['▁', '전환', '을', '▁위한', '▁팝', '송']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.0 train no.20010  loss = 5.38069 avg_loss = 4.08007\n",
      "epoch no.0 train no.20020  loss = 5.47226 avg_loss = 4.10386\n",
      "epoch no.0 train no.20030  loss = 5.84869 avg_loss = 4.10864\n",
      "epoch no.0 train no.20040  loss = 4.26698 avg_loss = 4.10819\n",
      "epoch no.0 train no.20050  loss = 3.34157 avg_loss = 4.10969\n",
      "epoch no.0 train no.20060  loss = 3.11682 avg_loss = 4.06707\n",
      "epoch no.0 train no.20070  loss = 5.24542 avg_loss = 4.04773\n",
      "epoch no.0 train no.20080  loss = 6.08307 avg_loss = 4.05924\n",
      "epoch no.0 train no.20090  loss = 2.84231 avg_loss = 3.98320\n",
      "epoch no.0 train no.20100  loss = 4.27296 avg_loss = 3.97001\n",
      "epoch no.0 train no.20110  loss = 5.30544 avg_loss = 4.02257\n",
      "epoch no.0 train no.20120  loss = 2.59788 avg_loss = 3.98782\n",
      "epoch no.0 train no.20130  loss = 2.48870 avg_loss = 3.95608\n",
      "epoch no.0 train no.20140  loss = 7.83984 avg_loss = 4.00341\n",
      "epoch no.0 train no.20150  loss = 2.74217 avg_loss = 3.98269\n",
      "epoch no.0 train no.20160  loss = 5.43091 avg_loss = 4.00544\n",
      "epoch no.0 train no.20170  loss = 3.55036 avg_loss = 3.98740\n",
      "epoch no.0 train no.20180  loss = 3.88793 avg_loss = 3.94874\n",
      "epoch no.0 train no.20190  loss = 4.94538 avg_loss = 3.98017\n",
      "epoch no.0 train no.20200  loss = 2.29799 avg_loss = 3.94377\n",
      "epoch no.0 train no.20210  loss = 6.23588 avg_loss = 3.98162\n",
      "epoch no.0 train no.20220  loss = 5.10595 avg_loss = 4.03563\n",
      "epoch no.0 train no.20230  loss = 5.15929 avg_loss = 4.10297\n",
      "epoch no.0 train no.20240  loss = 3.55461 avg_loss = 4.11935\n",
      "epoch no.0 train no.20250  loss = 3.24746 avg_loss = 4.08867\n",
      "epoch no.0 train no.20260  loss = 3.03278 avg_loss = 4.16585\n",
      "epoch no.0 train no.20270  loss = 2.79274 avg_loss = 4.20492\n",
      "epoch no.0 train no.20280  loss = 5.76880 avg_loss = 4.26632\n",
      "epoch no.0 train no.20290  loss = 4.94256 avg_loss = 4.30256\n",
      "epoch no.0 train no.20300  loss = 4.86635 avg_loss = 4.26014\n",
      "epoch no.0 train no.20310  loss = 5.47337 avg_loss = 4.27594\n",
      "epoch no.0 train no.20320  loss = 4.12197 avg_loss = 4.28226\n",
      "epoch no.0 train no.20330  loss = 4.05489 avg_loss = 4.25761\n",
      "epoch no.0 train no.20340  loss = 2.57248 avg_loss = 4.21240\n",
      "epoch no.0 train no.20350  loss = 4.77572 avg_loss = 4.20807\n",
      "epoch no.0 train no.20360  loss = 4.39948 avg_loss = 4.22740\n",
      "epoch no.0 train no.20370  loss = 2.02937 avg_loss = 4.19143\n",
      "epoch no.0 train no.20380  loss = 4.14641 avg_loss = 4.15915\n",
      "epoch no.0 train no.20390  loss = 3.39959 avg_loss = 4.13053\n",
      "epoch no.0 train no.20400  loss = 2.55065 avg_loss = 4.15565\n",
      "epoch no.0 train no.20410  loss = 5.55607 avg_loss = 4.23748\n",
      "epoch no.0 train no.20420  loss = 3.52127 avg_loss = 4.22490\n",
      "epoch no.0 train no.20430  loss = 4.52727 avg_loss = 4.23010\n",
      "epoch no.0 train no.20440  loss = 4.50042 avg_loss = 4.20597\n",
      "epoch no.0 train no.20450  loss = 2.37292 avg_loss = 4.19947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.20460  loss = 4.03734 avg_loss = 4.20200\n",
      "epoch no.0 train no.20470  loss = 4.35076 avg_loss = 4.27549\n",
      "epoch no.0 train no.20480  loss = 3.55837 avg_loss = 4.26919\n",
      "epoch no.0 train no.20490  loss = 3.62092 avg_loss = 4.26026\n",
      "epoch no.0 train no.20500  loss = 3.82149 avg_loss = 4.28590\n",
      "epoch no.0 train no.20510  loss = 3.15163 avg_loss = 4.35797\n",
      "epoch no.0 train no.20520  loss = 6.32655 avg_loss = 4.39100\n",
      "epoch no.0 train no.20530  loss = 3.52175 avg_loss = 4.34772\n",
      "epoch no.0 train no.20540  loss = 2.77478 avg_loss = 4.35214\n",
      "epoch no.0 train no.20550  loss = 3.63752 avg_loss = 4.28326\n",
      "epoch no.0 train no.20560  loss = 3.02352 avg_loss = 4.30199\n",
      "epoch no.0 train no.20570  loss = 3.76181 avg_loss = 4.27763\n",
      "epoch no.0 train no.20580  loss = 5.30926 avg_loss = 4.27153\n",
      "epoch no.0 train no.20590  loss = 4.25186 avg_loss = 4.27055\n",
      "epoch no.0 train no.20600  loss = 5.16333 avg_loss = 4.29805\n",
      "epoch no.0 train no.20610  loss = 3.36122 avg_loss = 4.23705\n",
      "epoch no.0 train no.20620  loss = 3.63757 avg_loss = 4.25797\n",
      "epoch no.0 train no.20630  loss = 2.41910 avg_loss = 4.17710\n",
      "epoch no.0 train no.20640  loss = 6.27963 avg_loss = 4.16957\n",
      "epoch no.0 train no.20650  loss = 2.55238 avg_loss = 4.09598\n",
      "epoch no.0 train no.20660  loss = 6.97187 avg_loss = 4.11736\n",
      "epoch no.0 train no.20670  loss = 3.74823 avg_loss = 4.09307\n",
      "epoch no.0 train no.20680  loss = 4.08265 avg_loss = 4.09068\n",
      "epoch no.0 train no.20690  loss = 4.48198 avg_loss = 4.09876\n",
      "epoch no.0 train no.20700  loss = 3.60872 avg_loss = 4.11542\n",
      "epoch no.0 train no.20710  loss = 2.90432 avg_loss = 4.08118\n",
      "epoch no.0 train no.20720  loss = 6.16392 avg_loss = 4.06956\n",
      "epoch no.0 train no.20730  loss = 3.72977 avg_loss = 4.04321\n",
      "epoch no.0 train no.20740  loss = 2.83247 avg_loss = 4.06054\n",
      "epoch no.0 train no.20750  loss = 3.34708 avg_loss = 4.06123\n",
      "epoch no.0 train no.20760  loss = 4.76733 avg_loss = 4.07062\n",
      "epoch no.0 train no.20770  loss = 3.06305 avg_loss = 4.04419\n",
      "epoch no.0 train no.20780  loss = 3.17667 avg_loss = 4.00111\n",
      "epoch no.0 train no.20790  loss = 3.40122 avg_loss = 4.02092\n",
      "epoch no.0 train no.20800  loss = 3.32470 avg_loss = 4.02022\n",
      "epoch no.0 train no.20810  loss = 2.83161 avg_loss = 4.01421\n",
      "epoch no.0 train no.20820  loss = 2.34522 avg_loss = 3.99057\n",
      "epoch no.0 train no.20830  loss = 3.37793 avg_loss = 4.00051\n",
      "epoch no.0 train no.20840  loss = 4.25361 avg_loss = 4.01673\n",
      "epoch no.0 train no.20850  loss = 3.53396 avg_loss = 3.98611\n",
      "epoch no.0 train no.20860  loss = 6.68052 avg_loss = 4.00570\n",
      "epoch no.0 train no.20870  loss = 5.12342 avg_loss = 3.95705\n",
      "epoch no.0 train no.20880  loss = 4.30243 avg_loss = 4.05002\n",
      "epoch no.0 train no.20890  loss = 4.28481 avg_loss = 4.11924\n",
      "epoch no.0 train no.20900  loss = 2.89485 avg_loss = 4.08402\n",
      "epoch no.0 train no.20910  loss = 3.62561 avg_loss = 4.03728\n",
      "epoch no.0 train no.20920  loss = 4.95489 avg_loss = 4.05475\n",
      "epoch no.0 train no.20930  loss = 3.82215 avg_loss = 4.07570\n",
      "epoch no.0 train no.20940  loss = 6.53298 avg_loss = 4.06444\n",
      "epoch no.0 train no.20950  loss = 3.30766 avg_loss = 4.11443\n",
      "epoch no.0 train no.20960  loss = 3.59990 avg_loss = 4.14718\n",
      "epoch no.0 train no.20970  loss = 5.96283 avg_loss = 4.16913\n",
      "epoch no.0 train no.20980  loss = 4.14121 avg_loss = 4.14242\n",
      "epoch no.0 train no.20990  loss = 2.51267 avg_loss = 4.11410\n",
      "epoch no.0 train no.21000  loss = 4.49746 avg_loss = 4.14554\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '을', '▁위한', '▁음악', '</s>']\n",
      "기분전환을 위한 재즈</s>\n",
      "epoch no.0 train no.21010  loss = 5.34632 avg_loss = 4.11199\n",
      "epoch no.0 train no.21020  loss = 4.29704 avg_loss = 4.02955\n",
      "epoch no.0 train no.21030  loss = 4.98014 avg_loss = 4.02420\n",
      "epoch no.0 train no.21040  loss = 6.27206 avg_loss = 4.05927\n",
      "epoch no.0 train no.21050  loss = 3.06186 avg_loss = 4.00813\n",
      "epoch no.0 train no.21060  loss = 3.77050 avg_loss = 4.08980\n",
      "epoch no.0 train no.21070  loss = 4.53561 avg_loss = 4.04582\n",
      "epoch no.0 train no.21080  loss = 5.80003 avg_loss = 4.04623\n",
      "epoch no.0 train no.21090  loss = 4.26257 avg_loss = 4.02678\n",
      "epoch no.0 train no.21100  loss = 4.85682 avg_loss = 4.00011\n",
      "epoch no.0 train no.21110  loss = 4.55473 avg_loss = 3.98984\n",
      "epoch no.0 train no.21120  loss = 3.90564 avg_loss = 4.08991\n",
      "epoch no.0 train no.21130  loss = 4.71769 avg_loss = 4.12460\n",
      "epoch no.0 train no.21140  loss = 4.21296 avg_loss = 4.09347\n",
      "epoch no.0 train no.21150  loss = 2.27331 avg_loss = 4.05785\n",
      "epoch no.0 train no.21160  loss = 3.25621 avg_loss = 4.08472\n",
      "epoch no.0 train no.21170  loss = 4.46528 avg_loss = 4.09267\n",
      "epoch no.0 train no.21180  loss = 3.93022 avg_loss = 4.07783\n",
      "epoch no.0 train no.21190  loss = 3.15236 avg_loss = 4.10086\n",
      "epoch no.0 train no.21200  loss = 4.10938 avg_loss = 4.06949\n",
      "epoch no.0 train no.21210  loss = 4.75853 avg_loss = 4.04792\n",
      "epoch no.0 train no.21220  loss = 3.80295 avg_loss = 4.05667\n",
      "epoch no.0 train no.21230  loss = 4.20835 avg_loss = 3.99906\n",
      "epoch no.0 train no.21240  loss = 5.16610 avg_loss = 4.05979\n",
      "epoch no.0 train no.21250  loss = 3.15584 avg_loss = 4.14773\n",
      "epoch no.0 train no.21260  loss = 4.21254 avg_loss = 4.13468\n",
      "epoch no.0 train no.21270  loss = 2.31938 avg_loss = 4.10788\n",
      "epoch no.0 train no.21280  loss = 3.79432 avg_loss = 4.05197\n",
      "epoch no.0 train no.21290  loss = 6.20520 avg_loss = 4.09099\n",
      "epoch no.0 train no.21300  loss = 2.23929 avg_loss = 4.05837\n",
      "epoch no.0 train no.21310  loss = 3.76409 avg_loss = 4.01356\n",
      "epoch no.0 train no.21320  loss = 5.57442 avg_loss = 4.01565\n",
      "epoch no.0 train no.21330  loss = 5.54629 avg_loss = 4.00201\n",
      "epoch no.0 train no.21340  loss = 3.93970 avg_loss = 3.98612\n",
      "epoch no.0 train no.21350  loss = 5.60821 avg_loss = 4.06374\n",
      "epoch no.0 train no.21360  loss = 3.33227 avg_loss = 4.13195\n",
      "epoch no.0 train no.21370  loss = 2.59044 avg_loss = 4.11777\n",
      "epoch no.0 train no.21380  loss = 4.35810 avg_loss = 4.12120\n",
      "epoch no.0 train no.21390  loss = 3.02251 avg_loss = 4.09794\n",
      "epoch no.0 train no.21400  loss = 3.47186 avg_loss = 4.04759\n",
      "epoch no.0 train no.21410  loss = 3.69071 avg_loss = 4.07862\n",
      "epoch no.0 train no.21420  loss = 3.33537 avg_loss = 4.02157\n",
      "epoch no.0 train no.21430  loss = 2.87841 avg_loss = 4.00120\n",
      "epoch no.0 train no.21440  loss = 4.35579 avg_loss = 3.96930\n",
      "epoch no.0 train no.21450  loss = 4.96563 avg_loss = 3.98344\n",
      "epoch no.0 train no.21460  loss = 4.22418 avg_loss = 4.01950\n",
      "epoch no.0 train no.21470  loss = 6.07744 avg_loss = 4.02463\n",
      "epoch no.0 train no.21480  loss = 5.74964 avg_loss = 4.12005\n",
      "epoch no.0 train no.21490  loss = 6.56054 avg_loss = 4.06393\n",
      "epoch no.0 train no.21500  loss = 2.93328 avg_loss = 4.03262\n",
      "epoch no.0 train no.21510  loss = 4.10188 avg_loss = 4.07500\n",
      "epoch no.0 train no.21520  loss = 2.11098 avg_loss = 4.11086\n",
      "epoch no.0 train no.21530  loss = 2.93902 avg_loss = 4.11725\n",
      "epoch no.0 train no.21540  loss = 2.93585 avg_loss = 4.09600\n",
      "epoch no.0 train no.21550  loss = 4.43219 avg_loss = 4.08428\n",
      "epoch no.0 train no.21560  loss = 4.20302 avg_loss = 4.08831\n",
      "epoch no.0 train no.21570  loss = 6.03758 avg_loss = 4.11451\n",
      "epoch no.0 train no.21580  loss = 3.05017 avg_loss = 4.10228\n",
      "epoch no.0 train no.21590  loss = 3.88608 avg_loss = 4.06114\n",
      "epoch no.0 train no.21600  loss = 4.43939 avg_loss = 4.04671\n",
      "epoch no.0 train no.21610  loss = 3.01234 avg_loss = 3.98683\n",
      "epoch no.0 train no.21620  loss = 5.25592 avg_loss = 3.99282\n",
      "epoch no.0 train no.21630  loss = 4.67095 avg_loss = 4.00114\n",
      "epoch no.0 train no.21640  loss = 4.64092 avg_loss = 3.97579\n",
      "epoch no.0 train no.21650  loss = 4.05875 avg_loss = 4.03144\n",
      "epoch no.0 train no.21660  loss = 5.50864 avg_loss = 4.04751\n",
      "epoch no.0 train no.21670  loss = 4.32253 avg_loss = 4.05099\n",
      "epoch no.0 train no.21680  loss = 3.17416 avg_loss = 4.07366\n",
      "epoch no.0 train no.21690  loss = 3.01489 avg_loss = 4.07410\n",
      "epoch no.0 train no.21700  loss = 5.86579 avg_loss = 4.13290\n",
      "epoch no.0 train no.21710  loss = 4.47609 avg_loss = 4.16581\n",
      "epoch no.0 train no.21720  loss = 3.18355 avg_loss = 4.12446\n",
      "epoch no.0 train no.21730  loss = 4.30658 avg_loss = 4.09587\n",
      "epoch no.0 train no.21740  loss = 3.75615 avg_loss = 4.09162\n",
      "epoch no.0 train no.21750  loss = 5.62571 avg_loss = 4.12450\n",
      "epoch no.0 train no.21760  loss = 5.19791 avg_loss = 4.08464\n",
      "epoch no.0 train no.21770  loss = 3.96026 avg_loss = 4.04607\n",
      "epoch no.0 train no.21780  loss = 2.73680 avg_loss = 4.01594\n",
      "epoch no.0 train no.21790  loss = 5.61828 avg_loss = 4.00693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.21800  loss = 2.99254 avg_loss = 3.99210\n",
      "epoch no.0 train no.21810  loss = 6.78718 avg_loss = 3.96400\n",
      "epoch no.0 train no.21820  loss = 2.30155 avg_loss = 3.92846\n",
      "epoch no.0 train no.21830  loss = 4.37504 avg_loss = 3.92952\n",
      "epoch no.0 train no.21840  loss = 3.09147 avg_loss = 3.93286\n",
      "epoch no.0 train no.21850  loss = 4.18666 avg_loss = 3.94087\n",
      "epoch no.0 train no.21860  loss = 3.13294 avg_loss = 3.96923\n",
      "epoch no.0 train no.21870  loss = 4.89999 avg_loss = 4.04860\n",
      "epoch no.0 train no.21880  loss = 3.55903 avg_loss = 4.00772\n",
      "epoch no.0 train no.21890  loss = 4.72769 avg_loss = 4.00371\n",
      "epoch no.0 train no.21900  loss = 4.67307 avg_loss = 4.01982\n",
      "epoch no.0 train no.21910  loss = 5.19189 avg_loss = 4.04475\n",
      "epoch no.0 train no.21920  loss = 3.40038 avg_loss = 4.04895\n",
      "epoch no.0 train no.21930  loss = 2.18609 avg_loss = 4.01594\n",
      "epoch no.0 train no.21940  loss = 4.56111 avg_loss = 3.99886\n",
      "epoch no.0 train no.21950  loss = 2.40097 avg_loss = 4.02061\n",
      "epoch no.0 train no.21960  loss = 3.23215 avg_loss = 3.99146\n",
      "epoch no.0 train no.21970  loss = 4.18774 avg_loss = 4.02371\n",
      "epoch no.0 train no.21980  loss = 6.15126 avg_loss = 4.12120\n",
      "epoch no.0 train no.21990  loss = 2.49881 avg_loss = 4.14120\n",
      "epoch no.0 train no.22000  loss = 2.89094 avg_loss = 4.12970\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁필요할', '▁때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.0 train no.22010  loss = 3.30550 avg_loss = 4.10887\n",
      "epoch no.0 train no.22020  loss = 4.28722 avg_loss = 4.10589\n",
      "epoch no.0 train no.22030  loss = 2.39539 avg_loss = 4.09733\n",
      "epoch no.0 train no.22040  loss = 4.38479 avg_loss = 4.11146\n",
      "epoch no.0 train no.22050  loss = 2.58196 avg_loss = 4.09150\n",
      "epoch no.0 train no.22060  loss = 4.96326 avg_loss = 4.11407\n",
      "epoch no.0 train no.22070  loss = 2.51254 avg_loss = 4.11198\n",
      "epoch no.0 train no.22080  loss = 3.21146 avg_loss = 4.13381\n",
      "epoch no.0 train no.22090  loss = 4.91224 avg_loss = 4.11878\n",
      "epoch no.0 train no.22100  loss = 4.31023 avg_loss = 4.21912\n",
      "epoch no.0 train no.22110  loss = 3.86447 avg_loss = 4.22575\n",
      "epoch no.0 train no.22120  loss = 4.70801 avg_loss = 4.25396\n",
      "epoch no.0 train no.22130  loss = 4.88245 avg_loss = 4.23606\n",
      "epoch no.0 train no.22140  loss = 2.61224 avg_loss = 4.24005\n",
      "epoch no.0 train no.22150  loss = 5.50347 avg_loss = 4.27369\n",
      "epoch no.0 train no.22160  loss = 3.67342 avg_loss = 4.28797\n",
      "epoch no.0 train no.22170  loss = 5.16927 avg_loss = 4.24513\n",
      "epoch no.0 train no.22180  loss = 4.47863 avg_loss = 4.25836\n",
      "epoch no.0 train no.22190  loss = 5.28806 avg_loss = 4.29185\n",
      "epoch no.0 train no.22200  loss = 3.98175 avg_loss = 4.29048\n",
      "epoch no.0 train no.22210  loss = 4.11651 avg_loss = 4.27229\n",
      "epoch no.0 train no.22220  loss = 2.23478 avg_loss = 4.25509\n",
      "epoch no.0 train no.22230  loss = 3.00433 avg_loss = 4.20273\n",
      "epoch no.0 train no.22240  loss = 4.01270 avg_loss = 4.15835\n",
      "epoch no.0 train no.22250  loss = 3.69587 avg_loss = 4.16039\n",
      "epoch no.0 train no.22260  loss = 6.01956 avg_loss = 4.14822\n",
      "epoch no.0 train no.22270  loss = 5.08368 avg_loss = 4.18149\n",
      "epoch no.0 train no.22280  loss = 6.25694 avg_loss = 4.10132\n",
      "epoch no.0 train no.22290  loss = 2.16295 avg_loss = 4.09464\n",
      "epoch no.0 train no.22300  loss = 4.49159 avg_loss = 4.07649\n",
      "epoch no.0 train no.22310  loss = 4.40926 avg_loss = 4.08312\n",
      "epoch no.0 train no.22320  loss = 3.89202 avg_loss = 4.10806\n",
      "epoch no.0 train no.22330  loss = 4.82869 avg_loss = 4.06474\n",
      "epoch no.0 train no.22340  loss = 2.55870 avg_loss = 4.03903\n",
      "epoch no.0 train no.22350  loss = 5.59467 avg_loss = 4.09070\n",
      "epoch no.0 train no.22360  loss = 3.57731 avg_loss = 4.15845\n",
      "epoch no.0 train no.22370  loss = 3.15026 avg_loss = 4.11034\n",
      "epoch no.0 train no.22380  loss = 2.87222 avg_loss = 4.10516\n",
      "epoch no.0 train no.22390  loss = 2.74214 avg_loss = 4.07779\n",
      "epoch no.0 train no.22400  loss = 2.75649 avg_loss = 4.08004\n",
      "epoch no.0 train no.22410  loss = 3.94052 avg_loss = 4.03397\n",
      "epoch no.0 train no.22420  loss = 5.82220 avg_loss = 4.04295\n",
      "epoch no.0 train no.22430  loss = 4.22591 avg_loss = 4.05724\n",
      "epoch no.0 train no.22440  loss = 6.18968 avg_loss = 4.10314\n",
      "epoch no.0 train no.22450  loss = 3.25435 avg_loss = 4.17204\n",
      "epoch no.0 train no.22460  loss = 2.77595 avg_loss = 4.10905\n",
      "epoch no.0 train no.22470  loss = 2.65448 avg_loss = 4.10615\n",
      "epoch no.0 train no.22480  loss = 2.74591 avg_loss = 4.07600\n",
      "epoch no.0 train no.22490  loss = 3.20810 avg_loss = 4.08983\n",
      "epoch no.0 train no.22500  loss = 3.16876 avg_loss = 4.08614\n",
      "epoch no.0 train no.22510  loss = 3.61187 avg_loss = 4.04491\n",
      "epoch no.0 train no.22520  loss = 3.44159 avg_loss = 4.06173\n",
      "epoch no.0 train no.22530  loss = 3.41977 avg_loss = 4.09726\n",
      "epoch no.0 train no.22540  loss = 5.55154 avg_loss = 4.12853\n",
      "epoch no.0 train no.22550  loss = 3.04940 avg_loss = 4.12349\n",
      "epoch no.0 train no.22560  loss = 3.59586 avg_loss = 4.10476\n",
      "epoch no.0 train no.22570  loss = 4.14424 avg_loss = 4.06805\n",
      "epoch no.0 train no.22580  loss = 4.00443 avg_loss = 4.01841\n",
      "epoch no.0 train no.22590  loss = 3.56422 avg_loss = 3.94941\n",
      "epoch no.0 train no.22600  loss = 2.45664 avg_loss = 3.98067\n",
      "epoch no.0 train no.22610  loss = 4.51881 avg_loss = 3.97962\n",
      "epoch no.0 train no.22620  loss = 4.91922 avg_loss = 4.05551\n",
      "epoch no.0 train no.22630  loss = 4.28991 avg_loss = 4.11550\n",
      "epoch no.0 train no.22640  loss = 2.82840 avg_loss = 4.11449\n",
      "epoch no.0 train no.22650  loss = 2.31317 avg_loss = 4.09386\n",
      "epoch no.0 train no.22660  loss = 4.25913 avg_loss = 4.14798\n",
      "epoch no.0 train no.22670  loss = 5.11343 avg_loss = 4.14599\n",
      "epoch no.0 train no.22680  loss = 3.63216 avg_loss = 4.13159\n",
      "epoch no.0 train no.22690  loss = 2.62013 avg_loss = 4.06345\n",
      "epoch no.0 train no.22700  loss = 4.21553 avg_loss = 4.04926\n",
      "epoch no.0 train no.22710  loss = 3.80812 avg_loss = 4.04989\n",
      "epoch no.0 train no.22720  loss = 2.69142 avg_loss = 3.97473\n",
      "epoch no.0 train no.22730  loss = 5.55501 avg_loss = 4.00452\n",
      "epoch no.0 train no.22740  loss = 2.61477 avg_loss = 4.02704\n",
      "epoch no.0 train no.22750  loss = 5.65808 avg_loss = 4.05030\n",
      "epoch no.0 train no.22760  loss = 4.61237 avg_loss = 4.09518\n",
      "epoch no.0 train no.22770  loss = 4.36072 avg_loss = 4.07505\n",
      "epoch no.0 train no.22780  loss = 5.00059 avg_loss = 4.05078\n",
      "epoch no.0 train no.22790  loss = 3.73676 avg_loss = 4.04474\n",
      "epoch no.0 train no.22800  loss = 3.72936 avg_loss = 4.03042\n",
      "epoch no.0 train no.22810  loss = 3.77813 avg_loss = 4.08700\n",
      "epoch no.0 train no.22820  loss = 6.78380 avg_loss = 4.07124\n",
      "epoch no.0 train no.22830  loss = 2.54320 avg_loss = 4.08059\n",
      "epoch no.0 train no.22840  loss = 4.75168 avg_loss = 4.12513\n",
      "epoch no.0 train no.22850  loss = 3.24873 avg_loss = 4.08367\n",
      "epoch no.0 train no.22860  loss = 2.65602 avg_loss = 4.09465\n",
      "epoch no.0 train no.22870  loss = 4.94513 avg_loss = 4.14212\n",
      "epoch no.0 train no.22880  loss = 4.29027 avg_loss = 4.14965\n",
      "epoch no.0 train no.22890  loss = 1.93175 avg_loss = 4.10891\n",
      "epoch no.0 train no.22900  loss = 4.67123 avg_loss = 4.09364\n",
      "epoch no.0 train no.22910  loss = 2.58110 avg_loss = 4.06500\n",
      "epoch no.0 train no.22920  loss = 3.42138 avg_loss = 3.99315\n",
      "epoch no.0 train no.22930  loss = 6.32923 avg_loss = 4.04200\n",
      "epoch no.0 train no.22940  loss = 5.14552 avg_loss = 4.06030\n",
      "epoch no.0 train no.22950  loss = 4.06869 avg_loss = 4.02125\n",
      "epoch no.0 train no.22960  loss = 2.60539 avg_loss = 4.00258\n",
      "epoch no.0 train no.22970  loss = 5.07819 avg_loss = 4.02681\n",
      "epoch no.0 train no.22980  loss = 3.03549 avg_loss = 4.03668\n",
      "epoch no.0 train no.22990  loss = 7.14964 avg_loss = 4.04117\n",
      "epoch no.0 train no.23000  loss = 2.83181 avg_loss = 4.01675\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁음악', '</s>']\n",
      "기분전환하기 좋은날</s>\n",
      "epoch no.0 train no.23010  loss = 3.89726 avg_loss = 4.01036\n",
      "epoch no.0 train no.23020  loss = 2.99939 avg_loss = 4.00531\n",
      "epoch no.0 train no.23030  loss = 3.92938 avg_loss = 3.95032\n",
      "epoch no.0 train no.23040  loss = 3.36154 avg_loss = 3.96877\n",
      "epoch no.0 train no.23050  loss = 2.68268 avg_loss = 3.96859\n",
      "epoch no.0 train no.23060  loss = 4.67132 avg_loss = 4.00908\n",
      "epoch no.0 train no.23070  loss = 3.49809 avg_loss = 4.04865\n",
      "epoch no.0 train no.23080  loss = 2.86863 avg_loss = 4.01128\n",
      "epoch no.0 train no.23090  loss = 6.43210 avg_loss = 4.12777\n",
      "epoch no.0 train no.23100  loss = 4.27236 avg_loss = 4.09670\n",
      "epoch no.0 train no.23110  loss = 2.86070 avg_loss = 4.09585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.23120  loss = 3.45545 avg_loss = 4.09418\n",
      "epoch no.0 train no.23130  loss = 5.70567 avg_loss = 4.10575\n",
      "epoch no.0 train no.23140  loss = 6.54536 avg_loss = 4.09154\n",
      "epoch no.0 train no.23150  loss = 3.52350 avg_loss = 4.13672\n",
      "epoch no.0 train no.23160  loss = 4.25927 avg_loss = 4.15376\n",
      "epoch no.0 train no.23170  loss = 5.67196 avg_loss = 4.13217\n",
      "epoch no.0 train no.23180  loss = 2.79862 avg_loss = 4.16939\n",
      "epoch no.0 train no.23190  loss = 3.91039 avg_loss = 4.15307\n",
      "epoch no.0 train no.23200  loss = 3.99129 avg_loss = 4.20688\n",
      "epoch no.0 train no.23210  loss = 4.12399 avg_loss = 4.16363\n",
      "epoch no.0 train no.23220  loss = 5.18593 avg_loss = 4.20718\n",
      "epoch no.0 train no.23230  loss = 3.33121 avg_loss = 4.18327\n",
      "epoch no.0 train no.23240  loss = 2.45385 avg_loss = 4.16183\n",
      "epoch no.0 train no.23250  loss = 5.93961 avg_loss = 4.22402\n",
      "epoch no.0 train no.23260  loss = 3.33239 avg_loss = 4.22233\n",
      "epoch no.0 train no.23270  loss = 4.68990 avg_loss = 4.28354\n",
      "epoch no.0 train no.23280  loss = 1.93197 avg_loss = 4.24146\n",
      "epoch no.0 train no.23290  loss = 6.05233 avg_loss = 4.23241\n",
      "epoch no.0 train no.23300  loss = 2.57728 avg_loss = 4.20768\n",
      "epoch no.0 train no.23310  loss = 2.82687 avg_loss = 4.12605\n",
      "epoch no.0 train no.23320  loss = 3.30981 avg_loss = 4.09171\n",
      "epoch no.0 train no.23330  loss = 3.43131 avg_loss = 4.09649\n",
      "epoch no.0 train no.23340  loss = 5.17345 avg_loss = 4.08200\n",
      "epoch no.0 train no.23350  loss = 6.95210 avg_loss = 4.09998\n",
      "epoch no.0 train no.23360  loss = 4.13441 avg_loss = 4.10428\n",
      "epoch no.0 train no.23370  loss = 5.48419 avg_loss = 4.14268\n",
      "epoch no.0 train no.23380  loss = 4.23196 avg_loss = 4.15936\n",
      "epoch no.0 train no.23390  loss = 6.78554 avg_loss = 4.14417\n",
      "epoch no.0 train no.23400  loss = 5.38196 avg_loss = 4.20543\n",
      "epoch no.0 train no.23410  loss = 2.87985 avg_loss = 4.16942\n",
      "epoch no.0 train no.23420  loss = 3.45082 avg_loss = 4.15358\n",
      "epoch no.0 train no.23430  loss = 3.21403 avg_loss = 4.16284\n",
      "epoch no.0 train no.23440  loss = 5.48813 avg_loss = 4.19369\n",
      "epoch no.0 train no.23450  loss = 2.82850 avg_loss = 4.22061\n",
      "epoch no.0 train no.23460  loss = 2.63619 avg_loss = 4.18296\n",
      "epoch no.0 train no.23470  loss = 4.08195 avg_loss = 4.16669\n",
      "epoch no.0 train no.23480  loss = 5.71232 avg_loss = 4.19229\n",
      "epoch no.0 train no.23490  loss = 4.75311 avg_loss = 4.21443\n",
      "epoch no.0 train no.23500  loss = 4.27578 avg_loss = 4.18793\n",
      "epoch no.0 train no.23510  loss = 2.21755 avg_loss = 4.17314\n",
      "epoch no.0 train no.23520  loss = 5.67324 avg_loss = 4.15162\n",
      "epoch no.0 train no.23530  loss = 5.03686 avg_loss = 4.10325\n",
      "epoch no.0 train no.23540  loss = 5.60835 avg_loss = 4.13817\n",
      "epoch no.0 train no.23550  loss = 3.67144 avg_loss = 4.09319\n",
      "epoch no.0 train no.23560  loss = 3.87589 avg_loss = 4.06639\n",
      "epoch no.0 train no.23570  loss = 4.66885 avg_loss = 4.06461\n",
      "epoch no.0 train no.23580  loss = 4.15945 avg_loss = 4.03821\n",
      "epoch no.0 train no.23590  loss = 3.57093 avg_loss = 4.02778\n",
      "epoch no.0 train no.23600  loss = 3.08152 avg_loss = 4.03739\n",
      "epoch no.0 train no.23610  loss = 6.81135 avg_loss = 4.03235\n",
      "epoch no.0 train no.23620  loss = 4.78190 avg_loss = 4.05677\n",
      "epoch no.0 train no.23630  loss = 4.18685 avg_loss = 4.02996\n",
      "epoch no.0 train no.23640  loss = 4.46886 avg_loss = 4.00377\n",
      "epoch no.0 train no.23650  loss = 2.91266 avg_loss = 3.96193\n",
      "epoch no.0 train no.23660  loss = 4.59237 avg_loss = 3.95102\n",
      "epoch no.0 train no.23670  loss = 4.73799 avg_loss = 3.99163\n",
      "epoch no.0 train no.23680  loss = 3.06017 avg_loss = 3.95219\n",
      "epoch no.0 train no.23690  loss = 4.80373 avg_loss = 4.00821\n",
      "epoch no.0 train no.23700  loss = 5.73027 avg_loss = 3.95175\n",
      "epoch no.0 train no.23710  loss = 4.47216 avg_loss = 3.92814\n",
      "epoch no.0 train no.23720  loss = 2.27969 avg_loss = 3.94528\n",
      "epoch no.0 train no.23730  loss = 3.77815 avg_loss = 3.95396\n",
      "epoch no.0 train no.23740  loss = 3.11664 avg_loss = 3.98327\n",
      "epoch no.0 train no.23750  loss = 4.18147 avg_loss = 4.00046\n",
      "epoch no.0 train no.23760  loss = 3.07085 avg_loss = 4.00521\n",
      "epoch no.0 train no.23770  loss = 4.32476 avg_loss = 3.98877\n",
      "epoch no.0 train no.23780  loss = 5.35977 avg_loss = 4.01469\n",
      "epoch no.0 train no.23790  loss = 4.78482 avg_loss = 4.08990\n",
      "epoch no.0 train no.23800  loss = 3.83556 avg_loss = 4.03770\n",
      "epoch no.0 train no.23810  loss = 2.66673 avg_loss = 4.02251\n",
      "epoch no.0 train no.23820  loss = 3.58013 avg_loss = 4.05314\n",
      "epoch no.0 train no.23830  loss = 4.61787 avg_loss = 4.06081\n",
      "epoch no.0 train no.23840  loss = 6.21239 avg_loss = 4.09771\n",
      "epoch no.0 train no.23850  loss = 3.69561 avg_loss = 4.13401\n",
      "epoch no.0 train no.23860  loss = 2.50216 avg_loss = 4.17518\n",
      "epoch no.0 train no.23870  loss = 3.64193 avg_loss = 4.18730\n",
      "epoch no.0 train no.23880  loss = 3.17971 avg_loss = 4.18608\n",
      "epoch no.0 train no.23890  loss = 3.56202 avg_loss = 4.12614\n",
      "epoch no.0 train no.23900  loss = 3.00323 avg_loss = 4.10541\n",
      "epoch no.0 train no.23910  loss = 2.97408 avg_loss = 4.10654\n",
      "epoch no.0 train no.23920  loss = 2.43396 avg_loss = 4.15942\n",
      "epoch no.0 train no.23930  loss = 4.07843 avg_loss = 4.15123\n",
      "epoch no.0 train no.23940  loss = 4.88035 avg_loss = 4.17674\n",
      "epoch no.0 train no.23950  loss = 6.10157 avg_loss = 4.18302\n",
      "epoch no.0 train no.23960  loss = 5.43547 avg_loss = 4.20869\n",
      "epoch no.0 train no.23970  loss = 5.79044 avg_loss = 4.17066\n",
      "epoch no.0 train no.23980  loss = 3.78021 avg_loss = 4.16223\n",
      "epoch no.0 train no.23990  loss = 3.91352 avg_loss = 4.10990\n",
      "epoch no.0 train no.24000  loss = 4.19981 avg_loss = 4.12961\n",
      "3\n",
      "to_tokens: ['▁가을', '좋은', '이', '▁싶을', '때', '▁듣는']\n",
      "기분전환하고 싶을때</s>\n",
      "epoch no.0 train no.24010  loss = 5.40803 avg_loss = 4.07747\n",
      "epoch no.0 train no.24020  loss = 2.18085 avg_loss = 4.02173\n",
      "epoch no.0 train no.24030  loss = 3.97237 avg_loss = 3.94958\n",
      "epoch no.0 train no.24040  loss = 2.54646 avg_loss = 3.91515\n",
      "epoch no.0 train no.24050  loss = 4.27162 avg_loss = 3.97071\n",
      "epoch no.0 train no.24060  loss = 3.27828 avg_loss = 4.00831\n",
      "epoch no.0 train no.24070  loss = 3.86922 avg_loss = 4.02029\n",
      "epoch no.0 train no.24080  loss = 5.60292 avg_loss = 4.05732\n",
      "epoch no.0 train no.24090  loss = 2.81222 avg_loss = 4.01880\n",
      "epoch no.0 train no.24100  loss = 5.24647 avg_loss = 4.06162\n",
      "epoch no.0 train no.24110  loss = 4.00889 avg_loss = 4.06556\n",
      "epoch no.0 train no.24120  loss = 3.90063 avg_loss = 4.04697\n",
      "epoch no.0 train no.24130  loss = 4.53496 avg_loss = 4.02910\n",
      "epoch no.0 train no.24140  loss = 3.85779 avg_loss = 4.06582\n",
      "epoch no.0 train no.24150  loss = 3.16514 avg_loss = 4.13239\n",
      "epoch no.0 train no.24160  loss = 3.69398 avg_loss = 4.08067\n",
      "epoch no.0 train no.24170  loss = 3.22422 avg_loss = 3.99392\n",
      "epoch no.0 train no.24180  loss = 4.31274 avg_loss = 4.00792\n",
      "epoch no.0 train no.24190  loss = 2.28441 avg_loss = 4.02050\n",
      "epoch no.0 train no.24200  loss = 4.39375 avg_loss = 4.07880\n",
      "epoch no.0 train no.24210  loss = 3.91803 avg_loss = 4.09185\n",
      "epoch no.0 train no.24220  loss = 3.86858 avg_loss = 4.06735\n",
      "epoch no.0 train no.24230  loss = 3.36756 avg_loss = 4.02020\n",
      "epoch no.0 train no.24240  loss = 3.07643 avg_loss = 4.00323\n",
      "epoch no.0 train no.24250  loss = 3.05147 avg_loss = 3.98818\n",
      "epoch no.0 train no.24260  loss = 4.19248 avg_loss = 4.00198\n",
      "epoch no.0 train no.24270  loss = 2.85189 avg_loss = 3.95108\n",
      "epoch no.0 train no.24280  loss = 5.29259 avg_loss = 3.98776\n",
      "epoch no.0 train no.24290  loss = 3.50333 avg_loss = 3.97628\n",
      "epoch no.0 train no.24300  loss = 3.28743 avg_loss = 4.04094\n",
      "epoch no.0 train no.24310  loss = 4.35105 avg_loss = 4.00565\n",
      "epoch no.0 train no.24320  loss = 3.15049 avg_loss = 4.01048\n",
      "epoch no.0 train no.24330  loss = 5.28303 avg_loss = 4.05130\n",
      "epoch no.0 train no.24340  loss = 2.95475 avg_loss = 4.03283\n",
      "epoch no.0 train no.24350  loss = 3.22762 avg_loss = 4.01547\n",
      "epoch no.0 train no.24360  loss = 2.53561 avg_loss = 3.99429\n",
      "epoch no.0 train no.24370  loss = 3.04169 avg_loss = 4.02861\n",
      "epoch no.0 train no.24380  loss = 3.26420 avg_loss = 3.97946\n",
      "epoch no.0 train no.24390  loss = 3.32838 avg_loss = 3.96344\n",
      "epoch no.0 train no.24400  loss = 4.52224 avg_loss = 3.97761\n",
      "epoch no.0 train no.24410  loss = 2.26873 avg_loss = 3.93203\n",
      "epoch no.0 train no.24420  loss = 4.98501 avg_loss = 3.91437\n",
      "epoch no.0 train no.24430  loss = 3.06702 avg_loss = 3.92003\n",
      "epoch no.0 train no.24440  loss = 2.61180 avg_loss = 4.00408\n",
      "epoch no.0 train no.24450  loss = 5.17166 avg_loss = 3.93637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.24460  loss = 3.82599 avg_loss = 3.92519\n",
      "epoch no.0 train no.24470  loss = 4.04444 avg_loss = 3.95381\n",
      "epoch no.0 train no.24480  loss = 5.81934 avg_loss = 3.97857\n",
      "epoch no.0 train no.24490  loss = 4.47273 avg_loss = 4.00364\n",
      "epoch no.0 train no.24500  loss = 6.22067 avg_loss = 4.01343\n",
      "epoch no.0 train no.24510  loss = 2.88350 avg_loss = 4.01571\n",
      "epoch no.0 train no.24520  loss = 4.18998 avg_loss = 4.01448\n",
      "epoch no.0 train no.24530  loss = 3.48701 avg_loss = 4.02803\n",
      "epoch no.0 train no.24540  loss = 4.67486 avg_loss = 4.05143\n",
      "epoch no.0 train no.24550  loss = 6.89047 avg_loss = 4.07383\n",
      "epoch no.0 train no.24560  loss = 3.05350 avg_loss = 4.02096\n",
      "epoch no.0 train no.24570  loss = 2.85725 avg_loss = 4.04759\n",
      "epoch no.0 train no.24580  loss = 6.36206 avg_loss = 4.04812\n",
      "epoch no.0 train no.24590  loss = 4.93569 avg_loss = 4.01239\n",
      "epoch no.0 train no.24600  loss = 1.97407 avg_loss = 3.95118\n",
      "epoch no.0 train no.24610  loss = 3.89837 avg_loss = 3.96103\n",
      "epoch no.0 train no.24620  loss = 4.95936 avg_loss = 4.07485\n",
      "epoch no.0 train no.24630  loss = 5.97285 avg_loss = 4.05899\n",
      "epoch no.0 train no.24640  loss = 3.85952 avg_loss = 4.06880\n",
      "epoch no.0 train no.24650  loss = 3.88819 avg_loss = 4.09319\n",
      "epoch no.0 train no.24660  loss = 6.03988 avg_loss = 4.08529\n",
      "epoch no.0 train no.24670  loss = 3.28508 avg_loss = 4.10665\n",
      "epoch no.0 train no.24680  loss = 2.76252 avg_loss = 4.06761\n",
      "epoch no.0 train no.24690  loss = 3.35562 avg_loss = 4.08914\n",
      "epoch no.0 train no.24700  loss = 2.80448 avg_loss = 4.06765\n",
      "epoch no.0 train no.24710  loss = 6.37989 avg_loss = 4.11647\n",
      "epoch no.0 train no.24720  loss = 4.53860 avg_loss = 4.13802\n",
      "epoch no.0 train no.24730  loss = 2.70545 avg_loss = 4.15629\n",
      "epoch no.0 train no.24740  loss = 1.98851 avg_loss = 4.12513\n",
      "epoch no.0 train no.24750  loss = 6.42285 avg_loss = 4.13973\n",
      "epoch no.0 train no.24760  loss = 3.98604 avg_loss = 4.14580\n",
      "epoch no.0 train no.24770  loss = 5.41214 avg_loss = 4.10774\n",
      "epoch no.0 train no.24780  loss = 4.49139 avg_loss = 4.12618\n",
      "epoch no.0 train no.24790  loss = 3.35986 avg_loss = 4.12182\n",
      "epoch no.0 train no.24800  loss = 5.21502 avg_loss = 4.12339\n",
      "epoch no.0 train no.24810  loss = 5.76865 avg_loss = 4.16076\n",
      "epoch no.0 train no.24820  loss = 4.75493 avg_loss = 4.14507\n",
      "epoch no.0 train no.24830  loss = 5.41799 avg_loss = 4.11553\n",
      "epoch no.0 train no.24840  loss = 2.63531 avg_loss = 4.03780\n",
      "epoch no.0 train no.24850  loss = 5.42569 avg_loss = 4.02786\n",
      "epoch no.0 train no.24860  loss = 4.51927 avg_loss = 4.05271\n",
      "epoch no.0 train no.24870  loss = 3.19136 avg_loss = 4.05693\n",
      "epoch no.0 train no.24880  loss = 4.56024 avg_loss = 4.09781\n",
      "epoch no.0 train no.24890  loss = 2.92421 avg_loss = 4.06200\n",
      "epoch no.0 train no.24900  loss = 3.25824 avg_loss = 4.07922\n",
      "epoch no.0 train no.24910  loss = 3.00360 avg_loss = 4.09103\n",
      "epoch no.0 train no.24920  loss = 4.65691 avg_loss = 4.03062\n",
      "epoch no.0 train no.24930  loss = 4.51079 avg_loss = 4.04867\n",
      "epoch no.0 train no.24940  loss = 2.52554 avg_loss = 4.03232\n",
      "epoch no.0 train no.24950  loss = 2.37456 avg_loss = 4.02714\n",
      "epoch no.0 train no.24960  loss = 6.00038 avg_loss = 4.01462\n",
      "epoch no.0 train no.24970  loss = 3.45191 avg_loss = 4.03807\n",
      "epoch no.0 train no.24980  loss = 5.42221 avg_loss = 4.05333\n",
      "epoch no.0 train no.24990  loss = 3.27901 avg_loss = 4.02068\n",
      "epoch no.0 train no.25000  loss = 5.15998 avg_loss = 3.93790\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁노래', '음악', '</s>']\n",
      "기분전환 신나는 인디음악</s>\n",
      "epoch no.0 train no.25010  loss = 2.99239 avg_loss = 3.92997\n",
      "epoch no.0 train no.25020  loss = 4.22978 avg_loss = 3.92312\n",
      "epoch no.0 train no.25030  loss = 3.27178 avg_loss = 3.91975\n",
      "epoch no.0 train no.25040  loss = 5.72005 avg_loss = 3.97131\n",
      "epoch no.0 train no.25050  loss = 3.79137 avg_loss = 3.97466\n",
      "epoch no.0 train no.25060  loss = 2.36670 avg_loss = 3.98303\n",
      "epoch no.0 train no.25070  loss = 2.59470 avg_loss = 3.93933\n",
      "epoch no.0 train no.25080  loss = 4.80502 avg_loss = 3.97717\n",
      "epoch no.0 train no.25090  loss = 4.01629 avg_loss = 4.02070\n",
      "epoch no.0 train no.25100  loss = 5.23574 avg_loss = 4.09064\n",
      "epoch no.0 train no.25110  loss = 4.56662 avg_loss = 4.17313\n",
      "epoch no.0 train no.25120  loss = 3.71866 avg_loss = 4.14009\n",
      "epoch no.0 train no.25130  loss = 3.45242 avg_loss = 4.11493\n",
      "epoch no.0 train no.25140  loss = 2.21768 avg_loss = 4.07846\n",
      "epoch no.0 train no.25150  loss = 5.03480 avg_loss = 4.07318\n",
      "epoch no.0 train no.25160  loss = 6.07249 avg_loss = 4.05905\n",
      "epoch no.0 train no.25170  loss = 5.58939 avg_loss = 4.09050\n",
      "epoch no.0 train no.25180  loss = 5.68683 avg_loss = 4.09542\n",
      "epoch no.0 train no.25190  loss = 4.73489 avg_loss = 4.11434\n",
      "epoch no.0 train no.25200  loss = 4.81207 avg_loss = 4.09265\n",
      "epoch no.0 train no.25210  loss = 3.50233 avg_loss = 4.07380\n",
      "epoch no.0 train no.25220  loss = 4.63759 avg_loss = 4.06670\n",
      "epoch no.0 train no.25230  loss = 4.54328 avg_loss = 4.08708\n",
      "epoch no.0 train no.25240  loss = 3.89003 avg_loss = 4.06734\n",
      "epoch no.0 train no.25250  loss = 4.92624 avg_loss = 4.13263\n",
      "epoch no.0 train no.25260  loss = 2.78345 avg_loss = 4.05559\n",
      "epoch no.0 train no.25270  loss = 4.89382 avg_loss = 4.07360\n",
      "epoch no.0 train no.25280  loss = 2.94523 avg_loss = 4.11303\n",
      "epoch no.0 train no.25290  loss = 2.88361 avg_loss = 4.09932\n",
      "epoch no.0 train no.25300  loss = 2.44291 avg_loss = 4.06464\n",
      "epoch no.0 train no.25310  loss = 2.39587 avg_loss = 4.02040\n",
      "epoch no.0 train no.25320  loss = 2.88152 avg_loss = 3.97824\n",
      "epoch no.0 train no.25330  loss = 4.21598 avg_loss = 4.08575\n",
      "epoch no.0 train no.25340  loss = 3.34934 avg_loss = 4.11446\n",
      "epoch no.0 train no.25350  loss = 3.82352 avg_loss = 4.12800\n",
      "epoch no.0 train no.25360  loss = 5.71401 avg_loss = 4.12968\n",
      "epoch no.0 train no.25370  loss = 4.16900 avg_loss = 4.15309\n",
      "epoch no.0 train no.25380  loss = 3.94834 avg_loss = 4.16246\n",
      "epoch no.0 train no.25390  loss = 6.15484 avg_loss = 4.16060\n",
      "epoch no.0 train no.25400  loss = 5.01814 avg_loss = 4.15431\n",
      "epoch no.0 train no.25410  loss = 4.87730 avg_loss = 4.15748\n",
      "epoch no.0 train no.25420  loss = 3.17103 avg_loss = 4.11500\n",
      "epoch no.0 train no.25430  loss = 4.99911 avg_loss = 4.11543\n",
      "epoch no.0 train no.25440  loss = 6.42907 avg_loss = 4.10261\n",
      "epoch no.0 train no.25450  loss = 6.33320 avg_loss = 4.12140\n",
      "epoch no.0 train no.25460  loss = 2.77778 avg_loss = 4.13741\n",
      "epoch no.0 train no.25470  loss = 6.13521 avg_loss = 4.17587\n",
      "epoch no.0 train no.25480  loss = 2.38959 avg_loss = 4.18582\n",
      "epoch no.0 train no.25490  loss = 4.92019 avg_loss = 4.17497\n",
      "epoch no.0 train no.25500  loss = 3.58079 avg_loss = 4.18227\n",
      "epoch no.0 train no.25510  loss = 3.14825 avg_loss = 4.19143\n",
      "epoch no.0 train no.25520  loss = 2.35700 avg_loss = 4.13071\n",
      "epoch no.0 train no.25530  loss = 2.33154 avg_loss = 4.14572\n",
      "epoch no.0 train no.25540  loss = 4.00927 avg_loss = 4.15722\n",
      "epoch no.0 train no.25550  loss = 3.30088 avg_loss = 4.24167\n",
      "epoch no.0 train no.25560  loss = 2.79263 avg_loss = 4.21511\n",
      "epoch no.0 train no.25570  loss = 5.42569 avg_loss = 4.23985\n",
      "epoch no.0 train no.25580  loss = 4.21158 avg_loss = 4.25500\n",
      "epoch no.0 train no.25590  loss = 2.96413 avg_loss = 4.22241\n",
      "epoch no.0 train no.25600  loss = 4.35403 avg_loss = 4.18934\n",
      "epoch no.0 train no.25610  loss = 5.19618 avg_loss = 4.24505\n",
      "epoch no.0 train no.25620  loss = 4.44084 avg_loss = 4.24235\n",
      "epoch no.0 train no.25630  loss = 1.81102 avg_loss = 4.17410\n",
      "epoch no.0 train no.25640  loss = 3.05661 avg_loss = 4.17288\n",
      "epoch no.0 train no.25650  loss = 2.29538 avg_loss = 4.13581\n",
      "epoch no.0 train no.25660  loss = 3.15792 avg_loss = 4.10786\n",
      "epoch no.0 train no.25670  loss = 5.46005 avg_loss = 4.18044\n",
      "epoch no.0 train no.25680  loss = 4.13113 avg_loss = 4.11749\n",
      "epoch no.0 train no.25690  loss = 5.46862 avg_loss = 4.12209\n",
      "epoch no.0 train no.25700  loss = 5.25155 avg_loss = 4.21384\n",
      "epoch no.0 train no.25710  loss = 6.02411 avg_loss = 4.24332\n",
      "epoch no.0 train no.25720  loss = 4.97667 avg_loss = 4.19191\n",
      "epoch no.0 train no.25730  loss = 3.34613 avg_loss = 4.15311\n",
      "epoch no.0 train no.25740  loss = 4.36542 avg_loss = 4.11693\n",
      "epoch no.0 train no.25750  loss = 2.27605 avg_loss = 4.08091\n",
      "epoch no.0 train no.25760  loss = 4.45940 avg_loss = 4.05269\n",
      "epoch no.0 train no.25770  loss = 4.09284 avg_loss = 4.13502\n",
      "epoch no.0 train no.25780  loss = 3.98220 avg_loss = 4.08574\n",
      "epoch no.0 train no.25790  loss = 4.87124 avg_loss = 4.07998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.25800  loss = 3.80908 avg_loss = 4.07493\n",
      "epoch no.0 train no.25810  loss = 4.24479 avg_loss = 4.06190\n",
      "epoch no.0 train no.25820  loss = 3.42703 avg_loss = 4.04258\n",
      "epoch no.0 train no.25830  loss = 3.03066 avg_loss = 4.00949\n",
      "epoch no.0 train no.25840  loss = 4.66798 avg_loss = 4.03615\n",
      "epoch no.0 train no.25850  loss = 3.64001 avg_loss = 4.01420\n",
      "epoch no.0 train no.25860  loss = 3.38198 avg_loss = 4.00699\n",
      "epoch no.0 train no.25870  loss = 2.42046 avg_loss = 3.98013\n",
      "epoch no.0 train no.25880  loss = 6.97446 avg_loss = 4.08075\n",
      "epoch no.0 train no.25890  loss = 5.04538 avg_loss = 4.08260\n",
      "epoch no.0 train no.25900  loss = 3.67753 avg_loss = 4.08973\n",
      "epoch no.0 train no.25910  loss = 5.31459 avg_loss = 4.09445\n",
      "epoch no.0 train no.25920  loss = 2.56995 avg_loss = 4.05805\n",
      "epoch no.0 train no.25930  loss = 3.34922 avg_loss = 4.09442\n",
      "epoch no.0 train no.25940  loss = 6.19057 avg_loss = 4.08603\n",
      "epoch no.0 train no.25950  loss = 3.10970 avg_loss = 4.10751\n",
      "epoch no.0 train no.25960  loss = 3.90854 avg_loss = 4.08465\n",
      "epoch no.0 train no.25970  loss = 5.00505 avg_loss = 4.07412\n",
      "epoch no.0 train no.25980  loss = 4.93593 avg_loss = 4.03195\n",
      "epoch no.0 train no.25990  loss = 4.33984 avg_loss = 4.06288\n",
      "epoch no.0 train no.26000  loss = 3.09744 avg_loss = 4.09576\n",
      "3\n",
      "to_tokens: ['▁가을', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.0 train no.26010  loss = 2.89016 avg_loss = 4.06470\n",
      "epoch no.0 train no.26020  loss = 3.19191 avg_loss = 4.09112\n",
      "epoch no.0 train no.26030  loss = 3.65871 avg_loss = 4.09640\n",
      "epoch no.0 train no.26040  loss = 6.79907 avg_loss = 4.07808\n",
      "epoch no.0 train no.26050  loss = 2.73062 avg_loss = 4.08323\n",
      "epoch no.0 train no.26060  loss = 3.96187 avg_loss = 4.08433\n",
      "epoch no.0 train no.26070  loss = 3.75874 avg_loss = 4.07032\n",
      "epoch no.0 train no.26080  loss = 5.62195 avg_loss = 4.01240\n",
      "epoch no.0 train no.26090  loss = 4.71572 avg_loss = 3.96249\n",
      "epoch no.0 train no.26100  loss = 6.85306 avg_loss = 3.98448\n",
      "epoch no.0 train no.26110  loss = 6.29786 avg_loss = 4.04839\n",
      "epoch no.0 train no.26120  loss = 3.28868 avg_loss = 3.98208\n",
      "epoch no.0 train no.26130  loss = 2.81997 avg_loss = 3.99072\n",
      "epoch no.0 train no.26140  loss = 2.36065 avg_loss = 3.97465\n",
      "epoch no.0 train no.26150  loss = 3.82492 avg_loss = 3.97207\n",
      "epoch no.0 train no.26160  loss = 2.17921 avg_loss = 3.95667\n",
      "epoch no.0 train no.26170  loss = 4.91808 avg_loss = 3.94597\n",
      "epoch no.0 train no.26180  loss = 6.03517 avg_loss = 4.03699\n",
      "epoch no.0 train no.26190  loss = 4.46455 avg_loss = 4.08526\n",
      "epoch no.0 train no.26200  loss = 7.50227 avg_loss = 4.11538\n",
      "epoch no.0 train no.26210  loss = 2.61972 avg_loss = 4.13267\n",
      "epoch no.0 train no.26220  loss = 4.51745 avg_loss = 4.16158\n",
      "epoch no.0 train no.26230  loss = 3.88750 avg_loss = 4.15510\n",
      "epoch no.0 train no.26240  loss = 1.98240 avg_loss = 4.15367\n",
      "epoch no.0 train no.26250  loss = 3.28232 avg_loss = 4.15129\n",
      "epoch no.0 train no.26260  loss = 3.70826 avg_loss = 4.14574\n",
      "epoch no.0 train no.26270  loss = 3.62484 avg_loss = 4.13677\n",
      "epoch no.0 train no.26280  loss = 5.04956 avg_loss = 4.20118\n",
      "epoch no.0 train no.26290  loss = 6.07183 avg_loss = 4.23491\n",
      "epoch no.0 train no.26300  loss = 3.35847 avg_loss = 4.26110\n",
      "epoch no.0 train no.26310  loss = 4.82941 avg_loss = 4.23679\n",
      "epoch no.0 train no.26320  loss = 4.80826 avg_loss = 4.22970\n",
      "epoch no.0 train no.26330  loss = 3.95704 avg_loss = 4.23757\n",
      "epoch no.0 train no.26340  loss = 2.73273 avg_loss = 4.25809\n",
      "epoch no.0 train no.26350  loss = 3.99617 avg_loss = 4.24736\n",
      "epoch no.0 train no.26360  loss = 4.44071 avg_loss = 4.26518\n",
      "epoch no.0 train no.26370  loss = 4.86188 avg_loss = 4.28461\n",
      "epoch no.0 train no.26380  loss = 6.40873 avg_loss = 4.28091\n",
      "epoch no.0 train no.26390  loss = 2.30515 avg_loss = 4.24860\n",
      "epoch no.0 train no.26400  loss = 3.63869 avg_loss = 4.25232\n",
      "epoch no.0 train no.26410  loss = 4.35672 avg_loss = 4.23156\n",
      "epoch no.0 train no.26420  loss = 4.74239 avg_loss = 4.15771\n",
      "epoch no.0 train no.26430  loss = 5.46310 avg_loss = 4.18394\n",
      "epoch no.0 train no.26440  loss = 3.81028 avg_loss = 4.14393\n",
      "epoch no.0 train no.26450  loss = 3.28039 avg_loss = 4.18936\n",
      "epoch no.0 train no.26460  loss = 4.04978 avg_loss = 4.15112\n",
      "epoch no.0 train no.26470  loss = 3.62753 avg_loss = 4.15052\n",
      "epoch no.0 train no.26480  loss = 7.08041 avg_loss = 4.22683\n",
      "epoch no.0 train no.26490  loss = 3.85316 avg_loss = 4.18936\n",
      "epoch no.0 train no.26500  loss = 1.51712 avg_loss = 4.13502\n",
      "epoch no.0 train no.26510  loss = 3.48349 avg_loss = 4.11137\n",
      "epoch no.0 train no.26520  loss = 3.02534 avg_loss = 4.11132\n",
      "epoch no.0 train no.26530  loss = 2.89213 avg_loss = 4.07205\n",
      "epoch no.0 train no.26540  loss = 4.63663 avg_loss = 4.06892\n",
      "epoch no.0 train no.26550  loss = 4.67820 avg_loss = 4.11184\n",
      "epoch no.0 train no.26560  loss = 3.73311 avg_loss = 4.10911\n",
      "epoch no.0 train no.26570  loss = 7.27173 avg_loss = 4.07321\n",
      "epoch no.0 train no.26580  loss = 2.42974 avg_loss = 4.04251\n",
      "epoch no.0 train no.26590  loss = 2.69184 avg_loss = 4.02113\n",
      "epoch no.0 train no.26600  loss = 3.53829 avg_loss = 4.02913\n",
      "epoch no.0 train no.26610  loss = 3.19800 avg_loss = 4.01733\n",
      "epoch no.0 train no.26620  loss = 6.00461 avg_loss = 4.02044\n",
      "epoch no.0 train no.26630  loss = 2.24791 avg_loss = 4.00895\n",
      "epoch no.0 train no.26640  loss = 4.37209 avg_loss = 4.04584\n",
      "epoch no.0 train no.26650  loss = 4.24978 avg_loss = 4.06092\n",
      "epoch no.0 train no.26660  loss = 2.88232 avg_loss = 4.03344\n",
      "epoch no.0 train no.26670  loss = 6.02308 avg_loss = 4.01418\n",
      "epoch no.0 train no.26680  loss = 3.56423 avg_loss = 4.01579\n",
      "epoch no.0 train no.26690  loss = 2.89216 avg_loss = 3.99257\n",
      "epoch no.0 train no.26700  loss = 3.33942 avg_loss = 3.92218\n",
      "epoch no.0 train no.26710  loss = 5.27778 avg_loss = 3.95173\n",
      "epoch no.0 train no.26720  loss = 4.44106 avg_loss = 3.97507\n",
      "epoch no.0 train no.26730  loss = 3.36646 avg_loss = 3.94786\n",
      "epoch no.0 train no.26740  loss = 4.90705 avg_loss = 4.01480\n",
      "epoch no.0 train no.26750  loss = 4.47086 avg_loss = 4.03762\n",
      "epoch no.0 train no.26760  loss = 5.58858 avg_loss = 4.12618\n",
      "epoch no.0 train no.26770  loss = 1.64577 avg_loss = 4.09124\n",
      "epoch no.0 train no.26780  loss = 4.66458 avg_loss = 4.13899\n",
      "epoch no.0 train no.26790  loss = 5.39103 avg_loss = 4.16853\n",
      "epoch no.0 train no.26800  loss = 2.53938 avg_loss = 4.10686\n",
      "epoch no.0 train no.26810  loss = 4.53338 avg_loss = 4.12924\n",
      "epoch no.0 train no.26820  loss = 3.95885 avg_loss = 4.11553\n",
      "epoch no.0 train no.26830  loss = 4.81021 avg_loss = 4.14352\n",
      "epoch no.0 train no.26840  loss = 4.14711 avg_loss = 4.14775\n",
      "epoch no.0 train no.26850  loss = 4.76463 avg_loss = 4.17773\n",
      "epoch no.0 train no.26860  loss = 3.03846 avg_loss = 4.18335\n",
      "epoch no.0 train no.26870  loss = 6.80407 avg_loss = 4.25566\n",
      "epoch no.0 train no.26880  loss = 4.53934 avg_loss = 4.27903\n",
      "epoch no.0 train no.26890  loss = 3.64433 avg_loss = 4.25842\n",
      "epoch no.0 train no.26900  loss = 5.31969 avg_loss = 4.27397\n",
      "epoch no.0 train no.26910  loss = 5.23468 avg_loss = 4.22898\n",
      "epoch no.0 train no.26920  loss = 2.93217 avg_loss = 4.25858\n",
      "epoch no.0 train no.26930  loss = 3.16665 avg_loss = 4.18486\n",
      "epoch no.0 train no.26940  loss = 3.75567 avg_loss = 4.16582\n",
      "epoch no.0 train no.26950  loss = 3.97566 avg_loss = 4.15580\n",
      "epoch no.0 train no.26960  loss = 3.93732 avg_loss = 4.24217\n",
      "epoch no.0 train no.26970  loss = 4.82034 avg_loss = 4.22205\n",
      "epoch no.0 train no.26980  loss = 2.67122 avg_loss = 4.23471\n",
      "epoch no.0 train no.26990  loss = 2.01791 avg_loss = 4.25497\n",
      "epoch no.0 train no.27000  loss = 3.62826 avg_loss = 4.23034\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요한', '▁당신을', '에게', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요한 당신에게 추천하는 노래</s>\n",
      "epoch no.0 train no.27010  loss = 2.91954 avg_loss = 4.18087\n",
      "epoch no.0 train no.27020  loss = 5.35492 avg_loss = 4.23561\n",
      "epoch no.0 train no.27030  loss = 4.35049 avg_loss = 4.21309\n",
      "epoch no.0 train no.27040  loss = 3.42350 avg_loss = 4.16404\n",
      "epoch no.0 train no.27050  loss = 5.64024 avg_loss = 4.14333\n",
      "epoch no.0 train no.27060  loss = 6.38518 avg_loss = 4.12219\n",
      "epoch no.0 train no.27070  loss = 3.12608 avg_loss = 4.09882\n",
      "epoch no.0 train no.27080  loss = 5.41244 avg_loss = 4.10566\n",
      "epoch no.0 train no.27090  loss = 6.16768 avg_loss = 4.13604\n",
      "epoch no.0 train no.27100  loss = 5.07087 avg_loss = 4.15247\n",
      "epoch no.0 train no.27110  loss = 2.92349 avg_loss = 4.16645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.27120  loss = 4.01171 avg_loss = 4.12666\n",
      "epoch no.0 train no.27130  loss = 2.46704 avg_loss = 4.13392\n",
      "epoch no.0 train no.27140  loss = 3.46007 avg_loss = 4.09487\n",
      "epoch no.0 train no.27150  loss = 4.10172 avg_loss = 4.14525\n",
      "epoch no.0 train no.27160  loss = 2.66772 avg_loss = 4.12950\n",
      "epoch no.0 train no.27170  loss = 4.11080 avg_loss = 4.08649\n",
      "epoch no.0 train no.27180  loss = 3.58074 avg_loss = 4.08538\n",
      "epoch no.0 train no.27190  loss = 4.55937 avg_loss = 4.08983\n",
      "epoch no.0 train no.27200  loss = 3.19919 avg_loss = 4.11963\n",
      "epoch no.0 train no.27210  loss = 2.68256 avg_loss = 4.07542\n",
      "epoch no.0 train no.27220  loss = 3.51346 avg_loss = 4.07426\n",
      "epoch no.0 train no.27230  loss = 3.43359 avg_loss = 4.11669\n",
      "epoch no.0 train no.27240  loss = 6.09120 avg_loss = 4.17474\n",
      "epoch no.0 train no.27250  loss = 4.35598 avg_loss = 4.09188\n",
      "epoch no.0 train no.27260  loss = 4.33893 avg_loss = 4.00742\n",
      "epoch no.0 train no.27270  loss = 5.84086 avg_loss = 4.01218\n",
      "epoch no.0 train no.27280  loss = 3.32205 avg_loss = 3.99700\n",
      "epoch no.0 train no.27290  loss = 4.64174 avg_loss = 4.09266\n",
      "epoch no.0 train no.27300  loss = 3.25814 avg_loss = 4.05990\n",
      "epoch no.0 train no.27310  loss = 3.87091 avg_loss = 4.08602\n",
      "epoch no.0 train no.27320  loss = 4.12706 avg_loss = 4.04361\n",
      "epoch no.0 train no.27330  loss = 4.84485 avg_loss = 4.02474\n",
      "epoch no.0 train no.27340  loss = 5.76496 avg_loss = 4.05639\n",
      "epoch no.0 train no.27350  loss = 5.47666 avg_loss = 4.05850\n",
      "epoch no.0 train no.27360  loss = 2.97471 avg_loss = 4.11167\n",
      "epoch no.0 train no.27370  loss = 1.91671 avg_loss = 4.03386\n",
      "epoch no.0 train no.27380  loss = 4.53607 avg_loss = 3.96873\n",
      "epoch no.0 train no.27390  loss = 4.77210 avg_loss = 3.99956\n",
      "epoch no.0 train no.27400  loss = 2.82493 avg_loss = 3.98204\n",
      "epoch no.0 train no.27410  loss = 2.89398 avg_loss = 3.93525\n",
      "epoch no.0 train no.27420  loss = 5.74934 avg_loss = 3.89715\n",
      "epoch no.0 train no.27430  loss = 4.34958 avg_loss = 3.92789\n",
      "epoch no.0 train no.27440  loss = 5.03364 avg_loss = 3.99436\n",
      "epoch no.0 train no.27450  loss = 3.73845 avg_loss = 4.00053\n",
      "epoch no.0 train no.27460  loss = 5.55820 avg_loss = 4.04982\n",
      "epoch no.0 train no.27470  loss = 4.86620 avg_loss = 4.10481\n",
      "epoch no.0 train no.27480  loss = 3.88840 avg_loss = 4.09862\n",
      "epoch no.0 train no.27490  loss = 2.37896 avg_loss = 4.10978\n",
      "epoch no.0 train no.27500  loss = 4.60534 avg_loss = 4.12422\n",
      "epoch no.0 train no.27510  loss = 3.81119 avg_loss = 4.09383\n",
      "epoch no.0 train no.27520  loss = 2.06674 avg_loss = 4.10261\n",
      "epoch no.0 train no.27530  loss = 4.38731 avg_loss = 4.04057\n",
      "epoch no.0 train no.27540  loss = 1.76805 avg_loss = 4.06223\n",
      "epoch no.0 train no.27550  loss = 4.50864 avg_loss = 4.10245\n",
      "epoch no.0 train no.27560  loss = 6.02947 avg_loss = 4.16199\n",
      "epoch no.0 train no.27570  loss = 4.88914 avg_loss = 4.12277\n",
      "epoch no.0 train no.27580  loss = 5.06582 avg_loss = 4.09319\n",
      "epoch no.0 train no.27590  loss = 5.05665 avg_loss = 4.05226\n",
      "epoch no.0 train no.27600  loss = 3.14281 avg_loss = 4.03115\n",
      "epoch no.0 train no.27610  loss = 5.66959 avg_loss = 4.02919\n",
      "epoch no.0 train no.27620  loss = 5.43680 avg_loss = 4.05825\n",
      "epoch no.0 train no.27630  loss = 5.30692 avg_loss = 4.05869\n",
      "epoch no.0 train no.27640  loss = 4.61435 avg_loss = 4.07015\n",
      "epoch no.0 train no.27650  loss = 4.14588 avg_loss = 4.10244\n",
      "epoch no.0 train no.27660  loss = 3.82792 avg_loss = 4.09030\n",
      "epoch no.0 train no.27670  loss = 5.46433 avg_loss = 4.07237\n",
      "epoch no.0 train no.27680  loss = 3.22821 avg_loss = 4.03979\n",
      "epoch no.0 train no.27690  loss = 5.10835 avg_loss = 4.03532\n",
      "epoch no.0 train no.27700  loss = 3.13316 avg_loss = 3.97392\n",
      "epoch no.0 train no.27710  loss = 2.24017 avg_loss = 3.92918\n",
      "epoch no.0 train no.27720  loss = 5.83021 avg_loss = 3.95358\n",
      "epoch no.0 train no.27730  loss = 3.05366 avg_loss = 3.99472\n",
      "epoch no.0 train no.27740  loss = 3.12367 avg_loss = 4.02545\n",
      "epoch no.0 train no.27750  loss = 5.21391 avg_loss = 4.10567\n",
      "epoch no.0 train no.27760  loss = 3.68759 avg_loss = 4.04760\n",
      "epoch no.0 train no.27770  loss = 4.34284 avg_loss = 4.09327\n",
      "epoch no.0 train no.27780  loss = 3.93522 avg_loss = 4.13207\n",
      "epoch no.0 train no.27790  loss = 2.89877 avg_loss = 4.14173\n",
      "epoch no.0 train no.27800  loss = 3.51410 avg_loss = 4.08698\n",
      "epoch no.0 train no.27810  loss = 3.35229 avg_loss = 4.04206\n",
      "epoch no.0 train no.27820  loss = 2.79317 avg_loss = 3.99947\n",
      "epoch no.0 train no.27830  loss = 6.15065 avg_loss = 4.05174\n",
      "epoch no.0 train no.27840  loss = 2.54162 avg_loss = 4.03453\n",
      "epoch no.0 train no.27850  loss = 3.16490 avg_loss = 4.01757\n",
      "epoch no.0 train no.27860  loss = 2.78372 avg_loss = 3.98625\n",
      "epoch no.0 train no.27870  loss = 2.42692 avg_loss = 3.99663\n",
      "epoch no.0 train no.27880  loss = 4.54729 avg_loss = 3.99126\n",
      "epoch no.0 train no.27890  loss = 8.63809 avg_loss = 4.05276\n",
      "epoch no.0 train no.27900  loss = 3.23782 avg_loss = 4.05860\n",
      "epoch no.0 train no.27910  loss = 5.32988 avg_loss = 4.05153\n",
      "epoch no.0 train no.27920  loss = 5.25574 avg_loss = 4.04387\n",
      "epoch no.0 train no.27930  loss = 2.07196 avg_loss = 4.00144\n",
      "epoch no.0 train no.27940  loss = 5.58619 avg_loss = 4.00559\n",
      "epoch no.0 train no.27950  loss = 3.58505 avg_loss = 3.96919\n",
      "epoch no.0 train no.27960  loss = 3.51054 avg_loss = 3.94413\n",
      "epoch no.0 train no.27970  loss = 3.97576 avg_loss = 3.94813\n",
      "epoch no.0 train no.27980  loss = 3.57025 avg_loss = 4.03231\n",
      "epoch no.0 train no.27990  loss = 2.88015 avg_loss = 3.98239\n",
      "epoch no.0 train no.28000  loss = 6.31846 avg_loss = 4.08205\n",
      "2\n",
      "to_tokens: ['▁라디오', '좋', '을', '▁노래', '</s>']\n",
      "기분전환 신나는 노래</s>\n",
      "epoch no.0 train no.28010  loss = 5.97718 avg_loss = 4.10992\n",
      "epoch no.0 train no.28020  loss = 4.54447 avg_loss = 4.12939\n",
      "epoch no.0 train no.28030  loss = 4.14022 avg_loss = 4.13151\n",
      "epoch no.0 train no.28040  loss = 1.99911 avg_loss = 4.10214\n",
      "epoch no.0 train no.28050  loss = 3.70322 avg_loss = 4.07062\n",
      "epoch no.0 train no.28060  loss = 3.32079 avg_loss = 4.06432\n",
      "epoch no.0 train no.28070  loss = 4.27521 avg_loss = 4.04492\n",
      "epoch no.0 train no.28080  loss = 4.13113 avg_loss = 4.07002\n",
      "epoch no.0 train no.28090  loss = 5.16940 avg_loss = 4.07542\n",
      "epoch no.0 train no.28100  loss = 3.95992 avg_loss = 4.12440\n",
      "epoch no.0 train no.28110  loss = 4.34656 avg_loss = 4.13666\n",
      "epoch no.0 train no.28120  loss = 3.84846 avg_loss = 4.20262\n",
      "epoch no.0 train no.28130  loss = 2.43929 avg_loss = 4.26892\n",
      "epoch no.0 train no.28140  loss = 2.41030 avg_loss = 4.21437\n",
      "epoch no.0 train no.28150  loss = 2.80527 avg_loss = 4.21829\n",
      "epoch no.0 train no.28160  loss = 5.48107 avg_loss = 4.16704\n",
      "epoch no.0 train no.28170  loss = 4.37746 avg_loss = 4.14205\n",
      "epoch no.0 train no.28180  loss = 5.15090 avg_loss = 4.12568\n",
      "epoch no.0 train no.28190  loss = 3.17446 avg_loss = 4.10821\n",
      "epoch no.0 train no.28200  loss = 5.97544 avg_loss = 4.16287\n",
      "epoch no.0 train no.28210  loss = 3.73045 avg_loss = 4.12284\n",
      "epoch no.0 train no.28220  loss = 3.69290 avg_loss = 4.13073\n",
      "epoch no.0 train no.28230  loss = 5.02012 avg_loss = 4.12168\n",
      "epoch no.0 train no.28240  loss = 4.23358 avg_loss = 4.11709\n",
      "epoch no.0 train no.28250  loss = 7.00097 avg_loss = 4.11642\n",
      "epoch no.0 train no.28260  loss = 3.33594 avg_loss = 4.13950\n",
      "epoch no.0 train no.28270  loss = 2.99586 avg_loss = 4.08807\n",
      "epoch no.0 train no.28280  loss = 3.00597 avg_loss = 4.08558\n",
      "epoch no.0 train no.28290  loss = 4.20598 avg_loss = 4.06803\n",
      "epoch no.0 train no.28300  loss = 2.06452 avg_loss = 4.01926\n",
      "epoch no.0 train no.28310  loss = 6.25857 avg_loss = 4.01492\n",
      "epoch no.0 train no.28320  loss = 3.20905 avg_loss = 4.01255\n",
      "epoch no.0 train no.28330  loss = 3.01922 avg_loss = 3.95389\n",
      "epoch no.0 train no.28340  loss = 3.99066 avg_loss = 3.94981\n",
      "epoch no.0 train no.28350  loss = 2.59629 avg_loss = 3.97581\n",
      "epoch no.0 train no.28360  loss = 2.34428 avg_loss = 3.99865\n",
      "epoch no.0 train no.28370  loss = 5.07789 avg_loss = 4.00629\n",
      "epoch no.0 train no.28380  loss = 3.41629 avg_loss = 3.99468\n",
      "epoch no.0 train no.28390  loss = 7.60600 avg_loss = 4.02798\n",
      "epoch no.0 train no.28400  loss = 3.31624 avg_loss = 4.04947\n",
      "epoch no.0 train no.28410  loss = 3.74620 avg_loss = 3.99521\n",
      "epoch no.0 train no.28420  loss = 3.08162 avg_loss = 3.99537\n",
      "epoch no.0 train no.28430  loss = 3.84562 avg_loss = 4.07891\n",
      "epoch no.0 train no.28440  loss = 3.64024 avg_loss = 4.09718\n",
      "epoch no.0 train no.28450  loss = 5.11659 avg_loss = 4.10204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.28460  loss = 3.34476 avg_loss = 4.11013\n",
      "epoch no.0 train no.28470  loss = 1.95201 avg_loss = 4.09419\n",
      "epoch no.0 train no.28480  loss = 3.61535 avg_loss = 4.03905\n",
      "epoch no.0 train no.28490  loss = 3.27382 avg_loss = 4.03420\n",
      "epoch no.0 train no.28500  loss = 5.45701 avg_loss = 4.03887\n",
      "epoch no.0 train no.28510  loss = 5.14007 avg_loss = 4.05209\n",
      "epoch no.0 train no.28520  loss = 2.79732 avg_loss = 4.10298\n",
      "epoch no.0 train no.28530  loss = 5.55962 avg_loss = 4.18787\n",
      "epoch no.0 train no.28540  loss = 5.13342 avg_loss = 4.18362\n",
      "epoch no.0 train no.28550  loss = 4.63917 avg_loss = 4.18828\n",
      "epoch no.0 train no.28560  loss = 5.31560 avg_loss = 4.18525\n",
      "epoch no.0 train no.28570  loss = 4.54446 avg_loss = 4.19150\n",
      "epoch no.0 train no.28580  loss = 3.73646 avg_loss = 4.23553\n",
      "epoch no.0 train no.28590  loss = 4.43502 avg_loss = 4.22589\n",
      "epoch no.0 train no.28600  loss = 2.52281 avg_loss = 4.25610\n",
      "epoch no.0 train no.28610  loss = 3.57605 avg_loss = 4.23695\n",
      "epoch no.0 train no.28620  loss = 3.67876 avg_loss = 4.17602\n",
      "epoch no.0 train no.28630  loss = 3.33238 avg_loss = 4.17282\n",
      "epoch no.0 train no.28640  loss = 3.37799 avg_loss = 4.13089\n",
      "epoch no.0 train no.28650  loss = 4.79416 avg_loss = 4.14154\n",
      "epoch no.0 train no.28660  loss = 4.82638 avg_loss = 4.16616\n",
      "epoch no.0 train no.28670  loss = 3.48019 avg_loss = 4.12215\n",
      "epoch no.0 train no.28680  loss = 3.10467 avg_loss = 4.17027\n",
      "epoch no.0 train no.28690  loss = 6.16356 avg_loss = 4.19399\n",
      "epoch no.0 train no.28700  loss = 2.71427 avg_loss = 4.17143\n",
      "epoch no.0 train no.28710  loss = 4.51463 avg_loss = 4.18446\n",
      "epoch no.0 train no.28720  loss = 3.48292 avg_loss = 4.15174\n",
      "epoch no.0 train no.28730  loss = 4.20490 avg_loss = 4.12461\n",
      "epoch no.0 train no.28740  loss = 2.31798 avg_loss = 4.16322\n",
      "epoch no.0 train no.28750  loss = 2.85272 avg_loss = 4.09859\n",
      "epoch no.0 train no.28760  loss = 3.84826 avg_loss = 4.10225\n",
      "epoch no.0 train no.28770  loss = 4.04992 avg_loss = 4.09234\n",
      "epoch no.0 train no.28780  loss = 5.09634 avg_loss = 4.10792\n",
      "epoch no.0 train no.28790  loss = 5.99226 avg_loss = 4.17147\n",
      "epoch no.0 train no.28800  loss = 3.57600 avg_loss = 4.10993\n",
      "epoch no.0 train no.28810  loss = 5.20404 avg_loss = 4.13656\n",
      "epoch no.0 train no.28820  loss = 3.54111 avg_loss = 4.16373\n",
      "epoch no.0 train no.28830  loss = 4.62380 avg_loss = 4.16550\n",
      "epoch no.0 train no.28840  loss = 2.85459 avg_loss = 4.19939\n",
      "epoch no.0 train no.28850  loss = 3.86093 avg_loss = 4.23922\n",
      "epoch no.0 train no.28860  loss = 3.74969 avg_loss = 4.20140\n",
      "epoch no.0 train no.28870  loss = 6.37852 avg_loss = 4.22724\n",
      "epoch no.0 train no.28880  loss = 4.07796 avg_loss = 4.20426\n",
      "epoch no.0 train no.28890  loss = 3.47448 avg_loss = 4.22533\n",
      "epoch no.0 train no.28900  loss = 4.10379 avg_loss = 4.23802\n",
      "epoch no.0 train no.28910  loss = 3.00129 avg_loss = 4.20188\n",
      "epoch no.0 train no.28920  loss = 2.72394 avg_loss = 4.21805\n",
      "epoch no.0 train no.28930  loss = 3.85304 avg_loss = 4.19060\n",
      "epoch no.0 train no.28940  loss = 3.51689 avg_loss = 4.12455\n",
      "epoch no.0 train no.28950  loss = 5.13546 avg_loss = 4.10607\n",
      "epoch no.0 train no.28960  loss = 5.17208 avg_loss = 4.13254\n",
      "epoch no.0 train no.28970  loss = 3.44001 avg_loss = 4.13483\n",
      "epoch no.0 train no.28980  loss = 4.05080 avg_loss = 4.07743\n",
      "epoch no.0 train no.28990  loss = 2.35147 avg_loss = 4.09139\n",
      "epoch no.0 train no.29000  loss = 3.20475 avg_loss = 4.11210\n",
      "1\n",
      "to_tokens: ['▁비', '전환', '용', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.0 train no.29010  loss = 5.13035 avg_loss = 4.08977\n",
      "epoch no.0 train no.29020  loss = 3.95615 avg_loss = 4.08215\n",
      "epoch no.0 train no.29030  loss = 3.89547 avg_loss = 4.09281\n",
      "epoch no.0 train no.29040  loss = 4.00389 avg_loss = 4.04133\n",
      "epoch no.0 train no.29050  loss = 3.14699 avg_loss = 4.09770\n",
      "epoch no.0 train no.29060  loss = 7.87606 avg_loss = 4.15795\n",
      "epoch no.0 train no.29070  loss = 5.46339 avg_loss = 4.13174\n",
      "epoch no.0 train no.29080  loss = 2.69084 avg_loss = 4.12982\n",
      "epoch no.0 train no.29090  loss = 5.49411 avg_loss = 4.18725\n",
      "epoch no.0 train no.29100  loss = 3.69372 avg_loss = 4.14547\n",
      "epoch no.0 train no.29110  loss = 3.77748 avg_loss = 4.09963\n",
      "epoch no.0 train no.29120  loss = 5.06046 avg_loss = 4.11996\n",
      "epoch no.0 train no.29130  loss = 3.00751 avg_loss = 4.06718\n",
      "epoch no.0 train no.29140  loss = 6.55630 avg_loss = 4.13719\n",
      "epoch no.0 train no.29150  loss = 2.55161 avg_loss = 4.06440\n",
      "epoch no.0 train no.29160  loss = 4.96844 avg_loss = 4.09546\n",
      "epoch no.0 train no.29170  loss = 4.97007 avg_loss = 4.06393\n",
      "epoch no.0 train no.29180  loss = 4.37085 avg_loss = 4.17367\n",
      "epoch no.0 train no.29190  loss = 3.65049 avg_loss = 4.10127\n",
      "epoch no.0 train no.29200  loss = 2.07265 avg_loss = 4.02182\n",
      "epoch no.0 train no.29210  loss = 4.07669 avg_loss = 4.04088\n",
      "epoch no.0 train no.29220  loss = 5.86365 avg_loss = 4.12732\n",
      "epoch no.0 train no.29230  loss = 3.37681 avg_loss = 4.08062\n",
      "epoch no.0 train no.29240  loss = 4.51901 avg_loss = 4.09840\n",
      "epoch no.0 train no.29250  loss = 3.98947 avg_loss = 4.10145\n",
      "epoch no.0 train no.29260  loss = 3.26545 avg_loss = 4.13324\n",
      "epoch no.0 train no.29270  loss = 2.97472 avg_loss = 4.16222\n",
      "epoch no.0 train no.29280  loss = 3.57929 avg_loss = 4.13912\n",
      "epoch no.0 train no.29290  loss = 2.87191 avg_loss = 4.20449\n",
      "epoch no.0 train no.29300  loss = 3.83035 avg_loss = 4.19623\n",
      "epoch no.0 train no.29310  loss = 4.13794 avg_loss = 4.12692\n",
      "epoch no.0 train no.29320  loss = 4.92697 avg_loss = 4.12752\n",
      "epoch no.0 train no.29330  loss = 3.33274 avg_loss = 4.15073\n",
      "epoch no.0 train no.29340  loss = 5.07963 avg_loss = 4.16152\n",
      "epoch no.0 train no.29350  loss = 2.74278 avg_loss = 4.12131\n",
      "epoch no.0 train no.29360  loss = 2.97648 avg_loss = 4.12236\n",
      "epoch no.0 train no.29370  loss = 3.26834 avg_loss = 4.09788\n",
      "epoch no.0 train no.29380  loss = 4.95289 avg_loss = 4.13732\n",
      "epoch no.0 train no.29390  loss = 5.10096 avg_loss = 4.09993\n",
      "epoch no.0 train no.29400  loss = 4.21209 avg_loss = 4.09231\n",
      "epoch no.0 train no.29410  loss = 2.68635 avg_loss = 4.02776\n",
      "epoch no.0 train no.29420  loss = 3.90126 avg_loss = 4.01498\n",
      "epoch no.0 train no.29430  loss = 6.79498 avg_loss = 4.06451\n",
      "epoch no.0 train no.29440  loss = 2.87288 avg_loss = 4.02967\n",
      "epoch no.0 train no.29450  loss = 2.37427 avg_loss = 4.03254\n",
      "epoch no.0 train no.29460  loss = 6.91818 avg_loss = 4.07285\n",
      "epoch no.0 train no.29470  loss = 4.57161 avg_loss = 4.08575\n",
      "epoch no.0 train no.29480  loss = 5.16958 avg_loss = 4.10366\n",
      "epoch no.0 train no.29490  loss = 7.80633 avg_loss = 4.13180\n",
      "epoch no.0 train no.29500  loss = 3.18304 avg_loss = 4.20283\n",
      "epoch no.0 train no.29510  loss = 2.37922 avg_loss = 4.18626\n",
      "epoch no.0 train no.29520  loss = 3.05163 avg_loss = 4.17704\n",
      "epoch no.0 train no.29530  loss = 5.16107 avg_loss = 4.09755\n",
      "epoch no.0 train no.29540  loss = 8.17098 avg_loss = 4.16679\n",
      "epoch no.0 train no.29550  loss = 4.75928 avg_loss = 4.14624\n",
      "epoch no.0 train no.29560  loss = 6.00880 avg_loss = 4.20773\n",
      "epoch no.0 train no.29570  loss = 6.57320 avg_loss = 4.20612\n",
      "epoch no.0 train no.29580  loss = 2.44148 avg_loss = 4.19603\n",
      "epoch no.0 train no.29590  loss = 5.27571 avg_loss = 4.16537\n",
      "epoch no.0 train no.29600  loss = 4.86126 avg_loss = 4.13605\n",
      "epoch no.0 train no.29610  loss = 4.57244 avg_loss = 4.14299\n",
      "epoch no.0 train no.29620  loss = 3.50531 avg_loss = 4.11198\n",
      "epoch no.0 train no.29630  loss = 2.73610 avg_loss = 4.06203\n",
      "epoch no.0 train no.29640  loss = 3.80241 avg_loss = 4.02066\n",
      "epoch no.0 train no.29650  loss = 3.53419 avg_loss = 4.00697\n",
      "epoch no.0 train no.29660  loss = 5.41499 avg_loss = 3.99074\n",
      "epoch no.0 train no.29670  loss = 2.82481 avg_loss = 3.93820\n",
      "epoch no.0 train no.29680  loss = 5.53223 avg_loss = 3.97342\n",
      "epoch no.0 train no.29690  loss = 5.40703 avg_loss = 4.00235\n",
      "epoch no.0 train no.29700  loss = 2.85365 avg_loss = 3.99900\n",
      "epoch no.0 train no.29710  loss = 2.97790 avg_loss = 3.96482\n",
      "epoch no.0 train no.29720  loss = 3.81227 avg_loss = 3.93023\n",
      "epoch no.0 train no.29730  loss = 2.51118 avg_loss = 3.92831\n",
      "epoch no.0 train no.29740  loss = 3.49039 avg_loss = 3.90294\n",
      "epoch no.0 train no.29750  loss = 4.59656 avg_loss = 3.91953\n",
      "epoch no.0 train no.29760  loss = 2.69957 avg_loss = 3.95899\n",
      "epoch no.0 train no.29770  loss = 5.86223 avg_loss = 3.94036\n",
      "epoch no.0 train no.29780  loss = 4.27218 avg_loss = 3.92404\n",
      "epoch no.0 train no.29790  loss = 2.59667 avg_loss = 3.92428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.29800  loss = 4.19857 avg_loss = 3.91149\n",
      "epoch no.0 train no.29810  loss = 3.53512 avg_loss = 3.85401\n",
      "epoch no.0 train no.29820  loss = 2.56740 avg_loss = 3.76482\n",
      "epoch no.0 train no.29830  loss = 3.41971 avg_loss = 3.76660\n",
      "epoch no.0 train no.29840  loss = 6.54705 avg_loss = 3.84045\n",
      "epoch no.0 train no.29850  loss = 3.91828 avg_loss = 3.91797\n",
      "epoch no.0 train no.29860  loss = 4.82421 avg_loss = 3.96127\n",
      "epoch no.0 train no.29870  loss = 2.79200 avg_loss = 3.97218\n",
      "epoch no.0 train no.29880  loss = 3.75215 avg_loss = 3.89128\n",
      "epoch no.0 train no.29890  loss = 3.13360 avg_loss = 3.84475\n",
      "epoch no.0 train no.29900  loss = 2.45963 avg_loss = 3.85906\n",
      "epoch no.0 train no.29910  loss = 2.67438 avg_loss = 3.88745\n",
      "epoch no.0 train no.29920  loss = 7.44958 avg_loss = 3.94082\n",
      "epoch no.0 train no.29930  loss = 3.15243 avg_loss = 3.92617\n",
      "epoch no.0 train no.29940  loss = 3.18643 avg_loss = 3.99782\n",
      "epoch no.0 train no.29950  loss = 8.13807 avg_loss = 4.11417\n",
      "epoch no.0 train no.29960  loss = 2.72206 avg_loss = 4.09690\n",
      "epoch no.0 train no.29970  loss = 2.14310 avg_loss = 4.06176\n",
      "epoch no.0 train no.29980  loss = 4.98805 avg_loss = 4.04852\n",
      "epoch no.0 train no.29990  loss = 3.59357 avg_loss = 4.03064\n",
      "epoch no.0 train no.30000  loss = 4.34923 avg_loss = 4.01277\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '용', '</s>', '리스트', '</s>']\n",
      "기분전환용 플레이리스트</s>\n",
      "epoch no.0 train no.30010  loss = 3.81429 avg_loss = 3.99865\n",
      "epoch no.0 train no.30020  loss = 4.76268 avg_loss = 3.99057\n",
      "epoch no.0 train no.30030  loss = 6.22886 avg_loss = 3.99442\n",
      "epoch no.0 train no.30040  loss = 3.33045 avg_loss = 3.99269\n",
      "epoch no.0 train no.30050  loss = 2.09641 avg_loss = 4.02041\n",
      "epoch no.0 train no.30060  loss = 6.47065 avg_loss = 4.01317\n",
      "epoch no.0 train no.30070  loss = 4.18397 avg_loss = 3.99954\n",
      "epoch no.0 train no.30080  loss = 4.39600 avg_loss = 3.97683\n",
      "epoch no.0 train no.30090  loss = 3.04039 avg_loss = 3.96433\n",
      "epoch no.0 train no.30100  loss = 5.12020 avg_loss = 3.93388\n",
      "epoch no.0 train no.30110  loss = 3.21289 avg_loss = 3.92559\n",
      "epoch no.0 train no.30120  loss = 5.23506 avg_loss = 3.88732\n",
      "epoch no.0 train no.30130  loss = 3.27297 avg_loss = 3.93527\n",
      "epoch no.0 train no.30140  loss = 4.51662 avg_loss = 3.95452\n",
      "epoch no.0 train no.30150  loss = 1.93821 avg_loss = 3.88402\n",
      "epoch no.0 train no.30160  loss = 4.00577 avg_loss = 3.89498\n",
      "epoch no.0 train no.30170  loss = 3.79817 avg_loss = 3.87017\n",
      "epoch no.0 train no.30180  loss = 2.31652 avg_loss = 3.87408\n",
      "epoch no.0 train no.30190  loss = 4.92479 avg_loss = 3.89616\n",
      "epoch no.0 train no.30200  loss = 3.33692 avg_loss = 3.95758\n",
      "epoch no.0 train no.30210  loss = 4.59440 avg_loss = 3.99589\n",
      "epoch no.0 train no.30220  loss = 4.91834 avg_loss = 3.99422\n",
      "epoch no.0 train no.30230  loss = 6.50481 avg_loss = 4.02620\n",
      "epoch no.0 train no.30240  loss = 2.58997 avg_loss = 4.07595\n",
      "epoch no.0 train no.30250  loss = 3.65852 avg_loss = 4.19566\n",
      "epoch no.0 train no.30260  loss = 4.24554 avg_loss = 4.17029\n",
      "epoch no.0 train no.30270  loss = 6.49799 avg_loss = 4.16838\n",
      "epoch no.0 train no.30280  loss = 3.87975 avg_loss = 4.10617\n",
      "epoch no.0 train no.30290  loss = 5.06456 avg_loss = 4.15627\n",
      "epoch no.0 train no.30300  loss = 5.27159 avg_loss = 4.17986\n",
      "epoch no.0 train no.30310  loss = 3.73733 avg_loss = 4.14369\n",
      "epoch no.0 train no.30320  loss = 6.45811 avg_loss = 4.12428\n",
      "epoch no.0 train no.30330  loss = 2.68863 avg_loss = 4.09553\n",
      "epoch no.0 train no.30340  loss = 3.44911 avg_loss = 4.07571\n",
      "epoch no.0 train no.30350  loss = 3.14392 avg_loss = 4.05892\n",
      "epoch no.0 train no.30360  loss = 4.04421 avg_loss = 4.06201\n",
      "epoch no.0 train no.30370  loss = 4.47054 avg_loss = 4.06436\n",
      "epoch no.0 train no.30380  loss = 4.51586 avg_loss = 4.09188\n",
      "epoch no.0 train no.30390  loss = 4.54479 avg_loss = 4.13451\n",
      "epoch no.0 train no.30400  loss = 5.41929 avg_loss = 4.09612\n",
      "epoch no.0 train no.30410  loss = 4.13620 avg_loss = 4.14188\n",
      "epoch no.0 train no.30420  loss = 3.24708 avg_loss = 4.08016\n",
      "epoch no.0 train no.30430  loss = 5.75797 avg_loss = 4.10468\n",
      "epoch no.0 train no.30440  loss = 3.10121 avg_loss = 4.04610\n",
      "epoch no.0 train no.30450  loss = 3.58623 avg_loss = 4.02956\n",
      "epoch no.0 train no.30460  loss = 5.38617 avg_loss = 4.04337\n",
      "epoch no.0 train no.30470  loss = 5.52249 avg_loss = 4.02198\n",
      "epoch no.0 train no.30480  loss = 4.64553 avg_loss = 4.09868\n",
      "epoch no.0 train no.30490  loss = 3.31523 avg_loss = 4.12903\n",
      "epoch no.0 train no.30500  loss = 4.97358 avg_loss = 4.09266\n",
      "epoch no.0 train no.30510  loss = 5.02336 avg_loss = 4.10618\n",
      "epoch no.0 train no.30520  loss = 5.14626 avg_loss = 4.11738\n",
      "epoch no.0 train no.30530  loss = 4.93404 avg_loss = 4.12504\n",
      "epoch no.0 train no.30540  loss = 2.75316 avg_loss = 4.12034\n",
      "epoch no.0 train no.30550  loss = 3.35852 avg_loss = 4.09388\n",
      "epoch no.0 train no.30560  loss = 5.55397 avg_loss = 4.14144\n",
      "epoch no.0 train no.30570  loss = 2.88025 avg_loss = 4.07287\n",
      "epoch no.0 train no.30580  loss = 3.68767 avg_loss = 4.04246\n",
      "epoch no.0 train no.30590  loss = 4.37281 avg_loss = 4.00952\n",
      "epoch no.0 train no.30600  loss = 6.50216 avg_loss = 4.00674\n",
      "epoch no.0 train no.30610  loss = 2.79166 avg_loss = 4.00666\n",
      "epoch no.0 train no.30620  loss = 6.52645 avg_loss = 4.01904\n",
      "epoch no.0 train no.30630  loss = 4.71576 avg_loss = 4.05116\n",
      "epoch no.0 train no.30640  loss = 4.36739 avg_loss = 4.03583\n",
      "epoch no.0 train no.30650  loss = 3.22215 avg_loss = 4.01619\n",
      "epoch no.0 train no.30660  loss = 5.24667 avg_loss = 3.97677\n",
      "epoch no.0 train no.30670  loss = 2.87754 avg_loss = 3.96518\n",
      "epoch no.0 train no.30680  loss = 1.92943 avg_loss = 3.94574\n",
      "epoch no.0 train no.30690  loss = 3.33034 avg_loss = 3.92671\n",
      "epoch no.0 train no.30700  loss = 5.87633 avg_loss = 4.00912\n",
      "epoch no.0 train no.30710  loss = 2.53094 avg_loss = 3.99095\n",
      "epoch no.0 train no.30720  loss = 6.05147 avg_loss = 3.97735\n",
      "epoch no.0 train no.30730  loss = 4.82209 avg_loss = 3.97841\n",
      "epoch no.0 train no.30740  loss = 2.97272 avg_loss = 3.96010\n",
      "epoch no.0 train no.30750  loss = 3.03003 avg_loss = 3.94906\n",
      "epoch no.0 train no.30760  loss = 4.51013 avg_loss = 3.95121\n",
      "epoch no.0 train no.30770  loss = 3.34222 avg_loss = 4.02605\n",
      "epoch no.0 train no.30780  loss = 3.61172 avg_loss = 4.03634\n",
      "epoch no.0 train no.30790  loss = 5.80403 avg_loss = 4.06991\n",
      "epoch no.0 train no.30800  loss = 2.91619 avg_loss = 4.02013\n",
      "epoch no.0 train no.30810  loss = 3.09113 avg_loss = 3.95136\n",
      "epoch no.0 train no.30820  loss = 3.32008 avg_loss = 3.98330\n",
      "epoch no.0 train no.30830  loss = 3.18778 avg_loss = 3.92863\n",
      "epoch no.0 train no.30840  loss = 4.07298 avg_loss = 3.86556\n",
      "epoch no.0 train no.30850  loss = 2.86396 avg_loss = 3.93584\n",
      "epoch no.0 train no.30860  loss = 3.81843 avg_loss = 3.94718\n",
      "epoch no.0 train no.30870  loss = 4.30093 avg_loss = 3.96642\n",
      "epoch no.0 train no.30880  loss = 3.20009 avg_loss = 3.95383\n",
      "epoch no.0 train no.30890  loss = 3.00429 avg_loss = 4.02695\n",
      "epoch no.0 train no.30900  loss = 4.41894 avg_loss = 4.09325\n",
      "epoch no.0 train no.30910  loss = 2.63510 avg_loss = 4.01650\n",
      "epoch no.0 train no.30920  loss = 3.76226 avg_loss = 4.05080\n",
      "epoch no.0 train no.30930  loss = 5.26900 avg_loss = 4.09152\n",
      "epoch no.0 train no.30940  loss = 4.98689 avg_loss = 4.07643\n",
      "epoch no.0 train no.30950  loss = 4.59859 avg_loss = 4.10815\n",
      "epoch no.0 train no.30960  loss = 3.18778 avg_loss = 4.10750\n",
      "epoch no.0 train no.30970  loss = 2.55450 avg_loss = 4.09928\n",
      "epoch no.0 train no.30980  loss = 4.36056 avg_loss = 4.12218\n",
      "epoch no.0 train no.30990  loss = 3.96046 avg_loss = 4.09415\n",
      "epoch no.0 train no.31000  loss = 2.32306 avg_loss = 4.05442\n",
      "0\n",
      "to_tokens: ['▁라디오', '전환', '을']\n",
      "기분전환</s>\n",
      "epoch no.0 train no.31010  loss = 3.04950 avg_loss = 4.04374\n",
      "epoch no.0 train no.31020  loss = 3.01731 avg_loss = 3.99920\n",
      "epoch no.0 train no.31030  loss = 4.62927 avg_loss = 4.04840\n",
      "epoch no.0 train no.31040  loss = 2.70773 avg_loss = 4.04896\n",
      "epoch no.0 train no.31050  loss = 5.33902 avg_loss = 4.08196\n",
      "epoch no.0 train no.31060  loss = 4.15740 avg_loss = 4.12340\n",
      "epoch no.0 train no.31070  loss = 2.33186 avg_loss = 4.08704\n",
      "epoch no.0 train no.31080  loss = 2.98998 avg_loss = 4.07736\n",
      "epoch no.0 train no.31090  loss = 2.66838 avg_loss = 4.05057\n",
      "epoch no.0 train no.31100  loss = 3.01062 avg_loss = 4.02599\n",
      "epoch no.0 train no.31110  loss = 2.51996 avg_loss = 4.06577\n",
      "epoch no.0 train no.31120  loss = 5.03051 avg_loss = 4.08009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.31130  loss = 2.88338 avg_loss = 4.04782\n",
      "epoch no.0 train no.31140  loss = 3.17433 avg_loss = 3.98407\n",
      "epoch no.0 train no.31150  loss = 4.64291 avg_loss = 4.00694\n",
      "epoch no.0 train no.31160  loss = 4.85208 avg_loss = 4.02795\n",
      "epoch no.0 train no.31170  loss = 2.84964 avg_loss = 4.00328\n",
      "epoch no.0 train no.31180  loss = 3.32975 avg_loss = 3.96322\n",
      "epoch no.0 train no.31190  loss = 5.57501 avg_loss = 3.99852\n",
      "epoch no.0 train no.31200  loss = 4.54597 avg_loss = 3.99914\n",
      "epoch no.0 train no.31210  loss = 4.17793 avg_loss = 3.96062\n",
      "epoch no.0 train no.31220  loss = 6.50117 avg_loss = 4.01388\n",
      "epoch no.0 train no.31230  loss = 6.26148 avg_loss = 4.01011\n",
      "epoch no.0 train no.31240  loss = 4.14058 avg_loss = 4.00080\n",
      "epoch no.0 train no.31250  loss = 1.62195 avg_loss = 3.97788\n",
      "epoch no.0 train no.31260  loss = 3.77020 avg_loss = 3.97273\n",
      "epoch no.0 train no.31270  loss = 4.62254 avg_loss = 4.03025\n",
      "epoch no.0 train no.31280  loss = 3.32395 avg_loss = 4.03382\n",
      "epoch no.0 train no.31290  loss = 6.33535 avg_loss = 4.07390\n",
      "epoch no.0 train no.31300  loss = 3.14894 avg_loss = 4.04505\n",
      "epoch no.0 train no.31310  loss = 8.29652 avg_loss = 4.07663\n",
      "epoch no.0 train no.31320  loss = 4.29069 avg_loss = 4.08667\n",
      "epoch no.0 train no.31330  loss = 5.23809 avg_loss = 4.04906\n",
      "epoch no.0 train no.31340  loss = 2.79390 avg_loss = 4.00782\n",
      "epoch no.0 train no.31350  loss = 3.60623 avg_loss = 4.03136\n",
      "epoch no.0 train no.31360  loss = 5.08535 avg_loss = 4.01009\n",
      "epoch no.0 train no.31370  loss = 3.30854 avg_loss = 4.01995\n",
      "epoch no.0 train no.31380  loss = 2.34542 avg_loss = 4.02580\n",
      "epoch no.0 train no.31390  loss = 4.05326 avg_loss = 4.00449\n",
      "epoch no.0 train no.31400  loss = 3.87002 avg_loss = 3.99757\n",
      "epoch no.0 train no.31410  loss = 3.30961 avg_loss = 4.03198\n",
      "epoch no.0 train no.31420  loss = 4.38627 avg_loss = 4.02568\n",
      "epoch no.0 train no.31430  loss = 4.96872 avg_loss = 4.04868\n",
      "epoch no.0 train no.31440  loss = 5.62125 avg_loss = 4.06774\n",
      "epoch no.0 train no.31450  loss = 3.65262 avg_loss = 4.05320\n",
      "epoch no.0 train no.31460  loss = 3.58623 avg_loss = 4.02322\n",
      "epoch no.0 train no.31470  loss = 4.06751 avg_loss = 4.05395\n",
      "epoch no.0 train no.31480  loss = 2.38419 avg_loss = 4.07174\n",
      "epoch no.0 train no.31490  loss = 2.58674 avg_loss = 4.09932\n",
      "epoch no.0 train no.31500  loss = 2.67903 avg_loss = 4.06469\n",
      "epoch no.0 train no.31510  loss = 2.37199 avg_loss = 4.06998\n",
      "epoch no.0 train no.31520  loss = 3.67062 avg_loss = 4.01948\n",
      "epoch no.0 train no.31530  loss = 5.18952 avg_loss = 4.00648\n",
      "epoch no.0 train no.31540  loss = 4.60605 avg_loss = 4.00035\n",
      "epoch no.0 train no.31550  loss = 1.77056 avg_loss = 4.00487\n",
      "epoch no.0 train no.31560  loss = 2.77882 avg_loss = 3.99453\n",
      "epoch no.0 train no.31570  loss = 3.54508 avg_loss = 3.98010\n",
      "epoch no.0 train no.31580  loss = 3.13073 avg_loss = 4.04029\n",
      "epoch no.0 train no.31590  loss = 4.51470 avg_loss = 4.04374\n",
      "epoch no.0 train no.31600  loss = 3.66620 avg_loss = 4.06383\n",
      "epoch no.0 train no.31610  loss = 4.67176 avg_loss = 4.05390\n",
      "epoch no.0 train no.31620  loss = 3.30346 avg_loss = 4.00499\n",
      "epoch no.0 train no.31630  loss = 2.84513 avg_loss = 3.96647\n",
      "epoch no.0 train no.31640  loss = 3.44871 avg_loss = 3.96780\n",
      "epoch no.0 train no.31650  loss = 3.30773 avg_loss = 3.95043\n",
      "epoch no.0 train no.31660  loss = 1.81225 avg_loss = 3.95973\n",
      "epoch no.0 train no.31670  loss = 5.11380 avg_loss = 3.98124\n",
      "epoch no.0 train no.31680  loss = 3.98350 avg_loss = 4.03162\n",
      "epoch no.0 train no.31690  loss = 3.85633 avg_loss = 3.97729\n",
      "epoch no.0 train no.31700  loss = 3.11732 avg_loss = 3.92484\n",
      "epoch no.0 train no.31710  loss = 6.92300 avg_loss = 3.96762\n",
      "epoch no.0 train no.31720  loss = 4.66566 avg_loss = 4.00300\n",
      "epoch no.0 train no.31730  loss = 2.64747 avg_loss = 3.99388\n",
      "epoch no.0 train no.31740  loss = 4.98792 avg_loss = 3.97188\n",
      "epoch no.0 train no.31750  loss = 4.37010 avg_loss = 3.96761\n",
      "epoch no.0 train no.31760  loss = 4.73961 avg_loss = 3.93154\n",
      "epoch no.0 train no.31770  loss = 2.94507 avg_loss = 3.97379\n",
      "epoch no.0 train no.31780  loss = 3.62590 avg_loss = 3.94793\n",
      "epoch no.0 train no.31790  loss = 3.92006 avg_loss = 3.99159\n",
      "epoch no.0 train no.31800  loss = 3.68873 avg_loss = 3.97735\n",
      "epoch no.0 train no.31810  loss = 4.18968 avg_loss = 3.91318\n",
      "epoch no.0 train no.31820  loss = 4.65173 avg_loss = 3.96432\n",
      "epoch no.0 train no.31830  loss = 3.70371 avg_loss = 3.93442\n",
      "epoch no.0 train no.31840  loss = 7.44430 avg_loss = 3.92629\n",
      "epoch no.0 train no.31850  loss = 2.64167 avg_loss = 3.89891\n",
      "epoch no.0 train no.31860  loss = 3.51460 avg_loss = 3.90926\n",
      "epoch no.0 train no.31870  loss = 5.92069 avg_loss = 3.89406\n",
      "epoch no.0 train no.31880  loss = 5.96436 avg_loss = 3.93055\n",
      "epoch no.0 train no.31890  loss = 3.34726 avg_loss = 3.93274\n",
      "epoch no.0 train no.31900  loss = 5.14439 avg_loss = 3.93182\n",
      "epoch no.0 train no.31910  loss = 5.25475 avg_loss = 3.99303\n",
      "epoch no.0 train no.31920  loss = 3.21066 avg_loss = 3.93879\n",
      "epoch no.0 train no.31930  loss = 3.47955 avg_loss = 3.97754\n",
      "epoch no.0 train no.31940  loss = 6.15993 avg_loss = 4.05019\n",
      "epoch no.0 train no.31950  loss = 2.15009 avg_loss = 4.03699\n",
      "epoch no.0 train no.31960  loss = 3.92597 avg_loss = 4.07594\n",
      "epoch no.0 train no.31970  loss = 3.49145 avg_loss = 4.07037\n",
      "epoch no.0 train no.31980  loss = 2.31779 avg_loss = 4.06962\n",
      "epoch no.0 train no.31990  loss = 1.35598 avg_loss = 4.03679\n",
      "epoch no.0 train no.32000  loss = 3.98789 avg_loss = 4.10009\n",
      "1\n",
      "to_tokens: ['▁', '좋은', '용', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.0 train no.32010  loss = 2.52326 avg_loss = 4.10890\n",
      "epoch no.0 train no.32020  loss = 2.46983 avg_loss = 4.11191\n",
      "epoch no.0 train no.32030  loss = 3.88831 avg_loss = 4.12647\n",
      "epoch no.0 train no.32040  loss = 4.73453 avg_loss = 4.12186\n",
      "epoch no.0 train no.32050  loss = 2.98761 avg_loss = 4.12574\n",
      "epoch no.0 train no.32060  loss = 4.81335 avg_loss = 4.09960\n",
      "epoch no.0 train no.32070  loss = 3.05942 avg_loss = 4.12252\n",
      "epoch no.0 train no.32080  loss = 5.12333 avg_loss = 4.15075\n",
      "epoch no.0 train no.32090  loss = 5.09207 avg_loss = 4.15989\n",
      "epoch no.0 train no.32100  loss = 4.19882 avg_loss = 4.11059\n",
      "epoch no.0 train no.32110  loss = 3.18768 avg_loss = 4.08032\n",
      "epoch no.0 train no.32120  loss = 5.01328 avg_loss = 4.03369\n",
      "epoch no.0 train no.32130  loss = 5.33507 avg_loss = 4.04190\n",
      "epoch no.0 train no.32140  loss = 4.26983 avg_loss = 4.09717\n",
      "epoch no.0 train no.32150  loss = 2.96546 avg_loss = 4.08907\n",
      "epoch no.0 train no.32160  loss = 4.37467 avg_loss = 4.08070\n",
      "epoch no.0 train no.32170  loss = 4.57696 avg_loss = 4.10772\n",
      "epoch no.0 train no.32180  loss = 3.43018 avg_loss = 4.11646\n",
      "epoch no.0 train no.32190  loss = 2.87525 avg_loss = 4.07240\n",
      "epoch no.0 train no.32200  loss = 3.73193 avg_loss = 4.06433\n",
      "epoch no.0 train no.32210  loss = 2.82038 avg_loss = 4.05965\n",
      "epoch no.0 train no.32220  loss = 4.37420 avg_loss = 4.11017\n",
      "epoch no.0 train no.32230  loss = 7.42034 avg_loss = 4.11329\n",
      "epoch no.0 train no.32240  loss = 4.53345 avg_loss = 4.10235\n",
      "epoch no.0 train no.32250  loss = 3.55197 avg_loss = 4.10600\n",
      "epoch no.0 train no.32260  loss = 3.64692 avg_loss = 4.08286\n",
      "epoch no.0 train no.32270  loss = 3.61565 avg_loss = 4.08626\n",
      "epoch no.0 train no.32280  loss = 1.97027 avg_loss = 4.07064\n",
      "epoch no.0 train no.32290  loss = 2.70441 avg_loss = 4.16895\n",
      "epoch no.0 train no.32300  loss = 3.95507 avg_loss = 4.19337\n",
      "epoch no.0 train no.32310  loss = 2.62844 avg_loss = 4.17869\n",
      "epoch no.0 train no.32320  loss = 4.28007 avg_loss = 4.15131\n",
      "epoch no.0 train no.32330  loss = 4.29806 avg_loss = 4.11600\n",
      "epoch no.0 train no.32340  loss = 3.25129 avg_loss = 4.07683\n",
      "epoch no.0 train no.32350  loss = 3.93703 avg_loss = 4.03838\n",
      "epoch no.0 train no.32360  loss = 2.65105 avg_loss = 4.05134\n",
      "epoch no.0 train no.32370  loss = 3.73788 avg_loss = 4.02104\n",
      "epoch no.0 train no.32380  loss = 4.07708 avg_loss = 4.10081\n",
      "epoch no.0 train no.32390  loss = 3.55184 avg_loss = 4.08770\n",
      "epoch no.0 train no.32400  loss = 2.95587 avg_loss = 4.07554\n",
      "epoch no.0 train no.32410  loss = 3.49246 avg_loss = 4.12917\n",
      "epoch no.0 train no.32420  loss = 5.91842 avg_loss = 4.09798\n",
      "epoch no.0 train no.32430  loss = 2.91612 avg_loss = 4.13920\n",
      "epoch no.0 train no.32440  loss = 6.28227 avg_loss = 4.18526\n",
      "epoch no.0 train no.32450  loss = 4.36202 avg_loss = 4.15223\n",
      "epoch no.0 train no.32460  loss = 4.55371 avg_loss = 4.14759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.32470  loss = 4.08843 avg_loss = 4.11259\n",
      "epoch no.0 train no.32480  loss = 2.22588 avg_loss = 4.01404\n",
      "epoch no.0 train no.32490  loss = 7.71776 avg_loss = 4.04431\n",
      "epoch no.0 train no.32500  loss = 3.87794 avg_loss = 4.00582\n",
      "epoch no.0 train no.32510  loss = 4.54037 avg_loss = 3.96156\n",
      "epoch no.0 train no.32520  loss = 2.92878 avg_loss = 3.92058\n",
      "epoch no.0 train no.32530  loss = 3.94363 avg_loss = 3.89758\n",
      "epoch no.0 train no.32540  loss = 3.93798 avg_loss = 3.92437\n",
      "epoch no.0 train no.32550  loss = 2.55471 avg_loss = 3.93244\n",
      "epoch no.0 train no.32560  loss = 2.83160 avg_loss = 3.89230\n",
      "epoch no.0 train no.32570  loss = 2.40833 avg_loss = 3.91188\n",
      "epoch no.0 train no.32580  loss = 5.89444 avg_loss = 4.01425\n",
      "epoch no.0 train no.32590  loss = 4.92397 avg_loss = 4.07378\n",
      "epoch no.0 train no.32600  loss = 4.83140 avg_loss = 4.08259\n",
      "epoch no.0 train no.32610  loss = 2.89445 avg_loss = 4.10751\n",
      "epoch no.0 train no.32620  loss = 3.95230 avg_loss = 4.13469\n",
      "epoch no.0 train no.32630  loss = 4.06903 avg_loss = 4.13664\n",
      "epoch no.0 train no.32640  loss = 4.55961 avg_loss = 4.16168\n",
      "epoch no.0 train no.32650  loss = 4.36569 avg_loss = 4.17756\n",
      "epoch no.0 train no.32660  loss = 4.58992 avg_loss = 4.13482\n",
      "epoch no.0 train no.32670  loss = 3.75636 avg_loss = 4.12180\n",
      "epoch no.0 train no.32680  loss = 4.25420 avg_loss = 4.11038\n",
      "epoch no.0 train no.32690  loss = 4.48783 avg_loss = 4.10176\n",
      "epoch no.0 train no.32700  loss = 4.70808 avg_loss = 4.13073\n",
      "epoch no.0 train no.32710  loss = 2.29935 avg_loss = 4.08657\n",
      "epoch no.0 train no.32720  loss = 4.52743 avg_loss = 4.06656\n",
      "epoch no.0 train no.32730  loss = 6.68804 avg_loss = 4.06489\n",
      "epoch no.0 train no.32740  loss = 4.43699 avg_loss = 4.07965\n",
      "epoch no.0 train no.32750  loss = 3.84716 avg_loss = 4.04148\n",
      "epoch no.0 train no.32760  loss = 3.41383 avg_loss = 4.03388\n",
      "epoch no.0 train no.32770  loss = 3.64713 avg_loss = 4.05215\n",
      "epoch no.0 train no.32780  loss = 5.05806 avg_loss = 4.09232\n",
      "epoch no.0 train no.32790  loss = 3.12251 avg_loss = 4.07034\n",
      "epoch no.0 train no.32800  loss = 4.15330 avg_loss = 4.06015\n",
      "epoch no.0 train no.32810  loss = 5.01838 avg_loss = 4.08953\n",
      "epoch no.0 train no.32820  loss = 4.31026 avg_loss = 4.09830\n",
      "epoch no.0 train no.32830  loss = 3.88464 avg_loss = 4.11808\n",
      "epoch no.0 train no.32840  loss = 5.50732 avg_loss = 4.12578\n",
      "epoch no.0 train no.32850  loss = 3.88804 avg_loss = 4.10891\n",
      "epoch no.0 train no.32860  loss = 5.06258 avg_loss = 4.14658\n",
      "epoch no.0 train no.32870  loss = 1.68056 avg_loss = 4.11097\n",
      "epoch no.0 train no.32880  loss = 2.29856 avg_loss = 4.05120\n",
      "epoch no.0 train no.32890  loss = 3.53177 avg_loss = 4.02461\n",
      "epoch no.0 train no.32900  loss = 3.14407 avg_loss = 4.05210\n",
      "epoch no.0 train no.32910  loss = 4.74553 avg_loss = 4.00761\n",
      "epoch no.0 train no.32920  loss = 4.33627 avg_loss = 3.95740\n",
      "epoch no.0 train no.32930  loss = 6.04648 avg_loss = 3.94908\n",
      "epoch no.0 train no.32940  loss = 3.95354 avg_loss = 3.91758\n",
      "epoch no.0 train no.32950  loss = 3.41716 avg_loss = 3.90169\n",
      "epoch no.0 train no.32960  loss = 6.58910 avg_loss = 3.96494\n",
      "epoch no.0 train no.32970  loss = 1.99941 avg_loss = 3.94227\n",
      "epoch no.0 train no.32980  loss = 3.47244 avg_loss = 3.89861\n",
      "epoch no.0 train no.32990  loss = 3.82960 avg_loss = 3.88038\n",
      "epoch no.0 train no.33000  loss = 3.36469 avg_loss = 3.91575\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '▁위한', '▁음악', '리스트', '</s>']\n",
      "기분전환을 위한 플레이리스트</s>\n",
      "epoch no.0 train no.33010  loss = 6.14573 avg_loss = 3.96415\n",
      "epoch no.0 train no.33020  loss = 4.49674 avg_loss = 3.93067\n",
      "epoch no.0 train no.33030  loss = 3.66348 avg_loss = 4.01745\n",
      "epoch no.0 train no.33040  loss = 3.14102 avg_loss = 4.05997\n",
      "epoch no.0 train no.33050  loss = 4.41173 avg_loss = 4.02062\n",
      "epoch no.0 train no.33060  loss = 4.32036 avg_loss = 3.98409\n",
      "epoch no.0 train no.33070  loss = 4.03634 avg_loss = 4.02543\n",
      "epoch no.0 train no.33080  loss = 5.58093 avg_loss = 4.09409\n",
      "epoch no.0 train no.33090  loss = 5.30910 avg_loss = 4.13985\n",
      "epoch no.0 train no.33100  loss = 3.16202 avg_loss = 4.10355\n",
      "epoch no.0 train no.33110  loss = 2.85044 avg_loss = 4.12772\n",
      "epoch no.0 train no.33120  loss = 2.87973 avg_loss = 4.11216\n",
      "epoch no.0 train no.33130  loss = 3.90484 avg_loss = 4.07725\n",
      "epoch no.0 train no.33140  loss = 6.74419 avg_loss = 4.05681\n",
      "epoch no.0 train no.33150  loss = 4.12025 avg_loss = 4.04573\n",
      "epoch no.0 train no.33160  loss = 5.20846 avg_loss = 4.13561\n",
      "epoch no.0 train no.33170  loss = 2.46524 avg_loss = 4.11519\n",
      "epoch no.0 train no.33180  loss = 5.03478 avg_loss = 4.12371\n",
      "epoch no.0 train no.33190  loss = 1.73783 avg_loss = 4.08250\n",
      "epoch no.0 train no.33200  loss = 2.95117 avg_loss = 4.06709\n",
      "epoch no.0 train no.33210  loss = 4.12759 avg_loss = 4.02731\n",
      "epoch no.0 train no.33220  loss = 3.77027 avg_loss = 4.03430\n",
      "epoch no.0 train no.33230  loss = 3.86225 avg_loss = 4.04542\n",
      "epoch no.0 train no.33240  loss = 5.58596 avg_loss = 4.07640\n",
      "epoch no.0 train no.33250  loss = 3.15548 avg_loss = 4.02024\n",
      "epoch no.0 train no.33260  loss = 3.37584 avg_loss = 4.00881\n",
      "epoch no.0 train no.33270  loss = 3.33950 avg_loss = 3.98150\n",
      "epoch no.0 train no.33280  loss = 5.04681 avg_loss = 4.05115\n",
      "epoch no.0 train no.33290  loss = 3.18999 avg_loss = 4.05188\n",
      "epoch no.0 train no.33300  loss = 2.60496 avg_loss = 4.10129\n",
      "epoch no.0 train no.33310  loss = 3.94901 avg_loss = 4.07324\n",
      "epoch no.0 train no.33320  loss = 3.40664 avg_loss = 4.04086\n",
      "epoch no.0 train no.33330  loss = 4.90061 avg_loss = 4.06124\n",
      "epoch no.0 train no.33340  loss = 4.16861 avg_loss = 3.98995\n",
      "epoch no.0 train no.33350  loss = 4.05471 avg_loss = 3.98459\n",
      "epoch no.0 train no.33360  loss = 5.21451 avg_loss = 4.00266\n",
      "epoch no.0 train no.33370  loss = 2.94118 avg_loss = 3.99315\n",
      "epoch no.0 train no.33380  loss = 2.20690 avg_loss = 3.98122\n",
      "epoch no.0 train no.33390  loss = 3.55570 avg_loss = 3.99179\n",
      "epoch no.0 train no.33400  loss = 2.22066 avg_loss = 3.96633\n",
      "epoch no.0 train no.33410  loss = 2.75760 avg_loss = 3.93458\n",
      "epoch no.0 train no.33420  loss = 3.37008 avg_loss = 3.99206\n",
      "epoch no.0 train no.33430  loss = 4.67892 avg_loss = 4.01041\n",
      "epoch no.0 train no.33440  loss = 2.78724 avg_loss = 3.96117\n",
      "epoch no.0 train no.33450  loss = 3.19334 avg_loss = 3.98204\n",
      "epoch no.0 train no.33460  loss = 5.58304 avg_loss = 4.02642\n",
      "epoch no.0 train no.33470  loss = 3.36280 avg_loss = 4.06542\n",
      "epoch no.0 train no.33480  loss = 6.35660 avg_loss = 4.08810\n",
      "epoch no.0 train no.33490  loss = 3.69782 avg_loss = 4.04439\n",
      "epoch no.0 train no.33500  loss = 3.68664 avg_loss = 3.96484\n",
      "epoch no.0 train no.33510  loss = 4.53422 avg_loss = 3.97389\n",
      "epoch no.0 train no.33520  loss = 2.39757 avg_loss = 3.99414\n",
      "epoch no.0 train no.33530  loss = 2.53061 avg_loss = 4.01081\n",
      "epoch no.0 train no.33540  loss = 2.92871 avg_loss = 4.03665\n",
      "epoch no.0 train no.33550  loss = 3.29525 avg_loss = 4.02238\n",
      "epoch no.0 train no.33560  loss = 4.13791 avg_loss = 4.02117\n",
      "epoch no.0 train no.33570  loss = 6.93304 avg_loss = 4.13552\n",
      "epoch no.0 train no.33580  loss = 2.66710 avg_loss = 4.12384\n",
      "epoch no.0 train no.33590  loss = 2.97718 avg_loss = 4.10200\n",
      "epoch no.0 train no.33600  loss = 2.89585 avg_loss = 4.04068\n",
      "epoch no.0 train no.33610  loss = 3.11353 avg_loss = 3.95564\n",
      "epoch no.0 train no.33620  loss = 5.61950 avg_loss = 3.92754\n",
      "epoch no.0 train no.33630  loss = 4.29622 avg_loss = 3.95398\n",
      "epoch no.0 train no.33640  loss = 2.63651 avg_loss = 3.96733\n",
      "epoch no.0 train no.33650  loss = 2.59323 avg_loss = 3.96342\n",
      "epoch no.0 train no.33660  loss = 3.70614 avg_loss = 3.98684\n",
      "epoch no.0 train no.33670  loss = 5.86623 avg_loss = 4.00193\n",
      "epoch no.0 train no.33680  loss = 5.85821 avg_loss = 4.07182\n",
      "epoch no.0 train no.33690  loss = 3.93719 avg_loss = 4.09295\n",
      "epoch no.0 train no.33700  loss = 2.96667 avg_loss = 4.04064\n",
      "epoch no.0 train no.33710  loss = 4.78349 avg_loss = 4.07891\n",
      "epoch no.0 train no.33720  loss = 4.23700 avg_loss = 4.05621\n",
      "epoch no.0 train no.33730  loss = 2.51625 avg_loss = 4.00838\n",
      "epoch no.0 train no.33740  loss = 2.08972 avg_loss = 3.95679\n",
      "epoch no.0 train no.33750  loss = 4.32438 avg_loss = 3.99537\n",
      "epoch no.0 train no.33760  loss = 2.46999 avg_loss = 3.97856\n",
      "epoch no.0 train no.33770  loss = 1.32038 avg_loss = 3.96417\n",
      "epoch no.0 train no.33780  loss = 3.94804 avg_loss = 3.88578\n",
      "epoch no.0 train no.33790  loss = 4.06918 avg_loss = 3.91020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.33800  loss = 2.17019 avg_loss = 3.92604\n",
      "epoch no.0 train no.33810  loss = 3.66250 avg_loss = 3.90392\n",
      "epoch no.0 train no.33820  loss = 3.36978 avg_loss = 3.92041\n",
      "epoch no.0 train no.33830  loss = 2.53358 avg_loss = 3.90793\n",
      "epoch no.0 train no.33840  loss = 4.33162 avg_loss = 3.86428\n",
      "epoch no.0 train no.33850  loss = 4.37909 avg_loss = 3.93565\n",
      "epoch no.0 train no.33860  loss = 3.87920 avg_loss = 3.99960\n",
      "epoch no.0 train no.33870  loss = 5.76079 avg_loss = 3.98173\n",
      "epoch no.0 train no.33880  loss = 4.67389 avg_loss = 3.93714\n",
      "epoch no.0 train no.33890  loss = 4.19382 avg_loss = 3.95314\n",
      "epoch no.0 train no.33900  loss = 6.10881 avg_loss = 3.98088\n",
      "epoch no.0 train no.33910  loss = 4.16676 avg_loss = 4.00120\n",
      "epoch no.0 train no.33920  loss = 5.42849 avg_loss = 4.00076\n",
      "epoch no.0 train no.33930  loss = 2.63554 avg_loss = 3.98383\n",
      "epoch no.0 train no.33940  loss = 3.23566 avg_loss = 3.99778\n",
      "epoch no.0 train no.33950  loss = 4.24046 avg_loss = 3.97714\n",
      "epoch no.0 train no.33960  loss = 3.44819 avg_loss = 3.94608\n",
      "epoch no.0 train no.33970  loss = 5.93299 avg_loss = 3.95500\n",
      "epoch no.0 train no.33980  loss = 3.80712 avg_loss = 3.99183\n",
      "epoch no.0 train no.33990  loss = 2.86476 avg_loss = 4.03049\n",
      "epoch no.0 train no.34000  loss = 5.84197 avg_loss = 4.11157\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '을', '▁좋은', '▁노래', '</s>', '</s>']\n",
      "기분전환하기 좋은 노래들</s>\n",
      "epoch no.0 train no.34010  loss = 5.27598 avg_loss = 4.12492\n",
      "epoch no.0 train no.34020  loss = 3.42564 avg_loss = 4.14883\n",
      "epoch no.0 train no.34030  loss = 6.95912 avg_loss = 4.25928\n",
      "epoch no.0 train no.34040  loss = 5.02225 avg_loss = 4.29094\n",
      "epoch no.0 train no.34050  loss = 4.36250 avg_loss = 4.28758\n",
      "epoch no.0 train no.34060  loss = 2.62115 avg_loss = 4.22054\n",
      "epoch no.0 train no.34070  loss = 2.51166 avg_loss = 4.19201\n",
      "epoch no.0 train no.34080  loss = 3.36928 avg_loss = 4.24434\n",
      "epoch no.0 train no.34090  loss = 3.39640 avg_loss = 4.19763\n",
      "epoch no.0 train no.34100  loss = 3.20153 avg_loss = 4.20572\n",
      "epoch no.0 train no.34110  loss = 4.49364 avg_loss = 4.18086\n",
      "epoch no.0 train no.34120  loss = 2.58992 avg_loss = 4.13304\n",
      "epoch no.0 train no.34130  loss = 4.53402 avg_loss = 4.11365\n",
      "epoch no.0 train no.34140  loss = 5.42660 avg_loss = 4.09512\n",
      "epoch no.0 train no.34150  loss = 3.27747 avg_loss = 4.09581\n",
      "epoch no.0 train no.34160  loss = 4.70827 avg_loss = 4.13135\n",
      "epoch no.0 train no.34170  loss = 5.02924 avg_loss = 4.10443\n",
      "epoch no.0 train no.34180  loss = 4.68349 avg_loss = 4.15556\n",
      "epoch no.0 train no.34190  loss = 3.66359 avg_loss = 4.12389\n",
      "epoch no.0 train no.34200  loss = 5.28241 avg_loss = 4.20440\n",
      "epoch no.0 train no.34210  loss = 2.63611 avg_loss = 4.17147\n",
      "epoch no.0 train no.34220  loss = 2.87857 avg_loss = 4.11153\n",
      "epoch no.0 train no.34230  loss = 2.34989 avg_loss = 4.04374\n",
      "epoch no.0 train no.34240  loss = 2.54730 avg_loss = 4.08575\n",
      "epoch no.0 train no.34250  loss = 3.74111 avg_loss = 4.13516\n",
      "epoch no.0 train no.34260  loss = 4.32617 avg_loss = 4.08808\n",
      "epoch no.0 train no.34270  loss = 4.62409 avg_loss = 4.06358\n",
      "epoch no.0 train no.34280  loss = 3.51924 avg_loss = 4.03631\n",
      "epoch no.0 train no.34290  loss = 3.59161 avg_loss = 4.02243\n",
      "epoch no.0 train no.34300  loss = 4.09742 avg_loss = 4.03481\n",
      "epoch no.0 train no.34310  loss = 5.58948 avg_loss = 4.11957\n",
      "epoch no.0 train no.34320  loss = 4.31844 avg_loss = 4.13033\n",
      "epoch no.0 train no.34330  loss = 2.44094 avg_loss = 4.12043\n",
      "epoch no.0 train no.34340  loss = 2.91388 avg_loss = 4.02891\n",
      "epoch no.0 train no.34350  loss = 3.15991 avg_loss = 4.00810\n",
      "epoch no.0 train no.34360  loss = 7.49872 avg_loss = 4.05717\n",
      "epoch no.0 train no.34370  loss = 3.75611 avg_loss = 4.13819\n",
      "epoch no.0 train no.34380  loss = 2.89946 avg_loss = 4.07517\n",
      "epoch no.0 train no.34390  loss = 2.73130 avg_loss = 4.06371\n",
      "epoch no.0 train no.34400  loss = 4.99146 avg_loss = 4.04883\n",
      "epoch no.0 train no.34410  loss = 2.29686 avg_loss = 3.98452\n",
      "epoch no.0 train no.34420  loss = 4.39102 avg_loss = 3.96474\n",
      "epoch no.0 train no.34430  loss = 5.37237 avg_loss = 3.94685\n",
      "epoch no.0 train no.34440  loss = 7.93222 avg_loss = 4.03074\n",
      "epoch no.0 train no.34450  loss = 3.53849 avg_loss = 4.03504\n",
      "epoch no.0 train no.34460  loss = 2.64027 avg_loss = 4.06481\n",
      "epoch no.0 train no.34470  loss = 4.25734 avg_loss = 4.11545\n",
      "epoch no.0 train no.34480  loss = 4.69035 avg_loss = 4.07568\n",
      "epoch no.0 train no.34490  loss = 4.68654 avg_loss = 4.06123\n",
      "epoch no.0 train no.34500  loss = 3.73346 avg_loss = 4.05772\n",
      "epoch no.0 train no.34510  loss = 7.10004 avg_loss = 4.10663\n",
      "epoch no.0 train no.34520  loss = 5.20287 avg_loss = 4.09748\n",
      "epoch no.0 train no.34530  loss = 5.23674 avg_loss = 4.09940\n",
      "epoch no.0 train no.34540  loss = 4.50811 avg_loss = 4.08781\n",
      "epoch no.0 train no.34550  loss = 7.05573 avg_loss = 4.09835\n",
      "epoch no.0 train no.34560  loss = 3.58301 avg_loss = 4.16308\n",
      "epoch no.0 train no.34570  loss = 4.42531 avg_loss = 4.14704\n",
      "epoch no.0 train no.34580  loss = 3.65394 avg_loss = 4.15930\n",
      "epoch no.0 train no.34590  loss = 5.75984 avg_loss = 4.13825\n",
      "epoch no.0 train no.34600  loss = 3.37102 avg_loss = 4.19672\n",
      "epoch no.0 train no.34610  loss = 5.18146 avg_loss = 4.23520\n",
      "epoch no.0 train no.34620  loss = 3.55344 avg_loss = 4.18686\n",
      "epoch no.0 train no.34630  loss = 3.62102 avg_loss = 4.22264\n",
      "epoch no.0 train no.34640  loss = 3.75754 avg_loss = 4.22517\n",
      "epoch no.0 train no.34650  loss = 4.65345 avg_loss = 4.24242\n",
      "epoch no.0 train no.34660  loss = 3.05209 avg_loss = 4.19243\n",
      "epoch no.0 train no.34670  loss = 3.64574 avg_loss = 4.13570\n",
      "epoch no.0 train no.34680  loss = 3.38911 avg_loss = 4.06046\n",
      "epoch no.0 train no.34690  loss = 3.94049 avg_loss = 4.02524\n",
      "epoch no.0 train no.34700  loss = 2.64000 avg_loss = 3.96932\n",
      "epoch no.0 train no.34710  loss = 4.03898 avg_loss = 3.88955\n",
      "epoch no.0 train no.34720  loss = 2.94494 avg_loss = 3.88252\n",
      "epoch no.0 train no.34730  loss = 2.96424 avg_loss = 3.84360\n",
      "epoch no.0 train no.34740  loss = 3.77744 avg_loss = 3.79301\n",
      "epoch no.0 train no.34750  loss = 3.66582 avg_loss = 3.81663\n",
      "epoch no.0 train no.34760  loss = 3.20518 avg_loss = 3.85723\n",
      "epoch no.0 train no.34770  loss = 2.88028 avg_loss = 3.79931\n",
      "epoch no.0 train no.34780  loss = 3.59328 avg_loss = 3.78806\n",
      "epoch no.0 train no.34790  loss = 2.29493 avg_loss = 3.78524\n",
      "epoch no.0 train no.34800  loss = 2.42039 avg_loss = 3.80009\n",
      "epoch no.0 train no.34810  loss = 3.24385 avg_loss = 3.82615\n",
      "epoch no.0 train no.34820  loss = 4.86699 avg_loss = 3.89156\n",
      "epoch no.0 train no.34830  loss = 3.48100 avg_loss = 3.89209\n",
      "epoch no.0 train no.34840  loss = 4.43179 avg_loss = 3.90895\n",
      "epoch no.0 train no.34850  loss = 3.07950 avg_loss = 3.87788\n",
      "epoch no.0 train no.34860  loss = 4.79450 avg_loss = 3.89201\n",
      "epoch no.0 train no.34870  loss = 4.85110 avg_loss = 3.91404\n",
      "epoch no.0 train no.34880  loss = 3.11511 avg_loss = 3.92793\n",
      "epoch no.0 train no.34890  loss = 4.01284 avg_loss = 3.88349\n",
      "epoch no.0 train no.34900  loss = 3.16588 avg_loss = 3.81280\n",
      "epoch no.0 train no.34910  loss = 6.99125 avg_loss = 3.86900\n",
      "epoch no.0 train no.34920  loss = 5.27730 avg_loss = 3.89035\n",
      "epoch no.0 train no.34930  loss = 3.85531 avg_loss = 3.89881\n",
      "epoch no.0 train no.34940  loss = 4.07513 avg_loss = 3.91696\n",
      "epoch no.0 train no.34950  loss = 2.90576 avg_loss = 3.91584\n",
      "epoch no.0 train no.34960  loss = 3.10173 avg_loss = 3.95075\n",
      "epoch no.0 train no.34970  loss = 5.14365 avg_loss = 3.93748\n",
      "epoch no.0 train no.34980  loss = 2.73974 avg_loss = 3.94103\n",
      "epoch no.0 train no.34990  loss = 3.41611 avg_loss = 3.95229\n",
      "epoch no.0 train no.35000  loss = 2.81874 avg_loss = 3.99050\n",
      "1\n",
      "to_tokens: ['▁비', '전환', '용', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.0 train no.35010  loss = 4.51537 avg_loss = 4.04197\n",
      "epoch no.0 train no.35020  loss = 5.47409 avg_loss = 4.04177\n",
      "epoch no.0 train no.35030  loss = 3.34643 avg_loss = 4.03976\n",
      "epoch no.0 train no.35040  loss = 2.10040 avg_loss = 4.03689\n",
      "epoch no.0 train no.35050  loss = 2.61309 avg_loss = 4.08058\n",
      "epoch no.0 train no.35060  loss = 5.07510 avg_loss = 4.11579\n",
      "epoch no.0 train no.35070  loss = 3.78485 avg_loss = 4.10098\n",
      "epoch no.0 train no.35080  loss = 5.36628 avg_loss = 4.09323\n",
      "epoch no.0 train no.35090  loss = 6.51302 avg_loss = 4.18184\n",
      "epoch no.0 train no.35100  loss = 5.68133 avg_loss = 4.23475\n",
      "epoch no.0 train no.35110  loss = 3.31166 avg_loss = 4.20129\n",
      "epoch no.0 train no.35120  loss = 4.45918 avg_loss = 4.20068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.35130  loss = 3.64451 avg_loss = 4.10393\n",
      "epoch no.0 train no.35140  loss = 3.66475 avg_loss = 4.12159\n",
      "epoch no.0 train no.35150  loss = 4.17098 avg_loss = 4.14803\n",
      "epoch no.0 train no.35160  loss = 4.80790 avg_loss = 4.18712\n",
      "epoch no.0 train no.35170  loss = 2.21119 avg_loss = 4.12101\n",
      "epoch no.0 train no.35180  loss = 3.14447 avg_loss = 4.14062\n",
      "epoch no.0 train no.35190  loss = 3.42106 avg_loss = 4.07049\n",
      "epoch no.0 train no.35200  loss = 2.57900 avg_loss = 4.00007\n",
      "epoch no.0 train no.35210  loss = 4.27902 avg_loss = 3.97444\n",
      "epoch no.0 train no.35220  loss = 4.99400 avg_loss = 4.02669\n",
      "epoch no.0 train no.35230  loss = 4.31128 avg_loss = 3.96563\n",
      "epoch no.0 train no.35240  loss = 3.21075 avg_loss = 3.98773\n",
      "epoch no.0 train no.35250  loss = 4.53117 avg_loss = 4.00033\n",
      "epoch no.0 train no.35260  loss = 3.13272 avg_loss = 3.98187\n",
      "epoch no.0 train no.35270  loss = 5.63399 avg_loss = 3.95780\n",
      "epoch no.0 train no.35280  loss = 5.35251 avg_loss = 4.01947\n",
      "epoch no.0 train no.35290  loss = 3.68153 avg_loss = 4.03832\n",
      "epoch no.0 train no.35300  loss = 3.22768 avg_loss = 4.00553\n",
      "epoch no.0 train no.35310  loss = 3.57026 avg_loss = 3.99258\n",
      "epoch no.0 train no.35320  loss = 3.75816 avg_loss = 3.97891\n",
      "epoch no.0 train no.35330  loss = 3.40296 avg_loss = 4.01154\n",
      "epoch no.0 train no.35340  loss = 4.03416 avg_loss = 3.99778\n",
      "epoch no.0 train no.35350  loss = 4.74036 avg_loss = 4.05515\n",
      "epoch no.0 train no.35360  loss = 4.65696 avg_loss = 4.02236\n",
      "epoch no.0 train no.35370  loss = 6.77037 avg_loss = 3.99800\n",
      "epoch no.0 train no.35380  loss = 2.96866 avg_loss = 4.03642\n",
      "epoch no.0 train no.35390  loss = 6.07941 avg_loss = 4.05745\n",
      "epoch no.0 train no.35400  loss = 2.87343 avg_loss = 4.12401\n",
      "epoch no.0 train no.35410  loss = 2.47851 avg_loss = 4.09268\n",
      "epoch no.0 train no.35420  loss = 3.64704 avg_loss = 4.06369\n",
      "epoch no.0 train no.35430  loss = 5.25012 avg_loss = 4.04258\n",
      "epoch no.0 train no.35440  loss = 6.12287 avg_loss = 4.02448\n",
      "epoch no.0 train no.35450  loss = 5.55509 avg_loss = 4.04660\n",
      "epoch no.0 train no.35460  loss = 3.74703 avg_loss = 4.03898\n",
      "epoch no.0 train no.35470  loss = 5.58780 avg_loss = 4.03653\n",
      "epoch no.0 train no.35480  loss = 2.57211 avg_loss = 4.10140\n",
      "epoch no.0 train no.35490  loss = 6.72415 avg_loss = 4.17521\n",
      "epoch no.0 train no.35500  loss = 4.07305 avg_loss = 4.22267\n",
      "epoch no.0 train no.35510  loss = 5.54070 avg_loss = 4.16223\n",
      "epoch no.0 train no.35520  loss = 2.99522 avg_loss = 4.24662\n",
      "epoch no.0 train no.35530  loss = 4.96182 avg_loss = 4.29731\n",
      "epoch no.0 train no.35540  loss = 4.16389 avg_loss = 4.26640\n",
      "epoch no.0 train no.35550  loss = 2.46241 avg_loss = 4.22001\n",
      "epoch no.0 train no.35560  loss = 4.21847 avg_loss = 4.16459\n",
      "epoch no.0 train no.35570  loss = 3.11801 avg_loss = 4.17216\n",
      "epoch no.0 train no.35580  loss = 3.03061 avg_loss = 4.14540\n",
      "epoch no.0 train no.35590  loss = 2.61719 avg_loss = 4.10460\n",
      "epoch no.0 train no.35600  loss = 3.60239 avg_loss = 4.11620\n",
      "epoch no.0 train no.35610  loss = 3.86944 avg_loss = 4.06119\n",
      "epoch no.0 train no.35620  loss = 5.42024 avg_loss = 4.00983\n",
      "epoch no.0 train no.35630  loss = 2.91178 avg_loss = 4.03574\n",
      "epoch no.0 train no.35640  loss = 2.88970 avg_loss = 4.06276\n",
      "epoch no.0 train no.35650  loss = 4.20461 avg_loss = 4.07216\n",
      "epoch no.0 train no.35660  loss = 3.34489 avg_loss = 4.03065\n",
      "epoch no.0 train no.35670  loss = 4.41225 avg_loss = 3.97968\n",
      "epoch no.0 train no.35680  loss = 2.78104 avg_loss = 4.01231\n",
      "epoch no.0 train no.35690  loss = 3.56298 avg_loss = 4.05334\n",
      "epoch no.0 train no.35700  loss = 3.74542 avg_loss = 4.04387\n",
      "epoch no.0 train no.35710  loss = 4.61633 avg_loss = 4.00320\n",
      "epoch no.0 train no.35720  loss = 3.54801 avg_loss = 4.01252\n",
      "epoch no.0 train no.35730  loss = 4.33903 avg_loss = 4.05186\n",
      "epoch no.0 train no.35740  loss = 4.24531 avg_loss = 4.04892\n",
      "epoch no.0 train no.35750  loss = 2.12478 avg_loss = 3.94187\n",
      "epoch no.0 train no.35760  loss = 4.56450 avg_loss = 3.91047\n",
      "epoch no.0 train no.35770  loss = 7.55660 avg_loss = 3.97482\n",
      "epoch no.0 train no.35780  loss = 6.94278 avg_loss = 4.13661\n",
      "epoch no.0 train no.35790  loss = 2.82780 avg_loss = 4.09912\n",
      "epoch no.0 train no.35800  loss = 6.84647 avg_loss = 4.14857\n",
      "epoch no.0 train no.35810  loss = 3.85151 avg_loss = 4.08125\n",
      "epoch no.0 train no.35820  loss = 3.03781 avg_loss = 4.14251\n",
      "epoch no.0 train no.35830  loss = 4.41264 avg_loss = 4.12144\n",
      "epoch no.0 train no.35840  loss = 3.69618 avg_loss = 4.09788\n",
      "epoch no.0 train no.35850  loss = 4.31709 avg_loss = 4.14409\n",
      "epoch no.0 train no.35860  loss = 2.29121 avg_loss = 4.09966\n",
      "epoch no.0 train no.35870  loss = 4.17834 avg_loss = 4.10264\n",
      "epoch no.0 train no.35880  loss = 3.96669 avg_loss = 4.07448\n",
      "epoch no.0 train no.35890  loss = 2.54984 avg_loss = 4.04839\n",
      "epoch no.0 train no.35900  loss = 6.52504 avg_loss = 4.03397\n",
      "epoch no.0 train no.35910  loss = 2.82269 avg_loss = 4.02835\n",
      "epoch no.0 train no.35920  loss = 3.01243 avg_loss = 4.05656\n",
      "epoch no.0 train no.35930  loss = 3.97903 avg_loss = 4.04055\n",
      "epoch no.0 train no.35940  loss = 4.31292 avg_loss = 4.04453\n",
      "epoch no.0 train no.35950  loss = 4.08142 avg_loss = 4.02578\n",
      "epoch no.0 train no.35960  loss = 4.13392 avg_loss = 4.00111\n",
      "epoch no.0 train no.35970  loss = 4.61403 avg_loss = 4.01412\n",
      "epoch no.0 train no.35980  loss = 3.98833 avg_loss = 4.05810\n",
      "epoch no.0 train no.35990  loss = 3.26660 avg_loss = 4.04004\n",
      "epoch no.0 train no.36000  loss = 4.10111 avg_loss = 3.99908\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '</s>', '송', '</s>']\n",
      "기분전환용 팝송</s>\n",
      "epoch no.0 train no.36010  loss = 5.18943 avg_loss = 4.07243\n",
      "epoch no.0 train no.36020  loss = 4.66782 avg_loss = 4.06901\n",
      "epoch no.0 train no.36030  loss = 5.26440 avg_loss = 4.03564\n",
      "epoch no.0 train no.36040  loss = 3.95714 avg_loss = 4.04224\n",
      "epoch no.0 train no.36050  loss = 3.12308 avg_loss = 4.03789\n",
      "epoch no.0 train no.36060  loss = 4.40567 avg_loss = 4.03321\n",
      "epoch no.0 train no.36070  loss = 5.04306 avg_loss = 4.05214\n",
      "epoch no.0 train no.36080  loss = 5.29012 avg_loss = 4.05492\n",
      "epoch no.0 train no.36090  loss = 4.34810 avg_loss = 4.09873\n",
      "epoch no.0 train no.36100  loss = 3.31387 avg_loss = 4.08226\n",
      "epoch no.0 train no.36110  loss = 4.43512 avg_loss = 4.06651\n",
      "epoch no.0 train no.36120  loss = 1.61705 avg_loss = 4.01744\n",
      "epoch no.0 train no.36130  loss = 3.24222 avg_loss = 4.06540\n",
      "epoch no.0 train no.36140  loss = 6.95697 avg_loss = 4.05324\n",
      "epoch no.0 train no.36150  loss = 3.27667 avg_loss = 4.05310\n",
      "epoch no.0 train no.36160  loss = 4.98217 avg_loss = 4.05236\n",
      "epoch no.0 train no.36170  loss = 3.62427 avg_loss = 3.99633\n",
      "epoch no.0 train no.36180  loss = 2.92426 avg_loss = 3.98260\n",
      "epoch no.0 train no.36190  loss = 3.31432 avg_loss = 3.96737\n",
      "epoch no.0 train no.36200  loss = 2.75803 avg_loss = 3.97726\n",
      "epoch no.0 train no.36210  loss = 4.48011 avg_loss = 3.95648\n",
      "epoch no.0 train no.36220  loss = 3.51006 avg_loss = 4.00698\n",
      "epoch no.0 train no.36230  loss = 3.42689 avg_loss = 3.97870\n",
      "epoch no.0 train no.36240  loss = 4.09568 avg_loss = 3.93874\n",
      "epoch no.0 train no.36250  loss = 3.72718 avg_loss = 3.96648\n",
      "epoch no.0 train no.36260  loss = 4.06966 avg_loss = 4.02300\n",
      "epoch no.0 train no.36270  loss = 3.37624 avg_loss = 4.01839\n",
      "epoch no.0 train no.36280  loss = 5.71664 avg_loss = 4.03705\n",
      "epoch no.0 train no.36290  loss = 3.03390 avg_loss = 4.04386\n",
      "epoch no.0 train no.36300  loss = 5.47677 avg_loss = 4.05415\n",
      "epoch no.0 train no.36310  loss = 5.96968 avg_loss = 4.05118\n",
      "epoch no.0 train no.36320  loss = 3.64373 avg_loss = 4.06362\n",
      "epoch no.0 train no.36330  loss = 3.04831 avg_loss = 4.04587\n",
      "epoch no.0 train no.36340  loss = 3.73340 avg_loss = 4.11226\n",
      "epoch no.0 train no.36350  loss = 4.76809 avg_loss = 4.06406\n",
      "epoch no.0 train no.36360  loss = 1.91773 avg_loss = 4.03091\n",
      "epoch no.0 train no.36370  loss = 5.95132 avg_loss = 4.02268\n",
      "epoch no.0 train no.36380  loss = 4.79272 avg_loss = 3.99634\n",
      "epoch no.0 train no.36390  loss = 2.56840 avg_loss = 3.98607\n",
      "epoch no.0 train no.36400  loss = 3.34525 avg_loss = 3.91368\n",
      "epoch no.0 train no.36410  loss = 2.51284 avg_loss = 3.85291\n",
      "epoch no.0 train no.36420  loss = 6.83090 avg_loss = 3.96094\n",
      "epoch no.0 train no.36430  loss = 6.81809 avg_loss = 4.00560\n",
      "epoch no.0 train no.36440  loss = 4.02925 avg_loss = 4.05411\n",
      "epoch no.0 train no.36450  loss = 4.88518 avg_loss = 4.07399\n",
      "epoch no.0 train no.36460  loss = 3.71226 avg_loss = 4.06663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.36470  loss = 2.88322 avg_loss = 4.03596\n",
      "epoch no.0 train no.36480  loss = 4.19268 avg_loss = 4.04789\n",
      "epoch no.0 train no.36490  loss = 7.15883 avg_loss = 4.05419\n",
      "epoch no.0 train no.36500  loss = 2.48842 avg_loss = 4.01942\n",
      "epoch no.0 train no.36510  loss = 3.82168 avg_loss = 3.96954\n",
      "epoch no.0 train no.36520  loss = 2.59592 avg_loss = 3.99789\n",
      "epoch no.0 train no.36530  loss = 4.08111 avg_loss = 4.01060\n",
      "epoch no.0 train no.36540  loss = 4.89670 avg_loss = 4.02136\n",
      "epoch no.0 train no.36550  loss = 6.45396 avg_loss = 4.05207\n",
      "epoch no.0 train no.36560  loss = 3.12678 avg_loss = 4.05873\n",
      "epoch no.0 train no.36570  loss = 2.08277 avg_loss = 4.06822\n",
      "epoch no.0 train no.36580  loss = 3.38846 avg_loss = 4.01761\n",
      "epoch no.0 train no.36590  loss = 4.15294 avg_loss = 4.03727\n",
      "epoch no.0 train no.36600  loss = 3.31383 avg_loss = 4.04692\n",
      "epoch no.0 train no.36610  loss = 8.62824 avg_loss = 4.12159\n",
      "epoch no.0 train no.36620  loss = 3.54850 avg_loss = 4.15331\n",
      "epoch no.0 train no.36630  loss = 6.39100 avg_loss = 4.17948\n",
      "epoch no.0 train no.36640  loss = 4.61183 avg_loss = 4.20576\n",
      "epoch no.0 train no.36650  loss = 5.05549 avg_loss = 4.24674\n",
      "epoch no.0 train no.36660  loss = 5.20933 avg_loss = 4.21400\n",
      "epoch no.0 train no.36670  loss = 5.12632 avg_loss = 4.20979\n",
      "epoch no.0 train no.36680  loss = 4.97154 avg_loss = 4.17652\n",
      "epoch no.0 train no.36690  loss = 3.31268 avg_loss = 4.18071\n",
      "epoch no.0 train no.36700  loss = 4.17906 avg_loss = 4.15484\n",
      "epoch no.0 train no.36710  loss = 3.04884 avg_loss = 4.12731\n",
      "epoch no.0 train no.36720  loss = 6.43697 avg_loss = 4.17086\n",
      "epoch no.0 train no.36730  loss = 2.06891 avg_loss = 4.12949\n",
      "epoch no.0 train no.36740  loss = 8.26536 avg_loss = 4.19585\n",
      "epoch no.0 train no.36750  loss = 4.90342 avg_loss = 4.27703\n",
      "epoch no.0 train no.36760  loss = 2.78963 avg_loss = 4.22062\n",
      "epoch no.0 train no.36770  loss = 2.77747 avg_loss = 4.25978\n",
      "epoch no.0 train no.36780  loss = 2.60061 avg_loss = 4.23914\n",
      "epoch no.0 train no.36790  loss = 4.83391 avg_loss = 4.19352\n",
      "epoch no.0 train no.36800  loss = 1.60910 avg_loss = 4.15959\n",
      "epoch no.0 train no.36810  loss = 4.44179 avg_loss = 4.11657\n",
      "epoch no.0 train no.36820  loss = 4.20420 avg_loss = 4.08579\n",
      "epoch no.0 train no.36830  loss = 3.40454 avg_loss = 4.09780\n",
      "epoch no.0 train no.36840  loss = 4.54709 avg_loss = 4.09477\n",
      "epoch no.0 train no.36850  loss = 3.85266 avg_loss = 4.09547\n",
      "epoch no.0 train no.36860  loss = 4.68187 avg_loss = 4.10562\n",
      "epoch no.0 train no.36870  loss = 5.10197 avg_loss = 4.13579\n",
      "epoch no.0 train no.36880  loss = 5.19809 avg_loss = 4.09327\n",
      "epoch no.0 train no.36890  loss = 2.54388 avg_loss = 4.02971\n",
      "epoch no.0 train no.36900  loss = 2.85097 avg_loss = 4.05159\n",
      "epoch no.0 train no.36910  loss = 4.83689 avg_loss = 4.03918\n",
      "epoch no.0 train no.36920  loss = 5.43234 avg_loss = 4.02724\n",
      "epoch no.0 train no.36930  loss = 2.18376 avg_loss = 4.00825\n",
      "epoch no.0 train no.36940  loss = 3.03995 avg_loss = 3.96048\n",
      "epoch no.0 train no.36950  loss = 5.41309 avg_loss = 3.93562\n",
      "epoch no.0 train no.36960  loss = 3.85081 avg_loss = 3.91637\n",
      "epoch no.0 train no.36970  loss = 7.84351 avg_loss = 3.95880\n",
      "epoch no.0 train no.36980  loss = 5.66026 avg_loss = 4.02538\n",
      "epoch no.0 train no.36990  loss = 4.33516 avg_loss = 4.04459\n",
      "epoch no.0 train no.37000  loss = 3.75476 avg_loss = 4.05787\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.0 train no.37010  loss = 2.76338 avg_loss = 4.04990\n",
      "epoch no.0 train no.37020  loss = 3.76883 avg_loss = 4.08267\n",
      "epoch no.0 train no.37030  loss = 5.02928 avg_loss = 4.07080\n",
      "epoch no.0 train no.37040  loss = 5.09867 avg_loss = 4.07746\n",
      "epoch no.0 train no.37050  loss = 6.28603 avg_loss = 4.05619\n",
      "epoch no.0 train no.37060  loss = 3.00192 avg_loss = 4.05113\n",
      "epoch no.0 train no.37070  loss = 3.93911 avg_loss = 4.09353\n",
      "epoch no.0 train no.37080  loss = 6.59384 avg_loss = 4.15510\n",
      "epoch no.0 train no.37090  loss = 3.87975 avg_loss = 4.13358\n",
      "epoch no.0 train no.37100  loss = 4.43871 avg_loss = 4.08849\n",
      "epoch no.0 train no.37110  loss = 6.50734 avg_loss = 4.04470\n",
      "epoch no.0 train no.37120  loss = 4.71334 avg_loss = 4.04566\n",
      "epoch no.0 train no.37130  loss = 5.08673 avg_loss = 4.07649\n",
      "epoch no.0 train no.37140  loss = 3.86419 avg_loss = 4.05188\n",
      "epoch no.0 train no.37150  loss = 3.22213 avg_loss = 4.10257\n",
      "epoch no.0 train no.37160  loss = 4.91913 avg_loss = 4.13879\n",
      "epoch no.0 train no.37170  loss = 4.13323 avg_loss = 4.08367\n",
      "epoch no.0 train no.37180  loss = 2.90114 avg_loss = 4.06665\n",
      "epoch no.0 train no.37190  loss = 4.01078 avg_loss = 4.01775\n",
      "epoch no.0 train no.37200  loss = 3.80359 avg_loss = 4.01355\n",
      "epoch no.0 train no.37210  loss = 2.70212 avg_loss = 3.90765\n",
      "epoch no.0 train no.37220  loss = 6.45581 avg_loss = 3.95224\n",
      "epoch no.0 train no.37230  loss = 2.88336 avg_loss = 3.96019\n",
      "epoch no.0 train no.37240  loss = 4.03154 avg_loss = 3.95216\n",
      "epoch no.0 train no.37250  loss = 2.86041 avg_loss = 3.97013\n",
      "epoch no.0 train no.37260  loss = 2.27004 avg_loss = 3.96061\n",
      "epoch no.0 train no.37270  loss = 5.66156 avg_loss = 3.95812\n",
      "epoch no.0 train no.37280  loss = 4.85907 avg_loss = 3.92375\n",
      "epoch no.0 train no.37290  loss = 5.52554 avg_loss = 3.96568\n",
      "epoch no.0 train no.37300  loss = 3.12484 avg_loss = 3.99504\n",
      "epoch no.0 train no.37310  loss = 3.39434 avg_loss = 3.92696\n",
      "epoch no.0 train no.37320  loss = 5.39055 avg_loss = 3.92810\n",
      "epoch no.0 train no.37330  loss = 4.15573 avg_loss = 3.97842\n",
      "epoch no.0 train no.37340  loss = 3.17466 avg_loss = 3.99827\n",
      "epoch no.0 train no.37350  loss = 4.95474 avg_loss = 3.98307\n",
      "epoch no.0 train no.37360  loss = 4.48931 avg_loss = 4.00550\n",
      "epoch no.0 train no.37370  loss = 5.71898 avg_loss = 4.06090\n",
      "epoch no.0 train no.37380  loss = 4.02066 avg_loss = 4.07147\n",
      "epoch no.0 train no.37390  loss = 3.45670 avg_loss = 4.06623\n",
      "epoch no.0 train no.37400  loss = 3.27723 avg_loss = 4.13446\n",
      "epoch no.0 train no.37410  loss = 3.38527 avg_loss = 4.20179\n",
      "epoch no.0 train no.37420  loss = 3.37299 avg_loss = 4.14997\n",
      "epoch no.0 train no.37430  loss = 3.32262 avg_loss = 4.04512\n",
      "epoch no.0 train no.37440  loss = 5.37315 avg_loss = 4.05172\n",
      "epoch no.0 train no.37450  loss = 5.39633 avg_loss = 3.99975\n",
      "epoch no.0 train no.37460  loss = 3.46021 avg_loss = 4.01711\n",
      "epoch no.0 train no.37470  loss = 3.91694 avg_loss = 4.00303\n",
      "epoch no.0 train no.37480  loss = 2.97118 avg_loss = 3.99522\n",
      "epoch no.0 train no.37490  loss = 2.96921 avg_loss = 4.00908\n",
      "epoch no.0 train no.37500  loss = 4.25811 avg_loss = 3.96998\n",
      "epoch no.0 train no.37510  loss = 2.56334 avg_loss = 4.01179\n",
      "epoch no.0 train no.37520  loss = 2.76176 avg_loss = 4.01620\n",
      "epoch no.0 train no.37530  loss = 5.38074 avg_loss = 3.98841\n",
      "epoch no.0 train no.37540  loss = 3.58044 avg_loss = 3.91917\n",
      "epoch no.0 train no.37550  loss = 2.85629 avg_loss = 3.93271\n",
      "epoch no.0 train no.37560  loss = 4.76256 avg_loss = 3.98573\n",
      "epoch no.0 train no.37570  loss = 1.95960 avg_loss = 3.93247\n",
      "epoch no.0 train no.37580  loss = 3.23074 avg_loss = 3.96458\n",
      "epoch no.0 train no.37590  loss = 4.46707 avg_loss = 3.98935\n",
      "epoch no.0 train no.37600  loss = 2.53489 avg_loss = 3.95565\n",
      "epoch no.0 train no.37610  loss = 5.89720 avg_loss = 3.97560\n",
      "epoch no.0 train no.37620  loss = 3.42744 avg_loss = 4.02103\n",
      "epoch no.0 train no.37630  loss = 4.76450 avg_loss = 4.01816\n",
      "epoch no.0 train no.37640  loss = 3.59454 avg_loss = 4.02425\n",
      "epoch no.0 train no.37650  loss = 3.86764 avg_loss = 4.06035\n",
      "epoch no.0 train no.37660  loss = 4.19939 avg_loss = 4.04806\n",
      "epoch no.0 train no.37670  loss = 4.34844 avg_loss = 4.02032\n",
      "epoch no.0 train no.37680  loss = 2.23595 avg_loss = 3.93020\n",
      "epoch no.0 train no.37690  loss = 3.18154 avg_loss = 3.88246\n",
      "epoch no.0 train no.37700  loss = 3.46968 avg_loss = 3.81905\n",
      "epoch no.0 train no.37710  loss = 2.64763 avg_loss = 3.84833\n",
      "epoch no.0 train no.37720  loss = 2.45431 avg_loss = 3.81053\n",
      "epoch no.0 train no.37730  loss = 2.99211 avg_loss = 3.84442\n",
      "epoch no.0 train no.37740  loss = 3.79006 avg_loss = 3.81592\n",
      "epoch no.0 train no.37750  loss = 2.71904 avg_loss = 3.81341\n",
      "epoch no.0 train no.37760  loss = 4.54935 avg_loss = 3.80306\n",
      "epoch no.0 train no.37770  loss = 2.26135 avg_loss = 3.83965\n",
      "epoch no.0 train no.37780  loss = 4.84196 avg_loss = 3.85603\n",
      "epoch no.0 train no.37790  loss = 2.49782 avg_loss = 3.87641\n",
      "epoch no.0 train no.37800  loss = 5.93193 avg_loss = 3.84643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.37810  loss = 3.54179 avg_loss = 3.91397\n",
      "epoch no.0 train no.37820  loss = 4.93279 avg_loss = 3.97031\n",
      "epoch no.0 train no.37830  loss = 6.84175 avg_loss = 3.92469\n",
      "epoch no.0 train no.37840  loss = 3.34114 avg_loss = 3.97263\n",
      "epoch no.0 train no.37850  loss = 3.13654 avg_loss = 3.93839\n",
      "epoch no.0 train no.37860  loss = 4.75674 avg_loss = 3.96124\n",
      "epoch no.0 train no.37870  loss = 3.90497 avg_loss = 3.95494\n",
      "epoch no.0 train no.37880  loss = 5.08461 avg_loss = 3.98862\n",
      "epoch no.0 train no.37890  loss = 3.39672 avg_loss = 4.01678\n",
      "epoch no.0 train no.37900  loss = 3.15528 avg_loss = 4.07335\n",
      "epoch no.0 train no.37910  loss = 4.50285 avg_loss = 4.01705\n",
      "epoch no.0 train no.37920  loss = 3.68978 avg_loss = 3.95047\n",
      "epoch no.0 train no.37930  loss = 4.74063 avg_loss = 3.94381\n",
      "epoch no.0 train no.37940  loss = 2.11666 avg_loss = 3.95868\n",
      "epoch no.0 train no.37950  loss = 4.88942 avg_loss = 3.96564\n",
      "epoch no.0 train no.37960  loss = 3.31560 avg_loss = 3.96886\n",
      "epoch no.0 train no.37970  loss = 5.04791 avg_loss = 3.97358\n",
      "epoch no.0 train no.37980  loss = 6.03725 avg_loss = 3.99330\n",
      "epoch no.0 train no.37990  loss = 4.15309 avg_loss = 3.97273\n",
      "epoch no.0 train no.38000  loss = 5.09941 avg_loss = 3.93739\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.0 train no.38010  loss = 2.40479 avg_loss = 3.95960\n",
      "epoch no.0 train no.38020  loss = 2.50155 avg_loss = 3.92917\n",
      "epoch no.0 train no.38030  loss = 5.26578 avg_loss = 3.99166\n",
      "epoch no.0 train no.38040  loss = 5.76124 avg_loss = 3.99574\n",
      "epoch no.0 train no.38050  loss = 3.57418 avg_loss = 3.95401\n",
      "epoch no.0 train no.38060  loss = 3.37208 avg_loss = 3.95084\n",
      "epoch no.0 train no.38070  loss = 3.60562 avg_loss = 3.88657\n",
      "epoch no.0 train no.38080  loss = 1.61317 avg_loss = 3.82812\n",
      "epoch no.0 train no.38090  loss = 3.00651 avg_loss = 3.92797\n",
      "epoch no.0 train no.38100  loss = 4.82158 avg_loss = 3.95433\n",
      "epoch no.0 train no.38110  loss = 3.55546 avg_loss = 3.92330\n",
      "epoch no.0 train no.38120  loss = 5.38233 avg_loss = 3.96205\n",
      "epoch no.0 train no.38130  loss = 2.74531 avg_loss = 3.97492\n",
      "epoch no.0 train no.38140  loss = 3.19618 avg_loss = 3.99180\n",
      "epoch no.0 train no.38150  loss = 3.30749 avg_loss = 3.96368\n",
      "epoch no.0 train no.38160  loss = 4.69759 avg_loss = 3.99797\n",
      "epoch no.0 train no.38170  loss = 4.46172 avg_loss = 3.99258\n",
      "epoch no.0 train no.38180  loss = 3.66804 avg_loss = 4.04221\n",
      "epoch no.0 train no.38190  loss = 3.54818 avg_loss = 4.08068\n",
      "epoch no.0 train no.38200  loss = 6.03702 avg_loss = 4.12653\n",
      "epoch no.0 train no.38210  loss = 4.51303 avg_loss = 4.11315\n",
      "epoch no.0 train no.38220  loss = 3.53724 avg_loss = 4.07431\n",
      "epoch no.0 train no.38230  loss = 3.07636 avg_loss = 4.09215\n",
      "epoch no.0 train no.38240  loss = 5.38346 avg_loss = 4.04282\n",
      "epoch no.0 train no.38250  loss = 5.25395 avg_loss = 4.04202\n",
      "epoch no.0 train no.38260  loss = 3.16535 avg_loss = 4.05758\n",
      "epoch no.0 train no.38270  loss = 3.95381 avg_loss = 4.02997\n",
      "epoch no.0 train no.38280  loss = 5.58379 avg_loss = 4.07190\n",
      "epoch no.0 train no.38290  loss = 4.55587 avg_loss = 4.05165\n",
      "epoch no.0 train no.38300  loss = 3.76074 avg_loss = 4.09491\n",
      "epoch no.0 train no.38310  loss = 8.09295 avg_loss = 4.11731\n",
      "epoch no.0 train no.38320  loss = 3.93219 avg_loss = 4.10224\n",
      "epoch no.0 train no.38330  loss = 3.81786 avg_loss = 4.13026\n",
      "epoch no.0 train no.38340  loss = 5.13172 avg_loss = 4.12772\n",
      "epoch no.0 train no.38350  loss = 3.10741 avg_loss = 4.08243\n",
      "epoch no.0 train no.38360  loss = 4.69546 avg_loss = 4.05430\n",
      "epoch no.0 train no.38370  loss = 3.65793 avg_loss = 4.00203\n",
      "epoch no.0 train no.38380  loss = 3.19340 avg_loss = 3.97987\n",
      "epoch no.0 train no.38390  loss = 2.52413 avg_loss = 3.94924\n",
      "epoch no.0 train no.38400  loss = 4.78497 avg_loss = 3.98556\n",
      "epoch no.0 train no.38410  loss = 3.61798 avg_loss = 4.04508\n",
      "epoch no.0 train no.38420  loss = 4.26157 avg_loss = 4.10031\n",
      "epoch no.0 train no.38430  loss = 4.88552 avg_loss = 4.06670\n",
      "epoch no.0 train no.38440  loss = 2.24757 avg_loss = 4.03601\n",
      "epoch no.0 train no.38450  loss = 2.67863 avg_loss = 4.08530\n",
      "epoch no.0 train no.38460  loss = 3.98006 avg_loss = 4.06445\n",
      "epoch no.0 train no.38470  loss = 3.01458 avg_loss = 4.06686\n",
      "epoch no.0 train no.38480  loss = 5.68232 avg_loss = 4.09618\n",
      "epoch no.0 train no.38490  loss = 4.42173 avg_loss = 4.09533\n",
      "epoch no.0 train no.38500  loss = 3.88647 avg_loss = 4.14391\n",
      "epoch no.0 train no.38510  loss = 2.94055 avg_loss = 4.10949\n",
      "epoch no.0 train no.38520  loss = 3.86161 avg_loss = 4.05446\n",
      "epoch no.0 train no.38530  loss = 5.52320 avg_loss = 4.08791\n",
      "epoch no.0 train no.38540  loss = 3.65590 avg_loss = 4.03728\n",
      "epoch no.0 train no.38550  loss = 3.31584 avg_loss = 4.08092\n",
      "epoch no.0 train no.38560  loss = 3.90502 avg_loss = 4.12407\n",
      "epoch no.0 train no.38570  loss = 2.31184 avg_loss = 4.15672\n",
      "epoch no.0 train no.38580  loss = 5.46977 avg_loss = 4.13138\n",
      "epoch no.0 train no.38590  loss = 4.60272 avg_loss = 4.17243\n",
      "epoch no.0 train no.38600  loss = 3.84169 avg_loss = 4.11077\n",
      "epoch no.0 train no.38610  loss = 7.46580 avg_loss = 4.17846\n",
      "epoch no.0 train no.38620  loss = 4.57522 avg_loss = 4.13875\n",
      "epoch no.0 train no.38630  loss = 3.83265 avg_loss = 4.07396\n",
      "epoch no.0 train no.38640  loss = 4.27668 avg_loss = 3.99011\n",
      "epoch no.0 train no.38650  loss = 2.81925 avg_loss = 3.94390\n",
      "epoch no.0 train no.38660  loss = 4.03313 avg_loss = 3.91333\n",
      "epoch no.0 train no.38670  loss = 4.62298 avg_loss = 3.94004\n",
      "epoch no.0 train no.38680  loss = 4.85209 avg_loss = 3.94237\n",
      "epoch no.0 train no.38690  loss = 3.34624 avg_loss = 3.90286\n",
      "epoch no.0 train no.38700  loss = 4.92032 avg_loss = 3.89855\n",
      "epoch no.0 train no.38710  loss = 3.70434 avg_loss = 3.97689\n",
      "epoch no.0 train no.38720  loss = 4.36523 avg_loss = 3.99726\n",
      "epoch no.0 train no.38730  loss = 4.69964 avg_loss = 4.01764\n",
      "epoch no.0 train no.38740  loss = 3.58418 avg_loss = 4.03558\n",
      "epoch no.0 train no.38750  loss = 2.45824 avg_loss = 3.97499\n",
      "epoch no.0 train no.38760  loss = 2.68571 avg_loss = 3.98323\n",
      "epoch no.0 train no.38770  loss = 6.20472 avg_loss = 3.95482\n",
      "epoch no.0 train no.38780  loss = 6.08837 avg_loss = 3.95150\n",
      "epoch no.0 train no.38790  loss = 3.48481 avg_loss = 3.94506\n",
      "epoch no.0 train no.38800  loss = 1.69249 avg_loss = 3.95396\n",
      "epoch no.0 train no.38810  loss = 5.49238 avg_loss = 3.97175\n",
      "epoch no.0 train no.38820  loss = 4.30567 avg_loss = 3.93620\n",
      "epoch no.0 train no.38830  loss = 4.80270 avg_loss = 3.99136\n",
      "epoch no.0 train no.38840  loss = 4.95608 avg_loss = 4.03492\n",
      "epoch no.0 train no.38850  loss = 3.11332 avg_loss = 4.03164\n",
      "epoch no.0 train no.38860  loss = 3.23280 avg_loss = 3.99242\n",
      "epoch no.0 train no.38870  loss = 3.16189 avg_loss = 3.93393\n",
      "epoch no.0 train no.38880  loss = 2.82192 avg_loss = 3.97520\n",
      "epoch no.0 train no.38890  loss = 4.57411 avg_loss = 3.96243\n",
      "epoch no.0 train no.38900  loss = 2.50164 avg_loss = 3.93589\n",
      "epoch no.0 train no.38910  loss = 2.10259 avg_loss = 3.84828\n",
      "epoch no.0 train no.38920  loss = 3.62413 avg_loss = 3.90819\n",
      "epoch no.0 train no.38930  loss = 3.15240 avg_loss = 3.90358\n",
      "epoch no.0 train no.38940  loss = 2.17919 avg_loss = 3.90959\n",
      "epoch no.0 train no.38950  loss = 3.00542 avg_loss = 3.89181\n",
      "epoch no.0 train no.38960  loss = 2.44362 avg_loss = 3.93958\n",
      "epoch no.0 train no.38970  loss = 3.42906 avg_loss = 3.93369\n",
      "epoch no.0 train no.38980  loss = 3.42417 avg_loss = 3.98612\n",
      "epoch no.0 train no.38990  loss = 3.70739 avg_loss = 4.05305\n",
      "epoch no.0 train no.39000  loss = 3.34738 avg_loss = 4.03136\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.0 train no.39010  loss = 2.19009 avg_loss = 4.06359\n",
      "epoch no.0 train no.39020  loss = 2.84184 avg_loss = 4.02518\n",
      "epoch no.0 train no.39030  loss = 2.67954 avg_loss = 4.00302\n",
      "epoch no.0 train no.39040  loss = 3.51119 avg_loss = 3.95823\n",
      "epoch no.0 train no.39050  loss = 1.93177 avg_loss = 3.91689\n",
      "epoch no.0 train no.39060  loss = 4.38565 avg_loss = 3.93089\n",
      "epoch no.0 train no.39070  loss = 4.47184 avg_loss = 3.98120\n",
      "epoch no.0 train no.39080  loss = 3.39081 avg_loss = 4.00941\n",
      "epoch no.0 train no.39090  loss = 2.95040 avg_loss = 4.03045\n",
      "epoch no.0 train no.39100  loss = 2.09288 avg_loss = 4.01431\n",
      "epoch no.0 train no.39110  loss = 3.56911 avg_loss = 4.00547\n",
      "epoch no.0 train no.39120  loss = 5.69770 avg_loss = 4.03767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.39130  loss = 6.77032 avg_loss = 4.07254\n",
      "epoch no.0 train no.39140  loss = 3.84025 avg_loss = 4.06892\n",
      "epoch no.0 train no.39150  loss = 2.70221 avg_loss = 4.06902\n",
      "epoch no.0 train no.39160  loss = 6.34937 avg_loss = 4.16985\n",
      "epoch no.0 train no.39170  loss = 5.56055 avg_loss = 4.20367\n",
      "epoch no.0 train no.39180  loss = 4.39348 avg_loss = 4.18137\n",
      "epoch no.0 train no.39190  loss = 2.68820 avg_loss = 4.15561\n",
      "epoch no.0 train no.39200  loss = 2.52757 avg_loss = 4.14962\n",
      "epoch no.0 train no.39210  loss = 4.04557 avg_loss = 4.19370\n",
      "epoch no.0 train no.39220  loss = 4.59604 avg_loss = 4.22879\n",
      "epoch no.0 train no.39230  loss = 5.82466 avg_loss = 4.32232\n",
      "epoch no.0 train no.39240  loss = 1.87705 avg_loss = 4.26838\n",
      "epoch no.0 train no.39250  loss = 2.83988 avg_loss = 4.22444\n",
      "epoch no.0 train no.39260  loss = 3.86534 avg_loss = 4.23324\n",
      "epoch no.0 train no.39270  loss = 2.92942 avg_loss = 4.20247\n",
      "epoch no.0 train no.39280  loss = 4.86286 avg_loss = 4.20370\n",
      "epoch no.0 train no.39290  loss = 6.96277 avg_loss = 4.27000\n",
      "epoch no.0 train no.39300  loss = 3.98776 avg_loss = 4.22708\n",
      "epoch no.0 train no.39310  loss = 6.98019 avg_loss = 4.29008\n",
      "epoch no.0 train no.39320  loss = 3.40699 avg_loss = 4.23226\n",
      "epoch no.0 train no.39330  loss = 4.23618 avg_loss = 4.22326\n",
      "epoch no.0 train no.39340  loss = 2.50793 avg_loss = 4.19137\n",
      "epoch no.0 train no.39350  loss = 5.63483 avg_loss = 4.21595\n",
      "epoch no.0 train no.39360  loss = 5.20293 avg_loss = 4.15145\n",
      "epoch no.0 train no.39370  loss = 4.74127 avg_loss = 4.13045\n",
      "epoch no.0 train no.39380  loss = 5.49303 avg_loss = 4.05292\n",
      "epoch no.0 train no.39390  loss = 5.39770 avg_loss = 4.12053\n",
      "epoch no.0 train no.39400  loss = 2.47068 avg_loss = 4.07675\n",
      "epoch no.0 train no.39410  loss = 3.21661 avg_loss = 4.07717\n",
      "epoch no.0 train no.39420  loss = 3.70032 avg_loss = 4.05040\n",
      "epoch no.0 train no.39430  loss = 2.67416 avg_loss = 4.06695\n",
      "epoch no.0 train no.39440  loss = 4.71067 avg_loss = 4.04206\n",
      "epoch no.0 train no.39450  loss = 4.33089 avg_loss = 4.10682\n",
      "epoch no.0 train no.39460  loss = 3.61313 avg_loss = 4.09280\n",
      "epoch no.0 train no.39470  loss = 3.39332 avg_loss = 4.05562\n",
      "epoch no.0 train no.39480  loss = 1.69639 avg_loss = 4.00652\n",
      "epoch no.0 train no.39490  loss = 3.77013 avg_loss = 3.96716\n",
      "epoch no.0 train no.39500  loss = 2.44904 avg_loss = 3.93020\n",
      "epoch no.0 train no.39510  loss = 2.97397 avg_loss = 3.90908\n",
      "epoch no.0 train no.39520  loss = 4.13612 avg_loss = 3.94655\n",
      "epoch no.0 train no.39530  loss = 4.74502 avg_loss = 3.94777\n",
      "epoch no.0 train no.39540  loss = 6.17653 avg_loss = 3.95622\n",
      "epoch no.0 train no.39550  loss = 3.66636 avg_loss = 3.93201\n",
      "epoch no.0 train no.39560  loss = 4.55020 avg_loss = 3.98282\n",
      "epoch no.0 train no.39570  loss = 3.68979 avg_loss = 4.01621\n",
      "epoch no.0 train no.39580  loss = 3.89534 avg_loss = 4.01206\n",
      "epoch no.0 train no.39590  loss = 3.96791 avg_loss = 4.04463\n",
      "epoch no.0 train no.39600  loss = 3.67672 avg_loss = 4.05923\n",
      "epoch no.0 train no.39610  loss = 4.20901 avg_loss = 4.05792\n",
      "epoch no.0 train no.39620  loss = 4.61886 avg_loss = 4.07058\n",
      "epoch no.0 train no.39630  loss = 3.22555 avg_loss = 4.05824\n",
      "epoch no.0 train no.39640  loss = 2.17276 avg_loss = 4.06062\n",
      "epoch no.0 train no.39650  loss = 4.56112 avg_loss = 4.03706\n",
      "epoch no.0 train no.39660  loss = 4.64929 avg_loss = 4.02569\n",
      "epoch no.0 train no.39670  loss = 4.07272 avg_loss = 4.05177\n",
      "epoch no.0 train no.39680  loss = 4.19916 avg_loss = 4.05416\n",
      "epoch no.0 train no.39690  loss = 4.62716 avg_loss = 4.06896\n",
      "epoch no.0 train no.39700  loss = 3.18238 avg_loss = 4.07504\n",
      "epoch no.0 train no.39710  loss = 2.99472 avg_loss = 4.08064\n",
      "epoch no.0 train no.39720  loss = 2.98912 avg_loss = 4.02750\n",
      "epoch no.0 train no.39730  loss = 2.50566 avg_loss = 3.97913\n",
      "epoch no.0 train no.39740  loss = 4.79854 avg_loss = 3.96991\n",
      "epoch no.0 train no.39750  loss = 5.17984 avg_loss = 3.98181\n",
      "epoch no.0 train no.39760  loss = 4.70857 avg_loss = 3.94938\n",
      "epoch no.0 train no.39770  loss = 2.53359 avg_loss = 3.87656\n",
      "epoch no.0 train no.39780  loss = 3.29562 avg_loss = 3.88196\n",
      "epoch no.0 train no.39790  loss = 6.54530 avg_loss = 3.94239\n",
      "epoch no.0 train no.39800  loss = 6.08905 avg_loss = 3.95445\n",
      "epoch no.0 train no.39810  loss = 3.77721 avg_loss = 3.98009\n",
      "epoch no.0 train no.39820  loss = 5.53224 avg_loss = 3.98363\n",
      "epoch no.0 train no.39830  loss = 2.77107 avg_loss = 3.89870\n",
      "epoch no.0 train no.39840  loss = 5.96092 avg_loss = 3.91767\n",
      "epoch no.0 train no.39850  loss = 4.10489 avg_loss = 3.93538\n",
      "epoch no.0 train no.39860  loss = 3.90711 avg_loss = 3.91856\n",
      "epoch no.0 train no.39870  loss = 2.31885 avg_loss = 3.95421\n",
      "epoch no.0 train no.39880  loss = 2.61200 avg_loss = 3.91217\n",
      "epoch no.0 train no.39890  loss = 2.88088 avg_loss = 3.93829\n",
      "epoch no.0 train no.39900  loss = 2.37213 avg_loss = 3.93496\n",
      "epoch no.0 train no.39910  loss = 3.07588 avg_loss = 3.95695\n",
      "epoch no.0 train no.39920  loss = 3.56065 avg_loss = 3.93449\n",
      "epoch no.0 train no.39930  loss = 4.27221 avg_loss = 3.96326\n",
      "epoch no.0 train no.39940  loss = 6.90561 avg_loss = 3.99128\n",
      "epoch no.0 train no.39950  loss = 5.27657 avg_loss = 3.97522\n",
      "epoch no.0 train no.39960  loss = 2.74703 avg_loss = 3.92934\n",
      "epoch no.0 train no.39970  loss = 3.90204 avg_loss = 3.95672\n",
      "epoch no.0 train no.39980  loss = 2.58250 avg_loss = 3.95571\n",
      "epoch no.0 train no.39990  loss = 4.25658 avg_loss = 3.94208\n",
      "epoch no.0 train no.40000  loss = 4.56571 avg_loss = 3.96577\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.0 train no.40010  loss = 3.66020 avg_loss = 3.98720\n",
      "epoch no.0 train no.40020  loss = 5.64357 avg_loss = 4.01142\n",
      "epoch no.0 train no.40030  loss = 2.85052 avg_loss = 3.94887\n",
      "epoch no.0 train no.40040  loss = 4.53934 avg_loss = 3.94813\n",
      "epoch no.0 train no.40050  loss = 2.85799 avg_loss = 3.90021\n",
      "epoch no.0 train no.40060  loss = 5.14608 avg_loss = 3.85812\n",
      "epoch no.0 train no.40070  loss = 3.43291 avg_loss = 3.85266\n",
      "epoch no.0 train no.40080  loss = 5.54662 avg_loss = 3.86028\n",
      "epoch no.0 train no.40090  loss = 7.58706 avg_loss = 3.90170\n",
      "epoch no.0 train no.40100  loss = 4.32308 avg_loss = 3.94203\n",
      "epoch no.0 train no.40110  loss = 3.71987 avg_loss = 3.94204\n",
      "epoch no.0 train no.40120  loss = 4.20017 avg_loss = 3.99186\n",
      "epoch no.0 train no.40130  loss = 4.67110 avg_loss = 3.96210\n",
      "epoch no.0 train no.40140  loss = 4.23806 avg_loss = 3.97887\n",
      "epoch no.0 train no.40150  loss = 5.63473 avg_loss = 3.96724\n",
      "epoch no.0 train no.40160  loss = 3.92794 avg_loss = 3.92444\n",
      "epoch no.0 train no.40170  loss = 2.90681 avg_loss = 3.90957\n",
      "epoch no.0 train no.40180  loss = 4.38460 avg_loss = 3.92835\n",
      "epoch no.0 train no.40190  loss = 3.44140 avg_loss = 3.95579\n",
      "epoch no.0 train no.40200  loss = 5.94841 avg_loss = 3.97269\n",
      "epoch no.0 train no.40210  loss = 3.34729 avg_loss = 3.94934\n",
      "epoch no.0 train no.40220  loss = 5.08434 avg_loss = 3.92786\n",
      "epoch no.0 train no.40230  loss = 2.65071 avg_loss = 3.92453\n",
      "epoch no.0 train no.40240  loss = 3.95566 avg_loss = 3.94005\n",
      "epoch no.0 train no.40250  loss = 2.55107 avg_loss = 3.92731\n",
      "epoch no.0 train no.40260  loss = 2.12555 avg_loss = 3.90020\n",
      "epoch no.0 train no.40270  loss = 3.31502 avg_loss = 3.87426\n",
      "epoch no.0 train no.40280  loss = 2.00690 avg_loss = 3.84966\n",
      "epoch no.0 train no.40290  loss = 1.53590 avg_loss = 3.84074\n",
      "epoch no.0 train no.40300  loss = 2.89659 avg_loss = 3.77564\n",
      "epoch no.0 train no.40310  loss = 3.79321 avg_loss = 3.84116\n",
      "epoch no.0 train no.40320  loss = 5.21498 avg_loss = 3.85594\n",
      "epoch no.0 train no.40330  loss = 3.93638 avg_loss = 3.90425\n",
      "epoch no.0 train no.40340  loss = 3.73302 avg_loss = 3.84918\n",
      "epoch no.0 train no.40350  loss = 6.71999 avg_loss = 3.91235\n",
      "epoch no.0 train no.40360  loss = 5.07979 avg_loss = 3.87825\n",
      "epoch no.0 train no.40370  loss = 4.42213 avg_loss = 3.84218\n",
      "epoch no.0 train no.40380  loss = 4.95109 avg_loss = 3.83454\n",
      "epoch no.0 train no.40390  loss = 6.83789 avg_loss = 3.88780\n",
      "epoch no.0 train no.40400  loss = 2.87153 avg_loss = 3.84556\n",
      "epoch no.0 train no.40410  loss = 7.08429 avg_loss = 3.92368\n",
      "epoch no.0 train no.40420  loss = 5.96384 avg_loss = 3.97534\n",
      "epoch no.0 train no.40430  loss = 5.52036 avg_loss = 3.98812\n",
      "epoch no.0 train no.40440  loss = 6.59091 avg_loss = 3.99784\n",
      "epoch no.0 train no.40450  loss = 2.58297 avg_loss = 4.02234\n",
      "epoch no.0 train no.40460  loss = 2.49712 avg_loss = 4.05195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.40470  loss = 6.83183 avg_loss = 4.07306\n",
      "epoch no.0 train no.40480  loss = 4.56174 avg_loss = 4.09333\n",
      "epoch no.0 train no.40490  loss = 4.65602 avg_loss = 4.04452\n",
      "epoch no.0 train no.40500  loss = 4.66136 avg_loss = 3.98995\n",
      "epoch no.0 train no.40510  loss = 2.31512 avg_loss = 3.98760\n",
      "epoch no.0 train no.40520  loss = 3.97167 avg_loss = 4.00873\n",
      "epoch no.0 train no.40530  loss = 3.22645 avg_loss = 3.97796\n",
      "epoch no.0 train no.40540  loss = 2.77424 avg_loss = 3.94448\n",
      "epoch no.0 train no.40550  loss = 6.75673 avg_loss = 3.96463\n",
      "epoch no.0 train no.40560  loss = 2.76465 avg_loss = 4.03124\n",
      "epoch no.0 train no.40570  loss = 2.99157 avg_loss = 3.98827\n",
      "epoch no.0 train no.40580  loss = 3.62928 avg_loss = 4.00753\n",
      "epoch no.0 train no.40590  loss = 2.11319 avg_loss = 3.98852\n",
      "epoch no.0 train no.40600  loss = 4.61052 avg_loss = 4.00008\n",
      "epoch no.0 train no.40610  loss = 3.89545 avg_loss = 3.96459\n",
      "epoch no.0 train no.40620  loss = 2.41413 avg_loss = 3.95127\n",
      "epoch no.0 train no.40630  loss = 4.42510 avg_loss = 3.91121\n",
      "epoch no.0 train no.40640  loss = 4.26401 avg_loss = 3.90558\n",
      "epoch no.0 train no.40650  loss = 5.32562 avg_loss = 3.95941\n",
      "epoch no.0 train no.40660  loss = 3.99891 avg_loss = 3.98019\n",
      "epoch no.0 train no.40670  loss = 3.83276 avg_loss = 3.99509\n",
      "epoch no.0 train no.40680  loss = 6.35968 avg_loss = 4.04803\n",
      "epoch no.0 train no.40690  loss = 3.10764 avg_loss = 4.04082\n",
      "epoch no.0 train no.40700  loss = 2.58435 avg_loss = 4.01483\n",
      "epoch no.0 train no.40710  loss = 4.05574 avg_loss = 4.00354\n",
      "epoch no.0 train no.40720  loss = 3.89938 avg_loss = 3.98782\n",
      "epoch no.0 train no.40730  loss = 3.68491 avg_loss = 3.97496\n",
      "epoch no.0 train no.40740  loss = 5.27961 avg_loss = 3.99035\n",
      "epoch no.0 train no.40750  loss = 3.62152 avg_loss = 3.94492\n",
      "epoch no.0 train no.40760  loss = 4.10924 avg_loss = 3.97405\n",
      "epoch no.0 train no.40770  loss = 3.94920 avg_loss = 3.98706\n",
      "epoch no.0 train no.40780  loss = 3.10087 avg_loss = 3.98201\n",
      "epoch no.0 train no.40790  loss = 3.60767 avg_loss = 4.02228\n",
      "epoch no.0 train no.40800  loss = 4.04655 avg_loss = 4.04282\n",
      "epoch no.0 train no.40810  loss = 6.20223 avg_loss = 4.02281\n",
      "epoch no.0 train no.40820  loss = 3.62720 avg_loss = 3.99860\n",
      "epoch no.0 train no.40830  loss = 3.63902 avg_loss = 4.00862\n",
      "epoch no.0 train no.40840  loss = 2.28847 avg_loss = 4.00545\n",
      "epoch no.0 train no.40850  loss = 3.26568 avg_loss = 3.95987\n",
      "epoch no.0 train no.40860  loss = 5.98285 avg_loss = 3.94291\n",
      "epoch no.0 train no.40870  loss = 5.81356 avg_loss = 4.04531\n",
      "epoch no.0 train no.40880  loss = 4.65275 avg_loss = 4.03291\n",
      "epoch no.0 train no.40890  loss = 3.02401 avg_loss = 4.01462\n",
      "epoch no.0 train no.40900  loss = 7.15749 avg_loss = 4.06121\n",
      "epoch no.0 train no.40910  loss = 3.04758 avg_loss = 4.07342\n",
      "epoch no.0 train no.40920  loss = 3.83872 avg_loss = 4.08035\n",
      "epoch no.0 train no.40930  loss = 3.27793 avg_loss = 4.03083\n",
      "epoch no.0 train no.40940  loss = 4.07908 avg_loss = 4.01614\n",
      "epoch no.0 train no.40950  loss = 3.73611 avg_loss = 4.06074\n",
      "epoch no.0 train no.40960  loss = 3.84396 avg_loss = 4.03157\n",
      "epoch no.0 train no.40970  loss = 1.60953 avg_loss = 4.01483\n",
      "epoch no.0 train no.40980  loss = 3.13819 avg_loss = 3.95610\n",
      "epoch no.0 train no.40990  loss = 5.70856 avg_loss = 3.96479\n",
      "epoch no.0 train no.41000  loss = 3.36952 avg_loss = 3.97367\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '용', '▁위한', '▁노래', '전환', '용', '</s>']\n",
      "기분전환을 위한 기분전환 뮤직</s>\n",
      "epoch no.0 train no.41010  loss = 4.21319 avg_loss = 4.02718\n",
      "epoch no.0 train no.41020  loss = 3.61873 avg_loss = 3.96076\n",
      "epoch no.0 train no.41030  loss = 4.14190 avg_loss = 3.99395\n",
      "epoch no.0 train no.41040  loss = 2.70549 avg_loss = 3.97478\n",
      "epoch no.0 train no.41050  loss = 4.42814 avg_loss = 3.96334\n",
      "epoch no.0 train no.41060  loss = 4.05467 avg_loss = 4.00289\n",
      "epoch no.0 train no.41070  loss = 4.22742 avg_loss = 3.99184\n",
      "epoch no.0 train no.41080  loss = 5.26304 avg_loss = 3.91190\n",
      "epoch no.0 train no.41090  loss = 4.32161 avg_loss = 3.91941\n",
      "epoch no.0 train no.41100  loss = 6.19371 avg_loss = 3.93518\n",
      "epoch no.0 train no.41110  loss = 6.31909 avg_loss = 3.98366\n",
      "epoch no.0 train no.41120  loss = 4.30901 avg_loss = 3.98878\n",
      "epoch no.0 train no.41130  loss = 5.67229 avg_loss = 3.97120\n",
      "epoch no.0 train no.41140  loss = 4.68975 avg_loss = 3.91378\n",
      "epoch no.0 train no.41150  loss = 2.81946 avg_loss = 3.92358\n",
      "epoch no.0 train no.41160  loss = 3.00574 avg_loss = 3.89824\n",
      "epoch no.0 train no.41170  loss = 2.57069 avg_loss = 3.87534\n",
      "epoch no.0 train no.41180  loss = 5.01370 avg_loss = 3.86712\n",
      "epoch no.0 train no.41190  loss = 6.27415 avg_loss = 3.90607\n",
      "epoch no.0 train no.41200  loss = 3.83326 avg_loss = 3.92315\n",
      "epoch no.0 train no.41210  loss = 2.44379 avg_loss = 3.89164\n",
      "epoch no.0 train no.41220  loss = 3.38053 avg_loss = 3.86928\n",
      "epoch no.0 train no.41230  loss = 5.13527 avg_loss = 3.84705\n",
      "epoch no.0 train no.41240  loss = 2.62520 avg_loss = 3.89795\n",
      "epoch no.0 train no.41250  loss = 5.95518 avg_loss = 3.90245\n",
      "epoch no.0 train no.41260  loss = 2.75245 avg_loss = 3.95052\n",
      "epoch no.0 train no.41270  loss = 3.15501 avg_loss = 3.95658\n",
      "epoch no.0 train no.41280  loss = 4.33306 avg_loss = 3.95145\n",
      "epoch no.0 train no.41290  loss = 4.89044 avg_loss = 3.91117\n",
      "epoch no.0 train no.41300  loss = 4.40057 avg_loss = 3.89489\n",
      "epoch no.0 train no.41310  loss = 8.31037 avg_loss = 3.93834\n",
      "epoch no.0 train no.41320  loss = 1.95070 avg_loss = 3.89160\n",
      "epoch no.0 train no.41330  loss = 3.79457 avg_loss = 3.89559\n",
      "epoch no.0 train no.41340  loss = 5.87586 avg_loss = 3.89401\n",
      "epoch no.0 train no.41350  loss = 3.13569 avg_loss = 3.95723\n",
      "epoch no.0 train no.41360  loss = 3.00768 avg_loss = 3.93927\n",
      "epoch no.0 train no.41370  loss = 8.67094 avg_loss = 3.95648\n",
      "epoch no.0 train no.41380  loss = 3.04906 avg_loss = 3.93388\n",
      "epoch no.0 train no.41390  loss = 5.66720 avg_loss = 3.95963\n",
      "epoch no.0 train no.41400  loss = 3.69236 avg_loss = 3.88794\n",
      "epoch no.0 train no.41410  loss = 3.19157 avg_loss = 3.90417\n",
      "epoch no.0 train no.41420  loss = 4.82659 avg_loss = 3.85518\n",
      "epoch no.0 train no.41430  loss = 3.63239 avg_loss = 3.87495\n",
      "epoch no.0 train no.41440  loss = 5.90366 avg_loss = 3.90830\n",
      "epoch no.0 train no.41450  loss = 3.40759 avg_loss = 3.92259\n",
      "epoch no.0 train no.41460  loss = 2.15015 avg_loss = 3.88862\n",
      "epoch no.0 train no.41470  loss = 2.81399 avg_loss = 3.89452\n",
      "epoch no.0 train no.41480  loss = 3.56826 avg_loss = 3.91001\n",
      "epoch no.0 train no.41490  loss = 4.78839 avg_loss = 3.97718\n",
      "epoch no.0 train no.41500  loss = 3.20094 avg_loss = 4.00082\n",
      "epoch no.0 train no.41510  loss = 3.34388 avg_loss = 3.95944\n",
      "epoch no.0 train no.41520  loss = 5.91332 avg_loss = 3.96984\n",
      "epoch no.0 train no.41530  loss = 4.28322 avg_loss = 3.93702\n",
      "epoch no.0 train no.41540  loss = 5.77645 avg_loss = 3.91876\n",
      "epoch no.0 train no.41550  loss = 1.99027 avg_loss = 3.90065\n",
      "epoch no.0 train no.41560  loss = 4.75985 avg_loss = 3.92359\n",
      "epoch no.0 train no.41570  loss = 2.92968 avg_loss = 3.91994\n",
      "epoch no.0 train no.41580  loss = 3.38552 avg_loss = 3.92701\n",
      "epoch no.0 train no.41590  loss = 4.30805 avg_loss = 3.91615\n",
      "epoch no.0 train no.41600  loss = 4.81780 avg_loss = 3.93825\n",
      "epoch no.0 train no.41610  loss = 5.04107 avg_loss = 3.93319\n",
      "epoch no.0 train no.41620  loss = 3.77593 avg_loss = 3.93852\n",
      "epoch no.0 train no.41630  loss = 2.65790 avg_loss = 4.00096\n",
      "epoch no.0 train no.41640  loss = 3.88470 avg_loss = 3.98321\n",
      "epoch no.0 train no.41650  loss = 2.76203 avg_loss = 3.96119\n",
      "epoch no.0 train no.41660  loss = 4.39818 avg_loss = 3.95133\n",
      "epoch no.0 train no.41670  loss = 3.91701 avg_loss = 3.98194\n",
      "epoch no.0 train no.41680  loss = 4.58633 avg_loss = 3.94891\n",
      "epoch no.0 train no.41690  loss = 3.24744 avg_loss = 3.96358\n",
      "epoch no.0 train no.41700  loss = 3.20096 avg_loss = 3.93836\n",
      "epoch no.0 train no.41710  loss = 3.04063 avg_loss = 3.97788\n",
      "epoch no.0 train no.41720  loss = 6.99152 avg_loss = 3.97193\n",
      "epoch no.0 train no.41730  loss = 3.60714 avg_loss = 3.98186\n",
      "epoch no.0 train no.41740  loss = 4.61934 avg_loss = 3.95752\n",
      "epoch no.0 train no.41750  loss = 4.95005 avg_loss = 4.02809\n",
      "epoch no.0 train no.41760  loss = 2.99661 avg_loss = 4.02236\n",
      "epoch no.0 train no.41770  loss = 4.24823 avg_loss = 4.05597\n",
      "epoch no.0 train no.41780  loss = 4.20926 avg_loss = 4.03420\n",
      "epoch no.0 train no.41790  loss = 4.09784 avg_loss = 3.99864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.41800  loss = 4.43742 avg_loss = 3.98835\n",
      "epoch no.0 train no.41810  loss = 2.98604 avg_loss = 4.00644\n",
      "epoch no.0 train no.41820  loss = 5.23659 avg_loss = 3.99991\n",
      "epoch no.0 train no.41830  loss = 2.90923 avg_loss = 3.97652\n",
      "epoch no.0 train no.41840  loss = 5.12933 avg_loss = 3.92671\n",
      "epoch no.0 train no.41850  loss = 4.06043 avg_loss = 3.92552\n",
      "epoch no.0 train no.41860  loss = 6.50077 avg_loss = 3.96220\n",
      "epoch no.0 train no.41870  loss = 4.93054 avg_loss = 3.90895\n",
      "epoch no.0 train no.41880  loss = 3.14999 avg_loss = 3.95837\n",
      "epoch no.0 train no.41890  loss = 3.34718 avg_loss = 4.03020\n",
      "epoch no.0 train no.41900  loss = 4.96231 avg_loss = 4.05069\n",
      "epoch no.0 train no.41910  loss = 2.93902 avg_loss = 4.07252\n",
      "epoch no.0 train no.41920  loss = 4.90920 avg_loss = 4.04491\n",
      "epoch no.0 train no.41930  loss = 6.48970 avg_loss = 4.07773\n",
      "epoch no.0 train no.41940  loss = 2.85648 avg_loss = 4.11246\n",
      "epoch no.0 train no.41950  loss = 2.03167 avg_loss = 4.07354\n",
      "epoch no.0 train no.41960  loss = 3.00954 avg_loss = 4.05629\n",
      "epoch no.0 train no.41970  loss = 4.39807 avg_loss = 4.06085\n",
      "epoch no.0 train no.41980  loss = 4.21968 avg_loss = 4.06359\n",
      "epoch no.0 train no.41990  loss = 4.99647 avg_loss = 3.98512\n",
      "epoch no.0 train no.42000  loss = 3.11300 avg_loss = 4.02658\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '에', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.0 train no.42010  loss = 4.38981 avg_loss = 3.96980\n",
      "epoch no.0 train no.42020  loss = 3.96564 avg_loss = 3.94604\n",
      "epoch no.0 train no.42030  loss = 3.93362 avg_loss = 3.92599\n",
      "epoch no.0 train no.42040  loss = 2.90834 avg_loss = 3.96991\n",
      "epoch no.0 train no.42050  loss = 4.13197 avg_loss = 3.98369\n",
      "epoch no.0 train no.42060  loss = 3.08709 avg_loss = 4.03359\n",
      "epoch no.0 train no.42070  loss = 7.19555 avg_loss = 4.06228\n",
      "epoch no.0 train no.42080  loss = 3.49453 avg_loss = 4.01386\n",
      "epoch no.0 train no.42090  loss = 5.08893 avg_loss = 4.05513\n",
      "epoch no.0 train no.42100  loss = 2.71627 avg_loss = 4.03814\n",
      "epoch no.0 train no.42110  loss = 4.57379 avg_loss = 4.00694\n",
      "epoch no.0 train no.42120  loss = 3.09797 avg_loss = 3.98107\n",
      "epoch no.0 train no.42130  loss = 3.32252 avg_loss = 3.93944\n",
      "epoch no.0 train no.42140  loss = 4.51133 avg_loss = 3.95843\n",
      "epoch no.0 train no.42150  loss = 3.05476 avg_loss = 4.00709\n",
      "epoch no.0 train no.42160  loss = 2.46282 avg_loss = 3.96492\n",
      "epoch no.0 train no.42170  loss = 5.52323 avg_loss = 4.00593\n",
      "epoch no.0 train no.42180  loss = 4.29196 avg_loss = 3.97575\n",
      "epoch no.0 train no.42190  loss = 3.21829 avg_loss = 3.97973\n",
      "epoch no.0 train no.42200  loss = 4.92442 avg_loss = 3.96296\n",
      "epoch no.0 train no.42210  loss = 5.65895 avg_loss = 3.98706\n",
      "epoch no.0 train no.42220  loss = 4.33688 avg_loss = 3.96547\n",
      "epoch no.0 train no.42230  loss = 4.37107 avg_loss = 3.93564\n",
      "epoch no.0 train no.42240  loss = 2.80133 avg_loss = 3.91537\n",
      "epoch no.0 train no.42250  loss = 5.90381 avg_loss = 3.91605\n",
      "epoch no.0 train no.42260  loss = 4.44400 avg_loss = 3.91423\n",
      "epoch no.0 train no.42270  loss = 2.74458 avg_loss = 3.88814\n",
      "epoch no.0 train no.42280  loss = 4.21296 avg_loss = 3.90937\n",
      "epoch no.0 train no.42290  loss = 4.56052 avg_loss = 3.98715\n",
      "epoch no.0 train no.42300  loss = 5.62234 avg_loss = 4.01207\n",
      "epoch no.0 train no.42310  loss = 3.69548 avg_loss = 4.01319\n",
      "epoch no.0 train no.42320  loss = 4.08910 avg_loss = 3.99654\n",
      "epoch no.0 train no.42330  loss = 3.20024 avg_loss = 3.94261\n",
      "epoch no.0 train no.42340  loss = 4.04608 avg_loss = 3.92492\n",
      "epoch no.0 train no.42350  loss = 2.39916 avg_loss = 3.84717\n",
      "epoch no.0 train no.42360  loss = 3.39979 avg_loss = 3.83016\n",
      "epoch no.0 train no.42370  loss = 3.86817 avg_loss = 3.88840\n",
      "epoch no.0 train no.42380  loss = 2.26340 avg_loss = 3.87596\n",
      "epoch no.0 train no.42390  loss = 6.39414 avg_loss = 3.93540\n",
      "epoch no.0 train no.42400  loss = 2.73661 avg_loss = 3.96465\n",
      "epoch no.0 train no.42410  loss = 5.40335 avg_loss = 3.92608\n",
      "epoch no.0 train no.42420  loss = 5.22016 avg_loss = 3.92067\n",
      "epoch no.0 train no.42430  loss = 3.25122 avg_loss = 3.89223\n",
      "epoch no.0 train no.42440  loss = 3.17003 avg_loss = 3.92513\n",
      "epoch no.0 train no.42450  loss = 4.62267 avg_loss = 3.88750\n",
      "epoch no.0 train no.42460  loss = 4.66180 avg_loss = 3.88072\n",
      "epoch no.0 train no.42470  loss = 4.49504 avg_loss = 3.89164\n",
      "epoch no.0 train no.42480  loss = 2.91490 avg_loss = 3.88885\n",
      "epoch no.0 train no.42490  loss = 3.46457 avg_loss = 3.96150\n",
      "epoch no.0 train no.42500  loss = 6.40030 avg_loss = 3.97333\n",
      "epoch no.0 train no.42510  loss = 3.34396 avg_loss = 3.99256\n",
      "epoch no.0 train no.42520  loss = 2.66237 avg_loss = 3.96314\n",
      "epoch no.0 train no.42530  loss = 5.34952 avg_loss = 4.09398\n",
      "epoch no.0 train no.42540  loss = 2.77724 avg_loss = 4.13284\n",
      "epoch no.0 train no.42550  loss = 4.16003 avg_loss = 4.12717\n",
      "epoch no.0 train no.42560  loss = 3.12708 avg_loss = 4.18156\n",
      "epoch no.0 train no.42570  loss = 7.35423 avg_loss = 4.19455\n",
      "epoch no.0 train no.42580  loss = 2.13467 avg_loss = 4.12108\n",
      "epoch no.0 train no.42590  loss = 3.70428 avg_loss = 4.11312\n",
      "epoch no.0 train no.42600  loss = 2.22035 avg_loss = 4.11784\n",
      "epoch no.0 train no.42610  loss = 3.97121 avg_loss = 4.15733\n",
      "epoch no.0 train no.42620  loss = 5.53941 avg_loss = 4.11842\n",
      "epoch no.0 train no.42630  loss = 3.56594 avg_loss = 4.10472\n",
      "epoch no.0 train no.42640  loss = 2.73561 avg_loss = 4.03551\n",
      "epoch no.0 train no.42650  loss = 3.34422 avg_loss = 4.05824\n",
      "epoch no.0 train no.42660  loss = 3.73319 avg_loss = 4.02094\n",
      "epoch no.0 train no.42670  loss = 3.43738 avg_loss = 3.99656\n",
      "epoch no.0 train no.42680  loss = 3.28662 avg_loss = 4.02478\n",
      "epoch no.0 train no.42690  loss = 5.35219 avg_loss = 4.06946\n",
      "epoch no.0 train no.42700  loss = 3.62942 avg_loss = 4.05463\n",
      "epoch no.0 train no.42710  loss = 6.71514 avg_loss = 4.10566\n",
      "epoch no.0 train no.42720  loss = 4.50517 avg_loss = 4.05212\n",
      "epoch no.0 train no.42730  loss = 3.81660 avg_loss = 4.05786\n",
      "epoch no.0 train no.42740  loss = 3.68256 avg_loss = 4.06246\n",
      "epoch no.0 train no.42750  loss = 3.07812 avg_loss = 4.05935\n",
      "epoch no.0 train no.42760  loss = 3.36772 avg_loss = 4.07485\n",
      "epoch no.0 train no.42770  loss = 5.81003 avg_loss = 4.06958\n",
      "epoch no.0 train no.42780  loss = 5.27747 avg_loss = 4.08097\n",
      "epoch no.0 train no.42790  loss = 1.81417 avg_loss = 4.06136\n",
      "epoch no.0 train no.42800  loss = 4.01396 avg_loss = 4.05000\n",
      "epoch no.0 train no.42810  loss = 3.34947 avg_loss = 4.07435\n",
      "epoch no.0 train no.42820  loss = 3.12458 avg_loss = 4.00093\n",
      "epoch no.0 train no.42830  loss = 3.72081 avg_loss = 4.00093\n",
      "epoch no.0 train no.42840  loss = 2.61050 avg_loss = 3.99288\n",
      "epoch no.0 train no.42850  loss = 4.88675 avg_loss = 3.99264\n",
      "epoch no.0 train no.42860  loss = 4.39615 avg_loss = 4.05937\n",
      "epoch no.0 train no.42870  loss = 2.80293 avg_loss = 4.02545\n",
      "epoch no.0 train no.42880  loss = 5.12684 avg_loss = 4.03066\n",
      "epoch no.0 train no.42890  loss = 3.19060 avg_loss = 4.00795\n",
      "epoch no.0 train no.42900  loss = 4.98707 avg_loss = 4.02277\n",
      "epoch no.0 train no.42910  loss = 4.95710 avg_loss = 4.03118\n",
      "epoch no.0 train no.42920  loss = 3.41586 avg_loss = 4.05136\n",
      "epoch no.0 train no.42930  loss = 4.01514 avg_loss = 4.06252\n",
      "epoch no.0 train no.42940  loss = 4.25339 avg_loss = 4.09033\n",
      "epoch no.0 train no.42950  loss = 3.13755 avg_loss = 4.03758\n",
      "epoch no.0 train no.42960  loss = 5.04059 avg_loss = 4.07257\n",
      "epoch no.0 train no.42970  loss = 4.45295 avg_loss = 4.04038\n",
      "epoch no.0 train no.42980  loss = 3.03998 avg_loss = 3.97764\n",
      "epoch no.0 train no.42990  loss = 3.43651 avg_loss = 4.02872\n",
      "epoch no.0 train no.43000  loss = 5.84944 avg_loss = 4.04279\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '용', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.0 train no.43010  loss = 3.93788 avg_loss = 4.03992\n",
      "epoch no.0 train no.43020  loss = 2.05586 avg_loss = 4.11557\n",
      "epoch no.0 train no.43030  loss = 6.25092 avg_loss = 4.10435\n",
      "epoch no.0 train no.43040  loss = 2.28870 avg_loss = 4.09384\n",
      "epoch no.0 train no.43050  loss = 2.99259 avg_loss = 4.08823\n",
      "epoch no.0 train no.43060  loss = 3.98383 avg_loss = 4.17081\n",
      "epoch no.0 train no.43070  loss = 4.09176 avg_loss = 4.19506\n",
      "epoch no.0 train no.43080  loss = 3.16695 avg_loss = 4.15210\n",
      "epoch no.0 train no.43090  loss = 2.47440 avg_loss = 4.11198\n",
      "epoch no.0 train no.43100  loss = 3.41091 avg_loss = 4.10965\n",
      "epoch no.0 train no.43110  loss = 2.73538 avg_loss = 4.11635\n",
      "epoch no.0 train no.43120  loss = 4.03159 avg_loss = 4.06449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.43130  loss = 7.29858 avg_loss = 4.07858\n",
      "epoch no.0 train no.43140  loss = 2.93853 avg_loss = 4.05767\n",
      "epoch no.0 train no.43150  loss = 5.25829 avg_loss = 4.08625\n",
      "epoch no.0 train no.43160  loss = 8.61366 avg_loss = 4.10582\n",
      "epoch no.0 train no.43170  loss = 2.27486 avg_loss = 4.00622\n",
      "epoch no.0 train no.43180  loss = 4.16538 avg_loss = 4.01385\n",
      "epoch no.0 train no.43190  loss = 4.15158 avg_loss = 4.00746\n",
      "epoch no.0 train no.43200  loss = 5.11687 avg_loss = 3.99181\n",
      "epoch no.0 train no.43210  loss = 3.54154 avg_loss = 4.00049\n",
      "epoch no.0 train no.43220  loss = 3.00743 avg_loss = 3.98412\n",
      "epoch no.0 train no.43230  loss = 4.28704 avg_loss = 4.03313\n",
      "epoch no.0 train no.43240  loss = 5.22301 avg_loss = 4.00524\n",
      "epoch no.0 train no.43250  loss = 4.18703 avg_loss = 4.01483\n",
      "epoch no.0 train no.43260  loss = 3.26441 avg_loss = 4.06892\n",
      "epoch no.0 train no.43270  loss = 2.16856 avg_loss = 4.07733\n",
      "epoch no.0 train no.43280  loss = 1.98810 avg_loss = 4.05231\n",
      "epoch no.0 train no.43290  loss = 6.27241 avg_loss = 4.05898\n",
      "epoch no.0 train no.43300  loss = 2.96905 avg_loss = 4.05736\n",
      "epoch no.0 train no.43310  loss = 5.04098 avg_loss = 4.00647\n",
      "epoch no.0 train no.43320  loss = 5.55607 avg_loss = 4.07705\n",
      "epoch no.0 train no.43330  loss = 2.41003 avg_loss = 4.00958\n",
      "epoch no.0 train no.43340  loss = 4.11785 avg_loss = 4.11744\n",
      "epoch no.0 train no.43350  loss = 3.03934 avg_loss = 4.15639\n",
      "epoch no.0 train no.43360  loss = 4.83688 avg_loss = 4.09181\n",
      "epoch no.0 train no.43370  loss = 4.20050 avg_loss = 4.08909\n",
      "epoch no.0 train no.43380  loss = 4.08426 avg_loss = 4.07050\n",
      "epoch no.0 train no.43390  loss = 2.40805 avg_loss = 4.03449\n",
      "epoch no.0 train no.43400  loss = 3.18226 avg_loss = 4.02656\n",
      "epoch no.0 train no.43410  loss = 5.60735 avg_loss = 4.01791\n",
      "epoch no.0 train no.43420  loss = 5.51898 avg_loss = 4.04085\n",
      "epoch no.0 train no.43430  loss = 3.95994 avg_loss = 3.98720\n",
      "epoch no.0 train no.43440  loss = 3.78582 avg_loss = 4.01415\n",
      "epoch no.0 train no.43450  loss = 3.90000 avg_loss = 4.02160\n",
      "epoch no.0 train no.43460  loss = 3.72458 avg_loss = 4.09125\n",
      "epoch no.0 train no.43470  loss = 4.41682 avg_loss = 4.07612\n",
      "epoch no.0 train no.43480  loss = 4.24313 avg_loss = 4.11117\n",
      "epoch no.0 train no.43490  loss = 3.84159 avg_loss = 4.04616\n",
      "epoch no.0 train no.43500  loss = 3.31620 avg_loss = 3.97024\n",
      "epoch no.0 train no.43510  loss = 6.52202 avg_loss = 4.02882\n",
      "epoch no.0 train no.43520  loss = 1.71096 avg_loss = 4.05907\n",
      "epoch no.0 train no.43530  loss = 3.97437 avg_loss = 4.13403\n",
      "epoch no.0 train no.43540  loss = 7.16620 avg_loss = 4.15316\n",
      "epoch no.0 train no.43550  loss = 2.11954 avg_loss = 4.14610\n",
      "epoch no.0 train no.43560  loss = 4.11952 avg_loss = 4.08564\n",
      "epoch no.0 train no.43570  loss = 2.11758 avg_loss = 4.05167\n",
      "epoch no.0 train no.43580  loss = 4.04770 avg_loss = 4.04600\n",
      "epoch no.0 train no.43590  loss = 3.82919 avg_loss = 4.05603\n",
      "epoch no.0 train no.43600  loss = 3.30043 avg_loss = 3.99270\n",
      "epoch no.0 train no.43610  loss = 1.70432 avg_loss = 3.97599\n",
      "epoch no.0 train no.43620  loss = 3.62040 avg_loss = 3.99311\n",
      "epoch no.0 train no.43630  loss = 5.17948 avg_loss = 4.03592\n",
      "epoch no.0 train no.43640  loss = 3.15986 avg_loss = 4.01270\n",
      "epoch no.0 train no.43650  loss = 5.01508 avg_loss = 3.98589\n",
      "epoch no.0 train no.43660  loss = 3.60374 avg_loss = 3.98485\n",
      "epoch no.0 train no.43670  loss = 2.68989 avg_loss = 3.94899\n",
      "epoch no.0 train no.43680  loss = 2.60575 avg_loss = 3.94566\n",
      "epoch no.0 train no.43690  loss = 2.85238 avg_loss = 3.97725\n",
      "epoch no.0 train no.43700  loss = 4.46028 avg_loss = 3.99106\n",
      "epoch no.0 train no.43710  loss = 4.75635 avg_loss = 4.03169\n",
      "epoch no.0 train no.43720  loss = 2.53571 avg_loss = 4.06301\n",
      "epoch no.0 train no.43730  loss = 3.72277 avg_loss = 4.05246\n",
      "epoch no.0 train no.43740  loss = 2.29877 avg_loss = 4.03646\n",
      "epoch no.0 train no.43750  loss = 4.10684 avg_loss = 4.05335\n",
      "epoch no.0 train no.43760  loss = 3.61829 avg_loss = 4.06157\n",
      "epoch no.0 train no.43770  loss = 2.57974 avg_loss = 4.14717\n",
      "epoch no.0 train no.43780  loss = 3.61423 avg_loss = 4.12037\n",
      "epoch no.0 train no.43790  loss = 3.67476 avg_loss = 4.11343\n",
      "epoch no.0 train no.43800  loss = 4.00011 avg_loss = 4.12088\n",
      "epoch no.0 train no.43810  loss = 5.04430 avg_loss = 4.14938\n",
      "epoch no.0 train no.43820  loss = 4.49721 avg_loss = 4.13173\n",
      "epoch no.0 train no.43830  loss = 4.11998 avg_loss = 4.08072\n",
      "epoch no.0 train no.43840  loss = 4.27049 avg_loss = 4.05365\n",
      "epoch no.0 train no.43850  loss = 2.56373 avg_loss = 4.02579\n",
      "epoch no.0 train no.43860  loss = 3.36866 avg_loss = 4.06338\n",
      "epoch no.0 train no.43870  loss = 3.90447 avg_loss = 4.02993\n",
      "epoch no.0 train no.43880  loss = 1.87526 avg_loss = 4.01467\n",
      "epoch no.0 train no.43890  loss = 5.88694 avg_loss = 4.09031\n",
      "epoch no.0 train no.43900  loss = 4.18794 avg_loss = 4.10660\n",
      "epoch no.0 train no.43910  loss = 3.59450 avg_loss = 4.09761\n",
      "epoch no.0 train no.43920  loss = 4.74881 avg_loss = 4.06550\n",
      "epoch no.0 train no.43930  loss = 4.51934 avg_loss = 4.04228\n",
      "epoch no.0 train no.43940  loss = 2.92531 avg_loss = 3.95687\n",
      "epoch no.0 train no.43950  loss = 2.65273 avg_loss = 4.01144\n",
      "epoch no.0 train no.43960  loss = 4.94106 avg_loss = 4.00522\n",
      "epoch no.0 train no.43970  loss = 2.81129 avg_loss = 4.01378\n",
      "epoch no.0 train no.43980  loss = 3.95156 avg_loss = 4.01891\n",
      "epoch no.0 train no.43990  loss = 2.96669 avg_loss = 4.02199\n",
      "epoch no.0 train no.44000  loss = 4.57306 avg_loss = 4.06477\n",
      "1\n",
      "to_tokens: ['▁비', '전환', '용', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.0 train no.44010  loss = 3.89331 avg_loss = 4.05619\n",
      "epoch no.0 train no.44020  loss = 1.77751 avg_loss = 3.99576\n",
      "epoch no.0 train no.44030  loss = 2.73662 avg_loss = 3.98014\n",
      "epoch no.0 train no.44040  loss = 2.91422 avg_loss = 3.98572\n",
      "epoch no.0 train no.44050  loss = 3.01278 avg_loss = 3.94695\n",
      "epoch no.0 train no.44060  loss = 2.94010 avg_loss = 4.00948\n",
      "epoch no.0 train no.44070  loss = 2.47715 avg_loss = 3.95977\n",
      "epoch no.0 train no.44080  loss = 1.76829 avg_loss = 3.91254\n",
      "epoch no.0 train no.44090  loss = 2.70733 avg_loss = 3.94345\n",
      "epoch no.0 train no.44100  loss = 4.21695 avg_loss = 3.97764\n",
      "epoch no.0 train no.44110  loss = 3.89016 avg_loss = 4.06937\n",
      "epoch no.0 train no.44120  loss = 2.83494 avg_loss = 4.06526\n",
      "epoch no.0 train no.44130  loss = 4.42456 avg_loss = 4.05668\n",
      "epoch no.0 train no.44140  loss = 2.60903 avg_loss = 4.06437\n",
      "epoch no.0 train no.44150  loss = 3.97385 avg_loss = 4.06046\n",
      "epoch no.0 train no.44160  loss = 5.63187 avg_loss = 4.09056\n",
      "epoch no.0 train no.44170  loss = 5.05897 avg_loss = 4.09334\n",
      "epoch no.0 train no.44180  loss = 5.37500 avg_loss = 4.10256\n",
      "epoch no.0 train no.44190  loss = 3.43280 avg_loss = 4.09841\n",
      "epoch no.0 train no.44200  loss = 4.41152 avg_loss = 4.05818\n",
      "epoch no.0 train no.44210  loss = 3.22059 avg_loss = 4.04970\n",
      "epoch no.0 train no.44220  loss = 4.38888 avg_loss = 4.06131\n",
      "epoch no.0 train no.44230  loss = 2.64015 avg_loss = 4.01222\n",
      "epoch no.0 train no.44240  loss = 3.63522 avg_loss = 4.03328\n",
      "epoch no.0 train no.44250  loss = 2.58022 avg_loss = 4.00977\n",
      "epoch no.0 train no.44260  loss = 2.71664 avg_loss = 4.02013\n",
      "epoch no.0 train no.44270  loss = 5.00773 avg_loss = 4.08116\n",
      "epoch no.0 train no.44280  loss = 3.01317 avg_loss = 4.03766\n",
      "epoch no.0 train no.44290  loss = 6.81881 avg_loss = 4.07390\n",
      "epoch no.0 train no.44300  loss = 3.59058 avg_loss = 4.01017\n",
      "epoch no.0 train no.44310  loss = 2.54935 avg_loss = 3.99722\n",
      "epoch no.0 train no.44320  loss = 3.76409 avg_loss = 3.95875\n",
      "epoch no.0 train no.44330  loss = 4.78167 avg_loss = 4.01721\n",
      "epoch no.0 train no.44340  loss = 3.84255 avg_loss = 4.02973\n",
      "epoch no.0 train no.44350  loss = 6.44576 avg_loss = 4.03404\n",
      "epoch no.0 train no.44360  loss = 3.19150 avg_loss = 3.98709\n",
      "epoch no.0 train no.44370  loss = 3.58998 avg_loss = 3.98689\n",
      "epoch no.0 train no.44380  loss = 2.82362 avg_loss = 4.04699\n",
      "epoch no.0 train no.44390  loss = 3.13444 avg_loss = 4.05516\n",
      "epoch no.0 train no.44400  loss = 6.62331 avg_loss = 4.16890\n",
      "epoch no.0 train no.44410  loss = 1.77740 avg_loss = 4.08666\n",
      "epoch no.0 train no.44420  loss = 4.11387 avg_loss = 4.06696\n",
      "epoch no.0 train no.44430  loss = 4.69652 avg_loss = 4.11942\n",
      "epoch no.0 train no.44440  loss = 3.64170 avg_loss = 4.09279\n",
      "epoch no.0 train no.44450  loss = 5.62044 avg_loss = 4.06196\n",
      "epoch no.0 train no.44460  loss = 6.46653 avg_loss = 4.06611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.44470  loss = 5.63782 avg_loss = 4.03929\n",
      "epoch no.0 train no.44480  loss = 3.66516 avg_loss = 4.03572\n",
      "epoch no.0 train no.44490  loss = 4.18683 avg_loss = 4.05404\n",
      "epoch no.0 train no.44500  loss = 6.67202 avg_loss = 4.09899\n",
      "epoch no.0 train no.44510  loss = 4.13817 avg_loss = 4.10547\n",
      "epoch no.0 train no.44520  loss = 3.02251 avg_loss = 4.09222\n",
      "epoch no.0 train no.44530  loss = 2.91609 avg_loss = 4.02230\n",
      "epoch no.0 train no.44540  loss = 2.84752 avg_loss = 4.00770\n",
      "epoch no.0 train no.44550  loss = 2.94060 avg_loss = 3.95258\n",
      "epoch no.0 train no.44560  loss = 3.55518 avg_loss = 3.92749\n",
      "epoch no.0 train no.44570  loss = 3.66616 avg_loss = 3.91892\n",
      "epoch no.0 train no.44580  loss = 5.07613 avg_loss = 3.95615\n",
      "epoch no.0 train no.44590  loss = 6.62898 avg_loss = 3.94648\n",
      "epoch no.0 train no.44600  loss = 5.27356 avg_loss = 4.00635\n",
      "epoch no.0 train no.44610  loss = 2.80319 avg_loss = 3.99169\n",
      "epoch no.0 train no.44620  loss = 4.74868 avg_loss = 4.00184\n",
      "epoch no.0 train no.44630  loss = 3.66001 avg_loss = 3.96605\n",
      "epoch no.0 train no.44640  loss = 2.98338 avg_loss = 3.93799\n",
      "epoch no.0 train no.44650  loss = 5.63789 avg_loss = 3.97857\n",
      "epoch no.0 train no.44660  loss = 3.32209 avg_loss = 3.93193\n",
      "epoch no.0 train no.44670  loss = 2.41072 avg_loss = 3.93050\n",
      "epoch no.0 train no.44680  loss = 2.71575 avg_loss = 3.97013\n",
      "epoch no.0 train no.44690  loss = 4.33826 avg_loss = 3.92298\n",
      "epoch no.0 train no.44700  loss = 4.87117 avg_loss = 3.89671\n",
      "epoch no.0 train no.44710  loss = 2.06970 avg_loss = 3.92021\n",
      "epoch no.0 train no.44720  loss = 5.11839 avg_loss = 3.96769\n",
      "epoch no.0 train no.44730  loss = 5.78912 avg_loss = 3.98554\n",
      "epoch no.0 train no.44740  loss = 3.01244 avg_loss = 3.99821\n",
      "epoch no.0 train no.44750  loss = 5.78114 avg_loss = 3.95924\n",
      "epoch no.0 train no.44760  loss = 3.84822 avg_loss = 3.98718\n",
      "epoch no.0 train no.44770  loss = 1.92510 avg_loss = 3.95305\n",
      "epoch no.0 train no.44780  loss = 4.62053 avg_loss = 3.94248\n",
      "epoch no.0 train no.44790  loss = 5.84260 avg_loss = 3.96823\n",
      "epoch no.0 train no.44800  loss = 4.47308 avg_loss = 3.99630\n",
      "epoch no.0 train no.44810  loss = 6.64465 avg_loss = 3.98109\n",
      "epoch no.0 train no.44820  loss = 3.09584 avg_loss = 4.02006\n",
      "epoch no.0 train no.44830  loss = 4.84958 avg_loss = 3.96749\n",
      "epoch no.0 train no.44840  loss = 2.66059 avg_loss = 3.97090\n",
      "epoch no.0 train no.44850  loss = 2.20348 avg_loss = 3.93592\n",
      "epoch no.0 train no.44860  loss = 3.60013 avg_loss = 3.95687\n",
      "epoch no.0 train no.44870  loss = 2.22564 avg_loss = 3.98335\n",
      "epoch no.0 train no.44880  loss = 3.44564 avg_loss = 3.96166\n",
      "epoch no.0 train no.44890  loss = 4.08792 avg_loss = 3.95362\n",
      "epoch no.0 train no.44900  loss = 3.11408 avg_loss = 3.91189\n",
      "epoch no.0 train no.44910  loss = 3.04138 avg_loss = 3.92610\n",
      "epoch no.0 train no.44920  loss = 1.95512 avg_loss = 3.90074\n",
      "epoch no.0 train no.44930  loss = 1.80714 avg_loss = 3.90229\n",
      "epoch no.0 train no.44940  loss = 6.02693 avg_loss = 3.98106\n",
      "epoch no.0 train no.44950  loss = 3.60299 avg_loss = 3.95825\n",
      "epoch no.0 train no.44960  loss = 3.40468 avg_loss = 4.03604\n",
      "epoch no.0 train no.44970  loss = 2.69591 avg_loss = 4.05698\n",
      "epoch no.0 train no.44980  loss = 2.43868 avg_loss = 4.09234\n",
      "epoch no.0 train no.44990  loss = 2.31358 avg_loss = 4.07638\n",
      "epoch no.0 train no.45000  loss = 4.40753 avg_loss = 4.06613\n",
      "2\n",
      "to_tokens: ['▁비', '전환', '용', '때', '</s>']\n",
      "기분전환할때</s>\n",
      "epoch no.0 train no.45010  loss = 2.91435 avg_loss = 4.00365\n",
      "epoch no.0 train no.45020  loss = 2.58367 avg_loss = 3.99716\n",
      "epoch no.0 train no.45030  loss = 4.88223 avg_loss = 3.98062\n",
      "epoch no.0 train no.45040  loss = 2.86099 avg_loss = 3.94016\n",
      "epoch no.0 train no.45050  loss = 4.45086 avg_loss = 3.94164\n",
      "epoch no.0 train no.45060  loss = 3.02542 avg_loss = 3.94797\n",
      "epoch no.0 train no.45070  loss = 4.63227 avg_loss = 3.98899\n",
      "epoch no.0 train no.45080  loss = 3.89906 avg_loss = 4.00169\n",
      "epoch no.0 train no.45090  loss = 4.44165 avg_loss = 4.00424\n",
      "epoch no.0 train no.45100  loss = 4.57060 avg_loss = 4.04438\n",
      "epoch no.0 train no.45110  loss = 2.71579 avg_loss = 4.06392\n",
      "epoch no.0 train no.45120  loss = 3.47486 avg_loss = 4.04299\n",
      "epoch no.0 train no.45130  loss = 6.21084 avg_loss = 4.08407\n",
      "epoch no.0 train no.45140  loss = 3.80616 avg_loss = 4.04203\n",
      "epoch no.0 train no.45150  loss = 3.97964 avg_loss = 4.10912\n",
      "epoch no.0 train no.45160  loss = 3.87738 avg_loss = 4.10240\n",
      "epoch no.0 train no.45170  loss = 2.78791 avg_loss = 4.12162\n",
      "epoch no.0 train no.45180  loss = 4.45372 avg_loss = 4.06120\n",
      "epoch no.0 train no.45190  loss = 3.39487 avg_loss = 4.04165\n",
      "epoch no.0 train no.45200  loss = 2.22670 avg_loss = 4.04313\n",
      "epoch no.0 train no.45210  loss = 3.32805 avg_loss = 4.05909\n",
      "epoch no.0 train no.45220  loss = 5.44976 avg_loss = 4.02404\n",
      "epoch no.0 train no.45230  loss = 6.03276 avg_loss = 4.00770\n",
      "epoch no.0 train no.45240  loss = 3.28002 avg_loss = 4.00409\n",
      "epoch no.0 train no.45250  loss = 3.74930 avg_loss = 3.95558\n",
      "epoch no.0 train no.45260  loss = 3.27443 avg_loss = 3.98332\n",
      "epoch no.0 train no.45270  loss = 6.12688 avg_loss = 4.04687\n",
      "epoch no.0 train no.45280  loss = 6.20206 avg_loss = 4.06765\n",
      "epoch no.0 train no.45290  loss = 3.27912 avg_loss = 4.05778\n",
      "epoch no.0 train no.45300  loss = 2.70232 avg_loss = 3.99594\n",
      "epoch no.0 train no.45310  loss = 3.82199 avg_loss = 4.00541\n",
      "epoch no.0 train no.45320  loss = 4.24517 avg_loss = 3.99236\n",
      "epoch no.0 train no.45330  loss = 4.21969 avg_loss = 3.96258\n",
      "epoch no.0 train no.45340  loss = 6.88058 avg_loss = 4.04949\n",
      "epoch no.0 train no.45350  loss = 3.92344 avg_loss = 4.05446\n",
      "epoch no.0 train no.45360  loss = 2.25334 avg_loss = 3.96621\n",
      "epoch no.0 train no.45370  loss = 2.82905 avg_loss = 3.95930\n",
      "epoch no.0 train no.45380  loss = 4.47351 avg_loss = 3.94355\n",
      "epoch no.0 train no.45390  loss = 3.31167 avg_loss = 3.92668\n",
      "epoch no.0 train no.45400  loss = 3.41293 avg_loss = 3.94475\n",
      "epoch no.0 train no.45410  loss = 4.82227 avg_loss = 3.95651\n",
      "epoch no.0 train no.45420  loss = 3.03673 avg_loss = 3.91613\n",
      "epoch no.0 train no.45430  loss = 4.05865 avg_loss = 3.91364\n",
      "epoch no.0 train no.45440  loss = 3.67083 avg_loss = 4.01892\n",
      "epoch no.0 train no.45450  loss = 4.10077 avg_loss = 4.02634\n",
      "epoch no.0 train no.45460  loss = 2.83411 avg_loss = 4.09866\n",
      "epoch no.0 train no.45470  loss = 2.63181 avg_loss = 4.06784\n",
      "epoch no.0 train no.45480  loss = 7.26674 avg_loss = 4.11681\n",
      "epoch no.0 train no.45490  loss = 3.00485 avg_loss = 4.08740\n",
      "epoch no.0 train no.45500  loss = 5.85026 avg_loss = 4.09744\n",
      "epoch no.0 train no.45510  loss = 2.95318 avg_loss = 4.02631\n",
      "epoch no.0 train no.45520  loss = 4.83738 avg_loss = 4.06737\n",
      "epoch no.0 train no.45530  loss = 2.83033 avg_loss = 4.05970\n",
      "epoch no.0 train no.45540  loss = 4.61614 avg_loss = 4.04239\n",
      "epoch no.0 train no.45550  loss = 3.07502 avg_loss = 3.99279\n",
      "epoch no.0 train no.45560  loss = 3.85510 avg_loss = 4.05441\n",
      "epoch no.0 train no.45570  loss = 3.25170 avg_loss = 4.01820\n",
      "epoch no.0 train no.45580  loss = 3.50405 avg_loss = 4.04322\n",
      "epoch no.0 train no.45590  loss = 1.78047 avg_loss = 4.04519\n",
      "epoch no.0 train no.45600  loss = 4.39810 avg_loss = 4.10606\n",
      "epoch no.0 train no.45610  loss = 1.45069 avg_loss = 4.04122\n",
      "epoch no.0 train no.45620  loss = 6.34534 avg_loss = 4.13050\n",
      "epoch no.0 train no.45630  loss = 3.62226 avg_loss = 4.08893\n",
      "epoch no.0 train no.45640  loss = 4.89474 avg_loss = 4.07676\n",
      "epoch no.0 train no.45650  loss = 2.91111 avg_loss = 4.15700\n",
      "epoch no.0 train no.45660  loss = 7.91375 avg_loss = 4.21750\n",
      "epoch no.0 train no.45670  loss = 5.64531 avg_loss = 4.17521\n",
      "epoch no.0 train no.45680  loss = 4.53211 avg_loss = 4.13383\n",
      "epoch no.0 train no.45690  loss = 5.35251 avg_loss = 4.10840\n",
      "epoch no.0 train no.45700  loss = 3.70836 avg_loss = 4.03217\n",
      "epoch no.0 train no.45710  loss = 1.76689 avg_loss = 4.01917\n",
      "epoch no.0 train no.45720  loss = 3.26634 avg_loss = 4.05106\n",
      "epoch no.0 train no.45730  loss = 2.81554 avg_loss = 4.10046\n",
      "epoch no.0 train no.45740  loss = 3.02500 avg_loss = 4.04119\n",
      "epoch no.0 train no.45750  loss = 3.02203 avg_loss = 4.06292\n",
      "epoch no.0 train no.45760  loss = 2.02880 avg_loss = 3.99797\n",
      "epoch no.0 train no.45770  loss = 3.82372 avg_loss = 3.96478\n",
      "epoch no.0 train no.45780  loss = 3.61840 avg_loss = 3.97087\n",
      "epoch no.0 train no.45790  loss = 2.81180 avg_loss = 3.90232\n",
      "epoch no.0 train no.45800  loss = 4.61669 avg_loss = 3.94108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.45810  loss = 3.76290 avg_loss = 3.91984\n",
      "epoch no.0 train no.45820  loss = 2.88591 avg_loss = 3.89360\n",
      "epoch no.0 train no.45830  loss = 4.99882 avg_loss = 3.89842\n",
      "epoch no.0 train no.45840  loss = 2.21825 avg_loss = 3.90266\n",
      "epoch no.0 train no.45850  loss = 3.72613 avg_loss = 3.99567\n",
      "epoch no.0 train no.45860  loss = 3.18699 avg_loss = 4.00667\n",
      "epoch no.0 train no.45870  loss = 3.02491 avg_loss = 4.00632\n",
      "epoch no.0 train no.45880  loss = 2.99260 avg_loss = 3.97594\n",
      "epoch no.0 train no.45890  loss = 2.01490 avg_loss = 4.01229\n",
      "epoch no.0 train no.45900  loss = 5.07261 avg_loss = 3.96254\n",
      "epoch no.0 train no.45910  loss = 4.26317 avg_loss = 4.02338\n",
      "epoch no.0 train no.45920  loss = 3.82457 avg_loss = 3.98752\n",
      "epoch no.0 train no.45930  loss = 2.41793 avg_loss = 4.05241\n",
      "epoch no.0 train no.45940  loss = 3.74856 avg_loss = 4.10538\n",
      "epoch no.0 train no.45950  loss = 3.41465 avg_loss = 4.09753\n",
      "epoch no.0 train no.45960  loss = 3.31424 avg_loss = 4.12417\n",
      "epoch no.0 train no.45970  loss = 2.52334 avg_loss = 4.07033\n",
      "epoch no.0 train no.45980  loss = 5.37827 avg_loss = 4.09689\n",
      "epoch no.0 train no.45990  loss = 5.32080 avg_loss = 4.08602\n",
      "epoch no.0 train no.46000  loss = 3.90241 avg_loss = 4.07289\n",
      "3\n",
      "to_tokens: ['▁비', '▁좋아', '을', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.0 train no.46010  loss = 2.58132 avg_loss = 4.03260\n",
      "epoch no.0 train no.46020  loss = 3.51278 avg_loss = 4.04374\n",
      "epoch no.0 train no.46030  loss = 4.26747 avg_loss = 4.05516\n",
      "epoch no.0 train no.46040  loss = 5.66012 avg_loss = 4.11282\n",
      "epoch no.0 train no.46050  loss = 4.84778 avg_loss = 4.17484\n",
      "epoch no.0 train no.46060  loss = 3.86396 avg_loss = 4.15948\n",
      "epoch no.0 train no.46070  loss = 3.85844 avg_loss = 4.11890\n",
      "epoch no.0 train no.46080  loss = 4.93020 avg_loss = 4.18239\n",
      "epoch no.0 train no.46090  loss = 3.67592 avg_loss = 4.16207\n",
      "epoch no.0 train no.46100  loss = 4.81943 avg_loss = 4.16679\n",
      "epoch no.0 train no.46110  loss = 3.77007 avg_loss = 4.21460\n",
      "epoch no.0 train no.46120  loss = 4.02110 avg_loss = 4.14088\n",
      "epoch no.0 train no.46130  loss = 3.58829 avg_loss = 4.19886\n",
      "epoch no.0 train no.46140  loss = 2.93314 avg_loss = 4.20437\n",
      "epoch no.0 train no.46150  loss = 2.71531 avg_loss = 4.18620\n",
      "epoch no.0 train no.46160  loss = 5.56488 avg_loss = 4.15731\n",
      "epoch no.0 train no.46170  loss = 2.51398 avg_loss = 4.13254\n",
      "epoch no.0 train no.46180  loss = 3.67042 avg_loss = 4.10292\n",
      "epoch no.0 train no.46190  loss = 3.84305 avg_loss = 4.08017\n",
      "epoch no.0 train no.46200  loss = 3.92040 avg_loss = 4.09256\n",
      "epoch no.0 train no.46210  loss = 2.72598 avg_loss = 4.07662\n",
      "epoch no.0 train no.46220  loss = 2.24139 avg_loss = 4.07417\n",
      "epoch no.0 train no.46230  loss = 4.69494 avg_loss = 4.08023\n",
      "epoch no.0 train no.46240  loss = 2.74414 avg_loss = 4.05953\n",
      "epoch no.0 train no.46250  loss = 2.93646 avg_loss = 4.10059\n",
      "epoch no.0 train no.46260  loss = 4.01429 avg_loss = 4.08541\n",
      "epoch no.0 train no.46270  loss = 3.51419 avg_loss = 4.06371\n",
      "epoch no.0 train no.46280  loss = 2.83278 avg_loss = 4.03056\n",
      "epoch no.0 train no.46290  loss = 4.26084 avg_loss = 4.01040\n",
      "epoch no.0 train no.46300  loss = 3.92608 avg_loss = 3.97742\n",
      "epoch no.0 train no.46310  loss = 3.65070 avg_loss = 3.92402\n",
      "epoch no.0 train no.46320  loss = 4.94315 avg_loss = 3.92021\n",
      "epoch no.0 train no.46330  loss = 3.96724 avg_loss = 3.91891\n",
      "epoch no.0 train no.46340  loss = 3.84352 avg_loss = 4.01787\n",
      "epoch no.0 train no.46350  loss = 2.57036 avg_loss = 4.01914\n",
      "epoch no.0 train no.46360  loss = 4.52183 avg_loss = 4.02363\n",
      "epoch no.0 train no.46370  loss = 3.12727 avg_loss = 4.01939\n",
      "epoch no.0 train no.46380  loss = 5.50029 avg_loss = 3.98156\n",
      "epoch no.0 train no.46390  loss = 4.07079 avg_loss = 3.95835\n",
      "epoch no.0 train no.46400  loss = 3.86911 avg_loss = 3.89771\n",
      "epoch no.0 train no.46410  loss = 5.43461 avg_loss = 3.87139\n",
      "epoch no.0 train no.46420  loss = 4.68673 avg_loss = 3.90116\n",
      "epoch no.0 train no.46430  loss = 3.12405 avg_loss = 3.90461\n",
      "epoch no.0 train no.46440  loss = 5.34199 avg_loss = 3.87263\n",
      "epoch no.0 train no.46450  loss = 3.08667 avg_loss = 3.82947\n",
      "epoch no.0 train no.46460  loss = 4.22037 avg_loss = 3.84618\n",
      "epoch no.0 train no.46470  loss = 4.88761 avg_loss = 3.83499\n",
      "epoch no.0 train no.46480  loss = 5.38884 avg_loss = 3.88655\n",
      "epoch no.0 train no.46490  loss = 2.39392 avg_loss = 3.90689\n",
      "epoch no.0 train no.46500  loss = 3.26301 avg_loss = 3.89882\n",
      "epoch no.0 train no.46510  loss = 5.04353 avg_loss = 3.94997\n",
      "epoch no.0 train no.46520  loss = 2.62691 avg_loss = 3.93488\n",
      "epoch no.0 train no.46530  loss = 4.82547 avg_loss = 3.95306\n",
      "epoch no.0 train no.46540  loss = 3.55328 avg_loss = 3.99614\n",
      "epoch no.0 train no.46550  loss = 3.34568 avg_loss = 3.95683\n",
      "epoch no.0 train no.46560  loss = 3.45124 avg_loss = 3.92421\n",
      "epoch no.0 train no.46570  loss = 4.52138 avg_loss = 3.96546\n",
      "epoch no.0 train no.46580  loss = 3.65473 avg_loss = 3.94591\n",
      "epoch no.0 train no.46590  loss = 3.95482 avg_loss = 3.97370\n",
      "epoch no.0 train no.46600  loss = 4.08706 avg_loss = 3.93269\n",
      "epoch no.0 train no.46610  loss = 5.85718 avg_loss = 3.96309\n",
      "epoch no.0 train no.46620  loss = 3.60102 avg_loss = 3.92997\n",
      "epoch no.0 train no.46630  loss = 2.24001 avg_loss = 3.91506\n",
      "epoch no.0 train no.46640  loss = 2.70455 avg_loss = 3.93431\n",
      "epoch no.0 train no.46650  loss = 2.40623 avg_loss = 3.96984\n",
      "epoch no.0 train no.46660  loss = 4.29498 avg_loss = 3.93580\n",
      "epoch no.0 train no.46670  loss = 4.44932 avg_loss = 3.93813\n",
      "epoch no.0 train no.46680  loss = 3.86517 avg_loss = 3.99583\n",
      "epoch no.0 train no.46690  loss = 3.00414 avg_loss = 3.99374\n",
      "epoch no.0 train no.46700  loss = 5.29882 avg_loss = 4.09690\n",
      "epoch no.0 train no.46710  loss = 3.34176 avg_loss = 4.04205\n",
      "epoch no.0 train no.46720  loss = 7.07442 avg_loss = 4.09224\n",
      "epoch no.0 train no.46730  loss = 4.92595 avg_loss = 4.14118\n",
      "epoch no.0 train no.46740  loss = 4.69416 avg_loss = 4.09320\n",
      "epoch no.0 train no.46750  loss = 2.93169 avg_loss = 4.11956\n",
      "epoch no.0 train no.46760  loss = 4.59982 avg_loss = 4.13280\n",
      "epoch no.0 train no.46770  loss = 6.02469 avg_loss = 4.17667\n",
      "epoch no.0 train no.46780  loss = 2.43998 avg_loss = 4.20993\n",
      "epoch no.0 train no.46790  loss = 5.35018 avg_loss = 4.19761\n",
      "epoch no.0 train no.46800  loss = 6.88854 avg_loss = 4.18352\n",
      "epoch no.0 train no.46810  loss = 3.52917 avg_loss = 4.17272\n",
      "epoch no.0 train no.46820  loss = 3.70610 avg_loss = 4.11715\n",
      "epoch no.0 train no.46830  loss = 2.97205 avg_loss = 4.05203\n",
      "epoch no.0 train no.46840  loss = 3.21819 avg_loss = 4.04653\n",
      "epoch no.0 train no.46850  loss = 4.37675 avg_loss = 4.07578\n",
      "epoch no.0 train no.46860  loss = 4.85945 avg_loss = 4.10333\n",
      "epoch no.0 train no.46870  loss = 2.56521 avg_loss = 4.03196\n",
      "epoch no.0 train no.46880  loss = 8.26166 avg_loss = 4.07980\n",
      "epoch no.0 train no.46890  loss = 2.37943 avg_loss = 4.01277\n",
      "epoch no.0 train no.46900  loss = 7.63517 avg_loss = 4.00977\n",
      "epoch no.0 train no.46910  loss = 2.10376 avg_loss = 4.01416\n",
      "epoch no.0 train no.46920  loss = 2.84588 avg_loss = 3.95698\n",
      "epoch no.0 train no.46930  loss = 2.79914 avg_loss = 3.96895\n",
      "epoch no.0 train no.46940  loss = 2.21462 avg_loss = 4.03706\n",
      "epoch no.0 train no.46950  loss = 2.84293 avg_loss = 4.04224\n",
      "epoch no.0 train no.46960  loss = 2.19983 avg_loss = 3.93792\n",
      "epoch no.0 train no.46970  loss = 3.06898 avg_loss = 3.89471\n",
      "epoch no.0 train no.46980  loss = 3.00838 avg_loss = 3.91491\n",
      "epoch no.0 train no.46990  loss = 3.83906 avg_loss = 3.89294\n",
      "epoch no.0 train no.47000  loss = 4.28866 avg_loss = 3.92032\n",
      "6\n",
      "to_tokens: ['▁비', '전환', '을', '때', '▁듣는', '▁노래', '▁노래', '</s>', '</s>']\n",
      "기분전환 할때 듣는 신나는 팝송</s>\n",
      "epoch no.0 train no.47010  loss = 5.06794 avg_loss = 3.93373\n",
      "epoch no.0 train no.47020  loss = 8.04558 avg_loss = 3.97839\n",
      "epoch no.0 train no.47030  loss = 2.34708 avg_loss = 3.94509\n",
      "epoch no.0 train no.47040  loss = 2.70863 avg_loss = 3.90837\n",
      "epoch no.0 train no.47050  loss = 2.87296 avg_loss = 3.89294\n",
      "epoch no.0 train no.47060  loss = 2.87985 avg_loss = 4.00378\n",
      "epoch no.0 train no.47070  loss = 2.11879 avg_loss = 4.04791\n",
      "epoch no.0 train no.47080  loss = 3.41479 avg_loss = 4.07404\n",
      "epoch no.0 train no.47090  loss = 2.53668 avg_loss = 4.04060\n",
      "epoch no.0 train no.47100  loss = 7.39377 avg_loss = 4.08216\n",
      "epoch no.0 train no.47110  loss = 3.22501 avg_loss = 4.13514\n",
      "epoch no.0 train no.47120  loss = 4.82466 avg_loss = 4.17041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.47130  loss = 5.51509 avg_loss = 4.19085\n",
      "epoch no.0 train no.47140  loss = 3.59533 avg_loss = 4.20281\n",
      "epoch no.0 train no.47150  loss = 4.84550 avg_loss = 4.24904\n",
      "epoch no.0 train no.47160  loss = 5.17374 avg_loss = 4.25911\n",
      "epoch no.0 train no.47170  loss = 5.05779 avg_loss = 4.29413\n",
      "epoch no.0 train no.47180  loss = 4.57746 avg_loss = 4.18500\n",
      "epoch no.0 train no.47190  loss = 4.78242 avg_loss = 4.15104\n",
      "epoch no.0 train no.47200  loss = 3.08758 avg_loss = 4.13232\n",
      "epoch no.0 train no.47210  loss = 4.67668 avg_loss = 4.09484\n",
      "epoch no.0 train no.47220  loss = 2.80103 avg_loss = 4.08575\n",
      "epoch no.0 train no.47230  loss = 6.92494 avg_loss = 4.06502\n",
      "epoch no.0 train no.47240  loss = 3.37757 avg_loss = 4.10265\n",
      "epoch no.0 train no.47250  loss = 3.27997 avg_loss = 4.10771\n",
      "epoch no.0 train no.47260  loss = 2.86425 avg_loss = 4.05207\n",
      "epoch no.0 train no.47270  loss = 4.15143 avg_loss = 3.98963\n",
      "epoch no.0 train no.47280  loss = 3.31617 avg_loss = 4.04415\n",
      "epoch no.0 train no.47290  loss = 2.85861 avg_loss = 4.06976\n",
      "epoch no.0 train no.47300  loss = 4.72246 avg_loss = 4.11089\n",
      "epoch no.0 train no.47310  loss = 4.19419 avg_loss = 4.16601\n",
      "epoch no.0 train no.47320  loss = 4.05831 avg_loss = 4.12101\n",
      "epoch no.0 train no.47330  loss = 3.27330 avg_loss = 4.15473\n",
      "epoch no.0 train no.47340  loss = 4.28544 avg_loss = 4.19110\n",
      "epoch no.0 train no.47350  loss = 4.70018 avg_loss = 4.12224\n",
      "epoch no.0 train no.47360  loss = 2.63803 avg_loss = 4.06169\n",
      "epoch no.0 train no.47370  loss = 4.36911 avg_loss = 4.04511\n",
      "epoch no.0 train no.47380  loss = 4.64850 avg_loss = 4.01799\n",
      "epoch no.0 train no.47390  loss = 3.97086 avg_loss = 4.03361\n",
      "epoch no.0 train no.47400  loss = 3.96212 avg_loss = 4.01940\n",
      "epoch no.0 train no.47410  loss = 2.94565 avg_loss = 4.00975\n",
      "epoch no.0 train no.47420  loss = 3.10452 avg_loss = 4.01477\n",
      "epoch no.0 train no.47430  loss = 3.27991 avg_loss = 4.02320\n",
      "epoch no.0 train no.47440  loss = 5.81730 avg_loss = 4.07770\n",
      "epoch no.0 train no.47450  loss = 6.88859 avg_loss = 4.20064\n",
      "epoch no.0 train no.47460  loss = 3.39140 avg_loss = 4.19337\n",
      "epoch no.0 train no.47470  loss = 4.57089 avg_loss = 4.16316\n",
      "epoch no.0 train no.47480  loss = 6.14814 avg_loss = 4.16535\n",
      "epoch no.0 train no.47490  loss = 3.61976 avg_loss = 4.11325\n",
      "epoch no.0 train no.47500  loss = 2.31584 avg_loss = 4.08517\n",
      "epoch no.0 train no.47510  loss = 3.38476 avg_loss = 4.05656\n",
      "epoch no.0 train no.47520  loss = 7.85288 avg_loss = 4.10563\n",
      "epoch no.0 train no.47530  loss = 3.61333 avg_loss = 4.07898\n",
      "epoch no.0 train no.47540  loss = 4.55966 avg_loss = 4.04684\n",
      "epoch no.0 train no.47550  loss = 4.09605 avg_loss = 4.06294\n",
      "epoch no.0 train no.47560  loss = 3.76629 avg_loss = 4.01536\n",
      "epoch no.0 train no.47570  loss = 5.04386 avg_loss = 4.00568\n",
      "epoch no.0 train no.47580  loss = 5.45237 avg_loss = 4.07605\n",
      "epoch no.0 train no.47590  loss = 4.27153 avg_loss = 4.11516\n",
      "epoch no.0 train no.47600  loss = 5.48320 avg_loss = 4.15019\n",
      "epoch no.0 train no.47610  loss = 2.76672 avg_loss = 4.08426\n",
      "epoch no.0 train no.47620  loss = 6.14710 avg_loss = 4.08794\n",
      "epoch no.0 train no.47630  loss = 3.56001 avg_loss = 4.08155\n",
      "epoch no.0 train no.47640  loss = 3.16160 avg_loss = 4.09331\n",
      "epoch no.0 train no.47650  loss = 3.45519 avg_loss = 4.06415\n",
      "epoch no.0 train no.47660  loss = 3.46536 avg_loss = 3.98549\n",
      "epoch no.0 train no.47670  loss = 3.83683 avg_loss = 3.96737\n",
      "epoch no.0 train no.47680  loss = 2.90028 avg_loss = 3.94221\n",
      "epoch no.0 train no.47690  loss = 4.32256 avg_loss = 3.91881\n",
      "epoch no.0 train no.47700  loss = 3.37587 avg_loss = 3.89044\n",
      "epoch no.0 train no.47710  loss = 2.31753 avg_loss = 3.91845\n",
      "epoch no.0 train no.47720  loss = 2.32260 avg_loss = 3.89965\n",
      "epoch no.0 train no.47730  loss = 2.89383 avg_loss = 3.91052\n",
      "epoch no.0 train no.47740  loss = 4.15777 avg_loss = 3.88810\n",
      "epoch no.0 train no.47750  loss = 3.49052 avg_loss = 3.86471\n",
      "epoch no.0 train no.47760  loss = 4.98609 avg_loss = 3.83818\n",
      "epoch no.0 train no.47770  loss = 2.74280 avg_loss = 3.82552\n",
      "epoch no.0 train no.47780  loss = 5.19334 avg_loss = 3.88602\n",
      "epoch no.0 train no.47790  loss = 5.33715 avg_loss = 3.92246\n",
      "epoch no.0 train no.47800  loss = 4.95530 avg_loss = 3.95051\n",
      "epoch no.0 train no.47810  loss = 4.23414 avg_loss = 3.90657\n",
      "epoch no.0 train no.47820  loss = 6.31293 avg_loss = 3.94870\n",
      "epoch no.0 train no.47830  loss = 2.76369 avg_loss = 3.93376\n",
      "epoch no.0 train no.47840  loss = 2.35458 avg_loss = 3.95302\n",
      "epoch no.0 train no.47850  loss = 5.62964 avg_loss = 3.91567\n",
      "epoch no.0 train no.47860  loss = 2.91236 avg_loss = 3.88942\n",
      "epoch no.0 train no.47870  loss = 2.49868 avg_loss = 3.92662\n",
      "epoch no.0 train no.47880  loss = 5.82997 avg_loss = 3.98890\n",
      "epoch no.0 train no.47890  loss = 5.48488 avg_loss = 4.03672\n",
      "epoch no.0 train no.47900  loss = 5.71556 avg_loss = 4.06143\n",
      "epoch no.0 train no.47910  loss = 5.49505 avg_loss = 4.08599\n",
      "epoch no.0 train no.47920  loss = 3.61254 avg_loss = 4.09165\n",
      "epoch no.0 train no.47930  loss = 6.78418 avg_loss = 4.11480\n",
      "epoch no.0 train no.47940  loss = 3.91073 avg_loss = 4.12989\n",
      "epoch no.0 train no.47950  loss = 3.18458 avg_loss = 4.11097\n",
      "epoch no.0 train no.47960  loss = 5.05703 avg_loss = 4.08068\n",
      "epoch no.0 train no.47970  loss = 3.40575 avg_loss = 4.06667\n",
      "epoch no.0 train no.47980  loss = 4.39015 avg_loss = 4.09590\n",
      "epoch no.0 train no.47990  loss = 6.27356 avg_loss = 4.13857\n",
      "epoch no.0 train no.48000  loss = 4.21750 avg_loss = 4.12280\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.0 train no.48010  loss = 3.83827 avg_loss = 4.17327\n",
      "epoch no.0 train no.48020  loss = 3.08460 avg_loss = 4.12178\n",
      "epoch no.0 train no.48030  loss = 5.06157 avg_loss = 4.16211\n",
      "epoch no.0 train no.48040  loss = 6.19638 avg_loss = 4.14555\n",
      "epoch no.0 train no.48050  loss = 2.43973 avg_loss = 4.08302\n",
      "epoch no.0 train no.48060  loss = 3.90722 avg_loss = 4.13158\n",
      "epoch no.0 train no.48070  loss = 2.63936 avg_loss = 4.13590\n",
      "epoch no.0 train no.48080  loss = 5.53523 avg_loss = 4.11659\n",
      "epoch no.0 train no.48090  loss = 3.50858 avg_loss = 4.12490\n",
      "epoch no.0 train no.48100  loss = 3.87028 avg_loss = 4.07904\n",
      "epoch no.0 train no.48110  loss = 4.44291 avg_loss = 4.09228\n",
      "epoch no.0 train no.48120  loss = 4.28339 avg_loss = 4.06455\n",
      "epoch no.0 train no.48130  loss = 3.06952 avg_loss = 3.96455\n",
      "epoch no.0 train no.48140  loss = 4.59343 avg_loss = 3.95642\n",
      "epoch no.0 train no.48150  loss = 2.68446 avg_loss = 3.97694\n",
      "epoch no.0 train no.48160  loss = 3.54786 avg_loss = 3.97845\n",
      "epoch no.0 train no.48170  loss = 4.98588 avg_loss = 4.02314\n",
      "epoch no.0 train no.48180  loss = 4.19380 avg_loss = 4.02167\n",
      "epoch no.0 train no.48190  loss = 1.94331 avg_loss = 4.04425\n",
      "epoch no.0 train no.48200  loss = 1.77175 avg_loss = 4.04687\n",
      "epoch no.0 train no.48210  loss = 3.66582 avg_loss = 4.05385\n",
      "epoch no.0 train no.48220  loss = 4.23576 avg_loss = 4.07195\n",
      "epoch no.0 train no.48230  loss = 3.03906 avg_loss = 3.96333\n",
      "epoch no.0 train no.48240  loss = 3.90502 avg_loss = 3.93120\n",
      "epoch no.0 train no.48250  loss = 3.43885 avg_loss = 3.92589\n",
      "epoch no.0 train no.48260  loss = 2.37398 avg_loss = 3.93990\n",
      "epoch no.0 train no.48270  loss = 5.20002 avg_loss = 3.99950\n",
      "epoch no.0 train no.48280  loss = 2.80452 avg_loss = 3.91641\n",
      "epoch no.0 train no.48290  loss = 4.24053 avg_loss = 3.94506\n",
      "epoch no.0 train no.48300  loss = 3.59474 avg_loss = 3.89522\n",
      "epoch no.0 train no.48310  loss = 6.40325 avg_loss = 3.91416\n",
      "epoch no.0 train no.48320  loss = 2.68254 avg_loss = 3.90101\n",
      "epoch no.0 train no.48330  loss = 4.37842 avg_loss = 3.85863\n",
      "epoch no.0 train no.48340  loss = 4.55467 avg_loss = 3.84063\n",
      "epoch no.0 train no.48350  loss = 3.38727 avg_loss = 3.82679\n",
      "epoch no.0 train no.48360  loss = 2.63493 avg_loss = 3.85290\n",
      "epoch no.0 train no.48370  loss = 3.27372 avg_loss = 3.89623\n",
      "epoch no.0 train no.48380  loss = 3.79492 avg_loss = 3.93622\n",
      "epoch no.0 train no.48390  loss = 4.69084 avg_loss = 4.00524\n",
      "epoch no.0 train no.48400  loss = 4.55781 avg_loss = 3.99764\n",
      "epoch no.0 train no.48410  loss = 3.60782 avg_loss = 3.99368\n",
      "epoch no.0 train no.48420  loss = 2.87996 avg_loss = 3.96363\n",
      "epoch no.0 train no.48430  loss = 3.05276 avg_loss = 3.94400\n",
      "epoch no.0 train no.48440  loss = 5.74974 avg_loss = 3.96637\n",
      "epoch no.0 train no.48450  loss = 4.40205 avg_loss = 3.99880\n",
      "epoch no.0 train no.48460  loss = 3.68838 avg_loss = 4.00962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.48470  loss = 3.54708 avg_loss = 3.96752\n",
      "epoch no.0 train no.48480  loss = 4.41832 avg_loss = 3.96792\n",
      "epoch no.0 train no.48490  loss = 7.92098 avg_loss = 3.97188\n",
      "epoch no.0 train no.48500  loss = 2.45946 avg_loss = 3.99464\n",
      "epoch no.0 train no.48510  loss = 5.63385 avg_loss = 3.98841\n",
      "epoch no.0 train no.48520  loss = 2.97263 avg_loss = 4.04697\n",
      "epoch no.0 train no.48530  loss = 4.32527 avg_loss = 4.07606\n",
      "epoch no.0 train no.48540  loss = 2.63983 avg_loss = 4.04404\n",
      "epoch no.0 train no.48550  loss = 3.75973 avg_loss = 3.99739\n",
      "epoch no.0 train no.48560  loss = 5.05685 avg_loss = 4.05620\n",
      "epoch no.0 train no.48570  loss = 3.57726 avg_loss = 4.09717\n",
      "epoch no.0 train no.48580  loss = 4.95504 avg_loss = 4.17015\n",
      "epoch no.0 train no.48590  loss = 1.99508 avg_loss = 4.14469\n",
      "epoch no.0 train no.48600  loss = 4.62909 avg_loss = 4.14865\n",
      "epoch no.0 train no.48610  loss = 2.83153 avg_loss = 4.12189\n",
      "epoch no.0 train no.48620  loss = 3.96620 avg_loss = 4.11808\n",
      "epoch no.0 train no.48630  loss = 3.28651 avg_loss = 4.11338\n",
      "epoch no.0 train no.48640  loss = 6.30392 avg_loss = 4.06451\n",
      "epoch no.0 train no.48650  loss = 4.41326 avg_loss = 4.05759\n",
      "epoch no.0 train no.48660  loss = 2.91849 avg_loss = 4.05355\n",
      "epoch no.0 train no.48670  loss = 2.82672 avg_loss = 4.04400\n",
      "epoch no.0 train no.48680  loss = 4.07174 avg_loss = 3.98525\n",
      "epoch no.0 train no.48690  loss = 6.65838 avg_loss = 3.96647\n",
      "epoch no.0 train no.48700  loss = 3.99159 avg_loss = 3.96870\n",
      "epoch no.0 train no.48710  loss = 3.57974 avg_loss = 3.96062\n",
      "epoch no.0 train no.48720  loss = 3.76292 avg_loss = 4.02032\n",
      "epoch no.0 train no.48730  loss = 5.76405 avg_loss = 4.06719\n",
      "epoch no.0 train no.48740  loss = 2.62357 avg_loss = 4.03444\n",
      "epoch no.0 train no.48750  loss = 3.44279 avg_loss = 4.07904\n",
      "epoch no.0 train no.48760  loss = 3.46361 avg_loss = 4.09380\n",
      "epoch no.0 train no.48770  loss = 2.39844 avg_loss = 4.05904\n",
      "epoch no.0 train no.48780  loss = 2.88103 avg_loss = 4.02435\n",
      "epoch no.0 train no.48790  loss = 5.13209 avg_loss = 4.03976\n",
      "epoch no.0 train no.48800  loss = 5.35093 avg_loss = 4.01660\n",
      "epoch no.0 train no.48810  loss = 2.48374 avg_loss = 4.07867\n",
      "epoch no.0 train no.48820  loss = 2.84096 avg_loss = 4.02009\n",
      "epoch no.0 train no.48830  loss = 5.40136 avg_loss = 4.01328\n",
      "epoch no.0 train no.48840  loss = 5.43891 avg_loss = 4.07244\n",
      "epoch no.0 train no.48850  loss = 4.02508 avg_loss = 4.10863\n",
      "epoch no.0 train no.48860  loss = 3.65139 avg_loss = 4.07109\n",
      "epoch no.0 train no.48870  loss = 4.05995 avg_loss = 4.07944\n",
      "epoch no.0 train no.48880  loss = 3.33458 avg_loss = 4.08874\n",
      "epoch no.0 train no.48890  loss = 6.82763 avg_loss = 4.11843\n",
      "epoch no.0 train no.48900  loss = 2.90813 avg_loss = 4.11292\n",
      "epoch no.0 train no.48910  loss = 5.48431 avg_loss = 4.14346\n",
      "epoch no.0 train no.48920  loss = 2.78189 avg_loss = 4.09629\n",
      "epoch no.0 train no.48930  loss = 3.99577 avg_loss = 4.12620\n",
      "epoch no.0 train no.48940  loss = 4.82091 avg_loss = 4.10737\n",
      "epoch no.0 train no.48950  loss = 3.37297 avg_loss = 4.04751\n",
      "epoch no.0 train no.48960  loss = 3.27949 avg_loss = 3.98486\n",
      "epoch no.0 train no.48970  loss = 4.47014 avg_loss = 4.00343\n",
      "epoch no.0 train no.48980  loss = 3.02056 avg_loss = 4.00908\n",
      "epoch no.0 train no.48990  loss = 3.68693 avg_loss = 4.00284\n",
      "epoch no.0 train no.49000  loss = 2.58685 avg_loss = 4.00500\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁좋은', '▁좋은', '▁노래', 'op', '</s>', '</s>']\n",
      "기분전환에 딱 좋은 pop 모음</s>\n",
      "epoch no.0 train no.49010  loss = 4.11408 avg_loss = 4.10255\n",
      "epoch no.0 train no.49020  loss = 3.18721 avg_loss = 4.05176\n",
      "epoch no.0 train no.49030  loss = 2.28739 avg_loss = 4.07941\n",
      "epoch no.0 train no.49040  loss = 4.34901 avg_loss = 4.07895\n",
      "epoch no.0 train no.49050  loss = 2.35279 avg_loss = 4.02050\n",
      "epoch no.0 train no.49060  loss = 3.12444 avg_loss = 3.98533\n",
      "epoch no.0 train no.49070  loss = 4.25982 avg_loss = 4.05281\n",
      "epoch no.0 train no.49080  loss = 2.52762 avg_loss = 4.05351\n",
      "epoch no.0 train no.49090  loss = 1.95228 avg_loss = 4.02551\n",
      "epoch no.0 train no.49100  loss = 4.07629 avg_loss = 4.01645\n",
      "epoch no.0 train no.49110  loss = 5.82378 avg_loss = 4.02791\n",
      "epoch no.0 train no.49120  loss = 4.00980 avg_loss = 4.02221\n",
      "epoch no.0 train no.49130  loss = 6.26434 avg_loss = 4.03658\n",
      "epoch no.0 train no.49140  loss = 2.90660 avg_loss = 4.02152\n",
      "epoch no.0 train no.49150  loss = 3.85341 avg_loss = 3.99092\n",
      "epoch no.0 train no.49160  loss = 2.25111 avg_loss = 4.03435\n",
      "epoch no.0 train no.49170  loss = 2.59273 avg_loss = 4.06909\n",
      "epoch no.0 train no.49180  loss = 3.71348 avg_loss = 4.10538\n",
      "epoch no.0 train no.49190  loss = 2.66797 avg_loss = 4.14259\n",
      "epoch no.0 train no.49200  loss = 2.05135 avg_loss = 4.11282\n",
      "epoch no.0 train no.49210  loss = 3.03983 avg_loss = 4.14564\n",
      "epoch no.0 train no.49220  loss = 3.95613 avg_loss = 4.06325\n",
      "epoch no.0 train no.49230  loss = 4.16933 avg_loss = 4.11841\n",
      "epoch no.0 train no.49240  loss = 2.00121 avg_loss = 4.10027\n",
      "epoch no.0 train no.49250  loss = 5.11882 avg_loss = 4.16806\n",
      "epoch no.0 train no.49260  loss = 4.72005 avg_loss = 4.17784\n",
      "epoch no.0 train no.49270  loss = 2.82067 avg_loss = 4.17077\n",
      "epoch no.0 train no.49280  loss = 4.58079 avg_loss = 4.17121\n",
      "epoch no.0 train no.49290  loss = 3.35815 avg_loss = 4.08383\n",
      "epoch no.0 train no.49300  loss = 6.40813 avg_loss = 4.09650\n",
      "epoch no.0 train no.49310  loss = 3.54076 avg_loss = 4.07227\n",
      "epoch no.0 train no.49320  loss = 3.73775 avg_loss = 3.98677\n",
      "epoch no.0 train no.49330  loss = 5.52304 avg_loss = 4.00778\n",
      "epoch no.0 train no.49340  loss = 3.82817 avg_loss = 4.03551\n",
      "epoch no.0 train no.49350  loss = 5.42891 avg_loss = 3.98915\n",
      "epoch no.0 train no.49360  loss = 5.70632 avg_loss = 3.97219\n",
      "epoch no.0 train no.49370  loss = 4.13065 avg_loss = 3.91302\n",
      "epoch no.0 train no.49380  loss = 5.62608 avg_loss = 3.94795\n",
      "epoch no.0 train no.49390  loss = 4.45217 avg_loss = 3.96081\n",
      "epoch no.0 train no.49400  loss = 4.47351 avg_loss = 3.94359\n",
      "epoch no.0 train no.49410  loss = 5.43573 avg_loss = 3.95153\n",
      "epoch no.0 train no.49420  loss = 2.70591 avg_loss = 3.97259\n",
      "epoch no.0 train no.49430  loss = 4.99346 avg_loss = 3.99252\n",
      "epoch no.0 train no.49440  loss = 2.34895 avg_loss = 3.99661\n",
      "epoch no.0 train no.49450  loss = 3.09936 avg_loss = 3.93383\n",
      "epoch no.0 train no.49460  loss = 4.93248 avg_loss = 3.95647\n",
      "epoch no.0 train no.49470  loss = 7.83573 avg_loss = 4.00483\n",
      "epoch no.0 train no.49480  loss = 3.42121 avg_loss = 4.00691\n",
      "epoch no.0 train no.49490  loss = 5.95740 avg_loss = 4.05993\n",
      "epoch no.0 train no.49500  loss = 4.50799 avg_loss = 4.01113\n",
      "epoch no.0 train no.49510  loss = 3.45866 avg_loss = 4.02520\n",
      "epoch no.0 train no.49520  loss = 3.22331 avg_loss = 4.00050\n",
      "epoch no.0 train no.49530  loss = 3.27449 avg_loss = 4.00801\n",
      "epoch no.0 train no.49540  loss = 3.88130 avg_loss = 4.05530\n",
      "epoch no.0 train no.49550  loss = 4.94691 avg_loss = 4.04543\n",
      "epoch no.0 train no.49560  loss = 3.16427 avg_loss = 4.06837\n",
      "epoch no.0 train no.49570  loss = 2.75633 avg_loss = 4.04540\n",
      "epoch no.0 train no.49580  loss = 2.27550 avg_loss = 3.99798\n",
      "epoch no.0 train no.49590  loss = 5.78174 avg_loss = 4.01722\n",
      "epoch no.0 train no.49600  loss = 4.03768 avg_loss = 3.96058\n",
      "epoch no.0 train no.49610  loss = 3.14228 avg_loss = 3.90816\n",
      "epoch no.0 train no.49620  loss = 4.90443 avg_loss = 3.89651\n",
      "epoch no.0 train no.49630  loss = 2.56676 avg_loss = 3.90792\n",
      "epoch no.0 train no.49640  loss = 4.04259 avg_loss = 3.88985\n",
      "epoch no.0 train no.49650  loss = 3.25115 avg_loss = 3.88538\n",
      "epoch no.0 train no.49660  loss = 4.21743 avg_loss = 3.91539\n",
      "epoch no.0 train no.49670  loss = 3.27508 avg_loss = 3.87744\n",
      "epoch no.0 train no.49680  loss = 2.24760 avg_loss = 3.85953\n",
      "epoch no.0 train no.49690  loss = 4.94250 avg_loss = 3.94817\n",
      "epoch no.0 train no.49700  loss = 8.84770 avg_loss = 4.01643\n",
      "epoch no.0 train no.49710  loss = 3.69500 avg_loss = 4.00900\n",
      "epoch no.0 train no.49720  loss = 5.19042 avg_loss = 4.01009\n",
      "epoch no.0 train no.49730  loss = 3.87153 avg_loss = 4.01264\n",
      "epoch no.0 train no.49740  loss = 3.10104 avg_loss = 4.02777\n",
      "epoch no.0 train no.49750  loss = 6.22814 avg_loss = 4.03093\n",
      "epoch no.0 train no.49760  loss = 4.73206 avg_loss = 4.07447\n",
      "epoch no.0 train no.49770  loss = 2.96177 avg_loss = 4.16193\n",
      "epoch no.0 train no.49780  loss = 4.15370 avg_loss = 4.14886\n",
      "epoch no.0 train no.49790  loss = 3.41415 avg_loss = 4.14957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.49800  loss = 5.20806 avg_loss = 4.12959\n",
      "epoch no.0 train no.49810  loss = 5.52291 avg_loss = 4.12458\n",
      "epoch no.0 train no.49820  loss = 5.56278 avg_loss = 4.13575\n",
      "epoch no.0 train no.49830  loss = 5.83568 avg_loss = 4.09363\n",
      "epoch no.0 train no.49840  loss = 3.03401 avg_loss = 4.07016\n",
      "epoch no.0 train no.49850  loss = 2.88945 avg_loss = 4.06618\n",
      "epoch no.0 train no.49860  loss = 3.88672 avg_loss = 4.04909\n",
      "epoch no.0 train no.49870  loss = 3.99797 avg_loss = 4.05808\n",
      "epoch no.0 train no.49880  loss = 3.93441 avg_loss = 4.08959\n",
      "epoch no.0 train no.49890  loss = 5.21766 avg_loss = 4.06047\n",
      "epoch no.0 train no.49900  loss = 2.87501 avg_loss = 4.06728\n",
      "epoch no.0 train no.49910  loss = 3.26745 avg_loss = 4.04880\n",
      "epoch no.0 train no.49920  loss = 3.84092 avg_loss = 4.09039\n",
      "epoch no.0 train no.49930  loss = 4.69491 avg_loss = 4.04072\n",
      "epoch no.0 train no.49940  loss = 2.91273 avg_loss = 4.02872\n",
      "epoch no.0 train no.49950  loss = 5.67542 avg_loss = 4.05551\n",
      "epoch no.0 train no.49960  loss = 3.11850 avg_loss = 4.02643\n",
      "epoch no.0 train no.49970  loss = 4.29478 avg_loss = 4.04545\n",
      "epoch no.0 train no.49980  loss = 5.71222 avg_loss = 4.08114\n",
      "epoch no.0 train no.49990  loss = 7.24228 avg_loss = 4.09433\n",
      "epoch no.0 train no.50000  loss = 2.78518 avg_loss = 4.01019\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁노래', '</s>']\n",
      "기분전환 해줄 노래</s>\n",
      "epoch no.0 train no.50010  loss = 2.27419 avg_loss = 4.02942\n",
      "epoch no.0 train no.50020  loss = 4.02588 avg_loss = 4.05575\n",
      "epoch no.0 train no.50030  loss = 4.65808 avg_loss = 4.09396\n",
      "epoch no.0 train no.50040  loss = 3.22179 avg_loss = 4.07302\n",
      "epoch no.0 train no.50050  loss = 4.51823 avg_loss = 4.04087\n",
      "epoch no.0 train no.50060  loss = 2.79433 avg_loss = 4.04347\n",
      "epoch no.0 train no.50070  loss = 8.71735 avg_loss = 4.09785\n",
      "epoch no.0 train no.50080  loss = 4.87204 avg_loss = 4.10379\n",
      "epoch no.0 train no.50090  loss = 5.32670 avg_loss = 4.09369\n",
      "epoch no.0 train no.50100  loss = 3.43460 avg_loss = 4.04764\n",
      "epoch no.0 train no.50110  loss = 2.28254 avg_loss = 4.00196\n",
      "epoch no.0 train no.50120  loss = 3.91437 avg_loss = 3.98148\n",
      "epoch no.0 train no.50130  loss = 1.98674 avg_loss = 3.96439\n",
      "epoch no.0 train no.50140  loss = 5.28472 avg_loss = 4.01453\n",
      "epoch no.0 train no.50150  loss = 4.14627 avg_loss = 4.03858\n",
      "epoch no.0 train no.50160  loss = 3.13087 avg_loss = 4.00446\n",
      "epoch no.0 train no.50170  loss = 5.21805 avg_loss = 4.01671\n",
      "epoch no.0 train no.50180  loss = 4.64200 avg_loss = 4.03146\n",
      "epoch no.0 train no.50190  loss = 3.69686 avg_loss = 4.03856\n",
      "epoch no.0 train no.50200  loss = 2.51422 avg_loss = 4.02315\n",
      "epoch no.0 train no.50210  loss = 5.14343 avg_loss = 4.09241\n",
      "epoch no.0 train no.50220  loss = 3.28102 avg_loss = 4.08774\n",
      "epoch no.0 train no.50230  loss = 3.84721 avg_loss = 4.09804\n",
      "epoch no.0 train no.50240  loss = 6.05552 avg_loss = 4.10783\n",
      "epoch no.0 train no.50250  loss = 5.16437 avg_loss = 4.08176\n",
      "epoch no.0 train no.50260  loss = 6.81936 avg_loss = 4.11347\n",
      "epoch no.0 train no.50270  loss = 4.51755 avg_loss = 4.08736\n",
      "epoch no.0 train no.50280  loss = 3.72471 avg_loss = 4.02512\n",
      "epoch no.0 train no.50290  loss = 3.01156 avg_loss = 4.05019\n",
      "epoch no.0 train no.50300  loss = 3.76559 avg_loss = 4.01066\n",
      "epoch no.0 train no.50310  loss = 3.23863 avg_loss = 4.01632\n",
      "epoch no.0 train no.50320  loss = 4.19637 avg_loss = 4.02079\n",
      "epoch no.0 train no.50330  loss = 2.59076 avg_loss = 4.03422\n",
      "epoch no.0 train no.50340  loss = 4.19084 avg_loss = 4.00417\n",
      "epoch no.0 train no.50350  loss = 4.90681 avg_loss = 4.03650\n",
      "epoch no.0 train no.50360  loss = 2.94403 avg_loss = 3.98941\n",
      "epoch no.0 train no.50370  loss = 2.99171 avg_loss = 3.99176\n",
      "epoch no.0 train no.50380  loss = 6.25635 avg_loss = 3.98736\n",
      "epoch no.0 train no.50390  loss = 4.81398 avg_loss = 4.01919\n",
      "epoch no.0 train no.50400  loss = 3.80096 avg_loss = 3.97605\n",
      "epoch no.0 train no.50410  loss = 3.12248 avg_loss = 3.96133\n",
      "epoch no.0 train no.50420  loss = 4.43659 avg_loss = 3.98679\n",
      "epoch no.0 train no.50430  loss = 3.61597 avg_loss = 3.99011\n",
      "epoch no.0 train no.50440  loss = 6.21962 avg_loss = 4.05995\n",
      "epoch no.0 train no.50450  loss = 3.18458 avg_loss = 3.99394\n",
      "epoch no.0 train no.50460  loss = 4.91287 avg_loss = 4.08897\n",
      "epoch no.0 train no.50470  loss = 3.71036 avg_loss = 4.06569\n",
      "epoch no.0 train no.50480  loss = 4.64883 avg_loss = 4.01590\n",
      "epoch no.0 train no.50490  loss = 4.34019 avg_loss = 4.11462\n",
      "epoch no.0 train no.50500  loss = 2.77195 avg_loss = 4.07705\n",
      "epoch no.0 train no.50510  loss = 6.77452 avg_loss = 4.10822\n",
      "epoch no.0 train no.50520  loss = 7.29624 avg_loss = 4.11741\n",
      "epoch no.0 train no.50530  loss = 6.08971 avg_loss = 4.17344\n",
      "epoch no.0 train no.50540  loss = 3.94524 avg_loss = 4.13754\n",
      "epoch no.0 train no.50550  loss = 3.04781 avg_loss = 4.09601\n",
      "epoch no.0 train no.50560  loss = 3.27225 avg_loss = 4.07773\n",
      "epoch no.0 train no.50570  loss = 3.21141 avg_loss = 4.06355\n",
      "epoch no.0 train no.50580  loss = 3.86252 avg_loss = 4.05826\n",
      "epoch no.0 train no.50590  loss = 5.41731 avg_loss = 4.06913\n",
      "epoch no.0 train no.50600  loss = 4.35763 avg_loss = 4.10108\n",
      "epoch no.0 train no.50610  loss = 5.23960 avg_loss = 4.09048\n",
      "epoch no.0 train no.50620  loss = 3.09801 avg_loss = 4.10607\n",
      "epoch no.0 train no.50630  loss = 5.29981 avg_loss = 4.08760\n",
      "epoch no.0 train no.50640  loss = 3.19568 avg_loss = 4.04380\n",
      "epoch no.0 train no.50650  loss = 4.23867 avg_loss = 4.02339\n",
      "epoch no.0 train no.50660  loss = 8.28106 avg_loss = 4.08455\n",
      "epoch no.0 train no.50670  loss = 4.31530 avg_loss = 4.09599\n",
      "epoch no.0 train no.50680  loss = 3.31760 avg_loss = 4.15265\n",
      "epoch no.0 train no.50690  loss = 5.60410 avg_loss = 4.19148\n",
      "epoch no.0 train no.50700  loss = 2.73442 avg_loss = 4.14402\n",
      "epoch no.0 train no.50710  loss = 3.51038 avg_loss = 4.14151\n",
      "epoch no.0 train no.50720  loss = 4.34641 avg_loss = 4.14242\n",
      "epoch no.0 train no.50730  loss = 6.41828 avg_loss = 4.16658\n",
      "epoch no.0 train no.50740  loss = 5.19070 avg_loss = 4.17288\n",
      "epoch no.0 train no.50750  loss = 4.35574 avg_loss = 4.16540\n",
      "epoch no.0 train no.50760  loss = 4.66700 avg_loss = 4.16437\n",
      "epoch no.0 train no.50770  loss = 6.30960 avg_loss = 4.20370\n",
      "epoch no.0 train no.50780  loss = 6.27723 avg_loss = 4.16934\n",
      "epoch no.0 train no.50790  loss = 2.30379 avg_loss = 4.17997\n",
      "epoch no.0 train no.50800  loss = 3.54523 avg_loss = 4.19833\n",
      "epoch no.0 train no.50810  loss = 4.88720 avg_loss = 4.19765\n",
      "epoch no.0 train no.50820  loss = 7.40844 avg_loss = 4.22981\n",
      "epoch no.0 train no.50830  loss = 2.15043 avg_loss = 4.16163\n",
      "epoch no.0 train no.50840  loss = 4.97120 avg_loss = 4.15198\n",
      "epoch no.0 train no.50850  loss = 3.30341 avg_loss = 4.14894\n",
      "epoch no.0 train no.50860  loss = 3.72419 avg_loss = 4.07292\n",
      "epoch no.0 train no.50870  loss = 4.25870 avg_loss = 4.06335\n",
      "epoch no.0 train no.50880  loss = 3.11109 avg_loss = 4.07609\n",
      "epoch no.0 train no.50890  loss = 3.23440 avg_loss = 4.06656\n",
      "epoch no.0 train no.50900  loss = 2.84390 avg_loss = 4.02153\n",
      "epoch no.0 train no.50910  loss = 3.38431 avg_loss = 4.03792\n",
      "epoch no.0 train no.50920  loss = 3.33695 avg_loss = 3.96400\n",
      "epoch no.0 train no.50930  loss = 2.69380 avg_loss = 3.88965\n",
      "epoch no.0 train no.50940  loss = 6.21809 avg_loss = 3.94092\n",
      "epoch no.0 train no.50950  loss = 5.71776 avg_loss = 3.93423\n",
      "epoch no.0 train no.50960  loss = 3.35053 avg_loss = 3.98807\n",
      "epoch no.0 train no.50970  loss = 3.94848 avg_loss = 3.97461\n",
      "epoch no.0 train no.50980  loss = 5.13732 avg_loss = 3.97801\n",
      "epoch no.0 train no.50990  loss = 4.17500 avg_loss = 3.97342\n",
      "epoch no.0 train no.51000  loss = 3.64177 avg_loss = 3.93498\n",
      "5\n",
      "to_tokens: ['▁가을', '좋은', '이', '싶', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환 하고 싶을 때 듣는 음악</s>\n",
      "epoch no.0 train no.51010  loss = 4.84870 avg_loss = 3.95664\n",
      "epoch no.0 train no.51020  loss = 4.61424 avg_loss = 3.99152\n",
      "epoch no.0 train no.51030  loss = 4.96560 avg_loss = 4.00941\n",
      "epoch no.0 train no.51040  loss = 4.89170 avg_loss = 3.98780\n",
      "epoch no.0 train no.51050  loss = 3.93471 avg_loss = 3.98914\n",
      "epoch no.0 train no.51060  loss = 3.40056 avg_loss = 3.89416\n",
      "epoch no.0 train no.51070  loss = 4.24519 avg_loss = 3.94270\n",
      "epoch no.0 train no.51080  loss = 2.44369 avg_loss = 3.88911\n",
      "epoch no.0 train no.51090  loss = 3.64481 avg_loss = 3.88154\n",
      "epoch no.0 train no.51100  loss = 3.40338 avg_loss = 3.86677\n",
      "epoch no.0 train no.51110  loss = 2.83658 avg_loss = 3.82285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.51120  loss = 6.32520 avg_loss = 3.80881\n",
      "epoch no.0 train no.51130  loss = 2.38483 avg_loss = 3.77035\n",
      "epoch no.0 train no.51140  loss = 3.18368 avg_loss = 3.76087\n",
      "epoch no.0 train no.51150  loss = 5.42511 avg_loss = 3.75288\n",
      "epoch no.0 train no.51160  loss = 3.54537 avg_loss = 3.74494\n",
      "epoch no.0 train no.51170  loss = 4.39935 avg_loss = 3.78841\n",
      "epoch no.0 train no.51180  loss = 3.77945 avg_loss = 3.76947\n",
      "epoch no.0 train no.51190  loss = 5.30251 avg_loss = 3.82427\n",
      "epoch no.0 train no.51200  loss = 4.80942 avg_loss = 3.80716\n",
      "epoch no.0 train no.51210  loss = 6.20296 avg_loss = 3.83499\n",
      "epoch no.0 train no.51220  loss = 3.37131 avg_loss = 3.81091\n",
      "epoch no.0 train no.51230  loss = 3.54724 avg_loss = 3.82873\n",
      "epoch no.0 train no.51240  loss = 4.97839 avg_loss = 3.84442\n",
      "epoch no.0 train no.51250  loss = 1.87872 avg_loss = 3.81349\n",
      "epoch no.0 train no.51260  loss = 5.52186 avg_loss = 3.91483\n",
      "epoch no.0 train no.51270  loss = 3.34337 avg_loss = 3.95619\n",
      "epoch no.0 train no.51280  loss = 2.98493 avg_loss = 3.94980\n",
      "epoch no.0 train no.51290  loss = 4.01686 avg_loss = 3.93071\n",
      "epoch no.0 train no.51300  loss = 5.46633 avg_loss = 3.89135\n",
      "epoch no.0 train no.51310  loss = 2.69863 avg_loss = 3.92206\n",
      "epoch no.0 train no.51320  loss = 6.18851 avg_loss = 3.93741\n",
      "epoch no.0 train no.51330  loss = 4.83682 avg_loss = 3.97583\n",
      "epoch no.0 train no.51340  loss = 5.77308 avg_loss = 4.01462\n",
      "epoch no.0 train no.51350  loss = 2.27979 avg_loss = 4.00208\n",
      "epoch no.0 train no.51360  loss = 4.73819 avg_loss = 3.96079\n",
      "epoch no.0 train no.51370  loss = 5.51100 avg_loss = 3.99827\n",
      "epoch no.0 train no.51380  loss = 4.46228 avg_loss = 3.99886\n",
      "epoch no.0 train no.51390  loss = 3.75754 avg_loss = 4.03040\n",
      "epoch no.0 train no.51400  loss = 2.72979 avg_loss = 4.09191\n",
      "epoch no.0 train no.51410  loss = 2.69327 avg_loss = 4.13621\n",
      "epoch no.0 train no.51420  loss = 3.40216 avg_loss = 4.17826\n",
      "epoch no.0 train no.51430  loss = 2.50199 avg_loss = 4.16443\n",
      "epoch no.0 train no.51440  loss = 4.44096 avg_loss = 4.11000\n",
      "epoch no.0 train no.51450  loss = 2.23557 avg_loss = 4.08026\n",
      "epoch no.0 train no.51460  loss = 4.34763 avg_loss = 4.08038\n",
      "epoch no.0 train no.51470  loss = 4.77741 avg_loss = 4.11648\n",
      "epoch no.0 train no.51480  loss = 2.68799 avg_loss = 4.03363\n",
      "epoch no.0 train no.51490  loss = 2.03419 avg_loss = 3.97207\n",
      "epoch no.0 train no.51500  loss = 3.98374 avg_loss = 4.04062\n",
      "epoch no.0 train no.51510  loss = 2.46598 avg_loss = 4.01901\n",
      "epoch no.0 train no.51520  loss = 3.01901 avg_loss = 4.01722\n",
      "epoch no.0 train no.51530  loss = 3.21219 avg_loss = 4.05855\n",
      "epoch no.0 train no.51540  loss = 5.72474 avg_loss = 4.07001\n",
      "epoch no.0 train no.51550  loss = 3.09019 avg_loss = 4.09153\n",
      "epoch no.0 train no.51560  loss = 2.81970 avg_loss = 4.14260\n",
      "epoch no.0 train no.51570  loss = 3.44106 avg_loss = 4.20729\n",
      "epoch no.0 train no.51580  loss = 6.75658 avg_loss = 4.14480\n",
      "epoch no.0 train no.51590  loss = 2.66016 avg_loss = 4.09387\n",
      "epoch no.0 train no.51600  loss = 4.25159 avg_loss = 4.19022\n",
      "epoch no.0 train no.51610  loss = 2.76840 avg_loss = 4.12420\n",
      "epoch no.0 train no.51620  loss = 6.60802 avg_loss = 4.16207\n",
      "epoch no.0 train no.51630  loss = 3.16749 avg_loss = 4.11320\n",
      "epoch no.0 train no.51640  loss = 7.18253 avg_loss = 4.16095\n",
      "epoch no.0 train no.51650  loss = 5.21597 avg_loss = 4.14011\n",
      "epoch no.0 train no.51660  loss = 3.81817 avg_loss = 4.09032\n",
      "epoch no.0 train no.51670  loss = 3.97971 avg_loss = 4.04210\n",
      "epoch no.0 train no.51680  loss = 3.96999 avg_loss = 4.01419\n",
      "epoch no.0 train no.51690  loss = 4.91638 avg_loss = 4.04148\n",
      "epoch no.0 train no.51700  loss = 3.82626 avg_loss = 3.98074\n",
      "epoch no.0 train no.51710  loss = 5.63631 avg_loss = 3.96291\n",
      "epoch no.0 train no.51720  loss = 4.30618 avg_loss = 4.00392\n",
      "epoch no.0 train no.51730  loss = 6.38454 avg_loss = 4.06545\n",
      "epoch no.0 train no.51740  loss = 5.92850 avg_loss = 4.14684\n",
      "epoch no.0 train no.51750  loss = 4.18317 avg_loss = 4.18077\n",
      "epoch no.0 train no.51760  loss = 5.22834 avg_loss = 4.15574\n",
      "epoch no.0 train no.51770  loss = 3.30611 avg_loss = 4.14441\n",
      "epoch no.0 train no.51780  loss = 3.36792 avg_loss = 4.11719\n",
      "epoch no.0 train no.51790  loss = 4.61585 avg_loss = 4.13535\n",
      "epoch no.0 train no.51800  loss = 3.46520 avg_loss = 4.12347\n",
      "epoch no.0 train no.51810  loss = 3.08297 avg_loss = 4.12358\n",
      "epoch no.0 train no.51820  loss = 5.35241 avg_loss = 4.09236\n",
      "epoch no.0 train no.51830  loss = 2.26082 avg_loss = 4.13829\n",
      "epoch no.0 train no.51840  loss = 5.50671 avg_loss = 4.13281\n",
      "epoch no.0 train no.51850  loss = 2.50026 avg_loss = 4.19686\n",
      "epoch no.0 train no.51860  loss = 3.91775 avg_loss = 4.16351\n",
      "epoch no.0 train no.51870  loss = 7.79460 avg_loss = 4.22160\n",
      "epoch no.0 train no.51880  loss = 5.36059 avg_loss = 4.22916\n",
      "epoch no.0 train no.51890  loss = 2.77607 avg_loss = 4.25223\n",
      "epoch no.0 train no.51900  loss = 3.91430 avg_loss = 4.27128\n",
      "epoch no.0 train no.51910  loss = 4.47744 avg_loss = 4.18061\n",
      "epoch no.0 train no.51920  loss = 2.53132 avg_loss = 4.10977\n",
      "epoch no.0 train no.51930  loss = 3.51830 avg_loss = 4.07611\n",
      "epoch no.0 train no.51940  loss = 2.94347 avg_loss = 4.05762\n",
      "epoch no.0 train no.51950  loss = 4.03406 avg_loss = 4.04850\n",
      "epoch no.0 train no.51960  loss = 6.63655 avg_loss = 4.08886\n",
      "epoch no.0 train no.51970  loss = 1.94060 avg_loss = 4.11929\n",
      "epoch no.0 train no.51980  loss = 2.40873 avg_loss = 4.01968\n",
      "epoch no.0 train no.51990  loss = 3.07279 avg_loss = 4.01225\n",
      "epoch no.0 train no.52000  loss = 2.61528 avg_loss = 4.01097\n",
      "4\n",
      "to_tokens: ['▁비', '좋은', '에', '▁싶을', '을', '때', '</s>']\n",
      "기분전환하고싶을때</s>\n",
      "epoch no.0 train no.52010  loss = 9.19781 avg_loss = 3.99645\n",
      "epoch no.0 train no.52020  loss = 3.44841 avg_loss = 4.02826\n",
      "epoch no.0 train no.52030  loss = 3.02186 avg_loss = 4.03218\n",
      "epoch no.0 train no.52040  loss = 2.94213 avg_loss = 4.04577\n",
      "epoch no.0 train no.52050  loss = 3.23051 avg_loss = 4.06001\n",
      "epoch no.0 train no.52060  loss = 3.71702 avg_loss = 4.06589\n",
      "epoch no.0 train no.52070  loss = 3.49465 avg_loss = 4.03912\n",
      "epoch no.0 train no.52080  loss = 3.60076 avg_loss = 4.03808\n",
      "epoch no.0 train no.52090  loss = 3.69453 avg_loss = 4.02012\n",
      "epoch no.0 train no.52100  loss = 2.85302 avg_loss = 3.99772\n",
      "epoch no.0 train no.52110  loss = 2.88999 avg_loss = 3.99844\n",
      "epoch no.0 train no.52120  loss = 3.77638 avg_loss = 4.03518\n",
      "epoch no.0 train no.52130  loss = 2.74519 avg_loss = 4.02891\n",
      "epoch no.0 train no.52140  loss = 2.99216 avg_loss = 4.00266\n",
      "epoch no.0 train no.52150  loss = 3.68561 avg_loss = 4.02191\n",
      "epoch no.0 train no.52160  loss = 4.95907 avg_loss = 3.97867\n",
      "epoch no.0 train no.52170  loss = 3.53684 avg_loss = 3.96733\n",
      "epoch no.0 train no.52180  loss = 4.50825 avg_loss = 3.94139\n",
      "epoch no.0 train no.52190  loss = 2.84818 avg_loss = 3.97803\n",
      "epoch no.0 train no.52200  loss = 3.09805 avg_loss = 3.94821\n",
      "epoch no.0 train no.52210  loss = 3.16330 avg_loss = 3.98039\n",
      "epoch no.0 train no.52220  loss = 4.49314 avg_loss = 3.97792\n",
      "epoch no.0 train no.52230  loss = 3.44855 avg_loss = 3.94686\n",
      "epoch no.0 train no.52240  loss = 2.63531 avg_loss = 3.88040\n",
      "epoch no.0 train no.52250  loss = 3.02111 avg_loss = 3.87554\n",
      "epoch no.0 train no.52260  loss = 3.01633 avg_loss = 3.87509\n",
      "epoch no.0 train no.52270  loss = 3.27166 avg_loss = 3.82175\n",
      "epoch no.0 train no.52280  loss = 2.26351 avg_loss = 3.80462\n",
      "epoch no.0 train no.52290  loss = 3.60724 avg_loss = 3.75348\n",
      "epoch no.0 train no.52300  loss = 2.69285 avg_loss = 3.76446\n",
      "epoch no.0 train no.52310  loss = 4.36366 avg_loss = 3.81996\n",
      "epoch no.0 train no.52320  loss = 3.97518 avg_loss = 3.83587\n",
      "epoch no.0 train no.52330  loss = 3.78317 avg_loss = 3.90518\n",
      "epoch no.0 train no.52340  loss = 5.75062 avg_loss = 3.92296\n",
      "epoch no.0 train no.52350  loss = 5.06306 avg_loss = 3.97767\n",
      "epoch no.0 train no.52360  loss = 4.81994 avg_loss = 3.98439\n",
      "epoch no.0 train no.52370  loss = 2.72176 avg_loss = 3.96009\n",
      "epoch no.0 train no.52380  loss = 3.46588 avg_loss = 3.99163\n",
      "epoch no.0 train no.52390  loss = 4.72352 avg_loss = 4.02702\n",
      "epoch no.0 train no.52400  loss = 3.95524 avg_loss = 4.07569\n",
      "epoch no.0 train no.52410  loss = 4.87903 avg_loss = 4.03445\n",
      "epoch no.0 train no.52420  loss = 5.32536 avg_loss = 4.07221\n",
      "epoch no.0 train no.52430  loss = 3.50085 avg_loss = 4.00963\n",
      "epoch no.0 train no.52440  loss = 2.03112 avg_loss = 3.98949\n",
      "epoch no.0 train no.52450  loss = 3.24405 avg_loss = 4.02718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.52460  loss = 4.45172 avg_loss = 4.04752\n",
      "epoch no.0 train no.52470  loss = 3.08208 avg_loss = 4.08311\n",
      "epoch no.0 train no.52480  loss = 2.73095 avg_loss = 4.14352\n",
      "epoch no.0 train no.52490  loss = 4.50002 avg_loss = 4.07277\n",
      "epoch no.0 train no.52500  loss = 2.73776 avg_loss = 4.04713\n",
      "epoch no.0 train no.52510  loss = 4.07475 avg_loss = 4.04158\n",
      "epoch no.0 train no.52520  loss = 6.22878 avg_loss = 4.08045\n",
      "epoch no.0 train no.52530  loss = 4.66627 avg_loss = 4.16777\n",
      "epoch no.0 train no.52540  loss = 5.70582 avg_loss = 4.21101\n",
      "epoch no.0 train no.52550  loss = 2.38123 avg_loss = 4.19226\n",
      "epoch no.0 train no.52560  loss = 7.62671 avg_loss = 4.21908\n",
      "epoch no.0 train no.52570  loss = 4.16637 avg_loss = 4.20787\n",
      "epoch no.0 train no.52580  loss = 2.80325 avg_loss = 4.14394\n",
      "epoch no.0 train no.52590  loss = 3.87313 avg_loss = 4.12753\n",
      "epoch no.0 train no.52600  loss = 3.03810 avg_loss = 4.07899\n",
      "epoch no.0 train no.52610  loss = 3.82208 avg_loss = 4.05847\n",
      "epoch no.0 train no.52620  loss = 4.98902 avg_loss = 4.05781\n",
      "epoch no.0 train no.52630  loss = 5.30874 avg_loss = 4.05289\n",
      "epoch no.0 train no.52640  loss = 3.00949 avg_loss = 4.00551\n",
      "epoch no.0 train no.52650  loss = 5.49951 avg_loss = 4.02006\n",
      "epoch no.0 train no.52660  loss = 2.98381 avg_loss = 3.97318\n",
      "epoch no.0 train no.52670  loss = 1.99372 avg_loss = 3.93607\n",
      "epoch no.0 train no.52680  loss = 2.91986 avg_loss = 3.94871\n",
      "epoch no.0 train no.52690  loss = 6.10545 avg_loss = 3.93209\n",
      "epoch no.0 train no.52700  loss = 2.87222 avg_loss = 3.95470\n",
      "epoch no.0 train no.52710  loss = 8.32430 avg_loss = 3.95974\n",
      "epoch no.0 train no.52720  loss = 2.09366 avg_loss = 3.90942\n",
      "epoch no.0 train no.52730  loss = 5.53666 avg_loss = 3.91014\n",
      "epoch no.0 train no.52740  loss = 4.98008 avg_loss = 3.91107\n",
      "epoch no.0 train no.52750  loss = 5.09080 avg_loss = 3.96636\n",
      "epoch no.0 train no.52760  loss = 4.09219 avg_loss = 3.94739\n",
      "epoch no.0 train no.52770  loss = 4.56446 avg_loss = 3.94507\n",
      "epoch no.0 train no.52780  loss = 4.56715 avg_loss = 4.00588\n",
      "epoch no.0 train no.52790  loss = 5.00236 avg_loss = 4.03807\n",
      "epoch no.0 train no.52800  loss = 3.64802 avg_loss = 4.01741\n",
      "epoch no.0 train no.52810  loss = 4.28421 avg_loss = 4.04327\n",
      "epoch no.0 train no.52820  loss = 3.23970 avg_loss = 3.99439\n",
      "epoch no.0 train no.52830  loss = 3.52038 avg_loss = 3.95434\n",
      "epoch no.0 train no.52840  loss = 2.82083 avg_loss = 3.88987\n",
      "epoch no.0 train no.52850  loss = 5.90565 avg_loss = 3.92253\n",
      "epoch no.0 train no.52860  loss = 2.37012 avg_loss = 3.96591\n",
      "epoch no.0 train no.52870  loss = 2.63149 avg_loss = 3.94406\n",
      "epoch no.0 train no.52880  loss = 4.65401 avg_loss = 3.87879\n",
      "epoch no.0 train no.52890  loss = 4.30973 avg_loss = 3.89897\n",
      "epoch no.0 train no.52900  loss = 3.46635 avg_loss = 3.89985\n",
      "epoch no.0 train no.52910  loss = 2.98230 avg_loss = 3.86936\n",
      "epoch no.0 train no.52920  loss = 3.51078 avg_loss = 3.82709\n",
      "epoch no.0 train no.52930  loss = 5.15692 avg_loss = 3.86583\n",
      "epoch no.0 train no.52940  loss = 2.72266 avg_loss = 3.87845\n",
      "epoch no.0 train no.52950  loss = 2.13925 avg_loss = 3.88232\n",
      "epoch no.0 train no.52960  loss = 5.28253 avg_loss = 3.88496\n",
      "epoch no.0 train no.52970  loss = 2.23108 avg_loss = 3.86733\n",
      "epoch no.0 train no.52980  loss = 2.19333 avg_loss = 3.83846\n",
      "epoch no.0 train no.52990  loss = 5.45766 avg_loss = 3.87139\n",
      "epoch no.0 train no.53000  loss = 3.10993 avg_loss = 3.84033\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '에', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.0 train no.53010  loss = 4.01689 avg_loss = 3.85302\n",
      "epoch no.0 train no.53020  loss = 2.56199 avg_loss = 3.85006\n",
      "epoch no.0 train no.53030  loss = 3.06417 avg_loss = 3.84009\n",
      "epoch no.0 train no.53040  loss = 6.04157 avg_loss = 3.92001\n",
      "epoch no.0 train no.53050  loss = 3.52499 avg_loss = 3.95632\n",
      "epoch no.0 train no.53060  loss = 3.88779 avg_loss = 3.93141\n",
      "epoch no.0 train no.53070  loss = 2.38320 avg_loss = 3.93334\n",
      "epoch no.0 train no.53080  loss = 5.74020 avg_loss = 4.00595\n",
      "epoch no.0 train no.53090  loss = 3.14183 avg_loss = 4.03040\n",
      "epoch no.0 train no.53100  loss = 4.09680 avg_loss = 4.05828\n",
      "epoch no.0 train no.53110  loss = 3.62810 avg_loss = 4.04910\n",
      "epoch no.0 train no.53120  loss = 3.50413 avg_loss = 3.98552\n",
      "epoch no.0 train no.53130  loss = 3.30744 avg_loss = 3.95315\n",
      "epoch no.0 train no.53140  loss = 2.61721 avg_loss = 3.93587\n",
      "epoch no.0 train no.53150  loss = 3.56240 avg_loss = 3.91944\n",
      "epoch no.0 train no.53160  loss = 3.89802 avg_loss = 3.86975\n",
      "epoch no.0 train no.53170  loss = 2.59525 avg_loss = 3.88578\n",
      "epoch no.0 train no.53180  loss = 5.29282 avg_loss = 3.89264\n",
      "epoch no.0 train no.53190  loss = 3.47982 avg_loss = 3.91521\n",
      "epoch no.0 train no.53200  loss = 2.69299 avg_loss = 3.88915\n",
      "epoch no.0 train no.53210  loss = 2.15849 avg_loss = 3.84653\n",
      "epoch no.0 train no.53220  loss = 3.84118 avg_loss = 3.90558\n",
      "epoch no.0 train no.53230  loss = 3.24170 avg_loss = 3.89471\n",
      "epoch no.0 train no.53240  loss = 4.14843 avg_loss = 3.92302\n",
      "epoch no.0 train no.53250  loss = 3.58500 avg_loss = 3.92104\n",
      "epoch no.0 train no.53260  loss = 5.27643 avg_loss = 3.97607\n",
      "epoch no.0 train no.53270  loss = 2.77495 avg_loss = 3.99194\n",
      "epoch no.0 train no.53280  loss = 2.37295 avg_loss = 4.06345\n",
      "epoch no.0 train no.53290  loss = 5.48306 avg_loss = 4.08890\n",
      "epoch no.0 train no.53300  loss = 4.42787 avg_loss = 4.06618\n",
      "epoch no.0 train no.53310  loss = 2.77553 avg_loss = 4.08891\n",
      "epoch no.0 train no.53320  loss = 4.85067 avg_loss = 4.08878\n",
      "epoch no.0 train no.53330  loss = 2.35548 avg_loss = 4.07565\n",
      "epoch no.0 train no.53340  loss = 2.84576 avg_loss = 4.13026\n",
      "epoch no.0 train no.53350  loss = 2.08460 avg_loss = 4.10745\n",
      "epoch no.0 train no.53360  loss = 3.79403 avg_loss = 4.10826\n",
      "epoch no.0 train no.53370  loss = 2.92129 avg_loss = 4.07540\n",
      "epoch no.0 train no.53380  loss = 2.81758 avg_loss = 4.01162\n",
      "epoch no.0 train no.53390  loss = 3.23305 avg_loss = 4.03279\n",
      "epoch no.0 train no.53400  loss = 3.30697 avg_loss = 4.10552\n",
      "epoch no.0 train no.53410  loss = 5.48080 avg_loss = 4.07677\n",
      "epoch no.0 train no.53420  loss = 2.98175 avg_loss = 4.01758\n",
      "epoch no.0 train no.53430  loss = 5.14813 avg_loss = 4.03291\n",
      "epoch no.0 train no.53440  loss = 3.06624 avg_loss = 4.00962\n",
      "epoch no.0 train no.53450  loss = 4.76542 avg_loss = 4.04995\n",
      "epoch no.0 train no.53460  loss = 6.33323 avg_loss = 4.08032\n",
      "epoch no.0 train no.53470  loss = 5.83748 avg_loss = 4.06968\n",
      "epoch no.0 train no.53480  loss = 3.12763 avg_loss = 4.06976\n",
      "epoch no.0 train no.53490  loss = 4.65012 avg_loss = 4.01759\n",
      "epoch no.0 train no.53500  loss = 4.21701 avg_loss = 4.04821\n",
      "epoch no.0 train no.53510  loss = 2.18387 avg_loss = 4.02317\n",
      "epoch no.0 train no.53520  loss = 3.28962 avg_loss = 4.00565\n",
      "epoch no.0 train no.53530  loss = 4.67314 avg_loss = 4.00882\n",
      "epoch no.0 train no.53540  loss = 4.32637 avg_loss = 4.00624\n",
      "epoch no.0 train no.53550  loss = 6.85572 avg_loss = 3.96680\n",
      "epoch no.0 train no.53560  loss = 3.35005 avg_loss = 4.04240\n",
      "epoch no.0 train no.53570  loss = 3.60036 avg_loss = 3.98066\n",
      "epoch no.0 train no.53580  loss = 3.06580 avg_loss = 3.95634\n",
      "epoch no.0 train no.53590  loss = 4.90282 avg_loss = 3.98271\n",
      "epoch no.0 train no.53600  loss = 4.84877 avg_loss = 4.01054\n",
      "epoch no.0 train no.53610  loss = 4.12948 avg_loss = 4.03506\n",
      "epoch no.0 train no.53620  loss = 2.54617 avg_loss = 4.03726\n",
      "epoch no.0 train no.53630  loss = 5.39409 avg_loss = 4.02197\n",
      "epoch no.0 train no.53640  loss = 2.91450 avg_loss = 3.99197\n",
      "epoch no.0 train no.53650  loss = 2.53666 avg_loss = 3.94307\n",
      "epoch no.0 train no.53660  loss = 4.70378 avg_loss = 3.94880\n",
      "epoch no.0 train no.53670  loss = 3.54086 avg_loss = 3.98597\n",
      "epoch no.0 train no.53680  loss = 3.63880 avg_loss = 3.96097\n",
      "epoch no.0 train no.53690  loss = 2.63425 avg_loss = 3.95286\n",
      "epoch no.0 train no.53700  loss = 3.26429 avg_loss = 3.96400\n",
      "epoch no.0 train no.53710  loss = 3.70605 avg_loss = 3.92509\n",
      "epoch no.0 train no.53720  loss = 4.37286 avg_loss = 3.91229\n",
      "epoch no.0 train no.53730  loss = 3.51574 avg_loss = 3.92279\n",
      "epoch no.0 train no.53740  loss = 3.84497 avg_loss = 4.00517\n",
      "epoch no.0 train no.53750  loss = 3.16044 avg_loss = 4.03289\n",
      "epoch no.0 train no.53760  loss = 5.28996 avg_loss = 4.01517\n",
      "epoch no.0 train no.53770  loss = 2.71143 avg_loss = 3.96257\n",
      "epoch no.0 train no.53780  loss = 3.56737 avg_loss = 3.96818\n",
      "epoch no.0 train no.53790  loss = 1.91597 avg_loss = 3.94403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.53800  loss = 4.72864 avg_loss = 4.03188\n",
      "epoch no.0 train no.53810  loss = 4.01214 avg_loss = 3.99869\n",
      "epoch no.0 train no.53820  loss = 4.05273 avg_loss = 3.98323\n",
      "epoch no.0 train no.53830  loss = 2.72725 avg_loss = 3.98894\n",
      "epoch no.0 train no.53840  loss = 3.47812 avg_loss = 3.99057\n",
      "epoch no.0 train no.53850  loss = 2.71249 avg_loss = 3.98639\n",
      "epoch no.0 train no.53860  loss = 4.89125 avg_loss = 4.01218\n",
      "epoch no.0 train no.53870  loss = 3.29690 avg_loss = 3.98164\n",
      "epoch no.0 train no.53880  loss = 4.54658 avg_loss = 4.03210\n",
      "epoch no.0 train no.53890  loss = 2.95739 avg_loss = 3.99183\n",
      "epoch no.0 train no.53900  loss = 2.69469 avg_loss = 3.97421\n",
      "epoch no.0 train no.53910  loss = 4.04024 avg_loss = 3.99696\n",
      "epoch no.0 train no.53920  loss = 3.88635 avg_loss = 4.00135\n",
      "epoch no.0 train no.53930  loss = 4.28774 avg_loss = 3.95393\n",
      "epoch no.0 train no.53940  loss = 3.67827 avg_loss = 3.94703\n",
      "epoch no.0 train no.53950  loss = 5.12607 avg_loss = 3.90651\n",
      "epoch no.0 train no.53960  loss = 5.83102 avg_loss = 3.94860\n",
      "epoch no.0 train no.53970  loss = 3.33140 avg_loss = 3.96580\n",
      "epoch no.0 train no.53980  loss = 3.01824 avg_loss = 3.98468\n",
      "epoch no.0 train no.53990  loss = 6.31083 avg_loss = 3.96491\n",
      "epoch no.0 train no.54000  loss = 2.85978 avg_loss = 3.96317\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '에', '▁좋은', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 딱 좋은 음악</s>\n",
      "epoch no.0 train no.54010  loss = 3.76335 avg_loss = 3.97574\n",
      "epoch no.0 train no.54020  loss = 4.16178 avg_loss = 4.00998\n",
      "epoch no.0 train no.54030  loss = 2.06389 avg_loss = 4.02827\n",
      "epoch no.0 train no.54040  loss = 4.90693 avg_loss = 4.00624\n",
      "epoch no.0 train no.54050  loss = 4.83321 avg_loss = 4.01301\n",
      "epoch no.0 train no.54060  loss = 4.82247 avg_loss = 3.96981\n",
      "epoch no.0 train no.54070  loss = 3.54266 avg_loss = 3.96212\n",
      "epoch no.0 train no.54080  loss = 3.67258 avg_loss = 3.98228\n",
      "epoch no.0 train no.54090  loss = 5.61781 avg_loss = 4.00757\n",
      "epoch no.0 train no.54100  loss = 5.34430 avg_loss = 4.01318\n",
      "epoch no.0 train no.54110  loss = 6.05113 avg_loss = 4.04566\n",
      "epoch no.0 train no.54120  loss = 2.86484 avg_loss = 4.03875\n",
      "epoch no.0 train no.54130  loss = 3.82124 avg_loss = 4.06055\n",
      "epoch no.0 train no.54140  loss = 1.52335 avg_loss = 4.00336\n",
      "epoch no.0 train no.54150  loss = 5.05449 avg_loss = 3.99669\n",
      "epoch no.0 train no.54160  loss = 4.17767 avg_loss = 4.00025\n",
      "epoch no.0 train no.54170  loss = 3.12200 avg_loss = 3.95900\n",
      "epoch no.0 train no.54180  loss = 4.22341 avg_loss = 3.95291\n",
      "epoch no.0 train no.54190  loss = 4.34928 avg_loss = 3.94344\n",
      "epoch no.0 train no.54200  loss = 4.28291 avg_loss = 3.94156\n",
      "epoch no.0 train no.54210  loss = 4.01906 avg_loss = 3.99838\n",
      "epoch no.0 train no.54220  loss = 1.96576 avg_loss = 4.01292\n",
      "epoch no.0 train no.54230  loss = 4.55718 avg_loss = 4.03285\n",
      "epoch no.0 train no.54240  loss = 1.92929 avg_loss = 4.02863\n",
      "epoch no.0 train no.54250  loss = 4.25832 avg_loss = 4.01034\n",
      "epoch no.0 train no.54260  loss = 5.68955 avg_loss = 4.06167\n",
      "epoch no.0 train no.54270  loss = 5.41338 avg_loss = 4.06293\n",
      "epoch no.0 train no.54280  loss = 4.81659 avg_loss = 4.06516\n",
      "epoch no.0 train no.54290  loss = 6.19918 avg_loss = 4.07045\n",
      "epoch no.0 train no.54300  loss = 6.90959 avg_loss = 4.11480\n",
      "epoch no.0 train no.54310  loss = 4.42351 avg_loss = 4.12771\n",
      "epoch no.0 train no.54320  loss = 2.81545 avg_loss = 4.12093\n",
      "epoch no.0 train no.54330  loss = 5.01061 avg_loss = 4.07248\n",
      "epoch no.0 train no.54340  loss = 2.20546 avg_loss = 4.04484\n",
      "epoch no.0 train no.54350  loss = 2.38800 avg_loss = 4.05396\n",
      "epoch no.0 train no.54360  loss = 3.73154 avg_loss = 4.03588\n",
      "epoch no.0 train no.54370  loss = 2.21954 avg_loss = 4.00779\n",
      "epoch no.0 train no.54380  loss = 3.61871 avg_loss = 4.00412\n",
      "epoch no.0 train no.54390  loss = 2.75698 avg_loss = 3.96534\n",
      "epoch no.0 train no.54400  loss = 3.34232 avg_loss = 3.93606\n",
      "epoch no.0 train no.54410  loss = 4.46064 avg_loss = 3.94746\n",
      "epoch no.0 train no.54420  loss = 2.76758 avg_loss = 3.96302\n",
      "epoch no.0 train no.54430  loss = 2.97938 avg_loss = 3.98551\n",
      "epoch no.0 train no.54440  loss = 3.37943 avg_loss = 3.98583\n",
      "epoch no.0 train no.54450  loss = 3.30700 avg_loss = 3.98900\n",
      "epoch no.0 train no.54460  loss = 3.38978 avg_loss = 3.99014\n",
      "epoch no.0 train no.54470  loss = 2.87593 avg_loss = 3.95217\n",
      "epoch no.0 train no.54480  loss = 3.65222 avg_loss = 3.97270\n",
      "epoch no.0 train no.54490  loss = 3.67178 avg_loss = 3.96535\n",
      "epoch no.0 train no.54500  loss = 3.91139 avg_loss = 4.03826\n",
      "epoch no.0 train no.54510  loss = 5.02835 avg_loss = 4.05711\n",
      "epoch no.0 train no.54520  loss = 5.90775 avg_loss = 4.06305\n",
      "epoch no.0 train no.54530  loss = 2.51284 avg_loss = 4.03568\n",
      "epoch no.0 train no.54540  loss = 3.88826 avg_loss = 4.01855\n",
      "epoch no.0 train no.54550  loss = 2.66410 avg_loss = 3.99359\n",
      "epoch no.0 train no.54560  loss = 2.85507 avg_loss = 3.96217\n",
      "epoch no.0 train no.54570  loss = 2.19270 avg_loss = 3.98832\n",
      "epoch no.0 train no.54580  loss = 3.42388 avg_loss = 4.03243\n",
      "epoch no.0 train no.54590  loss = 3.74460 avg_loss = 4.05945\n",
      "epoch no.0 train no.54600  loss = 2.89219 avg_loss = 4.02837\n",
      "epoch no.0 train no.54610  loss = 4.01100 avg_loss = 4.02910\n",
      "epoch no.0 train no.54620  loss = 5.71833 avg_loss = 4.04303\n",
      "epoch no.0 train no.54630  loss = 3.99305 avg_loss = 4.10273\n",
      "epoch no.0 train no.54640  loss = 4.94017 avg_loss = 4.15243\n",
      "epoch no.0 train no.54650  loss = 4.56899 avg_loss = 4.12251\n",
      "epoch no.0 train no.54660  loss = 5.98832 avg_loss = 4.17466\n",
      "epoch no.0 train no.54670  loss = 4.73009 avg_loss = 4.16272\n",
      "epoch no.0 train no.54680  loss = 6.42949 avg_loss = 4.11491\n",
      "epoch no.0 train no.54690  loss = 4.28277 avg_loss = 4.11619\n",
      "epoch no.0 train no.54700  loss = 4.09876 avg_loss = 4.15882\n",
      "epoch no.0 train no.54710  loss = 3.95662 avg_loss = 4.10850\n",
      "epoch no.0 train no.54720  loss = 4.66043 avg_loss = 4.09317\n",
      "epoch no.0 train no.54730  loss = 6.63391 avg_loss = 4.17263\n",
      "epoch no.0 train no.54740  loss = 4.49956 avg_loss = 4.12718\n",
      "epoch no.0 train no.54750  loss = 2.70807 avg_loss = 4.11105\n",
      "epoch no.0 train no.54760  loss = 3.62118 avg_loss = 4.12058\n",
      "epoch no.0 train no.54770  loss = 3.47408 avg_loss = 4.07160\n",
      "epoch no.0 train no.54780  loss = 3.59259 avg_loss = 4.06718\n",
      "epoch no.0 train no.54790  loss = 3.14991 avg_loss = 4.07606\n",
      "epoch no.0 train no.54800  loss = 3.38385 avg_loss = 4.15460\n",
      "epoch no.0 train no.54810  loss = 4.34561 avg_loss = 4.16378\n",
      "epoch no.0 train no.54820  loss = 5.32862 avg_loss = 4.14278\n",
      "epoch no.0 train no.54830  loss = 3.57406 avg_loss = 4.18467\n",
      "epoch no.0 train no.54840  loss = 5.76987 avg_loss = 4.16629\n",
      "epoch no.0 train no.54850  loss = 5.58728 avg_loss = 4.13825\n",
      "epoch no.0 train no.54860  loss = 2.51357 avg_loss = 4.11682\n",
      "epoch no.0 train no.54870  loss = 3.35308 avg_loss = 4.12695\n",
      "epoch no.0 train no.54880  loss = 4.02928 avg_loss = 4.18347\n",
      "epoch no.0 train no.54890  loss = 2.46979 avg_loss = 4.16050\n",
      "epoch no.0 train no.54900  loss = 2.98089 avg_loss = 4.09246\n",
      "epoch no.0 train no.54910  loss = 5.79881 avg_loss = 4.11494\n",
      "epoch no.0 train no.54920  loss = 4.94982 avg_loss = 4.18180\n",
      "epoch no.0 train no.54930  loss = 5.10364 avg_loss = 4.17363\n",
      "epoch no.0 train no.54940  loss = 3.33604 avg_loss = 4.12805\n",
      "epoch no.0 train no.54950  loss = 4.58773 avg_loss = 4.09950\n",
      "epoch no.0 train no.54960  loss = 2.96491 avg_loss = 4.04459\n",
      "epoch no.0 train no.54970  loss = 2.17636 avg_loss = 4.03698\n",
      "epoch no.0 train no.54980  loss = 2.90764 avg_loss = 4.07704\n",
      "epoch no.0 train no.54990  loss = 2.12267 avg_loss = 4.06846\n",
      "epoch no.0 train no.55000  loss = 5.68230 avg_loss = 4.02585\n",
      "5\n",
      "to_tokens: ['▁비', '좋은', '이', '▁딱', '▁좋은', '▁노래', '음악', '</s>']\n",
      "기분전환에 딱 좋은 인디음악</s>\n",
      "epoch no.0 train no.55010  loss = 3.75831 avg_loss = 4.07593\n",
      "epoch no.0 train no.55020  loss = 3.46889 avg_loss = 4.00509\n",
      "epoch no.0 train no.55030  loss = 3.43303 avg_loss = 3.99827\n",
      "epoch no.0 train no.55040  loss = 2.35303 avg_loss = 3.95649\n",
      "epoch no.0 train no.55050  loss = 3.11823 avg_loss = 3.90585\n",
      "epoch no.0 train no.55060  loss = 4.52764 avg_loss = 3.92450\n",
      "epoch no.0 train no.55070  loss = 3.05185 avg_loss = 3.90489\n",
      "epoch no.0 train no.55080  loss = 4.82418 avg_loss = 3.88177\n",
      "epoch no.0 train no.55090  loss = 2.60450 avg_loss = 3.86933\n",
      "epoch no.0 train no.55100  loss = 5.26268 avg_loss = 3.90270\n",
      "epoch no.0 train no.55110  loss = 4.03034 avg_loss = 3.97461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.55120  loss = 5.61386 avg_loss = 3.99088\n",
      "epoch no.0 train no.55130  loss = 4.57054 avg_loss = 4.02968\n",
      "epoch no.0 train no.55140  loss = 2.31567 avg_loss = 4.02601\n",
      "epoch no.0 train no.55150  loss = 1.88026 avg_loss = 4.04024\n",
      "epoch no.0 train no.55160  loss = 5.13507 avg_loss = 4.04729\n",
      "epoch no.0 train no.55170  loss = 4.68714 avg_loss = 4.02917\n",
      "epoch no.0 train no.55180  loss = 4.02441 avg_loss = 4.02033\n",
      "epoch no.0 train no.55190  loss = 3.05323 avg_loss = 4.02209\n",
      "epoch no.0 train no.55200  loss = 5.32963 avg_loss = 4.01102\n",
      "epoch no.0 train no.55210  loss = 4.91868 avg_loss = 3.97640\n",
      "epoch no.0 train no.55220  loss = 3.78146 avg_loss = 3.96677\n",
      "epoch no.0 train no.55230  loss = 3.88901 avg_loss = 3.95576\n",
      "epoch no.0 train no.55240  loss = 6.44320 avg_loss = 3.90199\n",
      "epoch no.0 train no.55250  loss = 4.48742 avg_loss = 3.82829\n",
      "epoch no.0 train no.55260  loss = 2.61951 avg_loss = 3.81925\n",
      "epoch no.0 train no.55270  loss = 5.61531 avg_loss = 3.83496\n",
      "epoch no.0 train no.55280  loss = 1.55955 avg_loss = 3.78726\n",
      "epoch no.0 train no.55290  loss = 3.83734 avg_loss = 3.77063\n",
      "epoch no.0 train no.55300  loss = 2.78094 avg_loss = 3.77203\n",
      "epoch no.0 train no.55310  loss = 3.50091 avg_loss = 3.77874\n",
      "epoch no.0 train no.55320  loss = 3.67615 avg_loss = 3.76498\n",
      "epoch no.0 train no.55330  loss = 4.08040 avg_loss = 3.75566\n",
      "epoch no.0 train no.55340  loss = 4.15556 avg_loss = 3.82770\n",
      "epoch no.0 train no.55350  loss = 3.58930 avg_loss = 3.85676\n",
      "epoch no.0 train no.55360  loss = 3.79111 avg_loss = 3.84736\n",
      "epoch no.0 train no.55370  loss = 2.56485 avg_loss = 3.89418\n",
      "epoch no.0 train no.55380  loss = 4.75605 avg_loss = 3.89494\n",
      "epoch no.0 train no.55390  loss = 4.48361 avg_loss = 3.94087\n",
      "epoch no.0 train no.55400  loss = 2.27225 avg_loss = 3.92816\n",
      "epoch no.0 train no.55410  loss = 4.08696 avg_loss = 3.90352\n",
      "epoch no.0 train no.55420  loss = 5.14611 avg_loss = 3.88789\n",
      "epoch no.0 train no.55430  loss = 4.98605 avg_loss = 3.84628\n",
      "epoch no.0 train no.55440  loss = 5.58332 avg_loss = 3.83530\n",
      "epoch no.0 train no.55450  loss = 2.20977 avg_loss = 3.87872\n",
      "epoch no.0 train no.55460  loss = 3.58532 avg_loss = 3.90513\n",
      "epoch no.0 train no.55470  loss = 3.28064 avg_loss = 3.88170\n",
      "epoch no.0 train no.55480  loss = 4.04357 avg_loss = 3.86487\n",
      "epoch no.0 train no.55490  loss = 5.34855 avg_loss = 3.94023\n",
      "epoch no.0 train no.55500  loss = 3.56759 avg_loss = 3.96755\n",
      "epoch no.0 train no.55510  loss = 5.71410 avg_loss = 3.96960\n",
      "epoch no.0 train no.55520  loss = 5.03650 avg_loss = 4.04223\n",
      "epoch no.0 train no.55530  loss = 3.35656 avg_loss = 3.96584\n",
      "epoch no.0 train no.55540  loss = 2.77825 avg_loss = 3.94426\n",
      "epoch no.0 train no.55550  loss = 3.26980 avg_loss = 3.96374\n",
      "epoch no.0 train no.55560  loss = 3.42685 avg_loss = 3.98000\n",
      "epoch no.0 train no.55570  loss = 4.07330 avg_loss = 3.93676\n",
      "epoch no.0 train no.55580  loss = 4.70881 avg_loss = 3.96371\n",
      "epoch no.0 train no.55590  loss = 5.28039 avg_loss = 3.93661\n",
      "epoch no.0 train no.55600  loss = 5.80217 avg_loss = 4.02422\n",
      "epoch no.0 train no.55610  loss = 1.82070 avg_loss = 3.98932\n",
      "epoch no.0 train no.55620  loss = 5.32068 avg_loss = 3.91390\n",
      "epoch no.0 train no.55630  loss = 3.36675 avg_loss = 3.89303\n",
      "epoch no.0 train no.55640  loss = 3.03204 avg_loss = 3.86175\n",
      "epoch no.0 train no.55650  loss = 4.58791 avg_loss = 3.91045\n",
      "epoch no.0 train no.55660  loss = 3.73770 avg_loss = 3.92684\n",
      "epoch no.0 train no.55670  loss = 5.20187 avg_loss = 4.03579\n",
      "epoch no.0 train no.55680  loss = 2.76514 avg_loss = 3.96595\n",
      "epoch no.0 train no.55690  loss = 5.31853 avg_loss = 3.97713\n",
      "epoch no.0 train no.55700  loss = 3.74882 avg_loss = 3.93860\n",
      "epoch no.0 train no.55710  loss = 4.51142 avg_loss = 3.91930\n",
      "epoch no.0 train no.55720  loss = 4.49851 avg_loss = 3.88237\n",
      "epoch no.0 train no.55730  loss = 5.57412 avg_loss = 3.94855\n",
      "epoch no.0 train no.55740  loss = 2.62496 avg_loss = 3.97219\n",
      "epoch no.0 train no.55750  loss = 4.46705 avg_loss = 3.94106\n",
      "epoch no.0 train no.55760  loss = 4.44543 avg_loss = 3.88520\n",
      "epoch no.0 train no.55770  loss = 3.44278 avg_loss = 3.89248\n",
      "epoch no.0 train no.55780  loss = 3.35554 avg_loss = 3.90155\n",
      "epoch no.0 train no.55790  loss = 2.82162 avg_loss = 3.91885\n",
      "epoch no.0 train no.55800  loss = 2.33793 avg_loss = 3.89582\n",
      "epoch no.0 train no.55810  loss = 3.04660 avg_loss = 3.92004\n",
      "epoch no.0 train no.55820  loss = 3.05346 avg_loss = 3.90810\n",
      "epoch no.0 train no.55830  loss = 3.31951 avg_loss = 3.93164\n",
      "epoch no.0 train no.55840  loss = 2.48838 avg_loss = 3.99109\n",
      "epoch no.0 train no.55850  loss = 3.51554 avg_loss = 3.98623\n",
      "epoch no.0 train no.55860  loss = 2.68001 avg_loss = 3.95520\n",
      "epoch no.0 train no.55870  loss = 4.39349 avg_loss = 3.93776\n",
      "epoch no.0 train no.55880  loss = 4.87670 avg_loss = 3.97882\n",
      "epoch no.0 train no.55890  loss = 3.36997 avg_loss = 4.00143\n",
      "epoch no.0 train no.55900  loss = 3.99931 avg_loss = 3.99477\n",
      "epoch no.0 train no.55910  loss = 4.20741 avg_loss = 4.04303\n",
      "epoch no.0 train no.55920  loss = 3.36136 avg_loss = 4.08769\n",
      "epoch no.0 train no.55930  loss = 2.87569 avg_loss = 4.02649\n",
      "epoch no.0 train no.55940  loss = 3.36446 avg_loss = 3.99152\n",
      "epoch no.0 train no.55950  loss = 3.02608 avg_loss = 3.93346\n",
      "epoch no.0 train no.55960  loss = 1.85863 avg_loss = 3.90633\n",
      "epoch no.0 train no.55970  loss = 4.65361 avg_loss = 3.88788\n",
      "epoch no.0 train no.55980  loss = 3.34662 avg_loss = 3.87715\n",
      "epoch no.0 train no.55990  loss = 2.34316 avg_loss = 3.86749\n",
      "epoch no.0 train no.56000  loss = 2.62091 avg_loss = 3.87202\n",
      "4\n",
      "to_tokens: ['▁비', '좋은', '이', '▁좋은', '▁노래', '송', '</s>']\n",
      "기분전환하기 좋은 팝송</s>\n",
      "epoch no.0 train no.56010  loss = 6.13429 avg_loss = 3.87187\n",
      "epoch no.0 train no.56020  loss = 5.01533 avg_loss = 3.89532\n",
      "epoch no.0 train no.56030  loss = 3.27481 avg_loss = 3.88065\n",
      "epoch no.0 train no.56040  loss = 3.18484 avg_loss = 3.91933\n",
      "epoch no.0 train no.56050  loss = 2.22262 avg_loss = 3.91021\n",
      "epoch no.0 train no.56060  loss = 4.03990 avg_loss = 3.94757\n",
      "epoch no.0 train no.56070  loss = 4.05835 avg_loss = 3.91036\n",
      "epoch no.0 train no.56080  loss = 4.84854 avg_loss = 3.88921\n",
      "epoch no.0 train no.56090  loss = 4.68335 avg_loss = 3.91672\n",
      "epoch no.0 train no.56100  loss = 3.09890 avg_loss = 3.90488\n",
      "epoch no.0 train no.56110  loss = 3.71794 avg_loss = 3.96895\n",
      "epoch no.0 train no.56120  loss = 2.85458 avg_loss = 3.93820\n",
      "epoch no.0 train no.56130  loss = 4.61441 avg_loss = 3.96280\n",
      "epoch no.0 train no.56140  loss = 4.96452 avg_loss = 4.00528\n",
      "epoch no.0 train no.56150  loss = 4.86618 avg_loss = 3.94355\n",
      "epoch no.0 train no.56160  loss = 5.52714 avg_loss = 4.00099\n",
      "epoch no.0 train no.56170  loss = 4.22461 avg_loss = 4.00783\n",
      "epoch no.0 train no.56180  loss = 5.10952 avg_loss = 4.00989\n",
      "epoch no.0 train no.56190  loss = 3.52035 avg_loss = 4.01555\n",
      "epoch no.0 train no.56200  loss = 2.04420 avg_loss = 3.98603\n",
      "epoch no.0 train no.56210  loss = 2.08069 avg_loss = 3.93551\n",
      "epoch no.0 train no.56220  loss = 3.94294 avg_loss = 3.85098\n",
      "epoch no.0 train no.56230  loss = 4.80250 avg_loss = 3.83445\n",
      "epoch no.0 train no.56240  loss = 5.25950 avg_loss = 3.89774\n",
      "epoch no.0 train no.56250  loss = 2.87818 avg_loss = 3.93434\n",
      "epoch no.0 train no.56260  loss = 3.03664 avg_loss = 3.99400\n",
      "epoch no.0 train no.56270  loss = 3.76530 avg_loss = 3.96770\n",
      "epoch no.0 train no.56280  loss = 3.34284 avg_loss = 3.99990\n",
      "epoch no.0 train no.56290  loss = 3.47658 avg_loss = 4.03331\n",
      "epoch no.0 train no.56300  loss = 4.36039 avg_loss = 4.01506\n",
      "epoch no.0 train no.56310  loss = 2.38559 avg_loss = 4.02219\n",
      "epoch no.0 train no.56320  loss = 3.24970 avg_loss = 4.00073\n",
      "epoch no.0 train no.56330  loss = 3.24174 avg_loss = 3.98138\n",
      "epoch no.0 train no.56340  loss = 7.50067 avg_loss = 3.99735\n",
      "epoch no.0 train no.56350  loss = 3.78376 avg_loss = 3.97050\n",
      "epoch no.0 train no.56360  loss = 4.71953 avg_loss = 3.97155\n",
      "epoch no.0 train no.56370  loss = 2.62161 avg_loss = 3.92531\n",
      "epoch no.0 train no.56380  loss = 3.84108 avg_loss = 3.90580\n",
      "epoch no.0 train no.56390  loss = 1.65712 avg_loss = 3.89909\n",
      "epoch no.0 train no.56400  loss = 4.17695 avg_loss = 3.93993\n",
      "epoch no.0 train no.56410  loss = 2.32924 avg_loss = 3.90494\n",
      "epoch no.0 train no.56420  loss = 4.36856 avg_loss = 3.90113\n",
      "epoch no.0 train no.56430  loss = 4.30380 avg_loss = 3.91642\n",
      "epoch no.0 train no.56440  loss = 5.79100 avg_loss = 3.91025\n",
      "epoch no.0 train no.56450  loss = 6.99236 avg_loss = 3.97146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.56460  loss = 2.25086 avg_loss = 3.90429\n",
      "epoch no.0 train no.56470  loss = 2.12399 avg_loss = 3.93643\n",
      "epoch no.0 train no.56480  loss = 3.83066 avg_loss = 3.94626\n",
      "epoch no.0 train no.56490  loss = 3.51348 avg_loss = 3.99531\n",
      "epoch no.0 train no.56500  loss = 3.18126 avg_loss = 3.97977\n",
      "epoch no.0 train no.56510  loss = 5.47611 avg_loss = 3.99751\n",
      "epoch no.0 train no.56520  loss = 4.30023 avg_loss = 3.94723\n",
      "epoch no.0 train no.56530  loss = 5.86247 avg_loss = 3.92564\n",
      "epoch no.0 train no.56540  loss = 5.28630 avg_loss = 3.97484\n",
      "epoch no.0 train no.56550  loss = 4.99562 avg_loss = 4.00178\n",
      "epoch no.0 train no.56560  loss = 3.05158 avg_loss = 3.98484\n",
      "epoch no.0 train no.56570  loss = 4.83801 avg_loss = 3.95818\n",
      "epoch no.0 train no.56580  loss = 5.80979 avg_loss = 3.99400\n",
      "epoch no.0 train no.56590  loss = 3.21467 avg_loss = 4.05158\n",
      "epoch no.0 train no.56600  loss = 3.41413 avg_loss = 4.03197\n",
      "epoch no.0 train no.56610  loss = 3.52343 avg_loss = 4.00953\n",
      "epoch no.0 train no.56620  loss = 2.91274 avg_loss = 4.03046\n",
      "epoch no.0 train no.56630  loss = 2.84948 avg_loss = 3.97752\n",
      "epoch no.0 train no.56640  loss = 3.12135 avg_loss = 3.95621\n",
      "epoch no.0 train no.56650  loss = 3.80723 avg_loss = 3.93524\n",
      "epoch no.0 train no.56660  loss = 4.87851 avg_loss = 3.93129\n",
      "epoch no.0 train no.56670  loss = 6.27453 avg_loss = 3.99140\n",
      "epoch no.0 train no.56680  loss = 4.54628 avg_loss = 4.02001\n",
      "epoch no.0 train no.56690  loss = 2.49019 avg_loss = 3.95090\n",
      "epoch no.0 train no.56700  loss = 4.73908 avg_loss = 3.95812\n",
      "epoch no.0 train no.56710  loss = 3.92517 avg_loss = 3.90667\n",
      "epoch no.0 train no.56720  loss = 2.80916 avg_loss = 3.90294\n",
      "epoch no.0 train no.56730  loss = 3.05290 avg_loss = 3.84098\n",
      "epoch no.0 train no.56740  loss = 3.81361 avg_loss = 3.89586\n",
      "epoch no.0 train no.56750  loss = 4.97224 avg_loss = 3.95052\n",
      "epoch no.0 train no.56760  loss = 3.35030 avg_loss = 3.97564\n",
      "epoch no.0 train no.56770  loss = 4.29886 avg_loss = 3.96078\n",
      "epoch no.0 train no.56780  loss = 2.75740 avg_loss = 3.91620\n",
      "epoch no.0 train no.56790  loss = 5.09225 avg_loss = 3.92198\n",
      "epoch no.0 train no.56800  loss = 5.64664 avg_loss = 3.91825\n",
      "epoch no.0 train no.56810  loss = 4.02775 avg_loss = 3.89214\n",
      "epoch no.0 train no.56820  loss = 1.84119 avg_loss = 3.86026\n",
      "epoch no.0 train no.56830  loss = 3.27420 avg_loss = 3.85750\n",
      "epoch no.0 train no.56840  loss = 4.59053 avg_loss = 3.88012\n",
      "epoch no.0 train no.56850  loss = 4.05090 avg_loss = 3.85045\n",
      "epoch no.0 train no.56860  loss = 4.10503 avg_loss = 3.87375\n",
      "epoch no.0 train no.56870  loss = 2.70825 avg_loss = 3.81416\n",
      "epoch no.0 train no.56880  loss = 4.07192 avg_loss = 3.85253\n",
      "epoch no.0 train no.56890  loss = 4.10347 avg_loss = 3.87944\n",
      "epoch no.0 train no.56900  loss = 3.07700 avg_loss = 3.86097\n",
      "epoch no.0 train no.56910  loss = 3.90942 avg_loss = 3.83056\n",
      "epoch no.0 train no.56920  loss = 4.09096 avg_loss = 3.83317\n",
      "epoch no.0 train no.56930  loss = 3.38656 avg_loss = 3.86225\n",
      "epoch no.0 train no.56940  loss = 2.49576 avg_loss = 3.80483\n",
      "epoch no.0 train no.56950  loss = 3.44382 avg_loss = 3.83872\n",
      "epoch no.0 train no.56960  loss = 3.36565 avg_loss = 3.94850\n",
      "epoch no.0 train no.56970  loss = 4.01936 avg_loss = 3.92461\n",
      "epoch no.0 train no.56980  loss = 3.59020 avg_loss = 3.92605\n",
      "epoch no.0 train no.56990  loss = 2.93971 avg_loss = 3.93957\n",
      "epoch no.0 train no.57000  loss = 3.14851 avg_loss = 3.93132\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '▁딱', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 좋은 음악</s>\n",
      "epoch no.0 train no.57010  loss = 4.89901 avg_loss = 3.94246\n",
      "epoch no.0 train no.57020  loss = 5.54586 avg_loss = 3.92996\n",
      "epoch no.0 train no.57030  loss = 3.30900 avg_loss = 3.94724\n",
      "epoch no.0 train no.57040  loss = 4.63025 avg_loss = 3.97659\n",
      "epoch no.0 train no.57050  loss = 4.87772 avg_loss = 3.98382\n",
      "epoch no.0 train no.57060  loss = 2.68874 avg_loss = 3.98468\n",
      "epoch no.0 train no.57070  loss = 3.34545 avg_loss = 3.99772\n",
      "epoch no.0 train no.57080  loss = 6.23977 avg_loss = 3.98807\n",
      "epoch no.0 train no.57090  loss = 3.53062 avg_loss = 4.00760\n",
      "epoch no.0 train no.57100  loss = 4.10809 avg_loss = 3.96851\n",
      "epoch no.0 train no.57110  loss = 3.07575 avg_loss = 3.95060\n",
      "epoch no.0 train no.57120  loss = 3.77607 avg_loss = 4.00407\n",
      "epoch no.0 train no.57130  loss = 3.83754 avg_loss = 3.95436\n",
      "epoch no.0 train no.57140  loss = 3.78877 avg_loss = 3.94209\n",
      "epoch no.0 train no.57150  loss = 3.62391 avg_loss = 3.95287\n",
      "epoch no.0 train no.57160  loss = 2.95375 avg_loss = 3.93569\n",
      "epoch no.0 train no.57170  loss = 5.02445 avg_loss = 3.97266\n",
      "epoch no.0 train no.57180  loss = 4.16426 avg_loss = 3.96173\n",
      "epoch no.0 train no.57190  loss = 2.25084 avg_loss = 3.97409\n",
      "epoch no.0 train no.57200  loss = 5.20378 avg_loss = 4.00847\n",
      "epoch no.0 train no.57210  loss = 3.46920 avg_loss = 4.01828\n",
      "epoch no.0 train no.57220  loss = 4.83154 avg_loss = 4.08456\n",
      "epoch no.0 train no.57230  loss = 4.95143 avg_loss = 4.07905\n",
      "epoch no.0 train no.57240  loss = 4.60366 avg_loss = 4.04644\n",
      "epoch no.0 train no.57250  loss = 4.37748 avg_loss = 4.07458\n",
      "epoch no.0 train no.57260  loss = 4.75719 avg_loss = 4.05447\n",
      "epoch no.0 train no.57270  loss = 4.75635 avg_loss = 4.10096\n",
      "epoch no.0 train no.57280  loss = 2.91446 avg_loss = 4.12155\n",
      "epoch no.0 train no.57290  loss = 3.34381 avg_loss = 4.01493\n",
      "epoch no.0 train no.57300  loss = 3.67021 avg_loss = 4.09142\n",
      "epoch no.0 train no.57310  loss = 4.15463 avg_loss = 4.02504\n",
      "epoch no.0 train no.57320  loss = 4.52959 avg_loss = 4.08248\n",
      "epoch no.0 train no.57330  loss = 3.45665 avg_loss = 4.07492\n",
      "epoch no.0 train no.57340  loss = 2.06377 avg_loss = 4.03833\n",
      "epoch no.0 train no.57350  loss = 4.92337 avg_loss = 4.03295\n",
      "epoch no.0 train no.57360  loss = 4.74264 avg_loss = 4.04033\n",
      "epoch no.0 train no.57370  loss = 4.68843 avg_loss = 3.99760\n",
      "epoch no.0 train no.57380  loss = 5.32198 avg_loss = 4.03871\n",
      "epoch no.0 train no.57390  loss = 2.62190 avg_loss = 4.02437\n",
      "epoch no.0 train no.57400  loss = 2.12825 avg_loss = 4.02918\n",
      "epoch no.0 train no.57410  loss = 3.00564 avg_loss = 3.94575\n",
      "epoch no.0 train no.57420  loss = 4.20632 avg_loss = 3.93651\n",
      "epoch no.0 train no.57430  loss = 3.87412 avg_loss = 3.98878\n",
      "epoch no.0 train no.57440  loss = 2.32622 avg_loss = 3.94244\n",
      "epoch no.0 train no.57450  loss = 5.48782 avg_loss = 3.95201\n",
      "epoch no.0 train no.57460  loss = 3.75291 avg_loss = 3.92514\n",
      "epoch no.0 train no.57470  loss = 2.51291 avg_loss = 3.93357\n",
      "epoch no.0 train no.57480  loss = 5.00606 avg_loss = 3.96642\n",
      "epoch no.0 train no.57490  loss = 4.14336 avg_loss = 4.00176\n",
      "epoch no.0 train no.57500  loss = 4.75361 avg_loss = 3.97367\n",
      "epoch no.0 train no.57510  loss = 5.37681 avg_loss = 3.94527\n",
      "epoch no.0 train no.57520  loss = 6.55391 avg_loss = 3.91411\n",
      "epoch no.0 train no.57530  loss = 2.98862 avg_loss = 3.89358\n",
      "epoch no.0 train no.57540  loss = 5.50288 avg_loss = 3.93355\n",
      "epoch no.0 train no.57550  loss = 2.51165 avg_loss = 3.88191\n",
      "epoch no.0 train no.57560  loss = 3.23874 avg_loss = 3.88244\n",
      "epoch no.0 train no.57570  loss = 4.30663 avg_loss = 3.88878\n",
      "epoch no.0 train no.57580  loss = 3.76498 avg_loss = 3.91618\n",
      "epoch no.0 train no.57590  loss = 6.11502 avg_loss = 3.97596\n",
      "epoch no.0 train no.57600  loss = 2.00219 avg_loss = 3.95389\n",
      "epoch no.0 train no.57610  loss = 2.64043 avg_loss = 3.88394\n",
      "epoch no.0 train no.57620  loss = 5.56286 avg_loss = 3.89231\n",
      "epoch no.0 train no.57630  loss = 2.43542 avg_loss = 3.90317\n",
      "epoch no.0 train no.57640  loss = 5.40649 avg_loss = 3.86214\n",
      "epoch no.0 train no.57650  loss = 6.28777 avg_loss = 3.87445\n",
      "epoch no.0 train no.57660  loss = 2.79617 avg_loss = 3.86428\n",
      "epoch no.0 train no.57670  loss = 4.57950 avg_loss = 3.87107\n",
      "epoch no.0 train no.57680  loss = 3.46793 avg_loss = 3.90127\n",
      "epoch no.0 train no.57690  loss = 2.22704 avg_loss = 3.94418\n",
      "epoch no.0 train no.57700  loss = 3.16951 avg_loss = 3.88291\n",
      "epoch no.0 train no.57710  loss = 4.30202 avg_loss = 3.88957\n",
      "epoch no.0 train no.57720  loss = 2.72190 avg_loss = 3.94118\n",
      "epoch no.0 train no.57730  loss = 4.67170 avg_loss = 3.94636\n",
      "epoch no.0 train no.57740  loss = 2.34150 avg_loss = 3.95492\n",
      "epoch no.0 train no.57750  loss = 5.27596 avg_loss = 3.93150\n",
      "epoch no.0 train no.57760  loss = 3.25229 avg_loss = 3.96599\n",
      "epoch no.0 train no.57770  loss = 3.44951 avg_loss = 3.94662\n",
      "epoch no.0 train no.57780  loss = 2.10244 avg_loss = 3.91559\n",
      "epoch no.0 train no.57790  loss = 7.01198 avg_loss = 3.93092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.57800  loss = 2.60264 avg_loss = 3.89574\n",
      "epoch no.0 train no.57810  loss = 6.65933 avg_loss = 3.87495\n",
      "epoch no.0 train no.57820  loss = 4.32909 avg_loss = 3.87480\n",
      "epoch no.0 train no.57830  loss = 5.07665 avg_loss = 3.88997\n",
      "epoch no.0 train no.57840  loss = 3.80126 avg_loss = 3.92999\n",
      "epoch no.0 train no.57850  loss = 4.30835 avg_loss = 3.97895\n",
      "epoch no.0 train no.57860  loss = 3.30424 avg_loss = 3.93692\n",
      "epoch no.0 train no.57870  loss = 3.74312 avg_loss = 3.97998\n",
      "epoch no.0 train no.57880  loss = 4.17035 avg_loss = 3.97783\n",
      "epoch no.0 train no.57890  loss = 4.81752 avg_loss = 3.99150\n",
      "epoch no.0 train no.57900  loss = 3.30482 avg_loss = 3.92838\n",
      "epoch no.0 train no.57910  loss = 5.14966 avg_loss = 3.94365\n",
      "epoch no.0 train no.57920  loss = 4.06993 avg_loss = 3.95365\n",
      "epoch no.0 train no.57930  loss = 3.29383 avg_loss = 4.00258\n",
      "epoch no.0 train no.57940  loss = 4.16785 avg_loss = 4.02985\n",
      "epoch no.0 train no.57950  loss = 4.81318 avg_loss = 4.01766\n",
      "epoch no.0 train no.57960  loss = 5.08595 avg_loss = 4.05954\n",
      "epoch no.0 train no.57970  loss = 4.96904 avg_loss = 4.09599\n",
      "epoch no.0 train no.57980  loss = 3.91311 avg_loss = 4.06253\n",
      "epoch no.0 train no.57990  loss = 4.92934 avg_loss = 4.06201\n",
      "epoch no.0 train no.58000  loss = 4.26630 avg_loss = 4.08216\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '에', '▁딱', '▁노래', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.0 train no.58010  loss = 4.22762 avg_loss = 4.07001\n",
      "epoch no.0 train no.58020  loss = 2.99396 avg_loss = 4.04973\n",
      "epoch no.0 train no.58030  loss = 4.26093 avg_loss = 4.03510\n",
      "epoch no.0 train no.58040  loss = 4.74267 avg_loss = 4.00680\n",
      "epoch no.0 train no.58050  loss = 4.00750 avg_loss = 3.99109\n",
      "epoch no.0 train no.58060  loss = 3.57503 avg_loss = 4.00181\n",
      "epoch no.0 train no.58070  loss = 3.05109 avg_loss = 3.98193\n",
      "epoch no.0 train no.58080  loss = 3.20493 avg_loss = 3.97224\n",
      "epoch no.0 train no.58090  loss = 5.10501 avg_loss = 4.00354\n",
      "epoch no.0 train no.58100  loss = 6.71642 avg_loss = 4.04277\n",
      "epoch no.0 train no.58110  loss = 4.45935 avg_loss = 4.01425\n",
      "epoch no.0 train no.58120  loss = 2.27595 avg_loss = 3.99167\n",
      "epoch no.0 train no.58130  loss = 3.05273 avg_loss = 3.98580\n",
      "epoch no.0 train no.58140  loss = 5.43765 avg_loss = 4.00859\n",
      "epoch no.0 train no.58150  loss = 4.78003 avg_loss = 3.99514\n",
      "epoch no.0 train no.58160  loss = 4.64668 avg_loss = 3.97142\n",
      "epoch no.0 train no.58170  loss = 2.98290 avg_loss = 3.93278\n",
      "epoch no.0 train no.58180  loss = 2.75792 avg_loss = 3.96799\n",
      "epoch no.0 train no.58190  loss = 2.12384 avg_loss = 3.95352\n",
      "epoch no.0 train no.58200  loss = 2.95699 avg_loss = 3.95314\n",
      "epoch no.0 train no.58210  loss = 5.07201 avg_loss = 3.96165\n",
      "epoch no.0 train no.58220  loss = 2.79639 avg_loss = 3.91097\n",
      "epoch no.0 train no.58230  loss = 3.00792 avg_loss = 3.88569\n",
      "epoch no.0 train no.58240  loss = 4.99598 avg_loss = 3.92141\n",
      "epoch no.0 train no.58250  loss = 5.16542 avg_loss = 3.89880\n",
      "epoch no.0 train no.58260  loss = 6.42593 avg_loss = 3.93398\n",
      "epoch no.0 train no.58270  loss = 4.75337 avg_loss = 3.95909\n",
      "epoch no.0 train no.58280  loss = 3.51022 avg_loss = 4.00934\n",
      "epoch no.0 train no.58290  loss = 4.18231 avg_loss = 4.01180\n",
      "epoch no.0 train no.58300  loss = 3.23338 avg_loss = 3.99212\n",
      "epoch no.0 train no.58310  loss = 4.90160 avg_loss = 3.97765\n",
      "epoch no.0 train no.58320  loss = 3.87841 avg_loss = 3.96475\n",
      "epoch no.0 train no.58330  loss = 2.83097 avg_loss = 3.96506\n",
      "epoch no.0 train no.58340  loss = 4.96008 avg_loss = 3.94340\n",
      "epoch no.0 train no.58350  loss = 3.45847 avg_loss = 3.94948\n",
      "epoch no.0 train no.58360  loss = 4.28408 avg_loss = 3.92374\n",
      "epoch no.0 train no.58370  loss = 5.86775 avg_loss = 3.92307\n",
      "epoch no.0 train no.58380  loss = 4.99098 avg_loss = 4.03366\n",
      "epoch no.0 train no.58390  loss = 3.49224 avg_loss = 3.99880\n",
      "epoch no.0 train no.58400  loss = 2.79188 avg_loss = 3.99142\n",
      "epoch no.0 train no.58410  loss = 3.05140 avg_loss = 3.97161\n",
      "epoch no.0 train no.58420  loss = 3.79283 avg_loss = 3.94191\n",
      "epoch no.0 train no.58430  loss = 4.11330 avg_loss = 3.96216\n",
      "epoch no.0 train no.58440  loss = 3.08324 avg_loss = 4.00393\n",
      "epoch no.0 train no.58450  loss = 3.26397 avg_loss = 4.03717\n",
      "epoch no.0 train no.58460  loss = 6.09635 avg_loss = 4.04948\n",
      "epoch no.0 train no.58470  loss = 3.15786 avg_loss = 4.01128\n",
      "epoch no.0 train no.58480  loss = 2.90581 avg_loss = 4.05926\n",
      "epoch no.0 train no.58490  loss = 4.25900 avg_loss = 4.04366\n",
      "epoch no.0 train no.58500  loss = 2.75178 avg_loss = 4.04236\n",
      "epoch no.0 train no.58510  loss = 5.57562 avg_loss = 4.02793\n",
      "epoch no.0 train no.58520  loss = 6.74101 avg_loss = 4.05822\n",
      "epoch no.0 train no.58530  loss = 4.40776 avg_loss = 4.07592\n",
      "epoch no.0 train no.58540  loss = 3.02193 avg_loss = 4.08358\n",
      "epoch no.0 train no.58550  loss = 2.87679 avg_loss = 4.02489\n",
      "epoch no.0 train no.58560  loss = 3.26335 avg_loss = 4.07114\n",
      "epoch no.0 train no.58570  loss = 4.79328 avg_loss = 4.04249\n",
      "epoch no.0 train no.58580  loss = 3.21014 avg_loss = 4.04189\n",
      "epoch no.0 train no.58590  loss = 2.93067 avg_loss = 4.04617\n",
      "epoch no.0 train no.58600  loss = 5.78795 avg_loss = 4.09749\n",
      "epoch no.0 train no.58610  loss = 2.46198 avg_loss = 4.03053\n",
      "epoch no.0 train no.58620  loss = 3.27754 avg_loss = 4.00650\n",
      "epoch no.0 train no.58630  loss = 4.13079 avg_loss = 3.98604\n",
      "epoch no.0 train no.58640  loss = 5.96384 avg_loss = 4.05292\n",
      "epoch no.0 train no.58650  loss = 4.89984 avg_loss = 4.04588\n",
      "epoch no.0 train no.58660  loss = 3.25400 avg_loss = 4.08529\n",
      "epoch no.0 train no.58670  loss = 2.72553 avg_loss = 4.05676\n",
      "epoch no.0 train no.58680  loss = 2.27367 avg_loss = 4.08109\n",
      "epoch no.0 train no.58690  loss = 3.03730 avg_loss = 4.05036\n",
      "epoch no.0 train no.58700  loss = 3.51114 avg_loss = 3.99544\n",
      "epoch no.0 train no.58710  loss = 7.02639 avg_loss = 4.00269\n",
      "epoch no.0 train no.58720  loss = 5.89283 avg_loss = 4.07142\n",
      "epoch no.0 train no.58730  loss = 6.25458 avg_loss = 4.10130\n",
      "epoch no.0 train no.58740  loss = 4.07098 avg_loss = 4.15283\n",
      "epoch no.0 train no.58750  loss = 3.56586 avg_loss = 4.08684\n",
      "epoch no.0 train no.58760  loss = 5.11483 avg_loss = 4.09546\n",
      "epoch no.0 train no.58770  loss = 3.53311 avg_loss = 4.13924\n",
      "epoch no.0 train no.58780  loss = 4.23994 avg_loss = 4.21430\n",
      "epoch no.0 train no.58790  loss = 4.48520 avg_loss = 4.25792\n",
      "epoch no.0 train no.58800  loss = 5.37379 avg_loss = 4.26181\n",
      "epoch no.0 train no.58810  loss = 6.45642 avg_loss = 4.23944\n",
      "epoch no.0 train no.58820  loss = 2.47339 avg_loss = 4.18147\n",
      "epoch no.0 train no.58830  loss = 4.83332 avg_loss = 4.14473\n",
      "epoch no.0 train no.58840  loss = 4.55381 avg_loss = 4.07737\n",
      "epoch no.0 train no.58850  loss = 5.15332 avg_loss = 4.12392\n",
      "epoch no.0 train no.58860  loss = 3.70723 avg_loss = 4.14732\n",
      "epoch no.0 train no.58870  loss = 3.32300 avg_loss = 4.08832\n",
      "epoch no.0 train no.58880  loss = 2.45728 avg_loss = 4.04228\n",
      "epoch no.0 train no.58890  loss = 5.73600 avg_loss = 4.07806\n",
      "epoch no.0 train no.58900  loss = 2.48432 avg_loss = 4.07661\n",
      "epoch no.0 train no.58910  loss = 3.38584 avg_loss = 4.16213\n",
      "epoch no.0 train no.58920  loss = 5.34494 avg_loss = 4.13272\n",
      "epoch no.0 train no.58930  loss = 4.21449 avg_loss = 4.12099\n",
      "epoch no.0 train no.58940  loss = 3.82059 avg_loss = 4.12788\n",
      "epoch no.0 train no.58950  loss = 2.88670 avg_loss = 4.11270\n",
      "epoch no.0 train no.58960  loss = 2.83051 avg_loss = 4.11034\n",
      "epoch no.0 train no.58970  loss = 2.90441 avg_loss = 4.11050\n",
      "epoch no.0 train no.58980  loss = 3.25994 avg_loss = 4.10155\n",
      "epoch no.0 train no.58990  loss = 5.75924 avg_loss = 4.12659\n",
      "epoch no.0 train no.59000  loss = 3.92623 avg_loss = 4.08879\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '에', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.0 train no.59010  loss = 4.41633 avg_loss = 4.09176\n",
      "epoch no.0 train no.59020  loss = 4.39815 avg_loss = 4.10198\n",
      "epoch no.0 train no.59030  loss = 2.77350 avg_loss = 4.10694\n",
      "epoch no.0 train no.59040  loss = 5.29845 avg_loss = 4.07113\n",
      "epoch no.0 train no.59050  loss = 2.20830 avg_loss = 4.05397\n",
      "epoch no.0 train no.59060  loss = 2.31569 avg_loss = 4.04195\n",
      "epoch no.0 train no.59070  loss = 4.41439 avg_loss = 4.02890\n",
      "epoch no.0 train no.59080  loss = 4.19822 avg_loss = 4.04123\n",
      "epoch no.0 train no.59090  loss = 4.77378 avg_loss = 4.05784\n",
      "epoch no.0 train no.59100  loss = 3.33740 avg_loss = 3.94385\n",
      "epoch no.0 train no.59110  loss = 4.71420 avg_loss = 3.95109\n",
      "epoch no.0 train no.59120  loss = 5.79669 avg_loss = 3.96478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.59130  loss = 3.67592 avg_loss = 4.03438\n",
      "epoch no.0 train no.59140  loss = 3.07639 avg_loss = 4.03631\n",
      "epoch no.0 train no.59150  loss = 3.27637 avg_loss = 3.98417\n",
      "epoch no.0 train no.59160  loss = 2.59998 avg_loss = 3.90785\n",
      "epoch no.0 train no.59170  loss = 2.48010 avg_loss = 3.89157\n",
      "epoch no.0 train no.59180  loss = 3.59434 avg_loss = 3.93930\n",
      "epoch no.0 train no.59190  loss = 3.78480 avg_loss = 4.01197\n",
      "epoch no.0 train no.59200  loss = 5.33809 avg_loss = 4.05191\n",
      "epoch no.0 train no.59210  loss = 4.94954 avg_loss = 4.01038\n",
      "epoch no.0 train no.59220  loss = 6.13024 avg_loss = 3.99160\n",
      "epoch no.0 train no.59230  loss = 5.61189 avg_loss = 4.04408\n",
      "epoch no.0 train no.59240  loss = 3.71838 avg_loss = 4.07090\n",
      "epoch no.0 train no.59250  loss = 3.21248 avg_loss = 4.07209\n",
      "epoch no.0 train no.59260  loss = 6.73008 avg_loss = 4.03754\n",
      "epoch no.0 train no.59270  loss = 3.90017 avg_loss = 4.03664\n",
      "epoch no.0 train no.59280  loss = 2.24243 avg_loss = 4.00559\n",
      "epoch no.0 train no.59290  loss = 4.74878 avg_loss = 4.04592\n",
      "epoch no.0 train no.59300  loss = 5.24004 avg_loss = 4.08246\n",
      "epoch no.0 train no.59310  loss = 4.60259 avg_loss = 4.03464\n",
      "epoch no.0 train no.59320  loss = 4.39486 avg_loss = 4.05647\n",
      "epoch no.0 train no.59330  loss = 3.73365 avg_loss = 4.02949\n",
      "epoch no.0 train no.59340  loss = 4.18573 avg_loss = 4.02087\n",
      "epoch no.0 train no.59350  loss = 6.11066 avg_loss = 4.04418\n",
      "epoch no.0 train no.59360  loss = 4.06215 avg_loss = 4.13732\n",
      "epoch no.0 train no.59370  loss = 5.56224 avg_loss = 4.16638\n",
      "epoch no.0 train no.59380  loss = 4.35443 avg_loss = 4.13834\n",
      "epoch no.0 train no.59390  loss = 4.46337 avg_loss = 4.12817\n",
      "epoch no.0 train no.59400  loss = 6.24368 avg_loss = 4.17109\n",
      "epoch no.0 train no.59410  loss = 3.47130 avg_loss = 4.16928\n",
      "epoch no.0 train no.59420  loss = 1.98029 avg_loss = 4.06484\n",
      "epoch no.0 train no.59430  loss = 6.44040 avg_loss = 4.11441\n",
      "epoch no.0 train no.59440  loss = 4.62366 avg_loss = 4.11375\n",
      "epoch no.0 train no.59450  loss = 3.18693 avg_loss = 4.09973\n",
      "epoch no.0 train no.59460  loss = 3.82670 avg_loss = 4.05892\n",
      "epoch no.0 train no.59470  loss = 4.25666 avg_loss = 4.04922\n",
      "epoch no.0 train no.59480  loss = 3.67870 avg_loss = 4.07496\n",
      "epoch no.0 train no.59490  loss = 2.53107 avg_loss = 4.08843\n",
      "epoch no.0 train no.59500  loss = 3.37962 avg_loss = 4.10851\n",
      "epoch no.0 train no.59510  loss = 5.58707 avg_loss = 4.16314\n",
      "epoch no.0 train no.59520  loss = 6.57344 avg_loss = 4.22754\n",
      "epoch no.0 train no.59530  loss = 5.76686 avg_loss = 4.19606\n",
      "epoch no.0 train no.59540  loss = 2.66618 avg_loss = 4.14462\n",
      "epoch no.0 train no.59550  loss = 4.29577 avg_loss = 4.14833\n",
      "epoch no.0 train no.59560  loss = 2.93463 avg_loss = 4.08430\n",
      "epoch no.0 train no.59570  loss = 4.25786 avg_loss = 4.09175\n",
      "epoch no.0 train no.59580  loss = 2.03675 avg_loss = 4.04217\n",
      "epoch no.0 train no.59590  loss = 2.17386 avg_loss = 4.06207\n",
      "epoch no.0 train no.59600  loss = 6.71342 avg_loss = 4.04799\n",
      "epoch no.0 train no.59610  loss = 2.84269 avg_loss = 4.04062\n",
      "epoch no.0 train no.59620  loss = 2.40221 avg_loss = 3.94972\n",
      "epoch no.0 train no.59630  loss = 2.17309 avg_loss = 3.96351\n",
      "epoch no.0 train no.59640  loss = 3.17432 avg_loss = 3.91151\n",
      "epoch no.0 train no.59650  loss = 4.75761 avg_loss = 3.91905\n",
      "epoch no.0 train no.59660  loss = 3.65492 avg_loss = 3.96857\n",
      "epoch no.0 train no.59670  loss = 4.51333 avg_loss = 3.94328\n",
      "epoch no.0 train no.59680  loss = 3.85172 avg_loss = 4.02065\n",
      "epoch no.0 train no.59690  loss = 4.72933 avg_loss = 4.01951\n",
      "epoch no.0 train no.59700  loss = 6.97601 avg_loss = 4.02784\n",
      "epoch no.0 train no.59710  loss = 3.64703 avg_loss = 4.05967\n",
      "epoch no.0 train no.59720  loss = 3.24307 avg_loss = 4.05666\n",
      "epoch no.0 train no.59730  loss = 6.04034 avg_loss = 4.05941\n",
      "epoch no.0 train no.59740  loss = 5.92142 avg_loss = 4.08002\n",
      "epoch no.0 train no.59750  loss = 3.67888 avg_loss = 4.04159\n",
      "epoch no.0 train no.59760  loss = 2.94500 avg_loss = 3.96910\n",
      "epoch no.0 train no.59770  loss = 4.23744 avg_loss = 3.96171\n",
      "epoch no.0 train no.59780  loss = 4.04033 avg_loss = 3.99492\n",
      "epoch no.0 train no.59790  loss = 2.84924 avg_loss = 3.94144\n",
      "epoch no.0 train no.59800  loss = 5.19836 avg_loss = 3.99748\n",
      "epoch no.0 train no.59810  loss = 6.09205 avg_loss = 4.01009\n",
      "epoch no.0 train no.59820  loss = 3.80631 avg_loss = 4.09284\n",
      "epoch no.0 train no.59830  loss = 5.30031 avg_loss = 4.09702\n",
      "epoch no.0 train no.59840  loss = 4.70785 avg_loss = 4.05151\n",
      "epoch no.0 train no.59850  loss = 3.97900 avg_loss = 4.09977\n",
      "epoch no.0 train no.59860  loss = 2.71078 avg_loss = 4.07411\n",
      "epoch no.0 train no.59870  loss = 2.68359 avg_loss = 4.07223\n",
      "epoch no.0 train no.59880  loss = 4.51508 avg_loss = 4.10685\n",
      "epoch no.0 train no.59890  loss = 4.16107 avg_loss = 4.13853\n",
      "epoch no.0 train no.59900  loss = 3.87102 avg_loss = 4.12241\n",
      "epoch no.0 train no.59910  loss = 4.62969 avg_loss = 4.16368\n",
      "epoch no.0 train no.59920  loss = 5.31433 avg_loss = 4.11451\n",
      "epoch no.0 train no.59930  loss = 3.54088 avg_loss = 4.06753\n",
      "epoch no.0 train no.59940  loss = 2.86052 avg_loss = 4.02902\n",
      "epoch no.0 train no.59950  loss = 3.08633 avg_loss = 4.01844\n",
      "epoch no.0 train no.59960  loss = 6.42443 avg_loss = 3.99931\n",
      "epoch no.0 train no.59970  loss = 4.70453 avg_loss = 3.99188\n",
      "epoch no.0 train no.59980  loss = 3.87318 avg_loss = 4.05406\n",
      "epoch no.0 train no.59990  loss = 2.98780 avg_loss = 4.03994\n",
      "epoch no.0 train no.60000  loss = 5.24508 avg_loss = 4.03761\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 좋은 음악</s>\n",
      "epoch no.0 train no.60010  loss = 4.29448 avg_loss = 3.96399\n",
      "epoch no.0 train no.60020  loss = 3.05325 avg_loss = 3.91607\n",
      "epoch no.0 train no.60030  loss = 2.56873 avg_loss = 3.90778\n",
      "epoch no.0 train no.60040  loss = 1.79742 avg_loss = 3.92461\n",
      "epoch no.0 train no.60050  loss = 3.08320 avg_loss = 3.99869\n",
      "epoch no.0 train no.60060  loss = 3.42755 avg_loss = 4.01671\n",
      "epoch no.0 train no.60070  loss = 4.28950 avg_loss = 4.01680\n",
      "epoch no.0 train no.60080  loss = 3.19915 avg_loss = 4.00156\n",
      "epoch no.0 train no.60090  loss = 4.46011 avg_loss = 3.98034\n",
      "epoch no.0 train no.60100  loss = 5.86327 avg_loss = 3.97066\n",
      "epoch no.0 train no.60110  loss = 4.72226 avg_loss = 3.98377\n",
      "epoch no.0 train no.60120  loss = 4.77638 avg_loss = 4.00240\n",
      "epoch no.0 train no.60130  loss = 5.93718 avg_loss = 4.02460\n",
      "epoch no.0 train no.60140  loss = 5.52662 avg_loss = 4.06207\n",
      "epoch no.0 train no.60150  loss = 3.83383 avg_loss = 4.07535\n",
      "epoch no.0 train no.60160  loss = 6.39938 avg_loss = 4.06089\n",
      "epoch no.0 train no.60170  loss = 3.54084 avg_loss = 4.01435\n",
      "epoch no.0 train no.60180  loss = 2.48426 avg_loss = 4.05901\n",
      "epoch no.0 train no.60190  loss = 3.35147 avg_loss = 4.08215\n",
      "epoch no.0 train no.60200  loss = 3.90613 avg_loss = 4.05375\n",
      "epoch no.0 train no.60210  loss = 3.53880 avg_loss = 4.04194\n",
      "epoch no.0 train no.60220  loss = 4.31822 avg_loss = 4.05442\n",
      "epoch no.0 train no.60230  loss = 3.47459 avg_loss = 4.02184\n",
      "epoch no.0 train no.60240  loss = 5.39272 avg_loss = 4.02582\n",
      "epoch no.0 train no.60250  loss = 4.80482 avg_loss = 4.06731\n",
      "epoch no.0 train no.60260  loss = 4.71485 avg_loss = 4.06036\n",
      "epoch no.0 train no.60270  loss = 3.09490 avg_loss = 4.00359\n",
      "epoch no.0 train no.60280  loss = 2.57211 avg_loss = 3.95560\n",
      "epoch no.0 train no.60290  loss = 5.15534 avg_loss = 3.95016\n",
      "epoch no.0 train no.60300  loss = 4.57134 avg_loss = 4.00059\n",
      "epoch no.0 train no.60310  loss = 5.01867 avg_loss = 4.02348\n",
      "epoch no.0 train no.60320  loss = 2.56935 avg_loss = 3.98284\n",
      "epoch no.0 train no.60330  loss = 3.68097 avg_loss = 3.94887\n",
      "epoch no.0 train no.60340  loss = 2.88642 avg_loss = 3.90449\n",
      "epoch no.0 train no.60350  loss = 4.61915 avg_loss = 3.90531\n",
      "epoch no.0 train no.60360  loss = 2.87054 avg_loss = 3.90370\n",
      "epoch no.0 train no.60370  loss = 5.96859 avg_loss = 3.99539\n",
      "epoch no.0 train no.60380  loss = 5.37060 avg_loss = 4.04645\n",
      "epoch no.0 train no.60390  loss = 3.10326 avg_loss = 4.02113\n",
      "epoch no.0 train no.60400  loss = 2.40191 avg_loss = 3.98736\n",
      "epoch no.0 train no.60410  loss = 1.91131 avg_loss = 3.95633\n",
      "epoch no.0 train no.60420  loss = 5.42044 avg_loss = 3.98411\n",
      "epoch no.0 train no.60430  loss = 2.19692 avg_loss = 4.04291\n",
      "epoch no.0 train no.60440  loss = 3.86967 avg_loss = 4.05802\n",
      "epoch no.0 train no.60450  loss = 3.99724 avg_loss = 4.02274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.60460  loss = 4.70762 avg_loss = 3.98428\n",
      "epoch no.0 train no.60470  loss = 4.04287 avg_loss = 3.99590\n",
      "epoch no.0 train no.60480  loss = 3.60345 avg_loss = 4.02940\n",
      "epoch no.0 train no.60490  loss = 5.23287 avg_loss = 4.09225\n",
      "epoch no.0 train no.60500  loss = 4.09099 avg_loss = 4.09808\n",
      "epoch no.0 train no.60510  loss = 2.01439 avg_loss = 4.09250\n",
      "epoch no.0 train no.60520  loss = 3.64889 avg_loss = 4.03046\n",
      "epoch no.0 train no.60530  loss = 4.87497 avg_loss = 4.06535\n",
      "epoch no.0 train no.60540  loss = 4.33206 avg_loss = 4.08040\n",
      "epoch no.0 train no.60550  loss = 4.65590 avg_loss = 4.07026\n",
      "epoch no.0 train no.60560  loss = 3.44093 avg_loss = 4.04686\n",
      "epoch no.0 train no.60570  loss = 4.66903 avg_loss = 4.04668\n",
      "epoch no.0 train no.60580  loss = 4.06150 avg_loss = 4.02348\n",
      "epoch no.0 train no.60590  loss = 2.06691 avg_loss = 3.99044\n",
      "epoch no.0 train no.60600  loss = 6.02892 avg_loss = 4.01183\n",
      "epoch no.0 train no.60610  loss = 3.11970 avg_loss = 3.98740\n",
      "epoch no.0 train no.60620  loss = 5.95523 avg_loss = 4.00453\n",
      "epoch no.0 train no.60630  loss = 3.11156 avg_loss = 3.99598\n",
      "epoch no.0 train no.60640  loss = 2.10913 avg_loss = 3.98509\n",
      "epoch no.0 train no.60650  loss = 4.07205 avg_loss = 3.99510\n",
      "epoch no.0 train no.60660  loss = 3.65804 avg_loss = 4.01752\n",
      "epoch no.0 train no.60670  loss = 3.40645 avg_loss = 4.01754\n",
      "epoch no.0 train no.60680  loss = 6.45109 avg_loss = 3.98733\n",
      "epoch no.0 train no.60690  loss = 4.28041 avg_loss = 4.01833\n",
      "epoch no.0 train no.60700  loss = 4.75392 avg_loss = 4.00210\n",
      "epoch no.0 train no.60710  loss = 4.18321 avg_loss = 4.01091\n",
      "epoch no.0 train no.60720  loss = 4.98115 avg_loss = 3.99608\n",
      "epoch no.0 train no.60730  loss = 4.68640 avg_loss = 3.99230\n",
      "epoch no.0 train no.60740  loss = 6.93894 avg_loss = 4.03038\n",
      "epoch no.0 train no.60750  loss = 4.08992 avg_loss = 4.02443\n",
      "epoch no.0 train no.60760  loss = 5.30894 avg_loss = 4.07678\n",
      "epoch no.0 train no.60770  loss = 3.42348 avg_loss = 4.10111\n",
      "epoch no.0 train no.60780  loss = 3.31008 avg_loss = 4.08626\n",
      "epoch no.0 train no.60790  loss = 3.65867 avg_loss = 4.12853\n",
      "epoch no.0 train no.60800  loss = 4.73146 avg_loss = 4.15767\n",
      "epoch no.0 train no.60810  loss = 3.93450 avg_loss = 4.15411\n",
      "epoch no.0 train no.60820  loss = 4.50854 avg_loss = 4.18003\n",
      "epoch no.0 train no.60830  loss = 6.04894 avg_loss = 4.16998\n",
      "epoch no.0 train no.60840  loss = 3.25660 avg_loss = 4.13360\n",
      "epoch no.0 train no.60850  loss = 2.78821 avg_loss = 4.09562\n",
      "epoch no.0 train no.60860  loss = 3.28312 avg_loss = 4.07636\n",
      "epoch no.0 train no.60870  loss = 3.72597 avg_loss = 4.05405\n",
      "epoch no.0 train no.60880  loss = 8.28097 avg_loss = 4.07987\n",
      "epoch no.0 train no.60890  loss = 2.91578 avg_loss = 4.06933\n",
      "epoch no.0 train no.60900  loss = 3.97829 avg_loss = 4.04945\n",
      "epoch no.0 train no.60910  loss = 4.21113 avg_loss = 4.01573\n",
      "epoch no.0 train no.60920  loss = 3.14508 avg_loss = 3.97560\n",
      "epoch no.0 train no.60930  loss = 2.82137 avg_loss = 3.96500\n",
      "epoch no.0 train no.60940  loss = 4.49422 avg_loss = 4.01741\n",
      "epoch no.0 train no.60950  loss = 3.41435 avg_loss = 3.91923\n",
      "epoch no.0 train no.60960  loss = 4.68224 avg_loss = 3.96846\n",
      "epoch no.0 train no.60970  loss = 3.39725 avg_loss = 3.97100\n",
      "epoch no.0 train no.60980  loss = 4.10125 avg_loss = 3.99847\n",
      "epoch no.0 train no.60990  loss = 3.20174 avg_loss = 4.00682\n",
      "epoch no.0 train no.61000  loss = 3.79204 avg_loss = 4.02263\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '용', '</s>', '▁노래', '</s>']\n",
      "기분전환용 신나는 노래</s>\n",
      "epoch no.0 train no.61010  loss = 6.44595 avg_loss = 4.03898\n",
      "epoch no.0 train no.61020  loss = 3.94469 avg_loss = 3.99953\n",
      "epoch no.0 train no.61030  loss = 3.23976 avg_loss = 3.97954\n",
      "epoch no.0 train no.61040  loss = 3.47397 avg_loss = 3.93091\n",
      "epoch no.0 train no.61050  loss = 3.56187 avg_loss = 3.88611\n",
      "epoch no.0 train no.61060  loss = 3.96795 avg_loss = 3.94857\n",
      "epoch no.0 train no.61070  loss = 2.22538 avg_loss = 3.96663\n",
      "epoch no.0 train no.61080  loss = 2.81462 avg_loss = 3.91981\n",
      "epoch no.0 train no.61090  loss = 3.06551 avg_loss = 3.91183\n",
      "epoch no.0 train no.61100  loss = 3.18449 avg_loss = 3.91191\n",
      "epoch no.0 train no.61110  loss = 3.14754 avg_loss = 3.92068\n",
      "epoch no.0 train no.61120  loss = 4.21686 avg_loss = 3.93345\n",
      "epoch no.0 train no.61130  loss = 2.36908 avg_loss = 3.95969\n",
      "epoch no.0 train no.61140  loss = 3.65043 avg_loss = 3.93409\n",
      "epoch no.0 train no.61150  loss = 4.82312 avg_loss = 3.97396\n",
      "epoch no.0 train no.61160  loss = 6.57565 avg_loss = 3.98994\n",
      "epoch no.0 train no.61170  loss = 4.57985 avg_loss = 3.93409\n",
      "epoch no.0 train no.61180  loss = 4.52259 avg_loss = 3.91660\n",
      "epoch no.0 train no.61190  loss = 2.80230 avg_loss = 3.85008\n",
      "epoch no.0 train no.61200  loss = 5.74450 avg_loss = 3.93667\n",
      "epoch no.0 train no.61210  loss = 4.37611 avg_loss = 3.96641\n",
      "epoch no.0 train no.61220  loss = 4.50214 avg_loss = 3.97521\n",
      "epoch no.0 train no.61230  loss = 5.20708 avg_loss = 3.95869\n",
      "epoch no.0 train no.61240  loss = 2.66782 avg_loss = 3.92560\n",
      "epoch no.0 train no.61250  loss = 2.85702 avg_loss = 3.93001\n",
      "epoch no.0 train no.61260  loss = 7.05056 avg_loss = 3.99974\n",
      "epoch no.0 train no.61270  loss = 4.22078 avg_loss = 4.00932\n",
      "epoch no.0 train no.61280  loss = 3.50293 avg_loss = 4.02727\n",
      "epoch no.0 train no.61290  loss = 7.10618 avg_loss = 4.06387\n",
      "epoch no.0 train no.61300  loss = 3.25846 avg_loss = 4.00654\n",
      "epoch no.0 train no.61310  loss = 4.96546 avg_loss = 4.03532\n",
      "epoch no.0 train no.61320  loss = 2.23347 avg_loss = 4.05189\n",
      "epoch no.0 train no.61330  loss = 3.08069 avg_loss = 4.08192\n",
      "epoch no.0 train no.61340  loss = 4.48169 avg_loss = 4.08099\n",
      "epoch no.0 train no.61350  loss = 3.45663 avg_loss = 4.07568\n",
      "epoch no.0 train no.61360  loss = 4.56501 avg_loss = 4.15339\n",
      "epoch no.0 train no.61370  loss = 4.68273 avg_loss = 4.10690\n",
      "epoch no.0 train no.61380  loss = 3.88697 avg_loss = 4.03730\n",
      "epoch no.0 train no.61390  loss = 6.41939 avg_loss = 3.98907\n",
      "epoch no.0 train no.61400  loss = 2.88000 avg_loss = 3.94330\n",
      "epoch no.0 train no.61410  loss = 5.54049 avg_loss = 3.97636\n",
      "epoch no.0 train no.61420  loss = 5.72534 avg_loss = 3.99918\n",
      "epoch no.0 train no.61430  loss = 7.35296 avg_loss = 4.00235\n",
      "epoch no.0 train no.61440  loss = 5.16547 avg_loss = 3.96168\n",
      "epoch no.0 train no.61450  loss = 3.23273 avg_loss = 3.93017\n",
      "epoch no.0 train no.61460  loss = 4.29657 avg_loss = 3.91597\n",
      "epoch no.0 train no.61470  loss = 2.94106 avg_loss = 3.90367\n",
      "epoch no.0 train no.61480  loss = 4.76136 avg_loss = 3.90793\n",
      "epoch no.0 train no.61490  loss = 3.47609 avg_loss = 3.87628\n",
      "epoch no.0 train no.61500  loss = 2.78291 avg_loss = 3.83227\n",
      "epoch no.0 train no.61510  loss = 2.36374 avg_loss = 3.78204\n",
      "epoch no.0 train no.61520  loss = 2.99485 avg_loss = 3.80342\n",
      "epoch no.0 train no.61530  loss = 3.01905 avg_loss = 3.81154\n",
      "epoch no.0 train no.61540  loss = 4.59205 avg_loss = 3.83590\n",
      "epoch no.0 train no.61550  loss = 5.02609 avg_loss = 3.82002\n",
      "epoch no.0 train no.61560  loss = 3.85440 avg_loss = 3.81476\n",
      "epoch no.0 train no.61570  loss = 5.80932 avg_loss = 3.90747\n",
      "epoch no.0 train no.61580  loss = 5.38361 avg_loss = 3.94034\n",
      "epoch no.0 train no.61590  loss = 4.35036 avg_loss = 3.99193\n",
      "epoch no.0 train no.61600  loss = 2.99921 avg_loss = 3.93109\n",
      "epoch no.0 train no.61610  loss = 3.49431 avg_loss = 3.90368\n",
      "epoch no.0 train no.61620  loss = 1.90498 avg_loss = 3.94386\n",
      "epoch no.0 train no.61630  loss = 5.83877 avg_loss = 4.01416\n",
      "epoch no.0 train no.61640  loss = 4.32939 avg_loss = 4.06087\n",
      "epoch no.0 train no.61650  loss = 2.19526 avg_loss = 4.01144\n",
      "epoch no.0 train no.61660  loss = 3.76979 avg_loss = 3.97906\n",
      "epoch no.0 train no.61670  loss = 4.64965 avg_loss = 3.95589\n",
      "epoch no.0 train no.61680  loss = 5.39878 avg_loss = 3.91827\n",
      "epoch no.0 train no.61690  loss = 3.41411 avg_loss = 4.00886\n",
      "epoch no.0 train no.61700  loss = 4.27105 avg_loss = 4.07047\n",
      "epoch no.0 train no.61710  loss = 3.92414 avg_loss = 4.09501\n",
      "epoch no.0 train no.61720  loss = 2.54387 avg_loss = 4.05674\n",
      "epoch no.0 train no.61730  loss = 4.67170 avg_loss = 4.08198\n",
      "epoch no.0 train no.61740  loss = 6.66765 avg_loss = 4.13173\n",
      "epoch no.0 train no.61750  loss = 4.28845 avg_loss = 4.06980\n",
      "epoch no.0 train no.61760  loss = 4.29260 avg_loss = 4.08689\n",
      "epoch no.0 train no.61770  loss = 4.81626 avg_loss = 4.04583\n",
      "epoch no.0 train no.61780  loss = 3.77792 avg_loss = 4.03894\n",
      "epoch no.0 train no.61790  loss = 3.28136 avg_loss = 4.02048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.61800  loss = 3.50788 avg_loss = 3.95800\n",
      "epoch no.0 train no.61810  loss = 6.50189 avg_loss = 3.98804\n",
      "epoch no.0 train no.61820  loss = 2.73657 avg_loss = 3.95894\n",
      "epoch no.0 train no.61830  loss = 4.02330 avg_loss = 3.95956\n",
      "epoch no.0 train no.61840  loss = 2.87536 avg_loss = 3.93983\n",
      "epoch no.0 train no.61850  loss = 3.40367 avg_loss = 3.96745\n",
      "epoch no.0 train no.61860  loss = 5.19839 avg_loss = 4.01670\n",
      "epoch no.0 train no.61870  loss = 4.76725 avg_loss = 4.08948\n",
      "epoch no.0 train no.61880  loss = 5.59979 avg_loss = 4.12281\n",
      "epoch no.0 train no.61890  loss = 5.73114 avg_loss = 4.11072\n",
      "epoch no.0 train no.61900  loss = 1.41092 avg_loss = 4.10798\n",
      "epoch no.0 train no.61910  loss = 4.50102 avg_loss = 4.08375\n",
      "epoch no.0 train no.61920  loss = 7.88611 avg_loss = 4.07825\n",
      "epoch no.0 train no.61930  loss = 5.94995 avg_loss = 4.12439\n",
      "epoch no.0 train no.61940  loss = 3.17451 avg_loss = 4.09538\n",
      "epoch no.0 train no.61950  loss = 3.71267 avg_loss = 4.12525\n",
      "epoch no.0 train no.61960  loss = 6.06398 avg_loss = 4.11939\n",
      "epoch no.0 train no.61970  loss = 2.68399 avg_loss = 4.13958\n",
      "epoch no.0 train no.61980  loss = 4.80131 avg_loss = 4.14872\n",
      "epoch no.0 train no.61990  loss = 2.97976 avg_loss = 4.09578\n",
      "epoch no.0 train no.62000  loss = 3.42630 avg_loss = 4.08813\n",
      "4\n",
      "to_tokens: ['▁가을', '전환', '에', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환 할때 듣는노래</s>\n",
      "epoch no.0 train no.62010  loss = 3.26529 avg_loss = 4.07890\n",
      "epoch no.0 train no.62020  loss = 4.09039 avg_loss = 4.06883\n",
      "epoch no.0 train no.62030  loss = 2.49132 avg_loss = 4.04149\n",
      "epoch no.0 train no.62040  loss = 2.38639 avg_loss = 4.04929\n",
      "epoch no.0 train no.62050  loss = 3.63701 avg_loss = 4.00561\n",
      "epoch no.0 train no.62060  loss = 4.17844 avg_loss = 4.01084\n",
      "epoch no.0 train no.62070  loss = 3.04120 avg_loss = 4.06061\n",
      "epoch no.0 train no.62080  loss = 4.14932 avg_loss = 4.05326\n",
      "epoch no.0 train no.62090  loss = 2.62056 avg_loss = 4.04458\n",
      "epoch no.0 train no.62100  loss = 4.87758 avg_loss = 4.02737\n",
      "epoch no.0 train no.62110  loss = 3.85663 avg_loss = 4.08080\n",
      "epoch no.0 train no.62120  loss = 4.38309 avg_loss = 4.05996\n",
      "epoch no.0 train no.62130  loss = 2.51996 avg_loss = 4.03337\n",
      "epoch no.0 train no.62140  loss = 2.46434 avg_loss = 4.03397\n",
      "epoch no.0 train no.62150  loss = 4.15503 avg_loss = 4.02734\n",
      "epoch no.0 train no.62160  loss = 3.98595 avg_loss = 4.03567\n",
      "epoch no.0 train no.62170  loss = 2.55799 avg_loss = 4.00410\n",
      "epoch no.0 train no.62180  loss = 3.24681 avg_loss = 4.02680\n",
      "epoch no.0 train no.62190  loss = 4.45598 avg_loss = 3.99394\n",
      "epoch no.0 train no.62200  loss = 3.86828 avg_loss = 4.04319\n",
      "epoch no.0 train no.62210  loss = 2.91593 avg_loss = 4.01371\n",
      "epoch no.0 train no.62220  loss = 3.44495 avg_loss = 3.99694\n",
      "epoch no.0 train no.62230  loss = 4.32264 avg_loss = 4.05376\n",
      "epoch no.0 train no.62240  loss = 5.42434 avg_loss = 4.08518\n",
      "epoch no.0 train no.62250  loss = 3.16196 avg_loss = 4.04985\n",
      "epoch no.0 train no.62260  loss = 2.78592 avg_loss = 4.07927\n",
      "epoch no.0 train no.62270  loss = 3.26198 avg_loss = 4.01728\n",
      "epoch no.0 train no.62280  loss = 3.74733 avg_loss = 3.96082\n",
      "epoch no.0 train no.62290  loss = 3.77935 avg_loss = 3.94447\n",
      "epoch no.0 train no.62300  loss = 2.41199 avg_loss = 3.91048\n",
      "epoch no.0 train no.62310  loss = 4.97435 avg_loss = 3.94506\n",
      "epoch no.0 train no.62320  loss = 5.35415 avg_loss = 3.93904\n",
      "epoch no.0 train no.62330  loss = 3.63570 avg_loss = 3.94995\n",
      "epoch no.0 train no.62340  loss = 3.49565 avg_loss = 3.94185\n",
      "epoch no.0 train no.62350  loss = 5.61456 avg_loss = 3.91692\n",
      "epoch no.0 train no.62360  loss = 4.46390 avg_loss = 3.96798\n",
      "epoch no.0 train no.62370  loss = 2.93818 avg_loss = 3.93235\n",
      "epoch no.0 train no.62380  loss = 2.52735 avg_loss = 3.92356\n",
      "epoch no.0 train no.62390  loss = 3.21883 avg_loss = 3.94782\n",
      "epoch no.0 train no.62400  loss = 2.91641 avg_loss = 3.99974\n",
      "epoch no.0 train no.62410  loss = 5.45717 avg_loss = 3.97855\n",
      "epoch no.0 train no.62420  loss = 6.34537 avg_loss = 4.00865\n",
      "epoch no.0 train no.62430  loss = 5.27572 avg_loss = 4.00200\n",
      "epoch no.0 train no.62440  loss = 5.02431 avg_loss = 4.00978\n",
      "epoch no.0 train no.62450  loss = 2.07513 avg_loss = 3.97283\n",
      "epoch no.0 train no.62460  loss = 3.62198 avg_loss = 3.96402\n",
      "epoch no.0 train no.62470  loss = 3.34750 avg_loss = 3.94671\n",
      "epoch no.0 train no.62480  loss = 3.80936 avg_loss = 3.93297\n",
      "epoch no.0 train no.62490  loss = 4.80261 avg_loss = 3.93392\n",
      "epoch no.0 train no.62500  loss = 4.34171 avg_loss = 3.93565\n",
      "epoch no.0 train no.62510  loss = 2.29179 avg_loss = 3.92207\n",
      "epoch no.0 train no.62520  loss = 3.77783 avg_loss = 3.91993\n",
      "epoch no.0 train no.62530  loss = 2.27674 avg_loss = 3.94890\n",
      "epoch no.0 train no.62540  loss = 1.78137 avg_loss = 3.89171\n",
      "epoch no.0 train no.62550  loss = 2.64001 avg_loss = 3.89784\n",
      "epoch no.0 train no.62560  loss = 3.94848 avg_loss = 3.88057\n",
      "epoch no.0 train no.62570  loss = 3.63372 avg_loss = 3.86740\n",
      "epoch no.0 train no.62580  loss = 2.59522 avg_loss = 3.86438\n",
      "epoch no.0 train no.62590  loss = 4.54249 avg_loss = 3.91205\n",
      "epoch no.0 train no.62600  loss = 4.51369 avg_loss = 3.88720\n",
      "epoch no.0 train no.62610  loss = 5.15949 avg_loss = 3.89605\n",
      "epoch no.0 train no.62620  loss = 2.60174 avg_loss = 3.82016\n",
      "epoch no.0 train no.62630  loss = 3.43349 avg_loss = 3.82300\n",
      "epoch no.0 train no.62640  loss = 5.84565 avg_loss = 3.88712\n",
      "epoch no.0 train no.62650  loss = 1.47927 avg_loss = 3.84727\n",
      "epoch no.0 train no.62660  loss = 6.34829 avg_loss = 3.85389\n",
      "epoch no.0 train no.62670  loss = 3.36309 avg_loss = 3.80254\n",
      "epoch no.0 train no.62680  loss = 3.18133 avg_loss = 3.78313\n",
      "epoch no.0 train no.62690  loss = 2.30755 avg_loss = 3.76738\n",
      "epoch no.0 train no.62700  loss = 4.19790 avg_loss = 3.86787\n",
      "epoch no.0 train no.62710  loss = 5.51653 avg_loss = 3.93523\n",
      "epoch no.0 train no.62720  loss = 5.29106 avg_loss = 3.94604\n",
      "epoch no.0 train no.62730  loss = 3.96958 avg_loss = 3.89072\n",
      "epoch no.0 train no.62740  loss = 5.49197 avg_loss = 3.98913\n",
      "epoch no.0 train no.62750  loss = 6.03007 avg_loss = 4.00322\n",
      "epoch no.0 train no.62760  loss = 4.87926 avg_loss = 4.12316\n",
      "epoch no.0 train no.62770  loss = 3.62186 avg_loss = 4.09576\n",
      "epoch no.0 train no.62780  loss = 6.25737 avg_loss = 4.09037\n",
      "epoch no.0 train no.62790  loss = 6.90797 avg_loss = 4.09532\n",
      "epoch no.0 train no.62800  loss = 3.70373 avg_loss = 4.04219\n",
      "epoch no.0 train no.62810  loss = 3.03100 avg_loss = 4.04877\n",
      "epoch no.0 train no.62820  loss = 3.91531 avg_loss = 4.08133\n",
      "epoch no.0 train no.62830  loss = 2.92117 avg_loss = 4.06683\n",
      "epoch no.0 train no.62840  loss = 6.57145 avg_loss = 4.07018\n",
      "epoch no.0 train no.62850  loss = 4.11937 avg_loss = 4.06972\n",
      "epoch no.0 train no.62860  loss = 3.66635 avg_loss = 4.02631\n",
      "epoch no.0 train no.62870  loss = 4.42197 avg_loss = 4.01538\n",
      "epoch no.0 train no.62880  loss = 6.32919 avg_loss = 4.06050\n",
      "epoch no.0 train no.62890  loss = 4.11170 avg_loss = 4.02447\n",
      "epoch no.0 train no.62900  loss = 3.74771 avg_loss = 3.97585\n",
      "epoch no.0 train no.62910  loss = 2.27128 avg_loss = 3.98207\n",
      "epoch no.0 train no.62920  loss = 2.21140 avg_loss = 3.97715\n",
      "epoch no.0 train no.62930  loss = 2.52431 avg_loss = 3.98855\n",
      "epoch no.0 train no.62940  loss = 5.58637 avg_loss = 3.99927\n",
      "epoch no.0 train no.62950  loss = 2.56169 avg_loss = 4.02570\n",
      "epoch no.0 train no.62960  loss = 5.09474 avg_loss = 4.01010\n",
      "epoch no.0 train no.62970  loss = 4.07868 avg_loss = 3.94601\n",
      "epoch no.0 train no.62980  loss = 3.39661 avg_loss = 3.98030\n",
      "epoch no.0 train no.62990  loss = 2.31208 avg_loss = 3.92402\n",
      "epoch no.0 train no.63000  loss = 2.28515 avg_loss = 3.94199\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁위한', '▁음악', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.0 train no.63010  loss = 5.35160 avg_loss = 3.94754\n",
      "epoch no.0 train no.63020  loss = 4.65680 avg_loss = 4.02611\n",
      "epoch no.0 train no.63030  loss = 4.13279 avg_loss = 4.01570\n",
      "epoch no.0 train no.63040  loss = 2.95732 avg_loss = 3.99262\n",
      "epoch no.0 train no.63050  loss = 2.66665 avg_loss = 4.02678\n",
      "epoch no.0 train no.63060  loss = 7.00525 avg_loss = 4.03292\n",
      "epoch no.0 train no.63070  loss = 5.51230 avg_loss = 4.03613\n",
      "epoch no.0 train no.63080  loss = 6.12236 avg_loss = 4.02650\n",
      "epoch no.0 train no.63090  loss = 1.71991 avg_loss = 3.96917\n",
      "epoch no.0 train no.63100  loss = 2.47692 avg_loss = 3.95486\n",
      "epoch no.0 train no.63110  loss = 3.60617 avg_loss = 3.93440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.63120  loss = 2.66687 avg_loss = 3.96541\n",
      "epoch no.0 train no.63130  loss = 2.11202 avg_loss = 3.96573\n",
      "epoch no.0 train no.63140  loss = 3.34385 avg_loss = 3.96529\n",
      "epoch no.0 train no.63150  loss = 3.72267 avg_loss = 4.00329\n",
      "epoch no.0 train no.63160  loss = 3.13801 avg_loss = 3.97727\n",
      "epoch no.0 train no.63170  loss = 2.43215 avg_loss = 3.96599\n",
      "epoch no.0 train no.63180  loss = 2.34768 avg_loss = 3.94243\n",
      "epoch no.0 train no.63190  loss = 3.55957 avg_loss = 3.88362\n",
      "epoch no.0 train no.63200  loss = 5.10093 avg_loss = 3.90169\n",
      "epoch no.0 train no.63210  loss = 4.50165 avg_loss = 3.86739\n",
      "epoch no.0 train no.63220  loss = 4.27130 avg_loss = 3.87500\n",
      "epoch no.0 train no.63230  loss = 1.96183 avg_loss = 3.88488\n",
      "epoch no.0 train no.63240  loss = 4.76916 avg_loss = 3.89934\n",
      "epoch no.0 train no.63250  loss = 4.59222 avg_loss = 3.90471\n",
      "epoch no.0 train no.63260  loss = 4.65415 avg_loss = 3.91299\n",
      "epoch no.0 train no.63270  loss = 5.01287 avg_loss = 3.91131\n",
      "epoch no.0 train no.63280  loss = 3.76803 avg_loss = 3.92657\n",
      "epoch no.0 train no.63290  loss = 4.51302 avg_loss = 3.99026\n",
      "epoch no.0 train no.63300  loss = 2.98655 avg_loss = 4.03501\n",
      "epoch no.0 train no.63310  loss = 4.24133 avg_loss = 4.02331\n",
      "epoch no.0 train no.63320  loss = 1.88669 avg_loss = 4.02895\n",
      "epoch no.0 train no.63330  loss = 2.30228 avg_loss = 4.06152\n",
      "epoch no.0 train no.63340  loss = 1.66109 avg_loss = 4.03910\n",
      "epoch no.0 train no.63350  loss = 5.41374 avg_loss = 4.04882\n",
      "epoch no.0 train no.63360  loss = 4.12993 avg_loss = 4.07235\n",
      "epoch no.0 train no.63370  loss = 3.56706 avg_loss = 4.04941\n",
      "epoch no.0 train no.63380  loss = 5.11885 avg_loss = 4.11552\n",
      "epoch no.0 train no.63390  loss = 3.77422 avg_loss = 4.11489\n",
      "epoch no.0 train no.63400  loss = 2.25125 avg_loss = 4.05597\n",
      "epoch no.0 train no.63410  loss = 4.51817 avg_loss = 4.10225\n",
      "epoch no.0 train no.63420  loss = 3.31846 avg_loss = 4.12264\n",
      "epoch no.0 train no.63430  loss = 6.66441 avg_loss = 4.11227\n",
      "epoch no.0 train no.63440  loss = 3.71162 avg_loss = 4.11849\n",
      "epoch no.0 train no.63450  loss = 2.94959 avg_loss = 4.08710\n",
      "epoch no.0 train no.63460  loss = 7.70478 avg_loss = 4.12107\n",
      "epoch no.0 train no.63470  loss = 2.49074 avg_loss = 4.08487\n",
      "epoch no.0 train no.63480  loss = 3.93203 avg_loss = 4.01723\n",
      "epoch no.0 train no.63490  loss = 2.53222 avg_loss = 3.97883\n",
      "epoch no.0 train no.63500  loss = 3.12567 avg_loss = 3.97618\n",
      "epoch no.0 train no.63510  loss = 4.22684 avg_loss = 4.00277\n",
      "epoch no.0 train no.63520  loss = 5.96784 avg_loss = 4.01712\n",
      "epoch no.0 train no.63530  loss = 4.39078 avg_loss = 4.00095\n",
      "epoch no.0 train no.63540  loss = 3.26501 avg_loss = 3.99218\n",
      "epoch no.0 train no.63550  loss = 3.86883 avg_loss = 4.00356\n",
      "epoch no.0 train no.63560  loss = 5.33902 avg_loss = 4.03049\n",
      "epoch no.0 train no.63570  loss = 4.23200 avg_loss = 4.00303\n",
      "epoch no.0 train no.63580  loss = 4.91058 avg_loss = 3.99174\n",
      "epoch no.0 train no.63590  loss = 3.61983 avg_loss = 4.02348\n",
      "epoch no.0 train no.63600  loss = 2.32972 avg_loss = 4.01667\n",
      "epoch no.0 train no.63610  loss = 3.45031 avg_loss = 3.94994\n",
      "epoch no.0 train no.63620  loss = 3.12929 avg_loss = 3.93228\n",
      "epoch no.0 train no.63630  loss = 3.69453 avg_loss = 3.90890\n",
      "epoch no.0 train no.63640  loss = 3.94585 avg_loss = 3.90691\n",
      "epoch no.0 train no.63650  loss = 2.35601 avg_loss = 3.94177\n",
      "epoch no.0 train no.63660  loss = 5.67653 avg_loss = 3.95369\n",
      "epoch no.0 train no.63670  loss = 4.44453 avg_loss = 3.94157\n",
      "epoch no.0 train no.63680  loss = 5.60323 avg_loss = 3.94975\n",
      "epoch no.0 train no.63690  loss = 2.59078 avg_loss = 3.96464\n",
      "epoch no.0 train no.63700  loss = 3.81658 avg_loss = 3.93456\n",
      "epoch no.0 train no.63710  loss = 2.86017 avg_loss = 3.94727\n",
      "epoch no.0 train no.63720  loss = 4.64423 avg_loss = 3.92150\n",
      "epoch no.0 train no.63730  loss = 3.00588 avg_loss = 3.88912\n",
      "epoch no.0 train no.63740  loss = 5.07238 avg_loss = 3.90028\n",
      "epoch no.0 train no.63750  loss = 3.73491 avg_loss = 3.93596\n",
      "epoch no.0 train no.63760  loss = 4.01715 avg_loss = 3.91844\n",
      "epoch no.0 train no.63770  loss = 3.85268 avg_loss = 3.94348\n",
      "epoch no.0 train no.63780  loss = 2.88571 avg_loss = 3.99611\n",
      "epoch no.0 train no.63790  loss = 6.46884 avg_loss = 4.02226\n",
      "epoch no.0 train no.63800  loss = 3.66002 avg_loss = 3.99378\n",
      "epoch no.0 train no.63810  loss = 3.26397 avg_loss = 3.95959\n",
      "epoch no.0 train no.63820  loss = 4.46057 avg_loss = 3.99605\n",
      "epoch no.0 train no.63830  loss = 8.41240 avg_loss = 4.07012\n",
      "epoch no.0 train no.63840  loss = 5.73882 avg_loss = 4.09727\n",
      "epoch no.0 train no.63850  loss = 3.64138 avg_loss = 4.07483\n",
      "epoch no.0 train no.63860  loss = 4.91285 avg_loss = 4.06081\n",
      "epoch no.0 train no.63870  loss = 5.66665 avg_loss = 4.06520\n",
      "epoch no.0 train no.63880  loss = 4.71898 avg_loss = 4.04555\n",
      "epoch no.0 train no.63890  loss = 3.25671 avg_loss = 4.00821\n",
      "epoch no.0 train no.63900  loss = 3.55238 avg_loss = 3.99876\n",
      "epoch no.0 train no.63910  loss = 3.47600 avg_loss = 3.95657\n",
      "epoch no.0 train no.63920  loss = 6.36156 avg_loss = 3.99101\n",
      "epoch no.0 train no.63930  loss = 5.92523 avg_loss = 3.97982\n",
      "epoch no.0 train no.63940  loss = 3.76426 avg_loss = 3.89164\n",
      "epoch no.0 train no.63950  loss = 5.15418 avg_loss = 3.91525\n",
      "epoch no.0 train no.63960  loss = 3.27691 avg_loss = 3.98965\n",
      "epoch no.0 train no.63970  loss = 5.64540 avg_loss = 3.98852\n",
      "epoch no.0 train no.63980  loss = 4.37553 avg_loss = 3.94856\n",
      "epoch no.0 train no.63990  loss = 5.17496 avg_loss = 3.99440\n",
      "epoch no.0 train no.64000  loss = 3.67866 avg_loss = 4.03514\n",
      "2\n",
      "to_tokens: ['▁가을', '전환', '이', '</s>', '</s>']\n",
      "기분전환용 노래</s>\n",
      "epoch no.0 train no.64010  loss = 4.02929 avg_loss = 4.00467\n",
      "epoch no.0 train no.64020  loss = 5.83459 avg_loss = 4.00802\n",
      "epoch no.0 train no.64030  loss = 5.80434 avg_loss = 4.05335\n",
      "epoch no.0 train no.64040  loss = 2.75727 avg_loss = 4.02928\n",
      "epoch no.0 train no.64050  loss = 4.69655 avg_loss = 3.99832\n",
      "epoch no.0 train no.64060  loss = 4.25378 avg_loss = 4.05518\n",
      "epoch no.0 train no.64070  loss = 3.12725 avg_loss = 4.03646\n",
      "epoch no.0 train no.64080  loss = 2.70192 avg_loss = 4.03856\n",
      "epoch no.0 train no.64090  loss = 3.89633 avg_loss = 4.00709\n",
      "epoch no.0 train no.64100  loss = 3.86724 avg_loss = 3.97308\n",
      "epoch no.0 train no.64110  loss = 5.55987 avg_loss = 3.97527\n",
      "epoch no.0 train no.64120  loss = 3.68979 avg_loss = 3.96597\n",
      "epoch no.0 train no.64130  loss = 3.23415 avg_loss = 4.01279\n",
      "epoch no.0 train no.64140  loss = 3.01706 avg_loss = 4.04167\n",
      "epoch no.0 train no.64150  loss = 2.06510 avg_loss = 4.07796\n",
      "epoch no.0 train no.64160  loss = 3.16255 avg_loss = 4.02959\n",
      "epoch no.0 train no.64170  loss = 3.49435 avg_loss = 4.02242\n",
      "epoch no.0 train no.64180  loss = 6.01197 avg_loss = 4.00490\n",
      "epoch no.0 train no.64190  loss = 3.79706 avg_loss = 3.99555\n",
      "epoch no.0 train no.64200  loss = 3.21356 avg_loss = 3.96783\n",
      "epoch no.0 train no.64210  loss = 3.20596 avg_loss = 3.96206\n",
      "epoch no.0 train no.64220  loss = 4.94023 avg_loss = 3.96847\n",
      "epoch no.0 train no.64230  loss = 6.65466 avg_loss = 4.00679\n",
      "epoch no.0 train no.64240  loss = 3.71773 avg_loss = 4.02919\n",
      "epoch no.0 train no.64250  loss = 3.28566 avg_loss = 4.00688\n",
      "epoch no.0 train no.64260  loss = 4.51855 avg_loss = 3.97865\n",
      "epoch no.0 train no.64270  loss = 5.78977 avg_loss = 3.94984\n",
      "epoch no.0 train no.64280  loss = 2.57314 avg_loss = 3.94156\n",
      "epoch no.0 train no.64290  loss = 3.39619 avg_loss = 3.94377\n",
      "epoch no.0 train no.64300  loss = 3.26320 avg_loss = 3.93477\n",
      "epoch no.0 train no.64310  loss = 5.11970 avg_loss = 3.94016\n",
      "epoch no.0 train no.64320  loss = 3.63419 avg_loss = 3.97318\n",
      "epoch no.0 train no.64330  loss = 4.13472 avg_loss = 4.01396\n",
      "epoch no.0 train no.64340  loss = 2.04588 avg_loss = 3.99153\n",
      "epoch no.0 train no.64350  loss = 5.04988 avg_loss = 4.01479\n",
      "epoch no.0 train no.64360  loss = 2.80918 avg_loss = 3.98456\n",
      "epoch no.0 train no.64370  loss = 4.28984 avg_loss = 4.06193\n",
      "epoch no.0 train no.64380  loss = 2.53122 avg_loss = 4.03539\n",
      "epoch no.0 train no.64390  loss = 4.02265 avg_loss = 4.00848\n",
      "epoch no.0 train no.64400  loss = 1.97541 avg_loss = 3.97456\n",
      "epoch no.0 train no.64410  loss = 5.82692 avg_loss = 4.00575\n",
      "epoch no.0 train no.64420  loss = 3.35154 avg_loss = 4.03988\n",
      "epoch no.0 train no.64430  loss = 2.72858 avg_loss = 3.99857\n",
      "epoch no.0 train no.64440  loss = 3.72650 avg_loss = 3.96127\n",
      "epoch no.0 train no.64450  loss = 3.50161 avg_loss = 3.94981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.64460  loss = 3.16512 avg_loss = 3.99677\n",
      "epoch no.0 train no.64470  loss = 5.98772 avg_loss = 4.01908\n",
      "epoch no.0 train no.64480  loss = 3.40781 avg_loss = 3.96297\n",
      "epoch no.0 train no.64490  loss = 3.20227 avg_loss = 3.97221\n",
      "epoch no.0 train no.64500  loss = 2.34940 avg_loss = 3.96025\n",
      "epoch no.0 train no.64510  loss = 3.62861 avg_loss = 4.02123\n",
      "epoch no.0 train no.64520  loss = 3.37315 avg_loss = 4.01853\n",
      "epoch no.0 train no.64530  loss = 3.63525 avg_loss = 4.07605\n",
      "epoch no.0 train no.64540  loss = 3.46400 avg_loss = 4.06968\n",
      "epoch no.0 train no.64550  loss = 5.59102 avg_loss = 4.07359\n",
      "epoch no.0 train no.64560  loss = 3.17268 avg_loss = 4.11473\n",
      "epoch no.0 train no.64570  loss = 7.72385 avg_loss = 4.12589\n",
      "epoch no.0 train no.64580  loss = 2.32111 avg_loss = 4.11458\n",
      "epoch no.0 train no.64590  loss = 6.26484 avg_loss = 4.13930\n",
      "epoch no.0 train no.64600  loss = 2.27484 avg_loss = 4.10886\n",
      "epoch no.0 train no.64610  loss = 3.59294 avg_loss = 4.03688\n",
      "epoch no.0 train no.64620  loss = 2.86274 avg_loss = 4.09990\n",
      "epoch no.0 train no.64630  loss = 4.21545 avg_loss = 4.11780\n",
      "epoch no.0 train no.64640  loss = 2.39999 avg_loss = 4.08270\n",
      "epoch no.0 train no.64650  loss = 4.84270 avg_loss = 4.09591\n",
      "epoch no.0 train no.64660  loss = 4.18460 avg_loss = 4.09036\n",
      "epoch no.0 train no.64670  loss = 3.55286 avg_loss = 4.07068\n",
      "epoch no.0 train no.64680  loss = 5.70392 avg_loss = 4.12064\n",
      "epoch no.0 train no.64690  loss = 4.89055 avg_loss = 4.20569\n",
      "epoch no.0 train no.64700  loss = 3.00428 avg_loss = 4.11986\n",
      "epoch no.0 train no.64710  loss = 4.04408 avg_loss = 4.10956\n",
      "epoch no.0 train no.64720  loss = 5.84318 avg_loss = 4.15944\n",
      "epoch no.0 train no.64730  loss = 4.13433 avg_loss = 4.11646\n",
      "epoch no.0 train no.64740  loss = 3.29502 avg_loss = 4.06903\n",
      "epoch no.0 train no.64750  loss = 7.18458 avg_loss = 4.08816\n",
      "epoch no.0 train no.64760  loss = 3.23829 avg_loss = 4.00766\n",
      "epoch no.0 train no.64770  loss = 2.55715 avg_loss = 3.99375\n",
      "epoch no.0 train no.64780  loss = 3.81469 avg_loss = 4.01745\n",
      "epoch no.0 train no.64790  loss = 3.95463 avg_loss = 4.08489\n",
      "epoch no.0 train no.64800  loss = 6.17151 avg_loss = 4.08177\n",
      "epoch no.0 train no.64810  loss = 3.42900 avg_loss = 4.06352\n",
      "epoch no.0 train no.64820  loss = 4.53846 avg_loss = 4.02687\n",
      "epoch no.0 train no.64830  loss = 3.88578 avg_loss = 4.11946\n",
      "epoch no.0 train no.64840  loss = 2.91735 avg_loss = 4.11806\n",
      "epoch no.0 train no.64850  loss = 2.17108 avg_loss = 4.05581\n",
      "epoch no.0 train no.64860  loss = 3.58872 avg_loss = 4.06317\n",
      "epoch no.0 train no.64870  loss = 4.27887 avg_loss = 4.05281\n",
      "epoch no.0 train no.64880  loss = 2.90660 avg_loss = 4.04136\n",
      "epoch no.0 train no.64890  loss = 2.01292 avg_loss = 4.01358\n",
      "epoch no.0 train no.64900  loss = 4.35619 avg_loss = 4.14348\n",
      "epoch no.0 train no.64910  loss = 2.68252 avg_loss = 4.13632\n",
      "epoch no.0 train no.64920  loss = 3.64640 avg_loss = 4.18443\n",
      "epoch no.0 train no.64930  loss = 1.68903 avg_loss = 4.15100\n",
      "epoch no.0 train no.64940  loss = 3.69575 avg_loss = 4.16353\n",
      "epoch no.0 train no.64950  loss = 5.35008 avg_loss = 4.12383\n",
      "epoch no.0 train no.64960  loss = 4.12659 avg_loss = 4.11787\n",
      "epoch no.0 train no.64970  loss = 6.12488 avg_loss = 4.14082\n",
      "epoch no.0 train no.64980  loss = 3.86378 avg_loss = 4.11561\n",
      "epoch no.0 train no.64990  loss = 2.33626 avg_loss = 4.07195\n",
      "epoch no.0 train no.65000  loss = 5.16088 avg_loss = 4.12482\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋', '이', '▁노래', '송', '</s>']\n",
      "기분전환 되는 팝송</s>\n",
      "epoch no.0 train no.65010  loss = 3.09170 avg_loss = 4.12096\n",
      "epoch no.0 train no.65020  loss = 2.39132 avg_loss = 4.05353\n",
      "epoch no.0 train no.65030  loss = 1.90213 avg_loss = 3.98066\n",
      "epoch no.0 train no.65040  loss = 4.87510 avg_loss = 4.01609\n",
      "epoch no.0 train no.65050  loss = 6.62606 avg_loss = 4.01376\n",
      "epoch no.0 train no.65060  loss = 2.95855 avg_loss = 4.02358\n",
      "epoch no.0 train no.65070  loss = 5.00717 avg_loss = 4.10774\n",
      "epoch no.0 train no.65080  loss = 3.56898 avg_loss = 4.04920\n",
      "epoch no.0 train no.65090  loss = 3.84638 avg_loss = 4.00769\n",
      "epoch no.0 train no.65100  loss = 4.59177 avg_loss = 4.00456\n",
      "epoch no.0 train no.65110  loss = 3.63525 avg_loss = 4.00707\n",
      "epoch no.0 train no.65120  loss = 3.29000 avg_loss = 3.98632\n",
      "epoch no.0 train no.65130  loss = 2.00411 avg_loss = 4.01434\n",
      "epoch no.0 train no.65140  loss = 3.45675 avg_loss = 3.94100\n",
      "epoch no.0 train no.65150  loss = 3.88559 avg_loss = 3.96139\n",
      "epoch no.0 train no.65160  loss = 1.99416 avg_loss = 3.93977\n",
      "epoch no.0 train no.65170  loss = 4.03695 avg_loss = 3.92230\n",
      "epoch no.0 train no.65180  loss = 3.68931 avg_loss = 3.91804\n",
      "epoch no.0 train no.65190  loss = 3.41316 avg_loss = 3.90736\n",
      "epoch no.0 train no.65200  loss = 5.48997 avg_loss = 3.90717\n",
      "epoch no.0 train no.65210  loss = 3.84308 avg_loss = 3.87356\n",
      "epoch no.0 train no.65220  loss = 4.98069 avg_loss = 3.89584\n",
      "epoch no.0 train no.65230  loss = 4.01014 avg_loss = 3.87239\n",
      "epoch no.0 train no.65240  loss = 3.87983 avg_loss = 3.83508\n",
      "epoch no.0 train no.65250  loss = 5.29587 avg_loss = 3.85121\n",
      "epoch no.0 train no.65260  loss = 5.45857 avg_loss = 3.86904\n",
      "epoch no.0 train no.65270  loss = 2.69027 avg_loss = 3.89015\n",
      "epoch no.0 train no.65280  loss = 4.82093 avg_loss = 3.88796\n",
      "epoch no.0 train no.65290  loss = 6.06935 avg_loss = 3.88276\n",
      "epoch no.0 train no.65300  loss = 5.90876 avg_loss = 3.90415\n",
      "epoch no.0 train no.65310  loss = 7.25300 avg_loss = 4.00665\n",
      "epoch no.0 train no.65320  loss = 4.81878 avg_loss = 4.06813\n",
      "epoch no.0 train no.65330  loss = 3.25723 avg_loss = 4.05816\n",
      "epoch no.0 train no.65340  loss = 1.62317 avg_loss = 4.02104\n",
      "epoch no.0 train no.65350  loss = 5.29654 avg_loss = 3.97209\n",
      "epoch no.0 train no.65360  loss = 6.88181 avg_loss = 4.01710\n",
      "epoch no.0 train no.65370  loss = 6.46731 avg_loss = 4.01459\n",
      "epoch no.0 train no.65380  loss = 4.48317 avg_loss = 4.06672\n",
      "epoch no.0 train no.65390  loss = 4.76286 avg_loss = 4.06627\n",
      "epoch no.0 train no.65400  loss = 3.65060 avg_loss = 3.99145\n",
      "epoch no.0 train no.65410  loss = 3.20996 avg_loss = 4.02969\n",
      "epoch no.0 train no.65420  loss = 4.02398 avg_loss = 4.07988\n",
      "epoch no.0 train no.65430  loss = 5.59372 avg_loss = 4.07023\n",
      "epoch no.0 train no.65440  loss = 4.33377 avg_loss = 4.08350\n",
      "epoch no.0 train no.65450  loss = 4.29068 avg_loss = 4.09224\n",
      "epoch no.0 train no.65460  loss = 3.83036 avg_loss = 4.05592\n",
      "epoch no.0 train no.65470  loss = 2.97652 avg_loss = 4.06157\n",
      "epoch no.0 train no.65480  loss = 5.23461 avg_loss = 4.06155\n",
      "epoch no.0 train no.65490  loss = 2.84583 avg_loss = 3.98592\n",
      "epoch no.0 train no.65500  loss = 3.12495 avg_loss = 3.95337\n",
      "epoch no.0 train no.65510  loss = 3.24009 avg_loss = 3.91432\n",
      "epoch no.0 train no.65520  loss = 4.07504 avg_loss = 3.93552\n",
      "epoch no.0 train no.65530  loss = 2.27063 avg_loss = 3.90410\n",
      "epoch no.0 train no.65540  loss = 2.07576 avg_loss = 3.88082\n",
      "epoch no.0 train no.65550  loss = 3.38922 avg_loss = 3.86874\n",
      "epoch no.0 train no.65560  loss = 3.88881 avg_loss = 3.84767\n",
      "epoch no.0 train no.65570  loss = 2.96846 avg_loss = 3.86248\n",
      "epoch no.0 train no.65580  loss = 3.79705 avg_loss = 3.88539\n",
      "epoch no.0 train no.65590  loss = 3.47363 avg_loss = 3.90612\n",
      "epoch no.0 train no.65600  loss = 4.68212 avg_loss = 3.92768\n",
      "epoch no.0 train no.65610  loss = 3.83968 avg_loss = 3.92341\n",
      "epoch no.0 train no.65620  loss = 3.46166 avg_loss = 3.91573\n",
      "epoch no.0 train no.65630  loss = 3.49382 avg_loss = 3.93286\n",
      "epoch no.0 train no.65640  loss = 3.68909 avg_loss = 3.92417\n",
      "epoch no.0 train no.65650  loss = 3.59528 avg_loss = 3.91893\n",
      "epoch no.0 train no.65660  loss = 4.24929 avg_loss = 3.92215\n",
      "epoch no.0 train no.65670  loss = 3.84988 avg_loss = 3.95423\n",
      "epoch no.0 train no.65680  loss = 3.17965 avg_loss = 3.97032\n",
      "epoch no.0 train no.65690  loss = 3.69825 avg_loss = 3.97291\n",
      "epoch no.0 train no.65700  loss = 4.04006 avg_loss = 4.00735\n",
      "epoch no.0 train no.65710  loss = 3.15572 avg_loss = 4.06159\n",
      "epoch no.0 train no.65720  loss = 5.46160 avg_loss = 4.09814\n",
      "epoch no.0 train no.65730  loss = 3.05560 avg_loss = 4.04546\n",
      "epoch no.0 train no.65740  loss = 1.96949 avg_loss = 4.03826\n",
      "epoch no.0 train no.65750  loss = 3.17114 avg_loss = 4.00114\n",
      "epoch no.0 train no.65760  loss = 2.37625 avg_loss = 4.02067\n",
      "epoch no.0 train no.65770  loss = 5.79670 avg_loss = 4.01972\n",
      "epoch no.0 train no.65780  loss = 5.97225 avg_loss = 4.07387\n",
      "epoch no.0 train no.65790  loss = 3.42987 avg_loss = 4.04531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.65800  loss = 2.94910 avg_loss = 4.02137\n",
      "epoch no.0 train no.65810  loss = 3.57817 avg_loss = 3.99771\n",
      "epoch no.0 train no.65820  loss = 3.26125 avg_loss = 4.02314\n",
      "epoch no.0 train no.65830  loss = 3.15327 avg_loss = 3.94809\n",
      "epoch no.0 train no.65840  loss = 3.59388 avg_loss = 3.95093\n",
      "epoch no.0 train no.65850  loss = 4.11907 avg_loss = 3.93582\n",
      "epoch no.0 train no.65860  loss = 2.66012 avg_loss = 3.98914\n",
      "epoch no.0 train no.65870  loss = 5.24962 avg_loss = 3.99795\n",
      "epoch no.0 train no.65880  loss = 4.51854 avg_loss = 3.99578\n",
      "epoch no.0 train no.65890  loss = 2.47499 avg_loss = 3.91730\n",
      "epoch no.0 train no.65900  loss = 3.93985 avg_loss = 3.93864\n",
      "epoch no.0 train no.65910  loss = 4.22236 avg_loss = 3.92849\n",
      "epoch no.0 train no.65920  loss = 4.66494 avg_loss = 3.92934\n",
      "epoch no.0 train no.65930  loss = 2.71077 avg_loss = 3.94072\n",
      "epoch no.0 train no.65940  loss = 9.56598 avg_loss = 3.90690\n",
      "epoch no.0 train no.65950  loss = 3.79435 avg_loss = 3.86769\n",
      "epoch no.0 train no.65960  loss = 6.67602 avg_loss = 3.91833\n",
      "epoch no.0 train no.65970  loss = 4.50445 avg_loss = 3.95528\n",
      "epoch no.0 train no.65980  loss = 2.87259 avg_loss = 4.00556\n",
      "epoch no.0 train no.65990  loss = 2.92936 avg_loss = 4.00069\n",
      "epoch no.0 train no.66000  loss = 1.70648 avg_loss = 3.96654\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '</s>', '</s>']\n",
      "기분전환을 위한 팝송</s>\n",
      "epoch no.0 train no.66010  loss = 5.71925 avg_loss = 3.97531\n",
      "epoch no.0 train no.66020  loss = 4.42445 avg_loss = 3.97191\n",
      "epoch no.0 train no.66030  loss = 7.06782 avg_loss = 3.94702\n",
      "epoch no.0 train no.66040  loss = 5.55813 avg_loss = 3.93842\n",
      "epoch no.0 train no.66050  loss = 3.33497 avg_loss = 3.87617\n",
      "epoch no.0 train no.66060  loss = 2.60242 avg_loss = 3.86560\n",
      "epoch no.0 train no.66070  loss = 2.97431 avg_loss = 3.87032\n",
      "epoch no.0 train no.66080  loss = 4.20435 avg_loss = 3.92847\n",
      "epoch no.0 train no.66090  loss = 2.60568 avg_loss = 3.89127\n",
      "epoch no.0 train no.66100  loss = 2.11283 avg_loss = 3.83522\n",
      "epoch no.0 train no.66110  loss = 4.78819 avg_loss = 3.85197\n",
      "epoch no.0 train no.66120  loss = 3.51321 avg_loss = 3.89367\n",
      "epoch no.0 train no.66130  loss = 4.58797 avg_loss = 3.87384\n",
      "epoch no.0 train no.66140  loss = 4.58569 avg_loss = 3.94142\n",
      "epoch no.0 train no.66150  loss = 1.61667 avg_loss = 3.91205\n",
      "epoch no.0 train no.66160  loss = 2.75237 avg_loss = 3.86715\n",
      "epoch no.0 train no.66170  loss = 5.82672 avg_loss = 3.95787\n",
      "epoch no.0 train no.66180  loss = 4.79725 avg_loss = 3.97779\n",
      "epoch no.0 train no.66190  loss = 2.83470 avg_loss = 3.99429\n",
      "epoch no.0 train no.66200  loss = 2.60461 avg_loss = 4.00538\n",
      "epoch no.0 train no.66210  loss = 4.72291 avg_loss = 4.01896\n",
      "epoch no.0 train no.66220  loss = 2.47510 avg_loss = 3.96372\n",
      "epoch no.0 train no.66230  loss = 2.60686 avg_loss = 3.98382\n",
      "epoch no.0 train no.66240  loss = 6.69963 avg_loss = 4.00664\n",
      "epoch no.0 train no.66250  loss = 2.81016 avg_loss = 3.96016\n",
      "epoch no.0 train no.66260  loss = 4.48250 avg_loss = 4.01889\n",
      "epoch no.0 train no.66270  loss = 1.80597 avg_loss = 4.01660\n",
      "epoch no.0 train no.66280  loss = 4.31643 avg_loss = 4.07197\n",
      "epoch no.0 train no.66290  loss = 4.87582 avg_loss = 4.10495\n",
      "epoch no.0 train no.66300  loss = 6.83331 avg_loss = 4.12600\n",
      "epoch no.0 train no.66310  loss = 4.22182 avg_loss = 4.16040\n",
      "epoch no.0 train no.66320  loss = 3.74815 avg_loss = 4.16062\n",
      "epoch no.0 train no.66330  loss = 6.38275 avg_loss = 4.17588\n",
      "epoch no.0 train no.66340  loss = 2.23345 avg_loss = 4.15002\n",
      "epoch no.0 train no.66350  loss = 3.16578 avg_loss = 4.12209\n",
      "epoch no.0 train no.66360  loss = 5.27845 avg_loss = 4.08815\n",
      "epoch no.0 train no.66370  loss = 2.60597 avg_loss = 4.05513\n",
      "epoch no.0 train no.66380  loss = 4.30177 avg_loss = 4.01974\n",
      "epoch no.0 train no.66390  loss = 4.93237 avg_loss = 4.01759\n",
      "epoch no.0 train no.66400  loss = 5.20986 avg_loss = 4.03310\n",
      "epoch no.0 train no.66410  loss = 2.65637 avg_loss = 3.95019\n",
      "epoch no.0 train no.66420  loss = 3.48880 avg_loss = 3.97344\n",
      "epoch no.0 train no.66430  loss = 3.43883 avg_loss = 4.07198\n",
      "epoch no.0 train no.66440  loss = 5.95719 avg_loss = 4.02714\n",
      "epoch no.0 train no.66450  loss = 2.37661 avg_loss = 4.10756\n",
      "epoch no.0 train no.66460  loss = 3.05659 avg_loss = 4.16051\n",
      "epoch no.0 train no.66470  loss = 1.64728 avg_loss = 4.16903\n",
      "epoch no.0 train no.66480  loss = 5.38791 avg_loss = 4.10566\n",
      "epoch no.0 train no.66490  loss = 3.89325 avg_loss = 4.07950\n",
      "epoch no.0 train no.66500  loss = 2.94711 avg_loss = 4.04688\n",
      "epoch no.0 train no.66510  loss = 3.89943 avg_loss = 4.11378\n",
      "epoch no.0 train no.66520  loss = 3.71371 avg_loss = 4.01878\n",
      "epoch no.0 train no.66530  loss = 5.25318 avg_loss = 4.00169\n",
      "epoch no.0 train no.66540  loss = 3.41381 avg_loss = 4.01504\n",
      "epoch no.0 train no.66550  loss = 4.08897 avg_loss = 4.00868\n",
      "epoch no.0 train no.66560  loss = 4.29683 avg_loss = 4.00277\n",
      "epoch no.0 train no.66570  loss = 4.07035 avg_loss = 3.99032\n",
      "epoch no.0 train no.66580  loss = 3.39813 avg_loss = 3.95744\n",
      "epoch no.0 train no.66590  loss = 2.25747 avg_loss = 3.97132\n",
      "epoch no.0 train no.66600  loss = 2.87381 avg_loss = 3.99997\n",
      "epoch no.0 train no.66610  loss = 3.36927 avg_loss = 3.97089\n",
      "epoch no.0 train no.66620  loss = 2.93599 avg_loss = 4.04545\n",
      "epoch no.0 train no.66630  loss = 2.92690 avg_loss = 4.01597\n",
      "epoch no.0 train no.66640  loss = 4.87792 avg_loss = 3.94101\n",
      "epoch no.0 train no.66650  loss = 3.87497 avg_loss = 3.94890\n",
      "epoch no.0 train no.66660  loss = 5.17388 avg_loss = 3.94211\n",
      "epoch no.0 train no.66670  loss = 4.30491 avg_loss = 3.95536\n",
      "epoch no.0 train no.66680  loss = 4.99788 avg_loss = 3.97773\n",
      "epoch no.0 train no.66690  loss = 2.56393 avg_loss = 4.01388\n",
      "epoch no.0 train no.66700  loss = 7.11109 avg_loss = 4.03069\n",
      "epoch no.0 train no.66710  loss = 3.78621 avg_loss = 4.06750\n",
      "epoch no.0 train no.66720  loss = 6.42196 avg_loss = 4.10342\n",
      "epoch no.0 train no.66730  loss = 2.68180 avg_loss = 4.06503\n",
      "epoch no.0 train no.66740  loss = 4.82844 avg_loss = 4.03515\n",
      "epoch no.0 train no.66750  loss = 2.94299 avg_loss = 4.01184\n",
      "epoch no.0 train no.66760  loss = 3.41805 avg_loss = 4.01733\n",
      "epoch no.0 train no.66770  loss = 5.73101 avg_loss = 4.06734\n",
      "epoch no.0 train no.66780  loss = 4.15991 avg_loss = 4.06330\n",
      "epoch no.0 train no.66790  loss = 5.38920 avg_loss = 3.97085\n",
      "epoch no.0 train no.66800  loss = 4.53995 avg_loss = 3.98299\n",
      "epoch no.0 train no.66810  loss = 4.61411 avg_loss = 3.99907\n",
      "epoch no.0 train no.66820  loss = 2.48010 avg_loss = 4.00304\n",
      "epoch no.0 train no.66830  loss = 3.92512 avg_loss = 4.02323\n",
      "epoch no.0 train no.66840  loss = 3.40923 avg_loss = 3.98493\n",
      "epoch no.0 train no.66850  loss = 6.81797 avg_loss = 4.04506\n",
      "epoch no.0 train no.66860  loss = 4.92671 avg_loss = 4.03964\n",
      "epoch no.0 train no.66870  loss = 3.05399 avg_loss = 3.97303\n",
      "epoch no.0 train no.66880  loss = 2.59454 avg_loss = 3.96902\n",
      "epoch no.0 train no.66890  loss = 4.97996 avg_loss = 3.96711\n",
      "epoch no.0 train no.66900  loss = 3.31595 avg_loss = 4.01154\n",
      "epoch no.0 train no.66910  loss = 3.75827 avg_loss = 4.05787\n",
      "epoch no.0 train no.66920  loss = 2.54436 avg_loss = 4.00028\n",
      "epoch no.0 train no.66930  loss = 2.45250 avg_loss = 4.01946\n",
      "epoch no.0 train no.66940  loss = 4.69787 avg_loss = 4.00399\n",
      "epoch no.0 train no.66950  loss = 3.79290 avg_loss = 3.98964\n",
      "epoch no.0 train no.66960  loss = 3.01863 avg_loss = 3.90968\n",
      "epoch no.0 train no.66970  loss = 3.54664 avg_loss = 3.94484\n",
      "epoch no.0 train no.66980  loss = 2.78832 avg_loss = 3.93808\n",
      "epoch no.0 train no.66990  loss = 3.75409 avg_loss = 3.98456\n",
      "epoch no.0 train no.67000  loss = 3.13331 avg_loss = 3.93251\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '싶', '을', '때', '</s>']\n",
      "기분전환 하고싶을때</s>\n",
      "epoch no.0 train no.67010  loss = 3.19953 avg_loss = 3.98646\n",
      "epoch no.0 train no.67020  loss = 2.84510 avg_loss = 3.96448\n",
      "epoch no.0 train no.67030  loss = 3.19556 avg_loss = 3.96319\n",
      "epoch no.0 train no.67040  loss = 4.01051 avg_loss = 3.96756\n",
      "epoch no.0 train no.67050  loss = 2.66019 avg_loss = 3.89809\n",
      "epoch no.0 train no.67060  loss = 3.26653 avg_loss = 3.91314\n",
      "epoch no.0 train no.67070  loss = 5.58162 avg_loss = 3.94991\n",
      "epoch no.0 train no.67080  loss = 2.73817 avg_loss = 3.90427\n",
      "epoch no.0 train no.67090  loss = 5.39597 avg_loss = 3.89804\n",
      "epoch no.0 train no.67100  loss = 2.72243 avg_loss = 3.92468\n",
      "epoch no.0 train no.67110  loss = 5.93288 avg_loss = 3.94992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.67120  loss = 3.02203 avg_loss = 3.93855\n",
      "epoch no.0 train no.67130  loss = 4.35293 avg_loss = 3.90960\n",
      "epoch no.0 train no.67140  loss = 4.63550 avg_loss = 3.90843\n",
      "epoch no.0 train no.67150  loss = 3.90743 avg_loss = 3.89615\n",
      "epoch no.0 train no.67160  loss = 4.10050 avg_loss = 3.89984\n",
      "epoch no.0 train no.67170  loss = 4.08223 avg_loss = 4.01188\n",
      "epoch no.0 train no.67180  loss = 3.55357 avg_loss = 4.01788\n",
      "epoch no.0 train no.67190  loss = 4.68069 avg_loss = 4.01725\n",
      "epoch no.0 train no.67200  loss = 6.28200 avg_loss = 3.98687\n",
      "epoch no.0 train no.67210  loss = 4.04783 avg_loss = 4.06787\n",
      "epoch no.0 train no.67220  loss = 2.57796 avg_loss = 4.11072\n",
      "epoch no.0 train no.67230  loss = 4.75088 avg_loss = 4.13554\n",
      "epoch no.0 train no.67240  loss = 5.43017 avg_loss = 4.14279\n",
      "epoch no.0 train no.67250  loss = 5.88301 avg_loss = 4.11689\n",
      "epoch no.0 train no.67260  loss = 3.44418 avg_loss = 4.06464\n",
      "epoch no.0 train no.67270  loss = 2.18587 avg_loss = 4.04669\n",
      "epoch no.0 train no.67280  loss = 6.88872 avg_loss = 4.03031\n",
      "epoch no.0 train no.67290  loss = 3.10356 avg_loss = 4.02566\n",
      "epoch no.0 train no.67300  loss = 2.21681 avg_loss = 4.05786\n",
      "epoch no.0 train no.67310  loss = 3.53077 avg_loss = 4.00938\n",
      "epoch no.0 train no.67320  loss = 1.26851 avg_loss = 4.03016\n",
      "epoch no.0 train no.67330  loss = 2.95235 avg_loss = 4.10405\n",
      "epoch no.0 train no.67340  loss = 3.20651 avg_loss = 4.09806\n",
      "epoch no.0 train no.67350  loss = 3.97925 avg_loss = 4.07460\n",
      "epoch no.0 train no.67360  loss = 3.03759 avg_loss = 4.13162\n",
      "epoch no.0 train no.67370  loss = 4.76496 avg_loss = 4.12243\n",
      "epoch no.0 train no.67380  loss = 5.25368 avg_loss = 4.09636\n",
      "epoch no.0 train no.67390  loss = 3.88234 avg_loss = 4.10408\n",
      "epoch no.0 train no.67400  loss = 3.69859 avg_loss = 4.12825\n",
      "epoch no.0 train no.67410  loss = 3.60534 avg_loss = 4.13564\n",
      "epoch no.0 train no.67420  loss = 3.79716 avg_loss = 4.18886\n",
      "epoch no.0 train no.67430  loss = 3.51545 avg_loss = 4.17389\n",
      "epoch no.0 train no.67440  loss = 4.79974 avg_loss = 4.21980\n",
      "epoch no.0 train no.67450  loss = 7.18450 avg_loss = 4.17337\n",
      "epoch no.0 train no.67460  loss = 3.19525 avg_loss = 4.12361\n",
      "epoch no.0 train no.67470  loss = 4.14948 avg_loss = 4.13896\n",
      "epoch no.0 train no.67480  loss = 3.61905 avg_loss = 4.08502\n",
      "epoch no.0 train no.67490  loss = 4.53578 avg_loss = 4.03773\n",
      "epoch no.0 train no.67500  loss = 2.36180 avg_loss = 4.04392\n",
      "epoch no.0 train no.67510  loss = 3.15175 avg_loss = 4.11367\n",
      "epoch no.0 train no.67520  loss = 3.01488 avg_loss = 4.10989\n",
      "epoch no.0 train no.67530  loss = 4.53431 avg_loss = 4.07353\n",
      "epoch no.0 train no.67540  loss = 4.21076 avg_loss = 4.02361\n",
      "epoch no.0 train no.67550  loss = 6.83768 avg_loss = 4.01772\n",
      "epoch no.0 train no.67560  loss = 4.15643 avg_loss = 4.05287\n",
      "epoch no.0 train no.67570  loss = 3.51236 avg_loss = 4.02128\n",
      "epoch no.0 train no.67580  loss = 5.33012 avg_loss = 4.04055\n",
      "epoch no.0 train no.67590  loss = 4.38470 avg_loss = 4.01561\n",
      "epoch no.0 train no.67600  loss = 1.55639 avg_loss = 3.96544\n",
      "epoch no.0 train no.67610  loss = 4.60543 avg_loss = 3.97815\n",
      "epoch no.0 train no.67620  loss = 5.02964 avg_loss = 4.00816\n",
      "epoch no.0 train no.67630  loss = 3.45507 avg_loss = 4.05639\n",
      "epoch no.0 train no.67640  loss = 3.65810 avg_loss = 4.07234\n",
      "epoch no.0 train no.67650  loss = 3.04265 avg_loss = 4.10379\n",
      "epoch no.0 train no.67660  loss = 3.84533 avg_loss = 4.06865\n",
      "epoch no.0 train no.67670  loss = 4.15806 avg_loss = 4.05192\n",
      "epoch no.0 train no.67680  loss = 6.06203 avg_loss = 4.10721\n",
      "epoch no.0 train no.67690  loss = 5.07699 avg_loss = 4.10574\n",
      "epoch no.0 train no.67700  loss = 3.71367 avg_loss = 4.15264\n",
      "epoch no.0 train no.67710  loss = 2.83344 avg_loss = 4.15217\n",
      "epoch no.0 train no.67720  loss = 5.66765 avg_loss = 4.18039\n",
      "epoch no.0 train no.67730  loss = 5.63807 avg_loss = 4.19317\n",
      "epoch no.0 train no.67740  loss = 3.95784 avg_loss = 4.20129\n",
      "epoch no.0 train no.67750  loss = 5.57988 avg_loss = 4.19408\n",
      "epoch no.0 train no.67760  loss = 4.67403 avg_loss = 4.20649\n",
      "epoch no.0 train no.67770  loss = 5.02218 avg_loss = 4.17667\n",
      "epoch no.0 train no.67780  loss = 4.21678 avg_loss = 4.09662\n",
      "epoch no.0 train no.67790  loss = 2.73021 avg_loss = 4.04831\n",
      "epoch no.0 train no.67800  loss = 1.99375 avg_loss = 4.02072\n",
      "epoch no.0 train no.67810  loss = 2.99602 avg_loss = 4.02389\n",
      "epoch no.0 train no.67820  loss = 2.47357 avg_loss = 4.00252\n",
      "epoch no.0 train no.67830  loss = 2.81389 avg_loss = 4.00818\n",
      "epoch no.0 train no.67840  loss = 2.88850 avg_loss = 3.98395\n",
      "epoch no.0 train no.67850  loss = 4.94074 avg_loss = 4.00267\n",
      "epoch no.0 train no.67860  loss = 2.41613 avg_loss = 3.94760\n",
      "epoch no.0 train no.67870  loss = 3.50357 avg_loss = 3.94974\n",
      "epoch no.0 train no.67880  loss = 5.48371 avg_loss = 4.01343\n",
      "epoch no.0 train no.67890  loss = 4.58036 avg_loss = 4.05391\n",
      "epoch no.0 train no.67900  loss = 3.40764 avg_loss = 4.00822\n",
      "epoch no.0 train no.67910  loss = 4.99865 avg_loss = 4.03006\n",
      "epoch no.0 train no.67920  loss = 2.64656 avg_loss = 4.06628\n",
      "epoch no.0 train no.67930  loss = 2.97524 avg_loss = 4.12725\n",
      "epoch no.0 train no.67940  loss = 3.40923 avg_loss = 4.08295\n",
      "epoch no.0 train no.67950  loss = 6.11191 avg_loss = 4.08339\n",
      "epoch no.0 train no.67960  loss = 3.58536 avg_loss = 4.11474\n",
      "epoch no.0 train no.67970  loss = 3.26012 avg_loss = 4.03846\n",
      "epoch no.0 train no.67980  loss = 2.70831 avg_loss = 4.02269\n",
      "epoch no.0 train no.67990  loss = 4.59343 avg_loss = 4.01011\n",
      "epoch no.0 train no.68000  loss = 4.06074 avg_loss = 3.97302\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁신나는']\n",
      "기분전환용</s>\n",
      "epoch no.0 train no.68010  loss = 3.98321 avg_loss = 3.91284\n",
      "epoch no.0 train no.68020  loss = 4.95302 avg_loss = 3.89060\n",
      "epoch no.0 train no.68030  loss = 3.15396 avg_loss = 3.87897\n",
      "epoch no.0 train no.68040  loss = 3.99246 avg_loss = 3.87829\n",
      "epoch no.0 train no.68050  loss = 4.59830 avg_loss = 3.91678\n",
      "epoch no.0 train no.68060  loss = 2.40873 avg_loss = 3.88235\n",
      "epoch no.0 train no.68070  loss = 3.30542 avg_loss = 3.86796\n",
      "epoch no.0 train no.68080  loss = 3.70586 avg_loss = 3.86178\n",
      "epoch no.0 train no.68090  loss = 1.57656 avg_loss = 3.87009\n",
      "epoch no.0 train no.68100  loss = 2.17090 avg_loss = 3.86505\n",
      "epoch no.0 train no.68110  loss = 3.38707 avg_loss = 3.86405\n",
      "epoch no.0 train no.68120  loss = 2.34069 avg_loss = 3.83523\n",
      "epoch no.0 train no.68130  loss = 3.92024 avg_loss = 3.80329\n",
      "epoch no.0 train no.68140  loss = 5.53158 avg_loss = 3.85550\n",
      "epoch no.0 train no.68150  loss = 2.77231 avg_loss = 3.89717\n",
      "epoch no.0 train no.68160  loss = 4.71600 avg_loss = 3.97027\n",
      "epoch no.0 train no.68170  loss = 3.63556 avg_loss = 3.98550\n",
      "epoch no.0 train no.68180  loss = 3.63667 avg_loss = 3.98196\n",
      "epoch no.0 train no.68190  loss = 3.44813 avg_loss = 4.00992\n",
      "epoch no.0 train no.68200  loss = 2.74425 avg_loss = 3.92757\n",
      "epoch no.0 train no.68210  loss = 4.73986 avg_loss = 4.00487\n",
      "epoch no.0 train no.68220  loss = 5.81637 avg_loss = 4.00302\n",
      "epoch no.0 train no.68230  loss = 3.26217 avg_loss = 4.02341\n",
      "epoch no.0 train no.68240  loss = 3.68477 avg_loss = 3.98663\n",
      "epoch no.0 train no.68250  loss = 3.88820 avg_loss = 3.95855\n",
      "epoch no.0 train no.68260  loss = 2.54611 avg_loss = 3.92471\n",
      "epoch no.0 train no.68270  loss = 2.65691 avg_loss = 3.90727\n",
      "epoch no.0 train no.68280  loss = 5.14439 avg_loss = 3.92193\n",
      "epoch no.0 train no.68290  loss = 3.72965 avg_loss = 4.02017\n",
      "epoch no.0 train no.68300  loss = 2.83258 avg_loss = 4.07477\n",
      "epoch no.0 train no.68310  loss = 5.27092 avg_loss = 4.09011\n",
      "epoch no.0 train no.68320  loss = 4.21540 avg_loss = 4.09594\n",
      "epoch no.0 train no.68330  loss = 4.01384 avg_loss = 4.03618\n",
      "epoch no.0 train no.68340  loss = 5.51933 avg_loss = 4.07278\n",
      "epoch no.0 train no.68350  loss = 7.16590 avg_loss = 4.08131\n",
      "epoch no.0 train no.68360  loss = 3.53370 avg_loss = 4.09655\n",
      "epoch no.0 train no.68370  loss = 6.77252 avg_loss = 4.11549\n",
      "epoch no.0 train no.68380  loss = 4.28013 avg_loss = 4.09259\n",
      "epoch no.0 train no.68390  loss = 3.92013 avg_loss = 4.10566\n",
      "epoch no.0 train no.68400  loss = 3.08662 avg_loss = 4.14125\n",
      "epoch no.0 train no.68410  loss = 4.89184 avg_loss = 4.16545\n",
      "epoch no.0 train no.68420  loss = 4.78550 avg_loss = 4.10339\n",
      "epoch no.0 train no.68430  loss = 2.64574 avg_loss = 4.11596\n",
      "epoch no.0 train no.68440  loss = 3.26241 avg_loss = 4.13660\n",
      "epoch no.0 train no.68450  loss = 3.11683 avg_loss = 4.16397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.68460  loss = 2.79181 avg_loss = 4.14401\n",
      "epoch no.0 train no.68470  loss = 2.95097 avg_loss = 4.11134\n",
      "epoch no.0 train no.68480  loss = 2.97863 avg_loss = 4.16578\n",
      "epoch no.0 train no.68490  loss = 2.52194 avg_loss = 4.23180\n",
      "epoch no.0 train no.68500  loss = 4.66510 avg_loss = 4.17174\n",
      "epoch no.0 train no.68510  loss = 5.35092 avg_loss = 4.23582\n",
      "epoch no.0 train no.68520  loss = 3.28058 avg_loss = 4.22112\n",
      "epoch no.0 train no.68530  loss = 7.07499 avg_loss = 4.22598\n",
      "epoch no.0 train no.68540  loss = 4.13976 avg_loss = 4.19438\n",
      "epoch no.0 train no.68550  loss = 4.12313 avg_loss = 4.22174\n",
      "epoch no.0 train no.68560  loss = 2.25354 avg_loss = 4.21123\n",
      "epoch no.0 train no.68570  loss = 3.09101 avg_loss = 4.24065\n",
      "epoch no.0 train no.68580  loss = 3.58506 avg_loss = 4.24483\n",
      "epoch no.0 train no.68590  loss = 2.39411 avg_loss = 4.20794\n",
      "epoch no.0 train no.68600  loss = 4.28101 avg_loss = 4.20538\n",
      "epoch no.0 train no.68610  loss = 5.78199 avg_loss = 4.15366\n",
      "epoch no.0 train no.68620  loss = 3.49434 avg_loss = 4.13849\n",
      "epoch no.0 train no.68630  loss = 2.97180 avg_loss = 4.09740\n",
      "epoch no.0 train no.68640  loss = 6.27001 avg_loss = 4.07702\n",
      "epoch no.0 train no.68650  loss = 3.32121 avg_loss = 4.05029\n",
      "epoch no.0 train no.68660  loss = 3.93127 avg_loss = 4.07083\n",
      "epoch no.0 train no.68670  loss = 2.61507 avg_loss = 4.05302\n",
      "epoch no.0 train no.68680  loss = 2.50656 avg_loss = 3.98443\n",
      "epoch no.0 train no.68690  loss = 5.03694 avg_loss = 3.99129\n",
      "epoch no.0 train no.68700  loss = 7.04952 avg_loss = 4.02484\n",
      "epoch no.0 train no.68710  loss = 7.02973 avg_loss = 4.13411\n",
      "epoch no.0 train no.68720  loss = 4.16423 avg_loss = 4.12643\n",
      "epoch no.0 train no.68730  loss = 4.96819 avg_loss = 4.09088\n",
      "epoch no.0 train no.68740  loss = 3.22269 avg_loss = 4.10177\n",
      "epoch no.0 train no.68750  loss = 4.66548 avg_loss = 4.04035\n",
      "epoch no.0 train no.68760  loss = 3.49616 avg_loss = 3.98691\n",
      "epoch no.0 train no.68770  loss = 6.24940 avg_loss = 3.98182\n",
      "epoch no.0 train no.68780  loss = 3.99863 avg_loss = 3.98751\n",
      "epoch no.0 train no.68790  loss = 3.11044 avg_loss = 3.94808\n",
      "epoch no.0 train no.68800  loss = 3.80776 avg_loss = 3.95562\n",
      "epoch no.0 train no.68810  loss = 5.21396 avg_loss = 3.95747\n",
      "epoch no.0 train no.68820  loss = 4.56138 avg_loss = 3.90957\n",
      "epoch no.0 train no.68830  loss = 3.36409 avg_loss = 3.91936\n",
      "epoch no.0 train no.68840  loss = 4.28700 avg_loss = 3.94702\n",
      "epoch no.0 train no.68850  loss = 3.04888 avg_loss = 3.92730\n",
      "epoch no.0 train no.68860  loss = 1.99754 avg_loss = 3.88573\n",
      "epoch no.0 train no.68870  loss = 3.59988 avg_loss = 3.96498\n",
      "epoch no.0 train no.68880  loss = 4.38158 avg_loss = 3.97825\n",
      "epoch no.0 train no.68890  loss = 5.49841 avg_loss = 3.95297\n",
      "epoch no.0 train no.68900  loss = 2.46961 avg_loss = 3.93715\n",
      "epoch no.0 train no.68910  loss = 2.67326 avg_loss = 4.01633\n",
      "epoch no.0 train no.68920  loss = 4.74541 avg_loss = 3.96505\n",
      "epoch no.0 train no.68930  loss = 3.38247 avg_loss = 4.00862\n",
      "epoch no.0 train no.68940  loss = 2.12044 avg_loss = 3.93269\n",
      "epoch no.0 train no.68950  loss = 2.78342 avg_loss = 3.90327\n",
      "epoch no.0 train no.68960  loss = 4.40614 avg_loss = 3.93338\n",
      "epoch no.0 train no.68970  loss = 3.83866 avg_loss = 3.93241\n",
      "epoch no.0 train no.68980  loss = 4.26378 avg_loss = 3.94548\n",
      "epoch no.0 train no.68990  loss = 7.14279 avg_loss = 3.99334\n",
      "epoch no.0 train no.69000  loss = 4.93921 avg_loss = 4.01782\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.0 train no.69010  loss = 5.63721 avg_loss = 4.05452\n",
      "epoch no.0 train no.69020  loss = 4.24444 avg_loss = 4.05737\n",
      "epoch no.0 train no.69030  loss = 5.39081 avg_loss = 4.10877\n",
      "epoch no.0 train no.69040  loss = 6.88842 avg_loss = 4.10369\n",
      "epoch no.0 train no.69050  loss = 6.47034 avg_loss = 4.09261\n",
      "epoch no.0 train no.69060  loss = 1.85859 avg_loss = 4.01041\n",
      "epoch no.0 train no.69070  loss = 4.53782 avg_loss = 4.06403\n",
      "epoch no.0 train no.69080  loss = 3.31626 avg_loss = 4.04573\n",
      "epoch no.0 train no.69090  loss = 2.58237 avg_loss = 3.99842\n",
      "epoch no.0 train no.69100  loss = 4.76323 avg_loss = 4.01648\n",
      "epoch no.0 train no.69110  loss = 4.29811 avg_loss = 3.99838\n",
      "epoch no.0 train no.69120  loss = 1.88221 avg_loss = 3.94293\n",
      "epoch no.0 train no.69130  loss = 3.64467 avg_loss = 3.90654\n",
      "epoch no.0 train no.69140  loss = 4.68053 avg_loss = 3.90266\n",
      "epoch no.0 train no.69150  loss = 5.16821 avg_loss = 3.94436\n",
      "epoch no.0 train no.69160  loss = 4.36890 avg_loss = 4.00813\n",
      "epoch no.0 train no.69170  loss = 3.31231 avg_loss = 4.02110\n",
      "epoch no.0 train no.69180  loss = 3.58194 avg_loss = 3.94437\n",
      "epoch no.0 train no.69190  loss = 2.69911 avg_loss = 3.96359\n",
      "epoch no.0 train no.69200  loss = 4.17838 avg_loss = 3.98443\n",
      "epoch no.0 train no.69210  loss = 4.05843 avg_loss = 3.98045\n",
      "epoch no.0 train no.69220  loss = 4.62853 avg_loss = 4.04706\n",
      "epoch no.0 train no.69230  loss = 2.59726 avg_loss = 4.01143\n",
      "epoch no.0 train no.69240  loss = 5.73743 avg_loss = 4.02035\n",
      "epoch no.0 train no.69250  loss = 3.41614 avg_loss = 4.04423\n",
      "epoch no.0 train no.69260  loss = 3.10267 avg_loss = 4.04611\n",
      "epoch no.0 train no.69270  loss = 3.48808 avg_loss = 4.03645\n",
      "epoch no.0 train no.69280  loss = 4.47194 avg_loss = 4.05186\n",
      "epoch no.0 train no.69290  loss = 4.24376 avg_loss = 4.02260\n",
      "epoch no.0 train no.69300  loss = 5.48357 avg_loss = 3.94134\n",
      "epoch no.0 train no.69310  loss = 5.14861 avg_loss = 4.03698\n",
      "epoch no.0 train no.69320  loss = 2.99249 avg_loss = 4.02701\n",
      "epoch no.0 train no.69330  loss = 3.97794 avg_loss = 4.07541\n",
      "epoch no.0 train no.69340  loss = 6.04147 avg_loss = 4.10848\n",
      "epoch no.0 train no.69350  loss = 2.06702 avg_loss = 4.08904\n",
      "epoch no.0 train no.69360  loss = 4.83662 avg_loss = 4.04433\n",
      "epoch no.0 train no.69370  loss = 3.28838 avg_loss = 4.02383\n",
      "epoch no.0 train no.69380  loss = 3.94269 avg_loss = 4.06697\n",
      "epoch no.0 train no.69390  loss = 3.24719 avg_loss = 4.08999\n",
      "epoch no.0 train no.69400  loss = 2.77331 avg_loss = 4.12626\n",
      "epoch no.0 train no.69410  loss = 6.51152 avg_loss = 4.09973\n",
      "epoch no.0 train no.69420  loss = 6.31662 avg_loss = 4.13314\n",
      "epoch no.0 train no.69430  loss = 2.98585 avg_loss = 4.13986\n",
      "epoch no.0 train no.69440  loss = 5.08415 avg_loss = 4.21271\n",
      "epoch no.0 train no.69450  loss = 3.32211 avg_loss = 4.12695\n",
      "epoch no.0 train no.69460  loss = 3.44339 avg_loss = 4.06785\n",
      "epoch no.0 train no.69470  loss = 2.82425 avg_loss = 4.04880\n",
      "epoch no.0 train no.69480  loss = 4.69147 avg_loss = 4.03295\n",
      "epoch no.0 train no.69490  loss = 4.50797 avg_loss = 3.98889\n",
      "epoch no.0 train no.69500  loss = 3.04183 avg_loss = 4.02230\n",
      "epoch no.0 train no.69510  loss = 5.15559 avg_loss = 4.03316\n",
      "epoch no.0 train no.69520  loss = 5.86664 avg_loss = 4.06560\n",
      "epoch no.0 train no.69530  loss = 3.45545 avg_loss = 4.10254\n",
      "epoch no.0 train no.69540  loss = 4.76481 avg_loss = 4.12579\n",
      "epoch no.0 train no.69550  loss = 5.83317 avg_loss = 4.16616\n",
      "epoch no.0 train no.69560  loss = 5.21729 avg_loss = 4.13613\n",
      "epoch no.0 train no.69570  loss = 4.23098 avg_loss = 4.09682\n",
      "epoch no.0 train no.69580  loss = 3.01724 avg_loss = 4.08793\n",
      "epoch no.0 train no.69590  loss = 2.78799 avg_loss = 4.13634\n",
      "epoch no.0 train no.69600  loss = 3.74214 avg_loss = 4.10148\n",
      "epoch no.0 train no.69610  loss = 7.37673 avg_loss = 4.08137\n",
      "epoch no.0 train no.69620  loss = 3.33818 avg_loss = 4.03274\n",
      "epoch no.0 train no.69630  loss = 3.99575 avg_loss = 4.06071\n",
      "epoch no.0 train no.69640  loss = 3.73716 avg_loss = 4.10097\n",
      "epoch no.0 train no.69650  loss = 4.57920 avg_loss = 4.10886\n",
      "epoch no.0 train no.69660  loss = 3.90485 avg_loss = 4.05397\n",
      "epoch no.0 train no.69670  loss = 4.43801 avg_loss = 4.00243\n",
      "epoch no.0 train no.69680  loss = 5.14454 avg_loss = 4.02665\n",
      "epoch no.0 train no.69690  loss = 5.12608 avg_loss = 4.05051\n",
      "epoch no.0 train no.69700  loss = 3.20597 avg_loss = 3.99801\n",
      "epoch no.0 train no.69710  loss = 2.82686 avg_loss = 3.94750\n",
      "epoch no.0 train no.69720  loss = 4.40184 avg_loss = 3.92750\n",
      "epoch no.0 train no.69730  loss = 4.35678 avg_loss = 3.94157\n",
      "epoch no.0 train no.69740  loss = 1.80570 avg_loss = 3.95221\n",
      "epoch no.0 train no.69750  loss = 4.12372 avg_loss = 4.01942\n",
      "epoch no.0 train no.69760  loss = 5.55618 avg_loss = 4.03948\n",
      "epoch no.0 train no.69770  loss = 3.16879 avg_loss = 3.98125\n",
      "epoch no.0 train no.69780  loss = 6.00928 avg_loss = 3.98547\n",
      "epoch no.0 train no.69790  loss = 4.85346 avg_loss = 3.96079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.69800  loss = 2.43394 avg_loss = 3.93983\n",
      "epoch no.0 train no.69810  loss = 2.20574 avg_loss = 3.98147\n",
      "epoch no.0 train no.69820  loss = 3.03022 avg_loss = 3.95297\n",
      "epoch no.0 train no.69830  loss = 5.65897 avg_loss = 3.92817\n",
      "epoch no.0 train no.69840  loss = 5.02698 avg_loss = 3.94970\n",
      "epoch no.0 train no.69850  loss = 4.02208 avg_loss = 3.98961\n",
      "epoch no.0 train no.69860  loss = 4.90584 avg_loss = 3.99472\n",
      "epoch no.0 train no.69870  loss = 3.64362 avg_loss = 3.99446\n",
      "epoch no.0 train no.69880  loss = 5.12923 avg_loss = 4.01008\n",
      "epoch no.0 train no.69890  loss = 4.89650 avg_loss = 3.98197\n",
      "epoch no.0 train no.69900  loss = 3.28858 avg_loss = 3.95309\n",
      "epoch no.0 train no.69910  loss = 2.54928 avg_loss = 3.89947\n",
      "epoch no.0 train no.69920  loss = 5.97075 avg_loss = 3.90418\n",
      "epoch no.0 train no.69930  loss = 5.22619 avg_loss = 3.92391\n",
      "epoch no.0 train no.69940  loss = 4.55780 avg_loss = 3.94854\n",
      "epoch no.0 train no.69950  loss = 3.10824 avg_loss = 3.89967\n",
      "epoch no.0 train no.69960  loss = 3.86879 avg_loss = 3.94452\n",
      "epoch no.0 train no.69970  loss = 2.80952 avg_loss = 4.00713\n",
      "epoch no.0 train no.69980  loss = 4.98255 avg_loss = 4.00757\n",
      "epoch no.0 train no.69990  loss = 4.82099 avg_loss = 4.04158\n",
      "epoch no.0 train no.70000  loss = 4.35972 avg_loss = 4.00995\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.0 train no.70010  loss = 3.57697 avg_loss = 4.00038\n",
      "epoch no.0 train no.70020  loss = 2.60622 avg_loss = 3.95226\n",
      "epoch no.0 train no.70030  loss = 3.06837 avg_loss = 3.96982\n",
      "epoch no.0 train no.70040  loss = 3.18113 avg_loss = 3.98220\n",
      "epoch no.0 train no.70050  loss = 3.93727 avg_loss = 3.96453\n",
      "epoch no.0 train no.70060  loss = 3.92425 avg_loss = 3.93187\n",
      "epoch no.0 train no.70070  loss = 3.93409 avg_loss = 3.95158\n",
      "epoch no.0 train no.70080  loss = 4.08771 avg_loss = 3.94804\n",
      "epoch no.0 train no.70090  loss = 3.08619 avg_loss = 3.97068\n",
      "epoch no.0 train no.70100  loss = 5.40348 avg_loss = 3.95960\n",
      "epoch no.0 train no.70110  loss = 2.72108 avg_loss = 3.86771\n",
      "epoch no.0 train no.70120  loss = 2.94518 avg_loss = 3.84144\n",
      "epoch no.0 train no.70130  loss = 2.96010 avg_loss = 3.84585\n",
      "epoch no.0 train no.70140  loss = 2.18006 avg_loss = 3.80453\n",
      "epoch no.0 train no.70150  loss = 5.60667 avg_loss = 3.86247\n",
      "epoch no.0 train no.70160  loss = 4.26693 avg_loss = 3.88596\n",
      "epoch no.0 train no.70170  loss = 4.49001 avg_loss = 3.89217\n",
      "epoch no.0 train no.70180  loss = 3.69383 avg_loss = 3.91106\n",
      "epoch no.0 train no.70190  loss = 3.32965 avg_loss = 3.97712\n",
      "epoch no.0 train no.70200  loss = 3.82750 avg_loss = 3.98695\n",
      "epoch no.0 train no.70210  loss = 4.12734 avg_loss = 4.01270\n",
      "epoch no.0 train no.70220  loss = 3.30905 avg_loss = 4.04662\n",
      "epoch no.0 train no.70230  loss = 4.09738 avg_loss = 4.07709\n",
      "epoch no.0 train no.70240  loss = 5.02969 avg_loss = 4.04617\n",
      "epoch no.0 train no.70250  loss = 3.11342 avg_loss = 4.00639\n",
      "epoch no.0 train no.70260  loss = 3.40972 avg_loss = 4.01119\n",
      "epoch no.0 train no.70270  loss = 4.85070 avg_loss = 3.98439\n",
      "epoch no.0 train no.70280  loss = 3.89717 avg_loss = 3.98900\n",
      "epoch no.0 train no.70290  loss = 3.85398 avg_loss = 3.93681\n",
      "epoch no.0 train no.70300  loss = 4.62063 avg_loss = 3.95485\n",
      "epoch no.0 train no.70310  loss = 3.52216 avg_loss = 3.96230\n",
      "epoch no.0 train no.70320  loss = 3.45975 avg_loss = 3.90060\n",
      "epoch no.0 train no.70330  loss = 5.97547 avg_loss = 3.87656\n",
      "epoch no.0 train no.70340  loss = 2.59760 avg_loss = 3.87198\n",
      "epoch no.0 train no.70350  loss = 5.22672 avg_loss = 3.88097\n",
      "epoch no.0 train no.70360  loss = 2.51745 avg_loss = 3.84424\n",
      "epoch no.0 train no.70370  loss = 2.99353 avg_loss = 3.83081\n",
      "epoch no.0 train no.70380  loss = 3.00235 avg_loss = 3.83174\n",
      "epoch no.0 train no.70390  loss = 4.72317 avg_loss = 3.89431\n",
      "epoch no.0 train no.70400  loss = 2.32338 avg_loss = 3.89137\n",
      "epoch no.0 train no.70410  loss = 4.46765 avg_loss = 3.83428\n",
      "epoch no.0 train no.70420  loss = 6.03346 avg_loss = 3.85373\n",
      "epoch no.0 train no.70430  loss = 5.23481 avg_loss = 3.93780\n",
      "epoch no.0 train no.70440  loss = 5.68171 avg_loss = 3.87883\n",
      "epoch no.0 train no.70450  loss = 2.35492 avg_loss = 3.79688\n",
      "epoch no.0 train no.70460  loss = 3.74201 avg_loss = 3.79982\n",
      "epoch no.0 train no.70470  loss = 4.74766 avg_loss = 3.84176\n",
      "epoch no.0 train no.70480  loss = 4.38916 avg_loss = 3.86032\n",
      "epoch no.0 train no.70490  loss = 2.28498 avg_loss = 3.84297\n",
      "epoch no.0 train no.70500  loss = 3.53995 avg_loss = 3.83634\n",
      "epoch no.0 train no.70510  loss = 5.74111 avg_loss = 3.88606\n",
      "epoch no.0 train no.70520  loss = 2.06096 avg_loss = 3.87051\n",
      "epoch no.0 train no.70530  loss = 5.25986 avg_loss = 3.87825\n",
      "epoch no.0 train no.70540  loss = 3.52848 avg_loss = 3.92159\n",
      "epoch no.0 train no.70550  loss = 6.67386 avg_loss = 3.93725\n",
      "epoch no.0 train no.70560  loss = 5.35654 avg_loss = 3.99602\n",
      "epoch no.0 train no.70570  loss = 5.23171 avg_loss = 4.00960\n",
      "epoch no.0 train no.70580  loss = 2.55930 avg_loss = 3.98661\n",
      "epoch no.0 train no.70590  loss = 2.98769 avg_loss = 4.01995\n",
      "epoch no.0 train no.70600  loss = 3.93146 avg_loss = 4.03025\n",
      "epoch no.0 train no.70610  loss = 3.74716 avg_loss = 4.06196\n",
      "epoch no.0 train no.70620  loss = 4.63355 avg_loss = 4.08331\n",
      "epoch no.0 train no.70630  loss = 3.55449 avg_loss = 4.08058\n",
      "epoch no.0 train no.70640  loss = 2.84994 avg_loss = 4.03301\n",
      "epoch no.0 train no.70650  loss = 3.27479 avg_loss = 3.97863\n",
      "epoch no.0 train no.70660  loss = 4.15769 avg_loss = 3.98174\n",
      "epoch no.0 train no.70670  loss = 2.82380 avg_loss = 3.95649\n",
      "epoch no.0 train no.70680  loss = 3.82811 avg_loss = 3.87151\n",
      "epoch no.0 train no.70690  loss = 3.33237 avg_loss = 3.88217\n",
      "epoch no.0 train no.70700  loss = 2.75899 avg_loss = 3.90479\n",
      "epoch no.0 train no.70710  loss = 2.79342 avg_loss = 3.89347\n",
      "epoch no.0 train no.70720  loss = 3.45721 avg_loss = 3.91501\n",
      "epoch no.0 train no.70730  loss = 3.90833 avg_loss = 3.91904\n",
      "epoch no.0 train no.70740  loss = 4.13057 avg_loss = 4.05267\n",
      "epoch no.0 train no.70750  loss = 3.53674 avg_loss = 4.06041\n",
      "epoch no.0 train no.70760  loss = 4.32473 avg_loss = 4.01113\n",
      "epoch no.0 train no.70770  loss = 4.77816 avg_loss = 4.00435\n",
      "epoch no.0 train no.70780  loss = 3.87557 avg_loss = 4.04344\n",
      "epoch no.0 train no.70790  loss = 2.54014 avg_loss = 3.97756\n",
      "epoch no.0 train no.70800  loss = 2.47789 avg_loss = 3.95525\n",
      "epoch no.0 train no.70810  loss = 5.12369 avg_loss = 3.96329\n",
      "epoch no.0 train no.70820  loss = 4.01985 avg_loss = 4.01042\n",
      "epoch no.0 train no.70830  loss = 4.22508 avg_loss = 3.99191\n",
      "epoch no.0 train no.70840  loss = 4.70142 avg_loss = 3.99448\n",
      "epoch no.0 train no.70850  loss = 5.01641 avg_loss = 4.01025\n",
      "epoch no.0 train no.70860  loss = 4.33409 avg_loss = 4.02433\n",
      "epoch no.0 train no.70870  loss = 2.47536 avg_loss = 3.98916\n",
      "epoch no.0 train no.70880  loss = 3.45517 avg_loss = 3.93447\n",
      "epoch no.0 train no.70890  loss = 4.22051 avg_loss = 3.93596\n",
      "epoch no.0 train no.70900  loss = 3.11559 avg_loss = 4.00023\n",
      "epoch no.0 train no.70910  loss = 5.37321 avg_loss = 3.98062\n",
      "epoch no.0 train no.70920  loss = 3.55292 avg_loss = 4.04954\n",
      "epoch no.0 train no.70930  loss = 3.60511 avg_loss = 3.99369\n",
      "epoch no.0 train no.70940  loss = 4.23224 avg_loss = 3.99285\n",
      "epoch no.0 train no.70950  loss = 2.50724 avg_loss = 4.05061\n",
      "epoch no.0 train no.70960  loss = 3.10259 avg_loss = 4.00053\n",
      "epoch no.0 train no.70970  loss = 3.47319 avg_loss = 3.96162\n",
      "epoch no.0 train no.70980  loss = 2.25410 avg_loss = 3.95502\n",
      "epoch no.0 train no.70990  loss = 4.47152 avg_loss = 3.90310\n",
      "epoch no.0 train no.71000  loss = 3.72141 avg_loss = 3.90986\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.0 train no.71010  loss = 4.73831 avg_loss = 3.89814\n",
      "epoch no.0 train no.71020  loss = 3.83139 avg_loss = 3.91539\n",
      "epoch no.0 train no.71030  loss = 3.26505 avg_loss = 3.98424\n",
      "epoch no.0 train no.71040  loss = 4.95852 avg_loss = 3.97907\n",
      "epoch no.0 train no.71050  loss = 3.22309 avg_loss = 4.00615\n",
      "epoch no.0 train no.71060  loss = 2.79059 avg_loss = 3.95732\n",
      "epoch no.0 train no.71070  loss = 3.77769 avg_loss = 3.96910\n",
      "epoch no.0 train no.71080  loss = 5.55757 avg_loss = 3.95843\n",
      "epoch no.0 train no.71090  loss = 5.86465 avg_loss = 3.99447\n",
      "epoch no.0 train no.71100  loss = 7.03817 avg_loss = 4.02669\n",
      "epoch no.0 train no.71110  loss = 2.52606 avg_loss = 4.01219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.71120  loss = 6.45848 avg_loss = 4.02482\n",
      "epoch no.0 train no.71130  loss = 2.43907 avg_loss = 4.00853\n",
      "epoch no.0 train no.71140  loss = 3.60010 avg_loss = 4.01801\n",
      "epoch no.0 train no.71150  loss = 2.40371 avg_loss = 4.05182\n",
      "epoch no.0 train no.71160  loss = 2.87697 avg_loss = 4.04696\n",
      "epoch no.0 train no.71170  loss = 5.10937 avg_loss = 4.02965\n",
      "epoch no.0 train no.71180  loss = 2.44261 avg_loss = 4.12914\n",
      "epoch no.0 train no.71190  loss = 3.07307 avg_loss = 4.03566\n",
      "epoch no.0 train no.71200  loss = 3.46802 avg_loss = 3.99373\n",
      "epoch no.0 train no.71210  loss = 3.77240 avg_loss = 4.04157\n",
      "epoch no.0 train no.71220  loss = 2.65696 avg_loss = 3.98223\n",
      "epoch no.0 train no.71230  loss = 6.12491 avg_loss = 3.99416\n",
      "epoch no.0 train no.71240  loss = 4.43353 avg_loss = 3.99297\n",
      "epoch no.0 train no.71250  loss = 2.78259 avg_loss = 3.95564\n",
      "epoch no.0 train no.71260  loss = 3.38113 avg_loss = 3.93825\n",
      "epoch no.0 train no.71270  loss = 3.35653 avg_loss = 3.95935\n",
      "epoch no.0 train no.71280  loss = 3.15278 avg_loss = 3.98565\n",
      "epoch no.0 train no.71290  loss = 4.60954 avg_loss = 4.05591\n",
      "epoch no.0 train no.71300  loss = 5.77743 avg_loss = 4.11504\n",
      "epoch no.0 train no.71310  loss = 5.50502 avg_loss = 4.13906\n",
      "epoch no.0 train no.71320  loss = 2.86452 avg_loss = 4.19671\n",
      "epoch no.0 train no.71330  loss = 4.55000 avg_loss = 4.20132\n",
      "epoch no.0 train no.71340  loss = 5.69206 avg_loss = 4.18714\n",
      "epoch no.0 train no.71350  loss = 3.30930 avg_loss = 4.14915\n",
      "epoch no.0 train no.71360  loss = 2.02408 avg_loss = 4.09701\n",
      "epoch no.0 train no.71370  loss = 3.87046 avg_loss = 4.07030\n",
      "epoch no.0 train no.71380  loss = 4.48296 avg_loss = 4.11232\n",
      "epoch no.0 train no.71390  loss = 3.57138 avg_loss = 4.09708\n",
      "epoch no.0 train no.71400  loss = 5.03060 avg_loss = 4.10985\n",
      "epoch no.0 train no.71410  loss = 5.80197 avg_loss = 4.11983\n",
      "epoch no.0 train no.71420  loss = 3.24212 avg_loss = 4.12078\n",
      "epoch no.0 train no.71430  loss = 3.38441 avg_loss = 4.13832\n",
      "epoch no.0 train no.71440  loss = 4.64934 avg_loss = 4.16810\n",
      "epoch no.0 train no.71450  loss = 4.51510 avg_loss = 4.15940\n",
      "epoch no.0 train no.71460  loss = 2.40440 avg_loss = 4.08253\n",
      "epoch no.0 train no.71470  loss = 3.20664 avg_loss = 4.06553\n",
      "epoch no.0 train no.71480  loss = 5.84355 avg_loss = 4.01367\n",
      "epoch no.0 train no.71490  loss = 3.50702 avg_loss = 3.94537\n",
      "epoch no.0 train no.71500  loss = 3.20368 avg_loss = 3.93578\n",
      "epoch no.0 train no.71510  loss = 2.90969 avg_loss = 3.85596\n",
      "epoch no.0 train no.71520  loss = 4.96291 avg_loss = 3.98352\n",
      "epoch no.0 train no.71530  loss = 2.55089 avg_loss = 3.96962\n",
      "epoch no.0 train no.71540  loss = 3.15054 avg_loss = 3.98811\n",
      "epoch no.0 train no.71550  loss = 3.85452 avg_loss = 3.99098\n",
      "epoch no.0 train no.71560  loss = 4.52610 avg_loss = 4.03198\n",
      "epoch no.0 train no.71570  loss = 6.50041 avg_loss = 4.04807\n",
      "epoch no.0 train no.71580  loss = 2.82093 avg_loss = 4.03366\n",
      "epoch no.0 train no.71590  loss = 5.76898 avg_loss = 4.07174\n",
      "epoch no.0 train no.71600  loss = 4.65108 avg_loss = 4.08631\n",
      "epoch no.0 train no.71610  loss = 4.76980 avg_loss = 4.09202\n",
      "epoch no.0 train no.71620  loss = 3.33341 avg_loss = 4.04102\n",
      "epoch no.0 train no.71630  loss = 3.00844 avg_loss = 4.01189\n",
      "epoch no.0 train no.71640  loss = 1.65994 avg_loss = 3.97280\n",
      "epoch no.0 train no.71650  loss = 4.31615 avg_loss = 3.99049\n",
      "epoch no.0 train no.71660  loss = 3.89814 avg_loss = 3.94159\n",
      "epoch no.0 train no.71670  loss = 3.02735 avg_loss = 3.94620\n",
      "epoch no.0 train no.71680  loss = 3.42123 avg_loss = 4.01241\n",
      "epoch no.0 train no.71690  loss = 3.68857 avg_loss = 3.98283\n",
      "epoch no.0 train no.71700  loss = 3.94025 avg_loss = 3.92264\n",
      "epoch no.0 train no.71710  loss = 5.01872 avg_loss = 3.92521\n",
      "epoch no.0 train no.71720  loss = 3.63675 avg_loss = 3.92165\n",
      "epoch no.0 train no.71730  loss = 3.91265 avg_loss = 3.93656\n",
      "epoch no.0 train no.71740  loss = 4.80117 avg_loss = 3.96953\n",
      "epoch no.0 train no.71750  loss = 2.82191 avg_loss = 3.91557\n",
      "epoch no.0 train no.71760  loss = 2.95355 avg_loss = 3.85905\n",
      "epoch no.0 train no.71770  loss = 3.06208 avg_loss = 3.87764\n",
      "epoch no.0 train no.71780  loss = 3.99046 avg_loss = 3.88229\n",
      "epoch no.0 train no.71790  loss = 2.58080 avg_loss = 3.83637\n",
      "epoch no.0 train no.71800  loss = 4.34847 avg_loss = 3.87240\n",
      "epoch no.0 train no.71810  loss = 6.04776 avg_loss = 3.91813\n",
      "epoch no.0 train no.71820  loss = 3.22301 avg_loss = 3.91243\n",
      "epoch no.0 train no.71830  loss = 4.78112 avg_loss = 3.89728\n",
      "epoch no.0 train no.71840  loss = 3.14145 avg_loss = 3.95249\n",
      "epoch no.0 train no.71850  loss = 3.61679 avg_loss = 3.96525\n",
      "epoch no.0 train no.71860  loss = 2.94697 avg_loss = 3.99094\n",
      "epoch no.0 train no.71870  loss = 5.82112 avg_loss = 3.96995\n",
      "epoch no.0 train no.71880  loss = 3.83531 avg_loss = 3.91725\n",
      "epoch no.0 train no.71890  loss = 3.02484 avg_loss = 3.92051\n",
      "epoch no.0 train no.71900  loss = 4.57391 avg_loss = 3.94465\n",
      "epoch no.0 train no.71910  loss = 6.23483 avg_loss = 3.98307\n",
      "epoch no.0 train no.71920  loss = 4.01071 avg_loss = 4.00763\n",
      "epoch no.0 train no.71930  loss = 3.50288 avg_loss = 3.95271\n",
      "epoch no.0 train no.71940  loss = 4.97862 avg_loss = 3.95035\n",
      "epoch no.0 train no.71950  loss = 4.87493 avg_loss = 3.92745\n",
      "epoch no.0 train no.71960  loss = 2.26977 avg_loss = 3.91836\n",
      "epoch no.0 train no.71970  loss = 4.77655 avg_loss = 3.94356\n",
      "epoch no.0 train no.71980  loss = 4.29658 avg_loss = 3.94682\n",
      "epoch no.0 train no.71990  loss = 3.84713 avg_loss = 4.01178\n",
      "epoch no.0 train no.72000  loss = 4.76476 avg_loss = 3.97606\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.0 train no.72010  loss = 4.61183 avg_loss = 3.94910\n",
      "epoch no.0 train no.72020  loss = 4.01135 avg_loss = 3.91959\n",
      "epoch no.0 train no.72030  loss = 5.24802 avg_loss = 3.95085\n",
      "epoch no.0 train no.72040  loss = 3.66019 avg_loss = 4.03996\n",
      "epoch no.0 train no.72050  loss = 3.61975 avg_loss = 4.04628\n",
      "epoch no.0 train no.72060  loss = 5.17970 avg_loss = 4.09856\n",
      "epoch no.0 train no.72070  loss = 2.25459 avg_loss = 4.03388\n",
      "epoch no.0 train no.72080  loss = 2.73636 avg_loss = 4.02841\n",
      "epoch no.0 train no.72090  loss = 2.29870 avg_loss = 3.99281\n",
      "epoch no.0 train no.72100  loss = 4.80685 avg_loss = 3.99055\n",
      "epoch no.0 train no.72110  loss = 4.04835 avg_loss = 3.94751\n",
      "epoch no.0 train no.72120  loss = 4.60785 avg_loss = 3.98399\n",
      "epoch no.0 train no.72130  loss = 3.91403 avg_loss = 3.98694\n",
      "epoch no.0 train no.72140  loss = 5.46237 avg_loss = 4.02419\n",
      "epoch no.0 train no.72150  loss = 1.78526 avg_loss = 3.98948\n",
      "epoch no.0 train no.72160  loss = 4.73522 avg_loss = 4.03118\n",
      "epoch no.0 train no.72170  loss = 4.61864 avg_loss = 4.03162\n",
      "epoch no.0 train no.72180  loss = 4.26723 avg_loss = 4.00905\n",
      "epoch no.0 train no.72190  loss = 1.88856 avg_loss = 3.94230\n",
      "epoch no.0 train no.72200  loss = 5.17004 avg_loss = 3.93213\n",
      "epoch no.0 train no.72210  loss = 3.41614 avg_loss = 3.92940\n",
      "epoch no.0 train no.72220  loss = 2.73282 avg_loss = 3.90594\n",
      "epoch no.0 train no.72230  loss = 3.98534 avg_loss = 3.82250\n",
      "epoch no.0 train no.72240  loss = 4.39579 avg_loss = 3.85336\n",
      "epoch no.0 train no.72250  loss = 2.70497 avg_loss = 3.82917\n",
      "epoch no.0 train no.72260  loss = 2.41316 avg_loss = 3.89850\n",
      "epoch no.0 train no.72270  loss = 3.51033 avg_loss = 3.91177\n",
      "epoch no.0 train no.72280  loss = 3.49923 avg_loss = 3.95920\n",
      "epoch no.0 train no.72290  loss = 5.06294 avg_loss = 3.99007\n",
      "epoch no.0 train no.72300  loss = 2.95905 avg_loss = 3.99979\n",
      "epoch no.0 train no.72310  loss = 5.01321 avg_loss = 4.03357\n",
      "epoch no.0 train no.72320  loss = 4.54473 avg_loss = 4.00340\n",
      "epoch no.0 train no.72330  loss = 4.52847 avg_loss = 3.92248\n",
      "epoch no.0 train no.72340  loss = 3.20572 avg_loss = 3.94520\n",
      "epoch no.0 train no.72350  loss = 6.65375 avg_loss = 3.97664\n",
      "epoch no.0 train no.72360  loss = 2.43208 avg_loss = 3.91724\n",
      "epoch no.0 train no.72370  loss = 2.56117 avg_loss = 3.98209\n",
      "epoch no.0 train no.72380  loss = 6.54419 avg_loss = 4.00834\n",
      "epoch no.0 train no.72390  loss = 3.66431 avg_loss = 3.99789\n",
      "epoch no.0 train no.72400  loss = 2.75603 avg_loss = 3.99269\n",
      "epoch no.0 train no.72410  loss = 6.34169 avg_loss = 3.99735\n",
      "epoch no.0 train no.72420  loss = 3.27153 avg_loss = 3.99748\n",
      "epoch no.0 train no.72430  loss = 4.83429 avg_loss = 3.99437\n",
      "epoch no.0 train no.72440  loss = 3.73689 avg_loss = 3.98989\n",
      "epoch no.0 train no.72450  loss = 3.04145 avg_loss = 3.99358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.72460  loss = 3.31898 avg_loss = 3.98927\n",
      "epoch no.0 train no.72470  loss = 2.82446 avg_loss = 4.01595\n",
      "epoch no.0 train no.72480  loss = 3.65081 avg_loss = 4.04994\n",
      "epoch no.0 train no.72490  loss = 3.43157 avg_loss = 4.05429\n",
      "epoch no.0 train no.72500  loss = 4.54016 avg_loss = 4.03862\n",
      "epoch no.0 train no.72510  loss = 6.21318 avg_loss = 4.08780\n",
      "epoch no.0 train no.72520  loss = 3.66670 avg_loss = 4.06913\n",
      "epoch no.0 train no.72530  loss = 3.82258 avg_loss = 4.06461\n",
      "epoch no.0 train no.72540  loss = 6.31557 avg_loss = 4.08613\n",
      "epoch no.0 train no.72550  loss = 5.90879 avg_loss = 4.10316\n",
      "epoch no.0 train no.72560  loss = 5.66798 avg_loss = 4.13716\n",
      "epoch no.0 train no.72570  loss = 3.76589 avg_loss = 4.16340\n",
      "epoch no.0 train no.72580  loss = 4.90232 avg_loss = 4.14740\n",
      "epoch no.0 train no.72590  loss = 7.12857 avg_loss = 4.16815\n",
      "epoch no.0 train no.72600  loss = 4.55910 avg_loss = 4.21699\n",
      "epoch no.0 train no.72610  loss = 3.78902 avg_loss = 4.17013\n",
      "epoch no.0 train no.72620  loss = 4.18423 avg_loss = 4.12716\n",
      "epoch no.0 train no.72630  loss = 3.56205 avg_loss = 4.13493\n",
      "epoch no.0 train no.72640  loss = 2.01558 avg_loss = 4.15686\n",
      "epoch no.0 train no.72650  loss = 4.82166 avg_loss = 4.13604\n",
      "epoch no.0 train no.72660  loss = 2.94264 avg_loss = 4.10565\n",
      "epoch no.0 train no.72670  loss = 2.84115 avg_loss = 4.08242\n",
      "epoch no.0 train no.72680  loss = 7.33586 avg_loss = 4.09637\n",
      "epoch no.0 train no.72690  loss = 6.55238 avg_loss = 4.16876\n",
      "epoch no.0 train no.72700  loss = 3.25101 avg_loss = 4.15707\n",
      "epoch no.0 train no.72710  loss = 6.33876 avg_loss = 4.11645\n",
      "epoch no.0 train no.72720  loss = 3.69118 avg_loss = 4.09271\n",
      "epoch no.0 train no.72730  loss = 2.19564 avg_loss = 4.07978\n",
      "epoch no.0 train no.72740  loss = 4.34846 avg_loss = 4.13794\n",
      "epoch no.0 train no.72750  loss = 2.94127 avg_loss = 4.09859\n",
      "epoch no.0 train no.72760  loss = 4.81218 avg_loss = 4.09481\n",
      "epoch no.0 train no.72770  loss = 7.92149 avg_loss = 4.15111\n",
      "epoch no.0 train no.72780  loss = 2.95994 avg_loss = 4.09541\n",
      "epoch no.0 train no.72790  loss = 2.58970 avg_loss = 4.00597\n",
      "epoch no.0 train no.72800  loss = 2.72940 avg_loss = 3.91790\n",
      "epoch no.0 train no.72810  loss = 4.59898 avg_loss = 3.94679\n",
      "epoch no.0 train no.72820  loss = 6.82573 avg_loss = 3.95321\n",
      "epoch no.0 train no.72830  loss = 7.40236 avg_loss = 4.03358\n",
      "epoch no.0 train no.72840  loss = 3.22470 avg_loss = 4.00907\n",
      "epoch no.0 train no.72850  loss = 4.44821 avg_loss = 3.98362\n",
      "epoch no.0 train no.72860  loss = 4.73084 avg_loss = 3.98364\n",
      "epoch no.0 train no.72870  loss = 4.59488 avg_loss = 4.00634\n",
      "epoch no.0 train no.72880  loss = 2.72732 avg_loss = 4.06554\n",
      "epoch no.0 train no.72890  loss = 4.23836 avg_loss = 4.08749\n",
      "epoch no.0 train no.72900  loss = 3.69447 avg_loss = 4.05230\n",
      "epoch no.0 train no.72910  loss = 2.80618 avg_loss = 3.99856\n",
      "epoch no.0 train no.72920  loss = 3.38051 avg_loss = 3.97395\n",
      "epoch no.0 train no.72930  loss = 5.50015 avg_loss = 3.92977\n",
      "epoch no.0 train no.72940  loss = 4.22484 avg_loss = 3.92382\n",
      "epoch no.0 train no.72950  loss = 5.88791 avg_loss = 3.91670\n",
      "epoch no.0 train no.72960  loss = 4.22658 avg_loss = 3.90585\n",
      "epoch no.0 train no.72970  loss = 4.42419 avg_loss = 3.89934\n",
      "epoch no.0 train no.72980  loss = 2.58258 avg_loss = 3.91209\n",
      "epoch no.0 train no.72990  loss = 4.04917 avg_loss = 3.90518\n",
      "epoch no.0 train no.73000  loss = 6.27221 avg_loss = 3.96429\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁노래', '▁노래', '</s>', '</s>']\n",
      "기분전환되는 신나는 락 음악</s>\n",
      "epoch no.0 train no.73010  loss = 4.57165 avg_loss = 3.91875\n",
      "epoch no.0 train no.73020  loss = 2.30189 avg_loss = 3.89054\n",
      "epoch no.0 train no.73030  loss = 5.87951 avg_loss = 3.95687\n",
      "epoch no.0 train no.73040  loss = 5.01299 avg_loss = 4.00713\n",
      "epoch no.0 train no.73050  loss = 4.21736 avg_loss = 3.97045\n",
      "epoch no.0 train no.73060  loss = 6.06424 avg_loss = 3.95706\n",
      "epoch no.0 train no.73070  loss = 2.92742 avg_loss = 3.90624\n",
      "epoch no.0 train no.73080  loss = 2.50520 avg_loss = 3.93545\n",
      "epoch no.0 train no.73090  loss = 2.84239 avg_loss = 3.90982\n",
      "epoch no.0 train no.73100  loss = 3.45438 avg_loss = 3.91130\n",
      "epoch no.0 train no.73110  loss = 2.98422 avg_loss = 3.90363\n",
      "epoch no.0 train no.73120  loss = 3.03883 avg_loss = 3.94017\n",
      "epoch no.0 train no.73130  loss = 2.01961 avg_loss = 4.00950\n",
      "epoch no.0 train no.73140  loss = 3.74635 avg_loss = 3.99863\n",
      "epoch no.0 train no.73150  loss = 2.63630 avg_loss = 4.01408\n",
      "epoch no.0 train no.73160  loss = 3.03508 avg_loss = 4.02977\n",
      "epoch no.0 train no.73170  loss = 4.79132 avg_loss = 4.11345\n",
      "epoch no.0 train no.73180  loss = 3.21724 avg_loss = 4.11013\n",
      "epoch no.0 train no.73190  loss = 2.81185 avg_loss = 4.15704\n",
      "epoch no.0 train no.73200  loss = 5.86601 avg_loss = 4.13980\n",
      "epoch no.0 train no.73210  loss = 5.26834 avg_loss = 4.13109\n",
      "epoch no.0 train no.73220  loss = 4.34796 avg_loss = 4.13886\n",
      "epoch no.0 train no.73230  loss = 2.95655 avg_loss = 4.08099\n",
      "epoch no.0 train no.73240  loss = 2.88703 avg_loss = 4.08019\n",
      "epoch no.0 train no.73250  loss = 3.70103 avg_loss = 4.06721\n",
      "epoch no.0 train no.73260  loss = 2.45872 avg_loss = 4.06053\n",
      "epoch no.0 train no.73270  loss = 2.31872 avg_loss = 4.05496\n",
      "epoch no.0 train no.73280  loss = 3.26053 avg_loss = 4.05845\n",
      "epoch no.0 train no.73290  loss = 4.36545 avg_loss = 4.07019\n",
      "epoch no.0 train no.73300  loss = 2.87789 avg_loss = 4.14520\n",
      "epoch no.0 train no.73310  loss = 4.14115 avg_loss = 4.06969\n",
      "epoch no.0 train no.73320  loss = 2.73813 avg_loss = 4.02840\n",
      "epoch no.0 train no.73330  loss = 2.96041 avg_loss = 4.07364\n",
      "epoch no.0 train no.73340  loss = 2.62104 avg_loss = 4.01126\n",
      "epoch no.0 train no.73350  loss = 6.55762 avg_loss = 4.09055\n",
      "epoch no.0 train no.73360  loss = 6.71232 avg_loss = 4.12412\n",
      "epoch no.0 train no.73370  loss = 2.49801 avg_loss = 4.12188\n",
      "epoch no.0 train no.73380  loss = 5.37277 avg_loss = 4.11723\n",
      "epoch no.0 train no.73390  loss = 2.64148 avg_loss = 4.12522\n",
      "epoch no.0 train no.73400  loss = 3.33846 avg_loss = 4.16355\n",
      "epoch no.0 train no.73410  loss = 5.36212 avg_loss = 4.14839\n",
      "epoch no.0 train no.73420  loss = 3.12769 avg_loss = 4.15620\n",
      "epoch no.0 train no.73430  loss = 5.74894 avg_loss = 4.12624\n",
      "epoch no.0 train no.73440  loss = 2.68930 avg_loss = 4.07996\n",
      "epoch no.0 train no.73450  loss = 2.89510 avg_loss = 4.10898\n",
      "epoch no.0 train no.73460  loss = 4.83791 avg_loss = 4.14056\n",
      "epoch no.0 train no.73470  loss = 5.28682 avg_loss = 4.14119\n",
      "epoch no.0 train no.73480  loss = 3.60348 avg_loss = 4.07365\n",
      "epoch no.0 train no.73490  loss = 5.80702 avg_loss = 4.10554\n",
      "epoch no.0 train no.73500  loss = 2.77376 avg_loss = 4.10566\n",
      "epoch no.0 train no.73510  loss = 4.13325 avg_loss = 4.04377\n",
      "epoch no.0 train no.73520  loss = 3.35899 avg_loss = 3.98534\n",
      "epoch no.0 train no.73530  loss = 4.26793 avg_loss = 4.01451\n",
      "epoch no.0 train no.73540  loss = 2.82609 avg_loss = 3.98713\n",
      "epoch no.0 train no.73550  loss = 1.91227 avg_loss = 3.88416\n",
      "epoch no.0 train no.73560  loss = 3.71044 avg_loss = 3.85276\n",
      "epoch no.0 train no.73570  loss = 4.81881 avg_loss = 3.91991\n",
      "epoch no.0 train no.73580  loss = 6.49738 avg_loss = 3.92043\n",
      "epoch no.0 train no.73590  loss = 2.66629 avg_loss = 3.91699\n",
      "epoch no.0 train no.73600  loss = 2.85789 avg_loss = 3.85716\n",
      "epoch no.0 train no.73610  loss = 3.48972 avg_loss = 3.82620\n",
      "epoch no.0 train no.73620  loss = 6.57292 avg_loss = 3.94008\n",
      "epoch no.0 train no.73630  loss = 3.49774 avg_loss = 3.92450\n",
      "epoch no.0 train no.73640  loss = 3.49354 avg_loss = 3.91679\n",
      "epoch no.0 train no.73650  loss = 3.57748 avg_loss = 3.95845\n",
      "epoch no.0 train no.73660  loss = 7.31090 avg_loss = 4.01502\n",
      "epoch no.0 train no.73670  loss = 3.38400 avg_loss = 3.99160\n",
      "epoch no.0 train no.73680  loss = 6.30125 avg_loss = 4.00305\n",
      "epoch no.0 train no.73690  loss = 1.42246 avg_loss = 3.96362\n",
      "epoch no.0 train no.73700  loss = 6.63825 avg_loss = 4.04790\n",
      "epoch no.0 train no.73710  loss = 2.97958 avg_loss = 3.93785\n",
      "epoch no.0 train no.73720  loss = 4.07939 avg_loss = 3.92599\n",
      "epoch no.0 train no.73730  loss = 4.27512 avg_loss = 3.91611\n",
      "epoch no.0 train no.73740  loss = 5.47183 avg_loss = 3.91056\n",
      "epoch no.0 train no.73750  loss = 3.04091 avg_loss = 3.86331\n",
      "epoch no.0 train no.73760  loss = 2.61867 avg_loss = 3.83254\n",
      "epoch no.0 train no.73770  loss = 3.09925 avg_loss = 3.85005\n",
      "epoch no.0 train no.73780  loss = 4.83736 avg_loss = 3.91887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.73790  loss = 7.99150 avg_loss = 3.99505\n",
      "epoch no.0 train no.73800  loss = 2.30199 avg_loss = 3.98494\n",
      "epoch no.0 train no.73810  loss = 4.60647 avg_loss = 3.98159\n",
      "epoch no.0 train no.73820  loss = 6.03381 avg_loss = 4.04007\n",
      "epoch no.0 train no.73830  loss = 2.46685 avg_loss = 3.95554\n",
      "epoch no.0 train no.73840  loss = 2.59617 avg_loss = 3.89894\n",
      "epoch no.0 train no.73850  loss = 2.12820 avg_loss = 3.93960\n",
      "epoch no.0 train no.73860  loss = 2.64793 avg_loss = 3.91286\n",
      "epoch no.0 train no.73870  loss = 5.48046 avg_loss = 3.95516\n",
      "epoch no.0 train no.73880  loss = 2.95289 avg_loss = 3.90094\n",
      "epoch no.0 train no.73890  loss = 2.86487 avg_loss = 3.91118\n",
      "epoch no.0 train no.73900  loss = 5.12039 avg_loss = 3.95158\n",
      "epoch no.0 train no.73910  loss = 3.15283 avg_loss = 3.97587\n",
      "epoch no.0 train no.73920  loss = 3.74231 avg_loss = 4.02525\n",
      "epoch no.0 train no.73930  loss = 5.83717 avg_loss = 4.09537\n",
      "epoch no.0 train no.73940  loss = 3.97515 avg_loss = 4.09288\n",
      "epoch no.0 train no.73950  loss = 4.49525 avg_loss = 4.05333\n",
      "epoch no.0 train no.73960  loss = 3.53859 avg_loss = 4.05335\n",
      "epoch no.0 train no.73970  loss = 4.04561 avg_loss = 4.05380\n",
      "epoch no.0 train no.73980  loss = 3.97919 avg_loss = 4.01169\n",
      "epoch no.0 train no.73990  loss = 4.04500 avg_loss = 3.95542\n",
      "epoch no.0 train no.74000  loss = 3.65025 avg_loss = 3.97017\n",
      "5\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.0 train no.74010  loss = 2.22497 avg_loss = 3.91071\n",
      "epoch no.0 train no.74020  loss = 3.63996 avg_loss = 3.86178\n",
      "epoch no.0 train no.74030  loss = 3.68001 avg_loss = 3.93466\n",
      "epoch no.0 train no.74040  loss = 2.61078 avg_loss = 3.89023\n",
      "epoch no.0 train no.74050  loss = 5.10342 avg_loss = 3.99340\n",
      "epoch no.0 train no.74060  loss = 2.79234 avg_loss = 3.92878\n",
      "epoch no.0 train no.74070  loss = 3.52281 avg_loss = 3.92961\n",
      "epoch no.0 train no.74080  loss = 3.51962 avg_loss = 3.98106\n",
      "epoch no.0 train no.74090  loss = 2.92100 avg_loss = 3.94992\n",
      "epoch no.0 train no.74100  loss = 6.69317 avg_loss = 3.93606\n",
      "epoch no.0 train no.74110  loss = 2.96670 avg_loss = 3.90852\n",
      "epoch no.0 train no.74120  loss = 3.25750 avg_loss = 3.88841\n",
      "epoch no.0 train no.74130  loss = 4.53960 avg_loss = 3.87223\n",
      "epoch no.0 train no.74140  loss = 3.77769 avg_loss = 3.85370\n",
      "epoch no.0 train no.74150  loss = 3.66908 avg_loss = 3.84372\n",
      "epoch no.0 train no.74160  loss = 5.12979 avg_loss = 3.84429\n",
      "epoch no.0 train no.74170  loss = 5.31601 avg_loss = 3.88661\n",
      "epoch no.0 train no.74180  loss = 3.68741 avg_loss = 3.86032\n",
      "epoch no.0 train no.74190  loss = 5.46504 avg_loss = 3.88849\n",
      "epoch no.0 train no.74200  loss = 5.43141 avg_loss = 3.88780\n",
      "epoch no.0 train no.74210  loss = 3.09471 avg_loss = 3.82789\n",
      "epoch no.0 train no.74220  loss = 4.86570 avg_loss = 3.92209\n",
      "epoch no.0 train no.74230  loss = 3.19141 avg_loss = 3.92436\n",
      "epoch no.0 train no.74240  loss = 3.09135 avg_loss = 3.91802\n",
      "epoch no.0 train no.74250  loss = 4.21689 avg_loss = 3.89040\n",
      "epoch no.0 train no.74260  loss = 2.94622 avg_loss = 3.88208\n",
      "epoch no.0 train no.74270  loss = 2.34465 avg_loss = 3.88906\n",
      "epoch no.0 train no.74280  loss = 7.74437 avg_loss = 3.95070\n",
      "epoch no.0 train no.74290  loss = 3.19996 avg_loss = 3.94442\n",
      "epoch no.0 train no.74300  loss = 5.81520 avg_loss = 3.94674\n",
      "epoch no.0 train no.74310  loss = 2.56919 avg_loss = 3.90537\n",
      "epoch no.0 train no.74320  loss = 2.13586 avg_loss = 3.90454\n",
      "epoch no.0 train no.74330  loss = 3.39581 avg_loss = 3.93505\n",
      "epoch no.0 train no.74340  loss = 4.18343 avg_loss = 3.95430\n",
      "epoch no.0 train no.74350  loss = 2.47792 avg_loss = 3.94029\n",
      "epoch no.0 train no.74360  loss = 6.44506 avg_loss = 3.90840\n",
      "epoch no.0 train no.74370  loss = 5.65965 avg_loss = 3.90464\n",
      "epoch no.0 train no.74380  loss = 3.76068 avg_loss = 3.92662\n",
      "epoch no.0 train no.74390  loss = 5.45704 avg_loss = 3.96727\n",
      "epoch no.0 train no.74400  loss = 2.33593 avg_loss = 3.91009\n",
      "epoch no.0 train no.74410  loss = 3.92816 avg_loss = 3.93687\n",
      "epoch no.0 train no.74420  loss = 3.34327 avg_loss = 3.88230\n",
      "epoch no.0 train no.74430  loss = 4.00887 avg_loss = 3.91035\n",
      "epoch no.0 train no.74440  loss = 4.63216 avg_loss = 3.99048\n",
      "epoch no.0 train no.74450  loss = 3.16898 avg_loss = 4.00778\n",
      "epoch no.0 train no.74460  loss = 4.91463 avg_loss = 4.00059\n",
      "epoch no.0 train no.74470  loss = 5.42223 avg_loss = 4.02424\n",
      "epoch no.0 train no.74480  loss = 6.63704 avg_loss = 4.00427\n",
      "epoch no.0 train no.74490  loss = 4.42164 avg_loss = 4.02902\n",
      "epoch no.0 train no.74500  loss = 2.72409 avg_loss = 3.94093\n",
      "epoch no.0 train no.74510  loss = 4.64499 avg_loss = 3.97691\n",
      "epoch no.0 train no.74520  loss = 5.45607 avg_loss = 4.01820\n",
      "epoch no.0 train no.74530  loss = 3.48156 avg_loss = 3.97846\n",
      "epoch no.0 train no.74540  loss = 3.31339 avg_loss = 3.90728\n",
      "epoch no.0 train no.74550  loss = 5.73351 avg_loss = 3.98641\n",
      "epoch no.0 train no.74560  loss = 4.35610 avg_loss = 3.97100\n",
      "epoch no.0 train no.74570  loss = 2.44915 avg_loss = 3.93827\n",
      "epoch no.0 train no.74580  loss = 3.40653 avg_loss = 3.91354\n",
      "epoch no.0 train no.74590  loss = 3.74720 avg_loss = 3.96795\n",
      "epoch no.0 train no.74600  loss = 2.71389 avg_loss = 3.93097\n",
      "epoch no.0 train no.74610  loss = 3.39058 avg_loss = 3.87861\n",
      "epoch no.0 train no.74620  loss = 1.83943 avg_loss = 3.86441\n",
      "epoch no.0 train no.74630  loss = 2.28454 avg_loss = 3.82243\n",
      "epoch no.0 train no.74640  loss = 4.35822 avg_loss = 3.86727\n",
      "epoch no.0 train no.74650  loss = 2.79111 avg_loss = 3.83402\n",
      "epoch no.0 train no.74660  loss = 3.47148 avg_loss = 3.84481\n",
      "epoch no.0 train no.74670  loss = 2.95312 avg_loss = 3.88447\n",
      "epoch no.0 train no.74680  loss = 4.23992 avg_loss = 3.86573\n",
      "epoch no.0 train no.74690  loss = 4.03970 avg_loss = 3.85470\n",
      "epoch no.0 train no.74700  loss = 2.36437 avg_loss = 3.83504\n",
      "epoch no.0 train no.74710  loss = 2.90580 avg_loss = 3.85999\n",
      "epoch no.0 train no.74720  loss = 4.60368 avg_loss = 3.91536\n",
      "epoch no.0 train no.74730  loss = 2.69136 avg_loss = 3.90525\n",
      "epoch no.0 train no.74740  loss = 2.38985 avg_loss = 3.86538\n",
      "epoch no.0 train no.74750  loss = 3.15975 avg_loss = 3.81954\n",
      "epoch no.0 train no.74760  loss = 4.78093 avg_loss = 3.79267\n",
      "epoch no.0 train no.74770  loss = 5.41699 avg_loss = 3.86084\n",
      "epoch no.0 train no.74780  loss = 2.98750 avg_loss = 3.89267\n",
      "epoch no.0 train no.74790  loss = 2.88132 avg_loss = 3.88047\n",
      "epoch no.0 train no.74800  loss = 3.14424 avg_loss = 3.94785\n",
      "epoch no.0 train no.74810  loss = 3.49119 avg_loss = 3.93212\n",
      "epoch no.0 train no.74820  loss = 4.69028 avg_loss = 3.96225\n",
      "epoch no.0 train no.74830  loss = 3.00465 avg_loss = 3.90022\n",
      "epoch no.0 train no.74840  loss = 3.67961 avg_loss = 3.89819\n",
      "epoch no.0 train no.74850  loss = 2.43896 avg_loss = 3.90047\n",
      "epoch no.0 train no.74860  loss = 2.55843 avg_loss = 3.88612\n",
      "epoch no.0 train no.74870  loss = 4.28353 avg_loss = 3.88438\n",
      "epoch no.0 train no.74880  loss = 5.06745 avg_loss = 3.95222\n",
      "epoch no.0 train no.74890  loss = 3.38076 avg_loss = 3.89166\n",
      "epoch no.0 train no.74900  loss = 3.33829 avg_loss = 3.84667\n",
      "epoch no.0 train no.74910  loss = 2.27004 avg_loss = 3.88386\n",
      "epoch no.0 train no.74920  loss = 2.83337 avg_loss = 3.89002\n",
      "epoch no.0 train no.74930  loss = 6.25981 avg_loss = 3.90987\n",
      "epoch no.0 train no.74940  loss = 5.36407 avg_loss = 3.92110\n",
      "epoch no.0 train no.74950  loss = 4.51690 avg_loss = 3.91551\n",
      "epoch no.0 train no.74960  loss = 3.18650 avg_loss = 3.89601\n",
      "epoch no.0 train no.74970  loss = 6.37172 avg_loss = 3.92641\n",
      "epoch no.0 train no.74980  loss = 5.06629 avg_loss = 3.88586\n",
      "epoch no.0 train no.74990  loss = 2.99096 avg_loss = 3.89308\n",
      "epoch no.0 train no.75000  loss = 5.32524 avg_loss = 3.91795\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁위한', '▁플레이', '곡']\n",
      "기분전환을 위한 댄스</s>\n",
      "epoch no.0 train no.75010  loss = 6.45991 avg_loss = 3.91102\n",
      "epoch no.0 train no.75020  loss = 6.07399 avg_loss = 3.95155\n",
      "epoch no.0 train no.75030  loss = 4.50312 avg_loss = 3.95812\n",
      "epoch no.0 train no.75040  loss = 3.89283 avg_loss = 3.95066\n",
      "epoch no.0 train no.75050  loss = 1.82256 avg_loss = 3.94827\n",
      "epoch no.0 train no.75060  loss = 4.82617 avg_loss = 3.92065\n",
      "epoch no.0 train no.75070  loss = 4.30345 avg_loss = 3.90271\n",
      "epoch no.0 train no.75080  loss = 3.23353 avg_loss = 3.88538\n",
      "epoch no.0 train no.75090  loss = 2.38829 avg_loss = 3.87548\n",
      "epoch no.0 train no.75100  loss = 3.98002 avg_loss = 3.86293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.75110  loss = 3.38439 avg_loss = 3.82259\n",
      "epoch no.0 train no.75120  loss = 3.55270 avg_loss = 3.86070\n",
      "epoch no.0 train no.75130  loss = 3.33532 avg_loss = 3.86372\n",
      "epoch no.0 train no.75140  loss = 3.75758 avg_loss = 3.90766\n",
      "epoch no.0 train no.75150  loss = 2.32950 avg_loss = 3.88606\n",
      "epoch no.0 train no.75160  loss = 2.27048 avg_loss = 3.92784\n",
      "epoch no.0 train no.75170  loss = 3.67148 avg_loss = 3.90599\n",
      "epoch no.0 train no.75180  loss = 4.25999 avg_loss = 3.90207\n",
      "epoch no.0 train no.75190  loss = 4.17925 avg_loss = 3.94041\n",
      "epoch no.0 train no.75200  loss = 6.03281 avg_loss = 4.01552\n",
      "epoch no.0 train no.75210  loss = 5.84812 avg_loss = 3.99705\n",
      "epoch no.0 train no.75220  loss = 4.80691 avg_loss = 3.95734\n",
      "epoch no.0 train no.75230  loss = 5.34579 avg_loss = 3.93586\n",
      "epoch no.0 train no.75240  loss = 3.58813 avg_loss = 3.94903\n",
      "epoch no.0 train no.75250  loss = 3.94151 avg_loss = 3.97285\n",
      "epoch no.0 train no.75260  loss = 3.50826 avg_loss = 3.95015\n",
      "epoch no.0 train no.75270  loss = 5.86436 avg_loss = 3.98389\n",
      "epoch no.0 train no.75280  loss = 4.63014 avg_loss = 3.94013\n",
      "epoch no.0 train no.75290  loss = 3.04539 avg_loss = 3.90222\n",
      "epoch no.0 train no.75300  loss = 3.54409 avg_loss = 3.86168\n",
      "epoch no.0 train no.75310  loss = 4.10314 avg_loss = 3.92516\n",
      "epoch no.0 train no.75320  loss = 4.10290 avg_loss = 3.95625\n",
      "epoch no.0 train no.75330  loss = 2.43035 avg_loss = 3.93832\n",
      "epoch no.0 train no.75340  loss = 3.50173 avg_loss = 3.93307\n",
      "epoch no.0 train no.75350  loss = 3.28599 avg_loss = 3.94261\n",
      "epoch no.0 train no.75360  loss = 2.94280 avg_loss = 3.95034\n",
      "epoch no.0 train no.75370  loss = 6.26441 avg_loss = 4.00330\n",
      "epoch no.0 train no.75380  loss = 4.38103 avg_loss = 3.98518\n",
      "epoch no.0 train no.75390  loss = 4.80980 avg_loss = 3.96245\n",
      "epoch no.0 train no.75400  loss = 2.57001 avg_loss = 3.95758\n",
      "epoch no.0 train no.75410  loss = 2.26030 avg_loss = 3.93817\n",
      "epoch no.0 train no.75420  loss = 1.96833 avg_loss = 3.90733\n",
      "epoch no.0 train no.75430  loss = 2.96751 avg_loss = 3.90789\n",
      "epoch no.0 train no.75440  loss = 6.26370 avg_loss = 3.96408\n",
      "epoch no.0 train no.75450  loss = 2.65315 avg_loss = 3.92530\n",
      "epoch no.0 train no.75460  loss = 3.51412 avg_loss = 3.91351\n",
      "epoch no.0 train no.75470  loss = 2.83351 avg_loss = 3.94326\n",
      "epoch no.0 train no.75480  loss = 2.77895 avg_loss = 3.93843\n",
      "epoch no.0 train no.75490  loss = 3.75310 avg_loss = 3.89822\n",
      "epoch no.0 train no.75500  loss = 5.07221 avg_loss = 3.91639\n",
      "epoch no.0 train no.75510  loss = 3.30179 avg_loss = 3.91167\n",
      "epoch no.0 train no.75520  loss = 5.88137 avg_loss = 3.96948\n",
      "epoch no.0 train no.75530  loss = 3.84372 avg_loss = 3.90823\n",
      "epoch no.0 train no.75540  loss = 5.88862 avg_loss = 3.89634\n",
      "epoch no.0 train no.75550  loss = 3.72222 avg_loss = 3.86391\n",
      "epoch no.0 train no.75560  loss = 4.66934 avg_loss = 3.90900\n",
      "epoch no.0 train no.75570  loss = 5.85631 avg_loss = 3.99424\n",
      "epoch no.0 train no.75580  loss = 4.50472 avg_loss = 3.98503\n",
      "epoch no.0 train no.75590  loss = 4.03742 avg_loss = 3.93751\n",
      "epoch no.0 train no.75600  loss = 2.95044 avg_loss = 3.96004\n",
      "epoch no.0 train no.75610  loss = 4.58833 avg_loss = 4.01647\n",
      "epoch no.0 train no.75620  loss = 3.40952 avg_loss = 3.99830\n",
      "epoch no.0 train no.75630  loss = 5.69945 avg_loss = 4.03487\n",
      "epoch no.0 train no.75640  loss = 7.59316 avg_loss = 4.09981\n",
      "epoch no.0 train no.75650  loss = 2.89571 avg_loss = 4.12984\n",
      "epoch no.0 train no.75660  loss = 3.08783 avg_loss = 4.07602\n",
      "epoch no.0 train no.75670  loss = 3.87379 avg_loss = 4.06659\n",
      "epoch no.0 train no.75680  loss = 5.57759 avg_loss = 4.07961\n",
      "epoch no.0 train no.75690  loss = 3.23878 avg_loss = 4.02858\n",
      "epoch no.0 train no.75700  loss = 3.75249 avg_loss = 3.98017\n",
      "epoch no.0 train no.75710  loss = 4.50590 avg_loss = 3.94070\n",
      "epoch no.0 train no.75720  loss = 6.55312 avg_loss = 3.99283\n",
      "epoch no.0 train no.75730  loss = 4.44424 avg_loss = 3.99792\n",
      "epoch no.0 train no.75740  loss = 6.69643 avg_loss = 4.10128\n",
      "epoch no.0 train no.75750  loss = 2.02739 avg_loss = 4.04034\n",
      "epoch no.0 train no.75760  loss = 1.84798 avg_loss = 4.01861\n",
      "epoch no.0 train no.75770  loss = 2.62279 avg_loss = 4.02448\n",
      "epoch no.0 train no.75780  loss = 2.60399 avg_loss = 3.99171\n",
      "epoch no.0 train no.75790  loss = 2.50997 avg_loss = 3.95360\n",
      "epoch no.0 train no.75800  loss = 4.23932 avg_loss = 3.94466\n",
      "epoch no.0 train no.75810  loss = 5.82393 avg_loss = 3.93464\n",
      "epoch no.0 train no.75820  loss = 2.96149 avg_loss = 3.98276\n",
      "epoch no.0 train no.75830  loss = 3.96933 avg_loss = 4.04950\n",
      "epoch no.0 train no.75840  loss = 4.54723 avg_loss = 4.05712\n",
      "epoch no.0 train no.75850  loss = 3.97115 avg_loss = 4.02307\n",
      "epoch no.0 train no.75860  loss = 3.13604 avg_loss = 3.96272\n",
      "epoch no.0 train no.75870  loss = 3.38427 avg_loss = 3.90225\n",
      "epoch no.0 train no.75880  loss = 3.91864 avg_loss = 3.87055\n",
      "epoch no.0 train no.75890  loss = 3.83089 avg_loss = 3.86537\n",
      "epoch no.0 train no.75900  loss = 3.57981 avg_loss = 3.83492\n",
      "epoch no.0 train no.75910  loss = 4.91403 avg_loss = 3.82654\n",
      "epoch no.0 train no.75920  loss = 2.85733 avg_loss = 3.83866\n",
      "epoch no.0 train no.75930  loss = 3.03497 avg_loss = 3.86824\n",
      "epoch no.0 train no.75940  loss = 5.37557 avg_loss = 3.85388\n",
      "epoch no.0 train no.75950  loss = 6.36618 avg_loss = 3.93957\n",
      "epoch no.0 train no.75960  loss = 4.23269 avg_loss = 3.90593\n",
      "epoch no.0 train no.75970  loss = 3.63189 avg_loss = 3.87967\n",
      "epoch no.0 train no.75980  loss = 3.89620 avg_loss = 3.87065\n",
      "epoch no.0 train no.75990  loss = 4.27542 avg_loss = 3.88151\n",
      "epoch no.0 train no.76000  loss = 4.48067 avg_loss = 3.93822\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '을', '▁좋은', '▁노래', '▁노래', '</s>']\n",
      "기분전환하기 좋은 신나는 노래</s>\n",
      "epoch no.0 train no.76010  loss = 2.31498 avg_loss = 3.94741\n",
      "epoch no.0 train no.76020  loss = 3.46206 avg_loss = 3.95651\n",
      "epoch no.0 train no.76030  loss = 7.40568 avg_loss = 3.99673\n",
      "epoch no.0 train no.76040  loss = 5.32822 avg_loss = 3.97443\n",
      "epoch no.0 train no.76050  loss = 5.99557 avg_loss = 3.91680\n",
      "epoch no.0 train no.76060  loss = 3.01912 avg_loss = 3.93315\n",
      "epoch no.0 train no.76070  loss = 5.48328 avg_loss = 3.94340\n",
      "epoch no.0 train no.76080  loss = 3.20835 avg_loss = 3.93462\n",
      "epoch no.0 train no.76090  loss = 3.23505 avg_loss = 4.00173\n",
      "epoch no.0 train no.76100  loss = 3.45933 avg_loss = 3.97787\n",
      "epoch no.0 train no.76110  loss = 2.30920 avg_loss = 3.93396\n",
      "epoch no.0 train no.76120  loss = 4.93242 avg_loss = 3.95010\n",
      "epoch no.0 train no.76130  loss = 4.52501 avg_loss = 3.89857\n",
      "epoch no.0 train no.76140  loss = 5.10654 avg_loss = 3.92797\n",
      "epoch no.0 train no.76150  loss = 4.49992 avg_loss = 3.93249\n",
      "epoch no.0 train no.76160  loss = 4.69637 avg_loss = 3.90787\n",
      "epoch no.0 train no.76170  loss = 5.72673 avg_loss = 3.90955\n",
      "epoch no.0 train no.76180  loss = 3.62037 avg_loss = 3.93358\n",
      "epoch no.0 train no.76190  loss = 3.02788 avg_loss = 3.95676\n",
      "epoch no.0 train no.76200  loss = 4.26634 avg_loss = 3.90304\n",
      "epoch no.0 train no.76210  loss = 5.25302 avg_loss = 3.92916\n",
      "epoch no.0 train no.76220  loss = 3.37012 avg_loss = 3.96411\n",
      "epoch no.0 train no.76230  loss = 3.58021 avg_loss = 3.92845\n",
      "epoch no.0 train no.76240  loss = 4.18645 avg_loss = 3.94250\n",
      "epoch no.0 train no.76250  loss = 2.81691 avg_loss = 3.97057\n",
      "epoch no.0 train no.76260  loss = 7.05579 avg_loss = 3.99727\n",
      "epoch no.0 train no.76270  loss = 2.12768 avg_loss = 3.92095\n",
      "epoch no.0 train no.76280  loss = 6.24828 avg_loss = 3.99633\n",
      "epoch no.0 train no.76290  loss = 3.68473 avg_loss = 3.99304\n",
      "epoch no.0 train no.76300  loss = 4.19273 avg_loss = 3.99823\n",
      "epoch no.0 train no.76310  loss = 6.31766 avg_loss = 4.01175\n",
      "epoch no.0 train no.76320  loss = 3.44797 avg_loss = 4.02249\n",
      "epoch no.0 train no.76330  loss = 4.36216 avg_loss = 4.02712\n",
      "epoch no.0 train no.76340  loss = 2.18944 avg_loss = 4.00093\n",
      "epoch no.0 train no.76350  loss = 5.39748 avg_loss = 3.96624\n",
      "epoch no.0 train no.76360  loss = 3.40101 avg_loss = 3.96163\n",
      "epoch no.0 train no.76370  loss = 4.54053 avg_loss = 3.99682\n",
      "epoch no.0 train no.76380  loss = 4.90383 avg_loss = 4.00900\n",
      "epoch no.0 train no.76390  loss = 2.38665 avg_loss = 3.96342\n",
      "epoch no.0 train no.76400  loss = 3.82429 avg_loss = 3.95089\n",
      "epoch no.0 train no.76410  loss = 2.61759 avg_loss = 3.93450\n",
      "epoch no.0 train no.76420  loss = 5.67766 avg_loss = 4.05153\n",
      "epoch no.0 train no.76430  loss = 4.36963 avg_loss = 4.02401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.76440  loss = 3.85015 avg_loss = 4.00973\n",
      "epoch no.0 train no.76450  loss = 1.89071 avg_loss = 3.98764\n",
      "epoch no.0 train no.76460  loss = 3.30387 avg_loss = 3.98005\n",
      "epoch no.0 train no.76470  loss = 3.11129 avg_loss = 4.01337\n",
      "epoch no.0 train no.76480  loss = 2.94188 avg_loss = 4.01265\n",
      "epoch no.0 train no.76490  loss = 2.67106 avg_loss = 3.97206\n",
      "epoch no.0 train no.76500  loss = 2.41098 avg_loss = 3.97924\n",
      "epoch no.0 train no.76510  loss = 2.90504 avg_loss = 3.98728\n",
      "epoch no.0 train no.76520  loss = 3.04917 avg_loss = 3.94986\n",
      "epoch no.0 train no.76530  loss = 3.14996 avg_loss = 3.93395\n",
      "epoch no.0 train no.76540  loss = 2.82030 avg_loss = 3.89696\n",
      "epoch no.0 train no.76550  loss = 2.18645 avg_loss = 3.80360\n",
      "epoch no.0 train no.76560  loss = 4.31490 avg_loss = 3.87033\n",
      "epoch no.0 train no.76570  loss = 9.74728 avg_loss = 3.99334\n",
      "epoch no.0 train no.76580  loss = 3.32420 avg_loss = 3.96024\n",
      "epoch no.0 train no.76590  loss = 5.44039 avg_loss = 3.95514\n",
      "epoch no.0 train no.76600  loss = 4.05019 avg_loss = 4.00893\n",
      "epoch no.0 train no.76610  loss = 3.94534 avg_loss = 3.97944\n",
      "epoch no.0 train no.76620  loss = 2.64955 avg_loss = 3.97030\n",
      "epoch no.0 train no.76630  loss = 5.83078 avg_loss = 3.96285\n",
      "epoch no.0 train no.76640  loss = 3.51225 avg_loss = 3.99808\n",
      "epoch no.0 train no.76650  loss = 3.57750 avg_loss = 4.01708\n",
      "epoch no.0 train no.76660  loss = 2.53394 avg_loss = 3.97428\n",
      "epoch no.0 train no.76670  loss = 3.93645 avg_loss = 4.06910\n",
      "epoch no.0 train no.76680  loss = 2.98586 avg_loss = 4.05146\n",
      "epoch no.0 train no.76690  loss = 2.41231 avg_loss = 3.98872\n",
      "epoch no.0 train no.76700  loss = 4.45723 avg_loss = 4.07212\n",
      "epoch no.0 train no.76710  loss = 2.12212 avg_loss = 4.01422\n",
      "epoch no.0 train no.76720  loss = 6.48945 avg_loss = 4.00585\n",
      "epoch no.0 train no.76730  loss = 3.05465 avg_loss = 4.00121\n",
      "epoch no.0 train no.76740  loss = 4.38304 avg_loss = 4.04081\n",
      "epoch no.0 train no.76750  loss = 2.99915 avg_loss = 4.00601\n",
      "epoch no.0 train no.76760  loss = 4.09008 avg_loss = 4.10492\n",
      "epoch no.0 train no.76770  loss = 2.48630 avg_loss = 4.08808\n",
      "epoch no.0 train no.76780  loss = 2.05491 avg_loss = 4.05286\n",
      "epoch no.0 train no.76790  loss = 6.63800 avg_loss = 4.04557\n",
      "epoch no.0 train no.76800  loss = 3.99788 avg_loss = 4.05016\n",
      "epoch no.0 train no.76810  loss = 5.21728 avg_loss = 4.10276\n",
      "epoch no.0 train no.76820  loss = 5.27050 avg_loss = 4.11001\n",
      "epoch no.0 train no.76830  loss = 3.54556 avg_loss = 4.13484\n",
      "epoch no.0 train no.76840  loss = 3.74386 avg_loss = 4.09107\n",
      "epoch no.0 train no.76850  loss = 4.15983 avg_loss = 4.10278\n",
      "epoch no.0 train no.76860  loss = 3.74568 avg_loss = 4.14035\n",
      "epoch no.0 train no.76870  loss = 4.69981 avg_loss = 4.17427\n",
      "epoch no.0 train no.76880  loss = 6.42132 avg_loss = 4.11796\n",
      "epoch no.0 train no.76890  loss = 5.38532 avg_loss = 4.13458\n",
      "epoch no.0 train no.76900  loss = 3.41077 avg_loss = 4.12959\n",
      "epoch no.0 train no.76910  loss = 6.63694 avg_loss = 4.15043\n",
      "epoch no.0 train no.76920  loss = 3.76891 avg_loss = 4.08188\n",
      "epoch no.0 train no.76930  loss = 2.93726 avg_loss = 4.01194\n",
      "epoch no.0 train no.76940  loss = 2.52496 avg_loss = 4.00510\n",
      "epoch no.0 train no.76950  loss = 4.52173 avg_loss = 4.00682\n",
      "epoch no.0 train no.76960  loss = 4.07473 avg_loss = 3.95786\n",
      "epoch no.0 train no.76970  loss = 2.67040 avg_loss = 3.87543\n",
      "epoch no.0 train no.76980  loss = 4.19715 avg_loss = 3.80766\n",
      "epoch no.0 train no.76990  loss = 3.29704 avg_loss = 3.78484\n",
      "epoch no.0 train no.77000  loss = 2.68431 avg_loss = 3.83486\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁위한', '▁음악', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.0 train no.77010  loss = 5.26224 avg_loss = 3.89393\n",
      "epoch no.0 train no.77020  loss = 4.14037 avg_loss = 3.92251\n",
      "epoch no.0 train no.77030  loss = 2.50050 avg_loss = 3.93936\n",
      "epoch no.0 train no.77040  loss = 2.92789 avg_loss = 3.96274\n",
      "epoch no.0 train no.77050  loss = 4.16722 avg_loss = 3.92892\n",
      "epoch no.0 train no.77060  loss = 3.43077 avg_loss = 3.95032\n",
      "epoch no.0 train no.77070  loss = 3.89913 avg_loss = 3.92513\n",
      "epoch no.0 train no.77080  loss = 4.36367 avg_loss = 3.88605\n",
      "epoch no.0 train no.77090  loss = 3.29254 avg_loss = 3.83491\n",
      "epoch no.0 train no.77100  loss = 4.15929 avg_loss = 3.88812\n",
      "epoch no.0 train no.77110  loss = 3.30113 avg_loss = 3.93000\n",
      "epoch no.0 train no.77120  loss = 3.67651 avg_loss = 3.93572\n",
      "epoch no.0 train no.77130  loss = 4.96651 avg_loss = 3.96948\n",
      "epoch no.0 train no.77140  loss = 2.84180 avg_loss = 3.89858\n",
      "epoch no.0 train no.77150  loss = 2.70237 avg_loss = 3.91803\n",
      "epoch no.0 train no.77160  loss = 6.03926 avg_loss = 3.95585\n",
      "epoch no.0 train no.77170  loss = 4.42068 avg_loss = 4.01107\n",
      "epoch no.0 train no.77180  loss = 2.84871 avg_loss = 3.94542\n",
      "epoch no.0 train no.77190  loss = 4.39129 avg_loss = 3.96059\n",
      "epoch no.0 train no.77200  loss = 2.24782 avg_loss = 3.94901\n",
      "epoch no.0 train no.77210  loss = 8.35564 avg_loss = 4.03637\n",
      "epoch no.0 train no.77220  loss = 4.51372 avg_loss = 3.99708\n",
      "epoch no.0 train no.77230  loss = 4.70342 avg_loss = 3.96298\n",
      "epoch no.0 train no.77240  loss = 3.20040 avg_loss = 3.91676\n",
      "epoch no.0 train no.77250  loss = 2.61481 avg_loss = 3.96980\n",
      "epoch no.0 train no.77260  loss = 3.19578 avg_loss = 3.97275\n",
      "epoch no.0 train no.77270  loss = 4.12915 avg_loss = 3.98763\n",
      "epoch no.0 train no.77280  loss = 5.15528 avg_loss = 4.05231\n",
      "epoch no.0 train no.77290  loss = 2.15816 avg_loss = 4.05686\n",
      "epoch no.0 train no.77300  loss = 4.47112 avg_loss = 4.07794\n",
      "epoch no.0 train no.77310  loss = 2.29951 avg_loss = 4.06918\n",
      "epoch no.0 train no.77320  loss = 1.64488 avg_loss = 4.01679\n",
      "epoch no.0 train no.77330  loss = 5.38559 avg_loss = 3.98977\n",
      "epoch no.0 train no.77340  loss = 4.40138 avg_loss = 4.00599\n",
      "epoch no.0 train no.77350  loss = 2.03491 avg_loss = 3.96521\n",
      "epoch no.0 train no.77360  loss = 2.52180 avg_loss = 3.90781\n",
      "epoch no.0 train no.77370  loss = 3.11249 avg_loss = 3.87683\n",
      "epoch no.0 train no.77380  loss = 6.24058 avg_loss = 3.96465\n",
      "epoch no.0 train no.77390  loss = 2.49140 avg_loss = 3.94431\n",
      "epoch no.0 train no.77400  loss = 6.76430 avg_loss = 3.94370\n",
      "epoch no.0 train no.77410  loss = 3.59229 avg_loss = 3.94138\n",
      "epoch no.0 train no.77420  loss = 3.06731 avg_loss = 3.84640\n",
      "epoch no.0 train no.77430  loss = 2.91033 avg_loss = 3.88400\n",
      "epoch no.0 train no.77440  loss = 4.01139 avg_loss = 3.90562\n",
      "epoch no.0 train no.77450  loss = 3.67296 avg_loss = 3.92918\n",
      "epoch no.0 train no.77460  loss = 3.35841 avg_loss = 3.94599\n",
      "epoch no.0 train no.77470  loss = 4.32425 avg_loss = 3.98826\n",
      "epoch no.0 train no.77480  loss = 4.34302 avg_loss = 3.98270\n",
      "epoch no.0 train no.77490  loss = 3.91550 avg_loss = 3.96164\n",
      "epoch no.0 train no.77500  loss = 3.18881 avg_loss = 3.91597\n",
      "epoch no.0 train no.77510  loss = 4.15216 avg_loss = 3.92803\n",
      "epoch no.0 train no.77520  loss = 8.68492 avg_loss = 3.96798\n",
      "epoch no.0 train no.77530  loss = 4.61926 avg_loss = 3.96757\n",
      "epoch no.0 train no.77540  loss = 4.90628 avg_loss = 4.04303\n",
      "epoch no.0 train no.77550  loss = 3.13748 avg_loss = 4.01971\n",
      "epoch no.0 train no.77560  loss = 3.34886 avg_loss = 4.05874\n",
      "epoch no.0 train no.77570  loss = 4.76845 avg_loss = 4.08382\n",
      "epoch no.0 train no.77580  loss = 2.44991 avg_loss = 4.02516\n",
      "epoch no.0 train no.77590  loss = 3.24787 avg_loss = 3.97909\n",
      "epoch no.0 train no.77600  loss = 5.49130 avg_loss = 3.90555\n",
      "epoch no.0 train no.77610  loss = 5.13804 avg_loss = 3.94877\n",
      "epoch no.0 train no.77620  loss = 2.99758 avg_loss = 3.94009\n",
      "epoch no.0 train no.77630  loss = 4.87603 avg_loss = 4.00740\n",
      "epoch no.0 train no.77640  loss = 3.89291 avg_loss = 3.98457\n",
      "epoch no.0 train no.77650  loss = 3.89127 avg_loss = 4.03109\n",
      "epoch no.0 train no.77660  loss = 2.46033 avg_loss = 4.02446\n",
      "epoch no.0 train no.77670  loss = 3.66477 avg_loss = 4.04429\n",
      "epoch no.0 train no.77680  loss = 5.29668 avg_loss = 4.06716\n",
      "epoch no.0 train no.77690  loss = 3.20808 avg_loss = 4.02078\n",
      "epoch no.0 train no.77700  loss = 3.87462 avg_loss = 4.08263\n",
      "epoch no.0 train no.77710  loss = 2.04086 avg_loss = 4.00641\n",
      "epoch no.0 train no.77720  loss = 3.74005 avg_loss = 3.96976\n",
      "epoch no.0 train no.77730  loss = 3.98500 avg_loss = 4.01734\n",
      "epoch no.0 train no.77740  loss = 5.53901 avg_loss = 4.06429\n",
      "epoch no.0 train no.77750  loss = 2.41187 avg_loss = 4.00080\n",
      "epoch no.0 train no.77760  loss = 4.07616 avg_loss = 4.00774\n",
      "epoch no.0 train no.77770  loss = 2.01124 avg_loss = 3.95411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.77780  loss = 3.18972 avg_loss = 4.00473\n",
      "epoch no.0 train no.77790  loss = 2.23208 avg_loss = 3.94105\n",
      "epoch no.0 train no.77800  loss = 5.48644 avg_loss = 3.91188\n",
      "epoch no.0 train no.77810  loss = 3.86305 avg_loss = 3.91752\n",
      "epoch no.0 train no.77820  loss = 2.67584 avg_loss = 3.94657\n",
      "epoch no.0 train no.77830  loss = 5.34577 avg_loss = 3.94025\n",
      "epoch no.0 train no.77840  loss = 3.14961 avg_loss = 3.86215\n",
      "epoch no.0 train no.77850  loss = 4.43584 avg_loss = 3.85586\n",
      "epoch no.0 train no.77860  loss = 3.56456 avg_loss = 3.81401\n",
      "epoch no.0 train no.77870  loss = 2.25125 avg_loss = 3.75438\n",
      "epoch no.0 train no.77880  loss = 5.64166 avg_loss = 3.77602\n",
      "epoch no.0 train no.77890  loss = 3.37326 avg_loss = 3.81541\n",
      "epoch no.0 train no.77900  loss = 4.72940 avg_loss = 3.87280\n",
      "epoch no.0 train no.77910  loss = 4.84521 avg_loss = 3.88634\n",
      "epoch no.0 train no.77920  loss = 3.92014 avg_loss = 3.86082\n",
      "epoch no.0 train no.77930  loss = 4.52962 avg_loss = 3.93984\n",
      "epoch no.0 train no.77940  loss = 5.71454 avg_loss = 3.98418\n",
      "epoch no.0 train no.77950  loss = 4.47579 avg_loss = 3.98170\n",
      "epoch no.0 train no.77960  loss = 2.39542 avg_loss = 3.97236\n",
      "epoch no.0 train no.77970  loss = 2.62801 avg_loss = 4.00206\n",
      "epoch no.0 train no.77980  loss = 5.05903 avg_loss = 4.01397\n",
      "epoch no.0 train no.77990  loss = 5.38332 avg_loss = 4.08378\n",
      "epoch no.0 train no.78000  loss = 4.22377 avg_loss = 4.02024\n",
      "5\n",
      "to_tokens: ['▁비', '좋은', '이', '▁위한', '▁신나는', '송', '</s>', '</s>']\n",
      "기분전환을 위한 팝송 모음</s>\n",
      "epoch no.0 train no.78010  loss = 3.11028 avg_loss = 4.04870\n",
      "epoch no.0 train no.78020  loss = 4.07241 avg_loss = 4.02114\n",
      "epoch no.0 train no.78030  loss = 4.75227 avg_loss = 4.04404\n",
      "epoch no.0 train no.78040  loss = 4.32251 avg_loss = 4.07802\n",
      "epoch no.0 train no.78050  loss = 4.50595 avg_loss = 4.09554\n",
      "epoch no.0 train no.78060  loss = 4.24102 avg_loss = 4.13716\n",
      "epoch no.0 train no.78070  loss = 3.45467 avg_loss = 4.11901\n",
      "epoch no.0 train no.78080  loss = 2.20433 avg_loss = 4.03867\n",
      "epoch no.0 train no.78090  loss = 5.35066 avg_loss = 4.04213\n",
      "epoch no.0 train no.78100  loss = 4.90087 avg_loss = 3.99069\n",
      "epoch no.0 train no.78110  loss = 3.46117 avg_loss = 3.95328\n",
      "epoch no.0 train no.78120  loss = 2.23992 avg_loss = 3.91976\n",
      "epoch no.0 train no.78130  loss = 2.77889 avg_loss = 3.94514\n",
      "epoch no.0 train no.78140  loss = 2.28213 avg_loss = 3.88677\n",
      "epoch no.0 train no.78150  loss = 2.62405 avg_loss = 3.89858\n",
      "epoch no.0 train no.78160  loss = 4.13073 avg_loss = 3.84866\n",
      "epoch no.0 train no.78170  loss = 3.46872 avg_loss = 3.84746\n",
      "epoch no.0 train no.78180  loss = 3.29403 avg_loss = 3.81041\n",
      "epoch no.0 train no.78190  loss = 4.66023 avg_loss = 3.78230\n",
      "epoch no.0 train no.78200  loss = 7.71878 avg_loss = 3.84546\n",
      "epoch no.0 train no.78210  loss = 3.86766 avg_loss = 3.80935\n",
      "epoch no.0 train no.78220  loss = 1.83697 avg_loss = 3.84977\n",
      "epoch no.0 train no.78230  loss = 4.01529 avg_loss = 3.87419\n",
      "epoch no.0 train no.78240  loss = 4.63865 avg_loss = 3.85136\n",
      "epoch no.0 train no.78250  loss = 5.11136 avg_loss = 3.96285\n",
      "epoch no.0 train no.78260  loss = 3.54690 avg_loss = 3.95908\n",
      "epoch no.0 train no.78270  loss = 2.80191 avg_loss = 4.02229\n",
      "epoch no.0 train no.78280  loss = 3.51292 avg_loss = 4.05810\n",
      "epoch no.0 train no.78290  loss = 2.26609 avg_loss = 3.96504\n",
      "epoch no.0 train no.78300  loss = 3.43722 avg_loss = 3.93020\n",
      "epoch no.0 train no.78310  loss = 4.62151 avg_loss = 3.97549\n",
      "epoch no.0 train no.78320  loss = 2.63162 avg_loss = 4.01493\n",
      "epoch no.0 train no.78330  loss = 2.94296 avg_loss = 3.95893\n",
      "epoch no.0 train no.78340  loss = 3.76874 avg_loss = 3.98525\n",
      "epoch no.0 train no.78350  loss = 3.12679 avg_loss = 3.96067\n",
      "epoch no.0 train no.78360  loss = 3.42291 avg_loss = 3.96622\n",
      "epoch no.0 train no.78370  loss = 3.95561 avg_loss = 4.02679\n",
      "epoch no.0 train no.78380  loss = 2.77943 avg_loss = 4.04338\n",
      "epoch no.0 train no.78390  loss = 6.11494 avg_loss = 4.03870\n",
      "epoch no.0 train no.78400  loss = 7.66566 avg_loss = 4.11576\n",
      "epoch no.0 train no.78410  loss = 2.52299 avg_loss = 4.10568\n",
      "epoch no.0 train no.78420  loss = 3.30644 avg_loss = 4.04065\n",
      "epoch no.0 train no.78430  loss = 3.94789 avg_loss = 4.01379\n",
      "epoch no.0 train no.78440  loss = 3.30509 avg_loss = 4.06031\n",
      "epoch no.0 train no.78450  loss = 2.34384 avg_loss = 4.04065\n",
      "epoch no.0 train no.78460  loss = 3.93496 avg_loss = 3.97119\n",
      "epoch no.0 train no.78470  loss = 2.83221 avg_loss = 3.96916\n",
      "epoch no.0 train no.78480  loss = 4.97428 avg_loss = 3.96917\n",
      "epoch no.0 train no.78490  loss = 3.24654 avg_loss = 4.00330\n",
      "epoch no.0 train no.78500  loss = 4.33927 avg_loss = 3.97792\n",
      "epoch no.0 train no.78510  loss = 3.82726 avg_loss = 3.99119\n",
      "epoch no.0 train no.78520  loss = 5.96276 avg_loss = 3.99892\n",
      "epoch no.0 train no.78530  loss = 2.59824 avg_loss = 3.95781\n",
      "epoch no.0 train no.78540  loss = 2.72670 avg_loss = 3.93173\n",
      "epoch no.0 train no.78550  loss = 3.60679 avg_loss = 3.89975\n",
      "epoch no.0 train no.78560  loss = 5.27389 avg_loss = 3.90866\n",
      "epoch no.0 train no.78570  loss = 4.37270 avg_loss = 3.87238\n",
      "epoch no.0 train no.78580  loss = 4.52725 avg_loss = 3.88719\n",
      "epoch no.0 train no.78590  loss = 7.87848 avg_loss = 3.93991\n",
      "epoch no.0 train no.78600  loss = 3.13061 avg_loss = 3.99019\n",
      "epoch no.0 train no.78610  loss = 3.42741 avg_loss = 4.07688\n",
      "epoch no.0 train no.78620  loss = 3.47819 avg_loss = 4.09111\n",
      "epoch no.0 train no.78630  loss = 2.76066 avg_loss = 4.06190\n",
      "epoch no.0 train no.78640  loss = 4.08059 avg_loss = 4.10541\n",
      "epoch no.0 train no.78650  loss = 3.70031 avg_loss = 4.05834\n",
      "epoch no.0 train no.78660  loss = 4.44567 avg_loss = 4.05119\n",
      "epoch no.0 train no.78670  loss = 2.86266 avg_loss = 4.02102\n",
      "epoch no.0 train no.78680  loss = 2.90720 avg_loss = 3.97697\n",
      "epoch no.0 train no.78690  loss = 7.69379 avg_loss = 4.04968\n",
      "epoch no.0 train no.78700  loss = 3.35999 avg_loss = 4.02439\n",
      "epoch no.0 train no.78710  loss = 2.73579 avg_loss = 4.01183\n",
      "epoch no.0 train no.78720  loss = 4.04345 avg_loss = 3.96838\n",
      "epoch no.0 train no.78730  loss = 3.78142 avg_loss = 3.98990\n",
      "epoch no.0 train no.78740  loss = 4.29650 avg_loss = 3.98693\n",
      "epoch no.0 train no.78750  loss = 7.24363 avg_loss = 4.03907\n",
      "epoch no.0 train no.78760  loss = 2.74045 avg_loss = 4.02928\n",
      "epoch no.0 train no.78770  loss = 4.14720 avg_loss = 4.01231\n",
      "epoch no.0 train no.78780  loss = 4.69057 avg_loss = 3.97340\n",
      "epoch no.0 train no.78790  loss = 4.95022 avg_loss = 3.94607\n",
      "epoch no.0 train no.78800  loss = 3.91567 avg_loss = 3.91361\n",
      "epoch no.0 train no.78810  loss = 4.26216 avg_loss = 3.89818\n",
      "epoch no.0 train no.78820  loss = 4.97949 avg_loss = 3.91372\n",
      "epoch no.0 train no.78830  loss = 3.20287 avg_loss = 3.99092\n",
      "epoch no.0 train no.78840  loss = 3.12319 avg_loss = 4.04950\n",
      "epoch no.0 train no.78850  loss = 4.21098 avg_loss = 4.09225\n",
      "epoch no.0 train no.78860  loss = 4.17035 avg_loss = 4.05208\n",
      "epoch no.0 train no.78870  loss = 3.82895 avg_loss = 3.98613\n",
      "epoch no.0 train no.78880  loss = 5.05247 avg_loss = 3.96048\n",
      "epoch no.0 train no.78890  loss = 3.77307 avg_loss = 4.05718\n",
      "epoch no.0 train no.78900  loss = 3.26013 avg_loss = 4.07267\n",
      "epoch no.0 train no.78910  loss = 6.25700 avg_loss = 4.06321\n",
      "epoch no.0 train no.78920  loss = 3.00955 avg_loss = 4.05371\n",
      "epoch no.0 train no.78930  loss = 2.23768 avg_loss = 4.08711\n",
      "epoch no.0 train no.78940  loss = 4.21970 avg_loss = 4.09949\n",
      "epoch no.0 train no.78950  loss = 3.67720 avg_loss = 4.13873\n",
      "epoch no.0 train no.78960  loss = 4.20156 avg_loss = 4.13027\n",
      "epoch no.0 train no.78970  loss = 3.62520 avg_loss = 4.14073\n",
      "epoch no.0 train no.78980  loss = 3.07785 avg_loss = 4.09256\n",
      "epoch no.0 train no.78990  loss = 3.70878 avg_loss = 4.11578\n",
      "epoch no.0 train no.79000  loss = 3.67743 avg_loss = 4.10495\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.0 train no.79010  loss = 3.35762 avg_loss = 4.05894\n",
      "epoch no.0 train no.79020  loss = 2.65916 avg_loss = 4.04871\n",
      "epoch no.0 train no.79030  loss = 4.47305 avg_loss = 4.04343\n",
      "epoch no.0 train no.79040  loss = 5.24198 avg_loss = 4.12896\n",
      "epoch no.0 train no.79050  loss = 5.03206 avg_loss = 4.11642\n",
      "epoch no.0 train no.79060  loss = 2.87523 avg_loss = 4.12626\n",
      "epoch no.0 train no.79070  loss = 2.64309 avg_loss = 4.06028\n",
      "epoch no.0 train no.79080  loss = 4.08289 avg_loss = 4.02148\n",
      "epoch no.0 train no.79090  loss = 3.05944 avg_loss = 4.02051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.79100  loss = 4.11020 avg_loss = 4.00457\n",
      "epoch no.0 train no.79110  loss = 3.54103 avg_loss = 4.02222\n",
      "epoch no.0 train no.79120  loss = 2.63626 avg_loss = 3.99222\n",
      "epoch no.0 train no.79130  loss = 2.19015 avg_loss = 3.99629\n",
      "epoch no.0 train no.79140  loss = 1.92409 avg_loss = 3.97271\n",
      "epoch no.0 train no.79150  loss = 2.83154 avg_loss = 3.95542\n",
      "epoch no.0 train no.79160  loss = 6.06162 avg_loss = 3.91787\n",
      "epoch no.0 train no.79170  loss = 6.34545 avg_loss = 3.95573\n",
      "epoch no.0 train no.79180  loss = 3.41727 avg_loss = 3.88897\n",
      "epoch no.0 train no.79190  loss = 6.38321 avg_loss = 3.87329\n",
      "epoch no.0 train no.79200  loss = 5.44834 avg_loss = 3.85807\n",
      "epoch no.0 train no.79210  loss = 4.69103 avg_loss = 3.93349\n",
      "epoch no.0 train no.79220  loss = 3.96225 avg_loss = 3.96639\n",
      "epoch no.0 train no.79230  loss = 3.35677 avg_loss = 3.92807\n",
      "epoch no.0 train no.79240  loss = 2.97193 avg_loss = 3.92546\n",
      "epoch no.0 train no.79250  loss = 3.99060 avg_loss = 3.98309\n",
      "epoch no.0 train no.79260  loss = 3.52989 avg_loss = 3.91646\n",
      "epoch no.0 train no.79270  loss = 2.45152 avg_loss = 3.93795\n",
      "epoch no.0 train no.79280  loss = 6.00561 avg_loss = 3.97407\n",
      "epoch no.0 train no.79290  loss = 3.33129 avg_loss = 3.95143\n",
      "epoch no.0 train no.79300  loss = 3.71278 avg_loss = 3.90065\n",
      "epoch no.0 train no.79310  loss = 4.63708 avg_loss = 3.96980\n",
      "epoch no.0 train no.79320  loss = 3.99653 avg_loss = 3.99299\n",
      "epoch no.0 train no.79330  loss = 5.93797 avg_loss = 4.04891\n",
      "epoch no.0 train no.79340  loss = 3.29162 avg_loss = 4.01592\n",
      "epoch no.0 train no.79350  loss = 4.94384 avg_loss = 3.99855\n",
      "epoch no.0 train no.79360  loss = 6.50396 avg_loss = 3.99173\n",
      "epoch no.0 train no.79370  loss = 2.20563 avg_loss = 3.90888\n",
      "epoch no.0 train no.79380  loss = 4.44968 avg_loss = 3.94820\n",
      "epoch no.0 train no.79390  loss = 4.28938 avg_loss = 3.89307\n",
      "epoch no.0 train no.79400  loss = 4.04414 avg_loss = 3.93945\n",
      "epoch no.0 train no.79410  loss = 2.79875 avg_loss = 3.98496\n",
      "epoch no.0 train no.79420  loss = 3.56154 avg_loss = 3.97331\n",
      "epoch no.0 train no.79430  loss = 2.68463 avg_loss = 3.99702\n",
      "epoch no.0 train no.79440  loss = 7.19301 avg_loss = 4.07386\n",
      "epoch no.0 train no.79450  loss = 2.88935 avg_loss = 4.03573\n",
      "epoch no.0 train no.79460  loss = 3.59781 avg_loss = 4.05545\n",
      "epoch no.0 train no.79470  loss = 4.17604 avg_loss = 3.98141\n",
      "epoch no.0 train no.79480  loss = 4.58132 avg_loss = 3.93776\n",
      "epoch no.0 train no.79490  loss = 5.75405 avg_loss = 3.96748\n",
      "epoch no.0 train no.79500  loss = 5.44230 avg_loss = 3.88749\n",
      "epoch no.0 train no.79510  loss = 2.76417 avg_loss = 3.90074\n",
      "epoch no.0 train no.79520  loss = 2.91793 avg_loss = 3.88531\n",
      "epoch no.0 train no.79530  loss = 2.54810 avg_loss = 3.87800\n",
      "epoch no.0 train no.79540  loss = 2.99015 avg_loss = 3.83148\n",
      "epoch no.0 train no.79550  loss = 3.32570 avg_loss = 3.90191\n",
      "epoch no.0 train no.79560  loss = 3.78146 avg_loss = 3.92164\n",
      "epoch no.0 train no.79570  loss = 3.13159 avg_loss = 3.91768\n",
      "epoch no.0 train no.79580  loss = 4.13573 avg_loss = 3.90484\n",
      "epoch no.0 train no.79590  loss = 3.02386 avg_loss = 3.91251\n",
      "epoch no.0 train no.79600  loss = 3.27700 avg_loss = 3.94034\n",
      "epoch no.0 train no.79610  loss = 6.46057 avg_loss = 3.94208\n",
      "epoch no.0 train no.79620  loss = 2.90234 avg_loss = 3.95215\n",
      "epoch no.0 train no.79630  loss = 2.72045 avg_loss = 3.94338\n",
      "epoch no.0 train no.79640  loss = 3.30175 avg_loss = 3.96647\n",
      "epoch no.0 train no.79650  loss = 3.70940 avg_loss = 3.97353\n",
      "epoch no.0 train no.79660  loss = 3.20099 avg_loss = 3.92449\n",
      "epoch no.0 train no.79670  loss = 4.17844 avg_loss = 3.87747\n",
      "epoch no.0 train no.79680  loss = 5.35055 avg_loss = 3.91471\n",
      "epoch no.0 train no.79690  loss = 4.62010 avg_loss = 3.93557\n",
      "epoch no.0 train no.79700  loss = 4.05852 avg_loss = 3.88777\n",
      "epoch no.0 train no.79710  loss = 3.91100 avg_loss = 3.91693\n",
      "epoch no.0 train no.79720  loss = 4.13525 avg_loss = 3.96472\n",
      "epoch no.0 train no.79730  loss = 4.72619 avg_loss = 4.00301\n",
      "epoch no.0 train no.79740  loss = 4.51796 avg_loss = 3.98427\n",
      "epoch no.0 train no.79750  loss = 5.18776 avg_loss = 3.95022\n",
      "epoch no.0 train no.79760  loss = 3.83380 avg_loss = 3.91036\n",
      "epoch no.0 train no.79770  loss = 3.23828 avg_loss = 3.91552\n",
      "epoch no.0 train no.79780  loss = 5.67728 avg_loss = 3.95502\n",
      "epoch no.0 train no.79790  loss = 2.87383 avg_loss = 3.94664\n",
      "epoch no.0 train no.79800  loss = 5.50688 avg_loss = 3.89835\n",
      "epoch no.0 train no.79810  loss = 5.23812 avg_loss = 3.94963\n",
      "epoch no.0 train no.79820  loss = 4.19659 avg_loss = 3.92645\n",
      "epoch no.0 train no.79830  loss = 6.99475 avg_loss = 3.94041\n",
      "epoch no.0 train no.79840  loss = 2.67252 avg_loss = 3.96324\n",
      "epoch no.0 train no.79850  loss = 3.86514 avg_loss = 3.96654\n",
      "epoch no.0 train no.79860  loss = 4.52414 avg_loss = 3.97266\n",
      "epoch no.0 train no.79870  loss = 2.61234 avg_loss = 3.94444\n",
      "epoch no.0 train no.79880  loss = 2.66157 avg_loss = 3.95916\n",
      "epoch no.0 train no.79890  loss = 2.60085 avg_loss = 3.96375\n",
      "epoch no.0 train no.79900  loss = 5.00038 avg_loss = 3.94083\n",
      "epoch no.0 train no.79910  loss = 4.40898 avg_loss = 3.93070\n",
      "epoch no.0 train no.79920  loss = 4.53434 avg_loss = 3.96780\n",
      "epoch no.0 train no.79930  loss = 3.60480 avg_loss = 4.01321\n",
      "epoch no.0 train no.79940  loss = 4.41095 avg_loss = 4.00956\n",
      "epoch no.0 train no.79950  loss = 1.97158 avg_loss = 3.94273\n",
      "epoch no.0 train no.79960  loss = 5.90723 avg_loss = 3.93131\n",
      "epoch no.0 train no.79970  loss = 3.97279 avg_loss = 4.01290\n",
      "epoch no.0 train no.79980  loss = 4.80671 avg_loss = 4.02513\n",
      "epoch no.0 train no.79990  loss = 4.53791 avg_loss = 4.01718\n",
      "epoch no.0 train no.80000  loss = 4.36591 avg_loss = 4.06321\n",
      "6\n",
      "to_tokens: ['▁가을', '전환', '이', '때', '▁듣는', '좋은', '▁노래', '▁노래', '</s>']\n",
      "기분전환 할때 듣기좋은 신나는 노래</s>\n",
      "epoch no.0 train no.80010  loss = 2.37583 avg_loss = 4.05663\n",
      "epoch no.0 train no.80020  loss = 3.67606 avg_loss = 4.05595\n",
      "epoch no.0 train no.80030  loss = 1.66434 avg_loss = 4.03031\n",
      "epoch no.0 train no.80040  loss = 3.58471 avg_loss = 4.01540\n",
      "epoch no.0 train no.80050  loss = 6.23298 avg_loss = 4.03786\n",
      "epoch no.0 train no.80060  loss = 4.71849 avg_loss = 4.01412\n",
      "epoch no.0 train no.80070  loss = 3.43023 avg_loss = 3.93728\n",
      "epoch no.0 train no.80080  loss = 3.14837 avg_loss = 3.89090\n",
      "epoch no.0 train no.80090  loss = 3.85155 avg_loss = 3.86878\n",
      "epoch no.0 train no.80100  loss = 5.63814 avg_loss = 3.83115\n",
      "epoch no.0 train no.80110  loss = 3.15462 avg_loss = 3.85324\n",
      "epoch no.0 train no.80120  loss = 3.15541 avg_loss = 3.90191\n",
      "epoch no.0 train no.80130  loss = 4.27633 avg_loss = 3.89904\n",
      "epoch no.0 train no.80140  loss = 7.00255 avg_loss = 3.96323\n",
      "epoch no.0 train no.80150  loss = 3.24620 avg_loss = 3.96872\n",
      "epoch no.0 train no.80160  loss = 3.67295 avg_loss = 3.94205\n",
      "epoch no.0 train no.80170  loss = 4.36943 avg_loss = 3.94329\n",
      "epoch no.0 train no.80180  loss = 4.87176 avg_loss = 3.93025\n",
      "epoch no.0 train no.80190  loss = 3.38381 avg_loss = 3.95500\n",
      "epoch no.0 train no.80200  loss = 2.61108 avg_loss = 3.98541\n",
      "epoch no.0 train no.80210  loss = 3.21193 avg_loss = 3.96726\n",
      "epoch no.0 train no.80220  loss = 4.25881 avg_loss = 3.94308\n",
      "epoch no.0 train no.80230  loss = 3.88012 avg_loss = 3.95895\n",
      "epoch no.0 train no.80240  loss = 2.05577 avg_loss = 3.95157\n",
      "epoch no.0 train no.80250  loss = 4.94193 avg_loss = 3.98546\n",
      "epoch no.0 train no.80260  loss = 4.70561 avg_loss = 3.97712\n",
      "epoch no.0 train no.80270  loss = 5.61011 avg_loss = 4.01125\n",
      "epoch no.0 train no.80280  loss = 3.83441 avg_loss = 4.02448\n",
      "epoch no.0 train no.80290  loss = 3.32400 avg_loss = 3.99022\n",
      "epoch no.0 train no.80300  loss = 5.39060 avg_loss = 4.01110\n",
      "epoch no.0 train no.80310  loss = 4.01113 avg_loss = 4.02909\n",
      "epoch no.0 train no.80320  loss = 3.86812 avg_loss = 4.03273\n",
      "epoch no.0 train no.80330  loss = 2.24737 avg_loss = 4.04604\n",
      "epoch no.0 train no.80340  loss = 4.28831 avg_loss = 4.04349\n",
      "epoch no.0 train no.80350  loss = 6.35822 avg_loss = 3.97674\n",
      "epoch no.0 train no.80360  loss = 3.39144 avg_loss = 3.97880\n",
      "epoch no.0 train no.80370  loss = 3.52685 avg_loss = 4.01711\n",
      "epoch no.0 train no.80380  loss = 4.90555 avg_loss = 4.03231\n",
      "epoch no.0 train no.80390  loss = 4.62649 avg_loss = 4.01857\n",
      "epoch no.0 train no.80400  loss = 4.80542 avg_loss = 3.98979\n",
      "epoch no.0 train no.80410  loss = 4.19531 avg_loss = 4.00776\n",
      "epoch no.0 train no.80420  loss = 2.66120 avg_loss = 3.97447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.80430  loss = 5.24915 avg_loss = 3.99621\n",
      "epoch no.0 train no.80440  loss = 2.57542 avg_loss = 4.05947\n",
      "epoch no.0 train no.80450  loss = 6.49981 avg_loss = 4.06000\n",
      "epoch no.0 train no.80460  loss = 2.74349 avg_loss = 4.01579\n",
      "epoch no.0 train no.80470  loss = 2.73838 avg_loss = 4.02661\n",
      "epoch no.0 train no.80480  loss = 3.36227 avg_loss = 3.99226\n",
      "epoch no.0 train no.80490  loss = 1.89972 avg_loss = 3.94052\n",
      "epoch no.0 train no.80500  loss = 2.35557 avg_loss = 3.95872\n",
      "epoch no.0 train no.80510  loss = 3.01044 avg_loss = 3.91050\n",
      "epoch no.0 train no.80520  loss = 3.38805 avg_loss = 3.86459\n",
      "epoch no.0 train no.80530  loss = 6.70451 avg_loss = 3.92808\n",
      "epoch no.0 train no.80540  loss = 3.79958 avg_loss = 3.87567\n",
      "epoch no.0 train no.80550  loss = 2.14996 avg_loss = 3.86386\n",
      "epoch no.0 train no.80560  loss = 2.65772 avg_loss = 3.85231\n",
      "epoch no.0 train no.80570  loss = 5.30327 avg_loss = 3.86012\n",
      "epoch no.0 train no.80580  loss = 5.94052 avg_loss = 3.86800\n",
      "epoch no.0 train no.80590  loss = 2.88748 avg_loss = 3.85693\n",
      "epoch no.0 train no.80600  loss = 2.93111 avg_loss = 3.84322\n",
      "epoch no.0 train no.80610  loss = 2.46050 avg_loss = 3.84093\n",
      "epoch no.0 train no.80620  loss = 3.60353 avg_loss = 3.88166\n",
      "epoch no.0 train no.80630  loss = 2.73115 avg_loss = 3.86577\n",
      "epoch no.0 train no.80640  loss = 2.90251 avg_loss = 3.82139\n",
      "epoch no.0 train no.80650  loss = 3.57545 avg_loss = 3.86731\n",
      "epoch no.0 train no.80660  loss = 3.26632 avg_loss = 3.79535\n",
      "epoch no.0 train no.80670  loss = 3.59328 avg_loss = 3.83457\n",
      "epoch no.0 train no.80680  loss = 3.50317 avg_loss = 3.83746\n",
      "epoch no.0 train no.80690  loss = 4.67622 avg_loss = 3.90720\n",
      "epoch no.0 train no.80700  loss = 4.37717 avg_loss = 3.89785\n",
      "epoch no.0 train no.80710  loss = 3.04470 avg_loss = 3.97818\n",
      "epoch no.0 train no.80720  loss = 3.38743 avg_loss = 3.94711\n",
      "epoch no.0 train no.80730  loss = 2.69768 avg_loss = 3.94910\n",
      "epoch no.0 train no.80740  loss = 4.30346 avg_loss = 3.95757\n",
      "epoch no.0 train no.80750  loss = 2.68391 avg_loss = 3.95434\n",
      "epoch no.0 train no.80760  loss = 3.88205 avg_loss = 3.99798\n",
      "epoch no.0 train no.80770  loss = 5.30721 avg_loss = 4.02104\n",
      "epoch no.0 train no.80780  loss = 4.29735 avg_loss = 4.08002\n",
      "epoch no.0 train no.80790  loss = 3.06204 avg_loss = 4.09577\n",
      "epoch no.0 train no.80800  loss = 3.09935 avg_loss = 4.16290\n",
      "epoch no.0 train no.80810  loss = 3.80167 avg_loss = 4.13288\n",
      "epoch no.0 train no.80820  loss = 2.69570 avg_loss = 4.04841\n",
      "epoch no.0 train no.80830  loss = 2.79215 avg_loss = 4.11277\n",
      "epoch no.0 train no.80840  loss = 3.48126 avg_loss = 4.03920\n",
      "epoch no.0 train no.80850  loss = 3.19931 avg_loss = 4.10805\n",
      "epoch no.0 train no.80860  loss = 3.91729 avg_loss = 4.11335\n",
      "epoch no.0 train no.80870  loss = 3.30094 avg_loss = 4.05840\n",
      "epoch no.0 train no.80880  loss = 6.18064 avg_loss = 4.16044\n",
      "epoch no.0 train no.80890  loss = 1.93569 avg_loss = 4.13290\n",
      "epoch no.0 train no.80900  loss = 3.80090 avg_loss = 4.08762\n",
      "epoch no.0 train no.80910  loss = 2.57233 avg_loss = 4.07633\n",
      "epoch no.0 train no.80920  loss = 3.83519 avg_loss = 4.07869\n",
      "epoch no.0 train no.80930  loss = 3.83365 avg_loss = 4.08534\n",
      "epoch no.0 train no.80940  loss = 2.79002 avg_loss = 4.02194\n",
      "epoch no.0 train no.80950  loss = 4.45523 avg_loss = 3.99706\n",
      "epoch no.0 train no.80960  loss = 3.69168 avg_loss = 3.98265\n",
      "epoch no.0 train no.80970  loss = 3.97248 avg_loss = 4.05897\n",
      "epoch no.0 train no.80980  loss = 4.47942 avg_loss = 3.99955\n",
      "epoch no.0 train no.80990  loss = 4.54292 avg_loss = 4.02523\n",
      "epoch no.0 train no.81000  loss = 2.41454 avg_loss = 4.06383\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁딱', '</s>']\n",
      "기분전환에 딱</s>\n",
      "epoch no.0 train no.81010  loss = 2.73902 avg_loss = 4.03616\n",
      "epoch no.0 train no.81020  loss = 3.73990 avg_loss = 3.98620\n",
      "epoch no.0 train no.81030  loss = 2.95589 avg_loss = 3.97492\n",
      "epoch no.0 train no.81040  loss = 5.06396 avg_loss = 3.98345\n",
      "epoch no.0 train no.81050  loss = 3.75048 avg_loss = 4.00304\n",
      "epoch no.0 train no.81060  loss = 4.54285 avg_loss = 4.04010\n",
      "epoch no.0 train no.81070  loss = 4.08657 avg_loss = 4.02143\n",
      "epoch no.0 train no.81080  loss = 3.21536 avg_loss = 4.00664\n",
      "epoch no.0 train no.81090  loss = 1.93250 avg_loss = 3.96515\n",
      "epoch no.0 train no.81100  loss = 5.15507 avg_loss = 3.93958\n",
      "epoch no.0 train no.81110  loss = 4.70353 avg_loss = 3.94874\n",
      "epoch no.0 train no.81120  loss = 4.29651 avg_loss = 3.93766\n",
      "epoch no.0 train no.81130  loss = 3.68961 avg_loss = 3.92894\n",
      "epoch no.0 train no.81140  loss = 1.89225 avg_loss = 3.91413\n",
      "epoch no.0 train no.81150  loss = 2.23721 avg_loss = 3.85786\n",
      "epoch no.0 train no.81160  loss = 3.61428 avg_loss = 3.85237\n",
      "epoch no.0 train no.81170  loss = 3.67492 avg_loss = 3.92048\n",
      "epoch no.0 train no.81180  loss = 4.47388 avg_loss = 3.93666\n",
      "epoch no.0 train no.81190  loss = 4.13286 avg_loss = 3.96069\n",
      "epoch no.0 train no.81200  loss = 4.44453 avg_loss = 3.93121\n",
      "epoch no.0 train no.81210  loss = 4.03658 avg_loss = 3.98675\n",
      "epoch no.0 train no.81220  loss = 4.33580 avg_loss = 3.97063\n",
      "epoch no.0 train no.81230  loss = 5.21174 avg_loss = 3.97168\n",
      "epoch no.0 train no.81240  loss = 2.56258 avg_loss = 3.95511\n",
      "epoch no.0 train no.81250  loss = 3.72755 avg_loss = 3.95975\n",
      "epoch no.0 train no.81260  loss = 3.07125 avg_loss = 3.98385\n",
      "epoch no.0 train no.81270  loss = 3.17387 avg_loss = 4.01010\n",
      "epoch no.0 train no.81280  loss = 4.22498 avg_loss = 4.01674\n",
      "epoch no.0 train no.81290  loss = 4.12884 avg_loss = 3.96187\n",
      "epoch no.0 train no.81300  loss = 3.28253 avg_loss = 3.93628\n",
      "epoch no.0 train no.81310  loss = 4.44649 avg_loss = 3.97941\n",
      "epoch no.0 train no.81320  loss = 3.65506 avg_loss = 3.98856\n",
      "epoch no.0 train no.81330  loss = 3.43683 avg_loss = 3.99012\n",
      "epoch no.0 train no.81340  loss = 5.92703 avg_loss = 4.06498\n",
      "epoch no.0 train no.81350  loss = 4.21009 avg_loss = 4.04971\n",
      "epoch no.0 train no.81360  loss = 3.78323 avg_loss = 3.99101\n",
      "epoch no.0 train no.81370  loss = 3.46455 avg_loss = 3.95485\n",
      "epoch no.0 train no.81380  loss = 3.94963 avg_loss = 3.91010\n",
      "epoch no.0 train no.81390  loss = 4.27036 avg_loss = 3.87053\n",
      "epoch no.0 train no.81400  loss = 3.76330 avg_loss = 3.85414\n",
      "epoch no.0 train no.81410  loss = 3.42871 avg_loss = 3.89686\n",
      "epoch no.0 train no.81420  loss = 4.10784 avg_loss = 3.86520\n",
      "epoch no.0 train no.81430  loss = 2.49212 avg_loss = 3.94749\n",
      "epoch no.0 train no.81440  loss = 4.11801 avg_loss = 3.94640\n",
      "epoch no.0 train no.81450  loss = 4.28779 avg_loss = 3.92518\n",
      "epoch no.0 train no.81460  loss = 3.63610 avg_loss = 3.94165\n",
      "epoch no.0 train no.81470  loss = 2.37425 avg_loss = 3.92074\n",
      "epoch no.0 train no.81480  loss = 5.22764 avg_loss = 3.97614\n",
      "epoch no.0 train no.81490  loss = 5.30494 avg_loss = 3.97496\n",
      "epoch no.0 train no.81500  loss = 3.84733 avg_loss = 4.00093\n",
      "epoch no.0 train no.81510  loss = 6.06449 avg_loss = 4.03753\n",
      "epoch no.0 train no.81520  loss = 3.56336 avg_loss = 3.96726\n",
      "epoch no.0 train no.81530  loss = 3.64673 avg_loss = 4.03316\n",
      "epoch no.0 train no.81540  loss = 3.92369 avg_loss = 4.03955\n",
      "epoch no.0 train no.81550  loss = 5.07646 avg_loss = 3.98883\n",
      "epoch no.0 train no.81560  loss = 2.92589 avg_loss = 4.00385\n",
      "epoch no.0 train no.81570  loss = 4.22145 avg_loss = 4.01845\n",
      "epoch no.0 train no.81580  loss = 4.06696 avg_loss = 4.11407\n",
      "epoch no.0 train no.81590  loss = 3.39206 avg_loss = 4.12044\n",
      "epoch no.0 train no.81600  loss = 3.40902 avg_loss = 4.14287\n",
      "epoch no.0 train no.81610  loss = 5.08520 avg_loss = 4.12663\n",
      "epoch no.0 train no.81620  loss = 3.37838 avg_loss = 4.08400\n",
      "epoch no.0 train no.81630  loss = 5.02139 avg_loss = 4.08939\n",
      "epoch no.0 train no.81640  loss = 3.84800 avg_loss = 4.08058\n",
      "epoch no.0 train no.81650  loss = 3.33655 avg_loss = 4.06740\n",
      "epoch no.0 train no.81660  loss = 3.53366 avg_loss = 4.04504\n",
      "epoch no.0 train no.81670  loss = 3.28531 avg_loss = 4.02648\n",
      "epoch no.0 train no.81680  loss = 2.76682 avg_loss = 4.03221\n",
      "epoch no.0 train no.81690  loss = 5.82711 avg_loss = 4.03419\n",
      "epoch no.0 train no.81700  loss = 2.56354 avg_loss = 4.01090\n",
      "epoch no.0 train no.81710  loss = 4.59153 avg_loss = 4.05539\n",
      "epoch no.0 train no.81720  loss = 2.87788 avg_loss = 4.07851\n",
      "epoch no.0 train no.81730  loss = 5.24646 avg_loss = 4.07420\n",
      "epoch no.0 train no.81740  loss = 3.00375 avg_loss = 4.07567\n",
      "epoch no.0 train no.81750  loss = 3.31859 avg_loss = 4.03654\n",
      "epoch no.0 train no.81760  loss = 4.25910 avg_loss = 4.05788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.81770  loss = 6.59506 avg_loss = 4.04585\n",
      "epoch no.0 train no.81780  loss = 4.11619 avg_loss = 3.96566\n",
      "epoch no.0 train no.81790  loss = 5.71102 avg_loss = 3.91050\n",
      "epoch no.0 train no.81800  loss = 4.50410 avg_loss = 3.90767\n",
      "epoch no.0 train no.81810  loss = 1.81531 avg_loss = 3.91661\n",
      "epoch no.0 train no.81820  loss = 2.17338 avg_loss = 3.87345\n",
      "epoch no.0 train no.81830  loss = 4.61702 avg_loss = 3.87524\n",
      "epoch no.0 train no.81840  loss = 3.61143 avg_loss = 3.90555\n",
      "epoch no.0 train no.81850  loss = 4.52450 avg_loss = 3.89899\n",
      "epoch no.0 train no.81860  loss = 6.91774 avg_loss = 3.95679\n",
      "epoch no.0 train no.81870  loss = 4.54110 avg_loss = 3.93014\n",
      "epoch no.0 train no.81880  loss = 4.96233 avg_loss = 3.90909\n",
      "epoch no.0 train no.81890  loss = 3.65404 avg_loss = 3.87810\n",
      "epoch no.0 train no.81900  loss = 4.04265 avg_loss = 3.89503\n",
      "epoch no.0 train no.81910  loss = 6.75429 avg_loss = 3.88293\n",
      "epoch no.0 train no.81920  loss = 4.42354 avg_loss = 3.93083\n",
      "epoch no.0 train no.81930  loss = 3.13490 avg_loss = 3.99921\n",
      "epoch no.0 train no.81940  loss = 2.76039 avg_loss = 4.02302\n",
      "epoch no.0 train no.81950  loss = 3.70716 avg_loss = 3.99582\n",
      "epoch no.0 train no.81960  loss = 2.51509 avg_loss = 3.99022\n",
      "epoch no.0 train no.81970  loss = 2.90144 avg_loss = 4.00630\n",
      "epoch no.0 train no.81980  loss = 4.10240 avg_loss = 4.04526\n",
      "epoch no.0 train no.81990  loss = 2.24737 avg_loss = 4.06420\n",
      "epoch no.0 train no.82000  loss = 2.73518 avg_loss = 4.03887\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '▁때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.0 train no.82010  loss = 4.55475 avg_loss = 4.04010\n",
      "epoch no.0 train no.82020  loss = 2.11382 avg_loss = 4.06948\n",
      "epoch no.0 train no.82030  loss = 5.24664 avg_loss = 4.07139\n",
      "epoch no.0 train no.82040  loss = 3.45863 avg_loss = 4.06693\n",
      "epoch no.0 train no.82050  loss = 3.77334 avg_loss = 4.05419\n",
      "epoch no.0 train no.82060  loss = 3.59619 avg_loss = 3.99614\n",
      "epoch no.0 train no.82070  loss = 2.96816 avg_loss = 3.98243\n",
      "epoch no.0 train no.82080  loss = 6.36768 avg_loss = 4.04567\n",
      "epoch no.0 train no.82090  loss = 2.43505 avg_loss = 4.00235\n",
      "epoch no.0 train no.82100  loss = 5.78528 avg_loss = 4.02261\n",
      "epoch no.0 train no.82110  loss = 3.28441 avg_loss = 4.04458\n",
      "epoch no.0 train no.82120  loss = 5.56608 avg_loss = 4.03388\n",
      "epoch no.0 train no.82130  loss = 4.33346 avg_loss = 4.02443\n",
      "epoch no.0 train no.82140  loss = 3.71555 avg_loss = 4.04336\n",
      "epoch no.0 train no.82150  loss = 5.28414 avg_loss = 4.05016\n",
      "epoch no.0 train no.82160  loss = 3.95194 avg_loss = 4.01401\n",
      "epoch no.0 train no.82170  loss = 6.06454 avg_loss = 3.99553\n",
      "epoch no.0 train no.82180  loss = 2.71232 avg_loss = 4.05069\n",
      "epoch no.0 train no.82190  loss = 3.70646 avg_loss = 4.02409\n",
      "epoch no.0 train no.82200  loss = 4.12816 avg_loss = 4.01251\n",
      "epoch no.0 train no.82210  loss = 5.08841 avg_loss = 4.02811\n",
      "epoch no.0 train no.82220  loss = 5.30558 avg_loss = 4.01904\n",
      "epoch no.0 train no.82230  loss = 5.78997 avg_loss = 4.05932\n",
      "epoch no.0 train no.82240  loss = 4.76291 avg_loss = 4.03542\n",
      "epoch no.0 train no.82250  loss = 3.91351 avg_loss = 3.99726\n",
      "epoch no.0 train no.82260  loss = 3.69684 avg_loss = 4.06027\n",
      "epoch no.0 train no.82270  loss = 4.12145 avg_loss = 4.02595\n",
      "epoch no.0 train no.82280  loss = 4.60562 avg_loss = 3.99754\n",
      "epoch no.0 train no.82290  loss = 2.96605 avg_loss = 3.99111\n",
      "epoch no.0 train no.82300  loss = 3.66518 avg_loss = 3.94507\n",
      "epoch no.0 train no.82310  loss = 2.30581 avg_loss = 3.88099\n",
      "epoch no.0 train no.82320  loss = 5.70453 avg_loss = 3.96797\n",
      "epoch no.0 train no.82330  loss = 2.68319 avg_loss = 3.97362\n",
      "epoch no.0 train no.82340  loss = 3.24592 avg_loss = 3.95470\n",
      "epoch no.0 train no.82350  loss = 2.47330 avg_loss = 3.97914\n",
      "epoch no.0 train no.82360  loss = 1.89843 avg_loss = 3.97544\n",
      "epoch no.0 train no.82370  loss = 6.36927 avg_loss = 3.98769\n",
      "epoch no.0 train no.82380  loss = 3.47470 avg_loss = 3.96643\n",
      "epoch no.0 train no.82390  loss = 2.49243 avg_loss = 4.00156\n",
      "epoch no.0 train no.82400  loss = 3.12267 avg_loss = 3.90695\n",
      "epoch no.0 train no.82410  loss = 4.52476 avg_loss = 3.93780\n",
      "epoch no.0 train no.82420  loss = 4.55398 avg_loss = 3.98724\n",
      "epoch no.0 train no.82430  loss = 3.38317 avg_loss = 4.01016\n",
      "epoch no.0 train no.82440  loss = 2.87487 avg_loss = 3.98128\n",
      "epoch no.0 train no.82450  loss = 2.44682 avg_loss = 4.00195\n",
      "epoch no.0 train no.82460  loss = 3.28787 avg_loss = 4.00317\n",
      "epoch no.0 train no.82470  loss = 3.26064 avg_loss = 4.03903\n",
      "epoch no.0 train no.82480  loss = 2.60430 avg_loss = 4.00570\n",
      "epoch no.0 train no.82490  loss = 1.88379 avg_loss = 3.96505\n",
      "epoch no.0 train no.82500  loss = 4.67118 avg_loss = 3.97277\n",
      "epoch no.0 train no.82510  loss = 3.41247 avg_loss = 3.99783\n",
      "epoch no.0 train no.82520  loss = 5.69255 avg_loss = 4.02538\n",
      "epoch no.0 train no.82530  loss = 4.81790 avg_loss = 4.06092\n",
      "epoch no.0 train no.82540  loss = 5.67873 avg_loss = 4.10454\n",
      "epoch no.0 train no.82550  loss = 3.92734 avg_loss = 4.10642\n",
      "epoch no.0 train no.82560  loss = 3.43456 avg_loss = 4.06299\n",
      "epoch no.0 train no.82570  loss = 3.65970 avg_loss = 3.98319\n",
      "epoch no.0 train no.82580  loss = 3.36781 avg_loss = 4.02788\n",
      "epoch no.0 train no.82590  loss = 5.44161 avg_loss = 3.94631\n",
      "epoch no.0 train no.82600  loss = 3.91937 avg_loss = 3.90189\n",
      "epoch no.0 train no.82610  loss = 3.26412 avg_loss = 3.93515\n",
      "epoch no.0 train no.82620  loss = 2.61358 avg_loss = 3.94244\n",
      "epoch no.0 train no.82630  loss = 4.50759 avg_loss = 3.92711\n",
      "epoch no.0 train no.82640  loss = 3.27912 avg_loss = 3.91049\n",
      "epoch no.0 train no.82650  loss = 6.82704 avg_loss = 3.95354\n",
      "epoch no.0 train no.82660  loss = 4.78072 avg_loss = 3.95829\n",
      "epoch no.0 train no.82670  loss = 5.52419 avg_loss = 3.96599\n",
      "epoch no.0 train no.82680  loss = 2.09322 avg_loss = 3.92388\n",
      "epoch no.0 train no.82690  loss = 4.02656 avg_loss = 3.91129\n",
      "epoch no.0 train no.82700  loss = 3.52009 avg_loss = 3.98078\n",
      "epoch no.0 train no.82710  loss = 3.29810 avg_loss = 3.96632\n",
      "epoch no.0 train no.82720  loss = 4.82043 avg_loss = 3.98984\n",
      "epoch no.0 train no.82730  loss = 3.65143 avg_loss = 3.97751\n",
      "epoch no.0 train no.82740  loss = 3.98441 avg_loss = 3.91160\n",
      "epoch no.0 train no.82750  loss = 4.90842 avg_loss = 3.96109\n",
      "epoch no.0 train no.82760  loss = 3.51666 avg_loss = 3.87346\n",
      "epoch no.0 train no.82770  loss = 4.21333 avg_loss = 3.86476\n",
      "epoch no.0 train no.82780  loss = 2.77217 avg_loss = 3.85967\n",
      "epoch no.0 train no.82790  loss = 3.02104 avg_loss = 3.82631\n",
      "epoch no.0 train no.82800  loss = 3.13268 avg_loss = 3.83537\n",
      "epoch no.0 train no.82810  loss = 3.04071 avg_loss = 3.80663\n",
      "epoch no.0 train no.82820  loss = 3.03101 avg_loss = 3.79163\n",
      "epoch no.0 train no.82830  loss = 4.91833 avg_loss = 3.89768\n",
      "epoch no.0 train no.82840  loss = 4.64466 avg_loss = 3.89039\n",
      "epoch no.0 train no.82850  loss = 3.23256 avg_loss = 3.84610\n",
      "epoch no.0 train no.82860  loss = 4.59260 avg_loss = 3.86854\n",
      "epoch no.0 train no.82870  loss = 4.06979 avg_loss = 3.92985\n",
      "epoch no.0 train no.82880  loss = 3.37224 avg_loss = 3.94593\n",
      "epoch no.0 train no.82890  loss = 3.36109 avg_loss = 3.89938\n",
      "epoch no.0 train no.82900  loss = 3.05571 avg_loss = 3.89871\n",
      "epoch no.0 train no.82910  loss = 5.72160 avg_loss = 3.98154\n",
      "epoch no.0 train no.82920  loss = 4.80143 avg_loss = 4.04155\n",
      "epoch no.0 train no.82930  loss = 3.94473 avg_loss = 4.03182\n",
      "epoch no.0 train no.82940  loss = 2.80130 avg_loss = 3.98275\n",
      "epoch no.0 train no.82950  loss = 2.37761 avg_loss = 3.95358\n",
      "epoch no.0 train no.82960  loss = 2.58271 avg_loss = 3.89427\n",
      "epoch no.0 train no.82970  loss = 3.29058 avg_loss = 3.94961\n",
      "epoch no.0 train no.82980  loss = 3.57479 avg_loss = 3.91082\n",
      "epoch no.0 train no.82990  loss = 2.04304 avg_loss = 3.88939\n",
      "epoch no.0 train no.83000  loss = 2.96218 avg_loss = 3.86978\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁좋은', '▁노래', '▁음악', 'op', '</s>']\n",
      "기분전환에 좋은 신나는 pop</s>\n",
      "epoch no.0 train no.83010  loss = 2.65802 avg_loss = 3.91946\n",
      "epoch no.0 train no.83020  loss = 5.85362 avg_loss = 4.01267\n",
      "epoch no.0 train no.83030  loss = 5.37950 avg_loss = 4.07845\n",
      "epoch no.0 train no.83040  loss = 4.90663 avg_loss = 4.08137\n",
      "epoch no.0 train no.83050  loss = 2.58165 avg_loss = 4.10817\n",
      "epoch no.0 train no.83060  loss = 2.79322 avg_loss = 4.11102\n",
      "epoch no.0 train no.83070  loss = 4.63486 avg_loss = 4.12386\n",
      "epoch no.0 train no.83080  loss = 3.07432 avg_loss = 4.03512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.83090  loss = 6.14507 avg_loss = 4.04999\n",
      "epoch no.0 train no.83100  loss = 3.37041 avg_loss = 4.01902\n",
      "epoch no.0 train no.83110  loss = 5.52586 avg_loss = 3.97658\n",
      "epoch no.0 train no.83120  loss = 3.25415 avg_loss = 3.99478\n",
      "epoch no.0 train no.83130  loss = 4.36044 avg_loss = 3.97966\n",
      "epoch no.0 train no.83140  loss = 5.55076 avg_loss = 3.95654\n",
      "epoch no.0 train no.83150  loss = 6.78070 avg_loss = 4.01766\n",
      "epoch no.0 train no.83160  loss = 3.76384 avg_loss = 3.99859\n",
      "epoch no.0 train no.83170  loss = 3.58016 avg_loss = 4.03970\n",
      "epoch no.0 train no.83180  loss = 4.93944 avg_loss = 4.09472\n",
      "epoch no.0 train no.83190  loss = 2.98353 avg_loss = 4.12644\n",
      "epoch no.0 train no.83200  loss = 1.92171 avg_loss = 4.09405\n",
      "epoch no.0 train no.83210  loss = 2.68414 avg_loss = 3.97529\n",
      "epoch no.0 train no.83220  loss = 4.37938 avg_loss = 3.95239\n",
      "epoch no.0 train no.83230  loss = 3.91537 avg_loss = 3.93678\n",
      "epoch no.0 train no.83240  loss = 3.75335 avg_loss = 3.88973\n",
      "epoch no.0 train no.83250  loss = 2.88622 avg_loss = 3.92463\n",
      "epoch no.0 train no.83260  loss = 4.38199 avg_loss = 3.92107\n",
      "epoch no.0 train no.83270  loss = 3.41448 avg_loss = 3.94442\n",
      "epoch no.0 train no.83280  loss = 6.43463 avg_loss = 3.93476\n",
      "epoch no.0 train no.83290  loss = 3.68793 avg_loss = 3.98910\n",
      "epoch no.0 train no.83300  loss = 3.12982 avg_loss = 3.96133\n",
      "epoch no.0 train no.83310  loss = 4.23526 avg_loss = 3.92219\n",
      "epoch no.0 train no.83320  loss = 5.36991 avg_loss = 3.99557\n",
      "epoch no.0 train no.83330  loss = 5.00988 avg_loss = 4.07687\n",
      "epoch no.0 train no.83340  loss = 3.29486 avg_loss = 4.10100\n",
      "epoch no.0 train no.83350  loss = 2.36140 avg_loss = 4.07757\n",
      "epoch no.0 train no.83360  loss = 5.10037 avg_loss = 4.05633\n",
      "epoch no.0 train no.83370  loss = 4.21562 avg_loss = 4.09350\n",
      "epoch no.0 train no.83380  loss = 4.01934 avg_loss = 4.06382\n",
      "epoch no.0 train no.83390  loss = 3.57816 avg_loss = 4.10741\n",
      "epoch no.0 train no.83400  loss = 2.52511 avg_loss = 4.10512\n",
      "epoch no.0 train no.83410  loss = 6.59824 avg_loss = 4.11194\n",
      "epoch no.0 train no.83420  loss = 4.16989 avg_loss = 4.07589\n",
      "epoch no.0 train no.83430  loss = 4.46096 avg_loss = 4.08383\n",
      "epoch no.0 train no.83440  loss = 5.31138 avg_loss = 4.17078\n",
      "epoch no.0 train no.83450  loss = 3.97231 avg_loss = 4.21618\n",
      "epoch no.0 train no.83460  loss = 4.15831 avg_loss = 4.23174\n",
      "epoch no.0 train no.83470  loss = 4.79561 avg_loss = 4.21680\n",
      "epoch no.0 train no.83480  loss = 5.15437 avg_loss = 4.24021\n",
      "epoch no.0 train no.83490  loss = 2.65561 avg_loss = 4.15335\n",
      "epoch no.0 train no.83500  loss = 3.61231 avg_loss = 4.15072\n",
      "epoch no.0 train no.83510  loss = 2.93436 avg_loss = 4.10804\n",
      "epoch no.0 train no.83520  loss = 2.88485 avg_loss = 4.07708\n",
      "epoch no.0 train no.83530  loss = 3.81330 avg_loss = 4.05339\n",
      "epoch no.0 train no.83540  loss = 3.41518 avg_loss = 4.02448\n",
      "epoch no.0 train no.83550  loss = 3.63287 avg_loss = 4.00322\n",
      "epoch no.0 train no.83560  loss = 2.55678 avg_loss = 3.95322\n",
      "epoch no.0 train no.83570  loss = 4.02014 avg_loss = 3.91085\n",
      "epoch no.0 train no.83580  loss = 3.09044 avg_loss = 3.93413\n",
      "epoch no.0 train no.83590  loss = 5.83823 avg_loss = 3.92837\n",
      "epoch no.0 train no.83600  loss = 4.14205 avg_loss = 3.92761\n",
      "epoch no.0 train no.83610  loss = 4.29493 avg_loss = 3.97173\n",
      "epoch no.0 train no.83620  loss = 5.35363 avg_loss = 3.99104\n",
      "epoch no.0 train no.83630  loss = 6.36405 avg_loss = 4.02422\n",
      "epoch no.0 train no.83640  loss = 3.71983 avg_loss = 3.98333\n",
      "epoch no.0 train no.83650  loss = 2.50057 avg_loss = 3.95418\n",
      "epoch no.0 train no.83660  loss = 4.86971 avg_loss = 3.96457\n",
      "epoch no.0 train no.83670  loss = 4.26054 avg_loss = 3.90234\n",
      "epoch no.0 train no.83680  loss = 6.88963 avg_loss = 3.97638\n",
      "epoch no.0 train no.83690  loss = 6.10242 avg_loss = 3.98006\n",
      "epoch no.0 train no.83700  loss = 5.08764 avg_loss = 4.00787\n",
      "epoch no.0 train no.83710  loss = 2.78072 avg_loss = 3.98385\n",
      "epoch no.0 train no.83720  loss = 5.23760 avg_loss = 4.05068\n",
      "epoch no.0 train no.83730  loss = 3.79175 avg_loss = 4.05534\n",
      "epoch no.0 train no.83740  loss = 4.68071 avg_loss = 4.00580\n",
      "epoch no.0 train no.83750  loss = 4.33126 avg_loss = 4.02502\n",
      "epoch no.0 train no.83760  loss = 4.48766 avg_loss = 4.10350\n",
      "epoch no.0 train no.83770  loss = 4.68748 avg_loss = 4.11802\n",
      "epoch no.0 train no.83780  loss = 4.75614 avg_loss = 4.11020\n",
      "epoch no.0 train no.83790  loss = 5.93041 avg_loss = 4.10963\n",
      "epoch no.0 train no.83800  loss = 2.34126 avg_loss = 4.11473\n",
      "epoch no.0 train no.83810  loss = 4.94791 avg_loss = 4.05562\n",
      "epoch no.0 train no.83820  loss = 5.64170 avg_loss = 4.10399\n",
      "epoch no.0 train no.83830  loss = 2.55843 avg_loss = 4.08201\n",
      "epoch no.0 train no.83840  loss = 4.90095 avg_loss = 4.03261\n",
      "epoch no.0 train no.83850  loss = 3.31514 avg_loss = 4.02499\n",
      "epoch no.0 train no.83860  loss = 3.06084 avg_loss = 4.04468\n",
      "epoch no.0 train no.83870  loss = 4.48841 avg_loss = 4.05552\n",
      "epoch no.0 train no.83880  loss = 2.56707 avg_loss = 4.10705\n",
      "epoch no.0 train no.83890  loss = 5.60599 avg_loss = 4.14513\n",
      "epoch no.0 train no.83900  loss = 4.23562 avg_loss = 4.12593\n",
      "epoch no.1 train no.83910  loss = 4.63363 avg_loss = 4.09213\n",
      "epoch no.1 train no.83920  loss = 3.27752 avg_loss = 4.04470\n",
      "epoch no.1 train no.83930  loss = 3.11872 avg_loss = 3.97144\n",
      "epoch no.1 train no.83940  loss = 2.41802 avg_loss = 3.88010\n",
      "epoch no.1 train no.83950  loss = 4.45476 avg_loss = 3.91917\n",
      "epoch no.1 train no.83960  loss = 5.74431 avg_loss = 3.90219\n",
      "epoch no.1 train no.83970  loss = 3.79507 avg_loss = 3.94827\n",
      "epoch no.1 train no.83980  loss = 2.91412 avg_loss = 3.97090\n",
      "epoch no.1 train no.83990  loss = 4.70978 avg_loss = 3.95307\n",
      "epoch no.1 train no.84000  loss = 1.83054 avg_loss = 3.94307\n",
      "5\n",
      "to_tokens: ['▁라디오', '▁좋은', '을', '▁필요할', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.1 train no.84010  loss = 2.78080 avg_loss = 3.86740\n",
      "epoch no.1 train no.84020  loss = 5.38424 avg_loss = 3.88171\n",
      "epoch no.1 train no.84030  loss = 2.85568 avg_loss = 3.83616\n",
      "epoch no.1 train no.84040  loss = 3.09667 avg_loss = 3.88280\n",
      "epoch no.1 train no.84050  loss = 4.00083 avg_loss = 3.89582\n",
      "epoch no.1 train no.84060  loss = 4.85352 avg_loss = 3.90469\n",
      "epoch no.1 train no.84070  loss = 3.29414 avg_loss = 3.88889\n",
      "epoch no.1 train no.84080  loss = 2.73810 avg_loss = 3.87864\n",
      "epoch no.1 train no.84090  loss = 6.48255 avg_loss = 3.85424\n",
      "epoch no.1 train no.84100  loss = 3.25241 avg_loss = 3.85496\n",
      "epoch no.1 train no.84110  loss = 3.76103 avg_loss = 3.79832\n",
      "epoch no.1 train no.84120  loss = 3.39984 avg_loss = 3.82842\n",
      "epoch no.1 train no.84130  loss = 3.03911 avg_loss = 3.81616\n",
      "epoch no.1 train no.84140  loss = 2.54923 avg_loss = 3.80679\n",
      "epoch no.1 train no.84150  loss = 3.89216 avg_loss = 3.77242\n",
      "epoch no.1 train no.84160  loss = 2.42728 avg_loss = 3.76569\n",
      "epoch no.1 train no.84170  loss = 3.65686 avg_loss = 3.76246\n",
      "epoch no.1 train no.84180  loss = 5.52329 avg_loss = 3.77212\n",
      "epoch no.1 train no.84190  loss = 4.86950 avg_loss = 3.78498\n",
      "epoch no.1 train no.84200  loss = 3.17568 avg_loss = 3.78759\n",
      "epoch no.1 train no.84210  loss = 2.19723 avg_loss = 3.75054\n",
      "epoch no.1 train no.84220  loss = 3.36461 avg_loss = 3.68528\n",
      "epoch no.1 train no.84230  loss = 4.41618 avg_loss = 3.70544\n",
      "epoch no.1 train no.84240  loss = 3.26268 avg_loss = 3.66188\n",
      "epoch no.1 train no.84250  loss = 3.99268 avg_loss = 3.62052\n",
      "epoch no.1 train no.84260  loss = 3.37220 avg_loss = 3.64303\n",
      "epoch no.1 train no.84270  loss = 4.86867 avg_loss = 3.63487\n",
      "epoch no.1 train no.84280  loss = 2.14842 avg_loss = 3.63324\n",
      "epoch no.1 train no.84290  loss = 1.79215 avg_loss = 3.63980\n",
      "epoch no.1 train no.84300  loss = 3.16368 avg_loss = 3.64623\n",
      "epoch no.1 train no.84310  loss = 1.90358 avg_loss = 3.66611\n",
      "epoch no.1 train no.84320  loss = 4.82490 avg_loss = 3.69522\n",
      "epoch no.1 train no.84330  loss = 5.30155 avg_loss = 3.73039\n",
      "epoch no.1 train no.84340  loss = 3.90508 avg_loss = 3.73563\n",
      "epoch no.1 train no.84350  loss = 2.44199 avg_loss = 3.73987\n",
      "epoch no.1 train no.84360  loss = 4.84785 avg_loss = 3.76477\n",
      "epoch no.1 train no.84370  loss = 5.45207 avg_loss = 3.76878\n",
      "epoch no.1 train no.84380  loss = 2.78318 avg_loss = 3.75962\n",
      "epoch no.1 train no.84390  loss = 3.36740 avg_loss = 3.75765\n",
      "epoch no.1 train no.84400  loss = 7.27943 avg_loss = 3.82912\n",
      "epoch no.1 train no.84410  loss = 2.81076 avg_loss = 3.78512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.84420  loss = 5.24881 avg_loss = 3.78304\n",
      "epoch no.1 train no.84430  loss = 2.67815 avg_loss = 3.77237\n",
      "epoch no.1 train no.84440  loss = 3.00934 avg_loss = 3.76951\n",
      "epoch no.1 train no.84450  loss = 5.01245 avg_loss = 3.75290\n",
      "epoch no.1 train no.84460  loss = 2.29752 avg_loss = 3.74777\n",
      "epoch no.1 train no.84470  loss = 4.77073 avg_loss = 3.73274\n",
      "epoch no.1 train no.84480  loss = 3.99862 avg_loss = 3.77169\n",
      "epoch no.1 train no.84490  loss = 3.45592 avg_loss = 3.72814\n",
      "epoch no.1 train no.84500  loss = 3.46866 avg_loss = 3.75047\n",
      "epoch no.1 train no.84510  loss = 3.73003 avg_loss = 3.73281\n",
      "epoch no.1 train no.84520  loss = 2.12733 avg_loss = 3.75585\n",
      "epoch no.1 train no.84530  loss = 4.89892 avg_loss = 3.78683\n",
      "epoch no.1 train no.84540  loss = 5.75243 avg_loss = 3.81526\n",
      "epoch no.1 train no.84550  loss = 2.64280 avg_loss = 3.79270\n",
      "epoch no.1 train no.84560  loss = 4.70253 avg_loss = 3.76647\n",
      "epoch no.1 train no.84570  loss = 5.14469 avg_loss = 3.77173\n",
      "epoch no.1 train no.84580  loss = 3.16314 avg_loss = 3.76982\n",
      "epoch no.1 train no.84590  loss = 2.91629 avg_loss = 3.75978\n",
      "epoch no.1 train no.84600  loss = 2.97996 avg_loss = 3.79425\n",
      "epoch no.1 train no.84610  loss = 2.78388 avg_loss = 3.82366\n",
      "epoch no.1 train no.84620  loss = 5.63405 avg_loss = 3.81542\n",
      "epoch no.1 train no.84630  loss = 3.34551 avg_loss = 3.81871\n",
      "epoch no.1 train no.84640  loss = 3.45551 avg_loss = 3.82800\n",
      "epoch no.1 train no.84650  loss = 2.32405 avg_loss = 3.75690\n",
      "epoch no.1 train no.84660  loss = 3.58177 avg_loss = 3.77741\n",
      "epoch no.1 train no.84670  loss = 4.13535 avg_loss = 3.73222\n",
      "epoch no.1 train no.84680  loss = 3.00960 avg_loss = 3.74762\n",
      "epoch no.1 train no.84690  loss = 4.83557 avg_loss = 3.74294\n",
      "epoch no.1 train no.84700  loss = 3.23395 avg_loss = 3.74793\n",
      "epoch no.1 train no.84710  loss = 4.20621 avg_loss = 3.81109\n",
      "epoch no.1 train no.84720  loss = 2.61912 avg_loss = 3.73718\n",
      "epoch no.1 train no.84730  loss = 2.55768 avg_loss = 3.70714\n",
      "epoch no.1 train no.84740  loss = 2.54410 avg_loss = 3.68401\n",
      "epoch no.1 train no.84750  loss = 4.12236 avg_loss = 3.65059\n",
      "epoch no.1 train no.84760  loss = 2.27582 avg_loss = 3.59604\n",
      "epoch no.1 train no.84770  loss = 1.40404 avg_loss = 3.55799\n",
      "epoch no.1 train no.84780  loss = 3.56916 avg_loss = 3.54997\n",
      "epoch no.1 train no.84790  loss = 2.59311 avg_loss = 3.61039\n",
      "epoch no.1 train no.84800  loss = 4.42447 avg_loss = 3.66264\n",
      "epoch no.1 train no.84810  loss = 2.57280 avg_loss = 3.66375\n",
      "epoch no.1 train no.84820  loss = 4.66180 avg_loss = 3.68013\n",
      "epoch no.1 train no.84830  loss = 4.21265 avg_loss = 3.69698\n",
      "epoch no.1 train no.84840  loss = 3.14012 avg_loss = 3.72374\n",
      "epoch no.1 train no.84850  loss = 2.94764 avg_loss = 3.68218\n",
      "epoch no.1 train no.84860  loss = 3.94196 avg_loss = 3.73648\n",
      "epoch no.1 train no.84870  loss = 1.97714 avg_loss = 3.71411\n",
      "epoch no.1 train no.84880  loss = 3.58829 avg_loss = 3.69778\n",
      "epoch no.1 train no.84890  loss = 3.91015 avg_loss = 3.66397\n",
      "epoch no.1 train no.84900  loss = 4.13836 avg_loss = 3.68271\n",
      "epoch no.1 train no.84910  loss = 3.25668 avg_loss = 3.66986\n",
      "epoch no.1 train no.84920  loss = 3.46670 avg_loss = 3.63096\n",
      "epoch no.1 train no.84930  loss = 2.75944 avg_loss = 3.61570\n",
      "epoch no.1 train no.84940  loss = 3.94504 avg_loss = 3.64152\n",
      "epoch no.1 train no.84950  loss = 3.32719 avg_loss = 3.63733\n",
      "epoch no.1 train no.84960  loss = 4.17945 avg_loss = 3.65052\n",
      "epoch no.1 train no.84970  loss = 1.77890 avg_loss = 3.54259\n",
      "epoch no.1 train no.84980  loss = 2.06631 avg_loss = 3.54211\n",
      "epoch no.1 train no.84990  loss = 2.25560 avg_loss = 3.56487\n",
      "epoch no.1 train no.85000  loss = 3.74600 avg_loss = 3.58107\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁신나는', '▁노래', '송', '</s>']\n",
      "기분전환용 신나는 팝송</s>\n",
      "epoch no.1 train no.85010  loss = 2.52580 avg_loss = 3.54691\n",
      "epoch no.1 train no.85020  loss = 5.73307 avg_loss = 3.57553\n",
      "epoch no.1 train no.85030  loss = 4.77327 avg_loss = 3.62465\n",
      "epoch no.1 train no.85040  loss = 4.06828 avg_loss = 3.62043\n",
      "epoch no.1 train no.85050  loss = 3.26263 avg_loss = 3.60540\n",
      "epoch no.1 train no.85060  loss = 2.06722 avg_loss = 3.56887\n",
      "epoch no.1 train no.85070  loss = 2.80698 avg_loss = 3.56761\n",
      "epoch no.1 train no.85080  loss = 3.63978 avg_loss = 3.54151\n",
      "epoch no.1 train no.85090  loss = 4.21811 avg_loss = 3.56069\n",
      "epoch no.1 train no.85100  loss = 3.07770 avg_loss = 3.55313\n",
      "epoch no.1 train no.85110  loss = 4.93161 avg_loss = 3.54094\n",
      "epoch no.1 train no.85120  loss = 5.11610 avg_loss = 3.57194\n",
      "epoch no.1 train no.85130  loss = 4.61114 avg_loss = 3.61494\n",
      "epoch no.1 train no.85140  loss = 3.07965 avg_loss = 3.65833\n",
      "epoch no.1 train no.85150  loss = 3.07685 avg_loss = 3.67700\n",
      "epoch no.1 train no.85160  loss = 3.30325 avg_loss = 3.68210\n",
      "epoch no.1 train no.85170  loss = 4.89917 avg_loss = 3.67668\n",
      "epoch no.1 train no.85180  loss = 1.88726 avg_loss = 3.65535\n",
      "epoch no.1 train no.85190  loss = 3.90881 avg_loss = 3.67118\n",
      "epoch no.1 train no.85200  loss = 3.49068 avg_loss = 3.75154\n",
      "epoch no.1 train no.85210  loss = 2.98041 avg_loss = 3.72504\n",
      "epoch no.1 train no.85220  loss = 3.51190 avg_loss = 3.71752\n",
      "epoch no.1 train no.85230  loss = 2.42627 avg_loss = 3.72472\n",
      "epoch no.1 train no.85240  loss = 3.11195 avg_loss = 3.70164\n",
      "epoch no.1 train no.85250  loss = 4.76513 avg_loss = 3.66304\n",
      "epoch no.1 train no.85260  loss = 4.06844 avg_loss = 3.69479\n",
      "epoch no.1 train no.85270  loss = 3.95139 avg_loss = 3.68994\n",
      "epoch no.1 train no.85280  loss = 4.00761 avg_loss = 3.70093\n",
      "epoch no.1 train no.85290  loss = 3.77980 avg_loss = 3.72707\n",
      "epoch no.1 train no.85300  loss = 3.55834 avg_loss = 3.73526\n",
      "epoch no.1 train no.85310  loss = 3.89618 avg_loss = 3.75016\n",
      "epoch no.1 train no.85320  loss = 4.76950 avg_loss = 3.77291\n",
      "epoch no.1 train no.85330  loss = 2.54675 avg_loss = 3.75967\n",
      "epoch no.1 train no.85340  loss = 3.26522 avg_loss = 3.74065\n",
      "epoch no.1 train no.85350  loss = 5.10327 avg_loss = 3.81085\n",
      "epoch no.1 train no.85360  loss = 3.15284 avg_loss = 3.76953\n",
      "epoch no.1 train no.85370  loss = 7.00693 avg_loss = 3.74820\n",
      "epoch no.1 train no.85380  loss = 1.96043 avg_loss = 3.71846\n",
      "epoch no.1 train no.85390  loss = 2.84935 avg_loss = 3.68921\n",
      "epoch no.1 train no.85400  loss = 3.09809 avg_loss = 3.67672\n",
      "epoch no.1 train no.85410  loss = 3.11741 avg_loss = 3.70342\n",
      "epoch no.1 train no.85420  loss = 2.03068 avg_loss = 3.70013\n",
      "epoch no.1 train no.85430  loss = 3.16793 avg_loss = 3.67507\n",
      "epoch no.1 train no.85440  loss = 3.20404 avg_loss = 3.72204\n",
      "epoch no.1 train no.85450  loss = 2.81838 avg_loss = 3.73939\n",
      "epoch no.1 train no.85460  loss = 3.96809 avg_loss = 3.79180\n",
      "epoch no.1 train no.85470  loss = 4.73534 avg_loss = 3.76088\n",
      "epoch no.1 train no.85480  loss = 3.25416 avg_loss = 3.72363\n",
      "epoch no.1 train no.85490  loss = 4.64258 avg_loss = 3.73015\n",
      "epoch no.1 train no.85500  loss = 4.27147 avg_loss = 3.72970\n",
      "epoch no.1 train no.85510  loss = 2.62169 avg_loss = 3.66564\n",
      "epoch no.1 train no.85520  loss = 5.76183 avg_loss = 3.70386\n",
      "epoch no.1 train no.85530  loss = 2.49459 avg_loss = 3.68276\n",
      "epoch no.1 train no.85540  loss = 4.43339 avg_loss = 3.71446\n",
      "epoch no.1 train no.85550  loss = 4.55926 avg_loss = 3.70736\n",
      "epoch no.1 train no.85560  loss = 2.15290 avg_loss = 3.68424\n",
      "epoch no.1 train no.85570  loss = 4.77935 avg_loss = 3.77245\n",
      "epoch no.1 train no.85580  loss = 2.00814 avg_loss = 3.70998\n",
      "epoch no.1 train no.85590  loss = 5.69796 avg_loss = 3.78700\n",
      "epoch no.1 train no.85600  loss = 3.12744 avg_loss = 3.78828\n",
      "epoch no.1 train no.85610  loss = 4.85636 avg_loss = 3.82155\n",
      "epoch no.1 train no.85620  loss = 3.27022 avg_loss = 3.84269\n",
      "epoch no.1 train no.85630  loss = 5.32744 avg_loss = 3.83564\n",
      "epoch no.1 train no.85640  loss = 2.47131 avg_loss = 3.78425\n",
      "epoch no.1 train no.85650  loss = 3.51642 avg_loss = 3.82455\n",
      "epoch no.1 train no.85660  loss = 3.47759 avg_loss = 3.81784\n",
      "epoch no.1 train no.85670  loss = 5.71798 avg_loss = 3.83943\n",
      "epoch no.1 train no.85680  loss = 4.17037 avg_loss = 3.83455\n",
      "epoch no.1 train no.85690  loss = 4.70957 avg_loss = 3.84379\n",
      "epoch no.1 train no.85700  loss = 1.86507 avg_loss = 3.84596\n",
      "epoch no.1 train no.85710  loss = 4.21388 avg_loss = 3.87811\n",
      "epoch no.1 train no.85720  loss = 3.21016 avg_loss = 3.83678\n",
      "epoch no.1 train no.85730  loss = 3.80282 avg_loss = 3.93683\n",
      "epoch no.1 train no.85740  loss = 2.92794 avg_loss = 3.92403\n",
      "epoch no.1 train no.85750  loss = 4.00264 avg_loss = 3.87200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.85760  loss = 2.54107 avg_loss = 3.83580\n",
      "epoch no.1 train no.85770  loss = 4.24326 avg_loss = 3.82822\n",
      "epoch no.1 train no.85780  loss = 3.29314 avg_loss = 3.80782\n",
      "epoch no.1 train no.85790  loss = 2.43553 avg_loss = 3.74517\n",
      "epoch no.1 train no.85800  loss = 4.62480 avg_loss = 3.73769\n",
      "epoch no.1 train no.85810  loss = 2.77883 avg_loss = 3.74516\n",
      "epoch no.1 train no.85820  loss = 2.75081 avg_loss = 3.71355\n",
      "epoch no.1 train no.85830  loss = 3.41026 avg_loss = 3.67419\n",
      "epoch no.1 train no.85840  loss = 2.53329 avg_loss = 3.73586\n",
      "epoch no.1 train no.85850  loss = 4.12366 avg_loss = 3.74966\n",
      "epoch no.1 train no.85860  loss = 2.60967 avg_loss = 3.72451\n",
      "epoch no.1 train no.85870  loss = 3.29965 avg_loss = 3.70146\n",
      "epoch no.1 train no.85880  loss = 2.64007 avg_loss = 3.65403\n",
      "epoch no.1 train no.85890  loss = 4.33968 avg_loss = 3.66858\n",
      "epoch no.1 train no.85900  loss = 4.48553 avg_loss = 3.59919\n",
      "epoch no.1 train no.85910  loss = 4.36237 avg_loss = 3.66556\n",
      "epoch no.1 train no.85920  loss = 2.27970 avg_loss = 3.64749\n",
      "epoch no.1 train no.85930  loss = 3.34834 avg_loss = 3.65986\n",
      "epoch no.1 train no.85940  loss = 4.07580 avg_loss = 3.61811\n",
      "epoch no.1 train no.85950  loss = 4.22613 avg_loss = 3.62264\n",
      "epoch no.1 train no.85960  loss = 2.92859 avg_loss = 3.67414\n",
      "epoch no.1 train no.85970  loss = 5.72834 avg_loss = 3.73227\n",
      "epoch no.1 train no.85980  loss = 3.76324 avg_loss = 3.73523\n",
      "epoch no.1 train no.85990  loss = 4.29536 avg_loss = 3.74453\n",
      "epoch no.1 train no.86000  loss = 4.06060 avg_loss = 3.71244\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '싶', '▁때', '</s>']\n",
      "기분전환 하고 싶을때</s>\n",
      "epoch no.1 train no.86010  loss = 3.08943 avg_loss = 3.70762\n",
      "epoch no.1 train no.86020  loss = 2.66156 avg_loss = 3.66071\n",
      "epoch no.1 train no.86030  loss = 5.83920 avg_loss = 3.66327\n",
      "epoch no.1 train no.86040  loss = 3.28663 avg_loss = 3.66234\n",
      "epoch no.1 train no.86050  loss = 3.47715 avg_loss = 3.66896\n",
      "epoch no.1 train no.86060  loss = 4.78311 avg_loss = 3.70456\n",
      "epoch no.1 train no.86070  loss = 2.77940 avg_loss = 3.72601\n",
      "epoch no.1 train no.86080  loss = 3.15455 avg_loss = 3.75988\n",
      "epoch no.1 train no.86090  loss = 2.74259 avg_loss = 3.78390\n",
      "epoch no.1 train no.86100  loss = 3.68110 avg_loss = 3.78283\n",
      "epoch no.1 train no.86110  loss = 2.68034 avg_loss = 3.78475\n",
      "epoch no.1 train no.86120  loss = 2.98928 avg_loss = 3.75574\n",
      "epoch no.1 train no.86130  loss = 2.85607 avg_loss = 3.71928\n",
      "epoch no.1 train no.86140  loss = 3.49008 avg_loss = 3.73501\n",
      "epoch no.1 train no.86150  loss = 2.35220 avg_loss = 3.68099\n",
      "epoch no.1 train no.86160  loss = 4.59494 avg_loss = 3.71443\n",
      "epoch no.1 train no.86170  loss = 4.30987 avg_loss = 3.74033\n",
      "epoch no.1 train no.86180  loss = 4.44208 avg_loss = 3.77413\n",
      "epoch no.1 train no.86190  loss = 4.04884 avg_loss = 3.75585\n",
      "epoch no.1 train no.86200  loss = 3.80058 avg_loss = 3.74788\n",
      "epoch no.1 train no.86210  loss = 3.84454 avg_loss = 3.76660\n",
      "epoch no.1 train no.86220  loss = 3.58322 avg_loss = 3.74164\n",
      "epoch no.1 train no.86230  loss = 3.06860 avg_loss = 3.68451\n",
      "epoch no.1 train no.86240  loss = 5.85308 avg_loss = 3.75727\n",
      "epoch no.1 train no.86250  loss = 4.23191 avg_loss = 3.75807\n",
      "epoch no.1 train no.86260  loss = 4.41701 avg_loss = 3.78264\n",
      "epoch no.1 train no.86270  loss = 2.88575 avg_loss = 3.75440\n",
      "epoch no.1 train no.86280  loss = 5.74281 avg_loss = 3.76632\n",
      "epoch no.1 train no.86290  loss = 4.92131 avg_loss = 3.72302\n",
      "epoch no.1 train no.86300  loss = 2.68337 avg_loss = 3.69551\n",
      "epoch no.1 train no.86310  loss = 4.21615 avg_loss = 3.73167\n",
      "epoch no.1 train no.86320  loss = 2.70844 avg_loss = 3.80446\n",
      "epoch no.1 train no.86330  loss = 3.31117 avg_loss = 3.81948\n",
      "epoch no.1 train no.86340  loss = 2.51734 avg_loss = 3.80127\n",
      "epoch no.1 train no.86350  loss = 3.90606 avg_loss = 3.80909\n",
      "epoch no.1 train no.86360  loss = 3.59266 avg_loss = 3.77986\n",
      "epoch no.1 train no.86370  loss = 3.16920 avg_loss = 3.74924\n",
      "epoch no.1 train no.86380  loss = 3.49348 avg_loss = 3.74018\n",
      "epoch no.1 train no.86390  loss = 3.05071 avg_loss = 3.70938\n",
      "epoch no.1 train no.86400  loss = 1.91416 avg_loss = 3.68202\n",
      "epoch no.1 train no.86410  loss = 2.20567 avg_loss = 3.71642\n",
      "epoch no.1 train no.86420  loss = 2.84625 avg_loss = 3.69215\n",
      "epoch no.1 train no.86430  loss = 4.66866 avg_loss = 3.65758\n",
      "epoch no.1 train no.86440  loss = 2.46119 avg_loss = 3.63388\n",
      "epoch no.1 train no.86450  loss = 2.99213 avg_loss = 3.61648\n",
      "epoch no.1 train no.86460  loss = 5.24690 avg_loss = 3.63160\n",
      "epoch no.1 train no.86470  loss = 4.66341 avg_loss = 3.59401\n",
      "epoch no.1 train no.86480  loss = 4.50312 avg_loss = 3.64559\n",
      "epoch no.1 train no.86490  loss = 3.34402 avg_loss = 3.65984\n",
      "epoch no.1 train no.86500  loss = 2.68926 avg_loss = 3.56882\n",
      "epoch no.1 train no.86510  loss = 4.18440 avg_loss = 3.59779\n",
      "epoch no.1 train no.86520  loss = 4.75382 avg_loss = 3.63021\n",
      "epoch no.1 train no.86530  loss = 4.93826 avg_loss = 3.69479\n",
      "epoch no.1 train no.86540  loss = 5.26348 avg_loss = 3.77469\n",
      "epoch no.1 train no.86550  loss = 5.24176 avg_loss = 3.80170\n",
      "epoch no.1 train no.86560  loss = 2.85827 avg_loss = 3.76533\n",
      "epoch no.1 train no.86570  loss = 4.52290 avg_loss = 3.75388\n",
      "epoch no.1 train no.86580  loss = 6.91809 avg_loss = 3.77993\n",
      "epoch no.1 train no.86590  loss = 4.15183 avg_loss = 3.73950\n",
      "epoch no.1 train no.86600  loss = 3.97756 avg_loss = 3.79529\n",
      "epoch no.1 train no.86610  loss = 3.47280 avg_loss = 3.76593\n",
      "epoch no.1 train no.86620  loss = 4.16683 avg_loss = 3.79302\n",
      "epoch no.1 train no.86630  loss = 5.68998 avg_loss = 3.80120\n",
      "epoch no.1 train no.86640  loss = 3.37474 avg_loss = 3.82232\n",
      "epoch no.1 train no.86650  loss = 4.26703 avg_loss = 3.79416\n",
      "epoch no.1 train no.86660  loss = 3.66067 avg_loss = 3.74338\n",
      "epoch no.1 train no.86670  loss = 2.86634 avg_loss = 3.72198\n",
      "epoch no.1 train no.86680  loss = 4.94879 avg_loss = 3.74125\n",
      "epoch no.1 train no.86690  loss = 3.28169 avg_loss = 3.76993\n",
      "epoch no.1 train no.86700  loss = 3.16533 avg_loss = 3.79624\n",
      "epoch no.1 train no.86710  loss = 4.67901 avg_loss = 3.77778\n",
      "epoch no.1 train no.86720  loss = 8.67990 avg_loss = 3.80387\n",
      "epoch no.1 train no.86730  loss = 3.02337 avg_loss = 3.73669\n",
      "epoch no.1 train no.86740  loss = 3.00162 avg_loss = 3.78896\n",
      "epoch no.1 train no.86750  loss = 6.28585 avg_loss = 3.79594\n",
      "epoch no.1 train no.86760  loss = 4.72493 avg_loss = 3.79062\n",
      "epoch no.1 train no.86770  loss = 2.87086 avg_loss = 3.79341\n",
      "epoch no.1 train no.86780  loss = 2.98703 avg_loss = 3.75851\n",
      "epoch no.1 train no.86790  loss = 3.66802 avg_loss = 3.72231\n",
      "epoch no.1 train no.86800  loss = 3.00812 avg_loss = 3.73140\n",
      "epoch no.1 train no.86810  loss = 5.26531 avg_loss = 3.70677\n",
      "epoch no.1 train no.86820  loss = 4.83817 avg_loss = 3.73360\n",
      "epoch no.1 train no.86830  loss = 4.92207 avg_loss = 3.75580\n",
      "epoch no.1 train no.86840  loss = 2.54183 avg_loss = 3.70753\n",
      "epoch no.1 train no.86850  loss = 3.20220 avg_loss = 3.73698\n",
      "epoch no.1 train no.86860  loss = 3.75483 avg_loss = 3.70099\n",
      "epoch no.1 train no.86870  loss = 3.73021 avg_loss = 3.68364\n",
      "epoch no.1 train no.86880  loss = 2.88984 avg_loss = 3.65876\n",
      "epoch no.1 train no.86890  loss = 4.04506 avg_loss = 3.67098\n",
      "epoch no.1 train no.86900  loss = 3.34300 avg_loss = 3.66866\n",
      "epoch no.1 train no.86910  loss = 2.46459 avg_loss = 3.66201\n",
      "epoch no.1 train no.86920  loss = 5.49004 avg_loss = 3.67726\n",
      "epoch no.1 train no.86930  loss = 3.22324 avg_loss = 3.69542\n",
      "epoch no.1 train no.86940  loss = 4.19418 avg_loss = 3.66473\n",
      "epoch no.1 train no.86950  loss = 4.49831 avg_loss = 3.65473\n",
      "epoch no.1 train no.86960  loss = 3.75597 avg_loss = 3.65379\n",
      "epoch no.1 train no.86970  loss = 2.96908 avg_loss = 3.69490\n",
      "epoch no.1 train no.86980  loss = 2.48337 avg_loss = 3.66240\n",
      "epoch no.1 train no.86990  loss = 1.59617 avg_loss = 3.63622\n",
      "epoch no.1 train no.87000  loss = 3.15101 avg_loss = 3.63681\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.87010  loss = 2.73173 avg_loss = 3.63536\n",
      "epoch no.1 train no.87020  loss = 3.53580 avg_loss = 3.62817\n",
      "epoch no.1 train no.87030  loss = 3.22467 avg_loss = 3.64409\n",
      "epoch no.1 train no.87040  loss = 3.09807 avg_loss = 3.59452\n",
      "epoch no.1 train no.87050  loss = 3.77376 avg_loss = 3.58980\n",
      "epoch no.1 train no.87060  loss = 3.41279 avg_loss = 3.59162\n",
      "epoch no.1 train no.87070  loss = 5.69703 avg_loss = 3.58418\n",
      "epoch no.1 train no.87080  loss = 2.48019 avg_loss = 3.58559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.87090  loss = 2.71417 avg_loss = 3.56762\n",
      "epoch no.1 train no.87100  loss = 4.05757 avg_loss = 3.55648\n",
      "epoch no.1 train no.87110  loss = 4.83164 avg_loss = 3.59246\n",
      "epoch no.1 train no.87120  loss = 2.48386 avg_loss = 3.54466\n",
      "epoch no.1 train no.87130  loss = 6.04738 avg_loss = 3.58317\n",
      "epoch no.1 train no.87140  loss = 4.54481 avg_loss = 3.65567\n",
      "epoch no.1 train no.87150  loss = 3.57996 avg_loss = 3.66692\n",
      "epoch no.1 train no.87160  loss = 3.87095 avg_loss = 3.70709\n",
      "epoch no.1 train no.87170  loss = 2.62013 avg_loss = 3.77628\n",
      "epoch no.1 train no.87180  loss = 2.55303 avg_loss = 3.72662\n",
      "epoch no.1 train no.87190  loss = 4.14355 avg_loss = 3.69793\n",
      "epoch no.1 train no.87200  loss = 2.72551 avg_loss = 3.65725\n",
      "epoch no.1 train no.87210  loss = 3.28706 avg_loss = 3.64063\n",
      "epoch no.1 train no.87220  loss = 4.49232 avg_loss = 3.69179\n",
      "epoch no.1 train no.87230  loss = 2.39747 avg_loss = 3.66363\n",
      "epoch no.1 train no.87240  loss = 2.95493 avg_loss = 3.62819\n",
      "epoch no.1 train no.87250  loss = 4.37037 avg_loss = 3.62941\n",
      "epoch no.1 train no.87260  loss = 4.80538 avg_loss = 3.65283\n",
      "epoch no.1 train no.87270  loss = 3.90064 avg_loss = 3.68247\n",
      "epoch no.1 train no.87280  loss = 2.48463 avg_loss = 3.63345\n",
      "epoch no.1 train no.87290  loss = 3.60151 avg_loss = 3.69287\n",
      "epoch no.1 train no.87300  loss = 4.23339 avg_loss = 3.65967\n",
      "epoch no.1 train no.87310  loss = 3.27897 avg_loss = 3.65523\n",
      "epoch no.1 train no.87320  loss = 2.84490 avg_loss = 3.68326\n",
      "epoch no.1 train no.87330  loss = 3.65279 avg_loss = 3.65448\n",
      "epoch no.1 train no.87340  loss = 4.70904 avg_loss = 3.66180\n",
      "epoch no.1 train no.87350  loss = 2.35271 avg_loss = 3.63354\n",
      "epoch no.1 train no.87360  loss = 3.22658 avg_loss = 3.60401\n",
      "epoch no.1 train no.87370  loss = 4.81308 avg_loss = 3.62948\n",
      "epoch no.1 train no.87380  loss = 2.42163 avg_loss = 3.57231\n",
      "epoch no.1 train no.87390  loss = 4.42127 avg_loss = 3.59651\n",
      "epoch no.1 train no.87400  loss = 3.99560 avg_loss = 3.67275\n",
      "epoch no.1 train no.87410  loss = 2.73397 avg_loss = 3.64352\n",
      "epoch no.1 train no.87420  loss = 5.93936 avg_loss = 3.64189\n",
      "epoch no.1 train no.87430  loss = 3.93877 avg_loss = 3.63019\n",
      "epoch no.1 train no.87440  loss = 2.95433 avg_loss = 3.63037\n",
      "epoch no.1 train no.87450  loss = 4.66823 avg_loss = 3.62304\n",
      "epoch no.1 train no.87460  loss = 3.84043 avg_loss = 3.58026\n",
      "epoch no.1 train no.87470  loss = 5.49034 avg_loss = 3.62158\n",
      "epoch no.1 train no.87480  loss = 3.09589 avg_loss = 3.61073\n",
      "epoch no.1 train no.87490  loss = 3.13231 avg_loss = 3.56407\n",
      "epoch no.1 train no.87500  loss = 5.44562 avg_loss = 3.58403\n",
      "epoch no.1 train no.87510  loss = 3.14501 avg_loss = 3.56617\n",
      "epoch no.1 train no.87520  loss = 4.87135 avg_loss = 3.58316\n",
      "epoch no.1 train no.87530  loss = 3.61272 avg_loss = 3.63605\n",
      "epoch no.1 train no.87540  loss = 3.39247 avg_loss = 3.60784\n",
      "epoch no.1 train no.87550  loss = 1.97265 avg_loss = 3.57577\n",
      "epoch no.1 train no.87560  loss = 3.38173 avg_loss = 3.60336\n",
      "epoch no.1 train no.87570  loss = 3.14877 avg_loss = 3.61894\n",
      "epoch no.1 train no.87580  loss = 5.30453 avg_loss = 3.59691\n",
      "epoch no.1 train no.87590  loss = 3.23558 avg_loss = 3.61220\n",
      "epoch no.1 train no.87600  loss = 7.24220 avg_loss = 3.66886\n",
      "epoch no.1 train no.87610  loss = 3.62494 avg_loss = 3.71974\n",
      "epoch no.1 train no.87620  loss = 4.56620 avg_loss = 3.73751\n",
      "epoch no.1 train no.87630  loss = 4.59974 avg_loss = 3.74807\n",
      "epoch no.1 train no.87640  loss = 2.34621 avg_loss = 3.74052\n",
      "epoch no.1 train no.87650  loss = 2.44406 avg_loss = 3.74825\n",
      "epoch no.1 train no.87660  loss = 2.99331 avg_loss = 3.72708\n",
      "epoch no.1 train no.87670  loss = 4.64056 avg_loss = 3.74740\n",
      "epoch no.1 train no.87680  loss = 3.99450 avg_loss = 3.71851\n",
      "epoch no.1 train no.87690  loss = 5.38393 avg_loss = 3.80376\n",
      "epoch no.1 train no.87700  loss = 5.89869 avg_loss = 3.81732\n",
      "epoch no.1 train no.87710  loss = 3.81930 avg_loss = 3.82138\n",
      "epoch no.1 train no.87720  loss = 5.12747 avg_loss = 3.81441\n",
      "epoch no.1 train no.87730  loss = 3.29436 avg_loss = 3.79505\n",
      "epoch no.1 train no.87740  loss = 3.71450 avg_loss = 3.73776\n",
      "epoch no.1 train no.87750  loss = 4.65617 avg_loss = 3.72634\n",
      "epoch no.1 train no.87760  loss = 4.13801 avg_loss = 3.74640\n",
      "epoch no.1 train no.87770  loss = 4.93591 avg_loss = 3.78941\n",
      "epoch no.1 train no.87780  loss = 4.91997 avg_loss = 3.79001\n",
      "epoch no.1 train no.87790  loss = 2.53517 avg_loss = 3.76406\n",
      "epoch no.1 train no.87800  loss = 2.82923 avg_loss = 3.76004\n",
      "epoch no.1 train no.87810  loss = 4.89585 avg_loss = 3.70708\n",
      "epoch no.1 train no.87820  loss = 4.31792 avg_loss = 3.71419\n",
      "epoch no.1 train no.87830  loss = 1.42543 avg_loss = 3.65429\n",
      "epoch no.1 train no.87840  loss = 3.33646 avg_loss = 3.64487\n",
      "epoch no.1 train no.87850  loss = 2.97435 avg_loss = 3.64269\n",
      "epoch no.1 train no.87860  loss = 6.59868 avg_loss = 3.62471\n",
      "epoch no.1 train no.87870  loss = 1.71271 avg_loss = 3.63034\n",
      "epoch no.1 train no.87880  loss = 5.39435 avg_loss = 3.65544\n",
      "epoch no.1 train no.87890  loss = 4.10568 avg_loss = 3.69322\n",
      "epoch no.1 train no.87900  loss = 2.72971 avg_loss = 3.68836\n",
      "epoch no.1 train no.87910  loss = 3.66348 avg_loss = 3.68022\n",
      "epoch no.1 train no.87920  loss = 1.97239 avg_loss = 3.67205\n",
      "epoch no.1 train no.87930  loss = 5.14618 avg_loss = 3.66509\n",
      "epoch no.1 train no.87940  loss = 2.91986 avg_loss = 3.68457\n",
      "epoch no.1 train no.87950  loss = 3.71229 avg_loss = 3.70247\n",
      "epoch no.1 train no.87960  loss = 3.40237 avg_loss = 3.72719\n",
      "epoch no.1 train no.87970  loss = 2.68091 avg_loss = 3.74780\n",
      "epoch no.1 train no.87980  loss = 4.23705 avg_loss = 3.71308\n",
      "epoch no.1 train no.87990  loss = 3.18023 avg_loss = 3.68939\n",
      "epoch no.1 train no.88000  loss = 3.97453 avg_loss = 3.67385\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋아', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.88010  loss = 2.43054 avg_loss = 3.64293\n",
      "epoch no.1 train no.88020  loss = 5.39104 avg_loss = 3.64157\n",
      "epoch no.1 train no.88030  loss = 4.08235 avg_loss = 3.64224\n",
      "epoch no.1 train no.88040  loss = 3.73503 avg_loss = 3.63322\n",
      "epoch no.1 train no.88050  loss = 3.27481 avg_loss = 3.63412\n",
      "epoch no.1 train no.88060  loss = 2.56410 avg_loss = 3.60672\n",
      "epoch no.1 train no.88070  loss = 4.29168 avg_loss = 3.67058\n",
      "epoch no.1 train no.88080  loss = 5.37884 avg_loss = 3.71414\n",
      "epoch no.1 train no.88090  loss = 4.62939 avg_loss = 3.76046\n",
      "epoch no.1 train no.88100  loss = 3.00827 avg_loss = 3.75441\n",
      "epoch no.1 train no.88110  loss = 6.01553 avg_loss = 3.79262\n",
      "epoch no.1 train no.88120  loss = 5.89011 avg_loss = 3.80021\n",
      "epoch no.1 train no.88130  loss = 2.13502 avg_loss = 3.74590\n",
      "epoch no.1 train no.88140  loss = 1.68571 avg_loss = 3.70252\n",
      "epoch no.1 train no.88150  loss = 3.92295 avg_loss = 3.62716\n",
      "epoch no.1 train no.88160  loss = 2.51043 avg_loss = 3.60232\n",
      "epoch no.1 train no.88170  loss = 4.05338 avg_loss = 3.60875\n",
      "epoch no.1 train no.88180  loss = 2.20778 avg_loss = 3.61076\n",
      "epoch no.1 train no.88190  loss = 2.44331 avg_loss = 3.60256\n",
      "epoch no.1 train no.88200  loss = 4.76900 avg_loss = 3.65029\n",
      "epoch no.1 train no.88210  loss = 2.95973 avg_loss = 3.73254\n",
      "epoch no.1 train no.88220  loss = 2.63534 avg_loss = 3.75344\n",
      "epoch no.1 train no.88230  loss = 3.32711 avg_loss = 3.67825\n",
      "epoch no.1 train no.88240  loss = 4.88315 avg_loss = 3.70227\n",
      "epoch no.1 train no.88250  loss = 4.33665 avg_loss = 3.71624\n",
      "epoch no.1 train no.88260  loss = 5.51045 avg_loss = 3.74189\n",
      "epoch no.1 train no.88270  loss = 3.74590 avg_loss = 3.71553\n",
      "epoch no.1 train no.88280  loss = 5.42182 avg_loss = 3.74476\n",
      "epoch no.1 train no.88290  loss = 3.77851 avg_loss = 3.71421\n",
      "epoch no.1 train no.88300  loss = 1.97671 avg_loss = 3.68827\n",
      "epoch no.1 train no.88310  loss = 3.36837 avg_loss = 3.64264\n",
      "epoch no.1 train no.88320  loss = 2.79112 avg_loss = 3.68280\n",
      "epoch no.1 train no.88330  loss = 3.10753 avg_loss = 3.69025\n",
      "epoch no.1 train no.88340  loss = 3.46601 avg_loss = 3.66787\n",
      "epoch no.1 train no.88350  loss = 3.68790 avg_loss = 3.71660\n",
      "epoch no.1 train no.88360  loss = 6.04870 avg_loss = 3.72399\n",
      "epoch no.1 train no.88370  loss = 2.95099 avg_loss = 3.70082\n",
      "epoch no.1 train no.88380  loss = 3.76888 avg_loss = 3.72401\n",
      "epoch no.1 train no.88390  loss = 5.43955 avg_loss = 3.71244\n",
      "epoch no.1 train no.88400  loss = 3.43290 avg_loss = 3.71749\n",
      "epoch no.1 train no.88410  loss = 5.02834 avg_loss = 3.77593\n",
      "epoch no.1 train no.88420  loss = 2.87657 avg_loss = 3.79077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.88430  loss = 5.32574 avg_loss = 3.77535\n",
      "epoch no.1 train no.88440  loss = 5.14506 avg_loss = 3.78410\n",
      "epoch no.1 train no.88450  loss = 6.19072 avg_loss = 3.75034\n",
      "epoch no.1 train no.88460  loss = 2.94802 avg_loss = 3.80922\n",
      "epoch no.1 train no.88470  loss = 4.07306 avg_loss = 3.79071\n",
      "epoch no.1 train no.88480  loss = 3.01972 avg_loss = 3.74953\n",
      "epoch no.1 train no.88490  loss = 2.95196 avg_loss = 3.78198\n",
      "epoch no.1 train no.88500  loss = 4.47323 avg_loss = 3.76744\n",
      "epoch no.1 train no.88510  loss = 3.92534 avg_loss = 3.76495\n",
      "epoch no.1 train no.88520  loss = 5.47126 avg_loss = 3.79433\n",
      "epoch no.1 train no.88530  loss = 4.30477 avg_loss = 3.82918\n",
      "epoch no.1 train no.88540  loss = 1.87354 avg_loss = 3.74803\n",
      "epoch no.1 train no.88550  loss = 3.12112 avg_loss = 3.76202\n",
      "epoch no.1 train no.88560  loss = 5.61339 avg_loss = 3.75123\n",
      "epoch no.1 train no.88570  loss = 4.84012 avg_loss = 3.73249\n",
      "epoch no.1 train no.88580  loss = 2.36814 avg_loss = 3.74150\n",
      "epoch no.1 train no.88590  loss = 2.01675 avg_loss = 3.66478\n",
      "epoch no.1 train no.88600  loss = 2.71790 avg_loss = 3.63100\n",
      "epoch no.1 train no.88610  loss = 3.21800 avg_loss = 3.67287\n",
      "epoch no.1 train no.88620  loss = 6.30059 avg_loss = 3.71828\n",
      "epoch no.1 train no.88630  loss = 2.25432 avg_loss = 3.67367\n",
      "epoch no.1 train no.88640  loss = 2.56871 avg_loss = 3.70631\n",
      "epoch no.1 train no.88650  loss = 4.21501 avg_loss = 3.72657\n",
      "epoch no.1 train no.88660  loss = 3.88010 avg_loss = 3.71835\n",
      "epoch no.1 train no.88670  loss = 4.18787 avg_loss = 3.71043\n",
      "epoch no.1 train no.88680  loss = 2.38141 avg_loss = 3.71847\n",
      "epoch no.1 train no.88690  loss = 5.29167 avg_loss = 3.72446\n",
      "epoch no.1 train no.88700  loss = 2.57809 avg_loss = 3.71475\n",
      "epoch no.1 train no.88710  loss = 3.84648 avg_loss = 3.71528\n",
      "epoch no.1 train no.88720  loss = 2.09648 avg_loss = 3.69982\n",
      "epoch no.1 train no.88730  loss = 4.66071 avg_loss = 3.62257\n",
      "epoch no.1 train no.88740  loss = 4.24301 avg_loss = 3.58707\n",
      "epoch no.1 train no.88750  loss = 3.92810 avg_loss = 3.58656\n",
      "epoch no.1 train no.88760  loss = 2.78604 avg_loss = 3.56596\n",
      "epoch no.1 train no.88770  loss = 4.21446 avg_loss = 3.59408\n",
      "epoch no.1 train no.88780  loss = 2.83711 avg_loss = 3.57831\n",
      "epoch no.1 train no.88790  loss = 3.23824 avg_loss = 3.54242\n",
      "epoch no.1 train no.88800  loss = 2.99831 avg_loss = 3.52762\n",
      "epoch no.1 train no.88810  loss = 3.69856 avg_loss = 3.52109\n",
      "epoch no.1 train no.88820  loss = 4.10698 avg_loss = 3.50956\n",
      "epoch no.1 train no.88830  loss = 5.09143 avg_loss = 3.55892\n",
      "epoch no.1 train no.88840  loss = 4.14258 avg_loss = 3.57745\n",
      "epoch no.1 train no.88850  loss = 3.01167 avg_loss = 3.62489\n",
      "epoch no.1 train no.88860  loss = 4.34981 avg_loss = 3.66841\n",
      "epoch no.1 train no.88870  loss = 2.19032 avg_loss = 3.70000\n",
      "epoch no.1 train no.88880  loss = 3.14252 avg_loss = 3.65294\n",
      "epoch no.1 train no.88890  loss = 5.41957 avg_loss = 3.68352\n",
      "epoch no.1 train no.88900  loss = 3.30068 avg_loss = 3.70713\n",
      "epoch no.1 train no.88910  loss = 3.57662 avg_loss = 3.70276\n",
      "epoch no.1 train no.88920  loss = 5.71331 avg_loss = 3.71547\n",
      "epoch no.1 train no.88930  loss = 2.69496 avg_loss = 3.71520\n",
      "epoch no.1 train no.88940  loss = 4.04493 avg_loss = 3.72747\n",
      "epoch no.1 train no.88950  loss = 5.51258 avg_loss = 3.73610\n",
      "epoch no.1 train no.88960  loss = 4.01358 avg_loss = 3.75779\n",
      "epoch no.1 train no.88970  loss = 3.45800 avg_loss = 3.74931\n",
      "epoch no.1 train no.88980  loss = 3.99046 avg_loss = 3.74891\n",
      "epoch no.1 train no.88990  loss = 4.25671 avg_loss = 3.75989\n",
      "epoch no.1 train no.89000  loss = 4.85863 avg_loss = 3.77669\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.89010  loss = 3.89234 avg_loss = 3.79197\n",
      "epoch no.1 train no.89020  loss = 4.22726 avg_loss = 3.79433\n",
      "epoch no.1 train no.89030  loss = 2.95756 avg_loss = 3.81894\n",
      "epoch no.1 train no.89040  loss = 2.42961 avg_loss = 3.78321\n",
      "epoch no.1 train no.89050  loss = 4.05459 avg_loss = 3.81262\n",
      "epoch no.1 train no.89060  loss = 2.99248 avg_loss = 3.78473\n",
      "epoch no.1 train no.89070  loss = 5.50069 avg_loss = 3.79380\n",
      "epoch no.1 train no.89080  loss = 4.04652 avg_loss = 3.78776\n",
      "epoch no.1 train no.89090  loss = 3.02341 avg_loss = 3.77628\n",
      "epoch no.1 train no.89100  loss = 2.27903 avg_loss = 3.74985\n",
      "epoch no.1 train no.89110  loss = 2.92304 avg_loss = 3.74458\n",
      "epoch no.1 train no.89120  loss = 3.17360 avg_loss = 3.74695\n",
      "epoch no.1 train no.89130  loss = 3.62547 avg_loss = 3.74626\n",
      "epoch no.1 train no.89140  loss = 3.37341 avg_loss = 3.77533\n",
      "epoch no.1 train no.89150  loss = 2.41160 avg_loss = 3.78862\n",
      "epoch no.1 train no.89160  loss = 2.67828 avg_loss = 3.81515\n",
      "epoch no.1 train no.89170  loss = 4.95430 avg_loss = 3.81878\n",
      "epoch no.1 train no.89180  loss = 3.42263 avg_loss = 3.85963\n",
      "epoch no.1 train no.89190  loss = 4.65766 avg_loss = 3.80540\n",
      "epoch no.1 train no.89200  loss = 6.54255 avg_loss = 3.82275\n",
      "epoch no.1 train no.89210  loss = 1.85387 avg_loss = 3.77029\n",
      "epoch no.1 train no.89220  loss = 3.65548 avg_loss = 3.74287\n",
      "epoch no.1 train no.89230  loss = 3.73237 avg_loss = 3.71925\n",
      "epoch no.1 train no.89240  loss = 6.52762 avg_loss = 3.73448\n",
      "epoch no.1 train no.89250  loss = 3.05613 avg_loss = 3.69245\n",
      "epoch no.1 train no.89260  loss = 4.30472 avg_loss = 3.72681\n",
      "epoch no.1 train no.89270  loss = 3.04776 avg_loss = 3.69567\n",
      "epoch no.1 train no.89280  loss = 4.42773 avg_loss = 3.66496\n",
      "epoch no.1 train no.89290  loss = 3.96932 avg_loss = 3.67503\n",
      "epoch no.1 train no.89300  loss = 4.16256 avg_loss = 3.66571\n",
      "epoch no.1 train no.89310  loss = 4.47728 avg_loss = 3.64315\n",
      "epoch no.1 train no.89320  loss = 3.20836 avg_loss = 3.66196\n",
      "epoch no.1 train no.89330  loss = 2.46155 avg_loss = 3.68401\n",
      "epoch no.1 train no.89340  loss = 4.88926 avg_loss = 3.68157\n",
      "epoch no.1 train no.89350  loss = 3.57396 avg_loss = 3.66240\n",
      "epoch no.1 train no.89360  loss = 2.67625 avg_loss = 3.70315\n",
      "epoch no.1 train no.89370  loss = 3.38842 avg_loss = 3.68877\n",
      "epoch no.1 train no.89380  loss = 2.84840 avg_loss = 3.67750\n",
      "epoch no.1 train no.89390  loss = 3.26920 avg_loss = 3.70326\n",
      "epoch no.1 train no.89400  loss = 4.55156 avg_loss = 3.77670\n",
      "epoch no.1 train no.89410  loss = 3.22763 avg_loss = 3.77149\n",
      "epoch no.1 train no.89420  loss = 3.06089 avg_loss = 3.75926\n",
      "epoch no.1 train no.89430  loss = 3.52181 avg_loss = 3.76963\n",
      "epoch no.1 train no.89440  loss = 4.62616 avg_loss = 3.75415\n",
      "epoch no.1 train no.89450  loss = 3.69753 avg_loss = 3.67191\n",
      "epoch no.1 train no.89460  loss = 3.31948 avg_loss = 3.69495\n",
      "epoch no.1 train no.89470  loss = 2.27344 avg_loss = 3.69938\n",
      "epoch no.1 train no.89480  loss = 2.72393 avg_loss = 3.75354\n",
      "epoch no.1 train no.89490  loss = 2.00283 avg_loss = 3.74425\n",
      "epoch no.1 train no.89500  loss = 3.29320 avg_loss = 3.75516\n",
      "epoch no.1 train no.89510  loss = 2.35389 avg_loss = 3.71161\n",
      "epoch no.1 train no.89520  loss = 5.39459 avg_loss = 3.71053\n",
      "epoch no.1 train no.89530  loss = 4.30964 avg_loss = 3.71209\n",
      "epoch no.1 train no.89540  loss = 2.35504 avg_loss = 3.67570\n",
      "epoch no.1 train no.89550  loss = 4.35868 avg_loss = 3.67907\n",
      "epoch no.1 train no.89560  loss = 3.29634 avg_loss = 3.74273\n",
      "epoch no.1 train no.89570  loss = 2.44047 avg_loss = 3.74208\n",
      "epoch no.1 train no.89580  loss = 4.71206 avg_loss = 3.79688\n",
      "epoch no.1 train no.89590  loss = 5.10130 avg_loss = 3.86112\n",
      "epoch no.1 train no.89600  loss = 3.16921 avg_loss = 3.81716\n",
      "epoch no.1 train no.89610  loss = 2.91987 avg_loss = 3.77116\n",
      "epoch no.1 train no.89620  loss = 2.23867 avg_loss = 3.76222\n",
      "epoch no.1 train no.89630  loss = 3.98944 avg_loss = 3.77919\n",
      "epoch no.1 train no.89640  loss = 2.70054 avg_loss = 3.75906\n",
      "epoch no.1 train no.89650  loss = 2.22893 avg_loss = 3.73740\n",
      "epoch no.1 train no.89660  loss = 4.62649 avg_loss = 3.75269\n",
      "epoch no.1 train no.89670  loss = 3.09522 avg_loss = 3.75036\n",
      "epoch no.1 train no.89680  loss = 2.50847 avg_loss = 3.76367\n",
      "epoch no.1 train no.89690  loss = 2.98581 avg_loss = 3.71767\n",
      "epoch no.1 train no.89700  loss = 3.79470 avg_loss = 3.76354\n",
      "epoch no.1 train no.89710  loss = 3.70313 avg_loss = 3.80912\n",
      "epoch no.1 train no.89720  loss = 3.87999 avg_loss = 3.83863\n",
      "epoch no.1 train no.89730  loss = 5.50648 avg_loss = 3.79032\n",
      "epoch no.1 train no.89740  loss = 1.97140 avg_loss = 3.77584\n",
      "epoch no.1 train no.89750  loss = 5.99886 avg_loss = 3.77795\n",
      "epoch no.1 train no.89760  loss = 2.30799 avg_loss = 3.75242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.89770  loss = 2.76093 avg_loss = 3.77675\n",
      "epoch no.1 train no.89780  loss = 3.15394 avg_loss = 3.72625\n",
      "epoch no.1 train no.89790  loss = 2.29706 avg_loss = 3.67927\n",
      "epoch no.1 train no.89800  loss = 4.36551 avg_loss = 3.68753\n",
      "epoch no.1 train no.89810  loss = 5.89749 avg_loss = 3.67870\n",
      "epoch no.1 train no.89820  loss = 2.24288 avg_loss = 3.68822\n",
      "epoch no.1 train no.89830  loss = 2.49431 avg_loss = 3.63624\n",
      "epoch no.1 train no.89840  loss = 2.39907 avg_loss = 3.63489\n",
      "epoch no.1 train no.89850  loss = 2.72259 avg_loss = 3.62533\n",
      "epoch no.1 train no.89860  loss = 1.70878 avg_loss = 3.62423\n",
      "epoch no.1 train no.89870  loss = 3.59168 avg_loss = 3.64880\n",
      "epoch no.1 train no.89880  loss = 4.24167 avg_loss = 3.65944\n",
      "epoch no.1 train no.89890  loss = 4.16164 avg_loss = 3.63685\n",
      "epoch no.1 train no.89900  loss = 3.03269 avg_loss = 3.69150\n",
      "epoch no.1 train no.89910  loss = 5.91651 avg_loss = 3.70642\n",
      "epoch no.1 train no.89920  loss = 3.63094 avg_loss = 3.72971\n",
      "epoch no.1 train no.89930  loss = 2.50608 avg_loss = 3.69480\n",
      "epoch no.1 train no.89940  loss = 2.27831 avg_loss = 3.63839\n",
      "epoch no.1 train no.89950  loss = 3.28371 avg_loss = 3.66272\n",
      "epoch no.1 train no.89960  loss = 2.43854 avg_loss = 3.63200\n",
      "epoch no.1 train no.89970  loss = 1.88436 avg_loss = 3.61955\n",
      "epoch no.1 train no.89980  loss = 3.69301 avg_loss = 3.64943\n",
      "epoch no.1 train no.89990  loss = 3.77062 avg_loss = 3.61050\n",
      "epoch no.1 train no.90000  loss = 2.52054 avg_loss = 3.61161\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.90010  loss = 3.94092 avg_loss = 3.56808\n",
      "epoch no.1 train no.90020  loss = 2.91840 avg_loss = 3.60632\n",
      "epoch no.1 train no.90030  loss = 3.51428 avg_loss = 3.65064\n",
      "epoch no.1 train no.90040  loss = 2.15614 avg_loss = 3.63748\n",
      "epoch no.1 train no.90050  loss = 3.56841 avg_loss = 3.65289\n",
      "epoch no.1 train no.90060  loss = 3.30213 avg_loss = 3.64454\n",
      "epoch no.1 train no.90070  loss = 2.21804 avg_loss = 3.58995\n",
      "epoch no.1 train no.90080  loss = 4.24747 avg_loss = 3.63420\n",
      "epoch no.1 train no.90090  loss = 3.55069 avg_loss = 3.67179\n",
      "epoch no.1 train no.90100  loss = 4.07834 avg_loss = 3.67070\n",
      "epoch no.1 train no.90110  loss = 2.00434 avg_loss = 3.67020\n",
      "epoch no.1 train no.90120  loss = 2.58202 avg_loss = 3.70036\n",
      "epoch no.1 train no.90130  loss = 3.14277 avg_loss = 3.65699\n",
      "epoch no.1 train no.90140  loss = 6.46975 avg_loss = 3.61459\n",
      "epoch no.1 train no.90150  loss = 3.11274 avg_loss = 3.63737\n",
      "epoch no.1 train no.90160  loss = 2.84937 avg_loss = 3.60022\n",
      "epoch no.1 train no.90170  loss = 6.30289 avg_loss = 3.59241\n",
      "epoch no.1 train no.90180  loss = 2.79885 avg_loss = 3.61560\n",
      "epoch no.1 train no.90190  loss = 3.83752 avg_loss = 3.68665\n",
      "epoch no.1 train no.90200  loss = 3.17157 avg_loss = 3.64763\n",
      "epoch no.1 train no.90210  loss = 5.02139 avg_loss = 3.67615\n",
      "epoch no.1 train no.90220  loss = 3.20068 avg_loss = 3.67869\n",
      "epoch no.1 train no.90230  loss = 4.08438 avg_loss = 3.69036\n",
      "epoch no.1 train no.90240  loss = 3.99011 avg_loss = 3.78378\n",
      "epoch no.1 train no.90250  loss = 2.77784 avg_loss = 3.70749\n",
      "epoch no.1 train no.90260  loss = 3.71873 avg_loss = 3.68379\n",
      "epoch no.1 train no.90270  loss = 2.42805 avg_loss = 3.66243\n",
      "epoch no.1 train no.90280  loss = 3.38112 avg_loss = 3.64251\n",
      "epoch no.1 train no.90290  loss = 2.71141 avg_loss = 3.69685\n",
      "epoch no.1 train no.90300  loss = 4.00789 avg_loss = 3.67712\n",
      "epoch no.1 train no.90310  loss = 4.08978 avg_loss = 3.69071\n",
      "epoch no.1 train no.90320  loss = 3.60258 avg_loss = 3.67596\n",
      "epoch no.1 train no.90330  loss = 2.02967 avg_loss = 3.71622\n",
      "epoch no.1 train no.90340  loss = 4.06587 avg_loss = 3.69227\n",
      "epoch no.1 train no.90350  loss = 2.47888 avg_loss = 3.69917\n",
      "epoch no.1 train no.90360  loss = 2.13230 avg_loss = 3.71983\n",
      "epoch no.1 train no.90370  loss = 4.88380 avg_loss = 3.70848\n",
      "epoch no.1 train no.90380  loss = 2.54893 avg_loss = 3.71480\n",
      "epoch no.1 train no.90390  loss = 2.53050 avg_loss = 3.67946\n",
      "epoch no.1 train no.90400  loss = 4.62195 avg_loss = 3.69158\n",
      "epoch no.1 train no.90410  loss = 2.74523 avg_loss = 3.68682\n",
      "epoch no.1 train no.90420  loss = 3.06976 avg_loss = 3.72605\n",
      "epoch no.1 train no.90430  loss = 5.13012 avg_loss = 3.68853\n",
      "epoch no.1 train no.90440  loss = 3.09239 avg_loss = 3.66538\n",
      "epoch no.1 train no.90450  loss = 7.03957 avg_loss = 3.68947\n",
      "epoch no.1 train no.90460  loss = 4.76733 avg_loss = 3.67808\n",
      "epoch no.1 train no.90470  loss = 4.63275 avg_loss = 3.68813\n",
      "epoch no.1 train no.90480  loss = 4.00690 avg_loss = 3.69996\n",
      "epoch no.1 train no.90490  loss = 5.14944 avg_loss = 3.67428\n",
      "epoch no.1 train no.90500  loss = 3.55194 avg_loss = 3.71548\n",
      "epoch no.1 train no.90510  loss = 3.35504 avg_loss = 3.73112\n",
      "epoch no.1 train no.90520  loss = 2.87844 avg_loss = 3.70903\n",
      "epoch no.1 train no.90530  loss = 4.24665 avg_loss = 3.70460\n",
      "epoch no.1 train no.90540  loss = 4.25401 avg_loss = 3.70585\n",
      "epoch no.1 train no.90550  loss = 2.92334 avg_loss = 3.67915\n",
      "epoch no.1 train no.90560  loss = 4.23138 avg_loss = 3.64673\n",
      "epoch no.1 train no.90570  loss = 2.84648 avg_loss = 3.65770\n",
      "epoch no.1 train no.90580  loss = 6.74480 avg_loss = 3.71737\n",
      "epoch no.1 train no.90590  loss = 2.17349 avg_loss = 3.71097\n",
      "epoch no.1 train no.90600  loss = 3.64808 avg_loss = 3.72671\n",
      "epoch no.1 train no.90610  loss = 4.06948 avg_loss = 3.71018\n",
      "epoch no.1 train no.90620  loss = 2.99534 avg_loss = 3.63089\n",
      "epoch no.1 train no.90630  loss = 3.34018 avg_loss = 3.65277\n",
      "epoch no.1 train no.90640  loss = 4.50899 avg_loss = 3.62465\n",
      "epoch no.1 train no.90650  loss = 2.66793 avg_loss = 3.58941\n",
      "epoch no.1 train no.90660  loss = 3.53995 avg_loss = 3.56845\n",
      "epoch no.1 train no.90670  loss = 3.00298 avg_loss = 3.59461\n",
      "epoch no.1 train no.90680  loss = 3.54152 avg_loss = 3.59193\n",
      "epoch no.1 train no.90690  loss = 4.92981 avg_loss = 3.61666\n",
      "epoch no.1 train no.90700  loss = 3.48829 avg_loss = 3.58862\n",
      "epoch no.1 train no.90710  loss = 2.36905 avg_loss = 3.59201\n",
      "epoch no.1 train no.90720  loss = 2.94747 avg_loss = 3.58753\n",
      "epoch no.1 train no.90730  loss = 3.94491 avg_loss = 3.57511\n",
      "epoch no.1 train no.90740  loss = 3.63391 avg_loss = 3.59256\n",
      "epoch no.1 train no.90750  loss = 5.86470 avg_loss = 3.63689\n",
      "epoch no.1 train no.90760  loss = 2.75420 avg_loss = 3.66903\n",
      "epoch no.1 train no.90770  loss = 3.88343 avg_loss = 3.64544\n",
      "epoch no.1 train no.90780  loss = 4.56435 avg_loss = 3.67897\n",
      "epoch no.1 train no.90790  loss = 1.96576 avg_loss = 3.66058\n",
      "epoch no.1 train no.90800  loss = 4.37247 avg_loss = 3.68085\n",
      "epoch no.1 train no.90810  loss = 3.41418 avg_loss = 3.69122\n",
      "epoch no.1 train no.90820  loss = 3.05528 avg_loss = 3.68086\n",
      "epoch no.1 train no.90830  loss = 4.49786 avg_loss = 3.67332\n",
      "epoch no.1 train no.90840  loss = 3.33724 avg_loss = 3.66572\n",
      "epoch no.1 train no.90850  loss = 2.99652 avg_loss = 3.69558\n",
      "epoch no.1 train no.90860  loss = 5.78183 avg_loss = 3.67502\n",
      "epoch no.1 train no.90870  loss = 4.10571 avg_loss = 3.73097\n",
      "epoch no.1 train no.90880  loss = 3.03526 avg_loss = 3.72038\n",
      "epoch no.1 train no.90890  loss = 2.50734 avg_loss = 3.74837\n",
      "epoch no.1 train no.90900  loss = 3.11349 avg_loss = 3.74074\n",
      "epoch no.1 train no.90910  loss = 3.96085 avg_loss = 3.68989\n",
      "epoch no.1 train no.90920  loss = 4.70482 avg_loss = 3.70846\n",
      "epoch no.1 train no.90930  loss = 2.55288 avg_loss = 3.72761\n",
      "epoch no.1 train no.90940  loss = 2.85750 avg_loss = 3.69825\n",
      "epoch no.1 train no.90950  loss = 4.02350 avg_loss = 3.69376\n",
      "epoch no.1 train no.90960  loss = 4.87808 avg_loss = 3.72963\n",
      "epoch no.1 train no.90970  loss = 6.41505 avg_loss = 3.76754\n",
      "epoch no.1 train no.90980  loss = 4.35708 avg_loss = 3.76764\n",
      "epoch no.1 train no.90990  loss = 3.35702 avg_loss = 3.74953\n",
      "epoch no.1 train no.91000  loss = 2.71971 avg_loss = 3.70360\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '▁때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.1 train no.91010  loss = 5.87275 avg_loss = 3.67586\n",
      "epoch no.1 train no.91020  loss = 3.61982 avg_loss = 3.69939\n",
      "epoch no.1 train no.91030  loss = 3.68594 avg_loss = 3.72377\n",
      "epoch no.1 train no.91040  loss = 5.00330 avg_loss = 3.76815\n",
      "epoch no.1 train no.91050  loss = 2.99205 avg_loss = 3.76193\n",
      "epoch no.1 train no.91060  loss = 2.86388 avg_loss = 3.77257\n",
      "epoch no.1 train no.91070  loss = 5.75542 avg_loss = 3.74128\n",
      "epoch no.1 train no.91080  loss = 3.37311 avg_loss = 3.72907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.91090  loss = 4.90249 avg_loss = 3.71875\n",
      "epoch no.1 train no.91100  loss = 4.38436 avg_loss = 3.73808\n",
      "epoch no.1 train no.91110  loss = 5.08258 avg_loss = 3.78507\n",
      "epoch no.1 train no.91120  loss = 3.49860 avg_loss = 3.81165\n",
      "epoch no.1 train no.91130  loss = 5.26065 avg_loss = 3.87422\n",
      "epoch no.1 train no.91140  loss = 2.18643 avg_loss = 3.84975\n",
      "epoch no.1 train no.91150  loss = 3.38779 avg_loss = 3.82884\n",
      "epoch no.1 train no.91160  loss = 2.56794 avg_loss = 3.75695\n",
      "epoch no.1 train no.91170  loss = 1.72997 avg_loss = 3.70242\n",
      "epoch no.1 train no.91180  loss = 3.77339 avg_loss = 3.69506\n",
      "epoch no.1 train no.91190  loss = 2.53946 avg_loss = 3.66215\n",
      "epoch no.1 train no.91200  loss = 2.50376 avg_loss = 3.62490\n",
      "epoch no.1 train no.91210  loss = 2.90220 avg_loss = 3.67127\n",
      "epoch no.1 train no.91220  loss = 3.03817 avg_loss = 3.69521\n",
      "epoch no.1 train no.91230  loss = 5.04747 avg_loss = 3.70881\n",
      "epoch no.1 train no.91240  loss = 1.90026 avg_loss = 3.72294\n",
      "epoch no.1 train no.91250  loss = 2.65926 avg_loss = 3.74636\n",
      "epoch no.1 train no.91260  loss = 5.00811 avg_loss = 3.73497\n",
      "epoch no.1 train no.91270  loss = 3.64633 avg_loss = 3.76533\n",
      "epoch no.1 train no.91280  loss = 3.57503 avg_loss = 3.74797\n",
      "epoch no.1 train no.91290  loss = 4.63152 avg_loss = 3.76568\n",
      "epoch no.1 train no.91300  loss = 4.01603 avg_loss = 3.76999\n",
      "epoch no.1 train no.91310  loss = 2.89707 avg_loss = 3.76876\n",
      "epoch no.1 train no.91320  loss = 3.96354 avg_loss = 3.74510\n",
      "epoch no.1 train no.91330  loss = 4.81482 avg_loss = 3.75349\n",
      "epoch no.1 train no.91340  loss = 5.39471 avg_loss = 3.75533\n",
      "epoch no.1 train no.91350  loss = 5.18853 avg_loss = 3.75466\n",
      "epoch no.1 train no.91360  loss = 4.28968 avg_loss = 3.77379\n",
      "epoch no.1 train no.91370  loss = 4.07672 avg_loss = 3.74596\n",
      "epoch no.1 train no.91380  loss = 4.12619 avg_loss = 3.76423\n",
      "epoch no.1 train no.91390  loss = 6.73846 avg_loss = 3.78251\n",
      "epoch no.1 train no.91400  loss = 3.61238 avg_loss = 3.79023\n",
      "epoch no.1 train no.91410  loss = 4.16874 avg_loss = 3.80077\n",
      "epoch no.1 train no.91420  loss = 4.38610 avg_loss = 3.79360\n",
      "epoch no.1 train no.91430  loss = 3.79556 avg_loss = 3.79927\n",
      "epoch no.1 train no.91440  loss = 2.14067 avg_loss = 3.77161\n",
      "epoch no.1 train no.91450  loss = 2.43182 avg_loss = 3.79548\n",
      "epoch no.1 train no.91460  loss = 3.25647 avg_loss = 3.72934\n",
      "epoch no.1 train no.91470  loss = 2.69523 avg_loss = 3.71548\n",
      "epoch no.1 train no.91480  loss = 3.42716 avg_loss = 3.70390\n",
      "epoch no.1 train no.91490  loss = 3.85232 avg_loss = 3.70408\n",
      "epoch no.1 train no.91500  loss = 6.35806 avg_loss = 3.73635\n",
      "epoch no.1 train no.91510  loss = 6.23003 avg_loss = 3.76078\n",
      "epoch no.1 train no.91520  loss = 6.51870 avg_loss = 3.75108\n",
      "epoch no.1 train no.91530  loss = 3.02365 avg_loss = 3.75042\n",
      "epoch no.1 train no.91540  loss = 5.27763 avg_loss = 3.74898\n",
      "epoch no.1 train no.91550  loss = 2.55334 avg_loss = 3.74386\n",
      "epoch no.1 train no.91560  loss = 3.77132 avg_loss = 3.74697\n",
      "epoch no.1 train no.91570  loss = 4.74808 avg_loss = 3.72910\n",
      "epoch no.1 train no.91580  loss = 2.81325 avg_loss = 3.71588\n",
      "epoch no.1 train no.91590  loss = 3.04535 avg_loss = 3.67938\n",
      "epoch no.1 train no.91600  loss = 2.89148 avg_loss = 3.63607\n",
      "epoch no.1 train no.91610  loss = 2.72363 avg_loss = 3.66923\n",
      "epoch no.1 train no.91620  loss = 2.78689 avg_loss = 3.68949\n",
      "epoch no.1 train no.91630  loss = 2.06607 avg_loss = 3.67817\n",
      "epoch no.1 train no.91640  loss = 6.80788 avg_loss = 3.72308\n",
      "epoch no.1 train no.91650  loss = 3.11501 avg_loss = 3.71254\n",
      "epoch no.1 train no.91660  loss = 3.53778 avg_loss = 3.73858\n",
      "epoch no.1 train no.91670  loss = 2.75744 avg_loss = 3.75483\n",
      "epoch no.1 train no.91680  loss = 2.25652 avg_loss = 3.72453\n",
      "epoch no.1 train no.91690  loss = 3.16362 avg_loss = 3.70631\n",
      "epoch no.1 train no.91700  loss = 2.77818 avg_loss = 3.73339\n",
      "epoch no.1 train no.91710  loss = 3.79971 avg_loss = 3.73514\n",
      "epoch no.1 train no.91720  loss = 4.26945 avg_loss = 3.73382\n",
      "epoch no.1 train no.91730  loss = 4.69627 avg_loss = 3.71112\n",
      "epoch no.1 train no.91740  loss = 2.34791 avg_loss = 3.68001\n",
      "epoch no.1 train no.91750  loss = 2.69532 avg_loss = 3.67536\n",
      "epoch no.1 train no.91760  loss = 2.32137 avg_loss = 3.65421\n",
      "epoch no.1 train no.91770  loss = 3.63076 avg_loss = 3.69000\n",
      "epoch no.1 train no.91780  loss = 3.01835 avg_loss = 3.72415\n",
      "epoch no.1 train no.91790  loss = 6.16122 avg_loss = 3.74559\n",
      "epoch no.1 train no.91800  loss = 3.23778 avg_loss = 3.75362\n",
      "epoch no.1 train no.91810  loss = 2.55438 avg_loss = 3.75996\n",
      "epoch no.1 train no.91820  loss = 5.36093 avg_loss = 3.73498\n",
      "epoch no.1 train no.91830  loss = 2.70231 avg_loss = 3.75832\n",
      "epoch no.1 train no.91840  loss = 3.87024 avg_loss = 3.74571\n",
      "epoch no.1 train no.91850  loss = 3.72775 avg_loss = 3.72834\n",
      "epoch no.1 train no.91860  loss = 4.24857 avg_loss = 3.73731\n",
      "epoch no.1 train no.91870  loss = 2.92511 avg_loss = 3.67863\n",
      "epoch no.1 train no.91880  loss = 4.67565 avg_loss = 3.72430\n",
      "epoch no.1 train no.91890  loss = 3.77509 avg_loss = 3.73433\n",
      "epoch no.1 train no.91900  loss = 3.71849 avg_loss = 3.76947\n",
      "epoch no.1 train no.91910  loss = 3.56785 avg_loss = 3.71749\n",
      "epoch no.1 train no.91920  loss = 3.13067 avg_loss = 3.65361\n",
      "epoch no.1 train no.91930  loss = 3.62206 avg_loss = 3.64758\n",
      "epoch no.1 train no.91940  loss = 5.10339 avg_loss = 3.64364\n",
      "epoch no.1 train no.91950  loss = 3.51736 avg_loss = 3.63265\n",
      "epoch no.1 train no.91960  loss = 4.72337 avg_loss = 3.68885\n",
      "epoch no.1 train no.91970  loss = 6.21630 avg_loss = 3.68687\n",
      "epoch no.1 train no.91980  loss = 2.58563 avg_loss = 3.77812\n",
      "epoch no.1 train no.91990  loss = 4.58284 avg_loss = 3.72586\n",
      "epoch no.1 train no.92000  loss = 3.36005 avg_loss = 3.71822\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.92010  loss = 4.02269 avg_loss = 3.74116\n",
      "epoch no.1 train no.92020  loss = 4.47714 avg_loss = 3.76600\n",
      "epoch no.1 train no.92030  loss = 1.73373 avg_loss = 3.78094\n",
      "epoch no.1 train no.92040  loss = 2.45978 avg_loss = 3.72134\n",
      "epoch no.1 train no.92050  loss = 2.58645 avg_loss = 3.66843\n",
      "epoch no.1 train no.92060  loss = 3.09295 avg_loss = 3.71944\n",
      "epoch no.1 train no.92070  loss = 2.95147 avg_loss = 3.70042\n",
      "epoch no.1 train no.92080  loss = 3.03980 avg_loss = 3.72707\n",
      "epoch no.1 train no.92090  loss = 4.50921 avg_loss = 3.81919\n",
      "epoch no.1 train no.92100  loss = 2.92681 avg_loss = 3.86181\n",
      "epoch no.1 train no.92110  loss = 3.17544 avg_loss = 3.86491\n",
      "epoch no.1 train no.92120  loss = 5.56328 avg_loss = 3.90700\n",
      "epoch no.1 train no.92130  loss = 4.53215 avg_loss = 3.90774\n",
      "epoch no.1 train no.92140  loss = 3.31671 avg_loss = 3.92429\n",
      "epoch no.1 train no.92150  loss = 2.98752 avg_loss = 3.86871\n",
      "epoch no.1 train no.92160  loss = 4.62857 avg_loss = 3.93087\n",
      "epoch no.1 train no.92170  loss = 4.37853 avg_loss = 3.95600\n",
      "epoch no.1 train no.92180  loss = 4.96370 avg_loss = 3.93507\n",
      "epoch no.1 train no.92190  loss = 3.65700 avg_loss = 3.90229\n",
      "epoch no.1 train no.92200  loss = 3.97854 avg_loss = 3.86661\n",
      "epoch no.1 train no.92210  loss = 5.80163 avg_loss = 3.82509\n",
      "epoch no.1 train no.92220  loss = 2.79058 avg_loss = 3.84039\n",
      "epoch no.1 train no.92230  loss = 3.96476 avg_loss = 3.78494\n",
      "epoch no.1 train no.92240  loss = 3.03706 avg_loss = 3.74010\n",
      "epoch no.1 train no.92250  loss = 5.48346 avg_loss = 3.75252\n",
      "epoch no.1 train no.92260  loss = 4.46391 avg_loss = 3.76880\n",
      "epoch no.1 train no.92270  loss = 2.41357 avg_loss = 3.75464\n",
      "epoch no.1 train no.92280  loss = 2.48790 avg_loss = 3.71380\n",
      "epoch no.1 train no.92290  loss = 3.65806 avg_loss = 3.67009\n",
      "epoch no.1 train no.92300  loss = 3.87078 avg_loss = 3.70244\n",
      "epoch no.1 train no.92310  loss = 5.29197 avg_loss = 3.65488\n",
      "epoch no.1 train no.92320  loss = 4.14319 avg_loss = 3.71865\n",
      "epoch no.1 train no.92330  loss = 2.45334 avg_loss = 3.68588\n",
      "epoch no.1 train no.92340  loss = 3.39896 avg_loss = 3.71671\n",
      "epoch no.1 train no.92350  loss = 2.98167 avg_loss = 3.73499\n",
      "epoch no.1 train no.92360  loss = 2.24269 avg_loss = 3.69004\n",
      "epoch no.1 train no.92370  loss = 3.66913 avg_loss = 3.68928\n",
      "epoch no.1 train no.92380  loss = 4.50729 avg_loss = 3.68622\n",
      "epoch no.1 train no.92390  loss = 2.97690 avg_loss = 3.70723\n",
      "epoch no.1 train no.92400  loss = 3.44759 avg_loss = 3.70260\n",
      "epoch no.1 train no.92410  loss = 4.53911 avg_loss = 3.72574\n",
      "epoch no.1 train no.92420  loss = 2.34837 avg_loss = 3.70706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.92430  loss = 3.16004 avg_loss = 3.66584\n",
      "epoch no.1 train no.92440  loss = 3.25375 avg_loss = 3.66864\n",
      "epoch no.1 train no.92450  loss = 3.83189 avg_loss = 3.71112\n",
      "epoch no.1 train no.92460  loss = 5.56124 avg_loss = 3.66986\n",
      "epoch no.1 train no.92470  loss = 2.96909 avg_loss = 3.66760\n",
      "epoch no.1 train no.92480  loss = 3.37421 avg_loss = 3.65305\n",
      "epoch no.1 train no.92490  loss = 3.88432 avg_loss = 3.63191\n",
      "epoch no.1 train no.92500  loss = 3.09833 avg_loss = 3.60874\n",
      "epoch no.1 train no.92510  loss = 3.50457 avg_loss = 3.63346\n",
      "epoch no.1 train no.92520  loss = 3.94331 avg_loss = 3.65490\n",
      "epoch no.1 train no.92530  loss = 2.33748 avg_loss = 3.69529\n",
      "epoch no.1 train no.92540  loss = 3.20213 avg_loss = 3.68369\n",
      "epoch no.1 train no.92550  loss = 3.23010 avg_loss = 3.63515\n",
      "epoch no.1 train no.92560  loss = 1.52907 avg_loss = 3.65319\n",
      "epoch no.1 train no.92570  loss = 4.67316 avg_loss = 3.64841\n",
      "epoch no.1 train no.92580  loss = 2.67427 avg_loss = 3.64463\n",
      "epoch no.1 train no.92590  loss = 3.24350 avg_loss = 3.64047\n",
      "epoch no.1 train no.92600  loss = 3.24221 avg_loss = 3.59881\n",
      "epoch no.1 train no.92610  loss = 3.34649 avg_loss = 3.64790\n",
      "epoch no.1 train no.92620  loss = 2.10664 avg_loss = 3.65552\n",
      "epoch no.1 train no.92630  loss = 3.48711 avg_loss = 3.71488\n",
      "epoch no.1 train no.92640  loss = 4.07701 avg_loss = 3.72183\n",
      "epoch no.1 train no.92650  loss = 6.92300 avg_loss = 3.76747\n",
      "epoch no.1 train no.92660  loss = 4.02460 avg_loss = 3.70660\n",
      "epoch no.1 train no.92670  loss = 2.35713 avg_loss = 3.65719\n",
      "epoch no.1 train no.92680  loss = 2.26597 avg_loss = 3.66205\n",
      "epoch no.1 train no.92690  loss = 2.96477 avg_loss = 3.65139\n",
      "epoch no.1 train no.92700  loss = 4.67971 avg_loss = 3.64481\n",
      "epoch no.1 train no.92710  loss = 2.89443 avg_loss = 3.59192\n",
      "epoch no.1 train no.92720  loss = 4.68984 avg_loss = 3.56835\n",
      "epoch no.1 train no.92730  loss = 4.13177 avg_loss = 3.61860\n",
      "epoch no.1 train no.92740  loss = 3.62289 avg_loss = 3.59959\n",
      "epoch no.1 train no.92750  loss = 5.34220 avg_loss = 3.63931\n",
      "epoch no.1 train no.92760  loss = 5.01831 avg_loss = 3.67922\n",
      "epoch no.1 train no.92770  loss = 3.87824 avg_loss = 3.71467\n",
      "epoch no.1 train no.92780  loss = 3.25736 avg_loss = 3.70338\n",
      "epoch no.1 train no.92790  loss = 3.63937 avg_loss = 3.68952\n",
      "epoch no.1 train no.92800  loss = 3.08441 avg_loss = 3.68131\n",
      "epoch no.1 train no.92810  loss = 2.91282 avg_loss = 3.67359\n",
      "epoch no.1 train no.92820  loss = 5.32041 avg_loss = 3.74903\n",
      "epoch no.1 train no.92830  loss = 5.70105 avg_loss = 3.73797\n",
      "epoch no.1 train no.92840  loss = 2.57600 avg_loss = 3.67630\n",
      "epoch no.1 train no.92850  loss = 5.47134 avg_loss = 3.69955\n",
      "epoch no.1 train no.92860  loss = 5.10906 avg_loss = 3.68479\n",
      "epoch no.1 train no.92870  loss = 4.77278 avg_loss = 3.72593\n",
      "epoch no.1 train no.92880  loss = 3.20783 avg_loss = 3.73710\n",
      "epoch no.1 train no.92890  loss = 3.57536 avg_loss = 3.81092\n",
      "epoch no.1 train no.92900  loss = 2.86209 avg_loss = 3.79138\n",
      "epoch no.1 train no.92910  loss = 3.54952 avg_loss = 3.81569\n",
      "epoch no.1 train no.92920  loss = 2.83336 avg_loss = 3.81334\n",
      "epoch no.1 train no.92930  loss = 3.49374 avg_loss = 3.78732\n",
      "epoch no.1 train no.92940  loss = 3.59034 avg_loss = 3.87345\n",
      "epoch no.1 train no.92950  loss = 3.37018 avg_loss = 3.93028\n",
      "epoch no.1 train no.92960  loss = 3.01993 avg_loss = 3.88227\n",
      "epoch no.1 train no.92970  loss = 2.92182 avg_loss = 3.84848\n",
      "epoch no.1 train no.92980  loss = 6.52073 avg_loss = 3.81059\n",
      "epoch no.1 train no.92990  loss = 3.31222 avg_loss = 3.79538\n",
      "epoch no.1 train no.93000  loss = 1.71198 avg_loss = 3.77036\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '▁팝', '송']\n",
      "기분전환이 필요할때 듣는 신나는 팝</s>\n",
      "epoch no.1 train no.93010  loss = 3.95849 avg_loss = 3.81551\n",
      "epoch no.1 train no.93020  loss = 2.78434 avg_loss = 3.76287\n",
      "epoch no.1 train no.93030  loss = 5.79582 avg_loss = 3.85124\n",
      "epoch no.1 train no.93040  loss = 2.25667 avg_loss = 3.75940\n",
      "epoch no.1 train no.93050  loss = 2.77484 avg_loss = 3.73059\n",
      "epoch no.1 train no.93060  loss = 2.77392 avg_loss = 3.74379\n",
      "epoch no.1 train no.93070  loss = 4.64185 avg_loss = 3.70923\n",
      "epoch no.1 train no.93080  loss = 3.01508 avg_loss = 3.73837\n",
      "epoch no.1 train no.93090  loss = 3.78803 avg_loss = 3.75989\n",
      "epoch no.1 train no.93100  loss = 5.47135 avg_loss = 3.86006\n",
      "epoch no.1 train no.93110  loss = 3.56463 avg_loss = 3.81026\n",
      "epoch no.1 train no.93120  loss = 2.86703 avg_loss = 3.76959\n",
      "epoch no.1 train no.93130  loss = 2.96113 avg_loss = 3.76417\n",
      "epoch no.1 train no.93140  loss = 2.81793 avg_loss = 3.72794\n",
      "epoch no.1 train no.93150  loss = 3.73699 avg_loss = 3.71687\n",
      "epoch no.1 train no.93160  loss = 6.50399 avg_loss = 3.72615\n",
      "epoch no.1 train no.93170  loss = 2.92403 avg_loss = 3.70496\n",
      "epoch no.1 train no.93180  loss = 2.87265 avg_loss = 3.67501\n",
      "epoch no.1 train no.93190  loss = 3.75811 avg_loss = 3.73912\n",
      "epoch no.1 train no.93200  loss = 5.07895 avg_loss = 3.74367\n",
      "epoch no.1 train no.93210  loss = 3.50668 avg_loss = 3.79355\n",
      "epoch no.1 train no.93220  loss = 5.12249 avg_loss = 3.82140\n",
      "epoch no.1 train no.93230  loss = 1.91005 avg_loss = 3.80560\n",
      "epoch no.1 train no.93240  loss = 3.25919 avg_loss = 3.76559\n",
      "epoch no.1 train no.93250  loss = 2.27359 avg_loss = 3.74186\n",
      "epoch no.1 train no.93260  loss = 5.51836 avg_loss = 3.73451\n",
      "epoch no.1 train no.93270  loss = 3.96441 avg_loss = 3.75374\n",
      "epoch no.1 train no.93280  loss = 3.51650 avg_loss = 3.79257\n",
      "epoch no.1 train no.93290  loss = 2.73067 avg_loss = 3.79309\n",
      "epoch no.1 train no.93300  loss = 3.93898 avg_loss = 3.80300\n",
      "epoch no.1 train no.93310  loss = 4.86959 avg_loss = 3.80516\n",
      "epoch no.1 train no.93320  loss = 2.56041 avg_loss = 3.77440\n",
      "epoch no.1 train no.93330  loss = 4.11352 avg_loss = 3.76324\n",
      "epoch no.1 train no.93340  loss = 5.10523 avg_loss = 3.75528\n",
      "epoch no.1 train no.93350  loss = 3.80071 avg_loss = 3.70997\n",
      "epoch no.1 train no.93360  loss = 5.11881 avg_loss = 3.71570\n",
      "epoch no.1 train no.93370  loss = 3.22182 avg_loss = 3.72925\n",
      "epoch no.1 train no.93380  loss = 2.16585 avg_loss = 3.73241\n",
      "epoch no.1 train no.93390  loss = 3.20748 avg_loss = 3.67863\n",
      "epoch no.1 train no.93400  loss = 4.23242 avg_loss = 3.69693\n",
      "epoch no.1 train no.93410  loss = 4.65966 avg_loss = 3.68722\n",
      "epoch no.1 train no.93420  loss = 3.97731 avg_loss = 3.72115\n",
      "epoch no.1 train no.93430  loss = 4.81702 avg_loss = 3.71143\n",
      "epoch no.1 train no.93440  loss = 4.74710 avg_loss = 3.75481\n",
      "epoch no.1 train no.93450  loss = 3.31070 avg_loss = 3.69150\n",
      "epoch no.1 train no.93460  loss = 2.94926 avg_loss = 3.67360\n",
      "epoch no.1 train no.93470  loss = 2.68900 avg_loss = 3.67651\n",
      "epoch no.1 train no.93480  loss = 3.95730 avg_loss = 3.67482\n",
      "epoch no.1 train no.93490  loss = 1.37004 avg_loss = 3.65322\n",
      "epoch no.1 train no.93500  loss = 3.95594 avg_loss = 3.71600\n",
      "epoch no.1 train no.93510  loss = 4.11108 avg_loss = 3.67612\n",
      "epoch no.1 train no.93520  loss = 5.98840 avg_loss = 3.77047\n",
      "epoch no.1 train no.93530  loss = 4.53176 avg_loss = 3.78413\n",
      "epoch no.1 train no.93540  loss = 3.12465 avg_loss = 3.76621\n",
      "epoch no.1 train no.93550  loss = 4.74767 avg_loss = 3.73291\n",
      "epoch no.1 train no.93560  loss = 3.35487 avg_loss = 3.77804\n",
      "epoch no.1 train no.93570  loss = 2.46304 avg_loss = 3.73491\n",
      "epoch no.1 train no.93580  loss = 3.80323 avg_loss = 3.70691\n",
      "epoch no.1 train no.93590  loss = 3.99552 avg_loss = 3.67960\n",
      "epoch no.1 train no.93600  loss = 3.92283 avg_loss = 3.66000\n",
      "epoch no.1 train no.93610  loss = 3.49540 avg_loss = 3.63609\n",
      "epoch no.1 train no.93620  loss = 5.34091 avg_loss = 3.67900\n",
      "epoch no.1 train no.93630  loss = 5.53289 avg_loss = 3.70406\n",
      "epoch no.1 train no.93640  loss = 3.37565 avg_loss = 3.72634\n",
      "epoch no.1 train no.93650  loss = 2.96765 avg_loss = 3.73794\n",
      "epoch no.1 train no.93660  loss = 4.97548 avg_loss = 3.74925\n",
      "epoch no.1 train no.93670  loss = 5.98063 avg_loss = 3.72506\n",
      "epoch no.1 train no.93680  loss = 3.14291 avg_loss = 3.71036\n",
      "epoch no.1 train no.93690  loss = 2.86117 avg_loss = 3.68087\n",
      "epoch no.1 train no.93700  loss = 2.22944 avg_loss = 3.67419\n",
      "epoch no.1 train no.93710  loss = 6.37362 avg_loss = 3.65619\n",
      "epoch no.1 train no.93720  loss = 4.13684 avg_loss = 3.63244\n",
      "epoch no.1 train no.93730  loss = 4.11681 avg_loss = 3.57972\n",
      "epoch no.1 train no.93740  loss = 4.24912 avg_loss = 3.59854\n",
      "epoch no.1 train no.93750  loss = 3.26957 avg_loss = 3.62503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.93760  loss = 3.56037 avg_loss = 3.70533\n",
      "epoch no.1 train no.93770  loss = 6.45822 avg_loss = 3.75779\n",
      "epoch no.1 train no.93780  loss = 3.74115 avg_loss = 3.76133\n",
      "epoch no.1 train no.93790  loss = 4.29145 avg_loss = 3.72496\n",
      "epoch no.1 train no.93800  loss = 2.92496 avg_loss = 3.75366\n",
      "epoch no.1 train no.93810  loss = 3.77944 avg_loss = 3.72775\n",
      "epoch no.1 train no.93820  loss = 6.08825 avg_loss = 3.76027\n",
      "epoch no.1 train no.93830  loss = 3.44906 avg_loss = 3.72067\n",
      "epoch no.1 train no.93840  loss = 3.14738 avg_loss = 3.69571\n",
      "epoch no.1 train no.93850  loss = 2.32887 avg_loss = 3.69516\n",
      "epoch no.1 train no.93860  loss = 2.24765 avg_loss = 3.73656\n",
      "epoch no.1 train no.93870  loss = 4.84976 avg_loss = 3.76015\n",
      "epoch no.1 train no.93880  loss = 5.06352 avg_loss = 3.78823\n",
      "epoch no.1 train no.93890  loss = 2.92923 avg_loss = 3.75551\n",
      "epoch no.1 train no.93900  loss = 3.96901 avg_loss = 3.76511\n",
      "epoch no.1 train no.93910  loss = 4.93160 avg_loss = 3.77158\n",
      "epoch no.1 train no.93920  loss = 4.06613 avg_loss = 3.76727\n",
      "epoch no.1 train no.93930  loss = 4.37080 avg_loss = 3.73762\n",
      "epoch no.1 train no.93940  loss = 3.17528 avg_loss = 3.74644\n",
      "epoch no.1 train no.93950  loss = 3.60510 avg_loss = 3.71711\n",
      "epoch no.1 train no.93960  loss = 5.83782 avg_loss = 3.77050\n",
      "epoch no.1 train no.93970  loss = 3.18305 avg_loss = 3.77845\n",
      "epoch no.1 train no.93980  loss = 2.18204 avg_loss = 3.77830\n",
      "epoch no.1 train no.93990  loss = 5.45502 avg_loss = 3.79946\n",
      "epoch no.1 train no.94000  loss = 3.13163 avg_loss = 3.84137\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.94010  loss = 3.05610 avg_loss = 3.84597\n",
      "epoch no.1 train no.94020  loss = 3.71873 avg_loss = 3.90290\n",
      "epoch no.1 train no.94030  loss = 3.01489 avg_loss = 3.84405\n",
      "epoch no.1 train no.94040  loss = 4.45337 avg_loss = 3.83196\n",
      "epoch no.1 train no.94050  loss = 5.60296 avg_loss = 3.81207\n",
      "epoch no.1 train no.94060  loss = 4.25500 avg_loss = 3.77795\n",
      "epoch no.1 train no.94070  loss = 4.20587 avg_loss = 3.74481\n",
      "epoch no.1 train no.94080  loss = 2.09690 avg_loss = 3.69038\n",
      "epoch no.1 train no.94090  loss = 4.52188 avg_loss = 3.72595\n",
      "epoch no.1 train no.94100  loss = 3.26901 avg_loss = 3.74565\n",
      "epoch no.1 train no.94110  loss = 3.96985 avg_loss = 3.77084\n",
      "epoch no.1 train no.94120  loss = 3.24027 avg_loss = 3.75535\n",
      "epoch no.1 train no.94130  loss = 5.33270 avg_loss = 3.74829\n",
      "epoch no.1 train no.94140  loss = 3.22108 avg_loss = 3.76965\n",
      "epoch no.1 train no.94150  loss = 3.62511 avg_loss = 3.74938\n",
      "epoch no.1 train no.94160  loss = 4.55632 avg_loss = 3.73594\n",
      "epoch no.1 train no.94170  loss = 1.75595 avg_loss = 3.72624\n",
      "epoch no.1 train no.94180  loss = 4.33372 avg_loss = 3.72506\n",
      "epoch no.1 train no.94190  loss = 4.88563 avg_loss = 3.78749\n",
      "epoch no.1 train no.94200  loss = 4.16002 avg_loss = 3.78339\n",
      "epoch no.1 train no.94210  loss = 2.26281 avg_loss = 3.75595\n",
      "epoch no.1 train no.94220  loss = 2.16579 avg_loss = 3.75264\n",
      "epoch no.1 train no.94230  loss = 3.28995 avg_loss = 3.74628\n",
      "epoch no.1 train no.94240  loss = 3.69215 avg_loss = 3.77793\n",
      "epoch no.1 train no.94250  loss = 2.61864 avg_loss = 3.79880\n",
      "epoch no.1 train no.94260  loss = 2.63375 avg_loss = 3.75861\n",
      "epoch no.1 train no.94270  loss = 4.24504 avg_loss = 3.69294\n",
      "epoch no.1 train no.94280  loss = 4.45204 avg_loss = 3.75682\n",
      "epoch no.1 train no.94290  loss = 3.53504 avg_loss = 3.75804\n",
      "epoch no.1 train no.94300  loss = 3.22521 avg_loss = 3.69422\n",
      "epoch no.1 train no.94310  loss = 3.14336 avg_loss = 3.68848\n",
      "epoch no.1 train no.94320  loss = 4.75888 avg_loss = 3.73174\n",
      "epoch no.1 train no.94330  loss = 1.79627 avg_loss = 3.65004\n",
      "epoch no.1 train no.94340  loss = 3.40956 avg_loss = 3.63830\n",
      "epoch no.1 train no.94350  loss = 2.49172 avg_loss = 3.63285\n",
      "epoch no.1 train no.94360  loss = 4.50025 avg_loss = 3.61653\n",
      "epoch no.1 train no.94370  loss = 3.22724 avg_loss = 3.65249\n",
      "epoch no.1 train no.94380  loss = 4.37377 avg_loss = 3.65855\n",
      "epoch no.1 train no.94390  loss = 2.43823 avg_loss = 3.69665\n",
      "epoch no.1 train no.94400  loss = 2.30613 avg_loss = 3.66311\n",
      "epoch no.1 train no.94410  loss = 2.33251 avg_loss = 3.66696\n",
      "epoch no.1 train no.94420  loss = 4.25836 avg_loss = 3.66039\n",
      "epoch no.1 train no.94430  loss = 3.18376 avg_loss = 3.69783\n",
      "epoch no.1 train no.94440  loss = 4.33134 avg_loss = 3.75164\n",
      "epoch no.1 train no.94450  loss = 2.30844 avg_loss = 3.71914\n",
      "epoch no.1 train no.94460  loss = 3.50545 avg_loss = 3.69831\n",
      "epoch no.1 train no.94470  loss = 6.05263 avg_loss = 3.73138\n",
      "epoch no.1 train no.94480  loss = 4.08409 avg_loss = 3.68991\n",
      "epoch no.1 train no.94490  loss = 3.19826 avg_loss = 3.67722\n",
      "epoch no.1 train no.94500  loss = 3.97078 avg_loss = 3.66414\n",
      "epoch no.1 train no.94510  loss = 3.77374 avg_loss = 3.63871\n",
      "epoch no.1 train no.94520  loss = 4.15912 avg_loss = 3.64596\n",
      "epoch no.1 train no.94530  loss = 3.32164 avg_loss = 3.60461\n",
      "epoch no.1 train no.94540  loss = 4.69700 avg_loss = 3.62578\n",
      "epoch no.1 train no.94550  loss = 3.17568 avg_loss = 3.60127\n",
      "epoch no.1 train no.94560  loss = 4.87062 avg_loss = 3.60473\n",
      "epoch no.1 train no.94570  loss = 1.55267 avg_loss = 3.60509\n",
      "epoch no.1 train no.94580  loss = 5.27288 avg_loss = 3.68125\n",
      "epoch no.1 train no.94590  loss = 2.01639 avg_loss = 3.65544\n",
      "epoch no.1 train no.94600  loss = 5.47786 avg_loss = 3.66759\n",
      "epoch no.1 train no.94610  loss = 2.85086 avg_loss = 3.67245\n",
      "epoch no.1 train no.94620  loss = 3.10647 avg_loss = 3.68087\n",
      "epoch no.1 train no.94630  loss = 3.45951 avg_loss = 3.68417\n",
      "epoch no.1 train no.94640  loss = 2.82038 avg_loss = 3.74389\n",
      "epoch no.1 train no.94650  loss = 4.13574 avg_loss = 3.77554\n",
      "epoch no.1 train no.94660  loss = 3.76646 avg_loss = 3.72703\n",
      "epoch no.1 train no.94670  loss = 3.89956 avg_loss = 3.65772\n",
      "epoch no.1 train no.94680  loss = 5.77179 avg_loss = 3.74403\n",
      "epoch no.1 train no.94690  loss = 4.53675 avg_loss = 3.72830\n",
      "epoch no.1 train no.94700  loss = 1.89003 avg_loss = 3.67239\n",
      "epoch no.1 train no.94710  loss = 5.00863 avg_loss = 3.66551\n",
      "epoch no.1 train no.94720  loss = 3.77825 avg_loss = 3.62653\n",
      "epoch no.1 train no.94730  loss = 5.44670 avg_loss = 3.66862\n",
      "epoch no.1 train no.94740  loss = 3.51675 avg_loss = 3.68961\n",
      "epoch no.1 train no.94750  loss = 6.34647 avg_loss = 3.70361\n",
      "epoch no.1 train no.94760  loss = 3.77761 avg_loss = 3.75312\n",
      "epoch no.1 train no.94770  loss = 2.86905 avg_loss = 3.66696\n",
      "epoch no.1 train no.94780  loss = 3.25971 avg_loss = 3.62738\n",
      "epoch no.1 train no.94790  loss = 3.71352 avg_loss = 3.61542\n",
      "epoch no.1 train no.94800  loss = 1.28059 avg_loss = 3.58992\n",
      "epoch no.1 train no.94810  loss = 4.31664 avg_loss = 3.61205\n",
      "epoch no.1 train no.94820  loss = 3.56221 avg_loss = 3.60189\n",
      "epoch no.1 train no.94830  loss = 1.84399 avg_loss = 3.56192\n",
      "epoch no.1 train no.94840  loss = 3.45367 avg_loss = 3.57693\n",
      "epoch no.1 train no.94850  loss = 6.10571 avg_loss = 3.63732\n",
      "epoch no.1 train no.94860  loss = 4.27372 avg_loss = 3.63365\n",
      "epoch no.1 train no.94870  loss = 6.36858 avg_loss = 3.63811\n",
      "epoch no.1 train no.94880  loss = 5.32595 avg_loss = 3.64502\n",
      "epoch no.1 train no.94890  loss = 3.05217 avg_loss = 3.72789\n",
      "epoch no.1 train no.94900  loss = 2.74901 avg_loss = 3.74167\n",
      "epoch no.1 train no.94910  loss = 4.03017 avg_loss = 3.72701\n",
      "epoch no.1 train no.94920  loss = 1.90671 avg_loss = 3.66009\n",
      "epoch no.1 train no.94930  loss = 3.78579 avg_loss = 3.67363\n",
      "epoch no.1 train no.94940  loss = 2.05683 avg_loss = 3.63202\n",
      "epoch no.1 train no.94950  loss = 4.19768 avg_loss = 3.65827\n",
      "epoch no.1 train no.94960  loss = 4.15630 avg_loss = 3.65271\n",
      "epoch no.1 train no.94970  loss = 2.81822 avg_loss = 3.63581\n",
      "epoch no.1 train no.94980  loss = 5.47957 avg_loss = 3.65849\n",
      "epoch no.1 train no.94990  loss = 3.32890 avg_loss = 3.71345\n",
      "epoch no.1 train no.95000  loss = 2.74736 avg_loss = 3.70143\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '이', '▁딱', '▁신나는', '▁노래', '들', '</s>']\n",
      "기분전환에 딱 좋은 곡들</s>\n",
      "epoch no.1 train no.95010  loss = 4.15466 avg_loss = 3.66092\n",
      "epoch no.1 train no.95020  loss = 2.87424 avg_loss = 3.69787\n",
      "epoch no.1 train no.95030  loss = 3.93693 avg_loss = 3.64916\n",
      "epoch no.1 train no.95040  loss = 4.73785 avg_loss = 3.63270\n",
      "epoch no.1 train no.95050  loss = 3.60394 avg_loss = 3.62709\n",
      "epoch no.1 train no.95060  loss = 3.10433 avg_loss = 3.55681\n",
      "epoch no.1 train no.95070  loss = 2.95784 avg_loss = 3.56904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.95080  loss = 2.77060 avg_loss = 3.59408\n",
      "epoch no.1 train no.95090  loss = 5.48103 avg_loss = 3.62245\n",
      "epoch no.1 train no.95100  loss = 2.93451 avg_loss = 3.68681\n",
      "epoch no.1 train no.95110  loss = 3.31232 avg_loss = 3.70639\n",
      "epoch no.1 train no.95120  loss = 3.40629 avg_loss = 3.76473\n",
      "epoch no.1 train no.95130  loss = 4.02346 avg_loss = 3.80462\n",
      "epoch no.1 train no.95140  loss = 2.09696 avg_loss = 3.75912\n",
      "epoch no.1 train no.95150  loss = 4.61845 avg_loss = 3.72812\n",
      "epoch no.1 train no.95160  loss = 3.40208 avg_loss = 3.76245\n",
      "epoch no.1 train no.95170  loss = 3.88020 avg_loss = 3.70456\n",
      "epoch no.1 train no.95180  loss = 3.71084 avg_loss = 3.73200\n",
      "epoch no.1 train no.95190  loss = 3.09361 avg_loss = 3.70563\n",
      "epoch no.1 train no.95200  loss = 3.36772 avg_loss = 3.71158\n",
      "epoch no.1 train no.95210  loss = 3.01323 avg_loss = 3.67835\n",
      "epoch no.1 train no.95220  loss = 3.31431 avg_loss = 3.71014\n",
      "epoch no.1 train no.95230  loss = 3.46398 avg_loss = 3.73652\n",
      "epoch no.1 train no.95240  loss = 3.89277 avg_loss = 3.75544\n",
      "epoch no.1 train no.95250  loss = 1.75669 avg_loss = 3.75729\n",
      "epoch no.1 train no.95260  loss = 3.23941 avg_loss = 3.75552\n",
      "epoch no.1 train no.95270  loss = 4.17305 avg_loss = 3.75694\n",
      "epoch no.1 train no.95280  loss = 3.27205 avg_loss = 3.73135\n",
      "epoch no.1 train no.95290  loss = 4.13046 avg_loss = 3.70643\n",
      "epoch no.1 train no.95300  loss = 4.15166 avg_loss = 3.71392\n",
      "epoch no.1 train no.95310  loss = 2.99382 avg_loss = 3.74748\n",
      "epoch no.1 train no.95320  loss = 3.38924 avg_loss = 3.76818\n",
      "epoch no.1 train no.95330  loss = 4.89559 avg_loss = 3.84746\n",
      "epoch no.1 train no.95340  loss = 2.07696 avg_loss = 3.79754\n",
      "epoch no.1 train no.95350  loss = 4.25868 avg_loss = 3.80952\n",
      "epoch no.1 train no.95360  loss = 4.07184 avg_loss = 3.82958\n",
      "epoch no.1 train no.95370  loss = 3.05842 avg_loss = 3.79733\n",
      "epoch no.1 train no.95380  loss = 4.05912 avg_loss = 3.76160\n",
      "epoch no.1 train no.95390  loss = 3.41130 avg_loss = 3.78691\n",
      "epoch no.1 train no.95400  loss = 3.93672 avg_loss = 3.81464\n",
      "epoch no.1 train no.95410  loss = 3.85552 avg_loss = 3.79811\n",
      "epoch no.1 train no.95420  loss = 3.81382 avg_loss = 3.77577\n",
      "epoch no.1 train no.95430  loss = 5.34651 avg_loss = 3.74488\n",
      "epoch no.1 train no.95440  loss = 4.22670 avg_loss = 3.69244\n",
      "epoch no.1 train no.95450  loss = 3.03878 avg_loss = 3.63334\n",
      "epoch no.1 train no.95460  loss = 2.94627 avg_loss = 3.63149\n",
      "epoch no.1 train no.95470  loss = 4.30905 avg_loss = 3.66258\n",
      "epoch no.1 train no.95480  loss = 2.99886 avg_loss = 3.69418\n",
      "epoch no.1 train no.95490  loss = 2.30643 avg_loss = 3.66693\n",
      "epoch no.1 train no.95500  loss = 2.20767 avg_loss = 3.64095\n",
      "epoch no.1 train no.95510  loss = 3.66921 avg_loss = 3.66918\n",
      "epoch no.1 train no.95520  loss = 2.65378 avg_loss = 3.63518\n",
      "epoch no.1 train no.95530  loss = 6.62192 avg_loss = 3.67586\n",
      "epoch no.1 train no.95540  loss = 3.89532 avg_loss = 3.67811\n",
      "epoch no.1 train no.95550  loss = 2.15243 avg_loss = 3.67411\n",
      "epoch no.1 train no.95560  loss = 3.93192 avg_loss = 3.73387\n",
      "epoch no.1 train no.95570  loss = 3.00917 avg_loss = 3.71461\n",
      "epoch no.1 train no.95580  loss = 7.20641 avg_loss = 3.77315\n",
      "epoch no.1 train no.95590  loss = 2.57353 avg_loss = 3.76150\n",
      "epoch no.1 train no.95600  loss = 3.92110 avg_loss = 3.68852\n",
      "epoch no.1 train no.95610  loss = 3.42107 avg_loss = 3.67250\n",
      "epoch no.1 train no.95620  loss = 2.49204 avg_loss = 3.69119\n",
      "epoch no.1 train no.95630  loss = 4.04552 avg_loss = 3.74458\n",
      "epoch no.1 train no.95640  loss = 1.50403 avg_loss = 3.71367\n",
      "epoch no.1 train no.95650  loss = 2.23769 avg_loss = 3.72178\n",
      "epoch no.1 train no.95660  loss = 3.42568 avg_loss = 3.70132\n",
      "epoch no.1 train no.95670  loss = 2.84166 avg_loss = 3.74094\n",
      "epoch no.1 train no.95680  loss = 5.93947 avg_loss = 3.71078\n",
      "epoch no.1 train no.95690  loss = 4.55432 avg_loss = 3.70196\n",
      "epoch no.1 train no.95700  loss = 2.92485 avg_loss = 3.72753\n",
      "epoch no.1 train no.95710  loss = 3.71779 avg_loss = 3.76403\n",
      "epoch no.1 train no.95720  loss = 5.15281 avg_loss = 3.77037\n",
      "epoch no.1 train no.95730  loss = 3.32987 avg_loss = 3.76326\n",
      "epoch no.1 train no.95740  loss = 3.60924 avg_loss = 3.85571\n",
      "epoch no.1 train no.95750  loss = 4.87649 avg_loss = 3.87782\n",
      "epoch no.1 train no.95760  loss = 2.64857 avg_loss = 3.84906\n",
      "epoch no.1 train no.95770  loss = 2.62075 avg_loss = 3.83549\n",
      "epoch no.1 train no.95780  loss = 3.68396 avg_loss = 3.79697\n",
      "epoch no.1 train no.95790  loss = 2.79077 avg_loss = 3.80267\n",
      "epoch no.1 train no.95800  loss = 4.20820 avg_loss = 3.90058\n",
      "epoch no.1 train no.95810  loss = 3.09612 avg_loss = 3.82628\n",
      "epoch no.1 train no.95820  loss = 3.86900 avg_loss = 3.79290\n",
      "epoch no.1 train no.95830  loss = 4.19823 avg_loss = 3.79008\n",
      "epoch no.1 train no.95840  loss = 3.39402 avg_loss = 3.81321\n",
      "epoch no.1 train no.95850  loss = 3.06338 avg_loss = 3.80187\n",
      "epoch no.1 train no.95860  loss = 3.42325 avg_loss = 3.82375\n",
      "epoch no.1 train no.95870  loss = 2.50766 avg_loss = 3.78554\n",
      "epoch no.1 train no.95880  loss = 3.51574 avg_loss = 3.77921\n",
      "epoch no.1 train no.95890  loss = 2.82360 avg_loss = 3.74575\n",
      "epoch no.1 train no.95900  loss = 3.53001 avg_loss = 3.76941\n",
      "epoch no.1 train no.95910  loss = 3.71429 avg_loss = 3.78254\n",
      "epoch no.1 train no.95920  loss = 3.74858 avg_loss = 3.83016\n",
      "epoch no.1 train no.95930  loss = 4.77076 avg_loss = 3.83423\n",
      "epoch no.1 train no.95940  loss = 4.16716 avg_loss = 3.83640\n",
      "epoch no.1 train no.95950  loss = 5.64942 avg_loss = 3.83927\n",
      "epoch no.1 train no.95960  loss = 3.97686 avg_loss = 3.84109\n",
      "epoch no.1 train no.95970  loss = 4.52471 avg_loss = 3.85313\n",
      "epoch no.1 train no.95980  loss = 3.91080 avg_loss = 3.81072\n",
      "epoch no.1 train no.95990  loss = 3.33361 avg_loss = 3.79595\n",
      "epoch no.1 train no.96000  loss = 4.66872 avg_loss = 3.74740\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '▁딱', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 좋은 노래</s>\n",
      "epoch no.1 train no.96010  loss = 3.23623 avg_loss = 3.69620\n",
      "epoch no.1 train no.96020  loss = 3.95171 avg_loss = 3.67653\n",
      "epoch no.1 train no.96030  loss = 5.35879 avg_loss = 3.74003\n",
      "epoch no.1 train no.96040  loss = 3.04311 avg_loss = 3.68179\n",
      "epoch no.1 train no.96050  loss = 4.52612 avg_loss = 3.68306\n",
      "epoch no.1 train no.96060  loss = 2.52459 avg_loss = 3.67942\n",
      "epoch no.1 train no.96070  loss = 5.66717 avg_loss = 3.68584\n",
      "epoch no.1 train no.96080  loss = 3.41089 avg_loss = 3.67664\n",
      "epoch no.1 train no.96090  loss = 3.23705 avg_loss = 3.68758\n",
      "epoch no.1 train no.96100  loss = 3.19259 avg_loss = 3.71483\n",
      "epoch no.1 train no.96110  loss = 5.20409 avg_loss = 3.69509\n",
      "epoch no.1 train no.96120  loss = 3.41495 avg_loss = 3.73753\n",
      "epoch no.1 train no.96130  loss = 5.62465 avg_loss = 3.73767\n",
      "epoch no.1 train no.96140  loss = 2.55104 avg_loss = 3.71233\n",
      "epoch no.1 train no.96150  loss = 2.91372 avg_loss = 3.73599\n",
      "epoch no.1 train no.96160  loss = 3.25150 avg_loss = 3.72890\n",
      "epoch no.1 train no.96170  loss = 3.09172 avg_loss = 3.70002\n",
      "epoch no.1 train no.96180  loss = 4.47416 avg_loss = 3.74166\n",
      "epoch no.1 train no.96190  loss = 5.39063 avg_loss = 3.73539\n",
      "epoch no.1 train no.96200  loss = 3.44976 avg_loss = 3.76397\n",
      "epoch no.1 train no.96210  loss = 4.60921 avg_loss = 3.79212\n",
      "epoch no.1 train no.96220  loss = 5.06566 avg_loss = 3.79357\n",
      "epoch no.1 train no.96230  loss = 2.33941 avg_loss = 3.68655\n",
      "epoch no.1 train no.96240  loss = 3.65757 avg_loss = 3.73485\n",
      "epoch no.1 train no.96250  loss = 3.37732 avg_loss = 3.75294\n",
      "epoch no.1 train no.96260  loss = 2.70252 avg_loss = 3.85687\n",
      "epoch no.1 train no.96270  loss = 3.07953 avg_loss = 3.81040\n",
      "epoch no.1 train no.96280  loss = 2.31902 avg_loss = 3.79557\n",
      "epoch no.1 train no.96290  loss = 2.99765 avg_loss = 3.79568\n",
      "epoch no.1 train no.96300  loss = 2.01662 avg_loss = 3.81034\n",
      "epoch no.1 train no.96310  loss = 4.62650 avg_loss = 3.77154\n",
      "epoch no.1 train no.96320  loss = 3.70653 avg_loss = 3.77919\n",
      "epoch no.1 train no.96330  loss = 4.19742 avg_loss = 3.77563\n",
      "epoch no.1 train no.96340  loss = 3.43076 avg_loss = 3.74413\n",
      "epoch no.1 train no.96350  loss = 2.06393 avg_loss = 3.76971\n",
      "epoch no.1 train no.96360  loss = 7.62490 avg_loss = 3.80714\n",
      "epoch no.1 train no.96370  loss = 3.07648 avg_loss = 3.80668\n",
      "epoch no.1 train no.96380  loss = 3.38285 avg_loss = 3.79737\n",
      "epoch no.1 train no.96390  loss = 3.31236 avg_loss = 3.82739\n",
      "epoch no.1 train no.96400  loss = 5.19080 avg_loss = 3.83264\n",
      "epoch no.1 train no.96410  loss = 2.49964 avg_loss = 3.77341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.96420  loss = 5.62016 avg_loss = 3.78264\n",
      "epoch no.1 train no.96430  loss = 2.58110 avg_loss = 3.74903\n",
      "epoch no.1 train no.96440  loss = 3.20593 avg_loss = 3.83439\n",
      "epoch no.1 train no.96450  loss = 7.37753 avg_loss = 3.84385\n",
      "epoch no.1 train no.96460  loss = 2.61782 avg_loss = 3.77541\n",
      "epoch no.1 train no.96470  loss = 2.39387 avg_loss = 3.72371\n",
      "epoch no.1 train no.96480  loss = 3.66665 avg_loss = 3.74679\n",
      "epoch no.1 train no.96490  loss = 4.81302 avg_loss = 3.71705\n",
      "epoch no.1 train no.96500  loss = 2.28135 avg_loss = 3.73013\n",
      "epoch no.1 train no.96510  loss = 2.50157 avg_loss = 3.73247\n",
      "epoch no.1 train no.96520  loss = 3.77106 avg_loss = 3.69988\n",
      "epoch no.1 train no.96530  loss = 2.41924 avg_loss = 3.65484\n",
      "epoch no.1 train no.96540  loss = 3.57694 avg_loss = 3.65849\n",
      "epoch no.1 train no.96550  loss = 5.22961 avg_loss = 3.69769\n",
      "epoch no.1 train no.96560  loss = 2.08075 avg_loss = 3.68024\n",
      "epoch no.1 train no.96570  loss = 4.42453 avg_loss = 3.68486\n",
      "epoch no.1 train no.96580  loss = 2.61748 avg_loss = 3.70627\n",
      "epoch no.1 train no.96590  loss = 4.08214 avg_loss = 3.70773\n",
      "epoch no.1 train no.96600  loss = 2.98882 avg_loss = 3.74760\n",
      "epoch no.1 train no.96610  loss = 4.21519 avg_loss = 3.74796\n",
      "epoch no.1 train no.96620  loss = 5.12456 avg_loss = 3.75225\n",
      "epoch no.1 train no.96630  loss = 3.04251 avg_loss = 3.67639\n",
      "epoch no.1 train no.96640  loss = 3.40170 avg_loss = 3.70637\n",
      "epoch no.1 train no.96650  loss = 5.23914 avg_loss = 3.75430\n",
      "epoch no.1 train no.96660  loss = 3.07814 avg_loss = 3.68904\n",
      "epoch no.1 train no.96670  loss = 1.98274 avg_loss = 3.68112\n",
      "epoch no.1 train no.96680  loss = 2.32697 avg_loss = 3.66163\n",
      "epoch no.1 train no.96690  loss = 2.98255 avg_loss = 3.63835\n",
      "epoch no.1 train no.96700  loss = 2.61860 avg_loss = 3.60986\n",
      "epoch no.1 train no.96710  loss = 1.96204 avg_loss = 3.66059\n",
      "epoch no.1 train no.96720  loss = 2.85679 avg_loss = 3.68238\n",
      "epoch no.1 train no.96730  loss = 5.09116 avg_loss = 3.70350\n",
      "epoch no.1 train no.96740  loss = 2.39587 avg_loss = 3.66482\n",
      "epoch no.1 train no.96750  loss = 4.72058 avg_loss = 3.68872\n",
      "epoch no.1 train no.96760  loss = 4.26235 avg_loss = 3.69174\n",
      "epoch no.1 train no.96770  loss = 2.84088 avg_loss = 3.70207\n",
      "epoch no.1 train no.96780  loss = 5.24044 avg_loss = 3.71969\n",
      "epoch no.1 train no.96790  loss = 3.72589 avg_loss = 3.75546\n",
      "epoch no.1 train no.96800  loss = 3.79346 avg_loss = 3.74696\n",
      "epoch no.1 train no.96810  loss = 3.39200 avg_loss = 3.69045\n",
      "epoch no.1 train no.96820  loss = 4.38134 avg_loss = 3.66823\n",
      "epoch no.1 train no.96830  loss = 2.52185 avg_loss = 3.65493\n",
      "epoch no.1 train no.96840  loss = 2.90195 avg_loss = 3.62758\n",
      "epoch no.1 train no.96850  loss = 3.37079 avg_loss = 3.65797\n",
      "epoch no.1 train no.96860  loss = 3.44377 avg_loss = 3.60407\n",
      "epoch no.1 train no.96870  loss = 3.15134 avg_loss = 3.66250\n",
      "epoch no.1 train no.96880  loss = 3.54625 avg_loss = 3.70264\n",
      "epoch no.1 train no.96890  loss = 3.27282 avg_loss = 3.70967\n",
      "epoch no.1 train no.96900  loss = 3.28437 avg_loss = 3.72591\n",
      "epoch no.1 train no.96910  loss = 3.90226 avg_loss = 3.68830\n",
      "epoch no.1 train no.96920  loss = 2.61960 avg_loss = 3.74294\n",
      "epoch no.1 train no.96930  loss = 3.67602 avg_loss = 3.71693\n",
      "epoch no.1 train no.96940  loss = 4.33165 avg_loss = 3.68447\n",
      "epoch no.1 train no.96950  loss = 5.15879 avg_loss = 3.70725\n",
      "epoch no.1 train no.96960  loss = 4.68419 avg_loss = 3.68797\n",
      "epoch no.1 train no.96970  loss = 3.17695 avg_loss = 3.68310\n",
      "epoch no.1 train no.96980  loss = 3.10573 avg_loss = 3.72429\n",
      "epoch no.1 train no.96990  loss = 4.61961 avg_loss = 3.73937\n",
      "epoch no.1 train no.97000  loss = 4.01759 avg_loss = 3.75227\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁음악', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.1 train no.97010  loss = 3.32204 avg_loss = 3.75138\n",
      "epoch no.1 train no.97020  loss = 3.31917 avg_loss = 3.77688\n",
      "epoch no.1 train no.97030  loss = 3.24506 avg_loss = 3.77385\n",
      "epoch no.1 train no.97040  loss = 3.58634 avg_loss = 3.71948\n",
      "epoch no.1 train no.97050  loss = 4.95870 avg_loss = 3.66480\n",
      "epoch no.1 train no.97060  loss = 1.35906 avg_loss = 3.61028\n",
      "epoch no.1 train no.97070  loss = 3.81142 avg_loss = 3.55243\n",
      "epoch no.1 train no.97080  loss = 2.60016 avg_loss = 3.54424\n",
      "epoch no.1 train no.97090  loss = 5.92571 avg_loss = 3.55993\n",
      "epoch no.1 train no.97100  loss = 2.58271 avg_loss = 3.52878\n",
      "epoch no.1 train no.97110  loss = 5.71561 avg_loss = 3.59676\n",
      "epoch no.1 train no.97120  loss = 3.11893 avg_loss = 3.64685\n",
      "epoch no.1 train no.97130  loss = 5.46427 avg_loss = 3.68374\n",
      "epoch no.1 train no.97140  loss = 3.27578 avg_loss = 3.67569\n",
      "epoch no.1 train no.97150  loss = 4.44514 avg_loss = 3.70628\n",
      "epoch no.1 train no.97160  loss = 2.72344 avg_loss = 3.70857\n",
      "epoch no.1 train no.97170  loss = 4.32271 avg_loss = 3.72646\n",
      "epoch no.1 train no.97180  loss = 3.40737 avg_loss = 3.73043\n",
      "epoch no.1 train no.97190  loss = 3.60456 avg_loss = 3.74022\n",
      "epoch no.1 train no.97200  loss = 2.32012 avg_loss = 3.74119\n",
      "epoch no.1 train no.97210  loss = 3.99706 avg_loss = 3.77191\n",
      "epoch no.1 train no.97220  loss = 1.90775 avg_loss = 3.71231\n",
      "epoch no.1 train no.97230  loss = 3.38269 avg_loss = 3.81190\n",
      "epoch no.1 train no.97240  loss = 3.67029 avg_loss = 3.78472\n",
      "epoch no.1 train no.97250  loss = 2.72798 avg_loss = 3.76367\n",
      "epoch no.1 train no.97260  loss = 3.16512 avg_loss = 3.77586\n",
      "epoch no.1 train no.97270  loss = 2.71664 avg_loss = 3.72169\n",
      "epoch no.1 train no.97280  loss = 3.18324 avg_loss = 3.68093\n",
      "epoch no.1 train no.97290  loss = 3.10323 avg_loss = 3.66089\n",
      "epoch no.1 train no.97300  loss = 4.03990 avg_loss = 3.68778\n",
      "epoch no.1 train no.97310  loss = 4.02883 avg_loss = 3.68990\n",
      "epoch no.1 train no.97320  loss = 3.35638 avg_loss = 3.66472\n",
      "epoch no.1 train no.97330  loss = 2.17841 avg_loss = 3.59074\n",
      "epoch no.1 train no.97340  loss = 2.26233 avg_loss = 3.64786\n",
      "epoch no.1 train no.97350  loss = 2.25896 avg_loss = 3.65949\n",
      "epoch no.1 train no.97360  loss = 3.18970 avg_loss = 3.61109\n",
      "epoch no.1 train no.97370  loss = 3.89926 avg_loss = 3.60335\n",
      "epoch no.1 train no.97380  loss = 2.75254 avg_loss = 3.63077\n",
      "epoch no.1 train no.97390  loss = 5.31743 avg_loss = 3.67918\n",
      "epoch no.1 train no.97400  loss = 2.53367 avg_loss = 3.68524\n",
      "epoch no.1 train no.97410  loss = 4.02817 avg_loss = 3.71770\n",
      "epoch no.1 train no.97420  loss = 2.48275 avg_loss = 3.78282\n",
      "epoch no.1 train no.97430  loss = 3.44262 avg_loss = 3.79001\n",
      "epoch no.1 train no.97440  loss = 2.47449 avg_loss = 3.78092\n",
      "epoch no.1 train no.97450  loss = 3.51461 avg_loss = 3.82118\n",
      "epoch no.1 train no.97460  loss = 3.04279 avg_loss = 3.80147\n",
      "epoch no.1 train no.97470  loss = 3.56639 avg_loss = 3.78739\n",
      "epoch no.1 train no.97480  loss = 3.46754 avg_loss = 3.78520\n",
      "epoch no.1 train no.97490  loss = 2.90661 avg_loss = 3.75086\n",
      "epoch no.1 train no.97500  loss = 6.34977 avg_loss = 3.81408\n",
      "epoch no.1 train no.97510  loss = 2.97709 avg_loss = 3.74562\n",
      "epoch no.1 train no.97520  loss = 3.33218 avg_loss = 3.75214\n",
      "epoch no.1 train no.97530  loss = 4.45488 avg_loss = 3.72767\n",
      "epoch no.1 train no.97540  loss = 3.40812 avg_loss = 3.66581\n",
      "epoch no.1 train no.97550  loss = 5.59183 avg_loss = 3.64528\n",
      "epoch no.1 train no.97560  loss = 1.93504 avg_loss = 3.61661\n",
      "epoch no.1 train no.97570  loss = 4.51911 avg_loss = 3.58732\n",
      "epoch no.1 train no.97580  loss = 4.37799 avg_loss = 3.61197\n",
      "epoch no.1 train no.97590  loss = 2.96119 avg_loss = 3.62157\n",
      "epoch no.1 train no.97600  loss = 3.40562 avg_loss = 3.58569\n",
      "epoch no.1 train no.97610  loss = 3.51363 avg_loss = 3.54804\n",
      "epoch no.1 train no.97620  loss = 4.38899 avg_loss = 3.59064\n",
      "epoch no.1 train no.97630  loss = 3.49787 avg_loss = 3.57601\n",
      "epoch no.1 train no.97640  loss = 3.49186 avg_loss = 3.61387\n",
      "epoch no.1 train no.97650  loss = 4.32328 avg_loss = 3.57488\n",
      "epoch no.1 train no.97660  loss = 4.04245 avg_loss = 3.57457\n",
      "epoch no.1 train no.97670  loss = 3.65973 avg_loss = 3.53811\n",
      "epoch no.1 train no.97680  loss = 3.41011 avg_loss = 3.63754\n",
      "epoch no.1 train no.97690  loss = 4.40828 avg_loss = 3.69984\n",
      "epoch no.1 train no.97700  loss = 3.70671 avg_loss = 3.72309\n",
      "epoch no.1 train no.97710  loss = 3.80857 avg_loss = 3.69625\n",
      "epoch no.1 train no.97720  loss = 4.75060 avg_loss = 3.73268\n",
      "epoch no.1 train no.97730  loss = 5.53228 avg_loss = 3.78120\n",
      "epoch no.1 train no.97740  loss = 3.22559 avg_loss = 3.78276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.97750  loss = 3.84077 avg_loss = 3.77851\n",
      "epoch no.1 train no.97760  loss = 3.72769 avg_loss = 3.77726\n",
      "epoch no.1 train no.97770  loss = 4.29922 avg_loss = 3.80494\n",
      "epoch no.1 train no.97780  loss = 4.04181 avg_loss = 3.82541\n",
      "epoch no.1 train no.97790  loss = 3.08218 avg_loss = 3.81204\n",
      "epoch no.1 train no.97800  loss = 3.04852 avg_loss = 3.76138\n",
      "epoch no.1 train no.97810  loss = 3.02563 avg_loss = 3.78615\n",
      "epoch no.1 train no.97820  loss = 2.89046 avg_loss = 3.83683\n",
      "epoch no.1 train no.97830  loss = 3.83127 avg_loss = 3.74505\n",
      "epoch no.1 train no.97840  loss = 2.61854 avg_loss = 3.79637\n",
      "epoch no.1 train no.97850  loss = 2.94084 avg_loss = 3.77102\n",
      "epoch no.1 train no.97860  loss = 3.49621 avg_loss = 3.76811\n",
      "epoch no.1 train no.97870  loss = 3.28871 avg_loss = 3.74088\n",
      "epoch no.1 train no.97880  loss = 3.76177 avg_loss = 3.72391\n",
      "epoch no.1 train no.97890  loss = 1.64726 avg_loss = 3.72117\n",
      "epoch no.1 train no.97900  loss = 3.12350 avg_loss = 3.74699\n",
      "epoch no.1 train no.97910  loss = 3.83635 avg_loss = 3.74065\n",
      "epoch no.1 train no.97920  loss = 3.70336 avg_loss = 3.69747\n",
      "epoch no.1 train no.97930  loss = 4.19452 avg_loss = 3.74634\n",
      "epoch no.1 train no.97940  loss = 3.36497 avg_loss = 3.71151\n",
      "epoch no.1 train no.97950  loss = 3.43552 avg_loss = 3.74361\n",
      "epoch no.1 train no.97960  loss = 4.06706 avg_loss = 3.72189\n",
      "epoch no.1 train no.97970  loss = 5.53162 avg_loss = 3.71990\n",
      "epoch no.1 train no.97980  loss = 2.88473 avg_loss = 3.72425\n",
      "epoch no.1 train no.97990  loss = 3.60719 avg_loss = 3.72342\n",
      "epoch no.1 train no.98000  loss = 4.98600 avg_loss = 3.75557\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁딱', '▁신나는', '▁노래', '</s>']\n",
      "기분전환에 딱 신나는 노래</s>\n",
      "epoch no.1 train no.98010  loss = 3.05785 avg_loss = 3.74105\n",
      "epoch no.1 train no.98020  loss = 5.10705 avg_loss = 3.77441\n",
      "epoch no.1 train no.98030  loss = 5.70336 avg_loss = 3.81734\n",
      "epoch no.1 train no.98040  loss = 4.95201 avg_loss = 3.86041\n",
      "epoch no.1 train no.98050  loss = 4.17076 avg_loss = 3.82775\n",
      "epoch no.1 train no.98060  loss = 2.61884 avg_loss = 3.81862\n",
      "epoch no.1 train no.98070  loss = 3.62823 avg_loss = 3.86316\n",
      "epoch no.1 train no.98080  loss = 6.43630 avg_loss = 3.91711\n",
      "epoch no.1 train no.98090  loss = 4.97986 avg_loss = 3.87001\n",
      "epoch no.1 train no.98100  loss = 6.12229 avg_loss = 3.94258\n",
      "epoch no.1 train no.98110  loss = 2.87512 avg_loss = 3.87676\n",
      "epoch no.1 train no.98120  loss = 5.06458 avg_loss = 3.85621\n",
      "epoch no.1 train no.98130  loss = 2.87386 avg_loss = 3.80590\n",
      "epoch no.1 train no.98140  loss = 3.89590 avg_loss = 3.81280\n",
      "epoch no.1 train no.98150  loss = 3.32542 avg_loss = 3.81189\n",
      "epoch no.1 train no.98160  loss = 4.73728 avg_loss = 3.84578\n",
      "epoch no.1 train no.98170  loss = 4.45354 avg_loss = 3.89806\n",
      "epoch no.1 train no.98180  loss = 2.69026 avg_loss = 3.87629\n",
      "epoch no.1 train no.98190  loss = 3.37302 avg_loss = 3.85207\n",
      "epoch no.1 train no.98200  loss = 5.77642 avg_loss = 3.85021\n",
      "epoch no.1 train no.98210  loss = 2.10627 avg_loss = 3.78555\n",
      "epoch no.1 train no.98220  loss = 2.99755 avg_loss = 3.81642\n",
      "epoch no.1 train no.98230  loss = 3.70452 avg_loss = 3.82033\n",
      "epoch no.1 train no.98240  loss = 3.20492 avg_loss = 3.80599\n",
      "epoch no.1 train no.98250  loss = 4.68921 avg_loss = 3.77166\n",
      "epoch no.1 train no.98260  loss = 4.32332 avg_loss = 3.77280\n",
      "epoch no.1 train no.98270  loss = 4.86642 avg_loss = 3.85004\n",
      "epoch no.1 train no.98280  loss = 4.57819 avg_loss = 3.82007\n",
      "epoch no.1 train no.98290  loss = 2.42987 avg_loss = 3.78268\n",
      "epoch no.1 train no.98300  loss = 3.23110 avg_loss = 3.82482\n",
      "epoch no.1 train no.98310  loss = 2.32794 avg_loss = 3.82693\n",
      "epoch no.1 train no.98320  loss = 3.03371 avg_loss = 3.82294\n",
      "epoch no.1 train no.98330  loss = 4.94585 avg_loss = 3.86637\n",
      "epoch no.1 train no.98340  loss = 4.11669 avg_loss = 3.90854\n",
      "epoch no.1 train no.98350  loss = 4.57719 avg_loss = 3.95431\n",
      "epoch no.1 train no.98360  loss = 4.02483 avg_loss = 3.93452\n",
      "epoch no.1 train no.98370  loss = 2.09714 avg_loss = 3.84924\n",
      "epoch no.1 train no.98380  loss = 5.74816 avg_loss = 3.88560\n",
      "epoch no.1 train no.98390  loss = 2.72182 avg_loss = 3.89915\n",
      "epoch no.1 train no.98400  loss = 6.99800 avg_loss = 3.95739\n",
      "epoch no.1 train no.98410  loss = 2.53580 avg_loss = 3.92153\n",
      "epoch no.1 train no.98420  loss = 4.11360 avg_loss = 3.90064\n",
      "epoch no.1 train no.98430  loss = 1.95592 avg_loss = 3.85169\n",
      "epoch no.1 train no.98440  loss = 2.95715 avg_loss = 3.85085\n",
      "epoch no.1 train no.98450  loss = 5.34273 avg_loss = 3.82778\n",
      "epoch no.1 train no.98460  loss = 2.37716 avg_loss = 3.78608\n",
      "epoch no.1 train no.98470  loss = 3.98539 avg_loss = 3.85842\n",
      "epoch no.1 train no.98480  loss = 2.72942 avg_loss = 3.85315\n",
      "epoch no.1 train no.98490  loss = 2.88355 avg_loss = 3.86089\n",
      "epoch no.1 train no.98500  loss = 3.06766 avg_loss = 3.88436\n",
      "epoch no.1 train no.98510  loss = 3.15421 avg_loss = 3.86382\n",
      "epoch no.1 train no.98520  loss = 3.48921 avg_loss = 3.83382\n",
      "epoch no.1 train no.98530  loss = 3.80965 avg_loss = 3.81274\n",
      "epoch no.1 train no.98540  loss = 3.91139 avg_loss = 3.83706\n",
      "epoch no.1 train no.98550  loss = 4.47991 avg_loss = 3.85948\n",
      "epoch no.1 train no.98560  loss = 3.75784 avg_loss = 3.84688\n",
      "epoch no.1 train no.98570  loss = 2.35999 avg_loss = 3.84645\n",
      "epoch no.1 train no.98580  loss = 3.01507 avg_loss = 3.87959\n",
      "epoch no.1 train no.98590  loss = 3.90297 avg_loss = 3.86696\n",
      "epoch no.1 train no.98600  loss = 5.06354 avg_loss = 3.82926\n",
      "epoch no.1 train no.98610  loss = 3.81723 avg_loss = 3.80532\n",
      "epoch no.1 train no.98620  loss = 4.05397 avg_loss = 3.77098\n",
      "epoch no.1 train no.98630  loss = 2.59029 avg_loss = 3.70160\n",
      "epoch no.1 train no.98640  loss = 2.22949 avg_loss = 3.67369\n",
      "epoch no.1 train no.98650  loss = 4.06022 avg_loss = 3.71546\n",
      "epoch no.1 train no.98660  loss = 4.66866 avg_loss = 3.72180\n",
      "epoch no.1 train no.98670  loss = 4.50233 avg_loss = 3.72135\n",
      "epoch no.1 train no.98680  loss = 2.75389 avg_loss = 3.71379\n",
      "epoch no.1 train no.98690  loss = 1.56178 avg_loss = 3.70742\n",
      "epoch no.1 train no.98700  loss = 2.41751 avg_loss = 3.70660\n",
      "epoch no.1 train no.98710  loss = 2.58129 avg_loss = 3.62304\n",
      "epoch no.1 train no.98720  loss = 5.16633 avg_loss = 3.71480\n",
      "epoch no.1 train no.98730  loss = 3.15139 avg_loss = 3.73250\n",
      "epoch no.1 train no.98740  loss = 3.87997 avg_loss = 3.71840\n",
      "epoch no.1 train no.98750  loss = 4.39953 avg_loss = 3.76088\n",
      "epoch no.1 train no.98760  loss = 3.70618 avg_loss = 3.76625\n",
      "epoch no.1 train no.98770  loss = 2.99325 avg_loss = 3.72976\n",
      "epoch no.1 train no.98780  loss = 2.77060 avg_loss = 3.73082\n",
      "epoch no.1 train no.98790  loss = 4.77350 avg_loss = 3.70686\n",
      "epoch no.1 train no.98800  loss = 3.46317 avg_loss = 3.72500\n",
      "epoch no.1 train no.98810  loss = 4.10800 avg_loss = 3.71232\n",
      "epoch no.1 train no.98820  loss = 2.90507 avg_loss = 3.71669\n",
      "epoch no.1 train no.98830  loss = 2.67872 avg_loss = 3.65429\n",
      "epoch no.1 train no.98840  loss = 3.43898 avg_loss = 3.65891\n",
      "epoch no.1 train no.98850  loss = 2.61967 avg_loss = 3.67942\n",
      "epoch no.1 train no.98860  loss = 2.61491 avg_loss = 3.67597\n",
      "epoch no.1 train no.98870  loss = 4.12536 avg_loss = 3.71140\n",
      "epoch no.1 train no.98880  loss = 1.80628 avg_loss = 3.68927\n",
      "epoch no.1 train no.98890  loss = 5.70127 avg_loss = 3.72023\n",
      "epoch no.1 train no.98900  loss = 5.44012 avg_loss = 3.70289\n",
      "epoch no.1 train no.98910  loss = 3.99346 avg_loss = 3.71011\n",
      "epoch no.1 train no.98920  loss = 3.76984 avg_loss = 3.71382\n",
      "epoch no.1 train no.98930  loss = 5.89166 avg_loss = 3.77712\n",
      "epoch no.1 train no.98940  loss = 3.98510 avg_loss = 3.82033\n",
      "epoch no.1 train no.98950  loss = 4.22472 avg_loss = 3.84834\n",
      "epoch no.1 train no.98960  loss = 2.87123 avg_loss = 3.85415\n",
      "epoch no.1 train no.98970  loss = 2.64051 avg_loss = 3.80410\n",
      "epoch no.1 train no.98980  loss = 3.11362 avg_loss = 3.82479\n",
      "epoch no.1 train no.98990  loss = 5.75622 avg_loss = 3.80649\n",
      "epoch no.1 train no.99000  loss = 3.99905 avg_loss = 3.80300\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '송', '</s>']\n",
      "기분전환을 위한 팝송</s>\n",
      "epoch no.1 train no.99010  loss = 4.39173 avg_loss = 3.87536\n",
      "epoch no.1 train no.99020  loss = 3.63340 avg_loss = 3.83182\n",
      "epoch no.1 train no.99030  loss = 3.97661 avg_loss = 3.86779\n",
      "epoch no.1 train no.99040  loss = 3.30169 avg_loss = 3.84106\n",
      "epoch no.1 train no.99050  loss = 5.24302 avg_loss = 3.80717\n",
      "epoch no.1 train no.99060  loss = 2.46329 avg_loss = 3.83100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.99070  loss = 2.11425 avg_loss = 3.78065\n",
      "epoch no.1 train no.99080  loss = 2.25943 avg_loss = 3.75677\n",
      "epoch no.1 train no.99090  loss = 2.60795 avg_loss = 3.77368\n",
      "epoch no.1 train no.99100  loss = 2.62975 avg_loss = 3.80585\n",
      "epoch no.1 train no.99110  loss = 3.49198 avg_loss = 3.80152\n",
      "epoch no.1 train no.99120  loss = 3.30470 avg_loss = 3.83946\n",
      "epoch no.1 train no.99130  loss = 3.76415 avg_loss = 3.82932\n",
      "epoch no.1 train no.99140  loss = 3.82989 avg_loss = 3.84077\n",
      "epoch no.1 train no.99150  loss = 5.36591 avg_loss = 3.83396\n",
      "epoch no.1 train no.99160  loss = 3.03182 avg_loss = 3.78806\n",
      "epoch no.1 train no.99170  loss = 4.98732 avg_loss = 3.84906\n",
      "epoch no.1 train no.99180  loss = 3.55537 avg_loss = 3.84444\n",
      "epoch no.1 train no.99190  loss = 3.81798 avg_loss = 3.81525\n",
      "epoch no.1 train no.99200  loss = 2.18657 avg_loss = 3.79990\n",
      "epoch no.1 train no.99210  loss = 1.93546 avg_loss = 3.77288\n",
      "epoch no.1 train no.99220  loss = 3.68080 avg_loss = 3.75244\n",
      "epoch no.1 train no.99230  loss = 2.15042 avg_loss = 3.77167\n",
      "epoch no.1 train no.99240  loss = 3.40149 avg_loss = 3.75144\n",
      "epoch no.1 train no.99250  loss = 4.30834 avg_loss = 3.72636\n",
      "epoch no.1 train no.99260  loss = 2.30822 avg_loss = 3.70507\n",
      "epoch no.1 train no.99270  loss = 2.83371 avg_loss = 3.67659\n",
      "epoch no.1 train no.99280  loss = 5.50766 avg_loss = 3.71798\n",
      "epoch no.1 train no.99290  loss = 4.17298 avg_loss = 3.68539\n",
      "epoch no.1 train no.99300  loss = 2.62675 avg_loss = 3.72999\n",
      "epoch no.1 train no.99310  loss = 4.40312 avg_loss = 3.75306\n",
      "epoch no.1 train no.99320  loss = 3.93160 avg_loss = 3.72896\n",
      "epoch no.1 train no.99330  loss = 2.40420 avg_loss = 3.68188\n",
      "epoch no.1 train no.99340  loss = 5.44464 avg_loss = 3.69072\n",
      "epoch no.1 train no.99350  loss = 5.42957 avg_loss = 3.72916\n",
      "epoch no.1 train no.99360  loss = 3.38761 avg_loss = 3.69680\n",
      "epoch no.1 train no.99370  loss = 5.13218 avg_loss = 3.69032\n",
      "epoch no.1 train no.99380  loss = 2.12827 avg_loss = 3.64343\n",
      "epoch no.1 train no.99390  loss = 2.50556 avg_loss = 3.62021\n",
      "epoch no.1 train no.99400  loss = 5.56209 avg_loss = 3.65276\n",
      "epoch no.1 train no.99410  loss = 3.42447 avg_loss = 3.64814\n",
      "epoch no.1 train no.99420  loss = 3.63888 avg_loss = 3.65104\n",
      "epoch no.1 train no.99430  loss = 5.61207 avg_loss = 3.70494\n",
      "epoch no.1 train no.99440  loss = 3.02722 avg_loss = 3.67473\n",
      "epoch no.1 train no.99450  loss = 4.85637 avg_loss = 3.76692\n",
      "epoch no.1 train no.99460  loss = 2.51599 avg_loss = 3.77329\n",
      "epoch no.1 train no.99470  loss = 3.91132 avg_loss = 3.74070\n",
      "epoch no.1 train no.99480  loss = 4.87231 avg_loss = 3.72723\n",
      "epoch no.1 train no.99490  loss = 3.04159 avg_loss = 3.72534\n",
      "epoch no.1 train no.99500  loss = 2.31500 avg_loss = 3.69209\n",
      "epoch no.1 train no.99510  loss = 3.94607 avg_loss = 3.66419\n",
      "epoch no.1 train no.99520  loss = 3.79828 avg_loss = 3.67861\n",
      "epoch no.1 train no.99530  loss = 3.23512 avg_loss = 3.66129\n",
      "epoch no.1 train no.99540  loss = 2.89395 avg_loss = 3.65944\n",
      "epoch no.1 train no.99550  loss = 2.87662 avg_loss = 3.66833\n",
      "epoch no.1 train no.99560  loss = 5.55397 avg_loss = 3.70479\n",
      "epoch no.1 train no.99570  loss = 4.12537 avg_loss = 3.68035\n",
      "epoch no.1 train no.99580  loss = 2.79324 avg_loss = 3.69020\n",
      "epoch no.1 train no.99590  loss = 2.58881 avg_loss = 3.69877\n",
      "epoch no.1 train no.99600  loss = 4.89677 avg_loss = 3.71674\n",
      "epoch no.1 train no.99610  loss = 2.39352 avg_loss = 3.70212\n",
      "epoch no.1 train no.99620  loss = 2.15153 avg_loss = 3.68488\n",
      "epoch no.1 train no.99630  loss = 3.06032 avg_loss = 3.66695\n",
      "epoch no.1 train no.99640  loss = 3.94161 avg_loss = 3.71122\n",
      "epoch no.1 train no.99650  loss = 3.95441 avg_loss = 3.69693\n",
      "epoch no.1 train no.99660  loss = 2.91417 avg_loss = 3.70053\n",
      "epoch no.1 train no.99670  loss = 4.96795 avg_loss = 3.70181\n",
      "epoch no.1 train no.99680  loss = 4.49767 avg_loss = 3.68084\n",
      "epoch no.1 train no.99690  loss = 4.07075 avg_loss = 3.66029\n",
      "epoch no.1 train no.99700  loss = 2.50148 avg_loss = 3.69422\n",
      "epoch no.1 train no.99710  loss = 4.25380 avg_loss = 3.69059\n",
      "epoch no.1 train no.99720  loss = 3.38732 avg_loss = 3.65813\n",
      "epoch no.1 train no.99730  loss = 3.52095 avg_loss = 3.67840\n",
      "epoch no.1 train no.99740  loss = 2.32433 avg_loss = 3.69495\n",
      "epoch no.1 train no.99750  loss = 4.72404 avg_loss = 3.72124\n",
      "epoch no.1 train no.99760  loss = 4.26619 avg_loss = 3.71051\n",
      "epoch no.1 train no.99770  loss = 2.58186 avg_loss = 3.69642\n",
      "epoch no.1 train no.99780  loss = 3.77888 avg_loss = 3.64218\n",
      "epoch no.1 train no.99790  loss = 3.10980 avg_loss = 3.63700\n",
      "epoch no.1 train no.99800  loss = 5.38378 avg_loss = 3.70102\n",
      "epoch no.1 train no.99810  loss = 5.14824 avg_loss = 3.71829\n",
      "epoch no.1 train no.99820  loss = 4.20614 avg_loss = 3.78991\n",
      "epoch no.1 train no.99830  loss = 2.92156 avg_loss = 3.79798\n",
      "epoch no.1 train no.99840  loss = 5.38884 avg_loss = 3.80728\n",
      "epoch no.1 train no.99850  loss = 4.25101 avg_loss = 3.79385\n",
      "epoch no.1 train no.99860  loss = 3.50712 avg_loss = 3.77338\n",
      "epoch no.1 train no.99870  loss = 1.43764 avg_loss = 3.76899\n",
      "epoch no.1 train no.99880  loss = 3.19608 avg_loss = 3.71715\n",
      "epoch no.1 train no.99890  loss = 3.98322 avg_loss = 3.70145\n",
      "epoch no.1 train no.99900  loss = 3.10510 avg_loss = 3.68639\n",
      "epoch no.1 train no.99910  loss = 4.07502 avg_loss = 3.67398\n",
      "epoch no.1 train no.99920  loss = 4.02226 avg_loss = 3.68488\n",
      "epoch no.1 train no.99930  loss = 4.13127 avg_loss = 3.75260\n",
      "epoch no.1 train no.99940  loss = 3.86603 avg_loss = 3.71016\n",
      "epoch no.1 train no.99950  loss = 2.19968 avg_loss = 3.66227\n",
      "epoch no.1 train no.99960  loss = 4.93368 avg_loss = 3.70977\n",
      "epoch no.1 train no.99970  loss = 3.60441 avg_loss = 3.64377\n",
      "epoch no.1 train no.99980  loss = 6.53740 avg_loss = 3.65911\n",
      "epoch no.1 train no.99990  loss = 2.10690 avg_loss = 3.65459\n",
      "epoch no.1 train no.100000  loss = 3.99253 avg_loss = 3.63981\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '▁노래', '</s>']\n",
      "기분전환을 위한 신나는 노래</s>\n",
      "epoch no.1 train no.100010  loss = 2.78685 avg_loss = 3.64241\n",
      "epoch no.1 train no.100020  loss = 3.64675 avg_loss = 3.64110\n",
      "epoch no.1 train no.100030  loss = 3.90995 avg_loss = 3.65142\n",
      "epoch no.1 train no.100040  loss = 2.72524 avg_loss = 3.77021\n",
      "epoch no.1 train no.100050  loss = 4.80897 avg_loss = 3.72205\n",
      "epoch no.1 train no.100060  loss = 2.77563 avg_loss = 3.71632\n",
      "epoch no.1 train no.100070  loss = 4.81094 avg_loss = 3.70012\n",
      "epoch no.1 train no.100080  loss = 3.75763 avg_loss = 3.70990\n",
      "epoch no.1 train no.100090  loss = 1.67059 avg_loss = 3.73892\n",
      "epoch no.1 train no.100100  loss = 6.90575 avg_loss = 3.79461\n",
      "epoch no.1 train no.100110  loss = 3.93340 avg_loss = 3.80400\n",
      "epoch no.1 train no.100120  loss = 2.41894 avg_loss = 3.78386\n",
      "epoch no.1 train no.100130  loss = 2.44516 avg_loss = 3.71522\n",
      "epoch no.1 train no.100140  loss = 2.53875 avg_loss = 3.68311\n",
      "epoch no.1 train no.100150  loss = 3.63914 avg_loss = 3.72871\n",
      "epoch no.1 train no.100160  loss = 5.99727 avg_loss = 3.75887\n",
      "epoch no.1 train no.100170  loss = 3.52327 avg_loss = 3.70150\n",
      "epoch no.1 train no.100180  loss = 4.67419 avg_loss = 3.68335\n",
      "epoch no.1 train no.100190  loss = 5.95926 avg_loss = 3.72421\n",
      "epoch no.1 train no.100200  loss = 4.70349 avg_loss = 3.79805\n",
      "epoch no.1 train no.100210  loss = 4.66452 avg_loss = 3.80265\n",
      "epoch no.1 train no.100220  loss = 2.92063 avg_loss = 3.76316\n",
      "epoch no.1 train no.100230  loss = 3.92096 avg_loss = 3.70918\n",
      "epoch no.1 train no.100240  loss = 2.44390 avg_loss = 3.64154\n",
      "epoch no.1 train no.100250  loss = 2.65696 avg_loss = 3.65234\n",
      "epoch no.1 train no.100260  loss = 4.10327 avg_loss = 3.67055\n",
      "epoch no.1 train no.100270  loss = 3.66095 avg_loss = 3.63183\n",
      "epoch no.1 train no.100280  loss = 2.68621 avg_loss = 3.63208\n",
      "epoch no.1 train no.100290  loss = 2.63209 avg_loss = 3.66232\n",
      "epoch no.1 train no.100300  loss = 3.39439 avg_loss = 3.65243\n",
      "epoch no.1 train no.100310  loss = 3.95022 avg_loss = 3.73015\n",
      "epoch no.1 train no.100320  loss = 4.97330 avg_loss = 3.76836\n",
      "epoch no.1 train no.100330  loss = 3.66711 avg_loss = 3.71902\n",
      "epoch no.1 train no.100340  loss = 5.14388 avg_loss = 3.68533\n",
      "epoch no.1 train no.100350  loss = 4.63849 avg_loss = 3.68521\n",
      "epoch no.1 train no.100360  loss = 3.99123 avg_loss = 3.73710\n",
      "epoch no.1 train no.100370  loss = 2.14402 avg_loss = 3.74208\n",
      "epoch no.1 train no.100380  loss = 2.68714 avg_loss = 3.68341\n",
      "epoch no.1 train no.100390  loss = 3.84631 avg_loss = 3.68443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.100400  loss = 4.29719 avg_loss = 3.68543\n",
      "epoch no.1 train no.100410  loss = 4.67080 avg_loss = 3.65528\n",
      "epoch no.1 train no.100420  loss = 1.97775 avg_loss = 3.63821\n",
      "epoch no.1 train no.100430  loss = 3.93415 avg_loss = 3.62305\n",
      "epoch no.1 train no.100440  loss = 2.74416 avg_loss = 3.58562\n",
      "epoch no.1 train no.100450  loss = 5.80641 avg_loss = 3.61729\n",
      "epoch no.1 train no.100460  loss = 2.78528 avg_loss = 3.57039\n",
      "epoch no.1 train no.100470  loss = 3.22569 avg_loss = 3.56759\n",
      "epoch no.1 train no.100480  loss = 4.07586 avg_loss = 3.58315\n",
      "epoch no.1 train no.100490  loss = 4.87125 avg_loss = 3.62824\n",
      "epoch no.1 train no.100500  loss = 3.06578 avg_loss = 3.62122\n",
      "epoch no.1 train no.100510  loss = 4.42035 avg_loss = 3.59262\n",
      "epoch no.1 train no.100520  loss = 3.39869 avg_loss = 3.63896\n",
      "epoch no.1 train no.100530  loss = 4.36736 avg_loss = 3.67315\n",
      "epoch no.1 train no.100540  loss = 2.26259 avg_loss = 3.62677\n",
      "epoch no.1 train no.100550  loss = 4.18360 avg_loss = 3.58720\n",
      "epoch no.1 train no.100560  loss = 2.14421 avg_loss = 3.59176\n",
      "epoch no.1 train no.100570  loss = 5.42415 avg_loss = 3.62228\n",
      "epoch no.1 train no.100580  loss = 2.93356 avg_loss = 3.63847\n",
      "epoch no.1 train no.100590  loss = 2.64937 avg_loss = 3.64020\n",
      "epoch no.1 train no.100600  loss = 3.07184 avg_loss = 3.67814\n",
      "epoch no.1 train no.100610  loss = 2.95142 avg_loss = 3.68577\n",
      "epoch no.1 train no.100620  loss = 5.15906 avg_loss = 3.72513\n",
      "epoch no.1 train no.100630  loss = 2.95785 avg_loss = 3.71101\n",
      "epoch no.1 train no.100640  loss = 4.75222 avg_loss = 3.72287\n",
      "epoch no.1 train no.100650  loss = 3.34120 avg_loss = 3.73139\n",
      "epoch no.1 train no.100660  loss = 4.27079 avg_loss = 3.70702\n",
      "epoch no.1 train no.100670  loss = 3.41341 avg_loss = 3.71131\n",
      "epoch no.1 train no.100680  loss = 3.34966 avg_loss = 3.68038\n",
      "epoch no.1 train no.100690  loss = 2.57741 avg_loss = 3.70875\n",
      "epoch no.1 train no.100700  loss = 6.07910 avg_loss = 3.68829\n",
      "epoch no.1 train no.100710  loss = 2.93776 avg_loss = 3.66851\n",
      "epoch no.1 train no.100720  loss = 2.79768 avg_loss = 3.64027\n",
      "epoch no.1 train no.100730  loss = 4.22712 avg_loss = 3.72368\n",
      "epoch no.1 train no.100740  loss = 2.92563 avg_loss = 3.72858\n",
      "epoch no.1 train no.100750  loss = 2.71253 avg_loss = 3.67262\n",
      "epoch no.1 train no.100760  loss = 3.45010 avg_loss = 3.68750\n",
      "epoch no.1 train no.100770  loss = 2.42135 avg_loss = 3.70486\n",
      "epoch no.1 train no.100780  loss = 2.83373 avg_loss = 3.74508\n",
      "epoch no.1 train no.100790  loss = 3.69766 avg_loss = 3.76659\n",
      "epoch no.1 train no.100800  loss = 3.80811 avg_loss = 3.75066\n",
      "epoch no.1 train no.100810  loss = 2.67348 avg_loss = 3.72377\n",
      "epoch no.1 train no.100820  loss = 3.47259 avg_loss = 3.74359\n",
      "epoch no.1 train no.100830  loss = 2.50362 avg_loss = 3.74360\n",
      "epoch no.1 train no.100840  loss = 3.18965 avg_loss = 3.77025\n",
      "epoch no.1 train no.100850  loss = 3.87702 avg_loss = 3.79772\n",
      "epoch no.1 train no.100860  loss = 4.39925 avg_loss = 3.79599\n",
      "epoch no.1 train no.100870  loss = 2.50937 avg_loss = 3.77223\n",
      "epoch no.1 train no.100880  loss = 4.17187 avg_loss = 3.78136\n",
      "epoch no.1 train no.100890  loss = 2.16358 avg_loss = 3.79290\n",
      "epoch no.1 train no.100900  loss = 2.26510 avg_loss = 3.74911\n",
      "epoch no.1 train no.100910  loss = 6.83846 avg_loss = 3.79496\n",
      "epoch no.1 train no.100920  loss = 5.33493 avg_loss = 3.85013\n",
      "epoch no.1 train no.100930  loss = 2.55684 avg_loss = 3.85233\n",
      "epoch no.1 train no.100940  loss = 2.87394 avg_loss = 3.79918\n",
      "epoch no.1 train no.100950  loss = 3.67175 avg_loss = 3.84826\n",
      "epoch no.1 train no.100960  loss = 3.58100 avg_loss = 3.82015\n",
      "epoch no.1 train no.100970  loss = 4.60095 avg_loss = 3.82152\n",
      "epoch no.1 train no.100980  loss = 3.24878 avg_loss = 3.79897\n",
      "epoch no.1 train no.100990  loss = 3.66130 avg_loss = 3.84265\n",
      "epoch no.1 train no.101000  loss = 3.45271 avg_loss = 3.78986\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁딱', '▁좋은', '▁노래', '송', '</s>']\n",
      "기분전환에 딱 좋은 팝송</s>\n",
      "epoch no.1 train no.101010  loss = 2.31852 avg_loss = 3.74059\n",
      "epoch no.1 train no.101020  loss = 4.10492 avg_loss = 3.76191\n",
      "epoch no.1 train no.101030  loss = 4.27638 avg_loss = 3.81358\n",
      "epoch no.1 train no.101040  loss = 4.52619 avg_loss = 3.80999\n",
      "epoch no.1 train no.101050  loss = 3.72474 avg_loss = 3.81192\n",
      "epoch no.1 train no.101060  loss = 5.74144 avg_loss = 3.82141\n",
      "epoch no.1 train no.101070  loss = 3.81173 avg_loss = 3.82446\n",
      "epoch no.1 train no.101080  loss = 3.45086 avg_loss = 3.81073\n",
      "epoch no.1 train no.101090  loss = 3.52623 avg_loss = 3.77706\n",
      "epoch no.1 train no.101100  loss = 3.72472 avg_loss = 3.80501\n",
      "epoch no.1 train no.101110  loss = 2.11852 avg_loss = 3.77639\n",
      "epoch no.1 train no.101120  loss = 4.89853 avg_loss = 3.79216\n",
      "epoch no.1 train no.101130  loss = 2.42328 avg_loss = 3.78310\n",
      "epoch no.1 train no.101140  loss = 2.58450 avg_loss = 3.82987\n",
      "epoch no.1 train no.101150  loss = 4.44868 avg_loss = 3.82934\n",
      "epoch no.1 train no.101160  loss = 1.87541 avg_loss = 3.85477\n",
      "epoch no.1 train no.101170  loss = 4.19889 avg_loss = 3.86272\n",
      "epoch no.1 train no.101180  loss = 3.75786 avg_loss = 3.88628\n",
      "epoch no.1 train no.101190  loss = 4.62775 avg_loss = 3.91920\n",
      "epoch no.1 train no.101200  loss = 3.88774 avg_loss = 3.92872\n",
      "epoch no.1 train no.101210  loss = 3.11501 avg_loss = 3.88192\n",
      "epoch no.1 train no.101220  loss = 4.41914 avg_loss = 3.87352\n",
      "epoch no.1 train no.101230  loss = 3.13279 avg_loss = 3.86415\n",
      "epoch no.1 train no.101240  loss = 2.74031 avg_loss = 3.79062\n",
      "epoch no.1 train no.101250  loss = 4.00606 avg_loss = 3.82584\n",
      "epoch no.1 train no.101260  loss = 3.44611 avg_loss = 3.85229\n",
      "epoch no.1 train no.101270  loss = 2.99276 avg_loss = 3.83724\n",
      "epoch no.1 train no.101280  loss = 2.22900 avg_loss = 3.82694\n",
      "epoch no.1 train no.101290  loss = 2.93251 avg_loss = 3.75414\n",
      "epoch no.1 train no.101300  loss = 2.37690 avg_loss = 3.69644\n",
      "epoch no.1 train no.101310  loss = 1.60487 avg_loss = 3.62719\n",
      "epoch no.1 train no.101320  loss = 5.49134 avg_loss = 3.65207\n",
      "epoch no.1 train no.101330  loss = 3.66290 avg_loss = 3.70126\n",
      "epoch no.1 train no.101340  loss = 4.72423 avg_loss = 3.72483\n",
      "epoch no.1 train no.101350  loss = 3.34029 avg_loss = 3.70643\n",
      "epoch no.1 train no.101360  loss = 4.22033 avg_loss = 3.69147\n",
      "epoch no.1 train no.101370  loss = 3.01404 avg_loss = 3.65412\n",
      "epoch no.1 train no.101380  loss = 3.10860 avg_loss = 3.65801\n",
      "epoch no.1 train no.101390  loss = 1.68219 avg_loss = 3.61002\n",
      "epoch no.1 train no.101400  loss = 3.06972 avg_loss = 3.66321\n",
      "epoch no.1 train no.101410  loss = 4.81991 avg_loss = 3.68419\n",
      "epoch no.1 train no.101420  loss = 3.62338 avg_loss = 3.72140\n",
      "epoch no.1 train no.101430  loss = 3.69302 avg_loss = 3.69258\n",
      "epoch no.1 train no.101440  loss = 3.08980 avg_loss = 3.62733\n",
      "epoch no.1 train no.101450  loss = 3.16414 avg_loss = 3.63901\n",
      "epoch no.1 train no.101460  loss = 3.61946 avg_loss = 3.69461\n",
      "epoch no.1 train no.101470  loss = 4.07861 avg_loss = 3.76526\n",
      "epoch no.1 train no.101480  loss = 4.30640 avg_loss = 3.77922\n",
      "epoch no.1 train no.101490  loss = 3.97231 avg_loss = 3.82444\n",
      "epoch no.1 train no.101500  loss = 3.56290 avg_loss = 3.80215\n",
      "epoch no.1 train no.101510  loss = 4.23179 avg_loss = 3.81642\n",
      "epoch no.1 train no.101520  loss = 3.98618 avg_loss = 3.78761\n",
      "epoch no.1 train no.101530  loss = 4.38550 avg_loss = 3.77969\n",
      "epoch no.1 train no.101540  loss = 5.83226 avg_loss = 3.79432\n",
      "epoch no.1 train no.101550  loss = 7.38958 avg_loss = 3.79841\n",
      "epoch no.1 train no.101560  loss = 2.74363 avg_loss = 3.78489\n",
      "epoch no.1 train no.101570  loss = 3.24501 avg_loss = 3.78920\n",
      "epoch no.1 train no.101580  loss = 3.30232 avg_loss = 3.79704\n",
      "epoch no.1 train no.101590  loss = 5.83153 avg_loss = 3.82064\n",
      "epoch no.1 train no.101600  loss = 2.38812 avg_loss = 3.78900\n",
      "epoch no.1 train no.101610  loss = 3.72230 avg_loss = 3.76054\n",
      "epoch no.1 train no.101620  loss = 4.19212 avg_loss = 3.77365\n",
      "epoch no.1 train no.101630  loss = 3.42852 avg_loss = 3.77928\n",
      "epoch no.1 train no.101640  loss = 6.00460 avg_loss = 3.79014\n",
      "epoch no.1 train no.101650  loss = 4.78240 avg_loss = 3.77587\n",
      "epoch no.1 train no.101660  loss = 3.75117 avg_loss = 3.76856\n",
      "epoch no.1 train no.101670  loss = 3.15676 avg_loss = 3.81254\n",
      "epoch no.1 train no.101680  loss = 2.24724 avg_loss = 3.78558\n",
      "epoch no.1 train no.101690  loss = 3.06607 avg_loss = 3.77278\n",
      "epoch no.1 train no.101700  loss = 2.96587 avg_loss = 3.71565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.101710  loss = 2.96505 avg_loss = 3.71042\n",
      "epoch no.1 train no.101720  loss = 2.71526 avg_loss = 3.70083\n",
      "epoch no.1 train no.101730  loss = 3.73008 avg_loss = 3.71294\n",
      "epoch no.1 train no.101740  loss = 4.69693 avg_loss = 3.67555\n",
      "epoch no.1 train no.101750  loss = 3.16768 avg_loss = 3.68015\n",
      "epoch no.1 train no.101760  loss = 3.28015 avg_loss = 3.67040\n",
      "epoch no.1 train no.101770  loss = 2.76400 avg_loss = 3.70169\n",
      "epoch no.1 train no.101780  loss = 2.82086 avg_loss = 3.66855\n",
      "epoch no.1 train no.101790  loss = 4.76760 avg_loss = 3.77733\n",
      "epoch no.1 train no.101800  loss = 2.88075 avg_loss = 3.75165\n",
      "epoch no.1 train no.101810  loss = 2.11192 avg_loss = 3.75601\n",
      "epoch no.1 train no.101820  loss = 2.86820 avg_loss = 3.74978\n",
      "epoch no.1 train no.101830  loss = 2.31508 avg_loss = 3.71466\n",
      "epoch no.1 train no.101840  loss = 2.57146 avg_loss = 3.70976\n",
      "epoch no.1 train no.101850  loss = 4.87240 avg_loss = 3.72814\n",
      "epoch no.1 train no.101860  loss = 2.48646 avg_loss = 3.71480\n",
      "epoch no.1 train no.101870  loss = 3.87643 avg_loss = 3.69262\n",
      "epoch no.1 train no.101880  loss = 6.55376 avg_loss = 3.72080\n",
      "epoch no.1 train no.101890  loss = 3.80808 avg_loss = 3.68842\n",
      "epoch no.1 train no.101900  loss = 4.04282 avg_loss = 3.74683\n",
      "epoch no.1 train no.101910  loss = 5.08534 avg_loss = 3.77664\n",
      "epoch no.1 train no.101920  loss = 3.45890 avg_loss = 3.77941\n",
      "epoch no.1 train no.101930  loss = 4.12766 avg_loss = 3.82258\n",
      "epoch no.1 train no.101940  loss = 1.62717 avg_loss = 3.78268\n",
      "epoch no.1 train no.101950  loss = 4.52112 avg_loss = 3.75527\n",
      "epoch no.1 train no.101960  loss = 5.69115 avg_loss = 3.80898\n",
      "epoch no.1 train no.101970  loss = 3.48240 avg_loss = 3.78975\n",
      "epoch no.1 train no.101980  loss = 4.33745 avg_loss = 3.78857\n",
      "epoch no.1 train no.101990  loss = 4.33450 avg_loss = 3.74242\n",
      "epoch no.1 train no.102000  loss = 2.55277 avg_loss = 3.75923\n",
      "5\n",
      "to_tokens: ['▁비', '좋은', '이', '▁필요할', '▁때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 노래</s>\n",
      "epoch no.1 train no.102010  loss = 4.65340 avg_loss = 3.74443\n",
      "epoch no.1 train no.102020  loss = 4.85769 avg_loss = 3.76057\n",
      "epoch no.1 train no.102030  loss = 4.07955 avg_loss = 3.73145\n",
      "epoch no.1 train no.102040  loss = 2.38366 avg_loss = 3.71155\n",
      "epoch no.1 train no.102050  loss = 3.54830 avg_loss = 3.64632\n",
      "epoch no.1 train no.102060  loss = 2.93784 avg_loss = 3.58204\n",
      "epoch no.1 train no.102070  loss = 3.68172 avg_loss = 3.54941\n",
      "epoch no.1 train no.102080  loss = 4.03364 avg_loss = 3.56653\n",
      "epoch no.1 train no.102090  loss = 2.28227 avg_loss = 3.55080\n",
      "epoch no.1 train no.102100  loss = 2.99835 avg_loss = 3.56032\n",
      "epoch no.1 train no.102110  loss = 2.75477 avg_loss = 3.58120\n",
      "epoch no.1 train no.102120  loss = 3.43231 avg_loss = 3.61332\n",
      "epoch no.1 train no.102130  loss = 6.05469 avg_loss = 3.67253\n",
      "epoch no.1 train no.102140  loss = 4.83427 avg_loss = 3.66792\n",
      "epoch no.1 train no.102150  loss = 3.51775 avg_loss = 3.73245\n",
      "epoch no.1 train no.102160  loss = 2.91830 avg_loss = 3.70335\n",
      "epoch no.1 train no.102170  loss = 3.26279 avg_loss = 3.68279\n",
      "epoch no.1 train no.102180  loss = 1.67040 avg_loss = 3.71230\n",
      "epoch no.1 train no.102190  loss = 6.82733 avg_loss = 3.77482\n",
      "epoch no.1 train no.102200  loss = 3.08775 avg_loss = 3.79085\n",
      "epoch no.1 train no.102210  loss = 1.93598 avg_loss = 3.74516\n",
      "epoch no.1 train no.102220  loss = 4.30666 avg_loss = 3.74414\n",
      "epoch no.1 train no.102230  loss = 4.46741 avg_loss = 3.73864\n",
      "epoch no.1 train no.102240  loss = 4.26481 avg_loss = 3.76154\n",
      "epoch no.1 train no.102250  loss = 2.30295 avg_loss = 3.71044\n",
      "epoch no.1 train no.102260  loss = 4.14051 avg_loss = 3.76822\n",
      "epoch no.1 train no.102270  loss = 4.28415 avg_loss = 3.79337\n",
      "epoch no.1 train no.102280  loss = 2.98463 avg_loss = 3.72763\n",
      "epoch no.1 train no.102290  loss = 2.95346 avg_loss = 3.75729\n",
      "epoch no.1 train no.102300  loss = 3.36133 avg_loss = 3.75995\n",
      "epoch no.1 train no.102310  loss = 2.66062 avg_loss = 3.73010\n",
      "epoch no.1 train no.102320  loss = 3.96321 avg_loss = 3.76122\n",
      "epoch no.1 train no.102330  loss = 3.65154 avg_loss = 3.76046\n",
      "epoch no.1 train no.102340  loss = 4.03387 avg_loss = 3.73194\n",
      "epoch no.1 train no.102350  loss = 4.59174 avg_loss = 3.73637\n",
      "epoch no.1 train no.102360  loss = 4.31386 avg_loss = 3.79261\n",
      "epoch no.1 train no.102370  loss = 2.05973 avg_loss = 3.80556\n",
      "epoch no.1 train no.102380  loss = 3.25534 avg_loss = 3.76380\n",
      "epoch no.1 train no.102390  loss = 5.67978 avg_loss = 3.74748\n",
      "epoch no.1 train no.102400  loss = 3.11735 avg_loss = 3.74719\n",
      "epoch no.1 train no.102410  loss = 3.45028 avg_loss = 3.68809\n",
      "epoch no.1 train no.102420  loss = 2.04754 avg_loss = 3.69148\n",
      "epoch no.1 train no.102430  loss = 3.73798 avg_loss = 3.78027\n",
      "epoch no.1 train no.102440  loss = 6.40360 avg_loss = 3.85255\n",
      "epoch no.1 train no.102450  loss = 4.62595 avg_loss = 3.85786\n",
      "epoch no.1 train no.102460  loss = 2.33477 avg_loss = 3.83804\n",
      "epoch no.1 train no.102470  loss = 3.29801 avg_loss = 3.79317\n",
      "epoch no.1 train no.102480  loss = 3.70909 avg_loss = 3.81792\n",
      "epoch no.1 train no.102490  loss = 5.51206 avg_loss = 3.86069\n",
      "epoch no.1 train no.102500  loss = 3.30849 avg_loss = 3.82573\n",
      "epoch no.1 train no.102510  loss = 4.69748 avg_loss = 3.80218\n",
      "epoch no.1 train no.102520  loss = 5.48854 avg_loss = 3.81811\n",
      "epoch no.1 train no.102530  loss = 3.63162 avg_loss = 3.80684\n",
      "epoch no.1 train no.102540  loss = 2.05937 avg_loss = 3.78473\n",
      "epoch no.1 train no.102550  loss = 2.99106 avg_loss = 3.78968\n",
      "epoch no.1 train no.102560  loss = 3.79272 avg_loss = 3.81241\n",
      "epoch no.1 train no.102570  loss = 2.83196 avg_loss = 3.81064\n",
      "epoch no.1 train no.102580  loss = 2.94470 avg_loss = 3.83335\n",
      "epoch no.1 train no.102590  loss = 1.29881 avg_loss = 3.77144\n",
      "epoch no.1 train no.102600  loss = 2.90529 avg_loss = 3.73320\n",
      "epoch no.1 train no.102610  loss = 5.36172 avg_loss = 3.81160\n",
      "epoch no.1 train no.102620  loss = 3.50016 avg_loss = 3.75749\n",
      "epoch no.1 train no.102630  loss = 4.01454 avg_loss = 3.73561\n",
      "epoch no.1 train no.102640  loss = 2.45137 avg_loss = 3.66288\n",
      "epoch no.1 train no.102650  loss = 3.57971 avg_loss = 3.71546\n",
      "epoch no.1 train no.102660  loss = 4.44952 avg_loss = 3.72642\n",
      "epoch no.1 train no.102670  loss = 3.33841 avg_loss = 3.71940\n",
      "epoch no.1 train no.102680  loss = 2.79814 avg_loss = 3.71505\n",
      "epoch no.1 train no.102690  loss = 4.73991 avg_loss = 3.69293\n",
      "epoch no.1 train no.102700  loss = 2.94643 avg_loss = 3.67180\n",
      "epoch no.1 train no.102710  loss = 6.29040 avg_loss = 3.69339\n",
      "epoch no.1 train no.102720  loss = 2.51915 avg_loss = 3.69010\n",
      "epoch no.1 train no.102730  loss = 4.00388 avg_loss = 3.66563\n",
      "epoch no.1 train no.102740  loss = 4.55894 avg_loss = 3.67161\n",
      "epoch no.1 train no.102750  loss = 2.54587 avg_loss = 3.69759\n",
      "epoch no.1 train no.102760  loss = 3.24039 avg_loss = 3.69902\n",
      "epoch no.1 train no.102770  loss = 3.88388 avg_loss = 3.78546\n",
      "epoch no.1 train no.102780  loss = 2.84784 avg_loss = 3.78173\n",
      "epoch no.1 train no.102790  loss = 4.10075 avg_loss = 3.81155\n",
      "epoch no.1 train no.102800  loss = 3.88228 avg_loss = 3.78218\n",
      "epoch no.1 train no.102810  loss = 3.40541 avg_loss = 3.81382\n",
      "epoch no.1 train no.102820  loss = 1.83045 avg_loss = 3.79883\n",
      "epoch no.1 train no.102830  loss = 4.88180 avg_loss = 3.78117\n",
      "epoch no.1 train no.102840  loss = 2.21770 avg_loss = 3.76642\n",
      "epoch no.1 train no.102850  loss = 2.05317 avg_loss = 3.75317\n",
      "epoch no.1 train no.102860  loss = 2.87846 avg_loss = 3.69589\n",
      "epoch no.1 train no.102870  loss = 3.20665 avg_loss = 3.67419\n",
      "epoch no.1 train no.102880  loss = 4.45014 avg_loss = 3.67030\n",
      "epoch no.1 train no.102890  loss = 2.62831 avg_loss = 3.64853\n",
      "epoch no.1 train no.102900  loss = 2.39746 avg_loss = 3.62350\n",
      "epoch no.1 train no.102910  loss = 1.52251 avg_loss = 3.59674\n",
      "epoch no.1 train no.102920  loss = 3.63107 avg_loss = 3.58146\n",
      "epoch no.1 train no.102930  loss = 3.39075 avg_loss = 3.65219\n",
      "epoch no.1 train no.102940  loss = 2.62402 avg_loss = 3.71939\n",
      "epoch no.1 train no.102950  loss = 3.50720 avg_loss = 3.74784\n",
      "epoch no.1 train no.102960  loss = 5.07099 avg_loss = 3.78156\n",
      "epoch no.1 train no.102970  loss = 2.83087 avg_loss = 3.81957\n",
      "epoch no.1 train no.102980  loss = 3.11896 avg_loss = 3.81556\n",
      "epoch no.1 train no.102990  loss = 4.59566 avg_loss = 3.84080\n",
      "epoch no.1 train no.103000  loss = 2.48683 avg_loss = 3.79039\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 좋은 노래</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.103010  loss = 3.71140 avg_loss = 3.85372\n",
      "epoch no.1 train no.103020  loss = 3.90890 avg_loss = 3.85692\n",
      "epoch no.1 train no.103030  loss = 4.85209 avg_loss = 3.86181\n",
      "epoch no.1 train no.103040  loss = 3.57416 avg_loss = 3.85519\n",
      "epoch no.1 train no.103050  loss = 4.06208 avg_loss = 3.83486\n",
      "epoch no.1 train no.103060  loss = 3.83311 avg_loss = 3.84248\n",
      "epoch no.1 train no.103070  loss = 3.50324 avg_loss = 3.84205\n",
      "epoch no.1 train no.103080  loss = 5.31006 avg_loss = 3.84122\n",
      "epoch no.1 train no.103090  loss = 5.17203 avg_loss = 3.79126\n",
      "epoch no.1 train no.103100  loss = 5.50575 avg_loss = 3.77769\n",
      "epoch no.1 train no.103110  loss = 4.88196 avg_loss = 3.77559\n",
      "epoch no.1 train no.103120  loss = 2.63142 avg_loss = 3.78957\n",
      "epoch no.1 train no.103130  loss = 4.32575 avg_loss = 3.80304\n",
      "epoch no.1 train no.103140  loss = 4.25656 avg_loss = 3.79098\n",
      "epoch no.1 train no.103150  loss = 2.71920 avg_loss = 3.75007\n",
      "epoch no.1 train no.103160  loss = 3.50945 avg_loss = 3.72989\n",
      "epoch no.1 train no.103170  loss = 2.28976 avg_loss = 3.68600\n",
      "epoch no.1 train no.103180  loss = 4.00428 avg_loss = 3.62037\n",
      "epoch no.1 train no.103190  loss = 3.25338 avg_loss = 3.63914\n",
      "epoch no.1 train no.103200  loss = 4.30204 avg_loss = 3.58398\n",
      "epoch no.1 train no.103210  loss = 5.02231 avg_loss = 3.61684\n",
      "epoch no.1 train no.103220  loss = 4.15242 avg_loss = 3.59911\n",
      "epoch no.1 train no.103230  loss = 4.11719 avg_loss = 3.54748\n",
      "epoch no.1 train no.103240  loss = 2.49552 avg_loss = 3.59045\n",
      "epoch no.1 train no.103250  loss = 3.11745 avg_loss = 3.64257\n",
      "epoch no.1 train no.103260  loss = 3.90761 avg_loss = 3.59224\n",
      "epoch no.1 train no.103270  loss = 3.75673 avg_loss = 3.57183\n",
      "epoch no.1 train no.103280  loss = 3.28158 avg_loss = 3.54260\n",
      "epoch no.1 train no.103290  loss = 3.02677 avg_loss = 3.57976\n",
      "epoch no.1 train no.103300  loss = 2.30345 avg_loss = 3.62094\n",
      "epoch no.1 train no.103310  loss = 2.42798 avg_loss = 3.61020\n",
      "epoch no.1 train no.103320  loss = 5.33259 avg_loss = 3.70045\n",
      "epoch no.1 train no.103330  loss = 2.15140 avg_loss = 3.68307\n",
      "epoch no.1 train no.103340  loss = 4.68929 avg_loss = 3.74210\n",
      "epoch no.1 train no.103350  loss = 5.00287 avg_loss = 3.72308\n",
      "epoch no.1 train no.103360  loss = 4.17903 avg_loss = 3.71517\n",
      "epoch no.1 train no.103370  loss = 2.88214 avg_loss = 3.71218\n",
      "epoch no.1 train no.103380  loss = 3.90551 avg_loss = 3.78223\n",
      "epoch no.1 train no.103390  loss = 4.98386 avg_loss = 3.85011\n",
      "epoch no.1 train no.103400  loss = 2.45719 avg_loss = 3.88169\n",
      "epoch no.1 train no.103410  loss = 3.61888 avg_loss = 3.87417\n",
      "epoch no.1 train no.103420  loss = 3.48401 avg_loss = 3.87427\n",
      "epoch no.1 train no.103430  loss = 1.10291 avg_loss = 3.85440\n",
      "epoch no.1 train no.103440  loss = 3.87042 avg_loss = 3.86352\n",
      "epoch no.1 train no.103450  loss = 2.88284 avg_loss = 3.86305\n",
      "epoch no.1 train no.103460  loss = 3.88255 avg_loss = 3.84558\n",
      "epoch no.1 train no.103470  loss = 2.47985 avg_loss = 3.82954\n",
      "epoch no.1 train no.103480  loss = 3.86712 avg_loss = 3.79644\n",
      "epoch no.1 train no.103490  loss = 3.94480 avg_loss = 3.82790\n",
      "epoch no.1 train no.103500  loss = 3.51815 avg_loss = 3.83145\n",
      "epoch no.1 train no.103510  loss = 2.64266 avg_loss = 3.82108\n",
      "epoch no.1 train no.103520  loss = 2.63058 avg_loss = 3.80877\n",
      "epoch no.1 train no.103530  loss = 2.42861 avg_loss = 3.81293\n",
      "epoch no.1 train no.103540  loss = 3.92025 avg_loss = 3.84434\n",
      "epoch no.1 train no.103550  loss = 3.01900 avg_loss = 3.81902\n",
      "epoch no.1 train no.103560  loss = 5.00267 avg_loss = 3.78764\n",
      "epoch no.1 train no.103570  loss = 3.78418 avg_loss = 3.83512\n",
      "epoch no.1 train no.103580  loss = 4.02783 avg_loss = 3.83425\n",
      "epoch no.1 train no.103590  loss = 2.80679 avg_loss = 3.79847\n",
      "epoch no.1 train no.103600  loss = 3.22198 avg_loss = 3.78846\n",
      "epoch no.1 train no.103610  loss = 2.92238 avg_loss = 3.74275\n",
      "epoch no.1 train no.103620  loss = 2.75152 avg_loss = 3.71651\n",
      "epoch no.1 train no.103630  loss = 5.89349 avg_loss = 3.74102\n",
      "epoch no.1 train no.103640  loss = 3.48091 avg_loss = 3.72810\n",
      "epoch no.1 train no.103650  loss = 5.81169 avg_loss = 3.77955\n",
      "epoch no.1 train no.103660  loss = 4.55473 avg_loss = 3.78885\n",
      "epoch no.1 train no.103670  loss = 4.31915 avg_loss = 3.80304\n",
      "epoch no.1 train no.103680  loss = 4.50084 avg_loss = 3.82067\n",
      "epoch no.1 train no.103690  loss = 3.69138 avg_loss = 3.81865\n",
      "epoch no.1 train no.103700  loss = 3.60275 avg_loss = 3.83024\n",
      "epoch no.1 train no.103710  loss = 5.13303 avg_loss = 3.78847\n",
      "epoch no.1 train no.103720  loss = 3.08796 avg_loss = 3.79809\n",
      "epoch no.1 train no.103730  loss = 4.71575 avg_loss = 3.81099\n",
      "epoch no.1 train no.103740  loss = 3.05094 avg_loss = 3.79369\n",
      "epoch no.1 train no.103750  loss = 3.97572 avg_loss = 3.83114\n",
      "epoch no.1 train no.103760  loss = 5.17681 avg_loss = 3.81120\n",
      "epoch no.1 train no.103770  loss = 3.06168 avg_loss = 3.80834\n",
      "epoch no.1 train no.103780  loss = 5.09359 avg_loss = 3.77451\n",
      "epoch no.1 train no.103790  loss = 2.75131 avg_loss = 3.80588\n",
      "epoch no.1 train no.103800  loss = 3.15802 avg_loss = 3.73675\n",
      "epoch no.1 train no.103810  loss = 3.13077 avg_loss = 3.71635\n",
      "epoch no.1 train no.103820  loss = 2.76650 avg_loss = 3.68680\n",
      "epoch no.1 train no.103830  loss = 2.84373 avg_loss = 3.74845\n",
      "epoch no.1 train no.103840  loss = 2.58628 avg_loss = 3.71034\n",
      "epoch no.1 train no.103850  loss = 2.99819 avg_loss = 3.70387\n",
      "epoch no.1 train no.103860  loss = 4.40996 avg_loss = 3.73775\n",
      "epoch no.1 train no.103870  loss = 4.37069 avg_loss = 3.73736\n",
      "epoch no.1 train no.103880  loss = 6.63158 avg_loss = 3.74633\n",
      "epoch no.1 train no.103890  loss = 3.70598 avg_loss = 3.73213\n",
      "epoch no.1 train no.103900  loss = 5.18376 avg_loss = 3.77486\n",
      "epoch no.1 train no.103910  loss = 2.98886 avg_loss = 3.79229\n",
      "epoch no.1 train no.103920  loss = 4.10046 avg_loss = 3.81324\n",
      "epoch no.1 train no.103930  loss = 2.70818 avg_loss = 3.76166\n",
      "epoch no.1 train no.103940  loss = 2.97697 avg_loss = 3.75009\n",
      "epoch no.1 train no.103950  loss = 5.19181 avg_loss = 3.73712\n",
      "epoch no.1 train no.103960  loss = 3.97820 avg_loss = 3.75994\n",
      "epoch no.1 train no.103970  loss = 4.72390 avg_loss = 3.80457\n",
      "epoch no.1 train no.103980  loss = 4.76694 avg_loss = 3.75404\n",
      "epoch no.1 train no.103990  loss = 5.38872 avg_loss = 3.75528\n",
      "epoch no.1 train no.104000  loss = 5.56977 avg_loss = 3.78826\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '이', '</s>']\n",
      "기분전환 드라이브</s>\n",
      "epoch no.1 train no.104010  loss = 4.16851 avg_loss = 3.77113\n",
      "epoch no.1 train no.104020  loss = 2.32613 avg_loss = 3.79133\n",
      "epoch no.1 train no.104030  loss = 4.47342 avg_loss = 3.79545\n",
      "epoch no.1 train no.104040  loss = 6.19583 avg_loss = 3.78533\n",
      "epoch no.1 train no.104050  loss = 4.81243 avg_loss = 3.82974\n",
      "epoch no.1 train no.104060  loss = 2.19763 avg_loss = 3.80642\n",
      "epoch no.1 train no.104070  loss = 2.31981 avg_loss = 3.72858\n",
      "epoch no.1 train no.104080  loss = 2.63993 avg_loss = 3.71362\n",
      "epoch no.1 train no.104090  loss = 3.70292 avg_loss = 3.71634\n",
      "epoch no.1 train no.104100  loss = 4.41575 avg_loss = 3.68650\n",
      "epoch no.1 train no.104110  loss = 5.47437 avg_loss = 3.66133\n",
      "epoch no.1 train no.104120  loss = 3.05592 avg_loss = 3.69517\n",
      "epoch no.1 train no.104130  loss = 3.49812 avg_loss = 3.74763\n",
      "epoch no.1 train no.104140  loss = 6.14925 avg_loss = 3.80277\n",
      "epoch no.1 train no.104150  loss = 2.41689 avg_loss = 3.78594\n",
      "epoch no.1 train no.104160  loss = 3.37761 avg_loss = 3.73917\n",
      "epoch no.1 train no.104170  loss = 5.30564 avg_loss = 3.76527\n",
      "epoch no.1 train no.104180  loss = 5.20232 avg_loss = 3.82422\n",
      "epoch no.1 train no.104190  loss = 3.77104 avg_loss = 3.85749\n",
      "epoch no.1 train no.104200  loss = 5.46155 avg_loss = 3.84338\n",
      "epoch no.1 train no.104210  loss = 4.43902 avg_loss = 3.89116\n",
      "epoch no.1 train no.104220  loss = 3.55242 avg_loss = 3.84067\n",
      "epoch no.1 train no.104230  loss = 2.29340 avg_loss = 3.84934\n",
      "epoch no.1 train no.104240  loss = 1.91337 avg_loss = 3.81254\n",
      "epoch no.1 train no.104250  loss = 3.59509 avg_loss = 3.82796\n",
      "epoch no.1 train no.104260  loss = 4.76104 avg_loss = 3.82759\n",
      "epoch no.1 train no.104270  loss = 5.53828 avg_loss = 3.77278\n",
      "epoch no.1 train no.104280  loss = 3.57254 avg_loss = 3.73259\n",
      "epoch no.1 train no.104290  loss = 5.70859 avg_loss = 3.71172\n",
      "epoch no.1 train no.104300  loss = 3.18111 avg_loss = 3.69190\n",
      "epoch no.1 train no.104310  loss = 4.66760 avg_loss = 3.71564\n",
      "epoch no.1 train no.104320  loss = 2.37664 avg_loss = 3.66887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.104330  loss = 5.51645 avg_loss = 3.75400\n",
      "epoch no.1 train no.104340  loss = 4.63748 avg_loss = 3.69178\n",
      "epoch no.1 train no.104350  loss = 3.81597 avg_loss = 3.71840\n",
      "epoch no.1 train no.104360  loss = 2.87339 avg_loss = 3.73035\n",
      "epoch no.1 train no.104370  loss = 5.20952 avg_loss = 3.73720\n",
      "epoch no.1 train no.104380  loss = 4.00619 avg_loss = 3.73443\n",
      "epoch no.1 train no.104390  loss = 4.45255 avg_loss = 3.72030\n",
      "epoch no.1 train no.104400  loss = 4.32426 avg_loss = 3.71598\n",
      "epoch no.1 train no.104410  loss = 3.22486 avg_loss = 3.69155\n",
      "epoch no.1 train no.104420  loss = 3.05710 avg_loss = 3.64682\n",
      "epoch no.1 train no.104430  loss = 4.17015 avg_loss = 3.64784\n",
      "epoch no.1 train no.104440  loss = 3.55717 avg_loss = 3.70124\n",
      "epoch no.1 train no.104450  loss = 4.57421 avg_loss = 3.76544\n",
      "epoch no.1 train no.104460  loss = 3.15407 avg_loss = 3.70944\n",
      "epoch no.1 train no.104470  loss = 2.81896 avg_loss = 3.67835\n",
      "epoch no.1 train no.104480  loss = 5.45586 avg_loss = 3.67753\n",
      "epoch no.1 train no.104490  loss = 6.98529 avg_loss = 3.71570\n",
      "epoch no.1 train no.104500  loss = 2.98074 avg_loss = 3.72971\n",
      "epoch no.1 train no.104510  loss = 5.11517 avg_loss = 3.80353\n",
      "epoch no.1 train no.104520  loss = 3.47986 avg_loss = 3.84768\n",
      "epoch no.1 train no.104530  loss = 4.35185 avg_loss = 3.85214\n",
      "epoch no.1 train no.104540  loss = 3.72791 avg_loss = 3.88520\n",
      "epoch no.1 train no.104550  loss = 4.69600 avg_loss = 3.89112\n",
      "epoch no.1 train no.104560  loss = 2.92097 avg_loss = 3.88971\n",
      "epoch no.1 train no.104570  loss = 2.87107 avg_loss = 3.90690\n",
      "epoch no.1 train no.104580  loss = 3.72621 avg_loss = 3.93049\n",
      "epoch no.1 train no.104590  loss = 4.28854 avg_loss = 3.95513\n",
      "epoch no.1 train no.104600  loss = 4.09484 avg_loss = 3.91132\n",
      "epoch no.1 train no.104610  loss = 3.04962 avg_loss = 3.92931\n",
      "epoch no.1 train no.104620  loss = 3.66512 avg_loss = 3.92733\n",
      "epoch no.1 train no.104630  loss = 3.40313 avg_loss = 3.88026\n",
      "epoch no.1 train no.104640  loss = 4.16230 avg_loss = 3.84252\n",
      "epoch no.1 train no.104650  loss = 3.38860 avg_loss = 3.82658\n",
      "epoch no.1 train no.104660  loss = 3.02496 avg_loss = 3.77081\n",
      "epoch no.1 train no.104670  loss = 7.18201 avg_loss = 3.81953\n",
      "epoch no.1 train no.104680  loss = 2.91661 avg_loss = 3.81009\n",
      "epoch no.1 train no.104690  loss = 3.27868 avg_loss = 3.81689\n",
      "epoch no.1 train no.104700  loss = 4.78633 avg_loss = 3.82616\n",
      "epoch no.1 train no.104710  loss = 3.67812 avg_loss = 3.79504\n",
      "epoch no.1 train no.104720  loss = 3.39634 avg_loss = 3.81387\n",
      "epoch no.1 train no.104730  loss = 2.80409 avg_loss = 3.77399\n",
      "epoch no.1 train no.104740  loss = 2.35682 avg_loss = 3.71255\n",
      "epoch no.1 train no.104750  loss = 1.87367 avg_loss = 3.67662\n",
      "epoch no.1 train no.104760  loss = 4.10272 avg_loss = 3.63604\n",
      "epoch no.1 train no.104770  loss = 3.03077 avg_loss = 3.67293\n",
      "epoch no.1 train no.104780  loss = 4.24448 avg_loss = 3.77480\n",
      "epoch no.1 train no.104790  loss = 3.89028 avg_loss = 3.77544\n",
      "epoch no.1 train no.104800  loss = 1.52699 avg_loss = 3.80345\n",
      "epoch no.1 train no.104810  loss = 3.83364 avg_loss = 3.73150\n",
      "epoch no.1 train no.104820  loss = 3.18468 avg_loss = 3.72526\n",
      "epoch no.1 train no.104830  loss = 3.77646 avg_loss = 3.69034\n",
      "epoch no.1 train no.104840  loss = 3.46740 avg_loss = 3.64471\n",
      "epoch no.1 train no.104850  loss = 4.77892 avg_loss = 3.67919\n",
      "epoch no.1 train no.104860  loss = 2.75269 avg_loss = 3.64469\n",
      "epoch no.1 train no.104870  loss = 3.27221 avg_loss = 3.63132\n",
      "epoch no.1 train no.104880  loss = 5.19442 avg_loss = 3.60820\n",
      "epoch no.1 train no.104890  loss = 5.16749 avg_loss = 3.63584\n",
      "epoch no.1 train no.104900  loss = 6.04789 avg_loss = 3.68579\n",
      "epoch no.1 train no.104910  loss = 3.98381 avg_loss = 3.69812\n",
      "epoch no.1 train no.104920  loss = 2.09935 avg_loss = 3.65613\n",
      "epoch no.1 train no.104930  loss = 5.90595 avg_loss = 3.68233\n",
      "epoch no.1 train no.104940  loss = 7.72056 avg_loss = 3.71855\n",
      "epoch no.1 train no.104950  loss = 2.86004 avg_loss = 3.71833\n",
      "epoch no.1 train no.104960  loss = 4.16802 avg_loss = 3.67009\n",
      "epoch no.1 train no.104970  loss = 5.11426 avg_loss = 3.66071\n",
      "epoch no.1 train no.104980  loss = 5.98513 avg_loss = 3.69671\n",
      "epoch no.1 train no.104990  loss = 3.23850 avg_loss = 3.68137\n",
      "epoch no.1 train no.105000  loss = 5.18855 avg_loss = 3.69621\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁신나는', '▁노래', '</s>']\n",
      "기분전환용 신나는 노래</s>\n",
      "epoch no.1 train no.105010  loss = 4.37555 avg_loss = 3.68024\n",
      "epoch no.1 train no.105020  loss = 3.02439 avg_loss = 3.66681\n",
      "epoch no.1 train no.105030  loss = 2.85640 avg_loss = 3.62901\n",
      "epoch no.1 train no.105040  loss = 3.32399 avg_loss = 3.63317\n",
      "epoch no.1 train no.105050  loss = 4.61686 avg_loss = 3.61891\n",
      "epoch no.1 train no.105060  loss = 3.16431 avg_loss = 3.64513\n",
      "epoch no.1 train no.105070  loss = 2.47715 avg_loss = 3.60428\n",
      "epoch no.1 train no.105080  loss = 4.49426 avg_loss = 3.67605\n",
      "epoch no.1 train no.105090  loss = 2.39336 avg_loss = 3.67606\n",
      "epoch no.1 train no.105100  loss = 5.99760 avg_loss = 3.64358\n",
      "epoch no.1 train no.105110  loss = 5.32092 avg_loss = 3.66707\n",
      "epoch no.1 train no.105120  loss = 4.49864 avg_loss = 3.70336\n",
      "epoch no.1 train no.105130  loss = 2.25748 avg_loss = 3.69427\n",
      "epoch no.1 train no.105140  loss = 3.86607 avg_loss = 3.64375\n",
      "epoch no.1 train no.105150  loss = 3.79777 avg_loss = 3.66756\n",
      "epoch no.1 train no.105160  loss = 2.85357 avg_loss = 3.71285\n",
      "epoch no.1 train no.105170  loss = 3.06862 avg_loss = 3.70296\n",
      "epoch no.1 train no.105180  loss = 4.12037 avg_loss = 3.70364\n",
      "epoch no.1 train no.105190  loss = 2.31986 avg_loss = 3.67055\n",
      "epoch no.1 train no.105200  loss = 2.33039 avg_loss = 3.68841\n",
      "epoch no.1 train no.105210  loss = 2.20086 avg_loss = 3.64135\n",
      "epoch no.1 train no.105220  loss = 3.55848 avg_loss = 3.62699\n",
      "epoch no.1 train no.105230  loss = 5.40865 avg_loss = 3.69443\n",
      "epoch no.1 train no.105240  loss = 3.89418 avg_loss = 3.70430\n",
      "epoch no.1 train no.105250  loss = 2.72506 avg_loss = 3.75746\n",
      "epoch no.1 train no.105260  loss = 4.21845 avg_loss = 3.76490\n",
      "epoch no.1 train no.105270  loss = 3.08358 avg_loss = 3.73921\n",
      "epoch no.1 train no.105280  loss = 4.34364 avg_loss = 3.71832\n",
      "epoch no.1 train no.105290  loss = 4.14456 avg_loss = 3.67433\n",
      "epoch no.1 train no.105300  loss = 4.01075 avg_loss = 3.69394\n",
      "epoch no.1 train no.105310  loss = 2.51006 avg_loss = 3.64429\n",
      "epoch no.1 train no.105320  loss = 3.54951 avg_loss = 3.60991\n",
      "epoch no.1 train no.105330  loss = 4.68254 avg_loss = 3.59144\n",
      "epoch no.1 train no.105340  loss = 2.07086 avg_loss = 3.64258\n",
      "epoch no.1 train no.105350  loss = 3.04661 avg_loss = 3.63695\n",
      "epoch no.1 train no.105360  loss = 4.20680 avg_loss = 3.63824\n",
      "epoch no.1 train no.105370  loss = 3.62805 avg_loss = 3.65961\n",
      "epoch no.1 train no.105380  loss = 6.97764 avg_loss = 3.68356\n",
      "epoch no.1 train no.105390  loss = 3.19784 avg_loss = 3.68012\n",
      "epoch no.1 train no.105400  loss = 2.90016 avg_loss = 3.69207\n",
      "epoch no.1 train no.105410  loss = 4.87223 avg_loss = 3.68776\n",
      "epoch no.1 train no.105420  loss = 2.76747 avg_loss = 3.69544\n",
      "epoch no.1 train no.105430  loss = 5.76791 avg_loss = 3.77427\n",
      "epoch no.1 train no.105440  loss = 3.87535 avg_loss = 3.75869\n",
      "epoch no.1 train no.105450  loss = 6.49149 avg_loss = 3.83487\n",
      "epoch no.1 train no.105460  loss = 5.63594 avg_loss = 3.86207\n",
      "epoch no.1 train no.105470  loss = 4.72860 avg_loss = 3.85075\n",
      "epoch no.1 train no.105480  loss = 3.15241 avg_loss = 3.84163\n",
      "epoch no.1 train no.105490  loss = 4.05917 avg_loss = 3.78045\n",
      "epoch no.1 train no.105500  loss = 2.86067 avg_loss = 3.74476\n",
      "epoch no.1 train no.105510  loss = 2.04184 avg_loss = 3.71162\n",
      "epoch no.1 train no.105520  loss = 3.16093 avg_loss = 3.69067\n",
      "epoch no.1 train no.105530  loss = 5.10315 avg_loss = 3.76439\n",
      "epoch no.1 train no.105540  loss = 2.44394 avg_loss = 3.78048\n",
      "epoch no.1 train no.105550  loss = 3.60577 avg_loss = 3.70997\n",
      "epoch no.1 train no.105560  loss = 2.67323 avg_loss = 3.78322\n",
      "epoch no.1 train no.105570  loss = 4.24955 avg_loss = 3.86112\n",
      "epoch no.1 train no.105580  loss = 3.41067 avg_loss = 3.89356\n",
      "epoch no.1 train no.105590  loss = 1.96635 avg_loss = 3.81109\n",
      "epoch no.1 train no.105600  loss = 2.74902 avg_loss = 3.80374\n",
      "epoch no.1 train no.105610  loss = 2.97777 avg_loss = 3.86590\n",
      "epoch no.1 train no.105620  loss = 5.13931 avg_loss = 3.92734\n",
      "epoch no.1 train no.105630  loss = 2.56840 avg_loss = 3.88514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.105640  loss = 3.67730 avg_loss = 3.86205\n",
      "epoch no.1 train no.105650  loss = 3.61623 avg_loss = 3.84806\n",
      "epoch no.1 train no.105660  loss = 5.16044 avg_loss = 3.85333\n",
      "epoch no.1 train no.105670  loss = 2.38033 avg_loss = 3.88601\n",
      "epoch no.1 train no.105680  loss = 4.09761 avg_loss = 3.85789\n",
      "epoch no.1 train no.105690  loss = 5.53278 avg_loss = 3.81812\n",
      "epoch no.1 train no.105700  loss = 3.42598 avg_loss = 3.79779\n",
      "epoch no.1 train no.105710  loss = 4.14944 avg_loss = 3.75420\n",
      "epoch no.1 train no.105720  loss = 4.35020 avg_loss = 3.78356\n",
      "epoch no.1 train no.105730  loss = 4.47425 avg_loss = 3.73383\n",
      "epoch no.1 train no.105740  loss = 2.66540 avg_loss = 3.71816\n",
      "epoch no.1 train no.105750  loss = 5.29726 avg_loss = 3.72481\n",
      "epoch no.1 train no.105760  loss = 2.92766 avg_loss = 3.73066\n",
      "epoch no.1 train no.105770  loss = 5.94085 avg_loss = 3.75249\n",
      "epoch no.1 train no.105780  loss = 4.59728 avg_loss = 3.78284\n",
      "epoch no.1 train no.105790  loss = 2.76103 avg_loss = 3.74941\n",
      "epoch no.1 train no.105800  loss = 4.71909 avg_loss = 3.74598\n",
      "epoch no.1 train no.105810  loss = 3.56843 avg_loss = 3.74340\n",
      "epoch no.1 train no.105820  loss = 2.32648 avg_loss = 3.80726\n",
      "epoch no.1 train no.105830  loss = 2.13933 avg_loss = 3.72365\n",
      "epoch no.1 train no.105840  loss = 5.71740 avg_loss = 3.85055\n",
      "epoch no.1 train no.105850  loss = 5.22750 avg_loss = 3.81176\n",
      "epoch no.1 train no.105860  loss = 2.81383 avg_loss = 3.79838\n",
      "epoch no.1 train no.105870  loss = 3.12410 avg_loss = 3.75095\n",
      "epoch no.1 train no.105880  loss = 2.59785 avg_loss = 3.77652\n",
      "epoch no.1 train no.105890  loss = 3.51312 avg_loss = 3.76475\n",
      "epoch no.1 train no.105900  loss = 3.69829 avg_loss = 3.73555\n",
      "epoch no.1 train no.105910  loss = 3.15050 avg_loss = 3.71547\n",
      "epoch no.1 train no.105920  loss = 2.22335 avg_loss = 3.71178\n",
      "epoch no.1 train no.105930  loss = 3.42117 avg_loss = 3.79098\n",
      "epoch no.1 train no.105940  loss = 3.96012 avg_loss = 3.77973\n",
      "epoch no.1 train no.105950  loss = 4.54354 avg_loss = 3.75614\n",
      "epoch no.1 train no.105960  loss = 6.04103 avg_loss = 3.76297\n",
      "epoch no.1 train no.105970  loss = 6.43823 avg_loss = 3.84565\n",
      "epoch no.1 train no.105980  loss = 4.71363 avg_loss = 3.85090\n",
      "epoch no.1 train no.105990  loss = 3.05796 avg_loss = 3.85543\n",
      "epoch no.1 train no.106000  loss = 4.07250 avg_loss = 3.82831\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.106010  loss = 4.38026 avg_loss = 3.86005\n",
      "epoch no.1 train no.106020  loss = 3.36533 avg_loss = 3.83832\n",
      "epoch no.1 train no.106030  loss = 2.14010 avg_loss = 3.79190\n",
      "epoch no.1 train no.106040  loss = 4.01847 avg_loss = 3.79923\n",
      "epoch no.1 train no.106050  loss = 4.20611 avg_loss = 3.77151\n",
      "epoch no.1 train no.106060  loss = 4.76385 avg_loss = 3.81860\n",
      "epoch no.1 train no.106070  loss = 2.50465 avg_loss = 3.80653\n",
      "epoch no.1 train no.106080  loss = 4.45167 avg_loss = 3.76934\n",
      "epoch no.1 train no.106090  loss = 4.41958 avg_loss = 3.76663\n",
      "epoch no.1 train no.106100  loss = 4.15785 avg_loss = 3.77051\n",
      "epoch no.1 train no.106110  loss = 3.22049 avg_loss = 3.80132\n",
      "epoch no.1 train no.106120  loss = 2.84179 avg_loss = 3.79573\n",
      "epoch no.1 train no.106130  loss = 2.92646 avg_loss = 3.81838\n",
      "epoch no.1 train no.106140  loss = 1.67473 avg_loss = 3.80028\n",
      "epoch no.1 train no.106150  loss = 3.21950 avg_loss = 3.76352\n",
      "epoch no.1 train no.106160  loss = 3.85691 avg_loss = 3.73433\n",
      "epoch no.1 train no.106170  loss = 3.45816 avg_loss = 3.74064\n",
      "epoch no.1 train no.106180  loss = 4.65199 avg_loss = 3.72889\n",
      "epoch no.1 train no.106190  loss = 3.04811 avg_loss = 3.67018\n",
      "epoch no.1 train no.106200  loss = 2.33823 avg_loss = 3.68574\n",
      "epoch no.1 train no.106210  loss = 4.37048 avg_loss = 3.71397\n",
      "epoch no.1 train no.106220  loss = 2.75556 avg_loss = 3.64429\n",
      "epoch no.1 train no.106230  loss = 3.15989 avg_loss = 3.61924\n",
      "epoch no.1 train no.106240  loss = 2.29686 avg_loss = 3.59700\n",
      "epoch no.1 train no.106250  loss = 3.54632 avg_loss = 3.59749\n",
      "epoch no.1 train no.106260  loss = 3.47995 avg_loss = 3.58918\n",
      "epoch no.1 train no.106270  loss = 1.87400 avg_loss = 3.65707\n",
      "epoch no.1 train no.106280  loss = 5.85731 avg_loss = 3.69693\n",
      "epoch no.1 train no.106290  loss = 3.88642 avg_loss = 3.75215\n",
      "epoch no.1 train no.106300  loss = 2.66738 avg_loss = 3.80797\n",
      "epoch no.1 train no.106310  loss = 2.62371 avg_loss = 3.76135\n",
      "epoch no.1 train no.106320  loss = 3.30449 avg_loss = 3.72602\n",
      "epoch no.1 train no.106330  loss = 2.30280 avg_loss = 3.73212\n",
      "epoch no.1 train no.106340  loss = 3.40638 avg_loss = 3.73764\n",
      "epoch no.1 train no.106350  loss = 3.96501 avg_loss = 3.71907\n",
      "epoch no.1 train no.106360  loss = 4.54885 avg_loss = 3.77534\n",
      "epoch no.1 train no.106370  loss = 2.80808 avg_loss = 3.82671\n",
      "epoch no.1 train no.106380  loss = 3.32255 avg_loss = 3.80339\n",
      "epoch no.1 train no.106390  loss = 3.53305 avg_loss = 3.77066\n",
      "epoch no.1 train no.106400  loss = 2.67844 avg_loss = 3.72026\n",
      "epoch no.1 train no.106410  loss = 2.12101 avg_loss = 3.66554\n",
      "epoch no.1 train no.106420  loss = 2.64606 avg_loss = 3.68251\n",
      "epoch no.1 train no.106430  loss = 5.08126 avg_loss = 3.69526\n",
      "epoch no.1 train no.106440  loss = 2.06226 avg_loss = 3.69108\n",
      "epoch no.1 train no.106450  loss = 6.90914 avg_loss = 3.73812\n",
      "epoch no.1 train no.106460  loss = 3.09726 avg_loss = 3.69924\n",
      "epoch no.1 train no.106470  loss = 2.27307 avg_loss = 3.71595\n",
      "epoch no.1 train no.106480  loss = 3.18580 avg_loss = 3.78192\n",
      "epoch no.1 train no.106490  loss = 2.90481 avg_loss = 3.76565\n",
      "epoch no.1 train no.106500  loss = 4.65316 avg_loss = 3.79468\n",
      "epoch no.1 train no.106510  loss = 2.53150 avg_loss = 3.79751\n",
      "epoch no.1 train no.106520  loss = 2.65331 avg_loss = 3.73882\n",
      "epoch no.1 train no.106530  loss = 1.90819 avg_loss = 3.69772\n",
      "epoch no.1 train no.106540  loss = 2.82809 avg_loss = 3.74221\n",
      "epoch no.1 train no.106550  loss = 2.96804 avg_loss = 3.74654\n",
      "epoch no.1 train no.106560  loss = 3.65853 avg_loss = 3.75001\n",
      "epoch no.1 train no.106570  loss = 4.68569 avg_loss = 3.75246\n",
      "epoch no.1 train no.106580  loss = 3.30978 avg_loss = 3.78425\n",
      "epoch no.1 train no.106590  loss = 3.80790 avg_loss = 3.75909\n",
      "epoch no.1 train no.106600  loss = 3.99893 avg_loss = 3.71857\n",
      "epoch no.1 train no.106610  loss = 4.99290 avg_loss = 3.69740\n",
      "epoch no.1 train no.106620  loss = 2.23950 avg_loss = 3.66077\n",
      "epoch no.1 train no.106630  loss = 5.14945 avg_loss = 3.71484\n",
      "epoch no.1 train no.106640  loss = 2.79775 avg_loss = 3.71182\n",
      "epoch no.1 train no.106650  loss = 2.41480 avg_loss = 3.66523\n",
      "epoch no.1 train no.106660  loss = 6.03964 avg_loss = 3.68751\n",
      "epoch no.1 train no.106670  loss = 4.01381 avg_loss = 3.64787\n",
      "epoch no.1 train no.106680  loss = 3.04200 avg_loss = 3.70864\n",
      "epoch no.1 train no.106690  loss = 4.20787 avg_loss = 3.72735\n",
      "epoch no.1 train no.106700  loss = 4.05118 avg_loss = 3.74890\n",
      "epoch no.1 train no.106710  loss = 3.59461 avg_loss = 3.75145\n",
      "epoch no.1 train no.106720  loss = 2.24445 avg_loss = 3.71886\n",
      "epoch no.1 train no.106730  loss = 3.39075 avg_loss = 3.69629\n",
      "epoch no.1 train no.106740  loss = 5.39805 avg_loss = 3.71947\n",
      "epoch no.1 train no.106750  loss = 2.44032 avg_loss = 3.68140\n",
      "epoch no.1 train no.106760  loss = 2.59852 avg_loss = 3.67936\n",
      "epoch no.1 train no.106770  loss = 5.82648 avg_loss = 3.69067\n",
      "epoch no.1 train no.106780  loss = 3.52986 avg_loss = 3.72056\n",
      "epoch no.1 train no.106790  loss = 3.82546 avg_loss = 3.69113\n",
      "epoch no.1 train no.106800  loss = 2.14604 avg_loss = 3.71609\n",
      "epoch no.1 train no.106810  loss = 4.71352 avg_loss = 3.79955\n",
      "epoch no.1 train no.106820  loss = 4.58042 avg_loss = 3.80593\n",
      "epoch no.1 train no.106830  loss = 3.30938 avg_loss = 3.80249\n",
      "epoch no.1 train no.106840  loss = 2.35135 avg_loss = 3.78024\n",
      "epoch no.1 train no.106850  loss = 2.57526 avg_loss = 3.76406\n",
      "epoch no.1 train no.106860  loss = 2.81460 avg_loss = 3.77697\n",
      "epoch no.1 train no.106870  loss = 5.93864 avg_loss = 3.80825\n",
      "epoch no.1 train no.106880  loss = 7.54675 avg_loss = 3.83063\n",
      "epoch no.1 train no.106890  loss = 3.54709 avg_loss = 3.88122\n",
      "epoch no.1 train no.106900  loss = 4.66157 avg_loss = 3.90641\n",
      "epoch no.1 train no.106910  loss = 2.51578 avg_loss = 3.91416\n",
      "epoch no.1 train no.106920  loss = 2.57608 avg_loss = 3.90913\n",
      "epoch no.1 train no.106930  loss = 3.21196 avg_loss = 3.85075\n",
      "epoch no.1 train no.106940  loss = 2.94396 avg_loss = 3.80932\n",
      "epoch no.1 train no.106950  loss = 2.97537 avg_loss = 3.81050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.106960  loss = 5.99206 avg_loss = 3.83238\n",
      "epoch no.1 train no.106970  loss = 2.40883 avg_loss = 3.79148\n",
      "epoch no.1 train no.106980  loss = 4.00680 avg_loss = 3.76246\n",
      "epoch no.1 train no.106990  loss = 4.79537 avg_loss = 3.78312\n",
      "epoch no.1 train no.107000  loss = 4.11246 avg_loss = 3.77681\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 신나는 노래</s>\n",
      "epoch no.1 train no.107010  loss = 4.75967 avg_loss = 3.81012\n",
      "epoch no.1 train no.107020  loss = 2.64242 avg_loss = 3.77991\n",
      "epoch no.1 train no.107030  loss = 4.20040 avg_loss = 3.76334\n",
      "epoch no.1 train no.107040  loss = 2.62919 avg_loss = 3.71699\n",
      "epoch no.1 train no.107050  loss = 3.91016 avg_loss = 3.78242\n",
      "epoch no.1 train no.107060  loss = 4.37290 avg_loss = 3.78587\n",
      "epoch no.1 train no.107070  loss = 3.73716 avg_loss = 3.78798\n",
      "epoch no.1 train no.107080  loss = 2.97456 avg_loss = 3.75977\n",
      "epoch no.1 train no.107090  loss = 4.42635 avg_loss = 3.77752\n",
      "epoch no.1 train no.107100  loss = 4.74410 avg_loss = 3.80435\n",
      "epoch no.1 train no.107110  loss = 3.06010 avg_loss = 3.76056\n",
      "epoch no.1 train no.107120  loss = 2.98294 avg_loss = 3.77689\n",
      "epoch no.1 train no.107130  loss = 2.85239 avg_loss = 3.76754\n",
      "epoch no.1 train no.107140  loss = 3.96050 avg_loss = 3.76596\n",
      "epoch no.1 train no.107150  loss = 4.67891 avg_loss = 3.72438\n",
      "epoch no.1 train no.107160  loss = 2.38466 avg_loss = 3.71159\n",
      "epoch no.1 train no.107170  loss = 2.44184 avg_loss = 3.70363\n",
      "epoch no.1 train no.107180  loss = 4.54747 avg_loss = 3.69883\n",
      "epoch no.1 train no.107190  loss = 2.54534 avg_loss = 3.70130\n",
      "epoch no.1 train no.107200  loss = 4.32418 avg_loss = 3.72282\n",
      "epoch no.1 train no.107210  loss = 4.55140 avg_loss = 3.78041\n",
      "epoch no.1 train no.107220  loss = 3.48604 avg_loss = 3.75888\n",
      "epoch no.1 train no.107230  loss = 2.83979 avg_loss = 3.77972\n",
      "epoch no.1 train no.107240  loss = 2.88639 avg_loss = 3.74234\n",
      "epoch no.1 train no.107250  loss = 6.80711 avg_loss = 3.77602\n",
      "epoch no.1 train no.107260  loss = 2.58081 avg_loss = 3.74294\n",
      "epoch no.1 train no.107270  loss = 5.91333 avg_loss = 3.78084\n",
      "epoch no.1 train no.107280  loss = 4.44999 avg_loss = 3.74707\n",
      "epoch no.1 train no.107290  loss = 2.68554 avg_loss = 3.76195\n",
      "epoch no.1 train no.107300  loss = 3.61329 avg_loss = 3.77517\n",
      "epoch no.1 train no.107310  loss = 3.19600 avg_loss = 3.79593\n",
      "epoch no.1 train no.107320  loss = 4.68092 avg_loss = 3.79397\n",
      "epoch no.1 train no.107330  loss = 4.22202 avg_loss = 3.77545\n",
      "epoch no.1 train no.107340  loss = 2.28011 avg_loss = 3.77898\n",
      "epoch no.1 train no.107350  loss = 3.06291 avg_loss = 3.75748\n",
      "epoch no.1 train no.107360  loss = 4.22377 avg_loss = 3.75851\n",
      "epoch no.1 train no.107370  loss = 3.57996 avg_loss = 3.75403\n",
      "epoch no.1 train no.107380  loss = 3.91419 avg_loss = 3.77775\n",
      "epoch no.1 train no.107390  loss = 4.45700 avg_loss = 3.83585\n",
      "epoch no.1 train no.107400  loss = 5.63753 avg_loss = 3.86980\n",
      "epoch no.1 train no.107410  loss = 5.95196 avg_loss = 3.94073\n",
      "epoch no.1 train no.107420  loss = 3.73603 avg_loss = 3.96818\n",
      "epoch no.1 train no.107430  loss = 3.78300 avg_loss = 3.95522\n",
      "epoch no.1 train no.107440  loss = 2.35381 avg_loss = 3.94147\n",
      "epoch no.1 train no.107450  loss = 3.16727 avg_loss = 3.92224\n",
      "epoch no.1 train no.107460  loss = 7.09755 avg_loss = 3.96959\n",
      "epoch no.1 train no.107470  loss = 3.28167 avg_loss = 3.91081\n",
      "epoch no.1 train no.107480  loss = 4.42641 avg_loss = 3.86931\n",
      "epoch no.1 train no.107490  loss = 2.05205 avg_loss = 3.86232\n",
      "epoch no.1 train no.107500  loss = 2.06838 avg_loss = 3.76779\n",
      "epoch no.1 train no.107510  loss = 3.16244 avg_loss = 3.67583\n",
      "epoch no.1 train no.107520  loss = 4.28502 avg_loss = 3.69888\n",
      "epoch no.1 train no.107530  loss = 4.33921 avg_loss = 3.72192\n",
      "epoch no.1 train no.107540  loss = 4.43034 avg_loss = 3.71701\n",
      "epoch no.1 train no.107550  loss = 3.02646 avg_loss = 3.76091\n",
      "epoch no.1 train no.107560  loss = 3.17149 avg_loss = 3.72968\n",
      "epoch no.1 train no.107570  loss = 4.91046 avg_loss = 3.70783\n",
      "epoch no.1 train no.107580  loss = 4.67024 avg_loss = 3.68545\n",
      "epoch no.1 train no.107590  loss = 5.32303 avg_loss = 3.72443\n",
      "epoch no.1 train no.107600  loss = 5.68577 avg_loss = 3.77996\n",
      "epoch no.1 train no.107610  loss = 4.91036 avg_loss = 3.81315\n",
      "epoch no.1 train no.107620  loss = 4.82516 avg_loss = 3.85116\n",
      "epoch no.1 train no.107630  loss = 5.71758 avg_loss = 3.83695\n",
      "epoch no.1 train no.107640  loss = 3.66433 avg_loss = 3.79777\n",
      "epoch no.1 train no.107650  loss = 2.04784 avg_loss = 3.73071\n",
      "epoch no.1 train no.107660  loss = 5.22830 avg_loss = 3.73512\n",
      "epoch no.1 train no.107670  loss = 3.15376 avg_loss = 3.75334\n",
      "epoch no.1 train no.107680  loss = 3.86089 avg_loss = 3.78829\n",
      "epoch no.1 train no.107690  loss = 3.91598 avg_loss = 3.82777\n",
      "epoch no.1 train no.107700  loss = 3.87468 avg_loss = 3.81609\n",
      "epoch no.1 train no.107710  loss = 3.33861 avg_loss = 3.85346\n",
      "epoch no.1 train no.107720  loss = 4.76942 avg_loss = 3.88680\n",
      "epoch no.1 train no.107730  loss = 3.86194 avg_loss = 3.84496\n",
      "epoch no.1 train no.107740  loss = 3.81728 avg_loss = 3.83633\n",
      "epoch no.1 train no.107750  loss = 2.01579 avg_loss = 3.80658\n",
      "epoch no.1 train no.107760  loss = 4.09311 avg_loss = 3.82189\n",
      "epoch no.1 train no.107770  loss = 6.06811 avg_loss = 3.81721\n",
      "epoch no.1 train no.107780  loss = 3.16898 avg_loss = 3.82456\n",
      "epoch no.1 train no.107790  loss = 2.65261 avg_loss = 3.86435\n",
      "epoch no.1 train no.107800  loss = 5.12999 avg_loss = 3.83154\n",
      "epoch no.1 train no.107810  loss = 5.47191 avg_loss = 3.84157\n",
      "epoch no.1 train no.107820  loss = 2.97404 avg_loss = 3.85599\n",
      "epoch no.1 train no.107830  loss = 2.66500 avg_loss = 3.85717\n",
      "epoch no.1 train no.107840  loss = 3.27477 avg_loss = 3.84013\n",
      "epoch no.1 train no.107850  loss = 2.51915 avg_loss = 3.88732\n",
      "epoch no.1 train no.107860  loss = 4.65405 avg_loss = 3.92692\n",
      "epoch no.1 train no.107870  loss = 3.48164 avg_loss = 3.91153\n",
      "epoch no.1 train no.107880  loss = 4.27334 avg_loss = 3.92243\n",
      "epoch no.1 train no.107890  loss = 2.94640 avg_loss = 3.93082\n",
      "epoch no.1 train no.107900  loss = 3.93363 avg_loss = 3.89498\n",
      "epoch no.1 train no.107910  loss = 3.70035 avg_loss = 3.87575\n",
      "epoch no.1 train no.107920  loss = 3.80140 avg_loss = 3.85203\n",
      "epoch no.1 train no.107930  loss = 4.10302 avg_loss = 3.89200\n",
      "epoch no.1 train no.107940  loss = 6.21693 avg_loss = 3.86447\n",
      "epoch no.1 train no.107950  loss = 5.37889 avg_loss = 3.92361\n",
      "epoch no.1 train no.107960  loss = 4.60899 avg_loss = 3.89132\n",
      "epoch no.1 train no.107970  loss = 2.49089 avg_loss = 3.87084\n",
      "epoch no.1 train no.107980  loss = 3.70739 avg_loss = 3.87312\n",
      "epoch no.1 train no.107990  loss = 2.55482 avg_loss = 3.84320\n",
      "epoch no.1 train no.108000  loss = 4.08702 avg_loss = 3.85331\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.108010  loss = 1.57334 avg_loss = 3.80080\n",
      "epoch no.1 train no.108020  loss = 4.48862 avg_loss = 3.79916\n",
      "epoch no.1 train no.108030  loss = 4.37098 avg_loss = 3.80489\n",
      "epoch no.1 train no.108040  loss = 3.85637 avg_loss = 3.78765\n",
      "epoch no.1 train no.108050  loss = 3.25830 avg_loss = 3.75378\n",
      "epoch no.1 train no.108060  loss = 2.99557 avg_loss = 3.73640\n",
      "epoch no.1 train no.108070  loss = 2.00376 avg_loss = 3.74617\n",
      "epoch no.1 train no.108080  loss = 3.54063 avg_loss = 3.76014\n",
      "epoch no.1 train no.108090  loss = 3.84210 avg_loss = 3.72250\n",
      "epoch no.1 train no.108100  loss = 2.85709 avg_loss = 3.72077\n",
      "epoch no.1 train no.108110  loss = 3.63053 avg_loss = 3.75303\n",
      "epoch no.1 train no.108120  loss = 3.06289 avg_loss = 3.78815\n",
      "epoch no.1 train no.108130  loss = 2.88838 avg_loss = 3.76535\n",
      "epoch no.1 train no.108140  loss = 1.88610 avg_loss = 3.71680\n",
      "epoch no.1 train no.108150  loss = 3.19614 avg_loss = 3.72305\n",
      "epoch no.1 train no.108160  loss = 2.52308 avg_loss = 3.68618\n",
      "epoch no.1 train no.108170  loss = 4.98147 avg_loss = 3.71389\n",
      "epoch no.1 train no.108180  loss = 7.53648 avg_loss = 3.75408\n",
      "epoch no.1 train no.108190  loss = 5.88549 avg_loss = 3.73415\n",
      "epoch no.1 train no.108200  loss = 3.17526 avg_loss = 3.69737\n",
      "epoch no.1 train no.108210  loss = 2.26514 avg_loss = 3.68053\n",
      "epoch no.1 train no.108220  loss = 4.85901 avg_loss = 3.76682\n",
      "epoch no.1 train no.108230  loss = 4.08698 avg_loss = 3.77789\n",
      "epoch no.1 train no.108240  loss = 4.26468 avg_loss = 3.77372\n",
      "epoch no.1 train no.108250  loss = 2.64325 avg_loss = 3.75608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.108260  loss = 3.07902 avg_loss = 3.75355\n",
      "epoch no.1 train no.108270  loss = 2.52630 avg_loss = 3.72313\n",
      "epoch no.1 train no.108280  loss = 5.79566 avg_loss = 3.73247\n",
      "epoch no.1 train no.108290  loss = 5.49553 avg_loss = 3.70378\n",
      "epoch no.1 train no.108300  loss = 4.64097 avg_loss = 3.70970\n",
      "epoch no.1 train no.108310  loss = 2.51479 avg_loss = 3.71970\n",
      "epoch no.1 train no.108320  loss = 3.13262 avg_loss = 3.73759\n",
      "epoch no.1 train no.108330  loss = 3.82863 avg_loss = 3.79029\n",
      "epoch no.1 train no.108340  loss = 3.48121 avg_loss = 3.83671\n",
      "epoch no.1 train no.108350  loss = 1.94072 avg_loss = 3.82447\n",
      "epoch no.1 train no.108360  loss = 3.66901 avg_loss = 3.79334\n",
      "epoch no.1 train no.108370  loss = 5.01833 avg_loss = 3.81047\n",
      "epoch no.1 train no.108380  loss = 4.51496 avg_loss = 3.82762\n",
      "epoch no.1 train no.108390  loss = 2.83091 avg_loss = 3.82433\n",
      "epoch no.1 train no.108400  loss = 3.08016 avg_loss = 3.80064\n",
      "epoch no.1 train no.108410  loss = 4.20062 avg_loss = 3.76821\n",
      "epoch no.1 train no.108420  loss = 2.71788 avg_loss = 3.77210\n",
      "epoch no.1 train no.108430  loss = 2.62893 avg_loss = 3.74357\n",
      "epoch no.1 train no.108440  loss = 2.68310 avg_loss = 3.70648\n",
      "epoch no.1 train no.108450  loss = 6.44275 avg_loss = 3.69250\n",
      "epoch no.1 train no.108460  loss = 2.64828 avg_loss = 3.70175\n",
      "epoch no.1 train no.108470  loss = 5.89855 avg_loss = 3.73958\n",
      "epoch no.1 train no.108480  loss = 2.64181 avg_loss = 3.72465\n",
      "epoch no.1 train no.108490  loss = 5.64808 avg_loss = 3.73927\n",
      "epoch no.1 train no.108500  loss = 5.02617 avg_loss = 3.76266\n",
      "epoch no.1 train no.108510  loss = 4.60872 avg_loss = 3.71426\n",
      "epoch no.1 train no.108520  loss = 4.87782 avg_loss = 3.70787\n",
      "epoch no.1 train no.108530  loss = 3.28842 avg_loss = 3.72335\n",
      "epoch no.1 train no.108540  loss = 4.92340 avg_loss = 3.73162\n",
      "epoch no.1 train no.108550  loss = 2.97253 avg_loss = 3.77277\n",
      "epoch no.1 train no.108560  loss = 4.04267 avg_loss = 3.79064\n",
      "epoch no.1 train no.108570  loss = 2.27114 avg_loss = 3.79262\n",
      "epoch no.1 train no.108580  loss = 7.03319 avg_loss = 3.81163\n",
      "epoch no.1 train no.108590  loss = 3.05614 avg_loss = 3.81240\n",
      "epoch no.1 train no.108600  loss = 5.43820 avg_loss = 3.84733\n",
      "epoch no.1 train no.108610  loss = 3.22384 avg_loss = 3.78837\n",
      "epoch no.1 train no.108620  loss = 2.89206 avg_loss = 3.77825\n",
      "epoch no.1 train no.108630  loss = 2.45929 avg_loss = 3.76497\n",
      "epoch no.1 train no.108640  loss = 4.50283 avg_loss = 3.72183\n",
      "epoch no.1 train no.108650  loss = 3.76054 avg_loss = 3.68985\n",
      "epoch no.1 train no.108660  loss = 6.98697 avg_loss = 3.72298\n",
      "epoch no.1 train no.108670  loss = 4.84034 avg_loss = 3.70122\n",
      "epoch no.1 train no.108680  loss = 1.82951 avg_loss = 3.67156\n",
      "epoch no.1 train no.108690  loss = 3.50069 avg_loss = 3.70346\n",
      "epoch no.1 train no.108700  loss = 5.72996 avg_loss = 3.69011\n",
      "epoch no.1 train no.108710  loss = 6.03902 avg_loss = 3.67906\n",
      "epoch no.1 train no.108720  loss = 2.79989 avg_loss = 3.63393\n",
      "epoch no.1 train no.108730  loss = 5.38386 avg_loss = 3.67611\n",
      "epoch no.1 train no.108740  loss = 3.18931 avg_loss = 3.65908\n",
      "epoch no.1 train no.108750  loss = 5.11186 avg_loss = 3.66287\n",
      "epoch no.1 train no.108760  loss = 2.93595 avg_loss = 3.63809\n",
      "epoch no.1 train no.108770  loss = 2.46442 avg_loss = 3.64156\n",
      "epoch no.1 train no.108780  loss = 3.00901 avg_loss = 3.63447\n",
      "epoch no.1 train no.108790  loss = 3.40094 avg_loss = 3.65869\n",
      "epoch no.1 train no.108800  loss = 5.24998 avg_loss = 3.69897\n",
      "epoch no.1 train no.108810  loss = 3.55225 avg_loss = 3.73726\n",
      "epoch no.1 train no.108820  loss = 4.50577 avg_loss = 3.75150\n",
      "epoch no.1 train no.108830  loss = 2.63173 avg_loss = 3.75132\n",
      "epoch no.1 train no.108840  loss = 4.29734 avg_loss = 3.76653\n",
      "epoch no.1 train no.108850  loss = 4.98638 avg_loss = 3.74131\n",
      "epoch no.1 train no.108860  loss = 2.84641 avg_loss = 3.72968\n",
      "epoch no.1 train no.108870  loss = 5.49081 avg_loss = 3.74878\n",
      "epoch no.1 train no.108880  loss = 2.99821 avg_loss = 3.76186\n",
      "epoch no.1 train no.108890  loss = 4.52540 avg_loss = 3.76690\n",
      "epoch no.1 train no.108900  loss = 2.97401 avg_loss = 3.79056\n",
      "epoch no.1 train no.108910  loss = 2.68097 avg_loss = 3.78262\n",
      "epoch no.1 train no.108920  loss = 3.32800 avg_loss = 3.79467\n",
      "epoch no.1 train no.108930  loss = 2.38605 avg_loss = 3.74314\n",
      "epoch no.1 train no.108940  loss = 3.44341 avg_loss = 3.73548\n",
      "epoch no.1 train no.108950  loss = 4.81779 avg_loss = 3.70547\n",
      "epoch no.1 train no.108960  loss = 2.77225 avg_loss = 3.71392\n",
      "epoch no.1 train no.108970  loss = 3.87835 avg_loss = 3.72232\n",
      "epoch no.1 train no.108980  loss = 2.78155 avg_loss = 3.70388\n",
      "epoch no.1 train no.108990  loss = 3.11220 avg_loss = 3.76990\n",
      "epoch no.1 train no.109000  loss = 3.26302 avg_loss = 3.73419\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.109010  loss = 2.29209 avg_loss = 3.74755\n",
      "epoch no.1 train no.109020  loss = 2.89882 avg_loss = 3.74827\n",
      "epoch no.1 train no.109030  loss = 2.95623 avg_loss = 3.71960\n",
      "epoch no.1 train no.109040  loss = 4.93691 avg_loss = 3.74247\n",
      "epoch no.1 train no.109050  loss = 4.94538 avg_loss = 3.76869\n",
      "epoch no.1 train no.109060  loss = 2.46557 avg_loss = 3.78186\n",
      "epoch no.1 train no.109070  loss = 3.39464 avg_loss = 3.80004\n",
      "epoch no.1 train no.109080  loss = 7.26009 avg_loss = 3.82362\n",
      "epoch no.1 train no.109090  loss = 5.40706 avg_loss = 3.84957\n",
      "epoch no.1 train no.109100  loss = 2.45782 avg_loss = 3.82453\n",
      "epoch no.1 train no.109110  loss = 2.76438 avg_loss = 3.79374\n",
      "epoch no.1 train no.109120  loss = 3.57479 avg_loss = 3.83312\n",
      "epoch no.1 train no.109130  loss = 4.17756 avg_loss = 3.86891\n",
      "epoch no.1 train no.109140  loss = 3.51282 avg_loss = 3.82656\n",
      "epoch no.1 train no.109150  loss = 2.21313 avg_loss = 3.84126\n",
      "epoch no.1 train no.109160  loss = 6.25129 avg_loss = 3.88372\n",
      "epoch no.1 train no.109170  loss = 2.40305 avg_loss = 3.86419\n",
      "epoch no.1 train no.109180  loss = 2.36889 avg_loss = 3.85220\n",
      "epoch no.1 train no.109190  loss = 3.18132 avg_loss = 3.85828\n",
      "epoch no.1 train no.109200  loss = 4.28633 avg_loss = 3.83050\n",
      "epoch no.1 train no.109210  loss = 2.83706 avg_loss = 3.80343\n",
      "epoch no.1 train no.109220  loss = 3.86397 avg_loss = 3.80360\n",
      "epoch no.1 train no.109230  loss = 4.07579 avg_loss = 3.76138\n",
      "epoch no.1 train no.109240  loss = 2.58566 avg_loss = 3.76323\n",
      "epoch no.1 train no.109250  loss = 3.38871 avg_loss = 3.76976\n",
      "epoch no.1 train no.109260  loss = 4.71611 avg_loss = 3.74056\n",
      "epoch no.1 train no.109270  loss = 4.33129 avg_loss = 3.75423\n",
      "epoch no.1 train no.109280  loss = 3.80732 avg_loss = 3.78229\n",
      "epoch no.1 train no.109290  loss = 3.48754 avg_loss = 3.78019\n",
      "epoch no.1 train no.109300  loss = 2.79791 avg_loss = 3.76137\n",
      "epoch no.1 train no.109310  loss = 2.81679 avg_loss = 3.71216\n",
      "epoch no.1 train no.109320  loss = 4.20516 avg_loss = 3.69697\n",
      "epoch no.1 train no.109330  loss = 3.20400 avg_loss = 3.81486\n",
      "epoch no.1 train no.109340  loss = 3.96418 avg_loss = 3.78648\n",
      "epoch no.1 train no.109350  loss = 2.39292 avg_loss = 3.80363\n",
      "epoch no.1 train no.109360  loss = 2.30527 avg_loss = 3.73961\n",
      "epoch no.1 train no.109370  loss = 3.89852 avg_loss = 3.74768\n",
      "epoch no.1 train no.109380  loss = 5.09804 avg_loss = 3.75298\n",
      "epoch no.1 train no.109390  loss = 5.47835 avg_loss = 3.75072\n",
      "epoch no.1 train no.109400  loss = 5.00558 avg_loss = 3.81097\n",
      "epoch no.1 train no.109410  loss = 2.35203 avg_loss = 3.75880\n",
      "epoch no.1 train no.109420  loss = 3.49201 avg_loss = 3.73334\n",
      "epoch no.1 train no.109430  loss = 2.84931 avg_loss = 3.74930\n",
      "epoch no.1 train no.109440  loss = 4.09463 avg_loss = 3.73533\n",
      "epoch no.1 train no.109450  loss = 4.13599 avg_loss = 3.69819\n",
      "epoch no.1 train no.109460  loss = 2.14531 avg_loss = 3.69143\n",
      "epoch no.1 train no.109470  loss = 3.53737 avg_loss = 3.72702\n",
      "epoch no.1 train no.109480  loss = 2.09641 avg_loss = 3.67334\n",
      "epoch no.1 train no.109490  loss = 3.31120 avg_loss = 3.67386\n",
      "epoch no.1 train no.109500  loss = 3.00571 avg_loss = 3.63224\n",
      "epoch no.1 train no.109510  loss = 3.43636 avg_loss = 3.68645\n",
      "epoch no.1 train no.109520  loss = 2.05684 avg_loss = 3.67955\n",
      "epoch no.1 train no.109530  loss = 4.11107 avg_loss = 3.68019\n",
      "epoch no.1 train no.109540  loss = 2.66751 avg_loss = 3.72360\n",
      "epoch no.1 train no.109550  loss = 4.13729 avg_loss = 3.76060\n",
      "epoch no.1 train no.109560  loss = 5.11771 avg_loss = 3.74142\n",
      "epoch no.1 train no.109570  loss = 3.57138 avg_loss = 3.74959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.109580  loss = 2.13812 avg_loss = 3.75859\n",
      "epoch no.1 train no.109590  loss = 5.23932 avg_loss = 3.76215\n",
      "epoch no.1 train no.109600  loss = 3.05064 avg_loss = 3.74334\n",
      "epoch no.1 train no.109610  loss = 2.95056 avg_loss = 3.79521\n",
      "epoch no.1 train no.109620  loss = 2.43890 avg_loss = 3.77055\n",
      "epoch no.1 train no.109630  loss = 3.50914 avg_loss = 3.77400\n",
      "epoch no.1 train no.109640  loss = 3.95985 avg_loss = 3.81876\n",
      "epoch no.1 train no.109650  loss = 4.57353 avg_loss = 3.82892\n",
      "epoch no.1 train no.109660  loss = 4.90675 avg_loss = 3.86859\n",
      "epoch no.1 train no.109670  loss = 2.44334 avg_loss = 3.89053\n",
      "epoch no.1 train no.109680  loss = 4.40122 avg_loss = 3.90392\n",
      "epoch no.1 train no.109690  loss = 6.37413 avg_loss = 3.89006\n",
      "epoch no.1 train no.109700  loss = 3.61052 avg_loss = 3.92039\n",
      "epoch no.1 train no.109710  loss = 3.21956 avg_loss = 3.88333\n",
      "epoch no.1 train no.109720  loss = 4.31366 avg_loss = 3.86831\n",
      "epoch no.1 train no.109730  loss = 3.52972 avg_loss = 3.92059\n",
      "epoch no.1 train no.109740  loss = 2.14613 avg_loss = 3.88601\n",
      "epoch no.1 train no.109750  loss = 3.21738 avg_loss = 3.91035\n",
      "epoch no.1 train no.109760  loss = 1.60601 avg_loss = 3.89724\n",
      "epoch no.1 train no.109770  loss = 2.91259 avg_loss = 3.90336\n",
      "epoch no.1 train no.109780  loss = 1.91419 avg_loss = 3.85060\n",
      "epoch no.1 train no.109790  loss = 4.15689 avg_loss = 3.83412\n",
      "epoch no.1 train no.109800  loss = 3.14704 avg_loss = 3.76882\n",
      "epoch no.1 train no.109810  loss = 3.42433 avg_loss = 3.78184\n",
      "epoch no.1 train no.109820  loss = 7.02932 avg_loss = 3.77463\n",
      "epoch no.1 train no.109830  loss = 3.11747 avg_loss = 3.74855\n",
      "epoch no.1 train no.109840  loss = 3.31366 avg_loss = 3.74029\n",
      "epoch no.1 train no.109850  loss = 3.93141 avg_loss = 3.73463\n",
      "epoch no.1 train no.109860  loss = 4.21229 avg_loss = 3.79912\n",
      "epoch no.1 train no.109870  loss = 5.14236 avg_loss = 3.81210\n",
      "epoch no.1 train no.109880  loss = 3.08319 avg_loss = 3.79229\n",
      "epoch no.1 train no.109890  loss = 5.26467 avg_loss = 3.77575\n",
      "epoch no.1 train no.109900  loss = 2.10169 avg_loss = 3.80609\n",
      "epoch no.1 train no.109910  loss = 2.38357 avg_loss = 3.76624\n",
      "epoch no.1 train no.109920  loss = 3.06295 avg_loss = 3.77528\n",
      "epoch no.1 train no.109930  loss = 5.90287 avg_loss = 3.76119\n",
      "epoch no.1 train no.109940  loss = 3.49882 avg_loss = 3.76457\n",
      "epoch no.1 train no.109950  loss = 3.93637 avg_loss = 3.79095\n",
      "epoch no.1 train no.109960  loss = 4.01545 avg_loss = 3.83740\n",
      "epoch no.1 train no.109970  loss = 4.06263 avg_loss = 3.78790\n",
      "epoch no.1 train no.109980  loss = 4.52014 avg_loss = 3.72783\n",
      "epoch no.1 train no.109990  loss = 4.23132 avg_loss = 3.71571\n",
      "epoch no.1 train no.110000  loss = 3.08433 avg_loss = 3.67819\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 노래</s>\n",
      "epoch no.1 train no.110010  loss = 3.01916 avg_loss = 3.63784\n",
      "epoch no.1 train no.110020  loss = 3.53111 avg_loss = 3.69040\n",
      "epoch no.1 train no.110030  loss = 3.47358 avg_loss = 3.71030\n",
      "epoch no.1 train no.110040  loss = 7.34642 avg_loss = 3.78400\n",
      "epoch no.1 train no.110050  loss = 5.28214 avg_loss = 3.79022\n",
      "epoch no.1 train no.110060  loss = 6.46856 avg_loss = 3.81128\n",
      "epoch no.1 train no.110070  loss = 5.44892 avg_loss = 3.81288\n",
      "epoch no.1 train no.110080  loss = 2.45255 avg_loss = 3.78036\n",
      "epoch no.1 train no.110090  loss = 3.65463 avg_loss = 3.80963\n",
      "epoch no.1 train no.110100  loss = 4.02683 avg_loss = 3.81740\n",
      "epoch no.1 train no.110110  loss = 3.41631 avg_loss = 3.79701\n",
      "epoch no.1 train no.110120  loss = 4.34804 avg_loss = 3.81419\n",
      "epoch no.1 train no.110130  loss = 2.77765 avg_loss = 3.86912\n",
      "epoch no.1 train no.110140  loss = 2.33583 avg_loss = 3.86464\n",
      "epoch no.1 train no.110150  loss = 6.82346 avg_loss = 3.86662\n",
      "epoch no.1 train no.110160  loss = 3.40016 avg_loss = 3.89624\n",
      "epoch no.1 train no.110170  loss = 4.10249 avg_loss = 3.91429\n",
      "epoch no.1 train no.110180  loss = 4.89709 avg_loss = 3.88017\n",
      "epoch no.1 train no.110190  loss = 1.89748 avg_loss = 3.84860\n",
      "epoch no.1 train no.110200  loss = 2.53665 avg_loss = 3.81314\n",
      "epoch no.1 train no.110210  loss = 5.06133 avg_loss = 3.84680\n",
      "epoch no.1 train no.110220  loss = 3.82013 avg_loss = 3.84143\n",
      "epoch no.1 train no.110230  loss = 2.82403 avg_loss = 3.80894\n",
      "epoch no.1 train no.110240  loss = 4.14726 avg_loss = 3.82346\n",
      "epoch no.1 train no.110250  loss = 3.78082 avg_loss = 3.74989\n",
      "epoch no.1 train no.110260  loss = 4.99172 avg_loss = 3.74946\n",
      "epoch no.1 train no.110270  loss = 4.66144 avg_loss = 3.75751\n",
      "epoch no.1 train no.110280  loss = 3.50432 avg_loss = 3.73481\n",
      "epoch no.1 train no.110290  loss = 3.77894 avg_loss = 3.70801\n",
      "epoch no.1 train no.110300  loss = 1.52035 avg_loss = 3.76318\n",
      "epoch no.1 train no.110310  loss = 3.17372 avg_loss = 3.75833\n",
      "epoch no.1 train no.110320  loss = 5.84435 avg_loss = 3.78123\n",
      "epoch no.1 train no.110330  loss = 2.68417 avg_loss = 3.69476\n",
      "epoch no.1 train no.110340  loss = 3.54193 avg_loss = 3.72342\n",
      "epoch no.1 train no.110350  loss = 3.82115 avg_loss = 3.76651\n",
      "epoch no.1 train no.110360  loss = 2.80980 avg_loss = 3.78678\n",
      "epoch no.1 train no.110370  loss = 3.02155 avg_loss = 3.85228\n",
      "epoch no.1 train no.110380  loss = 3.62288 avg_loss = 3.87004\n",
      "epoch no.1 train no.110390  loss = 5.21008 avg_loss = 3.84368\n",
      "epoch no.1 train no.110400  loss = 3.68824 avg_loss = 3.79648\n",
      "epoch no.1 train no.110410  loss = 3.58320 avg_loss = 3.79081\n",
      "epoch no.1 train no.110420  loss = 2.87816 avg_loss = 3.83122\n",
      "epoch no.1 train no.110430  loss = 3.46859 avg_loss = 3.80517\n",
      "epoch no.1 train no.110440  loss = 3.60640 avg_loss = 3.84281\n",
      "epoch no.1 train no.110450  loss = 3.55016 avg_loss = 3.83117\n",
      "epoch no.1 train no.110460  loss = 2.69141 avg_loss = 3.78157\n",
      "epoch no.1 train no.110470  loss = 3.07685 avg_loss = 3.77001\n",
      "epoch no.1 train no.110480  loss = 3.87826 avg_loss = 3.84242\n",
      "epoch no.1 train no.110490  loss = 3.22743 avg_loss = 3.83185\n",
      "epoch no.1 train no.110500  loss = 4.35871 avg_loss = 3.85130\n",
      "epoch no.1 train no.110510  loss = 2.24629 avg_loss = 3.81226\n",
      "epoch no.1 train no.110520  loss = 4.74139 avg_loss = 3.81690\n",
      "epoch no.1 train no.110530  loss = 3.79516 avg_loss = 3.81194\n",
      "epoch no.1 train no.110540  loss = 2.30132 avg_loss = 3.77938\n",
      "epoch no.1 train no.110550  loss = 5.60388 avg_loss = 3.88142\n",
      "epoch no.1 train no.110560  loss = 4.39773 avg_loss = 3.90639\n",
      "epoch no.1 train no.110570  loss = 4.29239 avg_loss = 3.86715\n",
      "epoch no.1 train no.110580  loss = 3.00405 avg_loss = 3.83759\n",
      "epoch no.1 train no.110590  loss = 1.88640 avg_loss = 3.78260\n",
      "epoch no.1 train no.110600  loss = 3.90038 avg_loss = 3.77546\n",
      "epoch no.1 train no.110610  loss = 3.02688 avg_loss = 3.72169\n",
      "epoch no.1 train no.110620  loss = 3.68729 avg_loss = 3.69452\n",
      "epoch no.1 train no.110630  loss = 2.18288 avg_loss = 3.71686\n",
      "epoch no.1 train no.110640  loss = 1.30323 avg_loss = 3.72467\n",
      "epoch no.1 train no.110650  loss = 5.27697 avg_loss = 3.74300\n",
      "epoch no.1 train no.110660  loss = 4.50509 avg_loss = 3.73252\n",
      "epoch no.1 train no.110670  loss = 3.77880 avg_loss = 3.73297\n",
      "epoch no.1 train no.110680  loss = 3.31382 avg_loss = 3.80202\n",
      "epoch no.1 train no.110690  loss = 5.89464 avg_loss = 3.80988\n",
      "epoch no.1 train no.110700  loss = 2.05889 avg_loss = 3.74081\n",
      "epoch no.1 train no.110710  loss = 3.05740 avg_loss = 3.70027\n",
      "epoch no.1 train no.110720  loss = 2.17328 avg_loss = 3.62607\n",
      "epoch no.1 train no.110730  loss = 5.32183 avg_loss = 3.61022\n",
      "epoch no.1 train no.110740  loss = 2.41467 avg_loss = 3.58967\n",
      "epoch no.1 train no.110750  loss = 1.83553 avg_loss = 3.60291\n",
      "epoch no.1 train no.110760  loss = 4.88960 avg_loss = 3.54730\n",
      "epoch no.1 train no.110770  loss = 3.11630 avg_loss = 3.56019\n",
      "epoch no.1 train no.110780  loss = 2.11333 avg_loss = 3.58003\n",
      "epoch no.1 train no.110790  loss = 1.88206 avg_loss = 3.52932\n",
      "epoch no.1 train no.110800  loss = 3.40388 avg_loss = 3.52792\n",
      "epoch no.1 train no.110810  loss = 6.20122 avg_loss = 3.58375\n",
      "epoch no.1 train no.110820  loss = 3.53117 avg_loss = 3.65071\n",
      "epoch no.1 train no.110830  loss = 5.32553 avg_loss = 3.69850\n",
      "epoch no.1 train no.110840  loss = 3.73854 avg_loss = 3.67788\n",
      "epoch no.1 train no.110850  loss = 2.53497 avg_loss = 3.68679\n",
      "epoch no.1 train no.110860  loss = 3.03560 avg_loss = 3.69625\n",
      "epoch no.1 train no.110870  loss = 3.51412 avg_loss = 3.68069\n",
      "epoch no.1 train no.110880  loss = 3.65241 avg_loss = 3.73284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.110890  loss = 2.65270 avg_loss = 3.76047\n",
      "epoch no.1 train no.110900  loss = 2.80102 avg_loss = 3.74418\n",
      "epoch no.1 train no.110910  loss = 5.27401 avg_loss = 3.78934\n",
      "epoch no.1 train no.110920  loss = 3.30417 avg_loss = 3.76366\n",
      "epoch no.1 train no.110930  loss = 2.68594 avg_loss = 3.74467\n",
      "epoch no.1 train no.110940  loss = 1.95739 avg_loss = 3.71041\n",
      "epoch no.1 train no.110950  loss = 4.77125 avg_loss = 3.71030\n",
      "epoch no.1 train no.110960  loss = 6.22400 avg_loss = 3.73016\n",
      "epoch no.1 train no.110970  loss = 3.58292 avg_loss = 3.71072\n",
      "epoch no.1 train no.110980  loss = 3.50980 avg_loss = 3.72039\n",
      "epoch no.1 train no.110990  loss = 5.14027 avg_loss = 3.70256\n",
      "epoch no.1 train no.111000  loss = 2.03532 avg_loss = 3.70821\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 음악</s>\n",
      "epoch no.1 train no.111010  loss = 4.22145 avg_loss = 3.74367\n",
      "epoch no.1 train no.111020  loss = 3.79070 avg_loss = 3.72776\n",
      "epoch no.1 train no.111030  loss = 4.07131 avg_loss = 3.70480\n",
      "epoch no.1 train no.111040  loss = 3.32461 avg_loss = 3.71619\n",
      "epoch no.1 train no.111050  loss = 3.43119 avg_loss = 3.74207\n",
      "epoch no.1 train no.111060  loss = 2.94679 avg_loss = 3.74069\n",
      "epoch no.1 train no.111070  loss = 2.71431 avg_loss = 3.70883\n",
      "epoch no.1 train no.111080  loss = 5.47835 avg_loss = 3.78239\n",
      "epoch no.1 train no.111090  loss = 3.74815 avg_loss = 3.76413\n",
      "epoch no.1 train no.111100  loss = 5.31499 avg_loss = 3.80210\n",
      "epoch no.1 train no.111110  loss = 4.31032 avg_loss = 3.79782\n",
      "epoch no.1 train no.111120  loss = 3.63199 avg_loss = 3.77756\n",
      "epoch no.1 train no.111130  loss = 4.70328 avg_loss = 3.79545\n",
      "epoch no.1 train no.111140  loss = 4.16137 avg_loss = 3.81384\n",
      "epoch no.1 train no.111150  loss = 3.69950 avg_loss = 3.77139\n",
      "epoch no.1 train no.111160  loss = 4.19205 avg_loss = 3.81110\n",
      "epoch no.1 train no.111170  loss = 3.20481 avg_loss = 3.82437\n",
      "epoch no.1 train no.111180  loss = 2.69586 avg_loss = 3.84258\n",
      "epoch no.1 train no.111190  loss = 3.56591 avg_loss = 3.78665\n",
      "epoch no.1 train no.111200  loss = 4.12504 avg_loss = 3.79885\n",
      "epoch no.1 train no.111210  loss = 4.06047 avg_loss = 3.76304\n",
      "epoch no.1 train no.111220  loss = 5.37153 avg_loss = 3.81019\n",
      "epoch no.1 train no.111230  loss = 3.21458 avg_loss = 3.78682\n",
      "epoch no.1 train no.111240  loss = 1.99273 avg_loss = 3.74120\n",
      "epoch no.1 train no.111250  loss = 3.60941 avg_loss = 3.68107\n",
      "epoch no.1 train no.111260  loss = 3.25267 avg_loss = 3.69755\n",
      "epoch no.1 train no.111270  loss = 3.54290 avg_loss = 3.66968\n",
      "epoch no.1 train no.111280  loss = 3.03147 avg_loss = 3.75196\n",
      "epoch no.1 train no.111290  loss = 2.60771 avg_loss = 3.73307\n",
      "epoch no.1 train no.111300  loss = 3.31902 avg_loss = 3.81760\n",
      "epoch no.1 train no.111310  loss = 4.21043 avg_loss = 3.79969\n",
      "epoch no.1 train no.111320  loss = 2.97344 avg_loss = 3.79385\n",
      "epoch no.1 train no.111330  loss = 4.40194 avg_loss = 3.86666\n",
      "epoch no.1 train no.111340  loss = 3.38576 avg_loss = 3.84896\n",
      "epoch no.1 train no.111350  loss = 2.11697 avg_loss = 3.84108\n",
      "epoch no.1 train no.111360  loss = 3.06919 avg_loss = 3.84695\n",
      "epoch no.1 train no.111370  loss = 4.61071 avg_loss = 3.83331\n",
      "epoch no.1 train no.111380  loss = 4.48332 avg_loss = 3.82222\n",
      "epoch no.1 train no.111390  loss = 3.84077 avg_loss = 3.81550\n",
      "epoch no.1 train no.111400  loss = 3.87077 avg_loss = 3.85185\n",
      "epoch no.1 train no.111410  loss = 6.00922 avg_loss = 3.85951\n",
      "epoch no.1 train no.111420  loss = 3.44488 avg_loss = 3.82791\n",
      "epoch no.1 train no.111430  loss = 2.46429 avg_loss = 3.79859\n",
      "epoch no.1 train no.111440  loss = 4.34995 avg_loss = 3.78734\n",
      "epoch no.1 train no.111450  loss = 2.87964 avg_loss = 3.80523\n",
      "epoch no.1 train no.111460  loss = 6.91168 avg_loss = 3.80871\n",
      "epoch no.1 train no.111470  loss = 2.77269 avg_loss = 3.78682\n",
      "epoch no.1 train no.111480  loss = 7.71562 avg_loss = 3.85184\n",
      "epoch no.1 train no.111490  loss = 3.07337 avg_loss = 3.80979\n",
      "epoch no.1 train no.111500  loss = 4.78986 avg_loss = 3.90306\n",
      "epoch no.1 train no.111510  loss = 4.62206 avg_loss = 3.88108\n",
      "epoch no.1 train no.111520  loss = 3.40270 avg_loss = 3.88891\n",
      "epoch no.1 train no.111530  loss = 5.07309 avg_loss = 3.91787\n",
      "epoch no.1 train no.111540  loss = 4.03641 avg_loss = 3.93793\n",
      "epoch no.1 train no.111550  loss = 1.98080 avg_loss = 3.92963\n",
      "epoch no.1 train no.111560  loss = 2.01093 avg_loss = 3.94713\n",
      "epoch no.1 train no.111570  loss = 4.35657 avg_loss = 3.89252\n",
      "epoch no.1 train no.111580  loss = 3.93028 avg_loss = 3.83617\n",
      "epoch no.1 train no.111590  loss = 7.35299 avg_loss = 3.86821\n",
      "epoch no.1 train no.111600  loss = 2.19954 avg_loss = 3.79547\n",
      "epoch no.1 train no.111610  loss = 3.98204 avg_loss = 3.80791\n",
      "epoch no.1 train no.111620  loss = 4.11066 avg_loss = 3.80053\n",
      "epoch no.1 train no.111630  loss = 2.48984 avg_loss = 3.81503\n",
      "epoch no.1 train no.111640  loss = 3.76765 avg_loss = 3.72784\n",
      "epoch no.1 train no.111650  loss = 4.62094 avg_loss = 3.77784\n",
      "epoch no.1 train no.111660  loss = 3.28377 avg_loss = 3.79249\n",
      "epoch no.1 train no.111670  loss = 3.76469 avg_loss = 3.76790\n",
      "epoch no.1 train no.111680  loss = 3.14228 avg_loss = 3.80099\n",
      "epoch no.1 train no.111690  loss = 6.49749 avg_loss = 3.80634\n",
      "epoch no.1 train no.111700  loss = 3.66174 avg_loss = 3.78720\n",
      "epoch no.1 train no.111710  loss = 7.86532 avg_loss = 3.82404\n",
      "epoch no.1 train no.111720  loss = 3.06202 avg_loss = 3.77660\n",
      "epoch no.1 train no.111730  loss = 6.35778 avg_loss = 3.88936\n",
      "epoch no.1 train no.111740  loss = 4.09968 avg_loss = 3.89101\n",
      "epoch no.1 train no.111750  loss = 3.35171 avg_loss = 3.82749\n",
      "epoch no.1 train no.111760  loss = 2.84951 avg_loss = 3.81400\n",
      "epoch no.1 train no.111770  loss = 3.30939 avg_loss = 3.78402\n",
      "epoch no.1 train no.111780  loss = 3.08219 avg_loss = 3.76905\n",
      "epoch no.1 train no.111790  loss = 2.85118 avg_loss = 3.76319\n",
      "epoch no.1 train no.111800  loss = 2.81421 avg_loss = 3.82023\n",
      "epoch no.1 train no.111810  loss = 2.25542 avg_loss = 3.80438\n",
      "epoch no.1 train no.111820  loss = 2.03078 avg_loss = 3.72965\n",
      "epoch no.1 train no.111830  loss = 3.04385 avg_loss = 3.79536\n",
      "epoch no.1 train no.111840  loss = 5.55337 avg_loss = 3.79877\n",
      "epoch no.1 train no.111850  loss = 5.69827 avg_loss = 3.78046\n",
      "epoch no.1 train no.111860  loss = 3.03028 avg_loss = 3.78930\n",
      "epoch no.1 train no.111870  loss = 3.71734 avg_loss = 3.78363\n",
      "epoch no.1 train no.111880  loss = 3.75402 avg_loss = 3.78463\n",
      "epoch no.1 train no.111890  loss = 2.14075 avg_loss = 3.82002\n",
      "epoch no.1 train no.111900  loss = 5.49723 avg_loss = 3.81710\n",
      "epoch no.1 train no.111910  loss = 2.45416 avg_loss = 3.84223\n",
      "epoch no.1 train no.111920  loss = 3.45703 avg_loss = 3.80093\n",
      "epoch no.1 train no.111930  loss = 2.74125 avg_loss = 3.80467\n",
      "epoch no.1 train no.111940  loss = 3.68900 avg_loss = 3.82523\n",
      "epoch no.1 train no.111950  loss = 2.79134 avg_loss = 3.82985\n",
      "epoch no.1 train no.111960  loss = 2.53979 avg_loss = 3.77777\n",
      "epoch no.1 train no.111970  loss = 3.46507 avg_loss = 3.78386\n",
      "epoch no.1 train no.111980  loss = 3.86753 avg_loss = 3.76273\n",
      "epoch no.1 train no.111990  loss = 2.87095 avg_loss = 3.76820\n",
      "epoch no.1 train no.112000  loss = 1.93722 avg_loss = 3.71100\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.112010  loss = 2.75496 avg_loss = 3.71857\n",
      "epoch no.1 train no.112020  loss = 4.04613 avg_loss = 3.67102\n",
      "epoch no.1 train no.112030  loss = 2.83746 avg_loss = 3.65120\n",
      "epoch no.1 train no.112040  loss = 3.18548 avg_loss = 3.64661\n",
      "epoch no.1 train no.112050  loss = 2.49827 avg_loss = 3.64387\n",
      "epoch no.1 train no.112060  loss = 5.18203 avg_loss = 3.70989\n",
      "epoch no.1 train no.112070  loss = 2.53708 avg_loss = 3.76790\n",
      "epoch no.1 train no.112080  loss = 2.66896 avg_loss = 3.74656\n",
      "epoch no.1 train no.112090  loss = 5.32553 avg_loss = 3.74453\n",
      "epoch no.1 train no.112100  loss = 3.76579 avg_loss = 3.77551\n",
      "epoch no.1 train no.112110  loss = 3.86724 avg_loss = 3.76345\n",
      "epoch no.1 train no.112120  loss = 3.04552 avg_loss = 3.76200\n",
      "epoch no.1 train no.112130  loss = 5.16550 avg_loss = 3.75768\n",
      "epoch no.1 train no.112140  loss = 3.03765 avg_loss = 3.78020\n",
      "epoch no.1 train no.112150  loss = 2.54909 avg_loss = 3.77177\n",
      "epoch no.1 train no.112160  loss = 3.88791 avg_loss = 3.77353\n",
      "epoch no.1 train no.112170  loss = 5.32082 avg_loss = 3.80287\n",
      "epoch no.1 train no.112180  loss = 4.90799 avg_loss = 3.81664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.112190  loss = 3.47733 avg_loss = 3.77225\n",
      "epoch no.1 train no.112200  loss = 4.06018 avg_loss = 3.77982\n",
      "epoch no.1 train no.112210  loss = 6.24640 avg_loss = 3.77906\n",
      "epoch no.1 train no.112220  loss = 2.69792 avg_loss = 3.74582\n",
      "epoch no.1 train no.112230  loss = 3.29278 avg_loss = 3.71140\n",
      "epoch no.1 train no.112240  loss = 4.18574 avg_loss = 3.70720\n",
      "epoch no.1 train no.112250  loss = 3.59990 avg_loss = 3.72237\n",
      "epoch no.1 train no.112260  loss = 5.30660 avg_loss = 3.73189\n",
      "epoch no.1 train no.112270  loss = 5.42230 avg_loss = 3.74829\n",
      "epoch no.1 train no.112280  loss = 4.97427 avg_loss = 3.75596\n",
      "epoch no.1 train no.112290  loss = 3.74525 avg_loss = 3.70020\n",
      "epoch no.1 train no.112300  loss = 2.43216 avg_loss = 3.66637\n",
      "epoch no.1 train no.112310  loss = 2.80870 avg_loss = 3.64665\n",
      "epoch no.1 train no.112320  loss = 3.69413 avg_loss = 3.65050\n",
      "epoch no.1 train no.112330  loss = 2.92811 avg_loss = 3.66230\n",
      "epoch no.1 train no.112340  loss = 3.43560 avg_loss = 3.67523\n",
      "epoch no.1 train no.112350  loss = 4.70465 avg_loss = 3.67424\n",
      "epoch no.1 train no.112360  loss = 2.64859 avg_loss = 3.63884\n",
      "epoch no.1 train no.112370  loss = 3.80799 avg_loss = 3.68736\n",
      "epoch no.1 train no.112380  loss = 2.71595 avg_loss = 3.64602\n",
      "epoch no.1 train no.112390  loss = 2.79532 avg_loss = 3.59744\n",
      "epoch no.1 train no.112400  loss = 3.95116 avg_loss = 3.64448\n",
      "epoch no.1 train no.112410  loss = 5.47563 avg_loss = 3.67234\n",
      "epoch no.1 train no.112420  loss = 3.48053 avg_loss = 3.72041\n",
      "epoch no.1 train no.112430  loss = 4.96017 avg_loss = 3.75483\n",
      "epoch no.1 train no.112440  loss = 3.95462 avg_loss = 3.79132\n",
      "epoch no.1 train no.112450  loss = 2.09521 avg_loss = 3.76192\n",
      "epoch no.1 train no.112460  loss = 3.54034 avg_loss = 3.77109\n",
      "epoch no.1 train no.112470  loss = 3.25520 avg_loss = 3.79887\n",
      "epoch no.1 train no.112480  loss = 4.44728 avg_loss = 3.78628\n",
      "epoch no.1 train no.112490  loss = 4.16666 avg_loss = 3.75533\n",
      "epoch no.1 train no.112500  loss = 6.61931 avg_loss = 3.79098\n",
      "epoch no.1 train no.112510  loss = 2.85175 avg_loss = 3.79139\n",
      "epoch no.1 train no.112520  loss = 3.32176 avg_loss = 3.79418\n",
      "epoch no.1 train no.112530  loss = 3.06725 avg_loss = 3.76544\n",
      "epoch no.1 train no.112540  loss = 3.41201 avg_loss = 3.79733\n",
      "epoch no.1 train no.112550  loss = 5.50190 avg_loss = 3.77161\n",
      "epoch no.1 train no.112560  loss = 2.90826 avg_loss = 3.70490\n",
      "epoch no.1 train no.112570  loss = 2.40962 avg_loss = 3.67469\n",
      "epoch no.1 train no.112580  loss = 3.36332 avg_loss = 3.68098\n",
      "epoch no.1 train no.112590  loss = 7.10632 avg_loss = 3.73942\n",
      "epoch no.1 train no.112600  loss = 5.10014 avg_loss = 3.72684\n",
      "epoch no.1 train no.112610  loss = 2.30815 avg_loss = 3.73382\n",
      "epoch no.1 train no.112620  loss = 4.67565 avg_loss = 3.72087\n",
      "epoch no.1 train no.112630  loss = 5.18929 avg_loss = 3.73566\n",
      "epoch no.1 train no.112640  loss = 2.57199 avg_loss = 3.67884\n",
      "epoch no.1 train no.112650  loss = 4.18300 avg_loss = 3.68814\n",
      "epoch no.1 train no.112660  loss = 3.99757 avg_loss = 3.73780\n",
      "epoch no.1 train no.112670  loss = 5.57610 avg_loss = 3.71490\n",
      "epoch no.1 train no.112680  loss = 3.93040 avg_loss = 3.70310\n",
      "epoch no.1 train no.112690  loss = 2.86449 avg_loss = 3.67480\n",
      "epoch no.1 train no.112700  loss = 2.24951 avg_loss = 3.68630\n",
      "epoch no.1 train no.112710  loss = 4.18649 avg_loss = 3.69461\n",
      "epoch no.1 train no.112720  loss = 2.43067 avg_loss = 3.69624\n",
      "epoch no.1 train no.112730  loss = 6.27826 avg_loss = 3.75628\n",
      "epoch no.1 train no.112740  loss = 4.40894 avg_loss = 3.78916\n",
      "epoch no.1 train no.112750  loss = 3.69148 avg_loss = 3.81377\n",
      "epoch no.1 train no.112760  loss = 4.93161 avg_loss = 3.82213\n",
      "epoch no.1 train no.112770  loss = 2.48534 avg_loss = 3.82283\n",
      "epoch no.1 train no.112780  loss = 3.89452 avg_loss = 3.79374\n",
      "epoch no.1 train no.112790  loss = 3.02586 avg_loss = 3.77618\n",
      "epoch no.1 train no.112800  loss = 3.79577 avg_loss = 3.77346\n",
      "epoch no.1 train no.112810  loss = 3.59375 avg_loss = 3.73432\n",
      "epoch no.1 train no.112820  loss = 4.74836 avg_loss = 3.75090\n",
      "epoch no.1 train no.112830  loss = 6.15888 avg_loss = 3.75220\n",
      "epoch no.1 train no.112840  loss = 3.71306 avg_loss = 3.77207\n",
      "epoch no.1 train no.112850  loss = 4.01855 avg_loss = 3.72461\n",
      "epoch no.1 train no.112860  loss = 2.71402 avg_loss = 3.66915\n",
      "epoch no.1 train no.112870  loss = 2.99703 avg_loss = 3.69079\n",
      "epoch no.1 train no.112880  loss = 3.83541 avg_loss = 3.68419\n",
      "epoch no.1 train no.112890  loss = 2.71348 avg_loss = 3.70093\n",
      "epoch no.1 train no.112900  loss = 3.07107 avg_loss = 3.67906\n",
      "epoch no.1 train no.112910  loss = 5.14613 avg_loss = 3.73255\n",
      "epoch no.1 train no.112920  loss = 5.03238 avg_loss = 3.76852\n",
      "epoch no.1 train no.112930  loss = 2.04209 avg_loss = 3.72221\n",
      "epoch no.1 train no.112940  loss = 2.86886 avg_loss = 3.72174\n",
      "epoch no.1 train no.112950  loss = 2.87272 avg_loss = 3.71429\n",
      "epoch no.1 train no.112960  loss = 4.50940 avg_loss = 3.70694\n",
      "epoch no.1 train no.112970  loss = 6.85982 avg_loss = 3.69347\n",
      "epoch no.1 train no.112980  loss = 6.45985 avg_loss = 3.76444\n",
      "epoch no.1 train no.112990  loss = 7.31881 avg_loss = 3.80134\n",
      "epoch no.1 train no.113000  loss = 1.69530 avg_loss = 3.76049\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '하기', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.1 train no.113010  loss = 2.95333 avg_loss = 3.75753\n",
      "epoch no.1 train no.113020  loss = 3.73609 avg_loss = 3.77472\n",
      "epoch no.1 train no.113030  loss = 3.27192 avg_loss = 3.71815\n",
      "epoch no.1 train no.113040  loss = 3.43791 avg_loss = 3.70761\n",
      "epoch no.1 train no.113050  loss = 3.82123 avg_loss = 3.69215\n",
      "epoch no.1 train no.113060  loss = 4.99087 avg_loss = 3.72291\n",
      "epoch no.1 train no.113070  loss = 3.94267 avg_loss = 3.78923\n",
      "epoch no.1 train no.113080  loss = 4.54731 avg_loss = 3.80293\n",
      "epoch no.1 train no.113090  loss = 4.43962 avg_loss = 3.80921\n",
      "epoch no.1 train no.113100  loss = 3.70156 avg_loss = 3.81204\n",
      "epoch no.1 train no.113110  loss = 4.47488 avg_loss = 3.81406\n",
      "epoch no.1 train no.113120  loss = 2.64293 avg_loss = 3.79116\n",
      "epoch no.1 train no.113130  loss = 5.89819 avg_loss = 3.82768\n",
      "epoch no.1 train no.113140  loss = 3.30325 avg_loss = 3.83487\n",
      "epoch no.1 train no.113150  loss = 1.77275 avg_loss = 3.79275\n",
      "epoch no.1 train no.113160  loss = 2.85661 avg_loss = 3.80347\n",
      "epoch no.1 train no.113170  loss = 2.40533 avg_loss = 3.81920\n",
      "epoch no.1 train no.113180  loss = 3.42543 avg_loss = 3.80093\n",
      "epoch no.1 train no.113190  loss = 3.18395 avg_loss = 3.82463\n",
      "epoch no.1 train no.113200  loss = 3.83805 avg_loss = 3.83128\n",
      "epoch no.1 train no.113210  loss = 2.89877 avg_loss = 3.81274\n",
      "epoch no.1 train no.113220  loss = 3.68482 avg_loss = 3.77593\n",
      "epoch no.1 train no.113230  loss = 3.31097 avg_loss = 3.75318\n",
      "epoch no.1 train no.113240  loss = 6.84148 avg_loss = 3.76284\n",
      "epoch no.1 train no.113250  loss = 2.00912 avg_loss = 3.77270\n",
      "epoch no.1 train no.113260  loss = 3.87815 avg_loss = 3.82738\n",
      "epoch no.1 train no.113270  loss = 5.83311 avg_loss = 3.80045\n",
      "epoch no.1 train no.113280  loss = 4.61052 avg_loss = 3.83919\n",
      "epoch no.1 train no.113290  loss = 3.37537 avg_loss = 3.78258\n",
      "epoch no.1 train no.113300  loss = 2.72450 avg_loss = 3.75490\n",
      "epoch no.1 train no.113310  loss = 1.84050 avg_loss = 3.72690\n",
      "epoch no.1 train no.113320  loss = 4.85156 avg_loss = 3.78279\n",
      "epoch no.1 train no.113330  loss = 2.92386 avg_loss = 3.77890\n",
      "epoch no.1 train no.113340  loss = 4.62626 avg_loss = 3.73063\n",
      "epoch no.1 train no.113350  loss = 3.60659 avg_loss = 3.70349\n",
      "epoch no.1 train no.113360  loss = 2.42413 avg_loss = 3.68515\n",
      "epoch no.1 train no.113370  loss = 4.35723 avg_loss = 3.72126\n",
      "epoch no.1 train no.113380  loss = 3.83055 avg_loss = 3.72518\n",
      "epoch no.1 train no.113390  loss = 2.90477 avg_loss = 3.70430\n",
      "epoch no.1 train no.113400  loss = 2.67468 avg_loss = 3.73232\n",
      "epoch no.1 train no.113410  loss = 3.00928 avg_loss = 3.68830\n",
      "epoch no.1 train no.113420  loss = 2.56177 avg_loss = 3.71608\n",
      "epoch no.1 train no.113430  loss = 4.16571 avg_loss = 3.73053\n",
      "epoch no.1 train no.113440  loss = 4.42399 avg_loss = 3.77948\n",
      "epoch no.1 train no.113450  loss = 6.14276 avg_loss = 3.79896\n",
      "epoch no.1 train no.113460  loss = 5.78476 avg_loss = 3.79612\n",
      "epoch no.1 train no.113470  loss = 2.89806 avg_loss = 3.75336\n",
      "epoch no.1 train no.113480  loss = 3.28548 avg_loss = 3.77931\n",
      "epoch no.1 train no.113490  loss = 2.94351 avg_loss = 3.73013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.113500  loss = 2.84697 avg_loss = 3.73256\n",
      "epoch no.1 train no.113510  loss = 2.53114 avg_loss = 3.75370\n",
      "epoch no.1 train no.113520  loss = 3.02483 avg_loss = 3.79915\n",
      "epoch no.1 train no.113530  loss = 2.55924 avg_loss = 3.80992\n",
      "epoch no.1 train no.113540  loss = 3.31271 avg_loss = 3.81978\n",
      "epoch no.1 train no.113550  loss = 2.41157 avg_loss = 3.74390\n",
      "epoch no.1 train no.113560  loss = 6.64846 avg_loss = 3.82525\n",
      "epoch no.1 train no.113570  loss = 3.43173 avg_loss = 3.80373\n",
      "epoch no.1 train no.113580  loss = 4.33991 avg_loss = 3.81990\n",
      "epoch no.1 train no.113590  loss = 3.70465 avg_loss = 3.78634\n",
      "epoch no.1 train no.113600  loss = 3.04103 avg_loss = 3.78518\n",
      "epoch no.1 train no.113610  loss = 1.79017 avg_loss = 3.80505\n",
      "epoch no.1 train no.113620  loss = 3.97931 avg_loss = 3.82818\n",
      "epoch no.1 train no.113630  loss = 3.59408 avg_loss = 3.84004\n",
      "epoch no.1 train no.113640  loss = 3.22335 avg_loss = 3.83057\n",
      "epoch no.1 train no.113650  loss = 3.73455 avg_loss = 3.84943\n",
      "epoch no.1 train no.113660  loss = 3.20653 avg_loss = 3.83609\n",
      "epoch no.1 train no.113670  loss = 5.20666 avg_loss = 3.80020\n",
      "epoch no.1 train no.113680  loss = 4.02983 avg_loss = 3.82523\n",
      "epoch no.1 train no.113690  loss = 6.02099 avg_loss = 3.83892\n",
      "epoch no.1 train no.113700  loss = 3.63156 avg_loss = 3.86053\n",
      "epoch no.1 train no.113710  loss = 2.86232 avg_loss = 3.88972\n",
      "epoch no.1 train no.113720  loss = 3.68581 avg_loss = 3.84097\n",
      "epoch no.1 train no.113730  loss = 4.02226 avg_loss = 3.83792\n",
      "epoch no.1 train no.113740  loss = 3.06425 avg_loss = 3.77233\n",
      "epoch no.1 train no.113750  loss = 3.70961 avg_loss = 3.72591\n",
      "epoch no.1 train no.113760  loss = 5.18885 avg_loss = 3.77196\n",
      "epoch no.1 train no.113770  loss = 3.42491 avg_loss = 3.82962\n",
      "epoch no.1 train no.113780  loss = 5.73410 avg_loss = 3.82635\n",
      "epoch no.1 train no.113790  loss = 4.09540 avg_loss = 3.83079\n",
      "epoch no.1 train no.113800  loss = 4.11185 avg_loss = 3.79149\n",
      "epoch no.1 train no.113810  loss = 4.54824 avg_loss = 3.79308\n",
      "epoch no.1 train no.113820  loss = 6.03982 avg_loss = 3.82149\n",
      "epoch no.1 train no.113830  loss = 3.60948 avg_loss = 3.79102\n",
      "epoch no.1 train no.113840  loss = 3.44183 avg_loss = 3.79806\n",
      "epoch no.1 train no.113850  loss = 2.54454 avg_loss = 3.74562\n",
      "epoch no.1 train no.113860  loss = 2.78277 avg_loss = 3.72881\n",
      "epoch no.1 train no.113870  loss = 2.26751 avg_loss = 3.70709\n",
      "epoch no.1 train no.113880  loss = 3.05057 avg_loss = 3.71896\n",
      "epoch no.1 train no.113890  loss = 4.14472 avg_loss = 3.71623\n",
      "epoch no.1 train no.113900  loss = 2.49302 avg_loss = 3.70532\n",
      "epoch no.1 train no.113910  loss = 3.46069 avg_loss = 3.72599\n",
      "epoch no.1 train no.113920  loss = 4.30231 avg_loss = 3.69259\n",
      "epoch no.1 train no.113930  loss = 5.44705 avg_loss = 3.70806\n",
      "epoch no.1 train no.113940  loss = 6.94718 avg_loss = 3.71546\n",
      "epoch no.1 train no.113950  loss = 4.73529 avg_loss = 3.69843\n",
      "epoch no.1 train no.113960  loss = 3.24392 avg_loss = 3.75622\n",
      "epoch no.1 train no.113970  loss = 4.76811 avg_loss = 3.76777\n",
      "epoch no.1 train no.113980  loss = 3.48227 avg_loss = 3.78087\n",
      "epoch no.1 train no.113990  loss = 3.81159 avg_loss = 3.75251\n",
      "epoch no.1 train no.114000  loss = 4.27176 avg_loss = 3.73865\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '하기', '▁좋은', '▁노래', '송', '</s>']\n",
      "기분전환하기 좋은 팝송</s>\n",
      "epoch no.1 train no.114010  loss = 2.71194 avg_loss = 3.74724\n",
      "epoch no.1 train no.114020  loss = 4.80464 avg_loss = 3.82114\n",
      "epoch no.1 train no.114030  loss = 4.73844 avg_loss = 3.90719\n",
      "epoch no.1 train no.114040  loss = 3.30469 avg_loss = 3.83982\n",
      "epoch no.1 train no.114050  loss = 3.63127 avg_loss = 3.83123\n",
      "epoch no.1 train no.114060  loss = 4.51364 avg_loss = 3.78090\n",
      "epoch no.1 train no.114070  loss = 3.47923 avg_loss = 3.72382\n",
      "epoch no.1 train no.114080  loss = 5.04633 avg_loss = 3.72994\n",
      "epoch no.1 train no.114090  loss = 4.55304 avg_loss = 3.72064\n",
      "epoch no.1 train no.114100  loss = 4.89953 avg_loss = 3.76447\n",
      "epoch no.1 train no.114110  loss = 2.74993 avg_loss = 3.72295\n",
      "epoch no.1 train no.114120  loss = 4.24162 avg_loss = 3.74714\n",
      "epoch no.1 train no.114130  loss = 2.25302 avg_loss = 3.77221\n",
      "epoch no.1 train no.114140  loss = 4.62745 avg_loss = 3.75934\n",
      "epoch no.1 train no.114150  loss = 4.84589 avg_loss = 3.76408\n",
      "epoch no.1 train no.114160  loss = 4.58922 avg_loss = 3.78471\n",
      "epoch no.1 train no.114170  loss = 2.72773 avg_loss = 3.84359\n",
      "epoch no.1 train no.114180  loss = 4.79260 avg_loss = 3.77474\n",
      "epoch no.1 train no.114190  loss = 2.17941 avg_loss = 3.74893\n",
      "epoch no.1 train no.114200  loss = 4.11612 avg_loss = 3.73896\n",
      "epoch no.1 train no.114210  loss = 2.43090 avg_loss = 3.68546\n",
      "epoch no.1 train no.114220  loss = 3.54468 avg_loss = 3.69775\n",
      "epoch no.1 train no.114230  loss = 3.97881 avg_loss = 3.73903\n",
      "epoch no.1 train no.114240  loss = 4.57906 avg_loss = 3.72167\n",
      "epoch no.1 train no.114250  loss = 2.51250 avg_loss = 3.70022\n",
      "epoch no.1 train no.114260  loss = 2.46070 avg_loss = 3.68227\n",
      "epoch no.1 train no.114270  loss = 5.62677 avg_loss = 3.68726\n",
      "epoch no.1 train no.114280  loss = 3.46554 avg_loss = 3.70093\n",
      "epoch no.1 train no.114290  loss = 4.77794 avg_loss = 3.67768\n",
      "epoch no.1 train no.114300  loss = 5.77958 avg_loss = 3.76167\n",
      "epoch no.1 train no.114310  loss = 4.78299 avg_loss = 3.75177\n",
      "epoch no.1 train no.114320  loss = 7.00938 avg_loss = 3.82184\n",
      "epoch no.1 train no.114330  loss = 4.03190 avg_loss = 3.80228\n",
      "epoch no.1 train no.114340  loss = 3.40456 avg_loss = 3.79458\n",
      "epoch no.1 train no.114350  loss = 3.10739 avg_loss = 3.75874\n",
      "epoch no.1 train no.114360  loss = 3.44473 avg_loss = 3.74364\n",
      "epoch no.1 train no.114370  loss = 2.83663 avg_loss = 3.73881\n",
      "epoch no.1 train no.114380  loss = 3.45500 avg_loss = 3.68547\n",
      "epoch no.1 train no.114390  loss = 4.21377 avg_loss = 3.70376\n",
      "epoch no.1 train no.114400  loss = 3.01929 avg_loss = 3.72267\n",
      "epoch no.1 train no.114410  loss = 2.06918 avg_loss = 3.68183\n",
      "epoch no.1 train no.114420  loss = 4.62549 avg_loss = 3.69955\n",
      "epoch no.1 train no.114430  loss = 3.53086 avg_loss = 3.77092\n",
      "epoch no.1 train no.114440  loss = 5.02989 avg_loss = 3.76703\n",
      "epoch no.1 train no.114450  loss = 3.58167 avg_loss = 3.76595\n",
      "epoch no.1 train no.114460  loss = 3.04722 avg_loss = 3.80018\n",
      "epoch no.1 train no.114470  loss = 3.99806 avg_loss = 3.79625\n",
      "epoch no.1 train no.114480  loss = 1.33060 avg_loss = 3.77172\n",
      "epoch no.1 train no.114490  loss = 3.16989 avg_loss = 3.80266\n",
      "epoch no.1 train no.114500  loss = 4.17795 avg_loss = 3.76858\n",
      "epoch no.1 train no.114510  loss = 3.32128 avg_loss = 3.77964\n",
      "epoch no.1 train no.114520  loss = 4.04364 avg_loss = 3.76614\n",
      "epoch no.1 train no.114530  loss = 4.00527 avg_loss = 3.80472\n",
      "epoch no.1 train no.114540  loss = 3.54068 avg_loss = 3.77012\n",
      "epoch no.1 train no.114550  loss = 3.08285 avg_loss = 3.79179\n",
      "epoch no.1 train no.114560  loss = 4.76944 avg_loss = 3.80048\n",
      "epoch no.1 train no.114570  loss = 4.39404 avg_loss = 3.79227\n",
      "epoch no.1 train no.114580  loss = 3.12489 avg_loss = 3.82347\n",
      "epoch no.1 train no.114590  loss = 2.36918 avg_loss = 3.76885\n",
      "epoch no.1 train no.114600  loss = 4.93830 avg_loss = 3.77307\n",
      "epoch no.1 train no.114610  loss = 4.55586 avg_loss = 3.83825\n",
      "epoch no.1 train no.114620  loss = 3.77645 avg_loss = 3.91320\n",
      "epoch no.1 train no.114630  loss = 4.89752 avg_loss = 3.92034\n",
      "epoch no.1 train no.114640  loss = 4.37302 avg_loss = 3.89348\n",
      "epoch no.1 train no.114650  loss = 2.97922 avg_loss = 3.90396\n",
      "epoch no.1 train no.114660  loss = 2.97605 avg_loss = 3.89120\n",
      "epoch no.1 train no.114670  loss = 3.35848 avg_loss = 3.89146\n",
      "epoch no.1 train no.114680  loss = 2.78509 avg_loss = 3.86220\n",
      "epoch no.1 train no.114690  loss = 4.10052 avg_loss = 3.81376\n",
      "epoch no.1 train no.114700  loss = 5.34423 avg_loss = 3.85177\n",
      "epoch no.1 train no.114710  loss = 8.09732 avg_loss = 3.85500\n",
      "epoch no.1 train no.114720  loss = 6.20850 avg_loss = 3.85972\n",
      "epoch no.1 train no.114730  loss = 3.11257 avg_loss = 3.82499\n",
      "epoch no.1 train no.114740  loss = 2.73309 avg_loss = 3.74731\n",
      "epoch no.1 train no.114750  loss = 6.16412 avg_loss = 3.83726\n",
      "epoch no.1 train no.114760  loss = 4.28725 avg_loss = 3.79540\n",
      "epoch no.1 train no.114770  loss = 3.30556 avg_loss = 3.78854\n",
      "epoch no.1 train no.114780  loss = 4.35342 avg_loss = 3.75378\n",
      "epoch no.1 train no.114790  loss = 4.53850 avg_loss = 3.77371\n",
      "epoch no.1 train no.114800  loss = 5.84004 avg_loss = 3.79874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.114810  loss = 6.02806 avg_loss = 3.74551\n",
      "epoch no.1 train no.114820  loss = 3.32699 avg_loss = 3.70285\n",
      "epoch no.1 train no.114830  loss = 3.83703 avg_loss = 3.65516\n",
      "epoch no.1 train no.114840  loss = 1.76459 avg_loss = 3.64433\n",
      "epoch no.1 train no.114850  loss = 4.23128 avg_loss = 3.69346\n",
      "epoch no.1 train no.114860  loss = 3.57489 avg_loss = 3.69012\n",
      "epoch no.1 train no.114870  loss = 3.99810 avg_loss = 3.71550\n",
      "epoch no.1 train no.114880  loss = 1.87399 avg_loss = 3.68057\n",
      "epoch no.1 train no.114890  loss = 3.55670 avg_loss = 3.64358\n",
      "epoch no.1 train no.114900  loss = 3.36449 avg_loss = 3.64106\n",
      "epoch no.1 train no.114910  loss = 4.10815 avg_loss = 3.64335\n",
      "epoch no.1 train no.114920  loss = 2.13422 avg_loss = 3.68637\n",
      "epoch no.1 train no.114930  loss = 2.26267 avg_loss = 3.60311\n",
      "epoch no.1 train no.114940  loss = 3.80882 avg_loss = 3.64735\n",
      "epoch no.1 train no.114950  loss = 2.89379 avg_loss = 3.63679\n",
      "epoch no.1 train no.114960  loss = 3.40409 avg_loss = 3.65877\n",
      "epoch no.1 train no.114970  loss = 3.39341 avg_loss = 3.67238\n",
      "epoch no.1 train no.114980  loss = 4.63600 avg_loss = 3.67067\n",
      "epoch no.1 train no.114990  loss = 1.66081 avg_loss = 3.70872\n",
      "epoch no.1 train no.115000  loss = 2.63216 avg_loss = 3.68499\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁신나는', '</s>']\n",
      "기분전환용 팝</s>\n",
      "epoch no.1 train no.115010  loss = 2.44319 avg_loss = 3.63287\n",
      "epoch no.1 train no.115020  loss = 3.19554 avg_loss = 3.66600\n",
      "epoch no.1 train no.115030  loss = 4.19207 avg_loss = 3.68272\n",
      "epoch no.1 train no.115040  loss = 4.18116 avg_loss = 3.70924\n",
      "epoch no.1 train no.115050  loss = 2.04173 avg_loss = 3.69735\n",
      "epoch no.1 train no.115060  loss = 4.15430 avg_loss = 3.79152\n",
      "epoch no.1 train no.115070  loss = 3.60399 avg_loss = 3.75522\n",
      "epoch no.1 train no.115080  loss = 4.00972 avg_loss = 3.74968\n",
      "epoch no.1 train no.115090  loss = 3.25004 avg_loss = 3.73796\n",
      "epoch no.1 train no.115100  loss = 3.71817 avg_loss = 3.77160\n",
      "epoch no.1 train no.115110  loss = 2.65938 avg_loss = 3.76226\n",
      "epoch no.1 train no.115120  loss = 4.62503 avg_loss = 3.78663\n",
      "epoch no.1 train no.115130  loss = 3.20546 avg_loss = 3.80778\n",
      "epoch no.1 train no.115140  loss = 3.55667 avg_loss = 3.85375\n",
      "epoch no.1 train no.115150  loss = 4.79756 avg_loss = 3.80610\n",
      "epoch no.1 train no.115160  loss = 3.75930 avg_loss = 3.78362\n",
      "epoch no.1 train no.115170  loss = 2.45873 avg_loss = 3.75493\n",
      "epoch no.1 train no.115180  loss = 2.66792 avg_loss = 3.73765\n",
      "epoch no.1 train no.115190  loss = 3.52892 avg_loss = 3.77687\n",
      "epoch no.1 train no.115200  loss = 3.46435 avg_loss = 3.79327\n",
      "epoch no.1 train no.115210  loss = 3.88030 avg_loss = 3.82734\n",
      "epoch no.1 train no.115220  loss = 3.35602 avg_loss = 3.76396\n",
      "epoch no.1 train no.115230  loss = 2.84381 avg_loss = 3.74291\n",
      "epoch no.1 train no.115240  loss = 3.01301 avg_loss = 3.77124\n",
      "epoch no.1 train no.115250  loss = 4.76823 avg_loss = 3.78851\n",
      "epoch no.1 train no.115260  loss = 5.35726 avg_loss = 3.81944\n",
      "epoch no.1 train no.115270  loss = 2.85806 avg_loss = 3.79575\n",
      "epoch no.1 train no.115280  loss = 2.82014 avg_loss = 3.81290\n",
      "epoch no.1 train no.115290  loss = 1.81177 avg_loss = 3.81035\n",
      "epoch no.1 train no.115300  loss = 2.31294 avg_loss = 3.77382\n",
      "epoch no.1 train no.115310  loss = 3.61741 avg_loss = 3.78018\n",
      "epoch no.1 train no.115320  loss = 4.94225 avg_loss = 3.84873\n",
      "epoch no.1 train no.115330  loss = 3.73804 avg_loss = 3.84938\n",
      "epoch no.1 train no.115340  loss = 2.43154 avg_loss = 3.85735\n",
      "epoch no.1 train no.115350  loss = 6.26737 avg_loss = 3.83856\n",
      "epoch no.1 train no.115360  loss = 2.71155 avg_loss = 3.86519\n",
      "epoch no.1 train no.115370  loss = 6.37190 avg_loss = 3.88483\n",
      "epoch no.1 train no.115380  loss = 5.95630 avg_loss = 3.86959\n",
      "epoch no.1 train no.115390  loss = 3.06169 avg_loss = 3.85465\n",
      "epoch no.1 train no.115400  loss = 5.15332 avg_loss = 3.83961\n",
      "epoch no.1 train no.115410  loss = 4.46255 avg_loss = 3.82954\n",
      "epoch no.1 train no.115420  loss = 3.36435 avg_loss = 3.81200\n",
      "epoch no.1 train no.115430  loss = 2.93097 avg_loss = 3.80914\n",
      "epoch no.1 train no.115440  loss = 4.24672 avg_loss = 3.83159\n",
      "epoch no.1 train no.115450  loss = 3.69024 avg_loss = 3.83087\n",
      "epoch no.1 train no.115460  loss = 3.51232 avg_loss = 3.85414\n",
      "epoch no.1 train no.115470  loss = 2.94194 avg_loss = 3.86724\n",
      "epoch no.1 train no.115480  loss = 4.40175 avg_loss = 3.86193\n",
      "epoch no.1 train no.115490  loss = 2.96152 avg_loss = 3.87156\n",
      "epoch no.1 train no.115500  loss = 4.00549 avg_loss = 3.85128\n",
      "epoch no.1 train no.115510  loss = 3.82034 avg_loss = 3.85497\n",
      "epoch no.1 train no.115520  loss = 5.03759 avg_loss = 3.89538\n",
      "epoch no.1 train no.115530  loss = 4.92884 avg_loss = 3.86688\n",
      "epoch no.1 train no.115540  loss = 2.42711 avg_loss = 3.83787\n",
      "epoch no.1 train no.115550  loss = 2.97459 avg_loss = 3.86977\n",
      "epoch no.1 train no.115560  loss = 2.72304 avg_loss = 3.87492\n",
      "epoch no.1 train no.115570  loss = 5.73378 avg_loss = 3.90522\n",
      "epoch no.1 train no.115580  loss = 2.78117 avg_loss = 3.85364\n",
      "epoch no.1 train no.115590  loss = 2.84236 avg_loss = 3.84271\n",
      "epoch no.1 train no.115600  loss = 2.94736 avg_loss = 3.84111\n",
      "epoch no.1 train no.115610  loss = 3.98230 avg_loss = 3.78208\n",
      "epoch no.1 train no.115620  loss = 3.34390 avg_loss = 3.76993\n",
      "epoch no.1 train no.115630  loss = 4.61956 avg_loss = 3.76550\n",
      "epoch no.1 train no.115640  loss = 2.34158 avg_loss = 3.73276\n",
      "epoch no.1 train no.115650  loss = 2.45076 avg_loss = 3.73467\n",
      "epoch no.1 train no.115660  loss = 3.94215 avg_loss = 3.72727\n",
      "epoch no.1 train no.115670  loss = 2.27091 avg_loss = 3.67502\n",
      "epoch no.1 train no.115680  loss = 6.57140 avg_loss = 3.70562\n",
      "epoch no.1 train no.115690  loss = 4.18087 avg_loss = 3.73873\n",
      "epoch no.1 train no.115700  loss = 3.64051 avg_loss = 3.71577\n",
      "epoch no.1 train no.115710  loss = 4.37679 avg_loss = 3.75895\n",
      "epoch no.1 train no.115720  loss = 4.82205 avg_loss = 3.76611\n",
      "epoch no.1 train no.115730  loss = 3.39825 avg_loss = 3.79455\n",
      "epoch no.1 train no.115740  loss = 3.51129 avg_loss = 3.75436\n",
      "epoch no.1 train no.115750  loss = 2.35558 avg_loss = 3.79346\n",
      "epoch no.1 train no.115760  loss = 2.73913 avg_loss = 3.78237\n",
      "epoch no.1 train no.115770  loss = 2.36618 avg_loss = 3.75270\n",
      "epoch no.1 train no.115780  loss = 2.27667 avg_loss = 3.69673\n",
      "epoch no.1 train no.115790  loss = 4.75540 avg_loss = 3.72831\n",
      "epoch no.1 train no.115800  loss = 4.88381 avg_loss = 3.79320\n",
      "epoch no.1 train no.115810  loss = 1.77921 avg_loss = 3.71419\n",
      "epoch no.1 train no.115820  loss = 3.79168 avg_loss = 3.74233\n",
      "epoch no.1 train no.115830  loss = 3.44385 avg_loss = 3.77389\n",
      "epoch no.1 train no.115840  loss = 3.66608 avg_loss = 3.81924\n",
      "epoch no.1 train no.115850  loss = 3.15337 avg_loss = 3.77769\n",
      "epoch no.1 train no.115860  loss = 4.40414 avg_loss = 3.80469\n",
      "epoch no.1 train no.115870  loss = 3.72653 avg_loss = 3.79290\n",
      "epoch no.1 train no.115880  loss = 4.63693 avg_loss = 3.77106\n",
      "epoch no.1 train no.115890  loss = 3.92525 avg_loss = 3.81465\n",
      "epoch no.1 train no.115900  loss = 5.25193 avg_loss = 3.79276\n",
      "epoch no.1 train no.115910  loss = 5.64597 avg_loss = 3.79199\n",
      "epoch no.1 train no.115920  loss = 4.09614 avg_loss = 3.84537\n",
      "epoch no.1 train no.115930  loss = 3.47632 avg_loss = 3.86753\n",
      "epoch no.1 train no.115940  loss = 4.68980 avg_loss = 3.90177\n",
      "epoch no.1 train no.115950  loss = 5.44243 avg_loss = 3.87250\n",
      "epoch no.1 train no.115960  loss = 5.38161 avg_loss = 3.89767\n",
      "epoch no.1 train no.115970  loss = 3.33959 avg_loss = 3.89568\n",
      "epoch no.1 train no.115980  loss = 3.32530 avg_loss = 3.90757\n",
      "epoch no.1 train no.115990  loss = 3.88463 avg_loss = 3.93866\n",
      "epoch no.1 train no.116000  loss = 3.53227 avg_loss = 3.94250\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.116010  loss = 4.12794 avg_loss = 3.95921\n",
      "epoch no.1 train no.116020  loss = 5.93305 avg_loss = 3.98235\n",
      "epoch no.1 train no.116030  loss = 2.97337 avg_loss = 3.94414\n",
      "epoch no.1 train no.116040  loss = 2.35654 avg_loss = 3.94553\n",
      "epoch no.1 train no.116050  loss = 3.39942 avg_loss = 3.93444\n",
      "epoch no.1 train no.116060  loss = 4.13346 avg_loss = 3.89381\n",
      "epoch no.1 train no.116070  loss = 2.84111 avg_loss = 3.81988\n",
      "epoch no.1 train no.116080  loss = 4.18453 avg_loss = 3.77888\n",
      "epoch no.1 train no.116090  loss = 2.92929 avg_loss = 3.72784\n",
      "epoch no.1 train no.116100  loss = 2.40030 avg_loss = 3.70256\n",
      "epoch no.1 train no.116110  loss = 2.93153 avg_loss = 3.66540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.116120  loss = 3.01671 avg_loss = 3.64568\n",
      "epoch no.1 train no.116130  loss = 2.85755 avg_loss = 3.62386\n",
      "epoch no.1 train no.116140  loss = 2.34928 avg_loss = 3.61306\n",
      "epoch no.1 train no.116150  loss = 4.39495 avg_loss = 3.60080\n",
      "epoch no.1 train no.116160  loss = 4.39967 avg_loss = 3.61989\n",
      "epoch no.1 train no.116170  loss = 4.04715 avg_loss = 3.66853\n",
      "epoch no.1 train no.116180  loss = 3.35691 avg_loss = 3.65844\n",
      "epoch no.1 train no.116190  loss = 4.29393 avg_loss = 3.66678\n",
      "epoch no.1 train no.116200  loss = 4.90156 avg_loss = 3.66478\n",
      "epoch no.1 train no.116210  loss = 3.21652 avg_loss = 3.68430\n",
      "epoch no.1 train no.116220  loss = 5.17600 avg_loss = 3.70125\n",
      "epoch no.1 train no.116230  loss = 2.99280 avg_loss = 3.65295\n",
      "epoch no.1 train no.116240  loss = 2.79778 avg_loss = 3.68001\n",
      "epoch no.1 train no.116250  loss = 2.68621 avg_loss = 3.67936\n",
      "epoch no.1 train no.116260  loss = 2.64788 avg_loss = 3.69743\n",
      "epoch no.1 train no.116270  loss = 3.85067 avg_loss = 3.67033\n",
      "epoch no.1 train no.116280  loss = 2.92058 avg_loss = 3.61821\n",
      "epoch no.1 train no.116290  loss = 2.58516 avg_loss = 3.62581\n",
      "epoch no.1 train no.116300  loss = 3.35402 avg_loss = 3.60364\n",
      "epoch no.1 train no.116310  loss = 3.40552 avg_loss = 3.64922\n",
      "epoch no.1 train no.116320  loss = 6.60436 avg_loss = 3.70777\n",
      "epoch no.1 train no.116330  loss = 2.22005 avg_loss = 3.67542\n",
      "epoch no.1 train no.116340  loss = 1.77938 avg_loss = 3.71750\n",
      "epoch no.1 train no.116350  loss = 3.64273 avg_loss = 3.74449\n",
      "epoch no.1 train no.116360  loss = 4.15281 avg_loss = 3.72937\n",
      "epoch no.1 train no.116370  loss = 3.95879 avg_loss = 3.73508\n",
      "epoch no.1 train no.116380  loss = 2.25085 avg_loss = 3.71523\n",
      "epoch no.1 train no.116390  loss = 5.65068 avg_loss = 3.74692\n",
      "epoch no.1 train no.116400  loss = 2.82909 avg_loss = 3.74287\n",
      "epoch no.1 train no.116410  loss = 4.65303 avg_loss = 3.73476\n",
      "epoch no.1 train no.116420  loss = 5.91411 avg_loss = 3.71383\n",
      "epoch no.1 train no.116430  loss = 2.94167 avg_loss = 3.77664\n",
      "epoch no.1 train no.116440  loss = 2.78786 avg_loss = 3.76948\n",
      "epoch no.1 train no.116450  loss = 2.99847 avg_loss = 3.74496\n",
      "epoch no.1 train no.116460  loss = 2.97870 avg_loss = 3.74857\n",
      "epoch no.1 train no.116470  loss = 6.05107 avg_loss = 3.76073\n",
      "epoch no.1 train no.116480  loss = 9.18096 avg_loss = 3.75354\n",
      "epoch no.1 train no.116490  loss = 3.91997 avg_loss = 3.75667\n",
      "epoch no.1 train no.116500  loss = 4.57530 avg_loss = 3.75369\n",
      "epoch no.1 train no.116510  loss = 3.26710 avg_loss = 3.73509\n",
      "epoch no.1 train no.116520  loss = 2.47035 avg_loss = 3.71136\n",
      "epoch no.1 train no.116530  loss = 3.20310 avg_loss = 3.71746\n",
      "epoch no.1 train no.116540  loss = 3.47908 avg_loss = 3.71175\n",
      "epoch no.1 train no.116550  loss = 3.52457 avg_loss = 3.72220\n",
      "epoch no.1 train no.116560  loss = 3.54613 avg_loss = 3.78166\n",
      "epoch no.1 train no.116570  loss = 3.31870 avg_loss = 3.81097\n",
      "epoch no.1 train no.116580  loss = 2.20464 avg_loss = 3.80784\n",
      "epoch no.1 train no.116590  loss = 3.35472 avg_loss = 3.84314\n",
      "epoch no.1 train no.116600  loss = 2.72097 avg_loss = 3.83607\n",
      "epoch no.1 train no.116610  loss = 3.50146 avg_loss = 3.85060\n",
      "epoch no.1 train no.116620  loss = 3.44364 avg_loss = 3.88595\n",
      "epoch no.1 train no.116630  loss = 3.88801 avg_loss = 3.90807\n",
      "epoch no.1 train no.116640  loss = 2.06252 avg_loss = 3.85989\n",
      "epoch no.1 train no.116650  loss = 4.27346 avg_loss = 3.79919\n",
      "epoch no.1 train no.116660  loss = 3.57205 avg_loss = 3.77246\n",
      "epoch no.1 train no.116670  loss = 4.37137 avg_loss = 3.76930\n",
      "epoch no.1 train no.116680  loss = 2.68758 avg_loss = 3.85333\n",
      "epoch no.1 train no.116690  loss = 2.92293 avg_loss = 3.81184\n",
      "epoch no.1 train no.116700  loss = 4.20543 avg_loss = 3.82992\n",
      "epoch no.1 train no.116710  loss = 3.90214 avg_loss = 3.78011\n",
      "epoch no.1 train no.116720  loss = 5.07221 avg_loss = 3.81188\n",
      "epoch no.1 train no.116730  loss = 4.27423 avg_loss = 3.78014\n",
      "epoch no.1 train no.116740  loss = 3.00622 avg_loss = 3.75856\n",
      "epoch no.1 train no.116750  loss = 3.79165 avg_loss = 3.72881\n",
      "epoch no.1 train no.116760  loss = 3.17462 avg_loss = 3.76189\n",
      "epoch no.1 train no.116770  loss = 4.66145 avg_loss = 3.74789\n",
      "epoch no.1 train no.116780  loss = 4.03668 avg_loss = 3.69550\n",
      "epoch no.1 train no.116790  loss = 4.09569 avg_loss = 3.67921\n",
      "epoch no.1 train no.116800  loss = 2.46892 avg_loss = 3.68148\n",
      "epoch no.1 train no.116810  loss = 2.84909 avg_loss = 3.65347\n",
      "epoch no.1 train no.116820  loss = 2.99596 avg_loss = 3.65471\n",
      "epoch no.1 train no.116830  loss = 4.13870 avg_loss = 3.64019\n",
      "epoch no.1 train no.116840  loss = 3.34286 avg_loss = 3.63167\n",
      "epoch no.1 train no.116850  loss = 2.25198 avg_loss = 3.65162\n",
      "epoch no.1 train no.116860  loss = 4.30647 avg_loss = 3.68730\n",
      "epoch no.1 train no.116870  loss = 1.93209 avg_loss = 3.67940\n",
      "epoch no.1 train no.116880  loss = 3.65111 avg_loss = 3.70564\n",
      "epoch no.1 train no.116890  loss = 2.41630 avg_loss = 3.70689\n",
      "epoch no.1 train no.116900  loss = 2.31841 avg_loss = 3.70730\n",
      "epoch no.1 train no.116910  loss = 4.50321 avg_loss = 3.72589\n",
      "epoch no.1 train no.116920  loss = 3.05810 avg_loss = 3.72730\n",
      "epoch no.1 train no.116930  loss = 3.67930 avg_loss = 3.67690\n",
      "epoch no.1 train no.116940  loss = 3.27964 avg_loss = 3.65117\n",
      "epoch no.1 train no.116950  loss = 3.66060 avg_loss = 3.70560\n",
      "epoch no.1 train no.116960  loss = 3.33203 avg_loss = 3.75464\n",
      "epoch no.1 train no.116970  loss = 3.63717 avg_loss = 3.76829\n",
      "epoch no.1 train no.116980  loss = 3.08508 avg_loss = 3.72258\n",
      "epoch no.1 train no.116990  loss = 4.35533 avg_loss = 3.74651\n",
      "epoch no.1 train no.117000  loss = 4.91352 avg_loss = 3.73931\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '하기', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.1 train no.117010  loss = 2.43428 avg_loss = 3.70962\n",
      "epoch no.1 train no.117020  loss = 3.75601 avg_loss = 3.69914\n",
      "epoch no.1 train no.117030  loss = 3.06986 avg_loss = 3.72382\n",
      "epoch no.1 train no.117040  loss = 3.32453 avg_loss = 3.70967\n",
      "epoch no.1 train no.117050  loss = 5.29744 avg_loss = 3.69249\n",
      "epoch no.1 train no.117060  loss = 2.21392 avg_loss = 3.68365\n",
      "epoch no.1 train no.117070  loss = 3.13511 avg_loss = 3.65887\n",
      "epoch no.1 train no.117080  loss = 3.76885 avg_loss = 3.64784\n",
      "epoch no.1 train no.117090  loss = 2.09917 avg_loss = 3.64995\n",
      "epoch no.1 train no.117100  loss = 3.38571 avg_loss = 3.62936\n",
      "epoch no.1 train no.117110  loss = 4.58804 avg_loss = 3.62589\n",
      "epoch no.1 train no.117120  loss = 4.15418 avg_loss = 3.63877\n",
      "epoch no.1 train no.117130  loss = 3.89187 avg_loss = 3.68087\n",
      "epoch no.1 train no.117140  loss = 5.02874 avg_loss = 3.66320\n",
      "epoch no.1 train no.117150  loss = 4.90904 avg_loss = 3.69891\n",
      "epoch no.1 train no.117160  loss = 3.72561 avg_loss = 3.73869\n",
      "epoch no.1 train no.117170  loss = 2.26263 avg_loss = 3.74866\n",
      "epoch no.1 train no.117180  loss = 4.62228 avg_loss = 3.72286\n",
      "epoch no.1 train no.117190  loss = 4.09083 avg_loss = 3.71994\n",
      "epoch no.1 train no.117200  loss = 2.68740 avg_loss = 3.77368\n",
      "epoch no.1 train no.117210  loss = 3.42264 avg_loss = 3.77859\n",
      "epoch no.1 train no.117220  loss = 2.02928 avg_loss = 3.71489\n",
      "epoch no.1 train no.117230  loss = 2.64268 avg_loss = 3.72986\n",
      "epoch no.1 train no.117240  loss = 3.64033 avg_loss = 3.68378\n",
      "epoch no.1 train no.117250  loss = 3.49438 avg_loss = 3.70650\n",
      "epoch no.1 train no.117260  loss = 4.45415 avg_loss = 3.79969\n",
      "epoch no.1 train no.117270  loss = 2.11664 avg_loss = 3.80128\n",
      "epoch no.1 train no.117280  loss = 4.51758 avg_loss = 3.82003\n",
      "epoch no.1 train no.117290  loss = 2.55876 avg_loss = 3.78213\n",
      "epoch no.1 train no.117300  loss = 3.89141 avg_loss = 3.77557\n",
      "epoch no.1 train no.117310  loss = 3.44071 avg_loss = 3.78291\n",
      "epoch no.1 train no.117320  loss = 3.37613 avg_loss = 3.78593\n",
      "epoch no.1 train no.117330  loss = 5.63866 avg_loss = 3.75017\n",
      "epoch no.1 train no.117340  loss = 3.05750 avg_loss = 3.77205\n",
      "epoch no.1 train no.117350  loss = 5.60105 avg_loss = 3.79795\n",
      "epoch no.1 train no.117360  loss = 3.89301 avg_loss = 3.79092\n",
      "epoch no.1 train no.117370  loss = 4.99392 avg_loss = 3.80570\n",
      "epoch no.1 train no.117380  loss = 1.91877 avg_loss = 3.68388\n",
      "epoch no.1 train no.117390  loss = 3.52783 avg_loss = 3.67938\n",
      "epoch no.1 train no.117400  loss = 2.37943 avg_loss = 3.67009\n",
      "epoch no.1 train no.117410  loss = 2.73025 avg_loss = 3.61176\n",
      "epoch no.1 train no.117420  loss = 2.81602 avg_loss = 3.69886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.117430  loss = 4.09668 avg_loss = 3.66783\n",
      "epoch no.1 train no.117440  loss = 4.74912 avg_loss = 3.66357\n",
      "epoch no.1 train no.117450  loss = 4.13926 avg_loss = 3.71947\n",
      "epoch no.1 train no.117460  loss = 2.81956 avg_loss = 3.75945\n",
      "epoch no.1 train no.117470  loss = 4.22750 avg_loss = 3.79808\n",
      "epoch no.1 train no.117480  loss = 3.83261 avg_loss = 3.79121\n",
      "epoch no.1 train no.117490  loss = 5.68313 avg_loss = 3.78555\n",
      "epoch no.1 train no.117500  loss = 3.62754 avg_loss = 3.79759\n",
      "epoch no.1 train no.117510  loss = 3.12120 avg_loss = 3.86336\n",
      "epoch no.1 train no.117520  loss = 3.01059 avg_loss = 3.82998\n",
      "epoch no.1 train no.117530  loss = 2.17823 avg_loss = 3.83505\n",
      "epoch no.1 train no.117540  loss = 3.18053 avg_loss = 3.86052\n",
      "epoch no.1 train no.117550  loss = 4.49635 avg_loss = 3.93481\n",
      "epoch no.1 train no.117560  loss = 2.12830 avg_loss = 3.92298\n",
      "epoch no.1 train no.117570  loss = 3.23119 avg_loss = 3.91896\n",
      "epoch no.1 train no.117580  loss = 4.34536 avg_loss = 3.90483\n",
      "epoch no.1 train no.117590  loss = 3.36428 avg_loss = 3.84910\n",
      "epoch no.1 train no.117600  loss = 3.96570 avg_loss = 3.90317\n",
      "epoch no.1 train no.117610  loss = 4.44387 avg_loss = 3.88335\n",
      "epoch no.1 train no.117620  loss = 2.20588 avg_loss = 3.91260\n",
      "epoch no.1 train no.117630  loss = 3.71118 avg_loss = 3.93100\n",
      "epoch no.1 train no.117640  loss = 2.64235 avg_loss = 3.92296\n",
      "epoch no.1 train no.117650  loss = 3.06632 avg_loss = 3.89168\n",
      "epoch no.1 train no.117660  loss = 4.15524 avg_loss = 3.91633\n",
      "epoch no.1 train no.117670  loss = 2.84300 avg_loss = 3.88040\n",
      "epoch no.1 train no.117680  loss = 2.90519 avg_loss = 3.87269\n",
      "epoch no.1 train no.117690  loss = 3.45927 avg_loss = 3.86652\n",
      "epoch no.1 train no.117700  loss = 4.70252 avg_loss = 3.85858\n",
      "epoch no.1 train no.117710  loss = 2.04345 avg_loss = 3.80362\n",
      "epoch no.1 train no.117720  loss = 3.55749 avg_loss = 3.75329\n",
      "epoch no.1 train no.117730  loss = 5.47859 avg_loss = 3.77598\n",
      "epoch no.1 train no.117740  loss = 2.67167 avg_loss = 3.77371\n",
      "epoch no.1 train no.117750  loss = 3.15695 avg_loss = 3.77966\n",
      "epoch no.1 train no.117760  loss = 2.95392 avg_loss = 3.77934\n",
      "epoch no.1 train no.117770  loss = 2.55676 avg_loss = 3.73781\n",
      "epoch no.1 train no.117780  loss = 3.74086 avg_loss = 3.72253\n",
      "epoch no.1 train no.117790  loss = 3.19373 avg_loss = 3.66983\n",
      "epoch no.1 train no.117800  loss = 5.30234 avg_loss = 3.70680\n",
      "epoch no.1 train no.117810  loss = 2.10725 avg_loss = 3.71513\n",
      "epoch no.1 train no.117820  loss = 2.41185 avg_loss = 3.72123\n",
      "epoch no.1 train no.117830  loss = 2.76994 avg_loss = 3.67061\n",
      "epoch no.1 train no.117840  loss = 3.01622 avg_loss = 3.66041\n",
      "epoch no.1 train no.117850  loss = 3.51742 avg_loss = 3.65671\n",
      "epoch no.1 train no.117860  loss = 3.67803 avg_loss = 3.68733\n",
      "epoch no.1 train no.117870  loss = 4.86908 avg_loss = 3.70128\n",
      "epoch no.1 train no.117880  loss = 2.07488 avg_loss = 3.77835\n",
      "epoch no.1 train no.117890  loss = 5.26655 avg_loss = 3.71712\n",
      "epoch no.1 train no.117900  loss = 4.12676 avg_loss = 3.68681\n",
      "epoch no.1 train no.117910  loss = 2.45439 avg_loss = 3.59812\n",
      "epoch no.1 train no.117920  loss = 4.11305 avg_loss = 3.69629\n",
      "epoch no.1 train no.117930  loss = 3.14040 avg_loss = 3.68838\n",
      "epoch no.1 train no.117940  loss = 3.85789 avg_loss = 3.66862\n",
      "epoch no.1 train no.117950  loss = 1.92639 avg_loss = 3.68412\n",
      "epoch no.1 train no.117960  loss = 2.88354 avg_loss = 3.65343\n",
      "epoch no.1 train no.117970  loss = 3.22711 avg_loss = 3.66756\n",
      "epoch no.1 train no.117980  loss = 3.05928 avg_loss = 3.65587\n",
      "epoch no.1 train no.117990  loss = 5.15542 avg_loss = 3.61880\n",
      "epoch no.1 train no.118000  loss = 5.46229 avg_loss = 3.66382\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '을', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환 할때 듣는 노래</s>\n",
      "epoch no.1 train no.118010  loss = 4.15505 avg_loss = 3.72213\n",
      "epoch no.1 train no.118020  loss = 3.52335 avg_loss = 3.71005\n",
      "epoch no.1 train no.118030  loss = 2.40835 avg_loss = 3.68955\n",
      "epoch no.1 train no.118040  loss = 2.83090 avg_loss = 3.69519\n",
      "epoch no.1 train no.118050  loss = 2.09147 avg_loss = 3.72033\n",
      "epoch no.1 train no.118060  loss = 2.50777 avg_loss = 3.74954\n",
      "epoch no.1 train no.118070  loss = 2.23740 avg_loss = 3.68522\n",
      "epoch no.1 train no.118080  loss = 4.47096 avg_loss = 3.71447\n",
      "epoch no.1 train no.118090  loss = 4.11557 avg_loss = 3.72254\n",
      "epoch no.1 train no.118100  loss = 4.15426 avg_loss = 3.74094\n",
      "epoch no.1 train no.118110  loss = 6.42951 avg_loss = 3.77884\n",
      "epoch no.1 train no.118120  loss = 4.60420 avg_loss = 3.76754\n",
      "epoch no.1 train no.118130  loss = 4.12595 avg_loss = 3.82856\n",
      "epoch no.1 train no.118140  loss = 1.41585 avg_loss = 3.82932\n",
      "epoch no.1 train no.118150  loss = 2.45684 avg_loss = 3.78086\n",
      "epoch no.1 train no.118160  loss = 3.53334 avg_loss = 3.78912\n",
      "epoch no.1 train no.118170  loss = 2.37927 avg_loss = 3.74640\n",
      "epoch no.1 train no.118180  loss = 4.29270 avg_loss = 3.82836\n",
      "epoch no.1 train no.118190  loss = 2.83528 avg_loss = 3.77700\n",
      "epoch no.1 train no.118200  loss = 3.25507 avg_loss = 3.73396\n",
      "epoch no.1 train no.118210  loss = 5.41408 avg_loss = 3.75277\n",
      "epoch no.1 train no.118220  loss = 3.73321 avg_loss = 3.74138\n",
      "epoch no.1 train no.118230  loss = 4.06371 avg_loss = 3.77343\n",
      "epoch no.1 train no.118240  loss = 4.00032 avg_loss = 3.77229\n",
      "epoch no.1 train no.118250  loss = 2.56009 avg_loss = 3.75680\n",
      "epoch no.1 train no.118260  loss = 3.22537 avg_loss = 3.72259\n",
      "epoch no.1 train no.118270  loss = 4.46749 avg_loss = 3.70582\n",
      "epoch no.1 train no.118280  loss = 3.00822 avg_loss = 3.71727\n",
      "epoch no.1 train no.118290  loss = 2.40929 avg_loss = 3.70735\n",
      "epoch no.1 train no.118300  loss = 4.52317 avg_loss = 3.73797\n",
      "epoch no.1 train no.118310  loss = 1.40670 avg_loss = 3.69528\n",
      "epoch no.1 train no.118320  loss = 5.09209 avg_loss = 3.69848\n",
      "epoch no.1 train no.118330  loss = 2.65334 avg_loss = 3.70736\n",
      "epoch no.1 train no.118340  loss = 6.24762 avg_loss = 3.75369\n",
      "epoch no.1 train no.118350  loss = 3.10931 avg_loss = 3.76050\n",
      "epoch no.1 train no.118360  loss = 3.19637 avg_loss = 3.79239\n",
      "epoch no.1 train no.118370  loss = 5.73394 avg_loss = 3.86595\n",
      "epoch no.1 train no.118380  loss = 5.12137 avg_loss = 3.90847\n",
      "epoch no.1 train no.118390  loss = 2.65325 avg_loss = 3.85547\n",
      "epoch no.1 train no.118400  loss = 2.78186 avg_loss = 3.86011\n",
      "epoch no.1 train no.118410  loss = 2.36163 avg_loss = 3.85045\n",
      "epoch no.1 train no.118420  loss = 3.42706 avg_loss = 3.82813\n",
      "epoch no.1 train no.118430  loss = 2.13794 avg_loss = 3.81679\n",
      "epoch no.1 train no.118440  loss = 4.91144 avg_loss = 3.85856\n",
      "epoch no.1 train no.118450  loss = 4.10558 avg_loss = 3.88136\n",
      "epoch no.1 train no.118460  loss = 2.43051 avg_loss = 3.84080\n",
      "epoch no.1 train no.118470  loss = 3.77907 avg_loss = 3.89686\n",
      "epoch no.1 train no.118480  loss = 2.10963 avg_loss = 3.88517\n",
      "epoch no.1 train no.118490  loss = 5.62015 avg_loss = 3.85162\n",
      "epoch no.1 train no.118500  loss = 2.84741 avg_loss = 3.81650\n",
      "epoch no.1 train no.118510  loss = 3.86349 avg_loss = 3.80628\n",
      "epoch no.1 train no.118520  loss = 2.53327 avg_loss = 3.75018\n",
      "epoch no.1 train no.118530  loss = 3.39936 avg_loss = 3.76625\n",
      "epoch no.1 train no.118540  loss = 2.66621 avg_loss = 3.73207\n",
      "epoch no.1 train no.118550  loss = 4.10246 avg_loss = 3.73022\n",
      "epoch no.1 train no.118560  loss = 2.03550 avg_loss = 3.71744\n",
      "epoch no.1 train no.118570  loss = 3.77263 avg_loss = 3.75887\n",
      "epoch no.1 train no.118580  loss = 3.18148 avg_loss = 3.74435\n",
      "epoch no.1 train no.118590  loss = 3.55261 avg_loss = 3.75130\n",
      "epoch no.1 train no.118600  loss = 2.53220 avg_loss = 3.75244\n",
      "epoch no.1 train no.118610  loss = 2.67865 avg_loss = 3.76614\n",
      "epoch no.1 train no.118620  loss = 3.22623 avg_loss = 3.75668\n",
      "epoch no.1 train no.118630  loss = 4.38411 avg_loss = 3.76510\n",
      "epoch no.1 train no.118640  loss = 4.64613 avg_loss = 3.78605\n",
      "epoch no.1 train no.118650  loss = 3.48540 avg_loss = 3.76331\n",
      "epoch no.1 train no.118660  loss = 3.61950 avg_loss = 3.77771\n",
      "epoch no.1 train no.118670  loss = 6.99979 avg_loss = 3.83413\n",
      "epoch no.1 train no.118680  loss = 4.46214 avg_loss = 3.85815\n",
      "epoch no.1 train no.118690  loss = 2.26872 avg_loss = 3.81542\n",
      "epoch no.1 train no.118700  loss = 3.74155 avg_loss = 3.81146\n",
      "epoch no.1 train no.118710  loss = 3.14552 avg_loss = 3.82763\n",
      "epoch no.1 train no.118720  loss = 3.19571 avg_loss = 3.82557\n",
      "epoch no.1 train no.118730  loss = 3.68371 avg_loss = 3.81494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.118740  loss = 2.28148 avg_loss = 3.80917\n",
      "epoch no.1 train no.118750  loss = 3.03410 avg_loss = 3.79880\n",
      "epoch no.1 train no.118760  loss = 2.99233 avg_loss = 3.77475\n",
      "epoch no.1 train no.118770  loss = 5.02502 avg_loss = 3.80334\n",
      "epoch no.1 train no.118780  loss = 4.21687 avg_loss = 3.79653\n",
      "epoch no.1 train no.118790  loss = 3.57111 avg_loss = 3.82620\n",
      "epoch no.1 train no.118800  loss = 1.99676 avg_loss = 3.78146\n",
      "epoch no.1 train no.118810  loss = 3.89453 avg_loss = 3.77926\n",
      "epoch no.1 train no.118820  loss = 4.20342 avg_loss = 3.77891\n",
      "epoch no.1 train no.118830  loss = 2.57128 avg_loss = 3.81337\n",
      "epoch no.1 train no.118840  loss = 4.46702 avg_loss = 3.85271\n",
      "epoch no.1 train no.118850  loss = 5.34555 avg_loss = 3.86435\n",
      "epoch no.1 train no.118860  loss = 3.86760 avg_loss = 3.84122\n",
      "epoch no.1 train no.118870  loss = 5.06615 avg_loss = 3.85849\n",
      "epoch no.1 train no.118880  loss = 5.42981 avg_loss = 3.89132\n",
      "epoch no.1 train no.118890  loss = 2.88903 avg_loss = 3.82342\n",
      "epoch no.1 train no.118900  loss = 2.75402 avg_loss = 3.80753\n",
      "epoch no.1 train no.118910  loss = 4.23283 avg_loss = 3.78803\n",
      "epoch no.1 train no.118920  loss = 5.07741 avg_loss = 3.82128\n",
      "epoch no.1 train no.118930  loss = 2.17552 avg_loss = 3.79799\n",
      "epoch no.1 train no.118940  loss = 4.22308 avg_loss = 3.76573\n",
      "epoch no.1 train no.118950  loss = 5.23374 avg_loss = 3.84543\n",
      "epoch no.1 train no.118960  loss = 2.24955 avg_loss = 3.75856\n",
      "epoch no.1 train no.118970  loss = 2.28327 avg_loss = 3.74106\n",
      "epoch no.1 train no.118980  loss = 2.92046 avg_loss = 3.70832\n",
      "epoch no.1 train no.118990  loss = 3.76442 avg_loss = 3.68684\n",
      "epoch no.1 train no.119000  loss = 3.29082 avg_loss = 3.69453\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '에', '▁좋은', '▁팝', '에이지', '</s>', '</s>']\n",
      "기분전환에 좋은 뉴에이지 음악</s>\n",
      "epoch no.1 train no.119010  loss = 2.86656 avg_loss = 3.70442\n",
      "epoch no.1 train no.119020  loss = 4.70519 avg_loss = 3.69295\n",
      "epoch no.1 train no.119030  loss = 5.53887 avg_loss = 3.73292\n",
      "epoch no.1 train no.119040  loss = 2.58064 avg_loss = 3.75172\n",
      "epoch no.1 train no.119050  loss = 3.87454 avg_loss = 3.80340\n",
      "epoch no.1 train no.119060  loss = 3.67915 avg_loss = 3.77665\n",
      "epoch no.1 train no.119070  loss = 2.99978 avg_loss = 3.67658\n",
      "epoch no.1 train no.119080  loss = 4.12966 avg_loss = 3.62585\n",
      "epoch no.1 train no.119090  loss = 3.97846 avg_loss = 3.65156\n",
      "epoch no.1 train no.119100  loss = 4.09424 avg_loss = 3.65962\n",
      "epoch no.1 train no.119110  loss = 3.17348 avg_loss = 3.69848\n",
      "epoch no.1 train no.119120  loss = 3.11869 avg_loss = 3.71012\n",
      "epoch no.1 train no.119130  loss = 2.73296 avg_loss = 3.71990\n",
      "epoch no.1 train no.119140  loss = 3.66337 avg_loss = 3.67202\n",
      "epoch no.1 train no.119150  loss = 6.79049 avg_loss = 3.70668\n",
      "epoch no.1 train no.119160  loss = 3.43067 avg_loss = 3.73654\n",
      "epoch no.1 train no.119170  loss = 1.71346 avg_loss = 3.68550\n",
      "epoch no.1 train no.119180  loss = 5.53162 avg_loss = 3.68147\n",
      "epoch no.1 train no.119190  loss = 4.62147 avg_loss = 3.65969\n",
      "epoch no.1 train no.119200  loss = 4.77563 avg_loss = 3.68324\n",
      "epoch no.1 train no.119210  loss = 3.41689 avg_loss = 3.68812\n",
      "epoch no.1 train no.119220  loss = 3.53598 avg_loss = 3.75653\n",
      "epoch no.1 train no.119230  loss = 4.07476 avg_loss = 3.78475\n",
      "epoch no.1 train no.119240  loss = 3.89770 avg_loss = 3.80034\n",
      "epoch no.1 train no.119250  loss = 3.53783 avg_loss = 3.80906\n",
      "epoch no.1 train no.119260  loss = 7.74219 avg_loss = 3.87516\n",
      "epoch no.1 train no.119270  loss = 2.85341 avg_loss = 3.88302\n",
      "epoch no.1 train no.119280  loss = 5.24401 avg_loss = 3.93681\n",
      "epoch no.1 train no.119290  loss = 5.57865 avg_loss = 3.92246\n",
      "epoch no.1 train no.119300  loss = 5.58602 avg_loss = 3.94960\n",
      "epoch no.1 train no.119310  loss = 4.32530 avg_loss = 3.91023\n",
      "epoch no.1 train no.119320  loss = 4.32379 avg_loss = 3.93960\n",
      "epoch no.1 train no.119330  loss = 1.98598 avg_loss = 3.95173\n",
      "epoch no.1 train no.119340  loss = 2.70433 avg_loss = 3.92249\n",
      "epoch no.1 train no.119350  loss = 3.43811 avg_loss = 3.94037\n",
      "epoch no.1 train no.119360  loss = 4.72055 avg_loss = 3.94467\n",
      "epoch no.1 train no.119370  loss = 3.08915 avg_loss = 3.93470\n",
      "epoch no.1 train no.119380  loss = 3.50191 avg_loss = 3.94139\n",
      "epoch no.1 train no.119390  loss = 2.54920 avg_loss = 3.91065\n",
      "epoch no.1 train no.119400  loss = 4.37403 avg_loss = 3.85791\n",
      "epoch no.1 train no.119410  loss = 4.27266 avg_loss = 3.88166\n",
      "epoch no.1 train no.119420  loss = 2.87352 avg_loss = 3.86773\n",
      "epoch no.1 train no.119430  loss = 3.36242 avg_loss = 3.84653\n",
      "epoch no.1 train no.119440  loss = 2.48556 avg_loss = 3.83123\n",
      "epoch no.1 train no.119450  loss = 2.51924 avg_loss = 3.81696\n",
      "epoch no.1 train no.119460  loss = 4.28112 avg_loss = 3.82129\n",
      "epoch no.1 train no.119470  loss = 4.22765 avg_loss = 3.82018\n",
      "epoch no.1 train no.119480  loss = 5.55786 avg_loss = 3.86722\n",
      "epoch no.1 train no.119490  loss = 4.92303 avg_loss = 3.88508\n",
      "epoch no.1 train no.119500  loss = 4.49661 avg_loss = 3.86415\n",
      "epoch no.1 train no.119510  loss = 4.51954 avg_loss = 3.85371\n",
      "epoch no.1 train no.119520  loss = 3.29954 avg_loss = 3.82125\n",
      "epoch no.1 train no.119530  loss = 4.64514 avg_loss = 3.79258\n",
      "epoch no.1 train no.119540  loss = 3.23758 avg_loss = 3.78514\n",
      "epoch no.1 train no.119550  loss = 3.50606 avg_loss = 3.75307\n",
      "epoch no.1 train no.119560  loss = 5.25274 avg_loss = 3.77511\n",
      "epoch no.1 train no.119570  loss = 3.71527 avg_loss = 3.74362\n",
      "epoch no.1 train no.119580  loss = 2.20172 avg_loss = 3.68835\n",
      "epoch no.1 train no.119590  loss = 2.95633 avg_loss = 3.66349\n",
      "epoch no.1 train no.119600  loss = 5.33197 avg_loss = 3.70683\n",
      "epoch no.1 train no.119610  loss = 3.75442 avg_loss = 3.71971\n",
      "epoch no.1 train no.119620  loss = 2.56008 avg_loss = 3.71877\n",
      "epoch no.1 train no.119630  loss = 2.99269 avg_loss = 3.67063\n",
      "epoch no.1 train no.119640  loss = 4.64622 avg_loss = 3.70492\n",
      "epoch no.1 train no.119650  loss = 2.98040 avg_loss = 3.71879\n",
      "epoch no.1 train no.119660  loss = 2.99513 avg_loss = 3.70508\n",
      "epoch no.1 train no.119670  loss = 2.97423 avg_loss = 3.70100\n",
      "epoch no.1 train no.119680  loss = 4.48165 avg_loss = 3.76211\n",
      "epoch no.1 train no.119690  loss = 3.36178 avg_loss = 3.73086\n",
      "epoch no.1 train no.119700  loss = 4.41058 avg_loss = 3.73256\n",
      "epoch no.1 train no.119710  loss = 3.48089 avg_loss = 3.77562\n",
      "epoch no.1 train no.119720  loss = 3.78497 avg_loss = 3.72317\n",
      "epoch no.1 train no.119730  loss = 5.91295 avg_loss = 3.78674\n",
      "epoch no.1 train no.119740  loss = 2.32937 avg_loss = 3.75940\n",
      "epoch no.1 train no.119750  loss = 1.94313 avg_loss = 3.72098\n",
      "epoch no.1 train no.119760  loss = 3.43789 avg_loss = 3.77541\n",
      "epoch no.1 train no.119770  loss = 2.62824 avg_loss = 3.78776\n",
      "epoch no.1 train no.119780  loss = 3.68061 avg_loss = 3.74768\n",
      "epoch no.1 train no.119790  loss = 3.90640 avg_loss = 3.72354\n",
      "epoch no.1 train no.119800  loss = 3.22166 avg_loss = 3.72529\n",
      "epoch no.1 train no.119810  loss = 3.57805 avg_loss = 3.73239\n",
      "epoch no.1 train no.119820  loss = 4.74803 avg_loss = 3.72469\n",
      "epoch no.1 train no.119830  loss = 3.93896 avg_loss = 3.69979\n",
      "epoch no.1 train no.119840  loss = 4.16422 avg_loss = 3.72932\n",
      "epoch no.1 train no.119850  loss = 2.42256 avg_loss = 3.70489\n",
      "epoch no.1 train no.119860  loss = 3.56924 avg_loss = 3.74286\n",
      "epoch no.1 train no.119870  loss = 4.10429 avg_loss = 3.72489\n",
      "epoch no.1 train no.119880  loss = 3.04691 avg_loss = 3.71747\n",
      "epoch no.1 train no.119890  loss = 3.62171 avg_loss = 3.73090\n",
      "epoch no.1 train no.119900  loss = 4.63266 avg_loss = 3.72394\n",
      "epoch no.1 train no.119910  loss = 4.27026 avg_loss = 3.70805\n",
      "epoch no.1 train no.119920  loss = 5.12261 avg_loss = 3.70357\n",
      "epoch no.1 train no.119930  loss = 4.88859 avg_loss = 3.72814\n",
      "epoch no.1 train no.119940  loss = 3.36942 avg_loss = 3.69874\n",
      "epoch no.1 train no.119950  loss = 2.91066 avg_loss = 3.67863\n",
      "epoch no.1 train no.119960  loss = 5.46561 avg_loss = 3.73435\n",
      "epoch no.1 train no.119970  loss = 3.04961 avg_loss = 3.75710\n",
      "epoch no.1 train no.119980  loss = 3.60182 avg_loss = 3.76265\n",
      "epoch no.1 train no.119990  loss = 4.55711 avg_loss = 3.73879\n",
      "epoch no.1 train no.120000  loss = 4.70547 avg_loss = 3.68358\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.1 train no.120010  loss = 1.91701 avg_loss = 3.69982\n",
      "epoch no.1 train no.120020  loss = 4.82078 avg_loss = 3.71185\n",
      "epoch no.1 train no.120030  loss = 5.78914 avg_loss = 3.77504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.120040  loss = 3.65532 avg_loss = 3.78083\n",
      "epoch no.1 train no.120050  loss = 2.56946 avg_loss = 3.81574\n",
      "epoch no.1 train no.120060  loss = 2.77780 avg_loss = 3.77826\n",
      "epoch no.1 train no.120070  loss = 4.29852 avg_loss = 3.78491\n",
      "epoch no.1 train no.120080  loss = 3.07591 avg_loss = 3.79285\n",
      "epoch no.1 train no.120090  loss = 3.02289 avg_loss = 3.83080\n",
      "epoch no.1 train no.120100  loss = 3.91432 avg_loss = 3.85567\n",
      "epoch no.1 train no.120110  loss = 5.16641 avg_loss = 3.85385\n",
      "epoch no.1 train no.120120  loss = 3.17718 avg_loss = 3.86754\n",
      "epoch no.1 train no.120130  loss = 3.88226 avg_loss = 3.83328\n",
      "epoch no.1 train no.120140  loss = 4.55308 avg_loss = 3.84243\n",
      "epoch no.1 train no.120150  loss = 3.06456 avg_loss = 3.82173\n",
      "epoch no.1 train no.120160  loss = 3.27619 avg_loss = 3.83738\n",
      "epoch no.1 train no.120170  loss = 4.70185 avg_loss = 3.78755\n",
      "epoch no.1 train no.120180  loss = 2.04077 avg_loss = 3.77755\n",
      "epoch no.1 train no.120190  loss = 7.51689 avg_loss = 3.74449\n",
      "epoch no.1 train no.120200  loss = 1.97164 avg_loss = 3.73315\n",
      "epoch no.1 train no.120210  loss = 3.18766 avg_loss = 3.75470\n",
      "epoch no.1 train no.120220  loss = 4.53381 avg_loss = 3.72082\n",
      "epoch no.1 train no.120230  loss = 2.23575 avg_loss = 3.71645\n",
      "epoch no.1 train no.120240  loss = 2.96610 avg_loss = 3.77999\n",
      "epoch no.1 train no.120250  loss = 2.41502 avg_loss = 3.77520\n",
      "epoch no.1 train no.120260  loss = 4.13928 avg_loss = 3.76071\n",
      "epoch no.1 train no.120270  loss = 4.20946 avg_loss = 3.74529\n",
      "epoch no.1 train no.120280  loss = 2.64126 avg_loss = 3.69759\n",
      "epoch no.1 train no.120290  loss = 2.72886 avg_loss = 3.63055\n",
      "epoch no.1 train no.120300  loss = 3.40102 avg_loss = 3.69692\n",
      "epoch no.1 train no.120310  loss = 3.44730 avg_loss = 3.72613\n",
      "epoch no.1 train no.120320  loss = 2.83264 avg_loss = 3.69104\n",
      "epoch no.1 train no.120330  loss = 5.19580 avg_loss = 3.71545\n",
      "epoch no.1 train no.120340  loss = 2.62276 avg_loss = 3.78223\n",
      "epoch no.1 train no.120350  loss = 4.67687 avg_loss = 3.74165\n",
      "epoch no.1 train no.120360  loss = 4.41727 avg_loss = 3.76746\n",
      "epoch no.1 train no.120370  loss = 3.59759 avg_loss = 3.76033\n",
      "epoch no.1 train no.120380  loss = 2.76404 avg_loss = 3.70774\n",
      "epoch no.1 train no.120390  loss = 3.33634 avg_loss = 3.73487\n",
      "epoch no.1 train no.120400  loss = 4.64299 avg_loss = 3.74773\n",
      "epoch no.1 train no.120410  loss = 2.81137 avg_loss = 3.72197\n",
      "epoch no.1 train no.120420  loss = 2.20252 avg_loss = 3.67914\n",
      "epoch no.1 train no.120430  loss = 3.36207 avg_loss = 3.65776\n",
      "epoch no.1 train no.120440  loss = 3.28086 avg_loss = 3.68283\n",
      "epoch no.1 train no.120450  loss = 4.69864 avg_loss = 3.67257\n",
      "epoch no.1 train no.120460  loss = 3.27444 avg_loss = 3.70784\n",
      "epoch no.1 train no.120470  loss = 5.55656 avg_loss = 3.68989\n",
      "epoch no.1 train no.120480  loss = 3.95514 avg_loss = 3.69635\n",
      "epoch no.1 train no.120490  loss = 2.76327 avg_loss = 3.75023\n",
      "epoch no.1 train no.120500  loss = 4.55421 avg_loss = 3.76407\n",
      "epoch no.1 train no.120510  loss = 5.85422 avg_loss = 3.80117\n",
      "epoch no.1 train no.120520  loss = 6.37275 avg_loss = 3.76159\n",
      "epoch no.1 train no.120530  loss = 4.87161 avg_loss = 3.76982\n",
      "epoch no.1 train no.120540  loss = 2.41198 avg_loss = 3.75805\n",
      "epoch no.1 train no.120550  loss = 6.16071 avg_loss = 3.79407\n",
      "epoch no.1 train no.120560  loss = 2.18964 avg_loss = 3.76266\n",
      "epoch no.1 train no.120570  loss = 2.76174 avg_loss = 3.75276\n",
      "epoch no.1 train no.120580  loss = 2.61287 avg_loss = 3.74270\n",
      "epoch no.1 train no.120590  loss = 4.86806 avg_loss = 3.74277\n",
      "epoch no.1 train no.120600  loss = 4.83159 avg_loss = 3.78461\n",
      "epoch no.1 train no.120610  loss = 5.08130 avg_loss = 3.87858\n",
      "epoch no.1 train no.120620  loss = 2.90144 avg_loss = 3.82564\n",
      "epoch no.1 train no.120630  loss = 3.64989 avg_loss = 3.85952\n",
      "epoch no.1 train no.120640  loss = 3.50041 avg_loss = 3.84888\n",
      "epoch no.1 train no.120650  loss = 3.85189 avg_loss = 3.79057\n",
      "epoch no.1 train no.120660  loss = 2.64762 avg_loss = 3.81323\n",
      "epoch no.1 train no.120670  loss = 2.17664 avg_loss = 3.77124\n",
      "epoch no.1 train no.120680  loss = 4.53899 avg_loss = 3.76783\n",
      "epoch no.1 train no.120690  loss = 2.51869 avg_loss = 3.68907\n",
      "epoch no.1 train no.120700  loss = 3.91069 avg_loss = 3.65521\n",
      "epoch no.1 train no.120710  loss = 2.93522 avg_loss = 3.60631\n",
      "epoch no.1 train no.120720  loss = 1.97021 avg_loss = 3.57194\n",
      "epoch no.1 train no.120730  loss = 2.85364 avg_loss = 3.60099\n",
      "epoch no.1 train no.120740  loss = 3.45554 avg_loss = 3.66866\n",
      "epoch no.1 train no.120750  loss = 3.84523 avg_loss = 3.67033\n",
      "epoch no.1 train no.120760  loss = 3.50169 avg_loss = 3.67786\n",
      "epoch no.1 train no.120770  loss = 2.46226 avg_loss = 3.68938\n",
      "epoch no.1 train no.120780  loss = 3.59444 avg_loss = 3.71771\n",
      "epoch no.1 train no.120790  loss = 2.76063 avg_loss = 3.69026\n",
      "epoch no.1 train no.120800  loss = 3.57401 avg_loss = 3.71819\n",
      "epoch no.1 train no.120810  loss = 2.79261 avg_loss = 3.75403\n",
      "epoch no.1 train no.120820  loss = 3.51526 avg_loss = 3.73712\n",
      "epoch no.1 train no.120830  loss = 6.82505 avg_loss = 3.78848\n",
      "epoch no.1 train no.120840  loss = 3.74279 avg_loss = 3.75120\n",
      "epoch no.1 train no.120850  loss = 2.26773 avg_loss = 3.70427\n",
      "epoch no.1 train no.120860  loss = 4.00696 avg_loss = 3.75191\n",
      "epoch no.1 train no.120870  loss = 4.11786 avg_loss = 3.75871\n",
      "epoch no.1 train no.120880  loss = 4.05893 avg_loss = 3.75868\n",
      "epoch no.1 train no.120890  loss = 3.82536 avg_loss = 3.74325\n",
      "epoch no.1 train no.120900  loss = 4.34916 avg_loss = 3.75081\n",
      "epoch no.1 train no.120910  loss = 2.69089 avg_loss = 3.74575\n",
      "epoch no.1 train no.120920  loss = 3.41201 avg_loss = 3.75530\n",
      "epoch no.1 train no.120930  loss = 3.42402 avg_loss = 3.80456\n",
      "epoch no.1 train no.120940  loss = 5.34660 avg_loss = 3.82331\n",
      "epoch no.1 train no.120950  loss = 4.93621 avg_loss = 3.92114\n",
      "epoch no.1 train no.120960  loss = 2.24178 avg_loss = 3.93237\n",
      "epoch no.1 train no.120970  loss = 6.61396 avg_loss = 3.90047\n",
      "epoch no.1 train no.120980  loss = 3.86343 avg_loss = 3.88626\n",
      "epoch no.1 train no.120990  loss = 6.79661 avg_loss = 3.86144\n",
      "epoch no.1 train no.121000  loss = 6.50390 avg_loss = 3.97243\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 좋은 음악</s>\n",
      "epoch no.1 train no.121010  loss = 2.95206 avg_loss = 3.94616\n",
      "epoch no.1 train no.121020  loss = 3.40017 avg_loss = 3.89923\n",
      "epoch no.1 train no.121030  loss = 2.16007 avg_loss = 3.87144\n",
      "epoch no.1 train no.121040  loss = 5.42276 avg_loss = 3.91029\n",
      "epoch no.1 train no.121050  loss = 2.48043 avg_loss = 3.89235\n",
      "epoch no.1 train no.121060  loss = 4.04493 avg_loss = 3.85017\n",
      "epoch no.1 train no.121070  loss = 7.05187 avg_loss = 3.87805\n",
      "epoch no.1 train no.121080  loss = 3.05308 avg_loss = 3.87299\n",
      "epoch no.1 train no.121090  loss = 6.32799 avg_loss = 3.87616\n",
      "epoch no.1 train no.121100  loss = 2.91275 avg_loss = 3.86782\n",
      "epoch no.1 train no.121110  loss = 2.54327 avg_loss = 3.82427\n",
      "epoch no.1 train no.121120  loss = 5.07406 avg_loss = 3.85097\n",
      "epoch no.1 train no.121130  loss = 3.83115 avg_loss = 3.84334\n",
      "epoch no.1 train no.121140  loss = 2.50391 avg_loss = 3.82383\n",
      "epoch no.1 train no.121150  loss = 3.13485 avg_loss = 3.77498\n",
      "epoch no.1 train no.121160  loss = 2.86402 avg_loss = 3.75893\n",
      "epoch no.1 train no.121170  loss = 3.70389 avg_loss = 3.76717\n",
      "epoch no.1 train no.121180  loss = 3.66608 avg_loss = 3.73885\n",
      "epoch no.1 train no.121190  loss = 3.05227 avg_loss = 3.67267\n",
      "epoch no.1 train no.121200  loss = 3.89993 avg_loss = 3.61645\n",
      "epoch no.1 train no.121210  loss = 2.54051 avg_loss = 3.61773\n",
      "epoch no.1 train no.121220  loss = 3.22049 avg_loss = 3.63403\n",
      "epoch no.1 train no.121230  loss = 4.61059 avg_loss = 3.65579\n",
      "epoch no.1 train no.121240  loss = 4.41925 avg_loss = 3.71100\n",
      "epoch no.1 train no.121250  loss = 5.35792 avg_loss = 3.77035\n",
      "epoch no.1 train no.121260  loss = 2.49109 avg_loss = 3.77324\n",
      "epoch no.1 train no.121270  loss = 2.98329 avg_loss = 3.76549\n",
      "epoch no.1 train no.121280  loss = 2.80359 avg_loss = 3.77063\n",
      "epoch no.1 train no.121290  loss = 3.29269 avg_loss = 3.74967\n",
      "epoch no.1 train no.121300  loss = 2.16750 avg_loss = 3.79873\n",
      "epoch no.1 train no.121310  loss = 3.82930 avg_loss = 3.80482\n",
      "epoch no.1 train no.121320  loss = 3.64264 avg_loss = 3.75668\n",
      "epoch no.1 train no.121330  loss = 3.12140 avg_loss = 3.74911\n",
      "epoch no.1 train no.121340  loss = 5.37707 avg_loss = 3.77737\n",
      "epoch no.1 train no.121350  loss = 3.18477 avg_loss = 3.75645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.121360  loss = 2.66470 avg_loss = 3.80063\n",
      "epoch no.1 train no.121370  loss = 3.01857 avg_loss = 3.79818\n",
      "epoch no.1 train no.121380  loss = 4.43390 avg_loss = 3.84527\n",
      "epoch no.1 train no.121390  loss = 2.62252 avg_loss = 3.76770\n",
      "epoch no.1 train no.121400  loss = 3.51538 avg_loss = 3.72379\n",
      "epoch no.1 train no.121410  loss = 4.39544 avg_loss = 3.72227\n",
      "epoch no.1 train no.121420  loss = 5.30471 avg_loss = 3.75961\n",
      "epoch no.1 train no.121430  loss = 4.12893 avg_loss = 3.78713\n",
      "epoch no.1 train no.121440  loss = 3.20523 avg_loss = 3.80810\n",
      "epoch no.1 train no.121450  loss = 2.62784 avg_loss = 3.78207\n",
      "epoch no.1 train no.121460  loss = 5.42181 avg_loss = 3.77093\n",
      "epoch no.1 train no.121470  loss = 3.21795 avg_loss = 3.83930\n",
      "epoch no.1 train no.121480  loss = 3.58860 avg_loss = 3.85085\n",
      "epoch no.1 train no.121490  loss = 4.30602 avg_loss = 3.83171\n",
      "epoch no.1 train no.121500  loss = 2.63823 avg_loss = 3.82125\n",
      "epoch no.1 train no.121510  loss = 2.85287 avg_loss = 3.79820\n",
      "epoch no.1 train no.121520  loss = 2.28521 avg_loss = 3.77431\n",
      "epoch no.1 train no.121530  loss = 6.33851 avg_loss = 3.83491\n",
      "epoch no.1 train no.121540  loss = 3.86314 avg_loss = 3.85992\n",
      "epoch no.1 train no.121550  loss = 2.94151 avg_loss = 3.83484\n",
      "epoch no.1 train no.121560  loss = 2.58738 avg_loss = 3.79707\n",
      "epoch no.1 train no.121570  loss = 4.63048 avg_loss = 3.82589\n",
      "epoch no.1 train no.121580  loss = 3.05780 avg_loss = 3.79697\n",
      "epoch no.1 train no.121590  loss = 2.94069 avg_loss = 3.75229\n",
      "epoch no.1 train no.121600  loss = 4.17150 avg_loss = 3.75283\n",
      "epoch no.1 train no.121610  loss = 4.06991 avg_loss = 3.78456\n",
      "epoch no.1 train no.121620  loss = 3.36883 avg_loss = 3.75114\n",
      "epoch no.1 train no.121630  loss = 5.72582 avg_loss = 3.73917\n",
      "epoch no.1 train no.121640  loss = 3.02005 avg_loss = 3.71243\n",
      "epoch no.1 train no.121650  loss = 3.21788 avg_loss = 3.69243\n",
      "epoch no.1 train no.121660  loss = 3.46477 avg_loss = 3.65352\n",
      "epoch no.1 train no.121670  loss = 2.69046 avg_loss = 3.67107\n",
      "epoch no.1 train no.121680  loss = 2.64579 avg_loss = 3.66838\n",
      "epoch no.1 train no.121690  loss = 2.23681 avg_loss = 3.68953\n",
      "epoch no.1 train no.121700  loss = 4.31205 avg_loss = 3.74878\n",
      "epoch no.1 train no.121710  loss = 3.47087 avg_loss = 3.73651\n",
      "epoch no.1 train no.121720  loss = 4.63488 avg_loss = 3.77612\n",
      "epoch no.1 train no.121730  loss = 3.93810 avg_loss = 3.85628\n",
      "epoch no.1 train no.121740  loss = 4.61033 avg_loss = 3.82379\n",
      "epoch no.1 train no.121750  loss = 4.08702 avg_loss = 3.86783\n",
      "epoch no.1 train no.121760  loss = 2.16530 avg_loss = 3.87621\n",
      "epoch no.1 train no.121770  loss = 3.10868 avg_loss = 3.90942\n",
      "epoch no.1 train no.121780  loss = 3.38247 avg_loss = 3.89312\n",
      "epoch no.1 train no.121790  loss = 3.56833 avg_loss = 3.91151\n",
      "epoch no.1 train no.121800  loss = 4.84994 avg_loss = 3.94279\n",
      "epoch no.1 train no.121810  loss = 3.31301 avg_loss = 3.97955\n",
      "epoch no.1 train no.121820  loss = 3.39737 avg_loss = 3.98054\n",
      "epoch no.1 train no.121830  loss = 3.98230 avg_loss = 3.90228\n",
      "epoch no.1 train no.121840  loss = 3.60658 avg_loss = 3.89872\n",
      "epoch no.1 train no.121850  loss = 3.03357 avg_loss = 3.95095\n",
      "epoch no.1 train no.121860  loss = 2.97390 avg_loss = 3.92350\n",
      "epoch no.1 train no.121870  loss = 3.59887 avg_loss = 3.91094\n",
      "epoch no.1 train no.121880  loss = 3.12084 avg_loss = 3.89826\n",
      "epoch no.1 train no.121890  loss = 3.11411 avg_loss = 3.85703\n",
      "epoch no.1 train no.121900  loss = 2.72445 avg_loss = 3.87040\n",
      "epoch no.1 train no.121910  loss = 2.73690 avg_loss = 3.83752\n",
      "epoch no.1 train no.121920  loss = 3.64462 avg_loss = 3.81081\n",
      "epoch no.1 train no.121930  loss = 4.22737 avg_loss = 3.78543\n",
      "epoch no.1 train no.121940  loss = 4.13624 avg_loss = 3.71387\n",
      "epoch no.1 train no.121950  loss = 4.01991 avg_loss = 3.68485\n",
      "epoch no.1 train no.121960  loss = 4.79594 avg_loss = 3.69601\n",
      "epoch no.1 train no.121970  loss = 3.28232 avg_loss = 3.67929\n",
      "epoch no.1 train no.121980  loss = 3.45640 avg_loss = 3.68249\n",
      "epoch no.1 train no.121990  loss = 2.55122 avg_loss = 3.71494\n",
      "epoch no.1 train no.122000  loss = 3.03072 avg_loss = 3.71385\n",
      "6\n",
      "to_tokens: ['▁가을', '▁좋은', '이', '▁필요할', '▁때', '</s>', '▁음악', '▁팝', '송']\n",
      "기분전환이 필요할때 듣는 신나는 팝</s>\n",
      "epoch no.1 train no.122010  loss = 3.77287 avg_loss = 3.70249\n",
      "epoch no.1 train no.122020  loss = 4.19228 avg_loss = 3.71320\n",
      "epoch no.1 train no.122030  loss = 2.97818 avg_loss = 3.71371\n",
      "epoch no.1 train no.122040  loss = 1.96379 avg_loss = 3.68393\n",
      "epoch no.1 train no.122050  loss = 4.63564 avg_loss = 3.73412\n",
      "epoch no.1 train no.122060  loss = 7.13986 avg_loss = 3.71538\n",
      "epoch no.1 train no.122070  loss = 4.07573 avg_loss = 3.75271\n",
      "epoch no.1 train no.122080  loss = 2.29651 avg_loss = 3.75255\n",
      "epoch no.1 train no.122090  loss = 2.39346 avg_loss = 3.77058\n",
      "epoch no.1 train no.122100  loss = 3.88735 avg_loss = 3.75925\n",
      "epoch no.1 train no.122110  loss = 3.73815 avg_loss = 3.79558\n",
      "epoch no.1 train no.122120  loss = 3.27590 avg_loss = 3.79732\n",
      "epoch no.1 train no.122130  loss = 3.12077 avg_loss = 3.75315\n",
      "epoch no.1 train no.122140  loss = 2.89044 avg_loss = 3.76043\n",
      "epoch no.1 train no.122150  loss = 5.62959 avg_loss = 3.83113\n",
      "epoch no.1 train no.122160  loss = 4.74387 avg_loss = 3.84961\n",
      "epoch no.1 train no.122170  loss = 5.16159 avg_loss = 3.79475\n",
      "epoch no.1 train no.122180  loss = 4.94678 avg_loss = 3.79052\n",
      "epoch no.1 train no.122190  loss = 2.37145 avg_loss = 3.74232\n",
      "epoch no.1 train no.122200  loss = 3.27328 avg_loss = 3.77013\n",
      "epoch no.1 train no.122210  loss = 6.30988 avg_loss = 3.81214\n",
      "epoch no.1 train no.122220  loss = 3.21818 avg_loss = 3.79164\n",
      "epoch no.1 train no.122230  loss = 4.02780 avg_loss = 3.77570\n",
      "epoch no.1 train no.122240  loss = 4.08635 avg_loss = 3.72435\n",
      "epoch no.1 train no.122250  loss = 3.27176 avg_loss = 3.71221\n",
      "epoch no.1 train no.122260  loss = 5.19060 avg_loss = 3.70302\n",
      "epoch no.1 train no.122270  loss = 2.36491 avg_loss = 3.69106\n",
      "epoch no.1 train no.122280  loss = 4.51170 avg_loss = 3.70838\n",
      "epoch no.1 train no.122290  loss = 4.11830 avg_loss = 3.74012\n",
      "epoch no.1 train no.122300  loss = 3.75151 avg_loss = 3.73458\n",
      "epoch no.1 train no.122310  loss = 2.63866 avg_loss = 3.66499\n",
      "epoch no.1 train no.122320  loss = 4.83559 avg_loss = 3.63310\n",
      "epoch no.1 train no.122330  loss = 1.89424 avg_loss = 3.60034\n",
      "epoch no.1 train no.122340  loss = 2.38660 avg_loss = 3.59722\n",
      "epoch no.1 train no.122350  loss = 3.35261 avg_loss = 3.56792\n",
      "epoch no.1 train no.122360  loss = 2.53101 avg_loss = 3.55532\n",
      "epoch no.1 train no.122370  loss = 3.61281 avg_loss = 3.56682\n",
      "epoch no.1 train no.122380  loss = 1.75722 avg_loss = 3.55886\n",
      "epoch no.1 train no.122390  loss = 4.07739 avg_loss = 3.58061\n",
      "epoch no.1 train no.122400  loss = 3.05545 avg_loss = 3.52512\n",
      "epoch no.1 train no.122410  loss = 2.70745 avg_loss = 3.57285\n",
      "epoch no.1 train no.122420  loss = 2.10794 avg_loss = 3.58626\n",
      "epoch no.1 train no.122430  loss = 1.85616 avg_loss = 3.61474\n",
      "epoch no.1 train no.122440  loss = 3.71767 avg_loss = 3.63977\n",
      "epoch no.1 train no.122450  loss = 3.98471 avg_loss = 3.71205\n",
      "epoch no.1 train no.122460  loss = 5.09777 avg_loss = 3.67154\n",
      "epoch no.1 train no.122470  loss = 4.68339 avg_loss = 3.68761\n",
      "epoch no.1 train no.122480  loss = 4.67827 avg_loss = 3.71474\n",
      "epoch no.1 train no.122490  loss = 3.55068 avg_loss = 3.70051\n",
      "epoch no.1 train no.122500  loss = 3.94418 avg_loss = 3.77238\n",
      "epoch no.1 train no.122510  loss = 4.39893 avg_loss = 3.82385\n",
      "epoch no.1 train no.122520  loss = 4.45285 avg_loss = 3.80032\n",
      "epoch no.1 train no.122530  loss = 2.42848 avg_loss = 3.77901\n",
      "epoch no.1 train no.122540  loss = 2.52443 avg_loss = 3.78627\n",
      "epoch no.1 train no.122550  loss = 2.89778 avg_loss = 3.79343\n",
      "epoch no.1 train no.122560  loss = 3.00429 avg_loss = 3.74419\n",
      "epoch no.1 train no.122570  loss = 2.44933 avg_loss = 3.72273\n",
      "epoch no.1 train no.122580  loss = 3.23383 avg_loss = 3.71379\n",
      "epoch no.1 train no.122590  loss = 4.47882 avg_loss = 3.70320\n",
      "epoch no.1 train no.122600  loss = 4.06329 avg_loss = 3.68815\n",
      "epoch no.1 train no.122610  loss = 4.34419 avg_loss = 3.68562\n",
      "epoch no.1 train no.122620  loss = 4.16345 avg_loss = 3.71072\n",
      "epoch no.1 train no.122630  loss = 4.61387 avg_loss = 3.68850\n",
      "epoch no.1 train no.122640  loss = 2.43221 avg_loss = 3.68376\n",
      "epoch no.1 train no.122650  loss = 1.78783 avg_loss = 3.65326\n",
      "epoch no.1 train no.122660  loss = 2.13681 avg_loss = 3.65971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.122670  loss = 6.12740 avg_loss = 3.70471\n",
      "epoch no.1 train no.122680  loss = 2.69691 avg_loss = 3.68913\n",
      "epoch no.1 train no.122690  loss = 4.46647 avg_loss = 3.68514\n",
      "epoch no.1 train no.122700  loss = 4.82205 avg_loss = 3.64281\n",
      "epoch no.1 train no.122710  loss = 2.90435 avg_loss = 3.65824\n",
      "epoch no.1 train no.122720  loss = 3.87546 avg_loss = 3.65926\n",
      "epoch no.1 train no.122730  loss = 5.10696 avg_loss = 3.62575\n",
      "epoch no.1 train no.122740  loss = 3.43887 avg_loss = 3.63417\n",
      "epoch no.1 train no.122750  loss = 5.27516 avg_loss = 3.69279\n",
      "epoch no.1 train no.122760  loss = 2.66781 avg_loss = 3.68736\n",
      "epoch no.1 train no.122770  loss = 2.51356 avg_loss = 3.68198\n",
      "epoch no.1 train no.122780  loss = 2.96662 avg_loss = 3.66204\n",
      "epoch no.1 train no.122790  loss = 2.39734 avg_loss = 3.68050\n",
      "epoch no.1 train no.122800  loss = 5.03166 avg_loss = 3.69053\n",
      "epoch no.1 train no.122810  loss = 2.84845 avg_loss = 3.64441\n",
      "epoch no.1 train no.122820  loss = 2.54569 avg_loss = 3.61892\n",
      "epoch no.1 train no.122830  loss = 5.30996 avg_loss = 3.59989\n",
      "epoch no.1 train no.122840  loss = 4.08363 avg_loss = 3.63365\n",
      "epoch no.1 train no.122850  loss = 3.66173 avg_loss = 3.64719\n",
      "epoch no.1 train no.122860  loss = 2.19854 avg_loss = 3.60892\n",
      "epoch no.1 train no.122870  loss = 5.87687 avg_loss = 3.64297\n",
      "epoch no.1 train no.122880  loss = 2.73721 avg_loss = 3.62170\n",
      "epoch no.1 train no.122890  loss = 3.35212 avg_loss = 3.71893\n",
      "epoch no.1 train no.122900  loss = 2.10465 avg_loss = 3.69829\n",
      "epoch no.1 train no.122910  loss = 2.09831 avg_loss = 3.72907\n",
      "epoch no.1 train no.122920  loss = 2.85171 avg_loss = 3.69676\n",
      "epoch no.1 train no.122930  loss = 4.41009 avg_loss = 3.73382\n",
      "epoch no.1 train no.122940  loss = 4.08791 avg_loss = 3.69950\n",
      "epoch no.1 train no.122950  loss = 3.96457 avg_loss = 3.68916\n",
      "epoch no.1 train no.122960  loss = 5.60281 avg_loss = 3.72370\n",
      "epoch no.1 train no.122970  loss = 4.79270 avg_loss = 3.76747\n",
      "epoch no.1 train no.122980  loss = 4.58532 avg_loss = 3.80167\n",
      "epoch no.1 train no.122990  loss = 4.98960 avg_loss = 3.79167\n",
      "epoch no.1 train no.123000  loss = 3.61210 avg_loss = 3.75789\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁좋은', '▁음악', '</s>']\n",
      "기분전환 하기 좋은 곡</s>\n",
      "epoch no.1 train no.123010  loss = 5.82571 avg_loss = 3.85121\n",
      "epoch no.1 train no.123020  loss = 4.33265 avg_loss = 3.87956\n",
      "epoch no.1 train no.123030  loss = 5.36362 avg_loss = 3.88373\n",
      "epoch no.1 train no.123040  loss = 6.46558 avg_loss = 3.90920\n",
      "epoch no.1 train no.123050  loss = 3.32375 avg_loss = 3.85190\n",
      "epoch no.1 train no.123060  loss = 4.24939 avg_loss = 3.82974\n",
      "epoch no.1 train no.123070  loss = 3.06563 avg_loss = 3.79470\n",
      "epoch no.1 train no.123080  loss = 4.09329 avg_loss = 3.80571\n",
      "epoch no.1 train no.123090  loss = 4.59401 avg_loss = 3.81001\n",
      "epoch no.1 train no.123100  loss = 3.28805 avg_loss = 3.83048\n",
      "epoch no.1 train no.123110  loss = 2.71546 avg_loss = 3.76704\n",
      "epoch no.1 train no.123120  loss = 2.65075 avg_loss = 3.75714\n",
      "epoch no.1 train no.123130  loss = 3.12337 avg_loss = 3.70221\n",
      "epoch no.1 train no.123140  loss = 2.46342 avg_loss = 3.66706\n",
      "epoch no.1 train no.123150  loss = 3.32820 avg_loss = 3.72974\n",
      "epoch no.1 train no.123160  loss = 3.31979 avg_loss = 3.70273\n",
      "epoch no.1 train no.123170  loss = 2.78159 avg_loss = 3.65788\n",
      "epoch no.1 train no.123180  loss = 5.37312 avg_loss = 3.69894\n",
      "epoch no.1 train no.123190  loss = 1.82198 avg_loss = 3.65605\n",
      "epoch no.1 train no.123200  loss = 2.96405 avg_loss = 3.63307\n",
      "epoch no.1 train no.123210  loss = 5.95943 avg_loss = 3.64968\n",
      "epoch no.1 train no.123220  loss = 4.43871 avg_loss = 3.60295\n",
      "epoch no.1 train no.123230  loss = 6.39771 avg_loss = 3.68925\n",
      "epoch no.1 train no.123240  loss = 4.27955 avg_loss = 3.70348\n",
      "epoch no.1 train no.123250  loss = 4.18519 avg_loss = 3.69858\n",
      "epoch no.1 train no.123260  loss = 2.59320 avg_loss = 3.70751\n",
      "epoch no.1 train no.123270  loss = 2.34865 avg_loss = 3.68033\n",
      "epoch no.1 train no.123280  loss = 4.33281 avg_loss = 3.66708\n",
      "epoch no.1 train no.123290  loss = 4.02502 avg_loss = 3.73274\n",
      "epoch no.1 train no.123300  loss = 4.22622 avg_loss = 3.75522\n",
      "epoch no.1 train no.123310  loss = 2.98638 avg_loss = 3.73709\n",
      "epoch no.1 train no.123320  loss = 2.03901 avg_loss = 3.69965\n",
      "epoch no.1 train no.123330  loss = 1.80780 avg_loss = 3.72561\n",
      "epoch no.1 train no.123340  loss = 3.23909 avg_loss = 3.78510\n",
      "epoch no.1 train no.123350  loss = 4.21973 avg_loss = 3.79904\n",
      "epoch no.1 train no.123360  loss = 3.03517 avg_loss = 3.78000\n",
      "epoch no.1 train no.123370  loss = 3.66611 avg_loss = 3.76297\n",
      "epoch no.1 train no.123380  loss = 3.78947 avg_loss = 3.77517\n",
      "epoch no.1 train no.123390  loss = 1.87460 avg_loss = 3.74311\n",
      "epoch no.1 train no.123400  loss = 2.75815 avg_loss = 3.68940\n",
      "epoch no.1 train no.123410  loss = 2.96603 avg_loss = 3.68865\n",
      "epoch no.1 train no.123420  loss = 4.61178 avg_loss = 3.70419\n",
      "epoch no.1 train no.123430  loss = 2.79169 avg_loss = 3.69711\n",
      "epoch no.1 train no.123440  loss = 2.15271 avg_loss = 3.73964\n",
      "epoch no.1 train no.123450  loss = 2.27884 avg_loss = 3.70482\n",
      "epoch no.1 train no.123460  loss = 2.06401 avg_loss = 3.77000\n",
      "epoch no.1 train no.123470  loss = 4.61378 avg_loss = 3.76297\n",
      "epoch no.1 train no.123480  loss = 3.06846 avg_loss = 3.75657\n",
      "epoch no.1 train no.123490  loss = 4.20204 avg_loss = 3.74366\n",
      "epoch no.1 train no.123500  loss = 2.80562 avg_loss = 3.68282\n",
      "epoch no.1 train no.123510  loss = 3.61452 avg_loss = 3.74120\n",
      "epoch no.1 train no.123520  loss = 2.56105 avg_loss = 3.71391\n",
      "epoch no.1 train no.123530  loss = 3.19171 avg_loss = 3.72352\n",
      "epoch no.1 train no.123540  loss = 4.89239 avg_loss = 3.78134\n",
      "epoch no.1 train no.123550  loss = 6.88876 avg_loss = 3.79324\n",
      "epoch no.1 train no.123560  loss = 3.28250 avg_loss = 3.77591\n",
      "epoch no.1 train no.123570  loss = 3.05225 avg_loss = 3.79491\n",
      "epoch no.1 train no.123580  loss = 4.20971 avg_loss = 3.80184\n",
      "epoch no.1 train no.123590  loss = 4.43916 avg_loss = 3.77121\n",
      "epoch no.1 train no.123600  loss = 3.69753 avg_loss = 3.76394\n",
      "epoch no.1 train no.123610  loss = 3.61320 avg_loss = 3.80273\n",
      "epoch no.1 train no.123620  loss = 2.77031 avg_loss = 3.77314\n",
      "epoch no.1 train no.123630  loss = 2.61908 avg_loss = 3.73780\n",
      "epoch no.1 train no.123640  loss = 3.81897 avg_loss = 3.73602\n",
      "epoch no.1 train no.123650  loss = 2.92915 avg_loss = 3.75471\n",
      "epoch no.1 train no.123660  loss = 5.58281 avg_loss = 3.74120\n",
      "epoch no.1 train no.123670  loss = 2.41774 avg_loss = 3.77995\n",
      "epoch no.1 train no.123680  loss = 5.19778 avg_loss = 3.77727\n",
      "epoch no.1 train no.123690  loss = 2.42982 avg_loss = 3.74392\n",
      "epoch no.1 train no.123700  loss = 4.31296 avg_loss = 3.76864\n",
      "epoch no.1 train no.123710  loss = 3.43014 avg_loss = 3.77505\n",
      "epoch no.1 train no.123720  loss = 3.54597 avg_loss = 3.81235\n",
      "epoch no.1 train no.123730  loss = 3.37023 avg_loss = 3.77636\n",
      "epoch no.1 train no.123740  loss = 2.59368 avg_loss = 3.73201\n",
      "epoch no.1 train no.123750  loss = 5.69010 avg_loss = 3.76801\n",
      "epoch no.1 train no.123760  loss = 3.10853 avg_loss = 3.72075\n",
      "epoch no.1 train no.123770  loss = 2.71739 avg_loss = 3.77156\n",
      "epoch no.1 train no.123780  loss = 4.68799 avg_loss = 3.74081\n",
      "epoch no.1 train no.123790  loss = 2.94588 avg_loss = 3.76423\n",
      "epoch no.1 train no.123800  loss = 2.74291 avg_loss = 3.70793\n",
      "epoch no.1 train no.123810  loss = 3.91884 avg_loss = 3.71084\n",
      "epoch no.1 train no.123820  loss = 4.76314 avg_loss = 3.74527\n",
      "epoch no.1 train no.123830  loss = 2.92329 avg_loss = 3.78402\n",
      "epoch no.1 train no.123840  loss = 5.55259 avg_loss = 3.78026\n",
      "epoch no.1 train no.123850  loss = 3.07052 avg_loss = 3.79376\n",
      "epoch no.1 train no.123860  loss = 2.55783 avg_loss = 3.74158\n",
      "epoch no.1 train no.123870  loss = 3.33414 avg_loss = 3.75869\n",
      "epoch no.1 train no.123880  loss = 3.58226 avg_loss = 3.79913\n",
      "epoch no.1 train no.123890  loss = 2.65398 avg_loss = 3.78980\n",
      "epoch no.1 train no.123900  loss = 4.88204 avg_loss = 3.80504\n",
      "epoch no.1 train no.123910  loss = 4.52954 avg_loss = 3.78747\n",
      "epoch no.1 train no.123920  loss = 3.49502 avg_loss = 3.80174\n",
      "epoch no.1 train no.123930  loss = 6.34537 avg_loss = 3.81377\n",
      "epoch no.1 train no.123940  loss = 4.46981 avg_loss = 3.80045\n",
      "epoch no.1 train no.123950  loss = 3.11194 avg_loss = 3.79325\n",
      "epoch no.1 train no.123960  loss = 6.02878 avg_loss = 3.86849\n",
      "epoch no.1 train no.123970  loss = 4.40721 avg_loss = 3.86014\n",
      "epoch no.1 train no.123980  loss = 3.09765 avg_loss = 3.86456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.123990  loss = 5.28823 avg_loss = 3.90706\n",
      "epoch no.1 train no.124000  loss = 3.12955 avg_loss = 3.91071\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '▁팝', '</s>']\n",
      "기분전환을 위한 신나는 음악</s>\n",
      "epoch no.1 train no.124010  loss = 3.43334 avg_loss = 3.89775\n",
      "epoch no.1 train no.124020  loss = 3.62847 avg_loss = 3.83267\n",
      "epoch no.1 train no.124030  loss = 6.47289 avg_loss = 3.85327\n",
      "epoch no.1 train no.124040  loss = 2.35575 avg_loss = 3.85596\n",
      "epoch no.1 train no.124050  loss = 3.05876 avg_loss = 3.82469\n",
      "epoch no.1 train no.124060  loss = 2.87386 avg_loss = 3.80101\n",
      "epoch no.1 train no.124070  loss = 2.78149 avg_loss = 3.82865\n",
      "epoch no.1 train no.124080  loss = 3.38751 avg_loss = 3.77379\n",
      "epoch no.1 train no.124090  loss = 5.22287 avg_loss = 3.78303\n",
      "epoch no.1 train no.124100  loss = 2.98942 avg_loss = 3.77805\n",
      "epoch no.1 train no.124110  loss = 2.55640 avg_loss = 3.74793\n",
      "epoch no.1 train no.124120  loss = 5.54368 avg_loss = 3.77582\n",
      "epoch no.1 train no.124130  loss = 4.10238 avg_loss = 3.78997\n",
      "epoch no.1 train no.124140  loss = 5.70322 avg_loss = 3.80337\n",
      "epoch no.1 train no.124150  loss = 2.59067 avg_loss = 3.72291\n",
      "epoch no.1 train no.124160  loss = 2.67731 avg_loss = 3.64996\n",
      "epoch no.1 train no.124170  loss = 4.78701 avg_loss = 3.63187\n",
      "epoch no.1 train no.124180  loss = 3.84318 avg_loss = 3.56214\n",
      "epoch no.1 train no.124190  loss = 5.80484 avg_loss = 3.68367\n",
      "epoch no.1 train no.124200  loss = 2.34814 avg_loss = 3.66294\n",
      "epoch no.1 train no.124210  loss = 5.29563 avg_loss = 3.63731\n",
      "epoch no.1 train no.124220  loss = 4.68933 avg_loss = 3.64202\n",
      "epoch no.1 train no.124230  loss = 3.77237 avg_loss = 3.69004\n",
      "epoch no.1 train no.124240  loss = 3.92050 avg_loss = 3.74770\n",
      "epoch no.1 train no.124250  loss = 7.50921 avg_loss = 3.74327\n",
      "epoch no.1 train no.124260  loss = 2.26624 avg_loss = 3.66291\n",
      "epoch no.1 train no.124270  loss = 5.20757 avg_loss = 3.69396\n",
      "epoch no.1 train no.124280  loss = 3.07004 avg_loss = 3.68684\n",
      "epoch no.1 train no.124290  loss = 3.68406 avg_loss = 3.74621\n",
      "epoch no.1 train no.124300  loss = 4.44582 avg_loss = 3.80838\n",
      "epoch no.1 train no.124310  loss = 2.64062 avg_loss = 3.80679\n",
      "epoch no.1 train no.124320  loss = 4.37865 avg_loss = 3.81573\n",
      "epoch no.1 train no.124330  loss = 3.75081 avg_loss = 3.83839\n",
      "epoch no.1 train no.124340  loss = 3.97537 avg_loss = 3.80489\n",
      "epoch no.1 train no.124350  loss = 3.47889 avg_loss = 3.79449\n",
      "epoch no.1 train no.124360  loss = 2.63638 avg_loss = 3.69245\n",
      "epoch no.1 train no.124370  loss = 3.15701 avg_loss = 3.67087\n",
      "epoch no.1 train no.124380  loss = 2.68602 avg_loss = 3.60666\n",
      "epoch no.1 train no.124390  loss = 4.76187 avg_loss = 3.63462\n",
      "epoch no.1 train no.124400  loss = 3.39889 avg_loss = 3.55990\n",
      "epoch no.1 train no.124410  loss = 3.53256 avg_loss = 3.60420\n",
      "epoch no.1 train no.124420  loss = 3.50119 avg_loss = 3.64041\n",
      "epoch no.1 train no.124430  loss = 3.55292 avg_loss = 3.64449\n",
      "epoch no.1 train no.124440  loss = 2.62296 avg_loss = 3.67285\n",
      "epoch no.1 train no.124450  loss = 4.97762 avg_loss = 3.70891\n",
      "epoch no.1 train no.124460  loss = 5.25319 avg_loss = 3.78196\n",
      "epoch no.1 train no.124470  loss = 3.63207 avg_loss = 3.71467\n",
      "epoch no.1 train no.124480  loss = 2.68465 avg_loss = 3.73099\n",
      "epoch no.1 train no.124490  loss = 4.32469 avg_loss = 3.77255\n",
      "epoch no.1 train no.124500  loss = 5.20216 avg_loss = 3.81112\n",
      "epoch no.1 train no.124510  loss = 4.11893 avg_loss = 3.77901\n",
      "epoch no.1 train no.124520  loss = 4.22041 avg_loss = 3.84024\n",
      "epoch no.1 train no.124530  loss = 3.33026 avg_loss = 3.85942\n",
      "epoch no.1 train no.124540  loss = 2.84777 avg_loss = 3.83458\n",
      "epoch no.1 train no.124550  loss = 5.25459 avg_loss = 3.83521\n",
      "epoch no.1 train no.124560  loss = 3.05247 avg_loss = 3.79346\n",
      "epoch no.1 train no.124570  loss = 2.29791 avg_loss = 3.73466\n",
      "epoch no.1 train no.124580  loss = 3.17123 avg_loss = 3.74329\n",
      "epoch no.1 train no.124590  loss = 3.64825 avg_loss = 3.74064\n",
      "epoch no.1 train no.124600  loss = 4.06311 avg_loss = 3.76347\n",
      "epoch no.1 train no.124610  loss = 3.78327 avg_loss = 3.72417\n",
      "epoch no.1 train no.124620  loss = 2.13853 avg_loss = 3.74826\n",
      "epoch no.1 train no.124630  loss = 3.07429 avg_loss = 3.74644\n",
      "epoch no.1 train no.124640  loss = 4.22997 avg_loss = 3.72381\n",
      "epoch no.1 train no.124650  loss = 3.53631 avg_loss = 3.74080\n",
      "epoch no.1 train no.124660  loss = 4.29469 avg_loss = 3.78186\n",
      "epoch no.1 train no.124670  loss = 3.86229 avg_loss = 3.77324\n",
      "epoch no.1 train no.124680  loss = 3.94102 avg_loss = 3.79350\n",
      "epoch no.1 train no.124690  loss = 2.85384 avg_loss = 3.76447\n",
      "epoch no.1 train no.124700  loss = 4.08423 avg_loss = 3.81190\n",
      "epoch no.1 train no.124710  loss = 3.23477 avg_loss = 3.81246\n",
      "epoch no.1 train no.124720  loss = 4.35750 avg_loss = 3.76286\n",
      "epoch no.1 train no.124730  loss = 2.88344 avg_loss = 3.76334\n",
      "epoch no.1 train no.124740  loss = 4.83694 avg_loss = 3.78829\n",
      "epoch no.1 train no.124750  loss = 3.27330 avg_loss = 3.77660\n",
      "epoch no.1 train no.124760  loss = 2.98923 avg_loss = 3.75407\n",
      "epoch no.1 train no.124770  loss = 5.24271 avg_loss = 3.80705\n",
      "epoch no.1 train no.124780  loss = 3.11357 avg_loss = 3.77744\n",
      "epoch no.1 train no.124790  loss = 3.73061 avg_loss = 3.80107\n",
      "epoch no.1 train no.124800  loss = 1.66077 avg_loss = 3.79012\n",
      "epoch no.1 train no.124810  loss = 6.07967 avg_loss = 3.80603\n",
      "epoch no.1 train no.124820  loss = 5.23537 avg_loss = 3.78281\n",
      "epoch no.1 train no.124830  loss = 4.86465 avg_loss = 3.80770\n",
      "epoch no.1 train no.124840  loss = 3.56290 avg_loss = 3.85518\n",
      "epoch no.1 train no.124850  loss = 3.40702 avg_loss = 3.87828\n",
      "epoch no.1 train no.124860  loss = 4.95221 avg_loss = 3.90607\n",
      "epoch no.1 train no.124870  loss = 3.52024 avg_loss = 3.88540\n",
      "epoch no.1 train no.124880  loss = 5.63609 avg_loss = 3.87402\n",
      "epoch no.1 train no.124890  loss = 5.20657 avg_loss = 3.91289\n",
      "epoch no.1 train no.124900  loss = 5.73050 avg_loss = 3.91759\n",
      "epoch no.1 train no.124910  loss = 3.56868 avg_loss = 3.87775\n",
      "epoch no.1 train no.124920  loss = 4.64059 avg_loss = 3.85597\n",
      "epoch no.1 train no.124930  loss = 3.10074 avg_loss = 3.80895\n",
      "epoch no.1 train no.124940  loss = 3.17896 avg_loss = 3.79327\n",
      "epoch no.1 train no.124950  loss = 5.98925 avg_loss = 3.84880\n",
      "epoch no.1 train no.124960  loss = 3.39060 avg_loss = 3.82625\n",
      "epoch no.1 train no.124970  loss = 3.56939 avg_loss = 3.77930\n",
      "epoch no.1 train no.124980  loss = 2.66710 avg_loss = 3.76419\n",
      "epoch no.1 train no.124990  loss = 3.07041 avg_loss = 3.77596\n",
      "epoch no.1 train no.125000  loss = 5.74995 avg_loss = 3.83234\n",
      "5\n",
      "to_tokens: ['▁가을', '좋은', '에', '▁좋은', '▁노래', '▁음악', '송', '</s>']\n",
      "기분전환에 좋은 신나는 팝송</s>\n",
      "epoch no.1 train no.125010  loss = 3.18404 avg_loss = 3.80592\n",
      "epoch no.1 train no.125020  loss = 3.63641 avg_loss = 3.80954\n",
      "epoch no.1 train no.125030  loss = 3.23132 avg_loss = 3.82388\n",
      "epoch no.1 train no.125040  loss = 3.40245 avg_loss = 3.80230\n",
      "epoch no.1 train no.125050  loss = 5.69576 avg_loss = 3.83143\n",
      "epoch no.1 train no.125060  loss = 4.24631 avg_loss = 3.80052\n",
      "epoch no.1 train no.125070  loss = 2.52824 avg_loss = 3.76772\n",
      "epoch no.1 train no.125080  loss = 4.55634 avg_loss = 3.81601\n",
      "epoch no.1 train no.125090  loss = 5.39130 avg_loss = 3.87599\n",
      "epoch no.1 train no.125100  loss = 6.52766 avg_loss = 3.90706\n",
      "epoch no.1 train no.125110  loss = 2.49344 avg_loss = 3.87319\n",
      "epoch no.1 train no.125120  loss = 4.59107 avg_loss = 3.89687\n",
      "epoch no.1 train no.125130  loss = 4.94370 avg_loss = 3.87404\n",
      "epoch no.1 train no.125140  loss = 3.05530 avg_loss = 3.85383\n",
      "epoch no.1 train no.125150  loss = 4.29723 avg_loss = 3.91285\n",
      "epoch no.1 train no.125160  loss = 3.33934 avg_loss = 3.90795\n",
      "epoch no.1 train no.125170  loss = 3.88970 avg_loss = 3.84734\n",
      "epoch no.1 train no.125180  loss = 3.51568 avg_loss = 3.88014\n",
      "epoch no.1 train no.125190  loss = 3.31122 avg_loss = 3.93997\n",
      "epoch no.1 train no.125200  loss = 2.74895 avg_loss = 3.87615\n",
      "epoch no.1 train no.125210  loss = 3.83144 avg_loss = 3.87772\n",
      "epoch no.1 train no.125220  loss = 3.39999 avg_loss = 3.91710\n",
      "epoch no.1 train no.125230  loss = 3.81155 avg_loss = 3.94093\n",
      "epoch no.1 train no.125240  loss = 3.18961 avg_loss = 3.90961\n",
      "epoch no.1 train no.125250  loss = 4.13976 avg_loss = 3.87019\n",
      "epoch no.1 train no.125260  loss = 5.27590 avg_loss = 3.86175\n",
      "epoch no.1 train no.125270  loss = 3.14723 avg_loss = 3.78154\n",
      "epoch no.1 train no.125280  loss = 4.81230 avg_loss = 3.79334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.125290  loss = 3.58241 avg_loss = 3.79747\n",
      "epoch no.1 train no.125300  loss = 3.47486 avg_loss = 3.80269\n",
      "epoch no.1 train no.125310  loss = 5.10630 avg_loss = 3.81597\n",
      "epoch no.1 train no.125320  loss = 3.15374 avg_loss = 3.77800\n",
      "epoch no.1 train no.125330  loss = 2.88956 avg_loss = 3.74976\n",
      "epoch no.1 train no.125340  loss = 2.99006 avg_loss = 3.75964\n",
      "epoch no.1 train no.125350  loss = 6.74420 avg_loss = 3.78746\n",
      "epoch no.1 train no.125360  loss = 5.22895 avg_loss = 3.75903\n",
      "epoch no.1 train no.125370  loss = 2.89282 avg_loss = 3.73559\n",
      "epoch no.1 train no.125380  loss = 4.31190 avg_loss = 3.75181\n",
      "epoch no.1 train no.125390  loss = 3.11557 avg_loss = 3.74861\n",
      "epoch no.1 train no.125400  loss = 2.08812 avg_loss = 3.75230\n",
      "epoch no.1 train no.125410  loss = 4.38454 avg_loss = 3.73854\n",
      "epoch no.1 train no.125420  loss = 3.67049 avg_loss = 3.66426\n",
      "epoch no.1 train no.125430  loss = 3.75805 avg_loss = 3.68988\n",
      "epoch no.1 train no.125440  loss = 2.26268 avg_loss = 3.65463\n",
      "epoch no.1 train no.125450  loss = 4.19381 avg_loss = 3.65756\n",
      "epoch no.1 train no.125460  loss = 2.22202 avg_loss = 3.64136\n",
      "epoch no.1 train no.125470  loss = 2.42500 avg_loss = 3.59669\n",
      "epoch no.1 train no.125480  loss = 3.73656 avg_loss = 3.58563\n",
      "epoch no.1 train no.125490  loss = 2.38520 avg_loss = 3.53519\n",
      "epoch no.1 train no.125500  loss = 3.83417 avg_loss = 3.57022\n",
      "epoch no.1 train no.125510  loss = 3.27020 avg_loss = 3.56304\n",
      "epoch no.1 train no.125520  loss = 5.04559 avg_loss = 3.59237\n",
      "epoch no.1 train no.125530  loss = 3.00681 avg_loss = 3.56719\n",
      "epoch no.1 train no.125540  loss = 4.86657 avg_loss = 3.67054\n",
      "epoch no.1 train no.125550  loss = 5.20891 avg_loss = 3.71705\n",
      "epoch no.1 train no.125560  loss = 5.61534 avg_loss = 3.78212\n",
      "epoch no.1 train no.125570  loss = 5.03429 avg_loss = 3.83401\n",
      "epoch no.1 train no.125580  loss = 2.42660 avg_loss = 3.80132\n",
      "epoch no.1 train no.125590  loss = 2.81633 avg_loss = 3.79236\n",
      "epoch no.1 train no.125600  loss = 3.90024 avg_loss = 3.79672\n",
      "epoch no.1 train no.125610  loss = 5.81886 avg_loss = 3.76597\n",
      "epoch no.1 train no.125620  loss = 6.23479 avg_loss = 3.78004\n",
      "epoch no.1 train no.125630  loss = 5.64802 avg_loss = 3.82309\n",
      "epoch no.1 train no.125640  loss = 4.18423 avg_loss = 3.81846\n",
      "epoch no.1 train no.125650  loss = 5.96283 avg_loss = 3.82803\n",
      "epoch no.1 train no.125660  loss = 4.84482 avg_loss = 3.82446\n",
      "epoch no.1 train no.125670  loss = 4.50868 avg_loss = 3.77922\n",
      "epoch no.1 train no.125680  loss = 3.67404 avg_loss = 3.76978\n",
      "epoch no.1 train no.125690  loss = 5.92602 avg_loss = 3.80643\n",
      "epoch no.1 train no.125700  loss = 3.49725 avg_loss = 3.76774\n",
      "epoch no.1 train no.125710  loss = 4.28137 avg_loss = 3.75617\n",
      "epoch no.1 train no.125720  loss = 3.34498 avg_loss = 3.73679\n",
      "epoch no.1 train no.125730  loss = 3.33857 avg_loss = 3.69267\n",
      "epoch no.1 train no.125740  loss = 3.79181 avg_loss = 3.66746\n",
      "epoch no.1 train no.125750  loss = 3.46176 avg_loss = 3.67672\n",
      "epoch no.1 train no.125760  loss = 3.72231 avg_loss = 3.67898\n",
      "epoch no.1 train no.125770  loss = 2.90503 avg_loss = 3.64499\n",
      "epoch no.1 train no.125780  loss = 4.72863 avg_loss = 3.66996\n",
      "epoch no.1 train no.125790  loss = 3.59142 avg_loss = 3.64434\n",
      "epoch no.1 train no.125800  loss = 4.80954 avg_loss = 3.65302\n",
      "epoch no.1 train no.125810  loss = 4.62302 avg_loss = 3.72930\n",
      "epoch no.1 train no.125820  loss = 2.89235 avg_loss = 3.68956\n",
      "epoch no.1 train no.125830  loss = 3.98858 avg_loss = 3.75413\n",
      "epoch no.1 train no.125840  loss = 4.85318 avg_loss = 3.78298\n",
      "epoch no.1 train no.125850  loss = 3.11855 avg_loss = 3.74165\n",
      "epoch no.1 train no.125860  loss = 6.09523 avg_loss = 3.79823\n",
      "epoch no.1 train no.125870  loss = 5.56761 avg_loss = 3.84884\n",
      "epoch no.1 train no.125880  loss = 3.94792 avg_loss = 3.78045\n",
      "epoch no.1 train no.125890  loss = 5.32162 avg_loss = 3.81715\n",
      "epoch no.1 train no.125900  loss = 7.72192 avg_loss = 3.89158\n",
      "epoch no.1 train no.125910  loss = 2.32200 avg_loss = 3.94839\n",
      "epoch no.1 train no.125920  loss = 2.66128 avg_loss = 3.88688\n",
      "epoch no.1 train no.125930  loss = 1.93549 avg_loss = 3.85527\n",
      "epoch no.1 train no.125940  loss = 4.37626 avg_loss = 3.82257\n",
      "epoch no.1 train no.125950  loss = 2.59219 avg_loss = 3.73940\n",
      "epoch no.1 train no.125960  loss = 2.66737 avg_loss = 3.71534\n",
      "epoch no.1 train no.125970  loss = 2.97765 avg_loss = 3.70522\n",
      "epoch no.1 train no.125980  loss = 4.10260 avg_loss = 3.73589\n",
      "epoch no.1 train no.125990  loss = 5.56445 avg_loss = 3.76558\n",
      "epoch no.1 train no.126000  loss = 3.37283 avg_loss = 3.80942\n",
      "5\n",
      "to_tokens: ['▁가을', '전환', '이', '▁필요할', '▁때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.1 train no.126010  loss = 3.37225 avg_loss = 3.81286\n",
      "epoch no.1 train no.126020  loss = 2.81115 avg_loss = 3.78186\n",
      "epoch no.1 train no.126030  loss = 2.17335 avg_loss = 3.72551\n",
      "epoch no.1 train no.126040  loss = 2.60133 avg_loss = 3.77958\n",
      "epoch no.1 train no.126050  loss = 2.28356 avg_loss = 3.78615\n",
      "epoch no.1 train no.126060  loss = 3.73873 avg_loss = 3.81104\n",
      "epoch no.1 train no.126070  loss = 3.32137 avg_loss = 3.84647\n",
      "epoch no.1 train no.126080  loss = 4.93741 avg_loss = 3.87084\n",
      "epoch no.1 train no.126090  loss = 5.30241 avg_loss = 3.86183\n",
      "epoch no.1 train no.126100  loss = 3.20746 avg_loss = 3.84647\n",
      "epoch no.1 train no.126110  loss = 4.65475 avg_loss = 3.80171\n",
      "epoch no.1 train no.126120  loss = 3.38910 avg_loss = 3.84720\n",
      "epoch no.1 train no.126130  loss = 2.09062 avg_loss = 3.80293\n",
      "epoch no.1 train no.126140  loss = 5.47971 avg_loss = 3.77380\n",
      "epoch no.1 train no.126150  loss = 3.95647 avg_loss = 3.76959\n",
      "epoch no.1 train no.126160  loss = 2.54492 avg_loss = 3.81480\n",
      "epoch no.1 train no.126170  loss = 2.75082 avg_loss = 3.76177\n",
      "epoch no.1 train no.126180  loss = 5.00752 avg_loss = 3.79658\n",
      "epoch no.1 train no.126190  loss = 3.60563 avg_loss = 3.78660\n",
      "epoch no.1 train no.126200  loss = 2.78418 avg_loss = 3.74981\n",
      "epoch no.1 train no.126210  loss = 3.96959 avg_loss = 3.71267\n",
      "epoch no.1 train no.126220  loss = 4.61518 avg_loss = 3.74110\n",
      "epoch no.1 train no.126230  loss = 3.45342 avg_loss = 3.71316\n",
      "epoch no.1 train no.126240  loss = 4.73067 avg_loss = 3.70970\n",
      "epoch no.1 train no.126250  loss = 4.75738 avg_loss = 3.66792\n",
      "epoch no.1 train no.126260  loss = 3.33270 avg_loss = 3.69231\n",
      "epoch no.1 train no.126270  loss = 4.14760 avg_loss = 3.72208\n",
      "epoch no.1 train no.126280  loss = 3.26432 avg_loss = 3.65031\n",
      "epoch no.1 train no.126290  loss = 3.40966 avg_loss = 3.62083\n",
      "epoch no.1 train no.126300  loss = 5.51448 avg_loss = 3.63665\n",
      "epoch no.1 train no.126310  loss = 3.41320 avg_loss = 3.62898\n",
      "epoch no.1 train no.126320  loss = 2.21327 avg_loss = 3.65280\n",
      "epoch no.1 train no.126330  loss = 1.76849 avg_loss = 3.63372\n",
      "epoch no.1 train no.126340  loss = 5.48312 avg_loss = 3.65041\n",
      "epoch no.1 train no.126350  loss = 5.07821 avg_loss = 3.70875\n",
      "epoch no.1 train no.126360  loss = 3.02062 avg_loss = 3.70920\n",
      "epoch no.1 train no.126370  loss = 4.34062 avg_loss = 3.71491\n",
      "epoch no.1 train no.126380  loss = 3.50169 avg_loss = 3.75789\n",
      "epoch no.1 train no.126390  loss = 3.52745 avg_loss = 3.78832\n",
      "epoch no.1 train no.126400  loss = 2.96773 avg_loss = 3.82085\n",
      "epoch no.1 train no.126410  loss = 3.92117 avg_loss = 3.83848\n",
      "epoch no.1 train no.126420  loss = 3.71977 avg_loss = 3.81334\n",
      "epoch no.1 train no.126430  loss = 3.08240 avg_loss = 3.76908\n",
      "epoch no.1 train no.126440  loss = 5.32317 avg_loss = 3.75353\n",
      "epoch no.1 train no.126450  loss = 4.11831 avg_loss = 3.76726\n",
      "epoch no.1 train no.126460  loss = 5.66151 avg_loss = 3.77899\n",
      "epoch no.1 train no.126470  loss = 4.20710 avg_loss = 3.74975\n",
      "epoch no.1 train no.126480  loss = 2.06667 avg_loss = 3.67396\n",
      "epoch no.1 train no.126490  loss = 2.84974 avg_loss = 3.71494\n",
      "epoch no.1 train no.126500  loss = 3.63647 avg_loss = 3.74455\n",
      "epoch no.1 train no.126510  loss = 4.69111 avg_loss = 3.75500\n",
      "epoch no.1 train no.126520  loss = 3.69394 avg_loss = 3.78557\n",
      "epoch no.1 train no.126530  loss = 4.22686 avg_loss = 3.82672\n",
      "epoch no.1 train no.126540  loss = 4.63340 avg_loss = 3.84137\n",
      "epoch no.1 train no.126550  loss = 3.15102 avg_loss = 3.84637\n",
      "epoch no.1 train no.126560  loss = 2.92303 avg_loss = 3.87907\n",
      "epoch no.1 train no.126570  loss = 5.05993 avg_loss = 3.84985\n",
      "epoch no.1 train no.126580  loss = 2.84155 avg_loss = 3.78477\n",
      "epoch no.1 train no.126590  loss = 2.75173 avg_loss = 3.77004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.126600  loss = 3.01970 avg_loss = 3.74633\n",
      "epoch no.1 train no.126610  loss = 3.03954 avg_loss = 3.73702\n",
      "epoch no.1 train no.126620  loss = 3.22561 avg_loss = 3.75111\n",
      "epoch no.1 train no.126630  loss = 2.38879 avg_loss = 3.74175\n",
      "epoch no.1 train no.126640  loss = 3.57596 avg_loss = 3.75431\n",
      "epoch no.1 train no.126650  loss = 3.30924 avg_loss = 3.70823\n",
      "epoch no.1 train no.126660  loss = 2.92588 avg_loss = 3.77182\n",
      "epoch no.1 train no.126670  loss = 3.40629 avg_loss = 3.72195\n",
      "epoch no.1 train no.126680  loss = 5.55232 avg_loss = 3.72808\n",
      "epoch no.1 train no.126690  loss = 4.21279 avg_loss = 3.70044\n",
      "epoch no.1 train no.126700  loss = 3.64840 avg_loss = 3.72008\n",
      "epoch no.1 train no.126710  loss = 3.75026 avg_loss = 3.71668\n",
      "epoch no.1 train no.126720  loss = 4.73788 avg_loss = 3.69116\n",
      "epoch no.1 train no.126730  loss = 2.47251 avg_loss = 3.72779\n",
      "epoch no.1 train no.126740  loss = 2.54262 avg_loss = 3.77164\n",
      "epoch no.1 train no.126750  loss = 3.86165 avg_loss = 3.82515\n",
      "epoch no.1 train no.126760  loss = 2.53874 avg_loss = 3.76500\n",
      "epoch no.1 train no.126770  loss = 5.41056 avg_loss = 3.81587\n",
      "epoch no.1 train no.126780  loss = 3.11243 avg_loss = 3.78054\n",
      "epoch no.1 train no.126790  loss = 5.66387 avg_loss = 3.78814\n",
      "epoch no.1 train no.126800  loss = 2.52291 avg_loss = 3.77810\n",
      "epoch no.1 train no.126810  loss = 4.02865 avg_loss = 3.80671\n",
      "epoch no.1 train no.126820  loss = 5.20105 avg_loss = 3.77520\n",
      "epoch no.1 train no.126830  loss = 3.46798 avg_loss = 3.80019\n",
      "epoch no.1 train no.126840  loss = 6.21727 avg_loss = 3.84214\n",
      "epoch no.1 train no.126850  loss = 5.92250 avg_loss = 3.87240\n",
      "epoch no.1 train no.126860  loss = 2.46066 avg_loss = 3.87373\n",
      "epoch no.1 train no.126870  loss = 2.92988 avg_loss = 3.82538\n",
      "epoch no.1 train no.126880  loss = 3.32620 avg_loss = 3.81771\n",
      "epoch no.1 train no.126890  loss = 3.11801 avg_loss = 3.81207\n",
      "epoch no.1 train no.126900  loss = 4.44381 avg_loss = 3.79383\n",
      "epoch no.1 train no.126910  loss = 3.31341 avg_loss = 3.77535\n",
      "epoch no.1 train no.126920  loss = 3.62591 avg_loss = 3.77972\n",
      "epoch no.1 train no.126930  loss = 4.13207 avg_loss = 3.74511\n",
      "epoch no.1 train no.126940  loss = 3.97585 avg_loss = 3.83175\n",
      "epoch no.1 train no.126950  loss = 1.97873 avg_loss = 3.77733\n",
      "epoch no.1 train no.126960  loss = 3.16370 avg_loss = 3.73945\n",
      "epoch no.1 train no.126970  loss = 2.97116 avg_loss = 3.80826\n",
      "epoch no.1 train no.126980  loss = 2.30499 avg_loss = 3.77991\n",
      "epoch no.1 train no.126990  loss = 3.66180 avg_loss = 3.81743\n",
      "epoch no.1 train no.127000  loss = 4.16791 avg_loss = 3.81946\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁팝', '▁팝', '</s>']\n",
      "기분전환용 신나는 노래</s>\n",
      "epoch no.1 train no.127010  loss = 5.80162 avg_loss = 3.81821\n",
      "epoch no.1 train no.127020  loss = 4.22109 avg_loss = 3.80925\n",
      "epoch no.1 train no.127030  loss = 3.46985 avg_loss = 3.76651\n",
      "epoch no.1 train no.127040  loss = 2.40063 avg_loss = 3.77801\n",
      "epoch no.1 train no.127050  loss = 3.46202 avg_loss = 3.82035\n",
      "epoch no.1 train no.127060  loss = 3.73276 avg_loss = 3.78494\n",
      "epoch no.1 train no.127070  loss = 5.67915 avg_loss = 3.81256\n",
      "epoch no.1 train no.127080  loss = 1.84302 avg_loss = 3.76312\n",
      "epoch no.1 train no.127090  loss = 3.18166 avg_loss = 3.72557\n",
      "epoch no.1 train no.127100  loss = 4.16733 avg_loss = 3.74562\n",
      "epoch no.1 train no.127110  loss = 5.36416 avg_loss = 3.81110\n",
      "epoch no.1 train no.127120  loss = 2.94462 avg_loss = 3.86654\n",
      "epoch no.1 train no.127130  loss = 3.47043 avg_loss = 3.84376\n",
      "epoch no.1 train no.127140  loss = 3.47003 avg_loss = 3.87421\n",
      "epoch no.1 train no.127150  loss = 1.84554 avg_loss = 3.79745\n",
      "epoch no.1 train no.127160  loss = 4.25689 avg_loss = 3.79532\n",
      "epoch no.1 train no.127170  loss = 3.75173 avg_loss = 3.76960\n",
      "epoch no.1 train no.127180  loss = 4.75633 avg_loss = 3.76425\n",
      "epoch no.1 train no.127190  loss = 4.63313 avg_loss = 3.85736\n",
      "epoch no.1 train no.127200  loss = 4.59347 avg_loss = 3.84588\n",
      "epoch no.1 train no.127210  loss = 3.06267 avg_loss = 3.88615\n",
      "epoch no.1 train no.127220  loss = 3.14685 avg_loss = 3.89871\n",
      "epoch no.1 train no.127230  loss = 4.95008 avg_loss = 3.86307\n",
      "epoch no.1 train no.127240  loss = 3.77257 avg_loss = 3.87126\n",
      "epoch no.1 train no.127250  loss = 5.43495 avg_loss = 3.95661\n",
      "epoch no.1 train no.127260  loss = 6.23270 avg_loss = 4.01786\n",
      "epoch no.1 train no.127270  loss = 3.01280 avg_loss = 3.92555\n",
      "epoch no.1 train no.127280  loss = 6.75154 avg_loss = 4.00688\n",
      "epoch no.1 train no.127290  loss = 3.40956 avg_loss = 3.97827\n",
      "epoch no.1 train no.127300  loss = 5.25326 avg_loss = 3.96012\n",
      "epoch no.1 train no.127310  loss = 4.21319 avg_loss = 3.89436\n",
      "epoch no.1 train no.127320  loss = 2.61907 avg_loss = 3.90402\n",
      "epoch no.1 train no.127330  loss = 4.09352 avg_loss = 3.92332\n",
      "epoch no.1 train no.127340  loss = 3.45365 avg_loss = 3.89636\n",
      "epoch no.1 train no.127350  loss = 2.44223 avg_loss = 3.85790\n",
      "epoch no.1 train no.127360  loss = 4.65421 avg_loss = 3.89430\n",
      "epoch no.1 train no.127370  loss = 4.09809 avg_loss = 3.87538\n",
      "epoch no.1 train no.127380  loss = 2.63822 avg_loss = 3.91114\n",
      "epoch no.1 train no.127390  loss = 3.27466 avg_loss = 3.91470\n",
      "epoch no.1 train no.127400  loss = 4.42531 avg_loss = 3.93956\n",
      "epoch no.1 train no.127410  loss = 3.41322 avg_loss = 3.86386\n",
      "epoch no.1 train no.127420  loss = 1.69662 avg_loss = 3.82113\n",
      "epoch no.1 train no.127430  loss = 4.81339 avg_loss = 3.82601\n",
      "epoch no.1 train no.127440  loss = 4.07563 avg_loss = 3.87402\n",
      "epoch no.1 train no.127450  loss = 3.28818 avg_loss = 3.87761\n",
      "epoch no.1 train no.127460  loss = 3.32372 avg_loss = 3.88937\n",
      "epoch no.1 train no.127470  loss = 4.85291 avg_loss = 3.88628\n",
      "epoch no.1 train no.127480  loss = 4.01296 avg_loss = 3.93658\n",
      "epoch no.1 train no.127490  loss = 4.25693 avg_loss = 3.90944\n",
      "epoch no.1 train no.127500  loss = 4.18692 avg_loss = 3.87882\n",
      "epoch no.1 train no.127510  loss = 2.31417 avg_loss = 3.86076\n",
      "epoch no.1 train no.127520  loss = 3.46016 avg_loss = 3.81416\n",
      "epoch no.1 train no.127530  loss = 1.60963 avg_loss = 3.80927\n",
      "epoch no.1 train no.127540  loss = 5.91799 avg_loss = 3.84377\n",
      "epoch no.1 train no.127550  loss = 2.91583 avg_loss = 3.84763\n",
      "epoch no.1 train no.127560  loss = 3.35811 avg_loss = 3.84713\n",
      "epoch no.1 train no.127570  loss = 2.83100 avg_loss = 3.80980\n",
      "epoch no.1 train no.127580  loss = 3.22050 avg_loss = 3.80141\n",
      "epoch no.1 train no.127590  loss = 4.11764 avg_loss = 3.81398\n",
      "epoch no.1 train no.127600  loss = 3.61187 avg_loss = 3.82937\n",
      "epoch no.1 train no.127610  loss = 3.94631 avg_loss = 3.80431\n",
      "epoch no.1 train no.127620  loss = 4.35475 avg_loss = 3.81772\n",
      "epoch no.1 train no.127630  loss = 3.40597 avg_loss = 3.80170\n",
      "epoch no.1 train no.127640  loss = 4.57970 avg_loss = 3.83384\n",
      "epoch no.1 train no.127650  loss = 1.01215 avg_loss = 3.78878\n",
      "epoch no.1 train no.127660  loss = 2.93987 avg_loss = 3.76720\n",
      "epoch no.1 train no.127670  loss = 3.05064 avg_loss = 3.77409\n",
      "epoch no.1 train no.127680  loss = 3.47348 avg_loss = 3.73745\n",
      "epoch no.1 train no.127690  loss = 2.53776 avg_loss = 3.76588\n",
      "epoch no.1 train no.127700  loss = 3.34206 avg_loss = 3.77235\n",
      "epoch no.1 train no.127710  loss = 2.96327 avg_loss = 3.75972\n",
      "epoch no.1 train no.127720  loss = 5.50100 avg_loss = 3.85310\n",
      "epoch no.1 train no.127730  loss = 3.12873 avg_loss = 3.83233\n",
      "epoch no.1 train no.127740  loss = 3.03774 avg_loss = 3.84466\n",
      "epoch no.1 train no.127750  loss = 3.76136 avg_loss = 3.79097\n",
      "epoch no.1 train no.127760  loss = 4.40829 avg_loss = 3.77522\n",
      "epoch no.1 train no.127770  loss = 3.79595 avg_loss = 3.79984\n",
      "epoch no.1 train no.127780  loss = 2.59102 avg_loss = 3.81260\n",
      "epoch no.1 train no.127790  loss = 4.16927 avg_loss = 3.77839\n",
      "epoch no.1 train no.127800  loss = 2.93768 avg_loss = 3.83403\n",
      "epoch no.1 train no.127810  loss = 3.55982 avg_loss = 3.78274\n",
      "epoch no.1 train no.127820  loss = 4.66071 avg_loss = 3.78962\n",
      "epoch no.1 train no.127830  loss = 4.40455 avg_loss = 3.82881\n",
      "epoch no.1 train no.127840  loss = 3.94840 avg_loss = 3.89316\n",
      "epoch no.1 train no.127850  loss = 3.73752 avg_loss = 3.82658\n",
      "epoch no.1 train no.127860  loss = 4.72762 avg_loss = 3.80240\n",
      "epoch no.1 train no.127870  loss = 5.65936 avg_loss = 3.88601\n",
      "epoch no.1 train no.127880  loss = 4.48142 avg_loss = 3.94224\n",
      "epoch no.1 train no.127890  loss = 5.42259 avg_loss = 3.96988\n",
      "epoch no.1 train no.127900  loss = 4.14910 avg_loss = 3.95524\n",
      "epoch no.1 train no.127910  loss = 4.48118 avg_loss = 3.97168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.127920  loss = 2.26636 avg_loss = 3.91955\n",
      "epoch no.1 train no.127930  loss = 3.09695 avg_loss = 3.92209\n",
      "epoch no.1 train no.127940  loss = 6.42641 avg_loss = 3.92891\n",
      "epoch no.1 train no.127950  loss = 5.17576 avg_loss = 3.92596\n",
      "epoch no.1 train no.127960  loss = 1.93543 avg_loss = 3.93833\n",
      "epoch no.1 train no.127970  loss = 3.05467 avg_loss = 3.96103\n",
      "epoch no.1 train no.127980  loss = 3.67965 avg_loss = 3.96635\n",
      "epoch no.1 train no.127990  loss = 2.72761 avg_loss = 3.95415\n",
      "epoch no.1 train no.128000  loss = 4.71507 avg_loss = 3.95750\n",
      "4\n",
      "to_tokens: ['▁가을', '전환', '이', '▁위한', '▁신나는', '▁팝', '</s>']\n",
      "기분전환을 위한 신나는 팝</s>\n",
      "epoch no.1 train no.128010  loss = 2.52284 avg_loss = 3.96843\n",
      "epoch no.1 train no.128020  loss = 3.19625 avg_loss = 3.99387\n",
      "epoch no.1 train no.128030  loss = 8.45668 avg_loss = 4.00208\n",
      "epoch no.1 train no.128040  loss = 3.19860 avg_loss = 3.97048\n",
      "epoch no.1 train no.128050  loss = 3.15277 avg_loss = 3.96500\n",
      "epoch no.1 train no.128060  loss = 4.29278 avg_loss = 3.92609\n",
      "epoch no.1 train no.128070  loss = 1.92736 avg_loss = 3.84470\n",
      "epoch no.1 train no.128080  loss = 2.41357 avg_loss = 3.85807\n",
      "epoch no.1 train no.128090  loss = 3.80511 avg_loss = 3.84536\n",
      "epoch no.1 train no.128100  loss = 3.29917 avg_loss = 3.85578\n",
      "epoch no.1 train no.128110  loss = 2.48212 avg_loss = 3.86072\n",
      "epoch no.1 train no.128120  loss = 3.85846 avg_loss = 3.86035\n",
      "epoch no.1 train no.128130  loss = 2.67094 avg_loss = 3.84322\n",
      "epoch no.1 train no.128140  loss = 4.11656 avg_loss = 3.83996\n",
      "epoch no.1 train no.128150  loss = 3.26723 avg_loss = 3.78881\n",
      "epoch no.1 train no.128160  loss = 3.62385 avg_loss = 3.77249\n",
      "epoch no.1 train no.128170  loss = 2.20843 avg_loss = 3.72232\n",
      "epoch no.1 train no.128180  loss = 4.17622 avg_loss = 3.74536\n",
      "epoch no.1 train no.128190  loss = 5.24461 avg_loss = 3.80072\n",
      "epoch no.1 train no.128200  loss = 3.53410 avg_loss = 3.81415\n",
      "epoch no.1 train no.128210  loss = 3.95642 avg_loss = 3.84987\n",
      "epoch no.1 train no.128220  loss = 2.93100 avg_loss = 3.79569\n",
      "epoch no.1 train no.128230  loss = 3.96029 avg_loss = 3.79115\n",
      "epoch no.1 train no.128240  loss = 2.83865 avg_loss = 3.77133\n",
      "epoch no.1 train no.128250  loss = 5.18206 avg_loss = 3.76624\n",
      "epoch no.1 train no.128260  loss = 3.02704 avg_loss = 3.82533\n",
      "epoch no.1 train no.128270  loss = 2.84794 avg_loss = 3.77654\n",
      "epoch no.1 train no.128280  loss = 3.67593 avg_loss = 3.73878\n",
      "epoch no.1 train no.128290  loss = 4.40844 avg_loss = 3.84554\n",
      "epoch no.1 train no.128300  loss = 2.42568 avg_loss = 3.83418\n",
      "epoch no.1 train no.128310  loss = 4.25885 avg_loss = 3.84381\n",
      "epoch no.1 train no.128320  loss = 3.70324 avg_loss = 3.83348\n",
      "epoch no.1 train no.128330  loss = 3.43441 avg_loss = 3.90382\n",
      "epoch no.1 train no.128340  loss = 1.97937 avg_loss = 3.91092\n",
      "epoch no.1 train no.128350  loss = 3.76589 avg_loss = 3.85890\n",
      "epoch no.1 train no.128360  loss = 4.82176 avg_loss = 3.84427\n",
      "epoch no.1 train no.128370  loss = 4.92263 avg_loss = 3.88091\n",
      "epoch no.1 train no.128380  loss = 2.66703 avg_loss = 3.84866\n",
      "epoch no.1 train no.128390  loss = 4.02092 avg_loss = 3.84577\n",
      "epoch no.1 train no.128400  loss = 5.14925 avg_loss = 3.84165\n",
      "epoch no.1 train no.128410  loss = 5.68939 avg_loss = 3.86787\n",
      "epoch no.1 train no.128420  loss = 5.63226 avg_loss = 3.77392\n",
      "epoch no.1 train no.128430  loss = 4.31852 avg_loss = 3.83986\n",
      "epoch no.1 train no.128440  loss = 2.99167 avg_loss = 3.79576\n",
      "epoch no.1 train no.128450  loss = 1.87065 avg_loss = 3.81745\n",
      "epoch no.1 train no.128460  loss = 4.01865 avg_loss = 3.83229\n",
      "epoch no.1 train no.128470  loss = 5.08651 avg_loss = 3.86813\n",
      "epoch no.1 train no.128480  loss = 4.04080 avg_loss = 3.82736\n",
      "epoch no.1 train no.128490  loss = 4.80067 avg_loss = 3.84492\n",
      "epoch no.1 train no.128500  loss = 1.87050 avg_loss = 3.82724\n",
      "epoch no.1 train no.128510  loss = 2.89966 avg_loss = 3.77696\n",
      "epoch no.1 train no.128520  loss = 4.33438 avg_loss = 3.80851\n",
      "epoch no.1 train no.128530  loss = 2.87714 avg_loss = 3.81861\n",
      "epoch no.1 train no.128540  loss = 6.75538 avg_loss = 3.87859\n",
      "epoch no.1 train no.128550  loss = 2.69630 avg_loss = 3.87918\n",
      "epoch no.1 train no.128560  loss = 5.22775 avg_loss = 3.94148\n",
      "epoch no.1 train no.128570  loss = 4.04461 avg_loss = 3.90595\n",
      "epoch no.1 train no.128580  loss = 2.74273 avg_loss = 3.95515\n",
      "epoch no.1 train no.128590  loss = 3.58411 avg_loss = 3.88062\n",
      "epoch no.1 train no.128600  loss = 3.71075 avg_loss = 3.87676\n",
      "epoch no.1 train no.128610  loss = 3.35035 avg_loss = 3.84720\n",
      "epoch no.1 train no.128620  loss = 5.43532 avg_loss = 3.82067\n",
      "epoch no.1 train no.128630  loss = 3.50438 avg_loss = 3.87848\n",
      "epoch no.1 train no.128640  loss = 5.52248 avg_loss = 3.87100\n",
      "epoch no.1 train no.128650  loss = 4.21917 avg_loss = 3.86274\n",
      "epoch no.1 train no.128660  loss = 7.01809 avg_loss = 3.87691\n",
      "epoch no.1 train no.128670  loss = 3.64539 avg_loss = 3.85649\n",
      "epoch no.1 train no.128680  loss = 3.13110 avg_loss = 3.78914\n",
      "epoch no.1 train no.128690  loss = 2.25183 avg_loss = 3.70369\n",
      "epoch no.1 train no.128700  loss = 3.46628 avg_loss = 3.77019\n",
      "epoch no.1 train no.128710  loss = 3.73646 avg_loss = 3.73071\n",
      "epoch no.1 train no.128720  loss = 2.79454 avg_loss = 3.81225\n",
      "epoch no.1 train no.128730  loss = 3.46313 avg_loss = 3.82061\n",
      "epoch no.1 train no.128740  loss = 2.43844 avg_loss = 3.80888\n",
      "epoch no.1 train no.128750  loss = 3.74650 avg_loss = 3.79712\n",
      "epoch no.1 train no.128760  loss = 3.14383 avg_loss = 3.78156\n",
      "epoch no.1 train no.128770  loss = 5.10243 avg_loss = 3.83530\n",
      "epoch no.1 train no.128780  loss = 5.66007 avg_loss = 3.89318\n",
      "epoch no.1 train no.128790  loss = 3.41291 avg_loss = 3.92900\n",
      "epoch no.1 train no.128800  loss = 2.61968 avg_loss = 3.85178\n",
      "epoch no.1 train no.128810  loss = 5.46292 avg_loss = 3.86153\n",
      "epoch no.1 train no.128820  loss = 3.00043 avg_loss = 3.87165\n",
      "epoch no.1 train no.128830  loss = 2.67488 avg_loss = 3.81633\n",
      "epoch no.1 train no.128840  loss = 2.12489 avg_loss = 3.80596\n",
      "epoch no.1 train no.128850  loss = 2.98592 avg_loss = 3.84164\n",
      "epoch no.1 train no.128860  loss = 3.15400 avg_loss = 3.82367\n",
      "epoch no.1 train no.128870  loss = 2.45332 avg_loss = 3.87390\n",
      "epoch no.1 train no.128880  loss = 4.10890 avg_loss = 3.87717\n",
      "epoch no.1 train no.128890  loss = 5.44489 avg_loss = 3.85514\n",
      "epoch no.1 train no.128900  loss = 3.63900 avg_loss = 3.90991\n",
      "epoch no.1 train no.128910  loss = 3.37038 avg_loss = 3.90163\n",
      "epoch no.1 train no.128920  loss = 3.47910 avg_loss = 3.93533\n",
      "epoch no.1 train no.128930  loss = 3.46075 avg_loss = 3.89940\n",
      "epoch no.1 train no.128940  loss = 4.11242 avg_loss = 3.84889\n",
      "epoch no.1 train no.128950  loss = 2.78509 avg_loss = 3.85201\n",
      "epoch no.1 train no.128960  loss = 4.45710 avg_loss = 3.81483\n",
      "epoch no.1 train no.128970  loss = 2.20539 avg_loss = 3.83630\n",
      "epoch no.1 train no.128980  loss = 3.01537 avg_loss = 3.79988\n",
      "epoch no.1 train no.128990  loss = 5.20292 avg_loss = 3.79351\n",
      "epoch no.1 train no.129000  loss = 3.62912 avg_loss = 3.82912\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.1 train no.129010  loss = 3.84600 avg_loss = 3.81229\n",
      "epoch no.1 train no.129020  loss = 2.53407 avg_loss = 3.82151\n",
      "epoch no.1 train no.129030  loss = 2.79565 avg_loss = 3.78364\n",
      "epoch no.1 train no.129040  loss = 5.62000 avg_loss = 3.78041\n",
      "epoch no.1 train no.129050  loss = 4.52769 avg_loss = 3.76542\n",
      "epoch no.1 train no.129060  loss = 3.83235 avg_loss = 3.78525\n",
      "epoch no.1 train no.129070  loss = 3.80791 avg_loss = 3.76025\n",
      "epoch no.1 train no.129080  loss = 2.63669 avg_loss = 3.74559\n",
      "epoch no.1 train no.129090  loss = 2.40202 avg_loss = 3.72148\n",
      "epoch no.1 train no.129100  loss = 2.90850 avg_loss = 3.75618\n",
      "epoch no.1 train no.129110  loss = 4.06828 avg_loss = 3.75839\n",
      "epoch no.1 train no.129120  loss = 2.45450 avg_loss = 3.74328\n",
      "epoch no.1 train no.129130  loss = 3.84992 avg_loss = 3.71105\n",
      "epoch no.1 train no.129140  loss = 4.66214 avg_loss = 3.69126\n",
      "epoch no.1 train no.129150  loss = 3.18343 avg_loss = 3.65634\n",
      "epoch no.1 train no.129160  loss = 4.45209 avg_loss = 3.70831\n",
      "epoch no.1 train no.129170  loss = 3.01378 avg_loss = 3.69458\n",
      "epoch no.1 train no.129180  loss = 4.76774 avg_loss = 3.74092\n",
      "epoch no.1 train no.129190  loss = 4.77513 avg_loss = 3.80416\n",
      "epoch no.1 train no.129200  loss = 2.79727 avg_loss = 3.78939\n",
      "epoch no.1 train no.129210  loss = 2.76270 avg_loss = 3.78514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.129220  loss = 3.31750 avg_loss = 3.75708\n",
      "epoch no.1 train no.129230  loss = 3.39204 avg_loss = 3.82316\n",
      "epoch no.1 train no.129240  loss = 3.17757 avg_loss = 3.77514\n",
      "epoch no.1 train no.129250  loss = 3.35046 avg_loss = 3.74723\n",
      "epoch no.1 train no.129260  loss = 3.03128 avg_loss = 3.74558\n",
      "epoch no.1 train no.129270  loss = 3.93155 avg_loss = 3.73759\n",
      "epoch no.1 train no.129280  loss = 4.48601 avg_loss = 3.73766\n",
      "epoch no.1 train no.129290  loss = 2.70889 avg_loss = 3.79655\n",
      "epoch no.1 train no.129300  loss = 1.98554 avg_loss = 3.75507\n",
      "epoch no.1 train no.129310  loss = 2.88861 avg_loss = 3.70965\n",
      "epoch no.1 train no.129320  loss = 2.43623 avg_loss = 3.64417\n",
      "epoch no.1 train no.129330  loss = 4.71736 avg_loss = 3.67536\n",
      "epoch no.1 train no.129340  loss = 4.35561 avg_loss = 3.71147\n",
      "epoch no.1 train no.129350  loss = 4.10979 avg_loss = 3.68489\n",
      "epoch no.1 train no.129360  loss = 2.52541 avg_loss = 3.70202\n",
      "epoch no.1 train no.129370  loss = 1.84304 avg_loss = 3.70657\n",
      "epoch no.1 train no.129380  loss = 3.80212 avg_loss = 3.75423\n",
      "epoch no.1 train no.129390  loss = 4.77587 avg_loss = 3.77763\n",
      "epoch no.1 train no.129400  loss = 4.99352 avg_loss = 3.82092\n",
      "epoch no.1 train no.129410  loss = 7.72881 avg_loss = 3.81341\n",
      "epoch no.1 train no.129420  loss = 4.71235 avg_loss = 3.77480\n",
      "epoch no.1 train no.129430  loss = 2.77993 avg_loss = 3.73829\n",
      "epoch no.1 train no.129440  loss = 3.77411 avg_loss = 3.79307\n",
      "epoch no.1 train no.129450  loss = 3.80897 avg_loss = 3.81467\n",
      "epoch no.1 train no.129460  loss = 3.86728 avg_loss = 3.81633\n",
      "epoch no.1 train no.129470  loss = 3.52208 avg_loss = 3.77537\n",
      "epoch no.1 train no.129480  loss = 2.71065 avg_loss = 3.72063\n",
      "epoch no.1 train no.129490  loss = 3.03927 avg_loss = 3.69625\n",
      "epoch no.1 train no.129500  loss = 2.39628 avg_loss = 3.72660\n",
      "epoch no.1 train no.129510  loss = 4.45032 avg_loss = 3.73039\n",
      "epoch no.1 train no.129520  loss = 4.01397 avg_loss = 3.76801\n",
      "epoch no.1 train no.129530  loss = 6.92557 avg_loss = 3.80831\n",
      "epoch no.1 train no.129540  loss = 4.49000 avg_loss = 3.77894\n",
      "epoch no.1 train no.129550  loss = 3.57904 avg_loss = 3.73066\n",
      "epoch no.1 train no.129560  loss = 4.49306 avg_loss = 3.70664\n",
      "epoch no.1 train no.129570  loss = 5.59353 avg_loss = 3.73340\n",
      "epoch no.1 train no.129580  loss = 2.10030 avg_loss = 3.70425\n",
      "epoch no.1 train no.129590  loss = 4.56961 avg_loss = 3.77050\n",
      "epoch no.1 train no.129600  loss = 4.16039 avg_loss = 3.75221\n",
      "epoch no.1 train no.129610  loss = 3.94378 avg_loss = 3.73026\n",
      "epoch no.1 train no.129620  loss = 1.85887 avg_loss = 3.71770\n",
      "epoch no.1 train no.129630  loss = 4.27161 avg_loss = 3.72251\n",
      "epoch no.1 train no.129640  loss = 2.80703 avg_loss = 3.70994\n",
      "epoch no.1 train no.129650  loss = 2.81533 avg_loss = 3.66875\n",
      "epoch no.1 train no.129660  loss = 4.30407 avg_loss = 3.69026\n",
      "epoch no.1 train no.129670  loss = 5.24984 avg_loss = 3.69953\n",
      "epoch no.1 train no.129680  loss = 2.52959 avg_loss = 3.74427\n",
      "epoch no.1 train no.129690  loss = 5.61710 avg_loss = 3.78085\n",
      "epoch no.1 train no.129700  loss = 3.89678 avg_loss = 3.75528\n",
      "epoch no.1 train no.129710  loss = 4.87449 avg_loss = 3.72934\n",
      "epoch no.1 train no.129720  loss = 3.52098 avg_loss = 3.75356\n",
      "epoch no.1 train no.129730  loss = 5.47244 avg_loss = 3.80416\n",
      "epoch no.1 train no.129740  loss = 2.95215 avg_loss = 3.84918\n",
      "epoch no.1 train no.129750  loss = 3.33573 avg_loss = 3.83072\n",
      "epoch no.1 train no.129760  loss = 4.76079 avg_loss = 3.81367\n",
      "epoch no.1 train no.129770  loss = 3.94143 avg_loss = 3.76267\n",
      "epoch no.1 train no.129780  loss = 4.00916 avg_loss = 3.72003\n",
      "epoch no.1 train no.129790  loss = 2.58045 avg_loss = 3.68342\n",
      "epoch no.1 train no.129800  loss = 3.39602 avg_loss = 3.68409\n",
      "epoch no.1 train no.129810  loss = 4.42271 avg_loss = 3.66123\n",
      "epoch no.1 train no.129820  loss = 4.49257 avg_loss = 3.65025\n",
      "epoch no.1 train no.129830  loss = 3.98956 avg_loss = 3.65505\n",
      "epoch no.1 train no.129840  loss = 3.29185 avg_loss = 3.68175\n",
      "epoch no.1 train no.129850  loss = 4.69439 avg_loss = 3.70849\n",
      "epoch no.1 train no.129860  loss = 3.52406 avg_loss = 3.71363\n",
      "epoch no.1 train no.129870  loss = 3.22339 avg_loss = 3.74668\n",
      "epoch no.1 train no.129880  loss = 3.07449 avg_loss = 3.77184\n",
      "epoch no.1 train no.129890  loss = 2.74918 avg_loss = 3.84046\n",
      "epoch no.1 train no.129900  loss = 4.50276 avg_loss = 3.87049\n",
      "epoch no.1 train no.129910  loss = 4.07694 avg_loss = 3.88828\n",
      "epoch no.1 train no.129920  loss = 3.91569 avg_loss = 3.89030\n",
      "epoch no.1 train no.129930  loss = 3.89485 avg_loss = 3.85589\n",
      "epoch no.1 train no.129940  loss = 3.06630 avg_loss = 3.84693\n",
      "epoch no.1 train no.129950  loss = 2.52167 avg_loss = 3.82073\n",
      "epoch no.1 train no.129960  loss = 3.27509 avg_loss = 3.72539\n",
      "epoch no.1 train no.129970  loss = 2.96127 avg_loss = 3.70349\n",
      "epoch no.1 train no.129980  loss = 4.36407 avg_loss = 3.71389\n",
      "epoch no.1 train no.129990  loss = 3.77573 avg_loss = 3.70738\n",
      "epoch no.1 train no.130000  loss = 4.15689 avg_loss = 3.74398\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '에', '▁좋은', '▁노래', '에이지', '▁음악', '</s>']\n",
      "기분전환에 좋은 뉴에이지 음악</s>\n",
      "epoch no.1 train no.130010  loss = 5.95217 avg_loss = 3.75059\n",
      "epoch no.1 train no.130020  loss = 4.50558 avg_loss = 3.73519\n",
      "epoch no.1 train no.130030  loss = 2.81824 avg_loss = 3.72362\n",
      "epoch no.1 train no.130040  loss = 3.94293 avg_loss = 3.71237\n",
      "epoch no.1 train no.130050  loss = 4.02310 avg_loss = 3.80368\n",
      "epoch no.1 train no.130060  loss = 3.45941 avg_loss = 3.78467\n",
      "epoch no.1 train no.130070  loss = 3.90985 avg_loss = 3.80323\n",
      "epoch no.1 train no.130080  loss = 3.99682 avg_loss = 3.85103\n",
      "epoch no.1 train no.130090  loss = 2.07505 avg_loss = 3.83997\n",
      "epoch no.1 train no.130100  loss = 2.34964 avg_loss = 3.81567\n",
      "epoch no.1 train no.130110  loss = 4.65039 avg_loss = 3.83275\n",
      "epoch no.1 train no.130120  loss = 2.92846 avg_loss = 3.79008\n",
      "epoch no.1 train no.130130  loss = 5.16051 avg_loss = 3.75850\n",
      "epoch no.1 train no.130140  loss = 3.23581 avg_loss = 3.74638\n",
      "epoch no.1 train no.130150  loss = 5.63635 avg_loss = 3.85596\n",
      "epoch no.1 train no.130160  loss = 2.64748 avg_loss = 3.86432\n",
      "epoch no.1 train no.130170  loss = 4.34557 avg_loss = 3.85744\n",
      "epoch no.1 train no.130180  loss = 5.34013 avg_loss = 3.89251\n",
      "epoch no.1 train no.130190  loss = 5.47796 avg_loss = 3.89310\n",
      "epoch no.1 train no.130200  loss = 3.43816 avg_loss = 3.86080\n",
      "epoch no.1 train no.130210  loss = 3.30127 avg_loss = 3.87362\n",
      "epoch no.1 train no.130220  loss = 3.08628 avg_loss = 3.81837\n",
      "epoch no.1 train no.130230  loss = 5.00697 avg_loss = 3.84838\n",
      "epoch no.1 train no.130240  loss = 3.18525 avg_loss = 3.85983\n",
      "epoch no.1 train no.130250  loss = 4.73435 avg_loss = 3.83304\n",
      "epoch no.1 train no.130260  loss = 3.31110 avg_loss = 3.85624\n",
      "epoch no.1 train no.130270  loss = 3.36209 avg_loss = 3.91092\n",
      "epoch no.1 train no.130280  loss = 3.39723 avg_loss = 3.86068\n",
      "epoch no.1 train no.130290  loss = 3.16604 avg_loss = 3.87115\n",
      "epoch no.1 train no.130300  loss = 2.41357 avg_loss = 3.85878\n",
      "epoch no.1 train no.130310  loss = 4.22868 avg_loss = 3.85850\n",
      "epoch no.1 train no.130320  loss = 5.39849 avg_loss = 3.82431\n",
      "epoch no.1 train no.130330  loss = 3.89025 avg_loss = 3.81420\n",
      "epoch no.1 train no.130340  loss = 4.62678 avg_loss = 3.86033\n",
      "epoch no.1 train no.130350  loss = 3.00002 avg_loss = 3.80556\n",
      "epoch no.1 train no.130360  loss = 3.77327 avg_loss = 3.81141\n",
      "epoch no.1 train no.130370  loss = 2.39302 avg_loss = 3.76522\n",
      "epoch no.1 train no.130380  loss = 4.36944 avg_loss = 3.75073\n",
      "epoch no.1 train no.130390  loss = 2.02986 avg_loss = 3.74686\n",
      "epoch no.1 train no.130400  loss = 3.40121 avg_loss = 3.71903\n",
      "epoch no.1 train no.130410  loss = 4.59898 avg_loss = 3.71362\n",
      "epoch no.1 train no.130420  loss = 2.84004 avg_loss = 3.65127\n",
      "epoch no.1 train no.130430  loss = 5.56545 avg_loss = 3.70657\n",
      "epoch no.1 train no.130440  loss = 3.50205 avg_loss = 3.73573\n",
      "epoch no.1 train no.130450  loss = 3.16399 avg_loss = 3.72208\n",
      "epoch no.1 train no.130460  loss = 2.80749 avg_loss = 3.62795\n",
      "epoch no.1 train no.130470  loss = 4.34600 avg_loss = 3.60562\n",
      "epoch no.1 train no.130480  loss = 2.32501 avg_loss = 3.56704\n",
      "epoch no.1 train no.130490  loss = 3.23888 avg_loss = 3.59545\n",
      "epoch no.1 train no.130500  loss = 2.80319 avg_loss = 3.58100\n",
      "epoch no.1 train no.130510  loss = 2.86789 avg_loss = 3.57949\n",
      "epoch no.1 train no.130520  loss = 3.54165 avg_loss = 3.57527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.130530  loss = 6.02183 avg_loss = 3.56320\n",
      "epoch no.1 train no.130540  loss = 2.44516 avg_loss = 3.58955\n",
      "epoch no.1 train no.130550  loss = 2.63286 avg_loss = 3.59700\n",
      "epoch no.1 train no.130560  loss = 5.93613 avg_loss = 3.64877\n",
      "epoch no.1 train no.130570  loss = 3.38193 avg_loss = 3.66631\n",
      "epoch no.1 train no.130580  loss = 3.07246 avg_loss = 3.64321\n",
      "epoch no.1 train no.130590  loss = 7.77076 avg_loss = 3.72520\n",
      "epoch no.1 train no.130600  loss = 4.00309 avg_loss = 3.67648\n",
      "epoch no.1 train no.130610  loss = 4.06389 avg_loss = 3.71177\n",
      "epoch no.1 train no.130620  loss = 4.92179 avg_loss = 3.70791\n",
      "epoch no.1 train no.130630  loss = 2.53275 avg_loss = 3.62681\n",
      "epoch no.1 train no.130640  loss = 5.22886 avg_loss = 3.64283\n",
      "epoch no.1 train no.130650  loss = 4.30320 avg_loss = 3.70273\n",
      "epoch no.1 train no.130660  loss = 2.69099 avg_loss = 3.68137\n",
      "epoch no.1 train no.130670  loss = 4.23389 avg_loss = 3.71613\n",
      "epoch no.1 train no.130680  loss = 4.03160 avg_loss = 3.75247\n",
      "epoch no.1 train no.130690  loss = 3.83532 avg_loss = 3.73258\n",
      "epoch no.1 train no.130700  loss = 3.78530 avg_loss = 3.73633\n",
      "epoch no.1 train no.130710  loss = 3.75296 avg_loss = 3.73186\n",
      "epoch no.1 train no.130720  loss = 3.72591 avg_loss = 3.76585\n",
      "epoch no.1 train no.130730  loss = 5.42352 avg_loss = 3.80054\n",
      "epoch no.1 train no.130740  loss = 3.30288 avg_loss = 3.79668\n",
      "epoch no.1 train no.130750  loss = 3.63001 avg_loss = 3.80767\n",
      "epoch no.1 train no.130760  loss = 2.09795 avg_loss = 3.82067\n",
      "epoch no.1 train no.130770  loss = 3.83102 avg_loss = 3.78481\n",
      "epoch no.1 train no.130780  loss = 3.55369 avg_loss = 3.78159\n",
      "epoch no.1 train no.130790  loss = 5.08073 avg_loss = 3.76974\n",
      "epoch no.1 train no.130800  loss = 2.76191 avg_loss = 3.76350\n",
      "epoch no.1 train no.130810  loss = 2.82439 avg_loss = 3.83108\n",
      "epoch no.1 train no.130820  loss = 2.71562 avg_loss = 3.85238\n",
      "epoch no.1 train no.130830  loss = 4.81083 avg_loss = 3.88671\n",
      "epoch no.1 train no.130840  loss = 2.82038 avg_loss = 3.85395\n",
      "epoch no.1 train no.130850  loss = 4.60412 avg_loss = 3.85598\n",
      "epoch no.1 train no.130860  loss = 3.74494 avg_loss = 3.86937\n",
      "epoch no.1 train no.130870  loss = 2.93610 avg_loss = 3.80528\n",
      "epoch no.1 train no.130880  loss = 2.96250 avg_loss = 3.76390\n",
      "epoch no.1 train no.130890  loss = 4.99962 avg_loss = 3.76480\n",
      "epoch no.1 train no.130900  loss = 3.72441 avg_loss = 3.75620\n",
      "epoch no.1 train no.130910  loss = 4.63155 avg_loss = 3.81407\n",
      "epoch no.1 train no.130920  loss = 2.58444 avg_loss = 3.74788\n",
      "epoch no.1 train no.130930  loss = 4.52305 avg_loss = 3.73506\n",
      "epoch no.1 train no.130940  loss = 3.66837 avg_loss = 3.72656\n",
      "epoch no.1 train no.130950  loss = 3.69698 avg_loss = 3.75972\n",
      "epoch no.1 train no.130960  loss = 5.48198 avg_loss = 3.76227\n",
      "epoch no.1 train no.130970  loss = 4.06550 avg_loss = 3.71040\n",
      "epoch no.1 train no.130980  loss = 3.08281 avg_loss = 3.69852\n",
      "epoch no.1 train no.130990  loss = 2.82331 avg_loss = 3.67665\n",
      "epoch no.1 train no.131000  loss = 6.97784 avg_loss = 3.72969\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.1 train no.131010  loss = 3.04332 avg_loss = 3.74340\n",
      "epoch no.1 train no.131020  loss = 3.58235 avg_loss = 3.70965\n",
      "epoch no.1 train no.131030  loss = 5.32974 avg_loss = 3.80670\n",
      "epoch no.1 train no.131040  loss = 6.14116 avg_loss = 3.85842\n",
      "epoch no.1 train no.131050  loss = 5.45906 avg_loss = 3.83567\n",
      "epoch no.1 train no.131060  loss = 4.04923 avg_loss = 3.86683\n",
      "epoch no.1 train no.131070  loss = 2.62321 avg_loss = 3.85203\n",
      "epoch no.1 train no.131080  loss = 3.73880 avg_loss = 3.87929\n",
      "epoch no.1 train no.131090  loss = 4.09635 avg_loss = 3.89759\n",
      "epoch no.1 train no.131100  loss = 2.49882 avg_loss = 3.93391\n",
      "epoch no.1 train no.131110  loss = 4.37356 avg_loss = 3.97612\n",
      "epoch no.1 train no.131120  loss = 3.08526 avg_loss = 3.95225\n",
      "epoch no.1 train no.131130  loss = 3.68244 avg_loss = 3.92633\n",
      "epoch no.1 train no.131140  loss = 3.18540 avg_loss = 3.89360\n",
      "epoch no.1 train no.131150  loss = 3.14490 avg_loss = 3.83560\n",
      "epoch no.1 train no.131160  loss = 2.87285 avg_loss = 3.83229\n",
      "epoch no.1 train no.131170  loss = 4.73917 avg_loss = 3.86432\n",
      "epoch no.1 train no.131180  loss = 1.67151 avg_loss = 3.81575\n",
      "epoch no.1 train no.131190  loss = 5.66814 avg_loss = 3.84862\n",
      "epoch no.1 train no.131200  loss = 5.45992 avg_loss = 3.93243\n",
      "epoch no.1 train no.131210  loss = 3.16418 avg_loss = 3.90766\n",
      "epoch no.1 train no.131220  loss = 2.98761 avg_loss = 3.91541\n",
      "epoch no.1 train no.131230  loss = 2.57537 avg_loss = 3.88312\n",
      "epoch no.1 train no.131240  loss = 4.08418 avg_loss = 3.85029\n",
      "epoch no.1 train no.131250  loss = 4.22786 avg_loss = 3.83350\n",
      "epoch no.1 train no.131260  loss = 2.70769 avg_loss = 3.84418\n",
      "epoch no.1 train no.131270  loss = 2.84674 avg_loss = 3.80212\n",
      "epoch no.1 train no.131280  loss = 3.67555 avg_loss = 3.80376\n",
      "epoch no.1 train no.131290  loss = 2.20996 avg_loss = 3.85936\n",
      "epoch no.1 train no.131300  loss = 3.63197 avg_loss = 3.81858\n",
      "epoch no.1 train no.131310  loss = 3.31032 avg_loss = 3.80931\n",
      "epoch no.1 train no.131320  loss = 2.12178 avg_loss = 3.81807\n",
      "epoch no.1 train no.131330  loss = 1.94598 avg_loss = 3.80688\n",
      "epoch no.1 train no.131340  loss = 5.46818 avg_loss = 3.80436\n",
      "epoch no.1 train no.131350  loss = 4.40137 avg_loss = 3.76832\n",
      "epoch no.1 train no.131360  loss = 3.66319 avg_loss = 3.79310\n",
      "epoch no.1 train no.131370  loss = 2.89376 avg_loss = 3.78983\n",
      "epoch no.1 train no.131380  loss = 3.47131 avg_loss = 3.82287\n",
      "epoch no.1 train no.131390  loss = 5.09765 avg_loss = 3.76651\n",
      "epoch no.1 train no.131400  loss = 3.37362 avg_loss = 3.79022\n",
      "epoch no.1 train no.131410  loss = 2.96492 avg_loss = 3.82609\n",
      "epoch no.1 train no.131420  loss = 2.63889 avg_loss = 3.80956\n",
      "epoch no.1 train no.131430  loss = 2.08429 avg_loss = 3.78276\n",
      "epoch no.1 train no.131440  loss = 3.95804 avg_loss = 3.81820\n",
      "epoch no.1 train no.131450  loss = 5.12204 avg_loss = 3.82030\n",
      "epoch no.1 train no.131460  loss = 3.24423 avg_loss = 3.82295\n",
      "epoch no.1 train no.131470  loss = 4.59303 avg_loss = 3.82407\n",
      "epoch no.1 train no.131480  loss = 2.78334 avg_loss = 3.78637\n",
      "epoch no.1 train no.131490  loss = 3.45852 avg_loss = 3.81273\n",
      "epoch no.1 train no.131500  loss = 3.19659 avg_loss = 3.78389\n",
      "epoch no.1 train no.131510  loss = 5.74856 avg_loss = 3.83662\n",
      "epoch no.1 train no.131520  loss = 3.01040 avg_loss = 3.85962\n",
      "epoch no.1 train no.131530  loss = 3.39651 avg_loss = 3.78163\n",
      "epoch no.1 train no.131540  loss = 2.43552 avg_loss = 3.76391\n",
      "epoch no.1 train no.131550  loss = 3.32877 avg_loss = 3.73637\n",
      "epoch no.1 train no.131560  loss = 3.41252 avg_loss = 3.74433\n",
      "epoch no.1 train no.131570  loss = 5.30106 avg_loss = 3.70646\n",
      "epoch no.1 train no.131580  loss = 3.51574 avg_loss = 3.71986\n",
      "epoch no.1 train no.131590  loss = 2.55178 avg_loss = 3.74577\n",
      "epoch no.1 train no.131600  loss = 2.21985 avg_loss = 3.74806\n",
      "epoch no.1 train no.131610  loss = 3.60144 avg_loss = 3.72818\n",
      "epoch no.1 train no.131620  loss = 6.55768 avg_loss = 3.75232\n",
      "epoch no.1 train no.131630  loss = 4.04180 avg_loss = 3.76577\n",
      "epoch no.1 train no.131640  loss = 3.83326 avg_loss = 3.76862\n",
      "epoch no.1 train no.131650  loss = 1.91864 avg_loss = 3.71088\n",
      "epoch no.1 train no.131660  loss = 4.90722 avg_loss = 3.72237\n",
      "epoch no.1 train no.131670  loss = 3.02033 avg_loss = 3.71527\n",
      "epoch no.1 train no.131680  loss = 3.35978 avg_loss = 3.75453\n",
      "epoch no.1 train no.131690  loss = 6.54373 avg_loss = 3.80993\n",
      "epoch no.1 train no.131700  loss = 4.97063 avg_loss = 3.77148\n",
      "epoch no.1 train no.131710  loss = 4.48961 avg_loss = 3.76749\n",
      "epoch no.1 train no.131720  loss = 4.95273 avg_loss = 3.73073\n",
      "epoch no.1 train no.131730  loss = 3.81700 avg_loss = 3.71197\n",
      "epoch no.1 train no.131740  loss = 4.98999 avg_loss = 3.74561\n",
      "epoch no.1 train no.131750  loss = 7.48630 avg_loss = 3.78086\n",
      "epoch no.1 train no.131760  loss = 4.21185 avg_loss = 3.78342\n",
      "epoch no.1 train no.131770  loss = 5.56734 avg_loss = 3.78644\n",
      "epoch no.1 train no.131780  loss = 3.63974 avg_loss = 3.77617\n",
      "epoch no.1 train no.131790  loss = 3.97692 avg_loss = 3.77370\n",
      "epoch no.1 train no.131800  loss = 2.81994 avg_loss = 3.80029\n",
      "epoch no.1 train no.131810  loss = 2.57120 avg_loss = 3.82676\n",
      "epoch no.1 train no.131820  loss = 5.02911 avg_loss = 3.86514\n",
      "epoch no.1 train no.131830  loss = 2.02980 avg_loss = 3.89065\n",
      "epoch no.1 train no.131840  loss = 4.62556 avg_loss = 3.87725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.131850  loss = 2.08508 avg_loss = 3.90821\n",
      "epoch no.1 train no.131860  loss = 3.80394 avg_loss = 3.93689\n",
      "epoch no.1 train no.131870  loss = 4.44739 avg_loss = 3.92669\n",
      "epoch no.1 train no.131880  loss = 6.98863 avg_loss = 3.91816\n",
      "epoch no.1 train no.131890  loss = 4.30966 avg_loss = 3.86741\n",
      "epoch no.1 train no.131900  loss = 1.43148 avg_loss = 3.81376\n",
      "epoch no.1 train no.131910  loss = 4.25404 avg_loss = 3.79065\n",
      "epoch no.1 train no.131920  loss = 3.23017 avg_loss = 3.78226\n",
      "epoch no.1 train no.131930  loss = 2.71160 avg_loss = 3.75934\n",
      "epoch no.1 train no.131940  loss = 4.86358 avg_loss = 3.74311\n",
      "epoch no.1 train no.131950  loss = 3.36727 avg_loss = 3.74030\n",
      "epoch no.1 train no.131960  loss = 5.73987 avg_loss = 3.73595\n",
      "epoch no.1 train no.131970  loss = 3.87199 avg_loss = 3.77086\n",
      "epoch no.1 train no.131980  loss = 4.19852 avg_loss = 3.75444\n",
      "epoch no.1 train no.131990  loss = 3.66495 avg_loss = 3.83815\n",
      "epoch no.1 train no.132000  loss = 5.85073 avg_loss = 3.82868\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.1 train no.132010  loss = 4.68056 avg_loss = 3.84965\n",
      "epoch no.1 train no.132020  loss = 3.83842 avg_loss = 3.82322\n",
      "epoch no.1 train no.132030  loss = 2.28740 avg_loss = 3.78934\n",
      "epoch no.1 train no.132040  loss = 2.27639 avg_loss = 3.75545\n",
      "epoch no.1 train no.132050  loss = 2.89504 avg_loss = 3.68639\n",
      "epoch no.1 train no.132060  loss = 5.69146 avg_loss = 3.72682\n",
      "epoch no.1 train no.132070  loss = 3.24552 avg_loss = 3.73970\n",
      "epoch no.1 train no.132080  loss = 6.93849 avg_loss = 3.72570\n",
      "epoch no.1 train no.132090  loss = 4.51286 avg_loss = 3.67240\n",
      "epoch no.1 train no.132100  loss = 4.80879 avg_loss = 3.69832\n",
      "epoch no.1 train no.132110  loss = 5.85231 avg_loss = 3.71444\n",
      "epoch no.1 train no.132120  loss = 2.37720 avg_loss = 3.69006\n",
      "epoch no.1 train no.132130  loss = 1.62751 avg_loss = 3.60544\n",
      "epoch no.1 train no.132140  loss = 3.61366 avg_loss = 3.59800\n",
      "epoch no.1 train no.132150  loss = 3.41648 avg_loss = 3.65553\n",
      "epoch no.1 train no.132160  loss = 2.76637 avg_loss = 3.63921\n",
      "epoch no.1 train no.132170  loss = 6.08815 avg_loss = 3.69382\n",
      "epoch no.1 train no.132180  loss = 2.40837 avg_loss = 3.67187\n",
      "epoch no.1 train no.132190  loss = 3.56064 avg_loss = 3.66852\n",
      "epoch no.1 train no.132200  loss = 2.92087 avg_loss = 3.67972\n",
      "epoch no.1 train no.132210  loss = 4.96238 avg_loss = 3.70904\n",
      "epoch no.1 train no.132220  loss = 2.06888 avg_loss = 3.73746\n",
      "epoch no.1 train no.132230  loss = 2.88777 avg_loss = 3.70295\n",
      "epoch no.1 train no.132240  loss = 7.42629 avg_loss = 3.71655\n",
      "epoch no.1 train no.132250  loss = 3.16147 avg_loss = 3.68916\n",
      "epoch no.1 train no.132260  loss = 3.15735 avg_loss = 3.67143\n",
      "epoch no.1 train no.132270  loss = 3.85603 avg_loss = 3.71336\n",
      "epoch no.1 train no.132280  loss = 4.98473 avg_loss = 3.68601\n",
      "epoch no.1 train no.132290  loss = 6.73283 avg_loss = 3.72061\n",
      "epoch no.1 train no.132300  loss = 3.82112 avg_loss = 3.67870\n",
      "epoch no.1 train no.132310  loss = 3.41857 avg_loss = 3.63985\n",
      "epoch no.1 train no.132320  loss = 5.71552 avg_loss = 3.70569\n",
      "epoch no.1 train no.132330  loss = 4.17994 avg_loss = 3.73149\n",
      "epoch no.1 train no.132340  loss = 3.73785 avg_loss = 3.73382\n",
      "epoch no.1 train no.132350  loss = 2.72469 avg_loss = 3.72729\n",
      "epoch no.1 train no.132360  loss = 5.35575 avg_loss = 3.78812\n",
      "epoch no.1 train no.132370  loss = 4.04408 avg_loss = 3.84756\n",
      "epoch no.1 train no.132380  loss = 5.46332 avg_loss = 3.86156\n",
      "epoch no.1 train no.132390  loss = 3.65170 avg_loss = 3.83808\n",
      "epoch no.1 train no.132400  loss = 2.47751 avg_loss = 3.78729\n",
      "epoch no.1 train no.132410  loss = 4.50147 avg_loss = 3.85088\n",
      "epoch no.1 train no.132420  loss = 3.55048 avg_loss = 3.87845\n",
      "epoch no.1 train no.132430  loss = 4.66535 avg_loss = 3.82784\n",
      "epoch no.1 train no.132440  loss = 3.49755 avg_loss = 3.84124\n",
      "epoch no.1 train no.132450  loss = 5.88356 avg_loss = 3.89780\n",
      "epoch no.1 train no.132460  loss = 4.17516 avg_loss = 3.92079\n",
      "epoch no.1 train no.132470  loss = 3.33715 avg_loss = 3.89484\n",
      "epoch no.1 train no.132480  loss = 4.76989 avg_loss = 3.93953\n",
      "epoch no.1 train no.132490  loss = 6.21603 avg_loss = 3.96107\n",
      "epoch no.1 train no.132500  loss = 2.80937 avg_loss = 3.92626\n",
      "epoch no.1 train no.132510  loss = 2.82375 avg_loss = 3.91807\n",
      "epoch no.1 train no.132520  loss = 2.68721 avg_loss = 3.89778\n",
      "epoch no.1 train no.132530  loss = 5.24361 avg_loss = 3.91616\n",
      "epoch no.1 train no.132540  loss = 2.15228 avg_loss = 3.92160\n",
      "epoch no.1 train no.132550  loss = 2.99083 avg_loss = 3.91404\n",
      "epoch no.1 train no.132560  loss = 3.25919 avg_loss = 3.94972\n",
      "epoch no.1 train no.132570  loss = 3.42872 avg_loss = 3.93150\n",
      "epoch no.1 train no.132580  loss = 3.70699 avg_loss = 3.93441\n",
      "epoch no.1 train no.132590  loss = 4.16207 avg_loss = 3.89495\n",
      "epoch no.1 train no.132600  loss = 1.88247 avg_loss = 3.81718\n",
      "epoch no.1 train no.132610  loss = 4.22426 avg_loss = 3.78265\n",
      "epoch no.1 train no.132620  loss = 2.60624 avg_loss = 3.76556\n",
      "epoch no.1 train no.132630  loss = 4.02450 avg_loss = 3.74494\n",
      "epoch no.1 train no.132640  loss = 3.70699 avg_loss = 3.76826\n",
      "epoch no.1 train no.132650  loss = 5.69200 avg_loss = 3.80586\n",
      "epoch no.1 train no.132660  loss = 4.98533 avg_loss = 3.81677\n",
      "epoch no.1 train no.132670  loss = 6.95487 avg_loss = 3.81540\n",
      "epoch no.1 train no.132680  loss = 5.62442 avg_loss = 3.86643\n",
      "epoch no.1 train no.132690  loss = 2.52168 avg_loss = 3.83943\n",
      "epoch no.1 train no.132700  loss = 5.87247 avg_loss = 3.77311\n",
      "epoch no.1 train no.132710  loss = 4.09735 avg_loss = 3.77348\n",
      "epoch no.1 train no.132720  loss = 4.10714 avg_loss = 3.74057\n",
      "epoch no.1 train no.132730  loss = 2.80944 avg_loss = 3.76165\n",
      "epoch no.1 train no.132740  loss = 3.09255 avg_loss = 3.82043\n",
      "epoch no.1 train no.132750  loss = 5.29197 avg_loss = 3.81616\n",
      "epoch no.1 train no.132760  loss = 3.07843 avg_loss = 3.74708\n",
      "epoch no.1 train no.132770  loss = 3.00780 avg_loss = 3.73160\n",
      "epoch no.1 train no.132780  loss = 4.27779 avg_loss = 3.73375\n",
      "epoch no.1 train no.132790  loss = 4.62875 avg_loss = 3.74015\n",
      "epoch no.1 train no.132800  loss = 2.91496 avg_loss = 3.70664\n",
      "epoch no.1 train no.132810  loss = 3.63447 avg_loss = 3.68190\n",
      "epoch no.1 train no.132820  loss = 4.87997 avg_loss = 3.67803\n",
      "epoch no.1 train no.132830  loss = 2.66111 avg_loss = 3.66847\n",
      "epoch no.1 train no.132840  loss = 3.32165 avg_loss = 3.69202\n",
      "epoch no.1 train no.132850  loss = 5.17453 avg_loss = 3.66573\n",
      "epoch no.1 train no.132860  loss = 4.28227 avg_loss = 3.66649\n",
      "epoch no.1 train no.132870  loss = 4.89933 avg_loss = 3.69413\n",
      "epoch no.1 train no.132880  loss = 4.40999 avg_loss = 3.70089\n",
      "epoch no.1 train no.132890  loss = 4.14601 avg_loss = 3.72365\n",
      "epoch no.1 train no.132900  loss = 3.45082 avg_loss = 3.78043\n",
      "epoch no.1 train no.132910  loss = 3.70365 avg_loss = 3.74993\n",
      "epoch no.1 train no.132920  loss = 2.87334 avg_loss = 3.77587\n",
      "epoch no.1 train no.132930  loss = 5.32840 avg_loss = 3.81053\n",
      "epoch no.1 train no.132940  loss = 3.91550 avg_loss = 3.84880\n",
      "epoch no.1 train no.132950  loss = 2.41033 avg_loss = 3.83470\n",
      "epoch no.1 train no.132960  loss = 3.10821 avg_loss = 3.85827\n",
      "epoch no.1 train no.132970  loss = 2.51436 avg_loss = 3.84718\n",
      "epoch no.1 train no.132980  loss = 2.90077 avg_loss = 3.86471\n",
      "epoch no.1 train no.132990  loss = 4.97061 avg_loss = 3.85147\n",
      "epoch no.1 train no.133000  loss = 3.23755 avg_loss = 3.80968\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁위한', '▁신나는', '에이지', '</s>']\n",
      "기분전환을 위한 뉴에이지</s>\n",
      "epoch no.1 train no.133010  loss = 4.48564 avg_loss = 3.78882\n",
      "epoch no.1 train no.133020  loss = 4.18393 avg_loss = 3.81772\n",
      "epoch no.1 train no.133030  loss = 2.86897 avg_loss = 3.81424\n",
      "epoch no.1 train no.133040  loss = 2.78195 avg_loss = 3.78789\n",
      "epoch no.1 train no.133050  loss = 2.82488 avg_loss = 3.79031\n",
      "epoch no.1 train no.133060  loss = 2.37306 avg_loss = 3.82451\n",
      "epoch no.1 train no.133070  loss = 5.50864 avg_loss = 3.83025\n",
      "epoch no.1 train no.133080  loss = 2.18092 avg_loss = 3.83770\n",
      "epoch no.1 train no.133090  loss = 4.59806 avg_loss = 3.84065\n",
      "epoch no.1 train no.133100  loss = 3.81101 avg_loss = 3.80968\n",
      "epoch no.1 train no.133110  loss = 3.71866 avg_loss = 3.77425\n",
      "epoch no.1 train no.133120  loss = 2.50824 avg_loss = 3.76049\n",
      "epoch no.1 train no.133130  loss = 4.96574 avg_loss = 3.78205\n",
      "epoch no.1 train no.133140  loss = 4.87677 avg_loss = 3.78597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.133150  loss = 2.04764 avg_loss = 3.80479\n",
      "epoch no.1 train no.133160  loss = 3.53686 avg_loss = 3.81830\n",
      "epoch no.1 train no.133170  loss = 6.07055 avg_loss = 3.84552\n",
      "epoch no.1 train no.133180  loss = 4.71439 avg_loss = 3.80152\n",
      "epoch no.1 train no.133190  loss = 2.66072 avg_loss = 3.81573\n",
      "epoch no.1 train no.133200  loss = 5.49342 avg_loss = 3.76972\n",
      "epoch no.1 train no.133210  loss = 2.25510 avg_loss = 3.76255\n",
      "epoch no.1 train no.133220  loss = 4.97428 avg_loss = 3.76916\n",
      "epoch no.1 train no.133230  loss = 2.95124 avg_loss = 3.79635\n",
      "epoch no.1 train no.133240  loss = 3.95623 avg_loss = 3.81380\n",
      "epoch no.1 train no.133250  loss = 3.80058 avg_loss = 3.85654\n",
      "epoch no.1 train no.133260  loss = 4.23122 avg_loss = 3.83184\n",
      "epoch no.1 train no.133270  loss = 4.58337 avg_loss = 3.84246\n",
      "epoch no.1 train no.133280  loss = 2.61136 avg_loss = 3.81537\n",
      "epoch no.1 train no.133290  loss = 4.13929 avg_loss = 3.80995\n",
      "epoch no.1 train no.133300  loss = 3.38237 avg_loss = 3.79573\n",
      "epoch no.1 train no.133310  loss = 5.04389 avg_loss = 3.77442\n",
      "epoch no.1 train no.133320  loss = 4.52888 avg_loss = 3.81856\n",
      "epoch no.1 train no.133330  loss = 2.55267 avg_loss = 3.86327\n",
      "epoch no.1 train no.133340  loss = 3.25605 avg_loss = 3.84051\n",
      "epoch no.1 train no.133350  loss = 3.94888 avg_loss = 3.79437\n",
      "epoch no.1 train no.133360  loss = 2.38116 avg_loss = 3.77254\n",
      "epoch no.1 train no.133370  loss = 4.51818 avg_loss = 3.81559\n",
      "epoch no.1 train no.133380  loss = 3.44500 avg_loss = 3.76494\n",
      "epoch no.1 train no.133390  loss = 2.99010 avg_loss = 3.76839\n",
      "epoch no.1 train no.133400  loss = 2.27870 avg_loss = 3.74254\n",
      "epoch no.1 train no.133410  loss = 3.59266 avg_loss = 3.74983\n",
      "epoch no.1 train no.133420  loss = 2.64811 avg_loss = 3.77817\n",
      "epoch no.1 train no.133430  loss = 3.82301 avg_loss = 3.78323\n",
      "epoch no.1 train no.133440  loss = 3.87040 avg_loss = 3.77335\n",
      "epoch no.1 train no.133450  loss = 3.79240 avg_loss = 3.72998\n",
      "epoch no.1 train no.133460  loss = 5.71931 avg_loss = 3.74363\n",
      "epoch no.1 train no.133470  loss = 5.15430 avg_loss = 3.75598\n",
      "epoch no.1 train no.133480  loss = 2.21008 avg_loss = 3.68642\n",
      "epoch no.1 train no.133490  loss = 3.38960 avg_loss = 3.62991\n",
      "epoch no.1 train no.133500  loss = 3.19428 avg_loss = 3.64666\n",
      "epoch no.1 train no.133510  loss = 2.94254 avg_loss = 3.65762\n",
      "epoch no.1 train no.133520  loss = 3.87628 avg_loss = 3.68762\n",
      "epoch no.1 train no.133530  loss = 4.49473 avg_loss = 3.71999\n",
      "epoch no.1 train no.133540  loss = 6.54444 avg_loss = 3.78036\n",
      "epoch no.1 train no.133550  loss = 2.84313 avg_loss = 3.75164\n",
      "epoch no.1 train no.133560  loss = 3.80010 avg_loss = 3.72678\n",
      "epoch no.1 train no.133570  loss = 3.54067 avg_loss = 3.76256\n",
      "epoch no.1 train no.133580  loss = 2.96185 avg_loss = 3.80694\n",
      "epoch no.1 train no.133590  loss = 4.99124 avg_loss = 3.78732\n",
      "epoch no.1 train no.133600  loss = 2.72318 avg_loss = 3.75428\n",
      "epoch no.1 train no.133610  loss = 3.93890 avg_loss = 3.77960\n",
      "epoch no.1 train no.133620  loss = 3.87005 avg_loss = 3.80675\n",
      "epoch no.1 train no.133630  loss = 5.64914 avg_loss = 3.85182\n",
      "epoch no.1 train no.133640  loss = 4.24686 avg_loss = 3.83482\n",
      "epoch no.1 train no.133650  loss = 5.37764 avg_loss = 3.81699\n",
      "epoch no.1 train no.133660  loss = 3.38067 avg_loss = 3.76657\n",
      "epoch no.1 train no.133670  loss = 3.04226 avg_loss = 3.69791\n",
      "epoch no.1 train no.133680  loss = 5.75833 avg_loss = 3.77998\n",
      "epoch no.1 train no.133690  loss = 4.72775 avg_loss = 3.82988\n",
      "epoch no.1 train no.133700  loss = 4.71874 avg_loss = 3.83153\n",
      "epoch no.1 train no.133710  loss = 3.77266 avg_loss = 3.81541\n",
      "epoch no.1 train no.133720  loss = 3.53383 avg_loss = 3.78088\n",
      "epoch no.1 train no.133730  loss = 3.70403 avg_loss = 3.81215\n",
      "epoch no.1 train no.133740  loss = 2.84047 avg_loss = 3.77056\n",
      "epoch no.1 train no.133750  loss = 4.73921 avg_loss = 3.80707\n",
      "epoch no.1 train no.133760  loss = 2.99266 avg_loss = 3.80197\n",
      "epoch no.1 train no.133770  loss = 3.80377 avg_loss = 3.76367\n",
      "epoch no.1 train no.133780  loss = 4.98124 avg_loss = 3.71742\n",
      "epoch no.1 train no.133790  loss = 5.78439 avg_loss = 3.73662\n",
      "epoch no.1 train no.133800  loss = 5.21991 avg_loss = 3.73758\n",
      "epoch no.1 train no.133810  loss = 3.75893 avg_loss = 3.65967\n",
      "epoch no.1 train no.133820  loss = 2.25869 avg_loss = 3.64889\n",
      "epoch no.1 train no.133830  loss = 2.95624 avg_loss = 3.64210\n",
      "epoch no.1 train no.133840  loss = 3.55007 avg_loss = 3.62918\n",
      "epoch no.1 train no.133850  loss = 3.17329 avg_loss = 3.62544\n",
      "epoch no.1 train no.133860  loss = 3.96460 avg_loss = 3.62069\n",
      "epoch no.1 train no.133870  loss = 5.09449 avg_loss = 3.65117\n",
      "epoch no.1 train no.133880  loss = 1.65853 avg_loss = 3.64080\n",
      "epoch no.1 train no.133890  loss = 2.95060 avg_loss = 3.67384\n",
      "epoch no.1 train no.133900  loss = 4.82385 avg_loss = 3.70693\n",
      "epoch no.1 train no.133910  loss = 4.07436 avg_loss = 3.70421\n",
      "epoch no.1 train no.133920  loss = 3.63833 avg_loss = 3.68107\n",
      "epoch no.1 train no.133930  loss = 3.76375 avg_loss = 3.70195\n",
      "epoch no.1 train no.133940  loss = 1.83627 avg_loss = 3.72115\n",
      "epoch no.1 train no.133950  loss = 3.40565 avg_loss = 3.70441\n",
      "epoch no.1 train no.133960  loss = 3.36941 avg_loss = 3.74347\n",
      "epoch no.1 train no.133970  loss = 2.49755 avg_loss = 3.71709\n",
      "epoch no.1 train no.133980  loss = 2.38034 avg_loss = 3.68745\n",
      "epoch no.1 train no.133990  loss = 6.40587 avg_loss = 3.68502\n",
      "epoch no.1 train no.134000  loss = 4.06472 avg_loss = 3.67232\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '▁음악', '▁노래', '</s>']\n",
      "기분전환하기 좋은 신나는 노래</s>\n",
      "epoch no.1 train no.134010  loss = 3.25683 avg_loss = 3.66869\n",
      "epoch no.1 train no.134020  loss = 4.72158 avg_loss = 3.67493\n",
      "epoch no.1 train no.134030  loss = 2.94540 avg_loss = 3.70085\n",
      "epoch no.1 train no.134040  loss = 5.45749 avg_loss = 3.69646\n",
      "epoch no.1 train no.134050  loss = 5.54173 avg_loss = 3.75834\n",
      "epoch no.1 train no.134060  loss = 3.92149 avg_loss = 3.76578\n",
      "epoch no.1 train no.134070  loss = 3.12140 avg_loss = 3.79918\n",
      "epoch no.1 train no.134080  loss = 3.20623 avg_loss = 3.73979\n",
      "epoch no.1 train no.134090  loss = 4.42369 avg_loss = 3.74557\n",
      "epoch no.1 train no.134100  loss = 5.69647 avg_loss = 3.77577\n",
      "epoch no.1 train no.134110  loss = 2.16824 avg_loss = 3.71952\n",
      "epoch no.1 train no.134120  loss = 4.01041 avg_loss = 3.70250\n",
      "epoch no.1 train no.134130  loss = 2.93342 avg_loss = 3.71440\n",
      "epoch no.1 train no.134140  loss = 3.42820 avg_loss = 3.72349\n",
      "epoch no.1 train no.134150  loss = 4.42248 avg_loss = 3.75445\n",
      "epoch no.1 train no.134160  loss = 5.78119 avg_loss = 3.76962\n",
      "epoch no.1 train no.134170  loss = 4.62236 avg_loss = 3.82566\n",
      "epoch no.1 train no.134180  loss = 4.70012 avg_loss = 3.82463\n",
      "epoch no.1 train no.134190  loss = 2.88206 avg_loss = 3.87481\n",
      "epoch no.1 train no.134200  loss = 5.98113 avg_loss = 3.87863\n",
      "epoch no.1 train no.134210  loss = 4.82299 avg_loss = 3.88480\n",
      "epoch no.1 train no.134220  loss = 3.29303 avg_loss = 3.84258\n",
      "epoch no.1 train no.134230  loss = 4.28678 avg_loss = 3.82978\n",
      "epoch no.1 train no.134240  loss = 4.14691 avg_loss = 3.85210\n",
      "epoch no.1 train no.134250  loss = 3.81601 avg_loss = 3.82182\n",
      "epoch no.1 train no.134260  loss = 3.78495 avg_loss = 3.81558\n",
      "epoch no.1 train no.134270  loss = 3.61356 avg_loss = 3.79491\n",
      "epoch no.1 train no.134280  loss = 3.62465 avg_loss = 3.80008\n",
      "epoch no.1 train no.134290  loss = 3.78945 avg_loss = 3.83103\n",
      "epoch no.1 train no.134300  loss = 4.74752 avg_loss = 3.83186\n",
      "epoch no.1 train no.134310  loss = 3.63993 avg_loss = 3.77974\n",
      "epoch no.1 train no.134320  loss = 2.98055 avg_loss = 3.80505\n",
      "epoch no.1 train no.134330  loss = 2.77300 avg_loss = 3.75286\n",
      "epoch no.1 train no.134340  loss = 4.14974 avg_loss = 3.79090\n",
      "epoch no.1 train no.134350  loss = 1.33619 avg_loss = 3.68689\n",
      "epoch no.1 train no.134360  loss = 3.93666 avg_loss = 3.74185\n",
      "epoch no.1 train no.134370  loss = 4.52171 avg_loss = 3.76407\n",
      "epoch no.1 train no.134380  loss = 2.83923 avg_loss = 3.73580\n",
      "epoch no.1 train no.134390  loss = 4.69130 avg_loss = 3.78275\n",
      "epoch no.1 train no.134400  loss = 3.35775 avg_loss = 3.80567\n",
      "epoch no.1 train no.134410  loss = 2.08418 avg_loss = 3.75890\n",
      "epoch no.1 train no.134420  loss = 6.29079 avg_loss = 3.73483\n",
      "epoch no.1 train no.134430  loss = 3.53699 avg_loss = 3.70201\n",
      "epoch no.1 train no.134440  loss = 4.76026 avg_loss = 3.72818\n",
      "epoch no.1 train no.134450  loss = 3.84804 avg_loss = 3.72678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.134460  loss = 2.80798 avg_loss = 3.76777\n",
      "epoch no.1 train no.134470  loss = 6.60885 avg_loss = 3.79795\n",
      "epoch no.1 train no.134480  loss = 2.97224 avg_loss = 3.74975\n",
      "epoch no.1 train no.134490  loss = 4.75243 avg_loss = 3.71733\n",
      "epoch no.1 train no.134500  loss = 4.56105 avg_loss = 3.75045\n",
      "epoch no.1 train no.134510  loss = 4.18258 avg_loss = 3.74779\n",
      "epoch no.1 train no.134520  loss = 4.65419 avg_loss = 3.75652\n",
      "epoch no.1 train no.134530  loss = 2.13727 avg_loss = 3.73815\n",
      "epoch no.1 train no.134540  loss = 4.83677 avg_loss = 3.76897\n",
      "epoch no.1 train no.134550  loss = 3.12187 avg_loss = 3.77923\n",
      "epoch no.1 train no.134560  loss = 2.05368 avg_loss = 3.76786\n",
      "epoch no.1 train no.134570  loss = 3.03852 avg_loss = 3.75251\n",
      "epoch no.1 train no.134580  loss = 2.25357 avg_loss = 3.73639\n",
      "epoch no.1 train no.134590  loss = 2.49219 avg_loss = 3.71660\n",
      "epoch no.1 train no.134600  loss = 3.62957 avg_loss = 3.72301\n",
      "epoch no.1 train no.134610  loss = 4.21508 avg_loss = 3.72677\n",
      "epoch no.1 train no.134620  loss = 5.35391 avg_loss = 3.75561\n",
      "epoch no.1 train no.134630  loss = 3.17681 avg_loss = 3.72897\n",
      "epoch no.1 train no.134640  loss = 2.59589 avg_loss = 3.69865\n",
      "epoch no.1 train no.134650  loss = 3.48300 avg_loss = 3.73721\n",
      "epoch no.1 train no.134660  loss = 3.07604 avg_loss = 3.73550\n",
      "epoch no.1 train no.134670  loss = 7.24906 avg_loss = 3.76921\n",
      "epoch no.1 train no.134680  loss = 5.01152 avg_loss = 3.80424\n",
      "epoch no.1 train no.134690  loss = 2.20598 avg_loss = 3.78053\n",
      "epoch no.1 train no.134700  loss = 4.89372 avg_loss = 3.75719\n",
      "epoch no.1 train no.134710  loss = 3.95780 avg_loss = 3.70258\n",
      "epoch no.1 train no.134720  loss = 3.02764 avg_loss = 3.73074\n",
      "epoch no.1 train no.134730  loss = 5.61736 avg_loss = 3.76716\n",
      "epoch no.1 train no.134740  loss = 3.39680 avg_loss = 3.81730\n",
      "epoch no.1 train no.134750  loss = 6.64091 avg_loss = 3.83370\n",
      "epoch no.1 train no.134760  loss = 2.09814 avg_loss = 3.81829\n",
      "epoch no.1 train no.134770  loss = 6.39162 avg_loss = 3.86943\n",
      "epoch no.1 train no.134780  loss = 3.22623 avg_loss = 3.86187\n",
      "epoch no.1 train no.134790  loss = 3.87593 avg_loss = 3.86842\n",
      "epoch no.1 train no.134800  loss = 5.95341 avg_loss = 3.86352\n",
      "epoch no.1 train no.134810  loss = 4.43793 avg_loss = 3.84047\n",
      "epoch no.1 train no.134820  loss = 4.07135 avg_loss = 3.81726\n",
      "epoch no.1 train no.134830  loss = 2.26522 avg_loss = 3.82174\n",
      "epoch no.1 train no.134840  loss = 2.65580 avg_loss = 3.75071\n",
      "epoch no.1 train no.134850  loss = 5.12717 avg_loss = 3.77959\n",
      "epoch no.1 train no.134860  loss = 5.33115 avg_loss = 3.79240\n",
      "epoch no.1 train no.134870  loss = 3.91459 avg_loss = 3.77470\n",
      "epoch no.1 train no.134880  loss = 4.89865 avg_loss = 3.77860\n",
      "epoch no.1 train no.134890  loss = 2.80167 avg_loss = 3.82783\n",
      "epoch no.1 train no.134900  loss = 5.57153 avg_loss = 3.84613\n",
      "epoch no.1 train no.134910  loss = 2.09451 avg_loss = 3.85877\n",
      "epoch no.1 train no.134920  loss = 4.42221 avg_loss = 3.87941\n",
      "epoch no.1 train no.134930  loss = 3.15284 avg_loss = 3.88293\n",
      "epoch no.1 train no.134940  loss = 5.88137 avg_loss = 3.88052\n",
      "epoch no.1 train no.134950  loss = 3.49954 avg_loss = 3.82812\n",
      "epoch no.1 train no.134960  loss = 3.56268 avg_loss = 3.79223\n",
      "epoch no.1 train no.134970  loss = 3.20651 avg_loss = 3.78696\n",
      "epoch no.1 train no.134980  loss = 4.09920 avg_loss = 3.81096\n",
      "epoch no.1 train no.134990  loss = 4.21040 avg_loss = 3.82333\n",
      "epoch no.1 train no.135000  loss = 4.11008 avg_loss = 3.80481\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '에', '▁좋은', '▁음악', '송', '</s>']\n",
      "기분전환에 좋은 팝송</s>\n",
      "epoch no.1 train no.135010  loss = 3.97097 avg_loss = 3.79560\n",
      "epoch no.1 train no.135020  loss = 3.13575 avg_loss = 3.79126\n",
      "epoch no.1 train no.135030  loss = 3.47766 avg_loss = 3.80997\n",
      "epoch no.1 train no.135040  loss = 5.91581 avg_loss = 3.81796\n",
      "epoch no.1 train no.135050  loss = 3.44225 avg_loss = 3.78921\n",
      "epoch no.1 train no.135060  loss = 3.21006 avg_loss = 3.78487\n",
      "epoch no.1 train no.135070  loss = 6.40044 avg_loss = 3.82636\n",
      "epoch no.1 train no.135080  loss = 4.79930 avg_loss = 3.81094\n",
      "epoch no.1 train no.135090  loss = 3.89541 avg_loss = 3.84605\n",
      "epoch no.1 train no.135100  loss = 5.00735 avg_loss = 3.84324\n",
      "epoch no.1 train no.135110  loss = 3.21252 avg_loss = 3.85018\n",
      "epoch no.1 train no.135120  loss = 4.91504 avg_loss = 3.83633\n",
      "epoch no.1 train no.135130  loss = 3.50873 avg_loss = 3.76694\n",
      "epoch no.1 train no.135140  loss = 3.21986 avg_loss = 3.76381\n",
      "epoch no.1 train no.135150  loss = 5.50762 avg_loss = 3.78587\n",
      "epoch no.1 train no.135160  loss = 5.10956 avg_loss = 3.88408\n",
      "epoch no.1 train no.135170  loss = 2.44639 avg_loss = 3.87527\n",
      "epoch no.1 train no.135180  loss = 3.68153 avg_loss = 3.87183\n",
      "epoch no.1 train no.135190  loss = 5.91755 avg_loss = 3.87300\n",
      "epoch no.1 train no.135200  loss = 4.69186 avg_loss = 3.89025\n",
      "epoch no.1 train no.135210  loss = 4.17053 avg_loss = 3.83911\n",
      "epoch no.1 train no.135220  loss = 2.82375 avg_loss = 3.81628\n",
      "epoch no.1 train no.135230  loss = 4.58428 avg_loss = 3.82717\n",
      "epoch no.1 train no.135240  loss = 5.63184 avg_loss = 3.82222\n",
      "epoch no.1 train no.135250  loss = 2.39925 avg_loss = 3.82786\n",
      "epoch no.1 train no.135260  loss = 6.47788 avg_loss = 3.86971\n",
      "epoch no.1 train no.135270  loss = 4.48444 avg_loss = 3.88692\n",
      "epoch no.1 train no.135280  loss = 5.41540 avg_loss = 3.85247\n",
      "epoch no.1 train no.135290  loss = 2.09073 avg_loss = 3.86578\n",
      "epoch no.1 train no.135300  loss = 2.32156 avg_loss = 3.87283\n",
      "epoch no.1 train no.135310  loss = 2.39291 avg_loss = 3.85523\n",
      "epoch no.1 train no.135320  loss = 3.46790 avg_loss = 3.83492\n",
      "epoch no.1 train no.135330  loss = 2.73535 avg_loss = 3.76130\n",
      "epoch no.1 train no.135340  loss = 3.99943 avg_loss = 3.81112\n",
      "epoch no.1 train no.135350  loss = 5.71356 avg_loss = 3.88652\n",
      "epoch no.1 train no.135360  loss = 2.74135 avg_loss = 3.84204\n",
      "epoch no.1 train no.135370  loss = 2.13929 avg_loss = 3.83353\n",
      "epoch no.1 train no.135380  loss = 2.79263 avg_loss = 3.78771\n",
      "epoch no.1 train no.135390  loss = 1.99820 avg_loss = 3.73148\n",
      "epoch no.1 train no.135400  loss = 5.29365 avg_loss = 3.79952\n",
      "epoch no.1 train no.135410  loss = 3.42810 avg_loss = 3.81792\n",
      "epoch no.1 train no.135420  loss = 3.18786 avg_loss = 3.77149\n",
      "epoch no.1 train no.135430  loss = 4.21323 avg_loss = 3.77492\n",
      "epoch no.1 train no.135440  loss = 4.35588 avg_loss = 3.83124\n",
      "epoch no.1 train no.135450  loss = 3.06155 avg_loss = 3.81988\n",
      "epoch no.1 train no.135460  loss = 3.54733 avg_loss = 3.80194\n",
      "epoch no.1 train no.135470  loss = 4.48547 avg_loss = 3.78913\n",
      "epoch no.1 train no.135480  loss = 3.05390 avg_loss = 3.72314\n",
      "epoch no.1 train no.135490  loss = 5.55778 avg_loss = 3.79027\n",
      "epoch no.1 train no.135500  loss = 4.18437 avg_loss = 3.77546\n",
      "epoch no.1 train no.135510  loss = 2.71408 avg_loss = 3.82682\n",
      "epoch no.1 train no.135520  loss = 3.35102 avg_loss = 3.79408\n",
      "epoch no.1 train no.135530  loss = 2.38784 avg_loss = 3.77808\n",
      "epoch no.1 train no.135540  loss = 4.20631 avg_loss = 3.74692\n",
      "epoch no.1 train no.135550  loss = 4.72771 avg_loss = 3.75753\n",
      "epoch no.1 train no.135560  loss = 4.26957 avg_loss = 3.75097\n",
      "epoch no.1 train no.135570  loss = 4.94868 avg_loss = 3.74521\n",
      "epoch no.1 train no.135580  loss = 2.82688 avg_loss = 3.76405\n",
      "epoch no.1 train no.135590  loss = 3.25845 avg_loss = 3.73771\n",
      "epoch no.1 train no.135600  loss = 2.99490 avg_loss = 3.75122\n",
      "epoch no.1 train no.135610  loss = 2.57643 avg_loss = 3.71173\n",
      "epoch no.1 train no.135620  loss = 2.13510 avg_loss = 3.67430\n",
      "epoch no.1 train no.135630  loss = 2.93399 avg_loss = 3.68925\n",
      "epoch no.1 train no.135640  loss = 2.62521 avg_loss = 3.72975\n",
      "epoch no.1 train no.135650  loss = 2.14101 avg_loss = 3.70156\n",
      "epoch no.1 train no.135660  loss = 2.63858 avg_loss = 3.72624\n",
      "epoch no.1 train no.135670  loss = 6.24007 avg_loss = 3.70518\n",
      "epoch no.1 train no.135680  loss = 3.19245 avg_loss = 3.65793\n",
      "epoch no.1 train no.135690  loss = 1.71329 avg_loss = 3.69782\n",
      "epoch no.1 train no.135700  loss = 6.93634 avg_loss = 3.73564\n",
      "epoch no.1 train no.135710  loss = 1.76618 avg_loss = 3.75759\n",
      "epoch no.1 train no.135720  loss = 3.04560 avg_loss = 3.74534\n",
      "epoch no.1 train no.135730  loss = 6.59237 avg_loss = 3.79008\n",
      "epoch no.1 train no.135740  loss = 3.32920 avg_loss = 3.77114\n",
      "epoch no.1 train no.135750  loss = 3.36636 avg_loss = 3.78494\n",
      "epoch no.1 train no.135760  loss = 2.88986 avg_loss = 3.75058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.135770  loss = 3.60578 avg_loss = 3.70735\n",
      "epoch no.1 train no.135780  loss = 5.88474 avg_loss = 3.72007\n",
      "epoch no.1 train no.135790  loss = 4.57565 avg_loss = 3.75967\n",
      "epoch no.1 train no.135800  loss = 2.42068 avg_loss = 3.74573\n",
      "epoch no.1 train no.135810  loss = 3.61801 avg_loss = 3.75955\n",
      "epoch no.1 train no.135820  loss = 6.55634 avg_loss = 3.80496\n",
      "epoch no.1 train no.135830  loss = 3.07752 avg_loss = 3.90384\n",
      "epoch no.1 train no.135840  loss = 5.08283 avg_loss = 3.91169\n",
      "epoch no.1 train no.135850  loss = 3.89285 avg_loss = 3.90019\n",
      "epoch no.1 train no.135860  loss = 3.32319 avg_loss = 3.87091\n",
      "epoch no.1 train no.135870  loss = 4.50638 avg_loss = 3.86050\n",
      "epoch no.1 train no.135880  loss = 3.00391 avg_loss = 3.79117\n",
      "epoch no.1 train no.135890  loss = 5.58993 avg_loss = 3.82231\n",
      "epoch no.1 train no.135900  loss = 2.33083 avg_loss = 3.77184\n",
      "epoch no.1 train no.135910  loss = 3.60482 avg_loss = 3.76815\n",
      "epoch no.1 train no.135920  loss = 6.02694 avg_loss = 3.84093\n",
      "epoch no.1 train no.135930  loss = 4.97275 avg_loss = 3.85486\n",
      "epoch no.1 train no.135940  loss = 4.03984 avg_loss = 3.82140\n",
      "epoch no.1 train no.135950  loss = 4.28838 avg_loss = 3.83525\n",
      "epoch no.1 train no.135960  loss = 3.76750 avg_loss = 3.81083\n",
      "epoch no.1 train no.135970  loss = 4.50886 avg_loss = 3.80335\n",
      "epoch no.1 train no.135980  loss = 4.86181 avg_loss = 3.82292\n",
      "epoch no.1 train no.135990  loss = 2.45557 avg_loss = 3.77555\n",
      "epoch no.1 train no.136000  loss = 3.58626 avg_loss = 3.78110\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '에', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.1 train no.136010  loss = 2.78078 avg_loss = 3.71212\n",
      "epoch no.1 train no.136020  loss = 3.70509 avg_loss = 3.71550\n",
      "epoch no.1 train no.136030  loss = 2.41639 avg_loss = 3.72339\n",
      "epoch no.1 train no.136040  loss = 3.72707 avg_loss = 3.71630\n",
      "epoch no.1 train no.136050  loss = 3.37851 avg_loss = 3.70821\n",
      "epoch no.1 train no.136060  loss = 3.28860 avg_loss = 3.71552\n",
      "epoch no.1 train no.136070  loss = 4.16984 avg_loss = 3.74009\n",
      "epoch no.1 train no.136080  loss = 2.31629 avg_loss = 3.75501\n",
      "epoch no.1 train no.136090  loss = 5.00496 avg_loss = 3.80933\n",
      "epoch no.1 train no.136100  loss = 3.55776 avg_loss = 3.81513\n",
      "epoch no.1 train no.136110  loss = 2.86299 avg_loss = 3.81025\n",
      "epoch no.1 train no.136120  loss = 4.08615 avg_loss = 3.85627\n",
      "epoch no.1 train no.136130  loss = 3.89709 avg_loss = 3.82862\n",
      "epoch no.1 train no.136140  loss = 5.40610 avg_loss = 3.83291\n",
      "epoch no.1 train no.136150  loss = 2.04469 avg_loss = 3.78431\n",
      "epoch no.1 train no.136160  loss = 5.25772 avg_loss = 3.74542\n",
      "epoch no.1 train no.136170  loss = 3.34183 avg_loss = 3.69793\n",
      "epoch no.1 train no.136180  loss = 4.06063 avg_loss = 3.74047\n",
      "epoch no.1 train no.136190  loss = 2.30359 avg_loss = 3.75882\n",
      "epoch no.1 train no.136200  loss = 4.92208 avg_loss = 3.74691\n",
      "epoch no.1 train no.136210  loss = 2.18410 avg_loss = 3.74960\n",
      "epoch no.1 train no.136220  loss = 5.50663 avg_loss = 3.73323\n",
      "epoch no.1 train no.136230  loss = 3.61310 avg_loss = 3.81602\n",
      "epoch no.1 train no.136240  loss = 3.46102 avg_loss = 3.79446\n",
      "epoch no.1 train no.136250  loss = 2.76593 avg_loss = 3.77710\n",
      "epoch no.1 train no.136260  loss = 3.21755 avg_loss = 3.74182\n",
      "epoch no.1 train no.136270  loss = 3.84147 avg_loss = 3.72655\n",
      "epoch no.1 train no.136280  loss = 2.79646 avg_loss = 3.72614\n",
      "epoch no.1 train no.136290  loss = 3.58531 avg_loss = 3.67962\n",
      "epoch no.1 train no.136300  loss = 5.89393 avg_loss = 3.72687\n",
      "epoch no.1 train no.136310  loss = 3.40574 avg_loss = 3.74851\n",
      "epoch no.1 train no.136320  loss = 3.12545 avg_loss = 3.76141\n",
      "epoch no.1 train no.136330  loss = 4.69449 avg_loss = 3.75431\n",
      "epoch no.1 train no.136340  loss = 3.21274 avg_loss = 3.73017\n",
      "epoch no.1 train no.136350  loss = 2.51090 avg_loss = 3.72818\n",
      "epoch no.1 train no.136360  loss = 4.39654 avg_loss = 3.72457\n",
      "epoch no.1 train no.136370  loss = 3.26647 avg_loss = 3.76163\n",
      "epoch no.1 train no.136380  loss = 2.93420 avg_loss = 3.78324\n",
      "epoch no.1 train no.136390  loss = 4.87180 avg_loss = 3.79145\n",
      "epoch no.1 train no.136400  loss = 4.68643 avg_loss = 3.80711\n",
      "epoch no.1 train no.136410  loss = 2.46141 avg_loss = 3.84804\n",
      "epoch no.1 train no.136420  loss = 3.65407 avg_loss = 3.86644\n",
      "epoch no.1 train no.136430  loss = 6.22912 avg_loss = 3.85049\n",
      "epoch no.1 train no.136440  loss = 2.39412 avg_loss = 3.75546\n",
      "epoch no.1 train no.136450  loss = 3.62245 avg_loss = 3.72105\n",
      "epoch no.1 train no.136460  loss = 4.89244 avg_loss = 3.70928\n",
      "epoch no.1 train no.136470  loss = 9.03594 avg_loss = 3.78629\n",
      "epoch no.1 train no.136480  loss = 5.26076 avg_loss = 3.77738\n",
      "epoch no.1 train no.136490  loss = 2.99861 avg_loss = 3.74771\n",
      "epoch no.1 train no.136500  loss = 4.49845 avg_loss = 3.80464\n",
      "epoch no.1 train no.136510  loss = 2.45604 avg_loss = 3.81055\n",
      "epoch no.1 train no.136520  loss = 4.01895 avg_loss = 3.78180\n",
      "epoch no.1 train no.136530  loss = 4.93727 avg_loss = 3.77053\n",
      "epoch no.1 train no.136540  loss = 4.83615 avg_loss = 3.73728\n",
      "epoch no.1 train no.136550  loss = 3.43606 avg_loss = 3.68516\n",
      "epoch no.1 train no.136560  loss = 4.00044 avg_loss = 3.67378\n",
      "epoch no.1 train no.136570  loss = 4.13368 avg_loss = 3.70838\n",
      "epoch no.1 train no.136580  loss = 2.37985 avg_loss = 3.68133\n",
      "epoch no.1 train no.136590  loss = 2.52643 avg_loss = 3.64474\n",
      "epoch no.1 train no.136600  loss = 5.08478 avg_loss = 3.69307\n",
      "epoch no.1 train no.136610  loss = 3.67635 avg_loss = 3.69693\n",
      "epoch no.1 train no.136620  loss = 4.96715 avg_loss = 3.74682\n",
      "epoch no.1 train no.136630  loss = 5.57174 avg_loss = 3.74866\n",
      "epoch no.1 train no.136640  loss = 1.99907 avg_loss = 3.77375\n",
      "epoch no.1 train no.136650  loss = 3.32981 avg_loss = 3.71912\n",
      "epoch no.1 train no.136660  loss = 2.63147 avg_loss = 3.70593\n",
      "epoch no.1 train no.136670  loss = 4.10894 avg_loss = 3.74439\n",
      "epoch no.1 train no.136680  loss = 4.93871 avg_loss = 3.72473\n",
      "epoch no.1 train no.136690  loss = 2.45502 avg_loss = 3.70554\n",
      "epoch no.1 train no.136700  loss = 2.71018 avg_loss = 3.69810\n",
      "epoch no.1 train no.136710  loss = 3.14176 avg_loss = 3.77763\n",
      "epoch no.1 train no.136720  loss = 3.13525 avg_loss = 3.79186\n",
      "epoch no.1 train no.136730  loss = 2.45244 avg_loss = 3.72563\n",
      "epoch no.1 train no.136740  loss = 3.88830 avg_loss = 3.67556\n",
      "epoch no.1 train no.136750  loss = 5.84401 avg_loss = 3.66722\n",
      "epoch no.1 train no.136760  loss = 3.16659 avg_loss = 3.70583\n",
      "epoch no.1 train no.136770  loss = 2.61138 avg_loss = 3.67986\n",
      "epoch no.1 train no.136780  loss = 3.60403 avg_loss = 3.71038\n",
      "epoch no.1 train no.136790  loss = 4.83154 avg_loss = 3.68982\n",
      "epoch no.1 train no.136800  loss = 4.28603 avg_loss = 3.70866\n",
      "epoch no.1 train no.136810  loss = 4.82262 avg_loss = 3.69762\n",
      "epoch no.1 train no.136820  loss = 3.16501 avg_loss = 3.66471\n",
      "epoch no.1 train no.136830  loss = 3.45268 avg_loss = 3.67173\n",
      "epoch no.1 train no.136840  loss = 4.98479 avg_loss = 3.69990\n",
      "epoch no.1 train no.136850  loss = 2.44077 avg_loss = 3.69132\n",
      "epoch no.1 train no.136860  loss = 2.94159 avg_loss = 3.67908\n",
      "epoch no.1 train no.136870  loss = 2.52260 avg_loss = 3.70244\n",
      "epoch no.1 train no.136880  loss = 5.02384 avg_loss = 3.70696\n",
      "epoch no.1 train no.136890  loss = 1.77791 avg_loss = 3.69489\n",
      "epoch no.1 train no.136900  loss = 2.44163 avg_loss = 3.68237\n",
      "epoch no.1 train no.136910  loss = 3.54887 avg_loss = 3.67362\n",
      "epoch no.1 train no.136920  loss = 2.11282 avg_loss = 3.66795\n",
      "epoch no.1 train no.136930  loss = 5.06698 avg_loss = 3.67620\n",
      "epoch no.1 train no.136940  loss = 4.43036 avg_loss = 3.71612\n",
      "epoch no.1 train no.136950  loss = 5.18199 avg_loss = 3.71808\n",
      "epoch no.1 train no.136960  loss = 3.32448 avg_loss = 3.73806\n",
      "epoch no.1 train no.136970  loss = 2.60594 avg_loss = 3.75868\n",
      "epoch no.1 train no.136980  loss = 2.84635 avg_loss = 3.78856\n",
      "epoch no.1 train no.136990  loss = 3.29190 avg_loss = 3.81879\n",
      "epoch no.1 train no.137000  loss = 3.35472 avg_loss = 3.76070\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁싶을', '때', '▁듣는', '▁노래', '음악']\n",
      "기분전환 하고 싶을 때 듣는 인디</s>\n",
      "epoch no.1 train no.137010  loss = 3.58985 avg_loss = 3.76607\n",
      "epoch no.1 train no.137020  loss = 2.95339 avg_loss = 3.78475\n",
      "epoch no.1 train no.137030  loss = 1.69200 avg_loss = 3.73442\n",
      "epoch no.1 train no.137040  loss = 5.03757 avg_loss = 3.72093\n",
      "epoch no.1 train no.137050  loss = 3.67032 avg_loss = 3.66405\n",
      "epoch no.1 train no.137060  loss = 4.10142 avg_loss = 3.67306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.137070  loss = 3.21092 avg_loss = 3.65852\n",
      "epoch no.1 train no.137080  loss = 3.69279 avg_loss = 3.70280\n",
      "epoch no.1 train no.137090  loss = 2.97130 avg_loss = 3.75674\n",
      "epoch no.1 train no.137100  loss = 6.07505 avg_loss = 3.76332\n",
      "epoch no.1 train no.137110  loss = 2.80593 avg_loss = 3.70301\n",
      "epoch no.1 train no.137120  loss = 3.54851 avg_loss = 3.71808\n",
      "epoch no.1 train no.137130  loss = 3.62276 avg_loss = 3.72040\n",
      "epoch no.1 train no.137140  loss = 4.91660 avg_loss = 3.70068\n",
      "epoch no.1 train no.137150  loss = 6.19280 avg_loss = 3.67492\n",
      "epoch no.1 train no.137160  loss = 3.25206 avg_loss = 3.66640\n",
      "epoch no.1 train no.137170  loss = 4.35734 avg_loss = 3.70069\n",
      "epoch no.1 train no.137180  loss = 2.59931 avg_loss = 3.70947\n",
      "epoch no.1 train no.137190  loss = 6.02237 avg_loss = 3.75226\n",
      "epoch no.1 train no.137200  loss = 2.52289 avg_loss = 3.73509\n",
      "epoch no.1 train no.137210  loss = 3.06823 avg_loss = 3.72960\n",
      "epoch no.1 train no.137220  loss = 6.50078 avg_loss = 3.77301\n",
      "epoch no.1 train no.137230  loss = 2.42761 avg_loss = 3.74080\n",
      "epoch no.1 train no.137240  loss = 3.26957 avg_loss = 3.74592\n",
      "epoch no.1 train no.137250  loss = 3.19515 avg_loss = 3.77242\n",
      "epoch no.1 train no.137260  loss = 5.19527 avg_loss = 3.76009\n",
      "epoch no.1 train no.137270  loss = 5.17757 avg_loss = 3.74773\n",
      "epoch no.1 train no.137280  loss = 2.67604 avg_loss = 3.79448\n",
      "epoch no.1 train no.137290  loss = 4.60021 avg_loss = 3.74514\n",
      "epoch no.1 train no.137300  loss = 1.72513 avg_loss = 3.70122\n",
      "epoch no.1 train no.137310  loss = 2.72663 avg_loss = 3.68064\n",
      "epoch no.1 train no.137320  loss = 5.13487 avg_loss = 3.64674\n",
      "epoch no.1 train no.137330  loss = 6.51011 avg_loss = 3.71085\n",
      "epoch no.1 train no.137340  loss = 4.21680 avg_loss = 3.65650\n",
      "epoch no.1 train no.137350  loss = 5.30269 avg_loss = 3.67785\n",
      "epoch no.1 train no.137360  loss = 4.33075 avg_loss = 3.71598\n",
      "epoch no.1 train no.137370  loss = 5.07346 avg_loss = 3.71452\n",
      "epoch no.1 train no.137380  loss = 2.53936 avg_loss = 3.73085\n",
      "epoch no.1 train no.137390  loss = 2.12582 avg_loss = 3.79847\n",
      "epoch no.1 train no.137400  loss = 2.52533 avg_loss = 3.76923\n",
      "epoch no.1 train no.137410  loss = 3.05426 avg_loss = 3.78585\n",
      "epoch no.1 train no.137420  loss = 5.37739 avg_loss = 3.81578\n",
      "epoch no.1 train no.137430  loss = 4.95787 avg_loss = 3.85806\n",
      "epoch no.1 train no.137440  loss = 2.60624 avg_loss = 3.83741\n",
      "epoch no.1 train no.137450  loss = 4.61819 avg_loss = 3.84686\n",
      "epoch no.1 train no.137460  loss = 2.48764 avg_loss = 3.77829\n",
      "epoch no.1 train no.137470  loss = 2.93248 avg_loss = 3.75061\n",
      "epoch no.1 train no.137480  loss = 4.08402 avg_loss = 3.77988\n",
      "epoch no.1 train no.137490  loss = 6.77092 avg_loss = 3.85721\n",
      "epoch no.1 train no.137500  loss = 6.05018 avg_loss = 3.86239\n",
      "epoch no.1 train no.137510  loss = 5.15663 avg_loss = 3.85324\n",
      "epoch no.1 train no.137520  loss = 3.10755 avg_loss = 3.87325\n",
      "epoch no.1 train no.137530  loss = 6.67284 avg_loss = 3.92602\n",
      "epoch no.1 train no.137540  loss = 3.96703 avg_loss = 3.85081\n",
      "epoch no.1 train no.137550  loss = 2.41732 avg_loss = 3.81389\n",
      "epoch no.1 train no.137560  loss = 2.20476 avg_loss = 3.80667\n",
      "epoch no.1 train no.137570  loss = 3.34010 avg_loss = 3.78309\n",
      "epoch no.1 train no.137580  loss = 3.97439 avg_loss = 3.82527\n",
      "epoch no.1 train no.137590  loss = 4.39185 avg_loss = 3.81987\n",
      "epoch no.1 train no.137600  loss = 4.52509 avg_loss = 3.80948\n",
      "epoch no.1 train no.137610  loss = 3.14189 avg_loss = 3.78472\n",
      "epoch no.1 train no.137620  loss = 3.64897 avg_loss = 3.82731\n",
      "epoch no.1 train no.137630  loss = 3.26567 avg_loss = 3.86403\n",
      "epoch no.1 train no.137640  loss = 6.97485 avg_loss = 3.88454\n",
      "epoch no.1 train no.137650  loss = 3.18516 avg_loss = 3.82800\n",
      "epoch no.1 train no.137660  loss = 2.83774 avg_loss = 3.89293\n",
      "epoch no.1 train no.137670  loss = 3.48988 avg_loss = 3.83071\n",
      "epoch no.1 train no.137680  loss = 2.44376 avg_loss = 3.76986\n",
      "epoch no.1 train no.137690  loss = 1.74019 avg_loss = 3.75512\n",
      "epoch no.1 train no.137700  loss = 3.94386 avg_loss = 3.73633\n",
      "epoch no.1 train no.137710  loss = 3.31940 avg_loss = 3.68136\n",
      "epoch no.1 train no.137720  loss = 4.64469 avg_loss = 3.65027\n",
      "epoch no.1 train no.137730  loss = 3.05777 avg_loss = 3.65016\n",
      "epoch no.1 train no.137740  loss = 4.42957 avg_loss = 3.69821\n",
      "epoch no.1 train no.137750  loss = 2.66837 avg_loss = 3.67100\n",
      "epoch no.1 train no.137760  loss = 2.96742 avg_loss = 3.65488\n",
      "epoch no.1 train no.137770  loss = 5.25764 avg_loss = 3.67156\n",
      "epoch no.1 train no.137780  loss = 3.43610 avg_loss = 3.69739\n",
      "epoch no.1 train no.137790  loss = 3.75676 avg_loss = 3.73917\n",
      "epoch no.1 train no.137800  loss = 5.55093 avg_loss = 3.71488\n",
      "epoch no.1 train no.137810  loss = 3.39786 avg_loss = 3.71405\n",
      "epoch no.1 train no.137820  loss = 3.20265 avg_loss = 3.70949\n",
      "epoch no.1 train no.137830  loss = 3.36841 avg_loss = 3.72894\n",
      "epoch no.1 train no.137840  loss = 4.06183 avg_loss = 3.71608\n",
      "epoch no.1 train no.137850  loss = 4.51511 avg_loss = 3.75872\n",
      "epoch no.1 train no.137860  loss = 2.40055 avg_loss = 3.70604\n",
      "epoch no.1 train no.137870  loss = 3.33765 avg_loss = 3.69757\n",
      "epoch no.1 train no.137880  loss = 3.82441 avg_loss = 3.72213\n",
      "epoch no.1 train no.137890  loss = 5.23752 avg_loss = 3.73128\n",
      "epoch no.1 train no.137900  loss = 2.09710 avg_loss = 3.73725\n",
      "epoch no.1 train no.137910  loss = 2.77895 avg_loss = 3.75220\n",
      "epoch no.1 train no.137920  loss = 3.84307 avg_loss = 3.72019\n",
      "epoch no.1 train no.137930  loss = 5.60309 avg_loss = 3.75418\n",
      "epoch no.1 train no.137940  loss = 3.41430 avg_loss = 3.70485\n",
      "epoch no.1 train no.137950  loss = 4.48217 avg_loss = 3.79135\n",
      "epoch no.1 train no.137960  loss = 4.19659 avg_loss = 3.83282\n",
      "epoch no.1 train no.137970  loss = 4.32977 avg_loss = 3.86527\n",
      "epoch no.1 train no.137980  loss = 4.32774 avg_loss = 3.86459\n",
      "epoch no.1 train no.137990  loss = 3.28185 avg_loss = 3.89102\n",
      "epoch no.1 train no.138000  loss = 4.63161 avg_loss = 3.90624\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁싶을', '때', '</s>']\n",
      "기분전환하고 싶을 때</s>\n",
      "epoch no.1 train no.138010  loss = 1.92104 avg_loss = 3.89081\n",
      "epoch no.1 train no.138020  loss = 5.38355 avg_loss = 3.87682\n",
      "epoch no.1 train no.138030  loss = 4.87552 avg_loss = 3.86588\n",
      "epoch no.1 train no.138040  loss = 2.85736 avg_loss = 3.81864\n",
      "epoch no.1 train no.138050  loss = 2.02897 avg_loss = 3.80281\n",
      "epoch no.1 train no.138060  loss = 3.31309 avg_loss = 3.79089\n",
      "epoch no.1 train no.138070  loss = 3.48714 avg_loss = 3.84141\n",
      "epoch no.1 train no.138080  loss = 3.31842 avg_loss = 3.82535\n",
      "epoch no.1 train no.138090  loss = 4.47954 avg_loss = 3.77619\n",
      "epoch no.1 train no.138100  loss = 1.34234 avg_loss = 3.77420\n",
      "epoch no.1 train no.138110  loss = 5.00868 avg_loss = 3.75357\n",
      "epoch no.1 train no.138120  loss = 2.18218 avg_loss = 3.71988\n",
      "epoch no.1 train no.138130  loss = 2.84424 avg_loss = 3.69399\n",
      "epoch no.1 train no.138140  loss = 4.92541 avg_loss = 3.71394\n",
      "epoch no.1 train no.138150  loss = 4.11389 avg_loss = 3.72469\n",
      "epoch no.1 train no.138160  loss = 3.62722 avg_loss = 3.70567\n",
      "epoch no.1 train no.138170  loss = 4.74569 avg_loss = 3.74302\n",
      "epoch no.1 train no.138180  loss = 3.37004 avg_loss = 3.71731\n",
      "epoch no.1 train no.138190  loss = 2.55545 avg_loss = 3.73806\n",
      "epoch no.1 train no.138200  loss = 3.82124 avg_loss = 3.75289\n",
      "epoch no.1 train no.138210  loss = 2.00781 avg_loss = 3.65252\n",
      "epoch no.1 train no.138220  loss = 2.28846 avg_loss = 3.64456\n",
      "epoch no.1 train no.138230  loss = 2.93689 avg_loss = 3.68119\n",
      "epoch no.1 train no.138240  loss = 5.52895 avg_loss = 3.69640\n",
      "epoch no.1 train no.138250  loss = 3.45429 avg_loss = 3.70476\n",
      "epoch no.1 train no.138260  loss = 4.71720 avg_loss = 3.77992\n",
      "epoch no.1 train no.138270  loss = 5.84571 avg_loss = 3.83663\n",
      "epoch no.1 train no.138280  loss = 4.37484 avg_loss = 3.86858\n",
      "epoch no.1 train no.138290  loss = 4.23833 avg_loss = 3.92075\n",
      "epoch no.1 train no.138300  loss = 3.51252 avg_loss = 3.93252\n",
      "epoch no.1 train no.138310  loss = 1.62087 avg_loss = 3.94464\n",
      "epoch no.1 train no.138320  loss = 4.55476 avg_loss = 3.99303\n",
      "epoch no.1 train no.138330  loss = 2.99286 avg_loss = 4.00513\n",
      "epoch no.1 train no.138340  loss = 5.99100 avg_loss = 3.99490\n",
      "epoch no.1 train no.138350  loss = 5.32832 avg_loss = 4.00551\n",
      "epoch no.1 train no.138360  loss = 3.31207 avg_loss = 4.07154\n",
      "epoch no.1 train no.138370  loss = 3.08075 avg_loss = 4.06532\n",
      "epoch no.1 train no.138380  loss = 2.36362 avg_loss = 4.02463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.138390  loss = 3.79586 avg_loss = 4.02433\n",
      "epoch no.1 train no.138400  loss = 2.26179 avg_loss = 4.09204\n",
      "epoch no.1 train no.138410  loss = 4.47974 avg_loss = 4.08524\n",
      "epoch no.1 train no.138420  loss = 3.81283 avg_loss = 4.02396\n",
      "epoch no.1 train no.138430  loss = 4.03692 avg_loss = 3.97067\n",
      "epoch no.1 train no.138440  loss = 4.21083 avg_loss = 3.99354\n",
      "epoch no.1 train no.138450  loss = 1.41470 avg_loss = 3.99754\n",
      "epoch no.1 train no.138460  loss = 3.60217 avg_loss = 3.97464\n",
      "epoch no.1 train no.138470  loss = 1.81718 avg_loss = 3.91674\n",
      "epoch no.1 train no.138480  loss = 4.82191 avg_loss = 3.87517\n",
      "epoch no.1 train no.138490  loss = 5.10620 avg_loss = 3.90748\n",
      "epoch no.1 train no.138500  loss = 4.02764 avg_loss = 3.89540\n",
      "epoch no.1 train no.138510  loss = 4.04900 avg_loss = 3.87142\n",
      "epoch no.1 train no.138520  loss = 4.32456 avg_loss = 3.89951\n",
      "epoch no.1 train no.138530  loss = 6.42617 avg_loss = 3.96632\n",
      "epoch no.1 train no.138540  loss = 3.69901 avg_loss = 3.97387\n",
      "epoch no.1 train no.138550  loss = 1.34230 avg_loss = 3.95480\n",
      "epoch no.1 train no.138560  loss = 2.40245 avg_loss = 3.91633\n",
      "epoch no.1 train no.138570  loss = 4.00416 avg_loss = 3.89606\n",
      "epoch no.1 train no.138580  loss = 2.41129 avg_loss = 3.85521\n",
      "epoch no.1 train no.138590  loss = 2.83142 avg_loss = 3.83072\n",
      "epoch no.1 train no.138600  loss = 5.73380 avg_loss = 3.92126\n",
      "epoch no.1 train no.138610  loss = 3.99247 avg_loss = 3.90202\n",
      "epoch no.1 train no.138620  loss = 1.89062 avg_loss = 3.86851\n",
      "epoch no.1 train no.138630  loss = 3.20010 avg_loss = 3.86633\n",
      "epoch no.1 train no.138640  loss = 2.86388 avg_loss = 3.84873\n",
      "epoch no.1 train no.138650  loss = 4.24675 avg_loss = 3.89102\n",
      "epoch no.1 train no.138660  loss = 3.81961 avg_loss = 3.89669\n",
      "epoch no.1 train no.138670  loss = 3.05662 avg_loss = 3.95643\n",
      "epoch no.1 train no.138680  loss = 4.02283 avg_loss = 3.87879\n",
      "epoch no.1 train no.138690  loss = 5.17460 avg_loss = 3.89349\n",
      "epoch no.1 train no.138700  loss = 4.94190 avg_loss = 3.85591\n",
      "epoch no.1 train no.138710  loss = 3.43820 avg_loss = 3.83283\n",
      "epoch no.1 train no.138720  loss = 4.30044 avg_loss = 3.77799\n",
      "epoch no.1 train no.138730  loss = 2.72182 avg_loss = 3.77069\n",
      "epoch no.1 train no.138740  loss = 3.70187 avg_loss = 3.76405\n",
      "epoch no.1 train no.138750  loss = 3.50649 avg_loss = 3.78588\n",
      "epoch no.1 train no.138760  loss = 4.04185 avg_loss = 3.76320\n",
      "epoch no.1 train no.138770  loss = 4.71251 avg_loss = 3.80078\n",
      "epoch no.1 train no.138780  loss = 2.51345 avg_loss = 3.74568\n",
      "epoch no.1 train no.138790  loss = 3.70440 avg_loss = 3.72292\n",
      "epoch no.1 train no.138800  loss = 1.83966 avg_loss = 3.73362\n",
      "epoch no.1 train no.138810  loss = 2.00312 avg_loss = 3.79400\n",
      "epoch no.1 train no.138820  loss = 5.42736 avg_loss = 3.78526\n",
      "epoch no.1 train no.138830  loss = 2.63030 avg_loss = 3.71047\n",
      "epoch no.1 train no.138840  loss = 4.63217 avg_loss = 3.70922\n",
      "epoch no.1 train no.138850  loss = 4.74941 avg_loss = 3.76584\n",
      "epoch no.1 train no.138860  loss = 1.77344 avg_loss = 3.73738\n",
      "epoch no.1 train no.138870  loss = 2.87544 avg_loss = 3.73174\n",
      "epoch no.1 train no.138880  loss = 2.89983 avg_loss = 3.72791\n",
      "epoch no.1 train no.138890  loss = 4.18886 avg_loss = 3.77119\n",
      "epoch no.1 train no.138900  loss = 3.40766 avg_loss = 3.73351\n",
      "epoch no.1 train no.138910  loss = 2.92638 avg_loss = 3.71122\n",
      "epoch no.1 train no.138920  loss = 2.53765 avg_loss = 3.69379\n",
      "epoch no.1 train no.138930  loss = 3.25878 avg_loss = 3.71491\n",
      "epoch no.1 train no.138940  loss = 2.80148 avg_loss = 3.65982\n",
      "epoch no.1 train no.138950  loss = 3.71302 avg_loss = 3.67068\n",
      "epoch no.1 train no.138960  loss = 4.33911 avg_loss = 3.67633\n",
      "epoch no.1 train no.138970  loss = 2.80867 avg_loss = 3.65658\n",
      "epoch no.1 train no.138980  loss = 6.03510 avg_loss = 3.69892\n",
      "epoch no.1 train no.138990  loss = 3.58117 avg_loss = 3.73939\n",
      "epoch no.1 train no.139000  loss = 5.46115 avg_loss = 3.74030\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.139010  loss = 2.77754 avg_loss = 3.75205\n",
      "epoch no.1 train no.139020  loss = 4.09951 avg_loss = 3.72057\n",
      "epoch no.1 train no.139030  loss = 5.25508 avg_loss = 3.71566\n",
      "epoch no.1 train no.139040  loss = 5.12747 avg_loss = 3.70552\n",
      "epoch no.1 train no.139050  loss = 3.34133 avg_loss = 3.68222\n",
      "epoch no.1 train no.139060  loss = 3.00187 avg_loss = 3.65075\n",
      "epoch no.1 train no.139070  loss = 3.17435 avg_loss = 3.65607\n",
      "epoch no.1 train no.139080  loss = 4.38835 avg_loss = 3.67903\n",
      "epoch no.1 train no.139090  loss = 5.76654 avg_loss = 3.68830\n",
      "epoch no.1 train no.139100  loss = 1.88663 avg_loss = 3.66940\n",
      "epoch no.1 train no.139110  loss = 3.64470 avg_loss = 3.68738\n",
      "epoch no.1 train no.139120  loss = 2.91325 avg_loss = 3.65178\n",
      "epoch no.1 train no.139130  loss = 2.86037 avg_loss = 3.63985\n",
      "epoch no.1 train no.139140  loss = 2.99373 avg_loss = 3.63704\n",
      "epoch no.1 train no.139150  loss = 4.49369 avg_loss = 3.65933\n",
      "epoch no.1 train no.139160  loss = 3.09124 avg_loss = 3.62340\n",
      "epoch no.1 train no.139170  loss = 2.64346 avg_loss = 3.59948\n",
      "epoch no.1 train no.139180  loss = 3.31401 avg_loss = 3.61051\n",
      "epoch no.1 train no.139190  loss = 2.30038 avg_loss = 3.67083\n",
      "epoch no.1 train no.139200  loss = 2.36174 avg_loss = 3.63511\n",
      "epoch no.1 train no.139210  loss = 3.41333 avg_loss = 3.63402\n",
      "epoch no.1 train no.139220  loss = 4.94882 avg_loss = 3.63307\n",
      "epoch no.1 train no.139230  loss = 3.59105 avg_loss = 3.72841\n",
      "epoch no.1 train no.139240  loss = 2.57392 avg_loss = 3.68522\n",
      "epoch no.1 train no.139250  loss = 4.28451 avg_loss = 3.75842\n",
      "epoch no.1 train no.139260  loss = 3.20092 avg_loss = 3.77502\n",
      "epoch no.1 train no.139270  loss = 4.20742 avg_loss = 3.80410\n",
      "epoch no.1 train no.139280  loss = 3.74267 avg_loss = 3.80401\n",
      "epoch no.1 train no.139290  loss = 2.41984 avg_loss = 3.78749\n",
      "epoch no.1 train no.139300  loss = 4.18764 avg_loss = 3.72061\n",
      "epoch no.1 train no.139310  loss = 3.27891 avg_loss = 3.75816\n",
      "epoch no.1 train no.139320  loss = 4.38429 avg_loss = 3.74262\n",
      "epoch no.1 train no.139330  loss = 2.09574 avg_loss = 3.72626\n",
      "epoch no.1 train no.139340  loss = 4.75934 avg_loss = 3.72576\n",
      "epoch no.1 train no.139350  loss = 3.23497 avg_loss = 3.69142\n",
      "epoch no.1 train no.139360  loss = 3.77206 avg_loss = 3.66991\n",
      "epoch no.1 train no.139370  loss = 2.89722 avg_loss = 3.64377\n",
      "epoch no.1 train no.139380  loss = 4.65643 avg_loss = 3.64882\n",
      "epoch no.1 train no.139390  loss = 3.73157 avg_loss = 3.66543\n",
      "epoch no.1 train no.139400  loss = 3.29737 avg_loss = 3.61616\n",
      "epoch no.1 train no.139410  loss = 3.70196 avg_loss = 3.69374\n",
      "epoch no.1 train no.139420  loss = 4.18387 avg_loss = 3.72714\n",
      "epoch no.1 train no.139430  loss = 3.16071 avg_loss = 3.77680\n",
      "epoch no.1 train no.139440  loss = 6.05991 avg_loss = 3.80265\n",
      "epoch no.1 train no.139450  loss = 4.28094 avg_loss = 3.79917\n",
      "epoch no.1 train no.139460  loss = 3.94530 avg_loss = 3.78851\n",
      "epoch no.1 train no.139470  loss = 3.83828 avg_loss = 3.83393\n",
      "epoch no.1 train no.139480  loss = 4.68118 avg_loss = 3.80773\n",
      "epoch no.1 train no.139490  loss = 4.14135 avg_loss = 3.80156\n",
      "epoch no.1 train no.139500  loss = 3.76475 avg_loss = 3.78280\n",
      "epoch no.1 train no.139510  loss = 1.39876 avg_loss = 3.73364\n",
      "epoch no.1 train no.139520  loss = 2.90987 avg_loss = 3.69214\n",
      "epoch no.1 train no.139530  loss = 3.03963 avg_loss = 3.72958\n",
      "epoch no.1 train no.139540  loss = 4.35421 avg_loss = 3.75034\n",
      "epoch no.1 train no.139550  loss = 2.68025 avg_loss = 3.74807\n",
      "epoch no.1 train no.139560  loss = 2.44380 avg_loss = 3.71189\n",
      "epoch no.1 train no.139570  loss = 3.70528 avg_loss = 3.73205\n",
      "epoch no.1 train no.139580  loss = 2.92265 avg_loss = 3.77866\n",
      "epoch no.1 train no.139590  loss = 1.91598 avg_loss = 3.79564\n",
      "epoch no.1 train no.139600  loss = 3.82139 avg_loss = 3.75021\n",
      "epoch no.1 train no.139610  loss = 3.91896 avg_loss = 3.72905\n",
      "epoch no.1 train no.139620  loss = 3.72489 avg_loss = 3.70960\n",
      "epoch no.1 train no.139630  loss = 3.72976 avg_loss = 3.72146\n",
      "epoch no.1 train no.139640  loss = 2.23794 avg_loss = 3.78462\n",
      "epoch no.1 train no.139650  loss = 4.46041 avg_loss = 3.72094\n",
      "epoch no.1 train no.139660  loss = 4.43183 avg_loss = 3.70413\n",
      "epoch no.1 train no.139670  loss = 3.92996 avg_loss = 3.74959\n",
      "epoch no.1 train no.139680  loss = 3.23114 avg_loss = 3.73207\n",
      "epoch no.1 train no.139690  loss = 3.87763 avg_loss = 3.76682\n",
      "epoch no.1 train no.139700  loss = 3.68921 avg_loss = 3.76990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.139710  loss = 3.11270 avg_loss = 3.78340\n",
      "epoch no.1 train no.139720  loss = 3.66306 avg_loss = 3.81409\n",
      "epoch no.1 train no.139730  loss = 3.91563 avg_loss = 3.82239\n",
      "epoch no.1 train no.139740  loss = 4.35597 avg_loss = 3.81483\n",
      "epoch no.1 train no.139750  loss = 1.83724 avg_loss = 3.82873\n",
      "epoch no.1 train no.139760  loss = 2.42769 avg_loss = 3.80761\n",
      "epoch no.1 train no.139770  loss = 3.94963 avg_loss = 3.80890\n",
      "epoch no.1 train no.139780  loss = 3.98443 avg_loss = 3.81145\n",
      "epoch no.1 train no.139790  loss = 3.65432 avg_loss = 3.81595\n",
      "epoch no.1 train no.139800  loss = 3.10740 avg_loss = 3.78332\n",
      "epoch no.1 train no.139810  loss = 4.12401 avg_loss = 3.82747\n",
      "epoch no.1 train no.139820  loss = 3.27849 avg_loss = 3.90484\n",
      "epoch no.1 train no.139830  loss = 4.26072 avg_loss = 3.90906\n",
      "epoch no.1 train no.139840  loss = 4.90769 avg_loss = 3.92485\n",
      "epoch no.1 train no.139850  loss = 4.10296 avg_loss = 3.85290\n",
      "epoch no.1 train no.139860  loss = 3.56098 avg_loss = 3.85892\n",
      "epoch no.1 train no.139870  loss = 4.28391 avg_loss = 3.83600\n",
      "epoch no.1 train no.139880  loss = 3.35204 avg_loss = 3.86886\n",
      "epoch no.1 train no.139890  loss = 4.33507 avg_loss = 3.88275\n",
      "epoch no.1 train no.139900  loss = 4.54600 avg_loss = 3.94237\n",
      "epoch no.1 train no.139910  loss = 2.30295 avg_loss = 3.90106\n",
      "epoch no.1 train no.139920  loss = 4.46895 avg_loss = 3.90970\n",
      "epoch no.1 train no.139930  loss = 1.17108 avg_loss = 3.90567\n",
      "epoch no.1 train no.139940  loss = 2.54392 avg_loss = 3.89224\n",
      "epoch no.1 train no.139950  loss = 3.83804 avg_loss = 3.85386\n",
      "epoch no.1 train no.139960  loss = 3.63650 avg_loss = 3.86847\n",
      "epoch no.1 train no.139970  loss = 4.98506 avg_loss = 3.88405\n",
      "epoch no.1 train no.139980  loss = 2.67250 avg_loss = 3.81451\n",
      "epoch no.1 train no.139990  loss = 4.17886 avg_loss = 3.81827\n",
      "epoch no.1 train no.140000  loss = 5.06275 avg_loss = 3.82595\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.140010  loss = 4.99558 avg_loss = 3.80454\n",
      "epoch no.1 train no.140020  loss = 3.20312 avg_loss = 3.77754\n",
      "epoch no.1 train no.140030  loss = 4.26622 avg_loss = 3.74401\n",
      "epoch no.1 train no.140040  loss = 3.72761 avg_loss = 3.73679\n",
      "epoch no.1 train no.140050  loss = 3.75527 avg_loss = 3.76629\n",
      "epoch no.1 train no.140060  loss = 2.82811 avg_loss = 3.78557\n",
      "epoch no.1 train no.140070  loss = 3.04454 avg_loss = 3.72291\n",
      "epoch no.1 train no.140080  loss = 2.61221 avg_loss = 3.73570\n",
      "epoch no.1 train no.140090  loss = 5.72977 avg_loss = 3.77624\n",
      "epoch no.1 train no.140100  loss = 4.87860 avg_loss = 3.82765\n",
      "epoch no.1 train no.140110  loss = 3.00500 avg_loss = 3.80419\n",
      "epoch no.1 train no.140120  loss = 3.49792 avg_loss = 3.79480\n",
      "epoch no.1 train no.140130  loss = 2.05640 avg_loss = 3.72829\n",
      "epoch no.1 train no.140140  loss = 2.34202 avg_loss = 3.73780\n",
      "epoch no.1 train no.140150  loss = 2.88372 avg_loss = 3.68744\n",
      "epoch no.1 train no.140160  loss = 2.70817 avg_loss = 3.69333\n",
      "epoch no.1 train no.140170  loss = 3.57892 avg_loss = 3.70390\n",
      "epoch no.1 train no.140180  loss = 4.62289 avg_loss = 3.73985\n",
      "epoch no.1 train no.140190  loss = 3.52335 avg_loss = 3.71398\n",
      "epoch no.1 train no.140200  loss = 2.67337 avg_loss = 3.74133\n",
      "epoch no.1 train no.140210  loss = 1.80716 avg_loss = 3.69753\n",
      "epoch no.1 train no.140220  loss = 2.27468 avg_loss = 3.65016\n",
      "epoch no.1 train no.140230  loss = 4.30377 avg_loss = 3.75671\n",
      "epoch no.1 train no.140240  loss = 4.38441 avg_loss = 3.72250\n",
      "epoch no.1 train no.140250  loss = 1.85206 avg_loss = 3.73789\n",
      "epoch no.1 train no.140260  loss = 2.55239 avg_loss = 3.80450\n",
      "epoch no.1 train no.140270  loss = 3.69798 avg_loss = 3.77739\n",
      "epoch no.1 train no.140280  loss = 2.21220 avg_loss = 3.74306\n",
      "epoch no.1 train no.140290  loss = 3.99587 avg_loss = 3.74709\n",
      "epoch no.1 train no.140300  loss = 4.67397 avg_loss = 3.74278\n",
      "epoch no.1 train no.140310  loss = 4.12047 avg_loss = 3.72239\n",
      "epoch no.1 train no.140320  loss = 2.79093 avg_loss = 3.72525\n",
      "epoch no.1 train no.140330  loss = 3.01321 avg_loss = 3.71166\n",
      "epoch no.1 train no.140340  loss = 2.63982 avg_loss = 3.75300\n",
      "epoch no.1 train no.140350  loss = 4.70877 avg_loss = 3.70293\n",
      "epoch no.1 train no.140360  loss = 3.39576 avg_loss = 3.69223\n",
      "epoch no.1 train no.140370  loss = 3.97589 avg_loss = 3.72111\n",
      "epoch no.1 train no.140380  loss = 3.88673 avg_loss = 3.74488\n",
      "epoch no.1 train no.140390  loss = 3.03531 avg_loss = 3.75980\n",
      "epoch no.1 train no.140400  loss = 4.78100 avg_loss = 3.76127\n",
      "epoch no.1 train no.140410  loss = 4.07914 avg_loss = 3.78393\n",
      "epoch no.1 train no.140420  loss = 3.27163 avg_loss = 3.83008\n",
      "epoch no.1 train no.140430  loss = 4.30354 avg_loss = 3.83461\n",
      "epoch no.1 train no.140440  loss = 5.08941 avg_loss = 3.85553\n",
      "epoch no.1 train no.140450  loss = 3.16096 avg_loss = 3.79682\n",
      "epoch no.1 train no.140460  loss = 2.49221 avg_loss = 3.75346\n",
      "epoch no.1 train no.140470  loss = 6.53268 avg_loss = 3.79934\n",
      "epoch no.1 train no.140480  loss = 4.21327 avg_loss = 3.79599\n",
      "epoch no.1 train no.140490  loss = 3.49008 avg_loss = 3.78549\n",
      "epoch no.1 train no.140500  loss = 4.49938 avg_loss = 3.79152\n",
      "epoch no.1 train no.140510  loss = 4.57506 avg_loss = 3.88799\n",
      "epoch no.1 train no.140520  loss = 3.86685 avg_loss = 3.91366\n",
      "epoch no.1 train no.140530  loss = 4.40019 avg_loss = 3.89365\n",
      "epoch no.1 train no.140540  loss = 6.22026 avg_loss = 3.92094\n",
      "epoch no.1 train no.140550  loss = 3.89424 avg_loss = 3.88126\n",
      "epoch no.1 train no.140560  loss = 3.60433 avg_loss = 3.95613\n",
      "epoch no.1 train no.140570  loss = 4.00039 avg_loss = 4.00082\n",
      "epoch no.1 train no.140580  loss = 4.79919 avg_loss = 4.02396\n",
      "epoch no.1 train no.140590  loss = 3.53839 avg_loss = 3.99212\n",
      "epoch no.1 train no.140600  loss = 2.46881 avg_loss = 3.94541\n",
      "epoch no.1 train no.140610  loss = 4.79007 avg_loss = 3.93177\n",
      "epoch no.1 train no.140620  loss = 5.64536 avg_loss = 3.92788\n",
      "epoch no.1 train no.140630  loss = 2.42489 avg_loss = 3.81694\n",
      "epoch no.1 train no.140640  loss = 4.00171 avg_loss = 3.78717\n",
      "epoch no.1 train no.140650  loss = 2.53770 avg_loss = 3.82374\n",
      "epoch no.1 train no.140660  loss = 5.88360 avg_loss = 3.81223\n",
      "epoch no.1 train no.140670  loss = 4.52437 avg_loss = 3.80713\n",
      "epoch no.1 train no.140680  loss = 3.93558 avg_loss = 3.85830\n",
      "epoch no.1 train no.140690  loss = 3.99925 avg_loss = 3.81573\n",
      "epoch no.1 train no.140700  loss = 3.54496 avg_loss = 3.80619\n",
      "epoch no.1 train no.140710  loss = 3.91254 avg_loss = 3.84083\n",
      "epoch no.1 train no.140720  loss = 4.81329 avg_loss = 3.88749\n",
      "epoch no.1 train no.140730  loss = 6.36628 avg_loss = 3.93783\n",
      "epoch no.1 train no.140740  loss = 5.13304 avg_loss = 3.91402\n",
      "epoch no.1 train no.140750  loss = 3.29097 avg_loss = 3.87606\n",
      "epoch no.1 train no.140760  loss = 4.33094 avg_loss = 3.84268\n",
      "epoch no.1 train no.140770  loss = 4.25046 avg_loss = 3.83288\n",
      "epoch no.1 train no.140780  loss = 5.67098 avg_loss = 3.87917\n",
      "epoch no.1 train no.140790  loss = 3.61182 avg_loss = 3.90441\n",
      "epoch no.1 train no.140800  loss = 2.92607 avg_loss = 3.89427\n",
      "epoch no.1 train no.140810  loss = 2.01008 avg_loss = 3.84935\n",
      "epoch no.1 train no.140820  loss = 3.49940 avg_loss = 3.83208\n",
      "epoch no.1 train no.140830  loss = 4.43133 avg_loss = 3.82676\n",
      "epoch no.1 train no.140840  loss = 4.15575 avg_loss = 3.88203\n",
      "epoch no.1 train no.140850  loss = 3.22442 avg_loss = 3.89108\n",
      "epoch no.1 train no.140860  loss = 3.37413 avg_loss = 3.85327\n",
      "epoch no.1 train no.140870  loss = 2.89151 avg_loss = 3.86551\n",
      "epoch no.1 train no.140880  loss = 3.71349 avg_loss = 3.89107\n",
      "epoch no.1 train no.140890  loss = 3.32903 avg_loss = 3.87410\n",
      "epoch no.1 train no.140900  loss = 3.82652 avg_loss = 3.85443\n",
      "epoch no.1 train no.140910  loss = 5.46355 avg_loss = 3.88241\n",
      "epoch no.1 train no.140920  loss = 2.84104 avg_loss = 3.86826\n",
      "epoch no.1 train no.140930  loss = 5.85169 avg_loss = 3.96075\n",
      "epoch no.1 train no.140940  loss = 2.51859 avg_loss = 3.96300\n",
      "epoch no.1 train no.140950  loss = 6.55391 avg_loss = 3.98146\n",
      "epoch no.1 train no.140960  loss = 4.21080 avg_loss = 3.91615\n",
      "epoch no.1 train no.140970  loss = 5.09647 avg_loss = 3.92261\n",
      "epoch no.1 train no.140980  loss = 3.99067 avg_loss = 3.88778\n",
      "epoch no.1 train no.140990  loss = 3.52510 avg_loss = 3.92716\n",
      "epoch no.1 train no.141000  loss = 4.09626 avg_loss = 3.89020\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁싶을', '때', '</s>']\n",
      "기분전환하고 싶을때</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.141010  loss = 4.36301 avg_loss = 3.86425\n",
      "epoch no.1 train no.141020  loss = 6.07586 avg_loss = 3.86208\n",
      "epoch no.1 train no.141030  loss = 4.27447 avg_loss = 3.86737\n",
      "epoch no.1 train no.141040  loss = 5.36533 avg_loss = 3.87227\n",
      "epoch no.1 train no.141050  loss = 2.84504 avg_loss = 3.83674\n",
      "epoch no.1 train no.141060  loss = 4.74304 avg_loss = 3.81453\n",
      "epoch no.1 train no.141070  loss = 2.90178 avg_loss = 3.74936\n",
      "epoch no.1 train no.141080  loss = 3.63658 avg_loss = 3.73886\n",
      "epoch no.1 train no.141090  loss = 4.24110 avg_loss = 3.76568\n",
      "epoch no.1 train no.141100  loss = 3.21306 avg_loss = 3.76670\n",
      "epoch no.1 train no.141110  loss = 4.11371 avg_loss = 3.77322\n",
      "epoch no.1 train no.141120  loss = 3.68982 avg_loss = 3.76685\n",
      "epoch no.1 train no.141130  loss = 2.94767 avg_loss = 3.80882\n",
      "epoch no.1 train no.141140  loss = 5.74199 avg_loss = 3.82565\n",
      "epoch no.1 train no.141150  loss = 3.53908 avg_loss = 3.86876\n",
      "epoch no.1 train no.141160  loss = 4.65072 avg_loss = 3.86519\n",
      "epoch no.1 train no.141170  loss = 3.86937 avg_loss = 3.84563\n",
      "epoch no.1 train no.141180  loss = 2.95583 avg_loss = 3.83511\n",
      "epoch no.1 train no.141190  loss = 3.84657 avg_loss = 3.83134\n",
      "epoch no.1 train no.141200  loss = 5.00463 avg_loss = 3.82573\n",
      "epoch no.1 train no.141210  loss = 3.22607 avg_loss = 3.86682\n",
      "epoch no.1 train no.141220  loss = 4.64573 avg_loss = 3.88714\n",
      "epoch no.1 train no.141230  loss = 3.96841 avg_loss = 3.94138\n",
      "epoch no.1 train no.141240  loss = 3.39150 avg_loss = 3.98764\n",
      "epoch no.1 train no.141250  loss = 3.87323 avg_loss = 3.98600\n",
      "epoch no.1 train no.141260  loss = 4.12784 avg_loss = 3.90964\n",
      "epoch no.1 train no.141270  loss = 2.58328 avg_loss = 3.93073\n",
      "epoch no.1 train no.141280  loss = 3.63634 avg_loss = 3.89733\n",
      "epoch no.1 train no.141290  loss = 2.44949 avg_loss = 3.86475\n",
      "epoch no.1 train no.141300  loss = 1.93518 avg_loss = 3.92340\n",
      "epoch no.1 train no.141310  loss = 2.65984 avg_loss = 3.84781\n",
      "epoch no.1 train no.141320  loss = 3.12069 avg_loss = 3.80637\n",
      "epoch no.1 train no.141330  loss = 6.16808 avg_loss = 3.85719\n",
      "epoch no.1 train no.141340  loss = 3.56588 avg_loss = 3.89056\n",
      "epoch no.1 train no.141350  loss = 4.60573 avg_loss = 3.95388\n",
      "epoch no.1 train no.141360  loss = 5.22466 avg_loss = 3.92069\n",
      "epoch no.1 train no.141370  loss = 3.65258 avg_loss = 3.89135\n",
      "epoch no.1 train no.141380  loss = 3.20582 avg_loss = 3.92259\n",
      "epoch no.1 train no.141390  loss = 2.62986 avg_loss = 3.87685\n",
      "epoch no.1 train no.141400  loss = 2.85559 avg_loss = 3.88648\n",
      "epoch no.1 train no.141410  loss = 5.81579 avg_loss = 3.91646\n",
      "epoch no.1 train no.141420  loss = 3.95514 avg_loss = 3.84021\n",
      "epoch no.1 train no.141430  loss = 4.67416 avg_loss = 3.84720\n",
      "epoch no.1 train no.141440  loss = 3.88745 avg_loss = 3.89875\n",
      "epoch no.1 train no.141450  loss = 4.14320 avg_loss = 3.94459\n",
      "epoch no.1 train no.141460  loss = 4.02648 avg_loss = 3.97489\n",
      "epoch no.1 train no.141470  loss = 4.20229 avg_loss = 3.95593\n",
      "epoch no.1 train no.141480  loss = 3.95484 avg_loss = 4.00730\n",
      "epoch no.1 train no.141490  loss = 2.45533 avg_loss = 3.97118\n",
      "epoch no.1 train no.141500  loss = 3.44691 avg_loss = 3.99405\n",
      "epoch no.1 train no.141510  loss = 2.51593 avg_loss = 3.97128\n",
      "epoch no.1 train no.141520  loss = 3.02930 avg_loss = 3.92542\n",
      "epoch no.1 train no.141530  loss = 4.16418 avg_loss = 3.89493\n",
      "epoch no.1 train no.141540  loss = 3.87551 avg_loss = 3.85764\n",
      "epoch no.1 train no.141550  loss = 3.80533 avg_loss = 3.80478\n",
      "epoch no.1 train no.141560  loss = 4.32545 avg_loss = 3.77099\n",
      "epoch no.1 train no.141570  loss = 2.87854 avg_loss = 3.73167\n",
      "epoch no.1 train no.141580  loss = 4.95977 avg_loss = 3.70719\n",
      "epoch no.1 train no.141590  loss = 4.77531 avg_loss = 3.72952\n",
      "epoch no.1 train no.141600  loss = 4.99137 avg_loss = 3.74437\n",
      "epoch no.1 train no.141610  loss = 3.50287 avg_loss = 3.69790\n",
      "epoch no.1 train no.141620  loss = 3.25612 avg_loss = 3.67806\n",
      "epoch no.1 train no.141630  loss = 2.76140 avg_loss = 3.72175\n",
      "epoch no.1 train no.141640  loss = 2.84985 avg_loss = 3.74322\n",
      "epoch no.1 train no.141650  loss = 3.35622 avg_loss = 3.74155\n",
      "epoch no.1 train no.141660  loss = 4.53946 avg_loss = 3.77558\n",
      "epoch no.1 train no.141670  loss = 4.48179 avg_loss = 3.80494\n",
      "epoch no.1 train no.141680  loss = 3.35342 avg_loss = 3.79934\n",
      "epoch no.1 train no.141690  loss = 3.29874 avg_loss = 3.83438\n",
      "epoch no.1 train no.141700  loss = 4.64871 avg_loss = 3.82478\n",
      "epoch no.1 train no.141710  loss = 2.49764 avg_loss = 3.85963\n",
      "epoch no.1 train no.141720  loss = 3.72998 avg_loss = 3.87888\n",
      "epoch no.1 train no.141730  loss = 3.73613 avg_loss = 3.83939\n",
      "epoch no.1 train no.141740  loss = 3.15459 avg_loss = 3.80895\n",
      "epoch no.1 train no.141750  loss = 2.31875 avg_loss = 3.75112\n",
      "epoch no.1 train no.141760  loss = 2.24761 avg_loss = 3.73329\n",
      "epoch no.1 train no.141770  loss = 5.53845 avg_loss = 3.77815\n",
      "epoch no.1 train no.141780  loss = 3.61437 avg_loss = 3.77072\n",
      "epoch no.1 train no.141790  loss = 6.59392 avg_loss = 3.80814\n",
      "epoch no.1 train no.141800  loss = 5.05565 avg_loss = 3.83543\n",
      "epoch no.1 train no.141810  loss = 3.13484 avg_loss = 3.87703\n",
      "epoch no.1 train no.141820  loss = 5.32431 avg_loss = 3.87351\n",
      "epoch no.1 train no.141830  loss = 3.58989 avg_loss = 3.83106\n",
      "epoch no.1 train no.141840  loss = 2.31714 avg_loss = 3.78685\n",
      "epoch no.1 train no.141850  loss = 4.00704 avg_loss = 3.75768\n",
      "epoch no.1 train no.141860  loss = 3.25814 avg_loss = 3.77991\n",
      "epoch no.1 train no.141870  loss = 6.80712 avg_loss = 3.89659\n",
      "epoch no.1 train no.141880  loss = 4.00753 avg_loss = 3.92627\n",
      "epoch no.1 train no.141890  loss = 2.46652 avg_loss = 3.96138\n",
      "epoch no.1 train no.141900  loss = 2.38591 avg_loss = 3.92780\n",
      "epoch no.1 train no.141910  loss = 1.75564 avg_loss = 3.88846\n",
      "epoch no.1 train no.141920  loss = 4.86408 avg_loss = 3.87383\n",
      "epoch no.1 train no.141930  loss = 2.71479 avg_loss = 3.81073\n",
      "epoch no.1 train no.141940  loss = 2.80145 avg_loss = 3.81323\n",
      "epoch no.1 train no.141950  loss = 3.36286 avg_loss = 3.78917\n",
      "epoch no.1 train no.141960  loss = 7.35188 avg_loss = 3.84453\n",
      "epoch no.1 train no.141970  loss = 5.19971 avg_loss = 3.86470\n",
      "epoch no.1 train no.141980  loss = 6.06389 avg_loss = 3.86936\n",
      "epoch no.1 train no.141990  loss = 3.68422 avg_loss = 3.88342\n",
      "epoch no.1 train no.142000  loss = 4.37407 avg_loss = 3.85706\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.1 train no.142010  loss = 4.99534 avg_loss = 3.83469\n",
      "epoch no.1 train no.142020  loss = 4.07178 avg_loss = 3.84948\n",
      "epoch no.1 train no.142030  loss = 3.10953 avg_loss = 3.84445\n",
      "epoch no.1 train no.142040  loss = 1.86037 avg_loss = 3.81694\n",
      "epoch no.1 train no.142050  loss = 3.36581 avg_loss = 3.81233\n",
      "epoch no.1 train no.142060  loss = 5.42846 avg_loss = 3.81496\n",
      "epoch no.1 train no.142070  loss = 3.46179 avg_loss = 3.77492\n",
      "epoch no.1 train no.142080  loss = 4.68756 avg_loss = 3.81708\n",
      "epoch no.1 train no.142090  loss = 3.97703 avg_loss = 3.84302\n",
      "epoch no.1 train no.142100  loss = 2.66867 avg_loss = 3.78291\n",
      "epoch no.1 train no.142110  loss = 3.59897 avg_loss = 3.72870\n",
      "epoch no.1 train no.142120  loss = 2.71920 avg_loss = 3.69875\n",
      "epoch no.1 train no.142130  loss = 2.80859 avg_loss = 3.66747\n",
      "epoch no.1 train no.142140  loss = 2.00268 avg_loss = 3.65141\n",
      "epoch no.1 train no.142150  loss = 3.73782 avg_loss = 3.69210\n",
      "epoch no.1 train no.142160  loss = 3.87562 avg_loss = 3.76052\n",
      "epoch no.1 train no.142170  loss = 5.24061 avg_loss = 3.77908\n",
      "epoch no.1 train no.142180  loss = 2.57378 avg_loss = 3.78683\n",
      "epoch no.1 train no.142190  loss = 2.82503 avg_loss = 3.74914\n",
      "epoch no.1 train no.142200  loss = 4.25326 avg_loss = 3.77669\n",
      "epoch no.1 train no.142210  loss = 5.30713 avg_loss = 3.85758\n",
      "epoch no.1 train no.142220  loss = 3.04623 avg_loss = 3.91708\n",
      "epoch no.1 train no.142230  loss = 4.98102 avg_loss = 3.96638\n",
      "epoch no.1 train no.142240  loss = 3.92336 avg_loss = 3.99898\n",
      "epoch no.1 train no.142250  loss = 4.59937 avg_loss = 3.98276\n",
      "epoch no.1 train no.142260  loss = 2.95041 avg_loss = 3.92767\n",
      "epoch no.1 train no.142270  loss = 2.78549 avg_loss = 3.88088\n",
      "epoch no.1 train no.142280  loss = 4.40426 avg_loss = 3.86853\n",
      "epoch no.1 train no.142290  loss = 2.96833 avg_loss = 3.84432\n",
      "epoch no.1 train no.142300  loss = 3.22113 avg_loss = 3.85165\n",
      "epoch no.1 train no.142310  loss = 2.36534 avg_loss = 3.82338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.142320  loss = 5.70053 avg_loss = 3.80139\n",
      "epoch no.1 train no.142330  loss = 3.51584 avg_loss = 3.83754\n",
      "epoch no.1 train no.142340  loss = 3.93904 avg_loss = 3.79143\n",
      "epoch no.1 train no.142350  loss = 2.08661 avg_loss = 3.80114\n",
      "epoch no.1 train no.142360  loss = 3.40392 avg_loss = 3.79170\n",
      "epoch no.1 train no.142370  loss = 3.56350 avg_loss = 3.78809\n",
      "epoch no.1 train no.142380  loss = 5.83114 avg_loss = 3.84320\n",
      "epoch no.1 train no.142390  loss = 2.95931 avg_loss = 3.82855\n",
      "epoch no.1 train no.142400  loss = 4.80168 avg_loss = 3.79071\n",
      "epoch no.1 train no.142410  loss = 2.90502 avg_loss = 3.74842\n",
      "epoch no.1 train no.142420  loss = 4.71552 avg_loss = 3.75661\n",
      "epoch no.1 train no.142430  loss = 3.46498 avg_loss = 3.75292\n",
      "epoch no.1 train no.142440  loss = 3.30869 avg_loss = 3.70395\n",
      "epoch no.1 train no.142450  loss = 5.58672 avg_loss = 3.75035\n",
      "epoch no.1 train no.142460  loss = 2.83597 avg_loss = 3.79837\n",
      "epoch no.1 train no.142470  loss = 2.07401 avg_loss = 3.75578\n",
      "epoch no.1 train no.142480  loss = 2.50152 avg_loss = 3.78034\n",
      "epoch no.1 train no.142490  loss = 3.10091 avg_loss = 3.71266\n",
      "epoch no.1 train no.142500  loss = 4.14041 avg_loss = 3.75039\n",
      "epoch no.1 train no.142510  loss = 2.92389 avg_loss = 3.74741\n",
      "epoch no.1 train no.142520  loss = 2.89057 avg_loss = 3.78502\n",
      "epoch no.1 train no.142530  loss = 1.99489 avg_loss = 3.72219\n",
      "epoch no.1 train no.142540  loss = 3.87552 avg_loss = 3.77007\n",
      "epoch no.1 train no.142550  loss = 2.88475 avg_loss = 3.75420\n",
      "epoch no.1 train no.142560  loss = 4.69771 avg_loss = 3.76419\n",
      "epoch no.1 train no.142570  loss = 6.28401 avg_loss = 3.77972\n",
      "epoch no.1 train no.142580  loss = 3.27136 avg_loss = 3.78474\n",
      "epoch no.1 train no.142590  loss = 5.01061 avg_loss = 3.81926\n",
      "epoch no.1 train no.142600  loss = 4.44803 avg_loss = 3.83445\n",
      "epoch no.1 train no.142610  loss = 4.04680 avg_loss = 3.87603\n",
      "epoch no.1 train no.142620  loss = 4.81344 avg_loss = 3.87264\n",
      "epoch no.1 train no.142630  loss = 7.06072 avg_loss = 3.84699\n",
      "epoch no.1 train no.142640  loss = 2.37974 avg_loss = 3.87048\n",
      "epoch no.1 train no.142650  loss = 3.47846 avg_loss = 3.84837\n",
      "epoch no.1 train no.142660  loss = 4.86216 avg_loss = 3.83202\n",
      "epoch no.1 train no.142670  loss = 2.42803 avg_loss = 3.79826\n",
      "epoch no.1 train no.142680  loss = 1.42021 avg_loss = 3.75184\n",
      "epoch no.1 train no.142690  loss = 4.14857 avg_loss = 3.75676\n",
      "epoch no.1 train no.142700  loss = 5.21585 avg_loss = 3.76363\n",
      "epoch no.1 train no.142710  loss = 3.16280 avg_loss = 3.78470\n",
      "epoch no.1 train no.142720  loss = 4.53777 avg_loss = 3.74156\n",
      "epoch no.1 train no.142730  loss = 3.87718 avg_loss = 3.75906\n",
      "epoch no.1 train no.142740  loss = 4.18535 avg_loss = 3.79967\n",
      "epoch no.1 train no.142750  loss = 5.53100 avg_loss = 3.81072\n",
      "epoch no.1 train no.142760  loss = 4.63518 avg_loss = 3.84955\n",
      "epoch no.1 train no.142770  loss = 2.14685 avg_loss = 3.77878\n",
      "epoch no.1 train no.142780  loss = 3.69968 avg_loss = 3.77607\n",
      "epoch no.1 train no.142790  loss = 4.24078 avg_loss = 3.77443\n",
      "epoch no.1 train no.142800  loss = 1.87085 avg_loss = 3.72051\n",
      "epoch no.1 train no.142810  loss = 6.01267 avg_loss = 3.74776\n",
      "epoch no.1 train no.142820  loss = 3.97710 avg_loss = 3.71270\n",
      "epoch no.1 train no.142830  loss = 2.89507 avg_loss = 3.65484\n",
      "epoch no.1 train no.142840  loss = 4.96376 avg_loss = 3.69014\n",
      "epoch no.1 train no.142850  loss = 3.31278 avg_loss = 3.64545\n",
      "epoch no.1 train no.142860  loss = 5.22018 avg_loss = 3.68716\n",
      "epoch no.1 train no.142870  loss = 3.28723 avg_loss = 3.69842\n",
      "epoch no.1 train no.142880  loss = 2.18949 avg_loss = 3.64640\n",
      "epoch no.1 train no.142890  loss = 2.29375 avg_loss = 3.66171\n",
      "epoch no.1 train no.142900  loss = 4.56589 avg_loss = 3.69688\n",
      "epoch no.1 train no.142910  loss = 2.10743 avg_loss = 3.65580\n",
      "epoch no.1 train no.142920  loss = 3.56157 avg_loss = 3.68387\n",
      "epoch no.1 train no.142930  loss = 5.00324 avg_loss = 3.68576\n",
      "epoch no.1 train no.142940  loss = 2.08116 avg_loss = 3.68025\n",
      "epoch no.1 train no.142950  loss = 2.71197 avg_loss = 3.64807\n",
      "epoch no.1 train no.142960  loss = 5.47184 avg_loss = 3.70422\n",
      "epoch no.1 train no.142970  loss = 4.74538 avg_loss = 3.69731\n",
      "epoch no.1 train no.142980  loss = 3.06115 avg_loss = 3.70183\n",
      "epoch no.1 train no.142990  loss = 4.08544 avg_loss = 3.76674\n",
      "epoch no.1 train no.143000  loss = 3.56673 avg_loss = 3.76680\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.143010  loss = 2.82935 avg_loss = 3.74916\n",
      "epoch no.1 train no.143020  loss = 4.62028 avg_loss = 3.70714\n",
      "epoch no.1 train no.143030  loss = 4.68893 avg_loss = 3.71095\n",
      "epoch no.1 train no.143040  loss = 4.56136 avg_loss = 3.74555\n",
      "epoch no.1 train no.143050  loss = 4.39313 avg_loss = 3.80272\n",
      "epoch no.1 train no.143060  loss = 2.92509 avg_loss = 3.70339\n",
      "epoch no.1 train no.143070  loss = 3.06238 avg_loss = 3.67876\n",
      "epoch no.1 train no.143080  loss = 6.22762 avg_loss = 3.72030\n",
      "epoch no.1 train no.143090  loss = 2.30285 avg_loss = 3.67928\n",
      "epoch no.1 train no.143100  loss = 2.32847 avg_loss = 3.67846\n",
      "epoch no.1 train no.143110  loss = 4.53193 avg_loss = 3.70225\n",
      "epoch no.1 train no.143120  loss = 5.49100 avg_loss = 3.76914\n",
      "epoch no.1 train no.143130  loss = 2.49922 avg_loss = 3.72772\n",
      "epoch no.1 train no.143140  loss = 3.35156 avg_loss = 3.71219\n",
      "epoch no.1 train no.143150  loss = 4.47788 avg_loss = 3.69066\n",
      "epoch no.1 train no.143160  loss = 4.89757 avg_loss = 3.70972\n",
      "epoch no.1 train no.143170  loss = 3.64740 avg_loss = 3.70082\n",
      "epoch no.1 train no.143180  loss = 2.91625 avg_loss = 3.73704\n",
      "epoch no.1 train no.143190  loss = 4.26364 avg_loss = 3.69179\n",
      "epoch no.1 train no.143200  loss = 3.72434 avg_loss = 3.68292\n",
      "epoch no.1 train no.143210  loss = 2.66167 avg_loss = 3.64056\n",
      "epoch no.1 train no.143220  loss = 4.66014 avg_loss = 3.65621\n",
      "epoch no.1 train no.143230  loss = 2.66373 avg_loss = 3.64525\n",
      "epoch no.1 train no.143240  loss = 3.43423 avg_loss = 3.62828\n",
      "epoch no.1 train no.143250  loss = 3.10355 avg_loss = 3.64449\n",
      "epoch no.1 train no.143260  loss = 3.53489 avg_loss = 3.72130\n",
      "epoch no.1 train no.143270  loss = 2.49933 avg_loss = 3.72898\n",
      "epoch no.1 train no.143280  loss = 2.41467 avg_loss = 3.70960\n",
      "epoch no.1 train no.143290  loss = 4.91350 avg_loss = 3.73086\n",
      "epoch no.1 train no.143300  loss = 3.02096 avg_loss = 3.79330\n",
      "epoch no.1 train no.143310  loss = 6.36076 avg_loss = 3.81485\n",
      "epoch no.1 train no.143320  loss = 3.91987 avg_loss = 3.85650\n",
      "epoch no.1 train no.143330  loss = 4.30409 avg_loss = 3.87706\n",
      "epoch no.1 train no.143340  loss = 1.68385 avg_loss = 3.93919\n",
      "epoch no.1 train no.143350  loss = 6.54807 avg_loss = 3.93460\n",
      "epoch no.1 train no.143360  loss = 4.91084 avg_loss = 3.90416\n",
      "epoch no.1 train no.143370  loss = 4.47366 avg_loss = 3.89699\n",
      "epoch no.1 train no.143380  loss = 2.70081 avg_loss = 3.87429\n",
      "epoch no.1 train no.143390  loss = 3.85715 avg_loss = 3.86736\n",
      "epoch no.1 train no.143400  loss = 2.94911 avg_loss = 3.85778\n",
      "epoch no.1 train no.143410  loss = 3.36482 avg_loss = 3.88967\n",
      "epoch no.1 train no.143420  loss = 4.55761 avg_loss = 3.86421\n",
      "epoch no.1 train no.143430  loss = 4.69677 avg_loss = 3.86166\n",
      "epoch no.1 train no.143440  loss = 2.16899 avg_loss = 3.84094\n",
      "epoch no.1 train no.143450  loss = 3.29486 avg_loss = 3.84932\n",
      "epoch no.1 train no.143460  loss = 3.46382 avg_loss = 3.84953\n",
      "epoch no.1 train no.143470  loss = 5.26484 avg_loss = 3.89450\n",
      "epoch no.1 train no.143480  loss = 6.07715 avg_loss = 3.90918\n",
      "epoch no.1 train no.143490  loss = 2.30529 avg_loss = 3.88350\n",
      "epoch no.1 train no.143500  loss = 4.60993 avg_loss = 3.82855\n",
      "epoch no.1 train no.143510  loss = 3.47821 avg_loss = 3.74355\n",
      "epoch no.1 train no.143520  loss = 3.05417 avg_loss = 3.76235\n",
      "epoch no.1 train no.143530  loss = 3.64177 avg_loss = 3.78366\n",
      "epoch no.1 train no.143540  loss = 3.49503 avg_loss = 3.71479\n",
      "epoch no.1 train no.143550  loss = 5.17655 avg_loss = 3.72548\n",
      "epoch no.1 train no.143560  loss = 2.89525 avg_loss = 3.70690\n",
      "epoch no.1 train no.143570  loss = 1.74471 avg_loss = 3.68809\n",
      "epoch no.1 train no.143580  loss = 5.88679 avg_loss = 3.72504\n",
      "epoch no.1 train no.143590  loss = 2.62967 avg_loss = 3.66543\n",
      "epoch no.1 train no.143600  loss = 4.05319 avg_loss = 3.64540\n",
      "epoch no.1 train no.143610  loss = 4.37152 avg_loss = 3.66767\n",
      "epoch no.1 train no.143620  loss = 3.87192 avg_loss = 3.69205\n",
      "epoch no.1 train no.143630  loss = 3.30272 avg_loss = 3.65753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.143640  loss = 2.39593 avg_loss = 3.58207\n",
      "epoch no.1 train no.143650  loss = 2.41459 avg_loss = 3.60337\n",
      "epoch no.1 train no.143660  loss = 4.63464 avg_loss = 3.62883\n",
      "epoch no.1 train no.143670  loss = 3.09611 avg_loss = 3.60934\n",
      "epoch no.1 train no.143680  loss = 4.03368 avg_loss = 3.65542\n",
      "epoch no.1 train no.143690  loss = 3.16845 avg_loss = 3.68431\n",
      "epoch no.1 train no.143700  loss = 4.74886 avg_loss = 3.70302\n",
      "epoch no.1 train no.143710  loss = 5.70381 avg_loss = 3.79687\n",
      "epoch no.1 train no.143720  loss = 4.10369 avg_loss = 3.77128\n",
      "epoch no.1 train no.143730  loss = 2.93380 avg_loss = 3.80542\n",
      "epoch no.1 train no.143740  loss = 3.27089 avg_loss = 3.75892\n",
      "epoch no.1 train no.143750  loss = 2.63261 avg_loss = 3.78393\n",
      "epoch no.1 train no.143760  loss = 5.00422 avg_loss = 3.78954\n",
      "epoch no.1 train no.143770  loss = 1.77774 avg_loss = 3.83258\n",
      "epoch no.1 train no.143780  loss = 4.24217 avg_loss = 3.86390\n",
      "epoch no.1 train no.143790  loss = 5.54397 avg_loss = 3.87191\n",
      "epoch no.1 train no.143800  loss = 4.37330 avg_loss = 3.83472\n",
      "epoch no.1 train no.143810  loss = 2.77767 avg_loss = 3.85497\n",
      "epoch no.1 train no.143820  loss = 2.66605 avg_loss = 3.83319\n",
      "epoch no.1 train no.143830  loss = 3.55980 avg_loss = 3.78067\n",
      "epoch no.1 train no.143840  loss = 3.60526 avg_loss = 3.73735\n",
      "epoch no.1 train no.143850  loss = 2.94710 avg_loss = 3.67858\n",
      "epoch no.1 train no.143860  loss = 3.90860 avg_loss = 3.71057\n",
      "epoch no.1 train no.143870  loss = 2.87663 avg_loss = 3.66727\n",
      "epoch no.1 train no.143880  loss = 3.64341 avg_loss = 3.67411\n",
      "epoch no.1 train no.143890  loss = 5.00151 avg_loss = 3.73674\n",
      "epoch no.1 train no.143900  loss = 3.83529 avg_loss = 3.73904\n",
      "epoch no.1 train no.143910  loss = 3.83450 avg_loss = 3.69855\n",
      "epoch no.1 train no.143920  loss = 3.76049 avg_loss = 3.69319\n",
      "epoch no.1 train no.143930  loss = 4.02243 avg_loss = 3.67207\n",
      "epoch no.1 train no.143940  loss = 3.50216 avg_loss = 3.72183\n",
      "epoch no.1 train no.143950  loss = 6.83346 avg_loss = 3.74619\n",
      "epoch no.1 train no.143960  loss = 4.47922 avg_loss = 3.71581\n",
      "epoch no.1 train no.143970  loss = 4.07818 avg_loss = 3.78042\n",
      "epoch no.1 train no.143980  loss = 3.10196 avg_loss = 3.79022\n",
      "epoch no.1 train no.143990  loss = 4.03221 avg_loss = 3.79561\n",
      "epoch no.1 train no.144000  loss = 3.03716 avg_loss = 3.85803\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '주는', '▁노래', '</s>']\n",
      "기분전환 시켜주는 노래</s>\n",
      "epoch no.1 train no.144010  loss = 6.49243 avg_loss = 3.83171\n",
      "epoch no.1 train no.144020  loss = 1.25928 avg_loss = 3.82284\n",
      "epoch no.1 train no.144030  loss = 3.32813 avg_loss = 3.78026\n",
      "epoch no.1 train no.144040  loss = 3.53723 avg_loss = 3.76395\n",
      "epoch no.1 train no.144050  loss = 3.36512 avg_loss = 3.82406\n",
      "epoch no.1 train no.144060  loss = 2.35574 avg_loss = 3.79903\n",
      "epoch no.1 train no.144070  loss = 4.87558 avg_loss = 3.87579\n",
      "epoch no.1 train no.144080  loss = 2.35320 avg_loss = 3.81624\n",
      "epoch no.1 train no.144090  loss = 3.25770 avg_loss = 3.80350\n",
      "epoch no.1 train no.144100  loss = 4.59758 avg_loss = 3.78621\n",
      "epoch no.1 train no.144110  loss = 4.10529 avg_loss = 3.81847\n",
      "epoch no.1 train no.144120  loss = 3.50220 avg_loss = 3.79530\n",
      "epoch no.1 train no.144130  loss = 2.76403 avg_loss = 3.73995\n",
      "epoch no.1 train no.144140  loss = 5.73807 avg_loss = 3.81316\n",
      "epoch no.1 train no.144150  loss = 3.95696 avg_loss = 3.77338\n",
      "epoch no.1 train no.144160  loss = 2.82631 avg_loss = 3.77303\n",
      "epoch no.1 train no.144170  loss = 3.78301 avg_loss = 3.78943\n",
      "epoch no.1 train no.144180  loss = 2.98844 avg_loss = 3.81769\n",
      "epoch no.1 train no.144190  loss = 3.31435 avg_loss = 3.82640\n",
      "epoch no.1 train no.144200  loss = 4.97745 avg_loss = 3.79728\n",
      "epoch no.1 train no.144210  loss = 2.86469 avg_loss = 3.77303\n",
      "epoch no.1 train no.144220  loss = 4.80120 avg_loss = 3.84867\n",
      "epoch no.1 train no.144230  loss = 2.01339 avg_loss = 3.82293\n",
      "epoch no.1 train no.144240  loss = 1.73009 avg_loss = 3.80086\n",
      "epoch no.1 train no.144250  loss = 4.59094 avg_loss = 3.83156\n",
      "epoch no.1 train no.144260  loss = 4.66799 avg_loss = 3.83393\n",
      "epoch no.1 train no.144270  loss = 4.25534 avg_loss = 3.83528\n",
      "epoch no.1 train no.144280  loss = 4.00602 avg_loss = 3.79523\n",
      "epoch no.1 train no.144290  loss = 5.02543 avg_loss = 3.86515\n",
      "epoch no.1 train no.144300  loss = 4.69121 avg_loss = 3.85168\n",
      "epoch no.1 train no.144310  loss = 3.91423 avg_loss = 3.86007\n",
      "epoch no.1 train no.144320  loss = 2.79299 avg_loss = 3.87267\n",
      "epoch no.1 train no.144330  loss = 1.97614 avg_loss = 3.87001\n",
      "epoch no.1 train no.144340  loss = 3.22342 avg_loss = 3.82746\n",
      "epoch no.1 train no.144350  loss = 3.11400 avg_loss = 3.85966\n",
      "epoch no.1 train no.144360  loss = 2.73213 avg_loss = 3.79350\n",
      "epoch no.1 train no.144370  loss = 3.33876 avg_loss = 3.76784\n",
      "epoch no.1 train no.144380  loss = 3.60099 avg_loss = 3.76367\n",
      "epoch no.1 train no.144390  loss = 3.13324 avg_loss = 3.76337\n",
      "epoch no.1 train no.144400  loss = 3.68743 avg_loss = 3.76432\n",
      "epoch no.1 train no.144410  loss = 2.05824 avg_loss = 3.76660\n",
      "epoch no.1 train no.144420  loss = 2.99261 avg_loss = 3.75125\n",
      "epoch no.1 train no.144430  loss = 3.67958 avg_loss = 3.74604\n",
      "epoch no.1 train no.144440  loss = 2.21480 avg_loss = 3.76703\n",
      "epoch no.1 train no.144450  loss = 2.67887 avg_loss = 3.67206\n",
      "epoch no.1 train no.144460  loss = 6.61114 avg_loss = 3.75305\n",
      "epoch no.1 train no.144470  loss = 3.29175 avg_loss = 3.75329\n",
      "epoch no.1 train no.144480  loss = 3.62506 avg_loss = 3.76944\n",
      "epoch no.1 train no.144490  loss = 3.72367 avg_loss = 3.78871\n",
      "epoch no.1 train no.144500  loss = 4.10088 avg_loss = 3.76115\n",
      "epoch no.1 train no.144510  loss = 4.19133 avg_loss = 3.76662\n",
      "epoch no.1 train no.144520  loss = 3.04926 avg_loss = 3.74004\n",
      "epoch no.1 train no.144530  loss = 3.48320 avg_loss = 3.72395\n",
      "epoch no.1 train no.144540  loss = 2.75957 avg_loss = 3.71489\n",
      "epoch no.1 train no.144550  loss = 4.65412 avg_loss = 3.71670\n",
      "epoch no.1 train no.144560  loss = 3.89352 avg_loss = 3.74103\n",
      "epoch no.1 train no.144570  loss = 5.38288 avg_loss = 3.76595\n",
      "epoch no.1 train no.144580  loss = 2.76001 avg_loss = 3.81101\n",
      "epoch no.1 train no.144590  loss = 3.26665 avg_loss = 3.84804\n",
      "epoch no.1 train no.144600  loss = 2.91989 avg_loss = 3.85577\n",
      "epoch no.1 train no.144610  loss = 4.06977 avg_loss = 3.89687\n",
      "epoch no.1 train no.144620  loss = 4.71894 avg_loss = 3.82790\n",
      "epoch no.1 train no.144630  loss = 4.89674 avg_loss = 3.79613\n",
      "epoch no.1 train no.144640  loss = 2.92967 avg_loss = 3.75856\n",
      "epoch no.1 train no.144650  loss = 3.21400 avg_loss = 3.82276\n",
      "epoch no.1 train no.144660  loss = 2.70430 avg_loss = 3.78939\n",
      "epoch no.1 train no.144670  loss = 3.80375 avg_loss = 3.78364\n",
      "epoch no.1 train no.144680  loss = 5.72960 avg_loss = 3.87486\n",
      "epoch no.1 train no.144690  loss = 4.95322 avg_loss = 3.89327\n",
      "epoch no.1 train no.144700  loss = 3.10616 avg_loss = 3.88645\n",
      "epoch no.1 train no.144710  loss = 3.15337 avg_loss = 3.85369\n",
      "epoch no.1 train no.144720  loss = 3.07276 avg_loss = 3.81095\n",
      "epoch no.1 train no.144730  loss = 3.35038 avg_loss = 3.75414\n",
      "epoch no.1 train no.144740  loss = 5.25184 avg_loss = 3.79057\n",
      "epoch no.1 train no.144750  loss = 3.76117 avg_loss = 3.76757\n",
      "epoch no.1 train no.144760  loss = 3.18991 avg_loss = 3.86342\n",
      "epoch no.1 train no.144770  loss = 5.10262 avg_loss = 3.90722\n",
      "epoch no.1 train no.144780  loss = 4.54850 avg_loss = 3.93044\n",
      "epoch no.1 train no.144790  loss = 2.37547 avg_loss = 3.86651\n",
      "epoch no.1 train no.144800  loss = 4.49225 avg_loss = 3.94285\n",
      "epoch no.1 train no.144810  loss = 3.11168 avg_loss = 3.93404\n",
      "epoch no.1 train no.144820  loss = 2.69576 avg_loss = 3.88298\n",
      "epoch no.1 train no.144830  loss = 3.72643 avg_loss = 3.80917\n",
      "epoch no.1 train no.144840  loss = 6.06407 avg_loss = 3.83431\n",
      "epoch no.1 train no.144850  loss = 5.04523 avg_loss = 3.87253\n",
      "epoch no.1 train no.144860  loss = 3.73879 avg_loss = 3.93966\n",
      "epoch no.1 train no.144870  loss = 5.14134 avg_loss = 3.95472\n",
      "epoch no.1 train no.144880  loss = 3.43919 avg_loss = 3.93285\n",
      "epoch no.1 train no.144890  loss = 5.31096 avg_loss = 3.91519\n",
      "epoch no.1 train no.144900  loss = 2.60319 avg_loss = 3.89673\n",
      "epoch no.1 train no.144910  loss = 3.30718 avg_loss = 3.84336\n",
      "epoch no.1 train no.144920  loss = 4.39638 avg_loss = 3.84404\n",
      "epoch no.1 train no.144930  loss = 8.12271 avg_loss = 3.88339\n",
      "epoch no.1 train no.144940  loss = 2.99471 avg_loss = 3.90806\n",
      "epoch no.1 train no.144950  loss = 5.29423 avg_loss = 3.89477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.144960  loss = 3.75400 avg_loss = 3.91293\n",
      "epoch no.1 train no.144970  loss = 3.07305 avg_loss = 3.90206\n",
      "epoch no.1 train no.144980  loss = 2.52358 avg_loss = 3.88316\n",
      "epoch no.1 train no.144990  loss = 2.30765 avg_loss = 3.82685\n",
      "epoch no.1 train no.145000  loss = 5.02356 avg_loss = 3.88628\n",
      "1\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.1 train no.145010  loss = 2.64357 avg_loss = 3.89163\n",
      "epoch no.1 train no.145020  loss = 2.77753 avg_loss = 3.87909\n",
      "epoch no.1 train no.145030  loss = 4.35305 avg_loss = 3.89674\n",
      "epoch no.1 train no.145040  loss = 6.12542 avg_loss = 3.89187\n",
      "epoch no.1 train no.145050  loss = 4.27555 avg_loss = 3.81655\n",
      "epoch no.1 train no.145060  loss = 6.26268 avg_loss = 3.76432\n",
      "epoch no.1 train no.145070  loss = 1.89855 avg_loss = 3.71743\n",
      "epoch no.1 train no.145080  loss = 5.21441 avg_loss = 3.78384\n",
      "epoch no.1 train no.145090  loss = 3.58482 avg_loss = 3.79605\n",
      "epoch no.1 train no.145100  loss = 3.13889 avg_loss = 3.78814\n",
      "epoch no.1 train no.145110  loss = 4.73685 avg_loss = 3.76231\n",
      "epoch no.1 train no.145120  loss = 3.28762 avg_loss = 3.73142\n",
      "epoch no.1 train no.145130  loss = 7.50589 avg_loss = 3.78142\n",
      "epoch no.1 train no.145140  loss = 4.25748 avg_loss = 3.81134\n",
      "epoch no.1 train no.145150  loss = 2.45613 avg_loss = 3.76735\n",
      "epoch no.1 train no.145160  loss = 5.28953 avg_loss = 3.81954\n",
      "epoch no.1 train no.145170  loss = 5.43571 avg_loss = 3.82206\n",
      "epoch no.1 train no.145180  loss = 2.55893 avg_loss = 3.80287\n",
      "epoch no.1 train no.145190  loss = 3.79266 avg_loss = 3.77743\n",
      "epoch no.1 train no.145200  loss = 3.24707 avg_loss = 3.75741\n",
      "epoch no.1 train no.145210  loss = 3.26588 avg_loss = 3.74910\n",
      "epoch no.1 train no.145220  loss = 4.38941 avg_loss = 3.76865\n",
      "epoch no.1 train no.145230  loss = 4.21746 avg_loss = 3.77331\n",
      "epoch no.1 train no.145240  loss = 3.20445 avg_loss = 3.76279\n",
      "epoch no.1 train no.145250  loss = 4.93379 avg_loss = 3.76940\n",
      "epoch no.1 train no.145260  loss = 6.43473 avg_loss = 3.78125\n",
      "epoch no.1 train no.145270  loss = 4.80920 avg_loss = 3.79610\n",
      "epoch no.1 train no.145280  loss = 2.82854 avg_loss = 3.78193\n",
      "epoch no.1 train no.145290  loss = 4.67703 avg_loss = 3.83349\n",
      "epoch no.1 train no.145300  loss = 3.24457 avg_loss = 3.80581\n",
      "epoch no.1 train no.145310  loss = 2.15046 avg_loss = 3.76809\n",
      "epoch no.1 train no.145320  loss = 3.17356 avg_loss = 3.82683\n",
      "epoch no.1 train no.145330  loss = 5.47869 avg_loss = 3.77876\n",
      "epoch no.1 train no.145340  loss = 4.71748 avg_loss = 3.78425\n",
      "epoch no.1 train no.145350  loss = 3.07781 avg_loss = 3.76741\n",
      "epoch no.1 train no.145360  loss = 5.20682 avg_loss = 3.74842\n",
      "epoch no.1 train no.145370  loss = 2.80146 avg_loss = 3.70268\n",
      "epoch no.1 train no.145380  loss = 4.28201 avg_loss = 3.66819\n",
      "epoch no.1 train no.145390  loss = 5.16567 avg_loss = 3.68489\n",
      "epoch no.1 train no.145400  loss = 2.77574 avg_loss = 3.67094\n",
      "epoch no.1 train no.145410  loss = 3.95115 avg_loss = 3.69105\n",
      "epoch no.1 train no.145420  loss = 4.32725 avg_loss = 3.70059\n",
      "epoch no.1 train no.145430  loss = 4.01946 avg_loss = 3.64596\n",
      "epoch no.1 train no.145440  loss = 3.57126 avg_loss = 3.65526\n",
      "epoch no.1 train no.145450  loss = 2.84163 avg_loss = 3.72045\n",
      "epoch no.1 train no.145460  loss = 2.78561 avg_loss = 3.67137\n",
      "epoch no.1 train no.145470  loss = 4.98778 avg_loss = 3.73094\n",
      "epoch no.1 train no.145480  loss = 3.28206 avg_loss = 3.69458\n",
      "epoch no.1 train no.145490  loss = 3.47597 avg_loss = 3.66813\n",
      "epoch no.1 train no.145500  loss = 3.48868 avg_loss = 3.68804\n",
      "epoch no.1 train no.145510  loss = 4.98402 avg_loss = 3.67151\n",
      "epoch no.1 train no.145520  loss = 7.27388 avg_loss = 3.71490\n",
      "epoch no.1 train no.145530  loss = 2.87655 avg_loss = 3.68421\n",
      "epoch no.1 train no.145540  loss = 4.04427 avg_loss = 3.73791\n",
      "epoch no.1 train no.145550  loss = 5.24857 avg_loss = 3.79469\n",
      "epoch no.1 train no.145560  loss = 2.77119 avg_loss = 3.78823\n",
      "epoch no.1 train no.145570  loss = 1.95692 avg_loss = 3.76441\n",
      "epoch no.1 train no.145580  loss = 3.51264 avg_loss = 3.80971\n",
      "epoch no.1 train no.145590  loss = 2.70081 avg_loss = 3.81330\n",
      "epoch no.1 train no.145600  loss = 3.83156 avg_loss = 3.80634\n",
      "epoch no.1 train no.145610  loss = 3.54113 avg_loss = 3.77128\n",
      "epoch no.1 train no.145620  loss = 2.14178 avg_loss = 3.71385\n",
      "epoch no.1 train no.145630  loss = 2.40861 avg_loss = 3.66639\n",
      "epoch no.1 train no.145640  loss = 5.21043 avg_loss = 3.65529\n",
      "epoch no.1 train no.145650  loss = 4.05794 avg_loss = 3.69806\n",
      "epoch no.1 train no.145660  loss = 5.67676 avg_loss = 3.77134\n",
      "epoch no.1 train no.145670  loss = 5.61676 avg_loss = 3.78824\n",
      "epoch no.1 train no.145680  loss = 6.46670 avg_loss = 3.81489\n",
      "epoch no.1 train no.145690  loss = 3.55860 avg_loss = 3.85694\n",
      "epoch no.1 train no.145700  loss = 3.90288 avg_loss = 3.80005\n",
      "epoch no.1 train no.145710  loss = 3.69180 avg_loss = 3.81151\n",
      "epoch no.1 train no.145720  loss = 4.22515 avg_loss = 3.84425\n",
      "epoch no.1 train no.145730  loss = 4.37897 avg_loss = 3.82758\n",
      "epoch no.1 train no.145740  loss = 2.76976 avg_loss = 3.87684\n",
      "epoch no.1 train no.145750  loss = 3.22442 avg_loss = 3.86878\n",
      "epoch no.1 train no.145760  loss = 2.44896 avg_loss = 3.82279\n",
      "epoch no.1 train no.145770  loss = 3.19866 avg_loss = 3.77458\n",
      "epoch no.1 train no.145780  loss = 3.94356 avg_loss = 3.86169\n",
      "epoch no.1 train no.145790  loss = 3.69365 avg_loss = 3.82910\n",
      "epoch no.1 train no.145800  loss = 6.73695 avg_loss = 3.81381\n",
      "epoch no.1 train no.145810  loss = 2.43238 avg_loss = 3.76748\n",
      "epoch no.1 train no.145820  loss = 3.18025 avg_loss = 3.72456\n",
      "epoch no.1 train no.145830  loss = 3.43964 avg_loss = 3.79050\n",
      "epoch no.1 train no.145840  loss = 4.25474 avg_loss = 3.76403\n",
      "epoch no.1 train no.145850  loss = 2.83423 avg_loss = 3.71236\n",
      "epoch no.1 train no.145860  loss = 3.88456 avg_loss = 3.76762\n",
      "epoch no.1 train no.145870  loss = 5.14707 avg_loss = 3.78270\n",
      "epoch no.1 train no.145880  loss = 2.73211 avg_loss = 3.74783\n",
      "epoch no.1 train no.145890  loss = 3.72202 avg_loss = 3.73722\n",
      "epoch no.1 train no.145900  loss = 2.04607 avg_loss = 3.75313\n",
      "epoch no.1 train no.145910  loss = 4.41041 avg_loss = 3.80108\n",
      "epoch no.1 train no.145920  loss = 3.67260 avg_loss = 3.83723\n",
      "epoch no.1 train no.145930  loss = 4.78664 avg_loss = 3.84511\n",
      "epoch no.1 train no.145940  loss = 2.95358 avg_loss = 3.80030\n",
      "epoch no.1 train no.145950  loss = 4.62916 avg_loss = 3.78273\n",
      "epoch no.1 train no.145960  loss = 3.72999 avg_loss = 3.75793\n",
      "epoch no.1 train no.145970  loss = 4.69959 avg_loss = 3.82125\n",
      "epoch no.1 train no.145980  loss = 3.13130 avg_loss = 3.79412\n",
      "epoch no.1 train no.145990  loss = 3.42319 avg_loss = 3.78923\n",
      "epoch no.1 train no.146000  loss = 3.94427 avg_loss = 3.80168\n",
      "1\n",
      "to_tokens: ['▁라디오', '좋은', '이', '</s>']\n",
      "기분전환 드라이브</s>\n",
      "epoch no.1 train no.146010  loss = 4.19811 avg_loss = 3.81605\n",
      "epoch no.1 train no.146020  loss = 2.68524 avg_loss = 3.80368\n",
      "epoch no.1 train no.146030  loss = 6.44397 avg_loss = 3.78904\n",
      "epoch no.1 train no.146040  loss = 3.61142 avg_loss = 3.74915\n",
      "epoch no.1 train no.146050  loss = 3.05828 avg_loss = 3.77896\n",
      "epoch no.1 train no.146060  loss = 4.57738 avg_loss = 3.77477\n",
      "epoch no.1 train no.146070  loss = 3.95850 avg_loss = 3.75109\n",
      "epoch no.1 train no.146080  loss = 4.01829 avg_loss = 3.75940\n",
      "epoch no.1 train no.146090  loss = 2.98286 avg_loss = 3.73037\n",
      "epoch no.1 train no.146100  loss = 2.61057 avg_loss = 3.78397\n",
      "epoch no.1 train no.146110  loss = 4.77269 avg_loss = 3.79846\n",
      "epoch no.1 train no.146120  loss = 3.99025 avg_loss = 3.76625\n",
      "epoch no.1 train no.146130  loss = 4.08213 avg_loss = 3.73512\n",
      "epoch no.1 train no.146140  loss = 4.72571 avg_loss = 3.71551\n",
      "epoch no.1 train no.146150  loss = 2.94416 avg_loss = 3.77789\n",
      "epoch no.1 train no.146160  loss = 4.59355 avg_loss = 3.77603\n",
      "epoch no.1 train no.146170  loss = 2.96778 avg_loss = 3.72142\n",
      "epoch no.1 train no.146180  loss = 3.36896 avg_loss = 3.71379\n",
      "epoch no.1 train no.146190  loss = 2.71286 avg_loss = 3.73733\n",
      "epoch no.1 train no.146200  loss = 2.77134 avg_loss = 3.74994\n",
      "epoch no.1 train no.146210  loss = 3.46901 avg_loss = 3.80179\n",
      "epoch no.1 train no.146220  loss = 5.14097 avg_loss = 3.84544\n",
      "epoch no.1 train no.146230  loss = 4.24334 avg_loss = 3.88830\n",
      "epoch no.1 train no.146240  loss = 2.88767 avg_loss = 3.88917\n",
      "epoch no.1 train no.146250  loss = 2.48976 avg_loss = 3.88362\n",
      "epoch no.1 train no.146260  loss = 2.91544 avg_loss = 3.84113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.146270  loss = 2.58301 avg_loss = 3.80826\n",
      "epoch no.1 train no.146280  loss = 3.21402 avg_loss = 3.82822\n",
      "epoch no.1 train no.146290  loss = 3.09302 avg_loss = 3.79613\n",
      "epoch no.1 train no.146300  loss = 4.87091 avg_loss = 3.79389\n",
      "epoch no.1 train no.146310  loss = 3.28882 avg_loss = 3.80379\n",
      "epoch no.1 train no.146320  loss = 3.79106 avg_loss = 3.76557\n",
      "epoch no.1 train no.146330  loss = 4.56423 avg_loss = 3.80976\n",
      "epoch no.1 train no.146340  loss = 3.27942 avg_loss = 3.77263\n",
      "epoch no.1 train no.146350  loss = 4.90045 avg_loss = 3.80909\n",
      "epoch no.1 train no.146360  loss = 4.31164 avg_loss = 3.78996\n",
      "epoch no.1 train no.146370  loss = 2.95829 avg_loss = 3.78269\n",
      "epoch no.1 train no.146380  loss = 3.02733 avg_loss = 3.80748\n",
      "epoch no.1 train no.146390  loss = 3.40451 avg_loss = 3.83505\n",
      "epoch no.1 train no.146400  loss = 2.17011 avg_loss = 3.87757\n",
      "epoch no.1 train no.146410  loss = 7.81544 avg_loss = 3.92567\n",
      "epoch no.1 train no.146420  loss = 5.12379 avg_loss = 3.89390\n",
      "epoch no.1 train no.146430  loss = 3.58425 avg_loss = 3.86944\n",
      "epoch no.1 train no.146440  loss = 2.80889 avg_loss = 3.87371\n",
      "epoch no.1 train no.146450  loss = 5.00223 avg_loss = 3.88182\n",
      "epoch no.1 train no.146460  loss = 5.15428 avg_loss = 3.88026\n",
      "epoch no.1 train no.146470  loss = 2.91866 avg_loss = 3.85130\n",
      "epoch no.1 train no.146480  loss = 2.61915 avg_loss = 3.83513\n",
      "epoch no.1 train no.146490  loss = 3.35222 avg_loss = 3.80190\n",
      "epoch no.1 train no.146500  loss = 3.13492 avg_loss = 3.83681\n",
      "epoch no.1 train no.146510  loss = 5.56605 avg_loss = 3.86724\n",
      "epoch no.1 train no.146520  loss = 7.48739 avg_loss = 3.90459\n",
      "epoch no.1 train no.146530  loss = 3.12581 avg_loss = 3.91539\n",
      "epoch no.1 train no.146540  loss = 3.32727 avg_loss = 3.84697\n",
      "epoch no.1 train no.146550  loss = 4.23911 avg_loss = 3.82195\n",
      "epoch no.1 train no.146560  loss = 3.21237 avg_loss = 3.84814\n",
      "epoch no.1 train no.146570  loss = 2.33431 avg_loss = 3.85892\n",
      "epoch no.1 train no.146580  loss = 3.14733 avg_loss = 3.86785\n",
      "epoch no.1 train no.146590  loss = 3.07450 avg_loss = 3.85966\n",
      "epoch no.1 train no.146600  loss = 2.90606 avg_loss = 3.82392\n",
      "epoch no.1 train no.146610  loss = 4.27232 avg_loss = 3.84274\n",
      "epoch no.1 train no.146620  loss = 4.32378 avg_loss = 3.81543\n",
      "epoch no.1 train no.146630  loss = 3.83539 avg_loss = 3.79123\n",
      "epoch no.1 train no.146640  loss = 4.37458 avg_loss = 3.83134\n",
      "epoch no.1 train no.146650  loss = 3.72103 avg_loss = 3.79600\n",
      "epoch no.1 train no.146660  loss = 2.79860 avg_loss = 3.74217\n",
      "epoch no.1 train no.146670  loss = 2.40746 avg_loss = 3.70546\n",
      "epoch no.1 train no.146680  loss = 3.79622 avg_loss = 3.68019\n",
      "epoch no.1 train no.146690  loss = 5.87718 avg_loss = 3.72294\n",
      "epoch no.1 train no.146700  loss = 5.60716 avg_loss = 3.76577\n",
      "epoch no.1 train no.146710  loss = 3.88078 avg_loss = 3.85501\n",
      "epoch no.1 train no.146720  loss = 4.04924 avg_loss = 3.85063\n",
      "epoch no.1 train no.146730  loss = 2.47035 avg_loss = 3.81146\n",
      "epoch no.1 train no.146740  loss = 4.16872 avg_loss = 3.85480\n",
      "epoch no.1 train no.146750  loss = 4.11305 avg_loss = 3.80985\n",
      "epoch no.1 train no.146760  loss = 3.21796 avg_loss = 3.77461\n",
      "epoch no.1 train no.146770  loss = 6.00535 avg_loss = 3.77194\n",
      "epoch no.1 train no.146780  loss = 3.10818 avg_loss = 3.72444\n",
      "epoch no.1 train no.146790  loss = 3.51354 avg_loss = 3.70942\n",
      "epoch no.1 train no.146800  loss = 4.37368 avg_loss = 3.71801\n",
      "epoch no.1 train no.146810  loss = 2.93752 avg_loss = 3.71015\n",
      "epoch no.1 train no.146820  loss = 3.23173 avg_loss = 3.69736\n",
      "epoch no.1 train no.146830  loss = 3.82295 avg_loss = 3.73236\n",
      "epoch no.1 train no.146840  loss = 3.61147 avg_loss = 3.67008\n",
      "epoch no.1 train no.146850  loss = 6.89626 avg_loss = 3.73076\n",
      "epoch no.1 train no.146860  loss = 4.57761 avg_loss = 3.78875\n",
      "epoch no.1 train no.146870  loss = 2.39411 avg_loss = 3.75777\n",
      "epoch no.1 train no.146880  loss = 4.24984 avg_loss = 3.75536\n",
      "epoch no.1 train no.146890  loss = 2.94959 avg_loss = 3.74756\n",
      "epoch no.1 train no.146900  loss = 3.03799 avg_loss = 3.75096\n",
      "epoch no.1 train no.146910  loss = 2.49483 avg_loss = 3.78903\n",
      "epoch no.1 train no.146920  loss = 2.92901 avg_loss = 3.78787\n",
      "epoch no.1 train no.146930  loss = 3.16331 avg_loss = 3.77209\n",
      "epoch no.1 train no.146940  loss = 3.85283 avg_loss = 3.80063\n",
      "epoch no.1 train no.146950  loss = 4.24732 avg_loss = 3.76381\n",
      "epoch no.1 train no.146960  loss = 2.16837 avg_loss = 3.77580\n",
      "epoch no.1 train no.146970  loss = 4.56570 avg_loss = 3.75399\n",
      "epoch no.1 train no.146980  loss = 2.17003 avg_loss = 3.77405\n",
      "epoch no.1 train no.146990  loss = 6.43468 avg_loss = 3.77056\n",
      "epoch no.1 train no.147000  loss = 2.25405 avg_loss = 3.71616\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.147010  loss = 3.28200 avg_loss = 3.69841\n",
      "epoch no.1 train no.147020  loss = 5.18844 avg_loss = 3.70553\n",
      "epoch no.1 train no.147030  loss = 5.48210 avg_loss = 3.77854\n",
      "epoch no.1 train no.147040  loss = 3.70279 avg_loss = 3.75371\n",
      "epoch no.1 train no.147050  loss = 2.49564 avg_loss = 3.78790\n",
      "epoch no.1 train no.147060  loss = 2.60326 avg_loss = 3.79740\n",
      "epoch no.1 train no.147070  loss = 4.74084 avg_loss = 3.80539\n",
      "epoch no.1 train no.147080  loss = 3.02771 avg_loss = 3.75063\n",
      "epoch no.1 train no.147090  loss = 4.14791 avg_loss = 3.79292\n",
      "epoch no.1 train no.147100  loss = 3.32622 avg_loss = 3.80466\n",
      "epoch no.1 train no.147110  loss = 2.53916 avg_loss = 3.79066\n",
      "epoch no.1 train no.147120  loss = 2.72736 avg_loss = 3.77090\n",
      "epoch no.1 train no.147130  loss = 7.01844 avg_loss = 3.78998\n",
      "epoch no.1 train no.147140  loss = 4.48097 avg_loss = 3.79767\n",
      "epoch no.1 train no.147150  loss = 6.38839 avg_loss = 3.83390\n",
      "epoch no.1 train no.147160  loss = 3.66167 avg_loss = 3.80565\n",
      "epoch no.1 train no.147170  loss = 2.76523 avg_loss = 3.78963\n",
      "epoch no.1 train no.147180  loss = 3.57280 avg_loss = 3.71882\n",
      "epoch no.1 train no.147190  loss = 5.20474 avg_loss = 3.73973\n",
      "epoch no.1 train no.147200  loss = 5.46522 avg_loss = 3.74171\n",
      "epoch no.1 train no.147210  loss = 3.40036 avg_loss = 3.73935\n",
      "epoch no.1 train no.147220  loss = 4.28433 avg_loss = 3.71353\n",
      "epoch no.1 train no.147230  loss = 3.92821 avg_loss = 3.74513\n",
      "epoch no.1 train no.147240  loss = 5.28740 avg_loss = 3.80831\n",
      "epoch no.1 train no.147250  loss = 3.65255 avg_loss = 3.82876\n",
      "epoch no.1 train no.147260  loss = 3.82157 avg_loss = 3.87898\n",
      "epoch no.1 train no.147270  loss = 2.21613 avg_loss = 3.90295\n",
      "epoch no.1 train no.147280  loss = 3.50276 avg_loss = 3.88936\n",
      "epoch no.1 train no.147290  loss = 2.67050 avg_loss = 3.86277\n",
      "epoch no.1 train no.147300  loss = 4.28926 avg_loss = 3.91032\n",
      "epoch no.1 train no.147310  loss = 3.35435 avg_loss = 3.94425\n",
      "epoch no.1 train no.147320  loss = 3.12767 avg_loss = 3.89326\n",
      "epoch no.1 train no.147330  loss = 5.67533 avg_loss = 3.89973\n",
      "epoch no.1 train no.147340  loss = 3.73668 avg_loss = 3.92293\n",
      "epoch no.1 train no.147350  loss = 3.35802 avg_loss = 3.87204\n",
      "epoch no.1 train no.147360  loss = 2.58817 avg_loss = 3.79826\n",
      "epoch no.1 train no.147370  loss = 2.46637 avg_loss = 3.81256\n",
      "epoch no.1 train no.147380  loss = 5.78012 avg_loss = 3.79771\n",
      "epoch no.1 train no.147390  loss = 2.97968 avg_loss = 3.80765\n",
      "epoch no.1 train no.147400  loss = 4.81489 avg_loss = 3.80334\n",
      "epoch no.1 train no.147410  loss = 3.62159 avg_loss = 3.78864\n",
      "epoch no.1 train no.147420  loss = 2.46837 avg_loss = 3.82590\n",
      "epoch no.1 train no.147430  loss = 5.23232 avg_loss = 3.84459\n",
      "epoch no.1 train no.147440  loss = 2.76856 avg_loss = 3.79320\n",
      "epoch no.1 train no.147450  loss = 3.29571 avg_loss = 3.76753\n",
      "epoch no.1 train no.147460  loss = 1.72868 avg_loss = 3.71346\n",
      "epoch no.1 train no.147470  loss = 2.65063 avg_loss = 3.71321\n",
      "epoch no.1 train no.147480  loss = 3.87319 avg_loss = 3.75605\n",
      "epoch no.1 train no.147490  loss = 2.78983 avg_loss = 3.75840\n",
      "epoch no.1 train no.147500  loss = 1.63793 avg_loss = 3.75806\n",
      "epoch no.1 train no.147510  loss = 4.84691 avg_loss = 3.75871\n",
      "epoch no.1 train no.147520  loss = 4.49876 avg_loss = 3.76974\n",
      "epoch no.1 train no.147530  loss = 2.45698 avg_loss = 3.72465\n",
      "epoch no.1 train no.147540  loss = 3.29260 avg_loss = 3.74532\n",
      "epoch no.1 train no.147550  loss = 2.50198 avg_loss = 3.72004\n",
      "epoch no.1 train no.147560  loss = 5.39520 avg_loss = 3.76617\n",
      "epoch no.1 train no.147570  loss = 3.12365 avg_loss = 3.78302\n",
      "epoch no.1 train no.147580  loss = 3.29513 avg_loss = 3.76114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.147590  loss = 2.74281 avg_loss = 3.76354\n",
      "epoch no.1 train no.147600  loss = 3.31334 avg_loss = 3.74112\n",
      "epoch no.1 train no.147610  loss = 1.87164 avg_loss = 3.73439\n",
      "epoch no.1 train no.147620  loss = 3.46438 avg_loss = 3.72751\n",
      "epoch no.1 train no.147630  loss = 3.40383 avg_loss = 3.74488\n",
      "epoch no.1 train no.147640  loss = 4.20867 avg_loss = 3.76380\n",
      "epoch no.1 train no.147650  loss = 3.09279 avg_loss = 3.81332\n",
      "epoch no.1 train no.147660  loss = 3.88420 avg_loss = 3.84910\n",
      "epoch no.1 train no.147670  loss = 3.76573 avg_loss = 3.82031\n",
      "epoch no.1 train no.147680  loss = 2.93614 avg_loss = 3.83334\n",
      "epoch no.1 train no.147690  loss = 2.92933 avg_loss = 3.80909\n",
      "epoch no.1 train no.147700  loss = 3.63913 avg_loss = 3.84670\n",
      "epoch no.1 train no.147710  loss = 4.59021 avg_loss = 3.89587\n",
      "epoch no.1 train no.147720  loss = 4.77638 avg_loss = 3.93959\n",
      "epoch no.1 train no.147730  loss = 2.37598 avg_loss = 3.91254\n",
      "epoch no.1 train no.147740  loss = 4.35060 avg_loss = 3.92601\n",
      "epoch no.1 train no.147750  loss = 1.83748 avg_loss = 3.83283\n",
      "epoch no.1 train no.147760  loss = 5.71100 avg_loss = 3.84088\n",
      "epoch no.1 train no.147770  loss = 2.54509 avg_loss = 3.87717\n",
      "epoch no.1 train no.147780  loss = 6.36720 avg_loss = 3.90076\n",
      "epoch no.1 train no.147790  loss = 2.47091 avg_loss = 3.89601\n",
      "epoch no.1 train no.147800  loss = 4.39592 avg_loss = 3.88189\n",
      "epoch no.1 train no.147810  loss = 2.60691 avg_loss = 3.85045\n",
      "epoch no.1 train no.147820  loss = 3.69734 avg_loss = 3.81708\n",
      "epoch no.1 train no.147830  loss = 5.60617 avg_loss = 3.82725\n",
      "epoch no.1 train no.147840  loss = 3.56020 avg_loss = 3.77843\n",
      "epoch no.1 train no.147850  loss = 3.37486 avg_loss = 3.70688\n",
      "epoch no.1 train no.147860  loss = 2.54733 avg_loss = 3.66889\n",
      "epoch no.1 train no.147870  loss = 3.79835 avg_loss = 3.68353\n",
      "epoch no.1 train no.147880  loss = 6.15274 avg_loss = 3.73736\n",
      "epoch no.1 train no.147890  loss = 6.24766 avg_loss = 3.78687\n",
      "epoch no.1 train no.147900  loss = 2.37099 avg_loss = 3.77751\n",
      "epoch no.1 train no.147910  loss = 2.78304 avg_loss = 3.80822\n",
      "epoch no.1 train no.147920  loss = 2.75021 avg_loss = 3.77109\n",
      "epoch no.1 train no.147930  loss = 4.06420 avg_loss = 3.79527\n",
      "epoch no.1 train no.147940  loss = 4.10184 avg_loss = 3.82573\n",
      "epoch no.1 train no.147950  loss = 3.30941 avg_loss = 3.79769\n",
      "epoch no.1 train no.147960  loss = 3.98455 avg_loss = 3.79662\n",
      "epoch no.1 train no.147970  loss = 3.84033 avg_loss = 3.82065\n",
      "epoch no.1 train no.147980  loss = 2.78296 avg_loss = 3.81141\n",
      "epoch no.1 train no.147990  loss = 3.87324 avg_loss = 3.82462\n",
      "epoch no.1 train no.148000  loss = 2.57674 avg_loss = 3.81139\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.148010  loss = 2.52602 avg_loss = 3.80055\n",
      "epoch no.1 train no.148020  loss = 6.11789 avg_loss = 3.85697\n",
      "epoch no.1 train no.148030  loss = 2.80508 avg_loss = 3.83771\n",
      "epoch no.1 train no.148040  loss = 4.17717 avg_loss = 3.80767\n",
      "epoch no.1 train no.148050  loss = 2.06170 avg_loss = 3.74334\n",
      "epoch no.1 train no.148060  loss = 3.79755 avg_loss = 3.77732\n",
      "epoch no.1 train no.148070  loss = 3.54046 avg_loss = 3.74928\n",
      "epoch no.1 train no.148080  loss = 4.08781 avg_loss = 3.77703\n",
      "epoch no.1 train no.148090  loss = 4.14085 avg_loss = 3.67888\n",
      "epoch no.1 train no.148100  loss = 1.98724 avg_loss = 3.71094\n",
      "epoch no.1 train no.148110  loss = 3.68175 avg_loss = 3.73158\n",
      "epoch no.1 train no.148120  loss = 3.28843 avg_loss = 3.76043\n",
      "epoch no.1 train no.148130  loss = 5.44290 avg_loss = 3.76833\n",
      "epoch no.1 train no.148140  loss = 2.72082 avg_loss = 3.78850\n",
      "epoch no.1 train no.148150  loss = 3.59718 avg_loss = 3.85714\n",
      "epoch no.1 train no.148160  loss = 3.37675 avg_loss = 3.83506\n",
      "epoch no.1 train no.148170  loss = 2.68889 avg_loss = 3.82722\n",
      "epoch no.1 train no.148180  loss = 3.77795 avg_loss = 3.85906\n",
      "epoch no.1 train no.148190  loss = 1.68152 avg_loss = 3.82852\n",
      "epoch no.1 train no.148200  loss = 2.41158 avg_loss = 3.82111\n",
      "epoch no.1 train no.148210  loss = 3.32292 avg_loss = 3.75751\n",
      "epoch no.1 train no.148220  loss = 4.90510 avg_loss = 3.71199\n",
      "epoch no.1 train no.148230  loss = 4.34666 avg_loss = 3.72061\n",
      "epoch no.1 train no.148240  loss = 3.38861 avg_loss = 3.80816\n",
      "epoch no.1 train no.148250  loss = 4.12103 avg_loss = 3.80187\n",
      "epoch no.1 train no.148260  loss = 3.73906 avg_loss = 3.84846\n",
      "epoch no.1 train no.148270  loss = 6.53280 avg_loss = 3.82541\n",
      "epoch no.1 train no.148280  loss = 5.45701 avg_loss = 3.89490\n",
      "epoch no.1 train no.148290  loss = 3.14496 avg_loss = 3.85416\n",
      "epoch no.1 train no.148300  loss = 5.62475 avg_loss = 3.93607\n",
      "epoch no.1 train no.148310  loss = 3.56982 avg_loss = 3.96354\n",
      "epoch no.1 train no.148320  loss = 3.43476 avg_loss = 3.99252\n",
      "epoch no.1 train no.148330  loss = 3.94352 avg_loss = 4.01692\n",
      "epoch no.1 train no.148340  loss = 6.57571 avg_loss = 3.98912\n",
      "epoch no.1 train no.148350  loss = 4.04039 avg_loss = 3.99884\n",
      "epoch no.1 train no.148360  loss = 5.27038 avg_loss = 3.96483\n",
      "epoch no.1 train no.148370  loss = 6.27024 avg_loss = 3.98096\n",
      "epoch no.1 train no.148380  loss = 5.01263 avg_loss = 3.96952\n",
      "epoch no.1 train no.148390  loss = 5.24403 avg_loss = 3.98085\n",
      "epoch no.1 train no.148400  loss = 2.48687 avg_loss = 3.93331\n",
      "epoch no.1 train no.148410  loss = 5.51559 avg_loss = 3.93308\n",
      "epoch no.1 train no.148420  loss = 3.68760 avg_loss = 3.99137\n",
      "epoch no.1 train no.148430  loss = 3.37584 avg_loss = 3.97595\n",
      "epoch no.1 train no.148440  loss = 3.31925 avg_loss = 3.92604\n",
      "epoch no.1 train no.148450  loss = 3.37802 avg_loss = 3.92975\n",
      "epoch no.1 train no.148460  loss = 3.91235 avg_loss = 3.93126\n",
      "epoch no.1 train no.148470  loss = 5.50976 avg_loss = 3.88460\n",
      "epoch no.1 train no.148480  loss = 2.88113 avg_loss = 3.84879\n",
      "epoch no.1 train no.148490  loss = 3.51761 avg_loss = 3.80446\n",
      "epoch no.1 train no.148500  loss = 3.47839 avg_loss = 3.79311\n",
      "epoch no.1 train no.148510  loss = 2.84877 avg_loss = 3.77945\n",
      "epoch no.1 train no.148520  loss = 3.82183 avg_loss = 3.79591\n",
      "epoch no.1 train no.148530  loss = 4.00888 avg_loss = 3.75296\n",
      "epoch no.1 train no.148540  loss = 3.59551 avg_loss = 3.76443\n",
      "epoch no.1 train no.148550  loss = 4.15989 avg_loss = 3.73912\n",
      "epoch no.1 train no.148560  loss = 3.81372 avg_loss = 3.76567\n",
      "epoch no.1 train no.148570  loss = 3.75889 avg_loss = 3.73706\n",
      "epoch no.1 train no.148580  loss = 2.03794 avg_loss = 3.71450\n",
      "epoch no.1 train no.148590  loss = 3.13341 avg_loss = 3.73208\n",
      "epoch no.1 train no.148600  loss = 1.82308 avg_loss = 3.75787\n",
      "epoch no.1 train no.148610  loss = 3.25359 avg_loss = 3.77137\n",
      "epoch no.1 train no.148620  loss = 2.69066 avg_loss = 3.73205\n",
      "epoch no.1 train no.148630  loss = 7.01485 avg_loss = 3.75890\n",
      "epoch no.1 train no.148640  loss = 3.04935 avg_loss = 3.75550\n",
      "epoch no.1 train no.148650  loss = 4.08427 avg_loss = 3.76203\n",
      "epoch no.1 train no.148660  loss = 2.82960 avg_loss = 3.67461\n",
      "epoch no.1 train no.148670  loss = 3.45715 avg_loss = 3.65656\n",
      "epoch no.1 train no.148680  loss = 3.62096 avg_loss = 3.68633\n",
      "epoch no.1 train no.148690  loss = 4.53576 avg_loss = 3.67393\n",
      "epoch no.1 train no.148700  loss = 5.25407 avg_loss = 3.66033\n",
      "epoch no.1 train no.148710  loss = 3.16307 avg_loss = 3.62975\n",
      "epoch no.1 train no.148720  loss = 4.06305 avg_loss = 3.61997\n",
      "epoch no.1 train no.148730  loss = 2.56028 avg_loss = 3.61734\n",
      "epoch no.1 train no.148740  loss = 3.70928 avg_loss = 3.65643\n",
      "epoch no.1 train no.148750  loss = 3.44877 avg_loss = 3.65852\n",
      "epoch no.1 train no.148760  loss = 3.85743 avg_loss = 3.59700\n",
      "epoch no.1 train no.148770  loss = 3.80677 avg_loss = 3.64360\n",
      "epoch no.1 train no.148780  loss = 4.21663 avg_loss = 3.68862\n",
      "epoch no.1 train no.148790  loss = 2.89713 avg_loss = 3.66903\n",
      "epoch no.1 train no.148800  loss = 3.15843 avg_loss = 3.65885\n",
      "epoch no.1 train no.148810  loss = 3.57685 avg_loss = 3.66518\n",
      "epoch no.1 train no.148820  loss = 2.78421 avg_loss = 3.71563\n",
      "epoch no.1 train no.148830  loss = 3.06948 avg_loss = 3.73626\n",
      "epoch no.1 train no.148840  loss = 2.61579 avg_loss = 3.70844\n",
      "epoch no.1 train no.148850  loss = 4.34140 avg_loss = 3.70889\n",
      "epoch no.1 train no.148860  loss = 1.80808 avg_loss = 3.71612\n",
      "epoch no.1 train no.148870  loss = 3.89574 avg_loss = 3.76320\n",
      "epoch no.1 train no.148880  loss = 5.62126 avg_loss = 3.75039\n",
      "epoch no.1 train no.148890  loss = 4.49268 avg_loss = 3.76253\n",
      "epoch no.1 train no.148900  loss = 3.50592 avg_loss = 3.72699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.148910  loss = 3.42359 avg_loss = 3.72912\n",
      "epoch no.1 train no.148920  loss = 3.08394 avg_loss = 3.71367\n",
      "epoch no.1 train no.148930  loss = 7.89903 avg_loss = 3.78399\n",
      "epoch no.1 train no.148940  loss = 3.49332 avg_loss = 3.85070\n",
      "epoch no.1 train no.148950  loss = 7.33001 avg_loss = 3.88657\n",
      "epoch no.1 train no.148960  loss = 2.53132 avg_loss = 3.96494\n",
      "epoch no.1 train no.148970  loss = 3.84439 avg_loss = 3.95857\n",
      "epoch no.1 train no.148980  loss = 5.17342 avg_loss = 3.98030\n",
      "epoch no.1 train no.148990  loss = 3.35249 avg_loss = 3.92460\n",
      "epoch no.1 train no.149000  loss = 5.75884 avg_loss = 3.95273\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '리스트', '</s>']\n",
      "기분전환을 위한 플레이리스트</s>\n",
      "epoch no.1 train no.149010  loss = 2.37274 avg_loss = 3.87308\n",
      "epoch no.1 train no.149020  loss = 4.12526 avg_loss = 3.87183\n",
      "epoch no.1 train no.149030  loss = 3.93549 avg_loss = 3.91889\n",
      "epoch no.1 train no.149040  loss = 5.67204 avg_loss = 3.87448\n",
      "epoch no.1 train no.149050  loss = 3.46214 avg_loss = 3.82213\n",
      "epoch no.1 train no.149060  loss = 4.49885 avg_loss = 3.83867\n",
      "epoch no.1 train no.149070  loss = 2.62875 avg_loss = 3.85344\n",
      "epoch no.1 train no.149080  loss = 4.92841 avg_loss = 3.85999\n",
      "epoch no.1 train no.149090  loss = 7.58450 avg_loss = 3.89219\n",
      "epoch no.1 train no.149100  loss = 3.78569 avg_loss = 3.86552\n",
      "epoch no.1 train no.149110  loss = 3.47522 avg_loss = 3.83424\n",
      "epoch no.1 train no.149120  loss = 3.11599 avg_loss = 3.82800\n",
      "epoch no.1 train no.149130  loss = 3.35406 avg_loss = 3.77591\n",
      "epoch no.1 train no.149140  loss = 2.25514 avg_loss = 3.72770\n",
      "epoch no.1 train no.149150  loss = 3.76345 avg_loss = 3.70348\n",
      "epoch no.1 train no.149160  loss = 4.52944 avg_loss = 3.67100\n",
      "epoch no.1 train no.149170  loss = 4.79755 avg_loss = 3.68937\n",
      "epoch no.1 train no.149180  loss = 4.80108 avg_loss = 3.67231\n",
      "epoch no.1 train no.149190  loss = 6.46607 avg_loss = 3.75855\n",
      "epoch no.1 train no.149200  loss = 4.90262 avg_loss = 3.78792\n",
      "epoch no.1 train no.149210  loss = 2.40711 avg_loss = 3.75270\n",
      "epoch no.1 train no.149220  loss = 3.36664 avg_loss = 3.72514\n",
      "epoch no.1 train no.149230  loss = 2.73203 avg_loss = 3.71861\n",
      "epoch no.1 train no.149240  loss = 4.58755 avg_loss = 3.72262\n",
      "epoch no.1 train no.149250  loss = 3.68667 avg_loss = 3.69685\n",
      "epoch no.1 train no.149260  loss = 2.74574 avg_loss = 3.67148\n",
      "epoch no.1 train no.149270  loss = 4.82192 avg_loss = 3.69866\n",
      "epoch no.1 train no.149280  loss = 4.17676 avg_loss = 3.65271\n",
      "epoch no.1 train no.149290  loss = 3.84038 avg_loss = 3.69277\n",
      "epoch no.1 train no.149300  loss = 3.60916 avg_loss = 3.66700\n",
      "epoch no.1 train no.149310  loss = 2.90488 avg_loss = 3.67859\n",
      "epoch no.1 train no.149320  loss = 3.72317 avg_loss = 3.63461\n",
      "epoch no.1 train no.149330  loss = 2.86474 avg_loss = 3.60526\n",
      "epoch no.1 train no.149340  loss = 4.15236 avg_loss = 3.64017\n",
      "epoch no.1 train no.149350  loss = 2.43843 avg_loss = 3.62154\n",
      "epoch no.1 train no.149360  loss = 2.52919 avg_loss = 3.60800\n",
      "epoch no.1 train no.149370  loss = 5.16753 avg_loss = 3.63293\n",
      "epoch no.1 train no.149380  loss = 2.97688 avg_loss = 3.68942\n",
      "epoch no.1 train no.149390  loss = 5.02257 avg_loss = 3.64973\n",
      "epoch no.1 train no.149400  loss = 5.30822 avg_loss = 3.70020\n",
      "epoch no.1 train no.149410  loss = 2.32571 avg_loss = 3.65544\n",
      "epoch no.1 train no.149420  loss = 3.54406 avg_loss = 3.64159\n",
      "epoch no.1 train no.149430  loss = 5.87139 avg_loss = 3.66572\n",
      "epoch no.1 train no.149440  loss = 2.92566 avg_loss = 3.71422\n",
      "epoch no.1 train no.149450  loss = 3.71816 avg_loss = 3.72532\n",
      "epoch no.1 train no.149460  loss = 2.74388 avg_loss = 3.69349\n",
      "epoch no.1 train no.149470  loss = 2.97966 avg_loss = 3.68107\n",
      "epoch no.1 train no.149480  loss = 4.01208 avg_loss = 3.71012\n",
      "epoch no.1 train no.149490  loss = 4.21727 avg_loss = 3.65750\n",
      "epoch no.1 train no.149500  loss = 3.32078 avg_loss = 3.64379\n",
      "epoch no.1 train no.149510  loss = 6.35109 avg_loss = 3.69824\n",
      "epoch no.1 train no.149520  loss = 5.42673 avg_loss = 3.68665\n",
      "epoch no.1 train no.149530  loss = 1.70326 avg_loss = 3.69278\n",
      "epoch no.1 train no.149540  loss = 2.15752 avg_loss = 3.67809\n",
      "epoch no.1 train no.149550  loss = 2.44859 avg_loss = 3.68323\n",
      "epoch no.1 train no.149560  loss = 6.49470 avg_loss = 3.71881\n",
      "epoch no.1 train no.149570  loss = 4.33809 avg_loss = 3.70951\n",
      "epoch no.1 train no.149580  loss = 2.57676 avg_loss = 3.74910\n",
      "epoch no.1 train no.149590  loss = 3.53506 avg_loss = 3.78673\n",
      "epoch no.1 train no.149600  loss = 1.94184 avg_loss = 3.75197\n",
      "epoch no.1 train no.149610  loss = 3.66661 avg_loss = 3.76905\n",
      "epoch no.1 train no.149620  loss = 3.99544 avg_loss = 3.76691\n",
      "epoch no.1 train no.149630  loss = 3.99279 avg_loss = 3.75801\n",
      "epoch no.1 train no.149640  loss = 2.23900 avg_loss = 3.72625\n",
      "epoch no.1 train no.149650  loss = 7.90848 avg_loss = 3.75473\n",
      "epoch no.1 train no.149660  loss = 4.67793 avg_loss = 3.79577\n",
      "epoch no.1 train no.149670  loss = 6.46304 avg_loss = 3.80921\n",
      "epoch no.1 train no.149680  loss = 4.54326 avg_loss = 3.82556\n",
      "epoch no.1 train no.149690  loss = 5.20611 avg_loss = 3.82756\n",
      "epoch no.1 train no.149700  loss = 2.24579 avg_loss = 3.88900\n",
      "epoch no.1 train no.149710  loss = 3.79943 avg_loss = 3.85438\n",
      "epoch no.1 train no.149720  loss = 2.89238 avg_loss = 3.86680\n",
      "epoch no.1 train no.149730  loss = 3.16019 avg_loss = 3.83818\n",
      "epoch no.1 train no.149740  loss = 3.43703 avg_loss = 3.85425\n",
      "epoch no.1 train no.149750  loss = 3.06622 avg_loss = 3.79421\n",
      "epoch no.1 train no.149760  loss = 4.46285 avg_loss = 3.78216\n",
      "epoch no.1 train no.149770  loss = 3.95419 avg_loss = 3.77913\n",
      "epoch no.1 train no.149780  loss = 3.83135 avg_loss = 3.73390\n",
      "epoch no.1 train no.149790  loss = 3.30436 avg_loss = 3.72391\n",
      "epoch no.1 train no.149800  loss = 2.72286 avg_loss = 3.68950\n",
      "epoch no.1 train no.149810  loss = 4.20963 avg_loss = 3.69508\n",
      "epoch no.1 train no.149820  loss = 5.33582 avg_loss = 3.72692\n",
      "epoch no.1 train no.149830  loss = 3.11392 avg_loss = 3.71866\n",
      "epoch no.1 train no.149840  loss = 3.55698 avg_loss = 3.73559\n",
      "epoch no.1 train no.149850  loss = 5.17972 avg_loss = 3.70278\n",
      "epoch no.1 train no.149860  loss = 3.00518 avg_loss = 3.72511\n",
      "epoch no.1 train no.149870  loss = 3.43795 avg_loss = 3.71741\n",
      "epoch no.1 train no.149880  loss = 3.25057 avg_loss = 3.72506\n",
      "epoch no.1 train no.149890  loss = 2.50632 avg_loss = 3.72252\n",
      "epoch no.1 train no.149900  loss = 3.15169 avg_loss = 3.73922\n",
      "epoch no.1 train no.149910  loss = 3.57207 avg_loss = 3.74889\n",
      "epoch no.1 train no.149920  loss = 5.38781 avg_loss = 3.76799\n",
      "epoch no.1 train no.149930  loss = 3.56369 avg_loss = 3.80080\n",
      "epoch no.1 train no.149940  loss = 3.22831 avg_loss = 3.81809\n",
      "epoch no.1 train no.149950  loss = 4.79682 avg_loss = 3.80903\n",
      "epoch no.1 train no.149960  loss = 5.44525 avg_loss = 3.77088\n",
      "epoch no.1 train no.149970  loss = 3.33542 avg_loss = 3.76019\n",
      "epoch no.1 train no.149980  loss = 4.56630 avg_loss = 3.81545\n",
      "epoch no.1 train no.149990  loss = 4.11454 avg_loss = 3.80741\n",
      "epoch no.1 train no.150000  loss = 2.83204 avg_loss = 3.76483\n",
      "5\n",
      "to_tokens: ['▁비', '좋은', '이', '때', '▁듣기', '좋은', '▁노래', '</s>']\n",
      "기분전환 할때 듣기좋은노래</s>\n",
      "epoch no.1 train no.150010  loss = 2.85578 avg_loss = 3.74111\n",
      "epoch no.1 train no.150020  loss = 4.23641 avg_loss = 3.75578\n",
      "epoch no.1 train no.150030  loss = 3.17205 avg_loss = 3.77562\n",
      "epoch no.1 train no.150040  loss = 4.44696 avg_loss = 3.77644\n",
      "epoch no.1 train no.150050  loss = 2.61620 avg_loss = 3.76393\n",
      "epoch no.1 train no.150060  loss = 2.02001 avg_loss = 3.71742\n",
      "epoch no.1 train no.150070  loss = 3.22131 avg_loss = 3.66354\n",
      "epoch no.1 train no.150080  loss = 3.29715 avg_loss = 3.71325\n",
      "epoch no.1 train no.150090  loss = 3.71014 avg_loss = 3.69107\n",
      "epoch no.1 train no.150100  loss = 2.73798 avg_loss = 3.71726\n",
      "epoch no.1 train no.150110  loss = 4.84866 avg_loss = 3.70593\n",
      "epoch no.1 train no.150120  loss = 6.40033 avg_loss = 3.73748\n",
      "epoch no.1 train no.150130  loss = 2.21640 avg_loss = 3.79370\n",
      "epoch no.1 train no.150140  loss = 3.70954 avg_loss = 3.83541\n",
      "epoch no.1 train no.150150  loss = 4.05791 avg_loss = 3.92274\n",
      "epoch no.1 train no.150160  loss = 4.12667 avg_loss = 3.91328\n",
      "epoch no.1 train no.150170  loss = 5.08648 avg_loss = 3.92969\n",
      "epoch no.1 train no.150180  loss = 2.55306 avg_loss = 3.91580\n",
      "epoch no.1 train no.150190  loss = 6.83785 avg_loss = 3.91555\n",
      "epoch no.1 train no.150200  loss = 2.06913 avg_loss = 3.89674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.150210  loss = 3.72323 avg_loss = 3.88511\n",
      "epoch no.1 train no.150220  loss = 4.86790 avg_loss = 3.93138\n",
      "epoch no.1 train no.150230  loss = 3.77886 avg_loss = 3.84469\n",
      "epoch no.1 train no.150240  loss = 4.37398 avg_loss = 3.83488\n",
      "epoch no.1 train no.150250  loss = 5.31983 avg_loss = 3.82671\n",
      "epoch no.1 train no.150260  loss = 3.01163 avg_loss = 3.78454\n",
      "epoch no.1 train no.150270  loss = 5.79770 avg_loss = 3.73798\n",
      "epoch no.1 train no.150280  loss = 3.21445 avg_loss = 3.71282\n",
      "epoch no.1 train no.150290  loss = 2.31134 avg_loss = 3.73128\n",
      "epoch no.1 train no.150300  loss = 3.08112 avg_loss = 3.71950\n",
      "epoch no.1 train no.150310  loss = 3.18581 avg_loss = 3.74348\n",
      "epoch no.1 train no.150320  loss = 2.51753 avg_loss = 3.79148\n",
      "epoch no.1 train no.150330  loss = 3.75117 avg_loss = 3.73177\n",
      "epoch no.1 train no.150340  loss = 3.79809 avg_loss = 3.70626\n",
      "epoch no.1 train no.150350  loss = 2.91280 avg_loss = 3.72288\n",
      "epoch no.1 train no.150360  loss = 2.36196 avg_loss = 3.71673\n",
      "epoch no.1 train no.150370  loss = 3.58400 avg_loss = 3.70932\n",
      "epoch no.1 train no.150380  loss = 5.19306 avg_loss = 3.73247\n",
      "epoch no.1 train no.150390  loss = 2.05625 avg_loss = 3.73010\n",
      "epoch no.1 train no.150400  loss = 4.60535 avg_loss = 3.74309\n",
      "epoch no.1 train no.150410  loss = 3.59653 avg_loss = 3.69993\n",
      "epoch no.1 train no.150420  loss = 3.07529 avg_loss = 3.68213\n",
      "epoch no.1 train no.150430  loss = 5.93225 avg_loss = 3.78330\n",
      "epoch no.1 train no.150440  loss = 5.88916 avg_loss = 3.81279\n",
      "epoch no.1 train no.150450  loss = 4.09453 avg_loss = 3.77368\n",
      "epoch no.1 train no.150460  loss = 1.44670 avg_loss = 3.77639\n",
      "epoch no.1 train no.150470  loss = 3.63953 avg_loss = 3.78353\n",
      "epoch no.1 train no.150480  loss = 2.19293 avg_loss = 3.74828\n",
      "epoch no.1 train no.150490  loss = 2.47599 avg_loss = 3.77969\n",
      "epoch no.1 train no.150500  loss = 3.08940 avg_loss = 3.79263\n",
      "epoch no.1 train no.150510  loss = 1.87360 avg_loss = 3.78192\n",
      "epoch no.1 train no.150520  loss = 2.16900 avg_loss = 3.82314\n",
      "epoch no.1 train no.150530  loss = 3.57278 avg_loss = 3.80381\n",
      "epoch no.1 train no.150540  loss = 3.25876 avg_loss = 3.76709\n",
      "epoch no.1 train no.150550  loss = 2.49499 avg_loss = 3.72453\n",
      "epoch no.1 train no.150560  loss = 4.14648 avg_loss = 3.75658\n",
      "epoch no.1 train no.150570  loss = 2.89560 avg_loss = 3.76891\n",
      "epoch no.1 train no.150580  loss = 7.27709 avg_loss = 3.87483\n",
      "epoch no.1 train no.150590  loss = 3.54455 avg_loss = 3.93224\n",
      "epoch no.1 train no.150600  loss = 2.10739 avg_loss = 3.91544\n",
      "epoch no.1 train no.150610  loss = 3.36876 avg_loss = 3.89677\n",
      "epoch no.1 train no.150620  loss = 2.76739 avg_loss = 3.86036\n",
      "epoch no.1 train no.150630  loss = 3.98853 avg_loss = 3.86263\n",
      "epoch no.1 train no.150640  loss = 4.79805 avg_loss = 3.89676\n",
      "epoch no.1 train no.150650  loss = 4.21237 avg_loss = 3.89252\n",
      "epoch no.1 train no.150660  loss = 3.57204 avg_loss = 3.89506\n",
      "epoch no.1 train no.150670  loss = 3.53367 avg_loss = 3.88081\n",
      "epoch no.1 train no.150680  loss = 4.93448 avg_loss = 3.86641\n",
      "epoch no.1 train no.150690  loss = 4.40435 avg_loss = 3.86150\n",
      "epoch no.1 train no.150700  loss = 2.39170 avg_loss = 3.84169\n",
      "epoch no.1 train no.150710  loss = 5.75585 avg_loss = 3.86918\n",
      "epoch no.1 train no.150720  loss = 2.95441 avg_loss = 3.84004\n",
      "epoch no.1 train no.150730  loss = 6.69921 avg_loss = 3.89478\n",
      "epoch no.1 train no.150740  loss = 2.87658 avg_loss = 3.87895\n",
      "epoch no.1 train no.150750  loss = 4.19002 avg_loss = 3.84815\n",
      "epoch no.1 train no.150760  loss = 2.91288 avg_loss = 3.85880\n",
      "epoch no.1 train no.150770  loss = 2.44599 avg_loss = 3.79186\n",
      "epoch no.1 train no.150780  loss = 2.98627 avg_loss = 3.82680\n",
      "epoch no.1 train no.150790  loss = 2.46527 avg_loss = 3.87838\n",
      "epoch no.1 train no.150800  loss = 7.76115 avg_loss = 3.85559\n",
      "epoch no.1 train no.150810  loss = 3.58571 avg_loss = 3.86800\n",
      "epoch no.1 train no.150820  loss = 3.99905 avg_loss = 3.81759\n",
      "epoch no.1 train no.150830  loss = 5.13656 avg_loss = 3.83376\n",
      "epoch no.1 train no.150840  loss = 2.95646 avg_loss = 3.79308\n",
      "epoch no.1 train no.150850  loss = 3.54697 avg_loss = 3.80349\n",
      "epoch no.1 train no.150860  loss = 3.30001 avg_loss = 3.80593\n",
      "epoch no.1 train no.150870  loss = 4.60559 avg_loss = 3.83322\n",
      "epoch no.1 train no.150880  loss = 7.62728 avg_loss = 3.93691\n",
      "epoch no.1 train no.150890  loss = 2.23500 avg_loss = 3.94698\n",
      "epoch no.1 train no.150900  loss = 2.79124 avg_loss = 3.91227\n",
      "epoch no.1 train no.150910  loss = 4.61967 avg_loss = 3.91869\n",
      "epoch no.1 train no.150920  loss = 4.27466 avg_loss = 3.92068\n",
      "epoch no.1 train no.150930  loss = 5.64270 avg_loss = 3.90749\n",
      "epoch no.1 train no.150940  loss = 2.83749 avg_loss = 3.88617\n",
      "epoch no.1 train no.150950  loss = 3.71731 avg_loss = 3.87702\n",
      "epoch no.1 train no.150960  loss = 4.44391 avg_loss = 3.84995\n",
      "epoch no.1 train no.150970  loss = 5.14581 avg_loss = 3.89549\n",
      "epoch no.1 train no.150980  loss = 2.54503 avg_loss = 3.92480\n",
      "epoch no.1 train no.150990  loss = 2.66154 avg_loss = 3.94910\n",
      "epoch no.1 train no.151000  loss = 2.12001 avg_loss = 3.95951\n",
      "4\n",
      "to_tokens: ['▁비', '좋은', '용', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환 할때 듣는 노래</s>\n",
      "epoch no.1 train no.151010  loss = 3.25760 avg_loss = 3.92383\n",
      "epoch no.1 train no.151020  loss = 1.91299 avg_loss = 3.90231\n",
      "epoch no.1 train no.151030  loss = 3.19158 avg_loss = 3.87330\n",
      "epoch no.1 train no.151040  loss = 2.59313 avg_loss = 3.82409\n",
      "epoch no.1 train no.151050  loss = 4.79341 avg_loss = 3.84732\n",
      "epoch no.1 train no.151060  loss = 3.01325 avg_loss = 3.84712\n",
      "epoch no.1 train no.151070  loss = 4.06595 avg_loss = 3.84394\n",
      "epoch no.1 train no.151080  loss = 3.61887 avg_loss = 3.85755\n",
      "epoch no.1 train no.151090  loss = 4.29283 avg_loss = 3.85368\n",
      "epoch no.1 train no.151100  loss = 3.59356 avg_loss = 3.90795\n",
      "epoch no.1 train no.151110  loss = 5.14486 avg_loss = 3.91502\n",
      "epoch no.1 train no.151120  loss = 4.63350 avg_loss = 3.91118\n",
      "epoch no.1 train no.151130  loss = 1.98852 avg_loss = 3.91548\n",
      "epoch no.1 train no.151140  loss = 3.72418 avg_loss = 3.90169\n",
      "epoch no.1 train no.151150  loss = 4.31498 avg_loss = 3.86339\n",
      "epoch no.1 train no.151160  loss = 4.66924 avg_loss = 3.86843\n",
      "epoch no.1 train no.151170  loss = 5.50187 avg_loss = 3.84334\n",
      "epoch no.1 train no.151180  loss = 3.63712 avg_loss = 3.91864\n",
      "epoch no.1 train no.151190  loss = 5.07288 avg_loss = 3.85376\n",
      "epoch no.1 train no.151200  loss = 4.18494 avg_loss = 3.83866\n",
      "epoch no.1 train no.151210  loss = 4.13949 avg_loss = 3.82291\n",
      "epoch no.1 train no.151220  loss = 4.09515 avg_loss = 3.80675\n",
      "epoch no.1 train no.151230  loss = 1.98758 avg_loss = 3.76742\n",
      "epoch no.1 train no.151240  loss = 5.70492 avg_loss = 3.82498\n",
      "epoch no.1 train no.151250  loss = 2.87516 avg_loss = 3.77901\n",
      "epoch no.1 train no.151260  loss = 3.96507 avg_loss = 3.80272\n",
      "epoch no.1 train no.151270  loss = 3.40973 avg_loss = 3.77331\n",
      "epoch no.1 train no.151280  loss = 4.05494 avg_loss = 3.78606\n",
      "epoch no.1 train no.151290  loss = 4.07155 avg_loss = 3.78357\n",
      "epoch no.1 train no.151300  loss = 3.29327 avg_loss = 3.80302\n",
      "epoch no.1 train no.151310  loss = 2.14803 avg_loss = 3.76682\n",
      "epoch no.1 train no.151320  loss = 2.76723 avg_loss = 3.71765\n",
      "epoch no.1 train no.151330  loss = 3.36167 avg_loss = 3.67061\n",
      "epoch no.1 train no.151340  loss = 3.89630 avg_loss = 3.67540\n",
      "epoch no.1 train no.151350  loss = 3.04373 avg_loss = 3.64433\n",
      "epoch no.1 train no.151360  loss = 2.14295 avg_loss = 3.69501\n",
      "epoch no.1 train no.151370  loss = 6.94183 avg_loss = 3.71236\n",
      "epoch no.1 train no.151380  loss = 5.77895 avg_loss = 3.75796\n",
      "epoch no.1 train no.151390  loss = 3.57550 avg_loss = 3.81102\n",
      "epoch no.1 train no.151400  loss = 3.11383 avg_loss = 3.77732\n",
      "epoch no.1 train no.151410  loss = 4.83394 avg_loss = 3.77805\n",
      "epoch no.1 train no.151420  loss = 7.10664 avg_loss = 3.83714\n",
      "epoch no.1 train no.151430  loss = 3.52784 avg_loss = 3.81508\n",
      "epoch no.1 train no.151440  loss = 4.50516 avg_loss = 3.86165\n",
      "epoch no.1 train no.151450  loss = 4.07630 avg_loss = 3.87864\n",
      "epoch no.1 train no.151460  loss = 4.03967 avg_loss = 3.83381\n",
      "epoch no.1 train no.151470  loss = 4.43393 avg_loss = 3.85746\n",
      "epoch no.1 train no.151480  loss = 2.55509 avg_loss = 3.85122\n",
      "epoch no.1 train no.151490  loss = 3.92481 avg_loss = 3.84472\n",
      "epoch no.1 train no.151500  loss = 3.28180 avg_loss = 3.88228\n",
      "epoch no.1 train no.151510  loss = 4.09611 avg_loss = 3.85216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.151520  loss = 6.11721 avg_loss = 3.87148\n",
      "epoch no.1 train no.151530  loss = 2.93674 avg_loss = 3.89908\n",
      "epoch no.1 train no.151540  loss = 3.37530 avg_loss = 3.89958\n",
      "epoch no.1 train no.151550  loss = 5.10875 avg_loss = 3.91078\n",
      "epoch no.1 train no.151560  loss = 5.15355 avg_loss = 3.91836\n",
      "epoch no.1 train no.151570  loss = 4.15694 avg_loss = 3.94469\n",
      "epoch no.1 train no.151580  loss = 5.04190 avg_loss = 3.93350\n",
      "epoch no.1 train no.151590  loss = 3.58029 avg_loss = 3.89697\n",
      "epoch no.1 train no.151600  loss = 3.27888 avg_loss = 3.88410\n",
      "epoch no.1 train no.151610  loss = 4.41800 avg_loss = 3.94276\n",
      "epoch no.1 train no.151620  loss = 4.93432 avg_loss = 3.94964\n",
      "epoch no.1 train no.151630  loss = 3.74248 avg_loss = 3.93881\n",
      "epoch no.1 train no.151640  loss = 2.45955 avg_loss = 3.95793\n",
      "epoch no.1 train no.151650  loss = 3.98622 avg_loss = 3.93241\n",
      "epoch no.1 train no.151660  loss = 4.79937 avg_loss = 3.92653\n",
      "epoch no.1 train no.151670  loss = 3.21193 avg_loss = 3.92046\n",
      "epoch no.1 train no.151680  loss = 3.55840 avg_loss = 3.91808\n",
      "epoch no.1 train no.151690  loss = 4.22702 avg_loss = 3.93679\n",
      "epoch no.1 train no.151700  loss = 4.18707 avg_loss = 3.85186\n",
      "epoch no.1 train no.151710  loss = 3.11756 avg_loss = 3.82494\n",
      "epoch no.1 train no.151720  loss = 4.42502 avg_loss = 3.77120\n",
      "epoch no.1 train no.151730  loss = 5.98479 avg_loss = 3.76981\n",
      "epoch no.1 train no.151740  loss = 2.86912 avg_loss = 3.75573\n",
      "epoch no.1 train no.151750  loss = 3.26366 avg_loss = 3.72071\n",
      "epoch no.1 train no.151760  loss = 4.26219 avg_loss = 3.71088\n",
      "epoch no.1 train no.151770  loss = 4.61715 avg_loss = 3.74782\n",
      "epoch no.1 train no.151780  loss = 5.14954 avg_loss = 3.74711\n",
      "epoch no.1 train no.151790  loss = 3.57335 avg_loss = 3.75744\n",
      "epoch no.1 train no.151800  loss = 4.33739 avg_loss = 3.77213\n",
      "epoch no.1 train no.151810  loss = 2.64525 avg_loss = 3.72601\n",
      "epoch no.1 train no.151820  loss = 3.35085 avg_loss = 3.73197\n",
      "epoch no.1 train no.151830  loss = 4.41874 avg_loss = 3.78447\n",
      "epoch no.1 train no.151840  loss = 3.05348 avg_loss = 3.72994\n",
      "epoch no.1 train no.151850  loss = 4.79960 avg_loss = 3.77341\n",
      "epoch no.1 train no.151860  loss = 3.61925 avg_loss = 3.80317\n",
      "epoch no.1 train no.151870  loss = 3.56290 avg_loss = 3.82036\n",
      "epoch no.1 train no.151880  loss = 6.16511 avg_loss = 3.80614\n",
      "epoch no.1 train no.151890  loss = 3.90442 avg_loss = 3.87043\n",
      "epoch no.1 train no.151900  loss = 2.77027 avg_loss = 3.88895\n",
      "epoch no.1 train no.151910  loss = 2.86000 avg_loss = 3.83342\n",
      "epoch no.1 train no.151920  loss = 5.75716 avg_loss = 3.80285\n",
      "epoch no.1 train no.151930  loss = 5.02493 avg_loss = 3.81249\n",
      "epoch no.1 train no.151940  loss = 2.84288 avg_loss = 3.78157\n",
      "epoch no.1 train no.151950  loss = 4.42155 avg_loss = 3.77968\n",
      "epoch no.1 train no.151960  loss = 4.12976 avg_loss = 3.71375\n",
      "epoch no.1 train no.151970  loss = 2.41982 avg_loss = 3.65727\n",
      "epoch no.1 train no.151980  loss = 2.48192 avg_loss = 3.64525\n",
      "epoch no.1 train no.151990  loss = 3.28488 avg_loss = 3.67822\n",
      "epoch no.1 train no.152000  loss = 2.56597 avg_loss = 3.72235\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '때', '▁듣는', '▁좋은', '▁노래', '</s>']\n",
      "기분전환 할 때 듣기 좋은 음악</s>\n",
      "epoch no.1 train no.152010  loss = 4.73495 avg_loss = 3.71432\n",
      "epoch no.1 train no.152020  loss = 3.57800 avg_loss = 3.73556\n",
      "epoch no.1 train no.152030  loss = 2.38375 avg_loss = 3.70745\n",
      "epoch no.1 train no.152040  loss = 3.29823 avg_loss = 3.68247\n",
      "epoch no.1 train no.152050  loss = 3.86995 avg_loss = 3.66599\n",
      "epoch no.1 train no.152060  loss = 4.10481 avg_loss = 3.64790\n",
      "epoch no.1 train no.152070  loss = 3.33225 avg_loss = 3.66165\n",
      "epoch no.1 train no.152080  loss = 2.97759 avg_loss = 3.60413\n",
      "epoch no.1 train no.152090  loss = 3.17129 avg_loss = 3.63897\n",
      "epoch no.1 train no.152100  loss = 4.11070 avg_loss = 3.65010\n",
      "epoch no.1 train no.152110  loss = 2.87900 avg_loss = 3.63925\n",
      "epoch no.1 train no.152120  loss = 2.41785 avg_loss = 3.63788\n",
      "epoch no.1 train no.152130  loss = 2.23316 avg_loss = 3.61822\n",
      "epoch no.1 train no.152140  loss = 4.30129 avg_loss = 3.59998\n",
      "epoch no.1 train no.152150  loss = 4.10177 avg_loss = 3.58314\n",
      "epoch no.1 train no.152160  loss = 3.57075 avg_loss = 3.62612\n",
      "epoch no.1 train no.152170  loss = 5.05621 avg_loss = 3.66751\n",
      "epoch no.1 train no.152180  loss = 4.05951 avg_loss = 3.75612\n",
      "epoch no.1 train no.152190  loss = 4.82746 avg_loss = 3.81958\n",
      "epoch no.1 train no.152200  loss = 6.14259 avg_loss = 3.79816\n",
      "epoch no.1 train no.152210  loss = 4.23956 avg_loss = 3.84566\n",
      "epoch no.1 train no.152220  loss = 4.83951 avg_loss = 3.83857\n",
      "epoch no.1 train no.152230  loss = 3.63874 avg_loss = 3.82878\n",
      "epoch no.1 train no.152240  loss = 2.04749 avg_loss = 3.88676\n",
      "epoch no.1 train no.152250  loss = 4.04128 avg_loss = 3.89355\n",
      "epoch no.1 train no.152260  loss = 2.36243 avg_loss = 3.83696\n",
      "epoch no.1 train no.152270  loss = 1.34784 avg_loss = 3.80986\n",
      "epoch no.1 train no.152280  loss = 1.97370 avg_loss = 3.76510\n",
      "epoch no.1 train no.152290  loss = 4.65784 avg_loss = 3.79033\n",
      "epoch no.1 train no.152300  loss = 3.80683 avg_loss = 3.80529\n",
      "epoch no.1 train no.152310  loss = 5.15262 avg_loss = 3.84262\n",
      "epoch no.1 train no.152320  loss = 1.91264 avg_loss = 3.86159\n",
      "epoch no.1 train no.152330  loss = 3.89105 avg_loss = 3.85597\n",
      "epoch no.1 train no.152340  loss = 4.78230 avg_loss = 3.87219\n",
      "epoch no.1 train no.152350  loss = 4.25731 avg_loss = 3.85662\n",
      "epoch no.1 train no.152360  loss = 3.70845 avg_loss = 3.87098\n",
      "epoch no.1 train no.152370  loss = 3.29536 avg_loss = 3.77528\n",
      "epoch no.1 train no.152380  loss = 2.78075 avg_loss = 3.77053\n",
      "epoch no.1 train no.152390  loss = 4.79856 avg_loss = 3.84325\n",
      "epoch no.1 train no.152400  loss = 7.23252 avg_loss = 3.87385\n",
      "epoch no.1 train no.152410  loss = 3.90888 avg_loss = 3.83824\n",
      "epoch no.1 train no.152420  loss = 3.03721 avg_loss = 3.83557\n",
      "epoch no.1 train no.152430  loss = 2.89238 avg_loss = 3.81071\n",
      "epoch no.1 train no.152440  loss = 5.53894 avg_loss = 3.83368\n",
      "epoch no.1 train no.152450  loss = 5.25831 avg_loss = 3.85637\n",
      "epoch no.1 train no.152460  loss = 2.19383 avg_loss = 3.84253\n",
      "epoch no.1 train no.152470  loss = 3.43247 avg_loss = 3.83571\n",
      "epoch no.1 train no.152480  loss = 3.61290 avg_loss = 3.79457\n",
      "epoch no.1 train no.152490  loss = 3.66998 avg_loss = 3.83901\n",
      "epoch no.1 train no.152500  loss = 3.36699 avg_loss = 3.78920\n",
      "epoch no.1 train no.152510  loss = 2.71595 avg_loss = 3.71160\n",
      "epoch no.1 train no.152520  loss = 5.34188 avg_loss = 3.74802\n",
      "epoch no.1 train no.152530  loss = 4.30818 avg_loss = 3.86026\n",
      "epoch no.1 train no.152540  loss = 4.11965 avg_loss = 3.85529\n",
      "epoch no.1 train no.152550  loss = 1.78563 avg_loss = 3.82774\n",
      "epoch no.1 train no.152560  loss = 5.01596 avg_loss = 3.83138\n",
      "epoch no.1 train no.152570  loss = 3.04660 avg_loss = 3.78620\n",
      "epoch no.1 train no.152580  loss = 5.69355 avg_loss = 3.76852\n",
      "epoch no.1 train no.152590  loss = 3.09541 avg_loss = 3.78155\n",
      "epoch no.1 train no.152600  loss = 3.27871 avg_loss = 3.76513\n",
      "epoch no.1 train no.152610  loss = 7.04581 avg_loss = 3.82250\n",
      "epoch no.1 train no.152620  loss = 1.81754 avg_loss = 3.80259\n",
      "epoch no.1 train no.152630  loss = 1.41960 avg_loss = 3.77537\n",
      "epoch no.1 train no.152640  loss = 1.97882 avg_loss = 3.77311\n",
      "epoch no.1 train no.152650  loss = 2.42376 avg_loss = 3.75442\n",
      "epoch no.1 train no.152660  loss = 4.99618 avg_loss = 3.86280\n",
      "epoch no.1 train no.152670  loss = 2.54885 avg_loss = 3.88428\n",
      "epoch no.1 train no.152680  loss = 5.17350 avg_loss = 3.83177\n",
      "epoch no.1 train no.152690  loss = 5.20622 avg_loss = 3.82999\n",
      "epoch no.1 train no.152700  loss = 3.81544 avg_loss = 3.76693\n",
      "epoch no.1 train no.152710  loss = 4.39350 avg_loss = 3.80913\n",
      "epoch no.1 train no.152720  loss = 4.01092 avg_loss = 3.84425\n",
      "epoch no.1 train no.152730  loss = 3.00463 avg_loss = 3.84771\n",
      "epoch no.1 train no.152740  loss = 2.92592 avg_loss = 3.81050\n",
      "epoch no.1 train no.152750  loss = 3.99682 avg_loss = 3.80276\n",
      "epoch no.1 train no.152760  loss = 3.35366 avg_loss = 3.83530\n",
      "epoch no.1 train no.152770  loss = 1.30450 avg_loss = 3.79409\n",
      "epoch no.1 train no.152780  loss = 3.40642 avg_loss = 3.84698\n",
      "epoch no.1 train no.152790  loss = 3.13592 avg_loss = 3.80093\n",
      "epoch no.1 train no.152800  loss = 2.88079 avg_loss = 3.81522\n",
      "epoch no.1 train no.152810  loss = 3.68866 avg_loss = 3.75800\n",
      "epoch no.1 train no.152820  loss = 2.66959 avg_loss = 3.77664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.152830  loss = 6.32534 avg_loss = 3.77729\n",
      "epoch no.1 train no.152840  loss = 2.53968 avg_loss = 3.76226\n",
      "epoch no.1 train no.152850  loss = 3.45645 avg_loss = 3.79150\n",
      "epoch no.1 train no.152860  loss = 5.80243 avg_loss = 3.76226\n",
      "epoch no.1 train no.152870  loss = 3.55138 avg_loss = 3.79195\n",
      "epoch no.1 train no.152880  loss = 7.59893 avg_loss = 3.82326\n",
      "epoch no.1 train no.152890  loss = 3.03113 avg_loss = 3.80706\n",
      "epoch no.1 train no.152900  loss = 2.07878 avg_loss = 3.80288\n",
      "epoch no.1 train no.152910  loss = 3.48800 avg_loss = 3.85422\n",
      "epoch no.1 train no.152920  loss = 3.74602 avg_loss = 3.82603\n",
      "epoch no.1 train no.152930  loss = 4.62304 avg_loss = 3.81085\n",
      "epoch no.1 train no.152940  loss = 2.76241 avg_loss = 3.79319\n",
      "epoch no.1 train no.152950  loss = 2.81420 avg_loss = 3.82490\n",
      "epoch no.1 train no.152960  loss = 2.45963 avg_loss = 3.85099\n",
      "epoch no.1 train no.152970  loss = 2.71161 avg_loss = 3.82446\n",
      "epoch no.1 train no.152980  loss = 4.19806 avg_loss = 3.78504\n",
      "epoch no.1 train no.152990  loss = 4.38404 avg_loss = 3.71488\n",
      "epoch no.1 train no.153000  loss = 2.63025 avg_loss = 3.68195\n",
      "1\n",
      "to_tokens: ['▁가을', '전환', '을', '▁신나는']\n",
      "기분전환용</s>\n",
      "epoch no.1 train no.153010  loss = 5.59767 avg_loss = 3.73547\n",
      "epoch no.1 train no.153020  loss = 2.25588 avg_loss = 3.70456\n",
      "epoch no.1 train no.153030  loss = 3.71963 avg_loss = 3.76423\n",
      "epoch no.1 train no.153040  loss = 2.32728 avg_loss = 3.75434\n",
      "epoch no.1 train no.153050  loss = 3.16421 avg_loss = 3.72637\n",
      "epoch no.1 train no.153060  loss = 3.45333 avg_loss = 3.78582\n",
      "epoch no.1 train no.153070  loss = 3.79468 avg_loss = 3.80497\n",
      "epoch no.1 train no.153080  loss = 3.40755 avg_loss = 3.81635\n",
      "epoch no.1 train no.153090  loss = 5.80616 avg_loss = 3.87358\n",
      "epoch no.1 train no.153100  loss = 4.33086 avg_loss = 3.91119\n",
      "epoch no.1 train no.153110  loss = 5.61981 avg_loss = 3.96363\n",
      "epoch no.1 train no.153120  loss = 2.96946 avg_loss = 3.91004\n",
      "epoch no.1 train no.153130  loss = 4.92523 avg_loss = 3.89321\n",
      "epoch no.1 train no.153140  loss = 3.99894 avg_loss = 3.82357\n",
      "epoch no.1 train no.153150  loss = 4.61007 avg_loss = 3.85253\n",
      "epoch no.1 train no.153160  loss = 3.17641 avg_loss = 3.81560\n",
      "epoch no.1 train no.153170  loss = 3.54645 avg_loss = 3.76701\n",
      "epoch no.1 train no.153180  loss = 3.68357 avg_loss = 3.74056\n",
      "epoch no.1 train no.153190  loss = 7.45430 avg_loss = 3.73387\n",
      "epoch no.1 train no.153200  loss = 5.33389 avg_loss = 3.74339\n",
      "epoch no.1 train no.153210  loss = 3.84958 avg_loss = 3.80492\n",
      "epoch no.1 train no.153220  loss = 3.22399 avg_loss = 3.83850\n",
      "epoch no.1 train no.153230  loss = 4.01748 avg_loss = 3.92381\n",
      "epoch no.1 train no.153240  loss = 3.57630 avg_loss = 3.88897\n",
      "epoch no.1 train no.153250  loss = 2.42997 avg_loss = 3.82643\n",
      "epoch no.1 train no.153260  loss = 4.17007 avg_loss = 3.84673\n",
      "epoch no.1 train no.153270  loss = 5.54890 avg_loss = 3.82038\n",
      "epoch no.1 train no.153280  loss = 2.79733 avg_loss = 3.85695\n",
      "epoch no.1 train no.153290  loss = 3.16798 avg_loss = 3.75556\n",
      "epoch no.1 train no.153300  loss = 4.61215 avg_loss = 3.70339\n",
      "epoch no.1 train no.153310  loss = 3.99675 avg_loss = 3.72355\n",
      "epoch no.1 train no.153320  loss = 5.25612 avg_loss = 3.69874\n",
      "epoch no.1 train no.153330  loss = 3.10754 avg_loss = 3.70799\n",
      "epoch no.1 train no.153340  loss = 2.10622 avg_loss = 3.68346\n",
      "epoch no.1 train no.153350  loss = 3.08013 avg_loss = 3.64667\n",
      "epoch no.1 train no.153360  loss = 3.24147 avg_loss = 3.65728\n",
      "epoch no.1 train no.153370  loss = 4.65912 avg_loss = 3.66315\n",
      "epoch no.1 train no.153380  loss = 2.56593 avg_loss = 3.63898\n",
      "epoch no.1 train no.153390  loss = 3.06367 avg_loss = 3.63745\n",
      "epoch no.1 train no.153400  loss = 4.56618 avg_loss = 3.65201\n",
      "epoch no.1 train no.153410  loss = 3.43804 avg_loss = 3.60782\n",
      "epoch no.1 train no.153420  loss = 4.92987 avg_loss = 3.63849\n",
      "epoch no.1 train no.153430  loss = 3.70056 avg_loss = 3.68133\n",
      "epoch no.1 train no.153440  loss = 5.04895 avg_loss = 3.74913\n",
      "epoch no.1 train no.153450  loss = 2.86522 avg_loss = 3.79951\n",
      "epoch no.1 train no.153460  loss = 3.94954 avg_loss = 3.73816\n",
      "epoch no.1 train no.153470  loss = 4.01576 avg_loss = 3.71854\n",
      "epoch no.1 train no.153480  loss = 5.16912 avg_loss = 3.75880\n",
      "epoch no.1 train no.153490  loss = 2.86958 avg_loss = 3.79281\n",
      "epoch no.1 train no.153500  loss = 2.28834 avg_loss = 3.74708\n",
      "epoch no.1 train no.153510  loss = 3.33988 avg_loss = 3.72396\n",
      "epoch no.1 train no.153520  loss = 4.69714 avg_loss = 3.69086\n",
      "epoch no.1 train no.153530  loss = 2.34960 avg_loss = 3.70330\n",
      "epoch no.1 train no.153540  loss = 3.59447 avg_loss = 3.69175\n",
      "epoch no.1 train no.153550  loss = 1.91928 avg_loss = 3.71212\n",
      "epoch no.1 train no.153560  loss = 3.46207 avg_loss = 3.70322\n",
      "epoch no.1 train no.153570  loss = 6.82858 avg_loss = 3.71954\n",
      "epoch no.1 train no.153580  loss = 4.73392 avg_loss = 3.73739\n",
      "epoch no.1 train no.153590  loss = 4.43086 avg_loss = 3.74748\n",
      "epoch no.1 train no.153600  loss = 5.71836 avg_loss = 3.74964\n",
      "epoch no.1 train no.153610  loss = 4.74266 avg_loss = 3.81128\n",
      "epoch no.1 train no.153620  loss = 4.37385 avg_loss = 3.76068\n",
      "epoch no.1 train no.153630  loss = 1.96353 avg_loss = 3.67592\n",
      "epoch no.1 train no.153640  loss = 5.42668 avg_loss = 3.65822\n",
      "epoch no.1 train no.153650  loss = 3.40106 avg_loss = 3.66179\n",
      "epoch no.1 train no.153660  loss = 2.88086 avg_loss = 3.67556\n",
      "epoch no.1 train no.153670  loss = 3.01174 avg_loss = 3.72016\n",
      "epoch no.1 train no.153680  loss = 3.15307 avg_loss = 3.69028\n",
      "epoch no.1 train no.153690  loss = 2.94578 avg_loss = 3.66156\n",
      "epoch no.1 train no.153700  loss = 3.12449 avg_loss = 3.69143\n",
      "epoch no.1 train no.153710  loss = 3.43717 avg_loss = 3.68718\n",
      "epoch no.1 train no.153720  loss = 3.01100 avg_loss = 3.70318\n",
      "epoch no.1 train no.153730  loss = 1.94421 avg_loss = 3.65802\n",
      "epoch no.1 train no.153740  loss = 3.33749 avg_loss = 3.62773\n",
      "epoch no.1 train no.153750  loss = 3.51136 avg_loss = 3.65220\n",
      "epoch no.1 train no.153760  loss = 3.67631 avg_loss = 3.65638\n",
      "epoch no.1 train no.153770  loss = 2.52468 avg_loss = 3.68113\n",
      "epoch no.1 train no.153780  loss = 3.99144 avg_loss = 3.66102\n",
      "epoch no.1 train no.153790  loss = 2.72688 avg_loss = 3.68071\n",
      "epoch no.1 train no.153800  loss = 4.85821 avg_loss = 3.71060\n",
      "epoch no.1 train no.153810  loss = 4.71665 avg_loss = 3.73865\n",
      "epoch no.1 train no.153820  loss = 2.85041 avg_loss = 3.74904\n",
      "epoch no.1 train no.153830  loss = 3.76885 avg_loss = 3.81892\n",
      "epoch no.1 train no.153840  loss = 4.30368 avg_loss = 3.84660\n",
      "epoch no.1 train no.153850  loss = 2.15345 avg_loss = 3.82370\n",
      "epoch no.1 train no.153860  loss = 3.27581 avg_loss = 3.83004\n",
      "epoch no.1 train no.153870  loss = 4.03608 avg_loss = 3.88021\n",
      "epoch no.1 train no.153880  loss = 3.35710 avg_loss = 3.87100\n",
      "epoch no.1 train no.153890  loss = 3.21773 avg_loss = 3.81751\n",
      "epoch no.1 train no.153900  loss = 4.47005 avg_loss = 3.84308\n",
      "epoch no.1 train no.153910  loss = 5.43643 avg_loss = 3.84786\n",
      "epoch no.1 train no.153920  loss = 3.07225 avg_loss = 3.85384\n",
      "epoch no.1 train no.153930  loss = 4.39632 avg_loss = 3.81190\n",
      "epoch no.1 train no.153940  loss = 2.63087 avg_loss = 3.85243\n",
      "epoch no.1 train no.153950  loss = 2.58288 avg_loss = 3.83409\n",
      "epoch no.1 train no.153960  loss = 6.13500 avg_loss = 3.86905\n",
      "epoch no.1 train no.153970  loss = 3.74741 avg_loss = 3.84772\n",
      "epoch no.1 train no.153980  loss = 1.76442 avg_loss = 3.85737\n",
      "epoch no.1 train no.153990  loss = 3.09259 avg_loss = 3.89058\n",
      "epoch no.1 train no.154000  loss = 2.96177 avg_loss = 3.90109\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁위한', '▁신나는', '▁팝', 'op', '</s>']\n",
      "기분전환을 위한 신나는 pop</s>\n",
      "epoch no.1 train no.154010  loss = 3.56033 avg_loss = 3.85826\n",
      "epoch no.1 train no.154020  loss = 5.04036 avg_loss = 3.88750\n",
      "epoch no.1 train no.154030  loss = 3.75672 avg_loss = 3.83078\n",
      "epoch no.1 train no.154040  loss = 4.77183 avg_loss = 3.77446\n",
      "epoch no.1 train no.154050  loss = 1.84951 avg_loss = 3.71525\n",
      "epoch no.1 train no.154060  loss = 4.52992 avg_loss = 3.75486\n",
      "epoch no.1 train no.154070  loss = 2.48756 avg_loss = 3.76339\n",
      "epoch no.1 train no.154080  loss = 3.34781 avg_loss = 3.79547\n",
      "epoch no.1 train no.154090  loss = 2.62232 avg_loss = 3.78713\n",
      "epoch no.1 train no.154100  loss = 3.39731 avg_loss = 3.77993\n",
      "epoch no.1 train no.154110  loss = 2.36183 avg_loss = 3.77056\n",
      "epoch no.1 train no.154120  loss = 2.36945 avg_loss = 3.72898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.154130  loss = 3.50979 avg_loss = 3.77601\n",
      "epoch no.1 train no.154140  loss = 3.58941 avg_loss = 3.72306\n",
      "epoch no.1 train no.154150  loss = 3.56099 avg_loss = 3.78016\n",
      "epoch no.1 train no.154160  loss = 2.97531 avg_loss = 3.74860\n",
      "epoch no.1 train no.154170  loss = 3.18257 avg_loss = 3.68538\n",
      "epoch no.1 train no.154180  loss = 3.04291 avg_loss = 3.74975\n",
      "epoch no.1 train no.154190  loss = 4.33711 avg_loss = 3.77959\n",
      "epoch no.1 train no.154200  loss = 3.15355 avg_loss = 3.81070\n",
      "epoch no.1 train no.154210  loss = 3.92920 avg_loss = 3.88030\n",
      "epoch no.1 train no.154220  loss = 2.59443 avg_loss = 3.85934\n",
      "epoch no.1 train no.154230  loss = 4.10321 avg_loss = 3.85862\n",
      "epoch no.1 train no.154240  loss = 2.25815 avg_loss = 3.81513\n",
      "epoch no.1 train no.154250  loss = 5.01843 avg_loss = 3.78509\n",
      "epoch no.1 train no.154260  loss = 3.50371 avg_loss = 3.79182\n",
      "epoch no.1 train no.154270  loss = 2.22713 avg_loss = 3.72667\n",
      "epoch no.1 train no.154280  loss = 3.84957 avg_loss = 3.75077\n",
      "epoch no.1 train no.154290  loss = 2.89813 avg_loss = 3.75003\n",
      "epoch no.1 train no.154300  loss = 3.73273 avg_loss = 3.71792\n",
      "epoch no.1 train no.154310  loss = 4.47147 avg_loss = 3.69244\n",
      "epoch no.1 train no.154320  loss = 3.30909 avg_loss = 3.72656\n",
      "epoch no.1 train no.154330  loss = 2.69027 avg_loss = 3.64629\n",
      "epoch no.1 train no.154340  loss = 3.55056 avg_loss = 3.60357\n",
      "epoch no.1 train no.154350  loss = 2.52883 avg_loss = 3.56877\n",
      "epoch no.1 train no.154360  loss = 3.84079 avg_loss = 3.68735\n",
      "epoch no.1 train no.154370  loss = 4.25455 avg_loss = 3.74665\n",
      "epoch no.1 train no.154380  loss = 2.96507 avg_loss = 3.72394\n",
      "epoch no.1 train no.154390  loss = 4.11911 avg_loss = 3.77811\n",
      "epoch no.1 train no.154400  loss = 3.75398 avg_loss = 3.76690\n",
      "epoch no.1 train no.154410  loss = 3.04609 avg_loss = 3.79350\n",
      "epoch no.1 train no.154420  loss = 2.93666 avg_loss = 3.81975\n",
      "epoch no.1 train no.154430  loss = 5.15834 avg_loss = 3.87216\n",
      "epoch no.1 train no.154440  loss = 2.95703 avg_loss = 3.80521\n",
      "epoch no.1 train no.154450  loss = 1.33807 avg_loss = 3.73192\n",
      "epoch no.1 train no.154460  loss = 1.60429 avg_loss = 3.72496\n",
      "epoch no.1 train no.154470  loss = 2.85475 avg_loss = 3.71594\n",
      "epoch no.1 train no.154480  loss = 3.96130 avg_loss = 3.74721\n",
      "epoch no.1 train no.154490  loss = 4.59586 avg_loss = 3.79210\n",
      "epoch no.1 train no.154500  loss = 4.05407 avg_loss = 3.77486\n",
      "epoch no.1 train no.154510  loss = 4.87792 avg_loss = 3.81882\n",
      "epoch no.1 train no.154520  loss = 2.67652 avg_loss = 3.76433\n",
      "epoch no.1 train no.154530  loss = 4.26886 avg_loss = 3.82551\n",
      "epoch no.1 train no.154540  loss = 5.50642 avg_loss = 3.83729\n",
      "epoch no.1 train no.154550  loss = 4.89302 avg_loss = 3.82184\n",
      "epoch no.1 train no.154560  loss = 2.85220 avg_loss = 3.80113\n",
      "epoch no.1 train no.154570  loss = 6.37019 avg_loss = 3.83797\n",
      "epoch no.1 train no.154580  loss = 3.88119 avg_loss = 3.78657\n",
      "epoch no.1 train no.154590  loss = 2.87597 avg_loss = 3.75310\n",
      "epoch no.1 train no.154600  loss = 4.27100 avg_loss = 3.71544\n",
      "epoch no.1 train no.154610  loss = 5.02531 avg_loss = 3.80187\n",
      "epoch no.1 train no.154620  loss = 1.85973 avg_loss = 3.78641\n",
      "epoch no.1 train no.154630  loss = 3.91120 avg_loss = 3.80382\n",
      "epoch no.1 train no.154640  loss = 3.05171 avg_loss = 3.79369\n",
      "epoch no.1 train no.154650  loss = 2.88077 avg_loss = 3.77308\n",
      "epoch no.1 train no.154660  loss = 3.66418 avg_loss = 3.75310\n",
      "epoch no.1 train no.154670  loss = 2.92600 avg_loss = 3.73123\n",
      "epoch no.1 train no.154680  loss = 4.15684 avg_loss = 3.78829\n",
      "epoch no.1 train no.154690  loss = 2.63565 avg_loss = 3.71581\n",
      "epoch no.1 train no.154700  loss = 3.13534 avg_loss = 3.69230\n",
      "epoch no.1 train no.154710  loss = 4.78199 avg_loss = 3.69675\n",
      "epoch no.1 train no.154720  loss = 4.10776 avg_loss = 3.69874\n",
      "epoch no.1 train no.154730  loss = 4.98086 avg_loss = 3.75567\n",
      "epoch no.1 train no.154740  loss = 2.14302 avg_loss = 3.73786\n",
      "epoch no.1 train no.154750  loss = 2.81371 avg_loss = 3.76210\n",
      "epoch no.1 train no.154760  loss = 4.66333 avg_loss = 3.76074\n",
      "epoch no.1 train no.154770  loss = 4.59008 avg_loss = 3.76761\n",
      "epoch no.1 train no.154780  loss = 3.80666 avg_loss = 3.74941\n",
      "epoch no.1 train no.154790  loss = 4.23407 avg_loss = 3.85722\n",
      "epoch no.1 train no.154800  loss = 2.57188 avg_loss = 3.81772\n",
      "epoch no.1 train no.154810  loss = 2.78227 avg_loss = 3.76967\n",
      "epoch no.1 train no.154820  loss = 5.14297 avg_loss = 3.74589\n",
      "epoch no.1 train no.154830  loss = 3.55400 avg_loss = 3.76103\n",
      "epoch no.1 train no.154840  loss = 3.85573 avg_loss = 3.71468\n",
      "epoch no.1 train no.154850  loss = 4.75261 avg_loss = 3.69295\n",
      "epoch no.1 train no.154860  loss = 3.03361 avg_loss = 3.68229\n",
      "epoch no.1 train no.154870  loss = 2.37217 avg_loss = 3.65246\n",
      "epoch no.1 train no.154880  loss = 6.57639 avg_loss = 3.67474\n",
      "epoch no.1 train no.154890  loss = 6.47400 avg_loss = 3.74151\n",
      "epoch no.1 train no.154900  loss = 3.16183 avg_loss = 3.74397\n",
      "epoch no.1 train no.154910  loss = 4.29211 avg_loss = 3.78075\n",
      "epoch no.1 train no.154920  loss = 2.09843 avg_loss = 3.74058\n",
      "epoch no.1 train no.154930  loss = 5.10757 avg_loss = 3.75097\n",
      "epoch no.1 train no.154940  loss = 3.43132 avg_loss = 3.75306\n",
      "epoch no.1 train no.154950  loss = 4.68996 avg_loss = 3.74900\n",
      "epoch no.1 train no.154960  loss = 2.21109 avg_loss = 3.71258\n",
      "epoch no.1 train no.154970  loss = 3.78676 avg_loss = 3.64881\n",
      "epoch no.1 train no.154980  loss = 3.93270 avg_loss = 3.63852\n",
      "epoch no.1 train no.154990  loss = 4.54989 avg_loss = 3.64043\n",
      "epoch no.1 train no.155000  loss = 2.54331 avg_loss = 3.65333\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.1 train no.155010  loss = 3.40871 avg_loss = 3.62508\n",
      "epoch no.1 train no.155020  loss = 3.69353 avg_loss = 3.63920\n",
      "epoch no.1 train no.155030  loss = 2.31521 avg_loss = 3.59839\n",
      "epoch no.1 train no.155040  loss = 3.64351 avg_loss = 3.59103\n",
      "epoch no.1 train no.155050  loss = 1.78963 avg_loss = 3.58402\n",
      "epoch no.1 train no.155060  loss = 2.29655 avg_loss = 3.59507\n",
      "epoch no.1 train no.155070  loss = 4.14490 avg_loss = 3.60761\n",
      "epoch no.1 train no.155080  loss = 2.28678 avg_loss = 3.65656\n",
      "epoch no.1 train no.155090  loss = 4.46346 avg_loss = 3.66427\n",
      "epoch no.1 train no.155100  loss = 4.51630 avg_loss = 3.64674\n",
      "epoch no.1 train no.155110  loss = 3.16491 avg_loss = 3.62027\n",
      "epoch no.1 train no.155120  loss = 3.42384 avg_loss = 3.60778\n",
      "epoch no.1 train no.155130  loss = 5.63249 avg_loss = 3.62303\n",
      "epoch no.1 train no.155140  loss = 2.79543 avg_loss = 3.60008\n",
      "epoch no.1 train no.155150  loss = 5.22818 avg_loss = 3.65899\n",
      "epoch no.1 train no.155160  loss = 2.41642 avg_loss = 3.65698\n",
      "epoch no.1 train no.155170  loss = 3.60092 avg_loss = 3.65780\n",
      "epoch no.1 train no.155180  loss = 3.26667 avg_loss = 3.66789\n",
      "epoch no.1 train no.155190  loss = 2.17810 avg_loss = 3.64010\n",
      "epoch no.1 train no.155200  loss = 5.00100 avg_loss = 3.61578\n",
      "epoch no.1 train no.155210  loss = 3.75678 avg_loss = 3.63132\n",
      "epoch no.1 train no.155220  loss = 2.87588 avg_loss = 3.58231\n",
      "epoch no.1 train no.155230  loss = 3.64450 avg_loss = 3.58770\n",
      "epoch no.1 train no.155240  loss = 5.65019 avg_loss = 3.67046\n",
      "epoch no.1 train no.155250  loss = 2.99590 avg_loss = 3.73403\n",
      "epoch no.1 train no.155260  loss = 3.51904 avg_loss = 3.72910\n",
      "epoch no.1 train no.155270  loss = 5.38478 avg_loss = 3.72125\n",
      "epoch no.1 train no.155280  loss = 3.64108 avg_loss = 3.76103\n",
      "epoch no.1 train no.155290  loss = 3.67511 avg_loss = 3.72957\n",
      "epoch no.1 train no.155300  loss = 3.12009 avg_loss = 3.75608\n",
      "epoch no.1 train no.155310  loss = 3.46486 avg_loss = 3.76875\n",
      "epoch no.1 train no.155320  loss = 3.33800 avg_loss = 3.77296\n",
      "epoch no.1 train no.155330  loss = 4.96650 avg_loss = 3.77305\n",
      "epoch no.1 train no.155340  loss = 2.82204 avg_loss = 3.79442\n",
      "epoch no.1 train no.155350  loss = 3.95725 avg_loss = 3.85147\n",
      "epoch no.1 train no.155360  loss = 3.77969 avg_loss = 3.82217\n",
      "epoch no.1 train no.155370  loss = 3.36082 avg_loss = 3.76526\n",
      "epoch no.1 train no.155380  loss = 2.63177 avg_loss = 3.72075\n",
      "epoch no.1 train no.155390  loss = 4.50085 avg_loss = 3.74293\n",
      "epoch no.1 train no.155400  loss = 4.09983 avg_loss = 3.75172\n",
      "epoch no.1 train no.155410  loss = 4.66985 avg_loss = 3.80881\n",
      "epoch no.1 train no.155420  loss = 3.00788 avg_loss = 3.76611\n",
      "epoch no.1 train no.155430  loss = 3.73654 avg_loss = 3.76283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.155440  loss = 2.69266 avg_loss = 3.73501\n",
      "epoch no.1 train no.155450  loss = 4.11687 avg_loss = 3.76612\n",
      "epoch no.1 train no.155460  loss = 4.58440 avg_loss = 3.74043\n",
      "epoch no.1 train no.155470  loss = 2.80079 avg_loss = 3.78533\n",
      "epoch no.1 train no.155480  loss = 3.43786 avg_loss = 3.74950\n",
      "epoch no.1 train no.155490  loss = 2.25183 avg_loss = 3.77810\n",
      "epoch no.1 train no.155500  loss = 2.61171 avg_loss = 3.72438\n",
      "epoch no.1 train no.155510  loss = 2.73267 avg_loss = 3.78855\n",
      "epoch no.1 train no.155520  loss = 4.52792 avg_loss = 3.75603\n",
      "epoch no.1 train no.155530  loss = 2.82012 avg_loss = 3.72359\n",
      "epoch no.1 train no.155540  loss = 7.73101 avg_loss = 3.77856\n",
      "epoch no.1 train no.155550  loss = 3.34243 avg_loss = 3.75623\n",
      "epoch no.1 train no.155560  loss = 4.80699 avg_loss = 3.75327\n",
      "epoch no.1 train no.155570  loss = 3.14864 avg_loss = 3.74632\n",
      "epoch no.1 train no.155580  loss = 4.56834 avg_loss = 3.78623\n",
      "epoch no.1 train no.155590  loss = 3.85216 avg_loss = 3.74314\n",
      "epoch no.1 train no.155600  loss = 4.31884 avg_loss = 3.78651\n",
      "epoch no.1 train no.155610  loss = 2.80106 avg_loss = 3.80155\n",
      "epoch no.1 train no.155620  loss = 3.71568 avg_loss = 3.72327\n",
      "epoch no.1 train no.155630  loss = 2.04150 avg_loss = 3.78125\n",
      "epoch no.1 train no.155640  loss = 2.80037 avg_loss = 3.69281\n",
      "epoch no.1 train no.155650  loss = 3.99882 avg_loss = 3.69537\n",
      "epoch no.1 train no.155660  loss = 3.78036 avg_loss = 3.71900\n",
      "epoch no.1 train no.155670  loss = 3.24727 avg_loss = 3.68614\n",
      "epoch no.1 train no.155680  loss = 2.61916 avg_loss = 3.70047\n",
      "epoch no.1 train no.155690  loss = 6.38662 avg_loss = 3.74002\n",
      "epoch no.1 train no.155700  loss = 4.20193 avg_loss = 3.77401\n",
      "epoch no.1 train no.155710  loss = 5.34096 avg_loss = 3.79742\n",
      "epoch no.1 train no.155720  loss = 2.99411 avg_loss = 3.72564\n",
      "epoch no.1 train no.155730  loss = 3.00964 avg_loss = 3.72464\n",
      "epoch no.1 train no.155740  loss = 4.06307 avg_loss = 3.71004\n",
      "epoch no.1 train no.155750  loss = 5.70668 avg_loss = 3.76921\n",
      "epoch no.1 train no.155760  loss = 4.19534 avg_loss = 3.74225\n",
      "epoch no.1 train no.155770  loss = 3.15630 avg_loss = 3.75115\n",
      "epoch no.1 train no.155780  loss = 4.20350 avg_loss = 3.79215\n",
      "epoch no.1 train no.155790  loss = 2.99036 avg_loss = 3.82836\n",
      "epoch no.1 train no.155800  loss = 5.04144 avg_loss = 3.74427\n",
      "epoch no.1 train no.155810  loss = 4.42307 avg_loss = 3.73189\n",
      "epoch no.1 train no.155820  loss = 3.08116 avg_loss = 3.69353\n",
      "epoch no.1 train no.155830  loss = 3.81032 avg_loss = 3.66966\n",
      "epoch no.1 train no.155840  loss = 5.08323 avg_loss = 3.73854\n",
      "epoch no.1 train no.155850  loss = 6.44506 avg_loss = 3.72216\n",
      "epoch no.1 train no.155860  loss = 4.95552 avg_loss = 3.70391\n",
      "epoch no.1 train no.155870  loss = 2.93667 avg_loss = 3.65837\n",
      "epoch no.1 train no.155880  loss = 2.54209 avg_loss = 3.65930\n",
      "epoch no.1 train no.155890  loss = 5.62509 avg_loss = 3.67188\n",
      "epoch no.1 train no.155900  loss = 2.91204 avg_loss = 3.68230\n",
      "epoch no.1 train no.155910  loss = 2.66320 avg_loss = 3.72178\n",
      "epoch no.1 train no.155920  loss = 2.71113 avg_loss = 3.67415\n",
      "epoch no.1 train no.155930  loss = 2.74551 avg_loss = 3.66480\n",
      "epoch no.1 train no.155940  loss = 4.72375 avg_loss = 3.68462\n",
      "epoch no.1 train no.155950  loss = 3.02552 avg_loss = 3.66928\n",
      "epoch no.1 train no.155960  loss = 3.74647 avg_loss = 3.65965\n",
      "epoch no.1 train no.155970  loss = 2.43273 avg_loss = 3.67647\n",
      "epoch no.1 train no.155980  loss = 5.90746 avg_loss = 3.70448\n",
      "epoch no.1 train no.155990  loss = 2.19513 avg_loss = 3.69896\n",
      "epoch no.1 train no.156000  loss = 3.91001 avg_loss = 3.69395\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '이', '싶', '을', '때', '</s>']\n",
      "기분전환 하고싶을때</s>\n",
      "epoch no.1 train no.156010  loss = 4.27413 avg_loss = 3.70539\n",
      "epoch no.1 train no.156020  loss = 4.67784 avg_loss = 3.73656\n",
      "epoch no.1 train no.156030  loss = 4.46891 avg_loss = 3.75517\n",
      "epoch no.1 train no.156040  loss = 1.59313 avg_loss = 3.77434\n",
      "epoch no.1 train no.156050  loss = 3.59345 avg_loss = 3.76522\n",
      "epoch no.1 train no.156060  loss = 4.21354 avg_loss = 3.78473\n",
      "epoch no.1 train no.156070  loss = 2.88547 avg_loss = 3.84348\n",
      "epoch no.1 train no.156080  loss = 5.27158 avg_loss = 3.83061\n",
      "epoch no.1 train no.156090  loss = 5.22588 avg_loss = 3.86136\n",
      "epoch no.1 train no.156100  loss = 2.92989 avg_loss = 3.86205\n",
      "epoch no.1 train no.156110  loss = 4.89003 avg_loss = 3.86610\n",
      "epoch no.1 train no.156120  loss = 3.36148 avg_loss = 3.79912\n",
      "epoch no.1 train no.156130  loss = 2.87802 avg_loss = 3.77418\n",
      "epoch no.1 train no.156140  loss = 4.17155 avg_loss = 3.77184\n",
      "epoch no.1 train no.156150  loss = 3.43674 avg_loss = 3.72297\n",
      "epoch no.1 train no.156160  loss = 3.00963 avg_loss = 3.65738\n",
      "epoch no.1 train no.156170  loss = 1.91637 avg_loss = 3.59201\n",
      "epoch no.1 train no.156180  loss = 2.42951 avg_loss = 3.59623\n",
      "epoch no.1 train no.156190  loss = 2.51670 avg_loss = 3.58039\n",
      "epoch no.1 train no.156200  loss = 3.63549 avg_loss = 3.55614\n",
      "epoch no.1 train no.156210  loss = 3.45737 avg_loss = 3.61170\n",
      "epoch no.1 train no.156220  loss = 4.46589 avg_loss = 3.65559\n",
      "epoch no.1 train no.156230  loss = 3.03144 avg_loss = 3.64217\n",
      "epoch no.1 train no.156240  loss = 2.36813 avg_loss = 3.64509\n",
      "epoch no.1 train no.156250  loss = 4.88991 avg_loss = 3.72201\n",
      "epoch no.1 train no.156260  loss = 3.05065 avg_loss = 3.79105\n",
      "epoch no.1 train no.156270  loss = 3.64960 avg_loss = 3.79677\n",
      "epoch no.1 train no.156280  loss = 2.71630 avg_loss = 3.74939\n",
      "epoch no.1 train no.156290  loss = 5.66712 avg_loss = 3.72550\n",
      "epoch no.1 train no.156300  loss = 3.32650 avg_loss = 3.78964\n",
      "epoch no.1 train no.156310  loss = 4.94058 avg_loss = 3.83072\n",
      "epoch no.1 train no.156320  loss = 2.18091 avg_loss = 3.80810\n",
      "epoch no.1 train no.156330  loss = 3.39619 avg_loss = 3.84131\n",
      "epoch no.1 train no.156340  loss = 2.65021 avg_loss = 3.85809\n",
      "epoch no.1 train no.156350  loss = 2.68402 avg_loss = 3.76614\n",
      "epoch no.1 train no.156360  loss = 2.97351 avg_loss = 3.78392\n",
      "epoch no.1 train no.156370  loss = 2.49499 avg_loss = 3.71258\n",
      "epoch no.1 train no.156380  loss = 4.34640 avg_loss = 3.70739\n",
      "epoch no.1 train no.156390  loss = 3.34713 avg_loss = 3.69049\n",
      "epoch no.1 train no.156400  loss = 4.18686 avg_loss = 3.70148\n",
      "epoch no.1 train no.156410  loss = 3.54902 avg_loss = 3.72667\n",
      "epoch no.1 train no.156420  loss = 1.98907 avg_loss = 3.74484\n",
      "epoch no.1 train no.156430  loss = 6.34437 avg_loss = 3.77232\n",
      "epoch no.1 train no.156440  loss = 2.31277 avg_loss = 3.75761\n",
      "epoch no.1 train no.156450  loss = 2.96320 avg_loss = 3.73144\n",
      "epoch no.1 train no.156460  loss = 3.99002 avg_loss = 3.77842\n",
      "epoch no.1 train no.156470  loss = 3.80561 avg_loss = 3.79434\n",
      "epoch no.1 train no.156480  loss = 5.78373 avg_loss = 3.83594\n",
      "epoch no.1 train no.156490  loss = 7.09975 avg_loss = 3.84054\n",
      "epoch no.1 train no.156500  loss = 6.08185 avg_loss = 3.91419\n",
      "epoch no.1 train no.156510  loss = 3.09347 avg_loss = 3.84319\n",
      "epoch no.1 train no.156520  loss = 3.94812 avg_loss = 3.80426\n",
      "epoch no.1 train no.156530  loss = 5.95459 avg_loss = 3.86998\n",
      "epoch no.1 train no.156540  loss = 4.11638 avg_loss = 3.88173\n",
      "epoch no.1 train no.156550  loss = 5.40425 avg_loss = 3.85196\n",
      "epoch no.1 train no.156560  loss = 3.97321 avg_loss = 3.96117\n",
      "epoch no.1 train no.156570  loss = 5.77914 avg_loss = 3.99092\n",
      "epoch no.1 train no.156580  loss = 5.94455 avg_loss = 3.99930\n",
      "epoch no.1 train no.156590  loss = 6.29195 avg_loss = 4.01059\n",
      "epoch no.1 train no.156600  loss = 2.85010 avg_loss = 3.97337\n",
      "epoch no.1 train no.156610  loss = 4.36509 avg_loss = 4.01311\n",
      "epoch no.1 train no.156620  loss = 4.40712 avg_loss = 3.98363\n",
      "epoch no.1 train no.156630  loss = 2.16811 avg_loss = 3.97493\n",
      "epoch no.1 train no.156640  loss = 2.70609 avg_loss = 3.95191\n",
      "epoch no.1 train no.156650  loss = 3.13484 avg_loss = 3.97191\n",
      "epoch no.1 train no.156660  loss = 4.76216 avg_loss = 3.94040\n",
      "epoch no.1 train no.156670  loss = 4.06556 avg_loss = 3.92470\n",
      "epoch no.1 train no.156680  loss = 2.77428 avg_loss = 3.86384\n",
      "epoch no.1 train no.156690  loss = 3.11415 avg_loss = 3.83143\n",
      "epoch no.1 train no.156700  loss = 3.63234 avg_loss = 3.82144\n",
      "epoch no.1 train no.156710  loss = 3.27560 avg_loss = 3.75699\n",
      "epoch no.1 train no.156720  loss = 4.39129 avg_loss = 3.75551\n",
      "epoch no.1 train no.156730  loss = 2.92032 avg_loss = 3.78383\n",
      "epoch no.1 train no.156740  loss = 3.57763 avg_loss = 3.76302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.156750  loss = 4.33757 avg_loss = 3.81622\n",
      "epoch no.1 train no.156760  loss = 2.83219 avg_loss = 3.82298\n",
      "epoch no.1 train no.156770  loss = 2.76137 avg_loss = 3.77440\n",
      "epoch no.1 train no.156780  loss = 4.80533 avg_loss = 3.84217\n",
      "epoch no.1 train no.156790  loss = 3.18412 avg_loss = 3.81068\n",
      "epoch no.1 train no.156800  loss = 3.42815 avg_loss = 3.80254\n",
      "epoch no.1 train no.156810  loss = 2.76798 avg_loss = 3.79872\n",
      "epoch no.1 train no.156820  loss = 5.71565 avg_loss = 3.78125\n",
      "epoch no.1 train no.156830  loss = 3.22904 avg_loss = 3.80167\n",
      "epoch no.1 train no.156840  loss = 3.34286 avg_loss = 3.78491\n",
      "epoch no.1 train no.156850  loss = 2.73597 avg_loss = 3.74120\n",
      "epoch no.1 train no.156860  loss = 3.04956 avg_loss = 3.69658\n",
      "epoch no.1 train no.156870  loss = 3.81165 avg_loss = 3.74790\n",
      "epoch no.1 train no.156880  loss = 3.62740 avg_loss = 3.77135\n",
      "epoch no.1 train no.156890  loss = 6.04375 avg_loss = 3.77241\n",
      "epoch no.1 train no.156900  loss = 5.50504 avg_loss = 3.72943\n",
      "epoch no.1 train no.156910  loss = 2.39065 avg_loss = 3.73480\n",
      "epoch no.1 train no.156920  loss = 3.23536 avg_loss = 3.77436\n",
      "epoch no.1 train no.156930  loss = 3.55277 avg_loss = 3.82580\n",
      "epoch no.1 train no.156940  loss = 3.81860 avg_loss = 3.84891\n",
      "epoch no.1 train no.156950  loss = 2.91608 avg_loss = 3.76716\n",
      "epoch no.1 train no.156960  loss = 3.45284 avg_loss = 3.79225\n",
      "epoch no.1 train no.156970  loss = 4.88692 avg_loss = 3.80517\n",
      "epoch no.1 train no.156980  loss = 4.17901 avg_loss = 3.78205\n",
      "epoch no.1 train no.156990  loss = 2.90696 avg_loss = 3.70801\n",
      "epoch no.1 train no.157000  loss = 2.28491 avg_loss = 3.68464\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '을', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 좋은 음악</s>\n",
      "epoch no.1 train no.157010  loss = 5.15908 avg_loss = 3.67096\n",
      "epoch no.1 train no.157020  loss = 5.19181 avg_loss = 3.68885\n",
      "epoch no.1 train no.157030  loss = 3.18553 avg_loss = 3.67389\n",
      "epoch no.1 train no.157040  loss = 2.56812 avg_loss = 3.69994\n",
      "epoch no.1 train no.157050  loss = 6.24758 avg_loss = 3.70881\n",
      "epoch no.1 train no.157060  loss = 2.89420 avg_loss = 3.68605\n",
      "epoch no.1 train no.157070  loss = 3.30183 avg_loss = 3.69952\n",
      "epoch no.1 train no.157080  loss = 3.15111 avg_loss = 3.71033\n",
      "epoch no.1 train no.157090  loss = 3.90798 avg_loss = 3.72737\n",
      "epoch no.1 train no.157100  loss = 2.64000 avg_loss = 3.72659\n",
      "epoch no.1 train no.157110  loss = 2.81002 avg_loss = 3.68840\n",
      "epoch no.1 train no.157120  loss = 3.86467 avg_loss = 3.63594\n",
      "epoch no.1 train no.157130  loss = 5.69402 avg_loss = 3.69872\n",
      "epoch no.1 train no.157140  loss = 4.64305 avg_loss = 3.70212\n",
      "epoch no.1 train no.157150  loss = 3.14590 avg_loss = 3.75248\n",
      "epoch no.1 train no.157160  loss = 3.75604 avg_loss = 3.77888\n",
      "epoch no.1 train no.157170  loss = 3.79347 avg_loss = 3.79104\n",
      "epoch no.1 train no.157180  loss = 4.15571 avg_loss = 3.84612\n",
      "epoch no.1 train no.157190  loss = 2.09765 avg_loss = 3.85974\n",
      "epoch no.1 train no.157200  loss = 3.56047 avg_loss = 3.88238\n",
      "epoch no.1 train no.157210  loss = 2.58801 avg_loss = 3.85097\n",
      "epoch no.1 train no.157220  loss = 4.21115 avg_loss = 3.80165\n",
      "epoch no.1 train no.157230  loss = 5.83854 avg_loss = 3.80806\n",
      "epoch no.1 train no.157240  loss = 4.86397 avg_loss = 3.82750\n",
      "epoch no.1 train no.157250  loss = 4.15637 avg_loss = 3.88542\n",
      "epoch no.1 train no.157260  loss = 3.33994 avg_loss = 3.85871\n",
      "epoch no.1 train no.157270  loss = 3.55946 avg_loss = 3.81212\n",
      "epoch no.1 train no.157280  loss = 4.80992 avg_loss = 3.89991\n",
      "epoch no.1 train no.157290  loss = 3.85271 avg_loss = 3.83867\n",
      "epoch no.1 train no.157300  loss = 3.75853 avg_loss = 3.82853\n",
      "epoch no.1 train no.157310  loss = 3.00770 avg_loss = 3.82998\n",
      "epoch no.1 train no.157320  loss = 3.16460 avg_loss = 3.87967\n",
      "epoch no.1 train no.157330  loss = 2.30826 avg_loss = 3.91478\n",
      "epoch no.1 train no.157340  loss = 6.64647 avg_loss = 3.92248\n",
      "epoch no.1 train no.157350  loss = 3.59769 avg_loss = 3.90779\n",
      "epoch no.1 train no.157360  loss = 4.98119 avg_loss = 3.91962\n",
      "epoch no.1 train no.157370  loss = 2.28142 avg_loss = 3.91855\n",
      "epoch no.1 train no.157380  loss = 4.75159 avg_loss = 3.89348\n",
      "epoch no.1 train no.157390  loss = 4.72185 avg_loss = 3.89464\n",
      "epoch no.1 train no.157400  loss = 4.25366 avg_loss = 3.86851\n",
      "epoch no.1 train no.157410  loss = 3.28492 avg_loss = 3.83601\n",
      "epoch no.1 train no.157420  loss = 2.91340 avg_loss = 3.80416\n",
      "epoch no.1 train no.157430  loss = 2.97559 avg_loss = 3.78292\n",
      "epoch no.1 train no.157440  loss = 3.48242 avg_loss = 3.80999\n",
      "epoch no.1 train no.157450  loss = 3.55334 avg_loss = 3.85606\n",
      "epoch no.1 train no.157460  loss = 5.19014 avg_loss = 3.84899\n",
      "epoch no.1 train no.157470  loss = 5.51824 avg_loss = 3.82461\n",
      "epoch no.1 train no.157480  loss = 3.79423 avg_loss = 3.82330\n",
      "epoch no.1 train no.157490  loss = 4.13171 avg_loss = 3.83113\n",
      "epoch no.1 train no.157500  loss = 4.00884 avg_loss = 3.88384\n",
      "epoch no.1 train no.157510  loss = 2.45004 avg_loss = 3.86816\n",
      "epoch no.1 train no.157520  loss = 3.37082 avg_loss = 3.81618\n",
      "epoch no.1 train no.157530  loss = 3.19139 avg_loss = 3.78969\n",
      "epoch no.1 train no.157540  loss = 2.77768 avg_loss = 3.79479\n",
      "epoch no.1 train no.157550  loss = 2.41334 avg_loss = 3.81898\n",
      "epoch no.1 train no.157560  loss = 3.51631 avg_loss = 3.82848\n",
      "epoch no.1 train no.157570  loss = 2.90963 avg_loss = 3.84405\n",
      "epoch no.1 train no.157580  loss = 3.27172 avg_loss = 3.86583\n",
      "epoch no.1 train no.157590  loss = 3.59741 avg_loss = 3.86941\n",
      "epoch no.1 train no.157600  loss = 4.33988 avg_loss = 3.89463\n",
      "epoch no.1 train no.157610  loss = 2.72220 avg_loss = 3.87440\n",
      "epoch no.1 train no.157620  loss = 4.11774 avg_loss = 3.85229\n",
      "epoch no.1 train no.157630  loss = 2.20509 avg_loss = 3.82282\n",
      "epoch no.1 train no.157640  loss = 4.66756 avg_loss = 3.80436\n",
      "epoch no.1 train no.157650  loss = 3.43267 avg_loss = 3.86340\n",
      "epoch no.1 train no.157660  loss = 4.49842 avg_loss = 3.89133\n",
      "epoch no.1 train no.157670  loss = 3.39593 avg_loss = 3.86830\n",
      "epoch no.1 train no.157680  loss = 4.25116 avg_loss = 3.90012\n",
      "epoch no.1 train no.157690  loss = 3.65028 avg_loss = 3.92308\n",
      "epoch no.1 train no.157700  loss = 4.40148 avg_loss = 3.86701\n",
      "epoch no.1 train no.157710  loss = 4.07777 avg_loss = 3.88537\n",
      "epoch no.1 train no.157720  loss = 4.36676 avg_loss = 3.93024\n",
      "epoch no.1 train no.157730  loss = 3.90359 avg_loss = 3.91886\n",
      "epoch no.1 train no.157740  loss = 3.51456 avg_loss = 3.91541\n",
      "epoch no.1 train no.157750  loss = 1.77794 avg_loss = 3.92493\n",
      "epoch no.1 train no.157760  loss = 4.54083 avg_loss = 3.90519\n",
      "epoch no.1 train no.157770  loss = 3.75737 avg_loss = 3.91574\n",
      "epoch no.1 train no.157780  loss = 2.91548 avg_loss = 3.91790\n",
      "epoch no.1 train no.157790  loss = 2.29809 avg_loss = 3.89068\n",
      "epoch no.1 train no.157800  loss = 3.49901 avg_loss = 3.87220\n",
      "epoch no.1 train no.157810  loss = 4.90639 avg_loss = 3.93939\n",
      "epoch no.1 train no.157820  loss = 3.22227 avg_loss = 3.91279\n",
      "epoch no.1 train no.157830  loss = 2.57045 avg_loss = 3.88520\n",
      "epoch no.1 train no.157840  loss = 2.67490 avg_loss = 3.83439\n",
      "epoch no.1 train no.157850  loss = 2.94361 avg_loss = 3.85041\n",
      "epoch no.1 train no.157860  loss = 4.36121 avg_loss = 3.85712\n",
      "epoch no.1 train no.157870  loss = 4.57185 avg_loss = 3.88540\n",
      "epoch no.1 train no.157880  loss = 3.93402 avg_loss = 3.91776\n",
      "epoch no.1 train no.157890  loss = 3.35126 avg_loss = 3.89278\n",
      "epoch no.1 train no.157900  loss = 3.43029 avg_loss = 3.92160\n",
      "epoch no.1 train no.157910  loss = 4.62397 avg_loss = 3.92337\n",
      "epoch no.1 train no.157920  loss = 5.01205 avg_loss = 3.90480\n",
      "epoch no.1 train no.157930  loss = 3.18312 avg_loss = 3.87308\n",
      "epoch no.1 train no.157940  loss = 4.39028 avg_loss = 3.88028\n",
      "epoch no.1 train no.157950  loss = 3.17433 avg_loss = 3.83301\n",
      "epoch no.1 train no.157960  loss = 2.69529 avg_loss = 3.82984\n",
      "epoch no.1 train no.157970  loss = 2.59474 avg_loss = 3.86147\n",
      "epoch no.1 train no.157980  loss = 3.89573 avg_loss = 3.82080\n",
      "epoch no.1 train no.157990  loss = 3.22669 avg_loss = 3.76610\n",
      "epoch no.1 train no.158000  loss = 6.36835 avg_loss = 3.79006\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '을', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.1 train no.158010  loss = 2.86425 avg_loss = 3.77896\n",
      "epoch no.1 train no.158020  loss = 3.46804 avg_loss = 3.71984\n",
      "epoch no.1 train no.158030  loss = 5.75382 avg_loss = 3.74136\n",
      "epoch no.1 train no.158040  loss = 5.15103 avg_loss = 3.76675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.158050  loss = 4.18384 avg_loss = 3.77373\n",
      "epoch no.1 train no.158060  loss = 3.54648 avg_loss = 3.72404\n",
      "epoch no.1 train no.158070  loss = 4.85967 avg_loss = 3.81699\n",
      "epoch no.1 train no.158080  loss = 1.89055 avg_loss = 3.77279\n",
      "epoch no.1 train no.158090  loss = 4.28503 avg_loss = 3.80824\n",
      "epoch no.1 train no.158100  loss = 5.04580 avg_loss = 3.80265\n",
      "epoch no.1 train no.158110  loss = 2.01379 avg_loss = 3.78240\n",
      "epoch no.1 train no.158120  loss = 2.82321 avg_loss = 3.72360\n",
      "epoch no.1 train no.158130  loss = 3.64428 avg_loss = 3.76691\n",
      "epoch no.1 train no.158140  loss = 2.53613 avg_loss = 3.76196\n",
      "epoch no.1 train no.158150  loss = 3.10503 avg_loss = 3.77775\n",
      "epoch no.1 train no.158160  loss = 2.05011 avg_loss = 3.80962\n",
      "epoch no.1 train no.158170  loss = 4.00409 avg_loss = 3.77878\n",
      "epoch no.1 train no.158180  loss = 4.34968 avg_loss = 3.76113\n",
      "epoch no.1 train no.158190  loss = 1.97285 avg_loss = 3.76413\n",
      "epoch no.1 train no.158200  loss = 3.33731 avg_loss = 3.77314\n",
      "epoch no.1 train no.158210  loss = 2.76086 avg_loss = 3.81092\n",
      "epoch no.1 train no.158220  loss = 2.92892 avg_loss = 3.85088\n",
      "epoch no.1 train no.158230  loss = 2.19417 avg_loss = 3.82373\n",
      "epoch no.1 train no.158240  loss = 2.48825 avg_loss = 3.81156\n",
      "epoch no.1 train no.158250  loss = 3.31802 avg_loss = 3.81410\n",
      "epoch no.1 train no.158260  loss = 3.14134 avg_loss = 3.79123\n",
      "epoch no.1 train no.158270  loss = 4.66461 avg_loss = 3.81579\n",
      "epoch no.1 train no.158280  loss = 3.46611 avg_loss = 3.78651\n",
      "epoch no.1 train no.158290  loss = 3.36063 avg_loss = 3.79027\n",
      "epoch no.1 train no.158300  loss = 1.73915 avg_loss = 3.81720\n",
      "epoch no.1 train no.158310  loss = 5.68099 avg_loss = 3.83881\n",
      "epoch no.1 train no.158320  loss = 5.88766 avg_loss = 3.79525\n",
      "epoch no.1 train no.158330  loss = 2.63791 avg_loss = 3.76027\n",
      "epoch no.1 train no.158340  loss = 3.86621 avg_loss = 3.73633\n",
      "epoch no.1 train no.158350  loss = 3.85380 avg_loss = 3.78279\n",
      "epoch no.1 train no.158360  loss = 3.25087 avg_loss = 3.72710\n",
      "epoch no.1 train no.158370  loss = 4.52383 avg_loss = 3.75089\n",
      "epoch no.1 train no.158380  loss = 2.57558 avg_loss = 3.69101\n",
      "epoch no.1 train no.158390  loss = 4.51996 avg_loss = 3.70070\n",
      "epoch no.1 train no.158400  loss = 3.28192 avg_loss = 3.66858\n",
      "epoch no.1 train no.158410  loss = 2.12633 avg_loss = 3.65212\n",
      "epoch no.1 train no.158420  loss = 3.63599 avg_loss = 3.63220\n",
      "epoch no.1 train no.158430  loss = 5.36187 avg_loss = 3.68869\n",
      "epoch no.1 train no.158440  loss = 2.90186 avg_loss = 3.68736\n",
      "epoch no.1 train no.158450  loss = 4.09106 avg_loss = 3.67854\n",
      "epoch no.1 train no.158460  loss = 3.52523 avg_loss = 3.72843\n",
      "epoch no.1 train no.158470  loss = 2.99487 avg_loss = 3.73218\n",
      "epoch no.1 train no.158480  loss = 2.72276 avg_loss = 3.72413\n",
      "epoch no.1 train no.158490  loss = 4.09086 avg_loss = 3.74912\n",
      "epoch no.1 train no.158500  loss = 4.30036 avg_loss = 3.78257\n",
      "epoch no.1 train no.158510  loss = 4.39454 avg_loss = 3.75746\n",
      "epoch no.1 train no.158520  loss = 2.40403 avg_loss = 3.75404\n",
      "epoch no.1 train no.158530  loss = 3.20808 avg_loss = 3.78130\n",
      "epoch no.1 train no.158540  loss = 2.39415 avg_loss = 3.80722\n",
      "epoch no.1 train no.158550  loss = 3.89777 avg_loss = 3.84124\n",
      "epoch no.1 train no.158560  loss = 5.25607 avg_loss = 3.88800\n",
      "epoch no.1 train no.158570  loss = 2.72042 avg_loss = 3.88369\n",
      "epoch no.1 train no.158580  loss = 3.96566 avg_loss = 3.91698\n",
      "epoch no.1 train no.158590  loss = 3.51379 avg_loss = 3.89394\n",
      "epoch no.1 train no.158600  loss = 2.64273 avg_loss = 3.91063\n",
      "epoch no.1 train no.158610  loss = 2.27571 avg_loss = 3.91619\n",
      "epoch no.1 train no.158620  loss = 2.60122 avg_loss = 3.89662\n",
      "epoch no.1 train no.158630  loss = 3.20816 avg_loss = 3.84531\n",
      "epoch no.1 train no.158640  loss = 3.63034 avg_loss = 3.84115\n",
      "epoch no.1 train no.158650  loss = 3.12487 avg_loss = 3.79500\n",
      "epoch no.1 train no.158660  loss = 5.67050 avg_loss = 3.82382\n",
      "epoch no.1 train no.158670  loss = 3.82959 avg_loss = 3.81544\n",
      "epoch no.1 train no.158680  loss = 2.59292 avg_loss = 3.86017\n",
      "epoch no.1 train no.158690  loss = 6.65713 avg_loss = 3.89550\n",
      "epoch no.1 train no.158700  loss = 2.09508 avg_loss = 3.85342\n",
      "epoch no.1 train no.158710  loss = 2.26024 avg_loss = 3.86092\n",
      "epoch no.1 train no.158720  loss = 3.83989 avg_loss = 3.82842\n",
      "epoch no.1 train no.158730  loss = 3.76904 avg_loss = 3.83410\n",
      "epoch no.1 train no.158740  loss = 2.54506 avg_loss = 3.82224\n",
      "epoch no.1 train no.158750  loss = 2.86857 avg_loss = 3.81045\n",
      "epoch no.1 train no.158760  loss = 3.39830 avg_loss = 3.78274\n",
      "epoch no.1 train no.158770  loss = 7.30407 avg_loss = 3.82623\n",
      "epoch no.1 train no.158780  loss = 2.22085 avg_loss = 3.83499\n",
      "epoch no.1 train no.158790  loss = 4.79481 avg_loss = 3.81698\n",
      "epoch no.1 train no.158800  loss = 4.17007 avg_loss = 3.80516\n",
      "epoch no.1 train no.158810  loss = 3.48923 avg_loss = 3.82106\n",
      "epoch no.1 train no.158820  loss = 3.84414 avg_loss = 3.77294\n",
      "epoch no.1 train no.158830  loss = 3.47659 avg_loss = 3.78854\n",
      "epoch no.1 train no.158840  loss = 6.73236 avg_loss = 3.79372\n",
      "epoch no.1 train no.158850  loss = 4.59607 avg_loss = 3.78616\n",
      "epoch no.1 train no.158860  loss = 2.85686 avg_loss = 3.77281\n",
      "epoch no.1 train no.158870  loss = 3.72261 avg_loss = 3.74771\n",
      "epoch no.1 train no.158880  loss = 4.05507 avg_loss = 3.84226\n",
      "epoch no.1 train no.158890  loss = 1.95893 avg_loss = 3.85370\n",
      "epoch no.1 train no.158900  loss = 3.45826 avg_loss = 3.86544\n",
      "epoch no.1 train no.158910  loss = 7.03279 avg_loss = 3.93981\n",
      "epoch no.1 train no.158920  loss = 2.17745 avg_loss = 3.91004\n",
      "epoch no.1 train no.158930  loss = 2.05942 avg_loss = 3.86327\n",
      "epoch no.1 train no.158940  loss = 2.21749 avg_loss = 3.81797\n",
      "epoch no.1 train no.158950  loss = 5.06911 avg_loss = 3.81339\n",
      "epoch no.1 train no.158960  loss = 4.66647 avg_loss = 3.81441\n",
      "epoch no.1 train no.158970  loss = 6.03028 avg_loss = 3.83846\n",
      "epoch no.1 train no.158980  loss = 6.45707 avg_loss = 3.87803\n",
      "epoch no.1 train no.158990  loss = 3.48004 avg_loss = 3.84259\n",
      "epoch no.1 train no.159000  loss = 3.58986 avg_loss = 3.85639\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.159010  loss = 3.72735 avg_loss = 3.86708\n",
      "epoch no.1 train no.159020  loss = 4.61083 avg_loss = 3.86857\n",
      "epoch no.1 train no.159030  loss = 2.87541 avg_loss = 3.82299\n",
      "epoch no.1 train no.159040  loss = 4.23327 avg_loss = 3.84076\n",
      "epoch no.1 train no.159050  loss = 3.43232 avg_loss = 3.89117\n",
      "epoch no.1 train no.159060  loss = 4.93710 avg_loss = 3.94740\n",
      "epoch no.1 train no.159070  loss = 3.20317 avg_loss = 3.97505\n",
      "epoch no.1 train no.159080  loss = 3.47675 avg_loss = 3.94750\n",
      "epoch no.1 train no.159090  loss = 4.45405 avg_loss = 3.96037\n",
      "epoch no.1 train no.159100  loss = 4.30497 avg_loss = 4.01171\n",
      "epoch no.1 train no.159110  loss = 3.47973 avg_loss = 3.97529\n",
      "epoch no.1 train no.159120  loss = 2.81141 avg_loss = 3.93887\n",
      "epoch no.1 train no.159130  loss = 4.02057 avg_loss = 3.92466\n",
      "epoch no.1 train no.159140  loss = 3.80076 avg_loss = 3.96604\n",
      "epoch no.1 train no.159150  loss = 4.69579 avg_loss = 3.98682\n",
      "epoch no.1 train no.159160  loss = 3.50760 avg_loss = 3.98473\n",
      "epoch no.1 train no.159170  loss = 2.58965 avg_loss = 3.95733\n",
      "epoch no.1 train no.159180  loss = 2.85209 avg_loss = 3.94539\n",
      "epoch no.1 train no.159190  loss = 2.83646 avg_loss = 3.93363\n",
      "epoch no.1 train no.159200  loss = 1.79342 avg_loss = 3.87351\n",
      "epoch no.1 train no.159210  loss = 4.02048 avg_loss = 3.85426\n",
      "epoch no.1 train no.159220  loss = 4.87448 avg_loss = 3.85545\n",
      "epoch no.1 train no.159230  loss = 4.33771 avg_loss = 3.84381\n",
      "epoch no.1 train no.159240  loss = 4.34717 avg_loss = 3.81552\n",
      "epoch no.1 train no.159250  loss = 4.47578 avg_loss = 3.79268\n",
      "epoch no.1 train no.159260  loss = 3.06564 avg_loss = 3.77782\n",
      "epoch no.1 train no.159270  loss = 3.87529 avg_loss = 3.81065\n",
      "epoch no.1 train no.159280  loss = 4.02559 avg_loss = 3.86136\n",
      "epoch no.1 train no.159290  loss = 3.83194 avg_loss = 3.91368\n",
      "epoch no.1 train no.159300  loss = 2.74814 avg_loss = 3.88315\n",
      "epoch no.1 train no.159310  loss = 2.52799 avg_loss = 3.89003\n",
      "epoch no.1 train no.159320  loss = 4.21071 avg_loss = 3.87331\n",
      "epoch no.1 train no.159330  loss = 5.97159 avg_loss = 3.93638\n",
      "epoch no.1 train no.159340  loss = 4.04933 avg_loss = 3.94767\n",
      "epoch no.1 train no.159350  loss = 4.39180 avg_loss = 3.89398\n",
      "epoch no.1 train no.159360  loss = 3.83453 avg_loss = 3.89936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.159370  loss = 4.13958 avg_loss = 3.85881\n",
      "epoch no.1 train no.159380  loss = 3.15346 avg_loss = 3.85777\n",
      "epoch no.1 train no.159390  loss = 2.80197 avg_loss = 3.85398\n",
      "epoch no.1 train no.159400  loss = 4.75677 avg_loss = 3.81600\n",
      "epoch no.1 train no.159410  loss = 3.83561 avg_loss = 3.84538\n",
      "epoch no.1 train no.159420  loss = 2.47605 avg_loss = 3.80997\n",
      "epoch no.1 train no.159430  loss = 3.13358 avg_loss = 3.79591\n",
      "epoch no.1 train no.159440  loss = 5.34169 avg_loss = 3.85126\n",
      "epoch no.1 train no.159450  loss = 3.72229 avg_loss = 3.81140\n",
      "epoch no.1 train no.159460  loss = 6.48832 avg_loss = 3.79840\n",
      "epoch no.1 train no.159470  loss = 2.37864 avg_loss = 3.74449\n",
      "epoch no.1 train no.159480  loss = 3.16887 avg_loss = 3.78484\n",
      "epoch no.1 train no.159490  loss = 2.41384 avg_loss = 3.79516\n",
      "epoch no.1 train no.159500  loss = 4.59591 avg_loss = 3.84176\n",
      "epoch no.1 train no.159510  loss = 4.79068 avg_loss = 3.86915\n",
      "epoch no.1 train no.159520  loss = 3.96237 avg_loss = 3.87150\n",
      "epoch no.1 train no.159530  loss = 3.22163 avg_loss = 3.86085\n",
      "epoch no.1 train no.159540  loss = 6.05226 avg_loss = 3.88756\n",
      "epoch no.1 train no.159550  loss = 5.70446 avg_loss = 3.89573\n",
      "epoch no.1 train no.159560  loss = 3.73460 avg_loss = 3.86147\n",
      "epoch no.1 train no.159570  loss = 2.73365 avg_loss = 3.86116\n",
      "epoch no.1 train no.159580  loss = 4.37005 avg_loss = 3.86570\n",
      "epoch no.1 train no.159590  loss = 2.80129 avg_loss = 3.87569\n",
      "epoch no.1 train no.159600  loss = 4.42136 avg_loss = 3.84486\n",
      "epoch no.1 train no.159610  loss = 2.01482 avg_loss = 3.82165\n",
      "epoch no.1 train no.159620  loss = 3.67875 avg_loss = 3.79963\n",
      "epoch no.1 train no.159630  loss = 4.20837 avg_loss = 3.78497\n",
      "epoch no.1 train no.159640  loss = 3.37452 avg_loss = 3.78589\n",
      "epoch no.1 train no.159650  loss = 2.08595 avg_loss = 3.76293\n",
      "epoch no.1 train no.159660  loss = 4.22122 avg_loss = 3.77138\n",
      "epoch no.1 train no.159670  loss = 3.37637 avg_loss = 3.76815\n",
      "epoch no.1 train no.159680  loss = 3.35475 avg_loss = 3.79073\n",
      "epoch no.1 train no.159690  loss = 6.53312 avg_loss = 3.85314\n",
      "epoch no.1 train no.159700  loss = 2.77310 avg_loss = 3.88255\n",
      "epoch no.1 train no.159710  loss = 4.15211 avg_loss = 3.85825\n",
      "epoch no.1 train no.159720  loss = 7.12083 avg_loss = 3.89348\n",
      "epoch no.1 train no.159730  loss = 3.59544 avg_loss = 3.89950\n",
      "epoch no.1 train no.159740  loss = 4.17446 avg_loss = 3.85893\n",
      "epoch no.1 train no.159750  loss = 3.32502 avg_loss = 3.84632\n",
      "epoch no.1 train no.159760  loss = 4.47227 avg_loss = 3.84776\n",
      "epoch no.1 train no.159770  loss = 3.66172 avg_loss = 3.83057\n",
      "epoch no.1 train no.159780  loss = 3.28365 avg_loss = 3.81352\n",
      "epoch no.1 train no.159790  loss = 3.77324 avg_loss = 3.77677\n",
      "epoch no.1 train no.159800  loss = 4.73077 avg_loss = 3.80075\n",
      "epoch no.1 train no.159810  loss = 5.38335 avg_loss = 3.76499\n",
      "epoch no.1 train no.159820  loss = 5.31031 avg_loss = 3.75938\n",
      "epoch no.1 train no.159830  loss = 3.35464 avg_loss = 3.74415\n",
      "epoch no.1 train no.159840  loss = 5.88171 avg_loss = 3.76398\n",
      "epoch no.1 train no.159850  loss = 2.04085 avg_loss = 3.72338\n",
      "epoch no.1 train no.159860  loss = 2.30404 avg_loss = 3.71432\n",
      "epoch no.1 train no.159870  loss = 6.79593 avg_loss = 3.75432\n",
      "epoch no.1 train no.159880  loss = 1.92036 avg_loss = 3.72156\n",
      "epoch no.1 train no.159890  loss = 3.54855 avg_loss = 3.77428\n",
      "epoch no.1 train no.159900  loss = 3.96207 avg_loss = 3.73632\n",
      "epoch no.1 train no.159910  loss = 2.62033 avg_loss = 3.76038\n",
      "epoch no.1 train no.159920  loss = 3.58742 avg_loss = 3.75047\n",
      "epoch no.1 train no.159930  loss = 2.59428 avg_loss = 3.75793\n",
      "epoch no.1 train no.159940  loss = 5.14260 avg_loss = 3.78294\n",
      "epoch no.1 train no.159950  loss = 5.01514 avg_loss = 3.70526\n",
      "epoch no.1 train no.159960  loss = 3.04801 avg_loss = 3.68924\n",
      "epoch no.1 train no.159970  loss = 3.94667 avg_loss = 3.69768\n",
      "epoch no.1 train no.159980  loss = 1.91296 avg_loss = 3.71188\n",
      "epoch no.1 train no.159990  loss = 3.78758 avg_loss = 3.68073\n",
      "epoch no.1 train no.160000  loss = 5.22546 avg_loss = 3.74389\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '▁신나는', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.1 train no.160010  loss = 2.74092 avg_loss = 3.72557\n",
      "epoch no.1 train no.160020  loss = 2.44833 avg_loss = 3.70252\n",
      "epoch no.1 train no.160030  loss = 2.71086 avg_loss = 3.69035\n",
      "epoch no.1 train no.160040  loss = 3.15061 avg_loss = 3.66431\n",
      "epoch no.1 train no.160050  loss = 2.63420 avg_loss = 3.69019\n",
      "epoch no.1 train no.160060  loss = 2.00224 avg_loss = 3.64825\n",
      "epoch no.1 train no.160070  loss = 2.08632 avg_loss = 3.68164\n",
      "epoch no.1 train no.160080  loss = 2.97666 avg_loss = 3.71453\n",
      "epoch no.1 train no.160090  loss = 2.82133 avg_loss = 3.71925\n",
      "epoch no.1 train no.160100  loss = 4.16248 avg_loss = 3.69495\n",
      "epoch no.1 train no.160110  loss = 3.77590 avg_loss = 3.74391\n",
      "epoch no.1 train no.160120  loss = 3.11097 avg_loss = 3.71617\n",
      "epoch no.1 train no.160130  loss = 4.26066 avg_loss = 3.75228\n",
      "epoch no.1 train no.160140  loss = 4.96489 avg_loss = 3.73073\n",
      "epoch no.1 train no.160150  loss = 4.28558 avg_loss = 3.73391\n",
      "epoch no.1 train no.160160  loss = 3.15592 avg_loss = 3.70173\n",
      "epoch no.1 train no.160170  loss = 3.11528 avg_loss = 3.68524\n",
      "epoch no.1 train no.160180  loss = 2.94913 avg_loss = 3.68224\n",
      "epoch no.1 train no.160190  loss = 6.24770 avg_loss = 3.70957\n",
      "epoch no.1 train no.160200  loss = 6.76362 avg_loss = 3.79813\n",
      "epoch no.1 train no.160210  loss = 4.24633 avg_loss = 3.80586\n",
      "epoch no.1 train no.160220  loss = 3.38078 avg_loss = 3.76229\n",
      "epoch no.1 train no.160230  loss = 4.69907 avg_loss = 3.75423\n",
      "epoch no.1 train no.160240  loss = 3.96767 avg_loss = 3.76285\n",
      "epoch no.1 train no.160250  loss = 1.46280 avg_loss = 3.71792\n",
      "epoch no.1 train no.160260  loss = 2.83590 avg_loss = 3.78611\n",
      "epoch no.1 train no.160270  loss = 3.29344 avg_loss = 3.75016\n",
      "epoch no.1 train no.160280  loss = 3.17263 avg_loss = 3.73517\n",
      "epoch no.1 train no.160290  loss = 3.18051 avg_loss = 3.77141\n",
      "epoch no.1 train no.160300  loss = 3.77243 avg_loss = 3.80539\n",
      "epoch no.1 train no.160310  loss = 3.40760 avg_loss = 3.77392\n",
      "epoch no.1 train no.160320  loss = 4.41291 avg_loss = 3.78668\n",
      "epoch no.1 train no.160330  loss = 4.16292 avg_loss = 3.75868\n",
      "epoch no.1 train no.160340  loss = 5.17892 avg_loss = 3.76983\n",
      "epoch no.1 train no.160350  loss = 2.56806 avg_loss = 3.74032\n",
      "epoch no.1 train no.160360  loss = 2.64272 avg_loss = 3.72342\n",
      "epoch no.1 train no.160370  loss = 2.82303 avg_loss = 3.69872\n",
      "epoch no.1 train no.160380  loss = 5.82023 avg_loss = 3.72525\n",
      "epoch no.1 train no.160390  loss = 4.29708 avg_loss = 3.78730\n",
      "epoch no.1 train no.160400  loss = 3.87271 avg_loss = 3.85111\n",
      "epoch no.1 train no.160410  loss = 4.09140 avg_loss = 3.80154\n",
      "epoch no.1 train no.160420  loss = 3.91022 avg_loss = 3.77135\n",
      "epoch no.1 train no.160430  loss = 1.33447 avg_loss = 3.76854\n",
      "epoch no.1 train no.160440  loss = 3.34895 avg_loss = 3.74529\n",
      "epoch no.1 train no.160450  loss = 3.81900 avg_loss = 3.68600\n",
      "epoch no.1 train no.160460  loss = 5.59662 avg_loss = 3.74656\n",
      "epoch no.1 train no.160470  loss = 1.96548 avg_loss = 3.74094\n",
      "epoch no.1 train no.160480  loss = 4.64160 avg_loss = 3.72728\n",
      "epoch no.1 train no.160490  loss = 5.54958 avg_loss = 3.73755\n",
      "epoch no.1 train no.160500  loss = 2.39786 avg_loss = 3.75787\n",
      "epoch no.1 train no.160510  loss = 4.92062 avg_loss = 3.77591\n",
      "epoch no.1 train no.160520  loss = 2.43009 avg_loss = 3.76357\n",
      "epoch no.1 train no.160530  loss = 3.38642 avg_loss = 3.77243\n",
      "epoch no.1 train no.160540  loss = 3.04717 avg_loss = 3.75336\n",
      "epoch no.1 train no.160550  loss = 3.17919 avg_loss = 3.81447\n",
      "epoch no.1 train no.160560  loss = 2.64828 avg_loss = 3.81219\n",
      "epoch no.1 train no.160570  loss = 3.87764 avg_loss = 3.85715\n",
      "epoch no.1 train no.160580  loss = 3.40883 avg_loss = 3.90110\n",
      "epoch no.1 train no.160590  loss = 4.97859 avg_loss = 3.89140\n",
      "epoch no.1 train no.160600  loss = 6.37280 avg_loss = 3.95719\n",
      "epoch no.1 train no.160610  loss = 3.73129 avg_loss = 3.88867\n",
      "epoch no.1 train no.160620  loss = 2.99133 avg_loss = 3.94365\n",
      "epoch no.1 train no.160630  loss = 3.33441 avg_loss = 3.94831\n",
      "epoch no.1 train no.160640  loss = 4.60175 avg_loss = 3.97383\n",
      "epoch no.1 train no.160650  loss = 4.01887 avg_loss = 3.93040\n",
      "epoch no.1 train no.160660  loss = 4.34014 avg_loss = 3.94500\n",
      "epoch no.1 train no.160670  loss = 2.41564 avg_loss = 3.93455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.160680  loss = 3.30223 avg_loss = 3.91096\n",
      "epoch no.1 train no.160690  loss = 3.61062 avg_loss = 3.91629\n",
      "epoch no.1 train no.160700  loss = 3.95471 avg_loss = 3.85590\n",
      "epoch no.1 train no.160710  loss = 2.69228 avg_loss = 3.86432\n",
      "epoch no.1 train no.160720  loss = 2.69259 avg_loss = 3.86579\n",
      "epoch no.1 train no.160730  loss = 3.87767 avg_loss = 3.89277\n",
      "epoch no.1 train no.160740  loss = 2.83955 avg_loss = 3.84246\n",
      "epoch no.1 train no.160750  loss = 4.06908 avg_loss = 3.88395\n",
      "epoch no.1 train no.160760  loss = 2.71693 avg_loss = 3.86903\n",
      "epoch no.1 train no.160770  loss = 2.25638 avg_loss = 3.79841\n",
      "epoch no.1 train no.160780  loss = 5.74702 avg_loss = 3.83522\n",
      "epoch no.1 train no.160790  loss = 3.53306 avg_loss = 3.80276\n",
      "epoch no.1 train no.160800  loss = 1.89450 avg_loss = 3.74201\n",
      "epoch no.1 train no.160810  loss = 3.57299 avg_loss = 3.75655\n",
      "epoch no.1 train no.160820  loss = 3.78238 avg_loss = 3.79727\n",
      "epoch no.1 train no.160830  loss = 7.64204 avg_loss = 3.87669\n",
      "epoch no.1 train no.160840  loss = 4.07090 avg_loss = 3.86341\n",
      "epoch no.1 train no.160850  loss = 4.48009 avg_loss = 3.86528\n",
      "epoch no.1 train no.160860  loss = 6.03944 avg_loss = 3.84208\n",
      "epoch no.1 train no.160870  loss = 3.93048 avg_loss = 3.89503\n",
      "epoch no.1 train no.160880  loss = 4.42875 avg_loss = 3.89718\n",
      "epoch no.1 train no.160890  loss = 2.70197 avg_loss = 3.90167\n",
      "epoch no.1 train no.160900  loss = 4.88425 avg_loss = 3.88361\n",
      "epoch no.1 train no.160910  loss = 3.01484 avg_loss = 3.86632\n",
      "epoch no.1 train no.160920  loss = 4.75522 avg_loss = 3.85856\n",
      "epoch no.1 train no.160930  loss = 3.14104 avg_loss = 3.84795\n",
      "epoch no.1 train no.160940  loss = 3.77479 avg_loss = 3.76445\n",
      "epoch no.1 train no.160950  loss = 2.57152 avg_loss = 3.77373\n",
      "epoch no.1 train no.160960  loss = 3.74864 avg_loss = 3.77367\n",
      "epoch no.1 train no.160970  loss = 4.43989 avg_loss = 3.77925\n",
      "epoch no.1 train no.160980  loss = 4.93732 avg_loss = 3.71194\n",
      "epoch no.1 train no.160990  loss = 3.11565 avg_loss = 3.67530\n",
      "epoch no.1 train no.161000  loss = 3.74966 avg_loss = 3.66848\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '을', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.1 train no.161010  loss = 5.76055 avg_loss = 3.72455\n",
      "epoch no.1 train no.161020  loss = 4.10540 avg_loss = 3.70489\n",
      "epoch no.1 train no.161030  loss = 2.70101 avg_loss = 3.74849\n",
      "epoch no.1 train no.161040  loss = 4.90202 avg_loss = 3.80068\n",
      "epoch no.1 train no.161050  loss = 4.94363 avg_loss = 3.76810\n",
      "epoch no.1 train no.161060  loss = 4.43258 avg_loss = 3.74100\n",
      "epoch no.1 train no.161070  loss = 3.63848 avg_loss = 3.73060\n",
      "epoch no.1 train no.161080  loss = 2.54370 avg_loss = 3.74336\n",
      "epoch no.1 train no.161090  loss = 2.72900 avg_loss = 3.80276\n",
      "epoch no.1 train no.161100  loss = 5.20748 avg_loss = 3.86452\n",
      "epoch no.1 train no.161110  loss = 2.92728 avg_loss = 3.84340\n",
      "epoch no.1 train no.161120  loss = 5.88116 avg_loss = 3.81226\n",
      "epoch no.1 train no.161130  loss = 4.24612 avg_loss = 3.78497\n",
      "epoch no.1 train no.161140  loss = 2.83124 avg_loss = 3.79253\n",
      "epoch no.1 train no.161150  loss = 3.09988 avg_loss = 3.82200\n",
      "epoch no.1 train no.161160  loss = 2.52760 avg_loss = 3.76308\n",
      "epoch no.1 train no.161170  loss = 2.12238 avg_loss = 3.73062\n",
      "epoch no.1 train no.161180  loss = 2.40528 avg_loss = 3.73766\n",
      "epoch no.1 train no.161190  loss = 4.04427 avg_loss = 3.71861\n",
      "epoch no.1 train no.161200  loss = 3.72681 avg_loss = 3.67031\n",
      "epoch no.1 train no.161210  loss = 2.61845 avg_loss = 3.60624\n",
      "epoch no.1 train no.161220  loss = 3.95332 avg_loss = 3.66756\n",
      "epoch no.1 train no.161230  loss = 2.72182 avg_loss = 3.66819\n",
      "epoch no.1 train no.161240  loss = 4.14301 avg_loss = 3.65471\n",
      "epoch no.1 train no.161250  loss = 2.83550 avg_loss = 3.60648\n",
      "epoch no.1 train no.161260  loss = 3.44115 avg_loss = 3.63939\n",
      "epoch no.1 train no.161270  loss = 2.59988 avg_loss = 3.64901\n",
      "epoch no.1 train no.161280  loss = 4.33761 avg_loss = 3.64471\n",
      "epoch no.1 train no.161290  loss = 5.71695 avg_loss = 3.65678\n",
      "epoch no.1 train no.161300  loss = 3.78157 avg_loss = 3.66072\n",
      "epoch no.1 train no.161310  loss = 4.80172 avg_loss = 3.66541\n",
      "epoch no.1 train no.161320  loss = 2.52715 avg_loss = 3.76018\n",
      "epoch no.1 train no.161330  loss = 7.70692 avg_loss = 3.85622\n",
      "epoch no.1 train no.161340  loss = 3.86489 avg_loss = 3.89173\n",
      "epoch no.1 train no.161350  loss = 3.58713 avg_loss = 3.91597\n",
      "epoch no.1 train no.161360  loss = 3.17687 avg_loss = 3.90530\n",
      "epoch no.1 train no.161370  loss = 5.61578 avg_loss = 3.91337\n",
      "epoch no.1 train no.161380  loss = 3.95649 avg_loss = 3.89600\n",
      "epoch no.1 train no.161390  loss = 5.25856 avg_loss = 3.89578\n",
      "epoch no.1 train no.161400  loss = 7.29929 avg_loss = 3.97433\n",
      "epoch no.1 train no.161410  loss = 2.85653 avg_loss = 3.93323\n",
      "epoch no.1 train no.161420  loss = 3.68059 avg_loss = 3.94623\n",
      "epoch no.1 train no.161430  loss = 3.03559 avg_loss = 3.90524\n",
      "epoch no.1 train no.161440  loss = 4.05412 avg_loss = 3.88981\n",
      "epoch no.1 train no.161450  loss = 2.31839 avg_loss = 3.85639\n",
      "epoch no.1 train no.161460  loss = 3.65911 avg_loss = 3.84099\n",
      "epoch no.1 train no.161470  loss = 3.79389 avg_loss = 3.87485\n",
      "epoch no.1 train no.161480  loss = 2.88757 avg_loss = 3.85348\n",
      "epoch no.1 train no.161490  loss = 4.32686 avg_loss = 3.86024\n",
      "epoch no.1 train no.161500  loss = 5.74494 avg_loss = 3.86294\n",
      "epoch no.1 train no.161510  loss = 2.31761 avg_loss = 3.78189\n",
      "epoch no.1 train no.161520  loss = 3.99512 avg_loss = 3.82411\n",
      "epoch no.1 train no.161530  loss = 1.67137 avg_loss = 3.85241\n",
      "epoch no.1 train no.161540  loss = 3.76478 avg_loss = 3.80867\n",
      "epoch no.1 train no.161550  loss = 2.99183 avg_loss = 3.73759\n",
      "epoch no.1 train no.161560  loss = 3.40367 avg_loss = 3.79727\n",
      "epoch no.1 train no.161570  loss = 5.13060 avg_loss = 3.84294\n",
      "epoch no.1 train no.161580  loss = 2.09988 avg_loss = 3.82511\n",
      "epoch no.1 train no.161590  loss = 5.20238 avg_loss = 3.84680\n",
      "epoch no.1 train no.161600  loss = 4.28290 avg_loss = 3.77472\n",
      "epoch no.1 train no.161610  loss = 5.63707 avg_loss = 3.82295\n",
      "epoch no.1 train no.161620  loss = 4.36802 avg_loss = 3.76902\n",
      "epoch no.1 train no.161630  loss = 2.64992 avg_loss = 3.82700\n",
      "epoch no.1 train no.161640  loss = 1.84943 avg_loss = 3.85470\n",
      "epoch no.1 train no.161650  loss = 3.47863 avg_loss = 3.83230\n",
      "epoch no.1 train no.161660  loss = 3.54088 avg_loss = 3.80767\n",
      "epoch no.1 train no.161670  loss = 3.07107 avg_loss = 3.76842\n",
      "epoch no.1 train no.161680  loss = 2.79538 avg_loss = 3.84333\n",
      "epoch no.1 train no.161690  loss = 5.87988 avg_loss = 3.84105\n",
      "epoch no.1 train no.161700  loss = 2.71052 avg_loss = 3.83842\n",
      "epoch no.1 train no.161710  loss = 3.12084 avg_loss = 3.79870\n",
      "epoch no.1 train no.161720  loss = 4.01313 avg_loss = 3.75391\n",
      "epoch no.1 train no.161730  loss = 3.97565 avg_loss = 3.76427\n",
      "epoch no.1 train no.161740  loss = 1.26921 avg_loss = 3.74244\n",
      "epoch no.1 train no.161750  loss = 3.93205 avg_loss = 3.71513\n",
      "epoch no.1 train no.161760  loss = 3.22277 avg_loss = 3.71982\n",
      "epoch no.1 train no.161770  loss = 4.45097 avg_loss = 3.75077\n",
      "epoch no.1 train no.161780  loss = 1.47639 avg_loss = 3.68131\n",
      "epoch no.1 train no.161790  loss = 2.12809 avg_loss = 3.73340\n",
      "epoch no.1 train no.161800  loss = 3.38182 avg_loss = 3.72353\n",
      "epoch no.1 train no.161810  loss = 1.93031 avg_loss = 3.69394\n",
      "epoch no.1 train no.161820  loss = 4.17607 avg_loss = 3.70576\n",
      "epoch no.1 train no.161830  loss = 3.80347 avg_loss = 3.70181\n",
      "epoch no.1 train no.161840  loss = 4.39675 avg_loss = 3.73344\n",
      "epoch no.1 train no.161850  loss = 4.65641 avg_loss = 3.78087\n",
      "epoch no.1 train no.161860  loss = 4.35924 avg_loss = 3.73328\n",
      "epoch no.1 train no.161870  loss = 4.80793 avg_loss = 3.76342\n",
      "epoch no.1 train no.161880  loss = 5.38303 avg_loss = 3.77833\n",
      "epoch no.1 train no.161890  loss = 3.52213 avg_loss = 3.79304\n",
      "epoch no.1 train no.161900  loss = 7.86438 avg_loss = 3.79483\n",
      "epoch no.1 train no.161910  loss = 2.45394 avg_loss = 3.75337\n",
      "epoch no.1 train no.161920  loss = 2.28617 avg_loss = 3.75341\n",
      "epoch no.1 train no.161930  loss = 2.93386 avg_loss = 3.77721\n",
      "epoch no.1 train no.161940  loss = 6.82221 avg_loss = 3.78555\n",
      "epoch no.1 train no.161950  loss = 5.24173 avg_loss = 3.77439\n",
      "epoch no.1 train no.161960  loss = 4.43317 avg_loss = 3.75238\n",
      "epoch no.1 train no.161970  loss = 3.43662 avg_loss = 3.73070\n",
      "epoch no.1 train no.161980  loss = 3.44716 avg_loss = 3.79569\n",
      "epoch no.1 train no.161990  loss = 4.78317 avg_loss = 3.78735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.162000  loss = 3.97626 avg_loss = 3.81230\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.1 train no.162010  loss = 3.20162 avg_loss = 3.79231\n",
      "epoch no.1 train no.162020  loss = 3.15058 avg_loss = 3.77363\n",
      "epoch no.1 train no.162030  loss = 4.26946 avg_loss = 3.82468\n",
      "epoch no.1 train no.162040  loss = 6.04421 avg_loss = 3.79038\n",
      "epoch no.1 train no.162050  loss = 3.73128 avg_loss = 3.76213\n",
      "epoch no.1 train no.162060  loss = 4.09695 avg_loss = 3.73609\n",
      "epoch no.1 train no.162070  loss = 3.69102 avg_loss = 3.71526\n",
      "epoch no.1 train no.162080  loss = 4.44343 avg_loss = 3.79211\n",
      "epoch no.1 train no.162090  loss = 6.06008 avg_loss = 3.83447\n",
      "epoch no.1 train no.162100  loss = 5.14999 avg_loss = 3.83268\n",
      "epoch no.1 train no.162110  loss = 4.46322 avg_loss = 3.86171\n",
      "epoch no.1 train no.162120  loss = 2.86504 avg_loss = 3.89673\n",
      "epoch no.1 train no.162130  loss = 4.60375 avg_loss = 3.88400\n",
      "epoch no.1 train no.162140  loss = 4.94444 avg_loss = 3.90037\n",
      "epoch no.1 train no.162150  loss = 2.82589 avg_loss = 3.85288\n",
      "epoch no.1 train no.162160  loss = 6.08264 avg_loss = 3.86975\n",
      "epoch no.1 train no.162170  loss = 2.86097 avg_loss = 3.85436\n",
      "epoch no.1 train no.162180  loss = 5.32709 avg_loss = 3.86485\n",
      "epoch no.1 train no.162190  loss = 3.06408 avg_loss = 3.86736\n",
      "epoch no.1 train no.162200  loss = 3.72133 avg_loss = 3.85263\n",
      "epoch no.1 train no.162210  loss = 4.61631 avg_loss = 3.89990\n",
      "epoch no.1 train no.162220  loss = 3.09358 avg_loss = 3.84661\n",
      "epoch no.1 train no.162230  loss = 4.32795 avg_loss = 3.84962\n",
      "epoch no.1 train no.162240  loss = 7.36219 avg_loss = 3.84235\n",
      "epoch no.1 train no.162250  loss = 4.15920 avg_loss = 3.86561\n",
      "epoch no.1 train no.162260  loss = 2.71272 avg_loss = 3.88296\n",
      "epoch no.1 train no.162270  loss = 3.89473 avg_loss = 3.92429\n",
      "epoch no.1 train no.162280  loss = 4.24310 avg_loss = 3.91855\n",
      "epoch no.1 train no.162290  loss = 5.28861 avg_loss = 3.93863\n",
      "epoch no.1 train no.162300  loss = 4.92858 avg_loss = 3.93033\n",
      "epoch no.1 train no.162310  loss = 4.66589 avg_loss = 3.88202\n",
      "epoch no.1 train no.162320  loss = 2.63178 avg_loss = 3.84493\n",
      "epoch no.1 train no.162330  loss = 6.45626 avg_loss = 3.86712\n",
      "epoch no.1 train no.162340  loss = 4.55090 avg_loss = 3.86696\n",
      "epoch no.1 train no.162350  loss = 3.45125 avg_loss = 3.81159\n",
      "epoch no.1 train no.162360  loss = 4.45422 avg_loss = 3.79081\n",
      "epoch no.1 train no.162370  loss = 3.61696 avg_loss = 3.79916\n",
      "epoch no.1 train no.162380  loss = 2.86964 avg_loss = 3.76818\n",
      "epoch no.1 train no.162390  loss = 2.10064 avg_loss = 3.74018\n",
      "epoch no.1 train no.162400  loss = 2.61829 avg_loss = 3.75505\n",
      "epoch no.1 train no.162410  loss = 3.84273 avg_loss = 3.73072\n",
      "epoch no.1 train no.162420  loss = 4.21072 avg_loss = 3.70877\n",
      "epoch no.1 train no.162430  loss = 2.89733 avg_loss = 3.69892\n",
      "epoch no.1 train no.162440  loss = 3.92168 avg_loss = 3.70876\n",
      "epoch no.1 train no.162450  loss = 3.85263 avg_loss = 3.74064\n",
      "epoch no.1 train no.162460  loss = 1.99749 avg_loss = 3.74330\n",
      "epoch no.1 train no.162470  loss = 2.62138 avg_loss = 3.67663\n",
      "epoch no.1 train no.162480  loss = 5.80225 avg_loss = 3.70627\n",
      "epoch no.1 train no.162490  loss = 3.07136 avg_loss = 3.68324\n",
      "epoch no.1 train no.162500  loss = 3.92424 avg_loss = 3.64511\n",
      "epoch no.1 train no.162510  loss = 4.98345 avg_loss = 3.63166\n",
      "epoch no.1 train no.162520  loss = 4.53272 avg_loss = 3.65832\n",
      "epoch no.1 train no.162530  loss = 3.65754 avg_loss = 3.69788\n",
      "epoch no.1 train no.162540  loss = 2.71102 avg_loss = 3.71556\n",
      "epoch no.1 train no.162550  loss = 5.95533 avg_loss = 3.73732\n",
      "epoch no.1 train no.162560  loss = 3.01318 avg_loss = 3.66290\n",
      "epoch no.1 train no.162570  loss = 3.01904 avg_loss = 3.69339\n",
      "epoch no.1 train no.162580  loss = 3.09447 avg_loss = 3.76655\n",
      "epoch no.1 train no.162590  loss = 2.47004 avg_loss = 3.72098\n",
      "epoch no.1 train no.162600  loss = 3.58806 avg_loss = 3.74698\n",
      "epoch no.1 train no.162610  loss = 4.50650 avg_loss = 3.77397\n",
      "epoch no.1 train no.162620  loss = 4.70272 avg_loss = 3.79738\n",
      "epoch no.1 train no.162630  loss = 3.69639 avg_loss = 3.86772\n",
      "epoch no.1 train no.162640  loss = 4.11917 avg_loss = 3.89677\n",
      "epoch no.1 train no.162650  loss = 2.68552 avg_loss = 3.82601\n",
      "epoch no.1 train no.162660  loss = 5.37585 avg_loss = 3.74662\n",
      "epoch no.1 train no.162670  loss = 4.22339 avg_loss = 3.80637\n",
      "epoch no.1 train no.162680  loss = 2.97550 avg_loss = 3.80521\n",
      "epoch no.1 train no.162690  loss = 3.34214 avg_loss = 3.77113\n",
      "epoch no.1 train no.162700  loss = 4.51347 avg_loss = 3.78582\n",
      "epoch no.1 train no.162710  loss = 2.91181 avg_loss = 3.76621\n",
      "epoch no.1 train no.162720  loss = 3.01537 avg_loss = 3.81430\n",
      "epoch no.1 train no.162730  loss = 3.43224 avg_loss = 3.81975\n",
      "epoch no.1 train no.162740  loss = 4.40917 avg_loss = 3.85031\n",
      "epoch no.1 train no.162750  loss = 5.28444 avg_loss = 3.87478\n",
      "epoch no.1 train no.162760  loss = 3.05092 avg_loss = 3.86103\n",
      "epoch no.1 train no.162770  loss = 2.95961 avg_loss = 3.86537\n",
      "epoch no.1 train no.162780  loss = 4.50391 avg_loss = 3.88935\n",
      "epoch no.1 train no.162790  loss = 4.11239 avg_loss = 3.83721\n",
      "epoch no.1 train no.162800  loss = 2.68377 avg_loss = 3.76935\n",
      "epoch no.1 train no.162810  loss = 3.63011 avg_loss = 3.75798\n",
      "epoch no.1 train no.162820  loss = 2.98956 avg_loss = 3.69753\n",
      "epoch no.1 train no.162830  loss = 4.38520 avg_loss = 3.77020\n",
      "epoch no.1 train no.162840  loss = 5.76905 avg_loss = 3.76515\n",
      "epoch no.1 train no.162850  loss = 2.77320 avg_loss = 3.76746\n",
      "epoch no.1 train no.162860  loss = 2.31242 avg_loss = 3.70007\n",
      "epoch no.1 train no.162870  loss = 2.99540 avg_loss = 3.68183\n",
      "epoch no.1 train no.162880  loss = 5.18742 avg_loss = 3.70761\n",
      "epoch no.1 train no.162890  loss = 2.58038 avg_loss = 3.72472\n",
      "epoch no.1 train no.162900  loss = 5.20496 avg_loss = 3.73369\n",
      "epoch no.1 train no.162910  loss = 6.11025 avg_loss = 3.73232\n",
      "epoch no.1 train no.162920  loss = 2.67471 avg_loss = 3.72391\n",
      "epoch no.1 train no.162930  loss = 4.18746 avg_loss = 3.77125\n",
      "epoch no.1 train no.162940  loss = 5.61421 avg_loss = 3.75303\n",
      "epoch no.1 train no.162950  loss = 4.45778 avg_loss = 3.76353\n",
      "epoch no.1 train no.162960  loss = 2.70037 avg_loss = 3.67560\n",
      "epoch no.1 train no.162970  loss = 4.16170 avg_loss = 3.67845\n",
      "epoch no.1 train no.162980  loss = 5.02115 avg_loss = 3.72737\n",
      "epoch no.1 train no.162990  loss = 3.12811 avg_loss = 3.67349\n",
      "epoch no.1 train no.163000  loss = 4.82628 avg_loss = 3.71843\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '에', '▁팝', 'op', '</s>']\n",
      "기분전환 신나는 pop</s>\n",
      "epoch no.1 train no.163010  loss = 4.14081 avg_loss = 3.71614\n",
      "epoch no.1 train no.163020  loss = 3.82941 avg_loss = 3.78049\n",
      "epoch no.1 train no.163030  loss = 3.92247 avg_loss = 3.76445\n",
      "epoch no.1 train no.163040  loss = 3.04656 avg_loss = 3.80223\n",
      "epoch no.1 train no.163050  loss = 3.64920 avg_loss = 3.80995\n",
      "epoch no.1 train no.163060  loss = 3.20367 avg_loss = 3.76418\n",
      "epoch no.1 train no.163070  loss = 2.49170 avg_loss = 3.75847\n",
      "epoch no.1 train no.163080  loss = 3.93037 avg_loss = 3.79947\n",
      "epoch no.1 train no.163090  loss = 5.25729 avg_loss = 3.77797\n",
      "epoch no.1 train no.163100  loss = 4.45625 avg_loss = 3.81802\n",
      "epoch no.1 train no.163110  loss = 5.45364 avg_loss = 3.79214\n",
      "epoch no.1 train no.163120  loss = 2.61466 avg_loss = 3.80432\n",
      "epoch no.1 train no.163130  loss = 2.00237 avg_loss = 3.78478\n",
      "epoch no.1 train no.163140  loss = 6.09621 avg_loss = 3.76285\n",
      "epoch no.1 train no.163150  loss = 3.29768 avg_loss = 3.78035\n",
      "epoch no.1 train no.163160  loss = 5.04648 avg_loss = 3.83452\n",
      "epoch no.1 train no.163170  loss = 3.99120 avg_loss = 3.78178\n",
      "epoch no.1 train no.163180  loss = 3.26407 avg_loss = 3.82457\n",
      "epoch no.1 train no.163190  loss = 3.42816 avg_loss = 3.81141\n",
      "epoch no.1 train no.163200  loss = 5.02156 avg_loss = 3.79375\n",
      "epoch no.1 train no.163210  loss = 5.89981 avg_loss = 3.80647\n",
      "epoch no.1 train no.163220  loss = 2.13642 avg_loss = 3.74766\n",
      "epoch no.1 train no.163230  loss = 4.12622 avg_loss = 3.75039\n",
      "epoch no.1 train no.163240  loss = 4.40468 avg_loss = 3.75004\n",
      "epoch no.1 train no.163250  loss = 3.53730 avg_loss = 3.72824\n",
      "epoch no.1 train no.163260  loss = 7.57389 avg_loss = 3.77003\n",
      "epoch no.1 train no.163270  loss = 2.62600 avg_loss = 3.69897\n",
      "epoch no.1 train no.163280  loss = 3.90181 avg_loss = 3.70686\n",
      "epoch no.1 train no.163290  loss = 3.77945 avg_loss = 3.72297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.163300  loss = 5.01003 avg_loss = 3.73791\n",
      "epoch no.1 train no.163310  loss = 4.09151 avg_loss = 3.76584\n",
      "epoch no.1 train no.163320  loss = 3.01061 avg_loss = 3.76865\n",
      "epoch no.1 train no.163330  loss = 5.86271 avg_loss = 3.83200\n",
      "epoch no.1 train no.163340  loss = 3.24072 avg_loss = 3.74327\n",
      "epoch no.1 train no.163350  loss = 4.40751 avg_loss = 3.75877\n",
      "epoch no.1 train no.163360  loss = 6.56007 avg_loss = 3.83934\n",
      "epoch no.1 train no.163370  loss = 4.63002 avg_loss = 3.83588\n",
      "epoch no.1 train no.163380  loss = 3.76231 avg_loss = 3.82409\n",
      "epoch no.1 train no.163390  loss = 2.65364 avg_loss = 3.82385\n",
      "epoch no.1 train no.163400  loss = 3.61558 avg_loss = 3.79041\n",
      "epoch no.1 train no.163410  loss = 3.39983 avg_loss = 3.80778\n",
      "epoch no.1 train no.163420  loss = 2.36868 avg_loss = 3.75660\n",
      "epoch no.1 train no.163430  loss = 2.68342 avg_loss = 3.69748\n",
      "epoch no.1 train no.163440  loss = 5.05104 avg_loss = 3.75991\n",
      "epoch no.1 train no.163450  loss = 2.70270 avg_loss = 3.72470\n",
      "epoch no.1 train no.163460  loss = 4.31323 avg_loss = 3.79350\n",
      "epoch no.1 train no.163470  loss = 6.37255 avg_loss = 3.78016\n",
      "epoch no.1 train no.163480  loss = 2.07957 avg_loss = 3.76259\n",
      "epoch no.1 train no.163490  loss = 2.75507 avg_loss = 3.77219\n",
      "epoch no.1 train no.163500  loss = 3.10151 avg_loss = 3.77716\n",
      "epoch no.1 train no.163510  loss = 2.98901 avg_loss = 3.75045\n",
      "epoch no.1 train no.163520  loss = 3.46217 avg_loss = 3.75456\n",
      "epoch no.1 train no.163530  loss = 4.80703 avg_loss = 3.70811\n",
      "epoch no.1 train no.163540  loss = 2.17205 avg_loss = 3.66970\n",
      "epoch no.1 train no.163550  loss = 3.83873 avg_loss = 3.68099\n",
      "epoch no.1 train no.163560  loss = 2.65780 avg_loss = 3.68469\n",
      "epoch no.1 train no.163570  loss = 4.88429 avg_loss = 3.67488\n",
      "epoch no.1 train no.163580  loss = 4.55562 avg_loss = 3.74749\n",
      "epoch no.1 train no.163590  loss = 4.99941 avg_loss = 3.78478\n",
      "epoch no.1 train no.163600  loss = 3.29135 avg_loss = 3.75854\n",
      "epoch no.1 train no.163610  loss = 3.45212 avg_loss = 3.74677\n",
      "epoch no.1 train no.163620  loss = 5.74244 avg_loss = 3.81841\n",
      "epoch no.1 train no.163630  loss = 4.77991 avg_loss = 3.81111\n",
      "epoch no.1 train no.163640  loss = 3.74813 avg_loss = 3.81534\n",
      "epoch no.1 train no.163650  loss = 3.25570 avg_loss = 3.76854\n",
      "epoch no.1 train no.163660  loss = 2.55382 avg_loss = 3.71672\n",
      "epoch no.1 train no.163670  loss = 4.00855 avg_loss = 3.71718\n",
      "epoch no.1 train no.163680  loss = 4.45740 avg_loss = 3.69561\n",
      "epoch no.1 train no.163690  loss = 4.16117 avg_loss = 3.78453\n",
      "epoch no.1 train no.163700  loss = 4.22108 avg_loss = 3.78283\n",
      "epoch no.1 train no.163710  loss = 4.71934 avg_loss = 3.78882\n",
      "epoch no.1 train no.163720  loss = 3.97563 avg_loss = 3.75637\n",
      "epoch no.1 train no.163730  loss = 4.13146 avg_loss = 3.73962\n",
      "epoch no.1 train no.163740  loss = 4.42254 avg_loss = 3.76412\n",
      "epoch no.1 train no.163750  loss = 3.46562 avg_loss = 3.72716\n",
      "epoch no.1 train no.163760  loss = 2.13149 avg_loss = 3.66584\n",
      "epoch no.1 train no.163770  loss = 5.62494 avg_loss = 3.70788\n",
      "epoch no.1 train no.163780  loss = 3.43508 avg_loss = 3.71103\n",
      "epoch no.1 train no.163790  loss = 6.71315 avg_loss = 3.72654\n",
      "epoch no.1 train no.163800  loss = 2.33973 avg_loss = 3.65430\n",
      "epoch no.1 train no.163810  loss = 3.40702 avg_loss = 3.60943\n",
      "epoch no.1 train no.163820  loss = 6.48132 avg_loss = 3.65654\n",
      "epoch no.1 train no.163830  loss = 5.00196 avg_loss = 3.64131\n",
      "epoch no.1 train no.163840  loss = 3.33984 avg_loss = 3.61284\n",
      "epoch no.1 train no.163850  loss = 7.10250 avg_loss = 3.66596\n",
      "epoch no.1 train no.163860  loss = 2.87413 avg_loss = 3.69654\n",
      "epoch no.1 train no.163870  loss = 5.70750 avg_loss = 3.77339\n",
      "epoch no.1 train no.163880  loss = 4.70415 avg_loss = 3.81708\n",
      "epoch no.1 train no.163890  loss = 2.20120 avg_loss = 3.84622\n",
      "epoch no.1 train no.163900  loss = 2.72070 avg_loss = 3.79088\n",
      "epoch no.1 train no.163910  loss = 3.86724 avg_loss = 3.77544\n",
      "epoch no.1 train no.163920  loss = 3.73419 avg_loss = 3.77108\n",
      "epoch no.1 train no.163930  loss = 3.19632 avg_loss = 3.74820\n",
      "epoch no.1 train no.163940  loss = 5.42227 avg_loss = 3.74539\n",
      "epoch no.1 train no.163950  loss = 4.16515 avg_loss = 3.79622\n",
      "epoch no.1 train no.163960  loss = 2.92363 avg_loss = 3.76465\n",
      "epoch no.1 train no.163970  loss = 2.83557 avg_loss = 3.69893\n",
      "epoch no.1 train no.163980  loss = 3.88369 avg_loss = 3.67503\n",
      "epoch no.1 train no.163990  loss = 4.66390 avg_loss = 3.66446\n",
      "epoch no.1 train no.164000  loss = 3.41835 avg_loss = 3.65628\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁딱', '▁좋은', '▁노래', '들', '</s>']\n",
      "기분전환에 딱 좋은 곡들</s>\n",
      "epoch no.1 train no.164010  loss = 3.22298 avg_loss = 3.65076\n",
      "epoch no.1 train no.164020  loss = 2.81504 avg_loss = 3.66828\n",
      "epoch no.1 train no.164030  loss = 3.14309 avg_loss = 3.71218\n",
      "epoch no.1 train no.164040  loss = 2.77229 avg_loss = 3.65851\n",
      "epoch no.1 train no.164050  loss = 6.09320 avg_loss = 3.73880\n",
      "epoch no.1 train no.164060  loss = 2.82687 avg_loss = 3.79441\n",
      "epoch no.1 train no.164070  loss = 5.09300 avg_loss = 3.73880\n",
      "epoch no.1 train no.164080  loss = 3.78724 avg_loss = 3.76329\n",
      "epoch no.1 train no.164090  loss = 2.76923 avg_loss = 3.75638\n",
      "epoch no.1 train no.164100  loss = 5.43025 avg_loss = 3.79943\n",
      "epoch no.1 train no.164110  loss = 3.96660 avg_loss = 3.86373\n",
      "epoch no.1 train no.164120  loss = 4.08765 avg_loss = 3.92418\n",
      "epoch no.1 train no.164130  loss = 2.65082 avg_loss = 3.94718\n",
      "epoch no.1 train no.164140  loss = 4.10245 avg_loss = 3.93929\n",
      "epoch no.1 train no.164150  loss = 1.52666 avg_loss = 3.89120\n",
      "epoch no.1 train no.164160  loss = 2.50887 avg_loss = 3.83154\n",
      "epoch no.1 train no.164170  loss = 4.56884 avg_loss = 3.80984\n",
      "epoch no.1 train no.164180  loss = 2.82693 avg_loss = 3.83019\n",
      "epoch no.1 train no.164190  loss = 2.02842 avg_loss = 3.86626\n",
      "epoch no.1 train no.164200  loss = 2.72168 avg_loss = 3.78667\n",
      "epoch no.1 train no.164210  loss = 2.23376 avg_loss = 3.68686\n",
      "epoch no.1 train no.164220  loss = 7.12682 avg_loss = 3.71037\n",
      "epoch no.1 train no.164230  loss = 2.98929 avg_loss = 3.70144\n",
      "epoch no.1 train no.164240  loss = 2.97728 avg_loss = 3.68377\n",
      "epoch no.1 train no.164250  loss = 4.63720 avg_loss = 3.68643\n",
      "epoch no.1 train no.164260  loss = 3.92604 avg_loss = 3.75056\n",
      "epoch no.1 train no.164270  loss = 4.09909 avg_loss = 3.79066\n",
      "epoch no.1 train no.164280  loss = 3.29118 avg_loss = 3.81098\n",
      "epoch no.1 train no.164290  loss = 2.91821 avg_loss = 3.77692\n",
      "epoch no.1 train no.164300  loss = 3.45516 avg_loss = 3.73701\n",
      "epoch no.1 train no.164310  loss = 3.91796 avg_loss = 3.72766\n",
      "epoch no.1 train no.164320  loss = 4.42762 avg_loss = 3.77920\n",
      "epoch no.1 train no.164330  loss = 1.21016 avg_loss = 3.75782\n",
      "epoch no.1 train no.164340  loss = 3.91846 avg_loss = 3.79242\n",
      "epoch no.1 train no.164350  loss = 2.43023 avg_loss = 3.75403\n",
      "epoch no.1 train no.164360  loss = 2.12924 avg_loss = 3.69703\n",
      "epoch no.1 train no.164370  loss = 4.87010 avg_loss = 3.69989\n",
      "epoch no.1 train no.164380  loss = 2.72693 avg_loss = 3.64439\n",
      "epoch no.1 train no.164390  loss = 4.19505 avg_loss = 3.68677\n",
      "epoch no.1 train no.164400  loss = 2.63464 avg_loss = 3.70539\n",
      "epoch no.1 train no.164410  loss = 4.30726 avg_loss = 3.76457\n",
      "epoch no.1 train no.164420  loss = 5.55503 avg_loss = 3.83300\n",
      "epoch no.1 train no.164430  loss = 4.78758 avg_loss = 3.81375\n",
      "epoch no.1 train no.164440  loss = 4.16459 avg_loss = 3.82603\n",
      "epoch no.1 train no.164450  loss = 2.46093 avg_loss = 3.80300\n",
      "epoch no.1 train no.164460  loss = 4.66820 avg_loss = 3.79950\n",
      "epoch no.1 train no.164470  loss = 2.66763 avg_loss = 3.80268\n",
      "epoch no.1 train no.164480  loss = 4.55271 avg_loss = 3.76499\n",
      "epoch no.1 train no.164490  loss = 3.78412 avg_loss = 3.82992\n",
      "epoch no.1 train no.164500  loss = 3.72071 avg_loss = 3.87109\n",
      "epoch no.1 train no.164510  loss = 4.27653 avg_loss = 3.89030\n",
      "epoch no.1 train no.164520  loss = 3.91898 avg_loss = 3.89390\n",
      "epoch no.1 train no.164530  loss = 3.58352 avg_loss = 3.93149\n",
      "epoch no.1 train no.164540  loss = 3.50247 avg_loss = 3.95791\n",
      "epoch no.1 train no.164550  loss = 3.99406 avg_loss = 3.95623\n",
      "epoch no.1 train no.164560  loss = 6.17631 avg_loss = 3.97052\n",
      "epoch no.1 train no.164570  loss = 3.56555 avg_loss = 3.93181\n",
      "epoch no.1 train no.164580  loss = 4.45125 avg_loss = 3.88792\n",
      "epoch no.1 train no.164590  loss = 3.07917 avg_loss = 3.89832\n",
      "epoch no.1 train no.164600  loss = 2.36200 avg_loss = 3.89575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.164610  loss = 3.09685 avg_loss = 3.86502\n",
      "epoch no.1 train no.164620  loss = 2.53759 avg_loss = 3.81847\n",
      "epoch no.1 train no.164630  loss = 2.43809 avg_loss = 3.85563\n",
      "epoch no.1 train no.164640  loss = 5.15553 avg_loss = 3.92712\n",
      "epoch no.1 train no.164650  loss = 4.82048 avg_loss = 3.90779\n",
      "epoch no.1 train no.164660  loss = 5.27219 avg_loss = 3.90804\n",
      "epoch no.1 train no.164670  loss = 2.59910 avg_loss = 3.89394\n",
      "epoch no.1 train no.164680  loss = 3.21304 avg_loss = 3.86823\n",
      "epoch no.1 train no.164690  loss = 4.63738 avg_loss = 3.85280\n",
      "epoch no.1 train no.164700  loss = 3.98246 avg_loss = 3.87704\n",
      "epoch no.1 train no.164710  loss = 3.09594 avg_loss = 3.84228\n",
      "epoch no.1 train no.164720  loss = 4.28400 avg_loss = 3.85044\n",
      "epoch no.1 train no.164730  loss = 5.29985 avg_loss = 3.88582\n",
      "epoch no.1 train no.164740  loss = 1.60912 avg_loss = 3.87352\n",
      "epoch no.1 train no.164750  loss = 3.31502 avg_loss = 3.84411\n",
      "epoch no.1 train no.164760  loss = 3.84110 avg_loss = 3.83115\n",
      "epoch no.1 train no.164770  loss = 2.82122 avg_loss = 3.85627\n",
      "epoch no.1 train no.164780  loss = 2.16387 avg_loss = 3.82029\n",
      "epoch no.1 train no.164790  loss = 3.64019 avg_loss = 3.83163\n",
      "epoch no.1 train no.164800  loss = 4.50794 avg_loss = 3.82605\n",
      "epoch no.1 train no.164810  loss = 7.80063 avg_loss = 3.89033\n",
      "epoch no.1 train no.164820  loss = 3.26872 avg_loss = 3.89115\n",
      "epoch no.1 train no.164830  loss = 3.46797 avg_loss = 3.89109\n",
      "epoch no.1 train no.164840  loss = 2.34230 avg_loss = 3.88690\n",
      "epoch no.1 train no.164850  loss = 3.55770 avg_loss = 3.91909\n",
      "epoch no.1 train no.164860  loss = 6.05221 avg_loss = 3.93968\n",
      "epoch no.1 train no.164870  loss = 3.25710 avg_loss = 3.89035\n",
      "epoch no.1 train no.164880  loss = 2.97075 avg_loss = 3.83501\n",
      "epoch no.1 train no.164890  loss = 2.19604 avg_loss = 3.82958\n",
      "epoch no.1 train no.164900  loss = 4.20830 avg_loss = 3.83330\n",
      "epoch no.1 train no.164910  loss = 4.10380 avg_loss = 3.84767\n",
      "epoch no.1 train no.164920  loss = 2.91508 avg_loss = 3.81650\n",
      "epoch no.1 train no.164930  loss = 2.34071 avg_loss = 3.82408\n",
      "epoch no.1 train no.164940  loss = 3.81173 avg_loss = 3.77499\n",
      "epoch no.1 train no.164950  loss = 5.66143 avg_loss = 3.78932\n",
      "epoch no.1 train no.164960  loss = 2.53612 avg_loss = 3.77522\n",
      "epoch no.1 train no.164970  loss = 5.44306 avg_loss = 3.83280\n",
      "epoch no.1 train no.164980  loss = 5.20793 avg_loss = 3.86692\n",
      "epoch no.1 train no.164990  loss = 3.15589 avg_loss = 3.91309\n",
      "epoch no.1 train no.165000  loss = 4.14438 avg_loss = 3.85881\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁딱', '▁음악', '에이지', '</s>']\n",
      "기분전환에 좋은 뉴에이지</s>\n",
      "epoch no.1 train no.165010  loss = 3.37747 avg_loss = 3.90238\n",
      "epoch no.1 train no.165020  loss = 2.75916 avg_loss = 3.90094\n",
      "epoch no.1 train no.165030  loss = 2.21722 avg_loss = 3.84667\n",
      "epoch no.1 train no.165040  loss = 2.77976 avg_loss = 3.85067\n",
      "epoch no.1 train no.165050  loss = 2.99694 avg_loss = 3.80849\n",
      "epoch no.1 train no.165060  loss = 2.98282 avg_loss = 3.80976\n",
      "epoch no.1 train no.165070  loss = 5.28945 avg_loss = 3.82370\n",
      "epoch no.1 train no.165080  loss = 3.86914 avg_loss = 3.81752\n",
      "epoch no.1 train no.165090  loss = 5.39442 avg_loss = 3.78814\n",
      "epoch no.1 train no.165100  loss = 3.66235 avg_loss = 3.79577\n",
      "epoch no.1 train no.165110  loss = 4.51369 avg_loss = 3.75784\n",
      "epoch no.1 train no.165120  loss = 2.82394 avg_loss = 3.84936\n",
      "epoch no.1 train no.165130  loss = 5.81471 avg_loss = 3.88594\n",
      "epoch no.1 train no.165140  loss = 3.12046 avg_loss = 3.81086\n",
      "epoch no.1 train no.165150  loss = 6.88014 avg_loss = 3.84436\n",
      "epoch no.1 train no.165160  loss = 3.55281 avg_loss = 3.78600\n",
      "epoch no.1 train no.165170  loss = 5.42320 avg_loss = 3.79398\n",
      "epoch no.1 train no.165180  loss = 4.35517 avg_loss = 3.80402\n",
      "epoch no.1 train no.165190  loss = 2.97450 avg_loss = 3.74758\n",
      "epoch no.1 train no.165200  loss = 4.76902 avg_loss = 3.72407\n",
      "epoch no.1 train no.165210  loss = 4.40480 avg_loss = 3.78933\n",
      "epoch no.1 train no.165220  loss = 2.03330 avg_loss = 3.73608\n",
      "epoch no.1 train no.165230  loss = 4.41667 avg_loss = 3.76138\n",
      "epoch no.1 train no.165240  loss = 5.31237 avg_loss = 3.76965\n",
      "epoch no.1 train no.165250  loss = 3.59525 avg_loss = 3.73625\n",
      "epoch no.1 train no.165260  loss = 2.11922 avg_loss = 3.73287\n",
      "epoch no.1 train no.165270  loss = 3.82387 avg_loss = 3.73500\n",
      "epoch no.1 train no.165280  loss = 2.70090 avg_loss = 3.77885\n",
      "epoch no.1 train no.165290  loss = 4.31718 avg_loss = 3.79596\n",
      "epoch no.1 train no.165300  loss = 6.29391 avg_loss = 3.87184\n",
      "epoch no.1 train no.165310  loss = 2.78609 avg_loss = 3.85042\n",
      "epoch no.1 train no.165320  loss = 3.45849 avg_loss = 3.77240\n",
      "epoch no.1 train no.165330  loss = 3.94470 avg_loss = 3.77597\n",
      "epoch no.1 train no.165340  loss = 2.70807 avg_loss = 3.80781\n",
      "epoch no.1 train no.165350  loss = 1.89836 avg_loss = 3.78142\n",
      "epoch no.1 train no.165360  loss = 4.52046 avg_loss = 3.73995\n",
      "epoch no.1 train no.165370  loss = 1.93919 avg_loss = 3.75187\n",
      "epoch no.1 train no.165380  loss = 4.32888 avg_loss = 3.76191\n",
      "epoch no.1 train no.165390  loss = 3.41668 avg_loss = 3.80975\n",
      "epoch no.1 train no.165400  loss = 2.30246 avg_loss = 3.82976\n",
      "epoch no.1 train no.165410  loss = 3.39872 avg_loss = 3.77079\n",
      "epoch no.1 train no.165420  loss = 4.21558 avg_loss = 3.79118\n",
      "epoch no.1 train no.165430  loss = 4.40940 avg_loss = 3.81093\n",
      "epoch no.1 train no.165440  loss = 3.12847 avg_loss = 3.78334\n",
      "epoch no.1 train no.165450  loss = 3.00902 avg_loss = 3.75587\n",
      "epoch no.1 train no.165460  loss = 4.06704 avg_loss = 3.77950\n",
      "epoch no.1 train no.165470  loss = 4.23364 avg_loss = 3.78123\n",
      "epoch no.1 train no.165480  loss = 3.27238 avg_loss = 3.76198\n",
      "epoch no.1 train no.165490  loss = 2.93362 avg_loss = 3.75787\n",
      "epoch no.1 train no.165500  loss = 3.50507 avg_loss = 3.78795\n",
      "epoch no.1 train no.165510  loss = 4.37692 avg_loss = 3.75127\n",
      "epoch no.1 train no.165520  loss = 5.69232 avg_loss = 3.78246\n",
      "epoch no.1 train no.165530  loss = 3.34084 avg_loss = 3.80145\n",
      "epoch no.1 train no.165540  loss = 3.66700 avg_loss = 3.85007\n",
      "epoch no.1 train no.165550  loss = 3.27655 avg_loss = 3.79262\n",
      "epoch no.1 train no.165560  loss = 3.82998 avg_loss = 3.75965\n",
      "epoch no.1 train no.165570  loss = 4.16023 avg_loss = 3.78645\n",
      "epoch no.1 train no.165580  loss = 3.82636 avg_loss = 3.80707\n",
      "epoch no.1 train no.165590  loss = 2.92379 avg_loss = 3.83394\n",
      "epoch no.1 train no.165600  loss = 2.85233 avg_loss = 3.87732\n",
      "epoch no.1 train no.165610  loss = 4.52277 avg_loss = 3.90344\n",
      "epoch no.1 train no.165620  loss = 2.52251 avg_loss = 3.92706\n",
      "epoch no.1 train no.165630  loss = 4.42182 avg_loss = 3.91402\n",
      "epoch no.1 train no.165640  loss = 2.43800 avg_loss = 3.94535\n",
      "epoch no.1 train no.165650  loss = 3.86238 avg_loss = 3.95688\n",
      "epoch no.1 train no.165660  loss = 3.49615 avg_loss = 3.95557\n",
      "epoch no.1 train no.165670  loss = 2.96281 avg_loss = 3.95851\n",
      "epoch no.1 train no.165680  loss = 2.54544 avg_loss = 3.92797\n",
      "epoch no.1 train no.165690  loss = 5.29245 avg_loss = 3.96634\n",
      "epoch no.1 train no.165700  loss = 3.19098 avg_loss = 3.93673\n",
      "epoch no.1 train no.165710  loss = 4.35430 avg_loss = 3.93645\n",
      "epoch no.1 train no.165720  loss = 1.90418 avg_loss = 3.97715\n",
      "epoch no.1 train no.165730  loss = 1.90152 avg_loss = 3.97078\n",
      "epoch no.1 train no.165740  loss = 2.63289 avg_loss = 3.97395\n",
      "epoch no.1 train no.165750  loss = 3.66459 avg_loss = 3.95405\n",
      "epoch no.1 train no.165760  loss = 3.18246 avg_loss = 3.87036\n",
      "epoch no.1 train no.165770  loss = 4.72413 avg_loss = 3.89668\n",
      "epoch no.1 train no.165780  loss = 2.47286 avg_loss = 3.91077\n",
      "epoch no.1 train no.165790  loss = 3.56527 avg_loss = 3.90016\n",
      "epoch no.1 train no.165800  loss = 3.32796 avg_loss = 3.86824\n",
      "epoch no.1 train no.165810  loss = 5.15960 avg_loss = 3.82860\n",
      "epoch no.1 train no.165820  loss = 3.06669 avg_loss = 3.79635\n",
      "epoch no.1 train no.165830  loss = 4.00209 avg_loss = 3.85093\n",
      "epoch no.1 train no.165840  loss = 2.18062 avg_loss = 3.78453\n",
      "epoch no.1 train no.165850  loss = 3.57353 avg_loss = 3.76379\n",
      "epoch no.1 train no.165860  loss = 2.62443 avg_loss = 3.79140\n",
      "epoch no.1 train no.165870  loss = 3.50600 avg_loss = 3.82022\n",
      "epoch no.1 train no.165880  loss = 6.12549 avg_loss = 3.81117\n",
      "epoch no.1 train no.165890  loss = 4.31925 avg_loss = 3.80236\n",
      "epoch no.1 train no.165900  loss = 5.54628 avg_loss = 3.85566\n",
      "epoch no.1 train no.165910  loss = 5.97678 avg_loss = 3.84322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.165920  loss = 4.92097 avg_loss = 3.84736\n",
      "epoch no.1 train no.165930  loss = 5.59352 avg_loss = 3.89057\n",
      "epoch no.1 train no.165940  loss = 2.63625 avg_loss = 3.87212\n",
      "epoch no.1 train no.165950  loss = 3.68900 avg_loss = 3.97225\n",
      "epoch no.1 train no.165960  loss = 3.76893 avg_loss = 3.87357\n",
      "epoch no.1 train no.165970  loss = 3.39821 avg_loss = 3.82800\n",
      "epoch no.1 train no.165980  loss = 2.08308 avg_loss = 3.83439\n",
      "epoch no.1 train no.165990  loss = 3.39529 avg_loss = 3.74274\n",
      "epoch no.1 train no.166000  loss = 3.51990 avg_loss = 3.77230\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '에', '▁위한', '▁팝', '송']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.1 train no.166010  loss = 2.57184 avg_loss = 3.77754\n",
      "epoch no.1 train no.166020  loss = 2.26062 avg_loss = 3.76360\n",
      "epoch no.1 train no.166030  loss = 4.25989 avg_loss = 3.80270\n",
      "epoch no.1 train no.166040  loss = 3.26849 avg_loss = 3.76699\n",
      "epoch no.1 train no.166050  loss = 5.20123 avg_loss = 3.83188\n",
      "epoch no.1 train no.166060  loss = 3.98367 avg_loss = 3.78693\n",
      "epoch no.1 train no.166070  loss = 5.38728 avg_loss = 3.77708\n",
      "epoch no.1 train no.166080  loss = 3.24114 avg_loss = 3.75198\n",
      "epoch no.1 train no.166090  loss = 4.94796 avg_loss = 3.78832\n",
      "epoch no.1 train no.166100  loss = 4.28339 avg_loss = 3.79140\n",
      "epoch no.1 train no.166110  loss = 3.30151 avg_loss = 3.76609\n",
      "epoch no.1 train no.166120  loss = 5.05788 avg_loss = 3.72751\n",
      "epoch no.1 train no.166130  loss = 3.25821 avg_loss = 3.72011\n",
      "epoch no.1 train no.166140  loss = 4.16305 avg_loss = 3.73966\n",
      "epoch no.1 train no.166150  loss = 3.52780 avg_loss = 3.73762\n",
      "epoch no.1 train no.166160  loss = 4.60249 avg_loss = 3.72471\n",
      "epoch no.1 train no.166170  loss = 3.19409 avg_loss = 3.74147\n",
      "epoch no.1 train no.166180  loss = 3.58035 avg_loss = 3.75001\n",
      "epoch no.1 train no.166190  loss = 7.04835 avg_loss = 3.82390\n",
      "epoch no.1 train no.166200  loss = 2.33395 avg_loss = 3.80292\n",
      "epoch no.1 train no.166210  loss = 3.17058 avg_loss = 3.81592\n",
      "epoch no.1 train no.166220  loss = 4.22054 avg_loss = 3.82190\n",
      "epoch no.1 train no.166230  loss = 5.21959 avg_loss = 3.83673\n",
      "epoch no.1 train no.166240  loss = 3.25513 avg_loss = 3.81927\n",
      "epoch no.1 train no.166250  loss = 3.69181 avg_loss = 3.83526\n",
      "epoch no.1 train no.166260  loss = 2.87337 avg_loss = 3.84499\n",
      "epoch no.1 train no.166270  loss = 4.09617 avg_loss = 3.79644\n",
      "epoch no.1 train no.166280  loss = 2.69865 avg_loss = 3.79103\n",
      "epoch no.1 train no.166290  loss = 4.85641 avg_loss = 3.77640\n",
      "epoch no.1 train no.166300  loss = 3.44374 avg_loss = 3.75834\n",
      "epoch no.1 train no.166310  loss = 2.56426 avg_loss = 3.76884\n",
      "epoch no.1 train no.166320  loss = 2.98560 avg_loss = 3.73688\n",
      "epoch no.1 train no.166330  loss = 5.92329 avg_loss = 3.72780\n",
      "epoch no.1 train no.166340  loss = 8.75817 avg_loss = 3.79071\n",
      "epoch no.1 train no.166350  loss = 2.81734 avg_loss = 3.76137\n",
      "epoch no.1 train no.166360  loss = 2.06822 avg_loss = 3.79044\n",
      "epoch no.1 train no.166370  loss = 5.22526 avg_loss = 3.80359\n",
      "epoch no.1 train no.166380  loss = 2.39122 avg_loss = 3.79317\n",
      "epoch no.1 train no.166390  loss = 2.84635 avg_loss = 3.85421\n",
      "epoch no.1 train no.166400  loss = 4.00530 avg_loss = 3.87473\n",
      "epoch no.1 train no.166410  loss = 3.02328 avg_loss = 3.80112\n",
      "epoch no.1 train no.166420  loss = 4.39231 avg_loss = 3.79016\n",
      "epoch no.1 train no.166430  loss = 3.39019 avg_loss = 3.81720\n",
      "epoch no.1 train no.166440  loss = 4.22820 avg_loss = 3.78577\n",
      "epoch no.1 train no.166450  loss = 3.29107 avg_loss = 3.82678\n",
      "epoch no.1 train no.166460  loss = 2.47668 avg_loss = 3.76643\n",
      "epoch no.1 train no.166470  loss = 3.03119 avg_loss = 3.74410\n",
      "epoch no.1 train no.166480  loss = 2.36198 avg_loss = 3.76954\n",
      "epoch no.1 train no.166490  loss = 2.06456 avg_loss = 3.77453\n",
      "epoch no.1 train no.166500  loss = 2.43689 avg_loss = 3.73966\n",
      "epoch no.1 train no.166510  loss = 5.58169 avg_loss = 3.72857\n",
      "epoch no.1 train no.166520  loss = 3.25747 avg_loss = 3.69680\n",
      "epoch no.1 train no.166530  loss = 4.48176 avg_loss = 3.67765\n",
      "epoch no.1 train no.166540  loss = 5.66032 avg_loss = 3.68720\n",
      "epoch no.1 train no.166550  loss = 4.71798 avg_loss = 3.71569\n",
      "epoch no.1 train no.166560  loss = 4.47645 avg_loss = 3.70446\n",
      "epoch no.1 train no.166570  loss = 3.99791 avg_loss = 3.67142\n",
      "epoch no.1 train no.166580  loss = 3.90264 avg_loss = 3.75563\n",
      "epoch no.1 train no.166590  loss = 4.67739 avg_loss = 3.75483\n",
      "epoch no.1 train no.166600  loss = 2.76092 avg_loss = 3.75893\n",
      "epoch no.1 train no.166610  loss = 4.02220 avg_loss = 3.75116\n",
      "epoch no.1 train no.166620  loss = 3.24999 avg_loss = 3.83130\n",
      "epoch no.1 train no.166630  loss = 3.58178 avg_loss = 3.82742\n",
      "epoch no.1 train no.166640  loss = 3.00259 avg_loss = 3.82756\n",
      "epoch no.1 train no.166650  loss = 2.66109 avg_loss = 3.84104\n",
      "epoch no.1 train no.166660  loss = 4.54116 avg_loss = 3.83978\n",
      "epoch no.1 train no.166670  loss = 2.59507 avg_loss = 3.80585\n",
      "epoch no.1 train no.166680  loss = 3.15360 avg_loss = 3.81860\n",
      "epoch no.1 train no.166690  loss = 3.07838 avg_loss = 3.80456\n",
      "epoch no.1 train no.166700  loss = 4.18164 avg_loss = 3.89166\n",
      "epoch no.1 train no.166710  loss = 2.26458 avg_loss = 3.87255\n",
      "epoch no.1 train no.166720  loss = 2.66949 avg_loss = 3.76457\n",
      "epoch no.1 train no.166730  loss = 5.43301 avg_loss = 3.79394\n",
      "epoch no.1 train no.166740  loss = 3.51412 avg_loss = 3.81386\n",
      "epoch no.1 train no.166750  loss = 2.61166 avg_loss = 3.82262\n",
      "epoch no.1 train no.166760  loss = 4.88587 avg_loss = 3.80985\n",
      "epoch no.1 train no.166770  loss = 4.86709 avg_loss = 3.79680\n",
      "epoch no.1 train no.166780  loss = 3.60743 avg_loss = 3.77665\n",
      "epoch no.1 train no.166790  loss = 6.02991 avg_loss = 3.81529\n",
      "epoch no.1 train no.166800  loss = 2.42169 avg_loss = 3.80868\n",
      "epoch no.1 train no.166810  loss = 5.53584 avg_loss = 3.87302\n",
      "epoch no.1 train no.166820  loss = 3.75672 avg_loss = 3.86367\n",
      "epoch no.1 train no.166830  loss = 2.16780 avg_loss = 3.80633\n",
      "epoch no.1 train no.166840  loss = 4.29915 avg_loss = 3.74031\n",
      "epoch no.1 train no.166850  loss = 4.10194 avg_loss = 3.74462\n",
      "epoch no.1 train no.166860  loss = 2.70419 avg_loss = 3.75083\n",
      "epoch no.1 train no.166870  loss = 4.34636 avg_loss = 3.77449\n",
      "epoch no.1 train no.166880  loss = 2.17781 avg_loss = 3.76822\n",
      "epoch no.1 train no.166890  loss = 3.84249 avg_loss = 3.74074\n",
      "epoch no.1 train no.166900  loss = 4.70253 avg_loss = 3.71556\n",
      "epoch no.1 train no.166910  loss = 3.77149 avg_loss = 3.72918\n",
      "epoch no.1 train no.166920  loss = 4.13102 avg_loss = 3.71385\n",
      "epoch no.1 train no.166930  loss = 2.81338 avg_loss = 3.69943\n",
      "epoch no.1 train no.166940  loss = 1.25241 avg_loss = 3.73864\n",
      "epoch no.1 train no.166950  loss = 3.07545 avg_loss = 3.73888\n",
      "epoch no.1 train no.166960  loss = 3.90792 avg_loss = 3.71839\n",
      "epoch no.1 train no.166970  loss = 3.67108 avg_loss = 3.73442\n",
      "epoch no.1 train no.166980  loss = 4.75568 avg_loss = 3.69928\n",
      "epoch no.1 train no.166990  loss = 3.77598 avg_loss = 3.72424\n",
      "epoch no.1 train no.167000  loss = 4.80826 avg_loss = 3.72748\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '에', '▁필요할', '▁때', '▁듣는']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.1 train no.167010  loss = 4.09932 avg_loss = 3.70410\n",
      "epoch no.1 train no.167020  loss = 3.15008 avg_loss = 3.67608\n",
      "epoch no.1 train no.167030  loss = 3.01179 avg_loss = 3.69354\n",
      "epoch no.1 train no.167040  loss = 2.42981 avg_loss = 3.67665\n",
      "epoch no.1 train no.167050  loss = 3.12400 avg_loss = 3.73163\n",
      "epoch no.1 train no.167060  loss = 1.06580 avg_loss = 3.73392\n",
      "epoch no.1 train no.167070  loss = 5.46360 avg_loss = 3.80007\n",
      "epoch no.1 train no.167080  loss = 4.28277 avg_loss = 3.79460\n",
      "epoch no.1 train no.167090  loss = 4.40101 avg_loss = 3.76544\n",
      "epoch no.1 train no.167100  loss = 2.62450 avg_loss = 3.70554\n",
      "epoch no.1 train no.167110  loss = 3.79366 avg_loss = 3.72344\n",
      "epoch no.1 train no.167120  loss = 3.71521 avg_loss = 3.78890\n",
      "epoch no.1 train no.167130  loss = 4.42165 avg_loss = 3.88317\n",
      "epoch no.1 train no.167140  loss = 6.68044 avg_loss = 3.91037\n",
      "epoch no.1 train no.167150  loss = 3.02606 avg_loss = 3.85022\n",
      "epoch no.1 train no.167160  loss = 3.05169 avg_loss = 3.81725\n",
      "epoch no.1 train no.167170  loss = 3.84665 avg_loss = 3.82656\n",
      "epoch no.1 train no.167180  loss = 5.18092 avg_loss = 3.91094\n",
      "epoch no.1 train no.167190  loss = 2.52477 avg_loss = 3.91193\n",
      "epoch no.1 train no.167200  loss = 3.46179 avg_loss = 3.90752\n",
      "epoch no.1 train no.167210  loss = 2.02423 avg_loss = 3.95392\n",
      "epoch no.1 train no.167220  loss = 5.30381 avg_loss = 3.96544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.167230  loss = 2.66307 avg_loss = 3.97814\n",
      "epoch no.1 train no.167240  loss = 4.56602 avg_loss = 3.99350\n",
      "epoch no.1 train no.167250  loss = 4.43919 avg_loss = 4.01054\n",
      "epoch no.1 train no.167260  loss = 2.81515 avg_loss = 3.98274\n",
      "epoch no.1 train no.167270  loss = 2.59669 avg_loss = 3.99985\n",
      "epoch no.1 train no.167280  loss = 4.75234 avg_loss = 3.95632\n",
      "epoch no.1 train no.167290  loss = 2.35501 avg_loss = 3.92693\n",
      "epoch no.1 train no.167300  loss = 3.64885 avg_loss = 3.96688\n",
      "epoch no.1 train no.167310  loss = 3.24083 avg_loss = 3.95698\n",
      "epoch no.1 train no.167320  loss = 2.04921 avg_loss = 3.93434\n",
      "epoch no.1 train no.167330  loss = 4.16055 avg_loss = 3.90134\n",
      "epoch no.1 train no.167340  loss = 2.86569 avg_loss = 3.91368\n",
      "epoch no.1 train no.167350  loss = 3.18329 avg_loss = 3.89220\n",
      "epoch no.1 train no.167360  loss = 2.90431 avg_loss = 3.90492\n",
      "epoch no.1 train no.167370  loss = 2.71361 avg_loss = 3.83422\n",
      "epoch no.1 train no.167380  loss = 6.45677 avg_loss = 3.88907\n",
      "epoch no.1 train no.167390  loss = 4.13170 avg_loss = 3.89551\n",
      "epoch no.1 train no.167400  loss = 3.13300 avg_loss = 3.85756\n",
      "epoch no.1 train no.167410  loss = 4.64065 avg_loss = 3.86573\n",
      "epoch no.1 train no.167420  loss = 2.17324 avg_loss = 3.84712\n",
      "epoch no.1 train no.167430  loss = 2.41919 avg_loss = 3.82435\n",
      "epoch no.1 train no.167440  loss = 4.87929 avg_loss = 3.82555\n",
      "epoch no.1 train no.167450  loss = 3.17471 avg_loss = 3.76796\n",
      "epoch no.1 train no.167460  loss = 4.84495 avg_loss = 3.86159\n",
      "epoch no.1 train no.167470  loss = 3.35923 avg_loss = 3.80293\n",
      "epoch no.1 train no.167480  loss = 4.01502 avg_loss = 3.87248\n",
      "epoch no.1 train no.167490  loss = 3.80285 avg_loss = 3.82422\n",
      "epoch no.1 train no.167500  loss = 4.91550 avg_loss = 3.78716\n",
      "epoch no.1 train no.167510  loss = 2.17684 avg_loss = 3.75902\n",
      "epoch no.1 train no.167520  loss = 2.97787 avg_loss = 3.73273\n",
      "epoch no.1 train no.167530  loss = 3.41479 avg_loss = 3.76776\n",
      "epoch no.1 train no.167540  loss = 2.96591 avg_loss = 3.79076\n",
      "epoch no.1 train no.167550  loss = 2.16761 avg_loss = 3.75124\n",
      "epoch no.1 train no.167560  loss = 5.64303 avg_loss = 3.74170\n",
      "epoch no.1 train no.167570  loss = 4.29018 avg_loss = 3.75396\n",
      "epoch no.1 train no.167580  loss = 3.57777 avg_loss = 3.75681\n",
      "epoch no.1 train no.167590  loss = 2.81794 avg_loss = 3.79755\n",
      "epoch no.1 train no.167600  loss = 4.49985 avg_loss = 3.77344\n",
      "epoch no.1 train no.167610  loss = 4.69203 avg_loss = 3.74201\n",
      "epoch no.1 train no.167620  loss = 3.94881 avg_loss = 3.72114\n",
      "epoch no.1 train no.167630  loss = 2.49705 avg_loss = 3.74487\n",
      "epoch no.1 train no.167640  loss = 4.57200 avg_loss = 3.78486\n",
      "epoch no.1 train no.167650  loss = 3.96804 avg_loss = 3.77385\n",
      "epoch no.1 train no.167660  loss = 3.23910 avg_loss = 3.74868\n",
      "epoch no.1 train no.167670  loss = 3.44735 avg_loss = 3.75688\n",
      "epoch no.1 train no.167680  loss = 2.92616 avg_loss = 3.75191\n",
      "epoch no.1 train no.167690  loss = 4.29356 avg_loss = 3.72402\n",
      "epoch no.1 train no.167700  loss = 2.69165 avg_loss = 3.75209\n",
      "epoch no.1 train no.167710  loss = 3.13355 avg_loss = 3.71900\n",
      "epoch no.1 train no.167720  loss = 2.81186 avg_loss = 3.76924\n",
      "epoch no.1 train no.167730  loss = 3.09812 avg_loss = 3.71218\n",
      "epoch no.1 train no.167740  loss = 1.67211 avg_loss = 3.67286\n",
      "epoch no.1 train no.167750  loss = 2.65066 avg_loss = 3.68417\n",
      "epoch no.1 train no.167760  loss = 5.30632 avg_loss = 3.73769\n",
      "epoch no.1 train no.167770  loss = 5.16111 avg_loss = 3.73670\n",
      "epoch no.1 train no.167780  loss = 3.64144 avg_loss = 3.81366\n",
      "epoch no.1 train no.167790  loss = 4.09583 avg_loss = 3.85732\n",
      "epoch no.1 train no.167800  loss = 3.95880 avg_loss = 3.88669\n",
      "epoch no.2 train no.167810  loss = 3.04236 avg_loss = 3.92553\n",
      "epoch no.2 train no.167820  loss = 4.22142 avg_loss = 3.88446\n",
      "epoch no.2 train no.167830  loss = 4.19065 avg_loss = 3.84566\n",
      "epoch no.2 train no.167840  loss = 4.16984 avg_loss = 3.84301\n",
      "epoch no.2 train no.167850  loss = 1.90225 avg_loss = 3.79662\n",
      "epoch no.2 train no.167860  loss = 3.30103 avg_loss = 3.77208\n",
      "epoch no.2 train no.167870  loss = 2.83163 avg_loss = 3.72925\n",
      "epoch no.2 train no.167880  loss = 2.22166 avg_loss = 3.64381\n",
      "epoch no.2 train no.167890  loss = 5.07971 avg_loss = 3.62740\n",
      "epoch no.2 train no.167900  loss = 4.79895 avg_loss = 3.61723\n",
      "epoch no.2 train no.167910  loss = 3.34969 avg_loss = 3.58133\n",
      "epoch no.2 train no.167920  loss = 3.47097 avg_loss = 3.62076\n",
      "epoch no.2 train no.167930  loss = 3.92581 avg_loss = 3.54509\n",
      "epoch no.2 train no.167940  loss = 3.38974 avg_loss = 3.57723\n",
      "epoch no.2 train no.167950  loss = 3.75594 avg_loss = 3.56565\n",
      "epoch no.2 train no.167960  loss = 5.34330 avg_loss = 3.60619\n",
      "epoch no.2 train no.167970  loss = 4.42725 avg_loss = 3.57823\n",
      "epoch no.2 train no.167980  loss = 3.88826 avg_loss = 3.61143\n",
      "epoch no.2 train no.167990  loss = 4.79454 avg_loss = 3.59708\n",
      "epoch no.2 train no.168000  loss = 1.53570 avg_loss = 3.57056\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '▁신나는', '▁딱', '▁신나는', '▁노래', '송']\n",
      "기분전환에 딱 좋은 팝</s>\n",
      "epoch no.2 train no.168010  loss = 2.77965 avg_loss = 3.55516\n",
      "epoch no.2 train no.168020  loss = 1.82115 avg_loss = 3.51386\n",
      "epoch no.2 train no.168030  loss = 4.88245 avg_loss = 3.49887\n",
      "epoch no.2 train no.168040  loss = 2.18013 avg_loss = 3.49974\n",
      "epoch no.2 train no.168050  loss = 2.21500 avg_loss = 3.52621\n",
      "epoch no.2 train no.168060  loss = 3.81123 avg_loss = 3.55499\n",
      "epoch no.2 train no.168070  loss = 3.56654 avg_loss = 3.56294\n",
      "epoch no.2 train no.168080  loss = 1.66316 avg_loss = 3.55833\n",
      "epoch no.2 train no.168090  loss = 4.62578 avg_loss = 3.54195\n",
      "epoch no.2 train no.168100  loss = 5.32183 avg_loss = 3.56841\n",
      "epoch no.2 train no.168110  loss = 3.47801 avg_loss = 3.55144\n",
      "epoch no.2 train no.168120  loss = 2.98042 avg_loss = 3.53757\n",
      "epoch no.2 train no.168130  loss = 3.68757 avg_loss = 3.57884\n",
      "epoch no.2 train no.168140  loss = 3.32163 avg_loss = 3.56751\n",
      "epoch no.2 train no.168150  loss = 2.13162 avg_loss = 3.54910\n",
      "epoch no.2 train no.168160  loss = 3.10092 avg_loss = 3.49224\n",
      "epoch no.2 train no.168170  loss = 2.16271 avg_loss = 3.48861\n",
      "epoch no.2 train no.168180  loss = 5.46746 avg_loss = 3.50739\n",
      "epoch no.2 train no.168190  loss = 4.20703 avg_loss = 3.49406\n",
      "epoch no.2 train no.168200  loss = 4.91303 avg_loss = 3.49774\n",
      "epoch no.2 train no.168210  loss = 3.90807 avg_loss = 3.51592\n",
      "epoch no.2 train no.168220  loss = 3.16377 avg_loss = 3.52135\n",
      "epoch no.2 train no.168230  loss = 3.60840 avg_loss = 3.55749\n",
      "epoch no.2 train no.168240  loss = 2.44230 avg_loss = 3.57340\n",
      "epoch no.2 train no.168250  loss = 2.40237 avg_loss = 3.56790\n",
      "epoch no.2 train no.168260  loss = 3.63543 avg_loss = 3.54657\n",
      "epoch no.2 train no.168270  loss = 5.39371 avg_loss = 3.54682\n",
      "epoch no.2 train no.168280  loss = 2.58623 avg_loss = 3.51248\n",
      "epoch no.2 train no.168290  loss = 4.22656 avg_loss = 3.53097\n",
      "epoch no.2 train no.168300  loss = 3.52218 avg_loss = 3.52004\n",
      "epoch no.2 train no.168310  loss = 3.93973 avg_loss = 3.50806\n",
      "epoch no.2 train no.168320  loss = 2.56816 avg_loss = 3.47030\n",
      "epoch no.2 train no.168330  loss = 2.95800 avg_loss = 3.45312\n",
      "epoch no.2 train no.168340  loss = 4.21519 avg_loss = 3.42265\n",
      "epoch no.2 train no.168350  loss = 2.71997 avg_loss = 3.41360\n",
      "epoch no.2 train no.168360  loss = 2.90073 avg_loss = 3.49562\n",
      "epoch no.2 train no.168370  loss = 2.55827 avg_loss = 3.48173\n",
      "epoch no.2 train no.168380  loss = 6.33309 avg_loss = 3.54978\n",
      "epoch no.2 train no.168390  loss = 2.27634 avg_loss = 3.58424\n",
      "epoch no.2 train no.168400  loss = 2.82014 avg_loss = 3.53167\n",
      "epoch no.2 train no.168410  loss = 3.05105 avg_loss = 3.57966\n",
      "epoch no.2 train no.168420  loss = 1.23219 avg_loss = 3.52879\n",
      "epoch no.2 train no.168430  loss = 4.73527 avg_loss = 3.51815\n",
      "epoch no.2 train no.168440  loss = 2.54878 avg_loss = 3.51675\n",
      "epoch no.2 train no.168450  loss = 2.05505 avg_loss = 3.50180\n",
      "epoch no.2 train no.168460  loss = 4.19566 avg_loss = 3.51051\n",
      "epoch no.2 train no.168470  loss = 3.82332 avg_loss = 3.51269\n",
      "epoch no.2 train no.168480  loss = 2.29681 avg_loss = 3.50240\n",
      "epoch no.2 train no.168490  loss = 2.47235 avg_loss = 3.51144\n",
      "epoch no.2 train no.168500  loss = 3.52878 avg_loss = 3.51851\n",
      "epoch no.2 train no.168510  loss = 3.81977 avg_loss = 3.53376\n",
      "epoch no.2 train no.168520  loss = 2.30000 avg_loss = 3.47268\n",
      "epoch no.2 train no.168530  loss = 4.92325 avg_loss = 3.47095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.168540  loss = 3.64680 avg_loss = 3.44479\n",
      "epoch no.2 train no.168550  loss = 5.89866 avg_loss = 3.43786\n",
      "epoch no.2 train no.168560  loss = 4.31769 avg_loss = 3.46738\n",
      "epoch no.2 train no.168570  loss = 2.39112 avg_loss = 3.42475\n",
      "epoch no.2 train no.168580  loss = 3.04787 avg_loss = 3.48751\n",
      "epoch no.2 train no.168590  loss = 2.86631 avg_loss = 3.45293\n",
      "epoch no.2 train no.168600  loss = 2.62869 avg_loss = 3.47106\n",
      "epoch no.2 train no.168610  loss = 3.61348 avg_loss = 3.50393\n",
      "epoch no.2 train no.168620  loss = 8.29845 avg_loss = 3.53913\n",
      "epoch no.2 train no.168630  loss = 4.28836 avg_loss = 3.56802\n",
      "epoch no.2 train no.168640  loss = 3.93203 avg_loss = 3.51427\n",
      "epoch no.2 train no.168650  loss = 4.07238 avg_loss = 3.49375\n",
      "epoch no.2 train no.168660  loss = 3.78369 avg_loss = 3.45886\n",
      "epoch no.2 train no.168670  loss = 2.18878 avg_loss = 3.48026\n",
      "epoch no.2 train no.168680  loss = 2.64796 avg_loss = 3.50638\n",
      "epoch no.2 train no.168690  loss = 2.68803 avg_loss = 3.49478\n",
      "epoch no.2 train no.168700  loss = 3.42207 avg_loss = 3.47135\n",
      "epoch no.2 train no.168710  loss = 3.53615 avg_loss = 3.44944\n",
      "epoch no.2 train no.168720  loss = 2.43484 avg_loss = 3.45513\n",
      "epoch no.2 train no.168730  loss = 2.08776 avg_loss = 3.46201\n",
      "epoch no.2 train no.168740  loss = 5.71816 avg_loss = 3.50243\n",
      "epoch no.2 train no.168750  loss = 2.60803 avg_loss = 3.46838\n",
      "epoch no.2 train no.168760  loss = 4.03656 avg_loss = 3.46121\n",
      "epoch no.2 train no.168770  loss = 4.17858 avg_loss = 3.48622\n",
      "epoch no.2 train no.168780  loss = 1.95702 avg_loss = 3.50766\n",
      "epoch no.2 train no.168790  loss = 2.83771 avg_loss = 3.53114\n",
      "epoch no.2 train no.168800  loss = 3.88460 avg_loss = 3.53758\n",
      "epoch no.2 train no.168810  loss = 5.08439 avg_loss = 3.55812\n",
      "epoch no.2 train no.168820  loss = 1.32662 avg_loss = 3.55403\n",
      "epoch no.2 train no.168830  loss = 3.44561 avg_loss = 3.53623\n",
      "epoch no.2 train no.168840  loss = 3.17670 avg_loss = 3.56853\n",
      "epoch no.2 train no.168850  loss = 2.36026 avg_loss = 3.57356\n",
      "epoch no.2 train no.168860  loss = 4.01502 avg_loss = 3.59282\n",
      "epoch no.2 train no.168870  loss = 2.93952 avg_loss = 3.59463\n",
      "epoch no.2 train no.168880  loss = 4.09446 avg_loss = 3.57256\n",
      "epoch no.2 train no.168890  loss = 2.45844 avg_loss = 3.54296\n",
      "epoch no.2 train no.168900  loss = 3.98626 avg_loss = 3.59485\n",
      "epoch no.2 train no.168910  loss = 3.11527 avg_loss = 3.55158\n",
      "epoch no.2 train no.168920  loss = 4.10449 avg_loss = 3.55079\n",
      "epoch no.2 train no.168930  loss = 3.81441 avg_loss = 3.56003\n",
      "epoch no.2 train no.168940  loss = 5.77237 avg_loss = 3.57350\n",
      "epoch no.2 train no.168950  loss = 3.83632 avg_loss = 3.57568\n",
      "epoch no.2 train no.168960  loss = 3.61678 avg_loss = 3.61360\n",
      "epoch no.2 train no.168970  loss = 4.37887 avg_loss = 3.62301\n",
      "epoch no.2 train no.168980  loss = 2.02816 avg_loss = 3.63762\n",
      "epoch no.2 train no.168990  loss = 2.85841 avg_loss = 3.60012\n",
      "epoch no.2 train no.169000  loss = 3.13602 avg_loss = 3.56096\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '에', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.2 train no.169010  loss = 3.15217 avg_loss = 3.52011\n",
      "epoch no.2 train no.169020  loss = 2.81444 avg_loss = 3.52517\n",
      "epoch no.2 train no.169030  loss = 2.53672 avg_loss = 3.48772\n",
      "epoch no.2 train no.169040  loss = 3.63044 avg_loss = 3.48719\n",
      "epoch no.2 train no.169050  loss = 3.15475 avg_loss = 3.48187\n",
      "epoch no.2 train no.169060  loss = 2.11021 avg_loss = 3.49836\n",
      "epoch no.2 train no.169070  loss = 3.57464 avg_loss = 3.47806\n",
      "epoch no.2 train no.169080  loss = 3.37659 avg_loss = 3.50975\n",
      "epoch no.2 train no.169090  loss = 2.59008 avg_loss = 3.50634\n",
      "epoch no.2 train no.169100  loss = 4.67656 avg_loss = 3.52952\n",
      "epoch no.2 train no.169110  loss = 3.94926 avg_loss = 3.51322\n",
      "epoch no.2 train no.169120  loss = 3.25902 avg_loss = 3.58292\n",
      "epoch no.2 train no.169130  loss = 2.88878 avg_loss = 3.61020\n",
      "epoch no.2 train no.169140  loss = 4.62074 avg_loss = 3.56619\n",
      "epoch no.2 train no.169150  loss = 3.60425 avg_loss = 3.54029\n",
      "epoch no.2 train no.169160  loss = 2.13076 avg_loss = 3.51679\n",
      "epoch no.2 train no.169170  loss = 3.15479 avg_loss = 3.52949\n",
      "epoch no.2 train no.169180  loss = 3.62696 avg_loss = 3.49119\n",
      "epoch no.2 train no.169190  loss = 3.66554 avg_loss = 3.46077\n",
      "epoch no.2 train no.169200  loss = 2.07842 avg_loss = 3.42050\n",
      "epoch no.2 train no.169210  loss = 2.15193 avg_loss = 3.41150\n",
      "epoch no.2 train no.169220  loss = 5.49399 avg_loss = 3.44799\n",
      "epoch no.2 train no.169230  loss = 3.65191 avg_loss = 3.43373\n",
      "epoch no.2 train no.169240  loss = 2.42242 avg_loss = 3.48255\n",
      "epoch no.2 train no.169250  loss = 3.48476 avg_loss = 3.50429\n",
      "epoch no.2 train no.169260  loss = 3.34389 avg_loss = 3.46551\n",
      "epoch no.2 train no.169270  loss = 4.19752 avg_loss = 3.47776\n",
      "epoch no.2 train no.169280  loss = 3.24734 avg_loss = 3.54092\n",
      "epoch no.2 train no.169290  loss = 3.26198 avg_loss = 3.58052\n",
      "epoch no.2 train no.169300  loss = 4.64752 avg_loss = 3.58538\n",
      "epoch no.2 train no.169310  loss = 3.59486 avg_loss = 3.55606\n",
      "epoch no.2 train no.169320  loss = 4.13829 avg_loss = 3.55305\n",
      "epoch no.2 train no.169330  loss = 3.39631 avg_loss = 3.54388\n",
      "epoch no.2 train no.169340  loss = 3.85741 avg_loss = 3.54273\n",
      "epoch no.2 train no.169350  loss = 4.39106 avg_loss = 3.56811\n",
      "epoch no.2 train no.169360  loss = 5.36636 avg_loss = 3.54674\n",
      "epoch no.2 train no.169370  loss = 4.86973 avg_loss = 3.59248\n",
      "epoch no.2 train no.169380  loss = 3.37603 avg_loss = 3.66456\n",
      "epoch no.2 train no.169390  loss = 2.62977 avg_loss = 3.63033\n",
      "epoch no.2 train no.169400  loss = 2.37704 avg_loss = 3.63312\n",
      "epoch no.2 train no.169410  loss = 3.60864 avg_loss = 3.62319\n",
      "epoch no.2 train no.169420  loss = 3.57141 avg_loss = 3.56858\n",
      "epoch no.2 train no.169430  loss = 2.19887 avg_loss = 3.53962\n",
      "epoch no.2 train no.169440  loss = 6.07101 avg_loss = 3.54373\n",
      "epoch no.2 train no.169450  loss = 3.40434 avg_loss = 3.55089\n",
      "epoch no.2 train no.169460  loss = 2.99654 avg_loss = 3.52323\n",
      "epoch no.2 train no.169470  loss = 3.03682 avg_loss = 3.52949\n",
      "epoch no.2 train no.169480  loss = 2.73218 avg_loss = 3.52242\n",
      "epoch no.2 train no.169490  loss = 3.75674 avg_loss = 3.52500\n",
      "epoch no.2 train no.169500  loss = 2.21316 avg_loss = 3.53620\n",
      "epoch no.2 train no.169510  loss = 4.88175 avg_loss = 3.48978\n",
      "epoch no.2 train no.169520  loss = 3.74027 avg_loss = 3.44421\n",
      "epoch no.2 train no.169530  loss = 3.09966 avg_loss = 3.37607\n",
      "epoch no.2 train no.169540  loss = 4.44344 avg_loss = 3.36772\n",
      "epoch no.2 train no.169550  loss = 2.46603 avg_loss = 3.41040\n",
      "epoch no.2 train no.169560  loss = 2.95507 avg_loss = 3.39105\n",
      "epoch no.2 train no.169570  loss = 2.99990 avg_loss = 3.38605\n",
      "epoch no.2 train no.169580  loss = 3.84517 avg_loss = 3.46847\n",
      "epoch no.2 train no.169590  loss = 3.26336 avg_loss = 3.46149\n",
      "epoch no.2 train no.169600  loss = 3.46307 avg_loss = 3.44176\n",
      "epoch no.2 train no.169610  loss = 3.71653 avg_loss = 3.45835\n",
      "epoch no.2 train no.169620  loss = 2.59417 avg_loss = 3.44190\n",
      "epoch no.2 train no.169630  loss = 3.62304 avg_loss = 3.45554\n",
      "epoch no.2 train no.169640  loss = 5.53462 avg_loss = 3.45863\n",
      "epoch no.2 train no.169650  loss = 3.67656 avg_loss = 3.47391\n",
      "epoch no.2 train no.169660  loss = 4.39408 avg_loss = 3.48179\n",
      "epoch no.2 train no.169670  loss = 2.54637 avg_loss = 3.48423\n",
      "epoch no.2 train no.169680  loss = 2.02532 avg_loss = 3.46022\n",
      "epoch no.2 train no.169690  loss = 2.45249 avg_loss = 3.48741\n",
      "epoch no.2 train no.169700  loss = 2.36774 avg_loss = 3.45676\n",
      "epoch no.2 train no.169710  loss = 3.31389 avg_loss = 3.44389\n",
      "epoch no.2 train no.169720  loss = 2.98023 avg_loss = 3.42096\n",
      "epoch no.2 train no.169730  loss = 4.01409 avg_loss = 3.47248\n",
      "epoch no.2 train no.169740  loss = 2.53962 avg_loss = 3.44358\n",
      "epoch no.2 train no.169750  loss = 3.16440 avg_loss = 3.43799\n",
      "epoch no.2 train no.169760  loss = 2.74874 avg_loss = 3.43470\n",
      "epoch no.2 train no.169770  loss = 3.37062 avg_loss = 3.50459\n",
      "epoch no.2 train no.169780  loss = 4.32410 avg_loss = 3.48861\n",
      "epoch no.2 train no.169790  loss = 2.94608 avg_loss = 3.47477\n",
      "epoch no.2 train no.169800  loss = 3.82409 avg_loss = 3.54739\n",
      "epoch no.2 train no.169810  loss = 4.05699 avg_loss = 3.54202\n",
      "epoch no.2 train no.169820  loss = 2.45448 avg_loss = 3.50122\n",
      "epoch no.2 train no.169830  loss = 8.44237 avg_loss = 3.55543\n",
      "epoch no.2 train no.169840  loss = 5.76336 avg_loss = 3.56560\n",
      "epoch no.2 train no.169850  loss = 3.47214 avg_loss = 3.56163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.169860  loss = 3.44197 avg_loss = 3.49250\n",
      "epoch no.2 train no.169870  loss = 2.19286 avg_loss = 3.43367\n",
      "epoch no.2 train no.169880  loss = 5.24353 avg_loss = 3.41223\n",
      "epoch no.2 train no.169890  loss = 3.30118 avg_loss = 3.37806\n",
      "epoch no.2 train no.169900  loss = 2.20760 avg_loss = 3.40105\n",
      "epoch no.2 train no.169910  loss = 4.26565 avg_loss = 3.43759\n",
      "epoch no.2 train no.169920  loss = 2.92297 avg_loss = 3.45924\n",
      "epoch no.2 train no.169930  loss = 2.95847 avg_loss = 3.50025\n",
      "epoch no.2 train no.169940  loss = 3.64610 avg_loss = 3.48361\n",
      "epoch no.2 train no.169950  loss = 3.02105 avg_loss = 3.52596\n",
      "epoch no.2 train no.169960  loss = 2.57845 avg_loss = 3.60404\n",
      "epoch no.2 train no.169970  loss = 4.31336 avg_loss = 3.59658\n",
      "epoch no.2 train no.169980  loss = 4.10082 avg_loss = 3.61278\n",
      "epoch no.2 train no.169990  loss = 1.11927 avg_loss = 3.61060\n",
      "epoch no.2 train no.170000  loss = 5.75887 avg_loss = 3.65064\n",
      "2\n",
      "to_tokens: ['▁비', '전환', '에', '▁딱', '▁좋은']\n",
      "기분전환에 딱</s>\n",
      "epoch no.2 train no.170010  loss = 3.38225 avg_loss = 3.62927\n",
      "epoch no.2 train no.170020  loss = 4.55915 avg_loss = 3.63077\n",
      "epoch no.2 train no.170030  loss = 5.21762 avg_loss = 3.67422\n",
      "epoch no.2 train no.170040  loss = 4.67979 avg_loss = 3.64610\n",
      "epoch no.2 train no.170050  loss = 2.12524 avg_loss = 3.63673\n",
      "epoch no.2 train no.170060  loss = 3.51593 avg_loss = 3.63999\n",
      "epoch no.2 train no.170070  loss = 4.03225 avg_loss = 3.65003\n",
      "epoch no.2 train no.170080  loss = 3.21091 avg_loss = 3.60349\n",
      "epoch no.2 train no.170090  loss = 3.45609 avg_loss = 3.64868\n",
      "epoch no.2 train no.170100  loss = 3.14990 avg_loss = 3.58482\n",
      "epoch no.2 train no.170110  loss = 3.24390 avg_loss = 3.58868\n",
      "epoch no.2 train no.170120  loss = 2.61846 avg_loss = 3.56876\n",
      "epoch no.2 train no.170130  loss = 2.28352 avg_loss = 3.59466\n",
      "epoch no.2 train no.170140  loss = 2.39445 avg_loss = 3.61308\n",
      "epoch no.2 train no.170150  loss = 3.20346 avg_loss = 3.60967\n",
      "epoch no.2 train no.170160  loss = 3.76867 avg_loss = 3.58247\n",
      "epoch no.2 train no.170170  loss = 2.64973 avg_loss = 3.55173\n",
      "epoch no.2 train no.170180  loss = 2.94924 avg_loss = 3.53153\n",
      "epoch no.2 train no.170190  loss = 2.20968 avg_loss = 3.52775\n",
      "epoch no.2 train no.170200  loss = 4.33955 avg_loss = 3.53364\n",
      "epoch no.2 train no.170210  loss = 3.65173 avg_loss = 3.60882\n",
      "epoch no.2 train no.170220  loss = 3.90711 avg_loss = 3.66947\n",
      "epoch no.2 train no.170230  loss = 4.46121 avg_loss = 3.71996\n",
      "epoch no.2 train no.170240  loss = 5.68990 avg_loss = 3.72869\n",
      "epoch no.2 train no.170250  loss = 4.60325 avg_loss = 3.71083\n",
      "epoch no.2 train no.170260  loss = 4.77528 avg_loss = 3.72003\n",
      "epoch no.2 train no.170270  loss = 2.46319 avg_loss = 3.69920\n",
      "epoch no.2 train no.170280  loss = 3.09333 avg_loss = 3.65195\n",
      "epoch no.2 train no.170290  loss = 3.99937 avg_loss = 3.66439\n",
      "epoch no.2 train no.170300  loss = 3.53532 avg_loss = 3.66350\n",
      "epoch no.2 train no.170310  loss = 3.73847 avg_loss = 3.63826\n",
      "epoch no.2 train no.170320  loss = 1.64782 avg_loss = 3.65702\n",
      "epoch no.2 train no.170330  loss = 3.52634 avg_loss = 3.63636\n",
      "epoch no.2 train no.170340  loss = 3.63656 avg_loss = 3.62519\n",
      "epoch no.2 train no.170350  loss = 4.68775 avg_loss = 3.64560\n",
      "epoch no.2 train no.170360  loss = 2.54184 avg_loss = 3.63323\n",
      "epoch no.2 train no.170370  loss = 3.31003 avg_loss = 3.65611\n",
      "epoch no.2 train no.170380  loss = 2.33599 avg_loss = 3.63178\n",
      "epoch no.2 train no.170390  loss = 4.01834 avg_loss = 3.63580\n",
      "epoch no.2 train no.170400  loss = 3.33900 avg_loss = 3.64900\n",
      "epoch no.2 train no.170410  loss = 2.79204 avg_loss = 3.61385\n",
      "epoch no.2 train no.170420  loss = 2.95077 avg_loss = 3.58637\n",
      "epoch no.2 train no.170430  loss = 1.65009 avg_loss = 3.55413\n",
      "epoch no.2 train no.170440  loss = 3.97807 avg_loss = 3.56009\n",
      "epoch no.2 train no.170450  loss = 2.70263 avg_loss = 3.54477\n",
      "epoch no.2 train no.170460  loss = 3.33732 avg_loss = 3.52352\n",
      "epoch no.2 train no.170470  loss = 3.47393 avg_loss = 3.55593\n",
      "epoch no.2 train no.170480  loss = 3.89779 avg_loss = 3.55806\n",
      "epoch no.2 train no.170490  loss = 2.53843 avg_loss = 3.59929\n",
      "epoch no.2 train no.170500  loss = 2.97731 avg_loss = 3.58883\n",
      "epoch no.2 train no.170510  loss = 3.41545 avg_loss = 3.59829\n",
      "epoch no.2 train no.170520  loss = 4.33078 avg_loss = 3.58370\n",
      "epoch no.2 train no.170530  loss = 2.48717 avg_loss = 3.53758\n",
      "epoch no.2 train no.170540  loss = 4.70601 avg_loss = 3.58074\n",
      "epoch no.2 train no.170550  loss = 3.92467 avg_loss = 3.55703\n",
      "epoch no.2 train no.170560  loss = 2.67824 avg_loss = 3.50374\n",
      "epoch no.2 train no.170570  loss = 2.08925 avg_loss = 3.53566\n",
      "epoch no.2 train no.170580  loss = 2.83661 avg_loss = 3.51475\n",
      "epoch no.2 train no.170590  loss = 3.66528 avg_loss = 3.46716\n",
      "epoch no.2 train no.170600  loss = 2.80604 avg_loss = 3.43613\n",
      "epoch no.2 train no.170610  loss = 2.90760 avg_loss = 3.44899\n",
      "epoch no.2 train no.170620  loss = 2.80571 avg_loss = 3.38799\n",
      "epoch no.2 train no.170630  loss = 3.57579 avg_loss = 3.45547\n",
      "epoch no.2 train no.170640  loss = 3.59419 avg_loss = 3.48495\n",
      "epoch no.2 train no.170650  loss = 4.25163 avg_loss = 3.45589\n",
      "epoch no.2 train no.170660  loss = 3.06232 avg_loss = 3.42638\n",
      "epoch no.2 train no.170670  loss = 2.64874 avg_loss = 3.40358\n",
      "epoch no.2 train no.170680  loss = 3.68084 avg_loss = 3.43904\n",
      "epoch no.2 train no.170690  loss = 3.04630 avg_loss = 3.43228\n",
      "epoch no.2 train no.170700  loss = 2.83120 avg_loss = 3.40641\n",
      "epoch no.2 train no.170710  loss = 3.81919 avg_loss = 3.44652\n",
      "epoch no.2 train no.170720  loss = 2.13534 avg_loss = 3.40014\n",
      "epoch no.2 train no.170730  loss = 3.99234 avg_loss = 3.39194\n",
      "epoch no.2 train no.170740  loss = 3.02548 avg_loss = 3.38852\n",
      "epoch no.2 train no.170750  loss = 3.19872 avg_loss = 3.42270\n",
      "epoch no.2 train no.170760  loss = 4.32647 avg_loss = 3.39849\n",
      "epoch no.2 train no.170770  loss = 3.94683 avg_loss = 3.48005\n",
      "epoch no.2 train no.170780  loss = 3.41253 avg_loss = 3.42499\n",
      "epoch no.2 train no.170790  loss = 2.43579 avg_loss = 3.46803\n",
      "epoch no.2 train no.170800  loss = 3.98796 avg_loss = 3.46756\n",
      "epoch no.2 train no.170810  loss = 3.79651 avg_loss = 3.49038\n",
      "epoch no.2 train no.170820  loss = 3.44348 avg_loss = 3.46068\n",
      "epoch no.2 train no.170830  loss = 3.12629 avg_loss = 3.42969\n",
      "epoch no.2 train no.170840  loss = 2.99445 avg_loss = 3.40809\n",
      "epoch no.2 train no.170850  loss = 3.49073 avg_loss = 3.42208\n",
      "epoch no.2 train no.170860  loss = 3.01864 avg_loss = 3.41192\n",
      "epoch no.2 train no.170870  loss = 3.34487 avg_loss = 3.40557\n",
      "epoch no.2 train no.170880  loss = 3.78403 avg_loss = 3.45018\n",
      "epoch no.2 train no.170890  loss = 3.32822 avg_loss = 3.49414\n",
      "epoch no.2 train no.170900  loss = 2.35794 avg_loss = 3.51519\n",
      "epoch no.2 train no.170910  loss = 3.23456 avg_loss = 3.48680\n",
      "epoch no.2 train no.170920  loss = 4.72277 avg_loss = 3.51002\n",
      "epoch no.2 train no.170930  loss = 3.56720 avg_loss = 3.50918\n",
      "epoch no.2 train no.170940  loss = 5.47248 avg_loss = 3.50050\n",
      "epoch no.2 train no.170950  loss = 3.30427 avg_loss = 3.54529\n",
      "epoch no.2 train no.170960  loss = 4.14064 avg_loss = 3.56989\n",
      "epoch no.2 train no.170970  loss = 2.70464 avg_loss = 3.49081\n",
      "epoch no.2 train no.170980  loss = 2.59274 avg_loss = 3.42282\n",
      "epoch no.2 train no.170990  loss = 5.95617 avg_loss = 3.49412\n",
      "epoch no.2 train no.171000  loss = 4.05361 avg_loss = 3.47907\n",
      "4\n",
      "to_tokens: ['▁가을', '전환', '이', '▁딱', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 좋은 음악</s>\n",
      "epoch no.2 train no.171010  loss = 4.40563 avg_loss = 3.44339\n",
      "epoch no.2 train no.171020  loss = 2.56192 avg_loss = 3.40233\n",
      "epoch no.2 train no.171030  loss = 3.65074 avg_loss = 3.38735\n",
      "epoch no.2 train no.171040  loss = 5.27831 avg_loss = 3.45141\n",
      "epoch no.2 train no.171050  loss = 4.16880 avg_loss = 3.45368\n",
      "epoch no.2 train no.171060  loss = 2.34168 avg_loss = 3.39384\n",
      "epoch no.2 train no.171070  loss = 1.81095 avg_loss = 3.39302\n",
      "epoch no.2 train no.171080  loss = 3.74139 avg_loss = 3.43765\n",
      "epoch no.2 train no.171090  loss = 4.11073 avg_loss = 3.48683\n",
      "epoch no.2 train no.171100  loss = 4.31233 avg_loss = 3.53436\n",
      "epoch no.2 train no.171110  loss = 3.35183 avg_loss = 3.50356\n",
      "epoch no.2 train no.171120  loss = 4.88323 avg_loss = 3.49276\n",
      "epoch no.2 train no.171130  loss = 2.84056 avg_loss = 3.50277\n",
      "epoch no.2 train no.171140  loss = 2.63112 avg_loss = 3.45055\n",
      "epoch no.2 train no.171150  loss = 4.00464 avg_loss = 3.45354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.171160  loss = 4.32914 avg_loss = 3.48457\n",
      "epoch no.2 train no.171170  loss = 4.03992 avg_loss = 3.49193\n",
      "epoch no.2 train no.171180  loss = 4.18962 avg_loss = 3.49482\n",
      "epoch no.2 train no.171190  loss = 3.31134 avg_loss = 3.49296\n",
      "epoch no.2 train no.171200  loss = 3.47024 avg_loss = 3.47695\n",
      "epoch no.2 train no.171210  loss = 3.30301 avg_loss = 3.50167\n",
      "epoch no.2 train no.171220  loss = 5.09846 avg_loss = 3.49710\n",
      "epoch no.2 train no.171230  loss = 3.50874 avg_loss = 3.47194\n",
      "epoch no.2 train no.171240  loss = 2.08817 avg_loss = 3.41482\n",
      "epoch no.2 train no.171250  loss = 3.07490 avg_loss = 3.46077\n",
      "epoch no.2 train no.171260  loss = 2.07446 avg_loss = 3.46553\n",
      "epoch no.2 train no.171270  loss = 2.46609 avg_loss = 3.47093\n",
      "epoch no.2 train no.171280  loss = 2.01757 avg_loss = 3.45492\n",
      "epoch no.2 train no.171290  loss = 1.86793 avg_loss = 3.44548\n",
      "epoch no.2 train no.171300  loss = 3.11431 avg_loss = 3.48228\n",
      "epoch no.2 train no.171310  loss = 2.46982 avg_loss = 3.46223\n",
      "epoch no.2 train no.171320  loss = 4.61246 avg_loss = 3.49431\n",
      "epoch no.2 train no.171330  loss = 2.50145 avg_loss = 3.44718\n",
      "epoch no.2 train no.171340  loss = 3.59702 avg_loss = 3.47750\n",
      "epoch no.2 train no.171350  loss = 4.90076 avg_loss = 3.49503\n",
      "epoch no.2 train no.171360  loss = 5.84312 avg_loss = 3.53698\n",
      "epoch no.2 train no.171370  loss = 3.32186 avg_loss = 3.52494\n",
      "epoch no.2 train no.171380  loss = 3.53047 avg_loss = 3.49005\n",
      "epoch no.2 train no.171390  loss = 2.57349 avg_loss = 3.48029\n",
      "epoch no.2 train no.171400  loss = 2.92041 avg_loss = 3.50187\n",
      "epoch no.2 train no.171410  loss = 4.26953 avg_loss = 3.51515\n",
      "epoch no.2 train no.171420  loss = 3.37178 avg_loss = 3.47944\n",
      "epoch no.2 train no.171430  loss = 4.01384 avg_loss = 3.53688\n",
      "epoch no.2 train no.171440  loss = 2.62259 avg_loss = 3.53687\n",
      "epoch no.2 train no.171450  loss = 2.60154 avg_loss = 3.45440\n",
      "epoch no.2 train no.171460  loss = 3.42951 avg_loss = 3.46924\n",
      "epoch no.2 train no.171470  loss = 3.14851 avg_loss = 3.47759\n",
      "epoch no.2 train no.171480  loss = 3.46695 avg_loss = 3.48994\n",
      "epoch no.2 train no.171490  loss = 3.22408 avg_loss = 3.49813\n",
      "epoch no.2 train no.171500  loss = 4.57837 avg_loss = 3.54873\n",
      "epoch no.2 train no.171510  loss = 2.14741 avg_loss = 3.52694\n",
      "epoch no.2 train no.171520  loss = 2.85857 avg_loss = 3.49096\n",
      "epoch no.2 train no.171530  loss = 4.14992 avg_loss = 3.49905\n",
      "epoch no.2 train no.171540  loss = 4.17731 avg_loss = 3.55581\n",
      "epoch no.2 train no.171550  loss = 4.61741 avg_loss = 3.55312\n",
      "epoch no.2 train no.171560  loss = 2.81374 avg_loss = 3.54393\n",
      "epoch no.2 train no.171570  loss = 4.62117 avg_loss = 3.57596\n",
      "epoch no.2 train no.171580  loss = 3.35096 avg_loss = 3.54885\n",
      "epoch no.2 train no.171590  loss = 3.20195 avg_loss = 3.56269\n",
      "epoch no.2 train no.171600  loss = 2.57452 avg_loss = 3.53058\n",
      "epoch no.2 train no.171610  loss = 3.53733 avg_loss = 3.60279\n",
      "epoch no.2 train no.171620  loss = 3.85196 avg_loss = 3.58360\n",
      "epoch no.2 train no.171630  loss = 5.33684 avg_loss = 3.54680\n",
      "epoch no.2 train no.171640  loss = 4.54870 avg_loss = 3.53218\n",
      "epoch no.2 train no.171650  loss = 2.92399 avg_loss = 3.54535\n",
      "epoch no.2 train no.171660  loss = 3.85041 avg_loss = 3.58105\n",
      "epoch no.2 train no.171670  loss = 4.86333 avg_loss = 3.54719\n",
      "epoch no.2 train no.171680  loss = 3.25858 avg_loss = 3.57943\n",
      "epoch no.2 train no.171690  loss = 2.93740 avg_loss = 3.52794\n",
      "epoch no.2 train no.171700  loss = 3.06036 avg_loss = 3.50876\n",
      "epoch no.2 train no.171710  loss = 3.60532 avg_loss = 3.53459\n",
      "epoch no.2 train no.171720  loss = 5.52132 avg_loss = 3.58697\n",
      "epoch no.2 train no.171730  loss = 2.43251 avg_loss = 3.57232\n",
      "epoch no.2 train no.171740  loss = 5.75608 avg_loss = 3.60173\n",
      "epoch no.2 train no.171750  loss = 3.94283 avg_loss = 3.58672\n",
      "epoch no.2 train no.171760  loss = 3.07286 avg_loss = 3.54301\n",
      "epoch no.2 train no.171770  loss = 6.24120 avg_loss = 3.56218\n",
      "epoch no.2 train no.171780  loss = 2.75267 avg_loss = 3.48728\n",
      "epoch no.2 train no.171790  loss = 3.41025 avg_loss = 3.51569\n",
      "epoch no.2 train no.171800  loss = 1.84520 avg_loss = 3.52032\n",
      "epoch no.2 train no.171810  loss = 3.86911 avg_loss = 3.47100\n",
      "epoch no.2 train no.171820  loss = 2.86321 avg_loss = 3.44274\n",
      "epoch no.2 train no.171830  loss = 3.51098 avg_loss = 3.51501\n",
      "epoch no.2 train no.171840  loss = 2.99851 avg_loss = 3.53567\n",
      "epoch no.2 train no.171850  loss = 4.18230 avg_loss = 3.54740\n",
      "epoch no.2 train no.171860  loss = 4.68429 avg_loss = 3.54081\n",
      "epoch no.2 train no.171870  loss = 3.44478 avg_loss = 3.54018\n",
      "epoch no.2 train no.171880  loss = 4.05858 avg_loss = 3.51085\n",
      "epoch no.2 train no.171890  loss = 3.43539 avg_loss = 3.50312\n",
      "epoch no.2 train no.171900  loss = 2.71298 avg_loss = 3.47903\n",
      "epoch no.2 train no.171910  loss = 2.18506 avg_loss = 3.47700\n",
      "epoch no.2 train no.171920  loss = 2.60786 avg_loss = 3.46678\n",
      "epoch no.2 train no.171930  loss = 3.57763 avg_loss = 3.46485\n",
      "epoch no.2 train no.171940  loss = 2.35107 avg_loss = 3.45651\n",
      "epoch no.2 train no.171950  loss = 4.83846 avg_loss = 3.45252\n",
      "epoch no.2 train no.171960  loss = 3.19853 avg_loss = 3.42303\n",
      "epoch no.2 train no.171970  loss = 4.17785 avg_loss = 3.44327\n",
      "epoch no.2 train no.171980  loss = 2.59223 avg_loss = 3.45064\n",
      "epoch no.2 train no.171990  loss = 5.74504 avg_loss = 3.47539\n",
      "epoch no.2 train no.172000  loss = 3.53132 avg_loss = 3.47432\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.2 train no.172010  loss = 3.17418 avg_loss = 3.47210\n",
      "epoch no.2 train no.172020  loss = 3.88891 avg_loss = 3.46959\n",
      "epoch no.2 train no.172030  loss = 4.73556 avg_loss = 3.46863\n",
      "epoch no.2 train no.172040  loss = 3.38206 avg_loss = 3.44570\n",
      "epoch no.2 train no.172050  loss = 2.96485 avg_loss = 3.45221\n",
      "epoch no.2 train no.172060  loss = 2.50543 avg_loss = 3.39614\n",
      "epoch no.2 train no.172070  loss = 4.28757 avg_loss = 3.36534\n",
      "epoch no.2 train no.172080  loss = 3.36709 avg_loss = 3.40856\n",
      "epoch no.2 train no.172090  loss = 3.21409 avg_loss = 3.40424\n",
      "epoch no.2 train no.172100  loss = 3.64535 avg_loss = 3.48978\n",
      "epoch no.2 train no.172110  loss = 1.75751 avg_loss = 3.47444\n",
      "epoch no.2 train no.172120  loss = 3.45125 avg_loss = 3.44739\n",
      "epoch no.2 train no.172130  loss = 4.97777 avg_loss = 3.44615\n",
      "epoch no.2 train no.172140  loss = 3.71687 avg_loss = 3.50130\n",
      "epoch no.2 train no.172150  loss = 5.56730 avg_loss = 3.52360\n",
      "epoch no.2 train no.172160  loss = 3.72672 avg_loss = 3.53394\n",
      "epoch no.2 train no.172170  loss = 2.58819 avg_loss = 3.55734\n",
      "epoch no.2 train no.172180  loss = 3.80998 avg_loss = 3.54720\n",
      "epoch no.2 train no.172190  loss = 3.14321 avg_loss = 3.57172\n",
      "epoch no.2 train no.172200  loss = 3.07104 avg_loss = 3.55897\n",
      "epoch no.2 train no.172210  loss = 2.68740 avg_loss = 3.55099\n",
      "epoch no.2 train no.172220  loss = 3.60237 avg_loss = 3.52835\n",
      "epoch no.2 train no.172230  loss = 2.22156 avg_loss = 3.53490\n",
      "epoch no.2 train no.172240  loss = 2.37173 avg_loss = 3.54744\n",
      "epoch no.2 train no.172250  loss = 5.60875 avg_loss = 3.56030\n",
      "epoch no.2 train no.172260  loss = 2.88365 avg_loss = 3.53825\n",
      "epoch no.2 train no.172270  loss = 4.94866 avg_loss = 3.54186\n",
      "epoch no.2 train no.172280  loss = 2.28424 avg_loss = 3.50479\n",
      "epoch no.2 train no.172290  loss = 6.80374 avg_loss = 3.56018\n",
      "epoch no.2 train no.172300  loss = 2.31557 avg_loss = 3.56961\n",
      "epoch no.2 train no.172310  loss = 5.13273 avg_loss = 3.53861\n",
      "epoch no.2 train no.172320  loss = 3.57402 avg_loss = 3.49980\n",
      "epoch no.2 train no.172330  loss = 3.22918 avg_loss = 3.48752\n",
      "epoch no.2 train no.172340  loss = 4.23666 avg_loss = 3.55616\n",
      "epoch no.2 train no.172350  loss = 2.46280 avg_loss = 3.58996\n",
      "epoch no.2 train no.172360  loss = 3.10386 avg_loss = 3.57629\n",
      "epoch no.2 train no.172370  loss = 3.03781 avg_loss = 3.54960\n",
      "epoch no.2 train no.172380  loss = 4.45899 avg_loss = 3.54999\n",
      "epoch no.2 train no.172390  loss = 3.50509 avg_loss = 3.54190\n",
      "epoch no.2 train no.172400  loss = 5.57305 avg_loss = 3.56108\n",
      "epoch no.2 train no.172410  loss = 3.73545 avg_loss = 3.59603\n",
      "epoch no.2 train no.172420  loss = 2.96985 avg_loss = 3.59181\n",
      "epoch no.2 train no.172430  loss = 5.63645 avg_loss = 3.62814\n",
      "epoch no.2 train no.172440  loss = 2.61729 avg_loss = 3.65015\n",
      "epoch no.2 train no.172450  loss = 2.99525 avg_loss = 3.66592\n",
      "epoch no.2 train no.172460  loss = 2.92861 avg_loss = 3.61764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.172470  loss = 4.63577 avg_loss = 3.62349\n",
      "epoch no.2 train no.172480  loss = 3.38604 avg_loss = 3.60418\n",
      "epoch no.2 train no.172490  loss = 3.66543 avg_loss = 3.66891\n",
      "epoch no.2 train no.172500  loss = 3.46187 avg_loss = 3.64799\n",
      "epoch no.2 train no.172510  loss = 3.31367 avg_loss = 3.61605\n",
      "epoch no.2 train no.172520  loss = 4.26316 avg_loss = 3.60017\n",
      "epoch no.2 train no.172530  loss = 5.22873 avg_loss = 3.54928\n",
      "epoch no.2 train no.172540  loss = 4.26657 avg_loss = 3.52221\n",
      "epoch no.2 train no.172550  loss = 2.92187 avg_loss = 3.46779\n",
      "epoch no.2 train no.172560  loss = 3.37524 avg_loss = 3.45920\n",
      "epoch no.2 train no.172570  loss = 3.15066 avg_loss = 3.49719\n",
      "epoch no.2 train no.172580  loss = 3.71754 avg_loss = 3.51559\n",
      "epoch no.2 train no.172590  loss = 3.14077 avg_loss = 3.52481\n",
      "epoch no.2 train no.172600  loss = 4.05706 avg_loss = 3.56593\n",
      "epoch no.2 train no.172610  loss = 2.65721 avg_loss = 3.56243\n",
      "epoch no.2 train no.172620  loss = 3.40968 avg_loss = 3.56898\n",
      "epoch no.2 train no.172630  loss = 3.60999 avg_loss = 3.50576\n",
      "epoch no.2 train no.172640  loss = 2.35410 avg_loss = 3.49871\n",
      "epoch no.2 train no.172650  loss = 3.22192 avg_loss = 3.47956\n",
      "epoch no.2 train no.172660  loss = 3.08849 avg_loss = 3.49002\n",
      "epoch no.2 train no.172670  loss = 3.91721 avg_loss = 3.46780\n",
      "epoch no.2 train no.172680  loss = 2.97795 avg_loss = 3.42947\n",
      "epoch no.2 train no.172690  loss = 2.99293 avg_loss = 3.40463\n",
      "epoch no.2 train no.172700  loss = 2.99793 avg_loss = 3.42056\n",
      "epoch no.2 train no.172710  loss = 4.67106 avg_loss = 3.45317\n",
      "epoch no.2 train no.172720  loss = 3.72842 avg_loss = 3.47608\n",
      "epoch no.2 train no.172730  loss = 3.35361 avg_loss = 3.45906\n",
      "epoch no.2 train no.172740  loss = 3.45600 avg_loss = 3.48076\n",
      "epoch no.2 train no.172750  loss = 3.56962 avg_loss = 3.48557\n",
      "epoch no.2 train no.172760  loss = 1.96416 avg_loss = 3.42385\n",
      "epoch no.2 train no.172770  loss = 1.20352 avg_loss = 3.40526\n",
      "epoch no.2 train no.172780  loss = 3.10280 avg_loss = 3.38497\n",
      "epoch no.2 train no.172790  loss = 4.87661 avg_loss = 3.43217\n",
      "epoch no.2 train no.172800  loss = 6.44888 avg_loss = 3.50710\n",
      "epoch no.2 train no.172810  loss = 3.49888 avg_loss = 3.50519\n",
      "epoch no.2 train no.172820  loss = 2.25616 avg_loss = 3.52547\n",
      "epoch no.2 train no.172830  loss = 3.26156 avg_loss = 3.53998\n",
      "epoch no.2 train no.172840  loss = 3.11562 avg_loss = 3.61468\n",
      "epoch no.2 train no.172850  loss = 3.72697 avg_loss = 3.58752\n",
      "epoch no.2 train no.172860  loss = 3.64781 avg_loss = 3.59792\n",
      "epoch no.2 train no.172870  loss = 3.49728 avg_loss = 3.60493\n",
      "epoch no.2 train no.172880  loss = 4.17608 avg_loss = 3.63573\n",
      "epoch no.2 train no.172890  loss = 4.01970 avg_loss = 3.59330\n",
      "epoch no.2 train no.172900  loss = 4.00534 avg_loss = 3.60611\n",
      "epoch no.2 train no.172910  loss = 3.46284 avg_loss = 3.59568\n",
      "epoch no.2 train no.172920  loss = 2.28945 avg_loss = 3.54738\n",
      "epoch no.2 train no.172930  loss = 4.78480 avg_loss = 3.55280\n",
      "epoch no.2 train no.172940  loss = 3.55841 avg_loss = 3.56385\n",
      "epoch no.2 train no.172950  loss = 2.62774 avg_loss = 3.51856\n",
      "epoch no.2 train no.172960  loss = 2.36231 avg_loss = 3.55828\n",
      "epoch no.2 train no.172970  loss = 3.10066 avg_loss = 3.58368\n",
      "epoch no.2 train no.172980  loss = 4.97122 avg_loss = 3.60840\n",
      "epoch no.2 train no.172990  loss = 1.78279 avg_loss = 3.58988\n",
      "epoch no.2 train no.173000  loss = 5.45666 avg_loss = 3.58869\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁좋은', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 좋은 노래</s>\n",
      "epoch no.2 train no.173010  loss = 3.31628 avg_loss = 3.57640\n",
      "epoch no.2 train no.173020  loss = 4.18669 avg_loss = 3.54764\n",
      "epoch no.2 train no.173030  loss = 4.11769 avg_loss = 3.53608\n",
      "epoch no.2 train no.173040  loss = 3.28480 avg_loss = 3.52415\n",
      "epoch no.2 train no.173050  loss = 4.25154 avg_loss = 3.55994\n",
      "epoch no.2 train no.173060  loss = 3.56761 avg_loss = 3.61661\n",
      "epoch no.2 train no.173070  loss = 2.73698 avg_loss = 3.60994\n",
      "epoch no.2 train no.173080  loss = 2.05749 avg_loss = 3.57402\n",
      "epoch no.2 train no.173090  loss = 2.53826 avg_loss = 3.52379\n",
      "epoch no.2 train no.173100  loss = 4.11660 avg_loss = 3.57502\n",
      "epoch no.2 train no.173110  loss = 3.59501 avg_loss = 3.55257\n",
      "epoch no.2 train no.173120  loss = 4.62243 avg_loss = 3.53898\n",
      "epoch no.2 train no.173130  loss = 3.05263 avg_loss = 3.60053\n",
      "epoch no.2 train no.173140  loss = 2.25251 avg_loss = 3.60319\n",
      "epoch no.2 train no.173150  loss = 3.27004 avg_loss = 3.63072\n",
      "epoch no.2 train no.173160  loss = 3.01591 avg_loss = 3.65188\n",
      "epoch no.2 train no.173170  loss = 1.69783 avg_loss = 3.64307\n",
      "epoch no.2 train no.173180  loss = 2.36555 avg_loss = 3.66791\n",
      "epoch no.2 train no.173190  loss = 4.09000 avg_loss = 3.67109\n",
      "epoch no.2 train no.173200  loss = 2.90697 avg_loss = 3.60847\n",
      "epoch no.2 train no.173210  loss = 2.34298 avg_loss = 3.54625\n",
      "epoch no.2 train no.173220  loss = 4.09127 avg_loss = 3.53680\n",
      "epoch no.2 train no.173230  loss = 2.58664 avg_loss = 3.54871\n",
      "epoch no.2 train no.173240  loss = 2.62996 avg_loss = 3.50388\n",
      "epoch no.2 train no.173250  loss = 3.30882 avg_loss = 3.44235\n",
      "epoch no.2 train no.173260  loss = 2.53138 avg_loss = 3.46054\n",
      "epoch no.2 train no.173270  loss = 4.14029 avg_loss = 3.46256\n",
      "epoch no.2 train no.173280  loss = 2.29207 avg_loss = 3.47375\n",
      "epoch no.2 train no.173290  loss = 3.42600 avg_loss = 3.49941\n",
      "epoch no.2 train no.173300  loss = 4.58972 avg_loss = 3.45396\n",
      "epoch no.2 train no.173310  loss = 5.17608 avg_loss = 3.44380\n",
      "epoch no.2 train no.173320  loss = 3.85977 avg_loss = 3.47229\n",
      "epoch no.2 train no.173330  loss = 2.09222 avg_loss = 3.46472\n",
      "epoch no.2 train no.173340  loss = 3.08713 avg_loss = 3.43943\n",
      "epoch no.2 train no.173350  loss = 2.53600 avg_loss = 3.42455\n",
      "epoch no.2 train no.173360  loss = 5.49927 avg_loss = 3.46933\n",
      "epoch no.2 train no.173370  loss = 2.24008 avg_loss = 3.48164\n",
      "epoch no.2 train no.173380  loss = 4.30938 avg_loss = 3.48264\n",
      "epoch no.2 train no.173390  loss = 3.56741 avg_loss = 3.52926\n",
      "epoch no.2 train no.173400  loss = 1.99713 avg_loss = 3.50261\n",
      "epoch no.2 train no.173410  loss = 3.79514 avg_loss = 3.51542\n",
      "epoch no.2 train no.173420  loss = 2.46888 avg_loss = 3.53727\n",
      "epoch no.2 train no.173430  loss = 2.52044 avg_loss = 3.50850\n",
      "epoch no.2 train no.173440  loss = 3.64954 avg_loss = 3.50704\n",
      "epoch no.2 train no.173450  loss = 5.37618 avg_loss = 3.50604\n",
      "epoch no.2 train no.173460  loss = 4.84452 avg_loss = 3.55698\n",
      "epoch no.2 train no.173470  loss = 2.38196 avg_loss = 3.53681\n",
      "epoch no.2 train no.173480  loss = 2.19058 avg_loss = 3.51275\n",
      "epoch no.2 train no.173490  loss = 2.72551 avg_loss = 3.54544\n",
      "epoch no.2 train no.173500  loss = 3.32094 avg_loss = 3.61153\n",
      "epoch no.2 train no.173510  loss = 6.67323 avg_loss = 3.63237\n",
      "epoch no.2 train no.173520  loss = 3.06689 avg_loss = 3.61839\n",
      "epoch no.2 train no.173530  loss = 3.16630 avg_loss = 3.61615\n",
      "epoch no.2 train no.173540  loss = 4.43514 avg_loss = 3.63257\n",
      "epoch no.2 train no.173550  loss = 5.92722 avg_loss = 3.70884\n",
      "epoch no.2 train no.173560  loss = 1.97977 avg_loss = 3.75787\n",
      "epoch no.2 train no.173570  loss = 3.37849 avg_loss = 3.73711\n",
      "epoch no.2 train no.173580  loss = 4.53113 avg_loss = 3.73477\n",
      "epoch no.2 train no.173590  loss = 2.61175 avg_loss = 3.72565\n",
      "epoch no.2 train no.173600  loss = 2.04439 avg_loss = 3.71338\n",
      "epoch no.2 train no.173610  loss = 4.04993 avg_loss = 3.67834\n",
      "epoch no.2 train no.173620  loss = 2.35629 avg_loss = 3.61180\n",
      "epoch no.2 train no.173630  loss = 2.41893 avg_loss = 3.56196\n",
      "epoch no.2 train no.173640  loss = 2.30977 avg_loss = 3.57249\n",
      "epoch no.2 train no.173650  loss = 2.44032 avg_loss = 3.51228\n",
      "epoch no.2 train no.173660  loss = 3.29459 avg_loss = 3.53534\n",
      "epoch no.2 train no.173670  loss = 3.32644 avg_loss = 3.47865\n",
      "epoch no.2 train no.173680  loss = 3.42419 avg_loss = 3.47702\n",
      "epoch no.2 train no.173690  loss = 5.46972 avg_loss = 3.51880\n",
      "epoch no.2 train no.173700  loss = 2.37075 avg_loss = 3.51883\n",
      "epoch no.2 train no.173710  loss = 3.02438 avg_loss = 3.47918\n",
      "epoch no.2 train no.173720  loss = 2.93811 avg_loss = 3.49433\n",
      "epoch no.2 train no.173730  loss = 4.09874 avg_loss = 3.48397\n",
      "epoch no.2 train no.173740  loss = 2.52360 avg_loss = 3.45152\n",
      "epoch no.2 train no.173750  loss = 1.99518 avg_loss = 3.46731\n",
      "epoch no.2 train no.173760  loss = 2.87420 avg_loss = 3.47988\n",
      "epoch no.2 train no.173770  loss = 4.48495 avg_loss = 3.46719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.173780  loss = 3.90521 avg_loss = 3.45568\n",
      "epoch no.2 train no.173790  loss = 5.13953 avg_loss = 3.53534\n",
      "epoch no.2 train no.173800  loss = 3.77578 avg_loss = 3.49979\n",
      "epoch no.2 train no.173810  loss = 2.78211 avg_loss = 3.45469\n",
      "epoch no.2 train no.173820  loss = 3.95765 avg_loss = 3.41097\n",
      "epoch no.2 train no.173830  loss = 2.93118 avg_loss = 3.38635\n",
      "epoch no.2 train no.173840  loss = 2.46523 avg_loss = 3.39749\n",
      "epoch no.2 train no.173850  loss = 3.23042 avg_loss = 3.40580\n",
      "epoch no.2 train no.173860  loss = 4.80539 avg_loss = 3.43001\n",
      "epoch no.2 train no.173870  loss = 3.34405 avg_loss = 3.47155\n",
      "epoch no.2 train no.173880  loss = 2.92436 avg_loss = 3.43915\n",
      "epoch no.2 train no.173890  loss = 4.20179 avg_loss = 3.44720\n",
      "epoch no.2 train no.173900  loss = 3.94831 avg_loss = 3.50014\n",
      "epoch no.2 train no.173910  loss = 2.88251 avg_loss = 3.56516\n",
      "epoch no.2 train no.173920  loss = 2.96964 avg_loss = 3.54388\n",
      "epoch no.2 train no.173930  loss = 5.77646 avg_loss = 3.58038\n",
      "epoch no.2 train no.173940  loss = 3.25434 avg_loss = 3.55224\n",
      "epoch no.2 train no.173950  loss = 4.15681 avg_loss = 3.54134\n",
      "epoch no.2 train no.173960  loss = 3.18128 avg_loss = 3.53931\n",
      "epoch no.2 train no.173970  loss = 2.67526 avg_loss = 3.50992\n",
      "epoch no.2 train no.173980  loss = 2.24796 avg_loss = 3.52581\n",
      "epoch no.2 train no.173990  loss = 2.15977 avg_loss = 3.53553\n",
      "epoch no.2 train no.174000  loss = 3.15297 avg_loss = 3.53238\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.2 train no.174010  loss = 4.37558 avg_loss = 3.53171\n",
      "epoch no.2 train no.174020  loss = 2.78113 avg_loss = 3.48662\n",
      "epoch no.2 train no.174030  loss = 5.86059 avg_loss = 3.52065\n",
      "epoch no.2 train no.174040  loss = 2.95498 avg_loss = 3.52880\n",
      "epoch no.2 train no.174050  loss = 6.56009 avg_loss = 3.55148\n",
      "epoch no.2 train no.174060  loss = 2.73124 avg_loss = 3.55444\n",
      "epoch no.2 train no.174070  loss = 2.69693 avg_loss = 3.53408\n",
      "epoch no.2 train no.174080  loss = 2.83659 avg_loss = 3.54442\n",
      "epoch no.2 train no.174090  loss = 4.65288 avg_loss = 3.60946\n",
      "epoch no.2 train no.174100  loss = 2.48848 avg_loss = 3.54731\n",
      "epoch no.2 train no.174110  loss = 3.27450 avg_loss = 3.52467\n",
      "epoch no.2 train no.174120  loss = 3.09688 avg_loss = 3.51307\n",
      "epoch no.2 train no.174130  loss = 3.49177 avg_loss = 3.50898\n",
      "epoch no.2 train no.174140  loss = 3.97383 avg_loss = 3.47475\n",
      "epoch no.2 train no.174150  loss = 3.68887 avg_loss = 3.47492\n",
      "epoch no.2 train no.174160  loss = 3.51474 avg_loss = 3.51125\n",
      "epoch no.2 train no.174170  loss = 3.65666 avg_loss = 3.50467\n",
      "epoch no.2 train no.174180  loss = 3.72957 avg_loss = 3.51821\n",
      "epoch no.2 train no.174190  loss = 3.69326 avg_loss = 3.47760\n",
      "epoch no.2 train no.174200  loss = 2.99148 avg_loss = 3.46421\n",
      "epoch no.2 train no.174210  loss = 2.99594 avg_loss = 3.48867\n",
      "epoch no.2 train no.174220  loss = 3.44825 avg_loss = 3.45371\n",
      "epoch no.2 train no.174230  loss = 4.06421 avg_loss = 3.45583\n",
      "epoch no.2 train no.174240  loss = 4.92056 avg_loss = 3.44818\n",
      "epoch no.2 train no.174250  loss = 2.06811 avg_loss = 3.42920\n",
      "epoch no.2 train no.174260  loss = 1.19957 avg_loss = 3.38686\n",
      "epoch no.2 train no.174270  loss = 2.61270 avg_loss = 3.38643\n",
      "epoch no.2 train no.174280  loss = 4.45446 avg_loss = 3.41044\n",
      "epoch no.2 train no.174290  loss = 3.30715 avg_loss = 3.44701\n",
      "epoch no.2 train no.174300  loss = 3.43487 avg_loss = 3.46795\n",
      "epoch no.2 train no.174310  loss = 3.93582 avg_loss = 3.46308\n",
      "epoch no.2 train no.174320  loss = 3.30559 avg_loss = 3.47782\n",
      "epoch no.2 train no.174330  loss = 2.90370 avg_loss = 3.46216\n",
      "epoch no.2 train no.174340  loss = 3.91521 avg_loss = 3.44811\n",
      "epoch no.2 train no.174350  loss = 3.89017 avg_loss = 3.52836\n",
      "epoch no.2 train no.174360  loss = 4.56690 avg_loss = 3.54626\n",
      "epoch no.2 train no.174370  loss = 4.14318 avg_loss = 3.55428\n",
      "epoch no.2 train no.174380  loss = 1.79768 avg_loss = 3.52627\n",
      "epoch no.2 train no.174390  loss = 2.88036 avg_loss = 3.52058\n",
      "epoch no.2 train no.174400  loss = 3.51262 avg_loss = 3.49889\n",
      "epoch no.2 train no.174410  loss = 5.18551 avg_loss = 3.56139\n",
      "epoch no.2 train no.174420  loss = 4.27306 avg_loss = 3.55260\n",
      "epoch no.2 train no.174430  loss = 4.49031 avg_loss = 3.54067\n",
      "epoch no.2 train no.174440  loss = 1.70213 avg_loss = 3.49339\n",
      "epoch no.2 train no.174450  loss = 4.81299 avg_loss = 3.55453\n",
      "epoch no.2 train no.174460  loss = 4.70630 avg_loss = 3.58260\n",
      "epoch no.2 train no.174470  loss = 4.32999 avg_loss = 3.57576\n",
      "epoch no.2 train no.174480  loss = 5.45435 avg_loss = 3.56644\n",
      "epoch no.2 train no.174490  loss = 3.77849 avg_loss = 3.57548\n",
      "epoch no.2 train no.174500  loss = 2.68415 avg_loss = 3.55062\n",
      "epoch no.2 train no.174510  loss = 4.41496 avg_loss = 3.54098\n",
      "epoch no.2 train no.174520  loss = 2.84896 avg_loss = 3.56037\n",
      "epoch no.2 train no.174530  loss = 2.48850 avg_loss = 3.57734\n",
      "epoch no.2 train no.174540  loss = 2.10529 avg_loss = 3.56452\n",
      "epoch no.2 train no.174550  loss = 4.34188 avg_loss = 3.56048\n",
      "epoch no.2 train no.174560  loss = 5.69457 avg_loss = 3.54421\n",
      "epoch no.2 train no.174570  loss = 4.25673 avg_loss = 3.55951\n",
      "epoch no.2 train no.174580  loss = 3.60228 avg_loss = 3.51042\n",
      "epoch no.2 train no.174590  loss = 3.58344 avg_loss = 3.49652\n",
      "epoch no.2 train no.174600  loss = 3.94947 avg_loss = 3.49970\n",
      "epoch no.2 train no.174610  loss = 3.42075 avg_loss = 3.45461\n",
      "epoch no.2 train no.174620  loss = 3.56260 avg_loss = 3.44898\n",
      "epoch no.2 train no.174630  loss = 3.70640 avg_loss = 3.43171\n",
      "epoch no.2 train no.174640  loss = 3.76809 avg_loss = 3.45286\n",
      "epoch no.2 train no.174650  loss = 3.54178 avg_loss = 3.44371\n",
      "epoch no.2 train no.174660  loss = 3.14825 avg_loss = 3.46035\n",
      "epoch no.2 train no.174670  loss = 2.70027 avg_loss = 3.47250\n",
      "epoch no.2 train no.174680  loss = 4.23437 avg_loss = 3.44188\n",
      "epoch no.2 train no.174690  loss = 3.47880 avg_loss = 3.46741\n",
      "epoch no.2 train no.174700  loss = 3.60087 avg_loss = 3.49855\n",
      "epoch no.2 train no.174710  loss = 4.99552 avg_loss = 3.51374\n",
      "epoch no.2 train no.174720  loss = 4.55704 avg_loss = 3.51917\n",
      "epoch no.2 train no.174730  loss = 3.52755 avg_loss = 3.50585\n",
      "epoch no.2 train no.174740  loss = 3.78663 avg_loss = 3.49650\n",
      "epoch no.2 train no.174750  loss = 2.24606 avg_loss = 3.46795\n",
      "epoch no.2 train no.174760  loss = 3.88275 avg_loss = 3.49934\n",
      "epoch no.2 train no.174770  loss = 1.64030 avg_loss = 3.46825\n",
      "epoch no.2 train no.174780  loss = 2.31920 avg_loss = 3.47595\n",
      "epoch no.2 train no.174790  loss = 3.98021 avg_loss = 3.41925\n",
      "epoch no.2 train no.174800  loss = 4.20564 avg_loss = 3.42484\n",
      "epoch no.2 train no.174810  loss = 4.16982 avg_loss = 3.42686\n",
      "epoch no.2 train no.174820  loss = 6.57073 avg_loss = 3.52070\n",
      "epoch no.2 train no.174830  loss = 3.43801 avg_loss = 3.53784\n",
      "epoch no.2 train no.174840  loss = 3.42915 avg_loss = 3.53133\n",
      "epoch no.2 train no.174850  loss = 3.31519 avg_loss = 3.49236\n",
      "epoch no.2 train no.174860  loss = 2.03987 avg_loss = 3.50447\n",
      "epoch no.2 train no.174870  loss = 2.07548 avg_loss = 3.49206\n",
      "epoch no.2 train no.174880  loss = 3.08691 avg_loss = 3.49619\n",
      "epoch no.2 train no.174890  loss = 5.08763 avg_loss = 3.48394\n",
      "epoch no.2 train no.174900  loss = 3.37901 avg_loss = 3.49364\n",
      "epoch no.2 train no.174910  loss = 1.82352 avg_loss = 3.42959\n",
      "epoch no.2 train no.174920  loss = 3.57197 avg_loss = 3.45297\n",
      "epoch no.2 train no.174930  loss = 5.15430 avg_loss = 3.46677\n",
      "epoch no.2 train no.174940  loss = 1.70018 avg_loss = 3.42889\n",
      "epoch no.2 train no.174950  loss = 4.90480 avg_loss = 3.47224\n",
      "epoch no.2 train no.174960  loss = 3.50362 avg_loss = 3.52833\n",
      "epoch no.2 train no.174970  loss = 2.18800 avg_loss = 3.47121\n",
      "epoch no.2 train no.174980  loss = 4.98990 avg_loss = 3.47805\n",
      "epoch no.2 train no.174990  loss = 2.49107 avg_loss = 3.46547\n",
      "epoch no.2 train no.175000  loss = 4.53894 avg_loss = 3.46242\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁팝', '▁노래', '</s>']\n",
      "기분전환용 신나는 음악</s>\n",
      "epoch no.2 train no.175010  loss = 4.42802 avg_loss = 3.47008\n",
      "epoch no.2 train no.175020  loss = 3.98867 avg_loss = 3.46600\n",
      "epoch no.2 train no.175030  loss = 2.36197 avg_loss = 3.42040\n",
      "epoch no.2 train no.175040  loss = 4.23587 avg_loss = 3.42487\n",
      "epoch no.2 train no.175050  loss = 2.56373 avg_loss = 3.37521\n",
      "epoch no.2 train no.175060  loss = 2.81442 avg_loss = 3.34849\n",
      "epoch no.2 train no.175070  loss = 4.30086 avg_loss = 3.30373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.175080  loss = 2.92605 avg_loss = 3.27884\n",
      "epoch no.2 train no.175090  loss = 4.18372 avg_loss = 3.27678\n",
      "epoch no.2 train no.175100  loss = 3.87351 avg_loss = 3.27175\n",
      "epoch no.2 train no.175110  loss = 3.17734 avg_loss = 3.26412\n",
      "epoch no.2 train no.175120  loss = 3.14228 avg_loss = 3.26558\n",
      "epoch no.2 train no.175130  loss = 2.05654 avg_loss = 3.23256\n",
      "epoch no.2 train no.175140  loss = 1.89986 avg_loss = 3.25428\n",
      "epoch no.2 train no.175150  loss = 2.65556 avg_loss = 3.29069\n",
      "epoch no.2 train no.175160  loss = 2.93492 avg_loss = 3.27894\n",
      "epoch no.2 train no.175170  loss = 4.72512 avg_loss = 3.33551\n",
      "epoch no.2 train no.175180  loss = 3.96968 avg_loss = 3.34002\n",
      "epoch no.2 train no.175190  loss = 3.67645 avg_loss = 3.36400\n",
      "epoch no.2 train no.175200  loss = 4.59643 avg_loss = 3.39215\n",
      "epoch no.2 train no.175210  loss = 3.40370 avg_loss = 3.44816\n",
      "epoch no.2 train no.175220  loss = 3.38681 avg_loss = 3.48916\n",
      "epoch no.2 train no.175230  loss = 5.20734 avg_loss = 3.49724\n",
      "epoch no.2 train no.175240  loss = 2.60692 avg_loss = 3.44664\n",
      "epoch no.2 train no.175250  loss = 5.49797 avg_loss = 3.45664\n",
      "epoch no.2 train no.175260  loss = 2.87235 avg_loss = 3.49235\n",
      "epoch no.2 train no.175270  loss = 1.94458 avg_loss = 3.45640\n",
      "epoch no.2 train no.175280  loss = 2.72087 avg_loss = 3.44786\n",
      "epoch no.2 train no.175290  loss = 6.16687 avg_loss = 3.47600\n",
      "epoch no.2 train no.175300  loss = 2.76106 avg_loss = 3.46173\n",
      "epoch no.2 train no.175310  loss = 3.21004 avg_loss = 3.47431\n",
      "epoch no.2 train no.175320  loss = 3.94923 avg_loss = 3.56930\n",
      "epoch no.2 train no.175330  loss = 2.25986 avg_loss = 3.57273\n",
      "epoch no.2 train no.175340  loss = 4.22307 avg_loss = 3.59119\n",
      "epoch no.2 train no.175350  loss = 4.14201 avg_loss = 3.58400\n",
      "epoch no.2 train no.175360  loss = 2.41023 avg_loss = 3.58504\n",
      "epoch no.2 train no.175370  loss = 3.37663 avg_loss = 3.56379\n",
      "epoch no.2 train no.175380  loss = 2.05157 avg_loss = 3.56003\n",
      "epoch no.2 train no.175390  loss = 4.65909 avg_loss = 3.57268\n",
      "epoch no.2 train no.175400  loss = 3.57002 avg_loss = 3.52890\n",
      "epoch no.2 train no.175410  loss = 4.65782 avg_loss = 3.56571\n",
      "epoch no.2 train no.175420  loss = 1.88034 avg_loss = 3.56673\n",
      "epoch no.2 train no.175430  loss = 4.59010 avg_loss = 3.57382\n",
      "epoch no.2 train no.175440  loss = 3.80398 avg_loss = 3.55725\n",
      "epoch no.2 train no.175450  loss = 3.43583 avg_loss = 3.49713\n",
      "epoch no.2 train no.175460  loss = 3.05263 avg_loss = 3.50184\n",
      "epoch no.2 train no.175470  loss = 4.36051 avg_loss = 3.52851\n",
      "epoch no.2 train no.175480  loss = 2.79299 avg_loss = 3.53224\n",
      "epoch no.2 train no.175490  loss = 2.67717 avg_loss = 3.46163\n",
      "epoch no.2 train no.175500  loss = 3.46282 avg_loss = 3.46329\n",
      "epoch no.2 train no.175510  loss = 2.13803 avg_loss = 3.47543\n",
      "epoch no.2 train no.175520  loss = 3.48273 avg_loss = 3.49178\n",
      "epoch no.2 train no.175530  loss = 3.71524 avg_loss = 3.43799\n",
      "epoch no.2 train no.175540  loss = 6.14094 avg_loss = 3.43253\n",
      "epoch no.2 train no.175550  loss = 3.72627 avg_loss = 3.50665\n",
      "epoch no.2 train no.175560  loss = 4.80496 avg_loss = 3.49353\n",
      "epoch no.2 train no.175570  loss = 3.09143 avg_loss = 3.47658\n",
      "epoch no.2 train no.175580  loss = 3.41240 avg_loss = 3.45716\n",
      "epoch no.2 train no.175590  loss = 2.88126 avg_loss = 3.47425\n",
      "epoch no.2 train no.175600  loss = 3.34889 avg_loss = 3.48790\n",
      "epoch no.2 train no.175610  loss = 2.97599 avg_loss = 3.45579\n",
      "epoch no.2 train no.175620  loss = 2.62194 avg_loss = 3.50302\n",
      "epoch no.2 train no.175630  loss = 3.35942 avg_loss = 3.47191\n",
      "epoch no.2 train no.175640  loss = 4.00374 avg_loss = 3.42546\n",
      "epoch no.2 train no.175650  loss = 4.34370 avg_loss = 3.45372\n",
      "epoch no.2 train no.175660  loss = 5.12315 avg_loss = 3.46188\n",
      "epoch no.2 train no.175670  loss = 4.63039 avg_loss = 3.49298\n",
      "epoch no.2 train no.175680  loss = 1.04434 avg_loss = 3.46021\n",
      "epoch no.2 train no.175690  loss = 3.34555 avg_loss = 3.44639\n",
      "epoch no.2 train no.175700  loss = 3.06353 avg_loss = 3.43599\n",
      "epoch no.2 train no.175710  loss = 5.06027 avg_loss = 3.47909\n",
      "epoch no.2 train no.175720  loss = 3.73931 avg_loss = 3.46454\n",
      "epoch no.2 train no.175730  loss = 2.89326 avg_loss = 3.47218\n",
      "epoch no.2 train no.175740  loss = 2.98034 avg_loss = 3.49401\n",
      "epoch no.2 train no.175750  loss = 4.60468 avg_loss = 3.49544\n",
      "epoch no.2 train no.175760  loss = 4.00687 avg_loss = 3.46625\n",
      "epoch no.2 train no.175770  loss = 2.21766 avg_loss = 3.39121\n",
      "epoch no.2 train no.175780  loss = 3.76093 avg_loss = 3.47880\n",
      "epoch no.2 train no.175790  loss = 2.43760 avg_loss = 3.46226\n",
      "epoch no.2 train no.175800  loss = 3.59929 avg_loss = 3.41673\n",
      "epoch no.2 train no.175810  loss = 5.39418 avg_loss = 3.45697\n",
      "epoch no.2 train no.175820  loss = 3.26487 avg_loss = 3.47427\n",
      "epoch no.2 train no.175830  loss = 3.13436 avg_loss = 3.42789\n",
      "epoch no.2 train no.175840  loss = 4.66245 avg_loss = 3.45157\n",
      "epoch no.2 train no.175850  loss = 4.31663 avg_loss = 3.46981\n",
      "epoch no.2 train no.175860  loss = 4.90069 avg_loss = 3.45387\n",
      "epoch no.2 train no.175870  loss = 2.85862 avg_loss = 3.49605\n",
      "epoch no.2 train no.175880  loss = 4.24831 avg_loss = 3.51665\n",
      "epoch no.2 train no.175890  loss = 4.35650 avg_loss = 3.50873\n",
      "epoch no.2 train no.175900  loss = 4.99952 avg_loss = 3.52865\n",
      "epoch no.2 train no.175910  loss = 2.56790 avg_loss = 3.49441\n",
      "epoch no.2 train no.175920  loss = 2.49495 avg_loss = 3.49280\n",
      "epoch no.2 train no.175930  loss = 3.04855 avg_loss = 3.51916\n",
      "epoch no.2 train no.175940  loss = 4.21560 avg_loss = 3.55252\n",
      "epoch no.2 train no.175950  loss = 5.97623 avg_loss = 3.56688\n",
      "epoch no.2 train no.175960  loss = 2.94762 avg_loss = 3.57590\n",
      "epoch no.2 train no.175970  loss = 5.88178 avg_loss = 3.63933\n",
      "epoch no.2 train no.175980  loss = 2.90718 avg_loss = 3.62307\n",
      "epoch no.2 train no.175990  loss = 3.09339 avg_loss = 3.64737\n",
      "epoch no.2 train no.176000  loss = 4.03805 avg_loss = 3.62239\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '용', '▁딱', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 좋은 힙합</s>\n",
      "epoch no.2 train no.176010  loss = 3.22053 avg_loss = 3.59698\n",
      "epoch no.2 train no.176020  loss = 4.15592 avg_loss = 3.53984\n",
      "epoch no.2 train no.176030  loss = 3.10436 avg_loss = 3.54103\n",
      "epoch no.2 train no.176040  loss = 3.27994 avg_loss = 3.53430\n",
      "epoch no.2 train no.176050  loss = 2.15450 avg_loss = 3.49570\n",
      "epoch no.2 train no.176060  loss = 3.16519 avg_loss = 3.52077\n",
      "epoch no.2 train no.176070  loss = 4.22579 avg_loss = 3.52350\n",
      "epoch no.2 train no.176080  loss = 3.71422 avg_loss = 3.54305\n",
      "epoch no.2 train no.176090  loss = 3.79988 avg_loss = 3.52828\n",
      "epoch no.2 train no.176100  loss = 2.54593 avg_loss = 3.55979\n",
      "epoch no.2 train no.176110  loss = 3.04388 avg_loss = 3.56655\n",
      "epoch no.2 train no.176120  loss = 3.77297 avg_loss = 3.59382\n",
      "epoch no.2 train no.176130  loss = 3.49629 avg_loss = 3.63974\n",
      "epoch no.2 train no.176140  loss = 1.62983 avg_loss = 3.58978\n",
      "epoch no.2 train no.176150  loss = 3.41705 avg_loss = 3.56729\n",
      "epoch no.2 train no.176160  loss = 3.85839 avg_loss = 3.60340\n",
      "epoch no.2 train no.176170  loss = 5.68654 avg_loss = 3.58903\n",
      "epoch no.2 train no.176180  loss = 5.81800 avg_loss = 3.61272\n",
      "epoch no.2 train no.176190  loss = 1.65005 avg_loss = 3.58285\n",
      "epoch no.2 train no.176200  loss = 3.44297 avg_loss = 3.55088\n",
      "epoch no.2 train no.176210  loss = 4.29649 avg_loss = 3.59561\n",
      "epoch no.2 train no.176220  loss = 3.09929 avg_loss = 3.60142\n",
      "epoch no.2 train no.176230  loss = 3.18945 avg_loss = 3.60336\n",
      "epoch no.2 train no.176240  loss = 4.79203 avg_loss = 3.64110\n",
      "epoch no.2 train no.176250  loss = 1.98428 avg_loss = 3.61870\n",
      "epoch no.2 train no.176260  loss = 3.43403 avg_loss = 3.62472\n",
      "epoch no.2 train no.176270  loss = 2.66495 avg_loss = 3.62463\n",
      "epoch no.2 train no.176280  loss = 3.93877 avg_loss = 3.66494\n",
      "epoch no.2 train no.176290  loss = 3.49523 avg_loss = 3.68589\n",
      "epoch no.2 train no.176300  loss = 3.70182 avg_loss = 3.70860\n",
      "epoch no.2 train no.176310  loss = 4.27665 avg_loss = 3.69238\n",
      "epoch no.2 train no.176320  loss = 1.95316 avg_loss = 3.63876\n",
      "epoch no.2 train no.176330  loss = 4.64056 avg_loss = 3.60060\n",
      "epoch no.2 train no.176340  loss = 2.78238 avg_loss = 3.56678\n",
      "epoch no.2 train no.176350  loss = 2.99191 avg_loss = 3.64050\n",
      "epoch no.2 train no.176360  loss = 3.90946 avg_loss = 3.67249\n",
      "epoch no.2 train no.176370  loss = 3.16275 avg_loss = 3.61421\n",
      "epoch no.2 train no.176380  loss = 4.99258 avg_loss = 3.66111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.176390  loss = 2.73700 avg_loss = 3.64500\n",
      "epoch no.2 train no.176400  loss = 4.75657 avg_loss = 3.61015\n",
      "epoch no.2 train no.176410  loss = 5.68720 avg_loss = 3.62858\n",
      "epoch no.2 train no.176420  loss = 3.97831 avg_loss = 3.64905\n",
      "epoch no.2 train no.176430  loss = 3.18350 avg_loss = 3.62962\n",
      "epoch no.2 train no.176440  loss = 4.45470 avg_loss = 3.67866\n",
      "epoch no.2 train no.176450  loss = 4.04742 avg_loss = 3.65918\n",
      "epoch no.2 train no.176460  loss = 3.76905 avg_loss = 3.63803\n",
      "epoch no.2 train no.176470  loss = 2.99869 avg_loss = 3.65421\n",
      "epoch no.2 train no.176480  loss = 2.68041 avg_loss = 3.61624\n",
      "epoch no.2 train no.176490  loss = 4.24725 avg_loss = 3.60445\n",
      "epoch no.2 train no.176500  loss = 3.20713 avg_loss = 3.60817\n",
      "epoch no.2 train no.176510  loss = 7.00467 avg_loss = 3.63541\n",
      "epoch no.2 train no.176520  loss = 3.45344 avg_loss = 3.62685\n",
      "epoch no.2 train no.176530  loss = 4.55898 avg_loss = 3.58099\n",
      "epoch no.2 train no.176540  loss = 3.02960 avg_loss = 3.56288\n",
      "epoch no.2 train no.176550  loss = 5.27399 avg_loss = 3.56611\n",
      "epoch no.2 train no.176560  loss = 2.69809 avg_loss = 3.51680\n",
      "epoch no.2 train no.176570  loss = 4.18563 avg_loss = 3.54987\n",
      "epoch no.2 train no.176580  loss = 3.90145 avg_loss = 3.55437\n",
      "epoch no.2 train no.176590  loss = 6.95464 avg_loss = 3.59584\n",
      "epoch no.2 train no.176600  loss = 3.02679 avg_loss = 3.54736\n",
      "epoch no.2 train no.176610  loss = 2.10325 avg_loss = 3.54391\n",
      "epoch no.2 train no.176620  loss = 2.60559 avg_loss = 3.49345\n",
      "epoch no.2 train no.176630  loss = 4.84929 avg_loss = 3.47029\n",
      "epoch no.2 train no.176640  loss = 3.86680 avg_loss = 3.49845\n",
      "epoch no.2 train no.176650  loss = 3.38540 avg_loss = 3.51102\n",
      "epoch no.2 train no.176660  loss = 3.99603 avg_loss = 3.52385\n",
      "epoch no.2 train no.176670  loss = 4.04650 avg_loss = 3.53633\n",
      "epoch no.2 train no.176680  loss = 4.06944 avg_loss = 3.50912\n",
      "epoch no.2 train no.176690  loss = 2.56950 avg_loss = 3.47445\n",
      "epoch no.2 train no.176700  loss = 5.23518 avg_loss = 3.48502\n",
      "epoch no.2 train no.176710  loss = 4.28650 avg_loss = 3.50923\n",
      "epoch no.2 train no.176720  loss = 4.17169 avg_loss = 3.55669\n",
      "epoch no.2 train no.176730  loss = 2.82258 avg_loss = 3.54631\n",
      "epoch no.2 train no.176740  loss = 3.66202 avg_loss = 3.51874\n",
      "epoch no.2 train no.176750  loss = 2.18097 avg_loss = 3.49799\n",
      "epoch no.2 train no.176760  loss = 5.05080 avg_loss = 3.48989\n",
      "epoch no.2 train no.176770  loss = 3.04516 avg_loss = 3.45805\n",
      "epoch no.2 train no.176780  loss = 4.75327 avg_loss = 3.44841\n",
      "epoch no.2 train no.176790  loss = 2.24151 avg_loss = 3.37692\n",
      "epoch no.2 train no.176800  loss = 2.39780 avg_loss = 3.40941\n",
      "epoch no.2 train no.176810  loss = 2.68440 avg_loss = 3.40552\n",
      "epoch no.2 train no.176820  loss = 3.99421 avg_loss = 3.48152\n",
      "epoch no.2 train no.176830  loss = 4.29227 avg_loss = 3.53182\n",
      "epoch no.2 train no.176840  loss = 4.73467 avg_loss = 3.51877\n",
      "epoch no.2 train no.176850  loss = 3.05284 avg_loss = 3.49277\n",
      "epoch no.2 train no.176860  loss = 2.43862 avg_loss = 3.46301\n",
      "epoch no.2 train no.176870  loss = 4.71865 avg_loss = 3.49059\n",
      "epoch no.2 train no.176880  loss = 3.90890 avg_loss = 3.48437\n",
      "epoch no.2 train no.176890  loss = 3.08868 avg_loss = 3.48776\n",
      "epoch no.2 train no.176900  loss = 2.41584 avg_loss = 3.49442\n",
      "epoch no.2 train no.176910  loss = 4.84777 avg_loss = 3.51680\n",
      "epoch no.2 train no.176920  loss = 3.89356 avg_loss = 3.46504\n",
      "epoch no.2 train no.176930  loss = 2.94310 avg_loss = 3.47602\n",
      "epoch no.2 train no.176940  loss = 4.12099 avg_loss = 3.45302\n",
      "epoch no.2 train no.176950  loss = 3.66385 avg_loss = 3.46822\n",
      "epoch no.2 train no.176960  loss = 2.82585 avg_loss = 3.46744\n",
      "epoch no.2 train no.176970  loss = 4.20940 avg_loss = 3.42441\n",
      "epoch no.2 train no.176980  loss = 3.28716 avg_loss = 3.37411\n",
      "epoch no.2 train no.176990  loss = 2.02591 avg_loss = 3.39857\n",
      "epoch no.2 train no.177000  loss = 2.85290 avg_loss = 3.38303\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '에', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.2 train no.177010  loss = 4.16127 avg_loss = 3.36300\n",
      "epoch no.2 train no.177020  loss = 3.42500 avg_loss = 3.39771\n",
      "epoch no.2 train no.177030  loss = 3.48761 avg_loss = 3.46053\n",
      "epoch no.2 train no.177040  loss = 2.31067 avg_loss = 3.40295\n",
      "epoch no.2 train no.177050  loss = 2.65552 avg_loss = 3.45499\n",
      "epoch no.2 train no.177060  loss = 1.70784 avg_loss = 3.43547\n",
      "epoch no.2 train no.177070  loss = 1.90738 avg_loss = 3.43249\n",
      "epoch no.2 train no.177080  loss = 2.58445 avg_loss = 3.43192\n",
      "epoch no.2 train no.177090  loss = 2.40113 avg_loss = 3.41062\n",
      "epoch no.2 train no.177100  loss = 6.27785 avg_loss = 3.43117\n",
      "epoch no.2 train no.177110  loss = 3.47132 avg_loss = 3.45476\n",
      "epoch no.2 train no.177120  loss = 1.21985 avg_loss = 3.43766\n",
      "epoch no.2 train no.177130  loss = 3.32739 avg_loss = 3.48149\n",
      "epoch no.2 train no.177140  loss = 2.57657 avg_loss = 3.48061\n",
      "epoch no.2 train no.177150  loss = 5.26590 avg_loss = 3.52236\n",
      "epoch no.2 train no.177160  loss = 2.65811 avg_loss = 3.51796\n",
      "epoch no.2 train no.177170  loss = 5.88069 avg_loss = 3.51647\n",
      "epoch no.2 train no.177180  loss = 3.72423 avg_loss = 3.49167\n",
      "epoch no.2 train no.177190  loss = 5.27639 avg_loss = 3.48944\n",
      "epoch no.2 train no.177200  loss = 3.90360 avg_loss = 3.48785\n",
      "epoch no.2 train no.177210  loss = 4.10873 avg_loss = 3.47119\n",
      "epoch no.2 train no.177220  loss = 6.10486 avg_loss = 3.55125\n",
      "epoch no.2 train no.177230  loss = 3.84174 avg_loss = 3.52577\n",
      "epoch no.2 train no.177240  loss = 3.94609 avg_loss = 3.51848\n",
      "epoch no.2 train no.177250  loss = 4.24075 avg_loss = 3.47052\n",
      "epoch no.2 train no.177260  loss = 2.94075 avg_loss = 3.50777\n",
      "epoch no.2 train no.177270  loss = 5.10892 avg_loss = 3.49455\n",
      "epoch no.2 train no.177280  loss = 4.13650 avg_loss = 3.53399\n",
      "epoch no.2 train no.177290  loss = 4.27667 avg_loss = 3.58633\n",
      "epoch no.2 train no.177300  loss = 2.91922 avg_loss = 3.56329\n",
      "epoch no.2 train no.177310  loss = 3.58640 avg_loss = 3.49833\n",
      "epoch no.2 train no.177320  loss = 4.95581 avg_loss = 3.52734\n",
      "epoch no.2 train no.177330  loss = 5.35687 avg_loss = 3.51705\n",
      "epoch no.2 train no.177340  loss = 4.12661 avg_loss = 3.51551\n",
      "epoch no.2 train no.177350  loss = 4.69697 avg_loss = 3.52087\n",
      "epoch no.2 train no.177360  loss = 3.08416 avg_loss = 3.53904\n",
      "epoch no.2 train no.177370  loss = 5.24076 avg_loss = 3.55373\n",
      "epoch no.2 train no.177380  loss = 2.80632 avg_loss = 3.54793\n",
      "epoch no.2 train no.177390  loss = 3.13744 avg_loss = 3.55998\n",
      "epoch no.2 train no.177400  loss = 4.48920 avg_loss = 3.57826\n",
      "epoch no.2 train no.177410  loss = 4.23193 avg_loss = 3.56215\n",
      "epoch no.2 train no.177420  loss = 2.99350 avg_loss = 3.58474\n",
      "epoch no.2 train no.177430  loss = 3.91838 avg_loss = 3.57094\n",
      "epoch no.2 train no.177440  loss = 2.30949 avg_loss = 3.56117\n",
      "epoch no.2 train no.177450  loss = 3.04444 avg_loss = 3.56418\n",
      "epoch no.2 train no.177460  loss = 3.54219 avg_loss = 3.55340\n",
      "epoch no.2 train no.177470  loss = 4.22661 avg_loss = 3.52609\n",
      "epoch no.2 train no.177480  loss = 2.46329 avg_loss = 3.52551\n",
      "epoch no.2 train no.177490  loss = 5.69050 avg_loss = 3.58349\n",
      "epoch no.2 train no.177500  loss = 2.46153 avg_loss = 3.56078\n",
      "epoch no.2 train no.177510  loss = 2.88231 avg_loss = 3.58458\n",
      "epoch no.2 train no.177520  loss = 2.71777 avg_loss = 3.58548\n",
      "epoch no.2 train no.177530  loss = 2.96494 avg_loss = 3.60243\n",
      "epoch no.2 train no.177540  loss = 5.00219 avg_loss = 3.62209\n",
      "epoch no.2 train no.177550  loss = 3.62545 avg_loss = 3.63132\n",
      "epoch no.2 train no.177560  loss = 3.15567 avg_loss = 3.62835\n",
      "epoch no.2 train no.177570  loss = 3.77662 avg_loss = 3.61232\n",
      "epoch no.2 train no.177580  loss = 2.85793 avg_loss = 3.65268\n",
      "epoch no.2 train no.177590  loss = 5.20553 avg_loss = 3.70163\n",
      "epoch no.2 train no.177600  loss = 3.79815 avg_loss = 3.72395\n",
      "epoch no.2 train no.177610  loss = 5.31554 avg_loss = 3.77624\n",
      "epoch no.2 train no.177620  loss = 4.66344 avg_loss = 3.75753\n",
      "epoch no.2 train no.177630  loss = 2.07738 avg_loss = 3.76988\n",
      "epoch no.2 train no.177640  loss = 2.54129 avg_loss = 3.68932\n",
      "epoch no.2 train no.177650  loss = 7.13654 avg_loss = 3.75139\n",
      "epoch no.2 train no.177660  loss = 4.65808 avg_loss = 3.77589\n",
      "epoch no.2 train no.177670  loss = 2.76379 avg_loss = 3.69930\n",
      "epoch no.2 train no.177680  loss = 3.79186 avg_loss = 3.69794\n",
      "epoch no.2 train no.177690  loss = 3.75414 avg_loss = 3.66558\n",
      "epoch no.2 train no.177700  loss = 2.76697 avg_loss = 3.68640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.177710  loss = 2.92745 avg_loss = 3.65605\n",
      "epoch no.2 train no.177720  loss = 2.62777 avg_loss = 3.63376\n",
      "epoch no.2 train no.177730  loss = 3.14728 avg_loss = 3.64270\n",
      "epoch no.2 train no.177740  loss = 3.18931 avg_loss = 3.64629\n",
      "epoch no.2 train no.177750  loss = 3.61356 avg_loss = 3.59557\n",
      "epoch no.2 train no.177760  loss = 4.64537 avg_loss = 3.58675\n",
      "epoch no.2 train no.177770  loss = 1.33453 avg_loss = 3.52863\n",
      "epoch no.2 train no.177780  loss = 3.27425 avg_loss = 3.49822\n",
      "epoch no.2 train no.177790  loss = 2.37691 avg_loss = 3.49995\n",
      "epoch no.2 train no.177800  loss = 2.83397 avg_loss = 3.55778\n",
      "epoch no.2 train no.177810  loss = 2.94107 avg_loss = 3.55557\n",
      "epoch no.2 train no.177820  loss = 3.34817 avg_loss = 3.53891\n",
      "epoch no.2 train no.177830  loss = 5.57095 avg_loss = 3.53288\n",
      "epoch no.2 train no.177840  loss = 4.86214 avg_loss = 3.58409\n",
      "epoch no.2 train no.177850  loss = 3.55685 avg_loss = 3.52530\n",
      "epoch no.2 train no.177860  loss = 4.25556 avg_loss = 3.55643\n",
      "epoch no.2 train no.177870  loss = 2.50331 avg_loss = 3.61872\n",
      "epoch no.2 train no.177880  loss = 4.27909 avg_loss = 3.61853\n",
      "epoch no.2 train no.177890  loss = 3.23551 avg_loss = 3.61181\n",
      "epoch no.2 train no.177900  loss = 2.34885 avg_loss = 3.60514\n",
      "epoch no.2 train no.177910  loss = 5.66821 avg_loss = 3.62154\n",
      "epoch no.2 train no.177920  loss = 2.92171 avg_loss = 3.60565\n",
      "epoch no.2 train no.177930  loss = 3.42272 avg_loss = 3.67479\n",
      "epoch no.2 train no.177940  loss = 3.84903 avg_loss = 3.64502\n",
      "epoch no.2 train no.177950  loss = 3.10804 avg_loss = 3.61472\n",
      "epoch no.2 train no.177960  loss = 1.97013 avg_loss = 3.58917\n",
      "epoch no.2 train no.177970  loss = 3.35935 avg_loss = 3.61164\n",
      "epoch no.2 train no.177980  loss = 2.36933 avg_loss = 3.57685\n",
      "epoch no.2 train no.177990  loss = 2.66756 avg_loss = 3.61324\n",
      "epoch no.2 train no.178000  loss = 3.42244 avg_loss = 3.60009\n",
      "5\n",
      "to_tokens: ['▁가을', '좋은', '이', '▁좋은', '▁좋은', '▁노래', '송', '</s>']\n",
      "기분전환에 딱 신나는 팝송</s>\n",
      "epoch no.2 train no.178010  loss = 2.07321 avg_loss = 3.62502\n",
      "epoch no.2 train no.178020  loss = 2.90878 avg_loss = 3.61342\n",
      "epoch no.2 train no.178030  loss = 3.81141 avg_loss = 3.64147\n",
      "epoch no.2 train no.178040  loss = 3.46784 avg_loss = 3.61185\n",
      "epoch no.2 train no.178050  loss = 2.22964 avg_loss = 3.63512\n",
      "epoch no.2 train no.178060  loss = 5.08043 avg_loss = 3.69433\n",
      "epoch no.2 train no.178070  loss = 3.29385 avg_loss = 3.67699\n",
      "epoch no.2 train no.178080  loss = 4.68569 avg_loss = 3.66451\n",
      "epoch no.2 train no.178090  loss = 3.88241 avg_loss = 3.61128\n",
      "epoch no.2 train no.178100  loss = 3.13315 avg_loss = 3.60539\n",
      "epoch no.2 train no.178110  loss = 4.36971 avg_loss = 3.61955\n",
      "epoch no.2 train no.178120  loss = 3.99202 avg_loss = 3.59733\n",
      "epoch no.2 train no.178130  loss = 3.98792 avg_loss = 3.56419\n",
      "epoch no.2 train no.178140  loss = 4.13327 avg_loss = 3.57521\n",
      "epoch no.2 train no.178150  loss = 2.65982 avg_loss = 3.58444\n",
      "epoch no.2 train no.178160  loss = 3.05069 avg_loss = 3.50992\n",
      "epoch no.2 train no.178170  loss = 4.69508 avg_loss = 3.52077\n",
      "epoch no.2 train no.178180  loss = 2.81386 avg_loss = 3.52391\n",
      "epoch no.2 train no.178190  loss = 2.79468 avg_loss = 3.55600\n",
      "epoch no.2 train no.178200  loss = 3.50380 avg_loss = 3.55353\n",
      "epoch no.2 train no.178210  loss = 2.64844 avg_loss = 3.53439\n",
      "epoch no.2 train no.178220  loss = 3.43416 avg_loss = 3.49330\n",
      "epoch no.2 train no.178230  loss = 4.16646 avg_loss = 3.47539\n",
      "epoch no.2 train no.178240  loss = 3.75726 avg_loss = 3.51008\n",
      "epoch no.2 train no.178250  loss = 2.42606 avg_loss = 3.46731\n",
      "epoch no.2 train no.178260  loss = 3.88045 avg_loss = 3.51201\n",
      "epoch no.2 train no.178270  loss = 3.42878 avg_loss = 3.53534\n",
      "epoch no.2 train no.178280  loss = 4.10518 avg_loss = 3.52728\n",
      "epoch no.2 train no.178290  loss = 4.88130 avg_loss = 3.52551\n",
      "epoch no.2 train no.178300  loss = 3.01076 avg_loss = 3.48825\n",
      "epoch no.2 train no.178310  loss = 3.73398 avg_loss = 3.50852\n",
      "epoch no.2 train no.178320  loss = 6.51481 avg_loss = 3.51723\n",
      "epoch no.2 train no.178330  loss = 5.80975 avg_loss = 3.54916\n",
      "epoch no.2 train no.178340  loss = 2.56352 avg_loss = 3.52354\n",
      "epoch no.2 train no.178350  loss = 3.39238 avg_loss = 3.52919\n",
      "epoch no.2 train no.178360  loss = 3.35901 avg_loss = 3.53597\n",
      "epoch no.2 train no.178370  loss = 3.67940 avg_loss = 3.53496\n",
      "epoch no.2 train no.178380  loss = 2.76959 avg_loss = 3.56350\n",
      "epoch no.2 train no.178390  loss = 3.24772 avg_loss = 3.59138\n",
      "epoch no.2 train no.178400  loss = 3.64760 avg_loss = 3.63257\n",
      "epoch no.2 train no.178410  loss = 3.98185 avg_loss = 3.61717\n",
      "epoch no.2 train no.178420  loss = 1.98789 avg_loss = 3.59617\n",
      "epoch no.2 train no.178430  loss = 4.74759 avg_loss = 3.52864\n",
      "epoch no.2 train no.178440  loss = 3.51559 avg_loss = 3.50015\n",
      "epoch no.2 train no.178450  loss = 4.42182 avg_loss = 3.53901\n",
      "epoch no.2 train no.178460  loss = 4.06262 avg_loss = 3.60960\n",
      "epoch no.2 train no.178470  loss = 3.43393 avg_loss = 3.53842\n",
      "epoch no.2 train no.178480  loss = 4.34186 avg_loss = 3.54092\n",
      "epoch no.2 train no.178490  loss = 2.72654 avg_loss = 3.56125\n",
      "epoch no.2 train no.178500  loss = 2.22844 avg_loss = 3.56672\n",
      "epoch no.2 train no.178510  loss = 3.76008 avg_loss = 3.58070\n",
      "epoch no.2 train no.178520  loss = 2.99446 avg_loss = 3.55431\n",
      "epoch no.2 train no.178530  loss = 3.88580 avg_loss = 3.56514\n",
      "epoch no.2 train no.178540  loss = 2.02334 avg_loss = 3.50233\n",
      "epoch no.2 train no.178550  loss = 5.09710 avg_loss = 3.53328\n",
      "epoch no.2 train no.178560  loss = 2.69867 avg_loss = 3.47568\n",
      "epoch no.2 train no.178570  loss = 3.10779 avg_loss = 3.43791\n",
      "epoch no.2 train no.178580  loss = 6.17844 avg_loss = 3.51078\n",
      "epoch no.2 train no.178590  loss = 3.22909 avg_loss = 3.50958\n",
      "epoch no.2 train no.178600  loss = 5.63871 avg_loss = 3.49845\n",
      "epoch no.2 train no.178610  loss = 3.27389 avg_loss = 3.48592\n",
      "epoch no.2 train no.178620  loss = 3.32503 avg_loss = 3.51991\n",
      "epoch no.2 train no.178630  loss = 3.89936 avg_loss = 3.53955\n",
      "epoch no.2 train no.178640  loss = 2.38114 avg_loss = 3.53217\n",
      "epoch no.2 train no.178650  loss = 2.94668 avg_loss = 3.53074\n",
      "epoch no.2 train no.178660  loss = 2.68705 avg_loss = 3.49838\n",
      "epoch no.2 train no.178670  loss = 4.51311 avg_loss = 3.51219\n",
      "epoch no.2 train no.178680  loss = 2.98242 avg_loss = 3.50553\n",
      "epoch no.2 train no.178690  loss = 5.50248 avg_loss = 3.53169\n",
      "epoch no.2 train no.178700  loss = 6.17965 avg_loss = 3.49767\n",
      "epoch no.2 train no.178710  loss = 6.57152 avg_loss = 3.48176\n",
      "epoch no.2 train no.178720  loss = 4.41434 avg_loss = 3.50672\n",
      "epoch no.2 train no.178730  loss = 3.60272 avg_loss = 3.54971\n",
      "epoch no.2 train no.178740  loss = 3.43700 avg_loss = 3.56625\n",
      "epoch no.2 train no.178750  loss = 3.83274 avg_loss = 3.53024\n",
      "epoch no.2 train no.178760  loss = 3.12132 avg_loss = 3.53118\n",
      "epoch no.2 train no.178770  loss = 3.66225 avg_loss = 3.49047\n",
      "epoch no.2 train no.178780  loss = 4.34116 avg_loss = 3.50106\n",
      "epoch no.2 train no.178790  loss = 2.44403 avg_loss = 3.47749\n",
      "epoch no.2 train no.178800  loss = 2.41156 avg_loss = 3.48943\n",
      "epoch no.2 train no.178810  loss = 5.19008 avg_loss = 3.52025\n",
      "epoch no.2 train no.178820  loss = 2.61920 avg_loss = 3.46924\n",
      "epoch no.2 train no.178830  loss = 2.05210 avg_loss = 3.48554\n",
      "epoch no.2 train no.178840  loss = 2.47398 avg_loss = 3.47158\n",
      "epoch no.2 train no.178850  loss = 4.20417 avg_loss = 3.47563\n",
      "epoch no.2 train no.178860  loss = 2.53560 avg_loss = 3.50024\n",
      "epoch no.2 train no.178870  loss = 3.59682 avg_loss = 3.51915\n",
      "epoch no.2 train no.178880  loss = 2.54091 avg_loss = 3.52279\n",
      "epoch no.2 train no.178890  loss = 3.67119 avg_loss = 3.49609\n",
      "epoch no.2 train no.178900  loss = 3.07611 avg_loss = 3.61378\n",
      "epoch no.2 train no.178910  loss = 3.72671 avg_loss = 3.60080\n",
      "epoch no.2 train no.178920  loss = 5.26882 avg_loss = 3.62391\n",
      "epoch no.2 train no.178930  loss = 2.81425 avg_loss = 3.64802\n",
      "epoch no.2 train no.178940  loss = 2.42805 avg_loss = 3.61827\n",
      "epoch no.2 train no.178950  loss = 3.69444 avg_loss = 3.64919\n",
      "epoch no.2 train no.178960  loss = 4.43642 avg_loss = 3.62164\n",
      "epoch no.2 train no.178970  loss = 2.97548 avg_loss = 3.57833\n",
      "epoch no.2 train no.178980  loss = 5.86454 avg_loss = 3.56960\n",
      "epoch no.2 train no.178990  loss = 4.83820 avg_loss = 3.54475\n",
      "epoch no.2 train no.179000  loss = 2.99208 avg_loss = 3.54309\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁필요할', '때', '▁듣는']\n",
      "기분전환이 필요할때</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.179010  loss = 4.56056 avg_loss = 3.55497\n",
      "epoch no.2 train no.179020  loss = 3.97045 avg_loss = 3.55303\n",
      "epoch no.2 train no.179030  loss = 2.77719 avg_loss = 3.56548\n",
      "epoch no.2 train no.179040  loss = 2.14374 avg_loss = 3.57501\n",
      "epoch no.2 train no.179050  loss = 3.68721 avg_loss = 3.56224\n",
      "epoch no.2 train no.179060  loss = 1.99520 avg_loss = 3.55009\n",
      "epoch no.2 train no.179070  loss = 3.35625 avg_loss = 3.56259\n",
      "epoch no.2 train no.179080  loss = 4.46659 avg_loss = 3.57725\n",
      "epoch no.2 train no.179090  loss = 3.62819 avg_loss = 3.53568\n",
      "epoch no.2 train no.179100  loss = 4.50587 avg_loss = 3.55815\n",
      "epoch no.2 train no.179110  loss = 3.09783 avg_loss = 3.56275\n",
      "epoch no.2 train no.179120  loss = 5.57443 avg_loss = 3.53317\n",
      "epoch no.2 train no.179130  loss = 2.72946 avg_loss = 3.50649\n",
      "epoch no.2 train no.179140  loss = 3.57079 avg_loss = 3.50163\n",
      "epoch no.2 train no.179150  loss = 2.63208 avg_loss = 3.52371\n",
      "epoch no.2 train no.179160  loss = 3.96758 avg_loss = 3.51309\n",
      "epoch no.2 train no.179170  loss = 4.99278 avg_loss = 3.52930\n",
      "epoch no.2 train no.179180  loss = 2.04924 avg_loss = 3.52978\n",
      "epoch no.2 train no.179190  loss = 2.64653 avg_loss = 3.52445\n",
      "epoch no.2 train no.179200  loss = 2.71699 avg_loss = 3.51597\n",
      "epoch no.2 train no.179210  loss = 3.45519 avg_loss = 3.48316\n",
      "epoch no.2 train no.179220  loss = 4.40279 avg_loss = 3.49964\n",
      "epoch no.2 train no.179230  loss = 5.15041 avg_loss = 3.50532\n",
      "epoch no.2 train no.179240  loss = 4.23746 avg_loss = 3.51935\n",
      "epoch no.2 train no.179250  loss = 3.23330 avg_loss = 3.51289\n",
      "epoch no.2 train no.179260  loss = 4.69767 avg_loss = 3.49021\n",
      "epoch no.2 train no.179270  loss = 2.52492 avg_loss = 3.46973\n",
      "epoch no.2 train no.179280  loss = 4.22360 avg_loss = 3.49529\n",
      "epoch no.2 train no.179290  loss = 2.09995 avg_loss = 3.42782\n",
      "epoch no.2 train no.179300  loss = 1.90137 avg_loss = 3.42832\n",
      "epoch no.2 train no.179310  loss = 3.17805 avg_loss = 3.50319\n",
      "epoch no.2 train no.179320  loss = 4.71232 avg_loss = 3.50164\n",
      "epoch no.2 train no.179330  loss = 3.50261 avg_loss = 3.51291\n",
      "epoch no.2 train no.179340  loss = 2.93001 avg_loss = 3.50290\n",
      "epoch no.2 train no.179350  loss = 5.54325 avg_loss = 3.56352\n",
      "epoch no.2 train no.179360  loss = 2.85080 avg_loss = 3.57119\n",
      "epoch no.2 train no.179370  loss = 2.36203 avg_loss = 3.55351\n",
      "epoch no.2 train no.179380  loss = 2.58939 avg_loss = 3.56732\n",
      "epoch no.2 train no.179390  loss = 3.00530 avg_loss = 3.56508\n",
      "epoch no.2 train no.179400  loss = 4.55137 avg_loss = 3.61681\n",
      "epoch no.2 train no.179410  loss = 2.44792 avg_loss = 3.59516\n",
      "epoch no.2 train no.179420  loss = 3.68723 avg_loss = 3.58303\n",
      "epoch no.2 train no.179430  loss = 4.09410 avg_loss = 3.53106\n",
      "epoch no.2 train no.179440  loss = 2.31979 avg_loss = 3.53107\n",
      "epoch no.2 train no.179450  loss = 5.39561 avg_loss = 3.59354\n",
      "epoch no.2 train no.179460  loss = 2.19428 avg_loss = 3.55455\n",
      "epoch no.2 train no.179470  loss = 4.72660 avg_loss = 3.56461\n",
      "epoch no.2 train no.179480  loss = 4.02040 avg_loss = 3.56083\n",
      "epoch no.2 train no.179490  loss = 2.61696 avg_loss = 3.55439\n",
      "epoch no.2 train no.179500  loss = 3.58368 avg_loss = 3.58592\n",
      "epoch no.2 train no.179510  loss = 3.34766 avg_loss = 3.54867\n",
      "epoch no.2 train no.179520  loss = 5.48260 avg_loss = 3.55363\n",
      "epoch no.2 train no.179530  loss = 4.07716 avg_loss = 3.56253\n",
      "epoch no.2 train no.179540  loss = 3.58246 avg_loss = 3.57065\n",
      "epoch no.2 train no.179550  loss = 3.12512 avg_loss = 3.57549\n",
      "epoch no.2 train no.179560  loss = 2.95794 avg_loss = 3.55391\n",
      "epoch no.2 train no.179570  loss = 3.68676 avg_loss = 3.58282\n",
      "epoch no.2 train no.179580  loss = 2.70920 avg_loss = 3.60516\n",
      "epoch no.2 train no.179590  loss = 3.47645 avg_loss = 3.59285\n",
      "epoch no.2 train no.179600  loss = 3.53418 avg_loss = 3.60307\n",
      "epoch no.2 train no.179610  loss = 5.68666 avg_loss = 3.59920\n",
      "epoch no.2 train no.179620  loss = 3.24508 avg_loss = 3.60520\n",
      "epoch no.2 train no.179630  loss = 4.09646 avg_loss = 3.58566\n",
      "epoch no.2 train no.179640  loss = 2.19544 avg_loss = 3.54526\n",
      "epoch no.2 train no.179650  loss = 4.08667 avg_loss = 3.51411\n",
      "epoch no.2 train no.179660  loss = 3.44668 avg_loss = 3.54488\n",
      "epoch no.2 train no.179670  loss = 2.77356 avg_loss = 3.54894\n",
      "epoch no.2 train no.179680  loss = 4.39042 avg_loss = 3.49404\n",
      "epoch no.2 train no.179690  loss = 3.65676 avg_loss = 3.57758\n",
      "epoch no.2 train no.179700  loss = 5.14838 avg_loss = 3.53953\n",
      "epoch no.2 train no.179710  loss = 3.24808 avg_loss = 3.57604\n",
      "epoch no.2 train no.179720  loss = 2.28620 avg_loss = 3.56135\n",
      "epoch no.2 train no.179730  loss = 2.78590 avg_loss = 3.49919\n",
      "epoch no.2 train no.179740  loss = 2.94842 avg_loss = 3.51923\n",
      "epoch no.2 train no.179750  loss = 4.60037 avg_loss = 3.49319\n",
      "epoch no.2 train no.179760  loss = 3.13339 avg_loss = 3.48832\n",
      "epoch no.2 train no.179770  loss = 3.61710 avg_loss = 3.52312\n",
      "epoch no.2 train no.179780  loss = 3.59052 avg_loss = 3.49753\n",
      "epoch no.2 train no.179790  loss = 4.21299 avg_loss = 3.48248\n",
      "epoch no.2 train no.179800  loss = 3.64509 avg_loss = 3.52672\n",
      "epoch no.2 train no.179810  loss = 3.01807 avg_loss = 3.50780\n",
      "epoch no.2 train no.179820  loss = 2.86148 avg_loss = 3.49387\n",
      "epoch no.2 train no.179830  loss = 1.97751 avg_loss = 3.47594\n",
      "epoch no.2 train no.179840  loss = 2.65212 avg_loss = 3.48938\n",
      "epoch no.2 train no.179850  loss = 2.73645 avg_loss = 3.47360\n",
      "epoch no.2 train no.179860  loss = 3.20835 avg_loss = 3.49833\n",
      "epoch no.2 train no.179870  loss = 3.37828 avg_loss = 3.53412\n",
      "epoch no.2 train no.179880  loss = 5.52953 avg_loss = 3.56893\n",
      "epoch no.2 train no.179890  loss = 4.24187 avg_loss = 3.61865\n",
      "epoch no.2 train no.179900  loss = 5.16260 avg_loss = 3.64233\n",
      "epoch no.2 train no.179910  loss = 4.88103 avg_loss = 3.63148\n",
      "epoch no.2 train no.179920  loss = 2.39532 avg_loss = 3.60542\n",
      "epoch no.2 train no.179930  loss = 2.49084 avg_loss = 3.61488\n",
      "epoch no.2 train no.179940  loss = 2.36852 avg_loss = 3.57640\n",
      "epoch no.2 train no.179950  loss = 4.50270 avg_loss = 3.56544\n",
      "epoch no.2 train no.179960  loss = 4.11991 avg_loss = 3.56126\n",
      "epoch no.2 train no.179970  loss = 3.95708 avg_loss = 3.57063\n",
      "epoch no.2 train no.179980  loss = 4.39206 avg_loss = 3.54174\n",
      "epoch no.2 train no.179990  loss = 3.90778 avg_loss = 3.53292\n",
      "epoch no.2 train no.180000  loss = 3.03718 avg_loss = 3.52532\n",
      "3\n",
      "to_tokens: ['▁가을', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.2 train no.180010  loss = 2.57823 avg_loss = 3.45917\n",
      "epoch no.2 train no.180020  loss = 2.96081 avg_loss = 3.46033\n",
      "epoch no.2 train no.180030  loss = 3.36479 avg_loss = 3.45371\n",
      "epoch no.2 train no.180040  loss = 3.34012 avg_loss = 3.48820\n",
      "epoch no.2 train no.180050  loss = 2.73249 avg_loss = 3.47455\n",
      "epoch no.2 train no.180060  loss = 3.22000 avg_loss = 3.51708\n",
      "epoch no.2 train no.180070  loss = 3.38902 avg_loss = 3.52294\n",
      "epoch no.2 train no.180080  loss = 3.22703 avg_loss = 3.53598\n",
      "epoch no.2 train no.180090  loss = 1.92232 avg_loss = 3.47103\n",
      "epoch no.2 train no.180100  loss = 2.38925 avg_loss = 3.42080\n",
      "epoch no.2 train no.180110  loss = 4.27557 avg_loss = 3.44957\n",
      "epoch no.2 train no.180120  loss = 4.34385 avg_loss = 3.39395\n",
      "epoch no.2 train no.180130  loss = 4.34364 avg_loss = 3.36527\n",
      "epoch no.2 train no.180140  loss = 4.36975 avg_loss = 3.37142\n",
      "epoch no.2 train no.180150  loss = 2.37080 avg_loss = 3.35603\n",
      "epoch no.2 train no.180160  loss = 3.37529 avg_loss = 3.34123\n",
      "epoch no.2 train no.180170  loss = 4.77946 avg_loss = 3.35992\n",
      "epoch no.2 train no.180180  loss = 3.00227 avg_loss = 3.39521\n",
      "epoch no.2 train no.180190  loss = 3.87164 avg_loss = 3.45005\n",
      "epoch no.2 train no.180200  loss = 3.31360 avg_loss = 3.43643\n",
      "epoch no.2 train no.180210  loss = 2.22970 avg_loss = 3.46677\n",
      "epoch no.2 train no.180220  loss = 2.58197 avg_loss = 3.47029\n",
      "epoch no.2 train no.180230  loss = 2.97432 avg_loss = 3.47080\n",
      "epoch no.2 train no.180240  loss = 3.14689 avg_loss = 3.46681\n",
      "epoch no.2 train no.180250  loss = 3.41965 avg_loss = 3.51655\n",
      "epoch no.2 train no.180260  loss = 3.32650 avg_loss = 3.56614\n",
      "epoch no.2 train no.180270  loss = 3.97280 avg_loss = 3.55467\n",
      "epoch no.2 train no.180280  loss = 4.83585 avg_loss = 3.58000\n",
      "epoch no.2 train no.180290  loss = 4.79036 avg_loss = 3.63857\n",
      "epoch no.2 train no.180300  loss = 2.50179 avg_loss = 3.57611\n",
      "epoch no.2 train no.180310  loss = 5.07755 avg_loss = 3.56770\n",
      "epoch no.2 train no.180320  loss = 3.41494 avg_loss = 3.55176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.180330  loss = 3.88559 avg_loss = 3.56830\n",
      "epoch no.2 train no.180340  loss = 4.34044 avg_loss = 3.60738\n",
      "epoch no.2 train no.180350  loss = 4.23629 avg_loss = 3.57783\n",
      "epoch no.2 train no.180360  loss = 6.06608 avg_loss = 3.61905\n",
      "epoch no.2 train no.180370  loss = 5.07545 avg_loss = 3.61080\n",
      "epoch no.2 train no.180380  loss = 4.79761 avg_loss = 3.58396\n",
      "epoch no.2 train no.180390  loss = 5.05711 avg_loss = 3.60321\n",
      "epoch no.2 train no.180400  loss = 3.75087 avg_loss = 3.57803\n",
      "epoch no.2 train no.180410  loss = 3.16225 avg_loss = 3.56209\n",
      "epoch no.2 train no.180420  loss = 4.28709 avg_loss = 3.55218\n",
      "epoch no.2 train no.180430  loss = 2.47069 avg_loss = 3.49611\n",
      "epoch no.2 train no.180440  loss = 3.55900 avg_loss = 3.49021\n",
      "epoch no.2 train no.180450  loss = 4.25181 avg_loss = 3.49004\n",
      "epoch no.2 train no.180460  loss = 3.47822 avg_loss = 3.49692\n",
      "epoch no.2 train no.180470  loss = 3.82346 avg_loss = 3.48185\n",
      "epoch no.2 train no.180480  loss = 3.67509 avg_loss = 3.51619\n",
      "epoch no.2 train no.180490  loss = 3.69424 avg_loss = 3.54663\n",
      "epoch no.2 train no.180500  loss = 3.30212 avg_loss = 3.54544\n",
      "epoch no.2 train no.180510  loss = 4.70014 avg_loss = 3.54664\n",
      "epoch no.2 train no.180520  loss = 3.11808 avg_loss = 3.53981\n",
      "epoch no.2 train no.180530  loss = 2.97103 avg_loss = 3.52181\n",
      "epoch no.2 train no.180540  loss = 3.92065 avg_loss = 3.52266\n",
      "epoch no.2 train no.180550  loss = 2.80623 avg_loss = 3.52096\n",
      "epoch no.2 train no.180560  loss = 3.43618 avg_loss = 3.59147\n",
      "epoch no.2 train no.180570  loss = 2.38919 avg_loss = 3.61302\n",
      "epoch no.2 train no.180580  loss = 3.88426 avg_loss = 3.57344\n",
      "epoch no.2 train no.180590  loss = 3.98851 avg_loss = 3.58325\n",
      "epoch no.2 train no.180600  loss = 4.37435 avg_loss = 3.57860\n",
      "epoch no.2 train no.180610  loss = 3.70683 avg_loss = 3.55558\n",
      "epoch no.2 train no.180620  loss = 6.93314 avg_loss = 3.60060\n",
      "epoch no.2 train no.180630  loss = 4.23427 avg_loss = 3.58277\n",
      "epoch no.2 train no.180640  loss = 5.57759 avg_loss = 3.58918\n",
      "epoch no.2 train no.180650  loss = 2.60265 avg_loss = 3.56051\n",
      "epoch no.2 train no.180660  loss = 3.14174 avg_loss = 3.61228\n",
      "epoch no.2 train no.180670  loss = 3.13678 avg_loss = 3.60129\n",
      "epoch no.2 train no.180680  loss = 4.21921 avg_loss = 3.61076\n",
      "epoch no.2 train no.180690  loss = 2.89073 avg_loss = 3.56212\n",
      "epoch no.2 train no.180700  loss = 4.88112 avg_loss = 3.59688\n",
      "epoch no.2 train no.180710  loss = 4.06543 avg_loss = 3.59409\n",
      "epoch no.2 train no.180720  loss = 2.93160 avg_loss = 3.61724\n",
      "epoch no.2 train no.180730  loss = 4.15273 avg_loss = 3.62483\n",
      "epoch no.2 train no.180740  loss = 2.33240 avg_loss = 3.60229\n",
      "epoch no.2 train no.180750  loss = 3.72561 avg_loss = 3.59532\n",
      "epoch no.2 train no.180760  loss = 3.05616 avg_loss = 3.55770\n",
      "epoch no.2 train no.180770  loss = 3.44194 avg_loss = 3.55355\n",
      "epoch no.2 train no.180780  loss = 3.62287 avg_loss = 3.57927\n",
      "epoch no.2 train no.180790  loss = 3.50787 avg_loss = 3.60527\n",
      "epoch no.2 train no.180800  loss = 3.97615 avg_loss = 3.60223\n",
      "epoch no.2 train no.180810  loss = 4.29156 avg_loss = 3.63976\n",
      "epoch no.2 train no.180820  loss = 3.32299 avg_loss = 3.58740\n",
      "epoch no.2 train no.180830  loss = 2.41691 avg_loss = 3.55394\n",
      "epoch no.2 train no.180840  loss = 3.65251 avg_loss = 3.51086\n",
      "epoch no.2 train no.180850  loss = 2.77149 avg_loss = 3.48938\n",
      "epoch no.2 train no.180860  loss = 4.87906 avg_loss = 3.54636\n",
      "epoch no.2 train no.180870  loss = 4.94841 avg_loss = 3.56269\n",
      "epoch no.2 train no.180880  loss = 2.72504 avg_loss = 3.56475\n",
      "epoch no.2 train no.180890  loss = 2.85790 avg_loss = 3.57612\n",
      "epoch no.2 train no.180900  loss = 4.81652 avg_loss = 3.55032\n",
      "epoch no.2 train no.180910  loss = 3.94593 avg_loss = 3.54301\n",
      "epoch no.2 train no.180920  loss = 1.99520 avg_loss = 3.53851\n",
      "epoch no.2 train no.180930  loss = 4.06583 avg_loss = 3.56288\n",
      "epoch no.2 train no.180940  loss = 0.92665 avg_loss = 3.55849\n",
      "epoch no.2 train no.180950  loss = 3.92919 avg_loss = 3.61465\n",
      "epoch no.2 train no.180960  loss = 3.64053 avg_loss = 3.56846\n",
      "epoch no.2 train no.180970  loss = 3.98815 avg_loss = 3.59223\n",
      "epoch no.2 train no.180980  loss = 4.63277 avg_loss = 3.56806\n",
      "epoch no.2 train no.180990  loss = 2.41828 avg_loss = 3.57169\n",
      "epoch no.2 train no.181000  loss = 4.63281 avg_loss = 3.61133\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.181010  loss = 6.01745 avg_loss = 3.59813\n",
      "epoch no.2 train no.181020  loss = 3.71174 avg_loss = 3.60852\n",
      "epoch no.2 train no.181030  loss = 4.28920 avg_loss = 3.62266\n",
      "epoch no.2 train no.181040  loss = 4.17959 avg_loss = 3.62374\n",
      "epoch no.2 train no.181050  loss = 3.49550 avg_loss = 3.63143\n",
      "epoch no.2 train no.181060  loss = 2.73769 avg_loss = 3.61292\n",
      "epoch no.2 train no.181070  loss = 4.02098 avg_loss = 3.60503\n",
      "epoch no.2 train no.181080  loss = 3.25551 avg_loss = 3.60482\n",
      "epoch no.2 train no.181090  loss = 3.63054 avg_loss = 3.59379\n",
      "epoch no.2 train no.181100  loss = 2.89323 avg_loss = 3.58678\n",
      "epoch no.2 train no.181110  loss = 3.58498 avg_loss = 3.60590\n",
      "epoch no.2 train no.181120  loss = 2.39249 avg_loss = 3.59662\n",
      "epoch no.2 train no.181130  loss = 2.15616 avg_loss = 3.56839\n",
      "epoch no.2 train no.181140  loss = 4.81892 avg_loss = 3.52336\n",
      "epoch no.2 train no.181150  loss = 5.14314 avg_loss = 3.53557\n",
      "epoch no.2 train no.181160  loss = 2.43151 avg_loss = 3.54109\n",
      "epoch no.2 train no.181170  loss = 4.70808 avg_loss = 3.61439\n",
      "epoch no.2 train no.181180  loss = 3.94817 avg_loss = 3.63164\n",
      "epoch no.2 train no.181190  loss = 3.69410 avg_loss = 3.59409\n",
      "epoch no.2 train no.181200  loss = 2.34568 avg_loss = 3.56014\n",
      "epoch no.2 train no.181210  loss = 4.82752 avg_loss = 3.57337\n",
      "epoch no.2 train no.181220  loss = 2.96965 avg_loss = 3.60780\n",
      "epoch no.2 train no.181230  loss = 3.43417 avg_loss = 3.56693\n",
      "epoch no.2 train no.181240  loss = 3.84376 avg_loss = 3.61520\n",
      "epoch no.2 train no.181250  loss = 3.16185 avg_loss = 3.61465\n",
      "epoch no.2 train no.181260  loss = 5.33481 avg_loss = 3.61892\n",
      "epoch no.2 train no.181270  loss = 2.89640 avg_loss = 3.57566\n",
      "epoch no.2 train no.181280  loss = 4.63525 avg_loss = 3.54013\n",
      "epoch no.2 train no.181290  loss = 3.16442 avg_loss = 3.51079\n",
      "epoch no.2 train no.181300  loss = 5.30170 avg_loss = 3.54662\n",
      "epoch no.2 train no.181310  loss = 5.99313 avg_loss = 3.54399\n",
      "epoch no.2 train no.181320  loss = 3.84973 avg_loss = 3.59913\n",
      "epoch no.2 train no.181330  loss = 2.27291 avg_loss = 3.59829\n",
      "epoch no.2 train no.181340  loss = 4.16558 avg_loss = 3.57212\n",
      "epoch no.2 train no.181350  loss = 2.62883 avg_loss = 3.55134\n",
      "epoch no.2 train no.181360  loss = 2.84959 avg_loss = 3.52858\n",
      "epoch no.2 train no.181370  loss = 2.52722 avg_loss = 3.54722\n",
      "epoch no.2 train no.181380  loss = 2.95218 avg_loss = 3.48377\n",
      "epoch no.2 train no.181390  loss = 3.57829 avg_loss = 3.52457\n",
      "epoch no.2 train no.181400  loss = 2.14825 avg_loss = 3.49899\n",
      "epoch no.2 train no.181410  loss = 4.93580 avg_loss = 3.49964\n",
      "epoch no.2 train no.181420  loss = 3.63143 avg_loss = 3.50466\n",
      "epoch no.2 train no.181430  loss = 2.82575 avg_loss = 3.42718\n",
      "epoch no.2 train no.181440  loss = 4.22908 avg_loss = 3.42344\n",
      "epoch no.2 train no.181450  loss = 2.91970 avg_loss = 3.36774\n",
      "epoch no.2 train no.181460  loss = 2.57423 avg_loss = 3.40052\n",
      "epoch no.2 train no.181470  loss = 3.61559 avg_loss = 3.40426\n",
      "epoch no.2 train no.181480  loss = 4.39103 avg_loss = 3.40428\n",
      "epoch no.2 train no.181490  loss = 3.95312 avg_loss = 3.41840\n",
      "epoch no.2 train no.181500  loss = 2.22988 avg_loss = 3.41472\n",
      "epoch no.2 train no.181510  loss = 5.79001 avg_loss = 3.46483\n",
      "epoch no.2 train no.181520  loss = 7.49641 avg_loss = 3.53275\n",
      "epoch no.2 train no.181530  loss = 3.83517 avg_loss = 3.56899\n",
      "epoch no.2 train no.181540  loss = 2.17963 avg_loss = 3.55158\n",
      "epoch no.2 train no.181550  loss = 2.62361 avg_loss = 3.59618\n",
      "epoch no.2 train no.181560  loss = 2.10744 avg_loss = 3.55079\n",
      "epoch no.2 train no.181570  loss = 4.80136 avg_loss = 3.55000\n",
      "epoch no.2 train no.181580  loss = 3.46132 avg_loss = 3.57452\n",
      "epoch no.2 train no.181590  loss = 5.58967 avg_loss = 3.59229\n",
      "epoch no.2 train no.181600  loss = 5.05331 avg_loss = 3.58963\n",
      "epoch no.2 train no.181610  loss = 3.55269 avg_loss = 3.56465\n",
      "epoch no.2 train no.181620  loss = 2.12366 avg_loss = 3.55414\n",
      "epoch no.2 train no.181630  loss = 4.19774 avg_loss = 3.63040\n",
      "epoch no.2 train no.181640  loss = 4.78855 avg_loss = 3.65134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.181650  loss = 2.74197 avg_loss = 3.60443\n",
      "epoch no.2 train no.181660  loss = 2.13406 avg_loss = 3.66141\n",
      "epoch no.2 train no.181670  loss = 6.69644 avg_loss = 3.69802\n",
      "epoch no.2 train no.181680  loss = 4.18359 avg_loss = 3.77162\n",
      "epoch no.2 train no.181690  loss = 3.13705 avg_loss = 3.75649\n",
      "epoch no.2 train no.181700  loss = 2.42082 avg_loss = 3.67957\n",
      "epoch no.2 train no.181710  loss = 3.34653 avg_loss = 3.69454\n",
      "epoch no.2 train no.181720  loss = 3.59688 avg_loss = 3.67241\n",
      "epoch no.2 train no.181730  loss = 4.96215 avg_loss = 3.71073\n",
      "epoch no.2 train no.181740  loss = 5.61724 avg_loss = 3.70141\n",
      "epoch no.2 train no.181750  loss = 3.92100 avg_loss = 3.67063\n",
      "epoch no.2 train no.181760  loss = 2.67415 avg_loss = 3.67567\n",
      "epoch no.2 train no.181770  loss = 5.80890 avg_loss = 3.71244\n",
      "epoch no.2 train no.181780  loss = 4.48551 avg_loss = 3.75193\n",
      "epoch no.2 train no.181790  loss = 4.33803 avg_loss = 3.74290\n",
      "epoch no.2 train no.181800  loss = 3.64233 avg_loss = 3.77305\n",
      "epoch no.2 train no.181810  loss = 3.12271 avg_loss = 3.78914\n",
      "epoch no.2 train no.181820  loss = 2.82496 avg_loss = 3.74312\n",
      "epoch no.2 train no.181830  loss = 1.96782 avg_loss = 3.78402\n",
      "epoch no.2 train no.181840  loss = 5.05771 avg_loss = 3.78445\n",
      "epoch no.2 train no.181850  loss = 4.30486 avg_loss = 3.80759\n",
      "epoch no.2 train no.181860  loss = 3.96210 avg_loss = 3.80160\n",
      "epoch no.2 train no.181870  loss = 2.72112 avg_loss = 3.77186\n",
      "epoch no.2 train no.181880  loss = 4.12094 avg_loss = 3.73875\n",
      "epoch no.2 train no.181890  loss = 4.10612 avg_loss = 3.72028\n",
      "epoch no.2 train no.181900  loss = 2.58296 avg_loss = 3.70357\n",
      "epoch no.2 train no.181910  loss = 2.28189 avg_loss = 3.66429\n",
      "epoch no.2 train no.181920  loss = 3.61104 avg_loss = 3.67005\n",
      "epoch no.2 train no.181930  loss = 3.34829 avg_loss = 3.65630\n",
      "epoch no.2 train no.181940  loss = 5.92606 avg_loss = 3.64161\n",
      "epoch no.2 train no.181950  loss = 3.11917 avg_loss = 3.59513\n",
      "epoch no.2 train no.181960  loss = 4.35009 avg_loss = 3.59667\n",
      "epoch no.2 train no.181970  loss = 5.76755 avg_loss = 3.63591\n",
      "epoch no.2 train no.181980  loss = 4.89963 avg_loss = 3.64986\n",
      "epoch no.2 train no.181990  loss = 2.56816 avg_loss = 3.62856\n",
      "epoch no.2 train no.182000  loss = 3.82703 avg_loss = 3.62271\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '용', '▁위한', '▁음악', '전환', '▁팝', '</s>']\n",
      "기분전환을 위한 기분전환 노래</s>\n",
      "epoch no.2 train no.182010  loss = 4.30235 avg_loss = 3.60665\n",
      "epoch no.2 train no.182020  loss = 5.76788 avg_loss = 3.62948\n",
      "epoch no.2 train no.182030  loss = 2.64561 avg_loss = 3.57221\n",
      "epoch no.2 train no.182040  loss = 5.19261 avg_loss = 3.57153\n",
      "epoch no.2 train no.182050  loss = 2.19145 avg_loss = 3.53272\n",
      "epoch no.2 train no.182060  loss = 7.16584 avg_loss = 3.58404\n",
      "epoch no.2 train no.182070  loss = 5.49680 avg_loss = 3.59760\n",
      "epoch no.2 train no.182080  loss = 2.26145 avg_loss = 3.57002\n",
      "epoch no.2 train no.182090  loss = 3.56363 avg_loss = 3.61469\n",
      "epoch no.2 train no.182100  loss = 4.62758 avg_loss = 3.61119\n",
      "epoch no.2 train no.182110  loss = 4.15866 avg_loss = 3.59751\n",
      "epoch no.2 train no.182120  loss = 3.77181 avg_loss = 3.55850\n",
      "epoch no.2 train no.182130  loss = 5.64704 avg_loss = 3.54083\n",
      "epoch no.2 train no.182140  loss = 3.87726 avg_loss = 3.52956\n",
      "epoch no.2 train no.182150  loss = 3.49488 avg_loss = 3.54197\n",
      "epoch no.2 train no.182160  loss = 3.81056 avg_loss = 3.58688\n",
      "epoch no.2 train no.182170  loss = 3.44151 avg_loss = 3.59822\n",
      "epoch no.2 train no.182180  loss = 4.23788 avg_loss = 3.63379\n",
      "epoch no.2 train no.182190  loss = 4.81518 avg_loss = 3.65977\n",
      "epoch no.2 train no.182200  loss = 3.31880 avg_loss = 3.60479\n",
      "epoch no.2 train no.182210  loss = 2.15503 avg_loss = 3.59991\n",
      "epoch no.2 train no.182220  loss = 3.50863 avg_loss = 3.59556\n",
      "epoch no.2 train no.182230  loss = 2.89065 avg_loss = 3.52131\n",
      "epoch no.2 train no.182240  loss = 3.90242 avg_loss = 3.53262\n",
      "epoch no.2 train no.182250  loss = 2.68869 avg_loss = 3.52991\n",
      "epoch no.2 train no.182260  loss = 4.19941 avg_loss = 3.53216\n",
      "epoch no.2 train no.182270  loss = 2.50534 avg_loss = 3.50121\n",
      "epoch no.2 train no.182280  loss = 2.47126 avg_loss = 3.51529\n",
      "epoch no.2 train no.182290  loss = 5.37158 avg_loss = 3.49490\n",
      "epoch no.2 train no.182300  loss = 3.36710 avg_loss = 3.53536\n",
      "epoch no.2 train no.182310  loss = 5.81858 avg_loss = 3.56649\n",
      "epoch no.2 train no.182320  loss = 2.59682 avg_loss = 3.54000\n",
      "epoch no.2 train no.182330  loss = 2.20317 avg_loss = 3.54423\n",
      "epoch no.2 train no.182340  loss = 3.21256 avg_loss = 3.55346\n",
      "epoch no.2 train no.182350  loss = 6.72163 avg_loss = 3.59310\n",
      "epoch no.2 train no.182360  loss = 4.07482 avg_loss = 3.58820\n",
      "epoch no.2 train no.182370  loss = 2.36496 avg_loss = 3.53398\n",
      "epoch no.2 train no.182380  loss = 3.65113 avg_loss = 3.55087\n",
      "epoch no.2 train no.182390  loss = 4.08541 avg_loss = 3.53703\n",
      "epoch no.2 train no.182400  loss = 5.49277 avg_loss = 3.52635\n",
      "epoch no.2 train no.182410  loss = 2.39432 avg_loss = 3.47699\n",
      "epoch no.2 train no.182420  loss = 4.14037 avg_loss = 3.44758\n",
      "epoch no.2 train no.182430  loss = 5.37303 avg_loss = 3.49184\n",
      "epoch no.2 train no.182440  loss = 3.76666 avg_loss = 3.44839\n",
      "epoch no.2 train no.182450  loss = 4.45618 avg_loss = 3.44001\n",
      "epoch no.2 train no.182460  loss = 2.37374 avg_loss = 3.42447\n",
      "epoch no.2 train no.182470  loss = 3.11933 avg_loss = 3.50740\n",
      "epoch no.2 train no.182480  loss = 2.97360 avg_loss = 3.49553\n",
      "epoch no.2 train no.182490  loss = 3.41794 avg_loss = 3.50180\n",
      "epoch no.2 train no.182500  loss = 5.46723 avg_loss = 3.53950\n",
      "epoch no.2 train no.182510  loss = 3.95932 avg_loss = 3.63095\n",
      "epoch no.2 train no.182520  loss = 6.40057 avg_loss = 3.63314\n",
      "epoch no.2 train no.182530  loss = 3.66751 avg_loss = 3.60391\n",
      "epoch no.2 train no.182540  loss = 2.83338 avg_loss = 3.58292\n",
      "epoch no.2 train no.182550  loss = 3.99787 avg_loss = 3.51414\n",
      "epoch no.2 train no.182560  loss = 4.01776 avg_loss = 3.50161\n",
      "epoch no.2 train no.182570  loss = 3.86874 avg_loss = 3.50683\n",
      "epoch no.2 train no.182580  loss = 4.18122 avg_loss = 3.52982\n",
      "epoch no.2 train no.182590  loss = 4.74927 avg_loss = 3.49716\n",
      "epoch no.2 train no.182600  loss = 1.92898 avg_loss = 3.43267\n",
      "epoch no.2 train no.182610  loss = 4.11408 avg_loss = 3.41229\n",
      "epoch no.2 train no.182620  loss = 3.94862 avg_loss = 3.46647\n",
      "epoch no.2 train no.182630  loss = 3.94401 avg_loss = 3.48327\n",
      "epoch no.2 train no.182640  loss = 3.05387 avg_loss = 3.44191\n",
      "epoch no.2 train no.182650  loss = 3.79986 avg_loss = 3.50727\n",
      "epoch no.2 train no.182660  loss = 3.22474 avg_loss = 3.53362\n",
      "epoch no.2 train no.182670  loss = 3.80319 avg_loss = 3.53246\n",
      "epoch no.2 train no.182680  loss = 3.44636 avg_loss = 3.48241\n",
      "epoch no.2 train no.182690  loss = 4.03412 avg_loss = 3.54123\n",
      "epoch no.2 train no.182700  loss = 5.05388 avg_loss = 3.59810\n",
      "epoch no.2 train no.182710  loss = 2.47532 avg_loss = 3.59819\n",
      "epoch no.2 train no.182720  loss = 3.59889 avg_loss = 3.58436\n",
      "epoch no.2 train no.182730  loss = 2.41169 avg_loss = 3.54600\n",
      "epoch no.2 train no.182740  loss = 2.64917 avg_loss = 3.53704\n",
      "epoch no.2 train no.182750  loss = 2.42221 avg_loss = 3.50873\n",
      "epoch no.2 train no.182760  loss = 5.59353 avg_loss = 3.55840\n",
      "epoch no.2 train no.182770  loss = 4.68382 avg_loss = 3.61992\n",
      "epoch no.2 train no.182780  loss = 3.72411 avg_loss = 3.64251\n",
      "epoch no.2 train no.182790  loss = 2.60557 avg_loss = 3.60778\n",
      "epoch no.2 train no.182800  loss = 2.37162 avg_loss = 3.61469\n",
      "epoch no.2 train no.182810  loss = 5.62004 avg_loss = 3.61714\n",
      "epoch no.2 train no.182820  loss = 4.36260 avg_loss = 3.60744\n",
      "epoch no.2 train no.182830  loss = 2.57873 avg_loss = 3.58771\n",
      "epoch no.2 train no.182840  loss = 4.34487 avg_loss = 3.55198\n",
      "epoch no.2 train no.182850  loss = 4.80953 avg_loss = 3.54199\n",
      "epoch no.2 train no.182860  loss = 3.16688 avg_loss = 3.52102\n",
      "epoch no.2 train no.182870  loss = 4.67465 avg_loss = 3.55173\n",
      "epoch no.2 train no.182880  loss = 3.40685 avg_loss = 3.56790\n",
      "epoch no.2 train no.182890  loss = 2.50472 avg_loss = 3.50751\n",
      "epoch no.2 train no.182900  loss = 2.63771 avg_loss = 3.47452\n",
      "epoch no.2 train no.182910  loss = 4.92860 avg_loss = 3.52091\n",
      "epoch no.2 train no.182920  loss = 3.45820 avg_loss = 3.52327\n",
      "epoch no.2 train no.182930  loss = 2.75184 avg_loss = 3.46919\n",
      "epoch no.2 train no.182940  loss = 3.14967 avg_loss = 3.47742\n",
      "epoch no.2 train no.182950  loss = 4.24487 avg_loss = 3.50007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.182960  loss = 4.40783 avg_loss = 3.53698\n",
      "epoch no.2 train no.182970  loss = 4.25914 avg_loss = 3.61061\n",
      "epoch no.2 train no.182980  loss = 4.43709 avg_loss = 3.60595\n",
      "epoch no.2 train no.182990  loss = 4.11879 avg_loss = 3.57743\n",
      "epoch no.2 train no.183000  loss = 3.53975 avg_loss = 3.57541\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '용', '싶', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환하고 싶을때 듣는 음악</s>\n",
      "epoch no.2 train no.183010  loss = 2.87851 avg_loss = 3.58741\n",
      "epoch no.2 train no.183020  loss = 3.02149 avg_loss = 3.57040\n",
      "epoch no.2 train no.183030  loss = 4.29346 avg_loss = 3.60570\n",
      "epoch no.2 train no.183040  loss = 4.65615 avg_loss = 3.58699\n",
      "epoch no.2 train no.183050  loss = 3.14667 avg_loss = 3.53698\n",
      "epoch no.2 train no.183060  loss = 4.13375 avg_loss = 3.54667\n",
      "epoch no.2 train no.183070  loss = 4.20215 avg_loss = 3.53581\n",
      "epoch no.2 train no.183080  loss = 3.82695 avg_loss = 3.57384\n",
      "epoch no.2 train no.183090  loss = 4.53144 avg_loss = 3.57575\n",
      "epoch no.2 train no.183100  loss = 3.87818 avg_loss = 3.55477\n",
      "epoch no.2 train no.183110  loss = 3.66044 avg_loss = 3.57752\n",
      "epoch no.2 train no.183120  loss = 3.82386 avg_loss = 3.62669\n",
      "epoch no.2 train no.183130  loss = 4.54595 avg_loss = 3.63181\n",
      "epoch no.2 train no.183140  loss = 2.68199 avg_loss = 3.57401\n",
      "epoch no.2 train no.183150  loss = 3.76224 avg_loss = 3.52820\n",
      "epoch no.2 train no.183160  loss = 3.45806 avg_loss = 3.50905\n",
      "epoch no.2 train no.183170  loss = 2.30985 avg_loss = 3.53781\n",
      "epoch no.2 train no.183180  loss = 1.92597 avg_loss = 3.52781\n",
      "epoch no.2 train no.183190  loss = 4.95171 avg_loss = 3.54167\n",
      "epoch no.2 train no.183200  loss = 2.44819 avg_loss = 3.53120\n",
      "epoch no.2 train no.183210  loss = 2.08227 avg_loss = 3.49765\n",
      "epoch no.2 train no.183220  loss = 3.48506 avg_loss = 3.56437\n",
      "epoch no.2 train no.183230  loss = 5.09699 avg_loss = 3.56932\n",
      "epoch no.2 train no.183240  loss = 3.55879 avg_loss = 3.57482\n",
      "epoch no.2 train no.183250  loss = 2.50812 avg_loss = 3.56110\n",
      "epoch no.2 train no.183260  loss = 3.75938 avg_loss = 3.59160\n",
      "epoch no.2 train no.183270  loss = 3.37340 avg_loss = 3.54648\n",
      "epoch no.2 train no.183280  loss = 4.04073 avg_loss = 3.59004\n",
      "epoch no.2 train no.183290  loss = 3.58195 avg_loss = 3.58319\n",
      "epoch no.2 train no.183300  loss = 3.33165 avg_loss = 3.59653\n",
      "epoch no.2 train no.183310  loss = 5.33048 avg_loss = 3.58041\n",
      "epoch no.2 train no.183320  loss = 3.25349 avg_loss = 3.56744\n",
      "epoch no.2 train no.183330  loss = 3.76264 avg_loss = 3.59670\n",
      "epoch no.2 train no.183340  loss = 2.59318 avg_loss = 3.50373\n",
      "epoch no.2 train no.183350  loss = 5.78678 avg_loss = 3.54724\n",
      "epoch no.2 train no.183360  loss = 2.85823 avg_loss = 3.53672\n",
      "epoch no.2 train no.183370  loss = 3.89652 avg_loss = 3.51776\n",
      "epoch no.2 train no.183380  loss = 2.92858 avg_loss = 3.50649\n",
      "epoch no.2 train no.183390  loss = 4.96840 avg_loss = 3.50793\n",
      "epoch no.2 train no.183400  loss = 2.63555 avg_loss = 3.51132\n",
      "epoch no.2 train no.183410  loss = 2.86197 avg_loss = 3.51438\n",
      "epoch no.2 train no.183420  loss = 4.48452 avg_loss = 3.52467\n",
      "epoch no.2 train no.183430  loss = 3.67957 avg_loss = 3.55480\n",
      "epoch no.2 train no.183440  loss = 4.59629 avg_loss = 3.57253\n",
      "epoch no.2 train no.183450  loss = 3.37125 avg_loss = 3.53464\n",
      "epoch no.2 train no.183460  loss = 3.70236 avg_loss = 3.50271\n",
      "epoch no.2 train no.183470  loss = 4.34857 avg_loss = 3.52150\n",
      "epoch no.2 train no.183480  loss = 3.43381 avg_loss = 3.51592\n",
      "epoch no.2 train no.183490  loss = 2.79679 avg_loss = 3.50553\n",
      "epoch no.2 train no.183500  loss = 4.91226 avg_loss = 3.51067\n",
      "epoch no.2 train no.183510  loss = 3.95488 avg_loss = 3.48973\n",
      "epoch no.2 train no.183520  loss = 4.15698 avg_loss = 3.52488\n",
      "epoch no.2 train no.183530  loss = 4.89001 avg_loss = 3.58246\n",
      "epoch no.2 train no.183540  loss = 4.46405 avg_loss = 3.57522\n",
      "epoch no.2 train no.183550  loss = 4.97537 avg_loss = 3.51742\n",
      "epoch no.2 train no.183560  loss = 3.69019 avg_loss = 3.49401\n",
      "epoch no.2 train no.183570  loss = 4.79506 avg_loss = 3.45526\n",
      "epoch no.2 train no.183580  loss = 6.14980 avg_loss = 3.47608\n",
      "epoch no.2 train no.183590  loss = 3.05751 avg_loss = 3.46540\n",
      "epoch no.2 train no.183600  loss = 5.75440 avg_loss = 3.47284\n",
      "epoch no.2 train no.183610  loss = 3.89453 avg_loss = 3.53464\n",
      "epoch no.2 train no.183620  loss = 5.55851 avg_loss = 3.56516\n",
      "epoch no.2 train no.183630  loss = 3.57900 avg_loss = 3.55137\n",
      "epoch no.2 train no.183640  loss = 2.63753 avg_loss = 3.51915\n",
      "epoch no.2 train no.183650  loss = 3.03272 avg_loss = 3.49197\n",
      "epoch no.2 train no.183660  loss = 2.98390 avg_loss = 3.46526\n",
      "epoch no.2 train no.183670  loss = 2.86860 avg_loss = 3.48098\n",
      "epoch no.2 train no.183680  loss = 5.04607 avg_loss = 3.51687\n",
      "epoch no.2 train no.183690  loss = 2.65188 avg_loss = 3.55924\n",
      "epoch no.2 train no.183700  loss = 3.30747 avg_loss = 3.60483\n",
      "epoch no.2 train no.183710  loss = 2.96451 avg_loss = 3.58822\n",
      "epoch no.2 train no.183720  loss = 4.69998 avg_loss = 3.60070\n",
      "epoch no.2 train no.183730  loss = 3.50314 avg_loss = 3.57049\n",
      "epoch no.2 train no.183740  loss = 2.05937 avg_loss = 3.55203\n",
      "epoch no.2 train no.183750  loss = 2.62802 avg_loss = 3.59424\n",
      "epoch no.2 train no.183760  loss = 3.70779 avg_loss = 3.63156\n",
      "epoch no.2 train no.183770  loss = 2.94650 avg_loss = 3.62752\n",
      "epoch no.2 train no.183780  loss = 3.29368 avg_loss = 3.64379\n",
      "epoch no.2 train no.183790  loss = 3.62213 avg_loss = 3.65338\n",
      "epoch no.2 train no.183800  loss = 3.63994 avg_loss = 3.61974\n",
      "epoch no.2 train no.183810  loss = 3.93211 avg_loss = 3.64546\n",
      "epoch no.2 train no.183820  loss = 4.02416 avg_loss = 3.63433\n",
      "epoch no.2 train no.183830  loss = 3.23774 avg_loss = 3.64729\n",
      "epoch no.2 train no.183840  loss = 2.49808 avg_loss = 3.64789\n",
      "epoch no.2 train no.183850  loss = 4.58570 avg_loss = 3.67692\n",
      "epoch no.2 train no.183860  loss = 3.32273 avg_loss = 3.64802\n",
      "epoch no.2 train no.183870  loss = 3.53169 avg_loss = 3.64948\n",
      "epoch no.2 train no.183880  loss = 1.65124 avg_loss = 3.60951\n",
      "epoch no.2 train no.183890  loss = 3.74950 avg_loss = 3.62338\n",
      "epoch no.2 train no.183900  loss = 4.80874 avg_loss = 3.62995\n",
      "epoch no.2 train no.183910  loss = 3.29827 avg_loss = 3.58472\n",
      "epoch no.2 train no.183920  loss = 5.73763 avg_loss = 3.60538\n",
      "epoch no.2 train no.183930  loss = 3.90056 avg_loss = 3.60085\n",
      "epoch no.2 train no.183940  loss = 4.41986 avg_loss = 3.65246\n",
      "epoch no.2 train no.183950  loss = 3.07404 avg_loss = 3.57778\n",
      "epoch no.2 train no.183960  loss = 4.42484 avg_loss = 3.57101\n",
      "epoch no.2 train no.183970  loss = 3.77352 avg_loss = 3.60231\n",
      "epoch no.2 train no.183980  loss = 4.13620 avg_loss = 3.62254\n",
      "epoch no.2 train no.183990  loss = 3.66784 avg_loss = 3.66560\n",
      "epoch no.2 train no.184000  loss = 4.45943 avg_loss = 3.68868\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.2 train no.184010  loss = 2.76368 avg_loss = 3.64460\n",
      "epoch no.2 train no.184020  loss = 2.59701 avg_loss = 3.61919\n",
      "epoch no.2 train no.184030  loss = 2.87975 avg_loss = 3.61421\n",
      "epoch no.2 train no.184040  loss = 3.32299 avg_loss = 3.61003\n",
      "epoch no.2 train no.184050  loss = 4.43297 avg_loss = 3.59311\n",
      "epoch no.2 train no.184060  loss = 1.20542 avg_loss = 3.60088\n",
      "epoch no.2 train no.184070  loss = 3.15652 avg_loss = 3.57840\n",
      "epoch no.2 train no.184080  loss = 4.27670 avg_loss = 3.61023\n",
      "epoch no.2 train no.184090  loss = 3.25387 avg_loss = 3.56958\n",
      "epoch no.2 train no.184100  loss = 2.39474 avg_loss = 3.55777\n",
      "epoch no.2 train no.184110  loss = 4.59931 avg_loss = 3.57910\n",
      "epoch no.2 train no.184120  loss = 2.49839 avg_loss = 3.54516\n",
      "epoch no.2 train no.184130  loss = 1.69511 avg_loss = 3.50707\n",
      "epoch no.2 train no.184140  loss = 3.81962 avg_loss = 3.48379\n",
      "epoch no.2 train no.184150  loss = 3.08846 avg_loss = 3.55759\n",
      "epoch no.2 train no.184160  loss = 2.75940 avg_loss = 3.56791\n",
      "epoch no.2 train no.184170  loss = 2.37186 avg_loss = 3.60125\n",
      "epoch no.2 train no.184180  loss = 3.57424 avg_loss = 3.61078\n",
      "epoch no.2 train no.184190  loss = 3.15306 avg_loss = 3.60181\n",
      "epoch no.2 train no.184200  loss = 3.23081 avg_loss = 3.55890\n",
      "epoch no.2 train no.184210  loss = 2.57102 avg_loss = 3.53957\n",
      "epoch no.2 train no.184220  loss = 2.63177 avg_loss = 3.56417\n",
      "epoch no.2 train no.184230  loss = 3.32911 avg_loss = 3.53642\n",
      "epoch no.2 train no.184240  loss = 2.82430 avg_loss = 3.54860\n",
      "epoch no.2 train no.184250  loss = 2.36930 avg_loss = 3.51862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.184260  loss = 1.40813 avg_loss = 3.48652\n",
      "epoch no.2 train no.184270  loss = 3.43456 avg_loss = 3.48270\n",
      "epoch no.2 train no.184280  loss = 3.60541 avg_loss = 3.48186\n",
      "epoch no.2 train no.184290  loss = 5.08219 avg_loss = 3.48608\n",
      "epoch no.2 train no.184300  loss = 2.48478 avg_loss = 3.48837\n",
      "epoch no.2 train no.184310  loss = 3.41849 avg_loss = 3.48557\n",
      "epoch no.2 train no.184320  loss = 4.30901 avg_loss = 3.49510\n",
      "epoch no.2 train no.184330  loss = 2.52170 avg_loss = 3.49311\n",
      "epoch no.2 train no.184340  loss = 4.11406 avg_loss = 3.49365\n",
      "epoch no.2 train no.184350  loss = 4.51847 avg_loss = 3.53463\n",
      "epoch no.2 train no.184360  loss = 1.38119 avg_loss = 3.56349\n",
      "epoch no.2 train no.184370  loss = 2.84119 avg_loss = 3.55252\n",
      "epoch no.2 train no.184380  loss = 2.81905 avg_loss = 3.57412\n",
      "epoch no.2 train no.184390  loss = 4.35555 avg_loss = 3.59391\n",
      "epoch no.2 train no.184400  loss = 3.56389 avg_loss = 3.56830\n",
      "epoch no.2 train no.184410  loss = 4.15614 avg_loss = 3.57389\n",
      "epoch no.2 train no.184420  loss = 2.46868 avg_loss = 3.51550\n",
      "epoch no.2 train no.184430  loss = 1.97538 avg_loss = 3.47756\n",
      "epoch no.2 train no.184440  loss = 2.15363 avg_loss = 3.46214\n",
      "epoch no.2 train no.184450  loss = 3.99980 avg_loss = 3.45154\n",
      "epoch no.2 train no.184460  loss = 2.74166 avg_loss = 3.41951\n",
      "epoch no.2 train no.184470  loss = 3.83565 avg_loss = 3.40557\n",
      "epoch no.2 train no.184480  loss = 2.32075 avg_loss = 3.38299\n",
      "epoch no.2 train no.184490  loss = 2.23940 avg_loss = 3.40694\n",
      "epoch no.2 train no.184500  loss = 2.87101 avg_loss = 3.38624\n",
      "epoch no.2 train no.184510  loss = 1.50339 avg_loss = 3.40447\n",
      "epoch no.2 train no.184520  loss = 4.53386 avg_loss = 3.42865\n",
      "epoch no.2 train no.184530  loss = 3.55762 avg_loss = 3.46696\n",
      "epoch no.2 train no.184540  loss = 2.68834 avg_loss = 3.44866\n",
      "epoch no.2 train no.184550  loss = 3.84698 avg_loss = 3.47576\n",
      "epoch no.2 train no.184560  loss = 3.26005 avg_loss = 3.45507\n",
      "epoch no.2 train no.184570  loss = 2.89062 avg_loss = 3.51121\n",
      "epoch no.2 train no.184580  loss = 1.52130 avg_loss = 3.43925\n",
      "epoch no.2 train no.184590  loss = 3.17961 avg_loss = 3.42802\n",
      "epoch no.2 train no.184600  loss = 3.36119 avg_loss = 3.46396\n",
      "epoch no.2 train no.184610  loss = 4.53484 avg_loss = 3.48992\n",
      "epoch no.2 train no.184620  loss = 5.43219 avg_loss = 3.52764\n",
      "epoch no.2 train no.184630  loss = 4.53177 avg_loss = 3.56253\n",
      "epoch no.2 train no.184640  loss = 2.39279 avg_loss = 3.51060\n",
      "epoch no.2 train no.184650  loss = 4.77200 avg_loss = 3.55200\n",
      "epoch no.2 train no.184660  loss = 2.03716 avg_loss = 3.53169\n",
      "epoch no.2 train no.184670  loss = 3.21095 avg_loss = 3.50286\n",
      "epoch no.2 train no.184680  loss = 2.60312 avg_loss = 3.52661\n",
      "epoch no.2 train no.184690  loss = 4.84428 avg_loss = 3.54484\n",
      "epoch no.2 train no.184700  loss = 4.83634 avg_loss = 3.50673\n",
      "epoch no.2 train no.184710  loss = 3.57622 avg_loss = 3.54787\n",
      "epoch no.2 train no.184720  loss = 5.01850 avg_loss = 3.51973\n",
      "epoch no.2 train no.184730  loss = 3.74080 avg_loss = 3.52349\n",
      "epoch no.2 train no.184740  loss = 3.17830 avg_loss = 3.54210\n",
      "epoch no.2 train no.184750  loss = 2.36048 avg_loss = 3.55116\n",
      "epoch no.2 train no.184760  loss = 3.48269 avg_loss = 3.52894\n",
      "epoch no.2 train no.184770  loss = 3.13048 avg_loss = 3.49575\n",
      "epoch no.2 train no.184780  loss = 3.71458 avg_loss = 3.50948\n",
      "epoch no.2 train no.184790  loss = 3.36770 avg_loss = 3.46590\n",
      "epoch no.2 train no.184800  loss = 7.16061 avg_loss = 3.56783\n",
      "epoch no.2 train no.184810  loss = 2.47611 avg_loss = 3.59140\n",
      "epoch no.2 train no.184820  loss = 4.07482 avg_loss = 3.60248\n",
      "epoch no.2 train no.184830  loss = 3.92916 avg_loss = 3.63944\n",
      "epoch no.2 train no.184840  loss = 3.25527 avg_loss = 3.62521\n",
      "epoch no.2 train no.184850  loss = 5.91349 avg_loss = 3.65063\n",
      "epoch no.2 train no.184860  loss = 2.56718 avg_loss = 3.62446\n",
      "epoch no.2 train no.184870  loss = 3.88073 avg_loss = 3.61037\n",
      "epoch no.2 train no.184880  loss = 2.55206 avg_loss = 3.59596\n",
      "epoch no.2 train no.184890  loss = 4.05253 avg_loss = 3.62995\n",
      "epoch no.2 train no.184900  loss = 2.78601 avg_loss = 3.62383\n",
      "epoch no.2 train no.184910  loss = 2.57299 avg_loss = 3.61224\n",
      "epoch no.2 train no.184920  loss = 5.17338 avg_loss = 3.62488\n",
      "epoch no.2 train no.184930  loss = 4.47896 avg_loss = 3.59989\n",
      "epoch no.2 train no.184940  loss = 3.83882 avg_loss = 3.62817\n",
      "epoch no.2 train no.184950  loss = 3.74951 avg_loss = 3.58392\n",
      "epoch no.2 train no.184960  loss = 3.33319 avg_loss = 3.54036\n",
      "epoch no.2 train no.184970  loss = 3.60380 avg_loss = 3.49996\n",
      "epoch no.2 train no.184980  loss = 3.97491 avg_loss = 3.52594\n",
      "epoch no.2 train no.184990  loss = 4.41204 avg_loss = 3.56746\n",
      "epoch no.2 train no.185000  loss = 6.13646 avg_loss = 3.58787\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '에', '▁좋은', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 좋은 힙합</s>\n",
      "epoch no.2 train no.185010  loss = 1.99490 avg_loss = 3.58283\n",
      "epoch no.2 train no.185020  loss = 2.91077 avg_loss = 3.60870\n",
      "epoch no.2 train no.185030  loss = 3.80679 avg_loss = 3.56245\n",
      "epoch no.2 train no.185040  loss = 4.35118 avg_loss = 3.55277\n",
      "epoch no.2 train no.185050  loss = 5.00620 avg_loss = 3.64168\n",
      "epoch no.2 train no.185060  loss = 2.46614 avg_loss = 3.58932\n",
      "epoch no.2 train no.185070  loss = 3.93843 avg_loss = 3.60112\n",
      "epoch no.2 train no.185080  loss = 3.87815 avg_loss = 3.60963\n",
      "epoch no.2 train no.185090  loss = 2.73708 avg_loss = 3.59180\n",
      "epoch no.2 train no.185100  loss = 2.87292 avg_loss = 3.64064\n",
      "epoch no.2 train no.185110  loss = 4.02227 avg_loss = 3.61360\n",
      "epoch no.2 train no.185120  loss = 4.74335 avg_loss = 3.59752\n",
      "epoch no.2 train no.185130  loss = 3.25865 avg_loss = 3.55666\n",
      "epoch no.2 train no.185140  loss = 2.90211 avg_loss = 3.55106\n",
      "epoch no.2 train no.185150  loss = 2.41444 avg_loss = 3.54298\n",
      "epoch no.2 train no.185160  loss = 4.65638 avg_loss = 3.57847\n",
      "epoch no.2 train no.185170  loss = 3.00033 avg_loss = 3.60813\n",
      "epoch no.2 train no.185180  loss = 3.81987 avg_loss = 3.58241\n",
      "epoch no.2 train no.185190  loss = 3.64224 avg_loss = 3.55343\n",
      "epoch no.2 train no.185200  loss = 1.54439 avg_loss = 3.47659\n",
      "epoch no.2 train no.185210  loss = 3.60353 avg_loss = 3.46480\n",
      "epoch no.2 train no.185220  loss = 3.14776 avg_loss = 3.42643\n",
      "epoch no.2 train no.185230  loss = 3.22332 avg_loss = 3.40682\n",
      "epoch no.2 train no.185240  loss = 3.35813 avg_loss = 3.34647\n",
      "epoch no.2 train no.185250  loss = 6.03014 avg_loss = 3.41272\n",
      "epoch no.2 train no.185260  loss = 3.93710 avg_loss = 3.44326\n",
      "epoch no.2 train no.185270  loss = 2.09643 avg_loss = 3.44913\n",
      "epoch no.2 train no.185280  loss = 3.10136 avg_loss = 3.43924\n",
      "epoch no.2 train no.185290  loss = 3.45699 avg_loss = 3.48344\n",
      "epoch no.2 train no.185300  loss = 3.70796 avg_loss = 3.44800\n",
      "epoch no.2 train no.185310  loss = 2.75058 avg_loss = 3.48756\n",
      "epoch no.2 train no.185320  loss = 2.86343 avg_loss = 3.52839\n",
      "epoch no.2 train no.185330  loss = 3.65301 avg_loss = 3.45910\n",
      "epoch no.2 train no.185340  loss = 2.81309 avg_loss = 3.50182\n",
      "epoch no.2 train no.185350  loss = 4.68085 avg_loss = 3.54072\n",
      "epoch no.2 train no.185360  loss = 3.89414 avg_loss = 3.51251\n",
      "epoch no.2 train no.185370  loss = 1.77569 avg_loss = 3.45845\n",
      "epoch no.2 train no.185380  loss = 3.59996 avg_loss = 3.47102\n",
      "epoch no.2 train no.185390  loss = 4.04104 avg_loss = 3.44321\n",
      "epoch no.2 train no.185400  loss = 6.13197 avg_loss = 3.46521\n",
      "epoch no.2 train no.185410  loss = 1.99723 avg_loss = 3.46714\n",
      "epoch no.2 train no.185420  loss = 3.58388 avg_loss = 3.50129\n",
      "epoch no.2 train no.185430  loss = 2.52635 avg_loss = 3.55517\n",
      "epoch no.2 train no.185440  loss = 3.52774 avg_loss = 3.54884\n",
      "epoch no.2 train no.185450  loss = 3.45474 avg_loss = 3.56680\n",
      "epoch no.2 train no.185460  loss = 4.72201 avg_loss = 3.54359\n",
      "epoch no.2 train no.185470  loss = 4.69577 avg_loss = 3.58852\n",
      "epoch no.2 train no.185480  loss = 4.99989 avg_loss = 3.57777\n",
      "epoch no.2 train no.185490  loss = 2.05034 avg_loss = 3.55113\n",
      "epoch no.2 train no.185500  loss = 4.00097 avg_loss = 3.58187\n",
      "epoch no.2 train no.185510  loss = 3.06021 avg_loss = 3.58576\n",
      "epoch no.2 train no.185520  loss = 3.08899 avg_loss = 3.57022\n",
      "epoch no.2 train no.185530  loss = 3.30513 avg_loss = 3.57881\n",
      "epoch no.2 train no.185540  loss = 3.26539 avg_loss = 3.62005\n",
      "epoch no.2 train no.185550  loss = 2.05860 avg_loss = 3.58588\n",
      "epoch no.2 train no.185560  loss = 4.22082 avg_loss = 3.59169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.185570  loss = 3.43028 avg_loss = 3.66209\n",
      "epoch no.2 train no.185580  loss = 1.73489 avg_loss = 3.61716\n",
      "epoch no.2 train no.185590  loss = 4.20147 avg_loss = 3.59029\n",
      "epoch no.2 train no.185600  loss = 3.97751 avg_loss = 3.61305\n",
      "epoch no.2 train no.185610  loss = 3.24296 avg_loss = 3.61986\n",
      "epoch no.2 train no.185620  loss = 3.65782 avg_loss = 3.62653\n",
      "epoch no.2 train no.185630  loss = 2.69123 avg_loss = 3.62907\n",
      "epoch no.2 train no.185640  loss = 3.78741 avg_loss = 3.59123\n",
      "epoch no.2 train no.185650  loss = 2.44243 avg_loss = 3.57701\n",
      "epoch no.2 train no.185660  loss = 4.76538 avg_loss = 3.49949\n",
      "epoch no.2 train no.185670  loss = 3.83656 avg_loss = 3.49803\n",
      "epoch no.2 train no.185680  loss = 2.26062 avg_loss = 3.49122\n",
      "epoch no.2 train no.185690  loss = 2.31603 avg_loss = 3.46037\n",
      "epoch no.2 train no.185700  loss = 3.79532 avg_loss = 3.48709\n",
      "epoch no.2 train no.185710  loss = 2.54427 avg_loss = 3.45977\n",
      "epoch no.2 train no.185720  loss = 3.76870 avg_loss = 3.46797\n",
      "epoch no.2 train no.185730  loss = 3.09666 avg_loss = 3.53180\n",
      "epoch no.2 train no.185740  loss = 3.06604 avg_loss = 3.52294\n",
      "epoch no.2 train no.185750  loss = 5.10431 avg_loss = 3.50843\n",
      "epoch no.2 train no.185760  loss = 2.82660 avg_loss = 3.55739\n",
      "epoch no.2 train no.185770  loss = 2.67153 avg_loss = 3.53024\n",
      "epoch no.2 train no.185780  loss = 3.48399 avg_loss = 3.54382\n",
      "epoch no.2 train no.185790  loss = 3.23500 avg_loss = 3.52934\n",
      "epoch no.2 train no.185800  loss = 2.29052 avg_loss = 3.51305\n",
      "epoch no.2 train no.185810  loss = 3.58812 avg_loss = 3.50188\n",
      "epoch no.2 train no.185820  loss = 1.71797 avg_loss = 3.48897\n",
      "epoch no.2 train no.185830  loss = 3.59751 avg_loss = 3.50763\n",
      "epoch no.2 train no.185840  loss = 2.89801 avg_loss = 3.45904\n",
      "epoch no.2 train no.185850  loss = 3.74932 avg_loss = 3.48571\n",
      "epoch no.2 train no.185860  loss = 3.60462 avg_loss = 3.51017\n",
      "epoch no.2 train no.185870  loss = 5.43553 avg_loss = 3.55122\n",
      "epoch no.2 train no.185880  loss = 3.19685 avg_loss = 3.54167\n",
      "epoch no.2 train no.185890  loss = 4.00715 avg_loss = 3.54091\n",
      "epoch no.2 train no.185900  loss = 4.07794 avg_loss = 3.58704\n",
      "epoch no.2 train no.185910  loss = 4.55084 avg_loss = 3.62642\n",
      "epoch no.2 train no.185920  loss = 5.86704 avg_loss = 3.66514\n",
      "epoch no.2 train no.185930  loss = 3.54594 avg_loss = 3.67443\n",
      "epoch no.2 train no.185940  loss = 3.65981 avg_loss = 3.62332\n",
      "epoch no.2 train no.185950  loss = 4.73030 avg_loss = 3.61017\n",
      "epoch no.2 train no.185960  loss = 4.45286 avg_loss = 3.58586\n",
      "epoch no.2 train no.185970  loss = 4.59008 avg_loss = 3.53860\n",
      "epoch no.2 train no.185980  loss = 2.86059 avg_loss = 3.53018\n",
      "epoch no.2 train no.185990  loss = 3.46050 avg_loss = 3.48125\n",
      "epoch no.2 train no.186000  loss = 5.21859 avg_loss = 3.54750\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '▁하고', '주는', '▁신나는', '▁노래', '</s>']\n",
      "기분전환 시켜주는 신나는 팝</s>\n",
      "epoch no.2 train no.186010  loss = 3.56301 avg_loss = 3.56266\n",
      "epoch no.2 train no.186020  loss = 2.71203 avg_loss = 3.53805\n",
      "epoch no.2 train no.186030  loss = 3.56232 avg_loss = 3.56099\n",
      "epoch no.2 train no.186040  loss = 2.42034 avg_loss = 3.51268\n",
      "epoch no.2 train no.186050  loss = 2.74746 avg_loss = 3.51353\n",
      "epoch no.2 train no.186060  loss = 3.88493 avg_loss = 3.49382\n",
      "epoch no.2 train no.186070  loss = 3.04842 avg_loss = 3.49820\n",
      "epoch no.2 train no.186080  loss = 2.81679 avg_loss = 3.46179\n",
      "epoch no.2 train no.186090  loss = 2.41419 avg_loss = 3.49177\n",
      "epoch no.2 train no.186100  loss = 3.41439 avg_loss = 3.51631\n",
      "epoch no.2 train no.186110  loss = 2.42929 avg_loss = 3.43335\n",
      "epoch no.2 train no.186120  loss = 2.44571 avg_loss = 3.48299\n",
      "epoch no.2 train no.186130  loss = 3.60862 avg_loss = 3.52729\n",
      "epoch no.2 train no.186140  loss = 2.15344 avg_loss = 3.55923\n",
      "epoch no.2 train no.186150  loss = 3.95942 avg_loss = 3.54046\n",
      "epoch no.2 train no.186160  loss = 5.07880 avg_loss = 3.54287\n",
      "epoch no.2 train no.186170  loss = 2.71209 avg_loss = 3.51365\n",
      "epoch no.2 train no.186180  loss = 1.91117 avg_loss = 3.50698\n",
      "epoch no.2 train no.186190  loss = 2.59649 avg_loss = 3.50362\n",
      "epoch no.2 train no.186200  loss = 4.02775 avg_loss = 3.58361\n",
      "epoch no.2 train no.186210  loss = 2.62458 avg_loss = 3.55641\n",
      "epoch no.2 train no.186220  loss = 4.56435 avg_loss = 3.55360\n",
      "epoch no.2 train no.186230  loss = 2.16218 avg_loss = 3.48421\n",
      "epoch no.2 train no.186240  loss = 3.12720 avg_loss = 3.48230\n",
      "epoch no.2 train no.186250  loss = 2.82250 avg_loss = 3.55348\n",
      "epoch no.2 train no.186260  loss = 2.57090 avg_loss = 3.57600\n",
      "epoch no.2 train no.186270  loss = 3.86969 avg_loss = 3.58951\n",
      "epoch no.2 train no.186280  loss = 3.25889 avg_loss = 3.57394\n",
      "epoch no.2 train no.186290  loss = 3.96196 avg_loss = 3.59266\n",
      "epoch no.2 train no.186300  loss = 4.30371 avg_loss = 3.56719\n",
      "epoch no.2 train no.186310  loss = 4.18707 avg_loss = 3.58976\n",
      "epoch no.2 train no.186320  loss = 3.09290 avg_loss = 3.59940\n",
      "epoch no.2 train no.186330  loss = 1.92117 avg_loss = 3.58970\n",
      "epoch no.2 train no.186340  loss = 5.26930 avg_loss = 3.57941\n",
      "epoch no.2 train no.186350  loss = 2.72611 avg_loss = 3.53741\n",
      "epoch no.2 train no.186360  loss = 2.49339 avg_loss = 3.48290\n",
      "epoch no.2 train no.186370  loss = 4.81417 avg_loss = 3.57387\n",
      "epoch no.2 train no.186380  loss = 3.16411 avg_loss = 3.55781\n",
      "epoch no.2 train no.186390  loss = 3.39394 avg_loss = 3.52774\n",
      "epoch no.2 train no.186400  loss = 2.45860 avg_loss = 3.56573\n",
      "epoch no.2 train no.186410  loss = 3.25293 avg_loss = 3.53731\n",
      "epoch no.2 train no.186420  loss = 2.30031 avg_loss = 3.58244\n",
      "epoch no.2 train no.186430  loss = 2.73428 avg_loss = 3.54422\n",
      "epoch no.2 train no.186440  loss = 4.62611 avg_loss = 3.54663\n",
      "epoch no.2 train no.186450  loss = 1.85507 avg_loss = 3.54969\n",
      "epoch no.2 train no.186460  loss = 2.62921 avg_loss = 3.52290\n",
      "epoch no.2 train no.186470  loss = 2.84482 avg_loss = 3.52744\n",
      "epoch no.2 train no.186480  loss = 4.04689 avg_loss = 3.57355\n",
      "epoch no.2 train no.186490  loss = 4.36865 avg_loss = 3.59673\n",
      "epoch no.2 train no.186500  loss = 2.59737 avg_loss = 3.57104\n",
      "epoch no.2 train no.186510  loss = 2.22786 avg_loss = 3.54355\n",
      "epoch no.2 train no.186520  loss = 2.97659 avg_loss = 3.53032\n",
      "epoch no.2 train no.186530  loss = 2.77087 avg_loss = 3.54926\n",
      "epoch no.2 train no.186540  loss = 3.00946 avg_loss = 3.58848\n",
      "epoch no.2 train no.186550  loss = 3.49942 avg_loss = 3.58023\n",
      "epoch no.2 train no.186560  loss = 2.09590 avg_loss = 3.52421\n",
      "epoch no.2 train no.186570  loss = 2.59526 avg_loss = 3.54992\n",
      "epoch no.2 train no.186580  loss = 4.47899 avg_loss = 3.55175\n",
      "epoch no.2 train no.186590  loss = 1.90048 avg_loss = 3.53357\n",
      "epoch no.2 train no.186600  loss = 3.61965 avg_loss = 3.61455\n",
      "epoch no.2 train no.186610  loss = 3.28995 avg_loss = 3.61872\n",
      "epoch no.2 train no.186620  loss = 3.15939 avg_loss = 3.59993\n",
      "epoch no.2 train no.186630  loss = 3.14004 avg_loss = 3.58558\n",
      "epoch no.2 train no.186640  loss = 3.08872 avg_loss = 3.54910\n",
      "epoch no.2 train no.186650  loss = 3.21590 avg_loss = 3.48282\n",
      "epoch no.2 train no.186660  loss = 2.50225 avg_loss = 3.47519\n",
      "epoch no.2 train no.186670  loss = 2.14019 avg_loss = 3.44995\n",
      "epoch no.2 train no.186680  loss = 2.20739 avg_loss = 3.44528\n",
      "epoch no.2 train no.186690  loss = 4.14833 avg_loss = 3.42725\n",
      "epoch no.2 train no.186700  loss = 3.67202 avg_loss = 3.44392\n",
      "epoch no.2 train no.186710  loss = 3.03806 avg_loss = 3.45421\n",
      "epoch no.2 train no.186720  loss = 2.89950 avg_loss = 3.46339\n",
      "epoch no.2 train no.186730  loss = 2.79521 avg_loss = 3.52433\n",
      "epoch no.2 train no.186740  loss = 3.26526 avg_loss = 3.54988\n",
      "epoch no.2 train no.186750  loss = 3.25291 avg_loss = 3.52771\n",
      "epoch no.2 train no.186760  loss = 1.77332 avg_loss = 3.47727\n",
      "epoch no.2 train no.186770  loss = 2.62311 avg_loss = 3.48303\n",
      "epoch no.2 train no.186780  loss = 3.14351 avg_loss = 3.48039\n",
      "epoch no.2 train no.186790  loss = 2.52866 avg_loss = 3.45601\n",
      "epoch no.2 train no.186800  loss = 2.92387 avg_loss = 3.58988\n",
      "epoch no.2 train no.186810  loss = 3.38411 avg_loss = 3.59132\n",
      "epoch no.2 train no.186820  loss = 4.77427 avg_loss = 3.66172\n",
      "epoch no.2 train no.186830  loss = 3.85235 avg_loss = 3.64660\n",
      "epoch no.2 train no.186840  loss = 3.23471 avg_loss = 3.61810\n",
      "epoch no.2 train no.186850  loss = 2.50537 avg_loss = 3.60406\n",
      "epoch no.2 train no.186860  loss = 4.12228 avg_loss = 3.57947\n",
      "epoch no.2 train no.186870  loss = 2.14578 avg_loss = 3.53410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.186880  loss = 2.83327 avg_loss = 3.52060\n",
      "epoch no.2 train no.186890  loss = 5.32525 avg_loss = 3.56952\n",
      "epoch no.2 train no.186900  loss = 4.28197 avg_loss = 3.61653\n",
      "epoch no.2 train no.186910  loss = 3.49314 avg_loss = 3.62276\n",
      "epoch no.2 train no.186920  loss = 3.37596 avg_loss = 3.64955\n",
      "epoch no.2 train no.186930  loss = 4.95764 avg_loss = 3.70053\n",
      "epoch no.2 train no.186940  loss = 3.84848 avg_loss = 3.74457\n",
      "epoch no.2 train no.186950  loss = 3.44815 avg_loss = 3.75847\n",
      "epoch no.2 train no.186960  loss = 4.24931 avg_loss = 3.83621\n",
      "epoch no.2 train no.186970  loss = 4.44400 avg_loss = 3.77273\n",
      "epoch no.2 train no.186980  loss = 2.54682 avg_loss = 3.75222\n",
      "epoch no.2 train no.186990  loss = 4.18592 avg_loss = 3.69755\n",
      "epoch no.2 train no.187000  loss = 4.56597 avg_loss = 3.67213\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '▁하고', '▁싶을', '때', '</s>', '▁음악', '</s>']\n",
      "기분전환하고 싶을 때 듣는 음악</s>\n",
      "epoch no.2 train no.187010  loss = 4.22661 avg_loss = 3.68072\n",
      "epoch no.2 train no.187020  loss = 3.27496 avg_loss = 3.62096\n",
      "epoch no.2 train no.187030  loss = 2.99318 avg_loss = 3.60821\n",
      "epoch no.2 train no.187040  loss = 3.74336 avg_loss = 3.57257\n",
      "epoch no.2 train no.187050  loss = 3.44544 avg_loss = 3.52768\n",
      "epoch no.2 train no.187060  loss = 5.10775 avg_loss = 3.53460\n",
      "epoch no.2 train no.187070  loss = 3.93893 avg_loss = 3.56213\n",
      "epoch no.2 train no.187080  loss = 4.38695 avg_loss = 3.57523\n",
      "epoch no.2 train no.187090  loss = 3.62394 avg_loss = 3.62186\n",
      "epoch no.2 train no.187100  loss = 4.58798 avg_loss = 3.61612\n",
      "epoch no.2 train no.187110  loss = 4.97989 avg_loss = 3.59752\n",
      "epoch no.2 train no.187120  loss = 3.09009 avg_loss = 3.58962\n",
      "epoch no.2 train no.187130  loss = 3.71121 avg_loss = 3.61650\n",
      "epoch no.2 train no.187140  loss = 4.19679 avg_loss = 3.62896\n",
      "epoch no.2 train no.187150  loss = 3.29558 avg_loss = 3.68983\n",
      "epoch no.2 train no.187160  loss = 2.76350 avg_loss = 3.61673\n",
      "epoch no.2 train no.187170  loss = 4.84893 avg_loss = 3.61352\n",
      "epoch no.2 train no.187180  loss = 2.27988 avg_loss = 3.59031\n",
      "epoch no.2 train no.187190  loss = 3.63478 avg_loss = 3.58144\n",
      "epoch no.2 train no.187200  loss = 3.47939 avg_loss = 3.54824\n",
      "epoch no.2 train no.187210  loss = 3.81970 avg_loss = 3.52283\n",
      "epoch no.2 train no.187220  loss = 2.86724 avg_loss = 3.48334\n",
      "epoch no.2 train no.187230  loss = 5.85445 avg_loss = 3.56036\n",
      "epoch no.2 train no.187240  loss = 3.26115 avg_loss = 3.58312\n",
      "epoch no.2 train no.187250  loss = 3.11605 avg_loss = 3.55782\n",
      "epoch no.2 train no.187260  loss = 2.60629 avg_loss = 3.54598\n",
      "epoch no.2 train no.187270  loss = 2.87253 avg_loss = 3.53592\n",
      "epoch no.2 train no.187280  loss = 2.88579 avg_loss = 3.49282\n",
      "epoch no.2 train no.187290  loss = 3.02823 avg_loss = 3.50417\n",
      "epoch no.2 train no.187300  loss = 3.57919 avg_loss = 3.56431\n",
      "epoch no.2 train no.187310  loss = 4.31334 avg_loss = 3.52917\n",
      "epoch no.2 train no.187320  loss = 3.67128 avg_loss = 3.51521\n",
      "epoch no.2 train no.187330  loss = 2.31594 avg_loss = 3.52046\n",
      "epoch no.2 train no.187340  loss = 2.92681 avg_loss = 3.51051\n",
      "epoch no.2 train no.187350  loss = 4.07691 avg_loss = 3.52205\n",
      "epoch no.2 train no.187360  loss = 2.33319 avg_loss = 3.48181\n",
      "epoch no.2 train no.187370  loss = 2.66834 avg_loss = 3.54583\n",
      "epoch no.2 train no.187380  loss = 4.34343 avg_loss = 3.48445\n",
      "epoch no.2 train no.187390  loss = 3.22799 avg_loss = 3.49920\n",
      "epoch no.2 train no.187400  loss = 1.63700 avg_loss = 3.42150\n",
      "epoch no.2 train no.187410  loss = 3.97209 avg_loss = 3.40661\n",
      "epoch no.2 train no.187420  loss = 4.97564 avg_loss = 3.38479\n",
      "epoch no.2 train no.187430  loss = 3.00378 avg_loss = 3.38919\n",
      "epoch no.2 train no.187440  loss = 2.68538 avg_loss = 3.37361\n",
      "epoch no.2 train no.187450  loss = 2.10882 avg_loss = 3.37705\n",
      "epoch no.2 train no.187460  loss = 3.25257 avg_loss = 3.37872\n",
      "epoch no.2 train no.187470  loss = 3.38619 avg_loss = 3.36251\n",
      "epoch no.2 train no.187480  loss = 2.06936 avg_loss = 3.33289\n",
      "epoch no.2 train no.187490  loss = 4.65471 avg_loss = 3.34498\n",
      "epoch no.2 train no.187500  loss = 4.94104 avg_loss = 3.35374\n",
      "epoch no.2 train no.187510  loss = 4.19031 avg_loss = 3.36343\n",
      "epoch no.2 train no.187520  loss = 2.79735 avg_loss = 3.40855\n",
      "epoch no.2 train no.187530  loss = 4.27374 avg_loss = 3.41796\n",
      "epoch no.2 train no.187540  loss = 3.49027 avg_loss = 3.38591\n",
      "epoch no.2 train no.187550  loss = 3.71389 avg_loss = 3.40203\n",
      "epoch no.2 train no.187560  loss = 4.95639 avg_loss = 3.45747\n",
      "epoch no.2 train no.187570  loss = 3.04193 avg_loss = 3.45173\n",
      "epoch no.2 train no.187580  loss = 2.77045 avg_loss = 3.40464\n",
      "epoch no.2 train no.187590  loss = 3.79674 avg_loss = 3.39587\n",
      "epoch no.2 train no.187600  loss = 5.15600 avg_loss = 3.38870\n",
      "epoch no.2 train no.187610  loss = 4.90002 avg_loss = 3.40044\n",
      "epoch no.2 train no.187620  loss = 2.80733 avg_loss = 3.44819\n",
      "epoch no.2 train no.187630  loss = 4.74995 avg_loss = 3.41567\n",
      "epoch no.2 train no.187640  loss = 1.90641 avg_loss = 3.39495\n",
      "epoch no.2 train no.187650  loss = 3.60418 avg_loss = 3.39064\n",
      "epoch no.2 train no.187660  loss = 2.30778 avg_loss = 3.37504\n",
      "epoch no.2 train no.187670  loss = 4.59489 avg_loss = 3.42005\n",
      "epoch no.2 train no.187680  loss = 3.11996 avg_loss = 3.50317\n",
      "epoch no.2 train no.187690  loss = 2.21513 avg_loss = 3.46456\n",
      "epoch no.2 train no.187700  loss = 2.31384 avg_loss = 3.49138\n",
      "epoch no.2 train no.187710  loss = 1.72676 avg_loss = 3.46183\n",
      "epoch no.2 train no.187720  loss = 2.15432 avg_loss = 3.43143\n",
      "epoch no.2 train no.187730  loss = 3.49369 avg_loss = 3.46891\n",
      "epoch no.2 train no.187740  loss = 2.18541 avg_loss = 3.47928\n",
      "epoch no.2 train no.187750  loss = 2.78735 avg_loss = 3.50991\n",
      "epoch no.2 train no.187760  loss = 2.73760 avg_loss = 3.50894\n",
      "epoch no.2 train no.187770  loss = 1.84106 avg_loss = 3.48345\n",
      "epoch no.2 train no.187780  loss = 3.17211 avg_loss = 3.51545\n",
      "epoch no.2 train no.187790  loss = 4.37120 avg_loss = 3.51744\n",
      "epoch no.2 train no.187800  loss = 2.73902 avg_loss = 3.53750\n",
      "epoch no.2 train no.187810  loss = 4.21270 avg_loss = 3.50663\n",
      "epoch no.2 train no.187820  loss = 3.63936 avg_loss = 3.53549\n",
      "epoch no.2 train no.187830  loss = 3.47586 avg_loss = 3.55163\n",
      "epoch no.2 train no.187840  loss = 3.28073 avg_loss = 3.53647\n",
      "epoch no.2 train no.187850  loss = 3.57482 avg_loss = 3.49824\n",
      "epoch no.2 train no.187860  loss = 4.27411 avg_loss = 3.44234\n",
      "epoch no.2 train no.187870  loss = 4.27002 avg_loss = 3.40099\n",
      "epoch no.2 train no.187880  loss = 5.45894 avg_loss = 3.42911\n",
      "epoch no.2 train no.187890  loss = 4.55202 avg_loss = 3.39086\n",
      "epoch no.2 train no.187900  loss = 5.41769 avg_loss = 3.45305\n",
      "epoch no.2 train no.187910  loss = 2.45219 avg_loss = 3.46214\n",
      "epoch no.2 train no.187920  loss = 2.86107 avg_loss = 3.47944\n",
      "epoch no.2 train no.187930  loss = 2.13146 avg_loss = 3.47060\n",
      "epoch no.2 train no.187940  loss = 4.13402 avg_loss = 3.47369\n",
      "epoch no.2 train no.187950  loss = 3.41548 avg_loss = 3.48091\n",
      "epoch no.2 train no.187960  loss = 4.36549 avg_loss = 3.48021\n",
      "epoch no.2 train no.187970  loss = 2.13395 avg_loss = 3.46121\n",
      "epoch no.2 train no.187980  loss = 2.83930 avg_loss = 3.48198\n",
      "epoch no.2 train no.187990  loss = 3.11637 avg_loss = 3.43147\n",
      "epoch no.2 train no.188000  loss = 2.28882 avg_loss = 3.42162\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁필요할', '때', '▁듣는', '▁노래', '▁p', '</s>']\n",
      "기분전환이 필요할때 듣는 신나는 음악</s>\n",
      "epoch no.2 train no.188010  loss = 3.86171 avg_loss = 3.39257\n",
      "epoch no.2 train no.188020  loss = 1.57041 avg_loss = 3.36737\n",
      "epoch no.2 train no.188030  loss = 4.72934 avg_loss = 3.38301\n",
      "epoch no.2 train no.188040  loss = 2.05088 avg_loss = 3.35121\n",
      "epoch no.2 train no.188050  loss = 5.18053 avg_loss = 3.37477\n",
      "epoch no.2 train no.188060  loss = 3.94488 avg_loss = 3.43152\n",
      "epoch no.2 train no.188070  loss = 3.49675 avg_loss = 3.43956\n",
      "epoch no.2 train no.188080  loss = 4.19830 avg_loss = 3.45683\n",
      "epoch no.2 train no.188090  loss = 4.60979 avg_loss = 3.47702\n",
      "epoch no.2 train no.188100  loss = 2.75761 avg_loss = 3.51251\n",
      "epoch no.2 train no.188110  loss = 2.53422 avg_loss = 3.54276\n",
      "epoch no.2 train no.188120  loss = 2.59425 avg_loss = 3.50797\n",
      "epoch no.2 train no.188130  loss = 2.88651 avg_loss = 3.45864\n",
      "epoch no.2 train no.188140  loss = 4.01103 avg_loss = 3.42996\n",
      "epoch no.2 train no.188150  loss = 2.14660 avg_loss = 3.42593\n",
      "epoch no.2 train no.188160  loss = 4.31274 avg_loss = 3.46513\n",
      "epoch no.2 train no.188170  loss = 4.68640 avg_loss = 3.47455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.188180  loss = 3.16648 avg_loss = 3.40994\n",
      "epoch no.2 train no.188190  loss = 2.91894 avg_loss = 3.43266\n",
      "epoch no.2 train no.188200  loss = 3.86121 avg_loss = 3.49255\n",
      "epoch no.2 train no.188210  loss = 3.45979 avg_loss = 3.47074\n",
      "epoch no.2 train no.188220  loss = 2.33272 avg_loss = 3.45670\n",
      "epoch no.2 train no.188230  loss = 5.11090 avg_loss = 3.47660\n",
      "epoch no.2 train no.188240  loss = 4.71636 avg_loss = 3.51626\n",
      "epoch no.2 train no.188250  loss = 3.51238 avg_loss = 3.55335\n",
      "epoch no.2 train no.188260  loss = 3.63069 avg_loss = 3.53029\n",
      "epoch no.2 train no.188270  loss = 3.95964 avg_loss = 3.56216\n",
      "epoch no.2 train no.188280  loss = 5.36234 avg_loss = 3.52924\n",
      "epoch no.2 train no.188290  loss = 3.62376 avg_loss = 3.51898\n",
      "epoch no.2 train no.188300  loss = 2.43549 avg_loss = 3.52091\n",
      "epoch no.2 train no.188310  loss = 2.60899 avg_loss = 3.46611\n",
      "epoch no.2 train no.188320  loss = 3.11106 avg_loss = 3.44638\n",
      "epoch no.2 train no.188330  loss = 2.48173 avg_loss = 3.45171\n",
      "epoch no.2 train no.188340  loss = 6.26079 avg_loss = 3.48006\n",
      "epoch no.2 train no.188350  loss = 4.83968 avg_loss = 3.49834\n",
      "epoch no.2 train no.188360  loss = 5.22420 avg_loss = 3.54489\n",
      "epoch no.2 train no.188370  loss = 2.86360 avg_loss = 3.53881\n",
      "epoch no.2 train no.188380  loss = 4.46229 avg_loss = 3.55588\n",
      "epoch no.2 train no.188390  loss = 2.21374 avg_loss = 3.54523\n",
      "epoch no.2 train no.188400  loss = 2.83111 avg_loss = 3.52019\n",
      "epoch no.2 train no.188410  loss = 3.74494 avg_loss = 3.54539\n",
      "epoch no.2 train no.188420  loss = 2.67313 avg_loss = 3.54683\n",
      "epoch no.2 train no.188430  loss = 3.10317 avg_loss = 3.58982\n",
      "epoch no.2 train no.188440  loss = 4.06901 avg_loss = 3.57476\n",
      "epoch no.2 train no.188450  loss = 4.03209 avg_loss = 3.55461\n",
      "epoch no.2 train no.188460  loss = 2.51015 avg_loss = 3.55348\n",
      "epoch no.2 train no.188470  loss = 4.78292 avg_loss = 3.57383\n",
      "epoch no.2 train no.188480  loss = 3.73933 avg_loss = 3.52911\n",
      "epoch no.2 train no.188490  loss = 5.19170 avg_loss = 3.55810\n",
      "epoch no.2 train no.188500  loss = 5.33233 avg_loss = 3.58412\n",
      "epoch no.2 train no.188510  loss = 6.24847 avg_loss = 3.59978\n",
      "epoch no.2 train no.188520  loss = 2.15954 avg_loss = 3.54308\n",
      "epoch no.2 train no.188530  loss = 2.57405 avg_loss = 3.49794\n",
      "epoch no.2 train no.188540  loss = 2.46631 avg_loss = 3.48220\n",
      "epoch no.2 train no.188550  loss = 3.67885 avg_loss = 3.49544\n",
      "epoch no.2 train no.188560  loss = 2.04201 avg_loss = 3.48742\n",
      "epoch no.2 train no.188570  loss = 4.49531 avg_loss = 3.51618\n",
      "epoch no.2 train no.188580  loss = 3.99499 avg_loss = 3.54948\n",
      "epoch no.2 train no.188590  loss = 3.81717 avg_loss = 3.56542\n",
      "epoch no.2 train no.188600  loss = 3.72998 avg_loss = 3.58146\n",
      "epoch no.2 train no.188610  loss = 3.49611 avg_loss = 3.59443\n",
      "epoch no.2 train no.188620  loss = 4.97448 avg_loss = 3.54423\n",
      "epoch no.2 train no.188630  loss = 3.54691 avg_loss = 3.53806\n",
      "epoch no.2 train no.188640  loss = 3.12023 avg_loss = 3.60715\n",
      "epoch no.2 train no.188650  loss = 2.96884 avg_loss = 3.58375\n",
      "epoch no.2 train no.188660  loss = 2.51009 avg_loss = 3.50722\n",
      "epoch no.2 train no.188670  loss = 2.20236 avg_loss = 3.50683\n",
      "epoch no.2 train no.188680  loss = 2.80865 avg_loss = 3.48442\n",
      "epoch no.2 train no.188690  loss = 3.68330 avg_loss = 3.54699\n",
      "epoch no.2 train no.188700  loss = 5.12237 avg_loss = 3.55479\n",
      "epoch no.2 train no.188710  loss = 2.70807 avg_loss = 3.50887\n",
      "epoch no.2 train no.188720  loss = 2.98209 avg_loss = 3.47227\n",
      "epoch no.2 train no.188730  loss = 4.85528 avg_loss = 3.47667\n",
      "epoch no.2 train no.188740  loss = 2.20188 avg_loss = 3.43841\n",
      "epoch no.2 train no.188750  loss = 2.64967 avg_loss = 3.46675\n",
      "epoch no.2 train no.188760  loss = 4.89281 avg_loss = 3.47468\n",
      "epoch no.2 train no.188770  loss = 2.93900 avg_loss = 3.46980\n",
      "epoch no.2 train no.188780  loss = 3.29803 avg_loss = 3.52922\n",
      "epoch no.2 train no.188790  loss = 2.27461 avg_loss = 3.51858\n",
      "epoch no.2 train no.188800  loss = 2.57602 avg_loss = 3.54145\n",
      "epoch no.2 train no.188810  loss = 3.04628 avg_loss = 3.51723\n",
      "epoch no.2 train no.188820  loss = 2.25051 avg_loss = 3.47749\n",
      "epoch no.2 train no.188830  loss = 2.74637 avg_loss = 3.48806\n",
      "epoch no.2 train no.188840  loss = 3.07173 avg_loss = 3.45950\n",
      "epoch no.2 train no.188850  loss = 3.41399 avg_loss = 3.41994\n",
      "epoch no.2 train no.188860  loss = 2.60719 avg_loss = 3.43384\n",
      "epoch no.2 train no.188870  loss = 4.60060 avg_loss = 3.44392\n",
      "epoch no.2 train no.188880  loss = 2.55242 avg_loss = 3.46986\n",
      "epoch no.2 train no.188890  loss = 2.74471 avg_loss = 3.52301\n",
      "epoch no.2 train no.188900  loss = 6.39890 avg_loss = 3.51141\n",
      "epoch no.2 train no.188910  loss = 3.32606 avg_loss = 3.54906\n",
      "epoch no.2 train no.188920  loss = 2.61016 avg_loss = 3.57719\n",
      "epoch no.2 train no.188930  loss = 3.77905 avg_loss = 3.57159\n",
      "epoch no.2 train no.188940  loss = 3.34778 avg_loss = 3.59077\n",
      "epoch no.2 train no.188950  loss = 3.47768 avg_loss = 3.57090\n",
      "epoch no.2 train no.188960  loss = 2.03186 avg_loss = 3.52450\n",
      "epoch no.2 train no.188970  loss = 4.01735 avg_loss = 3.49519\n",
      "epoch no.2 train no.188980  loss = 4.91910 avg_loss = 3.56311\n",
      "epoch no.2 train no.188990  loss = 3.38145 avg_loss = 3.54446\n",
      "epoch no.2 train no.189000  loss = 3.94693 avg_loss = 3.56933\n",
      "3\n",
      "to_tokens: ['▁비', '▁좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.189010  loss = 3.37952 avg_loss = 3.58146\n",
      "epoch no.2 train no.189020  loss = 3.18731 avg_loss = 3.55600\n",
      "epoch no.2 train no.189030  loss = 5.51118 avg_loss = 3.55398\n",
      "epoch no.2 train no.189040  loss = 5.17614 avg_loss = 3.60005\n",
      "epoch no.2 train no.189050  loss = 4.01186 avg_loss = 3.57230\n",
      "epoch no.2 train no.189060  loss = 4.08595 avg_loss = 3.57366\n",
      "epoch no.2 train no.189070  loss = 2.40708 avg_loss = 3.48443\n",
      "epoch no.2 train no.189080  loss = 2.94193 avg_loss = 3.46293\n",
      "epoch no.2 train no.189090  loss = 2.51704 avg_loss = 3.48567\n",
      "epoch no.2 train no.189100  loss = 3.30325 avg_loss = 3.50061\n",
      "epoch no.2 train no.189110  loss = 2.64687 avg_loss = 3.51159\n",
      "epoch no.2 train no.189120  loss = 2.25034 avg_loss = 3.51705\n",
      "epoch no.2 train no.189130  loss = 2.75424 avg_loss = 3.50345\n",
      "epoch no.2 train no.189140  loss = 3.29081 avg_loss = 3.47568\n",
      "epoch no.2 train no.189150  loss = 3.29847 avg_loss = 3.47033\n",
      "epoch no.2 train no.189160  loss = 3.90691 avg_loss = 3.50698\n",
      "epoch no.2 train no.189170  loss = 3.01462 avg_loss = 3.55752\n",
      "epoch no.2 train no.189180  loss = 3.57984 avg_loss = 3.54258\n",
      "epoch no.2 train no.189190  loss = 3.91747 avg_loss = 3.53488\n",
      "epoch no.2 train no.189200  loss = 4.49045 avg_loss = 3.52287\n",
      "epoch no.2 train no.189210  loss = 1.87109 avg_loss = 3.44444\n",
      "epoch no.2 train no.189220  loss = 2.24382 avg_loss = 3.48770\n",
      "epoch no.2 train no.189230  loss = 2.90551 avg_loss = 3.46542\n",
      "epoch no.2 train no.189240  loss = 3.33385 avg_loss = 3.48159\n",
      "epoch no.2 train no.189250  loss = 2.25469 avg_loss = 3.47373\n",
      "epoch no.2 train no.189260  loss = 2.30951 avg_loss = 3.49662\n",
      "epoch no.2 train no.189270  loss = 3.83981 avg_loss = 3.49539\n",
      "epoch no.2 train no.189280  loss = 4.00803 avg_loss = 3.47742\n",
      "epoch no.2 train no.189290  loss = 3.23810 avg_loss = 3.49729\n",
      "epoch no.2 train no.189300  loss = 2.42966 avg_loss = 3.51652\n",
      "epoch no.2 train no.189310  loss = 2.55702 avg_loss = 3.48091\n",
      "epoch no.2 train no.189320  loss = 3.43794 avg_loss = 3.46792\n",
      "epoch no.2 train no.189330  loss = 2.77092 avg_loss = 3.44711\n",
      "epoch no.2 train no.189340  loss = 2.78254 avg_loss = 3.44544\n",
      "epoch no.2 train no.189350  loss = 4.70347 avg_loss = 3.44371\n",
      "epoch no.2 train no.189360  loss = 4.05028 avg_loss = 3.43669\n",
      "epoch no.2 train no.189370  loss = 5.17886 avg_loss = 3.42681\n",
      "epoch no.2 train no.189380  loss = 1.68672 avg_loss = 3.50190\n",
      "epoch no.2 train no.189390  loss = 4.40622 avg_loss = 3.50124\n",
      "epoch no.2 train no.189400  loss = 3.49657 avg_loss = 3.51080\n",
      "epoch no.2 train no.189410  loss = 2.78235 avg_loss = 3.49961\n",
      "epoch no.2 train no.189420  loss = 4.85284 avg_loss = 3.53354\n",
      "epoch no.2 train no.189430  loss = 3.71583 avg_loss = 3.54513\n",
      "epoch no.2 train no.189440  loss = 3.54495 avg_loss = 3.52313\n",
      "epoch no.2 train no.189450  loss = 2.15130 avg_loss = 3.54902\n",
      "epoch no.2 train no.189460  loss = 4.07730 avg_loss = 3.58268\n",
      "epoch no.2 train no.189470  loss = 3.65629 avg_loss = 3.55839\n",
      "epoch no.2 train no.189480  loss = 2.68134 avg_loss = 3.53525\n",
      "epoch no.2 train no.189490  loss = 4.06940 avg_loss = 3.51496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.189500  loss = 3.39592 avg_loss = 3.55514\n",
      "epoch no.2 train no.189510  loss = 3.05061 avg_loss = 3.56979\n",
      "epoch no.2 train no.189520  loss = 2.55038 avg_loss = 3.57415\n",
      "epoch no.2 train no.189530  loss = 4.54962 avg_loss = 3.58079\n",
      "epoch no.2 train no.189540  loss = 2.62237 avg_loss = 3.59736\n",
      "epoch no.2 train no.189550  loss = 3.04937 avg_loss = 3.62299\n",
      "epoch no.2 train no.189560  loss = 3.22537 avg_loss = 3.59225\n",
      "epoch no.2 train no.189570  loss = 3.21651 avg_loss = 3.54640\n",
      "epoch no.2 train no.189580  loss = 3.07465 avg_loss = 3.52539\n",
      "epoch no.2 train no.189590  loss = 2.85499 avg_loss = 3.51657\n",
      "epoch no.2 train no.189600  loss = 4.85796 avg_loss = 3.50610\n",
      "epoch no.2 train no.189610  loss = 1.20934 avg_loss = 3.43093\n",
      "epoch no.2 train no.189620  loss = 3.94376 avg_loss = 3.49405\n",
      "epoch no.2 train no.189630  loss = 3.79709 avg_loss = 3.48189\n",
      "epoch no.2 train no.189640  loss = 3.41360 avg_loss = 3.48203\n",
      "epoch no.2 train no.189650  loss = 2.52560 avg_loss = 3.46144\n",
      "epoch no.2 train no.189660  loss = 3.00565 avg_loss = 3.47664\n",
      "epoch no.2 train no.189670  loss = 4.14078 avg_loss = 3.51169\n",
      "epoch no.2 train no.189680  loss = 4.28814 avg_loss = 3.57706\n",
      "epoch no.2 train no.189690  loss = 2.88460 avg_loss = 3.60086\n",
      "epoch no.2 train no.189700  loss = 2.88491 avg_loss = 3.52536\n",
      "epoch no.2 train no.189710  loss = 2.47743 avg_loss = 3.52435\n",
      "epoch no.2 train no.189720  loss = 1.91005 avg_loss = 3.53990\n",
      "epoch no.2 train no.189730  loss = 3.13905 avg_loss = 3.56887\n",
      "epoch no.2 train no.189740  loss = 3.23507 avg_loss = 3.53453\n",
      "epoch no.2 train no.189750  loss = 3.38849 avg_loss = 3.51889\n",
      "epoch no.2 train no.189760  loss = 3.05538 avg_loss = 3.53723\n",
      "epoch no.2 train no.189770  loss = 2.28365 avg_loss = 3.57666\n",
      "epoch no.2 train no.189780  loss = 3.36249 avg_loss = 3.55530\n",
      "epoch no.2 train no.189790  loss = 2.60325 avg_loss = 3.53289\n",
      "epoch no.2 train no.189800  loss = 6.17157 avg_loss = 3.53287\n",
      "epoch no.2 train no.189810  loss = 3.13497 avg_loss = 3.56557\n",
      "epoch no.2 train no.189820  loss = 2.11455 avg_loss = 3.58542\n",
      "epoch no.2 train no.189830  loss = 3.00703 avg_loss = 3.56548\n",
      "epoch no.2 train no.189840  loss = 4.25858 avg_loss = 3.56083\n",
      "epoch no.2 train no.189850  loss = 3.33134 avg_loss = 3.54349\n",
      "epoch no.2 train no.189860  loss = 3.92363 avg_loss = 3.57977\n",
      "epoch no.2 train no.189870  loss = 4.04766 avg_loss = 3.61320\n",
      "epoch no.2 train no.189880  loss = 2.67902 avg_loss = 3.55731\n",
      "epoch no.2 train no.189890  loss = 3.79780 avg_loss = 3.57663\n",
      "epoch no.2 train no.189900  loss = 5.82965 avg_loss = 3.61530\n",
      "epoch no.2 train no.189910  loss = 3.71625 avg_loss = 3.61758\n",
      "epoch no.2 train no.189920  loss = 4.43210 avg_loss = 3.59193\n",
      "epoch no.2 train no.189930  loss = 4.24604 avg_loss = 3.61239\n",
      "epoch no.2 train no.189940  loss = 3.57893 avg_loss = 3.62869\n",
      "epoch no.2 train no.189950  loss = 2.97418 avg_loss = 3.56962\n",
      "epoch no.2 train no.189960  loss = 4.69191 avg_loss = 3.55816\n",
      "epoch no.2 train no.189970  loss = 3.08252 avg_loss = 3.55273\n",
      "epoch no.2 train no.189980  loss = 4.42474 avg_loss = 3.56266\n",
      "epoch no.2 train no.189990  loss = 1.87747 avg_loss = 3.51198\n",
      "epoch no.2 train no.190000  loss = 2.71420 avg_loss = 3.51113\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '하기', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.2 train no.190010  loss = 4.25400 avg_loss = 3.52203\n",
      "epoch no.2 train no.190020  loss = 3.71554 avg_loss = 3.52922\n",
      "epoch no.2 train no.190030  loss = 3.42787 avg_loss = 3.53009\n",
      "epoch no.2 train no.190040  loss = 3.28590 avg_loss = 3.53990\n",
      "epoch no.2 train no.190050  loss = 3.06656 avg_loss = 3.55473\n",
      "epoch no.2 train no.190060  loss = 3.36255 avg_loss = 3.53373\n",
      "epoch no.2 train no.190070  loss = 4.73616 avg_loss = 3.57370\n",
      "epoch no.2 train no.190080  loss = 4.70136 avg_loss = 3.58912\n",
      "epoch no.2 train no.190090  loss = 3.96481 avg_loss = 3.60220\n",
      "epoch no.2 train no.190100  loss = 2.26364 avg_loss = 3.55239\n",
      "epoch no.2 train no.190110  loss = 4.26831 avg_loss = 3.56875\n",
      "epoch no.2 train no.190120  loss = 2.71519 avg_loss = 3.58573\n",
      "epoch no.2 train no.190130  loss = 3.66764 avg_loss = 3.56784\n",
      "epoch no.2 train no.190140  loss = 5.77233 avg_loss = 3.59899\n",
      "epoch no.2 train no.190150  loss = 3.12729 avg_loss = 3.58279\n",
      "epoch no.2 train no.190160  loss = 4.76062 avg_loss = 3.60036\n",
      "epoch no.2 train no.190170  loss = 2.88696 avg_loss = 3.60875\n",
      "epoch no.2 train no.190180  loss = 3.28802 avg_loss = 3.63407\n",
      "epoch no.2 train no.190190  loss = 5.42937 avg_loss = 3.64596\n",
      "epoch no.2 train no.190200  loss = 3.03896 avg_loss = 3.62840\n",
      "epoch no.2 train no.190210  loss = 4.26559 avg_loss = 3.64590\n",
      "epoch no.2 train no.190220  loss = 3.41205 avg_loss = 3.60941\n",
      "epoch no.2 train no.190230  loss = 3.76555 avg_loss = 3.64146\n",
      "epoch no.2 train no.190240  loss = 5.50348 avg_loss = 3.63655\n",
      "epoch no.2 train no.190250  loss = 2.69783 avg_loss = 3.62062\n",
      "epoch no.2 train no.190260  loss = 2.69386 avg_loss = 3.63753\n",
      "epoch no.2 train no.190270  loss = 4.19503 avg_loss = 3.65776\n",
      "epoch no.2 train no.190280  loss = 2.77322 avg_loss = 3.64521\n",
      "epoch no.2 train no.190290  loss = 2.82160 avg_loss = 3.62803\n",
      "epoch no.2 train no.190300  loss = 3.36620 avg_loss = 3.60510\n",
      "epoch no.2 train no.190310  loss = 4.24852 avg_loss = 3.60953\n",
      "epoch no.2 train no.190320  loss = 4.26002 avg_loss = 3.61825\n",
      "epoch no.2 train no.190330  loss = 5.57904 avg_loss = 3.62874\n",
      "epoch no.2 train no.190340  loss = 6.31051 avg_loss = 3.59566\n",
      "epoch no.2 train no.190350  loss = 2.57599 avg_loss = 3.56985\n",
      "epoch no.2 train no.190360  loss = 3.16644 avg_loss = 3.53690\n",
      "epoch no.2 train no.190370  loss = 3.28022 avg_loss = 3.52432\n",
      "epoch no.2 train no.190380  loss = 2.51635 avg_loss = 3.54213\n",
      "epoch no.2 train no.190390  loss = 3.09459 avg_loss = 3.47619\n",
      "epoch no.2 train no.190400  loss = 4.92208 avg_loss = 3.51740\n",
      "epoch no.2 train no.190410  loss = 3.93222 avg_loss = 3.59177\n",
      "epoch no.2 train no.190420  loss = 2.08957 avg_loss = 3.66098\n",
      "epoch no.2 train no.190430  loss = 2.29022 avg_loss = 3.65578\n",
      "epoch no.2 train no.190440  loss = 3.20038 avg_loss = 3.64111\n",
      "epoch no.2 train no.190450  loss = 3.50756 avg_loss = 3.62222\n",
      "epoch no.2 train no.190460  loss = 5.88732 avg_loss = 3.68298\n",
      "epoch no.2 train no.190470  loss = 2.91980 avg_loss = 3.66765\n",
      "epoch no.2 train no.190480  loss = 4.23400 avg_loss = 3.66466\n",
      "epoch no.2 train no.190490  loss = 3.62703 avg_loss = 3.67103\n",
      "epoch no.2 train no.190500  loss = 3.80359 avg_loss = 3.66884\n",
      "epoch no.2 train no.190510  loss = 4.39392 avg_loss = 3.69549\n",
      "epoch no.2 train no.190520  loss = 2.76957 avg_loss = 3.69959\n",
      "epoch no.2 train no.190530  loss = 4.46746 avg_loss = 3.73024\n",
      "epoch no.2 train no.190540  loss = 3.47157 avg_loss = 3.74952\n",
      "epoch no.2 train no.190550  loss = 4.21792 avg_loss = 3.68717\n",
      "epoch no.2 train no.190560  loss = 2.54177 avg_loss = 3.68372\n",
      "epoch no.2 train no.190570  loss = 3.25934 avg_loss = 3.65021\n",
      "epoch no.2 train no.190580  loss = 3.84613 avg_loss = 3.65691\n",
      "epoch no.2 train no.190590  loss = 2.57150 avg_loss = 3.66508\n",
      "epoch no.2 train no.190600  loss = 3.42885 avg_loss = 3.65404\n",
      "epoch no.2 train no.190610  loss = 4.76261 avg_loss = 3.63421\n",
      "epoch no.2 train no.190620  loss = 3.21081 avg_loss = 3.59559\n",
      "epoch no.2 train no.190630  loss = 4.08306 avg_loss = 3.60458\n",
      "epoch no.2 train no.190640  loss = 3.60370 avg_loss = 3.60211\n",
      "epoch no.2 train no.190650  loss = 4.13572 avg_loss = 3.63529\n",
      "epoch no.2 train no.190660  loss = 5.40821 avg_loss = 3.67647\n",
      "epoch no.2 train no.190670  loss = 5.93725 avg_loss = 3.66986\n",
      "epoch no.2 train no.190680  loss = 2.54963 avg_loss = 3.63062\n",
      "epoch no.2 train no.190690  loss = 2.98929 avg_loss = 3.60318\n",
      "epoch no.2 train no.190700  loss = 2.58679 avg_loss = 3.57726\n",
      "epoch no.2 train no.190710  loss = 3.74407 avg_loss = 3.60058\n",
      "epoch no.2 train no.190720  loss = 3.87984 avg_loss = 3.61900\n",
      "epoch no.2 train no.190730  loss = 2.43408 avg_loss = 3.59422\n",
      "epoch no.2 train no.190740  loss = 3.82796 avg_loss = 3.54280\n",
      "epoch no.2 train no.190750  loss = 3.08183 avg_loss = 3.60495\n",
      "epoch no.2 train no.190760  loss = 4.02028 avg_loss = 3.60756\n",
      "epoch no.2 train no.190770  loss = 4.52112 avg_loss = 3.58323\n",
      "epoch no.2 train no.190780  loss = 2.60871 avg_loss = 3.60735\n",
      "epoch no.2 train no.190790  loss = 4.38198 avg_loss = 3.59727\n",
      "epoch no.2 train no.190800  loss = 3.82944 avg_loss = 3.55437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.190810  loss = 3.40662 avg_loss = 3.58026\n",
      "epoch no.2 train no.190820  loss = 5.84025 avg_loss = 3.59145\n",
      "epoch no.2 train no.190830  loss = 3.96878 avg_loss = 3.53171\n",
      "epoch no.2 train no.190840  loss = 1.55421 avg_loss = 3.51655\n",
      "epoch no.2 train no.190850  loss = 2.95816 avg_loss = 3.53125\n",
      "epoch no.2 train no.190860  loss = 5.85083 avg_loss = 3.52788\n",
      "epoch no.2 train no.190870  loss = 1.87553 avg_loss = 3.50115\n",
      "epoch no.2 train no.190880  loss = 3.55443 avg_loss = 3.51696\n",
      "epoch no.2 train no.190890  loss = 3.04279 avg_loss = 3.51064\n",
      "epoch no.2 train no.190900  loss = 3.35777 avg_loss = 3.49862\n",
      "epoch no.2 train no.190910  loss = 2.38962 avg_loss = 3.50325\n",
      "epoch no.2 train no.190920  loss = 3.42466 avg_loss = 3.50387\n",
      "epoch no.2 train no.190930  loss = 3.04950 avg_loss = 3.52233\n",
      "epoch no.2 train no.190940  loss = 2.56031 avg_loss = 3.51626\n",
      "epoch no.2 train no.190950  loss = 3.67166 avg_loss = 3.49652\n",
      "epoch no.2 train no.190960  loss = 4.95130 avg_loss = 3.51416\n",
      "epoch no.2 train no.190970  loss = 2.53212 avg_loss = 3.51324\n",
      "epoch no.2 train no.190980  loss = 3.35469 avg_loss = 3.53866\n",
      "epoch no.2 train no.190990  loss = 2.50089 avg_loss = 3.52314\n",
      "epoch no.2 train no.191000  loss = 3.00686 avg_loss = 3.50105\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁딱', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 딱 좋은 음악</s>\n",
      "epoch no.2 train no.191010  loss = 3.65400 avg_loss = 3.49149\n",
      "epoch no.2 train no.191020  loss = 2.70000 avg_loss = 3.51277\n",
      "epoch no.2 train no.191030  loss = 3.62890 avg_loss = 3.45505\n",
      "epoch no.2 train no.191040  loss = 4.24994 avg_loss = 3.49288\n",
      "epoch no.2 train no.191050  loss = 2.63050 avg_loss = 3.47876\n",
      "epoch no.2 train no.191060  loss = 3.85684 avg_loss = 3.46466\n",
      "epoch no.2 train no.191070  loss = 3.54829 avg_loss = 3.46570\n",
      "epoch no.2 train no.191080  loss = 4.93661 avg_loss = 3.44941\n",
      "epoch no.2 train no.191090  loss = 2.66515 avg_loss = 3.49916\n",
      "epoch no.2 train no.191100  loss = 5.41514 avg_loss = 3.53927\n",
      "epoch no.2 train no.191110  loss = 3.63106 avg_loss = 3.57031\n",
      "epoch no.2 train no.191120  loss = 3.59683 avg_loss = 3.55530\n",
      "epoch no.2 train no.191130  loss = 2.86980 avg_loss = 3.55040\n",
      "epoch no.2 train no.191140  loss = 3.14890 avg_loss = 3.56135\n",
      "epoch no.2 train no.191150  loss = 2.75852 avg_loss = 3.56814\n",
      "epoch no.2 train no.191160  loss = 4.27125 avg_loss = 3.53418\n",
      "epoch no.2 train no.191170  loss = 2.61924 avg_loss = 3.48053\n",
      "epoch no.2 train no.191180  loss = 2.73264 avg_loss = 3.48102\n",
      "epoch no.2 train no.191190  loss = 5.15342 avg_loss = 3.51942\n",
      "epoch no.2 train no.191200  loss = 3.11296 avg_loss = 3.53312\n",
      "epoch no.2 train no.191210  loss = 3.03273 avg_loss = 3.50760\n",
      "epoch no.2 train no.191220  loss = 3.80886 avg_loss = 3.53780\n",
      "epoch no.2 train no.191230  loss = 3.26989 avg_loss = 3.56477\n",
      "epoch no.2 train no.191240  loss = 3.25956 avg_loss = 3.55459\n",
      "epoch no.2 train no.191250  loss = 3.53282 avg_loss = 3.49889\n",
      "epoch no.2 train no.191260  loss = 2.68573 avg_loss = 3.47174\n",
      "epoch no.2 train no.191270  loss = 2.99073 avg_loss = 3.44407\n",
      "epoch no.2 train no.191280  loss = 3.90897 avg_loss = 3.46747\n",
      "epoch no.2 train no.191290  loss = 1.99737 avg_loss = 3.43441\n",
      "epoch no.2 train no.191300  loss = 2.54186 avg_loss = 3.47477\n",
      "epoch no.2 train no.191310  loss = 4.36218 avg_loss = 3.49942\n",
      "epoch no.2 train no.191320  loss = 2.78270 avg_loss = 3.46620\n",
      "epoch no.2 train no.191330  loss = 2.54204 avg_loss = 3.49199\n",
      "epoch no.2 train no.191340  loss = 3.05266 avg_loss = 3.44985\n",
      "epoch no.2 train no.191350  loss = 4.19909 avg_loss = 3.48646\n",
      "epoch no.2 train no.191360  loss = 3.15531 avg_loss = 3.45268\n",
      "epoch no.2 train no.191370  loss = 3.51108 avg_loss = 3.46430\n",
      "epoch no.2 train no.191380  loss = 3.30184 avg_loss = 3.42147\n",
      "epoch no.2 train no.191390  loss = 2.56298 avg_loss = 3.38509\n",
      "epoch no.2 train no.191400  loss = 4.91858 avg_loss = 3.44010\n",
      "epoch no.2 train no.191410  loss = 3.60795 avg_loss = 3.46460\n",
      "epoch no.2 train no.191420  loss = 5.06948 avg_loss = 3.48434\n",
      "epoch no.2 train no.191430  loss = 4.67004 avg_loss = 3.46917\n",
      "epoch no.2 train no.191440  loss = 2.62364 avg_loss = 3.40387\n",
      "epoch no.2 train no.191450  loss = 2.37647 avg_loss = 3.40453\n",
      "epoch no.2 train no.191460  loss = 3.60418 avg_loss = 3.49834\n",
      "epoch no.2 train no.191470  loss = 4.74644 avg_loss = 3.49305\n",
      "epoch no.2 train no.191480  loss = 4.75200 avg_loss = 3.49618\n",
      "epoch no.2 train no.191490  loss = 6.08476 avg_loss = 3.48857\n",
      "epoch no.2 train no.191500  loss = 5.53200 avg_loss = 3.54796\n",
      "epoch no.2 train no.191510  loss = 4.14382 avg_loss = 3.50254\n",
      "epoch no.2 train no.191520  loss = 3.83832 avg_loss = 3.48987\n",
      "epoch no.2 train no.191530  loss = 2.69419 avg_loss = 3.49022\n",
      "epoch no.2 train no.191540  loss = 2.42937 avg_loss = 3.50154\n",
      "epoch no.2 train no.191550  loss = 3.16884 avg_loss = 3.55222\n",
      "epoch no.2 train no.191560  loss = 2.98299 avg_loss = 3.52363\n",
      "epoch no.2 train no.191570  loss = 5.10133 avg_loss = 3.55553\n",
      "epoch no.2 train no.191580  loss = 3.62675 avg_loss = 3.56602\n",
      "epoch no.2 train no.191590  loss = 4.01052 avg_loss = 3.58867\n",
      "epoch no.2 train no.191600  loss = 6.52936 avg_loss = 3.63501\n",
      "epoch no.2 train no.191610  loss = 3.07139 avg_loss = 3.65482\n",
      "epoch no.2 train no.191620  loss = 4.45086 avg_loss = 3.61826\n",
      "epoch no.2 train no.191630  loss = 3.90733 avg_loss = 3.61136\n",
      "epoch no.2 train no.191640  loss = 3.18739 avg_loss = 3.59947\n",
      "epoch no.2 train no.191650  loss = 3.86637 avg_loss = 3.56463\n",
      "epoch no.2 train no.191660  loss = 2.33472 avg_loss = 3.55561\n",
      "epoch no.2 train no.191670  loss = 2.04371 avg_loss = 3.51464\n",
      "epoch no.2 train no.191680  loss = 3.90309 avg_loss = 3.55764\n",
      "epoch no.2 train no.191690  loss = 2.70486 avg_loss = 3.58456\n",
      "epoch no.2 train no.191700  loss = 4.05984 avg_loss = 3.63217\n",
      "epoch no.2 train no.191710  loss = 3.92069 avg_loss = 3.56625\n",
      "epoch no.2 train no.191720  loss = 3.91721 avg_loss = 3.55714\n",
      "epoch no.2 train no.191730  loss = 3.43375 avg_loss = 3.55707\n",
      "epoch no.2 train no.191740  loss = 4.42118 avg_loss = 3.59856\n",
      "epoch no.2 train no.191750  loss = 2.18607 avg_loss = 3.53639\n",
      "epoch no.2 train no.191760  loss = 6.47824 avg_loss = 3.55198\n",
      "epoch no.2 train no.191770  loss = 3.58969 avg_loss = 3.57470\n",
      "epoch no.2 train no.191780  loss = 4.27096 avg_loss = 3.62691\n",
      "epoch no.2 train no.191790  loss = 3.66510 avg_loss = 3.64260\n",
      "epoch no.2 train no.191800  loss = 3.67073 avg_loss = 3.63778\n",
      "epoch no.2 train no.191810  loss = 1.97731 avg_loss = 3.65633\n",
      "epoch no.2 train no.191820  loss = 1.96482 avg_loss = 3.72046\n",
      "epoch no.2 train no.191830  loss = 5.92197 avg_loss = 3.71529\n",
      "epoch no.2 train no.191840  loss = 1.56712 avg_loss = 3.68983\n",
      "epoch no.2 train no.191850  loss = 2.84802 avg_loss = 3.63411\n",
      "epoch no.2 train no.191860  loss = 4.95180 avg_loss = 3.63326\n",
      "epoch no.2 train no.191870  loss = 3.14980 avg_loss = 3.57458\n",
      "epoch no.2 train no.191880  loss = 4.37429 avg_loss = 3.56707\n",
      "epoch no.2 train no.191890  loss = 3.59781 avg_loss = 3.61299\n",
      "epoch no.2 train no.191900  loss = 3.45304 avg_loss = 3.59449\n",
      "epoch no.2 train no.191910  loss = 4.27932 avg_loss = 3.58121\n",
      "epoch no.2 train no.191920  loss = 3.06486 avg_loss = 3.59774\n",
      "epoch no.2 train no.191930  loss = 4.16291 avg_loss = 3.59835\n",
      "epoch no.2 train no.191940  loss = 2.76801 avg_loss = 3.63568\n",
      "epoch no.2 train no.191950  loss = 3.84735 avg_loss = 3.60930\n",
      "epoch no.2 train no.191960  loss = 2.26443 avg_loss = 3.60881\n",
      "epoch no.2 train no.191970  loss = 2.26464 avg_loss = 3.62666\n",
      "epoch no.2 train no.191980  loss = 3.29088 avg_loss = 3.64151\n",
      "epoch no.2 train no.191990  loss = 4.46580 avg_loss = 3.68587\n",
      "epoch no.2 train no.192000  loss = 2.51132 avg_loss = 3.70404\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '싶', '때', '▁듣는']\n",
      "기분전환하고 싶을 때</s>\n",
      "epoch no.2 train no.192010  loss = 3.58728 avg_loss = 3.66501\n",
      "epoch no.2 train no.192020  loss = 4.83971 avg_loss = 3.67563\n",
      "epoch no.2 train no.192030  loss = 3.83126 avg_loss = 3.68019\n",
      "epoch no.2 train no.192040  loss = 2.37407 avg_loss = 3.63739\n",
      "epoch no.2 train no.192050  loss = 2.11225 avg_loss = 3.65157\n",
      "epoch no.2 train no.192060  loss = 3.23747 avg_loss = 3.69948\n",
      "epoch no.2 train no.192070  loss = 5.43954 avg_loss = 3.72160\n",
      "epoch no.2 train no.192080  loss = 2.74175 avg_loss = 3.69140\n",
      "epoch no.2 train no.192090  loss = 3.08556 avg_loss = 3.65587\n",
      "epoch no.2 train no.192100  loss = 3.39068 avg_loss = 3.64763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.192110  loss = 2.84117 avg_loss = 3.64954\n",
      "epoch no.2 train no.192120  loss = 2.25573 avg_loss = 3.63411\n",
      "epoch no.2 train no.192130  loss = 2.10049 avg_loss = 3.60155\n",
      "epoch no.2 train no.192140  loss = 3.05584 avg_loss = 3.54730\n",
      "epoch no.2 train no.192150  loss = 3.39095 avg_loss = 3.58674\n",
      "epoch no.2 train no.192160  loss = 3.02462 avg_loss = 3.58679\n",
      "epoch no.2 train no.192170  loss = 6.77953 avg_loss = 3.64356\n",
      "epoch no.2 train no.192180  loss = 2.36460 avg_loss = 3.60473\n",
      "epoch no.2 train no.192190  loss = 3.85768 avg_loss = 3.62501\n",
      "epoch no.2 train no.192200  loss = 5.10467 avg_loss = 3.66472\n",
      "epoch no.2 train no.192210  loss = 3.38351 avg_loss = 3.65926\n",
      "epoch no.2 train no.192220  loss = 2.87367 avg_loss = 3.67084\n",
      "epoch no.2 train no.192230  loss = 1.48351 avg_loss = 3.61339\n",
      "epoch no.2 train no.192240  loss = 4.21732 avg_loss = 3.60306\n",
      "epoch no.2 train no.192250  loss = 2.41771 avg_loss = 3.56220\n",
      "epoch no.2 train no.192260  loss = 5.33838 avg_loss = 3.56603\n",
      "epoch no.2 train no.192270  loss = 3.69129 avg_loss = 3.56590\n",
      "epoch no.2 train no.192280  loss = 2.94125 avg_loss = 3.51980\n",
      "epoch no.2 train no.192290  loss = 3.41752 avg_loss = 3.50483\n",
      "epoch no.2 train no.192300  loss = 2.10485 avg_loss = 3.49993\n",
      "epoch no.2 train no.192310  loss = 2.18614 avg_loss = 3.47545\n",
      "epoch no.2 train no.192320  loss = 4.13422 avg_loss = 3.48258\n",
      "epoch no.2 train no.192330  loss = 3.48986 avg_loss = 3.43292\n",
      "epoch no.2 train no.192340  loss = 3.14880 avg_loss = 3.44413\n",
      "epoch no.2 train no.192350  loss = 3.70750 avg_loss = 3.42329\n",
      "epoch no.2 train no.192360  loss = 4.70188 avg_loss = 3.45174\n",
      "epoch no.2 train no.192370  loss = 4.88280 avg_loss = 3.42419\n",
      "epoch no.2 train no.192380  loss = 3.07794 avg_loss = 3.41218\n",
      "epoch no.2 train no.192390  loss = 3.77960 avg_loss = 3.38085\n",
      "epoch no.2 train no.192400  loss = 4.37139 avg_loss = 3.32540\n",
      "epoch no.2 train no.192410  loss = 2.57068 avg_loss = 3.29132\n",
      "epoch no.2 train no.192420  loss = 2.73881 avg_loss = 3.29391\n",
      "epoch no.2 train no.192430  loss = 5.08518 avg_loss = 3.33704\n",
      "epoch no.2 train no.192440  loss = 3.14840 avg_loss = 3.33492\n",
      "epoch no.2 train no.192450  loss = 3.09151 avg_loss = 3.37337\n",
      "epoch no.2 train no.192460  loss = 3.17822 avg_loss = 3.42842\n",
      "epoch no.2 train no.192470  loss = 3.40324 avg_loss = 3.40625\n",
      "epoch no.2 train no.192480  loss = 3.59628 avg_loss = 3.35965\n",
      "epoch no.2 train no.192490  loss = 3.31340 avg_loss = 3.35506\n",
      "epoch no.2 train no.192500  loss = 3.02302 avg_loss = 3.38932\n",
      "epoch no.2 train no.192510  loss = 3.37805 avg_loss = 3.39162\n",
      "epoch no.2 train no.192520  loss = 4.23527 avg_loss = 3.36776\n",
      "epoch no.2 train no.192530  loss = 3.86754 avg_loss = 3.42755\n",
      "epoch no.2 train no.192540  loss = 2.38223 avg_loss = 3.35920\n",
      "epoch no.2 train no.192550  loss = 2.83103 avg_loss = 3.37430\n",
      "epoch no.2 train no.192560  loss = 5.13208 avg_loss = 3.40513\n",
      "epoch no.2 train no.192570  loss = 3.05831 avg_loss = 3.46093\n",
      "epoch no.2 train no.192580  loss = 3.36840 avg_loss = 3.43949\n",
      "epoch no.2 train no.192590  loss = 2.18275 avg_loss = 3.43957\n",
      "epoch no.2 train no.192600  loss = 3.51532 avg_loss = 3.43715\n",
      "epoch no.2 train no.192610  loss = 2.31890 avg_loss = 3.40492\n",
      "epoch no.2 train no.192620  loss = 6.30326 avg_loss = 3.49186\n",
      "epoch no.2 train no.192630  loss = 2.18028 avg_loss = 3.47738\n",
      "epoch no.2 train no.192640  loss = 5.50989 avg_loss = 3.51303\n",
      "epoch no.2 train no.192650  loss = 2.68776 avg_loss = 3.54164\n",
      "epoch no.2 train no.192660  loss = 3.09307 avg_loss = 3.53000\n",
      "epoch no.2 train no.192670  loss = 4.60787 avg_loss = 3.52867\n",
      "epoch no.2 train no.192680  loss = 4.87503 avg_loss = 3.51512\n",
      "epoch no.2 train no.192690  loss = 4.03490 avg_loss = 3.54607\n",
      "epoch no.2 train no.192700  loss = 3.16201 avg_loss = 3.50864\n",
      "epoch no.2 train no.192710  loss = 2.70159 avg_loss = 3.53034\n",
      "epoch no.2 train no.192720  loss = 3.96783 avg_loss = 3.57797\n",
      "epoch no.2 train no.192730  loss = 3.56674 avg_loss = 3.55098\n",
      "epoch no.2 train no.192740  loss = 6.30469 avg_loss = 3.60605\n",
      "epoch no.2 train no.192750  loss = 5.84822 avg_loss = 3.61519\n",
      "epoch no.2 train no.192760  loss = 3.57614 avg_loss = 3.64472\n",
      "epoch no.2 train no.192770  loss = 2.33808 avg_loss = 3.59576\n",
      "epoch no.2 train no.192780  loss = 3.28880 avg_loss = 3.59501\n",
      "epoch no.2 train no.192790  loss = 3.80869 avg_loss = 3.62245\n",
      "epoch no.2 train no.192800  loss = 4.93191 avg_loss = 3.61362\n",
      "epoch no.2 train no.192810  loss = 5.54329 avg_loss = 3.62858\n",
      "epoch no.2 train no.192820  loss = 3.04711 avg_loss = 3.59825\n",
      "epoch no.2 train no.192830  loss = 3.83689 avg_loss = 3.57931\n",
      "epoch no.2 train no.192840  loss = 2.02951 avg_loss = 3.56147\n",
      "epoch no.2 train no.192850  loss = 3.58248 avg_loss = 3.48612\n",
      "epoch no.2 train no.192860  loss = 3.32198 avg_loss = 3.50104\n",
      "epoch no.2 train no.192870  loss = 5.27631 avg_loss = 3.52378\n",
      "epoch no.2 train no.192880  loss = 3.60285 avg_loss = 3.53680\n",
      "epoch no.2 train no.192890  loss = 2.21838 avg_loss = 3.50793\n",
      "epoch no.2 train no.192900  loss = 1.46155 avg_loss = 3.49019\n",
      "epoch no.2 train no.192910  loss = 3.43870 avg_loss = 3.52728\n",
      "epoch no.2 train no.192920  loss = 3.48246 avg_loss = 3.54324\n",
      "epoch no.2 train no.192930  loss = 2.92132 avg_loss = 3.51999\n",
      "epoch no.2 train no.192940  loss = 5.63278 avg_loss = 3.58018\n",
      "epoch no.2 train no.192950  loss = 4.55743 avg_loss = 3.62385\n",
      "epoch no.2 train no.192960  loss = 1.79897 avg_loss = 3.61803\n",
      "epoch no.2 train no.192970  loss = 3.29701 avg_loss = 3.65231\n",
      "epoch no.2 train no.192980  loss = 1.72924 avg_loss = 3.59325\n",
      "epoch no.2 train no.192990  loss = 2.89949 avg_loss = 3.54614\n",
      "epoch no.2 train no.193000  loss = 6.04217 avg_loss = 3.57456\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '싶', '을', '때', '</s>']\n",
      "기분전환 하고싶을때</s>\n",
      "epoch no.2 train no.193010  loss = 2.89150 avg_loss = 3.61057\n",
      "epoch no.2 train no.193020  loss = 5.24591 avg_loss = 3.63730\n",
      "epoch no.2 train no.193030  loss = 2.86571 avg_loss = 3.58899\n",
      "epoch no.2 train no.193040  loss = 5.13384 avg_loss = 3.55338\n",
      "epoch no.2 train no.193050  loss = 2.60252 avg_loss = 3.55856\n",
      "epoch no.2 train no.193060  loss = 4.19244 avg_loss = 3.58071\n",
      "epoch no.2 train no.193070  loss = 3.14651 avg_loss = 3.57102\n",
      "epoch no.2 train no.193080  loss = 3.72985 avg_loss = 3.56675\n",
      "epoch no.2 train no.193090  loss = 3.29468 avg_loss = 3.52125\n",
      "epoch no.2 train no.193100  loss = 1.91176 avg_loss = 3.50260\n",
      "epoch no.2 train no.193110  loss = 5.66551 avg_loss = 3.56504\n",
      "epoch no.2 train no.193120  loss = 2.65249 avg_loss = 3.55576\n",
      "epoch no.2 train no.193130  loss = 3.98984 avg_loss = 3.60125\n",
      "epoch no.2 train no.193140  loss = 2.62621 avg_loss = 3.56664\n",
      "epoch no.2 train no.193150  loss = 5.64817 avg_loss = 3.57341\n",
      "epoch no.2 train no.193160  loss = 3.79339 avg_loss = 3.54914\n",
      "epoch no.2 train no.193170  loss = 4.58223 avg_loss = 3.56051\n",
      "epoch no.2 train no.193180  loss = 3.37057 avg_loss = 3.56412\n",
      "epoch no.2 train no.193190  loss = 3.36570 avg_loss = 3.58363\n",
      "epoch no.2 train no.193200  loss = 3.05569 avg_loss = 3.59641\n",
      "epoch no.2 train no.193210  loss = 2.77055 avg_loss = 3.59478\n",
      "epoch no.2 train no.193220  loss = 3.56499 avg_loss = 3.56670\n",
      "epoch no.2 train no.193230  loss = 3.29117 avg_loss = 3.57606\n",
      "epoch no.2 train no.193240  loss = 3.87354 avg_loss = 3.60645\n",
      "epoch no.2 train no.193250  loss = 5.34864 avg_loss = 3.62525\n",
      "epoch no.2 train no.193260  loss = 2.24301 avg_loss = 3.64533\n",
      "epoch no.2 train no.193270  loss = 2.88815 avg_loss = 3.62741\n",
      "epoch no.2 train no.193280  loss = 3.95373 avg_loss = 3.57886\n",
      "epoch no.2 train no.193290  loss = 2.92735 avg_loss = 3.58238\n",
      "epoch no.2 train no.193300  loss = 4.16362 avg_loss = 3.66358\n",
      "epoch no.2 train no.193310  loss = 2.22891 avg_loss = 3.69626\n",
      "epoch no.2 train no.193320  loss = 4.60377 avg_loss = 3.71772\n",
      "epoch no.2 train no.193330  loss = 5.17002 avg_loss = 3.73853\n",
      "epoch no.2 train no.193340  loss = 3.90648 avg_loss = 3.70735\n",
      "epoch no.2 train no.193350  loss = 5.70796 avg_loss = 3.68919\n",
      "epoch no.2 train no.193360  loss = 4.89499 avg_loss = 3.72334\n",
      "epoch no.2 train no.193370  loss = 3.75194 avg_loss = 3.72032\n",
      "epoch no.2 train no.193380  loss = 3.66435 avg_loss = 3.73523\n",
      "epoch no.2 train no.193390  loss = 2.99568 avg_loss = 3.76739\n",
      "epoch no.2 train no.193400  loss = 4.20452 avg_loss = 3.78483\n",
      "epoch no.2 train no.193410  loss = 7.04916 avg_loss = 3.76264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.193420  loss = 3.27503 avg_loss = 3.82055\n",
      "epoch no.2 train no.193430  loss = 2.81372 avg_loss = 3.81056\n",
      "epoch no.2 train no.193440  loss = 3.92923 avg_loss = 3.78299\n",
      "epoch no.2 train no.193450  loss = 3.33415 avg_loss = 3.74972\n",
      "epoch no.2 train no.193460  loss = 4.26764 avg_loss = 3.71999\n",
      "epoch no.2 train no.193470  loss = 2.84548 avg_loss = 3.71188\n",
      "epoch no.2 train no.193480  loss = 2.28355 avg_loss = 3.63098\n",
      "epoch no.2 train no.193490  loss = 3.94912 avg_loss = 3.69348\n",
      "epoch no.2 train no.193500  loss = 5.12214 avg_loss = 3.73824\n",
      "epoch no.2 train no.193510  loss = 2.38468 avg_loss = 3.71387\n",
      "epoch no.2 train no.193520  loss = 4.22957 avg_loss = 3.74179\n",
      "epoch no.2 train no.193530  loss = 2.18078 avg_loss = 3.71930\n",
      "epoch no.2 train no.193540  loss = 3.40885 avg_loss = 3.69222\n",
      "epoch no.2 train no.193550  loss = 4.77135 avg_loss = 3.74009\n",
      "epoch no.2 train no.193560  loss = 3.35052 avg_loss = 3.69738\n",
      "epoch no.2 train no.193570  loss = 5.58294 avg_loss = 3.69371\n",
      "epoch no.2 train no.193580  loss = 2.49455 avg_loss = 3.65245\n",
      "epoch no.2 train no.193590  loss = 3.27372 avg_loss = 3.66194\n",
      "epoch no.2 train no.193600  loss = 1.13405 avg_loss = 3.61214\n",
      "epoch no.2 train no.193610  loss = 3.97352 avg_loss = 3.62527\n",
      "epoch no.2 train no.193620  loss = 2.81049 avg_loss = 3.68155\n",
      "epoch no.2 train no.193630  loss = 3.52633 avg_loss = 3.66436\n",
      "epoch no.2 train no.193640  loss = 3.69141 avg_loss = 3.68856\n",
      "epoch no.2 train no.193650  loss = 6.33703 avg_loss = 3.72354\n",
      "epoch no.2 train no.193660  loss = 2.67908 avg_loss = 3.67495\n",
      "epoch no.2 train no.193670  loss = 2.66088 avg_loss = 3.66845\n",
      "epoch no.2 train no.193680  loss = 3.61426 avg_loss = 3.69024\n",
      "epoch no.2 train no.193690  loss = 6.71177 avg_loss = 3.68246\n",
      "epoch no.2 train no.193700  loss = 4.98898 avg_loss = 3.70327\n",
      "epoch no.2 train no.193710  loss = 2.79567 avg_loss = 3.65980\n",
      "epoch no.2 train no.193720  loss = 4.11293 avg_loss = 3.69012\n",
      "epoch no.2 train no.193730  loss = 3.11161 avg_loss = 3.74210\n",
      "epoch no.2 train no.193740  loss = 3.68399 avg_loss = 3.74231\n",
      "epoch no.2 train no.193750  loss = 2.87346 avg_loss = 3.70680\n",
      "epoch no.2 train no.193760  loss = 3.83715 avg_loss = 3.68248\n",
      "epoch no.2 train no.193770  loss = 3.35414 avg_loss = 3.61326\n",
      "epoch no.2 train no.193780  loss = 2.19007 avg_loss = 3.55012\n",
      "epoch no.2 train no.193790  loss = 4.37409 avg_loss = 3.54851\n",
      "epoch no.2 train no.193800  loss = 4.41244 avg_loss = 3.55177\n",
      "epoch no.2 train no.193810  loss = 3.55068 avg_loss = 3.52819\n",
      "epoch no.2 train no.193820  loss = 4.16469 avg_loss = 3.53554\n",
      "epoch no.2 train no.193830  loss = 3.36881 avg_loss = 3.51506\n",
      "epoch no.2 train no.193840  loss = 4.69608 avg_loss = 3.53044\n",
      "epoch no.2 train no.193850  loss = 2.76893 avg_loss = 3.52215\n",
      "epoch no.2 train no.193860  loss = 5.12901 avg_loss = 3.50855\n",
      "epoch no.2 train no.193870  loss = 3.04912 avg_loss = 3.48738\n",
      "epoch no.2 train no.193880  loss = 3.90626 avg_loss = 3.51292\n",
      "epoch no.2 train no.193890  loss = 3.79001 avg_loss = 3.51502\n",
      "epoch no.2 train no.193900  loss = 2.58012 avg_loss = 3.50146\n",
      "epoch no.2 train no.193910  loss = 2.66556 avg_loss = 3.45474\n",
      "epoch no.2 train no.193920  loss = 3.11598 avg_loss = 3.48507\n",
      "epoch no.2 train no.193930  loss = 3.33588 avg_loss = 3.45153\n",
      "epoch no.2 train no.193940  loss = 2.79017 avg_loss = 3.48358\n",
      "epoch no.2 train no.193950  loss = 4.28390 avg_loss = 3.47469\n",
      "epoch no.2 train no.193960  loss = 4.96388 avg_loss = 3.52433\n",
      "epoch no.2 train no.193970  loss = 2.80223 avg_loss = 3.50076\n",
      "epoch no.2 train no.193980  loss = 4.92267 avg_loss = 3.55423\n",
      "epoch no.2 train no.193990  loss = 2.67477 avg_loss = 3.53089\n",
      "epoch no.2 train no.194000  loss = 3.83198 avg_loss = 3.54640\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.194010  loss = 3.95547 avg_loss = 3.51454\n",
      "epoch no.2 train no.194020  loss = 2.76446 avg_loss = 3.49687\n",
      "epoch no.2 train no.194030  loss = 3.29123 avg_loss = 3.48363\n",
      "epoch no.2 train no.194040  loss = 5.46700 avg_loss = 3.53186\n",
      "epoch no.2 train no.194050  loss = 4.36263 avg_loss = 3.51942\n",
      "epoch no.2 train no.194060  loss = 3.35344 avg_loss = 3.47607\n",
      "epoch no.2 train no.194070  loss = 3.34677 avg_loss = 3.47890\n",
      "epoch no.2 train no.194080  loss = 2.56506 avg_loss = 3.46713\n",
      "epoch no.2 train no.194090  loss = 4.15445 avg_loss = 3.46542\n",
      "epoch no.2 train no.194100  loss = 1.37018 avg_loss = 3.40980\n",
      "epoch no.2 train no.194110  loss = 3.31242 avg_loss = 3.42869\n",
      "epoch no.2 train no.194120  loss = 6.20798 avg_loss = 3.48821\n",
      "epoch no.2 train no.194130  loss = 4.94483 avg_loss = 3.53872\n",
      "epoch no.2 train no.194140  loss = 4.80898 avg_loss = 3.58913\n",
      "epoch no.2 train no.194150  loss = 3.05736 avg_loss = 3.55534\n",
      "epoch no.2 train no.194160  loss = 3.97895 avg_loss = 3.56443\n",
      "epoch no.2 train no.194170  loss = 3.35799 avg_loss = 3.55056\n",
      "epoch no.2 train no.194180  loss = 4.05306 avg_loss = 3.55619\n",
      "epoch no.2 train no.194190  loss = 3.62979 avg_loss = 3.55637\n",
      "epoch no.2 train no.194200  loss = 2.57910 avg_loss = 3.63053\n",
      "epoch no.2 train no.194210  loss = 4.86246 avg_loss = 3.66243\n",
      "epoch no.2 train no.194220  loss = 4.22322 avg_loss = 3.63922\n",
      "epoch no.2 train no.194230  loss = 2.75983 avg_loss = 3.64801\n",
      "epoch no.2 train no.194240  loss = 3.33773 avg_loss = 3.62872\n",
      "epoch no.2 train no.194250  loss = 3.39870 avg_loss = 3.58488\n",
      "epoch no.2 train no.194260  loss = 2.93073 avg_loss = 3.58874\n",
      "epoch no.2 train no.194270  loss = 3.26267 avg_loss = 3.54853\n",
      "epoch no.2 train no.194280  loss = 2.92241 avg_loss = 3.51048\n",
      "epoch no.2 train no.194290  loss = 3.91829 avg_loss = 3.48404\n",
      "epoch no.2 train no.194300  loss = 3.97872 avg_loss = 3.45766\n",
      "epoch no.2 train no.194310  loss = 3.80326 avg_loss = 3.50529\n",
      "epoch no.2 train no.194320  loss = 5.91396 avg_loss = 3.49712\n",
      "epoch no.2 train no.194330  loss = 4.30700 avg_loss = 3.51281\n",
      "epoch no.2 train no.194340  loss = 2.14372 avg_loss = 3.54425\n",
      "epoch no.2 train no.194350  loss = 3.22647 avg_loss = 3.53128\n",
      "epoch no.2 train no.194360  loss = 5.69317 avg_loss = 3.54754\n",
      "epoch no.2 train no.194370  loss = 3.17544 avg_loss = 3.56355\n",
      "epoch no.2 train no.194380  loss = 5.70636 avg_loss = 3.60421\n",
      "epoch no.2 train no.194390  loss = 3.03222 avg_loss = 3.60152\n",
      "epoch no.2 train no.194400  loss = 2.37167 avg_loss = 3.59704\n",
      "epoch no.2 train no.194410  loss = 4.33507 avg_loss = 3.59020\n",
      "epoch no.2 train no.194420  loss = 3.49559 avg_loss = 3.58439\n",
      "epoch no.2 train no.194430  loss = 2.95591 avg_loss = 3.57149\n",
      "epoch no.2 train no.194440  loss = 2.57881 avg_loss = 3.57857\n",
      "epoch no.2 train no.194450  loss = 3.27328 avg_loss = 3.60007\n",
      "epoch no.2 train no.194460  loss = 2.60929 avg_loss = 3.58318\n",
      "epoch no.2 train no.194470  loss = 4.19135 avg_loss = 3.56503\n",
      "epoch no.2 train no.194480  loss = 1.71761 avg_loss = 3.55061\n",
      "epoch no.2 train no.194490  loss = 4.19765 avg_loss = 3.53481\n",
      "epoch no.2 train no.194500  loss = 2.34755 avg_loss = 3.49831\n",
      "epoch no.2 train no.194510  loss = 2.44378 avg_loss = 3.48977\n",
      "epoch no.2 train no.194520  loss = 5.11111 avg_loss = 3.54511\n",
      "epoch no.2 train no.194530  loss = 3.95021 avg_loss = 3.55520\n",
      "epoch no.2 train no.194540  loss = 2.19564 avg_loss = 3.57015\n",
      "epoch no.2 train no.194550  loss = 2.33196 avg_loss = 3.58983\n",
      "epoch no.2 train no.194560  loss = 4.70765 avg_loss = 3.61094\n",
      "epoch no.2 train no.194570  loss = 2.38796 avg_loss = 3.59193\n",
      "epoch no.2 train no.194580  loss = 2.67427 avg_loss = 3.52585\n",
      "epoch no.2 train no.194590  loss = 2.95316 avg_loss = 3.52299\n",
      "epoch no.2 train no.194600  loss = 4.62921 avg_loss = 3.55082\n",
      "epoch no.2 train no.194610  loss = 2.81233 avg_loss = 3.53913\n",
      "epoch no.2 train no.194620  loss = 3.60401 avg_loss = 3.58472\n",
      "epoch no.2 train no.194630  loss = 3.57012 avg_loss = 3.58401\n",
      "epoch no.2 train no.194640  loss = 4.07482 avg_loss = 3.56016\n",
      "epoch no.2 train no.194650  loss = 3.20771 avg_loss = 3.53391\n",
      "epoch no.2 train no.194660  loss = 2.22029 avg_loss = 3.49788\n",
      "epoch no.2 train no.194670  loss = 4.99655 avg_loss = 3.56082\n",
      "epoch no.2 train no.194680  loss = 3.96641 avg_loss = 3.54449\n",
      "epoch no.2 train no.194690  loss = 3.63359 avg_loss = 3.57453\n",
      "epoch no.2 train no.194700  loss = 3.26911 avg_loss = 3.63480\n",
      "epoch no.2 train no.194710  loss = 3.95785 avg_loss = 3.63301\n",
      "epoch no.2 train no.194720  loss = 4.74146 avg_loss = 3.62266\n",
      "epoch no.2 train no.194730  loss = 3.65542 avg_loss = 3.57722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.194740  loss = 3.45007 avg_loss = 3.56004\n",
      "epoch no.2 train no.194750  loss = 4.32511 avg_loss = 3.54747\n",
      "epoch no.2 train no.194760  loss = 2.86185 avg_loss = 3.56757\n",
      "epoch no.2 train no.194770  loss = 1.92712 avg_loss = 3.54799\n",
      "epoch no.2 train no.194780  loss = 2.64032 avg_loss = 3.55701\n",
      "epoch no.2 train no.194790  loss = 4.05433 avg_loss = 3.53764\n",
      "epoch no.2 train no.194800  loss = 3.13599 avg_loss = 3.57716\n",
      "epoch no.2 train no.194810  loss = 2.25344 avg_loss = 3.57233\n",
      "epoch no.2 train no.194820  loss = 4.55796 avg_loss = 3.63612\n",
      "epoch no.2 train no.194830  loss = 3.70799 avg_loss = 3.67182\n",
      "epoch no.2 train no.194840  loss = 3.75940 avg_loss = 3.66292\n",
      "epoch no.2 train no.194850  loss = 4.09788 avg_loss = 3.63266\n",
      "epoch no.2 train no.194860  loss = 4.13217 avg_loss = 3.60709\n",
      "epoch no.2 train no.194870  loss = 2.70704 avg_loss = 3.56107\n",
      "epoch no.2 train no.194880  loss = 3.10302 avg_loss = 3.58949\n",
      "epoch no.2 train no.194890  loss = 2.81955 avg_loss = 3.56405\n",
      "epoch no.2 train no.194900  loss = 1.42324 avg_loss = 3.51850\n",
      "epoch no.2 train no.194910  loss = 1.95114 avg_loss = 3.52610\n",
      "epoch no.2 train no.194920  loss = 2.11177 avg_loss = 3.53851\n",
      "epoch no.2 train no.194930  loss = 2.43837 avg_loss = 3.51380\n",
      "epoch no.2 train no.194940  loss = 4.11026 avg_loss = 3.51419\n",
      "epoch no.2 train no.194950  loss = 4.77955 avg_loss = 3.47982\n",
      "epoch no.2 train no.194960  loss = 2.06696 avg_loss = 3.45915\n",
      "epoch no.2 train no.194970  loss = 4.72167 avg_loss = 3.47754\n",
      "epoch no.2 train no.194980  loss = 2.11462 avg_loss = 3.45832\n",
      "epoch no.2 train no.194990  loss = 2.54322 avg_loss = 3.45757\n",
      "epoch no.2 train no.195000  loss = 4.17420 avg_loss = 3.45269\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 노래</s>\n",
      "epoch no.2 train no.195010  loss = 2.58495 avg_loss = 3.44589\n",
      "epoch no.2 train no.195020  loss = 3.80462 avg_loss = 3.49355\n",
      "epoch no.2 train no.195030  loss = 4.85036 avg_loss = 3.49759\n",
      "epoch no.2 train no.195040  loss = 3.29758 avg_loss = 3.46357\n",
      "epoch no.2 train no.195050  loss = 3.75342 avg_loss = 3.49455\n",
      "epoch no.2 train no.195060  loss = 3.82149 avg_loss = 3.48828\n",
      "epoch no.2 train no.195070  loss = 2.71226 avg_loss = 3.53636\n",
      "epoch no.2 train no.195080  loss = 4.28544 avg_loss = 3.54090\n",
      "epoch no.2 train no.195090  loss = 4.54631 avg_loss = 3.55314\n",
      "epoch no.2 train no.195100  loss = 3.45147 avg_loss = 3.56613\n",
      "epoch no.2 train no.195110  loss = 3.08934 avg_loss = 3.52415\n",
      "epoch no.2 train no.195120  loss = 2.65228 avg_loss = 3.51515\n",
      "epoch no.2 train no.195130  loss = 3.66405 avg_loss = 3.54903\n",
      "epoch no.2 train no.195140  loss = 4.00619 avg_loss = 3.61438\n",
      "epoch no.2 train no.195150  loss = 3.34392 avg_loss = 3.60623\n",
      "epoch no.2 train no.195160  loss = 4.52260 avg_loss = 3.62267\n",
      "epoch no.2 train no.195170  loss = 2.02926 avg_loss = 3.62377\n",
      "epoch no.2 train no.195180  loss = 3.81040 avg_loss = 3.60934\n",
      "epoch no.2 train no.195190  loss = 3.15325 avg_loss = 3.59564\n",
      "epoch no.2 train no.195200  loss = 1.85764 avg_loss = 3.57748\n",
      "epoch no.2 train no.195210  loss = 4.31417 avg_loss = 3.59319\n",
      "epoch no.2 train no.195220  loss = 3.67364 avg_loss = 3.62801\n",
      "epoch no.2 train no.195230  loss = 2.58911 avg_loss = 3.66604\n",
      "epoch no.2 train no.195240  loss = 2.36983 avg_loss = 3.66380\n",
      "epoch no.2 train no.195250  loss = 2.82991 avg_loss = 3.64974\n",
      "epoch no.2 train no.195260  loss = 3.09925 avg_loss = 3.64275\n",
      "epoch no.2 train no.195270  loss = 3.64424 avg_loss = 3.64095\n",
      "epoch no.2 train no.195280  loss = 5.10309 avg_loss = 3.64907\n",
      "epoch no.2 train no.195290  loss = 4.89982 avg_loss = 3.67524\n",
      "epoch no.2 train no.195300  loss = 3.46561 avg_loss = 3.67419\n",
      "epoch no.2 train no.195310  loss = 4.18309 avg_loss = 3.66362\n",
      "epoch no.2 train no.195320  loss = 3.41791 avg_loss = 3.67268\n",
      "epoch no.2 train no.195330  loss = 2.82979 avg_loss = 3.62765\n",
      "epoch no.2 train no.195340  loss = 3.62545 avg_loss = 3.65234\n",
      "epoch no.2 train no.195350  loss = 4.99187 avg_loss = 3.64558\n",
      "epoch no.2 train no.195360  loss = 4.15823 avg_loss = 3.70659\n",
      "epoch no.2 train no.195370  loss = 3.45718 avg_loss = 3.71462\n",
      "epoch no.2 train no.195380  loss = 4.83791 avg_loss = 3.71389\n",
      "epoch no.2 train no.195390  loss = 4.53873 avg_loss = 3.70742\n",
      "epoch no.2 train no.195400  loss = 3.58658 avg_loss = 3.64445\n",
      "epoch no.2 train no.195410  loss = 2.77136 avg_loss = 3.64699\n",
      "epoch no.2 train no.195420  loss = 1.41198 avg_loss = 3.64761\n",
      "epoch no.2 train no.195430  loss = 3.67868 avg_loss = 3.59869\n",
      "epoch no.2 train no.195440  loss = 2.78481 avg_loss = 3.55590\n",
      "epoch no.2 train no.195450  loss = 1.51091 avg_loss = 3.56118\n",
      "epoch no.2 train no.195460  loss = 2.46759 avg_loss = 3.52131\n",
      "epoch no.2 train no.195470  loss = 5.41729 avg_loss = 3.51773\n",
      "epoch no.2 train no.195480  loss = 2.15034 avg_loss = 3.51330\n",
      "epoch no.2 train no.195490  loss = 4.43371 avg_loss = 3.53945\n",
      "epoch no.2 train no.195500  loss = 3.62267 avg_loss = 3.51768\n",
      "epoch no.2 train no.195510  loss = 3.70484 avg_loss = 3.49553\n",
      "epoch no.2 train no.195520  loss = 5.29559 avg_loss = 3.52375\n",
      "epoch no.2 train no.195530  loss = 5.92493 avg_loss = 3.50585\n",
      "epoch no.2 train no.195540  loss = 4.19014 avg_loss = 3.58115\n",
      "epoch no.2 train no.195550  loss = 3.82074 avg_loss = 3.60090\n",
      "epoch no.2 train no.195560  loss = 3.48174 avg_loss = 3.59420\n",
      "epoch no.2 train no.195570  loss = 4.49733 avg_loss = 3.63658\n",
      "epoch no.2 train no.195580  loss = 5.88403 avg_loss = 3.64618\n",
      "epoch no.2 train no.195590  loss = 6.23696 avg_loss = 3.67184\n",
      "epoch no.2 train no.195600  loss = 3.57388 avg_loss = 3.70372\n",
      "epoch no.2 train no.195610  loss = 3.07159 avg_loss = 3.63852\n",
      "epoch no.2 train no.195620  loss = 4.04522 avg_loss = 3.59541\n",
      "epoch no.2 train no.195630  loss = 3.74424 avg_loss = 3.61545\n",
      "epoch no.2 train no.195640  loss = 4.08439 avg_loss = 3.58059\n",
      "epoch no.2 train no.195650  loss = 3.42343 avg_loss = 3.52547\n",
      "epoch no.2 train no.195660  loss = 4.90438 avg_loss = 3.49964\n",
      "epoch no.2 train no.195670  loss = 2.39001 avg_loss = 3.46296\n",
      "epoch no.2 train no.195680  loss = 4.15675 avg_loss = 3.47090\n",
      "epoch no.2 train no.195690  loss = 2.79636 avg_loss = 3.52104\n",
      "epoch no.2 train no.195700  loss = 3.50526 avg_loss = 3.51628\n",
      "epoch no.2 train no.195710  loss = 4.47617 avg_loss = 3.52689\n",
      "epoch no.2 train no.195720  loss = 3.04238 avg_loss = 3.49502\n",
      "epoch no.2 train no.195730  loss = 3.95989 avg_loss = 3.49279\n",
      "epoch no.2 train no.195740  loss = 3.98655 avg_loss = 3.48343\n",
      "epoch no.2 train no.195750  loss = 5.61469 avg_loss = 3.56606\n",
      "epoch no.2 train no.195760  loss = 1.83317 avg_loss = 3.54891\n",
      "epoch no.2 train no.195770  loss = 2.68041 avg_loss = 3.61335\n",
      "epoch no.2 train no.195780  loss = 2.87314 avg_loss = 3.63766\n",
      "epoch no.2 train no.195790  loss = 3.94507 avg_loss = 3.66947\n",
      "epoch no.2 train no.195800  loss = 4.53988 avg_loss = 3.66032\n",
      "epoch no.2 train no.195810  loss = 3.96909 avg_loss = 3.62267\n",
      "epoch no.2 train no.195820  loss = 3.96943 avg_loss = 3.61795\n",
      "epoch no.2 train no.195830  loss = 3.14302 avg_loss = 3.65046\n",
      "epoch no.2 train no.195840  loss = 4.32612 avg_loss = 3.61185\n",
      "epoch no.2 train no.195850  loss = 2.39536 avg_loss = 3.60579\n",
      "epoch no.2 train no.195860  loss = 3.68273 avg_loss = 3.65079\n",
      "epoch no.2 train no.195870  loss = 2.49746 avg_loss = 3.60107\n",
      "epoch no.2 train no.195880  loss = 3.08832 avg_loss = 3.57617\n",
      "epoch no.2 train no.195890  loss = 3.87387 avg_loss = 3.57161\n",
      "epoch no.2 train no.195900  loss = 2.30867 avg_loss = 3.55049\n",
      "epoch no.2 train no.195910  loss = 4.78296 avg_loss = 3.56175\n",
      "epoch no.2 train no.195920  loss = 4.25854 avg_loss = 3.55204\n",
      "epoch no.2 train no.195930  loss = 5.38491 avg_loss = 3.56722\n",
      "epoch no.2 train no.195940  loss = 3.48391 avg_loss = 3.55529\n",
      "epoch no.2 train no.195950  loss = 2.22901 avg_loss = 3.50099\n",
      "epoch no.2 train no.195960  loss = 3.69332 avg_loss = 3.53411\n",
      "epoch no.2 train no.195970  loss = 4.32024 avg_loss = 3.57266\n",
      "epoch no.2 train no.195980  loss = 8.05390 avg_loss = 3.58383\n",
      "epoch no.2 train no.195990  loss = 3.92402 avg_loss = 3.54053\n",
      "epoch no.2 train no.196000  loss = 2.07126 avg_loss = 3.51830\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '곡', '</s>']\n",
      "기분전환을 위한 댄스곡</s>\n",
      "epoch no.2 train no.196010  loss = 2.22366 avg_loss = 3.44979\n",
      "epoch no.2 train no.196020  loss = 3.67124 avg_loss = 3.53676\n",
      "epoch no.2 train no.196030  loss = 1.87047 avg_loss = 3.50250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.196040  loss = 3.89721 avg_loss = 3.53733\n",
      "epoch no.2 train no.196050  loss = 2.80184 avg_loss = 3.55131\n",
      "epoch no.2 train no.196060  loss = 3.93953 avg_loss = 3.53618\n",
      "epoch no.2 train no.196070  loss = 3.38978 avg_loss = 3.54298\n",
      "epoch no.2 train no.196080  loss = 2.89479 avg_loss = 3.55736\n",
      "epoch no.2 train no.196090  loss = 2.24278 avg_loss = 3.56048\n",
      "epoch no.2 train no.196100  loss = 4.07742 avg_loss = 3.59476\n",
      "epoch no.2 train no.196110  loss = 3.68164 avg_loss = 3.59343\n",
      "epoch no.2 train no.196120  loss = 2.57582 avg_loss = 3.64850\n",
      "epoch no.2 train no.196130  loss = 2.72206 avg_loss = 3.60006\n",
      "epoch no.2 train no.196140  loss = 2.53958 avg_loss = 3.61107\n",
      "epoch no.2 train no.196150  loss = 3.88144 avg_loss = 3.61199\n",
      "epoch no.2 train no.196160  loss = 3.73536 avg_loss = 3.58574\n",
      "epoch no.2 train no.196170  loss = 4.60511 avg_loss = 3.56126\n",
      "epoch no.2 train no.196180  loss = 3.75404 avg_loss = 3.53430\n",
      "epoch no.2 train no.196190  loss = 3.06866 avg_loss = 3.51369\n",
      "epoch no.2 train no.196200  loss = 3.39312 avg_loss = 3.52020\n",
      "epoch no.2 train no.196210  loss = 4.44704 avg_loss = 3.54096\n",
      "epoch no.2 train no.196220  loss = 3.64220 avg_loss = 3.53618\n",
      "epoch no.2 train no.196230  loss = 5.02716 avg_loss = 3.51141\n",
      "epoch no.2 train no.196240  loss = 3.81352 avg_loss = 3.49131\n",
      "epoch no.2 train no.196250  loss = 1.78648 avg_loss = 3.43317\n",
      "epoch no.2 train no.196260  loss = 3.33214 avg_loss = 3.45868\n",
      "epoch no.2 train no.196270  loss = 4.18121 avg_loss = 3.42909\n",
      "epoch no.2 train no.196280  loss = 3.39731 avg_loss = 3.42576\n",
      "epoch no.2 train no.196290  loss = 4.62108 avg_loss = 3.47138\n",
      "epoch no.2 train no.196300  loss = 5.74658 avg_loss = 3.45466\n",
      "epoch no.2 train no.196310  loss = 2.81272 avg_loss = 3.41881\n",
      "epoch no.2 train no.196320  loss = 2.24598 avg_loss = 3.41855\n",
      "epoch no.2 train no.196330  loss = 3.24146 avg_loss = 3.41540\n",
      "epoch no.2 train no.196340  loss = 1.52792 avg_loss = 3.41734\n",
      "epoch no.2 train no.196350  loss = 2.11508 avg_loss = 3.46872\n",
      "epoch no.2 train no.196360  loss = 3.35758 avg_loss = 3.53862\n",
      "epoch no.2 train no.196370  loss = 3.93501 avg_loss = 3.61130\n",
      "epoch no.2 train no.196380  loss = 3.20944 avg_loss = 3.68329\n",
      "epoch no.2 train no.196390  loss = 4.55033 avg_loss = 3.66184\n",
      "epoch no.2 train no.196400  loss = 3.49256 avg_loss = 3.63565\n",
      "epoch no.2 train no.196410  loss = 5.09757 avg_loss = 3.68393\n",
      "epoch no.2 train no.196420  loss = 2.82269 avg_loss = 3.74551\n",
      "epoch no.2 train no.196430  loss = 5.67640 avg_loss = 3.71905\n",
      "epoch no.2 train no.196440  loss = 2.99946 avg_loss = 3.75135\n",
      "epoch no.2 train no.196450  loss = 3.60102 avg_loss = 3.74182\n",
      "epoch no.2 train no.196460  loss = 4.66111 avg_loss = 3.75563\n",
      "epoch no.2 train no.196470  loss = 3.51606 avg_loss = 3.75497\n",
      "epoch no.2 train no.196480  loss = 2.34610 avg_loss = 3.68029\n",
      "epoch no.2 train no.196490  loss = 3.45849 avg_loss = 3.68485\n",
      "epoch no.2 train no.196500  loss = 5.52611 avg_loss = 3.66603\n",
      "epoch no.2 train no.196510  loss = 3.53815 avg_loss = 3.69228\n",
      "epoch no.2 train no.196520  loss = 5.09407 avg_loss = 3.68269\n",
      "epoch no.2 train no.196530  loss = 3.83436 avg_loss = 3.64080\n",
      "epoch no.2 train no.196540  loss = 3.21891 avg_loss = 3.62659\n",
      "epoch no.2 train no.196550  loss = 4.89322 avg_loss = 3.65497\n",
      "epoch no.2 train no.196560  loss = 3.73758 avg_loss = 3.60771\n",
      "epoch no.2 train no.196570  loss = 5.89800 avg_loss = 3.63048\n",
      "epoch no.2 train no.196580  loss = 3.73348 avg_loss = 3.56914\n",
      "epoch no.2 train no.196590  loss = 3.79499 avg_loss = 3.57378\n",
      "epoch no.2 train no.196600  loss = 3.35890 avg_loss = 3.56803\n",
      "epoch no.2 train no.196610  loss = 4.31464 avg_loss = 3.63257\n",
      "epoch no.2 train no.196620  loss = 4.40244 avg_loss = 3.69135\n",
      "epoch no.2 train no.196630  loss = 3.99224 avg_loss = 3.71401\n",
      "epoch no.2 train no.196640  loss = 3.50005 avg_loss = 3.70718\n",
      "epoch no.2 train no.196650  loss = 4.81248 avg_loss = 3.70758\n",
      "epoch no.2 train no.196660  loss = 2.02294 avg_loss = 3.68162\n",
      "epoch no.2 train no.196670  loss = 2.32736 avg_loss = 3.67782\n",
      "epoch no.2 train no.196680  loss = 2.20100 avg_loss = 3.67949\n",
      "epoch no.2 train no.196690  loss = 3.22715 avg_loss = 3.64704\n",
      "epoch no.2 train no.196700  loss = 3.91134 avg_loss = 3.63151\n",
      "epoch no.2 train no.196710  loss = 2.41144 avg_loss = 3.65491\n",
      "epoch no.2 train no.196720  loss = 4.35939 avg_loss = 3.63511\n",
      "epoch no.2 train no.196730  loss = 3.07724 avg_loss = 3.63717\n",
      "epoch no.2 train no.196740  loss = 3.17211 avg_loss = 3.63957\n",
      "epoch no.2 train no.196750  loss = 2.90131 avg_loss = 3.57720\n",
      "epoch no.2 train no.196760  loss = 5.42974 avg_loss = 3.56809\n",
      "epoch no.2 train no.196770  loss = 5.32668 avg_loss = 3.55724\n",
      "epoch no.2 train no.196780  loss = 3.30697 avg_loss = 3.62635\n",
      "epoch no.2 train no.196790  loss = 4.14307 avg_loss = 3.64150\n",
      "epoch no.2 train no.196800  loss = 2.87362 avg_loss = 3.61754\n",
      "epoch no.2 train no.196810  loss = 4.36614 avg_loss = 3.58298\n",
      "epoch no.2 train no.196820  loss = 1.76409 avg_loss = 3.59859\n",
      "epoch no.2 train no.196830  loss = 2.97972 avg_loss = 3.53607\n",
      "epoch no.2 train no.196840  loss = 4.12142 avg_loss = 3.58762\n",
      "epoch no.2 train no.196850  loss = 2.88309 avg_loss = 3.57363\n",
      "epoch no.2 train no.196860  loss = 3.21573 avg_loss = 3.60116\n",
      "epoch no.2 train no.196870  loss = 2.60836 avg_loss = 3.59252\n",
      "epoch no.2 train no.196880  loss = 2.56488 avg_loss = 3.56746\n",
      "epoch no.2 train no.196890  loss = 2.40685 avg_loss = 3.57993\n",
      "epoch no.2 train no.196900  loss = 3.57026 avg_loss = 3.61487\n",
      "epoch no.2 train no.196910  loss = 2.45724 avg_loss = 3.58651\n",
      "epoch no.2 train no.196920  loss = 2.00017 avg_loss = 3.54844\n",
      "epoch no.2 train no.196930  loss = 3.39783 avg_loss = 3.53474\n",
      "epoch no.2 train no.196940  loss = 2.93846 avg_loss = 3.49023\n",
      "epoch no.2 train no.196950  loss = 2.56440 avg_loss = 3.45641\n",
      "epoch no.2 train no.196960  loss = 4.17453 avg_loss = 3.49317\n",
      "epoch no.2 train no.196970  loss = 1.99553 avg_loss = 3.43852\n",
      "epoch no.2 train no.196980  loss = 2.86783 avg_loss = 3.46168\n",
      "epoch no.2 train no.196990  loss = 3.43019 avg_loss = 3.45699\n",
      "epoch no.2 train no.197000  loss = 1.86649 avg_loss = 3.43162\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 노래</s>\n",
      "epoch no.2 train no.197010  loss = 4.49247 avg_loss = 3.45826\n",
      "epoch no.2 train no.197020  loss = 5.09422 avg_loss = 3.50436\n",
      "epoch no.2 train no.197030  loss = 3.89224 avg_loss = 3.51709\n",
      "epoch no.2 train no.197040  loss = 3.62208 avg_loss = 3.53817\n",
      "epoch no.2 train no.197050  loss = 1.90789 avg_loss = 3.56196\n",
      "epoch no.2 train no.197060  loss = 3.97225 avg_loss = 3.54133\n",
      "epoch no.2 train no.197070  loss = 4.08374 avg_loss = 3.55141\n",
      "epoch no.2 train no.197080  loss = 3.99134 avg_loss = 3.53267\n",
      "epoch no.2 train no.197090  loss = 3.97905 avg_loss = 3.53588\n",
      "epoch no.2 train no.197100  loss = 2.62326 avg_loss = 3.51837\n",
      "epoch no.2 train no.197110  loss = 2.48046 avg_loss = 3.57433\n",
      "epoch no.2 train no.197120  loss = 4.29234 avg_loss = 3.61154\n",
      "epoch no.2 train no.197130  loss = 4.05098 avg_loss = 3.60061\n",
      "epoch no.2 train no.197140  loss = 2.54425 avg_loss = 3.59027\n",
      "epoch no.2 train no.197150  loss = 4.31266 avg_loss = 3.62570\n",
      "epoch no.2 train no.197160  loss = 3.30453 avg_loss = 3.58006\n",
      "epoch no.2 train no.197170  loss = 3.27684 avg_loss = 3.61836\n",
      "epoch no.2 train no.197180  loss = 2.76421 avg_loss = 3.57260\n",
      "epoch no.2 train no.197190  loss = 2.26399 avg_loss = 3.53546\n",
      "epoch no.2 train no.197200  loss = 3.01408 avg_loss = 3.51029\n",
      "epoch no.2 train no.197210  loss = 3.08322 avg_loss = 3.54437\n",
      "epoch no.2 train no.197220  loss = 4.29282 avg_loss = 3.55015\n",
      "epoch no.2 train no.197230  loss = 3.79415 avg_loss = 3.60595\n",
      "epoch no.2 train no.197240  loss = 3.52937 avg_loss = 3.59255\n",
      "epoch no.2 train no.197250  loss = 2.29039 avg_loss = 3.61612\n",
      "epoch no.2 train no.197260  loss = 6.20306 avg_loss = 3.67543\n",
      "epoch no.2 train no.197270  loss = 3.10404 avg_loss = 3.64568\n",
      "epoch no.2 train no.197280  loss = 2.31122 avg_loss = 3.58308\n",
      "epoch no.2 train no.197290  loss = 3.10248 avg_loss = 3.63719\n",
      "epoch no.2 train no.197300  loss = 3.22006 avg_loss = 3.63213\n",
      "epoch no.2 train no.197310  loss = 2.70437 avg_loss = 3.65176\n",
      "epoch no.2 train no.197320  loss = 3.20253 avg_loss = 3.68541\n",
      "epoch no.2 train no.197330  loss = 5.68233 avg_loss = 3.68366\n",
      "epoch no.2 train no.197340  loss = 3.27283 avg_loss = 3.70080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.197350  loss = 4.24662 avg_loss = 3.69470\n",
      "epoch no.2 train no.197360  loss = 3.73779 avg_loss = 3.64758\n",
      "epoch no.2 train no.197370  loss = 6.59067 avg_loss = 3.69636\n",
      "epoch no.2 train no.197380  loss = 3.39641 avg_loss = 3.64826\n",
      "epoch no.2 train no.197390  loss = 3.39371 avg_loss = 3.57551\n",
      "epoch no.2 train no.197400  loss = 4.67131 avg_loss = 3.63382\n",
      "epoch no.2 train no.197410  loss = 2.02382 avg_loss = 3.59699\n",
      "epoch no.2 train no.197420  loss = 3.10275 avg_loss = 3.57975\n",
      "epoch no.2 train no.197430  loss = 3.55548 avg_loss = 3.55560\n",
      "epoch no.2 train no.197440  loss = 3.88192 avg_loss = 3.56892\n",
      "epoch no.2 train no.197450  loss = 3.58801 avg_loss = 3.63074\n",
      "epoch no.2 train no.197460  loss = 2.93736 avg_loss = 3.67948\n",
      "epoch no.2 train no.197470  loss = 3.72400 avg_loss = 3.65859\n",
      "epoch no.2 train no.197480  loss = 3.88496 avg_loss = 3.69164\n",
      "epoch no.2 train no.197490  loss = 2.92450 avg_loss = 3.65120\n",
      "epoch no.2 train no.197500  loss = 3.40701 avg_loss = 3.68561\n",
      "epoch no.2 train no.197510  loss = 3.26212 avg_loss = 3.69296\n",
      "epoch no.2 train no.197520  loss = 2.62716 avg_loss = 3.65365\n",
      "epoch no.2 train no.197530  loss = 4.86143 avg_loss = 3.62205\n",
      "epoch no.2 train no.197540  loss = 3.80915 avg_loss = 3.65172\n",
      "epoch no.2 train no.197550  loss = 4.04584 avg_loss = 3.68200\n",
      "epoch no.2 train no.197560  loss = 3.01348 avg_loss = 3.66726\n",
      "epoch no.2 train no.197570  loss = 3.69269 avg_loss = 3.59946\n",
      "epoch no.2 train no.197580  loss = 4.80663 avg_loss = 3.59761\n",
      "epoch no.2 train no.197590  loss = 4.74592 avg_loss = 3.64406\n",
      "epoch no.2 train no.197600  loss = 2.94251 avg_loss = 3.65201\n",
      "epoch no.2 train no.197610  loss = 2.83555 avg_loss = 3.63940\n",
      "epoch no.2 train no.197620  loss = 3.59236 avg_loss = 3.63823\n",
      "epoch no.2 train no.197630  loss = 4.29401 avg_loss = 3.58330\n",
      "epoch no.2 train no.197640  loss = 4.59832 avg_loss = 3.59413\n",
      "epoch no.2 train no.197650  loss = 5.39822 avg_loss = 3.56354\n",
      "epoch no.2 train no.197660  loss = 3.18943 avg_loss = 3.53957\n",
      "epoch no.2 train no.197670  loss = 3.12544 avg_loss = 3.53024\n",
      "epoch no.2 train no.197680  loss = 3.14786 avg_loss = 3.50638\n",
      "epoch no.2 train no.197690  loss = 2.52643 avg_loss = 3.49268\n",
      "epoch no.2 train no.197700  loss = 3.26930 avg_loss = 3.50295\n",
      "epoch no.2 train no.197710  loss = 4.01984 avg_loss = 3.53266\n",
      "epoch no.2 train no.197720  loss = 5.13978 avg_loss = 3.53608\n",
      "epoch no.2 train no.197730  loss = 3.41505 avg_loss = 3.54851\n",
      "epoch no.2 train no.197740  loss = 3.18839 avg_loss = 3.52932\n",
      "epoch no.2 train no.197750  loss = 3.95152 avg_loss = 3.53865\n",
      "epoch no.2 train no.197760  loss = 2.31425 avg_loss = 3.52276\n",
      "epoch no.2 train no.197770  loss = 3.27242 avg_loss = 3.48259\n",
      "epoch no.2 train no.197780  loss = 4.18078 avg_loss = 3.51982\n",
      "epoch no.2 train no.197790  loss = 3.49466 avg_loss = 3.52876\n",
      "epoch no.2 train no.197800  loss = 2.46124 avg_loss = 3.52270\n",
      "epoch no.2 train no.197810  loss = 2.68655 avg_loss = 3.50839\n",
      "epoch no.2 train no.197820  loss = 4.88385 avg_loss = 3.53977\n",
      "epoch no.2 train no.197830  loss = 4.36623 avg_loss = 3.54614\n",
      "epoch no.2 train no.197840  loss = 2.51700 avg_loss = 3.53047\n",
      "epoch no.2 train no.197850  loss = 4.88076 avg_loss = 3.58291\n",
      "epoch no.2 train no.197860  loss = 4.32574 avg_loss = 3.58769\n",
      "epoch no.2 train no.197870  loss = 3.49645 avg_loss = 3.59623\n",
      "epoch no.2 train no.197880  loss = 3.51381 avg_loss = 3.59474\n",
      "epoch no.2 train no.197890  loss = 3.94802 avg_loss = 3.52671\n",
      "epoch no.2 train no.197900  loss = 3.65706 avg_loss = 3.50338\n",
      "epoch no.2 train no.197910  loss = 5.75597 avg_loss = 3.56005\n",
      "epoch no.2 train no.197920  loss = 3.25202 avg_loss = 3.55227\n",
      "epoch no.2 train no.197930  loss = 5.78343 avg_loss = 3.57433\n",
      "epoch no.2 train no.197940  loss = 3.63209 avg_loss = 3.57486\n",
      "epoch no.2 train no.197950  loss = 2.82448 avg_loss = 3.54702\n",
      "epoch no.2 train no.197960  loss = 2.06124 avg_loss = 3.54196\n",
      "epoch no.2 train no.197970  loss = 2.80133 avg_loss = 3.51715\n",
      "epoch no.2 train no.197980  loss = 3.96889 avg_loss = 3.51622\n",
      "epoch no.2 train no.197990  loss = 5.13085 avg_loss = 3.57080\n",
      "epoch no.2 train no.198000  loss = 1.86077 avg_loss = 3.54408\n",
      "3\n",
      "to_tokens: ['▁가을', '▁좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.198010  loss = 3.31261 avg_loss = 3.52705\n",
      "epoch no.2 train no.198020  loss = 4.13949 avg_loss = 3.50113\n",
      "epoch no.2 train no.198030  loss = 2.81065 avg_loss = 3.51686\n",
      "epoch no.2 train no.198040  loss = 5.29923 avg_loss = 3.53109\n",
      "epoch no.2 train no.198050  loss = 3.55646 avg_loss = 3.52409\n",
      "epoch no.2 train no.198060  loss = 4.06081 avg_loss = 3.55623\n",
      "epoch no.2 train no.198070  loss = 5.23593 avg_loss = 3.55730\n",
      "epoch no.2 train no.198080  loss = 3.18934 avg_loss = 3.59204\n",
      "epoch no.2 train no.198090  loss = 3.28092 avg_loss = 3.60524\n",
      "epoch no.2 train no.198100  loss = 1.81759 avg_loss = 3.57381\n",
      "epoch no.2 train no.198110  loss = 2.43279 avg_loss = 3.49261\n",
      "epoch no.2 train no.198120  loss = 1.98559 avg_loss = 3.52809\n",
      "epoch no.2 train no.198130  loss = 3.18264 avg_loss = 3.57019\n",
      "epoch no.2 train no.198140  loss = 2.99265 avg_loss = 3.63337\n",
      "epoch no.2 train no.198150  loss = 1.15974 avg_loss = 3.57464\n",
      "epoch no.2 train no.198160  loss = 2.78117 avg_loss = 3.52037\n",
      "epoch no.2 train no.198170  loss = 3.65148 avg_loss = 3.53626\n",
      "epoch no.2 train no.198180  loss = 2.27116 avg_loss = 3.49606\n",
      "epoch no.2 train no.198190  loss = 4.10468 avg_loss = 3.48326\n",
      "epoch no.2 train no.198200  loss = 3.62881 avg_loss = 3.53683\n",
      "epoch no.2 train no.198210  loss = 4.85544 avg_loss = 3.55409\n",
      "epoch no.2 train no.198220  loss = 2.18295 avg_loss = 3.52989\n",
      "epoch no.2 train no.198230  loss = 2.27435 avg_loss = 3.52073\n",
      "epoch no.2 train no.198240  loss = 2.80908 avg_loss = 3.53704\n",
      "epoch no.2 train no.198250  loss = 2.87330 avg_loss = 3.52850\n",
      "epoch no.2 train no.198260  loss = 3.28949 avg_loss = 3.56907\n",
      "epoch no.2 train no.198270  loss = 3.84661 avg_loss = 3.58802\n",
      "epoch no.2 train no.198280  loss = 4.60851 avg_loss = 3.55444\n",
      "epoch no.2 train no.198290  loss = 2.33736 avg_loss = 3.54695\n",
      "epoch no.2 train no.198300  loss = 2.45372 avg_loss = 3.56492\n",
      "epoch no.2 train no.198310  loss = 3.98798 avg_loss = 3.49662\n",
      "epoch no.2 train no.198320  loss = 3.10228 avg_loss = 3.50405\n",
      "epoch no.2 train no.198330  loss = 2.11823 avg_loss = 3.50023\n",
      "epoch no.2 train no.198340  loss = 3.12550 avg_loss = 3.48208\n",
      "epoch no.2 train no.198350  loss = 2.85767 avg_loss = 3.50182\n",
      "epoch no.2 train no.198360  loss = 2.48988 avg_loss = 3.50949\n",
      "epoch no.2 train no.198370  loss = 2.77021 avg_loss = 3.52336\n",
      "epoch no.2 train no.198380  loss = 2.61726 avg_loss = 3.49850\n",
      "epoch no.2 train no.198390  loss = 4.16186 avg_loss = 3.48797\n",
      "epoch no.2 train no.198400  loss = 3.65524 avg_loss = 3.44426\n",
      "epoch no.2 train no.198410  loss = 3.67856 avg_loss = 3.40434\n",
      "epoch no.2 train no.198420  loss = 3.54233 avg_loss = 3.44992\n",
      "epoch no.2 train no.198430  loss = 4.30713 avg_loss = 3.52018\n",
      "epoch no.2 train no.198440  loss = 2.64366 avg_loss = 3.49788\n",
      "epoch no.2 train no.198450  loss = 2.64463 avg_loss = 3.48685\n",
      "epoch no.2 train no.198460  loss = 2.01299 avg_loss = 3.47718\n",
      "epoch no.2 train no.198470  loss = 4.27404 avg_loss = 3.48508\n",
      "epoch no.2 train no.198480  loss = 2.23449 avg_loss = 3.49330\n",
      "epoch no.2 train no.198490  loss = 1.76906 avg_loss = 3.41768\n",
      "epoch no.2 train no.198500  loss = 2.52577 avg_loss = 3.44669\n",
      "epoch no.2 train no.198510  loss = 3.02734 avg_loss = 3.42566\n",
      "epoch no.2 train no.198520  loss = 5.21253 avg_loss = 3.49650\n",
      "epoch no.2 train no.198530  loss = 3.80665 avg_loss = 3.46744\n",
      "epoch no.2 train no.198540  loss = 3.46273 avg_loss = 3.44768\n",
      "epoch no.2 train no.198550  loss = 2.26853 avg_loss = 3.41971\n",
      "epoch no.2 train no.198560  loss = 4.29834 avg_loss = 3.45133\n",
      "epoch no.2 train no.198570  loss = 3.37606 avg_loss = 3.48082\n",
      "epoch no.2 train no.198580  loss = 3.25785 avg_loss = 3.45424\n",
      "epoch no.2 train no.198590  loss = 4.45504 avg_loss = 3.51987\n",
      "epoch no.2 train no.198600  loss = 3.36980 avg_loss = 3.55916\n",
      "epoch no.2 train no.198610  loss = 3.75091 avg_loss = 3.58376\n",
      "epoch no.2 train no.198620  loss = 5.41430 avg_loss = 3.54775\n",
      "epoch no.2 train no.198630  loss = 3.82698 avg_loss = 3.54798\n",
      "epoch no.2 train no.198640  loss = 3.60270 avg_loss = 3.54333\n",
      "epoch no.2 train no.198650  loss = 3.59584 avg_loss = 3.55037\n",
      "epoch no.2 train no.198660  loss = 2.85042 avg_loss = 3.57887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.198670  loss = 2.38396 avg_loss = 3.55959\n",
      "epoch no.2 train no.198680  loss = 3.56688 avg_loss = 3.54585\n",
      "epoch no.2 train no.198690  loss = 4.87833 avg_loss = 3.53929\n",
      "epoch no.2 train no.198700  loss = 3.73420 avg_loss = 3.54987\n",
      "epoch no.2 train no.198710  loss = 4.18732 avg_loss = 3.55152\n",
      "epoch no.2 train no.198720  loss = 2.24577 avg_loss = 3.48059\n",
      "epoch no.2 train no.198730  loss = 4.07941 avg_loss = 3.50287\n",
      "epoch no.2 train no.198740  loss = 3.08641 avg_loss = 3.49963\n",
      "epoch no.2 train no.198750  loss = 6.34717 avg_loss = 3.54513\n",
      "epoch no.2 train no.198760  loss = 3.69587 avg_loss = 3.57371\n",
      "epoch no.2 train no.198770  loss = 4.11837 avg_loss = 3.61627\n",
      "epoch no.2 train no.198780  loss = 3.02036 avg_loss = 3.64583\n",
      "epoch no.2 train no.198790  loss = 4.55342 avg_loss = 3.66047\n",
      "epoch no.2 train no.198800  loss = 2.26440 avg_loss = 3.64964\n",
      "epoch no.2 train no.198810  loss = 2.95108 avg_loss = 3.60550\n",
      "epoch no.2 train no.198820  loss = 2.70601 avg_loss = 3.64363\n",
      "epoch no.2 train no.198830  loss = 3.06566 avg_loss = 3.63728\n",
      "epoch no.2 train no.198840  loss = 3.50983 avg_loss = 3.62888\n",
      "epoch no.2 train no.198850  loss = 3.41078 avg_loss = 3.59983\n",
      "epoch no.2 train no.198860  loss = 3.24812 avg_loss = 3.60235\n",
      "epoch no.2 train no.198870  loss = 4.62583 avg_loss = 3.60882\n",
      "epoch no.2 train no.198880  loss = 4.75767 avg_loss = 3.65666\n",
      "epoch no.2 train no.198890  loss = 3.83863 avg_loss = 3.65002\n",
      "epoch no.2 train no.198900  loss = 3.14245 avg_loss = 3.61880\n",
      "epoch no.2 train no.198910  loss = 3.39194 avg_loss = 3.71197\n",
      "epoch no.2 train no.198920  loss = 2.95801 avg_loss = 3.69349\n",
      "epoch no.2 train no.198930  loss = 4.13908 avg_loss = 3.71362\n",
      "epoch no.2 train no.198940  loss = 3.52596 avg_loss = 3.69410\n",
      "epoch no.2 train no.198950  loss = 2.45859 avg_loss = 3.61934\n",
      "epoch no.2 train no.198960  loss = 3.95804 avg_loss = 3.62598\n",
      "epoch no.2 train no.198970  loss = 3.32890 avg_loss = 3.60774\n",
      "epoch no.2 train no.198980  loss = 3.15878 avg_loss = 3.59402\n",
      "epoch no.2 train no.198990  loss = 5.14365 avg_loss = 3.62705\n",
      "epoch no.2 train no.199000  loss = 3.65021 avg_loss = 3.65334\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.2 train no.199010  loss = 4.44301 avg_loss = 3.67769\n",
      "epoch no.2 train no.199020  loss = 3.54856 avg_loss = 3.66448\n",
      "epoch no.2 train no.199030  loss = 2.67225 avg_loss = 3.67912\n",
      "epoch no.2 train no.199040  loss = 6.26297 avg_loss = 3.74611\n",
      "epoch no.2 train no.199050  loss = 2.38191 avg_loss = 3.74438\n",
      "epoch no.2 train no.199060  loss = 5.45195 avg_loss = 3.75054\n",
      "epoch no.2 train no.199070  loss = 2.43924 avg_loss = 3.68716\n",
      "epoch no.2 train no.199080  loss = 3.94514 avg_loss = 3.63948\n",
      "epoch no.2 train no.199090  loss = 3.11534 avg_loss = 3.58972\n",
      "epoch no.2 train no.199100  loss = 3.35257 avg_loss = 3.64066\n",
      "epoch no.2 train no.199110  loss = 2.93413 avg_loss = 3.62198\n",
      "epoch no.2 train no.199120  loss = 4.88772 avg_loss = 3.67246\n",
      "epoch no.2 train no.199130  loss = 2.70198 avg_loss = 3.67495\n",
      "epoch no.2 train no.199140  loss = 2.44569 avg_loss = 3.68223\n",
      "epoch no.2 train no.199150  loss = 3.56161 avg_loss = 3.73718\n",
      "epoch no.2 train no.199160  loss = 3.30943 avg_loss = 3.69466\n",
      "epoch no.2 train no.199170  loss = 2.38951 avg_loss = 3.68439\n",
      "epoch no.2 train no.199180  loss = 2.68755 avg_loss = 3.62205\n",
      "epoch no.2 train no.199190  loss = 2.40341 avg_loss = 3.64758\n",
      "epoch no.2 train no.199200  loss = 5.23798 avg_loss = 3.66212\n",
      "epoch no.2 train no.199210  loss = 5.07923 avg_loss = 3.67699\n",
      "epoch no.2 train no.199220  loss = 3.64164 avg_loss = 3.66765\n",
      "epoch no.2 train no.199230  loss = 3.11349 avg_loss = 3.60871\n",
      "epoch no.2 train no.199240  loss = 2.72538 avg_loss = 3.61521\n",
      "epoch no.2 train no.199250  loss = 3.98178 avg_loss = 3.66199\n",
      "epoch no.2 train no.199260  loss = 3.17723 avg_loss = 3.66114\n",
      "epoch no.2 train no.199270  loss = 3.02299 avg_loss = 3.64656\n",
      "epoch no.2 train no.199280  loss = 2.88681 avg_loss = 3.62464\n",
      "epoch no.2 train no.199290  loss = 1.83430 avg_loss = 3.58045\n",
      "epoch no.2 train no.199300  loss = 4.81804 avg_loss = 3.58355\n",
      "epoch no.2 train no.199310  loss = 4.07593 avg_loss = 3.54100\n",
      "epoch no.2 train no.199320  loss = 3.45766 avg_loss = 3.47930\n",
      "epoch no.2 train no.199330  loss = 4.42552 avg_loss = 3.48630\n",
      "epoch no.2 train no.199340  loss = 5.30017 avg_loss = 3.51896\n",
      "epoch no.2 train no.199350  loss = 3.53682 avg_loss = 3.53222\n",
      "epoch no.2 train no.199360  loss = 3.49947 avg_loss = 3.54366\n",
      "epoch no.2 train no.199370  loss = 5.53821 avg_loss = 3.58207\n",
      "epoch no.2 train no.199380  loss = 3.69551 avg_loss = 3.56134\n",
      "epoch no.2 train no.199390  loss = 4.23710 avg_loss = 3.59666\n",
      "epoch no.2 train no.199400  loss = 3.64329 avg_loss = 3.63183\n",
      "epoch no.2 train no.199410  loss = 5.51802 avg_loss = 3.63998\n",
      "epoch no.2 train no.199420  loss = 2.99450 avg_loss = 3.64147\n",
      "epoch no.2 train no.199430  loss = 1.89094 avg_loss = 3.57770\n",
      "epoch no.2 train no.199440  loss = 2.67349 avg_loss = 3.56593\n",
      "epoch no.2 train no.199450  loss = 4.73810 avg_loss = 3.56288\n",
      "epoch no.2 train no.199460  loss = 4.76002 avg_loss = 3.55644\n",
      "epoch no.2 train no.199470  loss = 3.39000 avg_loss = 3.52998\n",
      "epoch no.2 train no.199480  loss = 3.49614 avg_loss = 3.53400\n",
      "epoch no.2 train no.199490  loss = 3.96752 avg_loss = 3.48826\n",
      "epoch no.2 train no.199500  loss = 3.85426 avg_loss = 3.48864\n",
      "epoch no.2 train no.199510  loss = 2.60899 avg_loss = 3.47771\n",
      "epoch no.2 train no.199520  loss = 3.78918 avg_loss = 3.48237\n",
      "epoch no.2 train no.199530  loss = 2.52857 avg_loss = 3.51673\n",
      "epoch no.2 train no.199540  loss = 3.66488 avg_loss = 3.52134\n",
      "epoch no.2 train no.199550  loss = 4.38499 avg_loss = 3.48717\n",
      "epoch no.2 train no.199560  loss = 3.46737 avg_loss = 3.50644\n",
      "epoch no.2 train no.199570  loss = 3.35217 avg_loss = 3.51919\n",
      "epoch no.2 train no.199580  loss = 3.22649 avg_loss = 3.56512\n",
      "epoch no.2 train no.199590  loss = 3.68040 avg_loss = 3.60180\n",
      "epoch no.2 train no.199600  loss = 6.20579 avg_loss = 3.59218\n",
      "epoch no.2 train no.199610  loss = 2.42810 avg_loss = 3.61633\n",
      "epoch no.2 train no.199620  loss = 4.75687 avg_loss = 3.64211\n",
      "epoch no.2 train no.199630  loss = 6.25150 avg_loss = 3.64972\n",
      "epoch no.2 train no.199640  loss = 3.41043 avg_loss = 3.60652\n",
      "epoch no.2 train no.199650  loss = 2.82687 avg_loss = 3.59549\n",
      "epoch no.2 train no.199660  loss = 4.63915 avg_loss = 3.61218\n",
      "epoch no.2 train no.199670  loss = 3.34787 avg_loss = 3.63527\n",
      "epoch no.2 train no.199680  loss = 3.37973 avg_loss = 3.58375\n",
      "epoch no.2 train no.199690  loss = 3.61774 avg_loss = 3.59753\n",
      "epoch no.2 train no.199700  loss = 3.25787 avg_loss = 3.53917\n",
      "epoch no.2 train no.199710  loss = 2.53430 avg_loss = 3.54212\n",
      "epoch no.2 train no.199720  loss = 4.42571 avg_loss = 3.58969\n",
      "epoch no.2 train no.199730  loss = 5.53579 avg_loss = 3.61401\n",
      "epoch no.2 train no.199740  loss = 2.46264 avg_loss = 3.61783\n",
      "epoch no.2 train no.199750  loss = 2.00216 avg_loss = 3.61272\n",
      "epoch no.2 train no.199760  loss = 3.80305 avg_loss = 3.64288\n",
      "epoch no.2 train no.199770  loss = 2.44584 avg_loss = 3.63128\n",
      "epoch no.2 train no.199780  loss = 3.51652 avg_loss = 3.63669\n",
      "epoch no.2 train no.199790  loss = 3.05787 avg_loss = 3.63261\n",
      "epoch no.2 train no.199800  loss = 3.46330 avg_loss = 3.64097\n",
      "epoch no.2 train no.199810  loss = 5.73841 avg_loss = 3.65132\n",
      "epoch no.2 train no.199820  loss = 3.43072 avg_loss = 3.66644\n",
      "epoch no.2 train no.199830  loss = 4.69546 avg_loss = 3.68610\n",
      "epoch no.2 train no.199840  loss = 4.44338 avg_loss = 3.66614\n",
      "epoch no.2 train no.199850  loss = 2.49519 avg_loss = 3.64582\n",
      "epoch no.2 train no.199860  loss = 3.19491 avg_loss = 3.63501\n",
      "epoch no.2 train no.199870  loss = 1.89517 avg_loss = 3.62820\n",
      "epoch no.2 train no.199880  loss = 3.13862 avg_loss = 3.66170\n",
      "epoch no.2 train no.199890  loss = 3.39814 avg_loss = 3.66583\n",
      "epoch no.2 train no.199900  loss = 2.41655 avg_loss = 3.64779\n",
      "epoch no.2 train no.199910  loss = 3.86603 avg_loss = 3.64401\n",
      "epoch no.2 train no.199920  loss = 3.13968 avg_loss = 3.62024\n",
      "epoch no.2 train no.199930  loss = 3.80792 avg_loss = 3.62493\n",
      "epoch no.2 train no.199940  loss = 5.40694 avg_loss = 3.63669\n",
      "epoch no.2 train no.199950  loss = 2.59638 avg_loss = 3.63514\n",
      "epoch no.2 train no.199960  loss = 1.61439 avg_loss = 3.55992\n",
      "epoch no.2 train no.199970  loss = 3.93892 avg_loss = 3.57353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.199980  loss = 4.22455 avg_loss = 3.58545\n",
      "epoch no.2 train no.199990  loss = 7.92394 avg_loss = 3.59216\n",
      "epoch no.2 train no.200000  loss = 4.39104 avg_loss = 3.56820\n",
      "2\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '송', '</s>']\n",
      "기분전환 팝송</s>\n",
      "epoch no.2 train no.200010  loss = 4.56179 avg_loss = 3.56989\n",
      "epoch no.2 train no.200020  loss = 2.54872 avg_loss = 3.53359\n",
      "epoch no.2 train no.200030  loss = 2.69175 avg_loss = 3.53298\n",
      "epoch no.2 train no.200040  loss = 4.91071 avg_loss = 3.55451\n",
      "epoch no.2 train no.200050  loss = 3.46994 avg_loss = 3.62308\n",
      "epoch no.2 train no.200060  loss = 5.00465 avg_loss = 3.65663\n",
      "epoch no.2 train no.200070  loss = 3.99400 avg_loss = 3.67665\n",
      "epoch no.2 train no.200080  loss = 4.84907 avg_loss = 3.67778\n",
      "epoch no.2 train no.200090  loss = 3.44789 avg_loss = 3.68463\n",
      "epoch no.2 train no.200100  loss = 2.99362 avg_loss = 3.60772\n",
      "epoch no.2 train no.200110  loss = 1.95420 avg_loss = 3.58724\n",
      "epoch no.2 train no.200120  loss = 2.12001 avg_loss = 3.56250\n",
      "epoch no.2 train no.200130  loss = 4.06216 avg_loss = 3.58700\n",
      "epoch no.2 train no.200140  loss = 2.89843 avg_loss = 3.53875\n",
      "epoch no.2 train no.200150  loss = 3.79872 avg_loss = 3.48722\n",
      "epoch no.2 train no.200160  loss = 4.87944 avg_loss = 3.55446\n",
      "epoch no.2 train no.200170  loss = 2.98496 avg_loss = 3.51726\n",
      "epoch no.2 train no.200180  loss = 4.00117 avg_loss = 3.52045\n",
      "epoch no.2 train no.200190  loss = 3.69671 avg_loss = 3.55765\n",
      "epoch no.2 train no.200200  loss = 2.62280 avg_loss = 3.56441\n",
      "epoch no.2 train no.200210  loss = 3.52079 avg_loss = 3.53281\n",
      "epoch no.2 train no.200220  loss = 2.42347 avg_loss = 3.49317\n",
      "epoch no.2 train no.200230  loss = 5.06221 avg_loss = 3.50526\n",
      "epoch no.2 train no.200240  loss = 2.20924 avg_loss = 3.56752\n",
      "epoch no.2 train no.200250  loss = 3.11797 avg_loss = 3.56332\n",
      "epoch no.2 train no.200260  loss = 4.12006 avg_loss = 3.58273\n",
      "epoch no.2 train no.200270  loss = 2.25638 avg_loss = 3.59306\n",
      "epoch no.2 train no.200280  loss = 5.46201 avg_loss = 3.60958\n",
      "epoch no.2 train no.200290  loss = 4.57783 avg_loss = 3.64582\n",
      "epoch no.2 train no.200300  loss = 2.80265 avg_loss = 3.55941\n",
      "epoch no.2 train no.200310  loss = 3.27230 avg_loss = 3.52661\n",
      "epoch no.2 train no.200320  loss = 5.30053 avg_loss = 3.47757\n",
      "epoch no.2 train no.200330  loss = 4.00458 avg_loss = 3.49733\n",
      "epoch no.2 train no.200340  loss = 4.76478 avg_loss = 3.51838\n",
      "epoch no.2 train no.200350  loss = 3.79909 avg_loss = 3.54611\n",
      "epoch no.2 train no.200360  loss = 3.66131 avg_loss = 3.53985\n",
      "epoch no.2 train no.200370  loss = 3.34558 avg_loss = 3.55435\n",
      "epoch no.2 train no.200380  loss = 4.00318 avg_loss = 3.52744\n",
      "epoch no.2 train no.200390  loss = 2.33532 avg_loss = 3.59777\n",
      "epoch no.2 train no.200400  loss = 2.84848 avg_loss = 3.60987\n",
      "epoch no.2 train no.200410  loss = 3.75917 avg_loss = 3.60077\n",
      "epoch no.2 train no.200420  loss = 2.88051 avg_loss = 3.57171\n",
      "epoch no.2 train no.200430  loss = 4.69794 avg_loss = 3.57963\n",
      "epoch no.2 train no.200440  loss = 3.00242 avg_loss = 3.51884\n",
      "epoch no.2 train no.200450  loss = 4.13256 avg_loss = 3.47423\n",
      "epoch no.2 train no.200460  loss = 3.34617 avg_loss = 3.48491\n",
      "epoch no.2 train no.200470  loss = 2.31278 avg_loss = 3.51292\n",
      "epoch no.2 train no.200480  loss = 2.87308 avg_loss = 3.52107\n",
      "epoch no.2 train no.200490  loss = 3.80001 avg_loss = 3.52927\n",
      "epoch no.2 train no.200500  loss = 3.02335 avg_loss = 3.49377\n",
      "epoch no.2 train no.200510  loss = 5.26820 avg_loss = 3.53881\n",
      "epoch no.2 train no.200520  loss = 4.94591 avg_loss = 3.58891\n",
      "epoch no.2 train no.200530  loss = 5.73626 avg_loss = 3.53440\n",
      "epoch no.2 train no.200540  loss = 3.52276 avg_loss = 3.52332\n",
      "epoch no.2 train no.200550  loss = 3.66978 avg_loss = 3.54237\n",
      "epoch no.2 train no.200560  loss = 3.13269 avg_loss = 3.57318\n",
      "epoch no.2 train no.200570  loss = 3.07184 avg_loss = 3.56466\n",
      "epoch no.2 train no.200580  loss = 4.28540 avg_loss = 3.54169\n",
      "epoch no.2 train no.200590  loss = 2.11322 avg_loss = 3.54267\n",
      "epoch no.2 train no.200600  loss = 2.82308 avg_loss = 3.50095\n",
      "epoch no.2 train no.200610  loss = 3.86228 avg_loss = 3.56260\n",
      "epoch no.2 train no.200620  loss = 3.11657 avg_loss = 3.54497\n",
      "epoch no.2 train no.200630  loss = 3.77750 avg_loss = 3.53139\n",
      "epoch no.2 train no.200640  loss = 2.68837 avg_loss = 3.51281\n",
      "epoch no.2 train no.200650  loss = 4.66561 avg_loss = 3.53869\n",
      "epoch no.2 train no.200660  loss = 3.41051 avg_loss = 3.53114\n",
      "epoch no.2 train no.200670  loss = 2.77143 avg_loss = 3.52517\n",
      "epoch no.2 train no.200680  loss = 2.84079 avg_loss = 3.49677\n",
      "epoch no.2 train no.200690  loss = 3.95351 avg_loss = 3.55492\n",
      "epoch no.2 train no.200700  loss = 3.59612 avg_loss = 3.53050\n",
      "epoch no.2 train no.200710  loss = 2.93311 avg_loss = 3.46880\n",
      "epoch no.2 train no.200720  loss = 2.60294 avg_loss = 3.47579\n",
      "epoch no.2 train no.200730  loss = 4.22649 avg_loss = 3.47334\n",
      "epoch no.2 train no.200740  loss = 2.95591 avg_loss = 3.49473\n",
      "epoch no.2 train no.200750  loss = 2.96085 avg_loss = 3.45788\n",
      "epoch no.2 train no.200760  loss = 2.02044 avg_loss = 3.48765\n",
      "epoch no.2 train no.200770  loss = 3.51035 avg_loss = 3.45326\n",
      "epoch no.2 train no.200780  loss = 6.95312 avg_loss = 3.52681\n",
      "epoch no.2 train no.200790  loss = 2.68766 avg_loss = 3.49340\n",
      "epoch no.2 train no.200800  loss = 3.21037 avg_loss = 3.49329\n",
      "epoch no.2 train no.200810  loss = 4.26819 avg_loss = 3.56385\n",
      "epoch no.2 train no.200820  loss = 3.80771 avg_loss = 3.59695\n",
      "epoch no.2 train no.200830  loss = 2.45373 avg_loss = 3.60544\n",
      "epoch no.2 train no.200840  loss = 3.41503 avg_loss = 3.60348\n",
      "epoch no.2 train no.200850  loss = 3.36265 avg_loss = 3.65153\n",
      "epoch no.2 train no.200860  loss = 4.81786 avg_loss = 3.68074\n",
      "epoch no.2 train no.200870  loss = 3.39806 avg_loss = 3.67596\n",
      "epoch no.2 train no.200880  loss = 2.72474 avg_loss = 3.65920\n",
      "epoch no.2 train no.200890  loss = 3.22729 avg_loss = 3.66592\n",
      "epoch no.2 train no.200900  loss = 2.39489 avg_loss = 3.62964\n",
      "epoch no.2 train no.200910  loss = 3.60713 avg_loss = 3.62502\n",
      "epoch no.2 train no.200920  loss = 3.56738 avg_loss = 3.61207\n",
      "epoch no.2 train no.200930  loss = 3.05761 avg_loss = 3.57314\n",
      "epoch no.2 train no.200940  loss = 2.71460 avg_loss = 3.54490\n",
      "epoch no.2 train no.200950  loss = 2.50425 avg_loss = 3.52100\n",
      "epoch no.2 train no.200960  loss = 3.43331 avg_loss = 3.50636\n",
      "epoch no.2 train no.200970  loss = 4.89212 avg_loss = 3.53826\n",
      "epoch no.2 train no.200980  loss = 3.36575 avg_loss = 3.56489\n",
      "epoch no.2 train no.200990  loss = 2.45208 avg_loss = 3.50903\n",
      "epoch no.2 train no.201000  loss = 4.77635 avg_loss = 3.57149\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁음악', '송', '</s>']\n",
      "기분전환을 위한 팝송</s>\n",
      "epoch no.2 train no.201010  loss = 3.44209 avg_loss = 3.56899\n",
      "epoch no.2 train no.201020  loss = 2.80905 avg_loss = 3.58478\n",
      "epoch no.2 train no.201030  loss = 4.88233 avg_loss = 3.51287\n",
      "epoch no.2 train no.201040  loss = 2.82251 avg_loss = 3.50884\n",
      "epoch no.2 train no.201050  loss = 5.49479 avg_loss = 3.54131\n",
      "epoch no.2 train no.201060  loss = 4.29862 avg_loss = 3.56941\n",
      "epoch no.2 train no.201070  loss = 2.96857 avg_loss = 3.55980\n",
      "epoch no.2 train no.201080  loss = 5.17574 avg_loss = 3.52791\n",
      "epoch no.2 train no.201090  loss = 2.59462 avg_loss = 3.50052\n",
      "epoch no.2 train no.201100  loss = 2.46502 avg_loss = 3.48464\n",
      "epoch no.2 train no.201110  loss = 3.54257 avg_loss = 3.45782\n",
      "epoch no.2 train no.201120  loss = 4.42243 avg_loss = 3.42764\n",
      "epoch no.2 train no.201130  loss = 2.20225 avg_loss = 3.44602\n",
      "epoch no.2 train no.201140  loss = 1.59760 avg_loss = 3.43154\n",
      "epoch no.2 train no.201150  loss = 3.77162 avg_loss = 3.38829\n",
      "epoch no.2 train no.201160  loss = 2.96461 avg_loss = 3.37108\n",
      "epoch no.2 train no.201170  loss = 3.52007 avg_loss = 3.39535\n",
      "epoch no.2 train no.201180  loss = 4.99189 avg_loss = 3.44743\n",
      "epoch no.2 train no.201190  loss = 2.63380 avg_loss = 3.44138\n",
      "epoch no.2 train no.201200  loss = 4.43238 avg_loss = 3.43821\n",
      "epoch no.2 train no.201210  loss = 2.99364 avg_loss = 3.44988\n",
      "epoch no.2 train no.201220  loss = 5.83320 avg_loss = 3.48641\n",
      "epoch no.2 train no.201230  loss = 3.78978 avg_loss = 3.53149\n",
      "epoch no.2 train no.201240  loss = 3.42003 avg_loss = 3.52278\n",
      "epoch no.2 train no.201250  loss = 2.27172 avg_loss = 3.55590\n",
      "epoch no.2 train no.201260  loss = 4.45698 avg_loss = 3.54955\n",
      "epoch no.2 train no.201270  loss = 5.05759 avg_loss = 3.60257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.201280  loss = 4.33709 avg_loss = 3.58390\n",
      "epoch no.2 train no.201290  loss = 4.82332 avg_loss = 3.60260\n",
      "epoch no.2 train no.201300  loss = 4.14645 avg_loss = 3.60446\n",
      "epoch no.2 train no.201310  loss = 6.05718 avg_loss = 3.61491\n",
      "epoch no.2 train no.201320  loss = 3.61597 avg_loss = 3.60687\n",
      "epoch no.2 train no.201330  loss = 3.95057 avg_loss = 3.64138\n",
      "epoch no.2 train no.201340  loss = 2.50376 avg_loss = 3.59196\n",
      "epoch no.2 train no.201350  loss = 5.45798 avg_loss = 3.59844\n",
      "epoch no.2 train no.201360  loss = 3.21188 avg_loss = 3.61403\n",
      "epoch no.2 train no.201370  loss = 2.96286 avg_loss = 3.62088\n",
      "epoch no.2 train no.201380  loss = 2.61046 avg_loss = 3.61778\n",
      "epoch no.2 train no.201390  loss = 2.34150 avg_loss = 3.61620\n",
      "epoch no.2 train no.201400  loss = 4.79583 avg_loss = 3.66572\n",
      "epoch no.2 train no.201410  loss = 3.93820 avg_loss = 3.66889\n",
      "epoch no.2 train no.201420  loss = 3.58316 avg_loss = 3.74719\n",
      "epoch no.2 train no.201430  loss = 3.53930 avg_loss = 3.73052\n",
      "epoch no.2 train no.201440  loss = 4.02453 avg_loss = 3.70861\n",
      "epoch no.2 train no.201450  loss = 2.57363 avg_loss = 3.70740\n",
      "epoch no.2 train no.201460  loss = 3.07339 avg_loss = 3.68043\n",
      "epoch no.2 train no.201470  loss = 2.97658 avg_loss = 3.59778\n",
      "epoch no.2 train no.201480  loss = 2.59219 avg_loss = 3.59179\n",
      "epoch no.2 train no.201490  loss = 3.82055 avg_loss = 3.61191\n",
      "epoch no.2 train no.201500  loss = 4.54274 avg_loss = 3.61842\n",
      "epoch no.2 train no.201510  loss = 5.00084 avg_loss = 3.65757\n",
      "epoch no.2 train no.201520  loss = 3.01820 avg_loss = 3.66270\n",
      "epoch no.2 train no.201530  loss = 4.70183 avg_loss = 3.69903\n",
      "epoch no.2 train no.201540  loss = 3.03306 avg_loss = 3.67369\n",
      "epoch no.2 train no.201550  loss = 2.57796 avg_loss = 3.70438\n",
      "epoch no.2 train no.201560  loss = 3.54086 avg_loss = 3.72758\n",
      "epoch no.2 train no.201570  loss = 3.02294 avg_loss = 3.71131\n",
      "epoch no.2 train no.201580  loss = 4.46132 avg_loss = 3.74150\n",
      "epoch no.2 train no.201590  loss = 2.99532 avg_loss = 3.78394\n",
      "epoch no.2 train no.201600  loss = 2.83333 avg_loss = 3.75375\n",
      "epoch no.2 train no.201610  loss = 1.86849 avg_loss = 3.76881\n",
      "epoch no.2 train no.201620  loss = 3.50312 avg_loss = 3.73164\n",
      "epoch no.2 train no.201630  loss = 2.28277 avg_loss = 3.70303\n",
      "epoch no.2 train no.201640  loss = 1.55141 avg_loss = 3.63396\n",
      "epoch no.2 train no.201650  loss = 4.09252 avg_loss = 3.60191\n",
      "epoch no.2 train no.201660  loss = 2.76305 avg_loss = 3.56686\n",
      "epoch no.2 train no.201670  loss = 2.67964 avg_loss = 3.60128\n",
      "epoch no.2 train no.201680  loss = 4.90045 avg_loss = 3.56645\n",
      "epoch no.2 train no.201690  loss = 3.00929 avg_loss = 3.56834\n",
      "epoch no.2 train no.201700  loss = 3.37737 avg_loss = 3.53250\n",
      "epoch no.2 train no.201710  loss = 4.22552 avg_loss = 3.58195\n",
      "epoch no.2 train no.201720  loss = 3.71330 avg_loss = 3.56271\n",
      "epoch no.2 train no.201730  loss = 3.97738 avg_loss = 3.58226\n",
      "epoch no.2 train no.201740  loss = 3.03201 avg_loss = 3.57169\n",
      "epoch no.2 train no.201750  loss = 2.09111 avg_loss = 3.57433\n",
      "epoch no.2 train no.201760  loss = 3.66787 avg_loss = 3.57014\n",
      "epoch no.2 train no.201770  loss = 1.71357 avg_loss = 3.54881\n",
      "epoch no.2 train no.201780  loss = 3.16825 avg_loss = 3.54291\n",
      "epoch no.2 train no.201790  loss = 3.15223 avg_loss = 3.49956\n",
      "epoch no.2 train no.201800  loss = 3.34820 avg_loss = 3.48411\n",
      "epoch no.2 train no.201810  loss = 2.27920 avg_loss = 3.53185\n",
      "epoch no.2 train no.201820  loss = 4.06369 avg_loss = 3.55632\n",
      "epoch no.2 train no.201830  loss = 2.40522 avg_loss = 3.57567\n",
      "epoch no.2 train no.201840  loss = 2.26821 avg_loss = 3.54515\n",
      "epoch no.2 train no.201850  loss = 2.32875 avg_loss = 3.51934\n",
      "epoch no.2 train no.201860  loss = 3.52276 avg_loss = 3.48348\n",
      "epoch no.2 train no.201870  loss = 2.91272 avg_loss = 3.44053\n",
      "epoch no.2 train no.201880  loss = 4.91675 avg_loss = 3.45583\n",
      "epoch no.2 train no.201890  loss = 3.39259 avg_loss = 3.44456\n",
      "epoch no.2 train no.201900  loss = 2.29391 avg_loss = 3.40750\n",
      "epoch no.2 train no.201910  loss = 3.28508 avg_loss = 3.42520\n",
      "epoch no.2 train no.201920  loss = 2.75980 avg_loss = 3.46109\n",
      "epoch no.2 train no.201930  loss = 4.02154 avg_loss = 3.49161\n",
      "epoch no.2 train no.201940  loss = 3.23320 avg_loss = 3.52257\n",
      "epoch no.2 train no.201950  loss = 2.90209 avg_loss = 3.58958\n",
      "epoch no.2 train no.201960  loss = 3.34878 avg_loss = 3.63797\n",
      "epoch no.2 train no.201970  loss = 5.56219 avg_loss = 3.62082\n",
      "epoch no.2 train no.201980  loss = 2.62694 avg_loss = 3.57613\n",
      "epoch no.2 train no.201990  loss = 2.00966 avg_loss = 3.53368\n",
      "epoch no.2 train no.202000  loss = 2.23495 avg_loss = 3.57641\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁딱', '▁좋은', '▁음악', '</s>', '</s>']\n",
      "기분전환에 딱 좋은 음악들</s>\n",
      "epoch no.2 train no.202010  loss = 4.17107 avg_loss = 3.60470\n",
      "epoch no.2 train no.202020  loss = 2.69918 avg_loss = 3.58937\n",
      "epoch no.2 train no.202030  loss = 2.30813 avg_loss = 3.59805\n",
      "epoch no.2 train no.202040  loss = 3.43238 avg_loss = 3.54712\n",
      "epoch no.2 train no.202050  loss = 5.29815 avg_loss = 3.53466\n",
      "epoch no.2 train no.202060  loss = 4.61793 avg_loss = 3.49355\n",
      "epoch no.2 train no.202070  loss = 2.93589 avg_loss = 3.47691\n",
      "epoch no.2 train no.202080  loss = 3.42457 avg_loss = 3.47760\n",
      "epoch no.2 train no.202090  loss = 3.00906 avg_loss = 3.51629\n",
      "epoch no.2 train no.202100  loss = 2.87073 avg_loss = 3.52365\n",
      "epoch no.2 train no.202110  loss = 3.17747 avg_loss = 3.56393\n",
      "epoch no.2 train no.202120  loss = 4.19215 avg_loss = 3.63822\n",
      "epoch no.2 train no.202130  loss = 4.69177 avg_loss = 3.66750\n",
      "epoch no.2 train no.202140  loss = 3.80459 avg_loss = 3.67671\n",
      "epoch no.2 train no.202150  loss = 3.19820 avg_loss = 3.67190\n",
      "epoch no.2 train no.202160  loss = 5.16880 avg_loss = 3.67620\n",
      "epoch no.2 train no.202170  loss = 2.63357 avg_loss = 3.65268\n",
      "epoch no.2 train no.202180  loss = 4.41657 avg_loss = 3.61723\n",
      "epoch no.2 train no.202190  loss = 5.61163 avg_loss = 3.64790\n",
      "epoch no.2 train no.202200  loss = 4.96337 avg_loss = 3.67405\n",
      "epoch no.2 train no.202210  loss = 2.83638 avg_loss = 3.68059\n",
      "epoch no.2 train no.202220  loss = 2.92495 avg_loss = 3.69841\n",
      "epoch no.2 train no.202230  loss = 3.68556 avg_loss = 3.67766\n",
      "epoch no.2 train no.202240  loss = 4.13480 avg_loss = 3.71086\n",
      "epoch no.2 train no.202250  loss = 2.66169 avg_loss = 3.65468\n",
      "epoch no.2 train no.202260  loss = 4.78547 avg_loss = 3.60073\n",
      "epoch no.2 train no.202270  loss = 2.61067 avg_loss = 3.57013\n",
      "epoch no.2 train no.202280  loss = 2.14482 avg_loss = 3.56170\n",
      "epoch no.2 train no.202290  loss = 2.43954 avg_loss = 3.56755\n",
      "epoch no.2 train no.202300  loss = 2.43588 avg_loss = 3.55049\n",
      "epoch no.2 train no.202310  loss = 4.01640 avg_loss = 3.59977\n",
      "epoch no.2 train no.202320  loss = 3.86290 avg_loss = 3.58471\n",
      "epoch no.2 train no.202330  loss = 3.08853 avg_loss = 3.52419\n",
      "epoch no.2 train no.202340  loss = 4.76262 avg_loss = 3.52827\n",
      "epoch no.2 train no.202350  loss = 4.79743 avg_loss = 3.58346\n",
      "epoch no.2 train no.202360  loss = 5.76103 avg_loss = 3.60331\n",
      "epoch no.2 train no.202370  loss = 4.08541 avg_loss = 3.61914\n",
      "epoch no.2 train no.202380  loss = 4.95177 avg_loss = 3.64532\n",
      "epoch no.2 train no.202390  loss = 4.51170 avg_loss = 3.65908\n",
      "epoch no.2 train no.202400  loss = 6.97830 avg_loss = 3.68395\n",
      "epoch no.2 train no.202410  loss = 3.97509 avg_loss = 3.72424\n",
      "epoch no.2 train no.202420  loss = 6.79083 avg_loss = 3.75538\n",
      "epoch no.2 train no.202430  loss = 2.75535 avg_loss = 3.70315\n",
      "epoch no.2 train no.202440  loss = 2.75316 avg_loss = 3.72747\n",
      "epoch no.2 train no.202450  loss = 3.89132 avg_loss = 3.69870\n",
      "epoch no.2 train no.202460  loss = 2.51975 avg_loss = 3.66933\n",
      "epoch no.2 train no.202470  loss = 3.37894 avg_loss = 3.60541\n",
      "epoch no.2 train no.202480  loss = 3.27208 avg_loss = 3.62726\n",
      "epoch no.2 train no.202490  loss = 4.35283 avg_loss = 3.63004\n",
      "epoch no.2 train no.202500  loss = 2.43691 avg_loss = 3.62062\n",
      "epoch no.2 train no.202510  loss = 3.05590 avg_loss = 3.58557\n",
      "epoch no.2 train no.202520  loss = 2.69907 avg_loss = 3.62730\n",
      "epoch no.2 train no.202530  loss = 2.18256 avg_loss = 3.66224\n",
      "epoch no.2 train no.202540  loss = 4.56664 avg_loss = 3.68441\n",
      "epoch no.2 train no.202550  loss = 2.89627 avg_loss = 3.68325\n",
      "epoch no.2 train no.202560  loss = 2.22016 avg_loss = 3.62753\n",
      "epoch no.2 train no.202570  loss = 2.84544 avg_loss = 3.62882\n",
      "epoch no.2 train no.202580  loss = 2.72936 avg_loss = 3.59566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.202590  loss = 2.63723 avg_loss = 3.59409\n",
      "epoch no.2 train no.202600  loss = 3.08520 avg_loss = 3.62687\n",
      "epoch no.2 train no.202610  loss = 3.58521 avg_loss = 3.61545\n",
      "epoch no.2 train no.202620  loss = 2.30523 avg_loss = 3.56271\n",
      "epoch no.2 train no.202630  loss = 2.82643 avg_loss = 3.55504\n",
      "epoch no.2 train no.202640  loss = 3.84953 avg_loss = 3.54300\n",
      "epoch no.2 train no.202650  loss = 1.82555 avg_loss = 3.60291\n",
      "epoch no.2 train no.202660  loss = 4.89515 avg_loss = 3.61078\n",
      "epoch no.2 train no.202670  loss = 4.03505 avg_loss = 3.62823\n",
      "epoch no.2 train no.202680  loss = 4.69882 avg_loss = 3.60498\n",
      "epoch no.2 train no.202690  loss = 5.45090 avg_loss = 3.61646\n",
      "epoch no.2 train no.202700  loss = 3.09789 avg_loss = 3.65528\n",
      "epoch no.2 train no.202710  loss = 2.60206 avg_loss = 3.64793\n",
      "epoch no.2 train no.202720  loss = 2.22460 avg_loss = 3.62171\n",
      "epoch no.2 train no.202730  loss = 4.35245 avg_loss = 3.63777\n",
      "epoch no.2 train no.202740  loss = 5.24339 avg_loss = 3.66594\n",
      "epoch no.2 train no.202750  loss = 2.87791 avg_loss = 3.67668\n",
      "epoch no.2 train no.202760  loss = 3.73812 avg_loss = 3.66351\n",
      "epoch no.2 train no.202770  loss = 2.92873 avg_loss = 3.60643\n",
      "epoch no.2 train no.202780  loss = 2.34892 avg_loss = 3.61357\n",
      "epoch no.2 train no.202790  loss = 5.27487 avg_loss = 3.62036\n",
      "epoch no.2 train no.202800  loss = 2.67391 avg_loss = 3.59188\n",
      "epoch no.2 train no.202810  loss = 5.36896 avg_loss = 3.62822\n",
      "epoch no.2 train no.202820  loss = 3.10934 avg_loss = 3.65909\n",
      "epoch no.2 train no.202830  loss = 3.31727 avg_loss = 3.64810\n",
      "epoch no.2 train no.202840  loss = 5.18168 avg_loss = 3.67444\n",
      "epoch no.2 train no.202850  loss = 3.95476 avg_loss = 3.61198\n",
      "epoch no.2 train no.202860  loss = 5.03557 avg_loss = 3.64521\n",
      "epoch no.2 train no.202870  loss = 2.91704 avg_loss = 3.60526\n",
      "epoch no.2 train no.202880  loss = 3.66533 avg_loss = 3.56468\n",
      "epoch no.2 train no.202890  loss = 1.97428 avg_loss = 3.56391\n",
      "epoch no.2 train no.202900  loss = 3.01916 avg_loss = 3.58067\n",
      "epoch no.2 train no.202910  loss = 4.19521 avg_loss = 3.54957\n",
      "epoch no.2 train no.202920  loss = 2.99772 avg_loss = 3.53681\n",
      "epoch no.2 train no.202930  loss = 3.60933 avg_loss = 3.54980\n",
      "epoch no.2 train no.202940  loss = 3.60077 avg_loss = 3.52275\n",
      "epoch no.2 train no.202950  loss = 2.95746 avg_loss = 3.49746\n",
      "epoch no.2 train no.202960  loss = 3.11587 avg_loss = 3.47863\n",
      "epoch no.2 train no.202970  loss = 2.15558 avg_loss = 3.44498\n",
      "epoch no.2 train no.202980  loss = 2.85057 avg_loss = 3.45214\n",
      "epoch no.2 train no.202990  loss = 3.89845 avg_loss = 3.44325\n",
      "epoch no.2 train no.203000  loss = 2.15265 avg_loss = 3.48820\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 좋은 음악</s>\n",
      "epoch no.2 train no.203010  loss = 2.74534 avg_loss = 3.53329\n",
      "epoch no.2 train no.203020  loss = 2.39513 avg_loss = 3.52807\n",
      "epoch no.2 train no.203030  loss = 2.93291 avg_loss = 3.52535\n",
      "epoch no.2 train no.203040  loss = 3.57817 avg_loss = 3.53815\n",
      "epoch no.2 train no.203050  loss = 2.79946 avg_loss = 3.54414\n",
      "epoch no.2 train no.203060  loss = 4.67348 avg_loss = 3.62106\n",
      "epoch no.2 train no.203070  loss = 5.44058 avg_loss = 3.63276\n",
      "epoch no.2 train no.203080  loss = 4.82513 avg_loss = 3.69045\n",
      "epoch no.2 train no.203090  loss = 2.63115 avg_loss = 3.65186\n",
      "epoch no.2 train no.203100  loss = 2.43744 avg_loss = 3.64854\n",
      "epoch no.2 train no.203110  loss = 3.07254 avg_loss = 3.66328\n",
      "epoch no.2 train no.203120  loss = 3.59400 avg_loss = 3.65541\n",
      "epoch no.2 train no.203130  loss = 4.15708 avg_loss = 3.61022\n",
      "epoch no.2 train no.203140  loss = 2.48186 avg_loss = 3.59187\n",
      "epoch no.2 train no.203150  loss = 3.47502 avg_loss = 3.58930\n",
      "epoch no.2 train no.203160  loss = 2.32727 avg_loss = 3.59666\n",
      "epoch no.2 train no.203170  loss = 3.16087 avg_loss = 3.64324\n",
      "epoch no.2 train no.203180  loss = 3.73517 avg_loss = 3.63053\n",
      "epoch no.2 train no.203190  loss = 2.95111 avg_loss = 3.63402\n",
      "epoch no.2 train no.203200  loss = 3.27125 avg_loss = 3.64287\n",
      "epoch no.2 train no.203210  loss = 2.70928 avg_loss = 3.60857\n",
      "epoch no.2 train no.203220  loss = 2.38732 avg_loss = 3.62597\n",
      "epoch no.2 train no.203230  loss = 4.18190 avg_loss = 3.59434\n",
      "epoch no.2 train no.203240  loss = 4.58865 avg_loss = 3.57911\n",
      "epoch no.2 train no.203250  loss = 2.57935 avg_loss = 3.57244\n",
      "epoch no.2 train no.203260  loss = 6.09478 avg_loss = 3.60099\n",
      "epoch no.2 train no.203270  loss = 3.04560 avg_loss = 3.59001\n",
      "epoch no.2 train no.203280  loss = 4.17158 avg_loss = 3.54598\n",
      "epoch no.2 train no.203290  loss = 5.78523 avg_loss = 3.58140\n",
      "epoch no.2 train no.203300  loss = 1.78331 avg_loss = 3.58076\n",
      "epoch no.2 train no.203310  loss = 3.58023 avg_loss = 3.59429\n",
      "epoch no.2 train no.203320  loss = 3.16662 avg_loss = 3.56902\n",
      "epoch no.2 train no.203330  loss = 7.07151 avg_loss = 3.62150\n",
      "epoch no.2 train no.203340  loss = 2.49968 avg_loss = 3.70362\n",
      "epoch no.2 train no.203350  loss = 4.59903 avg_loss = 3.74660\n",
      "epoch no.2 train no.203360  loss = 2.34145 avg_loss = 3.72365\n",
      "epoch no.2 train no.203370  loss = 3.38020 avg_loss = 3.74581\n",
      "epoch no.2 train no.203380  loss = 4.18594 avg_loss = 3.69089\n",
      "epoch no.2 train no.203390  loss = 3.56667 avg_loss = 3.64249\n",
      "epoch no.2 train no.203400  loss = 5.55104 avg_loss = 3.67480\n",
      "epoch no.2 train no.203410  loss = 2.97808 avg_loss = 3.62866\n",
      "epoch no.2 train no.203420  loss = 3.17165 avg_loss = 3.62754\n",
      "epoch no.2 train no.203430  loss = 2.59009 avg_loss = 3.61918\n",
      "epoch no.2 train no.203440  loss = 4.80432 avg_loss = 3.55972\n",
      "epoch no.2 train no.203450  loss = 5.92444 avg_loss = 3.63141\n",
      "epoch no.2 train no.203460  loss = 2.89813 avg_loss = 3.59423\n",
      "epoch no.2 train no.203470  loss = 4.41718 avg_loss = 3.58646\n",
      "epoch no.2 train no.203480  loss = 5.03898 avg_loss = 3.61437\n",
      "epoch no.2 train no.203490  loss = 2.64323 avg_loss = 3.64576\n",
      "epoch no.2 train no.203500  loss = 2.02350 avg_loss = 3.64672\n",
      "epoch no.2 train no.203510  loss = 2.72648 avg_loss = 3.66906\n",
      "epoch no.2 train no.203520  loss = 3.41804 avg_loss = 3.66705\n",
      "epoch no.2 train no.203530  loss = 5.33438 avg_loss = 3.67761\n",
      "epoch no.2 train no.203540  loss = 5.67775 avg_loss = 3.74137\n",
      "epoch no.2 train no.203550  loss = 2.25400 avg_loss = 3.73021\n",
      "epoch no.2 train no.203560  loss = 3.31877 avg_loss = 3.66204\n",
      "epoch no.2 train no.203570  loss = 3.17442 avg_loss = 3.68485\n",
      "epoch no.2 train no.203580  loss = 3.29983 avg_loss = 3.67768\n",
      "epoch no.2 train no.203590  loss = 1.99438 avg_loss = 3.63906\n",
      "epoch no.2 train no.203600  loss = 2.95997 avg_loss = 3.64953\n",
      "epoch no.2 train no.203610  loss = 2.51938 avg_loss = 3.63819\n",
      "epoch no.2 train no.203620  loss = 3.44848 avg_loss = 3.63282\n",
      "epoch no.2 train no.203630  loss = 2.92979 avg_loss = 3.61748\n",
      "epoch no.2 train no.203640  loss = 2.38080 avg_loss = 3.64160\n",
      "epoch no.2 train no.203650  loss = 3.44509 avg_loss = 3.66478\n",
      "epoch no.2 train no.203660  loss = 2.47856 avg_loss = 3.58991\n",
      "epoch no.2 train no.203670  loss = 2.84876 avg_loss = 3.63858\n",
      "epoch no.2 train no.203680  loss = 2.82367 avg_loss = 3.62642\n",
      "epoch no.2 train no.203690  loss = 2.78639 avg_loss = 3.63453\n",
      "epoch no.2 train no.203700  loss = 5.01963 avg_loss = 3.66925\n",
      "epoch no.2 train no.203710  loss = 5.38573 avg_loss = 3.70342\n",
      "epoch no.2 train no.203720  loss = 5.26917 avg_loss = 3.71239\n",
      "epoch no.2 train no.203730  loss = 4.13502 avg_loss = 3.75381\n",
      "epoch no.2 train no.203740  loss = 2.77414 avg_loss = 3.80032\n",
      "epoch no.2 train no.203750  loss = 3.19119 avg_loss = 3.72562\n",
      "epoch no.2 train no.203760  loss = 2.31967 avg_loss = 3.67545\n",
      "epoch no.2 train no.203770  loss = 2.94639 avg_loss = 3.68707\n",
      "epoch no.2 train no.203780  loss = 2.76090 avg_loss = 3.66903\n",
      "epoch no.2 train no.203790  loss = 3.67238 avg_loss = 3.67351\n",
      "epoch no.2 train no.203800  loss = 2.36808 avg_loss = 3.63634\n",
      "epoch no.2 train no.203810  loss = 3.22659 avg_loss = 3.62458\n",
      "epoch no.2 train no.203820  loss = 5.36962 avg_loss = 3.66461\n",
      "epoch no.2 train no.203830  loss = 4.19237 avg_loss = 3.59445\n",
      "epoch no.2 train no.203840  loss = 3.21506 avg_loss = 3.57115\n",
      "epoch no.2 train no.203850  loss = 3.78395 avg_loss = 3.55672\n",
      "epoch no.2 train no.203860  loss = 2.38174 avg_loss = 3.60115\n",
      "epoch no.2 train no.203870  loss = 3.25527 avg_loss = 3.63657\n",
      "epoch no.2 train no.203880  loss = 4.47751 avg_loss = 3.63694\n",
      "epoch no.2 train no.203890  loss = 4.29611 avg_loss = 3.62113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.203900  loss = 5.00898 avg_loss = 3.65238\n",
      "epoch no.2 train no.203910  loss = 2.48611 avg_loss = 3.64232\n",
      "epoch no.2 train no.203920  loss = 2.94063 avg_loss = 3.66880\n",
      "epoch no.2 train no.203930  loss = 3.88046 avg_loss = 3.66624\n",
      "epoch no.2 train no.203940  loss = 4.25889 avg_loss = 3.59752\n",
      "epoch no.2 train no.203950  loss = 7.18635 avg_loss = 3.66580\n",
      "epoch no.2 train no.203960  loss = 2.73060 avg_loss = 3.65140\n",
      "epoch no.2 train no.203970  loss = 3.00447 avg_loss = 3.59866\n",
      "epoch no.2 train no.203980  loss = 2.77515 avg_loss = 3.58588\n",
      "epoch no.2 train no.203990  loss = 3.79690 avg_loss = 3.58304\n",
      "epoch no.2 train no.204000  loss = 3.17448 avg_loss = 3.55849\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.2 train no.204010  loss = 3.38017 avg_loss = 3.52239\n",
      "epoch no.2 train no.204020  loss = 2.93586 avg_loss = 3.56837\n",
      "epoch no.2 train no.204030  loss = 4.51816 avg_loss = 3.53008\n",
      "epoch no.2 train no.204040  loss = 3.43826 avg_loss = 3.55264\n",
      "epoch no.2 train no.204050  loss = 4.39859 avg_loss = 3.58128\n",
      "epoch no.2 train no.204060  loss = 3.70498 avg_loss = 3.61521\n",
      "epoch no.2 train no.204070  loss = 2.79161 avg_loss = 3.65509\n",
      "epoch no.2 train no.204080  loss = 7.24966 avg_loss = 3.62041\n",
      "epoch no.2 train no.204090  loss = 2.69786 avg_loss = 3.61235\n",
      "epoch no.2 train no.204100  loss = 2.38495 avg_loss = 3.59569\n",
      "epoch no.2 train no.204110  loss = 6.60280 avg_loss = 3.65753\n",
      "epoch no.2 train no.204120  loss = 2.69397 avg_loss = 3.67086\n",
      "epoch no.2 train no.204130  loss = 3.35024 avg_loss = 3.65063\n",
      "epoch no.2 train no.204140  loss = 2.50708 avg_loss = 3.65469\n",
      "epoch no.2 train no.204150  loss = 4.84861 avg_loss = 3.64932\n",
      "epoch no.2 train no.204160  loss = 2.86692 avg_loss = 3.64682\n",
      "epoch no.2 train no.204170  loss = 3.37644 avg_loss = 3.67306\n",
      "epoch no.2 train no.204180  loss = 5.76805 avg_loss = 3.69025\n",
      "epoch no.2 train no.204190  loss = 3.49719 avg_loss = 3.74228\n",
      "epoch no.2 train no.204200  loss = 2.92648 avg_loss = 3.79552\n",
      "epoch no.2 train no.204210  loss = 4.20074 avg_loss = 3.77456\n",
      "epoch no.2 train no.204220  loss = 2.52238 avg_loss = 3.72740\n",
      "epoch no.2 train no.204230  loss = 2.76073 avg_loss = 3.71990\n",
      "epoch no.2 train no.204240  loss = 3.41829 avg_loss = 3.76343\n",
      "epoch no.2 train no.204250  loss = 4.05441 avg_loss = 3.74923\n",
      "epoch no.2 train no.204260  loss = 4.42234 avg_loss = 3.78719\n",
      "epoch no.2 train no.204270  loss = 2.28718 avg_loss = 3.71853\n",
      "epoch no.2 train no.204280  loss = 2.95356 avg_loss = 3.72346\n",
      "epoch no.2 train no.204290  loss = 4.71447 avg_loss = 3.70189\n",
      "epoch no.2 train no.204300  loss = 5.37403 avg_loss = 3.70464\n",
      "epoch no.2 train no.204310  loss = 3.92882 avg_loss = 3.67763\n",
      "epoch no.2 train no.204320  loss = 2.26843 avg_loss = 3.65957\n",
      "epoch no.2 train no.204330  loss = 3.43258 avg_loss = 3.68805\n",
      "epoch no.2 train no.204340  loss = 2.91193 avg_loss = 3.71811\n",
      "epoch no.2 train no.204350  loss = 5.70571 avg_loss = 3.79567\n",
      "epoch no.2 train no.204360  loss = 3.88813 avg_loss = 3.75111\n",
      "epoch no.2 train no.204370  loss = 2.57597 avg_loss = 3.71785\n",
      "epoch no.2 train no.204380  loss = 2.58421 avg_loss = 3.72721\n",
      "epoch no.2 train no.204390  loss = 2.74183 avg_loss = 3.65326\n",
      "epoch no.2 train no.204400  loss = 4.91559 avg_loss = 3.65103\n",
      "epoch no.2 train no.204410  loss = 3.17310 avg_loss = 3.61802\n",
      "epoch no.2 train no.204420  loss = 4.80716 avg_loss = 3.63399\n",
      "epoch no.2 train no.204430  loss = 3.61241 avg_loss = 3.62759\n",
      "epoch no.2 train no.204440  loss = 7.32071 avg_loss = 3.74218\n",
      "epoch no.2 train no.204450  loss = 3.98185 avg_loss = 3.70851\n",
      "epoch no.2 train no.204460  loss = 2.45820 avg_loss = 3.69086\n",
      "epoch no.2 train no.204470  loss = 2.16967 avg_loss = 3.65738\n",
      "epoch no.2 train no.204480  loss = 3.01282 avg_loss = 3.61801\n",
      "epoch no.2 train no.204490  loss = 3.28090 avg_loss = 3.63759\n",
      "epoch no.2 train no.204500  loss = 2.05118 avg_loss = 3.62115\n",
      "epoch no.2 train no.204510  loss = 3.52748 avg_loss = 3.62359\n",
      "epoch no.2 train no.204520  loss = 3.02919 avg_loss = 3.63951\n",
      "epoch no.2 train no.204530  loss = 2.68405 avg_loss = 3.62164\n",
      "epoch no.2 train no.204540  loss = 3.67749 avg_loss = 3.62612\n",
      "epoch no.2 train no.204550  loss = 4.52401 avg_loss = 3.63278\n",
      "epoch no.2 train no.204560  loss = 2.59487 avg_loss = 3.69111\n",
      "epoch no.2 train no.204570  loss = 4.30363 avg_loss = 3.66851\n",
      "epoch no.2 train no.204580  loss = 4.73797 avg_loss = 3.68092\n",
      "epoch no.2 train no.204590  loss = 3.75278 avg_loss = 3.70328\n",
      "epoch no.2 train no.204600  loss = 3.66556 avg_loss = 3.68317\n",
      "epoch no.2 train no.204610  loss = 4.54072 avg_loss = 3.68206\n",
      "epoch no.2 train no.204620  loss = 3.06516 avg_loss = 3.70241\n",
      "epoch no.2 train no.204630  loss = 2.89127 avg_loss = 3.69302\n",
      "epoch no.2 train no.204640  loss = 2.48583 avg_loss = 3.67123\n",
      "epoch no.2 train no.204650  loss = 3.16497 avg_loss = 3.72257\n",
      "epoch no.2 train no.204660  loss = 5.77605 avg_loss = 3.78403\n",
      "epoch no.2 train no.204670  loss = 2.47775 avg_loss = 3.76689\n",
      "epoch no.2 train no.204680  loss = 4.00528 avg_loss = 3.78385\n",
      "epoch no.2 train no.204690  loss = 4.42393 avg_loss = 3.76810\n",
      "epoch no.2 train no.204700  loss = 3.65650 avg_loss = 3.70296\n",
      "epoch no.2 train no.204710  loss = 3.49702 avg_loss = 3.64820\n",
      "epoch no.2 train no.204720  loss = 3.37716 avg_loss = 3.66608\n",
      "epoch no.2 train no.204730  loss = 3.55807 avg_loss = 3.67632\n",
      "epoch no.2 train no.204740  loss = 3.51964 avg_loss = 3.68936\n",
      "epoch no.2 train no.204750  loss = 6.13702 avg_loss = 3.70357\n",
      "epoch no.2 train no.204760  loss = 4.00105 avg_loss = 3.69750\n",
      "epoch no.2 train no.204770  loss = 4.15744 avg_loss = 3.68478\n",
      "epoch no.2 train no.204780  loss = 3.75453 avg_loss = 3.70443\n",
      "epoch no.2 train no.204790  loss = 4.66807 avg_loss = 3.72226\n",
      "epoch no.2 train no.204800  loss = 2.62925 avg_loss = 3.73552\n",
      "epoch no.2 train no.204810  loss = 3.07050 avg_loss = 3.70090\n",
      "epoch no.2 train no.204820  loss = 1.98665 avg_loss = 3.68482\n",
      "epoch no.2 train no.204830  loss = 5.85064 avg_loss = 3.71295\n",
      "epoch no.2 train no.204840  loss = 2.52900 avg_loss = 3.71540\n",
      "epoch no.2 train no.204850  loss = 4.08740 avg_loss = 3.69767\n",
      "epoch no.2 train no.204860  loss = 2.94778 avg_loss = 3.69304\n",
      "epoch no.2 train no.204870  loss = 3.02531 avg_loss = 3.69357\n",
      "epoch no.2 train no.204880  loss = 3.14354 avg_loss = 3.68031\n",
      "epoch no.2 train no.204890  loss = 4.03944 avg_loss = 3.67577\n",
      "epoch no.2 train no.204900  loss = 6.22353 avg_loss = 3.74979\n",
      "epoch no.2 train no.204910  loss = 5.02893 avg_loss = 3.66563\n",
      "epoch no.2 train no.204920  loss = 3.10702 avg_loss = 3.68093\n",
      "epoch no.2 train no.204930  loss = 5.19260 avg_loss = 3.67440\n",
      "epoch no.2 train no.204940  loss = 2.32683 avg_loss = 3.65995\n",
      "epoch no.2 train no.204950  loss = 4.06128 avg_loss = 3.64408\n",
      "epoch no.2 train no.204960  loss = 3.82903 avg_loss = 3.63213\n",
      "epoch no.2 train no.204970  loss = 3.25521 avg_loss = 3.63594\n",
      "epoch no.2 train no.204980  loss = 2.98768 avg_loss = 3.66520\n",
      "epoch no.2 train no.204990  loss = 3.72406 avg_loss = 3.73083\n",
      "epoch no.2 train no.205000  loss = 4.97553 avg_loss = 3.65483\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁위한', '▁뉴', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.2 train no.205010  loss = 3.17575 avg_loss = 3.66375\n",
      "epoch no.2 train no.205020  loss = 3.81685 avg_loss = 3.64702\n",
      "epoch no.2 train no.205030  loss = 3.71351 avg_loss = 3.60434\n",
      "epoch no.2 train no.205040  loss = 2.45477 avg_loss = 3.59121\n",
      "epoch no.2 train no.205050  loss = 4.28759 avg_loss = 3.59095\n",
      "epoch no.2 train no.205060  loss = 2.74372 avg_loss = 3.56950\n",
      "epoch no.2 train no.205070  loss = 2.61694 avg_loss = 3.57380\n",
      "epoch no.2 train no.205080  loss = 2.58566 avg_loss = 3.52016\n",
      "epoch no.2 train no.205090  loss = 3.62566 avg_loss = 3.51327\n",
      "epoch no.2 train no.205100  loss = 1.91094 avg_loss = 3.50540\n",
      "epoch no.2 train no.205110  loss = 5.32785 avg_loss = 3.53358\n",
      "epoch no.2 train no.205120  loss = 4.63812 avg_loss = 3.52535\n",
      "epoch no.2 train no.205130  loss = 3.51879 avg_loss = 3.55202\n",
      "epoch no.2 train no.205140  loss = 2.55650 avg_loss = 3.55805\n",
      "epoch no.2 train no.205150  loss = 2.74629 avg_loss = 3.55984\n",
      "epoch no.2 train no.205160  loss = 7.28249 avg_loss = 3.57507\n",
      "epoch no.2 train no.205170  loss = 3.23960 avg_loss = 3.57460\n",
      "epoch no.2 train no.205180  loss = 5.77946 avg_loss = 3.56339\n",
      "epoch no.2 train no.205190  loss = 2.03993 avg_loss = 3.54708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.205200  loss = 4.08137 avg_loss = 3.51438\n",
      "epoch no.2 train no.205210  loss = 5.07929 avg_loss = 3.53079\n",
      "epoch no.2 train no.205220  loss = 2.50730 avg_loss = 3.60009\n",
      "epoch no.2 train no.205230  loss = 3.60853 avg_loss = 3.56695\n",
      "epoch no.2 train no.205240  loss = 2.82251 avg_loss = 3.49851\n",
      "epoch no.2 train no.205250  loss = 2.46116 avg_loss = 3.55023\n",
      "epoch no.2 train no.205260  loss = 3.43039 avg_loss = 3.57558\n",
      "epoch no.2 train no.205270  loss = 3.00283 avg_loss = 3.52537\n",
      "epoch no.2 train no.205280  loss = 3.51557 avg_loss = 3.56405\n",
      "epoch no.2 train no.205290  loss = 3.60663 avg_loss = 3.55100\n",
      "epoch no.2 train no.205300  loss = 3.67409 avg_loss = 3.57695\n",
      "epoch no.2 train no.205310  loss = 3.45614 avg_loss = 3.58378\n",
      "epoch no.2 train no.205320  loss = 2.80870 avg_loss = 3.60411\n",
      "epoch no.2 train no.205330  loss = 2.18455 avg_loss = 3.52510\n",
      "epoch no.2 train no.205340  loss = 3.48519 avg_loss = 3.50444\n",
      "epoch no.2 train no.205350  loss = 4.57630 avg_loss = 3.50414\n",
      "epoch no.2 train no.205360  loss = 3.75097 avg_loss = 3.50844\n",
      "epoch no.2 train no.205370  loss = 2.88293 avg_loss = 3.46975\n",
      "epoch no.2 train no.205380  loss = 2.79070 avg_loss = 3.48274\n",
      "epoch no.2 train no.205390  loss = 5.71799 avg_loss = 3.52410\n",
      "epoch no.2 train no.205400  loss = 4.28941 avg_loss = 3.56346\n",
      "epoch no.2 train no.205410  loss = 2.47731 avg_loss = 3.54696\n",
      "epoch no.2 train no.205420  loss = 3.45518 avg_loss = 3.53831\n",
      "epoch no.2 train no.205430  loss = 4.07089 avg_loss = 3.54442\n",
      "epoch no.2 train no.205440  loss = 2.58684 avg_loss = 3.48808\n",
      "epoch no.2 train no.205450  loss = 4.98173 avg_loss = 3.56613\n",
      "epoch no.2 train no.205460  loss = 3.68950 avg_loss = 3.59380\n",
      "epoch no.2 train no.205470  loss = 3.62658 avg_loss = 3.58964\n",
      "epoch no.2 train no.205480  loss = 2.91434 avg_loss = 3.54220\n",
      "epoch no.2 train no.205490  loss = 3.60388 avg_loss = 3.58105\n",
      "epoch no.2 train no.205500  loss = 3.62788 avg_loss = 3.56358\n",
      "epoch no.2 train no.205510  loss = 3.30241 avg_loss = 3.56169\n",
      "epoch no.2 train no.205520  loss = 4.45647 avg_loss = 3.65103\n",
      "epoch no.2 train no.205530  loss = 6.18498 avg_loss = 3.64620\n",
      "epoch no.2 train no.205540  loss = 3.24200 avg_loss = 3.65472\n",
      "epoch no.2 train no.205550  loss = 1.85646 avg_loss = 3.64939\n",
      "epoch no.2 train no.205560  loss = 2.89210 avg_loss = 3.63625\n",
      "epoch no.2 train no.205570  loss = 5.22596 avg_loss = 3.61214\n",
      "epoch no.2 train no.205580  loss = 2.88033 avg_loss = 3.56605\n",
      "epoch no.2 train no.205590  loss = 3.71193 avg_loss = 3.57473\n",
      "epoch no.2 train no.205600  loss = 5.37512 avg_loss = 3.59503\n",
      "epoch no.2 train no.205610  loss = 4.85673 avg_loss = 3.67044\n",
      "epoch no.2 train no.205620  loss = 3.29004 avg_loss = 3.69846\n",
      "epoch no.2 train no.205630  loss = 3.86966 avg_loss = 3.74072\n",
      "epoch no.2 train no.205640  loss = 4.15763 avg_loss = 3.78076\n",
      "epoch no.2 train no.205650  loss = 4.04521 avg_loss = 3.73490\n",
      "epoch no.2 train no.205660  loss = 2.94835 avg_loss = 3.72828\n",
      "epoch no.2 train no.205670  loss = 3.86407 avg_loss = 3.69018\n",
      "epoch no.2 train no.205680  loss = 2.70707 avg_loss = 3.67202\n",
      "epoch no.2 train no.205690  loss = 3.95363 avg_loss = 3.69561\n",
      "epoch no.2 train no.205700  loss = 3.02982 avg_loss = 3.65070\n",
      "epoch no.2 train no.205710  loss = 3.17845 avg_loss = 3.64820\n",
      "epoch no.2 train no.205720  loss = 4.08481 avg_loss = 3.68519\n",
      "epoch no.2 train no.205730  loss = 3.07642 avg_loss = 3.71178\n",
      "epoch no.2 train no.205740  loss = 4.78150 avg_loss = 3.70047\n",
      "epoch no.2 train no.205750  loss = 3.74403 avg_loss = 3.67513\n",
      "epoch no.2 train no.205760  loss = 4.75101 avg_loss = 3.67582\n",
      "epoch no.2 train no.205770  loss = 3.83483 avg_loss = 3.72341\n",
      "epoch no.2 train no.205780  loss = 2.45138 avg_loss = 3.75703\n",
      "epoch no.2 train no.205790  loss = 3.00544 avg_loss = 3.75328\n",
      "epoch no.2 train no.205800  loss = 2.87721 avg_loss = 3.78954\n",
      "epoch no.2 train no.205810  loss = 2.70600 avg_loss = 3.75308\n",
      "epoch no.2 train no.205820  loss = 3.16181 avg_loss = 3.68293\n",
      "epoch no.2 train no.205830  loss = 4.21926 avg_loss = 3.68254\n",
      "epoch no.2 train no.205840  loss = 3.82937 avg_loss = 3.72193\n",
      "epoch no.2 train no.205850  loss = 2.88149 avg_loss = 3.71123\n",
      "epoch no.2 train no.205860  loss = 5.53249 avg_loss = 3.70190\n",
      "epoch no.2 train no.205870  loss = 2.17400 avg_loss = 3.66247\n",
      "epoch no.2 train no.205880  loss = 4.00356 avg_loss = 3.65794\n",
      "epoch no.2 train no.205890  loss = 2.21049 avg_loss = 3.63459\n",
      "epoch no.2 train no.205900  loss = 4.09125 avg_loss = 3.64924\n",
      "epoch no.2 train no.205910  loss = 2.26519 avg_loss = 3.57177\n",
      "epoch no.2 train no.205920  loss = 2.28875 avg_loss = 3.57975\n",
      "epoch no.2 train no.205930  loss = 4.13810 avg_loss = 3.63852\n",
      "epoch no.2 train no.205940  loss = 4.21137 avg_loss = 3.60076\n",
      "epoch no.2 train no.205950  loss = 1.94156 avg_loss = 3.58933\n",
      "epoch no.2 train no.205960  loss = 4.11627 avg_loss = 3.57343\n",
      "epoch no.2 train no.205970  loss = 3.01156 avg_loss = 3.58159\n",
      "epoch no.2 train no.205980  loss = 2.62486 avg_loss = 3.53861\n",
      "epoch no.2 train no.205990  loss = 2.90832 avg_loss = 3.56933\n",
      "epoch no.2 train no.206000  loss = 4.90961 avg_loss = 3.63396\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '을', '▁위한', '▁플레이', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.2 train no.206010  loss = 3.71627 avg_loss = 3.59395\n",
      "epoch no.2 train no.206020  loss = 3.78182 avg_loss = 3.57723\n",
      "epoch no.2 train no.206030  loss = 4.81640 avg_loss = 3.55690\n",
      "epoch no.2 train no.206040  loss = 5.19277 avg_loss = 3.58066\n",
      "epoch no.2 train no.206050  loss = 3.50776 avg_loss = 3.61731\n",
      "epoch no.2 train no.206060  loss = 2.96324 avg_loss = 3.61457\n",
      "epoch no.2 train no.206070  loss = 3.89495 avg_loss = 3.60208\n",
      "epoch no.2 train no.206080  loss = 4.00313 avg_loss = 3.53924\n",
      "epoch no.2 train no.206090  loss = 4.03823 avg_loss = 3.54333\n",
      "epoch no.2 train no.206100  loss = 3.61011 avg_loss = 3.59216\n",
      "epoch no.2 train no.206110  loss = 3.20906 avg_loss = 3.59952\n",
      "epoch no.2 train no.206120  loss = 1.48871 avg_loss = 3.56424\n",
      "epoch no.2 train no.206130  loss = 2.75143 avg_loss = 3.52299\n",
      "epoch no.2 train no.206140  loss = 2.75396 avg_loss = 3.54948\n",
      "epoch no.2 train no.206150  loss = 4.82840 avg_loss = 3.57294\n",
      "epoch no.2 train no.206160  loss = 3.95498 avg_loss = 3.54150\n",
      "epoch no.2 train no.206170  loss = 4.65312 avg_loss = 3.52298\n",
      "epoch no.2 train no.206180  loss = 4.82628 avg_loss = 3.56544\n",
      "epoch no.2 train no.206190  loss = 4.79529 avg_loss = 3.54262\n",
      "epoch no.2 train no.206200  loss = 3.11878 avg_loss = 3.54032\n",
      "epoch no.2 train no.206210  loss = 2.79446 avg_loss = 3.52465\n",
      "epoch no.2 train no.206220  loss = 2.56357 avg_loss = 3.49284\n",
      "epoch no.2 train no.206230  loss = 3.50701 avg_loss = 3.47884\n",
      "epoch no.2 train no.206240  loss = 1.97360 avg_loss = 3.47968\n",
      "epoch no.2 train no.206250  loss = 3.13677 avg_loss = 3.48627\n",
      "epoch no.2 train no.206260  loss = 7.65584 avg_loss = 3.55924\n",
      "epoch no.2 train no.206270  loss = 3.78115 avg_loss = 3.52826\n",
      "epoch no.2 train no.206280  loss = 2.82711 avg_loss = 3.57156\n",
      "epoch no.2 train no.206290  loss = 3.24045 avg_loss = 3.56259\n",
      "epoch no.2 train no.206300  loss = 5.15812 avg_loss = 3.56776\n",
      "epoch no.2 train no.206310  loss = 2.90828 avg_loss = 3.56509\n",
      "epoch no.2 train no.206320  loss = 4.32397 avg_loss = 3.66899\n",
      "epoch no.2 train no.206330  loss = 5.36369 avg_loss = 3.69097\n",
      "epoch no.2 train no.206340  loss = 4.27093 avg_loss = 3.67293\n",
      "epoch no.2 train no.206350  loss = 4.72781 avg_loss = 3.64267\n",
      "epoch no.2 train no.206360  loss = 4.79118 avg_loss = 3.65476\n",
      "epoch no.2 train no.206370  loss = 2.61242 avg_loss = 3.61249\n",
      "epoch no.2 train no.206380  loss = 2.70005 avg_loss = 3.64868\n",
      "epoch no.2 train no.206390  loss = 4.40634 avg_loss = 3.69795\n",
      "epoch no.2 train no.206400  loss = 4.28505 avg_loss = 3.73050\n",
      "epoch no.2 train no.206410  loss = 2.62461 avg_loss = 3.70454\n",
      "epoch no.2 train no.206420  loss = 3.64238 avg_loss = 3.66154\n",
      "epoch no.2 train no.206430  loss = 4.98571 avg_loss = 3.67527\n",
      "epoch no.2 train no.206440  loss = 2.55686 avg_loss = 3.63614\n",
      "epoch no.2 train no.206450  loss = 4.40559 avg_loss = 3.64646\n",
      "epoch no.2 train no.206460  loss = 3.78421 avg_loss = 3.63162\n",
      "epoch no.2 train no.206470  loss = 2.75007 avg_loss = 3.61465\n",
      "epoch no.2 train no.206480  loss = 2.65686 avg_loss = 3.59894\n",
      "epoch no.2 train no.206490  loss = 3.79452 avg_loss = 3.59546\n",
      "epoch no.2 train no.206500  loss = 3.70637 avg_loss = 3.59370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.206510  loss = 3.92857 avg_loss = 3.59311\n",
      "epoch no.2 train no.206520  loss = 3.72632 avg_loss = 3.59536\n",
      "epoch no.2 train no.206530  loss = 2.54589 avg_loss = 3.61830\n",
      "epoch no.2 train no.206540  loss = 4.68791 avg_loss = 3.61275\n",
      "epoch no.2 train no.206550  loss = 3.40372 avg_loss = 3.59152\n",
      "epoch no.2 train no.206560  loss = 3.28093 avg_loss = 3.56409\n",
      "epoch no.2 train no.206570  loss = 5.16110 avg_loss = 3.59181\n",
      "epoch no.2 train no.206580  loss = 4.38882 avg_loss = 3.65703\n",
      "epoch no.2 train no.206590  loss = 5.05800 avg_loss = 3.64750\n",
      "epoch no.2 train no.206600  loss = 2.85706 avg_loss = 3.68664\n",
      "epoch no.2 train no.206610  loss = 3.96111 avg_loss = 3.71987\n",
      "epoch no.2 train no.206620  loss = 2.30732 avg_loss = 3.66908\n",
      "epoch no.2 train no.206630  loss = 3.06679 avg_loss = 3.65077\n",
      "epoch no.2 train no.206640  loss = 2.49745 avg_loss = 3.65282\n",
      "epoch no.2 train no.206650  loss = 2.06982 avg_loss = 3.60923\n",
      "epoch no.2 train no.206660  loss = 2.73177 avg_loss = 3.59154\n",
      "epoch no.2 train no.206670  loss = 2.67147 avg_loss = 3.60673\n",
      "epoch no.2 train no.206680  loss = 3.66796 avg_loss = 3.63421\n",
      "epoch no.2 train no.206690  loss = 4.66984 avg_loss = 3.67726\n",
      "epoch no.2 train no.206700  loss = 2.18796 avg_loss = 3.64871\n",
      "epoch no.2 train no.206710  loss = 4.74540 avg_loss = 3.67386\n",
      "epoch no.2 train no.206720  loss = 4.78470 avg_loss = 3.65591\n",
      "epoch no.2 train no.206730  loss = 3.80884 avg_loss = 3.60725\n",
      "epoch no.2 train no.206740  loss = 4.35869 avg_loss = 3.61731\n",
      "epoch no.2 train no.206750  loss = 2.85990 avg_loss = 3.60185\n",
      "epoch no.2 train no.206760  loss = 3.27676 avg_loss = 3.62883\n",
      "epoch no.2 train no.206770  loss = 2.26434 avg_loss = 3.58969\n",
      "epoch no.2 train no.206780  loss = 1.88820 avg_loss = 3.58175\n",
      "epoch no.2 train no.206790  loss = 2.95020 avg_loss = 3.57200\n",
      "epoch no.2 train no.206800  loss = 3.06901 avg_loss = 3.56475\n",
      "epoch no.2 train no.206810  loss = 4.47639 avg_loss = 3.60535\n",
      "epoch no.2 train no.206820  loss = 3.93202 avg_loss = 3.59733\n",
      "epoch no.2 train no.206830  loss = 2.97404 avg_loss = 3.60740\n",
      "epoch no.2 train no.206840  loss = 3.15637 avg_loss = 3.57877\n",
      "epoch no.2 train no.206850  loss = 2.75261 avg_loss = 3.55873\n",
      "epoch no.2 train no.206860  loss = 4.17763 avg_loss = 3.60072\n",
      "epoch no.2 train no.206870  loss = 2.44203 avg_loss = 3.55079\n",
      "epoch no.2 train no.206880  loss = 2.24301 avg_loss = 3.53322\n",
      "epoch no.2 train no.206890  loss = 4.69754 avg_loss = 3.57399\n",
      "epoch no.2 train no.206900  loss = 2.93930 avg_loss = 3.60825\n",
      "epoch no.2 train no.206910  loss = 4.94425 avg_loss = 3.66529\n",
      "epoch no.2 train no.206920  loss = 4.05365 avg_loss = 3.64461\n",
      "epoch no.2 train no.206930  loss = 3.26152 avg_loss = 3.62977\n",
      "epoch no.2 train no.206940  loss = 3.23967 avg_loss = 3.58286\n",
      "epoch no.2 train no.206950  loss = 2.59334 avg_loss = 3.54006\n",
      "epoch no.2 train no.206960  loss = 4.56422 avg_loss = 3.57209\n",
      "epoch no.2 train no.206970  loss = 2.55600 avg_loss = 3.52271\n",
      "epoch no.2 train no.206980  loss = 3.96515 avg_loss = 3.55968\n",
      "epoch no.2 train no.206990  loss = 2.75216 avg_loss = 3.59879\n",
      "epoch no.2 train no.207000  loss = 3.77565 avg_loss = 3.64201\n",
      "5\n",
      "to_tokens: ['▁라디오', '▁좋은', '에', '싶', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환하고 싶을때 듣는 노래</s>\n",
      "epoch no.2 train no.207010  loss = 2.26439 avg_loss = 3.65285\n",
      "epoch no.2 train no.207020  loss = 2.84278 avg_loss = 3.68569\n",
      "epoch no.2 train no.207030  loss = 2.68023 avg_loss = 3.67132\n",
      "epoch no.2 train no.207040  loss = 2.79936 avg_loss = 3.67835\n",
      "epoch no.2 train no.207050  loss = 0.92082 avg_loss = 3.71604\n",
      "epoch no.2 train no.207060  loss = 3.87924 avg_loss = 3.69169\n",
      "epoch no.2 train no.207070  loss = 2.68431 avg_loss = 3.69795\n",
      "epoch no.2 train no.207080  loss = 6.80778 avg_loss = 3.71829\n",
      "epoch no.2 train no.207090  loss = 3.53263 avg_loss = 3.71735\n",
      "epoch no.2 train no.207100  loss = 2.88275 avg_loss = 3.67768\n",
      "epoch no.2 train no.207110  loss = 3.45160 avg_loss = 3.69702\n",
      "epoch no.2 train no.207120  loss = 3.11253 avg_loss = 3.65757\n",
      "epoch no.2 train no.207130  loss = 2.73392 avg_loss = 3.56150\n",
      "epoch no.2 train no.207140  loss = 4.77803 avg_loss = 3.57498\n",
      "epoch no.2 train no.207150  loss = 4.91286 avg_loss = 3.61990\n",
      "epoch no.2 train no.207160  loss = 3.01065 avg_loss = 3.65056\n",
      "epoch no.2 train no.207170  loss = 3.56723 avg_loss = 3.71900\n",
      "epoch no.2 train no.207180  loss = 4.65181 avg_loss = 3.68241\n",
      "epoch no.2 train no.207190  loss = 4.42197 avg_loss = 3.67597\n",
      "epoch no.2 train no.207200  loss = 4.97107 avg_loss = 3.68042\n",
      "epoch no.2 train no.207210  loss = 3.34298 avg_loss = 3.64200\n",
      "epoch no.2 train no.207220  loss = 7.66883 avg_loss = 3.72692\n",
      "epoch no.2 train no.207230  loss = 2.77241 avg_loss = 3.68946\n",
      "epoch no.2 train no.207240  loss = 3.10454 avg_loss = 3.62634\n",
      "epoch no.2 train no.207250  loss = 3.44967 avg_loss = 3.64310\n",
      "epoch no.2 train no.207260  loss = 3.68358 avg_loss = 3.59830\n",
      "epoch no.2 train no.207270  loss = 3.72817 avg_loss = 3.59340\n",
      "epoch no.2 train no.207280  loss = 5.75779 avg_loss = 3.57594\n",
      "epoch no.2 train no.207290  loss = 4.99023 avg_loss = 3.57780\n",
      "epoch no.2 train no.207300  loss = 5.94361 avg_loss = 3.54798\n",
      "epoch no.2 train no.207310  loss = 4.21198 avg_loss = 3.56200\n",
      "epoch no.2 train no.207320  loss = 2.17710 avg_loss = 3.57994\n",
      "epoch no.2 train no.207330  loss = 3.63514 avg_loss = 3.56950\n",
      "epoch no.2 train no.207340  loss = 4.72494 avg_loss = 3.57160\n",
      "epoch no.2 train no.207350  loss = 5.28325 avg_loss = 3.52866\n",
      "epoch no.2 train no.207360  loss = 5.03870 avg_loss = 3.52312\n",
      "epoch no.2 train no.207370  loss = 4.15664 avg_loss = 3.53519\n",
      "epoch no.2 train no.207380  loss = 1.86961 avg_loss = 3.49364\n",
      "epoch no.2 train no.207390  loss = 6.42950 avg_loss = 3.51866\n",
      "epoch no.2 train no.207400  loss = 3.27441 avg_loss = 3.46727\n",
      "epoch no.2 train no.207410  loss = 4.33053 avg_loss = 3.47223\n",
      "epoch no.2 train no.207420  loss = 2.30410 avg_loss = 3.46114\n",
      "epoch no.2 train no.207430  loss = 5.02325 avg_loss = 3.48790\n",
      "epoch no.2 train no.207440  loss = 2.77232 avg_loss = 3.51484\n",
      "epoch no.2 train no.207450  loss = 4.50889 avg_loss = 3.52009\n",
      "epoch no.2 train no.207460  loss = 3.30473 avg_loss = 3.53690\n",
      "epoch no.2 train no.207470  loss = 4.09556 avg_loss = 3.53394\n",
      "epoch no.2 train no.207480  loss = 5.08404 avg_loss = 3.52966\n",
      "epoch no.2 train no.207490  loss = 3.00635 avg_loss = 3.51928\n",
      "epoch no.2 train no.207500  loss = 2.26090 avg_loss = 3.51298\n",
      "epoch no.2 train no.207510  loss = 4.35128 avg_loss = 3.52484\n",
      "epoch no.2 train no.207520  loss = 3.36827 avg_loss = 3.60217\n",
      "epoch no.2 train no.207530  loss = 2.52008 avg_loss = 3.58325\n",
      "epoch no.2 train no.207540  loss = 3.92941 avg_loss = 3.66582\n",
      "epoch no.2 train no.207550  loss = 1.83361 avg_loss = 3.62964\n",
      "epoch no.2 train no.207560  loss = 2.51415 avg_loss = 3.65997\n",
      "epoch no.2 train no.207570  loss = 2.98737 avg_loss = 3.65833\n",
      "epoch no.2 train no.207580  loss = 4.16059 avg_loss = 3.69844\n",
      "epoch no.2 train no.207590  loss = 3.59928 avg_loss = 3.67606\n",
      "epoch no.2 train no.207600  loss = 2.85296 avg_loss = 3.66716\n",
      "epoch no.2 train no.207610  loss = 2.35552 avg_loss = 3.67445\n",
      "epoch no.2 train no.207620  loss = 3.44189 avg_loss = 3.70384\n",
      "epoch no.2 train no.207630  loss = 5.60048 avg_loss = 3.72171\n",
      "epoch no.2 train no.207640  loss = 4.89050 avg_loss = 3.71231\n",
      "epoch no.2 train no.207650  loss = 4.40755 avg_loss = 3.71969\n",
      "epoch no.2 train no.207660  loss = 2.10256 avg_loss = 3.63011\n",
      "epoch no.2 train no.207670  loss = 3.20773 avg_loss = 3.61600\n",
      "epoch no.2 train no.207680  loss = 3.44933 avg_loss = 3.66812\n",
      "epoch no.2 train no.207690  loss = 4.58609 avg_loss = 3.68968\n",
      "epoch no.2 train no.207700  loss = 1.94321 avg_loss = 3.74605\n",
      "epoch no.2 train no.207710  loss = 2.37868 avg_loss = 3.70160\n",
      "epoch no.2 train no.207720  loss = 2.35106 avg_loss = 3.65916\n",
      "epoch no.2 train no.207730  loss = 1.33061 avg_loss = 3.64518\n",
      "epoch no.2 train no.207740  loss = 3.87085 avg_loss = 3.61379\n",
      "epoch no.2 train no.207750  loss = 2.93095 avg_loss = 3.62848\n",
      "epoch no.2 train no.207760  loss = 4.80765 avg_loss = 3.59508\n",
      "epoch no.2 train no.207770  loss = 5.85460 avg_loss = 3.60163\n",
      "epoch no.2 train no.207780  loss = 2.80749 avg_loss = 3.59759\n",
      "epoch no.2 train no.207790  loss = 3.52288 avg_loss = 3.61652\n",
      "epoch no.2 train no.207800  loss = 2.99173 avg_loss = 3.64334\n",
      "epoch no.2 train no.207810  loss = 2.54918 avg_loss = 3.64089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.207820  loss = 3.81751 avg_loss = 3.62678\n",
      "epoch no.2 train no.207830  loss = 2.56126 avg_loss = 3.63132\n",
      "epoch no.2 train no.207840  loss = 3.78582 avg_loss = 3.65075\n",
      "epoch no.2 train no.207850  loss = 3.71841 avg_loss = 3.62144\n",
      "epoch no.2 train no.207860  loss = 3.27853 avg_loss = 3.65748\n",
      "epoch no.2 train no.207870  loss = 4.72200 avg_loss = 3.63285\n",
      "epoch no.2 train no.207880  loss = 2.16989 avg_loss = 3.58416\n",
      "epoch no.2 train no.207890  loss = 4.01471 avg_loss = 3.54215\n",
      "epoch no.2 train no.207900  loss = 3.25780 avg_loss = 3.54493\n",
      "epoch no.2 train no.207910  loss = 4.02896 avg_loss = 3.53253\n",
      "epoch no.2 train no.207920  loss = 3.95393 avg_loss = 3.56481\n",
      "epoch no.2 train no.207930  loss = 5.02222 avg_loss = 3.53092\n",
      "epoch no.2 train no.207940  loss = 2.25713 avg_loss = 3.54233\n",
      "epoch no.2 train no.207950  loss = 3.92647 avg_loss = 3.58084\n",
      "epoch no.2 train no.207960  loss = 5.63327 avg_loss = 3.59683\n",
      "epoch no.2 train no.207970  loss = 4.61908 avg_loss = 3.66556\n",
      "epoch no.2 train no.207980  loss = 9.00112 avg_loss = 3.67019\n",
      "epoch no.2 train no.207990  loss = 3.44132 avg_loss = 3.69071\n",
      "epoch no.2 train no.208000  loss = 2.41981 avg_loss = 3.63355\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.2 train no.208010  loss = 5.69465 avg_loss = 3.65541\n",
      "epoch no.2 train no.208020  loss = 4.91152 avg_loss = 3.66602\n",
      "epoch no.2 train no.208030  loss = 4.55495 avg_loss = 3.65669\n",
      "epoch no.2 train no.208040  loss = 3.09488 avg_loss = 3.67165\n",
      "epoch no.2 train no.208050  loss = 2.47180 avg_loss = 3.66147\n",
      "epoch no.2 train no.208060  loss = 3.60989 avg_loss = 3.60320\n",
      "epoch no.2 train no.208070  loss = 4.09724 avg_loss = 3.56890\n",
      "epoch no.2 train no.208080  loss = 4.34711 avg_loss = 3.56538\n",
      "epoch no.2 train no.208090  loss = 3.10218 avg_loss = 3.57431\n",
      "epoch no.2 train no.208100  loss = 3.67889 avg_loss = 3.54718\n",
      "epoch no.2 train no.208110  loss = 2.45670 avg_loss = 3.56655\n",
      "epoch no.2 train no.208120  loss = 2.12372 avg_loss = 3.53964\n",
      "epoch no.2 train no.208130  loss = 3.95718 avg_loss = 3.53050\n",
      "epoch no.2 train no.208140  loss = 3.24837 avg_loss = 3.54382\n",
      "epoch no.2 train no.208150  loss = 3.57105 avg_loss = 3.49830\n",
      "epoch no.2 train no.208160  loss = 3.64089 avg_loss = 3.55730\n",
      "epoch no.2 train no.208170  loss = 3.50570 avg_loss = 3.52837\n",
      "epoch no.2 train no.208180  loss = 2.77151 avg_loss = 3.50299\n",
      "epoch no.2 train no.208190  loss = 4.30114 avg_loss = 3.52785\n",
      "epoch no.2 train no.208200  loss = 3.98079 avg_loss = 3.55280\n",
      "epoch no.2 train no.208210  loss = 5.74193 avg_loss = 3.64536\n",
      "epoch no.2 train no.208220  loss = 3.13053 avg_loss = 3.63931\n",
      "epoch no.2 train no.208230  loss = 3.14417 avg_loss = 3.60681\n",
      "epoch no.2 train no.208240  loss = 3.52124 avg_loss = 3.62379\n",
      "epoch no.2 train no.208250  loss = 3.43163 avg_loss = 3.59545\n",
      "epoch no.2 train no.208260  loss = 3.67131 avg_loss = 3.59031\n",
      "epoch no.2 train no.208270  loss = 4.27030 avg_loss = 3.56576\n",
      "epoch no.2 train no.208280  loss = 3.71790 avg_loss = 3.55487\n",
      "epoch no.2 train no.208290  loss = 2.98817 avg_loss = 3.58060\n",
      "epoch no.2 train no.208300  loss = 4.69851 avg_loss = 3.54422\n",
      "epoch no.2 train no.208310  loss = 3.92452 avg_loss = 3.57525\n",
      "epoch no.2 train no.208320  loss = 3.80178 avg_loss = 3.56007\n",
      "epoch no.2 train no.208330  loss = 1.92848 avg_loss = 3.57369\n",
      "epoch no.2 train no.208340  loss = 3.87956 avg_loss = 3.59111\n",
      "epoch no.2 train no.208350  loss = 2.11165 avg_loss = 3.54973\n",
      "epoch no.2 train no.208360  loss = 2.67470 avg_loss = 3.56293\n",
      "epoch no.2 train no.208370  loss = 2.97667 avg_loss = 3.57258\n",
      "epoch no.2 train no.208380  loss = 2.55740 avg_loss = 3.53499\n",
      "epoch no.2 train no.208390  loss = 4.69862 avg_loss = 3.56405\n",
      "epoch no.2 train no.208400  loss = 3.68543 avg_loss = 3.53165\n",
      "epoch no.2 train no.208410  loss = 3.32539 avg_loss = 3.51492\n",
      "epoch no.2 train no.208420  loss = 3.40531 avg_loss = 3.47304\n",
      "epoch no.2 train no.208430  loss = 3.57959 avg_loss = 3.45585\n",
      "epoch no.2 train no.208440  loss = 4.14560 avg_loss = 3.48203\n",
      "epoch no.2 train no.208450  loss = 2.67154 avg_loss = 3.51144\n",
      "epoch no.2 train no.208460  loss = 7.21434 avg_loss = 3.56471\n",
      "epoch no.2 train no.208470  loss = 3.72891 avg_loss = 3.53778\n",
      "epoch no.2 train no.208480  loss = 2.45139 avg_loss = 3.52341\n",
      "epoch no.2 train no.208490  loss = 1.93066 avg_loss = 3.47675\n",
      "epoch no.2 train no.208500  loss = 2.94918 avg_loss = 3.51971\n",
      "epoch no.2 train no.208510  loss = 3.41905 avg_loss = 3.49668\n",
      "epoch no.2 train no.208520  loss = 2.54036 avg_loss = 3.52534\n",
      "epoch no.2 train no.208530  loss = 3.63825 avg_loss = 3.54670\n",
      "epoch no.2 train no.208540  loss = 4.38538 avg_loss = 3.50232\n",
      "epoch no.2 train no.208550  loss = 2.40095 avg_loss = 3.47687\n",
      "epoch no.2 train no.208560  loss = 5.49148 avg_loss = 3.52164\n",
      "epoch no.2 train no.208570  loss = 2.59204 avg_loss = 3.50964\n",
      "epoch no.2 train no.208580  loss = 2.42843 avg_loss = 3.46323\n",
      "epoch no.2 train no.208590  loss = 2.25711 avg_loss = 3.44561\n",
      "epoch no.2 train no.208600  loss = 3.57292 avg_loss = 3.45475\n",
      "epoch no.2 train no.208610  loss = 1.95445 avg_loss = 3.42107\n",
      "epoch no.2 train no.208620  loss = 3.27097 avg_loss = 3.43037\n",
      "epoch no.2 train no.208630  loss = 3.79434 avg_loss = 3.48355\n",
      "epoch no.2 train no.208640  loss = 5.00296 avg_loss = 3.47163\n",
      "epoch no.2 train no.208650  loss = 3.50596 avg_loss = 3.44906\n",
      "epoch no.2 train no.208660  loss = 2.80723 avg_loss = 3.46798\n",
      "epoch no.2 train no.208670  loss = 2.69039 avg_loss = 3.49895\n",
      "epoch no.2 train no.208680  loss = 4.20658 avg_loss = 3.48712\n",
      "epoch no.2 train no.208690  loss = 3.63743 avg_loss = 3.49695\n",
      "epoch no.2 train no.208700  loss = 5.33478 avg_loss = 3.48318\n",
      "epoch no.2 train no.208710  loss = 3.19324 avg_loss = 3.46700\n",
      "epoch no.2 train no.208720  loss = 3.11843 avg_loss = 3.46747\n",
      "epoch no.2 train no.208730  loss = 3.87293 avg_loss = 3.40375\n",
      "epoch no.2 train no.208740  loss = 2.25352 avg_loss = 3.42031\n",
      "epoch no.2 train no.208750  loss = 3.50806 avg_loss = 3.48789\n",
      "epoch no.2 train no.208760  loss = 3.58475 avg_loss = 3.51952\n",
      "epoch no.2 train no.208770  loss = 4.44568 avg_loss = 3.52668\n",
      "epoch no.2 train no.208780  loss = 2.98010 avg_loss = 3.48574\n",
      "epoch no.2 train no.208790  loss = 3.09196 avg_loss = 3.44829\n",
      "epoch no.2 train no.208800  loss = 3.75677 avg_loss = 3.48903\n",
      "epoch no.2 train no.208810  loss = 6.84943 avg_loss = 3.46365\n",
      "epoch no.2 train no.208820  loss = 2.75194 avg_loss = 3.51070\n",
      "epoch no.2 train no.208830  loss = 3.13967 avg_loss = 3.52416\n",
      "epoch no.2 train no.208840  loss = 3.23067 avg_loss = 3.51255\n",
      "epoch no.2 train no.208850  loss = 3.35747 avg_loss = 3.50731\n",
      "epoch no.2 train no.208860  loss = 3.18079 avg_loss = 3.51301\n",
      "epoch no.2 train no.208870  loss = 4.24429 avg_loss = 3.52061\n",
      "epoch no.2 train no.208880  loss = 4.33721 avg_loss = 3.54387\n",
      "epoch no.2 train no.208890  loss = 3.62332 avg_loss = 3.58704\n",
      "epoch no.2 train no.208900  loss = 4.27631 avg_loss = 3.61264\n",
      "epoch no.2 train no.208910  loss = 2.19385 avg_loss = 3.59624\n",
      "epoch no.2 train no.208920  loss = 2.57601 avg_loss = 3.58913\n",
      "epoch no.2 train no.208930  loss = 5.07449 avg_loss = 3.61539\n",
      "epoch no.2 train no.208940  loss = 5.93984 avg_loss = 3.63762\n",
      "epoch no.2 train no.208950  loss = 2.80057 avg_loss = 3.72406\n",
      "epoch no.2 train no.208960  loss = 2.55839 avg_loss = 3.67889\n",
      "epoch no.2 train no.208970  loss = 3.51552 avg_loss = 3.62389\n",
      "epoch no.2 train no.208980  loss = 4.18370 avg_loss = 3.65364\n",
      "epoch no.2 train no.208990  loss = 3.01238 avg_loss = 3.61406\n",
      "epoch no.2 train no.209000  loss = 4.04143 avg_loss = 3.57367\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '▁때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.209010  loss = 1.23451 avg_loss = 3.56597\n",
      "epoch no.2 train no.209020  loss = 4.20607 avg_loss = 3.56936\n",
      "epoch no.2 train no.209030  loss = 3.37373 avg_loss = 3.55045\n",
      "epoch no.2 train no.209040  loss = 2.08653 avg_loss = 3.58214\n",
      "epoch no.2 train no.209050  loss = 2.04605 avg_loss = 3.56744\n",
      "epoch no.2 train no.209060  loss = 3.88505 avg_loss = 3.55600\n",
      "epoch no.2 train no.209070  loss = 2.60298 avg_loss = 3.56207\n",
      "epoch no.2 train no.209080  loss = 2.93061 avg_loss = 3.58317\n",
      "epoch no.2 train no.209090  loss = 3.84011 avg_loss = 3.63151\n",
      "epoch no.2 train no.209100  loss = 4.29496 avg_loss = 3.68011\n",
      "epoch no.2 train no.209110  loss = 3.19822 avg_loss = 3.70066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.209120  loss = 2.58267 avg_loss = 3.72592\n",
      "epoch no.2 train no.209130  loss = 6.50495 avg_loss = 3.72745\n",
      "epoch no.2 train no.209140  loss = 3.81277 avg_loss = 3.70200\n",
      "epoch no.2 train no.209150  loss = 3.44884 avg_loss = 3.71248\n",
      "epoch no.2 train no.209160  loss = 2.46767 avg_loss = 3.75597\n",
      "epoch no.2 train no.209170  loss = 4.21580 avg_loss = 3.70770\n",
      "epoch no.2 train no.209180  loss = 4.30504 avg_loss = 3.71854\n",
      "epoch no.2 train no.209190  loss = 2.98048 avg_loss = 3.68741\n",
      "epoch no.2 train no.209200  loss = 4.13946 avg_loss = 3.71966\n",
      "epoch no.2 train no.209210  loss = 3.91542 avg_loss = 3.66942\n",
      "epoch no.2 train no.209220  loss = 2.98651 avg_loss = 3.63840\n",
      "epoch no.2 train no.209230  loss = 5.56022 avg_loss = 3.70385\n",
      "epoch no.2 train no.209240  loss = 4.70441 avg_loss = 3.70095\n",
      "epoch no.2 train no.209250  loss = 2.21435 avg_loss = 3.70997\n",
      "epoch no.2 train no.209260  loss = 2.99301 avg_loss = 3.70561\n",
      "epoch no.2 train no.209270  loss = 3.93858 avg_loss = 3.77041\n",
      "epoch no.2 train no.209280  loss = 2.85061 avg_loss = 3.73667\n",
      "epoch no.2 train no.209290  loss = 3.03397 avg_loss = 3.71797\n",
      "epoch no.2 train no.209300  loss = 4.08950 avg_loss = 3.70358\n",
      "epoch no.2 train no.209310  loss = 3.97440 avg_loss = 3.68696\n",
      "epoch no.2 train no.209320  loss = 3.22524 avg_loss = 3.68841\n",
      "epoch no.2 train no.209330  loss = 3.23757 avg_loss = 3.69451\n",
      "epoch no.2 train no.209340  loss = 5.46328 avg_loss = 3.69950\n",
      "epoch no.2 train no.209350  loss = 4.38295 avg_loss = 3.67691\n",
      "epoch no.2 train no.209360  loss = 3.94199 avg_loss = 3.71230\n",
      "epoch no.2 train no.209370  loss = 2.57304 avg_loss = 3.75598\n",
      "epoch no.2 train no.209380  loss = 3.29623 avg_loss = 3.81463\n",
      "epoch no.2 train no.209390  loss = 2.56293 avg_loss = 3.79455\n",
      "epoch no.2 train no.209400  loss = 5.29902 avg_loss = 3.78690\n",
      "epoch no.2 train no.209410  loss = 3.12788 avg_loss = 3.81825\n",
      "epoch no.2 train no.209420  loss = 1.62140 avg_loss = 3.78529\n",
      "epoch no.2 train no.209430  loss = 2.78036 avg_loss = 3.77386\n",
      "epoch no.2 train no.209440  loss = 4.74527 avg_loss = 3.77492\n",
      "epoch no.2 train no.209450  loss = 2.51576 avg_loss = 3.75338\n",
      "epoch no.2 train no.209460  loss = 3.32158 avg_loss = 3.69150\n",
      "epoch no.2 train no.209470  loss = 6.43847 avg_loss = 3.75945\n",
      "epoch no.2 train no.209480  loss = 4.17368 avg_loss = 3.75622\n",
      "epoch no.2 train no.209490  loss = 2.92644 avg_loss = 3.70655\n",
      "epoch no.2 train no.209500  loss = 3.53491 avg_loss = 3.72638\n",
      "epoch no.2 train no.209510  loss = 3.45752 avg_loss = 3.72638\n",
      "epoch no.2 train no.209520  loss = 3.92855 avg_loss = 3.73402\n",
      "epoch no.2 train no.209530  loss = 2.97807 avg_loss = 3.74594\n",
      "epoch no.2 train no.209540  loss = 3.36098 avg_loss = 3.72604\n",
      "epoch no.2 train no.209550  loss = 3.65028 avg_loss = 3.68379\n",
      "epoch no.2 train no.209560  loss = 5.17419 avg_loss = 3.70524\n",
      "epoch no.2 train no.209570  loss = 3.35379 avg_loss = 3.69125\n",
      "epoch no.2 train no.209580  loss = 3.43261 avg_loss = 3.69967\n",
      "epoch no.2 train no.209590  loss = 2.98938 avg_loss = 3.71767\n",
      "epoch no.2 train no.209600  loss = 3.27097 avg_loss = 3.63999\n",
      "epoch no.2 train no.209610  loss = 3.86082 avg_loss = 3.62874\n",
      "epoch no.2 train no.209620  loss = 4.00527 avg_loss = 3.63462\n",
      "epoch no.2 train no.209630  loss = 2.15899 avg_loss = 3.58164\n",
      "epoch no.2 train no.209640  loss = 4.08237 avg_loss = 3.62014\n",
      "epoch no.2 train no.209650  loss = 2.51722 avg_loss = 3.65293\n",
      "epoch no.2 train no.209660  loss = 2.56403 avg_loss = 3.64897\n",
      "epoch no.2 train no.209670  loss = 3.08659 avg_loss = 3.62722\n",
      "epoch no.2 train no.209680  loss = 4.83616 avg_loss = 3.63777\n",
      "epoch no.2 train no.209690  loss = 5.00967 avg_loss = 3.63825\n",
      "epoch no.2 train no.209700  loss = 4.74041 avg_loss = 3.57840\n",
      "epoch no.2 train no.209710  loss = 4.08115 avg_loss = 3.58386\n",
      "epoch no.2 train no.209720  loss = 6.62168 avg_loss = 3.70657\n",
      "epoch no.2 train no.209730  loss = 2.04006 avg_loss = 3.69923\n",
      "epoch no.2 train no.209740  loss = 4.04336 avg_loss = 3.67020\n",
      "epoch no.2 train no.209750  loss = 4.45861 avg_loss = 3.69336\n",
      "epoch no.2 train no.209760  loss = 3.18254 avg_loss = 3.65835\n",
      "epoch no.2 train no.209770  loss = 2.92840 avg_loss = 3.59052\n",
      "epoch no.2 train no.209780  loss = 5.24715 avg_loss = 3.59016\n",
      "epoch no.2 train no.209790  loss = 5.87917 avg_loss = 3.60730\n",
      "epoch no.2 train no.209800  loss = 3.02479 avg_loss = 3.63707\n",
      "epoch no.2 train no.209810  loss = 3.41025 avg_loss = 3.64272\n",
      "epoch no.2 train no.209820  loss = 3.43008 avg_loss = 3.65415\n",
      "epoch no.2 train no.209830  loss = 2.93820 avg_loss = 3.60950\n",
      "epoch no.2 train no.209840  loss = 2.05610 avg_loss = 3.58938\n",
      "epoch no.2 train no.209850  loss = 3.18116 avg_loss = 3.56568\n",
      "epoch no.2 train no.209860  loss = 4.47255 avg_loss = 3.62494\n",
      "epoch no.2 train no.209870  loss = 5.44348 avg_loss = 3.67353\n",
      "epoch no.2 train no.209880  loss = 3.37967 avg_loss = 3.62560\n",
      "epoch no.2 train no.209890  loss = 7.43427 avg_loss = 3.69065\n",
      "epoch no.2 train no.209900  loss = 2.99486 avg_loss = 3.69208\n",
      "epoch no.2 train no.209910  loss = 3.03662 avg_loss = 3.73845\n",
      "epoch no.2 train no.209920  loss = 3.09524 avg_loss = 3.71895\n",
      "epoch no.2 train no.209930  loss = 3.15276 avg_loss = 3.71322\n",
      "epoch no.2 train no.209940  loss = 3.75843 avg_loss = 3.71818\n",
      "epoch no.2 train no.209950  loss = 2.73234 avg_loss = 3.66151\n",
      "epoch no.2 train no.209960  loss = 4.21417 avg_loss = 3.69394\n",
      "epoch no.2 train no.209970  loss = 5.64646 avg_loss = 3.72827\n",
      "epoch no.2 train no.209980  loss = 4.31944 avg_loss = 3.77542\n",
      "epoch no.2 train no.209990  loss = 1.88589 avg_loss = 3.75290\n",
      "epoch no.2 train no.210000  loss = 2.46623 avg_loss = 3.69997\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.2 train no.210010  loss = 2.32278 avg_loss = 3.72306\n",
      "epoch no.2 train no.210020  loss = 3.13650 avg_loss = 3.70821\n",
      "epoch no.2 train no.210030  loss = 2.74294 avg_loss = 3.66317\n",
      "epoch no.2 train no.210040  loss = 2.96233 avg_loss = 3.66415\n",
      "epoch no.2 train no.210050  loss = 3.47355 avg_loss = 3.63584\n",
      "epoch no.2 train no.210060  loss = 5.37503 avg_loss = 3.62026\n",
      "epoch no.2 train no.210070  loss = 4.27555 avg_loss = 3.59862\n",
      "epoch no.2 train no.210080  loss = 3.90533 avg_loss = 3.55827\n",
      "epoch no.2 train no.210090  loss = 3.12426 avg_loss = 3.55422\n",
      "epoch no.2 train no.210100  loss = 2.30601 avg_loss = 3.53657\n",
      "epoch no.2 train no.210110  loss = 4.12524 avg_loss = 3.57928\n",
      "epoch no.2 train no.210120  loss = 3.79882 avg_loss = 3.55651\n",
      "epoch no.2 train no.210130  loss = 1.90237 avg_loss = 3.56167\n",
      "epoch no.2 train no.210140  loss = 3.23227 avg_loss = 3.51702\n",
      "epoch no.2 train no.210150  loss = 2.58597 avg_loss = 3.53623\n",
      "epoch no.2 train no.210160  loss = 2.04044 avg_loss = 3.50539\n",
      "epoch no.2 train no.210170  loss = 3.00577 avg_loss = 3.49537\n",
      "epoch no.2 train no.210180  loss = 5.76349 avg_loss = 3.63190\n",
      "epoch no.2 train no.210190  loss = 4.92212 avg_loss = 3.58741\n",
      "epoch no.2 train no.210200  loss = 4.36730 avg_loss = 3.57727\n",
      "epoch no.2 train no.210210  loss = 3.57103 avg_loss = 3.57962\n",
      "epoch no.2 train no.210220  loss = 3.99742 avg_loss = 3.53568\n",
      "epoch no.2 train no.210230  loss = 4.48196 avg_loss = 3.55924\n",
      "epoch no.2 train no.210240  loss = 2.67976 avg_loss = 3.58894\n",
      "epoch no.2 train no.210250  loss = 3.73602 avg_loss = 3.63527\n",
      "epoch no.2 train no.210260  loss = 3.05291 avg_loss = 3.65146\n",
      "epoch no.2 train no.210270  loss = 3.59117 avg_loss = 3.64806\n",
      "epoch no.2 train no.210280  loss = 4.50967 avg_loss = 3.66674\n",
      "epoch no.2 train no.210290  loss = 4.95985 avg_loss = 3.63532\n",
      "epoch no.2 train no.210300  loss = 3.33877 avg_loss = 3.67619\n",
      "epoch no.2 train no.210310  loss = 2.72860 avg_loss = 3.68559\n",
      "epoch no.2 train no.210320  loss = 5.12284 avg_loss = 3.69843\n",
      "epoch no.2 train no.210330  loss = 2.54465 avg_loss = 3.66571\n",
      "epoch no.2 train no.210340  loss = 2.28641 avg_loss = 3.62393\n",
      "epoch no.2 train no.210350  loss = 3.06700 avg_loss = 3.63337\n",
      "epoch no.2 train no.210360  loss = 2.87405 avg_loss = 3.61170\n",
      "epoch no.2 train no.210370  loss = 4.28715 avg_loss = 3.57430\n",
      "epoch no.2 train no.210380  loss = 2.20122 avg_loss = 3.52686\n",
      "epoch no.2 train no.210390  loss = 3.95213 avg_loss = 3.52904\n",
      "epoch no.2 train no.210400  loss = 3.17861 avg_loss = 3.57516\n",
      "epoch no.2 train no.210410  loss = 4.08283 avg_loss = 3.52161\n",
      "epoch no.2 train no.210420  loss = 1.98404 avg_loss = 3.50279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.210430  loss = 3.73977 avg_loss = 3.50946\n",
      "epoch no.2 train no.210440  loss = 3.73434 avg_loss = 3.49961\n",
      "epoch no.2 train no.210450  loss = 4.23991 avg_loss = 3.55714\n",
      "epoch no.2 train no.210460  loss = 3.64189 avg_loss = 3.56416\n",
      "epoch no.2 train no.210470  loss = 3.64352 avg_loss = 3.59045\n",
      "epoch no.2 train no.210480  loss = 1.77384 avg_loss = 3.61051\n",
      "epoch no.2 train no.210490  loss = 4.68578 avg_loss = 3.61315\n",
      "epoch no.2 train no.210500  loss = 2.48140 avg_loss = 3.65034\n",
      "epoch no.2 train no.210510  loss = 2.65886 avg_loss = 3.60309\n",
      "epoch no.2 train no.210520  loss = 3.19117 avg_loss = 3.58036\n",
      "epoch no.2 train no.210530  loss = 2.30293 avg_loss = 3.61023\n",
      "epoch no.2 train no.210540  loss = 4.12628 avg_loss = 3.61357\n",
      "epoch no.2 train no.210550  loss = 2.14202 avg_loss = 3.68420\n",
      "epoch no.2 train no.210560  loss = 4.29304 avg_loss = 3.69272\n",
      "epoch no.2 train no.210570  loss = 6.87106 avg_loss = 3.67386\n",
      "epoch no.2 train no.210580  loss = 2.96249 avg_loss = 3.69960\n",
      "epoch no.2 train no.210590  loss = 3.89481 avg_loss = 3.70491\n",
      "epoch no.2 train no.210600  loss = 3.43686 avg_loss = 3.68477\n",
      "epoch no.2 train no.210610  loss = 2.19288 avg_loss = 3.65952\n",
      "epoch no.2 train no.210620  loss = 3.93701 avg_loss = 3.63627\n",
      "epoch no.2 train no.210630  loss = 3.42042 avg_loss = 3.60440\n",
      "epoch no.2 train no.210640  loss = 6.02891 avg_loss = 3.62843\n",
      "epoch no.2 train no.210650  loss = 5.62576 avg_loss = 3.69923\n",
      "epoch no.2 train no.210660  loss = 2.06594 avg_loss = 3.61335\n",
      "epoch no.2 train no.210670  loss = 2.92896 avg_loss = 3.62321\n",
      "epoch no.2 train no.210680  loss = 4.38151 avg_loss = 3.61912\n",
      "epoch no.2 train no.210690  loss = 3.75830 avg_loss = 3.62392\n",
      "epoch no.2 train no.210700  loss = 3.00603 avg_loss = 3.61859\n",
      "epoch no.2 train no.210710  loss = 3.25562 avg_loss = 3.58767\n",
      "epoch no.2 train no.210720  loss = 3.72243 avg_loss = 3.59340\n",
      "epoch no.2 train no.210730  loss = 2.06431 avg_loss = 3.54541\n",
      "epoch no.2 train no.210740  loss = 3.67157 avg_loss = 3.51417\n",
      "epoch no.2 train no.210750  loss = 1.98436 avg_loss = 3.48420\n",
      "epoch no.2 train no.210760  loss = 4.42407 avg_loss = 3.49542\n",
      "epoch no.2 train no.210770  loss = 2.64641 avg_loss = 3.47541\n",
      "epoch no.2 train no.210780  loss = 4.16613 avg_loss = 3.54134\n",
      "epoch no.2 train no.210790  loss = 2.67221 avg_loss = 3.53529\n",
      "epoch no.2 train no.210800  loss = 2.51332 avg_loss = 3.51744\n",
      "epoch no.2 train no.210810  loss = 2.75494 avg_loss = 3.55216\n",
      "epoch no.2 train no.210820  loss = 4.91031 avg_loss = 3.55368\n",
      "epoch no.2 train no.210830  loss = 2.92916 avg_loss = 3.54601\n",
      "epoch no.2 train no.210840  loss = 7.42561 avg_loss = 3.59905\n",
      "epoch no.2 train no.210850  loss = 4.00894 avg_loss = 3.54531\n",
      "epoch no.2 train no.210860  loss = 3.93390 avg_loss = 3.59177\n",
      "epoch no.2 train no.210870  loss = 4.26982 avg_loss = 3.59057\n",
      "epoch no.2 train no.210880  loss = 4.02037 avg_loss = 3.58523\n",
      "epoch no.2 train no.210890  loss = 1.98541 avg_loss = 3.52487\n",
      "epoch no.2 train no.210900  loss = 3.15090 avg_loss = 3.50494\n",
      "epoch no.2 train no.210910  loss = 3.64343 avg_loss = 3.49896\n",
      "epoch no.2 train no.210920  loss = 1.57215 avg_loss = 3.45797\n",
      "epoch no.2 train no.210930  loss = 3.85917 avg_loss = 3.46765\n",
      "epoch no.2 train no.210940  loss = 2.60909 avg_loss = 3.49724\n",
      "epoch no.2 train no.210950  loss = 3.92705 avg_loss = 3.53449\n",
      "epoch no.2 train no.210960  loss = 2.81236 avg_loss = 3.55276\n",
      "epoch no.2 train no.210970  loss = 3.34923 avg_loss = 3.54396\n",
      "epoch no.2 train no.210980  loss = 5.92933 avg_loss = 3.58090\n",
      "epoch no.2 train no.210990  loss = 2.74987 avg_loss = 3.53907\n",
      "epoch no.2 train no.211000  loss = 5.27870 avg_loss = 3.53742\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '하고', '▁좋은', '▁팝', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.2 train no.211010  loss = 2.60771 avg_loss = 3.45697\n",
      "epoch no.2 train no.211020  loss = 2.96168 avg_loss = 3.45420\n",
      "epoch no.2 train no.211030  loss = 3.04128 avg_loss = 3.51419\n",
      "epoch no.2 train no.211040  loss = 2.28760 avg_loss = 3.47542\n",
      "epoch no.2 train no.211050  loss = 6.26213 avg_loss = 3.46843\n",
      "epoch no.2 train no.211060  loss = 4.03673 avg_loss = 3.52629\n",
      "epoch no.2 train no.211070  loss = 3.14884 avg_loss = 3.54470\n",
      "epoch no.2 train no.211080  loss = 3.62961 avg_loss = 3.61073\n",
      "epoch no.2 train no.211090  loss = 5.70843 avg_loss = 3.62090\n",
      "epoch no.2 train no.211100  loss = 3.68153 avg_loss = 3.61083\n",
      "epoch no.2 train no.211110  loss = 3.67005 avg_loss = 3.57603\n",
      "epoch no.2 train no.211120  loss = 2.62396 avg_loss = 3.57340\n",
      "epoch no.2 train no.211130  loss = 3.90243 avg_loss = 3.56251\n",
      "epoch no.2 train no.211140  loss = 4.09173 avg_loss = 3.59760\n",
      "epoch no.2 train no.211150  loss = 2.60242 avg_loss = 3.56005\n",
      "epoch no.2 train no.211160  loss = 7.52183 avg_loss = 3.64898\n",
      "epoch no.2 train no.211170  loss = 4.38651 avg_loss = 3.60190\n",
      "epoch no.2 train no.211180  loss = 2.59166 avg_loss = 3.61324\n",
      "epoch no.2 train no.211190  loss = 4.12737 avg_loss = 3.63046\n",
      "epoch no.2 train no.211200  loss = 2.62627 avg_loss = 3.64677\n",
      "epoch no.2 train no.211210  loss = 3.07340 avg_loss = 3.65963\n",
      "epoch no.2 train no.211220  loss = 3.40457 avg_loss = 3.68780\n",
      "epoch no.2 train no.211230  loss = 2.90471 avg_loss = 3.68949\n",
      "epoch no.2 train no.211240  loss = 3.96931 avg_loss = 3.61282\n",
      "epoch no.2 train no.211250  loss = 3.30791 avg_loss = 3.61981\n",
      "epoch no.2 train no.211260  loss = 2.97079 avg_loss = 3.63362\n",
      "epoch no.2 train no.211270  loss = 3.39079 avg_loss = 3.64737\n",
      "epoch no.2 train no.211280  loss = 3.53173 avg_loss = 3.66321\n",
      "epoch no.2 train no.211290  loss = 2.23821 avg_loss = 3.62790\n",
      "epoch no.2 train no.211300  loss = 3.38741 avg_loss = 3.63144\n",
      "epoch no.2 train no.211310  loss = 3.63382 avg_loss = 3.66108\n",
      "epoch no.2 train no.211320  loss = 3.07235 avg_loss = 3.68802\n",
      "epoch no.2 train no.211330  loss = 3.20291 avg_loss = 3.67280\n",
      "epoch no.2 train no.211340  loss = 3.62122 avg_loss = 3.63885\n",
      "epoch no.2 train no.211350  loss = 3.04814 avg_loss = 3.72522\n",
      "epoch no.2 train no.211360  loss = 4.48697 avg_loss = 3.70251\n",
      "epoch no.2 train no.211370  loss = 2.40098 avg_loss = 3.65764\n",
      "epoch no.2 train no.211380  loss = 5.41043 avg_loss = 3.63642\n",
      "epoch no.2 train no.211390  loss = 2.99109 avg_loss = 3.65196\n",
      "epoch no.2 train no.211400  loss = 5.47977 avg_loss = 3.71509\n",
      "epoch no.2 train no.211410  loss = 3.83150 avg_loss = 3.75177\n",
      "epoch no.2 train no.211420  loss = 2.78981 avg_loss = 3.76714\n",
      "epoch no.2 train no.211430  loss = 5.05343 avg_loss = 3.78824\n",
      "epoch no.2 train no.211440  loss = 3.62850 avg_loss = 3.78011\n",
      "epoch no.2 train no.211450  loss = 3.90733 avg_loss = 3.78365\n",
      "epoch no.2 train no.211460  loss = 2.04511 avg_loss = 3.74503\n",
      "epoch no.2 train no.211470  loss = 3.38230 avg_loss = 3.76283\n",
      "epoch no.2 train no.211480  loss = 5.41733 avg_loss = 3.71311\n",
      "epoch no.2 train no.211490  loss = 4.02416 avg_loss = 3.70500\n",
      "epoch no.2 train no.211500  loss = 3.45849 avg_loss = 3.66584\n",
      "epoch no.2 train no.211510  loss = 4.51856 avg_loss = 3.64150\n",
      "epoch no.2 train no.211520  loss = 4.00240 avg_loss = 3.60047\n",
      "epoch no.2 train no.211530  loss = 4.75059 avg_loss = 3.63976\n",
      "epoch no.2 train no.211540  loss = 4.24699 avg_loss = 3.69629\n",
      "epoch no.2 train no.211550  loss = 4.71401 avg_loss = 3.67969\n",
      "epoch no.2 train no.211560  loss = 3.94328 avg_loss = 3.65342\n",
      "epoch no.2 train no.211570  loss = 2.56833 avg_loss = 3.69779\n",
      "epoch no.2 train no.211580  loss = 3.10111 avg_loss = 3.69148\n",
      "epoch no.2 train no.211590  loss = 2.99471 avg_loss = 3.66784\n",
      "epoch no.2 train no.211600  loss = 5.85391 avg_loss = 3.66012\n",
      "epoch no.2 train no.211610  loss = 4.50012 avg_loss = 3.68723\n",
      "epoch no.2 train no.211620  loss = 4.58328 avg_loss = 3.66836\n",
      "epoch no.2 train no.211630  loss = 2.26785 avg_loss = 3.63922\n",
      "epoch no.2 train no.211640  loss = 3.71843 avg_loss = 3.60738\n",
      "epoch no.2 train no.211650  loss = 2.47254 avg_loss = 3.63906\n",
      "epoch no.2 train no.211660  loss = 5.24448 avg_loss = 3.70512\n",
      "epoch no.2 train no.211670  loss = 1.55913 avg_loss = 3.67077\n",
      "epoch no.2 train no.211680  loss = 2.84906 avg_loss = 3.66214\n",
      "epoch no.2 train no.211690  loss = 2.33007 avg_loss = 3.65064\n",
      "epoch no.2 train no.211700  loss = 3.94820 avg_loss = 3.69792\n",
      "epoch no.2 train no.211710  loss = 2.79513 avg_loss = 3.70756\n",
      "epoch no.2 train no.211720  loss = 2.63043 avg_loss = 3.66549\n",
      "epoch no.2 train no.211730  loss = 4.89075 avg_loss = 3.66517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.211740  loss = 6.71965 avg_loss = 3.67731\n",
      "epoch no.2 train no.211750  loss = 4.32737 avg_loss = 3.65529\n",
      "epoch no.2 train no.211760  loss = 3.45776 avg_loss = 3.61926\n",
      "epoch no.2 train no.211770  loss = 4.35508 avg_loss = 3.60760\n",
      "epoch no.2 train no.211780  loss = 2.22720 avg_loss = 3.59438\n",
      "epoch no.2 train no.211790  loss = 4.11852 avg_loss = 3.58944\n",
      "epoch no.2 train no.211800  loss = 3.24831 avg_loss = 3.66953\n",
      "epoch no.2 train no.211810  loss = 4.33082 avg_loss = 3.63162\n",
      "epoch no.2 train no.211820  loss = 2.25726 avg_loss = 3.63497\n",
      "epoch no.2 train no.211830  loss = 3.46442 avg_loss = 3.62699\n",
      "epoch no.2 train no.211840  loss = 3.51606 avg_loss = 3.63740\n",
      "epoch no.2 train no.211850  loss = 4.79650 avg_loss = 3.68212\n",
      "epoch no.2 train no.211860  loss = 2.70349 avg_loss = 3.67329\n",
      "epoch no.2 train no.211870  loss = 2.63822 avg_loss = 3.67277\n",
      "epoch no.2 train no.211880  loss = 2.22891 avg_loss = 3.61636\n",
      "epoch no.2 train no.211890  loss = 5.21583 avg_loss = 3.69799\n",
      "epoch no.2 train no.211900  loss = 3.17298 avg_loss = 3.68421\n",
      "epoch no.2 train no.211910  loss = 3.70440 avg_loss = 3.65591\n",
      "epoch no.2 train no.211920  loss = 3.55936 avg_loss = 3.68294\n",
      "epoch no.2 train no.211930  loss = 2.89214 avg_loss = 3.68104\n",
      "epoch no.2 train no.211940  loss = 4.33292 avg_loss = 3.65763\n",
      "epoch no.2 train no.211950  loss = 3.63533 avg_loss = 3.68980\n",
      "epoch no.2 train no.211960  loss = 5.00623 avg_loss = 3.71348\n",
      "epoch no.2 train no.211970  loss = 5.64257 avg_loss = 3.68382\n",
      "epoch no.2 train no.211980  loss = 3.45269 avg_loss = 3.67430\n",
      "epoch no.2 train no.211990  loss = 2.83602 avg_loss = 3.67190\n",
      "epoch no.2 train no.212000  loss = 2.99152 avg_loss = 3.62695\n",
      "3\n",
      "to_tokens: ['▁비', '▁좋은', '하기', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 음악</s>\n",
      "epoch no.2 train no.212010  loss = 3.81227 avg_loss = 3.61439\n",
      "epoch no.2 train no.212020  loss = 3.67807 avg_loss = 3.60979\n",
      "epoch no.2 train no.212030  loss = 3.08642 avg_loss = 3.61279\n",
      "epoch no.2 train no.212040  loss = 2.50555 avg_loss = 3.58124\n",
      "epoch no.2 train no.212050  loss = 2.88073 avg_loss = 3.57011\n",
      "epoch no.2 train no.212060  loss = 2.13285 avg_loss = 3.58316\n",
      "epoch no.2 train no.212070  loss = 2.55514 avg_loss = 3.58182\n",
      "epoch no.2 train no.212080  loss = 3.71647 avg_loss = 3.59461\n",
      "epoch no.2 train no.212090  loss = 3.16610 avg_loss = 3.62373\n",
      "epoch no.2 train no.212100  loss = 4.00744 avg_loss = 3.62207\n",
      "epoch no.2 train no.212110  loss = 4.59964 avg_loss = 3.58340\n",
      "epoch no.2 train no.212120  loss = 3.43323 avg_loss = 3.57184\n",
      "epoch no.2 train no.212130  loss = 3.50174 avg_loss = 3.56200\n",
      "epoch no.2 train no.212140  loss = 3.34257 avg_loss = 3.54309\n",
      "epoch no.2 train no.212150  loss = 3.30181 avg_loss = 3.53006\n",
      "epoch no.2 train no.212160  loss = 2.60869 avg_loss = 3.53522\n",
      "epoch no.2 train no.212170  loss = 2.84817 avg_loss = 3.55306\n",
      "epoch no.2 train no.212180  loss = 3.52010 avg_loss = 3.58148\n",
      "epoch no.2 train no.212190  loss = 3.10789 avg_loss = 3.59474\n",
      "epoch no.2 train no.212200  loss = 4.54538 avg_loss = 3.58529\n",
      "epoch no.2 train no.212210  loss = 3.83738 avg_loss = 3.53057\n",
      "epoch no.2 train no.212220  loss = 2.86185 avg_loss = 3.56364\n",
      "epoch no.2 train no.212230  loss = 3.40343 avg_loss = 3.60247\n",
      "epoch no.2 train no.212240  loss = 6.26074 avg_loss = 3.64099\n",
      "epoch no.2 train no.212250  loss = 3.54325 avg_loss = 3.64195\n",
      "epoch no.2 train no.212260  loss = 3.38701 avg_loss = 3.60337\n",
      "epoch no.2 train no.212270  loss = 3.11096 avg_loss = 3.55831\n",
      "epoch no.2 train no.212280  loss = 5.30197 avg_loss = 3.57889\n",
      "epoch no.2 train no.212290  loss = 3.59840 avg_loss = 3.60491\n",
      "epoch no.2 train no.212300  loss = 3.39112 avg_loss = 3.54764\n",
      "epoch no.2 train no.212310  loss = 3.88225 avg_loss = 3.53983\n",
      "epoch no.2 train no.212320  loss = 5.89903 avg_loss = 3.58341\n",
      "epoch no.2 train no.212330  loss = 3.31485 avg_loss = 3.58015\n",
      "epoch no.2 train no.212340  loss = 2.08814 avg_loss = 3.52973\n",
      "epoch no.2 train no.212350  loss = 4.01261 avg_loss = 3.59293\n",
      "epoch no.2 train no.212360  loss = 2.72199 avg_loss = 3.63276\n",
      "epoch no.2 train no.212370  loss = 2.13558 avg_loss = 3.65720\n",
      "epoch no.2 train no.212380  loss = 4.04487 avg_loss = 3.61987\n",
      "epoch no.2 train no.212390  loss = 4.21403 avg_loss = 3.67127\n",
      "epoch no.2 train no.212400  loss = 5.65937 avg_loss = 3.72212\n",
      "epoch no.2 train no.212410  loss = 2.98385 avg_loss = 3.72782\n",
      "epoch no.2 train no.212420  loss = 3.32040 avg_loss = 3.70948\n",
      "epoch no.2 train no.212430  loss = 4.18549 avg_loss = 3.70275\n",
      "epoch no.2 train no.212440  loss = 2.63296 avg_loss = 3.71726\n",
      "epoch no.2 train no.212450  loss = 2.47198 avg_loss = 3.70587\n",
      "epoch no.2 train no.212460  loss = 4.72226 avg_loss = 3.65521\n",
      "epoch no.2 train no.212470  loss = 5.58350 avg_loss = 3.68739\n",
      "epoch no.2 train no.212480  loss = 6.69004 avg_loss = 3.68710\n",
      "epoch no.2 train no.212490  loss = 3.48234 avg_loss = 3.72744\n",
      "epoch no.2 train no.212500  loss = 2.29461 avg_loss = 3.71166\n",
      "epoch no.2 train no.212510  loss = 2.77666 avg_loss = 3.71516\n",
      "epoch no.2 train no.212520  loss = 2.79737 avg_loss = 3.67714\n",
      "epoch no.2 train no.212530  loss = 5.97304 avg_loss = 3.71870\n",
      "epoch no.2 train no.212540  loss = 4.65243 avg_loss = 3.69491\n",
      "epoch no.2 train no.212550  loss = 2.93099 avg_loss = 3.67874\n",
      "epoch no.2 train no.212560  loss = 2.16947 avg_loss = 3.66213\n",
      "epoch no.2 train no.212570  loss = 4.91139 avg_loss = 3.63461\n",
      "epoch no.2 train no.212580  loss = 3.86941 avg_loss = 3.61483\n",
      "epoch no.2 train no.212590  loss = 4.71628 avg_loss = 3.59857\n",
      "epoch no.2 train no.212600  loss = 2.71060 avg_loss = 3.60424\n",
      "epoch no.2 train no.212610  loss = 4.62021 avg_loss = 3.64181\n",
      "epoch no.2 train no.212620  loss = 3.36855 avg_loss = 3.59123\n",
      "epoch no.2 train no.212630  loss = 2.82378 avg_loss = 3.61326\n",
      "epoch no.2 train no.212640  loss = 3.20765 avg_loss = 3.58469\n",
      "epoch no.2 train no.212650  loss = 3.50654 avg_loss = 3.58707\n",
      "epoch no.2 train no.212660  loss = 2.45746 avg_loss = 3.55468\n",
      "epoch no.2 train no.212670  loss = 2.88714 avg_loss = 3.53019\n",
      "epoch no.2 train no.212680  loss = 2.87744 avg_loss = 3.50430\n",
      "epoch no.2 train no.212690  loss = 3.20017 avg_loss = 3.48631\n",
      "epoch no.2 train no.212700  loss = 4.88127 avg_loss = 3.49086\n",
      "epoch no.2 train no.212710  loss = 3.95197 avg_loss = 3.53737\n",
      "epoch no.2 train no.212720  loss = 3.97591 avg_loss = 3.53058\n",
      "epoch no.2 train no.212730  loss = 3.12345 avg_loss = 3.57223\n",
      "epoch no.2 train no.212740  loss = 2.74511 avg_loss = 3.56555\n",
      "epoch no.2 train no.212750  loss = 2.29415 avg_loss = 3.57577\n",
      "epoch no.2 train no.212760  loss = 3.79654 avg_loss = 3.57738\n",
      "epoch no.2 train no.212770  loss = 2.88469 avg_loss = 3.56855\n",
      "epoch no.2 train no.212780  loss = 3.21177 avg_loss = 3.58426\n",
      "epoch no.2 train no.212790  loss = 2.92917 avg_loss = 3.57211\n",
      "epoch no.2 train no.212800  loss = 3.65435 avg_loss = 3.64038\n",
      "epoch no.2 train no.212810  loss = 2.79226 avg_loss = 3.63942\n",
      "epoch no.2 train no.212820  loss = 4.05754 avg_loss = 3.63250\n",
      "epoch no.2 train no.212830  loss = 3.20130 avg_loss = 3.64839\n",
      "epoch no.2 train no.212840  loss = 3.73068 avg_loss = 3.63606\n",
      "epoch no.2 train no.212850  loss = 3.00055 avg_loss = 3.61841\n",
      "epoch no.2 train no.212860  loss = 3.05047 avg_loss = 3.61850\n",
      "epoch no.2 train no.212870  loss = 3.24147 avg_loss = 3.56877\n",
      "epoch no.2 train no.212880  loss = 3.98683 avg_loss = 3.59053\n",
      "epoch no.2 train no.212890  loss = 5.78429 avg_loss = 3.64875\n",
      "epoch no.2 train no.212900  loss = 3.12802 avg_loss = 3.66381\n",
      "epoch no.2 train no.212910  loss = 3.90620 avg_loss = 3.66376\n",
      "epoch no.2 train no.212920  loss = 3.65029 avg_loss = 3.62266\n",
      "epoch no.2 train no.212930  loss = 3.15903 avg_loss = 3.61679\n",
      "epoch no.2 train no.212940  loss = 2.59541 avg_loss = 3.62516\n",
      "epoch no.2 train no.212950  loss = 4.53571 avg_loss = 3.68463\n",
      "epoch no.2 train no.212960  loss = 4.67453 avg_loss = 3.66126\n",
      "epoch no.2 train no.212970  loss = 2.58521 avg_loss = 3.67308\n",
      "epoch no.2 train no.212980  loss = 3.98310 avg_loss = 3.67715\n",
      "epoch no.2 train no.212990  loss = 3.30637 avg_loss = 3.65266\n",
      "epoch no.2 train no.213000  loss = 4.10080 avg_loss = 3.68022\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁신나는', '음악', '</s>']\n",
      "기분전환용 인디음악</s>\n",
      "epoch no.2 train no.213010  loss = 3.07481 avg_loss = 3.66005\n",
      "epoch no.2 train no.213020  loss = 4.76840 avg_loss = 3.67433\n",
      "epoch no.2 train no.213030  loss = 2.86977 avg_loss = 3.69061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.213040  loss = 5.28284 avg_loss = 3.71955\n",
      "epoch no.2 train no.213050  loss = 2.67821 avg_loss = 3.71317\n",
      "epoch no.2 train no.213060  loss = 3.10931 avg_loss = 3.68336\n",
      "epoch no.2 train no.213070  loss = 4.77799 avg_loss = 3.69736\n",
      "epoch no.2 train no.213080  loss = 2.98725 avg_loss = 3.69143\n",
      "epoch no.2 train no.213090  loss = 4.53278 avg_loss = 3.74045\n",
      "epoch no.2 train no.213100  loss = 2.37545 avg_loss = 3.67897\n",
      "epoch no.2 train no.213110  loss = 2.10473 avg_loss = 3.62120\n",
      "epoch no.2 train no.213120  loss = 2.51365 avg_loss = 3.59413\n",
      "epoch no.2 train no.213130  loss = 3.03741 avg_loss = 3.57705\n",
      "epoch no.2 train no.213140  loss = 3.49208 avg_loss = 3.60978\n",
      "epoch no.2 train no.213150  loss = 3.04287 avg_loss = 3.56710\n",
      "epoch no.2 train no.213160  loss = 3.83066 avg_loss = 3.56356\n",
      "epoch no.2 train no.213170  loss = 2.95931 avg_loss = 3.55959\n",
      "epoch no.2 train no.213180  loss = 3.33404 avg_loss = 3.55523\n",
      "epoch no.2 train no.213190  loss = 3.03713 avg_loss = 3.54766\n",
      "epoch no.2 train no.213200  loss = 5.42095 avg_loss = 3.54864\n",
      "epoch no.2 train no.213210  loss = 4.86353 avg_loss = 3.59464\n",
      "epoch no.2 train no.213220  loss = 4.84250 avg_loss = 3.61546\n",
      "epoch no.2 train no.213230  loss = 2.41945 avg_loss = 3.63631\n",
      "epoch no.2 train no.213240  loss = 2.67035 avg_loss = 3.60807\n",
      "epoch no.2 train no.213250  loss = 2.42474 avg_loss = 3.60485\n",
      "epoch no.2 train no.213260  loss = 3.45892 avg_loss = 3.54853\n",
      "epoch no.2 train no.213270  loss = 3.26719 avg_loss = 3.59743\n",
      "epoch no.2 train no.213280  loss = 3.35718 avg_loss = 3.55237\n",
      "epoch no.2 train no.213290  loss = 4.72159 avg_loss = 3.53956\n",
      "epoch no.2 train no.213300  loss = 3.05381 avg_loss = 3.55557\n",
      "epoch no.2 train no.213310  loss = 3.65515 avg_loss = 3.53201\n",
      "epoch no.2 train no.213320  loss = 2.00316 avg_loss = 3.48741\n",
      "epoch no.2 train no.213330  loss = 3.88494 avg_loss = 3.51110\n",
      "epoch no.2 train no.213340  loss = 3.14736 avg_loss = 3.55408\n",
      "epoch no.2 train no.213350  loss = 3.25197 avg_loss = 3.53234\n",
      "epoch no.2 train no.213360  loss = 3.01130 avg_loss = 3.52618\n",
      "epoch no.2 train no.213370  loss = 3.76730 avg_loss = 3.53419\n",
      "epoch no.2 train no.213380  loss = 3.67304 avg_loss = 3.56316\n",
      "epoch no.2 train no.213390  loss = 2.42325 avg_loss = 3.58509\n",
      "epoch no.2 train no.213400  loss = 2.07023 avg_loss = 3.55258\n",
      "epoch no.2 train no.213410  loss = 2.80208 avg_loss = 3.53163\n",
      "epoch no.2 train no.213420  loss = 4.63657 avg_loss = 3.52245\n",
      "epoch no.2 train no.213430  loss = 3.60029 avg_loss = 3.54101\n",
      "epoch no.2 train no.213440  loss = 3.78776 avg_loss = 3.51918\n",
      "epoch no.2 train no.213450  loss = 2.97641 avg_loss = 3.50747\n",
      "epoch no.2 train no.213460  loss = 3.67175 avg_loss = 3.51629\n",
      "epoch no.2 train no.213470  loss = 3.53190 avg_loss = 3.53303\n",
      "epoch no.2 train no.213480  loss = 3.22292 avg_loss = 3.49838\n",
      "epoch no.2 train no.213490  loss = 4.04283 avg_loss = 3.51392\n",
      "epoch no.2 train no.213500  loss = 3.89853 avg_loss = 3.49521\n",
      "epoch no.2 train no.213510  loss = 3.96312 avg_loss = 3.51814\n",
      "epoch no.2 train no.213520  loss = 3.19575 avg_loss = 3.49768\n",
      "epoch no.2 train no.213530  loss = 3.02557 avg_loss = 3.47237\n",
      "epoch no.2 train no.213540  loss = 6.07444 avg_loss = 3.47051\n",
      "epoch no.2 train no.213550  loss = 3.70005 avg_loss = 3.47595\n",
      "epoch no.2 train no.213560  loss = 4.28392 avg_loss = 3.49952\n",
      "epoch no.2 train no.213570  loss = 3.39792 avg_loss = 3.54965\n",
      "epoch no.2 train no.213580  loss = 4.93498 avg_loss = 3.55711\n",
      "epoch no.2 train no.213590  loss = 2.23879 avg_loss = 3.54981\n",
      "epoch no.2 train no.213600  loss = 2.89281 avg_loss = 3.55943\n",
      "epoch no.2 train no.213610  loss = 2.90027 avg_loss = 3.53496\n",
      "epoch no.2 train no.213620  loss = 3.10438 avg_loss = 3.54273\n",
      "epoch no.2 train no.213630  loss = 3.41873 avg_loss = 3.53202\n",
      "epoch no.2 train no.213640  loss = 3.36058 avg_loss = 3.54162\n",
      "epoch no.2 train no.213650  loss = 3.42290 avg_loss = 3.49965\n",
      "epoch no.2 train no.213660  loss = 2.53796 avg_loss = 3.54896\n",
      "epoch no.2 train no.213670  loss = 4.09222 avg_loss = 3.58586\n",
      "epoch no.2 train no.213680  loss = 2.72576 avg_loss = 3.59884\n",
      "epoch no.2 train no.213690  loss = 3.95448 avg_loss = 3.60602\n",
      "epoch no.2 train no.213700  loss = 1.96776 avg_loss = 3.55067\n",
      "epoch no.2 train no.213710  loss = 6.66456 avg_loss = 3.52007\n",
      "epoch no.2 train no.213720  loss = 2.86432 avg_loss = 3.51189\n",
      "epoch no.2 train no.213730  loss = 5.47517 avg_loss = 3.55029\n",
      "epoch no.2 train no.213740  loss = 4.58604 avg_loss = 3.56057\n",
      "epoch no.2 train no.213750  loss = 2.86446 avg_loss = 3.55004\n",
      "epoch no.2 train no.213760  loss = 3.65832 avg_loss = 3.54777\n",
      "epoch no.2 train no.213770  loss = 3.25527 avg_loss = 3.48989\n",
      "epoch no.2 train no.213780  loss = 3.01965 avg_loss = 3.49936\n",
      "epoch no.2 train no.213790  loss = 5.03990 avg_loss = 3.54652\n",
      "epoch no.2 train no.213800  loss = 3.41244 avg_loss = 3.51928\n",
      "epoch no.2 train no.213810  loss = 2.95162 avg_loss = 3.50706\n",
      "epoch no.2 train no.213820  loss = 2.91058 avg_loss = 3.53516\n",
      "epoch no.2 train no.213830  loss = 1.62148 avg_loss = 3.52543\n",
      "epoch no.2 train no.213840  loss = 3.88655 avg_loss = 3.51462\n",
      "epoch no.2 train no.213850  loss = 3.67500 avg_loss = 3.53409\n",
      "epoch no.2 train no.213860  loss = 2.72653 avg_loss = 3.52595\n",
      "epoch no.2 train no.213870  loss = 4.26529 avg_loss = 3.61899\n",
      "epoch no.2 train no.213880  loss = 5.11722 avg_loss = 3.63291\n",
      "epoch no.2 train no.213890  loss = 4.36383 avg_loss = 3.62977\n",
      "epoch no.2 train no.213900  loss = 3.52102 avg_loss = 3.66691\n",
      "epoch no.2 train no.213910  loss = 4.45300 avg_loss = 3.72907\n",
      "epoch no.2 train no.213920  loss = 3.64808 avg_loss = 3.73617\n",
      "epoch no.2 train no.213930  loss = 2.60515 avg_loss = 3.72798\n",
      "epoch no.2 train no.213940  loss = 2.77417 avg_loss = 3.68342\n",
      "epoch no.2 train no.213950  loss = 4.31779 avg_loss = 3.66859\n",
      "epoch no.2 train no.213960  loss = 2.54178 avg_loss = 3.65735\n",
      "epoch no.2 train no.213970  loss = 4.16188 avg_loss = 3.67633\n",
      "epoch no.2 train no.213980  loss = 2.58114 avg_loss = 3.68007\n",
      "epoch no.2 train no.213990  loss = 4.02745 avg_loss = 3.64889\n",
      "epoch no.2 train no.214000  loss = 4.86516 avg_loss = 3.63992\n",
      "2\n",
      "to_tokens: ['▁라디오', '▁좋은', '을', '</s>', '</s>']\n",
      "기분전환 드라이브 뮤직</s>\n",
      "epoch no.2 train no.214010  loss = 4.72776 avg_loss = 3.61119\n",
      "epoch no.2 train no.214020  loss = 3.48081 avg_loss = 3.59840\n",
      "epoch no.2 train no.214030  loss = 3.02430 avg_loss = 3.61332\n",
      "epoch no.2 train no.214040  loss = 2.30334 avg_loss = 3.55183\n",
      "epoch no.2 train no.214050  loss = 3.31356 avg_loss = 3.59183\n",
      "epoch no.2 train no.214060  loss = 2.39593 avg_loss = 3.59576\n",
      "epoch no.2 train no.214070  loss = 5.41814 avg_loss = 3.63031\n",
      "epoch no.2 train no.214080  loss = 3.59298 avg_loss = 3.65000\n",
      "epoch no.2 train no.214090  loss = 3.29844 avg_loss = 3.66861\n",
      "epoch no.2 train no.214100  loss = 3.17604 avg_loss = 3.72006\n",
      "epoch no.2 train no.214110  loss = 4.65706 avg_loss = 3.72386\n",
      "epoch no.2 train no.214120  loss = 4.42433 avg_loss = 3.75814\n",
      "epoch no.2 train no.214130  loss = 6.38795 avg_loss = 3.76674\n",
      "epoch no.2 train no.214140  loss = 3.85069 avg_loss = 3.78872\n",
      "epoch no.2 train no.214150  loss = 3.84153 avg_loss = 3.78087\n",
      "epoch no.2 train no.214160  loss = 3.98353 avg_loss = 3.84052\n",
      "epoch no.2 train no.214170  loss = 3.35233 avg_loss = 3.79629\n",
      "epoch no.2 train no.214180  loss = 4.25764 avg_loss = 3.74807\n",
      "epoch no.2 train no.214190  loss = 3.34637 avg_loss = 3.68444\n",
      "epoch no.2 train no.214200  loss = 3.76054 avg_loss = 3.67981\n",
      "epoch no.2 train no.214210  loss = 3.35846 avg_loss = 3.65074\n",
      "epoch no.2 train no.214220  loss = 3.55752 avg_loss = 3.63825\n",
      "epoch no.2 train no.214230  loss = 3.35706 avg_loss = 3.62440\n",
      "epoch no.2 train no.214240  loss = 5.97544 avg_loss = 3.66716\n",
      "epoch no.2 train no.214250  loss = 2.46586 avg_loss = 3.65986\n",
      "epoch no.2 train no.214260  loss = 4.07913 avg_loss = 3.65441\n",
      "epoch no.2 train no.214270  loss = 3.58486 avg_loss = 3.63165\n",
      "epoch no.2 train no.214280  loss = 2.47092 avg_loss = 3.66648\n",
      "epoch no.2 train no.214290  loss = 4.30826 avg_loss = 3.66496\n",
      "epoch no.2 train no.214300  loss = 2.70263 avg_loss = 3.65918\n",
      "epoch no.2 train no.214310  loss = 3.83182 avg_loss = 3.63800\n",
      "epoch no.2 train no.214320  loss = 3.05991 avg_loss = 3.64588\n",
      "epoch no.2 train no.214330  loss = 3.63726 avg_loss = 3.62579\n",
      "epoch no.2 train no.214340  loss = 3.04673 avg_loss = 3.59939\n",
      "epoch no.2 train no.214350  loss = 4.64475 avg_loss = 3.58594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.214360  loss = 5.09635 avg_loss = 3.62218\n",
      "epoch no.2 train no.214370  loss = 3.27192 avg_loss = 3.69005\n",
      "epoch no.2 train no.214380  loss = 4.56951 avg_loss = 3.66830\n",
      "epoch no.2 train no.214390  loss = 3.34083 avg_loss = 3.60813\n",
      "epoch no.2 train no.214400  loss = 3.23305 avg_loss = 3.59545\n",
      "epoch no.2 train no.214410  loss = 2.34956 avg_loss = 3.56238\n",
      "epoch no.2 train no.214420  loss = 2.69670 avg_loss = 3.53564\n",
      "epoch no.2 train no.214430  loss = 3.86405 avg_loss = 3.57177\n",
      "epoch no.2 train no.214440  loss = 4.61440 avg_loss = 3.61830\n",
      "epoch no.2 train no.214450  loss = 2.25807 avg_loss = 3.63163\n",
      "epoch no.2 train no.214460  loss = 2.88669 avg_loss = 3.63837\n",
      "epoch no.2 train no.214470  loss = 2.52559 avg_loss = 3.66599\n",
      "epoch no.2 train no.214480  loss = 3.27571 avg_loss = 3.64403\n",
      "epoch no.2 train no.214490  loss = 3.12302 avg_loss = 3.62371\n",
      "epoch no.2 train no.214500  loss = 2.86774 avg_loss = 3.57770\n",
      "epoch no.2 train no.214510  loss = 3.63143 avg_loss = 3.56027\n",
      "epoch no.2 train no.214520  loss = 4.15521 avg_loss = 3.55061\n",
      "epoch no.2 train no.214530  loss = 4.10456 avg_loss = 3.56481\n",
      "epoch no.2 train no.214540  loss = 2.53307 avg_loss = 3.57153\n",
      "epoch no.2 train no.214550  loss = 4.37843 avg_loss = 3.56741\n",
      "epoch no.2 train no.214560  loss = 3.68548 avg_loss = 3.60437\n",
      "epoch no.2 train no.214570  loss = 2.23796 avg_loss = 3.65334\n",
      "epoch no.2 train no.214580  loss = 2.89196 avg_loss = 3.63290\n",
      "epoch no.2 train no.214590  loss = 6.21980 avg_loss = 3.61109\n",
      "epoch no.2 train no.214600  loss = 2.53728 avg_loss = 3.65978\n",
      "epoch no.2 train no.214610  loss = 3.45806 avg_loss = 3.61151\n",
      "epoch no.2 train no.214620  loss = 4.18640 avg_loss = 3.64217\n",
      "epoch no.2 train no.214630  loss = 6.17908 avg_loss = 3.62448\n",
      "epoch no.2 train no.214640  loss = 1.75308 avg_loss = 3.59181\n",
      "epoch no.2 train no.214650  loss = 3.43783 avg_loss = 3.58093\n",
      "epoch no.2 train no.214660  loss = 4.48771 avg_loss = 3.58804\n",
      "epoch no.2 train no.214670  loss = 1.16165 avg_loss = 3.59091\n",
      "epoch no.2 train no.214680  loss = 3.91981 avg_loss = 3.59834\n",
      "epoch no.2 train no.214690  loss = 4.48827 avg_loss = 3.64092\n",
      "epoch no.2 train no.214700  loss = 4.51217 avg_loss = 3.68545\n",
      "epoch no.2 train no.214710  loss = 3.41349 avg_loss = 3.63739\n",
      "epoch no.2 train no.214720  loss = 3.62189 avg_loss = 3.61225\n",
      "epoch no.2 train no.214730  loss = 3.32082 avg_loss = 3.61196\n",
      "epoch no.2 train no.214740  loss = 6.05202 avg_loss = 3.61517\n",
      "epoch no.2 train no.214750  loss = 3.98218 avg_loss = 3.59841\n",
      "epoch no.2 train no.214760  loss = 2.87660 avg_loss = 3.64280\n",
      "epoch no.2 train no.214770  loss = 3.10382 avg_loss = 3.57772\n",
      "epoch no.2 train no.214780  loss = 4.35369 avg_loss = 3.61084\n",
      "epoch no.2 train no.214790  loss = 4.40253 avg_loss = 3.60501\n",
      "epoch no.2 train no.214800  loss = 3.47263 avg_loss = 3.61368\n",
      "epoch no.2 train no.214810  loss = 3.58655 avg_loss = 3.65096\n",
      "epoch no.2 train no.214820  loss = 6.49782 avg_loss = 3.67234\n",
      "epoch no.2 train no.214830  loss = 2.15343 avg_loss = 3.66692\n",
      "epoch no.2 train no.214840  loss = 4.50084 avg_loss = 3.72763\n",
      "epoch no.2 train no.214850  loss = 2.72270 avg_loss = 3.74579\n",
      "epoch no.2 train no.214860  loss = 3.86444 avg_loss = 3.77857\n",
      "epoch no.2 train no.214870  loss = 3.95330 avg_loss = 3.77173\n",
      "epoch no.2 train no.214880  loss = 3.61293 avg_loss = 3.75418\n",
      "epoch no.2 train no.214890  loss = 2.65424 avg_loss = 3.70047\n",
      "epoch no.2 train no.214900  loss = 3.55226 avg_loss = 3.61950\n",
      "epoch no.2 train no.214910  loss = 2.12971 avg_loss = 3.65188\n",
      "epoch no.2 train no.214920  loss = 5.12314 avg_loss = 3.68147\n",
      "epoch no.2 train no.214930  loss = 3.30998 avg_loss = 3.66540\n",
      "epoch no.2 train no.214940  loss = 2.91044 avg_loss = 3.59804\n",
      "epoch no.2 train no.214950  loss = 2.71939 avg_loss = 3.64432\n",
      "epoch no.2 train no.214960  loss = 3.32490 avg_loss = 3.68119\n",
      "epoch no.2 train no.214970  loss = 2.89647 avg_loss = 3.62395\n",
      "epoch no.2 train no.214980  loss = 2.85622 avg_loss = 3.66298\n",
      "epoch no.2 train no.214990  loss = 4.70669 avg_loss = 3.67789\n",
      "epoch no.2 train no.215000  loss = 4.86213 avg_loss = 3.66044\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.2 train no.215010  loss = 3.10686 avg_loss = 3.67181\n",
      "epoch no.2 train no.215020  loss = 3.18840 avg_loss = 3.63644\n",
      "epoch no.2 train no.215030  loss = 4.64217 avg_loss = 3.59579\n",
      "epoch no.2 train no.215040  loss = 2.57573 avg_loss = 3.58612\n",
      "epoch no.2 train no.215050  loss = 4.02114 avg_loss = 3.54751\n",
      "epoch no.2 train no.215060  loss = 4.56836 avg_loss = 3.58616\n",
      "epoch no.2 train no.215070  loss = 2.45654 avg_loss = 3.52960\n",
      "epoch no.2 train no.215080  loss = 5.04920 avg_loss = 3.56387\n",
      "epoch no.2 train no.215090  loss = 2.79405 avg_loss = 3.58263\n",
      "epoch no.2 train no.215100  loss = 3.84455 avg_loss = 3.63543\n",
      "epoch no.2 train no.215110  loss = 5.18190 avg_loss = 3.60798\n",
      "epoch no.2 train no.215120  loss = 4.23771 avg_loss = 3.60859\n",
      "epoch no.2 train no.215130  loss = 2.67819 avg_loss = 3.56579\n",
      "epoch no.2 train no.215140  loss = 4.13393 avg_loss = 3.55959\n",
      "epoch no.2 train no.215150  loss = 3.62512 avg_loss = 3.55737\n",
      "epoch no.2 train no.215160  loss = 2.82344 avg_loss = 3.54535\n",
      "epoch no.2 train no.215170  loss = 4.30512 avg_loss = 3.54408\n",
      "epoch no.2 train no.215180  loss = 2.39616 avg_loss = 3.53407\n",
      "epoch no.2 train no.215190  loss = 3.04907 avg_loss = 3.54261\n",
      "epoch no.2 train no.215200  loss = 5.01273 avg_loss = 3.54970\n",
      "epoch no.2 train no.215210  loss = 5.83112 avg_loss = 3.56794\n",
      "epoch no.2 train no.215220  loss = 3.79968 avg_loss = 3.53993\n",
      "epoch no.2 train no.215230  loss = 4.70765 avg_loss = 3.61943\n",
      "epoch no.2 train no.215240  loss = 4.07939 avg_loss = 3.63700\n",
      "epoch no.2 train no.215250  loss = 4.12408 avg_loss = 3.59468\n",
      "epoch no.2 train no.215260  loss = 3.97020 avg_loss = 3.59208\n",
      "epoch no.2 train no.215270  loss = 4.03401 avg_loss = 3.58468\n",
      "epoch no.2 train no.215280  loss = 1.92102 avg_loss = 3.57761\n",
      "epoch no.2 train no.215290  loss = 3.01861 avg_loss = 3.57181\n",
      "epoch no.2 train no.215300  loss = 3.66332 avg_loss = 3.65156\n",
      "epoch no.2 train no.215310  loss = 2.31778 avg_loss = 3.66741\n",
      "epoch no.2 train no.215320  loss = 5.76672 avg_loss = 3.72766\n",
      "epoch no.2 train no.215330  loss = 5.92881 avg_loss = 3.75157\n",
      "epoch no.2 train no.215340  loss = 2.85039 avg_loss = 3.73769\n",
      "epoch no.2 train no.215350  loss = 4.69279 avg_loss = 3.70243\n",
      "epoch no.2 train no.215360  loss = 3.42217 avg_loss = 3.68830\n",
      "epoch no.2 train no.215370  loss = 3.69322 avg_loss = 3.68332\n",
      "epoch no.2 train no.215380  loss = 3.26485 avg_loss = 3.64482\n",
      "epoch no.2 train no.215390  loss = 2.03802 avg_loss = 3.60540\n",
      "epoch no.2 train no.215400  loss = 3.41140 avg_loss = 3.59146\n",
      "epoch no.2 train no.215410  loss = 2.39688 avg_loss = 3.58823\n",
      "epoch no.2 train no.215420  loss = 5.19154 avg_loss = 3.61625\n",
      "epoch no.2 train no.215430  loss = 4.20955 avg_loss = 3.63028\n",
      "epoch no.2 train no.215440  loss = 4.81913 avg_loss = 3.63452\n",
      "epoch no.2 train no.215450  loss = 4.15190 avg_loss = 3.65865\n",
      "epoch no.2 train no.215460  loss = 4.25583 avg_loss = 3.66809\n",
      "epoch no.2 train no.215470  loss = 2.01342 avg_loss = 3.64225\n",
      "epoch no.2 train no.215480  loss = 1.13699 avg_loss = 3.58975\n",
      "epoch no.2 train no.215490  loss = 4.20169 avg_loss = 3.57081\n",
      "epoch no.2 train no.215500  loss = 4.65353 avg_loss = 3.59613\n",
      "epoch no.2 train no.215510  loss = 4.35207 avg_loss = 3.58588\n",
      "epoch no.2 train no.215520  loss = 4.29771 avg_loss = 3.58854\n",
      "epoch no.2 train no.215530  loss = 2.70017 avg_loss = 3.56325\n",
      "epoch no.2 train no.215540  loss = 4.09360 avg_loss = 3.59550\n",
      "epoch no.2 train no.215550  loss = 6.19708 avg_loss = 3.64873\n",
      "epoch no.2 train no.215560  loss = 3.11766 avg_loss = 3.63221\n",
      "epoch no.2 train no.215570  loss = 3.47548 avg_loss = 3.65343\n",
      "epoch no.2 train no.215580  loss = 3.09535 avg_loss = 3.67981\n",
      "epoch no.2 train no.215590  loss = 2.50919 avg_loss = 3.71045\n",
      "epoch no.2 train no.215600  loss = 3.78217 avg_loss = 3.67832\n",
      "epoch no.2 train no.215610  loss = 2.97769 avg_loss = 3.63648\n",
      "epoch no.2 train no.215620  loss = 4.15357 avg_loss = 3.63672\n",
      "epoch no.2 train no.215630  loss = 4.48327 avg_loss = 3.64332\n",
      "epoch no.2 train no.215640  loss = 3.65196 avg_loss = 3.66635\n",
      "epoch no.2 train no.215650  loss = 2.45746 avg_loss = 3.64873\n",
      "epoch no.2 train no.215660  loss = 3.03444 avg_loss = 3.59762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.215670  loss = 3.50745 avg_loss = 3.60154\n",
      "epoch no.2 train no.215680  loss = 3.91611 avg_loss = 3.60386\n",
      "epoch no.2 train no.215690  loss = 2.00304 avg_loss = 3.55894\n",
      "epoch no.2 train no.215700  loss = 4.27034 avg_loss = 3.56319\n",
      "epoch no.2 train no.215710  loss = 2.89098 avg_loss = 3.55967\n",
      "epoch no.2 train no.215720  loss = 4.00445 avg_loss = 3.59691\n",
      "epoch no.2 train no.215730  loss = 3.01845 avg_loss = 3.60273\n",
      "epoch no.2 train no.215740  loss = 3.60364 avg_loss = 3.60537\n",
      "epoch no.2 train no.215750  loss = 3.86231 avg_loss = 3.66797\n",
      "epoch no.2 train no.215760  loss = 2.75297 avg_loss = 3.67277\n",
      "epoch no.2 train no.215770  loss = 3.54191 avg_loss = 3.65175\n",
      "epoch no.2 train no.215780  loss = 3.25354 avg_loss = 3.61659\n",
      "epoch no.2 train no.215790  loss = 4.66330 avg_loss = 3.63521\n",
      "epoch no.2 train no.215800  loss = 2.98914 avg_loss = 3.64738\n",
      "epoch no.2 train no.215810  loss = 4.12588 avg_loss = 3.64396\n",
      "epoch no.2 train no.215820  loss = 4.17130 avg_loss = 3.62005\n",
      "epoch no.2 train no.215830  loss = 3.23365 avg_loss = 3.64771\n",
      "epoch no.2 train no.215840  loss = 3.03685 avg_loss = 3.61991\n",
      "epoch no.2 train no.215850  loss = 2.56412 avg_loss = 3.66853\n",
      "epoch no.2 train no.215860  loss = 2.18613 avg_loss = 3.67061\n",
      "epoch no.2 train no.215870  loss = 3.20381 avg_loss = 3.63977\n",
      "epoch no.2 train no.215880  loss = 3.75729 avg_loss = 3.61087\n",
      "epoch no.2 train no.215890  loss = 3.29767 avg_loss = 3.66069\n",
      "epoch no.2 train no.215900  loss = 2.15670 avg_loss = 3.66965\n",
      "epoch no.2 train no.215910  loss = 3.92077 avg_loss = 3.66842\n",
      "epoch no.2 train no.215920  loss = 5.42133 avg_loss = 3.69925\n",
      "epoch no.2 train no.215930  loss = 3.47402 avg_loss = 3.65579\n",
      "epoch no.2 train no.215940  loss = 3.51556 avg_loss = 3.64753\n",
      "epoch no.2 train no.215950  loss = 3.21843 avg_loss = 3.68129\n",
      "epoch no.2 train no.215960  loss = 2.78997 avg_loss = 3.70991\n",
      "epoch no.2 train no.215970  loss = 4.53295 avg_loss = 3.69479\n",
      "epoch no.2 train no.215980  loss = 2.65830 avg_loss = 3.73596\n",
      "epoch no.2 train no.215990  loss = 2.28289 avg_loss = 3.70587\n",
      "epoch no.2 train no.216000  loss = 2.70524 avg_loss = 3.65718\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁필요할', '</s>']\n",
      "기분전환이 필요해</s>\n",
      "epoch no.2 train no.216010  loss = 2.14012 avg_loss = 3.69473\n",
      "epoch no.2 train no.216020  loss = 1.81308 avg_loss = 3.66157\n",
      "epoch no.2 train no.216030  loss = 5.51423 avg_loss = 3.67314\n",
      "epoch no.2 train no.216040  loss = 3.01342 avg_loss = 3.65421\n",
      "epoch no.2 train no.216050  loss = 5.43803 avg_loss = 3.66397\n",
      "epoch no.2 train no.216060  loss = 4.39090 avg_loss = 3.62530\n",
      "epoch no.2 train no.216070  loss = 3.28548 avg_loss = 3.61427\n",
      "epoch no.2 train no.216080  loss = 2.35625 avg_loss = 3.65639\n",
      "epoch no.2 train no.216090  loss = 4.44089 avg_loss = 3.69069\n",
      "epoch no.2 train no.216100  loss = 2.26514 avg_loss = 3.64486\n",
      "epoch no.2 train no.216110  loss = 3.92033 avg_loss = 3.62191\n",
      "epoch no.2 train no.216120  loss = 4.95227 avg_loss = 3.67149\n",
      "epoch no.2 train no.216130  loss = 2.37199 avg_loss = 3.66598\n",
      "epoch no.2 train no.216140  loss = 3.34108 avg_loss = 3.66884\n",
      "epoch no.2 train no.216150  loss = 3.22836 avg_loss = 3.64952\n",
      "epoch no.2 train no.216160  loss = 2.92037 avg_loss = 3.63290\n",
      "epoch no.2 train no.216170  loss = 2.92357 avg_loss = 3.59278\n",
      "epoch no.2 train no.216180  loss = 1.91891 avg_loss = 3.62517\n",
      "epoch no.2 train no.216190  loss = 2.14203 avg_loss = 3.58945\n",
      "epoch no.2 train no.216200  loss = 2.10222 avg_loss = 3.60463\n",
      "epoch no.2 train no.216210  loss = 3.57746 avg_loss = 3.58873\n",
      "epoch no.2 train no.216220  loss = 2.92161 avg_loss = 3.54046\n",
      "epoch no.2 train no.216230  loss = 4.37610 avg_loss = 3.57148\n",
      "epoch no.2 train no.216240  loss = 2.84820 avg_loss = 3.61528\n",
      "epoch no.2 train no.216250  loss = 3.88327 avg_loss = 3.53380\n",
      "epoch no.2 train no.216260  loss = 3.48547 avg_loss = 3.48833\n",
      "epoch no.2 train no.216270  loss = 2.17579 avg_loss = 3.53061\n",
      "epoch no.2 train no.216280  loss = 4.17525 avg_loss = 3.50988\n",
      "epoch no.2 train no.216290  loss = 4.05662 avg_loss = 3.51838\n",
      "epoch no.2 train no.216300  loss = 6.63896 avg_loss = 3.53001\n",
      "epoch no.2 train no.216310  loss = 2.69352 avg_loss = 3.55091\n",
      "epoch no.2 train no.216320  loss = 4.15201 avg_loss = 3.62757\n",
      "epoch no.2 train no.216330  loss = 4.03701 avg_loss = 3.61485\n",
      "epoch no.2 train no.216340  loss = 3.19876 avg_loss = 3.62776\n",
      "epoch no.2 train no.216350  loss = 2.42466 avg_loss = 3.62604\n",
      "epoch no.2 train no.216360  loss = 2.05092 avg_loss = 3.56444\n",
      "epoch no.2 train no.216370  loss = 5.08689 avg_loss = 3.56043\n",
      "epoch no.2 train no.216380  loss = 3.76809 avg_loss = 3.56470\n",
      "epoch no.2 train no.216390  loss = 3.90840 avg_loss = 3.56293\n",
      "epoch no.2 train no.216400  loss = 3.89182 avg_loss = 3.54966\n",
      "epoch no.2 train no.216410  loss = 3.61707 avg_loss = 3.56288\n",
      "epoch no.2 train no.216420  loss = 4.25232 avg_loss = 3.53597\n",
      "epoch no.2 train no.216430  loss = 5.60314 avg_loss = 3.54144\n",
      "epoch no.2 train no.216440  loss = 3.33339 avg_loss = 3.54539\n",
      "epoch no.2 train no.216450  loss = 4.14736 avg_loss = 3.58006\n",
      "epoch no.2 train no.216460  loss = 3.86835 avg_loss = 3.56765\n",
      "epoch no.2 train no.216470  loss = 3.72737 avg_loss = 3.64519\n",
      "epoch no.2 train no.216480  loss = 3.49180 avg_loss = 3.61439\n",
      "epoch no.2 train no.216490  loss = 3.00712 avg_loss = 3.62010\n",
      "epoch no.2 train no.216500  loss = 3.62411 avg_loss = 3.59538\n",
      "epoch no.2 train no.216510  loss = 3.43248 avg_loss = 3.60690\n",
      "epoch no.2 train no.216520  loss = 2.55623 avg_loss = 3.57874\n",
      "epoch no.2 train no.216530  loss = 2.09275 avg_loss = 3.56198\n",
      "epoch no.2 train no.216540  loss = 4.26233 avg_loss = 3.59662\n",
      "epoch no.2 train no.216550  loss = 4.95586 avg_loss = 3.60834\n",
      "epoch no.2 train no.216560  loss = 2.94679 avg_loss = 3.57126\n",
      "epoch no.2 train no.216570  loss = 3.40079 avg_loss = 3.56710\n",
      "epoch no.2 train no.216580  loss = 3.20995 avg_loss = 3.56133\n",
      "epoch no.2 train no.216590  loss = 3.82491 avg_loss = 3.54972\n",
      "epoch no.2 train no.216600  loss = 4.01008 avg_loss = 3.55651\n",
      "epoch no.2 train no.216610  loss = 2.58934 avg_loss = 3.56669\n",
      "epoch no.2 train no.216620  loss = 2.72030 avg_loss = 3.62279\n",
      "epoch no.2 train no.216630  loss = 3.50493 avg_loss = 3.56754\n",
      "epoch no.2 train no.216640  loss = 4.86412 avg_loss = 3.59246\n",
      "epoch no.2 train no.216650  loss = 4.08410 avg_loss = 3.59074\n",
      "epoch no.2 train no.216660  loss = 4.14264 avg_loss = 3.59273\n",
      "epoch no.2 train no.216670  loss = 3.32494 avg_loss = 3.55150\n",
      "epoch no.2 train no.216680  loss = 6.22429 avg_loss = 3.57839\n",
      "epoch no.2 train no.216690  loss = 3.34558 avg_loss = 3.52849\n",
      "epoch no.2 train no.216700  loss = 2.72551 avg_loss = 3.51085\n",
      "epoch no.2 train no.216710  loss = 3.06944 avg_loss = 3.53820\n",
      "epoch no.2 train no.216720  loss = 3.34996 avg_loss = 3.52527\n",
      "epoch no.2 train no.216730  loss = 2.54658 avg_loss = 3.54340\n",
      "epoch no.2 train no.216740  loss = 3.75036 avg_loss = 3.61217\n",
      "epoch no.2 train no.216750  loss = 3.35509 avg_loss = 3.59407\n",
      "epoch no.2 train no.216760  loss = 4.22604 avg_loss = 3.56545\n",
      "epoch no.2 train no.216770  loss = 2.99928 avg_loss = 3.56237\n",
      "epoch no.2 train no.216780  loss = 4.11072 avg_loss = 3.61078\n",
      "epoch no.2 train no.216790  loss = 4.31643 avg_loss = 3.60620\n",
      "epoch no.2 train no.216800  loss = 3.84596 avg_loss = 3.65763\n",
      "epoch no.2 train no.216810  loss = 3.96346 avg_loss = 3.64378\n",
      "epoch no.2 train no.216820  loss = 3.70590 avg_loss = 3.57564\n",
      "epoch no.2 train no.216830  loss = 4.36185 avg_loss = 3.56832\n",
      "epoch no.2 train no.216840  loss = 3.55705 avg_loss = 3.57355\n",
      "epoch no.2 train no.216850  loss = 4.12537 avg_loss = 3.55929\n",
      "epoch no.2 train no.216860  loss = 4.29863 avg_loss = 3.57867\n",
      "epoch no.2 train no.216870  loss = 3.20433 avg_loss = 3.55503\n",
      "epoch no.2 train no.216880  loss = 2.97570 avg_loss = 3.57876\n",
      "epoch no.2 train no.216890  loss = 5.02037 avg_loss = 3.60658\n",
      "epoch no.2 train no.216900  loss = 2.90342 avg_loss = 3.61732\n",
      "epoch no.2 train no.216910  loss = 2.20715 avg_loss = 3.55980\n",
      "epoch no.2 train no.216920  loss = 2.56587 avg_loss = 3.54824\n",
      "epoch no.2 train no.216930  loss = 1.96966 avg_loss = 3.50850\n",
      "epoch no.2 train no.216940  loss = 3.35098 avg_loss = 3.49072\n",
      "epoch no.2 train no.216950  loss = 3.35149 avg_loss = 3.54366\n",
      "epoch no.2 train no.216960  loss = 5.59670 avg_loss = 3.53188\n",
      "epoch no.2 train no.216970  loss = 2.19721 avg_loss = 3.51449\n",
      "epoch no.2 train no.216980  loss = 2.77700 avg_loss = 3.50834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.216990  loss = 2.71122 avg_loss = 3.49941\n",
      "epoch no.2 train no.217000  loss = 3.14299 avg_loss = 3.54750\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.2 train no.217010  loss = 2.53241 avg_loss = 3.53874\n",
      "epoch no.2 train no.217020  loss = 2.79233 avg_loss = 3.50813\n",
      "epoch no.2 train no.217030  loss = 3.28856 avg_loss = 3.46519\n",
      "epoch no.2 train no.217040  loss = 4.13445 avg_loss = 3.44462\n",
      "epoch no.2 train no.217050  loss = 2.25597 avg_loss = 3.48835\n",
      "epoch no.2 train no.217060  loss = 4.20148 avg_loss = 3.49972\n",
      "epoch no.2 train no.217070  loss = 4.30713 avg_loss = 3.47231\n",
      "epoch no.2 train no.217080  loss = 3.33483 avg_loss = 3.49312\n",
      "epoch no.2 train no.217090  loss = 3.08078 avg_loss = 3.44419\n",
      "epoch no.2 train no.217100  loss = 4.31517 avg_loss = 3.49003\n",
      "epoch no.2 train no.217110  loss = 5.96698 avg_loss = 3.54432\n",
      "epoch no.2 train no.217120  loss = 2.34784 avg_loss = 3.55557\n",
      "epoch no.2 train no.217130  loss = 5.07428 avg_loss = 3.58427\n",
      "epoch no.2 train no.217140  loss = 2.67803 avg_loss = 3.59736\n",
      "epoch no.2 train no.217150  loss = 5.21719 avg_loss = 3.61031\n",
      "epoch no.2 train no.217160  loss = 4.33035 avg_loss = 3.57234\n",
      "epoch no.2 train no.217170  loss = 4.54244 avg_loss = 3.56414\n",
      "epoch no.2 train no.217180  loss = 2.90269 avg_loss = 3.58601\n",
      "epoch no.2 train no.217190  loss = 5.41460 avg_loss = 3.64014\n",
      "epoch no.2 train no.217200  loss = 1.99691 avg_loss = 3.61528\n",
      "epoch no.2 train no.217210  loss = 3.37307 avg_loss = 3.56536\n",
      "epoch no.2 train no.217220  loss = 3.09699 avg_loss = 3.56236\n",
      "epoch no.2 train no.217230  loss = 2.78830 avg_loss = 3.59175\n",
      "epoch no.2 train no.217240  loss = 3.59849 avg_loss = 3.60192\n",
      "epoch no.2 train no.217250  loss = 3.54418 avg_loss = 3.64752\n",
      "epoch no.2 train no.217260  loss = 2.87117 avg_loss = 3.65766\n",
      "epoch no.2 train no.217270  loss = 4.24035 avg_loss = 3.65441\n",
      "epoch no.2 train no.217280  loss = 3.17623 avg_loss = 3.61374\n",
      "epoch no.2 train no.217290  loss = 4.20549 avg_loss = 3.64319\n",
      "epoch no.2 train no.217300  loss = 3.82134 avg_loss = 3.65429\n",
      "epoch no.2 train no.217310  loss = 3.24846 avg_loss = 3.65747\n",
      "epoch no.2 train no.217320  loss = 2.09906 avg_loss = 3.62971\n",
      "epoch no.2 train no.217330  loss = 2.74505 avg_loss = 3.62722\n",
      "epoch no.2 train no.217340  loss = 2.45083 avg_loss = 3.59440\n",
      "epoch no.2 train no.217350  loss = 4.82778 avg_loss = 3.58837\n",
      "epoch no.2 train no.217360  loss = 1.92691 avg_loss = 3.54266\n",
      "epoch no.2 train no.217370  loss = 3.97416 avg_loss = 3.51895\n",
      "epoch no.2 train no.217380  loss = 4.18861 avg_loss = 3.58206\n",
      "epoch no.2 train no.217390  loss = 2.69816 avg_loss = 3.62032\n",
      "epoch no.2 train no.217400  loss = 3.30798 avg_loss = 3.59504\n",
      "epoch no.2 train no.217410  loss = 5.89928 avg_loss = 3.61165\n",
      "epoch no.2 train no.217420  loss = 3.66100 avg_loss = 3.56102\n",
      "epoch no.2 train no.217430  loss = 3.21141 avg_loss = 3.51105\n",
      "epoch no.2 train no.217440  loss = 2.74135 avg_loss = 3.47595\n",
      "epoch no.2 train no.217450  loss = 3.83152 avg_loss = 3.51436\n",
      "epoch no.2 train no.217460  loss = 6.23080 avg_loss = 3.58349\n",
      "epoch no.2 train no.217470  loss = 2.40109 avg_loss = 3.59391\n",
      "epoch no.2 train no.217480  loss = 3.85988 avg_loss = 3.60255\n",
      "epoch no.2 train no.217490  loss = 4.05730 avg_loss = 3.62781\n",
      "epoch no.2 train no.217500  loss = 2.48272 avg_loss = 3.56913\n",
      "epoch no.2 train no.217510  loss = 3.52532 avg_loss = 3.56710\n",
      "epoch no.2 train no.217520  loss = 3.69082 avg_loss = 3.54455\n",
      "epoch no.2 train no.217530  loss = 2.54095 avg_loss = 3.52266\n",
      "epoch no.2 train no.217540  loss = 5.57577 avg_loss = 3.51897\n",
      "epoch no.2 train no.217550  loss = 2.24977 avg_loss = 3.49422\n",
      "epoch no.2 train no.217560  loss = 2.07448 avg_loss = 3.46062\n",
      "epoch no.2 train no.217570  loss = 4.77355 avg_loss = 3.49210\n",
      "epoch no.2 train no.217580  loss = 2.57008 avg_loss = 3.46213\n",
      "epoch no.2 train no.217590  loss = 5.93172 avg_loss = 3.50394\n",
      "epoch no.2 train no.217600  loss = 3.53732 avg_loss = 3.46147\n",
      "epoch no.2 train no.217610  loss = 3.14434 avg_loss = 3.48318\n",
      "epoch no.2 train no.217620  loss = 2.13471 avg_loss = 3.46925\n",
      "epoch no.2 train no.217630  loss = 2.78780 avg_loss = 3.44287\n",
      "epoch no.2 train no.217640  loss = 2.81146 avg_loss = 3.46656\n",
      "epoch no.2 train no.217650  loss = 2.78626 avg_loss = 3.47380\n",
      "epoch no.2 train no.217660  loss = 3.84028 avg_loss = 3.54162\n",
      "epoch no.2 train no.217670  loss = 1.54457 avg_loss = 3.48447\n",
      "epoch no.2 train no.217680  loss = 4.45024 avg_loss = 3.51020\n",
      "epoch no.2 train no.217690  loss = 3.29881 avg_loss = 3.55168\n",
      "epoch no.2 train no.217700  loss = 3.17165 avg_loss = 3.55840\n",
      "epoch no.2 train no.217710  loss = 4.40595 avg_loss = 3.54533\n",
      "epoch no.2 train no.217720  loss = 3.18067 avg_loss = 3.52092\n",
      "epoch no.2 train no.217730  loss = 4.11425 avg_loss = 3.51839\n",
      "epoch no.2 train no.217740  loss = 5.06991 avg_loss = 3.56281\n",
      "epoch no.2 train no.217750  loss = 3.50097 avg_loss = 3.51732\n",
      "epoch no.2 train no.217760  loss = 4.08391 avg_loss = 3.52458\n",
      "epoch no.2 train no.217770  loss = 2.75258 avg_loss = 3.47687\n",
      "epoch no.2 train no.217780  loss = 4.69411 avg_loss = 3.48392\n",
      "epoch no.2 train no.217790  loss = 4.46465 avg_loss = 3.47073\n",
      "epoch no.2 train no.217800  loss = 4.48797 avg_loss = 3.47751\n",
      "epoch no.2 train no.217810  loss = 4.56725 avg_loss = 3.47925\n",
      "epoch no.2 train no.217820  loss = 4.47118 avg_loss = 3.48284\n",
      "epoch no.2 train no.217830  loss = 2.98226 avg_loss = 3.51348\n",
      "epoch no.2 train no.217840  loss = 4.93025 avg_loss = 3.55382\n",
      "epoch no.2 train no.217850  loss = 3.05718 avg_loss = 3.53429\n",
      "epoch no.2 train no.217860  loss = 4.07616 avg_loss = 3.56112\n",
      "epoch no.2 train no.217870  loss = 4.04007 avg_loss = 3.53842\n",
      "epoch no.2 train no.217880  loss = 3.07103 avg_loss = 3.53800\n",
      "epoch no.2 train no.217890  loss = 4.76488 avg_loss = 3.63553\n",
      "epoch no.2 train no.217900  loss = 2.31877 avg_loss = 3.65286\n",
      "epoch no.2 train no.217910  loss = 2.83870 avg_loss = 3.62612\n",
      "epoch no.2 train no.217920  loss = 4.11838 avg_loss = 3.60194\n",
      "epoch no.2 train no.217930  loss = 2.57174 avg_loss = 3.60295\n",
      "epoch no.2 train no.217940  loss = 6.81214 avg_loss = 3.63815\n",
      "epoch no.2 train no.217950  loss = 2.62568 avg_loss = 3.60608\n",
      "epoch no.2 train no.217960  loss = 2.85841 avg_loss = 3.65208\n",
      "epoch no.2 train no.217970  loss = 4.92813 avg_loss = 3.65228\n",
      "epoch no.2 train no.217980  loss = 2.99850 avg_loss = 3.65472\n",
      "epoch no.2 train no.217990  loss = 2.30205 avg_loss = 3.62812\n",
      "epoch no.2 train no.218000  loss = 2.22960 avg_loss = 3.57481\n",
      "3\n",
      "to_tokens: ['▁가을', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.218010  loss = 2.89376 avg_loss = 3.56636\n",
      "epoch no.2 train no.218020  loss = 3.84518 avg_loss = 3.57224\n",
      "epoch no.2 train no.218030  loss = 4.76530 avg_loss = 3.56584\n",
      "epoch no.2 train no.218040  loss = 5.22073 avg_loss = 3.58309\n",
      "epoch no.2 train no.218050  loss = 4.51020 avg_loss = 3.57853\n",
      "epoch no.2 train no.218060  loss = 3.92969 avg_loss = 3.58485\n",
      "epoch no.2 train no.218070  loss = 5.14265 avg_loss = 3.59212\n",
      "epoch no.2 train no.218080  loss = 3.12486 avg_loss = 3.59664\n",
      "epoch no.2 train no.218090  loss = 2.37825 avg_loss = 3.61545\n",
      "epoch no.2 train no.218100  loss = 2.92869 avg_loss = 3.62538\n",
      "epoch no.2 train no.218110  loss = 2.89125 avg_loss = 3.58042\n",
      "epoch no.2 train no.218120  loss = 1.27582 avg_loss = 3.55503\n",
      "epoch no.2 train no.218130  loss = 2.67032 avg_loss = 3.51736\n",
      "epoch no.2 train no.218140  loss = 3.69751 avg_loss = 3.57121\n",
      "epoch no.2 train no.218150  loss = 3.54105 avg_loss = 3.59123\n",
      "epoch no.2 train no.218160  loss = 5.01384 avg_loss = 3.58406\n",
      "epoch no.2 train no.218170  loss = 5.34809 avg_loss = 3.60908\n",
      "epoch no.2 train no.218180  loss = 4.42158 avg_loss = 3.60483\n",
      "epoch no.2 train no.218190  loss = 2.51602 avg_loss = 3.60122\n",
      "epoch no.2 train no.218200  loss = 2.27732 avg_loss = 3.60641\n",
      "epoch no.2 train no.218210  loss = 4.66556 avg_loss = 3.58954\n",
      "epoch no.2 train no.218220  loss = 3.37305 avg_loss = 3.57764\n",
      "epoch no.2 train no.218230  loss = 2.60413 avg_loss = 3.56405\n",
      "epoch no.2 train no.218240  loss = 4.50796 avg_loss = 3.56493\n",
      "epoch no.2 train no.218250  loss = 3.77644 avg_loss = 3.57320\n",
      "epoch no.2 train no.218260  loss = 1.99966 avg_loss = 3.56769\n",
      "epoch no.2 train no.218270  loss = 2.72393 avg_loss = 3.58589\n",
      "epoch no.2 train no.218280  loss = 3.26114 avg_loss = 3.65615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.218290  loss = 3.90077 avg_loss = 3.66610\n",
      "epoch no.2 train no.218300  loss = 2.82105 avg_loss = 3.65934\n",
      "epoch no.2 train no.218310  loss = 2.92969 avg_loss = 3.68031\n",
      "epoch no.2 train no.218320  loss = 6.32741 avg_loss = 3.72556\n",
      "epoch no.2 train no.218330  loss = 2.67373 avg_loss = 3.67859\n",
      "epoch no.2 train no.218340  loss = 5.56674 avg_loss = 3.67023\n",
      "epoch no.2 train no.218350  loss = 4.18417 avg_loss = 3.67652\n",
      "epoch no.2 train no.218360  loss = 3.88490 avg_loss = 3.67109\n",
      "epoch no.2 train no.218370  loss = 3.21313 avg_loss = 3.68283\n",
      "epoch no.2 train no.218380  loss = 2.64655 avg_loss = 3.64141\n",
      "epoch no.2 train no.218390  loss = 2.75533 avg_loss = 3.68880\n",
      "epoch no.2 train no.218400  loss = 4.80390 avg_loss = 3.72615\n",
      "epoch no.2 train no.218410  loss = 3.03305 avg_loss = 3.67330\n",
      "epoch no.2 train no.218420  loss = 4.05680 avg_loss = 3.75642\n",
      "epoch no.2 train no.218430  loss = 3.19211 avg_loss = 3.70283\n",
      "epoch no.2 train no.218440  loss = 2.16783 avg_loss = 3.67726\n",
      "epoch no.2 train no.218450  loss = 3.58619 avg_loss = 3.65679\n",
      "epoch no.2 train no.218460  loss = 4.05339 avg_loss = 3.61047\n",
      "epoch no.2 train no.218470  loss = 3.98242 avg_loss = 3.63331\n",
      "epoch no.2 train no.218480  loss = 4.88254 avg_loss = 3.70884\n",
      "epoch no.2 train no.218490  loss = 3.69997 avg_loss = 3.66169\n",
      "epoch no.2 train no.218500  loss = 3.19737 avg_loss = 3.62241\n",
      "epoch no.2 train no.218510  loss = 4.16795 avg_loss = 3.68830\n",
      "epoch no.2 train no.218520  loss = 3.51391 avg_loss = 3.64854\n",
      "epoch no.2 train no.218530  loss = 3.33438 avg_loss = 3.63087\n",
      "epoch no.2 train no.218540  loss = 2.81668 avg_loss = 3.61953\n",
      "epoch no.2 train no.218550  loss = 4.85250 avg_loss = 3.63479\n",
      "epoch no.2 train no.218560  loss = 2.98132 avg_loss = 3.63222\n",
      "epoch no.2 train no.218570  loss = 4.49851 avg_loss = 3.62075\n",
      "epoch no.2 train no.218580  loss = 2.26671 avg_loss = 3.57060\n",
      "epoch no.2 train no.218590  loss = 3.20874 avg_loss = 3.60664\n",
      "epoch no.2 train no.218600  loss = 2.89331 avg_loss = 3.59656\n",
      "epoch no.2 train no.218610  loss = 3.90843 avg_loss = 3.57095\n",
      "epoch no.2 train no.218620  loss = 2.66464 avg_loss = 3.58024\n",
      "epoch no.2 train no.218630  loss = 3.63859 avg_loss = 3.57034\n",
      "epoch no.2 train no.218640  loss = 2.30808 avg_loss = 3.53616\n",
      "epoch no.2 train no.218650  loss = 4.14804 avg_loss = 3.56729\n",
      "epoch no.2 train no.218660  loss = 2.04379 avg_loss = 3.56162\n",
      "epoch no.2 train no.218670  loss = 3.03484 avg_loss = 3.57805\n",
      "epoch no.2 train no.218680  loss = 2.42370 avg_loss = 3.55478\n",
      "epoch no.2 train no.218690  loss = 4.95308 avg_loss = 3.60728\n",
      "epoch no.2 train no.218700  loss = 4.07006 avg_loss = 3.63593\n",
      "epoch no.2 train no.218710  loss = 3.62541 avg_loss = 3.62245\n",
      "epoch no.2 train no.218720  loss = 5.01500 avg_loss = 3.63264\n",
      "epoch no.2 train no.218730  loss = 4.39514 avg_loss = 3.64184\n",
      "epoch no.2 train no.218740  loss = 3.62160 avg_loss = 3.65162\n",
      "epoch no.2 train no.218750  loss = 5.92595 avg_loss = 3.70573\n",
      "epoch no.2 train no.218760  loss = 2.67810 avg_loss = 3.68204\n",
      "epoch no.2 train no.218770  loss = 2.98988 avg_loss = 3.62351\n",
      "epoch no.2 train no.218780  loss = 4.89867 avg_loss = 3.65524\n",
      "epoch no.2 train no.218790  loss = 3.62998 avg_loss = 3.58413\n",
      "epoch no.2 train no.218800  loss = 2.58682 avg_loss = 3.65664\n",
      "epoch no.2 train no.218810  loss = 4.08918 avg_loss = 3.68838\n",
      "epoch no.2 train no.218820  loss = 3.24308 avg_loss = 3.64718\n",
      "epoch no.2 train no.218830  loss = 5.16353 avg_loss = 3.60112\n",
      "epoch no.2 train no.218840  loss = 3.21963 avg_loss = 3.58963\n",
      "epoch no.2 train no.218850  loss = 3.56196 avg_loss = 3.58982\n",
      "epoch no.2 train no.218860  loss = 3.20448 avg_loss = 3.56991\n",
      "epoch no.2 train no.218870  loss = 4.95075 avg_loss = 3.60172\n",
      "epoch no.2 train no.218880  loss = 4.18748 avg_loss = 3.61915\n",
      "epoch no.2 train no.218890  loss = 4.48449 avg_loss = 3.57874\n",
      "epoch no.2 train no.218900  loss = 2.59797 avg_loss = 3.57951\n",
      "epoch no.2 train no.218910  loss = 2.23556 avg_loss = 3.54534\n",
      "epoch no.2 train no.218920  loss = 2.05982 avg_loss = 3.53892\n",
      "epoch no.2 train no.218930  loss = 4.08694 avg_loss = 3.53912\n",
      "epoch no.2 train no.218940  loss = 5.53547 avg_loss = 3.61029\n",
      "epoch no.2 train no.218950  loss = 4.25365 avg_loss = 3.67012\n",
      "epoch no.2 train no.218960  loss = 6.33049 avg_loss = 3.71857\n",
      "epoch no.2 train no.218970  loss = 3.94746 avg_loss = 3.73367\n",
      "epoch no.2 train no.218980  loss = 4.34512 avg_loss = 3.73964\n",
      "epoch no.2 train no.218990  loss = 4.50817 avg_loss = 3.72148\n",
      "epoch no.2 train no.219000  loss = 3.38686 avg_loss = 3.70925\n",
      "7\n",
      "to_tokens: ['▁라디오', '좋은', '에', '▁필요할', '때', '▁듣는', '▁신나는', '▁노래', '</s>', '</s>']\n",
      "기분전환이 필요할때 듣는 신나는 노래들</s>\n",
      "epoch no.2 train no.219010  loss = 3.87782 avg_loss = 3.68553\n",
      "epoch no.2 train no.219020  loss = 1.93944 avg_loss = 3.63561\n",
      "epoch no.2 train no.219030  loss = 2.39569 avg_loss = 3.62487\n",
      "epoch no.2 train no.219040  loss = 3.77402 avg_loss = 3.60328\n",
      "epoch no.2 train no.219050  loss = 4.96874 avg_loss = 3.64127\n",
      "epoch no.2 train no.219060  loss = 2.69742 avg_loss = 3.63396\n",
      "epoch no.2 train no.219070  loss = 3.77586 avg_loss = 3.57982\n",
      "epoch no.2 train no.219080  loss = 3.19798 avg_loss = 3.56588\n",
      "epoch no.2 train no.219090  loss = 4.50219 avg_loss = 3.57866\n",
      "epoch no.2 train no.219100  loss = 3.46013 avg_loss = 3.60166\n",
      "epoch no.2 train no.219110  loss = 3.02218 avg_loss = 3.62402\n",
      "epoch no.2 train no.219120  loss = 3.15812 avg_loss = 3.62845\n",
      "epoch no.2 train no.219130  loss = 2.14885 avg_loss = 3.61679\n",
      "epoch no.2 train no.219140  loss = 1.82827 avg_loss = 3.62954\n",
      "epoch no.2 train no.219150  loss = 4.81619 avg_loss = 3.69467\n",
      "epoch no.2 train no.219160  loss = 4.48602 avg_loss = 3.67485\n",
      "epoch no.2 train no.219170  loss = 2.80044 avg_loss = 3.65720\n",
      "epoch no.2 train no.219180  loss = 3.88460 avg_loss = 3.63084\n",
      "epoch no.2 train no.219190  loss = 2.17583 avg_loss = 3.61652\n",
      "epoch no.2 train no.219200  loss = 4.64435 avg_loss = 3.66308\n",
      "epoch no.2 train no.219210  loss = 3.93697 avg_loss = 3.62244\n",
      "epoch no.2 train no.219220  loss = 3.72255 avg_loss = 3.65983\n",
      "epoch no.2 train no.219230  loss = 4.04350 avg_loss = 3.68407\n",
      "epoch no.2 train no.219240  loss = 3.69821 avg_loss = 3.70081\n",
      "epoch no.2 train no.219250  loss = 2.58899 avg_loss = 3.72790\n",
      "epoch no.2 train no.219260  loss = 3.42749 avg_loss = 3.75318\n",
      "epoch no.2 train no.219270  loss = 4.10889 avg_loss = 3.73972\n",
      "epoch no.2 train no.219280  loss = 4.38996 avg_loss = 3.74963\n",
      "epoch no.2 train no.219290  loss = 3.45710 avg_loss = 3.76646\n",
      "epoch no.2 train no.219300  loss = 2.85764 avg_loss = 3.70792\n",
      "epoch no.2 train no.219310  loss = 2.70123 avg_loss = 3.69099\n",
      "epoch no.2 train no.219320  loss = 3.43973 avg_loss = 3.70683\n",
      "epoch no.2 train no.219330  loss = 2.71663 avg_loss = 3.69060\n",
      "epoch no.2 train no.219340  loss = 2.76495 avg_loss = 3.70013\n",
      "epoch no.2 train no.219350  loss = 4.06840 avg_loss = 3.67998\n",
      "epoch no.2 train no.219360  loss = 2.33605 avg_loss = 3.64269\n",
      "epoch no.2 train no.219370  loss = 2.89014 avg_loss = 3.61819\n",
      "epoch no.2 train no.219380  loss = 2.63303 avg_loss = 3.65761\n",
      "epoch no.2 train no.219390  loss = 4.39654 avg_loss = 3.63491\n",
      "epoch no.2 train no.219400  loss = 4.88619 avg_loss = 3.64414\n",
      "epoch no.2 train no.219410  loss = 2.28200 avg_loss = 3.60555\n",
      "epoch no.2 train no.219420  loss = 2.34616 avg_loss = 3.62667\n",
      "epoch no.2 train no.219430  loss = 3.80372 avg_loss = 3.64908\n",
      "epoch no.2 train no.219440  loss = 4.05385 avg_loss = 3.64968\n",
      "epoch no.2 train no.219450  loss = 6.13570 avg_loss = 3.67191\n",
      "epoch no.2 train no.219460  loss = 5.50677 avg_loss = 3.65045\n",
      "epoch no.2 train no.219470  loss = 4.89493 avg_loss = 3.68578\n",
      "epoch no.2 train no.219480  loss = 3.91810 avg_loss = 3.68060\n",
      "epoch no.2 train no.219490  loss = 2.96881 avg_loss = 3.70297\n",
      "epoch no.2 train no.219500  loss = 5.35124 avg_loss = 3.73913\n",
      "epoch no.2 train no.219510  loss = 3.53788 avg_loss = 3.77013\n",
      "epoch no.2 train no.219520  loss = 2.12865 avg_loss = 3.76064\n",
      "epoch no.2 train no.219530  loss = 3.71970 avg_loss = 3.74112\n",
      "epoch no.2 train no.219540  loss = 5.82860 avg_loss = 3.75441\n",
      "epoch no.2 train no.219550  loss = 4.61101 avg_loss = 3.76760\n",
      "epoch no.2 train no.219560  loss = 5.04824 avg_loss = 3.80006\n",
      "epoch no.2 train no.219570  loss = 3.86454 avg_loss = 3.80718\n",
      "epoch no.2 train no.219580  loss = 3.25269 avg_loss = 3.77051\n",
      "epoch no.2 train no.219590  loss = 5.25372 avg_loss = 3.76306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.219600  loss = 4.80972 avg_loss = 3.74373\n",
      "epoch no.2 train no.219610  loss = 1.96540 avg_loss = 3.69292\n",
      "epoch no.2 train no.219620  loss = 3.34624 avg_loss = 3.71175\n",
      "epoch no.2 train no.219630  loss = 3.39107 avg_loss = 3.69801\n",
      "epoch no.2 train no.219640  loss = 3.07043 avg_loss = 3.72403\n",
      "epoch no.2 train no.219650  loss = 4.40208 avg_loss = 3.74121\n",
      "epoch no.2 train no.219660  loss = 2.51627 avg_loss = 3.73457\n",
      "epoch no.2 train no.219670  loss = 3.14481 avg_loss = 3.68432\n",
      "epoch no.2 train no.219680  loss = 3.56181 avg_loss = 3.70541\n",
      "epoch no.2 train no.219690  loss = 2.82202 avg_loss = 3.65947\n",
      "epoch no.2 train no.219700  loss = 2.08219 avg_loss = 3.58031\n",
      "epoch no.2 train no.219710  loss = 3.13052 avg_loss = 3.63082\n",
      "epoch no.2 train no.219720  loss = 1.94819 avg_loss = 3.63293\n",
      "epoch no.2 train no.219730  loss = 5.18454 avg_loss = 3.64476\n",
      "epoch no.2 train no.219740  loss = 3.78704 avg_loss = 3.65812\n",
      "epoch no.2 train no.219750  loss = 5.42130 avg_loss = 3.69591\n",
      "epoch no.2 train no.219760  loss = 3.87407 avg_loss = 3.69589\n",
      "epoch no.2 train no.219770  loss = 2.21350 avg_loss = 3.71848\n",
      "epoch no.2 train no.219780  loss = 4.67997 avg_loss = 3.67892\n",
      "epoch no.2 train no.219790  loss = 3.01912 avg_loss = 3.67723\n",
      "epoch no.2 train no.219800  loss = 3.62489 avg_loss = 3.65022\n",
      "epoch no.2 train no.219810  loss = 2.56645 avg_loss = 3.60145\n",
      "epoch no.2 train no.219820  loss = 5.45802 avg_loss = 3.66385\n",
      "epoch no.2 train no.219830  loss = 3.58649 avg_loss = 3.66111\n",
      "epoch no.2 train no.219840  loss = 3.76161 avg_loss = 3.66374\n",
      "epoch no.2 train no.219850  loss = 3.65036 avg_loss = 3.61261\n",
      "epoch no.2 train no.219860  loss = 3.44956 avg_loss = 3.62954\n",
      "epoch no.2 train no.219870  loss = 3.50496 avg_loss = 3.64168\n",
      "epoch no.2 train no.219880  loss = 2.75198 avg_loss = 3.68111\n",
      "epoch no.2 train no.219890  loss = 4.22583 avg_loss = 3.65615\n",
      "epoch no.2 train no.219900  loss = 3.01702 avg_loss = 3.57984\n",
      "epoch no.2 train no.219910  loss = 4.26374 avg_loss = 3.55650\n",
      "epoch no.2 train no.219920  loss = 4.64508 avg_loss = 3.59661\n",
      "epoch no.2 train no.219930  loss = 5.05026 avg_loss = 3.55981\n",
      "epoch no.2 train no.219940  loss = 3.93464 avg_loss = 3.56866\n",
      "epoch no.2 train no.219950  loss = 5.04202 avg_loss = 3.61107\n",
      "epoch no.2 train no.219960  loss = 2.69153 avg_loss = 3.56315\n",
      "epoch no.2 train no.219970  loss = 4.14862 avg_loss = 3.55342\n",
      "epoch no.2 train no.219980  loss = 3.85529 avg_loss = 3.59956\n",
      "epoch no.2 train no.219990  loss = 5.05542 avg_loss = 3.58445\n",
      "epoch no.2 train no.220000  loss = 6.80353 avg_loss = 3.68242\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '을', '▁위한', '▁신나는', '전환', '▁음악', '</s>', '</s>']\n",
      "기분전환을 위한 기분전환 팝송</s>\n",
      "epoch no.2 train no.220010  loss = 2.74045 avg_loss = 3.70729\n",
      "epoch no.2 train no.220020  loss = 5.51881 avg_loss = 3.71749\n",
      "epoch no.2 train no.220030  loss = 3.70922 avg_loss = 3.70309\n",
      "epoch no.2 train no.220040  loss = 5.50365 avg_loss = 3.69502\n",
      "epoch no.2 train no.220050  loss = 3.21613 avg_loss = 3.68986\n",
      "epoch no.2 train no.220060  loss = 4.45800 avg_loss = 3.62690\n",
      "epoch no.2 train no.220070  loss = 5.46120 avg_loss = 3.63974\n",
      "epoch no.2 train no.220080  loss = 3.80742 avg_loss = 3.58506\n",
      "epoch no.2 train no.220090  loss = 2.57959 avg_loss = 3.56569\n",
      "epoch no.2 train no.220100  loss = 2.32892 avg_loss = 3.52558\n",
      "epoch no.2 train no.220110  loss = 1.70080 avg_loss = 3.56886\n",
      "epoch no.2 train no.220120  loss = 3.60602 avg_loss = 3.56458\n",
      "epoch no.2 train no.220130  loss = 2.95398 avg_loss = 3.62735\n",
      "epoch no.2 train no.220140  loss = 4.07511 avg_loss = 3.61604\n",
      "epoch no.2 train no.220150  loss = 2.97254 avg_loss = 3.60776\n",
      "epoch no.2 train no.220160  loss = 2.56863 avg_loss = 3.55331\n",
      "epoch no.2 train no.220170  loss = 3.95195 avg_loss = 3.54533\n",
      "epoch no.2 train no.220180  loss = 4.52866 avg_loss = 3.58309\n",
      "epoch no.2 train no.220190  loss = 2.92405 avg_loss = 3.65453\n",
      "epoch no.2 train no.220200  loss = 4.66662 avg_loss = 3.70817\n",
      "epoch no.2 train no.220210  loss = 3.72634 avg_loss = 3.66290\n",
      "epoch no.2 train no.220220  loss = 4.67892 avg_loss = 3.64148\n",
      "epoch no.2 train no.220230  loss = 3.21685 avg_loss = 3.64200\n",
      "epoch no.2 train no.220240  loss = 3.25988 avg_loss = 3.58432\n",
      "epoch no.2 train no.220250  loss = 3.63876 avg_loss = 3.55127\n",
      "epoch no.2 train no.220260  loss = 2.65210 avg_loss = 3.55159\n",
      "epoch no.2 train no.220270  loss = 4.50822 avg_loss = 3.62179\n",
      "epoch no.2 train no.220280  loss = 2.16661 avg_loss = 3.57996\n",
      "epoch no.2 train no.220290  loss = 6.61576 avg_loss = 3.61999\n",
      "epoch no.2 train no.220300  loss = 3.38630 avg_loss = 3.61077\n",
      "epoch no.2 train no.220310  loss = 3.17519 avg_loss = 3.60625\n",
      "epoch no.2 train no.220320  loss = 2.73070 avg_loss = 3.59793\n",
      "epoch no.2 train no.220330  loss = 6.73031 avg_loss = 3.59368\n",
      "epoch no.2 train no.220340  loss = 2.93306 avg_loss = 3.56117\n",
      "epoch no.2 train no.220350  loss = 2.65227 avg_loss = 3.54588\n",
      "epoch no.2 train no.220360  loss = 1.06015 avg_loss = 3.50396\n",
      "epoch no.2 train no.220370  loss = 3.25018 avg_loss = 3.50981\n",
      "epoch no.2 train no.220380  loss = 3.83079 avg_loss = 3.55025\n",
      "epoch no.2 train no.220390  loss = 2.75305 avg_loss = 3.52431\n",
      "epoch no.2 train no.220400  loss = 5.36636 avg_loss = 3.57889\n",
      "epoch no.2 train no.220410  loss = 3.51629 avg_loss = 3.61607\n",
      "epoch no.2 train no.220420  loss = 3.04313 avg_loss = 3.62263\n",
      "epoch no.2 train no.220430  loss = 6.13907 avg_loss = 3.66403\n",
      "epoch no.2 train no.220440  loss = 4.04160 avg_loss = 3.67067\n",
      "epoch no.2 train no.220450  loss = 3.08682 avg_loss = 3.65427\n",
      "epoch no.2 train no.220460  loss = 3.21844 avg_loss = 3.67478\n",
      "epoch no.2 train no.220470  loss = 3.84701 avg_loss = 3.64700\n",
      "epoch no.2 train no.220480  loss = 2.62676 avg_loss = 3.64326\n",
      "epoch no.2 train no.220490  loss = 3.56635 avg_loss = 3.62420\n",
      "epoch no.2 train no.220500  loss = 5.87225 avg_loss = 3.64850\n",
      "epoch no.2 train no.220510  loss = 2.53258 avg_loss = 3.66050\n",
      "epoch no.2 train no.220520  loss = 5.58585 avg_loss = 3.68973\n",
      "epoch no.2 train no.220530  loss = 3.30311 avg_loss = 3.69893\n",
      "epoch no.2 train no.220540  loss = 4.31318 avg_loss = 3.68971\n",
      "epoch no.2 train no.220550  loss = 2.28417 avg_loss = 3.66364\n",
      "epoch no.2 train no.220560  loss = 2.49735 avg_loss = 3.60296\n",
      "epoch no.2 train no.220570  loss = 2.93251 avg_loss = 3.64143\n",
      "epoch no.2 train no.220580  loss = 3.12416 avg_loss = 3.64695\n",
      "epoch no.2 train no.220590  loss = 1.48410 avg_loss = 3.64500\n",
      "epoch no.2 train no.220600  loss = 2.51820 avg_loss = 3.66567\n",
      "epoch no.2 train no.220610  loss = 3.13145 avg_loss = 3.64957\n",
      "epoch no.2 train no.220620  loss = 3.90488 avg_loss = 3.68348\n",
      "epoch no.2 train no.220630  loss = 3.32726 avg_loss = 3.68640\n",
      "epoch no.2 train no.220640  loss = 1.53523 avg_loss = 3.65376\n",
      "epoch no.2 train no.220650  loss = 4.08793 avg_loss = 3.61890\n",
      "epoch no.2 train no.220660  loss = 2.88529 avg_loss = 3.60784\n",
      "epoch no.2 train no.220670  loss = 2.23805 avg_loss = 3.57320\n",
      "epoch no.2 train no.220680  loss = 4.46500 avg_loss = 3.60996\n",
      "epoch no.2 train no.220690  loss = 1.96249 avg_loss = 3.59706\n",
      "epoch no.2 train no.220700  loss = 4.55822 avg_loss = 3.61738\n",
      "epoch no.2 train no.220710  loss = 2.73739 avg_loss = 3.53779\n",
      "epoch no.2 train no.220720  loss = 2.89920 avg_loss = 3.49476\n",
      "epoch no.2 train no.220730  loss = 6.30079 avg_loss = 3.54358\n",
      "epoch no.2 train no.220740  loss = 3.59724 avg_loss = 3.51784\n",
      "epoch no.2 train no.220750  loss = 4.88851 avg_loss = 3.54542\n",
      "epoch no.2 train no.220760  loss = 4.11393 avg_loss = 3.52534\n",
      "epoch no.2 train no.220770  loss = 3.63813 avg_loss = 3.57034\n",
      "epoch no.2 train no.220780  loss = 4.43748 avg_loss = 3.53854\n",
      "epoch no.2 train no.220790  loss = 4.09684 avg_loss = 3.53966\n",
      "epoch no.2 train no.220800  loss = 1.89180 avg_loss = 3.54309\n",
      "epoch no.2 train no.220810  loss = 5.46076 avg_loss = 3.54635\n",
      "epoch no.2 train no.220820  loss = 4.47801 avg_loss = 3.56415\n",
      "epoch no.2 train no.220830  loss = 2.65185 avg_loss = 3.58444\n",
      "epoch no.2 train no.220840  loss = 3.51103 avg_loss = 3.57785\n",
      "epoch no.2 train no.220850  loss = 3.32425 avg_loss = 3.62903\n",
      "epoch no.2 train no.220860  loss = 1.95702 avg_loss = 3.64583\n",
      "epoch no.2 train no.220870  loss = 2.91998 avg_loss = 3.61705\n",
      "epoch no.2 train no.220880  loss = 2.52144 avg_loss = 3.56665\n",
      "epoch no.2 train no.220890  loss = 3.53492 avg_loss = 3.55455\n",
      "epoch no.2 train no.220900  loss = 4.50282 avg_loss = 3.56135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.220910  loss = 3.90118 avg_loss = 3.61764\n",
      "epoch no.2 train no.220920  loss = 3.69908 avg_loss = 3.61994\n",
      "epoch no.2 train no.220930  loss = 3.03453 avg_loss = 3.64287\n",
      "epoch no.2 train no.220940  loss = 3.22632 avg_loss = 3.57445\n",
      "epoch no.2 train no.220950  loss = 3.39640 avg_loss = 3.63159\n",
      "epoch no.2 train no.220960  loss = 4.64977 avg_loss = 3.60427\n",
      "epoch no.2 train no.220970  loss = 2.84053 avg_loss = 3.58893\n",
      "epoch no.2 train no.220980  loss = 3.25572 avg_loss = 3.59574\n",
      "epoch no.2 train no.220990  loss = 3.66762 avg_loss = 3.62082\n",
      "epoch no.2 train no.221000  loss = 6.24444 avg_loss = 3.64091\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁싶을', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환하고 싶을때 듣는 음악</s>\n",
      "epoch no.2 train no.221010  loss = 3.33520 avg_loss = 3.64076\n",
      "epoch no.2 train no.221020  loss = 2.52288 avg_loss = 3.60602\n",
      "epoch no.2 train no.221030  loss = 3.53362 avg_loss = 3.64388\n",
      "epoch no.2 train no.221040  loss = 3.29878 avg_loss = 3.64777\n",
      "epoch no.2 train no.221050  loss = 2.42587 avg_loss = 3.56955\n",
      "epoch no.2 train no.221060  loss = 3.09639 avg_loss = 3.55956\n",
      "epoch no.2 train no.221070  loss = 2.58719 avg_loss = 3.56954\n",
      "epoch no.2 train no.221080  loss = 2.85602 avg_loss = 3.60033\n",
      "epoch no.2 train no.221090  loss = 1.84339 avg_loss = 3.58721\n",
      "epoch no.2 train no.221100  loss = 2.78451 avg_loss = 3.64457\n",
      "epoch no.2 train no.221110  loss = 2.50590 avg_loss = 3.59915\n",
      "epoch no.2 train no.221120  loss = 2.49120 avg_loss = 3.58123\n",
      "epoch no.2 train no.221130  loss = 3.29376 avg_loss = 3.52737\n",
      "epoch no.2 train no.221140  loss = 4.76255 avg_loss = 3.53678\n",
      "epoch no.2 train no.221150  loss = 3.76030 avg_loss = 3.61004\n",
      "epoch no.2 train no.221160  loss = 5.83538 avg_loss = 3.66490\n",
      "epoch no.2 train no.221170  loss = 3.71796 avg_loss = 3.63195\n",
      "epoch no.2 train no.221180  loss = 3.55034 avg_loss = 3.63121\n",
      "epoch no.2 train no.221190  loss = 3.86207 avg_loss = 3.59514\n",
      "epoch no.2 train no.221200  loss = 4.49060 avg_loss = 3.57940\n",
      "epoch no.2 train no.221210  loss = 3.80081 avg_loss = 3.59569\n",
      "epoch no.2 train no.221220  loss = 3.60948 avg_loss = 3.55388\n",
      "epoch no.2 train no.221230  loss = 3.57160 avg_loss = 3.54710\n",
      "epoch no.2 train no.221240  loss = 2.73391 avg_loss = 3.55980\n",
      "epoch no.2 train no.221250  loss = 3.92030 avg_loss = 3.54025\n",
      "epoch no.2 train no.221260  loss = 4.38215 avg_loss = 3.50599\n",
      "epoch no.2 train no.221270  loss = 4.36133 avg_loss = 3.51618\n",
      "epoch no.2 train no.221280  loss = 1.40129 avg_loss = 3.47817\n",
      "epoch no.2 train no.221290  loss = 3.86883 avg_loss = 3.52678\n",
      "epoch no.2 train no.221300  loss = 1.10732 avg_loss = 3.50563\n",
      "epoch no.2 train no.221310  loss = 2.73159 avg_loss = 3.53901\n",
      "epoch no.2 train no.221320  loss = 2.43088 avg_loss = 3.55403\n",
      "epoch no.2 train no.221330  loss = 2.99246 avg_loss = 3.52021\n",
      "epoch no.2 train no.221340  loss = 2.53069 avg_loss = 3.54433\n",
      "epoch no.2 train no.221350  loss = 3.17181 avg_loss = 3.58035\n",
      "epoch no.2 train no.221360  loss = 4.28953 avg_loss = 3.58909\n",
      "epoch no.2 train no.221370  loss = 4.55861 avg_loss = 3.60742\n",
      "epoch no.2 train no.221380  loss = 3.41860 avg_loss = 3.64039\n",
      "epoch no.2 train no.221390  loss = 6.20200 avg_loss = 3.64006\n",
      "epoch no.2 train no.221400  loss = 3.82220 avg_loss = 3.60799\n",
      "epoch no.2 train no.221410  loss = 3.87984 avg_loss = 3.52808\n",
      "epoch no.2 train no.221420  loss = 2.56958 avg_loss = 3.58249\n",
      "epoch no.2 train no.221430  loss = 3.77647 avg_loss = 3.54717\n",
      "epoch no.2 train no.221440  loss = 3.29091 avg_loss = 3.56146\n",
      "epoch no.2 train no.221450  loss = 3.58582 avg_loss = 3.54000\n",
      "epoch no.2 train no.221460  loss = 4.27061 avg_loss = 3.50448\n",
      "epoch no.2 train no.221470  loss = 1.98846 avg_loss = 3.49806\n",
      "epoch no.2 train no.221480  loss = 5.13125 avg_loss = 3.50034\n",
      "epoch no.2 train no.221490  loss = 3.62851 avg_loss = 3.49342\n",
      "epoch no.2 train no.221500  loss = 3.77501 avg_loss = 3.52123\n",
      "epoch no.2 train no.221510  loss = 4.07466 avg_loss = 3.53322\n",
      "epoch no.2 train no.221520  loss = 3.16747 avg_loss = 3.49916\n",
      "epoch no.2 train no.221530  loss = 2.80767 avg_loss = 3.48834\n",
      "epoch no.2 train no.221540  loss = 3.77178 avg_loss = 3.49917\n",
      "epoch no.2 train no.221550  loss = 3.61652 avg_loss = 3.53564\n",
      "epoch no.2 train no.221560  loss = 4.39705 avg_loss = 3.56240\n",
      "epoch no.2 train no.221570  loss = 4.44015 avg_loss = 3.57110\n",
      "epoch no.2 train no.221580  loss = 3.27259 avg_loss = 3.52124\n",
      "epoch no.2 train no.221590  loss = 3.09143 avg_loss = 3.52026\n",
      "epoch no.2 train no.221600  loss = 5.21211 avg_loss = 3.54589\n",
      "epoch no.2 train no.221610  loss = 2.48610 avg_loss = 3.53672\n",
      "epoch no.2 train no.221620  loss = 3.78403 avg_loss = 3.58046\n",
      "epoch no.2 train no.221630  loss = 3.65280 avg_loss = 3.58009\n",
      "epoch no.2 train no.221640  loss = 3.98249 avg_loss = 3.58472\n",
      "epoch no.2 train no.221650  loss = 3.52342 avg_loss = 3.50638\n",
      "epoch no.2 train no.221660  loss = 5.06965 avg_loss = 3.48812\n",
      "epoch no.2 train no.221670  loss = 3.12903 avg_loss = 3.46222\n",
      "epoch no.2 train no.221680  loss = 3.31602 avg_loss = 3.43517\n",
      "epoch no.2 train no.221690  loss = 3.01566 avg_loss = 3.48486\n",
      "epoch no.2 train no.221700  loss = 2.77255 avg_loss = 3.47048\n",
      "epoch no.2 train no.221710  loss = 3.66958 avg_loss = 3.49388\n",
      "epoch no.2 train no.221720  loss = 3.01713 avg_loss = 3.45870\n",
      "epoch no.2 train no.221730  loss = 3.90547 avg_loss = 3.53358\n",
      "epoch no.2 train no.221740  loss = 3.17087 avg_loss = 3.50767\n",
      "epoch no.2 train no.221750  loss = 4.23597 avg_loss = 3.51874\n",
      "epoch no.2 train no.221760  loss = 5.88574 avg_loss = 3.55628\n",
      "epoch no.2 train no.221770  loss = 4.44708 avg_loss = 3.60100\n",
      "epoch no.2 train no.221780  loss = 4.84754 avg_loss = 3.59887\n",
      "epoch no.2 train no.221790  loss = 4.61252 avg_loss = 3.59301\n",
      "epoch no.2 train no.221800  loss = 3.72727 avg_loss = 3.58857\n",
      "epoch no.2 train no.221810  loss = 3.26708 avg_loss = 3.57292\n",
      "epoch no.2 train no.221820  loss = 3.09223 avg_loss = 3.60341\n",
      "epoch no.2 train no.221830  loss = 3.73163 avg_loss = 3.61678\n",
      "epoch no.2 train no.221840  loss = 6.16686 avg_loss = 3.67569\n",
      "epoch no.2 train no.221850  loss = 2.83895 avg_loss = 3.64441\n",
      "epoch no.2 train no.221860  loss = 2.86930 avg_loss = 3.62982\n",
      "epoch no.2 train no.221870  loss = 2.77381 avg_loss = 3.59249\n",
      "epoch no.2 train no.221880  loss = 3.79191 avg_loss = 3.52606\n",
      "epoch no.2 train no.221890  loss = 3.17547 avg_loss = 3.49332\n",
      "epoch no.2 train no.221900  loss = 4.74411 avg_loss = 3.50487\n",
      "epoch no.2 train no.221910  loss = 3.15356 avg_loss = 3.51869\n",
      "epoch no.2 train no.221920  loss = 2.68828 avg_loss = 3.55165\n",
      "epoch no.2 train no.221930  loss = 2.91060 avg_loss = 3.55996\n",
      "epoch no.2 train no.221940  loss = 3.90619 avg_loss = 3.55498\n",
      "epoch no.2 train no.221950  loss = 5.55975 avg_loss = 3.56013\n",
      "epoch no.2 train no.221960  loss = 1.75441 avg_loss = 3.55107\n",
      "epoch no.2 train no.221970  loss = 3.93718 avg_loss = 3.56038\n",
      "epoch no.2 train no.221980  loss = 2.50092 avg_loss = 3.47956\n",
      "epoch no.2 train no.221990  loss = 4.38932 avg_loss = 3.48268\n",
      "epoch no.2 train no.222000  loss = 2.82997 avg_loss = 3.48091\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁위한', '▁팝', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.2 train no.222010  loss = 4.97433 avg_loss = 3.50803\n",
      "epoch no.2 train no.222020  loss = 2.23719 avg_loss = 3.56531\n",
      "epoch no.2 train no.222030  loss = 3.97761 avg_loss = 3.55483\n",
      "epoch no.2 train no.222040  loss = 2.57015 avg_loss = 3.50415\n",
      "epoch no.2 train no.222050  loss = 5.61600 avg_loss = 3.53424\n",
      "epoch no.2 train no.222060  loss = 6.53940 avg_loss = 3.53661\n",
      "epoch no.2 train no.222070  loss = 2.23007 avg_loss = 3.53901\n",
      "epoch no.2 train no.222080  loss = 4.47912 avg_loss = 3.55027\n",
      "epoch no.2 train no.222090  loss = 2.38270 avg_loss = 3.56977\n",
      "epoch no.2 train no.222100  loss = 3.00349 avg_loss = 3.53740\n",
      "epoch no.2 train no.222110  loss = 3.65727 avg_loss = 3.59508\n",
      "epoch no.2 train no.222120  loss = 3.79506 avg_loss = 3.63072\n",
      "epoch no.2 train no.222130  loss = 4.57212 avg_loss = 3.67100\n",
      "epoch no.2 train no.222140  loss = 4.38965 avg_loss = 3.69078\n",
      "epoch no.2 train no.222150  loss = 4.29813 avg_loss = 3.67654\n",
      "epoch no.2 train no.222160  loss = 5.00628 avg_loss = 3.67731\n",
      "epoch no.2 train no.222170  loss = 4.29404 avg_loss = 3.63452\n",
      "epoch no.2 train no.222180  loss = 3.45793 avg_loss = 3.66277\n",
      "epoch no.2 train no.222190  loss = 2.17912 avg_loss = 3.62362\n",
      "epoch no.2 train no.222200  loss = 3.98875 avg_loss = 3.63758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.222210  loss = 2.68570 avg_loss = 3.64901\n",
      "epoch no.2 train no.222220  loss = 3.59613 avg_loss = 3.62296\n",
      "epoch no.2 train no.222230  loss = 1.84643 avg_loss = 3.58908\n",
      "epoch no.2 train no.222240  loss = 4.01482 avg_loss = 3.60488\n",
      "epoch no.2 train no.222250  loss = 2.25122 avg_loss = 3.59800\n",
      "epoch no.2 train no.222260  loss = 5.15791 avg_loss = 3.63035\n",
      "epoch no.2 train no.222270  loss = 4.66781 avg_loss = 3.65168\n",
      "epoch no.2 train no.222280  loss = 3.99218 avg_loss = 3.61866\n",
      "epoch no.2 train no.222290  loss = 2.47571 avg_loss = 3.60904\n",
      "epoch no.2 train no.222300  loss = 3.54093 avg_loss = 3.64896\n",
      "epoch no.2 train no.222310  loss = 2.36031 avg_loss = 3.66619\n",
      "epoch no.2 train no.222320  loss = 3.80221 avg_loss = 3.70075\n",
      "epoch no.2 train no.222330  loss = 4.05318 avg_loss = 3.73007\n",
      "epoch no.2 train no.222340  loss = 2.11345 avg_loss = 3.70629\n",
      "epoch no.2 train no.222350  loss = 4.14538 avg_loss = 3.64896\n",
      "epoch no.2 train no.222360  loss = 3.74391 avg_loss = 3.62805\n",
      "epoch no.2 train no.222370  loss = 3.34478 avg_loss = 3.63431\n",
      "epoch no.2 train no.222380  loss = 4.90401 avg_loss = 3.70496\n",
      "epoch no.2 train no.222390  loss = 2.52321 avg_loss = 3.63829\n",
      "epoch no.2 train no.222400  loss = 4.54541 avg_loss = 3.65601\n",
      "epoch no.2 train no.222410  loss = 4.42707 avg_loss = 3.67790\n",
      "epoch no.2 train no.222420  loss = 2.32752 avg_loss = 3.66602\n",
      "epoch no.2 train no.222430  loss = 4.99708 avg_loss = 3.68632\n",
      "epoch no.2 train no.222440  loss = 3.76732 avg_loss = 3.68543\n",
      "epoch no.2 train no.222450  loss = 5.47147 avg_loss = 3.66743\n",
      "epoch no.2 train no.222460  loss = 3.27303 avg_loss = 3.66654\n",
      "epoch no.2 train no.222470  loss = 3.31958 avg_loss = 3.64031\n",
      "epoch no.2 train no.222480  loss = 2.87534 avg_loss = 3.60292\n",
      "epoch no.2 train no.222490  loss = 3.22771 avg_loss = 3.60377\n",
      "epoch no.2 train no.222500  loss = 3.77085 avg_loss = 3.65233\n",
      "epoch no.2 train no.222510  loss = 4.15415 avg_loss = 3.61782\n",
      "epoch no.2 train no.222520  loss = 2.57139 avg_loss = 3.58197\n",
      "epoch no.2 train no.222530  loss = 2.46555 avg_loss = 3.57188\n",
      "epoch no.2 train no.222540  loss = 4.80634 avg_loss = 3.57943\n",
      "epoch no.2 train no.222550  loss = 4.03673 avg_loss = 3.60438\n",
      "epoch no.2 train no.222560  loss = 3.15749 avg_loss = 3.57465\n",
      "epoch no.2 train no.222570  loss = 4.32175 avg_loss = 3.62774\n",
      "epoch no.2 train no.222580  loss = 4.17747 avg_loss = 3.65035\n",
      "epoch no.2 train no.222590  loss = 2.11873 avg_loss = 3.65269\n",
      "epoch no.2 train no.222600  loss = 3.50058 avg_loss = 3.61983\n",
      "epoch no.2 train no.222610  loss = 2.91329 avg_loss = 3.59114\n",
      "epoch no.2 train no.222620  loss = 2.97923 avg_loss = 3.55669\n",
      "epoch no.2 train no.222630  loss = 3.61103 avg_loss = 3.54801\n",
      "epoch no.2 train no.222640  loss = 4.84347 avg_loss = 3.52331\n",
      "epoch no.2 train no.222650  loss = 2.24058 avg_loss = 3.55196\n",
      "epoch no.2 train no.222660  loss = 3.98224 avg_loss = 3.56730\n",
      "epoch no.2 train no.222670  loss = 6.59634 avg_loss = 3.67796\n",
      "epoch no.2 train no.222680  loss = 4.55604 avg_loss = 3.71321\n",
      "epoch no.2 train no.222690  loss = 2.96566 avg_loss = 3.66687\n",
      "epoch no.2 train no.222700  loss = 3.32700 avg_loss = 3.67949\n",
      "epoch no.2 train no.222710  loss = 3.52441 avg_loss = 3.65047\n",
      "epoch no.2 train no.222720  loss = 4.37983 avg_loss = 3.62822\n",
      "epoch no.2 train no.222730  loss = 3.01978 avg_loss = 3.63628\n",
      "epoch no.2 train no.222740  loss = 3.16810 avg_loss = 3.59409\n",
      "epoch no.2 train no.222750  loss = 2.97222 avg_loss = 3.59973\n",
      "epoch no.2 train no.222760  loss = 1.53736 avg_loss = 3.53980\n",
      "epoch no.2 train no.222770  loss = 2.66196 avg_loss = 3.60665\n",
      "epoch no.2 train no.222780  loss = 5.93313 avg_loss = 3.62809\n",
      "epoch no.2 train no.222790  loss = 2.44771 avg_loss = 3.64242\n",
      "epoch no.2 train no.222800  loss = 3.72480 avg_loss = 3.63194\n",
      "epoch no.2 train no.222810  loss = 4.74170 avg_loss = 3.60263\n",
      "epoch no.2 train no.222820  loss = 3.37886 avg_loss = 3.57261\n",
      "epoch no.2 train no.222830  loss = 4.34011 avg_loss = 3.54457\n",
      "epoch no.2 train no.222840  loss = 3.05744 avg_loss = 3.54333\n",
      "epoch no.2 train no.222850  loss = 2.79648 avg_loss = 3.48181\n",
      "epoch no.2 train no.222860  loss = 4.05767 avg_loss = 3.48951\n",
      "epoch no.2 train no.222870  loss = 5.33020 avg_loss = 3.52733\n",
      "epoch no.2 train no.222880  loss = 3.23874 avg_loss = 3.56913\n",
      "epoch no.2 train no.222890  loss = 3.53279 avg_loss = 3.59521\n",
      "epoch no.2 train no.222900  loss = 2.90163 avg_loss = 3.64525\n",
      "epoch no.2 train no.222910  loss = 2.13487 avg_loss = 3.61118\n",
      "epoch no.2 train no.222920  loss = 2.24686 avg_loss = 3.57622\n",
      "epoch no.2 train no.222930  loss = 3.42181 avg_loss = 3.58246\n",
      "epoch no.2 train no.222940  loss = 2.57868 avg_loss = 3.58284\n",
      "epoch no.2 train no.222950  loss = 3.15460 avg_loss = 3.55282\n",
      "epoch no.2 train no.222960  loss = 5.40113 avg_loss = 3.53809\n",
      "epoch no.2 train no.222970  loss = 2.09313 avg_loss = 3.57787\n",
      "epoch no.2 train no.222980  loss = 3.84079 avg_loss = 3.56885\n",
      "epoch no.2 train no.222990  loss = 3.10888 avg_loss = 3.54352\n",
      "epoch no.2 train no.223000  loss = 5.08717 avg_loss = 3.59460\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.223010  loss = 4.38339 avg_loss = 3.58531\n",
      "epoch no.2 train no.223020  loss = 4.28043 avg_loss = 3.60319\n",
      "epoch no.2 train no.223030  loss = 1.76452 avg_loss = 3.58237\n",
      "epoch no.2 train no.223040  loss = 3.65360 avg_loss = 3.58540\n",
      "epoch no.2 train no.223050  loss = 6.46001 avg_loss = 3.56334\n",
      "epoch no.2 train no.223060  loss = 3.49359 avg_loss = 3.60268\n",
      "epoch no.2 train no.223070  loss = 4.82029 avg_loss = 3.63284\n",
      "epoch no.2 train no.223080  loss = 3.77456 avg_loss = 3.62476\n",
      "epoch no.2 train no.223090  loss = 5.92031 avg_loss = 3.65979\n",
      "epoch no.2 train no.223100  loss = 3.26790 avg_loss = 3.64798\n",
      "epoch no.2 train no.223110  loss = 3.12045 avg_loss = 3.61043\n",
      "epoch no.2 train no.223120  loss = 4.00767 avg_loss = 3.60844\n",
      "epoch no.2 train no.223130  loss = 2.93054 avg_loss = 3.60969\n",
      "epoch no.2 train no.223140  loss = 4.01116 avg_loss = 3.62183\n",
      "epoch no.2 train no.223150  loss = 2.60686 avg_loss = 3.58009\n",
      "epoch no.2 train no.223160  loss = 3.06570 avg_loss = 3.52039\n",
      "epoch no.2 train no.223170  loss = 2.50481 avg_loss = 3.48987\n",
      "epoch no.2 train no.223180  loss = 3.15370 avg_loss = 3.58165\n",
      "epoch no.2 train no.223190  loss = 3.05396 avg_loss = 3.59738\n",
      "epoch no.2 train no.223200  loss = 2.42561 avg_loss = 3.59545\n",
      "epoch no.2 train no.223210  loss = 2.93380 avg_loss = 3.62293\n",
      "epoch no.2 train no.223220  loss = 4.32010 avg_loss = 3.65249\n",
      "epoch no.2 train no.223230  loss = 2.88602 avg_loss = 3.65166\n",
      "epoch no.2 train no.223240  loss = 4.80895 avg_loss = 3.69635\n",
      "epoch no.2 train no.223250  loss = 4.14813 avg_loss = 3.71788\n",
      "epoch no.2 train no.223260  loss = 2.23884 avg_loss = 3.76112\n",
      "epoch no.2 train no.223270  loss = 4.57523 avg_loss = 3.70472\n",
      "epoch no.2 train no.223280  loss = 4.09673 avg_loss = 3.69109\n",
      "epoch no.2 train no.223290  loss = 5.19500 avg_loss = 3.69057\n",
      "epoch no.2 train no.223300  loss = 5.51544 avg_loss = 3.72241\n",
      "epoch no.2 train no.223310  loss = 4.10214 avg_loss = 3.74026\n",
      "epoch no.2 train no.223320  loss = 5.14393 avg_loss = 3.76116\n",
      "epoch no.2 train no.223330  loss = 4.18313 avg_loss = 3.76959\n",
      "epoch no.2 train no.223340  loss = 3.33212 avg_loss = 3.77421\n",
      "epoch no.2 train no.223350  loss = 3.14612 avg_loss = 3.72563\n",
      "epoch no.2 train no.223360  loss = 4.50310 avg_loss = 3.66847\n",
      "epoch no.2 train no.223370  loss = 5.16726 avg_loss = 3.66382\n",
      "epoch no.2 train no.223380  loss = 5.44972 avg_loss = 3.67438\n",
      "epoch no.2 train no.223390  loss = 4.01555 avg_loss = 3.68857\n",
      "epoch no.2 train no.223400  loss = 3.35360 avg_loss = 3.66191\n",
      "epoch no.2 train no.223410  loss = 4.55638 avg_loss = 3.67292\n",
      "epoch no.2 train no.223420  loss = 2.91463 avg_loss = 3.62101\n",
      "epoch no.2 train no.223430  loss = 2.69293 avg_loss = 3.60103\n",
      "epoch no.2 train no.223440  loss = 5.60218 avg_loss = 3.58766\n",
      "epoch no.2 train no.223450  loss = 3.73258 avg_loss = 3.59610\n",
      "epoch no.2 train no.223460  loss = 4.69789 avg_loss = 3.61354\n",
      "epoch no.2 train no.223470  loss = 6.00177 avg_loss = 3.65464\n",
      "epoch no.2 train no.223480  loss = 4.12329 avg_loss = 3.64592\n",
      "epoch no.2 train no.223490  loss = 4.85226 avg_loss = 3.66982\n",
      "epoch no.2 train no.223500  loss = 2.88663 avg_loss = 3.72011\n",
      "epoch no.2 train no.223510  loss = 2.27278 avg_loss = 3.74172\n",
      "epoch no.2 train no.223520  loss = 3.03601 avg_loss = 3.65946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.223530  loss = 4.52414 avg_loss = 3.64673\n",
      "epoch no.2 train no.223540  loss = 5.00539 avg_loss = 3.61055\n",
      "epoch no.2 train no.223550  loss = 2.60777 avg_loss = 3.64888\n",
      "epoch no.2 train no.223560  loss = 3.08307 avg_loss = 3.62821\n",
      "epoch no.2 train no.223570  loss = 2.94418 avg_loss = 3.57563\n",
      "epoch no.2 train no.223580  loss = 3.18055 avg_loss = 3.56284\n",
      "epoch no.2 train no.223590  loss = 1.70246 avg_loss = 3.53277\n",
      "epoch no.2 train no.223600  loss = 3.95863 avg_loss = 3.52505\n",
      "epoch no.2 train no.223610  loss = 3.82138 avg_loss = 3.47975\n",
      "epoch no.2 train no.223620  loss = 2.77993 avg_loss = 3.45313\n",
      "epoch no.2 train no.223630  loss = 4.63234 avg_loss = 3.49397\n",
      "epoch no.2 train no.223640  loss = 4.28777 avg_loss = 3.56191\n",
      "epoch no.2 train no.223650  loss = 2.43356 avg_loss = 3.54183\n",
      "epoch no.2 train no.223660  loss = 3.53621 avg_loss = 3.52596\n",
      "epoch no.2 train no.223670  loss = 3.23135 avg_loss = 3.46670\n",
      "epoch no.2 train no.223680  loss = 2.32664 avg_loss = 3.46661\n",
      "epoch no.2 train no.223690  loss = 3.81815 avg_loss = 3.47693\n",
      "epoch no.2 train no.223700  loss = 5.20430 avg_loss = 3.53961\n",
      "epoch no.2 train no.223710  loss = 2.59972 avg_loss = 3.50744\n",
      "epoch no.2 train no.223720  loss = 4.05719 avg_loss = 3.58410\n",
      "epoch no.2 train no.223730  loss = 4.64964 avg_loss = 3.58139\n",
      "epoch no.2 train no.223740  loss = 3.21036 avg_loss = 3.61795\n",
      "epoch no.2 train no.223750  loss = 3.50670 avg_loss = 3.58053\n",
      "epoch no.2 train no.223760  loss = 3.49841 avg_loss = 3.49416\n",
      "epoch no.2 train no.223770  loss = 3.26456 avg_loss = 3.50899\n",
      "epoch no.2 train no.223780  loss = 2.26432 avg_loss = 3.49526\n",
      "epoch no.2 train no.223790  loss = 3.75273 avg_loss = 3.51712\n",
      "epoch no.2 train no.223800  loss = 4.25180 avg_loss = 3.52158\n",
      "epoch no.2 train no.223810  loss = 4.87932 avg_loss = 3.53384\n",
      "epoch no.2 train no.223820  loss = 3.50038 avg_loss = 3.51112\n",
      "epoch no.2 train no.223830  loss = 3.32889 avg_loss = 3.51272\n",
      "epoch no.2 train no.223840  loss = 3.69019 avg_loss = 3.46869\n",
      "epoch no.2 train no.223850  loss = 2.59147 avg_loss = 3.46757\n",
      "epoch no.2 train no.223860  loss = 2.74225 avg_loss = 3.43015\n",
      "epoch no.2 train no.223870  loss = 2.52406 avg_loss = 3.39322\n",
      "epoch no.2 train no.223880  loss = 2.63104 avg_loss = 3.37606\n",
      "epoch no.2 train no.223890  loss = 3.69474 avg_loss = 3.39780\n",
      "epoch no.2 train no.223900  loss = 2.99863 avg_loss = 3.40364\n",
      "epoch no.2 train no.223910  loss = 3.24159 avg_loss = 3.41243\n",
      "epoch no.2 train no.223920  loss = 2.69691 avg_loss = 3.43177\n",
      "epoch no.2 train no.223930  loss = 2.97286 avg_loss = 3.44335\n",
      "epoch no.2 train no.223940  loss = 4.01772 avg_loss = 3.46798\n",
      "epoch no.2 train no.223950  loss = 7.07495 avg_loss = 3.47181\n",
      "epoch no.2 train no.223960  loss = 2.48257 avg_loss = 3.46951\n",
      "epoch no.2 train no.223970  loss = 1.22732 avg_loss = 3.40709\n",
      "epoch no.2 train no.223980  loss = 4.00860 avg_loss = 3.44406\n",
      "epoch no.2 train no.223990  loss = 3.10139 avg_loss = 3.43628\n",
      "epoch no.2 train no.224000  loss = 3.46205 avg_loss = 3.43034\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.2 train no.224010  loss = 3.68605 avg_loss = 3.48082\n",
      "epoch no.2 train no.224020  loss = 3.12944 avg_loss = 3.50418\n",
      "epoch no.2 train no.224030  loss = 2.78630 avg_loss = 3.51212\n",
      "epoch no.2 train no.224040  loss = 2.50247 avg_loss = 3.51457\n",
      "epoch no.2 train no.224050  loss = 1.94204 avg_loss = 3.49177\n",
      "epoch no.2 train no.224060  loss = 3.10239 avg_loss = 3.52515\n",
      "epoch no.2 train no.224070  loss = 4.31075 avg_loss = 3.55165\n",
      "epoch no.2 train no.224080  loss = 2.77222 avg_loss = 3.56736\n",
      "epoch no.2 train no.224090  loss = 4.02124 avg_loss = 3.59785\n",
      "epoch no.2 train no.224100  loss = 3.36034 avg_loss = 3.61460\n",
      "epoch no.2 train no.224110  loss = 3.28578 avg_loss = 3.56908\n",
      "epoch no.2 train no.224120  loss = 2.76784 avg_loss = 3.55180\n",
      "epoch no.2 train no.224130  loss = 4.13300 avg_loss = 3.59100\n",
      "epoch no.2 train no.224140  loss = 2.68463 avg_loss = 3.58518\n",
      "epoch no.2 train no.224150  loss = 3.12338 avg_loss = 3.56543\n",
      "epoch no.2 train no.224160  loss = 4.55580 avg_loss = 3.56537\n",
      "epoch no.2 train no.224170  loss = 3.20045 avg_loss = 3.55371\n",
      "epoch no.2 train no.224180  loss = 3.72197 avg_loss = 3.56750\n",
      "epoch no.2 train no.224190  loss = 4.06136 avg_loss = 3.55887\n",
      "epoch no.2 train no.224200  loss = 3.88173 avg_loss = 3.55390\n",
      "epoch no.2 train no.224210  loss = 3.14420 avg_loss = 3.54123\n",
      "epoch no.2 train no.224220  loss = 2.43465 avg_loss = 3.51565\n",
      "epoch no.2 train no.224230  loss = 3.84260 avg_loss = 3.49596\n",
      "epoch no.2 train no.224240  loss = 4.09092 avg_loss = 3.48464\n",
      "epoch no.2 train no.224250  loss = 4.78241 avg_loss = 3.47093\n",
      "epoch no.2 train no.224260  loss = 5.20955 avg_loss = 3.49428\n",
      "epoch no.2 train no.224270  loss = 3.72793 avg_loss = 3.54123\n",
      "epoch no.2 train no.224280  loss = 5.26221 avg_loss = 3.54339\n",
      "epoch no.2 train no.224290  loss = 3.51370 avg_loss = 3.50968\n",
      "epoch no.2 train no.224300  loss = 3.25316 avg_loss = 3.50449\n",
      "epoch no.2 train no.224310  loss = 3.87816 avg_loss = 3.52230\n",
      "epoch no.2 train no.224320  loss = 4.94452 avg_loss = 3.53581\n",
      "epoch no.2 train no.224330  loss = 3.06963 avg_loss = 3.49538\n",
      "epoch no.2 train no.224340  loss = 6.32405 avg_loss = 3.52590\n",
      "epoch no.2 train no.224350  loss = 2.62892 avg_loss = 3.49140\n",
      "epoch no.2 train no.224360  loss = 3.13682 avg_loss = 3.52294\n",
      "epoch no.2 train no.224370  loss = 4.20360 avg_loss = 3.53627\n",
      "epoch no.2 train no.224380  loss = 7.00183 avg_loss = 3.61403\n",
      "epoch no.2 train no.224390  loss = 5.30126 avg_loss = 3.61479\n",
      "epoch no.2 train no.224400  loss = 4.31603 avg_loss = 3.63822\n",
      "epoch no.2 train no.224410  loss = 5.33075 avg_loss = 3.63597\n",
      "epoch no.2 train no.224420  loss = 6.73768 avg_loss = 3.66216\n",
      "epoch no.2 train no.224430  loss = 2.23662 avg_loss = 3.70424\n",
      "epoch no.2 train no.224440  loss = 3.73440 avg_loss = 3.67383\n",
      "epoch no.2 train no.224450  loss = 5.37314 avg_loss = 3.68325\n",
      "epoch no.2 train no.224460  loss = 2.79509 avg_loss = 3.62059\n",
      "epoch no.2 train no.224470  loss = 3.09637 avg_loss = 3.54019\n",
      "epoch no.2 train no.224480  loss = 5.11791 avg_loss = 3.54860\n",
      "epoch no.2 train no.224490  loss = 4.53103 avg_loss = 3.56885\n",
      "epoch no.2 train no.224500  loss = 3.70190 avg_loss = 3.57606\n",
      "epoch no.2 train no.224510  loss = 2.36009 avg_loss = 3.54001\n",
      "epoch no.2 train no.224520  loss = 4.03411 avg_loss = 3.51837\n",
      "epoch no.2 train no.224530  loss = 4.59319 avg_loss = 3.54800\n",
      "epoch no.2 train no.224540  loss = 4.08156 avg_loss = 3.50693\n",
      "epoch no.2 train no.224550  loss = 3.16970 avg_loss = 3.54580\n",
      "epoch no.2 train no.224560  loss = 3.21887 avg_loss = 3.50256\n",
      "epoch no.2 train no.224570  loss = 4.24943 avg_loss = 3.51587\n",
      "epoch no.2 train no.224580  loss = 3.81539 avg_loss = 3.56333\n",
      "epoch no.2 train no.224590  loss = 2.72845 avg_loss = 3.61808\n",
      "epoch no.2 train no.224600  loss = 4.42908 avg_loss = 3.65992\n",
      "epoch no.2 train no.224610  loss = 4.42374 avg_loss = 3.68694\n",
      "epoch no.2 train no.224620  loss = 2.73576 avg_loss = 3.68935\n",
      "epoch no.2 train no.224630  loss = 3.72863 avg_loss = 3.64602\n",
      "epoch no.2 train no.224640  loss = 4.64507 avg_loss = 3.67882\n",
      "epoch no.2 train no.224650  loss = 1.69146 avg_loss = 3.67971\n",
      "epoch no.2 train no.224660  loss = 5.32172 avg_loss = 3.62554\n",
      "epoch no.2 train no.224670  loss = 2.77586 avg_loss = 3.59945\n",
      "epoch no.2 train no.224680  loss = 2.80037 avg_loss = 3.60659\n",
      "epoch no.2 train no.224690  loss = 5.63277 avg_loss = 3.60114\n",
      "epoch no.2 train no.224700  loss = 4.60639 avg_loss = 3.58385\n",
      "epoch no.2 train no.224710  loss = 2.11629 avg_loss = 3.54443\n",
      "epoch no.2 train no.224720  loss = 3.93596 avg_loss = 3.49927\n",
      "epoch no.2 train no.224730  loss = 2.85308 avg_loss = 3.50882\n",
      "epoch no.2 train no.224740  loss = 3.04615 avg_loss = 3.53650\n",
      "epoch no.2 train no.224750  loss = 4.66952 avg_loss = 3.52689\n",
      "epoch no.2 train no.224760  loss = 3.96940 avg_loss = 3.57897\n",
      "epoch no.2 train no.224770  loss = 3.47349 avg_loss = 3.55996\n",
      "epoch no.2 train no.224780  loss = 3.87083 avg_loss = 3.54615\n",
      "epoch no.2 train no.224790  loss = 4.55260 avg_loss = 3.65694\n",
      "epoch no.2 train no.224800  loss = 4.17991 avg_loss = 3.68681\n",
      "epoch no.2 train no.224810  loss = 2.83845 avg_loss = 3.68855\n",
      "epoch no.2 train no.224820  loss = 4.67305 avg_loss = 3.67563\n",
      "epoch no.2 train no.224830  loss = 3.36133 avg_loss = 3.67813\n",
      "epoch no.2 train no.224840  loss = 5.84334 avg_loss = 3.69808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.224850  loss = 4.76470 avg_loss = 3.66729\n",
      "epoch no.2 train no.224860  loss = 3.11259 avg_loss = 3.62506\n",
      "epoch no.2 train no.224870  loss = 2.68913 avg_loss = 3.61926\n",
      "epoch no.2 train no.224880  loss = 4.76877 avg_loss = 3.66641\n",
      "epoch no.2 train no.224890  loss = 1.61675 avg_loss = 3.58097\n",
      "epoch no.2 train no.224900  loss = 3.65926 avg_loss = 3.55989\n",
      "epoch no.2 train no.224910  loss = 3.46237 avg_loss = 3.54595\n",
      "epoch no.2 train no.224920  loss = 3.06628 avg_loss = 3.59141\n",
      "epoch no.2 train no.224930  loss = 2.19312 avg_loss = 3.53324\n",
      "epoch no.2 train no.224940  loss = 3.39221 avg_loss = 3.54639\n",
      "epoch no.2 train no.224950  loss = 4.51248 avg_loss = 3.54214\n",
      "epoch no.2 train no.224960  loss = 3.45571 avg_loss = 3.55212\n",
      "epoch no.2 train no.224970  loss = 3.92324 avg_loss = 3.58097\n",
      "epoch no.2 train no.224980  loss = 2.57646 avg_loss = 3.60661\n",
      "epoch no.2 train no.224990  loss = 3.80688 avg_loss = 3.62262\n",
      "epoch no.2 train no.225000  loss = 5.03479 avg_loss = 3.60014\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.2 train no.225010  loss = 5.28642 avg_loss = 3.62636\n",
      "epoch no.2 train no.225020  loss = 3.60584 avg_loss = 3.60996\n",
      "epoch no.2 train no.225030  loss = 2.52237 avg_loss = 3.65931\n",
      "epoch no.2 train no.225040  loss = 4.34503 avg_loss = 3.69324\n",
      "epoch no.2 train no.225050  loss = 2.40367 avg_loss = 3.65419\n",
      "epoch no.2 train no.225060  loss = 4.52712 avg_loss = 3.70780\n",
      "epoch no.2 train no.225070  loss = 2.99077 avg_loss = 3.68981\n",
      "epoch no.2 train no.225080  loss = 3.72276 avg_loss = 3.66444\n",
      "epoch no.2 train no.225090  loss = 2.90128 avg_loss = 3.69548\n",
      "epoch no.2 train no.225100  loss = 3.15881 avg_loss = 3.65432\n",
      "epoch no.2 train no.225110  loss = 4.36164 avg_loss = 3.67597\n",
      "epoch no.2 train no.225120  loss = 3.94934 avg_loss = 3.63984\n",
      "epoch no.2 train no.225130  loss = 3.58610 avg_loss = 3.67218\n",
      "epoch no.2 train no.225140  loss = 2.64535 avg_loss = 3.62947\n",
      "epoch no.2 train no.225150  loss = 5.09404 avg_loss = 3.65010\n",
      "epoch no.2 train no.225160  loss = 3.75921 avg_loss = 3.67257\n",
      "epoch no.2 train no.225170  loss = 3.51660 avg_loss = 3.64702\n",
      "epoch no.2 train no.225180  loss = 3.45829 avg_loss = 3.61017\n",
      "epoch no.2 train no.225190  loss = 2.87510 avg_loss = 3.56661\n",
      "epoch no.2 train no.225200  loss = 2.60958 avg_loss = 3.58048\n",
      "epoch no.2 train no.225210  loss = 2.72848 avg_loss = 3.54545\n",
      "epoch no.2 train no.225220  loss = 5.85856 avg_loss = 3.63314\n",
      "epoch no.2 train no.225230  loss = 2.22765 avg_loss = 3.60399\n",
      "epoch no.2 train no.225240  loss = 5.31293 avg_loss = 3.58216\n",
      "epoch no.2 train no.225250  loss = 3.13512 avg_loss = 3.55582\n",
      "epoch no.2 train no.225260  loss = 3.19757 avg_loss = 3.52790\n",
      "epoch no.2 train no.225270  loss = 2.67448 avg_loss = 3.56782\n",
      "epoch no.2 train no.225280  loss = 3.27258 avg_loss = 3.55710\n",
      "epoch no.2 train no.225290  loss = 3.55765 avg_loss = 3.54595\n",
      "epoch no.2 train no.225300  loss = 3.48643 avg_loss = 3.56668\n",
      "epoch no.2 train no.225310  loss = 4.06432 avg_loss = 3.57070\n",
      "epoch no.2 train no.225320  loss = 3.97862 avg_loss = 3.54636\n",
      "epoch no.2 train no.225330  loss = 3.07632 avg_loss = 3.57714\n",
      "epoch no.2 train no.225340  loss = 5.98673 avg_loss = 3.63807\n",
      "epoch no.2 train no.225350  loss = 4.10763 avg_loss = 3.58176\n",
      "epoch no.2 train no.225360  loss = 4.95312 avg_loss = 3.61962\n",
      "epoch no.2 train no.225370  loss = 3.09848 avg_loss = 3.58645\n",
      "epoch no.2 train no.225380  loss = 4.11831 avg_loss = 3.59155\n",
      "epoch no.2 train no.225390  loss = 2.82449 avg_loss = 3.59981\n",
      "epoch no.2 train no.225400  loss = 3.68488 avg_loss = 3.56329\n",
      "epoch no.2 train no.225410  loss = 4.34314 avg_loss = 3.58043\n",
      "epoch no.2 train no.225420  loss = 2.09039 avg_loss = 3.53462\n",
      "epoch no.2 train no.225430  loss = 2.85598 avg_loss = 3.54361\n",
      "epoch no.2 train no.225440  loss = 4.46625 avg_loss = 3.52778\n",
      "epoch no.2 train no.225450  loss = 2.11250 avg_loss = 3.47915\n",
      "epoch no.2 train no.225460  loss = 3.15004 avg_loss = 3.44319\n",
      "epoch no.2 train no.225470  loss = 2.99686 avg_loss = 3.48977\n",
      "epoch no.2 train no.225480  loss = 4.28204 avg_loss = 3.48655\n",
      "epoch no.2 train no.225490  loss = 2.96448 avg_loss = 3.50490\n",
      "epoch no.2 train no.225500  loss = 2.96224 avg_loss = 3.53119\n",
      "epoch no.2 train no.225510  loss = 4.59941 avg_loss = 3.54400\n",
      "epoch no.2 train no.225520  loss = 4.13693 avg_loss = 3.58815\n",
      "epoch no.2 train no.225530  loss = 2.82351 avg_loss = 3.61510\n",
      "epoch no.2 train no.225540  loss = 3.25777 avg_loss = 3.61882\n",
      "epoch no.2 train no.225550  loss = 3.36857 avg_loss = 3.60885\n",
      "epoch no.2 train no.225560  loss = 4.45007 avg_loss = 3.64130\n",
      "epoch no.2 train no.225570  loss = 6.11770 avg_loss = 3.67274\n",
      "epoch no.2 train no.225580  loss = 3.47294 avg_loss = 3.73557\n",
      "epoch no.2 train no.225590  loss = 5.05005 avg_loss = 3.77145\n",
      "epoch no.2 train no.225600  loss = 1.94866 avg_loss = 3.71545\n",
      "epoch no.2 train no.225610  loss = 3.82380 avg_loss = 3.70123\n",
      "epoch no.2 train no.225620  loss = 3.47748 avg_loss = 3.72173\n",
      "epoch no.2 train no.225630  loss = 3.34778 avg_loss = 3.69616\n",
      "epoch no.2 train no.225640  loss = 4.52769 avg_loss = 3.68385\n",
      "epoch no.2 train no.225650  loss = 1.82969 avg_loss = 3.67306\n",
      "epoch no.2 train no.225660  loss = 3.75728 avg_loss = 3.68318\n",
      "epoch no.2 train no.225670  loss = 5.01158 avg_loss = 3.68093\n",
      "epoch no.2 train no.225680  loss = 5.35244 avg_loss = 3.67891\n",
      "epoch no.2 train no.225690  loss = 4.48417 avg_loss = 3.69457\n",
      "epoch no.2 train no.225700  loss = 2.61790 avg_loss = 3.66054\n",
      "epoch no.2 train no.225710  loss = 3.86927 avg_loss = 3.70906\n",
      "epoch no.2 train no.225720  loss = 3.29138 avg_loss = 3.75410\n",
      "epoch no.2 train no.225730  loss = 3.70287 avg_loss = 3.78572\n",
      "epoch no.2 train no.225740  loss = 5.38402 avg_loss = 3.79352\n",
      "epoch no.2 train no.225750  loss = 2.80226 avg_loss = 3.74084\n",
      "epoch no.2 train no.225760  loss = 4.66832 avg_loss = 3.72998\n",
      "epoch no.2 train no.225770  loss = 3.53454 avg_loss = 3.74903\n",
      "epoch no.2 train no.225780  loss = 4.73005 avg_loss = 3.77075\n",
      "epoch no.2 train no.225790  loss = 4.42124 avg_loss = 3.72607\n",
      "epoch no.2 train no.225800  loss = 4.52617 avg_loss = 3.70586\n",
      "epoch no.2 train no.225810  loss = 3.58050 avg_loss = 3.68783\n",
      "epoch no.2 train no.225820  loss = 3.57888 avg_loss = 3.72632\n",
      "epoch no.2 train no.225830  loss = 4.07817 avg_loss = 3.69856\n",
      "epoch no.2 train no.225840  loss = 2.69308 avg_loss = 3.63120\n",
      "epoch no.2 train no.225850  loss = 2.98407 avg_loss = 3.68136\n",
      "epoch no.2 train no.225860  loss = 2.65810 avg_loss = 3.66389\n",
      "epoch no.2 train no.225870  loss = 3.66361 avg_loss = 3.64911\n",
      "epoch no.2 train no.225880  loss = 2.95288 avg_loss = 3.60620\n",
      "epoch no.2 train no.225890  loss = 3.56512 avg_loss = 3.61533\n",
      "epoch no.2 train no.225900  loss = 3.34226 avg_loss = 3.59869\n",
      "epoch no.2 train no.225910  loss = 4.06391 avg_loss = 3.60700\n",
      "epoch no.2 train no.225920  loss = 3.31242 avg_loss = 3.65813\n",
      "epoch no.2 train no.225930  loss = 2.37706 avg_loss = 3.61338\n",
      "epoch no.2 train no.225940  loss = 3.70431 avg_loss = 3.60871\n",
      "epoch no.2 train no.225950  loss = 3.78793 avg_loss = 3.62037\n",
      "epoch no.2 train no.225960  loss = 4.09441 avg_loss = 3.62987\n",
      "epoch no.2 train no.225970  loss = 4.07850 avg_loss = 3.66277\n",
      "epoch no.2 train no.225980  loss = 5.18996 avg_loss = 3.65278\n",
      "epoch no.2 train no.225990  loss = 5.07421 avg_loss = 3.64518\n",
      "epoch no.2 train no.226000  loss = 5.51112 avg_loss = 3.62800\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁필요할', '▁때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.2 train no.226010  loss = 4.05267 avg_loss = 3.65233\n",
      "epoch no.2 train no.226020  loss = 4.28636 avg_loss = 3.65203\n",
      "epoch no.2 train no.226030  loss = 3.76023 avg_loss = 3.67879\n",
      "epoch no.2 train no.226040  loss = 2.70583 avg_loss = 3.67212\n",
      "epoch no.2 train no.226050  loss = 4.41890 avg_loss = 3.67577\n",
      "epoch no.2 train no.226060  loss = 4.22651 avg_loss = 3.68305\n",
      "epoch no.2 train no.226070  loss = 3.40263 avg_loss = 3.69013\n",
      "epoch no.2 train no.226080  loss = 5.42026 avg_loss = 3.69001\n",
      "epoch no.2 train no.226090  loss = 4.43728 avg_loss = 3.66371\n",
      "epoch no.2 train no.226100  loss = 3.58319 avg_loss = 3.61191\n",
      "epoch no.2 train no.226110  loss = 4.42540 avg_loss = 3.64082\n",
      "epoch no.2 train no.226120  loss = 2.96914 avg_loss = 3.64970\n",
      "epoch no.2 train no.226130  loss = 3.62873 avg_loss = 3.70682\n",
      "epoch no.2 train no.226140  loss = 3.74133 avg_loss = 3.71011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.226150  loss = 3.07034 avg_loss = 3.65760\n",
      "epoch no.2 train no.226160  loss = 3.37638 avg_loss = 3.59120\n",
      "epoch no.2 train no.226170  loss = 3.95526 avg_loss = 3.63674\n",
      "epoch no.2 train no.226180  loss = 5.05661 avg_loss = 3.67272\n",
      "epoch no.2 train no.226190  loss = 4.59623 avg_loss = 3.65495\n",
      "epoch no.2 train no.226200  loss = 4.56111 avg_loss = 3.71080\n",
      "epoch no.2 train no.226210  loss = 4.51398 avg_loss = 3.73880\n",
      "epoch no.2 train no.226220  loss = 6.12651 avg_loss = 3.74649\n",
      "epoch no.2 train no.226230  loss = 5.52277 avg_loss = 3.81557\n",
      "epoch no.2 train no.226240  loss = 4.91964 avg_loss = 3.81046\n",
      "epoch no.2 train no.226250  loss = 4.19880 avg_loss = 3.79309\n",
      "epoch no.2 train no.226260  loss = 1.66159 avg_loss = 3.74189\n",
      "epoch no.2 train no.226270  loss = 2.99288 avg_loss = 3.73502\n",
      "epoch no.2 train no.226280  loss = 4.36344 avg_loss = 3.71378\n",
      "epoch no.2 train no.226290  loss = 5.78305 avg_loss = 3.73653\n",
      "epoch no.2 train no.226300  loss = 4.46086 avg_loss = 3.75251\n",
      "epoch no.2 train no.226310  loss = 3.40620 avg_loss = 3.73511\n",
      "epoch no.2 train no.226320  loss = 3.11411 avg_loss = 3.71726\n",
      "epoch no.2 train no.226330  loss = 4.96910 avg_loss = 3.70033\n",
      "epoch no.2 train no.226340  loss = 2.32079 avg_loss = 3.68330\n",
      "epoch no.2 train no.226350  loss = 3.31720 avg_loss = 3.63589\n",
      "epoch no.2 train no.226360  loss = 3.09128 avg_loss = 3.62196\n",
      "epoch no.2 train no.226370  loss = 4.82097 avg_loss = 3.64029\n",
      "epoch no.2 train no.226380  loss = 4.53868 avg_loss = 3.63101\n",
      "epoch no.2 train no.226390  loss = 5.00230 avg_loss = 3.61949\n",
      "epoch no.2 train no.226400  loss = 3.77689 avg_loss = 3.65142\n",
      "epoch no.2 train no.226410  loss = 5.93814 avg_loss = 3.62866\n",
      "epoch no.2 train no.226420  loss = 4.03952 avg_loss = 3.61404\n",
      "epoch no.2 train no.226430  loss = 2.78707 avg_loss = 3.58724\n",
      "epoch no.2 train no.226440  loss = 3.17336 avg_loss = 3.57282\n",
      "epoch no.2 train no.226450  loss = 4.66746 avg_loss = 3.62021\n",
      "epoch no.2 train no.226460  loss = 4.23815 avg_loss = 3.55696\n",
      "epoch no.2 train no.226470  loss = 3.08129 avg_loss = 3.56137\n",
      "epoch no.2 train no.226480  loss = 4.11643 avg_loss = 3.56618\n",
      "epoch no.2 train no.226490  loss = 3.63350 avg_loss = 3.59926\n",
      "epoch no.2 train no.226500  loss = 5.07511 avg_loss = 3.61517\n",
      "epoch no.2 train no.226510  loss = 5.45490 avg_loss = 3.67864\n",
      "epoch no.2 train no.226520  loss = 2.98759 avg_loss = 3.69603\n",
      "epoch no.2 train no.226530  loss = 2.81334 avg_loss = 3.66643\n",
      "epoch no.2 train no.226540  loss = 3.19535 avg_loss = 3.65849\n",
      "epoch no.2 train no.226550  loss = 5.45495 avg_loss = 3.67306\n",
      "epoch no.2 train no.226560  loss = 5.70224 avg_loss = 3.71250\n",
      "epoch no.2 train no.226570  loss = 4.72335 avg_loss = 3.70673\n",
      "epoch no.2 train no.226580  loss = 3.55479 avg_loss = 3.71594\n",
      "epoch no.2 train no.226590  loss = 3.60823 avg_loss = 3.73625\n",
      "epoch no.2 train no.226600  loss = 3.37451 avg_loss = 3.72374\n",
      "epoch no.2 train no.226610  loss = 3.52276 avg_loss = 3.77768\n",
      "epoch no.2 train no.226620  loss = 2.40943 avg_loss = 3.73136\n",
      "epoch no.2 train no.226630  loss = 3.81002 avg_loss = 3.73946\n",
      "epoch no.2 train no.226640  loss = 3.08764 avg_loss = 3.70693\n",
      "epoch no.2 train no.226650  loss = 4.43444 avg_loss = 3.70027\n",
      "epoch no.2 train no.226660  loss = 2.74745 avg_loss = 3.71623\n",
      "epoch no.2 train no.226670  loss = 2.48342 avg_loss = 3.66221\n",
      "epoch no.2 train no.226680  loss = 5.83187 avg_loss = 3.71238\n",
      "epoch no.2 train no.226690  loss = 3.39911 avg_loss = 3.67318\n",
      "epoch no.2 train no.226700  loss = 3.90223 avg_loss = 3.73327\n",
      "epoch no.2 train no.226710  loss = 2.51081 avg_loss = 3.75227\n",
      "epoch no.2 train no.226720  loss = 5.73240 avg_loss = 3.73165\n",
      "epoch no.2 train no.226730  loss = 2.33275 avg_loss = 3.70377\n",
      "epoch no.2 train no.226740  loss = 2.49381 avg_loss = 3.68848\n",
      "epoch no.2 train no.226750  loss = 4.54524 avg_loss = 3.67000\n",
      "epoch no.2 train no.226760  loss = 3.26456 avg_loss = 3.67175\n",
      "epoch no.2 train no.226770  loss = 4.41882 avg_loss = 3.69667\n",
      "epoch no.2 train no.226780  loss = 3.94686 avg_loss = 3.71232\n",
      "epoch no.2 train no.226790  loss = 3.17443 avg_loss = 3.70155\n",
      "epoch no.2 train no.226800  loss = 4.04186 avg_loss = 3.68866\n",
      "epoch no.2 train no.226810  loss = 4.20026 avg_loss = 3.71601\n",
      "epoch no.2 train no.226820  loss = 4.86235 avg_loss = 3.73090\n",
      "epoch no.2 train no.226830  loss = 3.86946 avg_loss = 3.70792\n",
      "epoch no.2 train no.226840  loss = 2.71383 avg_loss = 3.70313\n",
      "epoch no.2 train no.226850  loss = 3.94915 avg_loss = 3.71623\n",
      "epoch no.2 train no.226860  loss = 4.22249 avg_loss = 3.73492\n",
      "epoch no.2 train no.226870  loss = 3.48312 avg_loss = 3.71254\n",
      "epoch no.2 train no.226880  loss = 2.00827 avg_loss = 3.69547\n",
      "epoch no.2 train no.226890  loss = 3.38584 avg_loss = 3.64210\n",
      "epoch no.2 train no.226900  loss = 4.87572 avg_loss = 3.64613\n",
      "epoch no.2 train no.226910  loss = 2.32919 avg_loss = 3.63679\n",
      "epoch no.2 train no.226920  loss = 3.20084 avg_loss = 3.63128\n",
      "epoch no.2 train no.226930  loss = 3.34256 avg_loss = 3.61641\n",
      "epoch no.2 train no.226940  loss = 5.60173 avg_loss = 3.64520\n",
      "epoch no.2 train no.226950  loss = 6.27042 avg_loss = 3.62294\n",
      "epoch no.2 train no.226960  loss = 3.24285 avg_loss = 3.64548\n",
      "epoch no.2 train no.226970  loss = 3.20211 avg_loss = 3.60537\n",
      "epoch no.2 train no.226980  loss = 2.87171 avg_loss = 3.59425\n",
      "epoch no.2 train no.226990  loss = 3.52164 avg_loss = 3.61385\n",
      "epoch no.2 train no.227000  loss = 4.15921 avg_loss = 3.62113\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁노래', '한', '▁노래', '</s>']\n",
      "기분전환에 좋은 잔잔한 노래</s>\n",
      "epoch no.2 train no.227010  loss = 3.62243 avg_loss = 3.57822\n",
      "epoch no.2 train no.227020  loss = 3.07692 avg_loss = 3.58089\n",
      "epoch no.2 train no.227030  loss = 2.38627 avg_loss = 3.54399\n",
      "epoch no.2 train no.227040  loss = 3.86263 avg_loss = 3.55149\n",
      "epoch no.2 train no.227050  loss = 5.58146 avg_loss = 3.60237\n",
      "epoch no.2 train no.227060  loss = 2.92138 avg_loss = 3.57776\n",
      "epoch no.2 train no.227070  loss = 2.22858 avg_loss = 3.59052\n",
      "epoch no.2 train no.227080  loss = 3.54501 avg_loss = 3.56027\n",
      "epoch no.2 train no.227090  loss = 2.64090 avg_loss = 3.58011\n",
      "epoch no.2 train no.227100  loss = 3.29453 avg_loss = 3.57794\n",
      "epoch no.2 train no.227110  loss = 2.95399 avg_loss = 3.58185\n",
      "epoch no.2 train no.227120  loss = 3.86219 avg_loss = 3.56929\n",
      "epoch no.2 train no.227130  loss = 4.67681 avg_loss = 3.59544\n",
      "epoch no.2 train no.227140  loss = 4.12382 avg_loss = 3.64993\n",
      "epoch no.2 train no.227150  loss = 1.74077 avg_loss = 3.64988\n",
      "epoch no.2 train no.227160  loss = 2.91276 avg_loss = 3.64514\n",
      "epoch no.2 train no.227170  loss = 6.85206 avg_loss = 3.69141\n",
      "epoch no.2 train no.227180  loss = 3.87738 avg_loss = 3.69097\n",
      "epoch no.2 train no.227190  loss = 3.51341 avg_loss = 3.65775\n",
      "epoch no.2 train no.227200  loss = 4.58405 avg_loss = 3.67882\n",
      "epoch no.2 train no.227210  loss = 3.79084 avg_loss = 3.63623\n",
      "epoch no.2 train no.227220  loss = 2.14821 avg_loss = 3.57579\n",
      "epoch no.2 train no.227230  loss = 2.45401 avg_loss = 3.55867\n",
      "epoch no.2 train no.227240  loss = 5.08949 avg_loss = 3.56137\n",
      "epoch no.2 train no.227250  loss = 2.95518 avg_loss = 3.52717\n",
      "epoch no.2 train no.227260  loss = 2.66545 avg_loss = 3.52540\n",
      "epoch no.2 train no.227270  loss = 4.95500 avg_loss = 3.61370\n",
      "epoch no.2 train no.227280  loss = 4.15821 avg_loss = 3.61014\n",
      "epoch no.2 train no.227290  loss = 3.25881 avg_loss = 3.57211\n",
      "epoch no.2 train no.227300  loss = 1.95759 avg_loss = 3.52753\n",
      "epoch no.2 train no.227310  loss = 5.02644 avg_loss = 3.51892\n",
      "epoch no.2 train no.227320  loss = 2.20469 avg_loss = 3.47667\n",
      "epoch no.2 train no.227330  loss = 3.85529 avg_loss = 3.43955\n",
      "epoch no.2 train no.227340  loss = 3.33319 avg_loss = 3.44663\n",
      "epoch no.2 train no.227350  loss = 2.35942 avg_loss = 3.44368\n",
      "epoch no.2 train no.227360  loss = 3.08086 avg_loss = 3.50348\n",
      "epoch no.2 train no.227370  loss = 2.93188 avg_loss = 3.52038\n",
      "epoch no.2 train no.227380  loss = 4.20724 avg_loss = 3.50198\n",
      "epoch no.2 train no.227390  loss = 1.95159 avg_loss = 3.49814\n",
      "epoch no.2 train no.227400  loss = 1.69796 avg_loss = 3.53629\n",
      "epoch no.2 train no.227410  loss = 2.93625 avg_loss = 3.58193\n",
      "epoch no.2 train no.227420  loss = 4.88704 avg_loss = 3.59304\n",
      "epoch no.2 train no.227430  loss = 2.42221 avg_loss = 3.64214\n",
      "epoch no.2 train no.227440  loss = 4.30132 avg_loss = 3.63980\n",
      "epoch no.2 train no.227450  loss = 3.28002 avg_loss = 3.62259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.227460  loss = 4.18047 avg_loss = 3.58325\n",
      "epoch no.2 train no.227470  loss = 5.27781 avg_loss = 3.61220\n",
      "epoch no.2 train no.227480  loss = 3.04208 avg_loss = 3.63582\n",
      "epoch no.2 train no.227490  loss = 3.78222 avg_loss = 3.64543\n",
      "epoch no.2 train no.227500  loss = 3.00974 avg_loss = 3.60676\n",
      "epoch no.2 train no.227510  loss = 4.33867 avg_loss = 3.63289\n",
      "epoch no.2 train no.227520  loss = 2.81981 avg_loss = 3.63040\n",
      "epoch no.2 train no.227530  loss = 2.46601 avg_loss = 3.63507\n",
      "epoch no.2 train no.227540  loss = 1.90460 avg_loss = 3.65605\n",
      "epoch no.2 train no.227550  loss = 2.57198 avg_loss = 3.62864\n",
      "epoch no.2 train no.227560  loss = 2.58768 avg_loss = 3.59213\n",
      "epoch no.2 train no.227570  loss = 2.93304 avg_loss = 3.59478\n",
      "epoch no.2 train no.227580  loss = 6.53513 avg_loss = 3.60219\n",
      "epoch no.2 train no.227590  loss = 2.18017 avg_loss = 3.61593\n",
      "epoch no.2 train no.227600  loss = 2.75161 avg_loss = 3.67112\n",
      "epoch no.2 train no.227610  loss = 2.46487 avg_loss = 3.72748\n",
      "epoch no.2 train no.227620  loss = 2.11183 avg_loss = 3.73978\n",
      "epoch no.2 train no.227630  loss = 3.37443 avg_loss = 3.66274\n",
      "epoch no.2 train no.227640  loss = 3.29899 avg_loss = 3.63857\n",
      "epoch no.2 train no.227650  loss = 5.25433 avg_loss = 3.71945\n",
      "epoch no.2 train no.227660  loss = 3.40306 avg_loss = 3.76801\n",
      "epoch no.2 train no.227670  loss = 2.59684 avg_loss = 3.69748\n",
      "epoch no.2 train no.227680  loss = 3.45667 avg_loss = 3.68279\n",
      "epoch no.2 train no.227690  loss = 2.29067 avg_loss = 3.62914\n",
      "epoch no.2 train no.227700  loss = 3.01964 avg_loss = 3.59267\n",
      "epoch no.2 train no.227710  loss = 2.54522 avg_loss = 3.55607\n",
      "epoch no.2 train no.227720  loss = 4.18511 avg_loss = 3.51184\n",
      "epoch no.2 train no.227730  loss = 2.26394 avg_loss = 3.50288\n",
      "epoch no.2 train no.227740  loss = 3.60979 avg_loss = 3.51883\n",
      "epoch no.2 train no.227750  loss = 2.89451 avg_loss = 3.49353\n",
      "epoch no.2 train no.227760  loss = 2.76430 avg_loss = 3.55873\n",
      "epoch no.2 train no.227770  loss = 3.69998 avg_loss = 3.56167\n",
      "epoch no.2 train no.227780  loss = 3.73837 avg_loss = 3.54652\n",
      "epoch no.2 train no.227790  loss = 3.83503 avg_loss = 3.52771\n",
      "epoch no.2 train no.227800  loss = 3.63632 avg_loss = 3.56178\n",
      "epoch no.2 train no.227810  loss = 4.53376 avg_loss = 3.54931\n",
      "epoch no.2 train no.227820  loss = 3.95235 avg_loss = 3.56723\n",
      "epoch no.2 train no.227830  loss = 1.97379 avg_loss = 3.55260\n",
      "epoch no.2 train no.227840  loss = 3.18078 avg_loss = 3.54338\n",
      "epoch no.2 train no.227850  loss = 4.41022 avg_loss = 3.58281\n",
      "epoch no.2 train no.227860  loss = 2.93333 avg_loss = 3.60128\n",
      "epoch no.2 train no.227870  loss = 5.30269 avg_loss = 3.67345\n",
      "epoch no.2 train no.227880  loss = 3.08905 avg_loss = 3.68970\n",
      "epoch no.2 train no.227890  loss = 4.49744 avg_loss = 3.67899\n",
      "epoch no.2 train no.227900  loss = 4.56421 avg_loss = 3.65108\n",
      "epoch no.2 train no.227910  loss = 2.78265 avg_loss = 3.61675\n",
      "epoch no.2 train no.227920  loss = 4.94558 avg_loss = 3.64938\n",
      "epoch no.2 train no.227930  loss = 3.94718 avg_loss = 3.61962\n",
      "epoch no.2 train no.227940  loss = 3.37017 avg_loss = 3.58113\n",
      "epoch no.2 train no.227950  loss = 3.39845 avg_loss = 3.56179\n",
      "epoch no.2 train no.227960  loss = 4.08104 avg_loss = 3.55159\n",
      "epoch no.2 train no.227970  loss = 2.71488 avg_loss = 3.57532\n",
      "epoch no.2 train no.227980  loss = 3.17280 avg_loss = 3.55524\n",
      "epoch no.2 train no.227990  loss = 3.79937 avg_loss = 3.54806\n",
      "epoch no.2 train no.228000  loss = 5.14648 avg_loss = 3.59572\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 곡</s>\n",
      "epoch no.2 train no.228010  loss = 2.50765 avg_loss = 3.58318\n",
      "epoch no.2 train no.228020  loss = 2.36730 avg_loss = 3.54314\n",
      "epoch no.2 train no.228030  loss = 2.04109 avg_loss = 3.50360\n",
      "epoch no.2 train no.228040  loss = 3.73499 avg_loss = 3.56874\n",
      "epoch no.2 train no.228050  loss = 6.16862 avg_loss = 3.63251\n",
      "epoch no.2 train no.228060  loss = 4.66872 avg_loss = 3.66367\n",
      "epoch no.2 train no.228070  loss = 4.79750 avg_loss = 3.67479\n",
      "epoch no.2 train no.228080  loss = 2.37342 avg_loss = 3.64725\n",
      "epoch no.2 train no.228090  loss = 2.86773 avg_loss = 3.59509\n",
      "epoch no.2 train no.228100  loss = 4.78655 avg_loss = 3.61143\n",
      "epoch no.2 train no.228110  loss = 3.98187 avg_loss = 3.59910\n",
      "epoch no.2 train no.228120  loss = 4.47288 avg_loss = 3.58981\n",
      "epoch no.2 train no.228130  loss = 3.00341 avg_loss = 3.61988\n",
      "epoch no.2 train no.228140  loss = 4.24723 avg_loss = 3.62858\n",
      "epoch no.2 train no.228150  loss = 2.38413 avg_loss = 3.57982\n",
      "epoch no.2 train no.228160  loss = 2.96421 avg_loss = 3.61359\n",
      "epoch no.2 train no.228170  loss = 3.01026 avg_loss = 3.57418\n",
      "epoch no.2 train no.228180  loss = 4.72172 avg_loss = 3.58891\n",
      "epoch no.2 train no.228190  loss = 2.54302 avg_loss = 3.59561\n",
      "epoch no.2 train no.228200  loss = 3.16149 avg_loss = 3.55674\n",
      "epoch no.2 train no.228210  loss = 3.96692 avg_loss = 3.61382\n",
      "epoch no.2 train no.228220  loss = 3.69255 avg_loss = 3.64427\n",
      "epoch no.2 train no.228230  loss = 3.81526 avg_loss = 3.63452\n",
      "epoch no.2 train no.228240  loss = 4.86708 avg_loss = 3.64749\n",
      "epoch no.2 train no.228250  loss = 3.31686 avg_loss = 3.70031\n",
      "epoch no.2 train no.228260  loss = 4.14491 avg_loss = 3.69799\n",
      "epoch no.2 train no.228270  loss = 2.83592 avg_loss = 3.65635\n",
      "epoch no.2 train no.228280  loss = 4.48250 avg_loss = 3.68716\n",
      "epoch no.2 train no.228290  loss = 3.22417 avg_loss = 3.65140\n",
      "epoch no.2 train no.228300  loss = 4.39961 avg_loss = 3.64780\n",
      "epoch no.2 train no.228310  loss = 2.86748 avg_loss = 3.66919\n",
      "epoch no.2 train no.228320  loss = 3.66967 avg_loss = 3.65673\n",
      "epoch no.2 train no.228330  loss = 4.12698 avg_loss = 3.68448\n",
      "epoch no.2 train no.228340  loss = 4.04627 avg_loss = 3.66401\n",
      "epoch no.2 train no.228350  loss = 3.61052 avg_loss = 3.64336\n",
      "epoch no.2 train no.228360  loss = 3.60740 avg_loss = 3.64168\n",
      "epoch no.2 train no.228370  loss = 3.74017 avg_loss = 3.60167\n",
      "epoch no.2 train no.228380  loss = 4.87074 avg_loss = 3.61841\n",
      "epoch no.2 train no.228390  loss = 3.86533 avg_loss = 3.64756\n",
      "epoch no.2 train no.228400  loss = 3.09901 avg_loss = 3.62816\n",
      "epoch no.2 train no.228410  loss = 6.52475 avg_loss = 3.62087\n",
      "epoch no.2 train no.228420  loss = 4.18063 avg_loss = 3.67309\n",
      "epoch no.2 train no.228430  loss = 4.40676 avg_loss = 3.64798\n",
      "epoch no.2 train no.228440  loss = 2.43374 avg_loss = 3.61692\n",
      "epoch no.2 train no.228450  loss = 3.57062 avg_loss = 3.58998\n",
      "epoch no.2 train no.228460  loss = 2.39015 avg_loss = 3.57270\n",
      "epoch no.2 train no.228470  loss = 4.31775 avg_loss = 3.58550\n",
      "epoch no.2 train no.228480  loss = 3.64601 avg_loss = 3.59250\n",
      "epoch no.2 train no.228490  loss = 3.20947 avg_loss = 3.54090\n",
      "epoch no.2 train no.228500  loss = 2.58718 avg_loss = 3.46656\n",
      "epoch no.2 train no.228510  loss = 4.32276 avg_loss = 3.46653\n",
      "epoch no.2 train no.228520  loss = 4.23373 avg_loss = 3.49202\n",
      "epoch no.2 train no.228530  loss = 4.65511 avg_loss = 3.50545\n",
      "epoch no.2 train no.228540  loss = 3.91550 avg_loss = 3.50222\n",
      "epoch no.2 train no.228550  loss = 4.11479 avg_loss = 3.52512\n",
      "epoch no.2 train no.228560  loss = 5.11249 avg_loss = 3.52816\n",
      "epoch no.2 train no.228570  loss = 3.43046 avg_loss = 3.51972\n",
      "epoch no.2 train no.228580  loss = 3.65925 avg_loss = 3.54647\n",
      "epoch no.2 train no.228590  loss = 3.47783 avg_loss = 3.54841\n",
      "epoch no.2 train no.228600  loss = 2.41433 avg_loss = 3.52536\n",
      "epoch no.2 train no.228610  loss = 3.81665 avg_loss = 3.54007\n",
      "epoch no.2 train no.228620  loss = 2.40820 avg_loss = 3.57284\n",
      "epoch no.2 train no.228630  loss = 3.89064 avg_loss = 3.63682\n",
      "epoch no.2 train no.228640  loss = 2.02707 avg_loss = 3.61812\n",
      "epoch no.2 train no.228650  loss = 5.21092 avg_loss = 3.62726\n",
      "epoch no.2 train no.228660  loss = 2.64237 avg_loss = 3.61381\n",
      "epoch no.2 train no.228670  loss = 3.90527 avg_loss = 3.57940\n",
      "epoch no.2 train no.228680  loss = 5.43458 avg_loss = 3.58701\n",
      "epoch no.2 train no.228690  loss = 6.47951 avg_loss = 3.60859\n",
      "epoch no.2 train no.228700  loss = 3.10798 avg_loss = 3.63027\n",
      "epoch no.2 train no.228710  loss = 5.55710 avg_loss = 3.65298\n",
      "epoch no.2 train no.228720  loss = 2.30135 avg_loss = 3.63289\n",
      "epoch no.2 train no.228730  loss = 4.89805 avg_loss = 3.59308\n",
      "epoch no.2 train no.228740  loss = 3.31084 avg_loss = 3.59113\n",
      "epoch no.2 train no.228750  loss = 2.83735 avg_loss = 3.56580\n",
      "epoch no.2 train no.228760  loss = 4.68738 avg_loss = 3.62777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.228770  loss = 2.30268 avg_loss = 3.62534\n",
      "epoch no.2 train no.228780  loss = 2.75901 avg_loss = 3.55590\n",
      "epoch no.2 train no.228790  loss = 3.28536 avg_loss = 3.55893\n",
      "epoch no.2 train no.228800  loss = 4.29826 avg_loss = 3.55357\n",
      "epoch no.2 train no.228810  loss = 5.91733 avg_loss = 3.53870\n",
      "epoch no.2 train no.228820  loss = 3.17905 avg_loss = 3.52187\n",
      "epoch no.2 train no.228830  loss = 4.20311 avg_loss = 3.56502\n",
      "epoch no.2 train no.228840  loss = 3.01404 avg_loss = 3.55599\n",
      "epoch no.2 train no.228850  loss = 1.50994 avg_loss = 3.54635\n",
      "epoch no.2 train no.228860  loss = 2.37428 avg_loss = 3.52469\n",
      "epoch no.2 train no.228870  loss = 2.69335 avg_loss = 3.49996\n",
      "epoch no.2 train no.228880  loss = 4.55658 avg_loss = 3.52868\n",
      "epoch no.2 train no.228890  loss = 3.28005 avg_loss = 3.49014\n",
      "epoch no.2 train no.228900  loss = 3.60059 avg_loss = 3.47711\n",
      "epoch no.2 train no.228910  loss = 3.63022 avg_loss = 3.49381\n",
      "epoch no.2 train no.228920  loss = 3.13993 avg_loss = 3.47657\n",
      "epoch no.2 train no.228930  loss = 5.45051 avg_loss = 3.51409\n",
      "epoch no.2 train no.228940  loss = 2.99218 avg_loss = 3.49247\n",
      "epoch no.2 train no.228950  loss = 3.65451 avg_loss = 3.52906\n",
      "epoch no.2 train no.228960  loss = 3.46286 avg_loss = 3.55191\n",
      "epoch no.2 train no.228970  loss = 3.88582 avg_loss = 3.58076\n",
      "epoch no.2 train no.228980  loss = 3.38185 avg_loss = 3.58465\n",
      "epoch no.2 train no.228990  loss = 4.18005 avg_loss = 3.61858\n",
      "epoch no.2 train no.229000  loss = 2.82728 avg_loss = 3.60143\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '송', '</s>']\n",
      "기분전환이 필요할 때 듣는 팝송</s>\n",
      "epoch no.2 train no.229010  loss = 2.89112 avg_loss = 3.58203\n",
      "epoch no.2 train no.229020  loss = 2.73210 avg_loss = 3.58496\n",
      "epoch no.2 train no.229030  loss = 1.99741 avg_loss = 3.57652\n",
      "epoch no.2 train no.229040  loss = 2.79588 avg_loss = 3.54652\n",
      "epoch no.2 train no.229050  loss = 3.64157 avg_loss = 3.53299\n",
      "epoch no.2 train no.229060  loss = 5.25859 avg_loss = 3.51905\n",
      "epoch no.2 train no.229070  loss = 5.24725 avg_loss = 3.54094\n",
      "epoch no.2 train no.229080  loss = 3.58207 avg_loss = 3.55079\n",
      "epoch no.2 train no.229090  loss = 2.86346 avg_loss = 3.54208\n",
      "epoch no.2 train no.229100  loss = 2.50441 avg_loss = 3.53320\n",
      "epoch no.2 train no.229110  loss = 2.12732 avg_loss = 3.50842\n",
      "epoch no.2 train no.229120  loss = 2.99388 avg_loss = 3.54548\n",
      "epoch no.2 train no.229130  loss = 3.59168 avg_loss = 3.57187\n",
      "epoch no.2 train no.229140  loss = 3.43519 avg_loss = 3.53992\n",
      "epoch no.2 train no.229150  loss = 4.66154 avg_loss = 3.59487\n",
      "epoch no.2 train no.229160  loss = 4.72266 avg_loss = 3.65750\n",
      "epoch no.2 train no.229170  loss = 3.40210 avg_loss = 3.66708\n",
      "epoch no.2 train no.229180  loss = 3.23046 avg_loss = 3.67738\n",
      "epoch no.2 train no.229190  loss = 3.14615 avg_loss = 3.70419\n",
      "epoch no.2 train no.229200  loss = 2.77055 avg_loss = 3.67321\n",
      "epoch no.2 train no.229210  loss = 4.72335 avg_loss = 3.67612\n",
      "epoch no.2 train no.229220  loss = 2.25358 avg_loss = 3.64808\n",
      "epoch no.2 train no.229230  loss = 2.49572 avg_loss = 3.62553\n",
      "epoch no.2 train no.229240  loss = 2.67939 avg_loss = 3.63146\n",
      "epoch no.2 train no.229250  loss = 1.54086 avg_loss = 3.63403\n",
      "epoch no.2 train no.229260  loss = 2.99515 avg_loss = 3.62133\n",
      "epoch no.2 train no.229270  loss = 2.94192 avg_loss = 3.58496\n",
      "epoch no.2 train no.229280  loss = 3.29682 avg_loss = 3.63192\n",
      "epoch no.2 train no.229290  loss = 2.71914 avg_loss = 3.64204\n",
      "epoch no.2 train no.229300  loss = 3.80831 avg_loss = 3.66441\n",
      "epoch no.2 train no.229310  loss = 3.57722 avg_loss = 3.61202\n",
      "epoch no.2 train no.229320  loss = 2.33085 avg_loss = 3.58419\n",
      "epoch no.2 train no.229330  loss = 3.56987 avg_loss = 3.54149\n",
      "epoch no.2 train no.229340  loss = 2.68172 avg_loss = 3.58724\n",
      "epoch no.2 train no.229350  loss = 3.30259 avg_loss = 3.50968\n",
      "epoch no.2 train no.229360  loss = 3.93020 avg_loss = 3.54318\n",
      "epoch no.2 train no.229370  loss = 4.07897 avg_loss = 3.52743\n",
      "epoch no.2 train no.229380  loss = 3.61458 avg_loss = 3.51045\n",
      "epoch no.2 train no.229390  loss = 2.72652 avg_loss = 3.56652\n",
      "epoch no.2 train no.229400  loss = 4.26715 avg_loss = 3.57703\n",
      "epoch no.2 train no.229410  loss = 3.52956 avg_loss = 3.58873\n",
      "epoch no.2 train no.229420  loss = 4.06249 avg_loss = 3.57353\n",
      "epoch no.2 train no.229430  loss = 1.76242 avg_loss = 3.56656\n",
      "epoch no.2 train no.229440  loss = 3.72451 avg_loss = 3.59213\n",
      "epoch no.2 train no.229450  loss = 2.51764 avg_loss = 3.62603\n",
      "epoch no.2 train no.229460  loss = 3.28004 avg_loss = 3.63791\n",
      "epoch no.2 train no.229470  loss = 4.61631 avg_loss = 3.66286\n",
      "epoch no.2 train no.229480  loss = 1.05082 avg_loss = 3.64302\n",
      "epoch no.2 train no.229490  loss = 4.51758 avg_loss = 3.64341\n",
      "epoch no.2 train no.229500  loss = 7.06541 avg_loss = 3.65251\n",
      "epoch no.2 train no.229510  loss = 3.77524 avg_loss = 3.65472\n",
      "epoch no.2 train no.229520  loss = 3.69246 avg_loss = 3.64425\n",
      "epoch no.2 train no.229530  loss = 4.21218 avg_loss = 3.72737\n",
      "epoch no.2 train no.229540  loss = 3.52649 avg_loss = 3.66543\n",
      "epoch no.2 train no.229550  loss = 4.38740 avg_loss = 3.67079\n",
      "epoch no.2 train no.229560  loss = 2.80962 avg_loss = 3.63919\n",
      "epoch no.2 train no.229570  loss = 5.62399 avg_loss = 3.72061\n",
      "epoch no.2 train no.229580  loss = 4.18297 avg_loss = 3.71456\n",
      "epoch no.2 train no.229590  loss = 5.53559 avg_loss = 3.79005\n",
      "epoch no.2 train no.229600  loss = 3.19550 avg_loss = 3.77159\n",
      "epoch no.2 train no.229610  loss = 2.82314 avg_loss = 3.73901\n",
      "epoch no.2 train no.229620  loss = 5.65576 avg_loss = 3.78323\n",
      "epoch no.2 train no.229630  loss = 3.81381 avg_loss = 3.76556\n",
      "epoch no.2 train no.229640  loss = 4.28437 avg_loss = 3.73581\n",
      "epoch no.2 train no.229650  loss = 3.17684 avg_loss = 3.73327\n",
      "epoch no.2 train no.229660  loss = 3.22236 avg_loss = 3.70370\n",
      "epoch no.2 train no.229670  loss = 4.21361 avg_loss = 3.73340\n",
      "epoch no.2 train no.229680  loss = 2.41205 avg_loss = 3.68415\n",
      "epoch no.2 train no.229690  loss = 2.86122 avg_loss = 3.67093\n",
      "epoch no.2 train no.229700  loss = 2.84157 avg_loss = 3.66886\n",
      "epoch no.2 train no.229710  loss = 5.27416 avg_loss = 3.72261\n",
      "epoch no.2 train no.229720  loss = 2.53083 avg_loss = 3.73386\n",
      "epoch no.2 train no.229730  loss = 5.70720 avg_loss = 3.69014\n",
      "epoch no.2 train no.229740  loss = 2.60260 avg_loss = 3.75071\n",
      "epoch no.2 train no.229750  loss = 3.17878 avg_loss = 3.71781\n",
      "epoch no.2 train no.229760  loss = 2.81158 avg_loss = 3.71201\n",
      "epoch no.2 train no.229770  loss = 4.68252 avg_loss = 3.75959\n",
      "epoch no.2 train no.229780  loss = 4.72245 avg_loss = 3.73757\n",
      "epoch no.2 train no.229790  loss = 2.14600 avg_loss = 3.81494\n",
      "epoch no.2 train no.229800  loss = 3.13234 avg_loss = 3.78867\n",
      "epoch no.2 train no.229810  loss = 4.04050 avg_loss = 3.79141\n",
      "epoch no.2 train no.229820  loss = 2.41775 avg_loss = 3.72967\n",
      "epoch no.2 train no.229830  loss = 2.53802 avg_loss = 3.66193\n",
      "epoch no.2 train no.229840  loss = 2.15451 avg_loss = 3.64021\n",
      "epoch no.2 train no.229850  loss = 5.72508 avg_loss = 3.67158\n",
      "epoch no.2 train no.229860  loss = 3.08202 avg_loss = 3.69640\n",
      "epoch no.2 train no.229870  loss = 4.57958 avg_loss = 3.67078\n",
      "epoch no.2 train no.229880  loss = 3.70087 avg_loss = 3.68910\n",
      "epoch no.2 train no.229890  loss = 4.72193 avg_loss = 3.69352\n",
      "epoch no.2 train no.229900  loss = 4.09605 avg_loss = 3.68479\n",
      "epoch no.2 train no.229910  loss = 2.91215 avg_loss = 3.61178\n",
      "epoch no.2 train no.229920  loss = 3.38532 avg_loss = 3.59502\n",
      "epoch no.2 train no.229930  loss = 2.76335 avg_loss = 3.55839\n",
      "epoch no.2 train no.229940  loss = 2.32320 avg_loss = 3.56171\n",
      "epoch no.2 train no.229950  loss = 2.67013 avg_loss = 3.59302\n",
      "epoch no.2 train no.229960  loss = 3.15309 avg_loss = 3.63472\n",
      "epoch no.2 train no.229970  loss = 4.14730 avg_loss = 3.65547\n",
      "epoch no.2 train no.229980  loss = 5.50595 avg_loss = 3.70287\n",
      "epoch no.2 train no.229990  loss = 2.92423 avg_loss = 3.67280\n",
      "epoch no.2 train no.230000  loss = 2.72953 avg_loss = 3.64344\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.230010  loss = 5.38416 avg_loss = 3.69976\n",
      "epoch no.2 train no.230020  loss = 5.55870 avg_loss = 3.71363\n",
      "epoch no.2 train no.230030  loss = 5.34554 avg_loss = 3.75455\n",
      "epoch no.2 train no.230040  loss = 2.80519 avg_loss = 3.72754\n",
      "epoch no.2 train no.230050  loss = 2.27473 avg_loss = 3.73984\n",
      "epoch no.2 train no.230060  loss = 4.03156 avg_loss = 3.70280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.230070  loss = 2.39635 avg_loss = 3.64610\n",
      "epoch no.2 train no.230080  loss = 2.33252 avg_loss = 3.67776\n",
      "epoch no.2 train no.230090  loss = 2.93081 avg_loss = 3.65075\n",
      "epoch no.2 train no.230100  loss = 3.55958 avg_loss = 3.65879\n",
      "epoch no.2 train no.230110  loss = 1.96402 avg_loss = 3.65383\n",
      "epoch no.2 train no.230120  loss = 4.94419 avg_loss = 3.67772\n",
      "epoch no.2 train no.230130  loss = 2.91479 avg_loss = 3.69177\n",
      "epoch no.2 train no.230140  loss = 3.88922 avg_loss = 3.70536\n",
      "epoch no.2 train no.230150  loss = 2.80173 avg_loss = 3.67557\n",
      "epoch no.2 train no.230160  loss = 4.27479 avg_loss = 3.69450\n",
      "epoch no.2 train no.230170  loss = 4.28132 avg_loss = 3.68379\n",
      "epoch no.2 train no.230180  loss = 3.91323 avg_loss = 3.65788\n",
      "epoch no.2 train no.230190  loss = 4.23502 avg_loss = 3.65259\n",
      "epoch no.2 train no.230200  loss = 3.54775 avg_loss = 3.64642\n",
      "epoch no.2 train no.230210  loss = 4.33857 avg_loss = 3.67743\n",
      "epoch no.2 train no.230220  loss = 2.62860 avg_loss = 3.63318\n",
      "epoch no.2 train no.230230  loss = 2.78388 avg_loss = 3.65002\n",
      "epoch no.2 train no.230240  loss = 3.64102 avg_loss = 3.57326\n",
      "epoch no.2 train no.230250  loss = 3.66609 avg_loss = 3.58926\n",
      "epoch no.2 train no.230260  loss = 3.15242 avg_loss = 3.60526\n",
      "epoch no.2 train no.230270  loss = 4.17645 avg_loss = 3.60191\n",
      "epoch no.2 train no.230280  loss = 4.34022 avg_loss = 3.61173\n",
      "epoch no.2 train no.230290  loss = 4.39513 avg_loss = 3.60371\n",
      "epoch no.2 train no.230300  loss = 2.85262 avg_loss = 3.61145\n",
      "epoch no.2 train no.230310  loss = 2.29211 avg_loss = 3.55870\n",
      "epoch no.2 train no.230320  loss = 5.30301 avg_loss = 3.62815\n",
      "epoch no.2 train no.230330  loss = 5.76643 avg_loss = 3.64494\n",
      "epoch no.2 train no.230340  loss = 4.44236 avg_loss = 3.66438\n",
      "epoch no.2 train no.230350  loss = 3.79252 avg_loss = 3.63564\n",
      "epoch no.2 train no.230360  loss = 6.78501 avg_loss = 3.62494\n",
      "epoch no.2 train no.230370  loss = 3.89613 avg_loss = 3.65525\n",
      "epoch no.2 train no.230380  loss = 3.50966 avg_loss = 3.72523\n",
      "epoch no.2 train no.230390  loss = 2.77514 avg_loss = 3.67792\n",
      "epoch no.2 train no.230400  loss = 3.86626 avg_loss = 3.65736\n",
      "epoch no.2 train no.230410  loss = 2.04372 avg_loss = 3.58487\n",
      "epoch no.2 train no.230420  loss = 3.16609 avg_loss = 3.59184\n",
      "epoch no.2 train no.230430  loss = 2.91214 avg_loss = 3.59168\n",
      "epoch no.2 train no.230440  loss = 4.35410 avg_loss = 3.60573\n",
      "epoch no.2 train no.230450  loss = 4.12056 avg_loss = 3.59885\n",
      "epoch no.2 train no.230460  loss = 4.48122 avg_loss = 3.57938\n",
      "epoch no.2 train no.230470  loss = 1.89260 avg_loss = 3.54605\n",
      "epoch no.2 train no.230480  loss = 2.19318 avg_loss = 3.56479\n",
      "epoch no.2 train no.230490  loss = 2.34818 avg_loss = 3.51013\n",
      "epoch no.2 train no.230500  loss = 4.89873 avg_loss = 3.53984\n",
      "epoch no.2 train no.230510  loss = 3.37896 avg_loss = 3.57823\n",
      "epoch no.2 train no.230520  loss = 3.48656 avg_loss = 3.65566\n",
      "epoch no.2 train no.230530  loss = 2.28476 avg_loss = 3.62851\n",
      "epoch no.2 train no.230540  loss = 3.37678 avg_loss = 3.62022\n",
      "epoch no.2 train no.230550  loss = 2.26595 avg_loss = 3.62209\n",
      "epoch no.2 train no.230560  loss = 4.08478 avg_loss = 3.63730\n",
      "epoch no.2 train no.230570  loss = 4.35425 avg_loss = 3.64406\n",
      "epoch no.2 train no.230580  loss = 2.82905 avg_loss = 3.62235\n",
      "epoch no.2 train no.230590  loss = 5.49812 avg_loss = 3.64982\n",
      "epoch no.2 train no.230600  loss = 4.89585 avg_loss = 3.70056\n",
      "epoch no.2 train no.230610  loss = 2.45846 avg_loss = 3.69638\n",
      "epoch no.2 train no.230620  loss = 2.95660 avg_loss = 3.70238\n",
      "epoch no.2 train no.230630  loss = 3.73734 avg_loss = 3.67733\n",
      "epoch no.2 train no.230640  loss = 4.04022 avg_loss = 3.64350\n",
      "epoch no.2 train no.230650  loss = 1.08348 avg_loss = 3.57892\n",
      "epoch no.2 train no.230660  loss = 3.39540 avg_loss = 3.56005\n",
      "epoch no.2 train no.230670  loss = 2.45761 avg_loss = 3.56305\n",
      "epoch no.2 train no.230680  loss = 3.16105 avg_loss = 3.58652\n",
      "epoch no.2 train no.230690  loss = 4.59910 avg_loss = 3.62170\n",
      "epoch no.2 train no.230700  loss = 3.97257 avg_loss = 3.63633\n",
      "epoch no.2 train no.230710  loss = 3.15895 avg_loss = 3.62868\n",
      "epoch no.2 train no.230720  loss = 2.08678 avg_loss = 3.60004\n",
      "epoch no.2 train no.230730  loss = 3.62782 avg_loss = 3.59437\n",
      "epoch no.2 train no.230740  loss = 5.74927 avg_loss = 3.56847\n",
      "epoch no.2 train no.230750  loss = 5.03112 avg_loss = 3.60673\n",
      "epoch no.2 train no.230760  loss = 3.53201 avg_loss = 3.61571\n",
      "epoch no.2 train no.230770  loss = 2.44721 avg_loss = 3.59255\n",
      "epoch no.2 train no.230780  loss = 2.63265 avg_loss = 3.59185\n",
      "epoch no.2 train no.230790  loss = 3.26383 avg_loss = 3.60003\n",
      "epoch no.2 train no.230800  loss = 4.43957 avg_loss = 3.63569\n",
      "epoch no.2 train no.230810  loss = 5.52430 avg_loss = 3.71791\n",
      "epoch no.2 train no.230820  loss = 2.93137 avg_loss = 3.67996\n",
      "epoch no.2 train no.230830  loss = 3.76802 avg_loss = 3.64751\n",
      "epoch no.2 train no.230840  loss = 2.88974 avg_loss = 3.60173\n",
      "epoch no.2 train no.230850  loss = 3.72931 avg_loss = 3.58001\n",
      "epoch no.2 train no.230860  loss = 2.62285 avg_loss = 3.59082\n",
      "epoch no.2 train no.230870  loss = 2.47996 avg_loss = 3.58700\n",
      "epoch no.2 train no.230880  loss = 3.27029 avg_loss = 3.56826\n",
      "epoch no.2 train no.230890  loss = 4.94209 avg_loss = 3.57604\n",
      "epoch no.2 train no.230900  loss = 5.05902 avg_loss = 3.58026\n",
      "epoch no.2 train no.230910  loss = 3.89210 avg_loss = 3.64528\n",
      "epoch no.2 train no.230920  loss = 5.00699 avg_loss = 3.67854\n",
      "epoch no.2 train no.230930  loss = 3.92214 avg_loss = 3.69498\n",
      "epoch no.2 train no.230940  loss = 4.00194 avg_loss = 3.70025\n",
      "epoch no.2 train no.230950  loss = 2.53495 avg_loss = 3.65782\n",
      "epoch no.2 train no.230960  loss = 2.22317 avg_loss = 3.66153\n",
      "epoch no.2 train no.230970  loss = 2.47487 avg_loss = 3.70476\n",
      "epoch no.2 train no.230980  loss = 3.46125 avg_loss = 3.73702\n",
      "epoch no.2 train no.230990  loss = 4.47934 avg_loss = 3.73639\n",
      "epoch no.2 train no.231000  loss = 4.66838 avg_loss = 3.75362\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁신나는', '</s>', '</s>']\n",
      "기분전환되는 노래들</s>\n",
      "epoch no.2 train no.231010  loss = 3.02628 avg_loss = 3.73200\n",
      "epoch no.2 train no.231020  loss = 4.95541 avg_loss = 3.75626\n",
      "epoch no.2 train no.231030  loss = 4.84176 avg_loss = 3.74681\n",
      "epoch no.2 train no.231040  loss = 2.42055 avg_loss = 3.76412\n",
      "epoch no.2 train no.231050  loss = 2.55369 avg_loss = 3.72300\n",
      "epoch no.2 train no.231060  loss = 2.98257 avg_loss = 3.72581\n",
      "epoch no.2 train no.231070  loss = 3.64072 avg_loss = 3.64604\n",
      "epoch no.2 train no.231080  loss = 3.62603 avg_loss = 3.67362\n",
      "epoch no.2 train no.231090  loss = 2.96134 avg_loss = 3.68302\n",
      "epoch no.2 train no.231100  loss = 4.14003 avg_loss = 3.68420\n",
      "epoch no.2 train no.231110  loss = 3.01811 avg_loss = 3.67665\n",
      "epoch no.2 train no.231120  loss = 2.65019 avg_loss = 3.62258\n",
      "epoch no.2 train no.231130  loss = 5.60541 avg_loss = 3.61580\n",
      "epoch no.2 train no.231140  loss = 3.24602 avg_loss = 3.58084\n",
      "epoch no.2 train no.231150  loss = 3.44833 avg_loss = 3.59979\n",
      "epoch no.2 train no.231160  loss = 3.61556 avg_loss = 3.60397\n",
      "epoch no.2 train no.231170  loss = 5.56353 avg_loss = 3.69754\n",
      "epoch no.2 train no.231180  loss = 1.47383 avg_loss = 3.62538\n",
      "epoch no.2 train no.231190  loss = 3.28229 avg_loss = 3.63792\n",
      "epoch no.2 train no.231200  loss = 2.96706 avg_loss = 3.68618\n",
      "epoch no.2 train no.231210  loss = 3.81191 avg_loss = 3.70348\n",
      "epoch no.2 train no.231220  loss = 3.54171 avg_loss = 3.66416\n",
      "epoch no.2 train no.231230  loss = 3.81900 avg_loss = 3.62747\n",
      "epoch no.2 train no.231240  loss = 3.86300 avg_loss = 3.58711\n",
      "epoch no.2 train no.231250  loss = 3.26965 avg_loss = 3.62202\n",
      "epoch no.2 train no.231260  loss = 4.35566 avg_loss = 3.59519\n",
      "epoch no.2 train no.231270  loss = 2.00651 avg_loss = 3.58356\n",
      "epoch no.2 train no.231280  loss = 2.95864 avg_loss = 3.58428\n",
      "epoch no.2 train no.231290  loss = 2.75191 avg_loss = 3.58976\n",
      "epoch no.2 train no.231300  loss = 4.51164 avg_loss = 3.56355\n",
      "epoch no.2 train no.231310  loss = 2.18065 avg_loss = 3.55305\n",
      "epoch no.2 train no.231320  loss = 3.68023 avg_loss = 3.57046\n",
      "epoch no.2 train no.231330  loss = 4.48575 avg_loss = 3.55080\n",
      "epoch no.2 train no.231340  loss = 3.51267 avg_loss = 3.55689\n",
      "epoch no.2 train no.231350  loss = 2.62766 avg_loss = 3.50328\n",
      "epoch no.2 train no.231360  loss = 2.87079 avg_loss = 3.54202\n",
      "epoch no.2 train no.231370  loss = 4.11462 avg_loss = 3.60318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.231380  loss = 4.41217 avg_loss = 3.62871\n",
      "epoch no.2 train no.231390  loss = 1.99834 avg_loss = 3.62676\n",
      "epoch no.2 train no.231400  loss = 3.23375 avg_loss = 3.59532\n",
      "epoch no.2 train no.231410  loss = 4.10011 avg_loss = 3.60843\n",
      "epoch no.2 train no.231420  loss = 3.11571 avg_loss = 3.58162\n",
      "epoch no.2 train no.231430  loss = 3.69874 avg_loss = 3.61984\n",
      "epoch no.2 train no.231440  loss = 4.05087 avg_loss = 3.67369\n",
      "epoch no.2 train no.231450  loss = 3.30046 avg_loss = 3.69141\n",
      "epoch no.2 train no.231460  loss = 4.19430 avg_loss = 3.70944\n",
      "epoch no.2 train no.231470  loss = 2.91502 avg_loss = 3.68668\n",
      "epoch no.2 train no.231480  loss = 1.41171 avg_loss = 3.63947\n",
      "epoch no.2 train no.231490  loss = 3.82033 avg_loss = 3.61711\n",
      "epoch no.2 train no.231500  loss = 2.32131 avg_loss = 3.59169\n",
      "epoch no.2 train no.231510  loss = 3.25807 avg_loss = 3.58332\n",
      "epoch no.2 train no.231520  loss = 3.80013 avg_loss = 3.63729\n",
      "epoch no.2 train no.231530  loss = 3.87262 avg_loss = 3.73435\n",
      "epoch no.2 train no.231540  loss = 2.97022 avg_loss = 3.66581\n",
      "epoch no.2 train no.231550  loss = 3.02065 avg_loss = 3.66352\n",
      "epoch no.2 train no.231560  loss = 3.26036 avg_loss = 3.65635\n",
      "epoch no.2 train no.231570  loss = 2.43510 avg_loss = 3.61810\n",
      "epoch no.2 train no.231580  loss = 4.28247 avg_loss = 3.61720\n",
      "epoch no.2 train no.231590  loss = 3.64878 avg_loss = 3.62778\n",
      "epoch no.2 train no.231600  loss = 3.41273 avg_loss = 3.63459\n",
      "epoch no.2 train no.231610  loss = 3.18364 avg_loss = 3.64871\n",
      "epoch no.2 train no.231620  loss = 3.51884 avg_loss = 3.57767\n",
      "epoch no.2 train no.231630  loss = 3.93679 avg_loss = 3.60029\n",
      "epoch no.2 train no.231640  loss = 5.10731 avg_loss = 3.60962\n",
      "epoch no.2 train no.231650  loss = 5.36474 avg_loss = 3.66196\n",
      "epoch no.2 train no.231660  loss = 4.40377 avg_loss = 3.63681\n",
      "epoch no.2 train no.231670  loss = 2.26207 avg_loss = 3.61672\n",
      "epoch no.2 train no.231680  loss = 4.46856 avg_loss = 3.59654\n",
      "epoch no.2 train no.231690  loss = 2.89631 avg_loss = 3.58058\n",
      "epoch no.2 train no.231700  loss = 4.56551 avg_loss = 3.62756\n",
      "epoch no.2 train no.231710  loss = 2.26864 avg_loss = 3.65444\n",
      "epoch no.2 train no.231720  loss = 2.38411 avg_loss = 3.64059\n",
      "epoch no.2 train no.231730  loss = 3.00266 avg_loss = 3.66627\n",
      "epoch no.2 train no.231740  loss = 5.35534 avg_loss = 3.68983\n",
      "epoch no.2 train no.231750  loss = 3.00079 avg_loss = 3.69054\n",
      "epoch no.2 train no.231760  loss = 4.68970 avg_loss = 3.73197\n",
      "epoch no.2 train no.231770  loss = 2.39388 avg_loss = 3.64255\n",
      "epoch no.2 train no.231780  loss = 2.89811 avg_loss = 3.66986\n",
      "epoch no.2 train no.231790  loss = 3.94321 avg_loss = 3.70896\n",
      "epoch no.2 train no.231800  loss = 3.11268 avg_loss = 3.68374\n",
      "epoch no.2 train no.231810  loss = 5.30162 avg_loss = 3.67254\n",
      "epoch no.2 train no.231820  loss = 3.16581 avg_loss = 3.66379\n",
      "epoch no.2 train no.231830  loss = 3.14350 avg_loss = 3.65599\n",
      "epoch no.2 train no.231840  loss = 3.93424 avg_loss = 3.66918\n",
      "epoch no.2 train no.231850  loss = 2.67522 avg_loss = 3.63382\n",
      "epoch no.2 train no.231860  loss = 3.03186 avg_loss = 3.62290\n",
      "epoch no.2 train no.231870  loss = 2.57528 avg_loss = 3.61993\n",
      "epoch no.2 train no.231880  loss = 2.51507 avg_loss = 3.55880\n",
      "epoch no.2 train no.231890  loss = 4.18174 avg_loss = 3.58187\n",
      "epoch no.2 train no.231900  loss = 2.72662 avg_loss = 3.60112\n",
      "epoch no.2 train no.231910  loss = 5.20781 avg_loss = 3.62856\n",
      "epoch no.2 train no.231920  loss = 2.75806 avg_loss = 3.66614\n",
      "epoch no.2 train no.231930  loss = 3.66551 avg_loss = 3.65421\n",
      "epoch no.2 train no.231940  loss = 3.40436 avg_loss = 3.63453\n",
      "epoch no.2 train no.231950  loss = 3.96107 avg_loss = 3.61920\n",
      "epoch no.2 train no.231960  loss = 2.28690 avg_loss = 3.62322\n",
      "epoch no.2 train no.231970  loss = 3.18989 avg_loss = 3.62932\n",
      "epoch no.2 train no.231980  loss = 2.74193 avg_loss = 3.60547\n",
      "epoch no.2 train no.231990  loss = 2.73912 avg_loss = 3.53646\n",
      "epoch no.2 train no.232000  loss = 3.79481 avg_loss = 3.55455\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 음악</s>\n",
      "epoch no.2 train no.232010  loss = 4.95978 avg_loss = 3.54944\n",
      "epoch no.2 train no.232020  loss = 2.98751 avg_loss = 3.57762\n",
      "epoch no.2 train no.232030  loss = 3.72520 avg_loss = 3.62559\n",
      "epoch no.2 train no.232040  loss = 2.22624 avg_loss = 3.59740\n",
      "epoch no.2 train no.232050  loss = 3.15891 avg_loss = 3.61297\n",
      "epoch no.2 train no.232060  loss = 3.86673 avg_loss = 3.63428\n",
      "epoch no.2 train no.232070  loss = 4.94486 avg_loss = 3.62485\n",
      "epoch no.2 train no.232080  loss = 5.72338 avg_loss = 3.66175\n",
      "epoch no.2 train no.232090  loss = 2.67177 avg_loss = 3.66835\n",
      "epoch no.2 train no.232100  loss = 2.41767 avg_loss = 3.62372\n",
      "epoch no.2 train no.232110  loss = 4.03101 avg_loss = 3.58605\n",
      "epoch no.2 train no.232120  loss = 6.17093 avg_loss = 3.63829\n",
      "epoch no.2 train no.232130  loss = 6.05108 avg_loss = 3.70955\n",
      "epoch no.2 train no.232140  loss = 3.10233 avg_loss = 3.69712\n",
      "epoch no.2 train no.232150  loss = 3.54748 avg_loss = 3.68129\n",
      "epoch no.2 train no.232160  loss = 4.33601 avg_loss = 3.70708\n",
      "epoch no.2 train no.232170  loss = 4.74589 avg_loss = 3.70466\n",
      "epoch no.2 train no.232180  loss = 3.50185 avg_loss = 3.74338\n",
      "epoch no.2 train no.232190  loss = 1.87636 avg_loss = 3.78799\n",
      "epoch no.2 train no.232200  loss = 3.72530 avg_loss = 3.74745\n",
      "epoch no.2 train no.232210  loss = 2.47725 avg_loss = 3.69707\n",
      "epoch no.2 train no.232220  loss = 6.21276 avg_loss = 3.68921\n",
      "epoch no.2 train no.232230  loss = 4.51631 avg_loss = 3.74590\n",
      "epoch no.2 train no.232240  loss = 2.58628 avg_loss = 3.68761\n",
      "epoch no.2 train no.232250  loss = 2.54998 avg_loss = 3.59624\n",
      "epoch no.2 train no.232260  loss = 3.92517 avg_loss = 3.55349\n",
      "epoch no.2 train no.232270  loss = 3.28891 avg_loss = 3.54056\n",
      "epoch no.2 train no.232280  loss = 3.66398 avg_loss = 3.58052\n",
      "epoch no.2 train no.232290  loss = 6.86879 avg_loss = 3.63682\n",
      "epoch no.2 train no.232300  loss = 3.86146 avg_loss = 3.66353\n",
      "epoch no.2 train no.232310  loss = 2.73582 avg_loss = 3.60985\n",
      "epoch no.2 train no.232320  loss = 3.97061 avg_loss = 3.63276\n",
      "epoch no.2 train no.232330  loss = 5.42089 avg_loss = 3.69454\n",
      "epoch no.2 train no.232340  loss = 5.10786 avg_loss = 3.64794\n",
      "epoch no.2 train no.232350  loss = 3.16999 avg_loss = 3.66047\n",
      "epoch no.2 train no.232360  loss = 4.53666 avg_loss = 3.65324\n",
      "epoch no.2 train no.232370  loss = 5.54464 avg_loss = 3.70530\n",
      "epoch no.2 train no.232380  loss = 3.06895 avg_loss = 3.63882\n",
      "epoch no.2 train no.232390  loss = 1.82357 avg_loss = 3.60081\n",
      "epoch no.2 train no.232400  loss = 3.50823 avg_loss = 3.62660\n",
      "epoch no.2 train no.232410  loss = 3.76166 avg_loss = 3.61974\n",
      "epoch no.2 train no.232420  loss = 2.47009 avg_loss = 3.54975\n",
      "epoch no.2 train no.232430  loss = 2.13735 avg_loss = 3.55837\n",
      "epoch no.2 train no.232440  loss = 2.86622 avg_loss = 3.60133\n",
      "epoch no.2 train no.232450  loss = 3.40240 avg_loss = 3.61059\n",
      "epoch no.2 train no.232460  loss = 2.69534 avg_loss = 3.57005\n",
      "epoch no.2 train no.232470  loss = 2.54940 avg_loss = 3.61956\n",
      "epoch no.2 train no.232480  loss = 2.81399 avg_loss = 3.59445\n",
      "epoch no.2 train no.232490  loss = 4.19083 avg_loss = 3.64072\n",
      "epoch no.2 train no.232500  loss = 2.37685 avg_loss = 3.60619\n",
      "epoch no.2 train no.232510  loss = 2.52479 avg_loss = 3.58251\n",
      "epoch no.2 train no.232520  loss = 3.00324 avg_loss = 3.61788\n",
      "epoch no.2 train no.232530  loss = 2.75040 avg_loss = 3.59026\n",
      "epoch no.2 train no.232540  loss = 1.93841 avg_loss = 3.58565\n",
      "epoch no.2 train no.232550  loss = 2.09205 avg_loss = 3.58829\n",
      "epoch no.2 train no.232560  loss = 2.99460 avg_loss = 3.61788\n",
      "epoch no.2 train no.232570  loss = 3.44003 avg_loss = 3.65771\n",
      "epoch no.2 train no.232580  loss = 6.27762 avg_loss = 3.70516\n",
      "epoch no.2 train no.232590  loss = 4.80168 avg_loss = 3.74188\n",
      "epoch no.2 train no.232600  loss = 3.19420 avg_loss = 3.72049\n",
      "epoch no.2 train no.232610  loss = 3.30790 avg_loss = 3.66477\n",
      "epoch no.2 train no.232620  loss = 4.41569 avg_loss = 3.62443\n",
      "epoch no.2 train no.232630  loss = 4.70131 avg_loss = 3.67034\n",
      "epoch no.2 train no.232640  loss = 3.44007 avg_loss = 3.68888\n",
      "epoch no.2 train no.232650  loss = 3.31062 avg_loss = 3.69633\n",
      "epoch no.2 train no.232660  loss = 2.93648 avg_loss = 3.60405\n",
      "epoch no.2 train no.232670  loss = 3.27816 avg_loss = 3.58959\n",
      "epoch no.2 train no.232680  loss = 2.74651 avg_loss = 3.59648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.232690  loss = 5.74173 avg_loss = 3.62599\n",
      "epoch no.2 train no.232700  loss = 4.16776 avg_loss = 3.59883\n",
      "epoch no.2 train no.232710  loss = 4.90177 avg_loss = 3.62241\n",
      "epoch no.2 train no.232720  loss = 3.47962 avg_loss = 3.66541\n",
      "epoch no.2 train no.232730  loss = 3.08315 avg_loss = 3.70399\n",
      "epoch no.2 train no.232740  loss = 1.30952 avg_loss = 3.66475\n",
      "epoch no.2 train no.232750  loss = 4.46371 avg_loss = 3.62075\n",
      "epoch no.2 train no.232760  loss = 3.63043 avg_loss = 3.58488\n",
      "epoch no.2 train no.232770  loss = 4.29661 avg_loss = 3.61117\n",
      "epoch no.2 train no.232780  loss = 3.95706 avg_loss = 3.61547\n",
      "epoch no.2 train no.232790  loss = 6.11160 avg_loss = 3.63621\n",
      "epoch no.2 train no.232800  loss = 7.01483 avg_loss = 3.62631\n",
      "epoch no.2 train no.232810  loss = 5.63727 avg_loss = 3.68556\n",
      "epoch no.2 train no.232820  loss = 2.68722 avg_loss = 3.71605\n",
      "epoch no.2 train no.232830  loss = 3.69751 avg_loss = 3.72476\n",
      "epoch no.2 train no.232840  loss = 4.17988 avg_loss = 3.71635\n",
      "epoch no.2 train no.232850  loss = 3.40130 avg_loss = 3.76596\n",
      "epoch no.2 train no.232860  loss = 3.13441 avg_loss = 3.69331\n",
      "epoch no.2 train no.232870  loss = 3.49253 avg_loss = 3.68805\n",
      "epoch no.2 train no.232880  loss = 3.38837 avg_loss = 3.72545\n",
      "epoch no.2 train no.232890  loss = 1.89371 avg_loss = 3.70666\n",
      "epoch no.2 train no.232900  loss = 3.88227 avg_loss = 3.72693\n",
      "epoch no.2 train no.232910  loss = 2.32223 avg_loss = 3.74611\n",
      "epoch no.2 train no.232920  loss = 4.57874 avg_loss = 3.76308\n",
      "epoch no.2 train no.232930  loss = 2.68921 avg_loss = 3.76978\n",
      "epoch no.2 train no.232940  loss = 1.63697 avg_loss = 3.74639\n",
      "epoch no.2 train no.232950  loss = 2.75146 avg_loss = 3.78454\n",
      "epoch no.2 train no.232960  loss = 1.92815 avg_loss = 3.70048\n",
      "epoch no.2 train no.232970  loss = 2.42487 avg_loss = 3.65275\n",
      "epoch no.2 train no.232980  loss = 3.70922 avg_loss = 3.61277\n",
      "epoch no.2 train no.232990  loss = 5.05098 avg_loss = 3.65179\n",
      "epoch no.2 train no.233000  loss = 2.98889 avg_loss = 3.69397\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁노래', '송', '</s>']\n",
      "기분전환에 좋은 팝송</s>\n",
      "epoch no.2 train no.233010  loss = 3.59100 avg_loss = 3.67814\n",
      "epoch no.2 train no.233020  loss = 5.53893 avg_loss = 3.67695\n",
      "epoch no.2 train no.233030  loss = 2.13186 avg_loss = 3.63404\n",
      "epoch no.2 train no.233040  loss = 3.43045 avg_loss = 3.62067\n",
      "epoch no.2 train no.233050  loss = 5.35089 avg_loss = 3.64234\n",
      "epoch no.2 train no.233060  loss = 2.60986 avg_loss = 3.72270\n",
      "epoch no.2 train no.233070  loss = 5.07364 avg_loss = 3.70012\n",
      "epoch no.2 train no.233080  loss = 2.40978 avg_loss = 3.66847\n",
      "epoch no.2 train no.233090  loss = 5.26131 avg_loss = 3.70226\n",
      "epoch no.2 train no.233100  loss = 2.49743 avg_loss = 3.67667\n",
      "epoch no.2 train no.233110  loss = 4.02353 avg_loss = 3.66897\n",
      "epoch no.2 train no.233120  loss = 5.18205 avg_loss = 3.73669\n",
      "epoch no.2 train no.233130  loss = 4.01648 avg_loss = 3.75260\n",
      "epoch no.2 train no.233140  loss = 5.00758 avg_loss = 3.76340\n",
      "epoch no.2 train no.233150  loss = 2.50698 avg_loss = 3.76050\n",
      "epoch no.2 train no.233160  loss = 3.53046 avg_loss = 3.73035\n",
      "epoch no.2 train no.233170  loss = 4.86715 avg_loss = 3.69368\n",
      "epoch no.2 train no.233180  loss = 2.38251 avg_loss = 3.67368\n",
      "epoch no.2 train no.233190  loss = 3.35714 avg_loss = 3.63535\n",
      "epoch no.2 train no.233200  loss = 4.22923 avg_loss = 3.62675\n",
      "epoch no.2 train no.233210  loss = 3.45306 avg_loss = 3.61345\n",
      "epoch no.2 train no.233220  loss = 2.01752 avg_loss = 3.59522\n",
      "epoch no.2 train no.233230  loss = 2.62007 avg_loss = 3.53560\n",
      "epoch no.2 train no.233240  loss = 1.47055 avg_loss = 3.54179\n",
      "epoch no.2 train no.233250  loss = 4.52544 avg_loss = 3.54838\n",
      "epoch no.2 train no.233260  loss = 4.15400 avg_loss = 3.51261\n",
      "epoch no.2 train no.233270  loss = 4.91139 avg_loss = 3.58888\n",
      "epoch no.2 train no.233280  loss = 4.81464 avg_loss = 3.56454\n",
      "epoch no.2 train no.233290  loss = 4.49432 avg_loss = 3.56723\n",
      "epoch no.2 train no.233300  loss = 3.09961 avg_loss = 3.55073\n",
      "epoch no.2 train no.233310  loss = 2.64062 avg_loss = 3.57128\n",
      "epoch no.2 train no.233320  loss = 4.22303 avg_loss = 3.53477\n",
      "epoch no.2 train no.233330  loss = 2.52043 avg_loss = 3.49298\n",
      "epoch no.2 train no.233340  loss = 3.10624 avg_loss = 3.48078\n",
      "epoch no.2 train no.233350  loss = 1.96288 avg_loss = 3.43533\n",
      "epoch no.2 train no.233360  loss = 4.85225 avg_loss = 3.46844\n",
      "epoch no.2 train no.233370  loss = 4.59242 avg_loss = 3.45620\n",
      "epoch no.2 train no.233380  loss = 4.05878 avg_loss = 3.53445\n",
      "epoch no.2 train no.233390  loss = 3.97337 avg_loss = 3.62183\n",
      "epoch no.2 train no.233400  loss = 3.02611 avg_loss = 3.59736\n",
      "epoch no.2 train no.233410  loss = 3.87076 avg_loss = 3.58050\n",
      "epoch no.2 train no.233420  loss = 5.97000 avg_loss = 3.61162\n",
      "epoch no.2 train no.233430  loss = 4.51353 avg_loss = 3.63566\n",
      "epoch no.2 train no.233440  loss = 2.21913 avg_loss = 3.58388\n",
      "epoch no.2 train no.233450  loss = 3.08145 avg_loss = 3.59593\n",
      "epoch no.2 train no.233460  loss = 3.77571 avg_loss = 3.61451\n",
      "epoch no.2 train no.233470  loss = 2.95186 avg_loss = 3.56766\n",
      "epoch no.2 train no.233480  loss = 1.53684 avg_loss = 3.55093\n",
      "epoch no.2 train no.233490  loss = 5.54083 avg_loss = 3.54617\n",
      "epoch no.2 train no.233500  loss = 2.79519 avg_loss = 3.55255\n",
      "epoch no.2 train no.233510  loss = 3.61140 avg_loss = 3.56259\n",
      "epoch no.2 train no.233520  loss = 4.82530 avg_loss = 3.53558\n",
      "epoch no.2 train no.233530  loss = 3.44139 avg_loss = 3.57366\n",
      "epoch no.2 train no.233540  loss = 4.53419 avg_loss = 3.56559\n",
      "epoch no.2 train no.233550  loss = 2.09743 avg_loss = 3.59751\n",
      "epoch no.2 train no.233560  loss = 3.23034 avg_loss = 3.62689\n",
      "epoch no.2 train no.233570  loss = 3.28882 avg_loss = 3.67368\n",
      "epoch no.2 train no.233580  loss = 3.31354 avg_loss = 3.62649\n",
      "epoch no.2 train no.233590  loss = 4.26435 avg_loss = 3.63832\n",
      "epoch no.2 train no.233600  loss = 4.16354 avg_loss = 3.62459\n",
      "epoch no.2 train no.233610  loss = 4.03839 avg_loss = 3.67864\n",
      "epoch no.2 train no.233620  loss = 2.81082 avg_loss = 3.62579\n",
      "epoch no.2 train no.233630  loss = 3.57360 avg_loss = 3.62799\n",
      "epoch no.2 train no.233640  loss = 1.87552 avg_loss = 3.56858\n",
      "epoch no.2 train no.233650  loss = 4.65640 avg_loss = 3.61330\n",
      "epoch no.2 train no.233660  loss = 3.05610 avg_loss = 3.64088\n",
      "epoch no.2 train no.233670  loss = 2.45373 avg_loss = 3.59558\n",
      "epoch no.2 train no.233680  loss = 2.24224 avg_loss = 3.57835\n",
      "epoch no.2 train no.233690  loss = 3.43247 avg_loss = 3.56028\n",
      "epoch no.2 train no.233700  loss = 2.80981 avg_loss = 3.52731\n",
      "epoch no.2 train no.233710  loss = 4.09098 avg_loss = 3.53151\n",
      "epoch no.2 train no.233720  loss = 3.19319 avg_loss = 3.56096\n",
      "epoch no.2 train no.233730  loss = 3.22251 avg_loss = 3.54206\n",
      "epoch no.2 train no.233740  loss = 6.12930 avg_loss = 3.56418\n",
      "epoch no.2 train no.233750  loss = 3.51552 avg_loss = 3.57299\n",
      "epoch no.2 train no.233760  loss = 4.68380 avg_loss = 3.63920\n",
      "epoch no.2 train no.233770  loss = 3.90928 avg_loss = 3.69900\n",
      "epoch no.2 train no.233780  loss = 3.98689 avg_loss = 3.71103\n",
      "epoch no.2 train no.233790  loss = 2.96810 avg_loss = 3.65788\n",
      "epoch no.2 train no.233800  loss = 2.87179 avg_loss = 3.63909\n",
      "epoch no.2 train no.233810  loss = 2.96124 avg_loss = 3.61503\n",
      "epoch no.2 train no.233820  loss = 2.12784 avg_loss = 3.60368\n",
      "epoch no.2 train no.233830  loss = 4.15152 avg_loss = 3.58213\n",
      "epoch no.2 train no.233840  loss = 4.38628 avg_loss = 3.60712\n",
      "epoch no.2 train no.233850  loss = 3.96450 avg_loss = 3.61330\n",
      "epoch no.2 train no.233860  loss = 4.63000 avg_loss = 3.62053\n",
      "epoch no.2 train no.233870  loss = 2.50509 avg_loss = 3.59397\n",
      "epoch no.2 train no.233880  loss = 3.70839 avg_loss = 3.55133\n",
      "epoch no.2 train no.233890  loss = 5.93546 avg_loss = 3.56292\n",
      "epoch no.2 train no.233900  loss = 3.42996 avg_loss = 3.61978\n",
      "epoch no.2 train no.233910  loss = 2.74497 avg_loss = 3.60341\n",
      "epoch no.2 train no.233920  loss = 2.66081 avg_loss = 3.59490\n",
      "epoch no.2 train no.233930  loss = 2.58212 avg_loss = 3.54812\n",
      "epoch no.2 train no.233940  loss = 4.16799 avg_loss = 3.55269\n",
      "epoch no.2 train no.233950  loss = 2.29835 avg_loss = 3.50604\n",
      "epoch no.2 train no.233960  loss = 2.31464 avg_loss = 3.54757\n",
      "epoch no.2 train no.233970  loss = 3.35509 avg_loss = 3.53244\n",
      "epoch no.2 train no.233980  loss = 2.74697 avg_loss = 3.56834\n",
      "epoch no.2 train no.233990  loss = 3.91219 avg_loss = 3.55441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.234000  loss = 4.29494 avg_loss = 3.56504\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁신나는', '</s>']\n",
      "기분전환이 필요할 때 듣는 음악</s>\n",
      "epoch no.2 train no.234010  loss = 5.03599 avg_loss = 3.57503\n",
      "epoch no.2 train no.234020  loss = 3.63088 avg_loss = 3.56562\n",
      "epoch no.2 train no.234030  loss = 4.35856 avg_loss = 3.62362\n",
      "epoch no.2 train no.234040  loss = 3.54976 avg_loss = 3.63450\n",
      "epoch no.2 train no.234050  loss = 3.97023 avg_loss = 3.61161\n",
      "epoch no.2 train no.234060  loss = 3.30218 avg_loss = 3.63527\n",
      "epoch no.2 train no.234070  loss = 2.64842 avg_loss = 3.62902\n",
      "epoch no.2 train no.234080  loss = 3.61301 avg_loss = 3.58843\n",
      "epoch no.2 train no.234090  loss = 3.80164 avg_loss = 3.54374\n",
      "epoch no.2 train no.234100  loss = 4.04260 avg_loss = 3.53953\n",
      "epoch no.2 train no.234110  loss = 2.95729 avg_loss = 3.51558\n",
      "epoch no.2 train no.234120  loss = 4.18888 avg_loss = 3.54833\n",
      "epoch no.2 train no.234130  loss = 3.78019 avg_loss = 3.62467\n",
      "epoch no.2 train no.234140  loss = 4.97300 avg_loss = 3.62771\n",
      "epoch no.2 train no.234150  loss = 2.68147 avg_loss = 3.64045\n",
      "epoch no.2 train no.234160  loss = 3.51035 avg_loss = 3.62319\n",
      "epoch no.2 train no.234170  loss = 3.01794 avg_loss = 3.64747\n",
      "epoch no.2 train no.234180  loss = 2.47237 avg_loss = 3.60604\n",
      "epoch no.2 train no.234190  loss = 2.91169 avg_loss = 3.60639\n",
      "epoch no.2 train no.234200  loss = 3.89719 avg_loss = 3.55573\n",
      "epoch no.2 train no.234210  loss = 2.29924 avg_loss = 3.57613\n",
      "epoch no.2 train no.234220  loss = 4.56158 avg_loss = 3.60200\n",
      "epoch no.2 train no.234230  loss = 3.28240 avg_loss = 3.61732\n",
      "epoch no.2 train no.234240  loss = 3.37994 avg_loss = 3.62933\n",
      "epoch no.2 train no.234250  loss = 3.19730 avg_loss = 3.61094\n",
      "epoch no.2 train no.234260  loss = 3.13959 avg_loss = 3.55682\n",
      "epoch no.2 train no.234270  loss = 3.05815 avg_loss = 3.56175\n",
      "epoch no.2 train no.234280  loss = 3.83934 avg_loss = 3.59819\n",
      "epoch no.2 train no.234290  loss = 2.56070 avg_loss = 3.66381\n",
      "epoch no.2 train no.234300  loss = 2.50539 avg_loss = 3.65686\n",
      "epoch no.2 train no.234310  loss = 4.02036 avg_loss = 3.61159\n",
      "epoch no.2 train no.234320  loss = 2.71538 avg_loss = 3.63839\n",
      "epoch no.2 train no.234330  loss = 4.57974 avg_loss = 3.67050\n",
      "epoch no.2 train no.234340  loss = 4.26430 avg_loss = 3.72195\n",
      "epoch no.2 train no.234350  loss = 2.76552 avg_loss = 3.75967\n",
      "epoch no.2 train no.234360  loss = 3.62396 avg_loss = 3.77111\n",
      "epoch no.2 train no.234370  loss = 5.37091 avg_loss = 3.74790\n",
      "epoch no.2 train no.234380  loss = 5.87142 avg_loss = 3.68578\n",
      "epoch no.2 train no.234390  loss = 4.54924 avg_loss = 3.69850\n",
      "epoch no.2 train no.234400  loss = 4.28324 avg_loss = 3.67801\n",
      "epoch no.2 train no.234410  loss = 3.11828 avg_loss = 3.70172\n",
      "epoch no.2 train no.234420  loss = 2.84287 avg_loss = 3.66374\n",
      "epoch no.2 train no.234430  loss = 5.17850 avg_loss = 3.70299\n",
      "epoch no.2 train no.234440  loss = 2.26890 avg_loss = 3.67794\n",
      "epoch no.2 train no.234450  loss = 4.20772 avg_loss = 3.69527\n",
      "epoch no.2 train no.234460  loss = 2.66880 avg_loss = 3.67988\n",
      "epoch no.2 train no.234470  loss = 3.39984 avg_loss = 3.70805\n",
      "epoch no.2 train no.234480  loss = 3.32231 avg_loss = 3.72883\n",
      "epoch no.2 train no.234490  loss = 4.23926 avg_loss = 3.77181\n",
      "epoch no.2 train no.234500  loss = 5.07928 avg_loss = 3.77120\n",
      "epoch no.2 train no.234510  loss = 3.70560 avg_loss = 3.71353\n",
      "epoch no.2 train no.234520  loss = 2.80492 avg_loss = 3.64680\n",
      "epoch no.2 train no.234530  loss = 2.89608 avg_loss = 3.66617\n",
      "epoch no.2 train no.234540  loss = 4.11695 avg_loss = 3.66052\n",
      "epoch no.2 train no.234550  loss = 3.68342 avg_loss = 3.60935\n",
      "epoch no.2 train no.234560  loss = 2.89590 avg_loss = 3.64706\n",
      "epoch no.2 train no.234570  loss = 3.10866 avg_loss = 3.59708\n",
      "epoch no.2 train no.234580  loss = 2.24259 avg_loss = 3.55087\n",
      "epoch no.2 train no.234590  loss = 3.61328 avg_loss = 3.59691\n",
      "epoch no.2 train no.234600  loss = 4.38242 avg_loss = 3.56697\n",
      "epoch no.2 train no.234610  loss = 4.74438 avg_loss = 3.55445\n",
      "epoch no.2 train no.234620  loss = 5.12253 avg_loss = 3.62783\n",
      "epoch no.2 train no.234630  loss = 2.80034 avg_loss = 3.62999\n",
      "epoch no.2 train no.234640  loss = 4.34133 avg_loss = 3.59998\n",
      "epoch no.2 train no.234650  loss = 3.24287 avg_loss = 3.61385\n",
      "epoch no.2 train no.234660  loss = 3.25011 avg_loss = 3.61848\n",
      "epoch no.2 train no.234670  loss = 4.29179 avg_loss = 3.58642\n",
      "epoch no.2 train no.234680  loss = 4.93076 avg_loss = 3.59731\n",
      "epoch no.2 train no.234690  loss = 3.80456 avg_loss = 3.57509\n",
      "epoch no.2 train no.234700  loss = 6.19259 avg_loss = 3.56104\n",
      "epoch no.2 train no.234710  loss = 3.41117 avg_loss = 3.58370\n",
      "epoch no.2 train no.234720  loss = 3.48582 avg_loss = 3.58267\n",
      "epoch no.2 train no.234730  loss = 2.47935 avg_loss = 3.55256\n",
      "epoch no.2 train no.234740  loss = 2.26464 avg_loss = 3.49302\n",
      "epoch no.2 train no.234750  loss = 2.24045 avg_loss = 3.46093\n",
      "epoch no.2 train no.234760  loss = 4.05509 avg_loss = 3.45220\n",
      "epoch no.2 train no.234770  loss = 3.33517 avg_loss = 3.45579\n",
      "epoch no.2 train no.234780  loss = 4.00877 avg_loss = 3.44208\n",
      "epoch no.2 train no.234790  loss = 2.59389 avg_loss = 3.47808\n",
      "epoch no.2 train no.234800  loss = 1.99267 avg_loss = 3.40513\n",
      "epoch no.2 train no.234810  loss = 3.76273 avg_loss = 3.39848\n",
      "epoch no.2 train no.234820  loss = 5.46992 avg_loss = 3.44103\n",
      "epoch no.2 train no.234830  loss = 8.14385 avg_loss = 3.49088\n",
      "epoch no.2 train no.234840  loss = 2.03327 avg_loss = 3.46978\n",
      "epoch no.2 train no.234850  loss = 4.55601 avg_loss = 3.51318\n",
      "epoch no.2 train no.234860  loss = 4.37665 avg_loss = 3.51753\n",
      "epoch no.2 train no.234870  loss = 2.20561 avg_loss = 3.56132\n",
      "epoch no.2 train no.234880  loss = 2.08877 avg_loss = 3.56368\n",
      "epoch no.2 train no.234890  loss = 3.89050 avg_loss = 3.57809\n",
      "epoch no.2 train no.234900  loss = 3.81521 avg_loss = 3.61421\n",
      "epoch no.2 train no.234910  loss = 3.02814 avg_loss = 3.60393\n",
      "epoch no.2 train no.234920  loss = 3.79702 avg_loss = 3.64430\n",
      "epoch no.2 train no.234930  loss = 3.30640 avg_loss = 3.64777\n",
      "epoch no.2 train no.234940  loss = 4.07735 avg_loss = 3.65923\n",
      "epoch no.2 train no.234950  loss = 1.85971 avg_loss = 3.72005\n",
      "epoch no.2 train no.234960  loss = 3.44296 avg_loss = 3.70216\n",
      "epoch no.2 train no.234970  loss = 2.67162 avg_loss = 3.70174\n",
      "epoch no.2 train no.234980  loss = 2.20521 avg_loss = 3.66118\n",
      "epoch no.2 train no.234990  loss = 3.54909 avg_loss = 3.67745\n",
      "epoch no.2 train no.235000  loss = 4.72560 avg_loss = 3.67966\n",
      "8\n",
      "to_tokens: ['▁비', '좋은', '이', '▁딱', '▁좋은', '▁신나는', '달', '한', '▁노래', '</s>', '</s>']\n",
      "기분전환에 딱인 달달한 노래들</s>\n",
      "epoch no.2 train no.235010  loss = 2.62401 avg_loss = 3.68918\n",
      "epoch no.2 train no.235020  loss = 3.73872 avg_loss = 3.70359\n",
      "epoch no.2 train no.235030  loss = 2.17932 avg_loss = 3.68554\n",
      "epoch no.2 train no.235040  loss = 4.09516 avg_loss = 3.71440\n",
      "epoch no.2 train no.235050  loss = 3.52449 avg_loss = 3.72420\n",
      "epoch no.2 train no.235060  loss = 1.44392 avg_loss = 3.66738\n",
      "epoch no.2 train no.235070  loss = 5.83493 avg_loss = 3.66269\n",
      "epoch no.2 train no.235080  loss = 3.01061 avg_loss = 3.60042\n",
      "epoch no.2 train no.235090  loss = 4.58942 avg_loss = 3.62994\n",
      "epoch no.2 train no.235100  loss = 3.36291 avg_loss = 3.64573\n",
      "epoch no.2 train no.235110  loss = 2.82576 avg_loss = 3.63107\n",
      "epoch no.2 train no.235120  loss = 4.07890 avg_loss = 3.62069\n",
      "epoch no.2 train no.235130  loss = 5.24363 avg_loss = 3.61089\n",
      "epoch no.2 train no.235140  loss = 3.51393 avg_loss = 3.56350\n",
      "epoch no.2 train no.235150  loss = 3.95517 avg_loss = 3.56411\n",
      "epoch no.2 train no.235160  loss = 4.21075 avg_loss = 3.62552\n",
      "epoch no.2 train no.235170  loss = 4.18125 avg_loss = 3.60799\n",
      "epoch no.2 train no.235180  loss = 1.90915 avg_loss = 3.57817\n",
      "epoch no.2 train no.235190  loss = 5.06832 avg_loss = 3.64006\n",
      "epoch no.2 train no.235200  loss = 5.71998 avg_loss = 3.67345\n",
      "epoch no.2 train no.235210  loss = 2.32839 avg_loss = 3.65930\n",
      "epoch no.2 train no.235220  loss = 3.32342 avg_loss = 3.60582\n",
      "epoch no.2 train no.235230  loss = 4.52507 avg_loss = 3.63255\n",
      "epoch no.2 train no.235240  loss = 3.80024 avg_loss = 3.61850\n",
      "epoch no.2 train no.235250  loss = 3.86040 avg_loss = 3.62774\n",
      "epoch no.2 train no.235260  loss = 2.59730 avg_loss = 3.56968\n",
      "epoch no.2 train no.235270  loss = 5.40941 avg_loss = 3.58393\n",
      "epoch no.2 train no.235280  loss = 3.71871 avg_loss = 3.59768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.235290  loss = 3.62205 avg_loss = 3.66101\n",
      "epoch no.2 train no.235300  loss = 2.33042 avg_loss = 3.66026\n",
      "epoch no.2 train no.235310  loss = 4.15705 avg_loss = 3.62027\n",
      "epoch no.2 train no.235320  loss = 3.69162 avg_loss = 3.62909\n",
      "epoch no.2 train no.235330  loss = 3.89597 avg_loss = 3.56770\n",
      "epoch no.2 train no.235340  loss = 4.61411 avg_loss = 3.55313\n",
      "epoch no.2 train no.235350  loss = 5.14158 avg_loss = 3.57353\n",
      "epoch no.2 train no.235360  loss = 2.74134 avg_loss = 3.51795\n",
      "epoch no.2 train no.235370  loss = 5.29249 avg_loss = 3.56139\n",
      "epoch no.2 train no.235380  loss = 2.78709 avg_loss = 3.51802\n",
      "epoch no.2 train no.235390  loss = 4.35134 avg_loss = 3.53350\n",
      "epoch no.2 train no.235400  loss = 4.50124 avg_loss = 3.52700\n",
      "epoch no.2 train no.235410  loss = 2.59926 avg_loss = 3.51397\n",
      "epoch no.2 train no.235420  loss = 2.56179 avg_loss = 3.50454\n",
      "epoch no.2 train no.235430  loss = 4.58709 avg_loss = 3.52507\n",
      "epoch no.2 train no.235440  loss = 2.77692 avg_loss = 3.55005\n",
      "epoch no.2 train no.235450  loss = 3.07638 avg_loss = 3.57921\n",
      "epoch no.2 train no.235460  loss = 3.75970 avg_loss = 3.56415\n",
      "epoch no.2 train no.235470  loss = 3.71775 avg_loss = 3.55063\n",
      "epoch no.2 train no.235480  loss = 2.83692 avg_loss = 3.54212\n",
      "epoch no.2 train no.235490  loss = 4.93393 avg_loss = 3.54688\n",
      "epoch no.2 train no.235500  loss = 3.04339 avg_loss = 3.56612\n",
      "epoch no.2 train no.235510  loss = 4.30506 avg_loss = 3.56926\n",
      "epoch no.2 train no.235520  loss = 3.38937 avg_loss = 3.53750\n",
      "epoch no.2 train no.235530  loss = 2.45991 avg_loss = 3.56929\n",
      "epoch no.2 train no.235540  loss = 3.82283 avg_loss = 3.59498\n",
      "epoch no.2 train no.235550  loss = 2.32922 avg_loss = 3.57535\n",
      "epoch no.2 train no.235560  loss = 2.70941 avg_loss = 3.56686\n",
      "epoch no.2 train no.235570  loss = 3.10143 avg_loss = 3.57885\n",
      "epoch no.2 train no.235580  loss = 3.93558 avg_loss = 3.54761\n",
      "epoch no.2 train no.235590  loss = 2.43753 avg_loss = 3.53944\n",
      "epoch no.2 train no.235600  loss = 2.60727 avg_loss = 3.49942\n",
      "epoch no.2 train no.235610  loss = 3.28358 avg_loss = 3.55597\n",
      "epoch no.2 train no.235620  loss = 2.21059 avg_loss = 3.56448\n",
      "epoch no.2 train no.235630  loss = 6.02792 avg_loss = 3.60007\n",
      "epoch no.2 train no.235640  loss = 3.46757 avg_loss = 3.57907\n",
      "epoch no.2 train no.235650  loss = 4.59044 avg_loss = 3.61356\n",
      "epoch no.2 train no.235660  loss = 3.34926 avg_loss = 3.61746\n",
      "epoch no.2 train no.235670  loss = 3.51805 avg_loss = 3.55319\n",
      "epoch no.2 train no.235680  loss = 2.60080 avg_loss = 3.51527\n",
      "epoch no.2 train no.235690  loss = 4.17045 avg_loss = 3.53860\n",
      "epoch no.2 train no.235700  loss = 2.96631 avg_loss = 3.54763\n",
      "epoch no.2 train no.235710  loss = 4.37437 avg_loss = 3.60984\n",
      "epoch no.2 train no.235720  loss = 4.44036 avg_loss = 3.56934\n",
      "epoch no.2 train no.235730  loss = 2.41859 avg_loss = 3.54928\n",
      "epoch no.2 train no.235740  loss = 3.01018 avg_loss = 3.56420\n",
      "epoch no.2 train no.235750  loss = 3.77451 avg_loss = 3.53694\n",
      "epoch no.2 train no.235760  loss = 2.28822 avg_loss = 3.60999\n",
      "epoch no.2 train no.235770  loss = 3.02834 avg_loss = 3.52894\n",
      "epoch no.2 train no.235780  loss = 2.80568 avg_loss = 3.57458\n",
      "epoch no.2 train no.235790  loss = 3.36567 avg_loss = 3.59196\n",
      "epoch no.2 train no.235800  loss = 2.79890 avg_loss = 3.62522\n",
      "epoch no.2 train no.235810  loss = 3.99198 avg_loss = 3.60601\n",
      "epoch no.2 train no.235820  loss = 2.46034 avg_loss = 3.54921\n",
      "epoch no.2 train no.235830  loss = 3.62564 avg_loss = 3.56163\n",
      "epoch no.2 train no.235840  loss = 3.31110 avg_loss = 3.56438\n",
      "epoch no.2 train no.235850  loss = 2.91836 avg_loss = 3.60128\n",
      "epoch no.2 train no.235860  loss = 4.13113 avg_loss = 3.64668\n",
      "epoch no.2 train no.235870  loss = 2.51156 avg_loss = 3.63173\n",
      "epoch no.2 train no.235880  loss = 2.50032 avg_loss = 3.56573\n",
      "epoch no.2 train no.235890  loss = 4.39972 avg_loss = 3.57849\n",
      "epoch no.2 train no.235900  loss = 2.09790 avg_loss = 3.54064\n",
      "epoch no.2 train no.235910  loss = 2.85857 avg_loss = 3.51574\n",
      "epoch no.2 train no.235920  loss = 4.09119 avg_loss = 3.50683\n",
      "epoch no.2 train no.235930  loss = 5.70311 avg_loss = 3.59066\n",
      "epoch no.2 train no.235940  loss = 3.56667 avg_loss = 3.58541\n",
      "epoch no.2 train no.235950  loss = 2.81444 avg_loss = 3.62337\n",
      "epoch no.2 train no.235960  loss = 4.01675 avg_loss = 3.60652\n",
      "epoch no.2 train no.235970  loss = 3.88009 avg_loss = 3.61370\n",
      "epoch no.2 train no.235980  loss = 4.62323 avg_loss = 3.59735\n",
      "epoch no.2 train no.235990  loss = 3.31086 avg_loss = 3.58367\n",
      "epoch no.2 train no.236000  loss = 6.98519 avg_loss = 3.59491\n",
      "7\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '▁듣는', '▁신나는', '한', '▁음악', '</s>']\n",
      "기분전환이 필요할 때 듣는 잔잔한 팝</s>\n",
      "epoch no.2 train no.236010  loss = 3.44229 avg_loss = 3.54196\n",
      "epoch no.2 train no.236020  loss = 4.09886 avg_loss = 3.54844\n",
      "epoch no.2 train no.236030  loss = 2.91726 avg_loss = 3.54726\n",
      "epoch no.2 train no.236040  loss = 4.59586 avg_loss = 3.58774\n",
      "epoch no.2 train no.236050  loss = 4.19185 avg_loss = 3.58752\n",
      "epoch no.2 train no.236060  loss = 3.29119 avg_loss = 3.56683\n",
      "epoch no.2 train no.236070  loss = 2.86858 avg_loss = 3.56763\n",
      "epoch no.2 train no.236080  loss = 4.55603 avg_loss = 3.57337\n",
      "epoch no.2 train no.236090  loss = 3.72909 avg_loss = 3.57180\n",
      "epoch no.2 train no.236100  loss = 4.17991 avg_loss = 3.55031\n",
      "epoch no.2 train no.236110  loss = 3.89082 avg_loss = 3.58584\n",
      "epoch no.2 train no.236120  loss = 3.63380 avg_loss = 3.62832\n",
      "epoch no.2 train no.236130  loss = 4.59719 avg_loss = 3.66126\n",
      "epoch no.2 train no.236140  loss = 3.77381 avg_loss = 3.66788\n",
      "epoch no.2 train no.236150  loss = 4.36356 avg_loss = 3.67343\n",
      "epoch no.2 train no.236160  loss = 3.52583 avg_loss = 3.65176\n",
      "epoch no.2 train no.236170  loss = 2.65969 avg_loss = 3.62718\n",
      "epoch no.2 train no.236180  loss = 2.70168 avg_loss = 3.63742\n",
      "epoch no.2 train no.236190  loss = 2.96722 avg_loss = 3.65806\n",
      "epoch no.2 train no.236200  loss = 6.76774 avg_loss = 3.68078\n",
      "epoch no.2 train no.236210  loss = 2.09471 avg_loss = 3.69072\n",
      "epoch no.2 train no.236220  loss = 2.10808 avg_loss = 3.65734\n",
      "epoch no.2 train no.236230  loss = 3.01484 avg_loss = 3.65094\n",
      "epoch no.2 train no.236240  loss = 4.56461 avg_loss = 3.68079\n",
      "epoch no.2 train no.236250  loss = 3.98362 avg_loss = 3.69927\n",
      "epoch no.2 train no.236260  loss = 5.48335 avg_loss = 3.71240\n",
      "epoch no.2 train no.236270  loss = 1.73376 avg_loss = 3.68764\n",
      "epoch no.2 train no.236280  loss = 2.84978 avg_loss = 3.61575\n",
      "epoch no.2 train no.236290  loss = 3.23444 avg_loss = 3.60238\n",
      "epoch no.2 train no.236300  loss = 4.03360 avg_loss = 3.63097\n",
      "epoch no.2 train no.236310  loss = 3.67191 avg_loss = 3.67053\n",
      "epoch no.2 train no.236320  loss = 4.00325 avg_loss = 3.68183\n",
      "epoch no.2 train no.236330  loss = 2.42808 avg_loss = 3.64236\n",
      "epoch no.2 train no.236340  loss = 5.61852 avg_loss = 3.65468\n",
      "epoch no.2 train no.236350  loss = 3.22021 avg_loss = 3.62349\n",
      "epoch no.2 train no.236360  loss = 6.01136 avg_loss = 3.64086\n",
      "epoch no.2 train no.236370  loss = 2.55416 avg_loss = 3.60934\n",
      "epoch no.2 train no.236380  loss = 2.48080 avg_loss = 3.61945\n",
      "epoch no.2 train no.236390  loss = 3.01842 avg_loss = 3.66175\n",
      "epoch no.2 train no.236400  loss = 2.76868 avg_loss = 3.71289\n",
      "epoch no.2 train no.236410  loss = 4.07145 avg_loss = 3.70873\n",
      "epoch no.2 train no.236420  loss = 4.46895 avg_loss = 3.79097\n",
      "epoch no.2 train no.236430  loss = 2.79843 avg_loss = 3.77388\n",
      "epoch no.2 train no.236440  loss = 3.45870 avg_loss = 3.77802\n",
      "epoch no.2 train no.236450  loss = 3.64303 avg_loss = 3.75777\n",
      "epoch no.2 train no.236460  loss = 4.17220 avg_loss = 3.73558\n",
      "epoch no.2 train no.236470  loss = 4.22608 avg_loss = 3.77129\n",
      "epoch no.2 train no.236480  loss = 6.21279 avg_loss = 3.80725\n",
      "epoch no.2 train no.236490  loss = 3.51753 avg_loss = 3.75709\n",
      "epoch no.2 train no.236500  loss = 3.17427 avg_loss = 3.74482\n",
      "epoch no.2 train no.236510  loss = 3.42487 avg_loss = 3.75355\n",
      "epoch no.2 train no.236520  loss = 2.53904 avg_loss = 3.73187\n",
      "epoch no.2 train no.236530  loss = 2.43784 avg_loss = 3.72858\n",
      "epoch no.2 train no.236540  loss = 4.29599 avg_loss = 3.72244\n",
      "epoch no.2 train no.236550  loss = 2.23532 avg_loss = 3.64746\n",
      "epoch no.2 train no.236560  loss = 5.20395 avg_loss = 3.63325\n",
      "epoch no.2 train no.236570  loss = 3.48923 avg_loss = 3.65364\n",
      "epoch no.2 train no.236580  loss = 2.45293 avg_loss = 3.64651\n",
      "epoch no.2 train no.236590  loss = 3.81735 avg_loss = 3.69458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.236600  loss = 2.59713 avg_loss = 3.65523\n",
      "epoch no.2 train no.236610  loss = 6.27755 avg_loss = 3.73125\n",
      "epoch no.2 train no.236620  loss = 4.38145 avg_loss = 3.71190\n",
      "epoch no.2 train no.236630  loss = 3.46083 avg_loss = 3.71026\n",
      "epoch no.2 train no.236640  loss = 4.40495 avg_loss = 3.66764\n",
      "epoch no.2 train no.236650  loss = 4.39340 avg_loss = 3.68099\n",
      "epoch no.2 train no.236660  loss = 3.12817 avg_loss = 3.74038\n",
      "epoch no.2 train no.236670  loss = 4.23984 avg_loss = 3.73523\n",
      "epoch no.2 train no.236680  loss = 2.83715 avg_loss = 3.70683\n",
      "epoch no.2 train no.236690  loss = 3.05373 avg_loss = 3.72437\n",
      "epoch no.2 train no.236700  loss = 3.11840 avg_loss = 3.68581\n",
      "epoch no.2 train no.236710  loss = 2.07988 avg_loss = 3.64623\n",
      "epoch no.2 train no.236720  loss = 1.96395 avg_loss = 3.58930\n",
      "epoch no.2 train no.236730  loss = 3.27811 avg_loss = 3.55740\n",
      "epoch no.2 train no.236740  loss = 5.26368 avg_loss = 3.54699\n",
      "epoch no.2 train no.236750  loss = 4.85750 avg_loss = 3.60990\n",
      "epoch no.2 train no.236760  loss = 2.78058 avg_loss = 3.58618\n",
      "epoch no.2 train no.236770  loss = 4.06334 avg_loss = 3.58782\n",
      "epoch no.2 train no.236780  loss = 3.06105 avg_loss = 3.58412\n",
      "epoch no.2 train no.236790  loss = 4.07866 avg_loss = 3.53507\n",
      "epoch no.2 train no.236800  loss = 3.63490 avg_loss = 3.59452\n",
      "epoch no.2 train no.236810  loss = 4.02070 avg_loss = 3.61091\n",
      "epoch no.2 train no.236820  loss = 2.97069 avg_loss = 3.56254\n",
      "epoch no.2 train no.236830  loss = 3.93077 avg_loss = 3.56701\n",
      "epoch no.2 train no.236840  loss = 4.34119 avg_loss = 3.53860\n",
      "epoch no.2 train no.236850  loss = 3.20493 avg_loss = 3.50674\n",
      "epoch no.2 train no.236860  loss = 3.21762 avg_loss = 3.49316\n",
      "epoch no.2 train no.236870  loss = 4.60135 avg_loss = 3.48107\n",
      "epoch no.2 train no.236880  loss = 4.60928 avg_loss = 3.47601\n",
      "epoch no.2 train no.236890  loss = 2.41802 avg_loss = 3.49088\n",
      "epoch no.2 train no.236900  loss = 1.96110 avg_loss = 3.51364\n",
      "epoch no.2 train no.236910  loss = 4.32339 avg_loss = 3.50112\n",
      "epoch no.2 train no.236920  loss = 2.58638 avg_loss = 3.47436\n",
      "epoch no.2 train no.236930  loss = 3.42522 avg_loss = 3.49668\n",
      "epoch no.2 train no.236940  loss = 3.18869 avg_loss = 3.48384\n",
      "epoch no.2 train no.236950  loss = 3.10368 avg_loss = 3.51470\n",
      "epoch no.2 train no.236960  loss = 3.88180 avg_loss = 3.57736\n",
      "epoch no.2 train no.236970  loss = 4.39953 avg_loss = 3.63654\n",
      "epoch no.2 train no.236980  loss = 4.73600 avg_loss = 3.65551\n",
      "epoch no.2 train no.236990  loss = 6.42661 avg_loss = 3.65543\n",
      "epoch no.2 train no.237000  loss = 4.13559 avg_loss = 3.65397\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁신나는', '</s>']\n",
      "기분전환용 노래</s>\n",
      "epoch no.2 train no.237010  loss = 1.72597 avg_loss = 3.64875\n",
      "epoch no.2 train no.237020  loss = 4.81095 avg_loss = 3.69199\n",
      "epoch no.2 train no.237030  loss = 2.86382 avg_loss = 3.71084\n",
      "epoch no.2 train no.237040  loss = 3.04427 avg_loss = 3.74462\n",
      "epoch no.2 train no.237050  loss = 4.93300 avg_loss = 3.76303\n",
      "epoch no.2 train no.237060  loss = 5.11048 avg_loss = 3.75225\n",
      "epoch no.2 train no.237070  loss = 3.31056 avg_loss = 3.78739\n",
      "epoch no.2 train no.237080  loss = 3.29020 avg_loss = 3.72753\n",
      "epoch no.2 train no.237090  loss = 2.69876 avg_loss = 3.68932\n",
      "epoch no.2 train no.237100  loss = 5.37162 avg_loss = 3.66320\n",
      "epoch no.2 train no.237110  loss = 5.00834 avg_loss = 3.66435\n",
      "epoch no.2 train no.237120  loss = 4.51263 avg_loss = 3.67080\n",
      "epoch no.2 train no.237130  loss = 1.71801 avg_loss = 3.62714\n",
      "epoch no.2 train no.237140  loss = 5.31294 avg_loss = 3.70453\n",
      "epoch no.2 train no.237150  loss = 4.38203 avg_loss = 3.70177\n",
      "epoch no.2 train no.237160  loss = 2.41638 avg_loss = 3.77923\n",
      "epoch no.2 train no.237170  loss = 2.40636 avg_loss = 3.71909\n",
      "epoch no.2 train no.237180  loss = 4.36846 avg_loss = 3.71040\n",
      "epoch no.2 train no.237190  loss = 3.14392 avg_loss = 3.69757\n",
      "epoch no.2 train no.237200  loss = 3.92017 avg_loss = 3.70364\n",
      "epoch no.2 train no.237210  loss = 2.22703 avg_loss = 3.60928\n",
      "epoch no.2 train no.237220  loss = 4.23180 avg_loss = 3.61426\n",
      "epoch no.2 train no.237230  loss = 3.47270 avg_loss = 3.60607\n",
      "epoch no.2 train no.237240  loss = 2.26152 avg_loss = 3.61002\n",
      "epoch no.2 train no.237250  loss = 4.13249 avg_loss = 3.65641\n",
      "epoch no.2 train no.237260  loss = 3.10467 avg_loss = 3.65873\n",
      "epoch no.2 train no.237270  loss = 1.16412 avg_loss = 3.66662\n",
      "epoch no.2 train no.237280  loss = 3.14343 avg_loss = 3.66845\n",
      "epoch no.2 train no.237290  loss = 3.60670 avg_loss = 3.70069\n",
      "epoch no.2 train no.237300  loss = 4.24359 avg_loss = 3.70517\n",
      "epoch no.2 train no.237310  loss = 3.11633 avg_loss = 3.70065\n",
      "epoch no.2 train no.237320  loss = 5.72796 avg_loss = 3.70904\n",
      "epoch no.2 train no.237330  loss = 2.33146 avg_loss = 3.69426\n",
      "epoch no.2 train no.237340  loss = 3.64719 avg_loss = 3.66289\n",
      "epoch no.2 train no.237350  loss = 4.83916 avg_loss = 3.68278\n",
      "epoch no.2 train no.237360  loss = 1.96195 avg_loss = 3.67898\n",
      "epoch no.2 train no.237370  loss = 6.05698 avg_loss = 3.69644\n",
      "epoch no.2 train no.237380  loss = 3.11440 avg_loss = 3.64032\n",
      "epoch no.2 train no.237390  loss = 3.08922 avg_loss = 3.61789\n",
      "epoch no.2 train no.237400  loss = 3.47253 avg_loss = 3.61225\n",
      "epoch no.2 train no.237410  loss = 6.32362 avg_loss = 3.69046\n",
      "epoch no.2 train no.237420  loss = 3.35200 avg_loss = 3.70463\n",
      "epoch no.2 train no.237430  loss = 4.53884 avg_loss = 3.72767\n",
      "epoch no.2 train no.237440  loss = 4.15267 avg_loss = 3.72420\n",
      "epoch no.2 train no.237450  loss = 3.33361 avg_loss = 3.80218\n",
      "epoch no.2 train no.237460  loss = 3.30764 avg_loss = 3.76478\n",
      "epoch no.2 train no.237470  loss = 4.26180 avg_loss = 3.77256\n",
      "epoch no.2 train no.237480  loss = 3.79677 avg_loss = 3.72960\n",
      "epoch no.2 train no.237490  loss = 3.96986 avg_loss = 3.71770\n",
      "epoch no.2 train no.237500  loss = 4.98104 avg_loss = 3.78306\n",
      "epoch no.2 train no.237510  loss = 4.59915 avg_loss = 3.75156\n",
      "epoch no.2 train no.237520  loss = 2.49777 avg_loss = 3.74363\n",
      "epoch no.2 train no.237530  loss = 3.12795 avg_loss = 3.66870\n",
      "epoch no.2 train no.237540  loss = 4.12234 avg_loss = 3.63672\n",
      "epoch no.2 train no.237550  loss = 5.01151 avg_loss = 3.65043\n",
      "epoch no.2 train no.237560  loss = 4.60710 avg_loss = 3.63965\n",
      "epoch no.2 train no.237570  loss = 3.42478 avg_loss = 3.62204\n",
      "epoch no.2 train no.237580  loss = 3.99871 avg_loss = 3.65561\n",
      "epoch no.2 train no.237590  loss = 2.43823 avg_loss = 3.59889\n",
      "epoch no.2 train no.237600  loss = 3.93272 avg_loss = 3.60952\n",
      "epoch no.2 train no.237610  loss = 5.54317 avg_loss = 3.61647\n",
      "epoch no.2 train no.237620  loss = 5.98896 avg_loss = 3.68009\n",
      "epoch no.2 train no.237630  loss = 2.17335 avg_loss = 3.65652\n",
      "epoch no.2 train no.237640  loss = 4.07045 avg_loss = 3.65130\n",
      "epoch no.2 train no.237650  loss = 3.62927 avg_loss = 3.65847\n",
      "epoch no.2 train no.237660  loss = 2.98219 avg_loss = 3.65052\n",
      "epoch no.2 train no.237670  loss = 2.85768 avg_loss = 3.62015\n",
      "epoch no.2 train no.237680  loss = 3.10869 avg_loss = 3.60426\n",
      "epoch no.2 train no.237690  loss = 3.28653 avg_loss = 3.65570\n",
      "epoch no.2 train no.237700  loss = 3.48238 avg_loss = 3.62193\n",
      "epoch no.2 train no.237710  loss = 2.59932 avg_loss = 3.62135\n",
      "epoch no.2 train no.237720  loss = 3.80411 avg_loss = 3.60753\n",
      "epoch no.2 train no.237730  loss = 6.76030 avg_loss = 3.58876\n",
      "epoch no.2 train no.237740  loss = 3.69079 avg_loss = 3.63338\n",
      "epoch no.2 train no.237750  loss = 3.62629 avg_loss = 3.63107\n",
      "epoch no.2 train no.237760  loss = 3.84345 avg_loss = 3.65370\n",
      "epoch no.2 train no.237770  loss = 2.64272 avg_loss = 3.68276\n",
      "epoch no.2 train no.237780  loss = 3.22360 avg_loss = 3.66578\n",
      "epoch no.2 train no.237790  loss = 4.24999 avg_loss = 3.66970\n",
      "epoch no.2 train no.237800  loss = 3.57432 avg_loss = 3.66189\n",
      "epoch no.2 train no.237810  loss = 3.00256 avg_loss = 3.64216\n",
      "epoch no.2 train no.237820  loss = 3.91126 avg_loss = 3.64348\n",
      "epoch no.2 train no.237830  loss = 4.37138 avg_loss = 3.66619\n",
      "epoch no.2 train no.237840  loss = 3.67134 avg_loss = 3.71764\n",
      "epoch no.2 train no.237850  loss = 2.29187 avg_loss = 3.74046\n",
      "epoch no.2 train no.237860  loss = 3.32693 avg_loss = 3.73194\n",
      "epoch no.2 train no.237870  loss = 5.01483 avg_loss = 3.73796\n",
      "epoch no.2 train no.237880  loss = 3.21120 avg_loss = 3.71855\n",
      "epoch no.2 train no.237890  loss = 3.39537 avg_loss = 3.68162\n",
      "epoch no.2 train no.237900  loss = 4.43955 avg_loss = 3.71927\n",
      "epoch no.2 train no.237910  loss = 4.60704 avg_loss = 3.73003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.237920  loss = 3.58891 avg_loss = 3.67442\n",
      "epoch no.2 train no.237930  loss = 2.14754 avg_loss = 3.65476\n",
      "epoch no.2 train no.237940  loss = 3.07452 avg_loss = 3.71688\n",
      "epoch no.2 train no.237950  loss = 3.27992 avg_loss = 3.69041\n",
      "epoch no.2 train no.237960  loss = 3.84771 avg_loss = 3.68057\n",
      "epoch no.2 train no.237970  loss = 2.55055 avg_loss = 3.68701\n",
      "epoch no.2 train no.237980  loss = 2.82453 avg_loss = 3.61868\n",
      "epoch no.2 train no.237990  loss = 3.01365 avg_loss = 3.58779\n",
      "epoch no.2 train no.238000  loss = 5.00846 avg_loss = 3.61673\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.238010  loss = 3.28055 avg_loss = 3.64748\n",
      "epoch no.2 train no.238020  loss = 2.72670 avg_loss = 3.58698\n",
      "epoch no.2 train no.238030  loss = 3.15730 avg_loss = 3.53587\n",
      "epoch no.2 train no.238040  loss = 2.42896 avg_loss = 3.51206\n",
      "epoch no.2 train no.238050  loss = 4.03668 avg_loss = 3.51971\n",
      "epoch no.2 train no.238060  loss = 2.74490 avg_loss = 3.49172\n",
      "epoch no.2 train no.238070  loss = 3.28787 avg_loss = 3.51753\n",
      "epoch no.2 train no.238080  loss = 3.80040 avg_loss = 3.49990\n",
      "epoch no.2 train no.238090  loss = 4.63909 avg_loss = 3.49520\n",
      "epoch no.2 train no.238100  loss = 5.54663 avg_loss = 3.53795\n",
      "epoch no.2 train no.238110  loss = 2.62940 avg_loss = 3.57427\n",
      "epoch no.2 train no.238120  loss = 2.34339 avg_loss = 3.56396\n",
      "epoch no.2 train no.238130  loss = 3.99503 avg_loss = 3.63691\n",
      "epoch no.2 train no.238140  loss = 3.44802 avg_loss = 3.65418\n",
      "epoch no.2 train no.238150  loss = 4.75380 avg_loss = 3.71351\n",
      "epoch no.2 train no.238160  loss = 7.52053 avg_loss = 3.71866\n",
      "epoch no.2 train no.238170  loss = 3.40558 avg_loss = 3.71223\n",
      "epoch no.2 train no.238180  loss = 4.53747 avg_loss = 3.76435\n",
      "epoch no.2 train no.238190  loss = 4.03785 avg_loss = 3.76539\n",
      "epoch no.2 train no.238200  loss = 3.17414 avg_loss = 3.71255\n",
      "epoch no.2 train no.238210  loss = 4.35673 avg_loss = 3.74804\n",
      "epoch no.2 train no.238220  loss = 2.53228 avg_loss = 3.71598\n",
      "epoch no.2 train no.238230  loss = 3.04072 avg_loss = 3.71430\n",
      "epoch no.2 train no.238240  loss = 2.34135 avg_loss = 3.72562\n",
      "epoch no.2 train no.238250  loss = 3.90983 avg_loss = 3.67957\n",
      "epoch no.2 train no.238260  loss = 4.70997 avg_loss = 3.65961\n",
      "epoch no.2 train no.238270  loss = 2.93942 avg_loss = 3.68618\n",
      "epoch no.2 train no.238280  loss = 4.97570 avg_loss = 3.70535\n",
      "epoch no.2 train no.238290  loss = 2.75940 avg_loss = 3.69826\n",
      "epoch no.2 train no.238300  loss = 3.84201 avg_loss = 3.75342\n",
      "epoch no.2 train no.238310  loss = 3.42634 avg_loss = 3.74484\n",
      "epoch no.2 train no.238320  loss = 6.43707 avg_loss = 3.81597\n",
      "epoch no.2 train no.238330  loss = 4.44130 avg_loss = 3.83739\n",
      "epoch no.2 train no.238340  loss = 3.28573 avg_loss = 3.79128\n",
      "epoch no.2 train no.238350  loss = 1.90293 avg_loss = 3.79640\n",
      "epoch no.2 train no.238360  loss = 2.19978 avg_loss = 3.77754\n",
      "epoch no.2 train no.238370  loss = 2.38039 avg_loss = 3.77555\n",
      "epoch no.2 train no.238380  loss = 2.72138 avg_loss = 3.77843\n",
      "epoch no.2 train no.238390  loss = 3.14399 avg_loss = 3.82929\n",
      "epoch no.2 train no.238400  loss = 3.64980 avg_loss = 3.81420\n",
      "epoch no.2 train no.238410  loss = 4.17293 avg_loss = 3.84590\n",
      "epoch no.2 train no.238420  loss = 3.02852 avg_loss = 3.79861\n",
      "epoch no.2 train no.238430  loss = 4.04142 avg_loss = 3.80269\n",
      "epoch no.2 train no.238440  loss = 2.74416 avg_loss = 3.77379\n",
      "epoch no.2 train no.238450  loss = 3.88408 avg_loss = 3.81411\n",
      "epoch no.2 train no.238460  loss = 3.70880 avg_loss = 3.78160\n",
      "epoch no.2 train no.238470  loss = 3.79096 avg_loss = 3.74523\n",
      "epoch no.2 train no.238480  loss = 3.78174 avg_loss = 3.73301\n",
      "epoch no.2 train no.238490  loss = 2.90486 avg_loss = 3.76414\n",
      "epoch no.2 train no.238500  loss = 3.11122 avg_loss = 3.78540\n",
      "epoch no.2 train no.238510  loss = 3.48944 avg_loss = 3.77262\n",
      "epoch no.2 train no.238520  loss = 3.89616 avg_loss = 3.76495\n",
      "epoch no.2 train no.238530  loss = 2.84083 avg_loss = 3.71729\n",
      "epoch no.2 train no.238540  loss = 3.97065 avg_loss = 3.68816\n",
      "epoch no.2 train no.238550  loss = 2.05720 avg_loss = 3.70724\n",
      "epoch no.2 train no.238560  loss = 3.72469 avg_loss = 3.75740\n",
      "epoch no.2 train no.238570  loss = 4.66243 avg_loss = 3.72688\n",
      "epoch no.2 train no.238580  loss = 2.39942 avg_loss = 3.70297\n",
      "epoch no.2 train no.238590  loss = 3.66564 avg_loss = 3.68621\n",
      "epoch no.2 train no.238600  loss = 3.63718 avg_loss = 3.65423\n",
      "epoch no.2 train no.238610  loss = 3.19648 avg_loss = 3.63561\n",
      "epoch no.2 train no.238620  loss = 4.00347 avg_loss = 3.62349\n",
      "epoch no.2 train no.238630  loss = 6.17030 avg_loss = 3.65385\n",
      "epoch no.2 train no.238640  loss = 1.26987 avg_loss = 3.65083\n",
      "epoch no.2 train no.238650  loss = 3.12065 avg_loss = 3.66251\n",
      "epoch no.2 train no.238660  loss = 2.42836 avg_loss = 3.63603\n",
      "epoch no.2 train no.238670  loss = 2.98589 avg_loss = 3.60129\n",
      "epoch no.2 train no.238680  loss = 2.69408 avg_loss = 3.53827\n",
      "epoch no.2 train no.238690  loss = 3.07149 avg_loss = 3.54399\n",
      "epoch no.2 train no.238700  loss = 3.50841 avg_loss = 3.60017\n",
      "epoch no.2 train no.238710  loss = 5.09314 avg_loss = 3.68086\n",
      "epoch no.2 train no.238720  loss = 2.78675 avg_loss = 3.65571\n",
      "epoch no.2 train no.238730  loss = 4.58344 avg_loss = 3.68597\n",
      "epoch no.2 train no.238740  loss = 2.45335 avg_loss = 3.64139\n",
      "epoch no.2 train no.238750  loss = 3.41450 avg_loss = 3.66558\n",
      "epoch no.2 train no.238760  loss = 3.75131 avg_loss = 3.65073\n",
      "epoch no.2 train no.238770  loss = 4.55567 avg_loss = 3.66604\n",
      "epoch no.2 train no.238780  loss = 3.01015 avg_loss = 3.64506\n",
      "epoch no.2 train no.238790  loss = 3.15098 avg_loss = 3.64631\n",
      "epoch no.2 train no.238800  loss = 4.15261 avg_loss = 3.67623\n",
      "epoch no.2 train no.238810  loss = 2.60159 avg_loss = 3.67144\n",
      "epoch no.2 train no.238820  loss = 4.67905 avg_loss = 3.70236\n",
      "epoch no.2 train no.238830  loss = 2.55146 avg_loss = 3.71438\n",
      "epoch no.2 train no.238840  loss = 4.28871 avg_loss = 3.71405\n",
      "epoch no.2 train no.238850  loss = 4.40879 avg_loss = 3.75738\n",
      "epoch no.2 train no.238860  loss = 3.31166 avg_loss = 3.75206\n",
      "epoch no.2 train no.238870  loss = 2.03653 avg_loss = 3.69018\n",
      "epoch no.2 train no.238880  loss = 3.40120 avg_loss = 3.70354\n",
      "epoch no.2 train no.238890  loss = 2.88780 avg_loss = 3.71323\n",
      "epoch no.2 train no.238900  loss = 2.38350 avg_loss = 3.69791\n",
      "epoch no.2 train no.238910  loss = 3.87584 avg_loss = 3.68346\n",
      "epoch no.2 train no.238920  loss = 3.46884 avg_loss = 3.73000\n",
      "epoch no.2 train no.238930  loss = 4.60508 avg_loss = 3.71022\n",
      "epoch no.2 train no.238940  loss = 2.96312 avg_loss = 3.68771\n",
      "epoch no.2 train no.238950  loss = 2.64862 avg_loss = 3.62651\n",
      "epoch no.2 train no.238960  loss = 2.93765 avg_loss = 3.69928\n",
      "epoch no.2 train no.238970  loss = 4.88992 avg_loss = 3.68577\n",
      "epoch no.2 train no.238980  loss = 3.19467 avg_loss = 3.69251\n",
      "epoch no.2 train no.238990  loss = 1.88478 avg_loss = 3.63721\n",
      "epoch no.2 train no.239000  loss = 3.07437 avg_loss = 3.65091\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁필요할', '▁때', '▁듣는', '▁신나는', '▁팝', '</s>']\n",
      "기분전환이 필요할 때 듣는 신나는 노래</s>\n",
      "epoch no.2 train no.239010  loss = 3.22399 avg_loss = 3.60656\n",
      "epoch no.2 train no.239020  loss = 3.61408 avg_loss = 3.60150\n",
      "epoch no.2 train no.239030  loss = 5.18446 avg_loss = 3.61675\n",
      "epoch no.2 train no.239040  loss = 2.00655 avg_loss = 3.62284\n",
      "epoch no.2 train no.239050  loss = 2.91038 avg_loss = 3.64793\n",
      "epoch no.2 train no.239060  loss = 3.02311 avg_loss = 3.65278\n",
      "epoch no.2 train no.239070  loss = 3.69767 avg_loss = 3.64457\n",
      "epoch no.2 train no.239080  loss = 2.42625 avg_loss = 3.63475\n",
      "epoch no.2 train no.239090  loss = 2.26302 avg_loss = 3.64821\n",
      "epoch no.2 train no.239100  loss = 3.14591 avg_loss = 3.64197\n",
      "epoch no.2 train no.239110  loss = 3.05773 avg_loss = 3.65720\n",
      "epoch no.2 train no.239120  loss = 5.16744 avg_loss = 3.71437\n",
      "epoch no.2 train no.239130  loss = 5.35243 avg_loss = 3.70634\n",
      "epoch no.2 train no.239140  loss = 3.00845 avg_loss = 3.67554\n",
      "epoch no.2 train no.239150  loss = 2.41866 avg_loss = 3.75215\n",
      "epoch no.2 train no.239160  loss = 2.85630 avg_loss = 3.71123\n",
      "epoch no.2 train no.239170  loss = 3.43386 avg_loss = 3.73265\n",
      "epoch no.2 train no.239180  loss = 3.01147 avg_loss = 3.69372\n",
      "epoch no.2 train no.239190  loss = 3.69216 avg_loss = 3.69165\n",
      "epoch no.2 train no.239200  loss = 4.45722 avg_loss = 3.70142\n",
      "epoch no.2 train no.239210  loss = 3.77850 avg_loss = 3.68115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.239220  loss = 3.66766 avg_loss = 3.68608\n",
      "epoch no.2 train no.239230  loss = 4.83524 avg_loss = 3.67546\n",
      "epoch no.2 train no.239240  loss = 3.46076 avg_loss = 3.67099\n",
      "epoch no.2 train no.239250  loss = 2.96252 avg_loss = 3.68716\n",
      "epoch no.2 train no.239260  loss = 3.71439 avg_loss = 3.66764\n",
      "epoch no.2 train no.239270  loss = 1.50667 avg_loss = 3.63614\n",
      "epoch no.2 train no.239280  loss = 2.49864 avg_loss = 3.63956\n",
      "epoch no.2 train no.239290  loss = 5.18132 avg_loss = 3.72386\n",
      "epoch no.2 train no.239300  loss = 4.26563 avg_loss = 3.76217\n",
      "epoch no.2 train no.239310  loss = 2.03774 avg_loss = 3.71280\n",
      "epoch no.2 train no.239320  loss = 6.00198 avg_loss = 3.74010\n",
      "epoch no.2 train no.239330  loss = 4.88059 avg_loss = 3.72796\n",
      "epoch no.2 train no.239340  loss = 2.78202 avg_loss = 3.73786\n",
      "epoch no.2 train no.239350  loss = 3.19607 avg_loss = 3.75346\n",
      "epoch no.2 train no.239360  loss = 4.21232 avg_loss = 3.77331\n",
      "epoch no.2 train no.239370  loss = 5.07737 avg_loss = 3.74975\n",
      "epoch no.2 train no.239380  loss = 3.24016 avg_loss = 3.76229\n",
      "epoch no.2 train no.239390  loss = 3.66412 avg_loss = 3.73174\n",
      "epoch no.2 train no.239400  loss = 6.20715 avg_loss = 3.73036\n",
      "epoch no.2 train no.239410  loss = 3.98385 avg_loss = 3.66662\n",
      "epoch no.2 train no.239420  loss = 3.32299 avg_loss = 3.64009\n",
      "epoch no.2 train no.239430  loss = 4.11890 avg_loss = 3.62864\n",
      "epoch no.2 train no.239440  loss = 3.89955 avg_loss = 3.56352\n",
      "epoch no.2 train no.239450  loss = 2.88317 avg_loss = 3.55434\n",
      "epoch no.2 train no.239460  loss = 2.31066 avg_loss = 3.48703\n",
      "epoch no.2 train no.239470  loss = 2.75005 avg_loss = 3.44888\n",
      "epoch no.2 train no.239480  loss = 2.27849 avg_loss = 3.49716\n",
      "epoch no.2 train no.239490  loss = 1.93701 avg_loss = 3.51317\n",
      "epoch no.2 train no.239500  loss = 3.78098 avg_loss = 3.52707\n",
      "epoch no.2 train no.239510  loss = 5.39095 avg_loss = 3.59294\n",
      "epoch no.2 train no.239520  loss = 3.24096 avg_loss = 3.60586\n",
      "epoch no.2 train no.239530  loss = 4.15711 avg_loss = 3.59460\n",
      "epoch no.2 train no.239540  loss = 2.79077 avg_loss = 3.59103\n",
      "epoch no.2 train no.239550  loss = 1.47689 avg_loss = 3.51281\n",
      "epoch no.2 train no.239560  loss = 3.73683 avg_loss = 3.54493\n",
      "epoch no.2 train no.239570  loss = 2.23727 avg_loss = 3.55420\n",
      "epoch no.2 train no.239580  loss = 3.23218 avg_loss = 3.57602\n",
      "epoch no.2 train no.239590  loss = 3.15444 avg_loss = 3.58271\n",
      "epoch no.2 train no.239600  loss = 3.93670 avg_loss = 3.55851\n",
      "epoch no.2 train no.239610  loss = 3.47313 avg_loss = 3.58841\n",
      "epoch no.2 train no.239620  loss = 2.09374 avg_loss = 3.60619\n",
      "epoch no.2 train no.239630  loss = 3.84179 avg_loss = 3.61654\n",
      "epoch no.2 train no.239640  loss = 3.81654 avg_loss = 3.63229\n",
      "epoch no.2 train no.239650  loss = 6.04217 avg_loss = 3.65007\n",
      "epoch no.2 train no.239660  loss = 4.61067 avg_loss = 3.65349\n",
      "epoch no.2 train no.239670  loss = 3.91225 avg_loss = 3.64350\n",
      "epoch no.2 train no.239680  loss = 2.25885 avg_loss = 3.64656\n",
      "epoch no.2 train no.239690  loss = 3.89890 avg_loss = 3.63987\n",
      "epoch no.2 train no.239700  loss = 3.91500 avg_loss = 3.65141\n",
      "epoch no.2 train no.239710  loss = 3.16560 avg_loss = 3.63365\n",
      "epoch no.2 train no.239720  loss = 2.35450 avg_loss = 3.62979\n",
      "epoch no.2 train no.239730  loss = 3.38162 avg_loss = 3.66369\n",
      "epoch no.2 train no.239740  loss = 2.59833 avg_loss = 3.71702\n",
      "epoch no.2 train no.239750  loss = 4.02985 avg_loss = 3.72799\n",
      "epoch no.2 train no.239760  loss = 5.02466 avg_loss = 3.78689\n",
      "epoch no.2 train no.239770  loss = 3.78109 avg_loss = 3.72567\n",
      "epoch no.2 train no.239780  loss = 2.65086 avg_loss = 3.68913\n",
      "epoch no.2 train no.239790  loss = 2.80791 avg_loss = 3.61233\n",
      "epoch no.2 train no.239800  loss = 3.19885 avg_loss = 3.61518\n",
      "epoch no.2 train no.239810  loss = 2.07099 avg_loss = 3.60734\n",
      "epoch no.2 train no.239820  loss = 3.67411 avg_loss = 3.58197\n",
      "epoch no.2 train no.239830  loss = 3.59578 avg_loss = 3.55363\n",
      "epoch no.2 train no.239840  loss = 3.60088 avg_loss = 3.55793\n",
      "epoch no.2 train no.239850  loss = 2.72760 avg_loss = 3.57220\n",
      "epoch no.2 train no.239860  loss = 2.56307 avg_loss = 3.56159\n",
      "epoch no.2 train no.239870  loss = 2.42651 avg_loss = 3.53090\n",
      "epoch no.2 train no.239880  loss = 2.85376 avg_loss = 3.56250\n",
      "epoch no.2 train no.239890  loss = 4.55310 avg_loss = 3.57351\n",
      "epoch no.2 train no.239900  loss = 3.22506 avg_loss = 3.61622\n",
      "epoch no.2 train no.239910  loss = 3.14565 avg_loss = 3.61413\n",
      "epoch no.2 train no.239920  loss = 3.60855 avg_loss = 3.63958\n",
      "epoch no.2 train no.239930  loss = 3.28603 avg_loss = 3.65240\n",
      "epoch no.2 train no.239940  loss = 2.83244 avg_loss = 3.63386\n",
      "epoch no.2 train no.239950  loss = 3.16231 avg_loss = 3.70781\n",
      "epoch no.2 train no.239960  loss = 3.79228 avg_loss = 3.70425\n",
      "epoch no.2 train no.239970  loss = 3.94470 avg_loss = 3.70667\n",
      "epoch no.2 train no.239980  loss = 2.67998 avg_loss = 3.65615\n",
      "epoch no.2 train no.239990  loss = 3.11194 avg_loss = 3.63686\n",
      "epoch no.2 train no.240000  loss = 2.83095 avg_loss = 3.66731\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '하기', '▁좋은', '▁음악', '들', '</s>']\n",
      "기분전환에 좋은 노래들</s>\n",
      "epoch no.2 train no.240010  loss = 3.56203 avg_loss = 3.65031\n",
      "epoch no.2 train no.240020  loss = 3.17656 avg_loss = 3.64974\n",
      "epoch no.2 train no.240030  loss = 2.48713 avg_loss = 3.60434\n",
      "epoch no.2 train no.240040  loss = 4.05763 avg_loss = 3.62634\n",
      "epoch no.2 train no.240050  loss = 2.12774 avg_loss = 3.60557\n",
      "epoch no.2 train no.240060  loss = 6.01204 avg_loss = 3.67137\n",
      "epoch no.2 train no.240070  loss = 3.10785 avg_loss = 3.63279\n",
      "epoch no.2 train no.240080  loss = 4.08127 avg_loss = 3.62042\n",
      "epoch no.2 train no.240090  loss = 4.67421 avg_loss = 3.59617\n",
      "epoch no.2 train no.240100  loss = 5.88131 avg_loss = 3.66492\n",
      "epoch no.2 train no.240110  loss = 3.42446 avg_loss = 3.64803\n",
      "epoch no.2 train no.240120  loss = 3.12095 avg_loss = 3.60767\n",
      "epoch no.2 train no.240130  loss = 4.06047 avg_loss = 3.59759\n",
      "epoch no.2 train no.240140  loss = 3.74393 avg_loss = 3.63568\n",
      "epoch no.2 train no.240150  loss = 3.49440 avg_loss = 3.63173\n",
      "epoch no.2 train no.240160  loss = 2.60668 avg_loss = 3.61882\n",
      "epoch no.2 train no.240170  loss = 3.13320 avg_loss = 3.63847\n",
      "epoch no.2 train no.240180  loss = 3.79789 avg_loss = 3.66717\n",
      "epoch no.2 train no.240190  loss = 2.97859 avg_loss = 3.65741\n",
      "epoch no.2 train no.240200  loss = 2.14360 avg_loss = 3.63793\n",
      "epoch no.2 train no.240210  loss = 3.00346 avg_loss = 3.64928\n",
      "epoch no.2 train no.240220  loss = 5.37048 avg_loss = 3.66733\n",
      "epoch no.2 train no.240230  loss = 1.64750 avg_loss = 3.60819\n",
      "epoch no.2 train no.240240  loss = 3.09565 avg_loss = 3.63760\n",
      "epoch no.2 train no.240250  loss = 2.51667 avg_loss = 3.58649\n",
      "epoch no.2 train no.240260  loss = 3.41812 avg_loss = 3.55253\n",
      "epoch no.2 train no.240270  loss = 3.91914 avg_loss = 3.56070\n",
      "epoch no.2 train no.240280  loss = 5.11173 avg_loss = 3.60202\n",
      "epoch no.2 train no.240290  loss = 1.13373 avg_loss = 3.57199\n",
      "epoch no.2 train no.240300  loss = 2.29868 avg_loss = 3.57530\n",
      "epoch no.2 train no.240310  loss = 4.14008 avg_loss = 3.61459\n",
      "epoch no.2 train no.240320  loss = 3.21743 avg_loss = 3.60130\n",
      "epoch no.2 train no.240330  loss = 4.97669 avg_loss = 3.61221\n",
      "epoch no.2 train no.240340  loss = 2.54234 avg_loss = 3.59300\n",
      "epoch no.2 train no.240350  loss = 4.75095 avg_loss = 3.64318\n",
      "epoch no.2 train no.240360  loss = 2.65904 avg_loss = 3.62352\n",
      "epoch no.2 train no.240370  loss = 2.97857 avg_loss = 3.63632\n",
      "epoch no.2 train no.240380  loss = 4.43811 avg_loss = 3.66790\n",
      "epoch no.2 train no.240390  loss = 2.02099 avg_loss = 3.62727\n",
      "epoch no.2 train no.240400  loss = 4.89475 avg_loss = 3.67273\n",
      "epoch no.2 train no.240410  loss = 4.13817 avg_loss = 3.64612\n",
      "epoch no.2 train no.240420  loss = 7.08204 avg_loss = 3.68368\n",
      "epoch no.2 train no.240430  loss = 3.62356 avg_loss = 3.67119\n",
      "epoch no.2 train no.240440  loss = 2.19185 avg_loss = 3.65918\n",
      "epoch no.2 train no.240450  loss = 4.56195 avg_loss = 3.67678\n",
      "epoch no.2 train no.240460  loss = 2.31832 avg_loss = 3.67589\n",
      "epoch no.2 train no.240470  loss = 2.72741 avg_loss = 3.67357\n",
      "epoch no.2 train no.240480  loss = 2.16887 avg_loss = 3.63531\n",
      "epoch no.2 train no.240490  loss = 3.99014 avg_loss = 3.65909\n",
      "epoch no.2 train no.240500  loss = 4.26194 avg_loss = 3.64029\n",
      "epoch no.2 train no.240510  loss = 7.95940 avg_loss = 3.65640\n",
      "epoch no.2 train no.240520  loss = 3.84010 avg_loss = 3.74025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.240530  loss = 3.14094 avg_loss = 3.77773\n",
      "epoch no.2 train no.240540  loss = 2.48792 avg_loss = 3.74543\n",
      "epoch no.2 train no.240550  loss = 3.59206 avg_loss = 3.72399\n",
      "epoch no.2 train no.240560  loss = 3.56704 avg_loss = 3.68458\n",
      "epoch no.2 train no.240570  loss = 2.15811 avg_loss = 3.64001\n",
      "epoch no.2 train no.240580  loss = 4.35218 avg_loss = 3.62690\n",
      "epoch no.2 train no.240590  loss = 2.51395 avg_loss = 3.61615\n",
      "epoch no.2 train no.240600  loss = 4.10200 avg_loss = 3.63149\n",
      "epoch no.2 train no.240610  loss = 2.45973 avg_loss = 3.58511\n",
      "epoch no.2 train no.240620  loss = 5.62682 avg_loss = 3.64618\n",
      "epoch no.2 train no.240630  loss = 5.02793 avg_loss = 3.66366\n",
      "epoch no.2 train no.240640  loss = 2.09997 avg_loss = 3.67370\n",
      "epoch no.2 train no.240650  loss = 4.09563 avg_loss = 3.70090\n",
      "epoch no.2 train no.240660  loss = 2.55148 avg_loss = 3.67756\n",
      "epoch no.2 train no.240670  loss = 2.14589 avg_loss = 3.68098\n",
      "epoch no.2 train no.240680  loss = 3.30454 avg_loss = 3.68431\n",
      "epoch no.2 train no.240690  loss = 3.55091 avg_loss = 3.74543\n",
      "epoch no.2 train no.240700  loss = 3.10251 avg_loss = 3.69535\n",
      "epoch no.2 train no.240710  loss = 2.44597 avg_loss = 3.69970\n",
      "epoch no.2 train no.240720  loss = 3.07639 avg_loss = 3.72763\n",
      "epoch no.2 train no.240730  loss = 2.67406 avg_loss = 3.69636\n",
      "epoch no.2 train no.240740  loss = 2.36928 avg_loss = 3.68154\n",
      "epoch no.2 train no.240750  loss = 3.16224 avg_loss = 3.68305\n",
      "epoch no.2 train no.240760  loss = 2.85851 avg_loss = 3.63741\n",
      "epoch no.2 train no.240770  loss = 3.72742 avg_loss = 3.58330\n",
      "epoch no.2 train no.240780  loss = 4.29329 avg_loss = 3.63846\n",
      "epoch no.2 train no.240790  loss = 2.93514 avg_loss = 3.60934\n",
      "epoch no.2 train no.240800  loss = 3.13584 avg_loss = 3.61188\n",
      "epoch no.2 train no.240810  loss = 2.45145 avg_loss = 3.62847\n",
      "epoch no.2 train no.240820  loss = 3.32652 avg_loss = 3.57705\n",
      "epoch no.2 train no.240830  loss = 4.43965 avg_loss = 3.62655\n",
      "epoch no.2 train no.240840  loss = 3.34431 avg_loss = 3.60266\n",
      "epoch no.2 train no.240850  loss = 2.52195 avg_loss = 3.53762\n",
      "epoch no.2 train no.240860  loss = 3.28708 avg_loss = 3.53997\n",
      "epoch no.2 train no.240870  loss = 3.71726 avg_loss = 3.48448\n",
      "epoch no.2 train no.240880  loss = 4.57973 avg_loss = 3.48062\n",
      "epoch no.2 train no.240890  loss = 2.60022 avg_loss = 3.47817\n",
      "epoch no.2 train no.240900  loss = 3.79584 avg_loss = 3.48888\n",
      "epoch no.2 train no.240910  loss = 4.03017 avg_loss = 3.48916\n",
      "epoch no.2 train no.240920  loss = 4.97963 avg_loss = 3.52993\n",
      "epoch no.2 train no.240930  loss = 4.52207 avg_loss = 3.60226\n",
      "epoch no.2 train no.240940  loss = 2.93239 avg_loss = 3.57447\n",
      "epoch no.2 train no.240950  loss = 3.61766 avg_loss = 3.57271\n",
      "epoch no.2 train no.240960  loss = 2.49182 avg_loss = 3.58102\n",
      "epoch no.2 train no.240970  loss = 3.22896 avg_loss = 3.60608\n",
      "epoch no.2 train no.240980  loss = 3.05095 avg_loss = 3.58918\n",
      "epoch no.2 train no.240990  loss = 4.37116 avg_loss = 3.60399\n",
      "epoch no.2 train no.241000  loss = 1.89181 avg_loss = 3.61226\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '하기', '때', '▁듣기', '▁신나는', '</s>']\n",
      "기분전환 할때 듣는 노래</s>\n",
      "epoch no.2 train no.241010  loss = 4.11438 avg_loss = 3.55660\n",
      "epoch no.2 train no.241020  loss = 3.15950 avg_loss = 3.58696\n",
      "epoch no.2 train no.241030  loss = 3.41049 avg_loss = 3.57476\n",
      "epoch no.2 train no.241040  loss = 3.65386 avg_loss = 3.55565\n",
      "epoch no.2 train no.241050  loss = 4.22108 avg_loss = 3.51616\n",
      "epoch no.2 train no.241060  loss = 4.11311 avg_loss = 3.51672\n",
      "epoch no.2 train no.241070  loss = 2.91212 avg_loss = 3.53160\n",
      "epoch no.2 train no.241080  loss = 3.32935 avg_loss = 3.52506\n",
      "epoch no.2 train no.241090  loss = 2.89462 avg_loss = 3.51479\n",
      "epoch no.2 train no.241100  loss = 2.30571 avg_loss = 3.53143\n",
      "epoch no.2 train no.241110  loss = 3.84188 avg_loss = 3.52855\n",
      "epoch no.2 train no.241120  loss = 2.89450 avg_loss = 3.49756\n",
      "epoch no.2 train no.241130  loss = 1.48701 avg_loss = 3.45196\n",
      "epoch no.2 train no.241140  loss = 2.95482 avg_loss = 3.47982\n",
      "epoch no.2 train no.241150  loss = 5.37617 avg_loss = 3.51764\n",
      "epoch no.2 train no.241160  loss = 4.03023 avg_loss = 3.55845\n",
      "epoch no.2 train no.241170  loss = 4.84209 avg_loss = 3.51935\n",
      "epoch no.2 train no.241180  loss = 4.71201 avg_loss = 3.50808\n",
      "epoch no.2 train no.241190  loss = 3.70290 avg_loss = 3.47815\n",
      "epoch no.2 train no.241200  loss = 2.51736 avg_loss = 3.50794\n",
      "epoch no.2 train no.241210  loss = 4.80018 avg_loss = 3.46765\n",
      "epoch no.2 train no.241220  loss = 4.89289 avg_loss = 3.48903\n",
      "epoch no.2 train no.241230  loss = 4.46312 avg_loss = 3.48640\n",
      "epoch no.2 train no.241240  loss = 2.75413 avg_loss = 3.48957\n",
      "epoch no.2 train no.241250  loss = 2.81819 avg_loss = 3.47273\n",
      "epoch no.2 train no.241260  loss = 1.74727 avg_loss = 3.48781\n",
      "epoch no.2 train no.241270  loss = 4.15780 avg_loss = 3.48529\n",
      "epoch no.2 train no.241280  loss = 2.13471 avg_loss = 3.49257\n",
      "epoch no.2 train no.241290  loss = 7.59343 avg_loss = 3.57240\n",
      "epoch no.2 train no.241300  loss = 2.69020 avg_loss = 3.64297\n",
      "epoch no.2 train no.241310  loss = 3.44394 avg_loss = 3.62072\n",
      "epoch no.2 train no.241320  loss = 3.33319 avg_loss = 3.66029\n",
      "epoch no.2 train no.241330  loss = 3.85426 avg_loss = 3.69907\n",
      "epoch no.2 train no.241340  loss = 2.11331 avg_loss = 3.75611\n",
      "epoch no.2 train no.241350  loss = 3.30329 avg_loss = 3.78530\n",
      "epoch no.2 train no.241360  loss = 2.91770 avg_loss = 3.82629\n",
      "epoch no.2 train no.241370  loss = 3.80301 avg_loss = 3.85745\n",
      "epoch no.2 train no.241380  loss = 3.55433 avg_loss = 3.81855\n",
      "epoch no.2 train no.241390  loss = 2.67107 avg_loss = 3.82720\n",
      "epoch no.2 train no.241400  loss = 3.18254 avg_loss = 3.77171\n",
      "epoch no.2 train no.241410  loss = 3.41947 avg_loss = 3.80090\n",
      "epoch no.2 train no.241420  loss = 5.34089 avg_loss = 3.85897\n",
      "epoch no.2 train no.241430  loss = 4.90356 avg_loss = 3.87042\n",
      "epoch no.2 train no.241440  loss = 3.21809 avg_loss = 3.82032\n",
      "epoch no.2 train no.241450  loss = 5.98406 avg_loss = 3.83195\n",
      "epoch no.2 train no.241460  loss = 3.83457 avg_loss = 3.85059\n",
      "epoch no.2 train no.241470  loss = 3.87564 avg_loss = 3.84263\n",
      "epoch no.2 train no.241480  loss = 2.66410 avg_loss = 3.81046\n",
      "epoch no.2 train no.241490  loss = 2.33486 avg_loss = 3.81586\n",
      "epoch no.2 train no.241500  loss = 3.31380 avg_loss = 3.79166\n",
      "epoch no.2 train no.241510  loss = 3.50975 avg_loss = 3.74761\n",
      "epoch no.2 train no.241520  loss = 2.84128 avg_loss = 3.70253\n",
      "epoch no.2 train no.241530  loss = 3.96164 avg_loss = 3.71168\n",
      "epoch no.2 train no.241540  loss = 4.01668 avg_loss = 3.70406\n",
      "epoch no.2 train no.241550  loss = 3.20677 avg_loss = 3.70279\n",
      "epoch no.2 train no.241560  loss = 7.32019 avg_loss = 3.71756\n",
      "epoch no.2 train no.241570  loss = 4.95862 avg_loss = 3.68084\n",
      "epoch no.2 train no.241580  loss = 4.35727 avg_loss = 3.75224\n",
      "epoch no.2 train no.241590  loss = 4.85766 avg_loss = 3.75658\n",
      "epoch no.2 train no.241600  loss = 2.67292 avg_loss = 3.71398\n",
      "epoch no.2 train no.241610  loss = 1.79801 avg_loss = 3.74169\n",
      "epoch no.2 train no.241620  loss = 4.91447 avg_loss = 3.76788\n",
      "epoch no.2 train no.241630  loss = 2.96888 avg_loss = 3.73955\n",
      "epoch no.2 train no.241640  loss = 3.52065 avg_loss = 3.76051\n",
      "epoch no.2 train no.241650  loss = 4.38820 avg_loss = 3.75722\n",
      "epoch no.2 train no.241660  loss = 2.50122 avg_loss = 3.80283\n",
      "epoch no.2 train no.241670  loss = 1.91666 avg_loss = 3.74913\n",
      "epoch no.2 train no.241680  loss = 3.83863 avg_loss = 3.75438\n",
      "epoch no.2 train no.241690  loss = 4.40059 avg_loss = 3.71859\n",
      "epoch no.2 train no.241700  loss = 3.47489 avg_loss = 3.65577\n",
      "epoch no.2 train no.241710  loss = 3.82807 avg_loss = 3.65275\n",
      "epoch no.2 train no.241720  loss = 4.40245 avg_loss = 3.65688\n",
      "epoch no.2 train no.241730  loss = 3.62689 avg_loss = 3.65688\n",
      "epoch no.2 train no.241740  loss = 3.54548 avg_loss = 3.67653\n",
      "epoch no.2 train no.241750  loss = 2.34206 avg_loss = 3.66189\n",
      "epoch no.2 train no.241760  loss = 4.58228 avg_loss = 3.69514\n",
      "epoch no.2 train no.241770  loss = 4.48397 avg_loss = 3.68315\n",
      "epoch no.2 train no.241780  loss = 3.32515 avg_loss = 3.60349\n",
      "epoch no.2 train no.241790  loss = 4.00085 avg_loss = 3.60776\n",
      "epoch no.2 train no.241800  loss = 5.04360 avg_loss = 3.63343\n",
      "epoch no.2 train no.241810  loss = 3.29124 avg_loss = 3.71070\n",
      "epoch no.2 train no.241820  loss = 4.39267 avg_loss = 3.69998\n",
      "epoch no.2 train no.241830  loss = 1.95720 avg_loss = 3.62320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.241840  loss = 5.24459 avg_loss = 3.66654\n",
      "epoch no.2 train no.241850  loss = 5.09815 avg_loss = 3.67596\n",
      "epoch no.2 train no.241860  loss = 3.30931 avg_loss = 3.66825\n",
      "epoch no.2 train no.241870  loss = 2.11617 avg_loss = 3.69250\n",
      "epoch no.2 train no.241880  loss = 4.26065 avg_loss = 3.67473\n",
      "epoch no.2 train no.241890  loss = 4.79917 avg_loss = 3.65941\n",
      "epoch no.2 train no.241900  loss = 4.34435 avg_loss = 3.65102\n",
      "epoch no.2 train no.241910  loss = 3.23861 avg_loss = 3.64700\n",
      "epoch no.2 train no.241920  loss = 3.81553 avg_loss = 3.67217\n",
      "epoch no.2 train no.241930  loss = 5.47687 avg_loss = 3.69364\n",
      "epoch no.2 train no.241940  loss = 5.61909 avg_loss = 3.65478\n",
      "epoch no.2 train no.241950  loss = 6.79821 avg_loss = 3.68289\n",
      "epoch no.2 train no.241960  loss = 3.04571 avg_loss = 3.72503\n",
      "epoch no.2 train no.241970  loss = 3.74481 avg_loss = 3.71150\n",
      "epoch no.2 train no.241980  loss = 3.39559 avg_loss = 3.75511\n",
      "epoch no.2 train no.241990  loss = 2.31052 avg_loss = 3.74786\n",
      "epoch no.2 train no.242000  loss = 3.35491 avg_loss = 3.78275\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁필요할', '때', '▁듣는', '▁음악', '송', '</s>']\n",
      "기분전환이 필요할때 듣는 팝송</s>\n",
      "epoch no.2 train no.242010  loss = 3.39448 avg_loss = 3.79294\n",
      "epoch no.2 train no.242020  loss = 5.53258 avg_loss = 3.87944\n",
      "epoch no.2 train no.242030  loss = 6.08692 avg_loss = 3.87409\n",
      "epoch no.2 train no.242040  loss = 2.97697 avg_loss = 3.88128\n",
      "epoch no.2 train no.242050  loss = 2.88921 avg_loss = 3.84733\n",
      "epoch no.2 train no.242060  loss = 3.85306 avg_loss = 3.85659\n",
      "epoch no.2 train no.242070  loss = 2.35775 avg_loss = 3.87679\n",
      "epoch no.2 train no.242080  loss = 2.57199 avg_loss = 3.83876\n",
      "epoch no.2 train no.242090  loss = 2.62761 avg_loss = 3.84676\n",
      "epoch no.2 train no.242100  loss = 2.61693 avg_loss = 3.85625\n",
      "epoch no.2 train no.242110  loss = 4.21884 avg_loss = 3.87072\n",
      "epoch no.2 train no.242120  loss = 3.08256 avg_loss = 3.83819\n",
      "epoch no.2 train no.242130  loss = 3.36706 avg_loss = 3.83539\n",
      "epoch no.2 train no.242140  loss = 2.47491 avg_loss = 3.80398\n",
      "epoch no.2 train no.242150  loss = 4.94638 avg_loss = 3.77487\n",
      "epoch no.2 train no.242160  loss = 3.09932 avg_loss = 3.73245\n",
      "epoch no.2 train no.242170  loss = 2.50012 avg_loss = 3.68503\n",
      "epoch no.2 train no.242180  loss = 4.68251 avg_loss = 3.69864\n",
      "epoch no.2 train no.242190  loss = 4.38932 avg_loss = 3.66103\n",
      "epoch no.2 train no.242200  loss = 4.03952 avg_loss = 3.62269\n",
      "epoch no.2 train no.242210  loss = 3.11379 avg_loss = 3.65306\n",
      "epoch no.2 train no.242220  loss = 3.26972 avg_loss = 3.64235\n",
      "epoch no.2 train no.242230  loss = 2.23992 avg_loss = 3.62966\n",
      "epoch no.2 train no.242240  loss = 3.60628 avg_loss = 3.59780\n",
      "epoch no.2 train no.242250  loss = 4.36431 avg_loss = 3.57847\n",
      "epoch no.2 train no.242260  loss = 1.91585 avg_loss = 3.56367\n",
      "epoch no.2 train no.242270  loss = 3.61297 avg_loss = 3.55928\n",
      "epoch no.2 train no.242280  loss = 4.32709 avg_loss = 3.56131\n",
      "epoch no.2 train no.242290  loss = 2.91554 avg_loss = 3.55326\n",
      "epoch no.2 train no.242300  loss = 4.64250 avg_loss = 3.55863\n",
      "epoch no.2 train no.242310  loss = 3.08335 avg_loss = 3.55926\n",
      "epoch no.2 train no.242320  loss = 3.99634 avg_loss = 3.57558\n",
      "epoch no.2 train no.242330  loss = 3.36091 avg_loss = 3.59432\n",
      "epoch no.2 train no.242340  loss = 5.18480 avg_loss = 3.65309\n",
      "epoch no.2 train no.242350  loss = 3.94597 avg_loss = 3.63814\n",
      "epoch no.2 train no.242360  loss = 2.90135 avg_loss = 3.62451\n",
      "epoch no.2 train no.242370  loss = 4.35585 avg_loss = 3.60343\n",
      "epoch no.2 train no.242380  loss = 4.16435 avg_loss = 3.62530\n",
      "epoch no.2 train no.242390  loss = 6.49288 avg_loss = 3.69298\n",
      "epoch no.2 train no.242400  loss = 3.15370 avg_loss = 3.73846\n",
      "epoch no.2 train no.242410  loss = 2.58690 avg_loss = 3.71029\n",
      "epoch no.2 train no.242420  loss = 4.84624 avg_loss = 3.69399\n",
      "epoch no.2 train no.242430  loss = 2.83692 avg_loss = 3.69983\n",
      "epoch no.2 train no.242440  loss = 4.27207 avg_loss = 3.66683\n",
      "epoch no.2 train no.242450  loss = 3.30094 avg_loss = 3.67392\n",
      "epoch no.2 train no.242460  loss = 2.70079 avg_loss = 3.66565\n",
      "epoch no.2 train no.242470  loss = 2.21210 avg_loss = 3.69407\n",
      "epoch no.2 train no.242480  loss = 3.95850 avg_loss = 3.69571\n",
      "epoch no.2 train no.242490  loss = 4.29671 avg_loss = 3.68162\n",
      "epoch no.2 train no.242500  loss = 3.21073 avg_loss = 3.67383\n",
      "epoch no.2 train no.242510  loss = 3.33294 avg_loss = 3.67858\n",
      "epoch no.2 train no.242520  loss = 2.00419 avg_loss = 3.71373\n",
      "epoch no.2 train no.242530  loss = 5.04861 avg_loss = 3.74268\n",
      "epoch no.2 train no.242540  loss = 2.37821 avg_loss = 3.74923\n",
      "epoch no.2 train no.242550  loss = 2.92374 avg_loss = 3.70317\n",
      "epoch no.2 train no.242560  loss = 4.73628 avg_loss = 3.72834\n",
      "epoch no.2 train no.242570  loss = 6.64939 avg_loss = 3.78243\n",
      "epoch no.2 train no.242580  loss = 3.80986 avg_loss = 3.73744\n",
      "epoch no.2 train no.242590  loss = 3.89692 avg_loss = 3.72627\n",
      "epoch no.2 train no.242600  loss = 3.21883 avg_loss = 3.70091\n",
      "epoch no.2 train no.242610  loss = 2.97761 avg_loss = 3.71539\n",
      "epoch no.2 train no.242620  loss = 4.14415 avg_loss = 3.69309\n",
      "epoch no.2 train no.242630  loss = 3.55357 avg_loss = 3.71368\n",
      "epoch no.2 train no.242640  loss = 5.43746 avg_loss = 3.73328\n",
      "epoch no.2 train no.242650  loss = 4.95493 avg_loss = 3.73467\n",
      "epoch no.2 train no.242660  loss = 4.83723 avg_loss = 3.78026\n",
      "epoch no.2 train no.242670  loss = 4.22048 avg_loss = 3.75864\n",
      "epoch no.2 train no.242680  loss = 3.10605 avg_loss = 3.77272\n",
      "epoch no.2 train no.242690  loss = 2.96945 avg_loss = 3.73725\n",
      "epoch no.2 train no.242700  loss = 3.46950 avg_loss = 3.72738\n",
      "epoch no.2 train no.242710  loss = 3.86529 avg_loss = 3.76513\n",
      "epoch no.2 train no.242720  loss = 2.79485 avg_loss = 3.76810\n",
      "epoch no.2 train no.242730  loss = 1.06769 avg_loss = 3.70631\n",
      "epoch no.2 train no.242740  loss = 3.03552 avg_loss = 3.64700\n",
      "epoch no.2 train no.242750  loss = 4.37980 avg_loss = 3.68605\n",
      "epoch no.2 train no.242760  loss = 3.57949 avg_loss = 3.66615\n",
      "epoch no.2 train no.242770  loss = 1.86602 avg_loss = 3.70225\n",
      "epoch no.2 train no.242780  loss = 2.65151 avg_loss = 3.69555\n",
      "epoch no.2 train no.242790  loss = 2.95655 avg_loss = 3.72462\n",
      "epoch no.2 train no.242800  loss = 5.30625 avg_loss = 3.74141\n",
      "epoch no.2 train no.242810  loss = 3.81366 avg_loss = 3.75627\n",
      "epoch no.2 train no.242820  loss = 2.56661 avg_loss = 3.77696\n",
      "epoch no.2 train no.242830  loss = 5.37298 avg_loss = 3.71753\n",
      "epoch no.2 train no.242840  loss = 3.31529 avg_loss = 3.68087\n",
      "epoch no.2 train no.242850  loss = 3.32468 avg_loss = 3.67350\n",
      "epoch no.2 train no.242860  loss = 2.91120 avg_loss = 3.65234\n",
      "epoch no.2 train no.242870  loss = 3.07352 avg_loss = 3.65848\n",
      "epoch no.2 train no.242880  loss = 2.53284 avg_loss = 3.69007\n",
      "epoch no.2 train no.242890  loss = 1.94996 avg_loss = 3.68553\n",
      "epoch no.2 train no.242900  loss = 3.76722 avg_loss = 3.65408\n",
      "epoch no.2 train no.242910  loss = 3.52912 avg_loss = 3.62295\n",
      "epoch no.2 train no.242920  loss = 3.09820 avg_loss = 3.61482\n",
      "epoch no.2 train no.242930  loss = 3.00683 avg_loss = 3.58933\n",
      "epoch no.2 train no.242940  loss = 3.36841 avg_loss = 3.60127\n",
      "epoch no.2 train no.242950  loss = 3.07709 avg_loss = 3.52734\n",
      "epoch no.2 train no.242960  loss = 7.70233 avg_loss = 3.62382\n",
      "epoch no.2 train no.242970  loss = 3.42898 avg_loss = 3.67891\n",
      "epoch no.2 train no.242980  loss = 3.48943 avg_loss = 3.68147\n",
      "epoch no.2 train no.242990  loss = 5.80199 avg_loss = 3.70647\n",
      "epoch no.2 train no.243000  loss = 2.41216 avg_loss = 3.69044\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁딱', '▁좋은', '▁팝', '</s>']\n",
      "기분전환에 딱 좋은 음악</s>\n",
      "epoch no.2 train no.243010  loss = 4.24157 avg_loss = 3.71336\n",
      "epoch no.2 train no.243020  loss = 4.23368 avg_loss = 3.71342\n",
      "epoch no.2 train no.243030  loss = 2.07409 avg_loss = 3.67208\n",
      "epoch no.2 train no.243040  loss = 4.58345 avg_loss = 3.66447\n",
      "epoch no.2 train no.243050  loss = 2.31169 avg_loss = 3.59327\n",
      "epoch no.2 train no.243060  loss = 4.09484 avg_loss = 3.61491\n",
      "epoch no.2 train no.243070  loss = 3.22598 avg_loss = 3.57841\n",
      "epoch no.2 train no.243080  loss = 3.21412 avg_loss = 3.54447\n",
      "epoch no.2 train no.243090  loss = 4.12854 avg_loss = 3.56362\n",
      "epoch no.2 train no.243100  loss = 3.36947 avg_loss = 3.55494\n",
      "epoch no.2 train no.243110  loss = 1.96235 avg_loss = 3.52165\n",
      "epoch no.2 train no.243120  loss = 7.19584 avg_loss = 3.57997\n",
      "epoch no.2 train no.243130  loss = 4.91707 avg_loss = 3.57574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.243140  loss = 4.97270 avg_loss = 3.64794\n",
      "epoch no.2 train no.243150  loss = 3.43946 avg_loss = 3.65993\n",
      "epoch no.2 train no.243160  loss = 3.34705 avg_loss = 3.68637\n",
      "epoch no.2 train no.243170  loss = 1.28674 avg_loss = 3.69851\n",
      "epoch no.2 train no.243180  loss = 2.57410 avg_loss = 3.68035\n",
      "epoch no.2 train no.243190  loss = 4.05350 avg_loss = 3.70612\n",
      "epoch no.2 train no.243200  loss = 3.76293 avg_loss = 3.76891\n",
      "epoch no.2 train no.243210  loss = 3.92200 avg_loss = 3.73784\n",
      "epoch no.2 train no.243220  loss = 4.15034 avg_loss = 3.71322\n",
      "epoch no.2 train no.243230  loss = 2.89278 avg_loss = 3.65473\n",
      "epoch no.2 train no.243240  loss = 4.81028 avg_loss = 3.62416\n",
      "epoch no.2 train no.243250  loss = 2.36179 avg_loss = 3.59022\n",
      "epoch no.2 train no.243260  loss = 3.71331 avg_loss = 3.63976\n",
      "epoch no.2 train no.243270  loss = 2.74455 avg_loss = 3.68085\n",
      "epoch no.2 train no.243280  loss = 4.70127 avg_loss = 3.72882\n",
      "epoch no.2 train no.243290  loss = 4.19062 avg_loss = 3.70517\n",
      "epoch no.2 train no.243300  loss = 4.50076 avg_loss = 3.68613\n",
      "epoch no.2 train no.243310  loss = 3.74466 avg_loss = 3.73312\n",
      "epoch no.2 train no.243320  loss = 5.09268 avg_loss = 3.75808\n",
      "epoch no.2 train no.243330  loss = 3.81955 avg_loss = 3.79190\n",
      "epoch no.2 train no.243340  loss = 4.01153 avg_loss = 3.80593\n",
      "epoch no.2 train no.243350  loss = 3.33387 avg_loss = 3.73211\n",
      "epoch no.2 train no.243360  loss = 2.92704 avg_loss = 3.73252\n",
      "epoch no.2 train no.243370  loss = 3.47388 avg_loss = 3.70473\n",
      "epoch no.2 train no.243380  loss = 2.56470 avg_loss = 3.69003\n",
      "epoch no.2 train no.243390  loss = 1.99536 avg_loss = 3.66614\n",
      "epoch no.2 train no.243400  loss = 3.47207 avg_loss = 3.65991\n",
      "epoch no.2 train no.243410  loss = 4.80912 avg_loss = 3.61130\n",
      "epoch no.2 train no.243420  loss = 3.10978 avg_loss = 3.60459\n",
      "epoch no.2 train no.243430  loss = 3.51536 avg_loss = 3.58329\n",
      "epoch no.2 train no.243440  loss = 3.36981 avg_loss = 3.57672\n",
      "epoch no.2 train no.243450  loss = 3.61722 avg_loss = 3.55437\n",
      "epoch no.2 train no.243460  loss = 5.81402 avg_loss = 3.60614\n",
      "epoch no.2 train no.243470  loss = 4.67317 avg_loss = 3.60918\n",
      "epoch no.2 train no.243480  loss = 4.33614 avg_loss = 3.63995\n",
      "epoch no.2 train no.243490  loss = 3.01246 avg_loss = 3.66113\n",
      "epoch no.2 train no.243500  loss = 4.01087 avg_loss = 3.65910\n",
      "epoch no.2 train no.243510  loss = 3.08897 avg_loss = 3.66887\n",
      "epoch no.2 train no.243520  loss = 2.00491 avg_loss = 3.65720\n",
      "epoch no.2 train no.243530  loss = 4.57081 avg_loss = 3.67975\n",
      "epoch no.2 train no.243540  loss = 3.88021 avg_loss = 3.60988\n",
      "epoch no.2 train no.243550  loss = 2.90616 avg_loss = 3.60777\n",
      "epoch no.2 train no.243560  loss = 2.85600 avg_loss = 3.61337\n",
      "epoch no.2 train no.243570  loss = 3.52110 avg_loss = 3.57771\n",
      "epoch no.2 train no.243580  loss = 3.41898 avg_loss = 3.62009\n",
      "epoch no.2 train no.243590  loss = 3.99018 avg_loss = 3.59311\n",
      "epoch no.2 train no.243600  loss = 4.20661 avg_loss = 3.62406\n",
      "epoch no.2 train no.243610  loss = 3.00021 avg_loss = 3.63920\n",
      "epoch no.2 train no.243620  loss = 4.09951 avg_loss = 3.64091\n",
      "epoch no.2 train no.243630  loss = 2.76776 avg_loss = 3.66417\n",
      "epoch no.2 train no.243640  loss = 6.63155 avg_loss = 3.72962\n",
      "epoch no.2 train no.243650  loss = 2.71622 avg_loss = 3.70348\n",
      "epoch no.2 train no.243660  loss = 2.22434 avg_loss = 3.67526\n",
      "epoch no.2 train no.243670  loss = 3.03663 avg_loss = 3.68211\n",
      "epoch no.2 train no.243680  loss = 5.24611 avg_loss = 3.66227\n",
      "epoch no.2 train no.243690  loss = 2.53128 avg_loss = 3.67826\n",
      "epoch no.2 train no.243700  loss = 2.52450 avg_loss = 3.64904\n",
      "epoch no.2 train no.243710  loss = 3.39936 avg_loss = 3.63092\n",
      "epoch no.2 train no.243720  loss = 3.36399 avg_loss = 3.64832\n",
      "epoch no.2 train no.243730  loss = 4.37498 avg_loss = 3.69888\n",
      "epoch no.2 train no.243740  loss = 3.62463 avg_loss = 3.69802\n",
      "epoch no.2 train no.243750  loss = 2.15027 avg_loss = 3.68853\n",
      "epoch no.2 train no.243760  loss = 4.51790 avg_loss = 3.68960\n",
      "epoch no.2 train no.243770  loss = 3.22417 avg_loss = 3.62638\n",
      "epoch no.2 train no.243780  loss = 3.57016 avg_loss = 3.63470\n",
      "epoch no.2 train no.243790  loss = 4.94735 avg_loss = 3.61814\n",
      "epoch no.2 train no.243800  loss = 3.85378 avg_loss = 3.68690\n",
      "epoch no.2 train no.243810  loss = 1.64293 avg_loss = 3.65413\n",
      "epoch no.2 train no.243820  loss = 3.45964 avg_loss = 3.62725\n",
      "epoch no.2 train no.243830  loss = 1.99609 avg_loss = 3.57563\n",
      "epoch no.2 train no.243840  loss = 2.88012 avg_loss = 3.62059\n",
      "epoch no.2 train no.243850  loss = 2.44941 avg_loss = 3.61003\n",
      "epoch no.2 train no.243860  loss = 1.44736 avg_loss = 3.61710\n",
      "epoch no.2 train no.243870  loss = 2.69254 avg_loss = 3.57402\n",
      "epoch no.2 train no.243880  loss = 3.21833 avg_loss = 3.60518\n",
      "epoch no.2 train no.243890  loss = 3.70449 avg_loss = 3.66680\n",
      "epoch no.2 train no.243900  loss = 3.57587 avg_loss = 3.63382\n",
      "epoch no.2 train no.243910  loss = 3.23201 avg_loss = 3.61406\n",
      "epoch no.2 train no.243920  loss = 4.70431 avg_loss = 3.65502\n",
      "epoch no.2 train no.243930  loss = 2.82932 avg_loss = 3.67223\n",
      "epoch no.2 train no.243940  loss = 3.38575 avg_loss = 3.65723\n",
      "epoch no.2 train no.243950  loss = 2.46901 avg_loss = 3.64674\n",
      "epoch no.2 train no.243960  loss = 4.50055 avg_loss = 3.63376\n",
      "epoch no.2 train no.243970  loss = 2.19712 avg_loss = 3.66359\n",
      "epoch no.2 train no.243980  loss = 2.20796 avg_loss = 3.65852\n",
      "epoch no.2 train no.243990  loss = 4.57351 avg_loss = 3.61710\n",
      "epoch no.2 train no.244000  loss = 3.47483 avg_loss = 3.61504\n",
      "5\n",
      "to_tokens: ['▁가을', '전환', '이', '▁필요할', '▁때', '▁듣는', '▁음악', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.2 train no.244010  loss = 3.84427 avg_loss = 3.65414\n",
      "epoch no.2 train no.244020  loss = 2.99592 avg_loss = 3.59627\n",
      "epoch no.2 train no.244030  loss = 4.24350 avg_loss = 3.58318\n",
      "epoch no.2 train no.244040  loss = 2.91600 avg_loss = 3.55432\n",
      "epoch no.2 train no.244050  loss = 2.06949 avg_loss = 3.55256\n",
      "epoch no.2 train no.244060  loss = 3.19989 avg_loss = 3.60956\n",
      "epoch no.2 train no.244070  loss = 2.60410 avg_loss = 3.63029\n",
      "epoch no.2 train no.244080  loss = 3.76967 avg_loss = 3.65111\n",
      "epoch no.2 train no.244090  loss = 1.34517 avg_loss = 3.64125\n",
      "epoch no.2 train no.244100  loss = 5.05132 avg_loss = 3.65263\n",
      "epoch no.2 train no.244110  loss = 3.27400 avg_loss = 3.66298\n",
      "epoch no.2 train no.244120  loss = 2.86932 avg_loss = 3.68592\n",
      "epoch no.2 train no.244130  loss = 3.80726 avg_loss = 3.69999\n",
      "epoch no.2 train no.244140  loss = 3.36392 avg_loss = 3.68874\n",
      "epoch no.2 train no.244150  loss = 1.98015 avg_loss = 3.64463\n",
      "epoch no.2 train no.244160  loss = 5.11957 avg_loss = 3.61155\n",
      "epoch no.2 train no.244170  loss = 5.60374 avg_loss = 3.64595\n",
      "epoch no.2 train no.244180  loss = 4.20217 avg_loss = 3.62938\n",
      "epoch no.2 train no.244190  loss = 3.29446 avg_loss = 3.64160\n",
      "epoch no.2 train no.244200  loss = 4.07728 avg_loss = 3.71307\n",
      "epoch no.2 train no.244210  loss = 4.09648 avg_loss = 3.68740\n",
      "epoch no.2 train no.244220  loss = 4.25887 avg_loss = 3.65221\n",
      "epoch no.2 train no.244230  loss = 4.20414 avg_loss = 3.58618\n",
      "epoch no.2 train no.244240  loss = 2.87792 avg_loss = 3.59699\n",
      "epoch no.2 train no.244250  loss = 4.16881 avg_loss = 3.61106\n",
      "epoch no.2 train no.244260  loss = 2.81722 avg_loss = 3.59782\n",
      "epoch no.2 train no.244270  loss = 4.55866 avg_loss = 3.59688\n",
      "epoch no.2 train no.244280  loss = 4.13314 avg_loss = 3.58127\n",
      "epoch no.2 train no.244290  loss = 2.36738 avg_loss = 3.60202\n",
      "epoch no.2 train no.244300  loss = 4.55177 avg_loss = 3.63953\n",
      "epoch no.2 train no.244310  loss = 1.53832 avg_loss = 3.68465\n",
      "epoch no.2 train no.244320  loss = 2.99585 avg_loss = 3.73434\n",
      "epoch no.2 train no.244330  loss = 3.04537 avg_loss = 3.68728\n",
      "epoch no.2 train no.244340  loss = 2.69052 avg_loss = 3.69190\n",
      "epoch no.2 train no.244350  loss = 3.59115 avg_loss = 3.68542\n",
      "epoch no.2 train no.244360  loss = 2.71325 avg_loss = 3.70007\n",
      "epoch no.2 train no.244370  loss = 4.11374 avg_loss = 3.72994\n",
      "epoch no.2 train no.244380  loss = 2.04822 avg_loss = 3.66394\n",
      "epoch no.2 train no.244390  loss = 2.92327 avg_loss = 3.62927\n",
      "epoch no.2 train no.244400  loss = 3.60254 avg_loss = 3.59266\n",
      "epoch no.2 train no.244410  loss = 2.50045 avg_loss = 3.60650\n",
      "epoch no.2 train no.244420  loss = 3.50138 avg_loss = 3.57737\n",
      "epoch no.2 train no.244430  loss = 3.03665 avg_loss = 3.60636\n",
      "epoch no.2 train no.244440  loss = 4.95382 avg_loss = 3.66047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.244450  loss = 2.92627 avg_loss = 3.66130\n",
      "epoch no.2 train no.244460  loss = 3.31516 avg_loss = 3.59962\n",
      "epoch no.2 train no.244470  loss = 4.11684 avg_loss = 3.59341\n",
      "epoch no.2 train no.244480  loss = 3.35898 avg_loss = 3.63249\n",
      "epoch no.2 train no.244490  loss = 2.48720 avg_loss = 3.59430\n",
      "epoch no.2 train no.244500  loss = 4.32393 avg_loss = 3.64287\n",
      "epoch no.2 train no.244510  loss = 3.83456 avg_loss = 3.63656\n",
      "epoch no.2 train no.244520  loss = 4.19266 avg_loss = 3.62181\n",
      "epoch no.2 train no.244530  loss = 3.39023 avg_loss = 3.57012\n",
      "epoch no.2 train no.244540  loss = 2.96740 avg_loss = 3.58067\n",
      "epoch no.2 train no.244550  loss = 3.92751 avg_loss = 3.56320\n",
      "epoch no.2 train no.244560  loss = 4.52906 avg_loss = 3.52698\n",
      "epoch no.2 train no.244570  loss = 4.16224 avg_loss = 3.51267\n",
      "epoch no.2 train no.244580  loss = 6.04833 avg_loss = 3.53203\n",
      "epoch no.2 train no.244590  loss = 3.27190 avg_loss = 3.50645\n",
      "epoch no.2 train no.244600  loss = 4.20193 avg_loss = 3.52074\n",
      "epoch no.2 train no.244610  loss = 4.60823 avg_loss = 3.53629\n",
      "epoch no.2 train no.244620  loss = 2.26372 avg_loss = 3.54940\n",
      "epoch no.2 train no.244630  loss = 2.73896 avg_loss = 3.57925\n",
      "epoch no.2 train no.244640  loss = 4.38051 avg_loss = 3.62193\n",
      "epoch no.2 train no.244650  loss = 2.37991 avg_loss = 3.58906\n",
      "epoch no.2 train no.244660  loss = 2.27265 avg_loss = 3.56118\n",
      "epoch no.2 train no.244670  loss = 3.33894 avg_loss = 3.58714\n",
      "epoch no.2 train no.244680  loss = 3.25216 avg_loss = 3.66124\n",
      "epoch no.2 train no.244690  loss = 2.83119 avg_loss = 3.60644\n",
      "epoch no.2 train no.244700  loss = 3.36812 avg_loss = 3.56656\n",
      "epoch no.2 train no.244710  loss = 4.05518 avg_loss = 3.53139\n",
      "epoch no.2 train no.244720  loss = 3.16236 avg_loss = 3.54487\n",
      "epoch no.2 train no.244730  loss = 2.62989 avg_loss = 3.52001\n",
      "epoch no.2 train no.244740  loss = 1.63997 avg_loss = 3.51383\n",
      "epoch no.2 train no.244750  loss = 4.48181 avg_loss = 3.53068\n",
      "epoch no.2 train no.244760  loss = 2.24491 avg_loss = 3.50203\n",
      "epoch no.2 train no.244770  loss = 3.10465 avg_loss = 3.50153\n",
      "epoch no.2 train no.244780  loss = 4.70691 avg_loss = 3.48793\n",
      "epoch no.2 train no.244790  loss = 2.88344 avg_loss = 3.51893\n",
      "epoch no.2 train no.244800  loss = 4.41664 avg_loss = 3.52518\n",
      "epoch no.2 train no.244810  loss = 3.31566 avg_loss = 3.54046\n",
      "epoch no.2 train no.244820  loss = 3.22135 avg_loss = 3.57646\n",
      "epoch no.2 train no.244830  loss = 3.96397 avg_loss = 3.56393\n",
      "epoch no.2 train no.244840  loss = 3.55667 avg_loss = 3.59013\n",
      "epoch no.2 train no.244850  loss = 4.44620 avg_loss = 3.60563\n",
      "epoch no.2 train no.244860  loss = 5.39725 avg_loss = 3.62674\n",
      "epoch no.2 train no.244870  loss = 5.95692 avg_loss = 3.73294\n",
      "epoch no.2 train no.244880  loss = 5.08704 avg_loss = 3.72772\n",
      "epoch no.2 train no.244890  loss = 4.02020 avg_loss = 3.76949\n",
      "epoch no.2 train no.244900  loss = 2.75764 avg_loss = 3.77999\n",
      "epoch no.2 train no.244910  loss = 3.79615 avg_loss = 3.77644\n",
      "epoch no.2 train no.244920  loss = 5.88401 avg_loss = 3.79262\n",
      "epoch no.2 train no.244930  loss = 6.29405 avg_loss = 3.82035\n",
      "epoch no.2 train no.244940  loss = 3.69289 avg_loss = 3.78632\n",
      "epoch no.2 train no.244950  loss = 4.59009 avg_loss = 3.79389\n",
      "epoch no.2 train no.244960  loss = 5.34952 avg_loss = 3.81702\n",
      "epoch no.2 train no.244970  loss = 2.65282 avg_loss = 3.83047\n",
      "epoch no.2 train no.244980  loss = 3.08507 avg_loss = 3.79953\n",
      "epoch no.2 train no.244990  loss = 2.95522 avg_loss = 3.78647\n",
      "epoch no.2 train no.245000  loss = 2.66358 avg_loss = 3.76301\n",
      "4\n",
      "to_tokens: ['▁가을', '전환', '에', '▁딱', '▁노래', '</s>', '</s>']\n",
      "기분전환에 좋은 팝송</s>\n",
      "epoch no.2 train no.245010  loss = 6.01683 avg_loss = 3.81568\n",
      "epoch no.2 train no.245020  loss = 4.83807 avg_loss = 3.84853\n",
      "epoch no.2 train no.245030  loss = 4.84649 avg_loss = 3.83860\n",
      "epoch no.2 train no.245040  loss = 2.81455 avg_loss = 3.79082\n",
      "epoch no.2 train no.245050  loss = 2.91745 avg_loss = 3.78331\n",
      "epoch no.2 train no.245060  loss = 2.71936 avg_loss = 3.77061\n",
      "epoch no.2 train no.245070  loss = 3.24167 avg_loss = 3.75978\n",
      "epoch no.2 train no.245080  loss = 4.12397 avg_loss = 3.72683\n",
      "epoch no.2 train no.245090  loss = 4.04200 avg_loss = 3.70531\n",
      "epoch no.2 train no.245100  loss = 2.97947 avg_loss = 3.71142\n",
      "epoch no.2 train no.245110  loss = 4.93532 avg_loss = 3.69055\n",
      "epoch no.2 train no.245120  loss = 4.00048 avg_loss = 3.65422\n",
      "epoch no.2 train no.245130  loss = 3.48721 avg_loss = 3.65795\n",
      "epoch no.2 train no.245140  loss = 4.06946 avg_loss = 3.66042\n",
      "epoch no.2 train no.245150  loss = 4.56505 avg_loss = 3.66065\n",
      "epoch no.2 train no.245160  loss = 2.95339 avg_loss = 3.60885\n",
      "epoch no.2 train no.245170  loss = 4.06787 avg_loss = 3.61626\n",
      "epoch no.2 train no.245180  loss = 3.06294 avg_loss = 3.61307\n",
      "epoch no.2 train no.245190  loss = 2.90401 avg_loss = 3.65250\n",
      "epoch no.2 train no.245200  loss = 3.93998 avg_loss = 3.61060\n",
      "epoch no.2 train no.245210  loss = 3.02576 avg_loss = 3.54755\n",
      "epoch no.2 train no.245220  loss = 3.96705 avg_loss = 3.54328\n",
      "epoch no.2 train no.245230  loss = 3.11940 avg_loss = 3.50888\n",
      "epoch no.2 train no.245240  loss = 3.68034 avg_loss = 3.48653\n",
      "epoch no.2 train no.245250  loss = 5.17630 avg_loss = 3.53133\n",
      "epoch no.2 train no.245260  loss = 3.52864 avg_loss = 3.54216\n",
      "epoch no.2 train no.245270  loss = 3.68615 avg_loss = 3.58103\n",
      "epoch no.2 train no.245280  loss = 1.21452 avg_loss = 3.60325\n",
      "epoch no.2 train no.245290  loss = 3.57747 avg_loss = 3.59697\n",
      "epoch no.2 train no.245300  loss = 5.13862 avg_loss = 3.61814\n",
      "epoch no.2 train no.245310  loss = 3.50144 avg_loss = 3.58016\n",
      "epoch no.2 train no.245320  loss = 3.68757 avg_loss = 3.58331\n",
      "epoch no.2 train no.245330  loss = 3.27655 avg_loss = 3.55971\n",
      "epoch no.2 train no.245340  loss = 3.09504 avg_loss = 3.64558\n",
      "epoch no.2 train no.245350  loss = 3.62981 avg_loss = 3.62840\n",
      "epoch no.2 train no.245360  loss = 2.33327 avg_loss = 3.59976\n",
      "epoch no.2 train no.245370  loss = 3.24554 avg_loss = 3.63428\n",
      "epoch no.2 train no.245380  loss = 4.44030 avg_loss = 3.66115\n",
      "epoch no.2 train no.245390  loss = 5.12319 avg_loss = 3.68770\n",
      "epoch no.2 train no.245400  loss = 3.06375 avg_loss = 3.64946\n",
      "epoch no.2 train no.245410  loss = 3.68188 avg_loss = 3.67370\n",
      "epoch no.2 train no.245420  loss = 2.09106 avg_loss = 3.67424\n",
      "epoch no.2 train no.245430  loss = 5.93456 avg_loss = 3.67690\n",
      "epoch no.2 train no.245440  loss = 4.41684 avg_loss = 3.68192\n",
      "epoch no.2 train no.245450  loss = 3.38108 avg_loss = 3.67237\n",
      "epoch no.2 train no.245460  loss = 3.37046 avg_loss = 3.66838\n",
      "epoch no.2 train no.245470  loss = 2.80907 avg_loss = 3.66140\n",
      "epoch no.2 train no.245480  loss = 3.24911 avg_loss = 3.68460\n",
      "epoch no.2 train no.245490  loss = 3.61282 avg_loss = 3.67359\n",
      "epoch no.2 train no.245500  loss = 3.84168 avg_loss = 3.68635\n",
      "epoch no.2 train no.245510  loss = 3.17338 avg_loss = 3.70829\n",
      "epoch no.2 train no.245520  loss = 4.19418 avg_loss = 3.66397\n",
      "epoch no.2 train no.245530  loss = 6.47609 avg_loss = 3.71662\n",
      "epoch no.2 train no.245540  loss = 6.63368 avg_loss = 3.73608\n",
      "epoch no.2 train no.245550  loss = 4.33752 avg_loss = 3.73802\n",
      "epoch no.2 train no.245560  loss = 2.22752 avg_loss = 3.74949\n",
      "epoch no.2 train no.245570  loss = 0.83534 avg_loss = 3.69028\n",
      "epoch no.2 train no.245580  loss = 2.36245 avg_loss = 3.68842\n",
      "epoch no.2 train no.245590  loss = 4.31837 avg_loss = 3.69356\n",
      "epoch no.2 train no.245600  loss = 2.34973 avg_loss = 3.60388\n",
      "epoch no.2 train no.245610  loss = 3.48028 avg_loss = 3.59093\n",
      "epoch no.2 train no.245620  loss = 4.08609 avg_loss = 3.59828\n",
      "epoch no.2 train no.245630  loss = 3.68869 avg_loss = 3.55329\n",
      "epoch no.2 train no.245640  loss = 3.79610 avg_loss = 3.56444\n",
      "epoch no.2 train no.245650  loss = 3.50364 avg_loss = 3.53968\n",
      "epoch no.2 train no.245660  loss = 3.77557 avg_loss = 3.52235\n",
      "epoch no.2 train no.245670  loss = 4.40492 avg_loss = 3.52837\n",
      "epoch no.2 train no.245680  loss = 2.24531 avg_loss = 3.48966\n",
      "epoch no.2 train no.245690  loss = 2.48163 avg_loss = 3.59524\n",
      "epoch no.2 train no.245700  loss = 3.17712 avg_loss = 3.57938\n",
      "epoch no.2 train no.245710  loss = 2.98285 avg_loss = 3.62531\n",
      "epoch no.2 train no.245720  loss = 3.76879 avg_loss = 3.65319\n",
      "epoch no.2 train no.245730  loss = 2.63293 avg_loss = 3.67330\n",
      "epoch no.2 train no.245740  loss = 3.25762 avg_loss = 3.66666\n",
      "epoch no.2 train no.245750  loss = 3.59240 avg_loss = 3.73457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.245760  loss = 4.65499 avg_loss = 3.72524\n",
      "epoch no.2 train no.245770  loss = 3.67486 avg_loss = 3.70352\n",
      "epoch no.2 train no.245780  loss = 3.49554 avg_loss = 3.64073\n",
      "epoch no.2 train no.245790  loss = 3.28709 avg_loss = 3.64091\n",
      "epoch no.2 train no.245800  loss = 3.00916 avg_loss = 3.62504\n",
      "epoch no.2 train no.245810  loss = 3.52556 avg_loss = 3.59971\n",
      "epoch no.2 train no.245820  loss = 2.66194 avg_loss = 3.69693\n",
      "epoch no.2 train no.245830  loss = 4.03964 avg_loss = 3.68497\n",
      "epoch no.2 train no.245840  loss = 5.72326 avg_loss = 3.68041\n",
      "epoch no.2 train no.245850  loss = 2.05410 avg_loss = 3.65712\n",
      "epoch no.2 train no.245860  loss = 3.50920 avg_loss = 3.66773\n",
      "epoch no.2 train no.245870  loss = 5.84740 avg_loss = 3.66178\n",
      "epoch no.2 train no.245880  loss = 2.89174 avg_loss = 3.65724\n",
      "epoch no.2 train no.245890  loss = 3.26343 avg_loss = 3.68983\n",
      "epoch no.2 train no.245900  loss = 4.61437 avg_loss = 3.65766\n",
      "epoch no.2 train no.245910  loss = 2.13262 avg_loss = 3.63251\n",
      "epoch no.2 train no.245920  loss = 3.86848 avg_loss = 3.68046\n",
      "epoch no.2 train no.245930  loss = 4.35328 avg_loss = 3.71768\n",
      "epoch no.2 train no.245940  loss = 2.15952 avg_loss = 3.74913\n",
      "epoch no.2 train no.245950  loss = 2.79563 avg_loss = 3.71822\n",
      "epoch no.2 train no.245960  loss = 6.18289 avg_loss = 3.77251\n",
      "epoch no.2 train no.245970  loss = 4.91348 avg_loss = 3.75320\n",
      "epoch no.2 train no.245980  loss = 3.78164 avg_loss = 3.77389\n",
      "epoch no.2 train no.245990  loss = 5.14170 avg_loss = 3.74475\n",
      "epoch no.2 train no.246000  loss = 3.49179 avg_loss = 3.68922\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁필요할', '▁때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 재즈</s>\n",
      "epoch no.2 train no.246010  loss = 3.80877 avg_loss = 3.72682\n",
      "epoch no.2 train no.246020  loss = 2.78122 avg_loss = 3.72573\n",
      "epoch no.2 train no.246030  loss = 2.47773 avg_loss = 3.69811\n",
      "epoch no.2 train no.246040  loss = 4.33591 avg_loss = 3.71193\n",
      "epoch no.2 train no.246050  loss = 2.15249 avg_loss = 3.65383\n",
      "epoch no.2 train no.246060  loss = 4.02351 avg_loss = 3.63223\n",
      "epoch no.2 train no.246070  loss = 3.36619 avg_loss = 3.64998\n",
      "epoch no.2 train no.246080  loss = 2.81925 avg_loss = 3.62605\n",
      "epoch no.2 train no.246090  loss = 6.24916 avg_loss = 3.69224\n",
      "epoch no.2 train no.246100  loss = 3.05435 avg_loss = 3.70417\n",
      "epoch no.2 train no.246110  loss = 4.14170 avg_loss = 3.72793\n",
      "epoch no.2 train no.246120  loss = 3.05255 avg_loss = 3.72034\n",
      "epoch no.2 train no.246130  loss = 2.74174 avg_loss = 3.73421\n",
      "epoch no.2 train no.246140  loss = 3.82569 avg_loss = 3.68410\n",
      "epoch no.2 train no.246150  loss = 2.44521 avg_loss = 3.62946\n",
      "epoch no.2 train no.246160  loss = 2.62601 avg_loss = 3.61119\n",
      "epoch no.2 train no.246170  loss = 3.93230 avg_loss = 3.58226\n",
      "epoch no.2 train no.246180  loss = 3.01626 avg_loss = 3.58051\n",
      "epoch no.2 train no.246190  loss = 3.51823 avg_loss = 3.55204\n",
      "epoch no.2 train no.246200  loss = 5.28484 avg_loss = 3.58977\n",
      "epoch no.2 train no.246210  loss = 4.21298 avg_loss = 3.63379\n",
      "epoch no.2 train no.246220  loss = 4.73736 avg_loss = 3.67186\n",
      "epoch no.2 train no.246230  loss = 4.22292 avg_loss = 3.65315\n",
      "epoch no.2 train no.246240  loss = 3.70487 avg_loss = 3.69687\n",
      "epoch no.2 train no.246250  loss = 2.96258 avg_loss = 3.67509\n",
      "epoch no.2 train no.246260  loss = 6.06832 avg_loss = 3.70577\n",
      "epoch no.2 train no.246270  loss = 2.02169 avg_loss = 3.68526\n",
      "epoch no.2 train no.246280  loss = 5.89377 avg_loss = 3.68658\n",
      "epoch no.2 train no.246290  loss = 2.68431 avg_loss = 3.68348\n",
      "epoch no.2 train no.246300  loss = 3.14088 avg_loss = 3.65272\n",
      "epoch no.2 train no.246310  loss = 2.80226 avg_loss = 3.65645\n",
      "epoch no.2 train no.246320  loss = 2.44423 avg_loss = 3.67385\n",
      "epoch no.2 train no.246330  loss = 6.23025 avg_loss = 3.74086\n",
      "epoch no.2 train no.246340  loss = 4.62431 avg_loss = 3.73804\n",
      "epoch no.2 train no.246350  loss = 2.48213 avg_loss = 3.72462\n",
      "epoch no.2 train no.246360  loss = 1.87148 avg_loss = 3.73110\n",
      "epoch no.2 train no.246370  loss = 2.97258 avg_loss = 3.70794\n",
      "epoch no.2 train no.246380  loss = 3.77784 avg_loss = 3.73636\n",
      "epoch no.2 train no.246390  loss = 3.96001 avg_loss = 3.76239\n",
      "epoch no.2 train no.246400  loss = 3.83546 avg_loss = 3.74606\n",
      "epoch no.2 train no.246410  loss = 1.86345 avg_loss = 3.67927\n",
      "epoch no.2 train no.246420  loss = 3.74157 avg_loss = 3.61844\n",
      "epoch no.2 train no.246430  loss = 2.27609 avg_loss = 3.62509\n",
      "epoch no.2 train no.246440  loss = 3.08263 avg_loss = 3.60395\n",
      "epoch no.2 train no.246450  loss = 3.36305 avg_loss = 3.57374\n",
      "epoch no.2 train no.246460  loss = 3.37286 avg_loss = 3.55791\n",
      "epoch no.2 train no.246470  loss = 2.94747 avg_loss = 3.58338\n",
      "epoch no.2 train no.246480  loss = 3.63133 avg_loss = 3.58643\n",
      "epoch no.2 train no.246490  loss = 2.73337 avg_loss = 3.56007\n",
      "epoch no.2 train no.246500  loss = 4.64408 avg_loss = 3.62166\n",
      "epoch no.2 train no.246510  loss = 4.05050 avg_loss = 3.64134\n",
      "epoch no.2 train no.246520  loss = 3.00093 avg_loss = 3.64820\n",
      "epoch no.2 train no.246530  loss = 1.81980 avg_loss = 3.58539\n",
      "epoch no.2 train no.246540  loss = 2.55891 avg_loss = 3.60174\n",
      "epoch no.2 train no.246550  loss = 2.51392 avg_loss = 3.54415\n",
      "epoch no.2 train no.246560  loss = 3.27461 avg_loss = 3.49567\n",
      "epoch no.2 train no.246570  loss = 4.11377 avg_loss = 3.54778\n",
      "epoch no.2 train no.246580  loss = 3.24744 avg_loss = 3.57790\n",
      "epoch no.2 train no.246590  loss = 3.13446 avg_loss = 3.57154\n",
      "epoch no.2 train no.246600  loss = 4.17552 avg_loss = 3.61897\n",
      "epoch no.2 train no.246610  loss = 2.89911 avg_loss = 3.63057\n",
      "epoch no.2 train no.246620  loss = 2.64408 avg_loss = 3.58384\n",
      "epoch no.2 train no.246630  loss = 3.68895 avg_loss = 3.57734\n",
      "epoch no.2 train no.246640  loss = 3.75043 avg_loss = 3.58737\n",
      "epoch no.2 train no.246650  loss = 2.70966 avg_loss = 3.59483\n",
      "epoch no.2 train no.246660  loss = 3.36252 avg_loss = 3.58755\n",
      "epoch no.2 train no.246670  loss = 3.16375 avg_loss = 3.59041\n",
      "epoch no.2 train no.246680  loss = 4.32822 avg_loss = 3.59003\n",
      "epoch no.2 train no.246690  loss = 5.50219 avg_loss = 3.68183\n",
      "epoch no.2 train no.246700  loss = 3.43662 avg_loss = 3.71162\n",
      "epoch no.2 train no.246710  loss = 2.53286 avg_loss = 3.69011\n",
      "epoch no.2 train no.246720  loss = 1.95009 avg_loss = 3.69184\n",
      "epoch no.2 train no.246730  loss = 4.47194 avg_loss = 3.64678\n",
      "epoch no.2 train no.246740  loss = 4.88088 avg_loss = 3.61355\n",
      "epoch no.2 train no.246750  loss = 4.73946 avg_loss = 3.61316\n",
      "epoch no.2 train no.246760  loss = 4.13837 avg_loss = 3.65119\n",
      "epoch no.2 train no.246770  loss = 3.50981 avg_loss = 3.63460\n",
      "epoch no.2 train no.246780  loss = 3.17373 avg_loss = 3.64587\n",
      "epoch no.2 train no.246790  loss = 4.87597 avg_loss = 3.66795\n",
      "epoch no.2 train no.246800  loss = 3.48771 avg_loss = 3.63123\n",
      "epoch no.2 train no.246810  loss = 5.30629 avg_loss = 3.64095\n",
      "epoch no.2 train no.246820  loss = 2.74095 avg_loss = 3.64504\n",
      "epoch no.2 train no.246830  loss = 2.91329 avg_loss = 3.63828\n",
      "epoch no.2 train no.246840  loss = 1.91266 avg_loss = 3.59314\n",
      "epoch no.2 train no.246850  loss = 3.92722 avg_loss = 3.56396\n",
      "epoch no.2 train no.246860  loss = 5.07551 avg_loss = 3.59474\n",
      "epoch no.2 train no.246870  loss = 2.11896 avg_loss = 3.62101\n",
      "epoch no.2 train no.246880  loss = 4.28500 avg_loss = 3.67059\n",
      "epoch no.2 train no.246890  loss = 2.70202 avg_loss = 3.64962\n",
      "epoch no.2 train no.246900  loss = 3.94669 avg_loss = 3.63738\n",
      "epoch no.2 train no.246910  loss = 3.10160 avg_loss = 3.58298\n",
      "epoch no.2 train no.246920  loss = 3.65168 avg_loss = 3.58049\n",
      "epoch no.2 train no.246930  loss = 2.89744 avg_loss = 3.59195\n",
      "epoch no.2 train no.246940  loss = 4.34363 avg_loss = 3.57697\n",
      "epoch no.2 train no.246950  loss = 3.68007 avg_loss = 3.57434\n",
      "epoch no.2 train no.246960  loss = 3.73079 avg_loss = 3.58050\n",
      "epoch no.2 train no.246970  loss = 2.54243 avg_loss = 3.53987\n",
      "epoch no.2 train no.246980  loss = 4.38216 avg_loss = 3.48111\n",
      "epoch no.2 train no.246990  loss = 4.39675 avg_loss = 3.50534\n",
      "epoch no.2 train no.247000  loss = 3.21425 avg_loss = 3.49468\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '하기', '▁필요할', '때', '▁듣는', '▁음악', 'op', '</s>']\n",
      "기분전환이 필요할때 듣는 pop</s>\n",
      "epoch no.2 train no.247010  loss = 4.13407 avg_loss = 3.49122\n",
      "epoch no.2 train no.247020  loss = 2.73114 avg_loss = 3.49994\n",
      "epoch no.2 train no.247030  loss = 4.12747 avg_loss = 3.51343\n",
      "epoch no.2 train no.247040  loss = 3.61597 avg_loss = 3.55460\n",
      "epoch no.2 train no.247050  loss = 5.96955 avg_loss = 3.62237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.247060  loss = 4.49366 avg_loss = 3.67490\n",
      "epoch no.2 train no.247070  loss = 3.01824 avg_loss = 3.62916\n",
      "epoch no.2 train no.247080  loss = 3.85703 avg_loss = 3.59627\n",
      "epoch no.2 train no.247090  loss = 3.34956 avg_loss = 3.58942\n",
      "epoch no.2 train no.247100  loss = 5.93101 avg_loss = 3.54959\n",
      "epoch no.2 train no.247110  loss = 2.63877 avg_loss = 3.51709\n",
      "epoch no.2 train no.247120  loss = 2.48148 avg_loss = 3.55836\n",
      "epoch no.2 train no.247130  loss = 2.22819 avg_loss = 3.53486\n",
      "epoch no.2 train no.247140  loss = 2.39175 avg_loss = 3.53791\n",
      "epoch no.2 train no.247150  loss = 3.77103 avg_loss = 3.51653\n",
      "epoch no.2 train no.247160  loss = 2.84085 avg_loss = 3.54671\n",
      "epoch no.2 train no.247170  loss = 1.70927 avg_loss = 3.55774\n",
      "epoch no.2 train no.247180  loss = 4.44997 avg_loss = 3.57799\n",
      "epoch no.2 train no.247190  loss = 3.90874 avg_loss = 3.62512\n",
      "epoch no.2 train no.247200  loss = 4.22005 avg_loss = 3.60877\n",
      "epoch no.2 train no.247210  loss = 2.86407 avg_loss = 3.63290\n",
      "epoch no.2 train no.247220  loss = 3.44208 avg_loss = 3.60392\n",
      "epoch no.2 train no.247230  loss = 3.59689 avg_loss = 3.63001\n",
      "epoch no.2 train no.247240  loss = 2.52255 avg_loss = 3.57800\n",
      "epoch no.2 train no.247250  loss = 6.10846 avg_loss = 3.56402\n",
      "epoch no.2 train no.247260  loss = 3.30857 avg_loss = 3.62786\n",
      "epoch no.2 train no.247270  loss = 3.92011 avg_loss = 3.63064\n",
      "epoch no.2 train no.247280  loss = 3.05172 avg_loss = 3.60985\n",
      "epoch no.2 train no.247290  loss = 3.27200 avg_loss = 3.65636\n",
      "epoch no.2 train no.247300  loss = 4.71529 avg_loss = 3.67191\n",
      "epoch no.2 train no.247310  loss = 4.34705 avg_loss = 3.63328\n",
      "epoch no.2 train no.247320  loss = 3.20170 avg_loss = 3.60857\n",
      "epoch no.2 train no.247330  loss = 2.47464 avg_loss = 3.65436\n",
      "epoch no.2 train no.247340  loss = 4.64848 avg_loss = 3.68512\n",
      "epoch no.2 train no.247350  loss = 3.23346 avg_loss = 3.70947\n",
      "epoch no.2 train no.247360  loss = 4.91528 avg_loss = 3.74374\n",
      "epoch no.2 train no.247370  loss = 4.43133 avg_loss = 3.74458\n",
      "epoch no.2 train no.247380  loss = 3.25291 avg_loss = 3.69160\n",
      "epoch no.2 train no.247390  loss = 5.20148 avg_loss = 3.69741\n",
      "epoch no.2 train no.247400  loss = 4.28066 avg_loss = 3.74887\n",
      "epoch no.2 train no.247410  loss = 3.33346 avg_loss = 3.76111\n",
      "epoch no.2 train no.247420  loss = 2.96910 avg_loss = 3.73670\n",
      "epoch no.2 train no.247430  loss = 5.39074 avg_loss = 3.83514\n",
      "epoch no.2 train no.247440  loss = 4.24052 avg_loss = 3.79363\n",
      "epoch no.2 train no.247450  loss = 4.63451 avg_loss = 3.73907\n",
      "epoch no.2 train no.247460  loss = 4.41250 avg_loss = 3.75876\n",
      "epoch no.2 train no.247470  loss = 3.24691 avg_loss = 3.68975\n",
      "epoch no.2 train no.247480  loss = 2.49653 avg_loss = 3.64316\n",
      "epoch no.2 train no.247490  loss = 2.71535 avg_loss = 3.66857\n",
      "epoch no.2 train no.247500  loss = 4.01315 avg_loss = 3.73676\n",
      "epoch no.2 train no.247510  loss = 4.19018 avg_loss = 3.71257\n",
      "epoch no.2 train no.247520  loss = 2.90500 avg_loss = 3.73233\n",
      "epoch no.2 train no.247530  loss = 2.40494 avg_loss = 3.73756\n",
      "epoch no.2 train no.247540  loss = 4.16365 avg_loss = 3.77045\n",
      "epoch no.2 train no.247550  loss = 2.74839 avg_loss = 3.74348\n",
      "epoch no.2 train no.247560  loss = 2.29919 avg_loss = 3.70468\n",
      "epoch no.2 train no.247570  loss = 3.00670 avg_loss = 3.66355\n",
      "epoch no.2 train no.247580  loss = 4.12884 avg_loss = 3.62611\n",
      "epoch no.2 train no.247590  loss = 2.57917 avg_loss = 3.62031\n",
      "epoch no.2 train no.247600  loss = 3.16701 avg_loss = 3.64586\n",
      "epoch no.2 train no.247610  loss = 3.53178 avg_loss = 3.62166\n",
      "epoch no.2 train no.247620  loss = 6.07077 avg_loss = 3.65687\n",
      "epoch no.2 train no.247630  loss = 3.90222 avg_loss = 3.63907\n",
      "epoch no.2 train no.247640  loss = 1.70843 avg_loss = 3.60012\n",
      "epoch no.2 train no.247650  loss = 3.63699 avg_loss = 3.62946\n",
      "epoch no.2 train no.247660  loss = 3.38716 avg_loss = 3.65398\n",
      "epoch no.2 train no.247670  loss = 4.48617 avg_loss = 3.65186\n",
      "epoch no.2 train no.247680  loss = 4.31400 avg_loss = 3.63881\n",
      "epoch no.2 train no.247690  loss = 1.73887 avg_loss = 3.60212\n",
      "epoch no.2 train no.247700  loss = 3.49459 avg_loss = 3.59040\n",
      "epoch no.2 train no.247710  loss = 3.73216 avg_loss = 3.59572\n",
      "epoch no.2 train no.247720  loss = 3.22109 avg_loss = 3.62679\n",
      "epoch no.2 train no.247730  loss = 3.23803 avg_loss = 3.65677\n",
      "epoch no.2 train no.247740  loss = 1.85285 avg_loss = 3.64626\n",
      "epoch no.2 train no.247750  loss = 4.37332 avg_loss = 3.60952\n",
      "epoch no.2 train no.247760  loss = 4.22223 avg_loss = 3.58871\n",
      "epoch no.2 train no.247770  loss = 3.35695 avg_loss = 3.61186\n",
      "epoch no.2 train no.247780  loss = 3.34999 avg_loss = 3.64450\n",
      "epoch no.2 train no.247790  loss = 2.35744 avg_loss = 3.62890\n",
      "epoch no.2 train no.247800  loss = 5.09091 avg_loss = 3.58581\n",
      "epoch no.2 train no.247810  loss = 2.79005 avg_loss = 3.58173\n",
      "epoch no.2 train no.247820  loss = 3.23544 avg_loss = 3.60294\n",
      "epoch no.2 train no.247830  loss = 2.92508 avg_loss = 3.58926\n",
      "epoch no.2 train no.247840  loss = 4.73182 avg_loss = 3.58189\n",
      "epoch no.2 train no.247850  loss = 4.02795 avg_loss = 3.56437\n",
      "epoch no.2 train no.247860  loss = 4.40446 avg_loss = 3.56432\n",
      "epoch no.2 train no.247870  loss = 2.53083 avg_loss = 3.55171\n",
      "epoch no.2 train no.247880  loss = 1.81704 avg_loss = 3.54733\n",
      "epoch no.2 train no.247890  loss = 2.20861 avg_loss = 3.54296\n",
      "epoch no.2 train no.247900  loss = 3.88323 avg_loss = 3.56527\n",
      "epoch no.2 train no.247910  loss = 3.16409 avg_loss = 3.52140\n",
      "epoch no.2 train no.247920  loss = 3.43043 avg_loss = 3.49947\n",
      "epoch no.2 train no.247930  loss = 2.38780 avg_loss = 3.45131\n",
      "epoch no.2 train no.247940  loss = 2.31763 avg_loss = 3.48762\n",
      "epoch no.2 train no.247950  loss = 4.58775 avg_loss = 3.52825\n",
      "epoch no.2 train no.247960  loss = 2.69037 avg_loss = 3.56020\n",
      "epoch no.2 train no.247970  loss = 3.41193 avg_loss = 3.54306\n",
      "epoch no.2 train no.247980  loss = 4.54935 avg_loss = 3.52974\n",
      "epoch no.2 train no.247990  loss = 7.66353 avg_loss = 3.60333\n",
      "epoch no.2 train no.248000  loss = 2.35583 avg_loss = 3.56649\n",
      "6\n",
      "to_tokens: ['▁가을', '전환', '에', '▁필요할', '때', '</s>', '▁신나는', '에이지', '</s>']\n",
      "기분전환이 필요할때 듣는 뉴에이지</s>\n",
      "epoch no.2 train no.248010  loss = 3.26977 avg_loss = 3.55223\n",
      "epoch no.2 train no.248020  loss = 3.46985 avg_loss = 3.55745\n",
      "epoch no.2 train no.248030  loss = 5.67261 avg_loss = 3.60532\n",
      "epoch no.2 train no.248040  loss = 5.00628 avg_loss = 3.59836\n",
      "epoch no.2 train no.248050  loss = 3.56371 avg_loss = 3.61062\n",
      "epoch no.2 train no.248060  loss = 3.23237 avg_loss = 3.63146\n",
      "epoch no.2 train no.248070  loss = 2.68737 avg_loss = 3.64398\n",
      "epoch no.2 train no.248080  loss = 1.96231 avg_loss = 3.66158\n",
      "epoch no.2 train no.248090  loss = 5.04867 avg_loss = 3.66044\n",
      "epoch no.2 train no.248100  loss = 3.07910 avg_loss = 3.68081\n",
      "epoch no.2 train no.248110  loss = 4.55633 avg_loss = 3.66190\n",
      "epoch no.2 train no.248120  loss = 4.62656 avg_loss = 3.65605\n",
      "epoch no.2 train no.248130  loss = 3.17014 avg_loss = 3.66086\n",
      "epoch no.2 train no.248140  loss = 2.91111 avg_loss = 3.65317\n",
      "epoch no.2 train no.248150  loss = 3.18415 avg_loss = 3.70009\n",
      "epoch no.2 train no.248160  loss = 2.66157 avg_loss = 3.65708\n",
      "epoch no.2 train no.248170  loss = 2.14577 avg_loss = 3.66335\n",
      "epoch no.2 train no.248180  loss = 2.04116 avg_loss = 3.63783\n",
      "epoch no.2 train no.248190  loss = 4.38764 avg_loss = 3.63662\n",
      "epoch no.2 train no.248200  loss = 4.05402 avg_loss = 3.58673\n",
      "epoch no.2 train no.248210  loss = 4.18547 avg_loss = 3.60417\n",
      "epoch no.2 train no.248220  loss = 2.88255 avg_loss = 3.60770\n",
      "epoch no.2 train no.248230  loss = 4.67861 avg_loss = 3.61884\n",
      "epoch no.2 train no.248240  loss = 2.80998 avg_loss = 3.66822\n",
      "epoch no.2 train no.248250  loss = 5.51904 avg_loss = 3.73351\n",
      "epoch no.2 train no.248260  loss = 4.10081 avg_loss = 3.72329\n",
      "epoch no.2 train no.248270  loss = 3.04805 avg_loss = 3.68235\n",
      "epoch no.2 train no.248280  loss = 4.37144 avg_loss = 3.68056\n",
      "epoch no.2 train no.248290  loss = 3.71718 avg_loss = 3.65853\n",
      "epoch no.2 train no.248300  loss = 3.34122 avg_loss = 3.68498\n",
      "epoch no.2 train no.248310  loss = 1.12716 avg_loss = 3.60881\n",
      "epoch no.2 train no.248320  loss = 6.37017 avg_loss = 3.64793\n",
      "epoch no.2 train no.248330  loss = 4.83740 avg_loss = 3.72010\n",
      "epoch no.2 train no.248340  loss = 3.79130 avg_loss = 3.70680\n",
      "epoch no.2 train no.248350  loss = 4.07170 avg_loss = 3.72371\n",
      "epoch no.2 train no.248360  loss = 3.28988 avg_loss = 3.73308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.248370  loss = 3.13554 avg_loss = 3.75146\n",
      "epoch no.2 train no.248380  loss = 2.96719 avg_loss = 3.77424\n",
      "epoch no.2 train no.248390  loss = 4.73570 avg_loss = 3.77569\n",
      "epoch no.2 train no.248400  loss = 2.26976 avg_loss = 3.72153\n",
      "epoch no.2 train no.248410  loss = 7.27666 avg_loss = 3.73521\n",
      "epoch no.2 train no.248420  loss = 3.37760 avg_loss = 3.70835\n",
      "epoch no.2 train no.248430  loss = 4.33985 avg_loss = 3.73244\n",
      "epoch no.2 train no.248440  loss = 4.97194 avg_loss = 3.81925\n",
      "epoch no.2 train no.248450  loss = 2.79104 avg_loss = 3.78942\n",
      "epoch no.2 train no.248460  loss = 2.75981 avg_loss = 3.74626\n",
      "epoch no.2 train no.248470  loss = 2.19473 avg_loss = 3.75057\n",
      "epoch no.2 train no.248480  loss = 3.03746 avg_loss = 3.71736\n",
      "epoch no.2 train no.248490  loss = 3.54958 avg_loss = 3.72022\n",
      "epoch no.2 train no.248500  loss = 3.27590 avg_loss = 3.71717\n",
      "epoch no.2 train no.248510  loss = 6.72434 avg_loss = 3.70321\n",
      "epoch no.2 train no.248520  loss = 3.95544 avg_loss = 3.72686\n",
      "epoch no.2 train no.248530  loss = 5.27580 avg_loss = 3.77033\n",
      "epoch no.2 train no.248540  loss = 4.69353 avg_loss = 3.77627\n",
      "epoch no.2 train no.248550  loss = 5.08160 avg_loss = 3.73558\n",
      "epoch no.2 train no.248560  loss = 2.44815 avg_loss = 3.71882\n",
      "epoch no.2 train no.248570  loss = 3.72386 avg_loss = 3.66853\n",
      "epoch no.2 train no.248580  loss = 4.97940 avg_loss = 3.69382\n",
      "epoch no.2 train no.248590  loss = 2.61485 avg_loss = 3.65138\n",
      "epoch no.2 train no.248600  loss = 3.61680 avg_loss = 3.62647\n",
      "epoch no.2 train no.248610  loss = 3.02822 avg_loss = 3.60988\n",
      "epoch no.2 train no.248620  loss = 5.25637 avg_loss = 3.64036\n",
      "epoch no.2 train no.248630  loss = 6.36491 avg_loss = 3.72363\n",
      "epoch no.2 train no.248640  loss = 1.96076 avg_loss = 3.75548\n",
      "epoch no.2 train no.248650  loss = 4.78959 avg_loss = 3.72801\n",
      "epoch no.2 train no.248660  loss = 3.73906 avg_loss = 3.71364\n",
      "epoch no.2 train no.248670  loss = 2.81060 avg_loss = 3.68491\n",
      "epoch no.2 train no.248680  loss = 2.63097 avg_loss = 3.64688\n",
      "epoch no.2 train no.248690  loss = 2.87708 avg_loss = 3.66102\n",
      "epoch no.2 train no.248700  loss = 1.95178 avg_loss = 3.64370\n",
      "epoch no.2 train no.248710  loss = 3.41623 avg_loss = 3.64830\n",
      "epoch no.2 train no.248720  loss = 2.89182 avg_loss = 3.58121\n",
      "epoch no.2 train no.248730  loss = 2.16186 avg_loss = 3.62577\n",
      "epoch no.2 train no.248740  loss = 3.72645 avg_loss = 3.62700\n",
      "epoch no.2 train no.248750  loss = 5.19757 avg_loss = 3.67949\n",
      "epoch no.2 train no.248760  loss = 3.94391 avg_loss = 3.67606\n",
      "epoch no.2 train no.248770  loss = 1.99644 avg_loss = 3.63706\n",
      "epoch no.2 train no.248780  loss = 2.37387 avg_loss = 3.62435\n",
      "epoch no.2 train no.248790  loss = 3.05768 avg_loss = 3.64419\n",
      "epoch no.2 train no.248800  loss = 3.71355 avg_loss = 3.67844\n",
      "epoch no.2 train no.248810  loss = 4.11442 avg_loss = 3.70473\n",
      "epoch no.2 train no.248820  loss = 4.52060 avg_loss = 3.70389\n",
      "epoch no.2 train no.248830  loss = 4.57135 avg_loss = 3.70335\n",
      "epoch no.2 train no.248840  loss = 3.78642 avg_loss = 3.71044\n",
      "epoch no.2 train no.248850  loss = 3.21549 avg_loss = 3.66358\n",
      "epoch no.2 train no.248860  loss = 4.03426 avg_loss = 3.71693\n",
      "epoch no.2 train no.248870  loss = 3.12519 avg_loss = 3.71294\n",
      "epoch no.2 train no.248880  loss = 2.25266 avg_loss = 3.67027\n",
      "epoch no.2 train no.248890  loss = 3.65262 avg_loss = 3.67250\n",
      "epoch no.2 train no.248900  loss = 4.98336 avg_loss = 3.64421\n",
      "epoch no.2 train no.248910  loss = 3.33008 avg_loss = 3.63571\n",
      "epoch no.2 train no.248920  loss = 4.15603 avg_loss = 3.61520\n",
      "epoch no.2 train no.248930  loss = 2.27745 avg_loss = 3.54955\n",
      "epoch no.2 train no.248940  loss = 3.57731 avg_loss = 3.53989\n",
      "epoch no.2 train no.248950  loss = 4.11046 avg_loss = 3.56770\n",
      "epoch no.2 train no.248960  loss = 3.57364 avg_loss = 3.57426\n",
      "epoch no.2 train no.248970  loss = 3.92841 avg_loss = 3.54573\n",
      "epoch no.2 train no.248980  loss = 2.78241 avg_loss = 3.56317\n",
      "epoch no.2 train no.248990  loss = 4.41007 avg_loss = 3.54551\n",
      "epoch no.2 train no.249000  loss = 3.77944 avg_loss = 3.56138\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.2 train no.249010  loss = 3.89040 avg_loss = 3.57996\n",
      "epoch no.2 train no.249020  loss = 3.24073 avg_loss = 3.54984\n",
      "epoch no.2 train no.249030  loss = 4.08648 avg_loss = 3.57869\n",
      "epoch no.2 train no.249040  loss = 2.80106 avg_loss = 3.58895\n",
      "epoch no.2 train no.249050  loss = 3.43620 avg_loss = 3.62713\n",
      "epoch no.2 train no.249060  loss = 3.01948 avg_loss = 3.61508\n",
      "epoch no.2 train no.249070  loss = 3.13262 avg_loss = 3.59981\n",
      "epoch no.2 train no.249080  loss = 5.77581 avg_loss = 3.60931\n",
      "epoch no.2 train no.249090  loss = 5.94056 avg_loss = 3.61396\n",
      "epoch no.2 train no.249100  loss = 4.13580 avg_loss = 3.61715\n",
      "epoch no.2 train no.249110  loss = 3.41608 avg_loss = 3.62153\n",
      "epoch no.2 train no.249120  loss = 2.99587 avg_loss = 3.63759\n",
      "epoch no.2 train no.249130  loss = 3.91531 avg_loss = 3.67073\n",
      "epoch no.2 train no.249140  loss = 4.79271 avg_loss = 3.70599\n",
      "epoch no.2 train no.249150  loss = 2.67311 avg_loss = 3.73027\n",
      "epoch no.2 train no.249160  loss = 3.49443 avg_loss = 3.72286\n",
      "epoch no.2 train no.249170  loss = 3.48777 avg_loss = 3.75393\n",
      "epoch no.2 train no.249180  loss = 2.39009 avg_loss = 3.71859\n",
      "epoch no.2 train no.249190  loss = 3.81012 avg_loss = 3.68127\n",
      "epoch no.2 train no.249200  loss = 3.32482 avg_loss = 3.65161\n",
      "epoch no.2 train no.249210  loss = 2.66197 avg_loss = 3.60562\n",
      "epoch no.2 train no.249220  loss = 3.73427 avg_loss = 3.62063\n",
      "epoch no.2 train no.249230  loss = 3.57113 avg_loss = 3.67252\n",
      "epoch no.2 train no.249240  loss = 4.88060 avg_loss = 3.69497\n",
      "epoch no.2 train no.249250  loss = 5.27491 avg_loss = 3.74547\n",
      "epoch no.2 train no.249260  loss = 5.91546 avg_loss = 3.76048\n",
      "epoch no.2 train no.249270  loss = 3.57717 avg_loss = 3.71779\n",
      "epoch no.2 train no.249280  loss = 3.91508 avg_loss = 3.72870\n",
      "epoch no.2 train no.249290  loss = 4.44825 avg_loss = 3.74513\n",
      "epoch no.2 train no.249300  loss = 3.00175 avg_loss = 3.71801\n",
      "epoch no.2 train no.249310  loss = 3.97229 avg_loss = 3.74385\n",
      "epoch no.2 train no.249320  loss = 3.46520 avg_loss = 3.74019\n",
      "epoch no.2 train no.249330  loss = 5.67125 avg_loss = 3.74558\n",
      "epoch no.2 train no.249340  loss = 1.79422 avg_loss = 3.70088\n",
      "epoch no.2 train no.249350  loss = 2.90710 avg_loss = 3.66417\n",
      "epoch no.2 train no.249360  loss = 2.86986 avg_loss = 3.70508\n",
      "epoch no.2 train no.249370  loss = 3.76517 avg_loss = 3.74037\n",
      "epoch no.2 train no.249380  loss = 5.47904 avg_loss = 3.72620\n",
      "epoch no.2 train no.249390  loss = 2.21760 avg_loss = 3.73506\n",
      "epoch no.2 train no.249400  loss = 4.61348 avg_loss = 3.74279\n",
      "epoch no.2 train no.249410  loss = 4.50112 avg_loss = 3.71602\n",
      "epoch no.2 train no.249420  loss = 2.88061 avg_loss = 3.80344\n",
      "epoch no.2 train no.249430  loss = 3.40383 avg_loss = 3.81464\n",
      "epoch no.2 train no.249440  loss = 4.36081 avg_loss = 3.81734\n",
      "epoch no.2 train no.249450  loss = 3.77334 avg_loss = 3.83637\n",
      "epoch no.2 train no.249460  loss = 3.16293 avg_loss = 3.83510\n",
      "epoch no.2 train no.249470  loss = 2.41438 avg_loss = 3.77293\n",
      "epoch no.2 train no.249480  loss = 5.00712 avg_loss = 3.74701\n",
      "epoch no.2 train no.249490  loss = 3.15821 avg_loss = 3.75159\n",
      "epoch no.2 train no.249500  loss = 2.45491 avg_loss = 3.70955\n",
      "epoch no.2 train no.249510  loss = 4.04708 avg_loss = 3.68945\n",
      "epoch no.2 train no.249520  loss = 2.30823 avg_loss = 3.70058\n",
      "epoch no.2 train no.249530  loss = 3.95894 avg_loss = 3.74736\n",
      "epoch no.2 train no.249540  loss = 2.24291 avg_loss = 3.71407\n",
      "epoch no.2 train no.249550  loss = 4.71411 avg_loss = 3.74898\n",
      "epoch no.2 train no.249560  loss = 3.31544 avg_loss = 3.74153\n",
      "epoch no.2 train no.249570  loss = 4.42631 avg_loss = 3.68194\n",
      "epoch no.2 train no.249580  loss = 3.06239 avg_loss = 3.70649\n",
      "epoch no.2 train no.249590  loss = 3.97687 avg_loss = 3.66248\n",
      "epoch no.2 train no.249600  loss = 3.27676 avg_loss = 3.65924\n",
      "epoch no.2 train no.249610  loss = 3.73884 avg_loss = 3.63919\n",
      "epoch no.2 train no.249620  loss = 3.63838 avg_loss = 3.69934\n",
      "epoch no.2 train no.249630  loss = 4.80165 avg_loss = 3.67472\n",
      "epoch no.2 train no.249640  loss = 1.99475 avg_loss = 3.64271\n",
      "epoch no.2 train no.249650  loss = 2.86874 avg_loss = 3.69948\n",
      "epoch no.2 train no.249660  loss = 3.23030 avg_loss = 3.73273\n",
      "epoch no.2 train no.249670  loss = 2.90736 avg_loss = 3.74224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.249680  loss = 4.32288 avg_loss = 3.72787\n",
      "epoch no.2 train no.249690  loss = 2.74428 avg_loss = 3.66712\n",
      "epoch no.2 train no.249700  loss = 2.53718 avg_loss = 3.60044\n",
      "epoch no.2 train no.249710  loss = 3.69291 avg_loss = 3.57782\n",
      "epoch no.2 train no.249720  loss = 3.27668 avg_loss = 3.54940\n",
      "epoch no.2 train no.249730  loss = 5.35179 avg_loss = 3.60855\n",
      "epoch no.2 train no.249740  loss = 3.06104 avg_loss = 3.59605\n",
      "epoch no.2 train no.249750  loss = 3.71570 avg_loss = 3.67408\n",
      "epoch no.2 train no.249760  loss = 3.64744 avg_loss = 3.62135\n",
      "epoch no.2 train no.249770  loss = 4.04216 avg_loss = 3.63017\n",
      "epoch no.2 train no.249780  loss = 3.28315 avg_loss = 3.59413\n",
      "epoch no.2 train no.249790  loss = 2.01333 avg_loss = 3.53383\n",
      "epoch no.2 train no.249800  loss = 3.93434 avg_loss = 3.56047\n",
      "epoch no.2 train no.249810  loss = 5.26713 avg_loss = 3.60221\n",
      "epoch no.2 train no.249820  loss = 3.58786 avg_loss = 3.58217\n",
      "epoch no.2 train no.249830  loss = 4.38154 avg_loss = 3.61998\n",
      "epoch no.2 train no.249840  loss = 3.52400 avg_loss = 3.60864\n",
      "epoch no.2 train no.249850  loss = 2.66879 avg_loss = 3.60755\n",
      "epoch no.2 train no.249860  loss = 4.48338 avg_loss = 3.59985\n",
      "epoch no.2 train no.249870  loss = 3.49111 avg_loss = 3.69638\n",
      "epoch no.2 train no.249880  loss = 2.05754 avg_loss = 3.70602\n",
      "epoch no.2 train no.249890  loss = 3.30863 avg_loss = 3.69680\n",
      "epoch no.2 train no.249900  loss = 5.72450 avg_loss = 3.76818\n",
      "epoch no.2 train no.249910  loss = 3.13304 avg_loss = 3.75306\n",
      "epoch no.2 train no.249920  loss = 5.34312 avg_loss = 3.73503\n",
      "epoch no.2 train no.249930  loss = 4.26723 avg_loss = 3.68698\n",
      "epoch no.2 train no.249940  loss = 4.27134 avg_loss = 3.76614\n",
      "epoch no.2 train no.249950  loss = 4.00163 avg_loss = 3.81898\n",
      "epoch no.2 train no.249960  loss = 3.37789 avg_loss = 3.79223\n",
      "epoch no.2 train no.249970  loss = 4.18043 avg_loss = 3.76622\n",
      "epoch no.2 train no.249980  loss = 2.94605 avg_loss = 3.75801\n",
      "epoch no.2 train no.249990  loss = 2.49730 avg_loss = 3.78606\n",
      "epoch no.2 train no.250000  loss = 3.34403 avg_loss = 3.72823\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '</s>', '▁노래', '곡', '</s>']\n",
      "기분전환용 신나는 댄스뮤직</s>\n",
      "epoch no.2 train no.250010  loss = 4.02894 avg_loss = 3.72012\n",
      "epoch no.2 train no.250020  loss = 3.53775 avg_loss = 3.72445\n",
      "epoch no.2 train no.250030  loss = 3.55452 avg_loss = 3.72323\n",
      "epoch no.2 train no.250040  loss = 3.54699 avg_loss = 3.66978\n",
      "epoch no.2 train no.250050  loss = 4.61798 avg_loss = 3.69315\n",
      "epoch no.2 train no.250060  loss = 3.96042 avg_loss = 3.70329\n",
      "epoch no.2 train no.250070  loss = 2.77786 avg_loss = 3.66286\n",
      "epoch no.2 train no.250080  loss = 2.72479 avg_loss = 3.74021\n",
      "epoch no.2 train no.250090  loss = 2.39150 avg_loss = 3.68927\n",
      "epoch no.2 train no.250100  loss = 3.48866 avg_loss = 3.63446\n",
      "epoch no.2 train no.250110  loss = 4.43965 avg_loss = 3.62017\n",
      "epoch no.2 train no.250120  loss = 3.94926 avg_loss = 3.62813\n",
      "epoch no.2 train no.250130  loss = 6.23456 avg_loss = 3.65688\n",
      "epoch no.2 train no.250140  loss = 3.74398 avg_loss = 3.72129\n",
      "epoch no.2 train no.250150  loss = 2.22356 avg_loss = 3.68791\n",
      "epoch no.2 train no.250160  loss = 3.07620 avg_loss = 3.71899\n",
      "epoch no.2 train no.250170  loss = 3.63399 avg_loss = 3.70394\n",
      "epoch no.2 train no.250180  loss = 4.25698 avg_loss = 3.73454\n",
      "epoch no.2 train no.250190  loss = 4.81281 avg_loss = 3.80780\n",
      "epoch no.2 train no.250200  loss = 2.49248 avg_loss = 3.75464\n",
      "epoch no.2 train no.250210  loss = 3.20646 avg_loss = 3.72423\n",
      "epoch no.2 train no.250220  loss = 2.59185 avg_loss = 3.74357\n",
      "epoch no.2 train no.250230  loss = 2.87212 avg_loss = 3.70347\n",
      "epoch no.2 train no.250240  loss = 3.26144 avg_loss = 3.67071\n",
      "epoch no.2 train no.250250  loss = 3.01921 avg_loss = 3.64379\n",
      "epoch no.2 train no.250260  loss = 2.38698 avg_loss = 3.70089\n",
      "epoch no.2 train no.250270  loss = 3.08349 avg_loss = 3.72356\n",
      "epoch no.2 train no.250280  loss = 4.50269 avg_loss = 3.75692\n",
      "epoch no.2 train no.250290  loss = 2.78519 avg_loss = 3.73184\n",
      "epoch no.2 train no.250300  loss = 2.79490 avg_loss = 3.70808\n",
      "epoch no.2 train no.250310  loss = 2.99564 avg_loss = 3.65231\n",
      "epoch no.2 train no.250320  loss = 3.39634 avg_loss = 3.61668\n",
      "epoch no.2 train no.250330  loss = 3.90670 avg_loss = 3.65610\n",
      "epoch no.2 train no.250340  loss = 4.61177 avg_loss = 3.62500\n",
      "epoch no.2 train no.250350  loss = 5.27246 avg_loss = 3.73238\n",
      "epoch no.2 train no.250360  loss = 5.17051 avg_loss = 3.79675\n",
      "epoch no.2 train no.250370  loss = 2.05557 avg_loss = 3.74964\n",
      "epoch no.2 train no.250380  loss = 3.28538 avg_loss = 3.76905\n",
      "epoch no.2 train no.250390  loss = 4.26283 avg_loss = 3.72578\n",
      "epoch no.2 train no.250400  loss = 5.75483 avg_loss = 3.73770\n",
      "epoch no.2 train no.250410  loss = 3.92428 avg_loss = 3.72388\n",
      "epoch no.2 train no.250420  loss = 4.38856 avg_loss = 3.76525\n",
      "epoch no.2 train no.250430  loss = 3.27204 avg_loss = 3.78580\n",
      "epoch no.2 train no.250440  loss = 5.69999 avg_loss = 3.81605\n",
      "epoch no.2 train no.250450  loss = 4.50303 avg_loss = 3.74629\n",
      "epoch no.2 train no.250460  loss = 2.91061 avg_loss = 3.74744\n",
      "epoch no.2 train no.250470  loss = 8.35346 avg_loss = 3.70662\n",
      "epoch no.2 train no.250480  loss = 3.20863 avg_loss = 3.67715\n",
      "epoch no.2 train no.250490  loss = 4.60150 avg_loss = 3.68793\n",
      "epoch no.2 train no.250500  loss = 2.73498 avg_loss = 3.70066\n",
      "epoch no.2 train no.250510  loss = 3.70057 avg_loss = 3.72078\n",
      "epoch no.2 train no.250520  loss = 3.49323 avg_loss = 3.66652\n",
      "epoch no.2 train no.250530  loss = 2.68877 avg_loss = 3.61443\n",
      "epoch no.2 train no.250540  loss = 4.53902 avg_loss = 3.66523\n",
      "epoch no.2 train no.250550  loss = 4.06986 avg_loss = 3.65599\n",
      "epoch no.2 train no.250560  loss = 2.05416 avg_loss = 3.63656\n",
      "epoch no.2 train no.250570  loss = 1.90745 avg_loss = 3.63832\n",
      "epoch no.2 train no.250580  loss = 3.20826 avg_loss = 3.65441\n",
      "epoch no.2 train no.250590  loss = 3.71181 avg_loss = 3.66603\n",
      "epoch no.2 train no.250600  loss = 4.20995 avg_loss = 3.65838\n",
      "epoch no.2 train no.250610  loss = 6.91719 avg_loss = 3.65258\n",
      "epoch no.2 train no.250620  loss = 4.21931 avg_loss = 3.71191\n",
      "epoch no.2 train no.250630  loss = 2.53087 avg_loss = 3.68078\n",
      "epoch no.2 train no.250640  loss = 3.21069 avg_loss = 3.60980\n",
      "epoch no.2 train no.250650  loss = 6.13892 avg_loss = 3.62408\n",
      "epoch no.2 train no.250660  loss = 4.96620 avg_loss = 3.67743\n",
      "epoch no.2 train no.250670  loss = 2.32158 avg_loss = 3.61146\n",
      "epoch no.2 train no.250680  loss = 2.96286 avg_loss = 3.55687\n",
      "epoch no.2 train no.250690  loss = 2.62351 avg_loss = 3.52093\n",
      "epoch no.2 train no.250700  loss = 2.99171 avg_loss = 3.56600\n",
      "epoch no.2 train no.250710  loss = 2.68269 avg_loss = 3.57349\n",
      "epoch no.2 train no.250720  loss = 5.71959 avg_loss = 3.54727\n",
      "epoch no.2 train no.250730  loss = 4.00918 avg_loss = 3.50251\n",
      "epoch no.2 train no.250740  loss = 3.07504 avg_loss = 3.51129\n",
      "epoch no.2 train no.250750  loss = 5.17715 avg_loss = 3.55101\n",
      "epoch no.2 train no.250760  loss = 3.61960 avg_loss = 3.57178\n",
      "epoch no.2 train no.250770  loss = 3.73356 avg_loss = 3.60170\n",
      "epoch no.2 train no.250780  loss = 2.49558 avg_loss = 3.57828\n",
      "epoch no.2 train no.250790  loss = 2.93010 avg_loss = 3.60585\n",
      "epoch no.2 train no.250800  loss = 3.79536 avg_loss = 3.60707\n",
      "epoch no.2 train no.250810  loss = 3.18798 avg_loss = 3.53421\n",
      "epoch no.2 train no.250820  loss = 4.30735 avg_loss = 3.53380\n",
      "epoch no.2 train no.250830  loss = 3.09428 avg_loss = 3.52164\n",
      "epoch no.2 train no.250840  loss = 2.89902 avg_loss = 3.52609\n",
      "epoch no.2 train no.250850  loss = 1.79048 avg_loss = 3.52304\n",
      "epoch no.2 train no.250860  loss = 3.33813 avg_loss = 3.52394\n",
      "epoch no.2 train no.250870  loss = 3.54625 avg_loss = 3.57654\n",
      "epoch no.2 train no.250880  loss = 2.45362 avg_loss = 3.51179\n",
      "epoch no.2 train no.250890  loss = 2.45800 avg_loss = 3.56755\n",
      "epoch no.2 train no.250900  loss = 4.73756 avg_loss = 3.56745\n",
      "epoch no.2 train no.250910  loss = 5.25056 avg_loss = 3.56943\n",
      "epoch no.2 train no.250920  loss = 3.33569 avg_loss = 3.55882\n",
      "epoch no.2 train no.250930  loss = 3.78679 avg_loss = 3.56856\n",
      "epoch no.2 train no.250940  loss = 2.50544 avg_loss = 3.56792\n",
      "epoch no.2 train no.250950  loss = 1.97337 avg_loss = 3.59077\n",
      "epoch no.2 train no.250960  loss = 4.10383 avg_loss = 3.60000\n",
      "epoch no.2 train no.250970  loss = 3.54281 avg_loss = 3.57611\n",
      "epoch no.2 train no.250980  loss = 4.51371 avg_loss = 3.61126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.250990  loss = 6.11333 avg_loss = 3.67027\n",
      "epoch no.2 train no.251000  loss = 2.52752 avg_loss = 3.62910\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '▁때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.2 train no.251010  loss = 4.16170 avg_loss = 3.61738\n",
      "epoch no.2 train no.251020  loss = 4.22483 avg_loss = 3.58753\n",
      "epoch no.2 train no.251030  loss = 6.72866 avg_loss = 3.61722\n",
      "epoch no.2 train no.251040  loss = 3.48493 avg_loss = 3.64877\n",
      "epoch no.2 train no.251050  loss = 4.52864 avg_loss = 3.60547\n",
      "epoch no.2 train no.251060  loss = 4.42031 avg_loss = 3.59720\n",
      "epoch no.2 train no.251070  loss = 2.98247 avg_loss = 3.66805\n",
      "epoch no.2 train no.251080  loss = 3.40297 avg_loss = 3.73917\n",
      "epoch no.2 train no.251090  loss = 2.81456 avg_loss = 3.71213\n",
      "epoch no.2 train no.251100  loss = 2.58266 avg_loss = 3.70257\n",
      "epoch no.2 train no.251110  loss = 4.25045 avg_loss = 3.68361\n",
      "epoch no.2 train no.251120  loss = 3.69296 avg_loss = 3.66893\n",
      "epoch no.2 train no.251130  loss = 4.14650 avg_loss = 3.64918\n",
      "epoch no.2 train no.251140  loss = 5.35207 avg_loss = 3.68725\n",
      "epoch no.2 train no.251150  loss = 3.51445 avg_loss = 3.64307\n",
      "epoch no.2 train no.251160  loss = 1.19227 avg_loss = 3.66669\n",
      "epoch no.2 train no.251170  loss = 4.43048 avg_loss = 3.66399\n",
      "epoch no.2 train no.251180  loss = 4.04295 avg_loss = 3.71061\n",
      "epoch no.2 train no.251190  loss = 2.54768 avg_loss = 3.65817\n",
      "epoch no.2 train no.251200  loss = 3.67683 avg_loss = 3.65434\n",
      "epoch no.2 train no.251210  loss = 3.29694 avg_loss = 3.69323\n",
      "epoch no.2 train no.251220  loss = 3.41633 avg_loss = 3.72949\n",
      "epoch no.2 train no.251230  loss = 2.93727 avg_loss = 3.73492\n",
      "epoch no.2 train no.251240  loss = 2.83859 avg_loss = 3.71219\n",
      "epoch no.2 train no.251250  loss = 4.85949 avg_loss = 3.69914\n",
      "epoch no.2 train no.251260  loss = 4.05666 avg_loss = 3.69573\n",
      "epoch no.2 train no.251270  loss = 3.37612 avg_loss = 3.71894\n",
      "epoch no.2 train no.251280  loss = 2.35911 avg_loss = 3.71204\n",
      "epoch no.2 train no.251290  loss = 5.36311 avg_loss = 3.70567\n",
      "epoch no.2 train no.251300  loss = 2.82212 avg_loss = 3.69539\n",
      "epoch no.2 train no.251310  loss = 2.87381 avg_loss = 3.71450\n",
      "epoch no.2 train no.251320  loss = 5.33587 avg_loss = 3.69540\n",
      "epoch no.2 train no.251330  loss = 4.11117 avg_loss = 3.66751\n",
      "epoch no.2 train no.251340  loss = 3.02838 avg_loss = 3.63486\n",
      "epoch no.2 train no.251350  loss = 4.38892 avg_loss = 3.61608\n",
      "epoch no.2 train no.251360  loss = 4.24379 avg_loss = 3.62635\n",
      "epoch no.2 train no.251370  loss = 3.51112 avg_loss = 3.62525\n",
      "epoch no.2 train no.251380  loss = 3.19186 avg_loss = 3.56284\n",
      "epoch no.2 train no.251390  loss = 3.00733 avg_loss = 3.58055\n",
      "epoch no.2 train no.251400  loss = 2.91468 avg_loss = 3.58751\n",
      "epoch no.2 train no.251410  loss = 3.99444 avg_loss = 3.62240\n",
      "epoch no.2 train no.251420  loss = 6.03315 avg_loss = 3.67211\n",
      "epoch no.2 train no.251430  loss = 4.02896 avg_loss = 3.71191\n",
      "epoch no.2 train no.251440  loss = 3.35734 avg_loss = 3.69820\n",
      "epoch no.2 train no.251450  loss = 5.83867 avg_loss = 3.75154\n",
      "epoch no.2 train no.251460  loss = 4.07971 avg_loss = 3.72579\n",
      "epoch no.2 train no.251470  loss = 3.35873 avg_loss = 3.77020\n",
      "epoch no.2 train no.251480  loss = 3.50434 avg_loss = 3.72507\n",
      "epoch no.2 train no.251490  loss = 4.90671 avg_loss = 3.75773\n",
      "epoch no.2 train no.251500  loss = 3.04857 avg_loss = 3.72082\n",
      "epoch no.2 train no.251510  loss = 3.50375 avg_loss = 3.72878\n",
      "epoch no.2 train no.251520  loss = 1.78937 avg_loss = 3.67441\n",
      "epoch no.2 train no.251530  loss = 4.49746 avg_loss = 3.67129\n",
      "epoch no.2 train no.251540  loss = 3.92950 avg_loss = 3.68602\n",
      "epoch no.2 train no.251550  loss = 2.97384 avg_loss = 3.68101\n",
      "epoch no.2 train no.251560  loss = 2.92967 avg_loss = 3.67155\n",
      "epoch no.2 train no.251570  loss = 4.33504 avg_loss = 3.65703\n",
      "epoch no.2 train no.251580  loss = 2.40345 avg_loss = 3.66145\n",
      "epoch no.2 train no.251590  loss = 3.07640 avg_loss = 3.67390\n",
      "epoch no.2 train no.251600  loss = 5.89326 avg_loss = 3.71962\n",
      "epoch no.2 train no.251610  loss = 1.99776 avg_loss = 3.67562\n",
      "epoch no.2 train no.251620  loss = 3.41030 avg_loss = 3.66996\n",
      "epoch no.2 train no.251630  loss = 3.79550 avg_loss = 3.70093\n",
      "epoch no.2 train no.251640  loss = 3.48751 avg_loss = 3.62208\n",
      "epoch no.2 train no.251650  loss = 3.47093 avg_loss = 3.61550\n",
      "epoch no.2 train no.251660  loss = 3.52322 avg_loss = 3.68471\n",
      "epoch no.2 train no.251670  loss = 3.68442 avg_loss = 3.70265\n",
      "epoch no.2 train no.251680  loss = 2.27836 avg_loss = 3.67139\n",
      "epoch no.2 train no.251690  loss = 3.80667 avg_loss = 3.63092\n",
      "epoch no.2 train no.251700  loss = 5.93142 avg_loss = 3.71740\n",
      "epoch no.3 train no.251710  loss = 3.21268 avg_loss = 3.73702\n",
      "epoch no.3 train no.251720  loss = 3.16190 avg_loss = 3.70314\n",
      "epoch no.3 train no.251730  loss = 2.88537 avg_loss = 3.71436\n",
      "epoch no.3 train no.251740  loss = 3.06540 avg_loss = 3.72927\n",
      "epoch no.3 train no.251750  loss = 3.65784 avg_loss = 3.68170\n",
      "epoch no.3 train no.251760  loss = 3.70173 avg_loss = 3.65259\n",
      "epoch no.3 train no.251770  loss = 2.37331 avg_loss = 3.65342\n",
      "epoch no.3 train no.251780  loss = 3.34156 avg_loss = 3.58711\n",
      "epoch no.3 train no.251790  loss = 2.95848 avg_loss = 3.55073\n",
      "epoch no.3 train no.251800  loss = 3.06289 avg_loss = 3.51534\n",
      "epoch no.3 train no.251810  loss = 5.02609 avg_loss = 3.52160\n",
      "epoch no.3 train no.251820  loss = 6.39799 avg_loss = 3.52380\n",
      "epoch no.3 train no.251830  loss = 4.36335 avg_loss = 3.52504\n",
      "epoch no.3 train no.251840  loss = 4.13631 avg_loss = 3.52424\n",
      "epoch no.3 train no.251850  loss = 3.75577 avg_loss = 3.55399\n",
      "epoch no.3 train no.251860  loss = 2.95508 avg_loss = 3.48052\n",
      "epoch no.3 train no.251870  loss = 2.86928 avg_loss = 3.44716\n",
      "epoch no.3 train no.251880  loss = 2.66262 avg_loss = 3.46611\n",
      "epoch no.3 train no.251890  loss = 2.92158 avg_loss = 3.39692\n",
      "epoch no.3 train no.251900  loss = 3.96170 avg_loss = 3.41696\n",
      "epoch no.3 train no.251910  loss = 2.50218 avg_loss = 3.39981\n",
      "epoch no.3 train no.251920  loss = 2.82865 avg_loss = 3.43070\n",
      "epoch no.3 train no.251930  loss = 4.65321 avg_loss = 3.41795\n",
      "epoch no.3 train no.251940  loss = 4.92293 avg_loss = 3.43002\n",
      "epoch no.3 train no.251950  loss = 3.31013 avg_loss = 3.40934\n",
      "epoch no.3 train no.251960  loss = 2.59805 avg_loss = 3.38310\n",
      "epoch no.3 train no.251970  loss = 4.55251 avg_loss = 3.35148\n",
      "epoch no.3 train no.251980  loss = 4.97638 avg_loss = 3.32990\n",
      "epoch no.3 train no.251990  loss = 2.30611 avg_loss = 3.35652\n",
      "epoch no.3 train no.252000  loss = 2.96454 avg_loss = 3.33959\n",
      "5\n",
      "to_tokens: ['▁가을', '전환', '에', '▁딱', '▁좋은', '▁노래', '</s>', '</s>']\n",
      "기분전환에 딱 좋은 노래들</s>\n",
      "epoch no.3 train no.252010  loss = 1.55250 avg_loss = 3.30672\n",
      "epoch no.3 train no.252020  loss = 3.11361 avg_loss = 3.38263\n",
      "epoch no.3 train no.252030  loss = 3.20971 avg_loss = 3.35841\n",
      "epoch no.3 train no.252040  loss = 2.24784 avg_loss = 3.35090\n",
      "epoch no.3 train no.252050  loss = 2.75252 avg_loss = 3.32041\n",
      "epoch no.3 train no.252060  loss = 2.26425 avg_loss = 3.34616\n",
      "epoch no.3 train no.252070  loss = 2.26938 avg_loss = 3.36981\n",
      "epoch no.3 train no.252080  loss = 2.18476 avg_loss = 3.36671\n",
      "epoch no.3 train no.252090  loss = 3.92848 avg_loss = 3.34404\n",
      "epoch no.3 train no.252100  loss = 2.00090 avg_loss = 3.31901\n",
      "epoch no.3 train no.252110  loss = 5.65092 avg_loss = 3.35781\n",
      "epoch no.3 train no.252120  loss = 2.95981 avg_loss = 3.34961\n",
      "epoch no.3 train no.252130  loss = 4.14988 avg_loss = 3.37725\n",
      "epoch no.3 train no.252140  loss = 2.43665 avg_loss = 3.36895\n",
      "epoch no.3 train no.252150  loss = 3.39245 avg_loss = 3.38424\n",
      "epoch no.3 train no.252160  loss = 2.82255 avg_loss = 3.39889\n",
      "epoch no.3 train no.252170  loss = 2.65545 avg_loss = 3.41631\n",
      "epoch no.3 train no.252180  loss = 3.85925 avg_loss = 3.38446\n",
      "epoch no.3 train no.252190  loss = 1.84710 avg_loss = 3.40041\n",
      "epoch no.3 train no.252200  loss = 2.86093 avg_loss = 3.37744\n",
      "epoch no.3 train no.252210  loss = 4.17482 avg_loss = 3.36142\n",
      "epoch no.3 train no.252220  loss = 2.79783 avg_loss = 3.35655\n",
      "epoch no.3 train no.252230  loss = 3.15252 avg_loss = 3.34579\n",
      "epoch no.3 train no.252240  loss = 3.48265 avg_loss = 3.35346\n",
      "epoch no.3 train no.252250  loss = 2.54525 avg_loss = 3.35852\n",
      "epoch no.3 train no.252260  loss = 3.74620 avg_loss = 3.34119\n",
      "epoch no.3 train no.252270  loss = 3.65921 avg_loss = 3.33628\n",
      "epoch no.3 train no.252280  loss = 4.18840 avg_loss = 3.36172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.252290  loss = 1.50555 avg_loss = 3.35876\n",
      "epoch no.3 train no.252300  loss = 2.40631 avg_loss = 3.32803\n",
      "epoch no.3 train no.252310  loss = 2.78438 avg_loss = 3.31904\n",
      "epoch no.3 train no.252320  loss = 3.32249 avg_loss = 3.33353\n",
      "epoch no.3 train no.252330  loss = 3.01592 avg_loss = 3.33428\n",
      "epoch no.3 train no.252340  loss = 2.41573 avg_loss = 3.30268\n",
      "epoch no.3 train no.252350  loss = 3.71214 avg_loss = 3.31094\n",
      "epoch no.3 train no.252360  loss = 4.49256 avg_loss = 3.30002\n",
      "epoch no.3 train no.252370  loss = 4.11099 avg_loss = 3.26562\n",
      "epoch no.3 train no.252380  loss = 4.30018 avg_loss = 3.31821\n",
      "epoch no.3 train no.252390  loss = 3.03348 avg_loss = 3.33934\n",
      "epoch no.3 train no.252400  loss = 3.06831 avg_loss = 3.28821\n",
      "epoch no.3 train no.252410  loss = 2.65711 avg_loss = 3.30237\n",
      "epoch no.3 train no.252420  loss = 2.58008 avg_loss = 3.27279\n",
      "epoch no.3 train no.252430  loss = 4.60856 avg_loss = 3.28842\n",
      "epoch no.3 train no.252440  loss = 2.62661 avg_loss = 3.29197\n",
      "epoch no.3 train no.252450  loss = 5.39704 avg_loss = 3.34104\n",
      "epoch no.3 train no.252460  loss = 2.36174 avg_loss = 3.33987\n",
      "epoch no.3 train no.252470  loss = 4.09786 avg_loss = 3.33164\n",
      "epoch no.3 train no.252480  loss = 2.97357 avg_loss = 3.30362\n",
      "epoch no.3 train no.252490  loss = 2.72685 avg_loss = 3.27458\n",
      "epoch no.3 train no.252500  loss = 3.75558 avg_loss = 3.32004\n",
      "epoch no.3 train no.252510  loss = 3.69305 avg_loss = 3.32179\n",
      "epoch no.3 train no.252520  loss = 3.22450 avg_loss = 3.32405\n",
      "epoch no.3 train no.252530  loss = 2.59091 avg_loss = 3.31740\n",
      "epoch no.3 train no.252540  loss = 3.61057 avg_loss = 3.26738\n",
      "epoch no.3 train no.252550  loss = 2.96155 avg_loss = 3.30096\n",
      "epoch no.3 train no.252560  loss = 3.76252 avg_loss = 3.30745\n",
      "epoch no.3 train no.252570  loss = 4.86900 avg_loss = 3.34065\n",
      "epoch no.3 train no.252580  loss = 2.68561 avg_loss = 3.33186\n",
      "epoch no.3 train no.252590  loss = 4.29309 avg_loss = 3.32416\n",
      "epoch no.3 train no.252600  loss = 3.00053 avg_loss = 3.31596\n",
      "epoch no.3 train no.252610  loss = 2.81890 avg_loss = 3.32165\n",
      "epoch no.3 train no.252620  loss = 2.31027 avg_loss = 3.28540\n",
      "epoch no.3 train no.252630  loss = 2.64669 avg_loss = 3.30207\n",
      "epoch no.3 train no.252640  loss = 2.41715 avg_loss = 3.34521\n",
      "epoch no.3 train no.252650  loss = 3.23265 avg_loss = 3.35809\n",
      "epoch no.3 train no.252660  loss = 2.92422 avg_loss = 3.33179\n",
      "epoch no.3 train no.252670  loss = 2.69863 avg_loss = 3.31759\n",
      "epoch no.3 train no.252680  loss = 3.73809 avg_loss = 3.30047\n",
      "epoch no.3 train no.252690  loss = 5.84253 avg_loss = 3.30677\n",
      "epoch no.3 train no.252700  loss = 4.12384 avg_loss = 3.35308\n",
      "epoch no.3 train no.252710  loss = 2.78492 avg_loss = 3.32249\n",
      "epoch no.3 train no.252720  loss = 2.68472 avg_loss = 3.32826\n",
      "epoch no.3 train no.252730  loss = 4.08405 avg_loss = 3.33153\n",
      "epoch no.3 train no.252740  loss = 2.57979 avg_loss = 3.29569\n",
      "epoch no.3 train no.252750  loss = 2.73578 avg_loss = 3.29724\n",
      "epoch no.3 train no.252760  loss = 5.68845 avg_loss = 3.36970\n",
      "epoch no.3 train no.252770  loss = 2.87789 avg_loss = 3.29682\n",
      "epoch no.3 train no.252780  loss = 2.77308 avg_loss = 3.28341\n",
      "epoch no.3 train no.252790  loss = 2.79542 avg_loss = 3.32580\n",
      "epoch no.3 train no.252800  loss = 3.13435 avg_loss = 3.32503\n",
      "epoch no.3 train no.252810  loss = 2.89315 avg_loss = 3.34795\n",
      "epoch no.3 train no.252820  loss = 2.74858 avg_loss = 3.34578\n",
      "epoch no.3 train no.252830  loss = 4.49277 avg_loss = 3.38751\n",
      "epoch no.3 train no.252840  loss = 2.66740 avg_loss = 3.39368\n",
      "epoch no.3 train no.252850  loss = 3.62495 avg_loss = 3.38869\n",
      "epoch no.3 train no.252860  loss = 2.60182 avg_loss = 3.40727\n",
      "epoch no.3 train no.252870  loss = 3.97407 avg_loss = 3.39995\n",
      "epoch no.3 train no.252880  loss = 3.35510 avg_loss = 3.40474\n",
      "epoch no.3 train no.252890  loss = 1.79950 avg_loss = 3.35418\n",
      "epoch no.3 train no.252900  loss = 4.22593 avg_loss = 3.35206\n",
      "epoch no.3 train no.252910  loss = 3.75292 avg_loss = 3.34650\n",
      "epoch no.3 train no.252920  loss = 2.67599 avg_loss = 3.36124\n",
      "epoch no.3 train no.252930  loss = 2.52043 avg_loss = 3.34760\n",
      "epoch no.3 train no.252940  loss = 3.13417 avg_loss = 3.31566\n",
      "epoch no.3 train no.252950  loss = 2.44570 avg_loss = 3.28832\n",
      "epoch no.3 train no.252960  loss = 2.44875 avg_loss = 3.31555\n",
      "epoch no.3 train no.252970  loss = 2.49800 avg_loss = 3.32195\n",
      "epoch no.3 train no.252980  loss = 3.53681 avg_loss = 3.31779\n",
      "epoch no.3 train no.252990  loss = 4.00167 avg_loss = 3.31820\n",
      "epoch no.3 train no.253000  loss = 3.23738 avg_loss = 3.35450\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁좋은', '▁좋은']\n",
      "기분전환에 딱</s>\n",
      "epoch no.3 train no.253010  loss = 3.63090 avg_loss = 3.38684\n",
      "epoch no.3 train no.253020  loss = 2.60226 avg_loss = 3.39485\n",
      "epoch no.3 train no.253030  loss = 3.41528 avg_loss = 3.39662\n",
      "epoch no.3 train no.253040  loss = 2.86632 avg_loss = 3.35471\n",
      "epoch no.3 train no.253050  loss = 4.41363 avg_loss = 3.35714\n",
      "epoch no.3 train no.253060  loss = 4.04418 avg_loss = 3.36939\n",
      "epoch no.3 train no.253070  loss = 3.52252 avg_loss = 3.43831\n",
      "epoch no.3 train no.253080  loss = 1.67967 avg_loss = 3.39215\n",
      "epoch no.3 train no.253090  loss = 1.83676 avg_loss = 3.34009\n",
      "epoch no.3 train no.253100  loss = 4.02405 avg_loss = 3.35309\n",
      "epoch no.3 train no.253110  loss = 3.12230 avg_loss = 3.39130\n",
      "epoch no.3 train no.253120  loss = 5.75014 avg_loss = 3.39076\n",
      "epoch no.3 train no.253130  loss = 2.75638 avg_loss = 3.39084\n",
      "epoch no.3 train no.253140  loss = 3.67357 avg_loss = 3.34939\n",
      "epoch no.3 train no.253150  loss = 3.49434 avg_loss = 3.34874\n",
      "epoch no.3 train no.253160  loss = 2.48462 avg_loss = 3.34485\n",
      "epoch no.3 train no.253170  loss = 2.54895 avg_loss = 3.33493\n",
      "epoch no.3 train no.253180  loss = 4.35807 avg_loss = 3.34681\n",
      "epoch no.3 train no.253190  loss = 3.01434 avg_loss = 3.32185\n",
      "epoch no.3 train no.253200  loss = 1.90349 avg_loss = 3.35127\n",
      "epoch no.3 train no.253210  loss = 5.37019 avg_loss = 3.37932\n",
      "epoch no.3 train no.253220  loss = 3.80424 avg_loss = 3.37548\n",
      "epoch no.3 train no.253230  loss = 2.14254 avg_loss = 3.34936\n",
      "epoch no.3 train no.253240  loss = 2.94162 avg_loss = 3.36166\n",
      "epoch no.3 train no.253250  loss = 3.90331 avg_loss = 3.37860\n",
      "epoch no.3 train no.253260  loss = 5.06877 avg_loss = 3.37752\n",
      "epoch no.3 train no.253270  loss = 2.50084 avg_loss = 3.37301\n",
      "epoch no.3 train no.253280  loss = 2.16767 avg_loss = 3.35907\n",
      "epoch no.3 train no.253290  loss = 2.35251 avg_loss = 3.32412\n",
      "epoch no.3 train no.253300  loss = 3.69549 avg_loss = 3.31507\n",
      "epoch no.3 train no.253310  loss = 4.25362 avg_loss = 3.28113\n",
      "epoch no.3 train no.253320  loss = 3.51133 avg_loss = 3.27759\n",
      "epoch no.3 train no.253330  loss = 4.59683 avg_loss = 3.32674\n",
      "epoch no.3 train no.253340  loss = 3.25229 avg_loss = 3.33616\n",
      "epoch no.3 train no.253350  loss = 3.00146 avg_loss = 3.30604\n",
      "epoch no.3 train no.253360  loss = 3.21915 avg_loss = 3.29455\n",
      "epoch no.3 train no.253370  loss = 2.75093 avg_loss = 3.26837\n",
      "epoch no.3 train no.253380  loss = 3.45814 avg_loss = 3.26895\n",
      "epoch no.3 train no.253390  loss = 2.34322 avg_loss = 3.27096\n",
      "epoch no.3 train no.253400  loss = 4.24205 avg_loss = 3.31800\n",
      "epoch no.3 train no.253410  loss = 4.75698 avg_loss = 3.30655\n",
      "epoch no.3 train no.253420  loss = 4.84001 avg_loss = 3.31656\n",
      "epoch no.3 train no.253430  loss = 2.44957 avg_loss = 3.32050\n",
      "epoch no.3 train no.253440  loss = 3.27963 avg_loss = 3.31932\n",
      "epoch no.3 train no.253450  loss = 2.89541 avg_loss = 3.33474\n",
      "epoch no.3 train no.253460  loss = 5.63059 avg_loss = 3.38774\n",
      "epoch no.3 train no.253470  loss = 3.22083 avg_loss = 3.35491\n",
      "epoch no.3 train no.253480  loss = 3.52034 avg_loss = 3.40831\n",
      "epoch no.3 train no.253490  loss = 3.47657 avg_loss = 3.43012\n",
      "epoch no.3 train no.253500  loss = 4.52014 avg_loss = 3.42503\n",
      "epoch no.3 train no.253510  loss = 4.39613 avg_loss = 3.43546\n",
      "epoch no.3 train no.253520  loss = 3.63031 avg_loss = 3.42823\n",
      "epoch no.3 train no.253530  loss = 3.43360 avg_loss = 3.45061\n",
      "epoch no.3 train no.253540  loss = 5.21496 avg_loss = 3.42290\n",
      "epoch no.3 train no.253550  loss = 2.03519 avg_loss = 3.38778\n",
      "epoch no.3 train no.253560  loss = 3.14104 avg_loss = 3.34773\n",
      "epoch no.3 train no.253570  loss = 4.47405 avg_loss = 3.35703\n",
      "epoch no.3 train no.253580  loss = 3.88842 avg_loss = 3.34410\n",
      "epoch no.3 train no.253590  loss = 3.75217 avg_loss = 3.32053\n",
      "epoch no.3 train no.253600  loss = 2.34137 avg_loss = 3.32847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.253610  loss = 2.55037 avg_loss = 3.27274\n",
      "epoch no.3 train no.253620  loss = 4.04727 avg_loss = 3.26973\n",
      "epoch no.3 train no.253630  loss = 2.36112 avg_loss = 3.25185\n",
      "epoch no.3 train no.253640  loss = 2.55043 avg_loss = 3.31808\n",
      "epoch no.3 train no.253650  loss = 3.19409 avg_loss = 3.30346\n",
      "epoch no.3 train no.253660  loss = 2.50433 avg_loss = 3.30190\n",
      "epoch no.3 train no.253670  loss = 4.89801 avg_loss = 3.28834\n",
      "epoch no.3 train no.253680  loss = 3.62650 avg_loss = 3.33192\n",
      "epoch no.3 train no.253690  loss = 3.10840 avg_loss = 3.28993\n",
      "epoch no.3 train no.253700  loss = 4.01941 avg_loss = 3.29154\n",
      "epoch no.3 train no.253710  loss = 2.77443 avg_loss = 3.28512\n",
      "epoch no.3 train no.253720  loss = 3.19895 avg_loss = 3.27102\n",
      "epoch no.3 train no.253730  loss = 1.68422 avg_loss = 3.26184\n",
      "epoch no.3 train no.253740  loss = 2.98686 avg_loss = 3.22585\n",
      "epoch no.3 train no.253750  loss = 3.92578 avg_loss = 3.23094\n",
      "epoch no.3 train no.253760  loss = 3.41029 avg_loss = 3.20671\n",
      "epoch no.3 train no.253770  loss = 2.61401 avg_loss = 3.16133\n",
      "epoch no.3 train no.253780  loss = 2.21997 avg_loss = 3.18095\n",
      "epoch no.3 train no.253790  loss = 2.15722 avg_loss = 3.14918\n",
      "epoch no.3 train no.253800  loss = 3.16980 avg_loss = 3.17355\n",
      "epoch no.3 train no.253810  loss = 3.74741 avg_loss = 3.19959\n",
      "epoch no.3 train no.253820  loss = 3.74409 avg_loss = 3.21487\n",
      "epoch no.3 train no.253830  loss = 3.41370 avg_loss = 3.25379\n",
      "epoch no.3 train no.253840  loss = 4.69679 avg_loss = 3.32687\n",
      "epoch no.3 train no.253850  loss = 5.34897 avg_loss = 3.39447\n",
      "epoch no.3 train no.253860  loss = 3.61002 avg_loss = 3.37591\n",
      "epoch no.3 train no.253870  loss = 2.04828 avg_loss = 3.33505\n",
      "epoch no.3 train no.253880  loss = 2.64492 avg_loss = 3.31075\n",
      "epoch no.3 train no.253890  loss = 2.53232 avg_loss = 3.31637\n",
      "epoch no.3 train no.253900  loss = 5.18260 avg_loss = 3.34411\n",
      "epoch no.3 train no.253910  loss = 4.30541 avg_loss = 3.34854\n",
      "epoch no.3 train no.253920  loss = 1.67624 avg_loss = 3.31776\n",
      "epoch no.3 train no.253930  loss = 5.25869 avg_loss = 3.30338\n",
      "epoch no.3 train no.253940  loss = 2.26584 avg_loss = 3.26871\n",
      "epoch no.3 train no.253950  loss = 4.38130 avg_loss = 3.28131\n",
      "epoch no.3 train no.253960  loss = 4.92367 avg_loss = 3.29300\n",
      "epoch no.3 train no.253970  loss = 3.40285 avg_loss = 3.29910\n",
      "epoch no.3 train no.253980  loss = 3.54963 avg_loss = 3.32917\n",
      "epoch no.3 train no.253990  loss = 4.80309 avg_loss = 3.38954\n",
      "epoch no.3 train no.254000  loss = 3.66798 avg_loss = 3.42186\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁위한', '▁신나는', '닉', '</s>']\n",
      "기분전환을 위한 일렉트로니카</s>\n",
      "epoch no.3 train no.254010  loss = 2.39380 avg_loss = 3.37878\n",
      "epoch no.3 train no.254020  loss = 5.09311 avg_loss = 3.37493\n",
      "epoch no.3 train no.254030  loss = 4.22517 avg_loss = 3.37392\n",
      "epoch no.3 train no.254040  loss = 3.15226 avg_loss = 3.35180\n",
      "epoch no.3 train no.254050  loss = 3.33692 avg_loss = 3.31501\n",
      "epoch no.3 train no.254060  loss = 2.09554 avg_loss = 3.33697\n",
      "epoch no.3 train no.254070  loss = 2.86279 avg_loss = 3.35625\n",
      "epoch no.3 train no.254080  loss = 2.54473 avg_loss = 3.35975\n",
      "epoch no.3 train no.254090  loss = 3.94007 avg_loss = 3.39215\n",
      "epoch no.3 train no.254100  loss = 2.38373 avg_loss = 3.35967\n",
      "epoch no.3 train no.254110  loss = 3.89897 avg_loss = 3.32540\n",
      "epoch no.3 train no.254120  loss = 3.30865 avg_loss = 3.31252\n",
      "epoch no.3 train no.254130  loss = 3.88578 avg_loss = 3.33869\n",
      "epoch no.3 train no.254140  loss = 2.81067 avg_loss = 3.33200\n",
      "epoch no.3 train no.254150  loss = 2.66583 avg_loss = 3.33799\n",
      "epoch no.3 train no.254160  loss = 2.69486 avg_loss = 3.30069\n",
      "epoch no.3 train no.254170  loss = 3.74824 avg_loss = 3.31870\n",
      "epoch no.3 train no.254180  loss = 3.90909 avg_loss = 3.30982\n",
      "epoch no.3 train no.254190  loss = 3.74048 avg_loss = 3.32524\n",
      "epoch no.3 train no.254200  loss = 3.36922 avg_loss = 3.36869\n",
      "epoch no.3 train no.254210  loss = 2.72027 avg_loss = 3.35162\n",
      "epoch no.3 train no.254220  loss = 3.05509 avg_loss = 3.31277\n",
      "epoch no.3 train no.254230  loss = 3.10472 avg_loss = 3.27767\n",
      "epoch no.3 train no.254240  loss = 2.98561 avg_loss = 3.27996\n",
      "epoch no.3 train no.254250  loss = 2.92684 avg_loss = 3.29449\n",
      "epoch no.3 train no.254260  loss = 2.76204 avg_loss = 3.28806\n",
      "epoch no.3 train no.254270  loss = 1.74023 avg_loss = 3.32236\n",
      "epoch no.3 train no.254280  loss = 3.75030 avg_loss = 3.30244\n",
      "epoch no.3 train no.254290  loss = 3.32512 avg_loss = 3.33807\n",
      "epoch no.3 train no.254300  loss = 3.48067 avg_loss = 3.32359\n",
      "epoch no.3 train no.254310  loss = 3.53297 avg_loss = 3.32719\n",
      "epoch no.3 train no.254320  loss = 2.01635 avg_loss = 3.28580\n",
      "epoch no.3 train no.254330  loss = 3.21336 avg_loss = 3.26026\n",
      "epoch no.3 train no.254340  loss = 3.10643 avg_loss = 3.33952\n",
      "epoch no.3 train no.254350  loss = 1.79359 avg_loss = 3.31034\n",
      "epoch no.3 train no.254360  loss = 4.17558 avg_loss = 3.31753\n",
      "epoch no.3 train no.254370  loss = 3.25233 avg_loss = 3.31658\n",
      "epoch no.3 train no.254380  loss = 3.04508 avg_loss = 3.30538\n",
      "epoch no.3 train no.254390  loss = 3.24419 avg_loss = 3.30768\n",
      "epoch no.3 train no.254400  loss = 4.65306 avg_loss = 3.33783\n",
      "epoch no.3 train no.254410  loss = 2.19987 avg_loss = 3.32091\n",
      "epoch no.3 train no.254420  loss = 4.47125 avg_loss = 3.34806\n",
      "epoch no.3 train no.254430  loss = 3.62406 avg_loss = 3.34758\n",
      "epoch no.3 train no.254440  loss = 3.70465 avg_loss = 3.37002\n",
      "epoch no.3 train no.254450  loss = 2.97317 avg_loss = 3.37727\n",
      "epoch no.3 train no.254460  loss = 3.09611 avg_loss = 3.33917\n",
      "epoch no.3 train no.254470  loss = 5.00155 avg_loss = 3.33822\n",
      "epoch no.3 train no.254480  loss = 3.62468 avg_loss = 3.33772\n",
      "epoch no.3 train no.254490  loss = 2.53404 avg_loss = 3.31180\n",
      "epoch no.3 train no.254500  loss = 4.19686 avg_loss = 3.32093\n",
      "epoch no.3 train no.254510  loss = 3.07891 avg_loss = 3.34280\n",
      "epoch no.3 train no.254520  loss = 4.12054 avg_loss = 3.32559\n",
      "epoch no.3 train no.254530  loss = 2.93440 avg_loss = 3.29781\n",
      "epoch no.3 train no.254540  loss = 3.98064 avg_loss = 3.33022\n",
      "epoch no.3 train no.254550  loss = 4.99448 avg_loss = 3.31100\n",
      "epoch no.3 train no.254560  loss = 2.58918 avg_loss = 3.35252\n",
      "epoch no.3 train no.254570  loss = 3.03043 avg_loss = 3.33056\n",
      "epoch no.3 train no.254580  loss = 2.74189 avg_loss = 3.34408\n",
      "epoch no.3 train no.254590  loss = 2.52349 avg_loss = 3.35772\n",
      "epoch no.3 train no.254600  loss = 3.59200 avg_loss = 3.32481\n",
      "epoch no.3 train no.254610  loss = 3.48522 avg_loss = 3.31548\n",
      "epoch no.3 train no.254620  loss = 3.49084 avg_loss = 3.30049\n",
      "epoch no.3 train no.254630  loss = 2.77974 avg_loss = 3.27849\n",
      "epoch no.3 train no.254640  loss = 4.75744 avg_loss = 3.25124\n",
      "epoch no.3 train no.254650  loss = 2.21135 avg_loss = 3.21447\n",
      "epoch no.3 train no.254660  loss = 3.07182 avg_loss = 3.23491\n",
      "epoch no.3 train no.254670  loss = 2.56719 avg_loss = 3.23169\n",
      "epoch no.3 train no.254680  loss = 2.69302 avg_loss = 3.21857\n",
      "epoch no.3 train no.254690  loss = 4.08994 avg_loss = 3.28639\n",
      "epoch no.3 train no.254700  loss = 3.16599 avg_loss = 3.28571\n",
      "epoch no.3 train no.254710  loss = 3.10113 avg_loss = 3.29458\n",
      "epoch no.3 train no.254720  loss = 3.42297 avg_loss = 3.33265\n",
      "epoch no.3 train no.254730  loss = 3.60871 avg_loss = 3.28939\n",
      "epoch no.3 train no.254740  loss = 4.85414 avg_loss = 3.35592\n",
      "epoch no.3 train no.254750  loss = 4.30636 avg_loss = 3.33164\n",
      "epoch no.3 train no.254760  loss = 3.00322 avg_loss = 3.32821\n",
      "epoch no.3 train no.254770  loss = 1.47191 avg_loss = 3.31112\n",
      "epoch no.3 train no.254780  loss = 3.12851 avg_loss = 3.29871\n",
      "epoch no.3 train no.254790  loss = 3.63880 avg_loss = 3.29741\n",
      "epoch no.3 train no.254800  loss = 4.76898 avg_loss = 3.29159\n",
      "epoch no.3 train no.254810  loss = 3.47834 avg_loss = 3.30641\n",
      "epoch no.3 train no.254820  loss = 2.06136 avg_loss = 3.33049\n",
      "epoch no.3 train no.254830  loss = 3.57103 avg_loss = 3.33574\n",
      "epoch no.3 train no.254840  loss = 3.57356 avg_loss = 3.35205\n",
      "epoch no.3 train no.254850  loss = 3.36530 avg_loss = 3.31420\n",
      "epoch no.3 train no.254860  loss = 2.62658 avg_loss = 3.33705\n",
      "epoch no.3 train no.254870  loss = 3.13687 avg_loss = 3.34724\n",
      "epoch no.3 train no.254880  loss = 3.42301 avg_loss = 3.30636\n",
      "epoch no.3 train no.254890  loss = 2.46477 avg_loss = 3.29129\n",
      "epoch no.3 train no.254900  loss = 2.05927 avg_loss = 3.29777\n",
      "epoch no.3 train no.254910  loss = 3.02673 avg_loss = 3.25591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.254920  loss = 3.18189 avg_loss = 3.22662\n",
      "epoch no.3 train no.254930  loss = 2.97491 avg_loss = 3.24154\n",
      "epoch no.3 train no.254940  loss = 3.21246 avg_loss = 3.22969\n",
      "epoch no.3 train no.254950  loss = 4.44651 avg_loss = 3.29578\n",
      "epoch no.3 train no.254960  loss = 4.05899 avg_loss = 3.28068\n",
      "epoch no.3 train no.254970  loss = 2.38600 avg_loss = 3.27123\n",
      "epoch no.3 train no.254980  loss = 2.75649 avg_loss = 3.22245\n",
      "epoch no.3 train no.254990  loss = 2.43013 avg_loss = 3.21947\n",
      "epoch no.3 train no.255000  loss = 3.25949 avg_loss = 3.24746\n",
      "4\n",
      "to_tokens: ['▁가을', '전환', '이', '▁위한', '▁음악', '송', '</s>']\n",
      "기분전환을 위한 팝송</s>\n",
      "epoch no.3 train no.255010  loss = 3.78188 avg_loss = 3.23996\n",
      "epoch no.3 train no.255020  loss = 3.42445 avg_loss = 3.22418\n",
      "epoch no.3 train no.255030  loss = 2.73765 avg_loss = 3.22470\n",
      "epoch no.3 train no.255040  loss = 1.96297 avg_loss = 3.19751\n",
      "epoch no.3 train no.255050  loss = 3.61903 avg_loss = 3.20346\n",
      "epoch no.3 train no.255060  loss = 3.29027 avg_loss = 3.25897\n",
      "epoch no.3 train no.255070  loss = 3.22951 avg_loss = 3.21922\n",
      "epoch no.3 train no.255080  loss = 4.39743 avg_loss = 3.20583\n",
      "epoch no.3 train no.255090  loss = 3.39427 avg_loss = 3.20431\n",
      "epoch no.3 train no.255100  loss = 1.69860 avg_loss = 3.19331\n",
      "epoch no.3 train no.255110  loss = 4.04091 avg_loss = 3.18893\n",
      "epoch no.3 train no.255120  loss = 3.80451 avg_loss = 3.18462\n",
      "epoch no.3 train no.255130  loss = 3.93942 avg_loss = 3.21364\n",
      "epoch no.3 train no.255140  loss = 2.64893 avg_loss = 3.17688\n",
      "epoch no.3 train no.255150  loss = 2.43112 avg_loss = 3.19237\n",
      "epoch no.3 train no.255160  loss = 3.20642 avg_loss = 3.19036\n",
      "epoch no.3 train no.255170  loss = 2.73087 avg_loss = 3.19670\n",
      "epoch no.3 train no.255180  loss = 3.58265 avg_loss = 3.24466\n",
      "epoch no.3 train no.255190  loss = 2.91951 avg_loss = 3.23297\n",
      "epoch no.3 train no.255200  loss = 2.23102 avg_loss = 3.22989\n",
      "epoch no.3 train no.255210  loss = 2.32623 avg_loss = 3.23795\n",
      "epoch no.3 train no.255220  loss = 2.12378 avg_loss = 3.26197\n",
      "epoch no.3 train no.255230  loss = 2.52393 avg_loss = 3.29501\n",
      "epoch no.3 train no.255240  loss = 6.26272 avg_loss = 3.33298\n",
      "epoch no.3 train no.255250  loss = 3.42107 avg_loss = 3.35414\n",
      "epoch no.3 train no.255260  loss = 4.02081 avg_loss = 3.40750\n",
      "epoch no.3 train no.255270  loss = 3.07315 avg_loss = 3.38472\n",
      "epoch no.3 train no.255280  loss = 3.27713 avg_loss = 3.40300\n",
      "epoch no.3 train no.255290  loss = 4.25729 avg_loss = 3.39423\n",
      "epoch no.3 train no.255300  loss = 5.45472 avg_loss = 3.39675\n",
      "epoch no.3 train no.255310  loss = 4.32395 avg_loss = 3.39415\n",
      "epoch no.3 train no.255320  loss = 4.42276 avg_loss = 3.38252\n",
      "epoch no.3 train no.255330  loss = 4.20122 avg_loss = 3.37954\n",
      "epoch no.3 train no.255340  loss = 4.65924 avg_loss = 3.39883\n",
      "epoch no.3 train no.255350  loss = 1.84499 avg_loss = 3.40930\n",
      "epoch no.3 train no.255360  loss = 5.01230 avg_loss = 3.44547\n",
      "epoch no.3 train no.255370  loss = 2.40820 avg_loss = 3.40028\n",
      "epoch no.3 train no.255380  loss = 3.54022 avg_loss = 3.38390\n",
      "epoch no.3 train no.255390  loss = 2.30729 avg_loss = 3.34192\n",
      "epoch no.3 train no.255400  loss = 2.92913 avg_loss = 3.36685\n",
      "epoch no.3 train no.255410  loss = 4.76616 avg_loss = 3.42517\n",
      "epoch no.3 train no.255420  loss = 2.92903 avg_loss = 3.37581\n",
      "epoch no.3 train no.255430  loss = 4.44522 avg_loss = 3.38185\n",
      "epoch no.3 train no.255440  loss = 2.81724 avg_loss = 3.38154\n",
      "epoch no.3 train no.255450  loss = 2.93118 avg_loss = 3.40469\n",
      "epoch no.3 train no.255460  loss = 3.08598 avg_loss = 3.34605\n",
      "epoch no.3 train no.255470  loss = 2.58596 avg_loss = 3.36136\n",
      "epoch no.3 train no.255480  loss = 5.52722 avg_loss = 3.37539\n",
      "epoch no.3 train no.255490  loss = 4.46434 avg_loss = 3.41954\n",
      "epoch no.3 train no.255500  loss = 2.57008 avg_loss = 3.42756\n",
      "epoch no.3 train no.255510  loss = 2.82538 avg_loss = 3.45204\n",
      "epoch no.3 train no.255520  loss = 3.30658 avg_loss = 3.43459\n",
      "epoch no.3 train no.255530  loss = 2.40598 avg_loss = 3.42522\n",
      "epoch no.3 train no.255540  loss = 4.28150 avg_loss = 3.44432\n",
      "epoch no.3 train no.255550  loss = 2.62626 avg_loss = 3.42078\n",
      "epoch no.3 train no.255560  loss = 2.74256 avg_loss = 3.40274\n",
      "epoch no.3 train no.255570  loss = 3.94222 avg_loss = 3.36790\n",
      "epoch no.3 train no.255580  loss = 6.53749 avg_loss = 3.41543\n",
      "epoch no.3 train no.255590  loss = 3.51632 avg_loss = 3.43540\n",
      "epoch no.3 train no.255600  loss = 3.04387 avg_loss = 3.41284\n",
      "epoch no.3 train no.255610  loss = 1.54548 avg_loss = 3.35880\n",
      "epoch no.3 train no.255620  loss = 3.31915 avg_loss = 3.36570\n",
      "epoch no.3 train no.255630  loss = 4.48901 avg_loss = 3.33964\n",
      "epoch no.3 train no.255640  loss = 3.78673 avg_loss = 3.36421\n",
      "epoch no.3 train no.255650  loss = 4.09795 avg_loss = 3.38181\n",
      "epoch no.3 train no.255660  loss = 2.29921 avg_loss = 3.35982\n",
      "epoch no.3 train no.255670  loss = 2.44395 avg_loss = 3.32265\n",
      "epoch no.3 train no.255680  loss = 2.80612 avg_loss = 3.30396\n",
      "epoch no.3 train no.255690  loss = 6.74619 avg_loss = 3.30600\n",
      "epoch no.3 train no.255700  loss = 2.91681 avg_loss = 3.33346\n",
      "epoch no.3 train no.255710  loss = 3.29618 avg_loss = 3.28518\n",
      "epoch no.3 train no.255720  loss = 4.53875 avg_loss = 3.30234\n",
      "epoch no.3 train no.255730  loss = 3.76235 avg_loss = 3.38294\n",
      "epoch no.3 train no.255740  loss = 2.38200 avg_loss = 3.33378\n",
      "epoch no.3 train no.255750  loss = 1.92436 avg_loss = 3.33246\n",
      "epoch no.3 train no.255760  loss = 3.26797 avg_loss = 3.35267\n",
      "epoch no.3 train no.255770  loss = 3.19465 avg_loss = 3.35527\n",
      "epoch no.3 train no.255780  loss = 4.96912 avg_loss = 3.34452\n",
      "epoch no.3 train no.255790  loss = 3.62306 avg_loss = 3.38852\n",
      "epoch no.3 train no.255800  loss = 3.13861 avg_loss = 3.34602\n",
      "epoch no.3 train no.255810  loss = 4.62229 avg_loss = 3.37530\n",
      "epoch no.3 train no.255820  loss = 3.12501 avg_loss = 3.39013\n",
      "epoch no.3 train no.255830  loss = 2.83369 avg_loss = 3.40703\n",
      "epoch no.3 train no.255840  loss = 2.77765 avg_loss = 3.39039\n",
      "epoch no.3 train no.255850  loss = 2.40765 avg_loss = 3.38115\n",
      "epoch no.3 train no.255860  loss = 3.52387 avg_loss = 3.35752\n",
      "epoch no.3 train no.255870  loss = 2.37468 avg_loss = 3.30349\n",
      "epoch no.3 train no.255880  loss = 2.64956 avg_loss = 3.28730\n",
      "epoch no.3 train no.255890  loss = 3.18517 avg_loss = 3.31738\n",
      "epoch no.3 train no.255900  loss = 2.78110 avg_loss = 3.30075\n",
      "epoch no.3 train no.255910  loss = 2.87069 avg_loss = 3.28805\n",
      "epoch no.3 train no.255920  loss = 3.05877 avg_loss = 3.28646\n",
      "epoch no.3 train no.255930  loss = 3.38266 avg_loss = 3.25640\n",
      "epoch no.3 train no.255940  loss = 3.18335 avg_loss = 3.28888\n",
      "epoch no.3 train no.255950  loss = 4.29624 avg_loss = 3.34287\n",
      "epoch no.3 train no.255960  loss = 2.54001 avg_loss = 3.28993\n",
      "epoch no.3 train no.255970  loss = 3.41930 avg_loss = 3.30294\n",
      "epoch no.3 train no.255980  loss = 2.88980 avg_loss = 3.30085\n",
      "epoch no.3 train no.255990  loss = 3.50203 avg_loss = 3.28701\n",
      "epoch no.3 train no.256000  loss = 5.79423 avg_loss = 3.34983\n",
      "5\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁딱', '▁노래', '▁노래', '송', '</s>']\n",
      "기분전환에 좋은 신나는 팝송</s>\n",
      "epoch no.3 train no.256010  loss = 3.56495 avg_loss = 3.29828\n",
      "epoch no.3 train no.256020  loss = 6.35850 avg_loss = 3.34190\n",
      "epoch no.3 train no.256030  loss = 3.59172 avg_loss = 3.34791\n",
      "epoch no.3 train no.256040  loss = 3.64331 avg_loss = 3.31790\n",
      "epoch no.3 train no.256050  loss = 2.58334 avg_loss = 3.30038\n",
      "epoch no.3 train no.256060  loss = 3.13670 avg_loss = 3.34604\n",
      "epoch no.3 train no.256070  loss = 3.32368 avg_loss = 3.35089\n",
      "epoch no.3 train no.256080  loss = 4.64835 avg_loss = 3.37951\n",
      "epoch no.3 train no.256090  loss = 2.99197 avg_loss = 3.39822\n",
      "epoch no.3 train no.256100  loss = 4.48785 avg_loss = 3.44060\n",
      "epoch no.3 train no.256110  loss = 2.40764 avg_loss = 3.42642\n",
      "epoch no.3 train no.256120  loss = 3.17474 avg_loss = 3.39674\n",
      "epoch no.3 train no.256130  loss = 2.08382 avg_loss = 3.39315\n",
      "epoch no.3 train no.256140  loss = 3.03677 avg_loss = 3.35289\n",
      "epoch no.3 train no.256150  loss = 5.55678 avg_loss = 3.33737\n",
      "epoch no.3 train no.256160  loss = 2.97299 avg_loss = 3.33087\n",
      "epoch no.3 train no.256170  loss = 2.21026 avg_loss = 3.30056\n",
      "epoch no.3 train no.256180  loss = 2.12469 avg_loss = 3.33032\n",
      "epoch no.3 train no.256190  loss = 1.68045 avg_loss = 3.30913\n",
      "epoch no.3 train no.256200  loss = 4.17944 avg_loss = 3.30909\n",
      "epoch no.3 train no.256210  loss = 3.77093 avg_loss = 3.36684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.256220  loss = 4.16032 avg_loss = 3.42206\n",
      "epoch no.3 train no.256230  loss = 4.31560 avg_loss = 3.43437\n",
      "epoch no.3 train no.256240  loss = 4.66942 avg_loss = 3.45028\n",
      "epoch no.3 train no.256250  loss = 3.37643 avg_loss = 3.39916\n",
      "epoch no.3 train no.256260  loss = 3.25395 avg_loss = 3.38194\n",
      "epoch no.3 train no.256270  loss = 2.69957 avg_loss = 3.34817\n",
      "epoch no.3 train no.256280  loss = 3.57872 avg_loss = 3.33129\n",
      "epoch no.3 train no.256290  loss = 2.89170 avg_loss = 3.34774\n",
      "epoch no.3 train no.256300  loss = 3.13278 avg_loss = 3.35571\n",
      "epoch no.3 train no.256310  loss = 3.91389 avg_loss = 3.39606\n",
      "epoch no.3 train no.256320  loss = 4.41883 avg_loss = 3.39330\n",
      "epoch no.3 train no.256330  loss = 5.07412 avg_loss = 3.38659\n",
      "epoch no.3 train no.256340  loss = 3.20942 avg_loss = 3.38232\n",
      "epoch no.3 train no.256350  loss = 3.59790 avg_loss = 3.40222\n",
      "epoch no.3 train no.256360  loss = 3.41662 avg_loss = 3.40310\n",
      "epoch no.3 train no.256370  loss = 2.88769 avg_loss = 3.39091\n",
      "epoch no.3 train no.256380  loss = 1.93200 avg_loss = 3.39874\n",
      "epoch no.3 train no.256390  loss = 2.22306 avg_loss = 3.35990\n",
      "epoch no.3 train no.256400  loss = 3.16304 avg_loss = 3.37780\n",
      "epoch no.3 train no.256410  loss = 2.45568 avg_loss = 3.39114\n",
      "epoch no.3 train no.256420  loss = 3.79052 avg_loss = 3.39663\n",
      "epoch no.3 train no.256430  loss = 2.16955 avg_loss = 3.34735\n",
      "epoch no.3 train no.256440  loss = 1.96929 avg_loss = 3.36032\n",
      "epoch no.3 train no.256450  loss = 2.87830 avg_loss = 3.37830\n",
      "epoch no.3 train no.256460  loss = 2.06492 avg_loss = 3.33823\n",
      "epoch no.3 train no.256470  loss = 2.24332 avg_loss = 3.28978\n",
      "epoch no.3 train no.256480  loss = 2.70509 avg_loss = 3.30472\n",
      "epoch no.3 train no.256490  loss = 2.43445 avg_loss = 3.30536\n",
      "epoch no.3 train no.256500  loss = 4.47338 avg_loss = 3.32151\n",
      "epoch no.3 train no.256510  loss = 4.13185 avg_loss = 3.33089\n",
      "epoch no.3 train no.256520  loss = 2.80480 avg_loss = 3.31797\n",
      "epoch no.3 train no.256530  loss = 3.77009 avg_loss = 3.33661\n",
      "epoch no.3 train no.256540  loss = 2.84082 avg_loss = 3.38254\n",
      "epoch no.3 train no.256550  loss = 3.33213 avg_loss = 3.44277\n",
      "epoch no.3 train no.256560  loss = 3.20462 avg_loss = 3.41912\n",
      "epoch no.3 train no.256570  loss = 2.93933 avg_loss = 3.35498\n",
      "epoch no.3 train no.256580  loss = 3.74783 avg_loss = 3.36165\n",
      "epoch no.3 train no.256590  loss = 3.43490 avg_loss = 3.33464\n",
      "epoch no.3 train no.256600  loss = 5.38433 avg_loss = 3.37281\n",
      "epoch no.3 train no.256610  loss = 1.97867 avg_loss = 3.37569\n",
      "epoch no.3 train no.256620  loss = 3.96927 avg_loss = 3.39025\n",
      "epoch no.3 train no.256630  loss = 7.20558 avg_loss = 3.41344\n",
      "epoch no.3 train no.256640  loss = 4.13575 avg_loss = 3.39874\n",
      "epoch no.3 train no.256650  loss = 2.36175 avg_loss = 3.44291\n",
      "epoch no.3 train no.256660  loss = 3.29025 avg_loss = 3.45413\n",
      "epoch no.3 train no.256670  loss = 2.79227 avg_loss = 3.41839\n",
      "epoch no.3 train no.256680  loss = 3.58412 avg_loss = 3.38853\n",
      "epoch no.3 train no.256690  loss = 4.09298 avg_loss = 3.40772\n",
      "epoch no.3 train no.256700  loss = 3.51564 avg_loss = 3.44126\n",
      "epoch no.3 train no.256710  loss = 3.89632 avg_loss = 3.44895\n",
      "epoch no.3 train no.256720  loss = 4.61480 avg_loss = 3.41979\n",
      "epoch no.3 train no.256730  loss = 4.05990 avg_loss = 3.41881\n",
      "epoch no.3 train no.256740  loss = 5.86085 avg_loss = 3.39785\n",
      "epoch no.3 train no.256750  loss = 5.22262 avg_loss = 3.41892\n",
      "epoch no.3 train no.256760  loss = 2.55527 avg_loss = 3.42440\n",
      "epoch no.3 train no.256770  loss = 2.69943 avg_loss = 3.40766\n",
      "epoch no.3 train no.256780  loss = 5.53881 avg_loss = 3.45164\n",
      "epoch no.3 train no.256790  loss = 3.68051 avg_loss = 3.44225\n",
      "epoch no.3 train no.256800  loss = 2.50302 avg_loss = 3.42576\n",
      "epoch no.3 train no.256810  loss = 3.34064 avg_loss = 3.42462\n",
      "epoch no.3 train no.256820  loss = 3.05898 avg_loss = 3.39580\n",
      "epoch no.3 train no.256830  loss = 4.86608 avg_loss = 3.37914\n",
      "epoch no.3 train no.256840  loss = 2.10543 avg_loss = 3.43547\n",
      "epoch no.3 train no.256850  loss = 3.86604 avg_loss = 3.47543\n",
      "epoch no.3 train no.256860  loss = 3.07149 avg_loss = 3.43701\n",
      "epoch no.3 train no.256870  loss = 3.04907 avg_loss = 3.46580\n",
      "epoch no.3 train no.256880  loss = 3.89846 avg_loss = 3.45031\n",
      "epoch no.3 train no.256890  loss = 2.13293 avg_loss = 3.47242\n",
      "epoch no.3 train no.256900  loss = 3.03953 avg_loss = 3.42264\n",
      "epoch no.3 train no.256910  loss = 4.04320 avg_loss = 3.40187\n",
      "epoch no.3 train no.256920  loss = 3.69162 avg_loss = 3.41645\n",
      "epoch no.3 train no.256930  loss = 3.70026 avg_loss = 3.44342\n",
      "epoch no.3 train no.256940  loss = 3.47617 avg_loss = 3.42651\n",
      "epoch no.3 train no.256950  loss = 2.21630 avg_loss = 3.43416\n",
      "epoch no.3 train no.256960  loss = 2.69377 avg_loss = 3.46067\n",
      "epoch no.3 train no.256970  loss = 2.24236 avg_loss = 3.41452\n",
      "epoch no.3 train no.256980  loss = 3.77317 avg_loss = 3.38036\n",
      "epoch no.3 train no.256990  loss = 2.81813 avg_loss = 3.37120\n",
      "epoch no.3 train no.257000  loss = 4.68813 avg_loss = 3.39469\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '에', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.3 train no.257010  loss = 4.07996 avg_loss = 3.40538\n",
      "epoch no.3 train no.257020  loss = 3.00352 avg_loss = 3.35445\n",
      "epoch no.3 train no.257030  loss = 4.06124 avg_loss = 3.34035\n",
      "epoch no.3 train no.257040  loss = 4.55156 avg_loss = 3.29589\n",
      "epoch no.3 train no.257050  loss = 2.92045 avg_loss = 3.30007\n",
      "epoch no.3 train no.257060  loss = 3.80676 avg_loss = 3.32261\n",
      "epoch no.3 train no.257070  loss = 3.90881 avg_loss = 3.31375\n",
      "epoch no.3 train no.257080  loss = 2.09771 avg_loss = 3.32007\n",
      "epoch no.3 train no.257090  loss = 4.32003 avg_loss = 3.39539\n",
      "epoch no.3 train no.257100  loss = 2.68800 avg_loss = 3.46355\n",
      "epoch no.3 train no.257110  loss = 4.16479 avg_loss = 3.44231\n",
      "epoch no.3 train no.257120  loss = 3.05153 avg_loss = 3.42918\n",
      "epoch no.3 train no.257130  loss = 3.09686 avg_loss = 3.43862\n",
      "epoch no.3 train no.257140  loss = 3.01155 avg_loss = 3.43921\n",
      "epoch no.3 train no.257150  loss = 4.51604 avg_loss = 3.46998\n",
      "epoch no.3 train no.257160  loss = 3.79628 avg_loss = 3.46703\n",
      "epoch no.3 train no.257170  loss = 3.41580 avg_loss = 3.46860\n",
      "epoch no.3 train no.257180  loss = 1.64905 avg_loss = 3.39056\n",
      "epoch no.3 train no.257190  loss = 2.81018 avg_loss = 3.41117\n",
      "epoch no.3 train no.257200  loss = 2.83896 avg_loss = 3.37795\n",
      "epoch no.3 train no.257210  loss = 3.17461 avg_loss = 3.40233\n",
      "epoch no.3 train no.257220  loss = 3.47505 avg_loss = 3.37278\n",
      "epoch no.3 train no.257230  loss = 5.14218 avg_loss = 3.38779\n",
      "epoch no.3 train no.257240  loss = 5.23774 avg_loss = 3.41329\n",
      "epoch no.3 train no.257250  loss = 2.23529 avg_loss = 3.38767\n",
      "epoch no.3 train no.257260  loss = 3.78796 avg_loss = 3.36060\n",
      "epoch no.3 train no.257270  loss = 5.71813 avg_loss = 3.33746\n",
      "epoch no.3 train no.257280  loss = 2.30171 avg_loss = 3.37072\n",
      "epoch no.3 train no.257290  loss = 4.27098 avg_loss = 3.39216\n",
      "epoch no.3 train no.257300  loss = 2.97055 avg_loss = 3.34824\n",
      "epoch no.3 train no.257310  loss = 3.74367 avg_loss = 3.36036\n",
      "epoch no.3 train no.257320  loss = 2.36588 avg_loss = 3.37593\n",
      "epoch no.3 train no.257330  loss = 3.70213 avg_loss = 3.40469\n",
      "epoch no.3 train no.257340  loss = 3.81463 avg_loss = 3.42973\n",
      "epoch no.3 train no.257350  loss = 3.13113 avg_loss = 3.38100\n",
      "epoch no.3 train no.257360  loss = 3.56009 avg_loss = 3.38321\n",
      "epoch no.3 train no.257370  loss = 2.02776 avg_loss = 3.37971\n",
      "epoch no.3 train no.257380  loss = 2.08144 avg_loss = 3.29870\n",
      "epoch no.3 train no.257390  loss = 3.54698 avg_loss = 3.28441\n",
      "epoch no.3 train no.257400  loss = 5.12157 avg_loss = 3.27403\n",
      "epoch no.3 train no.257410  loss = 2.58108 avg_loss = 3.27985\n",
      "epoch no.3 train no.257420  loss = 2.20154 avg_loss = 3.30996\n",
      "epoch no.3 train no.257430  loss = 3.64516 avg_loss = 3.37900\n",
      "epoch no.3 train no.257440  loss = 4.67200 avg_loss = 3.35059\n",
      "epoch no.3 train no.257450  loss = 2.06569 avg_loss = 3.30564\n",
      "epoch no.3 train no.257460  loss = 3.54313 avg_loss = 3.31277\n",
      "epoch no.3 train no.257470  loss = 3.06869 avg_loss = 3.29372\n",
      "epoch no.3 train no.257480  loss = 4.91751 avg_loss = 3.31466\n",
      "epoch no.3 train no.257490  loss = 2.35965 avg_loss = 3.29851\n",
      "epoch no.3 train no.257500  loss = 2.60132 avg_loss = 3.28051\n",
      "epoch no.3 train no.257510  loss = 3.47770 avg_loss = 3.28995\n",
      "epoch no.3 train no.257520  loss = 4.46725 avg_loss = 3.29237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.257530  loss = 4.13946 avg_loss = 3.34789\n",
      "epoch no.3 train no.257540  loss = 2.34296 avg_loss = 3.33372\n",
      "epoch no.3 train no.257550  loss = 2.78876 avg_loss = 3.33765\n",
      "epoch no.3 train no.257560  loss = 4.14424 avg_loss = 3.30225\n",
      "epoch no.3 train no.257570  loss = 6.51120 avg_loss = 3.35619\n",
      "epoch no.3 train no.257580  loss = 1.80080 avg_loss = 3.35765\n",
      "epoch no.3 train no.257590  loss = 3.99769 avg_loss = 3.33669\n",
      "epoch no.3 train no.257600  loss = 4.01427 avg_loss = 3.32977\n",
      "epoch no.3 train no.257610  loss = 4.28343 avg_loss = 3.30673\n",
      "epoch no.3 train no.257620  loss = 1.84105 avg_loss = 3.30570\n",
      "epoch no.3 train no.257630  loss = 3.24017 avg_loss = 3.27026\n",
      "epoch no.3 train no.257640  loss = 2.42009 avg_loss = 3.29278\n",
      "epoch no.3 train no.257650  loss = 2.94682 avg_loss = 3.23568\n",
      "epoch no.3 train no.257660  loss = 3.31637 avg_loss = 3.26432\n",
      "epoch no.3 train no.257670  loss = 2.35123 avg_loss = 3.27103\n",
      "epoch no.3 train no.257680  loss = 4.29855 avg_loss = 3.28364\n",
      "epoch no.3 train no.257690  loss = 3.27319 avg_loss = 3.32737\n",
      "epoch no.3 train no.257700  loss = 2.39641 avg_loss = 3.31742\n",
      "epoch no.3 train no.257710  loss = 3.25698 avg_loss = 3.29159\n",
      "epoch no.3 train no.257720  loss = 2.37170 avg_loss = 3.24916\n",
      "epoch no.3 train no.257730  loss = 2.84820 avg_loss = 3.23274\n",
      "epoch no.3 train no.257740  loss = 4.02014 avg_loss = 3.24088\n",
      "epoch no.3 train no.257750  loss = 3.57575 avg_loss = 3.26438\n",
      "epoch no.3 train no.257760  loss = 3.62256 avg_loss = 3.26350\n",
      "epoch no.3 train no.257770  loss = 4.68796 avg_loss = 3.24447\n",
      "epoch no.3 train no.257780  loss = 5.27140 avg_loss = 3.29758\n",
      "epoch no.3 train no.257790  loss = 5.02109 avg_loss = 3.28309\n",
      "epoch no.3 train no.257800  loss = 3.93426 avg_loss = 3.32699\n",
      "epoch no.3 train no.257810  loss = 4.01835 avg_loss = 3.34612\n",
      "epoch no.3 train no.257820  loss = 4.32803 avg_loss = 3.29926\n",
      "epoch no.3 train no.257830  loss = 2.21768 avg_loss = 3.29062\n",
      "epoch no.3 train no.257840  loss = 4.00425 avg_loss = 3.32459\n",
      "epoch no.3 train no.257850  loss = 3.97949 avg_loss = 3.32393\n",
      "epoch no.3 train no.257860  loss = 3.27178 avg_loss = 3.28449\n",
      "epoch no.3 train no.257870  loss = 3.56796 avg_loss = 3.30888\n",
      "epoch no.3 train no.257880  loss = 3.40563 avg_loss = 3.31985\n",
      "epoch no.3 train no.257890  loss = 2.82063 avg_loss = 3.32286\n",
      "epoch no.3 train no.257900  loss = 3.25848 avg_loss = 3.27669\n",
      "epoch no.3 train no.257910  loss = 3.58561 avg_loss = 3.29251\n",
      "epoch no.3 train no.257920  loss = 2.91267 avg_loss = 3.27108\n",
      "epoch no.3 train no.257930  loss = 4.75615 avg_loss = 3.32718\n",
      "epoch no.3 train no.257940  loss = 2.78693 avg_loss = 3.32364\n",
      "epoch no.3 train no.257950  loss = 3.47294 avg_loss = 3.30841\n",
      "epoch no.3 train no.257960  loss = 3.65888 avg_loss = 3.31277\n",
      "epoch no.3 train no.257970  loss = 2.91534 avg_loss = 3.29507\n",
      "epoch no.3 train no.257980  loss = 2.90152 avg_loss = 3.26873\n",
      "epoch no.3 train no.257990  loss = 4.12767 avg_loss = 3.30846\n",
      "epoch no.3 train no.258000  loss = 4.43976 avg_loss = 3.39521\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '▁음악', '힙']\n",
      "기분전환에 좋은 재즈</s>\n",
      "epoch no.3 train no.258010  loss = 4.80383 avg_loss = 3.35898\n",
      "epoch no.3 train no.258020  loss = 3.09889 avg_loss = 3.35840\n",
      "epoch no.3 train no.258030  loss = 2.86470 avg_loss = 3.36609\n",
      "epoch no.3 train no.258040  loss = 3.33876 avg_loss = 3.33514\n",
      "epoch no.3 train no.258050  loss = 2.37998 avg_loss = 3.35545\n",
      "epoch no.3 train no.258060  loss = 2.86324 avg_loss = 3.35556\n",
      "epoch no.3 train no.258070  loss = 3.64455 avg_loss = 3.33388\n",
      "epoch no.3 train no.258080  loss = 3.49458 avg_loss = 3.33008\n",
      "epoch no.3 train no.258090  loss = 3.91312 avg_loss = 3.29453\n",
      "epoch no.3 train no.258100  loss = 2.21454 avg_loss = 3.30658\n",
      "epoch no.3 train no.258110  loss = 3.99581 avg_loss = 3.31716\n",
      "epoch no.3 train no.258120  loss = 2.23504 avg_loss = 3.29801\n",
      "epoch no.3 train no.258130  loss = 2.85155 avg_loss = 3.31029\n",
      "epoch no.3 train no.258140  loss = 3.60853 avg_loss = 3.30575\n",
      "epoch no.3 train no.258150  loss = 3.17934 avg_loss = 3.32320\n",
      "epoch no.3 train no.258160  loss = 5.07311 avg_loss = 3.34295\n",
      "epoch no.3 train no.258170  loss = 2.25041 avg_loss = 3.33051\n",
      "epoch no.3 train no.258180  loss = 2.89924 avg_loss = 3.29483\n",
      "epoch no.3 train no.258190  loss = 4.53416 avg_loss = 3.28655\n",
      "epoch no.3 train no.258200  loss = 3.68416 avg_loss = 3.32661\n",
      "epoch no.3 train no.258210  loss = 3.90721 avg_loss = 3.33349\n",
      "epoch no.3 train no.258220  loss = 2.45545 avg_loss = 3.35296\n",
      "epoch no.3 train no.258230  loss = 3.64460 avg_loss = 3.38306\n",
      "epoch no.3 train no.258240  loss = 3.97764 avg_loss = 3.40624\n",
      "epoch no.3 train no.258250  loss = 2.62789 avg_loss = 3.41190\n",
      "epoch no.3 train no.258260  loss = 2.90692 avg_loss = 3.36043\n",
      "epoch no.3 train no.258270  loss = 2.07008 avg_loss = 3.32700\n",
      "epoch no.3 train no.258280  loss = 2.25962 avg_loss = 3.29959\n",
      "epoch no.3 train no.258290  loss = 2.02025 avg_loss = 3.30610\n",
      "epoch no.3 train no.258300  loss = 2.45665 avg_loss = 3.31810\n",
      "epoch no.3 train no.258310  loss = 4.39226 avg_loss = 3.29802\n",
      "epoch no.3 train no.258320  loss = 3.54648 avg_loss = 3.28346\n",
      "epoch no.3 train no.258330  loss = 2.98184 avg_loss = 3.32727\n",
      "epoch no.3 train no.258340  loss = 2.82110 avg_loss = 3.30842\n",
      "epoch no.3 train no.258350  loss = 4.72714 avg_loss = 3.30831\n",
      "epoch no.3 train no.258360  loss = 1.32803 avg_loss = 3.28142\n",
      "epoch no.3 train no.258370  loss = 1.97254 avg_loss = 3.25560\n",
      "epoch no.3 train no.258380  loss = 4.68345 avg_loss = 3.26891\n",
      "epoch no.3 train no.258390  loss = 3.25455 avg_loss = 3.28363\n",
      "epoch no.3 train no.258400  loss = 4.01609 avg_loss = 3.28844\n",
      "epoch no.3 train no.258410  loss = 2.85455 avg_loss = 3.31518\n",
      "epoch no.3 train no.258420  loss = 4.05926 avg_loss = 3.31934\n",
      "epoch no.3 train no.258430  loss = 2.82475 avg_loss = 3.36747\n",
      "epoch no.3 train no.258440  loss = 6.47986 avg_loss = 3.37176\n",
      "epoch no.3 train no.258450  loss = 2.60494 avg_loss = 3.33308\n",
      "epoch no.3 train no.258460  loss = 5.48972 avg_loss = 3.35313\n",
      "epoch no.3 train no.258470  loss = 2.29462 avg_loss = 3.35775\n",
      "epoch no.3 train no.258480  loss = 2.49225 avg_loss = 3.37405\n",
      "epoch no.3 train no.258490  loss = 3.06370 avg_loss = 3.39612\n",
      "epoch no.3 train no.258500  loss = 2.15944 avg_loss = 3.35654\n",
      "epoch no.3 train no.258510  loss = 5.16901 avg_loss = 3.39406\n",
      "epoch no.3 train no.258520  loss = 3.00496 avg_loss = 3.39457\n",
      "epoch no.3 train no.258530  loss = 4.39996 avg_loss = 3.39099\n",
      "epoch no.3 train no.258540  loss = 3.62465 avg_loss = 3.35851\n",
      "epoch no.3 train no.258550  loss = 4.66604 avg_loss = 3.41672\n",
      "epoch no.3 train no.258560  loss = 4.15360 avg_loss = 3.42134\n",
      "epoch no.3 train no.258570  loss = 5.24110 avg_loss = 3.47320\n",
      "epoch no.3 train no.258580  loss = 2.86135 avg_loss = 3.47559\n",
      "epoch no.3 train no.258590  loss = 3.19723 avg_loss = 3.44561\n",
      "epoch no.3 train no.258600  loss = 4.96403 avg_loss = 3.47087\n",
      "epoch no.3 train no.258610  loss = 2.08786 avg_loss = 3.43652\n",
      "epoch no.3 train no.258620  loss = 2.87574 avg_loss = 3.47933\n",
      "epoch no.3 train no.258630  loss = 3.58115 avg_loss = 3.49049\n",
      "epoch no.3 train no.258640  loss = 3.32034 avg_loss = 3.49987\n",
      "epoch no.3 train no.258650  loss = 2.62413 avg_loss = 3.54472\n",
      "epoch no.3 train no.258660  loss = 5.32370 avg_loss = 3.51984\n",
      "epoch no.3 train no.258670  loss = 4.84939 avg_loss = 3.52175\n",
      "epoch no.3 train no.258680  loss = 2.54698 avg_loss = 3.44719\n",
      "epoch no.3 train no.258690  loss = 4.31939 avg_loss = 3.46827\n",
      "epoch no.3 train no.258700  loss = 2.97829 avg_loss = 3.47497\n",
      "epoch no.3 train no.258710  loss = 2.94428 avg_loss = 3.40052\n",
      "epoch no.3 train no.258720  loss = 4.67169 avg_loss = 3.41477\n",
      "epoch no.3 train no.258730  loss = 3.54147 avg_loss = 3.43779\n",
      "epoch no.3 train no.258740  loss = 2.82326 avg_loss = 3.42116\n",
      "epoch no.3 train no.258750  loss = 5.52557 avg_loss = 3.41823\n",
      "epoch no.3 train no.258760  loss = 3.96288 avg_loss = 3.43455\n",
      "epoch no.3 train no.258770  loss = 2.84915 avg_loss = 3.46175\n",
      "epoch no.3 train no.258780  loss = 1.93389 avg_loss = 3.47150\n",
      "epoch no.3 train no.258790  loss = 2.96214 avg_loss = 3.49441\n",
      "epoch no.3 train no.258800  loss = 1.57185 avg_loss = 3.48412\n",
      "epoch no.3 train no.258810  loss = 3.99625 avg_loss = 3.47704\n",
      "epoch no.3 train no.258820  loss = 3.67797 avg_loss = 3.47684\n",
      "epoch no.3 train no.258830  loss = 3.08524 avg_loss = 3.47344\n",
      "epoch no.3 train no.258840  loss = 5.06884 avg_loss = 3.46699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.258850  loss = 3.02294 avg_loss = 3.47989\n",
      "epoch no.3 train no.258860  loss = 1.18128 avg_loss = 3.42520\n",
      "epoch no.3 train no.258870  loss = 3.74077 avg_loss = 3.39281\n",
      "epoch no.3 train no.258880  loss = 3.74361 avg_loss = 3.40188\n",
      "epoch no.3 train no.258890  loss = 3.75177 avg_loss = 3.35776\n",
      "epoch no.3 train no.258900  loss = 2.69027 avg_loss = 3.31523\n",
      "epoch no.3 train no.258910  loss = 4.39360 avg_loss = 3.39270\n",
      "epoch no.3 train no.258920  loss = 3.97323 avg_loss = 3.38587\n",
      "epoch no.3 train no.258930  loss = 2.17291 avg_loss = 3.34097\n",
      "epoch no.3 train no.258940  loss = 3.41768 avg_loss = 3.34730\n",
      "epoch no.3 train no.258950  loss = 3.75656 avg_loss = 3.36167\n",
      "epoch no.3 train no.258960  loss = 3.60174 avg_loss = 3.34337\n",
      "epoch no.3 train no.258970  loss = 4.12773 avg_loss = 3.35650\n",
      "epoch no.3 train no.258980  loss = 4.00613 avg_loss = 3.37360\n",
      "epoch no.3 train no.258990  loss = 3.30967 avg_loss = 3.43163\n",
      "epoch no.3 train no.259000  loss = 3.78703 avg_loss = 3.44561\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 좋은 음악</s>\n",
      "epoch no.3 train no.259010  loss = 3.24544 avg_loss = 3.43902\n",
      "epoch no.3 train no.259020  loss = 4.67737 avg_loss = 3.42432\n",
      "epoch no.3 train no.259030  loss = 2.91832 avg_loss = 3.40125\n",
      "epoch no.3 train no.259040  loss = 1.84813 avg_loss = 3.37912\n",
      "epoch no.3 train no.259050  loss = 4.51921 avg_loss = 3.40099\n",
      "epoch no.3 train no.259060  loss = 4.49870 avg_loss = 3.39347\n",
      "epoch no.3 train no.259070  loss = 3.73054 avg_loss = 3.37453\n",
      "epoch no.3 train no.259080  loss = 3.02836 avg_loss = 3.35799\n",
      "epoch no.3 train no.259090  loss = 2.30852 avg_loss = 3.38454\n",
      "epoch no.3 train no.259100  loss = 3.69936 avg_loss = 3.39625\n",
      "epoch no.3 train no.259110  loss = 2.33195 avg_loss = 3.41392\n",
      "epoch no.3 train no.259120  loss = 2.76602 avg_loss = 3.42517\n",
      "epoch no.3 train no.259130  loss = 3.71336 avg_loss = 3.41829\n",
      "epoch no.3 train no.259140  loss = 2.20348 avg_loss = 3.40750\n",
      "epoch no.3 train no.259150  loss = 2.30892 avg_loss = 3.39228\n",
      "epoch no.3 train no.259160  loss = 5.35926 avg_loss = 3.42919\n",
      "epoch no.3 train no.259170  loss = 5.59525 avg_loss = 3.46831\n",
      "epoch no.3 train no.259180  loss = 3.49902 avg_loss = 3.46924\n",
      "epoch no.3 train no.259190  loss = 4.55609 avg_loss = 3.45751\n",
      "epoch no.3 train no.259200  loss = 6.14896 avg_loss = 3.42029\n",
      "epoch no.3 train no.259210  loss = 3.18391 avg_loss = 3.44312\n",
      "epoch no.3 train no.259220  loss = 3.19949 avg_loss = 3.39379\n",
      "epoch no.3 train no.259230  loss = 4.76831 avg_loss = 3.43796\n",
      "epoch no.3 train no.259240  loss = 4.35956 avg_loss = 3.45748\n",
      "epoch no.3 train no.259250  loss = 3.65234 avg_loss = 3.46341\n",
      "epoch no.3 train no.259260  loss = 5.91067 avg_loss = 3.49485\n",
      "epoch no.3 train no.259270  loss = 2.77582 avg_loss = 3.44975\n",
      "epoch no.3 train no.259280  loss = 2.95578 avg_loss = 3.43944\n",
      "epoch no.3 train no.259290  loss = 3.99644 avg_loss = 3.48260\n",
      "epoch no.3 train no.259300  loss = 4.70721 avg_loss = 3.50266\n",
      "epoch no.3 train no.259310  loss = 4.00094 avg_loss = 3.46959\n",
      "epoch no.3 train no.259320  loss = 3.48585 avg_loss = 3.44956\n",
      "epoch no.3 train no.259330  loss = 2.72694 avg_loss = 3.42522\n",
      "epoch no.3 train no.259340  loss = 1.33306 avg_loss = 3.36908\n",
      "epoch no.3 train no.259350  loss = 1.97580 avg_loss = 3.38883\n",
      "epoch no.3 train no.259360  loss = 4.27625 avg_loss = 3.41193\n",
      "epoch no.3 train no.259370  loss = 4.85377 avg_loss = 3.42562\n",
      "epoch no.3 train no.259380  loss = 2.81315 avg_loss = 3.38340\n",
      "epoch no.3 train no.259390  loss = 2.74872 avg_loss = 3.35891\n",
      "epoch no.3 train no.259400  loss = 3.94661 avg_loss = 3.38356\n",
      "epoch no.3 train no.259410  loss = 3.20678 avg_loss = 3.43832\n",
      "epoch no.3 train no.259420  loss = 1.91242 avg_loss = 3.42295\n",
      "epoch no.3 train no.259430  loss = 4.11699 avg_loss = 3.40207\n",
      "epoch no.3 train no.259440  loss = 3.73489 avg_loss = 3.42057\n",
      "epoch no.3 train no.259450  loss = 1.67986 avg_loss = 3.39900\n",
      "epoch no.3 train no.259460  loss = 3.41226 avg_loss = 3.40965\n",
      "epoch no.3 train no.259470  loss = 4.92864 avg_loss = 3.42398\n",
      "epoch no.3 train no.259480  loss = 4.95403 avg_loss = 3.43210\n",
      "epoch no.3 train no.259490  loss = 3.52809 avg_loss = 3.38679\n",
      "epoch no.3 train no.259500  loss = 4.28192 avg_loss = 3.37471\n",
      "epoch no.3 train no.259510  loss = 2.78082 avg_loss = 3.33032\n",
      "epoch no.3 train no.259520  loss = 2.64367 avg_loss = 3.38361\n",
      "epoch no.3 train no.259530  loss = 1.71933 avg_loss = 3.41524\n",
      "epoch no.3 train no.259540  loss = 4.08965 avg_loss = 3.43075\n",
      "epoch no.3 train no.259550  loss = 3.30901 avg_loss = 3.46632\n",
      "epoch no.3 train no.259560  loss = 2.48741 avg_loss = 3.45499\n",
      "epoch no.3 train no.259570  loss = 2.85478 avg_loss = 3.47589\n",
      "epoch no.3 train no.259580  loss = 3.44575 avg_loss = 3.44818\n",
      "epoch no.3 train no.259590  loss = 3.63379 avg_loss = 3.43856\n",
      "epoch no.3 train no.259600  loss = 3.30204 avg_loss = 3.41425\n",
      "epoch no.3 train no.259610  loss = 3.41736 avg_loss = 3.42936\n",
      "epoch no.3 train no.259620  loss = 4.00218 avg_loss = 3.40719\n",
      "epoch no.3 train no.259630  loss = 3.37299 avg_loss = 3.35545\n",
      "epoch no.3 train no.259640  loss = 1.44920 avg_loss = 3.37143\n",
      "epoch no.3 train no.259650  loss = 2.16118 avg_loss = 3.33992\n",
      "epoch no.3 train no.259660  loss = 3.84724 avg_loss = 3.30801\n",
      "epoch no.3 train no.259670  loss = 3.59078 avg_loss = 3.37272\n",
      "epoch no.3 train no.259680  loss = 2.53262 avg_loss = 3.36905\n",
      "epoch no.3 train no.259690  loss = 1.93569 avg_loss = 3.34936\n",
      "epoch no.3 train no.259700  loss = 3.48007 avg_loss = 3.32721\n",
      "epoch no.3 train no.259710  loss = 4.12081 avg_loss = 3.30874\n",
      "epoch no.3 train no.259720  loss = 3.75598 avg_loss = 3.29122\n",
      "epoch no.3 train no.259730  loss = 3.08856 avg_loss = 3.30527\n",
      "epoch no.3 train no.259740  loss = 4.50524 avg_loss = 3.32136\n",
      "epoch no.3 train no.259750  loss = 4.51287 avg_loss = 3.31402\n",
      "epoch no.3 train no.259760  loss = 3.83732 avg_loss = 3.28820\n",
      "epoch no.3 train no.259770  loss = 2.39723 avg_loss = 3.27374\n",
      "epoch no.3 train no.259780  loss = 3.03303 avg_loss = 3.26592\n",
      "epoch no.3 train no.259790  loss = 3.36678 avg_loss = 3.30790\n",
      "epoch no.3 train no.259800  loss = 3.24988 avg_loss = 3.29413\n",
      "epoch no.3 train no.259810  loss = 3.31963 avg_loss = 3.28392\n",
      "epoch no.3 train no.259820  loss = 3.21831 avg_loss = 3.27010\n",
      "epoch no.3 train no.259830  loss = 3.17340 avg_loss = 3.27684\n",
      "epoch no.3 train no.259840  loss = 2.66515 avg_loss = 3.26125\n",
      "epoch no.3 train no.259850  loss = 3.03612 avg_loss = 3.21368\n",
      "epoch no.3 train no.259860  loss = 3.90106 avg_loss = 3.22966\n",
      "epoch no.3 train no.259870  loss = 3.13500 avg_loss = 3.24226\n",
      "epoch no.3 train no.259880  loss = 4.65750 avg_loss = 3.24258\n",
      "epoch no.3 train no.259890  loss = 5.87540 avg_loss = 3.27850\n",
      "epoch no.3 train no.259900  loss = 2.47005 avg_loss = 3.25982\n",
      "epoch no.3 train no.259910  loss = 6.56947 avg_loss = 3.29652\n",
      "epoch no.3 train no.259920  loss = 4.63985 avg_loss = 3.33640\n",
      "epoch no.3 train no.259930  loss = 2.75607 avg_loss = 3.34776\n",
      "epoch no.3 train no.259940  loss = 3.81942 avg_loss = 3.35822\n",
      "epoch no.3 train no.259950  loss = 3.75469 avg_loss = 3.36641\n",
      "epoch no.3 train no.259960  loss = 3.20372 avg_loss = 3.36917\n",
      "epoch no.3 train no.259970  loss = 2.04127 avg_loss = 3.34838\n",
      "epoch no.3 train no.259980  loss = 2.27181 avg_loss = 3.37815\n",
      "epoch no.3 train no.259990  loss = 2.81898 avg_loss = 3.39717\n",
      "epoch no.3 train no.260000  loss = 3.54767 avg_loss = 3.39035\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '▁노래', '음악', '</s>', '</s>']\n",
      "기분전환하기 좋은 인디음악들</s>\n",
      "epoch no.3 train no.260010  loss = 2.68698 avg_loss = 3.38105\n",
      "epoch no.3 train no.260020  loss = 2.57633 avg_loss = 3.39229\n",
      "epoch no.3 train no.260030  loss = 6.43050 avg_loss = 3.41394\n",
      "epoch no.3 train no.260040  loss = 1.41601 avg_loss = 3.41556\n",
      "epoch no.3 train no.260050  loss = 4.14124 avg_loss = 3.40263\n",
      "epoch no.3 train no.260060  loss = 3.45602 avg_loss = 3.32774\n",
      "epoch no.3 train no.260070  loss = 4.09612 avg_loss = 3.35788\n",
      "epoch no.3 train no.260080  loss = 3.87058 avg_loss = 3.33986\n",
      "epoch no.3 train no.260090  loss = 3.33351 avg_loss = 3.34621\n",
      "epoch no.3 train no.260100  loss = 2.13185 avg_loss = 3.32988\n",
      "epoch no.3 train no.260110  loss = 4.46002 avg_loss = 3.34785\n",
      "epoch no.3 train no.260120  loss = 4.22507 avg_loss = 3.30445\n",
      "epoch no.3 train no.260130  loss = 4.85635 avg_loss = 3.29175\n",
      "epoch no.3 train no.260140  loss = 3.87107 avg_loss = 3.30711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.260150  loss = 2.85609 avg_loss = 3.29859\n",
      "epoch no.3 train no.260160  loss = 2.90311 avg_loss = 3.27456\n",
      "epoch no.3 train no.260170  loss = 3.03070 avg_loss = 3.33876\n",
      "epoch no.3 train no.260180  loss = 3.15583 avg_loss = 3.36302\n",
      "epoch no.3 train no.260190  loss = 4.34080 avg_loss = 3.35547\n",
      "epoch no.3 train no.260200  loss = 2.56064 avg_loss = 3.35041\n",
      "epoch no.3 train no.260210  loss = 2.27842 avg_loss = 3.34023\n",
      "epoch no.3 train no.260220  loss = 2.84193 avg_loss = 3.33498\n",
      "epoch no.3 train no.260230  loss = 4.12287 avg_loss = 3.33266\n",
      "epoch no.3 train no.260240  loss = 2.37361 avg_loss = 3.27426\n",
      "epoch no.3 train no.260250  loss = 1.93703 avg_loss = 3.21461\n",
      "epoch no.3 train no.260260  loss = 2.28238 avg_loss = 3.20606\n",
      "epoch no.3 train no.260270  loss = 4.10314 avg_loss = 3.22053\n",
      "epoch no.3 train no.260280  loss = 3.55895 avg_loss = 3.23179\n",
      "epoch no.3 train no.260290  loss = 3.32809 avg_loss = 3.24537\n",
      "epoch no.3 train no.260300  loss = 3.27022 avg_loss = 3.27159\n",
      "epoch no.3 train no.260310  loss = 3.37594 avg_loss = 3.31293\n",
      "epoch no.3 train no.260320  loss = 1.82817 avg_loss = 3.32988\n",
      "epoch no.3 train no.260330  loss = 3.55999 avg_loss = 3.28991\n",
      "epoch no.3 train no.260340  loss = 5.21340 avg_loss = 3.25853\n",
      "epoch no.3 train no.260350  loss = 2.48586 avg_loss = 3.24452\n",
      "epoch no.3 train no.260360  loss = 3.21106 avg_loss = 3.30774\n",
      "epoch no.3 train no.260370  loss = 4.10765 avg_loss = 3.29937\n",
      "epoch no.3 train no.260380  loss = 1.77713 avg_loss = 3.27289\n",
      "epoch no.3 train no.260390  loss = 2.63229 avg_loss = 3.28143\n",
      "epoch no.3 train no.260400  loss = 4.87874 avg_loss = 3.30642\n",
      "epoch no.3 train no.260410  loss = 3.48313 avg_loss = 3.30297\n",
      "epoch no.3 train no.260420  loss = 2.68381 avg_loss = 3.30560\n",
      "epoch no.3 train no.260430  loss = 2.54177 avg_loss = 3.32351\n",
      "epoch no.3 train no.260440  loss = 2.20397 avg_loss = 3.32597\n",
      "epoch no.3 train no.260450  loss = 2.97482 avg_loss = 3.30141\n",
      "epoch no.3 train no.260460  loss = 3.83002 avg_loss = 3.30824\n",
      "epoch no.3 train no.260470  loss = 3.24767 avg_loss = 3.31188\n",
      "epoch no.3 train no.260480  loss = 3.63440 avg_loss = 3.31673\n",
      "epoch no.3 train no.260490  loss = 6.86384 avg_loss = 3.33166\n",
      "epoch no.3 train no.260500  loss = 2.53910 avg_loss = 3.30434\n",
      "epoch no.3 train no.260510  loss = 4.48998 avg_loss = 3.36449\n",
      "epoch no.3 train no.260520  loss = 5.18268 avg_loss = 3.42261\n",
      "epoch no.3 train no.260530  loss = 3.32255 avg_loss = 3.39392\n",
      "epoch no.3 train no.260540  loss = 3.60900 avg_loss = 3.38806\n",
      "epoch no.3 train no.260550  loss = 2.97629 avg_loss = 3.37523\n",
      "epoch no.3 train no.260560  loss = 4.69759 avg_loss = 3.35209\n",
      "epoch no.3 train no.260570  loss = 2.20563 avg_loss = 3.30973\n",
      "epoch no.3 train no.260580  loss = 3.84019 avg_loss = 3.34321\n",
      "epoch no.3 train no.260590  loss = 2.22081 avg_loss = 3.28419\n",
      "epoch no.3 train no.260600  loss = 3.70248 avg_loss = 3.26422\n",
      "epoch no.3 train no.260610  loss = 2.57885 avg_loss = 3.27303\n",
      "epoch no.3 train no.260620  loss = 3.24223 avg_loss = 3.29059\n",
      "epoch no.3 train no.260630  loss = 3.08895 avg_loss = 3.28218\n",
      "epoch no.3 train no.260640  loss = 3.79772 avg_loss = 3.26952\n",
      "epoch no.3 train no.260650  loss = 2.19464 avg_loss = 3.28972\n",
      "epoch no.3 train no.260660  loss = 2.51015 avg_loss = 3.31818\n",
      "epoch no.3 train no.260670  loss = 3.42138 avg_loss = 3.38062\n",
      "epoch no.3 train no.260680  loss = 2.90514 avg_loss = 3.38501\n",
      "epoch no.3 train no.260690  loss = 4.38236 avg_loss = 3.41244\n",
      "epoch no.3 train no.260700  loss = 2.64448 avg_loss = 3.41657\n",
      "epoch no.3 train no.260710  loss = 2.84930 avg_loss = 3.43039\n",
      "epoch no.3 train no.260720  loss = 2.32369 avg_loss = 3.40137\n",
      "epoch no.3 train no.260730  loss = 2.70518 avg_loss = 3.38785\n",
      "epoch no.3 train no.260740  loss = 3.33145 avg_loss = 3.38563\n",
      "epoch no.3 train no.260750  loss = 2.43264 avg_loss = 3.35196\n",
      "epoch no.3 train no.260760  loss = 3.04570 avg_loss = 3.32033\n",
      "epoch no.3 train no.260770  loss = 3.15289 avg_loss = 3.32427\n",
      "epoch no.3 train no.260780  loss = 5.05378 avg_loss = 3.38516\n",
      "epoch no.3 train no.260790  loss = 2.05588 avg_loss = 3.32527\n",
      "epoch no.3 train no.260800  loss = 3.55858 avg_loss = 3.27857\n",
      "epoch no.3 train no.260810  loss = 4.59081 avg_loss = 3.25744\n",
      "epoch no.3 train no.260820  loss = 3.12373 avg_loss = 3.25962\n",
      "epoch no.3 train no.260830  loss = 2.11368 avg_loss = 3.30698\n",
      "epoch no.3 train no.260840  loss = 3.08515 avg_loss = 3.35140\n",
      "epoch no.3 train no.260850  loss = 4.20265 avg_loss = 3.39104\n",
      "epoch no.3 train no.260860  loss = 3.70267 avg_loss = 3.39519\n",
      "epoch no.3 train no.260870  loss = 3.83695 avg_loss = 3.37569\n",
      "epoch no.3 train no.260880  loss = 3.36807 avg_loss = 3.32829\n",
      "epoch no.3 train no.260890  loss = 4.39177 avg_loss = 3.30553\n",
      "epoch no.3 train no.260900  loss = 2.46195 avg_loss = 3.30491\n",
      "epoch no.3 train no.260910  loss = 4.60946 avg_loss = 3.29225\n",
      "epoch no.3 train no.260920  loss = 4.32414 avg_loss = 3.29505\n",
      "epoch no.3 train no.260930  loss = 2.34353 avg_loss = 3.32343\n",
      "epoch no.3 train no.260940  loss = 3.26786 avg_loss = 3.31077\n",
      "epoch no.3 train no.260950  loss = 2.34783 avg_loss = 3.29663\n",
      "epoch no.3 train no.260960  loss = 3.49834 avg_loss = 3.31934\n",
      "epoch no.3 train no.260970  loss = 4.83795 avg_loss = 3.36598\n",
      "epoch no.3 train no.260980  loss = 3.10645 avg_loss = 3.33623\n",
      "epoch no.3 train no.260990  loss = 2.38117 avg_loss = 3.30080\n",
      "epoch no.3 train no.261000  loss = 2.77617 avg_loss = 3.32709\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁좋은', '▁신나는', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.3 train no.261010  loss = 2.27465 avg_loss = 3.27850\n",
      "epoch no.3 train no.261020  loss = 2.70778 avg_loss = 3.28829\n",
      "epoch no.3 train no.261030  loss = 4.71288 avg_loss = 3.29072\n",
      "epoch no.3 train no.261040  loss = 5.28488 avg_loss = 3.30259\n",
      "epoch no.3 train no.261050  loss = 2.74152 avg_loss = 3.35552\n",
      "epoch no.3 train no.261060  loss = 2.08916 avg_loss = 3.33382\n",
      "epoch no.3 train no.261070  loss = 4.29419 avg_loss = 3.35648\n",
      "epoch no.3 train no.261080  loss = 3.06026 avg_loss = 3.44057\n",
      "epoch no.3 train no.261090  loss = 3.72975 avg_loss = 3.42985\n",
      "epoch no.3 train no.261100  loss = 3.06756 avg_loss = 3.40469\n",
      "epoch no.3 train no.261110  loss = 4.42645 avg_loss = 3.40895\n",
      "epoch no.3 train no.261120  loss = 2.68673 avg_loss = 3.41760\n",
      "epoch no.3 train no.261130  loss = 2.73891 avg_loss = 3.42454\n",
      "epoch no.3 train no.261140  loss = 3.63005 avg_loss = 3.42662\n",
      "epoch no.3 train no.261150  loss = 2.20234 avg_loss = 3.37421\n",
      "epoch no.3 train no.261160  loss = 3.75233 avg_loss = 3.42929\n",
      "epoch no.3 train no.261170  loss = 2.06055 avg_loss = 3.37514\n",
      "epoch no.3 train no.261180  loss = 3.00167 avg_loss = 3.37523\n",
      "epoch no.3 train no.261190  loss = 4.16500 avg_loss = 3.33268\n",
      "epoch no.3 train no.261200  loss = 3.67623 avg_loss = 3.34806\n",
      "epoch no.3 train no.261210  loss = 5.88424 avg_loss = 3.42304\n",
      "epoch no.3 train no.261220  loss = 3.99668 avg_loss = 3.42910\n",
      "epoch no.3 train no.261230  loss = 3.02519 avg_loss = 3.38743\n",
      "epoch no.3 train no.261240  loss = 2.82459 avg_loss = 3.33962\n",
      "epoch no.3 train no.261250  loss = 3.13474 avg_loss = 3.33458\n",
      "epoch no.3 train no.261260  loss = 3.53480 avg_loss = 3.38085\n",
      "epoch no.3 train no.261270  loss = 3.24036 avg_loss = 3.34617\n",
      "epoch no.3 train no.261280  loss = 3.23311 avg_loss = 3.34178\n",
      "epoch no.3 train no.261290  loss = 3.51696 avg_loss = 3.36009\n",
      "epoch no.3 train no.261300  loss = 4.12593 avg_loss = 3.40273\n",
      "epoch no.3 train no.261310  loss = 2.75725 avg_loss = 3.42604\n",
      "epoch no.3 train no.261320  loss = 3.33910 avg_loss = 3.44490\n",
      "epoch no.3 train no.261330  loss = 2.87309 avg_loss = 3.42439\n",
      "epoch no.3 train no.261340  loss = 2.41585 avg_loss = 3.42136\n",
      "epoch no.3 train no.261350  loss = 3.91686 avg_loss = 3.38458\n",
      "epoch no.3 train no.261360  loss = 3.11130 avg_loss = 3.38064\n",
      "epoch no.3 train no.261370  loss = 2.26257 avg_loss = 3.38666\n",
      "epoch no.3 train no.261380  loss = 3.22347 avg_loss = 3.36289\n",
      "epoch no.3 train no.261390  loss = 1.65259 avg_loss = 3.31952\n",
      "epoch no.3 train no.261400  loss = 2.91348 avg_loss = 3.31190\n",
      "epoch no.3 train no.261410  loss = 2.79735 avg_loss = 3.35405\n",
      "epoch no.3 train no.261420  loss = 2.32999 avg_loss = 3.35999\n",
      "epoch no.3 train no.261430  loss = 2.23693 avg_loss = 3.35629\n",
      "epoch no.3 train no.261440  loss = 4.35604 avg_loss = 3.40531\n",
      "epoch no.3 train no.261450  loss = 3.37050 avg_loss = 3.44878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.261460  loss = 3.12068 avg_loss = 3.44397\n",
      "epoch no.3 train no.261470  loss = 2.95437 avg_loss = 3.43642\n",
      "epoch no.3 train no.261480  loss = 3.69401 avg_loss = 3.42931\n",
      "epoch no.3 train no.261490  loss = 4.86773 avg_loss = 3.41253\n",
      "epoch no.3 train no.261500  loss = 3.91984 avg_loss = 3.35244\n",
      "epoch no.3 train no.261510  loss = 3.84973 avg_loss = 3.37725\n",
      "epoch no.3 train no.261520  loss = 3.53387 avg_loss = 3.38234\n",
      "epoch no.3 train no.261530  loss = 1.94381 avg_loss = 3.33576\n",
      "epoch no.3 train no.261540  loss = 2.85723 avg_loss = 3.34999\n",
      "epoch no.3 train no.261550  loss = 4.27322 avg_loss = 3.34386\n",
      "epoch no.3 train no.261560  loss = 2.48011 avg_loss = 3.31664\n",
      "epoch no.3 train no.261570  loss = 2.71707 avg_loss = 3.32110\n",
      "epoch no.3 train no.261580  loss = 2.56061 avg_loss = 3.33687\n",
      "epoch no.3 train no.261590  loss = 3.63087 avg_loss = 3.38896\n",
      "epoch no.3 train no.261600  loss = 4.87827 avg_loss = 3.37521\n",
      "epoch no.3 train no.261610  loss = 3.79312 avg_loss = 3.45084\n",
      "epoch no.3 train no.261620  loss = 3.33432 avg_loss = 3.43219\n",
      "epoch no.3 train no.261630  loss = 4.48521 avg_loss = 3.47872\n",
      "epoch no.3 train no.261640  loss = 2.02955 avg_loss = 3.48184\n",
      "epoch no.3 train no.261650  loss = 4.68029 avg_loss = 3.44800\n",
      "epoch no.3 train no.261660  loss = 3.67210 avg_loss = 3.44894\n",
      "epoch no.3 train no.261670  loss = 3.24263 avg_loss = 3.38171\n",
      "epoch no.3 train no.261680  loss = 1.97944 avg_loss = 3.35329\n",
      "epoch no.3 train no.261690  loss = 3.54640 avg_loss = 3.40150\n",
      "epoch no.3 train no.261700  loss = 2.41607 avg_loss = 3.39875\n",
      "epoch no.3 train no.261710  loss = 3.78148 avg_loss = 3.39829\n",
      "epoch no.3 train no.261720  loss = 3.18860 avg_loss = 3.44257\n",
      "epoch no.3 train no.261730  loss = 3.03431 avg_loss = 3.41997\n",
      "epoch no.3 train no.261740  loss = 3.93197 avg_loss = 3.39679\n",
      "epoch no.3 train no.261750  loss = 3.24680 avg_loss = 3.42000\n",
      "epoch no.3 train no.261760  loss = 3.27904 avg_loss = 3.42196\n",
      "epoch no.3 train no.261770  loss = 3.62930 avg_loss = 3.36850\n",
      "epoch no.3 train no.261780  loss = 2.38434 avg_loss = 3.34762\n",
      "epoch no.3 train no.261790  loss = 4.05582 avg_loss = 3.34749\n",
      "epoch no.3 train no.261800  loss = 2.31008 avg_loss = 3.34017\n",
      "epoch no.3 train no.261810  loss = 3.26880 avg_loss = 3.33759\n",
      "epoch no.3 train no.261820  loss = 2.47137 avg_loss = 3.35318\n",
      "epoch no.3 train no.261830  loss = 2.96684 avg_loss = 3.31848\n",
      "epoch no.3 train no.261840  loss = 5.31857 avg_loss = 3.34982\n",
      "epoch no.3 train no.261850  loss = 4.11252 avg_loss = 3.36770\n",
      "epoch no.3 train no.261860  loss = 5.34319 avg_loss = 3.39577\n",
      "epoch no.3 train no.261870  loss = 2.90031 avg_loss = 3.37999\n",
      "epoch no.3 train no.261880  loss = 1.84929 avg_loss = 3.36707\n",
      "epoch no.3 train no.261890  loss = 4.05120 avg_loss = 3.38072\n",
      "epoch no.3 train no.261900  loss = 3.46414 avg_loss = 3.39560\n",
      "epoch no.3 train no.261910  loss = 3.85734 avg_loss = 3.41910\n",
      "epoch no.3 train no.261920  loss = 4.63650 avg_loss = 3.42771\n",
      "epoch no.3 train no.261930  loss = 1.95844 avg_loss = 3.39821\n",
      "epoch no.3 train no.261940  loss = 1.35445 avg_loss = 3.40207\n",
      "epoch no.3 train no.261950  loss = 6.08914 avg_loss = 3.42164\n",
      "epoch no.3 train no.261960  loss = 2.76796 avg_loss = 3.39757\n",
      "epoch no.3 train no.261970  loss = 2.22945 avg_loss = 3.35529\n",
      "epoch no.3 train no.261980  loss = 4.45150 avg_loss = 3.36686\n",
      "epoch no.3 train no.261990  loss = 4.00042 avg_loss = 3.41174\n",
      "epoch no.3 train no.262000  loss = 2.73858 avg_loss = 3.40814\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는', '▁신나는', '</s>']\n",
      "기분전환이 필요할 때 듣는 음악</s>\n",
      "epoch no.3 train no.262010  loss = 3.71586 avg_loss = 3.45824\n",
      "epoch no.3 train no.262020  loss = 3.71902 avg_loss = 3.48194\n",
      "epoch no.3 train no.262030  loss = 3.18426 avg_loss = 3.43211\n",
      "epoch no.3 train no.262040  loss = 3.21922 avg_loss = 3.43033\n",
      "epoch no.3 train no.262050  loss = 2.90093 avg_loss = 3.44005\n",
      "epoch no.3 train no.262060  loss = 3.06815 avg_loss = 3.43712\n",
      "epoch no.3 train no.262070  loss = 2.50695 avg_loss = 3.39441\n",
      "epoch no.3 train no.262080  loss = 6.23745 avg_loss = 3.41474\n",
      "epoch no.3 train no.262090  loss = 3.23696 avg_loss = 3.40102\n",
      "epoch no.3 train no.262100  loss = 5.91257 avg_loss = 3.39899\n",
      "epoch no.3 train no.262110  loss = 3.39265 avg_loss = 3.45470\n",
      "epoch no.3 train no.262120  loss = 3.64499 avg_loss = 3.44152\n",
      "epoch no.3 train no.262130  loss = 3.77525 avg_loss = 3.44371\n",
      "epoch no.3 train no.262140  loss = 6.17471 avg_loss = 3.49215\n",
      "epoch no.3 train no.262150  loss = 2.33840 avg_loss = 3.49549\n",
      "epoch no.3 train no.262160  loss = 4.34826 avg_loss = 3.52634\n",
      "epoch no.3 train no.262170  loss = 2.54444 avg_loss = 3.49963\n",
      "epoch no.3 train no.262180  loss = 4.74829 avg_loss = 3.47737\n",
      "epoch no.3 train no.262190  loss = 3.07704 avg_loss = 3.48033\n",
      "epoch no.3 train no.262200  loss = 2.82997 avg_loss = 3.47207\n",
      "epoch no.3 train no.262210  loss = 3.02652 avg_loss = 3.48485\n",
      "epoch no.3 train no.262220  loss = 5.45134 avg_loss = 3.54283\n",
      "epoch no.3 train no.262230  loss = 2.41205 avg_loss = 3.59381\n",
      "epoch no.3 train no.262240  loss = 3.05870 avg_loss = 3.51896\n",
      "epoch no.3 train no.262250  loss = 2.59646 avg_loss = 3.54213\n",
      "epoch no.3 train no.262260  loss = 3.85059 avg_loss = 3.55136\n",
      "epoch no.3 train no.262270  loss = 2.57144 avg_loss = 3.54989\n",
      "epoch no.3 train no.262280  loss = 0.90031 avg_loss = 3.47878\n",
      "epoch no.3 train no.262290  loss = 2.95573 avg_loss = 3.42477\n",
      "epoch no.3 train no.262300  loss = 2.85003 avg_loss = 3.34759\n",
      "epoch no.3 train no.262310  loss = 3.17636 avg_loss = 3.38179\n",
      "epoch no.3 train no.262320  loss = 4.39774 avg_loss = 3.36833\n",
      "epoch no.3 train no.262330  loss = 3.16749 avg_loss = 3.32640\n",
      "epoch no.3 train no.262340  loss = 5.42261 avg_loss = 3.37321\n",
      "epoch no.3 train no.262350  loss = 4.41588 avg_loss = 3.39929\n",
      "epoch no.3 train no.262360  loss = 2.62726 avg_loss = 3.35287\n",
      "epoch no.3 train no.262370  loss = 2.91971 avg_loss = 3.35157\n",
      "epoch no.3 train no.262380  loss = 3.48032 avg_loss = 3.30594\n",
      "epoch no.3 train no.262390  loss = 2.87378 avg_loss = 3.32652\n",
      "epoch no.3 train no.262400  loss = 2.36104 avg_loss = 3.30260\n",
      "epoch no.3 train no.262410  loss = 4.83646 avg_loss = 3.33221\n",
      "epoch no.3 train no.262420  loss = 4.77181 avg_loss = 3.38576\n",
      "epoch no.3 train no.262430  loss = 3.24293 avg_loss = 3.41870\n",
      "epoch no.3 train no.262440  loss = 3.38938 avg_loss = 3.39735\n",
      "epoch no.3 train no.262450  loss = 3.79033 avg_loss = 3.39197\n",
      "epoch no.3 train no.262460  loss = 1.92185 avg_loss = 3.36612\n",
      "epoch no.3 train no.262470  loss = 2.84312 avg_loss = 3.36726\n",
      "epoch no.3 train no.262480  loss = 3.23155 avg_loss = 3.38998\n",
      "epoch no.3 train no.262490  loss = 4.55872 avg_loss = 3.35577\n",
      "epoch no.3 train no.262500  loss = 3.77867 avg_loss = 3.36452\n",
      "epoch no.3 train no.262510  loss = 4.22604 avg_loss = 3.38515\n",
      "epoch no.3 train no.262520  loss = 3.61573 avg_loss = 3.34643\n",
      "epoch no.3 train no.262530  loss = 4.92745 avg_loss = 3.33415\n",
      "epoch no.3 train no.262540  loss = 2.55625 avg_loss = 3.31175\n",
      "epoch no.3 train no.262550  loss = 3.17900 avg_loss = 3.32264\n",
      "epoch no.3 train no.262560  loss = 3.62609 avg_loss = 3.35763\n",
      "epoch no.3 train no.262570  loss = 3.19937 avg_loss = 3.35697\n",
      "epoch no.3 train no.262580  loss = 3.66260 avg_loss = 3.43018\n",
      "epoch no.3 train no.262590  loss = 3.61973 avg_loss = 3.38229\n",
      "epoch no.3 train no.262600  loss = 3.17095 avg_loss = 3.44001\n",
      "epoch no.3 train no.262610  loss = 2.98221 avg_loss = 3.41267\n",
      "epoch no.3 train no.262620  loss = 3.43964 avg_loss = 3.45132\n",
      "epoch no.3 train no.262630  loss = 2.88542 avg_loss = 3.46014\n",
      "epoch no.3 train no.262640  loss = 2.72787 avg_loss = 3.49626\n",
      "epoch no.3 train no.262650  loss = 2.59116 avg_loss = 3.51957\n",
      "epoch no.3 train no.262660  loss = 4.39628 avg_loss = 3.49637\n",
      "epoch no.3 train no.262670  loss = 2.76849 avg_loss = 3.46636\n",
      "epoch no.3 train no.262680  loss = 3.28913 avg_loss = 3.48030\n",
      "epoch no.3 train no.262690  loss = 3.31004 avg_loss = 3.47918\n",
      "epoch no.3 train no.262700  loss = 3.15258 avg_loss = 3.49512\n",
      "epoch no.3 train no.262710  loss = 2.48680 avg_loss = 3.54951\n",
      "epoch no.3 train no.262720  loss = 4.22144 avg_loss = 3.51801\n",
      "epoch no.3 train no.262730  loss = 2.63372 avg_loss = 3.43238\n",
      "epoch no.3 train no.262740  loss = 3.30063 avg_loss = 3.45820\n",
      "epoch no.3 train no.262750  loss = 2.88251 avg_loss = 3.44672\n",
      "epoch no.3 train no.262760  loss = 3.18653 avg_loss = 3.42354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.262770  loss = 2.21536 avg_loss = 3.40003\n",
      "epoch no.3 train no.262780  loss = 4.94071 avg_loss = 3.40168\n",
      "epoch no.3 train no.262790  loss = 2.97761 avg_loss = 3.41047\n",
      "epoch no.3 train no.262800  loss = 1.20900 avg_loss = 3.41097\n",
      "epoch no.3 train no.262810  loss = 4.18245 avg_loss = 3.40230\n",
      "epoch no.3 train no.262820  loss = 3.40916 avg_loss = 3.39769\n",
      "epoch no.3 train no.262830  loss = 4.61807 avg_loss = 3.41955\n",
      "epoch no.3 train no.262840  loss = 2.30578 avg_loss = 3.41650\n",
      "epoch no.3 train no.262850  loss = 2.88695 avg_loss = 3.37274\n",
      "epoch no.3 train no.262860  loss = 2.92023 avg_loss = 3.39917\n",
      "epoch no.3 train no.262870  loss = 2.09761 avg_loss = 3.41744\n",
      "epoch no.3 train no.262880  loss = 4.54274 avg_loss = 3.36854\n",
      "epoch no.3 train no.262890  loss = 3.02979 avg_loss = 3.40214\n",
      "epoch no.3 train no.262900  loss = 2.84026 avg_loss = 3.40873\n",
      "epoch no.3 train no.262910  loss = 2.81208 avg_loss = 3.44818\n",
      "epoch no.3 train no.262920  loss = 4.07369 avg_loss = 3.42603\n",
      "epoch no.3 train no.262930  loss = 3.10315 avg_loss = 3.44179\n",
      "epoch no.3 train no.262940  loss = 4.06112 avg_loss = 3.43293\n",
      "epoch no.3 train no.262950  loss = 2.09896 avg_loss = 3.43288\n",
      "epoch no.3 train no.262960  loss = 1.78770 avg_loss = 3.38477\n",
      "epoch no.3 train no.262970  loss = 1.97232 avg_loss = 3.41291\n",
      "epoch no.3 train no.262980  loss = 3.38312 avg_loss = 3.40731\n",
      "epoch no.3 train no.262990  loss = 4.54585 avg_loss = 3.40554\n",
      "epoch no.3 train no.263000  loss = 2.27728 avg_loss = 3.42324\n",
      "6\n",
      "to_tokens: ['▁비', '전환', '용', '▁필요할', '때', '</s>', '▁신나는', '</s>', '</s>']\n",
      "기분전환이 필요할때 듣는 음악들</s>\n",
      "epoch no.3 train no.263010  loss = 3.69793 avg_loss = 3.43047\n",
      "epoch no.3 train no.263020  loss = 5.11336 avg_loss = 3.40615\n",
      "epoch no.3 train no.263030  loss = 3.07836 avg_loss = 3.39296\n",
      "epoch no.3 train no.263040  loss = 3.05751 avg_loss = 3.34916\n",
      "epoch no.3 train no.263050  loss = 3.91964 avg_loss = 3.34172\n",
      "epoch no.3 train no.263060  loss = 3.53943 avg_loss = 3.34203\n",
      "epoch no.3 train no.263070  loss = 2.33772 avg_loss = 3.31586\n",
      "epoch no.3 train no.263080  loss = 2.40357 avg_loss = 3.33998\n",
      "epoch no.3 train no.263090  loss = 1.89081 avg_loss = 3.31351\n",
      "epoch no.3 train no.263100  loss = 3.85615 avg_loss = 3.32233\n",
      "epoch no.3 train no.263110  loss = 3.12332 avg_loss = 3.34923\n",
      "epoch no.3 train no.263120  loss = 4.46437 avg_loss = 3.36831\n",
      "epoch no.3 train no.263130  loss = 2.99522 avg_loss = 3.35101\n",
      "epoch no.3 train no.263140  loss = 4.21470 avg_loss = 3.35373\n",
      "epoch no.3 train no.263150  loss = 2.65262 avg_loss = 3.31902\n",
      "epoch no.3 train no.263160  loss = 3.15906 avg_loss = 3.32145\n",
      "epoch no.3 train no.263170  loss = 3.44625 avg_loss = 3.30533\n",
      "epoch no.3 train no.263180  loss = 2.84325 avg_loss = 3.25338\n",
      "epoch no.3 train no.263190  loss = 4.27800 avg_loss = 3.24350\n",
      "epoch no.3 train no.263200  loss = 4.56891 avg_loss = 3.23390\n",
      "epoch no.3 train no.263210  loss = 2.48750 avg_loss = 3.31535\n",
      "epoch no.3 train no.263220  loss = 3.91187 avg_loss = 3.28855\n",
      "epoch no.3 train no.263230  loss = 3.43073 avg_loss = 3.35190\n",
      "epoch no.3 train no.263240  loss = 4.26319 avg_loss = 3.32880\n",
      "epoch no.3 train no.263250  loss = 2.14220 avg_loss = 3.36194\n",
      "epoch no.3 train no.263260  loss = 2.54760 avg_loss = 3.37085\n",
      "epoch no.3 train no.263270  loss = 2.87123 avg_loss = 3.37417\n",
      "epoch no.3 train no.263280  loss = 2.24305 avg_loss = 3.38869\n",
      "epoch no.3 train no.263290  loss = 3.61471 avg_loss = 3.33346\n",
      "epoch no.3 train no.263300  loss = 2.97983 avg_loss = 3.32739\n",
      "epoch no.3 train no.263310  loss = 2.54404 avg_loss = 3.37173\n",
      "epoch no.3 train no.263320  loss = 3.21039 avg_loss = 3.35453\n",
      "epoch no.3 train no.263330  loss = 4.01234 avg_loss = 3.35127\n",
      "epoch no.3 train no.263340  loss = 3.12090 avg_loss = 3.36862\n",
      "epoch no.3 train no.263350  loss = 2.85651 avg_loss = 3.34317\n",
      "epoch no.3 train no.263360  loss = 2.05168 avg_loss = 3.31198\n",
      "epoch no.3 train no.263370  loss = 4.61238 avg_loss = 3.33433\n",
      "epoch no.3 train no.263380  loss = 5.72874 avg_loss = 3.38300\n",
      "epoch no.3 train no.263390  loss = 2.96499 avg_loss = 3.34941\n",
      "epoch no.3 train no.263400  loss = 2.32187 avg_loss = 3.29945\n",
      "epoch no.3 train no.263410  loss = 2.69423 avg_loss = 3.29530\n",
      "epoch no.3 train no.263420  loss = 1.85017 avg_loss = 3.30242\n",
      "epoch no.3 train no.263430  loss = 3.96964 avg_loss = 3.26161\n",
      "epoch no.3 train no.263440  loss = 3.21945 avg_loss = 3.24828\n",
      "epoch no.3 train no.263450  loss = 3.71293 avg_loss = 3.23869\n",
      "epoch no.3 train no.263460  loss = 3.29587 avg_loss = 3.25501\n",
      "epoch no.3 train no.263470  loss = 1.83644 avg_loss = 3.25436\n",
      "epoch no.3 train no.263480  loss = 4.36304 avg_loss = 3.25947\n",
      "epoch no.3 train no.263490  loss = 2.82211 avg_loss = 3.26011\n",
      "epoch no.3 train no.263500  loss = 2.87429 avg_loss = 3.28355\n",
      "epoch no.3 train no.263510  loss = 3.06902 avg_loss = 3.22453\n",
      "epoch no.3 train no.263520  loss = 4.33402 avg_loss = 3.25535\n",
      "epoch no.3 train no.263530  loss = 2.78555 avg_loss = 3.27392\n",
      "epoch no.3 train no.263540  loss = 3.07332 avg_loss = 3.26954\n",
      "epoch no.3 train no.263550  loss = 3.81072 avg_loss = 3.26840\n",
      "epoch no.3 train no.263560  loss = 2.97772 avg_loss = 3.29928\n",
      "epoch no.3 train no.263570  loss = 3.24160 avg_loss = 3.32951\n",
      "epoch no.3 train no.263580  loss = 4.44438 avg_loss = 3.36062\n",
      "epoch no.3 train no.263590  loss = 2.97151 avg_loss = 3.35604\n",
      "epoch no.3 train no.263600  loss = 2.36668 avg_loss = 3.35337\n",
      "epoch no.3 train no.263610  loss = 4.18117 avg_loss = 3.46698\n",
      "epoch no.3 train no.263620  loss = 4.16709 avg_loss = 3.47167\n",
      "epoch no.3 train no.263630  loss = 2.55565 avg_loss = 3.44206\n",
      "epoch no.3 train no.263640  loss = 2.35056 avg_loss = 3.44028\n",
      "epoch no.3 train no.263650  loss = 3.09347 avg_loss = 3.44181\n",
      "epoch no.3 train no.263660  loss = 4.87504 avg_loss = 3.46566\n",
      "epoch no.3 train no.263670  loss = 4.26766 avg_loss = 3.47401\n",
      "epoch no.3 train no.263680  loss = 3.03465 avg_loss = 3.41701\n",
      "epoch no.3 train no.263690  loss = 2.34866 avg_loss = 3.39212\n",
      "epoch no.3 train no.263700  loss = 3.00357 avg_loss = 3.35829\n",
      "epoch no.3 train no.263710  loss = 2.65314 avg_loss = 3.32654\n",
      "epoch no.3 train no.263720  loss = 3.71036 avg_loss = 3.33922\n",
      "epoch no.3 train no.263730  loss = 3.07097 avg_loss = 3.35255\n",
      "epoch no.3 train no.263740  loss = 3.04533 avg_loss = 3.34596\n",
      "epoch no.3 train no.263750  loss = 2.37551 avg_loss = 3.31937\n",
      "epoch no.3 train no.263760  loss = 2.31364 avg_loss = 3.32262\n",
      "epoch no.3 train no.263770  loss = 3.22766 avg_loss = 3.33786\n",
      "epoch no.3 train no.263780  loss = 2.71584 avg_loss = 3.32780\n",
      "epoch no.3 train no.263790  loss = 3.93494 avg_loss = 3.34798\n",
      "epoch no.3 train no.263800  loss = 2.16960 avg_loss = 3.32702\n",
      "epoch no.3 train no.263810  loss = 1.98717 avg_loss = 3.33136\n",
      "epoch no.3 train no.263820  loss = 2.84549 avg_loss = 3.32831\n",
      "epoch no.3 train no.263830  loss = 4.37379 avg_loss = 3.36414\n",
      "epoch no.3 train no.263840  loss = 3.53176 avg_loss = 3.31080\n",
      "epoch no.3 train no.263850  loss = 2.14148 avg_loss = 3.33177\n",
      "epoch no.3 train no.263860  loss = 3.66493 avg_loss = 3.36927\n",
      "epoch no.3 train no.263870  loss = 1.98879 avg_loss = 3.31813\n",
      "epoch no.3 train no.263880  loss = 5.17362 avg_loss = 3.33572\n",
      "epoch no.3 train no.263890  loss = 4.12644 avg_loss = 3.28229\n",
      "epoch no.3 train no.263900  loss = 4.02760 avg_loss = 3.28382\n",
      "epoch no.3 train no.263910  loss = 3.11600 avg_loss = 3.26653\n",
      "epoch no.3 train no.263920  loss = 3.74726 avg_loss = 3.29698\n",
      "epoch no.3 train no.263930  loss = 3.85692 avg_loss = 3.27909\n",
      "epoch no.3 train no.263940  loss = 3.22774 avg_loss = 3.29666\n",
      "epoch no.3 train no.263950  loss = 3.69272 avg_loss = 3.31559\n",
      "epoch no.3 train no.263960  loss = 4.96050 avg_loss = 3.36505\n",
      "epoch no.3 train no.263970  loss = 3.80809 avg_loss = 3.33410\n",
      "epoch no.3 train no.263980  loss = 2.52686 avg_loss = 3.30965\n",
      "epoch no.3 train no.263990  loss = 4.41820 avg_loss = 3.33703\n",
      "epoch no.3 train no.264000  loss = 3.75900 avg_loss = 3.33310\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>', '▁신나는', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.3 train no.264010  loss = 3.05982 avg_loss = 3.34082\n",
      "epoch no.3 train no.264020  loss = 2.32529 avg_loss = 3.33770\n",
      "epoch no.3 train no.264030  loss = 4.50206 avg_loss = 3.33224\n",
      "epoch no.3 train no.264040  loss = 2.84551 avg_loss = 3.30793\n",
      "epoch no.3 train no.264050  loss = 4.06260 avg_loss = 3.28622\n",
      "epoch no.3 train no.264060  loss = 3.17652 avg_loss = 3.25520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.264070  loss = 4.62976 avg_loss = 3.33029\n",
      "epoch no.3 train no.264080  loss = 2.95612 avg_loss = 3.32718\n",
      "epoch no.3 train no.264090  loss = 4.52766 avg_loss = 3.32740\n",
      "epoch no.3 train no.264100  loss = 3.67080 avg_loss = 3.33108\n",
      "epoch no.3 train no.264110  loss = 2.90280 avg_loss = 3.31322\n",
      "epoch no.3 train no.264120  loss = 3.47075 avg_loss = 3.28325\n",
      "epoch no.3 train no.264130  loss = 1.98502 avg_loss = 3.20708\n",
      "epoch no.3 train no.264140  loss = 4.19246 avg_loss = 3.18651\n",
      "epoch no.3 train no.264150  loss = 2.42787 avg_loss = 3.17496\n",
      "epoch no.3 train no.264160  loss = 5.23310 avg_loss = 3.20522\n",
      "epoch no.3 train no.264170  loss = 4.03900 avg_loss = 3.24033\n",
      "epoch no.3 train no.264180  loss = 3.21735 avg_loss = 3.24946\n",
      "epoch no.3 train no.264190  loss = 3.65653 avg_loss = 3.33512\n",
      "epoch no.3 train no.264200  loss = 2.20359 avg_loss = 3.36913\n",
      "epoch no.3 train no.264210  loss = 2.49946 avg_loss = 3.38786\n",
      "epoch no.3 train no.264220  loss = 2.11943 avg_loss = 3.37830\n",
      "epoch no.3 train no.264230  loss = 1.85184 avg_loss = 3.34952\n",
      "epoch no.3 train no.264240  loss = 2.77340 avg_loss = 3.37207\n",
      "epoch no.3 train no.264250  loss = 4.30372 avg_loss = 3.34248\n",
      "epoch no.3 train no.264260  loss = 4.52237 avg_loss = 3.35702\n",
      "epoch no.3 train no.264270  loss = 5.75706 avg_loss = 3.37130\n",
      "epoch no.3 train no.264280  loss = 3.58384 avg_loss = 3.38885\n",
      "epoch no.3 train no.264290  loss = 2.51466 avg_loss = 3.37565\n",
      "epoch no.3 train no.264300  loss = 2.60577 avg_loss = 3.35634\n",
      "epoch no.3 train no.264310  loss = 2.33043 avg_loss = 3.30922\n",
      "epoch no.3 train no.264320  loss = 2.79697 avg_loss = 3.34422\n",
      "epoch no.3 train no.264330  loss = 3.75901 avg_loss = 3.36496\n",
      "epoch no.3 train no.264340  loss = 2.54663 avg_loss = 3.34294\n",
      "epoch no.3 train no.264350  loss = 4.27474 avg_loss = 3.37835\n",
      "epoch no.3 train no.264360  loss = 3.75959 avg_loss = 3.41365\n",
      "epoch no.3 train no.264370  loss = 3.22159 avg_loss = 3.43049\n",
      "epoch no.3 train no.264380  loss = 3.20856 avg_loss = 3.42605\n",
      "epoch no.3 train no.264390  loss = 2.34664 avg_loss = 3.43121\n",
      "epoch no.3 train no.264400  loss = 3.34490 avg_loss = 3.40613\n",
      "epoch no.3 train no.264410  loss = 3.99843 avg_loss = 3.41798\n",
      "epoch no.3 train no.264420  loss = 3.99757 avg_loss = 3.44445\n",
      "epoch no.3 train no.264430  loss = 3.97383 avg_loss = 3.45052\n",
      "epoch no.3 train no.264440  loss = 2.30063 avg_loss = 3.45143\n",
      "epoch no.3 train no.264450  loss = 3.37041 avg_loss = 3.41120\n",
      "epoch no.3 train no.264460  loss = 2.40859 avg_loss = 3.45041\n",
      "epoch no.3 train no.264470  loss = 3.18101 avg_loss = 3.45129\n",
      "epoch no.3 train no.264480  loss = 2.50147 avg_loss = 3.44379\n",
      "epoch no.3 train no.264490  loss = 2.42850 avg_loss = 3.43604\n",
      "epoch no.3 train no.264500  loss = 1.89087 avg_loss = 3.42769\n",
      "epoch no.3 train no.264510  loss = 3.62309 avg_loss = 3.44280\n",
      "epoch no.3 train no.264520  loss = 2.49925 avg_loss = 3.42706\n",
      "epoch no.3 train no.264530  loss = 3.45805 avg_loss = 3.44233\n",
      "epoch no.3 train no.264540  loss = 2.21340 avg_loss = 3.45513\n",
      "epoch no.3 train no.264550  loss = 3.71385 avg_loss = 3.45421\n",
      "epoch no.3 train no.264560  loss = 3.97589 avg_loss = 3.46138\n",
      "epoch no.3 train no.264570  loss = 3.26986 avg_loss = 3.47290\n",
      "epoch no.3 train no.264580  loss = 2.19254 avg_loss = 3.44785\n",
      "epoch no.3 train no.264590  loss = 3.35812 avg_loss = 3.43464\n",
      "epoch no.3 train no.264600  loss = 4.41190 avg_loss = 3.45651\n",
      "epoch no.3 train no.264610  loss = 2.86892 avg_loss = 3.46116\n",
      "epoch no.3 train no.264620  loss = 4.10383 avg_loss = 3.45701\n",
      "epoch no.3 train no.264630  loss = 2.77157 avg_loss = 3.43518\n",
      "epoch no.3 train no.264640  loss = 3.47159 avg_loss = 3.44716\n",
      "epoch no.3 train no.264650  loss = 3.47946 avg_loss = 3.41917\n",
      "epoch no.3 train no.264660  loss = 4.45672 avg_loss = 3.46997\n",
      "epoch no.3 train no.264670  loss = 5.02142 avg_loss = 3.48409\n",
      "epoch no.3 train no.264680  loss = 3.14089 avg_loss = 3.50796\n",
      "epoch no.3 train no.264690  loss = 3.64285 avg_loss = 3.48235\n",
      "epoch no.3 train no.264700  loss = 2.34269 avg_loss = 3.43166\n",
      "epoch no.3 train no.264710  loss = 1.40270 avg_loss = 3.45751\n",
      "epoch no.3 train no.264720  loss = 2.64450 avg_loss = 3.42488\n",
      "epoch no.3 train no.264730  loss = 3.01001 avg_loss = 3.43820\n",
      "epoch no.3 train no.264740  loss = 4.20587 avg_loss = 3.42049\n",
      "epoch no.3 train no.264750  loss = 4.49705 avg_loss = 3.45059\n",
      "epoch no.3 train no.264760  loss = 2.11291 avg_loss = 3.44600\n",
      "epoch no.3 train no.264770  loss = 2.92436 avg_loss = 3.38590\n",
      "epoch no.3 train no.264780  loss = 4.28825 avg_loss = 3.40317\n",
      "epoch no.3 train no.264790  loss = 3.30179 avg_loss = 3.41397\n",
      "epoch no.3 train no.264800  loss = 3.35688 avg_loss = 3.41337\n",
      "epoch no.3 train no.264810  loss = 4.55291 avg_loss = 3.43004\n",
      "epoch no.3 train no.264820  loss = 4.25174 avg_loss = 3.38059\n",
      "epoch no.3 train no.264830  loss = 4.35957 avg_loss = 3.42703\n",
      "epoch no.3 train no.264840  loss = 2.89870 avg_loss = 3.42216\n",
      "epoch no.3 train no.264850  loss = 2.78674 avg_loss = 3.44240\n",
      "epoch no.3 train no.264860  loss = 2.87606 avg_loss = 3.46434\n",
      "epoch no.3 train no.264870  loss = 5.89986 avg_loss = 3.47125\n",
      "epoch no.3 train no.264880  loss = 1.27786 avg_loss = 3.47077\n",
      "epoch no.3 train no.264890  loss = 3.15077 avg_loss = 3.48737\n",
      "epoch no.3 train no.264900  loss = 3.45599 avg_loss = 3.49701\n",
      "epoch no.3 train no.264910  loss = 3.46382 avg_loss = 3.45627\n",
      "epoch no.3 train no.264920  loss = 2.95333 avg_loss = 3.45285\n",
      "epoch no.3 train no.264930  loss = 3.17367 avg_loss = 3.39125\n",
      "epoch no.3 train no.264940  loss = 3.25193 avg_loss = 3.43261\n",
      "epoch no.3 train no.264950  loss = 2.74367 avg_loss = 3.38003\n",
      "epoch no.3 train no.264960  loss = 2.11017 avg_loss = 3.41751\n",
      "epoch no.3 train no.264970  loss = 3.14248 avg_loss = 3.42240\n",
      "epoch no.3 train no.264980  loss = 3.80146 avg_loss = 3.43856\n",
      "epoch no.3 train no.264990  loss = 3.41316 avg_loss = 3.46345\n",
      "epoch no.3 train no.265000  loss = 4.11895 avg_loss = 3.45702\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁위한', '▁팝', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.3 train no.265010  loss = 2.82777 avg_loss = 3.43203\n",
      "epoch no.3 train no.265020  loss = 3.83467 avg_loss = 3.40859\n",
      "epoch no.3 train no.265030  loss = 4.91734 avg_loss = 3.42015\n",
      "epoch no.3 train no.265040  loss = 3.06972 avg_loss = 3.39188\n",
      "epoch no.3 train no.265050  loss = 3.47811 avg_loss = 3.41394\n",
      "epoch no.3 train no.265060  loss = 2.75153 avg_loss = 3.39158\n",
      "epoch no.3 train no.265070  loss = 3.67236 avg_loss = 3.40495\n",
      "epoch no.3 train no.265080  loss = 5.06842 avg_loss = 3.41872\n",
      "epoch no.3 train no.265090  loss = 2.53371 avg_loss = 3.39232\n",
      "epoch no.3 train no.265100  loss = 1.70324 avg_loss = 3.40239\n",
      "epoch no.3 train no.265110  loss = 3.22274 avg_loss = 3.43788\n",
      "epoch no.3 train no.265120  loss = 3.44348 avg_loss = 3.43986\n",
      "epoch no.3 train no.265130  loss = 3.13241 avg_loss = 3.45063\n",
      "epoch no.3 train no.265140  loss = 3.24291 avg_loss = 3.42222\n",
      "epoch no.3 train no.265150  loss = 3.12281 avg_loss = 3.42784\n",
      "epoch no.3 train no.265160  loss = 3.14137 avg_loss = 3.43094\n",
      "epoch no.3 train no.265170  loss = 3.48371 avg_loss = 3.43219\n",
      "epoch no.3 train no.265180  loss = 4.45290 avg_loss = 3.43598\n",
      "epoch no.3 train no.265190  loss = 2.78752 avg_loss = 3.43035\n",
      "epoch no.3 train no.265200  loss = 2.65959 avg_loss = 3.43977\n",
      "epoch no.3 train no.265210  loss = 4.28579 avg_loss = 3.45939\n",
      "epoch no.3 train no.265220  loss = 3.49551 avg_loss = 3.47731\n",
      "epoch no.3 train no.265230  loss = 3.19080 avg_loss = 3.45563\n",
      "epoch no.3 train no.265240  loss = 3.77607 avg_loss = 3.46055\n",
      "epoch no.3 train no.265250  loss = 2.86385 avg_loss = 3.43590\n",
      "epoch no.3 train no.265260  loss = 2.83738 avg_loss = 3.40542\n",
      "epoch no.3 train no.265270  loss = 3.14195 avg_loss = 3.40061\n",
      "epoch no.3 train no.265280  loss = 4.42634 avg_loss = 3.37849\n",
      "epoch no.3 train no.265290  loss = 2.77818 avg_loss = 3.36425\n",
      "epoch no.3 train no.265300  loss = 3.50946 avg_loss = 3.36828\n",
      "epoch no.3 train no.265310  loss = 3.04323 avg_loss = 3.37153\n",
      "epoch no.3 train no.265320  loss = 4.52994 avg_loss = 3.42142\n",
      "epoch no.3 train no.265330  loss = 2.41455 avg_loss = 3.39460\n",
      "epoch no.3 train no.265340  loss = 2.22362 avg_loss = 3.34714\n",
      "epoch no.3 train no.265350  loss = 2.91956 avg_loss = 3.36333\n",
      "epoch no.3 train no.265360  loss = 4.03242 avg_loss = 3.39822\n",
      "epoch no.3 train no.265370  loss = 2.53203 avg_loss = 3.40028\n",
      "epoch no.3 train no.265380  loss = 3.80836 avg_loss = 3.42809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.265390  loss = 2.12357 avg_loss = 3.43068\n",
      "epoch no.3 train no.265400  loss = 3.76947 avg_loss = 3.43673\n",
      "epoch no.3 train no.265410  loss = 2.41280 avg_loss = 3.47842\n",
      "epoch no.3 train no.265420  loss = 2.18849 avg_loss = 3.43081\n",
      "epoch no.3 train no.265430  loss = 5.26994 avg_loss = 3.52102\n",
      "epoch no.3 train no.265440  loss = 2.56883 avg_loss = 3.50870\n",
      "epoch no.3 train no.265450  loss = 3.38704 avg_loss = 3.49805\n",
      "epoch no.3 train no.265460  loss = 1.11852 avg_loss = 3.46653\n",
      "epoch no.3 train no.265470  loss = 3.74571 avg_loss = 3.47723\n",
      "epoch no.3 train no.265480  loss = 4.44761 avg_loss = 3.49384\n",
      "epoch no.3 train no.265490  loss = 2.08814 avg_loss = 3.44702\n",
      "epoch no.3 train no.265500  loss = 3.51371 avg_loss = 3.44960\n",
      "epoch no.3 train no.265510  loss = 3.46510 avg_loss = 3.51785\n",
      "epoch no.3 train no.265520  loss = 2.49869 avg_loss = 3.46856\n",
      "epoch no.3 train no.265530  loss = 2.99548 avg_loss = 3.43678\n",
      "epoch no.3 train no.265540  loss = 3.73582 avg_loss = 3.47629\n",
      "epoch no.3 train no.265550  loss = 2.58857 avg_loss = 3.46978\n",
      "epoch no.3 train no.265560  loss = 6.19604 avg_loss = 3.53961\n",
      "epoch no.3 train no.265570  loss = 3.02656 avg_loss = 3.50037\n",
      "epoch no.3 train no.265580  loss = 2.38683 avg_loss = 3.48157\n",
      "epoch no.3 train no.265590  loss = 3.86608 avg_loss = 3.47324\n",
      "epoch no.3 train no.265600  loss = 2.90286 avg_loss = 3.44665\n",
      "epoch no.3 train no.265610  loss = 3.81602 avg_loss = 3.47077\n",
      "epoch no.3 train no.265620  loss = 2.79902 avg_loss = 3.46153\n",
      "epoch no.3 train no.265630  loss = 4.12718 avg_loss = 3.46122\n",
      "epoch no.3 train no.265640  loss = 4.68110 avg_loss = 3.48556\n",
      "epoch no.3 train no.265650  loss = 3.03404 avg_loss = 3.47461\n",
      "epoch no.3 train no.265660  loss = 2.53246 avg_loss = 3.46798\n",
      "epoch no.3 train no.265670  loss = 3.46181 avg_loss = 3.46256\n",
      "epoch no.3 train no.265680  loss = 2.16946 avg_loss = 3.38761\n",
      "epoch no.3 train no.265690  loss = 2.98255 avg_loss = 3.42309\n",
      "epoch no.3 train no.265700  loss = 3.13459 avg_loss = 3.42305\n",
      "epoch no.3 train no.265710  loss = 2.65878 avg_loss = 3.43077\n",
      "epoch no.3 train no.265720  loss = 4.27008 avg_loss = 3.38894\n",
      "epoch no.3 train no.265730  loss = 4.68827 avg_loss = 3.41542\n",
      "epoch no.3 train no.265740  loss = 3.83138 avg_loss = 3.42463\n",
      "epoch no.3 train no.265750  loss = 3.38539 avg_loss = 3.40184\n",
      "epoch no.3 train no.265760  loss = 2.37628 avg_loss = 3.42107\n",
      "epoch no.3 train no.265770  loss = 7.26784 avg_loss = 3.46369\n",
      "epoch no.3 train no.265780  loss = 2.24217 avg_loss = 3.40568\n",
      "epoch no.3 train no.265790  loss = 2.79476 avg_loss = 3.37731\n",
      "epoch no.3 train no.265800  loss = 4.63751 avg_loss = 3.40357\n",
      "epoch no.3 train no.265810  loss = 2.47802 avg_loss = 3.37463\n",
      "epoch no.3 train no.265820  loss = 2.26588 avg_loss = 3.29791\n",
      "epoch no.3 train no.265830  loss = 2.87007 avg_loss = 3.28683\n",
      "epoch no.3 train no.265840  loss = 3.99536 avg_loss = 3.29657\n",
      "epoch no.3 train no.265850  loss = 3.22796 avg_loss = 3.30814\n",
      "epoch no.3 train no.265860  loss = 4.42206 avg_loss = 3.31748\n",
      "epoch no.3 train no.265870  loss = 2.17870 avg_loss = 3.25739\n",
      "epoch no.3 train no.265880  loss = 2.77783 avg_loss = 3.24327\n",
      "epoch no.3 train no.265890  loss = 4.03425 avg_loss = 3.21700\n",
      "epoch no.3 train no.265900  loss = 3.87639 avg_loss = 3.24379\n",
      "epoch no.3 train no.265910  loss = 3.09166 avg_loss = 3.26137\n",
      "epoch no.3 train no.265920  loss = 4.69627 avg_loss = 3.32610\n",
      "epoch no.3 train no.265930  loss = 4.76218 avg_loss = 3.32131\n",
      "epoch no.3 train no.265940  loss = 4.03900 avg_loss = 3.30862\n",
      "epoch no.3 train no.265950  loss = 4.52334 avg_loss = 3.38896\n",
      "epoch no.3 train no.265960  loss = 4.30175 avg_loss = 3.42261\n",
      "epoch no.3 train no.265970  loss = 3.42645 avg_loss = 3.41648\n",
      "epoch no.3 train no.265980  loss = 3.84830 avg_loss = 3.39498\n",
      "epoch no.3 train no.265990  loss = 2.31659 avg_loss = 3.35979\n",
      "epoch no.3 train no.266000  loss = 4.70768 avg_loss = 3.35556\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁딱', '▁노래', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.3 train no.266010  loss = 3.53308 avg_loss = 3.36324\n",
      "epoch no.3 train no.266020  loss = 3.63802 avg_loss = 3.38496\n",
      "epoch no.3 train no.266030  loss = 3.98393 avg_loss = 3.38126\n",
      "epoch no.3 train no.266040  loss = 2.10540 avg_loss = 3.39045\n",
      "epoch no.3 train no.266050  loss = 4.10584 avg_loss = 3.37354\n",
      "epoch no.3 train no.266060  loss = 2.91535 avg_loss = 3.33637\n",
      "epoch no.3 train no.266070  loss = 3.13568 avg_loss = 3.31428\n",
      "epoch no.3 train no.266080  loss = 3.13557 avg_loss = 3.34979\n",
      "epoch no.3 train no.266090  loss = 4.65450 avg_loss = 3.31133\n",
      "epoch no.3 train no.266100  loss = 3.02739 avg_loss = 3.31516\n",
      "epoch no.3 train no.266110  loss = 2.55686 avg_loss = 3.33565\n",
      "epoch no.3 train no.266120  loss = 4.31542 avg_loss = 3.33520\n",
      "epoch no.3 train no.266130  loss = 4.05638 avg_loss = 3.37301\n",
      "epoch no.3 train no.266140  loss = 1.64432 avg_loss = 3.34544\n",
      "epoch no.3 train no.266150  loss = 3.55450 avg_loss = 3.34025\n",
      "epoch no.3 train no.266160  loss = 2.81957 avg_loss = 3.28025\n",
      "epoch no.3 train no.266170  loss = 4.28639 avg_loss = 3.28921\n",
      "epoch no.3 train no.266180  loss = 5.18205 avg_loss = 3.30929\n",
      "epoch no.3 train no.266190  loss = 4.59314 avg_loss = 3.32608\n",
      "epoch no.3 train no.266200  loss = 1.94871 avg_loss = 3.32783\n",
      "epoch no.3 train no.266210  loss = 4.47772 avg_loss = 3.32630\n",
      "epoch no.3 train no.266220  loss = 2.29458 avg_loss = 3.31208\n",
      "epoch no.3 train no.266230  loss = 4.38642 avg_loss = 3.29882\n",
      "epoch no.3 train no.266240  loss = 2.17760 avg_loss = 3.26461\n",
      "epoch no.3 train no.266250  loss = 2.80449 avg_loss = 3.28183\n",
      "epoch no.3 train no.266260  loss = 2.68124 avg_loss = 3.28794\n",
      "epoch no.3 train no.266270  loss = 3.71261 avg_loss = 3.28295\n",
      "epoch no.3 train no.266280  loss = 3.65534 avg_loss = 3.29198\n",
      "epoch no.3 train no.266290  loss = 3.42385 avg_loss = 3.30962\n",
      "epoch no.3 train no.266300  loss = 3.29100 avg_loss = 3.29377\n",
      "epoch no.3 train no.266310  loss = 5.72385 avg_loss = 3.31480\n",
      "epoch no.3 train no.266320  loss = 4.63382 avg_loss = 3.34509\n",
      "epoch no.3 train no.266330  loss = 5.45507 avg_loss = 3.33281\n",
      "epoch no.3 train no.266340  loss = 4.68968 avg_loss = 3.36308\n",
      "epoch no.3 train no.266350  loss = 2.75506 avg_loss = 3.34345\n",
      "epoch no.3 train no.266360  loss = 3.57211 avg_loss = 3.32332\n",
      "epoch no.3 train no.266370  loss = 4.13493 avg_loss = 3.31069\n",
      "epoch no.3 train no.266380  loss = 3.18038 avg_loss = 3.31401\n",
      "epoch no.3 train no.266390  loss = 2.23261 avg_loss = 3.29564\n",
      "epoch no.3 train no.266400  loss = 3.29988 avg_loss = 3.30284\n",
      "epoch no.3 train no.266410  loss = 3.00156 avg_loss = 3.27780\n",
      "epoch no.3 train no.266420  loss = 2.15986 avg_loss = 3.27906\n",
      "epoch no.3 train no.266430  loss = 1.74669 avg_loss = 3.29517\n",
      "epoch no.3 train no.266440  loss = 2.49031 avg_loss = 3.30696\n",
      "epoch no.3 train no.266450  loss = 4.29531 avg_loss = 3.31961\n",
      "epoch no.3 train no.266460  loss = 3.45986 avg_loss = 3.33922\n",
      "epoch no.3 train no.266470  loss = 3.50689 avg_loss = 3.35489\n",
      "epoch no.3 train no.266480  loss = 4.67289 avg_loss = 3.37917\n",
      "epoch no.3 train no.266490  loss = 2.61430 avg_loss = 3.32890\n",
      "epoch no.3 train no.266500  loss = 4.83777 avg_loss = 3.37797\n",
      "epoch no.3 train no.266510  loss = 4.91295 avg_loss = 3.39439\n",
      "epoch no.3 train no.266520  loss = 3.02964 avg_loss = 3.42869\n",
      "epoch no.3 train no.266530  loss = 4.33283 avg_loss = 3.39679\n",
      "epoch no.3 train no.266540  loss = 2.42718 avg_loss = 3.40325\n",
      "epoch no.3 train no.266550  loss = 4.91778 avg_loss = 3.42931\n",
      "epoch no.3 train no.266560  loss = 2.78930 avg_loss = 3.46915\n",
      "epoch no.3 train no.266570  loss = 2.40310 avg_loss = 3.43885\n",
      "epoch no.3 train no.266580  loss = 2.90425 avg_loss = 3.46573\n",
      "epoch no.3 train no.266590  loss = 3.07672 avg_loss = 3.41957\n",
      "epoch no.3 train no.266600  loss = 5.07713 avg_loss = 3.45559\n",
      "epoch no.3 train no.266610  loss = 4.44422 avg_loss = 3.46966\n",
      "epoch no.3 train no.266620  loss = 4.27062 avg_loss = 3.47080\n",
      "epoch no.3 train no.266630  loss = 2.90658 avg_loss = 3.42381\n",
      "epoch no.3 train no.266640  loss = 3.95948 avg_loss = 3.41034\n",
      "epoch no.3 train no.266650  loss = 2.90464 avg_loss = 3.41435\n",
      "epoch no.3 train no.266660  loss = 4.57034 avg_loss = 3.42193\n",
      "epoch no.3 train no.266670  loss = 3.95682 avg_loss = 3.41661\n",
      "epoch no.3 train no.266680  loss = 2.86896 avg_loss = 3.35847\n",
      "epoch no.3 train no.266690  loss = 2.14164 avg_loss = 3.34740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.266700  loss = 3.79942 avg_loss = 3.37517\n",
      "epoch no.3 train no.266710  loss = 5.96935 avg_loss = 3.38501\n",
      "epoch no.3 train no.266720  loss = 3.41235 avg_loss = 3.39445\n",
      "epoch no.3 train no.266730  loss = 4.15361 avg_loss = 3.35326\n",
      "epoch no.3 train no.266740  loss = 3.40360 avg_loss = 3.38579\n",
      "epoch no.3 train no.266750  loss = 2.05113 avg_loss = 3.36748\n",
      "epoch no.3 train no.266760  loss = 3.31286 avg_loss = 3.38105\n",
      "epoch no.3 train no.266770  loss = 2.95329 avg_loss = 3.35238\n",
      "epoch no.3 train no.266780  loss = 3.89939 avg_loss = 3.34976\n",
      "epoch no.3 train no.266790  loss = 2.20540 avg_loss = 3.33888\n",
      "epoch no.3 train no.266800  loss = 3.08168 avg_loss = 3.36747\n",
      "epoch no.3 train no.266810  loss = 3.77003 avg_loss = 3.36048\n",
      "epoch no.3 train no.266820  loss = 3.41505 avg_loss = 3.35874\n",
      "epoch no.3 train no.266830  loss = 4.02809 avg_loss = 3.35313\n",
      "epoch no.3 train no.266840  loss = 2.47440 avg_loss = 3.31890\n",
      "epoch no.3 train no.266850  loss = 3.73468 avg_loss = 3.32705\n",
      "epoch no.3 train no.266860  loss = 2.84530 avg_loss = 3.30105\n",
      "epoch no.3 train no.266870  loss = 3.58381 avg_loss = 3.31533\n",
      "epoch no.3 train no.266880  loss = 4.51483 avg_loss = 3.34124\n",
      "epoch no.3 train no.266890  loss = 3.04631 avg_loss = 3.31120\n",
      "epoch no.3 train no.266900  loss = 1.62412 avg_loss = 3.30159\n",
      "epoch no.3 train no.266910  loss = 2.76321 avg_loss = 3.28718\n",
      "epoch no.3 train no.266920  loss = 2.36091 avg_loss = 3.28199\n",
      "epoch no.3 train no.266930  loss = 4.54971 avg_loss = 3.34330\n",
      "epoch no.3 train no.266940  loss = 2.99799 avg_loss = 3.32379\n",
      "epoch no.3 train no.266950  loss = 2.83485 avg_loss = 3.35594\n",
      "epoch no.3 train no.266960  loss = 3.88761 avg_loss = 3.37688\n",
      "epoch no.3 train no.266970  loss = 3.75331 avg_loss = 3.33278\n",
      "epoch no.3 train no.266980  loss = 5.03700 avg_loss = 3.33649\n",
      "epoch no.3 train no.266990  loss = 2.65909 avg_loss = 3.33154\n",
      "epoch no.3 train no.267000  loss = 1.75090 avg_loss = 3.33263\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.3 train no.267010  loss = 3.24478 avg_loss = 3.36168\n",
      "epoch no.3 train no.267020  loss = 3.48702 avg_loss = 3.35012\n",
      "epoch no.3 train no.267030  loss = 3.14896 avg_loss = 3.33358\n",
      "epoch no.3 train no.267040  loss = 2.62339 avg_loss = 3.35671\n",
      "epoch no.3 train no.267050  loss = 4.06586 avg_loss = 3.35065\n",
      "epoch no.3 train no.267060  loss = 4.35386 avg_loss = 3.35535\n",
      "epoch no.3 train no.267070  loss = 4.51555 avg_loss = 3.34997\n",
      "epoch no.3 train no.267080  loss = 2.95220 avg_loss = 3.37001\n",
      "epoch no.3 train no.267090  loss = 3.64126 avg_loss = 3.41221\n",
      "epoch no.3 train no.267100  loss = 2.80030 avg_loss = 3.37452\n",
      "epoch no.3 train no.267110  loss = 2.88336 avg_loss = 3.35023\n",
      "epoch no.3 train no.267120  loss = 2.29372 avg_loss = 3.38186\n",
      "epoch no.3 train no.267130  loss = 4.95310 avg_loss = 3.38764\n",
      "epoch no.3 train no.267140  loss = 3.24654 avg_loss = 3.37108\n",
      "epoch no.3 train no.267150  loss = 2.06885 avg_loss = 3.30890\n",
      "epoch no.3 train no.267160  loss = 3.08612 avg_loss = 3.28161\n",
      "epoch no.3 train no.267170  loss = 3.14856 avg_loss = 3.27301\n",
      "epoch no.3 train no.267180  loss = 4.78800 avg_loss = 3.29078\n",
      "epoch no.3 train no.267190  loss = 4.07613 avg_loss = 3.27738\n",
      "epoch no.3 train no.267200  loss = 3.31742 avg_loss = 3.31646\n",
      "epoch no.3 train no.267210  loss = 2.54810 avg_loss = 3.32200\n",
      "epoch no.3 train no.267220  loss = 3.52730 avg_loss = 3.34548\n",
      "epoch no.3 train no.267230  loss = 3.61499 avg_loss = 3.38101\n",
      "epoch no.3 train no.267240  loss = 5.37934 avg_loss = 3.39030\n",
      "epoch no.3 train no.267250  loss = 3.24087 avg_loss = 3.33212\n",
      "epoch no.3 train no.267260  loss = 2.51342 avg_loss = 3.31193\n",
      "epoch no.3 train no.267270  loss = 3.81263 avg_loss = 3.32477\n",
      "epoch no.3 train no.267280  loss = 4.18370 avg_loss = 3.38776\n",
      "epoch no.3 train no.267290  loss = 1.99021 avg_loss = 3.36882\n",
      "epoch no.3 train no.267300  loss = 4.71742 avg_loss = 3.37964\n",
      "epoch no.3 train no.267310  loss = 3.09570 avg_loss = 3.37668\n",
      "epoch no.3 train no.267320  loss = 3.53905 avg_loss = 3.39145\n",
      "epoch no.3 train no.267330  loss = 3.40425 avg_loss = 3.37221\n",
      "epoch no.3 train no.267340  loss = 1.99557 avg_loss = 3.36713\n",
      "epoch no.3 train no.267350  loss = 3.77731 avg_loss = 3.35235\n",
      "epoch no.3 train no.267360  loss = 3.55326 avg_loss = 3.36057\n",
      "epoch no.3 train no.267370  loss = 3.71645 avg_loss = 3.39151\n",
      "epoch no.3 train no.267380  loss = 4.44232 avg_loss = 3.40249\n",
      "epoch no.3 train no.267390  loss = 3.93595 avg_loss = 3.45872\n",
      "epoch no.3 train no.267400  loss = 3.87171 avg_loss = 3.44983\n",
      "epoch no.3 train no.267410  loss = 3.57094 avg_loss = 3.45092\n",
      "epoch no.3 train no.267420  loss = 5.62498 avg_loss = 3.45363\n",
      "epoch no.3 train no.267430  loss = 3.23820 avg_loss = 3.45063\n",
      "epoch no.3 train no.267440  loss = 3.67684 avg_loss = 3.45690\n",
      "epoch no.3 train no.267450  loss = 4.52285 avg_loss = 3.44429\n",
      "epoch no.3 train no.267460  loss = 1.39859 avg_loss = 3.38007\n",
      "epoch no.3 train no.267470  loss = 4.21819 avg_loss = 3.37114\n",
      "epoch no.3 train no.267480  loss = 3.03673 avg_loss = 3.37559\n",
      "epoch no.3 train no.267490  loss = 3.48705 avg_loss = 3.38781\n",
      "epoch no.3 train no.267500  loss = 2.52684 avg_loss = 3.35923\n",
      "epoch no.3 train no.267510  loss = 5.09765 avg_loss = 3.41557\n",
      "epoch no.3 train no.267520  loss = 4.49702 avg_loss = 3.39459\n",
      "epoch no.3 train no.267530  loss = 4.54154 avg_loss = 3.38507\n",
      "epoch no.3 train no.267540  loss = 4.83971 avg_loss = 3.38822\n",
      "epoch no.3 train no.267550  loss = 3.95844 avg_loss = 3.38133\n",
      "epoch no.3 train no.267560  loss = 3.42905 avg_loss = 3.38907\n",
      "epoch no.3 train no.267570  loss = 3.57119 avg_loss = 3.43646\n",
      "epoch no.3 train no.267580  loss = 5.53476 avg_loss = 3.47017\n",
      "epoch no.3 train no.267590  loss = 2.31931 avg_loss = 3.45550\n",
      "epoch no.3 train no.267600  loss = 4.07537 avg_loss = 3.43762\n",
      "epoch no.3 train no.267610  loss = 3.09277 avg_loss = 3.43219\n",
      "epoch no.3 train no.267620  loss = 3.82280 avg_loss = 3.49248\n",
      "epoch no.3 train no.267630  loss = 2.59437 avg_loss = 3.45272\n",
      "epoch no.3 train no.267640  loss = 4.44187 avg_loss = 3.43092\n",
      "epoch no.3 train no.267650  loss = 4.06652 avg_loss = 3.45985\n",
      "epoch no.3 train no.267660  loss = 3.27119 avg_loss = 3.43295\n",
      "epoch no.3 train no.267670  loss = 3.31814 avg_loss = 3.40450\n",
      "epoch no.3 train no.267680  loss = 2.07394 avg_loss = 3.38233\n",
      "epoch no.3 train no.267690  loss = 2.43208 avg_loss = 3.40574\n",
      "epoch no.3 train no.267700  loss = 3.20277 avg_loss = 3.37129\n",
      "epoch no.3 train no.267710  loss = 4.16907 avg_loss = 3.41400\n",
      "epoch no.3 train no.267720  loss = 3.40057 avg_loss = 3.42684\n",
      "epoch no.3 train no.267730  loss = 3.25202 avg_loss = 3.42950\n",
      "epoch no.3 train no.267740  loss = 2.84787 avg_loss = 3.43332\n",
      "epoch no.3 train no.267750  loss = 4.32737 avg_loss = 3.42536\n",
      "epoch no.3 train no.267760  loss = 2.33531 avg_loss = 3.39860\n",
      "epoch no.3 train no.267770  loss = 3.71464 avg_loss = 3.43035\n",
      "epoch no.3 train no.267780  loss = 2.26062 avg_loss = 3.39564\n",
      "epoch no.3 train no.267790  loss = 2.70831 avg_loss = 3.36761\n",
      "epoch no.3 train no.267800  loss = 2.14350 avg_loss = 3.34365\n",
      "epoch no.3 train no.267810  loss = 2.27324 avg_loss = 3.34112\n",
      "epoch no.3 train no.267820  loss = 3.96857 avg_loss = 3.36157\n",
      "epoch no.3 train no.267830  loss = 2.79832 avg_loss = 3.38820\n",
      "epoch no.3 train no.267840  loss = 2.20484 avg_loss = 3.37996\n",
      "epoch no.3 train no.267850  loss = 5.50983 avg_loss = 3.34492\n",
      "epoch no.3 train no.267860  loss = 4.29251 avg_loss = 3.35338\n",
      "epoch no.3 train no.267870  loss = 3.35870 avg_loss = 3.40395\n",
      "epoch no.3 train no.267880  loss = 4.49486 avg_loss = 3.39789\n",
      "epoch no.3 train no.267890  loss = 3.99266 avg_loss = 3.37962\n",
      "epoch no.3 train no.267900  loss = 5.62095 avg_loss = 3.37288\n",
      "epoch no.3 train no.267910  loss = 4.16750 avg_loss = 3.39983\n",
      "epoch no.3 train no.267920  loss = 3.25644 avg_loss = 3.42372\n",
      "epoch no.3 train no.267930  loss = 4.03392 avg_loss = 3.39371\n",
      "epoch no.3 train no.267940  loss = 2.76315 avg_loss = 3.33377\n",
      "epoch no.3 train no.267950  loss = 2.87418 avg_loss = 3.32568\n",
      "epoch no.3 train no.267960  loss = 4.43847 avg_loss = 3.30283\n",
      "epoch no.3 train no.267970  loss = 3.21166 avg_loss = 3.30661\n",
      "epoch no.3 train no.267980  loss = 3.25505 avg_loss = 3.30605\n",
      "epoch no.3 train no.267990  loss = 2.27588 avg_loss = 3.35663\n",
      "epoch no.3 train no.268000  loss = 3.67078 avg_loss = 3.40591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "to_tokens: ['▁', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.3 train no.268010  loss = 3.51324 avg_loss = 3.42733\n",
      "epoch no.3 train no.268020  loss = 2.42218 avg_loss = 3.39672\n",
      "epoch no.3 train no.268030  loss = 3.71016 avg_loss = 3.39355\n",
      "epoch no.3 train no.268040  loss = 3.45893 avg_loss = 3.33521\n",
      "epoch no.3 train no.268050  loss = 3.94949 avg_loss = 3.31906\n",
      "epoch no.3 train no.268060  loss = 1.27471 avg_loss = 3.29516\n",
      "epoch no.3 train no.268070  loss = 3.55915 avg_loss = 3.31270\n",
      "epoch no.3 train no.268080  loss = 3.96647 avg_loss = 3.34218\n",
      "epoch no.3 train no.268090  loss = 3.71999 avg_loss = 3.29273\n",
      "epoch no.3 train no.268100  loss = 3.68165 avg_loss = 3.29715\n",
      "epoch no.3 train no.268110  loss = 5.63824 avg_loss = 3.35675\n",
      "epoch no.3 train no.268120  loss = 6.26846 avg_loss = 3.38405\n",
      "epoch no.3 train no.268130  loss = 2.56727 avg_loss = 3.39387\n",
      "epoch no.3 train no.268140  loss = 3.57746 avg_loss = 3.39573\n",
      "epoch no.3 train no.268150  loss = 3.05059 avg_loss = 3.41572\n",
      "epoch no.3 train no.268160  loss = 3.30057 avg_loss = 3.38591\n",
      "epoch no.3 train no.268170  loss = 3.59739 avg_loss = 3.43628\n",
      "epoch no.3 train no.268180  loss = 2.76425 avg_loss = 3.42043\n",
      "epoch no.3 train no.268190  loss = 3.56860 avg_loss = 3.41420\n",
      "epoch no.3 train no.268200  loss = 2.42576 avg_loss = 3.41499\n",
      "epoch no.3 train no.268210  loss = 3.16991 avg_loss = 3.38018\n",
      "epoch no.3 train no.268220  loss = 3.18194 avg_loss = 3.39701\n",
      "epoch no.3 train no.268230  loss = 2.79416 avg_loss = 3.39648\n",
      "epoch no.3 train no.268240  loss = 3.77575 avg_loss = 3.39478\n",
      "epoch no.3 train no.268250  loss = 3.31207 avg_loss = 3.41227\n",
      "epoch no.3 train no.268260  loss = 2.55931 avg_loss = 3.42654\n",
      "epoch no.3 train no.268270  loss = 3.55823 avg_loss = 3.44370\n",
      "epoch no.3 train no.268280  loss = 4.17623 avg_loss = 3.44077\n",
      "epoch no.3 train no.268290  loss = 3.65798 avg_loss = 3.41123\n",
      "epoch no.3 train no.268300  loss = 2.27922 avg_loss = 3.41445\n",
      "epoch no.3 train no.268310  loss = 2.59302 avg_loss = 3.37164\n",
      "epoch no.3 train no.268320  loss = 4.10082 avg_loss = 3.34306\n",
      "epoch no.3 train no.268330  loss = 5.75193 avg_loss = 3.32528\n",
      "epoch no.3 train no.268340  loss = 2.75594 avg_loss = 3.33400\n",
      "epoch no.3 train no.268350  loss = 2.11801 avg_loss = 3.33519\n",
      "epoch no.3 train no.268360  loss = 3.27280 avg_loss = 3.33842\n",
      "epoch no.3 train no.268370  loss = 4.01164 avg_loss = 3.37316\n",
      "epoch no.3 train no.268380  loss = 3.38786 avg_loss = 3.34870\n",
      "epoch no.3 train no.268390  loss = 5.05817 avg_loss = 3.40064\n",
      "epoch no.3 train no.268400  loss = 3.79811 avg_loss = 3.42952\n",
      "epoch no.3 train no.268410  loss = 5.17393 avg_loss = 3.44721\n",
      "epoch no.3 train no.268420  loss = 3.66545 avg_loss = 3.42327\n",
      "epoch no.3 train no.268430  loss = 4.76793 avg_loss = 3.42394\n",
      "epoch no.3 train no.268440  loss = 3.56022 avg_loss = 3.40592\n",
      "epoch no.3 train no.268450  loss = 3.03703 avg_loss = 3.45832\n",
      "epoch no.3 train no.268460  loss = 4.62539 avg_loss = 3.46135\n",
      "epoch no.3 train no.268470  loss = 2.60490 avg_loss = 3.46887\n",
      "epoch no.3 train no.268480  loss = 5.29846 avg_loss = 3.48934\n",
      "epoch no.3 train no.268490  loss = 2.61785 avg_loss = 3.40512\n",
      "epoch no.3 train no.268500  loss = 2.94439 avg_loss = 3.38693\n",
      "epoch no.3 train no.268510  loss = 3.18032 avg_loss = 3.36871\n",
      "epoch no.3 train no.268520  loss = 3.41549 avg_loss = 3.38373\n",
      "epoch no.3 train no.268530  loss = 1.84315 avg_loss = 3.36576\n",
      "epoch no.3 train no.268540  loss = 4.26939 avg_loss = 3.35741\n",
      "epoch no.3 train no.268550  loss = 3.80097 avg_loss = 3.36829\n",
      "epoch no.3 train no.268560  loss = 4.76832 avg_loss = 3.37214\n",
      "epoch no.3 train no.268570  loss = 2.61606 avg_loss = 3.38011\n",
      "epoch no.3 train no.268580  loss = 4.76952 avg_loss = 3.41228\n",
      "epoch no.3 train no.268590  loss = 2.99154 avg_loss = 3.41958\n",
      "epoch no.3 train no.268600  loss = 2.82332 avg_loss = 3.40791\n",
      "epoch no.3 train no.268610  loss = 4.20231 avg_loss = 3.37946\n",
      "epoch no.3 train no.268620  loss = 3.31534 avg_loss = 3.38007\n",
      "epoch no.3 train no.268630  loss = 5.19002 avg_loss = 3.39649\n",
      "epoch no.3 train no.268640  loss = 4.13683 avg_loss = 3.41361\n",
      "epoch no.3 train no.268650  loss = 2.34802 avg_loss = 3.39367\n",
      "epoch no.3 train no.268660  loss = 2.55314 avg_loss = 3.37040\n",
      "epoch no.3 train no.268670  loss = 4.19831 avg_loss = 3.34769\n",
      "epoch no.3 train no.268680  loss = 4.15266 avg_loss = 3.33067\n",
      "epoch no.3 train no.268690  loss = 3.45720 avg_loss = 3.36231\n",
      "epoch no.3 train no.268700  loss = 2.98563 avg_loss = 3.32966\n",
      "epoch no.3 train no.268710  loss = 3.59527 avg_loss = 3.34647\n",
      "epoch no.3 train no.268720  loss = 2.28320 avg_loss = 3.30383\n",
      "epoch no.3 train no.268730  loss = 3.73721 avg_loss = 3.33596\n",
      "epoch no.3 train no.268740  loss = 3.26857 avg_loss = 3.33154\n",
      "epoch no.3 train no.268750  loss = 5.17916 avg_loss = 3.35135\n",
      "epoch no.3 train no.268760  loss = 3.79269 avg_loss = 3.34558\n",
      "epoch no.3 train no.268770  loss = 3.26839 avg_loss = 3.32928\n",
      "epoch no.3 train no.268780  loss = 3.10466 avg_loss = 3.32318\n",
      "epoch no.3 train no.268790  loss = 2.76415 avg_loss = 3.29121\n",
      "epoch no.3 train no.268800  loss = 2.71275 avg_loss = 3.29157\n",
      "epoch no.3 train no.268810  loss = 3.36690 avg_loss = 3.29503\n",
      "epoch no.3 train no.268820  loss = 2.76325 avg_loss = 3.27563\n",
      "epoch no.3 train no.268830  loss = 3.04891 avg_loss = 3.25158\n",
      "epoch no.3 train no.268840  loss = 3.96418 avg_loss = 3.24418\n",
      "epoch no.3 train no.268850  loss = 3.76138 avg_loss = 3.25532\n",
      "epoch no.3 train no.268860  loss = 2.08739 avg_loss = 3.25726\n",
      "epoch no.3 train no.268870  loss = 3.84480 avg_loss = 3.29310\n",
      "epoch no.3 train no.268880  loss = 2.57857 avg_loss = 3.29543\n",
      "epoch no.3 train no.268890  loss = 3.09583 avg_loss = 3.31398\n",
      "epoch no.3 train no.268900  loss = 4.05152 avg_loss = 3.31040\n",
      "epoch no.3 train no.268910  loss = 2.62820 avg_loss = 3.28456\n",
      "epoch no.3 train no.268920  loss = 2.35165 avg_loss = 3.29713\n",
      "epoch no.3 train no.268930  loss = 2.19059 avg_loss = 3.32331\n",
      "epoch no.3 train no.268940  loss = 4.30081 avg_loss = 3.36627\n",
      "epoch no.3 train no.268950  loss = 2.90143 avg_loss = 3.35377\n",
      "epoch no.3 train no.268960  loss = 4.37967 avg_loss = 3.32317\n",
      "epoch no.3 train no.268970  loss = 2.78479 avg_loss = 3.34524\n",
      "epoch no.3 train no.268980  loss = 3.85318 avg_loss = 3.29356\n",
      "epoch no.3 train no.268990  loss = 2.77240 avg_loss = 3.25210\n",
      "epoch no.3 train no.269000  loss = 4.77488 avg_loss = 3.27297\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁필요할', '▁때', '</s>', '▁신나는', 'op', '</s>']\n",
      "기분전환이 필요할때 듣는 pop</s>\n",
      "epoch no.3 train no.269010  loss = 3.15938 avg_loss = 3.26690\n",
      "epoch no.3 train no.269020  loss = 4.72519 avg_loss = 3.28228\n",
      "epoch no.3 train no.269030  loss = 2.74394 avg_loss = 3.30115\n",
      "epoch no.3 train no.269040  loss = 2.42294 avg_loss = 3.30950\n",
      "epoch no.3 train no.269050  loss = 3.30790 avg_loss = 3.34840\n",
      "epoch no.3 train no.269060  loss = 4.55550 avg_loss = 3.32997\n",
      "epoch no.3 train no.269070  loss = 3.28242 avg_loss = 3.33621\n",
      "epoch no.3 train no.269080  loss = 4.09883 avg_loss = 3.33444\n",
      "epoch no.3 train no.269090  loss = 3.59244 avg_loss = 3.34987\n",
      "epoch no.3 train no.269100  loss = 3.52435 avg_loss = 3.40535\n",
      "epoch no.3 train no.269110  loss = 2.28822 avg_loss = 3.43200\n",
      "epoch no.3 train no.269120  loss = 3.59517 avg_loss = 3.42472\n",
      "epoch no.3 train no.269130  loss = 4.39306 avg_loss = 3.42413\n",
      "epoch no.3 train no.269140  loss = 3.79192 avg_loss = 3.41177\n",
      "epoch no.3 train no.269150  loss = 2.89806 avg_loss = 3.37017\n",
      "epoch no.3 train no.269160  loss = 3.73771 avg_loss = 3.35360\n",
      "epoch no.3 train no.269170  loss = 2.96240 avg_loss = 3.38576\n",
      "epoch no.3 train no.269180  loss = 6.21471 avg_loss = 3.37737\n",
      "epoch no.3 train no.269190  loss = 3.75629 avg_loss = 3.37580\n",
      "epoch no.3 train no.269200  loss = 2.57227 avg_loss = 3.36476\n",
      "epoch no.3 train no.269210  loss = 2.39408 avg_loss = 3.33905\n",
      "epoch no.3 train no.269220  loss = 2.84483 avg_loss = 3.30497\n",
      "epoch no.3 train no.269230  loss = 2.40070 avg_loss = 3.29825\n",
      "epoch no.3 train no.269240  loss = 5.00684 avg_loss = 3.32830\n",
      "epoch no.3 train no.269250  loss = 3.48170 avg_loss = 3.32617\n",
      "epoch no.3 train no.269260  loss = 2.53512 avg_loss = 3.38453\n",
      "epoch no.3 train no.269270  loss = 2.39155 avg_loss = 3.36612\n",
      "epoch no.3 train no.269280  loss = 4.80159 avg_loss = 3.37100\n",
      "epoch no.3 train no.269290  loss = 2.91148 avg_loss = 3.36823\n",
      "epoch no.3 train no.269300  loss = 2.39418 avg_loss = 3.41059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.269310  loss = 3.61463 avg_loss = 3.44975\n",
      "epoch no.3 train no.269320  loss = 2.93545 avg_loss = 3.44478\n",
      "epoch no.3 train no.269330  loss = 4.29632 avg_loss = 3.46996\n",
      "epoch no.3 train no.269340  loss = 3.56597 avg_loss = 3.53370\n",
      "epoch no.3 train no.269350  loss = 4.19025 avg_loss = 3.51300\n",
      "epoch no.3 train no.269360  loss = 2.06547 avg_loss = 3.49388\n",
      "epoch no.3 train no.269370  loss = 4.98594 avg_loss = 3.48254\n",
      "epoch no.3 train no.269380  loss = 4.36557 avg_loss = 3.51125\n",
      "epoch no.3 train no.269390  loss = 2.63683 avg_loss = 3.51759\n",
      "epoch no.3 train no.269400  loss = 2.88790 avg_loss = 3.50580\n",
      "epoch no.3 train no.269410  loss = 4.86613 avg_loss = 3.50189\n",
      "epoch no.3 train no.269420  loss = 3.22092 avg_loss = 3.51120\n",
      "epoch no.3 train no.269430  loss = 2.43679 avg_loss = 3.51512\n",
      "epoch no.3 train no.269440  loss = 3.46990 avg_loss = 3.50808\n",
      "epoch no.3 train no.269450  loss = 2.81019 avg_loss = 3.46964\n",
      "epoch no.3 train no.269460  loss = 2.73225 avg_loss = 3.46691\n",
      "epoch no.3 train no.269470  loss = 2.86325 avg_loss = 3.44363\n",
      "epoch no.3 train no.269480  loss = 3.55553 avg_loss = 3.43536\n",
      "epoch no.3 train no.269490  loss = 4.89104 avg_loss = 3.47735\n",
      "epoch no.3 train no.269500  loss = 3.51819 avg_loss = 3.44888\n",
      "epoch no.3 train no.269510  loss = 4.53731 avg_loss = 3.51517\n",
      "epoch no.3 train no.269520  loss = 3.84417 avg_loss = 3.51660\n",
      "epoch no.3 train no.269530  loss = 2.04628 avg_loss = 3.48563\n",
      "epoch no.3 train no.269540  loss = 2.70162 avg_loss = 3.49188\n",
      "epoch no.3 train no.269550  loss = 3.00142 avg_loss = 3.50339\n",
      "epoch no.3 train no.269560  loss = 2.15731 avg_loss = 3.51490\n",
      "epoch no.3 train no.269570  loss = 1.96539 avg_loss = 3.46238\n",
      "epoch no.3 train no.269580  loss = 2.21787 avg_loss = 3.47270\n",
      "epoch no.3 train no.269590  loss = 2.18485 avg_loss = 3.47090\n",
      "epoch no.3 train no.269600  loss = 2.96224 avg_loss = 3.47480\n",
      "epoch no.3 train no.269610  loss = 5.46557 avg_loss = 3.47401\n",
      "epoch no.3 train no.269620  loss = 3.75369 avg_loss = 3.47460\n",
      "epoch no.3 train no.269630  loss = 4.58274 avg_loss = 3.47093\n",
      "epoch no.3 train no.269640  loss = 2.50942 avg_loss = 3.42703\n",
      "epoch no.3 train no.269650  loss = 4.20986 avg_loss = 3.44409\n",
      "epoch no.3 train no.269660  loss = 3.37553 avg_loss = 3.43797\n",
      "epoch no.3 train no.269670  loss = 2.90341 avg_loss = 3.37150\n",
      "epoch no.3 train no.269680  loss = 2.66542 avg_loss = 3.35307\n",
      "epoch no.3 train no.269690  loss = 5.43258 avg_loss = 3.39448\n",
      "epoch no.3 train no.269700  loss = 4.22594 avg_loss = 3.39664\n",
      "epoch no.3 train no.269710  loss = 4.11736 avg_loss = 3.38147\n",
      "epoch no.3 train no.269720  loss = 4.20041 avg_loss = 3.40454\n",
      "epoch no.3 train no.269730  loss = 2.63396 avg_loss = 3.38375\n",
      "epoch no.3 train no.269740  loss = 3.39474 avg_loss = 3.37576\n",
      "epoch no.3 train no.269750  loss = 3.51228 avg_loss = 3.37515\n",
      "epoch no.3 train no.269760  loss = 2.85838 avg_loss = 3.38181\n",
      "epoch no.3 train no.269770  loss = 3.99630 avg_loss = 3.38284\n",
      "epoch no.3 train no.269780  loss = 4.46698 avg_loss = 3.40161\n",
      "epoch no.3 train no.269790  loss = 2.64898 avg_loss = 3.39410\n",
      "epoch no.3 train no.269800  loss = 2.57615 avg_loss = 3.43999\n",
      "epoch no.3 train no.269810  loss = 3.01229 avg_loss = 3.42554\n",
      "epoch no.3 train no.269820  loss = 3.48109 avg_loss = 3.39793\n",
      "epoch no.3 train no.269830  loss = 3.96475 avg_loss = 3.36420\n",
      "epoch no.3 train no.269840  loss = 4.48279 avg_loss = 3.38422\n",
      "epoch no.3 train no.269850  loss = 4.27193 avg_loss = 3.32857\n",
      "epoch no.3 train no.269860  loss = 3.03778 avg_loss = 3.37355\n",
      "epoch no.3 train no.269870  loss = 3.38855 avg_loss = 3.40497\n",
      "epoch no.3 train no.269880  loss = 5.02171 avg_loss = 3.41844\n",
      "epoch no.3 train no.269890  loss = 3.34184 avg_loss = 3.38162\n",
      "epoch no.3 train no.269900  loss = 4.14151 avg_loss = 3.37765\n",
      "epoch no.3 train no.269910  loss = 2.07425 avg_loss = 3.37709\n",
      "epoch no.3 train no.269920  loss = 3.46972 avg_loss = 3.40095\n",
      "epoch no.3 train no.269930  loss = 2.49332 avg_loss = 3.42955\n",
      "epoch no.3 train no.269940  loss = 3.98270 avg_loss = 3.43702\n",
      "epoch no.3 train no.269950  loss = 2.30815 avg_loss = 3.40918\n",
      "epoch no.3 train no.269960  loss = 3.54531 avg_loss = 3.43225\n",
      "epoch no.3 train no.269970  loss = 2.93371 avg_loss = 3.48469\n",
      "epoch no.3 train no.269980  loss = 2.22328 avg_loss = 3.43635\n",
      "epoch no.3 train no.269990  loss = 3.07669 avg_loss = 3.45487\n",
      "epoch no.3 train no.270000  loss = 3.62247 avg_loss = 3.47594\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '이', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.3 train no.270010  loss = 3.16458 avg_loss = 3.44449\n",
      "epoch no.3 train no.270020  loss = 4.27751 avg_loss = 3.46218\n",
      "epoch no.3 train no.270030  loss = 3.97929 avg_loss = 3.43803\n",
      "epoch no.3 train no.270040  loss = 1.91350 avg_loss = 3.45132\n",
      "epoch no.3 train no.270050  loss = 2.94888 avg_loss = 3.44920\n",
      "epoch no.3 train no.270060  loss = 2.01133 avg_loss = 3.39805\n",
      "epoch no.3 train no.270070  loss = 2.77704 avg_loss = 3.39120\n",
      "epoch no.3 train no.270080  loss = 3.23309 avg_loss = 3.36441\n",
      "epoch no.3 train no.270090  loss = 2.35616 avg_loss = 3.36909\n",
      "epoch no.3 train no.270100  loss = 1.85622 avg_loss = 3.34566\n",
      "epoch no.3 train no.270110  loss = 2.33219 avg_loss = 3.33937\n",
      "epoch no.3 train no.270120  loss = 3.68900 avg_loss = 3.33301\n",
      "epoch no.3 train no.270130  loss = 2.55260 avg_loss = 3.39131\n",
      "epoch no.3 train no.270140  loss = 1.98357 avg_loss = 3.38610\n",
      "epoch no.3 train no.270150  loss = 3.03291 avg_loss = 3.39945\n",
      "epoch no.3 train no.270160  loss = 4.04580 avg_loss = 3.40904\n",
      "epoch no.3 train no.270170  loss = 4.75446 avg_loss = 3.39689\n",
      "epoch no.3 train no.270180  loss = 3.12291 avg_loss = 3.38599\n",
      "epoch no.3 train no.270190  loss = 4.94395 avg_loss = 3.43777\n",
      "epoch no.3 train no.270200  loss = 2.59791 avg_loss = 3.43045\n",
      "epoch no.3 train no.270210  loss = 2.79313 avg_loss = 3.44073\n",
      "epoch no.3 train no.270220  loss = 3.73476 avg_loss = 3.40128\n",
      "epoch no.3 train no.270230  loss = 2.49108 avg_loss = 3.41337\n",
      "epoch no.3 train no.270240  loss = 4.65072 avg_loss = 3.47820\n",
      "epoch no.3 train no.270250  loss = 3.21307 avg_loss = 3.51048\n",
      "epoch no.3 train no.270260  loss = 2.78063 avg_loss = 3.46234\n",
      "epoch no.3 train no.270270  loss = 3.28718 avg_loss = 3.47119\n",
      "epoch no.3 train no.270280  loss = 1.93264 avg_loss = 3.43886\n",
      "epoch no.3 train no.270290  loss = 5.91397 avg_loss = 3.42811\n",
      "epoch no.3 train no.270300  loss = 2.89878 avg_loss = 3.37216\n",
      "epoch no.3 train no.270310  loss = 3.89926 avg_loss = 3.37733\n",
      "epoch no.3 train no.270320  loss = 3.27077 avg_loss = 3.38138\n",
      "epoch no.3 train no.270330  loss = 4.72197 avg_loss = 3.35226\n",
      "epoch no.3 train no.270340  loss = 2.63440 avg_loss = 3.38522\n",
      "epoch no.3 train no.270350  loss = 2.88483 avg_loss = 3.39501\n",
      "epoch no.3 train no.270360  loss = 3.84557 avg_loss = 3.35291\n",
      "epoch no.3 train no.270370  loss = 3.68424 avg_loss = 3.34071\n",
      "epoch no.3 train no.270380  loss = 3.37296 avg_loss = 3.33714\n",
      "epoch no.3 train no.270390  loss = 2.64969 avg_loss = 3.36631\n",
      "epoch no.3 train no.270400  loss = 4.27681 avg_loss = 3.34696\n",
      "epoch no.3 train no.270410  loss = 4.09709 avg_loss = 3.31480\n",
      "epoch no.3 train no.270420  loss = 4.69145 avg_loss = 3.33587\n",
      "epoch no.3 train no.270430  loss = 2.62374 avg_loss = 3.31569\n",
      "epoch no.3 train no.270440  loss = 3.33142 avg_loss = 3.29287\n",
      "epoch no.3 train no.270450  loss = 3.96443 avg_loss = 3.40747\n",
      "epoch no.3 train no.270460  loss = 4.20523 avg_loss = 3.38843\n",
      "epoch no.3 train no.270470  loss = 4.41667 avg_loss = 3.40408\n",
      "epoch no.3 train no.270480  loss = 4.10084 avg_loss = 3.38193\n",
      "epoch no.3 train no.270490  loss = 2.29951 avg_loss = 3.33105\n",
      "epoch no.3 train no.270500  loss = 3.10385 avg_loss = 3.32772\n",
      "epoch no.3 train no.270510  loss = 4.13336 avg_loss = 3.36018\n",
      "epoch no.3 train no.270520  loss = 2.65008 avg_loss = 3.34760\n",
      "epoch no.3 train no.270530  loss = 4.37059 avg_loss = 3.37396\n",
      "epoch no.3 train no.270540  loss = 2.52909 avg_loss = 3.32933\n",
      "epoch no.3 train no.270550  loss = 3.63575 avg_loss = 3.35966\n",
      "epoch no.3 train no.270560  loss = 6.28691 avg_loss = 3.33686\n",
      "epoch no.3 train no.270570  loss = 3.10119 avg_loss = 3.35985\n",
      "epoch no.3 train no.270580  loss = 4.60165 avg_loss = 3.32614\n",
      "epoch no.3 train no.270590  loss = 3.87430 avg_loss = 3.33575\n",
      "epoch no.3 train no.270600  loss = 2.17129 avg_loss = 3.33394\n",
      "epoch no.3 train no.270610  loss = 3.99141 avg_loss = 3.35795\n",
      "epoch no.3 train no.270620  loss = 3.41932 avg_loss = 3.40649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.270630  loss = 2.12055 avg_loss = 3.38796\n",
      "epoch no.3 train no.270640  loss = 2.19798 avg_loss = 3.36556\n",
      "epoch no.3 train no.270650  loss = 3.31570 avg_loss = 3.34236\n",
      "epoch no.3 train no.270660  loss = 2.41194 avg_loss = 3.31370\n",
      "epoch no.3 train no.270670  loss = 5.37788 avg_loss = 3.33523\n",
      "epoch no.3 train no.270680  loss = 3.56798 avg_loss = 3.35412\n",
      "epoch no.3 train no.270690  loss = 4.27738 avg_loss = 3.41163\n",
      "epoch no.3 train no.270700  loss = 3.85465 avg_loss = 3.40736\n",
      "epoch no.3 train no.270710  loss = 3.21549 avg_loss = 3.40591\n",
      "epoch no.3 train no.270720  loss = 2.19740 avg_loss = 3.38713\n",
      "epoch no.3 train no.270730  loss = 3.81945 avg_loss = 3.32199\n",
      "epoch no.3 train no.270740  loss = 6.06208 avg_loss = 3.34267\n",
      "epoch no.3 train no.270750  loss = 3.00105 avg_loss = 3.34204\n",
      "epoch no.3 train no.270760  loss = 1.93700 avg_loss = 3.35993\n",
      "epoch no.3 train no.270770  loss = 1.47114 avg_loss = 3.36240\n",
      "epoch no.3 train no.270780  loss = 2.87495 avg_loss = 3.39690\n",
      "epoch no.3 train no.270790  loss = 3.37296 avg_loss = 3.38533\n",
      "epoch no.3 train no.270800  loss = 4.59514 avg_loss = 3.38935\n",
      "epoch no.3 train no.270810  loss = 3.31582 avg_loss = 3.36203\n",
      "epoch no.3 train no.270820  loss = 2.70374 avg_loss = 3.38088\n",
      "epoch no.3 train no.270830  loss = 2.56579 avg_loss = 3.38465\n",
      "epoch no.3 train no.270840  loss = 4.45397 avg_loss = 3.37640\n",
      "epoch no.3 train no.270850  loss = 3.92483 avg_loss = 3.40473\n",
      "epoch no.3 train no.270860  loss = 2.80129 avg_loss = 3.41838\n",
      "epoch no.3 train no.270870  loss = 2.00195 avg_loss = 3.41455\n",
      "epoch no.3 train no.270880  loss = 1.63890 avg_loss = 3.37963\n",
      "epoch no.3 train no.270890  loss = 4.01633 avg_loss = 3.40289\n",
      "epoch no.3 train no.270900  loss = 3.00289 avg_loss = 3.42350\n",
      "epoch no.3 train no.270910  loss = 3.71184 avg_loss = 3.44540\n",
      "epoch no.3 train no.270920  loss = 2.83562 avg_loss = 3.39038\n",
      "epoch no.3 train no.270930  loss = 2.07529 avg_loss = 3.37333\n",
      "epoch no.3 train no.270940  loss = 3.62326 avg_loss = 3.37247\n",
      "epoch no.3 train no.270950  loss = 3.28382 avg_loss = 3.36194\n",
      "epoch no.3 train no.270960  loss = 2.03551 avg_loss = 3.38070\n",
      "epoch no.3 train no.270970  loss = 3.27662 avg_loss = 3.35586\n",
      "epoch no.3 train no.270980  loss = 3.28927 avg_loss = 3.34841\n",
      "epoch no.3 train no.270990  loss = 4.17956 avg_loss = 3.35853\n",
      "epoch no.3 train no.271000  loss = 2.65082 avg_loss = 3.36861\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은노래</s>\n",
      "epoch no.3 train no.271010  loss = 3.12975 avg_loss = 3.37117\n",
      "epoch no.3 train no.271020  loss = 1.76322 avg_loss = 3.36736\n",
      "epoch no.3 train no.271030  loss = 4.60287 avg_loss = 3.38472\n",
      "epoch no.3 train no.271040  loss = 5.03963 avg_loss = 3.46621\n",
      "epoch no.3 train no.271050  loss = 3.27417 avg_loss = 3.45577\n",
      "epoch no.3 train no.271060  loss = 2.39941 avg_loss = 3.41956\n",
      "epoch no.3 train no.271070  loss = 3.13467 avg_loss = 3.40751\n",
      "epoch no.3 train no.271080  loss = 4.14540 avg_loss = 3.43720\n",
      "epoch no.3 train no.271090  loss = 3.32476 avg_loss = 3.41859\n",
      "epoch no.3 train no.271100  loss = 4.80690 avg_loss = 3.42844\n",
      "epoch no.3 train no.271110  loss = 3.77028 avg_loss = 3.41326\n",
      "epoch no.3 train no.271120  loss = 3.11564 avg_loss = 3.42910\n",
      "epoch no.3 train no.271130  loss = 2.93314 avg_loss = 3.42401\n",
      "epoch no.3 train no.271140  loss = 3.15302 avg_loss = 3.39872\n",
      "epoch no.3 train no.271150  loss = 3.17740 avg_loss = 3.37580\n",
      "epoch no.3 train no.271160  loss = 2.68167 avg_loss = 3.42427\n",
      "epoch no.3 train no.271170  loss = 4.03566 avg_loss = 3.47429\n",
      "epoch no.3 train no.271180  loss = 4.84516 avg_loss = 3.48234\n",
      "epoch no.3 train no.271190  loss = 4.28349 avg_loss = 3.46552\n",
      "epoch no.3 train no.271200  loss = 4.46384 avg_loss = 3.46464\n",
      "epoch no.3 train no.271210  loss = 2.50441 avg_loss = 3.43572\n",
      "epoch no.3 train no.271220  loss = 2.95807 avg_loss = 3.39455\n",
      "epoch no.3 train no.271230  loss = 4.34407 avg_loss = 3.39687\n",
      "epoch no.3 train no.271240  loss = 3.72968 avg_loss = 3.38793\n",
      "epoch no.3 train no.271250  loss = 3.15979 avg_loss = 3.42991\n",
      "epoch no.3 train no.271260  loss = 4.47319 avg_loss = 3.43985\n",
      "epoch no.3 train no.271270  loss = 2.93347 avg_loss = 3.39750\n",
      "epoch no.3 train no.271280  loss = 2.59862 avg_loss = 3.37485\n",
      "epoch no.3 train no.271290  loss = 4.94176 avg_loss = 3.38715\n",
      "epoch no.3 train no.271300  loss = 2.80639 avg_loss = 3.41341\n",
      "epoch no.3 train no.271310  loss = 4.04572 avg_loss = 3.44429\n",
      "epoch no.3 train no.271320  loss = 2.46456 avg_loss = 3.50862\n",
      "epoch no.3 train no.271330  loss = 2.93124 avg_loss = 3.50409\n",
      "epoch no.3 train no.271340  loss = 3.03175 avg_loss = 3.51491\n",
      "epoch no.3 train no.271350  loss = 4.45992 avg_loss = 3.51436\n",
      "epoch no.3 train no.271360  loss = 2.66611 avg_loss = 3.53823\n",
      "epoch no.3 train no.271370  loss = 2.43751 avg_loss = 3.53212\n",
      "epoch no.3 train no.271380  loss = 3.55901 avg_loss = 3.50540\n",
      "epoch no.3 train no.271390  loss = 3.28665 avg_loss = 3.48507\n",
      "epoch no.3 train no.271400  loss = 5.10291 avg_loss = 3.51201\n",
      "epoch no.3 train no.271410  loss = 3.13741 avg_loss = 3.50367\n",
      "epoch no.3 train no.271420  loss = 2.33697 avg_loss = 3.47025\n",
      "epoch no.3 train no.271430  loss = 3.07612 avg_loss = 3.48912\n",
      "epoch no.3 train no.271440  loss = 2.23168 avg_loss = 3.45285\n",
      "epoch no.3 train no.271450  loss = 3.15555 avg_loss = 3.46012\n",
      "epoch no.3 train no.271460  loss = 2.45391 avg_loss = 3.48181\n",
      "epoch no.3 train no.271470  loss = 4.08338 avg_loss = 3.46085\n",
      "epoch no.3 train no.271480  loss = 2.99819 avg_loss = 3.44987\n",
      "epoch no.3 train no.271490  loss = 3.37624 avg_loss = 3.46543\n",
      "epoch no.3 train no.271500  loss = 1.97202 avg_loss = 3.43849\n",
      "epoch no.3 train no.271510  loss = 3.97432 avg_loss = 3.40841\n",
      "epoch no.3 train no.271520  loss = 4.41391 avg_loss = 3.39351\n",
      "epoch no.3 train no.271530  loss = 5.75235 avg_loss = 3.44402\n",
      "epoch no.3 train no.271540  loss = 2.88400 avg_loss = 3.45073\n",
      "epoch no.3 train no.271550  loss = 3.42489 avg_loss = 3.41351\n",
      "epoch no.3 train no.271560  loss = 4.40364 avg_loss = 3.46707\n",
      "epoch no.3 train no.271570  loss = 2.23482 avg_loss = 3.46142\n",
      "epoch no.3 train no.271580  loss = 2.23971 avg_loss = 3.44971\n",
      "epoch no.3 train no.271590  loss = 3.00055 avg_loss = 3.48849\n",
      "epoch no.3 train no.271600  loss = 2.86413 avg_loss = 3.40017\n",
      "epoch no.3 train no.271610  loss = 2.85254 avg_loss = 3.39168\n",
      "epoch no.3 train no.271620  loss = 1.77369 avg_loss = 3.38246\n",
      "epoch no.3 train no.271630  loss = 3.90136 avg_loss = 3.40730\n",
      "epoch no.3 train no.271640  loss = 4.23188 avg_loss = 3.41557\n",
      "epoch no.3 train no.271650  loss = 3.35557 avg_loss = 3.45523\n",
      "epoch no.3 train no.271660  loss = 2.96757 avg_loss = 3.43711\n",
      "epoch no.3 train no.271670  loss = 3.68249 avg_loss = 3.43264\n",
      "epoch no.3 train no.271680  loss = 2.79418 avg_loss = 3.42303\n",
      "epoch no.3 train no.271690  loss = 3.37944 avg_loss = 3.41148\n",
      "epoch no.3 train no.271700  loss = 2.88642 avg_loss = 3.37436\n",
      "epoch no.3 train no.271710  loss = 2.21020 avg_loss = 3.39307\n",
      "epoch no.3 train no.271720  loss = 2.26689 avg_loss = 3.37890\n",
      "epoch no.3 train no.271730  loss = 2.69389 avg_loss = 3.34021\n",
      "epoch no.3 train no.271740  loss = 3.13023 avg_loss = 3.33129\n",
      "epoch no.3 train no.271750  loss = 3.12569 avg_loss = 3.32147\n",
      "epoch no.3 train no.271760  loss = 1.94925 avg_loss = 3.29751\n",
      "epoch no.3 train no.271770  loss = 2.22829 avg_loss = 3.31717\n",
      "epoch no.3 train no.271780  loss = 3.02198 avg_loss = 3.33328\n",
      "epoch no.3 train no.271790  loss = 3.17277 avg_loss = 3.34203\n",
      "epoch no.3 train no.271800  loss = 3.39090 avg_loss = 3.32876\n",
      "epoch no.3 train no.271810  loss = 4.59231 avg_loss = 3.35687\n",
      "epoch no.3 train no.271820  loss = 2.49178 avg_loss = 3.36409\n",
      "epoch no.3 train no.271830  loss = 4.00008 avg_loss = 3.38037\n",
      "epoch no.3 train no.271840  loss = 1.86088 avg_loss = 3.36942\n",
      "epoch no.3 train no.271850  loss = 1.90250 avg_loss = 3.34044\n",
      "epoch no.3 train no.271860  loss = 3.76288 avg_loss = 3.31749\n",
      "epoch no.3 train no.271870  loss = 1.81164 avg_loss = 3.29880\n",
      "epoch no.3 train no.271880  loss = 3.11077 avg_loss = 3.27833\n",
      "epoch no.3 train no.271890  loss = 3.00786 avg_loss = 3.29571\n",
      "epoch no.3 train no.271900  loss = 3.89999 avg_loss = 3.32824\n",
      "epoch no.3 train no.271910  loss = 2.34432 avg_loss = 3.34343\n",
      "epoch no.3 train no.271920  loss = 2.65661 avg_loss = 3.35283\n",
      "epoch no.3 train no.271930  loss = 3.36166 avg_loss = 3.30847\n",
      "epoch no.3 train no.271940  loss = 4.68236 avg_loss = 3.28313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.271950  loss = 2.71720 avg_loss = 3.27824\n",
      "epoch no.3 train no.271960  loss = 2.57034 avg_loss = 3.24890\n",
      "epoch no.3 train no.271970  loss = 4.75676 avg_loss = 3.25481\n",
      "epoch no.3 train no.271980  loss = 1.85300 avg_loss = 3.29188\n",
      "epoch no.3 train no.271990  loss = 2.19034 avg_loss = 3.29662\n",
      "epoch no.3 train no.272000  loss = 4.11860 avg_loss = 3.31678\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁좋은', '▁음악', '</s>', '</s>']\n",
      "기분전환에 좋은 노래들</s>\n",
      "epoch no.3 train no.272010  loss = 3.82055 avg_loss = 3.30571\n",
      "epoch no.3 train no.272020  loss = 5.01024 avg_loss = 3.31526\n",
      "epoch no.3 train no.272030  loss = 4.29109 avg_loss = 3.27595\n",
      "epoch no.3 train no.272040  loss = 2.89625 avg_loss = 3.27409\n",
      "epoch no.3 train no.272050  loss = 3.39451 avg_loss = 3.24561\n",
      "epoch no.3 train no.272060  loss = 4.51768 avg_loss = 3.29840\n",
      "epoch no.3 train no.272070  loss = 2.37135 avg_loss = 3.30270\n",
      "epoch no.3 train no.272080  loss = 3.16159 avg_loss = 3.29144\n",
      "epoch no.3 train no.272090  loss = 2.64284 avg_loss = 3.28188\n",
      "epoch no.3 train no.272100  loss = 3.34176 avg_loss = 3.28798\n",
      "epoch no.3 train no.272110  loss = 3.07766 avg_loss = 3.29706\n",
      "epoch no.3 train no.272120  loss = 3.25013 avg_loss = 3.32797\n",
      "epoch no.3 train no.272130  loss = 2.51499 avg_loss = 3.28891\n",
      "epoch no.3 train no.272140  loss = 3.03226 avg_loss = 3.30437\n",
      "epoch no.3 train no.272150  loss = 4.77696 avg_loss = 3.27978\n",
      "epoch no.3 train no.272160  loss = 2.83734 avg_loss = 3.28407\n",
      "epoch no.3 train no.272170  loss = 3.78051 avg_loss = 3.31708\n",
      "epoch no.3 train no.272180  loss = 2.27531 avg_loss = 3.30226\n",
      "epoch no.3 train no.272190  loss = 2.46795 avg_loss = 3.33871\n",
      "epoch no.3 train no.272200  loss = 4.37510 avg_loss = 3.35310\n",
      "epoch no.3 train no.272210  loss = 4.75565 avg_loss = 3.42527\n",
      "epoch no.3 train no.272220  loss = 3.26769 avg_loss = 3.44263\n",
      "epoch no.3 train no.272230  loss = 4.93102 avg_loss = 3.48869\n",
      "epoch no.3 train no.272240  loss = 5.02477 avg_loss = 3.49539\n",
      "epoch no.3 train no.272250  loss = 3.99212 avg_loss = 3.53127\n",
      "epoch no.3 train no.272260  loss = 3.28714 avg_loss = 3.55043\n",
      "epoch no.3 train no.272270  loss = 2.37797 avg_loss = 3.51337\n",
      "epoch no.3 train no.272280  loss = 2.82501 avg_loss = 3.50307\n",
      "epoch no.3 train no.272290  loss = 4.22221 avg_loss = 3.53509\n",
      "epoch no.3 train no.272300  loss = 3.26301 avg_loss = 3.47498\n",
      "epoch no.3 train no.272310  loss = 3.84410 avg_loss = 3.45102\n",
      "epoch no.3 train no.272320  loss = 4.38679 avg_loss = 3.47365\n",
      "epoch no.3 train no.272330  loss = 4.68397 avg_loss = 3.50844\n",
      "epoch no.3 train no.272340  loss = 3.10933 avg_loss = 3.45829\n",
      "epoch no.3 train no.272350  loss = 2.98633 avg_loss = 3.47732\n",
      "epoch no.3 train no.272360  loss = 5.06042 avg_loss = 3.52507\n",
      "epoch no.3 train no.272370  loss = 2.95767 avg_loss = 3.48951\n",
      "epoch no.3 train no.272380  loss = 2.31789 avg_loss = 3.47456\n",
      "epoch no.3 train no.272390  loss = 3.58378 avg_loss = 3.49612\n",
      "epoch no.3 train no.272400  loss = 2.60835 avg_loss = 3.44630\n",
      "epoch no.3 train no.272410  loss = 5.11259 avg_loss = 3.44287\n",
      "epoch no.3 train no.272420  loss = 3.99414 avg_loss = 3.44651\n",
      "epoch no.3 train no.272430  loss = 2.62347 avg_loss = 3.41513\n",
      "epoch no.3 train no.272440  loss = 5.28929 avg_loss = 3.43643\n",
      "epoch no.3 train no.272450  loss = 4.75097 avg_loss = 3.44314\n",
      "epoch no.3 train no.272460  loss = 4.18924 avg_loss = 3.43388\n",
      "epoch no.3 train no.272470  loss = 2.70669 avg_loss = 3.38623\n",
      "epoch no.3 train no.272480  loss = 5.66482 avg_loss = 3.43229\n",
      "epoch no.3 train no.272490  loss = 4.13214 avg_loss = 3.46423\n",
      "epoch no.3 train no.272500  loss = 2.58832 avg_loss = 3.48119\n",
      "epoch no.3 train no.272510  loss = 2.84603 avg_loss = 3.44296\n",
      "epoch no.3 train no.272520  loss = 3.58028 avg_loss = 3.44569\n",
      "epoch no.3 train no.272530  loss = 3.29991 avg_loss = 3.47155\n",
      "epoch no.3 train no.272540  loss = 4.46056 avg_loss = 3.44893\n",
      "epoch no.3 train no.272550  loss = 4.07248 avg_loss = 3.46772\n",
      "epoch no.3 train no.272560  loss = 2.33015 avg_loss = 3.45659\n",
      "epoch no.3 train no.272570  loss = 2.85542 avg_loss = 3.41833\n",
      "epoch no.3 train no.272580  loss = 3.45632 avg_loss = 3.46474\n",
      "epoch no.3 train no.272590  loss = 5.34346 avg_loss = 3.46114\n",
      "epoch no.3 train no.272600  loss = 2.59954 avg_loss = 3.40055\n",
      "epoch no.3 train no.272610  loss = 3.31264 avg_loss = 3.44382\n",
      "epoch no.3 train no.272620  loss = 4.09640 avg_loss = 3.48096\n",
      "epoch no.3 train no.272630  loss = 3.48227 avg_loss = 3.49963\n",
      "epoch no.3 train no.272640  loss = 2.69312 avg_loss = 3.48017\n",
      "epoch no.3 train no.272650  loss = 1.67854 avg_loss = 3.48366\n",
      "epoch no.3 train no.272660  loss = 3.53165 avg_loss = 3.44643\n",
      "epoch no.3 train no.272670  loss = 5.75118 avg_loss = 3.48001\n",
      "epoch no.3 train no.272680  loss = 2.19863 avg_loss = 3.46411\n",
      "epoch no.3 train no.272690  loss = 2.60992 avg_loss = 3.43418\n",
      "epoch no.3 train no.272700  loss = 2.90775 avg_loss = 3.42352\n",
      "epoch no.3 train no.272710  loss = 3.08381 avg_loss = 3.39715\n",
      "epoch no.3 train no.272720  loss = 3.20761 avg_loss = 3.39956\n",
      "epoch no.3 train no.272730  loss = 6.16149 avg_loss = 3.43484\n",
      "epoch no.3 train no.272740  loss = 5.88053 avg_loss = 3.42408\n",
      "epoch no.3 train no.272750  loss = 2.87490 avg_loss = 3.43048\n",
      "epoch no.3 train no.272760  loss = 1.69626 avg_loss = 3.38833\n",
      "epoch no.3 train no.272770  loss = 4.48034 avg_loss = 3.42276\n",
      "epoch no.3 train no.272780  loss = 2.56491 avg_loss = 3.39657\n",
      "epoch no.3 train no.272790  loss = 2.61789 avg_loss = 3.44089\n",
      "epoch no.3 train no.272800  loss = 2.49726 avg_loss = 3.40754\n",
      "epoch no.3 train no.272810  loss = 3.91244 avg_loss = 3.40980\n",
      "epoch no.3 train no.272820  loss = 3.65977 avg_loss = 3.39458\n",
      "epoch no.3 train no.272830  loss = 2.98858 avg_loss = 3.37643\n",
      "epoch no.3 train no.272840  loss = 4.49866 avg_loss = 3.40488\n",
      "epoch no.3 train no.272850  loss = 3.28747 avg_loss = 3.44892\n",
      "epoch no.3 train no.272860  loss = 3.47231 avg_loss = 3.42382\n",
      "epoch no.3 train no.272870  loss = 2.68294 avg_loss = 3.42240\n",
      "epoch no.3 train no.272880  loss = 2.74577 avg_loss = 3.37961\n",
      "epoch no.3 train no.272890  loss = 6.14510 avg_loss = 3.37142\n",
      "epoch no.3 train no.272900  loss = 3.71422 avg_loss = 3.41091\n",
      "epoch no.3 train no.272910  loss = 1.93960 avg_loss = 3.40805\n",
      "epoch no.3 train no.272920  loss = 2.82336 avg_loss = 3.42409\n",
      "epoch no.3 train no.272930  loss = 3.51300 avg_loss = 3.41564\n",
      "epoch no.3 train no.272940  loss = 2.78922 avg_loss = 3.41823\n",
      "epoch no.3 train no.272950  loss = 4.49523 avg_loss = 3.42452\n",
      "epoch no.3 train no.272960  loss = 3.67851 avg_loss = 3.38741\n",
      "epoch no.3 train no.272970  loss = 2.40499 avg_loss = 3.38101\n",
      "epoch no.3 train no.272980  loss = 3.75070 avg_loss = 3.37974\n",
      "epoch no.3 train no.272990  loss = 2.56192 avg_loss = 3.38101\n",
      "epoch no.3 train no.273000  loss = 3.19654 avg_loss = 3.37037\n",
      "4\n",
      "to_tokens: ['▁라디오', '▁좋은', '용', '▁위한', '▁신나는', '송', '</s>']\n",
      "기분전환을 위한 팝송</s>\n",
      "epoch no.3 train no.273010  loss = 2.16365 avg_loss = 3.34777\n",
      "epoch no.3 train no.273020  loss = 2.56315 avg_loss = 3.31528\n",
      "epoch no.3 train no.273030  loss = 3.74519 avg_loss = 3.32347\n",
      "epoch no.3 train no.273040  loss = 4.55458 avg_loss = 3.36357\n",
      "epoch no.3 train no.273050  loss = 3.92210 avg_loss = 3.37667\n",
      "epoch no.3 train no.273060  loss = 3.42054 avg_loss = 3.39717\n",
      "epoch no.3 train no.273070  loss = 2.79535 avg_loss = 3.37475\n",
      "epoch no.3 train no.273080  loss = 1.83071 avg_loss = 3.37255\n",
      "epoch no.3 train no.273090  loss = 3.10169 avg_loss = 3.39042\n",
      "epoch no.3 train no.273100  loss = 2.85525 avg_loss = 3.38108\n",
      "epoch no.3 train no.273110  loss = 3.81193 avg_loss = 3.35025\n",
      "epoch no.3 train no.273120  loss = 2.05135 avg_loss = 3.33677\n",
      "epoch no.3 train no.273130  loss = 2.23244 avg_loss = 3.31064\n",
      "epoch no.3 train no.273140  loss = 4.16526 avg_loss = 3.31035\n",
      "epoch no.3 train no.273150  loss = 6.14321 avg_loss = 3.32329\n",
      "epoch no.3 train no.273160  loss = 3.71948 avg_loss = 3.32767\n",
      "epoch no.3 train no.273170  loss = 3.53466 avg_loss = 3.32422\n",
      "epoch no.3 train no.273180  loss = 2.13356 avg_loss = 3.29075\n",
      "epoch no.3 train no.273190  loss = 3.11555 avg_loss = 3.27704\n",
      "epoch no.3 train no.273200  loss = 2.62793 avg_loss = 3.29212\n",
      "epoch no.3 train no.273210  loss = 2.69208 avg_loss = 3.34316\n",
      "epoch no.3 train no.273220  loss = 3.63180 avg_loss = 3.37199\n",
      "epoch no.3 train no.273230  loss = 2.83087 avg_loss = 3.36683\n",
      "epoch no.3 train no.273240  loss = 2.48623 avg_loss = 3.41815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.273250  loss = 1.80123 avg_loss = 3.38740\n",
      "epoch no.3 train no.273260  loss = 3.69218 avg_loss = 3.37107\n",
      "epoch no.3 train no.273270  loss = 4.97003 avg_loss = 3.41441\n",
      "epoch no.3 train no.273280  loss = 1.99409 avg_loss = 3.35058\n",
      "epoch no.3 train no.273290  loss = 3.59341 avg_loss = 3.40019\n",
      "epoch no.3 train no.273300  loss = 3.76654 avg_loss = 3.37263\n",
      "epoch no.3 train no.273310  loss = 3.60125 avg_loss = 3.36655\n",
      "epoch no.3 train no.273320  loss = 3.22856 avg_loss = 3.34222\n",
      "epoch no.3 train no.273330  loss = 2.44544 avg_loss = 3.32531\n",
      "epoch no.3 train no.273340  loss = 5.18307 avg_loss = 3.35281\n",
      "epoch no.3 train no.273350  loss = 2.72822 avg_loss = 3.42229\n",
      "epoch no.3 train no.273360  loss = 3.64801 avg_loss = 3.43557\n",
      "epoch no.3 train no.273370  loss = 2.82291 avg_loss = 3.44812\n",
      "epoch no.3 train no.273380  loss = 3.36445 avg_loss = 3.41920\n",
      "epoch no.3 train no.273390  loss = 3.92034 avg_loss = 3.41564\n",
      "epoch no.3 train no.273400  loss = 3.36750 avg_loss = 3.38986\n",
      "epoch no.3 train no.273410  loss = 2.08302 avg_loss = 3.40872\n",
      "epoch no.3 train no.273420  loss = 4.93331 avg_loss = 3.45026\n",
      "epoch no.3 train no.273430  loss = 4.43225 avg_loss = 3.43734\n",
      "epoch no.3 train no.273440  loss = 4.56913 avg_loss = 3.45597\n",
      "epoch no.3 train no.273450  loss = 3.87383 avg_loss = 3.43062\n",
      "epoch no.3 train no.273460  loss = 2.25840 avg_loss = 3.43426\n",
      "epoch no.3 train no.273470  loss = 3.28681 avg_loss = 3.43225\n",
      "epoch no.3 train no.273480  loss = 2.59080 avg_loss = 3.44525\n",
      "epoch no.3 train no.273490  loss = 2.81935 avg_loss = 3.43096\n",
      "epoch no.3 train no.273500  loss = 3.12286 avg_loss = 3.39215\n",
      "epoch no.3 train no.273510  loss = 4.33762 avg_loss = 3.41394\n",
      "epoch no.3 train no.273520  loss = 2.92428 avg_loss = 3.39183\n",
      "epoch no.3 train no.273530  loss = 5.50395 avg_loss = 3.39229\n",
      "epoch no.3 train no.273540  loss = 2.31342 avg_loss = 3.38196\n",
      "epoch no.3 train no.273550  loss = 5.93801 avg_loss = 3.40795\n",
      "epoch no.3 train no.273560  loss = 3.54253 avg_loss = 3.41839\n",
      "epoch no.3 train no.273570  loss = 2.46377 avg_loss = 3.40424\n",
      "epoch no.3 train no.273580  loss = 4.63989 avg_loss = 3.39483\n",
      "epoch no.3 train no.273590  loss = 4.12732 avg_loss = 3.41701\n",
      "epoch no.3 train no.273600  loss = 3.60137 avg_loss = 3.42761\n",
      "epoch no.3 train no.273610  loss = 5.08974 avg_loss = 3.39711\n",
      "epoch no.3 train no.273620  loss = 2.25320 avg_loss = 3.37289\n",
      "epoch no.3 train no.273630  loss = 3.61391 avg_loss = 3.38840\n",
      "epoch no.3 train no.273640  loss = 4.60352 avg_loss = 3.35843\n",
      "epoch no.3 train no.273650  loss = 4.61203 avg_loss = 3.37370\n",
      "epoch no.3 train no.273660  loss = 3.92999 avg_loss = 3.39914\n",
      "epoch no.3 train no.273670  loss = 2.83307 avg_loss = 3.42458\n",
      "epoch no.3 train no.273680  loss = 1.82569 avg_loss = 3.38966\n",
      "epoch no.3 train no.273690  loss = 3.55134 avg_loss = 3.35948\n",
      "epoch no.3 train no.273700  loss = 2.84855 avg_loss = 3.34637\n",
      "epoch no.3 train no.273710  loss = 3.09102 avg_loss = 3.33657\n",
      "epoch no.3 train no.273720  loss = 3.82720 avg_loss = 3.33587\n",
      "epoch no.3 train no.273730  loss = 3.24903 avg_loss = 3.35190\n",
      "epoch no.3 train no.273740  loss = 5.12984 avg_loss = 3.35823\n",
      "epoch no.3 train no.273750  loss = 3.01893 avg_loss = 3.39211\n",
      "epoch no.3 train no.273760  loss = 4.07025 avg_loss = 3.40074\n",
      "epoch no.3 train no.273770  loss = 3.61872 avg_loss = 3.42236\n",
      "epoch no.3 train no.273780  loss = 3.63544 avg_loss = 3.39374\n",
      "epoch no.3 train no.273790  loss = 4.92418 avg_loss = 3.40558\n",
      "epoch no.3 train no.273800  loss = 4.05988 avg_loss = 3.37159\n",
      "epoch no.3 train no.273810  loss = 3.44220 avg_loss = 3.38312\n",
      "epoch no.3 train no.273820  loss = 3.28378 avg_loss = 3.35824\n",
      "epoch no.3 train no.273830  loss = 2.19284 avg_loss = 3.32550\n",
      "epoch no.3 train no.273840  loss = 3.68519 avg_loss = 3.28472\n",
      "epoch no.3 train no.273850  loss = 4.37390 avg_loss = 3.26949\n",
      "epoch no.3 train no.273860  loss = 3.23913 avg_loss = 3.25550\n",
      "epoch no.3 train no.273870  loss = 3.00020 avg_loss = 3.23456\n",
      "epoch no.3 train no.273880  loss = 3.00433 avg_loss = 3.17913\n",
      "epoch no.3 train no.273890  loss = 1.45872 avg_loss = 3.13409\n",
      "epoch no.3 train no.273900  loss = 4.53875 avg_loss = 3.18565\n",
      "epoch no.3 train no.273910  loss = 3.96089 avg_loss = 3.17704\n",
      "epoch no.3 train no.273920  loss = 2.89961 avg_loss = 3.23876\n",
      "epoch no.3 train no.273930  loss = 3.86150 avg_loss = 3.25819\n",
      "epoch no.3 train no.273940  loss = 4.34196 avg_loss = 3.25420\n",
      "epoch no.3 train no.273950  loss = 4.35116 avg_loss = 3.23093\n",
      "epoch no.3 train no.273960  loss = 3.99635 avg_loss = 3.25215\n",
      "epoch no.3 train no.273970  loss = 2.35741 avg_loss = 3.26558\n",
      "epoch no.3 train no.273980  loss = 3.36175 avg_loss = 3.24256\n",
      "epoch no.3 train no.273990  loss = 1.52893 avg_loss = 3.31844\n",
      "epoch no.3 train no.274000  loss = 3.94889 avg_loss = 3.31558\n",
      "4\n",
      "to_tokens: ['▁라디오', '▁좋은', '에', '▁신나는', '▁노래', 'op', '</s>']\n",
      "기분전환용 신나는 pop</s>\n",
      "epoch no.3 train no.274010  loss = 3.41015 avg_loss = 3.31537\n",
      "epoch no.3 train no.274020  loss = 2.85504 avg_loss = 3.31393\n",
      "epoch no.3 train no.274030  loss = 3.02658 avg_loss = 3.31219\n",
      "epoch no.3 train no.274040  loss = 1.84948 avg_loss = 3.31512\n",
      "epoch no.3 train no.274050  loss = 5.03192 avg_loss = 3.32157\n",
      "epoch no.3 train no.274060  loss = 2.07486 avg_loss = 3.33985\n",
      "epoch no.3 train no.274070  loss = 5.81424 avg_loss = 3.38322\n",
      "epoch no.3 train no.274080  loss = 5.80663 avg_loss = 3.40421\n",
      "epoch no.3 train no.274090  loss = 2.09826 avg_loss = 3.37177\n",
      "epoch no.3 train no.274100  loss = 6.38366 avg_loss = 3.40454\n",
      "epoch no.3 train no.274110  loss = 3.41200 avg_loss = 3.41369\n",
      "epoch no.3 train no.274120  loss = 4.44875 avg_loss = 3.41357\n",
      "epoch no.3 train no.274130  loss = 3.53312 avg_loss = 3.35030\n",
      "epoch no.3 train no.274140  loss = 3.45454 avg_loss = 3.36850\n",
      "epoch no.3 train no.274150  loss = 3.23169 avg_loss = 3.30924\n",
      "epoch no.3 train no.274160  loss = 3.69805 avg_loss = 3.35108\n",
      "epoch no.3 train no.274170  loss = 3.32765 avg_loss = 3.36890\n",
      "epoch no.3 train no.274180  loss = 4.40082 avg_loss = 3.40797\n",
      "epoch no.3 train no.274190  loss = 2.60585 avg_loss = 3.37106\n",
      "epoch no.3 train no.274200  loss = 4.48775 avg_loss = 3.40822\n",
      "epoch no.3 train no.274210  loss = 4.45417 avg_loss = 3.41546\n",
      "epoch no.3 train no.274220  loss = 3.04000 avg_loss = 3.39744\n",
      "epoch no.3 train no.274230  loss = 2.83572 avg_loss = 3.39592\n",
      "epoch no.3 train no.274240  loss = 3.73590 avg_loss = 3.42834\n",
      "epoch no.3 train no.274250  loss = 2.52169 avg_loss = 3.43654\n",
      "epoch no.3 train no.274260  loss = 2.72229 avg_loss = 3.46133\n",
      "epoch no.3 train no.274270  loss = 3.60615 avg_loss = 3.48093\n",
      "epoch no.3 train no.274280  loss = 3.49822 avg_loss = 3.49246\n",
      "epoch no.3 train no.274290  loss = 3.83565 avg_loss = 3.49466\n",
      "epoch no.3 train no.274300  loss = 4.70798 avg_loss = 3.49040\n",
      "epoch no.3 train no.274310  loss = 3.44870 avg_loss = 3.51718\n",
      "epoch no.3 train no.274320  loss = 3.09668 avg_loss = 3.48072\n",
      "epoch no.3 train no.274330  loss = 1.70851 avg_loss = 3.42993\n",
      "epoch no.3 train no.274340  loss = 3.68013 avg_loss = 3.46538\n",
      "epoch no.3 train no.274350  loss = 3.46991 avg_loss = 3.40498\n",
      "epoch no.3 train no.274360  loss = 2.44356 avg_loss = 3.40959\n",
      "epoch no.3 train no.274370  loss = 2.64974 avg_loss = 3.39978\n",
      "epoch no.3 train no.274380  loss = 1.80515 avg_loss = 3.37024\n",
      "epoch no.3 train no.274390  loss = 4.80378 avg_loss = 3.37733\n",
      "epoch no.3 train no.274400  loss = 2.52022 avg_loss = 3.37907\n",
      "epoch no.3 train no.274410  loss = 4.85883 avg_loss = 3.38840\n",
      "epoch no.3 train no.274420  loss = 2.97813 avg_loss = 3.38335\n",
      "epoch no.3 train no.274430  loss = 3.34512 avg_loss = 3.40579\n",
      "epoch no.3 train no.274440  loss = 3.30775 avg_loss = 3.40728\n",
      "epoch no.3 train no.274450  loss = 3.25786 avg_loss = 3.42056\n",
      "epoch no.3 train no.274460  loss = 2.61350 avg_loss = 3.40264\n",
      "epoch no.3 train no.274470  loss = 3.11077 avg_loss = 3.38329\n",
      "epoch no.3 train no.274480  loss = 3.79813 avg_loss = 3.37370\n",
      "epoch no.3 train no.274490  loss = 4.49495 avg_loss = 3.40026\n",
      "epoch no.3 train no.274500  loss = 2.51010 avg_loss = 3.43842\n",
      "epoch no.3 train no.274510  loss = 3.44631 avg_loss = 3.45559\n",
      "epoch no.3 train no.274520  loss = 3.95563 avg_loss = 3.45013\n",
      "epoch no.3 train no.274530  loss = 2.11095 avg_loss = 3.46054\n",
      "epoch no.3 train no.274540  loss = 3.89316 avg_loss = 3.46832\n",
      "epoch no.3 train no.274550  loss = 2.89581 avg_loss = 3.44305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.274560  loss = 3.81831 avg_loss = 3.43684\n",
      "epoch no.3 train no.274570  loss = 2.50469 avg_loss = 3.41701\n",
      "epoch no.3 train no.274580  loss = 3.49171 avg_loss = 3.45341\n",
      "epoch no.3 train no.274590  loss = 3.14386 avg_loss = 3.45077\n",
      "epoch no.3 train no.274600  loss = 2.48684 avg_loss = 3.43873\n",
      "epoch no.3 train no.274610  loss = 2.84773 avg_loss = 3.39230\n",
      "epoch no.3 train no.274620  loss = 5.00502 avg_loss = 3.38521\n",
      "epoch no.3 train no.274630  loss = 2.68501 avg_loss = 3.37014\n",
      "epoch no.3 train no.274640  loss = 2.39932 avg_loss = 3.33793\n",
      "epoch no.3 train no.274650  loss = 2.32427 avg_loss = 3.32642\n",
      "epoch no.3 train no.274660  loss = 2.63501 avg_loss = 3.31390\n",
      "epoch no.3 train no.274670  loss = 5.58996 avg_loss = 3.31287\n",
      "epoch no.3 train no.274680  loss = 2.98863 avg_loss = 3.30478\n",
      "epoch no.3 train no.274690  loss = 2.24810 avg_loss = 3.28538\n",
      "epoch no.3 train no.274700  loss = 3.06993 avg_loss = 3.34358\n",
      "epoch no.3 train no.274710  loss = 2.13587 avg_loss = 3.33626\n",
      "epoch no.3 train no.274720  loss = 3.04247 avg_loss = 3.42704\n",
      "epoch no.3 train no.274730  loss = 3.32235 avg_loss = 3.45441\n",
      "epoch no.3 train no.274740  loss = 3.67775 avg_loss = 3.45457\n",
      "epoch no.3 train no.274750  loss = 2.35327 avg_loss = 3.45707\n",
      "epoch no.3 train no.274760  loss = 3.85426 avg_loss = 3.42040\n",
      "epoch no.3 train no.274770  loss = 2.40614 avg_loss = 3.37191\n",
      "epoch no.3 train no.274780  loss = 4.32657 avg_loss = 3.39571\n",
      "epoch no.3 train no.274790  loss = 3.47233 avg_loss = 3.38490\n",
      "epoch no.3 train no.274800  loss = 4.04949 avg_loss = 3.37764\n",
      "epoch no.3 train no.274810  loss = 2.63103 avg_loss = 3.37340\n",
      "epoch no.3 train no.274820  loss = 2.20690 avg_loss = 3.34610\n",
      "epoch no.3 train no.274830  loss = 4.62369 avg_loss = 3.40112\n",
      "epoch no.3 train no.274840  loss = 6.52851 avg_loss = 3.47964\n",
      "epoch no.3 train no.274850  loss = 3.58668 avg_loss = 3.46685\n",
      "epoch no.3 train no.274860  loss = 2.48951 avg_loss = 3.42641\n",
      "epoch no.3 train no.274870  loss = 1.98278 avg_loss = 3.41455\n",
      "epoch no.3 train no.274880  loss = 5.14934 avg_loss = 3.46589\n",
      "epoch no.3 train no.274890  loss = 3.27911 avg_loss = 3.47200\n",
      "epoch no.3 train no.274900  loss = 2.07421 avg_loss = 3.47159\n",
      "epoch no.3 train no.274910  loss = 3.37269 avg_loss = 3.50608\n",
      "epoch no.3 train no.274920  loss = 3.65727 avg_loss = 3.47969\n",
      "epoch no.3 train no.274930  loss = 2.24223 avg_loss = 3.47952\n",
      "epoch no.3 train no.274940  loss = 3.89145 avg_loss = 3.45100\n",
      "epoch no.3 train no.274950  loss = 0.96956 avg_loss = 3.40232\n",
      "epoch no.3 train no.274960  loss = 3.48210 avg_loss = 3.40432\n",
      "epoch no.3 train no.274970  loss = 3.50004 avg_loss = 3.42219\n",
      "epoch no.3 train no.274980  loss = 2.24803 avg_loss = 3.36480\n",
      "epoch no.3 train no.274990  loss = 4.63946 avg_loss = 3.39453\n",
      "epoch no.3 train no.275000  loss = 3.56069 avg_loss = 3.42228\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '▁좋은', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 딱 좋은 곡</s>\n",
      "epoch no.3 train no.275010  loss = 4.03658 avg_loss = 3.41585\n",
      "epoch no.3 train no.275020  loss = 2.59811 avg_loss = 3.39969\n",
      "epoch no.3 train no.275030  loss = 2.75437 avg_loss = 3.38464\n",
      "epoch no.3 train no.275040  loss = 2.31968 avg_loss = 3.37525\n",
      "epoch no.3 train no.275050  loss = 4.67480 avg_loss = 3.36663\n",
      "epoch no.3 train no.275060  loss = 3.60118 avg_loss = 3.35156\n",
      "epoch no.3 train no.275070  loss = 2.18364 avg_loss = 3.34850\n",
      "epoch no.3 train no.275080  loss = 2.57642 avg_loss = 3.33224\n",
      "epoch no.3 train no.275090  loss = 4.20618 avg_loss = 3.36590\n",
      "epoch no.3 train no.275100  loss = 3.94891 avg_loss = 3.40963\n",
      "epoch no.3 train no.275110  loss = 2.85046 avg_loss = 3.41580\n",
      "epoch no.3 train no.275120  loss = 6.05886 avg_loss = 3.47480\n",
      "epoch no.3 train no.275130  loss = 4.00393 avg_loss = 3.47409\n",
      "epoch no.3 train no.275140  loss = 4.42845 avg_loss = 3.49455\n",
      "epoch no.3 train no.275150  loss = 3.53375 avg_loss = 3.51760\n",
      "epoch no.3 train no.275160  loss = 2.33637 avg_loss = 3.52737\n",
      "epoch no.3 train no.275170  loss = 2.79147 avg_loss = 3.49246\n",
      "epoch no.3 train no.275180  loss = 3.42171 avg_loss = 3.50093\n",
      "epoch no.3 train no.275190  loss = 3.94477 avg_loss = 3.49023\n",
      "epoch no.3 train no.275200  loss = 3.35934 avg_loss = 3.48176\n",
      "epoch no.3 train no.275210  loss = 4.35906 avg_loss = 3.50252\n",
      "epoch no.3 train no.275220  loss = 3.14929 avg_loss = 3.53583\n",
      "epoch no.3 train no.275230  loss = 2.38782 avg_loss = 3.50444\n",
      "epoch no.3 train no.275240  loss = 2.66969 avg_loss = 3.52108\n",
      "epoch no.3 train no.275250  loss = 2.14320 avg_loss = 3.55455\n",
      "epoch no.3 train no.275260  loss = 4.01000 avg_loss = 3.56458\n",
      "epoch no.3 train no.275270  loss = 1.62176 avg_loss = 3.50573\n",
      "epoch no.3 train no.275280  loss = 2.80613 avg_loss = 3.51345\n",
      "epoch no.3 train no.275290  loss = 2.31745 avg_loss = 3.49673\n",
      "epoch no.3 train no.275300  loss = 2.45061 avg_loss = 3.47932\n",
      "epoch no.3 train no.275310  loss = 3.95419 avg_loss = 3.45339\n",
      "epoch no.3 train no.275320  loss = 3.66215 avg_loss = 3.43024\n",
      "epoch no.3 train no.275330  loss = 1.71242 avg_loss = 3.40111\n",
      "epoch no.3 train no.275340  loss = 3.22045 avg_loss = 3.37185\n",
      "epoch no.3 train no.275350  loss = 3.85008 avg_loss = 3.40591\n",
      "epoch no.3 train no.275360  loss = 3.36034 avg_loss = 3.42207\n",
      "epoch no.3 train no.275370  loss = 3.80206 avg_loss = 3.43191\n",
      "epoch no.3 train no.275380  loss = 2.71766 avg_loss = 3.36230\n",
      "epoch no.3 train no.275390  loss = 2.29426 avg_loss = 3.40618\n",
      "epoch no.3 train no.275400  loss = 2.71905 avg_loss = 3.40460\n",
      "epoch no.3 train no.275410  loss = 4.03013 avg_loss = 3.44069\n",
      "epoch no.3 train no.275420  loss = 5.06394 avg_loss = 3.43641\n",
      "epoch no.3 train no.275430  loss = 3.62204 avg_loss = 3.44681\n",
      "epoch no.3 train no.275440  loss = 3.36199 avg_loss = 3.39659\n",
      "epoch no.3 train no.275450  loss = 2.93097 avg_loss = 3.37176\n",
      "epoch no.3 train no.275460  loss = 3.62063 avg_loss = 3.41183\n",
      "epoch no.3 train no.275470  loss = 3.00260 avg_loss = 3.37441\n",
      "epoch no.3 train no.275480  loss = 4.33227 avg_loss = 3.34534\n",
      "epoch no.3 train no.275490  loss = 2.58004 avg_loss = 3.37350\n",
      "epoch no.3 train no.275500  loss = 3.36579 avg_loss = 3.38875\n",
      "epoch no.3 train no.275510  loss = 4.39964 avg_loss = 3.39925\n",
      "epoch no.3 train no.275520  loss = 3.99288 avg_loss = 3.38966\n",
      "epoch no.3 train no.275530  loss = 4.26665 avg_loss = 3.39393\n",
      "epoch no.3 train no.275540  loss = 3.29236 avg_loss = 3.35706\n",
      "epoch no.3 train no.275550  loss = 2.87357 avg_loss = 3.35914\n",
      "epoch no.3 train no.275560  loss = 4.58699 avg_loss = 3.36603\n",
      "epoch no.3 train no.275570  loss = 2.29656 avg_loss = 3.40268\n",
      "epoch no.3 train no.275580  loss = 3.43668 avg_loss = 3.45458\n",
      "epoch no.3 train no.275590  loss = 3.93027 avg_loss = 3.43488\n",
      "epoch no.3 train no.275600  loss = 1.68126 avg_loss = 3.38877\n",
      "epoch no.3 train no.275610  loss = 2.98180 avg_loss = 3.37464\n",
      "epoch no.3 train no.275620  loss = 2.28074 avg_loss = 3.43653\n",
      "epoch no.3 train no.275630  loss = 2.71510 avg_loss = 3.48071\n",
      "epoch no.3 train no.275640  loss = 3.31944 avg_loss = 3.50026\n",
      "epoch no.3 train no.275650  loss = 3.00232 avg_loss = 3.49949\n",
      "epoch no.3 train no.275660  loss = 3.35423 avg_loss = 3.49966\n",
      "epoch no.3 train no.275670  loss = 2.48494 avg_loss = 3.51991\n",
      "epoch no.3 train no.275680  loss = 2.59273 avg_loss = 3.46331\n",
      "epoch no.3 train no.275690  loss = 4.29953 avg_loss = 3.44613\n",
      "epoch no.3 train no.275700  loss = 2.27438 avg_loss = 3.44904\n",
      "epoch no.3 train no.275710  loss = 2.51259 avg_loss = 3.44458\n",
      "epoch no.3 train no.275720  loss = 3.03641 avg_loss = 3.44218\n",
      "epoch no.3 train no.275730  loss = 3.11007 avg_loss = 3.45883\n",
      "epoch no.3 train no.275740  loss = 3.28723 avg_loss = 3.44461\n",
      "epoch no.3 train no.275750  loss = 5.60060 avg_loss = 3.44775\n",
      "epoch no.3 train no.275760  loss = 3.28633 avg_loss = 3.38883\n",
      "epoch no.3 train no.275770  loss = 3.72530 avg_loss = 3.40853\n",
      "epoch no.3 train no.275780  loss = 3.44943 avg_loss = 3.37885\n",
      "epoch no.3 train no.275790  loss = 4.27616 avg_loss = 3.38362\n",
      "epoch no.3 train no.275800  loss = 3.03759 avg_loss = 3.35880\n",
      "epoch no.3 train no.275810  loss = 4.09756 avg_loss = 3.37757\n",
      "epoch no.3 train no.275820  loss = 2.76021 avg_loss = 3.36473\n",
      "epoch no.3 train no.275830  loss = 3.29401 avg_loss = 3.34237\n",
      "epoch no.3 train no.275840  loss = 3.44852 avg_loss = 3.38056\n",
      "epoch no.3 train no.275850  loss = 3.15276 avg_loss = 3.36997\n",
      "epoch no.3 train no.275860  loss = 3.87052 avg_loss = 3.40382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.275870  loss = 2.91248 avg_loss = 3.41925\n",
      "epoch no.3 train no.275880  loss = 3.20758 avg_loss = 3.45471\n",
      "epoch no.3 train no.275890  loss = 3.00373 avg_loss = 3.46585\n",
      "epoch no.3 train no.275900  loss = 2.77591 avg_loss = 3.41863\n",
      "epoch no.3 train no.275910  loss = 3.07573 avg_loss = 3.40715\n",
      "epoch no.3 train no.275920  loss = 4.29277 avg_loss = 3.38456\n",
      "epoch no.3 train no.275930  loss = 2.75459 avg_loss = 3.43723\n",
      "epoch no.3 train no.275940  loss = 2.36112 avg_loss = 3.42831\n",
      "epoch no.3 train no.275950  loss = 2.48543 avg_loss = 3.44304\n",
      "epoch no.3 train no.275960  loss = 2.33943 avg_loss = 3.42810\n",
      "epoch no.3 train no.275970  loss = 3.49004 avg_loss = 3.38994\n",
      "epoch no.3 train no.275980  loss = 3.94392 avg_loss = 3.39382\n",
      "epoch no.3 train no.275990  loss = 4.86799 avg_loss = 3.43696\n",
      "epoch no.3 train no.276000  loss = 3.73502 avg_loss = 3.45351\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '용', '싶', '을', '▁날', '</s>']\n",
      "기분전환 하고싶은날</s>\n",
      "epoch no.3 train no.276010  loss = 2.46260 avg_loss = 3.45847\n",
      "epoch no.3 train no.276020  loss = 2.49902 avg_loss = 3.48079\n",
      "epoch no.3 train no.276030  loss = 2.87267 avg_loss = 3.48578\n",
      "epoch no.3 train no.276040  loss = 2.62495 avg_loss = 3.45256\n",
      "epoch no.3 train no.276050  loss = 2.93219 avg_loss = 3.43908\n",
      "epoch no.3 train no.276060  loss = 2.54373 avg_loss = 3.40579\n",
      "epoch no.3 train no.276070  loss = 2.58274 avg_loss = 3.42478\n",
      "epoch no.3 train no.276080  loss = 4.91714 avg_loss = 3.40454\n",
      "epoch no.3 train no.276090  loss = 3.56226 avg_loss = 3.34094\n",
      "epoch no.3 train no.276100  loss = 4.57579 avg_loss = 3.35972\n",
      "epoch no.3 train no.276110  loss = 3.13838 avg_loss = 3.36860\n",
      "epoch no.3 train no.276120  loss = 3.74845 avg_loss = 3.38752\n",
      "epoch no.3 train no.276130  loss = 2.26991 avg_loss = 3.38444\n",
      "epoch no.3 train no.276140  loss = 4.39713 avg_loss = 3.38752\n",
      "epoch no.3 train no.276150  loss = 3.07998 avg_loss = 3.38762\n",
      "epoch no.3 train no.276160  loss = 3.09986 avg_loss = 3.38249\n",
      "epoch no.3 train no.276170  loss = 2.90554 avg_loss = 3.37253\n",
      "epoch no.3 train no.276180  loss = 3.72726 avg_loss = 3.39035\n",
      "epoch no.3 train no.276190  loss = 2.04526 avg_loss = 3.39191\n",
      "epoch no.3 train no.276200  loss = 3.90389 avg_loss = 3.37236\n",
      "epoch no.3 train no.276210  loss = 3.42751 avg_loss = 3.35081\n",
      "epoch no.3 train no.276220  loss = 2.36651 avg_loss = 3.31570\n",
      "epoch no.3 train no.276230  loss = 2.28028 avg_loss = 3.30722\n",
      "epoch no.3 train no.276240  loss = 3.86526 avg_loss = 3.32135\n",
      "epoch no.3 train no.276250  loss = 3.98077 avg_loss = 3.35721\n",
      "epoch no.3 train no.276260  loss = 8.37335 avg_loss = 3.39033\n",
      "epoch no.3 train no.276270  loss = 3.95345 avg_loss = 3.38909\n",
      "epoch no.3 train no.276280  loss = 3.24043 avg_loss = 3.40013\n",
      "epoch no.3 train no.276290  loss = 3.57144 avg_loss = 3.40978\n",
      "epoch no.3 train no.276300  loss = 2.94067 avg_loss = 3.44243\n",
      "epoch no.3 train no.276310  loss = 3.96771 avg_loss = 3.42023\n",
      "epoch no.3 train no.276320  loss = 5.45545 avg_loss = 3.46331\n",
      "epoch no.3 train no.276330  loss = 3.37631 avg_loss = 3.46734\n",
      "epoch no.3 train no.276340  loss = 2.81737 avg_loss = 3.46875\n",
      "epoch no.3 train no.276350  loss = 2.46175 avg_loss = 3.42986\n",
      "epoch no.3 train no.276360  loss = 3.07815 avg_loss = 3.39438\n",
      "epoch no.3 train no.276370  loss = 3.22855 avg_loss = 3.39099\n",
      "epoch no.3 train no.276380  loss = 3.98299 avg_loss = 3.38216\n",
      "epoch no.3 train no.276390  loss = 2.31066 avg_loss = 3.35668\n",
      "epoch no.3 train no.276400  loss = 2.16850 avg_loss = 3.33339\n",
      "epoch no.3 train no.276410  loss = 1.75948 avg_loss = 3.32891\n",
      "epoch no.3 train no.276420  loss = 4.03278 avg_loss = 3.34688\n",
      "epoch no.3 train no.276430  loss = 2.96360 avg_loss = 3.35129\n",
      "epoch no.3 train no.276440  loss = 4.32205 avg_loss = 3.37328\n",
      "epoch no.3 train no.276450  loss = 4.45853 avg_loss = 3.37941\n",
      "epoch no.3 train no.276460  loss = 3.87400 avg_loss = 3.36962\n",
      "epoch no.3 train no.276470  loss = 2.76821 avg_loss = 3.37735\n",
      "epoch no.3 train no.276480  loss = 3.37317 avg_loss = 3.37852\n",
      "epoch no.3 train no.276490  loss = 3.28354 avg_loss = 3.36428\n",
      "epoch no.3 train no.276500  loss = 2.39017 avg_loss = 3.40309\n",
      "epoch no.3 train no.276510  loss = 4.06375 avg_loss = 3.40002\n",
      "epoch no.3 train no.276520  loss = 4.01041 avg_loss = 3.44198\n",
      "epoch no.3 train no.276530  loss = 4.28313 avg_loss = 3.48313\n",
      "epoch no.3 train no.276540  loss = 4.88583 avg_loss = 3.51048\n",
      "epoch no.3 train no.276550  loss = 2.11319 avg_loss = 3.52910\n",
      "epoch no.3 train no.276560  loss = 2.97013 avg_loss = 3.54540\n",
      "epoch no.3 train no.276570  loss = 3.30512 avg_loss = 3.49498\n",
      "epoch no.3 train no.276580  loss = 3.09295 avg_loss = 3.48048\n",
      "epoch no.3 train no.276590  loss = 3.27800 avg_loss = 3.44185\n",
      "epoch no.3 train no.276600  loss = 1.86107 avg_loss = 3.42538\n",
      "epoch no.3 train no.276610  loss = 3.44966 avg_loss = 3.45970\n",
      "epoch no.3 train no.276620  loss = 2.63516 avg_loss = 3.38010\n",
      "epoch no.3 train no.276630  loss = 2.98697 avg_loss = 3.40315\n",
      "epoch no.3 train no.276640  loss = 3.53738 avg_loss = 3.44357\n",
      "epoch no.3 train no.276650  loss = 2.37379 avg_loss = 3.39974\n",
      "epoch no.3 train no.276660  loss = 3.24049 avg_loss = 3.41344\n",
      "epoch no.3 train no.276670  loss = 1.86459 avg_loss = 3.34960\n",
      "epoch no.3 train no.276680  loss = 3.28238 avg_loss = 3.36567\n",
      "epoch no.3 train no.276690  loss = 3.97116 avg_loss = 3.36061\n",
      "epoch no.3 train no.276700  loss = 4.73981 avg_loss = 3.37170\n",
      "epoch no.3 train no.276710  loss = 4.22275 avg_loss = 3.38145\n",
      "epoch no.3 train no.276720  loss = 3.62768 avg_loss = 3.36920\n",
      "epoch no.3 train no.276730  loss = 3.63739 avg_loss = 3.34410\n",
      "epoch no.3 train no.276740  loss = 2.63499 avg_loss = 3.36804\n",
      "epoch no.3 train no.276750  loss = 4.49589 avg_loss = 3.39072\n",
      "epoch no.3 train no.276760  loss = 3.40505 avg_loss = 3.37868\n",
      "epoch no.3 train no.276770  loss = 2.68909 avg_loss = 3.42356\n",
      "epoch no.3 train no.276780  loss = 5.21548 avg_loss = 3.38363\n",
      "epoch no.3 train no.276790  loss = 4.48611 avg_loss = 3.41694\n",
      "epoch no.3 train no.276800  loss = 3.24362 avg_loss = 3.42902\n",
      "epoch no.3 train no.276810  loss = 3.21247 avg_loss = 3.49040\n",
      "epoch no.3 train no.276820  loss = 3.64369 avg_loss = 3.56017\n",
      "epoch no.3 train no.276830  loss = 2.32035 avg_loss = 3.49413\n",
      "epoch no.3 train no.276840  loss = 1.86488 avg_loss = 3.45694\n",
      "epoch no.3 train no.276850  loss = 4.22128 avg_loss = 3.48387\n",
      "epoch no.3 train no.276860  loss = 4.03994 avg_loss = 3.46991\n",
      "epoch no.3 train no.276870  loss = 3.05233 avg_loss = 3.44679\n",
      "epoch no.3 train no.276880  loss = 4.63493 avg_loss = 3.49423\n",
      "epoch no.3 train no.276890  loss = 2.57511 avg_loss = 3.43663\n",
      "epoch no.3 train no.276900  loss = 2.69330 avg_loss = 3.41784\n",
      "epoch no.3 train no.276910  loss = 3.62331 avg_loss = 3.41394\n",
      "epoch no.3 train no.276920  loss = 3.84053 avg_loss = 3.41016\n",
      "epoch no.3 train no.276930  loss = 4.60303 avg_loss = 3.43041\n",
      "epoch no.3 train no.276940  loss = 3.98118 avg_loss = 3.38574\n",
      "epoch no.3 train no.276950  loss = 4.64980 avg_loss = 3.37934\n",
      "epoch no.3 train no.276960  loss = 3.20009 avg_loss = 3.37008\n",
      "epoch no.3 train no.276970  loss = 2.92199 avg_loss = 3.38418\n",
      "epoch no.3 train no.276980  loss = 2.56862 avg_loss = 3.40225\n",
      "epoch no.3 train no.276990  loss = 5.09657 avg_loss = 3.41738\n",
      "epoch no.3 train no.277000  loss = 3.76281 avg_loss = 3.44486\n",
      "1\n",
      "to_tokens: ['▁비', '전환', '용', '</s>']\n",
      "기분전환하기</s>\n",
      "epoch no.3 train no.277010  loss = 4.33076 avg_loss = 3.49829\n",
      "epoch no.3 train no.277020  loss = 2.56200 avg_loss = 3.52132\n",
      "epoch no.3 train no.277030  loss = 3.25564 avg_loss = 3.51304\n",
      "epoch no.3 train no.277040  loss = 2.80214 avg_loss = 3.47884\n",
      "epoch no.3 train no.277050  loss = 3.80989 avg_loss = 3.48869\n",
      "epoch no.3 train no.277060  loss = 4.85800 avg_loss = 3.48962\n",
      "epoch no.3 train no.277070  loss = 4.14750 avg_loss = 3.43976\n",
      "epoch no.3 train no.277080  loss = 3.46431 avg_loss = 3.48441\n",
      "epoch no.3 train no.277090  loss = 3.59027 avg_loss = 3.47453\n",
      "epoch no.3 train no.277100  loss = 3.08943 avg_loss = 3.45835\n",
      "epoch no.3 train no.277110  loss = 2.70013 avg_loss = 3.41965\n",
      "epoch no.3 train no.277120  loss = 3.68517 avg_loss = 3.40862\n",
      "epoch no.3 train no.277130  loss = 5.12472 avg_loss = 3.43484\n",
      "epoch no.3 train no.277140  loss = 2.79948 avg_loss = 3.41431\n",
      "epoch no.3 train no.277150  loss = 2.39512 avg_loss = 3.35165\n",
      "epoch no.3 train no.277160  loss = 4.90244 avg_loss = 3.38172\n",
      "epoch no.3 train no.277170  loss = 3.97504 avg_loss = 3.45792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.277180  loss = 3.01284 avg_loss = 3.45385\n",
      "epoch no.3 train no.277190  loss = 4.18723 avg_loss = 3.48341\n",
      "epoch no.3 train no.277200  loss = 3.73870 avg_loss = 3.50418\n",
      "epoch no.3 train no.277210  loss = 3.26628 avg_loss = 3.46516\n",
      "epoch no.3 train no.277220  loss = 2.32842 avg_loss = 3.41737\n",
      "epoch no.3 train no.277230  loss = 3.71651 avg_loss = 3.48006\n",
      "epoch no.3 train no.277240  loss = 3.03804 avg_loss = 3.45327\n",
      "epoch no.3 train no.277250  loss = 2.94783 avg_loss = 3.45615\n",
      "epoch no.3 train no.277260  loss = 4.75592 avg_loss = 3.50167\n",
      "epoch no.3 train no.277270  loss = 2.91196 avg_loss = 3.45479\n",
      "epoch no.3 train no.277280  loss = 2.25074 avg_loss = 3.43223\n",
      "epoch no.3 train no.277290  loss = 2.93919 avg_loss = 3.41902\n",
      "epoch no.3 train no.277300  loss = 4.69266 avg_loss = 3.46992\n",
      "epoch no.3 train no.277310  loss = 4.02693 avg_loss = 3.48451\n",
      "epoch no.3 train no.277320  loss = 3.99755 avg_loss = 3.48508\n",
      "epoch no.3 train no.277330  loss = 2.81192 avg_loss = 3.49259\n",
      "epoch no.3 train no.277340  loss = 3.07489 avg_loss = 3.50031\n",
      "epoch no.3 train no.277350  loss = 3.18996 avg_loss = 3.47697\n",
      "epoch no.3 train no.277360  loss = 3.41893 avg_loss = 3.43927\n",
      "epoch no.3 train no.277370  loss = 3.61425 avg_loss = 3.47424\n",
      "epoch no.3 train no.277380  loss = 4.56483 avg_loss = 3.47093\n",
      "epoch no.3 train no.277390  loss = 3.76342 avg_loss = 3.46788\n",
      "epoch no.3 train no.277400  loss = 3.35075 avg_loss = 3.43308\n",
      "epoch no.3 train no.277410  loss = 4.22428 avg_loss = 3.45219\n",
      "epoch no.3 train no.277420  loss = 4.59446 avg_loss = 3.42146\n",
      "epoch no.3 train no.277430  loss = 2.55660 avg_loss = 3.45240\n",
      "epoch no.3 train no.277440  loss = 3.05135 avg_loss = 3.38570\n",
      "epoch no.3 train no.277450  loss = 3.72092 avg_loss = 3.44758\n",
      "epoch no.3 train no.277460  loss = 4.43902 avg_loss = 3.47263\n",
      "epoch no.3 train no.277470  loss = 4.12521 avg_loss = 3.44589\n",
      "epoch no.3 train no.277480  loss = 3.53863 avg_loss = 3.42195\n",
      "epoch no.3 train no.277490  loss = 3.11793 avg_loss = 3.41620\n",
      "epoch no.3 train no.277500  loss = 3.93708 avg_loss = 3.43853\n",
      "epoch no.3 train no.277510  loss = 2.12226 avg_loss = 3.45265\n",
      "epoch no.3 train no.277520  loss = 2.60682 avg_loss = 3.45967\n",
      "epoch no.3 train no.277530  loss = 2.57051 avg_loss = 3.46932\n",
      "epoch no.3 train no.277540  loss = 2.52828 avg_loss = 3.48740\n",
      "epoch no.3 train no.277550  loss = 3.69071 avg_loss = 3.56245\n",
      "epoch no.3 train no.277560  loss = 2.67733 avg_loss = 3.53615\n",
      "epoch no.3 train no.277570  loss = 2.03287 avg_loss = 3.51869\n",
      "epoch no.3 train no.277580  loss = 2.59544 avg_loss = 3.46040\n",
      "epoch no.3 train no.277590  loss = 3.74147 avg_loss = 3.47333\n",
      "epoch no.3 train no.277600  loss = 5.70656 avg_loss = 3.45258\n",
      "epoch no.3 train no.277610  loss = 2.96030 avg_loss = 3.46306\n",
      "epoch no.3 train no.277620  loss = 7.02262 avg_loss = 3.49232\n",
      "epoch no.3 train no.277630  loss = 2.18592 avg_loss = 3.44802\n",
      "epoch no.3 train no.277640  loss = 3.43881 avg_loss = 3.52916\n",
      "epoch no.3 train no.277650  loss = 4.88382 avg_loss = 3.54793\n",
      "epoch no.3 train no.277660  loss = 3.08161 avg_loss = 3.53492\n",
      "epoch no.3 train no.277670  loss = 3.26009 avg_loss = 3.49462\n",
      "epoch no.3 train no.277680  loss = 3.50042 avg_loss = 3.51967\n",
      "epoch no.3 train no.277690  loss = 3.11918 avg_loss = 3.56126\n",
      "epoch no.3 train no.277700  loss = 2.85982 avg_loss = 3.54405\n",
      "epoch no.3 train no.277710  loss = 3.75003 avg_loss = 3.52089\n",
      "epoch no.3 train no.277720  loss = 4.45376 avg_loss = 3.54457\n",
      "epoch no.3 train no.277730  loss = 1.64947 avg_loss = 3.51092\n",
      "epoch no.3 train no.277740  loss = 3.87942 avg_loss = 3.51591\n",
      "epoch no.3 train no.277750  loss = 2.55397 avg_loss = 3.46236\n",
      "epoch no.3 train no.277760  loss = 5.69816 avg_loss = 3.48390\n",
      "epoch no.3 train no.277770  loss = 2.82775 avg_loss = 3.45799\n",
      "epoch no.3 train no.277780  loss = 2.20759 avg_loss = 3.44932\n",
      "epoch no.3 train no.277790  loss = 3.42094 avg_loss = 3.46565\n",
      "epoch no.3 train no.277800  loss = 3.22068 avg_loss = 3.41306\n",
      "epoch no.3 train no.277810  loss = 1.94648 avg_loss = 3.42577\n",
      "epoch no.3 train no.277820  loss = 3.57488 avg_loss = 3.41441\n",
      "epoch no.3 train no.277830  loss = 4.51369 avg_loss = 3.46267\n",
      "epoch no.3 train no.277840  loss = 2.11664 avg_loss = 3.48741\n",
      "epoch no.3 train no.277850  loss = 2.69461 avg_loss = 3.50130\n",
      "epoch no.3 train no.277860  loss = 4.41742 avg_loss = 3.51819\n",
      "epoch no.3 train no.277870  loss = 2.57715 avg_loss = 3.53117\n",
      "epoch no.3 train no.277880  loss = 1.12059 avg_loss = 3.48953\n",
      "epoch no.3 train no.277890  loss = 1.48041 avg_loss = 3.47330\n",
      "epoch no.3 train no.277900  loss = 3.19411 avg_loss = 3.45108\n",
      "epoch no.3 train no.277910  loss = 3.64925 avg_loss = 3.51397\n",
      "epoch no.3 train no.277920  loss = 2.62780 avg_loss = 3.44996\n",
      "epoch no.3 train no.277930  loss = 2.15594 avg_loss = 3.41734\n",
      "epoch no.3 train no.277940  loss = 3.43934 avg_loss = 3.39911\n",
      "epoch no.3 train no.277950  loss = 4.97450 avg_loss = 3.43136\n",
      "epoch no.3 train no.277960  loss = 2.20755 avg_loss = 3.40822\n",
      "epoch no.3 train no.277970  loss = 2.64934 avg_loss = 3.38415\n",
      "epoch no.3 train no.277980  loss = 4.96297 avg_loss = 3.34558\n",
      "epoch no.3 train no.277990  loss = 2.96759 avg_loss = 3.33629\n",
      "epoch no.3 train no.278000  loss = 2.17597 avg_loss = 3.35460\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁아이돌', '노래', '</s>']\n",
      "기분전환 신나는 여름노래</s>\n",
      "epoch no.3 train no.278010  loss = 2.93931 avg_loss = 3.34485\n",
      "epoch no.3 train no.278020  loss = 2.32411 avg_loss = 3.34665\n",
      "epoch no.3 train no.278030  loss = 2.90725 avg_loss = 3.37211\n",
      "epoch no.3 train no.278040  loss = 2.72982 avg_loss = 3.35558\n",
      "epoch no.3 train no.278050  loss = 2.10988 avg_loss = 3.36039\n",
      "epoch no.3 train no.278060  loss = 3.97076 avg_loss = 3.42826\n",
      "epoch no.3 train no.278070  loss = 2.84156 avg_loss = 3.38841\n",
      "epoch no.3 train no.278080  loss = 3.45477 avg_loss = 3.42291\n",
      "epoch no.3 train no.278090  loss = 3.69481 avg_loss = 3.45845\n",
      "epoch no.3 train no.278100  loss = 2.73086 avg_loss = 3.40659\n",
      "epoch no.3 train no.278110  loss = 3.06569 avg_loss = 3.43161\n",
      "epoch no.3 train no.278120  loss = 3.34476 avg_loss = 3.46347\n",
      "epoch no.3 train no.278130  loss = 4.22304 avg_loss = 3.46220\n",
      "epoch no.3 train no.278140  loss = 3.98719 avg_loss = 3.49536\n",
      "epoch no.3 train no.278150  loss = 4.45335 avg_loss = 3.51399\n",
      "epoch no.3 train no.278160  loss = 3.40661 avg_loss = 3.47955\n",
      "epoch no.3 train no.278170  loss = 3.64603 avg_loss = 3.44047\n",
      "epoch no.3 train no.278180  loss = 3.14237 avg_loss = 3.40622\n",
      "epoch no.3 train no.278190  loss = 1.42847 avg_loss = 3.36071\n",
      "epoch no.3 train no.278200  loss = 3.04652 avg_loss = 3.41327\n",
      "epoch no.3 train no.278210  loss = 3.63312 avg_loss = 3.43828\n",
      "epoch no.3 train no.278220  loss = 3.27966 avg_loss = 3.41441\n",
      "epoch no.3 train no.278230  loss = 3.75482 avg_loss = 3.41668\n",
      "epoch no.3 train no.278240  loss = 2.06260 avg_loss = 3.42865\n",
      "epoch no.3 train no.278250  loss = 2.82931 avg_loss = 3.43844\n",
      "epoch no.3 train no.278260  loss = 2.19849 avg_loss = 3.41364\n",
      "epoch no.3 train no.278270  loss = 3.42547 avg_loss = 3.33947\n",
      "epoch no.3 train no.278280  loss = 3.74271 avg_loss = 3.33837\n",
      "epoch no.3 train no.278290  loss = 4.82450 avg_loss = 3.39578\n",
      "epoch no.3 train no.278300  loss = 3.61421 avg_loss = 3.44548\n",
      "epoch no.3 train no.278310  loss = 4.77198 avg_loss = 3.48187\n",
      "epoch no.3 train no.278320  loss = 1.68767 avg_loss = 3.43712\n",
      "epoch no.3 train no.278330  loss = 4.28838 avg_loss = 3.40950\n",
      "epoch no.3 train no.278340  loss = 3.81576 avg_loss = 3.45071\n",
      "epoch no.3 train no.278350  loss = 4.34685 avg_loss = 3.48793\n",
      "epoch no.3 train no.278360  loss = 2.48832 avg_loss = 3.46894\n",
      "epoch no.3 train no.278370  loss = 4.30070 avg_loss = 3.49286\n",
      "epoch no.3 train no.278380  loss = 3.89358 avg_loss = 3.51646\n",
      "epoch no.3 train no.278390  loss = 2.80319 avg_loss = 3.45603\n",
      "epoch no.3 train no.278400  loss = 3.63337 avg_loss = 3.46338\n",
      "epoch no.3 train no.278410  loss = 3.05176 avg_loss = 3.45631\n",
      "epoch no.3 train no.278420  loss = 2.94725 avg_loss = 3.44220\n",
      "epoch no.3 train no.278430  loss = 3.89486 avg_loss = 3.38341\n",
      "epoch no.3 train no.278440  loss = 3.45477 avg_loss = 3.38507\n",
      "epoch no.3 train no.278450  loss = 0.95526 avg_loss = 3.38755\n",
      "epoch no.3 train no.278460  loss = 1.78601 avg_loss = 3.32701\n",
      "epoch no.3 train no.278470  loss = 3.33585 avg_loss = 3.29567\n",
      "epoch no.3 train no.278480  loss = 4.24432 avg_loss = 3.33050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.278490  loss = 3.00169 avg_loss = 3.31367\n",
      "epoch no.3 train no.278500  loss = 2.49305 avg_loss = 3.29486\n",
      "epoch no.3 train no.278510  loss = 4.88906 avg_loss = 3.30628\n",
      "epoch no.3 train no.278520  loss = 4.77633 avg_loss = 3.31386\n",
      "epoch no.3 train no.278530  loss = 2.50542 avg_loss = 3.31917\n",
      "epoch no.3 train no.278540  loss = 2.45032 avg_loss = 3.32727\n",
      "epoch no.3 train no.278550  loss = 2.19822 avg_loss = 3.34670\n",
      "epoch no.3 train no.278560  loss = 2.48878 avg_loss = 3.34560\n",
      "epoch no.3 train no.278570  loss = 4.20855 avg_loss = 3.38273\n",
      "epoch no.3 train no.278580  loss = 4.14485 avg_loss = 3.35496\n",
      "epoch no.3 train no.278590  loss = 3.72783 avg_loss = 3.34523\n",
      "epoch no.3 train no.278600  loss = 4.19505 avg_loss = 3.38561\n",
      "epoch no.3 train no.278610  loss = 3.76235 avg_loss = 3.39800\n",
      "epoch no.3 train no.278620  loss = 3.68903 avg_loss = 3.41833\n",
      "epoch no.3 train no.278630  loss = 2.67162 avg_loss = 3.45598\n",
      "epoch no.3 train no.278640  loss = 3.19244 avg_loss = 3.45354\n",
      "epoch no.3 train no.278650  loss = 2.36638 avg_loss = 3.44346\n",
      "epoch no.3 train no.278660  loss = 3.78715 avg_loss = 3.45047\n",
      "epoch no.3 train no.278670  loss = 4.13522 avg_loss = 3.42554\n",
      "epoch no.3 train no.278680  loss = 2.18962 avg_loss = 3.42578\n",
      "epoch no.3 train no.278690  loss = 4.41092 avg_loss = 3.45210\n",
      "epoch no.3 train no.278700  loss = 2.88106 avg_loss = 3.50085\n",
      "epoch no.3 train no.278710  loss = 2.22757 avg_loss = 3.43654\n",
      "epoch no.3 train no.278720  loss = 3.09871 avg_loss = 3.39887\n",
      "epoch no.3 train no.278730  loss = 5.54325 avg_loss = 3.41810\n",
      "epoch no.3 train no.278740  loss = 3.13833 avg_loss = 3.39365\n",
      "epoch no.3 train no.278750  loss = 3.45424 avg_loss = 3.38997\n",
      "epoch no.3 train no.278760  loss = 5.04035 avg_loss = 3.40933\n",
      "epoch no.3 train no.278770  loss = 1.94794 avg_loss = 3.39300\n",
      "epoch no.3 train no.278780  loss = 3.61019 avg_loss = 3.37904\n",
      "epoch no.3 train no.278790  loss = 2.93195 avg_loss = 3.35368\n",
      "epoch no.3 train no.278800  loss = 3.63377 avg_loss = 3.38738\n",
      "epoch no.3 train no.278810  loss = 2.40494 avg_loss = 3.39620\n",
      "epoch no.3 train no.278820  loss = 4.14127 avg_loss = 3.39907\n",
      "epoch no.3 train no.278830  loss = 3.11997 avg_loss = 3.39147\n",
      "epoch no.3 train no.278840  loss = 3.06668 avg_loss = 3.35223\n",
      "epoch no.3 train no.278850  loss = 3.02710 avg_loss = 3.32975\n",
      "epoch no.3 train no.278860  loss = 5.02444 avg_loss = 3.33340\n",
      "epoch no.3 train no.278870  loss = 3.71584 avg_loss = 3.34577\n",
      "epoch no.3 train no.278880  loss = 2.65475 avg_loss = 3.33151\n",
      "epoch no.3 train no.278890  loss = 3.28400 avg_loss = 3.27509\n",
      "epoch no.3 train no.278900  loss = 2.63490 avg_loss = 3.27480\n",
      "epoch no.3 train no.278910  loss = 3.50547 avg_loss = 3.33448\n",
      "epoch no.3 train no.278920  loss = 1.23299 avg_loss = 3.29543\n",
      "epoch no.3 train no.278930  loss = 3.87757 avg_loss = 3.34349\n",
      "epoch no.3 train no.278940  loss = 4.15599 avg_loss = 3.34721\n",
      "epoch no.3 train no.278950  loss = 3.04262 avg_loss = 3.36557\n",
      "epoch no.3 train no.278960  loss = 5.02187 avg_loss = 3.35795\n",
      "epoch no.3 train no.278970  loss = 3.44634 avg_loss = 3.37374\n",
      "epoch no.3 train no.278980  loss = 3.91213 avg_loss = 3.37729\n",
      "epoch no.3 train no.278990  loss = 5.36359 avg_loss = 3.42275\n",
      "epoch no.3 train no.279000  loss = 2.05916 avg_loss = 3.40505\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.279010  loss = 3.83845 avg_loss = 3.39268\n",
      "epoch no.3 train no.279020  loss = 4.83694 avg_loss = 3.39360\n",
      "epoch no.3 train no.279030  loss = 2.65969 avg_loss = 3.34705\n",
      "epoch no.3 train no.279040  loss = 3.14420 avg_loss = 3.37672\n",
      "epoch no.3 train no.279050  loss = 3.35188 avg_loss = 3.40500\n",
      "epoch no.3 train no.279060  loss = 6.42116 avg_loss = 3.42588\n",
      "epoch no.3 train no.279070  loss = 5.45367 avg_loss = 3.46025\n",
      "epoch no.3 train no.279080  loss = 4.79079 avg_loss = 3.48163\n",
      "epoch no.3 train no.279090  loss = 2.72128 avg_loss = 3.49777\n",
      "epoch no.3 train no.279100  loss = 4.18041 avg_loss = 3.47230\n",
      "epoch no.3 train no.279110  loss = 3.46975 avg_loss = 3.46531\n",
      "epoch no.3 train no.279120  loss = 4.08475 avg_loss = 3.45276\n",
      "epoch no.3 train no.279130  loss = 2.12640 avg_loss = 3.40068\n",
      "epoch no.3 train no.279140  loss = 3.06025 avg_loss = 3.35930\n",
      "epoch no.3 train no.279150  loss = 2.25171 avg_loss = 3.34625\n",
      "epoch no.3 train no.279160  loss = 3.78728 avg_loss = 3.38269\n",
      "epoch no.3 train no.279170  loss = 2.44921 avg_loss = 3.36053\n",
      "epoch no.3 train no.279180  loss = 3.08149 avg_loss = 3.33583\n",
      "epoch no.3 train no.279190  loss = 2.36050 avg_loss = 3.35274\n",
      "epoch no.3 train no.279200  loss = 2.69596 avg_loss = 3.34533\n",
      "epoch no.3 train no.279210  loss = 3.70697 avg_loss = 3.30259\n",
      "epoch no.3 train no.279220  loss = 3.77617 avg_loss = 3.29653\n",
      "epoch no.3 train no.279230  loss = 3.15529 avg_loss = 3.30242\n",
      "epoch no.3 train no.279240  loss = 4.03397 avg_loss = 3.34138\n",
      "epoch no.3 train no.279250  loss = 2.91761 avg_loss = 3.34308\n",
      "epoch no.3 train no.279260  loss = 3.96346 avg_loss = 3.32657\n",
      "epoch no.3 train no.279270  loss = 2.67584 avg_loss = 3.32013\n",
      "epoch no.3 train no.279280  loss = 5.17325 avg_loss = 3.29726\n",
      "epoch no.3 train no.279290  loss = 2.96126 avg_loss = 3.33679\n",
      "epoch no.3 train no.279300  loss = 3.37295 avg_loss = 3.31151\n",
      "epoch no.3 train no.279310  loss = 3.48971 avg_loss = 3.34282\n",
      "epoch no.3 train no.279320  loss = 3.09964 avg_loss = 3.36387\n",
      "epoch no.3 train no.279330  loss = 5.27422 avg_loss = 3.37735\n",
      "epoch no.3 train no.279340  loss = 2.45964 avg_loss = 3.34937\n",
      "epoch no.3 train no.279350  loss = 3.66518 avg_loss = 3.36664\n",
      "epoch no.3 train no.279360  loss = 3.58496 avg_loss = 3.35928\n",
      "epoch no.3 train no.279370  loss = 4.28922 avg_loss = 3.35600\n",
      "epoch no.3 train no.279380  loss = 3.15002 avg_loss = 3.34555\n",
      "epoch no.3 train no.279390  loss = 3.09218 avg_loss = 3.36069\n",
      "epoch no.3 train no.279400  loss = 2.58054 avg_loss = 3.32886\n",
      "epoch no.3 train no.279410  loss = 2.95978 avg_loss = 3.33995\n",
      "epoch no.3 train no.279420  loss = 5.10410 avg_loss = 3.36597\n",
      "epoch no.3 train no.279430  loss = 3.57401 avg_loss = 3.36015\n",
      "epoch no.3 train no.279440  loss = 3.90662 avg_loss = 3.41888\n",
      "epoch no.3 train no.279450  loss = 3.61935 avg_loss = 3.45569\n",
      "epoch no.3 train no.279460  loss = 3.95026 avg_loss = 3.50243\n",
      "epoch no.3 train no.279470  loss = 2.48425 avg_loss = 3.48370\n",
      "epoch no.3 train no.279480  loss = 2.22554 avg_loss = 3.44920\n",
      "epoch no.3 train no.279490  loss = 6.70501 avg_loss = 3.44387\n",
      "epoch no.3 train no.279500  loss = 5.29270 avg_loss = 3.45711\n",
      "epoch no.3 train no.279510  loss = 2.86392 avg_loss = 3.42335\n",
      "epoch no.3 train no.279520  loss = 3.46469 avg_loss = 3.41143\n",
      "epoch no.3 train no.279530  loss = 1.89562 avg_loss = 3.39328\n",
      "epoch no.3 train no.279540  loss = 2.29648 avg_loss = 3.34775\n",
      "epoch no.3 train no.279550  loss = 3.28137 avg_loss = 3.34623\n",
      "epoch no.3 train no.279560  loss = 3.58170 avg_loss = 3.36172\n",
      "epoch no.3 train no.279570  loss = 4.82659 avg_loss = 3.36955\n",
      "epoch no.3 train no.279580  loss = 3.77540 avg_loss = 3.36652\n",
      "epoch no.3 train no.279590  loss = 4.06862 avg_loss = 3.36424\n",
      "epoch no.3 train no.279600  loss = 2.35811 avg_loss = 3.34372\n",
      "epoch no.3 train no.279610  loss = 4.20203 avg_loss = 3.36397\n",
      "epoch no.3 train no.279620  loss = 3.73808 avg_loss = 3.34698\n",
      "epoch no.3 train no.279630  loss = 3.65687 avg_loss = 3.35513\n",
      "epoch no.3 train no.279640  loss = 3.13523 avg_loss = 3.37214\n",
      "epoch no.3 train no.279650  loss = 2.23865 avg_loss = 3.37802\n",
      "epoch no.3 train no.279660  loss = 2.32133 avg_loss = 3.37637\n",
      "epoch no.3 train no.279670  loss = 4.09164 avg_loss = 3.40236\n",
      "epoch no.3 train no.279680  loss = 3.57823 avg_loss = 3.42860\n",
      "epoch no.3 train no.279690  loss = 4.02497 avg_loss = 3.46970\n",
      "epoch no.3 train no.279700  loss = 3.27119 avg_loss = 3.47195\n",
      "epoch no.3 train no.279710  loss = 3.81019 avg_loss = 3.48583\n",
      "epoch no.3 train no.279720  loss = 2.86879 avg_loss = 3.47605\n",
      "epoch no.3 train no.279730  loss = 3.16860 avg_loss = 3.49462\n",
      "epoch no.3 train no.279740  loss = 2.88230 avg_loss = 3.49605\n",
      "epoch no.3 train no.279750  loss = 4.31341 avg_loss = 3.50425\n",
      "epoch no.3 train no.279760  loss = 3.51503 avg_loss = 3.53742\n",
      "epoch no.3 train no.279770  loss = 3.85181 avg_loss = 3.58254\n",
      "epoch no.3 train no.279780  loss = 3.40213 avg_loss = 3.57491\n",
      "epoch no.3 train no.279790  loss = 3.73916 avg_loss = 3.56073\n",
      "epoch no.3 train no.279800  loss = 3.48468 avg_loss = 3.57036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.279810  loss = 2.48964 avg_loss = 3.52114\n",
      "epoch no.3 train no.279820  loss = 3.42364 avg_loss = 3.50497\n",
      "epoch no.3 train no.279830  loss = 3.70970 avg_loss = 3.52680\n",
      "epoch no.3 train no.279840  loss = 3.69489 avg_loss = 3.51399\n",
      "epoch no.3 train no.279850  loss = 2.17148 avg_loss = 3.48734\n",
      "epoch no.3 train no.279860  loss = 2.57485 avg_loss = 3.43890\n",
      "epoch no.3 train no.279870  loss = 2.20553 avg_loss = 3.41978\n",
      "epoch no.3 train no.279880  loss = 3.53646 avg_loss = 3.43613\n",
      "epoch no.3 train no.279890  loss = 3.39384 avg_loss = 3.37920\n",
      "epoch no.3 train no.279900  loss = 3.20447 avg_loss = 3.42233\n",
      "epoch no.3 train no.279910  loss = 3.89096 avg_loss = 3.44115\n",
      "epoch no.3 train no.279920  loss = 2.36583 avg_loss = 3.45661\n",
      "epoch no.3 train no.279930  loss = 2.54080 avg_loss = 3.44278\n",
      "epoch no.3 train no.279940  loss = 2.91873 avg_loss = 3.44826\n",
      "epoch no.3 train no.279950  loss = 3.82877 avg_loss = 3.42310\n",
      "epoch no.3 train no.279960  loss = 2.76395 avg_loss = 3.37572\n",
      "epoch no.3 train no.279970  loss = 3.17705 avg_loss = 3.35234\n",
      "epoch no.3 train no.279980  loss = 3.18899 avg_loss = 3.37078\n",
      "epoch no.3 train no.279990  loss = 3.06362 avg_loss = 3.37738\n",
      "epoch no.3 train no.280000  loss = 3.81170 avg_loss = 3.39547\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '▁드라이브', '</s>']\n",
      "기분전환 드라이브</s>\n",
      "epoch no.3 train no.280010  loss = 4.16234 avg_loss = 3.42990\n",
      "epoch no.3 train no.280020  loss = 2.59658 avg_loss = 3.44734\n",
      "epoch no.3 train no.280030  loss = 2.15639 avg_loss = 3.38658\n",
      "epoch no.3 train no.280040  loss = 3.76265 avg_loss = 3.39858\n",
      "epoch no.3 train no.280050  loss = 4.89487 avg_loss = 3.39558\n",
      "epoch no.3 train no.280060  loss = 3.42935 avg_loss = 3.37157\n",
      "epoch no.3 train no.280070  loss = 4.28612 avg_loss = 3.41211\n",
      "epoch no.3 train no.280080  loss = 4.14192 avg_loss = 3.46906\n",
      "epoch no.3 train no.280090  loss = 2.54758 avg_loss = 3.41289\n",
      "epoch no.3 train no.280100  loss = 2.37544 avg_loss = 3.39793\n",
      "epoch no.3 train no.280110  loss = 3.14309 avg_loss = 3.33931\n",
      "epoch no.3 train no.280120  loss = 3.44928 avg_loss = 3.30400\n",
      "epoch no.3 train no.280130  loss = 5.06520 avg_loss = 3.33201\n",
      "epoch no.3 train no.280140  loss = 4.77064 avg_loss = 3.37143\n",
      "epoch no.3 train no.280150  loss = 2.85911 avg_loss = 3.35629\n",
      "epoch no.3 train no.280160  loss = 4.06416 avg_loss = 3.35549\n",
      "epoch no.3 train no.280170  loss = 4.55484 avg_loss = 3.32592\n",
      "epoch no.3 train no.280180  loss = 4.22008 avg_loss = 3.33014\n",
      "epoch no.3 train no.280190  loss = 2.05739 avg_loss = 3.33158\n",
      "epoch no.3 train no.280200  loss = 4.35961 avg_loss = 3.37620\n",
      "epoch no.3 train no.280210  loss = 3.48743 avg_loss = 3.37371\n",
      "epoch no.3 train no.280220  loss = 3.66863 avg_loss = 3.38268\n",
      "epoch no.3 train no.280230  loss = 2.59421 avg_loss = 3.40029\n",
      "epoch no.3 train no.280240  loss = 3.69719 avg_loss = 3.39426\n",
      "epoch no.3 train no.280250  loss = 4.30971 avg_loss = 3.36653\n",
      "epoch no.3 train no.280260  loss = 2.50723 avg_loss = 3.33258\n",
      "epoch no.3 train no.280270  loss = 2.92465 avg_loss = 3.39035\n",
      "epoch no.3 train no.280280  loss = 2.75109 avg_loss = 3.37048\n",
      "epoch no.3 train no.280290  loss = 0.86255 avg_loss = 3.34268\n",
      "epoch no.3 train no.280300  loss = 2.51792 avg_loss = 3.37030\n",
      "epoch no.3 train no.280310  loss = 5.97424 avg_loss = 3.44323\n",
      "epoch no.3 train no.280320  loss = 2.64563 avg_loss = 3.42305\n",
      "epoch no.3 train no.280330  loss = 3.32658 avg_loss = 3.45385\n",
      "epoch no.3 train no.280340  loss = 2.51653 avg_loss = 3.46278\n",
      "epoch no.3 train no.280350  loss = 2.99537 avg_loss = 3.47051\n",
      "epoch no.3 train no.280360  loss = 2.62748 avg_loss = 3.42385\n",
      "epoch no.3 train no.280370  loss = 3.05387 avg_loss = 3.39174\n",
      "epoch no.3 train no.280380  loss = 3.32889 avg_loss = 3.35865\n",
      "epoch no.3 train no.280390  loss = 3.62194 avg_loss = 3.39408\n",
      "epoch no.3 train no.280400  loss = 2.21577 avg_loss = 3.34612\n",
      "epoch no.3 train no.280410  loss = 3.22668 avg_loss = 3.39224\n",
      "epoch no.3 train no.280420  loss = 3.34244 avg_loss = 3.39152\n",
      "epoch no.3 train no.280430  loss = 2.83613 avg_loss = 3.39766\n",
      "epoch no.3 train no.280440  loss = 3.22578 avg_loss = 3.42641\n",
      "epoch no.3 train no.280450  loss = 2.53555 avg_loss = 3.42338\n",
      "epoch no.3 train no.280460  loss = 2.70404 avg_loss = 3.43510\n",
      "epoch no.3 train no.280470  loss = 4.39613 avg_loss = 3.45766\n",
      "epoch no.3 train no.280480  loss = 4.67831 avg_loss = 3.51472\n",
      "epoch no.3 train no.280490  loss = 2.13070 avg_loss = 3.49898\n",
      "epoch no.3 train no.280500  loss = 4.22164 avg_loss = 3.46982\n",
      "epoch no.3 train no.280510  loss = 3.47335 avg_loss = 3.49723\n",
      "epoch no.3 train no.280520  loss = 2.79371 avg_loss = 3.45872\n",
      "epoch no.3 train no.280530  loss = 3.03269 avg_loss = 3.45121\n",
      "epoch no.3 train no.280540  loss = 3.16891 avg_loss = 3.44689\n",
      "epoch no.3 train no.280550  loss = 2.63801 avg_loss = 3.49674\n",
      "epoch no.3 train no.280560  loss = 3.64459 avg_loss = 3.44847\n",
      "epoch no.3 train no.280570  loss = 3.28811 avg_loss = 3.47483\n",
      "epoch no.3 train no.280580  loss = 2.92875 avg_loss = 3.42959\n",
      "epoch no.3 train no.280590  loss = 3.02301 avg_loss = 3.40418\n",
      "epoch no.3 train no.280600  loss = 3.13476 avg_loss = 3.44763\n",
      "epoch no.3 train no.280610  loss = 1.12348 avg_loss = 3.42450\n",
      "epoch no.3 train no.280620  loss = 3.15657 avg_loss = 3.40672\n",
      "epoch no.3 train no.280630  loss = 2.35711 avg_loss = 3.36738\n",
      "epoch no.3 train no.280640  loss = 2.55832 avg_loss = 3.39226\n",
      "epoch no.3 train no.280650  loss = 2.79633 avg_loss = 3.38504\n",
      "epoch no.3 train no.280660  loss = 3.65061 avg_loss = 3.39269\n",
      "epoch no.3 train no.280670  loss = 1.86720 avg_loss = 3.39080\n",
      "epoch no.3 train no.280680  loss = 4.34244 avg_loss = 3.44806\n",
      "epoch no.3 train no.280690  loss = 4.82985 avg_loss = 3.46083\n",
      "epoch no.3 train no.280700  loss = 5.24321 avg_loss = 3.55708\n",
      "epoch no.3 train no.280710  loss = 2.92157 avg_loss = 3.50575\n",
      "epoch no.3 train no.280720  loss = 1.74413 avg_loss = 3.51377\n",
      "epoch no.3 train no.280730  loss = 3.93474 avg_loss = 3.47300\n",
      "epoch no.3 train no.280740  loss = 2.79892 avg_loss = 3.49873\n",
      "epoch no.3 train no.280750  loss = 3.88296 avg_loss = 3.45199\n",
      "epoch no.3 train no.280760  loss = 4.66378 avg_loss = 3.49247\n",
      "epoch no.3 train no.280770  loss = 2.46773 avg_loss = 3.46998\n",
      "epoch no.3 train no.280780  loss = 4.48479 avg_loss = 3.46450\n",
      "epoch no.3 train no.280790  loss = 3.53757 avg_loss = 3.48901\n",
      "epoch no.3 train no.280800  loss = 5.96910 avg_loss = 3.53038\n",
      "epoch no.3 train no.280810  loss = 3.43778 avg_loss = 3.53052\n",
      "epoch no.3 train no.280820  loss = 2.35948 avg_loss = 3.55946\n",
      "epoch no.3 train no.280830  loss = 2.13944 avg_loss = 3.55920\n",
      "epoch no.3 train no.280840  loss = 3.38319 avg_loss = 3.58432\n",
      "epoch no.3 train no.280850  loss = 3.56743 avg_loss = 3.62375\n",
      "epoch no.3 train no.280860  loss = 3.12774 avg_loss = 3.61384\n",
      "epoch no.3 train no.280870  loss = 4.49547 avg_loss = 3.63853\n",
      "epoch no.3 train no.280880  loss = 3.61996 avg_loss = 3.60856\n",
      "epoch no.3 train no.280890  loss = 3.44753 avg_loss = 3.61878\n",
      "epoch no.3 train no.280900  loss = 2.51711 avg_loss = 3.60419\n",
      "epoch no.3 train no.280910  loss = 2.39182 avg_loss = 3.62241\n",
      "epoch no.3 train no.280920  loss = 3.99375 avg_loss = 3.61832\n",
      "epoch no.3 train no.280930  loss = 4.88537 avg_loss = 3.62564\n",
      "epoch no.3 train no.280940  loss = 4.82372 avg_loss = 3.62446\n",
      "epoch no.3 train no.280950  loss = 4.94854 avg_loss = 3.62271\n",
      "epoch no.3 train no.280960  loss = 4.34249 avg_loss = 3.57974\n",
      "epoch no.3 train no.280970  loss = 2.87444 avg_loss = 3.55586\n",
      "epoch no.3 train no.280980  loss = 1.63215 avg_loss = 3.49615\n",
      "epoch no.3 train no.280990  loss = 2.16782 avg_loss = 3.47283\n",
      "epoch no.3 train no.281000  loss = 3.03216 avg_loss = 3.46193\n",
      "1\n",
      "to_tokens: ['▁라디오', '▁좋은', '▁드라이브', '▁신나는']\n",
      "기분전환용</s>\n",
      "epoch no.3 train no.281010  loss = 3.62322 avg_loss = 3.46324\n",
      "epoch no.3 train no.281020  loss = 4.02474 avg_loss = 3.47460\n",
      "epoch no.3 train no.281030  loss = 2.61162 avg_loss = 3.49285\n",
      "epoch no.3 train no.281040  loss = 4.08045 avg_loss = 3.45775\n",
      "epoch no.3 train no.281050  loss = 4.88502 avg_loss = 3.48928\n",
      "epoch no.3 train no.281060  loss = 3.62861 avg_loss = 3.44774\n",
      "epoch no.3 train no.281070  loss = 2.80912 avg_loss = 3.48368\n",
      "epoch no.3 train no.281080  loss = 3.93108 avg_loss = 3.43398\n",
      "epoch no.3 train no.281090  loss = 2.66729 avg_loss = 3.41366\n",
      "epoch no.3 train no.281100  loss = 2.68305 avg_loss = 3.44690\n",
      "epoch no.3 train no.281110  loss = 3.64484 avg_loss = 3.49374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.281120  loss = 3.41285 avg_loss = 3.52460\n",
      "epoch no.3 train no.281130  loss = 4.61817 avg_loss = 3.52923\n",
      "epoch no.3 train no.281140  loss = 2.73843 avg_loss = 3.52879\n",
      "epoch no.3 train no.281150  loss = 2.92680 avg_loss = 3.50287\n",
      "epoch no.3 train no.281160  loss = 4.18856 avg_loss = 3.54861\n",
      "epoch no.3 train no.281170  loss = 1.73826 avg_loss = 3.51071\n",
      "epoch no.3 train no.281180  loss = 5.73598 avg_loss = 3.51348\n",
      "epoch no.3 train no.281190  loss = 1.93396 avg_loss = 3.46253\n",
      "epoch no.3 train no.281200  loss = 3.80867 avg_loss = 3.42243\n",
      "epoch no.3 train no.281210  loss = 3.20276 avg_loss = 3.44245\n",
      "epoch no.3 train no.281220  loss = 3.09636 avg_loss = 3.47374\n",
      "epoch no.3 train no.281230  loss = 3.65541 avg_loss = 3.46983\n",
      "epoch no.3 train no.281240  loss = 3.18035 avg_loss = 3.52018\n",
      "epoch no.3 train no.281250  loss = 3.90063 avg_loss = 3.50449\n",
      "epoch no.3 train no.281260  loss = 5.62341 avg_loss = 3.52061\n",
      "epoch no.3 train no.281270  loss = 3.59752 avg_loss = 3.52323\n",
      "epoch no.3 train no.281280  loss = 3.80176 avg_loss = 3.56418\n",
      "epoch no.3 train no.281290  loss = 3.14616 avg_loss = 3.58278\n",
      "epoch no.3 train no.281300  loss = 3.92287 avg_loss = 3.60218\n",
      "epoch no.3 train no.281310  loss = 2.57511 avg_loss = 3.59282\n",
      "epoch no.3 train no.281320  loss = 2.47221 avg_loss = 3.53571\n",
      "epoch no.3 train no.281330  loss = 4.94362 avg_loss = 3.55394\n",
      "epoch no.3 train no.281340  loss = 1.98831 avg_loss = 3.55868\n",
      "epoch no.3 train no.281350  loss = 2.04794 avg_loss = 3.57077\n",
      "epoch no.3 train no.281360  loss = 3.88388 avg_loss = 3.59203\n",
      "epoch no.3 train no.281370  loss = 3.01842 avg_loss = 3.58363\n",
      "epoch no.3 train no.281380  loss = 2.85813 avg_loss = 3.56625\n",
      "epoch no.3 train no.281390  loss = 4.61042 avg_loss = 3.59276\n",
      "epoch no.3 train no.281400  loss = 3.24526 avg_loss = 3.61689\n",
      "epoch no.3 train no.281410  loss = 3.72534 avg_loss = 3.60787\n",
      "epoch no.3 train no.281420  loss = 3.54630 avg_loss = 3.62681\n",
      "epoch no.3 train no.281430  loss = 2.45772 avg_loss = 3.58784\n",
      "epoch no.3 train no.281440  loss = 3.64536 avg_loss = 3.60994\n",
      "epoch no.3 train no.281450  loss = 3.01169 avg_loss = 3.63799\n",
      "epoch no.3 train no.281460  loss = 1.97279 avg_loss = 3.58564\n",
      "epoch no.3 train no.281470  loss = 2.60660 avg_loss = 3.56018\n",
      "epoch no.3 train no.281480  loss = 3.57635 avg_loss = 3.57930\n",
      "epoch no.3 train no.281490  loss = 2.26349 avg_loss = 3.55511\n",
      "epoch no.3 train no.281500  loss = 2.58005 avg_loss = 3.52948\n",
      "epoch no.3 train no.281510  loss = 4.15835 avg_loss = 3.48872\n",
      "epoch no.3 train no.281520  loss = 3.41989 avg_loss = 3.47024\n",
      "epoch no.3 train no.281530  loss = 3.82770 avg_loss = 3.48061\n",
      "epoch no.3 train no.281540  loss = 3.52241 avg_loss = 3.47952\n",
      "epoch no.3 train no.281550  loss = 3.59228 avg_loss = 3.47107\n",
      "epoch no.3 train no.281560  loss = 2.04014 avg_loss = 3.48311\n",
      "epoch no.3 train no.281570  loss = 3.08754 avg_loss = 3.54204\n",
      "epoch no.3 train no.281580  loss = 2.24084 avg_loss = 3.58614\n",
      "epoch no.3 train no.281590  loss = 4.21545 avg_loss = 3.55602\n",
      "epoch no.3 train no.281600  loss = 2.96055 avg_loss = 3.53465\n",
      "epoch no.3 train no.281610  loss = 3.76878 avg_loss = 3.50521\n",
      "epoch no.3 train no.281620  loss = 4.51797 avg_loss = 3.50890\n",
      "epoch no.3 train no.281630  loss = 3.49964 avg_loss = 3.52056\n",
      "epoch no.3 train no.281640  loss = 3.45743 avg_loss = 3.49523\n",
      "epoch no.3 train no.281650  loss = 4.52718 avg_loss = 3.52400\n",
      "epoch no.3 train no.281660  loss = 2.36287 avg_loss = 3.50627\n",
      "epoch no.3 train no.281670  loss = 2.13300 avg_loss = 3.49426\n",
      "epoch no.3 train no.281680  loss = 2.62950 avg_loss = 3.44307\n",
      "epoch no.3 train no.281690  loss = 4.21006 avg_loss = 3.43384\n",
      "epoch no.3 train no.281700  loss = 2.64996 avg_loss = 3.40011\n",
      "epoch no.3 train no.281710  loss = 4.66279 avg_loss = 3.47620\n",
      "epoch no.3 train no.281720  loss = 2.44404 avg_loss = 3.48089\n",
      "epoch no.3 train no.281730  loss = 3.51078 avg_loss = 3.45388\n",
      "epoch no.3 train no.281740  loss = 2.39740 avg_loss = 3.42595\n",
      "epoch no.3 train no.281750  loss = 2.88041 avg_loss = 3.46023\n",
      "epoch no.3 train no.281760  loss = 2.68045 avg_loss = 3.43648\n",
      "epoch no.3 train no.281770  loss = 4.61559 avg_loss = 3.46481\n",
      "epoch no.3 train no.281780  loss = 5.44109 avg_loss = 3.46078\n",
      "epoch no.3 train no.281790  loss = 2.43557 avg_loss = 3.47190\n",
      "epoch no.3 train no.281800  loss = 2.72989 avg_loss = 3.41935\n",
      "epoch no.3 train no.281810  loss = 4.48841 avg_loss = 3.44954\n",
      "epoch no.3 train no.281820  loss = 3.41130 avg_loss = 3.45772\n",
      "epoch no.3 train no.281830  loss = 4.26328 avg_loss = 3.45317\n",
      "epoch no.3 train no.281840  loss = 3.03413 avg_loss = 3.42211\n",
      "epoch no.3 train no.281850  loss = 2.60374 avg_loss = 3.43001\n",
      "epoch no.3 train no.281860  loss = 4.86736 avg_loss = 3.46874\n",
      "epoch no.3 train no.281870  loss = 2.88126 avg_loss = 3.41171\n",
      "epoch no.3 train no.281880  loss = 4.73074 avg_loss = 3.41206\n",
      "epoch no.3 train no.281890  loss = 4.28595 avg_loss = 3.40519\n",
      "epoch no.3 train no.281900  loss = 2.05178 avg_loss = 3.38880\n",
      "epoch no.3 train no.281910  loss = 4.92775 avg_loss = 3.41277\n",
      "epoch no.3 train no.281920  loss = 3.75067 avg_loss = 3.42160\n",
      "epoch no.3 train no.281930  loss = 2.86175 avg_loss = 3.45475\n",
      "epoch no.3 train no.281940  loss = 3.18948 avg_loss = 3.45018\n",
      "epoch no.3 train no.281950  loss = 3.41831 avg_loss = 3.46636\n",
      "epoch no.3 train no.281960  loss = 2.18346 avg_loss = 3.46832\n",
      "epoch no.3 train no.281970  loss = 2.38051 avg_loss = 3.46103\n",
      "epoch no.3 train no.281980  loss = 3.29562 avg_loss = 3.47225\n",
      "epoch no.3 train no.281990  loss = 6.42135 avg_loss = 3.50427\n",
      "epoch no.3 train no.282000  loss = 3.03949 avg_loss = 3.49153\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '이', '</s>']\n",
      "기분전환 드라이브</s>\n",
      "epoch no.3 train no.282010  loss = 4.93030 avg_loss = 3.50207\n",
      "epoch no.3 train no.282020  loss = 3.53796 avg_loss = 3.53755\n",
      "epoch no.3 train no.282030  loss = 4.49520 avg_loss = 3.53992\n",
      "epoch no.3 train no.282040  loss = 3.47923 avg_loss = 3.53153\n",
      "epoch no.3 train no.282050  loss = 4.09034 avg_loss = 3.50771\n",
      "epoch no.3 train no.282060  loss = 3.36301 avg_loss = 3.50383\n",
      "epoch no.3 train no.282070  loss = 3.92646 avg_loss = 3.50145\n",
      "epoch no.3 train no.282080  loss = 2.05483 avg_loss = 3.47228\n",
      "epoch no.3 train no.282090  loss = 4.18010 avg_loss = 3.49481\n",
      "epoch no.3 train no.282100  loss = 2.99208 avg_loss = 3.45155\n",
      "epoch no.3 train no.282110  loss = 1.92309 avg_loss = 3.45871\n",
      "epoch no.3 train no.282120  loss = 4.13984 avg_loss = 3.41279\n",
      "epoch no.3 train no.282130  loss = 2.61387 avg_loss = 3.43525\n",
      "epoch no.3 train no.282140  loss = 4.06541 avg_loss = 3.45621\n",
      "epoch no.3 train no.282150  loss = 2.87464 avg_loss = 3.47752\n",
      "epoch no.3 train no.282160  loss = 3.61690 avg_loss = 3.42403\n",
      "epoch no.3 train no.282170  loss = 2.71051 avg_loss = 3.39167\n",
      "epoch no.3 train no.282180  loss = 2.58724 avg_loss = 3.35685\n",
      "epoch no.3 train no.282190  loss = 0.79196 avg_loss = 3.37248\n",
      "epoch no.3 train no.282200  loss = 2.49955 avg_loss = 3.37723\n",
      "epoch no.3 train no.282210  loss = 4.03964 avg_loss = 3.40633\n",
      "epoch no.3 train no.282220  loss = 3.64361 avg_loss = 3.43086\n",
      "epoch no.3 train no.282230  loss = 3.44347 avg_loss = 3.39512\n",
      "epoch no.3 train no.282240  loss = 4.96728 avg_loss = 3.38134\n",
      "epoch no.3 train no.282250  loss = 4.63465 avg_loss = 3.36249\n",
      "epoch no.3 train no.282260  loss = 5.34994 avg_loss = 3.39714\n",
      "epoch no.3 train no.282270  loss = 4.03770 avg_loss = 3.38927\n",
      "epoch no.3 train no.282280  loss = 2.32081 avg_loss = 3.39077\n",
      "epoch no.3 train no.282290  loss = 3.53894 avg_loss = 3.42182\n",
      "epoch no.3 train no.282300  loss = 2.54713 avg_loss = 3.41841\n",
      "epoch no.3 train no.282310  loss = 2.33701 avg_loss = 3.41917\n",
      "epoch no.3 train no.282320  loss = 3.56369 avg_loss = 3.41502\n",
      "epoch no.3 train no.282330  loss = 4.82195 avg_loss = 3.41523\n",
      "epoch no.3 train no.282340  loss = 2.85952 avg_loss = 3.43110\n",
      "epoch no.3 train no.282350  loss = 6.74615 avg_loss = 3.42230\n",
      "epoch no.3 train no.282360  loss = 3.86596 avg_loss = 3.46035\n",
      "epoch no.3 train no.282370  loss = 4.05137 avg_loss = 3.46185\n",
      "epoch no.3 train no.282380  loss = 4.00406 avg_loss = 3.49148\n",
      "epoch no.3 train no.282390  loss = 4.80682 avg_loss = 3.45434\n",
      "epoch no.3 train no.282400  loss = 5.62153 avg_loss = 3.47124\n",
      "epoch no.3 train no.282410  loss = 5.15566 avg_loss = 3.48354\n",
      "epoch no.3 train no.282420  loss = 3.18160 avg_loss = 3.49892\n",
      "epoch no.3 train no.282430  loss = 2.76914 avg_loss = 3.49998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.282440  loss = 3.44019 avg_loss = 3.45995\n",
      "epoch no.3 train no.282450  loss = 3.75938 avg_loss = 3.48456\n",
      "epoch no.3 train no.282460  loss = 2.39154 avg_loss = 3.45645\n",
      "epoch no.3 train no.282470  loss = 2.89569 avg_loss = 3.47008\n",
      "epoch no.3 train no.282480  loss = 1.52361 avg_loss = 3.48321\n",
      "epoch no.3 train no.282490  loss = 3.02939 avg_loss = 3.48831\n",
      "epoch no.3 train no.282500  loss = 2.75351 avg_loss = 3.47321\n",
      "epoch no.3 train no.282510  loss = 5.89668 avg_loss = 3.42669\n",
      "epoch no.3 train no.282520  loss = 2.56805 avg_loss = 3.44309\n",
      "epoch no.3 train no.282530  loss = 3.56999 avg_loss = 3.42254\n",
      "epoch no.3 train no.282540  loss = 4.40584 avg_loss = 3.43194\n",
      "epoch no.3 train no.282550  loss = 2.15593 avg_loss = 3.42662\n",
      "epoch no.3 train no.282560  loss = 5.28545 avg_loss = 3.42996\n",
      "epoch no.3 train no.282570  loss = 4.80530 avg_loss = 3.49539\n",
      "epoch no.3 train no.282580  loss = 2.95595 avg_loss = 3.50531\n",
      "epoch no.3 train no.282590  loss = 2.30678 avg_loss = 3.51839\n",
      "epoch no.3 train no.282600  loss = 4.65435 avg_loss = 3.50824\n",
      "epoch no.3 train no.282610  loss = 4.90047 avg_loss = 3.49437\n",
      "epoch no.3 train no.282620  loss = 3.45721 avg_loss = 3.46550\n",
      "epoch no.3 train no.282630  loss = 3.95596 avg_loss = 3.45369\n",
      "epoch no.3 train no.282640  loss = 4.25463 avg_loss = 3.48401\n",
      "epoch no.3 train no.282650  loss = 5.18138 avg_loss = 3.50388\n",
      "epoch no.3 train no.282660  loss = 4.42130 avg_loss = 3.53183\n",
      "epoch no.3 train no.282670  loss = 3.68898 avg_loss = 3.59318\n",
      "epoch no.3 train no.282680  loss = 5.47679 avg_loss = 3.61900\n",
      "epoch no.3 train no.282690  loss = 2.69615 avg_loss = 3.59869\n",
      "epoch no.3 train no.282700  loss = 2.73021 avg_loss = 3.54493\n",
      "epoch no.3 train no.282710  loss = 3.01685 avg_loss = 3.54969\n",
      "epoch no.3 train no.282720  loss = 3.81789 avg_loss = 3.52646\n",
      "epoch no.3 train no.282730  loss = 3.97683 avg_loss = 3.56635\n",
      "epoch no.3 train no.282740  loss = 3.32258 avg_loss = 3.55281\n",
      "epoch no.3 train no.282750  loss = 3.48177 avg_loss = 3.55096\n",
      "epoch no.3 train no.282760  loss = 4.16033 avg_loss = 3.54758\n",
      "epoch no.3 train no.282770  loss = 2.09422 avg_loss = 3.49781\n",
      "epoch no.3 train no.282780  loss = 2.78637 avg_loss = 3.45863\n",
      "epoch no.3 train no.282790  loss = 3.32939 avg_loss = 3.44438\n",
      "epoch no.3 train no.282800  loss = 4.96544 avg_loss = 3.44015\n",
      "epoch no.3 train no.282810  loss = 3.72357 avg_loss = 3.48354\n",
      "epoch no.3 train no.282820  loss = 1.96189 avg_loss = 3.46881\n",
      "epoch no.3 train no.282830  loss = 4.77529 avg_loss = 3.49307\n",
      "epoch no.3 train no.282840  loss = 3.34843 avg_loss = 3.47398\n",
      "epoch no.3 train no.282850  loss = 4.46470 avg_loss = 3.50141\n",
      "epoch no.3 train no.282860  loss = 2.56603 avg_loss = 3.52387\n",
      "epoch no.3 train no.282870  loss = 3.49210 avg_loss = 3.51348\n",
      "epoch no.3 train no.282880  loss = 2.03997 avg_loss = 3.51430\n",
      "epoch no.3 train no.282890  loss = 4.31854 avg_loss = 3.49915\n",
      "epoch no.3 train no.282900  loss = 3.42527 avg_loss = 3.44467\n",
      "epoch no.3 train no.282910  loss = 2.25244 avg_loss = 3.41907\n",
      "epoch no.3 train no.282920  loss = 2.94345 avg_loss = 3.37984\n",
      "epoch no.3 train no.282930  loss = 3.15801 avg_loss = 3.41741\n",
      "epoch no.3 train no.282940  loss = 4.24077 avg_loss = 3.40821\n",
      "epoch no.3 train no.282950  loss = 4.28832 avg_loss = 3.43021\n",
      "epoch no.3 train no.282960  loss = 2.58403 avg_loss = 3.41086\n",
      "epoch no.3 train no.282970  loss = 3.68337 avg_loss = 3.38879\n",
      "epoch no.3 train no.282980  loss = 3.64975 avg_loss = 3.41938\n",
      "epoch no.3 train no.282990  loss = 2.79619 avg_loss = 3.41587\n",
      "epoch no.3 train no.283000  loss = 2.79284 avg_loss = 3.41750\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.283010  loss = 2.81058 avg_loss = 3.46422\n",
      "epoch no.3 train no.283020  loss = 2.21611 avg_loss = 3.48167\n",
      "epoch no.3 train no.283030  loss = 3.15284 avg_loss = 3.50803\n",
      "epoch no.3 train no.283040  loss = 3.31825 avg_loss = 3.47711\n",
      "epoch no.3 train no.283050  loss = 3.27254 avg_loss = 3.45034\n",
      "epoch no.3 train no.283060  loss = 2.48242 avg_loss = 3.45360\n",
      "epoch no.3 train no.283070  loss = 4.65319 avg_loss = 3.45906\n",
      "epoch no.3 train no.283080  loss = 3.80910 avg_loss = 3.40847\n",
      "epoch no.3 train no.283090  loss = 5.04644 avg_loss = 3.40207\n",
      "epoch no.3 train no.283100  loss = 3.67629 avg_loss = 3.42461\n",
      "epoch no.3 train no.283110  loss = 3.79551 avg_loss = 3.45083\n",
      "epoch no.3 train no.283120  loss = 4.33600 avg_loss = 3.47286\n",
      "epoch no.3 train no.283130  loss = 2.93644 avg_loss = 3.45622\n",
      "epoch no.3 train no.283140  loss = 4.61550 avg_loss = 3.43271\n",
      "epoch no.3 train no.283150  loss = 2.41402 avg_loss = 3.42916\n",
      "epoch no.3 train no.283160  loss = 4.28286 avg_loss = 3.42136\n",
      "epoch no.3 train no.283170  loss = 2.69346 avg_loss = 3.47303\n",
      "epoch no.3 train no.283180  loss = 3.65332 avg_loss = 3.49031\n",
      "epoch no.3 train no.283190  loss = 3.57364 avg_loss = 3.49527\n",
      "epoch no.3 train no.283200  loss = 4.95637 avg_loss = 3.46984\n",
      "epoch no.3 train no.283210  loss = 2.48806 avg_loss = 3.48994\n",
      "epoch no.3 train no.283220  loss = 2.51086 avg_loss = 3.50774\n",
      "epoch no.3 train no.283230  loss = 4.30944 avg_loss = 3.54710\n",
      "epoch no.3 train no.283240  loss = 3.70016 avg_loss = 3.53944\n",
      "epoch no.3 train no.283250  loss = 2.84789 avg_loss = 3.51646\n",
      "epoch no.3 train no.283260  loss = 3.10624 avg_loss = 3.48939\n",
      "epoch no.3 train no.283270  loss = 3.44088 avg_loss = 3.49074\n",
      "epoch no.3 train no.283280  loss = 3.54927 avg_loss = 3.50193\n",
      "epoch no.3 train no.283290  loss = 4.61414 avg_loss = 3.48631\n",
      "epoch no.3 train no.283300  loss = 2.91962 avg_loss = 3.46089\n",
      "epoch no.3 train no.283310  loss = 2.70154 avg_loss = 3.45547\n",
      "epoch no.3 train no.283320  loss = 3.02992 avg_loss = 3.46424\n",
      "epoch no.3 train no.283330  loss = 4.14446 avg_loss = 3.44238\n",
      "epoch no.3 train no.283340  loss = 6.95974 avg_loss = 3.44491\n",
      "epoch no.3 train no.283350  loss = 3.00382 avg_loss = 3.37969\n",
      "epoch no.3 train no.283360  loss = 3.31229 avg_loss = 3.39989\n",
      "epoch no.3 train no.283370  loss = 2.94316 avg_loss = 3.33015\n",
      "epoch no.3 train no.283380  loss = 3.19761 avg_loss = 3.33343\n",
      "epoch no.3 train no.283390  loss = 4.26144 avg_loss = 3.36764\n",
      "epoch no.3 train no.283400  loss = 3.40240 avg_loss = 3.38272\n",
      "epoch no.3 train no.283410  loss = 4.09953 avg_loss = 3.43435\n",
      "epoch no.3 train no.283420  loss = 3.51413 avg_loss = 3.39887\n",
      "epoch no.3 train no.283430  loss = 3.57209 avg_loss = 3.41863\n",
      "epoch no.3 train no.283440  loss = 2.44481 avg_loss = 3.43079\n",
      "epoch no.3 train no.283450  loss = 5.13433 avg_loss = 3.44157\n",
      "epoch no.3 train no.283460  loss = 3.47065 avg_loss = 3.42333\n",
      "epoch no.3 train no.283470  loss = 3.88664 avg_loss = 3.43312\n",
      "epoch no.3 train no.283480  loss = 3.94185 avg_loss = 3.41507\n",
      "epoch no.3 train no.283490  loss = 2.84356 avg_loss = 3.41143\n",
      "epoch no.3 train no.283500  loss = 1.99332 avg_loss = 3.39705\n",
      "epoch no.3 train no.283510  loss = 2.77208 avg_loss = 3.39048\n",
      "epoch no.3 train no.283520  loss = 4.00553 avg_loss = 3.41056\n",
      "epoch no.3 train no.283530  loss = 2.79694 avg_loss = 3.41832\n",
      "epoch no.3 train no.283540  loss = 3.04904 avg_loss = 3.39918\n",
      "epoch no.3 train no.283550  loss = 3.67747 avg_loss = 3.41508\n",
      "epoch no.3 train no.283560  loss = 2.93154 avg_loss = 3.41427\n",
      "epoch no.3 train no.283570  loss = 2.80926 avg_loss = 3.38560\n",
      "epoch no.3 train no.283580  loss = 3.30072 avg_loss = 3.43316\n",
      "epoch no.3 train no.283590  loss = 4.34258 avg_loss = 3.44950\n",
      "epoch no.3 train no.283600  loss = 3.97164 avg_loss = 3.46998\n",
      "epoch no.3 train no.283610  loss = 2.96563 avg_loss = 3.46233\n",
      "epoch no.3 train no.283620  loss = 5.19270 avg_loss = 3.45897\n",
      "epoch no.3 train no.283630  loss = 2.85902 avg_loss = 3.43211\n",
      "epoch no.3 train no.283640  loss = 2.53610 avg_loss = 3.40145\n",
      "epoch no.3 train no.283650  loss = 4.60857 avg_loss = 3.42071\n",
      "epoch no.3 train no.283660  loss = 3.73638 avg_loss = 3.37398\n",
      "epoch no.3 train no.283670  loss = 5.50202 avg_loss = 3.35584\n",
      "epoch no.3 train no.283680  loss = 4.29924 avg_loss = 3.37737\n",
      "epoch no.3 train no.283690  loss = 3.23700 avg_loss = 3.41769\n",
      "epoch no.3 train no.283700  loss = 1.87966 avg_loss = 3.46115\n",
      "epoch no.3 train no.283710  loss = 2.74497 avg_loss = 3.42690\n",
      "epoch no.3 train no.283720  loss = 3.79797 avg_loss = 3.41364\n",
      "epoch no.3 train no.283730  loss = 3.06019 avg_loss = 3.41126\n",
      "epoch no.3 train no.283740  loss = 3.42834 avg_loss = 3.39301\n",
      "epoch no.3 train no.283750  loss = 3.44421 avg_loss = 3.40454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.283760  loss = 2.03708 avg_loss = 3.33663\n",
      "epoch no.3 train no.283770  loss = 3.59369 avg_loss = 3.35055\n",
      "epoch no.3 train no.283780  loss = 3.64732 avg_loss = 3.41099\n",
      "epoch no.3 train no.283790  loss = 4.88748 avg_loss = 3.39760\n",
      "epoch no.3 train no.283800  loss = 3.15008 avg_loss = 3.38425\n",
      "epoch no.3 train no.283810  loss = 6.60301 avg_loss = 3.36894\n",
      "epoch no.3 train no.283820  loss = 5.46862 avg_loss = 3.46298\n",
      "epoch no.3 train no.283830  loss = 3.29011 avg_loss = 3.47488\n",
      "epoch no.3 train no.283840  loss = 4.25443 avg_loss = 3.52207\n",
      "epoch no.3 train no.283850  loss = 3.47298 avg_loss = 3.52346\n",
      "epoch no.3 train no.283860  loss = 3.49681 avg_loss = 3.52446\n",
      "epoch no.3 train no.283870  loss = 2.88674 avg_loss = 3.50942\n",
      "epoch no.3 train no.283880  loss = 4.57865 avg_loss = 3.48268\n",
      "epoch no.3 train no.283890  loss = 3.53323 avg_loss = 3.49294\n",
      "epoch no.3 train no.283900  loss = 3.51208 avg_loss = 3.44004\n",
      "epoch no.3 train no.283910  loss = 3.04117 avg_loss = 3.44277\n",
      "epoch no.3 train no.283920  loss = 5.88716 avg_loss = 3.51930\n",
      "epoch no.3 train no.283930  loss = 5.25895 avg_loss = 3.55938\n",
      "epoch no.3 train no.283940  loss = 2.89736 avg_loss = 3.57729\n",
      "epoch no.3 train no.283950  loss = 4.29897 avg_loss = 3.60439\n",
      "epoch no.3 train no.283960  loss = 3.93161 avg_loss = 3.63323\n",
      "epoch no.3 train no.283970  loss = 2.12400 avg_loss = 3.60194\n",
      "epoch no.3 train no.283980  loss = 2.57687 avg_loss = 3.63295\n",
      "epoch no.3 train no.283990  loss = 3.09750 avg_loss = 3.58006\n",
      "epoch no.3 train no.284000  loss = 3.07145 avg_loss = 3.54898\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '이', '싶', '을', '때', '</s>', '노래', '</s>']\n",
      "기분전환하고싶을때 듣는노래</s>\n",
      "epoch no.3 train no.284010  loss = 4.40055 avg_loss = 3.48837\n",
      "epoch no.3 train no.284020  loss = 1.47881 avg_loss = 3.56193\n",
      "epoch no.3 train no.284030  loss = 3.32993 avg_loss = 3.53555\n",
      "epoch no.3 train no.284040  loss = 3.80912 avg_loss = 3.52343\n",
      "epoch no.3 train no.284050  loss = 2.24891 avg_loss = 3.51579\n",
      "epoch no.3 train no.284060  loss = 2.52491 avg_loss = 3.50465\n",
      "epoch no.3 train no.284070  loss = 3.09587 avg_loss = 3.49605\n",
      "epoch no.3 train no.284080  loss = 2.98518 avg_loss = 3.50041\n",
      "epoch no.3 train no.284090  loss = 3.04795 avg_loss = 3.47542\n",
      "epoch no.3 train no.284100  loss = 2.33514 avg_loss = 3.45527\n",
      "epoch no.3 train no.284110  loss = 2.90071 avg_loss = 3.47959\n",
      "epoch no.3 train no.284120  loss = 2.94484 avg_loss = 3.49245\n",
      "epoch no.3 train no.284130  loss = 3.52176 avg_loss = 3.51583\n",
      "epoch no.3 train no.284140  loss = 3.14738 avg_loss = 3.48180\n",
      "epoch no.3 train no.284150  loss = 2.89036 avg_loss = 3.46393\n",
      "epoch no.3 train no.284160  loss = 1.92473 avg_loss = 3.48282\n",
      "epoch no.3 train no.284170  loss = 3.39809 avg_loss = 3.42909\n",
      "epoch no.3 train no.284180  loss = 2.90372 avg_loss = 3.39880\n",
      "epoch no.3 train no.284190  loss = 3.41672 avg_loss = 3.32872\n",
      "epoch no.3 train no.284200  loss = 3.87129 avg_loss = 3.36873\n",
      "epoch no.3 train no.284210  loss = 3.51984 avg_loss = 3.35058\n",
      "epoch no.3 train no.284220  loss = 2.97045 avg_loss = 3.33102\n",
      "epoch no.3 train no.284230  loss = 2.64071 avg_loss = 3.35669\n",
      "epoch no.3 train no.284240  loss = 2.28118 avg_loss = 3.39063\n",
      "epoch no.3 train no.284250  loss = 3.34632 avg_loss = 3.39150\n",
      "epoch no.3 train no.284260  loss = 3.94909 avg_loss = 3.37281\n",
      "epoch no.3 train no.284270  loss = 3.75787 avg_loss = 3.40948\n",
      "epoch no.3 train no.284280  loss = 3.87563 avg_loss = 3.42212\n",
      "epoch no.3 train no.284290  loss = 3.22194 avg_loss = 3.42934\n",
      "epoch no.3 train no.284300  loss = 3.17953 avg_loss = 3.42866\n",
      "epoch no.3 train no.284310  loss = 2.92341 avg_loss = 3.41272\n",
      "epoch no.3 train no.284320  loss = 3.88805 avg_loss = 3.44578\n",
      "epoch no.3 train no.284330  loss = 4.88145 avg_loss = 3.46782\n",
      "epoch no.3 train no.284340  loss = 1.81339 avg_loss = 3.40683\n",
      "epoch no.3 train no.284350  loss = 2.65211 avg_loss = 3.44121\n",
      "epoch no.3 train no.284360  loss = 5.18907 avg_loss = 3.45845\n",
      "epoch no.3 train no.284370  loss = 2.85835 avg_loss = 3.49862\n",
      "epoch no.3 train no.284380  loss = 4.48817 avg_loss = 3.52050\n",
      "epoch no.3 train no.284390  loss = 1.69836 avg_loss = 3.47928\n",
      "epoch no.3 train no.284400  loss = 3.37231 avg_loss = 3.46559\n",
      "epoch no.3 train no.284410  loss = 4.28668 avg_loss = 3.46407\n",
      "epoch no.3 train no.284420  loss = 3.23633 avg_loss = 3.47635\n",
      "epoch no.3 train no.284430  loss = 2.43641 avg_loss = 3.47476\n",
      "epoch no.3 train no.284440  loss = 5.21790 avg_loss = 3.46269\n",
      "epoch no.3 train no.284450  loss = 5.24041 avg_loss = 3.44246\n",
      "epoch no.3 train no.284460  loss = 6.83983 avg_loss = 3.49866\n",
      "epoch no.3 train no.284470  loss = 2.79489 avg_loss = 3.45433\n",
      "epoch no.3 train no.284480  loss = 4.28874 avg_loss = 3.44523\n",
      "epoch no.3 train no.284490  loss = 3.26810 avg_loss = 3.48511\n",
      "epoch no.3 train no.284500  loss = 5.23292 avg_loss = 3.48703\n",
      "epoch no.3 train no.284510  loss = 3.31911 avg_loss = 3.48708\n",
      "epoch no.3 train no.284520  loss = 4.17450 avg_loss = 3.47523\n",
      "epoch no.3 train no.284530  loss = 2.36081 avg_loss = 3.45370\n",
      "epoch no.3 train no.284540  loss = 5.13166 avg_loss = 3.50801\n",
      "epoch no.3 train no.284550  loss = 3.04102 avg_loss = 3.47008\n",
      "epoch no.3 train no.284560  loss = 4.83551 avg_loss = 3.47016\n",
      "epoch no.3 train no.284570  loss = 4.93221 avg_loss = 3.49148\n",
      "epoch no.3 train no.284580  loss = 5.05436 avg_loss = 3.49499\n",
      "epoch no.3 train no.284590  loss = 3.49036 avg_loss = 3.50580\n",
      "epoch no.3 train no.284600  loss = 3.62400 avg_loss = 3.51248\n",
      "epoch no.3 train no.284610  loss = 3.26330 avg_loss = 3.47923\n",
      "epoch no.3 train no.284620  loss = 3.18442 avg_loss = 3.44725\n",
      "epoch no.3 train no.284630  loss = 3.74048 avg_loss = 3.38825\n",
      "epoch no.3 train no.284640  loss = 4.22522 avg_loss = 3.43672\n",
      "epoch no.3 train no.284650  loss = 3.21911 avg_loss = 3.39758\n",
      "epoch no.3 train no.284660  loss = 4.41608 avg_loss = 3.38650\n",
      "epoch no.3 train no.284670  loss = 4.37784 avg_loss = 3.36473\n",
      "epoch no.3 train no.284680  loss = 4.21439 avg_loss = 3.46247\n",
      "epoch no.3 train no.284690  loss = 3.80982 avg_loss = 3.50628\n",
      "epoch no.3 train no.284700  loss = 3.72105 avg_loss = 3.46587\n",
      "epoch no.3 train no.284710  loss = 2.09494 avg_loss = 3.39779\n",
      "epoch no.3 train no.284720  loss = 3.70687 avg_loss = 3.39429\n",
      "epoch no.3 train no.284730  loss = 3.61848 avg_loss = 3.36425\n",
      "epoch no.3 train no.284740  loss = 2.43022 avg_loss = 3.31842\n",
      "epoch no.3 train no.284750  loss = 3.38169 avg_loss = 3.39052\n",
      "epoch no.3 train no.284760  loss = 4.98672 avg_loss = 3.40606\n",
      "epoch no.3 train no.284770  loss = 2.84608 avg_loss = 3.35684\n",
      "epoch no.3 train no.284780  loss = 2.34874 avg_loss = 3.36242\n",
      "epoch no.3 train no.284790  loss = 3.80553 avg_loss = 3.38278\n",
      "epoch no.3 train no.284800  loss = 2.70046 avg_loss = 3.36824\n",
      "epoch no.3 train no.284810  loss = 4.12273 avg_loss = 3.35809\n",
      "epoch no.3 train no.284820  loss = 2.62096 avg_loss = 3.30832\n",
      "epoch no.3 train no.284830  loss = 3.00183 avg_loss = 3.28712\n",
      "epoch no.3 train no.284840  loss = 4.52115 avg_loss = 3.33047\n",
      "epoch no.3 train no.284850  loss = 3.25891 avg_loss = 3.36432\n",
      "epoch no.3 train no.284860  loss = 2.47104 avg_loss = 3.36741\n",
      "epoch no.3 train no.284870  loss = 3.06376 avg_loss = 3.39129\n",
      "epoch no.3 train no.284880  loss = 3.31564 avg_loss = 3.42310\n",
      "epoch no.3 train no.284890  loss = 3.51035 avg_loss = 3.44379\n",
      "epoch no.3 train no.284900  loss = 3.46320 avg_loss = 3.44882\n",
      "epoch no.3 train no.284910  loss = 2.61826 avg_loss = 3.44158\n",
      "epoch no.3 train no.284920  loss = 3.28611 avg_loss = 3.44413\n",
      "epoch no.3 train no.284930  loss = 2.65394 avg_loss = 3.44600\n",
      "epoch no.3 train no.284940  loss = 2.83554 avg_loss = 3.41220\n",
      "epoch no.3 train no.284950  loss = 2.66629 avg_loss = 3.42575\n",
      "epoch no.3 train no.284960  loss = 3.00049 avg_loss = 3.42200\n",
      "epoch no.3 train no.284970  loss = 4.40469 avg_loss = 3.46433\n",
      "epoch no.3 train no.284980  loss = 3.79659 avg_loss = 3.50167\n",
      "epoch no.3 train no.284990  loss = 4.25393 avg_loss = 3.51502\n",
      "epoch no.3 train no.285000  loss = 3.00316 avg_loss = 3.54386\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '이', '</s>']\n",
      "기분전환 드라이브</s>\n",
      "epoch no.3 train no.285010  loss = 2.63082 avg_loss = 3.53100\n",
      "epoch no.3 train no.285020  loss = 3.31023 avg_loss = 3.50568\n",
      "epoch no.3 train no.285030  loss = 3.47092 avg_loss = 3.45227\n",
      "epoch no.3 train no.285040  loss = 3.42539 avg_loss = 3.44229\n",
      "epoch no.3 train no.285050  loss = 4.74191 avg_loss = 3.47211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.285060  loss = 2.34737 avg_loss = 3.45825\n",
      "epoch no.3 train no.285070  loss = 1.91441 avg_loss = 3.43508\n",
      "epoch no.3 train no.285080  loss = 3.84076 avg_loss = 3.42784\n",
      "epoch no.3 train no.285090  loss = 4.26208 avg_loss = 3.41612\n",
      "epoch no.3 train no.285100  loss = 3.60810 avg_loss = 3.42195\n",
      "epoch no.3 train no.285110  loss = 3.29284 avg_loss = 3.41521\n",
      "epoch no.3 train no.285120  loss = 3.81789 avg_loss = 3.39680\n",
      "epoch no.3 train no.285130  loss = 2.72923 avg_loss = 3.39619\n",
      "epoch no.3 train no.285140  loss = 3.39846 avg_loss = 3.40277\n",
      "epoch no.3 train no.285150  loss = 3.12947 avg_loss = 3.40658\n",
      "epoch no.3 train no.285160  loss = 3.26396 avg_loss = 3.43538\n",
      "epoch no.3 train no.285170  loss = 3.20367 avg_loss = 3.44100\n",
      "epoch no.3 train no.285180  loss = 2.58920 avg_loss = 3.45173\n",
      "epoch no.3 train no.285190  loss = 5.61443 avg_loss = 3.46019\n",
      "epoch no.3 train no.285200  loss = 4.31991 avg_loss = 3.44847\n",
      "epoch no.3 train no.285210  loss = 3.62364 avg_loss = 3.44010\n",
      "epoch no.3 train no.285220  loss = 4.05398 avg_loss = 3.48890\n",
      "epoch no.3 train no.285230  loss = 2.59468 avg_loss = 3.45844\n",
      "epoch no.3 train no.285240  loss = 2.74814 avg_loss = 3.45023\n",
      "epoch no.3 train no.285250  loss = 5.56926 avg_loss = 3.49363\n",
      "epoch no.3 train no.285260  loss = 4.81725 avg_loss = 3.52685\n",
      "epoch no.3 train no.285270  loss = 3.00723 avg_loss = 3.47584\n",
      "epoch no.3 train no.285280  loss = 5.53781 avg_loss = 3.52090\n",
      "epoch no.3 train no.285290  loss = 4.16020 avg_loss = 3.50536\n",
      "epoch no.3 train no.285300  loss = 2.11873 avg_loss = 3.51846\n",
      "epoch no.3 train no.285310  loss = 4.17015 avg_loss = 3.47829\n",
      "epoch no.3 train no.285320  loss = 2.88928 avg_loss = 3.45852\n",
      "epoch no.3 train no.285330  loss = 5.32790 avg_loss = 3.42468\n",
      "epoch no.3 train no.285340  loss = 3.79838 avg_loss = 3.41685\n",
      "epoch no.3 train no.285350  loss = 3.62885 avg_loss = 3.42658\n",
      "epoch no.3 train no.285360  loss = 5.43945 avg_loss = 3.49143\n",
      "epoch no.3 train no.285370  loss = 2.61072 avg_loss = 3.47316\n",
      "epoch no.3 train no.285380  loss = 2.79914 avg_loss = 3.47705\n",
      "epoch no.3 train no.285390  loss = 3.30754 avg_loss = 3.49736\n",
      "epoch no.3 train no.285400  loss = 3.90810 avg_loss = 3.45603\n",
      "epoch no.3 train no.285410  loss = 1.78970 avg_loss = 3.45942\n",
      "epoch no.3 train no.285420  loss = 2.82393 avg_loss = 3.44230\n",
      "epoch no.3 train no.285430  loss = 4.76576 avg_loss = 3.43816\n",
      "epoch no.3 train no.285440  loss = 3.36718 avg_loss = 3.44193\n",
      "epoch no.3 train no.285450  loss = 3.40418 avg_loss = 3.42400\n",
      "epoch no.3 train no.285460  loss = 3.60252 avg_loss = 3.44885\n",
      "epoch no.3 train no.285470  loss = 3.09696 avg_loss = 3.42116\n",
      "epoch no.3 train no.285480  loss = 3.77656 avg_loss = 3.42814\n",
      "epoch no.3 train no.285490  loss = 3.70991 avg_loss = 3.43532\n",
      "epoch no.3 train no.285500  loss = 3.61643 avg_loss = 3.46416\n",
      "epoch no.3 train no.285510  loss = 3.42169 avg_loss = 3.46328\n",
      "epoch no.3 train no.285520  loss = 4.44428 avg_loss = 3.42752\n",
      "epoch no.3 train no.285530  loss = 3.23235 avg_loss = 3.49915\n",
      "epoch no.3 train no.285540  loss = 3.96709 avg_loss = 3.49337\n",
      "epoch no.3 train no.285550  loss = 2.53370 avg_loss = 3.48969\n",
      "epoch no.3 train no.285560  loss = 3.32081 avg_loss = 3.49700\n",
      "epoch no.3 train no.285570  loss = 3.98944 avg_loss = 3.48166\n",
      "epoch no.3 train no.285580  loss = 3.56919 avg_loss = 3.45610\n",
      "epoch no.3 train no.285590  loss = 4.14575 avg_loss = 3.43507\n",
      "epoch no.3 train no.285600  loss = 2.03279 avg_loss = 3.44761\n",
      "epoch no.3 train no.285610  loss = 2.73786 avg_loss = 3.41323\n",
      "epoch no.3 train no.285620  loss = 3.14487 avg_loss = 3.42114\n",
      "epoch no.3 train no.285630  loss = 4.65958 avg_loss = 3.39777\n",
      "epoch no.3 train no.285640  loss = 3.44041 avg_loss = 3.37743\n",
      "epoch no.3 train no.285650  loss = 3.82215 avg_loss = 3.34345\n",
      "epoch no.3 train no.285660  loss = 2.94813 avg_loss = 3.39384\n",
      "epoch no.3 train no.285670  loss = 3.61560 avg_loss = 3.41045\n",
      "epoch no.3 train no.285680  loss = 3.02848 avg_loss = 3.38176\n",
      "epoch no.3 train no.285690  loss = 3.25318 avg_loss = 3.40124\n",
      "epoch no.3 train no.285700  loss = 2.00012 avg_loss = 3.39705\n",
      "epoch no.3 train no.285710  loss = 3.82404 avg_loss = 3.39600\n",
      "epoch no.3 train no.285720  loss = 2.44487 avg_loss = 3.38210\n",
      "epoch no.3 train no.285730  loss = 3.19916 avg_loss = 3.39001\n",
      "epoch no.3 train no.285740  loss = 3.20739 avg_loss = 3.38431\n",
      "epoch no.3 train no.285750  loss = 2.43610 avg_loss = 3.30259\n",
      "epoch no.3 train no.285760  loss = 2.27749 avg_loss = 3.34870\n",
      "epoch no.3 train no.285770  loss = 3.09007 avg_loss = 3.40824\n",
      "epoch no.3 train no.285780  loss = 3.40664 avg_loss = 3.36056\n",
      "epoch no.3 train no.285790  loss = 5.53762 avg_loss = 3.47219\n",
      "epoch no.3 train no.285800  loss = 3.63884 avg_loss = 3.44223\n",
      "epoch no.3 train no.285810  loss = 4.06040 avg_loss = 3.42298\n",
      "epoch no.3 train no.285820  loss = 2.76380 avg_loss = 3.43597\n",
      "epoch no.3 train no.285830  loss = 3.00310 avg_loss = 3.38865\n",
      "epoch no.3 train no.285840  loss = 4.62677 avg_loss = 3.45786\n",
      "epoch no.3 train no.285850  loss = 3.08089 avg_loss = 3.42650\n",
      "epoch no.3 train no.285860  loss = 3.64174 avg_loss = 3.42164\n",
      "epoch no.3 train no.285870  loss = 5.92996 avg_loss = 3.45235\n",
      "epoch no.3 train no.285880  loss = 3.61623 avg_loss = 3.42192\n",
      "epoch no.3 train no.285890  loss = 2.38142 avg_loss = 3.47186\n",
      "epoch no.3 train no.285900  loss = 3.37406 avg_loss = 3.49655\n",
      "epoch no.3 train no.285910  loss = 2.45912 avg_loss = 3.45556\n",
      "epoch no.3 train no.285920  loss = 3.12662 avg_loss = 3.44181\n",
      "epoch no.3 train no.285930  loss = 3.07077 avg_loss = 3.39335\n",
      "epoch no.3 train no.285940  loss = 5.32127 avg_loss = 3.44004\n",
      "epoch no.3 train no.285950  loss = 3.55739 avg_loss = 3.39173\n",
      "epoch no.3 train no.285960  loss = 3.10410 avg_loss = 3.38744\n",
      "epoch no.3 train no.285970  loss = 2.18545 avg_loss = 3.36950\n",
      "epoch no.3 train no.285980  loss = 5.00473 avg_loss = 3.38767\n",
      "epoch no.3 train no.285990  loss = 4.63600 avg_loss = 3.43254\n",
      "epoch no.3 train no.286000  loss = 2.97225 avg_loss = 3.39455\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '이', '싶', '을', '때', '</s>']\n",
      "기분전환하고싶을때</s>\n",
      "epoch no.3 train no.286010  loss = 3.05877 avg_loss = 3.39849\n",
      "epoch no.3 train no.286020  loss = 2.75652 avg_loss = 3.41486\n",
      "epoch no.3 train no.286030  loss = 3.34934 avg_loss = 3.38838\n",
      "epoch no.3 train no.286040  loss = 2.81305 avg_loss = 3.35128\n",
      "epoch no.3 train no.286050  loss = 3.31797 avg_loss = 3.30782\n",
      "epoch no.3 train no.286060  loss = 3.97765 avg_loss = 3.32109\n",
      "epoch no.3 train no.286070  loss = 4.13278 avg_loss = 3.29296\n",
      "epoch no.3 train no.286080  loss = 3.69932 avg_loss = 3.29514\n",
      "epoch no.3 train no.286090  loss = 2.74089 avg_loss = 3.36239\n",
      "epoch no.3 train no.286100  loss = 2.77028 avg_loss = 3.33876\n",
      "epoch no.3 train no.286110  loss = 2.55696 avg_loss = 3.39750\n",
      "epoch no.3 train no.286120  loss = 2.58671 avg_loss = 3.40643\n",
      "epoch no.3 train no.286130  loss = 4.01075 avg_loss = 3.42192\n",
      "epoch no.3 train no.286140  loss = 4.02853 avg_loss = 3.41028\n",
      "epoch no.3 train no.286150  loss = 1.65812 avg_loss = 3.40830\n",
      "epoch no.3 train no.286160  loss = 2.98151 avg_loss = 3.39640\n",
      "epoch no.3 train no.286170  loss = 3.46861 avg_loss = 3.41480\n",
      "epoch no.3 train no.286180  loss = 2.59930 avg_loss = 3.37629\n",
      "epoch no.3 train no.286190  loss = 4.16080 avg_loss = 3.39744\n",
      "epoch no.3 train no.286200  loss = 2.54809 avg_loss = 3.35658\n",
      "epoch no.3 train no.286210  loss = 2.22230 avg_loss = 3.33624\n",
      "epoch no.3 train no.286220  loss = 3.60394 avg_loss = 3.35522\n",
      "epoch no.3 train no.286230  loss = 3.74592 avg_loss = 3.39692\n",
      "epoch no.3 train no.286240  loss = 3.21598 avg_loss = 3.42257\n",
      "epoch no.3 train no.286250  loss = 4.22873 avg_loss = 3.45012\n",
      "epoch no.3 train no.286260  loss = 1.72190 avg_loss = 3.43043\n",
      "epoch no.3 train no.286270  loss = 2.82554 avg_loss = 3.40441\n",
      "epoch no.3 train no.286280  loss = 4.13519 avg_loss = 3.43627\n",
      "epoch no.3 train no.286290  loss = 2.87088 avg_loss = 3.39442\n",
      "epoch no.3 train no.286300  loss = 2.94282 avg_loss = 3.36301\n",
      "epoch no.3 train no.286310  loss = 3.18793 avg_loss = 3.35511\n",
      "epoch no.3 train no.286320  loss = 2.38479 avg_loss = 3.32139\n",
      "epoch no.3 train no.286330  loss = 1.64533 avg_loss = 3.33696\n",
      "epoch no.3 train no.286340  loss = 3.42733 avg_loss = 3.30220\n",
      "epoch no.3 train no.286350  loss = 5.24651 avg_loss = 3.34752\n",
      "epoch no.3 train no.286360  loss = 2.78279 avg_loss = 3.34164\n",
      "epoch no.3 train no.286370  loss = 2.31629 avg_loss = 3.34564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.286380  loss = 5.03385 avg_loss = 3.34445\n",
      "epoch no.3 train no.286390  loss = 3.43704 avg_loss = 3.37802\n",
      "epoch no.3 train no.286400  loss = 2.80067 avg_loss = 3.40882\n",
      "epoch no.3 train no.286410  loss = 2.60538 avg_loss = 3.40016\n",
      "epoch no.3 train no.286420  loss = 3.77961 avg_loss = 3.44695\n",
      "epoch no.3 train no.286430  loss = 3.76150 avg_loss = 3.46911\n",
      "epoch no.3 train no.286440  loss = 2.84334 avg_loss = 3.46873\n",
      "epoch no.3 train no.286450  loss = 4.77212 avg_loss = 3.49315\n",
      "epoch no.3 train no.286460  loss = 4.85908 avg_loss = 3.55773\n",
      "epoch no.3 train no.286470  loss = 2.81630 avg_loss = 3.57084\n",
      "epoch no.3 train no.286480  loss = 3.42627 avg_loss = 3.57086\n",
      "epoch no.3 train no.286490  loss = 2.53647 avg_loss = 3.55005\n",
      "epoch no.3 train no.286500  loss = 3.79146 avg_loss = 3.52456\n",
      "epoch no.3 train no.286510  loss = 3.87225 avg_loss = 3.54659\n",
      "epoch no.3 train no.286520  loss = 4.34558 avg_loss = 3.52944\n",
      "epoch no.3 train no.286530  loss = 2.34752 avg_loss = 3.53943\n",
      "epoch no.3 train no.286540  loss = 3.24531 avg_loss = 3.51705\n",
      "epoch no.3 train no.286550  loss = 4.76560 avg_loss = 3.53191\n",
      "epoch no.3 train no.286560  loss = 2.87426 avg_loss = 3.50429\n",
      "epoch no.3 train no.286570  loss = 2.30522 avg_loss = 3.48234\n",
      "epoch no.3 train no.286580  loss = 5.20988 avg_loss = 3.47483\n",
      "epoch no.3 train no.286590  loss = 4.09796 avg_loss = 3.46220\n",
      "epoch no.3 train no.286600  loss = 3.38183 avg_loss = 3.42103\n",
      "epoch no.3 train no.286610  loss = 1.90949 avg_loss = 3.42069\n",
      "epoch no.3 train no.286620  loss = 3.54465 avg_loss = 3.44443\n",
      "epoch no.3 train no.286630  loss = 2.76126 avg_loss = 3.41497\n",
      "epoch no.3 train no.286640  loss = 4.62268 avg_loss = 3.43630\n",
      "epoch no.3 train no.286650  loss = 3.15914 avg_loss = 3.46843\n",
      "epoch no.3 train no.286660  loss = 1.94976 avg_loss = 3.47629\n",
      "epoch no.3 train no.286670  loss = 2.15739 avg_loss = 3.44359\n",
      "epoch no.3 train no.286680  loss = 3.18140 avg_loss = 3.44629\n",
      "epoch no.3 train no.286690  loss = 3.24135 avg_loss = 3.46212\n",
      "epoch no.3 train no.286700  loss = 2.86564 avg_loss = 3.46100\n",
      "epoch no.3 train no.286710  loss = 4.57828 avg_loss = 3.45683\n",
      "epoch no.3 train no.286720  loss = 3.85126 avg_loss = 3.47719\n",
      "epoch no.3 train no.286730  loss = 5.26071 avg_loss = 3.44377\n",
      "epoch no.3 train no.286740  loss = 4.63852 avg_loss = 3.46113\n",
      "epoch no.3 train no.286750  loss = 2.66330 avg_loss = 3.44453\n",
      "epoch no.3 train no.286760  loss = 2.95255 avg_loss = 3.40324\n",
      "epoch no.3 train no.286770  loss = 3.44660 avg_loss = 3.43339\n",
      "epoch no.3 train no.286780  loss = 2.69476 avg_loss = 3.44267\n",
      "epoch no.3 train no.286790  loss = 2.71714 avg_loss = 3.45023\n",
      "epoch no.3 train no.286800  loss = 4.35982 avg_loss = 3.52143\n",
      "epoch no.3 train no.286810  loss = 4.32071 avg_loss = 3.53254\n",
      "epoch no.3 train no.286820  loss = 2.89563 avg_loss = 3.54240\n",
      "epoch no.3 train no.286830  loss = 3.70305 avg_loss = 3.54871\n",
      "epoch no.3 train no.286840  loss = 3.50833 avg_loss = 3.55756\n",
      "epoch no.3 train no.286850  loss = 3.08016 avg_loss = 3.58535\n",
      "epoch no.3 train no.286860  loss = 5.19710 avg_loss = 3.59643\n",
      "epoch no.3 train no.286870  loss = 3.25405 avg_loss = 3.53773\n",
      "epoch no.3 train no.286880  loss = 4.92741 avg_loss = 3.53091\n",
      "epoch no.3 train no.286890  loss = 3.94289 avg_loss = 3.50394\n",
      "epoch no.3 train no.286900  loss = 2.41434 avg_loss = 3.47013\n",
      "epoch no.3 train no.286910  loss = 3.97672 avg_loss = 3.54494\n",
      "epoch no.3 train no.286920  loss = 4.67529 avg_loss = 3.54781\n",
      "epoch no.3 train no.286930  loss = 5.46906 avg_loss = 3.59078\n",
      "epoch no.3 train no.286940  loss = 2.67212 avg_loss = 3.56669\n",
      "epoch no.3 train no.286950  loss = 2.35852 avg_loss = 3.56549\n",
      "epoch no.3 train no.286960  loss = 2.61723 avg_loss = 3.52467\n",
      "epoch no.3 train no.286970  loss = 2.67855 avg_loss = 3.49860\n",
      "epoch no.3 train no.286980  loss = 3.12324 avg_loss = 3.44874\n",
      "epoch no.3 train no.286990  loss = 3.15644 avg_loss = 3.41298\n",
      "epoch no.3 train no.287000  loss = 4.04388 avg_loss = 3.40829\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.287010  loss = 2.54727 avg_loss = 3.43487\n",
      "epoch no.3 train no.287020  loss = 2.62452 avg_loss = 3.42973\n",
      "epoch no.3 train no.287030  loss = 5.41850 avg_loss = 3.46592\n",
      "epoch no.3 train no.287040  loss = 3.45034 avg_loss = 3.45512\n",
      "epoch no.3 train no.287050  loss = 2.25834 avg_loss = 3.44651\n",
      "epoch no.3 train no.287060  loss = 3.11474 avg_loss = 3.45693\n",
      "epoch no.3 train no.287070  loss = 4.79719 avg_loss = 3.46452\n",
      "epoch no.3 train no.287080  loss = 3.74022 avg_loss = 3.41877\n",
      "epoch no.3 train no.287090  loss = 3.81223 avg_loss = 3.44074\n",
      "epoch no.3 train no.287100  loss = 3.70548 avg_loss = 3.43631\n",
      "epoch no.3 train no.287110  loss = 1.96263 avg_loss = 3.39273\n",
      "epoch no.3 train no.287120  loss = 2.69421 avg_loss = 3.34344\n",
      "epoch no.3 train no.287130  loss = 4.27499 avg_loss = 3.34142\n",
      "epoch no.3 train no.287140  loss = 2.95076 avg_loss = 3.33462\n",
      "epoch no.3 train no.287150  loss = 3.05184 avg_loss = 3.33010\n",
      "epoch no.3 train no.287160  loss = 2.79539 avg_loss = 3.33421\n",
      "epoch no.3 train no.287170  loss = 3.97569 avg_loss = 3.36713\n",
      "epoch no.3 train no.287180  loss = 5.81464 avg_loss = 3.36737\n",
      "epoch no.3 train no.287190  loss = 4.45357 avg_loss = 3.36669\n",
      "epoch no.3 train no.287200  loss = 4.73663 avg_loss = 3.38889\n",
      "epoch no.3 train no.287210  loss = 2.39229 avg_loss = 3.37786\n",
      "epoch no.3 train no.287220  loss = 2.53696 avg_loss = 3.32497\n",
      "epoch no.3 train no.287230  loss = 2.73779 avg_loss = 3.30261\n",
      "epoch no.3 train no.287240  loss = 2.28675 avg_loss = 3.31229\n",
      "epoch no.3 train no.287250  loss = 4.15311 avg_loss = 3.31526\n",
      "epoch no.3 train no.287260  loss = 2.86680 avg_loss = 3.31858\n",
      "epoch no.3 train no.287270  loss = 5.09029 avg_loss = 3.35759\n",
      "epoch no.3 train no.287280  loss = 2.21442 avg_loss = 3.38903\n",
      "epoch no.3 train no.287290  loss = 2.82540 avg_loss = 3.39400\n",
      "epoch no.3 train no.287300  loss = 2.34742 avg_loss = 3.36999\n",
      "epoch no.3 train no.287310  loss = 2.17945 avg_loss = 3.34506\n",
      "epoch no.3 train no.287320  loss = 4.05045 avg_loss = 3.37006\n",
      "epoch no.3 train no.287330  loss = 2.57402 avg_loss = 3.37076\n",
      "epoch no.3 train no.287340  loss = 3.07313 avg_loss = 3.39321\n",
      "epoch no.3 train no.287350  loss = 2.98493 avg_loss = 3.38946\n",
      "epoch no.3 train no.287360  loss = 3.66119 avg_loss = 3.39624\n",
      "epoch no.3 train no.287370  loss = 2.68312 avg_loss = 3.37466\n",
      "epoch no.3 train no.287380  loss = 3.97477 avg_loss = 3.37740\n",
      "epoch no.3 train no.287390  loss = 2.77874 avg_loss = 3.41702\n",
      "epoch no.3 train no.287400  loss = 3.71467 avg_loss = 3.40534\n",
      "epoch no.3 train no.287410  loss = 4.31992 avg_loss = 3.41829\n",
      "epoch no.3 train no.287420  loss = 2.14002 avg_loss = 3.36528\n",
      "epoch no.3 train no.287430  loss = 3.49755 avg_loss = 3.34188\n",
      "epoch no.3 train no.287440  loss = 2.78459 avg_loss = 3.34345\n",
      "epoch no.3 train no.287450  loss = 4.50469 avg_loss = 3.36684\n",
      "epoch no.3 train no.287460  loss = 3.48610 avg_loss = 3.37683\n",
      "epoch no.3 train no.287470  loss = 2.91444 avg_loss = 3.33971\n",
      "epoch no.3 train no.287480  loss = 4.19361 avg_loss = 3.34495\n",
      "epoch no.3 train no.287490  loss = 2.97567 avg_loss = 3.36652\n",
      "epoch no.3 train no.287500  loss = 3.46924 avg_loss = 3.37911\n",
      "epoch no.3 train no.287510  loss = 4.42211 avg_loss = 3.37096\n",
      "epoch no.3 train no.287520  loss = 4.67830 avg_loss = 3.38392\n",
      "epoch no.3 train no.287530  loss = 3.63556 avg_loss = 3.44490\n",
      "epoch no.3 train no.287540  loss = 2.71174 avg_loss = 3.43449\n",
      "epoch no.3 train no.287550  loss = 2.57321 avg_loss = 3.46294\n",
      "epoch no.3 train no.287560  loss = 3.44526 avg_loss = 3.49635\n",
      "epoch no.3 train no.287570  loss = 3.18602 avg_loss = 3.48256\n",
      "epoch no.3 train no.287580  loss = 2.35093 avg_loss = 3.48292\n",
      "epoch no.3 train no.287590  loss = 3.20710 avg_loss = 3.51803\n",
      "epoch no.3 train no.287600  loss = 2.49279 avg_loss = 3.53076\n",
      "epoch no.3 train no.287610  loss = 5.98336 avg_loss = 3.57302\n",
      "epoch no.3 train no.287620  loss = 3.62914 avg_loss = 3.52885\n",
      "epoch no.3 train no.287630  loss = 5.45874 avg_loss = 3.54846\n",
      "epoch no.3 train no.287640  loss = 2.80732 avg_loss = 3.62236\n",
      "epoch no.3 train no.287650  loss = 4.45474 avg_loss = 3.62383\n",
      "epoch no.3 train no.287660  loss = 1.90433 avg_loss = 3.58458\n",
      "epoch no.3 train no.287670  loss = 1.78682 avg_loss = 3.53939\n",
      "epoch no.3 train no.287680  loss = 4.58092 avg_loss = 3.52107\n",
      "epoch no.3 train no.287690  loss = 3.30389 avg_loss = 3.49481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.287700  loss = 2.13674 avg_loss = 3.43701\n",
      "epoch no.3 train no.287710  loss = 3.46828 avg_loss = 3.41618\n",
      "epoch no.3 train no.287720  loss = 2.31574 avg_loss = 3.45721\n",
      "epoch no.3 train no.287730  loss = 3.32099 avg_loss = 3.47945\n",
      "epoch no.3 train no.287740  loss = 4.34719 avg_loss = 3.48129\n",
      "epoch no.3 train no.287750  loss = 3.12170 avg_loss = 3.47425\n",
      "epoch no.3 train no.287760  loss = 5.89923 avg_loss = 3.50724\n",
      "epoch no.3 train no.287770  loss = 5.17643 avg_loss = 3.53163\n",
      "epoch no.3 train no.287780  loss = 4.45410 avg_loss = 3.52071\n",
      "epoch no.3 train no.287790  loss = 3.80242 avg_loss = 3.51253\n",
      "epoch no.3 train no.287800  loss = 3.80709 avg_loss = 3.51681\n",
      "epoch no.3 train no.287810  loss = 2.26848 avg_loss = 3.50723\n",
      "epoch no.3 train no.287820  loss = 3.44509 avg_loss = 3.52459\n",
      "epoch no.3 train no.287830  loss = 3.31797 avg_loss = 3.52293\n",
      "epoch no.3 train no.287840  loss = 3.38289 avg_loss = 3.51797\n",
      "epoch no.3 train no.287850  loss = 2.72585 avg_loss = 3.49572\n",
      "epoch no.3 train no.287860  loss = 5.49813 avg_loss = 3.51446\n",
      "epoch no.3 train no.287870  loss = 3.09834 avg_loss = 3.53075\n",
      "epoch no.3 train no.287880  loss = 4.02059 avg_loss = 3.52486\n",
      "epoch no.3 train no.287890  loss = 2.11011 avg_loss = 3.48396\n",
      "epoch no.3 train no.287900  loss = 2.95892 avg_loss = 3.48467\n",
      "epoch no.3 train no.287910  loss = 3.53739 avg_loss = 3.52085\n",
      "epoch no.3 train no.287920  loss = 3.82107 avg_loss = 3.50279\n",
      "epoch no.3 train no.287930  loss = 5.25797 avg_loss = 3.51040\n",
      "epoch no.3 train no.287940  loss = 1.95498 avg_loss = 3.51510\n",
      "epoch no.3 train no.287950  loss = 3.85772 avg_loss = 3.48527\n",
      "epoch no.3 train no.287960  loss = 2.96425 avg_loss = 3.45377\n",
      "epoch no.3 train no.287970  loss = 2.03846 avg_loss = 3.41671\n",
      "epoch no.3 train no.287980  loss = 4.96115 avg_loss = 3.46370\n",
      "epoch no.3 train no.287990  loss = 2.47408 avg_loss = 3.48616\n",
      "epoch no.3 train no.288000  loss = 2.81822 avg_loss = 3.45923\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.3 train no.288010  loss = 3.24430 avg_loss = 3.46383\n",
      "epoch no.3 train no.288020  loss = 1.63610 avg_loss = 3.45899\n",
      "epoch no.3 train no.288030  loss = 2.33975 avg_loss = 3.47648\n",
      "epoch no.3 train no.288040  loss = 2.07543 avg_loss = 3.48803\n",
      "epoch no.3 train no.288050  loss = 4.87927 avg_loss = 3.54455\n",
      "epoch no.3 train no.288060  loss = 3.38721 avg_loss = 3.58108\n",
      "epoch no.3 train no.288070  loss = 4.04561 avg_loss = 3.59794\n",
      "epoch no.3 train no.288080  loss = 3.51794 avg_loss = 3.58109\n",
      "epoch no.3 train no.288090  loss = 3.99864 avg_loss = 3.62821\n",
      "epoch no.3 train no.288100  loss = 4.60900 avg_loss = 3.59760\n",
      "epoch no.3 train no.288110  loss = 2.36003 avg_loss = 3.61844\n",
      "epoch no.3 train no.288120  loss = 2.60833 avg_loss = 3.61554\n",
      "epoch no.3 train no.288130  loss = 3.79243 avg_loss = 3.57643\n",
      "epoch no.3 train no.288140  loss = 3.44938 avg_loss = 3.56144\n",
      "epoch no.3 train no.288150  loss = 2.63595 avg_loss = 3.50058\n",
      "epoch no.3 train no.288160  loss = 2.35950 avg_loss = 3.50991\n",
      "epoch no.3 train no.288170  loss = 5.22552 avg_loss = 3.55060\n",
      "epoch no.3 train no.288180  loss = 3.17578 avg_loss = 3.47376\n",
      "epoch no.3 train no.288190  loss = 4.08901 avg_loss = 3.47740\n",
      "epoch no.3 train no.288200  loss = 4.36267 avg_loss = 3.54490\n",
      "epoch no.3 train no.288210  loss = 4.81994 avg_loss = 3.58281\n",
      "epoch no.3 train no.288220  loss = 3.76384 avg_loss = 3.52643\n",
      "epoch no.3 train no.288230  loss = 2.99632 avg_loss = 3.51332\n",
      "epoch no.3 train no.288240  loss = 3.38961 avg_loss = 3.52171\n",
      "epoch no.3 train no.288250  loss = 2.56942 avg_loss = 3.51205\n",
      "epoch no.3 train no.288260  loss = 2.72797 avg_loss = 3.45385\n",
      "epoch no.3 train no.288270  loss = 3.08395 avg_loss = 3.49728\n",
      "epoch no.3 train no.288280  loss = 4.65051 avg_loss = 3.50980\n",
      "epoch no.3 train no.288290  loss = 2.92094 avg_loss = 3.52160\n",
      "epoch no.3 train no.288300  loss = 5.04955 avg_loss = 3.55411\n",
      "epoch no.3 train no.288310  loss = 4.19106 avg_loss = 3.56741\n",
      "epoch no.3 train no.288320  loss = 3.25315 avg_loss = 3.59072\n",
      "epoch no.3 train no.288330  loss = 2.55713 avg_loss = 3.57513\n",
      "epoch no.3 train no.288340  loss = 2.30898 avg_loss = 3.52275\n",
      "epoch no.3 train no.288350  loss = 3.42054 avg_loss = 3.54454\n",
      "epoch no.3 train no.288360  loss = 4.48034 avg_loss = 3.52984\n",
      "epoch no.3 train no.288370  loss = 3.94789 avg_loss = 3.49039\n",
      "epoch no.3 train no.288380  loss = 1.80606 avg_loss = 3.48855\n",
      "epoch no.3 train no.288390  loss = 3.20829 avg_loss = 3.49695\n",
      "epoch no.3 train no.288400  loss = 2.76118 avg_loss = 3.46154\n",
      "epoch no.3 train no.288410  loss = 5.25812 avg_loss = 3.47297\n",
      "epoch no.3 train no.288420  loss = 3.79617 avg_loss = 3.43999\n",
      "epoch no.3 train no.288430  loss = 2.74628 avg_loss = 3.41765\n",
      "epoch no.3 train no.288440  loss = 2.47694 avg_loss = 3.41528\n",
      "epoch no.3 train no.288450  loss = 5.57664 avg_loss = 3.45362\n",
      "epoch no.3 train no.288460  loss = 4.73596 avg_loss = 3.46883\n",
      "epoch no.3 train no.288470  loss = 4.98752 avg_loss = 3.44929\n",
      "epoch no.3 train no.288480  loss = 2.49320 avg_loss = 3.43863\n",
      "epoch no.3 train no.288490  loss = 4.89646 avg_loss = 3.48382\n",
      "epoch no.3 train no.288500  loss = 4.81145 avg_loss = 3.51989\n",
      "epoch no.3 train no.288510  loss = 4.25832 avg_loss = 3.54336\n",
      "epoch no.3 train no.288520  loss = 3.52783 avg_loss = 3.51561\n",
      "epoch no.3 train no.288530  loss = 2.05895 avg_loss = 3.52148\n",
      "epoch no.3 train no.288540  loss = 2.67547 avg_loss = 3.46748\n",
      "epoch no.3 train no.288550  loss = 5.03928 avg_loss = 3.46960\n",
      "epoch no.3 train no.288560  loss = 3.26627 avg_loss = 3.44766\n",
      "epoch no.3 train no.288570  loss = 5.36831 avg_loss = 3.44497\n",
      "epoch no.3 train no.288580  loss = 2.62784 avg_loss = 3.40753\n",
      "epoch no.3 train no.288590  loss = 2.91077 avg_loss = 3.41118\n",
      "epoch no.3 train no.288600  loss = 2.87489 avg_loss = 3.38409\n",
      "epoch no.3 train no.288610  loss = 5.84204 avg_loss = 3.41110\n",
      "epoch no.3 train no.288620  loss = 3.75837 avg_loss = 3.42855\n",
      "epoch no.3 train no.288630  loss = 3.93557 avg_loss = 3.44641\n",
      "epoch no.3 train no.288640  loss = 4.83289 avg_loss = 3.47558\n",
      "epoch no.3 train no.288650  loss = 3.46665 avg_loss = 3.46021\n",
      "epoch no.3 train no.288660  loss = 2.95485 avg_loss = 3.44231\n",
      "epoch no.3 train no.288670  loss = 2.96022 avg_loss = 3.42205\n",
      "epoch no.3 train no.288680  loss = 2.71339 avg_loss = 3.40896\n",
      "epoch no.3 train no.288690  loss = 4.41597 avg_loss = 3.41970\n",
      "epoch no.3 train no.288700  loss = 4.28474 avg_loss = 3.39191\n",
      "epoch no.3 train no.288710  loss = 5.22676 avg_loss = 3.38865\n",
      "epoch no.3 train no.288720  loss = 3.64728 avg_loss = 3.36194\n",
      "epoch no.3 train no.288730  loss = 3.23365 avg_loss = 3.37627\n",
      "epoch no.3 train no.288740  loss = 2.77238 avg_loss = 3.35535\n",
      "epoch no.3 train no.288750  loss = 3.57615 avg_loss = 3.37628\n",
      "epoch no.3 train no.288760  loss = 3.91898 avg_loss = 3.39255\n",
      "epoch no.3 train no.288770  loss = 3.30153 avg_loss = 3.38542\n",
      "epoch no.3 train no.288780  loss = 3.84900 avg_loss = 3.37283\n",
      "epoch no.3 train no.288790  loss = 2.45501 avg_loss = 3.33836\n",
      "epoch no.3 train no.288800  loss = 2.84690 avg_loss = 3.40422\n",
      "epoch no.3 train no.288810  loss = 2.26402 avg_loss = 3.34657\n",
      "epoch no.3 train no.288820  loss = 2.50580 avg_loss = 3.30059\n",
      "epoch no.3 train no.288830  loss = 3.04457 avg_loss = 3.36705\n",
      "epoch no.3 train no.288840  loss = 2.88510 avg_loss = 3.40861\n",
      "epoch no.3 train no.288850  loss = 2.11808 avg_loss = 3.40662\n",
      "epoch no.3 train no.288860  loss = 2.86041 avg_loss = 3.39801\n",
      "epoch no.3 train no.288870  loss = 1.24724 avg_loss = 3.37493\n",
      "epoch no.3 train no.288880  loss = 3.83254 avg_loss = 3.41893\n",
      "epoch no.3 train no.288890  loss = 5.46144 avg_loss = 3.45375\n",
      "epoch no.3 train no.288900  loss = 2.89410 avg_loss = 3.46567\n",
      "epoch no.3 train no.288910  loss = 5.30494 avg_loss = 3.53454\n",
      "epoch no.3 train no.288920  loss = 3.33937 avg_loss = 3.52997\n",
      "epoch no.3 train no.288930  loss = 2.52549 avg_loss = 3.47351\n",
      "epoch no.3 train no.288940  loss = 5.07593 avg_loss = 3.52885\n",
      "epoch no.3 train no.288950  loss = 2.92026 avg_loss = 3.49820\n",
      "epoch no.3 train no.288960  loss = 5.68208 avg_loss = 3.50853\n",
      "epoch no.3 train no.288970  loss = 2.57857 avg_loss = 3.47758\n",
      "epoch no.3 train no.288980  loss = 3.89498 avg_loss = 3.53745\n",
      "epoch no.3 train no.288990  loss = 3.00053 avg_loss = 3.49675\n",
      "epoch no.3 train no.289000  loss = 3.33667 avg_loss = 3.49804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.3 train no.289010  loss = 2.17014 avg_loss = 3.47834\n",
      "epoch no.3 train no.289020  loss = 2.23710 avg_loss = 3.50229\n",
      "epoch no.3 train no.289030  loss = 4.22598 avg_loss = 3.48494\n",
      "epoch no.3 train no.289040  loss = 1.96784 avg_loss = 3.43903\n",
      "epoch no.3 train no.289050  loss = 4.15440 avg_loss = 3.42196\n",
      "epoch no.3 train no.289060  loss = 4.09562 avg_loss = 3.41059\n",
      "epoch no.3 train no.289070  loss = 4.00131 avg_loss = 3.44692\n",
      "epoch no.3 train no.289080  loss = 2.65179 avg_loss = 3.48860\n",
      "epoch no.3 train no.289090  loss = 5.09455 avg_loss = 3.51178\n",
      "epoch no.3 train no.289100  loss = 3.42234 avg_loss = 3.54255\n",
      "epoch no.3 train no.289110  loss = 2.16351 avg_loss = 3.51274\n",
      "epoch no.3 train no.289120  loss = 4.06452 avg_loss = 3.47428\n",
      "epoch no.3 train no.289130  loss = 5.37127 avg_loss = 3.46817\n",
      "epoch no.3 train no.289140  loss = 3.55781 avg_loss = 3.46512\n",
      "epoch no.3 train no.289150  loss = 2.58594 avg_loss = 3.43619\n",
      "epoch no.3 train no.289160  loss = 2.18598 avg_loss = 3.45021\n",
      "epoch no.3 train no.289170  loss = 4.87032 avg_loss = 3.50866\n",
      "epoch no.3 train no.289180  loss = 3.89918 avg_loss = 3.50531\n",
      "epoch no.3 train no.289190  loss = 2.92065 avg_loss = 3.48252\n",
      "epoch no.3 train no.289200  loss = 3.39590 avg_loss = 3.48732\n",
      "epoch no.3 train no.289210  loss = 3.84772 avg_loss = 3.51854\n",
      "epoch no.3 train no.289220  loss = 2.42042 avg_loss = 3.52713\n",
      "epoch no.3 train no.289230  loss = 2.77345 avg_loss = 3.49500\n",
      "epoch no.3 train no.289240  loss = 4.53042 avg_loss = 3.54225\n",
      "epoch no.3 train no.289250  loss = 4.13386 avg_loss = 3.59772\n",
      "epoch no.3 train no.289260  loss = 2.60893 avg_loss = 3.57843\n",
      "epoch no.3 train no.289270  loss = 4.37025 avg_loss = 3.53692\n",
      "epoch no.3 train no.289280  loss = 3.20657 avg_loss = 3.53741\n",
      "epoch no.3 train no.289290  loss = 3.30621 avg_loss = 3.53293\n",
      "epoch no.3 train no.289300  loss = 3.94938 avg_loss = 3.56991\n",
      "epoch no.3 train no.289310  loss = 3.61056 avg_loss = 3.58785\n",
      "epoch no.3 train no.289320  loss = 3.63731 avg_loss = 3.61815\n",
      "epoch no.3 train no.289330  loss = 2.26655 avg_loss = 3.58836\n",
      "epoch no.3 train no.289340  loss = 2.67502 avg_loss = 3.58191\n",
      "epoch no.3 train no.289350  loss = 3.88975 avg_loss = 3.59342\n",
      "epoch no.3 train no.289360  loss = 3.44923 avg_loss = 3.59347\n",
      "epoch no.3 train no.289370  loss = 3.78822 avg_loss = 3.58980\n",
      "epoch no.3 train no.289380  loss = 2.81800 avg_loss = 3.58313\n",
      "epoch no.3 train no.289390  loss = 2.95862 avg_loss = 3.53500\n",
      "epoch no.3 train no.289400  loss = 3.59699 avg_loss = 3.56358\n",
      "epoch no.3 train no.289410  loss = 2.47178 avg_loss = 3.52017\n",
      "epoch no.3 train no.289420  loss = 3.20254 avg_loss = 3.55053\n",
      "epoch no.3 train no.289430  loss = 1.69566 avg_loss = 3.56911\n",
      "epoch no.3 train no.289440  loss = 5.13089 avg_loss = 3.53411\n",
      "epoch no.3 train no.289450  loss = 6.33988 avg_loss = 3.57296\n",
      "epoch no.3 train no.289460  loss = 2.05110 avg_loss = 3.58075\n",
      "epoch no.3 train no.289470  loss = 3.63925 avg_loss = 3.55069\n",
      "epoch no.3 train no.289480  loss = 7.45702 avg_loss = 3.58950\n",
      "epoch no.3 train no.289490  loss = 3.86965 avg_loss = 3.59777\n",
      "epoch no.3 train no.289500  loss = 3.58252 avg_loss = 3.55314\n",
      "epoch no.3 train no.289510  loss = 3.26423 avg_loss = 3.54411\n",
      "epoch no.3 train no.289520  loss = 3.03057 avg_loss = 3.52849\n",
      "epoch no.3 train no.289530  loss = 2.46422 avg_loss = 3.50594\n",
      "epoch no.3 train no.289540  loss = 4.71838 avg_loss = 3.48715\n",
      "epoch no.3 train no.289550  loss = 3.20055 avg_loss = 3.48391\n",
      "epoch no.3 train no.289560  loss = 3.93818 avg_loss = 3.48285\n",
      "epoch no.3 train no.289570  loss = 4.00567 avg_loss = 3.50119\n",
      "epoch no.3 train no.289580  loss = 2.40409 avg_loss = 3.41933\n",
      "epoch no.3 train no.289590  loss = 5.45847 avg_loss = 3.48005\n",
      "epoch no.3 train no.289600  loss = 3.21718 avg_loss = 3.48729\n",
      "epoch no.3 train no.289610  loss = 3.85286 avg_loss = 3.49556\n",
      "epoch no.3 train no.289620  loss = 3.74931 avg_loss = 3.51326\n",
      "epoch no.3 train no.289630  loss = 2.79925 avg_loss = 3.51911\n",
      "epoch no.3 train no.289640  loss = 5.23158 avg_loss = 3.53743\n",
      "epoch no.3 train no.289650  loss = 5.29254 avg_loss = 3.52221\n",
      "epoch no.3 train no.289660  loss = 4.29412 avg_loss = 3.56429\n",
      "epoch no.3 train no.289670  loss = 4.16672 avg_loss = 3.57148\n",
      "epoch no.3 train no.289680  loss = 3.78013 avg_loss = 3.62686\n",
      "epoch no.3 train no.289690  loss = 2.16178 avg_loss = 3.58167\n",
      "epoch no.3 train no.289700  loss = 3.37620 avg_loss = 3.57323\n",
      "epoch no.3 train no.289710  loss = 4.01880 avg_loss = 3.55027\n",
      "epoch no.3 train no.289720  loss = 3.52158 avg_loss = 3.52247\n",
      "epoch no.3 train no.289730  loss = 3.79383 avg_loss = 3.52248\n",
      "epoch no.3 train no.289740  loss = 4.50944 avg_loss = 3.51843\n",
      "epoch no.3 train no.289750  loss = 2.36063 avg_loss = 3.52015\n",
      "epoch no.3 train no.289760  loss = 1.61766 avg_loss = 3.49500\n",
      "epoch no.3 train no.289770  loss = 4.95477 avg_loss = 3.50169\n",
      "epoch no.3 train no.289780  loss = 3.34520 avg_loss = 3.51573\n",
      "epoch no.3 train no.289790  loss = 2.51107 avg_loss = 3.47856\n",
      "epoch no.3 train no.289800  loss = 2.33698 avg_loss = 3.49969\n",
      "epoch no.3 train no.289810  loss = 4.32970 avg_loss = 3.51248\n",
      "epoch no.3 train no.289820  loss = 3.77781 avg_loss = 3.51013\n",
      "epoch no.3 train no.289830  loss = 2.44869 avg_loss = 3.52710\n",
      "epoch no.3 train no.289840  loss = 3.09066 avg_loss = 3.51754\n",
      "epoch no.3 train no.289850  loss = 2.24609 avg_loss = 3.50702\n",
      "epoch no.3 train no.289860  loss = 5.18699 avg_loss = 3.52607\n",
      "epoch no.3 train no.289870  loss = 4.05817 avg_loss = 3.52964\n",
      "epoch no.3 train no.289880  loss = 2.15123 avg_loss = 3.54656\n",
      "epoch no.3 train no.289890  loss = 4.12042 avg_loss = 3.58780\n",
      "epoch no.3 train no.289900  loss = 3.13702 avg_loss = 3.57698\n",
      "epoch no.3 train no.289910  loss = 2.87955 avg_loss = 3.63715\n",
      "epoch no.3 train no.289920  loss = 3.55143 avg_loss = 3.62442\n",
      "epoch no.3 train no.289930  loss = 4.02980 avg_loss = 3.61561\n",
      "epoch no.3 train no.289940  loss = 2.37549 avg_loss = 3.56447\n",
      "epoch no.3 train no.289950  loss = 5.91122 avg_loss = 3.54158\n",
      "epoch no.3 train no.289960  loss = 2.34595 avg_loss = 3.55401\n",
      "epoch no.3 train no.289970  loss = 2.97899 avg_loss = 3.52912\n",
      "epoch no.3 train no.289980  loss = 2.19500 avg_loss = 3.50944\n",
      "epoch no.3 train no.289990  loss = 3.40029 avg_loss = 3.48283\n",
      "epoch no.3 train no.290000  loss = 2.75144 avg_loss = 3.46540\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.290010  loss = 2.21824 avg_loss = 3.42671\n",
      "epoch no.3 train no.290020  loss = 2.87213 avg_loss = 3.37238\n",
      "epoch no.3 train no.290030  loss = 3.22610 avg_loss = 3.34825\n",
      "epoch no.3 train no.290040  loss = 3.84949 avg_loss = 3.36511\n",
      "epoch no.3 train no.290050  loss = 2.98379 avg_loss = 3.34445\n",
      "epoch no.3 train no.290060  loss = 1.58017 avg_loss = 3.42304\n",
      "epoch no.3 train no.290070  loss = 3.51834 avg_loss = 3.40152\n",
      "epoch no.3 train no.290080  loss = 3.96723 avg_loss = 3.40076\n",
      "epoch no.3 train no.290090  loss = 2.46077 avg_loss = 3.41710\n",
      "epoch no.3 train no.290100  loss = 3.02395 avg_loss = 3.44102\n",
      "epoch no.3 train no.290110  loss = 2.22120 avg_loss = 3.45347\n",
      "epoch no.3 train no.290120  loss = 3.47841 avg_loss = 3.41840\n",
      "epoch no.3 train no.290130  loss = 5.64526 avg_loss = 3.42195\n",
      "epoch no.3 train no.290140  loss = 3.58974 avg_loss = 3.40604\n",
      "epoch no.3 train no.290150  loss = 4.07676 avg_loss = 3.37605\n",
      "epoch no.3 train no.290160  loss = 4.73288 avg_loss = 3.37371\n",
      "epoch no.3 train no.290170  loss = 2.92218 avg_loss = 3.36991\n",
      "epoch no.3 train no.290180  loss = 3.82393 avg_loss = 3.34991\n",
      "epoch no.3 train no.290190  loss = 1.68356 avg_loss = 3.31266\n",
      "epoch no.3 train no.290200  loss = 2.53051 avg_loss = 3.30277\n",
      "epoch no.3 train no.290210  loss = 4.28259 avg_loss = 3.35241\n",
      "epoch no.3 train no.290220  loss = 2.96836 avg_loss = 3.33307\n",
      "epoch no.3 train no.290230  loss = 2.49835 avg_loss = 3.37278\n",
      "epoch no.3 train no.290240  loss = 1.58782 avg_loss = 3.36951\n",
      "epoch no.3 train no.290250  loss = 4.78393 avg_loss = 3.36304\n",
      "epoch no.3 train no.290260  loss = 2.81359 avg_loss = 3.34058\n",
      "epoch no.3 train no.290270  loss = 2.52259 avg_loss = 3.32930\n",
      "epoch no.3 train no.290280  loss = 2.91900 avg_loss = 3.37424\n",
      "epoch no.3 train no.290290  loss = 2.55899 avg_loss = 3.37507\n",
      "epoch no.3 train no.290300  loss = 3.32616 avg_loss = 3.41076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.290310  loss = 3.64652 avg_loss = 3.40060\n",
      "epoch no.3 train no.290320  loss = 4.07359 avg_loss = 3.35914\n",
      "epoch no.3 train no.290330  loss = 6.00664 avg_loss = 3.33928\n",
      "epoch no.3 train no.290340  loss = 3.09280 avg_loss = 3.40129\n",
      "epoch no.3 train no.290350  loss = 2.52891 avg_loss = 3.36011\n",
      "epoch no.3 train no.290360  loss = 3.31926 avg_loss = 3.42316\n",
      "epoch no.3 train no.290370  loss = 3.96962 avg_loss = 3.43129\n",
      "epoch no.3 train no.290380  loss = 2.45703 avg_loss = 3.43426\n",
      "epoch no.3 train no.290390  loss = 3.01832 avg_loss = 3.43782\n",
      "epoch no.3 train no.290400  loss = 2.21678 avg_loss = 3.43949\n",
      "epoch no.3 train no.290410  loss = 4.68503 avg_loss = 3.43091\n",
      "epoch no.3 train no.290420  loss = 2.53868 avg_loss = 3.45547\n",
      "epoch no.3 train no.290430  loss = 2.60339 avg_loss = 3.40857\n",
      "epoch no.3 train no.290440  loss = 2.13415 avg_loss = 3.40821\n",
      "epoch no.3 train no.290450  loss = 4.06384 avg_loss = 3.42563\n",
      "epoch no.3 train no.290460  loss = 3.41882 avg_loss = 3.44967\n",
      "epoch no.3 train no.290470  loss = 2.48651 avg_loss = 3.47950\n",
      "epoch no.3 train no.290480  loss = 3.46504 avg_loss = 3.43992\n",
      "epoch no.3 train no.290490  loss = 2.03838 avg_loss = 3.44013\n",
      "epoch no.3 train no.290500  loss = 2.88976 avg_loss = 3.43809\n",
      "epoch no.3 train no.290510  loss = 2.40149 avg_loss = 3.41016\n",
      "epoch no.3 train no.290520  loss = 3.75915 avg_loss = 3.50858\n",
      "epoch no.3 train no.290530  loss = 3.89064 avg_loss = 3.53492\n",
      "epoch no.3 train no.290540  loss = 4.04025 avg_loss = 3.54097\n",
      "epoch no.3 train no.290550  loss = 4.17978 avg_loss = 3.54780\n",
      "epoch no.3 train no.290560  loss = 3.20471 avg_loss = 3.53075\n",
      "epoch no.3 train no.290570  loss = 2.95537 avg_loss = 3.47710\n",
      "epoch no.3 train no.290580  loss = 3.56053 avg_loss = 3.49973\n",
      "epoch no.3 train no.290590  loss = 1.74136 avg_loss = 3.48640\n",
      "epoch no.3 train no.290600  loss = 1.83148 avg_loss = 3.45403\n",
      "epoch no.3 train no.290610  loss = 2.55606 avg_loss = 3.45395\n",
      "epoch no.3 train no.290620  loss = 3.01396 avg_loss = 3.43578\n",
      "epoch no.3 train no.290630  loss = 2.19236 avg_loss = 3.44197\n",
      "epoch no.3 train no.290640  loss = 4.07146 avg_loss = 3.43040\n",
      "epoch no.3 train no.290650  loss = 4.69811 avg_loss = 3.45661\n",
      "epoch no.3 train no.290660  loss = 3.10313 avg_loss = 3.43866\n",
      "epoch no.3 train no.290670  loss = 4.94107 avg_loss = 3.44236\n",
      "epoch no.3 train no.290680  loss = 3.77994 avg_loss = 3.44720\n",
      "epoch no.3 train no.290690  loss = 3.59792 avg_loss = 3.48847\n",
      "epoch no.3 train no.290700  loss = 2.56979 avg_loss = 3.43374\n",
      "epoch no.3 train no.290710  loss = 4.77991 avg_loss = 3.46220\n",
      "epoch no.3 train no.290720  loss = 5.66199 avg_loss = 3.52412\n",
      "epoch no.3 train no.290730  loss = 4.19719 avg_loss = 3.51393\n",
      "epoch no.3 train no.290740  loss = 4.28696 avg_loss = 3.49217\n",
      "epoch no.3 train no.290750  loss = 2.94046 avg_loss = 3.47930\n",
      "epoch no.3 train no.290760  loss = 4.48322 avg_loss = 3.48335\n",
      "epoch no.3 train no.290770  loss = 2.57020 avg_loss = 3.52890\n",
      "epoch no.3 train no.290780  loss = 5.21558 avg_loss = 3.54503\n",
      "epoch no.3 train no.290790  loss = 3.28139 avg_loss = 3.49000\n",
      "epoch no.3 train no.290800  loss = 3.21190 avg_loss = 3.51255\n",
      "epoch no.3 train no.290810  loss = 3.04745 avg_loss = 3.47491\n",
      "epoch no.3 train no.290820  loss = 5.48318 avg_loss = 3.42565\n",
      "epoch no.3 train no.290830  loss = 3.04678 avg_loss = 3.41705\n",
      "epoch no.3 train no.290840  loss = 2.23580 avg_loss = 3.41307\n",
      "epoch no.3 train no.290850  loss = 4.27006 avg_loss = 3.45091\n",
      "epoch no.3 train no.290860  loss = 4.68012 avg_loss = 3.47711\n",
      "epoch no.3 train no.290870  loss = 2.26340 avg_loss = 3.52843\n",
      "epoch no.3 train no.290880  loss = 3.46583 avg_loss = 3.51667\n",
      "epoch no.3 train no.290890  loss = 3.34154 avg_loss = 3.50148\n",
      "epoch no.3 train no.290900  loss = 3.74058 avg_loss = 3.48528\n",
      "epoch no.3 train no.290910  loss = 2.48764 avg_loss = 3.48438\n",
      "epoch no.3 train no.290920  loss = 3.11991 avg_loss = 3.46680\n",
      "epoch no.3 train no.290930  loss = 2.20110 avg_loss = 3.49015\n",
      "epoch no.3 train no.290940  loss = 4.16582 avg_loss = 3.48246\n",
      "epoch no.3 train no.290950  loss = 4.92644 avg_loss = 3.48855\n",
      "epoch no.3 train no.290960  loss = 3.90364 avg_loss = 3.46981\n",
      "epoch no.3 train no.290970  loss = 2.52387 avg_loss = 3.44171\n",
      "epoch no.3 train no.290980  loss = 3.92603 avg_loss = 3.43852\n",
      "epoch no.3 train no.290990  loss = 4.99893 avg_loss = 3.44365\n",
      "epoch no.3 train no.291000  loss = 2.95780 avg_loss = 3.40139\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.3 train no.291010  loss = 3.55836 avg_loss = 3.40877\n",
      "epoch no.3 train no.291020  loss = 3.89003 avg_loss = 3.42973\n",
      "epoch no.3 train no.291030  loss = 3.35263 avg_loss = 3.42623\n",
      "epoch no.3 train no.291040  loss = 3.20259 avg_loss = 3.45851\n",
      "epoch no.3 train no.291050  loss = 2.96078 avg_loss = 3.44580\n",
      "epoch no.3 train no.291060  loss = 2.42636 avg_loss = 3.45948\n",
      "epoch no.3 train no.291070  loss = 2.98287 avg_loss = 3.42749\n",
      "epoch no.3 train no.291080  loss = 3.01707 avg_loss = 3.46399\n",
      "epoch no.3 train no.291090  loss = 2.93744 avg_loss = 3.41645\n",
      "epoch no.3 train no.291100  loss = 1.73853 avg_loss = 3.41507\n",
      "epoch no.3 train no.291110  loss = 4.21182 avg_loss = 3.40975\n",
      "epoch no.3 train no.291120  loss = 4.25356 avg_loss = 3.44578\n",
      "epoch no.3 train no.291130  loss = 2.16897 avg_loss = 3.40678\n",
      "epoch no.3 train no.291140  loss = 4.90641 avg_loss = 3.44747\n",
      "epoch no.3 train no.291150  loss = 4.09123 avg_loss = 3.42230\n",
      "epoch no.3 train no.291160  loss = 4.14582 avg_loss = 3.40543\n",
      "epoch no.3 train no.291170  loss = 2.66391 avg_loss = 3.39272\n",
      "epoch no.3 train no.291180  loss = 3.72548 avg_loss = 3.37716\n",
      "epoch no.3 train no.291190  loss = 3.66145 avg_loss = 3.35629\n",
      "epoch no.3 train no.291200  loss = 3.97183 avg_loss = 3.38286\n",
      "epoch no.3 train no.291210  loss = 4.18773 avg_loss = 3.33676\n",
      "epoch no.3 train no.291220  loss = 2.54696 avg_loss = 3.35690\n",
      "epoch no.3 train no.291230  loss = 4.80736 avg_loss = 3.42325\n",
      "epoch no.3 train no.291240  loss = 4.65691 avg_loss = 3.40506\n",
      "epoch no.3 train no.291250  loss = 3.40912 avg_loss = 3.42083\n",
      "epoch no.3 train no.291260  loss = 2.78879 avg_loss = 3.45336\n",
      "epoch no.3 train no.291270  loss = 2.91014 avg_loss = 3.43616\n",
      "epoch no.3 train no.291280  loss = 4.42423 avg_loss = 3.42874\n",
      "epoch no.3 train no.291290  loss = 5.24940 avg_loss = 3.44512\n",
      "epoch no.3 train no.291300  loss = 1.83254 avg_loss = 3.41156\n",
      "epoch no.3 train no.291310  loss = 2.88886 avg_loss = 3.42487\n",
      "epoch no.3 train no.291320  loss = 4.89073 avg_loss = 3.47188\n",
      "epoch no.3 train no.291330  loss = 2.33667 avg_loss = 3.48104\n",
      "epoch no.3 train no.291340  loss = 2.15085 avg_loss = 3.46319\n",
      "epoch no.3 train no.291350  loss = 3.45583 avg_loss = 3.43923\n",
      "epoch no.3 train no.291360  loss = 3.69611 avg_loss = 3.43127\n",
      "epoch no.3 train no.291370  loss = 4.17309 avg_loss = 3.40299\n",
      "epoch no.3 train no.291380  loss = 4.77662 avg_loss = 3.42386\n",
      "epoch no.3 train no.291390  loss = 4.27648 avg_loss = 3.44954\n",
      "epoch no.3 train no.291400  loss = 3.27141 avg_loss = 3.46053\n",
      "epoch no.3 train no.291410  loss = 2.64519 avg_loss = 3.43689\n",
      "epoch no.3 train no.291420  loss = 2.36416 avg_loss = 3.44012\n",
      "epoch no.3 train no.291430  loss = 3.11194 avg_loss = 3.39406\n",
      "epoch no.3 train no.291440  loss = 4.88398 avg_loss = 3.37666\n",
      "epoch no.3 train no.291450  loss = 3.41957 avg_loss = 3.42076\n",
      "epoch no.3 train no.291460  loss = 2.62581 avg_loss = 3.46692\n",
      "epoch no.3 train no.291470  loss = 6.72418 avg_loss = 3.48910\n",
      "epoch no.3 train no.291480  loss = 4.41583 avg_loss = 3.48781\n",
      "epoch no.3 train no.291490  loss = 3.34119 avg_loss = 3.46805\n",
      "epoch no.3 train no.291500  loss = 2.93310 avg_loss = 3.47620\n",
      "epoch no.3 train no.291510  loss = 3.57313 avg_loss = 3.45497\n",
      "epoch no.3 train no.291520  loss = 3.57052 avg_loss = 3.43012\n",
      "epoch no.3 train no.291530  loss = 4.18988 avg_loss = 3.43701\n",
      "epoch no.3 train no.291540  loss = 1.96094 avg_loss = 3.42717\n",
      "epoch no.3 train no.291550  loss = 3.01123 avg_loss = 3.44655\n",
      "epoch no.3 train no.291560  loss = 5.21920 avg_loss = 3.46285\n",
      "epoch no.3 train no.291570  loss = 2.11213 avg_loss = 3.42816\n",
      "epoch no.3 train no.291580  loss = 2.16470 avg_loss = 3.45698\n",
      "epoch no.3 train no.291590  loss = 4.39436 avg_loss = 3.50951\n",
      "epoch no.3 train no.291600  loss = 2.98486 avg_loss = 3.49713\n",
      "epoch no.3 train no.291610  loss = 4.08669 avg_loss = 3.51054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.291620  loss = 3.79969 avg_loss = 3.54852\n",
      "epoch no.3 train no.291630  loss = 2.95957 avg_loss = 3.56281\n",
      "epoch no.3 train no.291640  loss = 5.20011 avg_loss = 3.54448\n",
      "epoch no.3 train no.291650  loss = 2.97565 avg_loss = 3.53118\n",
      "epoch no.3 train no.291660  loss = 3.56007 avg_loss = 3.55217\n",
      "epoch no.3 train no.291670  loss = 2.81715 avg_loss = 3.56075\n",
      "epoch no.3 train no.291680  loss = 4.21799 avg_loss = 3.60010\n",
      "epoch no.3 train no.291690  loss = 2.29854 avg_loss = 3.57329\n",
      "epoch no.3 train no.291700  loss = 2.55433 avg_loss = 3.54179\n",
      "epoch no.3 train no.291710  loss = 3.66998 avg_loss = 3.56831\n",
      "epoch no.3 train no.291720  loss = 3.22924 avg_loss = 3.53912\n",
      "epoch no.3 train no.291730  loss = 4.13414 avg_loss = 3.54121\n",
      "epoch no.3 train no.291740  loss = 3.27163 avg_loss = 3.58593\n",
      "epoch no.3 train no.291750  loss = 3.22847 avg_loss = 3.53498\n",
      "epoch no.3 train no.291760  loss = 2.30154 avg_loss = 3.50056\n",
      "epoch no.3 train no.291770  loss = 3.18616 avg_loss = 3.50281\n",
      "epoch no.3 train no.291780  loss = 2.16373 avg_loss = 3.46925\n",
      "epoch no.3 train no.291790  loss = 4.03618 avg_loss = 3.44428\n",
      "epoch no.3 train no.291800  loss = 4.38934 avg_loss = 3.42621\n",
      "epoch no.3 train no.291810  loss = 3.09092 avg_loss = 3.45743\n",
      "epoch no.3 train no.291820  loss = 3.46066 avg_loss = 3.42878\n",
      "epoch no.3 train no.291830  loss = 4.51787 avg_loss = 3.44958\n",
      "epoch no.3 train no.291840  loss = 4.15768 avg_loss = 3.41277\n",
      "epoch no.3 train no.291850  loss = 3.69428 avg_loss = 3.41085\n",
      "epoch no.3 train no.291860  loss = 4.16370 avg_loss = 3.41105\n",
      "epoch no.3 train no.291870  loss = 2.67167 avg_loss = 3.41040\n",
      "epoch no.3 train no.291880  loss = 3.19471 avg_loss = 3.39238\n",
      "epoch no.3 train no.291890  loss = 4.28920 avg_loss = 3.39993\n",
      "epoch no.3 train no.291900  loss = 4.41283 avg_loss = 3.38561\n",
      "epoch no.3 train no.291910  loss = 2.93302 avg_loss = 3.41127\n",
      "epoch no.3 train no.291920  loss = 4.98963 avg_loss = 3.44990\n",
      "epoch no.3 train no.291930  loss = 2.19869 avg_loss = 3.42616\n",
      "epoch no.3 train no.291940  loss = 3.39602 avg_loss = 3.44687\n",
      "epoch no.3 train no.291950  loss = 3.24691 avg_loss = 3.48513\n",
      "epoch no.3 train no.291960  loss = 3.44123 avg_loss = 3.45453\n",
      "epoch no.3 train no.291970  loss = 4.22009 avg_loss = 3.51561\n",
      "epoch no.3 train no.291980  loss = 3.13238 avg_loss = 3.51895\n",
      "epoch no.3 train no.291990  loss = 5.18394 avg_loss = 3.52493\n",
      "epoch no.3 train no.292000  loss = 3.54298 avg_loss = 3.52015\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.292010  loss = 3.31873 avg_loss = 3.57542\n",
      "epoch no.3 train no.292020  loss = 3.83731 avg_loss = 3.57057\n",
      "epoch no.3 train no.292030  loss = 3.62677 avg_loss = 3.54665\n",
      "epoch no.3 train no.292040  loss = 3.88215 avg_loss = 3.53169\n",
      "epoch no.3 train no.292050  loss = 3.98600 avg_loss = 3.53180\n",
      "epoch no.3 train no.292060  loss = 3.17726 avg_loss = 3.50054\n",
      "epoch no.3 train no.292070  loss = 3.10455 avg_loss = 3.48468\n",
      "epoch no.3 train no.292080  loss = 2.72785 avg_loss = 3.49583\n",
      "epoch no.3 train no.292090  loss = 2.95546 avg_loss = 3.51859\n",
      "epoch no.3 train no.292100  loss = 2.83989 avg_loss = 3.54962\n",
      "epoch no.3 train no.292110  loss = 3.41066 avg_loss = 3.51121\n",
      "epoch no.3 train no.292120  loss = 2.29135 avg_loss = 3.48584\n",
      "epoch no.3 train no.292130  loss = 2.90476 avg_loss = 3.43285\n",
      "epoch no.3 train no.292140  loss = 2.60968 avg_loss = 3.43234\n",
      "epoch no.3 train no.292150  loss = 3.64269 avg_loss = 3.41305\n",
      "epoch no.3 train no.292160  loss = 3.00963 avg_loss = 3.40240\n",
      "epoch no.3 train no.292170  loss = 2.07570 avg_loss = 3.36322\n",
      "epoch no.3 train no.292180  loss = 4.21750 avg_loss = 3.36168\n",
      "epoch no.3 train no.292190  loss = 2.63387 avg_loss = 3.36309\n",
      "epoch no.3 train no.292200  loss = 4.20708 avg_loss = 3.36044\n",
      "epoch no.3 train no.292210  loss = 3.47304 avg_loss = 3.35523\n",
      "epoch no.3 train no.292220  loss = 3.26703 avg_loss = 3.38718\n",
      "epoch no.3 train no.292230  loss = 3.39142 avg_loss = 3.38078\n",
      "epoch no.3 train no.292240  loss = 3.90114 avg_loss = 3.42206\n",
      "epoch no.3 train no.292250  loss = 3.94978 avg_loss = 3.42950\n",
      "epoch no.3 train no.292260  loss = 3.40315 avg_loss = 3.41056\n",
      "epoch no.3 train no.292270  loss = 2.70535 avg_loss = 3.36597\n",
      "epoch no.3 train no.292280  loss = 2.03378 avg_loss = 3.35926\n",
      "epoch no.3 train no.292290  loss = 1.78042 avg_loss = 3.33114\n",
      "epoch no.3 train no.292300  loss = 2.44542 avg_loss = 3.33630\n",
      "epoch no.3 train no.292310  loss = 2.33703 avg_loss = 3.35107\n",
      "epoch no.3 train no.292320  loss = 2.30730 avg_loss = 3.42137\n",
      "epoch no.3 train no.292330  loss = 3.72136 avg_loss = 3.47184\n",
      "epoch no.3 train no.292340  loss = 3.40263 avg_loss = 3.48208\n",
      "epoch no.3 train no.292350  loss = 3.42691 avg_loss = 3.49616\n",
      "epoch no.3 train no.292360  loss = 3.36543 avg_loss = 3.47979\n",
      "epoch no.3 train no.292370  loss = 3.78846 avg_loss = 3.44949\n",
      "epoch no.3 train no.292380  loss = 2.84059 avg_loss = 3.45076\n",
      "epoch no.3 train no.292390  loss = 3.22282 avg_loss = 3.44264\n",
      "epoch no.3 train no.292400  loss = 2.71783 avg_loss = 3.36430\n",
      "epoch no.3 train no.292410  loss = 3.33539 avg_loss = 3.37569\n",
      "epoch no.3 train no.292420  loss = 3.77267 avg_loss = 3.36458\n",
      "epoch no.3 train no.292430  loss = 3.01260 avg_loss = 3.31911\n",
      "epoch no.3 train no.292440  loss = 3.55386 avg_loss = 3.36871\n",
      "epoch no.3 train no.292450  loss = 4.17356 avg_loss = 3.37131\n",
      "epoch no.3 train no.292460  loss = 2.85556 avg_loss = 3.35882\n",
      "epoch no.3 train no.292470  loss = 2.94304 avg_loss = 3.34272\n",
      "epoch no.3 train no.292480  loss = 4.43363 avg_loss = 3.38456\n",
      "epoch no.3 train no.292490  loss = 7.48102 avg_loss = 3.43916\n",
      "epoch no.3 train no.292500  loss = 4.20437 avg_loss = 3.48171\n",
      "epoch no.3 train no.292510  loss = 3.38856 avg_loss = 3.45202\n",
      "epoch no.3 train no.292520  loss = 2.81217 avg_loss = 3.39980\n",
      "epoch no.3 train no.292530  loss = 3.58341 avg_loss = 3.41353\n",
      "epoch no.3 train no.292540  loss = 2.00633 avg_loss = 3.37333\n",
      "epoch no.3 train no.292550  loss = 5.23692 avg_loss = 3.40287\n",
      "epoch no.3 train no.292560  loss = 2.04881 avg_loss = 3.39657\n",
      "epoch no.3 train no.292570  loss = 2.52109 avg_loss = 3.38068\n",
      "epoch no.3 train no.292580  loss = 3.48896 avg_loss = 3.40079\n",
      "epoch no.3 train no.292590  loss = 3.91826 avg_loss = 3.36910\n",
      "epoch no.3 train no.292600  loss = 2.75032 avg_loss = 3.41004\n",
      "epoch no.3 train no.292610  loss = 3.14187 avg_loss = 3.39639\n",
      "epoch no.3 train no.292620  loss = 2.61241 avg_loss = 3.40799\n",
      "epoch no.3 train no.292630  loss = 2.03218 avg_loss = 3.43447\n",
      "epoch no.3 train no.292640  loss = 4.26106 avg_loss = 3.48115\n",
      "epoch no.3 train no.292650  loss = 2.29042 avg_loss = 3.46097\n",
      "epoch no.3 train no.292660  loss = 3.24084 avg_loss = 3.43847\n",
      "epoch no.3 train no.292670  loss = 4.82028 avg_loss = 3.43126\n",
      "epoch no.3 train no.292680  loss = 3.66145 avg_loss = 3.44582\n",
      "epoch no.3 train no.292690  loss = 6.72304 avg_loss = 3.48872\n",
      "epoch no.3 train no.292700  loss = 2.72190 avg_loss = 3.45497\n",
      "epoch no.3 train no.292710  loss = 4.14386 avg_loss = 3.48417\n",
      "epoch no.3 train no.292720  loss = 1.92133 avg_loss = 3.46810\n",
      "epoch no.3 train no.292730  loss = 4.55636 avg_loss = 3.50068\n",
      "epoch no.3 train no.292740  loss = 5.33587 avg_loss = 3.48116\n",
      "epoch no.3 train no.292750  loss = 3.43023 avg_loss = 3.46599\n",
      "epoch no.3 train no.292760  loss = 4.67234 avg_loss = 3.44799\n",
      "epoch no.3 train no.292770  loss = 5.14632 avg_loss = 3.50870\n",
      "epoch no.3 train no.292780  loss = 4.73026 avg_loss = 3.50536\n",
      "epoch no.3 train no.292790  loss = 6.42151 avg_loss = 3.57208\n",
      "epoch no.3 train no.292800  loss = 3.39316 avg_loss = 3.56035\n",
      "epoch no.3 train no.292810  loss = 3.84345 avg_loss = 3.56883\n",
      "epoch no.3 train no.292820  loss = 3.67377 avg_loss = 3.55413\n",
      "epoch no.3 train no.292830  loss = 2.41215 avg_loss = 3.47143\n",
      "epoch no.3 train no.292840  loss = 3.25392 avg_loss = 3.53672\n",
      "epoch no.3 train no.292850  loss = 3.72564 avg_loss = 3.50659\n",
      "epoch no.3 train no.292860  loss = 4.48264 avg_loss = 3.53116\n",
      "epoch no.3 train no.292870  loss = 4.68796 avg_loss = 3.58154\n",
      "epoch no.3 train no.292880  loss = 3.19140 avg_loss = 3.56661\n",
      "epoch no.3 train no.292890  loss = 5.97479 avg_loss = 3.64215\n",
      "epoch no.3 train no.292900  loss = 3.83515 avg_loss = 3.61346\n",
      "epoch no.3 train no.292910  loss = 3.57119 avg_loss = 3.56871\n",
      "epoch no.3 train no.292920  loss = 2.65350 avg_loss = 3.53359\n",
      "epoch no.3 train no.292930  loss = 2.98185 avg_loss = 3.54064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.292940  loss = 2.45370 avg_loss = 3.50629\n",
      "epoch no.3 train no.292950  loss = 4.73210 avg_loss = 3.50368\n",
      "epoch no.3 train no.292960  loss = 4.90978 avg_loss = 3.48687\n",
      "epoch no.3 train no.292970  loss = 3.22837 avg_loss = 3.53088\n",
      "epoch no.3 train no.292980  loss = 2.70885 avg_loss = 3.56508\n",
      "epoch no.3 train no.292990  loss = 2.75523 avg_loss = 3.53445\n",
      "epoch no.3 train no.293000  loss = 2.78727 avg_loss = 3.52384\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.293010  loss = 2.39684 avg_loss = 3.48630\n",
      "epoch no.3 train no.293020  loss = 5.36415 avg_loss = 3.49336\n",
      "epoch no.3 train no.293030  loss = 2.64086 avg_loss = 3.44740\n",
      "epoch no.3 train no.293040  loss = 2.79100 avg_loss = 3.50225\n",
      "epoch no.3 train no.293050  loss = 3.91722 avg_loss = 3.47756\n",
      "epoch no.3 train no.293060  loss = 4.75454 avg_loss = 3.47272\n",
      "epoch no.3 train no.293070  loss = 2.21687 avg_loss = 3.44335\n",
      "epoch no.3 train no.293080  loss = 3.72184 avg_loss = 3.45138\n",
      "epoch no.3 train no.293090  loss = 4.20450 avg_loss = 3.47180\n",
      "epoch no.3 train no.293100  loss = 1.71304 avg_loss = 3.49592\n",
      "epoch no.3 train no.293110  loss = 2.74793 avg_loss = 3.49014\n",
      "epoch no.3 train no.293120  loss = 2.27694 avg_loss = 3.44902\n",
      "epoch no.3 train no.293130  loss = 2.91320 avg_loss = 3.44697\n",
      "epoch no.3 train no.293140  loss = 4.78951 avg_loss = 3.46364\n",
      "epoch no.3 train no.293150  loss = 3.19207 avg_loss = 3.46050\n",
      "epoch no.3 train no.293160  loss = 4.25775 avg_loss = 3.47906\n",
      "epoch no.3 train no.293170  loss = 2.81061 avg_loss = 3.44754\n",
      "epoch no.3 train no.293180  loss = 4.75064 avg_loss = 3.46136\n",
      "epoch no.3 train no.293190  loss = 3.29778 avg_loss = 3.42907\n",
      "epoch no.3 train no.293200  loss = 4.48855 avg_loss = 3.40346\n",
      "epoch no.3 train no.293210  loss = 5.05087 avg_loss = 3.47013\n",
      "epoch no.3 train no.293220  loss = 2.52809 avg_loss = 3.42814\n",
      "epoch no.3 train no.293230  loss = 5.69476 avg_loss = 3.47635\n",
      "epoch no.3 train no.293240  loss = 4.46970 avg_loss = 3.49640\n",
      "epoch no.3 train no.293250  loss = 3.05170 avg_loss = 3.44801\n",
      "epoch no.3 train no.293260  loss = 2.51834 avg_loss = 3.47319\n",
      "epoch no.3 train no.293270  loss = 3.15605 avg_loss = 3.50979\n",
      "epoch no.3 train no.293280  loss = 2.65785 avg_loss = 3.51997\n",
      "epoch no.3 train no.293290  loss = 2.55740 avg_loss = 3.47815\n",
      "epoch no.3 train no.293300  loss = 3.57576 avg_loss = 3.46149\n",
      "epoch no.3 train no.293310  loss = 4.23469 avg_loss = 3.50399\n",
      "epoch no.3 train no.293320  loss = 2.45270 avg_loss = 3.53027\n",
      "epoch no.3 train no.293330  loss = 3.43194 avg_loss = 3.52191\n",
      "epoch no.3 train no.293340  loss = 2.95564 avg_loss = 3.48955\n",
      "epoch no.3 train no.293350  loss = 3.59344 avg_loss = 3.49650\n",
      "epoch no.3 train no.293360  loss = 3.00964 avg_loss = 3.47521\n",
      "epoch no.3 train no.293370  loss = 4.58925 avg_loss = 3.47680\n",
      "epoch no.3 train no.293380  loss = 6.23336 avg_loss = 3.50739\n",
      "epoch no.3 train no.293390  loss = 2.67583 avg_loss = 3.53376\n",
      "epoch no.3 train no.293400  loss = 6.35177 avg_loss = 3.51067\n",
      "epoch no.3 train no.293410  loss = 3.79571 avg_loss = 3.50259\n",
      "epoch no.3 train no.293420  loss = 1.88799 avg_loss = 3.49290\n",
      "epoch no.3 train no.293430  loss = 2.87476 avg_loss = 3.46819\n",
      "epoch no.3 train no.293440  loss = 3.73708 avg_loss = 3.42840\n",
      "epoch no.3 train no.293450  loss = 2.81732 avg_loss = 3.40119\n",
      "epoch no.3 train no.293460  loss = 2.67605 avg_loss = 3.40027\n",
      "epoch no.3 train no.293470  loss = 3.65887 avg_loss = 3.37512\n",
      "epoch no.3 train no.293480  loss = 3.00273 avg_loss = 3.34621\n",
      "epoch no.3 train no.293490  loss = 3.63138 avg_loss = 3.35564\n",
      "epoch no.3 train no.293500  loss = 3.25954 avg_loss = 3.43356\n",
      "epoch no.3 train no.293510  loss = 3.69049 avg_loss = 3.46938\n",
      "epoch no.3 train no.293520  loss = 3.48905 avg_loss = 3.45461\n",
      "epoch no.3 train no.293530  loss = 2.34377 avg_loss = 3.43666\n",
      "epoch no.3 train no.293540  loss = 4.26462 avg_loss = 3.46741\n",
      "epoch no.3 train no.293550  loss = 5.89009 avg_loss = 3.49366\n",
      "epoch no.3 train no.293560  loss = 3.12393 avg_loss = 3.48892\n",
      "epoch no.3 train no.293570  loss = 4.41919 avg_loss = 3.40977\n",
      "epoch no.3 train no.293580  loss = 1.51771 avg_loss = 3.38997\n",
      "epoch no.3 train no.293590  loss = 5.66544 avg_loss = 3.42833\n",
      "epoch no.3 train no.293600  loss = 2.65856 avg_loss = 3.43919\n",
      "epoch no.3 train no.293610  loss = 4.81106 avg_loss = 3.44684\n",
      "epoch no.3 train no.293620  loss = 3.16273 avg_loss = 3.46523\n",
      "epoch no.3 train no.293630  loss = 3.11183 avg_loss = 3.44167\n",
      "epoch no.3 train no.293640  loss = 4.55693 avg_loss = 3.42023\n",
      "epoch no.3 train no.293650  loss = 2.87585 avg_loss = 3.36541\n",
      "epoch no.3 train no.293660  loss = 3.85778 avg_loss = 3.36555\n",
      "epoch no.3 train no.293670  loss = 4.05642 avg_loss = 3.34337\n",
      "epoch no.3 train no.293680  loss = 3.15469 avg_loss = 3.33564\n",
      "epoch no.3 train no.293690  loss = 2.16554 avg_loss = 3.29949\n",
      "epoch no.3 train no.293700  loss = 3.50524 avg_loss = 3.34885\n",
      "epoch no.3 train no.293710  loss = 1.83059 avg_loss = 3.30934\n",
      "epoch no.3 train no.293720  loss = 3.60902 avg_loss = 3.35443\n",
      "epoch no.3 train no.293730  loss = 2.20472 avg_loss = 3.36748\n",
      "epoch no.3 train no.293740  loss = 3.99841 avg_loss = 3.40135\n",
      "epoch no.3 train no.293750  loss = 2.86100 avg_loss = 3.36182\n",
      "epoch no.3 train no.293760  loss = 2.69397 avg_loss = 3.37295\n",
      "epoch no.3 train no.293770  loss = 1.17988 avg_loss = 3.35988\n",
      "epoch no.3 train no.293780  loss = 2.91176 avg_loss = 3.33236\n",
      "epoch no.3 train no.293790  loss = 4.00565 avg_loss = 3.36415\n",
      "epoch no.3 train no.293800  loss = 3.45085 avg_loss = 3.35859\n",
      "epoch no.3 train no.293810  loss = 3.54161 avg_loss = 3.39550\n",
      "epoch no.3 train no.293820  loss = 2.74305 avg_loss = 3.37956\n",
      "epoch no.3 train no.293830  loss = 4.78691 avg_loss = 3.42722\n",
      "epoch no.3 train no.293840  loss = 3.17864 avg_loss = 3.40342\n",
      "epoch no.3 train no.293850  loss = 2.83537 avg_loss = 3.37825\n",
      "epoch no.3 train no.293860  loss = 3.36995 avg_loss = 3.43932\n",
      "epoch no.3 train no.293870  loss = 3.16743 avg_loss = 3.37861\n",
      "epoch no.3 train no.293880  loss = 2.88767 avg_loss = 3.33063\n",
      "epoch no.3 train no.293890  loss = 3.03548 avg_loss = 3.42199\n",
      "epoch no.3 train no.293900  loss = 3.57538 avg_loss = 3.48519\n",
      "epoch no.3 train no.293910  loss = 3.97151 avg_loss = 3.52440\n",
      "epoch no.3 train no.293920  loss = 2.57175 avg_loss = 3.48743\n",
      "epoch no.3 train no.293930  loss = 3.54384 avg_loss = 3.51294\n",
      "epoch no.3 train no.293940  loss = 4.36337 avg_loss = 3.53683\n",
      "epoch no.3 train no.293950  loss = 3.73207 avg_loss = 3.49866\n",
      "epoch no.3 train no.293960  loss = 2.56035 avg_loss = 3.49658\n",
      "epoch no.3 train no.293970  loss = 3.05450 avg_loss = 3.50622\n",
      "epoch no.3 train no.293980  loss = 3.38359 avg_loss = 3.55038\n",
      "epoch no.3 train no.293990  loss = 3.17735 avg_loss = 3.50979\n",
      "epoch no.3 train no.294000  loss = 4.13022 avg_loss = 3.53233\n",
      "7\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '▁노래', 'op', '</s>']\n",
      "기분전환이 필요할때 듣는 신나는 pop</s>\n",
      "epoch no.3 train no.294010  loss = 2.24439 avg_loss = 3.50174\n",
      "epoch no.3 train no.294020  loss = 3.42882 avg_loss = 3.49637\n",
      "epoch no.3 train no.294030  loss = 3.14079 avg_loss = 3.52608\n",
      "epoch no.3 train no.294040  loss = 2.97239 avg_loss = 3.51680\n",
      "epoch no.3 train no.294050  loss = 4.62149 avg_loss = 3.52514\n",
      "epoch no.3 train no.294060  loss = 2.17227 avg_loss = 3.56392\n",
      "epoch no.3 train no.294070  loss = 2.89317 avg_loss = 3.58069\n",
      "epoch no.3 train no.294080  loss = 4.98287 avg_loss = 3.56786\n",
      "epoch no.3 train no.294090  loss = 4.00158 avg_loss = 3.57779\n",
      "epoch no.3 train no.294100  loss = 2.05657 avg_loss = 3.59026\n",
      "epoch no.3 train no.294110  loss = 3.38055 avg_loss = 3.54835\n",
      "epoch no.3 train no.294120  loss = 2.03450 avg_loss = 3.52209\n",
      "epoch no.3 train no.294130  loss = 5.01800 avg_loss = 3.54101\n",
      "epoch no.3 train no.294140  loss = 3.00623 avg_loss = 3.52748\n",
      "epoch no.3 train no.294150  loss = 2.57666 avg_loss = 3.53988\n",
      "epoch no.3 train no.294160  loss = 3.48423 avg_loss = 3.49786\n",
      "epoch no.3 train no.294170  loss = 2.20843 avg_loss = 3.44815\n",
      "epoch no.3 train no.294180  loss = 2.66152 avg_loss = 3.44958\n",
      "epoch no.3 train no.294190  loss = 3.43135 avg_loss = 3.47406\n",
      "epoch no.3 train no.294200  loss = 3.87907 avg_loss = 3.46793\n",
      "epoch no.3 train no.294210  loss = 3.08585 avg_loss = 3.46831\n",
      "epoch no.3 train no.294220  loss = 3.42883 avg_loss = 3.41697\n",
      "epoch no.3 train no.294230  loss = 2.64038 avg_loss = 3.36979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.294240  loss = 3.56951 avg_loss = 3.37114\n",
      "epoch no.3 train no.294250  loss = 2.82513 avg_loss = 3.35863\n",
      "epoch no.3 train no.294260  loss = 2.75109 avg_loss = 3.36019\n",
      "epoch no.3 train no.294270  loss = 2.82488 avg_loss = 3.35660\n",
      "epoch no.3 train no.294280  loss = 3.50776 avg_loss = 3.36763\n",
      "epoch no.3 train no.294290  loss = 2.53685 avg_loss = 3.42086\n",
      "epoch no.3 train no.294300  loss = 2.50121 avg_loss = 3.45696\n",
      "epoch no.3 train no.294310  loss = 5.32735 avg_loss = 3.46439\n",
      "epoch no.3 train no.294320  loss = 2.44213 avg_loss = 3.44565\n",
      "epoch no.3 train no.294330  loss = 3.02665 avg_loss = 3.42434\n",
      "epoch no.3 train no.294340  loss = 4.21865 avg_loss = 3.40776\n",
      "epoch no.3 train no.294350  loss = 2.89207 avg_loss = 3.41153\n",
      "epoch no.3 train no.294360  loss = 3.96205 avg_loss = 3.40501\n",
      "epoch no.3 train no.294370  loss = 3.81441 avg_loss = 3.41988\n",
      "epoch no.3 train no.294380  loss = 3.18034 avg_loss = 3.39710\n",
      "epoch no.3 train no.294390  loss = 4.04379 avg_loss = 3.39011\n",
      "epoch no.3 train no.294400  loss = 3.65025 avg_loss = 3.41245\n",
      "epoch no.3 train no.294410  loss = 3.32240 avg_loss = 3.36552\n",
      "epoch no.3 train no.294420  loss = 3.04010 avg_loss = 3.37601\n",
      "epoch no.3 train no.294430  loss = 2.82940 avg_loss = 3.39254\n",
      "epoch no.3 train no.294440  loss = 2.91530 avg_loss = 3.39316\n",
      "epoch no.3 train no.294450  loss = 4.96431 avg_loss = 3.39793\n",
      "epoch no.3 train no.294460  loss = 4.17457 avg_loss = 3.44343\n",
      "epoch no.3 train no.294470  loss = 3.26130 avg_loss = 3.48272\n",
      "epoch no.3 train no.294480  loss = 5.33360 avg_loss = 3.47737\n",
      "epoch no.3 train no.294490  loss = 3.78965 avg_loss = 3.44309\n",
      "epoch no.3 train no.294500  loss = 2.65701 avg_loss = 3.42747\n",
      "epoch no.3 train no.294510  loss = 3.00126 avg_loss = 3.39639\n",
      "epoch no.3 train no.294520  loss = 3.31864 avg_loss = 3.39857\n",
      "epoch no.3 train no.294530  loss = 2.35949 avg_loss = 3.35211\n",
      "epoch no.3 train no.294540  loss = 4.58576 avg_loss = 3.40735\n",
      "epoch no.3 train no.294550  loss = 2.52295 avg_loss = 3.38440\n",
      "epoch no.3 train no.294560  loss = 2.39318 avg_loss = 3.33858\n",
      "epoch no.3 train no.294570  loss = 3.81286 avg_loss = 3.36140\n",
      "epoch no.3 train no.294580  loss = 1.22549 avg_loss = 3.32639\n",
      "epoch no.3 train no.294590  loss = 4.41703 avg_loss = 3.30583\n",
      "epoch no.3 train no.294600  loss = 2.89777 avg_loss = 3.33867\n",
      "epoch no.3 train no.294610  loss = 3.07011 avg_loss = 3.34470\n",
      "epoch no.3 train no.294620  loss = 2.32732 avg_loss = 3.32885\n",
      "epoch no.3 train no.294630  loss = 2.75619 avg_loss = 3.33945\n",
      "epoch no.3 train no.294640  loss = 3.18523 avg_loss = 3.32697\n",
      "epoch no.3 train no.294650  loss = 3.56171 avg_loss = 3.35058\n",
      "epoch no.3 train no.294660  loss = 3.06811 avg_loss = 3.34340\n",
      "epoch no.3 train no.294670  loss = 3.02263 avg_loss = 3.34486\n",
      "epoch no.3 train no.294680  loss = 2.76932 avg_loss = 3.35330\n",
      "epoch no.3 train no.294690  loss = 2.96106 avg_loss = 3.33107\n",
      "epoch no.3 train no.294700  loss = 4.92285 avg_loss = 3.34969\n",
      "epoch no.3 train no.294710  loss = 5.04343 avg_loss = 3.33094\n",
      "epoch no.3 train no.294720  loss = 2.47191 avg_loss = 3.33849\n",
      "epoch no.3 train no.294730  loss = 3.53882 avg_loss = 3.36136\n",
      "epoch no.3 train no.294740  loss = 3.77839 avg_loss = 3.33625\n",
      "epoch no.3 train no.294750  loss = 3.62296 avg_loss = 3.35646\n",
      "epoch no.3 train no.294760  loss = 3.63778 avg_loss = 3.34634\n",
      "epoch no.3 train no.294770  loss = 4.86247 avg_loss = 3.40358\n",
      "epoch no.3 train no.294780  loss = 4.57114 avg_loss = 3.41315\n",
      "epoch no.3 train no.294790  loss = 3.39000 avg_loss = 3.41526\n",
      "epoch no.3 train no.294800  loss = 2.89818 avg_loss = 3.40040\n",
      "epoch no.3 train no.294810  loss = 4.29817 avg_loss = 3.42033\n",
      "epoch no.3 train no.294820  loss = 4.74832 avg_loss = 3.37271\n",
      "epoch no.3 train no.294830  loss = 2.95081 avg_loss = 3.40670\n",
      "epoch no.3 train no.294840  loss = 1.95246 avg_loss = 3.32886\n",
      "epoch no.3 train no.294850  loss = 2.87316 avg_loss = 3.34405\n",
      "epoch no.3 train no.294860  loss = 4.60748 avg_loss = 3.36697\n",
      "epoch no.3 train no.294870  loss = 5.65776 avg_loss = 3.42973\n",
      "epoch no.3 train no.294880  loss = 1.60356 avg_loss = 3.46106\n",
      "epoch no.3 train no.294890  loss = 3.46892 avg_loss = 3.52552\n",
      "epoch no.3 train no.294900  loss = 4.65820 avg_loss = 3.52334\n",
      "epoch no.3 train no.294910  loss = 4.07929 avg_loss = 3.56413\n",
      "epoch no.3 train no.294920  loss = 2.42712 avg_loss = 3.56576\n",
      "epoch no.3 train no.294930  loss = 4.08660 avg_loss = 3.54129\n",
      "epoch no.3 train no.294940  loss = 3.53209 avg_loss = 3.54095\n",
      "epoch no.3 train no.294950  loss = 1.68725 avg_loss = 3.50905\n",
      "epoch no.3 train no.294960  loss = 2.77231 avg_loss = 3.48794\n",
      "epoch no.3 train no.294970  loss = 2.45534 avg_loss = 3.49404\n",
      "epoch no.3 train no.294980  loss = 3.09923 avg_loss = 3.50327\n",
      "epoch no.3 train no.294990  loss = 3.27732 avg_loss = 3.50923\n",
      "epoch no.3 train no.295000  loss = 3.14035 avg_loss = 3.48986\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>', '▁음악', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.3 train no.295010  loss = 3.39124 avg_loss = 3.49052\n",
      "epoch no.3 train no.295020  loss = 3.30512 avg_loss = 3.49474\n",
      "epoch no.3 train no.295030  loss = 2.37803 avg_loss = 3.46169\n",
      "epoch no.3 train no.295040  loss = 5.23026 avg_loss = 3.52874\n",
      "epoch no.3 train no.295050  loss = 1.65305 avg_loss = 3.51089\n",
      "epoch no.3 train no.295060  loss = 2.93559 avg_loss = 3.48427\n",
      "epoch no.3 train no.295070  loss = 3.72004 avg_loss = 3.46283\n",
      "epoch no.3 train no.295080  loss = 3.94615 avg_loss = 3.47300\n",
      "epoch no.3 train no.295090  loss = 3.87261 avg_loss = 3.45459\n",
      "epoch no.3 train no.295100  loss = 3.23884 avg_loss = 3.43264\n",
      "epoch no.3 train no.295110  loss = 4.22506 avg_loss = 3.43558\n",
      "epoch no.3 train no.295120  loss = 3.99163 avg_loss = 3.44977\n",
      "epoch no.3 train no.295130  loss = 4.78974 avg_loss = 3.42631\n",
      "epoch no.3 train no.295140  loss = 4.32881 avg_loss = 3.42289\n",
      "epoch no.3 train no.295150  loss = 3.31777 avg_loss = 3.38128\n",
      "epoch no.3 train no.295160  loss = 2.72600 avg_loss = 3.39049\n",
      "epoch no.3 train no.295170  loss = 1.72023 avg_loss = 3.37953\n",
      "epoch no.3 train no.295180  loss = 1.98470 avg_loss = 3.40388\n",
      "epoch no.3 train no.295190  loss = 3.31891 avg_loss = 3.41263\n",
      "epoch no.3 train no.295200  loss = 3.27588 avg_loss = 3.38484\n",
      "epoch no.3 train no.295210  loss = 3.68435 avg_loss = 3.35547\n",
      "epoch no.3 train no.295220  loss = 3.62336 avg_loss = 3.37022\n",
      "epoch no.3 train no.295230  loss = 3.22678 avg_loss = 3.38317\n",
      "epoch no.3 train no.295240  loss = 2.85024 avg_loss = 3.38020\n",
      "epoch no.3 train no.295250  loss = 3.10740 avg_loss = 3.39690\n",
      "epoch no.3 train no.295260  loss = 5.78849 avg_loss = 3.43511\n",
      "epoch no.3 train no.295270  loss = 2.79295 avg_loss = 3.47025\n",
      "epoch no.3 train no.295280  loss = 5.15703 avg_loss = 3.57115\n",
      "epoch no.3 train no.295290  loss = 5.22695 avg_loss = 3.59755\n",
      "epoch no.3 train no.295300  loss = 4.45472 avg_loss = 3.54211\n",
      "epoch no.3 train no.295310  loss = 1.72132 avg_loss = 3.51186\n",
      "epoch no.3 train no.295320  loss = 2.33276 avg_loss = 3.55555\n",
      "epoch no.3 train no.295330  loss = 5.95511 avg_loss = 3.60108\n",
      "epoch no.3 train no.295340  loss = 2.40650 avg_loss = 3.56798\n",
      "epoch no.3 train no.295350  loss = 3.37174 avg_loss = 3.55178\n",
      "epoch no.3 train no.295360  loss = 3.46741 avg_loss = 3.57025\n",
      "epoch no.3 train no.295370  loss = 3.72914 avg_loss = 3.53402\n",
      "epoch no.3 train no.295380  loss = 4.75129 avg_loss = 3.52395\n",
      "epoch no.3 train no.295390  loss = 1.96284 avg_loss = 3.53228\n",
      "epoch no.3 train no.295400  loss = 3.55875 avg_loss = 3.48987\n",
      "epoch no.3 train no.295410  loss = 4.26226 avg_loss = 3.43260\n",
      "epoch no.3 train no.295420  loss = 4.39184 avg_loss = 3.44139\n",
      "epoch no.3 train no.295430  loss = 2.32920 avg_loss = 3.38365\n",
      "epoch no.3 train no.295440  loss = 4.72119 avg_loss = 3.40090\n",
      "epoch no.3 train no.295450  loss = 4.27254 avg_loss = 3.41705\n",
      "epoch no.3 train no.295460  loss = 1.91412 avg_loss = 3.41889\n",
      "epoch no.3 train no.295470  loss = 2.99484 avg_loss = 3.40145\n",
      "epoch no.3 train no.295480  loss = 4.09873 avg_loss = 3.41049\n",
      "epoch no.3 train no.295490  loss = 2.97329 avg_loss = 3.40595\n",
      "epoch no.3 train no.295500  loss = 4.01631 avg_loss = 3.43001\n",
      "epoch no.3 train no.295510  loss = 3.35485 avg_loss = 3.44638\n",
      "epoch no.3 train no.295520  loss = 4.53266 avg_loss = 3.47366\n",
      "epoch no.3 train no.295530  loss = 4.15252 avg_loss = 3.52636\n",
      "epoch no.3 train no.295540  loss = 3.05761 avg_loss = 3.50249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.295550  loss = 3.93872 avg_loss = 3.50366\n",
      "epoch no.3 train no.295560  loss = 3.00120 avg_loss = 3.48607\n",
      "epoch no.3 train no.295570  loss = 2.97698 avg_loss = 3.49559\n",
      "epoch no.3 train no.295580  loss = 4.46711 avg_loss = 3.46323\n",
      "epoch no.3 train no.295590  loss = 2.02987 avg_loss = 3.42407\n",
      "epoch no.3 train no.295600  loss = 3.66613 avg_loss = 3.38546\n",
      "epoch no.3 train no.295610  loss = 5.13017 avg_loss = 3.40398\n",
      "epoch no.3 train no.295620  loss = 2.98503 avg_loss = 3.38010\n",
      "epoch no.3 train no.295630  loss = 1.80251 avg_loss = 3.39196\n",
      "epoch no.3 train no.295640  loss = 3.08636 avg_loss = 3.35928\n",
      "epoch no.3 train no.295650  loss = 3.40195 avg_loss = 3.40170\n",
      "epoch no.3 train no.295660  loss = 3.08203 avg_loss = 3.35035\n",
      "epoch no.3 train no.295670  loss = 4.33965 avg_loss = 3.33177\n",
      "epoch no.3 train no.295680  loss = 3.86303 avg_loss = 3.35488\n",
      "epoch no.3 train no.295690  loss = 2.49748 avg_loss = 3.38256\n",
      "epoch no.3 train no.295700  loss = 1.88101 avg_loss = 3.44910\n",
      "epoch no.3 train no.295710  loss = 2.29235 avg_loss = 3.42095\n",
      "epoch no.3 train no.295720  loss = 2.61348 avg_loss = 3.39614\n",
      "epoch no.3 train no.295730  loss = 4.94080 avg_loss = 3.42642\n",
      "epoch no.3 train no.295740  loss = 3.86545 avg_loss = 3.42866\n",
      "epoch no.3 train no.295750  loss = 2.65281 avg_loss = 3.42762\n",
      "epoch no.3 train no.295760  loss = 3.28520 avg_loss = 3.42571\n",
      "epoch no.3 train no.295770  loss = 4.04602 avg_loss = 3.45040\n",
      "epoch no.3 train no.295780  loss = 5.95739 avg_loss = 3.52080\n",
      "epoch no.3 train no.295790  loss = 3.03379 avg_loss = 3.56590\n",
      "epoch no.3 train no.295800  loss = 4.75676 avg_loss = 3.58561\n",
      "epoch no.3 train no.295810  loss = 2.45005 avg_loss = 3.58162\n",
      "epoch no.3 train no.295820  loss = 3.98876 avg_loss = 3.57840\n",
      "epoch no.3 train no.295830  loss = 4.02946 avg_loss = 3.60727\n",
      "epoch no.3 train no.295840  loss = 2.92545 avg_loss = 3.59267\n",
      "epoch no.3 train no.295850  loss = 2.51462 avg_loss = 3.57356\n",
      "epoch no.3 train no.295860  loss = 2.29302 avg_loss = 3.56774\n",
      "epoch no.3 train no.295870  loss = 2.07789 avg_loss = 3.56478\n",
      "epoch no.3 train no.295880  loss = 3.54955 avg_loss = 3.56341\n",
      "epoch no.3 train no.295890  loss = 5.03836 avg_loss = 3.56084\n",
      "epoch no.3 train no.295900  loss = 3.76682 avg_loss = 3.56497\n",
      "epoch no.3 train no.295910  loss = 2.45345 avg_loss = 3.55867\n",
      "epoch no.3 train no.295920  loss = 3.32513 avg_loss = 3.57130\n",
      "epoch no.3 train no.295930  loss = 2.51836 avg_loss = 3.50286\n",
      "epoch no.3 train no.295940  loss = 4.04977 avg_loss = 3.50803\n",
      "epoch no.3 train no.295950  loss = 3.30713 avg_loss = 3.50380\n",
      "epoch no.3 train no.295960  loss = 4.10141 avg_loss = 3.50597\n",
      "epoch no.3 train no.295970  loss = 4.03986 avg_loss = 3.53137\n",
      "epoch no.3 train no.295980  loss = 5.55473 avg_loss = 3.52610\n",
      "epoch no.3 train no.295990  loss = 3.91062 avg_loss = 3.51666\n",
      "epoch no.3 train no.296000  loss = 2.88360 avg_loss = 3.51553\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.296010  loss = 3.24351 avg_loss = 3.48026\n",
      "epoch no.3 train no.296020  loss = 5.70433 avg_loss = 3.49533\n",
      "epoch no.3 train no.296030  loss = 3.88176 avg_loss = 3.52362\n",
      "epoch no.3 train no.296040  loss = 1.96224 avg_loss = 3.52263\n",
      "epoch no.3 train no.296050  loss = 1.87761 avg_loss = 3.50778\n",
      "epoch no.3 train no.296060  loss = 3.60773 avg_loss = 3.52169\n",
      "epoch no.3 train no.296070  loss = 2.73862 avg_loss = 3.55943\n",
      "epoch no.3 train no.296080  loss = 1.54091 avg_loss = 3.46871\n",
      "epoch no.3 train no.296090  loss = 4.89173 avg_loss = 3.48681\n",
      "epoch no.3 train no.296100  loss = 2.22221 avg_loss = 3.45960\n",
      "epoch no.3 train no.296110  loss = 4.06320 avg_loss = 3.47253\n",
      "epoch no.3 train no.296120  loss = 2.44690 avg_loss = 3.49247\n",
      "epoch no.3 train no.296130  loss = 2.64643 avg_loss = 3.57193\n",
      "epoch no.3 train no.296140  loss = 3.39731 avg_loss = 3.56016\n",
      "epoch no.3 train no.296150  loss = 2.81150 avg_loss = 3.54142\n",
      "epoch no.3 train no.296160  loss = 2.71526 avg_loss = 3.53276\n",
      "epoch no.3 train no.296170  loss = 3.11417 avg_loss = 3.51775\n",
      "epoch no.3 train no.296180  loss = 3.57482 avg_loss = 3.52473\n",
      "epoch no.3 train no.296190  loss = 3.36254 avg_loss = 3.47158\n",
      "epoch no.3 train no.296200  loss = 6.28005 avg_loss = 3.58847\n",
      "epoch no.3 train no.296210  loss = 2.94022 avg_loss = 3.58529\n",
      "epoch no.3 train no.296220  loss = 2.37182 avg_loss = 3.58100\n",
      "epoch no.3 train no.296230  loss = 4.08767 avg_loss = 3.56246\n",
      "epoch no.3 train no.296240  loss = 2.62524 avg_loss = 3.50673\n",
      "epoch no.3 train no.296250  loss = 2.93943 avg_loss = 3.47524\n",
      "epoch no.3 train no.296260  loss = 3.97110 avg_loss = 3.49788\n",
      "epoch no.3 train no.296270  loss = 4.43165 avg_loss = 3.51511\n",
      "epoch no.3 train no.296280  loss = 4.52617 avg_loss = 3.51293\n",
      "epoch no.3 train no.296290  loss = 2.64461 avg_loss = 3.48137\n",
      "epoch no.3 train no.296300  loss = 3.38278 avg_loss = 3.45991\n",
      "epoch no.3 train no.296310  loss = 4.06117 avg_loss = 3.47176\n",
      "epoch no.3 train no.296320  loss = 1.88514 avg_loss = 3.46333\n",
      "epoch no.3 train no.296330  loss = 4.92492 avg_loss = 3.50500\n",
      "epoch no.3 train no.296340  loss = 2.78185 avg_loss = 3.43931\n",
      "epoch no.3 train no.296350  loss = 2.56587 avg_loss = 3.42276\n",
      "epoch no.3 train no.296360  loss = 2.45994 avg_loss = 3.43612\n",
      "epoch no.3 train no.296370  loss = 2.83620 avg_loss = 3.37162\n",
      "epoch no.3 train no.296380  loss = 4.65394 avg_loss = 3.36119\n",
      "epoch no.3 train no.296390  loss = 4.20244 avg_loss = 3.38561\n",
      "epoch no.3 train no.296400  loss = 3.59873 avg_loss = 3.41081\n",
      "epoch no.3 train no.296410  loss = 3.56028 avg_loss = 3.48118\n",
      "epoch no.3 train no.296420  loss = 3.78052 avg_loss = 3.47803\n",
      "epoch no.3 train no.296430  loss = 2.57846 avg_loss = 3.48607\n",
      "epoch no.3 train no.296440  loss = 2.03901 avg_loss = 3.44310\n",
      "epoch no.3 train no.296450  loss = 2.60369 avg_loss = 3.43991\n",
      "epoch no.3 train no.296460  loss = 4.34470 avg_loss = 3.42397\n",
      "epoch no.3 train no.296470  loss = 3.86554 avg_loss = 3.43078\n",
      "epoch no.3 train no.296480  loss = 2.89789 avg_loss = 3.43012\n",
      "epoch no.3 train no.296490  loss = 3.12651 avg_loss = 3.41575\n",
      "epoch no.3 train no.296500  loss = 3.88216 avg_loss = 3.37201\n",
      "epoch no.3 train no.296510  loss = 3.03439 avg_loss = 3.37374\n",
      "epoch no.3 train no.296520  loss = 4.52216 avg_loss = 3.41489\n",
      "epoch no.3 train no.296530  loss = 2.97943 avg_loss = 3.36909\n",
      "epoch no.3 train no.296540  loss = 2.85661 avg_loss = 3.38249\n",
      "epoch no.3 train no.296550  loss = 3.13750 avg_loss = 3.38812\n",
      "epoch no.3 train no.296560  loss = 2.56797 avg_loss = 3.39491\n",
      "epoch no.3 train no.296570  loss = 4.63763 avg_loss = 3.47282\n",
      "epoch no.3 train no.296580  loss = 3.30163 avg_loss = 3.50647\n",
      "epoch no.3 train no.296590  loss = 2.93446 avg_loss = 3.45354\n",
      "epoch no.3 train no.296600  loss = 2.65490 avg_loss = 3.48538\n",
      "epoch no.3 train no.296610  loss = 3.34634 avg_loss = 3.50251\n",
      "epoch no.3 train no.296620  loss = 2.72323 avg_loss = 3.46207\n",
      "epoch no.3 train no.296630  loss = 4.14565 avg_loss = 3.43155\n",
      "epoch no.3 train no.296640  loss = 4.01122 avg_loss = 3.41953\n",
      "epoch no.3 train no.296650  loss = 5.84415 avg_loss = 3.46085\n",
      "epoch no.3 train no.296660  loss = 4.64897 avg_loss = 3.45143\n",
      "epoch no.3 train no.296670  loss = 2.17149 avg_loss = 3.44840\n",
      "epoch no.3 train no.296680  loss = 2.65206 avg_loss = 3.42784\n",
      "epoch no.3 train no.296690  loss = 5.43740 avg_loss = 3.46842\n",
      "epoch no.3 train no.296700  loss = 3.39192 avg_loss = 3.46098\n",
      "epoch no.3 train no.296710  loss = 2.93904 avg_loss = 3.44528\n",
      "epoch no.3 train no.296720  loss = 4.94042 avg_loss = 3.50619\n",
      "epoch no.3 train no.296730  loss = 1.76456 avg_loss = 3.50747\n",
      "epoch no.3 train no.296740  loss = 4.97166 avg_loss = 3.50914\n",
      "epoch no.3 train no.296750  loss = 2.67299 avg_loss = 3.53215\n",
      "epoch no.3 train no.296760  loss = 5.37232 avg_loss = 3.53868\n",
      "epoch no.3 train no.296770  loss = 3.17212 avg_loss = 3.50812\n",
      "epoch no.3 train no.296780  loss = 3.65073 avg_loss = 3.56778\n",
      "epoch no.3 train no.296790  loss = 4.24824 avg_loss = 3.57267\n",
      "epoch no.3 train no.296800  loss = 5.49004 avg_loss = 3.56313\n",
      "epoch no.3 train no.296810  loss = 3.73266 avg_loss = 3.55407\n",
      "epoch no.3 train no.296820  loss = 3.58395 avg_loss = 3.58765\n",
      "epoch no.3 train no.296830  loss = 3.82246 avg_loss = 3.62215\n",
      "epoch no.3 train no.296840  loss = 2.83113 avg_loss = 3.55331\n",
      "epoch no.3 train no.296850  loss = 3.89730 avg_loss = 3.51830\n",
      "epoch no.3 train no.296860  loss = 3.55224 avg_loss = 3.51756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.296870  loss = 4.39571 avg_loss = 3.49923\n",
      "epoch no.3 train no.296880  loss = 3.78266 avg_loss = 3.49926\n",
      "epoch no.3 train no.296890  loss = 3.37180 avg_loss = 3.48401\n",
      "epoch no.3 train no.296900  loss = 5.44194 avg_loss = 3.52474\n",
      "epoch no.3 train no.296910  loss = 4.07662 avg_loss = 3.51081\n",
      "epoch no.3 train no.296920  loss = 5.51040 avg_loss = 3.57416\n",
      "epoch no.3 train no.296930  loss = 2.52290 avg_loss = 3.53577\n",
      "epoch no.3 train no.296940  loss = 2.39854 avg_loss = 3.52890\n",
      "epoch no.3 train no.296950  loss = 3.30439 avg_loss = 3.53411\n",
      "epoch no.3 train no.296960  loss = 1.95696 avg_loss = 3.48760\n",
      "epoch no.3 train no.296970  loss = 4.15298 avg_loss = 3.46783\n",
      "epoch no.3 train no.296980  loss = 1.71872 avg_loss = 3.44125\n",
      "epoch no.3 train no.296990  loss = 2.58155 avg_loss = 3.39922\n",
      "epoch no.3 train no.297000  loss = 3.95837 avg_loss = 3.40890\n",
      "5\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁필요할', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.3 train no.297010  loss = 3.24137 avg_loss = 3.42873\n",
      "epoch no.3 train no.297020  loss = 3.25307 avg_loss = 3.42840\n",
      "epoch no.3 train no.297030  loss = 3.96932 avg_loss = 3.39392\n",
      "epoch no.3 train no.297040  loss = 5.23056 avg_loss = 3.35767\n",
      "epoch no.3 train no.297050  loss = 2.80108 avg_loss = 3.34576\n",
      "epoch no.3 train no.297060  loss = 3.97136 avg_loss = 3.36163\n",
      "epoch no.3 train no.297070  loss = 2.60029 avg_loss = 3.33895\n",
      "epoch no.3 train no.297080  loss = 4.03072 avg_loss = 3.37993\n",
      "epoch no.3 train no.297090  loss = 3.72529 avg_loss = 3.43805\n",
      "epoch no.3 train no.297100  loss = 5.45261 avg_loss = 3.49527\n",
      "epoch no.3 train no.297110  loss = 2.53617 avg_loss = 3.49891\n",
      "epoch no.3 train no.297120  loss = 3.62906 avg_loss = 3.48975\n",
      "epoch no.3 train no.297130  loss = 4.07569 avg_loss = 3.47803\n",
      "epoch no.3 train no.297140  loss = 2.30960 avg_loss = 3.40652\n",
      "epoch no.3 train no.297150  loss = 2.76876 avg_loss = 3.38794\n",
      "epoch no.3 train no.297160  loss = 2.46625 avg_loss = 3.34787\n",
      "epoch no.3 train no.297170  loss = 2.94964 avg_loss = 3.36681\n",
      "epoch no.3 train no.297180  loss = 3.57262 avg_loss = 3.37733\n",
      "epoch no.3 train no.297190  loss = 2.31666 avg_loss = 3.36187\n",
      "epoch no.3 train no.297200  loss = 4.44900 avg_loss = 3.39672\n",
      "epoch no.3 train no.297210  loss = 4.49176 avg_loss = 3.40780\n",
      "epoch no.3 train no.297220  loss = 3.07520 avg_loss = 3.42555\n",
      "epoch no.3 train no.297230  loss = 4.25295 avg_loss = 3.41320\n",
      "epoch no.3 train no.297240  loss = 2.41124 avg_loss = 3.38911\n",
      "epoch no.3 train no.297250  loss = 3.35516 avg_loss = 3.43281\n",
      "epoch no.3 train no.297260  loss = 3.99011 avg_loss = 3.46909\n",
      "epoch no.3 train no.297270  loss = 3.78654 avg_loss = 3.50768\n",
      "epoch no.3 train no.297280  loss = 3.42909 avg_loss = 3.52463\n",
      "epoch no.3 train no.297290  loss = 3.55831 avg_loss = 3.51958\n",
      "epoch no.3 train no.297300  loss = 3.73769 avg_loss = 3.52561\n",
      "epoch no.3 train no.297310  loss = 3.30756 avg_loss = 3.55288\n",
      "epoch no.3 train no.297320  loss = 4.36933 avg_loss = 3.54926\n",
      "epoch no.3 train no.297330  loss = 2.63308 avg_loss = 3.56782\n",
      "epoch no.3 train no.297340  loss = 2.80426 avg_loss = 3.55306\n",
      "epoch no.3 train no.297350  loss = 5.56554 avg_loss = 3.61658\n",
      "epoch no.3 train no.297360  loss = 2.49759 avg_loss = 3.60386\n",
      "epoch no.3 train no.297370  loss = 2.12734 avg_loss = 3.57207\n",
      "epoch no.3 train no.297380  loss = 4.79231 avg_loss = 3.60194\n",
      "epoch no.3 train no.297390  loss = 2.94764 avg_loss = 3.57914\n",
      "epoch no.3 train no.297400  loss = 3.90066 avg_loss = 3.56070\n",
      "epoch no.3 train no.297410  loss = 5.93324 avg_loss = 3.53928\n",
      "epoch no.3 train no.297420  loss = 3.46566 avg_loss = 3.56037\n",
      "epoch no.3 train no.297430  loss = 4.13575 avg_loss = 3.52634\n",
      "epoch no.3 train no.297440  loss = 3.74964 avg_loss = 3.49590\n",
      "epoch no.3 train no.297450  loss = 3.00457 avg_loss = 3.46635\n",
      "epoch no.3 train no.297460  loss = 4.85805 avg_loss = 3.46035\n",
      "epoch no.3 train no.297470  loss = 4.07379 avg_loss = 3.50790\n",
      "epoch no.3 train no.297480  loss = 2.54521 avg_loss = 3.51903\n",
      "epoch no.3 train no.297490  loss = 4.43284 avg_loss = 3.54324\n",
      "epoch no.3 train no.297500  loss = 5.48349 avg_loss = 3.58035\n",
      "epoch no.3 train no.297510  loss = 3.49421 avg_loss = 3.56967\n",
      "epoch no.3 train no.297520  loss = 3.12484 avg_loss = 3.56235\n",
      "epoch no.3 train no.297530  loss = 2.90701 avg_loss = 3.58360\n",
      "epoch no.3 train no.297540  loss = 5.95963 avg_loss = 3.62034\n",
      "epoch no.3 train no.297550  loss = 2.44377 avg_loss = 3.60704\n",
      "epoch no.3 train no.297560  loss = 1.46870 avg_loss = 3.56266\n",
      "epoch no.3 train no.297570  loss = 3.66417 avg_loss = 3.52966\n",
      "epoch no.3 train no.297580  loss = 5.88023 avg_loss = 3.48408\n",
      "epoch no.3 train no.297590  loss = 6.06224 avg_loss = 3.44911\n",
      "epoch no.3 train no.297600  loss = 5.26251 avg_loss = 3.49507\n",
      "epoch no.3 train no.297610  loss = 3.59526 avg_loss = 3.45593\n",
      "epoch no.3 train no.297620  loss = 4.34776 avg_loss = 3.43990\n",
      "epoch no.3 train no.297630  loss = 5.58395 avg_loss = 3.43560\n",
      "epoch no.3 train no.297640  loss = 2.53833 avg_loss = 3.43359\n",
      "epoch no.3 train no.297650  loss = 3.57599 avg_loss = 3.47324\n",
      "epoch no.3 train no.297660  loss = 3.56806 avg_loss = 3.46770\n",
      "epoch no.3 train no.297670  loss = 2.95071 avg_loss = 3.48027\n",
      "epoch no.3 train no.297680  loss = 3.84688 avg_loss = 3.48651\n",
      "epoch no.3 train no.297690  loss = 2.31957 avg_loss = 3.46029\n",
      "epoch no.3 train no.297700  loss = 3.02842 avg_loss = 3.47031\n",
      "epoch no.3 train no.297710  loss = 3.22414 avg_loss = 3.47442\n",
      "epoch no.3 train no.297720  loss = 3.27747 avg_loss = 3.42183\n",
      "epoch no.3 train no.297730  loss = 3.20223 avg_loss = 3.40736\n",
      "epoch no.3 train no.297740  loss = 3.10588 avg_loss = 3.41004\n",
      "epoch no.3 train no.297750  loss = 3.99990 avg_loss = 3.46133\n",
      "epoch no.3 train no.297760  loss = 2.71981 avg_loss = 3.47164\n",
      "epoch no.3 train no.297770  loss = 7.48731 avg_loss = 3.49319\n",
      "epoch no.3 train no.297780  loss = 2.84558 avg_loss = 3.51074\n",
      "epoch no.3 train no.297790  loss = 3.91903 avg_loss = 3.54242\n",
      "epoch no.3 train no.297800  loss = 2.37262 avg_loss = 3.49294\n",
      "epoch no.3 train no.297810  loss = 5.01164 avg_loss = 3.48391\n",
      "epoch no.3 train no.297820  loss = 2.12464 avg_loss = 3.48930\n",
      "epoch no.3 train no.297830  loss = 4.58387 avg_loss = 3.52668\n",
      "epoch no.3 train no.297840  loss = 3.59985 avg_loss = 3.54740\n",
      "epoch no.3 train no.297850  loss = 2.11063 avg_loss = 3.50503\n",
      "epoch no.3 train no.297860  loss = 3.36074 avg_loss = 3.52244\n",
      "epoch no.3 train no.297870  loss = 1.94491 avg_loss = 3.49458\n",
      "epoch no.3 train no.297880  loss = 2.84298 avg_loss = 3.48792\n",
      "epoch no.3 train no.297890  loss = 3.48413 avg_loss = 3.49292\n",
      "epoch no.3 train no.297900  loss = 4.33187 avg_loss = 3.50092\n",
      "epoch no.3 train no.297910  loss = 2.48617 avg_loss = 3.47964\n",
      "epoch no.3 train no.297920  loss = 3.15358 avg_loss = 3.45034\n",
      "epoch no.3 train no.297930  loss = 5.41503 avg_loss = 3.46252\n",
      "epoch no.3 train no.297940  loss = 3.32639 avg_loss = 3.45377\n",
      "epoch no.3 train no.297950  loss = 4.35332 avg_loss = 3.46441\n",
      "epoch no.3 train no.297960  loss = 3.22018 avg_loss = 3.50179\n",
      "epoch no.3 train no.297970  loss = 5.23779 avg_loss = 3.57151\n",
      "epoch no.3 train no.297980  loss = 5.84152 avg_loss = 3.56934\n",
      "epoch no.3 train no.297990  loss = 4.26338 avg_loss = 3.58240\n",
      "epoch no.3 train no.298000  loss = 3.44507 avg_loss = 3.56547\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '▁드라이브', '▁필요할', '때', '</s>', '▁신나는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 신나는 곡</s>\n",
      "epoch no.3 train no.298010  loss = 5.13009 avg_loss = 3.54057\n",
      "epoch no.3 train no.298020  loss = 3.64750 avg_loss = 3.52985\n",
      "epoch no.3 train no.298030  loss = 2.83394 avg_loss = 3.53346\n",
      "epoch no.3 train no.298040  loss = 4.03831 avg_loss = 3.52913\n",
      "epoch no.3 train no.298050  loss = 3.21412 avg_loss = 3.49602\n",
      "epoch no.3 train no.298060  loss = 3.16517 avg_loss = 3.46125\n",
      "epoch no.3 train no.298070  loss = 3.03149 avg_loss = 3.43641\n",
      "epoch no.3 train no.298080  loss = 4.66475 avg_loss = 3.46801\n",
      "epoch no.3 train no.298090  loss = 4.71468 avg_loss = 3.48029\n",
      "epoch no.3 train no.298100  loss = 3.33249 avg_loss = 3.42403\n",
      "epoch no.3 train no.298110  loss = 2.73494 avg_loss = 3.45385\n",
      "epoch no.3 train no.298120  loss = 4.55279 avg_loss = 3.50461\n",
      "epoch no.3 train no.298130  loss = 3.77493 avg_loss = 3.52050\n",
      "epoch no.3 train no.298140  loss = 1.67870 avg_loss = 3.51963\n",
      "epoch no.3 train no.298150  loss = 4.49823 avg_loss = 3.45613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.298160  loss = 3.50856 avg_loss = 3.43412\n",
      "epoch no.3 train no.298170  loss = 5.34513 avg_loss = 3.45553\n",
      "epoch no.3 train no.298180  loss = 2.57813 avg_loss = 3.47910\n",
      "epoch no.3 train no.298190  loss = 4.16195 avg_loss = 3.52076\n",
      "epoch no.3 train no.298200  loss = 3.45516 avg_loss = 3.52855\n",
      "epoch no.3 train no.298210  loss = 4.95323 avg_loss = 3.55077\n",
      "epoch no.3 train no.298220  loss = 4.31471 avg_loss = 3.55477\n",
      "epoch no.3 train no.298230  loss = 4.22319 avg_loss = 3.52715\n",
      "epoch no.3 train no.298240  loss = 2.59879 avg_loss = 3.52055\n",
      "epoch no.3 train no.298250  loss = 2.38392 avg_loss = 3.48471\n",
      "epoch no.3 train no.298260  loss = 2.09972 avg_loss = 3.48459\n",
      "epoch no.3 train no.298270  loss = 3.03437 avg_loss = 3.49648\n",
      "epoch no.3 train no.298280  loss = 3.69458 avg_loss = 3.52796\n",
      "epoch no.3 train no.298290  loss = 2.24772 avg_loss = 3.52330\n",
      "epoch no.3 train no.298300  loss = 5.80299 avg_loss = 3.48788\n",
      "epoch no.3 train no.298310  loss = 4.82173 avg_loss = 3.52391\n",
      "epoch no.3 train no.298320  loss = 2.12687 avg_loss = 3.53399\n",
      "epoch no.3 train no.298330  loss = 3.42660 avg_loss = 3.50628\n",
      "epoch no.3 train no.298340  loss = 3.17728 avg_loss = 3.46424\n",
      "epoch no.3 train no.298350  loss = 3.01569 avg_loss = 3.48146\n",
      "epoch no.3 train no.298360  loss = 3.74945 avg_loss = 3.47212\n",
      "epoch no.3 train no.298370  loss = 2.98416 avg_loss = 3.46724\n",
      "epoch no.3 train no.298380  loss = 3.61701 avg_loss = 3.48215\n",
      "epoch no.3 train no.298390  loss = 5.43094 avg_loss = 3.49701\n",
      "epoch no.3 train no.298400  loss = 2.89783 avg_loss = 3.45307\n",
      "epoch no.3 train no.298410  loss = 3.02954 avg_loss = 3.49242\n",
      "epoch no.3 train no.298420  loss = 5.04797 avg_loss = 3.52958\n",
      "epoch no.3 train no.298430  loss = 2.13197 avg_loss = 3.55406\n",
      "epoch no.3 train no.298440  loss = 2.49877 avg_loss = 3.56952\n",
      "epoch no.3 train no.298450  loss = 3.57696 avg_loss = 3.56931\n",
      "epoch no.3 train no.298460  loss = 2.16493 avg_loss = 3.49982\n",
      "epoch no.3 train no.298470  loss = 2.46900 avg_loss = 3.49600\n",
      "epoch no.3 train no.298480  loss = 3.54838 avg_loss = 3.47429\n",
      "epoch no.3 train no.298490  loss = 5.06158 avg_loss = 3.50906\n",
      "epoch no.3 train no.298500  loss = 2.70702 avg_loss = 3.49831\n",
      "epoch no.3 train no.298510  loss = 4.69288 avg_loss = 3.48537\n",
      "epoch no.3 train no.298520  loss = 2.67796 avg_loss = 3.49902\n",
      "epoch no.3 train no.298530  loss = 2.63071 avg_loss = 3.50208\n",
      "epoch no.3 train no.298540  loss = 5.21623 avg_loss = 3.51293\n",
      "epoch no.3 train no.298550  loss = 2.33745 avg_loss = 3.48058\n",
      "epoch no.3 train no.298560  loss = 1.92345 avg_loss = 3.46490\n",
      "epoch no.3 train no.298570  loss = 2.37799 avg_loss = 3.49403\n",
      "epoch no.3 train no.298580  loss = 2.31797 avg_loss = 3.46613\n",
      "epoch no.3 train no.298590  loss = 3.80582 avg_loss = 3.44963\n",
      "epoch no.3 train no.298600  loss = 3.83036 avg_loss = 3.42571\n",
      "epoch no.3 train no.298610  loss = 6.20589 avg_loss = 3.45923\n",
      "epoch no.3 train no.298620  loss = 3.58911 avg_loss = 3.50404\n",
      "epoch no.3 train no.298630  loss = 2.79635 avg_loss = 3.47363\n",
      "epoch no.3 train no.298640  loss = 4.07811 avg_loss = 3.52649\n",
      "epoch no.3 train no.298650  loss = 3.78694 avg_loss = 3.47662\n",
      "epoch no.3 train no.298660  loss = 3.62292 avg_loss = 3.45035\n",
      "epoch no.3 train no.298670  loss = 4.47312 avg_loss = 3.45318\n",
      "epoch no.3 train no.298680  loss = 3.02596 avg_loss = 3.43652\n",
      "epoch no.3 train no.298690  loss = 4.36934 avg_loss = 3.43331\n",
      "epoch no.3 train no.298700  loss = 3.25803 avg_loss = 3.41583\n",
      "epoch no.3 train no.298710  loss = 2.70699 avg_loss = 3.40614\n",
      "epoch no.3 train no.298720  loss = 3.11893 avg_loss = 3.38712\n",
      "epoch no.3 train no.298730  loss = 6.28141 avg_loss = 3.48849\n",
      "epoch no.3 train no.298740  loss = 2.22414 avg_loss = 3.48371\n",
      "epoch no.3 train no.298750  loss = 2.24767 avg_loss = 3.46466\n",
      "epoch no.3 train no.298760  loss = 2.83484 avg_loss = 3.47520\n",
      "epoch no.3 train no.298770  loss = 3.45111 avg_loss = 3.47402\n",
      "epoch no.3 train no.298780  loss = 2.24050 avg_loss = 3.47441\n",
      "epoch no.3 train no.298790  loss = 2.64982 avg_loss = 3.49702\n",
      "epoch no.3 train no.298800  loss = 2.75113 avg_loss = 3.54066\n",
      "epoch no.3 train no.298810  loss = 2.99583 avg_loss = 3.52841\n",
      "epoch no.3 train no.298820  loss = 4.63901 avg_loss = 3.54271\n",
      "epoch no.3 train no.298830  loss = 3.46312 avg_loss = 3.56687\n",
      "epoch no.3 train no.298840  loss = 2.67843 avg_loss = 3.55400\n",
      "epoch no.3 train no.298850  loss = 2.89058 avg_loss = 3.56522\n",
      "epoch no.3 train no.298860  loss = 2.30432 avg_loss = 3.54818\n",
      "epoch no.3 train no.298870  loss = 3.80863 avg_loss = 3.50237\n",
      "epoch no.3 train no.298880  loss = 2.08975 avg_loss = 3.49357\n",
      "epoch no.3 train no.298890  loss = 2.87540 avg_loss = 3.49454\n",
      "epoch no.3 train no.298900  loss = 3.26823 avg_loss = 3.53883\n",
      "epoch no.3 train no.298910  loss = 4.10424 avg_loss = 3.51802\n",
      "epoch no.3 train no.298920  loss = 3.03619 avg_loss = 3.55615\n",
      "epoch no.3 train no.298930  loss = 4.40856 avg_loss = 3.53711\n",
      "epoch no.3 train no.298940  loss = 3.24410 avg_loss = 3.50172\n",
      "epoch no.3 train no.298950  loss = 4.53088 avg_loss = 3.53384\n",
      "epoch no.3 train no.298960  loss = 3.82637 avg_loss = 3.54609\n",
      "epoch no.3 train no.298970  loss = 2.68090 avg_loss = 3.52882\n",
      "epoch no.3 train no.298980  loss = 2.24406 avg_loss = 3.53727\n",
      "epoch no.3 train no.298990  loss = 3.72785 avg_loss = 3.52405\n",
      "epoch no.3 train no.299000  loss = 3.65430 avg_loss = 3.53822\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '▁때', '▁듣는']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.3 train no.299010  loss = 2.39921 avg_loss = 3.50873\n",
      "epoch no.3 train no.299020  loss = 3.69392 avg_loss = 3.52262\n",
      "epoch no.3 train no.299030  loss = 3.54243 avg_loss = 3.47408\n",
      "epoch no.3 train no.299040  loss = 3.04334 avg_loss = 3.46747\n",
      "epoch no.3 train no.299050  loss = 3.86585 avg_loss = 3.47559\n",
      "epoch no.3 train no.299060  loss = 6.27358 avg_loss = 3.52687\n",
      "epoch no.3 train no.299070  loss = 4.15165 avg_loss = 3.55642\n",
      "epoch no.3 train no.299080  loss = 2.96182 avg_loss = 3.49922\n",
      "epoch no.3 train no.299090  loss = 2.51674 avg_loss = 3.52464\n",
      "epoch no.3 train no.299100  loss = 4.99180 avg_loss = 3.59386\n",
      "epoch no.3 train no.299110  loss = 3.21953 avg_loss = 3.61481\n",
      "epoch no.3 train no.299120  loss = 3.11062 avg_loss = 3.52351\n",
      "epoch no.3 train no.299130  loss = 2.54736 avg_loss = 3.53526\n",
      "epoch no.3 train no.299140  loss = 2.61037 avg_loss = 3.56767\n",
      "epoch no.3 train no.299150  loss = 2.90055 avg_loss = 3.52529\n",
      "epoch no.3 train no.299160  loss = 3.58375 avg_loss = 3.54752\n",
      "epoch no.3 train no.299170  loss = 3.34366 avg_loss = 3.54279\n",
      "epoch no.3 train no.299180  loss = 3.55960 avg_loss = 3.54104\n",
      "epoch no.3 train no.299190  loss = 5.39139 avg_loss = 3.58957\n",
      "epoch no.3 train no.299200  loss = 3.38597 avg_loss = 3.59067\n",
      "epoch no.3 train no.299210  loss = 2.67292 avg_loss = 3.57139\n",
      "epoch no.3 train no.299220  loss = 4.29241 avg_loss = 3.57179\n",
      "epoch no.3 train no.299230  loss = 2.94703 avg_loss = 3.53723\n",
      "epoch no.3 train no.299240  loss = 4.48194 avg_loss = 3.56484\n",
      "epoch no.3 train no.299250  loss = 2.98124 avg_loss = 3.60442\n",
      "epoch no.3 train no.299260  loss = 3.50556 avg_loss = 3.56512\n",
      "epoch no.3 train no.299270  loss = 3.32754 avg_loss = 3.53898\n",
      "epoch no.3 train no.299280  loss = 2.10303 avg_loss = 3.51062\n",
      "epoch no.3 train no.299290  loss = 4.63699 avg_loss = 3.52206\n",
      "epoch no.3 train no.299300  loss = 5.07622 avg_loss = 3.50045\n",
      "epoch no.3 train no.299310  loss = 2.44899 avg_loss = 3.47404\n",
      "epoch no.3 train no.299320  loss = 2.21618 avg_loss = 3.44268\n",
      "epoch no.3 train no.299330  loss = 5.63679 avg_loss = 3.43623\n",
      "epoch no.3 train no.299340  loss = 3.31901 avg_loss = 3.41451\n",
      "epoch no.3 train no.299350  loss = 4.67260 avg_loss = 3.48309\n",
      "epoch no.3 train no.299360  loss = 4.52757 avg_loss = 3.47358\n",
      "epoch no.3 train no.299370  loss = 3.43806 avg_loss = 3.43392\n",
      "epoch no.3 train no.299380  loss = 1.31839 avg_loss = 3.40776\n",
      "epoch no.3 train no.299390  loss = 4.99455 avg_loss = 3.41822\n",
      "epoch no.3 train no.299400  loss = 3.24507 avg_loss = 3.44610\n",
      "epoch no.3 train no.299410  loss = 3.89753 avg_loss = 3.44002\n",
      "epoch no.3 train no.299420  loss = 2.91552 avg_loss = 3.45133\n",
      "epoch no.3 train no.299430  loss = 3.19653 avg_loss = 3.43062\n",
      "epoch no.3 train no.299440  loss = 4.95122 avg_loss = 3.45924\n",
      "epoch no.3 train no.299450  loss = 3.87679 avg_loss = 3.46992\n",
      "epoch no.3 train no.299460  loss = 3.43548 avg_loss = 3.49391\n",
      "epoch no.3 train no.299470  loss = 2.80952 avg_loss = 3.53282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.299480  loss = 6.15966 avg_loss = 3.57377\n",
      "epoch no.3 train no.299490  loss = 4.49754 avg_loss = 3.59576\n",
      "epoch no.3 train no.299500  loss = 4.19656 avg_loss = 3.63263\n",
      "epoch no.3 train no.299510  loss = 4.66762 avg_loss = 3.68011\n",
      "epoch no.3 train no.299520  loss = 3.78364 avg_loss = 3.66928\n",
      "epoch no.3 train no.299530  loss = 3.46141 avg_loss = 3.62686\n",
      "epoch no.3 train no.299540  loss = 3.10799 avg_loss = 3.62059\n",
      "epoch no.3 train no.299550  loss = 3.67839 avg_loss = 3.60822\n",
      "epoch no.3 train no.299560  loss = 3.46521 avg_loss = 3.57762\n",
      "epoch no.3 train no.299570  loss = 1.74698 avg_loss = 3.54131\n",
      "epoch no.3 train no.299580  loss = 4.39754 avg_loss = 3.57914\n",
      "epoch no.3 train no.299590  loss = 3.83241 avg_loss = 3.61504\n",
      "epoch no.3 train no.299600  loss = 4.92860 avg_loss = 3.61070\n",
      "epoch no.3 train no.299610  loss = 3.33457 avg_loss = 3.58318\n",
      "epoch no.3 train no.299620  loss = 1.77729 avg_loss = 3.63347\n",
      "epoch no.3 train no.299630  loss = 5.61282 avg_loss = 3.65418\n",
      "epoch no.3 train no.299640  loss = 4.63165 avg_loss = 3.65353\n",
      "epoch no.3 train no.299650  loss = 2.69826 avg_loss = 3.62631\n",
      "epoch no.3 train no.299660  loss = 2.79039 avg_loss = 3.58071\n",
      "epoch no.3 train no.299670  loss = 4.07724 avg_loss = 3.64842\n",
      "epoch no.3 train no.299680  loss = 2.68702 avg_loss = 3.60965\n",
      "epoch no.3 train no.299690  loss = 4.23599 avg_loss = 3.62023\n",
      "epoch no.3 train no.299700  loss = 3.74566 avg_loss = 3.63591\n",
      "epoch no.3 train no.299710  loss = 3.49533 avg_loss = 3.62631\n",
      "epoch no.3 train no.299720  loss = 1.99511 avg_loss = 3.61774\n",
      "epoch no.3 train no.299730  loss = 4.13203 avg_loss = 3.56081\n",
      "epoch no.3 train no.299740  loss = 3.48926 avg_loss = 3.53803\n",
      "epoch no.3 train no.299750  loss = 4.55856 avg_loss = 3.53980\n",
      "epoch no.3 train no.299760  loss = 3.70769 avg_loss = 3.53699\n",
      "epoch no.3 train no.299770  loss = 2.20815 avg_loss = 3.54170\n",
      "epoch no.3 train no.299780  loss = 2.89206 avg_loss = 3.53832\n",
      "epoch no.3 train no.299790  loss = 4.12126 avg_loss = 3.53249\n",
      "epoch no.3 train no.299800  loss = 3.17340 avg_loss = 3.55417\n",
      "epoch no.3 train no.299810  loss = 4.36840 avg_loss = 3.56778\n",
      "epoch no.3 train no.299820  loss = 3.04049 avg_loss = 3.55133\n",
      "epoch no.3 train no.299830  loss = 3.50563 avg_loss = 3.56112\n",
      "epoch no.3 train no.299840  loss = 3.76485 avg_loss = 3.59507\n",
      "epoch no.3 train no.299850  loss = 3.25450 avg_loss = 3.59045\n",
      "epoch no.3 train no.299860  loss = 3.76401 avg_loss = 3.55174\n",
      "epoch no.3 train no.299870  loss = 5.35029 avg_loss = 3.60797\n",
      "epoch no.3 train no.299880  loss = 3.40707 avg_loss = 3.60078\n",
      "epoch no.3 train no.299890  loss = 4.91263 avg_loss = 3.58902\n",
      "epoch no.3 train no.299900  loss = 3.42870 avg_loss = 3.57260\n",
      "epoch no.3 train no.299910  loss = 2.92843 avg_loss = 3.55523\n",
      "epoch no.3 train no.299920  loss = 2.61133 avg_loss = 3.52473\n",
      "epoch no.3 train no.299930  loss = 4.26576 avg_loss = 3.52349\n",
      "epoch no.3 train no.299940  loss = 3.74990 avg_loss = 3.54409\n",
      "epoch no.3 train no.299950  loss = 3.38900 avg_loss = 3.51848\n",
      "epoch no.3 train no.299960  loss = 2.42677 avg_loss = 3.55128\n",
      "epoch no.3 train no.299970  loss = 3.35871 avg_loss = 3.53355\n",
      "epoch no.3 train no.299980  loss = 4.07821 avg_loss = 3.50489\n",
      "epoch no.3 train no.299990  loss = 2.82945 avg_loss = 3.51405\n",
      "epoch no.3 train no.300000  loss = 3.09887 avg_loss = 3.50321\n",
      "1\n",
      "to_tokens: ['▁라디오', '좋은', '이', '</s>']\n",
      "기분전환 드라이브</s>\n",
      "epoch no.3 train no.300010  loss = 6.19056 avg_loss = 3.55707\n",
      "epoch no.3 train no.300020  loss = 2.21164 avg_loss = 3.54608\n",
      "epoch no.3 train no.300030  loss = 4.59789 avg_loss = 3.56744\n",
      "epoch no.3 train no.300040  loss = 2.92507 avg_loss = 3.57845\n",
      "epoch no.3 train no.300050  loss = 3.28412 avg_loss = 3.56269\n",
      "epoch no.3 train no.300060  loss = 3.64049 avg_loss = 3.53484\n",
      "epoch no.3 train no.300070  loss = 3.75304 avg_loss = 3.52464\n",
      "epoch no.3 train no.300080  loss = 3.42159 avg_loss = 3.51346\n",
      "epoch no.3 train no.300090  loss = 3.43011 avg_loss = 3.51478\n",
      "epoch no.3 train no.300100  loss = 3.83268 avg_loss = 3.53863\n",
      "epoch no.3 train no.300110  loss = 3.84971 avg_loss = 3.55074\n",
      "epoch no.3 train no.300120  loss = 3.69476 avg_loss = 3.51774\n",
      "epoch no.3 train no.300130  loss = 3.43929 avg_loss = 3.55206\n",
      "epoch no.3 train no.300140  loss = 3.44412 avg_loss = 3.52985\n",
      "epoch no.3 train no.300150  loss = 3.04310 avg_loss = 3.49902\n",
      "epoch no.3 train no.300160  loss = 2.49752 avg_loss = 3.51440\n",
      "epoch no.3 train no.300170  loss = 3.45500 avg_loss = 3.50366\n",
      "epoch no.3 train no.300180  loss = 3.34532 avg_loss = 3.52359\n",
      "epoch no.3 train no.300190  loss = 4.90556 avg_loss = 3.61408\n",
      "epoch no.3 train no.300200  loss = 4.19781 avg_loss = 3.57986\n",
      "epoch no.3 train no.300210  loss = 4.57856 avg_loss = 3.56314\n",
      "epoch no.3 train no.300220  loss = 2.26012 avg_loss = 3.54883\n",
      "epoch no.3 train no.300230  loss = 3.14534 avg_loss = 3.50065\n",
      "epoch no.3 train no.300240  loss = 4.28642 avg_loss = 3.53585\n",
      "epoch no.3 train no.300250  loss = 3.65622 avg_loss = 3.54632\n",
      "epoch no.3 train no.300260  loss = 2.01626 avg_loss = 3.52506\n",
      "epoch no.3 train no.300270  loss = 3.85403 avg_loss = 3.52642\n",
      "epoch no.3 train no.300280  loss = 2.48975 avg_loss = 3.52207\n",
      "epoch no.3 train no.300290  loss = 4.31837 avg_loss = 3.56262\n",
      "epoch no.3 train no.300300  loss = 2.66641 avg_loss = 3.56655\n",
      "epoch no.3 train no.300310  loss = 2.81770 avg_loss = 3.53524\n",
      "epoch no.3 train no.300320  loss = 3.63309 avg_loss = 3.50295\n",
      "epoch no.3 train no.300330  loss = 4.71972 avg_loss = 3.53575\n",
      "epoch no.3 train no.300340  loss = 4.17933 avg_loss = 3.56082\n",
      "epoch no.3 train no.300350  loss = 4.53702 avg_loss = 3.57936\n",
      "epoch no.3 train no.300360  loss = 3.01391 avg_loss = 3.54776\n",
      "epoch no.3 train no.300370  loss = 3.76668 avg_loss = 3.58628\n",
      "epoch no.3 train no.300380  loss = 3.31558 avg_loss = 3.57335\n",
      "epoch no.3 train no.300390  loss = 2.66188 avg_loss = 3.58209\n",
      "epoch no.3 train no.300400  loss = 3.54449 avg_loss = 3.53640\n",
      "epoch no.3 train no.300410  loss = 2.04759 avg_loss = 3.52945\n",
      "epoch no.3 train no.300420  loss = 5.04400 avg_loss = 3.53817\n",
      "epoch no.3 train no.300430  loss = 2.97675 avg_loss = 3.49939\n",
      "epoch no.3 train no.300440  loss = 3.66170 avg_loss = 3.45588\n",
      "epoch no.3 train no.300450  loss = 3.39932 avg_loss = 3.44668\n",
      "epoch no.3 train no.300460  loss = 3.74473 avg_loss = 3.41517\n",
      "epoch no.3 train no.300470  loss = 3.45624 avg_loss = 3.39684\n",
      "epoch no.3 train no.300480  loss = 2.06880 avg_loss = 3.36981\n",
      "epoch no.3 train no.300490  loss = 3.59228 avg_loss = 3.39200\n",
      "epoch no.3 train no.300500  loss = 4.82832 avg_loss = 3.43968\n",
      "epoch no.3 train no.300510  loss = 3.36531 avg_loss = 3.43772\n",
      "epoch no.3 train no.300520  loss = 2.42322 avg_loss = 3.44928\n",
      "epoch no.3 train no.300530  loss = 2.95820 avg_loss = 3.43136\n",
      "epoch no.3 train no.300540  loss = 0.99738 avg_loss = 3.42416\n",
      "epoch no.3 train no.300550  loss = 2.61473 avg_loss = 3.41453\n",
      "epoch no.3 train no.300560  loss = 4.32865 avg_loss = 3.40481\n",
      "epoch no.3 train no.300570  loss = 4.66131 avg_loss = 3.38361\n",
      "epoch no.3 train no.300580  loss = 2.49823 avg_loss = 3.37684\n",
      "epoch no.3 train no.300590  loss = 4.91290 avg_loss = 3.41190\n",
      "epoch no.3 train no.300600  loss = 2.59590 avg_loss = 3.40886\n",
      "epoch no.3 train no.300610  loss = 5.65598 avg_loss = 3.43425\n",
      "epoch no.3 train no.300620  loss = 2.61736 avg_loss = 3.42570\n",
      "epoch no.3 train no.300630  loss = 3.04846 avg_loss = 3.39159\n",
      "epoch no.3 train no.300640  loss = 1.64657 avg_loss = 3.35883\n",
      "epoch no.3 train no.300650  loss = 2.93432 avg_loss = 3.38644\n",
      "epoch no.3 train no.300660  loss = 2.92391 avg_loss = 3.39118\n",
      "epoch no.3 train no.300670  loss = 1.96107 avg_loss = 3.36499\n",
      "epoch no.3 train no.300680  loss = 3.42699 avg_loss = 3.27290\n",
      "epoch no.3 train no.300690  loss = 5.18279 avg_loss = 3.27800\n",
      "epoch no.3 train no.300700  loss = 3.53792 avg_loss = 3.31552\n",
      "epoch no.3 train no.300710  loss = 4.58993 avg_loss = 3.36414\n",
      "epoch no.3 train no.300720  loss = 3.10674 avg_loss = 3.42480\n",
      "epoch no.3 train no.300730  loss = 4.05273 avg_loss = 3.43567\n",
      "epoch no.3 train no.300740  loss = 3.97265 avg_loss = 3.39933\n",
      "epoch no.3 train no.300750  loss = 3.08178 avg_loss = 3.37720\n",
      "epoch no.3 train no.300760  loss = 3.64818 avg_loss = 3.38301\n",
      "epoch no.3 train no.300770  loss = 4.57960 avg_loss = 3.38086\n",
      "epoch no.3 train no.300780  loss = 4.02413 avg_loss = 3.42781\n",
      "epoch no.3 train no.300790  loss = 3.82020 avg_loss = 3.42415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.300800  loss = 3.32511 avg_loss = 3.41935\n",
      "epoch no.3 train no.300810  loss = 2.05421 avg_loss = 3.45215\n",
      "epoch no.3 train no.300820  loss = 1.98009 avg_loss = 3.43313\n",
      "epoch no.3 train no.300830  loss = 2.00960 avg_loss = 3.40354\n",
      "epoch no.3 train no.300840  loss = 4.03551 avg_loss = 3.41492\n",
      "epoch no.3 train no.300850  loss = 2.58098 avg_loss = 3.38007\n",
      "epoch no.3 train no.300860  loss = 2.90528 avg_loss = 3.35494\n",
      "epoch no.3 train no.300870  loss = 1.87209 avg_loss = 3.32030\n",
      "epoch no.3 train no.300880  loss = 2.76318 avg_loss = 3.32925\n",
      "epoch no.3 train no.300890  loss = 3.08924 avg_loss = 3.36351\n",
      "epoch no.3 train no.300900  loss = 2.33431 avg_loss = 3.33145\n",
      "epoch no.3 train no.300910  loss = 3.24672 avg_loss = 3.33545\n",
      "epoch no.3 train no.300920  loss = 3.86773 avg_loss = 3.32400\n",
      "epoch no.3 train no.300930  loss = 2.53199 avg_loss = 3.35744\n",
      "epoch no.3 train no.300940  loss = 4.08474 avg_loss = 3.43570\n",
      "epoch no.3 train no.300950  loss = 1.84299 avg_loss = 3.39206\n",
      "epoch no.3 train no.300960  loss = 4.70234 avg_loss = 3.41118\n",
      "epoch no.3 train no.300970  loss = 3.15282 avg_loss = 3.40284\n",
      "epoch no.3 train no.300980  loss = 2.37636 avg_loss = 3.36649\n",
      "epoch no.3 train no.300990  loss = 4.14059 avg_loss = 3.35838\n",
      "epoch no.3 train no.301000  loss = 2.14027 avg_loss = 3.34543\n",
      "5\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁위한', '▁신나는', '▁팝', '송', '</s>']\n",
      "기분전환을 위한 신나는 팝송</s>\n",
      "epoch no.3 train no.301010  loss = 3.96316 avg_loss = 3.39985\n",
      "epoch no.3 train no.301020  loss = 3.58596 avg_loss = 3.40924\n",
      "epoch no.3 train no.301030  loss = 2.22859 avg_loss = 3.36643\n",
      "epoch no.3 train no.301040  loss = 3.40834 avg_loss = 3.36297\n",
      "epoch no.3 train no.301050  loss = 2.35602 avg_loss = 3.32255\n",
      "epoch no.3 train no.301060  loss = 4.63859 avg_loss = 3.29439\n",
      "epoch no.3 train no.301070  loss = 2.04022 avg_loss = 3.30560\n",
      "epoch no.3 train no.301080  loss = 3.27247 avg_loss = 3.30576\n",
      "epoch no.3 train no.301090  loss = 1.91709 avg_loss = 3.31551\n",
      "epoch no.3 train no.301100  loss = 4.12790 avg_loss = 3.31830\n",
      "epoch no.3 train no.301110  loss = 4.20436 avg_loss = 3.32904\n",
      "epoch no.3 train no.301120  loss = 2.56353 avg_loss = 3.33790\n",
      "epoch no.3 train no.301130  loss = 3.12222 avg_loss = 3.40104\n",
      "epoch no.3 train no.301140  loss = 5.28431 avg_loss = 3.41833\n",
      "epoch no.3 train no.301150  loss = 5.01932 avg_loss = 3.48735\n",
      "epoch no.3 train no.301160  loss = 5.10346 avg_loss = 3.47879\n",
      "epoch no.3 train no.301170  loss = 4.55891 avg_loss = 3.45774\n",
      "epoch no.3 train no.301180  loss = 3.04470 avg_loss = 3.46915\n",
      "epoch no.3 train no.301190  loss = 2.99323 avg_loss = 3.44558\n",
      "epoch no.3 train no.301200  loss = 4.41792 avg_loss = 3.45081\n",
      "epoch no.3 train no.301210  loss = 4.62758 avg_loss = 3.47113\n",
      "epoch no.3 train no.301220  loss = 3.89481 avg_loss = 3.47126\n",
      "epoch no.3 train no.301230  loss = 4.93303 avg_loss = 3.41183\n",
      "epoch no.3 train no.301240  loss = 2.97114 avg_loss = 3.41416\n",
      "epoch no.3 train no.301250  loss = 3.45218 avg_loss = 3.42590\n",
      "epoch no.3 train no.301260  loss = 3.03873 avg_loss = 3.44402\n",
      "epoch no.3 train no.301270  loss = 4.83550 avg_loss = 3.41044\n",
      "epoch no.3 train no.301280  loss = 4.43542 avg_loss = 3.45655\n",
      "epoch no.3 train no.301290  loss = 2.87772 avg_loss = 3.43196\n",
      "epoch no.3 train no.301300  loss = 3.80157 avg_loss = 3.42130\n",
      "epoch no.3 train no.301310  loss = 3.06212 avg_loss = 3.44928\n",
      "epoch no.3 train no.301320  loss = 3.60229 avg_loss = 3.48416\n",
      "epoch no.3 train no.301330  loss = 3.76443 avg_loss = 3.47463\n",
      "epoch no.3 train no.301340  loss = 1.66505 avg_loss = 3.46754\n",
      "epoch no.3 train no.301350  loss = 2.92869 avg_loss = 3.45770\n",
      "epoch no.3 train no.301360  loss = 4.39467 avg_loss = 3.46464\n",
      "epoch no.3 train no.301370  loss = 3.55830 avg_loss = 3.39901\n",
      "epoch no.3 train no.301380  loss = 3.54376 avg_loss = 3.39827\n",
      "epoch no.3 train no.301390  loss = 2.09360 avg_loss = 3.37718\n",
      "epoch no.3 train no.301400  loss = 4.06773 avg_loss = 3.40218\n",
      "epoch no.3 train no.301410  loss = 3.34450 avg_loss = 3.35552\n",
      "epoch no.3 train no.301420  loss = 4.17285 avg_loss = 3.36959\n",
      "epoch no.3 train no.301430  loss = 4.36192 avg_loss = 3.40153\n",
      "epoch no.3 train no.301440  loss = 3.39897 avg_loss = 3.40330\n",
      "epoch no.3 train no.301450  loss = 4.99683 avg_loss = 3.36223\n",
      "epoch no.3 train no.301460  loss = 5.14990 avg_loss = 3.39023\n",
      "epoch no.3 train no.301470  loss = 2.59863 avg_loss = 3.39962\n",
      "epoch no.3 train no.301480  loss = 3.85413 avg_loss = 3.38846\n",
      "epoch no.3 train no.301490  loss = 3.60194 avg_loss = 3.39042\n",
      "epoch no.3 train no.301500  loss = 2.35519 avg_loss = 3.41328\n",
      "epoch no.3 train no.301510  loss = 2.78381 avg_loss = 3.41441\n",
      "epoch no.3 train no.301520  loss = 4.85003 avg_loss = 3.40855\n",
      "epoch no.3 train no.301530  loss = 3.05780 avg_loss = 3.41011\n",
      "epoch no.3 train no.301540  loss = 2.25438 avg_loss = 3.40801\n",
      "epoch no.3 train no.301550  loss = 4.13497 avg_loss = 3.41222\n",
      "epoch no.3 train no.301560  loss = 2.84279 avg_loss = 3.40548\n",
      "epoch no.3 train no.301570  loss = 2.05592 avg_loss = 3.37657\n",
      "epoch no.3 train no.301580  loss = 3.38445 avg_loss = 3.36991\n",
      "epoch no.3 train no.301590  loss = 3.80506 avg_loss = 3.32780\n",
      "epoch no.3 train no.301600  loss = 1.52662 avg_loss = 3.32549\n",
      "epoch no.3 train no.301610  loss = 2.98941 avg_loss = 3.36085\n",
      "epoch no.3 train no.301620  loss = 3.71397 avg_loss = 3.37862\n",
      "epoch no.3 train no.301630  loss = 3.27991 avg_loss = 3.40751\n",
      "epoch no.3 train no.301640  loss = 4.04305 avg_loss = 3.46758\n",
      "epoch no.3 train no.301650  loss = 3.30187 avg_loss = 3.45222\n",
      "epoch no.3 train no.301660  loss = 4.24390 avg_loss = 3.45887\n",
      "epoch no.3 train no.301670  loss = 3.37578 avg_loss = 3.43098\n",
      "epoch no.3 train no.301680  loss = 2.35871 avg_loss = 3.44126\n",
      "epoch no.3 train no.301690  loss = 2.34521 avg_loss = 3.43436\n",
      "epoch no.3 train no.301700  loss = 3.71502 avg_loss = 3.41830\n",
      "epoch no.3 train no.301710  loss = 2.48968 avg_loss = 3.42333\n",
      "epoch no.3 train no.301720  loss = 3.01170 avg_loss = 3.41460\n",
      "epoch no.3 train no.301730  loss = 4.28778 avg_loss = 3.39697\n",
      "epoch no.3 train no.301740  loss = 4.63214 avg_loss = 3.40694\n",
      "epoch no.3 train no.301750  loss = 2.17283 avg_loss = 3.39447\n",
      "epoch no.3 train no.301760  loss = 3.34584 avg_loss = 3.38202\n",
      "epoch no.3 train no.301770  loss = 3.38500 avg_loss = 3.41094\n",
      "epoch no.3 train no.301780  loss = 2.45555 avg_loss = 3.38491\n",
      "epoch no.3 train no.301790  loss = 5.08139 avg_loss = 3.40407\n",
      "epoch no.3 train no.301800  loss = 2.64861 avg_loss = 3.35073\n",
      "epoch no.3 train no.301810  loss = 2.39327 avg_loss = 3.40020\n",
      "epoch no.3 train no.301820  loss = 1.96783 avg_loss = 3.39682\n",
      "epoch no.3 train no.301830  loss = 3.01525 avg_loss = 3.39261\n",
      "epoch no.3 train no.301840  loss = 4.02543 avg_loss = 3.43029\n",
      "epoch no.3 train no.301850  loss = 2.93486 avg_loss = 3.38601\n",
      "epoch no.3 train no.301860  loss = 2.72078 avg_loss = 3.38013\n",
      "epoch no.3 train no.301870  loss = 3.75731 avg_loss = 3.41569\n",
      "epoch no.3 train no.301880  loss = 2.77397 avg_loss = 3.38878\n",
      "epoch no.3 train no.301890  loss = 4.06887 avg_loss = 3.37085\n",
      "epoch no.3 train no.301900  loss = 4.34722 avg_loss = 3.36855\n",
      "epoch no.3 train no.301910  loss = 2.22226 avg_loss = 3.39123\n",
      "epoch no.3 train no.301920  loss = 3.06359 avg_loss = 3.39689\n",
      "epoch no.3 train no.301930  loss = 4.27669 avg_loss = 3.40085\n",
      "epoch no.3 train no.301940  loss = 2.29038 avg_loss = 3.36714\n",
      "epoch no.3 train no.301950  loss = 2.08398 avg_loss = 3.38966\n",
      "epoch no.3 train no.301960  loss = 5.45894 avg_loss = 3.37870\n",
      "epoch no.3 train no.301970  loss = 2.54865 avg_loss = 3.35350\n",
      "epoch no.3 train no.301980  loss = 4.42963 avg_loss = 3.39612\n",
      "epoch no.3 train no.301990  loss = 3.76026 avg_loss = 3.42612\n",
      "epoch no.3 train no.302000  loss = 3.77831 avg_loss = 3.43101\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁딱', '▁딱', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 딱 딱 좋은 노래</s>\n",
      "epoch no.3 train no.302010  loss = 5.23561 avg_loss = 3.44556\n",
      "epoch no.3 train no.302020  loss = 2.55085 avg_loss = 3.39768\n",
      "epoch no.3 train no.302030  loss = 2.48254 avg_loss = 3.42603\n",
      "epoch no.3 train no.302040  loss = 4.91376 avg_loss = 3.46336\n",
      "epoch no.3 train no.302050  loss = 3.45468 avg_loss = 3.46968\n",
      "epoch no.3 train no.302060  loss = 3.08585 avg_loss = 3.47949\n",
      "epoch no.3 train no.302070  loss = 3.24765 avg_loss = 3.48315\n",
      "epoch no.3 train no.302080  loss = 4.37312 avg_loss = 3.48029\n",
      "epoch no.3 train no.302090  loss = 3.58664 avg_loss = 3.46399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.302100  loss = 2.61261 avg_loss = 3.53565\n",
      "epoch no.3 train no.302110  loss = 4.81670 avg_loss = 3.51233\n",
      "epoch no.3 train no.302120  loss = 5.04962 avg_loss = 3.49137\n",
      "epoch no.3 train no.302130  loss = 3.81720 avg_loss = 3.53205\n",
      "epoch no.3 train no.302140  loss = 3.60649 avg_loss = 3.49022\n",
      "epoch no.3 train no.302150  loss = 4.35389 avg_loss = 3.54059\n",
      "epoch no.3 train no.302160  loss = 4.17259 avg_loss = 3.51146\n",
      "epoch no.3 train no.302170  loss = 1.44896 avg_loss = 3.47044\n",
      "epoch no.3 train no.302180  loss = 2.23161 avg_loss = 3.47281\n",
      "epoch no.3 train no.302190  loss = 2.85724 avg_loss = 3.47285\n",
      "epoch no.3 train no.302200  loss = 4.47052 avg_loss = 3.45860\n",
      "epoch no.3 train no.302210  loss = 2.87373 avg_loss = 3.41755\n",
      "epoch no.3 train no.302220  loss = 4.07698 avg_loss = 3.44505\n",
      "epoch no.3 train no.302230  loss = 2.98725 avg_loss = 3.48103\n",
      "epoch no.3 train no.302240  loss = 3.38742 avg_loss = 3.45878\n",
      "epoch no.3 train no.302250  loss = 2.97049 avg_loss = 3.51700\n",
      "epoch no.3 train no.302260  loss = 2.40478 avg_loss = 3.46544\n",
      "epoch no.3 train no.302270  loss = 2.98887 avg_loss = 3.42392\n",
      "epoch no.3 train no.302280  loss = 3.57108 avg_loss = 3.42306\n",
      "epoch no.3 train no.302290  loss = 3.59768 avg_loss = 3.42836\n",
      "epoch no.3 train no.302300  loss = 3.57853 avg_loss = 3.40791\n",
      "epoch no.3 train no.302310  loss = 4.28953 avg_loss = 3.47525\n",
      "epoch no.3 train no.302320  loss = 3.11606 avg_loss = 3.53210\n",
      "epoch no.3 train no.302330  loss = 3.75053 avg_loss = 3.53797\n",
      "epoch no.3 train no.302340  loss = 5.69590 avg_loss = 3.56642\n",
      "epoch no.3 train no.302350  loss = 4.07461 avg_loss = 3.53924\n",
      "epoch no.3 train no.302360  loss = 2.83587 avg_loss = 3.53881\n",
      "epoch no.3 train no.302370  loss = 1.76175 avg_loss = 3.56849\n",
      "epoch no.3 train no.302380  loss = 4.21267 avg_loss = 3.57126\n",
      "epoch no.3 train no.302390  loss = 3.23157 avg_loss = 3.54675\n",
      "epoch no.3 train no.302400  loss = 2.94240 avg_loss = 3.52568\n",
      "epoch no.3 train no.302410  loss = 3.95853 avg_loss = 3.56055\n",
      "epoch no.3 train no.302420  loss = 3.92156 avg_loss = 3.54535\n",
      "epoch no.3 train no.302430  loss = 2.37476 avg_loss = 3.52258\n",
      "epoch no.3 train no.302440  loss = 6.11548 avg_loss = 3.53887\n",
      "epoch no.3 train no.302450  loss = 1.24588 avg_loss = 3.50495\n",
      "epoch no.3 train no.302460  loss = 3.70322 avg_loss = 3.48461\n",
      "epoch no.3 train no.302470  loss = 5.10201 avg_loss = 3.51745\n",
      "epoch no.3 train no.302480  loss = 2.56234 avg_loss = 3.49230\n",
      "epoch no.3 train no.302490  loss = 2.51428 avg_loss = 3.45446\n",
      "epoch no.3 train no.302500  loss = 3.40148 avg_loss = 3.50307\n",
      "epoch no.3 train no.302510  loss = 2.58120 avg_loss = 3.52992\n",
      "epoch no.3 train no.302520  loss = 4.34355 avg_loss = 3.57523\n",
      "epoch no.3 train no.302530  loss = 2.01133 avg_loss = 3.57678\n",
      "epoch no.3 train no.302540  loss = 2.81383 avg_loss = 3.52576\n",
      "epoch no.3 train no.302550  loss = 2.43362 avg_loss = 3.50701\n",
      "epoch no.3 train no.302560  loss = 4.37818 avg_loss = 3.53302\n",
      "epoch no.3 train no.302570  loss = 2.01318 avg_loss = 3.48353\n",
      "epoch no.3 train no.302580  loss = 2.78256 avg_loss = 3.54190\n",
      "epoch no.3 train no.302590  loss = 3.13429 avg_loss = 3.53503\n",
      "epoch no.3 train no.302600  loss = 1.83614 avg_loss = 3.53439\n",
      "epoch no.3 train no.302610  loss = 4.14099 avg_loss = 3.56170\n",
      "epoch no.3 train no.302620  loss = 2.82171 avg_loss = 3.52932\n",
      "epoch no.3 train no.302630  loss = 3.62804 avg_loss = 3.52976\n",
      "epoch no.3 train no.302640  loss = 2.84512 avg_loss = 3.55077\n",
      "epoch no.3 train no.302650  loss = 4.23571 avg_loss = 3.57841\n",
      "epoch no.3 train no.302660  loss = 3.00511 avg_loss = 3.55780\n",
      "epoch no.3 train no.302670  loss = 5.13621 avg_loss = 3.55266\n",
      "epoch no.3 train no.302680  loss = 4.21026 avg_loss = 3.56854\n",
      "epoch no.3 train no.302690  loss = 3.76608 avg_loss = 3.52838\n",
      "epoch no.3 train no.302700  loss = 4.55874 avg_loss = 3.51021\n",
      "epoch no.3 train no.302710  loss = 4.90418 avg_loss = 3.53664\n",
      "epoch no.3 train no.302720  loss = 3.46680 avg_loss = 3.48379\n",
      "epoch no.3 train no.302730  loss = 3.84930 avg_loss = 3.49417\n",
      "epoch no.3 train no.302740  loss = 4.47011 avg_loss = 3.52198\n",
      "epoch no.3 train no.302750  loss = 2.67356 avg_loss = 3.52606\n",
      "epoch no.3 train no.302760  loss = 2.63917 avg_loss = 3.54534\n",
      "epoch no.3 train no.302770  loss = 4.05410 avg_loss = 3.56110\n",
      "epoch no.3 train no.302780  loss = 4.62909 avg_loss = 3.56539\n",
      "epoch no.3 train no.302790  loss = 4.24453 avg_loss = 3.56675\n",
      "epoch no.3 train no.302800  loss = 3.02807 avg_loss = 3.57785\n",
      "epoch no.3 train no.302810  loss = 2.96327 avg_loss = 3.61929\n",
      "epoch no.3 train no.302820  loss = 2.77158 avg_loss = 3.59080\n",
      "epoch no.3 train no.302830  loss = 4.91101 avg_loss = 3.55457\n",
      "epoch no.3 train no.302840  loss = 4.39944 avg_loss = 3.54266\n",
      "epoch no.3 train no.302850  loss = 2.55324 avg_loss = 3.51456\n",
      "epoch no.3 train no.302860  loss = 3.41869 avg_loss = 3.47028\n",
      "epoch no.3 train no.302870  loss = 4.14975 avg_loss = 3.46145\n",
      "epoch no.3 train no.302880  loss = 2.79915 avg_loss = 3.44316\n",
      "epoch no.3 train no.302890  loss = 3.69060 avg_loss = 3.46093\n",
      "epoch no.3 train no.302900  loss = 2.70151 avg_loss = 3.45218\n",
      "epoch no.3 train no.302910  loss = 3.50682 avg_loss = 3.42210\n",
      "epoch no.3 train no.302920  loss = 3.32450 avg_loss = 3.40735\n",
      "epoch no.3 train no.302930  loss = 4.00580 avg_loss = 3.47188\n",
      "epoch no.3 train no.302940  loss = 3.59240 avg_loss = 3.50759\n",
      "epoch no.3 train no.302950  loss = 4.83958 avg_loss = 3.54379\n",
      "epoch no.3 train no.302960  loss = 2.21038 avg_loss = 3.47746\n",
      "epoch no.3 train no.302970  loss = 3.06273 avg_loss = 3.44580\n",
      "epoch no.3 train no.302980  loss = 4.17838 avg_loss = 3.50025\n",
      "epoch no.3 train no.302990  loss = 3.23529 avg_loss = 3.47459\n",
      "epoch no.3 train no.303000  loss = 4.76708 avg_loss = 3.52358\n",
      "5\n",
      "to_tokens: ['▁비', '▁좋은', '이', '▁필요할', '▁때', '</s>', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 음악</s>\n",
      "epoch no.3 train no.303010  loss = 4.91541 avg_loss = 3.53094\n",
      "epoch no.3 train no.303020  loss = 5.24663 avg_loss = 3.52736\n",
      "epoch no.3 train no.303030  loss = 4.58912 avg_loss = 3.48362\n",
      "epoch no.3 train no.303040  loss = 3.00314 avg_loss = 3.49845\n",
      "epoch no.3 train no.303050  loss = 2.78497 avg_loss = 3.47710\n",
      "epoch no.3 train no.303060  loss = 4.49123 avg_loss = 3.49489\n",
      "epoch no.3 train no.303070  loss = 5.41592 avg_loss = 3.52583\n",
      "epoch no.3 train no.303080  loss = 2.97739 avg_loss = 3.55508\n",
      "epoch no.3 train no.303090  loss = 2.57543 avg_loss = 3.48237\n",
      "epoch no.3 train no.303100  loss = 4.24749 avg_loss = 3.47983\n",
      "epoch no.3 train no.303110  loss = 3.14215 avg_loss = 3.44157\n",
      "epoch no.3 train no.303120  loss = 4.33044 avg_loss = 3.46221\n",
      "epoch no.3 train no.303130  loss = 4.05787 avg_loss = 3.44287\n",
      "epoch no.3 train no.303140  loss = 4.19150 avg_loss = 3.49100\n",
      "epoch no.3 train no.303150  loss = 2.05707 avg_loss = 3.47744\n",
      "epoch no.3 train no.303160  loss = 3.69105 avg_loss = 3.45041\n",
      "epoch no.3 train no.303170  loss = 4.77204 avg_loss = 3.47666\n",
      "epoch no.3 train no.303180  loss = 2.41055 avg_loss = 3.47353\n",
      "epoch no.3 train no.303190  loss = 2.25977 avg_loss = 3.36983\n",
      "epoch no.3 train no.303200  loss = 4.14942 avg_loss = 3.42489\n",
      "epoch no.3 train no.303210  loss = 2.27501 avg_loss = 3.39857\n",
      "epoch no.3 train no.303220  loss = 3.16259 avg_loss = 3.37679\n",
      "epoch no.3 train no.303230  loss = 3.98531 avg_loss = 3.35547\n",
      "epoch no.3 train no.303240  loss = 2.39283 avg_loss = 3.37348\n",
      "epoch no.3 train no.303250  loss = 3.12043 avg_loss = 3.39025\n",
      "epoch no.3 train no.303260  loss = 2.73737 avg_loss = 3.33912\n",
      "epoch no.3 train no.303270  loss = 4.18906 avg_loss = 3.37616\n",
      "epoch no.3 train no.303280  loss = 3.15900 avg_loss = 3.37037\n",
      "epoch no.3 train no.303290  loss = 2.19435 avg_loss = 3.38528\n",
      "epoch no.3 train no.303300  loss = 3.08909 avg_loss = 3.35527\n",
      "epoch no.3 train no.303310  loss = 3.63761 avg_loss = 3.36217\n",
      "epoch no.3 train no.303320  loss = 5.70977 avg_loss = 3.35904\n",
      "epoch no.3 train no.303330  loss = 3.42384 avg_loss = 3.33339\n",
      "epoch no.3 train no.303340  loss = 3.71588 avg_loss = 3.34549\n",
      "epoch no.3 train no.303350  loss = 5.41550 avg_loss = 3.35703\n",
      "epoch no.3 train no.303360  loss = 4.15998 avg_loss = 3.36500\n",
      "epoch no.3 train no.303370  loss = 4.96915 avg_loss = 3.40767\n",
      "epoch no.3 train no.303380  loss = 3.33416 avg_loss = 3.38848\n",
      "epoch no.3 train no.303390  loss = 5.76504 avg_loss = 3.47192\n",
      "epoch no.3 train no.303400  loss = 2.30213 avg_loss = 3.49106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.303410  loss = 3.89380 avg_loss = 3.47066\n",
      "epoch no.3 train no.303420  loss = 5.61221 avg_loss = 3.50121\n",
      "epoch no.3 train no.303430  loss = 3.66813 avg_loss = 3.53114\n",
      "epoch no.3 train no.303440  loss = 2.94539 avg_loss = 3.51031\n",
      "epoch no.3 train no.303450  loss = 3.52061 avg_loss = 3.47660\n",
      "epoch no.3 train no.303460  loss = 3.05207 avg_loss = 3.48073\n",
      "epoch no.3 train no.303470  loss = 2.03155 avg_loss = 3.45004\n",
      "epoch no.3 train no.303480  loss = 3.15469 avg_loss = 3.45219\n",
      "epoch no.3 train no.303490  loss = 3.99940 avg_loss = 3.46440\n",
      "epoch no.3 train no.303500  loss = 3.65913 avg_loss = 3.45934\n",
      "epoch no.3 train no.303510  loss = 4.13689 avg_loss = 3.48558\n",
      "epoch no.3 train no.303520  loss = 3.01874 avg_loss = 3.49123\n",
      "epoch no.3 train no.303530  loss = 3.73772 avg_loss = 3.47727\n",
      "epoch no.3 train no.303540  loss = 2.95794 avg_loss = 3.50372\n",
      "epoch no.3 train no.303550  loss = 5.51090 avg_loss = 3.50750\n",
      "epoch no.3 train no.303560  loss = 6.46241 avg_loss = 3.53704\n",
      "epoch no.3 train no.303570  loss = 5.18309 avg_loss = 3.49186\n",
      "epoch no.3 train no.303580  loss = 3.75513 avg_loss = 3.51439\n",
      "epoch no.3 train no.303590  loss = 2.90887 avg_loss = 3.57407\n",
      "epoch no.3 train no.303600  loss = 3.32691 avg_loss = 3.54525\n",
      "epoch no.3 train no.303610  loss = 2.88542 avg_loss = 3.53513\n",
      "epoch no.3 train no.303620  loss = 5.32928 avg_loss = 3.58023\n",
      "epoch no.3 train no.303630  loss = 5.35988 avg_loss = 3.57912\n",
      "epoch no.3 train no.303640  loss = 4.49387 avg_loss = 3.58169\n",
      "epoch no.3 train no.303650  loss = 4.25953 avg_loss = 3.60081\n",
      "epoch no.3 train no.303660  loss = 4.92040 avg_loss = 3.60483\n",
      "epoch no.3 train no.303670  loss = 4.46054 avg_loss = 3.63118\n",
      "epoch no.3 train no.303680  loss = 4.65916 avg_loss = 3.62451\n",
      "epoch no.3 train no.303690  loss = 4.29355 avg_loss = 3.60018\n",
      "epoch no.3 train no.303700  loss = 3.43835 avg_loss = 3.57519\n",
      "epoch no.3 train no.303710  loss = 3.72444 avg_loss = 3.61484\n",
      "epoch no.3 train no.303720  loss = 3.17899 avg_loss = 3.60477\n",
      "epoch no.3 train no.303730  loss = 3.05220 avg_loss = 3.55553\n",
      "epoch no.3 train no.303740  loss = 2.73227 avg_loss = 3.50902\n",
      "epoch no.3 train no.303750  loss = 1.97981 avg_loss = 3.47271\n",
      "epoch no.3 train no.303760  loss = 1.94341 avg_loss = 3.42810\n",
      "epoch no.3 train no.303770  loss = 2.55128 avg_loss = 3.45484\n",
      "epoch no.3 train no.303780  loss = 3.95970 avg_loss = 3.42564\n",
      "epoch no.3 train no.303790  loss = 2.19709 avg_loss = 3.42475\n",
      "epoch no.3 train no.303800  loss = 3.85202 avg_loss = 3.41873\n",
      "epoch no.3 train no.303810  loss = 4.35670 avg_loss = 3.44702\n",
      "epoch no.3 train no.303820  loss = 3.62563 avg_loss = 3.44151\n",
      "epoch no.3 train no.303830  loss = 2.97565 avg_loss = 3.45622\n",
      "epoch no.3 train no.303840  loss = 4.20189 avg_loss = 3.48868\n",
      "epoch no.3 train no.303850  loss = 2.15089 avg_loss = 3.51412\n",
      "epoch no.3 train no.303860  loss = 3.32053 avg_loss = 3.53466\n",
      "epoch no.3 train no.303870  loss = 3.12236 avg_loss = 3.51296\n",
      "epoch no.3 train no.303880  loss = 2.78004 avg_loss = 3.48428\n",
      "epoch no.3 train no.303890  loss = 3.12195 avg_loss = 3.51915\n",
      "epoch no.3 train no.303900  loss = 2.85404 avg_loss = 3.53153\n",
      "epoch no.3 train no.303910  loss = 3.47797 avg_loss = 3.49371\n",
      "epoch no.3 train no.303920  loss = 3.08924 avg_loss = 3.51734\n",
      "epoch no.3 train no.303930  loss = 1.77439 avg_loss = 3.45895\n",
      "epoch no.3 train no.303940  loss = 3.80619 avg_loss = 3.45866\n",
      "epoch no.3 train no.303950  loss = 3.41183 avg_loss = 3.43840\n",
      "epoch no.3 train no.303960  loss = 2.26121 avg_loss = 3.42248\n",
      "epoch no.3 train no.303970  loss = 3.38585 avg_loss = 3.37785\n",
      "epoch no.3 train no.303980  loss = 4.32440 avg_loss = 3.38473\n",
      "epoch no.3 train no.303990  loss = 3.75257 avg_loss = 3.39410\n",
      "epoch no.3 train no.304000  loss = 5.69489 avg_loss = 3.41194\n",
      "4\n",
      "to_tokens: ['▁비', '좋은', '이', '</s>', '▁노래', '</s>', '</s>']\n",
      "기분전환용 신나는 노래들</s>\n",
      "epoch no.3 train no.304010  loss = 4.40763 avg_loss = 3.43208\n",
      "epoch no.3 train no.304020  loss = 3.58889 avg_loss = 3.42723\n",
      "epoch no.3 train no.304030  loss = 2.69939 avg_loss = 3.43807\n",
      "epoch no.3 train no.304040  loss = 3.20069 avg_loss = 3.48461\n",
      "epoch no.3 train no.304050  loss = 3.27840 avg_loss = 3.49160\n",
      "epoch no.3 train no.304060  loss = 4.81564 avg_loss = 3.49200\n",
      "epoch no.3 train no.304070  loss = 2.54980 avg_loss = 3.47118\n",
      "epoch no.3 train no.304080  loss = 3.83077 avg_loss = 3.46506\n",
      "epoch no.3 train no.304090  loss = 3.45911 avg_loss = 3.44961\n",
      "epoch no.3 train no.304100  loss = 3.81210 avg_loss = 3.45236\n",
      "epoch no.3 train no.304110  loss = 3.62970 avg_loss = 3.49948\n",
      "epoch no.3 train no.304120  loss = 2.07501 avg_loss = 3.48673\n",
      "epoch no.3 train no.304130  loss = 3.82291 avg_loss = 3.47765\n",
      "epoch no.3 train no.304140  loss = 3.07401 avg_loss = 3.44576\n",
      "epoch no.3 train no.304150  loss = 4.20945 avg_loss = 3.45723\n",
      "epoch no.3 train no.304160  loss = 3.40502 avg_loss = 3.45386\n",
      "epoch no.3 train no.304170  loss = 3.01081 avg_loss = 3.45388\n",
      "epoch no.3 train no.304180  loss = 3.31535 avg_loss = 3.45882\n",
      "epoch no.3 train no.304190  loss = 4.17851 avg_loss = 3.42445\n",
      "epoch no.3 train no.304200  loss = 5.70497 avg_loss = 3.44861\n",
      "epoch no.3 train no.304210  loss = 2.38501 avg_loss = 3.42562\n",
      "epoch no.3 train no.304220  loss = 3.27356 avg_loss = 3.43750\n",
      "epoch no.3 train no.304230  loss = 3.03675 avg_loss = 3.40984\n",
      "epoch no.3 train no.304240  loss = 4.30269 avg_loss = 3.41951\n",
      "epoch no.3 train no.304250  loss = 2.60175 avg_loss = 3.45097\n",
      "epoch no.3 train no.304260  loss = 2.38815 avg_loss = 3.46439\n",
      "epoch no.3 train no.304270  loss = 4.86385 avg_loss = 3.46811\n",
      "epoch no.3 train no.304280  loss = 3.43135 avg_loss = 3.42795\n",
      "epoch no.3 train no.304290  loss = 4.93432 avg_loss = 3.47060\n",
      "epoch no.3 train no.304300  loss = 3.18792 avg_loss = 3.47695\n",
      "epoch no.3 train no.304310  loss = 3.49454 avg_loss = 3.48133\n",
      "epoch no.3 train no.304320  loss = 3.05797 avg_loss = 3.49116\n",
      "epoch no.3 train no.304330  loss = 4.95603 avg_loss = 3.54068\n",
      "epoch no.3 train no.304340  loss = 1.84558 avg_loss = 3.53988\n",
      "epoch no.3 train no.304350  loss = 3.72480 avg_loss = 3.57658\n",
      "epoch no.3 train no.304360  loss = 5.25344 avg_loss = 3.54909\n",
      "epoch no.3 train no.304370  loss = 3.02986 avg_loss = 3.52168\n",
      "epoch no.3 train no.304380  loss = 2.59921 avg_loss = 3.50556\n",
      "epoch no.3 train no.304390  loss = 3.27130 avg_loss = 3.50548\n",
      "epoch no.3 train no.304400  loss = 1.99229 avg_loss = 3.48342\n",
      "epoch no.3 train no.304410  loss = 3.03827 avg_loss = 3.47284\n",
      "epoch no.3 train no.304420  loss = 5.61403 avg_loss = 3.49327\n",
      "epoch no.3 train no.304430  loss = 2.99583 avg_loss = 3.52970\n",
      "epoch no.3 train no.304440  loss = 5.16637 avg_loss = 3.53721\n",
      "epoch no.3 train no.304450  loss = 3.53052 avg_loss = 3.51883\n",
      "epoch no.3 train no.304460  loss = 3.06945 avg_loss = 3.53822\n",
      "epoch no.3 train no.304470  loss = 4.33505 avg_loss = 3.57212\n",
      "epoch no.3 train no.304480  loss = 3.69456 avg_loss = 3.60356\n",
      "epoch no.3 train no.304490  loss = 1.75202 avg_loss = 3.56695\n",
      "epoch no.3 train no.304500  loss = 4.15474 avg_loss = 3.60997\n",
      "epoch no.3 train no.304510  loss = 3.24715 avg_loss = 3.54703\n",
      "epoch no.3 train no.304520  loss = 2.15687 avg_loss = 3.53948\n",
      "epoch no.3 train no.304530  loss = 3.50196 avg_loss = 3.52218\n",
      "epoch no.3 train no.304540  loss = 3.41925 avg_loss = 3.49902\n",
      "epoch no.3 train no.304550  loss = 1.14178 avg_loss = 3.49027\n",
      "epoch no.3 train no.304560  loss = 3.91939 avg_loss = 3.53514\n",
      "epoch no.3 train no.304570  loss = 3.25147 avg_loss = 3.55000\n",
      "epoch no.3 train no.304580  loss = 4.71691 avg_loss = 3.54753\n",
      "epoch no.3 train no.304590  loss = 2.96324 avg_loss = 3.54661\n",
      "epoch no.3 train no.304600  loss = 4.87008 avg_loss = 3.57573\n",
      "epoch no.3 train no.304610  loss = 4.44720 avg_loss = 3.57536\n",
      "epoch no.3 train no.304620  loss = 4.32697 avg_loss = 3.56892\n",
      "epoch no.3 train no.304630  loss = 3.10579 avg_loss = 3.56559\n",
      "epoch no.3 train no.304640  loss = 2.40417 avg_loss = 3.55844\n",
      "epoch no.3 train no.304650  loss = 2.94480 avg_loss = 3.59480\n",
      "epoch no.3 train no.304660  loss = 5.34442 avg_loss = 3.56154\n",
      "epoch no.3 train no.304670  loss = 2.40771 avg_loss = 3.53237\n",
      "epoch no.3 train no.304680  loss = 3.73648 avg_loss = 3.52990\n",
      "epoch no.3 train no.304690  loss = 2.30528 avg_loss = 3.50018\n",
      "epoch no.3 train no.304700  loss = 4.68180 avg_loss = 3.46433\n",
      "epoch no.3 train no.304710  loss = 2.12984 avg_loss = 3.45519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.304720  loss = 4.01481 avg_loss = 3.43988\n",
      "epoch no.3 train no.304730  loss = 3.55557 avg_loss = 3.45693\n",
      "epoch no.3 train no.304740  loss = 3.73003 avg_loss = 3.49540\n",
      "epoch no.3 train no.304750  loss = 3.13817 avg_loss = 3.50970\n",
      "epoch no.3 train no.304760  loss = 3.88170 avg_loss = 3.49166\n",
      "epoch no.3 train no.304770  loss = 2.04304 avg_loss = 3.46926\n",
      "epoch no.3 train no.304780  loss = 3.54071 avg_loss = 3.47863\n",
      "epoch no.3 train no.304790  loss = 4.59946 avg_loss = 3.48544\n",
      "epoch no.3 train no.304800  loss = 2.10613 avg_loss = 3.49914\n",
      "epoch no.3 train no.304810  loss = 3.70892 avg_loss = 3.45019\n",
      "epoch no.3 train no.304820  loss = 2.51597 avg_loss = 3.41195\n",
      "epoch no.3 train no.304830  loss = 2.45081 avg_loss = 3.40808\n",
      "epoch no.3 train no.304840  loss = 4.98088 avg_loss = 3.40097\n",
      "epoch no.3 train no.304850  loss = 4.26076 avg_loss = 3.41351\n",
      "epoch no.3 train no.304860  loss = 4.21374 avg_loss = 3.47026\n",
      "epoch no.3 train no.304870  loss = 3.52858 avg_loss = 3.46811\n",
      "epoch no.3 train no.304880  loss = 3.49583 avg_loss = 3.47476\n",
      "epoch no.3 train no.304890  loss = 3.67846 avg_loss = 3.50993\n",
      "epoch no.3 train no.304900  loss = 4.30303 avg_loss = 3.53536\n",
      "epoch no.3 train no.304910  loss = 2.61186 avg_loss = 3.56796\n",
      "epoch no.3 train no.304920  loss = 4.29544 avg_loss = 3.61035\n",
      "epoch no.3 train no.304930  loss = 2.97076 avg_loss = 3.61297\n",
      "epoch no.3 train no.304940  loss = 2.17963 avg_loss = 3.58997\n",
      "epoch no.3 train no.304950  loss = 2.02837 avg_loss = 3.53707\n",
      "epoch no.3 train no.304960  loss = 4.51393 avg_loss = 3.54929\n",
      "epoch no.3 train no.304970  loss = 4.77635 avg_loss = 3.55946\n",
      "epoch no.3 train no.304980  loss = 3.01924 avg_loss = 3.49105\n",
      "epoch no.3 train no.304990  loss = 4.08568 avg_loss = 3.48988\n",
      "epoch no.3 train no.305000  loss = 2.11823 avg_loss = 3.43943\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '</s>', '▁신나는', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.3 train no.305010  loss = 2.84372 avg_loss = 3.49110\n",
      "epoch no.3 train no.305020  loss = 3.17766 avg_loss = 3.54396\n",
      "epoch no.3 train no.305030  loss = 2.95009 avg_loss = 3.53675\n",
      "epoch no.3 train no.305040  loss = 2.53680 avg_loss = 3.53602\n",
      "epoch no.3 train no.305050  loss = 4.26772 avg_loss = 3.54268\n",
      "epoch no.3 train no.305060  loss = 1.94273 avg_loss = 3.53264\n",
      "epoch no.3 train no.305070  loss = 2.89963 avg_loss = 3.53315\n",
      "epoch no.3 train no.305080  loss = 3.32023 avg_loss = 3.55361\n",
      "epoch no.3 train no.305090  loss = 3.48232 avg_loss = 3.53001\n",
      "epoch no.3 train no.305100  loss = 2.59171 avg_loss = 3.52807\n",
      "epoch no.3 train no.305110  loss = 4.11924 avg_loss = 3.58464\n",
      "epoch no.3 train no.305120  loss = 2.14817 avg_loss = 3.62768\n",
      "epoch no.3 train no.305130  loss = 2.11595 avg_loss = 3.55058\n",
      "epoch no.3 train no.305140  loss = 3.30917 avg_loss = 3.53912\n",
      "epoch no.3 train no.305150  loss = 2.70761 avg_loss = 3.53742\n",
      "epoch no.3 train no.305160  loss = 2.87380 avg_loss = 3.49333\n",
      "epoch no.3 train no.305170  loss = 4.98179 avg_loss = 3.48690\n",
      "epoch no.3 train no.305180  loss = 2.63238 avg_loss = 3.47774\n",
      "epoch no.3 train no.305190  loss = 2.21785 avg_loss = 3.43815\n",
      "epoch no.3 train no.305200  loss = 4.42763 avg_loss = 3.41750\n",
      "epoch no.3 train no.305210  loss = 4.39889 avg_loss = 3.46083\n",
      "epoch no.3 train no.305220  loss = 2.40112 avg_loss = 3.45321\n",
      "epoch no.3 train no.305230  loss = 2.38380 avg_loss = 3.42809\n",
      "epoch no.3 train no.305240  loss = 2.90634 avg_loss = 3.45510\n",
      "epoch no.3 train no.305250  loss = 4.25204 avg_loss = 3.48634\n",
      "epoch no.3 train no.305260  loss = 2.89996 avg_loss = 3.54315\n",
      "epoch no.3 train no.305270  loss = 3.35020 avg_loss = 3.57474\n",
      "epoch no.3 train no.305280  loss = 4.39362 avg_loss = 3.57984\n",
      "epoch no.3 train no.305290  loss = 3.71881 avg_loss = 3.53928\n",
      "epoch no.3 train no.305300  loss = 3.24490 avg_loss = 3.53005\n",
      "epoch no.3 train no.305310  loss = 3.16692 avg_loss = 3.56283\n",
      "epoch no.3 train no.305320  loss = 3.05989 avg_loss = 3.54084\n",
      "epoch no.3 train no.305330  loss = 3.01441 avg_loss = 3.49043\n",
      "epoch no.3 train no.305340  loss = 5.16582 avg_loss = 3.53872\n",
      "epoch no.3 train no.305350  loss = 2.65308 avg_loss = 3.53232\n",
      "epoch no.3 train no.305360  loss = 2.99978 avg_loss = 3.48523\n",
      "epoch no.3 train no.305370  loss = 3.68277 avg_loss = 3.51686\n",
      "epoch no.3 train no.305380  loss = 4.84556 avg_loss = 3.54307\n",
      "epoch no.3 train no.305390  loss = 4.71515 avg_loss = 3.57342\n",
      "epoch no.3 train no.305400  loss = 4.14650 avg_loss = 3.61202\n",
      "epoch no.3 train no.305410  loss = 3.49663 avg_loss = 3.56543\n",
      "epoch no.3 train no.305420  loss = 2.16741 avg_loss = 3.50276\n",
      "epoch no.3 train no.305430  loss = 3.25174 avg_loss = 3.49649\n",
      "epoch no.3 train no.305440  loss = 1.63237 avg_loss = 3.42619\n",
      "epoch no.3 train no.305450  loss = 3.34804 avg_loss = 3.44192\n",
      "epoch no.3 train no.305460  loss = 2.18319 avg_loss = 3.38419\n",
      "epoch no.3 train no.305470  loss = 3.96524 avg_loss = 3.40106\n",
      "epoch no.3 train no.305480  loss = 3.43325 avg_loss = 3.42017\n",
      "epoch no.3 train no.305490  loss = 3.63417 avg_loss = 3.40590\n",
      "epoch no.3 train no.305500  loss = 2.91664 avg_loss = 3.37514\n",
      "epoch no.3 train no.305510  loss = 1.90422 avg_loss = 3.36561\n",
      "epoch no.3 train no.305520  loss = 3.37597 avg_loss = 3.40431\n",
      "epoch no.3 train no.305530  loss = 2.28038 avg_loss = 3.40497\n",
      "epoch no.3 train no.305540  loss = 5.45669 avg_loss = 3.46718\n",
      "epoch no.3 train no.305550  loss = 3.26652 avg_loss = 3.46849\n",
      "epoch no.3 train no.305560  loss = 2.82969 avg_loss = 3.44008\n",
      "epoch no.3 train no.305570  loss = 3.08882 avg_loss = 3.47602\n",
      "epoch no.3 train no.305580  loss = 5.04422 avg_loss = 3.50871\n",
      "epoch no.3 train no.305590  loss = 2.42123 avg_loss = 3.49468\n",
      "epoch no.3 train no.305600  loss = 4.13879 avg_loss = 3.51943\n",
      "epoch no.3 train no.305610  loss = 3.73510 avg_loss = 3.51838\n",
      "epoch no.3 train no.305620  loss = 2.33119 avg_loss = 3.49802\n",
      "epoch no.3 train no.305630  loss = 4.46589 avg_loss = 3.52130\n",
      "epoch no.3 train no.305640  loss = 3.47105 avg_loss = 3.52300\n",
      "epoch no.3 train no.305650  loss = 4.77935 avg_loss = 3.51169\n",
      "epoch no.3 train no.305660  loss = 3.24624 avg_loss = 3.53730\n",
      "epoch no.3 train no.305670  loss = 4.42502 avg_loss = 3.54901\n",
      "epoch no.3 train no.305680  loss = 2.91858 avg_loss = 3.53777\n",
      "epoch no.3 train no.305690  loss = 3.54579 avg_loss = 3.53792\n",
      "epoch no.3 train no.305700  loss = 4.93737 avg_loss = 3.54057\n",
      "epoch no.3 train no.305710  loss = 2.52895 avg_loss = 3.49388\n",
      "epoch no.3 train no.305720  loss = 2.36390 avg_loss = 3.47483\n",
      "epoch no.3 train no.305730  loss = 3.45527 avg_loss = 3.44248\n",
      "epoch no.3 train no.305740  loss = 3.61113 avg_loss = 3.42892\n",
      "epoch no.3 train no.305750  loss = 3.22560 avg_loss = 3.42266\n",
      "epoch no.3 train no.305760  loss = 2.63611 avg_loss = 3.43666\n",
      "epoch no.3 train no.305770  loss = 1.82992 avg_loss = 3.44675\n",
      "epoch no.3 train no.305780  loss = 2.98156 avg_loss = 3.41336\n",
      "epoch no.3 train no.305790  loss = 3.19573 avg_loss = 3.40436\n",
      "epoch no.3 train no.305800  loss = 2.69798 avg_loss = 3.38533\n",
      "epoch no.3 train no.305810  loss = 4.19702 avg_loss = 3.44567\n",
      "epoch no.3 train no.305820  loss = 2.63355 avg_loss = 3.47258\n",
      "epoch no.3 train no.305830  loss = 3.54519 avg_loss = 3.48847\n",
      "epoch no.3 train no.305840  loss = 2.94789 avg_loss = 3.45573\n",
      "epoch no.3 train no.305850  loss = 1.77586 avg_loss = 3.40438\n",
      "epoch no.3 train no.305860  loss = 2.53020 avg_loss = 3.40274\n",
      "epoch no.3 train no.305870  loss = 4.49115 avg_loss = 3.46209\n",
      "epoch no.3 train no.305880  loss = 2.98029 avg_loss = 3.44011\n",
      "epoch no.3 train no.305890  loss = 2.42218 avg_loss = 3.44901\n",
      "epoch no.3 train no.305900  loss = 1.75003 avg_loss = 3.40738\n",
      "epoch no.3 train no.305910  loss = 3.33949 avg_loss = 3.39389\n",
      "epoch no.3 train no.305920  loss = 2.78954 avg_loss = 3.39656\n",
      "epoch no.3 train no.305930  loss = 3.71418 avg_loss = 3.41391\n",
      "epoch no.3 train no.305940  loss = 2.00656 avg_loss = 3.42158\n",
      "epoch no.3 train no.305950  loss = 3.92624 avg_loss = 3.46518\n",
      "epoch no.3 train no.305960  loss = 3.61753 avg_loss = 3.44984\n",
      "epoch no.3 train no.305970  loss = 3.33497 avg_loss = 3.46124\n",
      "epoch no.3 train no.305980  loss = 4.98662 avg_loss = 3.52306\n",
      "epoch no.3 train no.305990  loss = 3.55568 avg_loss = 3.50552\n",
      "epoch no.3 train no.306000  loss = 4.10030 avg_loss = 3.56152\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '▁위한', '▁신나는', '▁팝', '</s>']\n",
      "기분전환을 위한 신나는 음악</s>\n",
      "epoch no.3 train no.306010  loss = 3.73549 avg_loss = 3.55839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.306020  loss = 3.38793 avg_loss = 3.58941\n",
      "epoch no.3 train no.306030  loss = 3.84233 avg_loss = 3.58865\n",
      "epoch no.3 train no.306040  loss = 3.44553 avg_loss = 3.56556\n",
      "epoch no.3 train no.306050  loss = 3.55508 avg_loss = 3.55693\n",
      "epoch no.3 train no.306060  loss = 5.13828 avg_loss = 3.54424\n",
      "epoch no.3 train no.306070  loss = 3.97640 avg_loss = 3.56125\n",
      "epoch no.3 train no.306080  loss = 2.87170 avg_loss = 3.60065\n",
      "epoch no.3 train no.306090  loss = 3.75030 avg_loss = 3.56092\n",
      "epoch no.3 train no.306100  loss = 2.34264 avg_loss = 3.56226\n",
      "epoch no.3 train no.306110  loss = 3.53025 avg_loss = 3.53788\n",
      "epoch no.3 train no.306120  loss = 3.16749 avg_loss = 3.50612\n",
      "epoch no.3 train no.306130  loss = 3.10816 avg_loss = 3.45630\n",
      "epoch no.3 train no.306140  loss = 2.92779 avg_loss = 3.45963\n",
      "epoch no.3 train no.306150  loss = 1.92010 avg_loss = 3.40068\n",
      "epoch no.3 train no.306160  loss = 3.43763 avg_loss = 3.39302\n",
      "epoch no.3 train no.306170  loss = 3.54399 avg_loss = 3.43886\n",
      "epoch no.3 train no.306180  loss = 3.80822 avg_loss = 3.43505\n",
      "epoch no.3 train no.306190  loss = 3.43366 avg_loss = 3.42888\n",
      "epoch no.3 train no.306200  loss = 3.94259 avg_loss = 3.39180\n",
      "epoch no.3 train no.306210  loss = 3.32296 avg_loss = 3.39851\n",
      "epoch no.3 train no.306220  loss = 3.99461 avg_loss = 3.40420\n",
      "epoch no.3 train no.306230  loss = 3.44401 avg_loss = 3.43103\n",
      "epoch no.3 train no.306240  loss = 2.58227 avg_loss = 3.43148\n",
      "epoch no.3 train no.306250  loss = 3.23606 avg_loss = 3.42976\n",
      "epoch no.3 train no.306260  loss = 4.05983 avg_loss = 3.47080\n",
      "epoch no.3 train no.306270  loss = 2.92660 avg_loss = 3.54761\n",
      "epoch no.3 train no.306280  loss = 7.05018 avg_loss = 3.55151\n",
      "epoch no.3 train no.306290  loss = 4.51515 avg_loss = 3.53426\n",
      "epoch no.3 train no.306300  loss = 2.66607 avg_loss = 3.50334\n",
      "epoch no.3 train no.306310  loss = 3.20322 avg_loss = 3.48686\n",
      "epoch no.3 train no.306320  loss = 4.58665 avg_loss = 3.44463\n",
      "epoch no.3 train no.306330  loss = 4.07188 avg_loss = 3.46564\n",
      "epoch no.3 train no.306340  loss = 2.50604 avg_loss = 3.46458\n",
      "epoch no.3 train no.306350  loss = 4.55115 avg_loss = 3.52208\n",
      "epoch no.3 train no.306360  loss = 2.47342 avg_loss = 3.52257\n",
      "epoch no.3 train no.306370  loss = 3.74843 avg_loss = 3.54953\n",
      "epoch no.3 train no.306380  loss = 2.56994 avg_loss = 3.52361\n",
      "epoch no.3 train no.306390  loss = 2.01658 avg_loss = 3.51716\n",
      "epoch no.3 train no.306400  loss = 6.18833 avg_loss = 3.56145\n",
      "epoch no.3 train no.306410  loss = 3.51955 avg_loss = 3.52539\n",
      "epoch no.3 train no.306420  loss = 2.95138 avg_loss = 3.52234\n",
      "epoch no.3 train no.306430  loss = 4.18872 avg_loss = 3.54129\n",
      "epoch no.3 train no.306440  loss = 3.64154 avg_loss = 3.53878\n",
      "epoch no.3 train no.306450  loss = 3.25293 avg_loss = 3.60280\n",
      "epoch no.3 train no.306460  loss = 2.68794 avg_loss = 3.60906\n",
      "epoch no.3 train no.306470  loss = 2.00567 avg_loss = 3.59468\n",
      "epoch no.3 train no.306480  loss = 2.55246 avg_loss = 3.51772\n",
      "epoch no.3 train no.306490  loss = 3.49398 avg_loss = 3.51667\n",
      "epoch no.3 train no.306500  loss = 2.04174 avg_loss = 3.46397\n",
      "epoch no.3 train no.306510  loss = 2.70514 avg_loss = 3.42963\n",
      "epoch no.3 train no.306520  loss = 3.47932 avg_loss = 3.37003\n",
      "epoch no.3 train no.306530  loss = 2.03104 avg_loss = 3.35254\n",
      "epoch no.3 train no.306540  loss = 3.40665 avg_loss = 3.34783\n",
      "epoch no.3 train no.306550  loss = 3.57416 avg_loss = 3.30321\n",
      "epoch no.3 train no.306560  loss = 1.67204 avg_loss = 3.26458\n",
      "epoch no.3 train no.306570  loss = 4.11814 avg_loss = 3.26000\n",
      "epoch no.3 train no.306580  loss = 2.08058 avg_loss = 3.21440\n",
      "epoch no.3 train no.306590  loss = 3.91875 avg_loss = 3.21468\n",
      "epoch no.3 train no.306600  loss = 4.28809 avg_loss = 3.23859\n",
      "epoch no.3 train no.306610  loss = 2.89042 avg_loss = 3.26700\n",
      "epoch no.3 train no.306620  loss = 3.42468 avg_loss = 3.30475\n",
      "epoch no.3 train no.306630  loss = 4.00918 avg_loss = 3.36410\n",
      "epoch no.3 train no.306640  loss = 2.86007 avg_loss = 3.33333\n",
      "epoch no.3 train no.306650  loss = 4.60160 avg_loss = 3.36597\n",
      "epoch no.3 train no.306660  loss = 4.08320 avg_loss = 3.41446\n",
      "epoch no.3 train no.306670  loss = 3.31293 avg_loss = 3.38752\n",
      "epoch no.3 train no.306680  loss = 2.50660 avg_loss = 3.37160\n",
      "epoch no.3 train no.306690  loss = 2.82578 avg_loss = 3.38652\n",
      "epoch no.3 train no.306700  loss = 3.86346 avg_loss = 3.39754\n",
      "epoch no.3 train no.306710  loss = 3.28704 avg_loss = 3.41093\n",
      "epoch no.3 train no.306720  loss = 2.63679 avg_loss = 3.39946\n",
      "epoch no.3 train no.306730  loss = 2.57986 avg_loss = 3.41079\n",
      "epoch no.3 train no.306740  loss = 2.88503 avg_loss = 3.38088\n",
      "epoch no.3 train no.306750  loss = 4.13033 avg_loss = 3.36918\n",
      "epoch no.3 train no.306760  loss = 2.88032 avg_loss = 3.36890\n",
      "epoch no.3 train no.306770  loss = 2.68694 avg_loss = 3.40861\n",
      "epoch no.3 train no.306780  loss = 3.04536 avg_loss = 3.40463\n",
      "epoch no.3 train no.306790  loss = 6.00863 avg_loss = 3.43424\n",
      "epoch no.3 train no.306800  loss = 2.70192 avg_loss = 3.38720\n",
      "epoch no.3 train no.306810  loss = 3.74531 avg_loss = 3.37126\n",
      "epoch no.3 train no.306820  loss = 2.98322 avg_loss = 3.34967\n",
      "epoch no.3 train no.306830  loss = 3.46482 avg_loss = 3.38508\n",
      "epoch no.3 train no.306840  loss = 2.50051 avg_loss = 3.33021\n",
      "epoch no.3 train no.306850  loss = 3.41898 avg_loss = 3.38401\n",
      "epoch no.3 train no.306860  loss = 3.30538 avg_loss = 3.37332\n",
      "epoch no.3 train no.306870  loss = 2.87802 avg_loss = 3.43582\n",
      "epoch no.3 train no.306880  loss = 2.95841 avg_loss = 3.45645\n",
      "epoch no.3 train no.306890  loss = 4.31621 avg_loss = 3.47169\n",
      "epoch no.3 train no.306900  loss = 2.00482 avg_loss = 3.44794\n",
      "epoch no.3 train no.306910  loss = 4.77257 avg_loss = 3.49881\n",
      "epoch no.3 train no.306920  loss = 5.09928 avg_loss = 3.50354\n",
      "epoch no.3 train no.306930  loss = 3.12220 avg_loss = 3.47131\n",
      "epoch no.3 train no.306940  loss = 5.75729 avg_loss = 3.49000\n",
      "epoch no.3 train no.306950  loss = 4.36314 avg_loss = 3.48823\n",
      "epoch no.3 train no.306960  loss = 2.55222 avg_loss = 3.46652\n",
      "epoch no.3 train no.306970  loss = 1.78921 avg_loss = 3.45944\n",
      "epoch no.3 train no.306980  loss = 4.26487 avg_loss = 3.51772\n",
      "epoch no.3 train no.306990  loss = 3.87762 avg_loss = 3.47809\n",
      "epoch no.3 train no.307000  loss = 3.79975 avg_loss = 3.47524\n",
      "6\n",
      "to_tokens: ['▁비', '좋은', '을', '▁필요할', '▁때', '▁듣는', '▁노래', 'op', '</s>']\n",
      "기분전환이 필요할 때 듣는 pop</s>\n",
      "epoch no.3 train no.307010  loss = 3.17084 avg_loss = 3.41550\n",
      "epoch no.3 train no.307020  loss = 3.52784 avg_loss = 3.45849\n",
      "epoch no.3 train no.307030  loss = 2.56572 avg_loss = 3.42678\n",
      "epoch no.3 train no.307040  loss = 4.11265 avg_loss = 3.46061\n",
      "epoch no.3 train no.307050  loss = 3.40776 avg_loss = 3.44713\n",
      "epoch no.3 train no.307060  loss = 3.86938 avg_loss = 3.48367\n",
      "epoch no.3 train no.307070  loss = 5.24592 avg_loss = 3.51577\n",
      "epoch no.3 train no.307080  loss = 4.16030 avg_loss = 3.58061\n",
      "epoch no.3 train no.307090  loss = 4.86233 avg_loss = 3.60838\n",
      "epoch no.3 train no.307100  loss = 4.81171 avg_loss = 3.59570\n",
      "epoch no.3 train no.307110  loss = 2.62062 avg_loss = 3.54432\n",
      "epoch no.3 train no.307120  loss = 4.04080 avg_loss = 3.56251\n",
      "epoch no.3 train no.307130  loss = 2.58637 avg_loss = 3.57242\n",
      "epoch no.3 train no.307140  loss = 3.35507 avg_loss = 3.56277\n",
      "epoch no.3 train no.307150  loss = 2.27877 avg_loss = 3.57083\n",
      "epoch no.3 train no.307160  loss = 1.52186 avg_loss = 3.51356\n",
      "epoch no.3 train no.307170  loss = 3.11044 avg_loss = 3.49260\n",
      "epoch no.3 train no.307180  loss = 3.53326 avg_loss = 3.51084\n",
      "epoch no.3 train no.307190  loss = 3.66184 avg_loss = 3.52715\n",
      "epoch no.3 train no.307200  loss = 2.91047 avg_loss = 3.57838\n",
      "epoch no.3 train no.307210  loss = 4.07006 avg_loss = 3.56859\n",
      "epoch no.3 train no.307220  loss = 3.88922 avg_loss = 3.53160\n",
      "epoch no.3 train no.307230  loss = 4.51520 avg_loss = 3.50851\n",
      "epoch no.3 train no.307240  loss = 5.52761 avg_loss = 3.52860\n",
      "epoch no.3 train no.307250  loss = 2.36346 avg_loss = 3.45123\n",
      "epoch no.3 train no.307260  loss = 3.12365 avg_loss = 3.47590\n",
      "epoch no.3 train no.307270  loss = 3.94109 avg_loss = 3.49101\n",
      "epoch no.3 train no.307280  loss = 3.47436 avg_loss = 3.48794\n",
      "epoch no.3 train no.307290  loss = 4.15088 avg_loss = 3.54207\n",
      "epoch no.3 train no.307300  loss = 4.66874 avg_loss = 3.55646\n",
      "epoch no.3 train no.307310  loss = 3.12548 avg_loss = 3.51057\n",
      "epoch no.3 train no.307320  loss = 3.15441 avg_loss = 3.50633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.307330  loss = 4.08237 avg_loss = 3.51738\n",
      "epoch no.3 train no.307340  loss = 4.85333 avg_loss = 3.52819\n",
      "epoch no.3 train no.307350  loss = 3.11257 avg_loss = 3.58012\n",
      "epoch no.3 train no.307360  loss = 3.74851 avg_loss = 3.58045\n",
      "epoch no.3 train no.307370  loss = 3.25931 avg_loss = 3.54706\n",
      "epoch no.3 train no.307380  loss = 4.71592 avg_loss = 3.53277\n",
      "epoch no.3 train no.307390  loss = 4.40494 avg_loss = 3.55222\n",
      "epoch no.3 train no.307400  loss = 2.90834 avg_loss = 3.51812\n",
      "epoch no.3 train no.307410  loss = 4.72280 avg_loss = 3.49039\n",
      "epoch no.3 train no.307420  loss = 3.00726 avg_loss = 3.48810\n",
      "epoch no.3 train no.307430  loss = 3.45357 avg_loss = 3.45073\n",
      "epoch no.3 train no.307440  loss = 3.63270 avg_loss = 3.48420\n",
      "epoch no.3 train no.307450  loss = 5.21290 avg_loss = 3.46965\n",
      "epoch no.3 train no.307460  loss = 3.34275 avg_loss = 3.49691\n",
      "epoch no.3 train no.307470  loss = 3.77095 avg_loss = 3.55342\n",
      "epoch no.3 train no.307480  loss = 2.23845 avg_loss = 3.53970\n",
      "epoch no.3 train no.307490  loss = 1.08930 avg_loss = 3.51217\n",
      "epoch no.3 train no.307500  loss = 4.04550 avg_loss = 3.54920\n",
      "epoch no.3 train no.307510  loss = 2.81940 avg_loss = 3.51352\n",
      "epoch no.3 train no.307520  loss = 5.29124 avg_loss = 3.50081\n",
      "epoch no.3 train no.307530  loss = 2.21163 avg_loss = 3.51374\n",
      "epoch no.3 train no.307540  loss = 3.83092 avg_loss = 3.54346\n",
      "epoch no.3 train no.307550  loss = 3.16925 avg_loss = 3.56501\n",
      "epoch no.3 train no.307560  loss = 3.39876 avg_loss = 3.52516\n",
      "epoch no.3 train no.307570  loss = 3.27070 avg_loss = 3.55110\n",
      "epoch no.3 train no.307580  loss = 4.40594 avg_loss = 3.55742\n",
      "epoch no.3 train no.307590  loss = 3.43694 avg_loss = 3.56098\n",
      "epoch no.3 train no.307600  loss = 3.22017 avg_loss = 3.56008\n",
      "epoch no.3 train no.307610  loss = 3.20036 avg_loss = 3.52739\n",
      "epoch no.3 train no.307620  loss = 2.69970 avg_loss = 3.51520\n",
      "epoch no.3 train no.307630  loss = 2.46709 avg_loss = 3.49038\n",
      "epoch no.3 train no.307640  loss = 4.00924 avg_loss = 3.52365\n",
      "epoch no.3 train no.307650  loss = 2.74612 avg_loss = 3.52639\n",
      "epoch no.3 train no.307660  loss = 2.31671 avg_loss = 3.51776\n",
      "epoch no.3 train no.307670  loss = 2.74925 avg_loss = 3.51058\n",
      "epoch no.3 train no.307680  loss = 3.66608 avg_loss = 3.49245\n",
      "epoch no.3 train no.307690  loss = 3.21498 avg_loss = 3.45255\n",
      "epoch no.3 train no.307700  loss = 3.16704 avg_loss = 3.46945\n",
      "epoch no.3 train no.307710  loss = 4.87429 avg_loss = 3.55155\n",
      "epoch no.3 train no.307720  loss = 3.64786 avg_loss = 3.54466\n",
      "epoch no.3 train no.307730  loss = 3.11135 avg_loss = 3.54212\n",
      "epoch no.3 train no.307740  loss = 4.03453 avg_loss = 3.57548\n",
      "epoch no.3 train no.307750  loss = 3.22573 avg_loss = 3.56121\n",
      "epoch no.3 train no.307760  loss = 1.58242 avg_loss = 3.51044\n",
      "epoch no.3 train no.307770  loss = 3.36395 avg_loss = 3.55005\n",
      "epoch no.3 train no.307780  loss = 1.15263 avg_loss = 3.49013\n",
      "epoch no.3 train no.307790  loss = 2.61459 avg_loss = 3.51088\n",
      "epoch no.3 train no.307800  loss = 3.15420 avg_loss = 3.51134\n",
      "epoch no.3 train no.307810  loss = 3.53977 avg_loss = 3.49623\n",
      "epoch no.3 train no.307820  loss = 3.93434 avg_loss = 3.48000\n",
      "epoch no.3 train no.307830  loss = 4.06778 avg_loss = 3.52743\n",
      "epoch no.3 train no.307840  loss = 4.29340 avg_loss = 3.59796\n",
      "epoch no.3 train no.307850  loss = 2.91321 avg_loss = 3.60077\n",
      "epoch no.3 train no.307860  loss = 2.40263 avg_loss = 3.55324\n",
      "epoch no.3 train no.307870  loss = 4.53367 avg_loss = 3.57974\n",
      "epoch no.3 train no.307880  loss = 4.11094 avg_loss = 3.55685\n",
      "epoch no.3 train no.307890  loss = 3.52074 avg_loss = 3.54919\n",
      "epoch no.3 train no.307900  loss = 3.72500 avg_loss = 3.55479\n",
      "epoch no.3 train no.307910  loss = 3.27380 avg_loss = 3.52215\n",
      "epoch no.3 train no.307920  loss = 1.90266 avg_loss = 3.51101\n",
      "epoch no.3 train no.307930  loss = 3.73927 avg_loss = 3.46569\n",
      "epoch no.3 train no.307940  loss = 2.34924 avg_loss = 3.45373\n",
      "epoch no.3 train no.307950  loss = 3.32101 avg_loss = 3.47274\n",
      "epoch no.3 train no.307960  loss = 2.06590 avg_loss = 3.45655\n",
      "epoch no.3 train no.307970  loss = 3.65493 avg_loss = 3.41263\n",
      "epoch no.3 train no.307980  loss = 4.23647 avg_loss = 3.42932\n",
      "epoch no.3 train no.307990  loss = 3.44144 avg_loss = 3.39491\n",
      "epoch no.3 train no.308000  loss = 3.87596 avg_loss = 3.41348\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '▁밤', '</s>']\n",
      "기분전환이 필요한 밤</s>\n",
      "epoch no.3 train no.308010  loss = 2.46341 avg_loss = 3.44083\n",
      "epoch no.3 train no.308020  loss = 2.34210 avg_loss = 3.46872\n",
      "epoch no.3 train no.308030  loss = 4.99357 avg_loss = 3.46973\n",
      "epoch no.3 train no.308040  loss = 3.32455 avg_loss = 3.47830\n",
      "epoch no.3 train no.308050  loss = 3.03375 avg_loss = 3.49511\n",
      "epoch no.3 train no.308060  loss = 2.35195 avg_loss = 3.46217\n",
      "epoch no.3 train no.308070  loss = 3.79712 avg_loss = 3.49816\n",
      "epoch no.3 train no.308080  loss = 1.88612 avg_loss = 3.51318\n",
      "epoch no.3 train no.308090  loss = 3.36028 avg_loss = 3.48182\n",
      "epoch no.3 train no.308100  loss = 3.36743 avg_loss = 3.46005\n",
      "epoch no.3 train no.308110  loss = 3.18253 avg_loss = 3.50036\n",
      "epoch no.3 train no.308120  loss = 4.57730 avg_loss = 3.53237\n",
      "epoch no.3 train no.308130  loss = 4.70697 avg_loss = 3.56645\n",
      "epoch no.3 train no.308140  loss = 3.38283 avg_loss = 3.52369\n",
      "epoch no.3 train no.308150  loss = 3.06886 avg_loss = 3.52757\n",
      "epoch no.3 train no.308160  loss = 3.50383 avg_loss = 3.49496\n",
      "epoch no.3 train no.308170  loss = 3.41101 avg_loss = 3.55594\n",
      "epoch no.3 train no.308180  loss = 2.22260 avg_loss = 3.48656\n",
      "epoch no.3 train no.308190  loss = 3.90833 avg_loss = 3.50114\n",
      "epoch no.3 train no.308200  loss = 4.30062 avg_loss = 3.46583\n",
      "epoch no.3 train no.308210  loss = 2.71158 avg_loss = 3.44664\n",
      "epoch no.3 train no.308220  loss = 2.35033 avg_loss = 3.41960\n",
      "epoch no.3 train no.308230  loss = 2.90513 avg_loss = 3.47649\n",
      "epoch no.3 train no.308240  loss = 2.77557 avg_loss = 3.49826\n",
      "epoch no.3 train no.308250  loss = 2.75442 avg_loss = 3.45216\n",
      "epoch no.3 train no.308260  loss = 3.19031 avg_loss = 3.39951\n",
      "epoch no.3 train no.308270  loss = 3.27196 avg_loss = 3.41619\n",
      "epoch no.3 train no.308280  loss = 2.81889 avg_loss = 3.42090\n",
      "epoch no.3 train no.308290  loss = 3.21555 avg_loss = 3.40328\n",
      "epoch no.3 train no.308300  loss = 2.67623 avg_loss = 3.44160\n",
      "epoch no.3 train no.308310  loss = 4.02720 avg_loss = 3.44209\n",
      "epoch no.3 train no.308320  loss = 4.03283 avg_loss = 3.47104\n",
      "epoch no.3 train no.308330  loss = 2.35405 avg_loss = 3.48832\n",
      "epoch no.3 train no.308340  loss = 2.89607 avg_loss = 3.48182\n",
      "epoch no.3 train no.308350  loss = 3.85155 avg_loss = 3.51576\n",
      "epoch no.3 train no.308360  loss = 2.24934 avg_loss = 3.48588\n",
      "epoch no.3 train no.308370  loss = 4.27427 avg_loss = 3.55430\n",
      "epoch no.3 train no.308380  loss = 3.17577 avg_loss = 3.57584\n",
      "epoch no.3 train no.308390  loss = 3.16370 avg_loss = 3.54201\n",
      "epoch no.3 train no.308400  loss = 4.56410 avg_loss = 3.51281\n",
      "epoch no.3 train no.308410  loss = 3.92101 avg_loss = 3.52808\n",
      "epoch no.3 train no.308420  loss = 4.02102 avg_loss = 3.52105\n",
      "epoch no.3 train no.308430  loss = 2.99323 avg_loss = 3.53411\n",
      "epoch no.3 train no.308440  loss = 3.58077 avg_loss = 3.53201\n",
      "epoch no.3 train no.308450  loss = 1.71662 avg_loss = 3.49433\n",
      "epoch no.3 train no.308460  loss = 3.66546 avg_loss = 3.51786\n",
      "epoch no.3 train no.308470  loss = 3.72139 avg_loss = 3.48605\n",
      "epoch no.3 train no.308480  loss = 2.69391 avg_loss = 3.50354\n",
      "epoch no.3 train no.308490  loss = 3.71290 avg_loss = 3.51982\n",
      "epoch no.3 train no.308500  loss = 3.21779 avg_loss = 3.51833\n",
      "epoch no.3 train no.308510  loss = 5.58539 avg_loss = 3.56888\n",
      "epoch no.3 train no.308520  loss = 3.69543 avg_loss = 3.56924\n",
      "epoch no.3 train no.308530  loss = 2.62986 avg_loss = 3.52592\n",
      "epoch no.3 train no.308540  loss = 3.15132 avg_loss = 3.50399\n",
      "epoch no.3 train no.308550  loss = 3.54800 avg_loss = 3.49843\n",
      "epoch no.3 train no.308560  loss = 4.17319 avg_loss = 3.46450\n",
      "epoch no.3 train no.308570  loss = 2.76059 avg_loss = 3.43195\n",
      "epoch no.3 train no.308580  loss = 4.07363 avg_loss = 3.42226\n",
      "epoch no.3 train no.308590  loss = 3.19592 avg_loss = 3.41498\n",
      "epoch no.3 train no.308600  loss = 5.56674 avg_loss = 3.46020\n",
      "epoch no.3 train no.308610  loss = 3.22754 avg_loss = 3.46751\n",
      "epoch no.3 train no.308620  loss = 2.51614 avg_loss = 3.42838\n",
      "epoch no.3 train no.308630  loss = 3.15693 avg_loss = 3.44899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.308640  loss = 3.57891 avg_loss = 3.46697\n",
      "epoch no.3 train no.308650  loss = 3.59776 avg_loss = 3.52723\n",
      "epoch no.3 train no.308660  loss = 3.56333 avg_loss = 3.49103\n",
      "epoch no.3 train no.308670  loss = 2.93848 avg_loss = 3.49708\n",
      "epoch no.3 train no.308680  loss = 3.70244 avg_loss = 3.51869\n",
      "epoch no.3 train no.308690  loss = 4.50209 avg_loss = 3.54587\n",
      "epoch no.3 train no.308700  loss = 2.96750 avg_loss = 3.49814\n",
      "epoch no.3 train no.308710  loss = 2.87414 avg_loss = 3.52929\n",
      "epoch no.3 train no.308720  loss = 2.75341 avg_loss = 3.46147\n",
      "epoch no.3 train no.308730  loss = 2.99110 avg_loss = 3.49479\n",
      "epoch no.3 train no.308740  loss = 2.61856 avg_loss = 3.47923\n",
      "epoch no.3 train no.308750  loss = 2.39583 avg_loss = 3.43623\n",
      "epoch no.3 train no.308760  loss = 2.68320 avg_loss = 3.44868\n",
      "epoch no.3 train no.308770  loss = 3.01356 avg_loss = 3.48043\n",
      "epoch no.3 train no.308780  loss = 2.64352 avg_loss = 3.50110\n",
      "epoch no.3 train no.308790  loss = 1.52966 avg_loss = 3.47788\n",
      "epoch no.3 train no.308800  loss = 2.09202 avg_loss = 3.48521\n",
      "epoch no.3 train no.308810  loss = 3.45513 avg_loss = 3.50529\n",
      "epoch no.3 train no.308820  loss = 2.98465 avg_loss = 3.48343\n",
      "epoch no.3 train no.308830  loss = 3.79316 avg_loss = 3.53707\n",
      "epoch no.3 train no.308840  loss = 4.85073 avg_loss = 3.57008\n",
      "epoch no.3 train no.308850  loss = 2.06557 avg_loss = 3.52480\n",
      "epoch no.3 train no.308860  loss = 4.64819 avg_loss = 3.53500\n",
      "epoch no.3 train no.308870  loss = 3.03174 avg_loss = 3.48880\n",
      "epoch no.3 train no.308880  loss = 3.10243 avg_loss = 3.49368\n",
      "epoch no.3 train no.308890  loss = 3.31473 avg_loss = 3.46286\n",
      "epoch no.3 train no.308900  loss = 3.42187 avg_loss = 3.45821\n",
      "epoch no.3 train no.308910  loss = 3.93845 avg_loss = 3.49170\n",
      "epoch no.3 train no.308920  loss = 3.16119 avg_loss = 3.53430\n",
      "epoch no.3 train no.308930  loss = 4.51433 avg_loss = 3.53846\n",
      "epoch no.3 train no.308940  loss = 4.20119 avg_loss = 3.53823\n",
      "epoch no.3 train no.308950  loss = 2.54002 avg_loss = 3.51675\n",
      "epoch no.3 train no.308960  loss = 3.95330 avg_loss = 3.52270\n",
      "epoch no.3 train no.308970  loss = 2.27744 avg_loss = 3.49691\n",
      "epoch no.3 train no.308980  loss = 2.68858 avg_loss = 3.48680\n",
      "epoch no.3 train no.308990  loss = 3.59274 avg_loss = 3.50805\n",
      "epoch no.3 train no.309000  loss = 3.13095 avg_loss = 3.51607\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.3 train no.309010  loss = 4.52389 avg_loss = 3.48855\n",
      "epoch no.3 train no.309020  loss = 4.45151 avg_loss = 3.50297\n",
      "epoch no.3 train no.309030  loss = 3.14891 avg_loss = 3.47959\n",
      "epoch no.3 train no.309040  loss = 3.79033 avg_loss = 3.45756\n",
      "epoch no.3 train no.309050  loss = 2.87723 avg_loss = 3.39907\n",
      "epoch no.3 train no.309060  loss = 2.48275 avg_loss = 3.42016\n",
      "epoch no.3 train no.309070  loss = 3.55587 avg_loss = 3.46714\n",
      "epoch no.3 train no.309080  loss = 3.72265 avg_loss = 3.51040\n",
      "epoch no.3 train no.309090  loss = 4.36711 avg_loss = 3.54262\n",
      "epoch no.3 train no.309100  loss = 2.33839 avg_loss = 3.55958\n",
      "epoch no.3 train no.309110  loss = 5.97117 avg_loss = 3.60523\n",
      "epoch no.3 train no.309120  loss = 3.16827 avg_loss = 3.59329\n",
      "epoch no.3 train no.309130  loss = 2.93311 avg_loss = 3.53174\n",
      "epoch no.3 train no.309140  loss = 3.39519 avg_loss = 3.54643\n",
      "epoch no.3 train no.309150  loss = 4.06304 avg_loss = 3.51635\n",
      "epoch no.3 train no.309160  loss = 2.46196 avg_loss = 3.52242\n",
      "epoch no.3 train no.309170  loss = 3.52816 avg_loss = 3.54071\n",
      "epoch no.3 train no.309180  loss = 4.48350 avg_loss = 3.57468\n",
      "epoch no.3 train no.309190  loss = 2.19284 avg_loss = 3.55056\n",
      "epoch no.3 train no.309200  loss = 2.73076 avg_loss = 3.48453\n",
      "epoch no.3 train no.309210  loss = 2.71005 avg_loss = 3.48546\n",
      "epoch no.3 train no.309220  loss = 6.55152 avg_loss = 3.52509\n",
      "epoch no.3 train no.309230  loss = 3.18046 avg_loss = 3.55108\n",
      "epoch no.3 train no.309240  loss = 3.25616 avg_loss = 3.53348\n",
      "epoch no.3 train no.309250  loss = 2.67970 avg_loss = 3.55675\n",
      "epoch no.3 train no.309260  loss = 3.02144 avg_loss = 3.57015\n",
      "epoch no.3 train no.309270  loss = 3.53474 avg_loss = 3.56795\n",
      "epoch no.3 train no.309280  loss = 2.48367 avg_loss = 3.53785\n",
      "epoch no.3 train no.309290  loss = 4.80000 avg_loss = 3.57027\n",
      "epoch no.3 train no.309300  loss = 2.07102 avg_loss = 3.60495\n",
      "epoch no.3 train no.309310  loss = 3.10973 avg_loss = 3.60666\n",
      "epoch no.3 train no.309320  loss = 3.74403 avg_loss = 3.63558\n",
      "epoch no.3 train no.309330  loss = 4.22807 avg_loss = 3.62115\n",
      "epoch no.3 train no.309340  loss = 3.30058 avg_loss = 3.59989\n",
      "epoch no.3 train no.309350  loss = 5.31271 avg_loss = 3.63410\n",
      "epoch no.3 train no.309360  loss = 2.14248 avg_loss = 3.60043\n",
      "epoch no.3 train no.309370  loss = 3.01052 avg_loss = 3.62021\n",
      "epoch no.3 train no.309380  loss = 4.15477 avg_loss = 3.59596\n",
      "epoch no.3 train no.309390  loss = 2.93914 avg_loss = 3.55502\n",
      "epoch no.3 train no.309400  loss = 1.53407 avg_loss = 3.56031\n",
      "epoch no.3 train no.309410  loss = 3.68626 avg_loss = 3.52755\n",
      "epoch no.3 train no.309420  loss = 2.92238 avg_loss = 3.49367\n",
      "epoch no.3 train no.309430  loss = 3.84710 avg_loss = 3.51464\n",
      "epoch no.3 train no.309440  loss = 2.80198 avg_loss = 3.49063\n",
      "epoch no.3 train no.309450  loss = 3.68499 avg_loss = 3.48802\n",
      "epoch no.3 train no.309460  loss = 5.65257 avg_loss = 3.45913\n",
      "epoch no.3 train no.309470  loss = 3.25660 avg_loss = 3.48590\n",
      "epoch no.3 train no.309480  loss = 4.38406 avg_loss = 3.48867\n",
      "epoch no.3 train no.309490  loss = 3.82457 avg_loss = 3.55591\n",
      "epoch no.3 train no.309500  loss = 2.88882 avg_loss = 3.57301\n",
      "epoch no.3 train no.309510  loss = 2.22690 avg_loss = 3.56971\n",
      "epoch no.3 train no.309520  loss = 2.05799 avg_loss = 3.50470\n",
      "epoch no.3 train no.309530  loss = 4.60383 avg_loss = 3.54361\n",
      "epoch no.3 train no.309540  loss = 3.89359 avg_loss = 3.56312\n",
      "epoch no.3 train no.309550  loss = 2.44330 avg_loss = 3.51212\n",
      "epoch no.3 train no.309560  loss = 3.62505 avg_loss = 3.50666\n",
      "epoch no.3 train no.309570  loss = 4.09575 avg_loss = 3.52309\n",
      "epoch no.3 train no.309580  loss = 3.52229 avg_loss = 3.48906\n",
      "epoch no.3 train no.309590  loss = 2.23107 avg_loss = 3.45776\n",
      "epoch no.3 train no.309600  loss = 4.25009 avg_loss = 3.47353\n",
      "epoch no.3 train no.309610  loss = 2.49299 avg_loss = 3.49589\n",
      "epoch no.3 train no.309620  loss = 3.79698 avg_loss = 3.49389\n",
      "epoch no.3 train no.309630  loss = 3.08842 avg_loss = 3.54005\n",
      "epoch no.3 train no.309640  loss = 2.02639 avg_loss = 3.53955\n",
      "epoch no.3 train no.309650  loss = 2.99109 avg_loss = 3.53351\n",
      "epoch no.3 train no.309660  loss = 2.85762 avg_loss = 3.55004\n",
      "epoch no.3 train no.309670  loss = 3.84215 avg_loss = 3.51132\n",
      "epoch no.3 train no.309680  loss = 3.84117 avg_loss = 3.46985\n",
      "epoch no.3 train no.309690  loss = 2.59141 avg_loss = 3.44441\n",
      "epoch no.3 train no.309700  loss = 3.05732 avg_loss = 3.45351\n",
      "epoch no.3 train no.309710  loss = 3.67671 avg_loss = 3.46471\n",
      "epoch no.3 train no.309720  loss = 5.18625 avg_loss = 3.48200\n",
      "epoch no.3 train no.309730  loss = 2.62105 avg_loss = 3.47808\n",
      "epoch no.3 train no.309740  loss = 3.02682 avg_loss = 3.43952\n",
      "epoch no.3 train no.309750  loss = 4.89834 avg_loss = 3.50919\n",
      "epoch no.3 train no.309760  loss = 2.88973 avg_loss = 3.48108\n",
      "epoch no.3 train no.309770  loss = 3.66282 avg_loss = 3.45775\n",
      "epoch no.3 train no.309780  loss = 3.79770 avg_loss = 3.50226\n",
      "epoch no.3 train no.309790  loss = 2.91046 avg_loss = 3.46020\n",
      "epoch no.3 train no.309800  loss = 3.06069 avg_loss = 3.44583\n",
      "epoch no.3 train no.309810  loss = 3.91380 avg_loss = 3.40336\n",
      "epoch no.3 train no.309820  loss = 1.74978 avg_loss = 3.36096\n",
      "epoch no.3 train no.309830  loss = 6.20990 avg_loss = 3.41362\n",
      "epoch no.3 train no.309840  loss = 3.57706 avg_loss = 3.39198\n",
      "epoch no.3 train no.309850  loss = 4.97264 avg_loss = 3.41400\n",
      "epoch no.3 train no.309860  loss = 3.41474 avg_loss = 3.38831\n",
      "epoch no.3 train no.309870  loss = 2.85779 avg_loss = 3.41040\n",
      "epoch no.3 train no.309880  loss = 2.48869 avg_loss = 3.36580\n",
      "epoch no.3 train no.309890  loss = 4.85731 avg_loss = 3.39200\n",
      "epoch no.3 train no.309900  loss = 3.39327 avg_loss = 3.35765\n",
      "epoch no.3 train no.309910  loss = 5.17380 avg_loss = 3.34901\n",
      "epoch no.3 train no.309920  loss = 3.61952 avg_loss = 3.41658\n",
      "epoch no.3 train no.309930  loss = 4.99049 avg_loss = 3.46613\n",
      "epoch no.3 train no.309940  loss = 3.00042 avg_loss = 3.44115\n",
      "epoch no.3 train no.309950  loss = 2.82405 avg_loss = 3.48550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.309960  loss = 2.21952 avg_loss = 3.47454\n",
      "epoch no.3 train no.309970  loss = 2.13497 avg_loss = 3.45602\n",
      "epoch no.3 train no.309980  loss = 4.62578 avg_loss = 3.50156\n",
      "epoch no.3 train no.309990  loss = 3.12733 avg_loss = 3.48281\n",
      "epoch no.3 train no.310000  loss = 3.85228 avg_loss = 3.48195\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '▁팝', '송']\n",
      "기분전환을 위한 감각적인 팝</s>\n",
      "epoch no.3 train no.310010  loss = 2.59807 avg_loss = 3.48444\n",
      "epoch no.3 train no.310020  loss = 3.30854 avg_loss = 3.45867\n",
      "epoch no.3 train no.310030  loss = 3.27330 avg_loss = 3.43198\n",
      "epoch no.3 train no.310040  loss = 3.19669 avg_loss = 3.48137\n",
      "epoch no.3 train no.310050  loss = 3.67365 avg_loss = 3.50504\n",
      "epoch no.3 train no.310060  loss = 3.93002 avg_loss = 3.50337\n",
      "epoch no.3 train no.310070  loss = 3.40508 avg_loss = 3.46896\n",
      "epoch no.3 train no.310080  loss = 3.31629 avg_loss = 3.49273\n",
      "epoch no.3 train no.310090  loss = 2.65073 avg_loss = 3.46583\n",
      "epoch no.3 train no.310100  loss = 3.23102 avg_loss = 3.43878\n",
      "epoch no.3 train no.310110  loss = 3.34622 avg_loss = 3.44460\n",
      "epoch no.3 train no.310120  loss = 4.18340 avg_loss = 3.46139\n",
      "epoch no.3 train no.310130  loss = 2.82068 avg_loss = 3.43098\n",
      "epoch no.3 train no.310140  loss = 2.38585 avg_loss = 3.38318\n",
      "epoch no.3 train no.310150  loss = 2.57477 avg_loss = 3.34232\n",
      "epoch no.3 train no.310160  loss = 3.36359 avg_loss = 3.36712\n",
      "epoch no.3 train no.310170  loss = 4.47315 avg_loss = 3.40443\n",
      "epoch no.3 train no.310180  loss = 2.59683 avg_loss = 3.36716\n",
      "epoch no.3 train no.310190  loss = 3.42511 avg_loss = 3.34434\n",
      "epoch no.3 train no.310200  loss = 2.30112 avg_loss = 3.33064\n",
      "epoch no.3 train no.310210  loss = 3.52698 avg_loss = 3.35689\n",
      "epoch no.3 train no.310220  loss = 3.80442 avg_loss = 3.37572\n",
      "epoch no.3 train no.310230  loss = 2.53552 avg_loss = 3.36597\n",
      "epoch no.3 train no.310240  loss = 3.12106 avg_loss = 3.33270\n",
      "epoch no.3 train no.310250  loss = 3.17816 avg_loss = 3.39810\n",
      "epoch no.3 train no.310260  loss = 2.50436 avg_loss = 3.43164\n",
      "epoch no.3 train no.310270  loss = 3.02677 avg_loss = 3.50507\n",
      "epoch no.3 train no.310280  loss = 4.00914 avg_loss = 3.49953\n",
      "epoch no.3 train no.310290  loss = 2.62813 avg_loss = 3.53039\n",
      "epoch no.3 train no.310300  loss = 3.27815 avg_loss = 3.55087\n",
      "epoch no.3 train no.310310  loss = 3.45280 avg_loss = 3.55672\n",
      "epoch no.3 train no.310320  loss = 5.15694 avg_loss = 3.49486\n",
      "epoch no.3 train no.310330  loss = 3.43900 avg_loss = 3.52301\n",
      "epoch no.3 train no.310340  loss = 4.98642 avg_loss = 3.59836\n",
      "epoch no.3 train no.310350  loss = 3.93277 avg_loss = 3.56012\n",
      "epoch no.3 train no.310360  loss = 4.77850 avg_loss = 3.53510\n",
      "epoch no.3 train no.310370  loss = 2.35383 avg_loss = 3.52764\n",
      "epoch no.3 train no.310380  loss = 3.55892 avg_loss = 3.56146\n",
      "epoch no.3 train no.310390  loss = 3.09814 avg_loss = 3.51693\n",
      "epoch no.3 train no.310400  loss = 3.29341 avg_loss = 3.49376\n",
      "epoch no.3 train no.310410  loss = 3.36869 avg_loss = 3.45625\n",
      "epoch no.3 train no.310420  loss = 3.79029 avg_loss = 3.50933\n",
      "epoch no.3 train no.310430  loss = 2.85428 avg_loss = 3.50815\n",
      "epoch no.3 train no.310440  loss = 2.08096 avg_loss = 3.50226\n",
      "epoch no.3 train no.310450  loss = 1.84768 avg_loss = 3.51147\n",
      "epoch no.3 train no.310460  loss = 3.04733 avg_loss = 3.50503\n",
      "epoch no.3 train no.310470  loss = 5.83013 avg_loss = 3.51321\n",
      "epoch no.3 train no.310480  loss = 2.42497 avg_loss = 3.42650\n",
      "epoch no.3 train no.310490  loss = 3.53608 avg_loss = 3.42307\n",
      "epoch no.3 train no.310500  loss = 3.61032 avg_loss = 3.40314\n",
      "epoch no.3 train no.310510  loss = 3.42520 avg_loss = 3.43509\n",
      "epoch no.3 train no.310520  loss = 2.03999 avg_loss = 3.45591\n",
      "epoch no.3 train no.310530  loss = 3.28277 avg_loss = 3.47109\n",
      "epoch no.3 train no.310540  loss = 4.26903 avg_loss = 3.48079\n",
      "epoch no.3 train no.310550  loss = 3.50533 avg_loss = 3.47784\n",
      "epoch no.3 train no.310560  loss = 4.49796 avg_loss = 3.48867\n",
      "epoch no.3 train no.310570  loss = 3.15350 avg_loss = 3.46586\n",
      "epoch no.3 train no.310580  loss = 4.47437 avg_loss = 3.46959\n",
      "epoch no.3 train no.310590  loss = 3.93447 avg_loss = 3.44726\n",
      "epoch no.3 train no.310600  loss = 3.92681 avg_loss = 3.44761\n",
      "epoch no.3 train no.310610  loss = 5.01397 avg_loss = 3.45585\n",
      "epoch no.3 train no.310620  loss = 4.72803 avg_loss = 3.48934\n",
      "epoch no.3 train no.310630  loss = 1.48940 avg_loss = 3.48727\n",
      "epoch no.3 train no.310640  loss = 5.11330 avg_loss = 3.49151\n",
      "epoch no.3 train no.310650  loss = 5.17562 avg_loss = 3.49092\n",
      "epoch no.3 train no.310660  loss = 2.59827 avg_loss = 3.50166\n",
      "epoch no.3 train no.310670  loss = 3.90729 avg_loss = 3.52153\n",
      "epoch no.3 train no.310680  loss = 3.45579 avg_loss = 3.47155\n",
      "epoch no.3 train no.310690  loss = 3.31715 avg_loss = 3.45399\n",
      "epoch no.3 train no.310700  loss = 3.02099 avg_loss = 3.47021\n",
      "epoch no.3 train no.310710  loss = 2.42131 avg_loss = 3.47157\n",
      "epoch no.3 train no.310720  loss = 2.99341 avg_loss = 3.44469\n",
      "epoch no.3 train no.310730  loss = 3.91508 avg_loss = 3.44256\n",
      "epoch no.3 train no.310740  loss = 2.20030 avg_loss = 3.42077\n",
      "epoch no.3 train no.310750  loss = 4.18077 avg_loss = 3.48032\n",
      "epoch no.3 train no.310760  loss = 3.25229 avg_loss = 3.45563\n",
      "epoch no.3 train no.310770  loss = 2.93593 avg_loss = 3.43920\n",
      "epoch no.3 train no.310780  loss = 4.32907 avg_loss = 3.47516\n",
      "epoch no.3 train no.310790  loss = 3.76067 avg_loss = 3.52869\n",
      "epoch no.3 train no.310800  loss = 1.88510 avg_loss = 3.48057\n",
      "epoch no.3 train no.310810  loss = 4.32841 avg_loss = 3.49489\n",
      "epoch no.3 train no.310820  loss = 4.41818 avg_loss = 3.46408\n",
      "epoch no.3 train no.310830  loss = 3.13281 avg_loss = 3.53500\n",
      "epoch no.3 train no.310840  loss = 3.94739 avg_loss = 3.53014\n",
      "epoch no.3 train no.310850  loss = 3.56278 avg_loss = 3.53871\n",
      "epoch no.3 train no.310860  loss = 2.00142 avg_loss = 3.57713\n",
      "epoch no.3 train no.310870  loss = 3.52427 avg_loss = 3.55141\n",
      "epoch no.3 train no.310880  loss = 4.71679 avg_loss = 3.53889\n",
      "epoch no.3 train no.310890  loss = 3.84068 avg_loss = 3.56967\n",
      "epoch no.3 train no.310900  loss = 4.82192 avg_loss = 3.55375\n",
      "epoch no.3 train no.310910  loss = 1.58681 avg_loss = 3.54461\n",
      "epoch no.3 train no.310920  loss = 2.22321 avg_loss = 3.49049\n",
      "epoch no.3 train no.310930  loss = 2.43399 avg_loss = 3.50293\n",
      "epoch no.3 train no.310940  loss = 4.36750 avg_loss = 3.47342\n",
      "epoch no.3 train no.310950  loss = 3.74317 avg_loss = 3.54815\n",
      "epoch no.3 train no.310960  loss = 4.76643 avg_loss = 3.55403\n",
      "epoch no.3 train no.310970  loss = 3.59305 avg_loss = 3.58596\n",
      "epoch no.3 train no.310980  loss = 3.20977 avg_loss = 3.57089\n",
      "epoch no.3 train no.310990  loss = 3.11845 avg_loss = 3.53116\n",
      "epoch no.3 train no.311000  loss = 4.04509 avg_loss = 3.52044\n",
      "5\n",
      "to_tokens: ['▁가을', '전환', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 음악</s>\n",
      "epoch no.3 train no.311010  loss = 2.27839 avg_loss = 3.55456\n",
      "epoch no.3 train no.311020  loss = 2.56748 avg_loss = 3.54498\n",
      "epoch no.3 train no.311030  loss = 6.30816 avg_loss = 3.60992\n",
      "epoch no.3 train no.311040  loss = 3.08292 avg_loss = 3.60924\n",
      "epoch no.3 train no.311050  loss = 5.17036 avg_loss = 3.61039\n",
      "epoch no.3 train no.311060  loss = 3.51786 avg_loss = 3.62323\n",
      "epoch no.3 train no.311070  loss = 3.38487 avg_loss = 3.59977\n",
      "epoch no.3 train no.311080  loss = 3.95888 avg_loss = 3.57369\n",
      "epoch no.3 train no.311090  loss = 1.89341 avg_loss = 3.55172\n",
      "epoch no.3 train no.311100  loss = 1.81538 avg_loss = 3.54073\n",
      "epoch no.3 train no.311110  loss = 2.72140 avg_loss = 3.50153\n",
      "epoch no.3 train no.311120  loss = 3.97308 avg_loss = 3.45030\n",
      "epoch no.3 train no.311130  loss = 2.60925 avg_loss = 3.44589\n",
      "epoch no.3 train no.311140  loss = 2.48115 avg_loss = 3.45409\n",
      "epoch no.3 train no.311150  loss = 2.78678 avg_loss = 3.41956\n",
      "epoch no.3 train no.311160  loss = 2.65035 avg_loss = 3.41165\n",
      "epoch no.3 train no.311170  loss = 4.69144 avg_loss = 3.45090\n",
      "epoch no.3 train no.311180  loss = 2.88404 avg_loss = 3.48705\n",
      "epoch no.3 train no.311190  loss = 4.11723 avg_loss = 3.47658\n",
      "epoch no.3 train no.311200  loss = 2.59448 avg_loss = 3.44733\n",
      "epoch no.3 train no.311210  loss = 3.77369 avg_loss = 3.48167\n",
      "epoch no.3 train no.311220  loss = 5.25373 avg_loss = 3.47321\n",
      "epoch no.3 train no.311230  loss = 2.66303 avg_loss = 3.45142\n",
      "epoch no.3 train no.311240  loss = 3.38214 avg_loss = 3.46383\n",
      "epoch no.3 train no.311250  loss = 3.78679 avg_loss = 3.46986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.311260  loss = 2.42258 avg_loss = 3.47429\n",
      "epoch no.3 train no.311270  loss = 4.05384 avg_loss = 3.50284\n",
      "epoch no.3 train no.311280  loss = 4.13790 avg_loss = 3.48669\n",
      "epoch no.3 train no.311290  loss = 3.98965 avg_loss = 3.49682\n",
      "epoch no.3 train no.311300  loss = 2.68343 avg_loss = 3.46721\n",
      "epoch no.3 train no.311310  loss = 2.56236 avg_loss = 3.46259\n",
      "epoch no.3 train no.311320  loss = 3.76292 avg_loss = 3.49094\n",
      "epoch no.3 train no.311330  loss = 3.93569 avg_loss = 3.50122\n",
      "epoch no.3 train no.311340  loss = 4.25929 avg_loss = 3.49603\n",
      "epoch no.3 train no.311350  loss = 4.26352 avg_loss = 3.50656\n",
      "epoch no.3 train no.311360  loss = 4.81325 avg_loss = 3.48031\n",
      "epoch no.3 train no.311370  loss = 4.27256 avg_loss = 3.46228\n",
      "epoch no.3 train no.311380  loss = 4.57935 avg_loss = 3.44633\n",
      "epoch no.3 train no.311390  loss = 5.86760 avg_loss = 3.48521\n",
      "epoch no.3 train no.311400  loss = 3.42302 avg_loss = 3.48175\n",
      "epoch no.3 train no.311410  loss = 2.38246 avg_loss = 3.45430\n",
      "epoch no.3 train no.311420  loss = 1.67024 avg_loss = 3.44578\n",
      "epoch no.3 train no.311430  loss = 3.66519 avg_loss = 3.44498\n",
      "epoch no.3 train no.311440  loss = 2.87209 avg_loss = 3.42970\n",
      "epoch no.3 train no.311450  loss = 2.87193 avg_loss = 3.42774\n",
      "epoch no.3 train no.311460  loss = 3.42352 avg_loss = 3.40624\n",
      "epoch no.3 train no.311470  loss = 3.88266 avg_loss = 3.42836\n",
      "epoch no.3 train no.311480  loss = 3.82531 avg_loss = 3.41289\n",
      "epoch no.3 train no.311490  loss = 3.13435 avg_loss = 3.38777\n",
      "epoch no.3 train no.311500  loss = 3.29327 avg_loss = 3.41018\n",
      "epoch no.3 train no.311510  loss = 2.95774 avg_loss = 3.41605\n",
      "epoch no.3 train no.311520  loss = 3.01346 avg_loss = 3.39805\n",
      "epoch no.3 train no.311530  loss = 4.13270 avg_loss = 3.37985\n",
      "epoch no.3 train no.311540  loss = 3.30054 avg_loss = 3.35863\n",
      "epoch no.3 train no.311550  loss = 3.97722 avg_loss = 3.38775\n",
      "epoch no.3 train no.311560  loss = 3.51981 avg_loss = 3.40700\n",
      "epoch no.3 train no.311570  loss = 3.09531 avg_loss = 3.42439\n",
      "epoch no.3 train no.311580  loss = 2.41912 avg_loss = 3.41180\n",
      "epoch no.3 train no.311590  loss = 3.58713 avg_loss = 3.43254\n",
      "epoch no.3 train no.311600  loss = 4.11712 avg_loss = 3.41253\n",
      "epoch no.3 train no.311610  loss = 3.58249 avg_loss = 3.38246\n",
      "epoch no.3 train no.311620  loss = 2.59065 avg_loss = 3.45895\n",
      "epoch no.3 train no.311630  loss = 2.90588 avg_loss = 3.49952\n",
      "epoch no.3 train no.311640  loss = 4.29832 avg_loss = 3.54547\n",
      "epoch no.3 train no.311650  loss = 4.83648 avg_loss = 3.54387\n",
      "epoch no.3 train no.311660  loss = 2.49560 avg_loss = 3.50783\n",
      "epoch no.3 train no.311670  loss = 2.69386 avg_loss = 3.49157\n",
      "epoch no.3 train no.311680  loss = 3.18005 avg_loss = 3.48167\n",
      "epoch no.3 train no.311690  loss = 4.90343 avg_loss = 3.45882\n",
      "epoch no.3 train no.311700  loss = 4.36175 avg_loss = 3.46317\n",
      "epoch no.3 train no.311710  loss = 2.42980 avg_loss = 3.46667\n",
      "epoch no.3 train no.311720  loss = 3.29075 avg_loss = 3.47357\n",
      "epoch no.3 train no.311730  loss = 2.97148 avg_loss = 3.45439\n",
      "epoch no.3 train no.311740  loss = 2.76515 avg_loss = 3.42623\n",
      "epoch no.3 train no.311750  loss = 3.07326 avg_loss = 3.43457\n",
      "epoch no.3 train no.311760  loss = 2.92494 avg_loss = 3.41586\n",
      "epoch no.3 train no.311770  loss = 3.06381 avg_loss = 3.42778\n",
      "epoch no.3 train no.311780  loss = 6.44560 avg_loss = 3.49137\n",
      "epoch no.3 train no.311790  loss = 3.62830 avg_loss = 3.48131\n",
      "epoch no.3 train no.311800  loss = 2.81568 avg_loss = 3.48071\n",
      "epoch no.3 train no.311810  loss = 3.95749 avg_loss = 3.47580\n",
      "epoch no.3 train no.311820  loss = 3.36400 avg_loss = 3.48113\n",
      "epoch no.3 train no.311830  loss = 4.24466 avg_loss = 3.55201\n",
      "epoch no.3 train no.311840  loss = 2.30359 avg_loss = 3.48859\n",
      "epoch no.3 train no.311850  loss = 3.50067 avg_loss = 3.45205\n",
      "epoch no.3 train no.311860  loss = 4.06248 avg_loss = 3.43929\n",
      "epoch no.3 train no.311870  loss = 2.50124 avg_loss = 3.45266\n",
      "epoch no.3 train no.311880  loss = 6.20694 avg_loss = 3.46367\n",
      "epoch no.3 train no.311890  loss = 1.59284 avg_loss = 3.45963\n",
      "epoch no.3 train no.311900  loss = 3.43518 avg_loss = 3.41821\n",
      "epoch no.3 train no.311910  loss = 1.92220 avg_loss = 3.42715\n",
      "epoch no.3 train no.311920  loss = 5.44387 avg_loss = 3.47912\n",
      "epoch no.3 train no.311930  loss = 3.23403 avg_loss = 3.46948\n",
      "epoch no.3 train no.311940  loss = 3.48786 avg_loss = 3.44915\n",
      "epoch no.3 train no.311950  loss = 2.98473 avg_loss = 3.43214\n",
      "epoch no.3 train no.311960  loss = 4.11641 avg_loss = 3.43551\n",
      "epoch no.3 train no.311970  loss = 4.60296 avg_loss = 3.46732\n",
      "epoch no.3 train no.311980  loss = 2.19689 avg_loss = 3.44577\n",
      "epoch no.3 train no.311990  loss = 4.17331 avg_loss = 3.44034\n",
      "epoch no.3 train no.312000  loss = 3.79871 avg_loss = 3.38118\n",
      "5\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁필요할', '때', '</s>', '▁신나는', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.3 train no.312010  loss = 3.70472 avg_loss = 3.35365\n",
      "epoch no.3 train no.312020  loss = 3.12520 avg_loss = 3.40029\n",
      "epoch no.3 train no.312030  loss = 3.86806 avg_loss = 3.44795\n",
      "epoch no.3 train no.312040  loss = 3.06260 avg_loss = 3.46142\n",
      "epoch no.3 train no.312050  loss = 2.95989 avg_loss = 3.45656\n",
      "epoch no.3 train no.312060  loss = 2.28766 avg_loss = 3.41615\n",
      "epoch no.3 train no.312070  loss = 4.18579 avg_loss = 3.43573\n",
      "epoch no.3 train no.312080  loss = 2.88168 avg_loss = 3.47462\n",
      "epoch no.3 train no.312090  loss = 2.88568 avg_loss = 3.47873\n",
      "epoch no.3 train no.312100  loss = 3.15085 avg_loss = 3.45473\n",
      "epoch no.3 train no.312110  loss = 4.11929 avg_loss = 3.45565\n",
      "epoch no.3 train no.312120  loss = 2.51073 avg_loss = 3.41179\n",
      "epoch no.3 train no.312130  loss = 2.82973 avg_loss = 3.39684\n",
      "epoch no.3 train no.312140  loss = 3.05167 avg_loss = 3.38891\n",
      "epoch no.3 train no.312150  loss = 1.95982 avg_loss = 3.35748\n",
      "epoch no.3 train no.312160  loss = 4.03492 avg_loss = 3.36485\n",
      "epoch no.3 train no.312170  loss = 3.52354 avg_loss = 3.38580\n",
      "epoch no.3 train no.312180  loss = 2.90894 avg_loss = 3.37302\n",
      "epoch no.3 train no.312190  loss = 4.23119 avg_loss = 3.38522\n",
      "epoch no.3 train no.312200  loss = 3.67105 avg_loss = 3.38282\n",
      "epoch no.3 train no.312210  loss = 2.22146 avg_loss = 3.40317\n",
      "epoch no.3 train no.312220  loss = 2.02809 avg_loss = 3.38954\n",
      "epoch no.3 train no.312230  loss = 2.76886 avg_loss = 3.40303\n",
      "epoch no.3 train no.312240  loss = 3.58814 avg_loss = 3.36934\n",
      "epoch no.3 train no.312250  loss = 3.01487 avg_loss = 3.35323\n",
      "epoch no.3 train no.312260  loss = 2.79418 avg_loss = 3.35988\n",
      "epoch no.3 train no.312270  loss = 2.22082 avg_loss = 3.33811\n",
      "epoch no.3 train no.312280  loss = 3.52345 avg_loss = 3.32176\n",
      "epoch no.3 train no.312290  loss = 2.68547 avg_loss = 3.35935\n",
      "epoch no.3 train no.312300  loss = 4.98945 avg_loss = 3.39003\n",
      "epoch no.3 train no.312310  loss = 2.84916 avg_loss = 3.37446\n",
      "epoch no.3 train no.312320  loss = 4.89891 avg_loss = 3.39102\n",
      "epoch no.3 train no.312330  loss = 2.74296 avg_loss = 3.41861\n",
      "epoch no.3 train no.312340  loss = 4.28574 avg_loss = 3.47023\n",
      "epoch no.3 train no.312350  loss = 2.49529 avg_loss = 3.47504\n",
      "epoch no.3 train no.312360  loss = 3.24817 avg_loss = 3.46259\n",
      "epoch no.3 train no.312370  loss = 3.56814 avg_loss = 3.48955\n",
      "epoch no.3 train no.312380  loss = 4.20703 avg_loss = 3.50972\n",
      "epoch no.3 train no.312390  loss = 3.46905 avg_loss = 3.48512\n",
      "epoch no.3 train no.312400  loss = 3.26806 avg_loss = 3.47971\n",
      "epoch no.3 train no.312410  loss = 3.61437 avg_loss = 3.45424\n",
      "epoch no.3 train no.312420  loss = 3.38066 avg_loss = 3.47779\n",
      "epoch no.3 train no.312430  loss = 2.81377 avg_loss = 3.49245\n",
      "epoch no.3 train no.312440  loss = 4.43713 avg_loss = 3.51699\n",
      "epoch no.3 train no.312450  loss = 3.51418 avg_loss = 3.54770\n",
      "epoch no.3 train no.312460  loss = 3.67924 avg_loss = 3.60289\n",
      "epoch no.3 train no.312470  loss = 2.66625 avg_loss = 3.57705\n",
      "epoch no.3 train no.312480  loss = 2.91704 avg_loss = 3.55940\n",
      "epoch no.3 train no.312490  loss = 2.90729 avg_loss = 3.57179\n",
      "epoch no.3 train no.312500  loss = 3.05701 avg_loss = 3.55207\n",
      "epoch no.3 train no.312510  loss = 2.48794 avg_loss = 3.51813\n",
      "epoch no.3 train no.312520  loss = 3.17530 avg_loss = 3.54325\n",
      "epoch no.3 train no.312530  loss = 3.27377 avg_loss = 3.53237\n",
      "epoch no.3 train no.312540  loss = 3.08386 avg_loss = 3.51341\n",
      "epoch no.3 train no.312550  loss = 3.19577 avg_loss = 3.47941\n",
      "epoch no.3 train no.312560  loss = 4.70720 avg_loss = 3.50347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.312570  loss = 2.33746 avg_loss = 3.47210\n",
      "epoch no.3 train no.312580  loss = 3.75137 avg_loss = 3.48389\n",
      "epoch no.3 train no.312590  loss = 2.10181 avg_loss = 3.44836\n",
      "epoch no.3 train no.312600  loss = 3.24937 avg_loss = 3.38995\n",
      "epoch no.3 train no.312610  loss = 2.50813 avg_loss = 3.41385\n",
      "epoch no.3 train no.312620  loss = 4.26828 avg_loss = 3.47695\n",
      "epoch no.3 train no.312630  loss = 2.47721 avg_loss = 3.49308\n",
      "epoch no.3 train no.312640  loss = 4.06352 avg_loss = 3.48050\n",
      "epoch no.3 train no.312650  loss = 1.95663 avg_loss = 3.45208\n",
      "epoch no.3 train no.312660  loss = 3.44276 avg_loss = 3.44954\n",
      "epoch no.3 train no.312670  loss = 2.80831 avg_loss = 3.41951\n",
      "epoch no.3 train no.312680  loss = 3.02288 avg_loss = 3.40215\n",
      "epoch no.3 train no.312690  loss = 2.91035 avg_loss = 3.42445\n",
      "epoch no.3 train no.312700  loss = 3.89918 avg_loss = 3.38305\n",
      "epoch no.3 train no.312710  loss = 4.43116 avg_loss = 3.40533\n",
      "epoch no.3 train no.312720  loss = 3.50267 avg_loss = 3.40902\n",
      "epoch no.3 train no.312730  loss = 1.38150 avg_loss = 3.36881\n",
      "epoch no.3 train no.312740  loss = 4.53212 avg_loss = 3.37711\n",
      "epoch no.3 train no.312750  loss = 2.48036 avg_loss = 3.39816\n",
      "epoch no.3 train no.312760  loss = 3.87533 avg_loss = 3.41401\n",
      "epoch no.3 train no.312770  loss = 2.66817 avg_loss = 3.39008\n",
      "epoch no.3 train no.312780  loss = 3.72698 avg_loss = 3.42964\n",
      "epoch no.3 train no.312790  loss = 2.78408 avg_loss = 3.41883\n",
      "epoch no.3 train no.312800  loss = 3.21002 avg_loss = 3.41753\n",
      "epoch no.3 train no.312810  loss = 3.76563 avg_loss = 3.47529\n",
      "epoch no.3 train no.312820  loss = 3.68953 avg_loss = 3.50052\n",
      "epoch no.3 train no.312830  loss = 5.03177 avg_loss = 3.47030\n",
      "epoch no.3 train no.312840  loss = 3.73847 avg_loss = 3.45899\n",
      "epoch no.3 train no.312850  loss = 2.49905 avg_loss = 3.51965\n",
      "epoch no.3 train no.312860  loss = 1.22500 avg_loss = 3.49015\n",
      "epoch no.3 train no.312870  loss = 4.93750 avg_loss = 3.47798\n",
      "epoch no.3 train no.312880  loss = 4.68654 avg_loss = 3.51243\n",
      "epoch no.3 train no.312890  loss = 3.42694 avg_loss = 3.52925\n",
      "epoch no.3 train no.312900  loss = 3.44448 avg_loss = 3.45471\n",
      "epoch no.3 train no.312910  loss = 5.72569 avg_loss = 3.44825\n",
      "epoch no.3 train no.312920  loss = 3.26505 avg_loss = 3.43564\n",
      "epoch no.3 train no.312930  loss = 2.92847 avg_loss = 3.45628\n",
      "epoch no.3 train no.312940  loss = 2.56575 avg_loss = 3.42176\n",
      "epoch no.3 train no.312950  loss = 4.48542 avg_loss = 3.50128\n",
      "epoch no.3 train no.312960  loss = 2.60920 avg_loss = 3.50472\n",
      "epoch no.3 train no.312970  loss = 2.90517 avg_loss = 3.46987\n",
      "epoch no.3 train no.312980  loss = 3.20716 avg_loss = 3.47542\n",
      "epoch no.3 train no.312990  loss = 4.30434 avg_loss = 3.52184\n",
      "epoch no.3 train no.313000  loss = 3.33158 avg_loss = 3.49451\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.3 train no.313010  loss = 3.44853 avg_loss = 3.47428\n",
      "epoch no.3 train no.313020  loss = 2.96126 avg_loss = 3.48579\n",
      "epoch no.3 train no.313030  loss = 3.63254 avg_loss = 3.43360\n",
      "epoch no.3 train no.313040  loss = 2.78161 avg_loss = 3.38230\n",
      "epoch no.3 train no.313050  loss = 2.09187 avg_loss = 3.35001\n",
      "epoch no.3 train no.313060  loss = 4.07315 avg_loss = 3.41207\n",
      "epoch no.3 train no.313070  loss = 3.36816 avg_loss = 3.39767\n",
      "epoch no.3 train no.313080  loss = 2.56281 avg_loss = 3.40268\n",
      "epoch no.3 train no.313090  loss = 2.78176 avg_loss = 3.37431\n",
      "epoch no.3 train no.313100  loss = 2.54045 avg_loss = 3.36862\n",
      "epoch no.3 train no.313110  loss = 2.85664 avg_loss = 3.32233\n",
      "epoch no.3 train no.313120  loss = 2.47825 avg_loss = 3.32550\n",
      "epoch no.3 train no.313130  loss = 4.69626 avg_loss = 3.35236\n",
      "epoch no.3 train no.313140  loss = 3.36932 avg_loss = 3.40266\n",
      "epoch no.3 train no.313150  loss = 2.69858 avg_loss = 3.37207\n",
      "epoch no.3 train no.313160  loss = 3.03504 avg_loss = 3.47911\n",
      "epoch no.3 train no.313170  loss = 5.72594 avg_loss = 3.47409\n",
      "epoch no.3 train no.313180  loss = 5.07886 avg_loss = 3.45553\n",
      "epoch no.3 train no.313190  loss = 2.24442 avg_loss = 3.49044\n",
      "epoch no.3 train no.313200  loss = 3.00202 avg_loss = 3.47115\n",
      "epoch no.3 train no.313210  loss = 3.97178 avg_loss = 3.49214\n",
      "epoch no.3 train no.313220  loss = 2.23669 avg_loss = 3.45699\n",
      "epoch no.3 train no.313230  loss = 2.46340 avg_loss = 3.47696\n",
      "epoch no.3 train no.313240  loss = 4.83994 avg_loss = 3.49083\n",
      "epoch no.3 train no.313250  loss = 3.96113 avg_loss = 3.50327\n",
      "epoch no.3 train no.313260  loss = 3.00164 avg_loss = 3.48151\n",
      "epoch no.3 train no.313270  loss = 2.88981 avg_loss = 3.46219\n",
      "epoch no.3 train no.313280  loss = 3.99264 avg_loss = 3.48747\n",
      "epoch no.3 train no.313290  loss = 4.23789 avg_loss = 3.45256\n",
      "epoch no.3 train no.313300  loss = 2.89544 avg_loss = 3.45517\n",
      "epoch no.3 train no.313310  loss = 3.12239 avg_loss = 3.48882\n",
      "epoch no.3 train no.313320  loss = 4.59932 avg_loss = 3.51178\n",
      "epoch no.3 train no.313330  loss = 2.30564 avg_loss = 3.51303\n",
      "epoch no.3 train no.313340  loss = 2.77353 avg_loss = 3.50314\n",
      "epoch no.3 train no.313350  loss = 3.33515 avg_loss = 3.48208\n",
      "epoch no.3 train no.313360  loss = 4.71118 avg_loss = 3.49867\n",
      "epoch no.3 train no.313370  loss = 4.26697 avg_loss = 3.52757\n",
      "epoch no.3 train no.313380  loss = 4.66059 avg_loss = 3.51250\n",
      "epoch no.3 train no.313390  loss = 3.28053 avg_loss = 3.48029\n",
      "epoch no.3 train no.313400  loss = 2.38423 avg_loss = 3.51478\n",
      "epoch no.3 train no.313410  loss = 2.31200 avg_loss = 3.55698\n",
      "epoch no.3 train no.313420  loss = 2.89038 avg_loss = 3.49762\n",
      "epoch no.3 train no.313430  loss = 2.86457 avg_loss = 3.45620\n",
      "epoch no.3 train no.313440  loss = 2.98970 avg_loss = 3.40102\n",
      "epoch no.3 train no.313450  loss = 2.65280 avg_loss = 3.39500\n",
      "epoch no.3 train no.313460  loss = 4.19110 avg_loss = 3.35819\n",
      "epoch no.3 train no.313470  loss = 3.07058 avg_loss = 3.36749\n",
      "epoch no.3 train no.313480  loss = 2.83736 avg_loss = 3.44471\n",
      "epoch no.3 train no.313490  loss = 3.30540 avg_loss = 3.43910\n",
      "epoch no.3 train no.313500  loss = 2.83311 avg_loss = 3.46431\n",
      "epoch no.3 train no.313510  loss = 3.35452 avg_loss = 3.47562\n",
      "epoch no.3 train no.313520  loss = 3.26475 avg_loss = 3.46483\n",
      "epoch no.3 train no.313530  loss = 4.32527 avg_loss = 3.48258\n",
      "epoch no.3 train no.313540  loss = 4.04148 avg_loss = 3.53955\n",
      "epoch no.3 train no.313550  loss = 3.53494 avg_loss = 3.50921\n",
      "epoch no.3 train no.313560  loss = 4.62456 avg_loss = 3.50458\n",
      "epoch no.3 train no.313570  loss = 4.91271 avg_loss = 3.51938\n",
      "epoch no.3 train no.313580  loss = 4.46794 avg_loss = 3.51000\n",
      "epoch no.3 train no.313590  loss = 2.45570 avg_loss = 3.51545\n",
      "epoch no.3 train no.313600  loss = 3.41276 avg_loss = 3.53188\n",
      "epoch no.3 train no.313610  loss = 2.88635 avg_loss = 3.50731\n",
      "epoch no.3 train no.313620  loss = 3.10860 avg_loss = 3.48377\n",
      "epoch no.3 train no.313630  loss = 3.31056 avg_loss = 3.45745\n",
      "epoch no.3 train no.313640  loss = 2.21152 avg_loss = 3.46951\n",
      "epoch no.3 train no.313650  loss = 3.73820 avg_loss = 3.50147\n",
      "epoch no.3 train no.313660  loss = 2.80036 avg_loss = 3.45117\n",
      "epoch no.3 train no.313670  loss = 3.03743 avg_loss = 3.44624\n",
      "epoch no.3 train no.313680  loss = 5.21654 avg_loss = 3.47348\n",
      "epoch no.3 train no.313690  loss = 4.35843 avg_loss = 3.49031\n",
      "epoch no.3 train no.313700  loss = 3.17802 avg_loss = 3.46078\n",
      "epoch no.3 train no.313710  loss = 3.65527 avg_loss = 3.47499\n",
      "epoch no.3 train no.313720  loss = 4.77636 avg_loss = 3.47920\n",
      "epoch no.3 train no.313730  loss = 3.21439 avg_loss = 3.46791\n",
      "epoch no.3 train no.313740  loss = 4.15490 avg_loss = 3.48953\n",
      "epoch no.3 train no.313750  loss = 4.41409 avg_loss = 3.51712\n",
      "epoch no.3 train no.313760  loss = 3.60582 avg_loss = 3.53284\n",
      "epoch no.3 train no.313770  loss = 2.79117 avg_loss = 3.52945\n",
      "epoch no.3 train no.313780  loss = 3.55044 avg_loss = 3.54444\n",
      "epoch no.3 train no.313790  loss = 2.77796 avg_loss = 3.56302\n",
      "epoch no.3 train no.313800  loss = 2.86498 avg_loss = 3.56301\n",
      "epoch no.3 train no.313810  loss = 2.66949 avg_loss = 3.55361\n",
      "epoch no.3 train no.313820  loss = 3.48030 avg_loss = 3.56266\n",
      "epoch no.3 train no.313830  loss = 3.08229 avg_loss = 3.59612\n",
      "epoch no.3 train no.313840  loss = 2.49461 avg_loss = 3.57632\n",
      "epoch no.3 train no.313850  loss = 4.64659 avg_loss = 3.56596\n",
      "epoch no.3 train no.313860  loss = 3.13935 avg_loss = 3.51090\n",
      "epoch no.3 train no.313870  loss = 2.12092 avg_loss = 3.42072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.313880  loss = 3.62081 avg_loss = 3.43337\n",
      "epoch no.3 train no.313890  loss = 3.56363 avg_loss = 3.43478\n",
      "epoch no.3 train no.313900  loss = 4.43986 avg_loss = 3.41601\n",
      "epoch no.3 train no.313910  loss = 2.81976 avg_loss = 3.39558\n",
      "epoch no.3 train no.313920  loss = 4.27212 avg_loss = 3.42820\n",
      "epoch no.3 train no.313930  loss = 3.24956 avg_loss = 3.39470\n",
      "epoch no.3 train no.313940  loss = 3.78250 avg_loss = 3.38206\n",
      "epoch no.3 train no.313950  loss = 4.01622 avg_loss = 3.41601\n",
      "epoch no.3 train no.313960  loss = 4.60148 avg_loss = 3.38677\n",
      "epoch no.3 train no.313970  loss = 4.53048 avg_loss = 3.42069\n",
      "epoch no.3 train no.313980  loss = 3.19042 avg_loss = 3.39043\n",
      "epoch no.3 train no.313990  loss = 2.36710 avg_loss = 3.43249\n",
      "epoch no.3 train no.314000  loss = 3.17576 avg_loss = 3.41339\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>', '▁노래', 'op', '</s>']\n",
      "기분전환이 필요할 때 듣는 pop</s>\n",
      "epoch no.3 train no.314010  loss = 3.46611 avg_loss = 3.43299\n",
      "epoch no.3 train no.314020  loss = 2.59416 avg_loss = 3.44585\n",
      "epoch no.3 train no.314030  loss = 3.00751 avg_loss = 3.47195\n",
      "epoch no.3 train no.314040  loss = 2.95789 avg_loss = 3.51687\n",
      "epoch no.3 train no.314050  loss = 4.09405 avg_loss = 3.50579\n",
      "epoch no.3 train no.314060  loss = 4.46512 avg_loss = 3.46991\n",
      "epoch no.3 train no.314070  loss = 3.41129 avg_loss = 3.46987\n",
      "epoch no.3 train no.314080  loss = 3.13126 avg_loss = 3.46795\n",
      "epoch no.3 train no.314090  loss = 3.84413 avg_loss = 3.49910\n",
      "epoch no.3 train no.314100  loss = 4.41850 avg_loss = 3.52016\n",
      "epoch no.3 train no.314110  loss = 3.65863 avg_loss = 3.52451\n",
      "epoch no.3 train no.314120  loss = 3.74764 avg_loss = 3.52468\n",
      "epoch no.3 train no.314130  loss = 3.01767 avg_loss = 3.53759\n",
      "epoch no.3 train no.314140  loss = 4.80814 avg_loss = 3.48369\n",
      "epoch no.3 train no.314150  loss = 6.09128 avg_loss = 3.52405\n",
      "epoch no.3 train no.314160  loss = 3.53737 avg_loss = 3.52791\n",
      "epoch no.3 train no.314170  loss = 2.65617 avg_loss = 3.52153\n",
      "epoch no.3 train no.314180  loss = 2.83382 avg_loss = 3.43532\n",
      "epoch no.3 train no.314190  loss = 3.94318 avg_loss = 3.42771\n",
      "epoch no.3 train no.314200  loss = 3.40294 avg_loss = 3.45946\n",
      "epoch no.3 train no.314210  loss = 4.03163 avg_loss = 3.47003\n",
      "epoch no.3 train no.314220  loss = 2.39627 avg_loss = 3.46924\n",
      "epoch no.3 train no.314230  loss = 4.00411 avg_loss = 3.45892\n",
      "epoch no.3 train no.314240  loss = 2.79480 avg_loss = 3.46047\n",
      "epoch no.3 train no.314250  loss = 4.46618 avg_loss = 3.47163\n",
      "epoch no.3 train no.314260  loss = 3.18595 avg_loss = 3.44383\n",
      "epoch no.3 train no.314270  loss = 2.66385 avg_loss = 3.42362\n",
      "epoch no.3 train no.314280  loss = 3.27859 avg_loss = 3.42510\n",
      "epoch no.3 train no.314290  loss = 2.59881 avg_loss = 3.43861\n",
      "epoch no.3 train no.314300  loss = 6.34438 avg_loss = 3.49118\n",
      "epoch no.3 train no.314310  loss = 4.37642 avg_loss = 3.44910\n",
      "epoch no.3 train no.314320  loss = 2.21584 avg_loss = 3.42519\n",
      "epoch no.3 train no.314330  loss = 2.40997 avg_loss = 3.46211\n",
      "epoch no.3 train no.314340  loss = 2.65179 avg_loss = 3.47973\n",
      "epoch no.3 train no.314350  loss = 3.29658 avg_loss = 3.53976\n",
      "epoch no.3 train no.314360  loss = 3.31190 avg_loss = 3.51311\n",
      "epoch no.3 train no.314370  loss = 3.23512 avg_loss = 3.51591\n",
      "epoch no.3 train no.314380  loss = 4.76415 avg_loss = 3.47855\n",
      "epoch no.3 train no.314390  loss = 2.45272 avg_loss = 3.46033\n",
      "epoch no.3 train no.314400  loss = 2.86029 avg_loss = 3.46639\n",
      "epoch no.3 train no.314410  loss = 2.71983 avg_loss = 3.41795\n",
      "epoch no.3 train no.314420  loss = 2.66830 avg_loss = 3.42339\n",
      "epoch no.3 train no.314430  loss = 3.65210 avg_loss = 3.43724\n",
      "epoch no.3 train no.314440  loss = 3.39249 avg_loss = 3.41461\n",
      "epoch no.3 train no.314450  loss = 6.38492 avg_loss = 3.47382\n",
      "epoch no.3 train no.314460  loss = 3.66180 avg_loss = 3.41091\n",
      "epoch no.3 train no.314470  loss = 2.58890 avg_loss = 3.39763\n",
      "epoch no.3 train no.314480  loss = 3.40121 avg_loss = 3.41372\n",
      "epoch no.3 train no.314490  loss = 3.20417 avg_loss = 3.42627\n",
      "epoch no.3 train no.314500  loss = 3.05921 avg_loss = 3.45752\n",
      "epoch no.3 train no.314510  loss = 3.68929 avg_loss = 3.47629\n",
      "epoch no.3 train no.314520  loss = 4.27088 avg_loss = 3.48671\n",
      "epoch no.3 train no.314530  loss = 3.22850 avg_loss = 3.45980\n",
      "epoch no.3 train no.314540  loss = 6.35013 avg_loss = 3.47849\n",
      "epoch no.3 train no.314550  loss = 3.47091 avg_loss = 3.51313\n",
      "epoch no.3 train no.314560  loss = 4.25404 avg_loss = 3.52334\n",
      "epoch no.3 train no.314570  loss = 4.12564 avg_loss = 3.52959\n",
      "epoch no.3 train no.314580  loss = 3.43623 avg_loss = 3.52410\n",
      "epoch no.3 train no.314590  loss = 2.79868 avg_loss = 3.47102\n",
      "epoch no.3 train no.314600  loss = 5.26397 avg_loss = 3.51432\n",
      "epoch no.3 train no.314610  loss = 3.44965 avg_loss = 3.49421\n",
      "epoch no.3 train no.314620  loss = 3.20109 avg_loss = 3.48182\n",
      "epoch no.3 train no.314630  loss = 1.23936 avg_loss = 3.46154\n",
      "epoch no.3 train no.314640  loss = 3.21371 avg_loss = 3.47074\n",
      "epoch no.3 train no.314650  loss = 3.92413 avg_loss = 3.50676\n",
      "epoch no.3 train no.314660  loss = 3.57462 avg_loss = 3.52963\n",
      "epoch no.3 train no.314670  loss = 2.49914 avg_loss = 3.53784\n",
      "epoch no.3 train no.314680  loss = 3.74425 avg_loss = 3.55058\n",
      "epoch no.3 train no.314690  loss = 4.61565 avg_loss = 3.57521\n",
      "epoch no.3 train no.314700  loss = 2.96588 avg_loss = 3.58206\n",
      "epoch no.3 train no.314710  loss = 3.08254 avg_loss = 3.55656\n",
      "epoch no.3 train no.314720  loss = 3.57539 avg_loss = 3.52359\n",
      "epoch no.3 train no.314730  loss = 3.07128 avg_loss = 3.50434\n",
      "epoch no.3 train no.314740  loss = 3.37859 avg_loss = 3.49027\n",
      "epoch no.3 train no.314750  loss = 3.38486 avg_loss = 3.48209\n",
      "epoch no.3 train no.314760  loss = 2.39593 avg_loss = 3.49191\n",
      "epoch no.3 train no.314770  loss = 3.64339 avg_loss = 3.47830\n",
      "epoch no.3 train no.314780  loss = 2.96168 avg_loss = 3.50644\n",
      "epoch no.3 train no.314790  loss = 2.56007 avg_loss = 3.46282\n",
      "epoch no.3 train no.314800  loss = 4.36230 avg_loss = 3.46963\n",
      "epoch no.3 train no.314810  loss = 4.50943 avg_loss = 3.48930\n",
      "epoch no.3 train no.314820  loss = 3.57460 avg_loss = 3.51161\n",
      "epoch no.3 train no.314830  loss = 5.66867 avg_loss = 3.51577\n",
      "epoch no.3 train no.314840  loss = 4.41090 avg_loss = 3.52533\n",
      "epoch no.3 train no.314850  loss = 3.54694 avg_loss = 3.51428\n",
      "epoch no.3 train no.314860  loss = 4.48036 avg_loss = 3.47206\n",
      "epoch no.3 train no.314870  loss = 2.93604 avg_loss = 3.46344\n",
      "epoch no.3 train no.314880  loss = 3.51825 avg_loss = 3.43769\n",
      "epoch no.3 train no.314890  loss = 4.38016 avg_loss = 3.46307\n",
      "epoch no.3 train no.314900  loss = 2.64188 avg_loss = 3.46289\n",
      "epoch no.3 train no.314910  loss = 2.56907 avg_loss = 3.41244\n",
      "epoch no.3 train no.314920  loss = 5.35457 avg_loss = 3.41531\n",
      "epoch no.3 train no.314930  loss = 5.45224 avg_loss = 3.46550\n",
      "epoch no.3 train no.314940  loss = 3.90331 avg_loss = 3.42663\n",
      "epoch no.3 train no.314950  loss = 5.95703 avg_loss = 3.52327\n",
      "epoch no.3 train no.314960  loss = 4.12793 avg_loss = 3.58179\n",
      "epoch no.3 train no.314970  loss = 3.84240 avg_loss = 3.58206\n",
      "epoch no.3 train no.314980  loss = 5.35723 avg_loss = 3.59556\n",
      "epoch no.3 train no.314990  loss = 2.28775 avg_loss = 3.60147\n",
      "epoch no.3 train no.315000  loss = 3.87247 avg_loss = 3.60664\n",
      "7\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '</s>', '▁음악', '한', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 잔잔한 팝</s>\n",
      "epoch no.3 train no.315010  loss = 3.41683 avg_loss = 3.56949\n",
      "epoch no.3 train no.315020  loss = 4.02122 avg_loss = 3.50567\n",
      "epoch no.3 train no.315030  loss = 3.19589 avg_loss = 3.57914\n",
      "epoch no.3 train no.315040  loss = 2.43618 avg_loss = 3.53384\n",
      "epoch no.3 train no.315050  loss = 3.58935 avg_loss = 3.53030\n",
      "epoch no.3 train no.315060  loss = 4.29649 avg_loss = 3.48828\n",
      "epoch no.3 train no.315070  loss = 3.30359 avg_loss = 3.49678\n",
      "epoch no.3 train no.315080  loss = 2.66187 avg_loss = 3.49038\n",
      "epoch no.3 train no.315090  loss = 4.25864 avg_loss = 3.47502\n",
      "epoch no.3 train no.315100  loss = 3.00198 avg_loss = 3.46106\n",
      "epoch no.3 train no.315110  loss = 4.18497 avg_loss = 3.53581\n",
      "epoch no.3 train no.315120  loss = 4.17105 avg_loss = 3.52977\n",
      "epoch no.3 train no.315130  loss = 1.90264 avg_loss = 3.48917\n",
      "epoch no.3 train no.315140  loss = 2.50918 avg_loss = 3.49141\n",
      "epoch no.3 train no.315150  loss = 3.01222 avg_loss = 3.46453\n",
      "epoch no.3 train no.315160  loss = 3.09762 avg_loss = 3.47184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.315170  loss = 2.56075 avg_loss = 3.49116\n",
      "epoch no.3 train no.315180  loss = 2.60290 avg_loss = 3.54523\n",
      "epoch no.3 train no.315190  loss = 2.94713 avg_loss = 3.53992\n",
      "epoch no.3 train no.315200  loss = 3.33824 avg_loss = 3.56252\n",
      "epoch no.3 train no.315210  loss = 2.53374 avg_loss = 3.59129\n",
      "epoch no.3 train no.315220  loss = 2.23879 avg_loss = 3.55965\n",
      "epoch no.3 train no.315230  loss = 2.68335 avg_loss = 3.54613\n",
      "epoch no.3 train no.315240  loss = 6.02599 avg_loss = 3.56855\n",
      "epoch no.3 train no.315250  loss = 4.54202 avg_loss = 3.56556\n",
      "epoch no.3 train no.315260  loss = 2.23437 avg_loss = 3.50435\n",
      "epoch no.3 train no.315270  loss = 3.97926 avg_loss = 3.47215\n",
      "epoch no.3 train no.315280  loss = 3.33746 avg_loss = 3.49455\n",
      "epoch no.3 train no.315290  loss = 2.55532 avg_loss = 3.48810\n",
      "epoch no.3 train no.315300  loss = 3.63272 avg_loss = 3.48382\n",
      "epoch no.3 train no.315310  loss = 2.26100 avg_loss = 3.45113\n",
      "epoch no.3 train no.315320  loss = 1.90631 avg_loss = 3.45308\n",
      "epoch no.3 train no.315330  loss = 3.78987 avg_loss = 3.50970\n",
      "epoch no.3 train no.315340  loss = 2.98463 avg_loss = 3.58471\n",
      "epoch no.3 train no.315350  loss = 2.86848 avg_loss = 3.56250\n",
      "epoch no.3 train no.315360  loss = 1.74001 avg_loss = 3.52588\n",
      "epoch no.3 train no.315370  loss = 3.34595 avg_loss = 3.53597\n",
      "epoch no.3 train no.315380  loss = 2.48829 avg_loss = 3.51548\n",
      "epoch no.3 train no.315390  loss = 2.66549 avg_loss = 3.52528\n",
      "epoch no.3 train no.315400  loss = 3.67307 avg_loss = 3.58231\n",
      "epoch no.3 train no.315410  loss = 3.94292 avg_loss = 3.62840\n",
      "epoch no.3 train no.315420  loss = 4.11503 avg_loss = 3.60253\n",
      "epoch no.3 train no.315430  loss = 2.95237 avg_loss = 3.58880\n",
      "epoch no.3 train no.315440  loss = 3.82543 avg_loss = 3.57346\n",
      "epoch no.3 train no.315450  loss = 3.66332 avg_loss = 3.60150\n",
      "epoch no.3 train no.315460  loss = 4.43027 avg_loss = 3.56910\n",
      "epoch no.3 train no.315470  loss = 5.31465 avg_loss = 3.60790\n",
      "epoch no.3 train no.315480  loss = 1.91969 avg_loss = 3.56236\n",
      "epoch no.3 train no.315490  loss = 3.99056 avg_loss = 3.50285\n",
      "epoch no.3 train no.315500  loss = 4.23955 avg_loss = 3.58724\n",
      "epoch no.3 train no.315510  loss = 3.16306 avg_loss = 3.59786\n",
      "epoch no.3 train no.315520  loss = 3.65545 avg_loss = 3.58894\n",
      "epoch no.3 train no.315530  loss = 2.69791 avg_loss = 3.57119\n",
      "epoch no.3 train no.315540  loss = 3.14295 avg_loss = 3.56740\n",
      "epoch no.3 train no.315550  loss = 5.28038 avg_loss = 3.56050\n",
      "epoch no.3 train no.315560  loss = 2.69014 avg_loss = 3.54547\n",
      "epoch no.3 train no.315570  loss = 2.73700 avg_loss = 3.55419\n",
      "epoch no.3 train no.315580  loss = 3.80593 avg_loss = 3.54543\n",
      "epoch no.3 train no.315590  loss = 3.31705 avg_loss = 3.58055\n",
      "epoch no.3 train no.315600  loss = 2.77454 avg_loss = 3.61329\n",
      "epoch no.3 train no.315610  loss = 3.90831 avg_loss = 3.59096\n",
      "epoch no.3 train no.315620  loss = 3.90156 avg_loss = 3.56020\n",
      "epoch no.3 train no.315630  loss = 3.17365 avg_loss = 3.57322\n",
      "epoch no.3 train no.315640  loss = 4.16153 avg_loss = 3.60171\n",
      "epoch no.3 train no.315650  loss = 5.60809 avg_loss = 3.65174\n",
      "epoch no.3 train no.315660  loss = 3.97433 avg_loss = 3.63343\n",
      "epoch no.3 train no.315670  loss = 3.75273 avg_loss = 3.64868\n",
      "epoch no.3 train no.315680  loss = 1.68827 avg_loss = 3.67412\n",
      "epoch no.3 train no.315690  loss = 2.58131 avg_loss = 3.64915\n",
      "epoch no.3 train no.315700  loss = 2.18348 avg_loss = 3.64978\n",
      "epoch no.3 train no.315710  loss = 2.80007 avg_loss = 3.65254\n",
      "epoch no.3 train no.315720  loss = 6.25808 avg_loss = 3.65844\n",
      "epoch no.3 train no.315730  loss = 5.07917 avg_loss = 3.64656\n",
      "epoch no.3 train no.315740  loss = 3.16510 avg_loss = 3.58753\n",
      "epoch no.3 train no.315750  loss = 4.38424 avg_loss = 3.61226\n",
      "epoch no.3 train no.315760  loss = 3.33704 avg_loss = 3.59688\n",
      "epoch no.3 train no.315770  loss = 4.54151 avg_loss = 3.56958\n",
      "epoch no.3 train no.315780  loss = 4.71586 avg_loss = 3.59602\n",
      "epoch no.3 train no.315790  loss = 3.21512 avg_loss = 3.56009\n",
      "epoch no.3 train no.315800  loss = 2.51434 avg_loss = 3.53757\n",
      "epoch no.3 train no.315810  loss = 4.54485 avg_loss = 3.51436\n",
      "epoch no.3 train no.315820  loss = 3.76783 avg_loss = 3.44664\n",
      "epoch no.3 train no.315830  loss = 2.88274 avg_loss = 3.45217\n",
      "epoch no.3 train no.315840  loss = 3.72523 avg_loss = 3.46963\n",
      "epoch no.3 train no.315850  loss = 3.49868 avg_loss = 3.52972\n",
      "epoch no.3 train no.315860  loss = 2.55385 avg_loss = 3.56459\n",
      "epoch no.3 train no.315870  loss = 0.98684 avg_loss = 3.54379\n",
      "epoch no.3 train no.315880  loss = 3.76635 avg_loss = 3.52230\n",
      "epoch no.3 train no.315890  loss = 3.47598 avg_loss = 3.50675\n",
      "epoch no.3 train no.315900  loss = 3.72291 avg_loss = 3.49494\n",
      "epoch no.3 train no.315910  loss = 3.19370 avg_loss = 3.49084\n",
      "epoch no.3 train no.315920  loss = 2.52693 avg_loss = 3.46985\n",
      "epoch no.3 train no.315930  loss = 4.16871 avg_loss = 3.44488\n",
      "epoch no.3 train no.315940  loss = 3.27610 avg_loss = 3.45643\n",
      "epoch no.3 train no.315950  loss = 5.32020 avg_loss = 3.46006\n",
      "epoch no.3 train no.315960  loss = 3.22627 avg_loss = 3.41352\n",
      "epoch no.3 train no.315970  loss = 3.86835 avg_loss = 3.44020\n",
      "epoch no.3 train no.315980  loss = 3.86883 avg_loss = 3.39563\n",
      "epoch no.3 train no.315990  loss = 4.43714 avg_loss = 3.44251\n",
      "epoch no.3 train no.316000  loss = 4.75781 avg_loss = 3.43830\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.316010  loss = 2.76991 avg_loss = 3.47365\n",
      "epoch no.3 train no.316020  loss = 3.96463 avg_loss = 3.50685\n",
      "epoch no.3 train no.316030  loss = 3.93723 avg_loss = 3.48565\n",
      "epoch no.3 train no.316040  loss = 2.87391 avg_loss = 3.55037\n",
      "epoch no.3 train no.316050  loss = 4.81083 avg_loss = 3.54205\n",
      "epoch no.3 train no.316060  loss = 2.60557 avg_loss = 3.52863\n",
      "epoch no.3 train no.316070  loss = 4.48658 avg_loss = 3.53912\n",
      "epoch no.3 train no.316080  loss = 2.80767 avg_loss = 3.53174\n",
      "epoch no.3 train no.316090  loss = 2.91113 avg_loss = 3.52703\n",
      "epoch no.3 train no.316100  loss = 2.48228 avg_loss = 3.53906\n",
      "epoch no.3 train no.316110  loss = 5.20166 avg_loss = 3.57558\n",
      "epoch no.3 train no.316120  loss = 1.80267 avg_loss = 3.54629\n",
      "epoch no.3 train no.316130  loss = 2.93240 avg_loss = 3.55550\n",
      "epoch no.3 train no.316140  loss = 2.60221 avg_loss = 3.54939\n",
      "epoch no.3 train no.316150  loss = 4.67044 avg_loss = 3.52444\n",
      "epoch no.3 train no.316160  loss = 5.58682 avg_loss = 3.49625\n",
      "epoch no.3 train no.316170  loss = 3.90776 avg_loss = 3.50356\n",
      "epoch no.3 train no.316180  loss = 2.68921 avg_loss = 3.48797\n",
      "epoch no.3 train no.316190  loss = 3.36807 avg_loss = 3.49731\n",
      "epoch no.3 train no.316200  loss = 5.38956 avg_loss = 3.47543\n",
      "epoch no.3 train no.316210  loss = 3.87541 avg_loss = 3.48675\n",
      "epoch no.3 train no.316220  loss = 3.22897 avg_loss = 3.47337\n",
      "epoch no.3 train no.316230  loss = 3.69495 avg_loss = 3.42525\n",
      "epoch no.3 train no.316240  loss = 3.50987 avg_loss = 3.40103\n",
      "epoch no.3 train no.316250  loss = 2.26875 avg_loss = 3.41243\n",
      "epoch no.3 train no.316260  loss = 2.95604 avg_loss = 3.42240\n",
      "epoch no.3 train no.316270  loss = 6.14276 avg_loss = 3.43344\n",
      "epoch no.3 train no.316280  loss = 4.56712 avg_loss = 3.48822\n",
      "epoch no.3 train no.316290  loss = 3.47224 avg_loss = 3.46784\n",
      "epoch no.3 train no.316300  loss = 3.15769 avg_loss = 3.47320\n",
      "epoch no.3 train no.316310  loss = 3.66614 avg_loss = 3.46420\n",
      "epoch no.3 train no.316320  loss = 3.34262 avg_loss = 3.49274\n",
      "epoch no.3 train no.316330  loss = 2.44556 avg_loss = 3.50301\n",
      "epoch no.3 train no.316340  loss = 2.95729 avg_loss = 3.48841\n",
      "epoch no.3 train no.316350  loss = 4.07979 avg_loss = 3.48774\n",
      "epoch no.3 train no.316360  loss = 4.37181 avg_loss = 3.46616\n",
      "epoch no.3 train no.316370  loss = 3.26663 avg_loss = 3.44875\n",
      "epoch no.3 train no.316380  loss = 2.31991 avg_loss = 3.50816\n",
      "epoch no.3 train no.316390  loss = 3.28001 avg_loss = 3.53459\n",
      "epoch no.3 train no.316400  loss = 3.99168 avg_loss = 3.50610\n",
      "epoch no.3 train no.316410  loss = 3.79199 avg_loss = 3.50324\n",
      "epoch no.3 train no.316420  loss = 3.67581 avg_loss = 3.52066\n",
      "epoch no.3 train no.316430  loss = 3.94495 avg_loss = 3.45684\n",
      "epoch no.3 train no.316440  loss = 6.11914 avg_loss = 3.50049\n",
      "epoch no.3 train no.316450  loss = 4.91188 avg_loss = 3.52066\n",
      "epoch no.3 train no.316460  loss = 3.98847 avg_loss = 3.52991\n",
      "epoch no.3 train no.316470  loss = 3.93437 avg_loss = 3.55702\n",
      "epoch no.3 train no.316480  loss = 3.13501 avg_loss = 3.52088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.316490  loss = 2.86117 avg_loss = 3.54948\n",
      "epoch no.3 train no.316500  loss = 4.14949 avg_loss = 3.58644\n",
      "epoch no.3 train no.316510  loss = 4.27512 avg_loss = 3.56741\n",
      "epoch no.3 train no.316520  loss = 3.59315 avg_loss = 3.53574\n",
      "epoch no.3 train no.316530  loss = 5.01600 avg_loss = 3.52554\n",
      "epoch no.3 train no.316540  loss = 3.31305 avg_loss = 3.51803\n",
      "epoch no.3 train no.316550  loss = 3.64348 avg_loss = 3.58871\n",
      "epoch no.3 train no.316560  loss = 4.82094 avg_loss = 3.58494\n",
      "epoch no.3 train no.316570  loss = 1.84817 avg_loss = 3.54108\n",
      "epoch no.3 train no.316580  loss = 4.14232 avg_loss = 3.56900\n",
      "epoch no.3 train no.316590  loss = 4.50894 avg_loss = 3.58089\n",
      "epoch no.3 train no.316600  loss = 3.15177 avg_loss = 3.56566\n",
      "epoch no.3 train no.316610  loss = 5.52279 avg_loss = 3.59876\n",
      "epoch no.3 train no.316620  loss = 3.97374 avg_loss = 3.60569\n",
      "epoch no.3 train no.316630  loss = 3.41684 avg_loss = 3.60563\n",
      "epoch no.3 train no.316640  loss = 3.68789 avg_loss = 3.61544\n",
      "epoch no.3 train no.316650  loss = 2.78373 avg_loss = 3.59382\n",
      "epoch no.3 train no.316660  loss = 2.35113 avg_loss = 3.53207\n",
      "epoch no.3 train no.316670  loss = 4.28605 avg_loss = 3.51830\n",
      "epoch no.3 train no.316680  loss = 3.60267 avg_loss = 3.46767\n",
      "epoch no.3 train no.316690  loss = 5.03297 avg_loss = 3.46704\n",
      "epoch no.3 train no.316700  loss = 4.76899 avg_loss = 3.50923\n",
      "epoch no.3 train no.316710  loss = 3.01769 avg_loss = 3.48528\n",
      "epoch no.3 train no.316720  loss = 2.75576 avg_loss = 3.47163\n",
      "epoch no.3 train no.316730  loss = 3.17008 avg_loss = 3.45509\n",
      "epoch no.3 train no.316740  loss = 5.17666 avg_loss = 3.54887\n",
      "epoch no.3 train no.316750  loss = 4.18380 avg_loss = 3.52198\n",
      "epoch no.3 train no.316760  loss = 3.20475 avg_loss = 3.51580\n",
      "epoch no.3 train no.316770  loss = 4.28952 avg_loss = 3.51530\n",
      "epoch no.3 train no.316780  loss = 1.79923 avg_loss = 3.48097\n",
      "epoch no.3 train no.316790  loss = 2.21360 avg_loss = 3.45629\n",
      "epoch no.3 train no.316800  loss = 2.25550 avg_loss = 3.45232\n",
      "epoch no.3 train no.316810  loss = 3.49402 avg_loss = 3.44090\n",
      "epoch no.3 train no.316820  loss = 2.69268 avg_loss = 3.42470\n",
      "epoch no.3 train no.316830  loss = 2.90681 avg_loss = 3.41699\n",
      "epoch no.3 train no.316840  loss = 3.96715 avg_loss = 3.43878\n",
      "epoch no.3 train no.316850  loss = 3.74687 avg_loss = 3.41568\n",
      "epoch no.3 train no.316860  loss = 3.52150 avg_loss = 3.41819\n",
      "epoch no.3 train no.316870  loss = 4.37976 avg_loss = 3.45400\n",
      "epoch no.3 train no.316880  loss = 2.88670 avg_loss = 3.49068\n",
      "epoch no.3 train no.316890  loss = 3.13207 avg_loss = 3.46889\n",
      "epoch no.3 train no.316900  loss = 3.91561 avg_loss = 3.51143\n",
      "epoch no.3 train no.316910  loss = 2.69639 avg_loss = 3.52358\n",
      "epoch no.3 train no.316920  loss = 3.58890 avg_loss = 3.52337\n",
      "epoch no.3 train no.316930  loss = 4.93834 avg_loss = 3.55362\n",
      "epoch no.3 train no.316940  loss = 4.17497 avg_loss = 3.55641\n",
      "epoch no.3 train no.316950  loss = 2.77316 avg_loss = 3.55377\n",
      "epoch no.3 train no.316960  loss = 5.07478 avg_loss = 3.60101\n",
      "epoch no.3 train no.316970  loss = 2.30126 avg_loss = 3.59648\n",
      "epoch no.3 train no.316980  loss = 2.31441 avg_loss = 3.58104\n",
      "epoch no.3 train no.316990  loss = 2.35540 avg_loss = 3.58570\n",
      "epoch no.3 train no.317000  loss = 3.74571 avg_loss = 3.59557\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁신나는', '송']\n",
      "기분전환이 필요할 때 듣는 팝</s>\n",
      "epoch no.3 train no.317010  loss = 3.71574 avg_loss = 3.56855\n",
      "epoch no.3 train no.317020  loss = 3.69051 avg_loss = 3.58036\n",
      "epoch no.3 train no.317030  loss = 3.42940 avg_loss = 3.63465\n",
      "epoch no.3 train no.317040  loss = 3.44491 avg_loss = 3.62483\n",
      "epoch no.3 train no.317050  loss = 2.93814 avg_loss = 3.60733\n",
      "epoch no.3 train no.317060  loss = 3.10868 avg_loss = 3.59157\n",
      "epoch no.3 train no.317070  loss = 2.37044 avg_loss = 3.55267\n",
      "epoch no.3 train no.317080  loss = 2.54190 avg_loss = 3.58544\n",
      "epoch no.3 train no.317090  loss = 4.64217 avg_loss = 3.60685\n",
      "epoch no.3 train no.317100  loss = 3.49581 avg_loss = 3.60533\n",
      "epoch no.3 train no.317110  loss = 2.84706 avg_loss = 3.61881\n",
      "epoch no.3 train no.317120  loss = 3.73777 avg_loss = 3.60680\n",
      "epoch no.3 train no.317130  loss = 3.16774 avg_loss = 3.60451\n",
      "epoch no.3 train no.317140  loss = 2.66529 avg_loss = 3.55993\n",
      "epoch no.3 train no.317150  loss = 2.54233 avg_loss = 3.53691\n",
      "epoch no.3 train no.317160  loss = 2.62564 avg_loss = 3.53433\n",
      "epoch no.3 train no.317170  loss = 3.43443 avg_loss = 3.56295\n",
      "epoch no.3 train no.317180  loss = 2.89237 avg_loss = 3.58536\n",
      "epoch no.3 train no.317190  loss = 2.24280 avg_loss = 3.57279\n",
      "epoch no.3 train no.317200  loss = 2.38710 avg_loss = 3.55487\n",
      "epoch no.3 train no.317210  loss = 3.66388 avg_loss = 3.57723\n",
      "epoch no.3 train no.317220  loss = 2.66775 avg_loss = 3.56621\n",
      "epoch no.3 train no.317230  loss = 2.91731 avg_loss = 3.55476\n",
      "epoch no.3 train no.317240  loss = 3.88558 avg_loss = 3.54844\n",
      "epoch no.3 train no.317250  loss = 3.41930 avg_loss = 3.54113\n",
      "epoch no.3 train no.317260  loss = 4.64727 avg_loss = 3.59565\n",
      "epoch no.3 train no.317270  loss = 3.13860 avg_loss = 3.62900\n",
      "epoch no.3 train no.317280  loss = 2.23135 avg_loss = 3.60460\n",
      "epoch no.3 train no.317290  loss = 5.56954 avg_loss = 3.59174\n",
      "epoch no.3 train no.317300  loss = 4.11570 avg_loss = 3.58932\n",
      "epoch no.3 train no.317310  loss = 3.13867 avg_loss = 3.53610\n",
      "epoch no.3 train no.317320  loss = 2.09623 avg_loss = 3.57031\n",
      "epoch no.3 train no.317330  loss = 4.59693 avg_loss = 3.58380\n",
      "epoch no.3 train no.317340  loss = 2.96373 avg_loss = 3.62488\n",
      "epoch no.3 train no.317350  loss = 2.32778 avg_loss = 3.56099\n",
      "epoch no.3 train no.317360  loss = 3.03260 avg_loss = 3.57649\n",
      "epoch no.3 train no.317370  loss = 2.80587 avg_loss = 3.58167\n",
      "epoch no.3 train no.317380  loss = 3.08709 avg_loss = 3.57486\n",
      "epoch no.3 train no.317390  loss = 4.11224 avg_loss = 3.59486\n",
      "epoch no.3 train no.317400  loss = 1.72274 avg_loss = 3.54180\n",
      "epoch no.3 train no.317410  loss = 3.43614 avg_loss = 3.53744\n",
      "epoch no.3 train no.317420  loss = 1.75133 avg_loss = 3.49913\n",
      "epoch no.3 train no.317430  loss = 1.85055 avg_loss = 3.46906\n",
      "epoch no.3 train no.317440  loss = 2.04723 avg_loss = 3.46252\n",
      "epoch no.3 train no.317450  loss = 3.29076 avg_loss = 3.43850\n",
      "epoch no.3 train no.317460  loss = 4.69464 avg_loss = 3.43589\n",
      "epoch no.3 train no.317470  loss = 3.24075 avg_loss = 3.39779\n",
      "epoch no.3 train no.317480  loss = 4.13043 avg_loss = 3.41354\n",
      "epoch no.3 train no.317490  loss = 3.20760 avg_loss = 3.43409\n",
      "epoch no.3 train no.317500  loss = 3.91859 avg_loss = 3.46237\n",
      "epoch no.3 train no.317510  loss = 2.53679 avg_loss = 3.44109\n",
      "epoch no.3 train no.317520  loss = 3.18545 avg_loss = 3.42903\n",
      "epoch no.3 train no.317530  loss = 3.13027 avg_loss = 3.44273\n",
      "epoch no.3 train no.317540  loss = 2.28999 avg_loss = 3.43812\n",
      "epoch no.3 train no.317550  loss = 3.40783 avg_loss = 3.44727\n",
      "epoch no.3 train no.317560  loss = 4.35143 avg_loss = 3.45669\n",
      "epoch no.3 train no.317570  loss = 3.48714 avg_loss = 3.46214\n",
      "epoch no.3 train no.317580  loss = 6.00814 avg_loss = 3.50823\n",
      "epoch no.3 train no.317590  loss = 4.35803 avg_loss = 3.51583\n",
      "epoch no.3 train no.317600  loss = 2.61163 avg_loss = 3.49666\n",
      "epoch no.3 train no.317610  loss = 2.52656 avg_loss = 3.47862\n",
      "epoch no.3 train no.317620  loss = 3.08553 avg_loss = 3.49656\n",
      "epoch no.3 train no.317630  loss = 3.90246 avg_loss = 3.47876\n",
      "epoch no.3 train no.317640  loss = 4.28018 avg_loss = 3.49947\n",
      "epoch no.3 train no.317650  loss = 5.09872 avg_loss = 3.47143\n",
      "epoch no.3 train no.317660  loss = 2.96854 avg_loss = 3.42678\n",
      "epoch no.3 train no.317670  loss = 2.07351 avg_loss = 3.43886\n",
      "epoch no.3 train no.317680  loss = 4.30025 avg_loss = 3.44240\n",
      "epoch no.3 train no.317690  loss = 2.49838 avg_loss = 3.51501\n",
      "epoch no.3 train no.317700  loss = 4.45659 avg_loss = 3.51179\n",
      "epoch no.3 train no.317710  loss = 3.81040 avg_loss = 3.49080\n",
      "epoch no.3 train no.317720  loss = 4.30292 avg_loss = 3.50551\n",
      "epoch no.3 train no.317730  loss = 3.22935 avg_loss = 3.48107\n",
      "epoch no.3 train no.317740  loss = 3.16587 avg_loss = 3.46844\n",
      "epoch no.3 train no.317750  loss = 3.99363 avg_loss = 3.51132\n",
      "epoch no.3 train no.317760  loss = 3.77835 avg_loss = 3.51387\n",
      "epoch no.3 train no.317770  loss = 3.46726 avg_loss = 3.51619\n",
      "epoch no.3 train no.317780  loss = 3.37306 avg_loss = 3.49307\n",
      "epoch no.3 train no.317790  loss = 3.95643 avg_loss = 3.50458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.317800  loss = 3.00144 avg_loss = 3.50273\n",
      "epoch no.3 train no.317810  loss = 5.71283 avg_loss = 3.50943\n",
      "epoch no.3 train no.317820  loss = 2.53280 avg_loss = 3.48837\n",
      "epoch no.3 train no.317830  loss = 3.73392 avg_loss = 3.50001\n",
      "epoch no.3 train no.317840  loss = 2.47845 avg_loss = 3.43454\n",
      "epoch no.3 train no.317850  loss = 2.69664 avg_loss = 3.41457\n",
      "epoch no.3 train no.317860  loss = 3.67826 avg_loss = 3.40361\n",
      "epoch no.3 train no.317870  loss = 2.14971 avg_loss = 3.41142\n",
      "epoch no.3 train no.317880  loss = 2.42089 avg_loss = 3.42900\n",
      "epoch no.3 train no.317890  loss = 2.59322 avg_loss = 3.39899\n",
      "epoch no.3 train no.317900  loss = 2.96547 avg_loss = 3.39288\n",
      "epoch no.3 train no.317910  loss = 3.58643 avg_loss = 3.41168\n",
      "epoch no.3 train no.317920  loss = 3.35123 avg_loss = 3.42503\n",
      "epoch no.3 train no.317930  loss = 2.90043 avg_loss = 3.42117\n",
      "epoch no.3 train no.317940  loss = 3.07371 avg_loss = 3.39048\n",
      "epoch no.3 train no.317950  loss = 3.53110 avg_loss = 3.40072\n",
      "epoch no.3 train no.317960  loss = 2.97481 avg_loss = 3.39448\n",
      "epoch no.3 train no.317970  loss = 2.55909 avg_loss = 3.39312\n",
      "epoch no.3 train no.317980  loss = 3.53787 avg_loss = 3.37561\n",
      "epoch no.3 train no.317990  loss = 4.34931 avg_loss = 3.39198\n",
      "epoch no.3 train no.318000  loss = 5.37467 avg_loss = 3.43046\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋', '이', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.3 train no.318010  loss = 2.61867 avg_loss = 3.47785\n",
      "epoch no.3 train no.318020  loss = 2.74721 avg_loss = 3.49241\n",
      "epoch no.3 train no.318030  loss = 3.97289 avg_loss = 3.49527\n",
      "epoch no.3 train no.318040  loss = 3.26381 avg_loss = 3.51013\n",
      "epoch no.3 train no.318050  loss = 3.12394 avg_loss = 3.48034\n",
      "epoch no.3 train no.318060  loss = 4.37666 avg_loss = 3.50730\n",
      "epoch no.3 train no.318070  loss = 2.57321 avg_loss = 3.47464\n",
      "epoch no.3 train no.318080  loss = 2.95821 avg_loss = 3.46113\n",
      "epoch no.3 train no.318090  loss = 6.22172 avg_loss = 3.50340\n",
      "epoch no.3 train no.318100  loss = 4.27488 avg_loss = 3.46434\n",
      "epoch no.3 train no.318110  loss = 4.73029 avg_loss = 3.52653\n",
      "epoch no.3 train no.318120  loss = 2.69483 avg_loss = 3.54437\n",
      "epoch no.3 train no.318130  loss = 3.03787 avg_loss = 3.57393\n",
      "epoch no.3 train no.318140  loss = 3.48001 avg_loss = 3.55935\n",
      "epoch no.3 train no.318150  loss = 3.57550 avg_loss = 3.54487\n",
      "epoch no.3 train no.318160  loss = 4.24864 avg_loss = 3.55416\n",
      "epoch no.3 train no.318170  loss = 3.06647 avg_loss = 3.58951\n",
      "epoch no.3 train no.318180  loss = 6.01662 avg_loss = 3.60041\n",
      "epoch no.3 train no.318190  loss = 2.36341 avg_loss = 3.61710\n",
      "epoch no.3 train no.318200  loss = 2.80775 avg_loss = 3.57969\n",
      "epoch no.3 train no.318210  loss = 3.22136 avg_loss = 3.56334\n",
      "epoch no.3 train no.318220  loss = 3.29607 avg_loss = 3.58858\n",
      "epoch no.3 train no.318230  loss = 2.36103 avg_loss = 3.59008\n",
      "epoch no.3 train no.318240  loss = 4.84541 avg_loss = 3.59254\n",
      "epoch no.3 train no.318250  loss = 2.40096 avg_loss = 3.65084\n",
      "epoch no.3 train no.318260  loss = 2.34424 avg_loss = 3.64148\n",
      "epoch no.3 train no.318270  loss = 4.05118 avg_loss = 3.64008\n",
      "epoch no.3 train no.318280  loss = 3.11526 avg_loss = 3.61665\n",
      "epoch no.3 train no.318290  loss = 3.63868 avg_loss = 3.56830\n",
      "epoch no.3 train no.318300  loss = 2.94963 avg_loss = 3.60551\n",
      "epoch no.3 train no.318310  loss = 3.89343 avg_loss = 3.59079\n",
      "epoch no.3 train no.318320  loss = 4.04950 avg_loss = 3.55747\n",
      "epoch no.3 train no.318330  loss = 3.75907 avg_loss = 3.56928\n",
      "epoch no.3 train no.318340  loss = 3.17473 avg_loss = 3.54203\n",
      "epoch no.3 train no.318350  loss = 2.37090 avg_loss = 3.49805\n",
      "epoch no.3 train no.318360  loss = 4.06479 avg_loss = 3.49399\n",
      "epoch no.3 train no.318370  loss = 3.31425 avg_loss = 3.49342\n",
      "epoch no.3 train no.318380  loss = 2.81843 avg_loss = 3.53702\n",
      "epoch no.3 train no.318390  loss = 3.30642 avg_loss = 3.49628\n",
      "epoch no.3 train no.318400  loss = 4.21556 avg_loss = 3.45450\n",
      "epoch no.3 train no.318410  loss = 3.73459 avg_loss = 3.45009\n",
      "epoch no.3 train no.318420  loss = 3.75545 avg_loss = 3.44568\n",
      "epoch no.3 train no.318430  loss = 4.43986 avg_loss = 3.51086\n",
      "epoch no.3 train no.318440  loss = 6.51602 avg_loss = 3.60474\n",
      "epoch no.3 train no.318450  loss = 3.35019 avg_loss = 3.57045\n",
      "epoch no.3 train no.318460  loss = 3.60721 avg_loss = 3.54874\n",
      "epoch no.3 train no.318470  loss = 3.52917 avg_loss = 3.54912\n",
      "epoch no.3 train no.318480  loss = 3.41825 avg_loss = 3.48725\n",
      "epoch no.3 train no.318490  loss = 4.31231 avg_loss = 3.46270\n",
      "epoch no.3 train no.318500  loss = 2.10240 avg_loss = 3.42870\n",
      "epoch no.3 train no.318510  loss = 2.42780 avg_loss = 3.44284\n",
      "epoch no.3 train no.318520  loss = 3.31889 avg_loss = 3.47809\n",
      "epoch no.3 train no.318530  loss = 3.90317 avg_loss = 3.44134\n",
      "epoch no.3 train no.318540  loss = 1.83987 avg_loss = 3.39698\n",
      "epoch no.3 train no.318550  loss = 2.85044 avg_loss = 3.41020\n",
      "epoch no.3 train no.318560  loss = 3.93786 avg_loss = 3.46560\n",
      "epoch no.3 train no.318570  loss = 2.08680 avg_loss = 3.45698\n",
      "epoch no.3 train no.318580  loss = 2.12510 avg_loss = 3.44045\n",
      "epoch no.3 train no.318590  loss = 1.98648 avg_loss = 3.38813\n",
      "epoch no.3 train no.318600  loss = 4.65194 avg_loss = 3.45013\n",
      "epoch no.3 train no.318610  loss = 4.31660 avg_loss = 3.47066\n",
      "epoch no.3 train no.318620  loss = 3.56993 avg_loss = 3.48945\n",
      "epoch no.3 train no.318630  loss = 3.62455 avg_loss = 3.52532\n",
      "epoch no.3 train no.318640  loss = 2.34159 avg_loss = 3.55280\n",
      "epoch no.3 train no.318650  loss = 1.57623 avg_loss = 3.49288\n",
      "epoch no.3 train no.318660  loss = 3.83966 avg_loss = 3.50139\n",
      "epoch no.3 train no.318670  loss = 2.31088 avg_loss = 3.45726\n",
      "epoch no.3 train no.318680  loss = 4.55790 avg_loss = 3.46396\n",
      "epoch no.3 train no.318690  loss = 2.48358 avg_loss = 3.41554\n",
      "epoch no.3 train no.318700  loss = 2.94704 avg_loss = 3.40942\n",
      "epoch no.3 train no.318710  loss = 3.74979 avg_loss = 3.46032\n",
      "epoch no.3 train no.318720  loss = 3.35139 avg_loss = 3.50113\n",
      "epoch no.3 train no.318730  loss = 1.27927 avg_loss = 3.45630\n",
      "epoch no.3 train no.318740  loss = 1.91730 avg_loss = 3.42180\n",
      "epoch no.3 train no.318750  loss = 3.87351 avg_loss = 3.43456\n",
      "epoch no.3 train no.318760  loss = 2.75571 avg_loss = 3.43489\n",
      "epoch no.3 train no.318770  loss = 3.36472 avg_loss = 3.41677\n",
      "epoch no.3 train no.318780  loss = 2.63558 avg_loss = 3.40953\n",
      "epoch no.3 train no.318790  loss = 3.89497 avg_loss = 3.39933\n",
      "epoch no.3 train no.318800  loss = 4.16148 avg_loss = 3.41716\n",
      "epoch no.3 train no.318810  loss = 2.31063 avg_loss = 3.39214\n",
      "epoch no.3 train no.318820  loss = 3.63239 avg_loss = 3.41723\n",
      "epoch no.3 train no.318830  loss = 2.57653 avg_loss = 3.40362\n",
      "epoch no.3 train no.318840  loss = 2.72018 avg_loss = 3.40577\n",
      "epoch no.3 train no.318850  loss = 3.14110 avg_loss = 3.42336\n",
      "epoch no.3 train no.318860  loss = 2.46026 avg_loss = 3.41156\n",
      "epoch no.3 train no.318870  loss = 2.94048 avg_loss = 3.43243\n",
      "epoch no.3 train no.318880  loss = 2.27026 avg_loss = 3.42243\n",
      "epoch no.3 train no.318890  loss = 3.11225 avg_loss = 3.45361\n",
      "epoch no.3 train no.318900  loss = 2.77412 avg_loss = 3.41983\n",
      "epoch no.3 train no.318910  loss = 3.60456 avg_loss = 3.48573\n",
      "epoch no.3 train no.318920  loss = 3.74137 avg_loss = 3.48546\n",
      "epoch no.3 train no.318930  loss = 3.03588 avg_loss = 3.48758\n",
      "epoch no.3 train no.318940  loss = 2.83938 avg_loss = 3.48640\n",
      "epoch no.3 train no.318950  loss = 3.01088 avg_loss = 3.44228\n",
      "epoch no.3 train no.318960  loss = 4.06610 avg_loss = 3.48418\n",
      "epoch no.3 train no.318970  loss = 4.12887 avg_loss = 3.47369\n",
      "epoch no.3 train no.318980  loss = 2.22988 avg_loss = 3.44001\n",
      "epoch no.3 train no.318990  loss = 4.89582 avg_loss = 3.50778\n",
      "epoch no.3 train no.319000  loss = 3.16740 avg_loss = 3.52120\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁신나는', '▁팝', '</s>']\n",
      "기분전환을 위한 신나는 음악</s>\n",
      "epoch no.3 train no.319010  loss = 3.46655 avg_loss = 3.46653\n",
      "epoch no.3 train no.319020  loss = 2.64111 avg_loss = 3.43570\n",
      "epoch no.3 train no.319030  loss = 3.46422 avg_loss = 3.47463\n",
      "epoch no.3 train no.319040  loss = 3.79643 avg_loss = 3.48567\n",
      "epoch no.3 train no.319050  loss = 2.91699 avg_loss = 3.48877\n",
      "epoch no.3 train no.319060  loss = 3.34426 avg_loss = 3.53625\n",
      "epoch no.3 train no.319070  loss = 4.18764 avg_loss = 3.50622\n",
      "epoch no.3 train no.319080  loss = 4.85700 avg_loss = 3.52672\n",
      "epoch no.3 train no.319090  loss = 3.68376 avg_loss = 3.51012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.319100  loss = 2.99395 avg_loss = 3.51727\n",
      "epoch no.3 train no.319110  loss = 2.53426 avg_loss = 3.49971\n",
      "epoch no.3 train no.319120  loss = 3.20045 avg_loss = 3.51126\n",
      "epoch no.3 train no.319130  loss = 2.60664 avg_loss = 3.49211\n",
      "epoch no.3 train no.319140  loss = 4.48922 avg_loss = 3.50977\n",
      "epoch no.3 train no.319150  loss = 3.16953 avg_loss = 3.50629\n",
      "epoch no.3 train no.319160  loss = 4.61595 avg_loss = 3.50893\n",
      "epoch no.3 train no.319170  loss = 4.81590 avg_loss = 3.51700\n",
      "epoch no.3 train no.319180  loss = 2.43442 avg_loss = 3.49651\n",
      "epoch no.3 train no.319190  loss = 2.38819 avg_loss = 3.46541\n",
      "epoch no.3 train no.319200  loss = 3.08174 avg_loss = 3.50948\n",
      "epoch no.3 train no.319210  loss = 4.58332 avg_loss = 3.55962\n",
      "epoch no.3 train no.319220  loss = 2.35062 avg_loss = 3.52703\n",
      "epoch no.3 train no.319230  loss = 1.91130 avg_loss = 3.50431\n",
      "epoch no.3 train no.319240  loss = 3.56667 avg_loss = 3.49696\n",
      "epoch no.3 train no.319250  loss = 2.75016 avg_loss = 3.48193\n",
      "epoch no.3 train no.319260  loss = 4.60110 avg_loss = 3.46572\n",
      "epoch no.3 train no.319270  loss = 3.26199 avg_loss = 3.48733\n",
      "epoch no.3 train no.319280  loss = 3.76996 avg_loss = 3.47301\n",
      "epoch no.3 train no.319290  loss = 3.75265 avg_loss = 3.50550\n",
      "epoch no.3 train no.319300  loss = 4.35148 avg_loss = 3.52844\n",
      "epoch no.3 train no.319310  loss = 2.16964 avg_loss = 3.53856\n",
      "epoch no.3 train no.319320  loss = 6.40097 avg_loss = 3.54742\n",
      "epoch no.3 train no.319330  loss = 1.87114 avg_loss = 3.54301\n",
      "epoch no.3 train no.319340  loss = 4.56559 avg_loss = 3.55962\n",
      "epoch no.3 train no.319350  loss = 3.41513 avg_loss = 3.53041\n",
      "epoch no.3 train no.319360  loss = 3.29018 avg_loss = 3.49783\n",
      "epoch no.3 train no.319370  loss = 3.72146 avg_loss = 3.52155\n",
      "epoch no.3 train no.319380  loss = 2.80324 avg_loss = 3.50141\n",
      "epoch no.3 train no.319390  loss = 3.07819 avg_loss = 3.43251\n",
      "epoch no.3 train no.319400  loss = 3.96597 avg_loss = 3.43638\n",
      "epoch no.3 train no.319410  loss = 3.23946 avg_loss = 3.44564\n",
      "epoch no.3 train no.319420  loss = 4.21806 avg_loss = 3.45662\n",
      "epoch no.3 train no.319430  loss = 3.97217 avg_loss = 3.44060\n",
      "epoch no.3 train no.319440  loss = 3.72686 avg_loss = 3.45584\n",
      "epoch no.3 train no.319450  loss = 3.46849 avg_loss = 3.45207\n",
      "epoch no.3 train no.319460  loss = 5.05845 avg_loss = 3.42913\n",
      "epoch no.3 train no.319470  loss = 3.17236 avg_loss = 3.38327\n",
      "epoch no.3 train no.319480  loss = 5.20256 avg_loss = 3.42811\n",
      "epoch no.3 train no.319490  loss = 4.06562 avg_loss = 3.41958\n",
      "epoch no.3 train no.319500  loss = 2.44696 avg_loss = 3.45218\n",
      "epoch no.3 train no.319510  loss = 3.81478 avg_loss = 3.45599\n",
      "epoch no.3 train no.319520  loss = 4.08438 avg_loss = 3.46575\n",
      "epoch no.3 train no.319530  loss = 3.18280 avg_loss = 3.41872\n",
      "epoch no.3 train no.319540  loss = 3.33044 avg_loss = 3.47255\n",
      "epoch no.3 train no.319550  loss = 3.18583 avg_loss = 3.47726\n",
      "epoch no.3 train no.319560  loss = 2.29600 avg_loss = 3.47807\n",
      "epoch no.3 train no.319570  loss = 4.12093 avg_loss = 3.45953\n",
      "epoch no.3 train no.319580  loss = 2.29720 avg_loss = 3.42597\n",
      "epoch no.3 train no.319590  loss = 2.62360 avg_loss = 3.45512\n",
      "epoch no.3 train no.319600  loss = 4.12994 avg_loss = 3.47721\n",
      "epoch no.3 train no.319610  loss = 2.32885 avg_loss = 3.48587\n",
      "epoch no.3 train no.319620  loss = 3.65317 avg_loss = 3.48687\n",
      "epoch no.3 train no.319630  loss = 2.87733 avg_loss = 3.47353\n",
      "epoch no.3 train no.319640  loss = 3.89227 avg_loss = 3.44771\n",
      "epoch no.3 train no.319650  loss = 3.88450 avg_loss = 3.45043\n",
      "epoch no.3 train no.319660  loss = 2.52802 avg_loss = 3.50374\n",
      "epoch no.3 train no.319670  loss = 2.50945 avg_loss = 3.49999\n",
      "epoch no.3 train no.319680  loss = 3.94724 avg_loss = 3.50792\n",
      "epoch no.3 train no.319690  loss = 3.83403 avg_loss = 3.51508\n",
      "epoch no.3 train no.319700  loss = 3.49060 avg_loss = 3.49063\n",
      "epoch no.3 train no.319710  loss = 4.49101 avg_loss = 3.52020\n",
      "epoch no.3 train no.319720  loss = 2.56462 avg_loss = 3.46771\n",
      "epoch no.3 train no.319730  loss = 3.59833 avg_loss = 3.50744\n",
      "epoch no.3 train no.319740  loss = 4.47362 avg_loss = 3.55118\n",
      "epoch no.3 train no.319750  loss = 4.20118 avg_loss = 3.61942\n",
      "epoch no.3 train no.319760  loss = 2.23601 avg_loss = 3.59215\n",
      "epoch no.3 train no.319770  loss = 4.76348 avg_loss = 3.58083\n",
      "epoch no.3 train no.319780  loss = 4.64133 avg_loss = 3.58748\n",
      "epoch no.3 train no.319790  loss = 3.39522 avg_loss = 3.54351\n",
      "epoch no.3 train no.319800  loss = 4.01444 avg_loss = 3.53373\n",
      "epoch no.3 train no.319810  loss = 2.10363 avg_loss = 3.56023\n",
      "epoch no.3 train no.319820  loss = 2.65245 avg_loss = 3.49835\n",
      "epoch no.3 train no.319830  loss = 2.12591 avg_loss = 3.45940\n",
      "epoch no.3 train no.319840  loss = 1.98992 avg_loss = 3.43907\n",
      "epoch no.3 train no.319850  loss = 3.12188 avg_loss = 3.49065\n",
      "epoch no.3 train no.319860  loss = 3.13337 avg_loss = 3.50880\n",
      "epoch no.3 train no.319870  loss = 2.08995 avg_loss = 3.54634\n",
      "epoch no.3 train no.319880  loss = 4.32260 avg_loss = 3.56659\n",
      "epoch no.3 train no.319890  loss = 4.12642 avg_loss = 3.64675\n",
      "epoch no.3 train no.319900  loss = 3.38473 avg_loss = 3.63033\n",
      "epoch no.3 train no.319910  loss = 2.64216 avg_loss = 3.62424\n",
      "epoch no.3 train no.319920  loss = 2.78104 avg_loss = 3.60604\n",
      "epoch no.3 train no.319930  loss = 2.19679 avg_loss = 3.55054\n",
      "epoch no.3 train no.319940  loss = 3.60207 avg_loss = 3.56483\n",
      "epoch no.3 train no.319950  loss = 2.82297 avg_loss = 3.48082\n",
      "epoch no.3 train no.319960  loss = 3.82042 avg_loss = 3.48330\n",
      "epoch no.3 train no.319970  loss = 3.64343 avg_loss = 3.46377\n",
      "epoch no.3 train no.319980  loss = 2.81738 avg_loss = 3.48628\n",
      "epoch no.3 train no.319990  loss = 2.73955 avg_loss = 3.50217\n",
      "epoch no.3 train no.320000  loss = 7.01683 avg_loss = 3.55599\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '에', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.320010  loss = 3.09689 avg_loss = 3.54777\n",
      "epoch no.3 train no.320020  loss = 4.92411 avg_loss = 3.56026\n",
      "epoch no.3 train no.320030  loss = 4.24306 avg_loss = 3.60221\n",
      "epoch no.3 train no.320040  loss = 3.81393 avg_loss = 3.58300\n",
      "epoch no.3 train no.320050  loss = 1.99833 avg_loss = 3.55518\n",
      "epoch no.3 train no.320060  loss = 5.25622 avg_loss = 3.54156\n",
      "epoch no.3 train no.320070  loss = 4.92467 avg_loss = 3.60588\n",
      "epoch no.3 train no.320080  loss = 3.20970 avg_loss = 3.57636\n",
      "epoch no.3 train no.320090  loss = 4.07267 avg_loss = 3.58209\n",
      "epoch no.3 train no.320100  loss = 3.34751 avg_loss = 3.52090\n",
      "epoch no.3 train no.320110  loss = 3.15732 avg_loss = 3.55570\n",
      "epoch no.3 train no.320120  loss = 4.44681 avg_loss = 3.61897\n",
      "epoch no.3 train no.320130  loss = 2.55645 avg_loss = 3.59091\n",
      "epoch no.3 train no.320140  loss = 4.16945 avg_loss = 3.58639\n",
      "epoch no.3 train no.320150  loss = 2.33710 avg_loss = 3.55271\n",
      "epoch no.3 train no.320160  loss = 3.45064 avg_loss = 3.59616\n",
      "epoch no.3 train no.320170  loss = 2.62554 avg_loss = 3.57054\n",
      "epoch no.3 train no.320180  loss = 3.33791 avg_loss = 3.57230\n",
      "epoch no.3 train no.320190  loss = 2.06802 avg_loss = 3.49811\n",
      "epoch no.3 train no.320200  loss = 3.57749 avg_loss = 3.52090\n",
      "epoch no.3 train no.320210  loss = 3.20089 avg_loss = 3.51879\n",
      "epoch no.3 train no.320220  loss = 3.14730 avg_loss = 3.48773\n",
      "epoch no.3 train no.320230  loss = 3.13139 avg_loss = 3.49728\n",
      "epoch no.3 train no.320240  loss = 4.20784 avg_loss = 3.56734\n",
      "epoch no.3 train no.320250  loss = 2.42685 avg_loss = 3.53774\n",
      "epoch no.3 train no.320260  loss = 3.47319 avg_loss = 3.57785\n",
      "epoch no.3 train no.320270  loss = 2.91551 avg_loss = 3.50672\n",
      "epoch no.3 train no.320280  loss = 2.57431 avg_loss = 3.46012\n",
      "epoch no.3 train no.320290  loss = 2.97827 avg_loss = 3.42505\n",
      "epoch no.3 train no.320300  loss = 4.82251 avg_loss = 3.42938\n",
      "epoch no.3 train no.320310  loss = 4.21118 avg_loss = 3.48476\n",
      "epoch no.3 train no.320320  loss = 3.75100 avg_loss = 3.53108\n",
      "epoch no.3 train no.320330  loss = 3.25169 avg_loss = 3.48904\n",
      "epoch no.3 train no.320340  loss = 3.14522 avg_loss = 3.48277\n",
      "epoch no.3 train no.320350  loss = 3.21648 avg_loss = 3.45366\n",
      "epoch no.3 train no.320360  loss = 2.59228 avg_loss = 3.49712\n",
      "epoch no.3 train no.320370  loss = 3.44149 avg_loss = 3.50242\n",
      "epoch no.3 train no.320380  loss = 4.18555 avg_loss = 3.53142\n",
      "epoch no.3 train no.320390  loss = 3.65901 avg_loss = 3.51570\n",
      "epoch no.3 train no.320400  loss = 3.81888 avg_loss = 3.51512\n",
      "epoch no.3 train no.320410  loss = 3.07060 avg_loss = 3.51844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.320420  loss = 3.26229 avg_loss = 3.44638\n",
      "epoch no.3 train no.320430  loss = 2.96127 avg_loss = 3.43052\n",
      "epoch no.3 train no.320440  loss = 3.66793 avg_loss = 3.42155\n",
      "epoch no.3 train no.320450  loss = 4.27526 avg_loss = 3.43335\n",
      "epoch no.3 train no.320460  loss = 2.82895 avg_loss = 3.47127\n",
      "epoch no.3 train no.320470  loss = 3.85766 avg_loss = 3.47433\n",
      "epoch no.3 train no.320480  loss = 3.59714 avg_loss = 3.46304\n",
      "epoch no.3 train no.320490  loss = 4.28623 avg_loss = 3.50347\n",
      "epoch no.3 train no.320500  loss = 4.14530 avg_loss = 3.49576\n",
      "epoch no.3 train no.320510  loss = 3.20784 avg_loss = 3.50686\n",
      "epoch no.3 train no.320520  loss = 2.83876 avg_loss = 3.50422\n",
      "epoch no.3 train no.320530  loss = 4.18468 avg_loss = 3.47582\n",
      "epoch no.3 train no.320540  loss = 4.30712 avg_loss = 3.52142\n",
      "epoch no.3 train no.320550  loss = 3.10533 avg_loss = 3.52912\n",
      "epoch no.3 train no.320560  loss = 2.73226 avg_loss = 3.50683\n",
      "epoch no.3 train no.320570  loss = 4.67225 avg_loss = 3.50265\n",
      "epoch no.3 train no.320580  loss = 2.82668 avg_loss = 3.52450\n",
      "epoch no.3 train no.320590  loss = 2.85147 avg_loss = 3.52564\n",
      "epoch no.3 train no.320600  loss = 4.06263 avg_loss = 3.52181\n",
      "epoch no.3 train no.320610  loss = 5.12702 avg_loss = 3.51727\n",
      "epoch no.3 train no.320620  loss = 2.92999 avg_loss = 3.45110\n",
      "epoch no.3 train no.320630  loss = 5.42070 avg_loss = 3.46424\n",
      "epoch no.3 train no.320640  loss = 1.69193 avg_loss = 3.43462\n",
      "epoch no.3 train no.320650  loss = 4.83205 avg_loss = 3.52056\n",
      "epoch no.3 train no.320660  loss = 4.35046 avg_loss = 3.49236\n",
      "epoch no.3 train no.320670  loss = 4.54363 avg_loss = 3.47750\n",
      "epoch no.3 train no.320680  loss = 5.78184 avg_loss = 3.48928\n",
      "epoch no.3 train no.320690  loss = 3.21186 avg_loss = 3.48414\n",
      "epoch no.3 train no.320700  loss = 5.98999 avg_loss = 3.51283\n",
      "epoch no.3 train no.320710  loss = 3.34251 avg_loss = 3.50261\n",
      "epoch no.3 train no.320720  loss = 2.48700 avg_loss = 3.51310\n",
      "epoch no.3 train no.320730  loss = 2.47184 avg_loss = 3.44829\n",
      "epoch no.3 train no.320740  loss = 4.93531 avg_loss = 3.43751\n",
      "epoch no.3 train no.320750  loss = 2.92276 avg_loss = 3.46391\n",
      "epoch no.3 train no.320760  loss = 3.10954 avg_loss = 3.44151\n",
      "epoch no.3 train no.320770  loss = 3.83916 avg_loss = 3.48533\n",
      "epoch no.3 train no.320780  loss = 2.44012 avg_loss = 3.50059\n",
      "epoch no.3 train no.320790  loss = 4.89307 avg_loss = 3.45513\n",
      "epoch no.3 train no.320800  loss = 4.53317 avg_loss = 3.48179\n",
      "epoch no.3 train no.320810  loss = 3.26840 avg_loss = 3.49388\n",
      "epoch no.3 train no.320820  loss = 3.18486 avg_loss = 3.52317\n",
      "epoch no.3 train no.320830  loss = 3.37815 avg_loss = 3.51385\n",
      "epoch no.3 train no.320840  loss = 2.35046 avg_loss = 3.49654\n",
      "epoch no.3 train no.320850  loss = 4.33140 avg_loss = 3.47756\n",
      "epoch no.3 train no.320860  loss = 5.12835 avg_loss = 3.46987\n",
      "epoch no.3 train no.320870  loss = 3.09978 avg_loss = 3.40750\n",
      "epoch no.3 train no.320880  loss = 3.18460 avg_loss = 3.45030\n",
      "epoch no.3 train no.320890  loss = 4.82233 avg_loss = 3.46932\n",
      "epoch no.3 train no.320900  loss = 2.56375 avg_loss = 3.44239\n",
      "epoch no.3 train no.320910  loss = 2.59086 avg_loss = 3.41765\n",
      "epoch no.3 train no.320920  loss = 6.16227 avg_loss = 3.52073\n",
      "epoch no.3 train no.320930  loss = 2.81377 avg_loss = 3.52383\n",
      "epoch no.3 train no.320940  loss = 5.10361 avg_loss = 3.53375\n",
      "epoch no.3 train no.320950  loss = 4.78326 avg_loss = 3.57257\n",
      "epoch no.3 train no.320960  loss = 7.23046 avg_loss = 3.60893\n",
      "epoch no.3 train no.320970  loss = 4.56619 avg_loss = 3.65029\n",
      "epoch no.3 train no.320980  loss = 3.52671 avg_loss = 3.59109\n",
      "epoch no.3 train no.320990  loss = 2.14882 avg_loss = 3.58183\n",
      "epoch no.3 train no.321000  loss = 4.39395 avg_loss = 3.61567\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '▁신나는', '▁댄스', '곡', '</s>']\n",
      "기분전환 신나는 댄스곡</s>\n",
      "epoch no.3 train no.321010  loss = 1.69988 avg_loss = 3.56465\n",
      "epoch no.3 train no.321020  loss = 4.24846 avg_loss = 3.57463\n",
      "epoch no.3 train no.321030  loss = 3.38065 avg_loss = 3.55525\n",
      "epoch no.3 train no.321040  loss = 4.21285 avg_loss = 3.59906\n",
      "epoch no.3 train no.321050  loss = 3.21498 avg_loss = 3.54488\n",
      "epoch no.3 train no.321060  loss = 3.08332 avg_loss = 3.53267\n",
      "epoch no.3 train no.321070  loss = 2.63046 avg_loss = 3.51973\n",
      "epoch no.3 train no.321080  loss = 1.27754 avg_loss = 3.50806\n",
      "epoch no.3 train no.321090  loss = 2.49648 avg_loss = 3.51244\n",
      "epoch no.3 train no.321100  loss = 4.37626 avg_loss = 3.50610\n",
      "epoch no.3 train no.321110  loss = 2.61822 avg_loss = 3.48680\n",
      "epoch no.3 train no.321120  loss = 4.76453 avg_loss = 3.45973\n",
      "epoch no.3 train no.321130  loss = 3.53401 avg_loss = 3.51441\n",
      "epoch no.3 train no.321140  loss = 4.35230 avg_loss = 3.51842\n",
      "epoch no.3 train no.321150  loss = 3.03716 avg_loss = 3.54415\n",
      "epoch no.3 train no.321160  loss = 3.24757 avg_loss = 3.55010\n",
      "epoch no.3 train no.321170  loss = 3.03159 avg_loss = 3.58232\n",
      "epoch no.3 train no.321180  loss = 2.68654 avg_loss = 3.56828\n",
      "epoch no.3 train no.321190  loss = 2.43459 avg_loss = 3.54894\n",
      "epoch no.3 train no.321200  loss = 3.06741 avg_loss = 3.53100\n",
      "epoch no.3 train no.321210  loss = 2.20914 avg_loss = 3.52179\n",
      "epoch no.3 train no.321220  loss = 3.32735 avg_loss = 3.56485\n",
      "epoch no.3 train no.321230  loss = 2.39430 avg_loss = 3.52043\n",
      "epoch no.3 train no.321240  loss = 1.67656 avg_loss = 3.47390\n",
      "epoch no.3 train no.321250  loss = 3.90826 avg_loss = 3.49361\n",
      "epoch no.3 train no.321260  loss = 2.76294 avg_loss = 3.50838\n",
      "epoch no.3 train no.321270  loss = 3.92090 avg_loss = 3.52416\n",
      "epoch no.3 train no.321280  loss = 2.91494 avg_loss = 3.47944\n",
      "epoch no.3 train no.321290  loss = 3.66647 avg_loss = 3.51552\n",
      "epoch no.3 train no.321300  loss = 5.03801 avg_loss = 3.49131\n",
      "epoch no.3 train no.321310  loss = 2.07859 avg_loss = 3.42636\n",
      "epoch no.3 train no.321320  loss = 3.96485 avg_loss = 3.35883\n",
      "epoch no.3 train no.321330  loss = 4.93395 avg_loss = 3.43720\n",
      "epoch no.3 train no.321340  loss = 3.39348 avg_loss = 3.39916\n",
      "epoch no.3 train no.321350  loss = 3.25335 avg_loss = 3.40559\n",
      "epoch no.3 train no.321360  loss = 2.78323 avg_loss = 3.42296\n",
      "epoch no.3 train no.321370  loss = 3.54015 avg_loss = 3.39510\n",
      "epoch no.3 train no.321380  loss = 4.11846 avg_loss = 3.35305\n",
      "epoch no.3 train no.321390  loss = 3.95812 avg_loss = 3.38594\n",
      "epoch no.3 train no.321400  loss = 4.27286 avg_loss = 3.37507\n",
      "epoch no.3 train no.321410  loss = 4.81985 avg_loss = 3.41287\n",
      "epoch no.3 train no.321420  loss = 4.21217 avg_loss = 3.41975\n",
      "epoch no.3 train no.321430  loss = 4.28061 avg_loss = 3.41357\n",
      "epoch no.3 train no.321440  loss = 4.13925 avg_loss = 3.38312\n",
      "epoch no.3 train no.321450  loss = 3.56524 avg_loss = 3.37172\n",
      "epoch no.3 train no.321460  loss = 2.21710 avg_loss = 3.44846\n",
      "epoch no.3 train no.321470  loss = 3.57226 avg_loss = 3.44987\n",
      "epoch no.3 train no.321480  loss = 3.93970 avg_loss = 3.45729\n",
      "epoch no.3 train no.321490  loss = 4.40848 avg_loss = 3.42333\n",
      "epoch no.3 train no.321500  loss = 2.83560 avg_loss = 3.43147\n",
      "epoch no.3 train no.321510  loss = 5.34889 avg_loss = 3.46841\n",
      "epoch no.3 train no.321520  loss = 4.67690 avg_loss = 3.43630\n",
      "epoch no.3 train no.321530  loss = 3.15456 avg_loss = 3.43266\n",
      "epoch no.3 train no.321540  loss = 3.43315 avg_loss = 3.39631\n",
      "epoch no.3 train no.321550  loss = 3.04329 avg_loss = 3.40671\n",
      "epoch no.3 train no.321560  loss = 2.80458 avg_loss = 3.43716\n",
      "epoch no.3 train no.321570  loss = 3.35663 avg_loss = 3.44761\n",
      "epoch no.3 train no.321580  loss = 2.83790 avg_loss = 3.46048\n",
      "epoch no.3 train no.321590  loss = 3.91333 avg_loss = 3.48690\n",
      "epoch no.3 train no.321600  loss = 2.31031 avg_loss = 3.46499\n",
      "epoch no.3 train no.321610  loss = 4.18782 avg_loss = 3.52352\n",
      "epoch no.3 train no.321620  loss = 3.27900 avg_loss = 3.49775\n",
      "epoch no.3 train no.321630  loss = 3.75725 avg_loss = 3.49603\n",
      "epoch no.3 train no.321640  loss = 3.03205 avg_loss = 3.46874\n",
      "epoch no.3 train no.321650  loss = 5.47949 avg_loss = 3.53344\n",
      "epoch no.3 train no.321660  loss = 2.51610 avg_loss = 3.48120\n",
      "epoch no.3 train no.321670  loss = 3.13365 avg_loss = 3.47117\n",
      "epoch no.3 train no.321680  loss = 5.40922 avg_loss = 3.52467\n",
      "epoch no.3 train no.321690  loss = 4.95585 avg_loss = 3.52563\n",
      "epoch no.3 train no.321700  loss = 3.35800 avg_loss = 3.51847\n",
      "epoch no.3 train no.321710  loss = 3.31774 avg_loss = 3.52325\n",
      "epoch no.3 train no.321720  loss = 2.45105 avg_loss = 3.50862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.321730  loss = 5.42956 avg_loss = 3.53028\n",
      "epoch no.3 train no.321740  loss = 3.36630 avg_loss = 3.52551\n",
      "epoch no.3 train no.321750  loss = 0.88056 avg_loss = 3.53035\n",
      "epoch no.3 train no.321760  loss = 2.30085 avg_loss = 3.51047\n",
      "epoch no.3 train no.321770  loss = 4.01944 avg_loss = 3.55918\n",
      "epoch no.3 train no.321780  loss = 1.78825 avg_loss = 3.51172\n",
      "epoch no.3 train no.321790  loss = 3.42025 avg_loss = 3.49363\n",
      "epoch no.3 train no.321800  loss = 1.93492 avg_loss = 3.44808\n",
      "epoch no.3 train no.321810  loss = 3.57598 avg_loss = 3.45189\n",
      "epoch no.3 train no.321820  loss = 3.61336 avg_loss = 3.44176\n",
      "epoch no.3 train no.321830  loss = 2.82711 avg_loss = 3.42266\n",
      "epoch no.3 train no.321840  loss = 4.31028 avg_loss = 3.47275\n",
      "epoch no.3 train no.321850  loss = 1.77031 avg_loss = 3.48155\n",
      "epoch no.3 train no.321860  loss = 4.68588 avg_loss = 3.49396\n",
      "epoch no.3 train no.321870  loss = 4.26917 avg_loss = 3.52839\n",
      "epoch no.3 train no.321880  loss = 3.42557 avg_loss = 3.50299\n",
      "epoch no.3 train no.321890  loss = 7.24505 avg_loss = 3.52775\n",
      "epoch no.3 train no.321900  loss = 3.83643 avg_loss = 3.58587\n",
      "epoch no.3 train no.321910  loss = 6.14758 avg_loss = 3.58601\n",
      "epoch no.3 train no.321920  loss = 4.31644 avg_loss = 3.59556\n",
      "epoch no.3 train no.321930  loss = 2.55756 avg_loss = 3.55804\n",
      "epoch no.3 train no.321940  loss = 3.68362 avg_loss = 3.55504\n",
      "epoch no.3 train no.321950  loss = 3.02094 avg_loss = 3.53363\n",
      "epoch no.3 train no.321960  loss = 2.67125 avg_loss = 3.50099\n",
      "epoch no.3 train no.321970  loss = 1.67310 avg_loss = 3.45497\n",
      "epoch no.3 train no.321980  loss = 3.27744 avg_loss = 3.41988\n",
      "epoch no.3 train no.321990  loss = 3.08918 avg_loss = 3.48037\n",
      "epoch no.3 train no.322000  loss = 2.37449 avg_loss = 3.47350\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할 때</s>\n",
      "epoch no.3 train no.322010  loss = 4.93533 avg_loss = 3.47639\n",
      "epoch no.3 train no.322020  loss = 5.04171 avg_loss = 3.50344\n",
      "epoch no.3 train no.322030  loss = 3.16177 avg_loss = 3.52490\n",
      "epoch no.3 train no.322040  loss = 2.75824 avg_loss = 3.53493\n",
      "epoch no.3 train no.322050  loss = 2.69379 avg_loss = 3.49119\n",
      "epoch no.3 train no.322060  loss = 5.47871 avg_loss = 3.53918\n",
      "epoch no.3 train no.322070  loss = 4.93210 avg_loss = 3.57072\n",
      "epoch no.3 train no.322080  loss = 3.59936 avg_loss = 3.59779\n",
      "epoch no.3 train no.322090  loss = 2.57867 avg_loss = 3.61506\n",
      "epoch no.3 train no.322100  loss = 3.85681 avg_loss = 3.60152\n",
      "epoch no.3 train no.322110  loss = 3.27356 avg_loss = 3.58025\n",
      "epoch no.3 train no.322120  loss = 3.24721 avg_loss = 3.58162\n",
      "epoch no.3 train no.322130  loss = 1.70874 avg_loss = 3.57362\n",
      "epoch no.3 train no.322140  loss = 2.46180 avg_loss = 3.58232\n",
      "epoch no.3 train no.322150  loss = 3.18200 avg_loss = 3.62780\n",
      "epoch no.3 train no.322160  loss = 1.81187 avg_loss = 3.59216\n",
      "epoch no.3 train no.322170  loss = 4.21491 avg_loss = 3.54442\n",
      "epoch no.3 train no.322180  loss = 3.57099 avg_loss = 3.53952\n",
      "epoch no.3 train no.322190  loss = 2.99193 avg_loss = 3.56026\n",
      "epoch no.3 train no.322200  loss = 4.01871 avg_loss = 3.52935\n",
      "epoch no.3 train no.322210  loss = 2.33631 avg_loss = 3.47579\n",
      "epoch no.3 train no.322220  loss = 5.44592 avg_loss = 3.48219\n",
      "epoch no.3 train no.322230  loss = 5.51582 avg_loss = 3.50056\n",
      "epoch no.3 train no.322240  loss = 2.18755 avg_loss = 3.51948\n",
      "epoch no.3 train no.322250  loss = 3.35281 avg_loss = 3.49794\n",
      "epoch no.3 train no.322260  loss = 3.23474 avg_loss = 3.54530\n",
      "epoch no.3 train no.322270  loss = 2.60393 avg_loss = 3.48425\n",
      "epoch no.3 train no.322280  loss = 2.80523 avg_loss = 3.53095\n",
      "epoch no.3 train no.322290  loss = 3.08730 avg_loss = 3.50639\n",
      "epoch no.3 train no.322300  loss = 2.43093 avg_loss = 3.52083\n",
      "epoch no.3 train no.322310  loss = 3.98008 avg_loss = 3.47067\n",
      "epoch no.3 train no.322320  loss = 2.82550 avg_loss = 3.48095\n",
      "epoch no.3 train no.322330  loss = 2.24873 avg_loss = 3.46362\n",
      "epoch no.3 train no.322340  loss = 1.63449 avg_loss = 3.40193\n",
      "epoch no.3 train no.322350  loss = 2.54644 avg_loss = 3.40878\n",
      "epoch no.3 train no.322360  loss = 2.22225 avg_loss = 3.39227\n",
      "epoch no.3 train no.322370  loss = 4.10072 avg_loss = 3.38944\n",
      "epoch no.3 train no.322380  loss = 2.45314 avg_loss = 3.35055\n",
      "epoch no.3 train no.322390  loss = 3.09254 avg_loss = 3.36698\n",
      "epoch no.3 train no.322400  loss = 6.88519 avg_loss = 3.43795\n",
      "epoch no.3 train no.322410  loss = 3.71981 avg_loss = 3.48166\n",
      "epoch no.3 train no.322420  loss = 3.25609 avg_loss = 3.45458\n",
      "epoch no.3 train no.322430  loss = 3.10030 avg_loss = 3.46071\n",
      "epoch no.3 train no.322440  loss = 3.98608 avg_loss = 3.45841\n",
      "epoch no.3 train no.322450  loss = 2.84128 avg_loss = 3.42968\n",
      "epoch no.3 train no.322460  loss = 4.12246 avg_loss = 3.45532\n",
      "epoch no.3 train no.322470  loss = 6.25365 avg_loss = 3.46694\n",
      "epoch no.3 train no.322480  loss = 2.25174 avg_loss = 3.47542\n",
      "epoch no.3 train no.322490  loss = 3.34356 avg_loss = 3.50876\n",
      "epoch no.3 train no.322500  loss = 2.31689 avg_loss = 3.50927\n",
      "epoch no.3 train no.322510  loss = 3.84937 avg_loss = 3.46744\n",
      "epoch no.3 train no.322520  loss = 2.51703 avg_loss = 3.47583\n",
      "epoch no.3 train no.322530  loss = 3.59512 avg_loss = 3.51609\n",
      "epoch no.3 train no.322540  loss = 3.82703 avg_loss = 3.51242\n",
      "epoch no.3 train no.322550  loss = 3.52747 avg_loss = 3.50720\n",
      "epoch no.3 train no.322560  loss = 2.97593 avg_loss = 3.46008\n",
      "epoch no.3 train no.322570  loss = 4.37212 avg_loss = 3.48238\n",
      "epoch no.3 train no.322580  loss = 2.52264 avg_loss = 3.50613\n",
      "epoch no.3 train no.322590  loss = 4.40731 avg_loss = 3.49469\n",
      "epoch no.3 train no.322600  loss = 3.94034 avg_loss = 3.58389\n",
      "epoch no.3 train no.322610  loss = 2.64899 avg_loss = 3.64518\n",
      "epoch no.3 train no.322620  loss = 3.33666 avg_loss = 3.65076\n",
      "epoch no.3 train no.322630  loss = 5.02977 avg_loss = 3.68388\n",
      "epoch no.3 train no.322640  loss = 3.66714 avg_loss = 3.62265\n",
      "epoch no.3 train no.322650  loss = 2.64635 avg_loss = 3.55919\n",
      "epoch no.3 train no.322660  loss = 4.44511 avg_loss = 3.60837\n",
      "epoch no.3 train no.322670  loss = 3.22596 avg_loss = 3.60278\n",
      "epoch no.3 train no.322680  loss = 6.24881 avg_loss = 3.65395\n",
      "epoch no.3 train no.322690  loss = 3.16960 avg_loss = 3.60888\n",
      "epoch no.3 train no.322700  loss = 2.96615 avg_loss = 3.63105\n",
      "epoch no.3 train no.322710  loss = 4.06050 avg_loss = 3.60208\n",
      "epoch no.3 train no.322720  loss = 2.57439 avg_loss = 3.52946\n",
      "epoch no.3 train no.322730  loss = 3.92640 avg_loss = 3.56025\n",
      "epoch no.3 train no.322740  loss = 2.35239 avg_loss = 3.53840\n",
      "epoch no.3 train no.322750  loss = 4.39944 avg_loss = 3.56427\n",
      "epoch no.3 train no.322760  loss = 4.36645 avg_loss = 3.57509\n",
      "epoch no.3 train no.322770  loss = 3.14932 avg_loss = 3.53529\n",
      "epoch no.3 train no.322780  loss = 5.78342 avg_loss = 3.51984\n",
      "epoch no.3 train no.322790  loss = 5.30960 avg_loss = 3.53569\n",
      "epoch no.3 train no.322800  loss = 3.10641 avg_loss = 3.50817\n",
      "epoch no.3 train no.322810  loss = 2.74511 avg_loss = 3.51852\n",
      "epoch no.3 train no.322820  loss = 4.38570 avg_loss = 3.50392\n",
      "epoch no.3 train no.322830  loss = 3.00304 avg_loss = 3.49734\n",
      "epoch no.3 train no.322840  loss = 5.86403 avg_loss = 3.51553\n",
      "epoch no.3 train no.322850  loss = 3.64289 avg_loss = 3.51516\n",
      "epoch no.3 train no.322860  loss = 4.08834 avg_loss = 3.49901\n",
      "epoch no.3 train no.322870  loss = 4.18683 avg_loss = 3.47797\n",
      "epoch no.3 train no.322880  loss = 2.07623 avg_loss = 3.47109\n",
      "epoch no.3 train no.322890  loss = 3.41885 avg_loss = 3.45859\n",
      "epoch no.3 train no.322900  loss = 2.91113 avg_loss = 3.43344\n",
      "epoch no.3 train no.322910  loss = 4.98768 avg_loss = 3.43727\n",
      "epoch no.3 train no.322920  loss = 3.85319 avg_loss = 3.41550\n",
      "epoch no.3 train no.322930  loss = 3.51186 avg_loss = 3.40041\n",
      "epoch no.3 train no.322940  loss = 3.66162 avg_loss = 3.42904\n",
      "epoch no.3 train no.322950  loss = 5.78073 avg_loss = 3.42441\n",
      "epoch no.3 train no.322960  loss = 4.58161 avg_loss = 3.41916\n",
      "epoch no.3 train no.322970  loss = 3.72740 avg_loss = 3.45764\n",
      "epoch no.3 train no.322980  loss = 4.10193 avg_loss = 3.44268\n",
      "epoch no.3 train no.322990  loss = 3.70841 avg_loss = 3.45808\n",
      "epoch no.3 train no.323000  loss = 4.26087 avg_loss = 3.47859\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁딱', '인', '▁노래', '▁노래', '송', '</s>']\n",
      "기분전환에 딱 좋은 신나는 팝송</s>\n",
      "epoch no.3 train no.323010  loss = 4.65972 avg_loss = 3.48366\n",
      "epoch no.3 train no.323020  loss = 3.04399 avg_loss = 3.45286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.323030  loss = 3.99878 avg_loss = 3.44206\n",
      "epoch no.3 train no.323040  loss = 3.63520 avg_loss = 3.54257\n",
      "epoch no.3 train no.323050  loss = 3.05043 avg_loss = 3.53694\n",
      "epoch no.3 train no.323060  loss = 3.46944 avg_loss = 3.49053\n",
      "epoch no.3 train no.323070  loss = 3.84470 avg_loss = 3.49439\n",
      "epoch no.3 train no.323080  loss = 2.83037 avg_loss = 3.51165\n",
      "epoch no.3 train no.323090  loss = 4.57253 avg_loss = 3.48537\n",
      "epoch no.3 train no.323100  loss = 2.05126 avg_loss = 3.45743\n",
      "epoch no.3 train no.323110  loss = 3.00095 avg_loss = 3.43594\n",
      "epoch no.3 train no.323120  loss = 4.09926 avg_loss = 3.39737\n",
      "epoch no.3 train no.323130  loss = 3.80036 avg_loss = 3.39738\n",
      "epoch no.3 train no.323140  loss = 5.47465 avg_loss = 3.39964\n",
      "epoch no.3 train no.323150  loss = 2.75067 avg_loss = 3.40845\n",
      "epoch no.3 train no.323160  loss = 3.08678 avg_loss = 3.39569\n",
      "epoch no.3 train no.323170  loss = 2.95975 avg_loss = 3.37756\n",
      "epoch no.3 train no.323180  loss = 3.04162 avg_loss = 3.39995\n",
      "epoch no.3 train no.323190  loss = 4.55982 avg_loss = 3.42226\n",
      "epoch no.3 train no.323200  loss = 3.47923 avg_loss = 3.41782\n",
      "epoch no.3 train no.323210  loss = 2.67531 avg_loss = 3.46332\n",
      "epoch no.3 train no.323220  loss = 4.40265 avg_loss = 3.52188\n",
      "epoch no.3 train no.323230  loss = 6.84098 avg_loss = 3.53555\n",
      "epoch no.3 train no.323240  loss = 3.88879 avg_loss = 3.56788\n",
      "epoch no.3 train no.323250  loss = 3.42778 avg_loss = 3.55774\n",
      "epoch no.3 train no.323260  loss = 3.21532 avg_loss = 3.56732\n",
      "epoch no.3 train no.323270  loss = 2.57035 avg_loss = 3.54186\n",
      "epoch no.3 train no.323280  loss = 3.18601 avg_loss = 3.52617\n",
      "epoch no.3 train no.323290  loss = 3.95829 avg_loss = 3.51650\n",
      "epoch no.3 train no.323300  loss = 3.88851 avg_loss = 3.50838\n",
      "epoch no.3 train no.323310  loss = 3.02724 avg_loss = 3.55436\n",
      "epoch no.3 train no.323320  loss = 3.38038 avg_loss = 3.52996\n",
      "epoch no.3 train no.323330  loss = 2.74748 avg_loss = 3.49691\n",
      "epoch no.3 train no.323340  loss = 4.67528 avg_loss = 3.48953\n",
      "epoch no.3 train no.323350  loss = 3.70094 avg_loss = 3.48005\n",
      "epoch no.3 train no.323360  loss = 2.85444 avg_loss = 3.45379\n",
      "epoch no.3 train no.323370  loss = 3.59551 avg_loss = 3.43618\n",
      "epoch no.3 train no.323380  loss = 3.45747 avg_loss = 3.44901\n",
      "epoch no.3 train no.323390  loss = 2.28808 avg_loss = 3.45385\n",
      "epoch no.3 train no.323400  loss = 4.49129 avg_loss = 3.46230\n",
      "epoch no.3 train no.323410  loss = 2.73657 avg_loss = 3.44173\n",
      "epoch no.3 train no.323420  loss = 3.21857 avg_loss = 3.41430\n",
      "epoch no.3 train no.323430  loss = 3.03207 avg_loss = 3.41176\n",
      "epoch no.3 train no.323440  loss = 2.96022 avg_loss = 3.38706\n",
      "epoch no.3 train no.323450  loss = 5.07989 avg_loss = 3.40843\n",
      "epoch no.3 train no.323460  loss = 4.56300 avg_loss = 3.43167\n",
      "epoch no.3 train no.323470  loss = 3.50980 avg_loss = 3.44262\n",
      "epoch no.3 train no.323480  loss = 3.77791 avg_loss = 3.45215\n",
      "epoch no.3 train no.323490  loss = 3.24736 avg_loss = 3.44726\n",
      "epoch no.3 train no.323500  loss = 1.98710 avg_loss = 3.42092\n",
      "epoch no.3 train no.323510  loss = 3.38487 avg_loss = 3.48410\n",
      "epoch no.3 train no.323520  loss = 3.11342 avg_loss = 3.46988\n",
      "epoch no.3 train no.323530  loss = 3.13213 avg_loss = 3.46589\n",
      "epoch no.3 train no.323540  loss = 2.77759 avg_loss = 3.49283\n",
      "epoch no.3 train no.323550  loss = 2.84480 avg_loss = 3.47384\n",
      "epoch no.3 train no.323560  loss = 4.84033 avg_loss = 3.51219\n",
      "epoch no.3 train no.323570  loss = 5.62544 avg_loss = 3.55714\n",
      "epoch no.3 train no.323580  loss = 4.04109 avg_loss = 3.56695\n",
      "epoch no.3 train no.323590  loss = 3.37320 avg_loss = 3.56155\n",
      "epoch no.3 train no.323600  loss = 3.19643 avg_loss = 3.55472\n",
      "epoch no.3 train no.323610  loss = 2.90957 avg_loss = 3.56218\n",
      "epoch no.3 train no.323620  loss = 3.93327 avg_loss = 3.53446\n",
      "epoch no.3 train no.323630  loss = 3.19242 avg_loss = 3.53330\n",
      "epoch no.3 train no.323640  loss = 3.67939 avg_loss = 3.49105\n",
      "epoch no.3 train no.323650  loss = 2.94408 avg_loss = 3.50086\n",
      "epoch no.3 train no.323660  loss = 2.90628 avg_loss = 3.52921\n",
      "epoch no.3 train no.323670  loss = 4.59461 avg_loss = 3.53118\n",
      "epoch no.3 train no.323680  loss = 4.45165 avg_loss = 3.55082\n",
      "epoch no.3 train no.323690  loss = 3.74492 avg_loss = 3.55898\n",
      "epoch no.3 train no.323700  loss = 3.43457 avg_loss = 3.55089\n",
      "epoch no.3 train no.323710  loss = 3.51439 avg_loss = 3.53847\n",
      "epoch no.3 train no.323720  loss = 2.36934 avg_loss = 3.49211\n",
      "epoch no.3 train no.323730  loss = 2.24866 avg_loss = 3.47492\n",
      "epoch no.3 train no.323740  loss = 4.59586 avg_loss = 3.48007\n",
      "epoch no.3 train no.323750  loss = 4.16501 avg_loss = 3.47451\n",
      "epoch no.3 train no.323760  loss = 2.53930 avg_loss = 3.48240\n",
      "epoch no.3 train no.323770  loss = 4.07625 avg_loss = 3.45990\n",
      "epoch no.3 train no.323780  loss = 1.92810 avg_loss = 3.44663\n",
      "epoch no.3 train no.323790  loss = 2.59984 avg_loss = 3.45943\n",
      "epoch no.3 train no.323800  loss = 2.58770 avg_loss = 3.48651\n",
      "epoch no.3 train no.323810  loss = 2.57748 avg_loss = 3.46324\n",
      "epoch no.3 train no.323820  loss = 4.77866 avg_loss = 3.44014\n",
      "epoch no.3 train no.323830  loss = 3.77358 avg_loss = 3.46504\n",
      "epoch no.3 train no.323840  loss = 3.91627 avg_loss = 3.52453\n",
      "epoch no.3 train no.323850  loss = 3.36326 avg_loss = 3.50408\n",
      "epoch no.3 train no.323860  loss = 4.58084 avg_loss = 3.49674\n",
      "epoch no.3 train no.323870  loss = 2.99203 avg_loss = 3.51728\n",
      "epoch no.3 train no.323880  loss = 2.61712 avg_loss = 3.51522\n",
      "epoch no.3 train no.323890  loss = 3.12818 avg_loss = 3.53166\n",
      "epoch no.3 train no.323900  loss = 3.20339 avg_loss = 3.54411\n",
      "epoch no.3 train no.323910  loss = 2.55829 avg_loss = 3.50734\n",
      "epoch no.3 train no.323920  loss = 2.81911 avg_loss = 3.45880\n",
      "epoch no.3 train no.323930  loss = 3.68677 avg_loss = 3.44501\n",
      "epoch no.3 train no.323940  loss = 2.17962 avg_loss = 3.40234\n",
      "epoch no.3 train no.323950  loss = 3.78561 avg_loss = 3.43035\n",
      "epoch no.3 train no.323960  loss = 3.76401 avg_loss = 3.46327\n",
      "epoch no.3 train no.323970  loss = 2.75551 avg_loss = 3.43037\n",
      "epoch no.3 train no.323980  loss = 1.94041 avg_loss = 3.40966\n",
      "epoch no.3 train no.323990  loss = 2.79738 avg_loss = 3.39985\n",
      "epoch no.3 train no.324000  loss = 4.49566 avg_loss = 3.42121\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.3 train no.324010  loss = 4.57790 avg_loss = 3.39717\n",
      "epoch no.3 train no.324020  loss = 3.55776 avg_loss = 3.42271\n",
      "epoch no.3 train no.324030  loss = 4.61233 avg_loss = 3.44743\n",
      "epoch no.3 train no.324040  loss = 3.11625 avg_loss = 3.43974\n",
      "epoch no.3 train no.324050  loss = 2.87695 avg_loss = 3.40251\n",
      "epoch no.3 train no.324060  loss = 3.78267 avg_loss = 3.49170\n",
      "epoch no.3 train no.324070  loss = 2.92639 avg_loss = 3.43650\n",
      "epoch no.3 train no.324080  loss = 2.51177 avg_loss = 3.42506\n",
      "epoch no.3 train no.324090  loss = 2.49629 avg_loss = 3.45122\n",
      "epoch no.3 train no.324100  loss = 2.48195 avg_loss = 3.45946\n",
      "epoch no.3 train no.324110  loss = 3.20468 avg_loss = 3.47601\n",
      "epoch no.3 train no.324120  loss = 2.75240 avg_loss = 3.47963\n",
      "epoch no.3 train no.324130  loss = 4.71743 avg_loss = 3.44021\n",
      "epoch no.3 train no.324140  loss = 4.55076 avg_loss = 3.43285\n",
      "epoch no.3 train no.324150  loss = 3.45040 avg_loss = 3.40775\n",
      "epoch no.3 train no.324160  loss = 3.76607 avg_loss = 3.37485\n",
      "epoch no.3 train no.324170  loss = 3.09584 avg_loss = 3.39520\n",
      "epoch no.3 train no.324180  loss = 2.22716 avg_loss = 3.38092\n",
      "epoch no.3 train no.324190  loss = 3.74356 avg_loss = 3.42665\n",
      "epoch no.3 train no.324200  loss = 2.61050 avg_loss = 3.44508\n",
      "epoch no.3 train no.324210  loss = 3.60572 avg_loss = 3.41533\n",
      "epoch no.3 train no.324220  loss = 2.60018 avg_loss = 3.41718\n",
      "epoch no.3 train no.324230  loss = 3.48279 avg_loss = 3.43370\n",
      "epoch no.3 train no.324240  loss = 4.15711 avg_loss = 3.42892\n",
      "epoch no.3 train no.324250  loss = 2.28121 avg_loss = 3.47252\n",
      "epoch no.3 train no.324260  loss = 2.82447 avg_loss = 3.46604\n",
      "epoch no.3 train no.324270  loss = 4.70853 avg_loss = 3.47241\n",
      "epoch no.3 train no.324280  loss = 3.19564 avg_loss = 3.44441\n",
      "epoch no.3 train no.324290  loss = 3.73904 avg_loss = 3.46760\n",
      "epoch no.3 train no.324300  loss = 3.55803 avg_loss = 3.48981\n",
      "epoch no.3 train no.324310  loss = 3.21552 avg_loss = 3.45798\n",
      "epoch no.3 train no.324320  loss = 2.92084 avg_loss = 3.41665\n",
      "epoch no.3 train no.324330  loss = 3.67648 avg_loss = 3.43348\n",
      "epoch no.3 train no.324340  loss = 3.87410 avg_loss = 3.39263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.324350  loss = 3.89586 avg_loss = 3.40662\n",
      "epoch no.3 train no.324360  loss = 2.33267 avg_loss = 3.44639\n",
      "epoch no.3 train no.324370  loss = 3.25155 avg_loss = 3.45170\n",
      "epoch no.3 train no.324380  loss = 4.05434 avg_loss = 3.46643\n",
      "epoch no.3 train no.324390  loss = 3.59896 avg_loss = 3.43148\n",
      "epoch no.3 train no.324400  loss = 2.98683 avg_loss = 3.43422\n",
      "epoch no.3 train no.324410  loss = 2.85942 avg_loss = 3.39414\n",
      "epoch no.3 train no.324420  loss = 4.59084 avg_loss = 3.39465\n",
      "epoch no.3 train no.324430  loss = 2.43010 avg_loss = 3.41195\n",
      "epoch no.3 train no.324440  loss = 6.98525 avg_loss = 3.50485\n",
      "epoch no.3 train no.324450  loss = 3.29931 avg_loss = 3.45428\n",
      "epoch no.3 train no.324460  loss = 4.48285 avg_loss = 3.46773\n",
      "epoch no.3 train no.324470  loss = 5.06865 avg_loss = 3.51970\n",
      "epoch no.3 train no.324480  loss = 4.04995 avg_loss = 3.49676\n",
      "epoch no.3 train no.324490  loss = 1.91908 avg_loss = 3.49123\n",
      "epoch no.3 train no.324500  loss = 3.19570 avg_loss = 3.51040\n",
      "epoch no.3 train no.324510  loss = 3.57432 avg_loss = 3.52250\n",
      "epoch no.3 train no.324520  loss = 3.41678 avg_loss = 3.48291\n",
      "epoch no.3 train no.324530  loss = 2.56961 avg_loss = 3.47719\n",
      "epoch no.3 train no.324540  loss = 1.38345 avg_loss = 3.46591\n",
      "epoch no.3 train no.324550  loss = 3.59529 avg_loss = 3.44803\n",
      "epoch no.3 train no.324560  loss = 2.71635 avg_loss = 3.47822\n",
      "epoch no.3 train no.324570  loss = 2.56567 avg_loss = 3.47421\n",
      "epoch no.3 train no.324580  loss = 2.84308 avg_loss = 3.45357\n",
      "epoch no.3 train no.324590  loss = 3.18454 avg_loss = 3.45187\n",
      "epoch no.3 train no.324600  loss = 3.50661 avg_loss = 3.47174\n",
      "epoch no.3 train no.324610  loss = 4.99962 avg_loss = 3.52105\n",
      "epoch no.3 train no.324620  loss = 2.91418 avg_loss = 3.52740\n",
      "epoch no.3 train no.324630  loss = 2.80455 avg_loss = 3.51006\n",
      "epoch no.3 train no.324640  loss = 5.54257 avg_loss = 3.50396\n",
      "epoch no.3 train no.324650  loss = 3.12205 avg_loss = 3.49020\n",
      "epoch no.3 train no.324660  loss = 3.69891 avg_loss = 3.50765\n",
      "epoch no.3 train no.324670  loss = 3.93411 avg_loss = 3.54831\n",
      "epoch no.3 train no.324680  loss = 4.33437 avg_loss = 3.53940\n",
      "epoch no.3 train no.324690  loss = 3.16968 avg_loss = 3.55639\n",
      "epoch no.3 train no.324700  loss = 5.26927 avg_loss = 3.51558\n",
      "epoch no.3 train no.324710  loss = 3.01830 avg_loss = 3.53553\n",
      "epoch no.3 train no.324720  loss = 3.38532 avg_loss = 3.47940\n",
      "epoch no.3 train no.324730  loss = 6.02613 avg_loss = 3.46122\n",
      "epoch no.3 train no.324740  loss = 1.60537 avg_loss = 3.37978\n",
      "epoch no.3 train no.324750  loss = 3.60859 avg_loss = 3.42561\n",
      "epoch no.3 train no.324760  loss = 3.57995 avg_loss = 3.47531\n",
      "epoch no.3 train no.324770  loss = 4.23823 avg_loss = 3.47041\n",
      "epoch no.3 train no.324780  loss = 6.31243 avg_loss = 3.50009\n",
      "epoch no.3 train no.324790  loss = 2.55239 avg_loss = 3.50217\n",
      "epoch no.3 train no.324800  loss = 2.97199 avg_loss = 3.51811\n",
      "epoch no.3 train no.324810  loss = 4.09874 avg_loss = 3.56419\n",
      "epoch no.3 train no.324820  loss = 2.38365 avg_loss = 3.54464\n",
      "epoch no.3 train no.324830  loss = 2.66409 avg_loss = 3.55009\n",
      "epoch no.3 train no.324840  loss = 3.64208 avg_loss = 3.55964\n",
      "epoch no.3 train no.324850  loss = 4.61771 avg_loss = 3.51953\n",
      "epoch no.3 train no.324860  loss = 3.08034 avg_loss = 3.57982\n",
      "epoch no.3 train no.324870  loss = 3.07512 avg_loss = 3.55403\n",
      "epoch no.3 train no.324880  loss = 3.79851 avg_loss = 3.58355\n",
      "epoch no.3 train no.324890  loss = 4.78008 avg_loss = 3.51939\n",
      "epoch no.3 train no.324900  loss = 5.15236 avg_loss = 3.53142\n",
      "epoch no.3 train no.324910  loss = 4.01345 avg_loss = 3.52973\n",
      "epoch no.3 train no.324920  loss = 2.89602 avg_loss = 3.50127\n",
      "epoch no.3 train no.324930  loss = 1.94145 avg_loss = 3.48675\n",
      "epoch no.3 train no.324940  loss = 3.71456 avg_loss = 3.50104\n",
      "epoch no.3 train no.324950  loss = 3.14198 avg_loss = 3.47608\n",
      "epoch no.3 train no.324960  loss = 4.86414 avg_loss = 3.49957\n",
      "epoch no.3 train no.324970  loss = 2.60745 avg_loss = 3.47900\n",
      "epoch no.3 train no.324980  loss = 4.93523 avg_loss = 3.49434\n",
      "epoch no.3 train no.324990  loss = 4.36931 avg_loss = 3.50315\n",
      "epoch no.3 train no.325000  loss = 4.30234 avg_loss = 3.52394\n",
      "4\n",
      "to_tokens: ['▁비', '▁좋은', '이', '▁위한', '▁노래', '▁팝', '</s>']\n",
      "기분전환을 위한 신나는 팝</s>\n",
      "epoch no.3 train no.325010  loss = 3.48894 avg_loss = 3.54207\n",
      "epoch no.3 train no.325020  loss = 3.21753 avg_loss = 3.51607\n",
      "epoch no.3 train no.325030  loss = 3.87824 avg_loss = 3.47813\n",
      "epoch no.3 train no.325040  loss = 3.19142 avg_loss = 3.50201\n",
      "epoch no.3 train no.325050  loss = 2.93257 avg_loss = 3.46864\n",
      "epoch no.3 train no.325060  loss = 3.42296 avg_loss = 3.45623\n",
      "epoch no.3 train no.325070  loss = 4.29833 avg_loss = 3.47170\n",
      "epoch no.3 train no.325080  loss = 1.94415 avg_loss = 3.49544\n",
      "epoch no.3 train no.325090  loss = 3.71063 avg_loss = 3.47666\n",
      "epoch no.3 train no.325100  loss = 2.25997 avg_loss = 3.46488\n",
      "epoch no.3 train no.325110  loss = 3.08932 avg_loss = 3.45993\n",
      "epoch no.3 train no.325120  loss = 4.13497 avg_loss = 3.50638\n",
      "epoch no.3 train no.325130  loss = 1.99336 avg_loss = 3.46097\n",
      "epoch no.3 train no.325140  loss = 3.51674 avg_loss = 3.46491\n",
      "epoch no.3 train no.325150  loss = 3.61502 avg_loss = 3.49141\n",
      "epoch no.3 train no.325160  loss = 2.91706 avg_loss = 3.51162\n",
      "epoch no.3 train no.325170  loss = 2.72629 avg_loss = 3.47233\n",
      "epoch no.3 train no.325180  loss = 1.89611 avg_loss = 3.42952\n",
      "epoch no.3 train no.325190  loss = 5.47447 avg_loss = 3.43408\n",
      "epoch no.3 train no.325200  loss = 2.96272 avg_loss = 3.44616\n",
      "epoch no.3 train no.325210  loss = 4.30444 avg_loss = 3.46429\n",
      "epoch no.3 train no.325220  loss = 5.43548 avg_loss = 3.49218\n",
      "epoch no.3 train no.325230  loss = 4.74717 avg_loss = 3.53493\n",
      "epoch no.3 train no.325240  loss = 3.00441 avg_loss = 3.52950\n",
      "epoch no.3 train no.325250  loss = 3.71019 avg_loss = 3.55949\n",
      "epoch no.3 train no.325260  loss = 2.83335 avg_loss = 3.54316\n",
      "epoch no.3 train no.325270  loss = 5.61046 avg_loss = 3.54497\n",
      "epoch no.3 train no.325280  loss = 2.14914 avg_loss = 3.52780\n",
      "epoch no.3 train no.325290  loss = 2.50566 avg_loss = 3.55059\n",
      "epoch no.3 train no.325300  loss = 3.83958 avg_loss = 3.54193\n",
      "epoch no.3 train no.325310  loss = 4.83554 avg_loss = 3.56580\n",
      "epoch no.3 train no.325320  loss = 3.93289 avg_loss = 3.56280\n",
      "epoch no.3 train no.325330  loss = 3.24476 avg_loss = 3.56786\n",
      "epoch no.3 train no.325340  loss = 3.68092 avg_loss = 3.56137\n",
      "epoch no.3 train no.325350  loss = 3.30985 avg_loss = 3.55293\n",
      "epoch no.3 train no.325360  loss = 2.59376 avg_loss = 3.53250\n",
      "epoch no.3 train no.325370  loss = 5.56111 avg_loss = 3.57974\n",
      "epoch no.3 train no.325380  loss = 4.01398 avg_loss = 3.63876\n",
      "epoch no.3 train no.325390  loss = 4.25687 avg_loss = 3.63061\n",
      "epoch no.3 train no.325400  loss = 4.03252 avg_loss = 3.58772\n",
      "epoch no.3 train no.325410  loss = 2.54377 avg_loss = 3.61717\n",
      "epoch no.3 train no.325420  loss = 3.38599 avg_loss = 3.63913\n",
      "epoch no.3 train no.325430  loss = 2.58168 avg_loss = 3.60273\n",
      "epoch no.3 train no.325440  loss = 3.10638 avg_loss = 3.57188\n",
      "epoch no.3 train no.325450  loss = 2.55351 avg_loss = 3.56257\n",
      "epoch no.3 train no.325460  loss = 3.21278 avg_loss = 3.53658\n",
      "epoch no.3 train no.325470  loss = 3.37272 avg_loss = 3.58909\n",
      "epoch no.3 train no.325480  loss = 3.63281 avg_loss = 3.56184\n",
      "epoch no.3 train no.325490  loss = 2.91178 avg_loss = 3.57564\n",
      "epoch no.3 train no.325500  loss = 2.84805 avg_loss = 3.55768\n",
      "epoch no.3 train no.325510  loss = 2.67814 avg_loss = 3.56465\n",
      "epoch no.3 train no.325520  loss = 3.64532 avg_loss = 3.54394\n",
      "epoch no.3 train no.325530  loss = 4.42018 avg_loss = 3.56313\n",
      "epoch no.3 train no.325540  loss = 2.78960 avg_loss = 3.53984\n",
      "epoch no.3 train no.325550  loss = 4.95010 avg_loss = 3.53689\n",
      "epoch no.3 train no.325560  loss = 3.81990 avg_loss = 3.51790\n",
      "epoch no.3 train no.325570  loss = 2.49267 avg_loss = 3.50139\n",
      "epoch no.3 train no.325580  loss = 2.23367 avg_loss = 3.45838\n",
      "epoch no.3 train no.325590  loss = 7.46122 avg_loss = 3.44244\n",
      "epoch no.3 train no.325600  loss = 3.99795 avg_loss = 3.45866\n",
      "epoch no.3 train no.325610  loss = 2.53135 avg_loss = 3.43401\n",
      "epoch no.3 train no.325620  loss = 4.50549 avg_loss = 3.52068\n",
      "epoch no.3 train no.325630  loss = 3.50576 avg_loss = 3.50536\n",
      "epoch no.3 train no.325640  loss = 3.12230 avg_loss = 3.52386\n",
      "epoch no.3 train no.325650  loss = 4.19479 avg_loss = 3.54773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.325660  loss = 1.99365 avg_loss = 3.57420\n",
      "epoch no.3 train no.325670  loss = 3.54751 avg_loss = 3.55767\n",
      "epoch no.3 train no.325680  loss = 3.45954 avg_loss = 3.56636\n",
      "epoch no.3 train no.325690  loss = 4.02524 avg_loss = 3.62587\n",
      "epoch no.3 train no.325700  loss = 3.02350 avg_loss = 3.61448\n",
      "epoch no.3 train no.325710  loss = 3.01425 avg_loss = 3.59923\n",
      "epoch no.3 train no.325720  loss = 3.47241 avg_loss = 3.56835\n",
      "epoch no.3 train no.325730  loss = 2.94369 avg_loss = 3.54727\n",
      "epoch no.3 train no.325740  loss = 3.79742 avg_loss = 3.57866\n",
      "epoch no.3 train no.325750  loss = 4.48045 avg_loss = 3.55557\n",
      "epoch no.3 train no.325760  loss = 6.16179 avg_loss = 3.54905\n",
      "epoch no.3 train no.325770  loss = 3.27701 avg_loss = 3.51744\n",
      "epoch no.3 train no.325780  loss = 3.26063 avg_loss = 3.57124\n",
      "epoch no.3 train no.325790  loss = 3.65749 avg_loss = 3.60023\n",
      "epoch no.3 train no.325800  loss = 6.46261 avg_loss = 3.59561\n",
      "epoch no.3 train no.325810  loss = 5.32128 avg_loss = 3.58378\n",
      "epoch no.3 train no.325820  loss = 4.11848 avg_loss = 3.56732\n",
      "epoch no.3 train no.325830  loss = 3.47674 avg_loss = 3.58421\n",
      "epoch no.3 train no.325840  loss = 3.65335 avg_loss = 3.59394\n",
      "epoch no.3 train no.325850  loss = 2.66301 avg_loss = 3.54790\n",
      "epoch no.3 train no.325860  loss = 4.35451 avg_loss = 3.52658\n",
      "epoch no.3 train no.325870  loss = 2.99404 avg_loss = 3.48527\n",
      "epoch no.3 train no.325880  loss = 2.99993 avg_loss = 3.51183\n",
      "epoch no.3 train no.325890  loss = 3.98054 avg_loss = 3.51342\n",
      "epoch no.3 train no.325900  loss = 1.98253 avg_loss = 3.47772\n",
      "epoch no.3 train no.325910  loss = 3.10677 avg_loss = 3.48017\n",
      "epoch no.3 train no.325920  loss = 2.28632 avg_loss = 3.47903\n",
      "epoch no.3 train no.325930  loss = 5.17353 avg_loss = 3.51027\n",
      "epoch no.3 train no.325940  loss = 4.48897 avg_loss = 3.53002\n",
      "epoch no.3 train no.325950  loss = 5.60252 avg_loss = 3.54399\n",
      "epoch no.3 train no.325960  loss = 3.35396 avg_loss = 3.55057\n",
      "epoch no.3 train no.325970  loss = 2.51103 avg_loss = 3.53988\n",
      "epoch no.3 train no.325980  loss = 2.79559 avg_loss = 3.59061\n",
      "epoch no.3 train no.325990  loss = 2.02797 avg_loss = 3.56649\n",
      "epoch no.3 train no.326000  loss = 3.92664 avg_loss = 3.56177\n",
      "4\n",
      "to_tokens: ['▁라디오', '▁좋은', '을', '▁위한', '▁음악', '▁팝', '</s>']\n",
      "기분전환을 위한 신나는 음악</s>\n",
      "epoch no.3 train no.326010  loss = 2.98271 avg_loss = 3.50946\n",
      "epoch no.3 train no.326020  loss = 4.41222 avg_loss = 3.49400\n",
      "epoch no.3 train no.326030  loss = 3.23065 avg_loss = 3.45114\n",
      "epoch no.3 train no.326040  loss = 2.31409 avg_loss = 3.45884\n",
      "epoch no.3 train no.326050  loss = 2.50533 avg_loss = 3.48162\n",
      "epoch no.3 train no.326060  loss = 3.16922 avg_loss = 3.49435\n",
      "epoch no.3 train no.326070  loss = 3.74312 avg_loss = 3.47133\n",
      "epoch no.3 train no.326080  loss = 2.86866 avg_loss = 3.47347\n",
      "epoch no.3 train no.326090  loss = 3.23992 avg_loss = 3.47877\n",
      "epoch no.3 train no.326100  loss = 2.91351 avg_loss = 3.42871\n",
      "epoch no.3 train no.326110  loss = 3.01266 avg_loss = 3.43349\n",
      "epoch no.3 train no.326120  loss = 3.72021 avg_loss = 3.47293\n",
      "epoch no.3 train no.326130  loss = 3.94896 avg_loss = 3.47588\n",
      "epoch no.3 train no.326140  loss = 3.03265 avg_loss = 3.44694\n",
      "epoch no.3 train no.326150  loss = 4.30991 avg_loss = 3.48161\n",
      "epoch no.3 train no.326160  loss = 3.15226 avg_loss = 3.51427\n",
      "epoch no.3 train no.326170  loss = 3.42843 avg_loss = 3.47757\n",
      "epoch no.3 train no.326180  loss = 1.95580 avg_loss = 3.44780\n",
      "epoch no.3 train no.326190  loss = 3.68143 avg_loss = 3.48410\n",
      "epoch no.3 train no.326200  loss = 3.92611 avg_loss = 3.47380\n",
      "epoch no.3 train no.326210  loss = 3.33623 avg_loss = 3.46472\n",
      "epoch no.3 train no.326220  loss = 3.58271 avg_loss = 3.46575\n",
      "epoch no.3 train no.326230  loss = 3.60865 avg_loss = 3.45718\n",
      "epoch no.3 train no.326240  loss = 3.25764 avg_loss = 3.43029\n",
      "epoch no.3 train no.326250  loss = 3.00850 avg_loss = 3.45396\n",
      "epoch no.3 train no.326260  loss = 3.22505 avg_loss = 3.44364\n",
      "epoch no.3 train no.326270  loss = 1.03025 avg_loss = 3.44170\n",
      "epoch no.3 train no.326280  loss = 3.70240 avg_loss = 3.43554\n",
      "epoch no.3 train no.326290  loss = 2.83880 avg_loss = 3.45955\n",
      "epoch no.3 train no.326300  loss = 3.85410 avg_loss = 3.48484\n",
      "epoch no.3 train no.326310  loss = 2.46137 avg_loss = 3.51053\n",
      "epoch no.3 train no.326320  loss = 4.95163 avg_loss = 3.55867\n",
      "epoch no.3 train no.326330  loss = 3.93163 avg_loss = 3.54220\n",
      "epoch no.3 train no.326340  loss = 5.53311 avg_loss = 3.60464\n",
      "epoch no.3 train no.326350  loss = 1.93771 avg_loss = 3.59533\n",
      "epoch no.3 train no.326360  loss = 3.28350 avg_loss = 3.58631\n",
      "epoch no.3 train no.326370  loss = 3.89365 avg_loss = 3.54141\n",
      "epoch no.3 train no.326380  loss = 2.72769 avg_loss = 3.51561\n",
      "epoch no.3 train no.326390  loss = 4.39803 avg_loss = 3.53612\n",
      "epoch no.3 train no.326400  loss = 3.32044 avg_loss = 3.53096\n",
      "epoch no.3 train no.326410  loss = 4.28664 avg_loss = 3.54340\n",
      "epoch no.3 train no.326420  loss = 4.64740 avg_loss = 3.51281\n",
      "epoch no.3 train no.326430  loss = 2.97349 avg_loss = 3.52644\n",
      "epoch no.3 train no.326440  loss = 5.15549 avg_loss = 3.49887\n",
      "epoch no.3 train no.326450  loss = 4.78459 avg_loss = 3.49369\n",
      "epoch no.3 train no.326460  loss = 3.05746 avg_loss = 3.54613\n",
      "epoch no.3 train no.326470  loss = 3.25466 avg_loss = 3.57138\n",
      "epoch no.3 train no.326480  loss = 2.97897 avg_loss = 3.57724\n",
      "epoch no.3 train no.326490  loss = 2.80534 avg_loss = 3.52636\n",
      "epoch no.3 train no.326500  loss = 5.01891 avg_loss = 3.52533\n",
      "epoch no.3 train no.326510  loss = 4.66928 avg_loss = 3.54705\n",
      "epoch no.3 train no.326520  loss = 3.73000 avg_loss = 3.56166\n",
      "epoch no.3 train no.326530  loss = 2.84340 avg_loss = 3.52684\n",
      "epoch no.3 train no.326540  loss = 2.77991 avg_loss = 3.48022\n",
      "epoch no.3 train no.326550  loss = 2.46220 avg_loss = 3.47204\n",
      "epoch no.3 train no.326560  loss = 4.10147 avg_loss = 3.48192\n",
      "epoch no.3 train no.326570  loss = 3.95352 avg_loss = 3.51982\n",
      "epoch no.3 train no.326580  loss = 3.91446 avg_loss = 3.52875\n",
      "epoch no.3 train no.326590  loss = 3.03677 avg_loss = 3.51144\n",
      "epoch no.3 train no.326600  loss = 3.27487 avg_loss = 3.48742\n",
      "epoch no.3 train no.326610  loss = 1.83684 avg_loss = 3.47010\n",
      "epoch no.3 train no.326620  loss = 3.21866 avg_loss = 3.47924\n",
      "epoch no.3 train no.326630  loss = 3.95349 avg_loss = 3.43797\n",
      "epoch no.3 train no.326640  loss = 2.92879 avg_loss = 3.46155\n",
      "epoch no.3 train no.326650  loss = 3.70748 avg_loss = 3.44995\n",
      "epoch no.3 train no.326660  loss = 3.57554 avg_loss = 3.46324\n",
      "epoch no.3 train no.326670  loss = 3.96993 avg_loss = 3.50706\n",
      "epoch no.3 train no.326680  loss = 3.12096 avg_loss = 3.55025\n",
      "epoch no.3 train no.326690  loss = 3.02766 avg_loss = 3.58055\n",
      "epoch no.3 train no.326700  loss = 7.77049 avg_loss = 3.56712\n",
      "epoch no.3 train no.326710  loss = 2.34192 avg_loss = 3.51874\n",
      "epoch no.3 train no.326720  loss = 3.79550 avg_loss = 3.52917\n",
      "epoch no.3 train no.326730  loss = 2.72008 avg_loss = 3.51230\n",
      "epoch no.3 train no.326740  loss = 1.78683 avg_loss = 3.46102\n",
      "epoch no.3 train no.326750  loss = 3.50986 avg_loss = 3.51131\n",
      "epoch no.3 train no.326760  loss = 3.06565 avg_loss = 3.49346\n",
      "epoch no.3 train no.326770  loss = 1.92998 avg_loss = 3.44732\n",
      "epoch no.3 train no.326780  loss = 2.21728 avg_loss = 3.46684\n",
      "epoch no.3 train no.326790  loss = 2.48395 avg_loss = 3.47781\n",
      "epoch no.3 train no.326800  loss = 2.79607 avg_loss = 3.47874\n",
      "epoch no.3 train no.326810  loss = 4.02685 avg_loss = 3.47519\n",
      "epoch no.3 train no.326820  loss = 1.42885 avg_loss = 3.46988\n",
      "epoch no.3 train no.326830  loss = 3.43938 avg_loss = 3.49186\n",
      "epoch no.3 train no.326840  loss = 2.74834 avg_loss = 3.50806\n",
      "epoch no.3 train no.326850  loss = 1.78500 avg_loss = 3.55282\n",
      "epoch no.3 train no.326860  loss = 3.34455 avg_loss = 3.51852\n",
      "epoch no.3 train no.326870  loss = 2.04880 avg_loss = 3.52306\n",
      "epoch no.3 train no.326880  loss = 4.32787 avg_loss = 3.53781\n",
      "epoch no.3 train no.326890  loss = 3.72606 avg_loss = 3.52376\n",
      "epoch no.3 train no.326900  loss = 2.75874 avg_loss = 3.55135\n",
      "epoch no.3 train no.326910  loss = 2.99059 avg_loss = 3.54456\n",
      "epoch no.3 train no.326920  loss = 3.26397 avg_loss = 3.52790\n",
      "epoch no.3 train no.326930  loss = 3.96898 avg_loss = 3.50908\n",
      "epoch no.3 train no.326940  loss = 2.92922 avg_loss = 3.52329\n",
      "epoch no.3 train no.326950  loss = 3.70968 avg_loss = 3.53657\n",
      "epoch no.3 train no.326960  loss = 2.24824 avg_loss = 3.57809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.326970  loss = 5.22592 avg_loss = 3.61596\n",
      "epoch no.3 train no.326980  loss = 4.40550 avg_loss = 3.61692\n",
      "epoch no.3 train no.326990  loss = 2.90329 avg_loss = 3.57641\n",
      "epoch no.3 train no.327000  loss = 4.85366 avg_loss = 3.58502\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁p', '▁노래', '</s>']\n",
      "기분전환 신나는 아이돌 노래</s>\n",
      "epoch no.3 train no.327010  loss = 3.60105 avg_loss = 3.53991\n",
      "epoch no.3 train no.327020  loss = 4.87675 avg_loss = 3.57626\n",
      "epoch no.3 train no.327030  loss = 3.08947 avg_loss = 3.56897\n",
      "epoch no.3 train no.327040  loss = 5.16515 avg_loss = 3.60766\n",
      "epoch no.3 train no.327050  loss = 2.40639 avg_loss = 3.56976\n",
      "epoch no.3 train no.327060  loss = 3.13834 avg_loss = 3.56393\n",
      "epoch no.3 train no.327070  loss = 3.89667 avg_loss = 3.57102\n",
      "epoch no.3 train no.327080  loss = 3.46783 avg_loss = 3.56458\n",
      "epoch no.3 train no.327090  loss = 4.67691 avg_loss = 3.57745\n",
      "epoch no.3 train no.327100  loss = 3.00920 avg_loss = 3.61523\n",
      "epoch no.3 train no.327110  loss = 3.77854 avg_loss = 3.59259\n",
      "epoch no.3 train no.327120  loss = 3.31382 avg_loss = 3.57366\n",
      "epoch no.3 train no.327130  loss = 4.93088 avg_loss = 3.59160\n",
      "epoch no.3 train no.327140  loss = 1.92020 avg_loss = 3.57020\n",
      "epoch no.3 train no.327150  loss = 2.11621 avg_loss = 3.51803\n",
      "epoch no.3 train no.327160  loss = 2.20289 avg_loss = 3.46420\n",
      "epoch no.3 train no.327170  loss = 3.08167 avg_loss = 3.44707\n",
      "epoch no.3 train no.327180  loss = 4.63427 avg_loss = 3.46642\n",
      "epoch no.3 train no.327190  loss = 3.60161 avg_loss = 3.48896\n",
      "epoch no.3 train no.327200  loss = 1.75060 avg_loss = 3.48969\n",
      "epoch no.3 train no.327210  loss = 5.02660 avg_loss = 3.51590\n",
      "epoch no.3 train no.327220  loss = 1.97360 avg_loss = 3.44983\n",
      "epoch no.3 train no.327230  loss = 5.55979 avg_loss = 3.47050\n",
      "epoch no.3 train no.327240  loss = 2.67500 avg_loss = 3.48118\n",
      "epoch no.3 train no.327250  loss = 2.40254 avg_loss = 3.45593\n",
      "epoch no.3 train no.327260  loss = 2.68533 avg_loss = 3.42012\n",
      "epoch no.3 train no.327270  loss = 4.37008 avg_loss = 3.45229\n",
      "epoch no.3 train no.327280  loss = 3.38829 avg_loss = 3.42934\n",
      "epoch no.3 train no.327290  loss = 3.92831 avg_loss = 3.41675\n",
      "epoch no.3 train no.327300  loss = 2.51550 avg_loss = 3.40541\n",
      "epoch no.3 train no.327310  loss = 3.63669 avg_loss = 3.40342\n",
      "epoch no.3 train no.327320  loss = 2.03829 avg_loss = 3.39263\n",
      "epoch no.3 train no.327330  loss = 4.45949 avg_loss = 3.42699\n",
      "epoch no.3 train no.327340  loss = 4.75907 avg_loss = 3.40898\n",
      "epoch no.3 train no.327350  loss = 2.79876 avg_loss = 3.42695\n",
      "epoch no.3 train no.327360  loss = 1.16990 avg_loss = 3.40208\n",
      "epoch no.3 train no.327370  loss = 5.04124 avg_loss = 3.42835\n",
      "epoch no.3 train no.327380  loss = 4.16515 avg_loss = 3.42315\n",
      "epoch no.3 train no.327390  loss = 3.30764 avg_loss = 3.45443\n",
      "epoch no.3 train no.327400  loss = 5.22086 avg_loss = 3.49870\n",
      "epoch no.3 train no.327410  loss = 3.31952 avg_loss = 3.48754\n",
      "epoch no.3 train no.327420  loss = 2.81912 avg_loss = 3.46188\n",
      "epoch no.3 train no.327430  loss = 3.94857 avg_loss = 3.50168\n",
      "epoch no.3 train no.327440  loss = 3.65403 avg_loss = 3.52563\n",
      "epoch no.3 train no.327450  loss = 2.12212 avg_loss = 3.51424\n",
      "epoch no.3 train no.327460  loss = 2.07965 avg_loss = 3.52107\n",
      "epoch no.3 train no.327470  loss = 3.33293 avg_loss = 3.51380\n",
      "epoch no.3 train no.327480  loss = 1.82764 avg_loss = 3.46345\n",
      "epoch no.3 train no.327490  loss = 3.59773 avg_loss = 3.46292\n",
      "epoch no.3 train no.327500  loss = 5.53109 avg_loss = 3.46530\n",
      "epoch no.3 train no.327510  loss = 5.69069 avg_loss = 3.51500\n",
      "epoch no.3 train no.327520  loss = 3.08802 avg_loss = 3.52637\n",
      "epoch no.3 train no.327530  loss = 4.40215 avg_loss = 3.52488\n",
      "epoch no.3 train no.327540  loss = 3.16691 avg_loss = 3.51862\n",
      "epoch no.3 train no.327550  loss = 5.90427 avg_loss = 3.58182\n",
      "epoch no.3 train no.327560  loss = 2.93848 avg_loss = 3.56520\n",
      "epoch no.3 train no.327570  loss = 3.85411 avg_loss = 3.51476\n",
      "epoch no.3 train no.327580  loss = 2.86289 avg_loss = 3.48722\n",
      "epoch no.3 train no.327590  loss = 4.83664 avg_loss = 3.55585\n",
      "epoch no.3 train no.327600  loss = 4.62764 avg_loss = 3.58992\n",
      "epoch no.3 train no.327610  loss = 1.83164 avg_loss = 3.55803\n",
      "epoch no.3 train no.327620  loss = 3.33573 avg_loss = 3.56107\n",
      "epoch no.3 train no.327630  loss = 4.69079 avg_loss = 3.58283\n",
      "epoch no.3 train no.327640  loss = 2.88427 avg_loss = 3.63391\n",
      "epoch no.3 train no.327650  loss = 4.21273 avg_loss = 3.58250\n",
      "epoch no.3 train no.327660  loss = 3.01206 avg_loss = 3.59574\n",
      "epoch no.3 train no.327670  loss = 2.57192 avg_loss = 3.59660\n",
      "epoch no.3 train no.327680  loss = 3.03755 avg_loss = 3.63322\n",
      "epoch no.3 train no.327690  loss = 3.25311 avg_loss = 3.61617\n",
      "epoch no.3 train no.327700  loss = 2.59877 avg_loss = 3.63521\n",
      "epoch no.3 train no.327710  loss = 3.65754 avg_loss = 3.62341\n",
      "epoch no.3 train no.327720  loss = 2.66712 avg_loss = 3.58213\n",
      "epoch no.3 train no.327730  loss = 4.38719 avg_loss = 3.62717\n",
      "epoch no.3 train no.327740  loss = 2.14076 avg_loss = 3.57810\n",
      "epoch no.3 train no.327750  loss = 4.89312 avg_loss = 3.57531\n",
      "epoch no.3 train no.327760  loss = 3.60891 avg_loss = 3.58576\n",
      "epoch no.3 train no.327770  loss = 4.08277 avg_loss = 3.60162\n",
      "epoch no.3 train no.327780  loss = 3.15018 avg_loss = 3.63018\n",
      "epoch no.3 train no.327790  loss = 4.14168 avg_loss = 3.66972\n",
      "epoch no.3 train no.327800  loss = 3.16846 avg_loss = 3.63128\n",
      "epoch no.3 train no.327810  loss = 2.15948 avg_loss = 3.62923\n",
      "epoch no.3 train no.327820  loss = 2.60324 avg_loss = 3.60566\n",
      "epoch no.3 train no.327830  loss = 1.98467 avg_loss = 3.61724\n",
      "epoch no.3 train no.327840  loss = 2.17920 avg_loss = 3.59419\n",
      "epoch no.3 train no.327850  loss = 2.64553 avg_loss = 3.54550\n",
      "epoch no.3 train no.327860  loss = 3.01640 avg_loss = 3.54247\n",
      "epoch no.3 train no.327870  loss = 2.90706 avg_loss = 3.48757\n",
      "epoch no.3 train no.327880  loss = 2.80457 avg_loss = 3.43752\n",
      "epoch no.3 train no.327890  loss = 2.86291 avg_loss = 3.47558\n",
      "epoch no.3 train no.327900  loss = 2.37674 avg_loss = 3.45329\n",
      "epoch no.3 train no.327910  loss = 2.68199 avg_loss = 3.47406\n",
      "epoch no.3 train no.327920  loss = 3.34838 avg_loss = 3.42704\n",
      "epoch no.3 train no.327930  loss = 3.77829 avg_loss = 3.46309\n",
      "epoch no.3 train no.327940  loss = 2.74161 avg_loss = 3.43169\n",
      "epoch no.3 train no.327950  loss = 2.44857 avg_loss = 3.45303\n",
      "epoch no.3 train no.327960  loss = 5.88509 avg_loss = 3.49368\n",
      "epoch no.3 train no.327970  loss = 3.46704 avg_loss = 3.46498\n",
      "epoch no.3 train no.327980  loss = 2.39534 avg_loss = 3.44267\n",
      "epoch no.3 train no.327990  loss = 1.87759 avg_loss = 3.46094\n",
      "epoch no.3 train no.328000  loss = 2.45234 avg_loss = 3.49878\n",
      "3\n",
      "to_tokens: ['▁가을', '좋은', '이', '▁위한', '▁팝', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.3 train no.328010  loss = 4.06548 avg_loss = 3.50791\n",
      "epoch no.3 train no.328020  loss = 2.63525 avg_loss = 3.52177\n",
      "epoch no.3 train no.328030  loss = 3.39423 avg_loss = 3.51691\n",
      "epoch no.3 train no.328040  loss = 4.68773 avg_loss = 3.53307\n",
      "epoch no.3 train no.328050  loss = 3.44263 avg_loss = 3.56988\n",
      "epoch no.3 train no.328060  loss = 3.67627 avg_loss = 3.57659\n",
      "epoch no.3 train no.328070  loss = 4.59544 avg_loss = 3.61119\n",
      "epoch no.3 train no.328080  loss = 3.59830 avg_loss = 3.61325\n",
      "epoch no.3 train no.328090  loss = 3.45174 avg_loss = 3.64321\n",
      "epoch no.3 train no.328100  loss = 2.54057 avg_loss = 3.63938\n",
      "epoch no.3 train no.328110  loss = 3.85947 avg_loss = 3.65679\n",
      "epoch no.3 train no.328120  loss = 3.38588 avg_loss = 3.64806\n",
      "epoch no.3 train no.328130  loss = 3.47159 avg_loss = 3.63311\n",
      "epoch no.3 train no.328140  loss = 2.30639 avg_loss = 3.58903\n",
      "epoch no.3 train no.328150  loss = 3.22906 avg_loss = 3.66332\n",
      "epoch no.3 train no.328160  loss = 3.58370 avg_loss = 3.67936\n",
      "epoch no.3 train no.328170  loss = 3.36814 avg_loss = 3.67106\n",
      "epoch no.3 train no.328180  loss = 2.91047 avg_loss = 3.62618\n",
      "epoch no.3 train no.328190  loss = 2.65705 avg_loss = 3.57547\n",
      "epoch no.3 train no.328200  loss = 2.75834 avg_loss = 3.56412\n",
      "epoch no.3 train no.328210  loss = 4.34772 avg_loss = 3.56214\n",
      "epoch no.3 train no.328220  loss = 3.94266 avg_loss = 3.57832\n",
      "epoch no.3 train no.328230  loss = 2.00307 avg_loss = 3.54011\n",
      "epoch no.3 train no.328240  loss = 2.75917 avg_loss = 3.52612\n",
      "epoch no.3 train no.328250  loss = 2.41407 avg_loss = 3.51172\n",
      "epoch no.3 train no.328260  loss = 4.75083 avg_loss = 3.58983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.328270  loss = 3.65440 avg_loss = 3.57873\n",
      "epoch no.3 train no.328280  loss = 4.22512 avg_loss = 3.55706\n",
      "epoch no.3 train no.328290  loss = 5.71822 avg_loss = 3.59661\n",
      "epoch no.3 train no.328300  loss = 3.21032 avg_loss = 3.58712\n",
      "epoch no.3 train no.328310  loss = 2.59392 avg_loss = 3.58674\n",
      "epoch no.3 train no.328320  loss = 2.83911 avg_loss = 3.61618\n",
      "epoch no.3 train no.328330  loss = 2.35930 avg_loss = 3.59794\n",
      "epoch no.3 train no.328340  loss = 2.80521 avg_loss = 3.54051\n",
      "epoch no.3 train no.328350  loss = 2.49927 avg_loss = 3.53071\n",
      "epoch no.3 train no.328360  loss = 2.64402 avg_loss = 3.55266\n",
      "epoch no.3 train no.328370  loss = 3.15646 avg_loss = 3.55392\n",
      "epoch no.3 train no.328380  loss = 3.71692 avg_loss = 3.57004\n",
      "epoch no.3 train no.328390  loss = 3.50441 avg_loss = 3.51273\n",
      "epoch no.3 train no.328400  loss = 3.37651 avg_loss = 3.50600\n",
      "epoch no.3 train no.328410  loss = 3.96889 avg_loss = 3.54293\n",
      "epoch no.3 train no.328420  loss = 3.57537 avg_loss = 3.57445\n",
      "epoch no.3 train no.328430  loss = 3.54788 avg_loss = 3.56508\n",
      "epoch no.3 train no.328440  loss = 2.95132 avg_loss = 3.55525\n",
      "epoch no.3 train no.328450  loss = 4.15801 avg_loss = 3.53750\n",
      "epoch no.3 train no.328460  loss = 2.25511 avg_loss = 3.49766\n",
      "epoch no.3 train no.328470  loss = 2.58629 avg_loss = 3.50656\n",
      "epoch no.3 train no.328480  loss = 5.72901 avg_loss = 3.51267\n",
      "epoch no.3 train no.328490  loss = 3.45486 avg_loss = 3.50506\n",
      "epoch no.3 train no.328500  loss = 2.20307 avg_loss = 3.49230\n",
      "epoch no.3 train no.328510  loss = 4.11099 avg_loss = 3.45159\n",
      "epoch no.3 train no.328520  loss = 3.70516 avg_loss = 3.42953\n",
      "epoch no.3 train no.328530  loss = 4.86018 avg_loss = 3.47690\n",
      "epoch no.3 train no.328540  loss = 5.57593 avg_loss = 3.54816\n",
      "epoch no.3 train no.328550  loss = 3.50449 avg_loss = 3.51093\n",
      "epoch no.3 train no.328560  loss = 3.86632 avg_loss = 3.53701\n",
      "epoch no.3 train no.328570  loss = 2.50316 avg_loss = 3.56667\n",
      "epoch no.3 train no.328580  loss = 2.76170 avg_loss = 3.53700\n",
      "epoch no.3 train no.328590  loss = 2.83966 avg_loss = 3.54581\n",
      "epoch no.3 train no.328600  loss = 4.16047 avg_loss = 3.53057\n",
      "epoch no.3 train no.328610  loss = 4.16756 avg_loss = 3.49171\n",
      "epoch no.3 train no.328620  loss = 2.62934 avg_loss = 3.56269\n",
      "epoch no.3 train no.328630  loss = 2.84159 avg_loss = 3.59742\n",
      "epoch no.3 train no.328640  loss = 3.01797 avg_loss = 3.56919\n",
      "epoch no.3 train no.328650  loss = 3.23270 avg_loss = 3.60307\n",
      "epoch no.3 train no.328660  loss = 2.38490 avg_loss = 3.59554\n",
      "epoch no.3 train no.328670  loss = 3.73122 avg_loss = 3.61332\n",
      "epoch no.3 train no.328680  loss = 4.16142 avg_loss = 3.61149\n",
      "epoch no.3 train no.328690  loss = 3.10821 avg_loss = 3.60082\n",
      "epoch no.3 train no.328700  loss = 3.07082 avg_loss = 3.61547\n",
      "epoch no.3 train no.328710  loss = 5.13420 avg_loss = 3.65065\n",
      "epoch no.3 train no.328720  loss = 3.06188 avg_loss = 3.66339\n",
      "epoch no.3 train no.328730  loss = 3.96915 avg_loss = 3.70126\n",
      "epoch no.3 train no.328740  loss = 2.47212 avg_loss = 3.70339\n",
      "epoch no.3 train no.328750  loss = 3.84405 avg_loss = 3.63762\n",
      "epoch no.3 train no.328760  loss = 1.80886 avg_loss = 3.63800\n",
      "epoch no.3 train no.328770  loss = 2.17879 avg_loss = 3.63370\n",
      "epoch no.3 train no.328780  loss = 2.85467 avg_loss = 3.61921\n",
      "epoch no.3 train no.328790  loss = 2.34275 avg_loss = 3.55721\n",
      "epoch no.3 train no.328800  loss = 3.21250 avg_loss = 3.48004\n",
      "epoch no.3 train no.328810  loss = 5.36978 avg_loss = 3.50451\n",
      "epoch no.3 train no.328820  loss = 3.75654 avg_loss = 3.49141\n",
      "epoch no.3 train no.328830  loss = 4.69682 avg_loss = 3.49299\n",
      "epoch no.3 train no.328840  loss = 3.16171 avg_loss = 3.50593\n",
      "epoch no.3 train no.328850  loss = 2.31755 avg_loss = 3.49078\n",
      "epoch no.3 train no.328860  loss = 4.31774 avg_loss = 3.50716\n",
      "epoch no.3 train no.328870  loss = 3.25937 avg_loss = 3.49172\n",
      "epoch no.3 train no.328880  loss = 5.33736 avg_loss = 3.48143\n",
      "epoch no.3 train no.328890  loss = 3.08645 avg_loss = 3.46605\n",
      "epoch no.3 train no.328900  loss = 6.87474 avg_loss = 3.57051\n",
      "epoch no.3 train no.328910  loss = 1.83753 avg_loss = 3.55359\n",
      "epoch no.3 train no.328920  loss = 3.83506 avg_loss = 3.56078\n",
      "epoch no.3 train no.328930  loss = 3.88942 avg_loss = 3.53669\n",
      "epoch no.3 train no.328940  loss = 3.10136 avg_loss = 3.56316\n",
      "epoch no.3 train no.328950  loss = 3.81912 avg_loss = 3.53363\n",
      "epoch no.3 train no.328960  loss = 3.36479 avg_loss = 3.48867\n",
      "epoch no.3 train no.328970  loss = 3.09544 avg_loss = 3.46607\n",
      "epoch no.3 train no.328980  loss = 3.21937 avg_loss = 3.47743\n",
      "epoch no.3 train no.328990  loss = 4.41386 avg_loss = 3.47721\n",
      "epoch no.3 train no.329000  loss = 3.12822 avg_loss = 3.45204\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁좋은', '▁딱', '▁좋은', '▁노래', '송', '</s>']\n",
      "기분전환에 딱 딱 좋은 팝송</s>\n",
      "epoch no.3 train no.329010  loss = 3.15579 avg_loss = 3.45283\n",
      "epoch no.3 train no.329020  loss = 3.20756 avg_loss = 3.50359\n",
      "epoch no.3 train no.329030  loss = 4.20423 avg_loss = 3.51415\n",
      "epoch no.3 train no.329040  loss = 2.26178 avg_loss = 3.53671\n",
      "epoch no.3 train no.329050  loss = 2.59206 avg_loss = 3.51219\n",
      "epoch no.3 train no.329060  loss = 2.24812 avg_loss = 3.48445\n",
      "epoch no.3 train no.329070  loss = 2.63555 avg_loss = 3.51880\n",
      "epoch no.3 train no.329080  loss = 3.31710 avg_loss = 3.53231\n",
      "epoch no.3 train no.329090  loss = 3.81673 avg_loss = 3.54495\n",
      "epoch no.3 train no.329100  loss = 2.66608 avg_loss = 3.54869\n",
      "epoch no.3 train no.329110  loss = 3.38914 avg_loss = 3.53986\n",
      "epoch no.3 train no.329120  loss = 3.25533 avg_loss = 3.53137\n",
      "epoch no.3 train no.329130  loss = 3.72908 avg_loss = 3.50019\n",
      "epoch no.3 train no.329140  loss = 3.00668 avg_loss = 3.50909\n",
      "epoch no.3 train no.329150  loss = 3.06582 avg_loss = 3.48571\n",
      "epoch no.3 train no.329160  loss = 4.73742 avg_loss = 3.48581\n",
      "epoch no.3 train no.329170  loss = 2.68628 avg_loss = 3.46627\n",
      "epoch no.3 train no.329180  loss = 3.77527 avg_loss = 3.51017\n",
      "epoch no.3 train no.329190  loss = 4.33640 avg_loss = 3.51654\n",
      "epoch no.3 train no.329200  loss = 2.39159 avg_loss = 3.49048\n",
      "epoch no.3 train no.329210  loss = 3.93483 avg_loss = 3.48164\n",
      "epoch no.3 train no.329220  loss = 2.77278 avg_loss = 3.48443\n",
      "epoch no.3 train no.329230  loss = 1.72688 avg_loss = 3.45538\n",
      "epoch no.3 train no.329240  loss = 3.41423 avg_loss = 3.48460\n",
      "epoch no.3 train no.329250  loss = 4.04489 avg_loss = 3.44370\n",
      "epoch no.3 train no.329260  loss = 1.97712 avg_loss = 3.45244\n",
      "epoch no.3 train no.329270  loss = 5.43078 avg_loss = 3.49921\n",
      "epoch no.3 train no.329280  loss = 3.86597 avg_loss = 3.48859\n",
      "epoch no.3 train no.329290  loss = 2.92374 avg_loss = 3.50501\n",
      "epoch no.3 train no.329300  loss = 5.65937 avg_loss = 3.48581\n",
      "epoch no.3 train no.329310  loss = 3.70239 avg_loss = 3.48152\n",
      "epoch no.3 train no.329320  loss = 3.44112 avg_loss = 3.45090\n",
      "epoch no.3 train no.329330  loss = 3.90809 avg_loss = 3.46526\n",
      "epoch no.3 train no.329340  loss = 2.67595 avg_loss = 3.46402\n",
      "epoch no.3 train no.329350  loss = 1.71870 avg_loss = 3.42402\n",
      "epoch no.3 train no.329360  loss = 3.26447 avg_loss = 3.43297\n",
      "epoch no.3 train no.329370  loss = 3.32595 avg_loss = 3.41975\n",
      "epoch no.3 train no.329380  loss = 2.74734 avg_loss = 3.42298\n",
      "epoch no.3 train no.329390  loss = 1.87377 avg_loss = 3.44386\n",
      "epoch no.3 train no.329400  loss = 2.88878 avg_loss = 3.42555\n",
      "epoch no.3 train no.329410  loss = 4.00086 avg_loss = 3.44390\n",
      "epoch no.3 train no.329420  loss = 4.41258 avg_loss = 3.51771\n",
      "epoch no.3 train no.329430  loss = 1.99508 avg_loss = 3.49125\n",
      "epoch no.3 train no.329440  loss = 3.99983 avg_loss = 3.46016\n",
      "epoch no.3 train no.329450  loss = 5.31587 avg_loss = 3.43806\n",
      "epoch no.3 train no.329460  loss = 3.89444 avg_loss = 3.46824\n",
      "epoch no.3 train no.329470  loss = 2.43234 avg_loss = 3.43155\n",
      "epoch no.3 train no.329480  loss = 2.78232 avg_loss = 3.41378\n",
      "epoch no.3 train no.329490  loss = 5.61015 avg_loss = 3.44222\n",
      "epoch no.3 train no.329500  loss = 3.16461 avg_loss = 3.44042\n",
      "epoch no.3 train no.329510  loss = 5.73532 avg_loss = 3.45525\n",
      "epoch no.3 train no.329520  loss = 2.96526 avg_loss = 3.42256\n",
      "epoch no.3 train no.329530  loss = 4.12492 avg_loss = 3.42527\n",
      "epoch no.3 train no.329540  loss = 3.30009 avg_loss = 3.40352\n",
      "epoch no.3 train no.329550  loss = 4.59018 avg_loss = 3.37375\n",
      "epoch no.3 train no.329560  loss = 5.17768 avg_loss = 3.45661\n",
      "epoch no.3 train no.329570  loss = 5.18055 avg_loss = 3.43240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.329580  loss = 2.58861 avg_loss = 3.48096\n",
      "epoch no.3 train no.329590  loss = 2.97127 avg_loss = 3.54623\n",
      "epoch no.3 train no.329600  loss = 4.05177 avg_loss = 3.49827\n",
      "epoch no.3 train no.329610  loss = 5.20945 avg_loss = 3.55681\n",
      "epoch no.3 train no.329620  loss = 3.67493 avg_loss = 3.53519\n",
      "epoch no.3 train no.329630  loss = 1.31053 avg_loss = 3.48300\n",
      "epoch no.3 train no.329640  loss = 2.55268 avg_loss = 3.46715\n",
      "epoch no.3 train no.329650  loss = 3.36763 avg_loss = 3.52028\n",
      "epoch no.3 train no.329660  loss = 4.50562 avg_loss = 3.53601\n",
      "epoch no.3 train no.329670  loss = 1.75983 avg_loss = 3.54486\n",
      "epoch no.3 train no.329680  loss = 2.72203 avg_loss = 3.53152\n",
      "epoch no.3 train no.329690  loss = 3.32517 avg_loss = 3.52236\n",
      "epoch no.3 train no.329700  loss = 3.96098 avg_loss = 3.51238\n",
      "epoch no.3 train no.329710  loss = 3.03659 avg_loss = 3.59141\n",
      "epoch no.3 train no.329720  loss = 3.25439 avg_loss = 3.60593\n",
      "epoch no.3 train no.329730  loss = 3.56131 avg_loss = 3.58790\n",
      "epoch no.3 train no.329740  loss = 1.82898 avg_loss = 3.60063\n",
      "epoch no.3 train no.329750  loss = 4.84570 avg_loss = 3.59490\n",
      "epoch no.3 train no.329760  loss = 3.03089 avg_loss = 3.56123\n",
      "epoch no.3 train no.329770  loss = 4.65408 avg_loss = 3.55066\n",
      "epoch no.3 train no.329780  loss = 3.31503 avg_loss = 3.55564\n",
      "epoch no.3 train no.329790  loss = 4.92652 avg_loss = 3.59578\n",
      "epoch no.3 train no.329800  loss = 3.62828 avg_loss = 3.60336\n",
      "epoch no.3 train no.329810  loss = 2.64462 avg_loss = 3.57149\n",
      "epoch no.3 train no.329820  loss = 3.85365 avg_loss = 3.52642\n",
      "epoch no.3 train no.329830  loss = 2.78550 avg_loss = 3.55005\n",
      "epoch no.3 train no.329840  loss = 2.99931 avg_loss = 3.52953\n",
      "epoch no.3 train no.329850  loss = 4.88157 avg_loss = 3.54130\n",
      "epoch no.3 train no.329860  loss = 1.95517 avg_loss = 3.50700\n",
      "epoch no.3 train no.329870  loss = 3.94027 avg_loss = 3.53476\n",
      "epoch no.3 train no.329880  loss = 3.82705 avg_loss = 3.60600\n",
      "epoch no.3 train no.329890  loss = 3.28535 avg_loss = 3.55240\n",
      "epoch no.3 train no.329900  loss = 5.46332 avg_loss = 3.53224\n",
      "epoch no.3 train no.329910  loss = 2.89822 avg_loss = 3.48969\n",
      "epoch no.3 train no.329920  loss = 5.46811 avg_loss = 3.50199\n",
      "epoch no.3 train no.329930  loss = 3.48625 avg_loss = 3.56377\n",
      "epoch no.3 train no.329940  loss = 3.38623 avg_loss = 3.51310\n",
      "epoch no.3 train no.329950  loss = 2.34272 avg_loss = 3.54781\n",
      "epoch no.3 train no.329960  loss = 2.16652 avg_loss = 3.50532\n",
      "epoch no.3 train no.329970  loss = 2.94487 avg_loss = 3.46791\n",
      "epoch no.3 train no.329980  loss = 2.92624 avg_loss = 3.46282\n",
      "epoch no.3 train no.329990  loss = 3.41059 avg_loss = 3.53501\n",
      "epoch no.3 train no.330000  loss = 3.68437 avg_loss = 3.50239\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '▁때', '▁듣는', '▁음악', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.3 train no.330010  loss = 3.08344 avg_loss = 3.50953\n",
      "epoch no.3 train no.330020  loss = 3.20455 avg_loss = 3.50983\n",
      "epoch no.3 train no.330030  loss = 2.90378 avg_loss = 3.42867\n",
      "epoch no.3 train no.330040  loss = 2.68491 avg_loss = 3.49778\n",
      "epoch no.3 train no.330050  loss = 2.97548 avg_loss = 3.49204\n",
      "epoch no.3 train no.330060  loss = 3.76278 avg_loss = 3.54268\n",
      "epoch no.3 train no.330070  loss = 2.12607 avg_loss = 3.54334\n",
      "epoch no.3 train no.330080  loss = 6.58576 avg_loss = 3.59536\n",
      "epoch no.3 train no.330090  loss = 2.83366 avg_loss = 3.54717\n",
      "epoch no.3 train no.330100  loss = 3.93647 avg_loss = 3.54484\n",
      "epoch no.3 train no.330110  loss = 3.34274 avg_loss = 3.56154\n",
      "epoch no.3 train no.330120  loss = 5.13366 avg_loss = 3.58904\n",
      "epoch no.3 train no.330130  loss = 2.97522 avg_loss = 3.60054\n",
      "epoch no.3 train no.330140  loss = 3.46676 avg_loss = 3.55551\n",
      "epoch no.3 train no.330150  loss = 2.60131 avg_loss = 3.54657\n",
      "epoch no.3 train no.330160  loss = 2.09767 avg_loss = 3.58539\n",
      "epoch no.3 train no.330170  loss = 3.12820 avg_loss = 3.60023\n",
      "epoch no.3 train no.330180  loss = 3.77920 avg_loss = 3.58192\n",
      "epoch no.3 train no.330190  loss = 5.60478 avg_loss = 3.62829\n",
      "epoch no.3 train no.330200  loss = 2.74850 avg_loss = 3.63645\n",
      "epoch no.3 train no.330210  loss = 3.78137 avg_loss = 3.60181\n",
      "epoch no.3 train no.330220  loss = 4.52205 avg_loss = 3.62349\n",
      "epoch no.3 train no.330230  loss = 2.39442 avg_loss = 3.65077\n",
      "epoch no.3 train no.330240  loss = 4.13501 avg_loss = 3.61117\n",
      "epoch no.3 train no.330250  loss = 3.33738 avg_loss = 3.61572\n",
      "epoch no.3 train no.330260  loss = 4.72211 avg_loss = 3.60669\n",
      "epoch no.3 train no.330270  loss = 3.91862 avg_loss = 3.60951\n",
      "epoch no.3 train no.330280  loss = 1.53671 avg_loss = 3.62553\n",
      "epoch no.3 train no.330290  loss = 4.27789 avg_loss = 3.61131\n",
      "epoch no.3 train no.330300  loss = 3.86243 avg_loss = 3.59836\n",
      "epoch no.3 train no.330310  loss = 3.85168 avg_loss = 3.57246\n",
      "epoch no.3 train no.330320  loss = 2.64381 avg_loss = 3.54431\n",
      "epoch no.3 train no.330330  loss = 3.91692 avg_loss = 3.52831\n",
      "epoch no.3 train no.330340  loss = 2.91535 avg_loss = 3.51938\n",
      "epoch no.3 train no.330350  loss = 5.14039 avg_loss = 3.52519\n",
      "epoch no.3 train no.330360  loss = 2.82820 avg_loss = 3.50180\n",
      "epoch no.3 train no.330370  loss = 3.81710 avg_loss = 3.54359\n",
      "epoch no.3 train no.330380  loss = 3.99401 avg_loss = 3.52485\n",
      "epoch no.3 train no.330390  loss = 5.63362 avg_loss = 3.57683\n",
      "epoch no.3 train no.330400  loss = 3.14470 avg_loss = 3.57464\n",
      "epoch no.3 train no.330410  loss = 2.65267 avg_loss = 3.49687\n",
      "epoch no.3 train no.330420  loss = 2.31662 avg_loss = 3.53652\n",
      "epoch no.3 train no.330430  loss = 4.27002 avg_loss = 3.55064\n",
      "epoch no.3 train no.330440  loss = 5.47746 avg_loss = 3.58104\n",
      "epoch no.3 train no.330450  loss = 3.76933 avg_loss = 3.65184\n",
      "epoch no.3 train no.330460  loss = 3.72719 avg_loss = 3.64328\n",
      "epoch no.3 train no.330470  loss = 2.52262 avg_loss = 3.55893\n",
      "epoch no.3 train no.330480  loss = 3.82968 avg_loss = 3.56373\n",
      "epoch no.3 train no.330490  loss = 3.35763 avg_loss = 3.55066\n",
      "epoch no.3 train no.330500  loss = 2.06164 avg_loss = 3.59139\n",
      "epoch no.3 train no.330510  loss = 3.86943 avg_loss = 3.56851\n",
      "epoch no.3 train no.330520  loss = 3.17343 avg_loss = 3.52869\n",
      "epoch no.3 train no.330530  loss = 3.22930 avg_loss = 3.56593\n",
      "epoch no.3 train no.330540  loss = 2.87670 avg_loss = 3.52251\n",
      "epoch no.3 train no.330550  loss = 4.77350 avg_loss = 3.49184\n",
      "epoch no.3 train no.330560  loss = 2.81002 avg_loss = 3.48017\n",
      "epoch no.3 train no.330570  loss = 3.28065 avg_loss = 3.47873\n",
      "epoch no.3 train no.330580  loss = 3.59166 avg_loss = 3.43845\n",
      "epoch no.3 train no.330590  loss = 3.79355 avg_loss = 3.46821\n",
      "epoch no.3 train no.330600  loss = 1.96262 avg_loss = 3.42348\n",
      "epoch no.3 train no.330610  loss = 2.53181 avg_loss = 3.41113\n",
      "epoch no.3 train no.330620  loss = 4.24167 avg_loss = 3.44056\n",
      "epoch no.3 train no.330630  loss = 3.34854 avg_loss = 3.41664\n",
      "epoch no.3 train no.330640  loss = 2.47258 avg_loss = 3.45361\n",
      "epoch no.3 train no.330650  loss = 2.98289 avg_loss = 3.44973\n",
      "epoch no.3 train no.330660  loss = 3.51384 avg_loss = 3.44378\n",
      "epoch no.3 train no.330670  loss = 3.52697 avg_loss = 3.45239\n",
      "epoch no.3 train no.330680  loss = 2.30369 avg_loss = 3.48326\n",
      "epoch no.3 train no.330690  loss = 3.49734 avg_loss = 3.50093\n",
      "epoch no.3 train no.330700  loss = 3.80335 avg_loss = 3.47536\n",
      "epoch no.3 train no.330710  loss = 7.55131 avg_loss = 3.52165\n",
      "epoch no.3 train no.330720  loss = 3.20746 avg_loss = 3.50510\n",
      "epoch no.3 train no.330730  loss = 2.79091 avg_loss = 3.52920\n",
      "epoch no.3 train no.330740  loss = 2.76036 avg_loss = 3.49241\n",
      "epoch no.3 train no.330750  loss = 4.84357 avg_loss = 3.48888\n",
      "epoch no.3 train no.330760  loss = 2.82121 avg_loss = 3.48603\n",
      "epoch no.3 train no.330770  loss = 3.14763 avg_loss = 3.48461\n",
      "epoch no.3 train no.330780  loss = 3.08705 avg_loss = 3.49845\n",
      "epoch no.3 train no.330790  loss = 3.00121 avg_loss = 3.55175\n",
      "epoch no.3 train no.330800  loss = 4.77818 avg_loss = 3.54803\n",
      "epoch no.3 train no.330810  loss = 5.07073 avg_loss = 3.57074\n",
      "epoch no.3 train no.330820  loss = 2.49461 avg_loss = 3.60141\n",
      "epoch no.3 train no.330830  loss = 5.44026 avg_loss = 3.60586\n",
      "epoch no.3 train no.330840  loss = 4.82212 avg_loss = 3.55361\n",
      "epoch no.3 train no.330850  loss = 4.03100 avg_loss = 3.58827\n",
      "epoch no.3 train no.330860  loss = 2.26156 avg_loss = 3.60751\n",
      "epoch no.3 train no.330870  loss = 5.05656 avg_loss = 3.61630\n",
      "epoch no.3 train no.330880  loss = 2.54234 avg_loss = 3.58530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.330890  loss = 2.80366 avg_loss = 3.61403\n",
      "epoch no.3 train no.330900  loss = 3.64516 avg_loss = 3.62119\n",
      "epoch no.3 train no.330910  loss = 2.34149 avg_loss = 3.57335\n",
      "epoch no.3 train no.330920  loss = 3.81020 avg_loss = 3.63944\n",
      "epoch no.3 train no.330930  loss = 2.91034 avg_loss = 3.67066\n",
      "epoch no.3 train no.330940  loss = 2.98794 avg_loss = 3.68108\n",
      "epoch no.3 train no.330950  loss = 4.97664 avg_loss = 3.66972\n",
      "epoch no.3 train no.330960  loss = 3.47384 avg_loss = 3.68831\n",
      "epoch no.3 train no.330970  loss = 3.55506 avg_loss = 3.69338\n",
      "epoch no.3 train no.330980  loss = 3.67664 avg_loss = 3.64200\n",
      "epoch no.3 train no.330990  loss = 2.92472 avg_loss = 3.65380\n",
      "epoch no.3 train no.331000  loss = 4.72012 avg_loss = 3.62794\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁팝', 'op', '</s>']\n",
      "기분전환 신나는 pop</s>\n",
      "epoch no.3 train no.331010  loss = 6.62884 avg_loss = 3.65081\n",
      "epoch no.3 train no.331020  loss = 3.34624 avg_loss = 3.64591\n",
      "epoch no.3 train no.331030  loss = 2.89718 avg_loss = 3.64648\n",
      "epoch no.3 train no.331040  loss = 4.20166 avg_loss = 3.61256\n",
      "epoch no.3 train no.331050  loss = 3.37347 avg_loss = 3.58325\n",
      "epoch no.3 train no.331060  loss = 3.22906 avg_loss = 3.53118\n",
      "epoch no.3 train no.331070  loss = 1.46911 avg_loss = 3.55539\n",
      "epoch no.3 train no.331080  loss = 3.15882 avg_loss = 3.56304\n",
      "epoch no.3 train no.331090  loss = 3.17148 avg_loss = 3.55934\n",
      "epoch no.3 train no.331100  loss = 4.91820 avg_loss = 3.55606\n",
      "epoch no.3 train no.331110  loss = 4.49363 avg_loss = 3.58269\n",
      "epoch no.3 train no.331120  loss = 4.72319 avg_loss = 3.56000\n",
      "epoch no.3 train no.331130  loss = 3.79116 avg_loss = 3.59387\n",
      "epoch no.3 train no.331140  loss = 3.34090 avg_loss = 3.57565\n",
      "epoch no.3 train no.331150  loss = 6.15371 avg_loss = 3.57749\n",
      "epoch no.3 train no.331160  loss = 5.02669 avg_loss = 3.67350\n",
      "epoch no.3 train no.331170  loss = 2.29758 avg_loss = 3.67005\n",
      "epoch no.3 train no.331180  loss = 2.98144 avg_loss = 3.70795\n",
      "epoch no.3 train no.331190  loss = 1.55390 avg_loss = 3.64651\n",
      "epoch no.3 train no.331200  loss = 3.13804 avg_loss = 3.62525\n",
      "epoch no.3 train no.331210  loss = 2.61689 avg_loss = 3.63971\n",
      "epoch no.3 train no.331220  loss = 3.13374 avg_loss = 3.66166\n",
      "epoch no.3 train no.331230  loss = 3.74152 avg_loss = 3.66976\n",
      "epoch no.3 train no.331240  loss = 3.12976 avg_loss = 3.68462\n",
      "epoch no.3 train no.331250  loss = 4.22613 avg_loss = 3.64941\n",
      "epoch no.3 train no.331260  loss = 4.07308 avg_loss = 3.64246\n",
      "epoch no.3 train no.331270  loss = 2.33676 avg_loss = 3.61343\n",
      "epoch no.3 train no.331280  loss = 2.36528 avg_loss = 3.58633\n",
      "epoch no.3 train no.331290  loss = 3.60413 avg_loss = 3.56808\n",
      "epoch no.3 train no.331300  loss = 4.79200 avg_loss = 3.59057\n",
      "epoch no.3 train no.331310  loss = 1.89588 avg_loss = 3.57071\n",
      "epoch no.3 train no.331320  loss = 3.85278 avg_loss = 3.61543\n",
      "epoch no.3 train no.331330  loss = 4.78430 avg_loss = 3.60692\n",
      "epoch no.3 train no.331340  loss = 5.52564 avg_loss = 3.64307\n",
      "epoch no.3 train no.331350  loss = 2.04075 avg_loss = 3.63071\n",
      "epoch no.3 train no.331360  loss = 3.50034 avg_loss = 3.65053\n",
      "epoch no.3 train no.331370  loss = 3.74416 avg_loss = 3.61925\n",
      "epoch no.3 train no.331380  loss = 2.83887 avg_loss = 3.60126\n",
      "epoch no.3 train no.331390  loss = 3.12810 avg_loss = 3.64347\n",
      "epoch no.3 train no.331400  loss = 3.57938 avg_loss = 3.63958\n",
      "epoch no.3 train no.331410  loss = 2.95099 avg_loss = 3.66393\n",
      "epoch no.3 train no.331420  loss = 2.70251 avg_loss = 3.62771\n",
      "epoch no.3 train no.331430  loss = 3.18877 avg_loss = 3.60689\n",
      "epoch no.3 train no.331440  loss = 3.73422 avg_loss = 3.64912\n",
      "epoch no.3 train no.331450  loss = 2.67139 avg_loss = 3.65658\n",
      "epoch no.3 train no.331460  loss = 3.10119 avg_loss = 3.66090\n",
      "epoch no.3 train no.331470  loss = 1.55140 avg_loss = 3.61698\n",
      "epoch no.3 train no.331480  loss = 2.85461 avg_loss = 3.61115\n",
      "epoch no.3 train no.331490  loss = 6.98337 avg_loss = 3.68002\n",
      "epoch no.3 train no.331500  loss = 3.78172 avg_loss = 3.69248\n",
      "epoch no.3 train no.331510  loss = 3.78808 avg_loss = 3.68325\n",
      "epoch no.3 train no.331520  loss = 2.72527 avg_loss = 3.64749\n",
      "epoch no.3 train no.331530  loss = 2.04967 avg_loss = 3.58376\n",
      "epoch no.3 train no.331540  loss = 4.92859 avg_loss = 3.62355\n",
      "epoch no.3 train no.331550  loss = 3.25992 avg_loss = 3.56610\n",
      "epoch no.3 train no.331560  loss = 3.53562 avg_loss = 3.59168\n",
      "epoch no.3 train no.331570  loss = 3.03592 avg_loss = 3.62952\n",
      "epoch no.3 train no.331580  loss = 4.51300 avg_loss = 3.59173\n",
      "epoch no.3 train no.331590  loss = 2.20841 avg_loss = 3.64111\n",
      "epoch no.3 train no.331600  loss = 4.08273 avg_loss = 3.61748\n",
      "epoch no.3 train no.331610  loss = 4.78769 avg_loss = 3.63894\n",
      "epoch no.3 train no.331620  loss = 2.30753 avg_loss = 3.55406\n",
      "epoch no.3 train no.331630  loss = 4.41858 avg_loss = 3.61937\n",
      "epoch no.3 train no.331640  loss = 3.78610 avg_loss = 3.59081\n",
      "epoch no.3 train no.331650  loss = 2.65823 avg_loss = 3.54735\n",
      "epoch no.3 train no.331660  loss = 2.81846 avg_loss = 3.58751\n",
      "epoch no.3 train no.331670  loss = 3.82679 avg_loss = 3.62195\n",
      "epoch no.3 train no.331680  loss = 3.20915 avg_loss = 3.61633\n",
      "epoch no.3 train no.331690  loss = 1.87620 avg_loss = 3.57394\n",
      "epoch no.3 train no.331700  loss = 2.54360 avg_loss = 3.54822\n",
      "epoch no.3 train no.331710  loss = 2.86932 avg_loss = 3.50556\n",
      "epoch no.3 train no.331720  loss = 3.66241 avg_loss = 3.49215\n",
      "epoch no.3 train no.331730  loss = 5.19755 avg_loss = 3.49746\n",
      "epoch no.3 train no.331740  loss = 3.87561 avg_loss = 3.45542\n",
      "epoch no.3 train no.331750  loss = 4.41283 avg_loss = 3.42615\n",
      "epoch no.3 train no.331760  loss = 3.77139 avg_loss = 3.40882\n",
      "epoch no.3 train no.331770  loss = 3.54288 avg_loss = 3.47026\n",
      "epoch no.3 train no.331780  loss = 3.75725 avg_loss = 3.46989\n",
      "epoch no.3 train no.331790  loss = 2.86956 avg_loss = 3.47401\n",
      "epoch no.3 train no.331800  loss = 2.56685 avg_loss = 3.50388\n",
      "epoch no.3 train no.331810  loss = 2.65833 avg_loss = 3.50829\n",
      "epoch no.3 train no.331820  loss = 4.33863 avg_loss = 3.58535\n",
      "epoch no.3 train no.331830  loss = 3.67755 avg_loss = 3.59303\n",
      "epoch no.3 train no.331840  loss = 4.17400 avg_loss = 3.59921\n",
      "epoch no.3 train no.331850  loss = 4.68602 avg_loss = 3.57514\n",
      "epoch no.3 train no.331860  loss = 3.06206 avg_loss = 3.55992\n",
      "epoch no.3 train no.331870  loss = 2.91596 avg_loss = 3.58843\n",
      "epoch no.3 train no.331880  loss = 5.22212 avg_loss = 3.60079\n",
      "epoch no.3 train no.331890  loss = 3.58435 avg_loss = 3.58153\n",
      "epoch no.3 train no.331900  loss = 4.36341 avg_loss = 3.58646\n",
      "epoch no.3 train no.331910  loss = 3.81973 avg_loss = 3.52110\n",
      "epoch no.3 train no.331920  loss = 2.50979 avg_loss = 3.54580\n",
      "epoch no.3 train no.331930  loss = 5.21347 avg_loss = 3.54087\n",
      "epoch no.3 train no.331940  loss = 5.39625 avg_loss = 3.58030\n",
      "epoch no.3 train no.331950  loss = 3.64176 avg_loss = 3.53074\n",
      "epoch no.3 train no.331960  loss = 2.14909 avg_loss = 3.52916\n",
      "epoch no.3 train no.331970  loss = 5.11829 avg_loss = 3.56314\n",
      "epoch no.3 train no.331980  loss = 3.84505 avg_loss = 3.57855\n",
      "epoch no.3 train no.331990  loss = 3.24241 avg_loss = 3.55947\n",
      "epoch no.3 train no.332000  loss = 2.78494 avg_loss = 3.52689\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.3 train no.332010  loss = 3.17951 avg_loss = 3.54147\n",
      "epoch no.3 train no.332020  loss = 5.20274 avg_loss = 3.53880\n",
      "epoch no.3 train no.332030  loss = 2.95554 avg_loss = 3.52784\n",
      "epoch no.3 train no.332040  loss = 4.40716 avg_loss = 3.57196\n",
      "epoch no.3 train no.332050  loss = 3.66115 avg_loss = 3.54578\n",
      "epoch no.3 train no.332060  loss = 3.54384 avg_loss = 3.52704\n",
      "epoch no.3 train no.332070  loss = 2.73255 avg_loss = 3.60004\n",
      "epoch no.3 train no.332080  loss = 3.55873 avg_loss = 3.57834\n",
      "epoch no.3 train no.332090  loss = 3.46456 avg_loss = 3.58855\n",
      "epoch no.3 train no.332100  loss = 2.22252 avg_loss = 3.61094\n",
      "epoch no.3 train no.332110  loss = 2.74159 avg_loss = 3.62322\n",
      "epoch no.3 train no.332120  loss = 2.29187 avg_loss = 3.58501\n",
      "epoch no.3 train no.332130  loss = 3.82330 avg_loss = 3.56077\n",
      "epoch no.3 train no.332140  loss = 4.82436 avg_loss = 3.55850\n",
      "epoch no.3 train no.332150  loss = 3.36074 avg_loss = 3.51431\n",
      "epoch no.3 train no.332160  loss = 4.25927 avg_loss = 3.49361\n",
      "epoch no.3 train no.332170  loss = 3.00555 avg_loss = 3.51485\n",
      "epoch no.3 train no.332180  loss = 3.98584 avg_loss = 3.45992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.332190  loss = 2.71563 avg_loss = 3.53760\n",
      "epoch no.3 train no.332200  loss = 5.57208 avg_loss = 3.54093\n",
      "epoch no.3 train no.332210  loss = 3.47592 avg_loss = 3.61419\n",
      "epoch no.3 train no.332220  loss = 3.18437 avg_loss = 3.58479\n",
      "epoch no.3 train no.332230  loss = 3.90487 avg_loss = 3.58544\n",
      "epoch no.3 train no.332240  loss = 3.35887 avg_loss = 3.59548\n",
      "epoch no.3 train no.332250  loss = 2.85085 avg_loss = 3.55924\n",
      "epoch no.3 train no.332260  loss = 6.81340 avg_loss = 3.58778\n",
      "epoch no.3 train no.332270  loss = 2.65312 avg_loss = 3.56610\n",
      "epoch no.3 train no.332280  loss = 4.58131 avg_loss = 3.53962\n",
      "epoch no.3 train no.332290  loss = 4.88241 avg_loss = 3.56312\n",
      "epoch no.3 train no.332300  loss = 3.10128 avg_loss = 3.47037\n",
      "epoch no.3 train no.332310  loss = 4.78588 avg_loss = 3.52389\n",
      "epoch no.3 train no.332320  loss = 6.22883 avg_loss = 3.59983\n",
      "epoch no.3 train no.332330  loss = 3.79948 avg_loss = 3.60602\n",
      "epoch no.3 train no.332340  loss = 2.79443 avg_loss = 3.54155\n",
      "epoch no.3 train no.332350  loss = 4.20972 avg_loss = 3.53200\n",
      "epoch no.3 train no.332360  loss = 2.94447 avg_loss = 3.53234\n",
      "epoch no.3 train no.332370  loss = 2.69765 avg_loss = 3.54442\n",
      "epoch no.3 train no.332380  loss = 2.18293 avg_loss = 3.50631\n",
      "epoch no.3 train no.332390  loss = 3.57143 avg_loss = 3.57333\n",
      "epoch no.3 train no.332400  loss = 5.05143 avg_loss = 3.55277\n",
      "epoch no.3 train no.332410  loss = 2.13490 avg_loss = 3.53724\n",
      "epoch no.3 train no.332420  loss = 3.58066 avg_loss = 3.53150\n",
      "epoch no.3 train no.332430  loss = 4.16881 avg_loss = 3.49104\n",
      "epoch no.3 train no.332440  loss = 4.23880 avg_loss = 3.52091\n",
      "epoch no.3 train no.332450  loss = 3.02263 avg_loss = 3.52457\n",
      "epoch no.3 train no.332460  loss = 2.54451 avg_loss = 3.47787\n",
      "epoch no.3 train no.332470  loss = 3.31086 avg_loss = 3.54875\n",
      "epoch no.3 train no.332480  loss = 2.72395 avg_loss = 3.56740\n",
      "epoch no.3 train no.332490  loss = 4.25382 avg_loss = 3.57471\n",
      "epoch no.3 train no.332500  loss = 1.98697 avg_loss = 3.53906\n",
      "epoch no.3 train no.332510  loss = 1.79635 avg_loss = 3.51410\n",
      "epoch no.3 train no.332520  loss = 3.12715 avg_loss = 3.51638\n",
      "epoch no.3 train no.332530  loss = 2.36147 avg_loss = 3.52467\n",
      "epoch no.3 train no.332540  loss = 4.00476 avg_loss = 3.49484\n",
      "epoch no.3 train no.332550  loss = 2.55008 avg_loss = 3.46431\n",
      "epoch no.3 train no.332560  loss = 2.82374 avg_loss = 3.40705\n",
      "epoch no.3 train no.332570  loss = 2.70455 avg_loss = 3.40880\n",
      "epoch no.3 train no.332580  loss = 3.88857 avg_loss = 3.40327\n",
      "epoch no.3 train no.332590  loss = 4.86832 avg_loss = 3.40023\n",
      "epoch no.3 train no.332600  loss = 3.24031 avg_loss = 3.38453\n",
      "epoch no.3 train no.332610  loss = 6.01516 avg_loss = 3.43689\n",
      "epoch no.3 train no.332620  loss = 3.37278 avg_loss = 3.47447\n",
      "epoch no.3 train no.332630  loss = 4.48035 avg_loss = 3.49365\n",
      "epoch no.3 train no.332640  loss = 2.51462 avg_loss = 3.47428\n",
      "epoch no.3 train no.332650  loss = 3.81366 avg_loss = 3.52772\n",
      "epoch no.3 train no.332660  loss = 5.12634 avg_loss = 3.51007\n",
      "epoch no.3 train no.332670  loss = 3.89268 avg_loss = 3.46882\n",
      "epoch no.3 train no.332680  loss = 5.13928 avg_loss = 3.51215\n",
      "epoch no.3 train no.332690  loss = 4.03635 avg_loss = 3.52800\n",
      "epoch no.3 train no.332700  loss = 3.53124 avg_loss = 3.50290\n",
      "epoch no.3 train no.332710  loss = 3.46761 avg_loss = 3.49268\n",
      "epoch no.3 train no.332720  loss = 6.10496 avg_loss = 3.48362\n",
      "epoch no.3 train no.332730  loss = 2.42372 avg_loss = 3.49215\n",
      "epoch no.3 train no.332740  loss = 4.72947 avg_loss = 3.51819\n",
      "epoch no.3 train no.332750  loss = 3.10572 avg_loss = 3.51803\n",
      "epoch no.3 train no.332760  loss = 3.12176 avg_loss = 3.49296\n",
      "epoch no.3 train no.332770  loss = 4.93340 avg_loss = 3.47931\n",
      "epoch no.3 train no.332780  loss = 3.28297 avg_loss = 3.48404\n",
      "epoch no.3 train no.332790  loss = 4.35123 avg_loss = 3.50172\n",
      "epoch no.3 train no.332800  loss = 4.08902 avg_loss = 3.49472\n",
      "epoch no.3 train no.332810  loss = 2.63093 avg_loss = 3.49728\n",
      "epoch no.3 train no.332820  loss = 4.45182 avg_loss = 3.59577\n",
      "epoch no.3 train no.332830  loss = 5.72085 avg_loss = 3.58034\n",
      "epoch no.3 train no.332840  loss = 3.68750 avg_loss = 3.53003\n",
      "epoch no.3 train no.332850  loss = 3.70485 avg_loss = 3.54816\n",
      "epoch no.3 train no.332860  loss = 4.63697 avg_loss = 3.53271\n",
      "epoch no.3 train no.332870  loss = 1.83624 avg_loss = 3.44029\n",
      "epoch no.3 train no.332880  loss = 3.41449 avg_loss = 3.46457\n",
      "epoch no.3 train no.332890  loss = 4.49424 avg_loss = 3.47480\n",
      "epoch no.3 train no.332900  loss = 2.85547 avg_loss = 3.44912\n",
      "epoch no.3 train no.332910  loss = 2.66491 avg_loss = 3.42497\n",
      "epoch no.3 train no.332920  loss = 5.06092 avg_loss = 3.45822\n",
      "epoch no.3 train no.332930  loss = 2.46073 avg_loss = 3.49427\n",
      "epoch no.3 train no.332940  loss = 3.67122 avg_loss = 3.47261\n",
      "epoch no.3 train no.332950  loss = 2.07541 avg_loss = 3.41961\n",
      "epoch no.3 train no.332960  loss = 3.64234 avg_loss = 3.38307\n",
      "epoch no.3 train no.332970  loss = 3.04271 avg_loss = 3.36157\n",
      "epoch no.3 train no.332980  loss = 3.77691 avg_loss = 3.38072\n",
      "epoch no.3 train no.332990  loss = 3.88597 avg_loss = 3.35663\n",
      "epoch no.3 train no.333000  loss = 3.36116 avg_loss = 3.35794\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '▁신나는', '▁딱', '▁좋은', '▁팝', '</s>']\n",
      "기분전환에 딱 좋은 힙합</s>\n",
      "epoch no.3 train no.333010  loss = 2.43682 avg_loss = 3.37962\n",
      "epoch no.3 train no.333020  loss = 3.01582 avg_loss = 3.40430\n",
      "epoch no.3 train no.333030  loss = 2.73621 avg_loss = 3.38326\n",
      "epoch no.3 train no.333040  loss = 3.33835 avg_loss = 3.43893\n",
      "epoch no.3 train no.333050  loss = 4.73825 avg_loss = 3.50130\n",
      "epoch no.3 train no.333060  loss = 3.68873 avg_loss = 3.46674\n",
      "epoch no.3 train no.333070  loss = 4.17567 avg_loss = 3.43408\n",
      "epoch no.3 train no.333080  loss = 4.78953 avg_loss = 3.43521\n",
      "epoch no.3 train no.333090  loss = 4.24701 avg_loss = 3.45873\n",
      "epoch no.3 train no.333100  loss = 5.16933 avg_loss = 3.46652\n",
      "epoch no.3 train no.333110  loss = 5.23227 avg_loss = 3.49120\n",
      "epoch no.3 train no.333120  loss = 3.38472 avg_loss = 3.46540\n",
      "epoch no.3 train no.333130  loss = 1.41543 avg_loss = 3.44532\n",
      "epoch no.3 train no.333140  loss = 5.76812 avg_loss = 3.49634\n",
      "epoch no.3 train no.333150  loss = 6.99026 avg_loss = 3.55530\n",
      "epoch no.3 train no.333160  loss = 2.06671 avg_loss = 3.52741\n",
      "epoch no.3 train no.333170  loss = 0.96053 avg_loss = 3.51897\n",
      "epoch no.3 train no.333180  loss = 1.71081 avg_loss = 3.51113\n",
      "epoch no.3 train no.333190  loss = 3.57650 avg_loss = 3.47580\n",
      "epoch no.3 train no.333200  loss = 2.45338 avg_loss = 3.45977\n",
      "epoch no.3 train no.333210  loss = 4.62820 avg_loss = 3.49926\n",
      "epoch no.3 train no.333220  loss = 4.41240 avg_loss = 3.50259\n",
      "epoch no.3 train no.333230  loss = 2.37374 avg_loss = 3.47862\n",
      "epoch no.3 train no.333240  loss = 2.14104 avg_loss = 3.50382\n",
      "epoch no.3 train no.333250  loss = 4.61011 avg_loss = 3.48585\n",
      "epoch no.3 train no.333260  loss = 5.09194 avg_loss = 3.49996\n",
      "epoch no.3 train no.333270  loss = 5.33908 avg_loss = 3.56607\n",
      "epoch no.3 train no.333280  loss = 2.94750 avg_loss = 3.57302\n",
      "epoch no.3 train no.333290  loss = 2.79954 avg_loss = 3.51883\n",
      "epoch no.3 train no.333300  loss = 4.19026 avg_loss = 3.54783\n",
      "epoch no.3 train no.333310  loss = 2.35523 avg_loss = 3.53299\n",
      "epoch no.3 train no.333320  loss = 2.74498 avg_loss = 3.51648\n",
      "epoch no.3 train no.333330  loss = 2.52063 avg_loss = 3.54878\n",
      "epoch no.3 train no.333340  loss = 2.18395 avg_loss = 3.55721\n",
      "epoch no.3 train no.333350  loss = 3.89870 avg_loss = 3.54876\n",
      "epoch no.3 train no.333360  loss = 2.97198 avg_loss = 3.48462\n",
      "epoch no.3 train no.333370  loss = 2.91564 avg_loss = 3.50127\n",
      "epoch no.3 train no.333380  loss = 3.08254 avg_loss = 3.53429\n",
      "epoch no.3 train no.333390  loss = 4.08584 avg_loss = 3.58109\n",
      "epoch no.3 train no.333400  loss = 4.44379 avg_loss = 3.59174\n",
      "epoch no.3 train no.333410  loss = 4.15279 avg_loss = 3.54026\n",
      "epoch no.3 train no.333420  loss = 3.43819 avg_loss = 3.51143\n",
      "epoch no.3 train no.333430  loss = 4.45426 avg_loss = 3.49557\n",
      "epoch no.3 train no.333440  loss = 1.91203 avg_loss = 3.47559\n",
      "epoch no.3 train no.333450  loss = 5.31898 avg_loss = 3.48732\n",
      "epoch no.3 train no.333460  loss = 2.29527 avg_loss = 3.46689\n",
      "epoch no.3 train no.333470  loss = 3.78356 avg_loss = 3.52408\n",
      "epoch no.3 train no.333480  loss = 2.13557 avg_loss = 3.51084\n",
      "epoch no.3 train no.333490  loss = 4.37686 avg_loss = 3.49793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.333500  loss = 4.44847 avg_loss = 3.48985\n",
      "epoch no.3 train no.333510  loss = 3.31761 avg_loss = 3.48965\n",
      "epoch no.3 train no.333520  loss = 3.72858 avg_loss = 3.52777\n",
      "epoch no.3 train no.333530  loss = 2.75371 avg_loss = 3.54774\n",
      "epoch no.3 train no.333540  loss = 5.34016 avg_loss = 3.59829\n",
      "epoch no.3 train no.333550  loss = 3.14325 avg_loss = 3.55339\n",
      "epoch no.3 train no.333560  loss = 2.34265 avg_loss = 3.55304\n",
      "epoch no.3 train no.333570  loss = 4.26154 avg_loss = 3.56041\n",
      "epoch no.3 train no.333580  loss = 2.20361 avg_loss = 3.58818\n",
      "epoch no.3 train no.333590  loss = 4.34579 avg_loss = 3.63370\n",
      "epoch no.3 train no.333600  loss = 2.89070 avg_loss = 3.61330\n",
      "epoch no.3 train no.333610  loss = 3.73759 avg_loss = 3.63179\n",
      "epoch no.3 train no.333620  loss = 3.74312 avg_loss = 3.65975\n",
      "epoch no.3 train no.333630  loss = 3.58302 avg_loss = 3.71831\n",
      "epoch no.3 train no.333640  loss = 3.65507 avg_loss = 3.65337\n",
      "epoch no.3 train no.333650  loss = 6.70574 avg_loss = 3.61734\n",
      "epoch no.3 train no.333660  loss = 3.99868 avg_loss = 3.60131\n",
      "epoch no.3 train no.333670  loss = 2.90883 avg_loss = 3.61275\n",
      "epoch no.3 train no.333680  loss = 2.98240 avg_loss = 3.58071\n",
      "epoch no.3 train no.333690  loss = 3.61068 avg_loss = 3.53395\n",
      "epoch no.3 train no.333700  loss = 4.99184 avg_loss = 3.53200\n",
      "epoch no.3 train no.333710  loss = 3.01418 avg_loss = 3.48425\n",
      "epoch no.3 train no.333720  loss = 2.47515 avg_loss = 3.51737\n",
      "epoch no.3 train no.333730  loss = 3.11995 avg_loss = 3.47477\n",
      "epoch no.3 train no.333740  loss = 3.12333 avg_loss = 3.50921\n",
      "epoch no.3 train no.333750  loss = 2.46044 avg_loss = 3.50820\n",
      "epoch no.3 train no.333760  loss = 4.04307 avg_loss = 3.45896\n",
      "epoch no.3 train no.333770  loss = 3.27760 avg_loss = 3.45752\n",
      "epoch no.3 train no.333780  loss = 2.78787 avg_loss = 3.44526\n",
      "epoch no.3 train no.333790  loss = 2.47794 avg_loss = 3.40576\n",
      "epoch no.3 train no.333800  loss = 1.28448 avg_loss = 3.38493\n",
      "epoch no.3 train no.333810  loss = 2.56930 avg_loss = 3.37502\n",
      "epoch no.3 train no.333820  loss = 4.60639 avg_loss = 3.41137\n",
      "epoch no.3 train no.333830  loss = 2.10172 avg_loss = 3.39312\n",
      "epoch no.3 train no.333840  loss = 4.30186 avg_loss = 3.41525\n",
      "epoch no.3 train no.333850  loss = 5.05158 avg_loss = 3.42977\n",
      "epoch no.3 train no.333860  loss = 5.11795 avg_loss = 3.47018\n",
      "epoch no.3 train no.333870  loss = 3.81716 avg_loss = 3.49338\n",
      "epoch no.3 train no.333880  loss = 3.69194 avg_loss = 3.47129\n",
      "epoch no.3 train no.333890  loss = 2.43007 avg_loss = 3.45807\n",
      "epoch no.3 train no.333900  loss = 3.07137 avg_loss = 3.44131\n",
      "epoch no.3 train no.333910  loss = 2.70939 avg_loss = 3.48546\n",
      "epoch no.3 train no.333920  loss = 3.14497 avg_loss = 3.55785\n",
      "epoch no.3 train no.333930  loss = 3.23169 avg_loss = 3.55850\n",
      "epoch no.3 train no.333940  loss = 4.16848 avg_loss = 3.58715\n",
      "epoch no.3 train no.333950  loss = 3.83725 avg_loss = 3.57546\n",
      "epoch no.3 train no.333960  loss = 2.31626 avg_loss = 3.54972\n",
      "epoch no.3 train no.333970  loss = 3.94664 avg_loss = 3.55697\n",
      "epoch no.3 train no.333980  loss = 2.68600 avg_loss = 3.51035\n",
      "epoch no.3 train no.333990  loss = 2.40945 avg_loss = 3.53997\n",
      "epoch no.3 train no.334000  loss = 1.84081 avg_loss = 3.48403\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁신나는']\n",
      "기분전환용</s>\n",
      "epoch no.3 train no.334010  loss = 3.86070 avg_loss = 3.46457\n",
      "epoch no.3 train no.334020  loss = 3.57649 avg_loss = 3.47886\n",
      "epoch no.3 train no.334030  loss = 3.11727 avg_loss = 3.47125\n",
      "epoch no.3 train no.334040  loss = 2.17737 avg_loss = 3.46598\n",
      "epoch no.3 train no.334050  loss = 3.37418 avg_loss = 3.43387\n",
      "epoch no.3 train no.334060  loss = 3.10819 avg_loss = 3.43751\n",
      "epoch no.3 train no.334070  loss = 3.23440 avg_loss = 3.40850\n",
      "epoch no.3 train no.334080  loss = 2.94360 avg_loss = 3.34857\n",
      "epoch no.3 train no.334090  loss = 4.44718 avg_loss = 3.39958\n",
      "epoch no.3 train no.334100  loss = 5.55547 avg_loss = 3.47752\n",
      "epoch no.3 train no.334110  loss = 3.98254 avg_loss = 3.50298\n",
      "epoch no.3 train no.334120  loss = 2.33072 avg_loss = 3.50158\n",
      "epoch no.3 train no.334130  loss = 2.82612 avg_loss = 3.54895\n",
      "epoch no.3 train no.334140  loss = 3.38538 avg_loss = 3.56658\n",
      "epoch no.3 train no.334150  loss = 3.86120 avg_loss = 3.58627\n",
      "epoch no.3 train no.334160  loss = 3.23549 avg_loss = 3.58229\n",
      "epoch no.3 train no.334170  loss = 4.61784 avg_loss = 3.60509\n",
      "epoch no.3 train no.334180  loss = 4.00024 avg_loss = 3.57510\n",
      "epoch no.3 train no.334190  loss = 3.87466 avg_loss = 3.57499\n",
      "epoch no.3 train no.334200  loss = 2.54673 avg_loss = 3.53207\n",
      "epoch no.3 train no.334210  loss = 4.51521 avg_loss = 3.50868\n",
      "epoch no.3 train no.334220  loss = 2.67292 avg_loss = 3.51068\n",
      "epoch no.3 train no.334230  loss = 3.17105 avg_loss = 3.48092\n",
      "epoch no.3 train no.334240  loss = 3.28161 avg_loss = 3.55839\n",
      "epoch no.3 train no.334250  loss = 3.74179 avg_loss = 3.57614\n",
      "epoch no.3 train no.334260  loss = 2.54909 avg_loss = 3.61621\n",
      "epoch no.3 train no.334270  loss = 4.40290 avg_loss = 3.57723\n",
      "epoch no.3 train no.334280  loss = 4.99853 avg_loss = 3.57019\n",
      "epoch no.3 train no.334290  loss = 3.50058 avg_loss = 3.56514\n",
      "epoch no.3 train no.334300  loss = 3.94915 avg_loss = 3.52975\n",
      "epoch no.3 train no.334310  loss = 3.86901 avg_loss = 3.52843\n",
      "epoch no.3 train no.334320  loss = 4.85692 avg_loss = 3.55239\n",
      "epoch no.3 train no.334330  loss = 4.94897 avg_loss = 3.54912\n",
      "epoch no.3 train no.334340  loss = 3.64957 avg_loss = 3.57178\n",
      "epoch no.3 train no.334350  loss = 3.46163 avg_loss = 3.55623\n",
      "epoch no.3 train no.334360  loss = 3.80650 avg_loss = 3.51586\n",
      "epoch no.3 train no.334370  loss = 3.17903 avg_loss = 3.49742\n",
      "epoch no.3 train no.334380  loss = 3.30430 avg_loss = 3.50717\n",
      "epoch no.3 train no.334390  loss = 4.53938 avg_loss = 3.50893\n",
      "epoch no.3 train no.334400  loss = 4.20270 avg_loss = 3.57079\n",
      "epoch no.3 train no.334410  loss = 6.29238 avg_loss = 3.60231\n",
      "epoch no.3 train no.334420  loss = 3.67954 avg_loss = 3.62607\n",
      "epoch no.3 train no.334430  loss = 2.76588 avg_loss = 3.66461\n",
      "epoch no.3 train no.334440  loss = 3.97802 avg_loss = 3.64697\n",
      "epoch no.3 train no.334450  loss = 5.66317 avg_loss = 3.65897\n",
      "epoch no.3 train no.334460  loss = 1.91505 avg_loss = 3.63613\n",
      "epoch no.3 train no.334470  loss = 2.03252 avg_loss = 3.64023\n",
      "epoch no.3 train no.334480  loss = 3.99724 avg_loss = 3.61465\n",
      "epoch no.3 train no.334490  loss = 3.38331 avg_loss = 3.59576\n",
      "epoch no.3 train no.334500  loss = 2.18516 avg_loss = 3.59459\n",
      "epoch no.3 train no.334510  loss = 2.28526 avg_loss = 3.59580\n",
      "epoch no.3 train no.334520  loss = 4.30959 avg_loss = 3.56356\n",
      "epoch no.3 train no.334530  loss = 4.15200 avg_loss = 3.59702\n",
      "epoch no.3 train no.334540  loss = 3.23196 avg_loss = 3.60181\n",
      "epoch no.3 train no.334550  loss = 4.54982 avg_loss = 3.61491\n",
      "epoch no.3 train no.334560  loss = 5.50521 avg_loss = 3.62074\n",
      "epoch no.3 train no.334570  loss = 3.09583 avg_loss = 3.63840\n",
      "epoch no.3 train no.334580  loss = 5.69277 avg_loss = 3.66054\n",
      "epoch no.3 train no.334590  loss = 3.21016 avg_loss = 3.59647\n",
      "epoch no.3 train no.334600  loss = 5.26531 avg_loss = 3.67036\n",
      "epoch no.3 train no.334610  loss = 2.27627 avg_loss = 3.61108\n",
      "epoch no.3 train no.334620  loss = 3.48271 avg_loss = 3.59283\n",
      "epoch no.3 train no.334630  loss = 2.96952 avg_loss = 3.58568\n",
      "epoch no.3 train no.334640  loss = 2.58573 avg_loss = 3.55338\n",
      "epoch no.3 train no.334650  loss = 4.71824 avg_loss = 3.54470\n",
      "epoch no.3 train no.334660  loss = 3.56275 avg_loss = 3.52454\n",
      "epoch no.3 train no.334670  loss = 5.40042 avg_loss = 3.57159\n",
      "epoch no.3 train no.334680  loss = 4.50386 avg_loss = 3.55389\n",
      "epoch no.3 train no.334690  loss = 3.72683 avg_loss = 3.56962\n",
      "epoch no.3 train no.334700  loss = 3.09566 avg_loss = 3.58963\n",
      "epoch no.3 train no.334710  loss = 3.95469 avg_loss = 3.61673\n",
      "epoch no.3 train no.334720  loss = 3.30262 avg_loss = 3.62081\n",
      "epoch no.3 train no.334730  loss = 4.60981 avg_loss = 3.64089\n",
      "epoch no.3 train no.334740  loss = 2.52540 avg_loss = 3.62453\n",
      "epoch no.3 train no.334750  loss = 2.87521 avg_loss = 3.60410\n",
      "epoch no.3 train no.334760  loss = 4.34023 avg_loss = 3.61724\n",
      "epoch no.3 train no.334770  loss = 4.93727 avg_loss = 3.62384\n",
      "epoch no.3 train no.334780  loss = 3.66146 avg_loss = 3.59349\n",
      "epoch no.3 train no.334790  loss = 2.67331 avg_loss = 3.53064\n",
      "epoch no.3 train no.334800  loss = 2.48654 avg_loss = 3.52877\n",
      "epoch no.3 train no.334810  loss = 3.22211 avg_loss = 3.55120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.334820  loss = 3.07693 avg_loss = 3.51378\n",
      "epoch no.3 train no.334830  loss = 4.74482 avg_loss = 3.54444\n",
      "epoch no.3 train no.334840  loss = 4.33695 avg_loss = 3.54459\n",
      "epoch no.3 train no.334850  loss = 2.94555 avg_loss = 3.53644\n",
      "epoch no.3 train no.334860  loss = 5.83540 avg_loss = 3.54826\n",
      "epoch no.3 train no.334870  loss = 4.93474 avg_loss = 3.54737\n",
      "epoch no.3 train no.334880  loss = 4.01024 avg_loss = 3.49738\n",
      "epoch no.3 train no.334890  loss = 2.11708 avg_loss = 3.49093\n",
      "epoch no.3 train no.334900  loss = 3.19731 avg_loss = 3.54552\n",
      "epoch no.3 train no.334910  loss = 4.49176 avg_loss = 3.52945\n",
      "epoch no.3 train no.334920  loss = 8.33485 avg_loss = 3.56026\n",
      "epoch no.3 train no.334930  loss = 4.00788 avg_loss = 3.56888\n",
      "epoch no.3 train no.334940  loss = 3.35903 avg_loss = 3.53739\n",
      "epoch no.3 train no.334950  loss = 3.08445 avg_loss = 3.56264\n",
      "epoch no.3 train no.334960  loss = 5.09906 avg_loss = 3.55825\n",
      "epoch no.3 train no.334970  loss = 2.99928 avg_loss = 3.54096\n",
      "epoch no.3 train no.334980  loss = 1.96865 avg_loss = 3.48161\n",
      "epoch no.3 train no.334990  loss = 2.64576 avg_loss = 3.50524\n",
      "epoch no.3 train no.335000  loss = 3.37131 avg_loss = 3.50658\n",
      "2\n",
      "to_tokens: ['▁비', '전환', '에', '▁필요할', '</s>']\n",
      "기분전환이 필요해</s>\n",
      "epoch no.3 train no.335010  loss = 2.12463 avg_loss = 3.47940\n",
      "epoch no.3 train no.335020  loss = 2.72476 avg_loss = 3.43519\n",
      "epoch no.3 train no.335030  loss = 3.71461 avg_loss = 3.45394\n",
      "epoch no.3 train no.335040  loss = 1.96822 avg_loss = 3.44253\n",
      "epoch no.3 train no.335050  loss = 2.71557 avg_loss = 3.44518\n",
      "epoch no.3 train no.335060  loss = 2.87005 avg_loss = 3.42264\n",
      "epoch no.3 train no.335070  loss = 2.85501 avg_loss = 3.37809\n",
      "epoch no.3 train no.335080  loss = 3.42135 avg_loss = 3.37529\n",
      "epoch no.3 train no.335090  loss = 3.21961 avg_loss = 3.38975\n",
      "epoch no.3 train no.335100  loss = 4.10381 avg_loss = 3.40598\n",
      "epoch no.3 train no.335110  loss = 4.32459 avg_loss = 3.44046\n",
      "epoch no.3 train no.335120  loss = 5.43507 avg_loss = 3.47587\n",
      "epoch no.3 train no.335130  loss = 4.31145 avg_loss = 3.51330\n",
      "epoch no.3 train no.335140  loss = 4.55085 avg_loss = 3.54017\n",
      "epoch no.3 train no.335150  loss = 3.33287 avg_loss = 3.54422\n",
      "epoch no.3 train no.335160  loss = 3.66810 avg_loss = 3.55959\n",
      "epoch no.3 train no.335170  loss = 2.76229 avg_loss = 3.54875\n",
      "epoch no.3 train no.335180  loss = 5.81914 avg_loss = 3.55414\n",
      "epoch no.3 train no.335190  loss = 4.45913 avg_loss = 3.54723\n",
      "epoch no.3 train no.335200  loss = 2.80088 avg_loss = 3.49135\n",
      "epoch no.3 train no.335210  loss = 1.80077 avg_loss = 3.46639\n",
      "epoch no.3 train no.335220  loss = 3.02602 avg_loss = 3.46063\n",
      "epoch no.3 train no.335230  loss = 3.74005 avg_loss = 3.47543\n",
      "epoch no.3 train no.335240  loss = 6.15851 avg_loss = 3.51761\n",
      "epoch no.3 train no.335250  loss = 3.64294 avg_loss = 3.49981\n",
      "epoch no.3 train no.335260  loss = 3.24825 avg_loss = 3.53050\n",
      "epoch no.3 train no.335270  loss = 5.68002 avg_loss = 3.56385\n",
      "epoch no.3 train no.335280  loss = 3.05228 avg_loss = 3.52732\n",
      "epoch no.3 train no.335290  loss = 3.99584 avg_loss = 3.53597\n",
      "epoch no.3 train no.335300  loss = 3.23120 avg_loss = 3.49731\n",
      "epoch no.3 train no.335310  loss = 3.12898 avg_loss = 3.52337\n",
      "epoch no.3 train no.335320  loss = 3.62744 avg_loss = 3.56649\n",
      "epoch no.3 train no.335330  loss = 3.12120 avg_loss = 3.56535\n",
      "epoch no.3 train no.335340  loss = 2.48272 avg_loss = 3.54600\n",
      "epoch no.3 train no.335350  loss = 2.79137 avg_loss = 3.55363\n",
      "epoch no.3 train no.335360  loss = 3.55568 avg_loss = 3.50379\n",
      "epoch no.3 train no.335370  loss = 2.42451 avg_loss = 3.49328\n",
      "epoch no.3 train no.335380  loss = 2.42384 avg_loss = 3.46444\n",
      "epoch no.3 train no.335390  loss = 4.41378 avg_loss = 3.53783\n",
      "epoch no.3 train no.335400  loss = 2.64044 avg_loss = 3.58065\n",
      "epoch no.3 train no.335410  loss = 3.20390 avg_loss = 3.57558\n",
      "epoch no.3 train no.335420  loss = 2.97797 avg_loss = 3.59775\n",
      "epoch no.3 train no.335430  loss = 4.52800 avg_loss = 3.66464\n",
      "epoch no.3 train no.335440  loss = 3.11954 avg_loss = 3.66510\n",
      "epoch no.3 train no.335450  loss = 4.26937 avg_loss = 3.65693\n",
      "epoch no.3 train no.335460  loss = 3.85252 avg_loss = 3.66742\n",
      "epoch no.3 train no.335470  loss = 2.71927 avg_loss = 3.62559\n",
      "epoch no.3 train no.335480  loss = 1.77848 avg_loss = 3.60780\n",
      "epoch no.3 train no.335490  loss = 5.20386 avg_loss = 3.57929\n",
      "epoch no.3 train no.335500  loss = 2.59213 avg_loss = 3.60890\n",
      "epoch no.3 train no.335510  loss = 4.64304 avg_loss = 3.60724\n",
      "epoch no.3 train no.335520  loss = 4.01797 avg_loss = 3.66924\n",
      "epoch no.3 train no.335530  loss = 3.22765 avg_loss = 3.63767\n",
      "epoch no.3 train no.335540  loss = 4.03882 avg_loss = 3.60306\n",
      "epoch no.3 train no.335550  loss = 4.62397 avg_loss = 3.61355\n",
      "epoch no.3 train no.335560  loss = 3.88820 avg_loss = 3.61903\n",
      "epoch no.3 train no.335570  loss = 6.15555 avg_loss = 3.61747\n",
      "epoch no.3 train no.335580  loss = 4.28848 avg_loss = 3.59055\n",
      "epoch no.3 train no.335590  loss = 4.18647 avg_loss = 3.57616\n",
      "epoch no.3 train no.335600  loss = 4.79439 avg_loss = 3.56907\n",
      "epoch no.3 train no.335610  loss = 3.84429 avg_loss = 3.52990\n",
      "epoch no.4 train no.335620  loss = 4.52744 avg_loss = 3.47458\n",
      "epoch no.4 train no.335630  loss = 2.42264 avg_loss = 3.42359\n",
      "epoch no.4 train no.335640  loss = 2.83461 avg_loss = 3.41580\n",
      "epoch no.4 train no.335650  loss = 3.09741 avg_loss = 3.46911\n",
      "epoch no.4 train no.335660  loss = 3.75809 avg_loss = 3.45026\n",
      "epoch no.4 train no.335670  loss = 2.58619 avg_loss = 3.40499\n",
      "epoch no.4 train no.335680  loss = 2.75931 avg_loss = 3.38321\n",
      "epoch no.4 train no.335690  loss = 1.51313 avg_loss = 3.37281\n",
      "epoch no.4 train no.335700  loss = 3.13576 avg_loss = 3.36213\n",
      "epoch no.4 train no.335710  loss = 3.40559 avg_loss = 3.36035\n",
      "epoch no.4 train no.335720  loss = 3.16174 avg_loss = 3.33064\n",
      "epoch no.4 train no.335730  loss = 2.12198 avg_loss = 3.28945\n",
      "epoch no.4 train no.335740  loss = 2.90262 avg_loss = 3.32105\n",
      "epoch no.4 train no.335750  loss = 2.90079 avg_loss = 3.30716\n",
      "epoch no.4 train no.335760  loss = 2.14904 avg_loss = 3.28807\n",
      "epoch no.4 train no.335770  loss = 3.03073 avg_loss = 3.28180\n",
      "epoch no.4 train no.335780  loss = 3.98667 avg_loss = 3.26771\n",
      "epoch no.4 train no.335790  loss = 3.14033 avg_loss = 3.25257\n",
      "epoch no.4 train no.335800  loss = 3.56422 avg_loss = 3.25649\n",
      "epoch no.4 train no.335810  loss = 3.21062 avg_loss = 3.26210\n",
      "epoch no.4 train no.335820  loss = 3.06240 avg_loss = 3.24876\n",
      "epoch no.4 train no.335830  loss = 3.72159 avg_loss = 3.22091\n",
      "epoch no.4 train no.335840  loss = 3.83152 avg_loss = 3.24080\n",
      "epoch no.4 train no.335850  loss = 2.98285 avg_loss = 3.27488\n",
      "epoch no.4 train no.335860  loss = 3.43674 avg_loss = 3.28811\n",
      "epoch no.4 train no.335870  loss = 3.38039 avg_loss = 3.29483\n",
      "epoch no.4 train no.335880  loss = 2.48279 avg_loss = 3.26701\n",
      "epoch no.4 train no.335890  loss = 3.08046 avg_loss = 3.23378\n",
      "epoch no.4 train no.335900  loss = 2.58037 avg_loss = 3.21043\n",
      "epoch no.4 train no.335910  loss = 3.01432 avg_loss = 3.21299\n",
      "epoch no.4 train no.335920  loss = 4.09804 avg_loss = 3.25302\n",
      "epoch no.4 train no.335930  loss = 4.15427 avg_loss = 3.34850\n",
      "epoch no.4 train no.335940  loss = 3.08460 avg_loss = 3.29610\n",
      "epoch no.4 train no.335950  loss = 3.14616 avg_loss = 3.28846\n",
      "epoch no.4 train no.335960  loss = 2.78303 avg_loss = 3.28187\n",
      "epoch no.4 train no.335970  loss = 2.73541 avg_loss = 3.24395\n",
      "epoch no.4 train no.335980  loss = 4.90673 avg_loss = 3.25925\n",
      "epoch no.4 train no.335990  loss = 3.19186 avg_loss = 3.21054\n",
      "epoch no.4 train no.336000  loss = 3.50613 avg_loss = 3.22822\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '</s>']\n",
      "기분전환이 필요해</s>\n",
      "epoch no.4 train no.336010  loss = 5.14138 avg_loss = 3.24269\n",
      "epoch no.4 train no.336020  loss = 3.16362 avg_loss = 3.23523\n",
      "epoch no.4 train no.336030  loss = 2.14144 avg_loss = 3.20553\n",
      "epoch no.4 train no.336040  loss = 2.65073 avg_loss = 3.21328\n",
      "epoch no.4 train no.336050  loss = 3.46542 avg_loss = 3.23770\n",
      "epoch no.4 train no.336060  loss = 4.14007 avg_loss = 3.22571\n",
      "epoch no.4 train no.336070  loss = 2.63371 avg_loss = 3.19883\n",
      "epoch no.4 train no.336080  loss = 3.96698 avg_loss = 3.23467\n",
      "epoch no.4 train no.336090  loss = 3.59596 avg_loss = 3.28465\n",
      "epoch no.4 train no.336100  loss = 2.17571 avg_loss = 3.28611\n",
      "epoch no.4 train no.336110  loss = 4.14859 avg_loss = 3.30418\n",
      "epoch no.4 train no.336120  loss = 3.45423 avg_loss = 3.31760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.336130  loss = 2.88569 avg_loss = 3.26683\n",
      "epoch no.4 train no.336140  loss = 4.71910 avg_loss = 3.30502\n",
      "epoch no.4 train no.336150  loss = 4.21263 avg_loss = 3.29996\n",
      "epoch no.4 train no.336160  loss = 3.76904 avg_loss = 3.28201\n",
      "epoch no.4 train no.336170  loss = 2.62780 avg_loss = 3.21625\n",
      "epoch no.4 train no.336180  loss = 3.74199 avg_loss = 3.23911\n",
      "epoch no.4 train no.336190  loss = 3.24325 avg_loss = 3.22252\n",
      "epoch no.4 train no.336200  loss = 2.52995 avg_loss = 3.19444\n",
      "epoch no.4 train no.336210  loss = 2.29257 avg_loss = 3.21721\n",
      "epoch no.4 train no.336220  loss = 2.37895 avg_loss = 3.20855\n",
      "epoch no.4 train no.336230  loss = 2.46568 avg_loss = 3.17804\n",
      "epoch no.4 train no.336240  loss = 2.47481 avg_loss = 3.17976\n",
      "epoch no.4 train no.336250  loss = 2.35126 avg_loss = 3.18032\n",
      "epoch no.4 train no.336260  loss = 2.98927 avg_loss = 3.19453\n",
      "epoch no.4 train no.336270  loss = 2.35803 avg_loss = 3.19913\n",
      "epoch no.4 train no.336280  loss = 4.20463 avg_loss = 3.23111\n",
      "epoch no.4 train no.336290  loss = 2.29182 avg_loss = 3.21811\n",
      "epoch no.4 train no.336300  loss = 3.26832 avg_loss = 3.22835\n",
      "epoch no.4 train no.336310  loss = 1.85913 avg_loss = 3.17406\n",
      "epoch no.4 train no.336320  loss = 3.75371 avg_loss = 3.14741\n",
      "epoch no.4 train no.336330  loss = 5.05099 avg_loss = 3.19177\n",
      "epoch no.4 train no.336340  loss = 3.54926 avg_loss = 3.21316\n",
      "epoch no.4 train no.336350  loss = 4.00091 avg_loss = 3.19867\n",
      "epoch no.4 train no.336360  loss = 2.51922 avg_loss = 3.20737\n",
      "epoch no.4 train no.336370  loss = 4.60085 avg_loss = 3.18848\n",
      "epoch no.4 train no.336380  loss = 4.31133 avg_loss = 3.17610\n",
      "epoch no.4 train no.336390  loss = 2.56025 avg_loss = 3.18402\n",
      "epoch no.4 train no.336400  loss = 4.16903 avg_loss = 3.25841\n",
      "epoch no.4 train no.336410  loss = 3.38344 avg_loss = 3.25208\n",
      "epoch no.4 train no.336420  loss = 2.88829 avg_loss = 3.23813\n",
      "epoch no.4 train no.336430  loss = 2.85167 avg_loss = 3.22644\n",
      "epoch no.4 train no.336440  loss = 2.37724 avg_loss = 3.27054\n",
      "epoch no.4 train no.336450  loss = 2.08527 avg_loss = 3.24247\n",
      "epoch no.4 train no.336460  loss = 2.49974 avg_loss = 3.25036\n",
      "epoch no.4 train no.336470  loss = 2.91201 avg_loss = 3.22193\n",
      "epoch no.4 train no.336480  loss = 2.20164 avg_loss = 3.19325\n",
      "epoch no.4 train no.336490  loss = 2.96263 avg_loss = 3.18186\n",
      "epoch no.4 train no.336500  loss = 2.87726 avg_loss = 3.18087\n",
      "epoch no.4 train no.336510  loss = 3.68365 avg_loss = 3.16423\n",
      "epoch no.4 train no.336520  loss = 2.79148 avg_loss = 3.17610\n",
      "epoch no.4 train no.336530  loss = 2.09233 avg_loss = 3.19694\n",
      "epoch no.4 train no.336540  loss = 3.96514 avg_loss = 3.26601\n",
      "epoch no.4 train no.336550  loss = 1.61739 avg_loss = 3.23008\n",
      "epoch no.4 train no.336560  loss = 2.71015 avg_loss = 3.23754\n",
      "epoch no.4 train no.336570  loss = 2.35148 avg_loss = 3.21508\n",
      "epoch no.4 train no.336580  loss = 2.15794 avg_loss = 3.20331\n",
      "epoch no.4 train no.336590  loss = 2.68667 avg_loss = 3.19296\n",
      "epoch no.4 train no.336600  loss = 4.45718 avg_loss = 3.16729\n",
      "epoch no.4 train no.336610  loss = 2.70660 avg_loss = 3.18686\n",
      "epoch no.4 train no.336620  loss = 3.92608 avg_loss = 3.17604\n",
      "epoch no.4 train no.336630  loss = 3.20461 avg_loss = 3.17259\n",
      "epoch no.4 train no.336640  loss = 5.07427 avg_loss = 3.18905\n",
      "epoch no.4 train no.336650  loss = 2.51003 avg_loss = 3.17382\n",
      "epoch no.4 train no.336660  loss = 5.24982 avg_loss = 3.16874\n",
      "epoch no.4 train no.336670  loss = 1.90771 avg_loss = 3.17718\n",
      "epoch no.4 train no.336680  loss = 3.40943 avg_loss = 3.19847\n",
      "epoch no.4 train no.336690  loss = 1.62621 avg_loss = 3.18619\n",
      "epoch no.4 train no.336700  loss = 2.77897 avg_loss = 3.11125\n",
      "epoch no.4 train no.336710  loss = 4.14037 avg_loss = 3.14575\n",
      "epoch no.4 train no.336720  loss = 2.13167 avg_loss = 3.14900\n",
      "epoch no.4 train no.336730  loss = 3.90862 avg_loss = 3.13924\n",
      "epoch no.4 train no.336740  loss = 4.57942 avg_loss = 3.12128\n",
      "epoch no.4 train no.336750  loss = 3.76307 avg_loss = 3.18034\n",
      "epoch no.4 train no.336760  loss = 4.71791 avg_loss = 3.23983\n",
      "epoch no.4 train no.336770  loss = 2.58354 avg_loss = 3.27324\n",
      "epoch no.4 train no.336780  loss = 3.73893 avg_loss = 3.25528\n",
      "epoch no.4 train no.336790  loss = 2.29255 avg_loss = 3.25234\n",
      "epoch no.4 train no.336800  loss = 2.67956 avg_loss = 3.28725\n",
      "epoch no.4 train no.336810  loss = 3.55448 avg_loss = 3.33288\n",
      "epoch no.4 train no.336820  loss = 3.25214 avg_loss = 3.31862\n",
      "epoch no.4 train no.336830  loss = 2.14100 avg_loss = 3.29012\n",
      "epoch no.4 train no.336840  loss = 3.39213 avg_loss = 3.31729\n",
      "epoch no.4 train no.336850  loss = 2.66223 avg_loss = 3.33175\n",
      "epoch no.4 train no.336860  loss = 2.77862 avg_loss = 3.30392\n",
      "epoch no.4 train no.336870  loss = 2.82499 avg_loss = 3.30899\n",
      "epoch no.4 train no.336880  loss = 2.59727 avg_loss = 3.31344\n",
      "epoch no.4 train no.336890  loss = 2.81825 avg_loss = 3.30048\n",
      "epoch no.4 train no.336900  loss = 4.10388 avg_loss = 3.28096\n",
      "epoch no.4 train no.336910  loss = 1.87718 avg_loss = 3.25127\n",
      "epoch no.4 train no.336920  loss = 3.31791 avg_loss = 3.24703\n",
      "epoch no.4 train no.336930  loss = 3.97346 avg_loss = 3.25811\n",
      "epoch no.4 train no.336940  loss = 2.93807 avg_loss = 3.25971\n",
      "epoch no.4 train no.336950  loss = 3.15034 avg_loss = 3.26007\n",
      "epoch no.4 train no.336960  loss = 2.65665 avg_loss = 3.28905\n",
      "epoch no.4 train no.336970  loss = 4.93031 avg_loss = 3.35196\n",
      "epoch no.4 train no.336980  loss = 2.17699 avg_loss = 3.32247\n",
      "epoch no.4 train no.336990  loss = 4.01040 avg_loss = 3.37867\n",
      "epoch no.4 train no.337000  loss = 2.39266 avg_loss = 3.35445\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋아', '하기', '▁좋은', '▁음악', '</s>']\n",
      "기분전환에 좋은 노래</s>\n",
      "epoch no.4 train no.337010  loss = 2.17184 avg_loss = 3.34912\n",
      "epoch no.4 train no.337020  loss = 3.39437 avg_loss = 3.30145\n",
      "epoch no.4 train no.337030  loss = 3.84598 avg_loss = 3.28832\n",
      "epoch no.4 train no.337040  loss = 2.44469 avg_loss = 3.31393\n",
      "epoch no.4 train no.337050  loss = 2.97076 avg_loss = 3.32805\n",
      "epoch no.4 train no.337060  loss = 3.73001 avg_loss = 3.33023\n",
      "epoch no.4 train no.337070  loss = 3.63733 avg_loss = 3.33319\n",
      "epoch no.4 train no.337080  loss = 3.14188 avg_loss = 3.31026\n",
      "epoch no.4 train no.337090  loss = 3.82999 avg_loss = 3.28183\n",
      "epoch no.4 train no.337100  loss = 2.30192 avg_loss = 3.23757\n",
      "epoch no.4 train no.337110  loss = 2.54667 avg_loss = 3.24076\n",
      "epoch no.4 train no.337120  loss = 1.72194 avg_loss = 3.22676\n",
      "epoch no.4 train no.337130  loss = 3.00772 avg_loss = 3.21644\n",
      "epoch no.4 train no.337140  loss = 4.35558 avg_loss = 3.23800\n",
      "epoch no.4 train no.337150  loss = 2.99568 avg_loss = 3.27153\n",
      "epoch no.4 train no.337160  loss = 4.60595 avg_loss = 3.23616\n",
      "epoch no.4 train no.337170  loss = 2.96455 avg_loss = 3.20292\n",
      "epoch no.4 train no.337180  loss = 3.00096 avg_loss = 3.20832\n",
      "epoch no.4 train no.337190  loss = 3.33836 avg_loss = 3.28078\n",
      "epoch no.4 train no.337200  loss = 2.60844 avg_loss = 3.22725\n",
      "epoch no.4 train no.337210  loss = 2.89260 avg_loss = 3.20839\n",
      "epoch no.4 train no.337220  loss = 3.96571 avg_loss = 3.25616\n",
      "epoch no.4 train no.337230  loss = 4.60685 avg_loss = 3.27093\n",
      "epoch no.4 train no.337240  loss = 2.20698 avg_loss = 3.25460\n",
      "epoch no.4 train no.337250  loss = 2.54546 avg_loss = 3.18616\n",
      "epoch no.4 train no.337260  loss = 4.08297 avg_loss = 3.19569\n",
      "epoch no.4 train no.337270  loss = 2.72858 avg_loss = 3.22418\n",
      "epoch no.4 train no.337280  loss = 2.45709 avg_loss = 3.18840\n",
      "epoch no.4 train no.337290  loss = 2.54561 avg_loss = 3.16256\n",
      "epoch no.4 train no.337300  loss = 4.78360 avg_loss = 3.17935\n",
      "epoch no.4 train no.337310  loss = 3.10841 avg_loss = 3.18737\n",
      "epoch no.4 train no.337320  loss = 7.32143 avg_loss = 3.22468\n",
      "epoch no.4 train no.337330  loss = 4.49908 avg_loss = 3.25575\n",
      "epoch no.4 train no.337340  loss = 3.01728 avg_loss = 3.24386\n",
      "epoch no.4 train no.337350  loss = 4.33873 avg_loss = 3.26572\n",
      "epoch no.4 train no.337360  loss = 3.79769 avg_loss = 3.28168\n",
      "epoch no.4 train no.337370  loss = 2.76841 avg_loss = 3.25922\n",
      "epoch no.4 train no.337380  loss = 2.77913 avg_loss = 3.24180\n",
      "epoch no.4 train no.337390  loss = 4.21873 avg_loss = 3.25744\n",
      "epoch no.4 train no.337400  loss = 1.10472 avg_loss = 3.18738\n",
      "epoch no.4 train no.337410  loss = 2.80262 avg_loss = 3.19210\n",
      "epoch no.4 train no.337420  loss = 2.34588 avg_loss = 3.25745\n",
      "epoch no.4 train no.337430  loss = 3.41250 avg_loss = 3.24835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.337440  loss = 4.45439 avg_loss = 3.22622\n",
      "epoch no.4 train no.337450  loss = 3.96393 avg_loss = 3.24817\n",
      "epoch no.4 train no.337460  loss = 2.07184 avg_loss = 3.20625\n",
      "epoch no.4 train no.337470  loss = 2.41369 avg_loss = 3.17938\n",
      "epoch no.4 train no.337480  loss = 3.43690 avg_loss = 3.17866\n",
      "epoch no.4 train no.337490  loss = 2.12478 avg_loss = 3.18241\n",
      "epoch no.4 train no.337500  loss = 2.64322 avg_loss = 3.20175\n",
      "epoch no.4 train no.337510  loss = 1.30962 avg_loss = 3.19567\n",
      "epoch no.4 train no.337520  loss = 4.04662 avg_loss = 3.21004\n",
      "epoch no.4 train no.337530  loss = 2.79976 avg_loss = 3.18725\n",
      "epoch no.4 train no.337540  loss = 2.36404 avg_loss = 3.16926\n",
      "epoch no.4 train no.337550  loss = 3.47678 avg_loss = 3.20447\n",
      "epoch no.4 train no.337560  loss = 3.03127 avg_loss = 3.21091\n",
      "epoch no.4 train no.337570  loss = 1.50368 avg_loss = 3.14142\n",
      "epoch no.4 train no.337580  loss = 4.89797 avg_loss = 3.14562\n",
      "epoch no.4 train no.337590  loss = 4.70183 avg_loss = 3.20028\n",
      "epoch no.4 train no.337600  loss = 1.97152 avg_loss = 3.23870\n",
      "epoch no.4 train no.337610  loss = 3.34092 avg_loss = 3.26518\n",
      "epoch no.4 train no.337620  loss = 2.84274 avg_loss = 3.24852\n",
      "epoch no.4 train no.337630  loss = 2.95641 avg_loss = 3.24685\n",
      "epoch no.4 train no.337640  loss = 3.89673 avg_loss = 3.26545\n",
      "epoch no.4 train no.337650  loss = 2.74308 avg_loss = 3.27463\n",
      "epoch no.4 train no.337660  loss = 2.51975 avg_loss = 3.20075\n",
      "epoch no.4 train no.337670  loss = 3.16192 avg_loss = 3.19711\n",
      "epoch no.4 train no.337680  loss = 3.22065 avg_loss = 3.16765\n",
      "epoch no.4 train no.337690  loss = 4.60205 avg_loss = 3.21468\n",
      "epoch no.4 train no.337700  loss = 3.57548 avg_loss = 3.15950\n",
      "epoch no.4 train no.337710  loss = 3.54563 avg_loss = 3.13625\n",
      "epoch no.4 train no.337720  loss = 4.24077 avg_loss = 3.18514\n",
      "epoch no.4 train no.337730  loss = 3.74902 avg_loss = 3.22478\n",
      "epoch no.4 train no.337740  loss = 2.17527 avg_loss = 3.21210\n",
      "epoch no.4 train no.337750  loss = 4.10943 avg_loss = 3.21674\n",
      "epoch no.4 train no.337760  loss = 2.29787 avg_loss = 3.18343\n",
      "epoch no.4 train no.337770  loss = 4.10315 avg_loss = 3.20164\n",
      "epoch no.4 train no.337780  loss = 2.59793 avg_loss = 3.20877\n",
      "epoch no.4 train no.337790  loss = 1.98627 avg_loss = 3.20817\n",
      "epoch no.4 train no.337800  loss = 4.21645 avg_loss = 3.19989\n",
      "epoch no.4 train no.337810  loss = 3.77980 avg_loss = 3.18713\n",
      "epoch no.4 train no.337820  loss = 2.49615 avg_loss = 3.17812\n",
      "epoch no.4 train no.337830  loss = 3.00783 avg_loss = 3.22193\n",
      "epoch no.4 train no.337840  loss = 1.89744 avg_loss = 3.20716\n",
      "epoch no.4 train no.337850  loss = 3.62945 avg_loss = 3.22434\n",
      "epoch no.4 train no.337860  loss = 3.45540 avg_loss = 3.20804\n",
      "epoch no.4 train no.337870  loss = 2.08955 avg_loss = 3.18666\n",
      "epoch no.4 train no.337880  loss = 4.04943 avg_loss = 3.19750\n",
      "epoch no.4 train no.337890  loss = 4.72215 avg_loss = 3.26129\n",
      "epoch no.4 train no.337900  loss = 3.04961 avg_loss = 3.27635\n",
      "epoch no.4 train no.337910  loss = 3.64298 avg_loss = 3.28168\n",
      "epoch no.4 train no.337920  loss = 2.89772 avg_loss = 3.27994\n",
      "epoch no.4 train no.337930  loss = 3.45820 avg_loss = 3.28101\n",
      "epoch no.4 train no.337940  loss = 1.96065 avg_loss = 3.25842\n",
      "epoch no.4 train no.337950  loss = 2.63671 avg_loss = 3.25057\n",
      "epoch no.4 train no.337960  loss = 4.35196 avg_loss = 3.27183\n",
      "epoch no.4 train no.337970  loss = 3.79314 avg_loss = 3.30166\n",
      "epoch no.4 train no.337980  loss = 3.51434 avg_loss = 3.28174\n",
      "epoch no.4 train no.337990  loss = 3.14765 avg_loss = 3.26709\n",
      "epoch no.4 train no.338000  loss = 3.76179 avg_loss = 3.24971\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁싶을', '▁때', '</s>', '▁노래', '▁음악', '</s>']\n",
      "기분전환 하고 싶을때 듣는 신나는 음악</s>\n",
      "epoch no.4 train no.338010  loss = 2.46061 avg_loss = 3.21403\n",
      "epoch no.4 train no.338020  loss = 3.03500 avg_loss = 3.20335\n",
      "epoch no.4 train no.338030  loss = 3.34802 avg_loss = 3.19227\n",
      "epoch no.4 train no.338040  loss = 2.91393 avg_loss = 3.18486\n",
      "epoch no.4 train no.338050  loss = 3.09362 avg_loss = 3.19450\n",
      "epoch no.4 train no.338060  loss = 4.32227 avg_loss = 3.19787\n",
      "epoch no.4 train no.338070  loss = 2.92080 avg_loss = 3.19246\n",
      "epoch no.4 train no.338080  loss = 3.53114 avg_loss = 3.20297\n",
      "epoch no.4 train no.338090  loss = 2.59527 avg_loss = 3.17405\n",
      "epoch no.4 train no.338100  loss = 3.08648 avg_loss = 3.16174\n",
      "epoch no.4 train no.338110  loss = 3.16273 avg_loss = 3.18746\n",
      "epoch no.4 train no.338120  loss = 2.77868 avg_loss = 3.17434\n",
      "epoch no.4 train no.338130  loss = 3.85475 avg_loss = 3.14839\n",
      "epoch no.4 train no.338140  loss = 2.59145 avg_loss = 3.15530\n",
      "epoch no.4 train no.338150  loss = 2.97476 avg_loss = 3.12318\n",
      "epoch no.4 train no.338160  loss = 3.00255 avg_loss = 3.08885\n",
      "epoch no.4 train no.338170  loss = 4.49933 avg_loss = 3.14071\n",
      "epoch no.4 train no.338180  loss = 2.24525 avg_loss = 3.16877\n",
      "epoch no.4 train no.338190  loss = 2.81401 avg_loss = 3.17015\n",
      "epoch no.4 train no.338200  loss = 2.86312 avg_loss = 3.16496\n",
      "epoch no.4 train no.338210  loss = 4.05009 avg_loss = 3.18827\n",
      "epoch no.4 train no.338220  loss = 2.98109 avg_loss = 3.16013\n",
      "epoch no.4 train no.338230  loss = 4.83409 avg_loss = 3.20370\n",
      "epoch no.4 train no.338240  loss = 4.11066 avg_loss = 3.21573\n",
      "epoch no.4 train no.338250  loss = 3.36684 avg_loss = 3.22166\n",
      "epoch no.4 train no.338260  loss = 1.71103 avg_loss = 3.17757\n",
      "epoch no.4 train no.338270  loss = 4.08920 avg_loss = 3.18869\n",
      "epoch no.4 train no.338280  loss = 2.89033 avg_loss = 3.19592\n",
      "epoch no.4 train no.338290  loss = 2.72446 avg_loss = 3.19239\n",
      "epoch no.4 train no.338300  loss = 1.62935 avg_loss = 3.18194\n",
      "epoch no.4 train no.338310  loss = 4.50822 avg_loss = 3.20751\n",
      "epoch no.4 train no.338320  loss = 2.81091 avg_loss = 3.22855\n",
      "epoch no.4 train no.338330  loss = 1.90096 avg_loss = 3.19383\n",
      "epoch no.4 train no.338340  loss = 2.85393 avg_loss = 3.16836\n",
      "epoch no.4 train no.338350  loss = 2.56665 avg_loss = 3.19194\n",
      "epoch no.4 train no.338360  loss = 2.67365 avg_loss = 3.22341\n",
      "epoch no.4 train no.338370  loss = 3.66189 avg_loss = 3.21092\n",
      "epoch no.4 train no.338380  loss = 2.58562 avg_loss = 3.24198\n",
      "epoch no.4 train no.338390  loss = 3.04046 avg_loss = 3.26415\n",
      "epoch no.4 train no.338400  loss = 6.14245 avg_loss = 3.26813\n",
      "epoch no.4 train no.338410  loss = 3.19511 avg_loss = 3.27383\n",
      "epoch no.4 train no.338420  loss = 2.65691 avg_loss = 3.25015\n",
      "epoch no.4 train no.338430  loss = 2.65140 avg_loss = 3.21576\n",
      "epoch no.4 train no.338440  loss = 3.06592 avg_loss = 3.20498\n",
      "epoch no.4 train no.338450  loss = 2.51052 avg_loss = 3.22912\n",
      "epoch no.4 train no.338460  loss = 7.19480 avg_loss = 3.24298\n",
      "epoch no.4 train no.338470  loss = 4.32890 avg_loss = 3.26680\n",
      "epoch no.4 train no.338480  loss = 3.56663 avg_loss = 3.25996\n",
      "epoch no.4 train no.338490  loss = 3.30137 avg_loss = 3.30080\n",
      "epoch no.4 train no.338500  loss = 3.96893 avg_loss = 3.28439\n",
      "epoch no.4 train no.338510  loss = 3.41570 avg_loss = 3.28760\n",
      "epoch no.4 train no.338520  loss = 3.72546 avg_loss = 3.29949\n",
      "epoch no.4 train no.338530  loss = 2.51128 avg_loss = 3.25085\n",
      "epoch no.4 train no.338540  loss = 2.89327 avg_loss = 3.28128\n",
      "epoch no.4 train no.338550  loss = 1.99918 avg_loss = 3.27997\n",
      "epoch no.4 train no.338560  loss = 2.23630 avg_loss = 3.23559\n",
      "epoch no.4 train no.338570  loss = 3.42399 avg_loss = 3.25912\n",
      "epoch no.4 train no.338580  loss = 2.80970 avg_loss = 3.25005\n",
      "epoch no.4 train no.338590  loss = 3.04931 avg_loss = 3.27205\n",
      "epoch no.4 train no.338600  loss = 2.83232 avg_loss = 3.22987\n",
      "epoch no.4 train no.338610  loss = 4.66765 avg_loss = 3.22581\n",
      "epoch no.4 train no.338620  loss = 3.14206 avg_loss = 3.19340\n",
      "epoch no.4 train no.338630  loss = 2.68175 avg_loss = 3.17365\n",
      "epoch no.4 train no.338640  loss = 4.50828 avg_loss = 3.19833\n",
      "epoch no.4 train no.338650  loss = 2.76469 avg_loss = 3.18446\n",
      "epoch no.4 train no.338660  loss = 2.21025 avg_loss = 3.19515\n",
      "epoch no.4 train no.338670  loss = 3.24348 avg_loss = 3.19578\n",
      "epoch no.4 train no.338680  loss = 2.22621 avg_loss = 3.14674\n",
      "epoch no.4 train no.338690  loss = 2.49020 avg_loss = 3.14852\n",
      "epoch no.4 train no.338700  loss = 2.83131 avg_loss = 3.11421\n",
      "epoch no.4 train no.338710  loss = 3.45822 avg_loss = 3.17429\n",
      "epoch no.4 train no.338720  loss = 3.56840 avg_loss = 3.18531\n",
      "epoch no.4 train no.338730  loss = 4.07464 avg_loss = 3.17682\n",
      "epoch no.4 train no.338740  loss = 1.76791 avg_loss = 3.10486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.338750  loss = 3.62183 avg_loss = 3.11445\n",
      "epoch no.4 train no.338760  loss = 3.68250 avg_loss = 3.14206\n",
      "epoch no.4 train no.338770  loss = 2.14801 avg_loss = 3.11611\n",
      "epoch no.4 train no.338780  loss = 3.13717 avg_loss = 3.12219\n",
      "epoch no.4 train no.338790  loss = 3.28520 avg_loss = 3.10441\n",
      "epoch no.4 train no.338800  loss = 2.32980 avg_loss = 3.08250\n",
      "epoch no.4 train no.338810  loss = 3.28237 avg_loss = 3.08613\n",
      "epoch no.4 train no.338820  loss = 2.75407 avg_loss = 3.09737\n",
      "epoch no.4 train no.338830  loss = 4.44412 avg_loss = 3.14734\n",
      "epoch no.4 train no.338840  loss = 2.69709 avg_loss = 3.14996\n",
      "epoch no.4 train no.338850  loss = 3.69212 avg_loss = 3.16648\n",
      "epoch no.4 train no.338860  loss = 2.96954 avg_loss = 3.15625\n",
      "epoch no.4 train no.338870  loss = 2.78200 avg_loss = 3.17334\n",
      "epoch no.4 train no.338880  loss = 3.25177 avg_loss = 3.19261\n",
      "epoch no.4 train no.338890  loss = 4.05794 avg_loss = 3.18243\n",
      "epoch no.4 train no.338900  loss = 4.07501 avg_loss = 3.20798\n",
      "epoch no.4 train no.338910  loss = 2.57628 avg_loss = 3.20419\n",
      "epoch no.4 train no.338920  loss = 3.99728 avg_loss = 3.21566\n",
      "epoch no.4 train no.338930  loss = 2.68493 avg_loss = 3.23616\n",
      "epoch no.4 train no.338940  loss = 4.24350 avg_loss = 3.26996\n",
      "epoch no.4 train no.338950  loss = 3.47268 avg_loss = 3.27720\n",
      "epoch no.4 train no.338960  loss = 2.04455 avg_loss = 3.24654\n",
      "epoch no.4 train no.338970  loss = 1.88296 avg_loss = 3.23158\n",
      "epoch no.4 train no.338980  loss = 3.06548 avg_loss = 3.25212\n",
      "epoch no.4 train no.338990  loss = 2.95199 avg_loss = 3.25201\n",
      "epoch no.4 train no.339000  loss = 3.90603 avg_loss = 3.26058\n",
      "7\n",
      "to_tokens: ['▁라디오', '좋은', '하기', '▁최고', '인', '겨운', '▁팝', '▁노래', '▁모음', '</s>']\n",
      "기분전환에 최고 흥나는 아이돌 노래 모음</s>\n",
      "epoch no.4 train no.339010  loss = 3.03378 avg_loss = 3.23932\n",
      "epoch no.4 train no.339020  loss = 1.76467 avg_loss = 3.24047\n",
      "epoch no.4 train no.339030  loss = 2.09974 avg_loss = 3.21212\n",
      "epoch no.4 train no.339040  loss = 3.57822 avg_loss = 3.22871\n",
      "epoch no.4 train no.339050  loss = 3.50207 avg_loss = 3.26230\n",
      "epoch no.4 train no.339060  loss = 2.25882 avg_loss = 3.29813\n",
      "epoch no.4 train no.339070  loss = 3.61718 avg_loss = 3.29917\n",
      "epoch no.4 train no.339080  loss = 3.16779 avg_loss = 3.29992\n",
      "epoch no.4 train no.339090  loss = 3.05396 avg_loss = 3.25715\n",
      "epoch no.4 train no.339100  loss = 2.48209 avg_loss = 3.27045\n",
      "epoch no.4 train no.339110  loss = 2.59687 avg_loss = 3.27010\n",
      "epoch no.4 train no.339120  loss = 3.58791 avg_loss = 3.25381\n",
      "epoch no.4 train no.339130  loss = 4.45931 avg_loss = 3.24516\n",
      "epoch no.4 train no.339140  loss = 3.28076 avg_loss = 3.23916\n",
      "epoch no.4 train no.339150  loss = 3.67514 avg_loss = 3.23954\n",
      "epoch no.4 train no.339160  loss = 3.62670 avg_loss = 3.22909\n",
      "epoch no.4 train no.339170  loss = 3.12904 avg_loss = 3.23104\n",
      "epoch no.4 train no.339180  loss = 2.25531 avg_loss = 3.20628\n",
      "epoch no.4 train no.339190  loss = 2.98333 avg_loss = 3.20123\n",
      "epoch no.4 train no.339200  loss = 1.70071 avg_loss = 3.16964\n",
      "epoch no.4 train no.339210  loss = 2.17807 avg_loss = 3.20600\n",
      "epoch no.4 train no.339220  loss = 2.78139 avg_loss = 3.28106\n",
      "epoch no.4 train no.339230  loss = 3.28082 avg_loss = 3.25898\n",
      "epoch no.4 train no.339240  loss = 3.98547 avg_loss = 3.28975\n",
      "epoch no.4 train no.339250  loss = 2.53389 avg_loss = 3.27546\n",
      "epoch no.4 train no.339260  loss = 3.34201 avg_loss = 3.27305\n",
      "epoch no.4 train no.339270  loss = 3.28922 avg_loss = 3.29141\n",
      "epoch no.4 train no.339280  loss = 2.84062 avg_loss = 3.30495\n",
      "epoch no.4 train no.339290  loss = 4.92266 avg_loss = 3.32361\n",
      "epoch no.4 train no.339300  loss = 3.23025 avg_loss = 3.33117\n",
      "epoch no.4 train no.339310  loss = 4.02841 avg_loss = 3.33903\n",
      "epoch no.4 train no.339320  loss = 3.99667 avg_loss = 3.34616\n",
      "epoch no.4 train no.339330  loss = 3.18754 avg_loss = 3.32090\n",
      "epoch no.4 train no.339340  loss = 2.96613 avg_loss = 3.26880\n",
      "epoch no.4 train no.339350  loss = 3.72104 avg_loss = 3.25438\n",
      "epoch no.4 train no.339360  loss = 2.80552 avg_loss = 3.28410\n",
      "epoch no.4 train no.339370  loss = 2.71848 avg_loss = 3.27387\n",
      "epoch no.4 train no.339380  loss = 3.11308 avg_loss = 3.27452\n",
      "epoch no.4 train no.339390  loss = 4.08690 avg_loss = 3.26765\n",
      "epoch no.4 train no.339400  loss = 3.44463 avg_loss = 3.28323\n",
      "epoch no.4 train no.339410  loss = 4.02513 avg_loss = 3.29315\n",
      "epoch no.4 train no.339420  loss = 2.97887 avg_loss = 3.25746\n",
      "epoch no.4 train no.339430  loss = 3.03662 avg_loss = 3.20934\n",
      "epoch no.4 train no.339440  loss = 3.27222 avg_loss = 3.23736\n",
      "epoch no.4 train no.339450  loss = 2.40662 avg_loss = 3.21847\n",
      "epoch no.4 train no.339460  loss = 1.92297 avg_loss = 3.24055\n",
      "epoch no.4 train no.339470  loss = 2.49219 avg_loss = 3.22268\n",
      "epoch no.4 train no.339480  loss = 3.57140 avg_loss = 3.22516\n",
      "epoch no.4 train no.339490  loss = 2.31158 avg_loss = 3.23406\n",
      "epoch no.4 train no.339500  loss = 3.28425 avg_loss = 3.24032\n",
      "epoch no.4 train no.339510  loss = 2.91251 avg_loss = 3.25191\n",
      "epoch no.4 train no.339520  loss = 4.31044 avg_loss = 3.27436\n",
      "epoch no.4 train no.339530  loss = 3.45843 avg_loss = 3.23417\n",
      "epoch no.4 train no.339540  loss = 3.38390 avg_loss = 3.22250\n",
      "epoch no.4 train no.339550  loss = 2.73773 avg_loss = 3.20148\n",
      "epoch no.4 train no.339560  loss = 3.27258 avg_loss = 3.20550\n",
      "epoch no.4 train no.339570  loss = 3.02791 avg_loss = 3.15522\n",
      "epoch no.4 train no.339580  loss = 3.87216 avg_loss = 3.18956\n",
      "epoch no.4 train no.339590  loss = 4.99119 avg_loss = 3.21658\n",
      "epoch no.4 train no.339600  loss = 3.31923 avg_loss = 3.25608\n",
      "epoch no.4 train no.339610  loss = 4.77796 avg_loss = 3.24930\n",
      "epoch no.4 train no.339620  loss = 3.04228 avg_loss = 3.20877\n",
      "epoch no.4 train no.339630  loss = 1.53487 avg_loss = 3.18563\n",
      "epoch no.4 train no.339640  loss = 2.22477 avg_loss = 3.16672\n",
      "epoch no.4 train no.339650  loss = 3.83597 avg_loss = 3.18085\n",
      "epoch no.4 train no.339660  loss = 3.22802 avg_loss = 3.18272\n",
      "epoch no.4 train no.339670  loss = 3.90541 avg_loss = 3.26941\n",
      "epoch no.4 train no.339680  loss = 3.65876 avg_loss = 3.23810\n",
      "epoch no.4 train no.339690  loss = 2.47438 avg_loss = 3.19988\n",
      "epoch no.4 train no.339700  loss = 4.84600 avg_loss = 3.18894\n",
      "epoch no.4 train no.339710  loss = 5.25822 avg_loss = 3.18325\n",
      "epoch no.4 train no.339720  loss = 3.47123 avg_loss = 3.18207\n",
      "epoch no.4 train no.339730  loss = 2.38933 avg_loss = 3.13935\n",
      "epoch no.4 train no.339740  loss = 2.45363 avg_loss = 3.14271\n",
      "epoch no.4 train no.339750  loss = 3.44268 avg_loss = 3.14559\n",
      "epoch no.4 train no.339760  loss = 3.10898 avg_loss = 3.13434\n",
      "epoch no.4 train no.339770  loss = 2.52885 avg_loss = 3.14897\n",
      "epoch no.4 train no.339780  loss = 2.67348 avg_loss = 3.16124\n",
      "epoch no.4 train no.339790  loss = 5.31725 avg_loss = 3.19376\n",
      "epoch no.4 train no.339800  loss = 2.19672 avg_loss = 3.18280\n",
      "epoch no.4 train no.339810  loss = 4.34124 avg_loss = 3.22380\n",
      "epoch no.4 train no.339820  loss = 2.83120 avg_loss = 3.23221\n",
      "epoch no.4 train no.339830  loss = 2.78692 avg_loss = 3.23556\n",
      "epoch no.4 train no.339840  loss = 1.84743 avg_loss = 3.22663\n",
      "epoch no.4 train no.339850  loss = 2.03101 avg_loss = 3.19795\n",
      "epoch no.4 train no.339860  loss = 3.31034 avg_loss = 3.19625\n",
      "epoch no.4 train no.339870  loss = 3.77546 avg_loss = 3.25293\n",
      "epoch no.4 train no.339880  loss = 2.64159 avg_loss = 3.24019\n",
      "epoch no.4 train no.339890  loss = 3.99321 avg_loss = 3.23871\n",
      "epoch no.4 train no.339900  loss = 3.21864 avg_loss = 3.25492\n",
      "epoch no.4 train no.339910  loss = 2.91345 avg_loss = 3.23723\n",
      "epoch no.4 train no.339920  loss = 1.96220 avg_loss = 3.21970\n",
      "epoch no.4 train no.339930  loss = 4.90769 avg_loss = 3.25313\n",
      "epoch no.4 train no.339940  loss = 5.48782 avg_loss = 3.24850\n",
      "epoch no.4 train no.339950  loss = 3.64747 avg_loss = 3.28543\n",
      "epoch no.4 train no.339960  loss = 5.17751 avg_loss = 3.31850\n",
      "epoch no.4 train no.339970  loss = 3.36937 avg_loss = 3.28312\n",
      "epoch no.4 train no.339980  loss = 5.18595 avg_loss = 3.31487\n",
      "epoch no.4 train no.339990  loss = 3.16570 avg_loss = 3.30936\n",
      "epoch no.4 train no.340000  loss = 4.43811 avg_loss = 3.30358\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '하기', '▁좋은', '▁음악', '한', '▁노래', '</s>', '</s>']\n",
      "기분전환하기 좋은 잔잔한 노래들</s>\n",
      "epoch no.4 train no.340010  loss = 3.60322 avg_loss = 3.31451\n",
      "epoch no.4 train no.340020  loss = 3.52557 avg_loss = 3.32046\n",
      "epoch no.4 train no.340030  loss = 2.44373 avg_loss = 3.29856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.340040  loss = 1.76454 avg_loss = 3.26737\n",
      "epoch no.4 train no.340050  loss = 2.19315 avg_loss = 3.24169\n",
      "epoch no.4 train no.340060  loss = 3.01269 avg_loss = 3.24210\n",
      "epoch no.4 train no.340070  loss = 4.45860 avg_loss = 3.27005\n",
      "epoch no.4 train no.340080  loss = 4.14075 avg_loss = 3.26343\n",
      "epoch no.4 train no.340090  loss = 4.17800 avg_loss = 3.30107\n",
      "epoch no.4 train no.340100  loss = 3.75670 avg_loss = 3.27354\n",
      "epoch no.4 train no.340110  loss = 4.50098 avg_loss = 3.32475\n",
      "epoch no.4 train no.340120  loss = 3.30699 avg_loss = 3.28750\n",
      "epoch no.4 train no.340130  loss = 2.78774 avg_loss = 3.27472\n",
      "epoch no.4 train no.340140  loss = 2.94836 avg_loss = 3.26291\n",
      "epoch no.4 train no.340150  loss = 3.02159 avg_loss = 3.22019\n",
      "epoch no.4 train no.340160  loss = 2.01535 avg_loss = 3.17975\n",
      "epoch no.4 train no.340170  loss = 3.85198 avg_loss = 3.15391\n",
      "epoch no.4 train no.340180  loss = 3.59858 avg_loss = 3.16320\n",
      "epoch no.4 train no.340190  loss = 3.02070 avg_loss = 3.23892\n",
      "epoch no.4 train no.340200  loss = 3.49864 avg_loss = 3.27637\n",
      "epoch no.4 train no.340210  loss = 3.62596 avg_loss = 3.27188\n",
      "epoch no.4 train no.340220  loss = 3.37672 avg_loss = 3.28018\n",
      "epoch no.4 train no.340230  loss = 4.37747 avg_loss = 3.31395\n",
      "epoch no.4 train no.340240  loss = 3.44455 avg_loss = 3.30056\n",
      "epoch no.4 train no.340250  loss = 4.69114 avg_loss = 3.32717\n",
      "epoch no.4 train no.340260  loss = 4.09268 avg_loss = 3.29297\n",
      "epoch no.4 train no.340270  loss = 2.65426 avg_loss = 3.27409\n",
      "epoch no.4 train no.340280  loss = 3.18887 avg_loss = 3.23983\n",
      "epoch no.4 train no.340290  loss = 2.35995 avg_loss = 3.22320\n",
      "epoch no.4 train no.340300  loss = 3.05779 avg_loss = 3.19490\n",
      "epoch no.4 train no.340310  loss = 2.34923 avg_loss = 3.19479\n",
      "epoch no.4 train no.340320  loss = 3.16613 avg_loss = 3.20034\n",
      "epoch no.4 train no.340330  loss = 2.54593 avg_loss = 3.20318\n",
      "epoch no.4 train no.340340  loss = 4.32945 avg_loss = 3.20348\n",
      "epoch no.4 train no.340350  loss = 3.05738 avg_loss = 3.23420\n",
      "epoch no.4 train no.340360  loss = 2.98062 avg_loss = 3.24638\n",
      "epoch no.4 train no.340370  loss = 2.14431 avg_loss = 3.18227\n",
      "epoch no.4 train no.340380  loss = 3.53965 avg_loss = 3.18037\n",
      "epoch no.4 train no.340390  loss = 4.03995 avg_loss = 3.18684\n",
      "epoch no.4 train no.340400  loss = 4.52906 avg_loss = 3.26327\n",
      "epoch no.4 train no.340410  loss = 3.25321 avg_loss = 3.23617\n",
      "epoch no.4 train no.340420  loss = 3.01122 avg_loss = 3.20111\n",
      "epoch no.4 train no.340430  loss = 2.81197 avg_loss = 3.24679\n",
      "epoch no.4 train no.340440  loss = 3.57153 avg_loss = 3.24558\n",
      "epoch no.4 train no.340450  loss = 3.52002 avg_loss = 3.22969\n",
      "epoch no.4 train no.340460  loss = 4.51808 avg_loss = 3.22463\n",
      "epoch no.4 train no.340470  loss = 2.36551 avg_loss = 3.20630\n",
      "epoch no.4 train no.340480  loss = 2.94960 avg_loss = 3.17137\n",
      "epoch no.4 train no.340490  loss = 2.61251 avg_loss = 3.16960\n",
      "epoch no.4 train no.340500  loss = 2.61838 avg_loss = 3.15048\n",
      "epoch no.4 train no.340510  loss = 3.03600 avg_loss = 3.21760\n",
      "epoch no.4 train no.340520  loss = 4.99476 avg_loss = 3.18318\n",
      "epoch no.4 train no.340530  loss = 4.60192 avg_loss = 3.15636\n",
      "epoch no.4 train no.340540  loss = 2.46554 avg_loss = 3.12584\n",
      "epoch no.4 train no.340550  loss = 4.43187 avg_loss = 3.17675\n",
      "epoch no.4 train no.340560  loss = 1.96928 avg_loss = 3.17250\n",
      "epoch no.4 train no.340570  loss = 3.99022 avg_loss = 3.18768\n",
      "epoch no.4 train no.340580  loss = 2.95359 avg_loss = 3.16647\n",
      "epoch no.4 train no.340590  loss = 3.82983 avg_loss = 3.15272\n",
      "epoch no.4 train no.340600  loss = 4.06050 avg_loss = 3.18436\n",
      "epoch no.4 train no.340610  loss = 3.62833 avg_loss = 3.21622\n",
      "epoch no.4 train no.340620  loss = 3.31606 avg_loss = 3.25218\n",
      "epoch no.4 train no.340630  loss = 2.57186 avg_loss = 3.23740\n",
      "epoch no.4 train no.340640  loss = 3.29654 avg_loss = 3.21318\n",
      "epoch no.4 train no.340650  loss = 2.50232 avg_loss = 3.18991\n",
      "epoch no.4 train no.340660  loss = 2.83724 avg_loss = 3.20850\n",
      "epoch no.4 train no.340670  loss = 5.05856 avg_loss = 3.22969\n",
      "epoch no.4 train no.340680  loss = 3.11235 avg_loss = 3.23690\n",
      "epoch no.4 train no.340690  loss = 2.35876 avg_loss = 3.20010\n",
      "epoch no.4 train no.340700  loss = 2.89220 avg_loss = 3.24736\n",
      "epoch no.4 train no.340710  loss = 3.83594 avg_loss = 3.23787\n",
      "epoch no.4 train no.340720  loss = 1.96743 avg_loss = 3.22659\n",
      "epoch no.4 train no.340730  loss = 4.70917 avg_loss = 3.23138\n",
      "epoch no.4 train no.340740  loss = 3.95259 avg_loss = 3.23879\n",
      "epoch no.4 train no.340750  loss = 3.59886 avg_loss = 3.27135\n",
      "epoch no.4 train no.340760  loss = 2.44087 avg_loss = 3.27988\n",
      "epoch no.4 train no.340770  loss = 3.04762 avg_loss = 3.26132\n",
      "epoch no.4 train no.340780  loss = 1.37172 avg_loss = 3.20913\n",
      "epoch no.4 train no.340790  loss = 2.50796 avg_loss = 3.20214\n",
      "epoch no.4 train no.340800  loss = 5.74654 avg_loss = 3.22577\n",
      "epoch no.4 train no.340810  loss = 5.53846 avg_loss = 3.27535\n",
      "epoch no.4 train no.340820  loss = 4.31199 avg_loss = 3.30930\n",
      "epoch no.4 train no.340830  loss = 4.16670 avg_loss = 3.33448\n",
      "epoch no.4 train no.340840  loss = 3.00054 avg_loss = 3.30810\n",
      "epoch no.4 train no.340850  loss = 2.59236 avg_loss = 3.31333\n",
      "epoch no.4 train no.340860  loss = 3.53279 avg_loss = 3.33194\n",
      "epoch no.4 train no.340870  loss = 2.97656 avg_loss = 3.33568\n",
      "epoch no.4 train no.340880  loss = 4.69031 avg_loss = 3.31399\n",
      "epoch no.4 train no.340890  loss = 3.00126 avg_loss = 3.30814\n",
      "epoch no.4 train no.340900  loss = 2.24466 avg_loss = 3.29025\n",
      "epoch no.4 train no.340910  loss = 4.96729 avg_loss = 3.28584\n",
      "epoch no.4 train no.340920  loss = 4.44134 avg_loss = 3.31213\n",
      "epoch no.4 train no.340930  loss = 3.45233 avg_loss = 3.32407\n",
      "epoch no.4 train no.340940  loss = 2.44187 avg_loss = 3.32278\n",
      "epoch no.4 train no.340950  loss = 3.10510 avg_loss = 3.29530\n",
      "epoch no.4 train no.340960  loss = 3.35814 avg_loss = 3.30955\n",
      "epoch no.4 train no.340970  loss = 2.23054 avg_loss = 3.28652\n",
      "epoch no.4 train no.340980  loss = 6.17039 avg_loss = 3.30531\n",
      "epoch no.4 train no.340990  loss = 2.51290 avg_loss = 3.28815\n",
      "epoch no.4 train no.341000  loss = 2.44413 avg_loss = 3.25677\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '▁신나는', '▁팝', '송']\n",
      "기분전환하기 좋은 신나는 팝</s>\n",
      "epoch no.4 train no.341010  loss = 2.34087 avg_loss = 3.25260\n",
      "epoch no.4 train no.341020  loss = 1.75828 avg_loss = 3.23430\n",
      "epoch no.4 train no.341030  loss = 4.29887 avg_loss = 3.21826\n",
      "epoch no.4 train no.341040  loss = 2.94923 avg_loss = 3.16656\n",
      "epoch no.4 train no.341050  loss = 3.39725 avg_loss = 3.23173\n",
      "epoch no.4 train no.341060  loss = 3.49093 avg_loss = 3.21754\n",
      "epoch no.4 train no.341070  loss = 2.62679 avg_loss = 3.17471\n",
      "epoch no.4 train no.341080  loss = 4.27480 avg_loss = 3.20783\n",
      "epoch no.4 train no.341090  loss = 3.62597 avg_loss = 3.20727\n",
      "epoch no.4 train no.341100  loss = 3.04445 avg_loss = 3.16884\n",
      "epoch no.4 train no.341110  loss = 2.03665 avg_loss = 3.21152\n",
      "epoch no.4 train no.341120  loss = 3.21313 avg_loss = 3.20683\n",
      "epoch no.4 train no.341130  loss = 3.43179 avg_loss = 3.30619\n",
      "epoch no.4 train no.341140  loss = 2.06495 avg_loss = 3.28370\n",
      "epoch no.4 train no.341150  loss = 3.70132 avg_loss = 3.28758\n",
      "epoch no.4 train no.341160  loss = 3.63706 avg_loss = 3.23328\n",
      "epoch no.4 train no.341170  loss = 1.68711 avg_loss = 3.21442\n",
      "epoch no.4 train no.341180  loss = 2.04593 avg_loss = 3.21909\n",
      "epoch no.4 train no.341190  loss = 3.20659 avg_loss = 3.21433\n",
      "epoch no.4 train no.341200  loss = 2.13428 avg_loss = 3.17572\n",
      "epoch no.4 train no.341210  loss = 3.05230 avg_loss = 3.18852\n",
      "epoch no.4 train no.341220  loss = 3.78863 avg_loss = 3.16169\n",
      "epoch no.4 train no.341230  loss = 4.13648 avg_loss = 3.19817\n",
      "epoch no.4 train no.341240  loss = 4.15445 avg_loss = 3.21269\n",
      "epoch no.4 train no.341250  loss = 1.74395 avg_loss = 3.21037\n",
      "epoch no.4 train no.341260  loss = 3.56087 avg_loss = 3.19671\n",
      "epoch no.4 train no.341270  loss = 1.59748 avg_loss = 3.12052\n",
      "epoch no.4 train no.341280  loss = 1.76075 avg_loss = 3.14366\n",
      "epoch no.4 train no.341290  loss = 2.61493 avg_loss = 3.13224\n",
      "epoch no.4 train no.341300  loss = 3.89037 avg_loss = 3.17346\n",
      "epoch no.4 train no.341310  loss = 2.45879 avg_loss = 3.17075\n",
      "epoch no.4 train no.341320  loss = 2.77952 avg_loss = 3.17366\n",
      "epoch no.4 train no.341330  loss = 5.69879 avg_loss = 3.18963\n",
      "epoch no.4 train no.341340  loss = 3.19495 avg_loss = 3.19073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.341350  loss = 4.04525 avg_loss = 3.19477\n",
      "epoch no.4 train no.341360  loss = 2.45215 avg_loss = 3.18247\n",
      "epoch no.4 train no.341370  loss = 5.50555 avg_loss = 3.25081\n",
      "epoch no.4 train no.341380  loss = 3.31911 avg_loss = 3.22152\n",
      "epoch no.4 train no.341390  loss = 2.20437 avg_loss = 3.19040\n",
      "epoch no.4 train no.341400  loss = 3.96215 avg_loss = 3.20265\n",
      "epoch no.4 train no.341410  loss = 2.10859 avg_loss = 3.19148\n",
      "epoch no.4 train no.341420  loss = 3.05482 avg_loss = 3.18364\n",
      "epoch no.4 train no.341430  loss = 2.58465 avg_loss = 3.16739\n",
      "epoch no.4 train no.341440  loss = 3.80534 avg_loss = 3.13955\n",
      "epoch no.4 train no.341450  loss = 2.99523 avg_loss = 3.10388\n",
      "epoch no.4 train no.341460  loss = 2.48199 avg_loss = 3.10527\n",
      "epoch no.4 train no.341470  loss = 3.82846 avg_loss = 3.10071\n",
      "epoch no.4 train no.341480  loss = 2.52893 avg_loss = 3.14110\n",
      "epoch no.4 train no.341490  loss = 1.97191 avg_loss = 3.10957\n",
      "epoch no.4 train no.341500  loss = 2.47162 avg_loss = 3.11271\n",
      "epoch no.4 train no.341510  loss = 2.07084 avg_loss = 3.11083\n",
      "epoch no.4 train no.341520  loss = 3.73581 avg_loss = 3.06898\n",
      "epoch no.4 train no.341530  loss = 6.42047 avg_loss = 3.13463\n",
      "epoch no.4 train no.341540  loss = 4.11646 avg_loss = 3.19254\n",
      "epoch no.4 train no.341550  loss = 3.91994 avg_loss = 3.20353\n",
      "epoch no.4 train no.341560  loss = 2.86373 avg_loss = 3.20681\n",
      "epoch no.4 train no.341570  loss = 2.72601 avg_loss = 3.22888\n",
      "epoch no.4 train no.341580  loss = 2.60882 avg_loss = 3.21805\n",
      "epoch no.4 train no.341590  loss = 3.46338 avg_loss = 3.20829\n",
      "epoch no.4 train no.341600  loss = 3.46018 avg_loss = 3.24748\n",
      "epoch no.4 train no.341610  loss = 4.13185 avg_loss = 3.24988\n",
      "epoch no.4 train no.341620  loss = 3.33598 avg_loss = 3.25401\n",
      "epoch no.4 train no.341630  loss = 1.55202 avg_loss = 3.22005\n",
      "epoch no.4 train no.341640  loss = 1.85684 avg_loss = 3.26530\n",
      "epoch no.4 train no.341650  loss = 2.83813 avg_loss = 3.29709\n",
      "epoch no.4 train no.341660  loss = 2.55210 avg_loss = 3.27378\n",
      "epoch no.4 train no.341670  loss = 1.81223 avg_loss = 3.23239\n",
      "epoch no.4 train no.341680  loss = 3.84475 avg_loss = 3.21305\n",
      "epoch no.4 train no.341690  loss = 2.37324 avg_loss = 3.20438\n",
      "epoch no.4 train no.341700  loss = 2.37744 avg_loss = 3.22461\n",
      "epoch no.4 train no.341710  loss = 2.21073 avg_loss = 3.24558\n",
      "epoch no.4 train no.341720  loss = 4.20525 avg_loss = 3.27288\n",
      "epoch no.4 train no.341730  loss = 3.77927 avg_loss = 3.25880\n",
      "epoch no.4 train no.341740  loss = 2.66717 avg_loss = 3.27189\n",
      "epoch no.4 train no.341750  loss = 3.28538 avg_loss = 3.24593\n",
      "epoch no.4 train no.341760  loss = 4.20079 avg_loss = 3.27774\n",
      "epoch no.4 train no.341770  loss = 2.26504 avg_loss = 3.28151\n",
      "epoch no.4 train no.341780  loss = 3.74234 avg_loss = 3.27285\n",
      "epoch no.4 train no.341790  loss = 3.79078 avg_loss = 3.24584\n",
      "epoch no.4 train no.341800  loss = 2.57598 avg_loss = 3.23652\n",
      "epoch no.4 train no.341810  loss = 3.50735 avg_loss = 3.25093\n",
      "epoch no.4 train no.341820  loss = 2.93888 avg_loss = 3.26705\n",
      "epoch no.4 train no.341830  loss = 3.97008 avg_loss = 3.28890\n",
      "epoch no.4 train no.341840  loss = 3.87104 avg_loss = 3.25624\n",
      "epoch no.4 train no.341850  loss = 4.02523 avg_loss = 3.20980\n",
      "epoch no.4 train no.341860  loss = 4.17691 avg_loss = 3.22459\n",
      "epoch no.4 train no.341870  loss = 3.80894 avg_loss = 3.24470\n",
      "epoch no.4 train no.341880  loss = 3.14185 avg_loss = 3.22909\n",
      "epoch no.4 train no.341890  loss = 2.20468 avg_loss = 3.24028\n",
      "epoch no.4 train no.341900  loss = 4.15076 avg_loss = 3.26788\n",
      "epoch no.4 train no.341910  loss = 4.78837 avg_loss = 3.28169\n",
      "epoch no.4 train no.341920  loss = 3.21170 avg_loss = 3.24273\n",
      "epoch no.4 train no.341930  loss = 4.12223 avg_loss = 3.22176\n",
      "epoch no.4 train no.341940  loss = 3.26623 avg_loss = 3.19962\n",
      "epoch no.4 train no.341950  loss = 3.60819 avg_loss = 3.22484\n",
      "epoch no.4 train no.341960  loss = 2.59871 avg_loss = 3.18936\n",
      "epoch no.4 train no.341970  loss = 2.35463 avg_loss = 3.13647\n",
      "epoch no.4 train no.341980  loss = 2.59796 avg_loss = 3.14662\n",
      "epoch no.4 train no.341990  loss = 2.67019 avg_loss = 3.12947\n",
      "epoch no.4 train no.342000  loss = 3.20435 avg_loss = 3.08102\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁흥', '▁p', 'op', '</s>']\n",
      "기분전환에 딱 신나는 pop</s>\n",
      "epoch no.4 train no.342010  loss = 4.77023 avg_loss = 3.09780\n",
      "epoch no.4 train no.342020  loss = 3.16835 avg_loss = 3.13440\n",
      "epoch no.4 train no.342030  loss = 2.63534 avg_loss = 3.12065\n",
      "epoch no.4 train no.342040  loss = 3.55629 avg_loss = 3.14854\n",
      "epoch no.4 train no.342050  loss = 3.79861 avg_loss = 3.13861\n",
      "epoch no.4 train no.342060  loss = 2.05456 avg_loss = 3.13471\n",
      "epoch no.4 train no.342070  loss = 3.00790 avg_loss = 3.18254\n",
      "epoch no.4 train no.342080  loss = 2.74302 avg_loss = 3.17841\n",
      "epoch no.4 train no.342090  loss = 4.48843 avg_loss = 3.18733\n",
      "epoch no.4 train no.342100  loss = 2.17811 avg_loss = 3.15862\n",
      "epoch no.4 train no.342110  loss = 4.80422 avg_loss = 3.19949\n",
      "epoch no.4 train no.342120  loss = 3.33292 avg_loss = 3.20233\n",
      "epoch no.4 train no.342130  loss = 4.28600 avg_loss = 3.18666\n",
      "epoch no.4 train no.342140  loss = 3.47813 avg_loss = 3.19443\n",
      "epoch no.4 train no.342150  loss = 2.97415 avg_loss = 3.22679\n",
      "epoch no.4 train no.342160  loss = 4.14965 avg_loss = 3.19583\n",
      "epoch no.4 train no.342170  loss = 3.50894 avg_loss = 3.21797\n",
      "epoch no.4 train no.342180  loss = 2.45101 avg_loss = 3.21085\n",
      "epoch no.4 train no.342190  loss = 3.61288 avg_loss = 3.21874\n",
      "epoch no.4 train no.342200  loss = 4.03666 avg_loss = 3.22029\n",
      "epoch no.4 train no.342210  loss = 3.43023 avg_loss = 3.22176\n",
      "epoch no.4 train no.342220  loss = 3.19853 avg_loss = 3.24584\n",
      "epoch no.4 train no.342230  loss = 2.96494 avg_loss = 3.25940\n",
      "epoch no.4 train no.342240  loss = 3.10603 avg_loss = 3.26133\n",
      "epoch no.4 train no.342250  loss = 2.46215 avg_loss = 3.28852\n",
      "epoch no.4 train no.342260  loss = 3.06903 avg_loss = 3.30914\n",
      "epoch no.4 train no.342270  loss = 2.55318 avg_loss = 3.27859\n",
      "epoch no.4 train no.342280  loss = 3.67774 avg_loss = 3.28481\n",
      "epoch no.4 train no.342290  loss = 2.62057 avg_loss = 3.26183\n",
      "epoch no.4 train no.342300  loss = 2.56201 avg_loss = 3.30392\n",
      "epoch no.4 train no.342310  loss = 2.71291 avg_loss = 3.26943\n",
      "epoch no.4 train no.342320  loss = 3.82563 avg_loss = 3.27566\n",
      "epoch no.4 train no.342330  loss = 3.42703 avg_loss = 3.28362\n",
      "epoch no.4 train no.342340  loss = 2.96639 avg_loss = 3.26398\n",
      "epoch no.4 train no.342350  loss = 3.01263 avg_loss = 3.25030\n",
      "epoch no.4 train no.342360  loss = 2.88131 avg_loss = 3.23566\n",
      "epoch no.4 train no.342370  loss = 5.12578 avg_loss = 3.28708\n",
      "epoch no.4 train no.342380  loss = 2.70690 avg_loss = 3.27035\n",
      "epoch no.4 train no.342390  loss = 3.48889 avg_loss = 3.27850\n",
      "epoch no.4 train no.342400  loss = 4.19669 avg_loss = 3.32130\n",
      "epoch no.4 train no.342410  loss = 3.21590 avg_loss = 3.34195\n",
      "epoch no.4 train no.342420  loss = 2.20829 avg_loss = 3.30337\n",
      "epoch no.4 train no.342430  loss = 3.67216 avg_loss = 3.28972\n",
      "epoch no.4 train no.342440  loss = 2.94810 avg_loss = 3.24443\n",
      "epoch no.4 train no.342450  loss = 2.61267 avg_loss = 3.25600\n",
      "epoch no.4 train no.342460  loss = 2.66164 avg_loss = 3.22484\n",
      "epoch no.4 train no.342470  loss = 3.05702 avg_loss = 3.25298\n",
      "epoch no.4 train no.342480  loss = 4.24846 avg_loss = 3.27359\n",
      "epoch no.4 train no.342490  loss = 2.30614 avg_loss = 3.19499\n",
      "epoch no.4 train no.342500  loss = 1.73468 avg_loss = 3.18396\n",
      "epoch no.4 train no.342510  loss = 4.54495 avg_loss = 3.18176\n",
      "epoch no.4 train no.342520  loss = 2.30347 avg_loss = 3.17362\n",
      "epoch no.4 train no.342530  loss = 3.53560 avg_loss = 3.19268\n",
      "epoch no.4 train no.342540  loss = 2.04080 avg_loss = 3.18075\n",
      "epoch no.4 train no.342550  loss = 2.33267 avg_loss = 3.17859\n",
      "epoch no.4 train no.342560  loss = 3.95398 avg_loss = 3.16917\n",
      "epoch no.4 train no.342570  loss = 2.33562 avg_loss = 3.18033\n",
      "epoch no.4 train no.342580  loss = 4.84156 avg_loss = 3.20444\n",
      "epoch no.4 train no.342590  loss = 1.88101 avg_loss = 3.15441\n",
      "epoch no.4 train no.342600  loss = 2.32985 avg_loss = 3.09335\n",
      "epoch no.4 train no.342610  loss = 3.26145 avg_loss = 3.14138\n",
      "epoch no.4 train no.342620  loss = 2.34235 avg_loss = 3.15368\n",
      "epoch no.4 train no.342630  loss = 1.70934 avg_loss = 3.09037\n",
      "epoch no.4 train no.342640  loss = 3.07177 avg_loss = 3.09946\n",
      "epoch no.4 train no.342650  loss = 2.73455 avg_loss = 3.06422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.342660  loss = 2.01031 avg_loss = 3.05689\n",
      "epoch no.4 train no.342670  loss = 3.58090 avg_loss = 3.11147\n",
      "epoch no.4 train no.342680  loss = 2.64835 avg_loss = 3.10490\n",
      "epoch no.4 train no.342690  loss = 4.14631 avg_loss = 3.13657\n",
      "epoch no.4 train no.342700  loss = 2.95155 avg_loss = 3.15623\n",
      "epoch no.4 train no.342710  loss = 2.13054 avg_loss = 3.14063\n",
      "epoch no.4 train no.342720  loss = 3.15807 avg_loss = 3.15628\n",
      "epoch no.4 train no.342730  loss = 3.88732 avg_loss = 3.12723\n",
      "epoch no.4 train no.342740  loss = 2.42640 avg_loss = 3.17803\n",
      "epoch no.4 train no.342750  loss = 2.56193 avg_loss = 3.17281\n",
      "epoch no.4 train no.342760  loss = 2.66856 avg_loss = 3.16330\n",
      "epoch no.4 train no.342770  loss = 2.79362 avg_loss = 3.14695\n",
      "epoch no.4 train no.342780  loss = 3.77421 avg_loss = 3.16358\n",
      "epoch no.4 train no.342790  loss = 3.78944 avg_loss = 3.18806\n",
      "epoch no.4 train no.342800  loss = 2.97449 avg_loss = 3.20957\n",
      "epoch no.4 train no.342810  loss = 2.84096 avg_loss = 3.18344\n",
      "epoch no.4 train no.342820  loss = 3.52364 avg_loss = 3.23461\n",
      "epoch no.4 train no.342830  loss = 2.89477 avg_loss = 3.22637\n",
      "epoch no.4 train no.342840  loss = 3.55113 avg_loss = 3.23839\n",
      "epoch no.4 train no.342850  loss = 2.57014 avg_loss = 3.25808\n",
      "epoch no.4 train no.342860  loss = 4.86287 avg_loss = 3.28174\n",
      "epoch no.4 train no.342870  loss = 1.87663 avg_loss = 3.25073\n",
      "epoch no.4 train no.342880  loss = 3.01334 avg_loss = 3.25192\n",
      "epoch no.4 train no.342890  loss = 2.40771 avg_loss = 3.22425\n",
      "epoch no.4 train no.342900  loss = 4.71593 avg_loss = 3.25478\n",
      "epoch no.4 train no.342910  loss = 2.78932 avg_loss = 3.22162\n",
      "epoch no.4 train no.342920  loss = 2.96172 avg_loss = 3.24711\n",
      "epoch no.4 train no.342930  loss = 6.52227 avg_loss = 3.34002\n",
      "epoch no.4 train no.342940  loss = 4.30941 avg_loss = 3.37136\n",
      "epoch no.4 train no.342950  loss = 2.67839 avg_loss = 3.38970\n",
      "epoch no.4 train no.342960  loss = 2.93159 avg_loss = 3.34984\n",
      "epoch no.4 train no.342970  loss = 3.06788 avg_loss = 3.34132\n",
      "epoch no.4 train no.342980  loss = 5.30913 avg_loss = 3.37220\n",
      "epoch no.4 train no.342990  loss = 3.09186 avg_loss = 3.39080\n",
      "epoch no.4 train no.343000  loss = 2.41833 avg_loss = 3.33378\n",
      "7\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁노래', '▁노래', 'op', '</s>']\n",
      "기분전환이 필요할 때 듣는 신나는 pop</s>\n",
      "epoch no.4 train no.343010  loss = 3.70772 avg_loss = 3.29587\n",
      "epoch no.4 train no.343020  loss = 3.60888 avg_loss = 3.30700\n",
      "epoch no.4 train no.343030  loss = 4.78089 avg_loss = 3.30468\n",
      "epoch no.4 train no.343040  loss = 2.81257 avg_loss = 3.28059\n",
      "epoch no.4 train no.343050  loss = 2.70786 avg_loss = 3.24108\n",
      "epoch no.4 train no.343060  loss = 5.80427 avg_loss = 3.19892\n",
      "epoch no.4 train no.343070  loss = 3.49217 avg_loss = 3.20557\n",
      "epoch no.4 train no.343080  loss = 3.86685 avg_loss = 3.18805\n",
      "epoch no.4 train no.343090  loss = 3.31365 avg_loss = 3.20937\n",
      "epoch no.4 train no.343100  loss = 2.30485 avg_loss = 3.18917\n",
      "epoch no.4 train no.343110  loss = 4.17373 avg_loss = 3.23854\n",
      "epoch no.4 train no.343120  loss = 1.76246 avg_loss = 3.22757\n",
      "epoch no.4 train no.343130  loss = 4.31226 avg_loss = 3.24306\n",
      "epoch no.4 train no.343140  loss = 2.70572 avg_loss = 3.23669\n",
      "epoch no.4 train no.343150  loss = 3.51476 avg_loss = 3.20962\n",
      "epoch no.4 train no.343160  loss = 3.84956 avg_loss = 3.20649\n",
      "epoch no.4 train no.343170  loss = 6.39111 avg_loss = 3.28356\n",
      "epoch no.4 train no.343180  loss = 2.27553 avg_loss = 3.26771\n",
      "epoch no.4 train no.343190  loss = 3.18242 avg_loss = 3.27094\n",
      "epoch no.4 train no.343200  loss = 1.98632 avg_loss = 3.21855\n",
      "epoch no.4 train no.343210  loss = 3.89328 avg_loss = 3.21167\n",
      "epoch no.4 train no.343220  loss = 2.77812 avg_loss = 3.22153\n",
      "epoch no.4 train no.343230  loss = 3.62622 avg_loss = 3.24051\n",
      "epoch no.4 train no.343240  loss = 3.21998 avg_loss = 3.24890\n",
      "epoch no.4 train no.343250  loss = 3.01927 avg_loss = 3.21426\n",
      "epoch no.4 train no.343260  loss = 4.28617 avg_loss = 3.21537\n",
      "epoch no.4 train no.343270  loss = 2.50095 avg_loss = 3.24120\n",
      "epoch no.4 train no.343280  loss = 2.94270 avg_loss = 3.24404\n",
      "epoch no.4 train no.343290  loss = 2.63288 avg_loss = 3.24128\n",
      "epoch no.4 train no.343300  loss = 4.33746 avg_loss = 3.28265\n",
      "epoch no.4 train no.343310  loss = 3.10972 avg_loss = 3.24883\n",
      "epoch no.4 train no.343320  loss = 4.52324 avg_loss = 3.28884\n",
      "epoch no.4 train no.343330  loss = 3.93131 avg_loss = 3.32384\n",
      "epoch no.4 train no.343340  loss = 2.77277 avg_loss = 3.29029\n",
      "epoch no.4 train no.343350  loss = 3.35035 avg_loss = 3.25503\n",
      "epoch no.4 train no.343360  loss = 3.29183 avg_loss = 3.28856\n",
      "epoch no.4 train no.343370  loss = 3.86024 avg_loss = 3.30943\n",
      "epoch no.4 train no.343380  loss = 2.96738 avg_loss = 3.27259\n",
      "epoch no.4 train no.343390  loss = 3.30827 avg_loss = 3.27412\n",
      "epoch no.4 train no.343400  loss = 1.85631 avg_loss = 3.23709\n",
      "epoch no.4 train no.343410  loss = 3.17583 avg_loss = 3.23980\n",
      "epoch no.4 train no.343420  loss = 2.78903 avg_loss = 3.21076\n",
      "epoch no.4 train no.343430  loss = 3.04716 avg_loss = 3.22201\n",
      "epoch no.4 train no.343440  loss = 4.58735 avg_loss = 3.26377\n",
      "epoch no.4 train no.343450  loss = 2.12687 avg_loss = 3.24269\n",
      "epoch no.4 train no.343460  loss = 4.51320 avg_loss = 3.20765\n",
      "epoch no.4 train no.343470  loss = 2.46028 avg_loss = 3.21451\n",
      "epoch no.4 train no.343480  loss = 4.43067 avg_loss = 3.22084\n",
      "epoch no.4 train no.343490  loss = 4.30436 avg_loss = 3.20578\n",
      "epoch no.4 train no.343500  loss = 4.12428 avg_loss = 3.22528\n",
      "epoch no.4 train no.343510  loss = 3.46765 avg_loss = 3.19459\n",
      "epoch no.4 train no.343520  loss = 1.46833 avg_loss = 3.19235\n",
      "epoch no.4 train no.343530  loss = 3.25666 avg_loss = 3.17573\n",
      "epoch no.4 train no.343540  loss = 4.26416 avg_loss = 3.24319\n",
      "epoch no.4 train no.343550  loss = 2.58365 avg_loss = 3.28980\n",
      "epoch no.4 train no.343560  loss = 2.71636 avg_loss = 3.26477\n",
      "epoch no.4 train no.343570  loss = 4.17019 avg_loss = 3.28008\n",
      "epoch no.4 train no.343580  loss = 3.85202 avg_loss = 3.29524\n",
      "epoch no.4 train no.343590  loss = 1.90657 avg_loss = 3.27955\n",
      "epoch no.4 train no.343600  loss = 2.89490 avg_loss = 3.24700\n",
      "epoch no.4 train no.343610  loss = 2.78688 avg_loss = 3.25514\n",
      "epoch no.4 train no.343620  loss = 2.57869 avg_loss = 3.20607\n",
      "epoch no.4 train no.343630  loss = 3.57922 avg_loss = 3.18912\n",
      "epoch no.4 train no.343640  loss = 2.88671 avg_loss = 3.18113\n",
      "epoch no.4 train no.343650  loss = 3.66300 avg_loss = 3.19314\n",
      "epoch no.4 train no.343660  loss = 2.84513 avg_loss = 3.19964\n",
      "epoch no.4 train no.343670  loss = 2.83035 avg_loss = 3.23356\n",
      "epoch no.4 train no.343680  loss = 2.84636 avg_loss = 3.23807\n",
      "epoch no.4 train no.343690  loss = 2.56811 avg_loss = 3.21210\n",
      "epoch no.4 train no.343700  loss = 2.74077 avg_loss = 3.17431\n",
      "epoch no.4 train no.343710  loss = 4.66173 avg_loss = 3.20793\n",
      "epoch no.4 train no.343720  loss = 2.68642 avg_loss = 3.23534\n",
      "epoch no.4 train no.343730  loss = 4.63322 avg_loss = 3.24011\n",
      "epoch no.4 train no.343740  loss = 3.00504 avg_loss = 3.24558\n",
      "epoch no.4 train no.343750  loss = 2.99390 avg_loss = 3.23145\n",
      "epoch no.4 train no.343760  loss = 3.09599 avg_loss = 3.21999\n",
      "epoch no.4 train no.343770  loss = 2.33575 avg_loss = 3.21855\n",
      "epoch no.4 train no.343780  loss = 3.89124 avg_loss = 3.27562\n",
      "epoch no.4 train no.343790  loss = 3.52303 avg_loss = 3.26583\n",
      "epoch no.4 train no.343800  loss = 2.45947 avg_loss = 3.26251\n",
      "epoch no.4 train no.343810  loss = 3.99687 avg_loss = 3.26029\n",
      "epoch no.4 train no.343820  loss = 4.21507 avg_loss = 3.25811\n",
      "epoch no.4 train no.343830  loss = 3.36817 avg_loss = 3.28794\n",
      "epoch no.4 train no.343840  loss = 6.26264 avg_loss = 3.27882\n",
      "epoch no.4 train no.343850  loss = 2.74710 avg_loss = 3.29078\n",
      "epoch no.4 train no.343860  loss = 3.34346 avg_loss = 3.31237\n",
      "epoch no.4 train no.343870  loss = 3.84463 avg_loss = 3.30687\n",
      "epoch no.4 train no.343880  loss = 3.01917 avg_loss = 3.29446\n",
      "epoch no.4 train no.343890  loss = 3.27524 avg_loss = 3.25437\n",
      "epoch no.4 train no.343900  loss = 3.42794 avg_loss = 3.24225\n",
      "epoch no.4 train no.343910  loss = 2.99817 avg_loss = 3.27025\n",
      "epoch no.4 train no.343920  loss = 2.24614 avg_loss = 3.22857\n",
      "epoch no.4 train no.343930  loss = 1.36033 avg_loss = 3.19440\n",
      "epoch no.4 train no.343940  loss = 2.07063 avg_loss = 3.19561\n",
      "epoch no.4 train no.343950  loss = 2.90380 avg_loss = 3.15912\n",
      "epoch no.4 train no.343960  loss = 4.34602 avg_loss = 3.17938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.343970  loss = 2.34905 avg_loss = 3.16528\n",
      "epoch no.4 train no.343980  loss = 1.63470 avg_loss = 3.15877\n",
      "epoch no.4 train no.343990  loss = 3.35544 avg_loss = 3.15888\n",
      "epoch no.4 train no.344000  loss = 2.96448 avg_loss = 3.20493\n",
      "3\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁위한', '▁팝', '</s>']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.4 train no.344010  loss = 2.88440 avg_loss = 3.23780\n",
      "epoch no.4 train no.344020  loss = 3.60709 avg_loss = 3.22976\n",
      "epoch no.4 train no.344030  loss = 3.32514 avg_loss = 3.23701\n",
      "epoch no.4 train no.344040  loss = 2.99145 avg_loss = 3.22136\n",
      "epoch no.4 train no.344050  loss = 1.58017 avg_loss = 3.16302\n",
      "epoch no.4 train no.344060  loss = 4.36522 avg_loss = 3.19387\n",
      "epoch no.4 train no.344070  loss = 2.57313 avg_loss = 3.20569\n",
      "epoch no.4 train no.344080  loss = 4.68264 avg_loss = 3.21067\n",
      "epoch no.4 train no.344090  loss = 1.49904 avg_loss = 3.20261\n",
      "epoch no.4 train no.344100  loss = 2.76763 avg_loss = 3.17791\n",
      "epoch no.4 train no.344110  loss = 3.25332 avg_loss = 3.20311\n",
      "epoch no.4 train no.344120  loss = 4.25133 avg_loss = 3.21655\n",
      "epoch no.4 train no.344130  loss = 3.86101 avg_loss = 3.25957\n",
      "epoch no.4 train no.344140  loss = 2.79348 avg_loss = 3.27659\n",
      "epoch no.4 train no.344150  loss = 2.31633 avg_loss = 3.30285\n",
      "epoch no.4 train no.344160  loss = 3.44670 avg_loss = 3.28853\n",
      "epoch no.4 train no.344170  loss = 3.49706 avg_loss = 3.28980\n",
      "epoch no.4 train no.344180  loss = 2.74484 avg_loss = 3.24267\n",
      "epoch no.4 train no.344190  loss = 5.90519 avg_loss = 3.23612\n",
      "epoch no.4 train no.344200  loss = 3.63819 avg_loss = 3.22288\n",
      "epoch no.4 train no.344210  loss = 4.08317 avg_loss = 3.25003\n",
      "epoch no.4 train no.344220  loss = 4.02015 avg_loss = 3.24608\n",
      "epoch no.4 train no.344230  loss = 4.26861 avg_loss = 3.26660\n",
      "epoch no.4 train no.344240  loss = 4.99675 avg_loss = 3.29351\n",
      "epoch no.4 train no.344250  loss = 3.53100 avg_loss = 3.27379\n",
      "epoch no.4 train no.344260  loss = 2.75968 avg_loss = 3.28825\n",
      "epoch no.4 train no.344270  loss = 2.33479 avg_loss = 3.24986\n",
      "epoch no.4 train no.344280  loss = 2.01251 avg_loss = 3.26453\n",
      "epoch no.4 train no.344290  loss = 1.58240 avg_loss = 3.22674\n",
      "epoch no.4 train no.344300  loss = 2.12031 avg_loss = 3.21481\n",
      "epoch no.4 train no.344310  loss = 2.93021 avg_loss = 3.19605\n",
      "epoch no.4 train no.344320  loss = 4.79609 avg_loss = 3.20793\n",
      "epoch no.4 train no.344330  loss = 2.99905 avg_loss = 3.17524\n",
      "epoch no.4 train no.344340  loss = 3.02536 avg_loss = 3.16530\n",
      "epoch no.4 train no.344350  loss = 2.68462 avg_loss = 3.15674\n",
      "epoch no.4 train no.344360  loss = 4.77419 avg_loss = 3.19238\n",
      "epoch no.4 train no.344370  loss = 2.26285 avg_loss = 3.22984\n",
      "epoch no.4 train no.344380  loss = 2.94726 avg_loss = 3.21561\n",
      "epoch no.4 train no.344390  loss = 4.43807 avg_loss = 3.23124\n",
      "epoch no.4 train no.344400  loss = 4.13253 avg_loss = 3.25461\n",
      "epoch no.4 train no.344410  loss = 3.48627 avg_loss = 3.24215\n",
      "epoch no.4 train no.344420  loss = 4.07855 avg_loss = 3.24626\n",
      "epoch no.4 train no.344430  loss = 3.45809 avg_loss = 3.27637\n",
      "epoch no.4 train no.344440  loss = 3.13049 avg_loss = 3.27564\n",
      "epoch no.4 train no.344450  loss = 2.02876 avg_loss = 3.23321\n",
      "epoch no.4 train no.344460  loss = 3.49195 avg_loss = 3.24846\n",
      "epoch no.4 train no.344470  loss = 2.49091 avg_loss = 3.22474\n",
      "epoch no.4 train no.344480  loss = 3.36811 avg_loss = 3.22408\n",
      "epoch no.4 train no.344490  loss = 3.98928 avg_loss = 3.24325\n",
      "epoch no.4 train no.344500  loss = 1.66615 avg_loss = 3.17968\n",
      "epoch no.4 train no.344510  loss = 3.93778 avg_loss = 3.20341\n",
      "epoch no.4 train no.344520  loss = 3.08723 avg_loss = 3.23269\n",
      "epoch no.4 train no.344530  loss = 3.26945 avg_loss = 3.24650\n",
      "epoch no.4 train no.344540  loss = 2.92292 avg_loss = 3.26123\n",
      "epoch no.4 train no.344550  loss = 2.65586 avg_loss = 3.26458\n",
      "epoch no.4 train no.344560  loss = 4.18340 avg_loss = 3.24115\n",
      "epoch no.4 train no.344570  loss = 2.80384 avg_loss = 3.16137\n",
      "epoch no.4 train no.344580  loss = 2.66547 avg_loss = 3.15994\n",
      "epoch no.4 train no.344590  loss = 3.02389 avg_loss = 3.12565\n",
      "epoch no.4 train no.344600  loss = 2.27026 avg_loss = 3.07628\n",
      "epoch no.4 train no.344610  loss = 2.81656 avg_loss = 3.06598\n",
      "epoch no.4 train no.344620  loss = 4.80741 avg_loss = 3.10652\n",
      "epoch no.4 train no.344630  loss = 2.50782 avg_loss = 3.14609\n",
      "epoch no.4 train no.344640  loss = 2.75552 avg_loss = 3.15219\n",
      "epoch no.4 train no.344650  loss = 3.67025 avg_loss = 3.14251\n",
      "epoch no.4 train no.344660  loss = 4.94429 avg_loss = 3.15183\n",
      "epoch no.4 train no.344670  loss = 2.54767 avg_loss = 3.12071\n",
      "epoch no.4 train no.344680  loss = 3.09481 avg_loss = 3.11567\n",
      "epoch no.4 train no.344690  loss = 3.25674 avg_loss = 3.11771\n",
      "epoch no.4 train no.344700  loss = 4.31987 avg_loss = 3.15834\n",
      "epoch no.4 train no.344710  loss = 2.37796 avg_loss = 3.12829\n",
      "epoch no.4 train no.344720  loss = 5.94374 avg_loss = 3.15767\n",
      "epoch no.4 train no.344730  loss = 2.41498 avg_loss = 3.15909\n",
      "epoch no.4 train no.344740  loss = 3.18497 avg_loss = 3.17277\n",
      "epoch no.4 train no.344750  loss = 3.29414 avg_loss = 3.16538\n",
      "epoch no.4 train no.344760  loss = 3.19495 avg_loss = 3.21356\n",
      "epoch no.4 train no.344770  loss = 2.22735 avg_loss = 3.25108\n",
      "epoch no.4 train no.344780  loss = 2.84946 avg_loss = 3.28291\n",
      "epoch no.4 train no.344790  loss = 2.82303 avg_loss = 3.29294\n",
      "epoch no.4 train no.344800  loss = 2.74198 avg_loss = 3.28606\n",
      "epoch no.4 train no.344810  loss = 3.50872 avg_loss = 3.27819\n",
      "epoch no.4 train no.344820  loss = 2.27351 avg_loss = 3.25965\n",
      "epoch no.4 train no.344830  loss = 3.35618 avg_loss = 3.25536\n",
      "epoch no.4 train no.344840  loss = 3.50095 avg_loss = 3.26676\n",
      "epoch no.4 train no.344850  loss = 3.57495 avg_loss = 3.26047\n",
      "epoch no.4 train no.344860  loss = 1.89556 avg_loss = 3.27906\n",
      "epoch no.4 train no.344870  loss = 3.07878 avg_loss = 3.31480\n",
      "epoch no.4 train no.344880  loss = 4.46220 avg_loss = 3.31654\n",
      "epoch no.4 train no.344890  loss = 3.77674 avg_loss = 3.30850\n",
      "epoch no.4 train no.344900  loss = 3.03123 avg_loss = 3.26988\n",
      "epoch no.4 train no.344910  loss = 2.50185 avg_loss = 3.25784\n",
      "epoch no.4 train no.344920  loss = 3.69612 avg_loss = 3.26847\n",
      "epoch no.4 train no.344930  loss = 2.61109 avg_loss = 3.26422\n",
      "epoch no.4 train no.344940  loss = 1.91603 avg_loss = 3.22212\n",
      "epoch no.4 train no.344950  loss = 3.21709 avg_loss = 3.19554\n",
      "epoch no.4 train no.344960  loss = 2.50457 avg_loss = 3.17186\n",
      "epoch no.4 train no.344970  loss = 2.68380 avg_loss = 3.22814\n",
      "epoch no.4 train no.344980  loss = 2.82465 avg_loss = 3.24779\n",
      "epoch no.4 train no.344990  loss = 2.77507 avg_loss = 3.25018\n",
      "epoch no.4 train no.345000  loss = 2.43830 avg_loss = 3.22984\n",
      "6\n",
      "to_tokens: ['▁라디오', '좋은', '에', '▁상큼한', '하고', '▁감각적인', '▁p', 'op', '</s>']\n",
      "기분전환엔 트렌디하고 감각적인 pop</s>\n",
      "epoch no.4 train no.345010  loss = 4.02180 avg_loss = 3.22416\n",
      "epoch no.4 train no.345020  loss = 6.64346 avg_loss = 3.27798\n",
      "epoch no.4 train no.345030  loss = 3.39434 avg_loss = 3.28796\n",
      "epoch no.4 train no.345040  loss = 3.16213 avg_loss = 3.30449\n",
      "epoch no.4 train no.345050  loss = 3.56206 avg_loss = 3.28040\n",
      "epoch no.4 train no.345060  loss = 3.45777 avg_loss = 3.27355\n",
      "epoch no.4 train no.345070  loss = 5.41709 avg_loss = 3.28968\n",
      "epoch no.4 train no.345080  loss = 3.66868 avg_loss = 3.29218\n",
      "epoch no.4 train no.345090  loss = 3.77709 avg_loss = 3.32653\n",
      "epoch no.4 train no.345100  loss = 3.52540 avg_loss = 3.31037\n",
      "epoch no.4 train no.345110  loss = 5.62211 avg_loss = 3.28738\n",
      "epoch no.4 train no.345120  loss = 3.05115 avg_loss = 3.25197\n",
      "epoch no.4 train no.345130  loss = 3.65314 avg_loss = 3.27393\n",
      "epoch no.4 train no.345140  loss = 2.09817 avg_loss = 3.29500\n",
      "epoch no.4 train no.345150  loss = 2.88422 avg_loss = 3.25820\n",
      "epoch no.4 train no.345160  loss = 3.64724 avg_loss = 3.23055\n",
      "epoch no.4 train no.345170  loss = 2.61442 avg_loss = 3.21047\n",
      "epoch no.4 train no.345180  loss = 3.54549 avg_loss = 3.25144\n",
      "epoch no.4 train no.345190  loss = 4.50419 avg_loss = 3.25711\n",
      "epoch no.4 train no.345200  loss = 4.03079 avg_loss = 3.23365\n",
      "epoch no.4 train no.345210  loss = 3.03298 avg_loss = 3.22176\n",
      "epoch no.4 train no.345220  loss = 3.75485 avg_loss = 3.23369\n",
      "epoch no.4 train no.345230  loss = 2.06298 avg_loss = 3.23018\n",
      "epoch no.4 train no.345240  loss = 3.48570 avg_loss = 3.18369\n",
      "epoch no.4 train no.345250  loss = 4.68887 avg_loss = 3.24450\n",
      "epoch no.4 train no.345260  loss = 2.76575 avg_loss = 3.20370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.345270  loss = 1.64235 avg_loss = 3.18664\n",
      "epoch no.4 train no.345280  loss = 3.95305 avg_loss = 3.19406\n",
      "epoch no.4 train no.345290  loss = 3.61313 avg_loss = 3.23412\n",
      "epoch no.4 train no.345300  loss = 2.84492 avg_loss = 3.24576\n",
      "epoch no.4 train no.345310  loss = 5.07577 avg_loss = 3.28626\n",
      "epoch no.4 train no.345320  loss = 2.43591 avg_loss = 3.25881\n",
      "epoch no.4 train no.345330  loss = 2.88795 avg_loss = 3.24857\n",
      "epoch no.4 train no.345340  loss = 3.63243 avg_loss = 3.26244\n",
      "epoch no.4 train no.345350  loss = 2.48850 avg_loss = 3.22260\n",
      "epoch no.4 train no.345360  loss = 1.89029 avg_loss = 3.21992\n",
      "epoch no.4 train no.345370  loss = 2.65698 avg_loss = 3.20148\n",
      "epoch no.4 train no.345380  loss = 3.69481 avg_loss = 3.23491\n",
      "epoch no.4 train no.345390  loss = 1.71235 avg_loss = 3.22005\n",
      "epoch no.4 train no.345400  loss = 2.64037 avg_loss = 3.21564\n",
      "epoch no.4 train no.345410  loss = 2.87137 avg_loss = 3.16409\n",
      "epoch no.4 train no.345420  loss = 3.92066 avg_loss = 3.17088\n",
      "epoch no.4 train no.345430  loss = 6.18292 avg_loss = 3.19856\n",
      "epoch no.4 train no.345440  loss = 4.48867 avg_loss = 3.19614\n",
      "epoch no.4 train no.345450  loss = 3.00870 avg_loss = 3.19693\n",
      "epoch no.4 train no.345460  loss = 2.87471 avg_loss = 3.17170\n",
      "epoch no.4 train no.345470  loss = 3.29421 avg_loss = 3.14687\n",
      "epoch no.4 train no.345480  loss = 2.79875 avg_loss = 3.14209\n",
      "epoch no.4 train no.345490  loss = 3.31867 avg_loss = 3.15368\n",
      "epoch no.4 train no.345500  loss = 2.42184 avg_loss = 3.19103\n",
      "epoch no.4 train no.345510  loss = 2.92645 avg_loss = 3.19340\n",
      "epoch no.4 train no.345520  loss = 5.89752 avg_loss = 3.20648\n",
      "epoch no.4 train no.345530  loss = 2.59386 avg_loss = 3.23580\n",
      "epoch no.4 train no.345540  loss = 2.88058 avg_loss = 3.21511\n",
      "epoch no.4 train no.345550  loss = 1.33154 avg_loss = 3.15252\n",
      "epoch no.4 train no.345560  loss = 2.03298 avg_loss = 3.15643\n",
      "epoch no.4 train no.345570  loss = 3.54530 avg_loss = 3.12917\n",
      "epoch no.4 train no.345580  loss = 3.19907 avg_loss = 3.15729\n",
      "epoch no.4 train no.345590  loss = 2.80008 avg_loss = 3.21042\n",
      "epoch no.4 train no.345600  loss = 3.90667 avg_loss = 3.21825\n",
      "epoch no.4 train no.345610  loss = 3.50277 avg_loss = 3.20626\n",
      "epoch no.4 train no.345620  loss = 2.24197 avg_loss = 3.21872\n",
      "epoch no.4 train no.345630  loss = 2.26044 avg_loss = 3.22397\n",
      "epoch no.4 train no.345640  loss = 2.75079 avg_loss = 3.20337\n",
      "epoch no.4 train no.345650  loss = 2.02222 avg_loss = 3.19738\n",
      "epoch no.4 train no.345660  loss = 2.97940 avg_loss = 3.19276\n",
      "epoch no.4 train no.345670  loss = 2.14404 avg_loss = 3.19486\n",
      "epoch no.4 train no.345680  loss = 2.77545 avg_loss = 3.18851\n",
      "epoch no.4 train no.345690  loss = 3.38511 avg_loss = 3.15827\n",
      "epoch no.4 train no.345700  loss = 4.88553 avg_loss = 3.17959\n",
      "epoch no.4 train no.345710  loss = 3.29676 avg_loss = 3.15435\n",
      "epoch no.4 train no.345720  loss = 3.11783 avg_loss = 3.17424\n",
      "epoch no.4 train no.345730  loss = 2.06531 avg_loss = 3.14116\n",
      "epoch no.4 train no.345740  loss = 2.52881 avg_loss = 3.13882\n",
      "epoch no.4 train no.345750  loss = 4.31509 avg_loss = 3.14935\n",
      "epoch no.4 train no.345760  loss = 3.06979 avg_loss = 3.16828\n",
      "epoch no.4 train no.345770  loss = 2.22112 avg_loss = 3.19012\n",
      "epoch no.4 train no.345780  loss = 3.10550 avg_loss = 3.23638\n",
      "epoch no.4 train no.345790  loss = 3.76771 avg_loss = 3.25501\n",
      "epoch no.4 train no.345800  loss = 3.16208 avg_loss = 3.25013\n",
      "epoch no.4 train no.345810  loss = 4.19386 avg_loss = 3.24422\n",
      "epoch no.4 train no.345820  loss = 4.10368 avg_loss = 3.32559\n",
      "epoch no.4 train no.345830  loss = 3.56569 avg_loss = 3.33815\n",
      "epoch no.4 train no.345840  loss = 3.27619 avg_loss = 3.34183\n",
      "epoch no.4 train no.345850  loss = 1.48288 avg_loss = 3.32842\n",
      "epoch no.4 train no.345860  loss = 4.18369 avg_loss = 3.34175\n",
      "epoch no.4 train no.345870  loss = 3.18919 avg_loss = 3.32694\n",
      "epoch no.4 train no.345880  loss = 4.68883 avg_loss = 3.32432\n",
      "epoch no.4 train no.345890  loss = 2.61424 avg_loss = 3.27292\n",
      "epoch no.4 train no.345900  loss = 2.18375 avg_loss = 3.25490\n",
      "epoch no.4 train no.345910  loss = 1.59384 avg_loss = 3.22680\n",
      "epoch no.4 train no.345920  loss = 2.17767 avg_loss = 3.20690\n",
      "epoch no.4 train no.345930  loss = 3.82327 avg_loss = 3.26672\n",
      "epoch no.4 train no.345940  loss = 3.07874 avg_loss = 3.25519\n",
      "epoch no.4 train no.345950  loss = 3.00861 avg_loss = 3.22089\n",
      "epoch no.4 train no.345960  loss = 3.12854 avg_loss = 3.21637\n",
      "epoch no.4 train no.345970  loss = 4.62173 avg_loss = 3.25681\n",
      "epoch no.4 train no.345980  loss = 3.11962 avg_loss = 3.31862\n",
      "epoch no.4 train no.345990  loss = 3.93723 avg_loss = 3.31286\n",
      "epoch no.4 train no.346000  loss = 5.17330 avg_loss = 3.35343\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '인', '▁노래', '</s>']\n",
      "기분전환에 최고인 노래</s>\n",
      "epoch no.4 train no.346010  loss = 4.96975 avg_loss = 3.34095\n",
      "epoch no.4 train no.346020  loss = 2.26124 avg_loss = 3.31771\n",
      "epoch no.4 train no.346030  loss = 2.72490 avg_loss = 3.31474\n",
      "epoch no.4 train no.346040  loss = 4.92027 avg_loss = 3.36669\n",
      "epoch no.4 train no.346050  loss = 3.51339 avg_loss = 3.35912\n",
      "epoch no.4 train no.346060  loss = 3.88758 avg_loss = 3.33163\n",
      "epoch no.4 train no.346070  loss = 2.49235 avg_loss = 3.35964\n",
      "epoch no.4 train no.346080  loss = 3.84487 avg_loss = 3.34415\n",
      "epoch no.4 train no.346090  loss = 2.12267 avg_loss = 3.29640\n",
      "epoch no.4 train no.346100  loss = 3.10968 avg_loss = 3.24023\n",
      "epoch no.4 train no.346110  loss = 3.26019 avg_loss = 3.21378\n",
      "epoch no.4 train no.346120  loss = 4.95354 avg_loss = 3.23727\n",
      "epoch no.4 train no.346130  loss = 3.01214 avg_loss = 3.23567\n",
      "epoch no.4 train no.346140  loss = 2.98262 avg_loss = 3.22686\n",
      "epoch no.4 train no.346150  loss = 2.16332 avg_loss = 3.21203\n",
      "epoch no.4 train no.346160  loss = 3.36978 avg_loss = 3.15955\n",
      "epoch no.4 train no.346170  loss = 2.75146 avg_loss = 3.13566\n",
      "epoch no.4 train no.346180  loss = 2.40359 avg_loss = 3.09877\n",
      "epoch no.4 train no.346190  loss = 3.66473 avg_loss = 3.13934\n",
      "epoch no.4 train no.346200  loss = 5.01305 avg_loss = 3.16272\n",
      "epoch no.4 train no.346210  loss = 3.82917 avg_loss = 3.13943\n",
      "epoch no.4 train no.346220  loss = 3.71272 avg_loss = 3.14610\n",
      "epoch no.4 train no.346230  loss = 4.89636 avg_loss = 3.17855\n",
      "epoch no.4 train no.346240  loss = 3.43927 avg_loss = 3.21324\n",
      "epoch no.4 train no.346250  loss = 2.76794 avg_loss = 3.24635\n",
      "epoch no.4 train no.346260  loss = 5.47209 avg_loss = 3.29097\n",
      "epoch no.4 train no.346270  loss = 4.07094 avg_loss = 3.25624\n",
      "epoch no.4 train no.346280  loss = 2.97346 avg_loss = 3.25536\n",
      "epoch no.4 train no.346290  loss = 3.02091 avg_loss = 3.22277\n",
      "epoch no.4 train no.346300  loss = 4.14062 avg_loss = 3.19405\n",
      "epoch no.4 train no.346310  loss = 1.33111 avg_loss = 3.17013\n",
      "epoch no.4 train no.346320  loss = 3.68730 avg_loss = 3.19147\n",
      "epoch no.4 train no.346330  loss = 4.30342 avg_loss = 3.19212\n",
      "epoch no.4 train no.346340  loss = 4.05816 avg_loss = 3.22073\n",
      "epoch no.4 train no.346350  loss = 4.69366 avg_loss = 3.25725\n",
      "epoch no.4 train no.346360  loss = 3.46244 avg_loss = 3.25152\n",
      "epoch no.4 train no.346370  loss = 3.84117 avg_loss = 3.30467\n",
      "epoch no.4 train no.346380  loss = 3.68425 avg_loss = 3.32374\n",
      "epoch no.4 train no.346390  loss = 2.83590 avg_loss = 3.30074\n",
      "epoch no.4 train no.346400  loss = 4.57142 avg_loss = 3.29472\n",
      "epoch no.4 train no.346410  loss = 2.57639 avg_loss = 3.29633\n",
      "epoch no.4 train no.346420  loss = 2.61169 avg_loss = 3.26574\n",
      "epoch no.4 train no.346430  loss = 2.91715 avg_loss = 3.26195\n",
      "epoch no.4 train no.346440  loss = 2.50076 avg_loss = 3.23888\n",
      "epoch no.4 train no.346450  loss = 3.15843 avg_loss = 3.27773\n",
      "epoch no.4 train no.346460  loss = 3.22204 avg_loss = 3.26655\n",
      "epoch no.4 train no.346470  loss = 3.24864 avg_loss = 3.28196\n",
      "epoch no.4 train no.346480  loss = 3.55113 avg_loss = 3.32830\n",
      "epoch no.4 train no.346490  loss = 2.49260 avg_loss = 3.35310\n",
      "epoch no.4 train no.346500  loss = 5.44764 avg_loss = 3.38296\n",
      "epoch no.4 train no.346510  loss = 2.32893 avg_loss = 3.36700\n",
      "epoch no.4 train no.346520  loss = 3.40296 avg_loss = 3.36331\n",
      "epoch no.4 train no.346530  loss = 4.73031 avg_loss = 3.30389\n",
      "epoch no.4 train no.346540  loss = 2.46261 avg_loss = 3.24454\n",
      "epoch no.4 train no.346550  loss = 4.08049 avg_loss = 3.24228\n",
      "epoch no.4 train no.346560  loss = 3.73970 avg_loss = 3.24752\n",
      "epoch no.4 train no.346570  loss = 2.69768 avg_loss = 3.21952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.346580  loss = 4.12558 avg_loss = 3.17171\n",
      "epoch no.4 train no.346590  loss = 3.43630 avg_loss = 3.15987\n",
      "epoch no.4 train no.346600  loss = 3.32117 avg_loss = 3.19778\n",
      "epoch no.4 train no.346610  loss = 3.13133 avg_loss = 3.21225\n",
      "epoch no.4 train no.346620  loss = 3.34890 avg_loss = 3.21959\n",
      "epoch no.4 train no.346630  loss = 1.82044 avg_loss = 3.20775\n",
      "epoch no.4 train no.346640  loss = 2.86411 avg_loss = 3.22390\n",
      "epoch no.4 train no.346650  loss = 4.46557 avg_loss = 3.25492\n",
      "epoch no.4 train no.346660  loss = 2.75541 avg_loss = 3.26300\n",
      "epoch no.4 train no.346670  loss = 2.66200 avg_loss = 3.24798\n",
      "epoch no.4 train no.346680  loss = 3.15805 avg_loss = 3.23911\n",
      "epoch no.4 train no.346690  loss = 2.07341 avg_loss = 3.21408\n",
      "epoch no.4 train no.346700  loss = 2.90193 avg_loss = 3.19683\n",
      "epoch no.4 train no.346710  loss = 3.54155 avg_loss = 3.22774\n",
      "epoch no.4 train no.346720  loss = 4.15818 avg_loss = 3.24281\n",
      "epoch no.4 train no.346730  loss = 2.80627 avg_loss = 3.23486\n",
      "epoch no.4 train no.346740  loss = 2.81561 avg_loss = 3.20893\n",
      "epoch no.4 train no.346750  loss = 2.83633 avg_loss = 3.20628\n",
      "epoch no.4 train no.346760  loss = 3.71774 avg_loss = 3.18296\n",
      "epoch no.4 train no.346770  loss = 2.46859 avg_loss = 3.13445\n",
      "epoch no.4 train no.346780  loss = 3.97708 avg_loss = 3.18617\n",
      "epoch no.4 train no.346790  loss = 3.26788 avg_loss = 3.14785\n",
      "epoch no.4 train no.346800  loss = 5.15252 avg_loss = 3.13213\n",
      "epoch no.4 train no.346810  loss = 2.21527 avg_loss = 3.16128\n",
      "epoch no.4 train no.346820  loss = 3.24771 avg_loss = 3.13536\n",
      "epoch no.4 train no.346830  loss = 2.33603 avg_loss = 3.15016\n",
      "epoch no.4 train no.346840  loss = 2.86339 avg_loss = 3.13822\n",
      "epoch no.4 train no.346850  loss = 4.66214 avg_loss = 3.11758\n",
      "epoch no.4 train no.346860  loss = 2.87408 avg_loss = 3.17476\n",
      "epoch no.4 train no.346870  loss = 3.02038 avg_loss = 3.18541\n",
      "epoch no.4 train no.346880  loss = 2.04206 avg_loss = 3.18642\n",
      "epoch no.4 train no.346890  loss = 3.51061 avg_loss = 3.18953\n",
      "epoch no.4 train no.346900  loss = 2.39915 avg_loss = 3.13775\n",
      "epoch no.4 train no.346910  loss = 3.00091 avg_loss = 3.12067\n",
      "epoch no.4 train no.346920  loss = 3.07878 avg_loss = 3.14710\n",
      "epoch no.4 train no.346930  loss = 3.66598 avg_loss = 3.17425\n",
      "epoch no.4 train no.346940  loss = 2.37953 avg_loss = 3.15073\n",
      "epoch no.4 train no.346950  loss = 2.93231 avg_loss = 3.13470\n",
      "epoch no.4 train no.346960  loss = 3.27767 avg_loss = 3.12948\n",
      "epoch no.4 train no.346970  loss = 6.44454 avg_loss = 3.13596\n",
      "epoch no.4 train no.346980  loss = 3.81380 avg_loss = 3.15916\n",
      "epoch no.4 train no.346990  loss = 2.45595 avg_loss = 3.12085\n",
      "epoch no.4 train no.347000  loss = 3.19435 avg_loss = 3.08705\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '에', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할 때 듣는 음악</s>\n",
      "epoch no.4 train no.347010  loss = 4.06682 avg_loss = 3.11111\n",
      "epoch no.4 train no.347020  loss = 2.83685 avg_loss = 3.18857\n",
      "epoch no.4 train no.347030  loss = 3.58828 avg_loss = 3.17962\n",
      "epoch no.4 train no.347040  loss = 3.83865 avg_loss = 3.21116\n",
      "epoch no.4 train no.347050  loss = 2.96055 avg_loss = 3.20858\n",
      "epoch no.4 train no.347060  loss = 3.97888 avg_loss = 3.26740\n",
      "epoch no.4 train no.347070  loss = 2.93385 avg_loss = 3.25206\n",
      "epoch no.4 train no.347080  loss = 2.75621 avg_loss = 3.21385\n",
      "epoch no.4 train no.347090  loss = 1.71413 avg_loss = 3.20938\n",
      "epoch no.4 train no.347100  loss = 3.42559 avg_loss = 3.22981\n",
      "epoch no.4 train no.347110  loss = 2.44727 avg_loss = 3.22990\n",
      "epoch no.4 train no.347120  loss = 3.09260 avg_loss = 3.25522\n",
      "epoch no.4 train no.347130  loss = 2.35194 avg_loss = 3.26834\n",
      "epoch no.4 train no.347140  loss = 3.15054 avg_loss = 3.34174\n",
      "epoch no.4 train no.347150  loss = 3.08057 avg_loss = 3.34048\n",
      "epoch no.4 train no.347160  loss = 3.86990 avg_loss = 3.32882\n",
      "epoch no.4 train no.347170  loss = 2.42537 avg_loss = 3.30502\n",
      "epoch no.4 train no.347180  loss = 3.91435 avg_loss = 3.29179\n",
      "epoch no.4 train no.347190  loss = 4.17839 avg_loss = 3.31345\n",
      "epoch no.4 train no.347200  loss = 3.07401 avg_loss = 3.29909\n",
      "epoch no.4 train no.347210  loss = 2.55938 avg_loss = 3.29897\n",
      "epoch no.4 train no.347220  loss = 5.11876 avg_loss = 3.31451\n",
      "epoch no.4 train no.347230  loss = 3.71332 avg_loss = 3.31666\n",
      "epoch no.4 train no.347240  loss = 4.25021 avg_loss = 3.33969\n",
      "epoch no.4 train no.347250  loss = 2.45744 avg_loss = 3.32140\n",
      "epoch no.4 train no.347260  loss = 3.25745 avg_loss = 3.31480\n",
      "epoch no.4 train no.347270  loss = 2.79261 avg_loss = 3.31150\n",
      "epoch no.4 train no.347280  loss = 4.21480 avg_loss = 3.32518\n",
      "epoch no.4 train no.347290  loss = 2.09248 avg_loss = 3.31650\n",
      "epoch no.4 train no.347300  loss = 3.29213 avg_loss = 3.26841\n",
      "epoch no.4 train no.347310  loss = 2.86453 avg_loss = 3.26124\n",
      "epoch no.4 train no.347320  loss = 3.01416 avg_loss = 3.25436\n",
      "epoch no.4 train no.347330  loss = 3.23399 avg_loss = 3.24688\n",
      "epoch no.4 train no.347340  loss = 2.71215 avg_loss = 3.20234\n",
      "epoch no.4 train no.347350  loss = 3.15031 avg_loss = 3.18867\n",
      "epoch no.4 train no.347360  loss = 3.12713 avg_loss = 3.21646\n",
      "epoch no.4 train no.347370  loss = 2.44954 avg_loss = 3.18122\n",
      "epoch no.4 train no.347380  loss = 3.07938 avg_loss = 3.13315\n",
      "epoch no.4 train no.347390  loss = 2.09022 avg_loss = 3.13315\n",
      "epoch no.4 train no.347400  loss = 4.13404 avg_loss = 3.14248\n",
      "epoch no.4 train no.347410  loss = 3.65875 avg_loss = 3.16613\n",
      "epoch no.4 train no.347420  loss = 2.55892 avg_loss = 3.21467\n",
      "epoch no.4 train no.347430  loss = 4.41989 avg_loss = 3.24136\n",
      "epoch no.4 train no.347440  loss = 4.31471 avg_loss = 3.24696\n",
      "epoch no.4 train no.347450  loss = 4.04175 avg_loss = 3.28343\n",
      "epoch no.4 train no.347460  loss = 3.31593 avg_loss = 3.24358\n",
      "epoch no.4 train no.347470  loss = 3.52993 avg_loss = 3.27226\n",
      "epoch no.4 train no.347480  loss = 2.06249 avg_loss = 3.22486\n",
      "epoch no.4 train no.347490  loss = 3.76269 avg_loss = 3.23389\n",
      "epoch no.4 train no.347500  loss = 1.81802 avg_loss = 3.24163\n",
      "epoch no.4 train no.347510  loss = 6.22808 avg_loss = 3.27057\n",
      "epoch no.4 train no.347520  loss = 2.86936 avg_loss = 3.21781\n",
      "epoch no.4 train no.347530  loss = 2.85143 avg_loss = 3.21861\n",
      "epoch no.4 train no.347540  loss = 2.21776 avg_loss = 3.20326\n",
      "epoch no.4 train no.347550  loss = 3.26055 avg_loss = 3.18060\n",
      "epoch no.4 train no.347560  loss = 2.65604 avg_loss = 3.19520\n",
      "epoch no.4 train no.347570  loss = 1.95540 avg_loss = 3.15121\n",
      "epoch no.4 train no.347580  loss = 2.90168 avg_loss = 3.14909\n",
      "epoch no.4 train no.347590  loss = 2.76001 avg_loss = 3.13767\n",
      "epoch no.4 train no.347600  loss = 1.40783 avg_loss = 3.14495\n",
      "epoch no.4 train no.347610  loss = 3.21066 avg_loss = 3.13302\n",
      "epoch no.4 train no.347620  loss = 3.32784 avg_loss = 3.19648\n",
      "epoch no.4 train no.347630  loss = 3.52832 avg_loss = 3.20338\n",
      "epoch no.4 train no.347640  loss = 4.32060 avg_loss = 3.22134\n",
      "epoch no.4 train no.347650  loss = 2.80629 avg_loss = 3.25851\n",
      "epoch no.4 train no.347660  loss = 3.67381 avg_loss = 3.23962\n",
      "epoch no.4 train no.347670  loss = 2.28078 avg_loss = 3.19420\n",
      "epoch no.4 train no.347680  loss = 3.54069 avg_loss = 3.24033\n",
      "epoch no.4 train no.347690  loss = 2.36754 avg_loss = 3.26698\n",
      "epoch no.4 train no.347700  loss = 3.09744 avg_loss = 3.29492\n",
      "epoch no.4 train no.347710  loss = 3.84721 avg_loss = 3.32825\n",
      "epoch no.4 train no.347720  loss = 2.83141 avg_loss = 3.30582\n",
      "epoch no.4 train no.347730  loss = 3.49698 avg_loss = 3.29079\n",
      "epoch no.4 train no.347740  loss = 1.25260 avg_loss = 3.28410\n",
      "epoch no.4 train no.347750  loss = 3.21173 avg_loss = 3.24583\n",
      "epoch no.4 train no.347760  loss = 2.79240 avg_loss = 3.23474\n",
      "epoch no.4 train no.347770  loss = 3.22946 avg_loss = 3.25811\n",
      "epoch no.4 train no.347780  loss = 4.06474 avg_loss = 3.27008\n",
      "epoch no.4 train no.347790  loss = 3.83272 avg_loss = 3.27128\n",
      "epoch no.4 train no.347800  loss = 4.23217 avg_loss = 3.28395\n",
      "epoch no.4 train no.347810  loss = 2.88243 avg_loss = 3.27204\n",
      "epoch no.4 train no.347820  loss = 5.08834 avg_loss = 3.27387\n",
      "epoch no.4 train no.347830  loss = 3.11914 avg_loss = 3.24047\n",
      "epoch no.4 train no.347840  loss = 3.99186 avg_loss = 3.24374\n",
      "epoch no.4 train no.347850  loss = 2.24558 avg_loss = 3.20660\n",
      "epoch no.4 train no.347860  loss = 4.07602 avg_loss = 3.21469\n",
      "epoch no.4 train no.347870  loss = 2.29749 avg_loss = 3.20613\n",
      "epoch no.4 train no.347880  loss = 2.56307 avg_loss = 3.21665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.347890  loss = 3.54968 avg_loss = 3.25540\n",
      "epoch no.4 train no.347900  loss = 3.07548 avg_loss = 3.26052\n",
      "epoch no.4 train no.347910  loss = 2.55631 avg_loss = 3.27091\n",
      "epoch no.4 train no.347920  loss = 2.52576 avg_loss = 3.24172\n",
      "epoch no.4 train no.347930  loss = 4.71673 avg_loss = 3.26571\n",
      "epoch no.4 train no.347940  loss = 3.12886 avg_loss = 3.29743\n",
      "epoch no.4 train no.347950  loss = 2.74803 avg_loss = 3.25707\n",
      "epoch no.4 train no.347960  loss = 3.16270 avg_loss = 3.24156\n",
      "epoch no.4 train no.347970  loss = 3.45934 avg_loss = 3.25678\n",
      "epoch no.4 train no.347980  loss = 3.39555 avg_loss = 3.21714\n",
      "epoch no.4 train no.347990  loss = 1.94211 avg_loss = 3.20970\n",
      "epoch no.4 train no.348000  loss = 4.31028 avg_loss = 3.20282\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '에', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.4 train no.348010  loss = 4.08470 avg_loss = 3.23420\n",
      "epoch no.4 train no.348020  loss = 3.18871 avg_loss = 3.21714\n",
      "epoch no.4 train no.348030  loss = 3.43092 avg_loss = 3.23778\n",
      "epoch no.4 train no.348040  loss = 2.85244 avg_loss = 3.24635\n",
      "epoch no.4 train no.348050  loss = 3.60287 avg_loss = 3.25658\n",
      "epoch no.4 train no.348060  loss = 3.43227 avg_loss = 3.22253\n",
      "epoch no.4 train no.348070  loss = 3.60937 avg_loss = 3.23964\n",
      "epoch no.4 train no.348080  loss = 3.74506 avg_loss = 3.24161\n",
      "epoch no.4 train no.348090  loss = 2.94278 avg_loss = 3.20968\n",
      "epoch no.4 train no.348100  loss = 2.63941 avg_loss = 3.21367\n",
      "epoch no.4 train no.348110  loss = 3.40839 avg_loss = 3.18873\n",
      "epoch no.4 train no.348120  loss = 4.00625 avg_loss = 3.20563\n",
      "epoch no.4 train no.348130  loss = 3.80367 avg_loss = 3.20912\n",
      "epoch no.4 train no.348140  loss = 1.81518 avg_loss = 3.21730\n",
      "epoch no.4 train no.348150  loss = 3.42163 avg_loss = 3.26704\n",
      "epoch no.4 train no.348160  loss = 4.01318 avg_loss = 3.30495\n",
      "epoch no.4 train no.348170  loss = 1.40049 avg_loss = 3.28960\n",
      "epoch no.4 train no.348180  loss = 4.04693 avg_loss = 3.32977\n",
      "epoch no.4 train no.348190  loss = 2.45606 avg_loss = 3.31279\n",
      "epoch no.4 train no.348200  loss = 2.38328 avg_loss = 3.29545\n",
      "epoch no.4 train no.348210  loss = 2.71677 avg_loss = 3.25465\n",
      "epoch no.4 train no.348220  loss = 2.16746 avg_loss = 3.22322\n",
      "epoch no.4 train no.348230  loss = 2.66669 avg_loss = 3.27278\n",
      "epoch no.4 train no.348240  loss = 3.02479 avg_loss = 3.27095\n",
      "epoch no.4 train no.348250  loss = 2.60890 avg_loss = 3.30636\n",
      "epoch no.4 train no.348260  loss = 3.45544 avg_loss = 3.29845\n",
      "epoch no.4 train no.348270  loss = 2.29474 avg_loss = 3.25170\n",
      "epoch no.4 train no.348280  loss = 3.82176 avg_loss = 3.29125\n",
      "epoch no.4 train no.348290  loss = 2.80074 avg_loss = 3.24577\n",
      "epoch no.4 train no.348300  loss = 5.41118 avg_loss = 3.29721\n",
      "epoch no.4 train no.348310  loss = 3.08418 avg_loss = 3.29452\n",
      "epoch no.4 train no.348320  loss = 2.56280 avg_loss = 3.28298\n",
      "epoch no.4 train no.348330  loss = 2.56975 avg_loss = 3.27578\n",
      "epoch no.4 train no.348340  loss = 3.36880 avg_loss = 3.22917\n",
      "epoch no.4 train no.348350  loss = 3.03286 avg_loss = 3.24957\n",
      "epoch no.4 train no.348360  loss = 2.32375 avg_loss = 3.25795\n",
      "epoch no.4 train no.348370  loss = 2.99097 avg_loss = 3.25422\n",
      "epoch no.4 train no.348380  loss = 2.82913 avg_loss = 3.25772\n",
      "epoch no.4 train no.348390  loss = 2.84382 avg_loss = 3.24836\n",
      "epoch no.4 train no.348400  loss = 3.85120 avg_loss = 3.29167\n",
      "epoch no.4 train no.348410  loss = 4.01036 avg_loss = 3.29399\n",
      "epoch no.4 train no.348420  loss = 3.42435 avg_loss = 3.29014\n",
      "epoch no.4 train no.348430  loss = 5.54654 avg_loss = 3.33130\n",
      "epoch no.4 train no.348440  loss = 3.51889 avg_loss = 3.30292\n",
      "epoch no.4 train no.348450  loss = 3.62452 avg_loss = 3.30590\n",
      "epoch no.4 train no.348460  loss = 4.68562 avg_loss = 3.34869\n",
      "epoch no.4 train no.348470  loss = 2.69199 avg_loss = 3.31086\n",
      "epoch no.4 train no.348480  loss = 3.21068 avg_loss = 3.31912\n",
      "epoch no.4 train no.348490  loss = 4.09173 avg_loss = 3.31430\n",
      "epoch no.4 train no.348500  loss = 3.85877 avg_loss = 3.32613\n",
      "epoch no.4 train no.348510  loss = 3.67389 avg_loss = 3.32432\n",
      "epoch no.4 train no.348520  loss = 4.38950 avg_loss = 3.32788\n",
      "epoch no.4 train no.348530  loss = 2.87078 avg_loss = 3.29514\n",
      "epoch no.4 train no.348540  loss = 2.89128 avg_loss = 3.28101\n",
      "epoch no.4 train no.348550  loss = 3.69820 avg_loss = 3.29787\n",
      "epoch no.4 train no.348560  loss = 2.63352 avg_loss = 3.29628\n",
      "epoch no.4 train no.348570  loss = 3.14526 avg_loss = 3.28780\n",
      "epoch no.4 train no.348580  loss = 3.15957 avg_loss = 3.25781\n",
      "epoch no.4 train no.348590  loss = 3.53781 avg_loss = 3.27032\n",
      "epoch no.4 train no.348600  loss = 2.72713 avg_loss = 3.24377\n",
      "epoch no.4 train no.348610  loss = 4.21542 avg_loss = 3.25792\n",
      "epoch no.4 train no.348620  loss = 4.05405 avg_loss = 3.23887\n",
      "epoch no.4 train no.348630  loss = 3.01756 avg_loss = 3.26894\n",
      "epoch no.4 train no.348640  loss = 2.53539 avg_loss = 3.21811\n",
      "epoch no.4 train no.348650  loss = 2.42180 avg_loss = 3.17419\n",
      "epoch no.4 train no.348660  loss = 4.38537 avg_loss = 3.22234\n",
      "epoch no.4 train no.348670  loss = 3.17467 avg_loss = 3.25255\n",
      "epoch no.4 train no.348680  loss = 3.79769 avg_loss = 3.27179\n",
      "epoch no.4 train no.348690  loss = 3.49938 avg_loss = 3.25642\n",
      "epoch no.4 train no.348700  loss = 3.40642 avg_loss = 3.25736\n",
      "epoch no.4 train no.348710  loss = 3.26961 avg_loss = 3.23474\n",
      "epoch no.4 train no.348720  loss = 2.62595 avg_loss = 3.25571\n",
      "epoch no.4 train no.348730  loss = 2.88950 avg_loss = 3.30231\n",
      "epoch no.4 train no.348740  loss = 4.01156 avg_loss = 3.31602\n",
      "epoch no.4 train no.348750  loss = 2.11873 avg_loss = 3.32037\n",
      "epoch no.4 train no.348760  loss = 3.33022 avg_loss = 3.31888\n",
      "epoch no.4 train no.348770  loss = 2.68178 avg_loss = 3.29557\n",
      "epoch no.4 train no.348780  loss = 3.99166 avg_loss = 3.31137\n",
      "epoch no.4 train no.348790  loss = 2.67185 avg_loss = 3.31160\n",
      "epoch no.4 train no.348800  loss = 3.28490 avg_loss = 3.30588\n",
      "epoch no.4 train no.348810  loss = 2.96279 avg_loss = 3.31787\n",
      "epoch no.4 train no.348820  loss = 2.49884 avg_loss = 3.31297\n",
      "epoch no.4 train no.348830  loss = 2.90057 avg_loss = 3.30277\n",
      "epoch no.4 train no.348840  loss = 2.67084 avg_loss = 3.32850\n",
      "epoch no.4 train no.348850  loss = 2.16372 avg_loss = 3.33201\n",
      "epoch no.4 train no.348860  loss = 4.00040 avg_loss = 3.32612\n",
      "epoch no.4 train no.348870  loss = 2.43743 avg_loss = 3.30192\n",
      "epoch no.4 train no.348880  loss = 3.66216 avg_loss = 3.29336\n",
      "epoch no.4 train no.348890  loss = 3.60992 avg_loss = 3.23825\n",
      "epoch no.4 train no.348900  loss = 1.90605 avg_loss = 3.21636\n",
      "epoch no.4 train no.348910  loss = 3.08918 avg_loss = 3.20700\n",
      "epoch no.4 train no.348920  loss = 3.56447 avg_loss = 3.22063\n",
      "epoch no.4 train no.348930  loss = 2.44135 avg_loss = 3.19284\n",
      "epoch no.4 train no.348940  loss = 4.97082 avg_loss = 3.20572\n",
      "epoch no.4 train no.348950  loss = 2.33620 avg_loss = 3.21995\n",
      "epoch no.4 train no.348960  loss = 2.12083 avg_loss = 3.22010\n",
      "epoch no.4 train no.348970  loss = 2.33976 avg_loss = 3.19930\n",
      "epoch no.4 train no.348980  loss = 2.61202 avg_loss = 3.22770\n",
      "epoch no.4 train no.348990  loss = 2.62722 avg_loss = 3.25717\n",
      "epoch no.4 train no.349000  loss = 2.21105 avg_loss = 3.22220\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '을', '▁위한', '▁팝', '▁음악', '▁노래', '</s>']\n",
      "기분전환을 위한 신나는 아이돌 노래</s>\n",
      "epoch no.4 train no.349010  loss = 3.19432 avg_loss = 3.21394\n",
      "epoch no.4 train no.349020  loss = 2.81579 avg_loss = 3.27255\n",
      "epoch no.4 train no.349030  loss = 4.74680 avg_loss = 3.32926\n",
      "epoch no.4 train no.349040  loss = 4.72358 avg_loss = 3.29068\n",
      "epoch no.4 train no.349050  loss = 2.72618 avg_loss = 3.32238\n",
      "epoch no.4 train no.349060  loss = 2.78214 avg_loss = 3.32763\n",
      "epoch no.4 train no.349070  loss = 3.66729 avg_loss = 3.33710\n",
      "epoch no.4 train no.349080  loss = 4.50606 avg_loss = 3.36407\n",
      "epoch no.4 train no.349090  loss = 1.85157 avg_loss = 3.34014\n",
      "epoch no.4 train no.349100  loss = 2.92934 avg_loss = 3.32088\n",
      "epoch no.4 train no.349110  loss = 2.03797 avg_loss = 3.28657\n",
      "epoch no.4 train no.349120  loss = 3.64879 avg_loss = 3.24541\n",
      "epoch no.4 train no.349130  loss = 3.71177 avg_loss = 3.24385\n",
      "epoch no.4 train no.349140  loss = 3.74540 avg_loss = 3.27451\n",
      "epoch no.4 train no.349150  loss = 3.24704 avg_loss = 3.28864\n",
      "epoch no.4 train no.349160  loss = 2.91503 avg_loss = 3.29997\n",
      "epoch no.4 train no.349170  loss = 3.94663 avg_loss = 3.29089\n",
      "epoch no.4 train no.349180  loss = 4.81669 avg_loss = 3.27898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.349190  loss = 3.67667 avg_loss = 3.28687\n",
      "epoch no.4 train no.349200  loss = 2.13104 avg_loss = 3.26401\n",
      "epoch no.4 train no.349210  loss = 3.68821 avg_loss = 3.24932\n",
      "epoch no.4 train no.349220  loss = 3.70835 avg_loss = 3.22315\n",
      "epoch no.4 train no.349230  loss = 3.09739 avg_loss = 3.25103\n",
      "epoch no.4 train no.349240  loss = 4.55245 avg_loss = 3.24243\n",
      "epoch no.4 train no.349250  loss = 2.88467 avg_loss = 3.26766\n",
      "epoch no.4 train no.349260  loss = 2.63292 avg_loss = 3.28218\n",
      "epoch no.4 train no.349270  loss = 3.46162 avg_loss = 3.27230\n",
      "epoch no.4 train no.349280  loss = 3.19075 avg_loss = 3.27267\n",
      "epoch no.4 train no.349290  loss = 2.83292 avg_loss = 3.25444\n",
      "epoch no.4 train no.349300  loss = 1.64591 avg_loss = 3.26299\n",
      "epoch no.4 train no.349310  loss = 2.37755 avg_loss = 3.26478\n",
      "epoch no.4 train no.349320  loss = 4.21819 avg_loss = 3.28111\n",
      "epoch no.4 train no.349330  loss = 2.45667 avg_loss = 3.25260\n",
      "epoch no.4 train no.349340  loss = 3.78793 avg_loss = 3.26600\n",
      "epoch no.4 train no.349350  loss = 4.61520 avg_loss = 3.29223\n",
      "epoch no.4 train no.349360  loss = 3.58645 avg_loss = 3.26778\n",
      "epoch no.4 train no.349370  loss = 1.74507 avg_loss = 3.24654\n",
      "epoch no.4 train no.349380  loss = 4.01963 avg_loss = 3.25402\n",
      "epoch no.4 train no.349390  loss = 3.40063 avg_loss = 3.31184\n",
      "epoch no.4 train no.349400  loss = 4.51993 avg_loss = 3.31726\n",
      "epoch no.4 train no.349410  loss = 2.67639 avg_loss = 3.30290\n",
      "epoch no.4 train no.349420  loss = 2.90092 avg_loss = 3.27262\n",
      "epoch no.4 train no.349430  loss = 4.91782 avg_loss = 3.27078\n",
      "epoch no.4 train no.349440  loss = 4.60084 avg_loss = 3.29769\n",
      "epoch no.4 train no.349450  loss = 2.65709 avg_loss = 3.29688\n",
      "epoch no.4 train no.349460  loss = 3.46057 avg_loss = 3.35507\n",
      "epoch no.4 train no.349470  loss = 3.65699 avg_loss = 3.33130\n",
      "epoch no.4 train no.349480  loss = 2.30830 avg_loss = 3.27561\n",
      "epoch no.4 train no.349490  loss = 3.25713 avg_loss = 3.29755\n",
      "epoch no.4 train no.349500  loss = 3.11793 avg_loss = 3.29755\n",
      "epoch no.4 train no.349510  loss = 3.81657 avg_loss = 3.32501\n",
      "epoch no.4 train no.349520  loss = 3.64079 avg_loss = 3.29013\n",
      "epoch no.4 train no.349530  loss = 4.15603 avg_loss = 3.32610\n",
      "epoch no.4 train no.349540  loss = 2.72962 avg_loss = 3.32980\n",
      "epoch no.4 train no.349550  loss = 4.61406 avg_loss = 3.33070\n",
      "epoch no.4 train no.349560  loss = 2.76570 avg_loss = 3.30605\n",
      "epoch no.4 train no.349570  loss = 2.37813 avg_loss = 3.30318\n",
      "epoch no.4 train no.349580  loss = 3.92622 avg_loss = 3.37467\n",
      "epoch no.4 train no.349590  loss = 3.34183 avg_loss = 3.36048\n",
      "epoch no.4 train no.349600  loss = 3.22462 avg_loss = 3.37888\n",
      "epoch no.4 train no.349610  loss = 4.29015 avg_loss = 3.36775\n",
      "epoch no.4 train no.349620  loss = 2.82924 avg_loss = 3.35986\n",
      "epoch no.4 train no.349630  loss = 2.92529 avg_loss = 3.36513\n",
      "epoch no.4 train no.349640  loss = 3.58324 avg_loss = 3.28965\n",
      "epoch no.4 train no.349650  loss = 3.75423 avg_loss = 3.28266\n",
      "epoch no.4 train no.349660  loss = 4.15992 avg_loss = 3.29711\n",
      "epoch no.4 train no.349670  loss = 2.67446 avg_loss = 3.25603\n",
      "epoch no.4 train no.349680  loss = 3.06089 avg_loss = 3.27280\n",
      "epoch no.4 train no.349690  loss = 2.36322 avg_loss = 3.28328\n",
      "epoch no.4 train no.349700  loss = 3.39411 avg_loss = 3.23100\n",
      "epoch no.4 train no.349710  loss = 3.33097 avg_loss = 3.26820\n",
      "epoch no.4 train no.349720  loss = 4.56792 avg_loss = 3.26700\n",
      "epoch no.4 train no.349730  loss = 3.01093 avg_loss = 3.27038\n",
      "epoch no.4 train no.349740  loss = 2.92925 avg_loss = 3.22017\n",
      "epoch no.4 train no.349750  loss = 4.63582 avg_loss = 3.24877\n",
      "epoch no.4 train no.349760  loss = 6.20997 avg_loss = 3.27310\n",
      "epoch no.4 train no.349770  loss = 3.61795 avg_loss = 3.27735\n",
      "epoch no.4 train no.349780  loss = 2.42248 avg_loss = 3.25339\n",
      "epoch no.4 train no.349790  loss = 3.44689 avg_loss = 3.30071\n",
      "epoch no.4 train no.349800  loss = 3.65242 avg_loss = 3.26693\n",
      "epoch no.4 train no.349810  loss = 4.41097 avg_loss = 3.28353\n",
      "epoch no.4 train no.349820  loss = 2.64999 avg_loss = 3.24973\n",
      "epoch no.4 train no.349830  loss = 2.26389 avg_loss = 3.24659\n",
      "epoch no.4 train no.349840  loss = 2.56871 avg_loss = 3.21098\n",
      "epoch no.4 train no.349850  loss = 3.72565 avg_loss = 3.24593\n",
      "epoch no.4 train no.349860  loss = 2.32356 avg_loss = 3.22223\n",
      "epoch no.4 train no.349870  loss = 2.53935 avg_loss = 3.24627\n",
      "epoch no.4 train no.349880  loss = 2.42409 avg_loss = 3.28200\n",
      "epoch no.4 train no.349890  loss = 2.24446 avg_loss = 3.25019\n",
      "epoch no.4 train no.349900  loss = 2.25309 avg_loss = 3.24609\n",
      "epoch no.4 train no.349910  loss = 4.12694 avg_loss = 3.30046\n",
      "epoch no.4 train no.349920  loss = 4.04606 avg_loss = 3.29770\n",
      "epoch no.4 train no.349930  loss = 2.23729 avg_loss = 3.26646\n",
      "epoch no.4 train no.349940  loss = 2.94962 avg_loss = 3.23345\n",
      "epoch no.4 train no.349950  loss = 4.55880 avg_loss = 3.24306\n",
      "epoch no.4 train no.349960  loss = 3.95465 avg_loss = 3.20418\n",
      "epoch no.4 train no.349970  loss = 3.04963 avg_loss = 3.23150\n",
      "epoch no.4 train no.349980  loss = 2.61410 avg_loss = 3.23950\n",
      "epoch no.4 train no.349990  loss = 2.72253 avg_loss = 3.28856\n",
      "epoch no.4 train no.350000  loss = 3.36262 avg_loss = 3.24174\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁노래', '</s>']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.4 train no.350010  loss = 1.96700 avg_loss = 3.20461\n",
      "epoch no.4 train no.350020  loss = 2.90957 avg_loss = 3.23907\n",
      "epoch no.4 train no.350030  loss = 3.02073 avg_loss = 3.27507\n",
      "epoch no.4 train no.350040  loss = 2.00753 avg_loss = 3.26183\n",
      "epoch no.4 train no.350050  loss = 3.03481 avg_loss = 3.27317\n",
      "epoch no.4 train no.350060  loss = 2.40517 avg_loss = 3.31804\n",
      "epoch no.4 train no.350070  loss = 2.05281 avg_loss = 3.35763\n",
      "epoch no.4 train no.350080  loss = 3.20721 avg_loss = 3.33480\n",
      "epoch no.4 train no.350090  loss = 2.90243 avg_loss = 3.34168\n",
      "epoch no.4 train no.350100  loss = 3.53441 avg_loss = 3.32309\n",
      "epoch no.4 train no.350110  loss = 2.86617 avg_loss = 3.31555\n",
      "epoch no.4 train no.350120  loss = 3.52063 avg_loss = 3.28032\n",
      "epoch no.4 train no.350130  loss = 2.55706 avg_loss = 3.27597\n",
      "epoch no.4 train no.350140  loss = 2.49750 avg_loss = 3.34858\n",
      "epoch no.4 train no.350150  loss = 3.36340 avg_loss = 3.33843\n",
      "epoch no.4 train no.350160  loss = 3.88300 avg_loss = 3.32780\n",
      "epoch no.4 train no.350170  loss = 2.80493 avg_loss = 3.35476\n",
      "epoch no.4 train no.350180  loss = 2.27093 avg_loss = 3.34862\n",
      "epoch no.4 train no.350190  loss = 3.03670 avg_loss = 3.34754\n",
      "epoch no.4 train no.350200  loss = 4.11357 avg_loss = 3.35561\n",
      "epoch no.4 train no.350210  loss = 2.48480 avg_loss = 3.34835\n",
      "epoch no.4 train no.350220  loss = 3.01082 avg_loss = 3.36789\n",
      "epoch no.4 train no.350230  loss = 2.87626 avg_loss = 3.37813\n",
      "epoch no.4 train no.350240  loss = 4.44726 avg_loss = 3.38389\n",
      "epoch no.4 train no.350250  loss = 2.15483 avg_loss = 3.33873\n",
      "epoch no.4 train no.350260  loss = 3.06678 avg_loss = 3.32465\n",
      "epoch no.4 train no.350270  loss = 3.04087 avg_loss = 3.32104\n",
      "epoch no.4 train no.350280  loss = 3.14256 avg_loss = 3.33495\n",
      "epoch no.4 train no.350290  loss = 3.31849 avg_loss = 3.34039\n",
      "epoch no.4 train no.350300  loss = 4.98407 avg_loss = 3.38789\n",
      "epoch no.4 train no.350310  loss = 3.59293 avg_loss = 3.41242\n",
      "epoch no.4 train no.350320  loss = 3.22765 avg_loss = 3.37689\n",
      "epoch no.4 train no.350330  loss = 4.60072 avg_loss = 3.36779\n",
      "epoch no.4 train no.350340  loss = 2.88833 avg_loss = 3.38434\n",
      "epoch no.4 train no.350350  loss = 2.01642 avg_loss = 3.37401\n",
      "epoch no.4 train no.350360  loss = 2.91247 avg_loss = 3.38207\n",
      "epoch no.4 train no.350370  loss = 2.85499 avg_loss = 3.36319\n",
      "epoch no.4 train no.350380  loss = 3.08451 avg_loss = 3.33452\n",
      "epoch no.4 train no.350390  loss = 2.64031 avg_loss = 3.33423\n",
      "epoch no.4 train no.350400  loss = 3.76622 avg_loss = 3.36917\n",
      "epoch no.4 train no.350410  loss = 2.28387 avg_loss = 3.33843\n",
      "epoch no.4 train no.350420  loss = 2.98704 avg_loss = 3.30851\n",
      "epoch no.4 train no.350430  loss = 4.71006 avg_loss = 3.30327\n",
      "epoch no.4 train no.350440  loss = 3.35608 avg_loss = 3.31312\n",
      "epoch no.4 train no.350450  loss = 2.51440 avg_loss = 3.23851\n",
      "epoch no.4 train no.350460  loss = 3.39330 avg_loss = 3.29200\n",
      "epoch no.4 train no.350470  loss = 2.97214 avg_loss = 3.29539\n",
      "epoch no.4 train no.350480  loss = 2.58683 avg_loss = 3.31607\n",
      "epoch no.4 train no.350490  loss = 2.34078 avg_loss = 3.28471\n",
      "epoch no.4 train no.350500  loss = 2.83946 avg_loss = 3.30067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.350510  loss = 2.68411 avg_loss = 3.26906\n",
      "epoch no.4 train no.350520  loss = 3.67343 avg_loss = 3.26916\n",
      "epoch no.4 train no.350530  loss = 3.12976 avg_loss = 3.23585\n",
      "epoch no.4 train no.350540  loss = 3.05591 avg_loss = 3.25126\n",
      "epoch no.4 train no.350550  loss = 4.26122 avg_loss = 3.27785\n",
      "epoch no.4 train no.350560  loss = 5.04076 avg_loss = 3.27079\n",
      "epoch no.4 train no.350570  loss = 1.40740 avg_loss = 3.21112\n",
      "epoch no.4 train no.350580  loss = 3.02859 avg_loss = 3.20812\n",
      "epoch no.4 train no.350590  loss = 3.58863 avg_loss = 3.24824\n",
      "epoch no.4 train no.350600  loss = 2.87916 avg_loss = 3.27770\n",
      "epoch no.4 train no.350610  loss = 2.91156 avg_loss = 3.23092\n",
      "epoch no.4 train no.350620  loss = 3.81290 avg_loss = 3.25094\n",
      "epoch no.4 train no.350630  loss = 2.92669 avg_loss = 3.27315\n",
      "epoch no.4 train no.350640  loss = 4.08174 avg_loss = 3.28349\n",
      "epoch no.4 train no.350650  loss = 3.84548 avg_loss = 3.25106\n",
      "epoch no.4 train no.350660  loss = 1.83219 avg_loss = 3.22571\n",
      "epoch no.4 train no.350670  loss = 4.15900 avg_loss = 3.21833\n",
      "epoch no.4 train no.350680  loss = 3.40397 avg_loss = 3.23137\n",
      "epoch no.4 train no.350690  loss = 2.30709 avg_loss = 3.17884\n",
      "epoch no.4 train no.350700  loss = 3.86969 avg_loss = 3.22049\n",
      "epoch no.4 train no.350710  loss = 3.67954 avg_loss = 3.25357\n",
      "epoch no.4 train no.350720  loss = 4.02667 avg_loss = 3.24321\n",
      "epoch no.4 train no.350730  loss = 5.06820 avg_loss = 3.25021\n",
      "epoch no.4 train no.350740  loss = 4.00797 avg_loss = 3.25959\n",
      "epoch no.4 train no.350750  loss = 3.79487 avg_loss = 3.28538\n",
      "epoch no.4 train no.350760  loss = 3.41243 avg_loss = 3.31300\n",
      "epoch no.4 train no.350770  loss = 4.66323 avg_loss = 3.33404\n",
      "epoch no.4 train no.350780  loss = 1.58678 avg_loss = 3.30631\n",
      "epoch no.4 train no.350790  loss = 3.11420 avg_loss = 3.28974\n",
      "epoch no.4 train no.350800  loss = 3.99825 avg_loss = 3.29996\n",
      "epoch no.4 train no.350810  loss = 2.67057 avg_loss = 3.28541\n",
      "epoch no.4 train no.350820  loss = 4.84195 avg_loss = 3.30698\n",
      "epoch no.4 train no.350830  loss = 4.19096 avg_loss = 3.31457\n",
      "epoch no.4 train no.350840  loss = 2.98994 avg_loss = 3.31418\n",
      "epoch no.4 train no.350850  loss = 2.55136 avg_loss = 3.26696\n",
      "epoch no.4 train no.350860  loss = 3.19204 avg_loss = 3.28126\n",
      "epoch no.4 train no.350870  loss = 3.85253 avg_loss = 3.23663\n",
      "epoch no.4 train no.350880  loss = 1.84107 avg_loss = 3.21815\n",
      "epoch no.4 train no.350890  loss = 2.94039 avg_loss = 3.20438\n",
      "epoch no.4 train no.350900  loss = 3.18430 avg_loss = 3.18554\n",
      "epoch no.4 train no.350910  loss = 4.06940 avg_loss = 3.19915\n",
      "epoch no.4 train no.350920  loss = 4.68591 avg_loss = 3.19148\n",
      "epoch no.4 train no.350930  loss = 3.19513 avg_loss = 3.18678\n",
      "epoch no.4 train no.350940  loss = 5.06179 avg_loss = 3.17637\n",
      "epoch no.4 train no.350950  loss = 2.71242 avg_loss = 3.20198\n",
      "epoch no.4 train no.350960  loss = 6.61338 avg_loss = 3.27568\n",
      "epoch no.4 train no.350970  loss = 3.04808 avg_loss = 3.28273\n",
      "epoch no.4 train no.350980  loss = 2.97058 avg_loss = 3.27718\n",
      "epoch no.4 train no.350990  loss = 3.67416 avg_loss = 3.28763\n",
      "epoch no.4 train no.351000  loss = 2.91106 avg_loss = 3.28132\n",
      "3\n",
      "to_tokens: ['▁비', '좋은', '이', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.4 train no.351010  loss = 3.84182 avg_loss = 3.29788\n",
      "epoch no.4 train no.351020  loss = 2.96773 avg_loss = 3.32155\n",
      "epoch no.4 train no.351030  loss = 3.49257 avg_loss = 3.29046\n",
      "epoch no.4 train no.351040  loss = 3.05324 avg_loss = 3.28785\n",
      "epoch no.4 train no.351050  loss = 3.15407 avg_loss = 3.30551\n",
      "epoch no.4 train no.351060  loss = 3.09476 avg_loss = 3.30658\n",
      "epoch no.4 train no.351070  loss = 3.35257 avg_loss = 3.29772\n",
      "epoch no.4 train no.351080  loss = 3.82609 avg_loss = 3.30215\n",
      "epoch no.4 train no.351090  loss = 3.11667 avg_loss = 3.32715\n",
      "epoch no.4 train no.351100  loss = 1.93632 avg_loss = 3.30095\n",
      "epoch no.4 train no.351110  loss = 3.69823 avg_loss = 3.33872\n",
      "epoch no.4 train no.351120  loss = 2.51362 avg_loss = 3.36221\n",
      "epoch no.4 train no.351130  loss = 4.61815 avg_loss = 3.34015\n",
      "epoch no.4 train no.351140  loss = 2.50241 avg_loss = 3.32915\n",
      "epoch no.4 train no.351150  loss = 3.75887 avg_loss = 3.31645\n",
      "epoch no.4 train no.351160  loss = 3.03482 avg_loss = 3.30867\n",
      "epoch no.4 train no.351170  loss = 2.60512 avg_loss = 3.27012\n",
      "epoch no.4 train no.351180  loss = 4.07464 avg_loss = 3.28711\n",
      "epoch no.4 train no.351190  loss = 5.60093 avg_loss = 3.27828\n",
      "epoch no.4 train no.351200  loss = 2.64839 avg_loss = 3.28043\n",
      "epoch no.4 train no.351210  loss = 2.70600 avg_loss = 3.28226\n",
      "epoch no.4 train no.351220  loss = 4.92511 avg_loss = 3.27545\n",
      "epoch no.4 train no.351230  loss = 2.09891 avg_loss = 3.26906\n",
      "epoch no.4 train no.351240  loss = 4.85982 avg_loss = 3.29201\n",
      "epoch no.4 train no.351250  loss = 5.38514 avg_loss = 3.27604\n",
      "epoch no.4 train no.351260  loss = 4.65720 avg_loss = 3.26334\n",
      "epoch no.4 train no.351270  loss = 2.05224 avg_loss = 3.25489\n",
      "epoch no.4 train no.351280  loss = 2.85330 avg_loss = 3.24046\n",
      "epoch no.4 train no.351290  loss = 2.12980 avg_loss = 3.21226\n",
      "epoch no.4 train no.351300  loss = 4.82258 avg_loss = 3.21554\n",
      "epoch no.4 train no.351310  loss = 2.28293 avg_loss = 3.26940\n",
      "epoch no.4 train no.351320  loss = 3.18171 avg_loss = 3.24182\n",
      "epoch no.4 train no.351330  loss = 2.30364 avg_loss = 3.21485\n",
      "epoch no.4 train no.351340  loss = 2.76099 avg_loss = 3.18401\n",
      "epoch no.4 train no.351350  loss = 2.04294 avg_loss = 3.15709\n",
      "epoch no.4 train no.351360  loss = 3.03318 avg_loss = 3.15514\n",
      "epoch no.4 train no.351370  loss = 2.12249 avg_loss = 3.18262\n",
      "epoch no.4 train no.351380  loss = 3.82524 avg_loss = 3.19987\n",
      "epoch no.4 train no.351390  loss = 3.67241 avg_loss = 3.17800\n",
      "epoch no.4 train no.351400  loss = 2.27765 avg_loss = 3.16068\n",
      "epoch no.4 train no.351410  loss = 1.66592 avg_loss = 3.19177\n",
      "epoch no.4 train no.351420  loss = 3.34444 avg_loss = 3.21043\n",
      "epoch no.4 train no.351430  loss = 1.75302 avg_loss = 3.21236\n",
      "epoch no.4 train no.351440  loss = 4.16225 avg_loss = 3.20616\n",
      "epoch no.4 train no.351450  loss = 4.00335 avg_loss = 3.25312\n",
      "epoch no.4 train no.351460  loss = 3.51936 avg_loss = 3.26376\n",
      "epoch no.4 train no.351470  loss = 3.50849 avg_loss = 3.27276\n",
      "epoch no.4 train no.351480  loss = 1.92541 avg_loss = 3.28720\n",
      "epoch no.4 train no.351490  loss = 2.79312 avg_loss = 3.30339\n",
      "epoch no.4 train no.351500  loss = 4.11266 avg_loss = 3.32004\n",
      "epoch no.4 train no.351510  loss = 1.91620 avg_loss = 3.28799\n",
      "epoch no.4 train no.351520  loss = 2.37474 avg_loss = 3.29614\n",
      "epoch no.4 train no.351530  loss = 1.47502 avg_loss = 3.24122\n",
      "epoch no.4 train no.351540  loss = 2.17423 avg_loss = 3.26541\n",
      "epoch no.4 train no.351550  loss = 4.16752 avg_loss = 3.30342\n",
      "epoch no.4 train no.351560  loss = 2.72986 avg_loss = 3.28723\n",
      "epoch no.4 train no.351570  loss = 3.58002 avg_loss = 3.29139\n",
      "epoch no.4 train no.351580  loss = 3.23456 avg_loss = 3.25096\n",
      "epoch no.4 train no.351590  loss = 3.53439 avg_loss = 3.27102\n",
      "epoch no.4 train no.351600  loss = 3.02609 avg_loss = 3.27073\n",
      "epoch no.4 train no.351610  loss = 2.15743 avg_loss = 3.25948\n",
      "epoch no.4 train no.351620  loss = 3.07044 avg_loss = 3.27733\n",
      "epoch no.4 train no.351630  loss = 4.03563 avg_loss = 3.28750\n",
      "epoch no.4 train no.351640  loss = 3.85862 avg_loss = 3.27343\n",
      "epoch no.4 train no.351650  loss = 3.09979 avg_loss = 3.21788\n",
      "epoch no.4 train no.351660  loss = 2.09249 avg_loss = 3.22706\n",
      "epoch no.4 train no.351670  loss = 3.74355 avg_loss = 3.26959\n",
      "epoch no.4 train no.351680  loss = 2.85299 avg_loss = 3.22188\n",
      "epoch no.4 train no.351690  loss = 3.46861 avg_loss = 3.20891\n",
      "epoch no.4 train no.351700  loss = 2.62444 avg_loss = 3.24227\n",
      "epoch no.4 train no.351710  loss = 2.19199 avg_loss = 3.27126\n",
      "epoch no.4 train no.351720  loss = 3.61535 avg_loss = 3.29894\n",
      "epoch no.4 train no.351730  loss = 2.95723 avg_loss = 3.26772\n",
      "epoch no.4 train no.351740  loss = 3.88731 avg_loss = 3.28213\n",
      "epoch no.4 train no.351750  loss = 3.57285 avg_loss = 3.29136\n",
      "epoch no.4 train no.351760  loss = 3.79206 avg_loss = 3.23921\n",
      "epoch no.4 train no.351770  loss = 4.15069 avg_loss = 3.20272\n",
      "epoch no.4 train no.351780  loss = 2.78606 avg_loss = 3.21486\n",
      "epoch no.4 train no.351790  loss = 2.21888 avg_loss = 3.23882\n",
      "epoch no.4 train no.351800  loss = 3.03114 avg_loss = 3.23865\n",
      "epoch no.4 train no.351810  loss = 2.64859 avg_loss = 3.24871\n",
      "epoch no.4 train no.351820  loss = 3.18965 avg_loss = 3.26186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.351830  loss = 2.37016 avg_loss = 3.24456\n",
      "epoch no.4 train no.351840  loss = 5.28893 avg_loss = 3.28304\n",
      "epoch no.4 train no.351850  loss = 2.74878 avg_loss = 3.29340\n",
      "epoch no.4 train no.351860  loss = 3.82726 avg_loss = 3.31038\n",
      "epoch no.4 train no.351870  loss = 3.55295 avg_loss = 3.28113\n",
      "epoch no.4 train no.351880  loss = 3.45529 avg_loss = 3.26935\n",
      "epoch no.4 train no.351890  loss = 3.58809 avg_loss = 3.25441\n",
      "epoch no.4 train no.351900  loss = 1.74101 avg_loss = 3.23119\n",
      "epoch no.4 train no.351910  loss = 3.04623 avg_loss = 3.23416\n",
      "epoch no.4 train no.351920  loss = 4.10756 avg_loss = 3.22338\n",
      "epoch no.4 train no.351930  loss = 3.16599 avg_loss = 3.26894\n",
      "epoch no.4 train no.351940  loss = 2.42696 avg_loss = 3.26744\n",
      "epoch no.4 train no.351950  loss = 3.61431 avg_loss = 3.29237\n",
      "epoch no.4 train no.351960  loss = 2.95594 avg_loss = 3.24266\n",
      "epoch no.4 train no.351970  loss = 3.29824 avg_loss = 3.24899\n",
      "epoch no.4 train no.351980  loss = 3.34703 avg_loss = 3.21174\n",
      "epoch no.4 train no.351990  loss = 3.42502 avg_loss = 3.20219\n",
      "epoch no.4 train no.352000  loss = 2.31984 avg_loss = 3.19289\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '▁위한', '▁음악', 'op', '</s>']\n",
      "기분전환을 위한 pop</s>\n",
      "epoch no.4 train no.352010  loss = 2.73779 avg_loss = 3.22157\n",
      "epoch no.4 train no.352020  loss = 3.06252 avg_loss = 3.25680\n",
      "epoch no.4 train no.352030  loss = 3.44578 avg_loss = 3.23444\n",
      "epoch no.4 train no.352040  loss = 3.37464 avg_loss = 3.28249\n",
      "epoch no.4 train no.352050  loss = 3.08976 avg_loss = 3.25216\n",
      "epoch no.4 train no.352060  loss = 2.54307 avg_loss = 3.21623\n",
      "epoch no.4 train no.352070  loss = 2.05681 avg_loss = 3.24160\n",
      "epoch no.4 train no.352080  loss = 2.01199 avg_loss = 3.21024\n",
      "epoch no.4 train no.352090  loss = 4.59150 avg_loss = 3.19751\n",
      "epoch no.4 train no.352100  loss = 2.66311 avg_loss = 3.17204\n",
      "epoch no.4 train no.352110  loss = 2.55413 avg_loss = 3.13975\n",
      "epoch no.4 train no.352120  loss = 2.76045 avg_loss = 3.16198\n",
      "epoch no.4 train no.352130  loss = 5.04611 avg_loss = 3.20106\n",
      "epoch no.4 train no.352140  loss = 3.55658 avg_loss = 3.19211\n",
      "epoch no.4 train no.352150  loss = 2.79399 avg_loss = 3.16768\n",
      "epoch no.4 train no.352160  loss = 3.06575 avg_loss = 3.17970\n",
      "epoch no.4 train no.352170  loss = 2.51436 avg_loss = 3.19297\n",
      "epoch no.4 train no.352180  loss = 5.02778 avg_loss = 3.26744\n",
      "epoch no.4 train no.352190  loss = 3.32869 avg_loss = 3.30730\n",
      "epoch no.4 train no.352200  loss = 3.80342 avg_loss = 3.32633\n",
      "epoch no.4 train no.352210  loss = 2.57971 avg_loss = 3.29185\n",
      "epoch no.4 train no.352220  loss = 2.99809 avg_loss = 3.30742\n",
      "epoch no.4 train no.352230  loss = 2.77499 avg_loss = 3.31437\n",
      "epoch no.4 train no.352240  loss = 4.17595 avg_loss = 3.30163\n",
      "epoch no.4 train no.352250  loss = 2.02789 avg_loss = 3.27938\n",
      "epoch no.4 train no.352260  loss = 3.59058 avg_loss = 3.25916\n",
      "epoch no.4 train no.352270  loss = 3.94855 avg_loss = 3.23968\n",
      "epoch no.4 train no.352280  loss = 3.06813 avg_loss = 3.22623\n",
      "epoch no.4 train no.352290  loss = 2.93150 avg_loss = 3.25975\n",
      "epoch no.4 train no.352300  loss = 3.09307 avg_loss = 3.24985\n",
      "epoch no.4 train no.352310  loss = 2.77475 avg_loss = 3.23527\n",
      "epoch no.4 train no.352320  loss = 2.53755 avg_loss = 3.27288\n",
      "epoch no.4 train no.352330  loss = 2.81704 avg_loss = 3.24897\n",
      "epoch no.4 train no.352340  loss = 3.10870 avg_loss = 3.22895\n",
      "epoch no.4 train no.352350  loss = 2.78394 avg_loss = 3.25063\n",
      "epoch no.4 train no.352360  loss = 2.15780 avg_loss = 3.28257\n",
      "epoch no.4 train no.352370  loss = 2.77544 avg_loss = 3.29085\n",
      "epoch no.4 train no.352380  loss = 4.41098 avg_loss = 3.31717\n",
      "epoch no.4 train no.352390  loss = 2.22218 avg_loss = 3.28492\n",
      "epoch no.4 train no.352400  loss = 3.52674 avg_loss = 3.32828\n",
      "epoch no.4 train no.352410  loss = 3.20819 avg_loss = 3.29647\n",
      "epoch no.4 train no.352420  loss = 3.74256 avg_loss = 3.29657\n",
      "epoch no.4 train no.352430  loss = 2.21112 avg_loss = 3.28755\n",
      "epoch no.4 train no.352440  loss = 1.54492 avg_loss = 3.27245\n",
      "epoch no.4 train no.352450  loss = 3.49547 avg_loss = 3.34055\n",
      "epoch no.4 train no.352460  loss = 2.68782 avg_loss = 3.30723\n",
      "epoch no.4 train no.352470  loss = 2.97049 avg_loss = 3.30620\n",
      "epoch no.4 train no.352480  loss = 4.64573 avg_loss = 3.30320\n",
      "epoch no.4 train no.352490  loss = 2.99541 avg_loss = 3.30910\n",
      "epoch no.4 train no.352500  loss = 4.93538 avg_loss = 3.33561\n",
      "epoch no.4 train no.352510  loss = 4.62299 avg_loss = 3.29689\n",
      "epoch no.4 train no.352520  loss = 4.97295 avg_loss = 3.30761\n",
      "epoch no.4 train no.352530  loss = 4.37250 avg_loss = 3.32539\n",
      "epoch no.4 train no.352540  loss = 3.58817 avg_loss = 3.33242\n",
      "epoch no.4 train no.352550  loss = 3.80929 avg_loss = 3.33915\n",
      "epoch no.4 train no.352560  loss = 2.42159 avg_loss = 3.29720\n",
      "epoch no.4 train no.352570  loss = 3.19888 avg_loss = 3.28579\n",
      "epoch no.4 train no.352580  loss = 4.42333 avg_loss = 3.28639\n",
      "epoch no.4 train no.352590  loss = 2.60976 avg_loss = 3.29779\n",
      "epoch no.4 train no.352600  loss = 3.49532 avg_loss = 3.27230\n",
      "epoch no.4 train no.352610  loss = 2.49924 avg_loss = 3.20036\n",
      "epoch no.4 train no.352620  loss = 3.74684 avg_loss = 3.21355\n",
      "epoch no.4 train no.352630  loss = 2.78035 avg_loss = 3.21029\n",
      "epoch no.4 train no.352640  loss = 4.86794 avg_loss = 3.21417\n",
      "epoch no.4 train no.352650  loss = 3.24231 avg_loss = 3.21655\n",
      "epoch no.4 train no.352660  loss = 3.80972 avg_loss = 3.21179\n",
      "epoch no.4 train no.352670  loss = 3.14190 avg_loss = 3.21907\n",
      "epoch no.4 train no.352680  loss = 2.31142 avg_loss = 3.20073\n",
      "epoch no.4 train no.352690  loss = 2.21355 avg_loss = 3.17692\n",
      "epoch no.4 train no.352700  loss = 2.13461 avg_loss = 3.18763\n",
      "epoch no.4 train no.352710  loss = 3.72060 avg_loss = 3.20403\n",
      "epoch no.4 train no.352720  loss = 3.26010 avg_loss = 3.24852\n",
      "epoch no.4 train no.352730  loss = 4.23903 avg_loss = 3.24575\n",
      "epoch no.4 train no.352740  loss = 3.80761 avg_loss = 3.30460\n",
      "epoch no.4 train no.352750  loss = 3.44683 avg_loss = 3.28659\n",
      "epoch no.4 train no.352760  loss = 3.92391 avg_loss = 3.28885\n",
      "epoch no.4 train no.352770  loss = 3.05163 avg_loss = 3.28647\n",
      "epoch no.4 train no.352780  loss = 2.49758 avg_loss = 3.27515\n",
      "epoch no.4 train no.352790  loss = 3.76198 avg_loss = 3.29411\n",
      "epoch no.4 train no.352800  loss = 3.32662 avg_loss = 3.29946\n",
      "epoch no.4 train no.352810  loss = 2.65960 avg_loss = 3.29033\n",
      "epoch no.4 train no.352820  loss = 2.56593 avg_loss = 3.25314\n",
      "epoch no.4 train no.352830  loss = 2.88506 avg_loss = 3.26117\n",
      "epoch no.4 train no.352840  loss = 2.89561 avg_loss = 3.27000\n",
      "epoch no.4 train no.352850  loss = 3.02777 avg_loss = 3.25083\n",
      "epoch no.4 train no.352860  loss = 4.00772 avg_loss = 3.28757\n",
      "epoch no.4 train no.352870  loss = 3.89316 avg_loss = 3.28566\n",
      "epoch no.4 train no.352880  loss = 2.83126 avg_loss = 3.30026\n",
      "epoch no.4 train no.352890  loss = 5.07961 avg_loss = 3.30473\n",
      "epoch no.4 train no.352900  loss = 2.74469 avg_loss = 3.32676\n",
      "epoch no.4 train no.352910  loss = 2.46069 avg_loss = 3.29476\n",
      "epoch no.4 train no.352920  loss = 1.59763 avg_loss = 3.27090\n",
      "epoch no.4 train no.352930  loss = 4.08112 avg_loss = 3.29524\n",
      "epoch no.4 train no.352940  loss = 2.69819 avg_loss = 3.33891\n",
      "epoch no.4 train no.352950  loss = 2.96202 avg_loss = 3.30883\n",
      "epoch no.4 train no.352960  loss = 2.53706 avg_loss = 3.31327\n",
      "epoch no.4 train no.352970  loss = 2.51319 avg_loss = 3.28082\n",
      "epoch no.4 train no.352980  loss = 3.50592 avg_loss = 3.25696\n",
      "epoch no.4 train no.352990  loss = 2.56706 avg_loss = 3.27017\n",
      "epoch no.4 train no.353000  loss = 1.16525 avg_loss = 3.24881\n",
      "2\n",
      "to_tokens: ['▁비', '전환', '을', '송', '</s>']\n",
      "기분전환 팝송</s>\n",
      "epoch no.4 train no.353010  loss = 4.32670 avg_loss = 3.27401\n",
      "epoch no.4 train no.353020  loss = 2.61008 avg_loss = 3.25599\n",
      "epoch no.4 train no.353030  loss = 2.77366 avg_loss = 3.27406\n",
      "epoch no.4 train no.353040  loss = 4.31290 avg_loss = 3.27470\n",
      "epoch no.4 train no.353050  loss = 2.91343 avg_loss = 3.27832\n",
      "epoch no.4 train no.353060  loss = 3.98060 avg_loss = 3.31349\n",
      "epoch no.4 train no.353070  loss = 3.53488 avg_loss = 3.33733\n",
      "epoch no.4 train no.353080  loss = 4.60671 avg_loss = 3.32196\n",
      "epoch no.4 train no.353090  loss = 3.21786 avg_loss = 3.27954\n",
      "epoch no.4 train no.353100  loss = 2.96710 avg_loss = 3.26639\n",
      "epoch no.4 train no.353110  loss = 4.21290 avg_loss = 3.27599\n",
      "epoch no.4 train no.353120  loss = 2.70235 avg_loss = 3.29353\n",
      "epoch no.4 train no.353130  loss = 2.68442 avg_loss = 3.27286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.353140  loss = 3.43340 avg_loss = 3.31974\n",
      "epoch no.4 train no.353150  loss = 2.91531 avg_loss = 3.33924\n",
      "epoch no.4 train no.353160  loss = 3.18008 avg_loss = 3.38825\n",
      "epoch no.4 train no.353170  loss = 2.95118 avg_loss = 3.37737\n",
      "epoch no.4 train no.353180  loss = 3.76046 avg_loss = 3.36489\n",
      "epoch no.4 train no.353190  loss = 2.93509 avg_loss = 3.35494\n",
      "epoch no.4 train no.353200  loss = 3.47084 avg_loss = 3.30080\n",
      "epoch no.4 train no.353210  loss = 5.31877 avg_loss = 3.29449\n",
      "epoch no.4 train no.353220  loss = 2.85491 avg_loss = 3.28856\n",
      "epoch no.4 train no.353230  loss = 2.45524 avg_loss = 3.32090\n",
      "epoch no.4 train no.353240  loss = 2.66091 avg_loss = 3.30182\n",
      "epoch no.4 train no.353250  loss = 3.38121 avg_loss = 3.29794\n",
      "epoch no.4 train no.353260  loss = 2.73626 avg_loss = 3.28165\n",
      "epoch no.4 train no.353270  loss = 2.85232 avg_loss = 3.25697\n",
      "epoch no.4 train no.353280  loss = 3.48537 avg_loss = 3.23608\n",
      "epoch no.4 train no.353290  loss = 5.25277 avg_loss = 3.24529\n",
      "epoch no.4 train no.353300  loss = 3.06083 avg_loss = 3.25719\n",
      "epoch no.4 train no.353310  loss = 1.81173 avg_loss = 3.22276\n",
      "epoch no.4 train no.353320  loss = 2.47984 avg_loss = 3.20271\n",
      "epoch no.4 train no.353330  loss = 5.20459 avg_loss = 3.25061\n",
      "epoch no.4 train no.353340  loss = 3.75553 avg_loss = 3.24677\n",
      "epoch no.4 train no.353350  loss = 2.41484 avg_loss = 3.27032\n",
      "epoch no.4 train no.353360  loss = 3.57738 avg_loss = 3.29734\n",
      "epoch no.4 train no.353370  loss = 3.03619 avg_loss = 3.28855\n",
      "epoch no.4 train no.353380  loss = 2.55038 avg_loss = 3.24775\n",
      "epoch no.4 train no.353390  loss = 3.08996 avg_loss = 3.25951\n",
      "epoch no.4 train no.353400  loss = 2.37735 avg_loss = 3.27327\n",
      "epoch no.4 train no.353410  loss = 4.82151 avg_loss = 3.26858\n",
      "epoch no.4 train no.353420  loss = 3.08195 avg_loss = 3.27823\n",
      "epoch no.4 train no.353430  loss = 3.86726 avg_loss = 3.30306\n",
      "epoch no.4 train no.353440  loss = 2.79339 avg_loss = 3.26967\n",
      "epoch no.4 train no.353450  loss = 2.14251 avg_loss = 3.25874\n",
      "epoch no.4 train no.353460  loss = 4.99477 avg_loss = 3.24917\n",
      "epoch no.4 train no.353470  loss = 2.85714 avg_loss = 3.27638\n",
      "epoch no.4 train no.353480  loss = 3.49599 avg_loss = 3.32747\n",
      "epoch no.4 train no.353490  loss = 3.35019 avg_loss = 3.29327\n",
      "epoch no.4 train no.353500  loss = 4.69181 avg_loss = 3.28731\n",
      "epoch no.4 train no.353510  loss = 2.78370 avg_loss = 3.26726\n",
      "epoch no.4 train no.353520  loss = 2.87946 avg_loss = 3.24980\n",
      "epoch no.4 train no.353530  loss = 3.81077 avg_loss = 3.24469\n",
      "epoch no.4 train no.353540  loss = 4.59949 avg_loss = 3.30332\n",
      "epoch no.4 train no.353550  loss = 3.64753 avg_loss = 3.29616\n",
      "epoch no.4 train no.353560  loss = 3.60976 avg_loss = 3.27673\n",
      "epoch no.4 train no.353570  loss = 2.99081 avg_loss = 3.29048\n",
      "epoch no.4 train no.353580  loss = 2.89390 avg_loss = 3.20918\n",
      "epoch no.4 train no.353590  loss = 2.09230 avg_loss = 3.19086\n",
      "epoch no.4 train no.353600  loss = 2.07539 avg_loss = 3.20438\n",
      "epoch no.4 train no.353610  loss = 3.65439 avg_loss = 3.22500\n",
      "epoch no.4 train no.353620  loss = 3.68607 avg_loss = 3.24252\n",
      "epoch no.4 train no.353630  loss = 2.85517 avg_loss = 3.26700\n",
      "epoch no.4 train no.353640  loss = 3.58067 avg_loss = 3.28268\n",
      "epoch no.4 train no.353650  loss = 3.77171 avg_loss = 3.25197\n",
      "epoch no.4 train no.353660  loss = 3.01933 avg_loss = 3.24051\n",
      "epoch no.4 train no.353670  loss = 4.69564 avg_loss = 3.24933\n",
      "epoch no.4 train no.353680  loss = 3.46443 avg_loss = 3.25812\n",
      "epoch no.4 train no.353690  loss = 2.70478 avg_loss = 3.25349\n",
      "epoch no.4 train no.353700  loss = 3.23492 avg_loss = 3.28618\n",
      "epoch no.4 train no.353710  loss = 3.20569 avg_loss = 3.29140\n",
      "epoch no.4 train no.353720  loss = 2.83405 avg_loss = 3.26237\n",
      "epoch no.4 train no.353730  loss = 3.92177 avg_loss = 3.24757\n",
      "epoch no.4 train no.353740  loss = 3.57316 avg_loss = 3.20094\n",
      "epoch no.4 train no.353750  loss = 3.08966 avg_loss = 3.21376\n",
      "epoch no.4 train no.353760  loss = 2.96361 avg_loss = 3.22445\n",
      "epoch no.4 train no.353770  loss = 2.93196 avg_loss = 3.19880\n",
      "epoch no.4 train no.353780  loss = 2.66324 avg_loss = 3.22497\n",
      "epoch no.4 train no.353790  loss = 2.62625 avg_loss = 3.28170\n",
      "epoch no.4 train no.353800  loss = 2.88071 avg_loss = 3.22094\n",
      "epoch no.4 train no.353810  loss = 4.42621 avg_loss = 3.23745\n",
      "epoch no.4 train no.353820  loss = 3.53027 avg_loss = 3.24467\n",
      "epoch no.4 train no.353830  loss = 4.41091 avg_loss = 3.23862\n",
      "epoch no.4 train no.353840  loss = 3.79260 avg_loss = 3.25006\n",
      "epoch no.4 train no.353850  loss = 2.81379 avg_loss = 3.25387\n",
      "epoch no.4 train no.353860  loss = 5.86615 avg_loss = 3.26927\n",
      "epoch no.4 train no.353870  loss = 3.38679 avg_loss = 3.24612\n",
      "epoch no.4 train no.353880  loss = 2.33883 avg_loss = 3.25271\n",
      "epoch no.4 train no.353890  loss = 1.92217 avg_loss = 3.26918\n",
      "epoch no.4 train no.353900  loss = 3.39464 avg_loss = 3.24618\n",
      "epoch no.4 train no.353910  loss = 3.02513 avg_loss = 3.23189\n",
      "epoch no.4 train no.353920  loss = 2.27273 avg_loss = 3.24662\n",
      "epoch no.4 train no.353930  loss = 3.11576 avg_loss = 3.26376\n",
      "epoch no.4 train no.353940  loss = 4.27724 avg_loss = 3.25434\n",
      "epoch no.4 train no.353950  loss = 4.10554 avg_loss = 3.27429\n",
      "epoch no.4 train no.353960  loss = 3.11585 avg_loss = 3.20515\n",
      "epoch no.4 train no.353970  loss = 2.53693 avg_loss = 3.17269\n",
      "epoch no.4 train no.353980  loss = 2.69777 avg_loss = 3.20587\n",
      "epoch no.4 train no.353990  loss = 1.83966 avg_loss = 3.18321\n",
      "epoch no.4 train no.354000  loss = 2.64220 avg_loss = 3.16451\n",
      "4\n",
      "to_tokens: ['▁라디오', '좋은', '을', '▁위한', '▁팝', '▁팝', '</s>']\n",
      "기분전환을 위한 드라이브 팝</s>\n",
      "epoch no.4 train no.354010  loss = 2.34973 avg_loss = 3.13523\n",
      "epoch no.4 train no.354020  loss = 2.60342 avg_loss = 3.18139\n",
      "epoch no.4 train no.354030  loss = 3.16550 avg_loss = 3.24564\n",
      "epoch no.4 train no.354040  loss = 2.08307 avg_loss = 3.24779\n",
      "epoch no.4 train no.354050  loss = 4.03482 avg_loss = 3.30701\n",
      "epoch no.4 train no.354060  loss = 3.22203 avg_loss = 3.28923\n",
      "epoch no.4 train no.354070  loss = 3.35463 avg_loss = 3.29639\n",
      "epoch no.4 train no.354080  loss = 4.13862 avg_loss = 3.33662\n",
      "epoch no.4 train no.354090  loss = 2.97230 avg_loss = 3.34302\n",
      "epoch no.4 train no.354100  loss = 3.96413 avg_loss = 3.31781\n",
      "epoch no.4 train no.354110  loss = 3.43625 avg_loss = 3.31412\n",
      "epoch no.4 train no.354120  loss = 2.25494 avg_loss = 3.32710\n",
      "epoch no.4 train no.354130  loss = 3.22728 avg_loss = 3.35216\n",
      "epoch no.4 train no.354140  loss = 2.80732 avg_loss = 3.31642\n",
      "epoch no.4 train no.354150  loss = 1.80890 avg_loss = 3.29645\n",
      "epoch no.4 train no.354160  loss = 6.34135 avg_loss = 3.32053\n",
      "epoch no.4 train no.354170  loss = 2.84346 avg_loss = 3.30159\n",
      "epoch no.4 train no.354180  loss = 3.26227 avg_loss = 3.28892\n",
      "epoch no.4 train no.354190  loss = 3.29593 avg_loss = 3.25374\n",
      "epoch no.4 train no.354200  loss = 3.20556 avg_loss = 3.30666\n",
      "epoch no.4 train no.354210  loss = 2.97117 avg_loss = 3.29639\n",
      "epoch no.4 train no.354220  loss = 3.96607 avg_loss = 3.28830\n",
      "epoch no.4 train no.354230  loss = 3.98741 avg_loss = 3.32753\n",
      "epoch no.4 train no.354240  loss = 4.48673 avg_loss = 3.26635\n",
      "epoch no.4 train no.354250  loss = 1.22456 avg_loss = 3.24581\n",
      "epoch no.4 train no.354260  loss = 3.71187 avg_loss = 3.24640\n",
      "epoch no.4 train no.354270  loss = 4.56061 avg_loss = 3.28839\n",
      "epoch no.4 train no.354280  loss = 3.18831 avg_loss = 3.25360\n",
      "epoch no.4 train no.354290  loss = 3.10717 avg_loss = 3.27432\n",
      "epoch no.4 train no.354300  loss = 3.20335 avg_loss = 3.28447\n",
      "epoch no.4 train no.354310  loss = 3.14665 avg_loss = 3.25228\n",
      "epoch no.4 train no.354320  loss = 1.83803 avg_loss = 3.24071\n",
      "epoch no.4 train no.354330  loss = 6.45709 avg_loss = 3.29650\n",
      "epoch no.4 train no.354340  loss = 3.46284 avg_loss = 3.31627\n",
      "epoch no.4 train no.354350  loss = 4.47569 avg_loss = 3.33335\n",
      "epoch no.4 train no.354360  loss = 2.02290 avg_loss = 3.28497\n",
      "epoch no.4 train no.354370  loss = 2.38853 avg_loss = 3.28823\n",
      "epoch no.4 train no.354380  loss = 3.58090 avg_loss = 3.33904\n",
      "epoch no.4 train no.354390  loss = 4.67683 avg_loss = 3.31058\n",
      "epoch no.4 train no.354400  loss = 3.10897 avg_loss = 3.32086\n",
      "epoch no.4 train no.354410  loss = 1.52129 avg_loss = 3.31855\n",
      "epoch no.4 train no.354420  loss = 3.18187 avg_loss = 3.30974\n",
      "epoch no.4 train no.354430  loss = 5.01326 avg_loss = 3.28551\n",
      "epoch no.4 train no.354440  loss = 3.38536 avg_loss = 3.30093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.354450  loss = 4.55382 avg_loss = 3.35569\n",
      "epoch no.4 train no.354460  loss = 3.49757 avg_loss = 3.36891\n",
      "epoch no.4 train no.354470  loss = 2.59218 avg_loss = 3.32167\n",
      "epoch no.4 train no.354480  loss = 4.69239 avg_loss = 3.38381\n",
      "epoch no.4 train no.354490  loss = 2.35515 avg_loss = 3.32697\n",
      "epoch no.4 train no.354500  loss = 4.24130 avg_loss = 3.33458\n",
      "epoch no.4 train no.354510  loss = 3.29759 avg_loss = 3.34615\n",
      "epoch no.4 train no.354520  loss = 3.18401 avg_loss = 3.33675\n",
      "epoch no.4 train no.354530  loss = 4.19081 avg_loss = 3.35733\n",
      "epoch no.4 train no.354540  loss = 2.79796 avg_loss = 3.35863\n",
      "epoch no.4 train no.354550  loss = 2.52081 avg_loss = 3.31502\n",
      "epoch no.4 train no.354560  loss = 1.99156 avg_loss = 3.29500\n",
      "epoch no.4 train no.354570  loss = 2.43034 avg_loss = 3.32486\n",
      "epoch no.4 train no.354580  loss = 1.88801 avg_loss = 3.31814\n",
      "epoch no.4 train no.354590  loss = 3.53338 avg_loss = 3.30935\n",
      "epoch no.4 train no.354600  loss = 2.97611 avg_loss = 3.27626\n",
      "epoch no.4 train no.354610  loss = 2.71642 avg_loss = 3.25592\n",
      "epoch no.4 train no.354620  loss = 4.93746 avg_loss = 3.26629\n",
      "epoch no.4 train no.354630  loss = 2.82284 avg_loss = 3.22823\n",
      "epoch no.4 train no.354640  loss = 1.75395 avg_loss = 3.23335\n",
      "epoch no.4 train no.354650  loss = 3.82771 avg_loss = 3.24209\n",
      "epoch no.4 train no.354660  loss = 2.50448 avg_loss = 3.24466\n",
      "epoch no.4 train no.354670  loss = 3.26942 avg_loss = 3.28041\n",
      "epoch no.4 train no.354680  loss = 2.78443 avg_loss = 3.29932\n",
      "epoch no.4 train no.354690  loss = 3.12202 avg_loss = 3.33357\n",
      "epoch no.4 train no.354700  loss = 2.71040 avg_loss = 3.29417\n",
      "epoch no.4 train no.354710  loss = 3.92403 avg_loss = 3.30940\n",
      "epoch no.4 train no.354720  loss = 3.97415 avg_loss = 3.30335\n",
      "epoch no.4 train no.354730  loss = 5.91926 avg_loss = 3.34430\n",
      "epoch no.4 train no.354740  loss = 3.25935 avg_loss = 3.29563\n",
      "epoch no.4 train no.354750  loss = 2.49863 avg_loss = 3.30654\n",
      "epoch no.4 train no.354760  loss = 2.22942 avg_loss = 3.29781\n",
      "epoch no.4 train no.354770  loss = 3.44238 avg_loss = 3.28748\n",
      "epoch no.4 train no.354780  loss = 4.20814 avg_loss = 3.27830\n",
      "epoch no.4 train no.354790  loss = 3.11451 avg_loss = 3.28723\n",
      "epoch no.4 train no.354800  loss = 2.14521 avg_loss = 3.30323\n",
      "epoch no.4 train no.354810  loss = 4.15200 avg_loss = 3.29195\n",
      "epoch no.4 train no.354820  loss = 3.62742 avg_loss = 3.31443\n",
      "epoch no.4 train no.354830  loss = 2.44961 avg_loss = 3.28557\n",
      "epoch no.4 train no.354840  loss = 4.49487 avg_loss = 3.28829\n",
      "epoch no.4 train no.354850  loss = 2.82318 avg_loss = 3.29719\n",
      "epoch no.4 train no.354860  loss = 2.39920 avg_loss = 3.27469\n",
      "epoch no.4 train no.354870  loss = 4.86489 avg_loss = 3.31984\n",
      "epoch no.4 train no.354880  loss = 2.52314 avg_loss = 3.30170\n",
      "epoch no.4 train no.354890  loss = 2.02133 avg_loss = 3.30242\n",
      "epoch no.4 train no.354900  loss = 3.36581 avg_loss = 3.33062\n",
      "epoch no.4 train no.354910  loss = 3.26151 avg_loss = 3.37204\n",
      "epoch no.4 train no.354920  loss = 2.39636 avg_loss = 3.36790\n",
      "epoch no.4 train no.354930  loss = 2.62618 avg_loss = 3.32611\n",
      "epoch no.4 train no.354940  loss = 3.29563 avg_loss = 3.36159\n",
      "epoch no.4 train no.354950  loss = 3.77603 avg_loss = 3.34728\n",
      "epoch no.4 train no.354960  loss = 4.06203 avg_loss = 3.37241\n",
      "epoch no.4 train no.354970  loss = 2.53341 avg_loss = 3.37074\n",
      "epoch no.4 train no.354980  loss = 5.23373 avg_loss = 3.37023\n",
      "epoch no.4 train no.354990  loss = 4.27015 avg_loss = 3.41138\n",
      "epoch no.4 train no.355000  loss = 3.18541 avg_loss = 3.37044\n",
      "3\n",
      "to_tokens: ['▁크리스마스', '전환', '용', '▁위한', '▁팝', '</s>']\n",
      "기분전환을 위한 재즈</s>\n",
      "epoch no.4 train no.355010  loss = 3.63446 avg_loss = 3.38329\n",
      "epoch no.4 train no.355020  loss = 3.09067 avg_loss = 3.37662\n",
      "epoch no.4 train no.355030  loss = 2.48373 avg_loss = 3.36841\n",
      "epoch no.4 train no.355040  loss = 4.95282 avg_loss = 3.38625\n",
      "epoch no.4 train no.355050  loss = 3.34694 avg_loss = 3.38228\n",
      "epoch no.4 train no.355060  loss = 4.93787 avg_loss = 3.34801\n",
      "epoch no.4 train no.355070  loss = 5.55822 avg_loss = 3.33500\n",
      "epoch no.4 train no.355080  loss = 3.88938 avg_loss = 3.34281\n",
      "epoch no.4 train no.355090  loss = 2.79457 avg_loss = 3.32932\n",
      "epoch no.4 train no.355100  loss = 3.70439 avg_loss = 3.39049\n",
      "epoch no.4 train no.355110  loss = 3.06334 avg_loss = 3.38742\n",
      "epoch no.4 train no.355120  loss = 2.70340 avg_loss = 3.32948\n",
      "epoch no.4 train no.355130  loss = 2.61679 avg_loss = 3.34570\n",
      "epoch no.4 train no.355140  loss = 3.01781 avg_loss = 3.38680\n",
      "epoch no.4 train no.355150  loss = 3.77673 avg_loss = 3.33937\n",
      "epoch no.4 train no.355160  loss = 3.37750 avg_loss = 3.32927\n",
      "epoch no.4 train no.355170  loss = 2.94379 avg_loss = 3.32595\n",
      "epoch no.4 train no.355180  loss = 3.39078 avg_loss = 3.33092\n",
      "epoch no.4 train no.355190  loss = 4.19553 avg_loss = 3.31195\n",
      "epoch no.4 train no.355200  loss = 3.75648 avg_loss = 3.29610\n",
      "epoch no.4 train no.355210  loss = 2.13322 avg_loss = 3.29456\n",
      "epoch no.4 train no.355220  loss = 3.49012 avg_loss = 3.29369\n",
      "epoch no.4 train no.355230  loss = 2.39493 avg_loss = 3.27746\n",
      "epoch no.4 train no.355240  loss = 7.69560 avg_loss = 3.34215\n",
      "epoch no.4 train no.355250  loss = 3.97717 avg_loss = 3.35725\n",
      "epoch no.4 train no.355260  loss = 3.04916 avg_loss = 3.33650\n",
      "epoch no.4 train no.355270  loss = 3.15563 avg_loss = 3.35931\n",
      "epoch no.4 train no.355280  loss = 4.09367 avg_loss = 3.37310\n",
      "epoch no.4 train no.355290  loss = 2.65548 avg_loss = 3.35967\n",
      "epoch no.4 train no.355300  loss = 3.19544 avg_loss = 3.30104\n",
      "epoch no.4 train no.355310  loss = 2.97811 avg_loss = 3.28824\n",
      "epoch no.4 train no.355320  loss = 2.31664 avg_loss = 3.28184\n",
      "epoch no.4 train no.355330  loss = 3.73072 avg_loss = 3.32132\n",
      "epoch no.4 train no.355340  loss = 3.53765 avg_loss = 3.36484\n",
      "epoch no.4 train no.355350  loss = 3.59801 avg_loss = 3.37420\n",
      "epoch no.4 train no.355360  loss = 3.77419 avg_loss = 3.32719\n",
      "epoch no.4 train no.355370  loss = 4.48558 avg_loss = 3.34576\n",
      "epoch no.4 train no.355380  loss = 2.85934 avg_loss = 3.30023\n",
      "epoch no.4 train no.355390  loss = 2.60546 avg_loss = 3.28434\n",
      "epoch no.4 train no.355400  loss = 2.83313 avg_loss = 3.24778\n",
      "epoch no.4 train no.355410  loss = 2.36916 avg_loss = 3.20973\n",
      "epoch no.4 train no.355420  loss = 3.95648 avg_loss = 3.18312\n",
      "epoch no.4 train no.355430  loss = 2.47562 avg_loss = 3.16324\n",
      "epoch no.4 train no.355440  loss = 3.54540 avg_loss = 3.18573\n",
      "epoch no.4 train no.355450  loss = 3.21924 avg_loss = 3.13185\n",
      "epoch no.4 train no.355460  loss = 3.24514 avg_loss = 3.14325\n",
      "epoch no.4 train no.355470  loss = 2.82086 avg_loss = 3.16240\n",
      "epoch no.4 train no.355480  loss = 3.75940 avg_loss = 3.16190\n",
      "epoch no.4 train no.355490  loss = 2.55289 avg_loss = 3.16284\n",
      "epoch no.4 train no.355500  loss = 4.64363 avg_loss = 3.12266\n",
      "epoch no.4 train no.355510  loss = 3.21367 avg_loss = 3.16035\n",
      "epoch no.4 train no.355520  loss = 4.49265 avg_loss = 3.21922\n",
      "epoch no.4 train no.355530  loss = 2.60621 avg_loss = 3.18718\n",
      "epoch no.4 train no.355540  loss = 3.54605 avg_loss = 3.14524\n",
      "epoch no.4 train no.355550  loss = 3.33561 avg_loss = 3.19865\n",
      "epoch no.4 train no.355560  loss = 4.99102 avg_loss = 3.18952\n",
      "epoch no.4 train no.355570  loss = 3.15157 avg_loss = 3.19603\n",
      "epoch no.4 train no.355580  loss = 4.00294 avg_loss = 3.20769\n",
      "epoch no.4 train no.355590  loss = 2.92359 avg_loss = 3.20300\n",
      "epoch no.4 train no.355600  loss = 4.17893 avg_loss = 3.20743\n",
      "epoch no.4 train no.355610  loss = 2.46143 avg_loss = 3.22714\n",
      "epoch no.4 train no.355620  loss = 3.11995 avg_loss = 3.22390\n",
      "epoch no.4 train no.355630  loss = 2.28739 avg_loss = 3.22995\n",
      "epoch no.4 train no.355640  loss = 4.54540 avg_loss = 3.23461\n",
      "epoch no.4 train no.355650  loss = 4.08944 avg_loss = 3.24807\n",
      "epoch no.4 train no.355660  loss = 2.42743 avg_loss = 3.23899\n",
      "epoch no.4 train no.355670  loss = 2.41579 avg_loss = 3.29805\n",
      "epoch no.4 train no.355680  loss = 2.90791 avg_loss = 3.29727\n",
      "epoch no.4 train no.355690  loss = 2.87167 avg_loss = 3.36335\n",
      "epoch no.4 train no.355700  loss = 3.30183 avg_loss = 3.32772\n",
      "epoch no.4 train no.355710  loss = 4.45122 avg_loss = 3.34496\n",
      "epoch no.4 train no.355720  loss = 5.84780 avg_loss = 3.37616\n",
      "epoch no.4 train no.355730  loss = 2.31541 avg_loss = 3.32690\n",
      "epoch no.4 train no.355740  loss = 2.90476 avg_loss = 3.31303\n",
      "epoch no.4 train no.355750  loss = 2.26454 avg_loss = 3.34942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.355760  loss = 3.59264 avg_loss = 3.35669\n",
      "epoch no.4 train no.355770  loss = 2.74644 avg_loss = 3.34577\n",
      "epoch no.4 train no.355780  loss = 2.85533 avg_loss = 3.33359\n",
      "epoch no.4 train no.355790  loss = 1.88618 avg_loss = 3.30316\n",
      "epoch no.4 train no.355800  loss = 2.03695 avg_loss = 3.30371\n",
      "epoch no.4 train no.355810  loss = 5.09436 avg_loss = 3.32523\n",
      "epoch no.4 train no.355820  loss = 2.45015 avg_loss = 3.29160\n",
      "epoch no.4 train no.355830  loss = 2.22306 avg_loss = 3.29428\n",
      "epoch no.4 train no.355840  loss = 4.22960 avg_loss = 3.34956\n",
      "epoch no.4 train no.355850  loss = 3.22552 avg_loss = 3.38428\n",
      "epoch no.4 train no.355860  loss = 3.65924 avg_loss = 3.35194\n",
      "epoch no.4 train no.355870  loss = 1.56712 avg_loss = 3.33745\n",
      "epoch no.4 train no.355880  loss = 3.26153 avg_loss = 3.35591\n",
      "epoch no.4 train no.355890  loss = 2.82964 avg_loss = 3.33792\n",
      "epoch no.4 train no.355900  loss = 3.73568 avg_loss = 3.34320\n",
      "epoch no.4 train no.355910  loss = 2.09535 avg_loss = 3.35080\n",
      "epoch no.4 train no.355920  loss = 0.92764 avg_loss = 3.31885\n",
      "epoch no.4 train no.355930  loss = 2.83403 avg_loss = 3.26672\n",
      "epoch no.4 train no.355940  loss = 2.88510 avg_loss = 3.28175\n",
      "epoch no.4 train no.355950  loss = 2.88899 avg_loss = 3.25306\n",
      "epoch no.4 train no.355960  loss = 3.17676 avg_loss = 3.24577\n",
      "epoch no.4 train no.355970  loss = 1.70845 avg_loss = 3.21411\n",
      "epoch no.4 train no.355980  loss = 3.55424 avg_loss = 3.22161\n",
      "epoch no.4 train no.355990  loss = 4.08057 avg_loss = 3.24107\n",
      "epoch no.4 train no.356000  loss = 2.67893 avg_loss = 3.27653\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '용', '▁위한', '▁신나는', '▁팝', '곡', '</s>']\n",
      "기분전환을 위한 신나는 댄스곡</s>\n",
      "epoch no.4 train no.356010  loss = 3.81005 avg_loss = 3.26821\n",
      "epoch no.4 train no.356020  loss = 3.40541 avg_loss = 3.24746\n",
      "epoch no.4 train no.356030  loss = 2.87100 avg_loss = 3.24529\n",
      "epoch no.4 train no.356040  loss = 2.11714 avg_loss = 3.21908\n",
      "epoch no.4 train no.356050  loss = 2.45217 avg_loss = 3.20988\n",
      "epoch no.4 train no.356060  loss = 3.02307 avg_loss = 3.23791\n",
      "epoch no.4 train no.356070  loss = 3.55854 avg_loss = 3.27907\n",
      "epoch no.4 train no.356080  loss = 2.39818 avg_loss = 3.28914\n",
      "epoch no.4 train no.356090  loss = 3.58838 avg_loss = 3.28287\n",
      "epoch no.4 train no.356100  loss = 4.22263 avg_loss = 3.29251\n",
      "epoch no.4 train no.356110  loss = 3.95181 avg_loss = 3.33027\n",
      "epoch no.4 train no.356120  loss = 3.88156 avg_loss = 3.40801\n",
      "epoch no.4 train no.356130  loss = 2.66199 avg_loss = 3.34099\n",
      "epoch no.4 train no.356140  loss = 3.03582 avg_loss = 3.35603\n",
      "epoch no.4 train no.356150  loss = 3.12525 avg_loss = 3.36612\n",
      "epoch no.4 train no.356160  loss = 2.68538 avg_loss = 3.39186\n",
      "epoch no.4 train no.356170  loss = 4.02535 avg_loss = 3.39618\n",
      "epoch no.4 train no.356180  loss = 4.14181 avg_loss = 3.41215\n",
      "epoch no.4 train no.356190  loss = 4.28276 avg_loss = 3.42473\n",
      "epoch no.4 train no.356200  loss = 4.44909 avg_loss = 3.43014\n",
      "epoch no.4 train no.356210  loss = 2.48211 avg_loss = 3.40977\n",
      "epoch no.4 train no.356220  loss = 1.81467 avg_loss = 3.37513\n",
      "epoch no.4 train no.356230  loss = 4.28209 avg_loss = 3.31151\n",
      "epoch no.4 train no.356240  loss = 3.91278 avg_loss = 3.32204\n",
      "epoch no.4 train no.356250  loss = 4.81709 avg_loss = 3.31317\n",
      "epoch no.4 train no.356260  loss = 3.17582 avg_loss = 3.33399\n",
      "epoch no.4 train no.356270  loss = 3.50400 avg_loss = 3.33311\n",
      "epoch no.4 train no.356280  loss = 2.63036 avg_loss = 3.31723\n",
      "epoch no.4 train no.356290  loss = 4.41245 avg_loss = 3.34798\n",
      "epoch no.4 train no.356300  loss = 2.64580 avg_loss = 3.34189\n",
      "epoch no.4 train no.356310  loss = 3.79736 avg_loss = 3.30910\n",
      "epoch no.4 train no.356320  loss = 2.94508 avg_loss = 3.30662\n",
      "epoch no.4 train no.356330  loss = 2.35882 avg_loss = 3.31813\n",
      "epoch no.4 train no.356340  loss = 4.79902 avg_loss = 3.35943\n",
      "epoch no.4 train no.356350  loss = 2.45108 avg_loss = 3.34140\n",
      "epoch no.4 train no.356360  loss = 1.77337 avg_loss = 3.27730\n",
      "epoch no.4 train no.356370  loss = 3.00779 avg_loss = 3.25443\n",
      "epoch no.4 train no.356380  loss = 3.72650 avg_loss = 3.24657\n",
      "epoch no.4 train no.356390  loss = 1.95448 avg_loss = 3.21046\n",
      "epoch no.4 train no.356400  loss = 4.50013 avg_loss = 3.22412\n",
      "epoch no.4 train no.356410  loss = 3.46628 avg_loss = 3.25374\n",
      "epoch no.4 train no.356420  loss = 2.62460 avg_loss = 3.29225\n",
      "epoch no.4 train no.356430  loss = 2.20274 avg_loss = 3.26884\n",
      "epoch no.4 train no.356440  loss = 3.42162 avg_loss = 3.28645\n",
      "epoch no.4 train no.356450  loss = 4.13991 avg_loss = 3.26869\n",
      "epoch no.4 train no.356460  loss = 2.08695 avg_loss = 3.24606\n",
      "epoch no.4 train no.356470  loss = 2.99405 avg_loss = 3.24131\n",
      "epoch no.4 train no.356480  loss = 2.80252 avg_loss = 3.23828\n",
      "epoch no.4 train no.356490  loss = 2.80279 avg_loss = 3.22407\n",
      "epoch no.4 train no.356500  loss = 3.79635 avg_loss = 3.21908\n",
      "epoch no.4 train no.356510  loss = 1.89264 avg_loss = 3.14360\n",
      "epoch no.4 train no.356520  loss = 3.20590 avg_loss = 3.14703\n",
      "epoch no.4 train no.356530  loss = 2.49447 avg_loss = 3.18986\n",
      "epoch no.4 train no.356540  loss = 2.91613 avg_loss = 3.20767\n",
      "epoch no.4 train no.356550  loss = 2.97002 avg_loss = 3.20515\n",
      "epoch no.4 train no.356560  loss = 3.86479 avg_loss = 3.22036\n",
      "epoch no.4 train no.356570  loss = 1.89623 avg_loss = 3.22017\n",
      "epoch no.4 train no.356580  loss = 3.45682 avg_loss = 3.27082\n",
      "epoch no.4 train no.356590  loss = 3.79946 avg_loss = 3.36151\n",
      "epoch no.4 train no.356600  loss = 1.12785 avg_loss = 3.31487\n",
      "epoch no.4 train no.356610  loss = 4.91885 avg_loss = 3.33195\n",
      "epoch no.4 train no.356620  loss = 2.06180 avg_loss = 3.30937\n",
      "epoch no.4 train no.356630  loss = 3.57663 avg_loss = 3.26954\n",
      "epoch no.4 train no.356640  loss = 3.68189 avg_loss = 3.25531\n",
      "epoch no.4 train no.356650  loss = 2.47860 avg_loss = 3.25439\n",
      "epoch no.4 train no.356660  loss = 4.13538 avg_loss = 3.26366\n",
      "epoch no.4 train no.356670  loss = 2.18193 avg_loss = 3.24100\n",
      "epoch no.4 train no.356680  loss = 4.73866 avg_loss = 3.23398\n",
      "epoch no.4 train no.356690  loss = 2.81455 avg_loss = 3.26739\n",
      "epoch no.4 train no.356700  loss = 2.14634 avg_loss = 3.28260\n",
      "epoch no.4 train no.356710  loss = 4.22093 avg_loss = 3.27229\n",
      "epoch no.4 train no.356720  loss = 1.86709 avg_loss = 3.24884\n",
      "epoch no.4 train no.356730  loss = 3.07126 avg_loss = 3.26507\n",
      "epoch no.4 train no.356740  loss = 4.01784 avg_loss = 3.27614\n",
      "epoch no.4 train no.356750  loss = 1.90887 avg_loss = 3.25817\n",
      "epoch no.4 train no.356760  loss = 2.15656 avg_loss = 3.24570\n",
      "epoch no.4 train no.356770  loss = 3.29019 avg_loss = 3.23088\n",
      "epoch no.4 train no.356780  loss = 3.40005 avg_loss = 3.24053\n",
      "epoch no.4 train no.356790  loss = 2.63438 avg_loss = 3.20647\n",
      "epoch no.4 train no.356800  loss = 3.97956 avg_loss = 3.21141\n",
      "epoch no.4 train no.356810  loss = 2.82174 avg_loss = 3.23550\n",
      "epoch no.4 train no.356820  loss = 1.42780 avg_loss = 3.24823\n",
      "epoch no.4 train no.356830  loss = 3.48834 avg_loss = 3.26388\n",
      "epoch no.4 train no.356840  loss = 3.67301 avg_loss = 3.30293\n",
      "epoch no.4 train no.356850  loss = 3.12841 avg_loss = 3.29006\n",
      "epoch no.4 train no.356860  loss = 3.51155 avg_loss = 3.28072\n",
      "epoch no.4 train no.356870  loss = 2.53052 avg_loss = 3.24844\n",
      "epoch no.4 train no.356880  loss = 5.28268 avg_loss = 3.23717\n",
      "epoch no.4 train no.356890  loss = 2.92767 avg_loss = 3.19267\n",
      "epoch no.4 train no.356900  loss = 3.53265 avg_loss = 3.25976\n",
      "epoch no.4 train no.356910  loss = 2.98440 avg_loss = 3.25697\n",
      "epoch no.4 train no.356920  loss = 3.31453 avg_loss = 3.19971\n",
      "epoch no.4 train no.356930  loss = 3.22782 avg_loss = 3.17494\n",
      "epoch no.4 train no.356940  loss = 2.00547 avg_loss = 3.11658\n",
      "epoch no.4 train no.356950  loss = 5.52587 avg_loss = 3.15927\n",
      "epoch no.4 train no.356960  loss = 3.95928 avg_loss = 3.20304\n",
      "epoch no.4 train no.356970  loss = 2.48314 avg_loss = 3.21046\n",
      "epoch no.4 train no.356980  loss = 3.48708 avg_loss = 3.23624\n",
      "epoch no.4 train no.356990  loss = 3.33776 avg_loss = 3.24171\n",
      "epoch no.4 train no.357000  loss = 2.40100 avg_loss = 3.18510\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '을', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.4 train no.357010  loss = 3.84290 avg_loss = 3.21222\n",
      "epoch no.4 train no.357020  loss = 3.07569 avg_loss = 3.17710\n",
      "epoch no.4 train no.357030  loss = 2.63181 avg_loss = 3.17009\n",
      "epoch no.4 train no.357040  loss = 3.37681 avg_loss = 3.15844\n",
      "epoch no.4 train no.357050  loss = 2.67325 avg_loss = 3.17865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.357060  loss = 2.47787 avg_loss = 3.16727\n",
      "epoch no.4 train no.357070  loss = 2.69633 avg_loss = 3.18251\n",
      "epoch no.4 train no.357080  loss = 2.14816 avg_loss = 3.20713\n",
      "epoch no.4 train no.357090  loss = 2.80647 avg_loss = 3.15702\n",
      "epoch no.4 train no.357100  loss = 3.05097 avg_loss = 3.15344\n",
      "epoch no.4 train no.357110  loss = 2.86491 avg_loss = 3.13739\n",
      "epoch no.4 train no.357120  loss = 3.32136 avg_loss = 3.16003\n",
      "epoch no.4 train no.357130  loss = 2.77404 avg_loss = 3.19205\n",
      "epoch no.4 train no.357140  loss = 2.95832 avg_loss = 3.18347\n",
      "epoch no.4 train no.357150  loss = 2.92150 avg_loss = 3.20073\n",
      "epoch no.4 train no.357160  loss = 3.68593 avg_loss = 3.19151\n",
      "epoch no.4 train no.357170  loss = 2.02384 avg_loss = 3.19687\n",
      "epoch no.4 train no.357180  loss = 4.02874 avg_loss = 3.18977\n",
      "epoch no.4 train no.357190  loss = 3.61341 avg_loss = 3.21024\n",
      "epoch no.4 train no.357200  loss = 3.75294 avg_loss = 3.24313\n",
      "epoch no.4 train no.357210  loss = 3.09842 avg_loss = 3.25004\n",
      "epoch no.4 train no.357220  loss = 2.77799 avg_loss = 3.22737\n",
      "epoch no.4 train no.357230  loss = 2.99363 avg_loss = 3.20796\n",
      "epoch no.4 train no.357240  loss = 1.88947 avg_loss = 3.23084\n",
      "epoch no.4 train no.357250  loss = 2.37491 avg_loss = 3.18445\n",
      "epoch no.4 train no.357260  loss = 4.17155 avg_loss = 3.18730\n",
      "epoch no.4 train no.357270  loss = 4.14076 avg_loss = 3.20349\n",
      "epoch no.4 train no.357280  loss = 2.21999 avg_loss = 3.20436\n",
      "epoch no.4 train no.357290  loss = 3.28208 avg_loss = 3.19616\n",
      "epoch no.4 train no.357300  loss = 2.64179 avg_loss = 3.23455\n",
      "epoch no.4 train no.357310  loss = 2.46909 avg_loss = 3.22774\n",
      "epoch no.4 train no.357320  loss = 3.21541 avg_loss = 3.18584\n",
      "epoch no.4 train no.357330  loss = 2.75470 avg_loss = 3.18162\n",
      "epoch no.4 train no.357340  loss = 3.80077 avg_loss = 3.17342\n",
      "epoch no.4 train no.357350  loss = 3.01266 avg_loss = 3.15724\n",
      "epoch no.4 train no.357360  loss = 4.55716 avg_loss = 3.18348\n",
      "epoch no.4 train no.357370  loss = 2.61503 avg_loss = 3.18207\n",
      "epoch no.4 train no.357380  loss = 2.40454 avg_loss = 3.19665\n",
      "epoch no.4 train no.357390  loss = 2.92586 avg_loss = 3.20891\n",
      "epoch no.4 train no.357400  loss = 2.06952 avg_loss = 3.22218\n",
      "epoch no.4 train no.357410  loss = 3.85150 avg_loss = 3.28031\n",
      "epoch no.4 train no.357420  loss = 3.54954 avg_loss = 3.32379\n",
      "epoch no.4 train no.357430  loss = 3.35767 avg_loss = 3.31424\n",
      "epoch no.4 train no.357440  loss = 4.04166 avg_loss = 3.31521\n",
      "epoch no.4 train no.357450  loss = 2.64589 avg_loss = 3.32500\n",
      "epoch no.4 train no.357460  loss = 2.81985 avg_loss = 3.31554\n",
      "epoch no.4 train no.357470  loss = 1.24670 avg_loss = 3.26286\n",
      "epoch no.4 train no.357480  loss = 2.78135 avg_loss = 3.28599\n",
      "epoch no.4 train no.357490  loss = 1.83099 avg_loss = 3.27478\n",
      "epoch no.4 train no.357500  loss = 3.28850 avg_loss = 3.30449\n",
      "epoch no.4 train no.357510  loss = 4.19670 avg_loss = 3.33634\n",
      "epoch no.4 train no.357520  loss = 2.95489 avg_loss = 3.32271\n",
      "epoch no.4 train no.357530  loss = 3.92344 avg_loss = 3.32278\n",
      "epoch no.4 train no.357540  loss = 5.79494 avg_loss = 3.35304\n",
      "epoch no.4 train no.357550  loss = 2.71919 avg_loss = 3.35100\n",
      "epoch no.4 train no.357560  loss = 2.02516 avg_loss = 3.32044\n",
      "epoch no.4 train no.357570  loss = 1.80390 avg_loss = 3.33132\n",
      "epoch no.4 train no.357580  loss = 3.28046 avg_loss = 3.31017\n",
      "epoch no.4 train no.357590  loss = 3.12414 avg_loss = 3.31071\n",
      "epoch no.4 train no.357600  loss = 3.42243 avg_loss = 3.32554\n",
      "epoch no.4 train no.357610  loss = 3.12712 avg_loss = 3.28712\n",
      "epoch no.4 train no.357620  loss = 4.16697 avg_loss = 3.34679\n",
      "epoch no.4 train no.357630  loss = 3.15610 avg_loss = 3.31273\n",
      "epoch no.4 train no.357640  loss = 2.41944 avg_loss = 3.32280\n",
      "epoch no.4 train no.357650  loss = 4.36073 avg_loss = 3.32203\n",
      "epoch no.4 train no.357660  loss = 5.80097 avg_loss = 3.27394\n",
      "epoch no.4 train no.357670  loss = 2.53216 avg_loss = 3.27526\n",
      "epoch no.4 train no.357680  loss = 2.66029 avg_loss = 3.24304\n",
      "epoch no.4 train no.357690  loss = 3.35538 avg_loss = 3.24357\n",
      "epoch no.4 train no.357700  loss = 2.40125 avg_loss = 3.22965\n",
      "epoch no.4 train no.357710  loss = 2.49779 avg_loss = 3.27458\n",
      "epoch no.4 train no.357720  loss = 1.89457 avg_loss = 3.28522\n",
      "epoch no.4 train no.357730  loss = 2.24914 avg_loss = 3.26297\n",
      "epoch no.4 train no.357740  loss = 2.65155 avg_loss = 3.23964\n",
      "epoch no.4 train no.357750  loss = 1.01903 avg_loss = 3.21385\n",
      "epoch no.4 train no.357760  loss = 2.36589 avg_loss = 3.23269\n",
      "epoch no.4 train no.357770  loss = 2.64343 avg_loss = 3.20219\n",
      "epoch no.4 train no.357780  loss = 2.97193 avg_loss = 3.17575\n",
      "epoch no.4 train no.357790  loss = 3.65469 avg_loss = 3.20432\n",
      "epoch no.4 train no.357800  loss = 2.72415 avg_loss = 3.21103\n",
      "epoch no.4 train no.357810  loss = 3.72910 avg_loss = 3.22589\n",
      "epoch no.4 train no.357820  loss = 4.01630 avg_loss = 3.23901\n",
      "epoch no.4 train no.357830  loss = 2.99580 avg_loss = 3.21753\n",
      "epoch no.4 train no.357840  loss = 3.46091 avg_loss = 3.20915\n",
      "epoch no.4 train no.357850  loss = 2.91458 avg_loss = 3.21695\n",
      "epoch no.4 train no.357860  loss = 4.01445 avg_loss = 3.25255\n",
      "epoch no.4 train no.357870  loss = 2.23606 avg_loss = 3.23592\n",
      "epoch no.4 train no.357880  loss = 2.12665 avg_loss = 3.19612\n",
      "epoch no.4 train no.357890  loss = 2.95189 avg_loss = 3.20510\n",
      "epoch no.4 train no.357900  loss = 2.47193 avg_loss = 3.24308\n",
      "epoch no.4 train no.357910  loss = 3.79752 avg_loss = 3.24776\n",
      "epoch no.4 train no.357920  loss = 2.91204 avg_loss = 3.23426\n",
      "epoch no.4 train no.357930  loss = 2.57324 avg_loss = 3.20870\n",
      "epoch no.4 train no.357940  loss = 3.34424 avg_loss = 3.24932\n",
      "epoch no.4 train no.357950  loss = 3.28177 avg_loss = 3.29164\n",
      "epoch no.4 train no.357960  loss = 3.12938 avg_loss = 3.31620\n",
      "epoch no.4 train no.357970  loss = 2.85709 avg_loss = 3.29591\n",
      "epoch no.4 train no.357980  loss = 3.14262 avg_loss = 3.29607\n",
      "epoch no.4 train no.357990  loss = 4.85833 avg_loss = 3.31032\n",
      "epoch no.4 train no.358000  loss = 4.08751 avg_loss = 3.28246\n",
      "1\n",
      "to_tokens: ['▁라디오', '좋은', '을', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.4 train no.358010  loss = 4.38335 avg_loss = 3.28348\n",
      "epoch no.4 train no.358020  loss = 1.99348 avg_loss = 3.30191\n",
      "epoch no.4 train no.358030  loss = 5.26131 avg_loss = 3.31622\n",
      "epoch no.4 train no.358040  loss = 3.12310 avg_loss = 3.33678\n",
      "epoch no.4 train no.358050  loss = 3.24393 avg_loss = 3.32745\n",
      "epoch no.4 train no.358060  loss = 3.66037 avg_loss = 3.32842\n",
      "epoch no.4 train no.358070  loss = 3.45608 avg_loss = 3.32662\n",
      "epoch no.4 train no.358080  loss = 3.48283 avg_loss = 3.31087\n",
      "epoch no.4 train no.358090  loss = 2.83967 avg_loss = 3.27507\n",
      "epoch no.4 train no.358100  loss = 4.86340 avg_loss = 3.29140\n",
      "epoch no.4 train no.358110  loss = 2.74751 avg_loss = 3.29091\n",
      "epoch no.4 train no.358120  loss = 3.86091 avg_loss = 3.32184\n",
      "epoch no.4 train no.358130  loss = 2.86757 avg_loss = 3.33997\n",
      "epoch no.4 train no.358140  loss = 2.47490 avg_loss = 3.36850\n",
      "epoch no.4 train no.358150  loss = 3.63206 avg_loss = 3.41247\n",
      "epoch no.4 train no.358160  loss = 3.09860 avg_loss = 3.39014\n",
      "epoch no.4 train no.358170  loss = 3.13419 avg_loss = 3.35883\n",
      "epoch no.4 train no.358180  loss = 3.13225 avg_loss = 3.33366\n",
      "epoch no.4 train no.358190  loss = 2.88184 avg_loss = 3.32389\n",
      "epoch no.4 train no.358200  loss = 4.99025 avg_loss = 3.32407\n",
      "epoch no.4 train no.358210  loss = 3.75458 avg_loss = 3.35885\n",
      "epoch no.4 train no.358220  loss = 4.56292 avg_loss = 3.37611\n",
      "epoch no.4 train no.358230  loss = 3.62695 avg_loss = 3.37153\n",
      "epoch no.4 train no.358240  loss = 3.33752 avg_loss = 3.39777\n",
      "epoch no.4 train no.358250  loss = 3.77499 avg_loss = 3.36062\n",
      "epoch no.4 train no.358260  loss = 2.93512 avg_loss = 3.36185\n",
      "epoch no.4 train no.358270  loss = 3.11292 avg_loss = 3.34563\n",
      "epoch no.4 train no.358280  loss = 2.47143 avg_loss = 3.38410\n",
      "epoch no.4 train no.358290  loss = 2.89247 avg_loss = 3.35133\n",
      "epoch no.4 train no.358300  loss = 3.33420 avg_loss = 3.34873\n",
      "epoch no.4 train no.358310  loss = 2.60770 avg_loss = 3.35986\n",
      "epoch no.4 train no.358320  loss = 2.96601 avg_loss = 3.32740\n",
      "epoch no.4 train no.358330  loss = 3.20196 avg_loss = 3.36097\n",
      "epoch no.4 train no.358340  loss = 4.17235 avg_loss = 3.38207\n",
      "epoch no.4 train no.358350  loss = 4.13113 avg_loss = 3.37798\n",
      "epoch no.4 train no.358360  loss = 2.95293 avg_loss = 3.37415\n",
      "epoch no.4 train no.358370  loss = 5.52719 avg_loss = 3.43586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.358380  loss = 3.40209 avg_loss = 3.40987\n",
      "epoch no.4 train no.358390  loss = 3.87569 avg_loss = 3.43264\n",
      "epoch no.4 train no.358400  loss = 2.62358 avg_loss = 3.37503\n",
      "epoch no.4 train no.358410  loss = 2.11724 avg_loss = 3.38290\n",
      "epoch no.4 train no.358420  loss = 2.41063 avg_loss = 3.35239\n",
      "epoch no.4 train no.358430  loss = 2.95962 avg_loss = 3.34699\n",
      "epoch no.4 train no.358440  loss = 3.51951 avg_loss = 3.31621\n",
      "epoch no.4 train no.358450  loss = 4.03557 avg_loss = 3.32920\n",
      "epoch no.4 train no.358460  loss = 2.10300 avg_loss = 3.31864\n",
      "epoch no.4 train no.358470  loss = 3.50665 avg_loss = 3.32277\n",
      "epoch no.4 train no.358480  loss = 3.43058 avg_loss = 3.33599\n",
      "epoch no.4 train no.358490  loss = 4.90461 avg_loss = 3.35070\n",
      "epoch no.4 train no.358500  loss = 2.95519 avg_loss = 3.34402\n",
      "epoch no.4 train no.358510  loss = 3.19424 avg_loss = 3.37536\n",
      "epoch no.4 train no.358520  loss = 3.11839 avg_loss = 3.37902\n",
      "epoch no.4 train no.358530  loss = 2.71513 avg_loss = 3.33952\n",
      "epoch no.4 train no.358540  loss = 2.65267 avg_loss = 3.30235\n",
      "epoch no.4 train no.358550  loss = 4.63606 avg_loss = 3.28409\n",
      "epoch no.4 train no.358560  loss = 3.17461 avg_loss = 3.28675\n",
      "epoch no.4 train no.358570  loss = 4.54925 avg_loss = 3.30593\n",
      "epoch no.4 train no.358580  loss = 3.32302 avg_loss = 3.26742\n",
      "epoch no.4 train no.358590  loss = 2.97367 avg_loss = 3.27094\n",
      "epoch no.4 train no.358600  loss = 2.75290 avg_loss = 3.27778\n",
      "epoch no.4 train no.358610  loss = 3.36465 avg_loss = 3.27187\n",
      "epoch no.4 train no.358620  loss = 2.19385 avg_loss = 3.25189\n",
      "epoch no.4 train no.358630  loss = 3.06967 avg_loss = 3.24726\n",
      "epoch no.4 train no.358640  loss = 2.64767 avg_loss = 3.23590\n",
      "epoch no.4 train no.358650  loss = 2.88755 avg_loss = 3.19198\n",
      "epoch no.4 train no.358660  loss = 3.64039 avg_loss = 3.23584\n",
      "epoch no.4 train no.358670  loss = 3.33068 avg_loss = 3.23376\n",
      "epoch no.4 train no.358680  loss = 3.58644 avg_loss = 3.25962\n",
      "epoch no.4 train no.358690  loss = 3.71932 avg_loss = 3.31067\n",
      "epoch no.4 train no.358700  loss = 1.91826 avg_loss = 3.31812\n",
      "epoch no.4 train no.358710  loss = 1.73257 avg_loss = 3.26160\n",
      "epoch no.4 train no.358720  loss = 2.34444 avg_loss = 3.28126\n",
      "epoch no.4 train no.358730  loss = 2.86910 avg_loss = 3.24503\n",
      "epoch no.4 train no.358740  loss = 4.36710 avg_loss = 3.32396\n",
      "epoch no.4 train no.358750  loss = 1.99286 avg_loss = 3.33286\n",
      "epoch no.4 train no.358760  loss = 2.45349 avg_loss = 3.33440\n",
      "epoch no.4 train no.358770  loss = 2.61211 avg_loss = 3.31820\n",
      "epoch no.4 train no.358780  loss = 2.05755 avg_loss = 3.32627\n",
      "epoch no.4 train no.358790  loss = 2.42008 avg_loss = 3.30010\n",
      "epoch no.4 train no.358800  loss = 2.05054 avg_loss = 3.30799\n",
      "epoch no.4 train no.358810  loss = 1.92371 avg_loss = 3.28757\n",
      "epoch no.4 train no.358820  loss = 2.47943 avg_loss = 3.32795\n",
      "epoch no.4 train no.358830  loss = 4.60252 avg_loss = 3.29777\n",
      "epoch no.4 train no.358840  loss = 3.37688 avg_loss = 3.28693\n",
      "epoch no.4 train no.358850  loss = 1.82251 avg_loss = 3.26169\n",
      "epoch no.4 train no.358860  loss = 3.74588 avg_loss = 3.27498\n",
      "epoch no.4 train no.358870  loss = 3.17160 avg_loss = 3.30154\n",
      "epoch no.4 train no.358880  loss = 2.45376 avg_loss = 3.30081\n",
      "epoch no.4 train no.358890  loss = 2.28180 avg_loss = 3.27510\n",
      "epoch no.4 train no.358900  loss = 2.71909 avg_loss = 3.28661\n",
      "epoch no.4 train no.358910  loss = 2.05078 avg_loss = 3.26369\n",
      "epoch no.4 train no.358920  loss = 3.65499 avg_loss = 3.27712\n",
      "epoch no.4 train no.358930  loss = 3.03299 avg_loss = 3.23438\n",
      "epoch no.4 train no.358940  loss = 3.19391 avg_loss = 3.22725\n",
      "epoch no.4 train no.358950  loss = 2.79481 avg_loss = 3.19613\n",
      "epoch no.4 train no.358960  loss = 3.45176 avg_loss = 3.21915\n",
      "epoch no.4 train no.358970  loss = 4.13944 avg_loss = 3.26509\n",
      "epoch no.4 train no.358980  loss = 2.73976 avg_loss = 3.28899\n",
      "epoch no.4 train no.358990  loss = 1.86263 avg_loss = 3.28733\n",
      "epoch no.4 train no.359000  loss = 3.76685 avg_loss = 3.25808\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '을', '▁좋은', '▁노래', '송', '</s>']\n",
      "기분전환하기 좋은 팝송</s>\n",
      "epoch no.4 train no.359010  loss = 2.34503 avg_loss = 3.25521\n",
      "epoch no.4 train no.359020  loss = 3.75615 avg_loss = 3.28672\n",
      "epoch no.4 train no.359030  loss = 2.25048 avg_loss = 3.29740\n",
      "epoch no.4 train no.359040  loss = 3.38311 avg_loss = 3.34729\n",
      "epoch no.4 train no.359050  loss = 3.12376 avg_loss = 3.33438\n",
      "epoch no.4 train no.359060  loss = 3.94003 avg_loss = 3.32730\n",
      "epoch no.4 train no.359070  loss = 2.26086 avg_loss = 3.32582\n",
      "epoch no.4 train no.359080  loss = 3.23768 avg_loss = 3.31221\n",
      "epoch no.4 train no.359090  loss = 3.52158 avg_loss = 3.29673\n",
      "epoch no.4 train no.359100  loss = 5.88916 avg_loss = 3.36812\n",
      "epoch no.4 train no.359110  loss = 4.25543 avg_loss = 3.39449\n",
      "epoch no.4 train no.359120  loss = 1.83050 avg_loss = 3.36034\n",
      "epoch no.4 train no.359130  loss = 2.59051 avg_loss = 3.33845\n",
      "epoch no.4 train no.359140  loss = 2.56803 avg_loss = 3.32876\n",
      "epoch no.4 train no.359150  loss = 3.80818 avg_loss = 3.30028\n",
      "epoch no.4 train no.359160  loss = 3.22731 avg_loss = 3.27820\n",
      "epoch no.4 train no.359170  loss = 3.10342 avg_loss = 3.29584\n",
      "epoch no.4 train no.359180  loss = 2.74562 avg_loss = 3.28020\n",
      "epoch no.4 train no.359190  loss = 2.80602 avg_loss = 3.30715\n",
      "epoch no.4 train no.359200  loss = 2.61521 avg_loss = 3.27173\n",
      "epoch no.4 train no.359210  loss = 3.35713 avg_loss = 3.27659\n",
      "epoch no.4 train no.359220  loss = 3.40969 avg_loss = 3.27932\n",
      "epoch no.4 train no.359230  loss = 1.84065 avg_loss = 3.25772\n",
      "epoch no.4 train no.359240  loss = 4.96565 avg_loss = 3.25844\n",
      "epoch no.4 train no.359250  loss = 2.67496 avg_loss = 3.23523\n",
      "epoch no.4 train no.359260  loss = 2.69218 avg_loss = 3.28381\n",
      "epoch no.4 train no.359270  loss = 2.19413 avg_loss = 3.28504\n",
      "epoch no.4 train no.359280  loss = 2.49244 avg_loss = 3.28188\n",
      "epoch no.4 train no.359290  loss = 3.44740 avg_loss = 3.31096\n",
      "epoch no.4 train no.359300  loss = 2.79437 avg_loss = 3.29784\n",
      "epoch no.4 train no.359310  loss = 2.25488 avg_loss = 3.27537\n",
      "epoch no.4 train no.359320  loss = 3.44063 avg_loss = 3.27493\n",
      "epoch no.4 train no.359330  loss = 2.70151 avg_loss = 3.24887\n",
      "epoch no.4 train no.359340  loss = 3.11435 avg_loss = 3.29729\n",
      "epoch no.4 train no.359350  loss = 4.22373 avg_loss = 3.32234\n",
      "epoch no.4 train no.359360  loss = 4.39928 avg_loss = 3.34941\n",
      "epoch no.4 train no.359370  loss = 2.47741 avg_loss = 3.38123\n",
      "epoch no.4 train no.359380  loss = 3.47092 avg_loss = 3.42420\n",
      "epoch no.4 train no.359390  loss = 2.46068 avg_loss = 3.38476\n",
      "epoch no.4 train no.359400  loss = 3.40111 avg_loss = 3.40079\n",
      "epoch no.4 train no.359410  loss = 4.01371 avg_loss = 3.40339\n",
      "epoch no.4 train no.359420  loss = 2.12949 avg_loss = 3.38455\n",
      "epoch no.4 train no.359430  loss = 3.82264 avg_loss = 3.34242\n",
      "epoch no.4 train no.359440  loss = 3.17472 avg_loss = 3.36280\n",
      "epoch no.4 train no.359450  loss = 3.86709 avg_loss = 3.37899\n",
      "epoch no.4 train no.359460  loss = 2.82837 avg_loss = 3.36890\n",
      "epoch no.4 train no.359470  loss = 5.76916 avg_loss = 3.36008\n",
      "epoch no.4 train no.359480  loss = 3.30238 avg_loss = 3.38568\n",
      "epoch no.4 train no.359490  loss = 2.39321 avg_loss = 3.38732\n",
      "epoch no.4 train no.359500  loss = 2.21909 avg_loss = 3.35168\n",
      "epoch no.4 train no.359510  loss = 4.57301 avg_loss = 3.31844\n",
      "epoch no.4 train no.359520  loss = 3.46242 avg_loss = 3.30231\n",
      "epoch no.4 train no.359530  loss = 3.91196 avg_loss = 3.28255\n",
      "epoch no.4 train no.359540  loss = 2.66843 avg_loss = 3.27998\n",
      "epoch no.4 train no.359550  loss = 3.58976 avg_loss = 3.31351\n",
      "epoch no.4 train no.359560  loss = 4.15547 avg_loss = 3.32781\n",
      "epoch no.4 train no.359570  loss = 3.31952 avg_loss = 3.33834\n",
      "epoch no.4 train no.359580  loss = 4.35183 avg_loss = 3.37888\n",
      "epoch no.4 train no.359590  loss = 3.28482 avg_loss = 3.44971\n",
      "epoch no.4 train no.359600  loss = 3.67409 avg_loss = 3.46957\n",
      "epoch no.4 train no.359610  loss = 3.39552 avg_loss = 3.43046\n",
      "epoch no.4 train no.359620  loss = 3.16671 avg_loss = 3.40210\n",
      "epoch no.4 train no.359630  loss = 2.87036 avg_loss = 3.36649\n",
      "epoch no.4 train no.359640  loss = 3.27465 avg_loss = 3.37119\n",
      "epoch no.4 train no.359650  loss = 3.60672 avg_loss = 3.36670\n",
      "epoch no.4 train no.359660  loss = 2.15903 avg_loss = 3.37533\n",
      "epoch no.4 train no.359670  loss = 2.75612 avg_loss = 3.38099\n",
      "epoch no.4 train no.359680  loss = 3.36910 avg_loss = 3.32247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.359690  loss = 3.93372 avg_loss = 3.31362\n",
      "epoch no.4 train no.359700  loss = 2.63063 avg_loss = 3.34475\n",
      "epoch no.4 train no.359710  loss = 4.51653 avg_loss = 3.33087\n",
      "epoch no.4 train no.359720  loss = 2.36653 avg_loss = 3.27371\n",
      "epoch no.4 train no.359730  loss = 1.94667 avg_loss = 3.25511\n",
      "epoch no.4 train no.359740  loss = 3.14641 avg_loss = 3.22024\n",
      "epoch no.4 train no.359750  loss = 5.36284 avg_loss = 3.25422\n",
      "epoch no.4 train no.359760  loss = 3.30316 avg_loss = 3.31036\n",
      "epoch no.4 train no.359770  loss = 3.13421 avg_loss = 3.28875\n",
      "epoch no.4 train no.359780  loss = 2.24506 avg_loss = 3.29881\n",
      "epoch no.4 train no.359790  loss = 3.38307 avg_loss = 3.27713\n",
      "epoch no.4 train no.359800  loss = 4.40511 avg_loss = 3.25091\n",
      "epoch no.4 train no.359810  loss = 4.96274 avg_loss = 3.26885\n",
      "epoch no.4 train no.359820  loss = 3.12836 avg_loss = 3.28297\n",
      "epoch no.4 train no.359830  loss = 3.54473 avg_loss = 3.28954\n",
      "epoch no.4 train no.359840  loss = 1.50257 avg_loss = 3.25024\n",
      "epoch no.4 train no.359850  loss = 4.18751 avg_loss = 3.27718\n",
      "epoch no.4 train no.359860  loss = 2.63836 avg_loss = 3.22411\n",
      "epoch no.4 train no.359870  loss = 3.61212 avg_loss = 3.24186\n",
      "epoch no.4 train no.359880  loss = 4.11497 avg_loss = 3.25401\n",
      "epoch no.4 train no.359890  loss = 3.24674 avg_loss = 3.26512\n",
      "epoch no.4 train no.359900  loss = 2.21202 avg_loss = 3.25310\n",
      "epoch no.4 train no.359910  loss = 4.04669 avg_loss = 3.28129\n",
      "epoch no.4 train no.359920  loss = 4.09198 avg_loss = 3.29128\n",
      "epoch no.4 train no.359930  loss = 3.14222 avg_loss = 3.32800\n",
      "epoch no.4 train no.359940  loss = 3.20242 avg_loss = 3.34092\n",
      "epoch no.4 train no.359950  loss = 3.21423 avg_loss = 3.33765\n",
      "epoch no.4 train no.359960  loss = 1.79897 avg_loss = 3.28915\n",
      "epoch no.4 train no.359970  loss = 2.96188 avg_loss = 3.30133\n",
      "epoch no.4 train no.359980  loss = 2.56606 avg_loss = 3.31514\n",
      "epoch no.4 train no.359990  loss = 5.71837 avg_loss = 3.32857\n",
      "epoch no.4 train no.360000  loss = 3.27219 avg_loss = 3.28055\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '용', '▁필요할', '▁당신을', '</s>']\n",
      "기분전환이 필요한 순간</s>\n",
      "epoch no.4 train no.360010  loss = 2.96749 avg_loss = 3.24547\n",
      "epoch no.4 train no.360020  loss = 2.43272 avg_loss = 3.19907\n",
      "epoch no.4 train no.360030  loss = 3.07614 avg_loss = 3.19001\n",
      "epoch no.4 train no.360040  loss = 3.44412 avg_loss = 3.15940\n",
      "epoch no.4 train no.360050  loss = 2.51637 avg_loss = 3.16576\n",
      "epoch no.4 train no.360060  loss = 5.51630 avg_loss = 3.20000\n",
      "epoch no.4 train no.360070  loss = 2.89638 avg_loss = 3.19552\n",
      "epoch no.4 train no.360080  loss = 1.86111 avg_loss = 3.17872\n",
      "epoch no.4 train no.360090  loss = 2.35489 avg_loss = 3.19711\n",
      "epoch no.4 train no.360100  loss = 2.97087 avg_loss = 3.17361\n",
      "epoch no.4 train no.360110  loss = 2.65572 avg_loss = 3.12106\n",
      "epoch no.4 train no.360120  loss = 5.77359 avg_loss = 3.19245\n",
      "epoch no.4 train no.360130  loss = 3.29403 avg_loss = 3.24535\n",
      "epoch no.4 train no.360140  loss = 2.75193 avg_loss = 3.23107\n",
      "epoch no.4 train no.360150  loss = 3.72792 avg_loss = 3.26395\n",
      "epoch no.4 train no.360160  loss = 4.04337 avg_loss = 3.27226\n",
      "epoch no.4 train no.360170  loss = 3.27113 avg_loss = 3.25623\n",
      "epoch no.4 train no.360180  loss = 2.90930 avg_loss = 3.22570\n",
      "epoch no.4 train no.360190  loss = 4.17527 avg_loss = 3.22008\n",
      "epoch no.4 train no.360200  loss = 3.28253 avg_loss = 3.25811\n",
      "epoch no.4 train no.360210  loss = 3.37688 avg_loss = 3.27143\n",
      "epoch no.4 train no.360220  loss = 3.82257 avg_loss = 3.27581\n",
      "epoch no.4 train no.360230  loss = 2.84986 avg_loss = 3.24242\n",
      "epoch no.4 train no.360240  loss = 3.91319 avg_loss = 3.25125\n",
      "epoch no.4 train no.360250  loss = 3.91389 avg_loss = 3.27244\n",
      "epoch no.4 train no.360260  loss = 3.20227 avg_loss = 3.28471\n",
      "epoch no.4 train no.360270  loss = 2.16401 avg_loss = 3.26356\n",
      "epoch no.4 train no.360280  loss = 3.22482 avg_loss = 3.24479\n",
      "epoch no.4 train no.360290  loss = 4.79561 avg_loss = 3.24803\n",
      "epoch no.4 train no.360300  loss = 2.59424 avg_loss = 3.25475\n",
      "epoch no.4 train no.360310  loss = 3.42653 avg_loss = 3.25382\n",
      "epoch no.4 train no.360320  loss = 2.90852 avg_loss = 3.25059\n",
      "epoch no.4 train no.360330  loss = 3.43452 avg_loss = 3.22881\n",
      "epoch no.4 train no.360340  loss = 3.52689 avg_loss = 3.21167\n",
      "epoch no.4 train no.360350  loss = 4.99022 avg_loss = 3.23584\n",
      "epoch no.4 train no.360360  loss = 1.84617 avg_loss = 3.19382\n",
      "epoch no.4 train no.360370  loss = 3.10189 avg_loss = 3.17086\n",
      "epoch no.4 train no.360380  loss = 1.93189 avg_loss = 3.20094\n",
      "epoch no.4 train no.360390  loss = 2.87723 avg_loss = 3.21369\n",
      "epoch no.4 train no.360400  loss = 2.68521 avg_loss = 3.20635\n",
      "epoch no.4 train no.360410  loss = 3.74634 avg_loss = 3.18090\n",
      "epoch no.4 train no.360420  loss = 2.73349 avg_loss = 3.21827\n",
      "epoch no.4 train no.360430  loss = 5.00511 avg_loss = 3.20243\n",
      "epoch no.4 train no.360440  loss = 2.01407 avg_loss = 3.21916\n",
      "epoch no.4 train no.360450  loss = 4.42603 avg_loss = 3.22332\n",
      "epoch no.4 train no.360460  loss = 3.43962 avg_loss = 3.23089\n",
      "epoch no.4 train no.360470  loss = 4.01787 avg_loss = 3.21514\n",
      "epoch no.4 train no.360480  loss = 2.98378 avg_loss = 3.19086\n",
      "epoch no.4 train no.360490  loss = 4.61440 avg_loss = 3.20500\n",
      "epoch no.4 train no.360500  loss = 3.06429 avg_loss = 3.19544\n",
      "epoch no.4 train no.360510  loss = 3.28060 avg_loss = 3.19525\n",
      "epoch no.4 train no.360520  loss = 3.49859 avg_loss = 3.20875\n",
      "epoch no.4 train no.360530  loss = 2.36359 avg_loss = 3.20879\n",
      "epoch no.4 train no.360540  loss = 4.97416 avg_loss = 3.21657\n",
      "epoch no.4 train no.360550  loss = 2.81711 avg_loss = 3.21820\n",
      "epoch no.4 train no.360560  loss = 2.93412 avg_loss = 3.20411\n",
      "epoch no.4 train no.360570  loss = 2.15547 avg_loss = 3.17791\n",
      "epoch no.4 train no.360580  loss = 2.36487 avg_loss = 3.22344\n",
      "epoch no.4 train no.360590  loss = 3.77509 avg_loss = 3.26745\n",
      "epoch no.4 train no.360600  loss = 4.14937 avg_loss = 3.26168\n",
      "epoch no.4 train no.360610  loss = 2.68353 avg_loss = 3.24999\n",
      "epoch no.4 train no.360620  loss = 5.10838 avg_loss = 3.29152\n",
      "epoch no.4 train no.360630  loss = 5.33973 avg_loss = 3.36539\n",
      "epoch no.4 train no.360640  loss = 2.00354 avg_loss = 3.35350\n",
      "epoch no.4 train no.360650  loss = 2.53741 avg_loss = 3.36251\n",
      "epoch no.4 train no.360660  loss = 2.35965 avg_loss = 3.34015\n",
      "epoch no.4 train no.360670  loss = 1.90949 avg_loss = 3.37193\n",
      "epoch no.4 train no.360680  loss = 2.09622 avg_loss = 3.32100\n",
      "epoch no.4 train no.360690  loss = 3.99099 avg_loss = 3.28950\n",
      "epoch no.4 train no.360700  loss = 3.36234 avg_loss = 3.33398\n",
      "epoch no.4 train no.360710  loss = 2.36667 avg_loss = 3.30690\n",
      "epoch no.4 train no.360720  loss = 4.04136 avg_loss = 3.28645\n",
      "epoch no.4 train no.360730  loss = 4.81634 avg_loss = 3.31244\n",
      "epoch no.4 train no.360740  loss = 3.54980 avg_loss = 3.29977\n",
      "epoch no.4 train no.360750  loss = 2.51868 avg_loss = 3.28348\n",
      "epoch no.4 train no.360760  loss = 1.93193 avg_loss = 3.30620\n",
      "epoch no.4 train no.360770  loss = 3.43765 avg_loss = 3.31144\n",
      "epoch no.4 train no.360780  loss = 3.43486 avg_loss = 3.26357\n",
      "epoch no.4 train no.360790  loss = 2.42223 avg_loss = 3.28771\n",
      "epoch no.4 train no.360800  loss = 2.18438 avg_loss = 3.30720\n",
      "epoch no.4 train no.360810  loss = 2.69104 avg_loss = 3.29745\n",
      "epoch no.4 train no.360820  loss = 3.14657 avg_loss = 3.28354\n",
      "epoch no.4 train no.360830  loss = 4.10749 avg_loss = 3.28663\n",
      "epoch no.4 train no.360840  loss = 2.83918 avg_loss = 3.28935\n",
      "epoch no.4 train no.360850  loss = 4.46785 avg_loss = 3.33212\n",
      "epoch no.4 train no.360860  loss = 4.02534 avg_loss = 3.33299\n",
      "epoch no.4 train no.360870  loss = 3.47179 avg_loss = 3.37049\n",
      "epoch no.4 train no.360880  loss = 2.11883 avg_loss = 3.29271\n",
      "epoch no.4 train no.360890  loss = 5.03712 avg_loss = 3.32454\n",
      "epoch no.4 train no.360900  loss = 2.97953 avg_loss = 3.31257\n",
      "epoch no.4 train no.360910  loss = 4.14613 avg_loss = 3.35901\n",
      "epoch no.4 train no.360920  loss = 4.75081 avg_loss = 3.33347\n",
      "epoch no.4 train no.360930  loss = 3.79628 avg_loss = 3.29551\n",
      "epoch no.4 train no.360940  loss = 2.83578 avg_loss = 3.32236\n",
      "epoch no.4 train no.360950  loss = 3.49094 avg_loss = 3.33348\n",
      "epoch no.4 train no.360960  loss = 2.68104 avg_loss = 3.33913\n",
      "epoch no.4 train no.360970  loss = 2.28490 avg_loss = 3.31136\n",
      "epoch no.4 train no.360980  loss = 2.45616 avg_loss = 3.30497\n",
      "epoch no.4 train no.360990  loss = 2.80066 avg_loss = 3.28039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.361000  loss = 2.31679 avg_loss = 3.24313\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋', '이', '▁좋은', '▁팝', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.4 train no.361010  loss = 2.79085 avg_loss = 3.23476\n",
      "epoch no.4 train no.361020  loss = 3.08313 avg_loss = 3.24864\n",
      "epoch no.4 train no.361030  loss = 4.20299 avg_loss = 3.26690\n",
      "epoch no.4 train no.361040  loss = 2.87705 avg_loss = 3.21520\n",
      "epoch no.4 train no.361050  loss = 1.70506 avg_loss = 3.18574\n",
      "epoch no.4 train no.361060  loss = 5.00449 avg_loss = 3.21045\n",
      "epoch no.4 train no.361070  loss = 4.05762 avg_loss = 3.25170\n",
      "epoch no.4 train no.361080  loss = 3.30647 avg_loss = 3.23403\n",
      "epoch no.4 train no.361090  loss = 3.10516 avg_loss = 3.24139\n",
      "epoch no.4 train no.361100  loss = 2.03616 avg_loss = 3.22295\n",
      "epoch no.4 train no.361110  loss = 1.91957 avg_loss = 3.18570\n",
      "epoch no.4 train no.361120  loss = 3.52133 avg_loss = 3.25380\n",
      "epoch no.4 train no.361130  loss = 3.95856 avg_loss = 3.26295\n",
      "epoch no.4 train no.361140  loss = 3.35465 avg_loss = 3.28026\n",
      "epoch no.4 train no.361150  loss = 4.05296 avg_loss = 3.33014\n",
      "epoch no.4 train no.361160  loss = 2.49172 avg_loss = 3.34026\n",
      "epoch no.4 train no.361170  loss = 2.86453 avg_loss = 3.34263\n",
      "epoch no.4 train no.361180  loss = 3.73239 avg_loss = 3.33445\n",
      "epoch no.4 train no.361190  loss = 3.06688 avg_loss = 3.36840\n",
      "epoch no.4 train no.361200  loss = 4.51732 avg_loss = 3.35394\n",
      "epoch no.4 train no.361210  loss = 1.82069 avg_loss = 3.31674\n",
      "epoch no.4 train no.361220  loss = 2.89323 avg_loss = 3.29896\n",
      "epoch no.4 train no.361230  loss = 1.87344 avg_loss = 3.29123\n",
      "epoch no.4 train no.361240  loss = 2.64467 avg_loss = 3.29972\n",
      "epoch no.4 train no.361250  loss = 3.70209 avg_loss = 3.31734\n",
      "epoch no.4 train no.361260  loss = 1.79900 avg_loss = 3.31770\n",
      "epoch no.4 train no.361270  loss = 2.05905 avg_loss = 3.35167\n",
      "epoch no.4 train no.361280  loss = 2.46435 avg_loss = 3.33646\n",
      "epoch no.4 train no.361290  loss = 2.65967 avg_loss = 3.33739\n",
      "epoch no.4 train no.361300  loss = 2.27849 avg_loss = 3.32782\n",
      "epoch no.4 train no.361310  loss = 3.60860 avg_loss = 3.35775\n",
      "epoch no.4 train no.361320  loss = 2.48556 avg_loss = 3.32151\n",
      "epoch no.4 train no.361330  loss = 2.75094 avg_loss = 3.31313\n",
      "epoch no.4 train no.361340  loss = 3.67315 avg_loss = 3.27561\n",
      "epoch no.4 train no.361350  loss = 2.61158 avg_loss = 3.24923\n",
      "epoch no.4 train no.361360  loss = 2.53665 avg_loss = 3.24520\n",
      "epoch no.4 train no.361370  loss = 3.94952 avg_loss = 3.28813\n",
      "epoch no.4 train no.361380  loss = 3.83685 avg_loss = 3.31140\n",
      "epoch no.4 train no.361390  loss = 2.31188 avg_loss = 3.30363\n",
      "epoch no.4 train no.361400  loss = 4.05801 avg_loss = 3.32329\n",
      "epoch no.4 train no.361410  loss = 3.05913 avg_loss = 3.32369\n",
      "epoch no.4 train no.361420  loss = 3.23536 avg_loss = 3.30484\n",
      "epoch no.4 train no.361430  loss = 3.70775 avg_loss = 3.28654\n",
      "epoch no.4 train no.361440  loss = 3.69564 avg_loss = 3.32073\n",
      "epoch no.4 train no.361450  loss = 2.09882 avg_loss = 3.31248\n",
      "epoch no.4 train no.361460  loss = 4.01641 avg_loss = 3.35254\n",
      "epoch no.4 train no.361470  loss = 2.69566 avg_loss = 3.37642\n",
      "epoch no.4 train no.361480  loss = 4.07771 avg_loss = 3.40672\n",
      "epoch no.4 train no.361490  loss = 3.14249 avg_loss = 3.39442\n",
      "epoch no.4 train no.361500  loss = 2.59982 avg_loss = 3.38314\n",
      "epoch no.4 train no.361510  loss = 4.09252 avg_loss = 3.38761\n",
      "epoch no.4 train no.361520  loss = 1.74045 avg_loss = 3.37156\n",
      "epoch no.4 train no.361530  loss = 3.58324 avg_loss = 3.37838\n",
      "epoch no.4 train no.361540  loss = 3.00021 avg_loss = 3.39863\n",
      "epoch no.4 train no.361550  loss = 1.95496 avg_loss = 3.32537\n",
      "epoch no.4 train no.361560  loss = 3.62692 avg_loss = 3.35092\n",
      "epoch no.4 train no.361570  loss = 2.90157 avg_loss = 3.31888\n",
      "epoch no.4 train no.361580  loss = 4.51289 avg_loss = 3.31330\n",
      "epoch no.4 train no.361590  loss = 4.80384 avg_loss = 3.33691\n",
      "epoch no.4 train no.361600  loss = 3.84878 avg_loss = 3.30202\n",
      "epoch no.4 train no.361610  loss = 3.77049 avg_loss = 3.31781\n",
      "epoch no.4 train no.361620  loss = 5.01938 avg_loss = 3.34780\n",
      "epoch no.4 train no.361630  loss = 4.45300 avg_loss = 3.37922\n",
      "epoch no.4 train no.361640  loss = 3.67911 avg_loss = 3.39944\n",
      "epoch no.4 train no.361650  loss = 3.20683 avg_loss = 3.37814\n",
      "epoch no.4 train no.361660  loss = 1.95838 avg_loss = 3.33255\n",
      "epoch no.4 train no.361670  loss = 2.59967 avg_loss = 3.35939\n",
      "epoch no.4 train no.361680  loss = 3.20677 avg_loss = 3.37230\n",
      "epoch no.4 train no.361690  loss = 1.62738 avg_loss = 3.36873\n",
      "epoch no.4 train no.361700  loss = 3.41381 avg_loss = 3.36890\n",
      "epoch no.4 train no.361710  loss = 2.32884 avg_loss = 3.31039\n",
      "epoch no.4 train no.361720  loss = 2.98527 avg_loss = 3.28487\n",
      "epoch no.4 train no.361730  loss = 3.31557 avg_loss = 3.28142\n",
      "epoch no.4 train no.361740  loss = 2.82041 avg_loss = 3.26213\n",
      "epoch no.4 train no.361750  loss = 2.59472 avg_loss = 3.28738\n",
      "epoch no.4 train no.361760  loss = 4.19535 avg_loss = 3.26842\n",
      "epoch no.4 train no.361770  loss = 4.44482 avg_loss = 3.31327\n",
      "epoch no.4 train no.361780  loss = 2.54490 avg_loss = 3.38056\n",
      "epoch no.4 train no.361790  loss = 3.77855 avg_loss = 3.40766\n",
      "epoch no.4 train no.361800  loss = 3.76551 avg_loss = 3.42984\n",
      "epoch no.4 train no.361810  loss = 2.38154 avg_loss = 3.39117\n",
      "epoch no.4 train no.361820  loss = 3.13003 avg_loss = 3.42847\n",
      "epoch no.4 train no.361830  loss = 2.18522 avg_loss = 3.37787\n",
      "epoch no.4 train no.361840  loss = 4.27913 avg_loss = 3.43614\n",
      "epoch no.4 train no.361850  loss = 2.68824 avg_loss = 3.40353\n",
      "epoch no.4 train no.361860  loss = 4.74798 avg_loss = 3.38520\n",
      "epoch no.4 train no.361870  loss = 2.28779 avg_loss = 3.36660\n",
      "epoch no.4 train no.361880  loss = 2.82364 avg_loss = 3.34863\n",
      "epoch no.4 train no.361890  loss = 2.52194 avg_loss = 3.34737\n",
      "epoch no.4 train no.361900  loss = 2.12260 avg_loss = 3.33167\n",
      "epoch no.4 train no.361910  loss = 3.03794 avg_loss = 3.32511\n",
      "epoch no.4 train no.361920  loss = 3.65487 avg_loss = 3.30571\n",
      "epoch no.4 train no.361930  loss = 2.62885 avg_loss = 3.28148\n",
      "epoch no.4 train no.361940  loss = 3.22378 avg_loss = 3.27960\n",
      "epoch no.4 train no.361950  loss = 2.80451 avg_loss = 3.27322\n",
      "epoch no.4 train no.361960  loss = 3.60832 avg_loss = 3.33592\n",
      "epoch no.4 train no.361970  loss = 3.54454 avg_loss = 3.36907\n",
      "epoch no.4 train no.361980  loss = 3.01395 avg_loss = 3.35014\n",
      "epoch no.4 train no.361990  loss = 3.19455 avg_loss = 3.34299\n",
      "epoch no.4 train no.362000  loss = 1.99975 avg_loss = 3.31069\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋', '을', '▁위한', '▁감성', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.4 train no.362010  loss = 3.56396 avg_loss = 3.26224\n",
      "epoch no.4 train no.362020  loss = 2.55479 avg_loss = 3.29612\n",
      "epoch no.4 train no.362030  loss = 3.73871 avg_loss = 3.31610\n",
      "epoch no.4 train no.362040  loss = 3.72835 avg_loss = 3.31742\n",
      "epoch no.4 train no.362050  loss = 3.62098 avg_loss = 3.33778\n",
      "epoch no.4 train no.362060  loss = 6.28333 avg_loss = 3.39761\n",
      "epoch no.4 train no.362070  loss = 5.42320 avg_loss = 3.39734\n",
      "epoch no.4 train no.362080  loss = 3.44988 avg_loss = 3.38829\n",
      "epoch no.4 train no.362090  loss = 3.94913 avg_loss = 3.36667\n",
      "epoch no.4 train no.362100  loss = 4.03994 avg_loss = 3.36804\n",
      "epoch no.4 train no.362110  loss = 3.51720 avg_loss = 3.41118\n",
      "epoch no.4 train no.362120  loss = 3.61908 avg_loss = 3.41649\n",
      "epoch no.4 train no.362130  loss = 4.61109 avg_loss = 3.43339\n",
      "epoch no.4 train no.362140  loss = 3.49599 avg_loss = 3.40664\n",
      "epoch no.4 train no.362150  loss = 2.23441 avg_loss = 3.35704\n",
      "epoch no.4 train no.362160  loss = 2.96555 avg_loss = 3.31578\n",
      "epoch no.4 train no.362170  loss = 3.59187 avg_loss = 3.31820\n",
      "epoch no.4 train no.362180  loss = 5.29776 avg_loss = 3.37592\n",
      "epoch no.4 train no.362190  loss = 3.43935 avg_loss = 3.35581\n",
      "epoch no.4 train no.362200  loss = 3.84857 avg_loss = 3.32796\n",
      "epoch no.4 train no.362210  loss = 3.79598 avg_loss = 3.33747\n",
      "epoch no.4 train no.362220  loss = 3.34690 avg_loss = 3.31372\n",
      "epoch no.4 train no.362230  loss = 5.05160 avg_loss = 3.34684\n",
      "epoch no.4 train no.362240  loss = 2.10251 avg_loss = 3.31726\n",
      "epoch no.4 train no.362250  loss = 2.96256 avg_loss = 3.29173\n",
      "epoch no.4 train no.362260  loss = 2.25970 avg_loss = 3.27319\n",
      "epoch no.4 train no.362270  loss = 2.69637 avg_loss = 3.29456\n",
      "epoch no.4 train no.362280  loss = 3.80186 avg_loss = 3.36380\n",
      "epoch no.4 train no.362290  loss = 3.22695 avg_loss = 3.34011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.362300  loss = 5.99643 avg_loss = 3.39843\n",
      "epoch no.4 train no.362310  loss = 3.62495 avg_loss = 3.42691\n",
      "epoch no.4 train no.362320  loss = 4.35065 avg_loss = 3.42944\n",
      "epoch no.4 train no.362330  loss = 4.21171 avg_loss = 3.39916\n",
      "epoch no.4 train no.362340  loss = 2.34613 avg_loss = 3.40539\n",
      "epoch no.4 train no.362350  loss = 2.59745 avg_loss = 3.39622\n",
      "epoch no.4 train no.362360  loss = 2.70680 avg_loss = 3.38412\n",
      "epoch no.4 train no.362370  loss = 2.02224 avg_loss = 3.34714\n",
      "epoch no.4 train no.362380  loss = 4.53884 avg_loss = 3.36309\n",
      "epoch no.4 train no.362390  loss = 3.09663 avg_loss = 3.40406\n",
      "epoch no.4 train no.362400  loss = 3.38459 avg_loss = 3.37870\n",
      "epoch no.4 train no.362410  loss = 2.60160 avg_loss = 3.37629\n",
      "epoch no.4 train no.362420  loss = 1.36192 avg_loss = 3.35682\n",
      "epoch no.4 train no.362430  loss = 3.14556 avg_loss = 3.34637\n",
      "epoch no.4 train no.362440  loss = 2.81087 avg_loss = 3.33237\n",
      "epoch no.4 train no.362450  loss = 2.55313 avg_loss = 3.36052\n",
      "epoch no.4 train no.362460  loss = 4.62220 avg_loss = 3.35505\n",
      "epoch no.4 train no.362470  loss = 2.90981 avg_loss = 3.38945\n",
      "epoch no.4 train no.362480  loss = 2.25130 avg_loss = 3.36388\n",
      "epoch no.4 train no.362490  loss = 2.77594 avg_loss = 3.36047\n",
      "epoch no.4 train no.362500  loss = 4.03476 avg_loss = 3.35640\n",
      "epoch no.4 train no.362510  loss = 2.40699 avg_loss = 3.38648\n",
      "epoch no.4 train no.362520  loss = 2.96375 avg_loss = 3.35543\n",
      "epoch no.4 train no.362530  loss = 4.03790 avg_loss = 3.36985\n",
      "epoch no.4 train no.362540  loss = 3.36582 avg_loss = 3.37179\n",
      "epoch no.4 train no.362550  loss = 3.65371 avg_loss = 3.38877\n",
      "epoch no.4 train no.362560  loss = 2.72718 avg_loss = 3.34107\n",
      "epoch no.4 train no.362570  loss = 3.60952 avg_loss = 3.34297\n",
      "epoch no.4 train no.362580  loss = 3.65179 avg_loss = 3.37156\n",
      "epoch no.4 train no.362590  loss = 3.96215 avg_loss = 3.37030\n",
      "epoch no.4 train no.362600  loss = 2.94016 avg_loss = 3.36302\n",
      "epoch no.4 train no.362610  loss = 2.71569 avg_loss = 3.42111\n",
      "epoch no.4 train no.362620  loss = 3.56392 avg_loss = 3.36636\n",
      "epoch no.4 train no.362630  loss = 4.32961 avg_loss = 3.40095\n",
      "epoch no.4 train no.362640  loss = 2.38277 avg_loss = 3.37096\n",
      "epoch no.4 train no.362650  loss = 3.27640 avg_loss = 3.39573\n",
      "epoch no.4 train no.362660  loss = 3.56447 avg_loss = 3.39763\n",
      "epoch no.4 train no.362670  loss = 3.75514 avg_loss = 3.39082\n",
      "epoch no.4 train no.362680  loss = 3.85287 avg_loss = 3.35934\n",
      "epoch no.4 train no.362690  loss = 3.99620 avg_loss = 3.34791\n",
      "epoch no.4 train no.362700  loss = 3.21871 avg_loss = 3.35293\n",
      "epoch no.4 train no.362710  loss = 2.50343 avg_loss = 3.33758\n",
      "epoch no.4 train no.362720  loss = 2.60972 avg_loss = 3.34135\n",
      "epoch no.4 train no.362730  loss = 3.82875 avg_loss = 3.31833\n",
      "epoch no.4 train no.362740  loss = 4.92892 avg_loss = 3.35144\n",
      "epoch no.4 train no.362750  loss = 1.85237 avg_loss = 3.33153\n",
      "epoch no.4 train no.362760  loss = 2.49153 avg_loss = 3.29598\n",
      "epoch no.4 train no.362770  loss = 3.53217 avg_loss = 3.27936\n",
      "epoch no.4 train no.362780  loss = 4.31343 avg_loss = 3.27083\n",
      "epoch no.4 train no.362790  loss = 2.88515 avg_loss = 3.27029\n",
      "epoch no.4 train no.362800  loss = 4.08480 avg_loss = 3.31161\n",
      "epoch no.4 train no.362810  loss = 2.20838 avg_loss = 3.26173\n",
      "epoch no.4 train no.362820  loss = 3.27248 avg_loss = 3.30418\n",
      "epoch no.4 train no.362830  loss = 2.81563 avg_loss = 3.29942\n",
      "epoch no.4 train no.362840  loss = 3.20295 avg_loss = 3.28194\n",
      "epoch no.4 train no.362850  loss = 3.14459 avg_loss = 3.27522\n",
      "epoch no.4 train no.362860  loss = 4.40185 avg_loss = 3.30103\n",
      "epoch no.4 train no.362870  loss = 2.99279 avg_loss = 3.27761\n",
      "epoch no.4 train no.362880  loss = 3.75211 avg_loss = 3.31363\n",
      "epoch no.4 train no.362890  loss = 2.99282 avg_loss = 3.31763\n",
      "epoch no.4 train no.362900  loss = 3.93954 avg_loss = 3.31641\n",
      "epoch no.4 train no.362910  loss = 2.53442 avg_loss = 3.27912\n",
      "epoch no.4 train no.362920  loss = 3.64719 avg_loss = 3.30573\n",
      "epoch no.4 train no.362930  loss = 3.00360 avg_loss = 3.32555\n",
      "epoch no.4 train no.362940  loss = 1.45618 avg_loss = 3.31471\n",
      "epoch no.4 train no.362950  loss = 4.21461 avg_loss = 3.31661\n",
      "epoch no.4 train no.362960  loss = 2.03801 avg_loss = 3.29084\n",
      "epoch no.4 train no.362970  loss = 4.66187 avg_loss = 3.28497\n",
      "epoch no.4 train no.362980  loss = 2.14523 avg_loss = 3.33292\n",
      "epoch no.4 train no.362990  loss = 1.21101 avg_loss = 3.30501\n",
      "epoch no.4 train no.363000  loss = 5.40903 avg_loss = 3.33471\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁딱', '▁좋은', '▁p', '</s>']\n",
      "기분전환에 딱 신나는 노래</s>\n",
      "epoch no.4 train no.363010  loss = 3.10499 avg_loss = 3.33656\n",
      "epoch no.4 train no.363020  loss = 2.63937 avg_loss = 3.29168\n",
      "epoch no.4 train no.363030  loss = 3.00676 avg_loss = 3.30486\n",
      "epoch no.4 train no.363040  loss = 3.88639 avg_loss = 3.29080\n",
      "epoch no.4 train no.363050  loss = 2.47425 avg_loss = 3.28027\n",
      "epoch no.4 train no.363060  loss = 4.57587 avg_loss = 3.30326\n",
      "epoch no.4 train no.363070  loss = 5.49353 avg_loss = 3.33096\n",
      "epoch no.4 train no.363080  loss = 2.50971 avg_loss = 3.32190\n",
      "epoch no.4 train no.363090  loss = 3.67073 avg_loss = 3.34477\n",
      "epoch no.4 train no.363100  loss = 3.44675 avg_loss = 3.32621\n",
      "epoch no.4 train no.363110  loss = 3.28494 avg_loss = 3.34221\n",
      "epoch no.4 train no.363120  loss = 4.52023 avg_loss = 3.34459\n",
      "epoch no.4 train no.363130  loss = 3.33850 avg_loss = 3.38404\n",
      "epoch no.4 train no.363140  loss = 4.11382 avg_loss = 3.39419\n",
      "epoch no.4 train no.363150  loss = 3.89316 avg_loss = 3.38113\n",
      "epoch no.4 train no.363160  loss = 5.62592 avg_loss = 3.40730\n",
      "epoch no.4 train no.363170  loss = 4.32323 avg_loss = 3.40581\n",
      "epoch no.4 train no.363180  loss = 3.96896 avg_loss = 3.41985\n",
      "epoch no.4 train no.363190  loss = 3.13090 avg_loss = 3.37041\n",
      "epoch no.4 train no.363200  loss = 3.42896 avg_loss = 3.38462\n",
      "epoch no.4 train no.363210  loss = 3.19840 avg_loss = 3.38680\n",
      "epoch no.4 train no.363220  loss = 2.37168 avg_loss = 3.36987\n",
      "epoch no.4 train no.363230  loss = 2.63383 avg_loss = 3.33590\n",
      "epoch no.4 train no.363240  loss = 3.98588 avg_loss = 3.35565\n",
      "epoch no.4 train no.363250  loss = 2.12288 avg_loss = 3.33226\n",
      "epoch no.4 train no.363260  loss = 2.51796 avg_loss = 3.33254\n",
      "epoch no.4 train no.363270  loss = 3.24315 avg_loss = 3.31250\n",
      "epoch no.4 train no.363280  loss = 2.41062 avg_loss = 3.28779\n",
      "epoch no.4 train no.363290  loss = 2.56400 avg_loss = 3.29492\n",
      "epoch no.4 train no.363300  loss = 4.54534 avg_loss = 3.34929\n",
      "epoch no.4 train no.363310  loss = 2.83556 avg_loss = 3.37185\n",
      "epoch no.4 train no.363320  loss = 3.86744 avg_loss = 3.32028\n",
      "epoch no.4 train no.363330  loss = 3.03028 avg_loss = 3.32198\n",
      "epoch no.4 train no.363340  loss = 3.34852 avg_loss = 3.29520\n",
      "epoch no.4 train no.363350  loss = 2.91113 avg_loss = 3.29773\n",
      "epoch no.4 train no.363360  loss = 3.73376 avg_loss = 3.27823\n",
      "epoch no.4 train no.363370  loss = 3.31111 avg_loss = 3.29563\n",
      "epoch no.4 train no.363380  loss = 2.23672 avg_loss = 3.31611\n",
      "epoch no.4 train no.363390  loss = 4.72925 avg_loss = 3.34669\n",
      "epoch no.4 train no.363400  loss = 2.70394 avg_loss = 3.33753\n",
      "epoch no.4 train no.363410  loss = 2.39311 avg_loss = 3.35275\n",
      "epoch no.4 train no.363420  loss = 2.26542 avg_loss = 3.32242\n",
      "epoch no.4 train no.363430  loss = 2.56151 avg_loss = 3.32172\n",
      "epoch no.4 train no.363440  loss = 2.33045 avg_loss = 3.31503\n",
      "epoch no.4 train no.363450  loss = 5.08724 avg_loss = 3.31054\n",
      "epoch no.4 train no.363460  loss = 3.00975 avg_loss = 3.34877\n",
      "epoch no.4 train no.363470  loss = 2.21933 avg_loss = 3.31623\n",
      "epoch no.4 train no.363480  loss = 3.98824 avg_loss = 3.37613\n",
      "epoch no.4 train no.363490  loss = 2.16157 avg_loss = 3.35048\n",
      "epoch no.4 train no.363500  loss = 4.13319 avg_loss = 3.38909\n",
      "epoch no.4 train no.363510  loss = 4.64594 avg_loss = 3.42244\n",
      "epoch no.4 train no.363520  loss = 5.14570 avg_loss = 3.43162\n",
      "epoch no.4 train no.363530  loss = 2.80751 avg_loss = 3.40072\n",
      "epoch no.4 train no.363540  loss = 3.60029 avg_loss = 3.39047\n",
      "epoch no.4 train no.363550  loss = 2.99666 avg_loss = 3.34490\n",
      "epoch no.4 train no.363560  loss = 3.26471 avg_loss = 3.37572\n",
      "epoch no.4 train no.363570  loss = 3.01506 avg_loss = 3.31806\n",
      "epoch no.4 train no.363580  loss = 1.58516 avg_loss = 3.28951\n",
      "epoch no.4 train no.363590  loss = 2.56693 avg_loss = 3.29941\n",
      "epoch no.4 train no.363600  loss = 3.91055 avg_loss = 3.30173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.363610  loss = 3.30743 avg_loss = 3.30496\n",
      "epoch no.4 train no.363620  loss = 3.62713 avg_loss = 3.31324\n",
      "epoch no.4 train no.363630  loss = 2.72916 avg_loss = 3.31583\n",
      "epoch no.4 train no.363640  loss = 2.19447 avg_loss = 3.29125\n",
      "epoch no.4 train no.363650  loss = 1.74498 avg_loss = 3.30421\n",
      "epoch no.4 train no.363660  loss = 2.73841 avg_loss = 3.29912\n",
      "epoch no.4 train no.363670  loss = 2.93983 avg_loss = 3.25445\n",
      "epoch no.4 train no.363680  loss = 6.55935 avg_loss = 3.28426\n",
      "epoch no.4 train no.363690  loss = 5.82731 avg_loss = 3.27938\n",
      "epoch no.4 train no.363700  loss = 3.17583 avg_loss = 3.27819\n",
      "epoch no.4 train no.363710  loss = 2.45878 avg_loss = 3.31758\n",
      "epoch no.4 train no.363720  loss = 3.03418 avg_loss = 3.30595\n",
      "epoch no.4 train no.363730  loss = 2.35444 avg_loss = 3.23736\n",
      "epoch no.4 train no.363740  loss = 2.38397 avg_loss = 3.27373\n",
      "epoch no.4 train no.363750  loss = 3.94482 avg_loss = 3.30587\n",
      "epoch no.4 train no.363760  loss = 3.49354 avg_loss = 3.31026\n",
      "epoch no.4 train no.363770  loss = 3.68825 avg_loss = 3.32407\n",
      "epoch no.4 train no.363780  loss = 4.46994 avg_loss = 3.34018\n",
      "epoch no.4 train no.363790  loss = 5.17294 avg_loss = 3.37037\n",
      "epoch no.4 train no.363800  loss = 3.32561 avg_loss = 3.41637\n",
      "epoch no.4 train no.363810  loss = 3.21131 avg_loss = 3.38723\n",
      "epoch no.4 train no.363820  loss = 4.27049 avg_loss = 3.34112\n",
      "epoch no.4 train no.363830  loss = 2.83774 avg_loss = 3.34199\n",
      "epoch no.4 train no.363840  loss = 2.73686 avg_loss = 3.31478\n",
      "epoch no.4 train no.363850  loss = 2.88266 avg_loss = 3.27246\n",
      "epoch no.4 train no.363860  loss = 2.95941 avg_loss = 3.25384\n",
      "epoch no.4 train no.363870  loss = 2.04441 avg_loss = 3.25673\n",
      "epoch no.4 train no.363880  loss = 2.42375 avg_loss = 3.28601\n",
      "epoch no.4 train no.363890  loss = 3.77301 avg_loss = 3.28585\n",
      "epoch no.4 train no.363900  loss = 3.22846 avg_loss = 3.26212\n",
      "epoch no.4 train no.363910  loss = 3.84126 avg_loss = 3.20791\n",
      "epoch no.4 train no.363920  loss = 2.96246 avg_loss = 3.19198\n",
      "epoch no.4 train no.363930  loss = 2.22319 avg_loss = 3.18048\n",
      "epoch no.4 train no.363940  loss = 4.97414 avg_loss = 3.17986\n",
      "epoch no.4 train no.363950  loss = 2.42878 avg_loss = 3.13564\n",
      "epoch no.4 train no.363960  loss = 2.65957 avg_loss = 3.15139\n",
      "epoch no.4 train no.363970  loss = 2.58341 avg_loss = 3.14334\n",
      "epoch no.4 train no.363980  loss = 2.98920 avg_loss = 3.15255\n",
      "epoch no.4 train no.363990  loss = 2.53212 avg_loss = 3.14638\n",
      "epoch no.4 train no.364000  loss = 4.32940 avg_loss = 3.14824\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁위한', '▁팝', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.4 train no.364010  loss = 3.35532 avg_loss = 3.14628\n",
      "epoch no.4 train no.364020  loss = 4.33921 avg_loss = 3.16734\n",
      "epoch no.4 train no.364030  loss = 5.06622 avg_loss = 3.19126\n",
      "epoch no.4 train no.364040  loss = 1.90281 avg_loss = 3.18183\n",
      "epoch no.4 train no.364050  loss = 2.04673 avg_loss = 3.19208\n",
      "epoch no.4 train no.364060  loss = 6.75053 avg_loss = 3.26922\n",
      "epoch no.4 train no.364070  loss = 2.55309 avg_loss = 3.25996\n",
      "epoch no.4 train no.364080  loss = 4.69112 avg_loss = 3.28883\n",
      "epoch no.4 train no.364090  loss = 4.46393 avg_loss = 3.29466\n",
      "epoch no.4 train no.364100  loss = 4.28630 avg_loss = 3.34356\n",
      "epoch no.4 train no.364110  loss = 2.71922 avg_loss = 3.31907\n",
      "epoch no.4 train no.364120  loss = 5.72539 avg_loss = 3.30318\n",
      "epoch no.4 train no.364130  loss = 4.96973 avg_loss = 3.36991\n",
      "epoch no.4 train no.364140  loss = 3.49340 avg_loss = 3.37845\n",
      "epoch no.4 train no.364150  loss = 2.65730 avg_loss = 3.35218\n",
      "epoch no.4 train no.364160  loss = 3.40990 avg_loss = 3.38356\n",
      "epoch no.4 train no.364170  loss = 3.01308 avg_loss = 3.37809\n",
      "epoch no.4 train no.364180  loss = 2.41169 avg_loss = 3.38007\n",
      "epoch no.4 train no.364190  loss = 3.65738 avg_loss = 3.36511\n",
      "epoch no.4 train no.364200  loss = 1.93657 avg_loss = 3.32556\n",
      "epoch no.4 train no.364210  loss = 1.99348 avg_loss = 3.31241\n",
      "epoch no.4 train no.364220  loss = 4.85399 avg_loss = 3.32230\n",
      "epoch no.4 train no.364230  loss = 3.29559 avg_loss = 3.35920\n",
      "epoch no.4 train no.364240  loss = 2.37396 avg_loss = 3.34947\n",
      "epoch no.4 train no.364250  loss = 3.42845 avg_loss = 3.36647\n",
      "epoch no.4 train no.364260  loss = 3.55277 avg_loss = 3.37011\n",
      "epoch no.4 train no.364270  loss = 2.67574 avg_loss = 3.35398\n",
      "epoch no.4 train no.364280  loss = 2.41185 avg_loss = 3.34535\n",
      "epoch no.4 train no.364290  loss = 2.01118 avg_loss = 3.29771\n",
      "epoch no.4 train no.364300  loss = 3.32242 avg_loss = 3.31598\n",
      "epoch no.4 train no.364310  loss = 2.88026 avg_loss = 3.32199\n",
      "epoch no.4 train no.364320  loss = 4.67938 avg_loss = 3.38811\n",
      "epoch no.4 train no.364330  loss = 4.70565 avg_loss = 3.37171\n",
      "epoch no.4 train no.364340  loss = 3.82562 avg_loss = 3.38654\n",
      "epoch no.4 train no.364350  loss = 2.63129 avg_loss = 3.33587\n",
      "epoch no.4 train no.364360  loss = 1.86543 avg_loss = 3.28653\n",
      "epoch no.4 train no.364370  loss = 3.54389 avg_loss = 3.29092\n",
      "epoch no.4 train no.364380  loss = 5.23608 avg_loss = 3.30614\n",
      "epoch no.4 train no.364390  loss = 4.30090 avg_loss = 3.28791\n",
      "epoch no.4 train no.364400  loss = 3.16652 avg_loss = 3.26168\n",
      "epoch no.4 train no.364410  loss = 4.19200 avg_loss = 3.23624\n",
      "epoch no.4 train no.364420  loss = 2.77264 avg_loss = 3.18543\n",
      "epoch no.4 train no.364430  loss = 3.98542 avg_loss = 3.21715\n",
      "epoch no.4 train no.364440  loss = 2.57417 avg_loss = 3.23223\n",
      "epoch no.4 train no.364450  loss = 3.41461 avg_loss = 3.21943\n",
      "epoch no.4 train no.364460  loss = 2.00743 avg_loss = 3.22309\n",
      "epoch no.4 train no.364470  loss = 3.75735 avg_loss = 3.21773\n",
      "epoch no.4 train no.364480  loss = 5.18767 avg_loss = 3.27096\n",
      "epoch no.4 train no.364490  loss = 5.21380 avg_loss = 3.27614\n",
      "epoch no.4 train no.364500  loss = 2.81276 avg_loss = 3.25370\n",
      "epoch no.4 train no.364510  loss = 3.01777 avg_loss = 3.24879\n",
      "epoch no.4 train no.364520  loss = 2.54149 avg_loss = 3.22801\n",
      "epoch no.4 train no.364530  loss = 3.81773 avg_loss = 3.23204\n",
      "epoch no.4 train no.364540  loss = 3.57311 avg_loss = 3.22657\n",
      "epoch no.4 train no.364550  loss = 1.94621 avg_loss = 3.21923\n",
      "epoch no.4 train no.364560  loss = 2.11506 avg_loss = 3.22573\n",
      "epoch no.4 train no.364570  loss = 2.23682 avg_loss = 3.21114\n",
      "epoch no.4 train no.364580  loss = 3.56500 avg_loss = 3.22608\n",
      "epoch no.4 train no.364590  loss = 3.97077 avg_loss = 3.21679\n",
      "epoch no.4 train no.364600  loss = 1.72858 avg_loss = 3.20286\n",
      "epoch no.4 train no.364610  loss = 2.13439 avg_loss = 3.19120\n",
      "epoch no.4 train no.364620  loss = 3.89482 avg_loss = 3.23035\n",
      "epoch no.4 train no.364630  loss = 2.23642 avg_loss = 3.23658\n",
      "epoch no.4 train no.364640  loss = 3.81384 avg_loss = 3.25072\n",
      "epoch no.4 train no.364650  loss = 4.39108 avg_loss = 3.23807\n",
      "epoch no.4 train no.364660  loss = 3.70760 avg_loss = 3.20335\n",
      "epoch no.4 train no.364670  loss = 4.90187 avg_loss = 3.21329\n",
      "epoch no.4 train no.364680  loss = 3.36257 avg_loss = 3.25628\n",
      "epoch no.4 train no.364690  loss = 3.85452 avg_loss = 3.25553\n",
      "epoch no.4 train no.364700  loss = 1.97213 avg_loss = 3.24387\n",
      "epoch no.4 train no.364710  loss = 3.91025 avg_loss = 3.24874\n",
      "epoch no.4 train no.364720  loss = 2.11407 avg_loss = 3.25853\n",
      "epoch no.4 train no.364730  loss = 1.97434 avg_loss = 3.28189\n",
      "epoch no.4 train no.364740  loss = 3.35668 avg_loss = 3.28524\n",
      "epoch no.4 train no.364750  loss = 3.48557 avg_loss = 3.31764\n",
      "epoch no.4 train no.364760  loss = 5.08740 avg_loss = 3.32476\n",
      "epoch no.4 train no.364770  loss = 3.20220 avg_loss = 3.32141\n",
      "epoch no.4 train no.364780  loss = 2.18056 avg_loss = 3.33644\n",
      "epoch no.4 train no.364790  loss = 5.67088 avg_loss = 3.36938\n",
      "epoch no.4 train no.364800  loss = 2.78935 avg_loss = 3.32354\n",
      "epoch no.4 train no.364810  loss = 6.46737 avg_loss = 3.34668\n",
      "epoch no.4 train no.364820  loss = 2.82050 avg_loss = 3.39209\n",
      "epoch no.4 train no.364830  loss = 3.73036 avg_loss = 3.40544\n",
      "epoch no.4 train no.364840  loss = 5.51076 avg_loss = 3.41131\n",
      "epoch no.4 train no.364850  loss = 2.02150 avg_loss = 3.41904\n",
      "epoch no.4 train no.364860  loss = 2.98479 avg_loss = 3.40557\n",
      "epoch no.4 train no.364870  loss = 3.20535 avg_loss = 3.37631\n",
      "epoch no.4 train no.364880  loss = 3.90122 avg_loss = 3.35641\n",
      "epoch no.4 train no.364890  loss = 3.73845 avg_loss = 3.36213\n",
      "epoch no.4 train no.364900  loss = 3.36501 avg_loss = 3.33680\n",
      "epoch no.4 train no.364910  loss = 2.45169 avg_loss = 3.34789\n",
      "epoch no.4 train no.364920  loss = 4.58187 avg_loss = 3.34550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.364930  loss = 3.02750 avg_loss = 3.32521\n",
      "epoch no.4 train no.364940  loss = 3.56366 avg_loss = 3.34762\n",
      "epoch no.4 train no.364950  loss = 3.84070 avg_loss = 3.34328\n",
      "epoch no.4 train no.364960  loss = 2.87281 avg_loss = 3.36505\n",
      "epoch no.4 train no.364970  loss = 2.99683 avg_loss = 3.37138\n",
      "epoch no.4 train no.364980  loss = 1.29343 avg_loss = 3.34261\n",
      "epoch no.4 train no.364990  loss = 1.56696 avg_loss = 3.32752\n",
      "epoch no.4 train no.365000  loss = 3.27948 avg_loss = 3.31734\n",
      "6\n",
      "to_tokens: ['▁비', '좋은', '을', '▁필요할', '때', '</s>', '▁음악', '에이지', '</s>']\n",
      "기분전환이 필요할때 듣는 뉴에이지</s>\n",
      "epoch no.4 train no.365010  loss = 2.14316 avg_loss = 3.28289\n",
      "epoch no.4 train no.365020  loss = 2.93991 avg_loss = 3.27848\n",
      "epoch no.4 train no.365030  loss = 3.11095 avg_loss = 3.26895\n",
      "epoch no.4 train no.365040  loss = 3.09430 avg_loss = 3.32332\n",
      "epoch no.4 train no.365050  loss = 4.06886 avg_loss = 3.31154\n",
      "epoch no.4 train no.365060  loss = 4.10071 avg_loss = 3.28326\n",
      "epoch no.4 train no.365070  loss = 4.12546 avg_loss = 3.31769\n",
      "epoch no.4 train no.365080  loss = 4.01534 avg_loss = 3.27943\n",
      "epoch no.4 train no.365090  loss = 2.54346 avg_loss = 3.28117\n",
      "epoch no.4 train no.365100  loss = 3.15444 avg_loss = 3.26259\n",
      "epoch no.4 train no.365110  loss = 2.10049 avg_loss = 3.28060\n",
      "epoch no.4 train no.365120  loss = 4.31633 avg_loss = 3.31185\n",
      "epoch no.4 train no.365130  loss = 3.14793 avg_loss = 3.34922\n",
      "epoch no.4 train no.365140  loss = 2.27945 avg_loss = 3.31909\n",
      "epoch no.4 train no.365150  loss = 3.75661 avg_loss = 3.32084\n",
      "epoch no.4 train no.365160  loss = 4.38113 avg_loss = 3.29957\n",
      "epoch no.4 train no.365170  loss = 3.09118 avg_loss = 3.26729\n",
      "epoch no.4 train no.365180  loss = 2.69107 avg_loss = 3.22198\n",
      "epoch no.4 train no.365190  loss = 3.17361 avg_loss = 3.24358\n",
      "epoch no.4 train no.365200  loss = 3.40374 avg_loss = 3.25719\n",
      "epoch no.4 train no.365210  loss = 5.17105 avg_loss = 3.26067\n",
      "epoch no.4 train no.365220  loss = 2.09195 avg_loss = 3.24892\n",
      "epoch no.4 train no.365230  loss = 2.86099 avg_loss = 3.30381\n",
      "epoch no.4 train no.365240  loss = 4.36754 avg_loss = 3.32010\n",
      "epoch no.4 train no.365250  loss = 3.67176 avg_loss = 3.31647\n",
      "epoch no.4 train no.365260  loss = 3.18591 avg_loss = 3.27805\n",
      "epoch no.4 train no.365270  loss = 2.00484 avg_loss = 3.26861\n",
      "epoch no.4 train no.365280  loss = 0.91772 avg_loss = 3.20265\n",
      "epoch no.4 train no.365290  loss = 4.22698 avg_loss = 3.23748\n",
      "epoch no.4 train no.365300  loss = 2.49196 avg_loss = 3.22852\n",
      "epoch no.4 train no.365310  loss = 2.28003 avg_loss = 3.22886\n",
      "epoch no.4 train no.365320  loss = 3.15336 avg_loss = 3.22892\n",
      "epoch no.4 train no.365330  loss = 3.02997 avg_loss = 3.26131\n",
      "epoch no.4 train no.365340  loss = 2.64496 avg_loss = 3.27015\n",
      "epoch no.4 train no.365350  loss = 3.59624 avg_loss = 3.27923\n",
      "epoch no.4 train no.365360  loss = 3.29736 avg_loss = 3.23379\n",
      "epoch no.4 train no.365370  loss = 3.19804 avg_loss = 3.24913\n",
      "epoch no.4 train no.365380  loss = 2.60534 avg_loss = 3.25718\n",
      "epoch no.4 train no.365390  loss = 2.50923 avg_loss = 3.21819\n",
      "epoch no.4 train no.365400  loss = 2.44824 avg_loss = 3.19836\n",
      "epoch no.4 train no.365410  loss = 3.67852 avg_loss = 3.24813\n",
      "epoch no.4 train no.365420  loss = 2.28723 avg_loss = 3.27785\n",
      "epoch no.4 train no.365430  loss = 2.18008 avg_loss = 3.30383\n",
      "epoch no.4 train no.365440  loss = 5.65799 avg_loss = 3.33991\n",
      "epoch no.4 train no.365450  loss = 2.48683 avg_loss = 3.35501\n",
      "epoch no.4 train no.365460  loss = 3.86484 avg_loss = 3.33815\n",
      "epoch no.4 train no.365470  loss = 4.38848 avg_loss = 3.34202\n",
      "epoch no.4 train no.365480  loss = 1.11620 avg_loss = 3.31665\n",
      "epoch no.4 train no.365490  loss = 2.92716 avg_loss = 3.33296\n",
      "epoch no.4 train no.365500  loss = 4.31857 avg_loss = 3.33050\n",
      "epoch no.4 train no.365510  loss = 2.43010 avg_loss = 3.32682\n",
      "epoch no.4 train no.365520  loss = 2.49446 avg_loss = 3.31453\n",
      "epoch no.4 train no.365530  loss = 2.38129 avg_loss = 3.31588\n",
      "epoch no.4 train no.365540  loss = 2.86047 avg_loss = 3.30009\n",
      "epoch no.4 train no.365550  loss = 4.27899 avg_loss = 3.27707\n",
      "epoch no.4 train no.365560  loss = 3.16692 avg_loss = 3.28135\n",
      "epoch no.4 train no.365570  loss = 2.16516 avg_loss = 3.29392\n",
      "epoch no.4 train no.365580  loss = 3.62856 avg_loss = 3.26510\n",
      "epoch no.4 train no.365590  loss = 3.08412 avg_loss = 3.34592\n",
      "epoch no.4 train no.365600  loss = 2.43980 avg_loss = 3.30807\n",
      "epoch no.4 train no.365610  loss = 3.27717 avg_loss = 3.28350\n",
      "epoch no.4 train no.365620  loss = 2.44689 avg_loss = 3.24934\n",
      "epoch no.4 train no.365630  loss = 4.19116 avg_loss = 3.32627\n",
      "epoch no.4 train no.365640  loss = 2.51450 avg_loss = 3.31188\n",
      "epoch no.4 train no.365650  loss = 4.82146 avg_loss = 3.36616\n",
      "epoch no.4 train no.365660  loss = 3.33733 avg_loss = 3.35553\n",
      "epoch no.4 train no.365670  loss = 3.18566 avg_loss = 3.35072\n",
      "epoch no.4 train no.365680  loss = 5.01484 avg_loss = 3.31671\n",
      "epoch no.4 train no.365690  loss = 2.42812 avg_loss = 3.30575\n",
      "epoch no.4 train no.365700  loss = 3.84785 avg_loss = 3.35216\n",
      "epoch no.4 train no.365710  loss = 2.72719 avg_loss = 3.32173\n",
      "epoch no.4 train no.365720  loss = 4.02567 avg_loss = 3.29662\n",
      "epoch no.4 train no.365730  loss = 2.61444 avg_loss = 3.30458\n",
      "epoch no.4 train no.365740  loss = 3.14234 avg_loss = 3.34018\n",
      "epoch no.4 train no.365750  loss = 2.72681 avg_loss = 3.31621\n",
      "epoch no.4 train no.365760  loss = 4.46879 avg_loss = 3.31898\n",
      "epoch no.4 train no.365770  loss = 2.81001 avg_loss = 3.29711\n",
      "epoch no.4 train no.365780  loss = 2.91564 avg_loss = 3.30417\n",
      "epoch no.4 train no.365790  loss = 2.63251 avg_loss = 3.25445\n",
      "epoch no.4 train no.365800  loss = 1.99802 avg_loss = 3.21911\n",
      "epoch no.4 train no.365810  loss = 4.40064 avg_loss = 3.23218\n",
      "epoch no.4 train no.365820  loss = 2.77088 avg_loss = 3.24450\n",
      "epoch no.4 train no.365830  loss = 2.49631 avg_loss = 3.20008\n",
      "epoch no.4 train no.365840  loss = 3.10831 avg_loss = 3.17858\n",
      "epoch no.4 train no.365850  loss = 3.48415 avg_loss = 3.18713\n",
      "epoch no.4 train no.365860  loss = 3.22499 avg_loss = 3.20165\n",
      "epoch no.4 train no.365870  loss = 3.43553 avg_loss = 3.20889\n",
      "epoch no.4 train no.365880  loss = 4.50878 avg_loss = 3.20612\n",
      "epoch no.4 train no.365890  loss = 2.50431 avg_loss = 3.19924\n",
      "epoch no.4 train no.365900  loss = 2.43969 avg_loss = 3.20532\n",
      "epoch no.4 train no.365910  loss = 2.24685 avg_loss = 3.22679\n",
      "epoch no.4 train no.365920  loss = 4.05313 avg_loss = 3.29569\n",
      "epoch no.4 train no.365930  loss = 4.14508 avg_loss = 3.33204\n",
      "epoch no.4 train no.365940  loss = 2.80816 avg_loss = 3.31152\n",
      "epoch no.4 train no.365950  loss = 2.48019 avg_loss = 3.32516\n",
      "epoch no.4 train no.365960  loss = 3.85869 avg_loss = 3.33344\n",
      "epoch no.4 train no.365970  loss = 4.57187 avg_loss = 3.35515\n",
      "epoch no.4 train no.365980  loss = 2.82961 avg_loss = 3.35609\n",
      "epoch no.4 train no.365990  loss = 3.96307 avg_loss = 3.34020\n",
      "epoch no.4 train no.366000  loss = 3.14244 avg_loss = 3.34909\n",
      "4\n",
      "to_tokens: ['▁비', '좋은', '이', '▁위한', '▁신나는', '▁노래', '</s>']\n",
      "기분전환을 위한 신나는 노래</s>\n",
      "epoch no.4 train no.366010  loss = 2.97860 avg_loss = 3.32266\n",
      "epoch no.4 train no.366020  loss = 2.95498 avg_loss = 3.30818\n",
      "epoch no.4 train no.366030  loss = 2.70296 avg_loss = 3.31690\n",
      "epoch no.4 train no.366040  loss = 2.45891 avg_loss = 3.29355\n",
      "epoch no.4 train no.366050  loss = 4.23994 avg_loss = 3.29320\n",
      "epoch no.4 train no.366060  loss = 3.47362 avg_loss = 3.28925\n",
      "epoch no.4 train no.366070  loss = 3.18390 avg_loss = 3.30478\n",
      "epoch no.4 train no.366080  loss = 3.02301 avg_loss = 3.30400\n",
      "epoch no.4 train no.366090  loss = 4.26860 avg_loss = 3.30281\n",
      "epoch no.4 train no.366100  loss = 2.30018 avg_loss = 3.32336\n",
      "epoch no.4 train no.366110  loss = 3.76940 avg_loss = 3.31765\n",
      "epoch no.4 train no.366120  loss = 2.70558 avg_loss = 3.39022\n",
      "epoch no.4 train no.366130  loss = 2.67089 avg_loss = 3.38043\n",
      "epoch no.4 train no.366140  loss = 4.41036 avg_loss = 3.39103\n",
      "epoch no.4 train no.366150  loss = 3.04884 avg_loss = 3.39960\n",
      "epoch no.4 train no.366160  loss = 3.34134 avg_loss = 3.36758\n",
      "epoch no.4 train no.366170  loss = 4.59937 avg_loss = 3.37540\n",
      "epoch no.4 train no.366180  loss = 3.59508 avg_loss = 3.35784\n",
      "epoch no.4 train no.366190  loss = 3.95459 avg_loss = 3.33595\n",
      "epoch no.4 train no.366200  loss = 4.29556 avg_loss = 3.37042\n",
      "epoch no.4 train no.366210  loss = 4.15251 avg_loss = 3.35512\n",
      "epoch no.4 train no.366220  loss = 3.77611 avg_loss = 3.37004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.366230  loss = 4.05949 avg_loss = 3.41971\n",
      "epoch no.4 train no.366240  loss = 3.87223 avg_loss = 3.43177\n",
      "epoch no.4 train no.366250  loss = 3.38226 avg_loss = 3.49619\n",
      "epoch no.4 train no.366260  loss = 4.31060 avg_loss = 3.49725\n",
      "epoch no.4 train no.366270  loss = 2.47599 avg_loss = 3.51581\n",
      "epoch no.4 train no.366280  loss = 4.96050 avg_loss = 3.49927\n",
      "epoch no.4 train no.366290  loss = 2.55886 avg_loss = 3.44127\n",
      "epoch no.4 train no.366300  loss = 2.84110 avg_loss = 3.42875\n",
      "epoch no.4 train no.366310  loss = 1.81575 avg_loss = 3.40109\n",
      "epoch no.4 train no.366320  loss = 0.88507 avg_loss = 3.36991\n",
      "epoch no.4 train no.366330  loss = 3.14732 avg_loss = 3.37668\n",
      "epoch no.4 train no.366340  loss = 3.13557 avg_loss = 3.38391\n",
      "epoch no.4 train no.366350  loss = 4.65325 avg_loss = 3.39544\n",
      "epoch no.4 train no.366360  loss = 4.40975 avg_loss = 3.39095\n",
      "epoch no.4 train no.366370  loss = 4.04256 avg_loss = 3.39299\n",
      "epoch no.4 train no.366380  loss = 3.46181 avg_loss = 3.38818\n",
      "epoch no.4 train no.366390  loss = 5.72206 avg_loss = 3.38453\n",
      "epoch no.4 train no.366400  loss = 3.79115 avg_loss = 3.37807\n",
      "epoch no.4 train no.366410  loss = 3.83132 avg_loss = 3.40726\n",
      "epoch no.4 train no.366420  loss = 2.13345 avg_loss = 3.41399\n",
      "epoch no.4 train no.366430  loss = 2.38501 avg_loss = 3.39273\n",
      "epoch no.4 train no.366440  loss = 6.20191 avg_loss = 3.39751\n",
      "epoch no.4 train no.366450  loss = 2.21739 avg_loss = 3.39906\n",
      "epoch no.4 train no.366460  loss = 2.58444 avg_loss = 3.36735\n",
      "epoch no.4 train no.366470  loss = 3.05053 avg_loss = 3.35555\n",
      "epoch no.4 train no.366480  loss = 4.32873 avg_loss = 3.35951\n",
      "epoch no.4 train no.366490  loss = 1.85335 avg_loss = 3.31800\n",
      "epoch no.4 train no.366500  loss = 3.86316 avg_loss = 3.33664\n",
      "epoch no.4 train no.366510  loss = 2.61978 avg_loss = 3.28784\n",
      "epoch no.4 train no.366520  loss = 4.06308 avg_loss = 3.36787\n",
      "epoch no.4 train no.366530  loss = 2.81253 avg_loss = 3.35788\n",
      "epoch no.4 train no.366540  loss = 4.55909 avg_loss = 3.35793\n",
      "epoch no.4 train no.366550  loss = 1.84133 avg_loss = 3.33625\n",
      "epoch no.4 train no.366560  loss = 3.09884 avg_loss = 3.30485\n",
      "epoch no.4 train no.366570  loss = 4.75683 avg_loss = 3.32642\n",
      "epoch no.4 train no.366580  loss = 4.08435 avg_loss = 3.30824\n",
      "epoch no.4 train no.366590  loss = 3.45234 avg_loss = 3.32815\n",
      "epoch no.4 train no.366600  loss = 3.44197 avg_loss = 3.33490\n",
      "epoch no.4 train no.366610  loss = 2.83531 avg_loss = 3.29983\n",
      "epoch no.4 train no.366620  loss = 2.56739 avg_loss = 3.30700\n",
      "epoch no.4 train no.366630  loss = 2.63662 avg_loss = 3.34182\n",
      "epoch no.4 train no.366640  loss = 3.34055 avg_loss = 3.38940\n",
      "epoch no.4 train no.366650  loss = 3.95671 avg_loss = 3.39983\n",
      "epoch no.4 train no.366660  loss = 3.52209 avg_loss = 3.39127\n",
      "epoch no.4 train no.366670  loss = 3.39412 avg_loss = 3.39792\n",
      "epoch no.4 train no.366680  loss = 3.90517 avg_loss = 3.36978\n",
      "epoch no.4 train no.366690  loss = 4.08094 avg_loss = 3.36676\n",
      "epoch no.4 train no.366700  loss = 3.22701 avg_loss = 3.34533\n",
      "epoch no.4 train no.366710  loss = 2.43080 avg_loss = 3.33717\n",
      "epoch no.4 train no.366720  loss = 3.53652 avg_loss = 3.36923\n",
      "epoch no.4 train no.366730  loss = 3.63623 avg_loss = 3.40813\n",
      "epoch no.4 train no.366740  loss = 3.40385 avg_loss = 3.41602\n",
      "epoch no.4 train no.366750  loss = 2.69999 avg_loss = 3.45234\n",
      "epoch no.4 train no.366760  loss = 3.51701 avg_loss = 3.45974\n",
      "epoch no.4 train no.366770  loss = 2.11525 avg_loss = 3.47781\n",
      "epoch no.4 train no.366780  loss = 4.54561 avg_loss = 3.42542\n",
      "epoch no.4 train no.366790  loss = 2.94383 avg_loss = 3.42928\n",
      "epoch no.4 train no.366800  loss = 2.92605 avg_loss = 3.43144\n",
      "epoch no.4 train no.366810  loss = 2.52576 avg_loss = 3.37919\n",
      "epoch no.4 train no.366820  loss = 3.72108 avg_loss = 3.36975\n",
      "epoch no.4 train no.366830  loss = 3.17730 avg_loss = 3.34134\n",
      "epoch no.4 train no.366840  loss = 4.16717 avg_loss = 3.36479\n",
      "epoch no.4 train no.366850  loss = 2.99021 avg_loss = 3.39555\n",
      "epoch no.4 train no.366860  loss = 5.96540 avg_loss = 3.36826\n",
      "epoch no.4 train no.366870  loss = 3.35531 avg_loss = 3.33051\n",
      "epoch no.4 train no.366880  loss = 3.99510 avg_loss = 3.32581\n",
      "epoch no.4 train no.366890  loss = 2.45349 avg_loss = 3.28117\n",
      "epoch no.4 train no.366900  loss = 4.10603 avg_loss = 3.31111\n",
      "epoch no.4 train no.366910  loss = 2.55676 avg_loss = 3.29361\n",
      "epoch no.4 train no.366920  loss = 2.64309 avg_loss = 3.31908\n",
      "epoch no.4 train no.366930  loss = 3.98957 avg_loss = 3.34505\n",
      "epoch no.4 train no.366940  loss = 2.81415 avg_loss = 3.33604\n",
      "epoch no.4 train no.366950  loss = 2.11425 avg_loss = 3.30400\n",
      "epoch no.4 train no.366960  loss = 4.17307 avg_loss = 3.31643\n",
      "epoch no.4 train no.366970  loss = 2.78744 avg_loss = 3.33279\n",
      "epoch no.4 train no.366980  loss = 3.15104 avg_loss = 3.27878\n",
      "epoch no.4 train no.366990  loss = 4.27665 avg_loss = 3.23997\n",
      "epoch no.4 train no.367000  loss = 3.03521 avg_loss = 3.19605\n",
      "5\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '때', '▁듣는', '▁노래', '송']\n",
      "기분전환이 필요할 때 듣는 팝</s>\n",
      "epoch no.4 train no.367010  loss = 2.21194 avg_loss = 3.18829\n",
      "epoch no.4 train no.367020  loss = 3.19505 avg_loss = 3.21856\n",
      "epoch no.4 train no.367030  loss = 4.83106 avg_loss = 3.25585\n",
      "epoch no.4 train no.367040  loss = 2.22706 avg_loss = 3.23931\n",
      "epoch no.4 train no.367050  loss = 6.51243 avg_loss = 3.27203\n",
      "epoch no.4 train no.367060  loss = 4.33858 avg_loss = 3.27115\n",
      "epoch no.4 train no.367070  loss = 2.06525 avg_loss = 3.24544\n",
      "epoch no.4 train no.367080  loss = 3.44785 avg_loss = 3.25861\n",
      "epoch no.4 train no.367090  loss = 2.92177 avg_loss = 3.27473\n",
      "epoch no.4 train no.367100  loss = 2.71631 avg_loss = 3.31092\n",
      "epoch no.4 train no.367110  loss = 2.59279 avg_loss = 3.34828\n",
      "epoch no.4 train no.367120  loss = 2.59181 avg_loss = 3.31832\n",
      "epoch no.4 train no.367130  loss = 3.48901 avg_loss = 3.31936\n",
      "epoch no.4 train no.367140  loss = 6.36836 avg_loss = 3.37343\n",
      "epoch no.4 train no.367150  loss = 5.26409 avg_loss = 3.43926\n",
      "epoch no.4 train no.367160  loss = 3.75257 avg_loss = 3.48482\n",
      "epoch no.4 train no.367170  loss = 3.31154 avg_loss = 3.48547\n",
      "epoch no.4 train no.367180  loss = 4.36047 avg_loss = 3.53531\n",
      "epoch no.4 train no.367190  loss = 2.44128 avg_loss = 3.53945\n",
      "epoch no.4 train no.367200  loss = 3.55365 avg_loss = 3.56596\n",
      "epoch no.4 train no.367210  loss = 3.84256 avg_loss = 3.57673\n",
      "epoch no.4 train no.367220  loss = 2.85369 avg_loss = 3.52993\n",
      "epoch no.4 train no.367230  loss = 3.18050 avg_loss = 3.51487\n",
      "epoch no.4 train no.367240  loss = 2.54288 avg_loss = 3.48467\n",
      "epoch no.4 train no.367250  loss = 4.18704 avg_loss = 3.47666\n",
      "epoch no.4 train no.367260  loss = 3.55570 avg_loss = 3.42306\n",
      "epoch no.4 train no.367270  loss = 1.76232 avg_loss = 3.42859\n",
      "epoch no.4 train no.367280  loss = 3.84354 avg_loss = 3.44708\n",
      "epoch no.4 train no.367290  loss = 2.46765 avg_loss = 3.45523\n",
      "epoch no.4 train no.367300  loss = 4.11919 avg_loss = 3.42669\n",
      "epoch no.4 train no.367310  loss = 3.49390 avg_loss = 3.39991\n",
      "epoch no.4 train no.367320  loss = 3.38440 avg_loss = 3.40405\n",
      "epoch no.4 train no.367330  loss = 2.87723 avg_loss = 3.40498\n",
      "epoch no.4 train no.367340  loss = 2.93234 avg_loss = 3.37790\n",
      "epoch no.4 train no.367350  loss = 2.67397 avg_loss = 3.33222\n",
      "epoch no.4 train no.367360  loss = 3.04741 avg_loss = 3.34643\n",
      "epoch no.4 train no.367370  loss = 1.84899 avg_loss = 3.37230\n",
      "epoch no.4 train no.367380  loss = 4.67323 avg_loss = 3.43549\n",
      "epoch no.4 train no.367390  loss = 3.37101 avg_loss = 3.39576\n",
      "epoch no.4 train no.367400  loss = 4.89518 avg_loss = 3.39089\n",
      "epoch no.4 train no.367410  loss = 3.10123 avg_loss = 3.34156\n",
      "epoch no.4 train no.367420  loss = 3.94810 avg_loss = 3.36777\n",
      "epoch no.4 train no.367430  loss = 3.20029 avg_loss = 3.36780\n",
      "epoch no.4 train no.367440  loss = 3.06620 avg_loss = 3.31822\n",
      "epoch no.4 train no.367450  loss = 3.09197 avg_loss = 3.30149\n",
      "epoch no.4 train no.367460  loss = 3.14253 avg_loss = 3.26750\n",
      "epoch no.4 train no.367470  loss = 4.10626 avg_loss = 3.26284\n",
      "epoch no.4 train no.367480  loss = 5.10050 avg_loss = 3.30075\n",
      "epoch no.4 train no.367490  loss = 3.50510 avg_loss = 3.27765\n",
      "epoch no.4 train no.367500  loss = 3.76589 avg_loss = 3.34553\n",
      "epoch no.4 train no.367510  loss = 3.79469 avg_loss = 3.32990\n",
      "epoch no.4 train no.367520  loss = 2.17394 avg_loss = 3.29161\n",
      "epoch no.4 train no.367530  loss = 3.79558 avg_loss = 3.31889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.367540  loss = 3.57701 avg_loss = 3.37023\n",
      "epoch no.4 train no.367550  loss = 4.57020 avg_loss = 3.36980\n",
      "epoch no.4 train no.367560  loss = 4.82944 avg_loss = 3.37042\n",
      "epoch no.4 train no.367570  loss = 4.09064 avg_loss = 3.34616\n",
      "epoch no.4 train no.367580  loss = 4.05698 avg_loss = 3.32927\n",
      "epoch no.4 train no.367590  loss = 3.38306 avg_loss = 3.35812\n",
      "epoch no.4 train no.367600  loss = 3.58352 avg_loss = 3.37334\n",
      "epoch no.4 train no.367610  loss = 4.68093 avg_loss = 3.40811\n",
      "epoch no.4 train no.367620  loss = 3.70912 avg_loss = 3.36732\n",
      "epoch no.4 train no.367630  loss = 2.09804 avg_loss = 3.37475\n",
      "epoch no.4 train no.367640  loss = 4.17158 avg_loss = 3.34646\n",
      "epoch no.4 train no.367650  loss = 3.88643 avg_loss = 3.32265\n",
      "epoch no.4 train no.367660  loss = 5.55081 avg_loss = 3.36736\n",
      "epoch no.4 train no.367670  loss = 1.98477 avg_loss = 3.29397\n",
      "epoch no.4 train no.367680  loss = 4.23636 avg_loss = 3.29349\n",
      "epoch no.4 train no.367690  loss = 3.18445 avg_loss = 3.27541\n",
      "epoch no.4 train no.367700  loss = 3.02942 avg_loss = 3.28793\n",
      "epoch no.4 train no.367710  loss = 5.02253 avg_loss = 3.31258\n",
      "epoch no.4 train no.367720  loss = 2.11659 avg_loss = 3.30592\n",
      "epoch no.4 train no.367730  loss = 2.43163 avg_loss = 3.25814\n",
      "epoch no.4 train no.367740  loss = 4.34924 avg_loss = 3.26829\n",
      "epoch no.4 train no.367750  loss = 3.19959 avg_loss = 3.34671\n",
      "epoch no.4 train no.367760  loss = 4.24205 avg_loss = 3.37376\n",
      "epoch no.4 train no.367770  loss = 3.14951 avg_loss = 3.37285\n",
      "epoch no.4 train no.367780  loss = 2.42644 avg_loss = 3.31941\n",
      "epoch no.4 train no.367790  loss = 2.09949 avg_loss = 3.29973\n",
      "epoch no.4 train no.367800  loss = 2.46979 avg_loss = 3.28170\n",
      "epoch no.4 train no.367810  loss = 2.58031 avg_loss = 3.26242\n",
      "epoch no.4 train no.367820  loss = 3.00856 avg_loss = 3.27808\n",
      "epoch no.4 train no.367830  loss = 4.95737 avg_loss = 3.28596\n",
      "epoch no.4 train no.367840  loss = 2.82616 avg_loss = 3.29859\n",
      "epoch no.4 train no.367850  loss = 2.62636 avg_loss = 3.26931\n",
      "epoch no.4 train no.367860  loss = 3.08588 avg_loss = 3.24794\n",
      "epoch no.4 train no.367870  loss = 3.15165 avg_loss = 3.27097\n",
      "epoch no.4 train no.367880  loss = 3.31423 avg_loss = 3.28557\n",
      "epoch no.4 train no.367890  loss = 3.92820 avg_loss = 3.29645\n",
      "epoch no.4 train no.367900  loss = 2.49540 avg_loss = 3.27994\n",
      "epoch no.4 train no.367910  loss = 2.66739 avg_loss = 3.25804\n",
      "epoch no.4 train no.367920  loss = 2.51779 avg_loss = 3.27865\n",
      "epoch no.4 train no.367930  loss = 4.20551 avg_loss = 3.30070\n",
      "epoch no.4 train no.367940  loss = 3.53599 avg_loss = 3.28509\n",
      "epoch no.4 train no.367950  loss = 3.41878 avg_loss = 3.29830\n",
      "epoch no.4 train no.367960  loss = 2.81844 avg_loss = 3.28581\n",
      "epoch no.4 train no.367970  loss = 3.77408 avg_loss = 3.31594\n",
      "epoch no.4 train no.367980  loss = 3.07981 avg_loss = 3.27262\n",
      "epoch no.4 train no.367990  loss = 2.21332 avg_loss = 3.30294\n",
      "epoch no.4 train no.368000  loss = 2.12675 avg_loss = 3.27864\n",
      "7\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁필요할', '▁때', '▁듣는', '으면', '▁좋은', '▁노래', '</s>']\n",
      "기분전환이 필요할때 들으면 좋은 노래</s>\n",
      "epoch no.4 train no.368010  loss = 2.32452 avg_loss = 3.24440\n",
      "epoch no.4 train no.368020  loss = 4.25854 avg_loss = 3.21365\n",
      "epoch no.4 train no.368030  loss = 2.97642 avg_loss = 3.25382\n",
      "epoch no.4 train no.368040  loss = 2.67156 avg_loss = 3.24400\n",
      "epoch no.4 train no.368050  loss = 3.89801 avg_loss = 3.25584\n",
      "epoch no.4 train no.368060  loss = 3.35328 avg_loss = 3.33441\n",
      "epoch no.4 train no.368070  loss = 3.24363 avg_loss = 3.31218\n",
      "epoch no.4 train no.368080  loss = 3.82562 avg_loss = 3.33141\n",
      "epoch no.4 train no.368090  loss = 2.19981 avg_loss = 3.35846\n",
      "epoch no.4 train no.368100  loss = 3.07667 avg_loss = 3.39464\n",
      "epoch no.4 train no.368110  loss = 2.16805 avg_loss = 3.40977\n",
      "epoch no.4 train no.368120  loss = 2.49522 avg_loss = 3.41509\n",
      "epoch no.4 train no.368130  loss = 3.04749 avg_loss = 3.40328\n",
      "epoch no.4 train no.368140  loss = 5.71369 avg_loss = 3.40754\n",
      "epoch no.4 train no.368150  loss = 3.80784 avg_loss = 3.43093\n",
      "epoch no.4 train no.368160  loss = 3.38096 avg_loss = 3.42656\n",
      "epoch no.4 train no.368170  loss = 1.10002 avg_loss = 3.35192\n",
      "epoch no.4 train no.368180  loss = 3.30625 avg_loss = 3.35678\n",
      "epoch no.4 train no.368190  loss = 2.93678 avg_loss = 3.35236\n",
      "epoch no.4 train no.368200  loss = 4.32083 avg_loss = 3.31327\n",
      "epoch no.4 train no.368210  loss = 3.42150 avg_loss = 3.27148\n",
      "epoch no.4 train no.368220  loss = 5.28147 avg_loss = 3.22620\n",
      "epoch no.4 train no.368230  loss = 2.70345 avg_loss = 3.20686\n",
      "epoch no.4 train no.368240  loss = 3.24778 avg_loss = 3.27789\n",
      "epoch no.4 train no.368250  loss = 2.58630 avg_loss = 3.24666\n",
      "epoch no.4 train no.368260  loss = 2.62070 avg_loss = 3.22098\n",
      "epoch no.4 train no.368270  loss = 2.75419 avg_loss = 3.18849\n",
      "epoch no.4 train no.368280  loss = 4.32669 avg_loss = 3.20021\n",
      "epoch no.4 train no.368290  loss = 3.22904 avg_loss = 3.22883\n",
      "epoch no.4 train no.368300  loss = 3.60989 avg_loss = 3.26148\n",
      "epoch no.4 train no.368310  loss = 2.99000 avg_loss = 3.31839\n",
      "epoch no.4 train no.368320  loss = 3.78551 avg_loss = 3.29924\n",
      "epoch no.4 train no.368330  loss = 3.31901 avg_loss = 3.26676\n",
      "epoch no.4 train no.368340  loss = 3.17291 avg_loss = 3.26626\n",
      "epoch no.4 train no.368350  loss = 2.03406 avg_loss = 3.27386\n",
      "epoch no.4 train no.368360  loss = 3.33321 avg_loss = 3.26620\n",
      "epoch no.4 train no.368370  loss = 3.45008 avg_loss = 3.26812\n",
      "epoch no.4 train no.368380  loss = 3.82572 avg_loss = 3.30310\n",
      "epoch no.4 train no.368390  loss = 3.00132 avg_loss = 3.29218\n",
      "epoch no.4 train no.368400  loss = 4.14232 avg_loss = 3.26915\n",
      "epoch no.4 train no.368410  loss = 2.54145 avg_loss = 3.26391\n",
      "epoch no.4 train no.368420  loss = 3.23309 avg_loss = 3.26716\n",
      "epoch no.4 train no.368430  loss = 2.49704 avg_loss = 3.24983\n",
      "epoch no.4 train no.368440  loss = 2.22901 avg_loss = 3.27302\n",
      "epoch no.4 train no.368450  loss = 3.37610 avg_loss = 3.31545\n",
      "epoch no.4 train no.368460  loss = 2.68783 avg_loss = 3.30024\n",
      "epoch no.4 train no.368470  loss = 3.18483 avg_loss = 3.27400\n",
      "epoch no.4 train no.368480  loss = 2.86400 avg_loss = 3.29357\n",
      "epoch no.4 train no.368490  loss = 4.26355 avg_loss = 3.27376\n",
      "epoch no.4 train no.368500  loss = 4.43002 avg_loss = 3.32360\n",
      "epoch no.4 train no.368510  loss = 3.35526 avg_loss = 3.35935\n",
      "epoch no.4 train no.368520  loss = 2.45246 avg_loss = 3.33349\n",
      "epoch no.4 train no.368530  loss = 4.81220 avg_loss = 3.30924\n",
      "epoch no.4 train no.368540  loss = 3.58307 avg_loss = 3.27843\n",
      "epoch no.4 train no.368550  loss = 3.39557 avg_loss = 3.27672\n",
      "epoch no.4 train no.368560  loss = 2.61932 avg_loss = 3.25076\n",
      "epoch no.4 train no.368570  loss = 4.16548 avg_loss = 3.24344\n",
      "epoch no.4 train no.368580  loss = 3.50406 avg_loss = 3.25137\n",
      "epoch no.4 train no.368590  loss = 2.24105 avg_loss = 3.25329\n",
      "epoch no.4 train no.368600  loss = 4.33085 avg_loss = 3.27910\n",
      "epoch no.4 train no.368610  loss = 2.14119 avg_loss = 3.26755\n",
      "epoch no.4 train no.368620  loss = 4.62930 avg_loss = 3.30451\n",
      "epoch no.4 train no.368630  loss = 4.29595 avg_loss = 3.31056\n",
      "epoch no.4 train no.368640  loss = 3.13144 avg_loss = 3.31889\n",
      "epoch no.4 train no.368650  loss = 1.98651 avg_loss = 3.31545\n",
      "epoch no.4 train no.368660  loss = 4.12985 avg_loss = 3.32835\n",
      "epoch no.4 train no.368670  loss = 3.40402 avg_loss = 3.31706\n",
      "epoch no.4 train no.368680  loss = 4.34353 avg_loss = 3.34873\n",
      "epoch no.4 train no.368690  loss = 4.15626 avg_loss = 3.39418\n",
      "epoch no.4 train no.368700  loss = 4.10217 avg_loss = 3.41574\n",
      "epoch no.4 train no.368710  loss = 3.50531 avg_loss = 3.42938\n",
      "epoch no.4 train no.368720  loss = 4.38112 avg_loss = 3.41927\n",
      "epoch no.4 train no.368730  loss = 4.87902 avg_loss = 3.41848\n",
      "epoch no.4 train no.368740  loss = 3.66818 avg_loss = 3.41207\n",
      "epoch no.4 train no.368750  loss = 3.88614 avg_loss = 3.44571\n",
      "epoch no.4 train no.368760  loss = 4.72506 avg_loss = 3.48561\n",
      "epoch no.4 train no.368770  loss = 4.49383 avg_loss = 3.45226\n",
      "epoch no.4 train no.368780  loss = 3.48506 avg_loss = 3.43025\n",
      "epoch no.4 train no.368790  loss = 4.04493 avg_loss = 3.45169\n",
      "epoch no.4 train no.368800  loss = 2.36938 avg_loss = 3.42680\n",
      "epoch no.4 train no.368810  loss = 3.74794 avg_loss = 3.41707\n",
      "epoch no.4 train no.368820  loss = 4.90939 avg_loss = 3.41263\n",
      "epoch no.4 train no.368830  loss = 3.53924 avg_loss = 3.38337\n",
      "epoch no.4 train no.368840  loss = 3.47977 avg_loss = 3.37979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.368850  loss = 4.26174 avg_loss = 3.40191\n",
      "epoch no.4 train no.368860  loss = 3.16679 avg_loss = 3.39883\n",
      "epoch no.4 train no.368870  loss = 2.75707 avg_loss = 3.36709\n",
      "epoch no.4 train no.368880  loss = 2.76160 avg_loss = 3.39156\n",
      "epoch no.4 train no.368890  loss = 5.39381 avg_loss = 3.40038\n",
      "epoch no.4 train no.368900  loss = 3.02134 avg_loss = 3.35874\n",
      "epoch no.4 train no.368910  loss = 2.14578 avg_loss = 3.34925\n",
      "epoch no.4 train no.368920  loss = 3.60992 avg_loss = 3.35469\n",
      "epoch no.4 train no.368930  loss = 4.71641 avg_loss = 3.42537\n",
      "epoch no.4 train no.368940  loss = 2.31620 avg_loss = 3.42531\n",
      "epoch no.4 train no.368950  loss = 2.38574 avg_loss = 3.41577\n",
      "epoch no.4 train no.368960  loss = 3.88606 avg_loss = 3.41778\n",
      "epoch no.4 train no.368970  loss = 2.25389 avg_loss = 3.38781\n",
      "epoch no.4 train no.368980  loss = 2.59039 avg_loss = 3.39877\n",
      "epoch no.4 train no.368990  loss = 1.23791 avg_loss = 3.43788\n",
      "epoch no.4 train no.369000  loss = 3.33779 avg_loss = 3.39483\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁위한', '▁팝', '</s>']\n",
      "기분전환을 위한 음악</s>\n",
      "epoch no.4 train no.369010  loss = 2.09259 avg_loss = 3.41327\n",
      "epoch no.4 train no.369020  loss = 3.84357 avg_loss = 3.38768\n",
      "epoch no.4 train no.369030  loss = 3.02062 avg_loss = 3.36062\n",
      "epoch no.4 train no.369040  loss = 2.59600 avg_loss = 3.34329\n",
      "epoch no.4 train no.369050  loss = 2.28378 avg_loss = 3.31120\n",
      "epoch no.4 train no.369060  loss = 3.55474 avg_loss = 3.28170\n",
      "epoch no.4 train no.369070  loss = 3.36345 avg_loss = 3.28610\n",
      "epoch no.4 train no.369080  loss = 2.84499 avg_loss = 3.26587\n",
      "epoch no.4 train no.369090  loss = 2.19370 avg_loss = 3.28947\n",
      "epoch no.4 train no.369100  loss = 2.93453 avg_loss = 3.30499\n",
      "epoch no.4 train no.369110  loss = 3.11577 avg_loss = 3.25018\n",
      "epoch no.4 train no.369120  loss = 2.37880 avg_loss = 3.23305\n",
      "epoch no.4 train no.369130  loss = 3.57826 avg_loss = 3.25099\n",
      "epoch no.4 train no.369140  loss = 3.08496 avg_loss = 3.24615\n",
      "epoch no.4 train no.369150  loss = 3.35765 avg_loss = 3.30673\n",
      "epoch no.4 train no.369160  loss = 3.89315 avg_loss = 3.34653\n",
      "epoch no.4 train no.369170  loss = 4.23374 avg_loss = 3.35005\n",
      "epoch no.4 train no.369180  loss = 2.26284 avg_loss = 3.34470\n",
      "epoch no.4 train no.369190  loss = 3.36603 avg_loss = 3.38740\n",
      "epoch no.4 train no.369200  loss = 5.41764 avg_loss = 3.41122\n",
      "epoch no.4 train no.369210  loss = 3.76740 avg_loss = 3.39111\n",
      "epoch no.4 train no.369220  loss = 4.42510 avg_loss = 3.39342\n",
      "epoch no.4 train no.369230  loss = 1.63395 avg_loss = 3.37107\n",
      "epoch no.4 train no.369240  loss = 3.47616 avg_loss = 3.36203\n",
      "epoch no.4 train no.369250  loss = 4.01164 avg_loss = 3.34269\n",
      "epoch no.4 train no.369260  loss = 3.00735 avg_loss = 3.34164\n",
      "epoch no.4 train no.369270  loss = 3.65324 avg_loss = 3.31976\n",
      "epoch no.4 train no.369280  loss = 1.85969 avg_loss = 3.28326\n",
      "epoch no.4 train no.369290  loss = 2.93938 avg_loss = 3.28651\n",
      "epoch no.4 train no.369300  loss = 2.54224 avg_loss = 3.31267\n",
      "epoch no.4 train no.369310  loss = 3.22640 avg_loss = 3.32464\n",
      "epoch no.4 train no.369320  loss = 3.65668 avg_loss = 3.30387\n",
      "epoch no.4 train no.369330  loss = 4.10000 avg_loss = 3.29675\n",
      "epoch no.4 train no.369340  loss = 3.46802 avg_loss = 3.30756\n",
      "epoch no.4 train no.369350  loss = 1.51952 avg_loss = 3.31979\n",
      "epoch no.4 train no.369360  loss = 2.65664 avg_loss = 3.28193\n",
      "epoch no.4 train no.369370  loss = 3.79385 avg_loss = 3.31170\n",
      "epoch no.4 train no.369380  loss = 3.24501 avg_loss = 3.29690\n",
      "epoch no.4 train no.369390  loss = 4.92717 avg_loss = 3.30526\n",
      "epoch no.4 train no.369400  loss = 3.01674 avg_loss = 3.29192\n",
      "epoch no.4 train no.369410  loss = 3.62021 avg_loss = 3.27813\n",
      "epoch no.4 train no.369420  loss = 4.26025 avg_loss = 3.30789\n",
      "epoch no.4 train no.369430  loss = 3.11307 avg_loss = 3.26275\n",
      "epoch no.4 train no.369440  loss = 3.37039 avg_loss = 3.25614\n",
      "epoch no.4 train no.369450  loss = 2.50489 avg_loss = 3.27938\n",
      "epoch no.4 train no.369460  loss = 3.62828 avg_loss = 3.27475\n",
      "epoch no.4 train no.369470  loss = 3.41181 avg_loss = 3.32966\n",
      "epoch no.4 train no.369480  loss = 4.17867 avg_loss = 3.33590\n",
      "epoch no.4 train no.369490  loss = 4.08650 avg_loss = 3.34895\n",
      "epoch no.4 train no.369500  loss = 2.59043 avg_loss = 3.27951\n",
      "epoch no.4 train no.369510  loss = 2.31825 avg_loss = 3.25873\n",
      "epoch no.4 train no.369520  loss = 4.58106 avg_loss = 3.26239\n",
      "epoch no.4 train no.369530  loss = 0.90482 avg_loss = 3.21940\n",
      "epoch no.4 train no.369540  loss = 3.43914 avg_loss = 3.24283\n",
      "epoch no.4 train no.369550  loss = 4.80331 avg_loss = 3.28684\n",
      "epoch no.4 train no.369560  loss = 3.35420 avg_loss = 3.34746\n",
      "epoch no.4 train no.369570  loss = 3.32651 avg_loss = 3.36039\n",
      "epoch no.4 train no.369580  loss = 4.24787 avg_loss = 3.37074\n",
      "epoch no.4 train no.369590  loss = 2.84428 avg_loss = 3.33791\n",
      "epoch no.4 train no.369600  loss = 4.32421 avg_loss = 3.30428\n",
      "epoch no.4 train no.369610  loss = 2.37303 avg_loss = 3.28236\n",
      "epoch no.4 train no.369620  loss = 3.12823 avg_loss = 3.25776\n",
      "epoch no.4 train no.369630  loss = 3.52577 avg_loss = 3.30580\n",
      "epoch no.4 train no.369640  loss = 2.09380 avg_loss = 3.26669\n",
      "epoch no.4 train no.369650  loss = 4.38636 avg_loss = 3.28641\n",
      "epoch no.4 train no.369660  loss = 3.65217 avg_loss = 3.28970\n",
      "epoch no.4 train no.369670  loss = 2.82249 avg_loss = 3.26987\n",
      "epoch no.4 train no.369680  loss = 3.65680 avg_loss = 3.24547\n",
      "epoch no.4 train no.369690  loss = 3.86504 avg_loss = 3.27315\n",
      "epoch no.4 train no.369700  loss = 3.80025 avg_loss = 3.32156\n",
      "epoch no.4 train no.369710  loss = 2.44192 avg_loss = 3.30413\n",
      "epoch no.4 train no.369720  loss = 3.36805 avg_loss = 3.29487\n",
      "epoch no.4 train no.369730  loss = 2.55839 avg_loss = 3.31058\n",
      "epoch no.4 train no.369740  loss = 2.80071 avg_loss = 3.30244\n",
      "epoch no.4 train no.369750  loss = 5.11831 avg_loss = 3.35181\n",
      "epoch no.4 train no.369760  loss = 5.15790 avg_loss = 3.38262\n",
      "epoch no.4 train no.369770  loss = 3.09757 avg_loss = 3.34873\n",
      "epoch no.4 train no.369780  loss = 2.70254 avg_loss = 3.35834\n",
      "epoch no.4 train no.369790  loss = 3.77529 avg_loss = 3.35468\n",
      "epoch no.4 train no.369800  loss = 4.18721 avg_loss = 3.35336\n",
      "epoch no.4 train no.369810  loss = 3.08220 avg_loss = 3.31925\n",
      "epoch no.4 train no.369820  loss = 4.11417 avg_loss = 3.32511\n",
      "epoch no.4 train no.369830  loss = 3.90206 avg_loss = 3.34689\n",
      "epoch no.4 train no.369840  loss = 2.58369 avg_loss = 3.33269\n",
      "epoch no.4 train no.369850  loss = 2.75961 avg_loss = 3.28889\n",
      "epoch no.4 train no.369860  loss = 4.55813 avg_loss = 3.29551\n",
      "epoch no.4 train no.369870  loss = 3.58660 avg_loss = 3.30410\n",
      "epoch no.4 train no.369880  loss = 2.42536 avg_loss = 3.25410\n",
      "epoch no.4 train no.369890  loss = 2.45652 avg_loss = 3.24788\n",
      "epoch no.4 train no.369900  loss = 1.75972 avg_loss = 3.27435\n",
      "epoch no.4 train no.369910  loss = 3.71966 avg_loss = 3.32410\n",
      "epoch no.4 train no.369920  loss = 3.36455 avg_loss = 3.33742\n",
      "epoch no.4 train no.369930  loss = 3.23367 avg_loss = 3.35369\n",
      "epoch no.4 train no.369940  loss = 3.52234 avg_loss = 3.31787\n",
      "epoch no.4 train no.369950  loss = 4.95093 avg_loss = 3.35045\n",
      "epoch no.4 train no.369960  loss = 3.13193 avg_loss = 3.32240\n",
      "epoch no.4 train no.369970  loss = 2.94978 avg_loss = 3.31361\n",
      "epoch no.4 train no.369980  loss = 4.25162 avg_loss = 3.33761\n",
      "epoch no.4 train no.369990  loss = 3.12409 avg_loss = 3.35343\n",
      "epoch no.4 train no.370000  loss = 3.27478 avg_loss = 3.33074\n",
      "3\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁싶을', '때', '▁듣는']\n",
      "기분전환하고 싶을때</s>\n",
      "epoch no.4 train no.370010  loss = 2.77430 avg_loss = 3.25650\n",
      "epoch no.4 train no.370020  loss = 4.38227 avg_loss = 3.30838\n",
      "epoch no.4 train no.370030  loss = 2.76172 avg_loss = 3.31567\n",
      "epoch no.4 train no.370040  loss = 2.75602 avg_loss = 3.34772\n",
      "epoch no.4 train no.370050  loss = 2.11500 avg_loss = 3.31613\n",
      "epoch no.4 train no.370060  loss = 2.80885 avg_loss = 3.30362\n",
      "epoch no.4 train no.370070  loss = 3.74017 avg_loss = 3.28802\n",
      "epoch no.4 train no.370080  loss = 4.23031 avg_loss = 3.27007\n",
      "epoch no.4 train no.370090  loss = 3.15957 avg_loss = 3.28045\n",
      "epoch no.4 train no.370100  loss = 3.30141 avg_loss = 3.25635\n",
      "epoch no.4 train no.370110  loss = 3.18038 avg_loss = 3.28889\n",
      "epoch no.4 train no.370120  loss = 3.43022 avg_loss = 3.28423\n",
      "epoch no.4 train no.370130  loss = 3.42656 avg_loss = 3.30505\n",
      "epoch no.4 train no.370140  loss = 2.38325 avg_loss = 3.30858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.370150  loss = 3.05036 avg_loss = 3.30460\n",
      "epoch no.4 train no.370160  loss = 2.19927 avg_loss = 3.29954\n",
      "epoch no.4 train no.370170  loss = 1.80748 avg_loss = 3.25791\n",
      "epoch no.4 train no.370180  loss = 1.62102 avg_loss = 3.27493\n",
      "epoch no.4 train no.370190  loss = 3.37524 avg_loss = 3.25215\n",
      "epoch no.4 train no.370200  loss = 2.89045 avg_loss = 3.26968\n",
      "epoch no.4 train no.370210  loss = 3.55496 avg_loss = 3.27710\n",
      "epoch no.4 train no.370220  loss = 2.65032 avg_loss = 3.27310\n",
      "epoch no.4 train no.370230  loss = 2.92237 avg_loss = 3.24236\n",
      "epoch no.4 train no.370240  loss = 2.57426 avg_loss = 3.21882\n",
      "epoch no.4 train no.370250  loss = 3.98162 avg_loss = 3.26586\n",
      "epoch no.4 train no.370260  loss = 5.28093 avg_loss = 3.30693\n",
      "epoch no.4 train no.370270  loss = 3.77343 avg_loss = 3.34424\n",
      "epoch no.4 train no.370280  loss = 2.92357 avg_loss = 3.30897\n",
      "epoch no.4 train no.370290  loss = 2.29520 avg_loss = 3.29879\n",
      "epoch no.4 train no.370300  loss = 3.43575 avg_loss = 3.28255\n",
      "epoch no.4 train no.370310  loss = 2.88186 avg_loss = 3.29227\n",
      "epoch no.4 train no.370320  loss = 4.13388 avg_loss = 3.28712\n",
      "epoch no.4 train no.370330  loss = 3.84530 avg_loss = 3.32248\n",
      "epoch no.4 train no.370340  loss = 2.03244 avg_loss = 3.27947\n",
      "epoch no.4 train no.370350  loss = 4.50051 avg_loss = 3.29071\n",
      "epoch no.4 train no.370360  loss = 2.80205 avg_loss = 3.28165\n",
      "epoch no.4 train no.370370  loss = 3.51543 avg_loss = 3.28687\n",
      "epoch no.4 train no.370380  loss = 4.59826 avg_loss = 3.30977\n",
      "epoch no.4 train no.370390  loss = 2.87317 avg_loss = 3.33623\n",
      "epoch no.4 train no.370400  loss = 4.08930 avg_loss = 3.31116\n",
      "epoch no.4 train no.370410  loss = 5.80478 avg_loss = 3.31383\n",
      "epoch no.4 train no.370420  loss = 3.85142 avg_loss = 3.35756\n",
      "epoch no.4 train no.370430  loss = 4.00209 avg_loss = 3.37175\n",
      "epoch no.4 train no.370440  loss = 5.42823 avg_loss = 3.34375\n",
      "epoch no.4 train no.370450  loss = 3.19130 avg_loss = 3.32032\n",
      "epoch no.4 train no.370460  loss = 4.14149 avg_loss = 3.29426\n",
      "epoch no.4 train no.370470  loss = 4.47206 avg_loss = 3.32189\n",
      "epoch no.4 train no.370480  loss = 2.68390 avg_loss = 3.32418\n",
      "epoch no.4 train no.370490  loss = 2.09191 avg_loss = 3.28826\n",
      "epoch no.4 train no.370500  loss = 3.48471 avg_loss = 3.25780\n",
      "epoch no.4 train no.370510  loss = 5.48178 avg_loss = 3.28102\n",
      "epoch no.4 train no.370520  loss = 4.65988 avg_loss = 3.30546\n",
      "epoch no.4 train no.370530  loss = 4.11832 avg_loss = 3.30758\n",
      "epoch no.4 train no.370540  loss = 3.71141 avg_loss = 3.30111\n",
      "epoch no.4 train no.370550  loss = 2.39765 avg_loss = 3.31643\n",
      "epoch no.4 train no.370560  loss = 4.21984 avg_loss = 3.31095\n",
      "epoch no.4 train no.370570  loss = 3.76565 avg_loss = 3.30644\n",
      "epoch no.4 train no.370580  loss = 2.83184 avg_loss = 3.30350\n",
      "epoch no.4 train no.370590  loss = 3.69476 avg_loss = 3.30948\n",
      "epoch no.4 train no.370600  loss = 3.23321 avg_loss = 3.29701\n",
      "epoch no.4 train no.370610  loss = 3.00122 avg_loss = 3.29696\n",
      "epoch no.4 train no.370620  loss = 3.14705 avg_loss = 3.31710\n",
      "epoch no.4 train no.370630  loss = 1.84133 avg_loss = 3.29363\n",
      "epoch no.4 train no.370640  loss = 4.53515 avg_loss = 3.32039\n",
      "epoch no.4 train no.370650  loss = 2.46432 avg_loss = 3.35075\n",
      "epoch no.4 train no.370660  loss = 3.22081 avg_loss = 3.31196\n",
      "epoch no.4 train no.370670  loss = 3.63308 avg_loss = 3.32430\n",
      "epoch no.4 train no.370680  loss = 3.89299 avg_loss = 3.32461\n",
      "epoch no.4 train no.370690  loss = 2.18529 avg_loss = 3.29973\n",
      "epoch no.4 train no.370700  loss = 2.54644 avg_loss = 3.28354\n",
      "epoch no.4 train no.370710  loss = 3.82445 avg_loss = 3.28885\n",
      "epoch no.4 train no.370720  loss = 5.60187 avg_loss = 3.29165\n",
      "epoch no.4 train no.370730  loss = 2.92935 avg_loss = 3.30172\n",
      "epoch no.4 train no.370740  loss = 2.99573 avg_loss = 3.29353\n",
      "epoch no.4 train no.370750  loss = 2.70981 avg_loss = 3.33719\n",
      "epoch no.4 train no.370760  loss = 1.73900 avg_loss = 3.28983\n",
      "epoch no.4 train no.370770  loss = 4.13043 avg_loss = 3.29533\n",
      "epoch no.4 train no.370780  loss = 2.04840 avg_loss = 3.27797\n",
      "epoch no.4 train no.370790  loss = 2.90284 avg_loss = 3.27424\n",
      "epoch no.4 train no.370800  loss = 2.58040 avg_loss = 3.28554\n",
      "epoch no.4 train no.370810  loss = 4.17525 avg_loss = 3.34608\n",
      "epoch no.4 train no.370820  loss = 4.33181 avg_loss = 3.36876\n",
      "epoch no.4 train no.370830  loss = 6.99983 avg_loss = 3.37995\n",
      "epoch no.4 train no.370840  loss = 2.52862 avg_loss = 3.37668\n",
      "epoch no.4 train no.370850  loss = 3.18891 avg_loss = 3.36431\n",
      "epoch no.4 train no.370860  loss = 3.44644 avg_loss = 3.33002\n",
      "epoch no.4 train no.370870  loss = 4.06189 avg_loss = 3.31323\n",
      "epoch no.4 train no.370880  loss = 2.78609 avg_loss = 3.25137\n",
      "epoch no.4 train no.370890  loss = 2.80007 avg_loss = 3.21947\n",
      "epoch no.4 train no.370900  loss = 3.11699 avg_loss = 3.23115\n",
      "epoch no.4 train no.370910  loss = 1.86659 avg_loss = 3.24508\n",
      "epoch no.4 train no.370920  loss = 3.38719 avg_loss = 3.29085\n",
      "epoch no.4 train no.370930  loss = 4.17746 avg_loss = 3.29453\n",
      "epoch no.4 train no.370940  loss = 3.60361 avg_loss = 3.28059\n",
      "epoch no.4 train no.370950  loss = 4.65383 avg_loss = 3.29622\n",
      "epoch no.4 train no.370960  loss = 2.74212 avg_loss = 3.25683\n",
      "epoch no.4 train no.370970  loss = 2.35395 avg_loss = 3.25854\n",
      "epoch no.4 train no.370980  loss = 2.85673 avg_loss = 3.24039\n",
      "epoch no.4 train no.370990  loss = 4.01232 avg_loss = 3.25321\n",
      "epoch no.4 train no.371000  loss = 4.35479 avg_loss = 3.28643\n",
      "8\n",
      "to_tokens: ['▁라디오', '좋은', '하기', '▁위한', '▁감각적인', '▁음악', '트의', '▁p', '힙', 'op', '</s>']\n",
      "기분전환을 위한 감각적인 비트의 해외 pop</s>\n",
      "epoch no.4 train no.371010  loss = 2.06787 avg_loss = 3.25264\n",
      "epoch no.4 train no.371020  loss = 3.11989 avg_loss = 3.33801\n",
      "epoch no.4 train no.371030  loss = 3.21199 avg_loss = 3.33836\n",
      "epoch no.4 train no.371040  loss = 3.24560 avg_loss = 3.31520\n",
      "epoch no.4 train no.371050  loss = 2.11814 avg_loss = 3.27589\n",
      "epoch no.4 train no.371060  loss = 2.45194 avg_loss = 3.30395\n",
      "epoch no.4 train no.371070  loss = 3.98072 avg_loss = 3.34033\n",
      "epoch no.4 train no.371080  loss = 2.43985 avg_loss = 3.36285\n",
      "epoch no.4 train no.371090  loss = 1.90781 avg_loss = 3.35788\n",
      "epoch no.4 train no.371100  loss = 2.52770 avg_loss = 3.32878\n",
      "epoch no.4 train no.371110  loss = 2.95499 avg_loss = 3.33560\n",
      "epoch no.4 train no.371120  loss = 4.48124 avg_loss = 3.36443\n",
      "epoch no.4 train no.371130  loss = 2.66987 avg_loss = 3.35252\n",
      "epoch no.4 train no.371140  loss = 3.06029 avg_loss = 3.32836\n",
      "epoch no.4 train no.371150  loss = 4.05545 avg_loss = 3.30727\n",
      "epoch no.4 train no.371160  loss = 3.32736 avg_loss = 3.29834\n",
      "epoch no.4 train no.371170  loss = 3.03328 avg_loss = 3.28175\n",
      "epoch no.4 train no.371180  loss = 2.71979 avg_loss = 3.29908\n",
      "epoch no.4 train no.371190  loss = 2.83432 avg_loss = 3.27165\n",
      "epoch no.4 train no.371200  loss = 3.27202 avg_loss = 3.27843\n",
      "epoch no.4 train no.371210  loss = 2.85738 avg_loss = 3.28367\n",
      "epoch no.4 train no.371220  loss = 2.98792 avg_loss = 3.26221\n",
      "epoch no.4 train no.371230  loss = 3.57005 avg_loss = 3.26111\n",
      "epoch no.4 train no.371240  loss = 2.76608 avg_loss = 3.24362\n",
      "epoch no.4 train no.371250  loss = 2.47572 avg_loss = 3.23029\n",
      "epoch no.4 train no.371260  loss = 3.31200 avg_loss = 3.22306\n",
      "epoch no.4 train no.371270  loss = 2.58484 avg_loss = 3.25632\n",
      "epoch no.4 train no.371280  loss = 2.27593 avg_loss = 3.26716\n",
      "epoch no.4 train no.371290  loss = 3.61562 avg_loss = 3.29594\n",
      "epoch no.4 train no.371300  loss = 3.33949 avg_loss = 3.35811\n",
      "epoch no.4 train no.371310  loss = 4.37988 avg_loss = 3.42071\n",
      "epoch no.4 train no.371320  loss = 2.01390 avg_loss = 3.37419\n",
      "epoch no.4 train no.371330  loss = 5.15384 avg_loss = 3.36324\n",
      "epoch no.4 train no.371340  loss = 3.33791 avg_loss = 3.36206\n",
      "epoch no.4 train no.371350  loss = 3.24941 avg_loss = 3.32740\n",
      "epoch no.4 train no.371360  loss = 3.32990 avg_loss = 3.34576\n",
      "epoch no.4 train no.371370  loss = 3.98276 avg_loss = 3.39465\n",
      "epoch no.4 train no.371380  loss = 2.57488 avg_loss = 3.36310\n",
      "epoch no.4 train no.371390  loss = 2.21306 avg_loss = 3.37837\n",
      "epoch no.4 train no.371400  loss = 4.78954 avg_loss = 3.41230\n",
      "epoch no.4 train no.371410  loss = 2.99312 avg_loss = 3.41666\n",
      "epoch no.4 train no.371420  loss = 3.92849 avg_loss = 3.43112\n",
      "epoch no.4 train no.371430  loss = 4.82540 avg_loss = 3.45794\n",
      "epoch no.4 train no.371440  loss = 1.81698 avg_loss = 3.39772\n",
      "epoch no.4 train no.371450  loss = 4.73141 avg_loss = 3.39087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.371460  loss = 1.97566 avg_loss = 3.34278\n",
      "epoch no.4 train no.371470  loss = 4.39951 avg_loss = 3.36853\n",
      "epoch no.4 train no.371480  loss = 3.95319 avg_loss = 3.37744\n",
      "epoch no.4 train no.371490  loss = 3.28651 avg_loss = 3.35503\n",
      "epoch no.4 train no.371500  loss = 4.60810 avg_loss = 3.36465\n",
      "epoch no.4 train no.371510  loss = 4.63504 avg_loss = 3.35209\n",
      "epoch no.4 train no.371520  loss = 3.94120 avg_loss = 3.39310\n",
      "epoch no.4 train no.371530  loss = 4.05895 avg_loss = 3.39724\n",
      "epoch no.4 train no.371540  loss = 4.51260 avg_loss = 3.36268\n",
      "epoch no.4 train no.371550  loss = 3.28917 avg_loss = 3.34847\n",
      "epoch no.4 train no.371560  loss = 2.88834 avg_loss = 3.38386\n",
      "epoch no.4 train no.371570  loss = 1.84829 avg_loss = 3.36341\n",
      "epoch no.4 train no.371580  loss = 2.16079 avg_loss = 3.35414\n",
      "epoch no.4 train no.371590  loss = 3.88388 avg_loss = 3.38363\n",
      "epoch no.4 train no.371600  loss = 3.87205 avg_loss = 3.37402\n",
      "epoch no.4 train no.371610  loss = 2.95840 avg_loss = 3.40012\n",
      "epoch no.4 train no.371620  loss = 2.62343 avg_loss = 3.36944\n",
      "epoch no.4 train no.371630  loss = 2.50299 avg_loss = 3.38262\n",
      "epoch no.4 train no.371640  loss = 3.37211 avg_loss = 3.39867\n",
      "epoch no.4 train no.371650  loss = 5.15326 avg_loss = 3.40950\n",
      "epoch no.4 train no.371660  loss = 3.66434 avg_loss = 3.39339\n",
      "epoch no.4 train no.371670  loss = 4.34992 avg_loss = 3.37672\n",
      "epoch no.4 train no.371680  loss = 2.18395 avg_loss = 3.36665\n",
      "epoch no.4 train no.371690  loss = 4.87972 avg_loss = 3.36387\n",
      "epoch no.4 train no.371700  loss = 3.78153 avg_loss = 3.40333\n",
      "epoch no.4 train no.371710  loss = 1.88923 avg_loss = 3.33895\n",
      "epoch no.4 train no.371720  loss = 2.74935 avg_loss = 3.36035\n",
      "epoch no.4 train no.371730  loss = 1.65997 avg_loss = 3.33772\n",
      "epoch no.4 train no.371740  loss = 4.64950 avg_loss = 3.39671\n",
      "epoch no.4 train no.371750  loss = 2.85780 avg_loss = 3.38111\n",
      "epoch no.4 train no.371760  loss = 4.30805 avg_loss = 3.35673\n",
      "epoch no.4 train no.371770  loss = 3.39074 avg_loss = 3.32915\n",
      "epoch no.4 train no.371780  loss = 3.82634 avg_loss = 3.35896\n",
      "epoch no.4 train no.371790  loss = 1.98524 avg_loss = 3.38919\n",
      "epoch no.4 train no.371800  loss = 4.89973 avg_loss = 3.37914\n",
      "epoch no.4 train no.371810  loss = 2.48309 avg_loss = 3.39852\n",
      "epoch no.4 train no.371820  loss = 3.27880 avg_loss = 3.41261\n",
      "epoch no.4 train no.371830  loss = 3.45241 avg_loss = 3.36887\n",
      "epoch no.4 train no.371840  loss = 2.87260 avg_loss = 3.37373\n",
      "epoch no.4 train no.371850  loss = 3.38409 avg_loss = 3.38265\n",
      "epoch no.4 train no.371860  loss = 2.26454 avg_loss = 3.37545\n",
      "epoch no.4 train no.371870  loss = 2.26258 avg_loss = 3.34263\n",
      "epoch no.4 train no.371880  loss = 3.66892 avg_loss = 3.32138\n",
      "epoch no.4 train no.371890  loss = 3.13905 avg_loss = 3.32786\n",
      "epoch no.4 train no.371900  loss = 3.56522 avg_loss = 3.35584\n",
      "epoch no.4 train no.371910  loss = 5.61452 avg_loss = 3.37443\n",
      "epoch no.4 train no.371920  loss = 2.65943 avg_loss = 3.33420\n",
      "epoch no.4 train no.371930  loss = 2.63011 avg_loss = 3.33182\n",
      "epoch no.4 train no.371940  loss = 2.66843 avg_loss = 3.29082\n",
      "epoch no.4 train no.371950  loss = 3.16507 avg_loss = 3.27008\n",
      "epoch no.4 train no.371960  loss = 2.91779 avg_loss = 3.28888\n",
      "epoch no.4 train no.371970  loss = 3.58799 avg_loss = 3.28181\n",
      "epoch no.4 train no.371980  loss = 4.56450 avg_loss = 3.33062\n",
      "epoch no.4 train no.371990  loss = 3.76212 avg_loss = 3.30705\n",
      "epoch no.4 train no.372000  loss = 4.02057 avg_loss = 3.33518\n",
      "3\n",
      "to_tokens: ['▁비', '전환', '이', '▁위한', '▁드라이브', '</s>']\n",
      "기분전환을 위한 팝</s>\n",
      "epoch no.4 train no.372010  loss = 3.38148 avg_loss = 3.38303\n",
      "epoch no.4 train no.372020  loss = 3.22835 avg_loss = 3.38422\n",
      "epoch no.4 train no.372030  loss = 2.44930 avg_loss = 3.31744\n",
      "epoch no.4 train no.372040  loss = 3.62821 avg_loss = 3.31399\n",
      "epoch no.4 train no.372050  loss = 3.07750 avg_loss = 3.27506\n",
      "epoch no.4 train no.372060  loss = 2.54255 avg_loss = 3.28676\n",
      "epoch no.4 train no.372070  loss = 2.84254 avg_loss = 3.25070\n",
      "epoch no.4 train no.372080  loss = 5.49784 avg_loss = 3.25136\n",
      "epoch no.4 train no.372090  loss = 3.75435 avg_loss = 3.27929\n",
      "epoch no.4 train no.372100  loss = 2.37938 avg_loss = 3.24502\n",
      "epoch no.4 train no.372110  loss = 2.47237 avg_loss = 3.28545\n",
      "epoch no.4 train no.372120  loss = 2.59873 avg_loss = 3.29150\n",
      "epoch no.4 train no.372130  loss = 4.99094 avg_loss = 3.33871\n",
      "epoch no.4 train no.372140  loss = 3.51082 avg_loss = 3.33321\n",
      "epoch no.4 train no.372150  loss = 4.90771 avg_loss = 3.31845\n",
      "epoch no.4 train no.372160  loss = 3.49677 avg_loss = 3.33728\n",
      "epoch no.4 train no.372170  loss = 2.26560 avg_loss = 3.30587\n",
      "epoch no.4 train no.372180  loss = 4.06185 avg_loss = 3.29782\n",
      "epoch no.4 train no.372190  loss = 1.82087 avg_loss = 3.24364\n",
      "epoch no.4 train no.372200  loss = 3.66768 avg_loss = 3.25910\n",
      "epoch no.4 train no.372210  loss = 3.94753 avg_loss = 3.28758\n",
      "epoch no.4 train no.372220  loss = 4.36420 avg_loss = 3.26371\n",
      "epoch no.4 train no.372230  loss = 2.61706 avg_loss = 3.24173\n",
      "epoch no.4 train no.372240  loss = 3.74224 avg_loss = 3.27842\n",
      "epoch no.4 train no.372250  loss = 3.12957 avg_loss = 3.26455\n",
      "epoch no.4 train no.372260  loss = 3.77669 avg_loss = 3.28563\n",
      "epoch no.4 train no.372270  loss = 4.48515 avg_loss = 3.33295\n",
      "epoch no.4 train no.372280  loss = 2.52580 avg_loss = 3.33234\n",
      "epoch no.4 train no.372290  loss = 3.77333 avg_loss = 3.39357\n",
      "epoch no.4 train no.372300  loss = 2.27045 avg_loss = 3.40088\n",
      "epoch no.4 train no.372310  loss = 2.74253 avg_loss = 3.36759\n",
      "epoch no.4 train no.372320  loss = 5.07917 avg_loss = 3.37952\n",
      "epoch no.4 train no.372330  loss = 4.36867 avg_loss = 3.34023\n",
      "epoch no.4 train no.372340  loss = 6.02931 avg_loss = 3.32798\n",
      "epoch no.4 train no.372350  loss = 5.13254 avg_loss = 3.31954\n",
      "epoch no.4 train no.372360  loss = 2.32342 avg_loss = 3.35370\n",
      "epoch no.4 train no.372370  loss = 3.13018 avg_loss = 3.35567\n",
      "epoch no.4 train no.372380  loss = 3.39064 avg_loss = 3.32821\n",
      "epoch no.4 train no.372390  loss = 2.32173 avg_loss = 3.39618\n",
      "epoch no.4 train no.372400  loss = 2.66897 avg_loss = 3.36052\n",
      "epoch no.4 train no.372410  loss = 2.85129 avg_loss = 3.34668\n",
      "epoch no.4 train no.372420  loss = 3.15774 avg_loss = 3.34868\n",
      "epoch no.4 train no.372430  loss = 3.51710 avg_loss = 3.35431\n",
      "epoch no.4 train no.372440  loss = 3.04352 avg_loss = 3.37922\n",
      "epoch no.4 train no.372450  loss = 3.28583 avg_loss = 3.32823\n",
      "epoch no.4 train no.372460  loss = 2.66709 avg_loss = 3.26440\n",
      "epoch no.4 train no.372470  loss = 3.33922 avg_loss = 3.25378\n",
      "epoch no.4 train no.372480  loss = 4.06894 avg_loss = 3.33968\n",
      "epoch no.4 train no.372490  loss = 3.43816 avg_loss = 3.34091\n",
      "epoch no.4 train no.372500  loss = 2.70671 avg_loss = 3.31320\n",
      "epoch no.4 train no.372510  loss = 3.27680 avg_loss = 3.34412\n",
      "epoch no.4 train no.372520  loss = 5.63902 avg_loss = 3.41916\n",
      "epoch no.4 train no.372530  loss = 3.49283 avg_loss = 3.39984\n",
      "epoch no.4 train no.372540  loss = 5.80840 avg_loss = 3.40271\n",
      "epoch no.4 train no.372550  loss = 2.01692 avg_loss = 3.35823\n",
      "epoch no.4 train no.372560  loss = 3.55124 avg_loss = 3.34456\n",
      "epoch no.4 train no.372570  loss = 2.05282 avg_loss = 3.32068\n",
      "epoch no.4 train no.372580  loss = 5.09633 avg_loss = 3.34075\n",
      "epoch no.4 train no.372590  loss = 3.70549 avg_loss = 3.31898\n",
      "epoch no.4 train no.372600  loss = 2.14009 avg_loss = 3.27836\n",
      "epoch no.4 train no.372610  loss = 2.14588 avg_loss = 3.25780\n",
      "epoch no.4 train no.372620  loss = 2.60488 avg_loss = 3.27739\n",
      "epoch no.4 train no.372630  loss = 2.93322 avg_loss = 3.25317\n",
      "epoch no.4 train no.372640  loss = 2.79629 avg_loss = 3.22810\n",
      "epoch no.4 train no.372650  loss = 2.66290 avg_loss = 3.20778\n",
      "epoch no.4 train no.372660  loss = 3.18800 avg_loss = 3.19917\n",
      "epoch no.4 train no.372670  loss = 4.16767 avg_loss = 3.18050\n",
      "epoch no.4 train no.372680  loss = 5.21150 avg_loss = 3.23838\n",
      "epoch no.4 train no.372690  loss = 2.26431 avg_loss = 3.27095\n",
      "epoch no.4 train no.372700  loss = 3.96009 avg_loss = 3.28759\n",
      "epoch no.4 train no.372710  loss = 2.26964 avg_loss = 3.30846\n",
      "epoch no.4 train no.372720  loss = 3.93616 avg_loss = 3.34485\n",
      "epoch no.4 train no.372730  loss = 4.02162 avg_loss = 3.35352\n",
      "epoch no.4 train no.372740  loss = 3.56974 avg_loss = 3.36355\n",
      "epoch no.4 train no.372750  loss = 3.29846 avg_loss = 3.33619\n",
      "epoch no.4 train no.372760  loss = 4.37800 avg_loss = 3.30997\n",
      "epoch no.4 train no.372770  loss = 2.64989 avg_loss = 3.32962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.372780  loss = 4.47897 avg_loss = 3.34748\n",
      "epoch no.4 train no.372790  loss = 3.35124 avg_loss = 3.34442\n",
      "epoch no.4 train no.372800  loss = 5.08892 avg_loss = 3.36228\n",
      "epoch no.4 train no.372810  loss = 3.87838 avg_loss = 3.44313\n",
      "epoch no.4 train no.372820  loss = 5.90937 avg_loss = 3.45559\n",
      "epoch no.4 train no.372830  loss = 2.80997 avg_loss = 3.38696\n",
      "epoch no.4 train no.372840  loss = 4.01311 avg_loss = 3.34779\n",
      "epoch no.4 train no.372850  loss = 4.32223 avg_loss = 3.33649\n",
      "epoch no.4 train no.372860  loss = 2.55420 avg_loss = 3.32303\n",
      "epoch no.4 train no.372870  loss = 3.54750 avg_loss = 3.29514\n",
      "epoch no.4 train no.372880  loss = 2.45252 avg_loss = 3.27081\n",
      "epoch no.4 train no.372890  loss = 3.38833 avg_loss = 3.28984\n",
      "epoch no.4 train no.372900  loss = 2.24285 avg_loss = 3.26608\n",
      "epoch no.4 train no.372910  loss = 4.44320 avg_loss = 3.28817\n",
      "epoch no.4 train no.372920  loss = 2.62423 avg_loss = 3.23901\n",
      "epoch no.4 train no.372930  loss = 4.78685 avg_loss = 3.20945\n",
      "epoch no.4 train no.372940  loss = 4.58283 avg_loss = 3.23517\n",
      "epoch no.4 train no.372950  loss = 3.44573 avg_loss = 3.29324\n",
      "epoch no.4 train no.372960  loss = 3.71925 avg_loss = 3.30659\n",
      "epoch no.4 train no.372970  loss = 2.85131 avg_loss = 3.38555\n",
      "epoch no.4 train no.372980  loss = 4.36325 avg_loss = 3.39394\n",
      "epoch no.4 train no.372990  loss = 2.88056 avg_loss = 3.37355\n",
      "epoch no.4 train no.373000  loss = 2.76774 avg_loss = 3.31381\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는', '▁팝', '</s>']\n",
      "기분전환이 필요할 때 듣는 재즈</s>\n",
      "epoch no.4 train no.373010  loss = 3.13242 avg_loss = 3.37047\n",
      "epoch no.4 train no.373020  loss = 5.26454 avg_loss = 3.33375\n",
      "epoch no.4 train no.373030  loss = 3.58953 avg_loss = 3.33572\n",
      "epoch no.4 train no.373040  loss = 3.76840 avg_loss = 3.32626\n",
      "epoch no.4 train no.373050  loss = 3.08137 avg_loss = 3.34935\n",
      "epoch no.4 train no.373060  loss = 2.92948 avg_loss = 3.35478\n",
      "epoch no.4 train no.373070  loss = 3.52091 avg_loss = 3.36917\n",
      "epoch no.4 train no.373080  loss = 2.06269 avg_loss = 3.30545\n",
      "epoch no.4 train no.373090  loss = 1.84022 avg_loss = 3.28614\n",
      "epoch no.4 train no.373100  loss = 3.53981 avg_loss = 3.25570\n",
      "epoch no.4 train no.373110  loss = 2.89897 avg_loss = 3.21154\n",
      "epoch no.4 train no.373120  loss = 1.27784 avg_loss = 3.18159\n",
      "epoch no.4 train no.373130  loss = 4.37782 avg_loss = 3.19060\n",
      "epoch no.4 train no.373140  loss = 2.84347 avg_loss = 3.17752\n",
      "epoch no.4 train no.373150  loss = 3.09602 avg_loss = 3.20580\n",
      "epoch no.4 train no.373160  loss = 3.63506 avg_loss = 3.22739\n",
      "epoch no.4 train no.373170  loss = 2.30839 avg_loss = 3.21804\n",
      "epoch no.4 train no.373180  loss = 3.29428 avg_loss = 3.24225\n",
      "epoch no.4 train no.373190  loss = 5.92308 avg_loss = 3.29193\n",
      "epoch no.4 train no.373200  loss = 3.65389 avg_loss = 3.27631\n",
      "epoch no.4 train no.373210  loss = 2.81262 avg_loss = 3.27408\n",
      "epoch no.4 train no.373220  loss = 4.24255 avg_loss = 3.29788\n",
      "epoch no.4 train no.373230  loss = 2.31860 avg_loss = 3.27205\n",
      "epoch no.4 train no.373240  loss = 2.21156 avg_loss = 3.23501\n",
      "epoch no.4 train no.373250  loss = 2.16782 avg_loss = 3.21145\n",
      "epoch no.4 train no.373260  loss = 3.65469 avg_loss = 3.17265\n",
      "epoch no.4 train no.373270  loss = 3.03510 avg_loss = 3.21324\n",
      "epoch no.4 train no.373280  loss = 3.91547 avg_loss = 3.20803\n",
      "epoch no.4 train no.373290  loss = 3.31152 avg_loss = 3.19708\n",
      "epoch no.4 train no.373300  loss = 3.47776 avg_loss = 3.19167\n",
      "epoch no.4 train no.373310  loss = 4.47815 avg_loss = 3.25222\n",
      "epoch no.4 train no.373320  loss = 3.19408 avg_loss = 3.24772\n",
      "epoch no.4 train no.373330  loss = 3.73562 avg_loss = 3.27817\n",
      "epoch no.4 train no.373340  loss = 3.84677 avg_loss = 3.27748\n",
      "epoch no.4 train no.373350  loss = 1.68947 avg_loss = 3.24378\n",
      "epoch no.4 train no.373360  loss = 3.03362 avg_loss = 3.23227\n",
      "epoch no.4 train no.373370  loss = 3.57042 avg_loss = 3.22715\n",
      "epoch no.4 train no.373380  loss = 5.40696 avg_loss = 3.30465\n",
      "epoch no.4 train no.373390  loss = 3.21275 avg_loss = 3.27779\n",
      "epoch no.4 train no.373400  loss = 2.48739 avg_loss = 3.26372\n",
      "epoch no.4 train no.373410  loss = 2.85907 avg_loss = 3.25516\n",
      "epoch no.4 train no.373420  loss = 2.71787 avg_loss = 3.26259\n",
      "epoch no.4 train no.373430  loss = 3.10338 avg_loss = 3.28617\n",
      "epoch no.4 train no.373440  loss = 4.31925 avg_loss = 3.29342\n",
      "epoch no.4 train no.373450  loss = 4.63614 avg_loss = 3.31831\n",
      "epoch no.4 train no.373460  loss = 2.09303 avg_loss = 3.30418\n",
      "epoch no.4 train no.373470  loss = 6.21360 avg_loss = 3.32261\n",
      "epoch no.4 train no.373480  loss = 2.43354 avg_loss = 3.30384\n",
      "epoch no.4 train no.373490  loss = 3.35225 avg_loss = 3.32335\n",
      "epoch no.4 train no.373500  loss = 2.12352 avg_loss = 3.32038\n",
      "epoch no.4 train no.373510  loss = 3.32022 avg_loss = 3.30906\n",
      "epoch no.4 train no.373520  loss = 3.03422 avg_loss = 3.32006\n",
      "epoch no.4 train no.373530  loss = 2.70616 avg_loss = 3.28254\n",
      "epoch no.4 train no.373540  loss = 3.01829 avg_loss = 3.26326\n",
      "epoch no.4 train no.373550  loss = 3.23391 avg_loss = 3.28088\n",
      "epoch no.4 train no.373560  loss = 2.94357 avg_loss = 3.23559\n",
      "epoch no.4 train no.373570  loss = 2.58528 avg_loss = 3.25708\n",
      "epoch no.4 train no.373580  loss = 4.43478 avg_loss = 3.30437\n",
      "epoch no.4 train no.373590  loss = 2.73732 avg_loss = 3.28266\n",
      "epoch no.4 train no.373600  loss = 4.83053 avg_loss = 3.26470\n",
      "epoch no.4 train no.373610  loss = 3.11745 avg_loss = 3.20456\n",
      "epoch no.4 train no.373620  loss = 3.29759 avg_loss = 3.23125\n",
      "epoch no.4 train no.373630  loss = 2.70941 avg_loss = 3.23660\n",
      "epoch no.4 train no.373640  loss = 3.03554 avg_loss = 3.28119\n",
      "epoch no.4 train no.373650  loss = 2.95703 avg_loss = 3.29384\n",
      "epoch no.4 train no.373660  loss = 3.61040 avg_loss = 3.32217\n",
      "epoch no.4 train no.373670  loss = 2.43246 avg_loss = 3.31868\n",
      "epoch no.4 train no.373680  loss = 1.55437 avg_loss = 3.26941\n",
      "epoch no.4 train no.373690  loss = 3.22516 avg_loss = 3.24303\n",
      "epoch no.4 train no.373700  loss = 3.51220 avg_loss = 3.25403\n",
      "epoch no.4 train no.373710  loss = 3.54704 avg_loss = 3.28816\n",
      "epoch no.4 train no.373720  loss = 3.29929 avg_loss = 3.25662\n",
      "epoch no.4 train no.373730  loss = 2.49552 avg_loss = 3.20214\n",
      "epoch no.4 train no.373740  loss = 3.96792 avg_loss = 3.20557\n",
      "epoch no.4 train no.373750  loss = 2.14607 avg_loss = 3.20715\n",
      "epoch no.4 train no.373760  loss = 4.09678 avg_loss = 3.23221\n",
      "epoch no.4 train no.373770  loss = 3.94864 avg_loss = 3.24047\n",
      "epoch no.4 train no.373780  loss = 3.25504 avg_loss = 3.22773\n",
      "epoch no.4 train no.373790  loss = 2.10020 avg_loss = 3.19053\n",
      "epoch no.4 train no.373800  loss = 5.47882 avg_loss = 3.25383\n",
      "epoch no.4 train no.373810  loss = 2.99833 avg_loss = 3.26575\n",
      "epoch no.4 train no.373820  loss = 2.99820 avg_loss = 3.24606\n",
      "epoch no.4 train no.373830  loss = 3.70210 avg_loss = 3.30637\n",
      "epoch no.4 train no.373840  loss = 3.42611 avg_loss = 3.27205\n",
      "epoch no.4 train no.373850  loss = 1.97509 avg_loss = 3.20862\n",
      "epoch no.4 train no.373860  loss = 4.06516 avg_loss = 3.30509\n",
      "epoch no.4 train no.373870  loss = 3.64076 avg_loss = 3.30938\n",
      "epoch no.4 train no.373880  loss = 2.60742 avg_loss = 3.27642\n",
      "epoch no.4 train no.373890  loss = 3.79166 avg_loss = 3.27304\n",
      "epoch no.4 train no.373900  loss = 2.43790 avg_loss = 3.23051\n",
      "epoch no.4 train no.373910  loss = 2.68575 avg_loss = 3.22681\n",
      "epoch no.4 train no.373920  loss = 3.02146 avg_loss = 3.19255\n",
      "epoch no.4 train no.373930  loss = 3.33509 avg_loss = 3.20825\n",
      "epoch no.4 train no.373940  loss = 2.90055 avg_loss = 3.25701\n",
      "epoch no.4 train no.373950  loss = 5.99238 avg_loss = 3.27048\n",
      "epoch no.4 train no.373960  loss = 2.68670 avg_loss = 3.28916\n",
      "epoch no.4 train no.373970  loss = 5.23642 avg_loss = 3.30750\n",
      "epoch no.4 train no.373980  loss = 2.42680 avg_loss = 3.29758\n",
      "epoch no.4 train no.373990  loss = 3.75246 avg_loss = 3.30780\n",
      "epoch no.4 train no.374000  loss = 3.25956 avg_loss = 3.32758\n",
      "4\n",
      "to_tokens: ['▁가을', '전환', '에', '▁싶을', '을', '때', '</s>']\n",
      "기분전환하고싶을때</s>\n",
      "epoch no.4 train no.374010  loss = 3.53146 avg_loss = 3.30262\n",
      "epoch no.4 train no.374020  loss = 3.04287 avg_loss = 3.32277\n",
      "epoch no.4 train no.374030  loss = 2.89028 avg_loss = 3.30374\n",
      "epoch no.4 train no.374040  loss = 4.16766 avg_loss = 3.30081\n",
      "epoch no.4 train no.374050  loss = 3.65519 avg_loss = 3.32715\n",
      "epoch no.4 train no.374060  loss = 4.66415 avg_loss = 3.35598\n",
      "epoch no.4 train no.374070  loss = 5.12803 avg_loss = 3.40279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.374080  loss = 1.61163 avg_loss = 3.38530\n",
      "epoch no.4 train no.374090  loss = 3.00009 avg_loss = 3.44445\n",
      "epoch no.4 train no.374100  loss = 3.87962 avg_loss = 3.43374\n",
      "epoch no.4 train no.374110  loss = 3.87370 avg_loss = 3.41853\n",
      "epoch no.4 train no.374120  loss = 2.92828 avg_loss = 3.42545\n",
      "epoch no.4 train no.374130  loss = 3.47530 avg_loss = 3.44554\n",
      "epoch no.4 train no.374140  loss = 3.20828 avg_loss = 3.41688\n",
      "epoch no.4 train no.374150  loss = 3.73119 avg_loss = 3.42486\n",
      "epoch no.4 train no.374160  loss = 2.36254 avg_loss = 3.45343\n",
      "epoch no.4 train no.374170  loss = 3.20440 avg_loss = 3.46136\n",
      "epoch no.4 train no.374180  loss = 3.33564 avg_loss = 3.48450\n",
      "epoch no.4 train no.374190  loss = 1.49735 avg_loss = 3.44648\n",
      "epoch no.4 train no.374200  loss = 2.64878 avg_loss = 3.46208\n",
      "epoch no.4 train no.374210  loss = 1.80587 avg_loss = 3.44407\n",
      "epoch no.4 train no.374220  loss = 3.66078 avg_loss = 3.45146\n",
      "epoch no.4 train no.374230  loss = 2.40062 avg_loss = 3.40466\n",
      "epoch no.4 train no.374240  loss = 3.46349 avg_loss = 3.42224\n",
      "epoch no.4 train no.374250  loss = 4.45148 avg_loss = 3.42957\n",
      "epoch no.4 train no.374260  loss = 3.79904 avg_loss = 3.41325\n",
      "epoch no.4 train no.374270  loss = 3.12979 avg_loss = 3.41121\n",
      "epoch no.4 train no.374280  loss = 2.12996 avg_loss = 3.36419\n",
      "epoch no.4 train no.374290  loss = 3.34613 avg_loss = 3.40004\n",
      "epoch no.4 train no.374300  loss = 3.17328 avg_loss = 3.40904\n",
      "epoch no.4 train no.374310  loss = 2.96959 avg_loss = 3.38263\n",
      "epoch no.4 train no.374320  loss = 4.04264 avg_loss = 3.35757\n",
      "epoch no.4 train no.374330  loss = 5.48093 avg_loss = 3.36729\n",
      "epoch no.4 train no.374340  loss = 2.24446 avg_loss = 3.33340\n",
      "epoch no.4 train no.374350  loss = 4.29401 avg_loss = 3.32852\n",
      "epoch no.4 train no.374360  loss = 4.05228 avg_loss = 3.33484\n",
      "epoch no.4 train no.374370  loss = 1.60944 avg_loss = 3.31073\n",
      "epoch no.4 train no.374380  loss = 3.52304 avg_loss = 3.33461\n",
      "epoch no.4 train no.374390  loss = 3.38381 avg_loss = 3.30826\n",
      "epoch no.4 train no.374400  loss = 4.33263 avg_loss = 3.31917\n",
      "epoch no.4 train no.374410  loss = 2.27122 avg_loss = 3.30820\n",
      "epoch no.4 train no.374420  loss = 2.41421 avg_loss = 3.24795\n",
      "epoch no.4 train no.374430  loss = 3.55739 avg_loss = 3.21558\n",
      "epoch no.4 train no.374440  loss = 1.93342 avg_loss = 3.19362\n",
      "epoch no.4 train no.374450  loss = 3.91785 avg_loss = 3.22848\n",
      "epoch no.4 train no.374460  loss = 2.52546 avg_loss = 3.25608\n",
      "epoch no.4 train no.374470  loss = 2.54536 avg_loss = 3.27083\n",
      "epoch no.4 train no.374480  loss = 2.30943 avg_loss = 3.32728\n",
      "epoch no.4 train no.374490  loss = 3.42625 avg_loss = 3.33682\n",
      "epoch no.4 train no.374500  loss = 4.16814 avg_loss = 3.38821\n",
      "epoch no.4 train no.374510  loss = 2.35609 avg_loss = 3.40728\n",
      "epoch no.4 train no.374520  loss = 4.10227 avg_loss = 3.41558\n",
      "epoch no.4 train no.374530  loss = 4.10493 avg_loss = 3.42828\n",
      "epoch no.4 train no.374540  loss = 2.40791 avg_loss = 3.42196\n",
      "epoch no.4 train no.374550  loss = 2.98414 avg_loss = 3.36897\n",
      "epoch no.4 train no.374560  loss = 2.47649 avg_loss = 3.38439\n",
      "epoch no.4 train no.374570  loss = 3.02039 avg_loss = 3.37465\n",
      "epoch no.4 train no.374580  loss = 3.92746 avg_loss = 3.39470\n",
      "epoch no.4 train no.374590  loss = 3.43390 avg_loss = 3.36757\n",
      "epoch no.4 train no.374600  loss = 3.75397 avg_loss = 3.40483\n",
      "epoch no.4 train no.374610  loss = 3.41701 avg_loss = 3.42839\n",
      "epoch no.4 train no.374620  loss = 4.31542 avg_loss = 3.42701\n",
      "epoch no.4 train no.374630  loss = 3.17327 avg_loss = 3.41947\n",
      "epoch no.4 train no.374640  loss = 4.12890 avg_loss = 3.38809\n",
      "epoch no.4 train no.374650  loss = 3.51402 avg_loss = 3.34732\n",
      "epoch no.4 train no.374660  loss = 2.34179 avg_loss = 3.32847\n",
      "epoch no.4 train no.374670  loss = 4.08115 avg_loss = 3.31761\n",
      "epoch no.4 train no.374680  loss = 2.77786 avg_loss = 3.29383\n",
      "epoch no.4 train no.374690  loss = 3.20448 avg_loss = 3.28372\n",
      "epoch no.4 train no.374700  loss = 2.41861 avg_loss = 3.32047\n",
      "epoch no.4 train no.374710  loss = 2.92408 avg_loss = 3.31369\n",
      "epoch no.4 train no.374720  loss = 2.45490 avg_loss = 3.35815\n",
      "epoch no.4 train no.374730  loss = 3.52407 avg_loss = 3.38203\n",
      "epoch no.4 train no.374740  loss = 3.16535 avg_loss = 3.37590\n",
      "epoch no.4 train no.374750  loss = 3.75301 avg_loss = 3.35446\n",
      "epoch no.4 train no.374760  loss = 3.15203 avg_loss = 3.36568\n",
      "epoch no.4 train no.374770  loss = 2.17393 avg_loss = 3.36426\n",
      "epoch no.4 train no.374780  loss = 2.57481 avg_loss = 3.32917\n",
      "epoch no.4 train no.374790  loss = 3.65515 avg_loss = 3.33807\n",
      "epoch no.4 train no.374800  loss = 3.18906 avg_loss = 3.31008\n",
      "epoch no.4 train no.374810  loss = 2.64918 avg_loss = 3.31880\n",
      "epoch no.4 train no.374820  loss = 2.44013 avg_loss = 3.27657\n",
      "epoch no.4 train no.374830  loss = 2.46935 avg_loss = 3.27155\n",
      "epoch no.4 train no.374840  loss = 2.47625 avg_loss = 3.30016\n",
      "epoch no.4 train no.374850  loss = 3.67633 avg_loss = 3.30218\n",
      "epoch no.4 train no.374860  loss = 3.22222 avg_loss = 3.34037\n",
      "epoch no.4 train no.374870  loss = 3.06663 avg_loss = 3.32104\n",
      "epoch no.4 train no.374880  loss = 3.07459 avg_loss = 3.30056\n",
      "epoch no.4 train no.374890  loss = 3.02243 avg_loss = 3.30014\n",
      "epoch no.4 train no.374900  loss = 2.89629 avg_loss = 3.32758\n",
      "epoch no.4 train no.374910  loss = 3.32946 avg_loss = 3.33291\n",
      "epoch no.4 train no.374920  loss = 3.38261 avg_loss = 3.35810\n",
      "epoch no.4 train no.374930  loss = 2.52310 avg_loss = 3.39764\n",
      "epoch no.4 train no.374940  loss = 2.47502 avg_loss = 3.39408\n",
      "epoch no.4 train no.374950  loss = 5.16580 avg_loss = 3.39609\n",
      "epoch no.4 train no.374960  loss = 4.12575 avg_loss = 3.45209\n",
      "epoch no.4 train no.374970  loss = 4.16407 avg_loss = 3.43479\n",
      "epoch no.4 train no.374980  loss = 2.36918 avg_loss = 3.39631\n",
      "epoch no.4 train no.374990  loss = 3.45898 avg_loss = 3.38472\n",
      "epoch no.4 train no.375000  loss = 5.24607 avg_loss = 3.42580\n",
      "4\n",
      "to_tokens: ['▁가을', '전환', '이', '▁위한', '▁감각적인', '힙', '</s>']\n",
      "기분전환을 위한 감성음악</s>\n",
      "epoch no.4 train no.375010  loss = 3.43227 avg_loss = 3.42399\n",
      "epoch no.4 train no.375020  loss = 2.89431 avg_loss = 3.42109\n",
      "epoch no.4 train no.375030  loss = 4.67113 avg_loss = 3.44690\n",
      "epoch no.4 train no.375040  loss = 3.82221 avg_loss = 3.43924\n",
      "epoch no.4 train no.375050  loss = 4.66807 avg_loss = 3.43383\n",
      "epoch no.4 train no.375060  loss = 2.75347 avg_loss = 3.41946\n",
      "epoch no.4 train no.375070  loss = 3.72353 avg_loss = 3.41402\n",
      "epoch no.4 train no.375080  loss = 2.21689 avg_loss = 3.36058\n",
      "epoch no.4 train no.375090  loss = 2.55936 avg_loss = 3.31754\n",
      "epoch no.4 train no.375100  loss = 3.53179 avg_loss = 3.35520\n",
      "epoch no.4 train no.375110  loss = 3.08209 avg_loss = 3.30735\n",
      "epoch no.4 train no.375120  loss = 2.16633 avg_loss = 3.34112\n",
      "epoch no.4 train no.375130  loss = 3.33405 avg_loss = 3.36141\n",
      "epoch no.4 train no.375140  loss = 4.64847 avg_loss = 3.42227\n",
      "epoch no.4 train no.375150  loss = 2.92846 avg_loss = 3.42753\n",
      "epoch no.4 train no.375160  loss = 4.85014 avg_loss = 3.43917\n",
      "epoch no.4 train no.375170  loss = 3.38757 avg_loss = 3.41354\n",
      "epoch no.4 train no.375180  loss = 3.01867 avg_loss = 3.38850\n",
      "epoch no.4 train no.375190  loss = 3.58713 avg_loss = 3.38634\n",
      "epoch no.4 train no.375200  loss = 2.13888 avg_loss = 3.35502\n",
      "epoch no.4 train no.375210  loss = 4.38817 avg_loss = 3.32111\n",
      "epoch no.4 train no.375220  loss = 2.87626 avg_loss = 3.28036\n",
      "epoch no.4 train no.375230  loss = 2.63113 avg_loss = 3.27191\n",
      "epoch no.4 train no.375240  loss = 1.99004 avg_loss = 3.23124\n",
      "epoch no.4 train no.375250  loss = 4.40520 avg_loss = 3.30497\n",
      "epoch no.4 train no.375260  loss = 3.51696 avg_loss = 3.33670\n",
      "epoch no.4 train no.375270  loss = 2.97399 avg_loss = 3.33014\n",
      "epoch no.4 train no.375280  loss = 2.26879 avg_loss = 3.34211\n",
      "epoch no.4 train no.375290  loss = 6.28799 avg_loss = 3.35093\n",
      "epoch no.4 train no.375300  loss = 3.91542 avg_loss = 3.37454\n",
      "epoch no.4 train no.375310  loss = 2.61046 avg_loss = 3.37187\n",
      "epoch no.4 train no.375320  loss = 3.39268 avg_loss = 3.40964\n",
      "epoch no.4 train no.375330  loss = 2.99747 avg_loss = 3.40026\n",
      "epoch no.4 train no.375340  loss = 4.35740 avg_loss = 3.40714\n",
      "epoch no.4 train no.375350  loss = 4.75606 avg_loss = 3.43054\n",
      "epoch no.4 train no.375360  loss = 2.59910 avg_loss = 3.43076\n",
      "epoch no.4 train no.375370  loss = 3.32565 avg_loss = 3.45106\n",
      "epoch no.4 train no.375380  loss = 5.43412 avg_loss = 3.44317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.375390  loss = 2.55007 avg_loss = 3.46732\n",
      "epoch no.4 train no.375400  loss = 3.85621 avg_loss = 3.43591\n",
      "epoch no.4 train no.375410  loss = 3.10312 avg_loss = 3.40506\n",
      "epoch no.4 train no.375420  loss = 1.79298 avg_loss = 3.40906\n",
      "epoch no.4 train no.375430  loss = 3.83107 avg_loss = 3.42410\n",
      "epoch no.4 train no.375440  loss = 3.15846 avg_loss = 3.42817\n",
      "epoch no.4 train no.375450  loss = 2.26171 avg_loss = 3.39093\n",
      "epoch no.4 train no.375460  loss = 4.40503 avg_loss = 3.36518\n",
      "epoch no.4 train no.375470  loss = 3.13626 avg_loss = 3.35704\n",
      "epoch no.4 train no.375480  loss = 2.39631 avg_loss = 3.36913\n",
      "epoch no.4 train no.375490  loss = 3.08327 avg_loss = 3.36258\n",
      "epoch no.4 train no.375500  loss = 3.73565 avg_loss = 3.33096\n",
      "epoch no.4 train no.375510  loss = 4.04845 avg_loss = 3.37099\n",
      "epoch no.4 train no.375520  loss = 4.30027 avg_loss = 3.37287\n",
      "epoch no.4 train no.375530  loss = 2.76910 avg_loss = 3.38256\n",
      "epoch no.4 train no.375540  loss = 3.24372 avg_loss = 3.36256\n",
      "epoch no.4 train no.375550  loss = 2.74191 avg_loss = 3.33132\n",
      "epoch no.4 train no.375560  loss = 2.95509 avg_loss = 3.30411\n",
      "epoch no.4 train no.375570  loss = 2.29705 avg_loss = 3.34558\n",
      "epoch no.4 train no.375580  loss = 2.70371 avg_loss = 3.34765\n",
      "epoch no.4 train no.375590  loss = 2.90088 avg_loss = 3.34826\n",
      "epoch no.4 train no.375600  loss = 5.85788 avg_loss = 3.37806\n",
      "epoch no.4 train no.375610  loss = 2.60584 avg_loss = 3.37900\n",
      "epoch no.4 train no.375620  loss = 1.79682 avg_loss = 3.36584\n",
      "epoch no.4 train no.375630  loss = 3.24254 avg_loss = 3.39814\n",
      "epoch no.4 train no.375640  loss = 4.34940 avg_loss = 3.42027\n",
      "epoch no.4 train no.375650  loss = 3.07018 avg_loss = 3.45665\n",
      "epoch no.4 train no.375660  loss = 4.40292 avg_loss = 3.42487\n",
      "epoch no.4 train no.375670  loss = 3.58040 avg_loss = 3.39416\n",
      "epoch no.4 train no.375680  loss = 2.75161 avg_loss = 3.38708\n",
      "epoch no.4 train no.375690  loss = 2.03029 avg_loss = 3.41503\n",
      "epoch no.4 train no.375700  loss = 3.43625 avg_loss = 3.42879\n",
      "epoch no.4 train no.375710  loss = 2.94330 avg_loss = 3.42859\n",
      "epoch no.4 train no.375720  loss = 3.73754 avg_loss = 3.47158\n",
      "epoch no.4 train no.375730  loss = 3.13999 avg_loss = 3.44235\n",
      "epoch no.4 train no.375740  loss = 4.40121 avg_loss = 3.44002\n",
      "epoch no.4 train no.375750  loss = 3.26539 avg_loss = 3.44612\n",
      "epoch no.4 train no.375760  loss = 2.59901 avg_loss = 3.41989\n",
      "epoch no.4 train no.375770  loss = 4.08267 avg_loss = 3.38329\n",
      "epoch no.4 train no.375780  loss = 3.80676 avg_loss = 3.39226\n",
      "epoch no.4 train no.375790  loss = 2.29777 avg_loss = 3.40437\n",
      "epoch no.4 train no.375800  loss = 3.66092 avg_loss = 3.42249\n",
      "epoch no.4 train no.375810  loss = 3.43399 avg_loss = 3.42434\n",
      "epoch no.4 train no.375820  loss = 3.18567 avg_loss = 3.43272\n",
      "epoch no.4 train no.375830  loss = 5.18315 avg_loss = 3.45704\n",
      "epoch no.4 train no.375840  loss = 2.63663 avg_loss = 3.44825\n",
      "epoch no.4 train no.375850  loss = 3.35995 avg_loss = 3.45160\n",
      "epoch no.4 train no.375860  loss = 1.73959 avg_loss = 3.42755\n",
      "epoch no.4 train no.375870  loss = 3.91942 avg_loss = 3.45146\n",
      "epoch no.4 train no.375880  loss = 2.91084 avg_loss = 3.43687\n",
      "epoch no.4 train no.375890  loss = 3.79304 avg_loss = 3.43090\n",
      "epoch no.4 train no.375900  loss = 3.52208 avg_loss = 3.43629\n",
      "epoch no.4 train no.375910  loss = 4.14249 avg_loss = 3.42761\n",
      "epoch no.4 train no.375920  loss = 3.27621 avg_loss = 3.39357\n",
      "epoch no.4 train no.375930  loss = 2.67297 avg_loss = 3.36077\n",
      "epoch no.4 train no.375940  loss = 3.11532 avg_loss = 3.34091\n",
      "epoch no.4 train no.375950  loss = 3.30126 avg_loss = 3.35891\n",
      "epoch no.4 train no.375960  loss = 3.49310 avg_loss = 3.35327\n",
      "epoch no.4 train no.375970  loss = 3.77238 avg_loss = 3.35248\n",
      "epoch no.4 train no.375980  loss = 3.80016 avg_loss = 3.31640\n",
      "epoch no.4 train no.375990  loss = 3.21641 avg_loss = 3.32283\n",
      "epoch no.4 train no.376000  loss = 5.42855 avg_loss = 3.33849\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '에', '▁드라이브', '▁뮤직', '</s>']\n",
      "기분전환 신나는 드라이브 뮤직</s>\n",
      "epoch no.4 train no.376010  loss = 3.94054 avg_loss = 3.33527\n",
      "epoch no.4 train no.376020  loss = 2.98152 avg_loss = 3.34428\n",
      "epoch no.4 train no.376030  loss = 2.72332 avg_loss = 3.33895\n",
      "epoch no.4 train no.376040  loss = 3.22587 avg_loss = 3.35140\n",
      "epoch no.4 train no.376050  loss = 5.11824 avg_loss = 3.39266\n",
      "epoch no.4 train no.376060  loss = 4.00268 avg_loss = 3.39728\n",
      "epoch no.4 train no.376070  loss = 3.51221 avg_loss = 3.39406\n",
      "epoch no.4 train no.376080  loss = 3.86942 avg_loss = 3.45301\n",
      "epoch no.4 train no.376090  loss = 4.41088 avg_loss = 3.39799\n",
      "epoch no.4 train no.376100  loss = 1.22761 avg_loss = 3.34117\n",
      "epoch no.4 train no.376110  loss = 3.30474 avg_loss = 3.28293\n",
      "epoch no.4 train no.376120  loss = 2.46889 avg_loss = 3.25964\n",
      "epoch no.4 train no.376130  loss = 3.25043 avg_loss = 3.24123\n",
      "epoch no.4 train no.376140  loss = 3.22697 avg_loss = 3.27027\n",
      "epoch no.4 train no.376150  loss = 3.16223 avg_loss = 3.32807\n",
      "epoch no.4 train no.376160  loss = 2.82291 avg_loss = 3.37393\n",
      "epoch no.4 train no.376170  loss = 4.01719 avg_loss = 3.35601\n",
      "epoch no.4 train no.376180  loss = 3.04459 avg_loss = 3.37151\n",
      "epoch no.4 train no.376190  loss = 3.86763 avg_loss = 3.36882\n",
      "epoch no.4 train no.376200  loss = 2.50425 avg_loss = 3.37849\n",
      "epoch no.4 train no.376210  loss = 3.80604 avg_loss = 3.38514\n",
      "epoch no.4 train no.376220  loss = 4.25118 avg_loss = 3.35101\n",
      "epoch no.4 train no.376230  loss = 3.78580 avg_loss = 3.31701\n",
      "epoch no.4 train no.376240  loss = 3.26113 avg_loss = 3.30388\n",
      "epoch no.4 train no.376250  loss = 3.23586 avg_loss = 3.30394\n",
      "epoch no.4 train no.376260  loss = 3.29197 avg_loss = 3.31736\n",
      "epoch no.4 train no.376270  loss = 5.32372 avg_loss = 3.37567\n",
      "epoch no.4 train no.376280  loss = 3.83369 avg_loss = 3.41347\n",
      "epoch no.4 train no.376290  loss = 4.31384 avg_loss = 3.45244\n",
      "epoch no.4 train no.376300  loss = 4.02614 avg_loss = 3.41701\n",
      "epoch no.4 train no.376310  loss = 2.93602 avg_loss = 3.41653\n",
      "epoch no.4 train no.376320  loss = 4.96450 avg_loss = 3.39956\n",
      "epoch no.4 train no.376330  loss = 7.21345 avg_loss = 3.42069\n",
      "epoch no.4 train no.376340  loss = 2.25799 avg_loss = 3.41121\n",
      "epoch no.4 train no.376350  loss = 4.70148 avg_loss = 3.41404\n",
      "epoch no.4 train no.376360  loss = 2.64625 avg_loss = 3.41340\n",
      "epoch no.4 train no.376370  loss = 3.35944 avg_loss = 3.39184\n",
      "epoch no.4 train no.376380  loss = 3.47625 avg_loss = 3.37669\n",
      "epoch no.4 train no.376390  loss = 3.02114 avg_loss = 3.36357\n",
      "epoch no.4 train no.376400  loss = 2.36575 avg_loss = 3.37101\n",
      "epoch no.4 train no.376410  loss = 2.81173 avg_loss = 3.39039\n",
      "epoch no.4 train no.376420  loss = 3.99783 avg_loss = 3.39336\n",
      "epoch no.4 train no.376430  loss = 2.06216 avg_loss = 3.33107\n",
      "epoch no.4 train no.376440  loss = 3.61185 avg_loss = 3.30662\n",
      "epoch no.4 train no.376450  loss = 5.89061 avg_loss = 3.33265\n",
      "epoch no.4 train no.376460  loss = 3.98326 avg_loss = 3.34656\n",
      "epoch no.4 train no.376470  loss = 4.79255 avg_loss = 3.37273\n",
      "epoch no.4 train no.376480  loss = 4.63839 avg_loss = 3.38835\n",
      "epoch no.4 train no.376490  loss = 3.32059 avg_loss = 3.40953\n",
      "epoch no.4 train no.376500  loss = 2.81250 avg_loss = 3.38986\n",
      "epoch no.4 train no.376510  loss = 3.34931 avg_loss = 3.42856\n",
      "epoch no.4 train no.376520  loss = 3.35721 avg_loss = 3.42845\n",
      "epoch no.4 train no.376530  loss = 3.49059 avg_loss = 3.42110\n",
      "epoch no.4 train no.376540  loss = 3.88470 avg_loss = 3.39263\n",
      "epoch no.4 train no.376550  loss = 2.88261 avg_loss = 3.37577\n",
      "epoch no.4 train no.376560  loss = 4.15621 avg_loss = 3.40226\n",
      "epoch no.4 train no.376570  loss = 2.18614 avg_loss = 3.38256\n",
      "epoch no.4 train no.376580  loss = 3.41081 avg_loss = 3.35673\n",
      "epoch no.4 train no.376590  loss = 3.72377 avg_loss = 3.32552\n",
      "epoch no.4 train no.376600  loss = 4.17260 avg_loss = 3.32334\n",
      "epoch no.4 train no.376610  loss = 3.86651 avg_loss = 3.32109\n",
      "epoch no.4 train no.376620  loss = 4.33834 avg_loss = 3.29014\n",
      "epoch no.4 train no.376630  loss = 2.21317 avg_loss = 3.28877\n",
      "epoch no.4 train no.376640  loss = 3.80634 avg_loss = 3.30330\n",
      "epoch no.4 train no.376650  loss = 2.33552 avg_loss = 3.32652\n",
      "epoch no.4 train no.376660  loss = 2.29909 avg_loss = 3.29436\n",
      "epoch no.4 train no.376670  loss = 2.96184 avg_loss = 3.30464\n",
      "epoch no.4 train no.376680  loss = 4.09910 avg_loss = 3.33721\n",
      "epoch no.4 train no.376690  loss = 2.95161 avg_loss = 3.31993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.376700  loss = 3.14033 avg_loss = 3.31271\n",
      "epoch no.4 train no.376710  loss = 3.28541 avg_loss = 3.32645\n",
      "epoch no.4 train no.376720  loss = 2.57704 avg_loss = 3.32723\n",
      "epoch no.4 train no.376730  loss = 2.67223 avg_loss = 3.28851\n",
      "epoch no.4 train no.376740  loss = 3.38003 avg_loss = 3.31944\n",
      "epoch no.4 train no.376750  loss = 3.72813 avg_loss = 3.32291\n",
      "epoch no.4 train no.376760  loss = 4.21882 avg_loss = 3.30153\n",
      "epoch no.4 train no.376770  loss = 2.62013 avg_loss = 3.30242\n",
      "epoch no.4 train no.376780  loss = 2.97322 avg_loss = 3.30156\n",
      "epoch no.4 train no.376790  loss = 2.46650 avg_loss = 3.27608\n",
      "epoch no.4 train no.376800  loss = 3.22083 avg_loss = 3.28332\n",
      "epoch no.4 train no.376810  loss = 3.65792 avg_loss = 3.26489\n",
      "epoch no.4 train no.376820  loss = 4.13677 avg_loss = 3.29494\n",
      "epoch no.4 train no.376830  loss = 2.72184 avg_loss = 3.25915\n",
      "epoch no.4 train no.376840  loss = 3.63460 avg_loss = 3.23131\n",
      "epoch no.4 train no.376850  loss = 3.78274 avg_loss = 3.26120\n",
      "epoch no.4 train no.376860  loss = 3.31786 avg_loss = 3.23781\n",
      "epoch no.4 train no.376870  loss = 3.95235 avg_loss = 3.24894\n",
      "epoch no.4 train no.376880  loss = 5.97154 avg_loss = 3.27153\n",
      "epoch no.4 train no.376890  loss = 2.27529 avg_loss = 3.28140\n",
      "epoch no.4 train no.376900  loss = 2.39953 avg_loss = 3.29254\n",
      "epoch no.4 train no.376910  loss = 2.84172 avg_loss = 3.32723\n",
      "epoch no.4 train no.376920  loss = 3.09635 avg_loss = 3.36912\n",
      "epoch no.4 train no.376930  loss = 3.29668 avg_loss = 3.41211\n",
      "epoch no.4 train no.376940  loss = 3.28533 avg_loss = 3.39407\n",
      "epoch no.4 train no.376950  loss = 2.24231 avg_loss = 3.39873\n",
      "epoch no.4 train no.376960  loss = 3.43593 avg_loss = 3.37024\n",
      "epoch no.4 train no.376970  loss = 5.41267 avg_loss = 3.41516\n",
      "epoch no.4 train no.376980  loss = 3.71869 avg_loss = 3.43443\n",
      "epoch no.4 train no.376990  loss = 3.92962 avg_loss = 3.42588\n",
      "epoch no.4 train no.377000  loss = 4.26466 avg_loss = 3.45778\n",
      "1\n",
      "to_tokens: ['▁가을', '전환', '이', '</s>']\n",
      "기분전환용</s>\n",
      "epoch no.4 train no.377010  loss = 2.46736 avg_loss = 3.42761\n",
      "epoch no.4 train no.377020  loss = 3.28186 avg_loss = 3.40463\n",
      "epoch no.4 train no.377030  loss = 2.52039 avg_loss = 3.36373\n",
      "epoch no.4 train no.377040  loss = 3.60030 avg_loss = 3.40295\n",
      "epoch no.4 train no.377050  loss = 2.70613 avg_loss = 3.39111\n",
      "epoch no.4 train no.377060  loss = 2.16442 avg_loss = 3.38226\n",
      "epoch no.4 train no.377070  loss = 5.30227 avg_loss = 3.34598\n",
      "epoch no.4 train no.377080  loss = 2.63376 avg_loss = 3.35343\n",
      "epoch no.4 train no.377090  loss = 3.97361 avg_loss = 3.37635\n",
      "epoch no.4 train no.377100  loss = 2.60536 avg_loss = 3.35825\n",
      "epoch no.4 train no.377110  loss = 3.97828 avg_loss = 3.36831\n",
      "epoch no.4 train no.377120  loss = 2.15559 avg_loss = 3.34822\n",
      "epoch no.4 train no.377130  loss = 3.32523 avg_loss = 3.34180\n",
      "epoch no.4 train no.377140  loss = 3.40318 avg_loss = 3.34620\n",
      "epoch no.4 train no.377150  loss = 2.86101 avg_loss = 3.33325\n",
      "epoch no.4 train no.377160  loss = 4.93163 avg_loss = 3.40603\n",
      "epoch no.4 train no.377170  loss = 3.41967 avg_loss = 3.40123\n",
      "epoch no.4 train no.377180  loss = 3.56534 avg_loss = 3.38336\n",
      "epoch no.4 train no.377190  loss = 4.07986 avg_loss = 3.38653\n",
      "epoch no.4 train no.377200  loss = 4.81371 avg_loss = 3.37993\n",
      "epoch no.4 train no.377210  loss = 2.63352 avg_loss = 3.36503\n",
      "epoch no.4 train no.377220  loss = 3.41716 avg_loss = 3.34376\n",
      "epoch no.4 train no.377230  loss = 2.90796 avg_loss = 3.33186\n",
      "epoch no.4 train no.377240  loss = 4.22155 avg_loss = 3.36955\n",
      "epoch no.4 train no.377250  loss = 3.24386 avg_loss = 3.31745\n",
      "epoch no.4 train no.377260  loss = 2.61102 avg_loss = 3.34580\n",
      "epoch no.4 train no.377270  loss = 3.69785 avg_loss = 3.35615\n",
      "epoch no.4 train no.377280  loss = 2.66424 avg_loss = 3.34857\n",
      "epoch no.4 train no.377290  loss = 3.67875 avg_loss = 3.35972\n",
      "epoch no.4 train no.377300  loss = 3.50822 avg_loss = 3.37725\n",
      "epoch no.4 train no.377310  loss = 3.08548 avg_loss = 3.36437\n",
      "epoch no.4 train no.377320  loss = 3.54462 avg_loss = 3.33277\n",
      "epoch no.4 train no.377330  loss = 2.23819 avg_loss = 3.32042\n",
      "epoch no.4 train no.377340  loss = 2.69208 avg_loss = 3.30668\n",
      "epoch no.4 train no.377350  loss = 4.58553 avg_loss = 3.32297\n",
      "epoch no.4 train no.377360  loss = 3.78889 avg_loss = 3.39681\n",
      "epoch no.4 train no.377370  loss = 3.22966 avg_loss = 3.37048\n",
      "epoch no.4 train no.377380  loss = 3.49461 avg_loss = 3.36230\n",
      "epoch no.4 train no.377390  loss = 2.09571 avg_loss = 3.31721\n",
      "epoch no.4 train no.377400  loss = 1.62951 avg_loss = 3.30023\n",
      "epoch no.4 train no.377410  loss = 4.24787 avg_loss = 3.33141\n",
      "epoch no.4 train no.377420  loss = 3.01402 avg_loss = 3.36854\n",
      "epoch no.4 train no.377430  loss = 3.28859 avg_loss = 3.38547\n",
      "epoch no.4 train no.377440  loss = 2.60817 avg_loss = 3.39040\n",
      "epoch no.4 train no.377450  loss = 2.91058 avg_loss = 3.37887\n",
      "epoch no.4 train no.377460  loss = 4.20041 avg_loss = 3.36334\n",
      "epoch no.4 train no.377470  loss = 3.65183 avg_loss = 3.32078\n",
      "epoch no.4 train no.377480  loss = 3.28561 avg_loss = 3.32016\n",
      "epoch no.4 train no.377490  loss = 3.49573 avg_loss = 3.35032\n",
      "epoch no.4 train no.377500  loss = 3.29378 avg_loss = 3.34170\n",
      "epoch no.4 train no.377510  loss = 2.39590 avg_loss = 3.30125\n",
      "epoch no.4 train no.377520  loss = 3.47915 avg_loss = 3.30475\n",
      "epoch no.4 train no.377530  loss = 3.04028 avg_loss = 3.35081\n",
      "epoch no.4 train no.377540  loss = 3.45407 avg_loss = 3.35912\n",
      "epoch no.4 train no.377550  loss = 2.87928 avg_loss = 3.34338\n",
      "epoch no.4 train no.377560  loss = 3.57890 avg_loss = 3.33020\n",
      "epoch no.4 train no.377570  loss = 3.20351 avg_loss = 3.31168\n",
      "epoch no.4 train no.377580  loss = 2.39548 avg_loss = 3.33645\n",
      "epoch no.4 train no.377590  loss = 3.41371 avg_loss = 3.33751\n",
      "epoch no.4 train no.377600  loss = 4.00270 avg_loss = 3.35234\n",
      "epoch no.4 train no.377610  loss = 5.99702 avg_loss = 3.33966\n",
      "epoch no.4 train no.377620  loss = 3.50717 avg_loss = 3.32059\n",
      "epoch no.4 train no.377630  loss = 3.56183 avg_loss = 3.35589\n",
      "epoch no.4 train no.377640  loss = 3.21566 avg_loss = 3.29222\n",
      "epoch no.4 train no.377650  loss = 3.74788 avg_loss = 3.29933\n",
      "epoch no.4 train no.377660  loss = 3.47072 avg_loss = 3.32897\n",
      "epoch no.4 train no.377670  loss = 2.84212 avg_loss = 3.31387\n",
      "epoch no.4 train no.377680  loss = 2.01052 avg_loss = 3.30376\n",
      "epoch no.4 train no.377690  loss = 3.26507 avg_loss = 3.31628\n",
      "epoch no.4 train no.377700  loss = 3.30184 avg_loss = 3.27076\n",
      "epoch no.4 train no.377710  loss = 3.53357 avg_loss = 3.33397\n",
      "epoch no.4 train no.377720  loss = 3.23857 avg_loss = 3.34140\n",
      "epoch no.4 train no.377730  loss = 2.55047 avg_loss = 3.35226\n",
      "epoch no.4 train no.377740  loss = 3.68288 avg_loss = 3.34414\n",
      "epoch no.4 train no.377750  loss = 2.88305 avg_loss = 3.36969\n",
      "epoch no.4 train no.377760  loss = 3.49857 avg_loss = 3.35337\n",
      "epoch no.4 train no.377770  loss = 2.46181 avg_loss = 3.31010\n",
      "epoch no.4 train no.377780  loss = 4.65088 avg_loss = 3.33869\n",
      "epoch no.4 train no.377790  loss = 3.18271 avg_loss = 3.34436\n",
      "epoch no.4 train no.377800  loss = 4.80560 avg_loss = 3.33067\n",
      "epoch no.4 train no.377810  loss = 2.90557 avg_loss = 3.37074\n",
      "epoch no.4 train no.377820  loss = 3.80019 avg_loss = 3.38183\n",
      "epoch no.4 train no.377830  loss = 1.84808 avg_loss = 3.35169\n",
      "epoch no.4 train no.377840  loss = 4.97206 avg_loss = 3.36607\n",
      "epoch no.4 train no.377850  loss = 3.16238 avg_loss = 3.35942\n",
      "epoch no.4 train no.377860  loss = 2.85672 avg_loss = 3.36020\n",
      "epoch no.4 train no.377870  loss = 3.02097 avg_loss = 3.35505\n",
      "epoch no.4 train no.377880  loss = 3.45729 avg_loss = 3.35811\n",
      "epoch no.4 train no.377890  loss = 4.30259 avg_loss = 3.35986\n",
      "epoch no.4 train no.377900  loss = 2.44908 avg_loss = 3.29957\n",
      "epoch no.4 train no.377910  loss = 2.30033 avg_loss = 3.24597\n",
      "epoch no.4 train no.377920  loss = 1.92621 avg_loss = 3.24588\n",
      "epoch no.4 train no.377930  loss = 3.48147 avg_loss = 3.27637\n",
      "epoch no.4 train no.377940  loss = 3.41339 avg_loss = 3.27920\n",
      "epoch no.4 train no.377950  loss = 3.11929 avg_loss = 3.23046\n",
      "epoch no.4 train no.377960  loss = 3.84530 avg_loss = 3.27471\n",
      "epoch no.4 train no.377970  loss = 3.15840 avg_loss = 3.31427\n",
      "epoch no.4 train no.377980  loss = 2.68577 avg_loss = 3.32385\n",
      "epoch no.4 train no.377990  loss = 2.74615 avg_loss = 3.30124\n",
      "epoch no.4 train no.378000  loss = 3.38687 avg_loss = 3.28168\n",
      "2\n",
      "to_tokens: ['▁가을', '전환', '을', '▁노래', '음악']\n",
      "기분전환 신나는 락</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.378010  loss = 3.20564 avg_loss = 3.26373\n",
      "epoch no.4 train no.378020  loss = 3.36420 avg_loss = 3.25369\n",
      "epoch no.4 train no.378030  loss = 3.42499 avg_loss = 3.26044\n",
      "epoch no.4 train no.378040  loss = 2.71001 avg_loss = 3.29828\n",
      "epoch no.4 train no.378050  loss = 3.14114 avg_loss = 3.31984\n",
      "epoch no.4 train no.378060  loss = 1.02415 avg_loss = 3.28357\n",
      "epoch no.4 train no.378070  loss = 4.97210 avg_loss = 3.24360\n",
      "epoch no.4 train no.378080  loss = 3.93535 avg_loss = 3.22162\n",
      "epoch no.4 train no.378090  loss = 2.88749 avg_loss = 3.22483\n",
      "epoch no.4 train no.378100  loss = 2.87346 avg_loss = 3.26270\n",
      "epoch no.4 train no.378110  loss = 2.08650 avg_loss = 3.21822\n",
      "epoch no.4 train no.378120  loss = 1.92578 avg_loss = 3.23002\n",
      "epoch no.4 train no.378130  loss = 6.95734 avg_loss = 3.32564\n",
      "epoch no.4 train no.378140  loss = 2.97380 avg_loss = 3.34839\n",
      "epoch no.4 train no.378150  loss = 3.43941 avg_loss = 3.35721\n",
      "epoch no.4 train no.378160  loss = 3.65248 avg_loss = 3.35941\n",
      "epoch no.4 train no.378170  loss = 5.10446 avg_loss = 3.37739\n",
      "epoch no.4 train no.378180  loss = 4.40007 avg_loss = 3.36184\n",
      "epoch no.4 train no.378190  loss = 2.63033 avg_loss = 3.30115\n",
      "epoch no.4 train no.378200  loss = 2.95452 avg_loss = 3.30617\n",
      "epoch no.4 train no.378210  loss = 3.60023 avg_loss = 3.28223\n",
      "epoch no.4 train no.378220  loss = 3.41251 avg_loss = 3.35435\n",
      "epoch no.4 train no.378230  loss = 1.08742 avg_loss = 3.32896\n",
      "epoch no.4 train no.378240  loss = 5.44866 avg_loss = 3.38590\n",
      "epoch no.4 train no.378250  loss = 2.53472 avg_loss = 3.36062\n",
      "epoch no.4 train no.378260  loss = 3.22477 avg_loss = 3.34313\n",
      "epoch no.4 train no.378270  loss = 2.39423 avg_loss = 3.31994\n",
      "epoch no.4 train no.378280  loss = 3.49902 avg_loss = 3.32322\n",
      "epoch no.4 train no.378290  loss = 3.09688 avg_loss = 3.31463\n",
      "epoch no.4 train no.378300  loss = 5.27205 avg_loss = 3.36339\n",
      "epoch no.4 train no.378310  loss = 3.14918 avg_loss = 3.34280\n",
      "epoch no.4 train no.378320  loss = 3.40980 avg_loss = 3.31633\n",
      "epoch no.4 train no.378330  loss = 3.07881 avg_loss = 3.38098\n",
      "epoch no.4 train no.378340  loss = 5.99331 avg_loss = 3.40866\n",
      "epoch no.4 train no.378350  loss = 3.71893 avg_loss = 3.40376\n",
      "epoch no.4 train no.378360  loss = 3.40064 avg_loss = 3.41396\n",
      "epoch no.4 train no.378370  loss = 3.30382 avg_loss = 3.37745\n",
      "epoch no.4 train no.378380  loss = 3.32983 avg_loss = 3.37327\n",
      "epoch no.4 train no.378390  loss = 3.40520 avg_loss = 3.38045\n",
      "epoch no.4 train no.378400  loss = 2.69034 avg_loss = 3.37858\n",
      "epoch no.4 train no.378410  loss = 2.98533 avg_loss = 3.34894\n",
      "epoch no.4 train no.378420  loss = 2.30462 avg_loss = 3.34807\n",
      "epoch no.4 train no.378430  loss = 3.36246 avg_loss = 3.35562\n",
      "epoch no.4 train no.378440  loss = 4.44368 avg_loss = 3.33172\n",
      "epoch no.4 train no.378450  loss = 5.47116 avg_loss = 3.39732\n",
      "epoch no.4 train no.378460  loss = 2.46788 avg_loss = 3.37026\n",
      "epoch no.4 train no.378470  loss = 3.17457 avg_loss = 3.37082\n",
      "epoch no.4 train no.378480  loss = 2.80765 avg_loss = 3.39792\n",
      "epoch no.4 train no.378490  loss = 3.29385 avg_loss = 3.37497\n",
      "epoch no.4 train no.378500  loss = 3.35782 avg_loss = 3.41702\n",
      "epoch no.4 train no.378510  loss = 5.11539 avg_loss = 3.43035\n",
      "epoch no.4 train no.378520  loss = 2.40296 avg_loss = 3.41957\n",
      "epoch no.4 train no.378530  loss = 2.73636 avg_loss = 3.42859\n",
      "epoch no.4 train no.378540  loss = 2.97797 avg_loss = 3.41619\n",
      "epoch no.4 train no.378550  loss = 1.67688 avg_loss = 3.37081\n",
      "epoch no.4 train no.378560  loss = 2.59558 avg_loss = 3.34465\n",
      "epoch no.4 train no.378570  loss = 3.27970 avg_loss = 3.37227\n",
      "epoch no.4 train no.378580  loss = 3.34677 avg_loss = 3.35848\n",
      "epoch no.4 train no.378590  loss = 3.66613 avg_loss = 3.36087\n",
      "epoch no.4 train no.378600  loss = 3.75364 avg_loss = 3.36126\n",
      "epoch no.4 train no.378610  loss = 3.50698 avg_loss = 3.37111\n",
      "epoch no.4 train no.378620  loss = 2.74562 avg_loss = 3.38826\n",
      "epoch no.4 train no.378630  loss = 2.80415 avg_loss = 3.36556\n",
      "epoch no.4 train no.378640  loss = 3.52322 avg_loss = 3.36027\n",
      "epoch no.4 train no.378650  loss = 4.14876 avg_loss = 3.40379\n",
      "epoch no.4 train no.378660  loss = 2.88904 avg_loss = 3.38602\n",
      "epoch no.4 train no.378670  loss = 2.87459 avg_loss = 3.35328\n",
      "epoch no.4 train no.378680  loss = 4.03319 avg_loss = 3.34936\n",
      "epoch no.4 train no.378690  loss = 5.67559 avg_loss = 3.32787\n",
      "epoch no.4 train no.378700  loss = 3.61630 avg_loss = 3.31673\n",
      "epoch no.4 train no.378710  loss = 5.43607 avg_loss = 3.34350\n",
      "epoch no.4 train no.378720  loss = 4.32684 avg_loss = 3.33950\n",
      "epoch no.4 train no.378730  loss = 3.50166 avg_loss = 3.33251\n",
      "epoch no.4 train no.378740  loss = 3.11855 avg_loss = 3.28511\n",
      "epoch no.4 train no.378750  loss = 3.60932 avg_loss = 3.28360\n",
      "epoch no.4 train no.378760  loss = 3.50644 avg_loss = 3.26191\n",
      "epoch no.4 train no.378770  loss = 4.13854 avg_loss = 3.27772\n",
      "epoch no.4 train no.378780  loss = 4.06123 avg_loss = 3.30338\n",
      "epoch no.4 train no.378790  loss = 6.05890 avg_loss = 3.28636\n",
      "epoch no.4 train no.378800  loss = 4.66322 avg_loss = 3.26315\n",
      "epoch no.4 train no.378810  loss = 2.87830 avg_loss = 3.28484\n",
      "epoch no.4 train no.378820  loss = 2.22905 avg_loss = 3.32753\n",
      "epoch no.4 train no.378830  loss = 5.36204 avg_loss = 3.32146\n",
      "epoch no.4 train no.378840  loss = 3.38837 avg_loss = 3.29803\n",
      "epoch no.4 train no.378850  loss = 3.57896 avg_loss = 3.26994\n",
      "epoch no.4 train no.378860  loss = 3.86299 avg_loss = 3.27695\n",
      "epoch no.4 train no.378870  loss = 3.96913 avg_loss = 3.29196\n",
      "epoch no.4 train no.378880  loss = 3.90970 avg_loss = 3.24724\n",
      "epoch no.4 train no.378890  loss = 5.99695 avg_loss = 3.24539\n",
      "epoch no.4 train no.378900  loss = 2.70818 avg_loss = 3.26045\n",
      "epoch no.4 train no.378910  loss = 2.99091 avg_loss = 3.25475\n",
      "epoch no.4 train no.378920  loss = 2.20386 avg_loss = 3.26030\n",
      "epoch no.4 train no.378930  loss = 2.16089 avg_loss = 3.27846\n",
      "epoch no.4 train no.378940  loss = 1.82038 avg_loss = 3.27762\n",
      "epoch no.4 train no.378950  loss = 4.59876 avg_loss = 3.30308\n",
      "epoch no.4 train no.378960  loss = 2.17791 avg_loss = 3.27023\n",
      "epoch no.4 train no.378970  loss = 2.16364 avg_loss = 3.26402\n",
      "epoch no.4 train no.378980  loss = 4.89610 avg_loss = 3.21968\n",
      "epoch no.4 train no.378990  loss = 4.20882 avg_loss = 3.21763\n",
      "epoch no.4 train no.379000  loss = 3.00944 avg_loss = 3.24886\n",
      "5\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁필요할', '때', '▁듣는', '▁신나는', '</s>']\n",
      "기분전환이 필요할때 듣는 음악</s>\n",
      "epoch no.4 train no.379010  loss = 3.30893 avg_loss = 3.26880\n",
      "epoch no.4 train no.379020  loss = 4.39846 avg_loss = 3.24686\n",
      "epoch no.4 train no.379030  loss = 3.64032 avg_loss = 3.25335\n",
      "epoch no.4 train no.379040  loss = 3.11527 avg_loss = 3.27991\n",
      "epoch no.4 train no.379050  loss = 3.71329 avg_loss = 3.29577\n",
      "epoch no.4 train no.379060  loss = 1.43741 avg_loss = 3.27622\n",
      "epoch no.4 train no.379070  loss = 4.00012 avg_loss = 3.29563\n",
      "epoch no.4 train no.379080  loss = 3.94612 avg_loss = 3.31838\n",
      "epoch no.4 train no.379090  loss = 3.20100 avg_loss = 3.31702\n",
      "epoch no.4 train no.379100  loss = 5.31043 avg_loss = 3.29395\n",
      "epoch no.4 train no.379110  loss = 3.07801 avg_loss = 3.29772\n",
      "epoch no.4 train no.379120  loss = 2.64971 avg_loss = 3.27848\n",
      "epoch no.4 train no.379130  loss = 3.71391 avg_loss = 3.26005\n",
      "epoch no.4 train no.379140  loss = 2.38821 avg_loss = 3.25029\n",
      "epoch no.4 train no.379150  loss = 3.36181 avg_loss = 3.26673\n",
      "epoch no.4 train no.379160  loss = 3.15663 avg_loss = 3.31012\n",
      "epoch no.4 train no.379170  loss = 2.39242 avg_loss = 3.31458\n",
      "epoch no.4 train no.379180  loss = 2.78785 avg_loss = 3.29551\n",
      "epoch no.4 train no.379190  loss = 1.94775 avg_loss = 3.25817\n",
      "epoch no.4 train no.379200  loss = 2.81576 avg_loss = 3.24211\n",
      "epoch no.4 train no.379210  loss = 2.65187 avg_loss = 3.24471\n",
      "epoch no.4 train no.379220  loss = 2.73366 avg_loss = 3.23952\n",
      "epoch no.4 train no.379230  loss = 1.97799 avg_loss = 3.25790\n",
      "epoch no.4 train no.379240  loss = 3.74027 avg_loss = 3.27258\n",
      "epoch no.4 train no.379250  loss = 2.89257 avg_loss = 3.27420\n",
      "epoch no.4 train no.379260  loss = 3.46031 avg_loss = 3.29630\n",
      "epoch no.4 train no.379270  loss = 3.07755 avg_loss = 3.38483\n",
      "epoch no.4 train no.379280  loss = 4.63539 avg_loss = 3.39354\n",
      "epoch no.4 train no.379290  loss = 3.21659 avg_loss = 3.40827\n",
      "epoch no.4 train no.379300  loss = 4.79707 avg_loss = 3.45646\n",
      "epoch no.4 train no.379310  loss = 3.47114 avg_loss = 3.41810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.379320  loss = 3.18353 avg_loss = 3.41924\n",
      "epoch no.4 train no.379330  loss = 2.14777 avg_loss = 3.38099\n",
      "epoch no.4 train no.379340  loss = 4.63829 avg_loss = 3.41096\n",
      "epoch no.4 train no.379350  loss = 3.58597 avg_loss = 3.42689\n",
      "epoch no.4 train no.379360  loss = 3.02730 avg_loss = 3.39545\n",
      "epoch no.4 train no.379370  loss = 4.68458 avg_loss = 3.41747\n",
      "epoch no.4 train no.379380  loss = 2.96152 avg_loss = 3.40650\n",
      "epoch no.4 train no.379390  loss = 4.89848 avg_loss = 3.39829\n",
      "epoch no.4 train no.379400  loss = 4.09797 avg_loss = 3.40756\n",
      "epoch no.4 train no.379410  loss = 2.74892 avg_loss = 3.39585\n",
      "epoch no.4 train no.379420  loss = 2.48745 avg_loss = 3.39488\n",
      "epoch no.4 train no.379430  loss = 2.84593 avg_loss = 3.38767\n",
      "epoch no.4 train no.379440  loss = 3.61255 avg_loss = 3.42100\n",
      "epoch no.4 train no.379450  loss = 2.98180 avg_loss = 3.38880\n",
      "epoch no.4 train no.379460  loss = 2.17658 avg_loss = 3.38559\n",
      "epoch no.4 train no.379470  loss = 3.02297 avg_loss = 3.34137\n",
      "epoch no.4 train no.379480  loss = 3.91674 avg_loss = 3.34522\n",
      "epoch no.4 train no.379490  loss = 3.28690 avg_loss = 3.38647\n",
      "epoch no.4 train no.379500  loss = 2.60326 avg_loss = 3.34245\n",
      "epoch no.4 train no.379510  loss = 2.18307 avg_loss = 3.34075\n",
      "epoch no.4 train no.379520  loss = 2.68511 avg_loss = 3.29626\n",
      "epoch no.4 train no.379530  loss = 4.26699 avg_loss = 3.30447\n",
      "epoch no.4 train no.379540  loss = 2.71883 avg_loss = 3.31838\n",
      "epoch no.4 train no.379550  loss = 3.57968 avg_loss = 3.30490\n",
      "epoch no.4 train no.379560  loss = 2.85530 avg_loss = 3.32367\n",
      "epoch no.4 train no.379570  loss = 2.41818 avg_loss = 3.32347\n",
      "epoch no.4 train no.379580  loss = 2.56882 avg_loss = 3.30844\n",
      "epoch no.4 train no.379590  loss = 3.97486 avg_loss = 3.31318\n",
      "epoch no.4 train no.379600  loss = 3.75881 avg_loss = 3.33274\n",
      "epoch no.4 train no.379610  loss = 2.86117 avg_loss = 3.28064\n",
      "epoch no.4 train no.379620  loss = 4.44768 avg_loss = 3.28770\n",
      "epoch no.4 train no.379630  loss = 4.04836 avg_loss = 3.26950\n",
      "epoch no.4 train no.379640  loss = 2.90724 avg_loss = 3.26356\n",
      "epoch no.4 train no.379650  loss = 4.52391 avg_loss = 3.29242\n",
      "epoch no.4 train no.379660  loss = 2.95573 avg_loss = 3.24718\n",
      "epoch no.4 train no.379670  loss = 2.94268 avg_loss = 3.27915\n",
      "epoch no.4 train no.379680  loss = 2.86907 avg_loss = 3.24085\n",
      "epoch no.4 train no.379690  loss = 3.34184 avg_loss = 3.22335\n",
      "epoch no.4 train no.379700  loss = 2.65525 avg_loss = 3.19174\n",
      "epoch no.4 train no.379710  loss = 2.30123 avg_loss = 3.18343\n",
      "epoch no.4 train no.379720  loss = 4.06171 avg_loss = 3.18387\n",
      "epoch no.4 train no.379730  loss = 3.03040 avg_loss = 3.18019\n",
      "epoch no.4 train no.379740  loss = 2.56041 avg_loss = 3.19621\n",
      "epoch no.4 train no.379750  loss = 3.79805 avg_loss = 3.19655\n",
      "epoch no.4 train no.379760  loss = 3.09101 avg_loss = 3.18057\n",
      "epoch no.4 train no.379770  loss = 4.28958 avg_loss = 3.17920\n",
      "epoch no.4 train no.379780  loss = 4.22243 avg_loss = 3.22586\n",
      "epoch no.4 train no.379790  loss = 4.50597 avg_loss = 3.24873\n",
      "epoch no.4 train no.379800  loss = 3.45953 avg_loss = 3.25450\n",
      "epoch no.4 train no.379810  loss = 3.21065 avg_loss = 3.24626\n",
      "epoch no.4 train no.379820  loss = 3.08830 avg_loss = 3.24864\n",
      "epoch no.4 train no.379830  loss = 2.25303 avg_loss = 3.26932\n",
      "epoch no.4 train no.379840  loss = 2.60654 avg_loss = 3.23527\n",
      "epoch no.4 train no.379850  loss = 3.39343 avg_loss = 3.23236\n",
      "epoch no.4 train no.379860  loss = 4.11335 avg_loss = 3.26642\n",
      "epoch no.4 train no.379870  loss = 3.36902 avg_loss = 3.25273\n",
      "epoch no.4 train no.379880  loss = 2.90662 avg_loss = 3.24604\n",
      "epoch no.4 train no.379890  loss = 3.48895 avg_loss = 3.24885\n",
      "epoch no.4 train no.379900  loss = 3.78051 avg_loss = 3.26047\n",
      "epoch no.4 train no.379910  loss = 4.48060 avg_loss = 3.31071\n",
      "epoch no.4 train no.379920  loss = 2.96905 avg_loss = 3.29418\n",
      "epoch no.4 train no.379930  loss = 2.18180 avg_loss = 3.27234\n",
      "epoch no.4 train no.379940  loss = 3.92348 avg_loss = 3.27169\n",
      "epoch no.4 train no.379950  loss = 3.25175 avg_loss = 3.27647\n",
      "epoch no.4 train no.379960  loss = 2.64796 avg_loss = 3.28677\n",
      "epoch no.4 train no.379970  loss = 4.33570 avg_loss = 3.33726\n",
      "epoch no.4 train no.379980  loss = 2.91857 avg_loss = 3.29333\n",
      "epoch no.4 train no.379990  loss = 2.26590 avg_loss = 3.29761\n",
      "epoch no.4 train no.380000  loss = 3.04333 avg_loss = 3.30994\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는', '▁노래', '</s>']\n",
      "기분전환이 필요할때 듣는 노래</s>\n",
      "epoch no.4 train no.380010  loss = 4.44109 avg_loss = 3.36715\n",
      "epoch no.4 train no.380020  loss = 3.69292 avg_loss = 3.33535\n",
      "epoch no.4 train no.380030  loss = 3.18334 avg_loss = 3.34269\n",
      "epoch no.4 train no.380040  loss = 3.35754 avg_loss = 3.28794\n",
      "epoch no.4 train no.380050  loss = 3.92725 avg_loss = 3.30021\n",
      "epoch no.4 train no.380060  loss = 4.49717 avg_loss = 3.25419\n",
      "epoch no.4 train no.380070  loss = 2.20756 avg_loss = 3.30030\n",
      "epoch no.4 train no.380080  loss = 3.80954 avg_loss = 3.30137\n",
      "epoch no.4 train no.380090  loss = 4.28669 avg_loss = 3.33791\n",
      "epoch no.4 train no.380100  loss = 2.63080 avg_loss = 3.31873\n",
      "epoch no.4 train no.380110  loss = 2.05101 avg_loss = 3.34623\n",
      "epoch no.4 train no.380120  loss = 5.09715 avg_loss = 3.34880\n",
      "epoch no.4 train no.380130  loss = 2.24463 avg_loss = 3.32509\n",
      "epoch no.4 train no.380140  loss = 2.56499 avg_loss = 3.34974\n",
      "epoch no.4 train no.380150  loss = 3.01635 avg_loss = 3.33808\n",
      "epoch no.4 train no.380160  loss = 1.67424 avg_loss = 3.36572\n",
      "epoch no.4 train no.380170  loss = 2.19674 avg_loss = 3.30206\n",
      "epoch no.4 train no.380180  loss = 4.25748 avg_loss = 3.31448\n",
      "epoch no.4 train no.380190  loss = 3.09332 avg_loss = 3.34447\n",
      "epoch no.4 train no.380200  loss = 2.66895 avg_loss = 3.34434\n",
      "epoch no.4 train no.380210  loss = 2.47505 avg_loss = 3.30878\n",
      "epoch no.4 train no.380220  loss = 4.10112 avg_loss = 3.30347\n",
      "epoch no.4 train no.380230  loss = 4.29131 avg_loss = 3.32995\n",
      "epoch no.4 train no.380240  loss = 3.41958 avg_loss = 3.29150\n",
      "epoch no.4 train no.380250  loss = 3.24862 avg_loss = 3.23917\n",
      "epoch no.4 train no.380260  loss = 2.57264 avg_loss = 3.24389\n",
      "epoch no.4 train no.380270  loss = 2.57578 avg_loss = 3.26940\n",
      "epoch no.4 train no.380280  loss = 4.08130 avg_loss = 3.25445\n",
      "epoch no.4 train no.380290  loss = 2.40186 avg_loss = 3.21523\n",
      "epoch no.4 train no.380300  loss = 2.65505 avg_loss = 3.20865\n",
      "epoch no.4 train no.380310  loss = 3.51557 avg_loss = 3.18861\n",
      "epoch no.4 train no.380320  loss = 2.77278 avg_loss = 3.20526\n",
      "epoch no.4 train no.380330  loss = 5.74704 avg_loss = 3.25041\n",
      "epoch no.4 train no.380340  loss = 5.01801 avg_loss = 3.24252\n",
      "epoch no.4 train no.380350  loss = 3.81743 avg_loss = 3.23476\n",
      "epoch no.4 train no.380360  loss = 3.38205 avg_loss = 3.22437\n",
      "epoch no.4 train no.380370  loss = 4.52640 avg_loss = 3.22712\n",
      "epoch no.4 train no.380380  loss = 2.82938 avg_loss = 3.22981\n",
      "epoch no.4 train no.380390  loss = 4.19362 avg_loss = 3.22078\n",
      "epoch no.4 train no.380400  loss = 3.00325 avg_loss = 3.22978\n",
      "epoch no.4 train no.380410  loss = 3.22944 avg_loss = 3.26561\n",
      "epoch no.4 train no.380420  loss = 3.97893 avg_loss = 3.27266\n",
      "epoch no.4 train no.380430  loss = 3.49395 avg_loss = 3.28351\n",
      "epoch no.4 train no.380440  loss = 2.02255 avg_loss = 3.30119\n",
      "epoch no.4 train no.380450  loss = 3.01252 avg_loss = 3.29000\n",
      "epoch no.4 train no.380460  loss = 3.90805 avg_loss = 3.32103\n",
      "epoch no.4 train no.380470  loss = 2.84579 avg_loss = 3.30555\n",
      "epoch no.4 train no.380480  loss = 4.51688 avg_loss = 3.34624\n",
      "epoch no.4 train no.380490  loss = 3.21569 avg_loss = 3.34555\n",
      "epoch no.4 train no.380500  loss = 6.36418 avg_loss = 3.32537\n",
      "epoch no.4 train no.380510  loss = 4.81303 avg_loss = 3.32525\n",
      "epoch no.4 train no.380520  loss = 2.55432 avg_loss = 3.32063\n",
      "epoch no.4 train no.380530  loss = 3.51089 avg_loss = 3.28252\n",
      "epoch no.4 train no.380540  loss = 2.86665 avg_loss = 3.27582\n",
      "epoch no.4 train no.380550  loss = 3.77522 avg_loss = 3.32306\n",
      "epoch no.4 train no.380560  loss = 4.66184 avg_loss = 3.34458\n",
      "epoch no.4 train no.380570  loss = 2.44165 avg_loss = 3.30951\n",
      "epoch no.4 train no.380580  loss = 2.97131 avg_loss = 3.31145\n",
      "epoch no.4 train no.380590  loss = 4.72454 avg_loss = 3.33236\n",
      "epoch no.4 train no.380600  loss = 2.77538 avg_loss = 3.34153\n",
      "epoch no.4 train no.380610  loss = 3.41873 avg_loss = 3.34580\n",
      "epoch no.4 train no.380620  loss = 3.43608 avg_loss = 3.35874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.380630  loss = 2.98865 avg_loss = 3.33970\n",
      "epoch no.4 train no.380640  loss = 3.59969 avg_loss = 3.32726\n",
      "epoch no.4 train no.380650  loss = 3.36101 avg_loss = 3.30721\n",
      "epoch no.4 train no.380660  loss = 2.69039 avg_loss = 3.34034\n",
      "epoch no.4 train no.380670  loss = 4.39678 avg_loss = 3.34508\n",
      "epoch no.4 train no.380680  loss = 2.38137 avg_loss = 3.34863\n",
      "epoch no.4 train no.380690  loss = 6.12630 avg_loss = 3.38678\n",
      "epoch no.4 train no.380700  loss = 1.78177 avg_loss = 3.35254\n",
      "epoch no.4 train no.380710  loss = 1.89613 avg_loss = 3.37864\n",
      "epoch no.4 train no.380720  loss = 3.59798 avg_loss = 3.39431\n",
      "epoch no.4 train no.380730  loss = 3.71950 avg_loss = 3.38013\n",
      "epoch no.4 train no.380740  loss = 3.10742 avg_loss = 3.38662\n",
      "epoch no.4 train no.380750  loss = 2.51733 avg_loss = 3.38468\n",
      "epoch no.4 train no.380760  loss = 3.57515 avg_loss = 3.40407\n",
      "epoch no.4 train no.380770  loss = 3.10310 avg_loss = 3.43768\n",
      "epoch no.4 train no.380780  loss = 3.06501 avg_loss = 3.43997\n",
      "epoch no.4 train no.380790  loss = 4.66723 avg_loss = 3.44228\n",
      "epoch no.4 train no.380800  loss = 5.55588 avg_loss = 3.46547\n",
      "epoch no.4 train no.380810  loss = 4.37436 avg_loss = 3.43089\n",
      "epoch no.4 train no.380820  loss = 3.75498 avg_loss = 3.43040\n",
      "epoch no.4 train no.380830  loss = 4.39985 avg_loss = 3.42472\n",
      "epoch no.4 train no.380840  loss = 2.76004 avg_loss = 3.41297\n",
      "epoch no.4 train no.380850  loss = 1.73969 avg_loss = 3.41506\n",
      "epoch no.4 train no.380860  loss = 2.59079 avg_loss = 3.39906\n",
      "epoch no.4 train no.380870  loss = 3.59549 avg_loss = 3.39032\n",
      "epoch no.4 train no.380880  loss = 2.98846 avg_loss = 3.33638\n",
      "epoch no.4 train no.380890  loss = 2.84262 avg_loss = 3.32246\n",
      "epoch no.4 train no.380900  loss = 4.11677 avg_loss = 3.35737\n",
      "epoch no.4 train no.380910  loss = 2.12598 avg_loss = 3.36319\n",
      "epoch no.4 train no.380920  loss = 1.76347 avg_loss = 3.34844\n",
      "epoch no.4 train no.380930  loss = 5.17018 avg_loss = 3.35573\n",
      "epoch no.4 train no.380940  loss = 2.45698 avg_loss = 3.32055\n",
      "epoch no.4 train no.380950  loss = 4.29100 avg_loss = 3.30191\n",
      "epoch no.4 train no.380960  loss = 3.55865 avg_loss = 3.29443\n",
      "epoch no.4 train no.380970  loss = 1.75217 avg_loss = 3.26659\n",
      "epoch no.4 train no.380980  loss = 3.09004 avg_loss = 3.27209\n",
      "epoch no.4 train no.380990  loss = 2.80356 avg_loss = 3.26309\n",
      "epoch no.4 train no.381000  loss = 5.21257 avg_loss = 3.28163\n",
      "6\n",
      "to_tokens: ['▁라디오', '▁좋은', '이', '▁필요할', '때', '</s>', '▁신나는', '▁p', '</s>']\n",
      "기분전환이 필요할때 듣는 신나는 음악</s>\n",
      "epoch no.4 train no.381010  loss = 4.82020 avg_loss = 3.30590\n",
      "epoch no.4 train no.381020  loss = 3.75429 avg_loss = 3.33336\n",
      "epoch no.4 train no.381030  loss = 2.84082 avg_loss = 3.31168\n",
      "epoch no.4 train no.381040  loss = 4.00197 avg_loss = 3.33388\n",
      "epoch no.4 train no.381050  loss = 2.88339 avg_loss = 3.26712\n",
      "epoch no.4 train no.381060  loss = 3.74324 avg_loss = 3.29357\n",
      "epoch no.4 train no.381070  loss = 4.20827 avg_loss = 3.30688\n",
      "epoch no.4 train no.381080  loss = 3.55776 avg_loss = 3.31793\n",
      "epoch no.4 train no.381090  loss = 4.23729 avg_loss = 3.31538\n",
      "epoch no.4 train no.381100  loss = 3.78961 avg_loss = 3.31929\n",
      "epoch no.4 train no.381110  loss = 2.50962 avg_loss = 3.32498\n",
      "epoch no.4 train no.381120  loss = 2.60706 avg_loss = 3.31244\n",
      "epoch no.4 train no.381130  loss = 3.09560 avg_loss = 3.31479\n",
      "epoch no.4 train no.381140  loss = 3.53903 avg_loss = 3.33171\n",
      "epoch no.4 train no.381150  loss = 3.77076 avg_loss = 3.29601\n",
      "epoch no.4 train no.381160  loss = 2.53650 avg_loss = 3.24472\n",
      "epoch no.4 train no.381170  loss = 3.06661 avg_loss = 3.26674\n",
      "epoch no.4 train no.381180  loss = 3.72017 avg_loss = 3.24549\n",
      "epoch no.4 train no.381190  loss = 2.36435 avg_loss = 3.28472\n",
      "epoch no.4 train no.381200  loss = 2.36846 avg_loss = 3.28874\n",
      "epoch no.4 train no.381210  loss = 3.64488 avg_loss = 3.28395\n",
      "epoch no.4 train no.381220  loss = 4.32830 avg_loss = 3.27613\n",
      "epoch no.4 train no.381230  loss = 3.83212 avg_loss = 3.29837\n",
      "epoch no.4 train no.381240  loss = 2.73861 avg_loss = 3.31695\n",
      "epoch no.4 train no.381250  loss = 2.64637 avg_loss = 3.31740\n",
      "epoch no.4 train no.381260  loss = 2.95384 avg_loss = 3.32418\n",
      "epoch no.4 train no.381270  loss = 3.48345 avg_loss = 3.30068\n",
      "epoch no.4 train no.381280  loss = 2.80142 avg_loss = 3.30785\n",
      "epoch no.4 train no.381290  loss = 2.10672 avg_loss = 3.30365\n",
      "epoch no.4 train no.381300  loss = 3.08642 avg_loss = 3.27385\n",
      "epoch no.4 train no.381310  loss = 2.88454 avg_loss = 3.28072\n",
      "epoch no.4 train no.381320  loss = 1.88478 avg_loss = 3.26567\n",
      "epoch no.4 train no.381330  loss = 2.31134 avg_loss = 3.23904\n",
      "epoch no.4 train no.381340  loss = 3.23045 avg_loss = 3.26850\n",
      "epoch no.4 train no.381350  loss = 1.99102 avg_loss = 3.27158\n",
      "epoch no.4 train no.381360  loss = 2.96739 avg_loss = 3.27623\n",
      "epoch no.4 train no.381370  loss = 2.35879 avg_loss = 3.29393\n",
      "epoch no.4 train no.381380  loss = 2.39655 avg_loss = 3.33540\n",
      "epoch no.4 train no.381390  loss = 2.24963 avg_loss = 3.31292\n",
      "epoch no.4 train no.381400  loss = 3.13085 avg_loss = 3.30035\n",
      "epoch no.4 train no.381410  loss = 4.56279 avg_loss = 3.33470\n",
      "epoch no.4 train no.381420  loss = 2.90157 avg_loss = 3.26152\n",
      "epoch no.4 train no.381430  loss = 3.18800 avg_loss = 3.30838\n",
      "epoch no.4 train no.381440  loss = 4.24734 avg_loss = 3.32480\n",
      "epoch no.4 train no.381450  loss = 2.84352 avg_loss = 3.24837\n",
      "epoch no.4 train no.381460  loss = 5.38427 avg_loss = 3.24447\n",
      "epoch no.4 train no.381470  loss = 2.52947 avg_loss = 3.21584\n",
      "epoch no.4 train no.381480  loss = 3.02459 avg_loss = 3.26734\n",
      "epoch no.4 train no.381490  loss = 2.59175 avg_loss = 3.26545\n",
      "epoch no.4 train no.381500  loss = 4.05941 avg_loss = 3.25924\n",
      "epoch no.4 train no.381510  loss = 3.63842 avg_loss = 3.25926\n",
      "epoch no.4 train no.381520  loss = 2.47214 avg_loss = 3.28907\n",
      "epoch no.4 train no.381530  loss = 4.60254 avg_loss = 3.30585\n",
      "epoch no.4 train no.381540  loss = 1.96032 avg_loss = 3.29333\n",
      "epoch no.4 train no.381550  loss = 3.47940 avg_loss = 3.27921\n",
      "epoch no.4 train no.381560  loss = 4.55870 avg_loss = 3.29402\n",
      "epoch no.4 train no.381570  loss = 3.03448 avg_loss = 3.28492\n",
      "epoch no.4 train no.381580  loss = 2.87728 avg_loss = 3.26425\n",
      "epoch no.4 train no.381590  loss = 4.88646 avg_loss = 3.26189\n",
      "epoch no.4 train no.381600  loss = 3.02583 avg_loss = 3.27321\n",
      "epoch no.4 train no.381610  loss = 3.68728 avg_loss = 3.29521\n",
      "epoch no.4 train no.381620  loss = 3.19466 avg_loss = 3.32125\n",
      "epoch no.4 train no.381630  loss = 2.25885 avg_loss = 3.29477\n",
      "epoch no.4 train no.381640  loss = 2.53920 avg_loss = 3.29249\n",
      "epoch no.4 train no.381650  loss = 2.24686 avg_loss = 3.28673\n",
      "epoch no.4 train no.381660  loss = 2.82525 avg_loss = 3.25846\n",
      "epoch no.4 train no.381670  loss = 3.90961 avg_loss = 3.28253\n",
      "epoch no.4 train no.381680  loss = 2.08210 avg_loss = 3.28016\n",
      "epoch no.4 train no.381690  loss = 3.01470 avg_loss = 3.31453\n",
      "epoch no.4 train no.381700  loss = 3.09863 avg_loss = 3.31033\n",
      "epoch no.4 train no.381710  loss = 5.31719 avg_loss = 3.32864\n",
      "epoch no.4 train no.381720  loss = 3.50854 avg_loss = 3.33038\n",
      "epoch no.4 train no.381730  loss = 2.46052 avg_loss = 3.31998\n",
      "epoch no.4 train no.381740  loss = 3.47121 avg_loss = 3.30851\n",
      "epoch no.4 train no.381750  loss = 3.49544 avg_loss = 3.31899\n",
      "epoch no.4 train no.381760  loss = 3.64632 avg_loss = 3.28516\n",
      "epoch no.4 train no.381770  loss = 3.89294 avg_loss = 3.31405\n",
      "epoch no.4 train no.381780  loss = 3.47469 avg_loss = 3.26133\n",
      "epoch no.4 train no.381790  loss = 3.43716 avg_loss = 3.26904\n",
      "epoch no.4 train no.381800  loss = 2.02597 avg_loss = 3.25987\n",
      "epoch no.4 train no.381810  loss = 2.69441 avg_loss = 3.28326\n",
      "epoch no.4 train no.381820  loss = 3.04909 avg_loss = 3.26331\n",
      "epoch no.4 train no.381830  loss = 4.99103 avg_loss = 3.30245\n",
      "epoch no.4 train no.381840  loss = 1.97966 avg_loss = 3.30717\n",
      "epoch no.4 train no.381850  loss = 2.66269 avg_loss = 3.29709\n",
      "epoch no.4 train no.381860  loss = 4.20836 avg_loss = 3.25441\n",
      "epoch no.4 train no.381870  loss = 3.89523 avg_loss = 3.29456\n",
      "epoch no.4 train no.381880  loss = 2.71283 avg_loss = 3.26099\n",
      "epoch no.4 train no.381890  loss = 2.57001 avg_loss = 3.28267\n",
      "epoch no.4 train no.381900  loss = 3.46007 avg_loss = 3.26551\n",
      "epoch no.4 train no.381910  loss = 3.50191 avg_loss = 3.24194\n",
      "epoch no.4 train no.381920  loss = 5.36584 avg_loss = 3.25088\n",
      "epoch no.4 train no.381930  loss = 2.34351 avg_loss = 3.22880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.381940  loss = 2.69779 avg_loss = 3.24484\n",
      "epoch no.4 train no.381950  loss = 2.76740 avg_loss = 3.27153\n",
      "epoch no.4 train no.381960  loss = 2.46129 avg_loss = 3.24162\n",
      "epoch no.4 train no.381970  loss = 3.02335 avg_loss = 3.22526\n",
      "epoch no.4 train no.381980  loss = 5.61023 avg_loss = 3.20310\n",
      "epoch no.4 train no.381990  loss = 5.19952 avg_loss = 3.25130\n",
      "epoch no.4 train no.382000  loss = 5.18806 avg_loss = 3.28623\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '</s>', '▁노래', '</s>', '</s>']\n",
      "기분전환이 필요할때 듣는 음악들</s>\n",
      "epoch no.4 train no.382010  loss = 2.39797 avg_loss = 3.26708\n",
      "epoch no.4 train no.382020  loss = 3.25576 avg_loss = 3.30998\n",
      "epoch no.4 train no.382030  loss = 2.29147 avg_loss = 3.27836\n",
      "epoch no.4 train no.382040  loss = 2.15287 avg_loss = 3.26221\n",
      "epoch no.4 train no.382050  loss = 3.89248 avg_loss = 3.29716\n",
      "epoch no.4 train no.382060  loss = 2.27331 avg_loss = 3.28000\n",
      "epoch no.4 train no.382070  loss = 3.47090 avg_loss = 3.30150\n",
      "epoch no.4 train no.382080  loss = 2.73194 avg_loss = 3.30627\n",
      "epoch no.4 train no.382090  loss = 2.25860 avg_loss = 3.27448\n",
      "epoch no.4 train no.382100  loss = 3.46606 avg_loss = 3.31927\n",
      "epoch no.4 train no.382110  loss = 2.37037 avg_loss = 3.27210\n",
      "epoch no.4 train no.382120  loss = 2.74923 avg_loss = 3.25395\n",
      "epoch no.4 train no.382130  loss = 3.00190 avg_loss = 3.24352\n",
      "epoch no.4 train no.382140  loss = 2.37314 avg_loss = 3.25426\n",
      "epoch no.4 train no.382150  loss = 2.44176 avg_loss = 3.29138\n",
      "epoch no.4 train no.382160  loss = 2.39891 avg_loss = 3.27628\n",
      "epoch no.4 train no.382170  loss = 3.87359 avg_loss = 3.31165\n",
      "epoch no.4 train no.382180  loss = 6.54996 avg_loss = 3.32685\n",
      "epoch no.4 train no.382190  loss = 3.39361 avg_loss = 3.30831\n",
      "epoch no.4 train no.382200  loss = 5.52838 avg_loss = 3.35380\n",
      "epoch no.4 train no.382210  loss = 4.03449 avg_loss = 3.32827\n",
      "epoch no.4 train no.382220  loss = 2.52999 avg_loss = 3.32868\n",
      "epoch no.4 train no.382230  loss = 1.95276 avg_loss = 3.34420\n",
      "epoch no.4 train no.382240  loss = 2.97450 avg_loss = 3.31531\n",
      "epoch no.4 train no.382250  loss = 4.34889 avg_loss = 3.34284\n",
      "epoch no.4 train no.382260  loss = 4.02670 avg_loss = 3.34287\n",
      "epoch no.4 train no.382270  loss = 3.26079 avg_loss = 3.35020\n",
      "epoch no.4 train no.382280  loss = 3.45042 avg_loss = 3.34643\n",
      "epoch no.4 train no.382290  loss = 3.43742 avg_loss = 3.37431\n",
      "epoch no.4 train no.382300  loss = 4.14441 avg_loss = 3.33699\n",
      "epoch no.4 train no.382310  loss = 4.17990 avg_loss = 3.32742\n",
      "epoch no.4 train no.382320  loss = 4.54057 avg_loss = 3.34303\n",
      "epoch no.4 train no.382330  loss = 3.65234 avg_loss = 3.33103\n",
      "epoch no.4 train no.382340  loss = 3.05705 avg_loss = 3.35153\n",
      "epoch no.4 train no.382350  loss = 3.96862 avg_loss = 3.34096\n",
      "epoch no.4 train no.382360  loss = 4.01958 avg_loss = 3.32353\n",
      "epoch no.4 train no.382370  loss = 2.53295 avg_loss = 3.34846\n",
      "epoch no.4 train no.382380  loss = 2.74127 avg_loss = 3.33530\n",
      "epoch no.4 train no.382390  loss = 4.21790 avg_loss = 3.35821\n",
      "epoch no.4 train no.382400  loss = 3.52131 avg_loss = 3.35447\n",
      "epoch no.4 train no.382410  loss = 3.52620 avg_loss = 3.34487\n",
      "epoch no.4 train no.382420  loss = 2.23890 avg_loss = 3.34491\n",
      "epoch no.4 train no.382430  loss = 3.10261 avg_loss = 3.33623\n",
      "epoch no.4 train no.382440  loss = 2.96575 avg_loss = 3.33274\n",
      "epoch no.4 train no.382450  loss = 3.68354 avg_loss = 3.33131\n",
      "epoch no.4 train no.382460  loss = 2.28160 avg_loss = 3.30166\n",
      "epoch no.4 train no.382470  loss = 3.47466 avg_loss = 3.28325\n",
      "epoch no.4 train no.382480  loss = 2.63001 avg_loss = 3.26710\n",
      "epoch no.4 train no.382490  loss = 3.39809 avg_loss = 3.29042\n",
      "epoch no.4 train no.382500  loss = 2.98924 avg_loss = 3.28275\n",
      "epoch no.4 train no.382510  loss = 5.53274 avg_loss = 3.32859\n",
      "epoch no.4 train no.382520  loss = 2.96213 avg_loss = 3.32391\n",
      "epoch no.4 train no.382530  loss = 5.21052 avg_loss = 3.35541\n",
      "epoch no.4 train no.382540  loss = 2.41927 avg_loss = 3.35506\n",
      "epoch no.4 train no.382550  loss = 3.39867 avg_loss = 3.37268\n",
      "epoch no.4 train no.382560  loss = 2.66350 avg_loss = 3.32102\n",
      "epoch no.4 train no.382570  loss = 4.57882 avg_loss = 3.29130\n",
      "epoch no.4 train no.382580  loss = 2.44131 avg_loss = 3.33253\n",
      "epoch no.4 train no.382590  loss = 1.93227 avg_loss = 3.28047\n",
      "epoch no.4 train no.382600  loss = 4.58358 avg_loss = 3.29443\n",
      "epoch no.4 train no.382610  loss = 1.77531 avg_loss = 3.28623\n",
      "epoch no.4 train no.382620  loss = 4.01090 avg_loss = 3.31368\n",
      "epoch no.4 train no.382630  loss = 2.69524 avg_loss = 3.31717\n",
      "epoch no.4 train no.382640  loss = 2.38505 avg_loss = 3.26754\n",
      "epoch no.4 train no.382650  loss = 3.37123 avg_loss = 3.28093\n",
      "epoch no.4 train no.382660  loss = 3.90354 avg_loss = 3.29265\n",
      "epoch no.4 train no.382670  loss = 2.45043 avg_loss = 3.30352\n",
      "epoch no.4 train no.382680  loss = 3.08239 avg_loss = 3.34025\n",
      "epoch no.4 train no.382690  loss = 2.74715 avg_loss = 3.37122\n",
      "epoch no.4 train no.382700  loss = 4.45689 avg_loss = 3.37971\n",
      "epoch no.4 train no.382710  loss = 2.57939 avg_loss = 3.38071\n",
      "epoch no.4 train no.382720  loss = 2.95150 avg_loss = 3.36941\n",
      "epoch no.4 train no.382730  loss = 3.38056 avg_loss = 3.34915\n",
      "epoch no.4 train no.382740  loss = 1.94011 avg_loss = 3.33318\n",
      "epoch no.4 train no.382750  loss = 4.29895 avg_loss = 3.33094\n",
      "epoch no.4 train no.382760  loss = 2.92102 avg_loss = 3.36246\n",
      "epoch no.4 train no.382770  loss = 4.06905 avg_loss = 3.36018\n",
      "epoch no.4 train no.382780  loss = 3.40651 avg_loss = 3.39301\n",
      "epoch no.4 train no.382790  loss = 4.84534 avg_loss = 3.39733\n",
      "epoch no.4 train no.382800  loss = 6.35575 avg_loss = 3.41164\n",
      "epoch no.4 train no.382810  loss = 3.64626 avg_loss = 3.42554\n",
      "epoch no.4 train no.382820  loss = 3.77987 avg_loss = 3.36670\n",
      "epoch no.4 train no.382830  loss = 2.15876 avg_loss = 3.37962\n",
      "epoch no.4 train no.382840  loss = 4.06538 avg_loss = 3.35302\n",
      "epoch no.4 train no.382850  loss = 2.47701 avg_loss = 3.31667\n",
      "epoch no.4 train no.382860  loss = 4.49332 avg_loss = 3.32170\n",
      "epoch no.4 train no.382870  loss = 3.42981 avg_loss = 3.35798\n",
      "epoch no.4 train no.382880  loss = 3.21474 avg_loss = 3.39104\n",
      "epoch no.4 train no.382890  loss = 3.44888 avg_loss = 3.38616\n",
      "epoch no.4 train no.382900  loss = 2.47825 avg_loss = 3.36665\n",
      "epoch no.4 train no.382910  loss = 3.16143 avg_loss = 3.34500\n",
      "epoch no.4 train no.382920  loss = 3.01291 avg_loss = 3.38053\n",
      "epoch no.4 train no.382930  loss = 2.13675 avg_loss = 3.37278\n",
      "epoch no.4 train no.382940  loss = 3.99473 avg_loss = 3.41969\n",
      "epoch no.4 train no.382950  loss = 1.99714 avg_loss = 3.39758\n",
      "epoch no.4 train no.382960  loss = 3.28951 avg_loss = 3.40927\n",
      "epoch no.4 train no.382970  loss = 4.31502 avg_loss = 3.41170\n",
      "epoch no.4 train no.382980  loss = 4.88001 avg_loss = 3.43521\n",
      "epoch no.4 train no.382990  loss = 2.65952 avg_loss = 3.40290\n",
      "epoch no.4 train no.383000  loss = 2.56307 avg_loss = 3.39873\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는', '▁음악', '</s>']\n",
      "기분전환이 필요할 때 듣는 음악</s>\n",
      "epoch no.4 train no.383010  loss = 2.99799 avg_loss = 3.39259\n",
      "epoch no.4 train no.383020  loss = 3.00358 avg_loss = 3.35463\n",
      "epoch no.4 train no.383030  loss = 2.14146 avg_loss = 3.34945\n",
      "epoch no.4 train no.383040  loss = 1.87469 avg_loss = 3.34717\n",
      "epoch no.4 train no.383050  loss = 4.07063 avg_loss = 3.28758\n",
      "epoch no.4 train no.383060  loss = 2.26380 avg_loss = 3.24657\n",
      "epoch no.4 train no.383070  loss = 4.24224 avg_loss = 3.25218\n",
      "epoch no.4 train no.383080  loss = 4.21721 avg_loss = 3.25584\n",
      "epoch no.4 train no.383090  loss = 4.31131 avg_loss = 3.27121\n",
      "epoch no.4 train no.383100  loss = 4.57574 avg_loss = 3.29412\n",
      "epoch no.4 train no.383110  loss = 3.12034 avg_loss = 3.31897\n",
      "epoch no.4 train no.383120  loss = 2.89973 avg_loss = 3.31080\n",
      "epoch no.4 train no.383130  loss = 2.46992 avg_loss = 3.32491\n",
      "epoch no.4 train no.383140  loss = 2.80543 avg_loss = 3.36460\n",
      "epoch no.4 train no.383150  loss = 2.56227 avg_loss = 3.36714\n",
      "epoch no.4 train no.383160  loss = 3.52494 avg_loss = 3.37829\n",
      "epoch no.4 train no.383170  loss = 3.02537 avg_loss = 3.39158\n",
      "epoch no.4 train no.383180  loss = 3.61026 avg_loss = 3.38599\n",
      "epoch no.4 train no.383190  loss = 3.14192 avg_loss = 3.35447\n",
      "epoch no.4 train no.383200  loss = 2.95542 avg_loss = 3.31635\n",
      "epoch no.4 train no.383210  loss = 2.56707 avg_loss = 3.33165\n",
      "epoch no.4 train no.383220  loss = 3.81986 avg_loss = 3.32968\n",
      "epoch no.4 train no.383230  loss = 2.04706 avg_loss = 3.33620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.383240  loss = 3.34821 avg_loss = 3.31987\n",
      "epoch no.4 train no.383250  loss = 3.05689 avg_loss = 3.36897\n",
      "epoch no.4 train no.383260  loss = 3.70162 avg_loss = 3.37795\n",
      "epoch no.4 train no.383270  loss = 3.64531 avg_loss = 3.35774\n",
      "epoch no.4 train no.383280  loss = 4.01021 avg_loss = 3.32732\n",
      "epoch no.4 train no.383290  loss = 2.98634 avg_loss = 3.33832\n",
      "epoch no.4 train no.383300  loss = 2.81950 avg_loss = 3.32033\n",
      "epoch no.4 train no.383310  loss = 2.83940 avg_loss = 3.33649\n",
      "epoch no.4 train no.383320  loss = 2.25510 avg_loss = 3.31301\n",
      "epoch no.4 train no.383330  loss = 1.81565 avg_loss = 3.27964\n",
      "epoch no.4 train no.383340  loss = 3.17729 avg_loss = 3.27989\n",
      "epoch no.4 train no.383350  loss = 2.66710 avg_loss = 3.24300\n",
      "epoch no.4 train no.383360  loss = 2.98160 avg_loss = 3.22205\n",
      "epoch no.4 train no.383370  loss = 1.94339 avg_loss = 3.19655\n",
      "epoch no.4 train no.383380  loss = 2.96625 avg_loss = 3.19547\n",
      "epoch no.4 train no.383390  loss = 4.88200 avg_loss = 3.24741\n",
      "epoch no.4 train no.383400  loss = 3.08059 avg_loss = 3.23535\n",
      "epoch no.4 train no.383410  loss = 3.91690 avg_loss = 3.24795\n",
      "epoch no.4 train no.383420  loss = 3.24319 avg_loss = 3.26731\n",
      "epoch no.4 train no.383430  loss = 3.33063 avg_loss = 3.29126\n",
      "epoch no.4 train no.383440  loss = 3.31138 avg_loss = 3.29072\n",
      "epoch no.4 train no.383450  loss = 2.59980 avg_loss = 3.30006\n",
      "epoch no.4 train no.383460  loss = 3.25631 avg_loss = 3.29337\n",
      "epoch no.4 train no.383470  loss = 3.32658 avg_loss = 3.26927\n",
      "epoch no.4 train no.383480  loss = 5.19555 avg_loss = 3.33400\n",
      "epoch no.4 train no.383490  loss = 4.42032 avg_loss = 3.36456\n",
      "epoch no.4 train no.383500  loss = 4.29623 avg_loss = 3.35193\n",
      "epoch no.4 train no.383510  loss = 2.96297 avg_loss = 3.36476\n",
      "epoch no.4 train no.383520  loss = 2.40777 avg_loss = 3.34042\n",
      "epoch no.4 train no.383530  loss = 2.59488 avg_loss = 3.34616\n",
      "epoch no.4 train no.383540  loss = 2.27588 avg_loss = 3.31360\n",
      "epoch no.4 train no.383550  loss = 4.39059 avg_loss = 3.34241\n",
      "epoch no.4 train no.383560  loss = 2.91739 avg_loss = 3.36429\n",
      "epoch no.4 train no.383570  loss = 3.91071 avg_loss = 3.36698\n",
      "epoch no.4 train no.383580  loss = 2.98403 avg_loss = 3.31033\n",
      "epoch no.4 train no.383590  loss = 4.09449 avg_loss = 3.36582\n",
      "epoch no.4 train no.383600  loss = 2.51425 avg_loss = 3.36763\n",
      "epoch no.4 train no.383610  loss = 1.76029 avg_loss = 3.35882\n",
      "epoch no.4 train no.383620  loss = 3.30484 avg_loss = 3.37056\n",
      "epoch no.4 train no.383630  loss = 2.42450 avg_loss = 3.37458\n",
      "epoch no.4 train no.383640  loss = 2.21559 avg_loss = 3.36072\n",
      "epoch no.4 train no.383650  loss = 4.88235 avg_loss = 3.34890\n",
      "epoch no.4 train no.383660  loss = 2.85678 avg_loss = 3.37123\n",
      "epoch no.4 train no.383670  loss = 4.60071 avg_loss = 3.42405\n",
      "epoch no.4 train no.383680  loss = 2.94596 avg_loss = 3.47679\n",
      "epoch no.4 train no.383690  loss = 3.63036 avg_loss = 3.44364\n",
      "epoch no.4 train no.383700  loss = 2.22715 avg_loss = 3.39439\n",
      "epoch no.4 train no.383710  loss = 4.21644 avg_loss = 3.44892\n",
      "epoch no.4 train no.383720  loss = 4.31397 avg_loss = 3.44161\n",
      "epoch no.4 train no.383730  loss = 4.18649 avg_loss = 3.52474\n",
      "epoch no.4 train no.383740  loss = 3.27012 avg_loss = 3.51625\n",
      "epoch no.4 train no.383750  loss = 2.72644 avg_loss = 3.52075\n",
      "epoch no.4 train no.383760  loss = 3.92046 avg_loss = 3.52061\n",
      "epoch no.4 train no.383770  loss = 1.99764 avg_loss = 3.54011\n",
      "epoch no.4 train no.383780  loss = 2.97632 avg_loss = 3.54411\n",
      "epoch no.4 train no.383790  loss = 2.28422 avg_loss = 3.50072\n",
      "epoch no.4 train no.383800  loss = 5.81788 avg_loss = 3.50909\n",
      "epoch no.4 train no.383810  loss = 3.19285 avg_loss = 3.49310\n",
      "epoch no.4 train no.383820  loss = 3.35342 avg_loss = 3.50393\n",
      "epoch no.4 train no.383830  loss = 4.97732 avg_loss = 3.47529\n",
      "epoch no.4 train no.383840  loss = 2.41169 avg_loss = 3.47003\n",
      "epoch no.4 train no.383850  loss = 4.01290 avg_loss = 3.48319\n",
      "epoch no.4 train no.383860  loss = 2.51194 avg_loss = 3.43449\n",
      "epoch no.4 train no.383870  loss = 2.87318 avg_loss = 3.44111\n",
      "epoch no.4 train no.383880  loss = 2.82153 avg_loss = 3.41566\n",
      "epoch no.4 train no.383890  loss = 4.32723 avg_loss = 3.45726\n",
      "epoch no.4 train no.383900  loss = 4.94941 avg_loss = 3.45520\n",
      "epoch no.4 train no.383910  loss = 2.49989 avg_loss = 3.42018\n",
      "epoch no.4 train no.383920  loss = 5.80206 avg_loss = 3.46681\n",
      "epoch no.4 train no.383930  loss = 4.08558 avg_loss = 3.48770\n",
      "epoch no.4 train no.383940  loss = 3.00102 avg_loss = 3.48061\n",
      "epoch no.4 train no.383950  loss = 4.01797 avg_loss = 3.50118\n",
      "epoch no.4 train no.383960  loss = 5.88164 avg_loss = 3.51768\n",
      "epoch no.4 train no.383970  loss = 4.39784 avg_loss = 3.52733\n",
      "epoch no.4 train no.383980  loss = 2.43049 avg_loss = 3.55211\n",
      "epoch no.4 train no.383990  loss = 1.84459 avg_loss = 3.54476\n",
      "epoch no.4 train no.384000  loss = 2.30873 avg_loss = 3.50603\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '싶', '을', '때', '</s>']\n",
      "기분전환하고싶을때</s>\n",
      "epoch no.4 train no.384010  loss = 3.92432 avg_loss = 3.49452\n",
      "epoch no.4 train no.384020  loss = 5.08581 avg_loss = 3.46983\n",
      "epoch no.4 train no.384030  loss = 1.86380 avg_loss = 3.49698\n",
      "epoch no.4 train no.384040  loss = 5.53733 avg_loss = 3.49599\n",
      "epoch no.4 train no.384050  loss = 2.47174 avg_loss = 3.48367\n",
      "epoch no.4 train no.384060  loss = 4.22056 avg_loss = 3.49157\n",
      "epoch no.4 train no.384070  loss = 4.33299 avg_loss = 3.47846\n",
      "epoch no.4 train no.384080  loss = 3.01223 avg_loss = 3.42849\n",
      "epoch no.4 train no.384090  loss = 3.57193 avg_loss = 3.41185\n",
      "epoch no.4 train no.384100  loss = 3.16041 avg_loss = 3.40639\n",
      "epoch no.4 train no.384110  loss = 3.30637 avg_loss = 3.37883\n",
      "epoch no.4 train no.384120  loss = 2.57733 avg_loss = 3.37434\n",
      "epoch no.4 train no.384130  loss = 2.92274 avg_loss = 3.34811\n",
      "epoch no.4 train no.384140  loss = 2.78854 avg_loss = 3.36652\n",
      "epoch no.4 train no.384150  loss = 4.32701 avg_loss = 3.36403\n",
      "epoch no.4 train no.384160  loss = 5.12253 avg_loss = 3.35342\n",
      "epoch no.4 train no.384170  loss = 4.76880 avg_loss = 3.30919\n",
      "epoch no.4 train no.384180  loss = 1.75086 avg_loss = 3.39535\n",
      "epoch no.4 train no.384190  loss = 4.13944 avg_loss = 3.43267\n",
      "epoch no.4 train no.384200  loss = 3.07053 avg_loss = 3.44510\n",
      "epoch no.4 train no.384210  loss = 2.54651 avg_loss = 3.48817\n",
      "epoch no.4 train no.384220  loss = 2.98986 avg_loss = 3.51797\n",
      "epoch no.4 train no.384230  loss = 3.47540 avg_loss = 3.53179\n",
      "epoch no.4 train no.384240  loss = 3.62120 avg_loss = 3.57276\n",
      "epoch no.4 train no.384250  loss = 4.25888 avg_loss = 3.56403\n",
      "epoch no.4 train no.384260  loss = 2.67354 avg_loss = 3.48826\n",
      "epoch no.4 train no.384270  loss = 3.77259 avg_loss = 3.47121\n",
      "epoch no.4 train no.384280  loss = 4.82696 avg_loss = 3.46718\n",
      "epoch no.4 train no.384290  loss = 6.45238 avg_loss = 3.47574\n",
      "epoch no.4 train no.384300  loss = 4.47150 avg_loss = 3.47918\n",
      "epoch no.4 train no.384310  loss = 3.97713 avg_loss = 3.51493\n",
      "epoch no.4 train no.384320  loss = 1.91760 avg_loss = 3.49006\n",
      "epoch no.4 train no.384330  loss = 3.40029 avg_loss = 3.44484\n",
      "epoch no.4 train no.384340  loss = 1.22175 avg_loss = 3.43016\n",
      "epoch no.4 train no.384350  loss = 3.16466 avg_loss = 3.41866\n",
      "epoch no.4 train no.384360  loss = 2.56161 avg_loss = 3.37241\n",
      "epoch no.4 train no.384370  loss = 2.45236 avg_loss = 3.43427\n",
      "epoch no.4 train no.384380  loss = 3.94946 avg_loss = 3.44921\n",
      "epoch no.4 train no.384390  loss = 3.37864 avg_loss = 3.45219\n",
      "epoch no.4 train no.384400  loss = 3.50116 avg_loss = 3.48920\n",
      "epoch no.4 train no.384410  loss = 3.61804 avg_loss = 3.45239\n",
      "epoch no.4 train no.384420  loss = 3.14295 avg_loss = 3.49550\n",
      "epoch no.4 train no.384430  loss = 3.76438 avg_loss = 3.45762\n",
      "epoch no.4 train no.384440  loss = 3.64576 avg_loss = 3.46420\n",
      "epoch no.4 train no.384450  loss = 3.35568 avg_loss = 3.46217\n",
      "epoch no.4 train no.384460  loss = 3.63502 avg_loss = 3.42098\n",
      "epoch no.4 train no.384470  loss = 3.06198 avg_loss = 3.42549\n",
      "epoch no.4 train no.384480  loss = 4.29801 avg_loss = 3.39567\n",
      "epoch no.4 train no.384490  loss = 1.81082 avg_loss = 3.35171\n",
      "epoch no.4 train no.384500  loss = 3.42084 avg_loss = 3.36862\n",
      "epoch no.4 train no.384510  loss = 1.51364 avg_loss = 3.34775\n",
      "epoch no.4 train no.384520  loss = 3.64078 avg_loss = 3.36505\n",
      "epoch no.4 train no.384530  loss = 4.33468 avg_loss = 3.40620\n",
      "epoch no.4 train no.384540  loss = 4.48676 avg_loss = 3.39068\n",
      "epoch no.4 train no.384550  loss = 2.50817 avg_loss = 3.43600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.384560  loss = 4.09751 avg_loss = 3.43311\n",
      "epoch no.4 train no.384570  loss = 2.58946 avg_loss = 3.40030\n",
      "epoch no.4 train no.384580  loss = 1.96722 avg_loss = 3.39092\n",
      "epoch no.4 train no.384590  loss = 2.72257 avg_loss = 3.38523\n",
      "epoch no.4 train no.384600  loss = 2.78229 avg_loss = 3.37177\n",
      "epoch no.4 train no.384610  loss = 5.09189 avg_loss = 3.35660\n",
      "epoch no.4 train no.384620  loss = 4.29687 avg_loss = 3.36929\n",
      "epoch no.4 train no.384630  loss = 4.10614 avg_loss = 3.36743\n",
      "epoch no.4 train no.384640  loss = 1.69366 avg_loss = 3.39524\n",
      "epoch no.4 train no.384650  loss = 6.36547 avg_loss = 3.39444\n",
      "epoch no.4 train no.384660  loss = 4.69727 avg_loss = 3.39511\n",
      "epoch no.4 train no.384670  loss = 2.59893 avg_loss = 3.38674\n",
      "epoch no.4 train no.384680  loss = 3.84191 avg_loss = 3.39209\n",
      "epoch no.4 train no.384690  loss = 4.16990 avg_loss = 3.34557\n",
      "epoch no.4 train no.384700  loss = 3.56483 avg_loss = 3.38169\n",
      "epoch no.4 train no.384710  loss = 3.84765 avg_loss = 3.45064\n",
      "epoch no.4 train no.384720  loss = 3.51362 avg_loss = 3.44087\n",
      "epoch no.4 train no.384730  loss = 1.71653 avg_loss = 3.45980\n",
      "epoch no.4 train no.384740  loss = 3.44373 avg_loss = 3.40965\n",
      "epoch no.4 train no.384750  loss = 2.38842 avg_loss = 3.34696\n",
      "epoch no.4 train no.384760  loss = 2.75934 avg_loss = 3.32772\n",
      "epoch no.4 train no.384770  loss = 2.67813 avg_loss = 3.31564\n",
      "epoch no.4 train no.384780  loss = 1.68144 avg_loss = 3.28585\n",
      "epoch no.4 train no.384790  loss = 2.57936 avg_loss = 3.26624\n",
      "epoch no.4 train no.384800  loss = 3.20593 avg_loss = 3.28425\n",
      "epoch no.4 train no.384810  loss = 4.17546 avg_loss = 3.33184\n",
      "epoch no.4 train no.384820  loss = 4.01590 avg_loss = 3.33841\n",
      "epoch no.4 train no.384830  loss = 2.45095 avg_loss = 3.30404\n",
      "epoch no.4 train no.384840  loss = 3.45456 avg_loss = 3.31741\n",
      "epoch no.4 train no.384850  loss = 5.14256 avg_loss = 3.34007\n",
      "epoch no.4 train no.384860  loss = 3.22820 avg_loss = 3.31412\n",
      "epoch no.4 train no.384870  loss = 4.11272 avg_loss = 3.32813\n",
      "epoch no.4 train no.384880  loss = 3.31305 avg_loss = 3.30716\n",
      "epoch no.4 train no.384890  loss = 3.24538 avg_loss = 3.32312\n",
      "epoch no.4 train no.384900  loss = 3.07363 avg_loss = 3.32644\n",
      "epoch no.4 train no.384910  loss = 2.36310 avg_loss = 3.28773\n",
      "epoch no.4 train no.384920  loss = 2.60849 avg_loss = 3.28708\n",
      "epoch no.4 train no.384930  loss = 2.82740 avg_loss = 3.26735\n",
      "epoch no.4 train no.384940  loss = 3.62822 avg_loss = 3.28863\n",
      "epoch no.4 train no.384950  loss = 2.58403 avg_loss = 3.33848\n",
      "epoch no.4 train no.384960  loss = 4.08756 avg_loss = 3.34364\n",
      "epoch no.4 train no.384970  loss = 2.44445 avg_loss = 3.32658\n",
      "epoch no.4 train no.384980  loss = 3.08999 avg_loss = 3.30207\n",
      "epoch no.4 train no.384990  loss = 3.11231 avg_loss = 3.30080\n",
      "epoch no.4 train no.385000  loss = 4.37471 avg_loss = 3.33932\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '싶', '▁때', '▁듣는']\n",
      "기분전환 하고 싶을때</s>\n",
      "epoch no.4 train no.385010  loss = 2.64893 avg_loss = 3.34505\n",
      "epoch no.4 train no.385020  loss = 3.52454 avg_loss = 3.37163\n",
      "epoch no.4 train no.385030  loss = 3.25831 avg_loss = 3.41394\n",
      "epoch no.4 train no.385040  loss = 2.94654 avg_loss = 3.39834\n",
      "epoch no.4 train no.385050  loss = 2.65819 avg_loss = 3.41632\n",
      "epoch no.4 train no.385060  loss = 2.48600 avg_loss = 3.37132\n",
      "epoch no.4 train no.385070  loss = 5.22836 avg_loss = 3.31586\n",
      "epoch no.4 train no.385080  loss = 3.60521 avg_loss = 3.30225\n",
      "epoch no.4 train no.385090  loss = 2.62157 avg_loss = 3.37094\n",
      "epoch no.4 train no.385100  loss = 2.88547 avg_loss = 3.35561\n",
      "epoch no.4 train no.385110  loss = 1.56887 avg_loss = 3.35828\n",
      "epoch no.4 train no.385120  loss = 4.21284 avg_loss = 3.35855\n",
      "epoch no.4 train no.385130  loss = 4.14745 avg_loss = 3.31113\n",
      "epoch no.4 train no.385140  loss = 2.03741 avg_loss = 3.33475\n",
      "epoch no.4 train no.385150  loss = 4.88842 avg_loss = 3.38782\n",
      "epoch no.4 train no.385160  loss = 3.28243 avg_loss = 3.38715\n",
      "epoch no.4 train no.385170  loss = 3.78564 avg_loss = 3.36196\n",
      "epoch no.4 train no.385180  loss = 2.60367 avg_loss = 3.34693\n",
      "epoch no.4 train no.385190  loss = 4.46494 avg_loss = 3.33057\n",
      "epoch no.4 train no.385200  loss = 2.85696 avg_loss = 3.32844\n",
      "epoch no.4 train no.385210  loss = 4.67679 avg_loss = 3.29568\n",
      "epoch no.4 train no.385220  loss = 2.38659 avg_loss = 3.30275\n",
      "epoch no.4 train no.385230  loss = 4.38914 avg_loss = 3.34146\n",
      "epoch no.4 train no.385240  loss = 3.81086 avg_loss = 3.36558\n",
      "epoch no.4 train no.385250  loss = 3.61951 avg_loss = 3.42579\n",
      "epoch no.4 train no.385260  loss = 2.19731 avg_loss = 3.42673\n",
      "epoch no.4 train no.385270  loss = 3.10660 avg_loss = 3.40523\n",
      "epoch no.4 train no.385280  loss = 2.07075 avg_loss = 3.38215\n",
      "epoch no.4 train no.385290  loss = 2.67019 avg_loss = 3.40538\n",
      "epoch no.4 train no.385300  loss = 3.93936 avg_loss = 3.45857\n",
      "epoch no.4 train no.385310  loss = 3.03236 avg_loss = 3.43704\n",
      "epoch no.4 train no.385320  loss = 2.08142 avg_loss = 3.42929\n",
      "epoch no.4 train no.385330  loss = 3.29189 avg_loss = 3.42548\n",
      "epoch no.4 train no.385340  loss = 2.82469 avg_loss = 3.39554\n",
      "epoch no.4 train no.385350  loss = 1.91540 avg_loss = 3.38863\n",
      "epoch no.4 train no.385360  loss = 3.16306 avg_loss = 3.36676\n",
      "epoch no.4 train no.385370  loss = 3.10063 avg_loss = 3.36233\n",
      "epoch no.4 train no.385380  loss = 2.76059 avg_loss = 3.38879\n",
      "epoch no.4 train no.385390  loss = 2.95286 avg_loss = 3.36004\n",
      "epoch no.4 train no.385400  loss = 3.28230 avg_loss = 3.36223\n",
      "epoch no.4 train no.385410  loss = 3.73350 avg_loss = 3.32683\n",
      "epoch no.4 train no.385420  loss = 4.12787 avg_loss = 3.30809\n",
      "epoch no.4 train no.385430  loss = 4.05781 avg_loss = 3.32177\n",
      "epoch no.4 train no.385440  loss = 3.66175 avg_loss = 3.37688\n",
      "epoch no.4 train no.385450  loss = 2.74993 avg_loss = 3.36770\n",
      "epoch no.4 train no.385460  loss = 2.55155 avg_loss = 3.32873\n",
      "epoch no.4 train no.385470  loss = 3.78753 avg_loss = 3.40363\n",
      "epoch no.4 train no.385480  loss = 2.91272 avg_loss = 3.34378\n",
      "epoch no.4 train no.385490  loss = 2.45021 avg_loss = 3.30787\n",
      "epoch no.4 train no.385500  loss = 3.13978 avg_loss = 3.35330\n",
      "epoch no.4 train no.385510  loss = 4.23401 avg_loss = 3.39525\n",
      "epoch no.4 train no.385520  loss = 2.94945 avg_loss = 3.41495\n",
      "epoch no.4 train no.385530  loss = 4.74819 avg_loss = 3.45142\n",
      "epoch no.4 train no.385540  loss = 2.60277 avg_loss = 3.45321\n",
      "epoch no.4 train no.385550  loss = 4.30106 avg_loss = 3.38044\n",
      "epoch no.4 train no.385560  loss = 2.33125 avg_loss = 3.41596\n",
      "epoch no.4 train no.385570  loss = 2.25383 avg_loss = 3.40325\n",
      "epoch no.4 train no.385580  loss = 3.89476 avg_loss = 3.45893\n",
      "epoch no.4 train no.385590  loss = 3.93370 avg_loss = 3.46506\n",
      "epoch no.4 train no.385600  loss = 3.16747 avg_loss = 3.45080\n",
      "epoch no.4 train no.385610  loss = 2.76572 avg_loss = 3.40865\n",
      "epoch no.4 train no.385620  loss = 4.08710 avg_loss = 3.41569\n",
      "epoch no.4 train no.385630  loss = 5.32574 avg_loss = 3.46567\n",
      "epoch no.4 train no.385640  loss = 4.18773 avg_loss = 3.45595\n",
      "epoch no.4 train no.385650  loss = 3.26536 avg_loss = 3.47361\n",
      "epoch no.4 train no.385660  loss = 2.39616 avg_loss = 3.48530\n",
      "epoch no.4 train no.385670  loss = 3.98759 avg_loss = 3.51247\n",
      "epoch no.4 train no.385680  loss = 4.45964 avg_loss = 3.53294\n",
      "epoch no.4 train no.385690  loss = 3.71731 avg_loss = 3.52258\n",
      "epoch no.4 train no.385700  loss = 3.10054 avg_loss = 3.51622\n",
      "epoch no.4 train no.385710  loss = 3.31047 avg_loss = 3.52707\n",
      "epoch no.4 train no.385720  loss = 2.99208 avg_loss = 3.51385\n",
      "epoch no.4 train no.385730  loss = 2.97942 avg_loss = 3.49435\n",
      "epoch no.4 train no.385740  loss = 3.13666 avg_loss = 3.44157\n",
      "epoch no.4 train no.385750  loss = 2.44232 avg_loss = 3.41626\n",
      "epoch no.4 train no.385760  loss = 2.40423 avg_loss = 3.43879\n",
      "epoch no.4 train no.385770  loss = 2.94303 avg_loss = 3.43304\n",
      "epoch no.4 train no.385780  loss = 2.38210 avg_loss = 3.42316\n",
      "epoch no.4 train no.385790  loss = 3.97466 avg_loss = 3.41763\n",
      "epoch no.4 train no.385800  loss = 2.36742 avg_loss = 3.39358\n",
      "epoch no.4 train no.385810  loss = 2.05491 avg_loss = 3.41269\n",
      "epoch no.4 train no.385820  loss = 5.08477 avg_loss = 3.44338\n",
      "epoch no.4 train no.385830  loss = 3.47403 avg_loss = 3.42312\n",
      "epoch no.4 train no.385840  loss = 3.00302 avg_loss = 3.47250\n",
      "epoch no.4 train no.385850  loss = 2.64830 avg_loss = 3.48569\n",
      "epoch no.4 train no.385860  loss = 3.03620 avg_loss = 3.46430\n",
      "epoch no.4 train no.385870  loss = 3.31567 avg_loss = 3.42392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.385880  loss = 3.40791 avg_loss = 3.40182\n",
      "epoch no.4 train no.385890  loss = 3.17218 avg_loss = 3.38015\n",
      "epoch no.4 train no.385900  loss = 2.73202 avg_loss = 3.37543\n",
      "epoch no.4 train no.385910  loss = 2.65018 avg_loss = 3.38171\n",
      "epoch no.4 train no.385920  loss = 3.07468 avg_loss = 3.33811\n",
      "epoch no.4 train no.385930  loss = 1.95833 avg_loss = 3.33142\n",
      "epoch no.4 train no.385940  loss = 3.40680 avg_loss = 3.32556\n",
      "epoch no.4 train no.385950  loss = 5.22007 avg_loss = 3.30473\n",
      "epoch no.4 train no.385960  loss = 3.92841 avg_loss = 3.27587\n",
      "epoch no.4 train no.385970  loss = 1.41976 avg_loss = 3.26686\n",
      "epoch no.4 train no.385980  loss = 1.60356 avg_loss = 3.21222\n",
      "epoch no.4 train no.385990  loss = 3.09869 avg_loss = 3.23372\n",
      "epoch no.4 train no.386000  loss = 3.05559 avg_loss = 3.26395\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁신나는', '▁노래', '</s>', '</s>']\n",
      "기분전환하기 좋은 신나는 노래 모음</s>\n",
      "epoch no.4 train no.386010  loss = 5.02658 avg_loss = 3.25930\n",
      "epoch no.4 train no.386020  loss = 3.10701 avg_loss = 3.24886\n",
      "epoch no.4 train no.386030  loss = 2.61101 avg_loss = 3.25685\n",
      "epoch no.4 train no.386040  loss = 2.72630 avg_loss = 3.29575\n",
      "epoch no.4 train no.386050  loss = 4.55550 avg_loss = 3.34777\n",
      "epoch no.4 train no.386060  loss = 3.14452 avg_loss = 3.34838\n",
      "epoch no.4 train no.386070  loss = 2.86809 avg_loss = 3.30812\n",
      "epoch no.4 train no.386080  loss = 3.84043 avg_loss = 3.35569\n",
      "epoch no.4 train no.386090  loss = 3.74719 avg_loss = 3.40850\n",
      "epoch no.4 train no.386100  loss = 1.99961 avg_loss = 3.39021\n",
      "epoch no.4 train no.386110  loss = 1.68138 avg_loss = 3.35777\n",
      "epoch no.4 train no.386120  loss = 3.75629 avg_loss = 3.37258\n",
      "epoch no.4 train no.386130  loss = 2.14798 avg_loss = 3.34836\n",
      "epoch no.4 train no.386140  loss = 3.14815 avg_loss = 3.36844\n",
      "epoch no.4 train no.386150  loss = 3.70690 avg_loss = 3.42758\n",
      "epoch no.4 train no.386160  loss = 4.08636 avg_loss = 3.43796\n",
      "epoch no.4 train no.386170  loss = 3.11386 avg_loss = 3.41393\n",
      "epoch no.4 train no.386180  loss = 2.26830 avg_loss = 3.39643\n",
      "epoch no.4 train no.386190  loss = 2.96891 avg_loss = 3.36395\n",
      "epoch no.4 train no.386200  loss = 2.81502 avg_loss = 3.35260\n",
      "epoch no.4 train no.386210  loss = 4.48156 avg_loss = 3.42483\n",
      "epoch no.4 train no.386220  loss = 1.85883 avg_loss = 3.33450\n",
      "epoch no.4 train no.386230  loss = 2.77308 avg_loss = 3.29608\n",
      "epoch no.4 train no.386240  loss = 2.47379 avg_loss = 3.28887\n",
      "epoch no.4 train no.386250  loss = 2.98022 avg_loss = 3.26440\n",
      "epoch no.4 train no.386260  loss = 3.84913 avg_loss = 3.27487\n",
      "epoch no.4 train no.386270  loss = 3.22699 avg_loss = 3.29200\n",
      "epoch no.4 train no.386280  loss = 1.89150 avg_loss = 3.31435\n",
      "epoch no.4 train no.386290  loss = 3.63702 avg_loss = 3.31552\n",
      "epoch no.4 train no.386300  loss = 2.59385 avg_loss = 3.33300\n",
      "epoch no.4 train no.386310  loss = 3.08438 avg_loss = 3.31464\n",
      "epoch no.4 train no.386320  loss = 3.60509 avg_loss = 3.36166\n",
      "epoch no.4 train no.386330  loss = 5.12378 avg_loss = 3.36391\n",
      "epoch no.4 train no.386340  loss = 3.71015 avg_loss = 3.36598\n",
      "epoch no.4 train no.386350  loss = 3.05696 avg_loss = 3.36575\n",
      "epoch no.4 train no.386360  loss = 2.50725 avg_loss = 3.34376\n",
      "epoch no.4 train no.386370  loss = 2.47975 avg_loss = 3.35280\n",
      "epoch no.4 train no.386380  loss = 3.40901 avg_loss = 3.38280\n",
      "epoch no.4 train no.386390  loss = 2.96238 avg_loss = 3.36339\n",
      "epoch no.4 train no.386400  loss = 2.43731 avg_loss = 3.33753\n",
      "epoch no.4 train no.386410  loss = 2.75458 avg_loss = 3.36470\n",
      "epoch no.4 train no.386420  loss = 3.43054 avg_loss = 3.37204\n",
      "epoch no.4 train no.386430  loss = 2.43264 avg_loss = 3.34186\n",
      "epoch no.4 train no.386440  loss = 2.44838 avg_loss = 3.35043\n",
      "epoch no.4 train no.386450  loss = 2.07948 avg_loss = 3.33665\n",
      "epoch no.4 train no.386460  loss = 4.66602 avg_loss = 3.32212\n",
      "epoch no.4 train no.386470  loss = 3.16432 avg_loss = 3.33676\n",
      "epoch no.4 train no.386480  loss = 4.82712 avg_loss = 3.33558\n",
      "epoch no.4 train no.386490  loss = 2.81980 avg_loss = 3.28286\n",
      "epoch no.4 train no.386500  loss = 3.33196 avg_loss = 3.30983\n",
      "epoch no.4 train no.386510  loss = 1.69773 avg_loss = 3.31515\n",
      "epoch no.4 train no.386520  loss = 2.32477 avg_loss = 3.30857\n",
      "epoch no.4 train no.386530  loss = 3.11969 avg_loss = 3.33297\n",
      "epoch no.4 train no.386540  loss = 3.74349 avg_loss = 3.32842\n",
      "epoch no.4 train no.386550  loss = 3.23790 avg_loss = 3.31568\n",
      "epoch no.4 train no.386560  loss = 3.13076 avg_loss = 3.32569\n",
      "epoch no.4 train no.386570  loss = 4.13008 avg_loss = 3.36673\n",
      "epoch no.4 train no.386580  loss = 1.80167 avg_loss = 3.31149\n",
      "epoch no.4 train no.386590  loss = 4.94543 avg_loss = 3.32574\n",
      "epoch no.4 train no.386600  loss = 3.33730 avg_loss = 3.31577\n",
      "epoch no.4 train no.386610  loss = 2.97835 avg_loss = 3.33013\n",
      "epoch no.4 train no.386620  loss = 3.85505 avg_loss = 3.32561\n",
      "epoch no.4 train no.386630  loss = 2.83392 avg_loss = 3.31944\n",
      "epoch no.4 train no.386640  loss = 3.32311 avg_loss = 3.31730\n",
      "epoch no.4 train no.386650  loss = 2.23164 avg_loss = 3.29678\n",
      "epoch no.4 train no.386660  loss = 3.51978 avg_loss = 3.32151\n",
      "epoch no.4 train no.386670  loss = 2.15779 avg_loss = 3.34471\n",
      "epoch no.4 train no.386680  loss = 2.32269 avg_loss = 3.34314\n",
      "epoch no.4 train no.386690  loss = 3.33915 avg_loss = 3.38846\n",
      "epoch no.4 train no.386700  loss = 2.86876 avg_loss = 3.38411\n",
      "epoch no.4 train no.386710  loss = 3.58686 avg_loss = 3.37078\n",
      "epoch no.4 train no.386720  loss = 2.82492 avg_loss = 3.35284\n",
      "epoch no.4 train no.386730  loss = 2.76885 avg_loss = 3.33466\n",
      "epoch no.4 train no.386740  loss = 2.38423 avg_loss = 3.29894\n",
      "epoch no.4 train no.386750  loss = 3.23770 avg_loss = 3.30786\n",
      "epoch no.4 train no.386760  loss = 2.90720 avg_loss = 3.31710\n",
      "epoch no.4 train no.386770  loss = 3.69228 avg_loss = 3.30747\n",
      "epoch no.4 train no.386780  loss = 3.33168 avg_loss = 3.31079\n",
      "epoch no.4 train no.386790  loss = 4.04085 avg_loss = 3.29152\n",
      "epoch no.4 train no.386800  loss = 4.31186 avg_loss = 3.29210\n",
      "epoch no.4 train no.386810  loss = 2.52999 avg_loss = 3.28354\n",
      "epoch no.4 train no.386820  loss = 3.77722 avg_loss = 3.31523\n",
      "epoch no.4 train no.386830  loss = 2.75203 avg_loss = 3.31376\n",
      "epoch no.4 train no.386840  loss = 5.36256 avg_loss = 3.38566\n",
      "epoch no.4 train no.386850  loss = 2.35716 avg_loss = 3.37643\n",
      "epoch no.4 train no.386860  loss = 2.44190 avg_loss = 3.36511\n",
      "epoch no.4 train no.386870  loss = 2.88030 avg_loss = 3.36164\n",
      "epoch no.4 train no.386880  loss = 3.52884 avg_loss = 3.36596\n",
      "epoch no.4 train no.386890  loss = 3.43795 avg_loss = 3.36022\n",
      "epoch no.4 train no.386900  loss = 3.21717 avg_loss = 3.34590\n",
      "epoch no.4 train no.386910  loss = 2.91425 avg_loss = 3.35009\n",
      "epoch no.4 train no.386920  loss = 2.57185 avg_loss = 3.32020\n",
      "epoch no.4 train no.386930  loss = 3.66398 avg_loss = 3.32830\n",
      "epoch no.4 train no.386940  loss = 2.94923 avg_loss = 3.37580\n",
      "epoch no.4 train no.386950  loss = 3.45685 avg_loss = 3.38695\n",
      "epoch no.4 train no.386960  loss = 3.34442 avg_loss = 3.37475\n",
      "epoch no.4 train no.386970  loss = 4.34158 avg_loss = 3.43582\n",
      "epoch no.4 train no.386980  loss = 3.15659 avg_loss = 3.40539\n",
      "epoch no.4 train no.386990  loss = 5.66342 avg_loss = 3.42662\n",
      "epoch no.4 train no.387000  loss = 2.58807 avg_loss = 3.42307\n",
      "4\n",
      "to_tokens: ['▁라디오', '전환', '이', '싶', '을', '때', '</s>']\n",
      "기분전환하고싶을때</s>\n",
      "epoch no.4 train no.387010  loss = 4.32726 avg_loss = 3.44539\n",
      "epoch no.4 train no.387020  loss = 4.76506 avg_loss = 3.49905\n",
      "epoch no.4 train no.387030  loss = 3.36737 avg_loss = 3.46388\n",
      "epoch no.4 train no.387040  loss = 3.93351 avg_loss = 3.43842\n",
      "epoch no.4 train no.387050  loss = 3.48592 avg_loss = 3.49912\n",
      "epoch no.4 train no.387060  loss = 1.91783 avg_loss = 3.46771\n",
      "epoch no.4 train no.387070  loss = 3.22395 avg_loss = 3.44263\n",
      "epoch no.4 train no.387080  loss = 3.90847 avg_loss = 3.37538\n",
      "epoch no.4 train no.387090  loss = 3.56148 avg_loss = 3.38390\n",
      "epoch no.4 train no.387100  loss = 5.07811 avg_loss = 3.40346\n",
      "epoch no.4 train no.387110  loss = 4.21718 avg_loss = 3.43491\n",
      "epoch no.4 train no.387120  loss = 2.81276 avg_loss = 3.40419\n",
      "epoch no.4 train no.387130  loss = 2.62352 avg_loss = 3.36151\n",
      "epoch no.4 train no.387140  loss = 4.99033 avg_loss = 3.36980\n",
      "epoch no.4 train no.387150  loss = 3.38622 avg_loss = 3.37613\n",
      "epoch no.4 train no.387160  loss = 1.84636 avg_loss = 3.37084\n",
      "epoch no.4 train no.387170  loss = 4.91780 avg_loss = 3.39443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.387180  loss = 4.43192 avg_loss = 3.45039\n",
      "epoch no.4 train no.387190  loss = 2.99068 avg_loss = 3.44857\n",
      "epoch no.4 train no.387200  loss = 1.84200 avg_loss = 3.43808\n",
      "epoch no.4 train no.387210  loss = 6.26758 avg_loss = 3.44402\n",
      "epoch no.4 train no.387220  loss = 2.12345 avg_loss = 3.37823\n",
      "epoch no.4 train no.387230  loss = 3.37865 avg_loss = 3.37890\n",
      "epoch no.4 train no.387240  loss = 3.66799 avg_loss = 3.37296\n",
      "epoch no.4 train no.387250  loss = 3.92605 avg_loss = 3.38796\n",
      "epoch no.4 train no.387260  loss = 2.83549 avg_loss = 3.34661\n",
      "epoch no.4 train no.387270  loss = 2.13720 avg_loss = 3.32280\n",
      "epoch no.4 train no.387280  loss = 3.69169 avg_loss = 3.33710\n",
      "epoch no.4 train no.387290  loss = 1.95386 avg_loss = 3.34325\n",
      "epoch no.4 train no.387300  loss = 4.16403 avg_loss = 3.38625\n",
      "epoch no.4 train no.387310  loss = 4.28004 avg_loss = 3.41063\n",
      "epoch no.4 train no.387320  loss = 2.37407 avg_loss = 3.37989\n",
      "epoch no.4 train no.387330  loss = 2.22228 avg_loss = 3.39624\n",
      "epoch no.4 train no.387340  loss = 3.99072 avg_loss = 3.37814\n",
      "epoch no.4 train no.387350  loss = 5.90108 avg_loss = 3.39677\n",
      "epoch no.4 train no.387360  loss = 3.73025 avg_loss = 3.38965\n",
      "epoch no.4 train no.387370  loss = 1.88123 avg_loss = 3.38006\n",
      "epoch no.4 train no.387380  loss = 4.57037 avg_loss = 3.38038\n",
      "epoch no.4 train no.387390  loss = 4.44508 avg_loss = 3.40895\n",
      "epoch no.4 train no.387400  loss = 2.70157 avg_loss = 3.41179\n",
      "epoch no.4 train no.387410  loss = 3.31281 avg_loss = 3.34849\n",
      "epoch no.4 train no.387420  loss = 3.25906 avg_loss = 3.31498\n",
      "epoch no.4 train no.387430  loss = 4.09490 avg_loss = 3.33848\n",
      "epoch no.4 train no.387440  loss = 2.58801 avg_loss = 3.32931\n",
      "epoch no.4 train no.387450  loss = 4.18644 avg_loss = 3.35177\n",
      "epoch no.4 train no.387460  loss = 3.89132 avg_loss = 3.37602\n",
      "epoch no.4 train no.387470  loss = 4.11889 avg_loss = 3.36589\n",
      "epoch no.4 train no.387480  loss = 4.02443 avg_loss = 3.34966\n",
      "epoch no.4 train no.387490  loss = 2.13844 avg_loss = 3.30333\n",
      "epoch no.4 train no.387500  loss = 3.33419 avg_loss = 3.31663\n",
      "epoch no.4 train no.387510  loss = 2.24765 avg_loss = 3.31992\n",
      "epoch no.4 train no.387520  loss = 2.47580 avg_loss = 3.34881\n",
      "epoch no.4 train no.387530  loss = 4.38753 avg_loss = 3.32734\n",
      "epoch no.4 train no.387540  loss = 3.08840 avg_loss = 3.30424\n",
      "epoch no.4 train no.387550  loss = 5.75792 avg_loss = 3.32768\n",
      "epoch no.4 train no.387560  loss = 4.82414 avg_loss = 3.33465\n",
      "epoch no.4 train no.387570  loss = 2.64077 avg_loss = 3.31962\n",
      "epoch no.4 train no.387580  loss = 5.20552 avg_loss = 3.35377\n",
      "epoch no.4 train no.387590  loss = 6.31383 avg_loss = 3.33057\n",
      "epoch no.4 train no.387600  loss = 2.99231 avg_loss = 3.31974\n",
      "epoch no.4 train no.387610  loss = 4.43688 avg_loss = 3.34276\n",
      "epoch no.4 train no.387620  loss = 3.26739 avg_loss = 3.34552\n",
      "epoch no.4 train no.387630  loss = 2.58520 avg_loss = 3.35100\n",
      "epoch no.4 train no.387640  loss = 2.80931 avg_loss = 3.33167\n",
      "epoch no.4 train no.387650  loss = 2.33495 avg_loss = 3.36213\n",
      "epoch no.4 train no.387660  loss = 4.72256 avg_loss = 3.35539\n",
      "epoch no.4 train no.387670  loss = 4.42110 avg_loss = 3.36525\n",
      "epoch no.4 train no.387680  loss = 2.62910 avg_loss = 3.39778\n",
      "epoch no.4 train no.387690  loss = 5.76583 avg_loss = 3.43769\n",
      "epoch no.4 train no.387700  loss = 3.06120 avg_loss = 3.40906\n",
      "epoch no.4 train no.387710  loss = 3.15017 avg_loss = 3.41908\n",
      "epoch no.4 train no.387720  loss = 2.83120 avg_loss = 3.43335\n",
      "epoch no.4 train no.387730  loss = 4.56006 avg_loss = 3.41794\n",
      "epoch no.4 train no.387740  loss = 5.05933 avg_loss = 3.41543\n",
      "epoch no.4 train no.387750  loss = 3.95461 avg_loss = 3.43878\n",
      "epoch no.4 train no.387760  loss = 3.55604 avg_loss = 3.44961\n",
      "epoch no.4 train no.387770  loss = 4.59413 avg_loss = 3.38968\n",
      "epoch no.4 train no.387780  loss = 2.57409 avg_loss = 3.39822\n",
      "epoch no.4 train no.387790  loss = 4.20394 avg_loss = 3.38348\n",
      "epoch no.4 train no.387800  loss = 2.02842 avg_loss = 3.37334\n",
      "epoch no.4 train no.387810  loss = 2.86446 avg_loss = 3.36847\n",
      "epoch no.4 train no.387820  loss = 4.20831 avg_loss = 3.34049\n",
      "epoch no.4 train no.387830  loss = 3.07709 avg_loss = 3.35129\n",
      "epoch no.4 train no.387840  loss = 3.54312 avg_loss = 3.40140\n",
      "epoch no.4 train no.387850  loss = 2.53813 avg_loss = 3.40048\n",
      "epoch no.4 train no.387860  loss = 1.56526 avg_loss = 3.37928\n",
      "epoch no.4 train no.387870  loss = 2.56883 avg_loss = 3.35139\n",
      "epoch no.4 train no.387880  loss = 3.54142 avg_loss = 3.34453\n",
      "epoch no.4 train no.387890  loss = 3.28535 avg_loss = 3.34146\n",
      "epoch no.4 train no.387900  loss = 3.10470 avg_loss = 3.31428\n",
      "epoch no.4 train no.387910  loss = 3.25961 avg_loss = 3.30468\n",
      "epoch no.4 train no.387920  loss = 4.11099 avg_loss = 3.29718\n",
      "epoch no.4 train no.387930  loss = 3.85957 avg_loss = 3.28128\n",
      "epoch no.4 train no.387940  loss = 2.43237 avg_loss = 3.28878\n",
      "epoch no.4 train no.387950  loss = 4.44928 avg_loss = 3.30399\n",
      "epoch no.4 train no.387960  loss = 2.27122 avg_loss = 3.33999\n",
      "epoch no.4 train no.387970  loss = 4.64077 avg_loss = 3.33429\n",
      "epoch no.4 train no.387980  loss = 3.24989 avg_loss = 3.32906\n",
      "epoch no.4 train no.387990  loss = 1.91453 avg_loss = 3.32435\n",
      "epoch no.4 train no.388000  loss = 1.89551 avg_loss = 3.37069\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁노래', '</s>']\n",
      "기분전환하기 좋은 음악</s>\n",
      "epoch no.4 train no.388010  loss = 4.04244 avg_loss = 3.33870\n",
      "epoch no.4 train no.388020  loss = 2.32690 avg_loss = 3.29882\n",
      "epoch no.4 train no.388030  loss = 2.57315 avg_loss = 3.29420\n",
      "epoch no.4 train no.388040  loss = 2.62237 avg_loss = 3.27154\n",
      "epoch no.4 train no.388050  loss = 3.01626 avg_loss = 3.28101\n",
      "epoch no.4 train no.388060  loss = 3.33888 avg_loss = 3.31868\n",
      "epoch no.4 train no.388070  loss = 3.06992 avg_loss = 3.31218\n",
      "epoch no.4 train no.388080  loss = 4.31604 avg_loss = 3.31339\n",
      "epoch no.4 train no.388090  loss = 5.03238 avg_loss = 3.32710\n",
      "epoch no.4 train no.388100  loss = 3.39859 avg_loss = 3.29660\n",
      "epoch no.4 train no.388110  loss = 3.46795 avg_loss = 3.33310\n",
      "epoch no.4 train no.388120  loss = 4.49818 avg_loss = 3.37253\n",
      "epoch no.4 train no.388130  loss = 3.55312 avg_loss = 3.35774\n",
      "epoch no.4 train no.388140  loss = 2.22684 avg_loss = 3.32262\n",
      "epoch no.4 train no.388150  loss = 5.29414 avg_loss = 3.36602\n",
      "epoch no.4 train no.388160  loss = 1.82875 avg_loss = 3.33671\n",
      "epoch no.4 train no.388170  loss = 5.53704 avg_loss = 3.36830\n",
      "epoch no.4 train no.388180  loss = 5.13686 avg_loss = 3.36525\n",
      "epoch no.4 train no.388190  loss = 6.18661 avg_loss = 3.38905\n",
      "epoch no.4 train no.388200  loss = 3.62626 avg_loss = 3.38915\n",
      "epoch no.4 train no.388210  loss = 3.20998 avg_loss = 3.40292\n",
      "epoch no.4 train no.388220  loss = 3.14152 avg_loss = 3.39625\n",
      "epoch no.4 train no.388230  loss = 3.28970 avg_loss = 3.39468\n",
      "epoch no.4 train no.388240  loss = 3.79576 avg_loss = 3.43082\n",
      "epoch no.4 train no.388250  loss = 3.07621 avg_loss = 3.38011\n",
      "epoch no.4 train no.388260  loss = 3.52823 avg_loss = 3.34490\n",
      "epoch no.4 train no.388270  loss = 3.57775 avg_loss = 3.31938\n",
      "epoch no.4 train no.388280  loss = 3.40232 avg_loss = 3.29644\n",
      "epoch no.4 train no.388290  loss = 3.84669 avg_loss = 3.30144\n",
      "epoch no.4 train no.388300  loss = 3.25144 avg_loss = 3.30231\n",
      "epoch no.4 train no.388310  loss = 4.99745 avg_loss = 3.36540\n",
      "epoch no.4 train no.388320  loss = 2.35776 avg_loss = 3.29593\n",
      "epoch no.4 train no.388330  loss = 1.97380 avg_loss = 3.31318\n",
      "epoch no.4 train no.388340  loss = 3.33453 avg_loss = 3.30364\n",
      "epoch no.4 train no.388350  loss = 2.75550 avg_loss = 3.31013\n",
      "epoch no.4 train no.388360  loss = 3.61818 avg_loss = 3.30851\n",
      "epoch no.4 train no.388370  loss = 3.13804 avg_loss = 3.28403\n",
      "epoch no.4 train no.388380  loss = 4.87597 avg_loss = 3.34740\n",
      "epoch no.4 train no.388390  loss = 2.68614 avg_loss = 3.35971\n",
      "epoch no.4 train no.388400  loss = 3.06371 avg_loss = 3.36497\n",
      "epoch no.4 train no.388410  loss = 4.49777 avg_loss = 3.41867\n",
      "epoch no.4 train no.388420  loss = 3.14175 avg_loss = 3.39972\n",
      "epoch no.4 train no.388430  loss = 2.98832 avg_loss = 3.40327\n",
      "epoch no.4 train no.388440  loss = 5.23445 avg_loss = 3.39828\n",
      "epoch no.4 train no.388450  loss = 2.62166 avg_loss = 3.41186\n",
      "epoch no.4 train no.388460  loss = 3.32426 avg_loss = 3.38149\n",
      "epoch no.4 train no.388470  loss = 3.87107 avg_loss = 3.38061\n",
      "epoch no.4 train no.388480  loss = 2.53490 avg_loss = 3.36122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.388490  loss = 3.32008 avg_loss = 3.34014\n",
      "epoch no.4 train no.388500  loss = 3.36409 avg_loss = 3.32416\n",
      "epoch no.4 train no.388510  loss = 2.60746 avg_loss = 3.31041\n",
      "epoch no.4 train no.388520  loss = 4.52754 avg_loss = 3.34207\n",
      "epoch no.4 train no.388530  loss = 3.79401 avg_loss = 3.33688\n",
      "epoch no.4 train no.388540  loss = 2.39724 avg_loss = 3.31447\n",
      "epoch no.4 train no.388550  loss = 3.64469 avg_loss = 3.31813\n",
      "epoch no.4 train no.388560  loss = 2.76919 avg_loss = 3.29684\n",
      "epoch no.4 train no.388570  loss = 3.58448 avg_loss = 3.31679\n",
      "epoch no.4 train no.388580  loss = 3.81323 avg_loss = 3.35195\n",
      "epoch no.4 train no.388590  loss = 3.86877 avg_loss = 3.33968\n",
      "epoch no.4 train no.388600  loss = 3.41076 avg_loss = 3.32792\n",
      "epoch no.4 train no.388610  loss = 2.26891 avg_loss = 3.30069\n",
      "epoch no.4 train no.388620  loss = 3.70129 avg_loss = 3.30673\n",
      "epoch no.4 train no.388630  loss = 6.08659 avg_loss = 3.33501\n",
      "epoch no.4 train no.388640  loss = 4.76816 avg_loss = 3.30045\n",
      "epoch no.4 train no.388650  loss = 2.71124 avg_loss = 3.26200\n",
      "epoch no.4 train no.388660  loss = 4.16903 avg_loss = 3.29948\n",
      "epoch no.4 train no.388670  loss = 3.93227 avg_loss = 3.31596\n",
      "epoch no.4 train no.388680  loss = 4.06837 avg_loss = 3.30053\n",
      "epoch no.4 train no.388690  loss = 2.99478 avg_loss = 3.33128\n",
      "epoch no.4 train no.388700  loss = 3.03243 avg_loss = 3.35349\n",
      "epoch no.4 train no.388710  loss = 3.29331 avg_loss = 3.38255\n",
      "epoch no.4 train no.388720  loss = 3.16959 avg_loss = 3.35248\n",
      "epoch no.4 train no.388730  loss = 3.77783 avg_loss = 3.34716\n",
      "epoch no.4 train no.388740  loss = 2.20103 avg_loss = 3.30656\n",
      "epoch no.4 train no.388750  loss = 4.20017 avg_loss = 3.31551\n",
      "epoch no.4 train no.388760  loss = 3.77375 avg_loss = 3.33397\n",
      "epoch no.4 train no.388770  loss = 4.89051 avg_loss = 3.32953\n",
      "epoch no.4 train no.388780  loss = 3.58730 avg_loss = 3.32828\n",
      "epoch no.4 train no.388790  loss = 2.82257 avg_loss = 3.31755\n",
      "epoch no.4 train no.388800  loss = 1.28261 avg_loss = 3.28209\n",
      "epoch no.4 train no.388810  loss = 2.96890 avg_loss = 3.31802\n",
      "epoch no.4 train no.388820  loss = 7.08179 avg_loss = 3.38155\n",
      "epoch no.4 train no.388830  loss = 4.66137 avg_loss = 3.34885\n",
      "epoch no.4 train no.388840  loss = 5.48622 avg_loss = 3.37003\n",
      "epoch no.4 train no.388850  loss = 2.19971 avg_loss = 3.36906\n",
      "epoch no.4 train no.388860  loss = 3.08807 avg_loss = 3.37364\n",
      "epoch no.4 train no.388870  loss = 2.37407 avg_loss = 3.37116\n",
      "epoch no.4 train no.388880  loss = 3.36184 avg_loss = 3.32235\n",
      "epoch no.4 train no.388890  loss = 5.02539 avg_loss = 3.34874\n",
      "epoch no.4 train no.388900  loss = 2.57250 avg_loss = 3.37184\n",
      "epoch no.4 train no.388910  loss = 2.68981 avg_loss = 3.33695\n",
      "epoch no.4 train no.388920  loss = 4.01975 avg_loss = 3.35259\n",
      "epoch no.4 train no.388930  loss = 2.39369 avg_loss = 3.28667\n",
      "epoch no.4 train no.388940  loss = 3.28451 avg_loss = 3.31515\n",
      "epoch no.4 train no.388950  loss = 2.07515 avg_loss = 3.33519\n",
      "epoch no.4 train no.388960  loss = 2.09041 avg_loss = 3.27335\n",
      "epoch no.4 train no.388970  loss = 3.88973 avg_loss = 3.25325\n",
      "epoch no.4 train no.388980  loss = 1.95403 avg_loss = 3.25506\n",
      "epoch no.4 train no.388990  loss = 3.91275 avg_loss = 3.26338\n",
      "epoch no.4 train no.389000  loss = 3.27177 avg_loss = 3.23546\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁필요할', '때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.4 train no.389010  loss = 3.43821 avg_loss = 3.19650\n",
      "epoch no.4 train no.389020  loss = 2.89337 avg_loss = 3.21901\n",
      "epoch no.4 train no.389030  loss = 2.18612 avg_loss = 3.26316\n",
      "epoch no.4 train no.389040  loss = 3.42783 avg_loss = 3.28631\n",
      "epoch no.4 train no.389050  loss = 2.30429 avg_loss = 3.26970\n",
      "epoch no.4 train no.389060  loss = 4.28044 avg_loss = 3.28295\n",
      "epoch no.4 train no.389070  loss = 2.69309 avg_loss = 3.26938\n",
      "epoch no.4 train no.389080  loss = 3.30108 avg_loss = 3.22986\n",
      "epoch no.4 train no.389090  loss = 3.61394 avg_loss = 3.23640\n",
      "epoch no.4 train no.389100  loss = 3.03521 avg_loss = 3.25114\n",
      "epoch no.4 train no.389110  loss = 2.97810 avg_loss = 3.27715\n",
      "epoch no.4 train no.389120  loss = 4.33692 avg_loss = 3.28106\n",
      "epoch no.4 train no.389130  loss = 4.38123 avg_loss = 3.27853\n",
      "epoch no.4 train no.389140  loss = 3.91133 avg_loss = 3.32669\n",
      "epoch no.4 train no.389150  loss = 1.50242 avg_loss = 3.28324\n",
      "epoch no.4 train no.389160  loss = 4.18023 avg_loss = 3.32608\n",
      "epoch no.4 train no.389170  loss = 2.25106 avg_loss = 3.31402\n",
      "epoch no.4 train no.389180  loss = 2.46107 avg_loss = 3.32361\n",
      "epoch no.4 train no.389190  loss = 3.19827 avg_loss = 3.29100\n",
      "epoch no.4 train no.389200  loss = 3.70581 avg_loss = 3.32272\n",
      "epoch no.4 train no.389210  loss = 4.40653 avg_loss = 3.36625\n",
      "epoch no.4 train no.389220  loss = 2.25035 avg_loss = 3.35624\n",
      "epoch no.4 train no.389230  loss = 2.72110 avg_loss = 3.39498\n",
      "epoch no.4 train no.389240  loss = 3.61931 avg_loss = 3.38575\n",
      "epoch no.4 train no.389250  loss = 2.32639 avg_loss = 3.37537\n",
      "epoch no.4 train no.389260  loss = 2.94944 avg_loss = 3.37264\n",
      "epoch no.4 train no.389270  loss = 3.73606 avg_loss = 3.36523\n",
      "epoch no.4 train no.389280  loss = 3.49127 avg_loss = 3.36310\n",
      "epoch no.4 train no.389290  loss = 3.72859 avg_loss = 3.38145\n",
      "epoch no.4 train no.389300  loss = 4.01843 avg_loss = 3.38069\n",
      "epoch no.4 train no.389310  loss = 5.34754 avg_loss = 3.43029\n",
      "epoch no.4 train no.389320  loss = 2.90786 avg_loss = 3.43268\n",
      "epoch no.4 train no.389330  loss = 3.69346 avg_loss = 3.39750\n",
      "epoch no.4 train no.389340  loss = 3.33485 avg_loss = 3.39647\n",
      "epoch no.4 train no.389350  loss = 5.55257 avg_loss = 3.36434\n",
      "epoch no.4 train no.389360  loss = 2.61437 avg_loss = 3.34692\n",
      "epoch no.4 train no.389370  loss = 3.74950 avg_loss = 3.38904\n",
      "epoch no.4 train no.389380  loss = 3.12275 avg_loss = 3.36236\n",
      "epoch no.4 train no.389390  loss = 2.37958 avg_loss = 3.35636\n",
      "epoch no.4 train no.389400  loss = 2.17086 avg_loss = 3.31159\n",
      "epoch no.4 train no.389410  loss = 1.77013 avg_loss = 3.30233\n",
      "epoch no.4 train no.389420  loss = 5.23793 avg_loss = 3.33137\n",
      "epoch no.4 train no.389430  loss = 3.07467 avg_loss = 3.36548\n",
      "epoch no.4 train no.389440  loss = 3.15662 avg_loss = 3.33678\n",
      "epoch no.4 train no.389450  loss = 5.81604 avg_loss = 3.36885\n",
      "epoch no.4 train no.389460  loss = 4.28040 avg_loss = 3.39826\n",
      "epoch no.4 train no.389470  loss = 3.44570 avg_loss = 3.41499\n",
      "epoch no.4 train no.389480  loss = 4.32493 avg_loss = 3.39973\n",
      "epoch no.4 train no.389490  loss = 3.89470 avg_loss = 3.40912\n",
      "epoch no.4 train no.389500  loss = 4.08197 avg_loss = 3.43913\n",
      "epoch no.4 train no.389510  loss = 3.66574 avg_loss = 3.46869\n",
      "epoch no.4 train no.389520  loss = 4.22819 avg_loss = 3.50393\n",
      "epoch no.4 train no.389530  loss = 0.71562 avg_loss = 3.47548\n",
      "epoch no.4 train no.389540  loss = 3.32540 avg_loss = 3.46980\n",
      "epoch no.4 train no.389550  loss = 2.49602 avg_loss = 3.43570\n",
      "epoch no.4 train no.389560  loss = 6.07389 avg_loss = 3.47772\n",
      "epoch no.4 train no.389570  loss = 3.81067 avg_loss = 3.46392\n",
      "epoch no.4 train no.389580  loss = 3.85168 avg_loss = 3.46339\n",
      "epoch no.4 train no.389590  loss = 2.40023 avg_loss = 3.47669\n",
      "epoch no.4 train no.389600  loss = 1.93833 avg_loss = 3.44484\n",
      "epoch no.4 train no.389610  loss = 2.25216 avg_loss = 3.37212\n",
      "epoch no.4 train no.389620  loss = 3.97420 avg_loss = 3.36399\n",
      "epoch no.4 train no.389630  loss = 3.65085 avg_loss = 3.40971\n",
      "epoch no.4 train no.389640  loss = 3.47027 avg_loss = 3.43052\n",
      "epoch no.4 train no.389650  loss = 4.81870 avg_loss = 3.44740\n",
      "epoch no.4 train no.389660  loss = 3.37976 avg_loss = 3.40805\n",
      "epoch no.4 train no.389670  loss = 2.91016 avg_loss = 3.44026\n",
      "epoch no.4 train no.389680  loss = 3.09175 avg_loss = 3.40980\n",
      "epoch no.4 train no.389690  loss = 2.83321 avg_loss = 3.39888\n",
      "epoch no.4 train no.389700  loss = 3.95555 avg_loss = 3.40122\n",
      "epoch no.4 train no.389710  loss = 2.08588 avg_loss = 3.37953\n",
      "epoch no.4 train no.389720  loss = 3.11906 avg_loss = 3.37079\n",
      "epoch no.4 train no.389730  loss = 3.14509 avg_loss = 3.42885\n",
      "epoch no.4 train no.389740  loss = 4.35902 avg_loss = 3.40240\n",
      "epoch no.4 train no.389750  loss = 2.57656 avg_loss = 3.40183\n",
      "epoch no.4 train no.389760  loss = 4.37751 avg_loss = 3.37543\n",
      "epoch no.4 train no.389770  loss = 2.56275 avg_loss = 3.35040\n",
      "epoch no.4 train no.389780  loss = 5.07062 avg_loss = 3.31851\n",
      "epoch no.4 train no.389790  loss = 4.03489 avg_loss = 3.32022\n",
      "epoch no.4 train no.389800  loss = 2.17932 avg_loss = 3.27762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.389810  loss = 4.59979 avg_loss = 3.29394\n",
      "epoch no.4 train no.389820  loss = 2.27908 avg_loss = 3.32559\n",
      "epoch no.4 train no.389830  loss = 4.28923 avg_loss = 3.31545\n",
      "epoch no.4 train no.389840  loss = 3.33624 avg_loss = 3.35776\n",
      "epoch no.4 train no.389850  loss = 3.12928 avg_loss = 3.39692\n",
      "epoch no.4 train no.389860  loss = 3.45762 avg_loss = 3.39609\n",
      "epoch no.4 train no.389870  loss = 2.93007 avg_loss = 3.41617\n",
      "epoch no.4 train no.389880  loss = 3.65912 avg_loss = 3.41316\n",
      "epoch no.4 train no.389890  loss = 3.17875 avg_loss = 3.40062\n",
      "epoch no.4 train no.389900  loss = 2.32279 avg_loss = 3.38546\n",
      "epoch no.4 train no.389910  loss = 2.41931 avg_loss = 3.37613\n",
      "epoch no.4 train no.389920  loss = 5.32423 avg_loss = 3.38234\n",
      "epoch no.4 train no.389930  loss = 2.76278 avg_loss = 3.34256\n",
      "epoch no.4 train no.389940  loss = 4.45200 avg_loss = 3.35486\n",
      "epoch no.4 train no.389950  loss = 2.97091 avg_loss = 3.33129\n",
      "epoch no.4 train no.389960  loss = 3.50353 avg_loss = 3.36312\n",
      "epoch no.4 train no.389970  loss = 2.12225 avg_loss = 3.36992\n",
      "epoch no.4 train no.389980  loss = 3.24994 avg_loss = 3.34626\n",
      "epoch no.4 train no.389990  loss = 5.26161 avg_loss = 3.36184\n",
      "epoch no.4 train no.390000  loss = 5.39932 avg_loss = 3.39942\n",
      "2\n",
      "to_tokens: ['▁비', '전환', '에', '▁노래', '</s>']\n",
      "기분전환 신나는 음악</s>\n",
      "epoch no.4 train no.390010  loss = 4.48617 avg_loss = 3.38113\n",
      "epoch no.4 train no.390020  loss = 3.87661 avg_loss = 3.34153\n",
      "epoch no.4 train no.390030  loss = 1.67447 avg_loss = 3.34296\n",
      "epoch no.4 train no.390040  loss = 2.93550 avg_loss = 3.39349\n",
      "epoch no.4 train no.390050  loss = 2.79790 avg_loss = 3.40925\n",
      "epoch no.4 train no.390060  loss = 4.62700 avg_loss = 3.38177\n",
      "epoch no.4 train no.390070  loss = 3.64274 avg_loss = 3.38707\n",
      "epoch no.4 train no.390080  loss = 2.71184 avg_loss = 3.40216\n",
      "epoch no.4 train no.390090  loss = 2.98345 avg_loss = 3.44424\n",
      "epoch no.4 train no.390100  loss = 3.20358 avg_loss = 3.45225\n",
      "epoch no.4 train no.390110  loss = 3.69954 avg_loss = 3.42991\n",
      "epoch no.4 train no.390120  loss = 3.05392 avg_loss = 3.43315\n",
      "epoch no.4 train no.390130  loss = 3.64204 avg_loss = 3.42586\n",
      "epoch no.4 train no.390140  loss = 5.41022 avg_loss = 3.42705\n",
      "epoch no.4 train no.390150  loss = 3.65112 avg_loss = 3.42614\n",
      "epoch no.4 train no.390160  loss = 3.60182 avg_loss = 3.42043\n",
      "epoch no.4 train no.390170  loss = 2.80591 avg_loss = 3.37313\n",
      "epoch no.4 train no.390180  loss = 2.83185 avg_loss = 3.33618\n",
      "epoch no.4 train no.390190  loss = 3.93192 avg_loss = 3.33507\n",
      "epoch no.4 train no.390200  loss = 2.32963 avg_loss = 3.34041\n",
      "epoch no.4 train no.390210  loss = 2.91252 avg_loss = 3.32551\n",
      "epoch no.4 train no.390220  loss = 2.70590 avg_loss = 3.32875\n",
      "epoch no.4 train no.390230  loss = 3.21593 avg_loss = 3.37276\n",
      "epoch no.4 train no.390240  loss = 2.75551 avg_loss = 3.36366\n",
      "epoch no.4 train no.390250  loss = 2.63085 avg_loss = 3.38060\n",
      "epoch no.4 train no.390260  loss = 4.29889 avg_loss = 3.38474\n",
      "epoch no.4 train no.390270  loss = 3.27294 avg_loss = 3.39482\n",
      "epoch no.4 train no.390280  loss = 3.76066 avg_loss = 3.35380\n",
      "epoch no.4 train no.390290  loss = 2.07691 avg_loss = 3.32695\n",
      "epoch no.4 train no.390300  loss = 3.95712 avg_loss = 3.32511\n",
      "epoch no.4 train no.390310  loss = 3.65997 avg_loss = 3.28495\n",
      "epoch no.4 train no.390320  loss = 3.11695 avg_loss = 3.33085\n",
      "epoch no.4 train no.390330  loss = 4.11597 avg_loss = 3.36148\n",
      "epoch no.4 train no.390340  loss = 4.00240 avg_loss = 3.36880\n",
      "epoch no.4 train no.390350  loss = 5.48155 avg_loss = 3.36994\n",
      "epoch no.4 train no.390360  loss = 4.82014 avg_loss = 3.33285\n",
      "epoch no.4 train no.390370  loss = 4.80303 avg_loss = 3.33579\n",
      "epoch no.4 train no.390380  loss = 3.62212 avg_loss = 3.35537\n",
      "epoch no.4 train no.390390  loss = 3.88488 avg_loss = 3.38351\n",
      "epoch no.4 train no.390400  loss = 2.47610 avg_loss = 3.36099\n",
      "epoch no.4 train no.390410  loss = 2.89278 avg_loss = 3.34465\n",
      "epoch no.4 train no.390420  loss = 3.81362 avg_loss = 3.37634\n",
      "epoch no.4 train no.390430  loss = 2.89171 avg_loss = 3.36271\n",
      "epoch no.4 train no.390440  loss = 1.95339 avg_loss = 3.39236\n",
      "epoch no.4 train no.390450  loss = 1.80172 avg_loss = 3.34037\n",
      "epoch no.4 train no.390460  loss = 2.06946 avg_loss = 3.31424\n",
      "epoch no.4 train no.390470  loss = 5.17699 avg_loss = 3.35326\n",
      "epoch no.4 train no.390480  loss = 3.93256 avg_loss = 3.37157\n",
      "epoch no.4 train no.390490  loss = 3.17021 avg_loss = 3.38917\n",
      "epoch no.4 train no.390500  loss = 3.49275 avg_loss = 3.37155\n",
      "epoch no.4 train no.390510  loss = 3.86713 avg_loss = 3.37285\n",
      "epoch no.4 train no.390520  loss = 2.91550 avg_loss = 3.37061\n",
      "epoch no.4 train no.390530  loss = 2.33755 avg_loss = 3.37719\n",
      "epoch no.4 train no.390540  loss = 4.75706 avg_loss = 3.38431\n",
      "epoch no.4 train no.390550  loss = 4.01169 avg_loss = 3.41350\n",
      "epoch no.4 train no.390560  loss = 3.68662 avg_loss = 3.42394\n",
      "epoch no.4 train no.390570  loss = 2.73373 avg_loss = 3.41602\n",
      "epoch no.4 train no.390580  loss = 3.78472 avg_loss = 3.41186\n",
      "epoch no.4 train no.390590  loss = 3.46913 avg_loss = 3.42161\n",
      "epoch no.4 train no.390600  loss = 4.53312 avg_loss = 3.45500\n",
      "epoch no.4 train no.390610  loss = 4.24607 avg_loss = 3.46198\n",
      "epoch no.4 train no.390620  loss = 4.31631 avg_loss = 3.46437\n",
      "epoch no.4 train no.390630  loss = 5.32432 avg_loss = 3.46521\n",
      "epoch no.4 train no.390640  loss = 3.28007 avg_loss = 3.52809\n",
      "epoch no.4 train no.390650  loss = 5.07322 avg_loss = 3.54450\n",
      "epoch no.4 train no.390660  loss = 3.63843 avg_loss = 3.55565\n",
      "epoch no.4 train no.390670  loss = 4.49736 avg_loss = 3.59397\n",
      "epoch no.4 train no.390680  loss = 3.84330 avg_loss = 3.56563\n",
      "epoch no.4 train no.390690  loss = 3.36986 avg_loss = 3.61919\n",
      "epoch no.4 train no.390700  loss = 2.49703 avg_loss = 3.57216\n",
      "epoch no.4 train no.390710  loss = 4.74485 avg_loss = 3.52616\n",
      "epoch no.4 train no.390720  loss = 2.87550 avg_loss = 3.53044\n",
      "epoch no.4 train no.390730  loss = 2.74087 avg_loss = 3.50388\n",
      "epoch no.4 train no.390740  loss = 2.08981 avg_loss = 3.48891\n",
      "epoch no.4 train no.390750  loss = 5.04340 avg_loss = 3.52496\n",
      "epoch no.4 train no.390760  loss = 1.54900 avg_loss = 3.46698\n",
      "epoch no.4 train no.390770  loss = 2.19756 avg_loss = 3.46171\n",
      "epoch no.4 train no.390780  loss = 2.85211 avg_loss = 3.47324\n",
      "epoch no.4 train no.390790  loss = 4.50744 avg_loss = 3.47034\n",
      "epoch no.4 train no.390800  loss = 4.67823 avg_loss = 3.46748\n",
      "epoch no.4 train no.390810  loss = 4.76313 avg_loss = 3.45337\n",
      "epoch no.4 train no.390820  loss = 3.43240 avg_loss = 3.46963\n",
      "epoch no.4 train no.390830  loss = 2.95676 avg_loss = 3.46445\n",
      "epoch no.4 train no.390840  loss = 3.05657 avg_loss = 3.43936\n",
      "epoch no.4 train no.390850  loss = 2.54570 avg_loss = 3.44528\n",
      "epoch no.4 train no.390860  loss = 3.19900 avg_loss = 3.40698\n",
      "epoch no.4 train no.390870  loss = 3.68011 avg_loss = 3.38881\n",
      "epoch no.4 train no.390880  loss = 3.94379 avg_loss = 3.41207\n",
      "epoch no.4 train no.390890  loss = 3.22925 avg_loss = 3.36901\n",
      "epoch no.4 train no.390900  loss = 2.93655 avg_loss = 3.35226\n",
      "epoch no.4 train no.390910  loss = 3.01194 avg_loss = 3.32983\n",
      "epoch no.4 train no.390920  loss = 2.46076 avg_loss = 3.30543\n",
      "epoch no.4 train no.390930  loss = 4.20680 avg_loss = 3.32282\n",
      "epoch no.4 train no.390940  loss = 4.09743 avg_loss = 3.33102\n",
      "epoch no.4 train no.390950  loss = 3.22972 avg_loss = 3.33501\n",
      "epoch no.4 train no.390960  loss = 4.22867 avg_loss = 3.33656\n",
      "epoch no.4 train no.390970  loss = 2.80722 avg_loss = 3.37901\n",
      "epoch no.4 train no.390980  loss = 4.22663 avg_loss = 3.39844\n",
      "epoch no.4 train no.390990  loss = 1.71675 avg_loss = 3.33589\n",
      "epoch no.4 train no.391000  loss = 3.39751 avg_loss = 3.31227\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '에', '▁좋은', '▁노래', '들', '</s>']\n",
      "기분전환에 좋은 노래들</s>\n",
      "epoch no.4 train no.391010  loss = 3.99200 avg_loss = 3.34681\n",
      "epoch no.4 train no.391020  loss = 3.34837 avg_loss = 3.28722\n",
      "epoch no.4 train no.391030  loss = 3.54442 avg_loss = 3.32566\n",
      "epoch no.4 train no.391040  loss = 1.86676 avg_loss = 3.26753\n",
      "epoch no.4 train no.391050  loss = 3.26428 avg_loss = 3.26228\n",
      "epoch no.4 train no.391060  loss = 2.66432 avg_loss = 3.25895\n",
      "epoch no.4 train no.391070  loss = 2.13434 avg_loss = 3.26467\n",
      "epoch no.4 train no.391080  loss = 3.30502 avg_loss = 3.25421\n",
      "epoch no.4 train no.391090  loss = 3.16268 avg_loss = 3.22839\n",
      "epoch no.4 train no.391100  loss = 5.30351 avg_loss = 3.27993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.391110  loss = 3.83909 avg_loss = 3.29494\n",
      "epoch no.4 train no.391120  loss = 2.15119 avg_loss = 3.30147\n",
      "epoch no.4 train no.391130  loss = 3.23648 avg_loss = 3.29163\n",
      "epoch no.4 train no.391140  loss = 3.04569 avg_loss = 3.27827\n",
      "epoch no.4 train no.391150  loss = 2.24765 avg_loss = 3.26032\n",
      "epoch no.4 train no.391160  loss = 5.37352 avg_loss = 3.27128\n",
      "epoch no.4 train no.391170  loss = 3.43926 avg_loss = 3.29288\n",
      "epoch no.4 train no.391180  loss = 3.03073 avg_loss = 3.26785\n",
      "epoch no.4 train no.391190  loss = 3.99060 avg_loss = 3.31065\n",
      "epoch no.4 train no.391200  loss = 5.49367 avg_loss = 3.28505\n",
      "epoch no.4 train no.391210  loss = 3.12731 avg_loss = 3.27121\n",
      "epoch no.4 train no.391220  loss = 2.35866 avg_loss = 3.28050\n",
      "epoch no.4 train no.391230  loss = 2.86884 avg_loss = 3.27766\n",
      "epoch no.4 train no.391240  loss = 5.10945 avg_loss = 3.29118\n",
      "epoch no.4 train no.391250  loss = 3.76276 avg_loss = 3.29835\n",
      "epoch no.4 train no.391260  loss = 4.29253 avg_loss = 3.28573\n",
      "epoch no.4 train no.391270  loss = 2.60504 avg_loss = 3.26826\n",
      "epoch no.4 train no.391280  loss = 2.21325 avg_loss = 3.26103\n",
      "epoch no.4 train no.391290  loss = 3.82329 avg_loss = 3.23310\n",
      "epoch no.4 train no.391300  loss = 3.33283 avg_loss = 3.24973\n",
      "epoch no.4 train no.391310  loss = 2.53037 avg_loss = 3.26942\n",
      "epoch no.4 train no.391320  loss = 2.84005 avg_loss = 3.27481\n",
      "epoch no.4 train no.391330  loss = 2.89296 avg_loss = 3.28100\n",
      "epoch no.4 train no.391340  loss = 3.54487 avg_loss = 3.28460\n",
      "epoch no.4 train no.391350  loss = 4.44475 avg_loss = 3.30590\n",
      "epoch no.4 train no.391360  loss = 3.00055 avg_loss = 3.33129\n",
      "epoch no.4 train no.391370  loss = 2.63835 avg_loss = 3.32556\n",
      "epoch no.4 train no.391380  loss = 2.65080 avg_loss = 3.33562\n",
      "epoch no.4 train no.391390  loss = 3.36681 avg_loss = 3.34961\n",
      "epoch no.4 train no.391400  loss = 4.87157 avg_loss = 3.35133\n",
      "epoch no.4 train no.391410  loss = 2.36848 avg_loss = 3.31463\n",
      "epoch no.4 train no.391420  loss = 4.22657 avg_loss = 3.32015\n",
      "epoch no.4 train no.391430  loss = 3.36101 avg_loss = 3.33986\n",
      "epoch no.4 train no.391440  loss = 2.84264 avg_loss = 3.33255\n",
      "epoch no.4 train no.391450  loss = 2.75921 avg_loss = 3.36345\n",
      "epoch no.4 train no.391460  loss = 3.07722 avg_loss = 3.34469\n",
      "epoch no.4 train no.391470  loss = 5.65932 avg_loss = 3.41662\n",
      "epoch no.4 train no.391480  loss = 2.33144 avg_loss = 3.37181\n",
      "epoch no.4 train no.391490  loss = 4.04854 avg_loss = 3.40091\n",
      "epoch no.4 train no.391500  loss = 2.33741 avg_loss = 3.42826\n",
      "epoch no.4 train no.391510  loss = 3.13374 avg_loss = 3.42376\n",
      "epoch no.4 train no.391520  loss = 2.66554 avg_loss = 3.41989\n",
      "epoch no.4 train no.391530  loss = 2.97068 avg_loss = 3.39257\n",
      "epoch no.4 train no.391540  loss = 3.34015 avg_loss = 3.38453\n",
      "epoch no.4 train no.391550  loss = 3.46573 avg_loss = 3.38215\n",
      "epoch no.4 train no.391560  loss = 2.21073 avg_loss = 3.35720\n",
      "epoch no.4 train no.391570  loss = 4.52213 avg_loss = 3.38300\n",
      "epoch no.4 train no.391580  loss = 3.98845 avg_loss = 3.40677\n",
      "epoch no.4 train no.391590  loss = 2.75702 avg_loss = 3.34225\n",
      "epoch no.4 train no.391600  loss = 4.94033 avg_loss = 3.35572\n",
      "epoch no.4 train no.391610  loss = 2.58569 avg_loss = 3.35155\n",
      "epoch no.4 train no.391620  loss = 3.67358 avg_loss = 3.35296\n",
      "epoch no.4 train no.391630  loss = 4.37723 avg_loss = 3.36369\n",
      "epoch no.4 train no.391640  loss = 3.37505 avg_loss = 3.37443\n",
      "epoch no.4 train no.391650  loss = 3.44090 avg_loss = 3.36525\n",
      "epoch no.4 train no.391660  loss = 4.06814 avg_loss = 3.36441\n",
      "epoch no.4 train no.391670  loss = 3.52369 avg_loss = 3.41888\n",
      "epoch no.4 train no.391680  loss = 2.65662 avg_loss = 3.41646\n",
      "epoch no.4 train no.391690  loss = 3.00346 avg_loss = 3.36056\n",
      "epoch no.4 train no.391700  loss = 3.13605 avg_loss = 3.35481\n",
      "epoch no.4 train no.391710  loss = 2.93613 avg_loss = 3.31172\n",
      "epoch no.4 train no.391720  loss = 3.66368 avg_loss = 3.31884\n",
      "epoch no.4 train no.391730  loss = 4.38240 avg_loss = 3.31267\n",
      "epoch no.4 train no.391740  loss = 2.83640 avg_loss = 3.25081\n",
      "epoch no.4 train no.391750  loss = 2.62972 avg_loss = 3.23008\n",
      "epoch no.4 train no.391760  loss = 2.98418 avg_loss = 3.23946\n",
      "epoch no.4 train no.391770  loss = 4.02017 avg_loss = 3.25635\n",
      "epoch no.4 train no.391780  loss = 2.31293 avg_loss = 3.26875\n",
      "epoch no.4 train no.391790  loss = 1.65440 avg_loss = 3.30483\n",
      "epoch no.4 train no.391800  loss = 4.07046 avg_loss = 3.26028\n",
      "epoch no.4 train no.391810  loss = 2.41458 avg_loss = 3.26712\n",
      "epoch no.4 train no.391820  loss = 4.02474 avg_loss = 3.23740\n",
      "epoch no.4 train no.391830  loss = 1.61157 avg_loss = 3.22196\n",
      "epoch no.4 train no.391840  loss = 4.27161 avg_loss = 3.27635\n",
      "epoch no.4 train no.391850  loss = 3.45586 avg_loss = 3.35682\n",
      "epoch no.4 train no.391860  loss = 1.89206 avg_loss = 3.31553\n",
      "epoch no.4 train no.391870  loss = 2.75004 avg_loss = 3.28184\n",
      "epoch no.4 train no.391880  loss = 3.35486 avg_loss = 3.27131\n",
      "epoch no.4 train no.391890  loss = 2.89041 avg_loss = 3.24154\n",
      "epoch no.4 train no.391900  loss = 3.21958 avg_loss = 3.26886\n",
      "epoch no.4 train no.391910  loss = 3.27378 avg_loss = 3.28444\n",
      "epoch no.4 train no.391920  loss = 3.82924 avg_loss = 3.29484\n",
      "epoch no.4 train no.391930  loss = 5.14528 avg_loss = 3.32158\n",
      "epoch no.4 train no.391940  loss = 3.55540 avg_loss = 3.33535\n",
      "epoch no.4 train no.391950  loss = 2.61434 avg_loss = 3.30425\n",
      "epoch no.4 train no.391960  loss = 3.83910 avg_loss = 3.30055\n",
      "epoch no.4 train no.391970  loss = 2.62582 avg_loss = 3.28503\n",
      "epoch no.4 train no.391980  loss = 3.20178 avg_loss = 3.26490\n",
      "epoch no.4 train no.391990  loss = 3.00430 avg_loss = 3.23628\n",
      "epoch no.4 train no.392000  loss = 2.31682 avg_loss = 3.22983\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁좋은', '▁신나는', '</s>']\n",
      "기분전환하기 좋은 노래</s>\n",
      "epoch no.4 train no.392010  loss = 4.78144 avg_loss = 3.26322\n",
      "epoch no.4 train no.392020  loss = 3.18594 avg_loss = 3.26327\n",
      "epoch no.4 train no.392030  loss = 4.29494 avg_loss = 3.26554\n",
      "epoch no.4 train no.392040  loss = 1.71048 avg_loss = 3.22230\n",
      "epoch no.4 train no.392050  loss = 2.32077 avg_loss = 3.21119\n",
      "epoch no.4 train no.392060  loss = 1.84282 avg_loss = 3.21501\n",
      "epoch no.4 train no.392070  loss = 3.44484 avg_loss = 3.21898\n",
      "epoch no.4 train no.392080  loss = 3.78654 avg_loss = 3.25669\n",
      "epoch no.4 train no.392090  loss = 2.45330 avg_loss = 3.28069\n",
      "epoch no.4 train no.392100  loss = 2.85829 avg_loss = 3.33556\n",
      "epoch no.4 train no.392110  loss = 2.73123 avg_loss = 3.31592\n",
      "epoch no.4 train no.392120  loss = 4.71786 avg_loss = 3.31623\n",
      "epoch no.4 train no.392130  loss = 4.79435 avg_loss = 3.28468\n",
      "epoch no.4 train no.392140  loss = 2.33935 avg_loss = 3.25350\n",
      "epoch no.4 train no.392150  loss = 4.06256 avg_loss = 3.33571\n",
      "epoch no.4 train no.392160  loss = 3.53582 avg_loss = 3.36150\n",
      "epoch no.4 train no.392170  loss = 4.59580 avg_loss = 3.37202\n",
      "epoch no.4 train no.392180  loss = 3.17374 avg_loss = 3.36282\n",
      "epoch no.4 train no.392190  loss = 4.18978 avg_loss = 3.38908\n",
      "epoch no.4 train no.392200  loss = 3.15560 avg_loss = 3.35100\n",
      "epoch no.4 train no.392210  loss = 2.55584 avg_loss = 3.34735\n",
      "epoch no.4 train no.392220  loss = 3.01562 avg_loss = 3.35849\n",
      "epoch no.4 train no.392230  loss = 2.84492 avg_loss = 3.35177\n",
      "epoch no.4 train no.392240  loss = 2.35219 avg_loss = 3.32089\n",
      "epoch no.4 train no.392250  loss = 3.75650 avg_loss = 3.36010\n",
      "epoch no.4 train no.392260  loss = 3.31558 avg_loss = 3.41321\n",
      "epoch no.4 train no.392270  loss = 1.94611 avg_loss = 3.37541\n",
      "epoch no.4 train no.392280  loss = 3.64494 avg_loss = 3.33150\n",
      "epoch no.4 train no.392290  loss = 5.15314 avg_loss = 3.34421\n",
      "epoch no.4 train no.392300  loss = 2.64614 avg_loss = 3.34014\n",
      "epoch no.4 train no.392310  loss = 2.86487 avg_loss = 3.36572\n",
      "epoch no.4 train no.392320  loss = 4.61844 avg_loss = 3.37903\n",
      "epoch no.4 train no.392330  loss = 4.10591 avg_loss = 3.38187\n",
      "epoch no.4 train no.392340  loss = 2.68689 avg_loss = 3.38958\n",
      "epoch no.4 train no.392350  loss = 3.91278 avg_loss = 3.39289\n",
      "epoch no.4 train no.392360  loss = 4.23198 avg_loss = 3.42036\n",
      "epoch no.4 train no.392370  loss = 3.10980 avg_loss = 3.43974\n",
      "epoch no.4 train no.392380  loss = 4.28648 avg_loss = 3.45809\n",
      "epoch no.4 train no.392390  loss = 4.42071 avg_loss = 3.44413\n",
      "epoch no.4 train no.392400  loss = 3.49295 avg_loss = 3.42587\n",
      "epoch no.4 train no.392410  loss = 3.46433 avg_loss = 3.44485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.392420  loss = 3.34064 avg_loss = 3.44186\n",
      "epoch no.4 train no.392430  loss = 2.28474 avg_loss = 3.40290\n",
      "epoch no.4 train no.392440  loss = 3.15706 avg_loss = 3.39342\n",
      "epoch no.4 train no.392450  loss = 3.74479 avg_loss = 3.39722\n",
      "epoch no.4 train no.392460  loss = 4.22269 avg_loss = 3.41167\n",
      "epoch no.4 train no.392470  loss = 4.54362 avg_loss = 3.43225\n",
      "epoch no.4 train no.392480  loss = 2.60799 avg_loss = 3.46333\n",
      "epoch no.4 train no.392490  loss = 2.81973 avg_loss = 3.44558\n",
      "epoch no.4 train no.392500  loss = 2.82300 avg_loss = 3.42706\n",
      "epoch no.4 train no.392510  loss = 2.67922 avg_loss = 3.42160\n",
      "epoch no.4 train no.392520  loss = 1.85138 avg_loss = 3.38841\n",
      "epoch no.4 train no.392530  loss = 1.98914 avg_loss = 3.31176\n",
      "epoch no.4 train no.392540  loss = 3.62988 avg_loss = 3.32705\n",
      "epoch no.4 train no.392550  loss = 3.80633 avg_loss = 3.36068\n",
      "epoch no.4 train no.392560  loss = 2.62246 avg_loss = 3.32493\n",
      "epoch no.4 train no.392570  loss = 3.26204 avg_loss = 3.33657\n",
      "epoch no.4 train no.392580  loss = 2.75000 avg_loss = 3.36068\n",
      "epoch no.4 train no.392590  loss = 2.84010 avg_loss = 3.33403\n",
      "epoch no.4 train no.392600  loss = 4.22237 avg_loss = 3.37016\n",
      "epoch no.4 train no.392610  loss = 3.50181 avg_loss = 3.33098\n",
      "epoch no.4 train no.392620  loss = 5.53003 avg_loss = 3.35224\n",
      "epoch no.4 train no.392630  loss = 4.66017 avg_loss = 3.38124\n",
      "epoch no.4 train no.392640  loss = 2.18675 avg_loss = 3.32393\n",
      "epoch no.4 train no.392650  loss = 2.63074 avg_loss = 3.31015\n",
      "epoch no.4 train no.392660  loss = 4.49353 avg_loss = 3.35074\n",
      "epoch no.4 train no.392670  loss = 3.29164 avg_loss = 3.33295\n",
      "epoch no.4 train no.392680  loss = 4.09550 avg_loss = 3.39443\n",
      "epoch no.4 train no.392690  loss = 3.22468 avg_loss = 3.40150\n",
      "epoch no.4 train no.392700  loss = 2.22968 avg_loss = 3.41968\n",
      "epoch no.4 train no.392710  loss = 2.77133 avg_loss = 3.36601\n",
      "epoch no.4 train no.392720  loss = 3.80170 avg_loss = 3.39257\n",
      "epoch no.4 train no.392730  loss = 2.57570 avg_loss = 3.40111\n",
      "epoch no.4 train no.392740  loss = 3.92301 avg_loss = 3.41481\n",
      "epoch no.4 train no.392750  loss = 3.17449 avg_loss = 3.43536\n",
      "epoch no.4 train no.392760  loss = 2.40111 avg_loss = 3.46253\n",
      "epoch no.4 train no.392770  loss = 2.28807 avg_loss = 3.45871\n",
      "epoch no.4 train no.392780  loss = 3.71798 avg_loss = 3.48896\n",
      "epoch no.4 train no.392790  loss = 5.20442 avg_loss = 3.48125\n",
      "epoch no.4 train no.392800  loss = 3.73076 avg_loss = 3.47894\n",
      "epoch no.4 train no.392810  loss = 3.71161 avg_loss = 3.49658\n",
      "epoch no.4 train no.392820  loss = 3.84648 avg_loss = 3.50721\n",
      "epoch no.4 train no.392830  loss = 2.04161 avg_loss = 3.48483\n",
      "epoch no.4 train no.392840  loss = 2.92179 avg_loss = 3.46093\n",
      "epoch no.4 train no.392850  loss = 3.89175 avg_loss = 3.42377\n",
      "epoch no.4 train no.392860  loss = 3.18607 avg_loss = 3.41106\n",
      "epoch no.4 train no.392870  loss = 2.36238 avg_loss = 3.38037\n",
      "epoch no.4 train no.392880  loss = 3.18116 avg_loss = 3.37379\n",
      "epoch no.4 train no.392890  loss = 3.16206 avg_loss = 3.38087\n",
      "epoch no.4 train no.392900  loss = 3.82513 avg_loss = 3.41307\n",
      "epoch no.4 train no.392910  loss = 2.10999 avg_loss = 3.39899\n",
      "epoch no.4 train no.392920  loss = 3.25456 avg_loss = 3.37921\n",
      "epoch no.4 train no.392930  loss = 2.56976 avg_loss = 3.33592\n",
      "epoch no.4 train no.392940  loss = 3.84536 avg_loss = 3.37099\n",
      "epoch no.4 train no.392950  loss = 3.09091 avg_loss = 3.33997\n",
      "epoch no.4 train no.392960  loss = 4.16697 avg_loss = 3.31390\n",
      "epoch no.4 train no.392970  loss = 3.50647 avg_loss = 3.31463\n",
      "epoch no.4 train no.392980  loss = 4.40528 avg_loss = 3.33306\n",
      "epoch no.4 train no.392990  loss = 2.31380 avg_loss = 3.36766\n",
      "epoch no.4 train no.393000  loss = 2.94102 avg_loss = 3.35712\n",
      "7\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁필요할', '때', '▁듣는', '▁신나는', '힙', '합', '</s>']\n",
      "기분전환이 필요할때 듣는 감성힙합</s>\n",
      "epoch no.4 train no.393010  loss = 3.49170 avg_loss = 3.37602\n",
      "epoch no.4 train no.393020  loss = 2.82194 avg_loss = 3.41865\n",
      "epoch no.4 train no.393030  loss = 5.48625 avg_loss = 3.40285\n",
      "epoch no.4 train no.393040  loss = 3.11846 avg_loss = 3.42470\n",
      "epoch no.4 train no.393050  loss = 2.70247 avg_loss = 3.38147\n",
      "epoch no.4 train no.393060  loss = 5.62502 avg_loss = 3.35086\n",
      "epoch no.4 train no.393070  loss = 5.28263 avg_loss = 3.37964\n",
      "epoch no.4 train no.393080  loss = 5.16471 avg_loss = 3.36978\n",
      "epoch no.4 train no.393090  loss = 2.65692 avg_loss = 3.34463\n",
      "epoch no.4 train no.393100  loss = 3.49375 avg_loss = 3.34579\n",
      "epoch no.4 train no.393110  loss = 3.75189 avg_loss = 3.36829\n",
      "epoch no.4 train no.393120  loss = 2.70447 avg_loss = 3.32423\n",
      "epoch no.4 train no.393130  loss = 1.48588 avg_loss = 3.32224\n",
      "epoch no.4 train no.393140  loss = 3.28292 avg_loss = 3.33666\n",
      "epoch no.4 train no.393150  loss = 2.81991 avg_loss = 3.38159\n",
      "epoch no.4 train no.393160  loss = 5.24976 avg_loss = 3.44207\n",
      "epoch no.4 train no.393170  loss = 3.04101 avg_loss = 3.44157\n",
      "epoch no.4 train no.393180  loss = 3.38778 avg_loss = 3.40998\n",
      "epoch no.4 train no.393190  loss = 2.46893 avg_loss = 3.37678\n",
      "epoch no.4 train no.393200  loss = 4.42087 avg_loss = 3.42252\n",
      "epoch no.4 train no.393210  loss = 3.10599 avg_loss = 3.41040\n",
      "epoch no.4 train no.393220  loss = 1.89074 avg_loss = 3.42318\n",
      "epoch no.4 train no.393230  loss = 2.25670 avg_loss = 3.42727\n",
      "epoch no.4 train no.393240  loss = 3.09847 avg_loss = 3.41974\n",
      "epoch no.4 train no.393250  loss = 3.82850 avg_loss = 3.43675\n",
      "epoch no.4 train no.393260  loss = 3.36952 avg_loss = 3.43394\n",
      "epoch no.4 train no.393270  loss = 3.42470 avg_loss = 3.44049\n",
      "epoch no.4 train no.393280  loss = 2.34450 avg_loss = 3.42843\n",
      "epoch no.4 train no.393290  loss = 2.10023 avg_loss = 3.43066\n",
      "epoch no.4 train no.393300  loss = 3.71707 avg_loss = 3.40455\n",
      "epoch no.4 train no.393310  loss = 3.13102 avg_loss = 3.39258\n",
      "epoch no.4 train no.393320  loss = 5.69948 avg_loss = 3.39435\n",
      "epoch no.4 train no.393330  loss = 2.30873 avg_loss = 3.35981\n",
      "epoch no.4 train no.393340  loss = 4.83493 avg_loss = 3.34562\n",
      "epoch no.4 train no.393350  loss = 2.43289 avg_loss = 3.35695\n",
      "epoch no.4 train no.393360  loss = 2.88268 avg_loss = 3.34383\n",
      "epoch no.4 train no.393370  loss = 3.39376 avg_loss = 3.36526\n",
      "epoch no.4 train no.393380  loss = 3.42743 avg_loss = 3.34579\n",
      "epoch no.4 train no.393390  loss = 2.93172 avg_loss = 3.33623\n",
      "epoch no.4 train no.393400  loss = 4.18399 avg_loss = 3.31130\n",
      "epoch no.4 train no.393410  loss = 2.82261 avg_loss = 3.37091\n",
      "epoch no.4 train no.393420  loss = 2.63415 avg_loss = 3.36572\n",
      "epoch no.4 train no.393430  loss = 3.36551 avg_loss = 3.41007\n",
      "epoch no.4 train no.393440  loss = 3.55776 avg_loss = 3.39201\n",
      "epoch no.4 train no.393450  loss = 4.98685 avg_loss = 3.42168\n",
      "epoch no.4 train no.393460  loss = 2.36810 avg_loss = 3.36201\n",
      "epoch no.4 train no.393470  loss = 3.09807 avg_loss = 3.32885\n",
      "epoch no.4 train no.393480  loss = 3.77827 avg_loss = 3.33871\n",
      "epoch no.4 train no.393490  loss = 3.18696 avg_loss = 3.32259\n",
      "epoch no.4 train no.393500  loss = 3.30388 avg_loss = 3.29722\n",
      "epoch no.4 train no.393510  loss = 3.97363 avg_loss = 3.32776\n",
      "epoch no.4 train no.393520  loss = 2.66463 avg_loss = 3.30995\n",
      "epoch no.4 train no.393530  loss = 2.72816 avg_loss = 3.36344\n",
      "epoch no.4 train no.393540  loss = 3.69433 avg_loss = 3.39169\n",
      "epoch no.4 train no.393550  loss = 2.61227 avg_loss = 3.38106\n",
      "epoch no.4 train no.393560  loss = 3.77225 avg_loss = 3.36627\n",
      "epoch no.4 train no.393570  loss = 3.98960 avg_loss = 3.41010\n",
      "epoch no.4 train no.393580  loss = 2.91801 avg_loss = 3.41509\n",
      "epoch no.4 train no.393590  loss = 3.33177 avg_loss = 3.39442\n",
      "epoch no.4 train no.393600  loss = 2.67210 avg_loss = 3.31746\n",
      "epoch no.4 train no.393610  loss = 4.35490 avg_loss = 3.30966\n",
      "epoch no.4 train no.393620  loss = 3.40041 avg_loss = 3.29803\n",
      "epoch no.4 train no.393630  loss = 3.58503 avg_loss = 3.30776\n",
      "epoch no.4 train no.393640  loss = 1.69008 avg_loss = 3.29861\n",
      "epoch no.4 train no.393650  loss = 4.44606 avg_loss = 3.28947\n",
      "epoch no.4 train no.393660  loss = 2.32093 avg_loss = 3.24088\n",
      "epoch no.4 train no.393670  loss = 2.86125 avg_loss = 3.31290\n",
      "epoch no.4 train no.393680  loss = 5.10822 avg_loss = 3.32172\n",
      "epoch no.4 train no.393690  loss = 2.72066 avg_loss = 3.39641\n",
      "epoch no.4 train no.393700  loss = 5.81193 avg_loss = 3.40564\n",
      "epoch no.4 train no.393710  loss = 2.48614 avg_loss = 3.43502\n",
      "epoch no.4 train no.393720  loss = 2.36897 avg_loss = 3.41163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.393730  loss = 2.91462 avg_loss = 3.39215\n",
      "epoch no.4 train no.393740  loss = 2.22374 avg_loss = 3.39041\n",
      "epoch no.4 train no.393750  loss = 2.36558 avg_loss = 3.39829\n",
      "epoch no.4 train no.393760  loss = 3.18799 avg_loss = 3.35466\n",
      "epoch no.4 train no.393770  loss = 2.09520 avg_loss = 3.32711\n",
      "epoch no.4 train no.393780  loss = 4.06561 avg_loss = 3.33645\n",
      "epoch no.4 train no.393790  loss = 3.32121 avg_loss = 3.31504\n",
      "epoch no.4 train no.393800  loss = 3.60222 avg_loss = 3.35489\n",
      "epoch no.4 train no.393810  loss = 1.81020 avg_loss = 3.36991\n",
      "epoch no.4 train no.393820  loss = 5.39738 avg_loss = 3.39378\n",
      "epoch no.4 train no.393830  loss = 4.14095 avg_loss = 3.40564\n",
      "epoch no.4 train no.393840  loss = 2.83819 avg_loss = 3.41458\n",
      "epoch no.4 train no.393850  loss = 4.01017 avg_loss = 3.43507\n",
      "epoch no.4 train no.393860  loss = 3.01426 avg_loss = 3.40711\n",
      "epoch no.4 train no.393870  loss = 3.20125 avg_loss = 3.39334\n",
      "epoch no.4 train no.393880  loss = 3.27971 avg_loss = 3.37384\n",
      "epoch no.4 train no.393890  loss = 3.61470 avg_loss = 3.37642\n",
      "epoch no.4 train no.393900  loss = 2.42868 avg_loss = 3.37886\n",
      "epoch no.4 train no.393910  loss = 3.91755 avg_loss = 3.34090\n",
      "epoch no.4 train no.393920  loss = 1.75414 avg_loss = 3.38605\n",
      "epoch no.4 train no.393930  loss = 2.63279 avg_loss = 3.38434\n",
      "epoch no.4 train no.393940  loss = 3.13690 avg_loss = 3.40526\n",
      "epoch no.4 train no.393950  loss = 2.85213 avg_loss = 3.38475\n",
      "epoch no.4 train no.393960  loss = 2.72618 avg_loss = 3.42673\n",
      "epoch no.4 train no.393970  loss = 2.62850 avg_loss = 3.37685\n",
      "epoch no.4 train no.393980  loss = 3.12862 avg_loss = 3.33919\n",
      "epoch no.4 train no.393990  loss = 3.94474 avg_loss = 3.35111\n",
      "epoch no.4 train no.394000  loss = 1.98854 avg_loss = 3.32103\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '싶', '을', '때', '</s>']\n",
      "기분전환 하고싶을때</s>\n",
      "epoch no.4 train no.394010  loss = 3.21953 avg_loss = 3.32779\n",
      "epoch no.4 train no.394020  loss = 3.33779 avg_loss = 3.33712\n",
      "epoch no.4 train no.394030  loss = 4.68292 avg_loss = 3.31031\n",
      "epoch no.4 train no.394040  loss = 3.05616 avg_loss = 3.30333\n",
      "epoch no.4 train no.394050  loss = 3.65817 avg_loss = 3.31431\n",
      "epoch no.4 train no.394060  loss = 3.73522 avg_loss = 3.34768\n",
      "epoch no.4 train no.394070  loss = 2.53503 avg_loss = 3.36962\n",
      "epoch no.4 train no.394080  loss = 3.03557 avg_loss = 3.36077\n",
      "epoch no.4 train no.394090  loss = 3.82702 avg_loss = 3.32501\n",
      "epoch no.4 train no.394100  loss = 3.50257 avg_loss = 3.30854\n",
      "epoch no.4 train no.394110  loss = 2.84093 avg_loss = 3.29977\n",
      "epoch no.4 train no.394120  loss = 3.22496 avg_loss = 3.29058\n",
      "epoch no.4 train no.394130  loss = 4.44993 avg_loss = 3.30591\n",
      "epoch no.4 train no.394140  loss = 3.41473 avg_loss = 3.31449\n",
      "epoch no.4 train no.394150  loss = 4.48237 avg_loss = 3.31012\n",
      "epoch no.4 train no.394160  loss = 2.88997 avg_loss = 3.31021\n",
      "epoch no.4 train no.394170  loss = 4.23496 avg_loss = 3.29560\n",
      "epoch no.4 train no.394180  loss = 3.94995 avg_loss = 3.32756\n",
      "epoch no.4 train no.394190  loss = 4.77819 avg_loss = 3.35418\n",
      "epoch no.4 train no.394200  loss = 3.56739 avg_loss = 3.38788\n",
      "epoch no.4 train no.394210  loss = 5.94888 avg_loss = 3.38675\n",
      "epoch no.4 train no.394220  loss = 3.18585 avg_loss = 3.35783\n",
      "epoch no.4 train no.394230  loss = 4.05216 avg_loss = 3.32194\n",
      "epoch no.4 train no.394240  loss = 2.70017 avg_loss = 3.34451\n",
      "epoch no.4 train no.394250  loss = 3.05654 avg_loss = 3.36619\n",
      "epoch no.4 train no.394260  loss = 3.11799 avg_loss = 3.37903\n",
      "epoch no.4 train no.394270  loss = 3.53696 avg_loss = 3.37737\n",
      "epoch no.4 train no.394280  loss = 2.61763 avg_loss = 3.37609\n",
      "epoch no.4 train no.394290  loss = 3.62722 avg_loss = 3.42585\n",
      "epoch no.4 train no.394300  loss = 2.81123 avg_loss = 3.44146\n",
      "epoch no.4 train no.394310  loss = 2.56899 avg_loss = 3.41837\n",
      "epoch no.4 train no.394320  loss = 3.39742 avg_loss = 3.42475\n",
      "epoch no.4 train no.394330  loss = 6.76643 avg_loss = 3.50094\n",
      "epoch no.4 train no.394340  loss = 2.97265 avg_loss = 3.52188\n",
      "epoch no.4 train no.394350  loss = 2.61668 avg_loss = 3.49754\n",
      "epoch no.4 train no.394360  loss = 2.11032 avg_loss = 3.47777\n",
      "epoch no.4 train no.394370  loss = 3.21363 avg_loss = 3.42548\n",
      "epoch no.4 train no.394380  loss = 2.03713 avg_loss = 3.38312\n",
      "epoch no.4 train no.394390  loss = 3.80612 avg_loss = 3.36113\n",
      "epoch no.4 train no.394400  loss = 3.22233 avg_loss = 3.38432\n",
      "epoch no.4 train no.394410  loss = 3.04502 avg_loss = 3.40929\n",
      "epoch no.4 train no.394420  loss = 3.15361 avg_loss = 3.40724\n",
      "epoch no.4 train no.394430  loss = 3.17129 avg_loss = 3.43835\n",
      "epoch no.4 train no.394440  loss = 2.60053 avg_loss = 3.42527\n",
      "epoch no.4 train no.394450  loss = 2.46840 avg_loss = 3.43668\n",
      "epoch no.4 train no.394460  loss = 4.35145 avg_loss = 3.41825\n",
      "epoch no.4 train no.394470  loss = 3.47424 avg_loss = 3.44802\n",
      "epoch no.4 train no.394480  loss = 2.72131 avg_loss = 3.42660\n",
      "epoch no.4 train no.394490  loss = 2.41643 avg_loss = 3.41703\n",
      "epoch no.4 train no.394500  loss = 4.12004 avg_loss = 3.39029\n",
      "epoch no.4 train no.394510  loss = 4.23800 avg_loss = 3.38325\n",
      "epoch no.4 train no.394520  loss = 3.54949 avg_loss = 3.37434\n",
      "epoch no.4 train no.394530  loss = 2.67955 avg_loss = 3.39965\n",
      "epoch no.4 train no.394540  loss = 3.08777 avg_loss = 3.36083\n",
      "epoch no.4 train no.394550  loss = 1.95441 avg_loss = 3.39579\n",
      "epoch no.4 train no.394560  loss = 4.24508 avg_loss = 3.39069\n",
      "epoch no.4 train no.394570  loss = 2.89322 avg_loss = 3.39564\n",
      "epoch no.4 train no.394580  loss = 3.36346 avg_loss = 3.39712\n",
      "epoch no.4 train no.394590  loss = 1.48787 avg_loss = 3.38071\n",
      "epoch no.4 train no.394600  loss = 2.85206 avg_loss = 3.32737\n",
      "epoch no.4 train no.394610  loss = 2.49263 avg_loss = 3.31567\n",
      "epoch no.4 train no.394620  loss = 4.07656 avg_loss = 3.28425\n",
      "epoch no.4 train no.394630  loss = 3.79570 avg_loss = 3.28158\n",
      "epoch no.4 train no.394640  loss = 3.27207 avg_loss = 3.27085\n",
      "epoch no.4 train no.394650  loss = 2.98901 avg_loss = 3.33623\n",
      "epoch no.4 train no.394660  loss = 3.02453 avg_loss = 3.30793\n",
      "epoch no.4 train no.394670  loss = 2.04513 avg_loss = 3.27246\n",
      "epoch no.4 train no.394680  loss = 3.31421 avg_loss = 3.26866\n",
      "epoch no.4 train no.394690  loss = 3.28649 avg_loss = 3.26647\n",
      "epoch no.4 train no.394700  loss = 4.69184 avg_loss = 3.28033\n",
      "epoch no.4 train no.394710  loss = 3.38263 avg_loss = 3.28344\n",
      "epoch no.4 train no.394720  loss = 3.42597 avg_loss = 3.29810\n",
      "epoch no.4 train no.394730  loss = 2.49932 avg_loss = 3.25935\n",
      "epoch no.4 train no.394740  loss = 2.81238 avg_loss = 3.26405\n",
      "epoch no.4 train no.394750  loss = 3.93220 avg_loss = 3.29765\n",
      "epoch no.4 train no.394760  loss = 2.93751 avg_loss = 3.29288\n",
      "epoch no.4 train no.394770  loss = 3.10501 avg_loss = 3.29867\n",
      "epoch no.4 train no.394780  loss = 2.73630 avg_loss = 3.29010\n",
      "epoch no.4 train no.394790  loss = 4.12071 avg_loss = 3.29176\n",
      "epoch no.4 train no.394800  loss = 2.70523 avg_loss = 3.33343\n",
      "epoch no.4 train no.394810  loss = 3.10331 avg_loss = 3.32844\n",
      "epoch no.4 train no.394820  loss = 3.13382 avg_loss = 3.34464\n",
      "epoch no.4 train no.394830  loss = 1.33827 avg_loss = 3.34076\n",
      "epoch no.4 train no.394840  loss = 2.60636 avg_loss = 3.35335\n",
      "epoch no.4 train no.394850  loss = 2.44284 avg_loss = 3.33174\n",
      "epoch no.4 train no.394860  loss = 3.90824 avg_loss = 3.33680\n",
      "epoch no.4 train no.394870  loss = 3.26118 avg_loss = 3.33457\n",
      "epoch no.4 train no.394880  loss = 5.43448 avg_loss = 3.36550\n",
      "epoch no.4 train no.394890  loss = 5.09415 avg_loss = 3.37047\n",
      "epoch no.4 train no.394900  loss = 3.52691 avg_loss = 3.30549\n",
      "epoch no.4 train no.394910  loss = 3.72124 avg_loss = 3.31632\n",
      "epoch no.4 train no.394920  loss = 3.10430 avg_loss = 3.33529\n",
      "epoch no.4 train no.394930  loss = 4.22751 avg_loss = 3.37771\n",
      "epoch no.4 train no.394940  loss = 2.52739 avg_loss = 3.38374\n",
      "epoch no.4 train no.394950  loss = 1.81691 avg_loss = 3.40390\n",
      "epoch no.4 train no.394960  loss = 2.83297 avg_loss = 3.40063\n",
      "epoch no.4 train no.394970  loss = 3.70521 avg_loss = 3.40246\n",
      "epoch no.4 train no.394980  loss = 2.43470 avg_loss = 3.34945\n",
      "epoch no.4 train no.394990  loss = 2.75228 avg_loss = 3.31146\n",
      "epoch no.4 train no.395000  loss = 5.41128 avg_loss = 3.35712\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '▁노래', '▁팝', 'op', '</s>']\n",
      "기분전환에 좋은 신나는 pop</s>\n",
      "epoch no.4 train no.395010  loss = 3.67879 avg_loss = 3.38233\n",
      "epoch no.4 train no.395020  loss = 3.24453 avg_loss = 3.33686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.395030  loss = 4.75009 avg_loss = 3.34014\n",
      "epoch no.4 train no.395040  loss = 3.42636 avg_loss = 3.30891\n",
      "epoch no.4 train no.395050  loss = 4.55177 avg_loss = 3.31281\n",
      "epoch no.4 train no.395060  loss = 5.12565 avg_loss = 3.35343\n",
      "epoch no.4 train no.395070  loss = 2.19443 avg_loss = 3.33635\n",
      "epoch no.4 train no.395080  loss = 2.44156 avg_loss = 3.30247\n",
      "epoch no.4 train no.395090  loss = 4.37473 avg_loss = 3.33363\n",
      "epoch no.4 train no.395100  loss = 1.64566 avg_loss = 3.30643\n",
      "epoch no.4 train no.395110  loss = 4.09659 avg_loss = 3.34888\n",
      "epoch no.4 train no.395120  loss = 3.69893 avg_loss = 3.34340\n",
      "epoch no.4 train no.395130  loss = 4.32458 avg_loss = 3.29890\n",
      "epoch no.4 train no.395140  loss = 2.01457 avg_loss = 3.29596\n",
      "epoch no.4 train no.395150  loss = 2.35952 avg_loss = 3.28832\n",
      "epoch no.4 train no.395160  loss = 3.09469 avg_loss = 3.29928\n",
      "epoch no.4 train no.395170  loss = 2.56727 avg_loss = 3.39048\n",
      "epoch no.4 train no.395180  loss = 4.65071 avg_loss = 3.42103\n",
      "epoch no.4 train no.395190  loss = 2.98869 avg_loss = 3.39495\n",
      "epoch no.4 train no.395200  loss = 4.99759 avg_loss = 3.42499\n",
      "epoch no.4 train no.395210  loss = 2.26851 avg_loss = 3.41231\n",
      "epoch no.4 train no.395220  loss = 4.24866 avg_loss = 3.38074\n",
      "epoch no.4 train no.395230  loss = 4.18223 avg_loss = 3.37812\n",
      "epoch no.4 train no.395240  loss = 2.53231 avg_loss = 3.32388\n",
      "epoch no.4 train no.395250  loss = 4.82506 avg_loss = 3.36233\n",
      "epoch no.4 train no.395260  loss = 3.92328 avg_loss = 3.31585\n",
      "epoch no.4 train no.395270  loss = 3.55866 avg_loss = 3.30786\n",
      "epoch no.4 train no.395280  loss = 3.51572 avg_loss = 3.32371\n",
      "epoch no.4 train no.395290  loss = 3.38961 avg_loss = 3.38443\n",
      "epoch no.4 train no.395300  loss = 2.50565 avg_loss = 3.38226\n",
      "epoch no.4 train no.395310  loss = 3.50207 avg_loss = 3.37327\n",
      "epoch no.4 train no.395320  loss = 3.08269 avg_loss = 3.32159\n",
      "epoch no.4 train no.395330  loss = 4.26704 avg_loss = 3.31290\n",
      "epoch no.4 train no.395340  loss = 2.15809 avg_loss = 3.34078\n",
      "epoch no.4 train no.395350  loss = 3.03101 avg_loss = 3.35026\n",
      "epoch no.4 train no.395360  loss = 4.54074 avg_loss = 3.39722\n",
      "epoch no.4 train no.395370  loss = 4.86475 avg_loss = 3.40717\n",
      "epoch no.4 train no.395380  loss = 3.25157 avg_loss = 3.43409\n",
      "epoch no.4 train no.395390  loss = 5.26596 avg_loss = 3.40994\n",
      "epoch no.4 train no.395400  loss = 2.14328 avg_loss = 3.39432\n",
      "epoch no.4 train no.395410  loss = 1.95287 avg_loss = 3.39618\n",
      "epoch no.4 train no.395420  loss = 3.28965 avg_loss = 3.38880\n",
      "epoch no.4 train no.395430  loss = 2.95772 avg_loss = 3.41315\n",
      "epoch no.4 train no.395440  loss = 2.89782 avg_loss = 3.37515\n",
      "epoch no.4 train no.395450  loss = 2.74797 avg_loss = 3.33873\n",
      "epoch no.4 train no.395460  loss = 3.57375 avg_loss = 3.30456\n",
      "epoch no.4 train no.395470  loss = 3.34694 avg_loss = 3.30480\n",
      "epoch no.4 train no.395480  loss = 3.63298 avg_loss = 3.31851\n",
      "epoch no.4 train no.395490  loss = 3.74128 avg_loss = 3.30352\n",
      "epoch no.4 train no.395500  loss = 3.02246 avg_loss = 3.33148\n",
      "epoch no.4 train no.395510  loss = 3.05807 avg_loss = 3.30978\n",
      "epoch no.4 train no.395520  loss = 3.39310 avg_loss = 3.26853\n",
      "epoch no.4 train no.395530  loss = 3.03278 avg_loss = 3.21132\n",
      "epoch no.4 train no.395540  loss = 3.42309 avg_loss = 3.24261\n",
      "epoch no.4 train no.395550  loss = 3.04627 avg_loss = 3.26211\n",
      "epoch no.4 train no.395560  loss = 3.77336 avg_loss = 3.28547\n",
      "epoch no.4 train no.395570  loss = 2.92672 avg_loss = 3.29240\n",
      "epoch no.4 train no.395580  loss = 1.98326 avg_loss = 3.28849\n",
      "epoch no.4 train no.395590  loss = 3.71738 avg_loss = 3.29234\n",
      "epoch no.4 train no.395600  loss = 2.69571 avg_loss = 3.28373\n",
      "epoch no.4 train no.395610  loss = 2.19509 avg_loss = 3.23881\n",
      "epoch no.4 train no.395620  loss = 1.98554 avg_loss = 3.23195\n",
      "epoch no.4 train no.395630  loss = 3.37369 avg_loss = 3.21437\n",
      "epoch no.4 train no.395640  loss = 2.58885 avg_loss = 3.24984\n",
      "epoch no.4 train no.395650  loss = 4.26485 avg_loss = 3.23999\n",
      "epoch no.4 train no.395660  loss = 3.17250 avg_loss = 3.27685\n",
      "epoch no.4 train no.395670  loss = 3.59765 avg_loss = 3.22974\n",
      "epoch no.4 train no.395680  loss = 2.17716 avg_loss = 3.18477\n",
      "epoch no.4 train no.395690  loss = 3.66212 avg_loss = 3.16854\n",
      "epoch no.4 train no.395700  loss = 3.83201 avg_loss = 3.22146\n",
      "epoch no.4 train no.395710  loss = 4.04627 avg_loss = 3.19804\n",
      "epoch no.4 train no.395720  loss = 4.96094 avg_loss = 3.21381\n",
      "epoch no.4 train no.395730  loss = 2.18079 avg_loss = 3.21567\n",
      "epoch no.4 train no.395740  loss = 4.42044 avg_loss = 3.29268\n",
      "epoch no.4 train no.395750  loss = 3.15674 avg_loss = 3.32766\n",
      "epoch no.4 train no.395760  loss = 2.15266 avg_loss = 3.31417\n",
      "epoch no.4 train no.395770  loss = 2.68651 avg_loss = 3.31635\n",
      "epoch no.4 train no.395780  loss = 1.93464 avg_loss = 3.32518\n",
      "epoch no.4 train no.395790  loss = 3.03097 avg_loss = 3.32276\n",
      "epoch no.4 train no.395800  loss = 3.10912 avg_loss = 3.30805\n",
      "epoch no.4 train no.395810  loss = 3.76249 avg_loss = 3.32029\n",
      "epoch no.4 train no.395820  loss = 2.83988 avg_loss = 3.28242\n",
      "epoch no.4 train no.395830  loss = 5.18485 avg_loss = 3.28687\n",
      "epoch no.4 train no.395840  loss = 4.55485 avg_loss = 3.29245\n",
      "epoch no.4 train no.395850  loss = 3.42431 avg_loss = 3.30020\n",
      "epoch no.4 train no.395860  loss = 2.64087 avg_loss = 3.32210\n",
      "epoch no.4 train no.395870  loss = 3.11884 avg_loss = 3.31914\n",
      "epoch no.4 train no.395880  loss = 3.21538 avg_loss = 3.33857\n",
      "epoch no.4 train no.395890  loss = 2.72402 avg_loss = 3.33716\n",
      "epoch no.4 train no.395900  loss = 2.79404 avg_loss = 3.36828\n",
      "epoch no.4 train no.395910  loss = 3.42530 avg_loss = 3.32689\n",
      "epoch no.4 train no.395920  loss = 1.60254 avg_loss = 3.29027\n",
      "epoch no.4 train no.395930  loss = 3.93714 avg_loss = 3.24449\n",
      "epoch no.4 train no.395940  loss = 2.31410 avg_loss = 3.22292\n",
      "epoch no.4 train no.395950  loss = 2.57873 avg_loss = 3.18571\n",
      "epoch no.4 train no.395960  loss = 2.88573 avg_loss = 3.15699\n",
      "epoch no.4 train no.395970  loss = 2.32135 avg_loss = 3.15026\n",
      "epoch no.4 train no.395980  loss = 2.68728 avg_loss = 3.12343\n",
      "epoch no.4 train no.395990  loss = 2.70985 avg_loss = 3.14369\n",
      "epoch no.4 train no.396000  loss = 2.93257 avg_loss = 3.17001\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁위한', '▁감성', '▁노래', '곡', '</s>', '</s>']\n",
      "기분전환을 위한 아이돌 댄스곡 모음</s>\n",
      "epoch no.4 train no.396010  loss = 3.08221 avg_loss = 3.21727\n",
      "epoch no.4 train no.396020  loss = 4.32298 avg_loss = 3.21551\n",
      "epoch no.4 train no.396030  loss = 2.35398 avg_loss = 3.20171\n",
      "epoch no.4 train no.396040  loss = 3.22489 avg_loss = 3.25583\n",
      "epoch no.4 train no.396050  loss = 3.65896 avg_loss = 3.24181\n",
      "epoch no.4 train no.396060  loss = 2.57219 avg_loss = 3.22367\n",
      "epoch no.4 train no.396070  loss = 2.36894 avg_loss = 3.17806\n",
      "epoch no.4 train no.396080  loss = 2.69063 avg_loss = 3.20088\n",
      "epoch no.4 train no.396090  loss = 4.21596 avg_loss = 3.24441\n",
      "epoch no.4 train no.396100  loss = 3.05525 avg_loss = 3.28053\n",
      "epoch no.4 train no.396110  loss = 4.58665 avg_loss = 3.21621\n",
      "epoch no.4 train no.396120  loss = 2.82928 avg_loss = 3.21456\n",
      "epoch no.4 train no.396130  loss = 1.60036 avg_loss = 3.18647\n",
      "epoch no.4 train no.396140  loss = 3.37005 avg_loss = 3.17243\n",
      "epoch no.4 train no.396150  loss = 2.18587 avg_loss = 3.17382\n",
      "epoch no.4 train no.396160  loss = 3.39476 avg_loss = 3.21327\n",
      "epoch no.4 train no.396170  loss = 3.16071 avg_loss = 3.19746\n",
      "epoch no.4 train no.396180  loss = 3.77781 avg_loss = 3.21520\n",
      "epoch no.4 train no.396190  loss = 2.95198 avg_loss = 3.25142\n",
      "epoch no.4 train no.396200  loss = 3.65437 avg_loss = 3.28585\n",
      "epoch no.4 train no.396210  loss = 3.82319 avg_loss = 3.33803\n",
      "epoch no.4 train no.396220  loss = 3.86873 avg_loss = 3.37135\n",
      "epoch no.4 train no.396230  loss = 3.15541 avg_loss = 3.33742\n",
      "epoch no.4 train no.396240  loss = 2.90566 avg_loss = 3.32337\n",
      "epoch no.4 train no.396250  loss = 3.01722 avg_loss = 3.32247\n",
      "epoch no.4 train no.396260  loss = 3.59510 avg_loss = 3.30228\n",
      "epoch no.4 train no.396270  loss = 4.41885 avg_loss = 3.33947\n",
      "epoch no.4 train no.396280  loss = 2.98010 avg_loss = 3.30224\n",
      "epoch no.4 train no.396290  loss = 3.29560 avg_loss = 3.22767\n",
      "epoch no.4 train no.396300  loss = 4.93695 avg_loss = 3.25735\n",
      "epoch no.4 train no.396310  loss = 3.87605 avg_loss = 3.24839\n",
      "epoch no.4 train no.396320  loss = 3.63222 avg_loss = 3.24898\n",
      "epoch no.4 train no.396330  loss = 3.18423 avg_loss = 3.28056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.396340  loss = 2.11333 avg_loss = 3.22687\n",
      "epoch no.4 train no.396350  loss = 2.91948 avg_loss = 3.26692\n",
      "epoch no.4 train no.396360  loss = 4.29892 avg_loss = 3.28829\n",
      "epoch no.4 train no.396370  loss = 2.33455 avg_loss = 3.29495\n",
      "epoch no.4 train no.396380  loss = 4.12165 avg_loss = 3.24150\n",
      "epoch no.4 train no.396390  loss = 3.00870 avg_loss = 3.23927\n",
      "epoch no.4 train no.396400  loss = 4.25778 avg_loss = 3.23405\n",
      "epoch no.4 train no.396410  loss = 4.20671 avg_loss = 3.26206\n",
      "epoch no.4 train no.396420  loss = 4.18489 avg_loss = 3.28546\n",
      "epoch no.4 train no.396430  loss = 3.58998 avg_loss = 3.26046\n",
      "epoch no.4 train no.396440  loss = 3.18461 avg_loss = 3.23415\n",
      "epoch no.4 train no.396450  loss = 3.65925 avg_loss = 3.27754\n",
      "epoch no.4 train no.396460  loss = 2.43478 avg_loss = 3.26281\n",
      "epoch no.4 train no.396470  loss = 3.08095 avg_loss = 3.27928\n",
      "epoch no.4 train no.396480  loss = 3.23182 avg_loss = 3.26520\n",
      "epoch no.4 train no.396490  loss = 2.24638 avg_loss = 3.25773\n",
      "epoch no.4 train no.396500  loss = 2.32580 avg_loss = 3.27836\n",
      "epoch no.4 train no.396510  loss = 4.04987 avg_loss = 3.34262\n",
      "epoch no.4 train no.396520  loss = 3.77164 avg_loss = 3.33446\n",
      "epoch no.4 train no.396530  loss = 2.29096 avg_loss = 3.32802\n",
      "epoch no.4 train no.396540  loss = 4.39244 avg_loss = 3.35062\n",
      "epoch no.4 train no.396550  loss = 2.27756 avg_loss = 3.34436\n",
      "epoch no.4 train no.396560  loss = 3.03170 avg_loss = 3.32674\n",
      "epoch no.4 train no.396570  loss = 3.62699 avg_loss = 3.31044\n",
      "epoch no.4 train no.396580  loss = 2.31360 avg_loss = 3.28600\n",
      "epoch no.4 train no.396590  loss = 2.00944 avg_loss = 3.27084\n",
      "epoch no.4 train no.396600  loss = 3.02468 avg_loss = 3.26566\n",
      "epoch no.4 train no.396610  loss = 3.83299 avg_loss = 3.24169\n",
      "epoch no.4 train no.396620  loss = 3.10650 avg_loss = 3.21958\n",
      "epoch no.4 train no.396630  loss = 3.04490 avg_loss = 3.22522\n",
      "epoch no.4 train no.396640  loss = 3.07172 avg_loss = 3.26306\n",
      "epoch no.4 train no.396650  loss = 3.01727 avg_loss = 3.28293\n",
      "epoch no.4 train no.396660  loss = 5.00997 avg_loss = 3.31044\n",
      "epoch no.4 train no.396670  loss = 3.00350 avg_loss = 3.28023\n",
      "epoch no.4 train no.396680  loss = 2.86958 avg_loss = 3.30061\n",
      "epoch no.4 train no.396690  loss = 4.87753 avg_loss = 3.29974\n",
      "epoch no.4 train no.396700  loss = 2.90225 avg_loss = 3.33953\n",
      "epoch no.4 train no.396710  loss = 3.10210 avg_loss = 3.35251\n",
      "epoch no.4 train no.396720  loss = 1.98384 avg_loss = 3.35473\n",
      "epoch no.4 train no.396730  loss = 2.80989 avg_loss = 3.34291\n",
      "epoch no.4 train no.396740  loss = 3.17866 avg_loss = 3.33105\n",
      "epoch no.4 train no.396750  loss = 3.40129 avg_loss = 3.30767\n",
      "epoch no.4 train no.396760  loss = 2.96024 avg_loss = 3.28582\n",
      "epoch no.4 train no.396770  loss = 3.92359 avg_loss = 3.28295\n",
      "epoch no.4 train no.396780  loss = 2.92014 avg_loss = 3.25591\n",
      "epoch no.4 train no.396790  loss = 2.61655 avg_loss = 3.25208\n",
      "epoch no.4 train no.396800  loss = 1.79495 avg_loss = 3.22251\n",
      "epoch no.4 train no.396810  loss = 3.70185 avg_loss = 3.26666\n",
      "epoch no.4 train no.396820  loss = 2.86309 avg_loss = 3.29080\n",
      "epoch no.4 train no.396830  loss = 3.00456 avg_loss = 3.29001\n",
      "epoch no.4 train no.396840  loss = 2.45441 avg_loss = 3.30029\n",
      "epoch no.4 train no.396850  loss = 3.28388 avg_loss = 3.31579\n",
      "epoch no.4 train no.396860  loss = 4.02350 avg_loss = 3.35062\n",
      "epoch no.4 train no.396870  loss = 2.26892 avg_loss = 3.32575\n",
      "epoch no.4 train no.396880  loss = 1.82783 avg_loss = 3.31484\n",
      "epoch no.4 train no.396890  loss = 2.78040 avg_loss = 3.31616\n",
      "epoch no.4 train no.396900  loss = 3.08722 avg_loss = 3.30879\n",
      "epoch no.4 train no.396910  loss = 3.99111 avg_loss = 3.32507\n",
      "epoch no.4 train no.396920  loss = 4.67437 avg_loss = 3.31085\n",
      "epoch no.4 train no.396930  loss = 2.47622 avg_loss = 3.33651\n",
      "epoch no.4 train no.396940  loss = 2.15312 avg_loss = 3.36144\n",
      "epoch no.4 train no.396950  loss = 3.60517 avg_loss = 3.33354\n",
      "epoch no.4 train no.396960  loss = 3.64323 avg_loss = 3.37703\n",
      "epoch no.4 train no.396970  loss = 2.91732 avg_loss = 3.35596\n",
      "epoch no.4 train no.396980  loss = 3.51443 avg_loss = 3.36076\n",
      "epoch no.4 train no.396990  loss = 4.34825 avg_loss = 3.41841\n",
      "epoch no.4 train no.397000  loss = 3.38391 avg_loss = 3.44829\n",
      "4\n",
      "to_tokens: ['▁', '전환', '에', '▁딱', '▁노래', '송', '</s>']\n",
      "기분전환에 좋은 팝송</s>\n",
      "epoch no.4 train no.397010  loss = 3.95906 avg_loss = 3.50144\n",
      "epoch no.4 train no.397020  loss = 3.34403 avg_loss = 3.47663\n",
      "epoch no.4 train no.397030  loss = 3.07899 avg_loss = 3.49709\n",
      "epoch no.4 train no.397040  loss = 3.71494 avg_loss = 3.48649\n",
      "epoch no.4 train no.397050  loss = 4.52044 avg_loss = 3.43840\n",
      "epoch no.4 train no.397060  loss = 4.16282 avg_loss = 3.39468\n",
      "epoch no.4 train no.397070  loss = 3.48235 avg_loss = 3.42189\n",
      "epoch no.4 train no.397080  loss = 3.60483 avg_loss = 3.39824\n",
      "epoch no.4 train no.397090  loss = 2.99709 avg_loss = 3.40536\n",
      "epoch no.4 train no.397100  loss = 4.37760 avg_loss = 3.41992\n",
      "epoch no.4 train no.397110  loss = 6.11347 avg_loss = 3.41771\n",
      "epoch no.4 train no.397120  loss = 2.49504 avg_loss = 3.40752\n",
      "epoch no.4 train no.397130  loss = 3.73064 avg_loss = 3.42472\n",
      "epoch no.4 train no.397140  loss = 2.93567 avg_loss = 3.45055\n",
      "epoch no.4 train no.397150  loss = 2.94248 avg_loss = 3.44078\n",
      "epoch no.4 train no.397160  loss = 2.78047 avg_loss = 3.41157\n",
      "epoch no.4 train no.397170  loss = 3.10280 avg_loss = 3.43534\n",
      "epoch no.4 train no.397180  loss = 3.53977 avg_loss = 3.36498\n",
      "epoch no.4 train no.397190  loss = 2.65000 avg_loss = 3.31265\n",
      "epoch no.4 train no.397200  loss = 4.49946 avg_loss = 3.35266\n",
      "epoch no.4 train no.397210  loss = 3.39748 avg_loss = 3.32261\n",
      "epoch no.4 train no.397220  loss = 3.50536 avg_loss = 3.30810\n",
      "epoch no.4 train no.397230  loss = 4.88912 avg_loss = 3.34299\n",
      "epoch no.4 train no.397240  loss = 3.95301 avg_loss = 3.36202\n",
      "epoch no.4 train no.397250  loss = 2.60699 avg_loss = 3.36834\n",
      "epoch no.4 train no.397260  loss = 1.84362 avg_loss = 3.34310\n",
      "epoch no.4 train no.397270  loss = 2.98462 avg_loss = 3.35175\n",
      "epoch no.4 train no.397280  loss = 3.82130 avg_loss = 3.36812\n",
      "epoch no.4 train no.397290  loss = 4.19404 avg_loss = 3.38156\n",
      "epoch no.4 train no.397300  loss = 3.13788 avg_loss = 3.38469\n",
      "epoch no.4 train no.397310  loss = 3.06853 avg_loss = 3.36286\n",
      "epoch no.4 train no.397320  loss = 3.56856 avg_loss = 3.34196\n",
      "epoch no.4 train no.397330  loss = 1.53759 avg_loss = 3.30346\n",
      "epoch no.4 train no.397340  loss = 6.10415 avg_loss = 3.31226\n",
      "epoch no.4 train no.397350  loss = 3.62294 avg_loss = 3.35499\n",
      "epoch no.4 train no.397360  loss = 2.54535 avg_loss = 3.29694\n",
      "epoch no.4 train no.397370  loss = 2.65527 avg_loss = 3.29412\n",
      "epoch no.4 train no.397380  loss = 3.79045 avg_loss = 3.31454\n",
      "epoch no.4 train no.397390  loss = 3.83501 avg_loss = 3.33185\n",
      "epoch no.4 train no.397400  loss = 4.54781 avg_loss = 3.36415\n",
      "epoch no.4 train no.397410  loss = 2.61102 avg_loss = 3.41684\n",
      "epoch no.4 train no.397420  loss = 4.97240 avg_loss = 3.43555\n",
      "epoch no.4 train no.397430  loss = 3.00400 avg_loss = 3.38770\n",
      "epoch no.4 train no.397440  loss = 3.80492 avg_loss = 3.37718\n",
      "epoch no.4 train no.397450  loss = 3.57526 avg_loss = 3.39617\n",
      "epoch no.4 train no.397460  loss = 2.80448 avg_loss = 3.42184\n",
      "epoch no.4 train no.397470  loss = 2.40366 avg_loss = 3.37003\n",
      "epoch no.4 train no.397480  loss = 3.21544 avg_loss = 3.36585\n",
      "epoch no.4 train no.397490  loss = 3.84566 avg_loss = 3.38693\n",
      "epoch no.4 train no.397500  loss = 3.64809 avg_loss = 3.37085\n",
      "epoch no.4 train no.397510  loss = 3.42101 avg_loss = 3.36565\n",
      "epoch no.4 train no.397520  loss = 5.29334 avg_loss = 3.36769\n",
      "epoch no.4 train no.397530  loss = 2.02272 avg_loss = 3.34968\n",
      "epoch no.4 train no.397540  loss = 3.13744 avg_loss = 3.34357\n",
      "epoch no.4 train no.397550  loss = 3.85633 avg_loss = 3.33031\n",
      "epoch no.4 train no.397560  loss = 2.77321 avg_loss = 3.35832\n",
      "epoch no.4 train no.397570  loss = 3.14253 avg_loss = 3.40274\n",
      "epoch no.4 train no.397580  loss = 4.10721 avg_loss = 3.38276\n",
      "epoch no.4 train no.397590  loss = 2.48842 avg_loss = 3.38773\n",
      "epoch no.4 train no.397600  loss = 2.88099 avg_loss = 3.35447\n",
      "epoch no.4 train no.397610  loss = 4.22200 avg_loss = 3.39355\n",
      "epoch no.4 train no.397620  loss = 3.62026 avg_loss = 3.41410\n",
      "epoch no.4 train no.397630  loss = 2.40458 avg_loss = 3.41420\n",
      "epoch no.4 train no.397640  loss = 3.90954 avg_loss = 3.41599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.397650  loss = 2.44207 avg_loss = 3.40306\n",
      "epoch no.4 train no.397660  loss = 3.77267 avg_loss = 3.34671\n",
      "epoch no.4 train no.397670  loss = 2.43416 avg_loss = 3.36131\n",
      "epoch no.4 train no.397680  loss = 2.08278 avg_loss = 3.32874\n",
      "epoch no.4 train no.397690  loss = 2.97582 avg_loss = 3.32315\n",
      "epoch no.4 train no.397700  loss = 2.90540 avg_loss = 3.36950\n",
      "epoch no.4 train no.397710  loss = 3.42041 avg_loss = 3.33737\n",
      "epoch no.4 train no.397720  loss = 2.87256 avg_loss = 3.33637\n",
      "epoch no.4 train no.397730  loss = 3.94110 avg_loss = 3.39046\n",
      "epoch no.4 train no.397740  loss = 1.93317 avg_loss = 3.35725\n",
      "epoch no.4 train no.397750  loss = 3.17602 avg_loss = 3.38579\n",
      "epoch no.4 train no.397760  loss = 2.59522 avg_loss = 3.35719\n",
      "epoch no.4 train no.397770  loss = 2.48915 avg_loss = 3.29320\n",
      "epoch no.4 train no.397780  loss = 3.47235 avg_loss = 3.30563\n",
      "epoch no.4 train no.397790  loss = 1.79448 avg_loss = 3.34229\n",
      "epoch no.4 train no.397800  loss = 1.67900 avg_loss = 3.32869\n",
      "epoch no.4 train no.397810  loss = 3.38628 avg_loss = 3.33797\n",
      "epoch no.4 train no.397820  loss = 2.85748 avg_loss = 3.31010\n",
      "epoch no.4 train no.397830  loss = 2.28191 avg_loss = 3.35126\n",
      "epoch no.4 train no.397840  loss = 2.54937 avg_loss = 3.37456\n",
      "epoch no.4 train no.397850  loss = 1.78059 avg_loss = 3.35970\n",
      "epoch no.4 train no.397860  loss = 2.64457 avg_loss = 3.34009\n",
      "epoch no.4 train no.397870  loss = 3.30886 avg_loss = 3.33621\n",
      "epoch no.4 train no.397880  loss = 1.57400 avg_loss = 3.34181\n",
      "epoch no.4 train no.397890  loss = 2.12901 avg_loss = 3.33541\n",
      "epoch no.4 train no.397900  loss = 4.30530 avg_loss = 3.40060\n",
      "epoch no.4 train no.397910  loss = 2.29275 avg_loss = 3.37147\n",
      "epoch no.4 train no.397920  loss = 3.64291 avg_loss = 3.40093\n",
      "epoch no.4 train no.397930  loss = 2.28918 avg_loss = 3.39311\n",
      "epoch no.4 train no.397940  loss = 3.59786 avg_loss = 3.38217\n",
      "epoch no.4 train no.397950  loss = 4.16543 avg_loss = 3.35715\n",
      "epoch no.4 train no.397960  loss = 3.10893 avg_loss = 3.36706\n",
      "epoch no.4 train no.397970  loss = 3.12236 avg_loss = 3.34493\n",
      "epoch no.4 train no.397980  loss = 5.21556 avg_loss = 3.37459\n",
      "epoch no.4 train no.397990  loss = 3.79787 avg_loss = 3.38208\n",
      "epoch no.4 train no.398000  loss = 3.95032 avg_loss = 3.37488\n",
      "7\n",
      "to_tokens: ['▁', '전환', '이', '▁필요할', '때', '▁듣는', '▁음악', '한', '▁팝', '</s>']\n",
      "기분전환이 필요할때 듣는 잔잔한 노래</s>\n",
      "epoch no.4 train no.398010  loss = 4.09324 avg_loss = 3.40137\n",
      "epoch no.4 train no.398020  loss = 4.53173 avg_loss = 3.37012\n",
      "epoch no.4 train no.398030  loss = 3.34229 avg_loss = 3.37635\n",
      "epoch no.4 train no.398040  loss = 3.24027 avg_loss = 3.37723\n",
      "epoch no.4 train no.398050  loss = 4.46328 avg_loss = 3.37497\n",
      "epoch no.4 train no.398060  loss = 3.06910 avg_loss = 3.35026\n",
      "epoch no.4 train no.398070  loss = 3.82268 avg_loss = 3.34246\n",
      "epoch no.4 train no.398080  loss = 2.93243 avg_loss = 3.34095\n",
      "epoch no.4 train no.398090  loss = 2.25564 avg_loss = 3.32204\n",
      "epoch no.4 train no.398100  loss = 2.75417 avg_loss = 3.29559\n",
      "epoch no.4 train no.398110  loss = 3.69808 avg_loss = 3.33626\n",
      "epoch no.4 train no.398120  loss = 2.46710 avg_loss = 3.27725\n",
      "epoch no.4 train no.398130  loss = 2.87746 avg_loss = 3.29693\n",
      "epoch no.4 train no.398140  loss = 2.48383 avg_loss = 3.28050\n",
      "epoch no.4 train no.398150  loss = 2.49487 avg_loss = 3.29592\n",
      "epoch no.4 train no.398160  loss = 3.04810 avg_loss = 3.32712\n",
      "epoch no.4 train no.398170  loss = 2.05737 avg_loss = 3.30566\n",
      "epoch no.4 train no.398180  loss = 4.08784 avg_loss = 3.29694\n",
      "epoch no.4 train no.398190  loss = 3.22345 avg_loss = 3.30828\n",
      "epoch no.4 train no.398200  loss = 3.77698 avg_loss = 3.29089\n",
      "epoch no.4 train no.398210  loss = 3.48375 avg_loss = 3.28840\n",
      "epoch no.4 train no.398220  loss = 2.24247 avg_loss = 3.29868\n",
      "epoch no.4 train no.398230  loss = 3.10257 avg_loss = 3.28058\n",
      "epoch no.4 train no.398240  loss = 1.89701 avg_loss = 3.31573\n",
      "epoch no.4 train no.398250  loss = 4.23595 avg_loss = 3.34683\n",
      "epoch no.4 train no.398260  loss = 3.40187 avg_loss = 3.38987\n",
      "epoch no.4 train no.398270  loss = 2.92657 avg_loss = 3.42465\n",
      "epoch no.4 train no.398280  loss = 3.08172 avg_loss = 3.42334\n",
      "epoch no.4 train no.398290  loss = 3.02657 avg_loss = 3.43484\n",
      "epoch no.4 train no.398300  loss = 3.73512 avg_loss = 3.46998\n",
      "epoch no.4 train no.398310  loss = 2.88225 avg_loss = 3.41238\n",
      "epoch no.4 train no.398320  loss = 2.52467 avg_loss = 3.36726\n",
      "epoch no.4 train no.398330  loss = 3.71876 avg_loss = 3.38722\n",
      "epoch no.4 train no.398340  loss = 3.70958 avg_loss = 3.38053\n",
      "epoch no.4 train no.398350  loss = 3.06034 avg_loss = 3.35049\n",
      "epoch no.4 train no.398360  loss = 4.38109 avg_loss = 3.37488\n",
      "epoch no.4 train no.398370  loss = 3.46881 avg_loss = 3.40060\n",
      "epoch no.4 train no.398380  loss = 3.70546 avg_loss = 3.41925\n",
      "epoch no.4 train no.398390  loss = 2.28667 avg_loss = 3.35898\n",
      "epoch no.4 train no.398400  loss = 2.81190 avg_loss = 3.38448\n",
      "epoch no.4 train no.398410  loss = 2.94748 avg_loss = 3.36837\n",
      "epoch no.4 train no.398420  loss = 3.98889 avg_loss = 3.37438\n",
      "epoch no.4 train no.398430  loss = 3.80594 avg_loss = 3.40255\n",
      "epoch no.4 train no.398440  loss = 3.03081 avg_loss = 3.37005\n",
      "epoch no.4 train no.398450  loss = 3.12862 avg_loss = 3.36301\n",
      "epoch no.4 train no.398460  loss = 2.72505 avg_loss = 3.30031\n",
      "epoch no.4 train no.398470  loss = 3.57186 avg_loss = 3.36125\n",
      "epoch no.4 train no.398480  loss = 3.37933 avg_loss = 3.37351\n",
      "epoch no.4 train no.398490  loss = 4.52796 avg_loss = 3.39735\n",
      "epoch no.4 train no.398500  loss = 2.94558 avg_loss = 3.41677\n",
      "epoch no.4 train no.398510  loss = 3.23483 avg_loss = 3.40546\n",
      "epoch no.4 train no.398520  loss = 3.37011 avg_loss = 3.38551\n",
      "epoch no.4 train no.398530  loss = 3.59740 avg_loss = 3.38251\n",
      "epoch no.4 train no.398540  loss = 2.18295 avg_loss = 3.37166\n",
      "epoch no.4 train no.398550  loss = 3.85220 avg_loss = 3.43395\n",
      "epoch no.4 train no.398560  loss = 3.93076 avg_loss = 3.42751\n",
      "epoch no.4 train no.398570  loss = 2.81465 avg_loss = 3.43608\n",
      "epoch no.4 train no.398580  loss = 2.85562 avg_loss = 3.39045\n",
      "epoch no.4 train no.398590  loss = 3.03505 avg_loss = 3.41903\n",
      "epoch no.4 train no.398600  loss = 3.78917 avg_loss = 3.42060\n",
      "epoch no.4 train no.398610  loss = 6.22020 avg_loss = 3.46280\n",
      "epoch no.4 train no.398620  loss = 2.56925 avg_loss = 3.44178\n",
      "epoch no.4 train no.398630  loss = 3.73106 avg_loss = 3.43635\n",
      "epoch no.4 train no.398640  loss = 3.17894 avg_loss = 3.45825\n",
      "epoch no.4 train no.398650  loss = 4.48375 avg_loss = 3.48663\n",
      "epoch no.4 train no.398660  loss = 2.70968 avg_loss = 3.51095\n",
      "epoch no.4 train no.398670  loss = 2.16546 avg_loss = 3.47761\n",
      "epoch no.4 train no.398680  loss = 2.84909 avg_loss = 3.44112\n",
      "epoch no.4 train no.398690  loss = 1.11513 avg_loss = 3.39377\n",
      "epoch no.4 train no.398700  loss = 3.16924 avg_loss = 3.38874\n",
      "epoch no.4 train no.398710  loss = 3.43283 avg_loss = 3.34117\n",
      "epoch no.4 train no.398720  loss = 4.23704 avg_loss = 3.39016\n",
      "epoch no.4 train no.398730  loss = 3.29779 avg_loss = 3.37237\n",
      "epoch no.4 train no.398740  loss = 3.21683 avg_loss = 3.40336\n",
      "epoch no.4 train no.398750  loss = 2.89018 avg_loss = 3.39444\n",
      "epoch no.4 train no.398760  loss = 2.04881 avg_loss = 3.38213\n",
      "epoch no.4 train no.398770  loss = 3.74935 avg_loss = 3.36315\n",
      "epoch no.4 train no.398780  loss = 2.66409 avg_loss = 3.35625\n",
      "epoch no.4 train no.398790  loss = 5.09524 avg_loss = 3.35234\n",
      "epoch no.4 train no.398800  loss = 3.87282 avg_loss = 3.38587\n",
      "epoch no.4 train no.398810  loss = 4.50636 avg_loss = 3.40447\n",
      "epoch no.4 train no.398820  loss = 2.47514 avg_loss = 3.40328\n",
      "epoch no.4 train no.398830  loss = 3.32894 avg_loss = 3.41937\n",
      "epoch no.4 train no.398840  loss = 2.55080 avg_loss = 3.37239\n",
      "epoch no.4 train no.398850  loss = 3.79066 avg_loss = 3.39256\n",
      "epoch no.4 train no.398860  loss = 4.37019 avg_loss = 3.44276\n",
      "epoch no.4 train no.398870  loss = 4.36921 avg_loss = 3.43431\n",
      "epoch no.4 train no.398880  loss = 3.68876 avg_loss = 3.42227\n",
      "epoch no.4 train no.398890  loss = 5.25038 avg_loss = 3.40774\n",
      "epoch no.4 train no.398900  loss = 3.80182 avg_loss = 3.34402\n",
      "epoch no.4 train no.398910  loss = 2.48080 avg_loss = 3.33297\n",
      "epoch no.4 train no.398920  loss = 1.79498 avg_loss = 3.33174\n",
      "epoch no.4 train no.398930  loss = 1.65223 avg_loss = 3.36076\n",
      "epoch no.4 train no.398940  loss = 2.47990 avg_loss = 3.33685\n",
      "epoch no.4 train no.398950  loss = 4.11247 avg_loss = 3.36543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.398960  loss = 3.44097 avg_loss = 3.33611\n",
      "epoch no.4 train no.398970  loss = 2.69536 avg_loss = 3.27045\n",
      "epoch no.4 train no.398980  loss = 4.86147 avg_loss = 3.32023\n",
      "epoch no.4 train no.398990  loss = 3.71401 avg_loss = 3.36250\n",
      "epoch no.4 train no.399000  loss = 2.29661 avg_loss = 3.33625\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁위한', '▁신나는', '</s>']\n",
      "기분전환을 위한 노래</s>\n",
      "epoch no.4 train no.399010  loss = 3.10418 avg_loss = 3.33958\n",
      "epoch no.4 train no.399020  loss = 2.48171 avg_loss = 3.34775\n",
      "epoch no.4 train no.399030  loss = 1.84675 avg_loss = 3.31916\n",
      "epoch no.4 train no.399040  loss = 4.11202 avg_loss = 3.33600\n",
      "epoch no.4 train no.399050  loss = 3.33364 avg_loss = 3.34028\n",
      "epoch no.4 train no.399060  loss = 2.95136 avg_loss = 3.36039\n",
      "epoch no.4 train no.399070  loss = 2.97556 avg_loss = 3.35493\n",
      "epoch no.4 train no.399080  loss = 2.65949 avg_loss = 3.35828\n",
      "epoch no.4 train no.399090  loss = 3.23332 avg_loss = 3.32619\n",
      "epoch no.4 train no.399100  loss = 3.82625 avg_loss = 3.33506\n",
      "epoch no.4 train no.399110  loss = 4.97670 avg_loss = 3.37200\n",
      "epoch no.4 train no.399120  loss = 3.48912 avg_loss = 3.40269\n",
      "epoch no.4 train no.399130  loss = 2.82631 avg_loss = 3.36852\n",
      "epoch no.4 train no.399140  loss = 2.03588 avg_loss = 3.34867\n",
      "epoch no.4 train no.399150  loss = 2.13322 avg_loss = 3.30632\n",
      "epoch no.4 train no.399160  loss = 3.84743 avg_loss = 3.26994\n",
      "epoch no.4 train no.399170  loss = 2.57909 avg_loss = 3.24246\n",
      "epoch no.4 train no.399180  loss = 2.75877 avg_loss = 3.24028\n",
      "epoch no.4 train no.399190  loss = 3.79193 avg_loss = 3.22753\n",
      "epoch no.4 train no.399200  loss = 2.18422 avg_loss = 3.16596\n",
      "epoch no.4 train no.399210  loss = 2.76360 avg_loss = 3.22459\n",
      "epoch no.4 train no.399220  loss = 3.73688 avg_loss = 3.27960\n",
      "epoch no.4 train no.399230  loss = 2.77856 avg_loss = 3.26446\n",
      "epoch no.4 train no.399240  loss = 3.81364 avg_loss = 3.32148\n",
      "epoch no.4 train no.399250  loss = 3.11397 avg_loss = 3.32075\n",
      "epoch no.4 train no.399260  loss = 5.72427 avg_loss = 3.36018\n",
      "epoch no.4 train no.399270  loss = 4.24532 avg_loss = 3.34069\n",
      "epoch no.4 train no.399280  loss = 3.55916 avg_loss = 3.35832\n",
      "epoch no.4 train no.399290  loss = 5.33812 avg_loss = 3.37517\n",
      "epoch no.4 train no.399300  loss = 3.01395 avg_loss = 3.41059\n",
      "epoch no.4 train no.399310  loss = 3.06381 avg_loss = 3.38697\n",
      "epoch no.4 train no.399320  loss = 5.19435 avg_loss = 3.39071\n",
      "epoch no.4 train no.399330  loss = 3.28044 avg_loss = 3.37209\n",
      "epoch no.4 train no.399340  loss = 3.94327 avg_loss = 3.38067\n",
      "epoch no.4 train no.399350  loss = 2.41881 avg_loss = 3.43262\n",
      "epoch no.4 train no.399360  loss = 1.96206 avg_loss = 3.36777\n",
      "epoch no.4 train no.399370  loss = 4.19792 avg_loss = 3.40703\n",
      "epoch no.4 train no.399380  loss = 1.89220 avg_loss = 3.41187\n",
      "epoch no.4 train no.399390  loss = 3.40097 avg_loss = 3.39719\n",
      "epoch no.4 train no.399400  loss = 3.05561 avg_loss = 3.41427\n",
      "epoch no.4 train no.399410  loss = 1.03996 avg_loss = 3.38677\n",
      "epoch no.4 train no.399420  loss = 2.90131 avg_loss = 3.39486\n",
      "epoch no.4 train no.399430  loss = 3.42351 avg_loss = 3.38607\n",
      "epoch no.4 train no.399440  loss = 2.20768 avg_loss = 3.39224\n",
      "epoch no.4 train no.399450  loss = 2.86433 avg_loss = 3.39420\n",
      "epoch no.4 train no.399460  loss = 3.85308 avg_loss = 3.46514\n",
      "epoch no.4 train no.399470  loss = 4.53541 avg_loss = 3.48358\n",
      "epoch no.4 train no.399480  loss = 3.15557 avg_loss = 3.44574\n",
      "epoch no.4 train no.399490  loss = 2.57939 avg_loss = 3.36748\n",
      "epoch no.4 train no.399500  loss = 3.78193 avg_loss = 3.34789\n",
      "epoch no.4 train no.399510  loss = 3.23624 avg_loss = 3.32539\n",
      "epoch no.4 train no.399520  loss = 2.84996 avg_loss = 3.40887\n",
      "epoch no.4 train no.399530  loss = 3.52012 avg_loss = 3.45745\n",
      "epoch no.4 train no.399540  loss = 3.87141 avg_loss = 3.49302\n",
      "epoch no.4 train no.399550  loss = 4.21050 avg_loss = 3.50907\n",
      "epoch no.4 train no.399560  loss = 2.65075 avg_loss = 3.48284\n",
      "epoch no.4 train no.399570  loss = 3.13351 avg_loss = 3.48100\n",
      "epoch no.4 train no.399580  loss = 2.95074 avg_loss = 3.46523\n",
      "epoch no.4 train no.399590  loss = 2.29946 avg_loss = 3.43220\n",
      "epoch no.4 train no.399600  loss = 2.31298 avg_loss = 3.42066\n",
      "epoch no.4 train no.399610  loss = 3.97726 avg_loss = 3.41356\n",
      "epoch no.4 train no.399620  loss = 2.85220 avg_loss = 3.43288\n",
      "epoch no.4 train no.399630  loss = 2.90131 avg_loss = 3.41250\n",
      "epoch no.4 train no.399640  loss = 4.56033 avg_loss = 3.45965\n",
      "epoch no.4 train no.399650  loss = 2.64156 avg_loss = 3.47782\n",
      "epoch no.4 train no.399660  loss = 2.02327 avg_loss = 3.49966\n",
      "epoch no.4 train no.399670  loss = 3.15347 avg_loss = 3.50842\n",
      "epoch no.4 train no.399680  loss = 5.88635 avg_loss = 3.49556\n",
      "epoch no.4 train no.399690  loss = 3.48080 avg_loss = 3.46860\n",
      "epoch no.4 train no.399700  loss = 3.69910 avg_loss = 3.52002\n",
      "epoch no.4 train no.399710  loss = 4.61268 avg_loss = 3.53382\n",
      "epoch no.4 train no.399720  loss = 4.34315 avg_loss = 3.55080\n",
      "epoch no.4 train no.399730  loss = 4.02006 avg_loss = 3.54935\n",
      "epoch no.4 train no.399740  loss = 4.78120 avg_loss = 3.52306\n",
      "epoch no.4 train no.399750  loss = 3.17060 avg_loss = 3.47156\n",
      "epoch no.4 train no.399760  loss = 3.66086 avg_loss = 3.44645\n",
      "epoch no.4 train no.399770  loss = 2.93468 avg_loss = 3.41728\n",
      "epoch no.4 train no.399780  loss = 3.85144 avg_loss = 3.40988\n",
      "epoch no.4 train no.399790  loss = 4.47879 avg_loss = 3.38924\n",
      "epoch no.4 train no.399800  loss = 3.17673 avg_loss = 3.38495\n",
      "epoch no.4 train no.399810  loss = 3.30825 avg_loss = 3.34964\n",
      "epoch no.4 train no.399820  loss = 3.81989 avg_loss = 3.33837\n",
      "epoch no.4 train no.399830  loss = 6.00059 avg_loss = 3.32527\n",
      "epoch no.4 train no.399840  loss = 5.33013 avg_loss = 3.32288\n",
      "epoch no.4 train no.399850  loss = 3.83157 avg_loss = 3.29785\n",
      "epoch no.4 train no.399860  loss = 4.06725 avg_loss = 3.33204\n",
      "epoch no.4 train no.399870  loss = 1.93481 avg_loss = 3.34465\n",
      "epoch no.4 train no.399880  loss = 3.82995 avg_loss = 3.33385\n",
      "epoch no.4 train no.399890  loss = 3.52184 avg_loss = 3.33130\n",
      "epoch no.4 train no.399900  loss = 1.90802 avg_loss = 3.29984\n",
      "epoch no.4 train no.399910  loss = 2.89259 avg_loss = 3.35520\n",
      "epoch no.4 train no.399920  loss = 2.77507 avg_loss = 3.35029\n",
      "epoch no.4 train no.399930  loss = 3.19401 avg_loss = 3.40240\n",
      "epoch no.4 train no.399940  loss = 2.91171 avg_loss = 3.37488\n",
      "epoch no.4 train no.399950  loss = 1.84998 avg_loss = 3.42337\n",
      "epoch no.4 train no.399960  loss = 1.68670 avg_loss = 3.40295\n",
      "epoch no.4 train no.399970  loss = 2.71042 avg_loss = 3.36511\n",
      "epoch no.4 train no.399980  loss = 2.89366 avg_loss = 3.35930\n",
      "epoch no.4 train no.399990  loss = 3.21600 avg_loss = 3.34697\n",
      "epoch no.4 train no.400000  loss = 2.77157 avg_loss = 3.38522\n",
      "3\n",
      "to_tokens: ['▁라디오', '전환', '에', '▁좋은', '▁노래', '</s>']\n",
      "기분전환에 좋은 음악</s>\n",
      "epoch no.4 train no.400010  loss = 5.01765 avg_loss = 3.40884\n",
      "epoch no.4 train no.400020  loss = 3.34063 avg_loss = 3.40647\n",
      "epoch no.4 train no.400030  loss = 2.44563 avg_loss = 3.39717\n",
      "epoch no.4 train no.400040  loss = 3.33957 avg_loss = 3.44753\n",
      "epoch no.4 train no.400050  loss = 2.79463 avg_loss = 3.42351\n",
      "epoch no.4 train no.400060  loss = 3.89531 avg_loss = 3.43830\n",
      "epoch no.4 train no.400070  loss = 2.19912 avg_loss = 3.41430\n",
      "epoch no.4 train no.400080  loss = 3.41584 avg_loss = 3.41373\n",
      "epoch no.4 train no.400090  loss = 1.91270 avg_loss = 3.36884\n",
      "epoch no.4 train no.400100  loss = 4.76746 avg_loss = 3.42706\n",
      "epoch no.4 train no.400110  loss = 3.33247 avg_loss = 3.40190\n",
      "epoch no.4 train no.400120  loss = 4.34120 avg_loss = 3.42065\n",
      "epoch no.4 train no.400130  loss = 2.86637 avg_loss = 3.38909\n",
      "epoch no.4 train no.400140  loss = 3.88099 avg_loss = 3.41855\n",
      "epoch no.4 train no.400150  loss = 2.27711 avg_loss = 3.39338\n",
      "epoch no.4 train no.400160  loss = 3.09721 avg_loss = 3.41705\n",
      "epoch no.4 train no.400170  loss = 3.50799 avg_loss = 3.42218\n",
      "epoch no.4 train no.400180  loss = 2.88856 avg_loss = 3.42990\n",
      "epoch no.4 train no.400190  loss = 4.53090 avg_loss = 3.44362\n",
      "epoch no.4 train no.400200  loss = 3.64730 avg_loss = 3.51791\n",
      "epoch no.4 train no.400210  loss = 3.38608 avg_loss = 3.49641\n",
      "epoch no.4 train no.400220  loss = 2.87069 avg_loss = 3.47012\n",
      "epoch no.4 train no.400230  loss = 4.36272 avg_loss = 3.46206\n",
      "epoch no.4 train no.400240  loss = 3.14499 avg_loss = 3.44939\n",
      "epoch no.4 train no.400250  loss = 2.25407 avg_loss = 3.44621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.400260  loss = 2.17712 avg_loss = 3.40134\n",
      "epoch no.4 train no.400270  loss = 2.65245 avg_loss = 3.39268\n",
      "epoch no.4 train no.400280  loss = 3.78991 avg_loss = 3.36878\n",
      "epoch no.4 train no.400290  loss = 2.73031 avg_loss = 3.39363\n",
      "epoch no.4 train no.400300  loss = 3.36194 avg_loss = 3.39385\n",
      "epoch no.4 train no.400310  loss = 3.19812 avg_loss = 3.38760\n",
      "epoch no.4 train no.400320  loss = 3.94430 avg_loss = 3.35809\n",
      "epoch no.4 train no.400330  loss = 2.81969 avg_loss = 3.36273\n",
      "epoch no.4 train no.400340  loss = 2.62090 avg_loss = 3.35278\n",
      "epoch no.4 train no.400350  loss = 3.81295 avg_loss = 3.39821\n",
      "epoch no.4 train no.400360  loss = 1.87619 avg_loss = 3.34925\n",
      "epoch no.4 train no.400370  loss = 2.71996 avg_loss = 3.35971\n",
      "epoch no.4 train no.400380  loss = 3.94708 avg_loss = 3.36258\n",
      "epoch no.4 train no.400390  loss = 2.46455 avg_loss = 3.34879\n",
      "epoch no.4 train no.400400  loss = 2.20909 avg_loss = 3.38043\n",
      "epoch no.4 train no.400410  loss = 2.54044 avg_loss = 3.38851\n",
      "epoch no.4 train no.400420  loss = 2.91694 avg_loss = 3.35999\n",
      "epoch no.4 train no.400430  loss = 4.75542 avg_loss = 3.33207\n",
      "epoch no.4 train no.400440  loss = 3.52529 avg_loss = 3.32901\n",
      "epoch no.4 train no.400450  loss = 2.39628 avg_loss = 3.30730\n",
      "epoch no.4 train no.400460  loss = 1.80971 avg_loss = 3.33198\n",
      "epoch no.4 train no.400470  loss = 3.17252 avg_loss = 3.31175\n",
      "epoch no.4 train no.400480  loss = 2.92841 avg_loss = 3.32699\n",
      "epoch no.4 train no.400490  loss = 3.42990 avg_loss = 3.27743\n",
      "epoch no.4 train no.400500  loss = 2.74787 avg_loss = 3.23290\n",
      "epoch no.4 train no.400510  loss = 3.80489 avg_loss = 3.25724\n",
      "epoch no.4 train no.400520  loss = 2.89778 avg_loss = 3.23648\n",
      "epoch no.4 train no.400530  loss = 2.83097 avg_loss = 3.27612\n",
      "epoch no.4 train no.400540  loss = 4.21449 avg_loss = 3.30155\n",
      "epoch no.4 train no.400550  loss = 3.86431 avg_loss = 3.31723\n",
      "epoch no.4 train no.400560  loss = 2.78237 avg_loss = 3.34454\n",
      "epoch no.4 train no.400570  loss = 3.41835 avg_loss = 3.33670\n",
      "epoch no.4 train no.400580  loss = 3.34144 avg_loss = 3.38692\n",
      "epoch no.4 train no.400590  loss = 3.28330 avg_loss = 3.40276\n",
      "epoch no.4 train no.400600  loss = 1.91471 avg_loss = 3.36017\n",
      "epoch no.4 train no.400610  loss = 2.23761 avg_loss = 3.30058\n",
      "epoch no.4 train no.400620  loss = 3.62037 avg_loss = 3.32507\n",
      "epoch no.4 train no.400630  loss = 5.25778 avg_loss = 3.32247\n",
      "epoch no.4 train no.400640  loss = 2.42522 avg_loss = 3.32166\n",
      "epoch no.4 train no.400650  loss = 2.42670 avg_loss = 3.31517\n",
      "epoch no.4 train no.400660  loss = 3.65542 avg_loss = 3.31410\n",
      "epoch no.4 train no.400670  loss = 3.38227 avg_loss = 3.33917\n",
      "epoch no.4 train no.400680  loss = 5.16246 avg_loss = 3.33600\n",
      "epoch no.4 train no.400690  loss = 4.31857 avg_loss = 3.34822\n",
      "epoch no.4 train no.400700  loss = 1.43257 avg_loss = 3.29980\n",
      "epoch no.4 train no.400710  loss = 2.29715 avg_loss = 3.29611\n",
      "epoch no.4 train no.400720  loss = 2.57219 avg_loss = 3.26540\n",
      "epoch no.4 train no.400730  loss = 3.19459 avg_loss = 3.26393\n",
      "epoch no.4 train no.400740  loss = 3.08405 avg_loss = 3.22130\n",
      "epoch no.4 train no.400750  loss = 4.10398 avg_loss = 3.25664\n",
      "epoch no.4 train no.400760  loss = 3.38346 avg_loss = 3.23631\n",
      "epoch no.4 train no.400770  loss = 2.60059 avg_loss = 3.21729\n",
      "epoch no.4 train no.400780  loss = 2.08432 avg_loss = 3.23043\n",
      "epoch no.4 train no.400790  loss = 3.94799 avg_loss = 3.25683\n",
      "epoch no.4 train no.400800  loss = 2.99937 avg_loss = 3.27675\n",
      "epoch no.4 train no.400810  loss = 4.16768 avg_loss = 3.24976\n",
      "epoch no.4 train no.400820  loss = 4.12606 avg_loss = 3.24406\n",
      "epoch no.4 train no.400830  loss = 2.04997 avg_loss = 3.24828\n",
      "epoch no.4 train no.400840  loss = 3.32832 avg_loss = 3.25143\n",
      "epoch no.4 train no.400850  loss = 3.59294 avg_loss = 3.27717\n",
      "epoch no.4 train no.400860  loss = 3.20952 avg_loss = 3.30572\n",
      "epoch no.4 train no.400870  loss = 2.91621 avg_loss = 3.29638\n",
      "epoch no.4 train no.400880  loss = 5.24819 avg_loss = 3.30747\n",
      "epoch no.4 train no.400890  loss = 2.64587 avg_loss = 3.31135\n",
      "epoch no.4 train no.400900  loss = 3.88222 avg_loss = 3.38957\n",
      "epoch no.4 train no.400910  loss = 3.62709 avg_loss = 3.38409\n",
      "epoch no.4 train no.400920  loss = 3.28757 avg_loss = 3.34977\n",
      "epoch no.4 train no.400930  loss = 4.10768 avg_loss = 3.39133\n",
      "epoch no.4 train no.400940  loss = 3.27536 avg_loss = 3.36854\n",
      "epoch no.4 train no.400950  loss = 3.78588 avg_loss = 3.41546\n",
      "epoch no.4 train no.400960  loss = 2.38003 avg_loss = 3.42912\n",
      "epoch no.4 train no.400970  loss = 2.70132 avg_loss = 3.40753\n",
      "epoch no.4 train no.400980  loss = 3.98826 avg_loss = 3.43290\n",
      "epoch no.4 train no.400990  loss = 2.23817 avg_loss = 3.45557\n",
      "epoch no.4 train no.401000  loss = 4.70813 avg_loss = 3.46881\n",
      "6\n",
      "to_tokens: ['▁비', '전환', '이', '싶', '을', '때', '</s>', '▁노래', '</s>']\n",
      "기분전환하고싶을때 듣는노래</s>\n",
      "epoch no.4 train no.401010  loss = 4.21269 avg_loss = 3.42957\n",
      "epoch no.4 train no.401020  loss = 3.02848 avg_loss = 3.42809\n",
      "epoch no.4 train no.401030  loss = 2.97543 avg_loss = 3.42389\n",
      "epoch no.4 train no.401040  loss = 3.78351 avg_loss = 3.45563\n",
      "epoch no.4 train no.401050  loss = 4.59772 avg_loss = 3.49929\n",
      "epoch no.4 train no.401060  loss = 5.40132 avg_loss = 3.48241\n",
      "epoch no.4 train no.401070  loss = 1.87790 avg_loss = 3.44045\n",
      "epoch no.4 train no.401080  loss = 5.01669 avg_loss = 3.47077\n",
      "epoch no.4 train no.401090  loss = 3.29571 avg_loss = 3.48795\n",
      "epoch no.4 train no.401100  loss = 3.16073 avg_loss = 3.43748\n",
      "epoch no.4 train no.401110  loss = 3.07374 avg_loss = 3.45680\n",
      "epoch no.4 train no.401120  loss = 3.13883 avg_loss = 3.42795\n",
      "epoch no.4 train no.401130  loss = 4.73988 avg_loss = 3.45805\n",
      "epoch no.4 train no.401140  loss = 3.62757 avg_loss = 3.44457\n",
      "epoch no.4 train no.401150  loss = 3.40188 avg_loss = 3.43577\n",
      "epoch no.4 train no.401160  loss = 2.27763 avg_loss = 3.40230\n",
      "epoch no.4 train no.401170  loss = 3.51187 avg_loss = 3.40492\n",
      "epoch no.4 train no.401180  loss = 3.75369 avg_loss = 3.38814\n",
      "epoch no.4 train no.401190  loss = 2.67185 avg_loss = 3.37567\n",
      "epoch no.4 train no.401200  loss = 3.89725 avg_loss = 3.38053\n",
      "epoch no.4 train no.401210  loss = 3.78725 avg_loss = 3.40167\n",
      "epoch no.4 train no.401220  loss = 3.09668 avg_loss = 3.39236\n",
      "epoch no.4 train no.401230  loss = 5.57751 avg_loss = 3.40111\n",
      "epoch no.4 train no.401240  loss = 4.58518 avg_loss = 3.45130\n",
      "epoch no.4 train no.401250  loss = 2.98954 avg_loss = 3.44912\n",
      "epoch no.4 train no.401260  loss = 2.46188 avg_loss = 3.41619\n",
      "epoch no.4 train no.401270  loss = 3.96915 avg_loss = 3.36699\n",
      "epoch no.4 train no.401280  loss = 4.66337 avg_loss = 3.33664\n",
      "epoch no.4 train no.401290  loss = 2.39831 avg_loss = 3.33486\n",
      "epoch no.4 train no.401300  loss = 5.04251 avg_loss = 3.38679\n",
      "epoch no.4 train no.401310  loss = 2.50116 avg_loss = 3.38404\n",
      "epoch no.4 train no.401320  loss = 3.18965 avg_loss = 3.39726\n",
      "epoch no.4 train no.401330  loss = 3.35113 avg_loss = 3.36462\n",
      "epoch no.4 train no.401340  loss = 3.19006 avg_loss = 3.38678\n",
      "epoch no.4 train no.401350  loss = 3.95410 avg_loss = 3.40200\n",
      "epoch no.4 train no.401360  loss = 4.23897 avg_loss = 3.40296\n",
      "epoch no.4 train no.401370  loss = 3.55589 avg_loss = 3.35578\n",
      "epoch no.4 train no.401380  loss = 4.06054 avg_loss = 3.38455\n",
      "epoch no.4 train no.401390  loss = 3.27388 avg_loss = 3.40044\n",
      "epoch no.4 train no.401400  loss = 3.73900 avg_loss = 3.38809\n",
      "epoch no.4 train no.401410  loss = 3.63996 avg_loss = 3.36232\n",
      "epoch no.4 train no.401420  loss = 6.52768 avg_loss = 3.43901\n",
      "epoch no.4 train no.401430  loss = 2.11380 avg_loss = 3.41950\n",
      "epoch no.4 train no.401440  loss = 3.17227 avg_loss = 3.41540\n",
      "epoch no.4 train no.401450  loss = 3.97967 avg_loss = 3.37425\n",
      "epoch no.4 train no.401460  loss = 3.28569 avg_loss = 3.40570\n",
      "epoch no.4 train no.401470  loss = 2.89938 avg_loss = 3.36928\n",
      "epoch no.4 train no.401480  loss = 3.88547 avg_loss = 3.37823\n",
      "epoch no.4 train no.401490  loss = 3.66433 avg_loss = 3.43806\n",
      "epoch no.4 train no.401500  loss = 3.55139 avg_loss = 3.37126\n",
      "epoch no.4 train no.401510  loss = 4.36890 avg_loss = 3.34351\n",
      "epoch no.4 train no.401520  loss = 4.54882 avg_loss = 3.36726\n",
      "epoch no.4 train no.401530  loss = 1.83727 avg_loss = 3.34822\n",
      "epoch no.4 train no.401540  loss = 2.17488 avg_loss = 3.29527\n",
      "epoch no.4 train no.401550  loss = 6.26705 avg_loss = 3.32756\n",
      "epoch no.4 train no.401560  loss = 4.79939 avg_loss = 3.37511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.401570  loss = 3.21247 avg_loss = 3.42258\n",
      "epoch no.4 train no.401580  loss = 7.41944 avg_loss = 3.44747\n",
      "epoch no.4 train no.401590  loss = 2.93662 avg_loss = 3.37565\n",
      "epoch no.4 train no.401600  loss = 2.58544 avg_loss = 3.33921\n",
      "epoch no.4 train no.401610  loss = 3.24487 avg_loss = 3.35102\n",
      "epoch no.4 train no.401620  loss = 5.82405 avg_loss = 3.43031\n",
      "epoch no.4 train no.401630  loss = 3.85140 avg_loss = 3.40800\n",
      "epoch no.4 train no.401640  loss = 3.47559 avg_loss = 3.37904\n",
      "epoch no.4 train no.401650  loss = 3.33586 avg_loss = 3.34745\n",
      "epoch no.4 train no.401660  loss = 2.86578 avg_loss = 3.36914\n",
      "epoch no.4 train no.401670  loss = 3.92086 avg_loss = 3.38117\n",
      "epoch no.4 train no.401680  loss = 3.75963 avg_loss = 3.40130\n",
      "epoch no.4 train no.401690  loss = 3.62767 avg_loss = 3.37902\n",
      "epoch no.4 train no.401700  loss = 2.18861 avg_loss = 3.37591\n",
      "epoch no.4 train no.401710  loss = 2.78707 avg_loss = 3.38389\n",
      "epoch no.4 train no.401720  loss = 4.60222 avg_loss = 3.38366\n",
      "epoch no.4 train no.401730  loss = 2.92297 avg_loss = 3.43191\n",
      "epoch no.4 train no.401740  loss = 2.86958 avg_loss = 3.42450\n",
      "epoch no.4 train no.401750  loss = 3.79859 avg_loss = 3.39787\n",
      "epoch no.4 train no.401760  loss = 3.57746 avg_loss = 3.34621\n",
      "epoch no.4 train no.401770  loss = 3.62672 avg_loss = 3.33559\n",
      "epoch no.4 train no.401780  loss = 4.92327 avg_loss = 3.38260\n",
      "epoch no.4 train no.401790  loss = 2.11315 avg_loss = 3.37218\n",
      "epoch no.4 train no.401800  loss = 2.86625 avg_loss = 3.38950\n",
      "epoch no.4 train no.401810  loss = 4.71468 avg_loss = 3.39484\n",
      "epoch no.4 train no.401820  loss = 3.31988 avg_loss = 3.39465\n",
      "epoch no.4 train no.401830  loss = 2.71443 avg_loss = 3.36439\n",
      "epoch no.4 train no.401840  loss = 4.87349 avg_loss = 3.35454\n",
      "epoch no.4 train no.401850  loss = 2.56516 avg_loss = 3.30966\n",
      "epoch no.4 train no.401860  loss = 4.16702 avg_loss = 3.32750\n",
      "epoch no.4 train no.401870  loss = 2.42963 avg_loss = 3.33621\n",
      "epoch no.4 train no.401880  loss = 4.42438 avg_loss = 3.33595\n",
      "epoch no.4 train no.401890  loss = 3.31172 avg_loss = 3.41186\n",
      "epoch no.4 train no.401900  loss = 3.76565 avg_loss = 3.42503\n",
      "epoch no.4 train no.401910  loss = 4.29565 avg_loss = 3.38969\n",
      "epoch no.4 train no.401920  loss = 2.56956 avg_loss = 3.36378\n",
      "epoch no.4 train no.401930  loss = 4.90319 avg_loss = 3.41184\n",
      "epoch no.4 train no.401940  loss = 3.25615 avg_loss = 3.39221\n",
      "epoch no.4 train no.401950  loss = 4.40269 avg_loss = 3.39612\n",
      "epoch no.4 train no.401960  loss = 3.14094 avg_loss = 3.38601\n",
      "epoch no.4 train no.401970  loss = 3.40861 avg_loss = 3.36582\n",
      "epoch no.4 train no.401980  loss = 5.96090 avg_loss = 3.38569\n",
      "epoch no.4 train no.401990  loss = 2.54306 avg_loss = 3.34205\n",
      "epoch no.4 train no.402000  loss = 4.02011 avg_loss = 3.40732\n",
      "3\n",
      "to_tokens: ['▁가을', '전환', '이', '▁좋은', '▁노래', '힙']\n",
      "기분전환에 좋은 재즈</s>\n",
      "epoch no.4 train no.402010  loss = 1.98367 avg_loss = 3.38966\n",
      "epoch no.4 train no.402020  loss = 3.90175 avg_loss = 3.38224\n",
      "epoch no.4 train no.402030  loss = 2.53494 avg_loss = 3.33050\n",
      "epoch no.4 train no.402040  loss = 3.49652 avg_loss = 3.36238\n",
      "epoch no.4 train no.402050  loss = 2.75161 avg_loss = 3.38732\n",
      "epoch no.4 train no.402060  loss = 2.32255 avg_loss = 3.41841\n",
      "epoch no.4 train no.402070  loss = 3.38512 avg_loss = 3.42829\n",
      "epoch no.4 train no.402080  loss = 4.30791 avg_loss = 3.40139\n",
      "epoch no.4 train no.402090  loss = 2.84303 avg_loss = 3.40901\n",
      "epoch no.4 train no.402100  loss = 2.36213 avg_loss = 3.43661\n",
      "epoch no.4 train no.402110  loss = 2.31545 avg_loss = 3.41107\n",
      "epoch no.4 train no.402120  loss = 2.74455 avg_loss = 3.39492\n",
      "epoch no.4 train no.402130  loss = 3.76417 avg_loss = 3.44786\n",
      "epoch no.4 train no.402140  loss = 2.65780 avg_loss = 3.41018\n",
      "epoch no.4 train no.402150  loss = 4.89690 avg_loss = 3.38286\n",
      "epoch no.4 train no.402160  loss = 2.49955 avg_loss = 3.36588\n",
      "epoch no.4 train no.402170  loss = 5.26132 avg_loss = 3.42745\n",
      "epoch no.4 train no.402180  loss = 1.90404 avg_loss = 3.41184\n",
      "epoch no.4 train no.402190  loss = 2.58671 avg_loss = 3.38490\n",
      "epoch no.4 train no.402200  loss = 2.54753 avg_loss = 3.37384\n",
      "epoch no.4 train no.402210  loss = 4.68916 avg_loss = 3.39546\n",
      "epoch no.4 train no.402220  loss = 3.57835 avg_loss = 3.41860\n",
      "epoch no.4 train no.402230  loss = 3.91442 avg_loss = 3.40925\n",
      "epoch no.4 train no.402240  loss = 3.90370 avg_loss = 3.39422\n",
      "epoch no.4 train no.402250  loss = 3.68915 avg_loss = 3.36479\n",
      "epoch no.4 train no.402260  loss = 3.64542 avg_loss = 3.33007\n",
      "epoch no.4 train no.402270  loss = 2.18582 avg_loss = 3.37256\n",
      "epoch no.4 train no.402280  loss = 4.19607 avg_loss = 3.41650\n",
      "epoch no.4 train no.402290  loss = 3.20824 avg_loss = 3.42474\n",
      "epoch no.4 train no.402300  loss = 5.53925 avg_loss = 3.44993\n",
      "epoch no.4 train no.402310  loss = 3.00918 avg_loss = 3.44161\n",
      "epoch no.4 train no.402320  loss = 2.95516 avg_loss = 3.41978\n",
      "epoch no.4 train no.402330  loss = 3.35593 avg_loss = 3.42208\n",
      "epoch no.4 train no.402340  loss = 3.72604 avg_loss = 3.41886\n",
      "epoch no.4 train no.402350  loss = 4.13982 avg_loss = 3.42656\n",
      "epoch no.4 train no.402360  loss = 3.06743 avg_loss = 3.43346\n",
      "epoch no.4 train no.402370  loss = 3.24292 avg_loss = 3.45055\n",
      "epoch no.4 train no.402380  loss = 2.49860 avg_loss = 3.43259\n",
      "epoch no.4 train no.402390  loss = 2.02666 avg_loss = 3.43631\n",
      "epoch no.4 train no.402400  loss = 3.40646 avg_loss = 3.44855\n",
      "epoch no.4 train no.402410  loss = 3.21951 avg_loss = 3.41143\n",
      "epoch no.4 train no.402420  loss = 4.18336 avg_loss = 3.36277\n",
      "epoch no.4 train no.402430  loss = 3.63375 avg_loss = 3.39318\n",
      "epoch no.4 train no.402440  loss = 3.58737 avg_loss = 3.40399\n",
      "epoch no.4 train no.402450  loss = 4.17689 avg_loss = 3.40125\n",
      "epoch no.4 train no.402460  loss = 4.05482 avg_loss = 3.41169\n",
      "epoch no.4 train no.402470  loss = 3.44125 avg_loss = 3.37394\n",
      "epoch no.4 train no.402480  loss = 3.89643 avg_loss = 3.38140\n",
      "epoch no.4 train no.402490  loss = 4.27121 avg_loss = 3.37922\n",
      "epoch no.4 train no.402500  loss = 2.89147 avg_loss = 3.40310\n",
      "epoch no.4 train no.402510  loss = 4.64554 avg_loss = 3.40045\n",
      "epoch no.4 train no.402520  loss = 3.93600 avg_loss = 3.36890\n",
      "epoch no.4 train no.402530  loss = 5.04577 avg_loss = 3.36683\n",
      "epoch no.4 train no.402540  loss = 2.95073 avg_loss = 3.37572\n",
      "epoch no.4 train no.402550  loss = 3.83946 avg_loss = 3.39603\n",
      "epoch no.4 train no.402560  loss = 2.28973 avg_loss = 3.38101\n",
      "epoch no.4 train no.402570  loss = 2.68275 avg_loss = 3.39308\n",
      "epoch no.4 train no.402580  loss = 6.06324 avg_loss = 3.39809\n",
      "epoch no.4 train no.402590  loss = 2.06213 avg_loss = 3.35086\n",
      "epoch no.4 train no.402600  loss = 3.19466 avg_loss = 3.34309\n",
      "epoch no.4 train no.402610  loss = 3.92166 avg_loss = 3.32556\n",
      "epoch no.4 train no.402620  loss = 3.34979 avg_loss = 3.35069\n",
      "epoch no.4 train no.402630  loss = 2.82595 avg_loss = 3.32330\n",
      "epoch no.4 train no.402640  loss = 2.74927 avg_loss = 3.33903\n",
      "epoch no.4 train no.402650  loss = 4.42146 avg_loss = 3.33478\n",
      "epoch no.4 train no.402660  loss = 3.85898 avg_loss = 3.37347\n",
      "epoch no.4 train no.402670  loss = 4.35841 avg_loss = 3.47511\n",
      "epoch no.4 train no.402680  loss = 4.38150 avg_loss = 3.41282\n",
      "epoch no.4 train no.402690  loss = 4.30362 avg_loss = 3.45313\n",
      "epoch no.4 train no.402700  loss = 3.03825 avg_loss = 3.39658\n",
      "epoch no.4 train no.402710  loss = 4.02587 avg_loss = 3.39873\n",
      "epoch no.4 train no.402720  loss = 5.06900 avg_loss = 3.37659\n",
      "epoch no.4 train no.402730  loss = 3.22541 avg_loss = 3.40318\n",
      "epoch no.4 train no.402740  loss = 3.39495 avg_loss = 3.37168\n",
      "epoch no.4 train no.402750  loss = 2.51536 avg_loss = 3.38033\n",
      "epoch no.4 train no.402760  loss = 4.62059 avg_loss = 3.38109\n",
      "epoch no.4 train no.402770  loss = 2.94330 avg_loss = 3.36350\n",
      "epoch no.4 train no.402780  loss = 1.97614 avg_loss = 3.39256\n",
      "epoch no.4 train no.402790  loss = 2.93678 avg_loss = 3.39331\n",
      "epoch no.4 train no.402800  loss = 2.78026 avg_loss = 3.41733\n",
      "epoch no.4 train no.402810  loss = 3.20435 avg_loss = 3.37917\n",
      "epoch no.4 train no.402820  loss = 3.60462 avg_loss = 3.35349\n",
      "epoch no.4 train no.402830  loss = 4.96080 avg_loss = 3.36545\n",
      "epoch no.4 train no.402840  loss = 3.80750 avg_loss = 3.36140\n",
      "epoch no.4 train no.402850  loss = 2.66645 avg_loss = 3.36643\n",
      "epoch no.4 train no.402860  loss = 3.68653 avg_loss = 3.32083\n",
      "epoch no.4 train no.402870  loss = 3.01405 avg_loss = 3.34199\n",
      "epoch no.4 train no.402880  loss = 2.32587 avg_loss = 3.34229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.402890  loss = 2.79998 avg_loss = 3.36573\n",
      "epoch no.4 train no.402900  loss = 3.44053 avg_loss = 3.36707\n",
      "epoch no.4 train no.402910  loss = 4.41484 avg_loss = 3.36332\n",
      "epoch no.4 train no.402920  loss = 2.07845 avg_loss = 3.35945\n",
      "epoch no.4 train no.402930  loss = 5.31334 avg_loss = 3.37472\n",
      "epoch no.4 train no.402940  loss = 4.20192 avg_loss = 3.38510\n",
      "epoch no.4 train no.402950  loss = 2.53667 avg_loss = 3.37533\n",
      "epoch no.4 train no.402960  loss = 2.41396 avg_loss = 3.40096\n",
      "epoch no.4 train no.402970  loss = 2.63894 avg_loss = 3.39318\n",
      "epoch no.4 train no.402980  loss = 2.56369 avg_loss = 3.37570\n",
      "epoch no.4 train no.402990  loss = 2.61661 avg_loss = 3.37390\n",
      "epoch no.4 train no.403000  loss = 3.51487 avg_loss = 3.33206\n",
      "3\n",
      "to_tokens: ['▁비', '▁좋은', '이', '▁필요할', '▁때', '</s>']\n",
      "기분전환이 필요할때</s>\n",
      "epoch no.4 train no.403010  loss = 7.03448 avg_loss = 3.34693\n",
      "epoch no.4 train no.403020  loss = 3.44037 avg_loss = 3.37903\n",
      "epoch no.4 train no.403030  loss = 3.13459 avg_loss = 3.37511\n",
      "epoch no.4 train no.403040  loss = 4.92816 avg_loss = 3.40872\n",
      "epoch no.4 train no.403050  loss = 3.84271 avg_loss = 3.39828\n",
      "epoch no.4 train no.403060  loss = 2.52934 avg_loss = 3.39551\n",
      "epoch no.4 train no.403070  loss = 2.99291 avg_loss = 3.38579\n",
      "epoch no.4 train no.403080  loss = 4.32367 avg_loss = 3.39274\n",
      "epoch no.4 train no.403090  loss = 2.41765 avg_loss = 3.40719\n",
      "epoch no.4 train no.403100  loss = 2.34397 avg_loss = 3.38013\n",
      "epoch no.4 train no.403110  loss = 2.88384 avg_loss = 3.40702\n",
      "epoch no.4 train no.403120  loss = 2.63784 avg_loss = 3.40552\n",
      "epoch no.4 train no.403130  loss = 3.75416 avg_loss = 3.40104\n",
      "epoch no.4 train no.403140  loss = 3.80499 avg_loss = 3.47131\n",
      "epoch no.4 train no.403150  loss = 2.43999 avg_loss = 3.44820\n",
      "epoch no.4 train no.403160  loss = 5.26852 avg_loss = 3.46507\n",
      "epoch no.4 train no.403170  loss = 2.81833 avg_loss = 3.42945\n",
      "epoch no.4 train no.403180  loss = 2.17128 avg_loss = 3.41473\n",
      "epoch no.4 train no.403190  loss = 1.58375 avg_loss = 3.42647\n",
      "epoch no.4 train no.403200  loss = 4.12296 avg_loss = 3.44775\n",
      "epoch no.4 train no.403210  loss = 3.85000 avg_loss = 3.47187\n",
      "epoch no.4 train no.403220  loss = 2.13388 avg_loss = 3.44491\n",
      "epoch no.4 train no.403230  loss = 4.06200 avg_loss = 3.43575\n",
      "epoch no.4 train no.403240  loss = 2.22922 avg_loss = 3.37653\n",
      "epoch no.4 train no.403250  loss = 5.18430 avg_loss = 3.33839\n",
      "epoch no.4 train no.403260  loss = 2.85432 avg_loss = 3.29963\n",
      "epoch no.4 train no.403270  loss = 3.25927 avg_loss = 3.29100\n",
      "epoch no.4 train no.403280  loss = 2.04783 avg_loss = 3.30814\n",
      "epoch no.4 train no.403290  loss = 2.96323 avg_loss = 3.32903\n",
      "epoch no.4 train no.403300  loss = 2.81367 avg_loss = 3.30161\n",
      "epoch no.4 train no.403310  loss = 2.86997 avg_loss = 3.27568\n",
      "epoch no.4 train no.403320  loss = 1.60079 avg_loss = 3.25728\n",
      "epoch no.4 train no.403330  loss = 2.54758 avg_loss = 3.24972\n",
      "epoch no.4 train no.403340  loss = 6.22257 avg_loss = 3.29747\n",
      "epoch no.4 train no.403350  loss = 3.16261 avg_loss = 3.33677\n",
      "epoch no.4 train no.403360  loss = 1.89685 avg_loss = 3.29018\n",
      "epoch no.4 train no.403370  loss = 3.54285 avg_loss = 3.27269\n",
      "epoch no.4 train no.403380  loss = 5.47032 avg_loss = 3.35335\n",
      "epoch no.4 train no.403390  loss = 3.01248 avg_loss = 3.41052\n",
      "epoch no.4 train no.403400  loss = 1.61415 avg_loss = 3.37927\n",
      "epoch no.4 train no.403410  loss = 4.43218 avg_loss = 3.41275\n",
      "epoch no.4 train no.403420  loss = 2.03663 avg_loss = 3.35602\n",
      "epoch no.4 train no.403430  loss = 3.43026 avg_loss = 3.33621\n",
      "epoch no.4 train no.403440  loss = 4.04628 avg_loss = 3.36624\n",
      "epoch no.4 train no.403450  loss = 4.22176 avg_loss = 3.37721\n",
      "epoch no.4 train no.403460  loss = 4.49489 avg_loss = 3.38883\n",
      "epoch no.4 train no.403470  loss = 2.65233 avg_loss = 3.39697\n",
      "epoch no.4 train no.403480  loss = 2.75941 avg_loss = 3.37294\n",
      "epoch no.4 train no.403490  loss = 2.22118 avg_loss = 3.34890\n",
      "epoch no.4 train no.403500  loss = 3.79086 avg_loss = 3.38332\n",
      "epoch no.4 train no.403510  loss = 3.68105 avg_loss = 3.35783\n",
      "epoch no.4 train no.403520  loss = 2.63767 avg_loss = 3.32548\n",
      "epoch no.4 train no.403530  loss = 3.22330 avg_loss = 3.32199\n",
      "epoch no.4 train no.403540  loss = 3.07060 avg_loss = 3.28646\n",
      "epoch no.4 train no.403550  loss = 2.11502 avg_loss = 3.26518\n",
      "epoch no.4 train no.403560  loss = 2.59565 avg_loss = 3.28114\n",
      "epoch no.4 train no.403570  loss = 2.75404 avg_loss = 3.28836\n",
      "epoch no.4 train no.403580  loss = 4.75134 avg_loss = 3.29861\n",
      "epoch no.4 train no.403590  loss = 4.40106 avg_loss = 3.31591\n",
      "epoch no.4 train no.403600  loss = 4.98990 avg_loss = 3.32608\n",
      "epoch no.4 train no.403610  loss = 2.43565 avg_loss = 3.29652\n",
      "epoch no.4 train no.403620  loss = 2.06873 avg_loss = 3.27276\n",
      "epoch no.4 train no.403630  loss = 4.32245 avg_loss = 3.27154\n",
      "epoch no.4 train no.403640  loss = 4.12617 avg_loss = 3.27627\n",
      "epoch no.4 train no.403650  loss = 3.55912 avg_loss = 3.31012\n",
      "epoch no.4 train no.403660  loss = 5.32142 avg_loss = 3.36335\n",
      "epoch no.4 train no.403670  loss = 2.32448 avg_loss = 3.33494\n",
      "epoch no.4 train no.403680  loss = 0.99686 avg_loss = 3.34140\n",
      "epoch no.4 train no.403690  loss = 2.26026 avg_loss = 3.33102\n",
      "epoch no.4 train no.403700  loss = 3.24671 avg_loss = 3.29891\n",
      "epoch no.4 train no.403710  loss = 1.10705 avg_loss = 3.28266\n",
      "epoch no.4 train no.403720  loss = 4.01308 avg_loss = 3.28126\n",
      "epoch no.4 train no.403730  loss = 3.70575 avg_loss = 3.29509\n",
      "epoch no.4 train no.403740  loss = 3.46278 avg_loss = 3.31480\n",
      "epoch no.4 train no.403750  loss = 3.17725 avg_loss = 3.31979\n",
      "epoch no.4 train no.403760  loss = 4.05074 avg_loss = 3.36240\n",
      "epoch no.4 train no.403770  loss = 4.56985 avg_loss = 3.39937\n",
      "epoch no.4 train no.403780  loss = 3.23809 avg_loss = 3.39573\n",
      "epoch no.4 train no.403790  loss = 3.67899 avg_loss = 3.43863\n",
      "epoch no.4 train no.403800  loss = 3.51791 avg_loss = 3.43930\n",
      "epoch no.4 train no.403810  loss = 4.03545 avg_loss = 3.43205\n",
      "epoch no.4 train no.403820  loss = 5.23442 avg_loss = 3.40047\n",
      "epoch no.4 train no.403830  loss = 2.91196 avg_loss = 3.41113\n",
      "epoch no.4 train no.403840  loss = 2.16144 avg_loss = 3.42884\n",
      "epoch no.4 train no.403850  loss = 3.04398 avg_loss = 3.48395\n",
      "epoch no.4 train no.403860  loss = 2.59118 avg_loss = 3.41238\n",
      "epoch no.4 train no.403870  loss = 2.75581 avg_loss = 3.39042\n",
      "epoch no.4 train no.403880  loss = 1.86859 avg_loss = 3.36971\n",
      "epoch no.4 train no.403890  loss = 4.37548 avg_loss = 3.36410\n",
      "epoch no.4 train no.403900  loss = 3.29390 avg_loss = 3.37585\n",
      "epoch no.4 train no.403910  loss = 5.14031 avg_loss = 3.38887\n",
      "epoch no.4 train no.403920  loss = 3.86548 avg_loss = 3.38320\n",
      "epoch no.4 train no.403930  loss = 4.31900 avg_loss = 3.40599\n",
      "epoch no.4 train no.403940  loss = 4.18978 avg_loss = 3.44790\n",
      "epoch no.4 train no.403950  loss = 4.35915 avg_loss = 3.46800\n",
      "epoch no.4 train no.403960  loss = 4.66386 avg_loss = 3.48471\n",
      "epoch no.4 train no.403970  loss = 4.31172 avg_loss = 3.50687\n",
      "epoch no.4 train no.403980  loss = 3.12372 avg_loss = 3.50633\n",
      "epoch no.4 train no.403990  loss = 3.80712 avg_loss = 3.56427\n",
      "epoch no.4 train no.404000  loss = 3.05880 avg_loss = 3.54716\n",
      "5\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁좋은', '▁노래', '▁노래', '송', '</s>']\n",
      "기분전환하기 좋은 신나는 팝송</s>\n",
      "epoch no.4 train no.404010  loss = 4.56031 avg_loss = 3.55860\n",
      "epoch no.4 train no.404020  loss = 3.79840 avg_loss = 3.52570\n",
      "epoch no.4 train no.404030  loss = 3.85415 avg_loss = 3.53937\n",
      "epoch no.4 train no.404040  loss = 3.02616 avg_loss = 3.49744\n",
      "epoch no.4 train no.404050  loss = 3.00842 avg_loss = 3.47309\n",
      "epoch no.4 train no.404060  loss = 3.05095 avg_loss = 3.43486\n",
      "epoch no.4 train no.404070  loss = 3.16090 avg_loss = 3.38043\n",
      "epoch no.4 train no.404080  loss = 3.01230 avg_loss = 3.38575\n",
      "epoch no.4 train no.404090  loss = 3.64067 avg_loss = 3.38994\n",
      "epoch no.4 train no.404100  loss = 3.12168 avg_loss = 3.38450\n",
      "epoch no.4 train no.404110  loss = 4.43068 avg_loss = 3.39989\n",
      "epoch no.4 train no.404120  loss = 1.34454 avg_loss = 3.34635\n",
      "epoch no.4 train no.404130  loss = 3.43855 avg_loss = 3.33840\n",
      "epoch no.4 train no.404140  loss = 3.07515 avg_loss = 3.32412\n",
      "epoch no.4 train no.404150  loss = 2.59324 avg_loss = 3.26603\n",
      "epoch no.4 train no.404160  loss = 3.76765 avg_loss = 3.29564\n",
      "epoch no.4 train no.404170  loss = 2.75933 avg_loss = 3.28704\n",
      "epoch no.4 train no.404180  loss = 2.37726 avg_loss = 3.28331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.404190  loss = 3.90795 avg_loss = 3.26471\n",
      "epoch no.4 train no.404200  loss = 2.78599 avg_loss = 3.25861\n",
      "epoch no.4 train no.404210  loss = 3.14478 avg_loss = 3.22672\n",
      "epoch no.4 train no.404220  loss = 1.95983 avg_loss = 3.19514\n",
      "epoch no.4 train no.404230  loss = 3.29581 avg_loss = 3.21092\n",
      "epoch no.4 train no.404240  loss = 3.40114 avg_loss = 3.24256\n",
      "epoch no.4 train no.404250  loss = 3.04997 avg_loss = 3.22502\n",
      "epoch no.4 train no.404260  loss = 2.24200 avg_loss = 3.21685\n",
      "epoch no.4 train no.404270  loss = 3.64459 avg_loss = 3.28356\n",
      "epoch no.4 train no.404280  loss = 3.01456 avg_loss = 3.31038\n",
      "epoch no.4 train no.404290  loss = 3.40181 avg_loss = 3.34759\n",
      "epoch no.4 train no.404300  loss = 3.11430 avg_loss = 3.30291\n",
      "epoch no.4 train no.404310  loss = 4.12806 avg_loss = 3.32034\n",
      "epoch no.4 train no.404320  loss = 3.53709 avg_loss = 3.29192\n",
      "epoch no.4 train no.404330  loss = 3.63855 avg_loss = 3.32134\n",
      "epoch no.4 train no.404340  loss = 3.68150 avg_loss = 3.32723\n",
      "epoch no.4 train no.404350  loss = 2.93296 avg_loss = 3.34441\n",
      "epoch no.4 train no.404360  loss = 4.50499 avg_loss = 3.37083\n",
      "epoch no.4 train no.404370  loss = 2.55036 avg_loss = 3.35360\n",
      "epoch no.4 train no.404380  loss = 3.50700 avg_loss = 3.39893\n",
      "epoch no.4 train no.404390  loss = 4.29829 avg_loss = 3.42402\n",
      "epoch no.4 train no.404400  loss = 3.29560 avg_loss = 3.38822\n",
      "epoch no.4 train no.404410  loss = 3.66542 avg_loss = 3.40443\n",
      "epoch no.4 train no.404420  loss = 3.96369 avg_loss = 3.40619\n",
      "epoch no.4 train no.404430  loss = 2.99303 avg_loss = 3.39600\n",
      "epoch no.4 train no.404440  loss = 2.06456 avg_loss = 3.34349\n",
      "epoch no.4 train no.404450  loss = 2.40960 avg_loss = 3.34522\n",
      "epoch no.4 train no.404460  loss = 5.74581 avg_loss = 3.39445\n",
      "epoch no.4 train no.404470  loss = 4.33678 avg_loss = 3.40659\n",
      "epoch no.4 train no.404480  loss = 3.84905 avg_loss = 3.41208\n",
      "epoch no.4 train no.404490  loss = 3.55764 avg_loss = 3.44376\n",
      "epoch no.4 train no.404500  loss = 2.98989 avg_loss = 3.44445\n",
      "epoch no.4 train no.404510  loss = 3.77742 avg_loss = 3.46416\n",
      "epoch no.4 train no.404520  loss = 2.29576 avg_loss = 3.47469\n",
      "epoch no.4 train no.404530  loss = 4.53767 avg_loss = 3.48317\n",
      "epoch no.4 train no.404540  loss = 4.45944 avg_loss = 3.47150\n",
      "epoch no.4 train no.404550  loss = 3.18546 avg_loss = 3.48533\n",
      "epoch no.4 train no.404560  loss = 2.48457 avg_loss = 3.46301\n",
      "epoch no.4 train no.404570  loss = 3.85337 avg_loss = 3.48490\n",
      "epoch no.4 train no.404580  loss = 2.79666 avg_loss = 3.48098\n",
      "epoch no.4 train no.404590  loss = 2.82182 avg_loss = 3.49825\n",
      "epoch no.4 train no.404600  loss = 4.18064 avg_loss = 3.52316\n",
      "epoch no.4 train no.404610  loss = 4.11431 avg_loss = 3.49864\n",
      "epoch no.4 train no.404620  loss = 2.40570 avg_loss = 3.48074\n",
      "epoch no.4 train no.404630  loss = 2.54218 avg_loss = 3.45120\n",
      "epoch no.4 train no.404640  loss = 3.15397 avg_loss = 3.46906\n",
      "epoch no.4 train no.404650  loss = 4.16179 avg_loss = 3.47295\n",
      "epoch no.4 train no.404660  loss = 3.07157 avg_loss = 3.41397\n",
      "epoch no.4 train no.404670  loss = 2.54505 avg_loss = 3.45648\n",
      "epoch no.4 train no.404680  loss = 5.32078 avg_loss = 3.47450\n",
      "epoch no.4 train no.404690  loss = 3.75720 avg_loss = 3.45032\n",
      "epoch no.4 train no.404700  loss = 3.48250 avg_loss = 3.47451\n",
      "epoch no.4 train no.404710  loss = 3.09601 avg_loss = 3.46418\n",
      "epoch no.4 train no.404720  loss = 3.07235 avg_loss = 3.39823\n",
      "epoch no.4 train no.404730  loss = 2.51889 avg_loss = 3.40548\n",
      "epoch no.4 train no.404740  loss = 3.34698 avg_loss = 3.40121\n",
      "epoch no.4 train no.404750  loss = 2.77999 avg_loss = 3.41118\n",
      "epoch no.4 train no.404760  loss = 2.94518 avg_loss = 3.46747\n",
      "epoch no.4 train no.404770  loss = 2.93618 avg_loss = 3.46028\n",
      "epoch no.4 train no.404780  loss = 2.27632 avg_loss = 3.40339\n",
      "epoch no.4 train no.404790  loss = 2.95154 avg_loss = 3.39092\n",
      "epoch no.4 train no.404800  loss = 2.49476 avg_loss = 3.36169\n",
      "epoch no.4 train no.404810  loss = 2.92122 avg_loss = 3.35873\n",
      "epoch no.4 train no.404820  loss = 3.42765 avg_loss = 3.37437\n",
      "epoch no.4 train no.404830  loss = 6.00918 avg_loss = 3.41992\n",
      "epoch no.4 train no.404840  loss = 4.01988 avg_loss = 3.40308\n",
      "epoch no.4 train no.404850  loss = 2.43838 avg_loss = 3.36963\n",
      "epoch no.4 train no.404860  loss = 2.42022 avg_loss = 3.34733\n",
      "epoch no.4 train no.404870  loss = 2.70820 avg_loss = 3.30819\n",
      "epoch no.4 train no.404880  loss = 4.36167 avg_loss = 3.29269\n",
      "epoch no.4 train no.404890  loss = 3.07926 avg_loss = 3.30319\n",
      "epoch no.4 train no.404900  loss = 2.46413 avg_loss = 3.28497\n",
      "epoch no.4 train no.404910  loss = 3.91500 avg_loss = 3.26305\n",
      "epoch no.4 train no.404920  loss = 3.57930 avg_loss = 3.24952\n",
      "epoch no.4 train no.404930  loss = 5.82257 avg_loss = 3.30880\n",
      "epoch no.4 train no.404940  loss = 3.67269 avg_loss = 3.30546\n",
      "epoch no.4 train no.404950  loss = 3.09283 avg_loss = 3.33673\n",
      "epoch no.4 train no.404960  loss = 3.35777 avg_loss = 3.33459\n",
      "epoch no.4 train no.404970  loss = 1.52432 avg_loss = 3.29162\n",
      "epoch no.4 train no.404980  loss = 2.71917 avg_loss = 3.31580\n",
      "epoch no.4 train no.404990  loss = 2.93003 avg_loss = 3.37198\n",
      "epoch no.4 train no.405000  loss = 2.92009 avg_loss = 3.40573\n",
      "1\n",
      "to_tokens: ['▁라디오', '좋은', '이', '▁좋은']\n",
      "기분전환하기</s>\n",
      "epoch no.4 train no.405010  loss = 1.84011 avg_loss = 3.38896\n",
      "epoch no.4 train no.405020  loss = 1.84660 avg_loss = 3.42110\n",
      "epoch no.4 train no.405030  loss = 1.81581 avg_loss = 3.39703\n",
      "epoch no.4 train no.405040  loss = 4.34163 avg_loss = 3.41764\n",
      "epoch no.4 train no.405050  loss = 3.56605 avg_loss = 3.41596\n",
      "epoch no.4 train no.405060  loss = 3.24215 avg_loss = 3.36751\n",
      "epoch no.4 train no.405070  loss = 3.50618 avg_loss = 3.36779\n",
      "epoch no.4 train no.405080  loss = 3.15368 avg_loss = 3.34903\n",
      "epoch no.4 train no.405090  loss = 3.63295 avg_loss = 3.35605\n",
      "epoch no.4 train no.405100  loss = 3.14485 avg_loss = 3.34675\n",
      "epoch no.4 train no.405110  loss = 3.33167 avg_loss = 3.33982\n",
      "epoch no.4 train no.405120  loss = 3.84520 avg_loss = 3.32993\n",
      "epoch no.4 train no.405130  loss = 5.04315 avg_loss = 3.38632\n",
      "epoch no.4 train no.405140  loss = 2.30240 avg_loss = 3.39733\n",
      "epoch no.4 train no.405150  loss = 3.40603 avg_loss = 3.38223\n",
      "epoch no.4 train no.405160  loss = 3.06961 avg_loss = 3.35409\n",
      "epoch no.4 train no.405170  loss = 5.68927 avg_loss = 3.39987\n",
      "epoch no.4 train no.405180  loss = 3.08693 avg_loss = 3.35312\n",
      "epoch no.4 train no.405190  loss = 3.60872 avg_loss = 3.33634\n",
      "epoch no.4 train no.405200  loss = 2.71010 avg_loss = 3.29435\n",
      "epoch no.4 train no.405210  loss = 3.19598 avg_loss = 3.31173\n",
      "epoch no.4 train no.405220  loss = 3.59146 avg_loss = 3.35245\n",
      "epoch no.4 train no.405230  loss = 3.79185 avg_loss = 3.37191\n",
      "epoch no.4 train no.405240  loss = 5.19001 avg_loss = 3.40797\n",
      "epoch no.4 train no.405250  loss = 3.16893 avg_loss = 3.34685\n",
      "epoch no.4 train no.405260  loss = 4.55283 avg_loss = 3.39306\n",
      "epoch no.4 train no.405270  loss = 4.15841 avg_loss = 3.38726\n",
      "epoch no.4 train no.405280  loss = 5.39211 avg_loss = 3.38680\n",
      "epoch no.4 train no.405290  loss = 2.46976 avg_loss = 3.37316\n",
      "epoch no.4 train no.405300  loss = 4.01193 avg_loss = 3.38811\n",
      "epoch no.4 train no.405310  loss = 3.17625 avg_loss = 3.40575\n",
      "epoch no.4 train no.405320  loss = 2.42445 avg_loss = 3.37315\n",
      "epoch no.4 train no.405330  loss = 2.47529 avg_loss = 3.36657\n",
      "epoch no.4 train no.405340  loss = 3.94461 avg_loss = 3.34224\n",
      "epoch no.4 train no.405350  loss = 3.22138 avg_loss = 3.32347\n",
      "epoch no.4 train no.405360  loss = 3.84799 avg_loss = 3.30145\n",
      "epoch no.4 train no.405370  loss = 3.59434 avg_loss = 3.29524\n",
      "epoch no.4 train no.405380  loss = 2.50573 avg_loss = 3.26812\n",
      "epoch no.4 train no.405390  loss = 3.62801 avg_loss = 3.28116\n",
      "epoch no.4 train no.405400  loss = 3.32768 avg_loss = 3.27002\n",
      "epoch no.4 train no.405410  loss = 2.98671 avg_loss = 3.32534\n",
      "epoch no.4 train no.405420  loss = 3.95352 avg_loss = 3.30644\n",
      "epoch no.4 train no.405430  loss = 3.50903 avg_loss = 3.26810\n",
      "epoch no.4 train no.405440  loss = 5.01947 avg_loss = 3.27046\n",
      "epoch no.4 train no.405450  loss = 4.48264 avg_loss = 3.28908\n",
      "epoch no.4 train no.405460  loss = 3.64918 avg_loss = 3.28364\n",
      "epoch no.4 train no.405470  loss = 4.48245 avg_loss = 3.29774\n",
      "epoch no.4 train no.405480  loss = 3.22728 avg_loss = 3.33245\n",
      "epoch no.4 train no.405490  loss = 3.49963 avg_loss = 3.35433\n",
      "epoch no.4 train no.405500  loss = 4.46172 avg_loss = 3.38729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.405510  loss = 4.48562 avg_loss = 3.38068\n",
      "epoch no.4 train no.405520  loss = 2.55680 avg_loss = 3.38617\n",
      "epoch no.4 train no.405530  loss = 4.85146 avg_loss = 3.40644\n",
      "epoch no.4 train no.405540  loss = 1.61113 avg_loss = 3.33236\n",
      "epoch no.4 train no.405550  loss = 3.15940 avg_loss = 3.31734\n",
      "epoch no.4 train no.405560  loss = 4.28643 avg_loss = 3.34661\n",
      "epoch no.4 train no.405570  loss = 3.66575 avg_loss = 3.37010\n",
      "epoch no.4 train no.405580  loss = 5.01099 avg_loss = 3.39544\n",
      "epoch no.4 train no.405590  loss = 3.44912 avg_loss = 3.37729\n",
      "epoch no.4 train no.405600  loss = 3.89370 avg_loss = 3.38017\n",
      "epoch no.4 train no.405610  loss = 2.62204 avg_loss = 3.35969\n",
      "epoch no.4 train no.405620  loss = 3.93873 avg_loss = 3.35203\n",
      "epoch no.4 train no.405630  loss = 3.77067 avg_loss = 3.37117\n",
      "epoch no.4 train no.405640  loss = 3.84345 avg_loss = 3.35344\n",
      "epoch no.4 train no.405650  loss = 4.37559 avg_loss = 3.35362\n",
      "epoch no.4 train no.405660  loss = 2.05586 avg_loss = 3.35751\n",
      "epoch no.4 train no.405670  loss = 2.84232 avg_loss = 3.38580\n",
      "epoch no.4 train no.405680  loss = 3.20741 avg_loss = 3.36616\n",
      "epoch no.4 train no.405690  loss = 3.78975 avg_loss = 3.39153\n",
      "epoch no.4 train no.405700  loss = 2.11532 avg_loss = 3.34896\n",
      "epoch no.4 train no.405710  loss = 2.59846 avg_loss = 3.32354\n",
      "epoch no.4 train no.405720  loss = 4.24592 avg_loss = 3.35997\n",
      "epoch no.4 train no.405730  loss = 4.21192 avg_loss = 3.36396\n",
      "epoch no.4 train no.405740  loss = 3.28606 avg_loss = 3.36008\n",
      "epoch no.4 train no.405750  loss = 3.11025 avg_loss = 3.33538\n",
      "epoch no.4 train no.405760  loss = 3.93613 avg_loss = 3.38644\n",
      "epoch no.4 train no.405770  loss = 3.22018 avg_loss = 3.37272\n",
      "epoch no.4 train no.405780  loss = 2.67583 avg_loss = 3.40355\n",
      "epoch no.4 train no.405790  loss = 4.76710 avg_loss = 3.37107\n",
      "epoch no.4 train no.405800  loss = 4.39157 avg_loss = 3.41842\n",
      "epoch no.4 train no.405810  loss = 3.89834 avg_loss = 3.41323\n",
      "epoch no.4 train no.405820  loss = 3.16626 avg_loss = 3.43273\n",
      "epoch no.4 train no.405830  loss = 4.62152 avg_loss = 3.42111\n",
      "epoch no.4 train no.405840  loss = 4.77115 avg_loss = 3.44006\n",
      "epoch no.4 train no.405850  loss = 2.57575 avg_loss = 3.41487\n",
      "epoch no.4 train no.405860  loss = 4.07464 avg_loss = 3.42950\n",
      "epoch no.4 train no.405870  loss = 3.83685 avg_loss = 3.47388\n",
      "epoch no.4 train no.405880  loss = 3.92692 avg_loss = 3.49511\n",
      "epoch no.4 train no.405890  loss = 3.69229 avg_loss = 3.50744\n",
      "epoch no.4 train no.405900  loss = 4.67616 avg_loss = 3.51609\n",
      "epoch no.4 train no.405910  loss = 3.64060 avg_loss = 3.48385\n",
      "epoch no.4 train no.405920  loss = 4.58393 avg_loss = 3.48207\n",
      "epoch no.4 train no.405930  loss = 2.94509 avg_loss = 3.47107\n",
      "epoch no.4 train no.405940  loss = 3.97199 avg_loss = 3.44456\n",
      "epoch no.4 train no.405950  loss = 2.76930 avg_loss = 3.43983\n",
      "epoch no.4 train no.405960  loss = 2.77460 avg_loss = 3.38925\n",
      "epoch no.4 train no.405970  loss = 2.68603 avg_loss = 3.37691\n",
      "epoch no.4 train no.405980  loss = 3.54199 avg_loss = 3.39871\n",
      "epoch no.4 train no.405990  loss = 3.03223 avg_loss = 3.43097\n",
      "epoch no.4 train no.406000  loss = 2.49863 avg_loss = 3.39550\n",
      "4\n",
      "to_tokens: ['▁비', '전환', '이', '▁필요할', '요한', '노래', '</s>']\n",
      "기분전환이필요한때</s>\n",
      "epoch no.4 train no.406010  loss = 3.05127 avg_loss = 3.38027\n",
      "epoch no.4 train no.406020  loss = 5.31847 avg_loss = 3.43502\n",
      "epoch no.4 train no.406030  loss = 2.83906 avg_loss = 3.42521\n",
      "epoch no.4 train no.406040  loss = 3.82610 avg_loss = 3.40986\n",
      "epoch no.4 train no.406050  loss = 6.14924 avg_loss = 3.43571\n",
      "epoch no.4 train no.406060  loss = 3.29333 avg_loss = 3.42864\n",
      "epoch no.4 train no.406070  loss = 3.77602 avg_loss = 3.39801\n",
      "epoch no.4 train no.406080  loss = 2.74288 avg_loss = 3.43018\n",
      "epoch no.4 train no.406090  loss = 2.74098 avg_loss = 3.41980\n",
      "epoch no.4 train no.406100  loss = 3.46561 avg_loss = 3.41073\n",
      "epoch no.4 train no.406110  loss = 4.41707 avg_loss = 3.36768\n",
      "epoch no.4 train no.406120  loss = 3.82422 avg_loss = 3.33227\n",
      "epoch no.4 train no.406130  loss = 1.91331 avg_loss = 3.34528\n",
      "epoch no.4 train no.406140  loss = 4.86194 avg_loss = 3.34677\n",
      "epoch no.4 train no.406150  loss = 2.94545 avg_loss = 3.35826\n",
      "epoch no.4 train no.406160  loss = 3.51557 avg_loss = 3.35489\n",
      "epoch no.4 train no.406170  loss = 2.79069 avg_loss = 3.35534\n",
      "epoch no.4 train no.406180  loss = 2.18907 avg_loss = 3.36075\n",
      "epoch no.4 train no.406190  loss = 3.19807 avg_loss = 3.32544\n",
      "epoch no.4 train no.406200  loss = 4.47867 avg_loss = 3.32607\n",
      "epoch no.4 train no.406210  loss = 2.06771 avg_loss = 3.32869\n",
      "epoch no.4 train no.406220  loss = 3.40150 avg_loss = 3.33722\n",
      "epoch no.4 train no.406230  loss = 3.44051 avg_loss = 3.29156\n",
      "epoch no.4 train no.406240  loss = 3.40599 avg_loss = 3.30283\n",
      "epoch no.4 train no.406250  loss = 2.63509 avg_loss = 3.36696\n",
      "epoch no.4 train no.406260  loss = 3.87104 avg_loss = 3.36155\n",
      "epoch no.4 train no.406270  loss = 3.53436 avg_loss = 3.35205\n",
      "epoch no.4 train no.406280  loss = 2.46386 avg_loss = 3.36403\n",
      "epoch no.4 train no.406290  loss = 2.12634 avg_loss = 3.35611\n",
      "epoch no.4 train no.406300  loss = 3.87207 avg_loss = 3.32296\n",
      "epoch no.4 train no.406310  loss = 2.61676 avg_loss = 3.35241\n",
      "epoch no.4 train no.406320  loss = 4.07488 avg_loss = 3.37956\n",
      "epoch no.4 train no.406330  loss = 3.72504 avg_loss = 3.38632\n",
      "epoch no.4 train no.406340  loss = 4.66852 avg_loss = 3.37597\n",
      "epoch no.4 train no.406350  loss = 5.44598 avg_loss = 3.42987\n",
      "epoch no.4 train no.406360  loss = 4.04072 avg_loss = 3.48063\n",
      "epoch no.4 train no.406370  loss = 4.16046 avg_loss = 3.43575\n",
      "epoch no.4 train no.406380  loss = 2.79953 avg_loss = 3.44530\n",
      "epoch no.4 train no.406390  loss = 1.71776 avg_loss = 3.40963\n",
      "epoch no.4 train no.406400  loss = 2.61426 avg_loss = 3.36986\n",
      "epoch no.4 train no.406410  loss = 4.75272 avg_loss = 3.41454\n",
      "epoch no.4 train no.406420  loss = 3.41331 avg_loss = 3.38880\n",
      "epoch no.4 train no.406430  loss = 4.58644 avg_loss = 3.40170\n",
      "epoch no.4 train no.406440  loss = 5.09000 avg_loss = 3.44426\n",
      "epoch no.4 train no.406450  loss = 3.35188 avg_loss = 3.43240\n",
      "epoch no.4 train no.406460  loss = 4.14661 avg_loss = 3.50635\n",
      "epoch no.4 train no.406470  loss = 2.74933 avg_loss = 3.48714\n",
      "epoch no.4 train no.406480  loss = 3.75916 avg_loss = 3.48418\n",
      "epoch no.4 train no.406490  loss = 3.39159 avg_loss = 3.44964\n",
      "epoch no.4 train no.406500  loss = 2.94366 avg_loss = 3.41753\n",
      "epoch no.4 train no.406510  loss = 5.38820 avg_loss = 3.42355\n",
      "epoch no.4 train no.406520  loss = 3.21476 avg_loss = 3.39687\n",
      "epoch no.4 train no.406530  loss = 3.37463 avg_loss = 3.39503\n",
      "epoch no.4 train no.406540  loss = 3.95044 avg_loss = 3.43388\n",
      "epoch no.4 train no.406550  loss = 3.97965 avg_loss = 3.38636\n",
      "epoch no.4 train no.406560  loss = 6.04333 avg_loss = 3.41059\n",
      "epoch no.4 train no.406570  loss = 4.58422 avg_loss = 3.46494\n",
      "epoch no.4 train no.406580  loss = 3.33076 avg_loss = 3.45851\n",
      "epoch no.4 train no.406590  loss = 2.34319 avg_loss = 3.40339\n",
      "epoch no.4 train no.406600  loss = 2.49644 avg_loss = 3.33431\n",
      "epoch no.4 train no.406610  loss = 3.50907 avg_loss = 3.42652\n",
      "epoch no.4 train no.406620  loss = 3.62855 avg_loss = 3.42469\n",
      "epoch no.4 train no.406630  loss = 2.25930 avg_loss = 3.38059\n",
      "epoch no.4 train no.406640  loss = 3.69689 avg_loss = 3.37764\n",
      "epoch no.4 train no.406650  loss = 2.35933 avg_loss = 3.37967\n",
      "epoch no.4 train no.406660  loss = 3.78461 avg_loss = 3.36883\n",
      "epoch no.4 train no.406670  loss = 2.62086 avg_loss = 3.34417\n",
      "epoch no.4 train no.406680  loss = 3.40309 avg_loss = 3.33863\n",
      "epoch no.4 train no.406690  loss = 3.41314 avg_loss = 3.34876\n",
      "epoch no.4 train no.406700  loss = 2.40227 avg_loss = 3.32632\n",
      "epoch no.4 train no.406710  loss = 4.28154 avg_loss = 3.33536\n",
      "epoch no.4 train no.406720  loss = 2.41720 avg_loss = 3.33535\n",
      "epoch no.4 train no.406730  loss = 2.74343 avg_loss = 3.32910\n",
      "epoch no.4 train no.406740  loss = 3.15443 avg_loss = 3.30308\n",
      "epoch no.4 train no.406750  loss = 2.22770 avg_loss = 3.32311\n",
      "epoch no.4 train no.406760  loss = 2.94597 avg_loss = 3.33698\n",
      "epoch no.4 train no.406770  loss = 4.55351 avg_loss = 3.33327\n",
      "epoch no.4 train no.406780  loss = 3.15678 avg_loss = 3.35621\n",
      "epoch no.4 train no.406790  loss = 5.32324 avg_loss = 3.38692\n",
      "epoch no.4 train no.406800  loss = 3.11204 avg_loss = 3.41471\n",
      "epoch no.4 train no.406810  loss = 3.89173 avg_loss = 3.41171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.406820  loss = 3.89231 avg_loss = 3.44564\n",
      "epoch no.4 train no.406830  loss = 2.74178 avg_loss = 3.44016\n",
      "epoch no.4 train no.406840  loss = 4.20882 avg_loss = 3.44641\n",
      "epoch no.4 train no.406850  loss = 3.70107 avg_loss = 3.44618\n",
      "epoch no.4 train no.406860  loss = 4.03469 avg_loss = 3.47136\n",
      "epoch no.4 train no.406870  loss = 3.35183 avg_loss = 3.45999\n",
      "epoch no.4 train no.406880  loss = 4.17854 avg_loss = 3.44305\n",
      "epoch no.4 train no.406890  loss = 2.94223 avg_loss = 3.45542\n",
      "epoch no.4 train no.406900  loss = 1.42358 avg_loss = 3.39265\n",
      "epoch no.4 train no.406910  loss = 3.43677 avg_loss = 3.45278\n",
      "epoch no.4 train no.406920  loss = 3.59432 avg_loss = 3.48497\n",
      "epoch no.4 train no.406930  loss = 4.22727 avg_loss = 3.48481\n",
      "epoch no.4 train no.406940  loss = 3.60272 avg_loss = 3.49695\n",
      "epoch no.4 train no.406950  loss = 4.12221 avg_loss = 3.50874\n",
      "epoch no.4 train no.406960  loss = 3.17055 avg_loss = 3.54754\n",
      "epoch no.4 train no.406970  loss = 2.72210 avg_loss = 3.46956\n",
      "epoch no.4 train no.406980  loss = 2.27138 avg_loss = 3.48470\n",
      "epoch no.4 train no.406990  loss = 6.41996 avg_loss = 3.55244\n",
      "epoch no.4 train no.407000  loss = 3.90884 avg_loss = 3.56119\n",
      "6\n",
      "to_tokens: ['▁라디오', '전환', '▁하고', '싶', '을', '때', '</s>', '▁신나는', '</s>']\n",
      "기분전환 하고싶을때 듣는노래</s>\n",
      "epoch no.4 train no.407010  loss = 3.13476 avg_loss = 3.55461\n",
      "epoch no.4 train no.407020  loss = 3.95644 avg_loss = 3.55612\n",
      "epoch no.4 train no.407030  loss = 2.96085 avg_loss = 3.51168\n",
      "epoch no.4 train no.407040  loss = 2.66882 avg_loss = 3.55508\n",
      "epoch no.4 train no.407050  loss = 4.78451 avg_loss = 3.54551\n",
      "epoch no.4 train no.407060  loss = 2.94305 avg_loss = 3.52098\n",
      "epoch no.4 train no.407070  loss = 3.37036 avg_loss = 3.52587\n",
      "epoch no.4 train no.407080  loss = 1.78195 avg_loss = 3.49851\n",
      "epoch no.4 train no.407090  loss = 2.66005 avg_loss = 3.44728\n",
      "epoch no.4 train no.407100  loss = 2.99824 avg_loss = 3.45479\n",
      "epoch no.4 train no.407110  loss = 3.02806 avg_loss = 3.45415\n",
      "epoch no.4 train no.407120  loss = 2.65496 avg_loss = 3.38016\n",
      "epoch no.4 train no.407130  loss = 2.66070 avg_loss = 3.39797\n",
      "epoch no.4 train no.407140  loss = 3.23557 avg_loss = 3.38042\n",
      "epoch no.4 train no.407150  loss = 4.30680 avg_loss = 3.40385\n",
      "epoch no.4 train no.407160  loss = 3.10819 avg_loss = 3.36751\n",
      "epoch no.4 train no.407170  loss = 3.22585 avg_loss = 3.38997\n",
      "epoch no.4 train no.407180  loss = 4.32195 avg_loss = 3.38795\n",
      "epoch no.4 train no.407190  loss = 3.04230 avg_loss = 3.37762\n",
      "epoch no.4 train no.407200  loss = 2.57290 avg_loss = 3.33800\n",
      "epoch no.4 train no.407210  loss = 3.03114 avg_loss = 3.35170\n",
      "epoch no.4 train no.407220  loss = 2.61035 avg_loss = 3.30157\n",
      "epoch no.4 train no.407230  loss = 2.82086 avg_loss = 3.26734\n",
      "epoch no.4 train no.407240  loss = 2.65168 avg_loss = 3.24890\n",
      "epoch no.4 train no.407250  loss = 4.73792 avg_loss = 3.26894\n",
      "epoch no.4 train no.407260  loss = 1.91771 avg_loss = 3.25824\n",
      "epoch no.4 train no.407270  loss = 4.25570 avg_loss = 3.27721\n",
      "epoch no.4 train no.407280  loss = 5.06766 avg_loss = 3.27659\n",
      "epoch no.4 train no.407290  loss = 4.56020 avg_loss = 3.30618\n",
      "epoch no.4 train no.407300  loss = 3.19753 avg_loss = 3.26947\n",
      "epoch no.4 train no.407310  loss = 5.07523 avg_loss = 3.28467\n",
      "epoch no.4 train no.407320  loss = 3.53484 avg_loss = 3.25690\n",
      "epoch no.4 train no.407330  loss = 4.27125 avg_loss = 3.26165\n",
      "epoch no.4 train no.407340  loss = 2.80773 avg_loss = 3.30117\n",
      "epoch no.4 train no.407350  loss = 4.53472 avg_loss = 3.30308\n",
      "epoch no.4 train no.407360  loss = 4.44107 avg_loss = 3.38268\n",
      "epoch no.4 train no.407370  loss = 4.53532 avg_loss = 3.39179\n",
      "epoch no.4 train no.407380  loss = 2.03560 avg_loss = 3.37426\n",
      "epoch no.4 train no.407390  loss = 3.98309 avg_loss = 3.35036\n",
      "epoch no.4 train no.407400  loss = 2.91635 avg_loss = 3.40535\n",
      "epoch no.4 train no.407410  loss = 2.16833 avg_loss = 3.35779\n",
      "epoch no.4 train no.407420  loss = 2.56735 avg_loss = 3.33588\n",
      "epoch no.4 train no.407430  loss = 3.35913 avg_loss = 3.33391\n",
      "epoch no.4 train no.407440  loss = 5.46245 avg_loss = 3.37289\n",
      "epoch no.4 train no.407450  loss = 3.32872 avg_loss = 3.36176\n",
      "epoch no.4 train no.407460  loss = 2.37108 avg_loss = 3.35187\n",
      "epoch no.4 train no.407470  loss = 4.21407 avg_loss = 3.35234\n",
      "epoch no.4 train no.407480  loss = 3.92594 avg_loss = 3.36890\n",
      "epoch no.4 train no.407490  loss = 3.07200 avg_loss = 3.36332\n",
      "epoch no.4 train no.407500  loss = 3.51196 avg_loss = 3.36265\n",
      "epoch no.4 train no.407510  loss = 2.57203 avg_loss = 3.34173\n",
      "epoch no.4 train no.407520  loss = 3.88611 avg_loss = 3.37092\n",
      "epoch no.4 train no.407530  loss = 3.47216 avg_loss = 3.37030\n",
      "epoch no.4 train no.407540  loss = 2.92736 avg_loss = 3.35988\n",
      "epoch no.4 train no.407550  loss = 3.52755 avg_loss = 3.33421\n",
      "epoch no.4 train no.407560  loss = 3.00292 avg_loss = 3.34226\n",
      "epoch no.4 train no.407570  loss = 3.69378 avg_loss = 3.34124\n",
      "epoch no.4 train no.407580  loss = 2.67023 avg_loss = 3.38796\n",
      "epoch no.4 train no.407590  loss = 3.66384 avg_loss = 3.34914\n",
      "epoch no.4 train no.407600  loss = 1.87474 avg_loss = 3.30244\n",
      "epoch no.4 train no.407610  loss = 4.40072 avg_loss = 3.32318\n",
      "epoch no.4 train no.407620  loss = 1.56961 avg_loss = 3.34209\n",
      "epoch no.4 train no.407630  loss = 4.63243 avg_loss = 3.36933\n",
      "epoch no.4 train no.407640  loss = 1.99692 avg_loss = 3.33914\n",
      "epoch no.4 train no.407650  loss = 2.37425 avg_loss = 3.38759\n",
      "epoch no.4 train no.407660  loss = 2.27479 avg_loss = 3.35610\n",
      "epoch no.4 train no.407670  loss = 3.15677 avg_loss = 3.33850\n",
      "epoch no.4 train no.407680  loss = 2.52736 avg_loss = 3.32714\n",
      "epoch no.4 train no.407690  loss = 2.67724 avg_loss = 3.33590\n",
      "epoch no.4 train no.407700  loss = 2.56834 avg_loss = 3.30156\n",
      "epoch no.4 train no.407710  loss = 3.01679 avg_loss = 3.28656\n",
      "epoch no.4 train no.407720  loss = 3.34792 avg_loss = 3.31697\n",
      "epoch no.4 train no.407730  loss = 2.22789 avg_loss = 3.29759\n",
      "epoch no.4 train no.407740  loss = 2.82452 avg_loss = 3.29435\n",
      "epoch no.4 train no.407750  loss = 5.27448 avg_loss = 3.32429\n",
      "epoch no.4 train no.407760  loss = 1.60353 avg_loss = 3.33184\n",
      "epoch no.4 train no.407770  loss = 2.23394 avg_loss = 3.32766\n",
      "epoch no.4 train no.407780  loss = 2.24300 avg_loss = 3.33099\n",
      "epoch no.4 train no.407790  loss = 3.85673 avg_loss = 3.35890\n",
      "epoch no.4 train no.407800  loss = 6.45844 avg_loss = 3.37680\n",
      "epoch no.4 train no.407810  loss = 5.24921 avg_loss = 3.37791\n",
      "epoch no.4 train no.407820  loss = 3.04211 avg_loss = 3.33986\n",
      "epoch no.4 train no.407830  loss = 2.40161 avg_loss = 3.32011\n",
      "epoch no.4 train no.407840  loss = 2.97844 avg_loss = 3.32421\n",
      "epoch no.4 train no.407850  loss = 3.91884 avg_loss = 3.32429\n",
      "epoch no.4 train no.407860  loss = 3.27744 avg_loss = 3.28693\n",
      "epoch no.4 train no.407870  loss = 2.77340 avg_loss = 3.26945\n",
      "epoch no.4 train no.407880  loss = 5.31009 avg_loss = 3.32065\n",
      "epoch no.4 train no.407890  loss = 3.29471 avg_loss = 3.31183\n",
      "epoch no.4 train no.407900  loss = 2.72544 avg_loss = 3.28454\n",
      "epoch no.4 train no.407910  loss = 3.50611 avg_loss = 3.27269\n",
      "epoch no.4 train no.407920  loss = 2.96554 avg_loss = 3.31037\n",
      "epoch no.4 train no.407930  loss = 5.96915 avg_loss = 3.34487\n",
      "epoch no.4 train no.407940  loss = 5.93090 avg_loss = 3.38733\n",
      "epoch no.4 train no.407950  loss = 2.24612 avg_loss = 3.36017\n",
      "epoch no.4 train no.407960  loss = 3.21990 avg_loss = 3.38253\n",
      "epoch no.4 train no.407970  loss = 3.42983 avg_loss = 3.41022\n",
      "epoch no.4 train no.407980  loss = 3.42595 avg_loss = 3.44377\n",
      "epoch no.4 train no.407990  loss = 3.27107 avg_loss = 3.42645\n",
      "epoch no.4 train no.408000  loss = 3.43256 avg_loss = 3.40821\n",
      "1\n",
      "to_tokens: ['▁라디오', '전환', '에', '</s>']\n",
      "기분전환 드라이브</s>\n",
      "epoch no.4 train no.408010  loss = 4.52759 avg_loss = 3.40424\n",
      "epoch no.4 train no.408020  loss = 2.91862 avg_loss = 3.37274\n",
      "epoch no.4 train no.408030  loss = 2.88159 avg_loss = 3.35165\n",
      "epoch no.4 train no.408040  loss = 4.65915 avg_loss = 3.32605\n",
      "epoch no.4 train no.408050  loss = 2.98022 avg_loss = 3.31235\n",
      "epoch no.4 train no.408060  loss = 2.99798 avg_loss = 3.33248\n",
      "epoch no.4 train no.408070  loss = 2.76480 avg_loss = 3.31219\n",
      "epoch no.4 train no.408080  loss = 1.76466 avg_loss = 3.27674\n",
      "epoch no.4 train no.408090  loss = 3.72600 avg_loss = 3.26255\n",
      "epoch no.4 train no.408100  loss = 3.49866 avg_loss = 3.24562\n",
      "epoch no.4 train no.408110  loss = 3.91562 avg_loss = 3.29529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.408120  loss = 2.26732 avg_loss = 3.35419\n",
      "epoch no.4 train no.408130  loss = 2.48877 avg_loss = 3.33210\n",
      "epoch no.4 train no.408140  loss = 3.25433 avg_loss = 3.31372\n",
      "epoch no.4 train no.408150  loss = 3.91861 avg_loss = 3.32572\n",
      "epoch no.4 train no.408160  loss = 2.94809 avg_loss = 3.28666\n",
      "epoch no.4 train no.408170  loss = 3.96392 avg_loss = 3.25167\n",
      "epoch no.4 train no.408180  loss = 3.16897 avg_loss = 3.27358\n",
      "epoch no.4 train no.408190  loss = 4.23407 avg_loss = 3.25466\n",
      "epoch no.4 train no.408200  loss = 4.16668 avg_loss = 3.26398\n",
      "epoch no.4 train no.408210  loss = 2.31811 avg_loss = 3.26365\n",
      "epoch no.4 train no.408220  loss = 2.88178 avg_loss = 3.27722\n",
      "epoch no.4 train no.408230  loss = 4.25844 avg_loss = 3.27964\n",
      "epoch no.4 train no.408240  loss = 2.66203 avg_loss = 3.22972\n",
      "epoch no.4 train no.408250  loss = 2.37622 avg_loss = 3.25406\n",
      "epoch no.4 train no.408260  loss = 4.94276 avg_loss = 3.31294\n",
      "epoch no.4 train no.408270  loss = 1.84297 avg_loss = 3.27547\n",
      "epoch no.4 train no.408280  loss = 2.61610 avg_loss = 3.26678\n",
      "epoch no.4 train no.408290  loss = 2.67838 avg_loss = 3.25676\n",
      "epoch no.4 train no.408300  loss = 2.51667 avg_loss = 3.26158\n",
      "epoch no.4 train no.408310  loss = 3.77390 avg_loss = 3.29446\n",
      "epoch no.4 train no.408320  loss = 2.08268 avg_loss = 3.27922\n",
      "epoch no.4 train no.408330  loss = 4.27045 avg_loss = 3.30352\n",
      "epoch no.4 train no.408340  loss = 4.12970 avg_loss = 3.29723\n",
      "epoch no.4 train no.408350  loss = 3.45998 avg_loss = 3.27816\n",
      "epoch no.4 train no.408360  loss = 4.82663 avg_loss = 3.26802\n",
      "epoch no.4 train no.408370  loss = 4.43091 avg_loss = 3.29176\n",
      "epoch no.4 train no.408380  loss = 2.47657 avg_loss = 3.34502\n",
      "epoch no.4 train no.408390  loss = 2.08777 avg_loss = 3.31397\n",
      "epoch no.4 train no.408400  loss = 3.80478 avg_loss = 3.33767\n",
      "epoch no.4 train no.408410  loss = 2.83283 avg_loss = 3.38067\n",
      "epoch no.4 train no.408420  loss = 2.59866 avg_loss = 3.34927\n",
      "epoch no.4 train no.408430  loss = 1.41282 avg_loss = 3.32090\n",
      "epoch no.4 train no.408440  loss = 3.54721 avg_loss = 3.30939\n",
      "epoch no.4 train no.408450  loss = 2.72908 avg_loss = 3.29096\n",
      "epoch no.4 train no.408460  loss = 3.06335 avg_loss = 3.30137\n",
      "epoch no.4 train no.408470  loss = 3.05996 avg_loss = 3.32645\n",
      "epoch no.4 train no.408480  loss = 3.75432 avg_loss = 3.31903\n",
      "epoch no.4 train no.408490  loss = 3.27467 avg_loss = 3.27020\n",
      "epoch no.4 train no.408500  loss = 4.42143 avg_loss = 3.32005\n",
      "epoch no.4 train no.408510  loss = 3.87228 avg_loss = 3.34608\n",
      "epoch no.4 train no.408520  loss = 3.59669 avg_loss = 3.42852\n",
      "epoch no.4 train no.408530  loss = 3.45128 avg_loss = 3.41276\n",
      "epoch no.4 train no.408540  loss = 3.38280 avg_loss = 3.40321\n",
      "epoch no.4 train no.408550  loss = 3.07282 avg_loss = 3.35981\n",
      "epoch no.4 train no.408560  loss = 2.29805 avg_loss = 3.36520\n",
      "epoch no.4 train no.408570  loss = 3.47761 avg_loss = 3.34378\n",
      "epoch no.4 train no.408580  loss = 3.37201 avg_loss = 3.34964\n",
      "epoch no.4 train no.408590  loss = 3.49195 avg_loss = 3.39208\n",
      "epoch no.4 train no.408600  loss = 2.21542 avg_loss = 3.36659\n",
      "epoch no.4 train no.408610  loss = 3.43541 avg_loss = 3.36780\n",
      "epoch no.4 train no.408620  loss = 2.66677 avg_loss = 3.33215\n",
      "epoch no.4 train no.408630  loss = 3.37335 avg_loss = 3.39474\n",
      "epoch no.4 train no.408640  loss = 5.71740 avg_loss = 3.44220\n",
      "epoch no.4 train no.408650  loss = 3.41921 avg_loss = 3.50844\n",
      "epoch no.4 train no.408660  loss = 2.78504 avg_loss = 3.47527\n",
      "epoch no.4 train no.408670  loss = 5.06013 avg_loss = 3.44940\n",
      "epoch no.4 train no.408680  loss = 4.19999 avg_loss = 3.47009\n",
      "epoch no.4 train no.408690  loss = 2.90392 avg_loss = 3.46037\n",
      "epoch no.4 train no.408700  loss = 2.96040 avg_loss = 3.46399\n",
      "epoch no.4 train no.408710  loss = 3.86557 avg_loss = 3.49971\n",
      "epoch no.4 train no.408720  loss = 3.81091 avg_loss = 3.51210\n",
      "epoch no.4 train no.408730  loss = 4.07555 avg_loss = 3.59952\n",
      "epoch no.4 train no.408740  loss = 3.02200 avg_loss = 3.60603\n",
      "epoch no.4 train no.408750  loss = 2.15753 avg_loss = 3.50732\n",
      "epoch no.4 train no.408760  loss = 2.94987 avg_loss = 3.48815\n",
      "epoch no.4 train no.408770  loss = 3.69238 avg_loss = 3.45481\n",
      "epoch no.4 train no.408780  loss = 3.99327 avg_loss = 3.46008\n",
      "epoch no.4 train no.408790  loss = 3.67526 avg_loss = 3.48024\n",
      "epoch no.4 train no.408800  loss = 5.04415 avg_loss = 3.51934\n",
      "epoch no.4 train no.408810  loss = 2.58811 avg_loss = 3.48680\n",
      "epoch no.4 train no.408820  loss = 3.32366 avg_loss = 3.44653\n",
      "epoch no.4 train no.408830  loss = 2.63351 avg_loss = 3.47254\n",
      "epoch no.4 train no.408840  loss = 2.71190 avg_loss = 3.44377\n",
      "epoch no.4 train no.408850  loss = 3.21373 avg_loss = 3.44452\n",
      "epoch no.4 train no.408860  loss = 2.12273 avg_loss = 3.44845\n",
      "epoch no.4 train no.408870  loss = 3.20796 avg_loss = 3.42556\n",
      "epoch no.4 train no.408880  loss = 4.12092 avg_loss = 3.44707\n",
      "epoch no.4 train no.408890  loss = 2.93151 avg_loss = 3.45399\n",
      "epoch no.4 train no.408900  loss = 1.79049 avg_loss = 3.39392\n",
      "epoch no.4 train no.408910  loss = 1.81511 avg_loss = 3.41580\n",
      "epoch no.4 train no.408920  loss = 2.59505 avg_loss = 3.37383\n",
      "epoch no.4 train no.408930  loss = 3.71008 avg_loss = 3.45303\n",
      "epoch no.4 train no.408940  loss = 3.00825 avg_loss = 3.41585\n",
      "epoch no.4 train no.408950  loss = 3.86448 avg_loss = 3.41782\n",
      "epoch no.4 train no.408960  loss = 2.87796 avg_loss = 3.40507\n",
      "epoch no.4 train no.408970  loss = 3.00899 avg_loss = 3.40390\n",
      "epoch no.4 train no.408980  loss = 2.93135 avg_loss = 3.41518\n",
      "epoch no.4 train no.408990  loss = 3.86708 avg_loss = 3.41554\n",
      "epoch no.4 train no.409000  loss = 4.67058 avg_loss = 3.41964\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '이', '▁드라이브', '</s>']\n",
      "기분전환 신나는 음악</s>\n",
      "epoch no.4 train no.409010  loss = 3.11908 avg_loss = 3.40407\n",
      "epoch no.4 train no.409020  loss = 3.68186 avg_loss = 3.36087\n",
      "epoch no.4 train no.409030  loss = 6.05024 avg_loss = 3.41185\n",
      "epoch no.4 train no.409040  loss = 4.38130 avg_loss = 3.40106\n",
      "epoch no.4 train no.409050  loss = 1.94991 avg_loss = 3.43515\n",
      "epoch no.4 train no.409060  loss = 3.13986 avg_loss = 3.41157\n",
      "epoch no.4 train no.409070  loss = 2.57454 avg_loss = 3.43169\n",
      "epoch no.4 train no.409080  loss = 2.85884 avg_loss = 3.42951\n",
      "epoch no.4 train no.409090  loss = 4.30034 avg_loss = 3.39204\n",
      "epoch no.4 train no.409100  loss = 3.10382 avg_loss = 3.40999\n",
      "epoch no.4 train no.409110  loss = 3.00645 avg_loss = 3.41918\n",
      "epoch no.4 train no.409120  loss = 2.99221 avg_loss = 3.41710\n",
      "epoch no.4 train no.409130  loss = 3.47157 avg_loss = 3.41893\n",
      "epoch no.4 train no.409140  loss = 4.54938 avg_loss = 3.43432\n",
      "epoch no.4 train no.409150  loss = 3.46439 avg_loss = 3.43477\n",
      "epoch no.4 train no.409160  loss = 4.20527 avg_loss = 3.42859\n",
      "epoch no.4 train no.409170  loss = 2.68363 avg_loss = 3.40163\n",
      "epoch no.4 train no.409180  loss = 5.57885 avg_loss = 3.41137\n",
      "epoch no.4 train no.409190  loss = 2.13024 avg_loss = 3.39190\n",
      "epoch no.4 train no.409200  loss = 1.82872 avg_loss = 3.39590\n",
      "epoch no.4 train no.409210  loss = 3.37700 avg_loss = 3.38939\n",
      "epoch no.4 train no.409220  loss = 3.95776 avg_loss = 3.39587\n",
      "epoch no.4 train no.409230  loss = 5.09764 avg_loss = 3.40769\n",
      "epoch no.4 train no.409240  loss = 3.08107 avg_loss = 3.40990\n",
      "epoch no.4 train no.409250  loss = 2.99236 avg_loss = 3.39719\n",
      "epoch no.4 train no.409260  loss = 4.67622 avg_loss = 3.43994\n",
      "epoch no.4 train no.409270  loss = 3.44815 avg_loss = 3.42488\n",
      "epoch no.4 train no.409280  loss = 2.75904 avg_loss = 3.42665\n",
      "epoch no.4 train no.409290  loss = 3.02497 avg_loss = 3.43874\n",
      "epoch no.4 train no.409300  loss = 2.47442 avg_loss = 3.43803\n",
      "epoch no.4 train no.409310  loss = 4.54616 avg_loss = 3.45261\n",
      "epoch no.4 train no.409320  loss = 2.93098 avg_loss = 3.41618\n",
      "epoch no.4 train no.409330  loss = 4.43885 avg_loss = 3.43024\n",
      "epoch no.4 train no.409340  loss = 2.29730 avg_loss = 3.42693\n",
      "epoch no.4 train no.409350  loss = 3.91966 avg_loss = 3.39114\n",
      "epoch no.4 train no.409360  loss = 2.97462 avg_loss = 3.36030\n",
      "epoch no.4 train no.409370  loss = 2.44479 avg_loss = 3.35800\n",
      "epoch no.4 train no.409380  loss = 4.24833 avg_loss = 3.34633\n",
      "epoch no.4 train no.409390  loss = 3.15425 avg_loss = 3.36396\n",
      "epoch no.4 train no.409400  loss = 2.43709 avg_loss = 3.35816\n",
      "epoch no.4 train no.409410  loss = 2.91861 avg_loss = 3.36846\n",
      "epoch no.4 train no.409420  loss = 4.32297 avg_loss = 3.37358\n",
      "epoch no.4 train no.409430  loss = 3.11919 avg_loss = 3.35960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.409440  loss = 3.30575 avg_loss = 3.38254\n",
      "epoch no.4 train no.409450  loss = 2.24669 avg_loss = 3.39574\n",
      "epoch no.4 train no.409460  loss = 2.33586 avg_loss = 3.38812\n",
      "epoch no.4 train no.409470  loss = 2.75914 avg_loss = 3.39032\n",
      "epoch no.4 train no.409480  loss = 3.05451 avg_loss = 3.38904\n",
      "epoch no.4 train no.409490  loss = 2.76034 avg_loss = 3.39155\n",
      "epoch no.4 train no.409500  loss = 4.90012 avg_loss = 3.40244\n",
      "epoch no.4 train no.409510  loss = 4.62180 avg_loss = 3.41370\n",
      "epoch no.4 train no.409520  loss = 3.22717 avg_loss = 3.39742\n",
      "epoch no.4 train no.409530  loss = 2.62366 avg_loss = 3.38056\n",
      "epoch no.4 train no.409540  loss = 4.32264 avg_loss = 3.40532\n",
      "epoch no.4 train no.409550  loss = 2.30681 avg_loss = 3.34756\n",
      "epoch no.4 train no.409560  loss = 3.66791 avg_loss = 3.36969\n",
      "epoch no.4 train no.409570  loss = 3.36680 avg_loss = 3.36112\n",
      "epoch no.4 train no.409580  loss = 3.71475 avg_loss = 3.41519\n",
      "epoch no.4 train no.409590  loss = 2.18841 avg_loss = 3.39580\n",
      "epoch no.4 train no.409600  loss = 3.92612 avg_loss = 3.40760\n",
      "epoch no.4 train no.409610  loss = 2.95405 avg_loss = 3.41260\n",
      "epoch no.4 train no.409620  loss = 3.77427 avg_loss = 3.41893\n",
      "epoch no.4 train no.409630  loss = 3.32985 avg_loss = 3.42495\n",
      "epoch no.4 train no.409640  loss = 3.87802 avg_loss = 3.44873\n",
      "epoch no.4 train no.409650  loss = 3.92252 avg_loss = 3.51859\n",
      "epoch no.4 train no.409660  loss = 2.82029 avg_loss = 3.48680\n",
      "epoch no.4 train no.409670  loss = 2.55831 avg_loss = 3.47464\n",
      "epoch no.4 train no.409680  loss = 3.56922 avg_loss = 3.50438\n",
      "epoch no.4 train no.409690  loss = 3.49286 avg_loss = 3.47751\n",
      "epoch no.4 train no.409700  loss = 4.73050 avg_loss = 3.49357\n",
      "epoch no.4 train no.409710  loss = 2.80319 avg_loss = 3.49029\n",
      "epoch no.4 train no.409720  loss = 3.59624 avg_loss = 3.46044\n",
      "epoch no.4 train no.409730  loss = 3.13779 avg_loss = 3.42531\n",
      "epoch no.4 train no.409740  loss = 2.97866 avg_loss = 3.39840\n",
      "epoch no.4 train no.409750  loss = 4.13789 avg_loss = 3.41202\n",
      "epoch no.4 train no.409760  loss = 3.13571 avg_loss = 3.39743\n",
      "epoch no.4 train no.409770  loss = 3.80669 avg_loss = 3.43425\n",
      "epoch no.4 train no.409780  loss = 3.28406 avg_loss = 3.39862\n",
      "epoch no.4 train no.409790  loss = 3.66619 avg_loss = 3.43372\n",
      "epoch no.4 train no.409800  loss = 2.66120 avg_loss = 3.41263\n",
      "epoch no.4 train no.409810  loss = 4.01293 avg_loss = 3.42296\n",
      "epoch no.4 train no.409820  loss = 3.52917 avg_loss = 3.43589\n",
      "epoch no.4 train no.409830  loss = 3.03377 avg_loss = 3.44366\n",
      "epoch no.4 train no.409840  loss = 2.19994 avg_loss = 3.44989\n",
      "epoch no.4 train no.409850  loss = 2.61952 avg_loss = 3.47734\n",
      "epoch no.4 train no.409860  loss = 3.54671 avg_loss = 3.49480\n",
      "epoch no.4 train no.409870  loss = 2.02025 avg_loss = 3.47316\n",
      "epoch no.4 train no.409880  loss = 3.12069 avg_loss = 3.49375\n",
      "epoch no.4 train no.409890  loss = 2.20658 avg_loss = 3.51599\n",
      "epoch no.4 train no.409900  loss = 2.94321 avg_loss = 3.50656\n",
      "epoch no.4 train no.409910  loss = 3.16200 avg_loss = 3.45880\n",
      "epoch no.4 train no.409920  loss = 2.58501 avg_loss = 3.45981\n",
      "epoch no.4 train no.409930  loss = 3.95085 avg_loss = 3.46810\n",
      "epoch no.4 train no.409940  loss = 3.49291 avg_loss = 3.42621\n",
      "epoch no.4 train no.409950  loss = 2.72255 avg_loss = 3.42407\n",
      "epoch no.4 train no.409960  loss = 3.63528 avg_loss = 3.38242\n",
      "epoch no.4 train no.409970  loss = 2.10475 avg_loss = 3.38347\n",
      "epoch no.4 train no.409980  loss = 2.52662 avg_loss = 3.37140\n",
      "epoch no.4 train no.409990  loss = 2.05995 avg_loss = 3.32526\n",
      "epoch no.4 train no.410000  loss = 5.21440 avg_loss = 3.36926\n",
      "2\n",
      "to_tokens: ['▁라디오', '전환', '하기', '싶', '</s>']\n",
      "기분전환 하고가자</s>\n",
      "epoch no.4 train no.410010  loss = 2.52542 avg_loss = 3.34567\n",
      "epoch no.4 train no.410020  loss = 2.25718 avg_loss = 3.34076\n",
      "epoch no.4 train no.410030  loss = 4.67068 avg_loss = 3.35500\n",
      "epoch no.4 train no.410040  loss = 2.29809 avg_loss = 3.33467\n",
      "epoch no.4 train no.410050  loss = 2.95168 avg_loss = 3.34013\n",
      "epoch no.4 train no.410060  loss = 3.62699 avg_loss = 3.33876\n",
      "epoch no.4 train no.410070  loss = 4.27346 avg_loss = 3.36182\n",
      "epoch no.4 train no.410080  loss = 2.56637 avg_loss = 3.34967\n",
      "epoch no.4 train no.410090  loss = 2.89667 avg_loss = 3.38538\n",
      "epoch no.4 train no.410100  loss = 5.84687 avg_loss = 3.39534\n",
      "epoch no.4 train no.410110  loss = 3.24813 avg_loss = 3.37011\n",
      "epoch no.4 train no.410120  loss = 5.93874 avg_loss = 3.36791\n",
      "epoch no.4 train no.410130  loss = 3.22300 avg_loss = 3.37668\n",
      "epoch no.4 train no.410140  loss = 3.82955 avg_loss = 3.38946\n",
      "epoch no.4 train no.410150  loss = 4.21120 avg_loss = 3.40140\n",
      "epoch no.4 train no.410160  loss = 1.30249 avg_loss = 3.39145\n",
      "epoch no.4 train no.410170  loss = 2.31140 avg_loss = 3.40056\n",
      "epoch no.4 train no.410180  loss = 3.91777 avg_loss = 3.43761\n",
      "epoch no.4 train no.410190  loss = 3.72183 avg_loss = 3.41927\n",
      "epoch no.4 train no.410200  loss = 3.12564 avg_loss = 3.40428\n",
      "epoch no.4 train no.410210  loss = 5.39706 avg_loss = 3.42384\n",
      "epoch no.4 train no.410220  loss = 2.84987 avg_loss = 3.51338\n",
      "epoch no.4 train no.410230  loss = 3.06892 avg_loss = 3.50754\n",
      "epoch no.4 train no.410240  loss = 2.63030 avg_loss = 3.45042\n",
      "epoch no.4 train no.410250  loss = 4.52267 avg_loss = 3.45841\n",
      "epoch no.4 train no.410260  loss = 4.33064 avg_loss = 3.46190\n",
      "epoch no.4 train no.410270  loss = 3.67726 avg_loss = 3.46216\n",
      "epoch no.4 train no.410280  loss = 3.36869 avg_loss = 3.48063\n",
      "epoch no.4 train no.410290  loss = 2.46666 avg_loss = 3.46978\n",
      "epoch no.4 train no.410300  loss = 4.79734 avg_loss = 3.44472\n",
      "epoch no.4 train no.410310  loss = 3.63619 avg_loss = 3.43330\n",
      "epoch no.4 train no.410320  loss = 3.45410 avg_loss = 3.41168\n",
      "epoch no.4 train no.410330  loss = 3.38936 avg_loss = 3.38717\n",
      "epoch no.4 train no.410340  loss = 1.87382 avg_loss = 3.42343\n",
      "epoch no.4 train no.410350  loss = 4.59804 avg_loss = 3.50109\n",
      "epoch no.4 train no.410360  loss = 2.34902 avg_loss = 3.47495\n",
      "epoch no.4 train no.410370  loss = 4.21448 avg_loss = 3.45914\n",
      "epoch no.4 train no.410380  loss = 4.47728 avg_loss = 3.45072\n",
      "epoch no.4 train no.410390  loss = 4.19652 avg_loss = 3.47408\n",
      "epoch no.4 train no.410400  loss = 3.06629 avg_loss = 3.50407\n",
      "epoch no.4 train no.410410  loss = 2.68343 avg_loss = 3.48639\n",
      "epoch no.4 train no.410420  loss = 2.79568 avg_loss = 3.42381\n",
      "epoch no.4 train no.410430  loss = 2.61283 avg_loss = 3.43320\n",
      "epoch no.4 train no.410440  loss = 4.65228 avg_loss = 3.43976\n",
      "epoch no.4 train no.410450  loss = 2.58446 avg_loss = 3.41293\n",
      "epoch no.4 train no.410460  loss = 2.24527 avg_loss = 3.44327\n",
      "epoch no.4 train no.410470  loss = 3.48966 avg_loss = 3.42038\n",
      "epoch no.4 train no.410480  loss = 4.15393 avg_loss = 3.38789\n",
      "epoch no.4 train no.410490  loss = 2.44167 avg_loss = 3.40019\n",
      "epoch no.4 train no.410500  loss = 3.13227 avg_loss = 3.42748\n",
      "epoch no.4 train no.410510  loss = 4.50095 avg_loss = 3.41158\n",
      "epoch no.4 train no.410520  loss = 2.22330 avg_loss = 3.45347\n",
      "epoch no.4 train no.410530  loss = 1.23179 avg_loss = 3.42610\n",
      "epoch no.4 train no.410540  loss = 4.85102 avg_loss = 3.36537\n",
      "epoch no.4 train no.410550  loss = 3.30376 avg_loss = 3.34907\n",
      "epoch no.4 train no.410560  loss = 2.71569 avg_loss = 3.38193\n",
      "epoch no.4 train no.410570  loss = 4.89026 avg_loss = 3.38945\n",
      "epoch no.4 train no.410580  loss = 4.05947 avg_loss = 3.38485\n",
      "epoch no.4 train no.410590  loss = 4.48600 avg_loss = 3.39566\n",
      "epoch no.4 train no.410600  loss = 1.72003 avg_loss = 3.38299\n",
      "epoch no.4 train no.410610  loss = 4.00713 avg_loss = 3.39424\n",
      "epoch no.4 train no.410620  loss = 4.66000 avg_loss = 3.39326\n",
      "epoch no.4 train no.410630  loss = 4.95426 avg_loss = 3.45281\n",
      "epoch no.4 train no.410640  loss = 2.39812 avg_loss = 3.44588\n",
      "epoch no.4 train no.410650  loss = 3.40539 avg_loss = 3.39192\n",
      "epoch no.4 train no.410660  loss = 3.36661 avg_loss = 3.39004\n",
      "epoch no.4 train no.410670  loss = 3.29425 avg_loss = 3.35403\n",
      "epoch no.4 train no.410680  loss = 3.64913 avg_loss = 3.33356\n",
      "epoch no.4 train no.410690  loss = 3.51303 avg_loss = 3.32706\n",
      "epoch no.4 train no.410700  loss = 1.94647 avg_loss = 3.32826\n",
      "epoch no.4 train no.410710  loss = 3.48870 avg_loss = 3.32942\n",
      "epoch no.4 train no.410720  loss = 3.36896 avg_loss = 3.33528\n",
      "epoch no.4 train no.410730  loss = 2.78077 avg_loss = 3.37010\n",
      "epoch no.4 train no.410740  loss = 3.45526 avg_loss = 3.37084\n",
      "epoch no.4 train no.410750  loss = 4.10328 avg_loss = 3.40274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.410760  loss = 5.35264 avg_loss = 3.39807\n",
      "epoch no.4 train no.410770  loss = 2.13558 avg_loss = 3.34269\n",
      "epoch no.4 train no.410780  loss = 4.43260 avg_loss = 3.35441\n",
      "epoch no.4 train no.410790  loss = 2.42094 avg_loss = 3.35411\n",
      "epoch no.4 train no.410800  loss = 4.01294 avg_loss = 3.33895\n",
      "epoch no.4 train no.410810  loss = 3.17279 avg_loss = 3.35684\n",
      "epoch no.4 train no.410820  loss = 2.71818 avg_loss = 3.36542\n",
      "epoch no.4 train no.410830  loss = 4.92749 avg_loss = 3.36769\n",
      "epoch no.4 train no.410840  loss = 1.53235 avg_loss = 3.37118\n",
      "epoch no.4 train no.410850  loss = 5.68631 avg_loss = 3.35820\n",
      "epoch no.4 train no.410860  loss = 3.75228 avg_loss = 3.34118\n",
      "epoch no.4 train no.410870  loss = 3.48898 avg_loss = 3.34495\n",
      "epoch no.4 train no.410880  loss = 3.87366 avg_loss = 3.35706\n",
      "epoch no.4 train no.410890  loss = 2.20845 avg_loss = 3.30910\n",
      "epoch no.4 train no.410900  loss = 2.97551 avg_loss = 3.31302\n",
      "epoch no.4 train no.410910  loss = 5.34529 avg_loss = 3.35859\n",
      "epoch no.4 train no.410920  loss = 2.49907 avg_loss = 3.35213\n",
      "epoch no.4 train no.410930  loss = 3.97779 avg_loss = 3.34175\n",
      "epoch no.4 train no.410940  loss = 2.53703 avg_loss = 3.37478\n",
      "epoch no.4 train no.410950  loss = 4.31085 avg_loss = 3.38408\n",
      "epoch no.4 train no.410960  loss = 3.15473 avg_loss = 3.43314\n",
      "epoch no.4 train no.410970  loss = 3.26127 avg_loss = 3.43951\n",
      "epoch no.4 train no.410980  loss = 4.05693 avg_loss = 3.41859\n",
      "epoch no.4 train no.410990  loss = 2.65975 avg_loss = 3.38626\n",
      "epoch no.4 train no.411000  loss = 1.22278 avg_loss = 3.36992\n",
      "Killed\n",
      "time : 24468.322361707687\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "!python3 main.py --epoch=200 --data_file_path=./data/train_2_space.csv --save_path=./checkpoint/ --load_path=./checkpoint/KoGPT2_checkpoint_80000.tar --batch_size=1\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
