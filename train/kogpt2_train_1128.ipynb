{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kogpt2 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/minji/.local/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n",
      "2020-11-22 16:58:39.649701: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n",
      "2020-11-22 16:58:41.238969: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2020-11-22 16:58:41.246604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-22 16:58:41.247719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-22 16:58:41.247797: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-22 16:58:41.250032: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-22 16:58:41.251962: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-22 16:58:41.252363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-22 16:58:41.254750: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-22 16:58:41.256040: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-22 16:58:41.260943: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-22 16:58:41.265301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-22 16:58:41.265711: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-22 16:58:41.291606: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3499910000 Hz\n",
      "2020-11-22 16:58:41.292469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x781b0b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-22 16:58:41.292517: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-22 16:58:41.584794: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7886c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-22 16:58:41.584844: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-22 16:58:41.584855: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5\n",
      "2020-11-22 16:58:41.586383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-22 16:58:41.587538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: TITAN RTX computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2020-11-22 16:58:41.587650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-22 16:58:41.587695: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-22 16:58:41.587715: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-22 16:58:41.587735: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-22 16:58:41.587753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-22 16:58:41.587772: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-22 16:58:41.587793: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-22 16:58:41.591750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\n",
      "2020-11-22 16:58:41.591877: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-22 16:58:43.343468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-11-22 16:58:43.343536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \n",
      "2020-11-22 16:58:43.343548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y \n",
      "2020-11-22 16:58:43.343556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N \n",
      "2020-11-22 16:58:43.347113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22128 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:65:00.0, compute capability: 7.5)\n",
      "2020-11-22 16:58:43.348810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 21866 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:b3:00.0, compute capability: 7.5)\n",
      "using cached model\n",
      "using cached model\n",
      "0\n",
      "using cached model\n",
      "tokenizer ending\n",
      "(73352,)\n",
      "Read_Dataset ok\n",
      "KoGPT-2 Transfer Learning Start\n",
      "epoch no.0 train no.0  loss = 6.32065 avg_loss = 6.32065\n",
      "epoch no.0 train no.10  loss = 7.47854 avg_loss = 6.28060\n",
      "epoch no.0 train no.20  loss = 5.11162 avg_loss = 5.97042\n",
      "epoch no.0 train no.30  loss = 5.21437 avg_loss = 6.00251\n",
      "epoch no.0 train no.40  loss = 4.33505 avg_loss = 5.77471\n",
      "epoch no.0 train no.50  loss = 5.04764 avg_loss = 5.63641\n",
      "epoch no.0 train no.60  loss = 5.76946 avg_loss = 5.65194\n",
      "epoch no.0 train no.70  loss = 5.04783 avg_loss = 5.54721\n",
      "epoch no.0 train no.80  loss = 4.29710 avg_loss = 5.56406\n",
      "epoch no.0 train no.90  loss = 4.99258 avg_loss = 5.52022\n",
      "epoch no.0 train no.100  loss = 4.53792 avg_loss = 5.43593\n",
      "epoch no.0 train no.110  loss = 5.19787 avg_loss = 5.41660\n",
      "epoch no.0 train no.120  loss = 4.79242 avg_loss = 5.41181\n",
      "epoch no.0 train no.130  loss = 4.43076 avg_loss = 5.38077\n",
      "epoch no.0 train no.140  loss = 5.12161 avg_loss = 5.29232\n",
      "epoch no.0 train no.150  loss = 4.31418 avg_loss = 5.26294\n",
      "epoch no.0 train no.160  loss = 6.40243 avg_loss = 5.24239\n",
      "epoch no.0 train no.170  loss = 5.14571 avg_loss = 5.19453\n",
      "epoch no.0 train no.180  loss = 7.32817 avg_loss = 5.14082\n",
      "epoch no.0 train no.190  loss = 2.63007 avg_loss = 5.08002\n",
      "epoch no.0 train no.200  loss = 6.47357 avg_loss = 5.06541\n",
      "epoch no.0 train no.210  loss = 4.91998 avg_loss = 5.04998\n",
      "epoch no.0 train no.220  loss = 3.87073 avg_loss = 5.02547\n",
      "epoch no.0 train no.230  loss = 5.02251 avg_loss = 4.98610\n",
      "epoch no.0 train no.240  loss = 6.96985 avg_loss = 4.99463\n",
      "epoch no.0 train no.250  loss = 3.61942 avg_loss = 4.91969\n",
      "epoch no.0 train no.260  loss = 3.88438 avg_loss = 4.89103\n",
      "epoch no.0 train no.270  loss = 5.17697 avg_loss = 4.95000\n",
      "epoch no.0 train no.280  loss = 4.68980 avg_loss = 4.90520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.290  loss = 3.39976 avg_loss = 4.87472\n",
      "epoch no.0 train no.300  loss = 5.14754 avg_loss = 4.89032\n",
      "epoch no.0 train no.310  loss = 3.51273 avg_loss = 4.93078\n",
      "epoch no.0 train no.320  loss = 7.36780 avg_loss = 4.95712\n",
      "epoch no.0 train no.330  loss = 4.45019 avg_loss = 4.93402\n",
      "epoch no.0 train no.340  loss = 4.00554 avg_loss = 4.92541\n",
      "epoch no.0 train no.350  loss = 4.94639 avg_loss = 4.99369\n",
      "epoch no.0 train no.360  loss = 6.95362 avg_loss = 4.97898\n",
      "epoch no.0 train no.370  loss = 5.07812 avg_loss = 4.96796\n",
      "epoch no.0 train no.380  loss = 5.74435 avg_loss = 5.00758\n",
      "epoch no.0 train no.390  loss = 4.66374 avg_loss = 5.00573\n",
      "epoch no.0 train no.400  loss = 5.09461 avg_loss = 4.98809\n",
      "epoch no.0 train no.410  loss = 3.66624 avg_loss = 4.95419\n",
      "epoch no.0 train no.420  loss = 3.98299 avg_loss = 4.90300\n",
      "epoch no.0 train no.430  loss = 4.96114 avg_loss = 4.90753\n",
      "epoch no.0 train no.440  loss = 3.91609 avg_loss = 4.88419\n",
      "epoch no.0 train no.450  loss = 5.48875 avg_loss = 4.85185\n",
      "epoch no.0 train no.460  loss = 5.20884 avg_loss = 4.84306\n",
      "epoch no.0 train no.470  loss = 6.33533 avg_loss = 4.85219\n",
      "epoch no.0 train no.480  loss = 7.16517 avg_loss = 4.89311\n",
      "epoch no.0 train no.490  loss = 4.45264 avg_loss = 4.95864\n",
      "epoch no.0 train no.500  loss = 4.76925 avg_loss = 4.94846\n",
      "epoch no.0 train no.510  loss = 2.99868 avg_loss = 4.91320\n",
      "epoch no.0 train no.520  loss = 4.30773 avg_loss = 4.88718\n",
      "epoch no.0 train no.530  loss = 4.34169 avg_loss = 4.80444\n",
      "epoch no.0 train no.540  loss = 4.60128 avg_loss = 4.86821\n",
      "epoch no.0 train no.550  loss = 5.67135 avg_loss = 4.87942\n",
      "epoch no.0 train no.560  loss = 7.19131 avg_loss = 4.87282\n",
      "epoch no.0 train no.570  loss = 6.02658 avg_loss = 4.89495\n",
      "epoch no.0 train no.580  loss = 3.84754 avg_loss = 4.90072\n",
      "epoch no.0 train no.590  loss = 3.56848 avg_loss = 4.93720\n",
      "epoch no.0 train no.600  loss = 4.25737 avg_loss = 4.95616\n",
      "epoch no.0 train no.610  loss = 5.32031 avg_loss = 4.86277\n",
      "epoch no.0 train no.620  loss = 6.83728 avg_loss = 4.86666\n",
      "epoch no.0 train no.630  loss = 4.39736 avg_loss = 4.89324\n",
      "epoch no.0 train no.640  loss = 3.68095 avg_loss = 4.83817\n",
      "epoch no.0 train no.650  loss = 3.65057 avg_loss = 4.76232\n",
      "epoch no.0 train no.660  loss = 5.21938 avg_loss = 4.77735\n",
      "epoch no.0 train no.670  loss = 4.61981 avg_loss = 4.73328\n",
      "epoch no.0 train no.680  loss = 4.90270 avg_loss = 4.71906\n",
      "epoch no.0 train no.690  loss = 4.32104 avg_loss = 4.72661\n",
      "epoch no.0 train no.700  loss = 4.61269 avg_loss = 4.73220\n",
      "epoch no.0 train no.710  loss = 4.41131 avg_loss = 4.69939\n",
      "epoch no.0 train no.720  loss = 6.10425 avg_loss = 4.70572\n",
      "epoch no.0 train no.730  loss = 4.68240 avg_loss = 4.68006\n",
      "epoch no.0 train no.740  loss = 3.45938 avg_loss = 4.70458\n",
      "epoch no.0 train no.750  loss = 6.66803 avg_loss = 4.76163\n",
      "epoch no.0 train no.760  loss = 7.34595 avg_loss = 4.79936\n",
      "epoch no.0 train no.770  loss = 4.36841 avg_loss = 4.77604\n",
      "epoch no.0 train no.780  loss = 4.02065 avg_loss = 4.74055\n",
      "epoch no.0 train no.790  loss = 3.95585 avg_loss = 4.75859\n",
      "epoch no.0 train no.800  loss = 3.21899 avg_loss = 4.70197\n",
      "epoch no.0 train no.810  loss = 5.68129 avg_loss = 4.71140\n",
      "epoch no.0 train no.820  loss = 4.91776 avg_loss = 4.68154\n",
      "epoch no.0 train no.830  loss = 2.41139 avg_loss = 4.62470\n",
      "epoch no.0 train no.840  loss = 2.73404 avg_loss = 4.64927\n",
      "epoch no.0 train no.850  loss = 3.93710 avg_loss = 4.60465\n",
      "epoch no.0 train no.860  loss = 3.88294 avg_loss = 4.60131\n",
      "epoch no.0 train no.870  loss = 5.74401 avg_loss = 4.61792\n",
      "epoch no.0 train no.880  loss = 2.74336 avg_loss = 4.57857\n",
      "epoch no.0 train no.890  loss = 7.31778 avg_loss = 4.61079\n",
      "epoch no.0 train no.900  loss = 4.07140 avg_loss = 4.61191\n",
      "epoch no.0 train no.910  loss = 4.28630 avg_loss = 4.57364\n",
      "epoch no.0 train no.920  loss = 5.81339 avg_loss = 4.58894\n",
      "epoch no.0 train no.930  loss = 6.00110 avg_loss = 4.60653\n",
      "epoch no.0 train no.940  loss = 4.06814 avg_loss = 4.59759\n",
      "epoch no.0 train no.950  loss = 4.40385 avg_loss = 4.60125\n",
      "epoch no.0 train no.960  loss = 2.66087 avg_loss = 4.58567\n",
      "epoch no.0 train no.970  loss = 4.39463 avg_loss = 4.59544\n",
      "epoch no.0 train no.980  loss = 3.30799 avg_loss = 4.61186\n",
      "epoch no.0 train no.990  loss = 5.00947 avg_loss = 4.59381\n",
      "epoch no.0 train no.1000  loss = 4.31859 avg_loss = 4.57430\n",
      "2\n",
      "to_tokens: ['▁가을', '▁노래', '들', '</s>']\n",
      "추억의 노래들</s>\n",
      "epoch no.0 train no.1010  loss = 4.20465 avg_loss = 4.56960\n",
      "epoch no.0 train no.1020  loss = 5.93479 avg_loss = 4.59564\n",
      "epoch no.0 train no.1030  loss = 5.32594 avg_loss = 4.61803\n",
      "epoch no.0 train no.1040  loss = 4.18714 avg_loss = 4.61476\n",
      "epoch no.0 train no.1050  loss = 3.46669 avg_loss = 4.56398\n",
      "epoch no.0 train no.1060  loss = 3.18899 avg_loss = 4.55507\n",
      "epoch no.0 train no.1070  loss = 4.76478 avg_loss = 4.56963\n",
      "epoch no.0 train no.1080  loss = 4.51767 avg_loss = 4.61586\n",
      "epoch no.0 train no.1090  loss = 5.47121 avg_loss = 4.62509\n",
      "epoch no.0 train no.1100  loss = 3.86336 avg_loss = 4.58649\n",
      "epoch no.0 train no.1110  loss = 5.53327 avg_loss = 4.64668\n",
      "epoch no.0 train no.1120  loss = 2.74025 avg_loss = 4.64302\n",
      "epoch no.0 train no.1130  loss = 6.57369 avg_loss = 4.66662\n",
      "epoch no.0 train no.1140  loss = 3.32600 avg_loss = 4.63653\n",
      "epoch no.0 train no.1150  loss = 3.13643 avg_loss = 4.56415\n",
      "epoch no.0 train no.1160  loss = 2.85887 avg_loss = 4.53579\n",
      "epoch no.0 train no.1170  loss = 4.35982 avg_loss = 4.52431\n",
      "epoch no.0 train no.1180  loss = 3.60077 avg_loss = 4.55509\n",
      "epoch no.0 train no.1190  loss = 4.22401 avg_loss = 4.55381\n",
      "epoch no.0 train no.1200  loss = 3.88001 avg_loss = 4.60947\n",
      "epoch no.0 train no.1210  loss = 4.43820 avg_loss = 4.61213\n",
      "epoch no.0 train no.1220  loss = 3.87202 avg_loss = 4.61854\n",
      "epoch no.0 train no.1230  loss = 6.12756 avg_loss = 4.60488\n",
      "epoch no.0 train no.1240  loss = 4.35661 avg_loss = 4.58691\n",
      "epoch no.0 train no.1250  loss = 4.50432 avg_loss = 4.56254\n",
      "epoch no.0 train no.1260  loss = 6.41357 avg_loss = 4.56034\n",
      "epoch no.0 train no.1270  loss = 2.59214 avg_loss = 4.53738\n",
      "epoch no.0 train no.1280  loss = 4.17707 avg_loss = 4.52925\n",
      "epoch no.0 train no.1290  loss = 3.17686 avg_loss = 4.49756\n",
      "epoch no.0 train no.1300  loss = 4.08803 avg_loss = 4.50453\n",
      "epoch no.0 train no.1310  loss = 3.41768 avg_loss = 4.48620\n",
      "epoch no.0 train no.1320  loss = 5.92516 avg_loss = 4.55017\n",
      "epoch no.0 train no.1330  loss = 7.13074 avg_loss = 4.55012\n",
      "epoch no.0 train no.1340  loss = 2.31206 avg_loss = 4.52758\n",
      "epoch no.0 train no.1350  loss = 3.01192 avg_loss = 4.52719\n",
      "epoch no.0 train no.1360  loss = 3.86573 avg_loss = 4.53224\n",
      "epoch no.0 train no.1370  loss = 5.90184 avg_loss = 4.49689\n",
      "epoch no.0 train no.1380  loss = 3.65310 avg_loss = 4.52568\n",
      "epoch no.0 train no.1390  loss = 5.52760 avg_loss = 4.52327\n",
      "epoch no.0 train no.1400  loss = 5.64115 avg_loss = 4.52302\n",
      "epoch no.0 train no.1410  loss = 3.20870 avg_loss = 4.57383\n",
      "epoch no.0 train no.1420  loss = 2.57060 avg_loss = 4.48822\n",
      "epoch no.0 train no.1430  loss = 5.14309 avg_loss = 4.53508\n",
      "epoch no.0 train no.1440  loss = 4.71506 avg_loss = 4.46985\n",
      "epoch no.0 train no.1450  loss = 5.74829 avg_loss = 4.46784\n",
      "epoch no.0 train no.1460  loss = 4.83102 avg_loss = 4.48370\n",
      "epoch no.0 train no.1470  loss = 3.17464 avg_loss = 4.51561\n",
      "epoch no.0 train no.1480  loss = 7.29510 avg_loss = 4.56207\n",
      "epoch no.0 train no.1490  loss = 4.43652 avg_loss = 4.51021\n",
      "epoch no.0 train no.1500  loss = 2.81123 avg_loss = 4.51637\n",
      "epoch no.0 train no.1510  loss = 3.94293 avg_loss = 4.47806\n",
      "epoch no.0 train no.1520  loss = 6.26170 avg_loss = 4.53974\n",
      "epoch no.0 train no.1530  loss = 3.60855 avg_loss = 4.51768\n",
      "epoch no.0 train no.1540  loss = 7.12132 avg_loss = 4.51378\n",
      "epoch no.0 train no.1550  loss = 4.11438 avg_loss = 4.48895\n",
      "epoch no.0 train no.1560  loss = 5.59846 avg_loss = 4.46400\n",
      "epoch no.0 train no.1570  loss = 4.95707 avg_loss = 4.49859\n",
      "epoch no.0 train no.1580  loss = 5.95483 avg_loss = 4.49741\n",
      "epoch no.0 train no.1590  loss = 5.69292 avg_loss = 4.56687\n",
      "epoch no.0 train no.1600  loss = 4.61703 avg_loss = 4.57392\n",
      "epoch no.0 train no.1610  loss = 2.60906 avg_loss = 4.59958\n",
      "epoch no.0 train no.1620  loss = 3.81612 avg_loss = 4.60397\n",
      "epoch no.0 train no.1630  loss = 2.45719 avg_loss = 4.57615\n",
      "epoch no.0 train no.1640  loss = 3.90400 avg_loss = 4.64400\n",
      "epoch no.0 train no.1650  loss = 4.10850 avg_loss = 4.60956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.1660  loss = 5.94359 avg_loss = 4.61609\n",
      "epoch no.0 train no.1670  loss = 6.56349 avg_loss = 4.73874\n",
      "epoch no.0 train no.1680  loss = 2.12710 avg_loss = 4.74005\n",
      "epoch no.0 train no.1690  loss = 4.47220 avg_loss = 4.70642\n",
      "epoch no.0 train no.1700  loss = 3.57863 avg_loss = 4.70811\n",
      "epoch no.0 train no.1710  loss = 4.19209 avg_loss = 4.65123\n",
      "epoch no.0 train no.1720  loss = 5.13090 avg_loss = 4.61311\n",
      "epoch no.0 train no.1730  loss = 4.87964 avg_loss = 4.58537\n",
      "epoch no.0 train no.1740  loss = 6.02049 avg_loss = 4.62505\n",
      "epoch no.0 train no.1750  loss = 2.18806 avg_loss = 4.56030\n",
      "epoch no.0 train no.1760  loss = 3.93888 avg_loss = 4.54043\n",
      "epoch no.0 train no.1770  loss = 4.54103 avg_loss = 4.48627\n",
      "epoch no.0 train no.1780  loss = 3.24309 avg_loss = 4.44099\n",
      "epoch no.0 train no.1790  loss = 4.01181 avg_loss = 4.48075\n",
      "epoch no.0 train no.1800  loss = 6.24782 avg_loss = 4.49384\n",
      "epoch no.0 train no.1810  loss = 6.18538 avg_loss = 4.45114\n",
      "epoch no.0 train no.1820  loss = 5.50905 avg_loss = 4.39630\n",
      "epoch no.0 train no.1830  loss = 5.06503 avg_loss = 4.40287\n",
      "epoch no.0 train no.1840  loss = 3.03487 avg_loss = 4.35921\n",
      "epoch no.0 train no.1850  loss = 3.85718 avg_loss = 4.35152\n",
      "epoch no.0 train no.1860  loss = 2.64113 avg_loss = 4.29245\n",
      "epoch no.0 train no.1870  loss = 4.43970 avg_loss = 4.29166\n",
      "epoch no.0 train no.1880  loss = 4.15705 avg_loss = 4.34084\n",
      "epoch no.0 train no.1890  loss = 5.40403 avg_loss = 4.31513\n",
      "epoch no.0 train no.1900  loss = 5.77558 avg_loss = 4.34166\n",
      "epoch no.0 train no.1910  loss = 5.31525 avg_loss = 4.28681\n",
      "epoch no.0 train no.1920  loss = 3.08002 avg_loss = 4.30758\n",
      "epoch no.0 train no.1930  loss = 5.94413 avg_loss = 4.32428\n",
      "epoch no.0 train no.1940  loss = 6.23974 avg_loss = 4.32191\n",
      "epoch no.0 train no.1950  loss = 2.68577 avg_loss = 4.30179\n",
      "epoch no.0 train no.1960  loss = 4.30208 avg_loss = 4.41738\n",
      "epoch no.0 train no.1970  loss = 4.00384 avg_loss = 4.39751\n",
      "epoch no.0 train no.1980  loss = 4.17270 avg_loss = 4.44079\n",
      "epoch no.0 train no.1990  loss = 5.85721 avg_loss = 4.49412\n",
      "epoch no.0 train no.2000  loss = 5.89839 avg_loss = 4.51666\n",
      "3\n",
      "to_tokens: ['▁비', '▁명', '송', '▁모음', '</s>']\n",
      "추억의 팝송 모음</s>\n",
      "epoch no.0 train no.2010  loss = 5.08371 avg_loss = 4.50692\n",
      "epoch no.0 train no.2020  loss = 4.35478 avg_loss = 4.50975\n",
      "epoch no.0 train no.2030  loss = 5.06365 avg_loss = 4.52423\n",
      "epoch no.0 train no.2040  loss = 4.01847 avg_loss = 4.54687\n",
      "epoch no.0 train no.2050  loss = 2.92166 avg_loss = 4.53394\n",
      "epoch no.0 train no.2060  loss = 6.03888 avg_loss = 4.57886\n",
      "epoch no.0 train no.2070  loss = 7.44194 avg_loss = 4.61579\n",
      "epoch no.0 train no.2080  loss = 5.47435 avg_loss = 4.67249\n",
      "epoch no.0 train no.2090  loss = 4.09891 avg_loss = 4.63430\n",
      "epoch no.0 train no.2100  loss = 3.53183 avg_loss = 4.62256\n",
      "epoch no.0 train no.2110  loss = 5.45081 avg_loss = 4.60964\n",
      "epoch no.0 train no.2120  loss = 3.41872 avg_loss = 4.56643\n",
      "epoch no.0 train no.2130  loss = 3.39873 avg_loss = 4.54824\n",
      "epoch no.0 train no.2140  loss = 3.36161 avg_loss = 4.52524\n",
      "epoch no.0 train no.2150  loss = 3.26895 avg_loss = 4.51000\n",
      "epoch no.0 train no.2160  loss = 1.78201 avg_loss = 4.49943\n",
      "epoch no.0 train no.2170  loss = 4.36632 avg_loss = 4.52745\n",
      "epoch no.0 train no.2180  loss = 8.21841 avg_loss = 4.55084\n",
      "epoch no.0 train no.2190  loss = 5.38325 avg_loss = 4.53161\n",
      "epoch no.0 train no.2200  loss = 3.64490 avg_loss = 4.52647\n",
      "epoch no.0 train no.2210  loss = 5.91082 avg_loss = 4.49728\n",
      "epoch no.0 train no.2220  loss = 3.73362 avg_loss = 4.49418\n",
      "epoch no.0 train no.2230  loss = 4.16219 avg_loss = 4.52126\n",
      "epoch no.0 train no.2240  loss = 4.06097 avg_loss = 4.55099\n",
      "epoch no.0 train no.2250  loss = 4.64770 avg_loss = 4.51797\n",
      "epoch no.0 train no.2260  loss = 2.28458 avg_loss = 4.50986\n",
      "epoch no.0 train no.2270  loss = 4.42860 avg_loss = 4.51248\n",
      "epoch no.0 train no.2280  loss = 6.07643 avg_loss = 4.55380\n",
      "epoch no.0 train no.2290  loss = 3.69059 avg_loss = 4.53804\n",
      "epoch no.0 train no.2300  loss = 3.57769 avg_loss = 4.52747\n",
      "epoch no.0 train no.2310  loss = 3.14492 avg_loss = 4.53453\n",
      "epoch no.0 train no.2320  loss = 5.77990 avg_loss = 4.56719\n",
      "epoch no.0 train no.2330  loss = 4.49032 avg_loss = 4.57969\n",
      "epoch no.0 train no.2340  loss = 2.78714 avg_loss = 4.54194\n",
      "epoch no.0 train no.2350  loss = 4.26546 avg_loss = 4.53563\n",
      "epoch no.0 train no.2360  loss = 3.16027 avg_loss = 4.49514\n",
      "epoch no.0 train no.2370  loss = 6.29852 avg_loss = 4.47317\n",
      "epoch no.0 train no.2380  loss = 7.76502 avg_loss = 4.53537\n",
      "epoch no.0 train no.2390  loss = 2.45339 avg_loss = 4.53237\n",
      "epoch no.0 train no.2400  loss = 5.79150 avg_loss = 4.52257\n",
      "epoch no.0 train no.2410  loss = 4.86268 avg_loss = 4.49315\n",
      "epoch no.0 train no.2420  loss = 4.15856 avg_loss = 4.51611\n",
      "epoch no.0 train no.2430  loss = 3.70861 avg_loss = 4.47138\n",
      "epoch no.0 train no.2440  loss = 2.52256 avg_loss = 4.44515\n",
      "epoch no.0 train no.2450  loss = 4.28260 avg_loss = 4.51177\n",
      "epoch no.0 train no.2460  loss = 3.95378 avg_loss = 4.53056\n",
      "epoch no.0 train no.2470  loss = 3.43615 avg_loss = 4.54621\n",
      "epoch no.0 train no.2480  loss = 5.68821 avg_loss = 4.52408\n",
      "epoch no.0 train no.2490  loss = 3.91277 avg_loss = 4.58012\n",
      "epoch no.0 train no.2500  loss = 3.94410 avg_loss = 4.55827\n",
      "epoch no.0 train no.2510  loss = 5.61776 avg_loss = 4.53887\n",
      "epoch no.0 train no.2520  loss = 3.43118 avg_loss = 4.56427\n",
      "epoch no.0 train no.2530  loss = 7.41779 avg_loss = 4.57623\n",
      "epoch no.0 train no.2540  loss = 5.79028 avg_loss = 4.57350\n",
      "epoch no.0 train no.2550  loss = 4.54642 avg_loss = 4.58330\n",
      "epoch no.0 train no.2560  loss = 4.61161 avg_loss = 4.58056\n",
      "epoch no.0 train no.2570  loss = 6.33993 avg_loss = 4.54847\n",
      "epoch no.0 train no.2580  loss = 3.79843 avg_loss = 4.45061\n",
      "epoch no.0 train no.2590  loss = 6.21954 avg_loss = 4.42850\n",
      "epoch no.0 train no.2600  loss = 3.40349 avg_loss = 4.37303\n",
      "epoch no.0 train no.2610  loss = 5.20175 avg_loss = 4.38327\n",
      "epoch no.0 train no.2620  loss = 4.40504 avg_loss = 4.30302\n",
      "epoch no.0 train no.2630  loss = 5.14144 avg_loss = 4.30639\n",
      "epoch no.0 train no.2640  loss = 3.46534 avg_loss = 4.29500\n",
      "epoch no.0 train no.2650  loss = 5.45959 avg_loss = 4.31185\n",
      "epoch no.0 train no.2660  loss = 3.51205 avg_loss = 4.32896\n",
      "epoch no.0 train no.2670  loss = 3.45008 avg_loss = 4.28021\n",
      "epoch no.0 train no.2680  loss = 4.63070 avg_loss = 4.37005\n",
      "epoch no.0 train no.2690  loss = 3.54413 avg_loss = 4.36815\n",
      "epoch no.0 train no.2700  loss = 3.06062 avg_loss = 4.36540\n",
      "epoch no.0 train no.2710  loss = 4.50987 avg_loss = 4.42564\n",
      "epoch no.0 train no.2720  loss = 4.55437 avg_loss = 4.43704\n",
      "epoch no.0 train no.2730  loss = 5.63142 avg_loss = 4.47046\n",
      "epoch no.0 train no.2740  loss = 5.43667 avg_loss = 4.50963\n",
      "epoch no.0 train no.2750  loss = 5.90930 avg_loss = 4.50071\n",
      "epoch no.0 train no.2760  loss = 4.08985 avg_loss = 4.44555\n",
      "epoch no.0 train no.2770  loss = 3.89024 avg_loss = 4.47070\n",
      "epoch no.0 train no.2780  loss = 4.56575 avg_loss = 4.47714\n",
      "epoch no.0 train no.2790  loss = 3.86174 avg_loss = 4.44291\n",
      "epoch no.0 train no.2800  loss = 2.41279 avg_loss = 4.36319\n",
      "epoch no.0 train no.2810  loss = 2.21788 avg_loss = 4.32084\n",
      "epoch no.0 train no.2820  loss = 2.35988 avg_loss = 4.27509\n",
      "epoch no.0 train no.2830  loss = 4.77445 avg_loss = 4.24372\n",
      "epoch no.0 train no.2840  loss = 3.21349 avg_loss = 4.22727\n",
      "epoch no.0 train no.2850  loss = 5.56056 avg_loss = 4.21029\n",
      "epoch no.0 train no.2860  loss = 4.14054 avg_loss = 4.25957\n",
      "epoch no.0 train no.2870  loss = 4.06292 avg_loss = 4.27871\n",
      "epoch no.0 train no.2880  loss = 4.41384 avg_loss = 4.21354\n",
      "epoch no.0 train no.2890  loss = 5.21373 avg_loss = 4.21465\n",
      "epoch no.0 train no.2900  loss = 4.55400 avg_loss = 4.21849\n",
      "epoch no.0 train no.2910  loss = 3.54001 avg_loss = 4.19537\n",
      "epoch no.0 train no.2920  loss = 4.80425 avg_loss = 4.17130\n",
      "epoch no.0 train no.2930  loss = 4.30275 avg_loss = 4.21570\n",
      "epoch no.0 train no.2940  loss = 5.77192 avg_loss = 4.23944\n",
      "epoch no.0 train no.2950  loss = 7.18324 avg_loss = 4.30027\n",
      "epoch no.0 train no.2960  loss = 3.65024 avg_loss = 4.30775\n",
      "epoch no.0 train no.2970  loss = 3.38668 avg_loss = 4.29922\n",
      "epoch no.0 train no.2980  loss = 3.33118 avg_loss = 4.30121\n",
      "epoch no.0 train no.2990  loss = 4.63469 avg_loss = 4.29053\n",
      "epoch no.0 train no.3000  loss = 3.61134 avg_loss = 4.31975\n",
      "3\n",
      "to_tokens: ['▁비', '▁노래', '곡', '들', '</s>']\n",
      "추억의 명곡들</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.3010  loss = 3.07486 avg_loss = 4.37854\n",
      "epoch no.0 train no.3020  loss = 4.74874 avg_loss = 4.41089\n",
      "epoch no.0 train no.3030  loss = 5.77404 avg_loss = 4.43720\n",
      "epoch no.0 train no.3040  loss = 3.33650 avg_loss = 4.44525\n",
      "epoch no.0 train no.3050  loss = 5.48988 avg_loss = 4.43677\n",
      "epoch no.0 train no.3060  loss = 5.92956 avg_loss = 4.46552\n",
      "epoch no.0 train no.3070  loss = 4.11783 avg_loss = 4.45228\n",
      "epoch no.0 train no.3080  loss = 4.29874 avg_loss = 4.43015\n",
      "epoch no.0 train no.3090  loss = 3.80124 avg_loss = 4.40540\n",
      "epoch no.0 train no.3100  loss = 3.42283 avg_loss = 4.43186\n",
      "epoch no.0 train no.3110  loss = 3.51953 avg_loss = 4.43199\n",
      "epoch no.0 train no.3120  loss = 4.01292 avg_loss = 4.43272\n",
      "epoch no.0 train no.3130  loss = 5.06190 avg_loss = 4.51562\n",
      "epoch no.0 train no.3140  loss = 4.59956 avg_loss = 4.52028\n",
      "epoch no.0 train no.3150  loss = 5.12949 avg_loss = 4.50366\n",
      "epoch no.0 train no.3160  loss = 6.84192 avg_loss = 4.49040\n",
      "epoch no.0 train no.3170  loss = 3.58611 avg_loss = 4.44348\n",
      "epoch no.0 train no.3180  loss = 5.36144 avg_loss = 4.38627\n",
      "epoch no.0 train no.3190  loss = 2.47844 avg_loss = 4.36862\n",
      "epoch no.0 train no.3200  loss = 2.46661 avg_loss = 4.31790\n",
      "epoch no.0 train no.3210  loss = 5.86137 avg_loss = 4.33108\n",
      "epoch no.0 train no.3220  loss = 4.15103 avg_loss = 4.30791\n",
      "epoch no.0 train no.3230  loss = 4.23569 avg_loss = 4.32337\n",
      "epoch no.0 train no.3240  loss = 3.85341 avg_loss = 4.30391\n",
      "epoch no.0 train no.3250  loss = 4.57222 avg_loss = 4.33755\n",
      "epoch no.0 train no.3260  loss = 3.81330 avg_loss = 4.38218\n",
      "epoch no.0 train no.3270  loss = 5.05713 avg_loss = 4.38040\n",
      "epoch no.0 train no.3280  loss = 2.56282 avg_loss = 4.31800\n",
      "epoch no.0 train no.3290  loss = 3.88439 avg_loss = 4.41958\n",
      "epoch no.0 train no.3300  loss = 4.49008 avg_loss = 4.44912\n",
      "epoch no.0 train no.3310  loss = 4.88422 avg_loss = 4.46518\n",
      "epoch no.0 train no.3320  loss = 3.41993 avg_loss = 4.39591\n",
      "epoch no.0 train no.3330  loss = 5.04078 avg_loss = 4.37883\n",
      "epoch no.0 train no.3340  loss = 4.35832 avg_loss = 4.33284\n",
      "epoch no.0 train no.3350  loss = 5.83582 avg_loss = 4.31917\n",
      "epoch no.0 train no.3360  loss = 4.28687 avg_loss = 4.29343\n",
      "epoch no.0 train no.3370  loss = 5.86634 avg_loss = 4.31247\n",
      "epoch no.0 train no.3380  loss = 4.21982 avg_loss = 4.33429\n",
      "epoch no.0 train no.3390  loss = 5.86423 avg_loss = 4.43343\n",
      "epoch no.0 train no.3400  loss = 6.25225 avg_loss = 4.41489\n",
      "epoch no.0 train no.3410  loss = 6.13540 avg_loss = 4.40264\n",
      "epoch no.0 train no.3420  loss = 5.50697 avg_loss = 4.44221\n",
      "epoch no.0 train no.3430  loss = 6.99620 avg_loss = 4.47258\n",
      "epoch no.0 train no.3440  loss = 4.28280 avg_loss = 4.47909\n",
      "epoch no.0 train no.3450  loss = 5.37169 avg_loss = 4.50528\n",
      "epoch no.0 train no.3460  loss = 4.90180 avg_loss = 4.48274\n",
      "epoch no.0 train no.3470  loss = 5.57175 avg_loss = 4.51609\n",
      "epoch no.0 train no.3480  loss = 2.26890 avg_loss = 4.50418\n",
      "epoch no.0 train no.3490  loss = 2.93065 avg_loss = 4.51641\n",
      "epoch no.0 train no.3500  loss = 4.99283 avg_loss = 4.52920\n",
      "epoch no.0 train no.3510  loss = 2.97948 avg_loss = 4.49879\n",
      "epoch no.0 train no.3520  loss = 6.23954 avg_loss = 4.53945\n",
      "epoch no.0 train no.3530  loss = 3.75230 avg_loss = 4.51088\n",
      "epoch no.0 train no.3540  loss = 2.00890 avg_loss = 4.47849\n",
      "epoch no.0 train no.3550  loss = 2.87687 avg_loss = 4.43205\n",
      "epoch no.0 train no.3560  loss = 4.44264 avg_loss = 4.45171\n",
      "epoch no.0 train no.3570  loss = 2.37617 avg_loss = 4.39471\n",
      "epoch no.0 train no.3580  loss = 3.72960 avg_loss = 4.36417\n",
      "epoch no.0 train no.3590  loss = 4.48180 avg_loss = 4.38147\n",
      "epoch no.0 train no.3600  loss = 3.55349 avg_loss = 4.39431\n",
      "epoch no.0 train no.3610  loss = 3.19969 avg_loss = 4.35077\n",
      "epoch no.0 train no.3620  loss = 2.77494 avg_loss = 4.35920\n",
      "epoch no.0 train no.3630  loss = 5.21103 avg_loss = 4.34794\n",
      "epoch no.0 train no.3640  loss = 5.33128 avg_loss = 4.34240\n",
      "epoch no.0 train no.3650  loss = 2.56993 avg_loss = 4.31207\n",
      "epoch no.0 train no.3660  loss = 5.21697 avg_loss = 4.36267\n",
      "epoch no.0 train no.3670  loss = 3.18388 avg_loss = 4.35684\n",
      "epoch no.0 train no.3680  loss = 3.18204 avg_loss = 4.32054\n",
      "epoch no.0 train no.3690  loss = 4.67035 avg_loss = 4.31270\n",
      "epoch no.0 train no.3700  loss = 4.02863 avg_loss = 4.30857\n",
      "epoch no.0 train no.3710  loss = 3.16747 avg_loss = 4.31123\n",
      "epoch no.0 train no.3720  loss = 3.72143 avg_loss = 4.28118\n",
      "epoch no.0 train no.3730  loss = 4.92727 avg_loss = 4.34376\n",
      "epoch no.0 train no.3740  loss = 3.17028 avg_loss = 4.37042\n",
      "epoch no.0 train no.3750  loss = 4.38107 avg_loss = 4.38368\n",
      "epoch no.0 train no.3760  loss = 3.14848 avg_loss = 4.38738\n",
      "epoch no.0 train no.3770  loss = 4.90492 avg_loss = 4.33406\n",
      "epoch no.0 train no.3780  loss = 4.88033 avg_loss = 4.29239\n",
      "epoch no.0 train no.3790  loss = 3.05919 avg_loss = 4.24181\n",
      "epoch no.0 train no.3800  loss = 4.65942 avg_loss = 4.18701\n",
      "epoch no.0 train no.3810  loss = 4.38380 avg_loss = 4.20851\n",
      "epoch no.0 train no.3820  loss = 3.10517 avg_loss = 4.21547\n",
      "epoch no.0 train no.3830  loss = 2.88076 avg_loss = 4.17180\n",
      "epoch no.0 train no.3840  loss = 5.82819 avg_loss = 4.22977\n",
      "epoch no.0 train no.3850  loss = 3.57840 avg_loss = 4.21982\n",
      "epoch no.0 train no.3860  loss = 5.35818 avg_loss = 4.21538\n",
      "epoch no.0 train no.3870  loss = 5.89528 avg_loss = 4.23845\n",
      "epoch no.0 train no.3880  loss = 4.46102 avg_loss = 4.21619\n",
      "epoch no.0 train no.3890  loss = 2.47089 avg_loss = 4.17114\n",
      "epoch no.0 train no.3900  loss = 3.45717 avg_loss = 4.22624\n",
      "epoch no.0 train no.3910  loss = 4.68303 avg_loss = 4.20699\n",
      "epoch no.0 train no.3920  loss = 5.32465 avg_loss = 4.21976\n",
      "epoch no.0 train no.3930  loss = 4.38841 avg_loss = 4.27841\n",
      "epoch no.0 train no.3940  loss = 5.10859 avg_loss = 4.27256\n",
      "epoch no.0 train no.3950  loss = 3.08547 avg_loss = 4.27605\n",
      "epoch no.0 train no.3960  loss = 3.15395 avg_loss = 4.28103\n",
      "epoch no.0 train no.3970  loss = 3.68041 avg_loss = 4.29077\n",
      "epoch no.0 train no.3980  loss = 6.48955 avg_loss = 4.29627\n",
      "epoch no.0 train no.3990  loss = 4.50236 avg_loss = 4.31030\n",
      "epoch no.0 train no.4000  loss = 2.80643 avg_loss = 4.27438\n",
      "2\n",
      "to_tokens: ['▁', '▁노래', '들', '</s>']\n",
      "추억의 노래 모음</s>\n",
      "epoch no.0 train no.4010  loss = 3.82332 avg_loss = 4.26583\n",
      "epoch no.0 train no.4020  loss = 5.26957 avg_loss = 4.29566\n",
      "epoch no.0 train no.4030  loss = 4.60968 avg_loss = 4.27884\n",
      "epoch no.0 train no.4040  loss = 3.36221 avg_loss = 4.26301\n",
      "epoch no.0 train no.4050  loss = 5.54740 avg_loss = 4.21531\n",
      "epoch no.0 train no.4060  loss = 4.63540 avg_loss = 4.24077\n",
      "epoch no.0 train no.4070  loss = 4.37275 avg_loss = 4.26552\n",
      "epoch no.0 train no.4080  loss = 3.97731 avg_loss = 4.23966\n",
      "epoch no.0 train no.4090  loss = 3.84318 avg_loss = 4.22651\n",
      "epoch no.0 train no.4100  loss = 6.29530 avg_loss = 4.25345\n",
      "epoch no.0 train no.4110  loss = 2.63116 avg_loss = 4.20860\n",
      "epoch no.0 train no.4120  loss = 3.31192 avg_loss = 4.23815\n",
      "epoch no.0 train no.4130  loss = 3.73573 avg_loss = 4.22996\n",
      "epoch no.0 train no.4140  loss = 6.34345 avg_loss = 4.24057\n",
      "epoch no.0 train no.4150  loss = 3.06779 avg_loss = 4.17033\n",
      "epoch no.0 train no.4160  loss = 3.13859 avg_loss = 4.15968\n",
      "epoch no.0 train no.4170  loss = 3.74468 avg_loss = 4.14880\n",
      "epoch no.0 train no.4180  loss = 4.95959 avg_loss = 4.13309\n",
      "epoch no.0 train no.4190  loss = 5.24112 avg_loss = 4.17687\n",
      "epoch no.0 train no.4200  loss = 3.49120 avg_loss = 4.21037\n",
      "epoch no.0 train no.4210  loss = 6.67853 avg_loss = 4.22779\n",
      "epoch no.0 train no.4220  loss = 4.19489 avg_loss = 4.28387\n",
      "epoch no.0 train no.4230  loss = 3.20851 avg_loss = 4.25985\n",
      "epoch no.0 train no.4240  loss = 5.27652 avg_loss = 4.27124\n",
      "epoch no.0 train no.4250  loss = 3.72311 avg_loss = 4.25910\n",
      "epoch no.0 train no.4260  loss = 4.25222 avg_loss = 4.28326\n",
      "epoch no.0 train no.4270  loss = 3.22545 avg_loss = 4.28548\n",
      "epoch no.0 train no.4280  loss = 1.68653 avg_loss = 4.26450\n",
      "epoch no.0 train no.4290  loss = 4.26688 avg_loss = 4.28565\n",
      "epoch no.0 train no.4300  loss = 4.95559 avg_loss = 4.25097\n",
      "epoch no.0 train no.4310  loss = 2.95062 avg_loss = 4.23942\n",
      "epoch no.0 train no.4320  loss = 6.87488 avg_loss = 4.37663\n",
      "epoch no.0 train no.4330  loss = 6.90001 avg_loss = 4.41168\n",
      "epoch no.0 train no.4340  loss = 3.26535 avg_loss = 4.35379\n",
      "epoch no.0 train no.4350  loss = 4.95624 avg_loss = 4.38208\n",
      "epoch no.0 train no.4360  loss = 3.96870 avg_loss = 4.37713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.4370  loss = 5.53168 avg_loss = 4.44319\n",
      "epoch no.0 train no.4380  loss = 3.57874 avg_loss = 4.42926\n",
      "epoch no.0 train no.4390  loss = 8.93536 avg_loss = 4.45755\n",
      "epoch no.0 train no.4400  loss = 3.76372 avg_loss = 4.42227\n",
      "epoch no.0 train no.4410  loss = 5.27669 avg_loss = 4.41337\n",
      "epoch no.0 train no.4420  loss = 3.68539 avg_loss = 4.37782\n",
      "epoch no.0 train no.4430  loss = 3.24643 avg_loss = 4.30086\n",
      "epoch no.0 train no.4440  loss = 4.72294 avg_loss = 4.37864\n",
      "epoch no.0 train no.4450  loss = 5.00326 avg_loss = 4.41035\n",
      "epoch no.0 train no.4460  loss = 2.88479 avg_loss = 4.35926\n",
      "epoch no.0 train no.4470  loss = 4.99974 avg_loss = 4.36484\n",
      "epoch no.0 train no.4480  loss = 3.54846 avg_loss = 4.35780\n",
      "epoch no.0 train no.4490  loss = 5.59690 avg_loss = 4.36086\n",
      "epoch no.0 train no.4500  loss = 4.30045 avg_loss = 4.39052\n",
      "epoch no.0 train no.4510  loss = 3.61685 avg_loss = 4.36628\n",
      "epoch no.0 train no.4520  loss = 2.54691 avg_loss = 4.33533\n",
      "epoch no.0 train no.4530  loss = 2.89892 avg_loss = 4.32081\n",
      "epoch no.0 train no.4540  loss = 6.87762 avg_loss = 4.29208\n",
      "epoch no.0 train no.4550  loss = 3.51041 avg_loss = 4.28304\n",
      "epoch no.0 train no.4560  loss = 4.15060 avg_loss = 4.30091\n",
      "epoch no.0 train no.4570  loss = 2.77468 avg_loss = 4.32278\n",
      "epoch no.0 train no.4580  loss = 4.22036 avg_loss = 4.34571\n",
      "epoch no.0 train no.4590  loss = 6.13194 avg_loss = 4.37773\n",
      "epoch no.0 train no.4600  loss = 4.29307 avg_loss = 4.40268\n",
      "epoch no.0 train no.4610  loss = 2.19669 avg_loss = 4.43517\n",
      "epoch no.0 train no.4620  loss = 3.48260 avg_loss = 4.37392\n",
      "epoch no.0 train no.4630  loss = 4.19767 avg_loss = 4.42494\n",
      "epoch no.0 train no.4640  loss = 4.90729 avg_loss = 4.38582\n",
      "epoch no.0 train no.4650  loss = 4.56668 avg_loss = 4.30770\n",
      "epoch no.0 train no.4660  loss = 4.23846 avg_loss = 4.32827\n",
      "epoch no.0 train no.4670  loss = 4.05324 avg_loss = 4.31591\n",
      "epoch no.0 train no.4680  loss = 4.81401 avg_loss = 4.32851\n",
      "epoch no.0 train no.4690  loss = 3.74161 avg_loss = 4.32537\n",
      "epoch no.0 train no.4700  loss = 4.17688 avg_loss = 4.36452\n",
      "epoch no.0 train no.4710  loss = 6.22745 avg_loss = 4.34145\n",
      "epoch no.0 train no.4720  loss = 3.01904 avg_loss = 4.37097\n",
      "epoch no.0 train no.4730  loss = 2.31153 avg_loss = 4.30272\n",
      "epoch no.0 train no.4740  loss = 5.12763 avg_loss = 4.27769\n",
      "epoch no.0 train no.4750  loss = 4.24789 avg_loss = 4.32761\n",
      "epoch no.0 train no.4760  loss = 4.36587 avg_loss = 4.27110\n",
      "epoch no.0 train no.4770  loss = 3.25124 avg_loss = 4.25203\n",
      "epoch no.0 train no.4780  loss = 3.20630 avg_loss = 4.22625\n",
      "epoch no.0 train no.4790  loss = 3.22937 avg_loss = 4.13946\n",
      "epoch no.0 train no.4800  loss = 6.53219 avg_loss = 4.20636\n",
      "epoch no.0 train no.4810  loss = 5.19864 avg_loss = 4.19762\n",
      "epoch no.0 train no.4820  loss = 5.20415 avg_loss = 4.15005\n",
      "epoch no.0 train no.4830  loss = 4.64538 avg_loss = 4.13879\n",
      "epoch no.0 train no.4840  loss = 3.86635 avg_loss = 4.09162\n",
      "epoch no.0 train no.4850  loss = 4.90805 avg_loss = 4.09579\n",
      "epoch no.0 train no.4860  loss = 4.33813 avg_loss = 4.16404\n",
      "epoch no.0 train no.4870  loss = 3.47769 avg_loss = 4.15296\n",
      "epoch no.0 train no.4880  loss = 4.49934 avg_loss = 4.18831\n",
      "epoch no.0 train no.4890  loss = 3.53096 avg_loss = 4.14721\n",
      "epoch no.0 train no.4900  loss = 4.98235 avg_loss = 4.15756\n",
      "epoch no.0 train no.4910  loss = 5.62508 avg_loss = 4.23061\n",
      "epoch no.0 train no.4920  loss = 2.78613 avg_loss = 4.19307\n",
      "epoch no.0 train no.4930  loss = 2.57886 avg_loss = 4.19816\n",
      "epoch no.0 train no.4940  loss = 3.86512 avg_loss = 4.21799\n",
      "epoch no.0 train no.4950  loss = 4.80180 avg_loss = 4.24517\n",
      "epoch no.0 train no.4960  loss = 3.85874 avg_loss = 4.23245\n",
      "epoch no.0 train no.4970  loss = 4.79331 avg_loss = 4.24667\n",
      "epoch no.0 train no.4980  loss = 3.43968 avg_loss = 4.28579\n",
      "epoch no.0 train no.4990  loss = 4.70301 avg_loss = 4.32326\n",
      "epoch no.0 train no.5000  loss = 3.59809 avg_loss = 4.30256\n",
      "2\n",
      "to_tokens: ['▁가을', '▁노래', '송', '</s>']\n",
      "추억의 팝송</s>\n",
      "epoch no.0 train no.5010  loss = 3.30079 avg_loss = 4.29841\n",
      "epoch no.0 train no.5020  loss = 4.06612 avg_loss = 4.34766\n",
      "epoch no.0 train no.5030  loss = 3.58248 avg_loss = 4.34317\n",
      "epoch no.0 train no.5040  loss = 3.36088 avg_loss = 4.26463\n",
      "epoch no.0 train no.5050  loss = 4.25886 avg_loss = 4.20915\n",
      "epoch no.0 train no.5060  loss = 3.17670 avg_loss = 4.16154\n",
      "epoch no.0 train no.5070  loss = 3.34843 avg_loss = 4.18077\n",
      "epoch no.0 train no.5080  loss = 5.06213 avg_loss = 4.16083\n",
      "epoch no.0 train no.5090  loss = 3.83681 avg_loss = 4.12404\n",
      "epoch no.0 train no.5100  loss = 5.79124 avg_loss = 4.11690\n",
      "epoch no.0 train no.5110  loss = 4.22964 avg_loss = 4.13000\n",
      "epoch no.0 train no.5120  loss = 3.36724 avg_loss = 4.14950\n",
      "epoch no.0 train no.5130  loss = 3.56375 avg_loss = 4.12419\n",
      "epoch no.0 train no.5140  loss = 5.96951 avg_loss = 4.17464\n",
      "epoch no.0 train no.5150  loss = 2.27653 avg_loss = 4.13232\n",
      "epoch no.0 train no.5160  loss = 6.87567 avg_loss = 4.16532\n",
      "epoch no.0 train no.5170  loss = 3.37014 avg_loss = 4.18609\n",
      "epoch no.0 train no.5180  loss = 8.24049 avg_loss = 4.25290\n",
      "epoch no.0 train no.5190  loss = 3.52757 avg_loss = 4.28609\n",
      "epoch no.0 train no.5200  loss = 3.14152 avg_loss = 4.26783\n",
      "epoch no.0 train no.5210  loss = 4.34437 avg_loss = 4.28955\n",
      "epoch no.0 train no.5220  loss = 4.96592 avg_loss = 4.24667\n",
      "epoch no.0 train no.5230  loss = 3.61213 avg_loss = 4.22641\n",
      "epoch no.0 train no.5240  loss = 3.82281 avg_loss = 4.22168\n",
      "epoch no.0 train no.5250  loss = 3.80933 avg_loss = 4.19901\n",
      "epoch no.0 train no.5260  loss = 4.28528 avg_loss = 4.31380\n",
      "epoch no.0 train no.5270  loss = 4.74912 avg_loss = 4.28693\n",
      "epoch no.0 train no.5280  loss = 4.59789 avg_loss = 4.32776\n",
      "epoch no.0 train no.5290  loss = 3.55081 avg_loss = 4.36820\n",
      "epoch no.0 train no.5300  loss = 3.61100 avg_loss = 4.34087\n",
      "epoch no.0 train no.5310  loss = 4.09978 avg_loss = 4.30138\n",
      "epoch no.0 train no.5320  loss = 3.89170 avg_loss = 4.31272\n",
      "epoch no.0 train no.5330  loss = 2.54071 avg_loss = 4.29226\n",
      "epoch no.0 train no.5340  loss = 4.44750 avg_loss = 4.27084\n",
      "epoch no.0 train no.5350  loss = 2.48613 avg_loss = 4.30773\n",
      "epoch no.0 train no.5360  loss = 4.85757 avg_loss = 4.32807\n",
      "epoch no.0 train no.5370  loss = 3.36778 avg_loss = 4.33869\n",
      "epoch no.0 train no.5380  loss = 3.58861 avg_loss = 4.33172\n",
      "epoch no.0 train no.5390  loss = 2.66935 avg_loss = 4.38405\n",
      "epoch no.0 train no.5400  loss = 5.32910 avg_loss = 4.31088\n",
      "epoch no.0 train no.5410  loss = 3.40699 avg_loss = 4.35384\n",
      "epoch no.0 train no.5420  loss = 4.74392 avg_loss = 4.39015\n",
      "epoch no.0 train no.5430  loss = 7.65199 avg_loss = 4.42438\n",
      "epoch no.0 train no.5440  loss = 2.89289 avg_loss = 4.38631\n",
      "epoch no.0 train no.5450  loss = 2.93288 avg_loss = 4.33909\n",
      "epoch no.0 train no.5460  loss = 6.61465 avg_loss = 4.31174\n",
      "epoch no.0 train no.5470  loss = 4.04001 avg_loss = 4.32134\n",
      "epoch no.0 train no.5480  loss = 4.85249 avg_loss = 4.29970\n",
      "epoch no.0 train no.5490  loss = 3.22266 avg_loss = 4.24785\n",
      "epoch no.0 train no.5500  loss = 2.88739 avg_loss = 4.23893\n",
      "epoch no.0 train no.5510  loss = 6.60117 avg_loss = 4.28175\n",
      "epoch no.0 train no.5520  loss = 3.88375 avg_loss = 4.23962\n",
      "epoch no.0 train no.5530  loss = 4.71802 avg_loss = 4.23429\n",
      "epoch no.0 train no.5540  loss = 3.18718 avg_loss = 4.23592\n",
      "epoch no.0 train no.5550  loss = 3.49809 avg_loss = 4.22571\n",
      "epoch no.0 train no.5560  loss = 4.45426 avg_loss = 4.20842\n",
      "epoch no.0 train no.5570  loss = 3.43298 avg_loss = 4.21308\n",
      "epoch no.0 train no.5580  loss = 5.64585 avg_loss = 4.30420\n",
      "epoch no.0 train no.5590  loss = 3.08951 avg_loss = 4.28632\n",
      "epoch no.0 train no.5600  loss = 5.93441 avg_loss = 4.34815\n",
      "epoch no.0 train no.5610  loss = 7.51520 avg_loss = 4.39754\n",
      "epoch no.0 train no.5620  loss = 5.74391 avg_loss = 4.42380\n",
      "epoch no.0 train no.5630  loss = 5.68578 avg_loss = 4.52753\n",
      "epoch no.0 train no.5640  loss = 4.61704 avg_loss = 4.40477\n",
      "epoch no.0 train no.5650  loss = 4.29079 avg_loss = 4.39142\n",
      "epoch no.0 train no.5660  loss = 2.90914 avg_loss = 4.36691\n",
      "epoch no.0 train no.5670  loss = 2.50188 avg_loss = 4.34823\n",
      "epoch no.0 train no.5680  loss = 3.14755 avg_loss = 4.30502\n",
      "epoch no.0 train no.5690  loss = 3.45413 avg_loss = 4.33791\n",
      "epoch no.0 train no.5700  loss = 3.73452 avg_loss = 4.31381\n",
      "epoch no.0 train no.5710  loss = 5.11344 avg_loss = 4.35023\n",
      "epoch no.0 train no.5720  loss = 6.68094 avg_loss = 4.33817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.5730  loss = 6.52276 avg_loss = 4.35231\n",
      "epoch no.0 train no.5740  loss = 3.70858 avg_loss = 4.27136\n",
      "epoch no.0 train no.5750  loss = 3.63617 avg_loss = 4.25735\n",
      "epoch no.0 train no.5760  loss = 2.95847 avg_loss = 4.22482\n",
      "epoch no.0 train no.5770  loss = 3.21765 avg_loss = 4.22246\n",
      "epoch no.0 train no.5780  loss = 2.81413 avg_loss = 4.19485\n",
      "epoch no.0 train no.5790  loss = 6.54848 avg_loss = 4.24690\n",
      "epoch no.0 train no.5800  loss = 3.64828 avg_loss = 4.22906\n",
      "epoch no.0 train no.5810  loss = 5.27256 avg_loss = 4.23295\n",
      "epoch no.0 train no.5820  loss = 2.77141 avg_loss = 4.28557\n",
      "epoch no.0 train no.5830  loss = 5.36096 avg_loss = 4.26008\n",
      "epoch no.0 train no.5840  loss = 7.88079 avg_loss = 4.28681\n",
      "epoch no.0 train no.5850  loss = 4.08467 avg_loss = 4.25714\n",
      "epoch no.0 train no.5860  loss = 5.46519 avg_loss = 4.32315\n",
      "epoch no.0 train no.5870  loss = 3.49821 avg_loss = 4.28799\n",
      "epoch no.0 train no.5880  loss = 4.22402 avg_loss = 4.25541\n",
      "epoch no.0 train no.5890  loss = 3.45840 avg_loss = 4.31143\n",
      "epoch no.0 train no.5900  loss = 4.91361 avg_loss = 4.29032\n",
      "epoch no.0 train no.5910  loss = 4.63409 avg_loss = 4.27306\n",
      "epoch no.0 train no.5920  loss = 6.03555 avg_loss = 4.24827\n",
      "epoch no.0 train no.5930  loss = 2.61361 avg_loss = 4.22392\n",
      "epoch no.0 train no.5940  loss = 2.72493 avg_loss = 4.16275\n",
      "epoch no.0 train no.5950  loss = 4.71408 avg_loss = 4.22559\n",
      "epoch no.0 train no.5960  loss = 4.04062 avg_loss = 4.18203\n",
      "epoch no.0 train no.5970  loss = 7.28907 avg_loss = 4.25021\n",
      "epoch no.0 train no.5980  loss = 3.95400 avg_loss = 4.23695\n",
      "epoch no.0 train no.5990  loss = 5.19633 avg_loss = 4.21684\n",
      "epoch no.0 train no.6000  loss = 6.09677 avg_loss = 4.26238\n",
      "4\n",
      "to_tokens: ['▁가을', '▁명', '▁명', '▁o', 'st', '</s>']\n",
      "추억의 추억의 드라마 ost</s>\n",
      "epoch no.0 train no.6010  loss = 3.18928 avg_loss = 4.21773\n",
      "epoch no.0 train no.6020  loss = 4.20957 avg_loss = 4.24931\n",
      "epoch no.0 train no.6030  loss = 3.80173 avg_loss = 4.23813\n",
      "epoch no.0 train no.6040  loss = 5.24697 avg_loss = 4.21619\n",
      "epoch no.0 train no.6050  loss = 4.24988 avg_loss = 4.21694\n",
      "epoch no.0 train no.6060  loss = 4.44868 avg_loss = 4.20958\n",
      "epoch no.0 train no.6070  loss = 3.80657 avg_loss = 4.13957\n",
      "epoch no.0 train no.6080  loss = 3.15064 avg_loss = 4.12748\n",
      "epoch no.0 train no.6090  loss = 6.56010 avg_loss = 4.15123\n",
      "epoch no.0 train no.6100  loss = 2.84873 avg_loss = 4.13266\n",
      "epoch no.0 train no.6110  loss = 4.39211 avg_loss = 4.18837\n",
      "epoch no.0 train no.6120  loss = 4.64333 avg_loss = 4.14179\n",
      "epoch no.0 train no.6130  loss = 3.93014 avg_loss = 4.10430\n",
      "epoch no.0 train no.6140  loss = 2.57412 avg_loss = 4.09296\n",
      "epoch no.0 train no.6150  loss = 3.39572 avg_loss = 4.09620\n",
      "epoch no.0 train no.6160  loss = 2.67789 avg_loss = 4.09717\n",
      "epoch no.0 train no.6170  loss = 3.50982 avg_loss = 4.12745\n",
      "epoch no.0 train no.6180  loss = 2.99306 avg_loss = 4.14160\n",
      "epoch no.0 train no.6190  loss = 4.56180 avg_loss = 4.17236\n",
      "epoch no.0 train no.6200  loss = 3.47812 avg_loss = 4.17739\n",
      "epoch no.0 train no.6210  loss = 3.49608 avg_loss = 4.17959\n",
      "epoch no.0 train no.6220  loss = 3.35390 avg_loss = 4.20727\n",
      "epoch no.0 train no.6230  loss = 4.76145 avg_loss = 4.23470\n",
      "epoch no.0 train no.6240  loss = 4.03303 avg_loss = 4.16830\n",
      "epoch no.0 train no.6250  loss = 2.56617 avg_loss = 4.21130\n",
      "epoch no.0 train no.6260  loss = 3.51874 avg_loss = 4.16464\n",
      "epoch no.0 train no.6270  loss = 3.73790 avg_loss = 4.13696\n",
      "epoch no.0 train no.6280  loss = 4.13036 avg_loss = 4.17699\n",
      "epoch no.0 train no.6290  loss = 3.85033 avg_loss = 4.19683\n",
      "epoch no.0 train no.6300  loss = 2.22331 avg_loss = 4.15611\n",
      "epoch no.0 train no.6310  loss = 4.92142 avg_loss = 4.14925\n",
      "epoch no.0 train no.6320  loss = 3.66500 avg_loss = 4.18324\n",
      "epoch no.0 train no.6330  loss = 4.83640 avg_loss = 4.22243\n",
      "epoch no.0 train no.6340  loss = 4.01802 avg_loss = 4.19372\n",
      "epoch no.0 train no.6350  loss = 4.65510 avg_loss = 4.16161\n",
      "epoch no.0 train no.6360  loss = 2.59915 avg_loss = 4.14339\n",
      "epoch no.0 train no.6370  loss = 7.30174 avg_loss = 4.17497\n",
      "epoch no.0 train no.6380  loss = 4.95988 avg_loss = 4.19769\n",
      "epoch no.0 train no.6390  loss = 3.49903 avg_loss = 4.19113\n",
      "epoch no.0 train no.6400  loss = 3.75907 avg_loss = 4.19726\n",
      "epoch no.0 train no.6410  loss = 3.78063 avg_loss = 4.23101\n",
      "epoch no.0 train no.6420  loss = 4.13237 avg_loss = 4.19433\n",
      "epoch no.0 train no.6430  loss = 5.72648 avg_loss = 4.17553\n",
      "epoch no.0 train no.6440  loss = 5.73746 avg_loss = 4.15584\n",
      "epoch no.0 train no.6450  loss = 2.92625 avg_loss = 4.14312\n",
      "epoch no.0 train no.6460  loss = 3.73322 avg_loss = 4.17706\n",
      "epoch no.0 train no.6470  loss = 7.19118 avg_loss = 4.21281\n",
      "epoch no.0 train no.6480  loss = 4.86736 avg_loss = 4.22201\n",
      "epoch no.0 train no.6490  loss = 4.52845 avg_loss = 4.18006\n",
      "epoch no.0 train no.6500  loss = 3.06501 avg_loss = 4.16599\n",
      "epoch no.0 train no.6510  loss = 2.40480 avg_loss = 4.21314\n",
      "epoch no.0 train no.6520  loss = 3.32889 avg_loss = 4.24122\n",
      "epoch no.0 train no.6530  loss = 2.81832 avg_loss = 4.18871\n",
      "epoch no.0 train no.6540  loss = 3.27373 avg_loss = 4.25148\n",
      "epoch no.0 train no.6550  loss = 4.39049 avg_loss = 4.23942\n",
      "epoch no.0 train no.6560  loss = 3.35877 avg_loss = 4.24424\n",
      "epoch no.0 train no.6570  loss = 3.80231 avg_loss = 4.24343\n",
      "epoch no.0 train no.6580  loss = 5.28333 avg_loss = 4.24541\n",
      "epoch no.0 train no.6590  loss = 2.24637 avg_loss = 4.18936\n",
      "epoch no.0 train no.6600  loss = 3.52743 avg_loss = 4.20822\n",
      "epoch no.0 train no.6610  loss = 4.29181 avg_loss = 4.21154\n",
      "epoch no.0 train no.6620  loss = 6.22040 avg_loss = 4.22325\n",
      "epoch no.0 train no.6630  loss = 3.93930 avg_loss = 4.22960\n",
      "epoch no.0 train no.6640  loss = 3.08563 avg_loss = 4.20058\n",
      "epoch no.0 train no.6650  loss = 3.22269 avg_loss = 4.19535\n",
      "epoch no.0 train no.6660  loss = 2.46540 avg_loss = 4.22872\n",
      "epoch no.0 train no.6670  loss = 5.93379 avg_loss = 4.26698\n",
      "epoch no.0 train no.6680  loss = 5.58408 avg_loss = 4.36376\n",
      "epoch no.0 train no.6690  loss = 2.90768 avg_loss = 4.31340\n",
      "epoch no.0 train no.6700  loss = 5.45806 avg_loss = 4.34712\n",
      "epoch no.0 train no.6710  loss = 4.66962 avg_loss = 4.38747\n",
      "epoch no.0 train no.6720  loss = 2.27601 avg_loss = 4.37957\n",
      "epoch no.0 train no.6730  loss = 4.61656 avg_loss = 4.41511\n",
      "epoch no.0 train no.6740  loss = 4.89887 avg_loss = 4.40869\n",
      "epoch no.0 train no.6750  loss = 5.86598 avg_loss = 4.37825\n",
      "epoch no.0 train no.6760  loss = 4.65346 avg_loss = 4.41927\n",
      "epoch no.0 train no.6770  loss = 5.33135 avg_loss = 4.39683\n",
      "epoch no.0 train no.6780  loss = 3.04527 avg_loss = 4.33609\n",
      "epoch no.0 train no.6790  loss = 5.88534 avg_loss = 4.40026\n",
      "epoch no.0 train no.6800  loss = 4.16562 avg_loss = 4.38933\n",
      "epoch no.0 train no.6810  loss = 4.73394 avg_loss = 4.41030\n",
      "epoch no.0 train no.6820  loss = 3.43315 avg_loss = 4.32359\n",
      "epoch no.0 train no.6830  loss = 6.05689 avg_loss = 4.29273\n",
      "epoch no.0 train no.6840  loss = 5.92180 avg_loss = 4.23204\n",
      "epoch no.0 train no.6850  loss = 3.30806 avg_loss = 4.25584\n",
      "epoch no.0 train no.6860  loss = 2.31425 avg_loss = 4.21962\n",
      "epoch no.0 train no.6870  loss = 3.19792 avg_loss = 4.19369\n",
      "epoch no.0 train no.6880  loss = 3.93699 avg_loss = 4.14972\n",
      "epoch no.0 train no.6890  loss = 4.99076 avg_loss = 4.15250\n",
      "epoch no.0 train no.6900  loss = 5.95890 avg_loss = 4.16565\n",
      "epoch no.0 train no.6910  loss = 5.28877 avg_loss = 4.15497\n",
      "epoch no.0 train no.6920  loss = 6.72655 avg_loss = 4.16652\n",
      "epoch no.0 train no.6930  loss = 3.22867 avg_loss = 4.15537\n",
      "epoch no.0 train no.6940  loss = 6.14312 avg_loss = 4.22680\n",
      "epoch no.0 train no.6950  loss = 3.58522 avg_loss = 4.25625\n",
      "epoch no.0 train no.6960  loss = 3.19213 avg_loss = 4.24213\n",
      "epoch no.0 train no.6970  loss = 5.70347 avg_loss = 4.24065\n",
      "epoch no.0 train no.6980  loss = 4.92415 avg_loss = 4.22004\n",
      "epoch no.0 train no.6990  loss = 5.20818 avg_loss = 4.26473\n",
      "epoch no.0 train no.7000  loss = 2.53427 avg_loss = 4.20246\n",
      "3\n",
      "to_tokens: ['▁가을', '▁노래', '▁플레이', '▁추억의', '</s>']\n",
      "추억의 게임 속 노래</s>\n",
      "epoch no.0 train no.7010  loss = 3.70498 avg_loss = 4.19937\n",
      "epoch no.0 train no.7020  loss = 2.72336 avg_loss = 4.24962\n",
      "epoch no.0 train no.7030  loss = 2.96839 avg_loss = 4.25309\n",
      "epoch no.0 train no.7040  loss = 4.40199 avg_loss = 4.25669\n",
      "epoch no.0 train no.7050  loss = 3.56547 avg_loss = 4.22204\n",
      "epoch no.0 train no.7060  loss = 3.49995 avg_loss = 4.21000\n",
      "epoch no.0 train no.7070  loss = 4.11820 avg_loss = 4.18414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.7080  loss = 6.88044 avg_loss = 4.21599\n",
      "epoch no.0 train no.7090  loss = 3.61618 avg_loss = 4.20935\n",
      "epoch no.0 train no.7100  loss = 4.60017 avg_loss = 4.22978\n",
      "epoch no.0 train no.7110  loss = 4.28043 avg_loss = 4.27362\n",
      "epoch no.0 train no.7120  loss = 3.98000 avg_loss = 4.24928\n",
      "epoch no.0 train no.7130  loss = 5.46691 avg_loss = 4.26154\n",
      "epoch no.0 train no.7140  loss = 4.80260 avg_loss = 4.30559\n",
      "epoch no.0 train no.7150  loss = 5.76022 avg_loss = 4.27618\n",
      "epoch no.0 train no.7160  loss = 5.09508 avg_loss = 4.24121\n",
      "epoch no.0 train no.7170  loss = 4.26104 avg_loss = 4.26358\n",
      "epoch no.0 train no.7180  loss = 4.61406 avg_loss = 4.26900\n",
      "epoch no.0 train no.7190  loss = 3.90570 avg_loss = 4.26869\n",
      "epoch no.0 train no.7200  loss = 4.32613 avg_loss = 4.24463\n",
      "epoch no.0 train no.7210  loss = 3.65891 avg_loss = 4.21534\n",
      "epoch no.0 train no.7220  loss = 4.56121 avg_loss = 4.25196\n",
      "epoch no.0 train no.7230  loss = 4.54823 avg_loss = 4.26222\n",
      "epoch no.0 train no.7240  loss = 6.66941 avg_loss = 4.22234\n",
      "epoch no.0 train no.7250  loss = 2.86877 avg_loss = 4.21694\n",
      "epoch no.0 train no.7260  loss = 3.76658 avg_loss = 4.20580\n",
      "epoch no.0 train no.7270  loss = 6.40974 avg_loss = 4.23732\n",
      "epoch no.0 train no.7280  loss = 3.10584 avg_loss = 4.16410\n",
      "epoch no.0 train no.7290  loss = 3.22881 avg_loss = 4.23111\n",
      "epoch no.0 train no.7300  loss = 2.81113 avg_loss = 4.25184\n",
      "epoch no.0 train no.7310  loss = 3.21150 avg_loss = 4.23568\n",
      "epoch no.0 train no.7320  loss = 3.76930 avg_loss = 4.19329\n",
      "epoch no.0 train no.7330  loss = 6.52243 avg_loss = 4.20874\n",
      "epoch no.0 train no.7340  loss = 4.05779 avg_loss = 4.28555\n",
      "epoch no.0 train no.7350  loss = 4.90025 avg_loss = 4.34575\n",
      "epoch no.0 train no.7360  loss = 3.93849 avg_loss = 4.32409\n",
      "epoch no.0 train no.7370  loss = 2.89107 avg_loss = 4.28817\n",
      "epoch no.0 train no.7380  loss = 5.36236 avg_loss = 4.25701\n",
      "epoch no.0 train no.7390  loss = 6.98686 avg_loss = 4.31136\n",
      "epoch no.0 train no.7400  loss = 3.86865 avg_loss = 4.29132\n",
      "epoch no.0 train no.7410  loss = 5.65420 avg_loss = 4.29602\n",
      "epoch no.0 train no.7420  loss = 2.99872 avg_loss = 4.24366\n",
      "epoch no.0 train no.7430  loss = 3.14026 avg_loss = 4.32403\n",
      "epoch no.0 train no.7440  loss = 4.61976 avg_loss = 4.28557\n",
      "epoch no.0 train no.7450  loss = 2.77743 avg_loss = 4.28862\n",
      "epoch no.0 train no.7460  loss = 4.28997 avg_loss = 4.32317\n",
      "epoch no.0 train no.7470  loss = 4.24632 avg_loss = 4.32997\n",
      "epoch no.0 train no.7480  loss = 3.45313 avg_loss = 4.30361\n",
      "epoch no.0 train no.7490  loss = 3.70364 avg_loss = 4.31481\n",
      "epoch no.0 train no.7500  loss = 6.84079 avg_loss = 4.32278\n",
      "epoch no.0 train no.7510  loss = 5.28107 avg_loss = 4.28258\n",
      "epoch no.0 train no.7520  loss = 3.58366 avg_loss = 4.21917\n",
      "epoch no.0 train no.7530  loss = 3.75284 avg_loss = 4.26359\n",
      "epoch no.0 train no.7540  loss = 1.59931 avg_loss = 4.16722\n",
      "epoch no.0 train no.7550  loss = 2.44621 avg_loss = 4.14645\n",
      "epoch no.0 train no.7560  loss = 3.57437 avg_loss = 4.16234\n",
      "epoch no.0 train no.7570  loss = 5.10834 avg_loss = 4.14644\n",
      "epoch no.0 train no.7580  loss = 1.72923 avg_loss = 4.10568\n",
      "epoch no.0 train no.7590  loss = 4.32953 avg_loss = 4.18912\n",
      "epoch no.0 train no.7600  loss = 4.73086 avg_loss = 4.14508\n",
      "epoch no.0 train no.7610  loss = 4.08260 avg_loss = 4.15728\n",
      "epoch no.0 train no.7620  loss = 2.68542 avg_loss = 4.16601\n",
      "epoch no.0 train no.7630  loss = 6.15296 avg_loss = 4.18249\n",
      "epoch no.0 train no.7640  loss = 5.42206 avg_loss = 4.16564\n",
      "epoch no.0 train no.7650  loss = 2.95411 avg_loss = 4.13855\n",
      "epoch no.0 train no.7660  loss = 4.58328 avg_loss = 4.06160\n",
      "epoch no.0 train no.7670  loss = 3.98449 avg_loss = 4.02470\n",
      "epoch no.0 train no.7680  loss = 3.97031 avg_loss = 3.98718\n",
      "epoch no.0 train no.7690  loss = 5.04589 avg_loss = 4.04584\n",
      "epoch no.0 train no.7700  loss = 6.03048 avg_loss = 4.05045\n",
      "epoch no.0 train no.7710  loss = 3.70240 avg_loss = 4.04129\n",
      "epoch no.0 train no.7720  loss = 3.26508 avg_loss = 4.05692\n",
      "epoch no.0 train no.7730  loss = 4.93099 avg_loss = 4.07428\n",
      "epoch no.0 train no.7740  loss = 4.00771 avg_loss = 4.05762\n",
      "epoch no.0 train no.7750  loss = 8.30984 avg_loss = 4.13464\n",
      "epoch no.0 train no.7760  loss = 2.03405 avg_loss = 4.08921\n",
      "epoch no.0 train no.7770  loss = 6.74518 avg_loss = 4.13268\n",
      "epoch no.0 train no.7780  loss = 4.94540 avg_loss = 4.17269\n",
      "epoch no.0 train no.7790  loss = 3.57050 avg_loss = 4.19725\n",
      "epoch no.0 train no.7800  loss = 5.35569 avg_loss = 4.23501\n",
      "epoch no.0 train no.7810  loss = 4.35792 avg_loss = 4.25875\n",
      "epoch no.0 train no.7820  loss = 4.17564 avg_loss = 4.25020\n",
      "epoch no.0 train no.7830  loss = 3.94557 avg_loss = 4.31258\n",
      "epoch no.0 train no.7840  loss = 3.41944 avg_loss = 4.28388\n",
      "epoch no.0 train no.7850  loss = 4.11929 avg_loss = 4.29379\n",
      "epoch no.0 train no.7860  loss = 3.88017 avg_loss = 4.27454\n",
      "epoch no.0 train no.7870  loss = 2.54659 avg_loss = 4.32279\n",
      "epoch no.0 train no.7880  loss = 3.72262 avg_loss = 4.34018\n",
      "epoch no.0 train no.7890  loss = 6.41266 avg_loss = 4.39027\n",
      "epoch no.0 train no.7900  loss = 4.39546 avg_loss = 4.35708\n",
      "epoch no.0 train no.7910  loss = 2.81611 avg_loss = 4.29404\n",
      "epoch no.0 train no.7920  loss = 4.60883 avg_loss = 4.29681\n",
      "epoch no.0 train no.7930  loss = 4.17567 avg_loss = 4.28143\n",
      "epoch no.0 train no.7940  loss = 3.18042 avg_loss = 4.26051\n",
      "epoch no.0 train no.7950  loss = 2.67177 avg_loss = 4.20661\n",
      "epoch no.0 train no.7960  loss = 5.01012 avg_loss = 4.21356\n",
      "epoch no.0 train no.7970  loss = 4.05722 avg_loss = 4.21864\n",
      "epoch no.0 train no.7980  loss = 3.26392 avg_loss = 4.14978\n",
      "epoch no.0 train no.7990  loss = 2.68809 avg_loss = 4.20702\n",
      "epoch no.0 train no.8000  loss = 4.66088 avg_loss = 4.23400\n",
      "2\n",
      "to_tokens: ['▁', '▁노래', '곡', '</s>']\n",
      "추억의 힙합팝</s>\n",
      "epoch no.0 train no.8010  loss = 3.28788 avg_loss = 4.22314\n",
      "epoch no.0 train no.8020  loss = 3.41272 avg_loss = 4.21505\n",
      "epoch no.0 train no.8030  loss = 4.33566 avg_loss = 4.23926\n",
      "epoch no.0 train no.8040  loss = 4.77481 avg_loss = 4.25487\n",
      "epoch no.0 train no.8050  loss = 5.31975 avg_loss = 4.24452\n",
      "epoch no.0 train no.8060  loss = 3.99200 avg_loss = 4.22205\n",
      "epoch no.0 train no.8070  loss = 7.10809 avg_loss = 4.27299\n",
      "epoch no.0 train no.8080  loss = 3.22015 avg_loss = 4.28177\n",
      "epoch no.0 train no.8090  loss = 3.58860 avg_loss = 4.25175\n",
      "epoch no.0 train no.8100  loss = 5.56715 avg_loss = 4.16645\n",
      "epoch no.0 train no.8110  loss = 5.86254 avg_loss = 4.18535\n",
      "epoch no.0 train no.8120  loss = 4.75721 avg_loss = 4.23444\n",
      "epoch no.0 train no.8130  loss = 4.80155 avg_loss = 4.22983\n",
      "epoch no.0 train no.8140  loss = 3.66240 avg_loss = 4.21752\n",
      "epoch no.0 train no.8150  loss = 3.70984 avg_loss = 4.21226\n",
      "epoch no.0 train no.8160  loss = 2.46091 avg_loss = 4.19608\n",
      "epoch no.0 train no.8170  loss = 5.10412 avg_loss = 4.20005\n",
      "epoch no.0 train no.8180  loss = 4.81168 avg_loss = 4.18731\n",
      "epoch no.0 train no.8190  loss = 4.41680 avg_loss = 4.26454\n",
      "epoch no.0 train no.8200  loss = 4.95269 avg_loss = 4.24119\n",
      "epoch no.0 train no.8210  loss = 2.40162 avg_loss = 4.21776\n",
      "epoch no.0 train no.8220  loss = 3.11986 avg_loss = 4.20256\n",
      "epoch no.0 train no.8230  loss = 3.26843 avg_loss = 4.19677\n",
      "epoch no.0 train no.8240  loss = 4.11842 avg_loss = 4.15927\n",
      "epoch no.0 train no.8250  loss = 3.32400 avg_loss = 4.22424\n",
      "epoch no.0 train no.8260  loss = 3.00673 avg_loss = 4.17528\n",
      "epoch no.0 train no.8270  loss = 4.89184 avg_loss = 4.24891\n",
      "epoch no.0 train no.8280  loss = 5.22060 avg_loss = 4.21508\n",
      "epoch no.0 train no.8290  loss = 3.18165 avg_loss = 4.20059\n",
      "epoch no.0 train no.8300  loss = 1.71484 avg_loss = 4.16189\n",
      "epoch no.0 train no.8310  loss = 3.82938 avg_loss = 4.13272\n",
      "epoch no.0 train no.8320  loss = 2.52831 avg_loss = 4.09472\n",
      "epoch no.0 train no.8330  loss = 4.31226 avg_loss = 4.09438\n",
      "epoch no.0 train no.8340  loss = 3.02708 avg_loss = 4.09769\n",
      "epoch no.0 train no.8350  loss = 3.75389 avg_loss = 4.05962\n",
      "epoch no.0 train no.8360  loss = 4.53924 avg_loss = 4.07343\n",
      "epoch no.0 train no.8370  loss = 6.19795 avg_loss = 4.12134\n",
      "epoch no.0 train no.8380  loss = 3.56890 avg_loss = 4.12659\n",
      "epoch no.0 train no.8390  loss = 3.17387 avg_loss = 4.09177\n",
      "epoch no.0 train no.8400  loss = 4.29434 avg_loss = 4.12571\n",
      "epoch no.0 train no.8410  loss = 4.09827 avg_loss = 4.12336\n",
      "epoch no.0 train no.8420  loss = 5.72580 avg_loss = 4.11447\n",
      "epoch no.0 train no.8430  loss = 3.45444 avg_loss = 4.11617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.8440  loss = 3.93618 avg_loss = 4.16529\n",
      "epoch no.0 train no.8450  loss = 5.48845 avg_loss = 4.19556\n",
      "epoch no.0 train no.8460  loss = 3.11171 avg_loss = 4.14744\n",
      "epoch no.0 train no.8470  loss = 4.20443 avg_loss = 4.17191\n",
      "epoch no.0 train no.8480  loss = 4.92989 avg_loss = 4.20458\n",
      "epoch no.0 train no.8490  loss = 4.34185 avg_loss = 4.16270\n",
      "epoch no.0 train no.8500  loss = 2.88645 avg_loss = 4.17794\n",
      "epoch no.0 train no.8510  loss = 3.61136 avg_loss = 4.16256\n",
      "epoch no.0 train no.8520  loss = 5.01423 avg_loss = 4.12497\n",
      "epoch no.0 train no.8530  loss = 4.05420 avg_loss = 4.09213\n",
      "epoch no.0 train no.8540  loss = 5.03797 avg_loss = 4.13157\n",
      "epoch no.0 train no.8550  loss = 3.33387 avg_loss = 4.08703\n",
      "epoch no.0 train no.8560  loss = 4.53155 avg_loss = 4.09756\n",
      "epoch no.0 train no.8570  loss = 5.11356 avg_loss = 4.12652\n",
      "epoch no.0 train no.8580  loss = 2.67612 avg_loss = 4.11452\n",
      "epoch no.0 train no.8590  loss = 3.59721 avg_loss = 4.13158\n",
      "epoch no.0 train no.8600  loss = 4.12090 avg_loss = 4.09684\n",
      "epoch no.0 train no.8610  loss = 3.40372 avg_loss = 4.08844\n",
      "epoch no.0 train no.8620  loss = 5.47057 avg_loss = 4.08462\n",
      "epoch no.0 train no.8630  loss = 3.45245 avg_loss = 4.09899\n",
      "epoch no.0 train no.8640  loss = 2.72572 avg_loss = 4.11268\n",
      "epoch no.0 train no.8650  loss = 4.24986 avg_loss = 4.13510\n",
      "epoch no.0 train no.8660  loss = 3.96151 avg_loss = 4.09937\n",
      "epoch no.0 train no.8670  loss = 3.39109 avg_loss = 4.05385\n",
      "epoch no.0 train no.8680  loss = 3.24625 avg_loss = 4.05683\n",
      "epoch no.0 train no.8690  loss = 3.43130 avg_loss = 4.11155\n",
      "epoch no.0 train no.8700  loss = 3.32291 avg_loss = 4.09626\n",
      "epoch no.0 train no.8710  loss = 6.56030 avg_loss = 4.07993\n",
      "epoch no.0 train no.8720  loss = 4.86094 avg_loss = 4.15309\n",
      "epoch no.0 train no.8730  loss = 4.48544 avg_loss = 4.15927\n",
      "epoch no.0 train no.8740  loss = 5.64812 avg_loss = 4.18607\n",
      "epoch no.0 train no.8750  loss = 3.87367 avg_loss = 4.16186\n",
      "epoch no.0 train no.8760  loss = 2.76436 avg_loss = 4.12714\n",
      "epoch no.0 train no.8770  loss = 5.73998 avg_loss = 4.12242\n",
      "epoch no.0 train no.8780  loss = 2.63658 avg_loss = 4.14474\n",
      "epoch no.0 train no.8790  loss = 3.76430 avg_loss = 4.20994\n",
      "epoch no.0 train no.8800  loss = 3.98345 avg_loss = 4.28134\n",
      "epoch no.0 train no.8810  loss = 2.30373 avg_loss = 4.24948\n",
      "epoch no.0 train no.8820  loss = 5.88703 avg_loss = 4.25684\n",
      "epoch no.0 train no.8830  loss = 3.48195 avg_loss = 4.26836\n",
      "epoch no.0 train no.8840  loss = 5.97404 avg_loss = 4.29473\n",
      "epoch no.0 train no.8850  loss = 4.55199 avg_loss = 4.27355\n",
      "epoch no.0 train no.8860  loss = 4.72459 avg_loss = 4.27605\n",
      "epoch no.0 train no.8870  loss = 2.95004 avg_loss = 4.25433\n",
      "epoch no.0 train no.8880  loss = 2.98130 avg_loss = 4.30997\n",
      "epoch no.0 train no.8890  loss = 3.97699 avg_loss = 4.32180\n",
      "epoch no.0 train no.8900  loss = 3.48506 avg_loss = 4.33213\n",
      "epoch no.0 train no.8910  loss = 7.35696 avg_loss = 4.35133\n",
      "epoch no.0 train no.8920  loss = 5.59202 avg_loss = 4.36993\n",
      "epoch no.0 train no.8930  loss = 3.11396 avg_loss = 4.32725\n",
      "epoch no.0 train no.8940  loss = 3.07474 avg_loss = 4.37953\n",
      "epoch no.0 train no.8950  loss = 3.13979 avg_loss = 4.36059\n",
      "epoch no.0 train no.8960  loss = 4.18688 avg_loss = 4.34949\n",
      "epoch no.0 train no.8970  loss = 5.25150 avg_loss = 4.37039\n",
      "epoch no.0 train no.8980  loss = 2.95834 avg_loss = 4.30894\n",
      "epoch no.0 train no.8990  loss = 2.41024 avg_loss = 4.25319\n",
      "epoch no.0 train no.9000  loss = 4.27769 avg_loss = 4.26418\n",
      "3\n",
      "to_tokens: ['▁', '▁팝', '▁o', 'st', '</s>']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.0 train no.9010  loss = 3.09602 avg_loss = 4.16794\n",
      "epoch no.0 train no.9020  loss = 7.70254 avg_loss = 4.19250\n",
      "epoch no.0 train no.9030  loss = 2.39907 avg_loss = 4.10587\n",
      "epoch no.0 train no.9040  loss = 4.86336 avg_loss = 4.13771\n",
      "epoch no.0 train no.9050  loss = 3.87676 avg_loss = 4.06165\n",
      "epoch no.0 train no.9060  loss = 4.59037 avg_loss = 4.05258\n",
      "epoch no.0 train no.9070  loss = 3.15589 avg_loss = 3.97866\n",
      "epoch no.0 train no.9080  loss = 3.37829 avg_loss = 3.99027\n",
      "epoch no.0 train no.9090  loss = 4.14961 avg_loss = 4.01514\n",
      "epoch no.0 train no.9100  loss = 3.86463 avg_loss = 4.01964\n",
      "epoch no.0 train no.9110  loss = 5.14809 avg_loss = 4.06275\n",
      "epoch no.0 train no.9120  loss = 6.21378 avg_loss = 4.13102\n",
      "epoch no.0 train no.9130  loss = 2.51767 avg_loss = 4.08804\n",
      "epoch no.0 train no.9140  loss = 4.03401 avg_loss = 4.09135\n",
      "epoch no.0 train no.9150  loss = 4.77401 avg_loss = 4.08266\n",
      "epoch no.0 train no.9160  loss = 3.64299 avg_loss = 4.06536\n",
      "epoch no.0 train no.9170  loss = 3.82299 avg_loss = 4.03903\n",
      "epoch no.0 train no.9180  loss = 3.54911 avg_loss = 4.05495\n",
      "epoch no.0 train no.9190  loss = 3.43612 avg_loss = 4.03351\n",
      "epoch no.0 train no.9200  loss = 4.92954 avg_loss = 4.05847\n",
      "epoch no.0 train no.9210  loss = 3.62538 avg_loss = 4.08357\n",
      "epoch no.0 train no.9220  loss = 6.18519 avg_loss = 4.13328\n",
      "epoch no.0 train no.9230  loss = 3.13308 avg_loss = 4.10960\n",
      "epoch no.0 train no.9240  loss = 3.09632 avg_loss = 4.04910\n",
      "epoch no.0 train no.9250  loss = 4.15179 avg_loss = 4.08383\n",
      "epoch no.0 train no.9260  loss = 4.07054 avg_loss = 4.11265\n",
      "epoch no.0 train no.9270  loss = 4.40576 avg_loss = 4.09320\n",
      "epoch no.0 train no.9280  loss = 5.42078 avg_loss = 4.11342\n",
      "epoch no.0 train no.9290  loss = 3.49380 avg_loss = 4.13095\n",
      "epoch no.0 train no.9300  loss = 4.18558 avg_loss = 4.19864\n",
      "epoch no.0 train no.9310  loss = 4.70890 avg_loss = 4.19826\n",
      "epoch no.0 train no.9320  loss = 3.07454 avg_loss = 4.19601\n",
      "epoch no.0 train no.9330  loss = 4.45021 avg_loss = 4.22198\n",
      "epoch no.0 train no.9340  loss = 4.04168 avg_loss = 4.26420\n",
      "epoch no.0 train no.9350  loss = 4.00249 avg_loss = 4.30515\n",
      "epoch no.0 train no.9360  loss = 5.08236 avg_loss = 4.27723\n",
      "epoch no.0 train no.9370  loss = 2.78284 avg_loss = 4.23049\n",
      "epoch no.0 train no.9380  loss = 2.44231 avg_loss = 4.20911\n",
      "epoch no.0 train no.9390  loss = 3.66512 avg_loss = 4.23238\n",
      "epoch no.0 train no.9400  loss = 5.41983 avg_loss = 4.25504\n",
      "epoch no.0 train no.9410  loss = 4.83636 avg_loss = 4.23124\n",
      "epoch no.0 train no.9420  loss = 5.37160 avg_loss = 4.18804\n",
      "epoch no.0 train no.9430  loss = 2.46720 avg_loss = 4.11566\n",
      "epoch no.0 train no.9440  loss = 3.43649 avg_loss = 4.15062\n",
      "epoch no.0 train no.9450  loss = 4.09008 avg_loss = 4.09540\n",
      "epoch no.0 train no.9460  loss = 3.77484 avg_loss = 4.04996\n",
      "epoch no.0 train no.9470  loss = 5.19322 avg_loss = 4.04723\n",
      "epoch no.0 train no.9480  loss = 3.97839 avg_loss = 4.02056\n",
      "epoch no.0 train no.9490  loss = 3.23551 avg_loss = 3.96390\n",
      "epoch no.0 train no.9500  loss = 4.76730 avg_loss = 4.04583\n",
      "epoch no.0 train no.9510  loss = 4.08342 avg_loss = 4.09637\n",
      "epoch no.0 train no.9520  loss = 2.93916 avg_loss = 4.02757\n",
      "epoch no.0 train no.9530  loss = 5.96708 avg_loss = 4.00783\n",
      "epoch no.0 train no.9540  loss = 3.81642 avg_loss = 3.95791\n",
      "epoch no.0 train no.9550  loss = 4.04025 avg_loss = 3.94380\n",
      "epoch no.0 train no.9560  loss = 3.91697 avg_loss = 3.92946\n",
      "epoch no.0 train no.9570  loss = 3.36838 avg_loss = 3.93224\n",
      "epoch no.0 train no.9580  loss = 3.51030 avg_loss = 3.97859\n",
      "epoch no.0 train no.9590  loss = 3.64645 avg_loss = 3.91979\n",
      "epoch no.0 train no.9600  loss = 3.84320 avg_loss = 3.98549\n",
      "epoch no.0 train no.9610  loss = 3.77733 avg_loss = 3.93996\n",
      "epoch no.0 train no.9620  loss = 4.78650 avg_loss = 3.92121\n",
      "epoch no.0 train no.9630  loss = 3.94390 avg_loss = 3.94251\n",
      "epoch no.0 train no.9640  loss = 4.71114 avg_loss = 4.04156\n",
      "epoch no.0 train no.9650  loss = 2.50585 avg_loss = 4.06949\n",
      "epoch no.0 train no.9660  loss = 4.86680 avg_loss = 4.11216\n",
      "epoch no.0 train no.9670  loss = 4.70117 avg_loss = 4.11597\n",
      "epoch no.0 train no.9680  loss = 3.13555 avg_loss = 4.07444\n",
      "epoch no.0 train no.9690  loss = 3.60275 avg_loss = 4.09007\n",
      "epoch no.0 train no.9700  loss = 2.82254 avg_loss = 4.10653\n",
      "epoch no.0 train no.9710  loss = 3.92427 avg_loss = 4.14566\n",
      "epoch no.0 train no.9720  loss = 2.97996 avg_loss = 4.13400\n",
      "epoch no.0 train no.9730  loss = 2.57623 avg_loss = 4.10173\n",
      "epoch no.0 train no.9740  loss = 3.57064 avg_loss = 4.06145\n",
      "epoch no.0 train no.9750  loss = 2.69171 avg_loss = 4.05197\n",
      "epoch no.0 train no.9760  loss = 2.50659 avg_loss = 4.03213\n",
      "epoch no.0 train no.9770  loss = 5.32886 avg_loss = 4.02631\n",
      "epoch no.0 train no.9780  loss = 6.66879 avg_loss = 4.05496\n",
      "epoch no.0 train no.9790  loss = 5.23107 avg_loss = 4.07566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.9800  loss = 3.11868 avg_loss = 4.04558\n",
      "epoch no.0 train no.9810  loss = 5.78150 avg_loss = 4.09044\n",
      "epoch no.0 train no.9820  loss = 4.64496 avg_loss = 4.13819\n",
      "epoch no.0 train no.9830  loss = 3.78305 avg_loss = 4.19021\n",
      "epoch no.0 train no.9840  loss = 3.78293 avg_loss = 4.24481\n",
      "epoch no.0 train no.9850  loss = 3.67664 avg_loss = 4.21530\n",
      "epoch no.0 train no.9860  loss = 5.16965 avg_loss = 4.21620\n",
      "epoch no.0 train no.9870  loss = 6.50252 avg_loss = 4.22281\n",
      "epoch no.0 train no.9880  loss = 3.55714 avg_loss = 4.20266\n",
      "epoch no.0 train no.9890  loss = 3.52883 avg_loss = 4.18070\n",
      "epoch no.0 train no.9900  loss = 3.99023 avg_loss = 4.18574\n",
      "epoch no.0 train no.9910  loss = 4.68244 avg_loss = 4.15087\n",
      "epoch no.0 train no.9920  loss = 4.39759 avg_loss = 4.11719\n",
      "epoch no.0 train no.9930  loss = 3.55619 avg_loss = 4.16765\n",
      "epoch no.0 train no.9940  loss = 4.49350 avg_loss = 4.12478\n",
      "epoch no.0 train no.9950  loss = 2.96799 avg_loss = 4.14172\n",
      "epoch no.0 train no.9960  loss = 4.45481 avg_loss = 4.14593\n",
      "epoch no.0 train no.9970  loss = 5.01084 avg_loss = 4.17865\n",
      "epoch no.0 train no.9980  loss = 4.05986 avg_loss = 4.21017\n",
      "epoch no.0 train no.9990  loss = 4.08128 avg_loss = 4.27278\n",
      "epoch no.0 train no.10000  loss = 4.94076 avg_loss = 4.32625\n",
      "2\n",
      "to_tokens: ['▁비', '▁노래', '송', '▁모음']\n",
      "추억의 팝송</s>\n",
      "epoch no.0 train no.10010  loss = 4.09602 avg_loss = 4.31687\n",
      "epoch no.0 train no.10020  loss = 4.75215 avg_loss = 4.36639\n",
      "epoch no.0 train no.10030  loss = 3.11045 avg_loss = 4.34451\n",
      "epoch no.0 train no.10040  loss = 2.53855 avg_loss = 4.35418\n",
      "epoch no.0 train no.10050  loss = 3.42631 avg_loss = 4.34381\n",
      "epoch no.0 train no.10060  loss = 3.50898 avg_loss = 4.31025\n",
      "epoch no.0 train no.10070  loss = 3.35760 avg_loss = 4.30340\n",
      "epoch no.0 train no.10080  loss = 5.58800 avg_loss = 4.32862\n",
      "epoch no.0 train no.10090  loss = 5.16152 avg_loss = 4.27725\n",
      "epoch no.0 train no.10100  loss = 3.10989 avg_loss = 4.26151\n",
      "epoch no.0 train no.10110  loss = 3.90300 avg_loss = 4.24950\n",
      "epoch no.0 train no.10120  loss = 3.32481 avg_loss = 4.27358\n",
      "epoch no.0 train no.10130  loss = 2.32715 avg_loss = 4.26980\n",
      "epoch no.0 train no.10140  loss = 5.79180 avg_loss = 4.23830\n",
      "epoch no.0 train no.10150  loss = 2.79380 avg_loss = 4.16388\n",
      "epoch no.0 train no.10160  loss = 4.19928 avg_loss = 4.16829\n",
      "epoch no.0 train no.10170  loss = 5.53035 avg_loss = 4.14400\n",
      "epoch no.0 train no.10180  loss = 3.36443 avg_loss = 4.08578\n",
      "epoch no.0 train no.10190  loss = 6.86840 avg_loss = 4.06043\n",
      "epoch no.0 train no.10200  loss = 4.28278 avg_loss = 4.10899\n",
      "epoch no.0 train no.10210  loss = 4.39106 avg_loss = 4.13579\n",
      "epoch no.0 train no.10220  loss = 3.28667 avg_loss = 4.07299\n",
      "epoch no.0 train no.10230  loss = 2.46922 avg_loss = 4.08256\n",
      "epoch no.0 train no.10240  loss = 3.59700 avg_loss = 4.06103\n",
      "epoch no.0 train no.10250  loss = 3.68879 avg_loss = 4.13709\n",
      "epoch no.0 train no.10260  loss = 2.48460 avg_loss = 4.08462\n",
      "epoch no.0 train no.10270  loss = 4.06639 avg_loss = 4.03775\n",
      "epoch no.0 train no.10280  loss = 3.34810 avg_loss = 4.02199\n",
      "epoch no.0 train no.10290  loss = 5.37225 avg_loss = 4.03134\n",
      "epoch no.0 train no.10300  loss = 4.03997 avg_loss = 4.04149\n",
      "epoch no.0 train no.10310  loss = 6.85636 avg_loss = 4.14296\n",
      "epoch no.0 train no.10320  loss = 3.91827 avg_loss = 4.12983\n",
      "epoch no.0 train no.10330  loss = 5.66226 avg_loss = 4.15561\n",
      "epoch no.0 train no.10340  loss = 3.20902 avg_loss = 4.13819\n",
      "epoch no.0 train no.10350  loss = 5.36947 avg_loss = 4.17309\n",
      "epoch no.0 train no.10360  loss = 4.09334 avg_loss = 4.18539\n",
      "epoch no.0 train no.10370  loss = 3.72387 avg_loss = 4.26233\n",
      "epoch no.0 train no.10380  loss = 4.09688 avg_loss = 4.30527\n",
      "epoch no.0 train no.10390  loss = 4.76898 avg_loss = 4.32575\n",
      "epoch no.0 train no.10400  loss = 3.55697 avg_loss = 4.34385\n",
      "epoch no.0 train no.10410  loss = 3.57612 avg_loss = 4.30632\n",
      "epoch no.0 train no.10420  loss = 3.96506 avg_loss = 4.27089\n",
      "epoch no.0 train no.10430  loss = 4.99831 avg_loss = 4.30291\n",
      "epoch no.0 train no.10440  loss = 5.87388 avg_loss = 4.27993\n",
      "epoch no.0 train no.10450  loss = 4.00597 avg_loss = 4.26192\n",
      "epoch no.0 train no.10460  loss = 2.34086 avg_loss = 4.21827\n",
      "epoch no.0 train no.10470  loss = 3.67786 avg_loss = 4.18129\n",
      "epoch no.0 train no.10480  loss = 5.10614 avg_loss = 4.15130\n",
      "epoch no.0 train no.10490  loss = 2.73374 avg_loss = 4.14027\n",
      "epoch no.0 train no.10500  loss = 4.31373 avg_loss = 4.15937\n",
      "epoch no.0 train no.10510  loss = 2.67953 avg_loss = 4.09457\n",
      "epoch no.0 train no.10520  loss = 6.33975 avg_loss = 4.16295\n",
      "epoch no.0 train no.10530  loss = 3.22441 avg_loss = 4.14874\n",
      "epoch no.0 train no.10540  loss = 5.05188 avg_loss = 4.12342\n",
      "epoch no.0 train no.10550  loss = 3.55614 avg_loss = 4.06629\n",
      "epoch no.0 train no.10560  loss = 5.58401 avg_loss = 4.04567\n",
      "epoch no.0 train no.10570  loss = 4.26887 avg_loss = 4.05041\n",
      "epoch no.0 train no.10580  loss = 4.67398 avg_loss = 4.03032\n",
      "epoch no.0 train no.10590  loss = 6.59319 avg_loss = 4.12456\n",
      "epoch no.0 train no.10600  loss = 3.25494 avg_loss = 4.15478\n",
      "epoch no.0 train no.10610  loss = 3.93769 avg_loss = 4.13206\n",
      "epoch no.0 train no.10620  loss = 6.71568 avg_loss = 4.11368\n",
      "epoch no.0 train no.10630  loss = 2.81135 avg_loss = 4.04805\n",
      "epoch no.0 train no.10640  loss = 2.93643 avg_loss = 4.08713\n",
      "epoch no.0 train no.10650  loss = 4.68438 avg_loss = 4.10413\n",
      "epoch no.0 train no.10660  loss = 2.77663 avg_loss = 4.06779\n",
      "epoch no.0 train no.10670  loss = 3.16942 avg_loss = 4.08090\n",
      "epoch no.0 train no.10680  loss = 3.38304 avg_loss = 4.07573\n",
      "epoch no.0 train no.10690  loss = 4.60392 avg_loss = 4.05587\n",
      "epoch no.0 train no.10700  loss = 3.74763 avg_loss = 4.08358\n",
      "epoch no.0 train no.10710  loss = 4.52686 avg_loss = 4.13899\n",
      "epoch no.0 train no.10720  loss = 5.26773 avg_loss = 4.18359\n",
      "epoch no.0 train no.10730  loss = 2.29143 avg_loss = 4.17662\n",
      "epoch no.0 train no.10740  loss = 4.57112 avg_loss = 4.20243\n",
      "epoch no.0 train no.10750  loss = 3.51974 avg_loss = 4.24003\n",
      "epoch no.0 train no.10760  loss = 4.64287 avg_loss = 4.21505\n",
      "epoch no.0 train no.10770  loss = 5.03382 avg_loss = 4.19046\n",
      "epoch no.0 train no.10780  loss = 6.54836 avg_loss = 4.20556\n",
      "epoch no.0 train no.10790  loss = 3.43417 avg_loss = 4.18598\n",
      "epoch no.0 train no.10800  loss = 3.41659 avg_loss = 4.10697\n",
      "epoch no.0 train no.10810  loss = 4.63331 avg_loss = 4.10169\n",
      "epoch no.0 train no.10820  loss = 4.74399 avg_loss = 4.08900\n",
      "epoch no.0 train no.10830  loss = 4.73657 avg_loss = 4.05255\n",
      "epoch no.0 train no.10840  loss = 5.54450 avg_loss = 4.08641\n",
      "epoch no.0 train no.10850  loss = 4.72604 avg_loss = 4.07904\n",
      "epoch no.0 train no.10860  loss = 4.16140 avg_loss = 4.12527\n",
      "epoch no.0 train no.10870  loss = 3.07235 avg_loss = 4.12731\n",
      "epoch no.0 train no.10880  loss = 5.24925 avg_loss = 4.15354\n",
      "epoch no.0 train no.10890  loss = 2.57302 avg_loss = 4.12462\n",
      "epoch no.0 train no.10900  loss = 3.35376 avg_loss = 4.07931\n",
      "epoch no.0 train no.10910  loss = 4.34701 avg_loss = 4.06990\n",
      "epoch no.0 train no.10920  loss = 4.72639 avg_loss = 4.12701\n",
      "epoch no.0 train no.10930  loss = 4.39057 avg_loss = 4.12720\n",
      "epoch no.0 train no.10940  loss = 2.71130 avg_loss = 4.04197\n",
      "epoch no.0 train no.10950  loss = 2.49237 avg_loss = 4.01658\n",
      "epoch no.0 train no.10960  loss = 3.72099 avg_loss = 4.04894\n",
      "epoch no.0 train no.10970  loss = 3.22913 avg_loss = 4.00893\n",
      "epoch no.0 train no.10980  loss = 3.45897 avg_loss = 4.05387\n",
      "epoch no.0 train no.10990  loss = 4.45977 avg_loss = 4.03685\n",
      "epoch no.0 train no.11000  loss = 3.14843 avg_loss = 4.02604\n",
      "2\n",
      "to_tokens: ['▁비', '▁아이돌', '송', '</s>']\n",
      "추억의 팝송</s>\n",
      "epoch no.0 train no.11010  loss = 4.01422 avg_loss = 3.99915\n",
      "epoch no.0 train no.11020  loss = 5.04215 avg_loss = 3.97127\n",
      "epoch no.0 train no.11030  loss = 4.32233 avg_loss = 3.94268\n",
      "epoch no.0 train no.11040  loss = 4.16523 avg_loss = 3.94980\n",
      "epoch no.0 train no.11050  loss = 5.97836 avg_loss = 3.97108\n",
      "epoch no.0 train no.11060  loss = 2.96225 avg_loss = 4.01428\n",
      "epoch no.0 train no.11070  loss = 4.55013 avg_loss = 3.99484\n",
      "epoch no.0 train no.11080  loss = 3.28689 avg_loss = 3.96344\n",
      "epoch no.0 train no.11090  loss = 1.99367 avg_loss = 3.92740\n",
      "epoch no.0 train no.11100  loss = 5.82551 avg_loss = 3.98094\n",
      "epoch no.0 train no.11110  loss = 4.73650 avg_loss = 3.99231\n",
      "epoch no.0 train no.11120  loss = 4.20654 avg_loss = 4.02355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.11130  loss = 3.69295 avg_loss = 4.06028\n",
      "epoch no.0 train no.11140  loss = 6.17572 avg_loss = 4.06953\n",
      "epoch no.0 train no.11150  loss = 3.22384 avg_loss = 4.03854\n",
      "epoch no.0 train no.11160  loss = 4.94071 avg_loss = 4.05579\n",
      "epoch no.0 train no.11170  loss = 2.91436 avg_loss = 4.03238\n",
      "epoch no.0 train no.11180  loss = 3.13408 avg_loss = 4.03503\n",
      "epoch no.0 train no.11190  loss = 4.99590 avg_loss = 3.99023\n",
      "epoch no.0 train no.11200  loss = 4.44626 avg_loss = 4.04123\n",
      "epoch no.0 train no.11210  loss = 4.60565 avg_loss = 4.10037\n",
      "epoch no.0 train no.11220  loss = 7.15796 avg_loss = 4.17467\n",
      "epoch no.0 train no.11230  loss = 3.65116 avg_loss = 4.13277\n",
      "epoch no.0 train no.11240  loss = 4.94945 avg_loss = 4.14459\n",
      "epoch no.0 train no.11250  loss = 2.60046 avg_loss = 4.11793\n",
      "epoch no.0 train no.11260  loss = 4.40664 avg_loss = 4.10991\n",
      "epoch no.0 train no.11270  loss = 2.65127 avg_loss = 4.14553\n",
      "epoch no.0 train no.11280  loss = 4.58064 avg_loss = 4.12817\n",
      "epoch no.0 train no.11290  loss = 3.28403 avg_loss = 4.13584\n",
      "epoch no.0 train no.11300  loss = 4.41597 avg_loss = 4.07539\n",
      "epoch no.0 train no.11310  loss = 4.37384 avg_loss = 4.10366\n",
      "epoch no.0 train no.11320  loss = 5.99429 avg_loss = 4.16116\n",
      "epoch no.0 train no.11330  loss = 3.26633 avg_loss = 4.10397\n",
      "epoch no.0 train no.11340  loss = 5.31183 avg_loss = 4.13906\n",
      "epoch no.0 train no.11350  loss = 2.60118 avg_loss = 4.16026\n",
      "epoch no.0 train no.11360  loss = 2.72861 avg_loss = 4.14826\n",
      "epoch no.0 train no.11370  loss = 3.83719 avg_loss = 4.14685\n",
      "epoch no.0 train no.11380  loss = 4.82955 avg_loss = 4.12407\n",
      "epoch no.0 train no.11390  loss = 2.50034 avg_loss = 4.07429\n",
      "epoch no.0 train no.11400  loss = 4.32302 avg_loss = 4.07846\n",
      "epoch no.0 train no.11410  loss = 6.74849 avg_loss = 4.10006\n",
      "epoch no.0 train no.11420  loss = 3.18760 avg_loss = 4.11794\n",
      "epoch no.0 train no.11430  loss = 6.61990 avg_loss = 4.07891\n",
      "epoch no.0 train no.11440  loss = 3.33093 avg_loss = 4.02807\n",
      "epoch no.0 train no.11450  loss = 4.38373 avg_loss = 3.99685\n",
      "epoch no.0 train no.11460  loss = 5.13969 avg_loss = 4.05267\n",
      "epoch no.0 train no.11470  loss = 3.74301 avg_loss = 4.01654\n",
      "epoch no.0 train no.11480  loss = 3.07535 avg_loss = 4.02494\n",
      "epoch no.0 train no.11490  loss = 5.94385 avg_loss = 4.02151\n",
      "epoch no.0 train no.11500  loss = 2.92812 avg_loss = 4.00558\n",
      "epoch no.0 train no.11510  loss = 3.91539 avg_loss = 4.00710\n",
      "epoch no.0 train no.11520  loss = 2.19279 avg_loss = 4.05513\n",
      "epoch no.0 train no.11530  loss = 4.05434 avg_loss = 4.00111\n",
      "epoch no.0 train no.11540  loss = 4.22500 avg_loss = 4.02058\n",
      "epoch no.0 train no.11550  loss = 6.27513 avg_loss = 4.06313\n",
      "epoch no.0 train no.11560  loss = 4.34111 avg_loss = 4.07504\n",
      "epoch no.0 train no.11570  loss = 5.55122 avg_loss = 4.10827\n",
      "epoch no.0 train no.11580  loss = 4.09310 avg_loss = 4.11778\n",
      "epoch no.0 train no.11590  loss = 3.55058 avg_loss = 4.13828\n",
      "epoch no.0 train no.11600  loss = 3.77890 avg_loss = 4.07395\n",
      "epoch no.0 train no.11610  loss = 5.40421 avg_loss = 4.12343\n",
      "epoch no.0 train no.11620  loss = 4.35965 avg_loss = 4.11187\n",
      "epoch no.0 train no.11630  loss = 7.12897 avg_loss = 4.18845\n",
      "epoch no.0 train no.11640  loss = 3.05224 avg_loss = 4.17127\n",
      "epoch no.0 train no.11650  loss = 5.95292 avg_loss = 4.20178\n",
      "epoch no.0 train no.11660  loss = 3.67203 avg_loss = 4.17441\n",
      "epoch no.0 train no.11670  loss = 3.23205 avg_loss = 4.16053\n",
      "epoch no.0 train no.11680  loss = 4.24052 avg_loss = 4.12090\n",
      "epoch no.0 train no.11690  loss = 3.67598 avg_loss = 4.08231\n",
      "epoch no.0 train no.11700  loss = 3.07314 avg_loss = 4.04498\n",
      "epoch no.0 train no.11710  loss = 3.06354 avg_loss = 4.02842\n",
      "epoch no.0 train no.11720  loss = 2.32975 avg_loss = 4.09443\n",
      "epoch no.0 train no.11730  loss = 4.38570 avg_loss = 4.14596\n",
      "epoch no.0 train no.11740  loss = 4.44384 avg_loss = 4.14468\n",
      "epoch no.0 train no.11750  loss = 6.84654 avg_loss = 4.14434\n",
      "epoch no.0 train no.11760  loss = 6.40685 avg_loss = 4.15250\n",
      "epoch no.0 train no.11770  loss = 4.93571 avg_loss = 4.20324\n",
      "epoch no.0 train no.11780  loss = 2.47994 avg_loss = 4.19006\n",
      "epoch no.0 train no.11790  loss = 4.93200 avg_loss = 4.14218\n",
      "epoch no.0 train no.11800  loss = 4.76576 avg_loss = 4.18479\n",
      "epoch no.0 train no.11810  loss = 5.28669 avg_loss = 4.11741\n",
      "epoch no.0 train no.11820  loss = 3.46723 avg_loss = 4.10449\n",
      "epoch no.0 train no.11830  loss = 4.21344 avg_loss = 4.11261\n",
      "epoch no.0 train no.11840  loss = 4.94427 avg_loss = 4.15613\n",
      "epoch no.0 train no.11850  loss = 4.60636 avg_loss = 4.15863\n",
      "epoch no.0 train no.11860  loss = 5.31646 avg_loss = 4.18256\n",
      "epoch no.0 train no.11870  loss = 4.82744 avg_loss = 4.15891\n",
      "epoch no.0 train no.11880  loss = 3.68306 avg_loss = 4.16311\n",
      "epoch no.0 train no.11890  loss = 6.66135 avg_loss = 4.14278\n",
      "epoch no.0 train no.11900  loss = 6.00169 avg_loss = 4.10091\n",
      "epoch no.0 train no.11910  loss = 6.74963 avg_loss = 4.11773\n",
      "epoch no.0 train no.11920  loss = 4.95670 avg_loss = 4.14056\n",
      "epoch no.0 train no.11930  loss = 5.71820 avg_loss = 4.11175\n",
      "epoch no.0 train no.11940  loss = 3.83668 avg_loss = 4.14168\n",
      "epoch no.0 train no.11950  loss = 5.09953 avg_loss = 4.14245\n",
      "epoch no.0 train no.11960  loss = 5.17739 avg_loss = 4.11402\n",
      "epoch no.0 train no.11970  loss = 3.76235 avg_loss = 4.02785\n",
      "epoch no.0 train no.11980  loss = 5.72010 avg_loss = 4.01785\n",
      "epoch no.0 train no.11990  loss = 4.60274 avg_loss = 4.06018\n",
      "epoch no.0 train no.12000  loss = 2.89911 avg_loss = 4.05408\n",
      "2\n",
      "to_tokens: ['▁', '▁90', '들', '</s>']\n",
      "추억의 노래들</s>\n",
      "epoch no.0 train no.12010  loss = 4.17782 avg_loss = 4.13728\n",
      "epoch no.0 train no.12020  loss = 4.60543 avg_loss = 4.14749\n",
      "epoch no.0 train no.12030  loss = 5.40456 avg_loss = 4.12210\n",
      "epoch no.0 train no.12040  loss = 3.53760 avg_loss = 4.05373\n",
      "epoch no.0 train no.12050  loss = 3.95669 avg_loss = 4.07646\n",
      "epoch no.0 train no.12060  loss = 5.03989 avg_loss = 4.03128\n",
      "epoch no.0 train no.12070  loss = 6.51271 avg_loss = 4.04995\n",
      "epoch no.0 train no.12080  loss = 3.39612 avg_loss = 4.02112\n",
      "epoch no.0 train no.12090  loss = 4.83042 avg_loss = 4.04483\n",
      "epoch no.0 train no.12100  loss = 3.67573 avg_loss = 4.05202\n",
      "epoch no.0 train no.12110  loss = 2.70927 avg_loss = 4.04005\n",
      "epoch no.0 train no.12120  loss = 3.76020 avg_loss = 4.03622\n",
      "epoch no.0 train no.12130  loss = 4.76104 avg_loss = 4.02747\n",
      "epoch no.0 train no.12140  loss = 4.94672 avg_loss = 4.04251\n",
      "epoch no.0 train no.12150  loss = 2.55288 avg_loss = 4.01960\n",
      "epoch no.0 train no.12160  loss = 2.13410 avg_loss = 3.99820\n",
      "epoch no.0 train no.12170  loss = 3.74625 avg_loss = 3.94585\n",
      "epoch no.0 train no.12180  loss = 3.72175 avg_loss = 3.93281\n",
      "epoch no.0 train no.12190  loss = 3.36057 avg_loss = 3.96277\n",
      "epoch no.0 train no.12200  loss = 2.91314 avg_loss = 3.96880\n",
      "epoch no.0 train no.12210  loss = 5.32869 avg_loss = 4.00078\n",
      "epoch no.0 train no.12220  loss = 2.89694 avg_loss = 3.96363\n",
      "epoch no.0 train no.12230  loss = 4.13960 avg_loss = 4.05360\n",
      "epoch no.0 train no.12240  loss = 2.70783 avg_loss = 4.10372\n",
      "epoch no.0 train no.12250  loss = 2.68866 avg_loss = 4.08399\n",
      "epoch no.0 train no.12260  loss = 3.17082 avg_loss = 4.07604\n",
      "epoch no.0 train no.12270  loss = 3.92547 avg_loss = 4.04861\n",
      "epoch no.0 train no.12280  loss = 3.51714 avg_loss = 4.09897\n",
      "epoch no.0 train no.12290  loss = 2.90996 avg_loss = 4.07568\n",
      "epoch no.0 train no.12300  loss = 2.66078 avg_loss = 4.05508\n",
      "epoch no.0 train no.12310  loss = 4.26781 avg_loss = 3.99881\n",
      "epoch no.0 train no.12320  loss = 5.74582 avg_loss = 3.99899\n",
      "epoch no.0 train no.12330  loss = 4.51536 avg_loss = 4.02432\n",
      "epoch no.0 train no.12340  loss = 2.59839 avg_loss = 4.01281\n",
      "epoch no.0 train no.12350  loss = 3.64837 avg_loss = 4.02594\n",
      "epoch no.0 train no.12360  loss = 4.52352 avg_loss = 4.01751\n",
      "epoch no.0 train no.12370  loss = 5.42951 avg_loss = 4.02822\n",
      "epoch no.0 train no.12380  loss = 5.68266 avg_loss = 4.07101\n",
      "epoch no.0 train no.12390  loss = 3.79654 avg_loss = 4.07670\n",
      "epoch no.0 train no.12400  loss = 3.03754 avg_loss = 4.06608\n",
      "epoch no.0 train no.12410  loss = 3.26015 avg_loss = 4.05041\n",
      "epoch no.0 train no.12420  loss = 3.86847 avg_loss = 4.08766\n",
      "epoch no.0 train no.12430  loss = 5.44400 avg_loss = 4.13017\n",
      "epoch no.0 train no.12440  loss = 5.25596 avg_loss = 4.10516\n",
      "epoch no.0 train no.12450  loss = 2.74047 avg_loss = 4.06147\n",
      "epoch no.0 train no.12460  loss = 5.52240 avg_loss = 4.05378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.12470  loss = 4.11809 avg_loss = 4.11342\n",
      "epoch no.0 train no.12480  loss = 3.39911 avg_loss = 4.09478\n",
      "epoch no.0 train no.12490  loss = 2.97374 avg_loss = 4.09168\n",
      "epoch no.0 train no.12500  loss = 5.43268 avg_loss = 4.09785\n",
      "epoch no.0 train no.12510  loss = 2.96049 avg_loss = 4.07130\n",
      "epoch no.0 train no.12520  loss = 4.49156 avg_loss = 4.06385\n",
      "epoch no.0 train no.12530  loss = 2.60543 avg_loss = 4.06359\n",
      "epoch no.0 train no.12540  loss = 3.78083 avg_loss = 4.08448\n",
      "epoch no.0 train no.12550  loss = 3.71996 avg_loss = 4.05298\n",
      "epoch no.0 train no.12560  loss = 3.33042 avg_loss = 3.99186\n",
      "epoch no.0 train no.12570  loss = 3.25433 avg_loss = 4.03926\n",
      "epoch no.0 train no.12580  loss = 5.55219 avg_loss = 4.01973\n",
      "epoch no.0 train no.12590  loss = 4.30989 avg_loss = 4.06904\n",
      "epoch no.0 train no.12600  loss = 4.30678 avg_loss = 4.06172\n",
      "epoch no.0 train no.12610  loss = 5.77454 avg_loss = 4.07498\n",
      "epoch no.0 train no.12620  loss = 5.92095 avg_loss = 4.07961\n",
      "epoch no.0 train no.12630  loss = 4.55731 avg_loss = 4.03407\n",
      "epoch no.0 train no.12640  loss = 5.54145 avg_loss = 4.02247\n",
      "epoch no.0 train no.12650  loss = 4.40832 avg_loss = 4.00006\n",
      "epoch no.0 train no.12660  loss = 5.39047 avg_loss = 4.03415\n",
      "epoch no.0 train no.12670  loss = 6.02232 avg_loss = 4.05233\n",
      "epoch no.0 train no.12680  loss = 6.67855 avg_loss = 4.08634\n",
      "epoch no.0 train no.12690  loss = 3.71421 avg_loss = 4.12518\n",
      "epoch no.0 train no.12700  loss = 6.00400 avg_loss = 4.13304\n",
      "epoch no.0 train no.12710  loss = 4.77906 avg_loss = 4.11848\n",
      "epoch no.0 train no.12720  loss = 4.76840 avg_loss = 4.12311\n",
      "epoch no.0 train no.12730  loss = 4.67029 avg_loss = 4.13359\n",
      "epoch no.0 train no.12740  loss = 4.17889 avg_loss = 4.18184\n",
      "epoch no.0 train no.12750  loss = 4.35114 avg_loss = 4.15941\n",
      "epoch no.0 train no.12760  loss = 3.98214 avg_loss = 4.13974\n",
      "epoch no.0 train no.12770  loss = 2.31036 avg_loss = 4.18480\n",
      "epoch no.0 train no.12780  loss = 3.06653 avg_loss = 4.17361\n",
      "epoch no.0 train no.12790  loss = 2.84323 avg_loss = 4.14475\n",
      "epoch no.0 train no.12800  loss = 4.79914 avg_loss = 4.15955\n",
      "epoch no.0 train no.12810  loss = 5.47564 avg_loss = 4.13879\n",
      "epoch no.0 train no.12820  loss = 2.55009 avg_loss = 4.07798\n",
      "epoch no.0 train no.12830  loss = 3.72476 avg_loss = 4.07357\n",
      "epoch no.0 train no.12840  loss = 2.69991 avg_loss = 4.09137\n",
      "epoch no.0 train no.12850  loss = 4.27395 avg_loss = 4.09399\n",
      "epoch no.0 train no.12860  loss = 4.28322 avg_loss = 4.07424\n",
      "epoch no.0 train no.12870  loss = 3.81932 avg_loss = 4.02859\n",
      "epoch no.0 train no.12880  loss = 2.50538 avg_loss = 4.03146\n",
      "epoch no.0 train no.12890  loss = 2.36185 avg_loss = 4.00134\n",
      "epoch no.0 train no.12900  loss = 3.36270 avg_loss = 4.03183\n",
      "epoch no.0 train no.12910  loss = 5.09460 avg_loss = 4.01525\n",
      "epoch no.0 train no.12920  loss = 3.16301 avg_loss = 4.03222\n",
      "epoch no.0 train no.12930  loss = 5.84334 avg_loss = 4.09362\n",
      "epoch no.0 train no.12940  loss = 3.61183 avg_loss = 4.17819\n",
      "epoch no.0 train no.12950  loss = 3.95512 avg_loss = 4.18387\n",
      "epoch no.0 train no.12960  loss = 3.12884 avg_loss = 4.24151\n",
      "epoch no.0 train no.12970  loss = 5.89747 avg_loss = 4.16181\n",
      "epoch no.0 train no.12980  loss = 3.36323 avg_loss = 4.15552\n",
      "epoch no.0 train no.12990  loss = 5.19600 avg_loss = 4.23304\n",
      "epoch no.0 train no.13000  loss = 3.82111 avg_loss = 4.16720\n",
      "2\n",
      "to_tokens: ['▁', '▁명', '모', '</s>']\n",
      "추억의 가요들</s>\n",
      "epoch no.0 train no.13010  loss = 3.45516 avg_loss = 4.11069\n",
      "epoch no.0 train no.13020  loss = 4.52655 avg_loss = 4.11574\n",
      "epoch no.0 train no.13030  loss = 4.09383 avg_loss = 4.12793\n",
      "epoch no.0 train no.13040  loss = 3.11786 avg_loss = 4.15994\n",
      "epoch no.0 train no.13050  loss = 4.26032 avg_loss = 4.15729\n",
      "epoch no.0 train no.13060  loss = 5.16498 avg_loss = 4.17745\n",
      "epoch no.0 train no.13070  loss = 5.39493 avg_loss = 4.21795\n",
      "epoch no.0 train no.13080  loss = 4.27719 avg_loss = 4.24825\n",
      "epoch no.0 train no.13090  loss = 3.21213 avg_loss = 4.24926\n",
      "epoch no.0 train no.13100  loss = 5.75637 avg_loss = 4.20421\n",
      "epoch no.0 train no.13110  loss = 3.81935 avg_loss = 4.14774\n",
      "epoch no.0 train no.13120  loss = 5.13495 avg_loss = 4.10567\n",
      "epoch no.0 train no.13130  loss = 4.95387 avg_loss = 4.09123\n",
      "epoch no.0 train no.13140  loss = 6.73134 avg_loss = 4.13205\n",
      "epoch no.0 train no.13150  loss = 3.87380 avg_loss = 4.09889\n",
      "epoch no.0 train no.13160  loss = 3.44831 avg_loss = 4.04030\n",
      "epoch no.0 train no.13170  loss = 8.29456 avg_loss = 4.07721\n",
      "epoch no.0 train no.13180  loss = 2.45856 avg_loss = 4.12502\n",
      "epoch no.0 train no.13190  loss = 3.83265 avg_loss = 4.20400\n",
      "epoch no.0 train no.13200  loss = 3.73260 avg_loss = 4.20905\n",
      "epoch no.0 train no.13210  loss = 4.14051 avg_loss = 4.18695\n",
      "epoch no.0 train no.13220  loss = 5.66995 avg_loss = 4.22483\n",
      "epoch no.0 train no.13230  loss = 5.99624 avg_loss = 4.18378\n",
      "epoch no.0 train no.13240  loss = 4.01374 avg_loss = 4.19235\n",
      "epoch no.0 train no.13250  loss = 2.96138 avg_loss = 4.15052\n",
      "epoch no.0 train no.13260  loss = 3.57427 avg_loss = 4.11077\n",
      "epoch no.0 train no.13270  loss = 3.43470 avg_loss = 4.15294\n",
      "epoch no.0 train no.13280  loss = 3.09767 avg_loss = 4.15656\n",
      "epoch no.0 train no.13290  loss = 4.05159 avg_loss = 4.27353\n",
      "epoch no.0 train no.13300  loss = 4.17554 avg_loss = 4.26504\n",
      "epoch no.0 train no.13310  loss = 3.35931 avg_loss = 4.18189\n",
      "epoch no.0 train no.13320  loss = 3.58147 avg_loss = 4.16647\n",
      "epoch no.0 train no.13330  loss = 4.45380 avg_loss = 4.10419\n",
      "epoch no.0 train no.13340  loss = 3.57920 avg_loss = 4.12473\n",
      "epoch no.0 train no.13350  loss = 4.16705 avg_loss = 4.08011\n",
      "epoch no.0 train no.13360  loss = 2.87370 avg_loss = 4.04483\n",
      "epoch no.0 train no.13370  loss = 2.92954 avg_loss = 4.07803\n",
      "epoch no.0 train no.13380  loss = 4.29862 avg_loss = 4.11069\n",
      "epoch no.0 train no.13390  loss = 4.12078 avg_loss = 4.09482\n",
      "epoch no.0 train no.13400  loss = 3.72107 avg_loss = 4.10744\n",
      "epoch no.0 train no.13410  loss = 3.33001 avg_loss = 4.16725\n",
      "epoch no.0 train no.13420  loss = 5.69610 avg_loss = 4.14350\n",
      "epoch no.0 train no.13430  loss = 2.83589 avg_loss = 4.13748\n",
      "epoch no.0 train no.13440  loss = 4.00331 avg_loss = 4.12677\n",
      "epoch no.0 train no.13450  loss = 3.89963 avg_loss = 4.08668\n",
      "epoch no.0 train no.13460  loss = 4.48262 avg_loss = 4.08823\n",
      "epoch no.0 train no.13470  loss = 4.14475 avg_loss = 4.05660\n",
      "epoch no.0 train no.13480  loss = 2.40829 avg_loss = 4.08139\n",
      "epoch no.0 train no.13490  loss = 5.48069 avg_loss = 4.03546\n",
      "epoch no.0 train no.13500  loss = 3.03415 avg_loss = 3.98257\n",
      "epoch no.0 train no.13510  loss = 3.20221 avg_loss = 4.07881\n",
      "epoch no.0 train no.13520  loss = 5.23373 avg_loss = 4.11238\n",
      "epoch no.0 train no.13530  loss = 2.82158 avg_loss = 4.11316\n",
      "epoch no.0 train no.13540  loss = 6.71112 avg_loss = 4.09674\n",
      "epoch no.0 train no.13550  loss = 4.47435 avg_loss = 4.11264\n",
      "epoch no.0 train no.13560  loss = 3.02003 avg_loss = 4.09982\n",
      "epoch no.0 train no.13570  loss = 3.61462 avg_loss = 4.09928\n",
      "epoch no.0 train no.13580  loss = 2.07781 avg_loss = 4.04862\n",
      "epoch no.0 train no.13590  loss = 2.87174 avg_loss = 4.10079\n",
      "epoch no.0 train no.13600  loss = 3.62623 avg_loss = 4.06008\n",
      "epoch no.0 train no.13610  loss = 3.71281 avg_loss = 4.07262\n",
      "epoch no.0 train no.13620  loss = 4.90442 avg_loss = 4.07516\n",
      "epoch no.0 train no.13630  loss = 6.61403 avg_loss = 4.11083\n",
      "epoch no.0 train no.13640  loss = 3.64766 avg_loss = 4.15100\n",
      "epoch no.0 train no.13650  loss = 2.88908 avg_loss = 4.15299\n",
      "epoch no.0 train no.13660  loss = 3.43116 avg_loss = 4.18594\n",
      "epoch no.0 train no.13670  loss = 3.08224 avg_loss = 4.18517\n",
      "epoch no.0 train no.13680  loss = 5.34986 avg_loss = 4.25847\n",
      "epoch no.0 train no.13690  loss = 3.84390 avg_loss = 4.31679\n",
      "epoch no.0 train no.13700  loss = 6.05889 avg_loss = 4.32117\n",
      "epoch no.0 train no.13710  loss = 4.27914 avg_loss = 4.35129\n",
      "epoch no.0 train no.13720  loss = 3.64489 avg_loss = 4.31039\n",
      "epoch no.0 train no.13730  loss = 4.43611 avg_loss = 4.32614\n",
      "epoch no.0 train no.13740  loss = 4.79107 avg_loss = 4.28227\n",
      "epoch no.0 train no.13750  loss = 3.58459 avg_loss = 4.24156\n",
      "epoch no.0 train no.13760  loss = 2.31103 avg_loss = 4.17003\n",
      "epoch no.0 train no.13770  loss = 3.62851 avg_loss = 4.13850\n",
      "epoch no.0 train no.13780  loss = 4.00473 avg_loss = 4.12988\n",
      "epoch no.0 train no.13790  loss = 4.17071 avg_loss = 4.14466\n",
      "epoch no.0 train no.13800  loss = 6.41202 avg_loss = 4.20306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.13810  loss = 3.00328 avg_loss = 4.17578\n",
      "epoch no.0 train no.13820  loss = 4.60508 avg_loss = 4.26462\n",
      "epoch no.0 train no.13830  loss = 4.55864 avg_loss = 4.27493\n",
      "epoch no.0 train no.13840  loss = 2.78354 avg_loss = 4.22890\n",
      "epoch no.0 train no.13850  loss = 4.40533 avg_loss = 4.21851\n",
      "epoch no.0 train no.13860  loss = 3.71485 avg_loss = 4.15403\n",
      "epoch no.0 train no.13870  loss = 4.27237 avg_loss = 4.13493\n",
      "epoch no.0 train no.13880  loss = 3.52604 avg_loss = 4.07939\n",
      "epoch no.0 train no.13890  loss = 2.55041 avg_loss = 4.08028\n",
      "epoch no.0 train no.13900  loss = 3.97113 avg_loss = 4.09536\n",
      "epoch no.0 train no.13910  loss = 2.50228 avg_loss = 4.09931\n",
      "epoch no.0 train no.13920  loss = 6.38188 avg_loss = 4.18506\n",
      "epoch no.0 train no.13930  loss = 3.56431 avg_loss = 4.11781\n",
      "epoch no.0 train no.13940  loss = 4.35620 avg_loss = 4.08962\n",
      "epoch no.0 train no.13950  loss = 5.10115 avg_loss = 4.10037\n",
      "epoch no.0 train no.13960  loss = 4.24301 avg_loss = 4.11405\n",
      "epoch no.0 train no.13970  loss = 3.17773 avg_loss = 4.08879\n",
      "epoch no.0 train no.13980  loss = 5.37932 avg_loss = 4.11188\n",
      "epoch no.0 train no.13990  loss = 6.30910 avg_loss = 4.10810\n",
      "epoch no.0 train no.14000  loss = 4.26431 avg_loss = 4.11891\n",
      "4\n",
      "to_tokens: ['▁가을', '▁추억의', '드', '</s>', '곡', '들']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.0 train no.14010  loss = 2.82727 avg_loss = 4.16928\n",
      "epoch no.0 train no.14020  loss = 3.39382 avg_loss = 4.15628\n",
      "epoch no.0 train no.14030  loss = 2.51138 avg_loss = 4.12628\n",
      "epoch no.0 train no.14040  loss = 3.12735 avg_loss = 4.07046\n",
      "epoch no.0 train no.14050  loss = 5.02004 avg_loss = 4.10479\n",
      "epoch no.0 train no.14060  loss = 3.37746 avg_loss = 4.11525\n",
      "epoch no.0 train no.14070  loss = 2.78844 avg_loss = 4.12452\n",
      "epoch no.0 train no.14080  loss = 5.26097 avg_loss = 4.14008\n",
      "epoch no.0 train no.14090  loss = 5.55418 avg_loss = 4.12018\n",
      "epoch no.0 train no.14100  loss = 2.73445 avg_loss = 4.07637\n",
      "epoch no.0 train no.14110  loss = 3.45784 avg_loss = 4.06686\n",
      "epoch no.0 train no.14120  loss = 3.99864 avg_loss = 4.00655\n",
      "epoch no.0 train no.14130  loss = 3.02353 avg_loss = 4.05412\n",
      "epoch no.0 train no.14140  loss = 3.10569 avg_loss = 4.06145\n",
      "epoch no.0 train no.14150  loss = 6.96530 avg_loss = 4.09426\n",
      "epoch no.0 train no.14160  loss = 5.06087 avg_loss = 4.10342\n",
      "epoch no.0 train no.14170  loss = 4.78067 avg_loss = 4.13093\n",
      "epoch no.0 train no.14180  loss = 6.52743 avg_loss = 4.17588\n",
      "epoch no.0 train no.14190  loss = 1.81058 avg_loss = 4.13049\n",
      "epoch no.0 train no.14200  loss = 4.69593 avg_loss = 4.07094\n",
      "epoch no.0 train no.14210  loss = 4.74087 avg_loss = 4.14564\n",
      "epoch no.0 train no.14220  loss = 3.18342 avg_loss = 4.14723\n",
      "epoch no.0 train no.14230  loss = 2.61266 avg_loss = 4.13757\n",
      "epoch no.0 train no.14240  loss = 3.40259 avg_loss = 4.20040\n",
      "epoch no.0 train no.14250  loss = 3.72170 avg_loss = 4.15519\n",
      "epoch no.0 train no.14260  loss = 1.50052 avg_loss = 4.11307\n",
      "epoch no.0 train no.14270  loss = 3.91336 avg_loss = 4.13025\n",
      "epoch no.0 train no.14280  loss = 4.45076 avg_loss = 4.18260\n",
      "epoch no.0 train no.14290  loss = 1.93109 avg_loss = 4.13340\n",
      "epoch no.0 train no.14300  loss = 6.94419 avg_loss = 4.17713\n",
      "epoch no.0 train no.14310  loss = 3.37243 avg_loss = 4.14695\n",
      "epoch no.0 train no.14320  loss = 2.68586 avg_loss = 4.15439\n",
      "epoch no.0 train no.14330  loss = 5.81097 avg_loss = 4.18233\n",
      "epoch no.0 train no.14340  loss = 4.74378 avg_loss = 4.12021\n",
      "epoch no.0 train no.14350  loss = 4.32319 avg_loss = 4.12282\n",
      "epoch no.0 train no.14360  loss = 4.27437 avg_loss = 4.10241\n",
      "epoch no.0 train no.14370  loss = 3.43141 avg_loss = 4.16754\n",
      "epoch no.0 train no.14380  loss = 3.06827 avg_loss = 4.13194\n",
      "epoch no.0 train no.14390  loss = 4.50340 avg_loss = 4.10032\n",
      "epoch no.0 train no.14400  loss = 3.25647 avg_loss = 4.12271\n",
      "epoch no.0 train no.14410  loss = 5.95345 avg_loss = 4.17030\n",
      "epoch no.0 train no.14420  loss = 4.61064 avg_loss = 4.18267\n",
      "epoch no.0 train no.14430  loss = 3.69554 avg_loss = 4.21963\n",
      "epoch no.0 train no.14440  loss = 1.90371 avg_loss = 4.26219\n",
      "epoch no.0 train no.14450  loss = 3.11787 avg_loss = 4.20631\n",
      "epoch no.0 train no.14460  loss = 2.95435 avg_loss = 4.17813\n",
      "epoch no.0 train no.14470  loss = 3.18318 avg_loss = 4.12475\n",
      "epoch no.0 train no.14480  loss = 3.14414 avg_loss = 4.13820\n",
      "epoch no.0 train no.14490  loss = 2.54416 avg_loss = 4.17782\n",
      "epoch no.0 train no.14500  loss = 3.24626 avg_loss = 4.22371\n",
      "epoch no.0 train no.14510  loss = 3.72501 avg_loss = 4.18764\n",
      "epoch no.0 train no.14520  loss = 4.21248 avg_loss = 4.13683\n",
      "epoch no.0 train no.14530  loss = 5.02130 avg_loss = 4.11954\n",
      "epoch no.0 train no.14540  loss = 3.65660 avg_loss = 4.11102\n",
      "epoch no.0 train no.14550  loss = 3.85685 avg_loss = 4.04501\n",
      "epoch no.0 train no.14560  loss = 2.83676 avg_loss = 4.02129\n",
      "epoch no.0 train no.14570  loss = 3.13885 avg_loss = 3.97366\n",
      "epoch no.0 train no.14580  loss = 4.59794 avg_loss = 3.95282\n",
      "epoch no.0 train no.14590  loss = 3.62742 avg_loss = 3.95953\n",
      "epoch no.0 train no.14600  loss = 4.13295 avg_loss = 3.98419\n",
      "epoch no.0 train no.14610  loss = 3.21568 avg_loss = 3.99574\n",
      "epoch no.0 train no.14620  loss = 3.06522 avg_loss = 3.97568\n",
      "epoch no.0 train no.14630  loss = 3.77532 avg_loss = 4.00228\n",
      "epoch no.0 train no.14640  loss = 3.21559 avg_loss = 3.99625\n",
      "epoch no.0 train no.14650  loss = 5.63585 avg_loss = 3.97422\n",
      "epoch no.0 train no.14660  loss = 3.07311 avg_loss = 3.89840\n",
      "epoch no.0 train no.14670  loss = 2.19235 avg_loss = 3.88044\n",
      "epoch no.0 train no.14680  loss = 5.33362 avg_loss = 3.92851\n",
      "epoch no.0 train no.14690  loss = 2.32261 avg_loss = 3.88907\n",
      "epoch no.0 train no.14700  loss = 3.34665 avg_loss = 3.88128\n",
      "epoch no.0 train no.14710  loss = 3.67089 avg_loss = 3.89337\n",
      "epoch no.0 train no.14720  loss = 5.98839 avg_loss = 3.95552\n",
      "epoch no.0 train no.14730  loss = 6.93944 avg_loss = 3.95131\n",
      "epoch no.0 train no.14740  loss = 2.86003 avg_loss = 3.99281\n",
      "epoch no.0 train no.14750  loss = 5.26808 avg_loss = 4.03488\n",
      "epoch no.0 train no.14760  loss = 2.70880 avg_loss = 4.02821\n",
      "epoch no.0 train no.14770  loss = 5.38590 avg_loss = 4.05496\n",
      "epoch no.0 train no.14780  loss = 4.88913 avg_loss = 4.04259\n",
      "epoch no.0 train no.14790  loss = 3.90322 avg_loss = 4.02515\n",
      "epoch no.0 train no.14800  loss = 2.84465 avg_loss = 4.02424\n",
      "epoch no.0 train no.14810  loss = 3.00559 avg_loss = 4.00938\n",
      "epoch no.0 train no.14820  loss = 4.01645 avg_loss = 4.03099\n",
      "epoch no.0 train no.14830  loss = 2.46627 avg_loss = 3.99584\n",
      "epoch no.0 train no.14840  loss = 3.88550 avg_loss = 3.95945\n",
      "epoch no.0 train no.14850  loss = 4.83101 avg_loss = 3.92597\n",
      "epoch no.0 train no.14860  loss = 2.87249 avg_loss = 3.97157\n",
      "epoch no.0 train no.14870  loss = 3.29555 avg_loss = 3.94453\n",
      "epoch no.0 train no.14880  loss = 3.82808 avg_loss = 3.92912\n",
      "epoch no.0 train no.14890  loss = 3.38426 avg_loss = 3.91422\n",
      "epoch no.0 train no.14900  loss = 5.82530 avg_loss = 3.91201\n",
      "epoch no.0 train no.14910  loss = 3.86752 avg_loss = 3.92371\n",
      "epoch no.0 train no.14920  loss = 2.46966 avg_loss = 3.88867\n",
      "epoch no.0 train no.14930  loss = 4.83106 avg_loss = 3.88255\n",
      "epoch no.0 train no.14940  loss = 2.73728 avg_loss = 3.92907\n",
      "epoch no.0 train no.14950  loss = 2.77381 avg_loss = 3.96269\n",
      "epoch no.0 train no.14960  loss = 5.96013 avg_loss = 4.02993\n",
      "epoch no.0 train no.14970  loss = 4.99235 avg_loss = 3.98931\n",
      "epoch no.0 train no.14980  loss = 3.79874 avg_loss = 4.00374\n",
      "epoch no.0 train no.14990  loss = 3.76571 avg_loss = 4.01299\n",
      "epoch no.0 train no.15000  loss = 3.88784 avg_loss = 4.01113\n",
      "3\n",
      "to_tokens: ['▁', '▁90', '트', '▁명', '</s>']\n",
      "추억의 트로트 모음</s>\n",
      "epoch no.0 train no.15010  loss = 4.50544 avg_loss = 4.03348\n",
      "epoch no.0 train no.15020  loss = 4.18910 avg_loss = 4.08544\n",
      "epoch no.0 train no.15030  loss = 4.31587 avg_loss = 4.08888\n",
      "epoch no.0 train no.15040  loss = 6.72366 avg_loss = 4.13919\n",
      "epoch no.0 train no.15050  loss = 2.70662 avg_loss = 4.16492\n",
      "epoch no.0 train no.15060  loss = 3.09365 avg_loss = 4.16582\n",
      "epoch no.0 train no.15070  loss = 1.93851 avg_loss = 4.10507\n",
      "epoch no.0 train no.15080  loss = 4.97933 avg_loss = 4.10893\n",
      "epoch no.0 train no.15090  loss = 3.01991 avg_loss = 4.11475\n",
      "epoch no.0 train no.15100  loss = 4.01042 avg_loss = 4.11253\n",
      "epoch no.0 train no.15110  loss = 4.27748 avg_loss = 4.10518\n",
      "epoch no.0 train no.15120  loss = 3.74218 avg_loss = 4.08182\n",
      "epoch no.0 train no.15130  loss = 5.49196 avg_loss = 4.14123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.15140  loss = 6.67110 avg_loss = 4.16173\n",
      "epoch no.0 train no.15150  loss = 4.38154 avg_loss = 4.12300\n",
      "epoch no.0 train no.15160  loss = 2.14182 avg_loss = 4.10012\n",
      "epoch no.0 train no.15170  loss = 3.83727 avg_loss = 4.08485\n",
      "epoch no.0 train no.15180  loss = 4.12303 avg_loss = 4.08981\n",
      "epoch no.0 train no.15190  loss = 3.91854 avg_loss = 4.05081\n",
      "epoch no.0 train no.15200  loss = 4.09490 avg_loss = 4.04741\n",
      "epoch no.0 train no.15210  loss = 4.41959 avg_loss = 3.95185\n",
      "epoch no.0 train no.15220  loss = 5.06918 avg_loss = 4.01130\n",
      "epoch no.0 train no.15230  loss = 5.29477 avg_loss = 4.00189\n",
      "epoch no.0 train no.15240  loss = 4.40421 avg_loss = 3.99697\n",
      "epoch no.0 train no.15250  loss = 6.00500 avg_loss = 3.96836\n",
      "epoch no.0 train no.15260  loss = 6.21207 avg_loss = 3.98368\n",
      "epoch no.0 train no.15270  loss = 3.52193 avg_loss = 3.92446\n",
      "epoch no.0 train no.15280  loss = 5.64321 avg_loss = 3.96690\n",
      "epoch no.0 train no.15290  loss = 3.03695 avg_loss = 3.96814\n",
      "epoch no.0 train no.15300  loss = 3.10327 avg_loss = 3.99346\n",
      "epoch no.0 train no.15310  loss = 5.16505 avg_loss = 4.02869\n",
      "epoch no.0 train no.15320  loss = 3.66445 avg_loss = 4.00531\n",
      "epoch no.0 train no.15330  loss = 6.19669 avg_loss = 4.07133\n",
      "epoch no.0 train no.15340  loss = 3.40468 avg_loss = 4.09024\n",
      "epoch no.0 train no.15350  loss = 4.61677 avg_loss = 4.07422\n",
      "epoch no.0 train no.15360  loss = 4.29546 avg_loss = 4.09198\n",
      "epoch no.0 train no.15370  loss = 4.88911 avg_loss = 4.07803\n",
      "epoch no.0 train no.15380  loss = 3.50002 avg_loss = 4.05385\n",
      "epoch no.0 train no.15390  loss = 5.69332 avg_loss = 4.12085\n",
      "epoch no.0 train no.15400  loss = 4.10320 avg_loss = 4.12805\n",
      "epoch no.0 train no.15410  loss = 2.46560 avg_loss = 4.04759\n",
      "epoch no.0 train no.15420  loss = 6.59985 avg_loss = 4.03927\n",
      "epoch no.0 train no.15430  loss = 2.72997 avg_loss = 4.03856\n",
      "epoch no.0 train no.15440  loss = 5.31828 avg_loss = 4.03914\n",
      "epoch no.0 train no.15450  loss = 1.76717 avg_loss = 4.02496\n",
      "epoch no.0 train no.15460  loss = 2.79441 avg_loss = 4.08057\n",
      "epoch no.0 train no.15470  loss = 4.21986 avg_loss = 4.10601\n",
      "epoch no.0 train no.15480  loss = 4.57041 avg_loss = 4.09388\n",
      "epoch no.0 train no.15490  loss = 2.99518 avg_loss = 4.01727\n",
      "epoch no.0 train no.15500  loss = 5.48629 avg_loss = 4.02253\n",
      "epoch no.0 train no.15510  loss = 3.95491 avg_loss = 3.97868\n",
      "epoch no.0 train no.15520  loss = 4.53535 avg_loss = 3.99820\n",
      "epoch no.0 train no.15530  loss = 4.02172 avg_loss = 3.98304\n",
      "epoch no.0 train no.15540  loss = 5.42498 avg_loss = 4.04095\n",
      "epoch no.0 train no.15550  loss = 3.84898 avg_loss = 3.98013\n",
      "epoch no.0 train no.15560  loss = 3.27906 avg_loss = 3.97509\n",
      "epoch no.0 train no.15570  loss = 3.52283 avg_loss = 3.99507\n",
      "epoch no.0 train no.15580  loss = 4.83594 avg_loss = 3.95676\n",
      "epoch no.0 train no.15590  loss = 2.09193 avg_loss = 3.90922\n",
      "epoch no.0 train no.15600  loss = 3.56813 avg_loss = 3.99443\n",
      "epoch no.0 train no.15610  loss = 4.09137 avg_loss = 3.99163\n",
      "epoch no.0 train no.15620  loss = 3.15514 avg_loss = 3.96914\n",
      "epoch no.0 train no.15630  loss = 5.48584 avg_loss = 3.96759\n",
      "epoch no.0 train no.15640  loss = 2.97634 avg_loss = 4.03469\n",
      "epoch no.0 train no.15650  loss = 4.41446 avg_loss = 4.03867\n",
      "epoch no.0 train no.15660  loss = 4.28452 avg_loss = 4.02191\n",
      "epoch no.0 train no.15670  loss = 3.32659 avg_loss = 4.07851\n",
      "epoch no.0 train no.15680  loss = 3.22176 avg_loss = 4.05677\n",
      "epoch no.0 train no.15690  loss = 2.99285 avg_loss = 4.04238\n",
      "epoch no.0 train no.15700  loss = 2.80905 avg_loss = 3.99412\n",
      "epoch no.0 train no.15710  loss = 4.77156 avg_loss = 3.94847\n",
      "epoch no.0 train no.15720  loss = 2.10337 avg_loss = 3.95406\n",
      "epoch no.0 train no.15730  loss = 4.51491 avg_loss = 3.90949\n",
      "epoch no.0 train no.15740  loss = 6.11469 avg_loss = 3.95406\n",
      "epoch no.0 train no.15750  loss = 3.81157 avg_loss = 3.93763\n",
      "epoch no.0 train no.15760  loss = 4.55984 avg_loss = 3.91918\n",
      "epoch no.0 train no.15770  loss = 4.72240 avg_loss = 3.94330\n",
      "epoch no.0 train no.15780  loss = 4.37556 avg_loss = 3.92029\n",
      "epoch no.0 train no.15790  loss = 4.69588 avg_loss = 4.02029\n",
      "epoch no.0 train no.15800  loss = 2.47840 avg_loss = 4.02065\n",
      "epoch no.0 train no.15810  loss = 2.95162 avg_loss = 4.01728\n",
      "epoch no.0 train no.15820  loss = 4.76342 avg_loss = 4.03011\n",
      "epoch no.0 train no.15830  loss = 3.40908 avg_loss = 4.02018\n",
      "epoch no.0 train no.15840  loss = 3.74988 avg_loss = 3.96975\n",
      "epoch no.0 train no.15850  loss = 5.19681 avg_loss = 4.00636\n",
      "epoch no.0 train no.15860  loss = 3.48606 avg_loss = 3.99297\n",
      "epoch no.0 train no.15870  loss = 3.46197 avg_loss = 3.95179\n",
      "epoch no.0 train no.15880  loss = 4.27654 avg_loss = 3.91002\n",
      "epoch no.0 train no.15890  loss = 2.98362 avg_loss = 3.95731\n",
      "epoch no.0 train no.15900  loss = 2.90286 avg_loss = 3.97522\n",
      "epoch no.0 train no.15910  loss = 4.16487 avg_loss = 3.99530\n",
      "epoch no.0 train no.15920  loss = 4.49027 avg_loss = 4.05225\n",
      "epoch no.0 train no.15930  loss = 5.39259 avg_loss = 4.05268\n",
      "epoch no.0 train no.15940  loss = 5.06993 avg_loss = 4.04728\n",
      "epoch no.0 train no.15950  loss = 3.79521 avg_loss = 4.14919\n",
      "epoch no.0 train no.15960  loss = 3.44748 avg_loss = 4.11226\n",
      "epoch no.0 train no.15970  loss = 4.53174 avg_loss = 4.09757\n",
      "epoch no.0 train no.15980  loss = 3.83585 avg_loss = 4.07054\n",
      "epoch no.0 train no.15990  loss = 4.41072 avg_loss = 4.11958\n",
      "epoch no.0 train no.16000  loss = 4.97916 avg_loss = 4.09969\n",
      "3\n",
      "to_tokens: ['▁가을', '▁명', '들', '음', '</s>']\n",
      "추억의 노래모음</s>\n",
      "epoch no.0 train no.16010  loss = 3.70112 avg_loss = 4.12987\n",
      "epoch no.0 train no.16020  loss = 4.83619 avg_loss = 4.10983\n",
      "epoch no.0 train no.16030  loss = 2.63225 avg_loss = 4.05889\n",
      "epoch no.0 train no.16040  loss = 4.67387 avg_loss = 4.06804\n",
      "epoch no.0 train no.16050  loss = 3.32732 avg_loss = 4.01356\n",
      "epoch no.0 train no.16060  loss = 4.63112 avg_loss = 3.91604\n",
      "epoch no.0 train no.16070  loss = 2.78330 avg_loss = 3.88968\n",
      "epoch no.0 train no.16080  loss = 5.23907 avg_loss = 3.90928\n",
      "epoch no.0 train no.16090  loss = 2.44150 avg_loss = 3.93660\n",
      "epoch no.0 train no.16100  loss = 5.86562 avg_loss = 3.99587\n",
      "epoch no.0 train no.16110  loss = 3.53973 avg_loss = 3.95361\n",
      "epoch no.0 train no.16120  loss = 2.93955 avg_loss = 3.98508\n",
      "epoch no.0 train no.16130  loss = 4.46732 avg_loss = 3.98358\n",
      "epoch no.0 train no.16140  loss = 4.69705 avg_loss = 3.98225\n",
      "epoch no.0 train no.16150  loss = 3.07444 avg_loss = 3.96765\n",
      "epoch no.0 train no.16160  loss = 2.89587 avg_loss = 4.02202\n",
      "epoch no.0 train no.16170  loss = 2.95244 avg_loss = 3.98724\n",
      "epoch no.0 train no.16180  loss = 4.56573 avg_loss = 4.05761\n",
      "epoch no.0 train no.16190  loss = 2.84349 avg_loss = 4.07946\n",
      "epoch no.0 train no.16200  loss = 5.04841 avg_loss = 4.14799\n",
      "epoch no.0 train no.16210  loss = 5.38796 avg_loss = 4.14008\n",
      "epoch no.0 train no.16220  loss = 4.33603 avg_loss = 4.12494\n",
      "epoch no.0 train no.16230  loss = 3.54336 avg_loss = 4.11483\n",
      "epoch no.0 train no.16240  loss = 3.21986 avg_loss = 4.17549\n",
      "epoch no.0 train no.16250  loss = 3.74040 avg_loss = 4.16318\n",
      "epoch no.0 train no.16260  loss = 3.04230 avg_loss = 4.06894\n",
      "epoch no.0 train no.16270  loss = 4.23679 avg_loss = 4.01263\n",
      "epoch no.0 train no.16280  loss = 4.10581 avg_loss = 3.99321\n",
      "epoch no.0 train no.16290  loss = 3.39849 avg_loss = 3.98900\n",
      "epoch no.0 train no.16300  loss = 5.11405 avg_loss = 4.00471\n",
      "epoch no.0 train no.16310  loss = 3.08236 avg_loss = 4.06860\n",
      "epoch no.0 train no.16320  loss = 4.05610 avg_loss = 4.01876\n",
      "epoch no.0 train no.16330  loss = 3.74904 avg_loss = 3.99163\n",
      "epoch no.0 train no.16340  loss = 3.52094 avg_loss = 4.04828\n",
      "epoch no.0 train no.16350  loss = 5.71969 avg_loss = 4.12652\n",
      "epoch no.0 train no.16360  loss = 4.37769 avg_loss = 4.17959\n",
      "epoch no.0 train no.16370  loss = 3.43631 avg_loss = 4.19373\n",
      "epoch no.0 train no.16380  loss = 4.64065 avg_loss = 4.15528\n",
      "epoch no.0 train no.16390  loss = 3.98596 avg_loss = 4.16676\n",
      "epoch no.0 train no.16400  loss = 3.41593 avg_loss = 4.15453\n",
      "epoch no.0 train no.16410  loss = 6.47609 avg_loss = 4.22198\n",
      "epoch no.0 train no.16420  loss = 4.25067 avg_loss = 4.16441\n",
      "epoch no.0 train no.16430  loss = 3.53877 avg_loss = 4.13330\n",
      "epoch no.0 train no.16440  loss = 4.86010 avg_loss = 4.13858\n",
      "epoch no.0 train no.16450  loss = 3.08925 avg_loss = 4.12718\n",
      "epoch no.0 train no.16460  loss = 5.28119 avg_loss = 4.11035\n",
      "epoch no.0 train no.16470  loss = 4.03291 avg_loss = 4.12297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.16480  loss = 4.89071 avg_loss = 4.09665\n",
      "epoch no.0 train no.16490  loss = 2.96842 avg_loss = 4.07570\n",
      "epoch no.0 train no.16500  loss = 4.67047 avg_loss = 4.02768\n",
      "epoch no.0 train no.16510  loss = 2.15800 avg_loss = 4.01053\n",
      "epoch no.0 train no.16520  loss = 3.45186 avg_loss = 3.95746\n",
      "epoch no.0 train no.16530  loss = 4.56914 avg_loss = 3.96717\n",
      "epoch no.0 train no.16540  loss = 5.33152 avg_loss = 3.98800\n",
      "epoch no.0 train no.16550  loss = 1.91782 avg_loss = 3.96581\n",
      "epoch no.0 train no.16560  loss = 4.09519 avg_loss = 3.92102\n",
      "epoch no.0 train no.16570  loss = 6.16512 avg_loss = 3.96530\n",
      "epoch no.0 train no.16580  loss = 3.38450 avg_loss = 3.97051\n",
      "epoch no.0 train no.16590  loss = 3.26584 avg_loss = 3.96443\n",
      "epoch no.0 train no.16600  loss = 4.97886 avg_loss = 3.98585\n",
      "epoch no.0 train no.16610  loss = 2.83655 avg_loss = 3.96096\n",
      "epoch no.0 train no.16620  loss = 5.72724 avg_loss = 3.94200\n",
      "epoch no.0 train no.16630  loss = 2.59801 avg_loss = 3.89047\n",
      "epoch no.0 train no.16640  loss = 2.66184 avg_loss = 3.92204\n",
      "epoch no.0 train no.16650  loss = 3.19566 avg_loss = 3.96326\n",
      "epoch no.0 train no.16660  loss = 2.75030 avg_loss = 3.90770\n",
      "epoch no.0 train no.16670  loss = 5.03349 avg_loss = 3.92540\n",
      "epoch no.0 train no.16680  loss = 3.01250 avg_loss = 3.92162\n",
      "epoch no.0 train no.16690  loss = 4.29311 avg_loss = 3.88223\n",
      "epoch no.0 train no.16700  loss = 3.47383 avg_loss = 3.94451\n",
      "epoch no.0 train no.16710  loss = 3.45804 avg_loss = 3.96041\n",
      "epoch no.0 train no.16720  loss = 5.55456 avg_loss = 4.02652\n",
      "epoch no.0 train no.16730  loss = 4.93462 avg_loss = 4.05447\n",
      "epoch no.0 train no.16740  loss = 3.07435 avg_loss = 4.07858\n",
      "epoch no.0 train no.16750  loss = 5.31373 avg_loss = 4.12221\n",
      "epoch no.0 train no.16760  loss = 5.15815 avg_loss = 4.12269\n",
      "epoch no.0 train no.16770  loss = 3.31077 avg_loss = 4.12214\n",
      "epoch no.0 train no.16780  loss = 2.28725 avg_loss = 4.11537\n",
      "epoch no.0 train no.16790  loss = 5.03648 avg_loss = 4.15830\n",
      "epoch no.0 train no.16800  loss = 4.16352 avg_loss = 4.16062\n",
      "epoch no.0 train no.16810  loss = 3.88377 avg_loss = 4.20970\n",
      "epoch no.0 train no.16820  loss = 5.22867 avg_loss = 4.19365\n",
      "epoch no.0 train no.16830  loss = 3.56087 avg_loss = 4.20447\n",
      "epoch no.0 train no.16840  loss = 4.65724 avg_loss = 4.26753\n",
      "epoch no.0 train no.16850  loss = 2.47684 avg_loss = 4.24752\n",
      "epoch no.0 train no.16860  loss = 3.21830 avg_loss = 4.25555\n",
      "epoch no.0 train no.16870  loss = 6.19303 avg_loss = 4.23120\n",
      "epoch no.0 train no.16880  loss = 3.24701 avg_loss = 4.18064\n",
      "epoch no.0 train no.16890  loss = 5.28994 avg_loss = 4.19897\n",
      "epoch no.0 train no.16900  loss = 5.79727 avg_loss = 4.26759\n",
      "epoch no.0 train no.16910  loss = 5.05011 avg_loss = 4.24514\n",
      "epoch no.0 train no.16920  loss = 4.45846 avg_loss = 4.20357\n",
      "epoch no.0 train no.16930  loss = 3.33161 avg_loss = 4.20887\n",
      "epoch no.0 train no.16940  loss = 2.17370 avg_loss = 4.20155\n",
      "epoch no.0 train no.16950  loss = 3.87861 avg_loss = 4.20753\n",
      "epoch no.0 train no.16960  loss = 4.54892 avg_loss = 4.25576\n",
      "epoch no.0 train no.16970  loss = 3.26934 avg_loss = 4.18555\n",
      "epoch no.0 train no.16980  loss = 2.98439 avg_loss = 4.08558\n",
      "epoch no.0 train no.16990  loss = 2.75108 avg_loss = 4.07027\n",
      "epoch no.0 train no.17000  loss = 4.04373 avg_loss = 4.05550\n",
      "3\n",
      "to_tokens: ['▁', '▁팝', '송', '▁모음', '</s>']\n",
      "추억의 팝송 베스트</s>\n",
      "epoch no.0 train no.17010  loss = 2.46788 avg_loss = 4.09258\n",
      "epoch no.0 train no.17020  loss = 2.69915 avg_loss = 4.05794\n",
      "epoch no.0 train no.17030  loss = 5.03293 avg_loss = 3.99545\n",
      "epoch no.0 train no.17040  loss = 4.42788 avg_loss = 4.00337\n",
      "epoch no.0 train no.17050  loss = 3.91069 avg_loss = 4.02652\n",
      "epoch no.0 train no.17060  loss = 3.53162 avg_loss = 4.03827\n",
      "epoch no.0 train no.17070  loss = 2.36338 avg_loss = 4.03602\n",
      "epoch no.0 train no.17080  loss = 2.59063 avg_loss = 4.07630\n",
      "epoch no.0 train no.17090  loss = 4.04886 avg_loss = 4.04750\n",
      "epoch no.0 train no.17100  loss = 4.94357 avg_loss = 4.08158\n",
      "epoch no.0 train no.17110  loss = 6.11874 avg_loss = 4.08003\n",
      "epoch no.0 train no.17120  loss = 2.64756 avg_loss = 4.07722\n",
      "epoch no.0 train no.17130  loss = 3.85347 avg_loss = 4.07112\n",
      "epoch no.0 train no.17140  loss = 4.39562 avg_loss = 4.04345\n",
      "epoch no.0 train no.17150  loss = 3.72421 avg_loss = 4.06943\n",
      "epoch no.0 train no.17160  loss = 2.82059 avg_loss = 4.06349\n",
      "epoch no.0 train no.17170  loss = 1.64738 avg_loss = 4.04481\n",
      "epoch no.0 train no.17180  loss = 4.94292 avg_loss = 4.06308\n",
      "epoch no.0 train no.17190  loss = 2.79142 avg_loss = 4.11929\n",
      "epoch no.0 train no.17200  loss = 6.53962 avg_loss = 4.14561\n",
      "epoch no.0 train no.17210  loss = 4.76932 avg_loss = 4.13277\n",
      "epoch no.0 train no.17220  loss = 2.48394 avg_loss = 4.18640\n",
      "epoch no.0 train no.17230  loss = 3.54388 avg_loss = 4.16062\n",
      "epoch no.0 train no.17240  loss = 4.37923 avg_loss = 4.16250\n",
      "epoch no.0 train no.17250  loss = 3.27818 avg_loss = 4.16774\n",
      "epoch no.0 train no.17260  loss = 2.85253 avg_loss = 4.12500\n",
      "epoch no.0 train no.17270  loss = 4.29388 avg_loss = 4.11477\n",
      "epoch no.0 train no.17280  loss = 3.73457 avg_loss = 4.09472\n",
      "epoch no.0 train no.17290  loss = 2.65098 avg_loss = 4.09177\n",
      "epoch no.0 train no.17300  loss = 4.92705 avg_loss = 4.12873\n",
      "epoch no.0 train no.17310  loss = 3.88975 avg_loss = 4.07508\n",
      "epoch no.0 train no.17320  loss = 3.44811 avg_loss = 4.11603\n",
      "epoch no.0 train no.17330  loss = 2.91985 avg_loss = 4.09587\n",
      "epoch no.0 train no.17340  loss = 5.03134 avg_loss = 4.11320\n",
      "epoch no.0 train no.17350  loss = 3.69664 avg_loss = 4.11277\n",
      "epoch no.0 train no.17360  loss = 2.33051 avg_loss = 4.05664\n",
      "epoch no.0 train no.17370  loss = 3.30250 avg_loss = 4.05104\n",
      "epoch no.0 train no.17380  loss = 4.40878 avg_loss = 4.08648\n",
      "epoch no.0 train no.17390  loss = 4.10216 avg_loss = 4.06565\n",
      "epoch no.0 train no.17400  loss = 3.40754 avg_loss = 4.07830\n",
      "epoch no.0 train no.17410  loss = 4.26884 avg_loss = 4.08653\n",
      "epoch no.0 train no.17420  loss = 4.23785 avg_loss = 4.04827\n",
      "epoch no.0 train no.17430  loss = 4.07155 avg_loss = 4.00335\n",
      "epoch no.0 train no.17440  loss = 3.68915 avg_loss = 4.00687\n",
      "epoch no.0 train no.17450  loss = 2.96311 avg_loss = 3.99835\n",
      "epoch no.0 train no.17460  loss = 5.83733 avg_loss = 4.08474\n",
      "epoch no.0 train no.17470  loss = 3.72905 avg_loss = 4.05132\n",
      "epoch no.0 train no.17480  loss = 3.21394 avg_loss = 4.02704\n",
      "epoch no.0 train no.17490  loss = 4.07772 avg_loss = 4.06358\n",
      "epoch no.0 train no.17500  loss = 6.25887 avg_loss = 4.06868\n",
      "epoch no.0 train no.17510  loss = 3.23246 avg_loss = 4.06613\n",
      "epoch no.0 train no.17520  loss = 3.80216 avg_loss = 4.04046\n",
      "epoch no.0 train no.17530  loss = 4.47589 avg_loss = 4.07795\n",
      "epoch no.0 train no.17540  loss = 3.64757 avg_loss = 4.12978\n",
      "epoch no.0 train no.17550  loss = 3.19806 avg_loss = 4.08218\n",
      "epoch no.0 train no.17560  loss = 2.53035 avg_loss = 4.06569\n",
      "epoch no.0 train no.17570  loss = 3.63393 avg_loss = 4.09477\n",
      "epoch no.0 train no.17580  loss = 5.86907 avg_loss = 4.12155\n",
      "epoch no.0 train no.17590  loss = 1.96481 avg_loss = 4.12072\n",
      "epoch no.0 train no.17600  loss = 4.13145 avg_loss = 4.12857\n",
      "epoch no.0 train no.17610  loss = 2.10683 avg_loss = 4.11002\n",
      "epoch no.0 train no.17620  loss = 3.52232 avg_loss = 4.09521\n",
      "epoch no.0 train no.17630  loss = 4.29860 avg_loss = 4.08227\n",
      "epoch no.0 train no.17640  loss = 5.29186 avg_loss = 4.07571\n",
      "epoch no.0 train no.17650  loss = 3.59780 avg_loss = 4.04048\n",
      "epoch no.0 train no.17660  loss = 3.73072 avg_loss = 4.04709\n",
      "epoch no.0 train no.17670  loss = 3.32563 avg_loss = 4.03156\n",
      "epoch no.0 train no.17680  loss = 3.78126 avg_loss = 3.97144\n",
      "epoch no.0 train no.17690  loss = 5.28730 avg_loss = 4.01895\n",
      "epoch no.0 train no.17700  loss = 4.45429 avg_loss = 4.01655\n",
      "epoch no.0 train no.17710  loss = 3.55686 avg_loss = 3.99215\n",
      "epoch no.0 train no.17720  loss = 2.20049 avg_loss = 4.02482\n",
      "epoch no.0 train no.17730  loss = 6.19567 avg_loss = 4.06387\n",
      "epoch no.0 train no.17740  loss = 1.90769 avg_loss = 4.06827\n",
      "epoch no.0 train no.17750  loss = 5.60074 avg_loss = 4.09259\n",
      "epoch no.0 train no.17760  loss = 3.41002 avg_loss = 4.11016\n",
      "epoch no.0 train no.17770  loss = 4.88067 avg_loss = 4.07301\n",
      "epoch no.0 train no.17780  loss = 3.11405 avg_loss = 4.04266\n",
      "epoch no.0 train no.17790  loss = 3.77040 avg_loss = 4.02174\n",
      "epoch no.0 train no.17800  loss = 3.65933 avg_loss = 4.01019\n",
      "epoch no.0 train no.17810  loss = 2.06143 avg_loss = 4.00968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.17820  loss = 3.73096 avg_loss = 3.99839\n",
      "epoch no.0 train no.17830  loss = 3.51180 avg_loss = 3.98256\n",
      "epoch no.0 train no.17840  loss = 5.00879 avg_loss = 3.99612\n",
      "epoch no.0 train no.17850  loss = 5.65318 avg_loss = 4.01087\n",
      "epoch no.0 train no.17860  loss = 6.79191 avg_loss = 4.00218\n",
      "epoch no.0 train no.17870  loss = 2.46586 avg_loss = 4.03775\n",
      "epoch no.0 train no.17880  loss = 4.33356 avg_loss = 3.99686\n",
      "epoch no.0 train no.17890  loss = 4.34798 avg_loss = 4.04227\n",
      "epoch no.0 train no.17900  loss = 3.01514 avg_loss = 4.05971\n",
      "epoch no.0 train no.17910  loss = 5.47396 avg_loss = 4.10693\n",
      "epoch no.0 train no.17920  loss = 4.12243 avg_loss = 4.06791\n",
      "epoch no.0 train no.17930  loss = 2.44002 avg_loss = 4.03820\n",
      "epoch no.0 train no.17940  loss = 3.21562 avg_loss = 4.05824\n",
      "epoch no.0 train no.17950  loss = 7.00688 avg_loss = 4.01769\n",
      "epoch no.0 train no.17960  loss = 3.37785 avg_loss = 4.06066\n",
      "epoch no.0 train no.17970  loss = 4.12735 avg_loss = 4.05400\n",
      "epoch no.0 train no.17980  loss = 3.26838 avg_loss = 4.05527\n",
      "epoch no.0 train no.17990  loss = 3.45033 avg_loss = 4.06554\n",
      "epoch no.0 train no.18000  loss = 3.34753 avg_loss = 4.07494\n",
      "3\n",
      "to_tokens: ['▁비', '▁명', '곡', '들', '</s>']\n",
      "추억의 명곡들</s>\n",
      "epoch no.0 train no.18010  loss = 4.64162 avg_loss = 4.08157\n",
      "epoch no.0 train no.18020  loss = 3.38737 avg_loss = 4.04688\n",
      "epoch no.0 train no.18030  loss = 4.37899 avg_loss = 4.01508\n",
      "epoch no.0 train no.18040  loss = 4.75871 avg_loss = 4.05441\n",
      "epoch no.0 train no.18050  loss = 3.88060 avg_loss = 4.03611\n",
      "epoch no.0 train no.18060  loss = 4.74824 avg_loss = 4.03883\n",
      "epoch no.0 train no.18070  loss = 4.27858 avg_loss = 4.01380\n",
      "epoch no.0 train no.18080  loss = 4.13607 avg_loss = 3.96123\n",
      "epoch no.0 train no.18090  loss = 4.17926 avg_loss = 3.95028\n",
      "epoch no.0 train no.18100  loss = 5.27072 avg_loss = 3.95999\n",
      "epoch no.0 train no.18110  loss = 1.67843 avg_loss = 3.93602\n",
      "epoch no.0 train no.18120  loss = 4.90749 avg_loss = 3.99359\n",
      "epoch no.0 train no.18130  loss = 6.14407 avg_loss = 4.07429\n",
      "epoch no.0 train no.18140  loss = 3.80026 avg_loss = 4.10747\n",
      "epoch no.0 train no.18150  loss = 3.12349 avg_loss = 4.07584\n",
      "epoch no.0 train no.18160  loss = 4.89799 avg_loss = 4.11774\n",
      "epoch no.0 train no.18170  loss = 5.30223 avg_loss = 4.10080\n",
      "epoch no.0 train no.18180  loss = 4.32567 avg_loss = 4.13873\n",
      "epoch no.0 train no.18190  loss = 2.64739 avg_loss = 4.09804\n",
      "epoch no.0 train no.18200  loss = 3.34318 avg_loss = 4.08796\n",
      "epoch no.0 train no.18210  loss = 3.96500 avg_loss = 4.03677\n",
      "epoch no.0 train no.18220  loss = 4.46361 avg_loss = 3.99495\n",
      "epoch no.0 train no.18230  loss = 3.52379 avg_loss = 3.95362\n",
      "epoch no.0 train no.18240  loss = 5.27360 avg_loss = 4.01276\n",
      "epoch no.0 train no.18250  loss = 2.92258 avg_loss = 3.99219\n",
      "epoch no.0 train no.18260  loss = 5.43918 avg_loss = 3.96743\n",
      "epoch no.0 train no.18270  loss = 3.47626 avg_loss = 3.91698\n",
      "epoch no.0 train no.18280  loss = 3.28241 avg_loss = 3.97402\n",
      "epoch no.0 train no.18290  loss = 3.20989 avg_loss = 3.90876\n",
      "epoch no.0 train no.18300  loss = 3.37115 avg_loss = 3.94601\n",
      "epoch no.0 train no.18310  loss = 6.71365 avg_loss = 3.99935\n",
      "epoch no.0 train no.18320  loss = 2.16499 avg_loss = 4.00435\n",
      "epoch no.0 train no.18330  loss = 6.16599 avg_loss = 3.99144\n",
      "epoch no.0 train no.18340  loss = 5.58465 avg_loss = 4.01403\n",
      "epoch no.0 train no.18350  loss = 3.87035 avg_loss = 4.07788\n",
      "epoch no.0 train no.18360  loss = 3.44360 avg_loss = 3.98304\n",
      "epoch no.0 train no.18370  loss = 3.76240 avg_loss = 3.95540\n",
      "epoch no.0 train no.18380  loss = 2.76937 avg_loss = 3.94483\n",
      "epoch no.0 train no.18390  loss = 4.28559 avg_loss = 4.00283\n",
      "epoch no.0 train no.18400  loss = 5.75242 avg_loss = 3.98759\n",
      "epoch no.0 train no.18410  loss = 3.25973 avg_loss = 4.03130\n",
      "epoch no.0 train no.18420  loss = 5.07705 avg_loss = 3.98137\n",
      "epoch no.0 train no.18430  loss = 2.86147 avg_loss = 3.99445\n",
      "epoch no.0 train no.18440  loss = 3.16203 avg_loss = 3.98202\n",
      "epoch no.0 train no.18450  loss = 2.71916 avg_loss = 3.90118\n",
      "epoch no.0 train no.18460  loss = 4.38608 avg_loss = 3.89113\n",
      "epoch no.0 train no.18470  loss = 2.59948 avg_loss = 3.84605\n",
      "epoch no.0 train no.18480  loss = 2.09410 avg_loss = 3.87812\n",
      "epoch no.0 train no.18490  loss = 3.67542 avg_loss = 3.94074\n",
      "epoch no.0 train no.18500  loss = 3.25637 avg_loss = 3.97065\n",
      "epoch no.0 train no.18510  loss = 4.54117 avg_loss = 3.97308\n",
      "epoch no.0 train no.18520  loss = 3.80030 avg_loss = 3.90158\n",
      "epoch no.0 train no.18530  loss = 4.50144 avg_loss = 3.89960\n",
      "epoch no.0 train no.18540  loss = 2.60934 avg_loss = 3.90029\n",
      "epoch no.0 train no.18550  loss = 2.38868 avg_loss = 3.82743\n",
      "epoch no.0 train no.18560  loss = 2.48067 avg_loss = 3.84525\n",
      "epoch no.0 train no.18570  loss = 5.17584 avg_loss = 3.85444\n",
      "epoch no.0 train no.18580  loss = 2.78821 avg_loss = 3.89139\n",
      "epoch no.0 train no.18590  loss = 2.21754 avg_loss = 3.96677\n",
      "epoch no.0 train no.18600  loss = 3.61597 avg_loss = 3.91783\n",
      "epoch no.0 train no.18610  loss = 4.63151 avg_loss = 3.90408\n",
      "epoch no.0 train no.18620  loss = 5.07465 avg_loss = 3.90927\n",
      "epoch no.0 train no.18630  loss = 4.45866 avg_loss = 3.90721\n",
      "epoch no.0 train no.18640  loss = 5.72373 avg_loss = 3.92341\n",
      "epoch no.0 train no.18650  loss = 2.53371 avg_loss = 3.90125\n",
      "epoch no.0 train no.18660  loss = 4.46023 avg_loss = 3.91615\n",
      "epoch no.0 train no.18670  loss = 3.66870 avg_loss = 3.98218\n",
      "epoch no.0 train no.18680  loss = 5.08819 avg_loss = 3.93779\n",
      "epoch no.0 train no.18690  loss = 4.22526 avg_loss = 3.95413\n",
      "epoch no.0 train no.18700  loss = 2.98336 avg_loss = 3.94327\n",
      "epoch no.0 train no.18710  loss = 4.33145 avg_loss = 3.97034\n",
      "epoch no.0 train no.18720  loss = 3.89505 avg_loss = 3.96033\n",
      "epoch no.0 train no.18730  loss = 3.62961 avg_loss = 3.96941\n",
      "epoch no.0 train no.18740  loss = 5.35735 avg_loss = 3.95571\n",
      "epoch no.0 train no.18750  loss = 5.06552 avg_loss = 3.94773\n",
      "epoch no.0 train no.18760  loss = 3.82160 avg_loss = 3.95980\n",
      "epoch no.0 train no.18770  loss = 5.99939 avg_loss = 3.95419\n",
      "epoch no.0 train no.18780  loss = 4.05078 avg_loss = 3.95354\n",
      "epoch no.0 train no.18790  loss = 5.53335 avg_loss = 3.93145\n",
      "epoch no.0 train no.18800  loss = 3.51557 avg_loss = 3.92407\n",
      "epoch no.0 train no.18810  loss = 4.77146 avg_loss = 3.96681\n",
      "epoch no.0 train no.18820  loss = 4.79386 avg_loss = 3.97863\n",
      "epoch no.0 train no.18830  loss = 3.00846 avg_loss = 3.94693\n",
      "epoch no.0 train no.18840  loss = 3.49378 avg_loss = 3.93041\n",
      "epoch no.0 train no.18850  loss = 3.38617 avg_loss = 3.92878\n",
      "epoch no.0 train no.18860  loss = 2.81390 avg_loss = 3.91189\n",
      "epoch no.0 train no.18870  loss = 3.27234 avg_loss = 3.91105\n",
      "epoch no.0 train no.18880  loss = 4.60142 avg_loss = 3.88889\n",
      "epoch no.0 train no.18890  loss = 3.75815 avg_loss = 3.89501\n",
      "epoch no.0 train no.18900  loss = 2.79642 avg_loss = 3.85344\n",
      "epoch no.0 train no.18910  loss = 3.52662 avg_loss = 3.90030\n",
      "epoch no.0 train no.18920  loss = 2.63150 avg_loss = 3.89808\n",
      "epoch no.0 train no.18930  loss = 2.83029 avg_loss = 3.93238\n",
      "epoch no.0 train no.18940  loss = 5.37565 avg_loss = 3.96225\n",
      "epoch no.0 train no.18950  loss = 2.56359 avg_loss = 3.95620\n",
      "epoch no.0 train no.18960  loss = 4.63265 avg_loss = 3.98448\n",
      "epoch no.0 train no.18970  loss = 4.10749 avg_loss = 3.97254\n",
      "epoch no.0 train no.18980  loss = 3.52982 avg_loss = 4.00922\n",
      "epoch no.0 train no.18990  loss = 2.51053 avg_loss = 3.95994\n",
      "epoch no.0 train no.19000  loss = 3.40843 avg_loss = 3.91895\n",
      "3\n",
      "to_tokens: ['▁가을', '▁아이돌', '팝', '송', '</s>']\n",
      "추억의 올드팝송</s>\n",
      "epoch no.0 train no.19010  loss = 6.49506 avg_loss = 3.98891\n",
      "epoch no.0 train no.19020  loss = 7.86311 avg_loss = 4.06697\n",
      "epoch no.0 train no.19030  loss = 5.20840 avg_loss = 4.07362\n",
      "epoch no.0 train no.19040  loss = 3.84124 avg_loss = 4.13655\n",
      "epoch no.0 train no.19050  loss = 3.63744 avg_loss = 4.08994\n",
      "epoch no.0 train no.19060  loss = 5.22542 avg_loss = 4.05465\n",
      "epoch no.0 train no.19070  loss = 5.31235 avg_loss = 4.01687\n",
      "epoch no.0 train no.19080  loss = 2.74033 avg_loss = 3.96442\n",
      "epoch no.0 train no.19090  loss = 2.67735 avg_loss = 3.94918\n",
      "epoch no.0 train no.19100  loss = 3.04233 avg_loss = 3.87523\n",
      "epoch no.0 train no.19110  loss = 3.97495 avg_loss = 3.88249\n",
      "epoch no.0 train no.19120  loss = 4.82075 avg_loss = 3.85641\n",
      "epoch no.0 train no.19130  loss = 7.06757 avg_loss = 3.84216\n",
      "epoch no.0 train no.19140  loss = 5.81852 avg_loss = 3.88124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.19150  loss = 2.82790 avg_loss = 3.87935\n",
      "epoch no.0 train no.19160  loss = 3.59684 avg_loss = 3.90233\n",
      "epoch no.0 train no.19170  loss = 5.87231 avg_loss = 3.92874\n",
      "epoch no.0 train no.19180  loss = 2.65640 avg_loss = 3.94137\n",
      "epoch no.0 train no.19190  loss = 2.49780 avg_loss = 3.90945\n",
      "epoch no.0 train no.19200  loss = 3.52387 avg_loss = 3.91906\n",
      "epoch no.0 train no.19210  loss = 4.21211 avg_loss = 3.93220\n",
      "epoch no.0 train no.19220  loss = 4.75333 avg_loss = 3.98895\n",
      "epoch no.0 train no.19230  loss = 3.62230 avg_loss = 3.97893\n",
      "epoch no.0 train no.19240  loss = 3.29122 avg_loss = 3.95558\n",
      "epoch no.0 train no.19250  loss = 6.32495 avg_loss = 3.99387\n",
      "epoch no.0 train no.19260  loss = 5.16231 avg_loss = 3.97215\n",
      "epoch no.0 train no.19270  loss = 3.22479 avg_loss = 3.98838\n",
      "epoch no.0 train no.19280  loss = 3.71197 avg_loss = 4.04058\n",
      "epoch no.0 train no.19290  loss = 4.04070 avg_loss = 4.03374\n",
      "epoch no.0 train no.19300  loss = 4.53603 avg_loss = 4.04959\n",
      "epoch no.0 train no.19310  loss = 2.90927 avg_loss = 4.04139\n",
      "epoch no.0 train no.19320  loss = 4.97596 avg_loss = 4.03865\n",
      "epoch no.0 train no.19330  loss = 4.17794 avg_loss = 4.04481\n",
      "epoch no.0 train no.19340  loss = 3.51184 avg_loss = 4.07673\n",
      "epoch no.0 train no.19350  loss = 4.36065 avg_loss = 4.09552\n",
      "epoch no.0 train no.19360  loss = 3.06914 avg_loss = 4.08103\n",
      "epoch no.0 train no.19370  loss = 5.21481 avg_loss = 4.11426\n",
      "epoch no.0 train no.19380  loss = 4.45303 avg_loss = 4.06219\n",
      "epoch no.0 train no.19390  loss = 3.27071 avg_loss = 4.03512\n",
      "epoch no.0 train no.19400  loss = 2.54227 avg_loss = 3.99087\n",
      "epoch no.0 train no.19410  loss = 5.26645 avg_loss = 4.04150\n",
      "epoch no.0 train no.19420  loss = 5.84559 avg_loss = 4.01664\n",
      "epoch no.0 train no.19430  loss = 2.01987 avg_loss = 3.99602\n",
      "epoch no.0 train no.19440  loss = 3.72096 avg_loss = 3.98227\n",
      "epoch no.0 train no.19450  loss = 4.91490 avg_loss = 4.01114\n",
      "epoch no.0 train no.19460  loss = 3.29717 avg_loss = 4.04352\n",
      "epoch no.0 train no.19470  loss = 3.66759 avg_loss = 3.99200\n",
      "epoch no.0 train no.19480  loss = 4.84087 avg_loss = 3.99354\n",
      "epoch no.0 train no.19490  loss = 3.03953 avg_loss = 4.01954\n",
      "epoch no.0 train no.19500  loss = 4.93538 avg_loss = 4.06836\n",
      "epoch no.0 train no.19510  loss = 4.53501 avg_loss = 4.09807\n",
      "epoch no.0 train no.19520  loss = 5.02228 avg_loss = 4.11169\n",
      "epoch no.0 train no.19530  loss = 3.44491 avg_loss = 4.14010\n",
      "epoch no.0 train no.19540  loss = 4.98967 avg_loss = 4.12934\n",
      "epoch no.0 train no.19550  loss = 3.58276 avg_loss = 4.13605\n",
      "epoch no.0 train no.19560  loss = 3.50199 avg_loss = 4.11073\n",
      "epoch no.0 train no.19570  loss = 4.09764 avg_loss = 4.08985\n",
      "epoch no.0 train no.19580  loss = 4.93627 avg_loss = 4.11905\n",
      "epoch no.0 train no.19590  loss = 4.08757 avg_loss = 4.09153\n",
      "epoch no.0 train no.19600  loss = 4.36613 avg_loss = 4.07313\n",
      "epoch no.0 train no.19610  loss = 3.66996 avg_loss = 4.05436\n",
      "epoch no.0 train no.19620  loss = 2.71994 avg_loss = 4.07614\n",
      "epoch no.0 train no.19630  loss = 3.60836 avg_loss = 4.04967\n",
      "epoch no.0 train no.19640  loss = 5.20772 avg_loss = 4.08320\n",
      "epoch no.0 train no.19650  loss = 7.24317 avg_loss = 4.09452\n",
      "epoch no.0 train no.19660  loss = 3.85530 avg_loss = 4.11936\n",
      "epoch no.0 train no.19670  loss = 2.89521 avg_loss = 4.14951\n",
      "epoch no.0 train no.19680  loss = 3.85316 avg_loss = 4.13331\n",
      "epoch no.0 train no.19690  loss = 2.82862 avg_loss = 4.13135\n",
      "epoch no.0 train no.19700  loss = 3.96132 avg_loss = 4.13859\n",
      "epoch no.0 train no.19710  loss = 2.76407 avg_loss = 4.07729\n",
      "epoch no.0 train no.19720  loss = 4.56869 avg_loss = 4.07058\n",
      "epoch no.0 train no.19730  loss = 2.69689 avg_loss = 3.97599\n",
      "epoch no.0 train no.19740  loss = 5.43372 avg_loss = 4.01245\n",
      "epoch no.0 train no.19750  loss = 3.43936 avg_loss = 4.08496\n",
      "epoch no.0 train no.19760  loss = 3.90390 avg_loss = 4.03296\n",
      "epoch no.0 train no.19770  loss = 6.21805 avg_loss = 4.08888\n",
      "epoch no.0 train no.19780  loss = 3.42138 avg_loss = 4.03883\n",
      "epoch no.0 train no.19790  loss = 4.37255 avg_loss = 4.07766\n",
      "epoch no.0 train no.19800  loss = 4.74205 avg_loss = 4.05341\n",
      "epoch no.0 train no.19810  loss = 5.71013 avg_loss = 4.05554\n",
      "epoch no.0 train no.19820  loss = 5.08077 avg_loss = 4.07153\n",
      "epoch no.0 train no.19830  loss = 3.77002 avg_loss = 4.04050\n",
      "epoch no.0 train no.19840  loss = 3.40977 avg_loss = 4.03838\n",
      "epoch no.0 train no.19850  loss = 4.30765 avg_loss = 4.06772\n",
      "epoch no.0 train no.19860  loss = 2.84619 avg_loss = 4.11362\n",
      "epoch no.0 train no.19870  loss = 2.94894 avg_loss = 4.11067\n",
      "epoch no.0 train no.19880  loss = 3.54131 avg_loss = 4.07895\n",
      "epoch no.0 train no.19890  loss = 6.22303 avg_loss = 4.08195\n",
      "epoch no.0 train no.19900  loss = 2.78430 avg_loss = 4.10520\n",
      "epoch no.0 train no.19910  loss = 2.87970 avg_loss = 4.10261\n",
      "epoch no.0 train no.19920  loss = 2.40518 avg_loss = 4.08547\n",
      "epoch no.0 train no.19930  loss = 3.76445 avg_loss = 4.07671\n",
      "epoch no.0 train no.19940  loss = 3.83025 avg_loss = 4.05473\n",
      "epoch no.0 train no.19950  loss = 4.60336 avg_loss = 4.05107\n",
      "epoch no.0 train no.19960  loss = 2.38294 avg_loss = 4.01240\n",
      "epoch no.0 train no.19970  loss = 2.39834 avg_loss = 3.99769\n",
      "epoch no.0 train no.19980  loss = 5.26290 avg_loss = 3.98263\n",
      "epoch no.0 train no.19990  loss = 2.65803 avg_loss = 3.98173\n",
      "epoch no.0 train no.20000  loss = 4.68392 avg_loss = 3.97446\n",
      "2\n",
      "to_tokens: ['▁비', '▁팝', '들', '</s>']\n",
      "추억의 노래들</s>\n",
      "epoch no.0 train no.20010  loss = 2.36064 avg_loss = 3.96677\n",
      "epoch no.0 train no.20020  loss = 6.09202 avg_loss = 3.96118\n",
      "epoch no.0 train no.20030  loss = 5.30391 avg_loss = 4.01892\n",
      "epoch no.0 train no.20040  loss = 5.06435 avg_loss = 4.03713\n",
      "epoch no.0 train no.20050  loss = 4.78829 avg_loss = 4.03314\n",
      "epoch no.0 train no.20060  loss = 5.40329 avg_loss = 4.07041\n",
      "epoch no.0 train no.20070  loss = 3.85473 avg_loss = 4.11092\n",
      "epoch no.0 train no.20080  loss = 4.20240 avg_loss = 4.05285\n",
      "epoch no.0 train no.20090  loss = 5.59132 avg_loss = 4.06726\n",
      "epoch no.0 train no.20100  loss = 3.17881 avg_loss = 4.02481\n",
      "epoch no.0 train no.20110  loss = 4.02824 avg_loss = 4.05038\n",
      "epoch no.0 train no.20120  loss = 3.44041 avg_loss = 4.00913\n",
      "epoch no.0 train no.20130  loss = 2.90919 avg_loss = 4.08792\n",
      "epoch no.0 train no.20140  loss = 8.00947 avg_loss = 4.14514\n",
      "epoch no.0 train no.20150  loss = 7.53005 avg_loss = 4.17588\n",
      "epoch no.0 train no.20160  loss = 4.39393 avg_loss = 4.18122\n",
      "epoch no.0 train no.20170  loss = 2.99656 avg_loss = 4.14012\n",
      "epoch no.0 train no.20180  loss = 3.10753 avg_loss = 4.08846\n",
      "epoch no.0 train no.20190  loss = 6.46252 avg_loss = 4.14749\n",
      "epoch no.0 train no.20200  loss = 2.57116 avg_loss = 4.16023\n",
      "epoch no.0 train no.20210  loss = 4.92069 avg_loss = 4.21097\n",
      "epoch no.0 train no.20220  loss = 2.89040 avg_loss = 4.16877\n",
      "epoch no.0 train no.20230  loss = 5.12395 avg_loss = 4.20060\n",
      "epoch no.0 train no.20240  loss = 4.29522 avg_loss = 4.18460\n",
      "epoch no.0 train no.20250  loss = 4.98322 avg_loss = 4.16457\n",
      "epoch no.0 train no.20260  loss = 2.45063 avg_loss = 4.09529\n",
      "epoch no.0 train no.20270  loss = 2.99017 avg_loss = 4.07836\n",
      "epoch no.0 train no.20280  loss = 4.03385 avg_loss = 4.07684\n",
      "epoch no.0 train no.20290  loss = 3.55909 avg_loss = 4.05040\n",
      "epoch no.0 train no.20300  loss = 3.62356 avg_loss = 3.96135\n",
      "epoch no.0 train no.20310  loss = 1.96104 avg_loss = 3.93751\n",
      "epoch no.0 train no.20320  loss = 3.48270 avg_loss = 3.91866\n",
      "epoch no.0 train no.20330  loss = 3.72105 avg_loss = 3.94723\n",
      "epoch no.0 train no.20340  loss = 5.60175 avg_loss = 3.96677\n",
      "epoch no.0 train no.20350  loss = 5.86233 avg_loss = 3.98254\n",
      "epoch no.0 train no.20360  loss = 2.94480 avg_loss = 3.94250\n",
      "epoch no.0 train no.20370  loss = 3.46554 avg_loss = 3.93265\n",
      "epoch no.0 train no.20380  loss = 5.57406 avg_loss = 3.93634\n",
      "epoch no.0 train no.20390  loss = 6.06875 avg_loss = 3.96552\n",
      "epoch no.0 train no.20400  loss = 4.14603 avg_loss = 3.98667\n",
      "epoch no.0 train no.20410  loss = 8.01894 avg_loss = 3.99662\n",
      "epoch no.0 train no.20420  loss = 4.72608 avg_loss = 4.01970\n",
      "epoch no.0 train no.20430  loss = 4.86504 avg_loss = 4.04649\n",
      "epoch no.0 train no.20440  loss = 2.40176 avg_loss = 4.06400\n",
      "epoch no.0 train no.20450  loss = 3.52790 avg_loss = 4.11904\n",
      "epoch no.0 train no.20460  loss = 5.55991 avg_loss = 4.10066\n",
      "epoch no.0 train no.20470  loss = 3.68213 avg_loss = 4.12037\n",
      "epoch no.0 train no.20480  loss = 4.55664 avg_loss = 4.15262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.20490  loss = 2.88264 avg_loss = 4.07864\n",
      "epoch no.0 train no.20500  loss = 3.17105 avg_loss = 4.08597\n",
      "epoch no.0 train no.20510  loss = 4.31891 avg_loss = 4.15004\n",
      "epoch no.0 train no.20520  loss = 4.55800 avg_loss = 4.06819\n",
      "epoch no.0 train no.20530  loss = 4.62818 avg_loss = 4.08746\n",
      "epoch no.0 train no.20540  loss = 3.09592 avg_loss = 4.05910\n",
      "epoch no.0 train no.20550  loss = 2.79951 avg_loss = 4.05144\n",
      "epoch no.0 train no.20560  loss = 3.97159 avg_loss = 4.01205\n",
      "epoch no.0 train no.20570  loss = 3.94430 avg_loss = 4.04133\n",
      "epoch no.0 train no.20580  loss = 5.53517 avg_loss = 4.06996\n",
      "epoch no.0 train no.20590  loss = 5.74327 avg_loss = 4.11696\n",
      "epoch no.0 train no.20600  loss = 5.80940 avg_loss = 4.11204\n",
      "epoch no.0 train no.20610  loss = 2.28400 avg_loss = 4.07716\n",
      "epoch no.0 train no.20620  loss = 2.98565 avg_loss = 4.10114\n",
      "epoch no.0 train no.20630  loss = 5.19831 avg_loss = 4.06829\n",
      "epoch no.0 train no.20640  loss = 1.44030 avg_loss = 4.02697\n",
      "epoch no.0 train no.20650  loss = 3.92131 avg_loss = 4.00448\n",
      "epoch no.0 train no.20660  loss = 3.31412 avg_loss = 3.98464\n",
      "epoch no.0 train no.20670  loss = 2.07566 avg_loss = 4.00549\n",
      "epoch no.0 train no.20680  loss = 4.82365 avg_loss = 4.02255\n",
      "epoch no.0 train no.20690  loss = 3.82725 avg_loss = 4.06749\n",
      "epoch no.0 train no.20700  loss = 3.54252 avg_loss = 4.06832\n",
      "epoch no.0 train no.20710  loss = 4.36895 avg_loss = 3.98464\n",
      "epoch no.0 train no.20720  loss = 4.62595 avg_loss = 4.04197\n",
      "epoch no.0 train no.20730  loss = 6.09348 avg_loss = 3.99938\n",
      "epoch no.0 train no.20740  loss = 4.35306 avg_loss = 3.99728\n",
      "epoch no.0 train no.20750  loss = 6.06331 avg_loss = 3.98737\n",
      "epoch no.0 train no.20760  loss = 4.69820 avg_loss = 3.98677\n",
      "epoch no.0 train no.20770  loss = 2.83474 avg_loss = 3.93819\n",
      "epoch no.0 train no.20780  loss = 4.30001 avg_loss = 3.95620\n",
      "epoch no.0 train no.20790  loss = 3.41291 avg_loss = 3.92787\n",
      "epoch no.0 train no.20800  loss = 3.73744 avg_loss = 3.96831\n",
      "epoch no.0 train no.20810  loss = 3.63423 avg_loss = 3.92254\n",
      "epoch no.0 train no.20820  loss = 3.12518 avg_loss = 3.87543\n",
      "epoch no.0 train no.20830  loss = 4.04446 avg_loss = 3.86796\n",
      "epoch no.0 train no.20840  loss = 4.99555 avg_loss = 3.89848\n",
      "epoch no.0 train no.20850  loss = 3.75799 avg_loss = 3.96914\n",
      "epoch no.0 train no.20860  loss = 2.40644 avg_loss = 3.92594\n",
      "epoch no.0 train no.20870  loss = 2.25847 avg_loss = 3.90207\n",
      "epoch no.0 train no.20880  loss = 2.54431 avg_loss = 3.85446\n",
      "epoch no.0 train no.20890  loss = 3.44087 avg_loss = 3.87917\n",
      "epoch no.0 train no.20900  loss = 5.52034 avg_loss = 3.90056\n",
      "epoch no.0 train no.20910  loss = 3.85816 avg_loss = 3.90236\n",
      "epoch no.0 train no.20920  loss = 6.72028 avg_loss = 3.94641\n",
      "epoch no.0 train no.20930  loss = 3.93939 avg_loss = 3.94407\n",
      "epoch no.0 train no.20940  loss = 4.27884 avg_loss = 3.98887\n",
      "epoch no.0 train no.20950  loss = 5.85431 avg_loss = 4.02723\n",
      "epoch no.0 train no.20960  loss = 2.76829 avg_loss = 4.08182\n",
      "epoch no.0 train no.20970  loss = 4.30201 avg_loss = 4.06406\n",
      "epoch no.0 train no.20980  loss = 3.87607 avg_loss = 4.02550\n",
      "epoch no.0 train no.20990  loss = 4.12172 avg_loss = 4.02889\n",
      "epoch no.0 train no.21000  loss = 5.75424 avg_loss = 3.99483\n",
      "2\n",
      "to_tokens: ['▁비', '▁명', '들', '</s>']\n",
      "추억의 노래들</s>\n",
      "epoch no.0 train no.21010  loss = 3.37556 avg_loss = 3.99831\n",
      "epoch no.0 train no.21020  loss = 2.71321 avg_loss = 3.97589\n",
      "epoch no.0 train no.21030  loss = 5.02777 avg_loss = 4.01771\n",
      "epoch no.0 train no.21040  loss = 5.06235 avg_loss = 4.03180\n",
      "epoch no.0 train no.21050  loss = 2.24842 avg_loss = 4.05268\n",
      "epoch no.0 train no.21060  loss = 4.94677 avg_loss = 4.09915\n",
      "epoch no.0 train no.21070  loss = 3.75805 avg_loss = 4.09820\n",
      "epoch no.0 train no.21080  loss = 2.64320 avg_loss = 4.08444\n",
      "epoch no.0 train no.21090  loss = 3.30459 avg_loss = 4.09941\n",
      "epoch no.0 train no.21100  loss = 4.28059 avg_loss = 4.11471\n",
      "epoch no.0 train no.21110  loss = 5.18379 avg_loss = 4.14733\n",
      "epoch no.0 train no.21120  loss = 4.95371 avg_loss = 4.14598\n",
      "epoch no.0 train no.21130  loss = 5.37298 avg_loss = 4.11765\n",
      "epoch no.0 train no.21140  loss = 5.38981 avg_loss = 4.11446\n",
      "epoch no.0 train no.21150  loss = 1.93184 avg_loss = 4.10120\n",
      "epoch no.0 train no.21160  loss = 2.06728 avg_loss = 4.04712\n",
      "epoch no.0 train no.21170  loss = 3.61094 avg_loss = 4.01854\n",
      "epoch no.0 train no.21180  loss = 5.32689 avg_loss = 4.00449\n",
      "epoch no.0 train no.21190  loss = 3.23254 avg_loss = 3.96929\n",
      "epoch no.0 train no.21200  loss = 4.12445 avg_loss = 3.98065\n",
      "epoch no.0 train no.21210  loss = 2.58747 avg_loss = 3.96252\n",
      "epoch no.0 train no.21220  loss = 4.61910 avg_loss = 3.98468\n",
      "epoch no.0 train no.21230  loss = 3.97505 avg_loss = 3.94673\n",
      "epoch no.0 train no.21240  loss = 6.07610 avg_loss = 3.95413\n",
      "epoch no.0 train no.21250  loss = 5.00835 avg_loss = 3.98188\n",
      "epoch no.0 train no.21260  loss = 5.92506 avg_loss = 3.93428\n",
      "epoch no.0 train no.21270  loss = 2.17038 avg_loss = 3.91354\n",
      "epoch no.0 train no.21280  loss = 3.99082 avg_loss = 3.94821\n",
      "epoch no.0 train no.21290  loss = 1.98364 avg_loss = 3.98318\n",
      "epoch no.0 train no.21300  loss = 2.66942 avg_loss = 3.93406\n",
      "epoch no.0 train no.21310  loss = 4.93009 avg_loss = 3.99253\n",
      "epoch no.0 train no.21320  loss = 4.17124 avg_loss = 4.03684\n",
      "epoch no.0 train no.21330  loss = 3.08034 avg_loss = 3.98553\n",
      "epoch no.0 train no.21340  loss = 2.31141 avg_loss = 3.93825\n",
      "epoch no.0 train no.21350  loss = 2.63635 avg_loss = 3.91108\n",
      "epoch no.0 train no.21360  loss = 5.45616 avg_loss = 3.92206\n",
      "epoch no.0 train no.21370  loss = 3.32482 avg_loss = 3.92157\n",
      "epoch no.0 train no.21380  loss = 2.51460 avg_loss = 3.90025\n",
      "epoch no.0 train no.21390  loss = 4.46729 avg_loss = 3.88577\n",
      "epoch no.0 train no.21400  loss = 2.43813 avg_loss = 3.87595\n",
      "epoch no.0 train no.21410  loss = 4.13505 avg_loss = 3.91367\n",
      "epoch no.0 train no.21420  loss = 6.44890 avg_loss = 3.91628\n",
      "epoch no.0 train no.21430  loss = 4.33811 avg_loss = 3.88492\n",
      "epoch no.0 train no.21440  loss = 4.32536 avg_loss = 3.84936\n",
      "epoch no.0 train no.21450  loss = 3.68752 avg_loss = 3.86772\n",
      "epoch no.0 train no.21460  loss = 3.12118 avg_loss = 3.88055\n",
      "epoch no.0 train no.21470  loss = 3.93532 avg_loss = 3.88199\n",
      "epoch no.0 train no.21480  loss = 3.71147 avg_loss = 3.91416\n",
      "epoch no.0 train no.21490  loss = 5.72459 avg_loss = 3.89627\n",
      "epoch no.0 train no.21500  loss = 4.60382 avg_loss = 3.93124\n",
      "epoch no.0 train no.21510  loss = 2.81867 avg_loss = 3.92017\n",
      "epoch no.0 train no.21520  loss = 2.86432 avg_loss = 3.93752\n",
      "epoch no.0 train no.21530  loss = 5.91503 avg_loss = 3.96644\n",
      "epoch no.0 train no.21540  loss = 4.10209 avg_loss = 4.00783\n",
      "epoch no.0 train no.21550  loss = 4.40625 avg_loss = 4.04514\n",
      "epoch no.0 train no.21560  loss = 2.74993 avg_loss = 3.97196\n",
      "epoch no.0 train no.21570  loss = 5.40257 avg_loss = 4.01659\n",
      "epoch no.0 train no.21580  loss = 3.40176 avg_loss = 4.05576\n",
      "epoch no.0 train no.21590  loss = 2.10301 avg_loss = 4.01749\n",
      "epoch no.0 train no.21600  loss = 2.60666 avg_loss = 4.01543\n",
      "epoch no.0 train no.21610  loss = 4.59872 avg_loss = 4.00183\n",
      "epoch no.0 train no.21620  loss = 3.84860 avg_loss = 3.98565\n",
      "epoch no.0 train no.21630  loss = 7.47224 avg_loss = 4.05804\n",
      "epoch no.0 train no.21640  loss = 4.62127 avg_loss = 4.03292\n",
      "epoch no.0 train no.21650  loss = 5.60258 avg_loss = 4.04145\n",
      "epoch no.0 train no.21660  loss = 4.31970 avg_loss = 4.07917\n",
      "epoch no.0 train no.21670  loss = 4.39546 avg_loss = 4.07655\n",
      "epoch no.0 train no.21680  loss = 4.75577 avg_loss = 4.08662\n",
      "epoch no.0 train no.21690  loss = 4.83819 avg_loss = 4.11190\n",
      "epoch no.0 train no.21700  loss = 4.37393 avg_loss = 4.09148\n",
      "epoch no.0 train no.21710  loss = 1.91978 avg_loss = 4.04724\n",
      "epoch no.0 train no.21720  loss = 2.76518 avg_loss = 3.99258\n",
      "epoch no.0 train no.21730  loss = 4.12223 avg_loss = 4.00410\n",
      "epoch no.0 train no.21740  loss = 6.24150 avg_loss = 3.98888\n",
      "epoch no.0 train no.21750  loss = 4.79130 avg_loss = 3.96980\n",
      "epoch no.0 train no.21760  loss = 3.06770 avg_loss = 3.91075\n",
      "epoch no.0 train no.21770  loss = 3.60402 avg_loss = 3.93669\n",
      "epoch no.0 train no.21780  loss = 1.99594 avg_loss = 3.91033\n",
      "epoch no.0 train no.21790  loss = 1.90356 avg_loss = 3.89549\n",
      "epoch no.0 train no.21800  loss = 4.05768 avg_loss = 3.94774\n",
      "epoch no.0 train no.21810  loss = 5.43635 avg_loss = 3.98966\n",
      "epoch no.0 train no.21820  loss = 7.13701 avg_loss = 4.06171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.21830  loss = 2.66258 avg_loss = 4.02903\n",
      "epoch no.0 train no.21840  loss = 4.56115 avg_loss = 3.99475\n",
      "epoch no.0 train no.21850  loss = 5.37210 avg_loss = 4.02278\n",
      "epoch no.0 train no.21860  loss = 4.77854 avg_loss = 3.99316\n",
      "epoch no.0 train no.21870  loss = 2.78686 avg_loss = 3.94952\n",
      "epoch no.0 train no.21880  loss = 4.59464 avg_loss = 3.95278\n",
      "epoch no.0 train no.21890  loss = 3.00807 avg_loss = 3.92256\n",
      "epoch no.0 train no.21900  loss = 3.44325 avg_loss = 4.01783\n",
      "epoch no.0 train no.21910  loss = 2.79973 avg_loss = 3.96514\n",
      "epoch no.0 train no.21920  loss = 5.42794 avg_loss = 4.00118\n",
      "epoch no.0 train no.21930  loss = 4.04109 avg_loss = 3.97920\n",
      "epoch no.0 train no.21940  loss = 2.66836 avg_loss = 3.94790\n",
      "epoch no.0 train no.21950  loss = 4.56509 avg_loss = 3.97198\n",
      "epoch no.0 train no.21960  loss = 4.18557 avg_loss = 3.97762\n",
      "epoch no.0 train no.21970  loss = 6.14130 avg_loss = 3.93815\n",
      "epoch no.0 train no.21980  loss = 4.00590 avg_loss = 3.90176\n",
      "epoch no.0 train no.21990  loss = 3.79739 avg_loss = 3.86349\n",
      "epoch no.0 train no.22000  loss = 3.50203 avg_loss = 3.87266\n",
      "3\n",
      "to_tokens: ['▁봄', '▁발라', '▁o', 'st', '</s>']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.0 train no.22010  loss = 5.32701 avg_loss = 3.91018\n",
      "epoch no.0 train no.22020  loss = 2.72506 avg_loss = 3.90901\n",
      "epoch no.0 train no.22030  loss = 4.14283 avg_loss = 3.95519\n",
      "epoch no.0 train no.22040  loss = 3.41042 avg_loss = 3.94432\n",
      "epoch no.0 train no.22050  loss = 4.21859 avg_loss = 3.99174\n",
      "epoch no.0 train no.22060  loss = 2.69896 avg_loss = 3.94697\n",
      "epoch no.0 train no.22070  loss = 2.52513 avg_loss = 3.89344\n",
      "epoch no.0 train no.22080  loss = 5.65063 avg_loss = 3.93450\n",
      "epoch no.0 train no.22090  loss = 4.30253 avg_loss = 3.95390\n",
      "epoch no.0 train no.22100  loss = 3.43648 avg_loss = 3.94403\n",
      "epoch no.0 train no.22110  loss = 5.40919 avg_loss = 3.97478\n",
      "epoch no.0 train no.22120  loss = 5.03707 avg_loss = 3.94541\n",
      "epoch no.0 train no.22130  loss = 4.89181 avg_loss = 3.97049\n",
      "epoch no.0 train no.22140  loss = 3.11958 avg_loss = 3.96111\n",
      "epoch no.0 train no.22150  loss = 4.74018 avg_loss = 4.00799\n",
      "epoch no.0 train no.22160  loss = 5.66869 avg_loss = 3.98820\n",
      "epoch no.0 train no.22170  loss = 5.35861 avg_loss = 3.93748\n",
      "epoch no.0 train no.22180  loss = 4.13324 avg_loss = 3.95727\n",
      "epoch no.0 train no.22190  loss = 4.98599 avg_loss = 3.93922\n",
      "epoch no.0 train no.22200  loss = 2.49644 avg_loss = 3.96487\n",
      "epoch no.0 train no.22210  loss = 2.29274 avg_loss = 3.92561\n",
      "epoch no.0 train no.22220  loss = 4.32151 avg_loss = 3.93640\n",
      "epoch no.0 train no.22230  loss = 4.10553 avg_loss = 3.91068\n",
      "epoch no.0 train no.22240  loss = 4.70586 avg_loss = 3.90789\n",
      "epoch no.0 train no.22250  loss = 2.96790 avg_loss = 3.91881\n",
      "epoch no.0 train no.22260  loss = 4.62476 avg_loss = 3.94992\n",
      "epoch no.0 train no.22270  loss = 3.22740 avg_loss = 3.92461\n",
      "epoch no.0 train no.22280  loss = 3.91827 avg_loss = 3.93299\n",
      "epoch no.0 train no.22290  loss = 2.65452 avg_loss = 3.92448\n",
      "epoch no.0 train no.22300  loss = 2.48999 avg_loss = 3.91916\n",
      "epoch no.0 train no.22310  loss = 4.56770 avg_loss = 3.95590\n",
      "epoch no.0 train no.22320  loss = 5.12224 avg_loss = 3.97771\n",
      "epoch no.0 train no.22330  loss = 2.65092 avg_loss = 3.96960\n",
      "epoch no.0 train no.22340  loss = 1.89372 avg_loss = 3.93612\n",
      "epoch no.0 train no.22350  loss = 5.07493 avg_loss = 3.92788\n",
      "epoch no.0 train no.22360  loss = 5.13130 avg_loss = 3.94651\n",
      "epoch no.0 train no.22370  loss = 5.53833 avg_loss = 3.96425\n",
      "epoch no.0 train no.22380  loss = 5.60109 avg_loss = 3.92923\n",
      "epoch no.0 train no.22390  loss = 2.71981 avg_loss = 3.93200\n",
      "epoch no.0 train no.22400  loss = 4.25464 avg_loss = 3.94139\n",
      "epoch no.0 train no.22410  loss = 2.68901 avg_loss = 4.00189\n",
      "epoch no.0 train no.22420  loss = 3.89637 avg_loss = 3.99954\n",
      "epoch no.0 train no.22430  loss = 5.34603 avg_loss = 4.02051\n",
      "epoch no.0 train no.22440  loss = 2.78365 avg_loss = 4.01863\n",
      "epoch no.0 train no.22450  loss = 2.31190 avg_loss = 4.03166\n",
      "epoch no.0 train no.22460  loss = 5.43241 avg_loss = 4.01822\n",
      "epoch no.0 train no.22470  loss = 3.22973 avg_loss = 4.02723\n",
      "epoch no.0 train no.22480  loss = 3.83210 avg_loss = 4.04611\n",
      "epoch no.0 train no.22490  loss = 3.23259 avg_loss = 4.07559\n",
      "epoch no.0 train no.22500  loss = 4.36790 avg_loss = 4.08310\n",
      "epoch no.0 train no.22510  loss = 4.46490 avg_loss = 4.04338\n",
      "epoch no.0 train no.22520  loss = 6.25479 avg_loss = 4.01605\n",
      "epoch no.0 train no.22530  loss = 2.39467 avg_loss = 3.99059\n",
      "epoch no.0 train no.22540  loss = 5.46700 avg_loss = 4.02813\n",
      "epoch no.0 train no.22550  loss = 4.24265 avg_loss = 4.09629\n",
      "epoch no.0 train no.22560  loss = 5.31653 avg_loss = 4.10683\n",
      "epoch no.0 train no.22570  loss = 3.36013 avg_loss = 4.13765\n",
      "epoch no.0 train no.22580  loss = 3.73167 avg_loss = 4.08656\n",
      "epoch no.0 train no.22590  loss = 5.90684 avg_loss = 4.11451\n",
      "epoch no.0 train no.22600  loss = 4.34141 avg_loss = 4.10789\n",
      "epoch no.0 train no.22610  loss = 4.54401 avg_loss = 4.11612\n",
      "epoch no.0 train no.22620  loss = 5.15917 avg_loss = 4.14426\n",
      "epoch no.0 train no.22630  loss = 3.74469 avg_loss = 4.13001\n",
      "epoch no.0 train no.22640  loss = 5.20984 avg_loss = 4.08966\n",
      "epoch no.0 train no.22650  loss = 3.80105 avg_loss = 4.07736\n",
      "epoch no.0 train no.22660  loss = 3.23388 avg_loss = 4.06802\n",
      "epoch no.0 train no.22670  loss = 5.16236 avg_loss = 4.09928\n",
      "epoch no.0 train no.22680  loss = 2.70932 avg_loss = 4.03783\n",
      "epoch no.0 train no.22690  loss = 3.61173 avg_loss = 4.08806\n",
      "epoch no.0 train no.22700  loss = 4.35361 avg_loss = 4.13093\n",
      "epoch no.0 train no.22710  loss = 4.36716 avg_loss = 4.06535\n",
      "epoch no.0 train no.22720  loss = 2.22779 avg_loss = 4.12291\n",
      "epoch no.0 train no.22730  loss = 3.43441 avg_loss = 4.06979\n",
      "epoch no.0 train no.22740  loss = 3.83251 avg_loss = 4.05781\n",
      "epoch no.0 train no.22750  loss = 2.61669 avg_loss = 4.08294\n",
      "epoch no.0 train no.22760  loss = 3.80544 avg_loss = 4.06417\n",
      "epoch no.0 train no.22770  loss = 3.67717 avg_loss = 4.10595\n",
      "epoch no.0 train no.22780  loss = 4.38415 avg_loss = 4.11674\n",
      "epoch no.0 train no.22790  loss = 5.79317 avg_loss = 4.11361\n",
      "epoch no.0 train no.22800  loss = 2.40128 avg_loss = 4.07392\n",
      "epoch no.0 train no.22810  loss = 3.33645 avg_loss = 4.03920\n",
      "epoch no.0 train no.22820  loss = 4.35827 avg_loss = 4.11816\n",
      "epoch no.0 train no.22830  loss = 3.46991 avg_loss = 4.13007\n",
      "epoch no.0 train no.22840  loss = 3.01500 avg_loss = 4.15831\n",
      "epoch no.0 train no.22850  loss = 4.87421 avg_loss = 4.12283\n",
      "epoch no.0 train no.22860  loss = 3.29670 avg_loss = 4.12124\n",
      "epoch no.0 train no.22870  loss = 3.15101 avg_loss = 4.14207\n",
      "epoch no.0 train no.22880  loss = 6.07004 avg_loss = 4.09430\n",
      "epoch no.0 train no.22890  loss = 2.52012 avg_loss = 4.02699\n",
      "epoch no.0 train no.22900  loss = 2.20766 avg_loss = 4.05685\n",
      "epoch no.0 train no.22910  loss = 2.70965 avg_loss = 3.98898\n",
      "epoch no.0 train no.22920  loss = 4.60950 avg_loss = 4.03143\n",
      "epoch no.0 train no.22930  loss = 4.15493 avg_loss = 4.00940\n",
      "epoch no.0 train no.22940  loss = 3.18111 avg_loss = 3.99787\n",
      "epoch no.0 train no.22950  loss = 5.41164 avg_loss = 3.99891\n",
      "epoch no.0 train no.22960  loss = 3.14318 avg_loss = 3.96737\n",
      "epoch no.0 train no.22970  loss = 4.95690 avg_loss = 3.97674\n",
      "epoch no.0 train no.22980  loss = 6.00290 avg_loss = 3.99884\n",
      "epoch no.0 train no.22990  loss = 2.21881 avg_loss = 3.95351\n",
      "epoch no.0 train no.23000  loss = 1.87591 avg_loss = 3.93567\n",
      "4\n",
      "to_tokens: ['▁가을', '▁노래', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.0 train no.23010  loss = 5.31847 avg_loss = 3.96167\n",
      "epoch no.0 train no.23020  loss = 2.36791 avg_loss = 3.99916\n",
      "epoch no.0 train no.23030  loss = 4.43873 avg_loss = 4.00253\n",
      "epoch no.0 train no.23040  loss = 3.31982 avg_loss = 3.98757\n",
      "epoch no.0 train no.23050  loss = 4.81364 avg_loss = 3.99926\n",
      "epoch no.0 train no.23060  loss = 5.37426 avg_loss = 3.95381\n",
      "epoch no.0 train no.23070  loss = 6.69137 avg_loss = 3.98822\n",
      "epoch no.0 train no.23080  loss = 2.88725 avg_loss = 4.03955\n",
      "epoch no.0 train no.23090  loss = 5.35120 avg_loss = 4.11027\n",
      "epoch no.0 train no.23100  loss = 4.31799 avg_loss = 4.09354\n",
      "epoch no.0 train no.23110  loss = 4.97704 avg_loss = 4.09329\n",
      "epoch no.0 train no.23120  loss = 4.03940 avg_loss = 4.07336\n",
      "epoch no.0 train no.23130  loss = 4.35444 avg_loss = 4.07477\n",
      "epoch no.0 train no.23140  loss = 3.82749 avg_loss = 4.10335\n",
      "epoch no.0 train no.23150  loss = 3.42265 avg_loss = 4.08063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.23160  loss = 4.60987 avg_loss = 4.00621\n",
      "epoch no.0 train no.23170  loss = 3.64865 avg_loss = 4.01192\n",
      "epoch no.0 train no.23180  loss = 5.03510 avg_loss = 4.04732\n",
      "epoch no.0 train no.23190  loss = 4.67601 avg_loss = 4.07016\n",
      "epoch no.0 train no.23200  loss = 3.76617 avg_loss = 4.04014\n",
      "epoch no.0 train no.23210  loss = 3.72205 avg_loss = 3.99702\n",
      "epoch no.0 train no.23220  loss = 6.11016 avg_loss = 3.99082\n",
      "epoch no.0 train no.23230  loss = 2.83166 avg_loss = 3.94888\n",
      "epoch no.0 train no.23240  loss = 3.42219 avg_loss = 3.93961\n",
      "epoch no.0 train no.23250  loss = 4.33226 avg_loss = 3.92694\n",
      "epoch no.0 train no.23260  loss = 4.01794 avg_loss = 3.93819\n",
      "epoch no.0 train no.23270  loss = 3.51035 avg_loss = 3.97393\n",
      "epoch no.0 train no.23280  loss = 3.92819 avg_loss = 4.00606\n",
      "epoch no.0 train no.23290  loss = 5.42469 avg_loss = 4.03591\n",
      "epoch no.0 train no.23300  loss = 4.89563 avg_loss = 4.07712\n",
      "epoch no.0 train no.23310  loss = 4.57201 avg_loss = 4.08778\n",
      "epoch no.0 train no.23320  loss = 3.99766 avg_loss = 4.18354\n",
      "epoch no.0 train no.23330  loss = 4.13744 avg_loss = 4.18430\n",
      "epoch no.0 train no.23340  loss = 5.04798 avg_loss = 4.19478\n",
      "epoch no.0 train no.23350  loss = 2.85544 avg_loss = 4.15052\n",
      "epoch no.0 train no.23360  loss = 4.52651 avg_loss = 4.14006\n",
      "epoch no.0 train no.23370  loss = 2.51199 avg_loss = 4.12828\n",
      "epoch no.0 train no.23380  loss = 2.63485 avg_loss = 4.05363\n",
      "epoch no.0 train no.23390  loss = 5.31707 avg_loss = 4.07697\n",
      "epoch no.0 train no.23400  loss = 3.54067 avg_loss = 4.08335\n",
      "epoch no.0 train no.23410  loss = 2.62643 avg_loss = 4.04962\n",
      "epoch no.0 train no.23420  loss = 3.76237 avg_loss = 4.10568\n",
      "epoch no.0 train no.23430  loss = 2.50471 avg_loss = 4.09927\n",
      "epoch no.0 train no.23440  loss = 4.49779 avg_loss = 4.09161\n",
      "epoch no.0 train no.23450  loss = 2.71119 avg_loss = 4.02506\n",
      "epoch no.0 train no.23460  loss = 4.18986 avg_loss = 3.96933\n",
      "epoch no.0 train no.23470  loss = 4.69990 avg_loss = 3.97702\n",
      "epoch no.0 train no.23480  loss = 3.20631 avg_loss = 3.94661\n",
      "epoch no.0 train no.23490  loss = 3.24062 avg_loss = 3.95402\n",
      "epoch no.0 train no.23500  loss = 2.66244 avg_loss = 3.87036\n",
      "epoch no.0 train no.23510  loss = 3.18705 avg_loss = 3.88559\n",
      "epoch no.0 train no.23520  loss = 3.78309 avg_loss = 3.92497\n",
      "epoch no.0 train no.23530  loss = 2.83607 avg_loss = 3.85643\n",
      "epoch no.0 train no.23540  loss = 4.01196 avg_loss = 3.91273\n",
      "epoch no.0 train no.23550  loss = 3.26358 avg_loss = 3.92979\n",
      "epoch no.0 train no.23560  loss = 3.17565 avg_loss = 3.93096\n",
      "epoch no.0 train no.23570  loss = 4.70773 avg_loss = 3.92398\n",
      "epoch no.0 train no.23580  loss = 4.70434 avg_loss = 3.91368\n",
      "epoch no.0 train no.23590  loss = 4.14531 avg_loss = 3.96260\n",
      "epoch no.0 train no.23600  loss = 2.12705 avg_loss = 3.97879\n",
      "epoch no.0 train no.23610  loss = 2.61968 avg_loss = 3.93447\n",
      "epoch no.0 train no.23620  loss = 4.67253 avg_loss = 3.97919\n",
      "epoch no.0 train no.23630  loss = 5.37805 avg_loss = 3.93983\n",
      "epoch no.0 train no.23640  loss = 3.84173 avg_loss = 4.01577\n",
      "epoch no.0 train no.23650  loss = 3.43033 avg_loss = 4.03856\n",
      "epoch no.0 train no.23660  loss = 3.12780 avg_loss = 3.97545\n",
      "epoch no.0 train no.23670  loss = 2.84695 avg_loss = 3.92581\n",
      "epoch no.0 train no.23680  loss = 7.10229 avg_loss = 3.94472\n",
      "epoch no.0 train no.23690  loss = 3.96513 avg_loss = 3.96442\n",
      "epoch no.0 train no.23700  loss = 4.02947 avg_loss = 3.97666\n",
      "epoch no.0 train no.23710  loss = 6.37208 avg_loss = 3.97117\n",
      "epoch no.0 train no.23720  loss = 4.00407 avg_loss = 3.96592\n",
      "epoch no.0 train no.23730  loss = 5.05728 avg_loss = 3.99533\n",
      "epoch no.0 train no.23740  loss = 5.29037 avg_loss = 3.99404\n",
      "epoch no.0 train no.23750  loss = 3.41177 avg_loss = 4.01640\n",
      "epoch no.0 train no.23760  loss = 4.17938 avg_loss = 4.04949\n",
      "epoch no.0 train no.23770  loss = 2.51365 avg_loss = 3.98954\n",
      "epoch no.0 train no.23780  loss = 3.82798 avg_loss = 4.05489\n",
      "epoch no.0 train no.23790  loss = 3.21771 avg_loss = 3.99794\n",
      "epoch no.0 train no.23800  loss = 3.41407 avg_loss = 3.95904\n",
      "epoch no.0 train no.23810  loss = 1.96990 avg_loss = 3.91473\n",
      "epoch no.0 train no.23820  loss = 4.95666 avg_loss = 3.97828\n",
      "epoch no.0 train no.23830  loss = 4.89578 avg_loss = 4.03403\n",
      "epoch no.0 train no.23840  loss = 3.80341 avg_loss = 4.04895\n",
      "epoch no.0 train no.23850  loss = 5.10977 avg_loss = 4.03339\n",
      "epoch no.0 train no.23860  loss = 6.88524 avg_loss = 4.05160\n",
      "epoch no.0 train no.23870  loss = 3.70142 avg_loss = 4.01492\n",
      "epoch no.0 train no.23880  loss = 3.87155 avg_loss = 4.03099\n",
      "epoch no.0 train no.23890  loss = 1.74783 avg_loss = 3.95517\n",
      "epoch no.0 train no.23900  loss = 3.79353 avg_loss = 3.93067\n",
      "epoch no.0 train no.23910  loss = 2.59609 avg_loss = 3.93814\n",
      "epoch no.0 train no.23920  loss = 4.60306 avg_loss = 4.01570\n",
      "epoch no.0 train no.23930  loss = 3.49070 avg_loss = 3.98931\n",
      "epoch no.0 train no.23940  loss = 1.67888 avg_loss = 4.02572\n",
      "epoch no.0 train no.23950  loss = 4.81651 avg_loss = 3.97399\n",
      "epoch no.0 train no.23960  loss = 2.95921 avg_loss = 3.92328\n",
      "epoch no.0 train no.23970  loss = 4.22946 avg_loss = 3.91496\n",
      "epoch no.0 train no.23980  loss = 3.76384 avg_loss = 3.90594\n",
      "epoch no.0 train no.23990  loss = 4.23722 avg_loss = 3.93339\n",
      "epoch no.0 train no.24000  loss = 5.01947 avg_loss = 3.91911\n",
      "5\n",
      "to_tokens: ['▁비', '▁발라', '트', '▁명', '곡', '</s>', '</s>']\n",
      "추억의 트로트 명곡 모음</s>\n",
      "epoch no.0 train no.24010  loss = 5.73804 avg_loss = 3.89571\n",
      "epoch no.0 train no.24020  loss = 4.61597 avg_loss = 3.89402\n",
      "epoch no.0 train no.24030  loss = 4.61577 avg_loss = 3.93374\n",
      "epoch no.0 train no.24040  loss = 3.70300 avg_loss = 3.89839\n",
      "epoch no.0 train no.24050  loss = 6.07874 avg_loss = 3.93048\n",
      "epoch no.0 train no.24060  loss = 3.42277 avg_loss = 3.98205\n",
      "epoch no.0 train no.24070  loss = 4.74333 avg_loss = 3.95861\n",
      "epoch no.0 train no.24080  loss = 4.40190 avg_loss = 3.92264\n",
      "epoch no.0 train no.24090  loss = 2.66956 avg_loss = 3.83618\n",
      "epoch no.0 train no.24100  loss = 2.73829 avg_loss = 3.81487\n",
      "epoch no.0 train no.24110  loss = 3.18368 avg_loss = 3.81868\n",
      "epoch no.0 train no.24120  loss = 5.15147 avg_loss = 3.80566\n",
      "epoch no.0 train no.24130  loss = 5.31316 avg_loss = 3.82318\n",
      "epoch no.0 train no.24140  loss = 3.95356 avg_loss = 3.88594\n",
      "epoch no.0 train no.24150  loss = 3.09727 avg_loss = 3.85627\n",
      "epoch no.0 train no.24160  loss = 3.38449 avg_loss = 3.81025\n",
      "epoch no.0 train no.24170  loss = 4.00070 avg_loss = 3.83451\n",
      "epoch no.0 train no.24180  loss = 3.11387 avg_loss = 3.83929\n",
      "epoch no.0 train no.24190  loss = 3.01514 avg_loss = 3.86911\n",
      "epoch no.0 train no.24200  loss = 2.47449 avg_loss = 3.89003\n",
      "epoch no.0 train no.24210  loss = 3.91784 avg_loss = 3.90575\n",
      "epoch no.0 train no.24220  loss = 6.00925 avg_loss = 3.90036\n",
      "epoch no.0 train no.24230  loss = 2.22500 avg_loss = 3.87497\n",
      "epoch no.0 train no.24240  loss = 4.80452 avg_loss = 3.83836\n",
      "epoch no.0 train no.24250  loss = 3.43775 avg_loss = 3.82740\n",
      "epoch no.0 train no.24260  loss = 4.62980 avg_loss = 3.78340\n",
      "epoch no.0 train no.24270  loss = 5.86839 avg_loss = 3.79340\n",
      "epoch no.0 train no.24280  loss = 2.69208 avg_loss = 3.84283\n",
      "epoch no.0 train no.24290  loss = 1.86173 avg_loss = 3.80580\n",
      "epoch no.0 train no.24300  loss = 3.81796 avg_loss = 3.83331\n",
      "epoch no.0 train no.24310  loss = 3.03893 avg_loss = 3.80920\n",
      "epoch no.0 train no.24320  loss = 4.65864 avg_loss = 3.88921\n",
      "epoch no.0 train no.24330  loss = 4.70611 avg_loss = 3.87016\n",
      "epoch no.0 train no.24340  loss = 7.36497 avg_loss = 3.86396\n",
      "epoch no.0 train no.24350  loss = 2.35928 avg_loss = 3.87519\n",
      "epoch no.0 train no.24360  loss = 4.67174 avg_loss = 3.87753\n",
      "epoch no.0 train no.24370  loss = 2.40484 avg_loss = 3.92985\n",
      "epoch no.0 train no.24380  loss = 4.57603 avg_loss = 3.90618\n",
      "epoch no.0 train no.24390  loss = 2.64338 avg_loss = 3.86968\n",
      "epoch no.0 train no.24400  loss = 3.26585 avg_loss = 3.86639\n",
      "epoch no.0 train no.24410  loss = 4.07072 avg_loss = 3.86178\n",
      "epoch no.0 train no.24420  loss = 3.36909 avg_loss = 3.84477\n",
      "epoch no.0 train no.24430  loss = 3.24502 avg_loss = 3.84338\n",
      "epoch no.0 train no.24440  loss = 6.45716 avg_loss = 3.84363\n",
      "epoch no.0 train no.24450  loss = 3.15232 avg_loss = 3.90430\n",
      "epoch no.0 train no.24460  loss = 4.80486 avg_loss = 3.90180\n",
      "epoch no.0 train no.24470  loss = 3.48652 avg_loss = 3.88403\n",
      "epoch no.0 train no.24480  loss = 4.93471 avg_loss = 3.94756\n",
      "epoch no.0 train no.24490  loss = 2.53675 avg_loss = 3.95560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.24500  loss = 4.20191 avg_loss = 3.98887\n",
      "epoch no.0 train no.24510  loss = 3.76805 avg_loss = 3.94622\n",
      "epoch no.0 train no.24520  loss = 2.41615 avg_loss = 3.91891\n",
      "epoch no.0 train no.24530  loss = 3.80099 avg_loss = 3.89989\n",
      "epoch no.0 train no.24540  loss = 5.97437 avg_loss = 3.91381\n",
      "epoch no.0 train no.24550  loss = 2.98022 avg_loss = 3.87667\n",
      "epoch no.0 train no.24560  loss = 3.40679 avg_loss = 3.87347\n",
      "epoch no.0 train no.24570  loss = 5.33185 avg_loss = 3.93107\n",
      "epoch no.0 train no.24580  loss = 1.92062 avg_loss = 3.89871\n",
      "epoch no.0 train no.24590  loss = 2.78583 avg_loss = 3.85719\n",
      "epoch no.0 train no.24600  loss = 3.51561 avg_loss = 3.82239\n",
      "epoch no.0 train no.24610  loss = 2.38347 avg_loss = 3.83087\n",
      "epoch no.0 train no.24620  loss = 4.41243 avg_loss = 3.81513\n",
      "epoch no.0 train no.24630  loss = 3.67563 avg_loss = 3.85441\n",
      "epoch no.0 train no.24640  loss = 4.10901 avg_loss = 3.91054\n",
      "epoch no.0 train no.24650  loss = 2.75302 avg_loss = 3.97007\n",
      "epoch no.0 train no.24660  loss = 3.27982 avg_loss = 3.95850\n",
      "epoch no.0 train no.24670  loss = 4.70562 avg_loss = 3.94580\n",
      "epoch no.0 train no.24680  loss = 2.97970 avg_loss = 3.92770\n",
      "epoch no.0 train no.24690  loss = 3.23033 avg_loss = 3.90464\n",
      "epoch no.0 train no.24700  loss = 1.93118 avg_loss = 3.83989\n",
      "epoch no.0 train no.24710  loss = 2.18515 avg_loss = 3.81714\n",
      "epoch no.0 train no.24720  loss = 5.61131 avg_loss = 3.84416\n",
      "epoch no.0 train no.24730  loss = 7.38403 avg_loss = 3.86530\n",
      "epoch no.0 train no.24740  loss = 5.13413 avg_loss = 3.85943\n",
      "epoch no.0 train no.24750  loss = 1.90659 avg_loss = 3.86818\n",
      "epoch no.0 train no.24760  loss = 3.68690 avg_loss = 3.86310\n",
      "epoch no.0 train no.24770  loss = 6.05829 avg_loss = 3.94558\n",
      "epoch no.0 train no.24780  loss = 5.33847 avg_loss = 3.91149\n",
      "epoch no.0 train no.24790  loss = 4.17009 avg_loss = 3.90491\n",
      "epoch no.0 train no.24800  loss = 3.04820 avg_loss = 3.87728\n",
      "epoch no.0 train no.24810  loss = 4.81171 avg_loss = 3.85501\n",
      "epoch no.0 train no.24820  loss = 2.52767 avg_loss = 3.80252\n",
      "epoch no.0 train no.24830  loss = 4.46946 avg_loss = 3.84052\n",
      "epoch no.0 train no.24840  loss = 3.41768 avg_loss = 3.83310\n",
      "epoch no.0 train no.24850  loss = 5.55243 avg_loss = 3.88273\n",
      "epoch no.0 train no.24860  loss = 2.39016 avg_loss = 3.89114\n",
      "epoch no.0 train no.24870  loss = 5.15443 avg_loss = 3.90182\n",
      "epoch no.0 train no.24880  loss = 3.10764 avg_loss = 3.91590\n",
      "epoch no.0 train no.24890  loss = 4.10378 avg_loss = 3.92555\n",
      "epoch no.0 train no.24900  loss = 3.25411 avg_loss = 3.88667\n",
      "epoch no.0 train no.24910  loss = 5.95757 avg_loss = 3.95695\n",
      "epoch no.0 train no.24920  loss = 3.50721 avg_loss = 3.99168\n",
      "epoch no.0 train no.24930  loss = 2.59800 avg_loss = 3.97638\n",
      "epoch no.0 train no.24940  loss = 3.55512 avg_loss = 3.94150\n",
      "epoch no.0 train no.24950  loss = 3.19158 avg_loss = 3.94678\n",
      "epoch no.0 train no.24960  loss = 3.93784 avg_loss = 3.93299\n",
      "epoch no.0 train no.24970  loss = 2.78659 avg_loss = 3.89064\n",
      "epoch no.0 train no.24980  loss = 2.44359 avg_loss = 3.85620\n",
      "epoch no.0 train no.24990  loss = 4.08903 avg_loss = 3.83423\n",
      "epoch no.0 train no.25000  loss = 5.70022 avg_loss = 3.91806\n",
      "3\n",
      "to_tokens: ['▁비', '▁드라마', '송', '▁모음', '</s>']\n",
      "추억의 팝송 모음</s>\n",
      "epoch no.0 train no.25010  loss = 2.00485 avg_loss = 3.89450\n",
      "epoch no.0 train no.25020  loss = 4.94752 avg_loss = 3.96388\n",
      "epoch no.0 train no.25030  loss = 4.08970 avg_loss = 3.95676\n",
      "epoch no.0 train no.25040  loss = 4.63800 avg_loss = 3.94392\n",
      "epoch no.0 train no.25050  loss = 4.28647 avg_loss = 3.94635\n",
      "epoch no.0 train no.25060  loss = 4.28778 avg_loss = 3.87468\n",
      "epoch no.0 train no.25070  loss = 3.87887 avg_loss = 3.85502\n",
      "epoch no.0 train no.25080  loss = 4.20201 avg_loss = 3.88565\n",
      "epoch no.0 train no.25090  loss = 3.09800 avg_loss = 3.91032\n",
      "epoch no.0 train no.25100  loss = 2.72699 avg_loss = 3.84403\n",
      "epoch no.0 train no.25110  loss = 4.23602 avg_loss = 3.85814\n",
      "epoch no.0 train no.25120  loss = 4.11901 avg_loss = 3.85965\n",
      "epoch no.0 train no.25130  loss = 4.69836 avg_loss = 3.87404\n",
      "epoch no.0 train no.25140  loss = 3.93835 avg_loss = 3.94311\n",
      "epoch no.0 train no.25150  loss = 5.28795 avg_loss = 4.00703\n",
      "epoch no.0 train no.25160  loss = 4.37791 avg_loss = 3.98267\n",
      "epoch no.0 train no.25170  loss = 4.42617 avg_loss = 3.96073\n",
      "epoch no.0 train no.25180  loss = 7.42236 avg_loss = 4.02140\n",
      "epoch no.0 train no.25190  loss = 3.05864 avg_loss = 4.00205\n",
      "epoch no.0 train no.25200  loss = 5.20870 avg_loss = 3.99432\n",
      "epoch no.0 train no.25210  loss = 3.18477 avg_loss = 4.04748\n",
      "epoch no.0 train no.25220  loss = 2.44160 avg_loss = 4.02025\n",
      "epoch no.0 train no.25230  loss = 4.76216 avg_loss = 4.01115\n",
      "epoch no.0 train no.25240  loss = 5.83105 avg_loss = 4.03273\n",
      "epoch no.0 train no.25250  loss = 5.93369 avg_loss = 4.04830\n",
      "epoch no.0 train no.25260  loss = 4.80936 avg_loss = 3.98900\n",
      "epoch no.0 train no.25270  loss = 2.63237 avg_loss = 3.94455\n",
      "epoch no.0 train no.25280  loss = 3.78849 avg_loss = 3.97286\n",
      "epoch no.0 train no.25290  loss = 6.06784 avg_loss = 4.01787\n",
      "epoch no.0 train no.25300  loss = 3.66134 avg_loss = 3.98518\n",
      "epoch no.0 train no.25310  loss = 3.78598 avg_loss = 3.94865\n",
      "epoch no.0 train no.25320  loss = 2.96719 avg_loss = 3.97433\n",
      "epoch no.0 train no.25330  loss = 5.58541 avg_loss = 4.00854\n",
      "epoch no.0 train no.25340  loss = 3.08395 avg_loss = 4.00605\n",
      "epoch no.0 train no.25350  loss = 3.17140 avg_loss = 3.94371\n",
      "epoch no.0 train no.25360  loss = 6.34796 avg_loss = 3.94753\n",
      "epoch no.0 train no.25370  loss = 2.83843 avg_loss = 3.95512\n",
      "epoch no.0 train no.25380  loss = 4.50890 avg_loss = 3.96691\n",
      "epoch no.0 train no.25390  loss = 5.66605 avg_loss = 3.93729\n",
      "epoch no.0 train no.25400  loss = 3.64326 avg_loss = 3.91077\n",
      "epoch no.0 train no.25410  loss = 2.17894 avg_loss = 3.94257\n",
      "epoch no.0 train no.25420  loss = 4.31228 avg_loss = 3.95993\n",
      "epoch no.0 train no.25430  loss = 3.85214 avg_loss = 3.92359\n",
      "epoch no.0 train no.25440  loss = 2.82758 avg_loss = 3.95427\n",
      "epoch no.0 train no.25450  loss = 4.87266 avg_loss = 3.95542\n",
      "epoch no.0 train no.25460  loss = 2.70667 avg_loss = 3.94106\n",
      "epoch no.0 train no.25470  loss = 3.88827 avg_loss = 3.96259\n",
      "epoch no.0 train no.25480  loss = 2.09908 avg_loss = 3.96050\n",
      "epoch no.0 train no.25490  loss = 4.12156 avg_loss = 3.95986\n",
      "epoch no.0 train no.25500  loss = 3.46910 avg_loss = 3.99826\n",
      "epoch no.0 train no.25510  loss = 2.82005 avg_loss = 3.95210\n",
      "epoch no.0 train no.25520  loss = 3.91424 avg_loss = 3.90836\n",
      "epoch no.0 train no.25530  loss = 3.36298 avg_loss = 3.93206\n",
      "epoch no.0 train no.25540  loss = 3.57051 avg_loss = 3.97974\n",
      "epoch no.0 train no.25550  loss = 4.64072 avg_loss = 4.01417\n",
      "epoch no.0 train no.25560  loss = 3.86001 avg_loss = 4.00189\n",
      "epoch no.0 train no.25570  loss = 3.43407 avg_loss = 4.01199\n",
      "epoch no.0 train no.25580  loss = 3.74196 avg_loss = 4.01183\n",
      "epoch no.0 train no.25590  loss = 4.70963 avg_loss = 3.94574\n",
      "epoch no.0 train no.25600  loss = 3.57591 avg_loss = 3.89258\n",
      "epoch no.0 train no.25610  loss = 2.18581 avg_loss = 3.94314\n",
      "epoch no.0 train no.25620  loss = 3.04049 avg_loss = 3.98135\n",
      "epoch no.0 train no.25630  loss = 4.40824 avg_loss = 3.92884\n",
      "epoch no.0 train no.25640  loss = 3.17872 avg_loss = 3.94202\n",
      "epoch no.0 train no.25650  loss = 4.92278 avg_loss = 3.97381\n",
      "epoch no.0 train no.25660  loss = 5.71069 avg_loss = 3.95387\n",
      "epoch no.0 train no.25670  loss = 3.58017 avg_loss = 3.96744\n",
      "epoch no.0 train no.25680  loss = 5.42585 avg_loss = 3.98042\n",
      "epoch no.0 train no.25690  loss = 4.62173 avg_loss = 3.99667\n",
      "epoch no.0 train no.25700  loss = 4.16615 avg_loss = 4.01518\n",
      "epoch no.0 train no.25710  loss = 3.59195 avg_loss = 4.02085\n",
      "epoch no.0 train no.25720  loss = 6.35875 avg_loss = 4.03604\n",
      "epoch no.0 train no.25730  loss = 4.66454 avg_loss = 4.08733\n",
      "epoch no.0 train no.25740  loss = 3.22835 avg_loss = 4.11503\n",
      "epoch no.0 train no.25750  loss = 3.95116 avg_loss = 4.08071\n",
      "epoch no.0 train no.25760  loss = 2.84736 avg_loss = 4.12585\n",
      "epoch no.0 train no.25770  loss = 4.43584 avg_loss = 4.09932\n",
      "epoch no.0 train no.25780  loss = 3.61817 avg_loss = 4.12449\n",
      "epoch no.0 train no.25790  loss = 5.62431 avg_loss = 4.16754\n",
      "epoch no.0 train no.25800  loss = 5.54984 avg_loss = 4.16645\n",
      "epoch no.0 train no.25810  loss = 5.03832 avg_loss = 4.21624\n",
      "epoch no.0 train no.25820  loss = 3.95143 avg_loss = 4.17908\n",
      "epoch no.0 train no.25830  loss = 2.83049 avg_loss = 4.17924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.25840  loss = 2.56759 avg_loss = 4.15925\n",
      "epoch no.0 train no.25850  loss = 2.74166 avg_loss = 4.13619\n",
      "epoch no.0 train no.25860  loss = 2.93596 avg_loss = 4.11461\n",
      "epoch no.0 train no.25870  loss = 5.31702 avg_loss = 4.11763\n",
      "epoch no.0 train no.25880  loss = 3.68505 avg_loss = 4.08216\n",
      "epoch no.0 train no.25890  loss = 4.63653 avg_loss = 4.08066\n",
      "epoch no.0 train no.25900  loss = 5.69111 avg_loss = 4.10500\n",
      "epoch no.0 train no.25910  loss = 3.85973 avg_loss = 4.10513\n",
      "epoch no.0 train no.25920  loss = 2.94941 avg_loss = 4.11142\n",
      "epoch no.0 train no.25930  loss = 3.85749 avg_loss = 4.05287\n",
      "epoch no.0 train no.25940  loss = 4.06942 avg_loss = 4.04187\n",
      "epoch no.0 train no.25950  loss = 3.53598 avg_loss = 4.06098\n",
      "epoch no.0 train no.25960  loss = 3.57466 avg_loss = 3.98807\n",
      "epoch no.0 train no.25970  loss = 4.14403 avg_loss = 3.93298\n",
      "epoch no.0 train no.25980  loss = 3.06020 avg_loss = 4.00200\n",
      "epoch no.0 train no.25990  loss = 2.69404 avg_loss = 3.99001\n",
      "epoch no.0 train no.26000  loss = 4.80182 avg_loss = 3.99322\n",
      "2\n",
      "to_tokens: ['▁비', '▁팝', '들', '</s>']\n",
      "추억의 노래들</s>\n",
      "epoch no.0 train no.26010  loss = 3.20412 avg_loss = 4.03788\n",
      "epoch no.0 train no.26020  loss = 4.10551 avg_loss = 4.02630\n",
      "epoch no.0 train no.26030  loss = 2.95693 avg_loss = 4.03470\n",
      "epoch no.0 train no.26040  loss = 3.58045 avg_loss = 3.97582\n",
      "epoch no.0 train no.26050  loss = 2.91156 avg_loss = 3.93180\n",
      "epoch no.0 train no.26060  loss = 3.40883 avg_loss = 3.91680\n",
      "epoch no.0 train no.26070  loss = 4.21492 avg_loss = 3.87445\n",
      "epoch no.0 train no.26080  loss = 2.97902 avg_loss = 3.93737\n",
      "epoch no.0 train no.26090  loss = 4.25794 avg_loss = 3.98077\n",
      "epoch no.0 train no.26100  loss = 3.82636 avg_loss = 3.97075\n",
      "epoch no.0 train no.26110  loss = 4.09519 avg_loss = 3.93401\n",
      "epoch no.0 train no.26120  loss = 4.01317 avg_loss = 3.96294\n",
      "epoch no.0 train no.26130  loss = 3.73688 avg_loss = 3.93525\n",
      "epoch no.0 train no.26140  loss = 2.30466 avg_loss = 3.91818\n",
      "epoch no.0 train no.26150  loss = 4.10751 avg_loss = 3.99191\n",
      "epoch no.0 train no.26160  loss = 5.97430 avg_loss = 4.05109\n",
      "epoch no.0 train no.26170  loss = 4.78489 avg_loss = 4.03590\n",
      "epoch no.0 train no.26180  loss = 5.69118 avg_loss = 4.04922\n",
      "epoch no.0 train no.26190  loss = 3.08946 avg_loss = 4.00498\n",
      "epoch no.0 train no.26200  loss = 3.81413 avg_loss = 4.00247\n",
      "epoch no.0 train no.26210  loss = 4.69034 avg_loss = 4.01804\n",
      "epoch no.0 train no.26220  loss = 3.23176 avg_loss = 4.01079\n",
      "epoch no.0 train no.26230  loss = 4.62514 avg_loss = 4.00293\n",
      "epoch no.0 train no.26240  loss = 5.79369 avg_loss = 4.04476\n",
      "epoch no.0 train no.26250  loss = 3.41463 avg_loss = 4.02556\n",
      "epoch no.0 train no.26260  loss = 3.60661 avg_loss = 4.03365\n",
      "epoch no.0 train no.26270  loss = 4.61234 avg_loss = 4.02575\n",
      "epoch no.0 train no.26280  loss = 4.86497 avg_loss = 4.01006\n",
      "epoch no.0 train no.26290  loss = 7.21246 avg_loss = 4.11528\n",
      "epoch no.0 train no.26300  loss = 3.00962 avg_loss = 4.09003\n",
      "epoch no.0 train no.26310  loss = 4.93400 avg_loss = 4.08693\n",
      "epoch no.0 train no.26320  loss = 3.29257 avg_loss = 4.05941\n",
      "epoch no.0 train no.26330  loss = 3.19990 avg_loss = 4.07620\n",
      "epoch no.0 train no.26340  loss = 5.30106 avg_loss = 4.05938\n",
      "epoch no.0 train no.26350  loss = 3.60939 avg_loss = 4.08192\n",
      "epoch no.0 train no.26360  loss = 2.15391 avg_loss = 4.01493\n",
      "epoch no.0 train no.26370  loss = 3.56376 avg_loss = 4.00179\n",
      "epoch no.0 train no.26380  loss = 2.63418 avg_loss = 4.08577\n",
      "epoch no.0 train no.26390  loss = 4.79511 avg_loss = 4.07647\n",
      "epoch no.0 train no.26400  loss = 3.93941 avg_loss = 4.14030\n",
      "epoch no.0 train no.26410  loss = 2.95098 avg_loss = 4.11263\n",
      "epoch no.0 train no.26420  loss = 7.54645 avg_loss = 4.18503\n",
      "epoch no.0 train no.26430  loss = 5.59306 avg_loss = 4.18490\n",
      "epoch no.0 train no.26440  loss = 4.38514 avg_loss = 4.12466\n",
      "epoch no.0 train no.26450  loss = 2.02141 avg_loss = 4.11840\n",
      "epoch no.0 train no.26460  loss = 5.07266 avg_loss = 4.10636\n",
      "epoch no.0 train no.26470  loss = 3.98680 avg_loss = 4.03062\n",
      "epoch no.0 train no.26480  loss = 4.31429 avg_loss = 4.00154\n",
      "epoch no.0 train no.26490  loss = 3.10065 avg_loss = 3.98201\n",
      "epoch no.0 train no.26500  loss = 3.48872 avg_loss = 4.00991\n",
      "epoch no.0 train no.26510  loss = 6.90229 avg_loss = 4.01931\n",
      "epoch no.0 train no.26520  loss = 4.37693 avg_loss = 4.04999\n",
      "epoch no.0 train no.26530  loss = 6.24369 avg_loss = 4.09616\n",
      "epoch no.0 train no.26540  loss = 3.47255 avg_loss = 4.16442\n",
      "epoch no.0 train no.26550  loss = 4.25352 avg_loss = 4.15522\n",
      "epoch no.0 train no.26560  loss = 2.89587 avg_loss = 4.07460\n",
      "epoch no.0 train no.26570  loss = 3.57832 avg_loss = 4.05246\n",
      "epoch no.0 train no.26580  loss = 7.57938 avg_loss = 4.14956\n",
      "epoch no.0 train no.26590  loss = 4.14131 avg_loss = 4.18618\n",
      "epoch no.0 train no.26600  loss = 2.72006 avg_loss = 4.14506\n",
      "epoch no.0 train no.26610  loss = 2.91139 avg_loss = 4.08110\n",
      "epoch no.0 train no.26620  loss = 2.60931 avg_loss = 4.07321\n",
      "epoch no.0 train no.26630  loss = 2.68999 avg_loss = 4.04275\n",
      "epoch no.0 train no.26640  loss = 4.46213 avg_loss = 4.00521\n",
      "epoch no.0 train no.26650  loss = 4.90968 avg_loss = 4.00668\n",
      "epoch no.0 train no.26660  loss = 2.50754 avg_loss = 3.99701\n",
      "epoch no.0 train no.26670  loss = 3.08807 avg_loss = 3.98575\n",
      "epoch no.0 train no.26680  loss = 3.21967 avg_loss = 3.93777\n",
      "epoch no.0 train no.26690  loss = 6.27978 avg_loss = 3.95181\n",
      "epoch no.0 train no.26700  loss = 3.20228 avg_loss = 3.91673\n",
      "epoch no.0 train no.26710  loss = 3.65937 avg_loss = 3.91627\n",
      "epoch no.0 train no.26720  loss = 3.72466 avg_loss = 3.88516\n",
      "epoch no.0 train no.26730  loss = 5.17654 avg_loss = 3.89287\n",
      "epoch no.0 train no.26740  loss = 2.77482 avg_loss = 3.86930\n",
      "epoch no.0 train no.26750  loss = 2.59122 avg_loss = 3.81941\n",
      "epoch no.0 train no.26760  loss = 4.01756 avg_loss = 3.81332\n",
      "epoch no.0 train no.26770  loss = 3.94404 avg_loss = 3.80306\n",
      "epoch no.0 train no.26780  loss = 2.78958 avg_loss = 3.77709\n",
      "epoch no.0 train no.26790  loss = 6.55826 avg_loss = 3.89387\n",
      "epoch no.0 train no.26800  loss = 4.46837 avg_loss = 3.91225\n",
      "epoch no.0 train no.26810  loss = 3.89845 avg_loss = 3.87170\n",
      "epoch no.0 train no.26820  loss = 2.59377 avg_loss = 3.83853\n",
      "epoch no.0 train no.26830  loss = 3.47217 avg_loss = 3.84840\n",
      "epoch no.0 train no.26840  loss = 2.13555 avg_loss = 3.80068\n",
      "epoch no.0 train no.26850  loss = 3.44646 avg_loss = 3.80417\n",
      "epoch no.0 train no.26860  loss = 2.54212 avg_loss = 3.81926\n",
      "epoch no.0 train no.26870  loss = 5.41195 avg_loss = 3.80523\n",
      "epoch no.0 train no.26880  loss = 4.89801 avg_loss = 3.85749\n",
      "epoch no.0 train no.26890  loss = 3.88479 avg_loss = 3.85165\n",
      "epoch no.0 train no.26900  loss = 2.32266 avg_loss = 3.79859\n",
      "epoch no.0 train no.26910  loss = 3.77660 avg_loss = 3.83296\n",
      "epoch no.0 train no.26920  loss = 5.19284 avg_loss = 3.83524\n",
      "epoch no.0 train no.26930  loss = 3.09594 avg_loss = 3.78758\n",
      "epoch no.0 train no.26940  loss = 3.95606 avg_loss = 3.74724\n",
      "epoch no.0 train no.26950  loss = 3.19810 avg_loss = 3.72572\n",
      "epoch no.0 train no.26960  loss = 5.41082 avg_loss = 3.74254\n",
      "epoch no.0 train no.26970  loss = 2.16449 avg_loss = 3.70879\n",
      "epoch no.0 train no.26980  loss = 3.24382 avg_loss = 3.66111\n",
      "epoch no.0 train no.26990  loss = 2.20269 avg_loss = 3.65354\n",
      "epoch no.0 train no.27000  loss = 5.23438 avg_loss = 3.65863\n",
      "3\n",
      "to_tokens: ['▁가을', '▁팝', '▁o', 'st', '</s>']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.0 train no.27010  loss = 1.97128 avg_loss = 3.68509\n",
      "epoch no.0 train no.27020  loss = 3.33773 avg_loss = 3.69804\n",
      "epoch no.0 train no.27030  loss = 3.48292 avg_loss = 3.75977\n",
      "epoch no.0 train no.27040  loss = 7.57276 avg_loss = 3.84936\n",
      "epoch no.0 train no.27050  loss = 4.01400 avg_loss = 3.85073\n",
      "epoch no.0 train no.27060  loss = 3.24390 avg_loss = 3.88069\n",
      "epoch no.0 train no.27070  loss = 2.18646 avg_loss = 3.78624\n",
      "epoch no.0 train no.27080  loss = 5.84134 avg_loss = 3.79630\n",
      "epoch no.0 train no.27090  loss = 2.88267 avg_loss = 3.74066\n",
      "epoch no.0 train no.27100  loss = 5.50863 avg_loss = 3.80893\n",
      "epoch no.0 train no.27110  loss = 3.65245 avg_loss = 3.76981\n",
      "epoch no.0 train no.27120  loss = 3.07982 avg_loss = 3.75989\n",
      "epoch no.0 train no.27130  loss = 2.39008 avg_loss = 3.72015\n",
      "epoch no.0 train no.27140  loss = 2.99517 avg_loss = 3.76801\n",
      "epoch no.0 train no.27150  loss = 4.60272 avg_loss = 3.81664\n",
      "epoch no.0 train no.27160  loss = 3.47787 avg_loss = 3.82182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.27170  loss = 5.48310 avg_loss = 3.77983\n",
      "epoch no.0 train no.27180  loss = 4.39588 avg_loss = 3.80364\n",
      "epoch no.0 train no.27190  loss = 2.34372 avg_loss = 3.85548\n",
      "epoch no.0 train no.27200  loss = 3.50562 avg_loss = 3.87148\n",
      "epoch no.0 train no.27210  loss = 3.11125 avg_loss = 3.91943\n",
      "epoch no.0 train no.27220  loss = 2.61196 avg_loss = 3.96512\n",
      "epoch no.0 train no.27230  loss = 6.83848 avg_loss = 3.93335\n",
      "epoch no.0 train no.27240  loss = 3.54726 avg_loss = 3.92567\n",
      "epoch no.0 train no.27250  loss = 2.54830 avg_loss = 3.91987\n",
      "epoch no.0 train no.27260  loss = 4.89793 avg_loss = 3.95280\n",
      "epoch no.0 train no.27270  loss = 2.75856 avg_loss = 3.96066\n",
      "epoch no.0 train no.27280  loss = 3.57344 avg_loss = 3.99407\n",
      "epoch no.0 train no.27290  loss = 7.37642 avg_loss = 4.00475\n",
      "epoch no.0 train no.27300  loss = 5.07262 avg_loss = 3.99785\n",
      "epoch no.0 train no.27310  loss = 3.14155 avg_loss = 3.98950\n",
      "epoch no.0 train no.27320  loss = 2.51087 avg_loss = 3.96597\n",
      "epoch no.0 train no.27330  loss = 3.98100 avg_loss = 3.93049\n",
      "epoch no.0 train no.27340  loss = 5.83215 avg_loss = 3.98517\n",
      "epoch no.0 train no.27350  loss = 3.68676 avg_loss = 3.95125\n",
      "epoch no.0 train no.27360  loss = 7.11777 avg_loss = 3.97602\n",
      "epoch no.0 train no.27370  loss = 5.72057 avg_loss = 3.94709\n",
      "epoch no.0 train no.27380  loss = 2.84582 avg_loss = 3.90732\n",
      "epoch no.0 train no.27390  loss = 4.90404 avg_loss = 3.90953\n",
      "epoch no.0 train no.27400  loss = 4.27147 avg_loss = 3.90921\n",
      "epoch no.0 train no.27410  loss = 2.63965 avg_loss = 3.89361\n",
      "epoch no.0 train no.27420  loss = 4.30645 avg_loss = 3.92188\n",
      "epoch no.0 train no.27430  loss = 5.37204 avg_loss = 3.94781\n",
      "epoch no.0 train no.27440  loss = 4.48744 avg_loss = 3.91990\n",
      "epoch no.0 train no.27450  loss = 3.38251 avg_loss = 3.95673\n",
      "epoch no.0 train no.27460  loss = 2.81821 avg_loss = 3.91196\n",
      "epoch no.0 train no.27470  loss = 3.18373 avg_loss = 3.89168\n",
      "epoch no.0 train no.27480  loss = 4.27352 avg_loss = 3.93435\n",
      "epoch no.0 train no.27490  loss = 6.15477 avg_loss = 3.94985\n",
      "epoch no.0 train no.27500  loss = 5.02825 avg_loss = 3.97711\n",
      "epoch no.0 train no.27510  loss = 3.55002 avg_loss = 3.95694\n",
      "epoch no.0 train no.27520  loss = 4.92574 avg_loss = 3.98699\n",
      "epoch no.0 train no.27530  loss = 4.67773 avg_loss = 4.03009\n",
      "epoch no.0 train no.27540  loss = 2.56483 avg_loss = 4.00791\n",
      "epoch no.0 train no.27550  loss = 2.03596 avg_loss = 4.00480\n",
      "epoch no.0 train no.27560  loss = 2.47844 avg_loss = 4.02208\n",
      "epoch no.0 train no.27570  loss = 3.64986 avg_loss = 4.05125\n",
      "epoch no.0 train no.27580  loss = 4.30760 avg_loss = 4.03983\n",
      "epoch no.0 train no.27590  loss = 5.27955 avg_loss = 4.06874\n",
      "epoch no.0 train no.27600  loss = 2.70195 avg_loss = 4.01703\n",
      "epoch no.0 train no.27610  loss = 2.51178 avg_loss = 3.97235\n",
      "epoch no.0 train no.27620  loss = 2.72304 avg_loss = 3.99425\n",
      "epoch no.0 train no.27630  loss = 3.72347 avg_loss = 3.97289\n",
      "epoch no.0 train no.27640  loss = 4.02184 avg_loss = 4.01376\n",
      "epoch no.0 train no.27650  loss = 4.01070 avg_loss = 3.92221\n",
      "epoch no.0 train no.27660  loss = 4.22692 avg_loss = 3.95450\n",
      "epoch no.0 train no.27670  loss = 3.45320 avg_loss = 3.96433\n",
      "epoch no.0 train no.27680  loss = 5.55121 avg_loss = 3.98735\n",
      "epoch no.0 train no.27690  loss = 3.26136 avg_loss = 4.05604\n",
      "epoch no.0 train no.27700  loss = 4.45820 avg_loss = 4.08659\n",
      "epoch no.0 train no.27710  loss = 3.67525 avg_loss = 4.08008\n",
      "epoch no.0 train no.27720  loss = 5.39154 avg_loss = 4.08082\n",
      "epoch no.0 train no.27730  loss = 3.68153 avg_loss = 4.03573\n",
      "epoch no.0 train no.27740  loss = 4.58721 avg_loss = 4.07237\n",
      "epoch no.0 train no.27750  loss = 3.19290 avg_loss = 4.06019\n",
      "epoch no.0 train no.27760  loss = 4.80254 avg_loss = 4.05687\n",
      "epoch no.0 train no.27770  loss = 2.82900 avg_loss = 4.09621\n",
      "epoch no.0 train no.27780  loss = 2.67269 avg_loss = 4.09333\n",
      "epoch no.0 train no.27790  loss = 6.31847 avg_loss = 4.19401\n",
      "epoch no.0 train no.27800  loss = 5.31688 avg_loss = 4.17495\n",
      "epoch no.0 train no.27810  loss = 3.21274 avg_loss = 4.13419\n",
      "epoch no.0 train no.27820  loss = 2.98153 avg_loss = 4.19435\n",
      "epoch no.0 train no.27830  loss = 3.29895 avg_loss = 4.18893\n",
      "epoch no.0 train no.27840  loss = 4.48760 avg_loss = 4.19105\n",
      "epoch no.0 train no.27850  loss = 3.28607 avg_loss = 4.15592\n",
      "epoch no.0 train no.27860  loss = 2.77236 avg_loss = 4.10889\n",
      "epoch no.0 train no.27870  loss = 2.12472 avg_loss = 4.08809\n",
      "epoch no.0 train no.27880  loss = 4.33804 avg_loss = 4.05954\n",
      "epoch no.0 train no.27890  loss = 3.71887 avg_loss = 4.01040\n",
      "epoch no.0 train no.27900  loss = 3.87794 avg_loss = 4.01956\n",
      "epoch no.0 train no.27910  loss = 3.24843 avg_loss = 3.96777\n",
      "epoch no.0 train no.27920  loss = 3.87659 avg_loss = 3.97882\n",
      "epoch no.0 train no.27930  loss = 1.61958 avg_loss = 3.96533\n",
      "epoch no.0 train no.27940  loss = 2.00366 avg_loss = 3.90578\n",
      "epoch no.0 train no.27950  loss = 2.97495 avg_loss = 3.86229\n",
      "epoch no.0 train no.27960  loss = 3.96362 avg_loss = 3.83726\n",
      "epoch no.0 train no.27970  loss = 3.50978 avg_loss = 3.86833\n",
      "epoch no.0 train no.27980  loss = 5.43987 avg_loss = 3.91956\n",
      "epoch no.0 train no.27990  loss = 3.06038 avg_loss = 3.95168\n",
      "epoch no.0 train no.28000  loss = 4.05442 avg_loss = 3.96481\n",
      "3\n",
      "to_tokens: ['▁비', '▁명', 'st', '▁모음', '</s>']\n",
      "추억의 ost 모음</s>\n",
      "epoch no.0 train no.28010  loss = 2.40180 avg_loss = 3.94020\n",
      "epoch no.0 train no.28020  loss = 4.55770 avg_loss = 3.92063\n",
      "epoch no.0 train no.28030  loss = 3.93648 avg_loss = 3.91561\n",
      "epoch no.0 train no.28040  loss = 4.39515 avg_loss = 3.92529\n",
      "epoch no.0 train no.28050  loss = 3.23420 avg_loss = 3.86829\n",
      "epoch no.0 train no.28060  loss = 2.36482 avg_loss = 3.89354\n",
      "epoch no.0 train no.28070  loss = 3.66472 avg_loss = 3.87457\n",
      "epoch no.0 train no.28080  loss = 2.95235 avg_loss = 3.87106\n",
      "epoch no.0 train no.28090  loss = 3.74714 avg_loss = 3.89713\n",
      "epoch no.0 train no.28100  loss = 2.78102 avg_loss = 3.94437\n",
      "epoch no.0 train no.28110  loss = 3.42002 avg_loss = 3.91881\n",
      "epoch no.0 train no.28120  loss = 4.09681 avg_loss = 3.91762\n",
      "epoch no.0 train no.28130  loss = 2.69809 avg_loss = 3.91170\n",
      "epoch no.0 train no.28140  loss = 4.87302 avg_loss = 3.88681\n",
      "epoch no.0 train no.28150  loss = 4.26894 avg_loss = 3.87238\n",
      "epoch no.0 train no.28160  loss = 1.85178 avg_loss = 3.81265\n",
      "epoch no.0 train no.28170  loss = 3.08956 avg_loss = 3.78066\n",
      "epoch no.0 train no.28180  loss = 5.00578 avg_loss = 3.75127\n",
      "epoch no.0 train no.28190  loss = 3.22283 avg_loss = 3.81182\n",
      "epoch no.0 train no.28200  loss = 3.29525 avg_loss = 3.83798\n",
      "epoch no.0 train no.28210  loss = 3.64379 avg_loss = 3.88495\n",
      "epoch no.0 train no.28220  loss = 2.34246 avg_loss = 3.84678\n",
      "epoch no.0 train no.28230  loss = 4.73158 avg_loss = 3.83427\n",
      "epoch no.0 train no.28240  loss = 3.32374 avg_loss = 3.83863\n",
      "epoch no.0 train no.28250  loss = 4.45014 avg_loss = 3.79371\n",
      "epoch no.0 train no.28260  loss = 4.12135 avg_loss = 3.79839\n",
      "epoch no.0 train no.28270  loss = 3.71194 avg_loss = 3.78177\n",
      "epoch no.0 train no.28280  loss = 2.56445 avg_loss = 3.83977\n",
      "epoch no.0 train no.28290  loss = 2.80327 avg_loss = 3.83937\n",
      "epoch no.0 train no.28300  loss = 3.49208 avg_loss = 3.83354\n",
      "epoch no.0 train no.28310  loss = 2.85943 avg_loss = 3.81274\n",
      "epoch no.0 train no.28320  loss = 4.49304 avg_loss = 3.88831\n",
      "epoch no.0 train no.28330  loss = 5.83678 avg_loss = 3.92496\n",
      "epoch no.0 train no.28340  loss = 3.66572 avg_loss = 3.86715\n",
      "epoch no.0 train no.28350  loss = 3.98215 avg_loss = 3.90297\n",
      "epoch no.0 train no.28360  loss = 3.27598 avg_loss = 3.87717\n",
      "epoch no.0 train no.28370  loss = 2.09847 avg_loss = 3.86453\n",
      "epoch no.0 train no.28380  loss = 3.63316 avg_loss = 3.85337\n",
      "epoch no.0 train no.28390  loss = 5.62577 avg_loss = 3.88082\n",
      "epoch no.0 train no.28400  loss = 5.21132 avg_loss = 3.90072\n",
      "epoch no.0 train no.28410  loss = 2.60193 avg_loss = 3.93034\n",
      "epoch no.0 train no.28420  loss = 3.90610 avg_loss = 3.93379\n",
      "epoch no.0 train no.28430  loss = 4.92177 avg_loss = 3.97477\n",
      "epoch no.0 train no.28440  loss = 1.76532 avg_loss = 3.99589\n",
      "epoch no.0 train no.28450  loss = 4.14649 avg_loss = 3.97304\n",
      "epoch no.0 train no.28460  loss = 4.19482 avg_loss = 3.95867\n",
      "epoch no.0 train no.28470  loss = 4.44445 avg_loss = 4.00083\n",
      "epoch no.0 train no.28480  loss = 4.69230 avg_loss = 3.96976\n",
      "epoch no.0 train no.28490  loss = 3.24401 avg_loss = 3.98952\n",
      "epoch no.0 train no.28500  loss = 1.82169 avg_loss = 3.96310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.28510  loss = 3.79216 avg_loss = 3.95844\n",
      "epoch no.0 train no.28520  loss = 4.04922 avg_loss = 3.95412\n",
      "epoch no.0 train no.28530  loss = 7.47081 avg_loss = 4.04485\n",
      "epoch no.0 train no.28540  loss = 4.03393 avg_loss = 4.06667\n",
      "epoch no.0 train no.28550  loss = 3.76321 avg_loss = 4.09589\n",
      "epoch no.0 train no.28560  loss = 3.71721 avg_loss = 4.05639\n",
      "epoch no.0 train no.28570  loss = 3.93170 avg_loss = 4.10900\n",
      "epoch no.0 train no.28580  loss = 5.33178 avg_loss = 4.11782\n",
      "epoch no.0 train no.28590  loss = 3.02765 avg_loss = 4.12741\n",
      "epoch no.0 train no.28600  loss = 2.89123 avg_loss = 4.03708\n",
      "epoch no.0 train no.28610  loss = 3.59317 avg_loss = 4.02345\n",
      "epoch no.0 train no.28620  loss = 3.46198 avg_loss = 4.02208\n",
      "epoch no.0 train no.28630  loss = 2.30968 avg_loss = 4.00775\n",
      "epoch no.0 train no.28640  loss = 4.27388 avg_loss = 3.99599\n",
      "epoch no.0 train no.28650  loss = 1.59156 avg_loss = 4.00773\n",
      "epoch no.0 train no.28660  loss = 3.67825 avg_loss = 4.02220\n",
      "epoch no.0 train no.28670  loss = 3.57085 avg_loss = 4.02918\n",
      "epoch no.0 train no.28680  loss = 4.90969 avg_loss = 4.04230\n",
      "epoch no.0 train no.28690  loss = 4.35839 avg_loss = 4.08183\n",
      "epoch no.0 train no.28700  loss = 4.30759 avg_loss = 4.08849\n",
      "epoch no.0 train no.28710  loss = 2.42251 avg_loss = 4.09080\n",
      "epoch no.0 train no.28720  loss = 5.39895 avg_loss = 4.08165\n",
      "epoch no.0 train no.28730  loss = 2.92903 avg_loss = 3.99893\n",
      "epoch no.0 train no.28740  loss = 1.87815 avg_loss = 3.95526\n",
      "epoch no.0 train no.28750  loss = 4.32790 avg_loss = 3.98817\n",
      "epoch no.0 train no.28760  loss = 3.96751 avg_loss = 3.97945\n",
      "epoch no.0 train no.28770  loss = 3.04292 avg_loss = 3.97055\n",
      "epoch no.0 train no.28780  loss = 3.59545 avg_loss = 3.97122\n",
      "epoch no.0 train no.28790  loss = 1.69959 avg_loss = 3.93521\n",
      "epoch no.0 train no.28800  loss = 3.95212 avg_loss = 3.96133\n",
      "epoch no.0 train no.28810  loss = 4.43213 avg_loss = 3.97642\n",
      "epoch no.0 train no.28820  loss = 3.43055 avg_loss = 3.99958\n",
      "epoch no.0 train no.28830  loss = 4.16196 avg_loss = 4.06987\n",
      "epoch no.0 train no.28840  loss = 2.79187 avg_loss = 4.06835\n",
      "epoch no.0 train no.28850  loss = 4.13174 avg_loss = 4.08296\n",
      "epoch no.0 train no.28860  loss = 2.68178 avg_loss = 4.05513\n",
      "epoch no.0 train no.28870  loss = 3.48837 avg_loss = 4.03748\n",
      "epoch no.0 train no.28880  loss = 4.47828 avg_loss = 4.04386\n",
      "epoch no.0 train no.28890  loss = 4.56960 avg_loss = 4.03658\n",
      "epoch no.0 train no.28900  loss = 3.74009 avg_loss = 4.04396\n",
      "epoch no.0 train no.28910  loss = 4.04659 avg_loss = 4.06975\n",
      "epoch no.0 train no.28920  loss = 2.67374 avg_loss = 4.04740\n",
      "epoch no.0 train no.28930  loss = 4.41209 avg_loss = 3.97523\n",
      "epoch no.0 train no.28940  loss = 4.86648 avg_loss = 4.00706\n",
      "epoch no.0 train no.28950  loss = 3.50882 avg_loss = 3.95578\n",
      "epoch no.0 train no.28960  loss = 3.44473 avg_loss = 3.91705\n",
      "epoch no.0 train no.28970  loss = 3.31463 avg_loss = 3.89456\n",
      "epoch no.0 train no.28980  loss = 3.73811 avg_loss = 3.94690\n",
      "epoch no.0 train no.28990  loss = 5.35939 avg_loss = 4.00865\n",
      "epoch no.0 train no.29000  loss = 5.10833 avg_loss = 4.00339\n",
      "3\n",
      "to_tokens: ['▁가을', '▁노래', '곡', '들', '</s>']\n",
      "추억의 명곡 베스트</s>\n",
      "epoch no.0 train no.29010  loss = 2.84163 avg_loss = 3.98515\n",
      "epoch no.0 train no.29020  loss = 4.75594 avg_loss = 4.00252\n",
      "epoch no.0 train no.29030  loss = 2.26341 avg_loss = 3.96143\n",
      "epoch no.0 train no.29040  loss = 2.73461 avg_loss = 3.94083\n",
      "epoch no.0 train no.29050  loss = 3.71634 avg_loss = 3.99953\n",
      "epoch no.0 train no.29060  loss = 2.73329 avg_loss = 3.93586\n",
      "epoch no.0 train no.29070  loss = 4.21779 avg_loss = 3.93342\n",
      "epoch no.0 train no.29080  loss = 5.22503 avg_loss = 3.93238\n",
      "epoch no.0 train no.29090  loss = 6.22802 avg_loss = 3.94203\n",
      "epoch no.0 train no.29100  loss = 6.19090 avg_loss = 4.00168\n",
      "epoch no.0 train no.29110  loss = 5.55687 avg_loss = 4.02079\n",
      "epoch no.0 train no.29120  loss = 3.08182 avg_loss = 3.99730\n",
      "epoch no.0 train no.29130  loss = 4.50237 avg_loss = 4.00664\n",
      "epoch no.0 train no.29140  loss = 3.70830 avg_loss = 4.02241\n",
      "epoch no.0 train no.29150  loss = 3.65944 avg_loss = 3.96168\n",
      "epoch no.0 train no.29160  loss = 4.89511 avg_loss = 3.94753\n",
      "epoch no.0 train no.29170  loss = 5.81544 avg_loss = 3.98004\n",
      "epoch no.0 train no.29180  loss = 4.95160 avg_loss = 3.91803\n",
      "epoch no.0 train no.29190  loss = 4.37623 avg_loss = 3.85858\n",
      "epoch no.0 train no.29200  loss = 3.43836 avg_loss = 3.87309\n",
      "epoch no.0 train no.29210  loss = 3.91532 avg_loss = 3.88816\n",
      "epoch no.0 train no.29220  loss = 5.56759 avg_loss = 3.94073\n",
      "epoch no.0 train no.29230  loss = 3.34774 avg_loss = 3.89809\n",
      "epoch no.0 train no.29240  loss = 5.05213 avg_loss = 3.83947\n",
      "epoch no.0 train no.29250  loss = 3.89799 avg_loss = 3.83524\n",
      "epoch no.0 train no.29260  loss = 3.45925 avg_loss = 3.83266\n",
      "epoch no.0 train no.29270  loss = 2.65937 avg_loss = 3.85998\n",
      "epoch no.0 train no.29280  loss = 6.40075 avg_loss = 3.90620\n",
      "epoch no.0 train no.29290  loss = 5.08855 avg_loss = 3.93829\n",
      "epoch no.0 train no.29300  loss = 4.19847 avg_loss = 3.93086\n",
      "epoch no.0 train no.29310  loss = 5.47089 avg_loss = 3.89515\n",
      "epoch no.0 train no.29320  loss = 3.02161 avg_loss = 3.85165\n",
      "epoch no.0 train no.29330  loss = 6.21544 avg_loss = 3.93652\n",
      "epoch no.0 train no.29340  loss = 3.49086 avg_loss = 3.96295\n",
      "epoch no.0 train no.29350  loss = 4.34887 avg_loss = 3.95000\n",
      "epoch no.0 train no.29360  loss = 3.72995 avg_loss = 3.96695\n",
      "epoch no.0 train no.29370  loss = 3.58093 avg_loss = 3.93394\n",
      "epoch no.0 train no.29380  loss = 3.06095 avg_loss = 3.94690\n",
      "epoch no.0 train no.29390  loss = 4.53012 avg_loss = 3.89031\n",
      "epoch no.0 train no.29400  loss = 5.77502 avg_loss = 3.88336\n",
      "epoch no.0 train no.29410  loss = 4.72903 avg_loss = 3.90077\n",
      "epoch no.0 train no.29420  loss = 2.38404 avg_loss = 3.91193\n",
      "epoch no.0 train no.29430  loss = 6.77564 avg_loss = 3.96352\n",
      "epoch no.0 train no.29440  loss = 2.43947 avg_loss = 3.95686\n",
      "epoch no.0 train no.29450  loss = 4.23711 avg_loss = 3.98013\n",
      "epoch no.0 train no.29460  loss = 3.32812 avg_loss = 3.97730\n",
      "epoch no.0 train no.29470  loss = 6.09028 avg_loss = 3.97683\n",
      "epoch no.0 train no.29480  loss = 5.75600 avg_loss = 4.05736\n",
      "epoch no.0 train no.29490  loss = 6.26531 avg_loss = 4.08944\n",
      "epoch no.0 train no.29500  loss = 5.11276 avg_loss = 4.15235\n",
      "epoch no.0 train no.29510  loss = 5.58291 avg_loss = 4.14046\n",
      "epoch no.0 train no.29520  loss = 2.59356 avg_loss = 4.17300\n",
      "epoch no.0 train no.29530  loss = 4.79972 avg_loss = 4.21282\n",
      "epoch no.0 train no.29540  loss = 4.33987 avg_loss = 4.24687\n",
      "epoch no.0 train no.29550  loss = 6.24853 avg_loss = 4.26367\n",
      "epoch no.0 train no.29560  loss = 1.61794 avg_loss = 4.21612\n",
      "epoch no.0 train no.29570  loss = 3.28337 avg_loss = 4.20162\n",
      "epoch no.0 train no.29580  loss = 4.50820 avg_loss = 4.17546\n",
      "epoch no.0 train no.29590  loss = 3.56122 avg_loss = 4.18116\n",
      "epoch no.0 train no.29600  loss = 4.62931 avg_loss = 4.18179\n",
      "epoch no.0 train no.29610  loss = 3.87859 avg_loss = 4.15172\n",
      "epoch no.0 train no.29620  loss = 2.33996 avg_loss = 4.12023\n",
      "epoch no.0 train no.29630  loss = 10.17424 avg_loss = 4.22576\n",
      "epoch no.0 train no.29640  loss = 4.77322 avg_loss = 4.21809\n",
      "epoch no.0 train no.29650  loss = 4.59250 avg_loss = 4.18347\n",
      "epoch no.0 train no.29660  loss = 3.11078 avg_loss = 4.19450\n",
      "epoch no.0 train no.29670  loss = 2.53775 avg_loss = 4.12489\n",
      "epoch no.0 train no.29680  loss = 2.06616 avg_loss = 4.08880\n",
      "epoch no.0 train no.29690  loss = 2.49925 avg_loss = 4.08910\n",
      "epoch no.0 train no.29700  loss = 4.88365 avg_loss = 4.06593\n",
      "epoch no.0 train no.29710  loss = 4.80094 avg_loss = 4.04795\n",
      "epoch no.0 train no.29720  loss = 2.64578 avg_loss = 4.03761\n",
      "epoch no.0 train no.29730  loss = 4.15460 avg_loss = 4.01264\n",
      "epoch no.0 train no.29740  loss = 5.34862 avg_loss = 4.02403\n",
      "epoch no.0 train no.29750  loss = 3.86055 avg_loss = 4.00753\n",
      "epoch no.0 train no.29760  loss = 2.91494 avg_loss = 3.96871\n",
      "epoch no.0 train no.29770  loss = 4.02114 avg_loss = 3.99534\n",
      "epoch no.0 train no.29780  loss = 1.88128 avg_loss = 3.94597\n",
      "epoch no.0 train no.29790  loss = 5.48245 avg_loss = 3.96103\n",
      "epoch no.0 train no.29800  loss = 2.68811 avg_loss = 3.92891\n",
      "epoch no.0 train no.29810  loss = 3.62499 avg_loss = 3.92032\n",
      "epoch no.0 train no.29820  loss = 3.27939 avg_loss = 3.88241\n",
      "epoch no.0 train no.29830  loss = 5.84147 avg_loss = 3.83893\n",
      "epoch no.0 train no.29840  loss = 6.97960 avg_loss = 3.86037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.29850  loss = 4.76013 avg_loss = 3.82441\n",
      "epoch no.0 train no.29860  loss = 4.07071 avg_loss = 3.86082\n",
      "epoch no.0 train no.29870  loss = 4.88134 avg_loss = 3.84382\n",
      "epoch no.0 train no.29880  loss = 2.65904 avg_loss = 3.80183\n",
      "epoch no.0 train no.29890  loss = 3.53797 avg_loss = 3.86699\n",
      "epoch no.0 train no.29900  loss = 3.52182 avg_loss = 3.90636\n",
      "epoch no.0 train no.29910  loss = 2.39610 avg_loss = 3.87828\n",
      "epoch no.0 train no.29920  loss = 4.90383 avg_loss = 3.91000\n",
      "epoch no.0 train no.29930  loss = 3.86473 avg_loss = 3.87227\n",
      "epoch no.0 train no.29940  loss = 2.40889 avg_loss = 3.83767\n",
      "epoch no.0 train no.29950  loss = 4.58651 avg_loss = 3.80806\n",
      "epoch no.0 train no.29960  loss = 4.59455 avg_loss = 3.87730\n",
      "epoch no.0 train no.29970  loss = 5.03822 avg_loss = 3.95065\n",
      "epoch no.0 train no.29980  loss = 2.57311 avg_loss = 3.88969\n",
      "epoch no.0 train no.29990  loss = 3.58850 avg_loss = 3.88284\n",
      "epoch no.0 train no.30000  loss = 2.96763 avg_loss = 3.90893\n",
      "2\n",
      "to_tokens: ['▁비', '▁노래', '들', '</s>']\n",
      "추억의 노래들</s>\n",
      "epoch no.0 train no.30010  loss = 2.91458 avg_loss = 3.95399\n",
      "epoch no.0 train no.30020  loss = 3.13430 avg_loss = 3.94468\n",
      "epoch no.0 train no.30030  loss = 2.12215 avg_loss = 3.98015\n",
      "epoch no.0 train no.30040  loss = 2.92759 avg_loss = 3.97410\n",
      "epoch no.0 train no.30050  loss = 2.85219 avg_loss = 4.00136\n",
      "epoch no.0 train no.30060  loss = 3.52161 avg_loss = 3.96936\n",
      "epoch no.0 train no.30070  loss = 4.16611 avg_loss = 3.95499\n",
      "epoch no.0 train no.30080  loss = 4.32443 avg_loss = 3.99621\n",
      "epoch no.0 train no.30090  loss = 4.47097 avg_loss = 3.97099\n",
      "epoch no.0 train no.30100  loss = 3.57398 avg_loss = 3.95369\n",
      "epoch no.0 train no.30110  loss = 5.57370 avg_loss = 3.96520\n",
      "epoch no.0 train no.30120  loss = 4.56585 avg_loss = 3.97600\n",
      "epoch no.0 train no.30130  loss = 2.44770 avg_loss = 3.91812\n",
      "epoch no.0 train no.30140  loss = 3.69975 avg_loss = 3.93111\n",
      "epoch no.0 train no.30150  loss = 3.02695 avg_loss = 3.96954\n",
      "epoch no.0 train no.30160  loss = 5.85183 avg_loss = 3.98259\n",
      "epoch no.0 train no.30170  loss = 4.10268 avg_loss = 4.00671\n",
      "epoch no.0 train no.30180  loss = 3.87230 avg_loss = 4.00426\n",
      "epoch no.0 train no.30190  loss = 5.76572 avg_loss = 4.06588\n",
      "epoch no.0 train no.30200  loss = 1.68031 avg_loss = 4.02694\n",
      "epoch no.0 train no.30210  loss = 4.28596 avg_loss = 4.06285\n",
      "epoch no.0 train no.30220  loss = 4.11987 avg_loss = 4.01726\n",
      "epoch no.0 train no.30230  loss = 2.30952 avg_loss = 3.99378\n",
      "epoch no.0 train no.30240  loss = 5.17099 avg_loss = 4.02177\n",
      "epoch no.0 train no.30250  loss = 5.96755 avg_loss = 3.98502\n",
      "epoch no.0 train no.30260  loss = 3.21937 avg_loss = 4.01398\n",
      "epoch no.0 train no.30270  loss = 4.11556 avg_loss = 3.98541\n",
      "epoch no.0 train no.30280  loss = 5.74211 avg_loss = 3.99073\n",
      "epoch no.0 train no.30290  loss = 3.41412 avg_loss = 4.02414\n",
      "epoch no.0 train no.30300  loss = 4.82771 avg_loss = 4.03573\n",
      "epoch no.0 train no.30310  loss = 7.16445 avg_loss = 4.07224\n",
      "epoch no.0 train no.30320  loss = 5.07709 avg_loss = 4.07780\n",
      "epoch no.0 train no.30330  loss = 6.15942 avg_loss = 4.12655\n",
      "epoch no.0 train no.30340  loss = 4.88835 avg_loss = 4.13736\n",
      "epoch no.0 train no.30350  loss = 3.49774 avg_loss = 4.17656\n",
      "epoch no.0 train no.30360  loss = 4.34208 avg_loss = 4.14036\n",
      "epoch no.0 train no.30370  loss = 3.14981 avg_loss = 4.05836\n",
      "epoch no.0 train no.30380  loss = 3.04963 avg_loss = 4.04444\n",
      "epoch no.0 train no.30390  loss = 4.44528 avg_loss = 4.04661\n",
      "epoch no.0 train no.30400  loss = 3.43683 avg_loss = 4.07004\n",
      "epoch no.0 train no.30410  loss = 2.43329 avg_loss = 4.04821\n",
      "epoch no.0 train no.30420  loss = 6.41311 avg_loss = 4.06854\n",
      "epoch no.0 train no.30430  loss = 1.99730 avg_loss = 4.02877\n",
      "epoch no.0 train no.30440  loss = 4.46939 avg_loss = 3.99600\n",
      "epoch no.0 train no.30450  loss = 4.00022 avg_loss = 4.03805\n",
      "epoch no.0 train no.30460  loss = 5.49642 avg_loss = 4.01432\n",
      "epoch no.0 train no.30470  loss = 2.99722 avg_loss = 4.04905\n",
      "epoch no.0 train no.30480  loss = 2.30303 avg_loss = 4.04844\n",
      "epoch no.0 train no.30490  loss = 4.75125 avg_loss = 4.06572\n",
      "epoch no.0 train no.30500  loss = 3.65188 avg_loss = 4.01363\n",
      "epoch no.0 train no.30510  loss = 5.80271 avg_loss = 4.00649\n",
      "epoch no.0 train no.30520  loss = 4.33156 avg_loss = 3.99856\n",
      "epoch no.0 train no.30530  loss = 7.03132 avg_loss = 3.97160\n",
      "epoch no.0 train no.30540  loss = 3.50234 avg_loss = 4.05499\n",
      "epoch no.0 train no.30550  loss = 6.76857 avg_loss = 4.10142\n",
      "epoch no.0 train no.30560  loss = 6.18921 avg_loss = 4.13685\n",
      "epoch no.0 train no.30570  loss = 3.08485 avg_loss = 4.14148\n",
      "epoch no.0 train no.30580  loss = 3.61798 avg_loss = 4.13720\n",
      "epoch no.0 train no.30590  loss = 4.21586 avg_loss = 4.05738\n",
      "epoch no.0 train no.30600  loss = 5.76158 avg_loss = 4.05471\n",
      "epoch no.0 train no.30610  loss = 3.76589 avg_loss = 4.05059\n",
      "epoch no.0 train no.30620  loss = 4.25228 avg_loss = 3.96488\n",
      "epoch no.0 train no.30630  loss = 2.51036 avg_loss = 3.91857\n",
      "epoch no.0 train no.30640  loss = 4.28596 avg_loss = 3.88692\n",
      "epoch no.0 train no.30650  loss = 3.54644 avg_loss = 3.95777\n",
      "epoch no.0 train no.30660  loss = 5.80440 avg_loss = 4.00723\n",
      "epoch no.0 train no.30670  loss = 4.14660 avg_loss = 3.96069\n",
      "epoch no.0 train no.30680  loss = 5.76967 avg_loss = 4.06667\n",
      "epoch no.0 train no.30690  loss = 2.73327 avg_loss = 4.04161\n",
      "epoch no.0 train no.30700  loss = 4.04302 avg_loss = 4.02115\n",
      "epoch no.0 train no.30710  loss = 3.75748 avg_loss = 3.95189\n",
      "epoch no.0 train no.30720  loss = 5.84659 avg_loss = 3.92220\n",
      "epoch no.0 train no.30730  loss = 2.82848 avg_loss = 3.94927\n",
      "epoch no.0 train no.30740  loss = 6.86061 avg_loss = 3.96612\n",
      "epoch no.0 train no.30750  loss = 5.71421 avg_loss = 4.04066\n",
      "epoch no.0 train no.30760  loss = 4.64092 avg_loss = 4.03193\n",
      "epoch no.0 train no.30770  loss = 4.33036 avg_loss = 4.05080\n",
      "epoch no.0 train no.30780  loss = 3.68446 avg_loss = 4.02441\n",
      "epoch no.0 train no.30790  loss = 2.10025 avg_loss = 3.98229\n",
      "epoch no.0 train no.30800  loss = 1.82584 avg_loss = 4.00203\n",
      "epoch no.0 train no.30810  loss = 2.87867 avg_loss = 3.97467\n",
      "epoch no.0 train no.30820  loss = 4.11805 avg_loss = 3.98566\n",
      "epoch no.0 train no.30830  loss = 1.43184 avg_loss = 3.96531\n",
      "epoch no.0 train no.30840  loss = 3.48159 avg_loss = 3.93520\n",
      "epoch no.0 train no.30850  loss = 4.14351 avg_loss = 3.89969\n",
      "epoch no.0 train no.30860  loss = 2.56862 avg_loss = 3.91918\n",
      "epoch no.0 train no.30870  loss = 3.16832 avg_loss = 3.89313\n",
      "epoch no.0 train no.30880  loss = 2.64732 avg_loss = 3.85181\n",
      "epoch no.0 train no.30890  loss = 3.19440 avg_loss = 3.81063\n",
      "epoch no.0 train no.30900  loss = 3.20759 avg_loss = 3.82237\n",
      "epoch no.0 train no.30910  loss = 4.39607 avg_loss = 3.82407\n",
      "epoch no.0 train no.30920  loss = 4.49996 avg_loss = 3.81980\n",
      "epoch no.0 train no.30930  loss = 3.31643 avg_loss = 3.80193\n",
      "epoch no.0 train no.30940  loss = 5.28971 avg_loss = 3.91480\n",
      "epoch no.0 train no.30950  loss = 3.97194 avg_loss = 3.89326\n",
      "epoch no.0 train no.30960  loss = 3.54335 avg_loss = 3.87610\n",
      "epoch no.0 train no.30970  loss = 4.36252 avg_loss = 3.91402\n",
      "epoch no.0 train no.30980  loss = 2.24778 avg_loss = 3.86561\n",
      "epoch no.0 train no.30990  loss = 4.27247 avg_loss = 3.85032\n",
      "epoch no.0 train no.31000  loss = 2.68197 avg_loss = 3.91490\n",
      "2\n",
      "to_tokens: ['▁비', '▁노래', '들', '</s>']\n",
      "추억의노래들</s>\n",
      "epoch no.0 train no.31010  loss = 2.43027 avg_loss = 3.90068\n",
      "epoch no.0 train no.31020  loss = 5.31905 avg_loss = 3.84416\n",
      "epoch no.0 train no.31030  loss = 3.63836 avg_loss = 3.89056\n",
      "epoch no.0 train no.31040  loss = 4.14198 avg_loss = 3.91534\n",
      "epoch no.0 train no.31050  loss = 3.16412 avg_loss = 3.95013\n",
      "epoch no.0 train no.31060  loss = 6.96153 avg_loss = 3.98487\n",
      "epoch no.0 train no.31070  loss = 2.93661 avg_loss = 3.94596\n",
      "epoch no.0 train no.31080  loss = 3.08545 avg_loss = 3.94193\n",
      "epoch no.0 train no.31090  loss = 2.91346 avg_loss = 3.94457\n",
      "epoch no.0 train no.31100  loss = 4.65392 avg_loss = 3.94046\n",
      "epoch no.0 train no.31110  loss = 3.93388 avg_loss = 3.95834\n",
      "epoch no.0 train no.31120  loss = 2.84447 avg_loss = 3.91134\n",
      "epoch no.0 train no.31130  loss = 6.04147 avg_loss = 3.91327\n",
      "epoch no.0 train no.31140  loss = 4.59274 avg_loss = 3.88262\n",
      "epoch no.0 train no.31150  loss = 2.80181 avg_loss = 3.90049\n",
      "epoch no.0 train no.31160  loss = 2.40393 avg_loss = 3.95160\n",
      "epoch no.0 train no.31170  loss = 2.60363 avg_loss = 3.85868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.31180  loss = 5.85221 avg_loss = 3.86126\n",
      "epoch no.0 train no.31190  loss = 2.41263 avg_loss = 3.87323\n",
      "epoch no.0 train no.31200  loss = 2.50703 avg_loss = 3.90611\n",
      "epoch no.0 train no.31210  loss = 4.00556 avg_loss = 3.95112\n",
      "epoch no.0 train no.31220  loss = 3.93507 avg_loss = 3.93505\n",
      "epoch no.0 train no.31230  loss = 4.42591 avg_loss = 3.93059\n",
      "epoch no.0 train no.31240  loss = 3.56318 avg_loss = 3.92353\n",
      "epoch no.0 train no.31250  loss = 6.64690 avg_loss = 4.01834\n",
      "epoch no.0 train no.31260  loss = 4.75358 avg_loss = 4.00896\n",
      "epoch no.0 train no.31270  loss = 1.99835 avg_loss = 3.97920\n",
      "epoch no.0 train no.31280  loss = 2.92219 avg_loss = 3.93496\n",
      "epoch no.0 train no.31290  loss = 2.50327 avg_loss = 3.88655\n",
      "epoch no.0 train no.31300  loss = 5.22813 avg_loss = 3.90268\n",
      "epoch no.0 train no.31310  loss = 7.19326 avg_loss = 3.99143\n",
      "epoch no.0 train no.31320  loss = 2.65825 avg_loss = 3.93246\n",
      "epoch no.0 train no.31330  loss = 4.53561 avg_loss = 3.97108\n",
      "epoch no.0 train no.31340  loss = 3.51324 avg_loss = 4.02411\n",
      "epoch no.0 train no.31350  loss = 5.73268 avg_loss = 3.97732\n",
      "epoch no.0 train no.31360  loss = 3.79937 avg_loss = 3.98620\n",
      "epoch no.0 train no.31370  loss = 4.20420 avg_loss = 3.98503\n",
      "epoch no.0 train no.31380  loss = 1.90561 avg_loss = 4.01726\n",
      "epoch no.0 train no.31390  loss = 3.85009 avg_loss = 3.99732\n",
      "epoch no.0 train no.31400  loss = 2.71962 avg_loss = 4.01781\n",
      "epoch no.0 train no.31410  loss = 3.80884 avg_loss = 3.99758\n",
      "epoch no.0 train no.31420  loss = 5.41654 avg_loss = 4.02371\n",
      "epoch no.0 train no.31430  loss = 3.85796 avg_loss = 4.00509\n",
      "epoch no.0 train no.31440  loss = 3.76076 avg_loss = 3.96413\n",
      "epoch no.0 train no.31450  loss = 3.51983 avg_loss = 3.93049\n",
      "epoch no.0 train no.31460  loss = 3.03913 avg_loss = 3.90498\n",
      "epoch no.0 train no.31470  loss = 5.25558 avg_loss = 3.89383\n",
      "epoch no.0 train no.31480  loss = 2.71802 avg_loss = 3.88931\n",
      "epoch no.0 train no.31490  loss = 2.50395 avg_loss = 3.86553\n",
      "epoch no.0 train no.31500  loss = 4.78922 avg_loss = 3.88789\n",
      "epoch no.0 train no.31510  loss = 4.85446 avg_loss = 3.92970\n",
      "epoch no.0 train no.31520  loss = 5.22261 avg_loss = 3.98535\n",
      "epoch no.0 train no.31530  loss = 2.76775 avg_loss = 3.96748\n",
      "epoch no.0 train no.31540  loss = 5.14632 avg_loss = 3.98751\n",
      "epoch no.0 train no.31550  loss = 4.73136 avg_loss = 4.04374\n",
      "epoch no.0 train no.31560  loss = 2.32148 avg_loss = 4.04671\n",
      "epoch no.0 train no.31570  loss = 4.65151 avg_loss = 4.02340\n",
      "epoch no.0 train no.31580  loss = 3.02782 avg_loss = 3.96974\n",
      "epoch no.0 train no.31590  loss = 5.11948 avg_loss = 3.97085\n",
      "epoch no.0 train no.31600  loss = 4.53236 avg_loss = 4.02359\n",
      "epoch no.0 train no.31610  loss = 2.65881 avg_loss = 4.00161\n",
      "epoch no.0 train no.31620  loss = 3.70686 avg_loss = 3.93716\n",
      "epoch no.0 train no.31630  loss = 3.59220 avg_loss = 3.88716\n",
      "epoch no.0 train no.31640  loss = 3.94972 avg_loss = 3.84725\n",
      "epoch no.0 train no.31650  loss = 3.48136 avg_loss = 3.84834\n",
      "epoch no.0 train no.31660  loss = 2.50045 avg_loss = 3.80613\n",
      "epoch no.0 train no.31670  loss = 3.80626 avg_loss = 3.78471\n",
      "epoch no.0 train no.31680  loss = 3.10297 avg_loss = 3.81124\n",
      "epoch no.0 train no.31690  loss = 4.97169 avg_loss = 3.84477\n",
      "epoch no.0 train no.31700  loss = 3.47770 avg_loss = 3.81455\n",
      "epoch no.0 train no.31710  loss = 4.56372 avg_loss = 3.77234\n",
      "epoch no.0 train no.31720  loss = 3.26235 avg_loss = 3.74437\n",
      "epoch no.0 train no.31730  loss = 4.69812 avg_loss = 3.77981\n",
      "epoch no.0 train no.31740  loss = 2.64410 avg_loss = 3.72532\n",
      "epoch no.0 train no.31750  loss = 3.48271 avg_loss = 3.79557\n",
      "epoch no.0 train no.31760  loss = 5.59231 avg_loss = 3.80759\n",
      "epoch no.0 train no.31770  loss = 3.23365 avg_loss = 3.82454\n",
      "epoch no.0 train no.31780  loss = 4.04834 avg_loss = 3.86007\n",
      "epoch no.0 train no.31790  loss = 3.87206 avg_loss = 3.83928\n",
      "epoch no.0 train no.31800  loss = 3.51555 avg_loss = 3.84388\n",
      "epoch no.0 train no.31810  loss = 5.57261 avg_loss = 3.82512\n",
      "epoch no.0 train no.31820  loss = 4.10876 avg_loss = 3.89280\n",
      "epoch no.0 train no.31830  loss = 3.21753 avg_loss = 3.85109\n",
      "epoch no.0 train no.31840  loss = 4.81896 avg_loss = 3.81044\n",
      "epoch no.0 train no.31850  loss = 1.53912 avg_loss = 3.78451\n",
      "epoch no.0 train no.31860  loss = 4.89006 avg_loss = 3.81078\n",
      "epoch no.0 train no.31870  loss = 1.59256 avg_loss = 3.81183\n",
      "epoch no.0 train no.31880  loss = 5.14969 avg_loss = 3.85362\n",
      "epoch no.0 train no.31890  loss = 4.13652 avg_loss = 3.93044\n",
      "epoch no.0 train no.31900  loss = 3.49844 avg_loss = 3.95350\n",
      "epoch no.0 train no.31910  loss = 5.39776 avg_loss = 4.00033\n",
      "epoch no.0 train no.31920  loss = 4.72837 avg_loss = 3.99045\n",
      "epoch no.0 train no.31930  loss = 4.12500 avg_loss = 4.01916\n",
      "epoch no.0 train no.31940  loss = 5.41639 avg_loss = 4.02543\n",
      "epoch no.0 train no.31950  loss = 2.53518 avg_loss = 3.99768\n",
      "epoch no.0 train no.31960  loss = 3.52476 avg_loss = 4.02691\n",
      "epoch no.0 train no.31970  loss = 5.52513 avg_loss = 4.05910\n",
      "epoch no.0 train no.31980  loss = 2.91664 avg_loss = 4.04417\n",
      "epoch no.0 train no.31990  loss = 2.37158 avg_loss = 4.03083\n",
      "epoch no.0 train no.32000  loss = 2.88892 avg_loss = 3.98969\n",
      "3\n",
      "to_tokens: ['▁비', '▁노래', '▁o', 'st', '</s>']\n",
      "추억의 영화 ost</s>\n",
      "epoch no.0 train no.32010  loss = 3.39942 avg_loss = 4.03768\n",
      "epoch no.0 train no.32020  loss = 5.03055 avg_loss = 4.01903\n",
      "epoch no.0 train no.32030  loss = 3.33972 avg_loss = 4.01275\n",
      "epoch no.0 train no.32040  loss = 3.40893 avg_loss = 4.00972\n",
      "epoch no.0 train no.32050  loss = 3.34061 avg_loss = 4.11235\n",
      "epoch no.0 train no.32060  loss = 3.73063 avg_loss = 4.03002\n",
      "epoch no.0 train no.32070  loss = 4.82429 avg_loss = 4.00527\n",
      "epoch no.0 train no.32080  loss = 4.26912 avg_loss = 4.09234\n",
      "epoch no.0 train no.32090  loss = 3.41396 avg_loss = 4.02638\n",
      "epoch no.0 train no.32100  loss = 2.98674 avg_loss = 4.01652\n",
      "epoch no.0 train no.32110  loss = 4.12728 avg_loss = 4.03500\n",
      "epoch no.0 train no.32120  loss = 3.08783 avg_loss = 4.03479\n",
      "epoch no.0 train no.32130  loss = 4.01292 avg_loss = 3.98044\n",
      "epoch no.0 train no.32140  loss = 4.05013 avg_loss = 3.93486\n",
      "epoch no.0 train no.32150  loss = 3.39841 avg_loss = 3.84850\n",
      "epoch no.0 train no.32160  loss = 2.99535 avg_loss = 3.83161\n",
      "epoch no.0 train no.32170  loss = 2.90512 avg_loss = 3.86770\n",
      "epoch no.0 train no.32180  loss = 4.54684 avg_loss = 3.89690\n",
      "epoch no.0 train no.32190  loss = 3.38395 avg_loss = 3.89504\n",
      "epoch no.0 train no.32200  loss = 2.35626 avg_loss = 3.87785\n",
      "epoch no.0 train no.32210  loss = 4.92286 avg_loss = 3.83278\n",
      "epoch no.0 train no.32220  loss = 4.01331 avg_loss = 3.82762\n",
      "epoch no.0 train no.32230  loss = 2.86687 avg_loss = 3.84923\n",
      "epoch no.0 train no.32240  loss = 2.85373 avg_loss = 3.82609\n",
      "epoch no.0 train no.32250  loss = 5.46139 avg_loss = 3.79686\n",
      "epoch no.0 train no.32260  loss = 4.07592 avg_loss = 3.85375\n",
      "epoch no.0 train no.32270  loss = 5.91935 avg_loss = 3.87792\n",
      "epoch no.0 train no.32280  loss = 4.97986 avg_loss = 3.88563\n",
      "epoch no.0 train no.32290  loss = 2.00042 avg_loss = 3.93332\n",
      "epoch no.0 train no.32300  loss = 4.38794 avg_loss = 4.00004\n",
      "epoch no.0 train no.32310  loss = 2.58438 avg_loss = 3.98536\n",
      "epoch no.0 train no.32320  loss = 3.26445 avg_loss = 3.96569\n",
      "epoch no.0 train no.32330  loss = 5.82833 avg_loss = 3.95982\n",
      "epoch no.0 train no.32340  loss = 6.08905 avg_loss = 3.98642\n",
      "epoch no.0 train no.32350  loss = 4.28063 avg_loss = 4.04755\n",
      "epoch no.0 train no.32360  loss = 3.33332 avg_loss = 4.06183\n",
      "epoch no.0 train no.32370  loss = 5.21540 avg_loss = 4.07837\n",
      "epoch no.0 train no.32380  loss = 3.72140 avg_loss = 4.00712\n",
      "epoch no.0 train no.32390  loss = 4.08616 avg_loss = 3.97250\n",
      "epoch no.0 train no.32400  loss = 2.89277 avg_loss = 3.90067\n",
      "epoch no.0 train no.32410  loss = 3.02584 avg_loss = 3.89980\n",
      "epoch no.0 train no.32420  loss = 6.05516 avg_loss = 3.94601\n",
      "epoch no.0 train no.32430  loss = 3.13563 avg_loss = 3.93997\n",
      "epoch no.0 train no.32440  loss = 4.06771 avg_loss = 3.95498\n",
      "epoch no.0 train no.32450  loss = 3.84408 avg_loss = 3.90855\n",
      "epoch no.0 train no.32460  loss = 3.14768 avg_loss = 3.86377\n",
      "epoch no.0 train no.32470  loss = 5.15833 avg_loss = 3.87877\n",
      "epoch no.0 train no.32480  loss = 3.22096 avg_loss = 3.83700\n",
      "epoch no.0 train no.32490  loss = 2.40765 avg_loss = 3.84582\n",
      "epoch no.0 train no.32500  loss = 4.70629 avg_loss = 3.86826\n",
      "epoch no.0 train no.32510  loss = 5.38821 avg_loss = 3.88542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.32520  loss = 5.08681 avg_loss = 3.88050\n",
      "epoch no.0 train no.32530  loss = 3.77006 avg_loss = 3.86474\n",
      "epoch no.0 train no.32540  loss = 4.30373 avg_loss = 3.92239\n",
      "epoch no.0 train no.32550  loss = 5.24890 avg_loss = 3.96400\n",
      "epoch no.0 train no.32560  loss = 5.15818 avg_loss = 4.02443\n",
      "epoch no.0 train no.32570  loss = 3.54184 avg_loss = 4.07506\n",
      "epoch no.0 train no.32580  loss = 3.18928 avg_loss = 4.07021\n",
      "epoch no.0 train no.32590  loss = 2.78445 avg_loss = 4.01415\n",
      "epoch no.0 train no.32600  loss = 2.78688 avg_loss = 3.98901\n",
      "epoch no.0 train no.32610  loss = 2.91382 avg_loss = 3.95817\n",
      "epoch no.0 train no.32620  loss = 5.56263 avg_loss = 3.97770\n",
      "epoch no.0 train no.32630  loss = 5.36728 avg_loss = 4.01094\n",
      "epoch no.0 train no.32640  loss = 1.48192 avg_loss = 3.99390\n",
      "epoch no.0 train no.32650  loss = 3.97071 avg_loss = 3.95906\n",
      "epoch no.0 train no.32660  loss = 5.15048 avg_loss = 3.95958\n",
      "epoch no.0 train no.32670  loss = 4.38330 avg_loss = 3.91599\n",
      "epoch no.0 train no.32680  loss = 4.89044 avg_loss = 3.92979\n",
      "epoch no.0 train no.32690  loss = 3.06007 avg_loss = 3.89998\n",
      "epoch no.0 train no.32700  loss = 4.93443 avg_loss = 3.86915\n",
      "epoch no.0 train no.32710  loss = 3.17185 avg_loss = 3.84959\n",
      "epoch no.0 train no.32720  loss = 5.20297 avg_loss = 3.84489\n",
      "epoch no.0 train no.32730  loss = 3.10686 avg_loss = 3.80199\n",
      "epoch no.0 train no.32740  loss = 3.12345 avg_loss = 3.75796\n",
      "epoch no.0 train no.32750  loss = 3.60079 avg_loss = 3.78483\n",
      "epoch no.0 train no.32760  loss = 4.02983 avg_loss = 3.79831\n",
      "epoch no.0 train no.32770  loss = 7.27907 avg_loss = 3.87896\n",
      "epoch no.0 train no.32780  loss = 4.38935 avg_loss = 3.84376\n",
      "epoch no.0 train no.32790  loss = 2.75952 avg_loss = 3.82407\n",
      "epoch no.0 train no.32800  loss = 4.08362 avg_loss = 3.78515\n",
      "epoch no.0 train no.32810  loss = 2.25630 avg_loss = 3.70906\n",
      "epoch no.0 train no.32820  loss = 3.83975 avg_loss = 3.69848\n",
      "epoch no.0 train no.32830  loss = 3.96855 avg_loss = 3.71370\n",
      "epoch no.0 train no.32840  loss = 3.95479 avg_loss = 3.72055\n",
      "epoch no.0 train no.32850  loss = 5.09895 avg_loss = 3.70194\n",
      "epoch no.0 train no.32860  loss = 2.64188 avg_loss = 3.65088\n",
      "epoch no.0 train no.32870  loss = 5.89850 avg_loss = 3.77223\n",
      "epoch no.0 train no.32880  loss = 4.31819 avg_loss = 3.79295\n",
      "epoch no.0 train no.32890  loss = 3.55103 avg_loss = 3.75514\n",
      "epoch no.0 train no.32900  loss = 3.77973 avg_loss = 3.80985\n",
      "epoch no.0 train no.32910  loss = 2.67295 avg_loss = 3.85251\n",
      "epoch no.0 train no.32920  loss = 2.38036 avg_loss = 3.91995\n",
      "epoch no.0 train no.32930  loss = 4.47266 avg_loss = 3.97231\n",
      "epoch no.0 train no.32940  loss = 3.02653 avg_loss = 3.93623\n",
      "epoch no.0 train no.32950  loss = 6.21519 avg_loss = 3.88577\n",
      "epoch no.0 train no.32960  loss = 7.04920 avg_loss = 3.95925\n",
      "epoch no.0 train no.32970  loss = 1.77896 avg_loss = 3.97489\n",
      "epoch no.0 train no.32980  loss = 3.37153 avg_loss = 3.97419\n",
      "epoch no.0 train no.32990  loss = 4.54520 avg_loss = 3.93526\n",
      "epoch no.0 train no.33000  loss = 1.64351 avg_loss = 3.90307\n",
      "5\n",
      "to_tokens: ['▁가을', '▁노래', '들', '</s>', '모아', '어', '</s>']\n",
      "추억의 노래들 모아봤어</s>\n",
      "epoch no.0 train no.33010  loss = 6.14385 avg_loss = 3.97445\n",
      "epoch no.0 train no.33020  loss = 3.85575 avg_loss = 3.99171\n",
      "epoch no.0 train no.33030  loss = 3.01243 avg_loss = 3.98421\n",
      "epoch no.0 train no.33040  loss = 3.37158 avg_loss = 3.92129\n",
      "epoch no.0 train no.33050  loss = 5.66363 avg_loss = 3.94275\n",
      "epoch no.0 train no.33060  loss = 3.33719 avg_loss = 3.96215\n",
      "epoch no.0 train no.33070  loss = 4.34875 avg_loss = 4.03493\n",
      "epoch no.0 train no.33080  loss = 3.50077 avg_loss = 4.13173\n",
      "epoch no.0 train no.33090  loss = 5.56250 avg_loss = 4.11539\n",
      "epoch no.0 train no.33100  loss = 2.59445 avg_loss = 4.09613\n",
      "epoch no.0 train no.33110  loss = 3.10892 avg_loss = 4.00622\n",
      "epoch no.0 train no.33120  loss = 4.05317 avg_loss = 3.99819\n",
      "epoch no.0 train no.33130  loss = 3.95815 avg_loss = 3.96474\n",
      "epoch no.0 train no.33140  loss = 4.06705 avg_loss = 3.92234\n",
      "epoch no.0 train no.33150  loss = 4.57469 avg_loss = 3.93588\n",
      "epoch no.0 train no.33160  loss = 2.51218 avg_loss = 3.93285\n",
      "epoch no.0 train no.33170  loss = 5.34950 avg_loss = 3.86812\n",
      "epoch no.0 train no.33180  loss = 3.20284 avg_loss = 3.90430\n",
      "epoch no.0 train no.33190  loss = 4.69215 avg_loss = 3.88606\n",
      "epoch no.0 train no.33200  loss = 3.40381 avg_loss = 3.90982\n",
      "epoch no.0 train no.33210  loss = 2.65139 avg_loss = 3.89042\n",
      "epoch no.0 train no.33220  loss = 3.40979 avg_loss = 3.89738\n",
      "epoch no.0 train no.33230  loss = 5.89519 avg_loss = 3.90419\n",
      "epoch no.0 train no.33240  loss = 4.13123 avg_loss = 3.86311\n",
      "epoch no.0 train no.33250  loss = 3.59494 avg_loss = 3.85196\n",
      "epoch no.0 train no.33260  loss = 2.34112 avg_loss = 3.84893\n",
      "epoch no.0 train no.33270  loss = 3.88523 avg_loss = 3.86246\n",
      "epoch no.0 train no.33280  loss = 3.95580 avg_loss = 3.81929\n",
      "epoch no.0 train no.33290  loss = 4.10626 avg_loss = 3.84506\n",
      "epoch no.0 train no.33300  loss = 5.29897 avg_loss = 3.86216\n",
      "epoch no.0 train no.33310  loss = 1.67727 avg_loss = 3.86767\n",
      "epoch no.0 train no.33320  loss = 2.90401 avg_loss = 3.79512\n",
      "epoch no.0 train no.33330  loss = 5.16643 avg_loss = 3.80288\n",
      "epoch no.0 train no.33340  loss = 2.95727 avg_loss = 3.80490\n",
      "epoch no.0 train no.33350  loss = 3.46368 avg_loss = 3.80217\n",
      "epoch no.0 train no.33360  loss = 4.50877 avg_loss = 3.79065\n",
      "epoch no.0 train no.33370  loss = 2.11192 avg_loss = 3.79738\n",
      "epoch no.0 train no.33380  loss = 6.71838 avg_loss = 3.82707\n",
      "epoch no.0 train no.33390  loss = 2.99167 avg_loss = 3.77637\n",
      "epoch no.0 train no.33400  loss = 1.95502 avg_loss = 3.81559\n",
      "epoch no.0 train no.33410  loss = 1.81906 avg_loss = 3.85706\n",
      "epoch no.0 train no.33420  loss = 3.51360 avg_loss = 3.87861\n",
      "epoch no.0 train no.33430  loss = 3.65808 avg_loss = 3.84904\n",
      "epoch no.0 train no.33440  loss = 4.77539 avg_loss = 3.84674\n",
      "epoch no.0 train no.33450  loss = 3.79626 avg_loss = 3.84878\n",
      "epoch no.0 train no.33460  loss = 3.93741 avg_loss = 3.85539\n",
      "epoch no.0 train no.33470  loss = 3.93304 avg_loss = 3.92534\n",
      "epoch no.0 train no.33480  loss = 6.07133 avg_loss = 3.91333\n",
      "epoch no.0 train no.33490  loss = 3.48496 avg_loss = 3.91811\n",
      "epoch no.0 train no.33500  loss = 4.79464 avg_loss = 3.92123\n",
      "epoch no.0 train no.33510  loss = 3.69996 avg_loss = 3.94463\n",
      "epoch no.0 train no.33520  loss = 1.83071 avg_loss = 3.93324\n",
      "epoch no.0 train no.33530  loss = 3.63035 avg_loss = 3.90124\n",
      "epoch no.0 train no.33540  loss = 4.96622 avg_loss = 3.89618\n",
      "epoch no.0 train no.33550  loss = 3.01285 avg_loss = 3.91464\n",
      "epoch no.0 train no.33560  loss = 2.44927 avg_loss = 3.81385\n",
      "epoch no.0 train no.33570  loss = 4.85649 avg_loss = 3.85497\n",
      "epoch no.0 train no.33580  loss = 3.36982 avg_loss = 3.86753\n",
      "epoch no.0 train no.33590  loss = 4.74195 avg_loss = 3.92237\n",
      "epoch no.0 train no.33600  loss = 5.39475 avg_loss = 3.92435\n",
      "epoch no.0 train no.33610  loss = 3.71600 avg_loss = 3.90375\n",
      "epoch no.0 train no.33620  loss = 2.60364 avg_loss = 3.89034\n",
      "epoch no.0 train no.33630  loss = 4.14548 avg_loss = 3.89342\n",
      "epoch no.0 train no.33640  loss = 3.91934 avg_loss = 3.88349\n",
      "epoch no.0 train no.33650  loss = 3.34959 avg_loss = 3.84109\n",
      "epoch no.0 train no.33660  loss = 3.22625 avg_loss = 3.82972\n",
      "epoch no.0 train no.33670  loss = 3.83397 avg_loss = 3.82587\n",
      "epoch no.0 train no.33680  loss = 2.62323 avg_loss = 3.75465\n",
      "epoch no.0 train no.33690  loss = 4.86421 avg_loss = 3.82447\n",
      "epoch no.0 train no.33700  loss = 3.65972 avg_loss = 3.86234\n",
      "epoch no.0 train no.33710  loss = 4.62058 avg_loss = 3.91436\n",
      "epoch no.0 train no.33720  loss = 4.24503 avg_loss = 3.88539\n",
      "epoch no.0 train no.33730  loss = 4.25530 avg_loss = 3.88749\n",
      "epoch no.0 train no.33740  loss = 3.96197 avg_loss = 3.92399\n",
      "epoch no.0 train no.33750  loss = 5.84407 avg_loss = 3.96085\n",
      "epoch no.0 train no.33760  loss = 3.25696 avg_loss = 3.92612\n",
      "epoch no.0 train no.33770  loss = 5.11767 avg_loss = 3.90493\n",
      "epoch no.0 train no.33780  loss = 4.30531 avg_loss = 3.90884\n",
      "epoch no.0 train no.33790  loss = 5.32033 avg_loss = 3.91435\n",
      "epoch no.0 train no.33800  loss = 3.33776 avg_loss = 3.91388\n",
      "epoch no.0 train no.33810  loss = 1.46121 avg_loss = 3.88595\n",
      "epoch no.0 train no.33820  loss = 3.82255 avg_loss = 3.90295\n",
      "epoch no.0 train no.33830  loss = 3.36743 avg_loss = 3.89001\n",
      "epoch no.0 train no.33840  loss = 3.00257 avg_loss = 3.89605\n",
      "epoch no.0 train no.33850  loss = 3.93478 avg_loss = 3.93861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.33860  loss = 3.48593 avg_loss = 3.90263\n",
      "epoch no.0 train no.33870  loss = 5.77674 avg_loss = 3.96326\n",
      "epoch no.0 train no.33880  loss = 3.75183 avg_loss = 3.91010\n",
      "epoch no.0 train no.33890  loss = 6.28446 avg_loss = 3.97490\n",
      "epoch no.0 train no.33900  loss = 5.25443 avg_loss = 3.94938\n",
      "epoch no.0 train no.33910  loss = 5.03540 avg_loss = 4.01077\n",
      "epoch no.0 train no.33920  loss = 3.33861 avg_loss = 3.98871\n",
      "epoch no.0 train no.33930  loss = 4.44665 avg_loss = 3.93988\n",
      "epoch no.0 train no.33940  loss = 2.97559 avg_loss = 3.95162\n",
      "epoch no.0 train no.33950  loss = 4.66966 avg_loss = 3.97236\n",
      "epoch no.0 train no.33960  loss = 1.87424 avg_loss = 4.01334\n",
      "epoch no.0 train no.33970  loss = 4.11339 avg_loss = 4.00169\n",
      "epoch no.0 train no.33980  loss = 4.47545 avg_loss = 3.97368\n",
      "epoch no.0 train no.33990  loss = 2.55131 avg_loss = 3.96874\n",
      "epoch no.0 train no.34000  loss = 3.20127 avg_loss = 3.92838\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '송', '모', '음', '</s>']\n",
      "추억의 팝송모음</s>\n",
      "epoch no.0 train no.34010  loss = 2.49522 avg_loss = 3.96977\n",
      "epoch no.0 train no.34020  loss = 3.12952 avg_loss = 3.96107\n",
      "epoch no.0 train no.34030  loss = 3.69922 avg_loss = 3.97623\n",
      "epoch no.0 train no.34040  loss = 4.94960 avg_loss = 4.03275\n",
      "epoch no.0 train no.34050  loss = 2.21125 avg_loss = 4.02401\n",
      "epoch no.0 train no.34060  loss = 4.79232 avg_loss = 3.98123\n",
      "epoch no.0 train no.34070  loss = 3.93559 avg_loss = 3.91228\n",
      "epoch no.0 train no.34080  loss = 4.67589 avg_loss = 3.88188\n",
      "epoch no.0 train no.34090  loss = 6.45630 avg_loss = 3.92524\n",
      "epoch no.0 train no.34100  loss = 3.27232 avg_loss = 3.91568\n",
      "epoch no.0 train no.34110  loss = 3.72691 avg_loss = 3.99573\n",
      "epoch no.0 train no.34120  loss = 5.83521 avg_loss = 4.00354\n",
      "epoch no.0 train no.34130  loss = 4.06429 avg_loss = 3.98210\n",
      "epoch no.0 train no.34140  loss = 5.52294 avg_loss = 4.00272\n",
      "epoch no.0 train no.34150  loss = 4.74300 avg_loss = 3.97573\n",
      "epoch no.0 train no.34160  loss = 5.39841 avg_loss = 3.94605\n",
      "epoch no.0 train no.34170  loss = 2.94285 avg_loss = 3.93252\n",
      "epoch no.0 train no.34180  loss = 2.63663 avg_loss = 3.92180\n",
      "epoch no.0 train no.34190  loss = 5.64082 avg_loss = 3.92162\n",
      "epoch no.0 train no.34200  loss = 4.68730 avg_loss = 3.94069\n",
      "epoch no.0 train no.34210  loss = 3.61373 avg_loss = 3.91679\n",
      "epoch no.0 train no.34220  loss = 2.45332 avg_loss = 3.91506\n",
      "epoch no.0 train no.34230  loss = 5.41919 avg_loss = 3.93572\n",
      "epoch no.0 train no.34240  loss = 4.53457 avg_loss = 3.89328\n",
      "epoch no.0 train no.34250  loss = 2.54835 avg_loss = 3.90491\n",
      "epoch no.0 train no.34260  loss = 4.84499 avg_loss = 3.96213\n",
      "epoch no.0 train no.34270  loss = 4.64889 avg_loss = 3.97808\n",
      "epoch no.0 train no.34280  loss = 4.61347 avg_loss = 3.99265\n",
      "epoch no.0 train no.34290  loss = 3.12429 avg_loss = 3.94867\n",
      "epoch no.0 train no.34300  loss = 7.62992 avg_loss = 4.02483\n",
      "epoch no.0 train no.34310  loss = 4.88687 avg_loss = 4.06144\n",
      "epoch no.0 train no.34320  loss = 5.54687 avg_loss = 4.03937\n",
      "epoch no.0 train no.34330  loss = 4.08706 avg_loss = 4.06351\n",
      "epoch no.0 train no.34340  loss = 3.94950 avg_loss = 4.05031\n",
      "epoch no.0 train no.34350  loss = 2.85524 avg_loss = 4.10037\n",
      "epoch no.0 train no.34360  loss = 3.07793 avg_loss = 4.03172\n",
      "epoch no.0 train no.34370  loss = 4.51006 avg_loss = 4.00309\n",
      "epoch no.0 train no.34380  loss = 5.87241 avg_loss = 3.96440\n",
      "epoch no.0 train no.34390  loss = 6.03675 avg_loss = 3.92538\n",
      "epoch no.0 train no.34400  loss = 7.41877 avg_loss = 3.99600\n",
      "epoch no.0 train no.34410  loss = 3.47243 avg_loss = 3.95792\n",
      "epoch no.0 train no.34420  loss = 4.31115 avg_loss = 3.95264\n",
      "epoch no.0 train no.34430  loss = 3.90352 avg_loss = 4.00486\n",
      "epoch no.0 train no.34440  loss = 2.94092 avg_loss = 3.99477\n",
      "epoch no.0 train no.34450  loss = 4.34904 avg_loss = 3.98791\n",
      "epoch no.0 train no.34460  loss = 2.76039 avg_loss = 4.09455\n",
      "epoch no.0 train no.34470  loss = 4.73223 avg_loss = 4.08216\n",
      "epoch no.0 train no.34480  loss = 5.12309 avg_loss = 4.05653\n",
      "epoch no.0 train no.34490  loss = 2.97047 avg_loss = 4.04370\n",
      "epoch no.0 train no.34500  loss = 5.45257 avg_loss = 4.07447\n",
      "epoch no.0 train no.34510  loss = 2.49723 avg_loss = 4.03838\n",
      "epoch no.0 train no.34520  loss = 7.05439 avg_loss = 4.07461\n",
      "epoch no.0 train no.34530  loss = 2.77294 avg_loss = 4.09826\n",
      "epoch no.0 train no.34540  loss = 4.16652 avg_loss = 4.11026\n",
      "epoch no.0 train no.34550  loss = 4.22713 avg_loss = 4.16571\n",
      "epoch no.0 train no.34560  loss = 4.53079 avg_loss = 4.13804\n",
      "epoch no.0 train no.34570  loss = 4.93239 avg_loss = 4.13678\n",
      "epoch no.0 train no.34580  loss = 3.75065 avg_loss = 4.10529\n",
      "epoch no.0 train no.34590  loss = 2.84620 avg_loss = 4.09245\n",
      "epoch no.0 train no.34600  loss = 5.68733 avg_loss = 4.10491\n",
      "epoch no.0 train no.34610  loss = 3.62085 avg_loss = 4.10280\n",
      "epoch no.0 train no.34620  loss = 4.45374 avg_loss = 4.11921\n",
      "epoch no.0 train no.34630  loss = 2.59358 avg_loss = 4.08697\n",
      "epoch no.0 train no.34640  loss = 6.22912 avg_loss = 4.09878\n",
      "epoch no.0 train no.34650  loss = 5.86010 avg_loss = 4.07290\n",
      "epoch no.0 train no.34660  loss = 3.77487 avg_loss = 4.02407\n",
      "epoch no.0 train no.34670  loss = 2.84276 avg_loss = 3.96987\n",
      "epoch no.0 train no.34680  loss = 4.27246 avg_loss = 3.98577\n",
      "epoch no.0 train no.34690  loss = 2.88200 avg_loss = 3.96421\n",
      "epoch no.0 train no.34700  loss = 3.54905 avg_loss = 3.96572\n",
      "epoch no.0 train no.34710  loss = 4.78594 avg_loss = 4.00060\n",
      "epoch no.0 train no.34720  loss = 4.45185 avg_loss = 4.05619\n",
      "epoch no.0 train no.34730  loss = 3.33254 avg_loss = 4.07159\n",
      "epoch no.0 train no.34740  loss = 5.64803 avg_loss = 4.11553\n",
      "epoch no.0 train no.34750  loss = 5.36804 avg_loss = 4.10192\n",
      "epoch no.0 train no.34760  loss = 3.73536 avg_loss = 4.05614\n",
      "epoch no.0 train no.34770  loss = 3.69456 avg_loss = 4.00357\n",
      "epoch no.0 train no.34780  loss = 4.15113 avg_loss = 4.01359\n",
      "epoch no.0 train no.34790  loss = 2.82742 avg_loss = 4.00605\n",
      "epoch no.0 train no.34800  loss = 5.33099 avg_loss = 4.02326\n",
      "epoch no.0 train no.34810  loss = 2.83615 avg_loss = 4.00652\n",
      "epoch no.0 train no.34820  loss = 2.21769 avg_loss = 3.99343\n",
      "epoch no.0 train no.34830  loss = 4.98118 avg_loss = 4.01287\n",
      "epoch no.0 train no.34840  loss = 5.72985 avg_loss = 4.04048\n",
      "epoch no.0 train no.34850  loss = 2.81132 avg_loss = 3.97833\n",
      "epoch no.0 train no.34860  loss = 2.67542 avg_loss = 3.94553\n",
      "epoch no.0 train no.34870  loss = 3.55550 avg_loss = 3.99282\n",
      "epoch no.0 train no.34880  loss = 2.94855 avg_loss = 3.96898\n",
      "epoch no.0 train no.34890  loss = 2.43121 avg_loss = 3.91870\n",
      "epoch no.0 train no.34900  loss = 5.27386 avg_loss = 3.97759\n",
      "epoch no.0 train no.34910  loss = 3.80274 avg_loss = 4.00360\n",
      "epoch no.0 train no.34920  loss = 3.39003 avg_loss = 4.01249\n",
      "epoch no.0 train no.34930  loss = 2.16527 avg_loss = 3.96291\n",
      "epoch no.0 train no.34940  loss = 3.32854 avg_loss = 3.96793\n",
      "epoch no.0 train no.34950  loss = 4.34351 avg_loss = 3.89308\n",
      "epoch no.0 train no.34960  loss = 5.29199 avg_loss = 3.92180\n",
      "epoch no.0 train no.34970  loss = 2.87408 avg_loss = 3.91227\n",
      "epoch no.0 train no.34980  loss = 1.92957 avg_loss = 3.88906\n",
      "epoch no.0 train no.34990  loss = 4.60710 avg_loss = 3.95079\n",
      "epoch no.0 train no.35000  loss = 6.62532 avg_loss = 3.98643\n",
      "3\n",
      "to_tokens: ['▁비', '▁노래', '▁노래', '곡', '</s>']\n",
      "추억의 걸그룹 명곡</s>\n",
      "epoch no.0 train no.35010  loss = 7.02452 avg_loss = 3.98002\n",
      "epoch no.0 train no.35020  loss = 4.75205 avg_loss = 4.02963\n",
      "epoch no.0 train no.35030  loss = 4.25392 avg_loss = 4.09189\n",
      "epoch no.0 train no.35040  loss = 4.70425 avg_loss = 4.07933\n",
      "epoch no.0 train no.35050  loss = 3.23600 avg_loss = 4.09919\n",
      "epoch no.0 train no.35060  loss = 5.35658 avg_loss = 4.09326\n",
      "epoch no.0 train no.35070  loss = 6.70830 avg_loss = 4.17433\n",
      "epoch no.0 train no.35080  loss = 3.22316 avg_loss = 4.13986\n",
      "epoch no.0 train no.35090  loss = 1.91513 avg_loss = 4.07476\n",
      "epoch no.0 train no.35100  loss = 5.51059 avg_loss = 4.09339\n",
      "epoch no.0 train no.35110  loss = 3.31930 avg_loss = 4.04913\n",
      "epoch no.0 train no.35120  loss = 3.41753 avg_loss = 4.05480\n",
      "epoch no.0 train no.35130  loss = 3.63680 avg_loss = 4.03555\n",
      "epoch no.0 train no.35140  loss = 3.19241 avg_loss = 4.00959\n",
      "epoch no.0 train no.35150  loss = 3.80585 avg_loss = 3.95235\n",
      "epoch no.0 train no.35160  loss = 3.16487 avg_loss = 3.95353\n",
      "epoch no.0 train no.35170  loss = 4.15025 avg_loss = 3.99821\n",
      "epoch no.0 train no.35180  loss = 2.24872 avg_loss = 3.96965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.35190  loss = 5.72903 avg_loss = 3.96376\n",
      "epoch no.0 train no.35200  loss = 2.26769 avg_loss = 3.93282\n",
      "epoch no.0 train no.35210  loss = 2.55323 avg_loss = 3.94579\n",
      "epoch no.0 train no.35220  loss = 5.22098 avg_loss = 3.98633\n",
      "epoch no.0 train no.35230  loss = 3.33929 avg_loss = 3.96912\n",
      "epoch no.0 train no.35240  loss = 4.36339 avg_loss = 4.05810\n",
      "epoch no.0 train no.35250  loss = 3.24320 avg_loss = 4.07362\n",
      "epoch no.0 train no.35260  loss = 3.01885 avg_loss = 4.07804\n",
      "epoch no.0 train no.35270  loss = 3.39373 avg_loss = 4.04647\n",
      "epoch no.0 train no.35280  loss = 3.21256 avg_loss = 3.99920\n",
      "epoch no.0 train no.35290  loss = 5.20950 avg_loss = 3.97913\n",
      "epoch no.0 train no.35300  loss = 3.38967 avg_loss = 3.92558\n",
      "epoch no.0 train no.35310  loss = 6.32194 avg_loss = 4.00895\n",
      "epoch no.0 train no.35320  loss = 3.12523 avg_loss = 3.99465\n",
      "epoch no.0 train no.35330  loss = 3.89170 avg_loss = 3.94541\n",
      "epoch no.0 train no.35340  loss = 2.05778 avg_loss = 3.96897\n",
      "epoch no.0 train no.35350  loss = 3.12115 avg_loss = 3.97717\n",
      "epoch no.0 train no.35360  loss = 2.59190 avg_loss = 3.94092\n",
      "epoch no.0 train no.35370  loss = 4.79974 avg_loss = 4.02359\n",
      "epoch no.0 train no.35380  loss = 4.80417 avg_loss = 4.01389\n",
      "epoch no.0 train no.35390  loss = 3.54553 avg_loss = 3.97823\n",
      "epoch no.0 train no.35400  loss = 3.89315 avg_loss = 3.95146\n",
      "epoch no.0 train no.35410  loss = 4.34626 avg_loss = 3.93197\n",
      "epoch no.0 train no.35420  loss = 2.89808 avg_loss = 3.92491\n",
      "epoch no.0 train no.35430  loss = 3.66458 avg_loss = 3.90693\n",
      "epoch no.0 train no.35440  loss = 6.42545 avg_loss = 3.91400\n",
      "epoch no.0 train no.35450  loss = 5.69412 avg_loss = 3.93312\n",
      "epoch no.0 train no.35460  loss = 4.58219 avg_loss = 3.98121\n",
      "epoch no.0 train no.35470  loss = 2.72208 avg_loss = 3.97711\n",
      "epoch no.0 train no.35480  loss = 2.92895 avg_loss = 3.96999\n",
      "epoch no.0 train no.35490  loss = 6.43751 avg_loss = 3.99193\n",
      "epoch no.0 train no.35500  loss = 5.42541 avg_loss = 3.96401\n",
      "epoch no.0 train no.35510  loss = 4.26905 avg_loss = 3.97400\n",
      "epoch no.0 train no.35520  loss = 3.58060 avg_loss = 4.00347\n",
      "epoch no.0 train no.35530  loss = 5.55188 avg_loss = 4.07798\n",
      "epoch no.0 train no.35540  loss = 2.78124 avg_loss = 4.08059\n",
      "epoch no.0 train no.35550  loss = 2.30747 avg_loss = 4.05565\n",
      "epoch no.0 train no.35560  loss = 3.16239 avg_loss = 3.99006\n",
      "epoch no.0 train no.35570  loss = 4.14276 avg_loss = 4.05826\n",
      "epoch no.0 train no.35580  loss = 4.88985 avg_loss = 4.07027\n",
      "epoch no.0 train no.35590  loss = 2.55822 avg_loss = 4.04049\n",
      "epoch no.0 train no.35600  loss = 2.05261 avg_loss = 4.04846\n",
      "epoch no.0 train no.35610  loss = 3.07089 avg_loss = 4.05008\n",
      "epoch no.0 train no.35620  loss = 3.73743 avg_loss = 4.02401\n",
      "epoch no.0 train no.35630  loss = 3.11866 avg_loss = 4.02299\n",
      "epoch no.0 train no.35640  loss = 1.66709 avg_loss = 4.01449\n",
      "epoch no.0 train no.35650  loss = 7.48498 avg_loss = 4.06220\n",
      "epoch no.0 train no.35660  loss = 5.34648 avg_loss = 4.09913\n",
      "epoch no.0 train no.35670  loss = 4.81672 avg_loss = 4.07743\n",
      "epoch no.0 train no.35680  loss = 2.63491 avg_loss = 4.04410\n",
      "epoch no.0 train no.35690  loss = 3.70142 avg_loss = 4.04912\n",
      "epoch no.0 train no.35700  loss = 5.01361 avg_loss = 4.09529\n",
      "epoch no.0 train no.35710  loss = 6.23060 avg_loss = 4.08891\n",
      "epoch no.0 train no.35720  loss = 3.10509 avg_loss = 4.08889\n",
      "epoch no.0 train no.35730  loss = 3.55026 avg_loss = 4.04723\n",
      "epoch no.0 train no.35740  loss = 4.85088 avg_loss = 4.04408\n",
      "epoch no.0 train no.35750  loss = 2.95328 avg_loss = 4.02492\n",
      "epoch no.0 train no.35760  loss = 4.75747 avg_loss = 4.00685\n",
      "epoch no.0 train no.35770  loss = 3.11765 avg_loss = 3.96367\n",
      "epoch no.0 train no.35780  loss = 2.44554 avg_loss = 3.93696\n",
      "epoch no.0 train no.35790  loss = 2.14511 avg_loss = 3.92844\n",
      "epoch no.0 train no.35800  loss = 5.55197 avg_loss = 3.85410\n",
      "epoch no.0 train no.35810  loss = 3.45747 avg_loss = 3.85540\n",
      "epoch no.0 train no.35820  loss = 2.68625 avg_loss = 3.80087\n",
      "epoch no.0 train no.35830  loss = 3.53564 avg_loss = 3.82587\n",
      "epoch no.0 train no.35840  loss = 6.26928 avg_loss = 3.86767\n",
      "epoch no.0 train no.35850  loss = 3.39119 avg_loss = 3.88247\n",
      "epoch no.0 train no.35860  loss = 3.12141 avg_loss = 3.90750\n",
      "epoch no.0 train no.35870  loss = 6.70013 avg_loss = 3.91941\n",
      "epoch no.0 train no.35880  loss = 4.55952 avg_loss = 3.99646\n",
      "epoch no.0 train no.35890  loss = 2.71874 avg_loss = 3.98403\n",
      "epoch no.0 train no.35900  loss = 3.35828 avg_loss = 3.96000\n",
      "epoch no.0 train no.35910  loss = 3.90684 avg_loss = 3.93035\n",
      "epoch no.0 train no.35920  loss = 3.14573 avg_loss = 3.84656\n",
      "epoch no.0 train no.35930  loss = 2.75085 avg_loss = 3.91738\n",
      "epoch no.0 train no.35940  loss = 4.88648 avg_loss = 3.94269\n",
      "epoch no.0 train no.35950  loss = 4.50490 avg_loss = 3.93990\n",
      "epoch no.0 train no.35960  loss = 5.79327 avg_loss = 3.93806\n",
      "epoch no.0 train no.35970  loss = 3.67790 avg_loss = 3.92804\n",
      "epoch no.0 train no.35980  loss = 3.40731 avg_loss = 3.88253\n",
      "epoch no.0 train no.35990  loss = 3.89800 avg_loss = 3.90599\n",
      "epoch no.0 train no.36000  loss = 3.93668 avg_loss = 3.89088\n",
      "3\n",
      "to_tokens: ['▁가을', '▁올드', '드', '▁명', '</s>']\n",
      "추억의 발라드 모음</s>\n",
      "epoch no.0 train no.36010  loss = 5.48478 avg_loss = 3.89456\n",
      "epoch no.0 train no.36020  loss = 3.99411 avg_loss = 3.92996\n",
      "epoch no.0 train no.36030  loss = 6.77724 avg_loss = 3.92328\n",
      "epoch no.0 train no.36040  loss = 4.04514 avg_loss = 3.89373\n",
      "epoch no.0 train no.36050  loss = 4.89778 avg_loss = 3.92916\n",
      "epoch no.0 train no.36060  loss = 3.39295 avg_loss = 3.98019\n",
      "epoch no.0 train no.36070  loss = 2.57659 avg_loss = 3.99323\n",
      "epoch no.0 train no.36080  loss = 2.98461 avg_loss = 3.97021\n",
      "epoch no.0 train no.36090  loss = 4.75744 avg_loss = 4.02556\n",
      "epoch no.0 train no.36100  loss = 1.73008 avg_loss = 3.94835\n",
      "epoch no.0 train no.36110  loss = 2.49779 avg_loss = 3.92324\n",
      "epoch no.0 train no.36120  loss = 6.17951 avg_loss = 3.93005\n",
      "epoch no.0 train no.36130  loss = 4.68246 avg_loss = 3.93101\n",
      "epoch no.0 train no.36140  loss = 3.57576 avg_loss = 3.91903\n",
      "epoch no.0 train no.36150  loss = 6.22775 avg_loss = 3.99041\n",
      "epoch no.0 train no.36160  loss = 4.34613 avg_loss = 3.97375\n",
      "epoch no.0 train no.36170  loss = 3.29115 avg_loss = 3.96101\n",
      "epoch no.0 train no.36180  loss = 3.24922 avg_loss = 4.02168\n",
      "epoch no.0 train no.36190  loss = 3.64503 avg_loss = 4.02195\n",
      "epoch no.0 train no.36200  loss = 2.69713 avg_loss = 3.97697\n",
      "epoch no.0 train no.36210  loss = 4.61674 avg_loss = 4.01008\n",
      "epoch no.0 train no.36220  loss = 3.43087 avg_loss = 3.99005\n",
      "epoch no.0 train no.36230  loss = 3.73893 avg_loss = 4.00987\n",
      "epoch no.0 train no.36240  loss = 3.31264 avg_loss = 3.95358\n",
      "epoch no.0 train no.36250  loss = 5.33208 avg_loss = 4.00607\n",
      "epoch no.0 train no.36260  loss = 2.94793 avg_loss = 4.02477\n",
      "epoch no.0 train no.36270  loss = 2.73974 avg_loss = 4.07667\n",
      "epoch no.0 train no.36280  loss = 2.66293 avg_loss = 4.00575\n",
      "epoch no.0 train no.36290  loss = 5.00422 avg_loss = 4.01727\n",
      "epoch no.0 train no.36300  loss = 4.48686 avg_loss = 3.98654\n",
      "epoch no.0 train no.36310  loss = 4.93852 avg_loss = 4.00432\n",
      "epoch no.0 train no.36320  loss = 2.11311 avg_loss = 3.95065\n",
      "epoch no.0 train no.36330  loss = 3.39143 avg_loss = 3.87681\n",
      "epoch no.0 train no.36340  loss = 5.70254 avg_loss = 3.86488\n",
      "epoch no.0 train no.36350  loss = 4.11569 avg_loss = 3.82030\n",
      "epoch no.0 train no.36360  loss = 3.95935 avg_loss = 3.85127\n",
      "epoch no.0 train no.36370  loss = 3.64581 avg_loss = 3.82125\n",
      "epoch no.0 train no.36380  loss = 3.40281 avg_loss = 3.83708\n",
      "epoch no.0 train no.36390  loss = 2.37029 avg_loss = 3.77412\n",
      "epoch no.0 train no.36400  loss = 4.62827 avg_loss = 3.84799\n",
      "epoch no.0 train no.36410  loss = 3.40303 avg_loss = 3.82435\n",
      "epoch no.0 train no.36420  loss = 3.65615 avg_loss = 3.86120\n",
      "epoch no.0 train no.36430  loss = 3.57853 avg_loss = 3.86915\n",
      "epoch no.0 train no.36440  loss = 5.43102 avg_loss = 3.94378\n",
      "epoch no.0 train no.36450  loss = 2.54133 avg_loss = 3.95986\n",
      "epoch no.0 train no.36460  loss = 2.51870 avg_loss = 3.93087\n",
      "epoch no.0 train no.36470  loss = 3.13105 avg_loss = 3.91244\n",
      "epoch no.0 train no.36480  loss = 1.92118 avg_loss = 3.84225\n",
      "epoch no.0 train no.36490  loss = 3.90779 avg_loss = 3.85128\n",
      "epoch no.0 train no.36500  loss = 4.14194 avg_loss = 3.88362\n",
      "epoch no.0 train no.36510  loss = 3.01028 avg_loss = 3.96973\n",
      "epoch no.0 train no.36520  loss = 2.42108 avg_loss = 3.96098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.36530  loss = 3.21688 avg_loss = 3.94546\n",
      "epoch no.0 train no.36540  loss = 4.57562 avg_loss = 4.03218\n",
      "epoch no.0 train no.36550  loss = 4.50658 avg_loss = 4.03624\n",
      "epoch no.0 train no.36560  loss = 2.05388 avg_loss = 4.03453\n",
      "epoch no.0 train no.36570  loss = 4.28866 avg_loss = 3.98183\n",
      "epoch no.0 train no.36580  loss = 2.92646 avg_loss = 4.00026\n",
      "epoch no.0 train no.36590  loss = 6.88470 avg_loss = 4.03785\n",
      "epoch no.0 train no.36600  loss = 3.30625 avg_loss = 4.07697\n",
      "epoch no.0 train no.36610  loss = 2.58312 avg_loss = 4.00809\n",
      "epoch no.0 train no.36620  loss = 2.66147 avg_loss = 4.04736\n",
      "epoch no.0 train no.36630  loss = 3.26137 avg_loss = 4.04129\n",
      "epoch no.0 train no.36640  loss = 4.05619 avg_loss = 4.04547\n",
      "epoch no.0 train no.36650  loss = 4.61322 avg_loss = 4.10289\n",
      "epoch no.0 train no.36660  loss = 3.71774 avg_loss = 4.08521\n",
      "epoch no.0 train no.36670  loss = 6.32097 avg_loss = 4.10529\n",
      "epoch no.0 train no.36680  loss = 4.23416 avg_loss = 4.09671\n",
      "epoch no.0 train no.36690  loss = 7.30302 avg_loss = 4.11748\n",
      "epoch no.0 train no.36700  loss = 3.11053 avg_loss = 4.05996\n",
      "epoch no.0 train no.36710  loss = 3.03116 avg_loss = 4.07832\n",
      "epoch no.0 train no.36720  loss = 5.80273 avg_loss = 4.11971\n",
      "epoch no.0 train no.36730  loss = 4.94789 avg_loss = 4.11618\n",
      "epoch no.0 train no.36740  loss = 6.13570 avg_loss = 4.12935\n",
      "epoch no.0 train no.36750  loss = 2.96199 avg_loss = 4.10780\n",
      "epoch no.0 train no.36760  loss = 4.69427 avg_loss = 4.07548\n",
      "epoch no.0 train no.36770  loss = 3.50175 avg_loss = 4.06121\n",
      "epoch no.0 train no.36780  loss = 5.01343 avg_loss = 4.05898\n",
      "epoch no.0 train no.36790  loss = 3.00270 avg_loss = 4.02842\n",
      "epoch no.0 train no.36800  loss = 3.04925 avg_loss = 4.01194\n",
      "epoch no.0 train no.36810  loss = 3.89033 avg_loss = 3.95032\n",
      "epoch no.0 train no.36820  loss = 5.37565 avg_loss = 3.98235\n",
      "epoch no.0 train no.36830  loss = 4.91597 avg_loss = 3.99254\n",
      "epoch no.0 train no.36840  loss = 4.56430 avg_loss = 4.01728\n",
      "epoch no.0 train no.36850  loss = 3.80713 avg_loss = 4.06528\n",
      "epoch no.0 train no.36860  loss = 2.53824 avg_loss = 3.97467\n",
      "epoch no.0 train no.36870  loss = 2.50147 avg_loss = 3.93315\n",
      "epoch no.0 train no.36880  loss = 3.90710 avg_loss = 3.97598\n",
      "epoch no.0 train no.36890  loss = 2.90871 avg_loss = 3.92092\n",
      "epoch no.0 train no.36900  loss = 3.43510 avg_loss = 3.92650\n",
      "epoch no.0 train no.36910  loss = 3.67516 avg_loss = 3.91677\n",
      "epoch no.0 train no.36920  loss = 4.02506 avg_loss = 3.88122\n",
      "epoch no.0 train no.36930  loss = 2.94647 avg_loss = 3.86698\n",
      "epoch no.0 train no.36940  loss = 4.83754 avg_loss = 3.86977\n",
      "epoch no.0 train no.36950  loss = 3.44062 avg_loss = 3.83666\n",
      "epoch no.0 train no.36960  loss = 5.93653 avg_loss = 3.86778\n",
      "epoch no.0 train no.36970  loss = 5.13392 avg_loss = 3.93440\n",
      "epoch no.0 train no.36980  loss = 4.77323 avg_loss = 3.96697\n",
      "epoch no.0 train no.36990  loss = 4.28869 avg_loss = 3.89751\n",
      "epoch no.0 train no.37000  loss = 3.61995 avg_loss = 3.90949\n",
      "5\n",
      "to_tokens: ['▁가을', '▁노래', '년대', '▁발라', '송', '▁모음', '</s>']\n",
      "추억의 2000년대 팝송 모음</s>\n",
      "epoch no.0 train no.37010  loss = 4.61164 avg_loss = 3.84677\n",
      "epoch no.0 train no.37020  loss = 4.46923 avg_loss = 3.86290\n",
      "epoch no.0 train no.37030  loss = 4.38834 avg_loss = 3.87232\n",
      "epoch no.0 train no.37040  loss = 3.37589 avg_loss = 3.88349\n",
      "epoch no.0 train no.37050  loss = 1.80145 avg_loss = 3.83144\n",
      "epoch no.0 train no.37060  loss = 5.35519 avg_loss = 3.91144\n",
      "epoch no.0 train no.37070  loss = 4.87578 avg_loss = 3.89862\n",
      "epoch no.0 train no.37080  loss = 1.66515 avg_loss = 3.84196\n",
      "epoch no.0 train no.37090  loss = 2.62134 avg_loss = 3.79075\n",
      "epoch no.0 train no.37100  loss = 3.10804 avg_loss = 3.81816\n",
      "epoch no.0 train no.37110  loss = 2.54627 avg_loss = 3.81671\n",
      "epoch no.0 train no.37120  loss = 2.42805 avg_loss = 3.77334\n",
      "epoch no.0 train no.37130  loss = 3.88715 avg_loss = 3.77003\n",
      "epoch no.0 train no.37140  loss = 3.21073 avg_loss = 3.74104\n",
      "epoch no.0 train no.37150  loss = 4.01634 avg_loss = 3.71813\n",
      "epoch no.0 train no.37160  loss = 3.14947 avg_loss = 3.74964\n",
      "epoch no.0 train no.37170  loss = 3.49197 avg_loss = 3.77766\n",
      "epoch no.0 train no.37180  loss = 4.00249 avg_loss = 3.81908\n",
      "epoch no.0 train no.37190  loss = 3.27291 avg_loss = 3.82441\n",
      "epoch no.0 train no.37200  loss = 4.86444 avg_loss = 3.80721\n",
      "epoch no.0 train no.37210  loss = 2.38441 avg_loss = 3.80339\n",
      "epoch no.0 train no.37220  loss = 3.38623 avg_loss = 3.80552\n",
      "epoch no.0 train no.37230  loss = 4.46082 avg_loss = 3.83148\n",
      "epoch no.0 train no.37240  loss = 3.62251 avg_loss = 3.86546\n",
      "epoch no.0 train no.37250  loss = 1.26432 avg_loss = 3.80802\n",
      "epoch no.0 train no.37260  loss = 5.81484 avg_loss = 3.91770\n",
      "epoch no.0 train no.37270  loss = 2.63358 avg_loss = 3.90499\n",
      "epoch no.0 train no.37280  loss = 3.41267 avg_loss = 3.86218\n",
      "epoch no.0 train no.37290  loss = 2.17136 avg_loss = 3.83024\n",
      "epoch no.0 train no.37300  loss = 3.39146 avg_loss = 3.90069\n",
      "epoch no.0 train no.37310  loss = 4.31184 avg_loss = 3.85728\n",
      "epoch no.0 train no.37320  loss = 5.01366 avg_loss = 3.89902\n",
      "epoch no.0 train no.37330  loss = 3.86151 avg_loss = 3.93333\n",
      "epoch no.0 train no.37340  loss = 4.57996 avg_loss = 3.95526\n",
      "epoch no.0 train no.37350  loss = 1.48831 avg_loss = 3.95069\n",
      "epoch no.0 train no.37360  loss = 3.24520 avg_loss = 3.98197\n",
      "epoch no.0 train no.37370  loss = 6.17809 avg_loss = 3.99830\n",
      "epoch no.0 train no.37380  loss = 5.68209 avg_loss = 3.96245\n",
      "epoch no.0 train no.37390  loss = 4.05290 avg_loss = 3.93745\n",
      "epoch no.0 train no.37400  loss = 3.53664 avg_loss = 3.96519\n",
      "epoch no.0 train no.37410  loss = 3.00707 avg_loss = 3.96096\n",
      "epoch no.0 train no.37420  loss = 2.63535 avg_loss = 3.94646\n",
      "epoch no.0 train no.37430  loss = 1.77136 avg_loss = 3.93313\n",
      "epoch no.0 train no.37440  loss = 3.48169 avg_loss = 3.90779\n",
      "epoch no.0 train no.37450  loss = 5.79357 avg_loss = 3.94692\n",
      "epoch no.0 train no.37460  loss = 3.64838 avg_loss = 3.94274\n",
      "epoch no.0 train no.37470  loss = 2.34748 avg_loss = 3.85670\n",
      "epoch no.0 train no.37480  loss = 3.91155 avg_loss = 3.85237\n",
      "epoch no.0 train no.37490  loss = 2.36105 avg_loss = 3.85507\n",
      "epoch no.0 train no.37500  loss = 2.90640 avg_loss = 3.84701\n",
      "epoch no.0 train no.37510  loss = 3.67777 avg_loss = 3.80762\n",
      "epoch no.0 train no.37520  loss = 2.96948 avg_loss = 3.84921\n",
      "epoch no.0 train no.37530  loss = 4.31637 avg_loss = 3.87131\n",
      "epoch no.0 train no.37540  loss = 4.80351 avg_loss = 3.84345\n",
      "epoch no.0 train no.37550  loss = 4.14930 avg_loss = 3.88699\n",
      "epoch no.0 train no.37560  loss = 4.34226 avg_loss = 3.85093\n",
      "epoch no.0 train no.37570  loss = 5.32185 avg_loss = 3.88840\n",
      "epoch no.0 train no.37580  loss = 4.93466 avg_loss = 3.96137\n",
      "epoch no.0 train no.37590  loss = 3.48425 avg_loss = 3.97650\n",
      "epoch no.0 train no.37600  loss = 4.17906 avg_loss = 3.96466\n",
      "epoch no.0 train no.37610  loss = 2.95669 avg_loss = 3.96440\n",
      "epoch no.0 train no.37620  loss = 4.91277 avg_loss = 4.03203\n",
      "epoch no.0 train no.37630  loss = 4.42264 avg_loss = 4.03270\n",
      "epoch no.0 train no.37640  loss = 2.43175 avg_loss = 4.04155\n",
      "epoch no.0 train no.37650  loss = 3.13082 avg_loss = 4.06791\n",
      "epoch no.0 train no.37660  loss = 2.44701 avg_loss = 4.01860\n",
      "epoch no.0 train no.37670  loss = 1.88992 avg_loss = 3.96345\n",
      "epoch no.0 train no.37680  loss = 5.39021 avg_loss = 3.98384\n",
      "epoch no.0 train no.37690  loss = 3.86735 avg_loss = 3.98172\n",
      "epoch no.0 train no.37700  loss = 5.78824 avg_loss = 4.03466\n",
      "epoch no.0 train no.37710  loss = 3.48749 avg_loss = 3.98782\n",
      "epoch no.0 train no.37720  loss = 3.74141 avg_loss = 3.97272\n",
      "epoch no.0 train no.37730  loss = 3.27908 avg_loss = 4.06077\n",
      "epoch no.0 train no.37740  loss = 3.23057 avg_loss = 4.11286\n",
      "epoch no.0 train no.37750  loss = 4.59081 avg_loss = 4.16234\n",
      "epoch no.0 train no.37760  loss = 3.60764 avg_loss = 4.11787\n",
      "epoch no.0 train no.37770  loss = 2.23485 avg_loss = 4.04759\n",
      "epoch no.0 train no.37780  loss = 4.84260 avg_loss = 4.03974\n",
      "epoch no.0 train no.37790  loss = 3.76260 avg_loss = 4.04204\n",
      "epoch no.0 train no.37800  loss = 4.56436 avg_loss = 4.00491\n",
      "epoch no.0 train no.37810  loss = 2.44770 avg_loss = 4.05619\n",
      "epoch no.0 train no.37820  loss = 4.85391 avg_loss = 4.03343\n",
      "epoch no.0 train no.37830  loss = 3.96208 avg_loss = 3.97612\n",
      "epoch no.0 train no.37840  loss = 4.28190 avg_loss = 3.93468\n",
      "epoch no.0 train no.37850  loss = 2.81227 avg_loss = 3.91486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.37860  loss = 2.51654 avg_loss = 3.86951\n",
      "epoch no.0 train no.37870  loss = 4.47468 avg_loss = 3.86119\n",
      "epoch no.0 train no.37880  loss = 4.88435 avg_loss = 3.83059\n",
      "epoch no.0 train no.37890  loss = 2.27293 avg_loss = 3.86163\n",
      "epoch no.0 train no.37900  loss = 3.31909 avg_loss = 3.95113\n",
      "epoch no.0 train no.37910  loss = 4.42002 avg_loss = 3.98798\n",
      "epoch no.0 train no.37920  loss = 2.81725 avg_loss = 3.95701\n",
      "epoch no.0 train no.37930  loss = 4.95698 avg_loss = 3.98436\n",
      "epoch no.0 train no.37940  loss = 4.36315 avg_loss = 4.02025\n",
      "epoch no.0 train no.37950  loss = 5.79584 avg_loss = 4.03296\n",
      "epoch no.0 train no.37960  loss = 6.13476 avg_loss = 4.07761\n",
      "epoch no.0 train no.37970  loss = 2.52790 avg_loss = 4.05662\n",
      "epoch no.0 train no.37980  loss = 2.50769 avg_loss = 4.06426\n",
      "epoch no.0 train no.37990  loss = 3.23952 avg_loss = 4.04259\n",
      "epoch no.0 train no.38000  loss = 4.82089 avg_loss = 4.00146\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '드', '▁명', '음', '</s>']\n",
      "추억의 발라드모음</s>\n",
      "epoch no.0 train no.38010  loss = 4.62540 avg_loss = 3.96567\n",
      "epoch no.0 train no.38020  loss = 5.33851 avg_loss = 4.01100\n",
      "epoch no.0 train no.38030  loss = 5.02531 avg_loss = 4.11796\n",
      "epoch no.0 train no.38040  loss = 3.08936 avg_loss = 4.08762\n",
      "epoch no.0 train no.38050  loss = 3.53977 avg_loss = 4.06918\n",
      "epoch no.0 train no.38060  loss = 3.32942 avg_loss = 4.04754\n",
      "epoch no.0 train no.38070  loss = 3.48279 avg_loss = 4.10384\n",
      "epoch no.0 train no.38080  loss = 3.46957 avg_loss = 4.08076\n",
      "epoch no.0 train no.38090  loss = 2.90626 avg_loss = 4.04552\n",
      "epoch no.0 train no.38100  loss = 4.61699 avg_loss = 4.10886\n",
      "epoch no.0 train no.38110  loss = 4.54801 avg_loss = 4.06940\n",
      "epoch no.0 train no.38120  loss = 4.35030 avg_loss = 4.10616\n",
      "epoch no.0 train no.38130  loss = 3.52254 avg_loss = 4.07103\n",
      "epoch no.0 train no.38140  loss = 4.27036 avg_loss = 4.08279\n",
      "epoch no.0 train no.38150  loss = 4.67207 avg_loss = 4.10742\n",
      "epoch no.0 train no.38160  loss = 5.87282 avg_loss = 4.13322\n",
      "epoch no.0 train no.38170  loss = 4.81300 avg_loss = 4.10713\n",
      "epoch no.0 train no.38180  loss = 4.40556 avg_loss = 4.04937\n",
      "epoch no.0 train no.38190  loss = 3.02887 avg_loss = 3.98766\n",
      "epoch no.0 train no.38200  loss = 3.29650 avg_loss = 4.01024\n",
      "epoch no.0 train no.38210  loss = 2.05094 avg_loss = 3.94952\n",
      "epoch no.0 train no.38220  loss = 5.57674 avg_loss = 3.94783\n",
      "epoch no.0 train no.38230  loss = 5.08464 avg_loss = 4.00088\n",
      "epoch no.0 train no.38240  loss = 4.49091 avg_loss = 4.12233\n",
      "epoch no.0 train no.38250  loss = 3.46118 avg_loss = 4.08427\n",
      "epoch no.0 train no.38260  loss = 4.08157 avg_loss = 4.08107\n",
      "epoch no.0 train no.38270  loss = 2.65505 avg_loss = 4.05188\n",
      "epoch no.0 train no.38280  loss = 6.06314 avg_loss = 4.02491\n",
      "epoch no.0 train no.38290  loss = 4.67281 avg_loss = 4.03261\n",
      "epoch no.0 train no.38300  loss = 3.83323 avg_loss = 4.00430\n",
      "epoch no.0 train no.38310  loss = 3.06049 avg_loss = 3.96300\n",
      "epoch no.0 train no.38320  loss = 5.94000 avg_loss = 3.91719\n",
      "epoch no.0 train no.38330  loss = 5.52255 avg_loss = 3.94230\n",
      "epoch no.0 train no.38340  loss = 4.52803 avg_loss = 3.91224\n",
      "epoch no.0 train no.38350  loss = 2.30958 avg_loss = 3.92435\n",
      "epoch no.0 train no.38360  loss = 6.21918 avg_loss = 3.91691\n",
      "epoch no.0 train no.38370  loss = 4.17469 avg_loss = 3.91346\n",
      "epoch no.0 train no.38380  loss = 4.16912 avg_loss = 3.97460\n",
      "epoch no.0 train no.38390  loss = 3.86449 avg_loss = 3.99464\n",
      "epoch no.0 train no.38400  loss = 4.13361 avg_loss = 3.98448\n",
      "epoch no.0 train no.38410  loss = 5.39312 avg_loss = 3.95894\n",
      "epoch no.0 train no.38420  loss = 2.39856 avg_loss = 3.98132\n",
      "epoch no.0 train no.38430  loss = 6.65332 avg_loss = 3.97107\n",
      "epoch no.0 train no.38440  loss = 3.24168 avg_loss = 3.94673\n",
      "epoch no.0 train no.38450  loss = 4.31367 avg_loss = 3.94466\n",
      "epoch no.0 train no.38460  loss = 5.76799 avg_loss = 4.01238\n",
      "epoch no.0 train no.38470  loss = 3.76942 avg_loss = 4.01476\n",
      "epoch no.0 train no.38480  loss = 3.34919 avg_loss = 4.03378\n",
      "epoch no.0 train no.38490  loss = 2.65500 avg_loss = 4.02261\n",
      "epoch no.0 train no.38500  loss = 4.10892 avg_loss = 3.98184\n",
      "epoch no.0 train no.38510  loss = 3.81356 avg_loss = 3.96122\n",
      "epoch no.0 train no.38520  loss = 4.18869 avg_loss = 3.98953\n",
      "epoch no.0 train no.38530  loss = 2.97408 avg_loss = 3.93423\n",
      "epoch no.0 train no.38540  loss = 2.12948 avg_loss = 3.93659\n",
      "epoch no.0 train no.38550  loss = 4.43570 avg_loss = 3.95637\n",
      "epoch no.0 train no.38560  loss = 2.53969 avg_loss = 3.99430\n",
      "epoch no.0 train no.38570  loss = 3.11515 avg_loss = 4.03432\n",
      "epoch no.0 train no.38580  loss = 6.15396 avg_loss = 4.08601\n",
      "epoch no.0 train no.38590  loss = 5.27526 avg_loss = 4.08305\n",
      "epoch no.0 train no.38600  loss = 4.27279 avg_loss = 4.09282\n",
      "epoch no.0 train no.38610  loss = 5.02713 avg_loss = 4.08812\n",
      "epoch no.0 train no.38620  loss = 3.92186 avg_loss = 4.05732\n",
      "epoch no.0 train no.38630  loss = 3.27634 avg_loss = 4.00350\n",
      "epoch no.0 train no.38640  loss = 2.52323 avg_loss = 3.97249\n",
      "epoch no.0 train no.38650  loss = 4.94322 avg_loss = 3.92950\n",
      "epoch no.0 train no.38660  loss = 3.34747 avg_loss = 3.92897\n",
      "epoch no.0 train no.38670  loss = 4.14115 avg_loss = 3.92373\n",
      "epoch no.0 train no.38680  loss = 3.75348 avg_loss = 3.90086\n",
      "epoch no.0 train no.38690  loss = 6.23607 avg_loss = 3.99689\n",
      "epoch no.0 train no.38700  loss = 4.26774 avg_loss = 3.95240\n",
      "epoch no.0 train no.38710  loss = 2.98291 avg_loss = 3.92938\n",
      "epoch no.0 train no.38720  loss = 2.50706 avg_loss = 3.88286\n",
      "epoch no.0 train no.38730  loss = 3.92915 avg_loss = 3.92495\n",
      "epoch no.0 train no.38740  loss = 3.05311 avg_loss = 3.90394\n",
      "epoch no.0 train no.38750  loss = 2.56557 avg_loss = 3.99118\n",
      "epoch no.0 train no.38760  loss = 2.54322 avg_loss = 3.93416\n",
      "epoch no.0 train no.38770  loss = 3.19918 avg_loss = 3.92086\n",
      "epoch no.0 train no.38780  loss = 3.16434 avg_loss = 3.90408\n",
      "epoch no.0 train no.38790  loss = 2.64511 avg_loss = 3.86491\n",
      "epoch no.0 train no.38800  loss = 4.69790 avg_loss = 3.86323\n",
      "epoch no.0 train no.38810  loss = 3.54121 avg_loss = 3.83031\n",
      "epoch no.0 train no.38820  loss = 3.16180 avg_loss = 3.87518\n",
      "epoch no.0 train no.38830  loss = 2.67361 avg_loss = 3.78098\n",
      "epoch no.0 train no.38840  loss = 3.34713 avg_loss = 3.75139\n",
      "epoch no.0 train no.38850  loss = 3.19119 avg_loss = 3.73660\n",
      "epoch no.0 train no.38860  loss = 4.31742 avg_loss = 3.77960\n",
      "epoch no.0 train no.38870  loss = 2.64932 avg_loss = 3.70867\n",
      "epoch no.0 train no.38880  loss = 3.44341 avg_loss = 3.73417\n",
      "epoch no.0 train no.38890  loss = 3.56460 avg_loss = 3.72826\n",
      "epoch no.0 train no.38900  loss = 3.48778 avg_loss = 3.80677\n",
      "epoch no.0 train no.38910  loss = 2.53081 avg_loss = 3.79641\n",
      "epoch no.0 train no.38920  loss = 4.40190 avg_loss = 3.77531\n",
      "epoch no.0 train no.38930  loss = 2.54104 avg_loss = 3.82123\n",
      "epoch no.0 train no.38940  loss = 2.00781 avg_loss = 3.90823\n",
      "epoch no.0 train no.38950  loss = 3.04739 avg_loss = 3.97385\n",
      "epoch no.0 train no.38960  loss = 2.33351 avg_loss = 3.92303\n",
      "epoch no.0 train no.38970  loss = 5.00973 avg_loss = 3.90602\n",
      "epoch no.0 train no.38980  loss = 4.02572 avg_loss = 3.90876\n",
      "epoch no.0 train no.38990  loss = 4.03060 avg_loss = 4.00374\n",
      "epoch no.0 train no.39000  loss = 3.04915 avg_loss = 3.97648\n",
      "3\n",
      "to_tokens: ['▁비', '▁노래', '들', '음', '</s>']\n",
      "추억의 노래모음</s>\n",
      "epoch no.0 train no.39010  loss = 4.41654 avg_loss = 3.93674\n",
      "epoch no.0 train no.39020  loss = 6.86084 avg_loss = 3.98914\n",
      "epoch no.0 train no.39030  loss = 3.22143 avg_loss = 3.97661\n",
      "epoch no.0 train no.39040  loss = 4.91317 avg_loss = 3.97507\n",
      "epoch no.0 train no.39050  loss = 4.24102 avg_loss = 4.00904\n",
      "epoch no.0 train no.39060  loss = 4.98175 avg_loss = 4.04511\n",
      "epoch no.0 train no.39070  loss = 6.65537 avg_loss = 4.06894\n",
      "epoch no.0 train no.39080  loss = 5.46869 avg_loss = 4.11918\n",
      "epoch no.0 train no.39090  loss = 2.54435 avg_loss = 4.12033\n",
      "epoch no.0 train no.39100  loss = 4.28930 avg_loss = 4.08472\n",
      "epoch no.0 train no.39110  loss = 5.08383 avg_loss = 4.05197\n",
      "epoch no.0 train no.39120  loss = 3.33960 avg_loss = 4.08586\n",
      "epoch no.0 train no.39130  loss = 2.90663 avg_loss = 4.06327\n",
      "epoch no.0 train no.39140  loss = 4.38699 avg_loss = 4.15238\n",
      "epoch no.0 train no.39150  loss = 4.25054 avg_loss = 4.06169\n",
      "epoch no.0 train no.39160  loss = 2.11290 avg_loss = 4.01161\n",
      "epoch no.0 train no.39170  loss = 3.04654 avg_loss = 3.96664\n",
      "epoch no.0 train no.39180  loss = 2.52018 avg_loss = 3.89997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.39190  loss = 4.26996 avg_loss = 3.96223\n",
      "epoch no.0 train no.39200  loss = 2.08739 avg_loss = 3.95806\n",
      "epoch no.0 train no.39210  loss = 3.42196 avg_loss = 3.97754\n",
      "epoch no.0 train no.39220  loss = 6.62030 avg_loss = 3.97960\n",
      "epoch no.0 train no.39230  loss = 7.99602 avg_loss = 4.01044\n",
      "epoch no.0 train no.39240  loss = 5.13031 avg_loss = 3.99609\n",
      "epoch no.0 train no.39250  loss = 3.21235 avg_loss = 3.96311\n",
      "epoch no.0 train no.39260  loss = 2.85296 avg_loss = 3.94863\n",
      "epoch no.0 train no.39270  loss = 3.95484 avg_loss = 3.91410\n",
      "epoch no.0 train no.39280  loss = 3.69260 avg_loss = 3.89886\n",
      "epoch no.0 train no.39290  loss = 5.08104 avg_loss = 3.92905\n",
      "epoch no.0 train no.39300  loss = 4.48674 avg_loss = 3.98577\n",
      "epoch no.0 train no.39310  loss = 3.48283 avg_loss = 3.97025\n",
      "epoch no.0 train no.39320  loss = 4.56367 avg_loss = 4.00670\n",
      "epoch no.0 train no.39330  loss = 3.01430 avg_loss = 4.00378\n",
      "epoch no.0 train no.39340  loss = 2.96684 avg_loss = 4.04389\n",
      "epoch no.0 train no.39350  loss = 4.23282 avg_loss = 4.03361\n",
      "epoch no.0 train no.39360  loss = 4.35979 avg_loss = 4.01688\n",
      "epoch no.0 train no.39370  loss = 3.16585 avg_loss = 4.01990\n",
      "epoch no.0 train no.39380  loss = 2.32475 avg_loss = 3.97214\n",
      "epoch no.0 train no.39390  loss = 2.58264 avg_loss = 3.96449\n",
      "epoch no.0 train no.39400  loss = 1.78861 avg_loss = 3.98319\n",
      "epoch no.0 train no.39410  loss = 6.57686 avg_loss = 4.00566\n",
      "epoch no.0 train no.39420  loss = 4.49269 avg_loss = 3.99203\n",
      "epoch no.0 train no.39430  loss = 2.10258 avg_loss = 4.00409\n",
      "epoch no.0 train no.39440  loss = 4.87643 avg_loss = 4.03332\n",
      "epoch no.0 train no.39450  loss = 1.50206 avg_loss = 4.03489\n",
      "epoch no.0 train no.39460  loss = 5.29989 avg_loss = 3.99463\n",
      "epoch no.0 train no.39470  loss = 3.82712 avg_loss = 3.93853\n",
      "epoch no.0 train no.39480  loss = 3.24320 avg_loss = 3.92550\n",
      "epoch no.0 train no.39490  loss = 4.13185 avg_loss = 3.91654\n",
      "epoch no.0 train no.39500  loss = 3.12915 avg_loss = 3.99271\n",
      "epoch no.0 train no.39510  loss = 6.14130 avg_loss = 3.99488\n",
      "epoch no.0 train no.39520  loss = 7.40725 avg_loss = 4.00028\n",
      "epoch no.0 train no.39530  loss = 4.82346 avg_loss = 3.93398\n",
      "epoch no.0 train no.39540  loss = 4.39126 avg_loss = 3.89356\n",
      "epoch no.0 train no.39550  loss = 2.39214 avg_loss = 3.87064\n",
      "epoch no.0 train no.39560  loss = 3.40289 avg_loss = 3.88776\n",
      "epoch no.0 train no.39570  loss = 3.26030 avg_loss = 3.87883\n",
      "epoch no.0 train no.39580  loss = 5.06651 avg_loss = 3.83646\n",
      "epoch no.0 train no.39590  loss = 3.82762 avg_loss = 3.85643\n",
      "epoch no.0 train no.39600  loss = 4.26026 avg_loss = 3.84802\n",
      "epoch no.0 train no.39610  loss = 4.24341 avg_loss = 3.85588\n",
      "epoch no.0 train no.39620  loss = 4.07008 avg_loss = 3.86940\n",
      "epoch no.0 train no.39630  loss = 5.22381 avg_loss = 3.88538\n",
      "epoch no.0 train no.39640  loss = 4.34877 avg_loss = 3.87606\n",
      "epoch no.0 train no.39650  loss = 3.55998 avg_loss = 3.93140\n",
      "epoch no.0 train no.39660  loss = 4.17907 avg_loss = 3.92905\n",
      "epoch no.0 train no.39670  loss = 2.92750 avg_loss = 3.92894\n",
      "epoch no.0 train no.39680  loss = 3.18057 avg_loss = 3.91218\n",
      "epoch no.0 train no.39690  loss = 4.55434 avg_loss = 3.90421\n",
      "epoch no.0 train no.39700  loss = 3.69015 avg_loss = 3.92237\n",
      "epoch no.0 train no.39710  loss = 3.70804 avg_loss = 3.91481\n",
      "epoch no.0 train no.39720  loss = 4.49513 avg_loss = 3.97182\n",
      "epoch no.0 train no.39730  loss = 3.92578 avg_loss = 3.99025\n",
      "epoch no.0 train no.39740  loss = 3.45252 avg_loss = 3.97294\n",
      "epoch no.0 train no.39750  loss = 2.28831 avg_loss = 3.92760\n",
      "epoch no.0 train no.39760  loss = 5.12886 avg_loss = 3.92873\n",
      "epoch no.0 train no.39770  loss = 4.00147 avg_loss = 3.92691\n",
      "epoch no.0 train no.39780  loss = 3.54209 avg_loss = 3.90448\n",
      "epoch no.0 train no.39790  loss = 5.01154 avg_loss = 3.93424\n",
      "epoch no.0 train no.39800  loss = 4.53044 avg_loss = 3.97692\n",
      "epoch no.0 train no.39810  loss = 4.30323 avg_loss = 3.91824\n",
      "epoch no.0 train no.39820  loss = 4.03833 avg_loss = 3.93162\n",
      "epoch no.0 train no.39830  loss = 4.72421 avg_loss = 3.93921\n",
      "epoch no.0 train no.39840  loss = 2.05005 avg_loss = 3.98513\n",
      "epoch no.0 train no.39850  loss = 3.15753 avg_loss = 4.00615\n",
      "epoch no.0 train no.39860  loss = 3.41528 avg_loss = 4.01385\n",
      "epoch no.0 train no.39870  loss = 2.83450 avg_loss = 3.97524\n",
      "epoch no.0 train no.39880  loss = 2.84578 avg_loss = 3.96568\n",
      "epoch no.0 train no.39890  loss = 4.26432 avg_loss = 3.94808\n",
      "epoch no.0 train no.39900  loss = 2.97983 avg_loss = 3.87767\n",
      "epoch no.0 train no.39910  loss = 2.02273 avg_loss = 3.87384\n",
      "epoch no.0 train no.39920  loss = 4.51154 avg_loss = 3.89182\n",
      "epoch no.0 train no.39930  loss = 4.10855 avg_loss = 3.94995\n",
      "epoch no.0 train no.39940  loss = 5.93252 avg_loss = 3.98327\n",
      "epoch no.0 train no.39950  loss = 5.27831 avg_loss = 3.98896\n",
      "epoch no.0 train no.39960  loss = 3.67989 avg_loss = 3.98496\n",
      "epoch no.0 train no.39970  loss = 2.18699 avg_loss = 3.98121\n",
      "epoch no.0 train no.39980  loss = 5.86674 avg_loss = 4.05888\n",
      "epoch no.0 train no.39990  loss = 1.91143 avg_loss = 3.99394\n",
      "epoch no.0 train no.40000  loss = 3.58826 avg_loss = 3.93898\n",
      "3\n",
      "to_tokens: ['▁비', '▁노래', '드', '▁모음', '</s>']\n",
      "추억의 발라드 모음</s>\n",
      "epoch no.0 train no.40010  loss = 2.58740 avg_loss = 3.84916\n",
      "epoch no.0 train no.40020  loss = 4.55114 avg_loss = 3.86308\n",
      "epoch no.0 train no.40030  loss = 2.40522 avg_loss = 3.89306\n",
      "epoch no.0 train no.40040  loss = 2.34423 avg_loss = 3.81093\n",
      "epoch no.0 train no.40050  loss = 6.29823 avg_loss = 3.84811\n",
      "epoch no.0 train no.40060  loss = 2.92323 avg_loss = 3.89102\n",
      "epoch no.0 train no.40070  loss = 4.35753 avg_loss = 3.92498\n",
      "epoch no.0 train no.40080  loss = 4.31565 avg_loss = 3.96447\n",
      "epoch no.0 train no.40090  loss = 3.66857 avg_loss = 3.91639\n",
      "epoch no.0 train no.40100  loss = 5.27674 avg_loss = 3.93835\n",
      "epoch no.0 train no.40110  loss = 3.18575 avg_loss = 3.93637\n",
      "epoch no.0 train no.40120  loss = 3.75860 avg_loss = 3.92678\n",
      "epoch no.0 train no.40130  loss = 4.54139 avg_loss = 3.92495\n",
      "epoch no.0 train no.40140  loss = 3.06917 avg_loss = 3.92938\n",
      "epoch no.0 train no.40150  loss = 4.76093 avg_loss = 3.96826\n",
      "epoch no.0 train no.40160  loss = 2.55433 avg_loss = 3.90098\n",
      "epoch no.0 train no.40170  loss = 4.52738 avg_loss = 3.87919\n",
      "epoch no.0 train no.40180  loss = 4.37465 avg_loss = 3.87069\n",
      "epoch no.0 train no.40190  loss = 3.08977 avg_loss = 3.91983\n",
      "epoch no.0 train no.40200  loss = 3.64066 avg_loss = 3.93192\n",
      "epoch no.0 train no.40210  loss = 3.79298 avg_loss = 3.90224\n",
      "epoch no.0 train no.40220  loss = 4.76112 avg_loss = 3.91376\n",
      "epoch no.0 train no.40230  loss = 6.03784 avg_loss = 3.88869\n",
      "epoch no.0 train no.40240  loss = 3.14327 avg_loss = 3.89629\n",
      "epoch no.0 train no.40250  loss = 4.02815 avg_loss = 3.92881\n",
      "epoch no.0 train no.40260  loss = 3.01563 avg_loss = 3.93518\n",
      "epoch no.0 train no.40270  loss = 2.91518 avg_loss = 3.91296\n",
      "epoch no.0 train no.40280  loss = 1.88703 avg_loss = 3.89335\n",
      "epoch no.0 train no.40290  loss = 4.25436 avg_loss = 3.92299\n",
      "epoch no.0 train no.40300  loss = 5.10673 avg_loss = 3.94197\n",
      "epoch no.0 train no.40310  loss = 2.93519 avg_loss = 3.93101\n",
      "epoch no.0 train no.40320  loss = 2.28405 avg_loss = 3.90547\n",
      "epoch no.0 train no.40330  loss = 3.72216 avg_loss = 3.89745\n",
      "epoch no.0 train no.40340  loss = 3.34874 avg_loss = 3.87862\n",
      "epoch no.0 train no.40350  loss = 3.12629 avg_loss = 3.85439\n",
      "epoch no.0 train no.40360  loss = 3.03088 avg_loss = 3.81021\n",
      "epoch no.0 train no.40370  loss = 2.69730 avg_loss = 3.76168\n",
      "epoch no.0 train no.40380  loss = 2.65472 avg_loss = 3.74181\n",
      "epoch no.0 train no.40390  loss = 1.79268 avg_loss = 3.75904\n",
      "epoch no.0 train no.40400  loss = 3.18916 avg_loss = 3.72622\n",
      "epoch no.0 train no.40410  loss = 2.46613 avg_loss = 3.73307\n",
      "epoch no.0 train no.40420  loss = 5.09869 avg_loss = 3.75157\n",
      "epoch no.0 train no.40430  loss = 7.69823 avg_loss = 3.80185\n",
      "epoch no.0 train no.40440  loss = 3.59989 avg_loss = 3.78351\n",
      "epoch no.0 train no.40450  loss = 2.54462 avg_loss = 3.81067\n",
      "epoch no.0 train no.40460  loss = 3.03713 avg_loss = 3.80129\n",
      "epoch no.0 train no.40470  loss = 3.08185 avg_loss = 3.80588\n",
      "epoch no.0 train no.40480  loss = 3.82696 avg_loss = 3.80796\n",
      "epoch no.0 train no.40490  loss = 5.36449 avg_loss = 3.80556\n",
      "epoch no.0 train no.40500  loss = 2.87918 avg_loss = 3.79337\n",
      "epoch no.0 train no.40510  loss = 3.32438 avg_loss = 3.77132\n",
      "epoch no.0 train no.40520  loss = 3.94399 avg_loss = 3.76377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.40530  loss = 5.07344 avg_loss = 3.77335\n",
      "epoch no.0 train no.40540  loss = 2.59808 avg_loss = 3.74311\n",
      "epoch no.0 train no.40550  loss = 5.32007 avg_loss = 3.78349\n",
      "epoch no.0 train no.40560  loss = 6.90618 avg_loss = 3.87548\n",
      "epoch no.0 train no.40570  loss = 3.89342 avg_loss = 3.90414\n",
      "epoch no.0 train no.40580  loss = 5.31945 avg_loss = 3.91513\n",
      "epoch no.0 train no.40590  loss = 7.40260 avg_loss = 3.93123\n",
      "epoch no.0 train no.40600  loss = 2.53921 avg_loss = 3.96559\n",
      "epoch no.0 train no.40610  loss = 5.36937 avg_loss = 3.99396\n",
      "epoch no.0 train no.40620  loss = 4.12610 avg_loss = 4.00221\n",
      "epoch no.0 train no.40630  loss = 5.11955 avg_loss = 3.99140\n",
      "epoch no.0 train no.40640  loss = 2.44398 avg_loss = 3.94154\n",
      "epoch no.0 train no.40650  loss = 3.51047 avg_loss = 3.97035\n",
      "epoch no.0 train no.40660  loss = 2.94683 avg_loss = 4.00293\n",
      "epoch no.0 train no.40670  loss = 4.26233 avg_loss = 3.98394\n",
      "epoch no.0 train no.40680  loss = 4.60267 avg_loss = 4.03355\n",
      "epoch no.0 train no.40690  loss = 3.55046 avg_loss = 4.03708\n",
      "epoch no.0 train no.40700  loss = 4.31670 avg_loss = 4.01418\n",
      "epoch no.0 train no.40710  loss = 5.26085 avg_loss = 3.99121\n",
      "epoch no.0 train no.40720  loss = 4.76381 avg_loss = 4.01133\n",
      "epoch no.0 train no.40730  loss = 2.19013 avg_loss = 3.96130\n",
      "epoch no.0 train no.40740  loss = 5.06118 avg_loss = 3.94608\n",
      "epoch no.0 train no.40750  loss = 3.89881 avg_loss = 3.92686\n",
      "epoch no.0 train no.40760  loss = 3.76338 avg_loss = 3.94363\n",
      "epoch no.0 train no.40770  loss = 2.35150 avg_loss = 3.92452\n",
      "epoch no.0 train no.40780  loss = 4.82411 avg_loss = 3.92304\n",
      "epoch no.0 train no.40790  loss = 4.85923 avg_loss = 3.96388\n",
      "epoch no.0 train no.40800  loss = 5.17223 avg_loss = 3.96915\n",
      "epoch no.0 train no.40810  loss = 2.60591 avg_loss = 3.93035\n",
      "epoch no.0 train no.40820  loss = 3.41023 avg_loss = 3.88127\n",
      "epoch no.0 train no.40830  loss = 3.36705 avg_loss = 3.85231\n",
      "epoch no.0 train no.40840  loss = 3.39444 avg_loss = 3.77705\n",
      "epoch no.0 train no.40850  loss = 3.78528 avg_loss = 3.83466\n",
      "epoch no.0 train no.40860  loss = 2.26811 avg_loss = 3.85801\n",
      "epoch no.0 train no.40870  loss = 4.25691 avg_loss = 3.88546\n",
      "epoch no.0 train no.40880  loss = 3.42181 avg_loss = 3.90421\n",
      "epoch no.0 train no.40890  loss = 3.93484 avg_loss = 3.87084\n",
      "epoch no.0 train no.40900  loss = 2.80121 avg_loss = 3.89477\n",
      "epoch no.0 train no.40910  loss = 3.39082 avg_loss = 3.91524\n",
      "epoch no.0 train no.40920  loss = 5.72624 avg_loss = 3.89660\n",
      "epoch no.0 train no.40930  loss = 6.80659 avg_loss = 3.99338\n",
      "epoch no.0 train no.40940  loss = 5.12233 avg_loss = 3.97930\n",
      "epoch no.0 train no.40950  loss = 2.50296 avg_loss = 3.94554\n",
      "epoch no.0 train no.40960  loss = 2.85204 avg_loss = 3.88144\n",
      "epoch no.0 train no.40970  loss = 5.83181 avg_loss = 3.90907\n",
      "epoch no.0 train no.40980  loss = 3.68431 avg_loss = 3.88666\n",
      "epoch no.0 train no.40990  loss = 3.43462 avg_loss = 3.88794\n",
      "epoch no.0 train no.41000  loss = 2.65144 avg_loss = 3.87355\n",
      "5\n",
      "to_tokens: ['▁비', '▁명', '년대', '▁발라', '▁o', 'st', '</s>']\n",
      "추억의 2000년대 드라마 ost</s>\n",
      "epoch no.0 train no.41010  loss = 5.52824 avg_loss = 3.85044\n",
      "epoch no.0 train no.41020  loss = 2.69120 avg_loss = 3.90038\n",
      "epoch no.0 train no.41030  loss = 4.60583 avg_loss = 3.88781\n",
      "epoch no.0 train no.41040  loss = 2.20038 avg_loss = 3.87243\n",
      "epoch no.0 train no.41050  loss = 5.85613 avg_loss = 3.89199\n",
      "epoch no.0 train no.41060  loss = 3.45346 avg_loss = 3.84328\n",
      "epoch no.0 train no.41070  loss = 6.88157 avg_loss = 3.89521\n",
      "epoch no.0 train no.41080  loss = 3.76044 avg_loss = 3.90528\n",
      "epoch no.0 train no.41090  loss = 5.84669 avg_loss = 3.94282\n",
      "epoch no.0 train no.41100  loss = 1.71305 avg_loss = 3.90322\n",
      "epoch no.0 train no.41110  loss = 2.96152 avg_loss = 3.87626\n",
      "epoch no.0 train no.41120  loss = 3.07487 avg_loss = 3.84994\n",
      "epoch no.0 train no.41130  loss = 4.24937 avg_loss = 3.90733\n",
      "epoch no.0 train no.41140  loss = 5.92723 avg_loss = 3.92015\n",
      "epoch no.0 train no.41150  loss = 4.42841 avg_loss = 3.89531\n",
      "epoch no.0 train no.41160  loss = 3.99449 avg_loss = 3.90434\n",
      "epoch no.0 train no.41170  loss = 3.95741 avg_loss = 3.92173\n",
      "epoch no.0 train no.41180  loss = 4.09689 avg_loss = 3.96311\n",
      "epoch no.0 train no.41190  loss = 5.04400 avg_loss = 3.94252\n",
      "epoch no.0 train no.41200  loss = 3.90944 avg_loss = 3.94640\n",
      "epoch no.0 train no.41210  loss = 3.64360 avg_loss = 4.00756\n",
      "epoch no.0 train no.41220  loss = 3.13585 avg_loss = 3.99384\n",
      "epoch no.0 train no.41230  loss = 4.92567 avg_loss = 3.97345\n",
      "epoch no.0 train no.41240  loss = 4.89120 avg_loss = 3.91902\n",
      "epoch no.0 train no.41250  loss = 3.77470 avg_loss = 3.89364\n",
      "epoch no.0 train no.41260  loss = 4.59376 avg_loss = 3.97510\n",
      "epoch no.0 train no.41270  loss = 3.62657 avg_loss = 4.04875\n",
      "epoch no.0 train no.41280  loss = 4.11456 avg_loss = 3.98645\n",
      "epoch no.0 train no.41290  loss = 5.06377 avg_loss = 3.99106\n",
      "epoch no.0 train no.41300  loss = 3.96513 avg_loss = 3.96484\n",
      "epoch no.0 train no.41310  loss = 2.91829 avg_loss = 3.95439\n",
      "epoch no.0 train no.41320  loss = 4.15721 avg_loss = 3.92449\n",
      "epoch no.0 train no.41330  loss = 3.86061 avg_loss = 3.96936\n",
      "epoch no.0 train no.41340  loss = 3.42094 avg_loss = 3.92826\n",
      "epoch no.0 train no.41350  loss = 6.48900 avg_loss = 3.87579\n",
      "epoch no.0 train no.41360  loss = 3.98331 avg_loss = 3.86441\n",
      "epoch no.0 train no.41370  loss = 3.39694 avg_loss = 3.90683\n",
      "epoch no.0 train no.41380  loss = 3.13180 avg_loss = 3.94942\n",
      "epoch no.0 train no.41390  loss = 3.79851 avg_loss = 3.90744\n",
      "epoch no.0 train no.41400  loss = 6.59666 avg_loss = 4.00084\n",
      "epoch no.0 train no.41410  loss = 2.79569 avg_loss = 4.02988\n",
      "epoch no.0 train no.41420  loss = 4.59028 avg_loss = 4.05978\n",
      "epoch no.0 train no.41430  loss = 1.90422 avg_loss = 4.02759\n",
      "epoch no.0 train no.41440  loss = 5.27506 avg_loss = 4.03734\n",
      "epoch no.0 train no.41450  loss = 4.96646 avg_loss = 4.14224\n",
      "epoch no.0 train no.41460  loss = 5.06090 avg_loss = 4.14977\n",
      "epoch no.0 train no.41470  loss = 4.71112 avg_loss = 4.11825\n",
      "epoch no.0 train no.41480  loss = 2.46871 avg_loss = 4.04138\n",
      "epoch no.0 train no.41490  loss = 4.63145 avg_loss = 4.02090\n",
      "epoch no.0 train no.41500  loss = 3.20622 avg_loss = 4.01572\n",
      "epoch no.0 train no.41510  loss = 2.77193 avg_loss = 3.97022\n",
      "epoch no.0 train no.41520  loss = 3.54625 avg_loss = 3.95688\n",
      "epoch no.0 train no.41530  loss = 3.58951 avg_loss = 3.91309\n",
      "epoch no.0 train no.41540  loss = 5.29989 avg_loss = 3.94977\n",
      "epoch no.0 train no.41550  loss = 3.37868 avg_loss = 3.98067\n",
      "epoch no.0 train no.41560  loss = 3.92388 avg_loss = 3.93689\n",
      "epoch no.0 train no.41570  loss = 5.61898 avg_loss = 3.96855\n",
      "epoch no.0 train no.41580  loss = 2.36599 avg_loss = 3.99623\n",
      "epoch no.0 train no.41590  loss = 5.84180 avg_loss = 4.02888\n",
      "epoch no.0 train no.41600  loss = 3.65334 avg_loss = 4.00851\n",
      "epoch no.0 train no.41610  loss = 4.37754 avg_loss = 4.03452\n",
      "epoch no.0 train no.41620  loss = 3.81890 avg_loss = 4.03184\n",
      "epoch no.0 train no.41630  loss = 2.97261 avg_loss = 4.02962\n",
      "epoch no.0 train no.41640  loss = 7.82714 avg_loss = 4.07150\n",
      "epoch no.0 train no.41650  loss = 2.59194 avg_loss = 4.04696\n",
      "epoch no.0 train no.41660  loss = 3.98787 avg_loss = 3.98543\n",
      "epoch no.0 train no.41670  loss = 3.83056 avg_loss = 3.97618\n",
      "epoch no.0 train no.41680  loss = 3.39787 avg_loss = 3.88844\n",
      "epoch no.0 train no.41690  loss = 4.35035 avg_loss = 3.90012\n",
      "epoch no.0 train no.41700  loss = 2.92030 avg_loss = 3.91497\n",
      "epoch no.0 train no.41710  loss = 5.70397 avg_loss = 3.89097\n",
      "epoch no.0 train no.41720  loss = 3.95764 avg_loss = 3.88107\n",
      "epoch no.0 train no.41730  loss = 4.94318 avg_loss = 3.86259\n",
      "epoch no.0 train no.41740  loss = 5.49376 avg_loss = 3.89179\n",
      "epoch no.0 train no.41750  loss = 3.26985 avg_loss = 3.88471\n",
      "epoch no.0 train no.41760  loss = 6.96492 avg_loss = 3.94352\n",
      "epoch no.0 train no.41770  loss = 3.04225 avg_loss = 3.93063\n",
      "epoch no.0 train no.41780  loss = 6.11695 avg_loss = 3.98354\n",
      "epoch no.0 train no.41790  loss = 3.37126 avg_loss = 3.96181\n",
      "epoch no.0 train no.41800  loss = 3.53013 avg_loss = 3.99487\n",
      "epoch no.0 train no.41810  loss = 2.69812 avg_loss = 3.98619\n",
      "epoch no.0 train no.41820  loss = 4.21921 avg_loss = 3.96133\n",
      "epoch no.0 train no.41830  loss = 3.40035 avg_loss = 3.94640\n",
      "epoch no.0 train no.41840  loss = 2.19953 avg_loss = 3.91786\n",
      "epoch no.0 train no.41850  loss = 2.69085 avg_loss = 3.86611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.41860  loss = 6.51305 avg_loss = 3.92856\n",
      "epoch no.0 train no.41870  loss = 4.19676 avg_loss = 3.94405\n",
      "epoch no.0 train no.41880  loss = 2.92763 avg_loss = 3.92382\n",
      "epoch no.0 train no.41890  loss = 3.27651 avg_loss = 3.90285\n",
      "epoch no.0 train no.41900  loss = 4.60076 avg_loss = 3.93227\n",
      "epoch no.0 train no.41910  loss = 4.02497 avg_loss = 3.95442\n",
      "epoch no.0 train no.41920  loss = 7.05102 avg_loss = 3.93742\n",
      "epoch no.0 train no.41930  loss = 3.80950 avg_loss = 3.93036\n",
      "epoch no.0 train no.41940  loss = 3.72126 avg_loss = 3.93639\n",
      "epoch no.0 train no.41950  loss = 3.58754 avg_loss = 3.95712\n",
      "epoch no.0 train no.41960  loss = 3.24062 avg_loss = 3.97489\n",
      "epoch no.0 train no.41970  loss = 4.57568 avg_loss = 3.99046\n",
      "epoch no.0 train no.41980  loss = 2.88139 avg_loss = 3.96811\n",
      "epoch no.0 train no.41990  loss = 2.81473 avg_loss = 3.99403\n",
      "epoch no.0 train no.42000  loss = 3.88915 avg_loss = 4.00264\n",
      "2\n",
      "to_tokens: ['▁비', '▁명', '드', '▁명']\n",
      "추억의 발라드</s>\n",
      "epoch no.0 train no.42010  loss = 7.39985 avg_loss = 4.07223\n",
      "epoch no.0 train no.42020  loss = 2.44961 avg_loss = 4.10187\n",
      "epoch no.0 train no.42030  loss = 4.23317 avg_loss = 4.14586\n",
      "epoch no.0 train no.42040  loss = 3.56700 avg_loss = 4.16753\n",
      "epoch no.0 train no.42050  loss = 4.07145 avg_loss = 4.09395\n",
      "epoch no.0 train no.42060  loss = 1.79524 avg_loss = 4.05281\n",
      "epoch no.0 train no.42070  loss = 4.61331 avg_loss = 4.06611\n",
      "epoch no.0 train no.42080  loss = 5.80297 avg_loss = 4.04848\n",
      "epoch no.0 train no.42090  loss = 2.84840 avg_loss = 4.05300\n",
      "epoch no.0 train no.42100  loss = 5.71158 avg_loss = 4.02490\n",
      "epoch no.0 train no.42110  loss = 3.98785 avg_loss = 4.00702\n",
      "epoch no.0 train no.42120  loss = 5.02166 avg_loss = 3.94854\n",
      "epoch no.0 train no.42130  loss = 3.05130 avg_loss = 3.89763\n",
      "epoch no.0 train no.42140  loss = 4.53969 avg_loss = 3.90073\n",
      "epoch no.0 train no.42150  loss = 3.41330 avg_loss = 3.90152\n",
      "epoch no.0 train no.42160  loss = 3.57964 avg_loss = 3.91916\n",
      "epoch no.0 train no.42170  loss = 4.02137 avg_loss = 3.91179\n",
      "epoch no.0 train no.42180  loss = 2.52743 avg_loss = 3.86961\n",
      "epoch no.0 train no.42190  loss = 2.48509 avg_loss = 3.81350\n",
      "epoch no.0 train no.42200  loss = 3.06141 avg_loss = 3.80801\n",
      "epoch no.0 train no.42210  loss = 3.77071 avg_loss = 3.86040\n",
      "epoch no.0 train no.42220  loss = 3.48728 avg_loss = 3.89276\n",
      "epoch no.0 train no.42230  loss = 3.58577 avg_loss = 3.85395\n",
      "epoch no.0 train no.42240  loss = 3.46477 avg_loss = 3.85076\n",
      "epoch no.0 train no.42250  loss = 1.93438 avg_loss = 3.81221\n",
      "epoch no.0 train no.42260  loss = 4.54466 avg_loss = 3.80292\n",
      "epoch no.0 train no.42270  loss = 3.34154 avg_loss = 3.78303\n",
      "epoch no.0 train no.42280  loss = 5.88727 avg_loss = 3.80720\n",
      "epoch no.0 train no.42290  loss = 3.53658 avg_loss = 3.79800\n",
      "epoch no.0 train no.42300  loss = 3.97334 avg_loss = 3.71627\n",
      "epoch no.0 train no.42310  loss = 5.16430 avg_loss = 3.73082\n",
      "epoch no.0 train no.42320  loss = 3.20345 avg_loss = 3.71992\n",
      "epoch no.0 train no.42330  loss = 5.25804 avg_loss = 3.74933\n",
      "epoch no.0 train no.42340  loss = 5.93054 avg_loss = 3.81715\n",
      "epoch no.0 train no.42350  loss = 2.66921 avg_loss = 3.77959\n",
      "epoch no.0 train no.42360  loss = 3.67130 avg_loss = 3.80127\n",
      "epoch no.0 train no.42370  loss = 4.33173 avg_loss = 3.80029\n",
      "epoch no.0 train no.42380  loss = 2.51406 avg_loss = 3.83949\n",
      "epoch no.0 train no.42390  loss = 3.70079 avg_loss = 3.86621\n",
      "epoch no.0 train no.42400  loss = 3.06075 avg_loss = 3.84017\n",
      "epoch no.0 train no.42410  loss = 5.84478 avg_loss = 3.82978\n",
      "epoch no.0 train no.42420  loss = 4.35447 avg_loss = 3.84847\n",
      "epoch no.0 train no.42430  loss = 2.18027 avg_loss = 3.82317\n",
      "epoch no.0 train no.42440  loss = 3.71465 avg_loss = 3.82280\n",
      "epoch no.0 train no.42450  loss = 3.95841 avg_loss = 3.79950\n",
      "epoch no.0 train no.42460  loss = 4.60321 avg_loss = 3.81383\n",
      "epoch no.0 train no.42470  loss = 3.86512 avg_loss = 3.77323\n",
      "epoch no.0 train no.42480  loss = 3.58101 avg_loss = 3.82523\n",
      "epoch no.0 train no.42490  loss = 2.93399 avg_loss = 3.82345\n",
      "epoch no.0 train no.42500  loss = 6.82309 avg_loss = 3.83623\n",
      "epoch no.0 train no.42510  loss = 4.03333 avg_loss = 3.81920\n",
      "epoch no.0 train no.42520  loss = 4.10965 avg_loss = 3.77721\n",
      "epoch no.0 train no.42530  loss = 2.35075 avg_loss = 3.81786\n",
      "epoch no.0 train no.42540  loss = 3.20330 avg_loss = 3.85356\n",
      "epoch no.0 train no.42550  loss = 3.24859 avg_loss = 3.85427\n",
      "epoch no.0 train no.42560  loss = 2.81623 avg_loss = 3.81901\n",
      "epoch no.0 train no.42570  loss = 5.72142 avg_loss = 3.86979\n",
      "epoch no.0 train no.42580  loss = 4.86759 avg_loss = 3.91148\n",
      "epoch no.0 train no.42590  loss = 2.25566 avg_loss = 3.83981\n",
      "epoch no.0 train no.42600  loss = 4.69211 avg_loss = 3.80243\n",
      "epoch no.0 train no.42610  loss = 2.48969 avg_loss = 3.73767\n",
      "epoch no.0 train no.42620  loss = 4.02949 avg_loss = 3.72775\n",
      "epoch no.0 train no.42630  loss = 3.38658 avg_loss = 3.70531\n",
      "epoch no.0 train no.42640  loss = 4.47629 avg_loss = 3.79033\n",
      "epoch no.0 train no.42650  loss = 2.60161 avg_loss = 3.78955\n",
      "epoch no.0 train no.42660  loss = 3.77977 avg_loss = 3.76731\n",
      "epoch no.0 train no.42670  loss = 3.74699 avg_loss = 3.76781\n",
      "epoch no.0 train no.42680  loss = 3.20241 avg_loss = 3.79904\n",
      "epoch no.0 train no.42690  loss = 4.57074 avg_loss = 3.79300\n",
      "epoch no.0 train no.42700  loss = 2.48646 avg_loss = 3.81328\n",
      "epoch no.0 train no.42710  loss = 5.42461 avg_loss = 3.83554\n",
      "epoch no.0 train no.42720  loss = 5.82150 avg_loss = 3.87360\n",
      "epoch no.0 train no.42730  loss = 6.32047 avg_loss = 3.86972\n",
      "epoch no.0 train no.42740  loss = 3.98376 avg_loss = 3.85607\n",
      "epoch no.0 train no.42750  loss = 3.29929 avg_loss = 3.81180\n",
      "epoch no.0 train no.42760  loss = 2.76049 avg_loss = 3.77438\n",
      "epoch no.0 train no.42770  loss = 3.22164 avg_loss = 3.76600\n",
      "epoch no.0 train no.42780  loss = 3.74894 avg_loss = 3.83037\n",
      "epoch no.0 train no.42790  loss = 5.21834 avg_loss = 3.87484\n",
      "epoch no.0 train no.42800  loss = 4.04911 avg_loss = 3.86812\n",
      "epoch no.0 train no.42810  loss = 2.52924 avg_loss = 3.84453\n",
      "epoch no.0 train no.42820  loss = 3.58347 avg_loss = 3.82645\n",
      "epoch no.0 train no.42830  loss = 1.85311 avg_loss = 3.83431\n",
      "epoch no.0 train no.42840  loss = 3.31438 avg_loss = 3.79593\n",
      "epoch no.0 train no.42850  loss = 2.67081 avg_loss = 3.81124\n",
      "epoch no.0 train no.42860  loss = 6.21293 avg_loss = 3.84364\n",
      "epoch no.0 train no.42870  loss = 5.16972 avg_loss = 3.90075\n",
      "epoch no.0 train no.42880  loss = 6.17238 avg_loss = 3.87197\n",
      "epoch no.0 train no.42890  loss = 2.61924 avg_loss = 3.82733\n",
      "epoch no.0 train no.42900  loss = 1.89601 avg_loss = 3.84334\n",
      "epoch no.0 train no.42910  loss = 2.16034 avg_loss = 3.81074\n",
      "epoch no.0 train no.42920  loss = 6.83230 avg_loss = 3.87773\n",
      "epoch no.0 train no.42930  loss = 5.90089 avg_loss = 3.83936\n",
      "epoch no.0 train no.42940  loss = 2.36653 avg_loss = 3.83505\n",
      "epoch no.0 train no.42950  loss = 4.34881 avg_loss = 3.85800\n",
      "epoch no.0 train no.42960  loss = 4.54264 avg_loss = 3.89316\n",
      "epoch no.0 train no.42970  loss = 2.24305 avg_loss = 3.95137\n",
      "epoch no.0 train no.42980  loss = 2.65151 avg_loss = 3.93403\n",
      "epoch no.0 train no.42990  loss = 4.44853 avg_loss = 3.97049\n",
      "epoch no.0 train no.43000  loss = 4.98024 avg_loss = 3.96462\n",
      "2\n",
      "to_tokens: ['▁비', '▁노래', '들', '</s>']\n",
      "추억의 노래들</s>\n",
      "epoch no.0 train no.43010  loss = 3.59814 avg_loss = 3.94589\n",
      "epoch no.0 train no.43020  loss = 6.83863 avg_loss = 4.01119\n",
      "epoch no.0 train no.43030  loss = 5.74783 avg_loss = 4.04655\n",
      "epoch no.0 train no.43040  loss = 4.02303 avg_loss = 4.05436\n",
      "epoch no.0 train no.43050  loss = 5.81905 avg_loss = 4.07108\n",
      "epoch no.0 train no.43060  loss = 4.66536 avg_loss = 4.09350\n",
      "epoch no.0 train no.43070  loss = 4.41279 avg_loss = 4.09426\n",
      "epoch no.0 train no.43080  loss = 4.69189 avg_loss = 4.10876\n",
      "epoch no.0 train no.43090  loss = 5.39311 avg_loss = 4.10253\n",
      "epoch no.0 train no.43100  loss = 3.60070 avg_loss = 4.06364\n",
      "epoch no.0 train no.43110  loss = 5.22266 avg_loss = 4.11037\n",
      "epoch no.0 train no.43120  loss = 4.91135 avg_loss = 4.09481\n",
      "epoch no.0 train no.43130  loss = 3.11570 avg_loss = 4.07999\n",
      "epoch no.0 train no.43140  loss = 3.97605 avg_loss = 4.01569\n",
      "epoch no.0 train no.43150  loss = 3.85727 avg_loss = 4.02014\n",
      "epoch no.0 train no.43160  loss = 5.84281 avg_loss = 4.00324\n",
      "epoch no.0 train no.43170  loss = 4.95994 avg_loss = 4.06761\n",
      "epoch no.0 train no.43180  loss = 3.61373 avg_loss = 4.01788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.43190  loss = 3.81679 avg_loss = 3.97047\n",
      "epoch no.0 train no.43200  loss = 5.53486 avg_loss = 4.00622\n",
      "epoch no.0 train no.43210  loss = 2.85448 avg_loss = 3.99842\n",
      "epoch no.0 train no.43220  loss = 4.48348 avg_loss = 4.02503\n",
      "epoch no.0 train no.43230  loss = 5.15359 avg_loss = 4.03873\n",
      "epoch no.0 train no.43240  loss = 2.79223 avg_loss = 3.99896\n",
      "epoch no.0 train no.43250  loss = 4.81154 avg_loss = 4.07795\n",
      "epoch no.0 train no.43260  loss = 4.49558 avg_loss = 4.12396\n",
      "epoch no.0 train no.43270  loss = 5.44146 avg_loss = 4.06694\n",
      "epoch no.0 train no.43280  loss = 4.55010 avg_loss = 4.05774\n",
      "epoch no.0 train no.43290  loss = 2.64799 avg_loss = 4.08654\n",
      "epoch no.0 train no.43300  loss = 2.42486 avg_loss = 4.07838\n",
      "epoch no.0 train no.43310  loss = 4.31091 avg_loss = 4.01344\n",
      "epoch no.0 train no.43320  loss = 3.27328 avg_loss = 3.92296\n",
      "epoch no.0 train no.43330  loss = 4.13637 avg_loss = 3.98602\n",
      "epoch no.0 train no.43340  loss = 2.98270 avg_loss = 3.99763\n",
      "epoch no.0 train no.43350  loss = 5.13878 avg_loss = 3.93686\n",
      "epoch no.0 train no.43360  loss = 4.35753 avg_loss = 3.98043\n",
      "epoch no.0 train no.43370  loss = 3.90868 avg_loss = 3.98708\n",
      "epoch no.0 train no.43380  loss = 4.11711 avg_loss = 3.98627\n",
      "epoch no.0 train no.43390  loss = 4.19657 avg_loss = 3.93437\n",
      "epoch no.0 train no.43400  loss = 3.05546 avg_loss = 3.90402\n",
      "epoch no.0 train no.43410  loss = 2.78556 avg_loss = 3.88055\n",
      "epoch no.0 train no.43420  loss = 2.62561 avg_loss = 3.84761\n",
      "epoch no.0 train no.43430  loss = 4.27754 avg_loss = 3.80903\n",
      "epoch no.0 train no.43440  loss = 2.55413 avg_loss = 3.81397\n",
      "epoch no.0 train no.43450  loss = 1.95776 avg_loss = 3.81669\n",
      "epoch no.0 train no.43460  loss = 3.46976 avg_loss = 3.76803\n",
      "epoch no.0 train no.43470  loss = 2.98286 avg_loss = 3.73119\n",
      "epoch no.0 train no.43480  loss = 4.64781 avg_loss = 3.74078\n",
      "epoch no.0 train no.43490  loss = 2.40156 avg_loss = 3.76103\n",
      "epoch no.0 train no.43500  loss = 5.05986 avg_loss = 3.80273\n",
      "epoch no.0 train no.43510  loss = 3.34708 avg_loss = 3.75660\n",
      "epoch no.0 train no.43520  loss = 2.99264 avg_loss = 3.76681\n",
      "epoch no.0 train no.43530  loss = 4.01603 avg_loss = 3.79390\n",
      "epoch no.0 train no.43540  loss = 3.87839 avg_loss = 3.80498\n",
      "epoch no.0 train no.43550  loss = 5.10341 avg_loss = 3.82358\n",
      "epoch no.0 train no.43560  loss = 3.18227 avg_loss = 3.92587\n",
      "epoch no.0 train no.43570  loss = 5.21553 avg_loss = 3.96367\n",
      "epoch no.0 train no.43580  loss = 2.68962 avg_loss = 3.97736\n",
      "epoch no.0 train no.43590  loss = 4.63652 avg_loss = 3.98858\n",
      "epoch no.0 train no.43600  loss = 3.76665 avg_loss = 4.01538\n",
      "epoch no.0 train no.43610  loss = 4.43180 avg_loss = 4.02338\n",
      "epoch no.0 train no.43620  loss = 4.97151 avg_loss = 3.99773\n",
      "epoch no.0 train no.43630  loss = 5.21483 avg_loss = 4.01706\n",
      "epoch no.0 train no.43640  loss = 4.65099 avg_loss = 4.01854\n",
      "epoch no.0 train no.43650  loss = 2.82226 avg_loss = 3.97101\n",
      "epoch no.0 train no.43660  loss = 6.09294 avg_loss = 3.95329\n",
      "epoch no.0 train no.43670  loss = 2.66464 avg_loss = 3.89179\n",
      "epoch no.0 train no.43680  loss = 2.57058 avg_loss = 3.86083\n",
      "epoch no.0 train no.43690  loss = 2.21486 avg_loss = 3.80189\n",
      "epoch no.0 train no.43700  loss = 4.24458 avg_loss = 3.81645\n",
      "epoch no.0 train no.43710  loss = 4.85828 avg_loss = 3.78571\n",
      "epoch no.0 train no.43720  loss = 3.64756 avg_loss = 3.75551\n",
      "epoch no.0 train no.43730  loss = 4.71841 avg_loss = 3.78725\n",
      "epoch no.0 train no.43740  loss = 4.51814 avg_loss = 3.77152\n",
      "epoch no.0 train no.43750  loss = 2.75837 avg_loss = 3.73567\n",
      "epoch no.0 train no.43760  loss = 6.19323 avg_loss = 3.78503\n",
      "epoch no.0 train no.43770  loss = 3.08042 avg_loss = 3.82707\n",
      "epoch no.0 train no.43780  loss = 4.42088 avg_loss = 3.88476\n",
      "epoch no.0 train no.43790  loss = 4.19680 avg_loss = 3.89432\n",
      "epoch no.0 train no.43800  loss = 3.74516 avg_loss = 3.86650\n",
      "epoch no.0 train no.43810  loss = 2.44089 avg_loss = 3.87392\n",
      "epoch no.0 train no.43820  loss = 3.85016 avg_loss = 3.83438\n",
      "epoch no.0 train no.43830  loss = 3.00576 avg_loss = 3.80803\n",
      "epoch no.0 train no.43840  loss = 2.73838 avg_loss = 3.77498\n",
      "epoch no.0 train no.43850  loss = 3.40417 avg_loss = 3.73312\n",
      "epoch no.0 train no.43860  loss = 4.18646 avg_loss = 3.70403\n",
      "epoch no.0 train no.43870  loss = 3.45161 avg_loss = 3.72430\n",
      "epoch no.0 train no.43880  loss = 4.37595 avg_loss = 3.76845\n",
      "epoch no.0 train no.43890  loss = 5.09793 avg_loss = 3.83518\n",
      "epoch no.0 train no.43900  loss = 4.84403 avg_loss = 3.88862\n",
      "epoch no.0 train no.43910  loss = 3.69180 avg_loss = 3.88848\n",
      "epoch no.0 train no.43920  loss = 3.92097 avg_loss = 3.90965\n",
      "epoch no.0 train no.43930  loss = 3.73442 avg_loss = 3.88013\n",
      "epoch no.0 train no.43940  loss = 2.49435 avg_loss = 3.84710\n",
      "epoch no.0 train no.43950  loss = 2.13040 avg_loss = 3.85611\n",
      "epoch no.0 train no.43960  loss = 2.85371 avg_loss = 3.87975\n",
      "epoch no.0 train no.43970  loss = 2.35647 avg_loss = 3.86734\n",
      "epoch no.0 train no.43980  loss = 3.66117 avg_loss = 3.89846\n",
      "epoch no.0 train no.43990  loss = 1.91543 avg_loss = 3.89172\n",
      "epoch no.0 train no.44000  loss = 3.45572 avg_loss = 3.86676\n",
      "4\n",
      "to_tokens: ['▁비', '▁노래', '년대', '▁발라', '곡', '</s>']\n",
      "추억의 2000년대 댄스곡</s>\n",
      "epoch no.0 train no.44010  loss = 3.41009 avg_loss = 3.90626\n",
      "epoch no.0 train no.44020  loss = 3.50148 avg_loss = 3.91604\n",
      "epoch no.0 train no.44030  loss = 5.59069 avg_loss = 3.92547\n",
      "epoch no.0 train no.44040  loss = 3.93965 avg_loss = 4.02757\n",
      "epoch no.0 train no.44050  loss = 4.18197 avg_loss = 4.02929\n",
      "epoch no.0 train no.44060  loss = 5.13050 avg_loss = 3.98025\n",
      "epoch no.0 train no.44070  loss = 5.85153 avg_loss = 3.99188\n",
      "epoch no.0 train no.44080  loss = 3.90216 avg_loss = 3.96500\n",
      "epoch no.0 train no.44090  loss = 5.32586 avg_loss = 3.92940\n",
      "epoch no.0 train no.44100  loss = 6.87493 avg_loss = 3.97035\n",
      "epoch no.0 train no.44110  loss = 2.64126 avg_loss = 3.92923\n",
      "epoch no.0 train no.44120  loss = 4.20588 avg_loss = 3.88470\n",
      "epoch no.0 train no.44130  loss = 4.66322 avg_loss = 3.89599\n",
      "epoch no.0 train no.44140  loss = 5.66971 avg_loss = 3.90765\n",
      "epoch no.0 train no.44150  loss = 2.75385 avg_loss = 3.89521\n",
      "epoch no.0 train no.44160  loss = 4.88407 avg_loss = 3.88926\n",
      "epoch no.0 train no.44170  loss = 5.34928 avg_loss = 3.94534\n",
      "epoch no.0 train no.44180  loss = 4.30655 avg_loss = 3.99798\n",
      "epoch no.0 train no.44190  loss = 2.49182 avg_loss = 4.07564\n",
      "epoch no.0 train no.44200  loss = 5.13924 avg_loss = 4.06226\n",
      "epoch no.0 train no.44210  loss = 4.68525 avg_loss = 4.02260\n",
      "epoch no.0 train no.44220  loss = 4.76294 avg_loss = 4.02155\n",
      "epoch no.0 train no.44230  loss = 1.92867 avg_loss = 4.00702\n",
      "epoch no.0 train no.44240  loss = 2.97751 avg_loss = 4.04967\n",
      "epoch no.0 train no.44250  loss = 2.98050 avg_loss = 4.02420\n",
      "epoch no.0 train no.44260  loss = 4.51898 avg_loss = 3.96367\n",
      "epoch no.0 train no.44270  loss = 2.59331 avg_loss = 3.99825\n",
      "epoch no.0 train no.44280  loss = 5.54150 avg_loss = 3.97820\n",
      "epoch no.0 train no.44290  loss = 3.29457 avg_loss = 3.94279\n",
      "epoch no.0 train no.44300  loss = 4.42087 avg_loss = 3.95767\n",
      "epoch no.0 train no.44310  loss = 5.02950 avg_loss = 3.91165\n",
      "epoch no.0 train no.44320  loss = 3.64676 avg_loss = 3.90114\n",
      "epoch no.0 train no.44330  loss = 3.13933 avg_loss = 3.82218\n",
      "epoch no.0 train no.44340  loss = 4.76880 avg_loss = 3.87529\n",
      "epoch no.0 train no.44350  loss = 4.39943 avg_loss = 3.88525\n",
      "epoch no.0 train no.44360  loss = 2.77470 avg_loss = 3.84276\n",
      "epoch no.0 train no.44370  loss = 6.90066 avg_loss = 3.84774\n",
      "epoch no.0 train no.44380  loss = 3.43696 avg_loss = 3.83582\n",
      "epoch no.0 train no.44390  loss = 4.24106 avg_loss = 3.79092\n",
      "epoch no.0 train no.44400  loss = 4.54749 avg_loss = 3.74912\n",
      "epoch no.0 train no.44410  loss = 2.39450 avg_loss = 3.71561\n",
      "epoch no.0 train no.44420  loss = 6.80789 avg_loss = 3.79742\n",
      "epoch no.0 train no.44430  loss = 3.62656 avg_loss = 3.75253\n",
      "epoch no.0 train no.44440  loss = 4.86816 avg_loss = 3.75334\n",
      "epoch no.0 train no.44450  loss = 1.83517 avg_loss = 3.74873\n",
      "epoch no.0 train no.44460  loss = 2.38028 avg_loss = 3.74202\n",
      "epoch no.0 train no.44470  loss = 5.37288 avg_loss = 3.72168\n",
      "epoch no.0 train no.44480  loss = 1.90585 avg_loss = 3.69481\n",
      "epoch no.0 train no.44490  loss = 4.76215 avg_loss = 3.76842\n",
      "epoch no.0 train no.44500  loss = 4.28167 avg_loss = 3.79526\n",
      "epoch no.0 train no.44510  loss = 2.72606 avg_loss = 3.86983\n",
      "epoch no.0 train no.44520  loss = 2.64583 avg_loss = 3.89771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.44530  loss = 3.24637 avg_loss = 3.90563\n",
      "epoch no.0 train no.44540  loss = 4.99803 avg_loss = 3.83349\n",
      "epoch no.0 train no.44550  loss = 3.75497 avg_loss = 3.84565\n",
      "epoch no.0 train no.44560  loss = 2.60442 avg_loss = 3.84537\n",
      "epoch no.0 train no.44570  loss = 1.68191 avg_loss = 3.89267\n",
      "epoch no.0 train no.44580  loss = 3.36540 avg_loss = 3.85205\n",
      "epoch no.0 train no.44590  loss = 4.01146 avg_loss = 3.82498\n",
      "epoch no.0 train no.44600  loss = 4.39419 avg_loss = 3.86679\n",
      "epoch no.0 train no.44610  loss = 5.38741 avg_loss = 3.87608\n",
      "epoch no.0 train no.44620  loss = 3.17406 avg_loss = 3.83416\n",
      "epoch no.0 train no.44630  loss = 4.49312 avg_loss = 3.87152\n",
      "epoch no.0 train no.44640  loss = 3.55974 avg_loss = 3.82187\n",
      "epoch no.0 train no.44650  loss = 3.83409 avg_loss = 3.84653\n",
      "epoch no.0 train no.44660  loss = 1.88141 avg_loss = 3.81767\n",
      "epoch no.0 train no.44670  loss = 3.21034 avg_loss = 3.82016\n",
      "epoch no.0 train no.44680  loss = 5.96893 avg_loss = 3.85557\n",
      "epoch no.0 train no.44690  loss = 4.15070 avg_loss = 3.91025\n",
      "epoch no.0 train no.44700  loss = 4.51520 avg_loss = 3.97867\n",
      "epoch no.0 train no.44710  loss = 5.40998 avg_loss = 3.97234\n",
      "epoch no.0 train no.44720  loss = 4.25872 avg_loss = 4.03458\n",
      "epoch no.0 train no.44730  loss = 3.32428 avg_loss = 4.01156\n",
      "epoch no.0 train no.44740  loss = 4.18586 avg_loss = 3.96697\n",
      "epoch no.0 train no.44750  loss = 4.94534 avg_loss = 4.00404\n",
      "epoch no.0 train no.44760  loss = 5.84273 avg_loss = 4.04532\n",
      "epoch no.0 train no.44770  loss = 4.89762 avg_loss = 4.02700\n",
      "epoch no.0 train no.44780  loss = 4.01690 avg_loss = 4.05772\n",
      "epoch no.0 train no.44790  loss = 4.36567 avg_loss = 4.02655\n",
      "epoch no.0 train no.44800  loss = 3.53934 avg_loss = 3.94826\n",
      "epoch no.0 train no.44810  loss = 6.48904 avg_loss = 3.94080\n",
      "epoch no.0 train no.44820  loss = 4.90703 avg_loss = 3.94183\n",
      "epoch no.0 train no.44830  loss = 2.34160 avg_loss = 3.93990\n",
      "epoch no.0 train no.44840  loss = 4.42780 avg_loss = 3.91976\n",
      "epoch no.0 train no.44850  loss = 4.93388 avg_loss = 3.93565\n",
      "epoch no.0 train no.44860  loss = 3.91522 avg_loss = 3.96633\n",
      "epoch no.0 train no.44870  loss = 4.92311 avg_loss = 3.96020\n",
      "epoch no.0 train no.44880  loss = 6.44268 avg_loss = 3.94208\n",
      "epoch no.0 train no.44890  loss = 4.03503 avg_loss = 3.97612\n",
      "epoch no.0 train no.44900  loss = 2.23080 avg_loss = 3.98353\n",
      "epoch no.0 train no.44910  loss = 3.88064 avg_loss = 3.96333\n",
      "epoch no.0 train no.44920  loss = 4.27666 avg_loss = 3.95188\n",
      "epoch no.0 train no.44930  loss = 3.62027 avg_loss = 4.04184\n",
      "epoch no.0 train no.44940  loss = 3.31909 avg_loss = 4.00093\n",
      "epoch no.0 train no.44950  loss = 2.55281 avg_loss = 4.00131\n",
      "epoch no.0 train no.44960  loss = 3.38419 avg_loss = 4.00890\n",
      "epoch no.0 train no.44970  loss = 3.12653 avg_loss = 3.99371\n",
      "epoch no.0 train no.44980  loss = 1.54743 avg_loss = 3.93067\n",
      "epoch no.0 train no.44990  loss = 3.77792 avg_loss = 3.83252\n",
      "epoch no.0 train no.45000  loss = 3.27861 avg_loss = 3.87161\n",
      "3\n",
      "to_tokens: ['▁비', '▁노래', '팝', '</s>', '</s>']\n",
      "추억의 올드팝송</s>\n",
      "epoch no.0 train no.45010  loss = 3.51893 avg_loss = 3.93198\n",
      "epoch no.0 train no.45020  loss = 2.66582 avg_loss = 3.91005\n",
      "epoch no.0 train no.45030  loss = 3.99820 avg_loss = 3.89387\n",
      "epoch no.0 train no.45040  loss = 3.77305 avg_loss = 3.87195\n",
      "epoch no.0 train no.45050  loss = 4.52305 avg_loss = 3.88373\n",
      "epoch no.0 train no.45060  loss = 5.43829 avg_loss = 3.95930\n",
      "epoch no.0 train no.45070  loss = 3.43543 avg_loss = 3.94461\n",
      "epoch no.0 train no.45080  loss = 3.01464 avg_loss = 4.00387\n",
      "epoch no.0 train no.45090  loss = 2.56215 avg_loss = 3.99213\n",
      "epoch no.0 train no.45100  loss = 3.99586 avg_loss = 3.95670\n",
      "epoch no.0 train no.45110  loss = 5.61294 avg_loss = 3.97500\n",
      "epoch no.0 train no.45120  loss = 3.47428 avg_loss = 3.93554\n",
      "epoch no.0 train no.45130  loss = 5.22535 avg_loss = 3.87251\n",
      "epoch no.0 train no.45140  loss = 1.81335 avg_loss = 3.86244\n",
      "epoch no.0 train no.45150  loss = 2.47981 avg_loss = 3.88030\n",
      "epoch no.0 train no.45160  loss = 2.64414 avg_loss = 3.84300\n",
      "epoch no.0 train no.45170  loss = 4.10467 avg_loss = 3.89393\n",
      "epoch no.0 train no.45180  loss = 4.01034 avg_loss = 3.92571\n",
      "epoch no.0 train no.45190  loss = 8.20097 avg_loss = 3.96183\n",
      "epoch no.0 train no.45200  loss = 3.39413 avg_loss = 3.98506\n",
      "epoch no.0 train no.45210  loss = 3.25377 avg_loss = 3.94435\n",
      "epoch no.0 train no.45220  loss = 6.92846 avg_loss = 3.91836\n",
      "epoch no.0 train no.45230  loss = 4.59738 avg_loss = 3.92502\n",
      "epoch no.0 train no.45240  loss = 4.10688 avg_loss = 3.97196\n",
      "epoch no.0 train no.45250  loss = 6.12508 avg_loss = 3.99161\n",
      "epoch no.0 train no.45260  loss = 4.11265 avg_loss = 4.01098\n",
      "epoch no.0 train no.45270  loss = 4.37767 avg_loss = 4.00092\n",
      "epoch no.0 train no.45280  loss = 4.82059 avg_loss = 4.03542\n",
      "epoch no.0 train no.45290  loss = 3.09941 avg_loss = 3.99269\n",
      "epoch no.0 train no.45300  loss = 4.97214 avg_loss = 3.99581\n",
      "epoch no.0 train no.45310  loss = 5.00366 avg_loss = 3.99492\n",
      "epoch no.0 train no.45320  loss = 3.85183 avg_loss = 4.03629\n",
      "epoch no.0 train no.45330  loss = 2.65616 avg_loss = 4.00475\n",
      "epoch no.0 train no.45340  loss = 2.11833 avg_loss = 3.93842\n",
      "epoch no.0 train no.45350  loss = 4.45074 avg_loss = 3.95854\n",
      "epoch no.0 train no.45360  loss = 6.18911 avg_loss = 3.98260\n",
      "epoch no.0 train no.45370  loss = 4.57376 avg_loss = 3.93225\n",
      "epoch no.0 train no.45380  loss = 3.54161 avg_loss = 3.95682\n",
      "epoch no.0 train no.45390  loss = 4.01112 avg_loss = 3.97669\n",
      "epoch no.0 train no.45400  loss = 3.72531 avg_loss = 3.98237\n",
      "epoch no.0 train no.45410  loss = 5.21698 avg_loss = 3.93216\n",
      "epoch no.0 train no.45420  loss = 2.64063 avg_loss = 3.90642\n",
      "epoch no.0 train no.45430  loss = 2.50396 avg_loss = 3.82083\n",
      "epoch no.0 train no.45440  loss = 2.66518 avg_loss = 3.80309\n",
      "epoch no.0 train no.45450  loss = 4.75871 avg_loss = 3.84572\n",
      "epoch no.0 train no.45460  loss = 2.54826 avg_loss = 3.78188\n",
      "epoch no.0 train no.45470  loss = 3.89229 avg_loss = 3.78368\n",
      "epoch no.0 train no.45480  loss = 3.04299 avg_loss = 3.73355\n",
      "epoch no.0 train no.45490  loss = 6.00875 avg_loss = 3.74355\n",
      "epoch no.0 train no.45500  loss = 3.83406 avg_loss = 3.70746\n",
      "epoch no.0 train no.45510  loss = 3.02253 avg_loss = 3.71350\n",
      "epoch no.0 train no.45520  loss = 6.27667 avg_loss = 3.76138\n",
      "epoch no.0 train no.45530  loss = 3.91607 avg_loss = 3.74559\n",
      "epoch no.0 train no.45540  loss = 3.65033 avg_loss = 3.68562\n",
      "epoch no.0 train no.45550  loss = 3.10370 avg_loss = 3.76626\n",
      "epoch no.0 train no.45560  loss = 2.47073 avg_loss = 3.73823\n",
      "epoch no.0 train no.45570  loss = 6.64694 avg_loss = 3.73981\n",
      "epoch no.0 train no.45580  loss = 2.26935 avg_loss = 3.73415\n",
      "epoch no.0 train no.45590  loss = 4.12759 avg_loss = 3.77178\n",
      "epoch no.0 train no.45600  loss = 4.30158 avg_loss = 3.78623\n",
      "epoch no.0 train no.45610  loss = 5.90779 avg_loss = 3.83516\n",
      "epoch no.0 train no.45620  loss = 2.27305 avg_loss = 3.81318\n",
      "epoch no.0 train no.45630  loss = 3.67891 avg_loss = 3.78814\n",
      "epoch no.0 train no.45640  loss = 2.23586 avg_loss = 3.78812\n",
      "epoch no.0 train no.45650  loss = 3.74786 avg_loss = 3.83571\n",
      "epoch no.0 train no.45660  loss = 3.52844 avg_loss = 3.84329\n",
      "epoch no.0 train no.45670  loss = 2.02466 avg_loss = 3.80815\n",
      "epoch no.0 train no.45680  loss = 4.96800 avg_loss = 3.85181\n",
      "epoch no.0 train no.45690  loss = 4.56584 avg_loss = 3.79526\n",
      "epoch no.0 train no.45700  loss = 4.10587 avg_loss = 3.79265\n",
      "epoch no.0 train no.45710  loss = 4.38117 avg_loss = 3.82255\n",
      "epoch no.0 train no.45720  loss = 5.15571 avg_loss = 3.79579\n",
      "epoch no.0 train no.45730  loss = 4.89811 avg_loss = 3.86173\n",
      "epoch no.0 train no.45740  loss = 2.89255 avg_loss = 3.87734\n",
      "epoch no.0 train no.45750  loss = 3.96882 avg_loss = 3.87510\n",
      "epoch no.0 train no.45760  loss = 3.80636 avg_loss = 3.91028\n",
      "epoch no.0 train no.45770  loss = 5.65605 avg_loss = 3.92174\n",
      "epoch no.0 train no.45780  loss = 3.45102 avg_loss = 3.96011\n",
      "epoch no.0 train no.45790  loss = 5.23438 avg_loss = 4.00400\n",
      "epoch no.0 train no.45800  loss = 3.33078 avg_loss = 3.96458\n",
      "epoch no.0 train no.45810  loss = 3.37693 avg_loss = 4.04009\n",
      "epoch no.0 train no.45820  loss = 5.53538 avg_loss = 4.07147\n",
      "epoch no.0 train no.45830  loss = 5.39977 avg_loss = 4.05462\n",
      "epoch no.0 train no.45840  loss = 5.10982 avg_loss = 4.07087\n",
      "epoch no.0 train no.45850  loss = 5.64895 avg_loss = 4.04113\n",
      "epoch no.0 train no.45860  loss = 3.47453 avg_loss = 4.02357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.45870  loss = 3.14425 avg_loss = 3.96987\n",
      "epoch no.0 train no.45880  loss = 3.23163 avg_loss = 3.98863\n",
      "epoch no.0 train no.45890  loss = 3.96641 avg_loss = 3.96252\n",
      "epoch no.0 train no.45900  loss = 3.35480 avg_loss = 3.92808\n",
      "epoch no.0 train no.45910  loss = 4.25174 avg_loss = 3.94671\n",
      "epoch no.0 train no.45920  loss = 3.39362 avg_loss = 3.93320\n",
      "epoch no.0 train no.45930  loss = 4.99231 avg_loss = 3.96121\n",
      "epoch no.0 train no.45940  loss = 5.03509 avg_loss = 3.93670\n",
      "epoch no.0 train no.45950  loss = 3.64601 avg_loss = 3.88975\n",
      "epoch no.0 train no.45960  loss = 3.95153 avg_loss = 3.85114\n",
      "epoch no.0 train no.45970  loss = 1.49022 avg_loss = 3.83805\n",
      "epoch no.0 train no.45980  loss = 3.95537 avg_loss = 3.84333\n",
      "epoch no.0 train no.45990  loss = 4.93386 avg_loss = 3.90394\n",
      "epoch no.0 train no.46000  loss = 2.83541 avg_loss = 3.84623\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '들', '</s>', '속으로', '</s>']\n",
      "추억의 노래들 추억속으로</s>\n",
      "epoch no.0 train no.46010  loss = 2.97238 avg_loss = 3.85504\n",
      "epoch no.0 train no.46020  loss = 4.77347 avg_loss = 3.81280\n",
      "epoch no.0 train no.46030  loss = 2.56297 avg_loss = 3.82960\n",
      "epoch no.0 train no.46040  loss = 3.72404 avg_loss = 3.87119\n",
      "epoch no.0 train no.46050  loss = 4.90805 avg_loss = 3.97360\n",
      "epoch no.0 train no.46060  loss = 4.41983 avg_loss = 3.97772\n",
      "epoch no.0 train no.46070  loss = 3.21760 avg_loss = 3.98791\n",
      "epoch no.0 train no.46080  loss = 2.94457 avg_loss = 3.97459\n",
      "epoch no.0 train no.46090  loss = 3.60824 avg_loss = 4.01682\n",
      "epoch no.0 train no.46100  loss = 2.48859 avg_loss = 3.97206\n",
      "epoch no.0 train no.46110  loss = 5.06184 avg_loss = 3.91433\n",
      "epoch no.0 train no.46120  loss = 1.96403 avg_loss = 3.83461\n",
      "epoch no.0 train no.46130  loss = 3.44326 avg_loss = 3.79124\n",
      "epoch no.0 train no.46140  loss = 1.35839 avg_loss = 3.77672\n",
      "epoch no.0 train no.46150  loss = 3.15528 avg_loss = 3.75330\n",
      "epoch no.0 train no.46160  loss = 4.84349 avg_loss = 3.78042\n",
      "epoch no.0 train no.46170  loss = 3.39905 avg_loss = 3.78335\n",
      "epoch no.0 train no.46180  loss = 4.05132 avg_loss = 3.80892\n",
      "epoch no.0 train no.46190  loss = 2.60359 avg_loss = 3.81817\n",
      "epoch no.0 train no.46200  loss = 4.55770 avg_loss = 3.87100\n",
      "epoch no.0 train no.46210  loss = 2.48577 avg_loss = 3.86104\n",
      "epoch no.0 train no.46220  loss = 3.44772 avg_loss = 3.92245\n",
      "epoch no.0 train no.46230  loss = 2.22405 avg_loss = 3.98437\n",
      "epoch no.0 train no.46240  loss = 2.37243 avg_loss = 3.99873\n",
      "epoch no.0 train no.46250  loss = 2.83739 avg_loss = 3.93055\n",
      "epoch no.0 train no.46260  loss = 3.01118 avg_loss = 3.89709\n",
      "epoch no.0 train no.46270  loss = 6.83708 avg_loss = 3.96136\n",
      "epoch no.0 train no.46280  loss = 4.28185 avg_loss = 3.89848\n",
      "epoch no.0 train no.46290  loss = 4.16770 avg_loss = 3.88121\n",
      "epoch no.0 train no.46300  loss = 4.60720 avg_loss = 3.87826\n",
      "epoch no.0 train no.46310  loss = 5.65728 avg_loss = 3.88960\n",
      "epoch no.0 train no.46320  loss = 2.54849 avg_loss = 3.90584\n",
      "epoch no.0 train no.46330  loss = 3.85447 avg_loss = 3.85197\n",
      "epoch no.0 train no.46340  loss = 3.46471 avg_loss = 3.84176\n",
      "epoch no.0 train no.46350  loss = 5.54147 avg_loss = 3.87102\n",
      "epoch no.0 train no.46360  loss = 2.13743 avg_loss = 3.85687\n",
      "epoch no.0 train no.46370  loss = 2.34484 avg_loss = 3.85215\n",
      "epoch no.0 train no.46380  loss = 4.55386 avg_loss = 3.89592\n",
      "epoch no.0 train no.46390  loss = 4.35475 avg_loss = 3.93357\n",
      "epoch no.0 train no.46400  loss = 3.46822 avg_loss = 3.91812\n",
      "epoch no.0 train no.46410  loss = 3.29980 avg_loss = 3.86090\n",
      "epoch no.0 train no.46420  loss = 2.78628 avg_loss = 3.86410\n",
      "epoch no.0 train no.46430  loss = 6.99685 avg_loss = 3.97360\n",
      "epoch no.0 train no.46440  loss = 3.54130 avg_loss = 3.97948\n",
      "epoch no.0 train no.46450  loss = 4.22756 avg_loss = 3.98636\n",
      "epoch no.0 train no.46460  loss = 3.71220 avg_loss = 3.91927\n",
      "epoch no.0 train no.46470  loss = 4.00771 avg_loss = 3.86371\n",
      "epoch no.0 train no.46480  loss = 2.15196 avg_loss = 3.81387\n",
      "epoch no.0 train no.46490  loss = 5.12221 avg_loss = 3.83927\n",
      "epoch no.0 train no.46500  loss = 4.88087 avg_loss = 3.84810\n",
      "epoch no.0 train no.46510  loss = 2.52186 avg_loss = 3.80411\n",
      "epoch no.0 train no.46520  loss = 5.70640 avg_loss = 3.82780\n",
      "epoch no.0 train no.46530  loss = 5.25459 avg_loss = 3.87844\n",
      "epoch no.0 train no.46540  loss = 2.80473 avg_loss = 3.87899\n",
      "epoch no.0 train no.46550  loss = 2.79355 avg_loss = 3.86668\n",
      "epoch no.0 train no.46560  loss = 3.99441 avg_loss = 3.81094\n",
      "epoch no.0 train no.46570  loss = 4.28419 avg_loss = 3.83829\n",
      "epoch no.0 train no.46580  loss = 3.75114 avg_loss = 3.83691\n",
      "epoch no.0 train no.46590  loss = 3.42918 avg_loss = 3.84047\n",
      "epoch no.0 train no.46600  loss = 2.63622 avg_loss = 3.80289\n",
      "epoch no.0 train no.46610  loss = 3.68667 avg_loss = 3.70424\n",
      "epoch no.0 train no.46620  loss = 5.68901 avg_loss = 3.76721\n",
      "epoch no.0 train no.46630  loss = 4.35115 avg_loss = 3.81851\n",
      "epoch no.0 train no.46640  loss = 3.05157 avg_loss = 3.81824\n",
      "epoch no.0 train no.46650  loss = 3.20042 avg_loss = 3.79915\n",
      "epoch no.0 train no.46660  loss = 5.65587 avg_loss = 3.86769\n",
      "epoch no.0 train no.46670  loss = 5.86558 avg_loss = 3.88651\n",
      "epoch no.0 train no.46680  loss = 3.74136 avg_loss = 3.90187\n",
      "epoch no.0 train no.46690  loss = 3.23444 avg_loss = 3.94855\n",
      "epoch no.0 train no.46700  loss = 2.81790 avg_loss = 3.92713\n",
      "epoch no.0 train no.46710  loss = 3.46153 avg_loss = 3.93606\n",
      "epoch no.0 train no.46720  loss = 4.02719 avg_loss = 3.93531\n",
      "epoch no.0 train no.46730  loss = 2.54029 avg_loss = 3.92017\n",
      "epoch no.0 train no.46740  loss = 3.58838 avg_loss = 3.86333\n",
      "epoch no.0 train no.46750  loss = 4.56243 avg_loss = 3.80148\n",
      "epoch no.0 train no.46760  loss = 2.37452 avg_loss = 3.76811\n",
      "epoch no.0 train no.46770  loss = 4.96706 avg_loss = 3.77234\n",
      "epoch no.0 train no.46780  loss = 2.44834 avg_loss = 3.74263\n",
      "epoch no.0 train no.46790  loss = 4.59648 avg_loss = 3.79750\n",
      "epoch no.0 train no.46800  loss = 5.18724 avg_loss = 3.84627\n",
      "epoch no.0 train no.46810  loss = 3.03053 avg_loss = 3.81684\n",
      "epoch no.0 train no.46820  loss = 4.28691 avg_loss = 3.76055\n",
      "epoch no.0 train no.46830  loss = 2.68347 avg_loss = 3.81374\n",
      "epoch no.0 train no.46840  loss = 3.36454 avg_loss = 3.83851\n",
      "epoch no.0 train no.46850  loss = 4.56500 avg_loss = 3.91778\n",
      "epoch no.0 train no.46860  loss = 2.99967 avg_loss = 3.90033\n",
      "epoch no.0 train no.46870  loss = 4.62150 avg_loss = 3.88915\n",
      "epoch no.0 train no.46880  loss = 4.21940 avg_loss = 3.88086\n",
      "epoch no.0 train no.46890  loss = 4.42071 avg_loss = 3.94520\n",
      "epoch no.0 train no.46900  loss = 3.91896 avg_loss = 3.94914\n",
      "epoch no.0 train no.46910  loss = 5.17321 avg_loss = 4.01079\n",
      "epoch no.0 train no.46920  loss = 2.02303 avg_loss = 4.01430\n",
      "epoch no.0 train no.46930  loss = 3.17598 avg_loss = 3.98731\n",
      "epoch no.0 train no.46940  loss = 2.67103 avg_loss = 3.99652\n",
      "epoch no.0 train no.46950  loss = 2.60209 avg_loss = 4.00827\n",
      "epoch no.0 train no.46960  loss = 5.23719 avg_loss = 4.04485\n",
      "epoch no.0 train no.46970  loss = 4.32197 avg_loss = 4.04810\n",
      "epoch no.0 train no.46980  loss = 3.33547 avg_loss = 4.06168\n",
      "epoch no.0 train no.46990  loss = 4.31849 avg_loss = 4.03661\n",
      "epoch no.0 train no.47000  loss = 3.68050 avg_loss = 4.04015\n",
      "4\n",
      "to_tokens: ['▁', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.0 train no.47010  loss = 4.50173 avg_loss = 4.01625\n",
      "epoch no.0 train no.47020  loss = 3.43077 avg_loss = 4.06889\n",
      "epoch no.0 train no.47030  loss = 5.25796 avg_loss = 4.08168\n",
      "epoch no.0 train no.47040  loss = 4.85322 avg_loss = 4.04129\n",
      "epoch no.0 train no.47050  loss = 3.27427 avg_loss = 4.05564\n",
      "epoch no.0 train no.47060  loss = 6.93513 avg_loss = 4.03382\n",
      "epoch no.0 train no.47070  loss = 6.13218 avg_loss = 4.03884\n",
      "epoch no.0 train no.47080  loss = 4.72461 avg_loss = 3.97064\n",
      "epoch no.0 train no.47090  loss = 3.13705 avg_loss = 3.93417\n",
      "epoch no.0 train no.47100  loss = 3.77827 avg_loss = 3.92136\n",
      "epoch no.0 train no.47110  loss = 6.96157 avg_loss = 3.93490\n",
      "epoch no.0 train no.47120  loss = 2.57023 avg_loss = 3.95931\n",
      "epoch no.0 train no.47130  loss = 4.16353 avg_loss = 3.98113\n",
      "epoch no.0 train no.47140  loss = 5.69964 avg_loss = 3.97188\n",
      "epoch no.0 train no.47150  loss = 5.63247 avg_loss = 4.04850\n",
      "epoch no.0 train no.47160  loss = 3.96909 avg_loss = 4.04784\n",
      "epoch no.0 train no.47170  loss = 3.01642 avg_loss = 3.99211\n",
      "epoch no.0 train no.47180  loss = 1.57565 avg_loss = 3.93938\n",
      "epoch no.0 train no.47190  loss = 3.11988 avg_loss = 3.96216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.47200  loss = 2.61653 avg_loss = 3.92696\n",
      "epoch no.0 train no.47210  loss = 3.27772 avg_loss = 3.91403\n",
      "epoch no.0 train no.47220  loss = 3.57272 avg_loss = 3.91732\n",
      "epoch no.0 train no.47230  loss = 3.39276 avg_loss = 3.90407\n",
      "epoch no.0 train no.47240  loss = 4.17279 avg_loss = 3.91642\n",
      "epoch no.0 train no.47250  loss = 6.18231 avg_loss = 3.93387\n",
      "epoch no.0 train no.47260  loss = 3.22729 avg_loss = 3.96217\n",
      "epoch no.0 train no.47270  loss = 2.93314 avg_loss = 3.93247\n",
      "epoch no.0 train no.47280  loss = 3.53430 avg_loss = 3.88146\n",
      "epoch no.0 train no.47290  loss = 2.13197 avg_loss = 3.92171\n",
      "epoch no.0 train no.47300  loss = 2.11501 avg_loss = 3.87755\n",
      "epoch no.0 train no.47310  loss = 3.12771 avg_loss = 3.91434\n",
      "epoch no.0 train no.47320  loss = 2.01152 avg_loss = 3.86652\n",
      "epoch no.0 train no.47330  loss = 3.12258 avg_loss = 3.83316\n",
      "epoch no.0 train no.47340  loss = 4.61721 avg_loss = 3.81207\n",
      "epoch no.0 train no.47350  loss = 2.92417 avg_loss = 3.76787\n",
      "epoch no.0 train no.47360  loss = 4.53257 avg_loss = 3.78030\n",
      "epoch no.0 train no.47370  loss = 5.70265 avg_loss = 3.74370\n",
      "epoch no.0 train no.47380  loss = 5.49476 avg_loss = 3.79215\n",
      "epoch no.0 train no.47390  loss = 4.52957 avg_loss = 3.82524\n",
      "epoch no.0 train no.47400  loss = 5.24705 avg_loss = 3.82344\n",
      "epoch no.0 train no.47410  loss = 3.19416 avg_loss = 3.79670\n",
      "epoch no.0 train no.47420  loss = 4.56780 avg_loss = 3.81697\n",
      "epoch no.0 train no.47430  loss = 4.92098 avg_loss = 3.82119\n",
      "epoch no.0 train no.47440  loss = 4.81748 avg_loss = 3.88887\n",
      "epoch no.0 train no.47450  loss = 2.96365 avg_loss = 3.91009\n",
      "epoch no.0 train no.47460  loss = 4.18276 avg_loss = 3.89636\n",
      "epoch no.0 train no.47470  loss = 3.18102 avg_loss = 3.87944\n",
      "epoch no.0 train no.47480  loss = 5.22587 avg_loss = 3.92471\n",
      "epoch no.0 train no.47490  loss = 3.66520 avg_loss = 3.89764\n",
      "epoch no.0 train no.47500  loss = 3.08543 avg_loss = 3.88384\n",
      "epoch no.0 train no.47510  loss = 3.65088 avg_loss = 3.92405\n",
      "epoch no.0 train no.47520  loss = 4.75797 avg_loss = 3.93975\n",
      "epoch no.0 train no.47530  loss = 2.92540 avg_loss = 3.95551\n",
      "epoch no.0 train no.47540  loss = 2.20406 avg_loss = 3.95684\n",
      "epoch no.0 train no.47550  loss = 3.43625 avg_loss = 3.96868\n",
      "epoch no.0 train no.47560  loss = 2.35524 avg_loss = 4.00189\n",
      "epoch no.0 train no.47570  loss = 4.05077 avg_loss = 3.95075\n",
      "epoch no.0 train no.47580  loss = 2.90951 avg_loss = 3.97137\n",
      "epoch no.0 train no.47590  loss = 3.50706 avg_loss = 3.93339\n",
      "epoch no.0 train no.47600  loss = 5.20046 avg_loss = 3.93149\n",
      "epoch no.0 train no.47610  loss = 4.46332 avg_loss = 3.97882\n",
      "epoch no.0 train no.47620  loss = 3.74399 avg_loss = 4.01927\n",
      "epoch no.0 train no.47630  loss = 4.68846 avg_loss = 4.01992\n",
      "epoch no.0 train no.47640  loss = 5.28682 avg_loss = 4.04097\n",
      "epoch no.0 train no.47650  loss = 4.65500 avg_loss = 4.02848\n",
      "epoch no.0 train no.47660  loss = 3.41432 avg_loss = 3.97996\n",
      "epoch no.0 train no.47670  loss = 2.31363 avg_loss = 3.96899\n",
      "epoch no.0 train no.47680  loss = 3.83927 avg_loss = 3.94847\n",
      "epoch no.0 train no.47690  loss = 4.67395 avg_loss = 3.92080\n",
      "epoch no.0 train no.47700  loss = 4.12402 avg_loss = 3.89371\n",
      "epoch no.0 train no.47710  loss = 3.69133 avg_loss = 3.92392\n",
      "epoch no.0 train no.47720  loss = 2.43133 avg_loss = 3.96833\n",
      "epoch no.0 train no.47730  loss = 4.10806 avg_loss = 3.95529\n",
      "epoch no.0 train no.47740  loss = 4.07454 avg_loss = 3.90879\n",
      "epoch no.0 train no.47750  loss = 6.19035 avg_loss = 3.91675\n",
      "epoch no.0 train no.47760  loss = 2.74654 avg_loss = 3.89036\n",
      "epoch no.0 train no.47770  loss = 2.59926 avg_loss = 3.95014\n",
      "epoch no.0 train no.47780  loss = 3.54512 avg_loss = 3.93053\n",
      "epoch no.0 train no.47790  loss = 3.08491 avg_loss = 3.92200\n",
      "epoch no.0 train no.47800  loss = 6.64056 avg_loss = 3.95538\n",
      "epoch no.0 train no.47810  loss = 4.77847 avg_loss = 3.99122\n",
      "epoch no.0 train no.47820  loss = 4.09674 avg_loss = 3.92520\n",
      "epoch no.0 train no.47830  loss = 3.76660 avg_loss = 3.90034\n",
      "epoch no.0 train no.47840  loss = 3.22119 avg_loss = 3.88476\n",
      "epoch no.0 train no.47850  loss = 4.25723 avg_loss = 3.87209\n",
      "epoch no.0 train no.47860  loss = 3.35471 avg_loss = 3.85429\n",
      "epoch no.0 train no.47870  loss = 2.52060 avg_loss = 3.89233\n",
      "epoch no.0 train no.47880  loss = 4.98348 avg_loss = 3.84505\n",
      "epoch no.0 train no.47890  loss = 2.64101 avg_loss = 3.82319\n",
      "epoch no.0 train no.47900  loss = 3.39939 avg_loss = 3.85445\n",
      "epoch no.0 train no.47910  loss = 4.65556 avg_loss = 3.84047\n",
      "epoch no.0 train no.47920  loss = 3.47355 avg_loss = 3.83786\n",
      "epoch no.0 train no.47930  loss = 3.28348 avg_loss = 3.87334\n",
      "epoch no.0 train no.47940  loss = 4.55355 avg_loss = 3.88789\n",
      "epoch no.0 train no.47950  loss = 2.72717 avg_loss = 3.88077\n",
      "epoch no.0 train no.47960  loss = 4.53432 avg_loss = 3.84349\n",
      "epoch no.0 train no.47970  loss = 3.58783 avg_loss = 3.84002\n",
      "epoch no.0 train no.47980  loss = 3.02766 avg_loss = 3.86554\n",
      "epoch no.0 train no.47990  loss = 3.85523 avg_loss = 3.85519\n",
      "epoch no.0 train no.48000  loss = 2.59244 avg_loss = 3.82249\n",
      "3\n",
      "to_tokens: ['▁가을', '▁노래', '▁o', 'st', '</s>']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.0 train no.48010  loss = 3.44218 avg_loss = 3.84120\n",
      "epoch no.0 train no.48020  loss = 3.31398 avg_loss = 3.86343\n",
      "epoch no.0 train no.48030  loss = 3.11406 avg_loss = 3.88432\n",
      "epoch no.0 train no.48040  loss = 4.42702 avg_loss = 3.84445\n",
      "epoch no.0 train no.48050  loss = 3.15215 avg_loss = 3.84646\n",
      "epoch no.0 train no.48060  loss = 4.62990 avg_loss = 3.81815\n",
      "epoch no.0 train no.48070  loss = 2.90498 avg_loss = 3.83250\n",
      "epoch no.0 train no.48080  loss = 4.02922 avg_loss = 3.89504\n",
      "epoch no.0 train no.48090  loss = 4.43557 avg_loss = 3.93391\n",
      "epoch no.0 train no.48100  loss = 4.80756 avg_loss = 3.98983\n",
      "epoch no.0 train no.48110  loss = 5.38182 avg_loss = 4.01786\n",
      "epoch no.0 train no.48120  loss = 4.33076 avg_loss = 3.99402\n",
      "epoch no.0 train no.48130  loss = 5.32820 avg_loss = 4.02873\n",
      "epoch no.0 train no.48140  loss = 3.13311 avg_loss = 4.02422\n",
      "epoch no.0 train no.48150  loss = 2.65127 avg_loss = 3.99472\n",
      "epoch no.0 train no.48160  loss = 4.49952 avg_loss = 3.96541\n",
      "epoch no.0 train no.48170  loss = 2.71754 avg_loss = 3.94496\n",
      "epoch no.0 train no.48180  loss = 2.87268 avg_loss = 3.91122\n",
      "epoch no.0 train no.48190  loss = 2.46377 avg_loss = 3.82549\n",
      "epoch no.0 train no.48200  loss = 3.41231 avg_loss = 3.85135\n",
      "epoch no.0 train no.48210  loss = 2.51199 avg_loss = 3.90343\n",
      "epoch no.0 train no.48220  loss = 4.99838 avg_loss = 3.91373\n",
      "epoch no.0 train no.48230  loss = 3.31031 avg_loss = 3.89488\n",
      "epoch no.0 train no.48240  loss = 4.87540 avg_loss = 3.88388\n",
      "epoch no.0 train no.48250  loss = 2.93686 avg_loss = 3.88846\n",
      "epoch no.0 train no.48260  loss = 7.45320 avg_loss = 3.91022\n",
      "epoch no.0 train no.48270  loss = 3.92061 avg_loss = 3.85625\n",
      "epoch no.0 train no.48280  loss = 2.86152 avg_loss = 3.86828\n",
      "epoch no.0 train no.48290  loss = 3.68950 avg_loss = 3.84824\n",
      "epoch no.0 train no.48300  loss = 1.87976 avg_loss = 3.82677\n",
      "epoch no.0 train no.48310  loss = 2.21167 avg_loss = 3.80942\n",
      "epoch no.0 train no.48320  loss = 6.70747 avg_loss = 3.76963\n",
      "epoch no.0 train no.48330  loss = 2.92308 avg_loss = 3.81302\n",
      "epoch no.0 train no.48340  loss = 2.79256 avg_loss = 3.86069\n",
      "epoch no.0 train no.48350  loss = 5.58771 avg_loss = 3.81325\n",
      "epoch no.0 train no.48360  loss = 4.97380 avg_loss = 3.85932\n",
      "epoch no.0 train no.48370  loss = 6.56210 avg_loss = 3.86007\n",
      "epoch no.0 train no.48380  loss = 4.06468 avg_loss = 3.87750\n",
      "epoch no.0 train no.48390  loss = 3.37623 avg_loss = 3.91132\n",
      "epoch no.0 train no.48400  loss = 2.78979 avg_loss = 3.88836\n",
      "epoch no.0 train no.48410  loss = 3.34823 avg_loss = 3.86417\n",
      "epoch no.0 train no.48420  loss = 3.37942 avg_loss = 3.82075\n",
      "epoch no.0 train no.48430  loss = 3.20030 avg_loss = 3.82886\n",
      "epoch no.0 train no.48440  loss = 4.33614 avg_loss = 3.84543\n",
      "epoch no.0 train no.48450  loss = 3.65922 avg_loss = 3.79705\n",
      "epoch no.0 train no.48460  loss = 3.32538 avg_loss = 3.85714\n",
      "epoch no.0 train no.48470  loss = 4.99944 avg_loss = 3.89970\n",
      "epoch no.0 train no.48480  loss = 2.95656 avg_loss = 3.84793\n",
      "epoch no.0 train no.48490  loss = 4.58833 avg_loss = 3.90164\n",
      "epoch no.0 train no.48500  loss = 2.24710 avg_loss = 3.93734\n",
      "epoch no.0 train no.48510  loss = 2.97433 avg_loss = 3.91367\n",
      "epoch no.0 train no.48520  loss = 3.85316 avg_loss = 3.90136\n",
      "epoch no.0 train no.48530  loss = 4.50741 avg_loss = 3.89965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.48540  loss = 4.49676 avg_loss = 3.89635\n",
      "epoch no.0 train no.48550  loss = 4.46627 avg_loss = 3.90677\n",
      "epoch no.0 train no.48560  loss = 2.46663 avg_loss = 3.87898\n",
      "epoch no.0 train no.48570  loss = 4.49497 avg_loss = 3.91483\n",
      "epoch no.0 train no.48580  loss = 6.23196 avg_loss = 3.93856\n",
      "epoch no.0 train no.48590  loss = 6.12407 avg_loss = 3.96446\n",
      "epoch no.0 train no.48600  loss = 3.76985 avg_loss = 3.94231\n",
      "epoch no.0 train no.48610  loss = 2.72570 avg_loss = 3.91264\n",
      "epoch no.0 train no.48620  loss = 1.91341 avg_loss = 3.93232\n",
      "epoch no.0 train no.48630  loss = 2.57223 avg_loss = 3.88809\n",
      "epoch no.0 train no.48640  loss = 3.91443 avg_loss = 3.96160\n",
      "epoch no.0 train no.48650  loss = 2.51465 avg_loss = 4.00813\n",
      "epoch no.0 train no.48660  loss = 3.50006 avg_loss = 4.05915\n",
      "epoch no.0 train no.48670  loss = 4.50760 avg_loss = 4.06467\n",
      "epoch no.0 train no.48680  loss = 4.60417 avg_loss = 4.03963\n",
      "epoch no.0 train no.48690  loss = 3.67284 avg_loss = 4.04598\n",
      "epoch no.0 train no.48700  loss = 3.66933 avg_loss = 4.02518\n",
      "epoch no.0 train no.48710  loss = 4.76426 avg_loss = 4.03444\n",
      "epoch no.0 train no.48720  loss = 5.03755 avg_loss = 3.97705\n",
      "epoch no.0 train no.48730  loss = 3.62723 avg_loss = 4.00563\n",
      "epoch no.0 train no.48740  loss = 3.13262 avg_loss = 4.03124\n",
      "epoch no.0 train no.48750  loss = 3.20176 avg_loss = 4.10187\n",
      "epoch no.0 train no.48760  loss = 2.67569 avg_loss = 4.10309\n",
      "epoch no.0 train no.48770  loss = 3.28923 avg_loss = 4.06327\n",
      "epoch no.0 train no.48780  loss = 3.77654 avg_loss = 4.03891\n",
      "epoch no.0 train no.48790  loss = 4.08957 avg_loss = 4.07844\n",
      "epoch no.0 train no.48800  loss = 3.40999 avg_loss = 4.10188\n",
      "epoch no.0 train no.48810  loss = 3.55245 avg_loss = 4.02807\n",
      "epoch no.0 train no.48820  loss = 3.67667 avg_loss = 4.05902\n",
      "epoch no.0 train no.48830  loss = 3.64261 avg_loss = 4.04942\n",
      "epoch no.0 train no.48840  loss = 4.29880 avg_loss = 4.01960\n",
      "epoch no.0 train no.48850  loss = 6.52742 avg_loss = 4.03105\n",
      "epoch no.0 train no.48860  loss = 4.25130 avg_loss = 4.01902\n",
      "epoch no.0 train no.48870  loss = 5.37584 avg_loss = 4.01812\n",
      "epoch no.0 train no.48880  loss = 5.96778 avg_loss = 4.05902\n",
      "epoch no.0 train no.48890  loss = 3.66501 avg_loss = 3.97360\n",
      "epoch no.0 train no.48900  loss = 5.00179 avg_loss = 3.92705\n",
      "epoch no.0 train no.48910  loss = 6.10413 avg_loss = 3.99915\n",
      "epoch no.0 train no.48920  loss = 2.78655 avg_loss = 3.95328\n",
      "epoch no.0 train no.48930  loss = 3.68216 avg_loss = 3.96693\n",
      "epoch no.0 train no.48940  loss = 2.92623 avg_loss = 3.93512\n",
      "epoch no.0 train no.48950  loss = 4.58298 avg_loss = 3.95983\n",
      "epoch no.0 train no.48960  loss = 3.21240 avg_loss = 3.97494\n",
      "epoch no.0 train no.48970  loss = 5.97158 avg_loss = 4.00323\n",
      "epoch no.0 train no.48980  loss = 4.11380 avg_loss = 3.98264\n",
      "epoch no.0 train no.48990  loss = 2.66866 avg_loss = 3.94069\n",
      "epoch no.0 train no.49000  loss = 1.97888 avg_loss = 3.93210\n",
      "7\n",
      "to_tokens: ['▁가을', '▁드라마', '들', '</s>', '▁듣고', '▁싶은', '은', '때', '</s>']\n",
      "추억의 노래들 다시 듣고싶을때</s>\n",
      "epoch no.0 train no.49010  loss = 2.71324 avg_loss = 3.96391\n",
      "epoch no.0 train no.49020  loss = 3.65487 avg_loss = 3.94899\n",
      "epoch no.0 train no.49030  loss = 5.49664 avg_loss = 3.95695\n",
      "epoch no.0 train no.49040  loss = 2.81845 avg_loss = 3.94163\n",
      "epoch no.0 train no.49050  loss = 4.86305 avg_loss = 3.91281\n",
      "epoch no.0 train no.49060  loss = 2.91667 avg_loss = 3.89952\n",
      "epoch no.0 train no.49070  loss = 3.10361 avg_loss = 3.88447\n",
      "epoch no.0 train no.49080  loss = 5.43397 avg_loss = 3.87431\n",
      "epoch no.0 train no.49090  loss = 3.51085 avg_loss = 3.86065\n",
      "epoch no.0 train no.49100  loss = 3.26590 avg_loss = 3.84394\n",
      "epoch no.0 train no.49110  loss = 3.52090 avg_loss = 3.83203\n",
      "epoch no.0 train no.49120  loss = 6.16321 avg_loss = 3.90725\n",
      "epoch no.0 train no.49130  loss = 5.04308 avg_loss = 3.92853\n",
      "epoch no.0 train no.49140  loss = 2.73320 avg_loss = 3.93545\n",
      "epoch no.0 train no.49150  loss = 4.18155 avg_loss = 3.93377\n",
      "epoch no.0 train no.49160  loss = 3.97583 avg_loss = 3.96041\n",
      "epoch no.0 train no.49170  loss = 4.33990 avg_loss = 3.94266\n",
      "epoch no.0 train no.49180  loss = 3.89237 avg_loss = 3.99977\n",
      "epoch no.0 train no.49190  loss = 3.24745 avg_loss = 4.00575\n",
      "epoch no.0 train no.49200  loss = 5.72573 avg_loss = 3.96213\n",
      "epoch no.0 train no.49210  loss = 4.77290 avg_loss = 4.01867\n",
      "epoch no.0 train no.49220  loss = 6.09856 avg_loss = 4.00294\n",
      "epoch no.0 train no.49230  loss = 5.44353 avg_loss = 4.04923\n",
      "epoch no.0 train no.49240  loss = 5.71226 avg_loss = 4.05580\n",
      "epoch no.0 train no.49250  loss = 2.96954 avg_loss = 4.05647\n",
      "epoch no.0 train no.49260  loss = 6.09114 avg_loss = 4.08833\n",
      "epoch no.0 train no.49270  loss = 3.48428 avg_loss = 4.10702\n",
      "epoch no.0 train no.49280  loss = 3.60667 avg_loss = 4.06473\n",
      "epoch no.0 train no.49290  loss = 4.52734 avg_loss = 4.02283\n",
      "epoch no.0 train no.49300  loss = 4.81587 avg_loss = 4.04372\n",
      "epoch no.0 train no.49310  loss = 3.30310 avg_loss = 3.99433\n",
      "epoch no.0 train no.49320  loss = 5.05324 avg_loss = 3.98231\n",
      "epoch no.0 train no.49330  loss = 3.19947 avg_loss = 3.95959\n",
      "epoch no.0 train no.49340  loss = 2.88307 avg_loss = 3.87373\n",
      "epoch no.0 train no.49350  loss = 4.77292 avg_loss = 3.90011\n",
      "epoch no.0 train no.49360  loss = 2.52156 avg_loss = 3.87121\n",
      "epoch no.0 train no.49370  loss = 1.81634 avg_loss = 3.79232\n",
      "epoch no.0 train no.49380  loss = 6.04486 avg_loss = 3.80247\n",
      "epoch no.0 train no.49390  loss = 2.70321 avg_loss = 3.84661\n",
      "epoch no.0 train no.49400  loss = 4.23497 avg_loss = 3.79685\n",
      "epoch no.0 train no.49410  loss = 3.76940 avg_loss = 3.79566\n",
      "epoch no.0 train no.49420  loss = 4.09693 avg_loss = 3.76346\n",
      "epoch no.0 train no.49430  loss = 2.88349 avg_loss = 3.78740\n",
      "epoch no.0 train no.49440  loss = 3.95266 avg_loss = 3.77421\n",
      "epoch no.0 train no.49450  loss = 5.07279 avg_loss = 3.80391\n",
      "epoch no.0 train no.49460  loss = 5.39025 avg_loss = 3.84154\n",
      "epoch no.0 train no.49470  loss = 2.08020 avg_loss = 3.81693\n",
      "epoch no.0 train no.49480  loss = 1.78789 avg_loss = 3.75821\n",
      "epoch no.0 train no.49490  loss = 3.18686 avg_loss = 3.84551\n",
      "epoch no.0 train no.49500  loss = 5.50574 avg_loss = 3.91186\n",
      "epoch no.0 train no.49510  loss = 5.52382 avg_loss = 3.93173\n",
      "epoch no.0 train no.49520  loss = 4.88797 avg_loss = 3.90499\n",
      "epoch no.0 train no.49530  loss = 2.92599 avg_loss = 3.91611\n",
      "epoch no.0 train no.49540  loss = 5.32978 avg_loss = 3.89027\n",
      "epoch no.0 train no.49550  loss = 4.37506 avg_loss = 3.96413\n",
      "epoch no.0 train no.49560  loss = 2.97055 avg_loss = 3.99033\n",
      "epoch no.0 train no.49570  loss = 2.49794 avg_loss = 3.99339\n",
      "epoch no.0 train no.49580  loss = 2.74492 avg_loss = 3.91339\n",
      "epoch no.0 train no.49590  loss = 3.09114 avg_loss = 3.90006\n",
      "epoch no.0 train no.49600  loss = 3.42995 avg_loss = 3.94441\n",
      "epoch no.0 train no.49610  loss = 3.34651 avg_loss = 3.92629\n",
      "epoch no.0 train no.49620  loss = 4.07282 avg_loss = 3.95558\n",
      "epoch no.0 train no.49630  loss = 3.70519 avg_loss = 3.92311\n",
      "epoch no.0 train no.49640  loss = 2.89485 avg_loss = 3.91360\n",
      "epoch no.0 train no.49650  loss = 4.23115 avg_loss = 3.92068\n",
      "epoch no.0 train no.49660  loss = 4.83043 avg_loss = 3.90554\n",
      "epoch no.0 train no.49670  loss = 4.15080 avg_loss = 3.87205\n",
      "epoch no.0 train no.49680  loss = 2.54050 avg_loss = 3.86780\n",
      "epoch no.0 train no.49690  loss = 3.64988 avg_loss = 3.82517\n",
      "epoch no.0 train no.49700  loss = 4.00154 avg_loss = 3.77906\n",
      "epoch no.0 train no.49710  loss = 5.99238 avg_loss = 3.79711\n",
      "epoch no.0 train no.49720  loss = 3.03247 avg_loss = 3.82658\n",
      "epoch no.0 train no.49730  loss = 3.26580 avg_loss = 3.79844\n",
      "epoch no.0 train no.49740  loss = 2.57494 avg_loss = 3.82993\n",
      "epoch no.0 train no.49750  loss = 5.90706 avg_loss = 3.82137\n",
      "epoch no.0 train no.49760  loss = 5.24488 avg_loss = 3.83593\n",
      "epoch no.0 train no.49770  loss = 3.16912 avg_loss = 3.93133\n",
      "epoch no.0 train no.49780  loss = 2.84461 avg_loss = 3.92602\n",
      "epoch no.0 train no.49790  loss = 2.91631 avg_loss = 3.93774\n",
      "epoch no.0 train no.49800  loss = 4.24424 avg_loss = 3.95771\n",
      "epoch no.0 train no.49810  loss = 3.43337 avg_loss = 3.94244\n",
      "epoch no.0 train no.49820  loss = 5.00176 avg_loss = 3.92746\n",
      "epoch no.0 train no.49830  loss = 4.87914 avg_loss = 3.91029\n",
      "epoch no.0 train no.49840  loss = 2.81147 avg_loss = 3.90130\n",
      "epoch no.0 train no.49850  loss = 2.78880 avg_loss = 3.94901\n",
      "epoch no.0 train no.49860  loss = 7.26786 avg_loss = 3.95594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.49870  loss = 2.55806 avg_loss = 3.89089\n",
      "epoch no.0 train no.49880  loss = 4.25107 avg_loss = 3.89407\n",
      "epoch no.0 train no.49890  loss = 4.11969 avg_loss = 3.89846\n",
      "epoch no.0 train no.49900  loss = 4.38147 avg_loss = 3.92413\n",
      "epoch no.0 train no.49910  loss = 3.51994 avg_loss = 3.93288\n",
      "epoch no.0 train no.49920  loss = 5.25924 avg_loss = 4.04150\n",
      "epoch no.0 train no.49930  loss = 4.02557 avg_loss = 4.05467\n",
      "epoch no.0 train no.49940  loss = 5.60162 avg_loss = 4.02789\n",
      "epoch no.0 train no.49950  loss = 2.52047 avg_loss = 4.04665\n",
      "epoch no.0 train no.49960  loss = 5.26146 avg_loss = 4.04215\n",
      "epoch no.0 train no.49970  loss = 1.87463 avg_loss = 3.95320\n",
      "epoch no.0 train no.49980  loss = 2.41369 avg_loss = 3.91192\n",
      "epoch no.0 train no.49990  loss = 3.92294 avg_loss = 3.95151\n",
      "epoch no.0 train no.50000  loss = 3.72701 avg_loss = 3.99519\n",
      "3\n",
      "to_tokens: ['▁비', '▁드라마', '모', '음', '</s>']\n",
      "추억의노래모음</s>\n",
      "epoch no.0 train no.50010  loss = 2.86315 avg_loss = 3.96912\n",
      "epoch no.0 train no.50020  loss = 2.81666 avg_loss = 3.97478\n",
      "epoch no.0 train no.50030  loss = 2.97119 avg_loss = 3.94334\n",
      "epoch no.0 train no.50040  loss = 2.66757 avg_loss = 3.93884\n",
      "epoch no.0 train no.50050  loss = 4.40328 avg_loss = 3.96530\n",
      "epoch no.0 train no.50060  loss = 2.87626 avg_loss = 3.97374\n",
      "epoch no.0 train no.50070  loss = 3.14373 avg_loss = 3.99584\n",
      "epoch no.0 train no.50080  loss = 4.00269 avg_loss = 4.00018\n",
      "epoch no.0 train no.50090  loss = 4.19599 avg_loss = 4.06593\n",
      "epoch no.0 train no.50100  loss = 2.99704 avg_loss = 4.03039\n",
      "epoch no.0 train no.50110  loss = 3.70163 avg_loss = 4.08825\n",
      "epoch no.0 train no.50120  loss = 4.23210 avg_loss = 4.06981\n",
      "epoch no.0 train no.50130  loss = 5.55184 avg_loss = 4.06667\n",
      "epoch no.0 train no.50140  loss = 5.54238 avg_loss = 4.03312\n",
      "epoch no.0 train no.50150  loss = 3.16408 avg_loss = 3.96364\n",
      "epoch no.0 train no.50160  loss = 5.03748 avg_loss = 3.94132\n",
      "epoch no.0 train no.50170  loss = 2.49124 avg_loss = 4.02168\n",
      "epoch no.0 train no.50180  loss = 4.95839 avg_loss = 3.96986\n",
      "epoch no.0 train no.50190  loss = 3.46969 avg_loss = 3.97959\n",
      "epoch no.0 train no.50200  loss = 2.26088 avg_loss = 3.91807\n",
      "epoch no.0 train no.50210  loss = 4.62965 avg_loss = 3.91637\n",
      "epoch no.0 train no.50220  loss = 1.71259 avg_loss = 3.88020\n",
      "epoch no.0 train no.50230  loss = 3.68474 avg_loss = 3.88289\n",
      "epoch no.0 train no.50240  loss = 4.93282 avg_loss = 3.91436\n",
      "epoch no.0 train no.50250  loss = 6.35679 avg_loss = 3.97575\n",
      "epoch no.0 train no.50260  loss = 3.12162 avg_loss = 4.00756\n",
      "epoch no.0 train no.50270  loss = 2.70489 avg_loss = 3.99106\n",
      "epoch no.0 train no.50280  loss = 3.27775 avg_loss = 3.93945\n",
      "epoch no.0 train no.50290  loss = 2.44241 avg_loss = 3.93067\n",
      "epoch no.0 train no.50300  loss = 4.02912 avg_loss = 3.89300\n",
      "epoch no.0 train no.50310  loss = 4.41413 avg_loss = 3.90787\n",
      "epoch no.0 train no.50320  loss = 4.25870 avg_loss = 3.92857\n",
      "epoch no.0 train no.50330  loss = 4.55628 avg_loss = 3.88195\n",
      "epoch no.0 train no.50340  loss = 2.18880 avg_loss = 3.90774\n",
      "epoch no.0 train no.50350  loss = 2.55792 avg_loss = 3.90627\n",
      "epoch no.0 train no.50360  loss = 3.00009 avg_loss = 3.89138\n",
      "epoch no.0 train no.50370  loss = 4.90511 avg_loss = 3.88440\n",
      "epoch no.0 train no.50380  loss = 3.52122 avg_loss = 3.89079\n",
      "epoch no.0 train no.50390  loss = 2.93384 avg_loss = 3.99571\n",
      "epoch no.0 train no.50400  loss = 3.00846 avg_loss = 4.03349\n",
      "epoch no.0 train no.50410  loss = 2.41023 avg_loss = 3.95762\n",
      "epoch no.0 train no.50420  loss = 2.74602 avg_loss = 3.97867\n",
      "epoch no.0 train no.50430  loss = 5.47202 avg_loss = 4.00270\n",
      "epoch no.0 train no.50440  loss = 6.75667 avg_loss = 4.04805\n",
      "epoch no.0 train no.50450  loss = 2.70396 avg_loss = 4.00705\n",
      "epoch no.0 train no.50460  loss = 6.07157 avg_loss = 4.05546\n",
      "epoch no.0 train no.50470  loss = 4.85159 avg_loss = 4.07016\n",
      "epoch no.0 train no.50480  loss = 3.62180 avg_loss = 4.10589\n",
      "epoch no.0 train no.50490  loss = 5.19415 avg_loss = 4.02363\n",
      "epoch no.0 train no.50500  loss = 4.74596 avg_loss = 4.07312\n",
      "epoch no.0 train no.50510  loss = 4.05108 avg_loss = 4.03949\n",
      "epoch no.0 train no.50520  loss = 4.60608 avg_loss = 4.05487\n",
      "epoch no.0 train no.50530  loss = 4.82886 avg_loss = 4.07051\n",
      "epoch no.0 train no.50540  loss = 4.09688 avg_loss = 4.04481\n",
      "epoch no.0 train no.50550  loss = 3.23833 avg_loss = 4.00360\n",
      "epoch no.0 train no.50560  loss = 4.88849 avg_loss = 3.99850\n",
      "epoch no.0 train no.50570  loss = 3.43628 avg_loss = 3.98634\n",
      "epoch no.0 train no.50580  loss = 4.14302 avg_loss = 4.01124\n",
      "epoch no.0 train no.50590  loss = 3.82996 avg_loss = 3.99707\n",
      "epoch no.0 train no.50600  loss = 3.89623 avg_loss = 4.04659\n",
      "epoch no.0 train no.50610  loss = 5.01269 avg_loss = 4.03912\n",
      "epoch no.0 train no.50620  loss = 3.28003 avg_loss = 4.03022\n",
      "epoch no.0 train no.50630  loss = 4.03120 avg_loss = 4.06089\n",
      "epoch no.0 train no.50640  loss = 4.22416 avg_loss = 4.04744\n",
      "epoch no.0 train no.50650  loss = 2.71811 avg_loss = 4.03671\n",
      "epoch no.0 train no.50660  loss = 3.50717 avg_loss = 4.02493\n",
      "epoch no.0 train no.50670  loss = 3.72287 avg_loss = 4.06056\n",
      "epoch no.0 train no.50680  loss = 3.10396 avg_loss = 4.00690\n",
      "epoch no.0 train no.50690  loss = 3.64230 avg_loss = 3.97842\n",
      "epoch no.0 train no.50700  loss = 2.95085 avg_loss = 3.88062\n",
      "epoch no.0 train no.50710  loss = 4.53895 avg_loss = 3.85210\n",
      "epoch no.0 train no.50720  loss = 3.55212 avg_loss = 3.85410\n",
      "epoch no.0 train no.50730  loss = 2.68094 avg_loss = 3.82713\n",
      "epoch no.0 train no.50740  loss = 4.15962 avg_loss = 3.86066\n",
      "epoch no.0 train no.50750  loss = 3.51251 avg_loss = 3.91775\n",
      "epoch no.0 train no.50760  loss = 3.72634 avg_loss = 4.03620\n",
      "epoch no.0 train no.50770  loss = 3.08564 avg_loss = 4.03737\n",
      "epoch no.0 train no.50780  loss = 5.79663 avg_loss = 4.05879\n",
      "epoch no.0 train no.50790  loss = 2.44308 avg_loss = 4.06577\n",
      "epoch no.0 train no.50800  loss = 4.13911 avg_loss = 3.99754\n",
      "epoch no.0 train no.50810  loss = 2.93303 avg_loss = 3.96534\n",
      "epoch no.0 train no.50820  loss = 3.68166 avg_loss = 3.92972\n",
      "epoch no.0 train no.50830  loss = 4.78789 avg_loss = 3.89582\n",
      "epoch no.0 train no.50840  loss = 3.22647 avg_loss = 3.86592\n",
      "epoch no.0 train no.50850  loss = 3.97638 avg_loss = 3.85522\n",
      "epoch no.0 train no.50860  loss = 3.42808 avg_loss = 3.85354\n",
      "epoch no.0 train no.50870  loss = 2.92897 avg_loss = 3.83771\n",
      "epoch no.0 train no.50880  loss = 3.00804 avg_loss = 3.82426\n",
      "epoch no.0 train no.50890  loss = 4.32763 avg_loss = 3.88929\n",
      "epoch no.0 train no.50900  loss = 2.71164 avg_loss = 3.97311\n",
      "epoch no.0 train no.50910  loss = 3.09343 avg_loss = 3.91863\n",
      "epoch no.0 train no.50920  loss = 3.44710 avg_loss = 3.85825\n",
      "epoch no.0 train no.50930  loss = 4.14630 avg_loss = 3.84231\n",
      "epoch no.0 train no.50940  loss = 2.97609 avg_loss = 3.84966\n",
      "epoch no.0 train no.50950  loss = 3.33650 avg_loss = 3.83784\n",
      "epoch no.0 train no.50960  loss = 5.05249 avg_loss = 3.81613\n",
      "epoch no.0 train no.50970  loss = 6.89685 avg_loss = 3.87962\n",
      "epoch no.0 train no.50980  loss = 3.83920 avg_loss = 3.91461\n",
      "epoch no.0 train no.50990  loss = 7.10332 avg_loss = 3.92211\n",
      "epoch no.0 train no.51000  loss = 2.86919 avg_loss = 3.88522\n",
      "3\n",
      "to_tokens: ['▁비', '▁발라', '▁명', '모', '</s>']\n",
      "추억의 아이돌 노래 모음</s>\n",
      "epoch no.0 train no.51010  loss = 5.31748 avg_loss = 3.94089\n",
      "epoch no.0 train no.51020  loss = 3.50005 avg_loss = 3.87117\n",
      "epoch no.0 train no.51030  loss = 4.08326 avg_loss = 3.85560\n",
      "epoch no.0 train no.51040  loss = 5.46315 avg_loss = 3.86432\n",
      "epoch no.0 train no.51050  loss = 4.09163 avg_loss = 3.84618\n",
      "epoch no.0 train no.51060  loss = 3.03366 avg_loss = 3.76175\n",
      "epoch no.0 train no.51070  loss = 3.50882 avg_loss = 3.72862\n",
      "epoch no.0 train no.51080  loss = 5.15771 avg_loss = 3.77313\n",
      "epoch no.0 train no.51090  loss = 2.93808 avg_loss = 3.81296\n",
      "epoch no.0 train no.51100  loss = 4.03786 avg_loss = 3.78982\n",
      "epoch no.0 train no.51110  loss = 3.38962 avg_loss = 3.79394\n",
      "epoch no.0 train no.51120  loss = 3.06056 avg_loss = 3.84499\n",
      "epoch no.0 train no.51130  loss = 4.30602 avg_loss = 3.85759\n",
      "epoch no.0 train no.51140  loss = 1.71193 avg_loss = 3.80813\n",
      "epoch no.0 train no.51150  loss = 2.91546 avg_loss = 3.81938\n",
      "epoch no.0 train no.51160  loss = 4.36470 avg_loss = 3.83252\n",
      "epoch no.0 train no.51170  loss = 3.06743 avg_loss = 3.80289\n",
      "epoch no.0 train no.51180  loss = 2.25133 avg_loss = 3.77087\n",
      "epoch no.0 train no.51190  loss = 4.52779 avg_loss = 3.76794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.51200  loss = 2.54770 avg_loss = 3.78388\n",
      "epoch no.0 train no.51210  loss = 3.12933 avg_loss = 3.81662\n",
      "epoch no.0 train no.51220  loss = 4.09956 avg_loss = 3.84062\n",
      "epoch no.0 train no.51230  loss = 3.60012 avg_loss = 3.83118\n",
      "epoch no.0 train no.51240  loss = 2.11543 avg_loss = 3.80625\n",
      "epoch no.0 train no.51250  loss = 4.17012 avg_loss = 3.86626\n",
      "epoch no.0 train no.51260  loss = 3.78295 avg_loss = 3.94811\n",
      "epoch no.0 train no.51270  loss = 3.76514 avg_loss = 3.89288\n",
      "epoch no.0 train no.51280  loss = 2.92232 avg_loss = 3.91162\n",
      "epoch no.0 train no.51290  loss = 4.56301 avg_loss = 3.94875\n",
      "epoch no.0 train no.51300  loss = 3.25019 avg_loss = 3.92954\n",
      "epoch no.0 train no.51310  loss = 4.08145 avg_loss = 3.92504\n",
      "epoch no.0 train no.51320  loss = 2.79230 avg_loss = 3.89568\n",
      "epoch no.0 train no.51330  loss = 6.47638 avg_loss = 3.90332\n",
      "epoch no.0 train no.51340  loss = 2.55943 avg_loss = 3.87772\n",
      "epoch no.0 train no.51350  loss = 5.01637 avg_loss = 3.83790\n",
      "epoch no.0 train no.51360  loss = 3.30698 avg_loss = 3.86264\n",
      "epoch no.0 train no.51370  loss = 6.04274 avg_loss = 3.82133\n",
      "epoch no.0 train no.51380  loss = 4.96194 avg_loss = 3.88138\n",
      "epoch no.0 train no.51390  loss = 4.73063 avg_loss = 3.90360\n",
      "epoch no.0 train no.51400  loss = 5.25052 avg_loss = 3.89402\n",
      "epoch no.0 train no.51410  loss = 5.88816 avg_loss = 3.85878\n",
      "epoch no.0 train no.51420  loss = 5.09379 avg_loss = 3.79704\n",
      "epoch no.0 train no.51430  loss = 3.46687 avg_loss = 3.87827\n",
      "epoch no.0 train no.51440  loss = 3.62529 avg_loss = 3.86624\n",
      "epoch no.0 train no.51450  loss = 2.69860 avg_loss = 3.90424\n",
      "epoch no.0 train no.51460  loss = 3.31129 avg_loss = 3.91035\n",
      "epoch no.0 train no.51470  loss = 4.68272 avg_loss = 3.87528\n",
      "epoch no.0 train no.51480  loss = 3.69794 avg_loss = 3.91057\n",
      "epoch no.0 train no.51490  loss = 4.07172 avg_loss = 3.83638\n",
      "epoch no.0 train no.51500  loss = 4.56778 avg_loss = 3.82246\n",
      "epoch no.0 train no.51510  loss = 2.73964 avg_loss = 3.85898\n",
      "epoch no.0 train no.51520  loss = 1.83850 avg_loss = 3.81620\n",
      "epoch no.0 train no.51530  loss = 3.12948 avg_loss = 3.80667\n",
      "epoch no.0 train no.51540  loss = 4.22508 avg_loss = 3.84625\n",
      "epoch no.0 train no.51550  loss = 3.80830 avg_loss = 3.85509\n",
      "epoch no.0 train no.51560  loss = 4.54893 avg_loss = 3.93745\n",
      "epoch no.0 train no.51570  loss = 5.02684 avg_loss = 3.93894\n",
      "epoch no.0 train no.51580  loss = 3.99542 avg_loss = 3.88357\n",
      "epoch no.0 train no.51590  loss = 3.71702 avg_loss = 3.90345\n",
      "epoch no.0 train no.51600  loss = 3.47367 avg_loss = 3.89570\n",
      "epoch no.0 train no.51610  loss = 5.91079 avg_loss = 3.89470\n",
      "epoch no.0 train no.51620  loss = 4.13400 avg_loss = 3.93660\n",
      "epoch no.0 train no.51630  loss = 3.05536 avg_loss = 3.89948\n",
      "epoch no.0 train no.51640  loss = 2.23180 avg_loss = 3.90484\n",
      "epoch no.0 train no.51650  loss = 5.38495 avg_loss = 3.94315\n",
      "epoch no.0 train no.51660  loss = 5.32884 avg_loss = 3.93749\n",
      "epoch no.0 train no.51670  loss = 2.53471 avg_loss = 3.88017\n",
      "epoch no.0 train no.51680  loss = 3.46544 avg_loss = 3.85303\n",
      "epoch no.0 train no.51690  loss = 2.71610 avg_loss = 3.84926\n",
      "epoch no.0 train no.51700  loss = 3.55053 avg_loss = 3.81104\n",
      "epoch no.0 train no.51710  loss = 2.09950 avg_loss = 3.75211\n",
      "epoch no.0 train no.51720  loss = 2.54696 avg_loss = 3.75470\n",
      "epoch no.0 train no.51730  loss = 4.17970 avg_loss = 3.78353\n",
      "epoch no.0 train no.51740  loss = 6.24236 avg_loss = 3.76161\n",
      "epoch no.0 train no.51750  loss = 3.67994 avg_loss = 3.75390\n",
      "epoch no.0 train no.51760  loss = 4.89924 avg_loss = 3.72842\n",
      "epoch no.0 train no.51770  loss = 6.22206 avg_loss = 3.81057\n",
      "epoch no.0 train no.51780  loss = 3.31012 avg_loss = 3.78305\n",
      "epoch no.0 train no.51790  loss = 5.89100 avg_loss = 3.86045\n",
      "epoch no.0 train no.51800  loss = 3.02736 avg_loss = 3.86297\n",
      "epoch no.0 train no.51810  loss = 4.57905 avg_loss = 3.89257\n",
      "epoch no.0 train no.51820  loss = 4.32434 avg_loss = 3.93854\n",
      "epoch no.0 train no.51830  loss = 2.78168 avg_loss = 3.89143\n",
      "epoch no.0 train no.51840  loss = 3.27750 avg_loss = 3.85460\n",
      "epoch no.0 train no.51850  loss = 2.89219 avg_loss = 3.87642\n",
      "epoch no.0 train no.51860  loss = 4.76337 avg_loss = 3.89203\n",
      "epoch no.0 train no.51870  loss = 4.15022 avg_loss = 3.91405\n",
      "epoch no.0 train no.51880  loss = 4.61257 avg_loss = 3.85084\n",
      "epoch no.0 train no.51890  loss = 3.62979 avg_loss = 3.83559\n",
      "epoch no.0 train no.51900  loss = 2.59232 avg_loss = 3.85831\n",
      "epoch no.0 train no.51910  loss = 4.64031 avg_loss = 3.91468\n",
      "epoch no.0 train no.51920  loss = 2.82063 avg_loss = 3.94033\n",
      "epoch no.0 train no.51930  loss = 4.37631 avg_loss = 3.91821\n",
      "epoch no.0 train no.51940  loss = 2.89022 avg_loss = 3.91226\n",
      "epoch no.0 train no.51950  loss = 5.03482 avg_loss = 3.96287\n",
      "epoch no.0 train no.51960  loss = 3.86378 avg_loss = 3.96703\n",
      "epoch no.0 train no.51970  loss = 7.12994 avg_loss = 3.99357\n",
      "epoch no.0 train no.51980  loss = 2.94672 avg_loss = 3.96319\n",
      "epoch no.0 train no.51990  loss = 3.83528 avg_loss = 3.89384\n",
      "epoch no.0 train no.52000  loss = 5.11991 avg_loss = 3.84480\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', 'st', '▁모음', '집', '</s>']\n",
      "추억의 ost 모음집</s>\n",
      "epoch no.0 train no.52010  loss = 3.85496 avg_loss = 3.79753\n",
      "epoch no.0 train no.52020  loss = 4.67759 avg_loss = 3.76685\n",
      "epoch no.0 train no.52030  loss = 3.04514 avg_loss = 3.72575\n",
      "epoch no.0 train no.52040  loss = 4.50235 avg_loss = 3.73711\n",
      "epoch no.0 train no.52050  loss = 4.18884 avg_loss = 3.81003\n",
      "epoch no.0 train no.52060  loss = 2.25010 avg_loss = 3.86147\n",
      "epoch no.0 train no.52070  loss = 4.20595 avg_loss = 3.84645\n",
      "epoch no.0 train no.52080  loss = 5.06938 avg_loss = 3.83189\n",
      "epoch no.0 train no.52090  loss = 5.79613 avg_loss = 3.82823\n",
      "epoch no.0 train no.52100  loss = 3.84972 avg_loss = 3.88697\n",
      "epoch no.0 train no.52110  loss = 2.58179 avg_loss = 3.87397\n",
      "epoch no.0 train no.52120  loss = 7.05079 avg_loss = 3.93889\n",
      "epoch no.0 train no.52130  loss = 2.58233 avg_loss = 3.89620\n",
      "epoch no.0 train no.52140  loss = 2.41372 avg_loss = 3.89158\n",
      "epoch no.0 train no.52150  loss = 3.49474 avg_loss = 3.89597\n",
      "epoch no.0 train no.52160  loss = 3.81336 avg_loss = 3.91731\n",
      "epoch no.0 train no.52170  loss = 2.60999 avg_loss = 3.94938\n",
      "epoch no.0 train no.52180  loss = 4.08368 avg_loss = 3.93591\n",
      "epoch no.0 train no.52190  loss = 3.43358 avg_loss = 3.92404\n",
      "epoch no.0 train no.52200  loss = 4.51041 avg_loss = 3.90874\n",
      "epoch no.0 train no.52210  loss = 2.67612 avg_loss = 3.82295\n",
      "epoch no.0 train no.52220  loss = 3.60081 avg_loss = 3.77348\n",
      "epoch no.0 train no.52230  loss = 2.64046 avg_loss = 3.80122\n",
      "epoch no.0 train no.52240  loss = 4.75264 avg_loss = 3.79201\n",
      "epoch no.0 train no.52250  loss = 2.74731 avg_loss = 3.83360\n",
      "epoch no.0 train no.52260  loss = 3.55452 avg_loss = 3.84335\n",
      "epoch no.0 train no.52270  loss = 2.72462 avg_loss = 3.87280\n",
      "epoch no.0 train no.52280  loss = 4.50118 avg_loss = 3.92751\n",
      "epoch no.0 train no.52290  loss = 3.55945 avg_loss = 3.92491\n",
      "epoch no.0 train no.52300  loss = 2.89683 avg_loss = 3.95704\n",
      "epoch no.0 train no.52310  loss = 4.25485 avg_loss = 3.95555\n",
      "epoch no.0 train no.52320  loss = 2.68149 avg_loss = 3.90013\n",
      "epoch no.0 train no.52330  loss = 4.00901 avg_loss = 3.88234\n",
      "epoch no.0 train no.52340  loss = 4.19899 avg_loss = 3.87994\n",
      "epoch no.0 train no.52350  loss = 2.08069 avg_loss = 3.88792\n",
      "epoch no.0 train no.52360  loss = 3.82983 avg_loss = 3.91579\n",
      "epoch no.0 train no.52370  loss = 2.58665 avg_loss = 3.93199\n",
      "epoch no.0 train no.52380  loss = 3.92079 avg_loss = 3.90982\n",
      "epoch no.0 train no.52390  loss = 4.96171 avg_loss = 3.98484\n",
      "epoch no.0 train no.52400  loss = 3.70894 avg_loss = 3.96761\n",
      "epoch no.0 train no.52410  loss = 3.50004 avg_loss = 3.98869\n",
      "epoch no.0 train no.52420  loss = 3.68473 avg_loss = 3.99747\n",
      "epoch no.0 train no.52430  loss = 2.29429 avg_loss = 3.93996\n",
      "epoch no.0 train no.52440  loss = 1.82173 avg_loss = 3.90456\n",
      "epoch no.0 train no.52450  loss = 4.23910 avg_loss = 3.92085\n",
      "epoch no.0 train no.52460  loss = 2.49573 avg_loss = 3.92636\n",
      "epoch no.0 train no.52470  loss = 5.71451 avg_loss = 4.01679\n",
      "epoch no.0 train no.52480  loss = 5.46230 avg_loss = 4.00780\n",
      "epoch no.0 train no.52490  loss = 6.78081 avg_loss = 4.06429\n",
      "epoch no.0 train no.52500  loss = 3.25481 avg_loss = 4.03167\n",
      "epoch no.0 train no.52510  loss = 3.38409 avg_loss = 3.98925\n",
      "epoch no.0 train no.52520  loss = 3.51492 avg_loss = 3.94887\n",
      "epoch no.0 train no.52530  loss = 2.41207 avg_loss = 3.88509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.52540  loss = 3.53185 avg_loss = 3.87846\n",
      "epoch no.0 train no.52550  loss = 3.15553 avg_loss = 3.85465\n",
      "epoch no.0 train no.52560  loss = 6.21129 avg_loss = 3.88683\n",
      "epoch no.0 train no.52570  loss = 5.06931 avg_loss = 3.89252\n",
      "epoch no.0 train no.52580  loss = 3.01124 avg_loss = 3.90673\n",
      "epoch no.0 train no.52590  loss = 2.98141 avg_loss = 3.83043\n",
      "epoch no.0 train no.52600  loss = 2.76753 avg_loss = 3.78568\n",
      "epoch no.0 train no.52610  loss = 4.74503 avg_loss = 3.84093\n",
      "epoch no.0 train no.52620  loss = 6.49420 avg_loss = 3.87520\n",
      "epoch no.0 train no.52630  loss = 1.75849 avg_loss = 3.84781\n",
      "epoch no.0 train no.52640  loss = 3.42716 avg_loss = 3.84344\n",
      "epoch no.0 train no.52650  loss = 3.90045 avg_loss = 3.80183\n",
      "epoch no.0 train no.52660  loss = 4.36130 avg_loss = 3.82219\n",
      "epoch no.0 train no.52670  loss = 3.89771 avg_loss = 3.76653\n",
      "epoch no.0 train no.52680  loss = 4.01949 avg_loss = 3.76756\n",
      "epoch no.0 train no.52690  loss = 3.23171 avg_loss = 3.81366\n",
      "epoch no.0 train no.52700  loss = 4.69214 avg_loss = 3.84157\n",
      "epoch no.0 train no.52710  loss = 3.99209 avg_loss = 3.84188\n",
      "epoch no.0 train no.52720  loss = 4.55570 avg_loss = 3.84681\n",
      "epoch no.0 train no.52730  loss = 2.18997 avg_loss = 3.87359\n",
      "epoch no.0 train no.52740  loss = 4.34636 avg_loss = 3.90294\n",
      "epoch no.0 train no.52750  loss = 3.26151 avg_loss = 3.90538\n",
      "epoch no.0 train no.52760  loss = 4.46803 avg_loss = 3.88552\n",
      "epoch no.0 train no.52770  loss = 2.63833 avg_loss = 3.87457\n",
      "epoch no.0 train no.52780  loss = 3.03347 avg_loss = 3.86439\n",
      "epoch no.0 train no.52790  loss = 4.64124 avg_loss = 3.87755\n",
      "epoch no.0 train no.52800  loss = 3.70699 avg_loss = 3.87047\n",
      "epoch no.0 train no.52810  loss = 4.25985 avg_loss = 3.95217\n",
      "epoch no.0 train no.52820  loss = 3.47952 avg_loss = 3.91715\n",
      "epoch no.0 train no.52830  loss = 3.62726 avg_loss = 3.95510\n",
      "epoch no.0 train no.52840  loss = 2.65993 avg_loss = 4.04220\n",
      "epoch no.0 train no.52850  loss = 4.36307 avg_loss = 4.02237\n",
      "epoch no.0 train no.52860  loss = 2.72783 avg_loss = 3.96391\n",
      "epoch no.0 train no.52870  loss = 3.78516 avg_loss = 4.01344\n",
      "epoch no.0 train no.52880  loss = 3.67243 avg_loss = 3.99923\n",
      "epoch no.0 train no.52890  loss = 6.37770 avg_loss = 4.00822\n",
      "epoch no.0 train no.52900  loss = 2.52781 avg_loss = 3.96990\n",
      "epoch no.0 train no.52910  loss = 1.24603 avg_loss = 3.97486\n",
      "epoch no.0 train no.52920  loss = 4.43714 avg_loss = 3.99133\n",
      "epoch no.0 train no.52930  loss = 2.49574 avg_loss = 3.95013\n",
      "epoch no.0 train no.52940  loss = 5.50748 avg_loss = 3.99333\n",
      "epoch no.0 train no.52950  loss = 2.84415 avg_loss = 3.95721\n",
      "epoch no.0 train no.52960  loss = 2.18443 avg_loss = 3.96584\n",
      "epoch no.0 train no.52970  loss = 4.56248 avg_loss = 4.00333\n",
      "epoch no.0 train no.52980  loss = 4.69787 avg_loss = 4.04258\n",
      "epoch no.0 train no.52990  loss = 2.50187 avg_loss = 4.03869\n",
      "epoch no.0 train no.53000  loss = 2.11652 avg_loss = 4.00123\n",
      "6\n",
      "to_tokens: ['▁비', '▁발라', '들', '</s>', '봤', '어요', '</s>']\n",
      "추억의 노래들  모아봤어요</s>\n",
      "epoch no.0 train no.53010  loss = 5.30719 avg_loss = 3.95461\n",
      "epoch no.0 train no.53020  loss = 2.70993 avg_loss = 3.90474\n",
      "epoch no.0 train no.53030  loss = 2.92502 avg_loss = 3.91030\n",
      "epoch no.0 train no.53040  loss = 4.80433 avg_loss = 3.89241\n",
      "epoch no.0 train no.53050  loss = 2.68255 avg_loss = 3.85196\n",
      "epoch no.0 train no.53060  loss = 3.59949 avg_loss = 3.82624\n",
      "epoch no.0 train no.53070  loss = 4.60406 avg_loss = 3.88389\n",
      "epoch no.0 train no.53080  loss = 7.13737 avg_loss = 3.88125\n",
      "epoch no.0 train no.53090  loss = 2.73513 avg_loss = 3.88434\n",
      "epoch no.0 train no.53100  loss = 5.89227 avg_loss = 3.87471\n",
      "epoch no.0 train no.53110  loss = 1.98622 avg_loss = 3.91903\n",
      "epoch no.0 train no.53120  loss = 3.15761 avg_loss = 3.98181\n",
      "epoch no.0 train no.53130  loss = 6.37722 avg_loss = 3.97242\n",
      "epoch no.0 train no.53140  loss = 2.19104 avg_loss = 3.95760\n",
      "epoch no.0 train no.53150  loss = 2.92815 avg_loss = 3.94932\n",
      "epoch no.0 train no.53160  loss = 3.75856 avg_loss = 3.91154\n",
      "epoch no.0 train no.53170  loss = 4.19211 avg_loss = 3.93088\n",
      "epoch no.0 train no.53180  loss = 4.13606 avg_loss = 3.96611\n",
      "epoch no.0 train no.53190  loss = 2.50051 avg_loss = 3.89561\n",
      "epoch no.0 train no.53200  loss = 3.80622 avg_loss = 3.85876\n",
      "epoch no.0 train no.53210  loss = 3.11422 avg_loss = 3.86143\n",
      "epoch no.0 train no.53220  loss = 6.42332 avg_loss = 3.98108\n",
      "epoch no.0 train no.53230  loss = 3.83429 avg_loss = 3.97337\n",
      "epoch no.0 train no.53240  loss = 2.43891 avg_loss = 3.95235\n",
      "epoch no.0 train no.53250  loss = 3.96761 avg_loss = 3.99338\n",
      "epoch no.0 train no.53260  loss = 2.55657 avg_loss = 3.99073\n",
      "epoch no.0 train no.53270  loss = 2.32865 avg_loss = 4.00249\n",
      "epoch no.0 train no.53280  loss = 4.73274 avg_loss = 3.99377\n",
      "epoch no.0 train no.53290  loss = 3.10880 avg_loss = 3.96722\n",
      "epoch no.0 train no.53300  loss = 3.09855 avg_loss = 3.95793\n",
      "epoch no.0 train no.53310  loss = 5.12947 avg_loss = 4.00257\n",
      "epoch no.0 train no.53320  loss = 4.01075 avg_loss = 4.00130\n",
      "epoch no.0 train no.53330  loss = 2.79927 avg_loss = 3.93900\n",
      "epoch no.0 train no.53340  loss = 5.02992 avg_loss = 3.92153\n",
      "epoch no.0 train no.53350  loss = 4.26827 avg_loss = 3.90767\n",
      "epoch no.0 train no.53360  loss = 3.21789 avg_loss = 3.92857\n",
      "epoch no.0 train no.53370  loss = 3.53419 avg_loss = 3.95387\n",
      "epoch no.0 train no.53380  loss = 3.15978 avg_loss = 3.97021\n",
      "epoch no.0 train no.53390  loss = 4.07198 avg_loss = 3.90463\n",
      "epoch no.0 train no.53400  loss = 4.65769 avg_loss = 3.88963\n",
      "epoch no.0 train no.53410  loss = 3.51972 avg_loss = 3.87629\n",
      "epoch no.0 train no.53420  loss = 3.40864 avg_loss = 3.86336\n",
      "epoch no.0 train no.53430  loss = 5.82880 avg_loss = 3.91324\n",
      "epoch no.0 train no.53440  loss = 4.86029 avg_loss = 3.90301\n",
      "epoch no.0 train no.53450  loss = 4.56896 avg_loss = 3.87042\n",
      "epoch no.0 train no.53460  loss = 4.18686 avg_loss = 3.83536\n",
      "epoch no.0 train no.53470  loss = 5.52511 avg_loss = 3.80862\n",
      "epoch no.0 train no.53480  loss = 4.55003 avg_loss = 3.85477\n",
      "epoch no.0 train no.53490  loss = 6.16564 avg_loss = 3.87365\n",
      "epoch no.0 train no.53500  loss = 4.90728 avg_loss = 3.91043\n",
      "epoch no.0 train no.53510  loss = 6.57905 avg_loss = 3.89470\n",
      "epoch no.0 train no.53520  loss = 2.27414 avg_loss = 3.87966\n",
      "epoch no.0 train no.53530  loss = 4.78777 avg_loss = 3.91516\n",
      "epoch no.0 train no.53540  loss = 2.17842 avg_loss = 3.87527\n",
      "epoch no.0 train no.53550  loss = 5.52566 avg_loss = 3.91141\n",
      "epoch no.0 train no.53560  loss = 2.23145 avg_loss = 3.84967\n",
      "epoch no.0 train no.53570  loss = 3.74475 avg_loss = 3.80187\n",
      "epoch no.0 train no.53580  loss = 4.49597 avg_loss = 3.86652\n",
      "epoch no.0 train no.53590  loss = 4.44979 avg_loss = 3.87704\n",
      "epoch no.0 train no.53600  loss = 4.20519 avg_loss = 3.87031\n",
      "epoch no.0 train no.53610  loss = 2.60551 avg_loss = 3.79941\n",
      "epoch no.0 train no.53620  loss = 2.39656 avg_loss = 3.75522\n",
      "epoch no.0 train no.53630  loss = 1.85968 avg_loss = 3.77903\n",
      "epoch no.0 train no.53640  loss = 5.75628 avg_loss = 3.86820\n",
      "epoch no.0 train no.53650  loss = 2.81503 avg_loss = 3.85319\n",
      "epoch no.0 train no.53660  loss = 3.89902 avg_loss = 3.85155\n",
      "epoch no.0 train no.53670  loss = 4.53552 avg_loss = 3.84263\n",
      "epoch no.0 train no.53680  loss = 4.52463 avg_loss = 3.87905\n",
      "epoch no.0 train no.53690  loss = 4.57692 avg_loss = 3.80928\n",
      "epoch no.0 train no.53700  loss = 3.60272 avg_loss = 3.83733\n",
      "epoch no.0 train no.53710  loss = 6.83791 avg_loss = 3.87635\n",
      "epoch no.0 train no.53720  loss = 5.14099 avg_loss = 3.90234\n",
      "epoch no.0 train no.53730  loss = 4.24600 avg_loss = 3.95249\n",
      "epoch no.0 train no.53740  loss = 3.71766 avg_loss = 3.96739\n",
      "epoch no.0 train no.53750  loss = 4.33812 avg_loss = 3.98298\n",
      "epoch no.0 train no.53760  loss = 3.90661 avg_loss = 3.94971\n",
      "epoch no.0 train no.53770  loss = 4.13024 avg_loss = 3.98274\n",
      "epoch no.0 train no.53780  loss = 2.75309 avg_loss = 3.95513\n",
      "epoch no.0 train no.53790  loss = 4.30232 avg_loss = 3.93523\n",
      "epoch no.0 train no.53800  loss = 2.91208 avg_loss = 3.93856\n",
      "epoch no.0 train no.53810  loss = 4.26471 avg_loss = 3.92170\n",
      "epoch no.0 train no.53820  loss = 2.01859 avg_loss = 3.89873\n",
      "epoch no.0 train no.53830  loss = 7.11043 avg_loss = 3.94487\n",
      "epoch no.0 train no.53840  loss = 4.75992 avg_loss = 3.93734\n",
      "epoch no.0 train no.53850  loss = 2.79454 avg_loss = 3.91671\n",
      "epoch no.0 train no.53860  loss = 3.98559 avg_loss = 3.92201\n",
      "epoch no.0 train no.53870  loss = 3.98338 avg_loss = 3.89459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.53880  loss = 2.97358 avg_loss = 3.88395\n",
      "epoch no.0 train no.53890  loss = 2.82734 avg_loss = 3.89370\n",
      "epoch no.0 train no.53900  loss = 2.62343 avg_loss = 3.89528\n",
      "epoch no.0 train no.53910  loss = 4.75977 avg_loss = 3.95869\n",
      "epoch no.0 train no.53920  loss = 3.12659 avg_loss = 3.97313\n",
      "epoch no.0 train no.53930  loss = 2.20113 avg_loss = 3.95845\n",
      "epoch no.0 train no.53940  loss = 4.50397 avg_loss = 3.94464\n",
      "epoch no.0 train no.53950  loss = 1.96886 avg_loss = 3.88136\n",
      "epoch no.0 train no.53960  loss = 2.98186 avg_loss = 3.88101\n",
      "epoch no.0 train no.53970  loss = 2.59254 avg_loss = 3.83447\n",
      "epoch no.0 train no.53980  loss = 3.54739 avg_loss = 3.82709\n",
      "epoch no.0 train no.53990  loss = 4.33790 avg_loss = 3.84708\n",
      "epoch no.0 train no.54000  loss = 2.66517 avg_loss = 3.83292\n",
      "3\n",
      "to_tokens: ['▁비', '▁발라', '들', '▁모음', '</s>']\n",
      "추억의 노래들 모음</s>\n",
      "epoch no.0 train no.54010  loss = 5.06102 avg_loss = 3.91081\n",
      "epoch no.0 train no.54020  loss = 2.93898 avg_loss = 3.91035\n",
      "epoch no.0 train no.54030  loss = 3.43156 avg_loss = 3.89330\n",
      "epoch no.0 train no.54040  loss = 4.30760 avg_loss = 3.91026\n",
      "epoch no.0 train no.54050  loss = 5.77620 avg_loss = 3.92136\n",
      "epoch no.0 train no.54060  loss = 2.87612 avg_loss = 3.88880\n",
      "epoch no.0 train no.54070  loss = 7.11451 avg_loss = 3.92579\n",
      "epoch no.0 train no.54080  loss = 4.36146 avg_loss = 3.93137\n",
      "epoch no.0 train no.54090  loss = 4.20075 avg_loss = 3.96120\n",
      "epoch no.0 train no.54100  loss = 1.87711 avg_loss = 3.91504\n",
      "epoch no.0 train no.54110  loss = 3.03484 avg_loss = 3.89343\n",
      "epoch no.0 train no.54120  loss = 5.14967 avg_loss = 3.93799\n",
      "epoch no.0 train no.54130  loss = 4.75398 avg_loss = 3.95561\n",
      "epoch no.0 train no.54140  loss = 4.50121 avg_loss = 3.95462\n",
      "epoch no.0 train no.54150  loss = 5.10541 avg_loss = 3.98742\n",
      "epoch no.0 train no.54160  loss = 4.71109 avg_loss = 4.03429\n",
      "epoch no.0 train no.54170  loss = 3.98124 avg_loss = 4.03637\n",
      "epoch no.0 train no.54180  loss = 2.83339 avg_loss = 4.03339\n",
      "epoch no.0 train no.54190  loss = 4.71618 avg_loss = 4.01775\n",
      "epoch no.0 train no.54200  loss = 5.94448 avg_loss = 4.05107\n",
      "epoch no.0 train no.54210  loss = 5.03472 avg_loss = 3.99588\n",
      "epoch no.0 train no.54220  loss = 3.87698 avg_loss = 4.04934\n",
      "epoch no.0 train no.54230  loss = 4.37957 avg_loss = 4.09861\n",
      "epoch no.0 train no.54240  loss = 3.22909 avg_loss = 4.13508\n",
      "epoch no.0 train no.54250  loss = 5.01957 avg_loss = 4.15420\n",
      "epoch no.0 train no.54260  loss = 3.79605 avg_loss = 4.13038\n",
      "epoch no.0 train no.54270  loss = 3.75159 avg_loss = 4.10965\n",
      "epoch no.0 train no.54280  loss = 3.43473 avg_loss = 4.04648\n",
      "epoch no.0 train no.54290  loss = 4.09092 avg_loss = 4.04619\n",
      "epoch no.0 train no.54300  loss = 4.87477 avg_loss = 4.09709\n",
      "epoch no.0 train no.54310  loss = 2.92601 avg_loss = 4.16199\n",
      "epoch no.0 train no.54320  loss = 3.79436 avg_loss = 4.18008\n",
      "epoch no.0 train no.54330  loss = 2.34957 avg_loss = 4.09333\n",
      "epoch no.0 train no.54340  loss = 3.26995 avg_loss = 3.99734\n",
      "epoch no.0 train no.54350  loss = 4.54468 avg_loss = 4.02684\n",
      "epoch no.0 train no.54360  loss = 2.95233 avg_loss = 3.96114\n",
      "epoch no.0 train no.54370  loss = 2.59742 avg_loss = 4.02571\n",
      "epoch no.0 train no.54380  loss = 3.62879 avg_loss = 3.98606\n",
      "epoch no.0 train no.54390  loss = 3.93861 avg_loss = 3.97058\n",
      "epoch no.0 train no.54400  loss = 2.28425 avg_loss = 3.93155\n",
      "epoch no.0 train no.54410  loss = 3.51302 avg_loss = 3.94065\n",
      "epoch no.0 train no.54420  loss = 3.33154 avg_loss = 3.95238\n",
      "epoch no.0 train no.54430  loss = 4.21356 avg_loss = 3.98193\n",
      "epoch no.0 train no.54440  loss = 3.62183 avg_loss = 3.92304\n",
      "epoch no.0 train no.54450  loss = 3.56388 avg_loss = 3.81803\n",
      "epoch no.0 train no.54460  loss = 2.49184 avg_loss = 3.80691\n",
      "epoch no.0 train no.54470  loss = 4.81747 avg_loss = 3.82740\n",
      "epoch no.0 train no.54480  loss = 3.55017 avg_loss = 3.84847\n",
      "epoch no.0 train no.54490  loss = 3.09351 avg_loss = 3.82287\n",
      "epoch no.0 train no.54500  loss = 2.51809 avg_loss = 3.82677\n",
      "epoch no.0 train no.54510  loss = 5.28446 avg_loss = 3.88825\n",
      "epoch no.0 train no.54520  loss = 3.59408 avg_loss = 3.91982\n",
      "epoch no.0 train no.54530  loss = 2.70873 avg_loss = 3.89518\n",
      "epoch no.0 train no.54540  loss = 2.81198 avg_loss = 3.85218\n",
      "epoch no.0 train no.54550  loss = 2.93766 avg_loss = 3.80904\n",
      "epoch no.0 train no.54560  loss = 4.24414 avg_loss = 3.83154\n",
      "epoch no.0 train no.54570  loss = 3.61244 avg_loss = 3.82905\n",
      "epoch no.0 train no.54580  loss = 2.20784 avg_loss = 3.75837\n",
      "epoch no.0 train no.54590  loss = 4.06925 avg_loss = 3.74250\n",
      "epoch no.0 train no.54600  loss = 3.97561 avg_loss = 3.77810\n",
      "epoch no.0 train no.54610  loss = 6.94201 avg_loss = 3.81321\n",
      "epoch no.0 train no.54620  loss = 4.87749 avg_loss = 3.81290\n",
      "epoch no.0 train no.54630  loss = 4.07265 avg_loss = 3.83890\n",
      "epoch no.0 train no.54640  loss = 5.51671 avg_loss = 3.84001\n",
      "epoch no.0 train no.54650  loss = 2.77308 avg_loss = 3.81979\n",
      "epoch no.0 train no.54660  loss = 5.87086 avg_loss = 3.92965\n",
      "epoch no.0 train no.54670  loss = 4.97152 avg_loss = 3.96452\n",
      "epoch no.0 train no.54680  loss = 3.91002 avg_loss = 4.02696\n",
      "epoch no.0 train no.54690  loss = 5.11104 avg_loss = 4.04457\n",
      "epoch no.0 train no.54700  loss = 4.19685 avg_loss = 4.08582\n",
      "epoch no.0 train no.54710  loss = 6.41648 avg_loss = 4.07889\n",
      "epoch no.0 train no.54720  loss = 4.29586 avg_loss = 4.08786\n",
      "epoch no.0 train no.54730  loss = 1.75238 avg_loss = 4.07249\n",
      "epoch no.0 train no.54740  loss = 3.28926 avg_loss = 4.04353\n",
      "epoch no.0 train no.54750  loss = 4.95950 avg_loss = 4.09685\n",
      "epoch no.0 train no.54760  loss = 4.20514 avg_loss = 4.10470\n",
      "epoch no.0 train no.54770  loss = 3.76337 avg_loss = 4.04655\n",
      "epoch no.0 train no.54780  loss = 2.34822 avg_loss = 3.98681\n",
      "epoch no.0 train no.54790  loss = 2.48476 avg_loss = 3.95374\n",
      "epoch no.0 train no.54800  loss = 2.34996 avg_loss = 3.94308\n",
      "epoch no.0 train no.54810  loss = 5.71302 avg_loss = 3.95927\n",
      "epoch no.0 train no.54820  loss = 3.02164 avg_loss = 3.98174\n",
      "epoch no.0 train no.54830  loss = 1.88910 avg_loss = 3.97838\n",
      "epoch no.0 train no.54840  loss = 5.49860 avg_loss = 3.95395\n",
      "epoch no.0 train no.54850  loss = 3.46906 avg_loss = 3.99142\n",
      "epoch no.0 train no.54860  loss = 4.34356 avg_loss = 4.02452\n",
      "epoch no.0 train no.54870  loss = 3.16510 avg_loss = 4.01488\n",
      "epoch no.0 train no.54880  loss = 4.52191 avg_loss = 4.00380\n",
      "epoch no.0 train no.54890  loss = 2.37262 avg_loss = 3.99950\n",
      "epoch no.0 train no.54900  loss = 6.42760 avg_loss = 4.02795\n",
      "epoch no.0 train no.54910  loss = 3.05200 avg_loss = 4.04583\n",
      "epoch no.0 train no.54920  loss = 1.56572 avg_loss = 4.00318\n",
      "epoch no.0 train no.54930  loss = 2.72611 avg_loss = 3.98748\n",
      "epoch no.0 train no.54940  loss = 3.58005 avg_loss = 3.98794\n",
      "epoch no.0 train no.54950  loss = 3.03455 avg_loss = 4.00719\n",
      "epoch no.0 train no.54960  loss = 5.61208 avg_loss = 4.02289\n",
      "epoch no.0 train no.54970  loss = 5.50745 avg_loss = 4.05217\n",
      "epoch no.0 train no.54980  loss = 3.28057 avg_loss = 4.07075\n",
      "epoch no.0 train no.54990  loss = 3.60338 avg_loss = 4.05515\n",
      "epoch no.0 train no.55000  loss = 2.65379 avg_loss = 4.00643\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁추억의', '▁팝', '</s>']\n",
      "추억의 2000년대 추억의 가요</s>\n",
      "epoch no.0 train no.55010  loss = 3.93395 avg_loss = 3.99607\n",
      "epoch no.0 train no.55020  loss = 3.09368 avg_loss = 3.97321\n",
      "epoch no.0 train no.55030  loss = 4.57906 avg_loss = 3.96875\n",
      "epoch no.0 train no.55040  loss = 2.54592 avg_loss = 3.96452\n",
      "epoch no.0 train no.55050  loss = 4.26296 avg_loss = 3.93408\n",
      "epoch no.0 train no.55060  loss = 3.41551 avg_loss = 3.89466\n",
      "epoch no.0 train no.55070  loss = 4.34562 avg_loss = 3.89729\n",
      "epoch no.0 train no.55080  loss = 3.02759 avg_loss = 3.89292\n",
      "epoch no.0 train no.55090  loss = 5.50865 avg_loss = 3.85379\n",
      "epoch no.0 train no.55100  loss = 3.98600 avg_loss = 3.91492\n",
      "epoch no.0 train no.55110  loss = 5.24801 avg_loss = 3.89525\n",
      "epoch no.0 train no.55120  loss = 2.64806 avg_loss = 3.87313\n",
      "epoch no.0 train no.55130  loss = 3.44316 avg_loss = 3.83659\n",
      "epoch no.0 train no.55140  loss = 3.34808 avg_loss = 3.80570\n",
      "epoch no.0 train no.55150  loss = 3.89282 avg_loss = 3.82581\n",
      "epoch no.0 train no.55160  loss = 3.48661 avg_loss = 3.80647\n",
      "epoch no.0 train no.55170  loss = 4.14727 avg_loss = 3.83413\n",
      "epoch no.0 train no.55180  loss = 3.70675 avg_loss = 3.84460\n",
      "epoch no.0 train no.55190  loss = 5.27143 avg_loss = 3.80862\n",
      "epoch no.0 train no.55200  loss = 4.40774 avg_loss = 3.81161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.55210  loss = 3.50339 avg_loss = 3.78961\n",
      "epoch no.0 train no.55220  loss = 5.57623 avg_loss = 3.84774\n",
      "epoch no.0 train no.55230  loss = 3.25599 avg_loss = 3.87084\n",
      "epoch no.0 train no.55240  loss = 6.88612 avg_loss = 3.91761\n",
      "epoch no.0 train no.55250  loss = 5.10839 avg_loss = 3.92620\n",
      "epoch no.0 train no.55260  loss = 5.02344 avg_loss = 3.88227\n",
      "epoch no.0 train no.55270  loss = 6.15420 avg_loss = 3.89358\n",
      "epoch no.0 train no.55280  loss = 4.94031 avg_loss = 3.89261\n",
      "epoch no.0 train no.55290  loss = 4.69327 avg_loss = 3.86329\n",
      "epoch no.0 train no.55300  loss = 5.58498 avg_loss = 3.82667\n",
      "epoch no.0 train no.55310  loss = 6.59626 avg_loss = 3.85964\n",
      "epoch no.0 train no.55320  loss = 1.94639 avg_loss = 3.81972\n",
      "epoch no.0 train no.55330  loss = 2.60176 avg_loss = 3.82451\n",
      "epoch no.0 train no.55340  loss = 6.58975 avg_loss = 3.87146\n",
      "epoch no.0 train no.55350  loss = 5.13224 avg_loss = 3.87728\n",
      "epoch no.0 train no.55360  loss = 4.96406 avg_loss = 3.93708\n",
      "epoch no.0 train no.55370  loss = 3.34679 avg_loss = 3.92505\n",
      "epoch no.0 train no.55380  loss = 4.08221 avg_loss = 3.94589\n",
      "epoch no.0 train no.55390  loss = 5.27096 avg_loss = 3.90595\n",
      "epoch no.0 train no.55400  loss = 2.49593 avg_loss = 3.86167\n",
      "epoch no.0 train no.55410  loss = 3.21645 avg_loss = 3.82114\n",
      "epoch no.0 train no.55420  loss = 4.33718 avg_loss = 3.86758\n",
      "epoch no.0 train no.55430  loss = 2.48633 avg_loss = 3.95601\n",
      "epoch no.0 train no.55440  loss = 2.93274 avg_loss = 3.90612\n",
      "epoch no.0 train no.55450  loss = 3.08027 avg_loss = 3.90089\n",
      "epoch no.0 train no.55460  loss = 3.62884 avg_loss = 3.92042\n",
      "epoch no.0 train no.55470  loss = 2.05206 avg_loss = 3.88352\n",
      "epoch no.0 train no.55480  loss = 2.85849 avg_loss = 3.86223\n",
      "epoch no.0 train no.55490  loss = 2.68345 avg_loss = 3.82039\n",
      "epoch no.0 train no.55500  loss = 5.24939 avg_loss = 3.74073\n",
      "epoch no.0 train no.55510  loss = 3.71881 avg_loss = 3.75243\n",
      "epoch no.0 train no.55520  loss = 4.29107 avg_loss = 3.75927\n",
      "epoch no.0 train no.55530  loss = 3.05935 avg_loss = 3.74973\n",
      "epoch no.0 train no.55540  loss = 3.27768 avg_loss = 3.74023\n",
      "epoch no.0 train no.55550  loss = 3.80368 avg_loss = 3.72158\n",
      "epoch no.0 train no.55560  loss = 3.15500 avg_loss = 3.72269\n",
      "epoch no.0 train no.55570  loss = 4.52412 avg_loss = 3.72821\n",
      "epoch no.0 train no.55580  loss = 6.65640 avg_loss = 3.79888\n",
      "epoch no.0 train no.55590  loss = 5.75132 avg_loss = 3.86520\n",
      "epoch no.0 train no.55600  loss = 3.03390 avg_loss = 3.91345\n",
      "epoch no.0 train no.55610  loss = 3.64041 avg_loss = 3.93350\n",
      "epoch no.0 train no.55620  loss = 3.56363 avg_loss = 3.93248\n",
      "epoch no.0 train no.55630  loss = 4.82707 avg_loss = 3.92678\n",
      "epoch no.0 train no.55640  loss = 3.83838 avg_loss = 3.89612\n",
      "epoch no.0 train no.55650  loss = 3.37423 avg_loss = 3.91189\n",
      "epoch no.0 train no.55660  loss = 2.77914 avg_loss = 3.84026\n",
      "epoch no.0 train no.55670  loss = 4.11815 avg_loss = 3.85406\n",
      "epoch no.0 train no.55680  loss = 3.73358 avg_loss = 3.85010\n",
      "epoch no.0 train no.55690  loss = 4.82241 avg_loss = 3.83319\n",
      "epoch no.0 train no.55700  loss = 2.93752 avg_loss = 3.82484\n",
      "epoch no.0 train no.55710  loss = 3.55596 avg_loss = 3.84809\n",
      "epoch no.0 train no.55720  loss = 2.72609 avg_loss = 3.83432\n",
      "epoch no.0 train no.55730  loss = 3.81872 avg_loss = 3.80310\n",
      "epoch no.0 train no.55740  loss = 6.10331 avg_loss = 3.80596\n",
      "epoch no.0 train no.55750  loss = 3.11558 avg_loss = 3.82153\n",
      "epoch no.0 train no.55760  loss = 2.59756 avg_loss = 3.76380\n",
      "epoch no.0 train no.55770  loss = 3.36465 avg_loss = 3.83439\n",
      "epoch no.0 train no.55780  loss = 4.22195 avg_loss = 3.82195\n",
      "epoch no.0 train no.55790  loss = 4.98420 avg_loss = 3.82746\n",
      "epoch no.0 train no.55800  loss = 2.77700 avg_loss = 3.81004\n",
      "epoch no.0 train no.55810  loss = 2.97049 avg_loss = 3.76252\n",
      "epoch no.0 train no.55820  loss = 5.11450 avg_loss = 3.78430\n",
      "epoch no.0 train no.55830  loss = 4.30843 avg_loss = 3.81542\n",
      "epoch no.0 train no.55840  loss = 3.05820 avg_loss = 3.83641\n",
      "epoch no.0 train no.55850  loss = 4.95137 avg_loss = 3.84624\n",
      "epoch no.0 train no.55860  loss = 4.69364 avg_loss = 3.78737\n",
      "epoch no.0 train no.55870  loss = 3.37283 avg_loss = 3.78118\n",
      "epoch no.0 train no.55880  loss = 6.50309 avg_loss = 3.90947\n",
      "epoch no.0 train no.55890  loss = 6.90831 avg_loss = 3.93130\n",
      "epoch no.0 train no.55900  loss = 3.77097 avg_loss = 3.93590\n",
      "epoch no.0 train no.55910  loss = 2.67604 avg_loss = 3.88744\n",
      "epoch no.0 train no.55920  loss = 6.87827 avg_loss = 3.86476\n",
      "epoch no.0 train no.55930  loss = 2.61980 avg_loss = 3.85019\n",
      "epoch no.0 train no.55940  loss = 2.82219 avg_loss = 3.92280\n",
      "epoch no.0 train no.55950  loss = 3.41987 avg_loss = 3.90253\n",
      "epoch no.0 train no.55960  loss = 3.47186 avg_loss = 3.86193\n",
      "epoch no.0 train no.55970  loss = 2.79460 avg_loss = 3.87829\n",
      "epoch no.0 train no.55980  loss = 5.08720 avg_loss = 3.87721\n",
      "epoch no.0 train no.55990  loss = 2.34151 avg_loss = 3.82966\n",
      "epoch no.0 train no.56000  loss = 5.29350 avg_loss = 3.83760\n",
      "3\n",
      "to_tokens: ['▁비', '▁발라', '곡', '들', '</s>']\n",
      "추억의 명곡들</s>\n",
      "epoch no.0 train no.56010  loss = 1.47227 avg_loss = 3.80610\n",
      "epoch no.0 train no.56020  loss = 5.62117 avg_loss = 3.86895\n",
      "epoch no.0 train no.56030  loss = 4.16531 avg_loss = 3.84295\n",
      "epoch no.0 train no.56040  loss = 5.04760 avg_loss = 3.91087\n",
      "epoch no.0 train no.56050  loss = 4.68140 avg_loss = 3.87551\n",
      "epoch no.0 train no.56060  loss = 4.78738 avg_loss = 3.84906\n",
      "epoch no.0 train no.56070  loss = 5.67730 avg_loss = 3.84895\n",
      "epoch no.0 train no.56080  loss = 4.33870 avg_loss = 3.82469\n",
      "epoch no.0 train no.56090  loss = 4.56793 avg_loss = 3.82360\n",
      "epoch no.0 train no.56100  loss = 2.94234 avg_loss = 3.77751\n",
      "epoch no.0 train no.56110  loss = 4.35569 avg_loss = 3.84037\n",
      "epoch no.0 train no.56120  loss = 3.42003 avg_loss = 3.84506\n",
      "epoch no.0 train no.56130  loss = 5.06371 avg_loss = 3.77102\n",
      "epoch no.0 train no.56140  loss = 3.11298 avg_loss = 3.74646\n",
      "epoch no.0 train no.56150  loss = 2.49799 avg_loss = 3.72632\n",
      "epoch no.0 train no.56160  loss = 3.23214 avg_loss = 3.77997\n",
      "epoch no.0 train no.56170  loss = 5.74333 avg_loss = 3.80051\n",
      "epoch no.0 train no.56180  loss = 4.65539 avg_loss = 3.80341\n",
      "epoch no.0 train no.56190  loss = 3.17869 avg_loss = 3.80609\n",
      "epoch no.0 train no.56200  loss = 4.77166 avg_loss = 3.84962\n",
      "epoch no.0 train no.56210  loss = 3.96707 avg_loss = 3.86297\n",
      "epoch no.0 train no.56220  loss = 6.38085 avg_loss = 3.94756\n",
      "epoch no.0 train no.56230  loss = 4.49685 avg_loss = 3.91828\n",
      "epoch no.0 train no.56240  loss = 2.87450 avg_loss = 3.84527\n",
      "epoch no.0 train no.56250  loss = 3.50235 avg_loss = 3.90653\n",
      "epoch no.0 train no.56260  loss = 3.65748 avg_loss = 3.90524\n",
      "epoch no.0 train no.56270  loss = 6.35102 avg_loss = 3.89664\n",
      "epoch no.0 train no.56280  loss = 3.38944 avg_loss = 3.88710\n",
      "epoch no.0 train no.56290  loss = 3.01556 avg_loss = 3.93374\n",
      "epoch no.0 train no.56300  loss = 4.12494 avg_loss = 3.91999\n",
      "epoch no.0 train no.56310  loss = 4.10271 avg_loss = 3.90359\n",
      "epoch no.0 train no.56320  loss = 3.40153 avg_loss = 3.92420\n",
      "epoch no.0 train no.56330  loss = 2.54415 avg_loss = 3.92891\n",
      "epoch no.0 train no.56340  loss = 2.84322 avg_loss = 3.93623\n",
      "epoch no.0 train no.56350  loss = 2.05041 avg_loss = 3.91150\n",
      "epoch no.0 train no.56360  loss = 3.34634 avg_loss = 3.92271\n",
      "epoch no.0 train no.56370  loss = 2.86299 avg_loss = 3.86138\n",
      "epoch no.0 train no.56380  loss = 5.32925 avg_loss = 3.86884\n",
      "epoch no.0 train no.56390  loss = 3.24854 avg_loss = 3.87916\n",
      "epoch no.0 train no.56400  loss = 3.43754 avg_loss = 3.80722\n",
      "epoch no.0 train no.56410  loss = 2.53912 avg_loss = 3.84292\n",
      "epoch no.0 train no.56420  loss = 2.58372 avg_loss = 3.87197\n",
      "epoch no.0 train no.56430  loss = 2.76366 avg_loss = 3.89105\n",
      "epoch no.0 train no.56440  loss = 3.09226 avg_loss = 3.87823\n",
      "epoch no.0 train no.56450  loss = 4.09341 avg_loss = 3.88398\n",
      "epoch no.0 train no.56460  loss = 4.21726 avg_loss = 3.86501\n",
      "epoch no.0 train no.56470  loss = 3.82694 avg_loss = 3.87248\n",
      "epoch no.0 train no.56480  loss = 2.84659 avg_loss = 3.90295\n",
      "epoch no.0 train no.56490  loss = 2.11551 avg_loss = 3.85183\n",
      "epoch no.0 train no.56500  loss = 3.44011 avg_loss = 3.84766\n",
      "epoch no.0 train no.56510  loss = 6.86857 avg_loss = 3.88934\n",
      "epoch no.0 train no.56520  loss = 4.80760 avg_loss = 3.94367\n",
      "epoch no.0 train no.56530  loss = 5.60235 avg_loss = 3.98749\n",
      "epoch no.0 train no.56540  loss = 5.26115 avg_loss = 4.02026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.56550  loss = 5.17207 avg_loss = 4.11489\n",
      "epoch no.0 train no.56560  loss = 3.98113 avg_loss = 4.10228\n",
      "epoch no.0 train no.56570  loss = 3.25334 avg_loss = 4.10729\n",
      "epoch no.0 train no.56580  loss = 4.39397 avg_loss = 4.10883\n",
      "epoch no.0 train no.56590  loss = 3.80346 avg_loss = 4.09989\n",
      "epoch no.0 train no.56600  loss = 3.73642 avg_loss = 4.12308\n",
      "epoch no.0 train no.56610  loss = 5.24517 avg_loss = 4.17709\n",
      "epoch no.0 train no.56620  loss = 2.39282 avg_loss = 4.15893\n",
      "epoch no.0 train no.56630  loss = 5.46449 avg_loss = 4.17532\n",
      "epoch no.0 train no.56640  loss = 4.49375 avg_loss = 4.14196\n",
      "epoch no.0 train no.56650  loss = 3.58582 avg_loss = 4.15794\n",
      "epoch no.0 train no.56660  loss = 5.59081 avg_loss = 4.13547\n",
      "epoch no.0 train no.56670  loss = 3.11302 avg_loss = 4.08341\n",
      "epoch no.0 train no.56680  loss = 3.50904 avg_loss = 4.07180\n",
      "epoch no.0 train no.56690  loss = 4.91691 avg_loss = 4.10619\n",
      "epoch no.0 train no.56700  loss = 3.38122 avg_loss = 4.03695\n",
      "epoch no.0 train no.56710  loss = 3.25926 avg_loss = 3.98600\n",
      "epoch no.0 train no.56720  loss = 3.84326 avg_loss = 3.97539\n",
      "epoch no.0 train no.56730  loss = 2.81707 avg_loss = 3.92122\n",
      "epoch no.0 train no.56740  loss = 3.10293 avg_loss = 3.97684\n",
      "epoch no.0 train no.56750  loss = 2.30384 avg_loss = 3.93314\n",
      "epoch no.0 train no.56760  loss = 2.89798 avg_loss = 3.93415\n",
      "epoch no.0 train no.56770  loss = 3.43430 avg_loss = 4.00109\n",
      "epoch no.0 train no.56780  loss = 5.77463 avg_loss = 3.98680\n",
      "epoch no.0 train no.56790  loss = 2.42969 avg_loss = 3.97597\n",
      "epoch no.0 train no.56800  loss = 6.76551 avg_loss = 4.01669\n",
      "epoch no.0 train no.56810  loss = 4.07442 avg_loss = 4.00716\n",
      "epoch no.0 train no.56820  loss = 4.62821 avg_loss = 3.94732\n",
      "epoch no.0 train no.56830  loss = 4.58548 avg_loss = 3.93964\n",
      "epoch no.0 train no.56840  loss = 3.41399 avg_loss = 3.95592\n",
      "epoch no.0 train no.56850  loss = 3.04720 avg_loss = 3.95618\n",
      "epoch no.0 train no.56860  loss = 3.18963 avg_loss = 3.95872\n",
      "epoch no.0 train no.56870  loss = 5.49757 avg_loss = 3.92324\n",
      "epoch no.0 train no.56880  loss = 2.03787 avg_loss = 3.94094\n",
      "epoch no.0 train no.56890  loss = 2.65494 avg_loss = 3.94760\n",
      "epoch no.0 train no.56900  loss = 2.61340 avg_loss = 3.92641\n",
      "epoch no.0 train no.56910  loss = 3.13108 avg_loss = 3.88269\n",
      "epoch no.0 train no.56920  loss = 3.33999 avg_loss = 3.87867\n",
      "epoch no.0 train no.56930  loss = 4.72874 avg_loss = 3.87815\n",
      "epoch no.0 train no.56940  loss = 3.78838 avg_loss = 3.87461\n",
      "epoch no.0 train no.56950  loss = 3.72356 avg_loss = 3.85428\n",
      "epoch no.0 train no.56960  loss = 4.90643 avg_loss = 3.82743\n",
      "epoch no.0 train no.56970  loss = 3.84333 avg_loss = 3.86380\n",
      "epoch no.0 train no.56980  loss = 3.06473 avg_loss = 3.83022\n",
      "epoch no.0 train no.56990  loss = 3.87012 avg_loss = 3.78722\n",
      "epoch no.0 train no.57000  loss = 3.63864 avg_loss = 3.84681\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '▁명', '곡', '</s>', '</s>']\n",
      "추억의 아이돌 명곡들</s>\n",
      "epoch no.0 train no.57010  loss = 4.83309 avg_loss = 3.86474\n",
      "epoch no.0 train no.57020  loss = 4.12696 avg_loss = 3.86525\n",
      "epoch no.0 train no.57030  loss = 3.66145 avg_loss = 3.88604\n",
      "epoch no.0 train no.57040  loss = 4.44673 avg_loss = 3.92872\n",
      "epoch no.0 train no.57050  loss = 3.31650 avg_loss = 3.89371\n",
      "epoch no.0 train no.57060  loss = 3.82000 avg_loss = 3.88580\n",
      "epoch no.0 train no.57070  loss = 7.59019 avg_loss = 3.93653\n",
      "epoch no.0 train no.57080  loss = 4.87560 avg_loss = 3.87896\n",
      "epoch no.0 train no.57090  loss = 3.55633 avg_loss = 3.84417\n",
      "epoch no.0 train no.57100  loss = 5.28307 avg_loss = 3.91653\n",
      "epoch no.0 train no.57110  loss = 3.65797 avg_loss = 3.93569\n",
      "epoch no.0 train no.57120  loss = 4.13741 avg_loss = 3.89948\n",
      "epoch no.0 train no.57130  loss = 5.32640 avg_loss = 3.95091\n",
      "epoch no.0 train no.57140  loss = 2.92960 avg_loss = 3.90836\n",
      "epoch no.0 train no.57150  loss = 6.78521 avg_loss = 3.91442\n",
      "epoch no.0 train no.57160  loss = 6.67991 avg_loss = 3.93274\n",
      "epoch no.0 train no.57170  loss = 3.11596 avg_loss = 3.92639\n",
      "epoch no.0 train no.57180  loss = 2.62157 avg_loss = 3.91235\n",
      "epoch no.0 train no.57190  loss = 3.27897 avg_loss = 3.92195\n",
      "epoch no.0 train no.57200  loss = 3.56420 avg_loss = 3.88548\n",
      "epoch no.0 train no.57210  loss = 5.91406 avg_loss = 3.96206\n",
      "epoch no.0 train no.57220  loss = 2.21024 avg_loss = 3.96540\n",
      "epoch no.0 train no.57230  loss = 4.61516 avg_loss = 3.91568\n",
      "epoch no.0 train no.57240  loss = 3.21675 avg_loss = 3.88863\n",
      "epoch no.0 train no.57250  loss = 2.36182 avg_loss = 3.87928\n",
      "epoch no.0 train no.57260  loss = 6.19501 avg_loss = 3.88931\n",
      "epoch no.0 train no.57270  loss = 2.37658 avg_loss = 3.82581\n",
      "epoch no.0 train no.57280  loss = 2.36649 avg_loss = 3.82641\n",
      "epoch no.0 train no.57290  loss = 3.21151 avg_loss = 3.82088\n",
      "epoch no.0 train no.57300  loss = 4.42100 avg_loss = 3.88835\n",
      "epoch no.0 train no.57310  loss = 5.21193 avg_loss = 3.88100\n",
      "epoch no.0 train no.57320  loss = 3.47029 avg_loss = 3.92402\n",
      "epoch no.0 train no.57330  loss = 4.38067 avg_loss = 3.99325\n",
      "epoch no.0 train no.57340  loss = 6.88137 avg_loss = 4.00219\n",
      "epoch no.0 train no.57350  loss = 4.68647 avg_loss = 4.01008\n",
      "epoch no.0 train no.57360  loss = 3.60189 avg_loss = 3.99056\n",
      "epoch no.0 train no.57370  loss = 3.01411 avg_loss = 4.09092\n",
      "epoch no.0 train no.57380  loss = 3.93861 avg_loss = 4.07128\n",
      "epoch no.0 train no.57390  loss = 6.83583 avg_loss = 4.16686\n",
      "epoch no.0 train no.57400  loss = 2.36321 avg_loss = 4.11706\n",
      "epoch no.0 train no.57410  loss = 3.10371 avg_loss = 4.11369\n",
      "epoch no.0 train no.57420  loss = 4.12430 avg_loss = 4.09530\n",
      "epoch no.0 train no.57430  loss = 2.04538 avg_loss = 4.11508\n",
      "epoch no.0 train no.57440  loss = 5.08671 avg_loss = 4.15416\n",
      "epoch no.0 train no.57450  loss = 3.65554 avg_loss = 4.14418\n",
      "epoch no.0 train no.57460  loss = 2.40915 avg_loss = 4.12322\n",
      "epoch no.0 train no.57470  loss = 3.26070 avg_loss = 4.06961\n",
      "epoch no.0 train no.57480  loss = 3.80625 avg_loss = 4.04876\n",
      "epoch no.0 train no.57490  loss = 4.96784 avg_loss = 4.03447\n",
      "epoch no.0 train no.57500  loss = 3.79197 avg_loss = 3.94160\n",
      "epoch no.0 train no.57510  loss = 2.50676 avg_loss = 3.88582\n",
      "epoch no.0 train no.57520  loss = 3.83582 avg_loss = 3.86318\n",
      "epoch no.0 train no.57530  loss = 4.49662 avg_loss = 3.84911\n",
      "epoch no.0 train no.57540  loss = 3.67163 avg_loss = 3.81034\n",
      "epoch no.0 train no.57550  loss = 5.88087 avg_loss = 3.87022\n",
      "epoch no.0 train no.57560  loss = 3.07249 avg_loss = 3.89135\n",
      "epoch no.0 train no.57570  loss = 4.01501 avg_loss = 3.95206\n",
      "epoch no.0 train no.57580  loss = 4.39069 avg_loss = 3.97261\n",
      "epoch no.0 train no.57590  loss = 4.77648 avg_loss = 3.98370\n",
      "epoch no.0 train no.57600  loss = 5.22149 avg_loss = 3.97740\n",
      "epoch no.0 train no.57610  loss = 2.09615 avg_loss = 3.94065\n",
      "epoch no.0 train no.57620  loss = 2.74746 avg_loss = 3.93110\n",
      "epoch no.0 train no.57630  loss = 4.40838 avg_loss = 3.89321\n",
      "epoch no.0 train no.57640  loss = 3.17606 avg_loss = 3.91402\n",
      "epoch no.0 train no.57650  loss = 3.90898 avg_loss = 3.86012\n",
      "epoch no.0 train no.57660  loss = 5.73082 avg_loss = 3.81403\n",
      "epoch no.0 train no.57670  loss = 4.11339 avg_loss = 3.78574\n",
      "epoch no.0 train no.57680  loss = 3.82171 avg_loss = 3.85294\n",
      "epoch no.0 train no.57690  loss = 6.53948 avg_loss = 3.89963\n",
      "epoch no.0 train no.57700  loss = 4.52896 avg_loss = 3.84154\n",
      "epoch no.0 train no.57710  loss = 2.45153 avg_loss = 3.80510\n",
      "epoch no.0 train no.57720  loss = 2.79766 avg_loss = 3.79910\n",
      "epoch no.0 train no.57730  loss = 3.05517 avg_loss = 3.78965\n",
      "epoch no.0 train no.57740  loss = 3.15877 avg_loss = 3.74842\n",
      "epoch no.0 train no.57750  loss = 2.90149 avg_loss = 3.68380\n",
      "epoch no.0 train no.57760  loss = 5.39617 avg_loss = 3.74744\n",
      "epoch no.0 train no.57770  loss = 2.21242 avg_loss = 3.78181\n",
      "epoch no.0 train no.57780  loss = 5.57107 avg_loss = 3.76315\n",
      "epoch no.0 train no.57790  loss = 3.66567 avg_loss = 3.78055\n",
      "epoch no.0 train no.57800  loss = 4.06486 avg_loss = 3.76310\n",
      "epoch no.0 train no.57810  loss = 3.86235 avg_loss = 3.70825\n",
      "epoch no.0 train no.57820  loss = 3.19117 avg_loss = 3.70161\n",
      "epoch no.0 train no.57830  loss = 2.27613 avg_loss = 3.68761\n",
      "epoch no.0 train no.57840  loss = 3.95670 avg_loss = 3.70496\n",
      "epoch no.0 train no.57850  loss = 4.70688 avg_loss = 3.74552\n",
      "epoch no.0 train no.57860  loss = 2.76925 avg_loss = 3.76553\n",
      "epoch no.0 train no.57870  loss = 3.25465 avg_loss = 3.75408\n",
      "epoch no.0 train no.57880  loss = 3.82046 avg_loss = 3.75352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.57890  loss = 3.29490 avg_loss = 3.77374\n",
      "epoch no.0 train no.57900  loss = 2.88098 avg_loss = 3.73926\n",
      "epoch no.0 train no.57910  loss = 3.27278 avg_loss = 3.76414\n",
      "epoch no.0 train no.57920  loss = 4.11546 avg_loss = 3.76854\n",
      "epoch no.0 train no.57930  loss = 2.78121 avg_loss = 3.75722\n",
      "epoch no.0 train no.57940  loss = 5.15549 avg_loss = 3.76723\n",
      "epoch no.0 train no.57950  loss = 4.46398 avg_loss = 3.76029\n",
      "epoch no.0 train no.57960  loss = 1.94397 avg_loss = 3.74221\n",
      "epoch no.0 train no.57970  loss = 3.42870 avg_loss = 3.71543\n",
      "epoch no.0 train no.57980  loss = 4.30003 avg_loss = 3.70368\n",
      "epoch no.0 train no.57990  loss = 2.91544 avg_loss = 3.67762\n",
      "epoch no.0 train no.58000  loss = 2.91704 avg_loss = 3.68628\n",
      "5\n",
      "to_tokens: ['▁가을', '▁명', '년대', '▁댄스', '▁댄스', '곡', '</s>']\n",
      "추억의 90년대 추억의 댄스곡</s>\n",
      "epoch no.0 train no.58010  loss = 3.16272 avg_loss = 3.66546\n",
      "epoch no.0 train no.58020  loss = 3.06757 avg_loss = 3.63288\n",
      "epoch no.0 train no.58030  loss = 2.41757 avg_loss = 3.62637\n",
      "epoch no.0 train no.58040  loss = 6.69751 avg_loss = 3.72521\n",
      "epoch no.0 train no.58050  loss = 4.86912 avg_loss = 3.80483\n",
      "epoch no.0 train no.58060  loss = 3.09471 avg_loss = 3.78486\n",
      "epoch no.0 train no.58070  loss = 2.94884 avg_loss = 3.83594\n",
      "epoch no.0 train no.58080  loss = 2.82225 avg_loss = 3.81889\n",
      "epoch no.0 train no.58090  loss = 2.81707 avg_loss = 3.79441\n",
      "epoch no.0 train no.58100  loss = 3.98733 avg_loss = 3.87844\n",
      "epoch no.0 train no.58110  loss = 3.18370 avg_loss = 3.85245\n",
      "epoch no.0 train no.58120  loss = 4.77361 avg_loss = 3.85496\n",
      "epoch no.0 train no.58130  loss = 4.08636 avg_loss = 3.88382\n",
      "epoch no.0 train no.58140  loss = 4.61294 avg_loss = 3.88328\n",
      "epoch no.0 train no.58150  loss = 2.79591 avg_loss = 3.88530\n",
      "epoch no.0 train no.58160  loss = 4.54458 avg_loss = 3.94665\n",
      "epoch no.0 train no.58170  loss = 3.00823 avg_loss = 3.91398\n",
      "epoch no.0 train no.58180  loss = 3.15690 avg_loss = 3.87800\n",
      "epoch no.0 train no.58190  loss = 5.78730 avg_loss = 3.87073\n",
      "epoch no.0 train no.58200  loss = 2.58768 avg_loss = 3.82547\n",
      "epoch no.0 train no.58210  loss = 3.93683 avg_loss = 3.85178\n",
      "epoch no.0 train no.58220  loss = 6.48736 avg_loss = 3.87792\n",
      "epoch no.0 train no.58230  loss = 5.35039 avg_loss = 3.86919\n",
      "epoch no.0 train no.58240  loss = 3.15992 avg_loss = 3.91536\n",
      "epoch no.0 train no.58250  loss = 2.54230 avg_loss = 3.84369\n",
      "epoch no.0 train no.58260  loss = 4.52357 avg_loss = 3.76719\n",
      "epoch no.0 train no.58270  loss = 2.99895 avg_loss = 3.79074\n",
      "epoch no.0 train no.58280  loss = 3.53135 avg_loss = 3.75372\n",
      "epoch no.0 train no.58290  loss = 4.46140 avg_loss = 3.76835\n",
      "epoch no.0 train no.58300  loss = 1.80587 avg_loss = 3.70717\n",
      "epoch no.0 train no.58310  loss = 2.53154 avg_loss = 3.73277\n",
      "epoch no.0 train no.58320  loss = 4.34813 avg_loss = 3.76007\n",
      "epoch no.0 train no.58330  loss = 7.31803 avg_loss = 3.81666\n",
      "epoch no.0 train no.58340  loss = 1.60237 avg_loss = 3.84141\n",
      "epoch no.0 train no.58350  loss = 3.40512 avg_loss = 3.78995\n",
      "epoch no.0 train no.58360  loss = 2.31329 avg_loss = 3.79411\n",
      "epoch no.0 train no.58370  loss = 3.84324 avg_loss = 3.81568\n",
      "epoch no.0 train no.58380  loss = 5.41498 avg_loss = 3.85502\n",
      "epoch no.0 train no.58390  loss = 4.79047 avg_loss = 3.80456\n",
      "epoch no.0 train no.58400  loss = 4.22677 avg_loss = 3.86080\n",
      "epoch no.0 train no.58410  loss = 4.93819 avg_loss = 3.86577\n",
      "epoch no.0 train no.58420  loss = 2.75622 avg_loss = 3.82302\n",
      "epoch no.0 train no.58430  loss = 2.24529 avg_loss = 3.81097\n",
      "epoch no.0 train no.58440  loss = 3.99751 avg_loss = 3.79543\n",
      "epoch no.0 train no.58450  loss = 2.92537 avg_loss = 3.76131\n",
      "epoch no.0 train no.58460  loss = 4.53164 avg_loss = 3.75461\n",
      "epoch no.0 train no.58470  loss = 2.96626 avg_loss = 3.73669\n",
      "epoch no.0 train no.58480  loss = 6.61360 avg_loss = 3.75119\n",
      "epoch no.0 train no.58490  loss = 4.62297 avg_loss = 3.77873\n",
      "epoch no.0 train no.58500  loss = 2.88354 avg_loss = 3.81920\n",
      "epoch no.0 train no.58510  loss = 5.21936 avg_loss = 3.80886\n",
      "epoch no.0 train no.58520  loss = 4.71371 avg_loss = 3.77754\n",
      "epoch no.0 train no.58530  loss = 4.43490 avg_loss = 3.80478\n",
      "epoch no.0 train no.58540  loss = 2.70844 avg_loss = 3.84947\n",
      "epoch no.0 train no.58550  loss = 3.90543 avg_loss = 3.79725\n",
      "epoch no.0 train no.58560  loss = 3.10760 avg_loss = 3.83477\n",
      "epoch no.0 train no.58570  loss = 3.66428 avg_loss = 3.85519\n",
      "epoch no.0 train no.58580  loss = 4.02678 avg_loss = 3.85328\n",
      "epoch no.0 train no.58590  loss = 4.78625 avg_loss = 3.89737\n",
      "epoch no.0 train no.58600  loss = 2.99246 avg_loss = 3.85986\n",
      "epoch no.0 train no.58610  loss = 2.89238 avg_loss = 3.86264\n",
      "epoch no.0 train no.58620  loss = 3.59511 avg_loss = 3.89055\n",
      "epoch no.0 train no.58630  loss = 6.58522 avg_loss = 3.89368\n",
      "epoch no.0 train no.58640  loss = 5.59115 avg_loss = 3.89968\n",
      "epoch no.0 train no.58650  loss = 3.59900 avg_loss = 3.90284\n",
      "epoch no.0 train no.58660  loss = 6.85332 avg_loss = 3.89699\n",
      "epoch no.0 train no.58670  loss = 3.11638 avg_loss = 3.95635\n",
      "epoch no.0 train no.58680  loss = 1.68534 avg_loss = 3.98442\n",
      "epoch no.0 train no.58690  loss = 2.86075 avg_loss = 3.95864\n",
      "epoch no.0 train no.58700  loss = 3.89810 avg_loss = 3.92559\n",
      "epoch no.0 train no.58710  loss = 1.68650 avg_loss = 3.92195\n",
      "epoch no.0 train no.58720  loss = 2.59875 avg_loss = 3.86430\n",
      "epoch no.0 train no.58730  loss = 6.11653 avg_loss = 3.86948\n",
      "epoch no.0 train no.58740  loss = 2.99443 avg_loss = 3.88452\n",
      "epoch no.0 train no.58750  loss = 5.53802 avg_loss = 3.85101\n",
      "epoch no.0 train no.58760  loss = 3.48906 avg_loss = 3.84166\n",
      "epoch no.0 train no.58770  loss = 6.86604 avg_loss = 3.80503\n",
      "epoch no.0 train no.58780  loss = 4.40203 avg_loss = 3.86215\n",
      "epoch no.0 train no.58790  loss = 3.15330 avg_loss = 3.77392\n",
      "epoch no.0 train no.58800  loss = 3.89871 avg_loss = 3.85085\n",
      "epoch no.0 train no.58810  loss = 3.41397 avg_loss = 3.84740\n",
      "epoch no.0 train no.58820  loss = 4.27195 avg_loss = 3.88729\n",
      "epoch no.0 train no.58830  loss = 2.84136 avg_loss = 3.86522\n",
      "epoch no.0 train no.58840  loss = 5.52176 avg_loss = 3.88955\n",
      "epoch no.0 train no.58850  loss = 3.52138 avg_loss = 3.86236\n",
      "epoch no.0 train no.58860  loss = 2.67917 avg_loss = 3.82042\n",
      "epoch no.0 train no.58870  loss = 5.33769 avg_loss = 3.85088\n",
      "epoch no.0 train no.58880  loss = 6.82513 avg_loss = 3.88166\n",
      "epoch no.0 train no.58890  loss = 3.99719 avg_loss = 3.88633\n",
      "epoch no.0 train no.58900  loss = 5.82689 avg_loss = 3.86701\n",
      "epoch no.0 train no.58910  loss = 2.71669 avg_loss = 3.80464\n",
      "epoch no.0 train no.58920  loss = 4.83499 avg_loss = 3.83946\n",
      "epoch no.0 train no.58930  loss = 4.20500 avg_loss = 3.82940\n",
      "epoch no.0 train no.58940  loss = 3.26655 avg_loss = 3.89843\n",
      "epoch no.0 train no.58950  loss = 2.92904 avg_loss = 3.89462\n",
      "epoch no.0 train no.58960  loss = 3.44029 avg_loss = 3.93191\n",
      "epoch no.0 train no.58970  loss = 5.42428 avg_loss = 3.92261\n",
      "epoch no.0 train no.58980  loss = 4.39693 avg_loss = 3.92483\n",
      "epoch no.0 train no.58990  loss = 6.19563 avg_loss = 3.94859\n",
      "epoch no.0 train no.59000  loss = 3.40119 avg_loss = 4.01233\n",
      "6\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁댄스', '드', '</s>', '곡', '</s>']\n",
      "추억의 2000년대 발라드 명곡</s>\n",
      "epoch no.0 train no.59010  loss = 4.19672 avg_loss = 3.95922\n",
      "epoch no.0 train no.59020  loss = 4.53704 avg_loss = 3.95044\n",
      "epoch no.0 train no.59030  loss = 5.46926 avg_loss = 3.93018\n",
      "epoch no.0 train no.59040  loss = 3.98910 avg_loss = 3.94262\n",
      "epoch no.0 train no.59050  loss = 4.41598 avg_loss = 3.90072\n",
      "epoch no.0 train no.59060  loss = 3.07185 avg_loss = 3.88758\n",
      "epoch no.0 train no.59070  loss = 5.43345 avg_loss = 3.88619\n",
      "epoch no.0 train no.59080  loss = 4.37346 avg_loss = 3.92411\n",
      "epoch no.0 train no.59090  loss = 4.75038 avg_loss = 4.00179\n",
      "epoch no.0 train no.59100  loss = 3.86334 avg_loss = 4.06522\n",
      "epoch no.0 train no.59110  loss = 2.78376 avg_loss = 4.02721\n",
      "epoch no.0 train no.59120  loss = 4.46732 avg_loss = 4.02612\n",
      "epoch no.0 train no.59130  loss = 4.26529 avg_loss = 4.00536\n",
      "epoch no.0 train no.59140  loss = 3.69042 avg_loss = 4.01535\n",
      "epoch no.0 train no.59150  loss = 4.50682 avg_loss = 4.03363\n",
      "epoch no.0 train no.59160  loss = 3.01719 avg_loss = 4.02344\n",
      "epoch no.0 train no.59170  loss = 3.76215 avg_loss = 3.98661\n",
      "epoch no.0 train no.59180  loss = 3.86892 avg_loss = 4.02984\n",
      "epoch no.0 train no.59190  loss = 5.05234 avg_loss = 3.99426\n",
      "epoch no.0 train no.59200  loss = 2.25536 avg_loss = 4.02184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.59210  loss = 4.90684 avg_loss = 4.02011\n",
      "epoch no.0 train no.59220  loss = 2.47652 avg_loss = 4.01402\n",
      "epoch no.0 train no.59230  loss = 4.12687 avg_loss = 4.04333\n",
      "epoch no.0 train no.59240  loss = 5.36131 avg_loss = 4.03029\n",
      "epoch no.0 train no.59250  loss = 6.93311 avg_loss = 4.11986\n",
      "epoch no.0 train no.59260  loss = 6.20434 avg_loss = 4.20267\n",
      "epoch no.0 train no.59270  loss = 4.46200 avg_loss = 4.14747\n",
      "epoch no.0 train no.59280  loss = 4.35600 avg_loss = 4.13588\n",
      "epoch no.0 train no.59290  loss = 3.30402 avg_loss = 4.06832\n",
      "epoch no.0 train no.59300  loss = 3.02281 avg_loss = 4.04298\n",
      "epoch no.0 train no.59310  loss = 2.64771 avg_loss = 4.02008\n",
      "epoch no.0 train no.59320  loss = 2.53689 avg_loss = 3.99598\n",
      "epoch no.0 train no.59330  loss = 2.88373 avg_loss = 3.98254\n",
      "epoch no.0 train no.59340  loss = 4.93641 avg_loss = 3.98817\n",
      "epoch no.0 train no.59350  loss = 2.03963 avg_loss = 3.96011\n",
      "epoch no.0 train no.59360  loss = 3.19684 avg_loss = 3.96942\n",
      "epoch no.0 train no.59370  loss = 4.93127 avg_loss = 4.00044\n",
      "epoch no.0 train no.59380  loss = 3.27253 avg_loss = 4.00704\n",
      "epoch no.0 train no.59390  loss = 3.10556 avg_loss = 3.96920\n",
      "epoch no.0 train no.59400  loss = 4.16224 avg_loss = 3.86797\n",
      "epoch no.0 train no.59410  loss = 2.92598 avg_loss = 3.89080\n",
      "epoch no.0 train no.59420  loss = 3.54671 avg_loss = 3.91986\n",
      "epoch no.0 train no.59430  loss = 3.97432 avg_loss = 3.85577\n",
      "epoch no.0 train no.59440  loss = 4.13603 avg_loss = 3.83634\n",
      "epoch no.0 train no.59450  loss = 4.99053 avg_loss = 3.82772\n",
      "epoch no.0 train no.59460  loss = 3.18369 avg_loss = 3.81324\n",
      "epoch no.0 train no.59470  loss = 2.69289 avg_loss = 3.77971\n",
      "epoch no.0 train no.59480  loss = 3.33217 avg_loss = 3.77527\n",
      "epoch no.0 train no.59490  loss = 2.27824 avg_loss = 3.73842\n",
      "epoch no.0 train no.59500  loss = 3.65303 avg_loss = 3.74877\n",
      "epoch no.0 train no.59510  loss = 5.07669 avg_loss = 3.76169\n",
      "epoch no.0 train no.59520  loss = 4.16088 avg_loss = 3.78581\n",
      "epoch no.0 train no.59530  loss = 5.42723 avg_loss = 3.81031\n",
      "epoch no.0 train no.59540  loss = 5.77110 avg_loss = 3.79171\n",
      "epoch no.0 train no.59550  loss = 3.61824 avg_loss = 3.78146\n",
      "epoch no.0 train no.59560  loss = 3.90522 avg_loss = 3.89913\n",
      "epoch no.0 train no.59570  loss = 6.40736 avg_loss = 3.95979\n",
      "epoch no.0 train no.59580  loss = 3.71222 avg_loss = 3.95492\n",
      "epoch no.0 train no.59590  loss = 4.06283 avg_loss = 3.98788\n",
      "epoch no.0 train no.59600  loss = 4.49930 avg_loss = 3.96985\n",
      "epoch no.0 train no.59610  loss = 2.74346 avg_loss = 3.99744\n",
      "epoch no.0 train no.59620  loss = 3.82359 avg_loss = 3.96691\n",
      "epoch no.0 train no.59630  loss = 5.77749 avg_loss = 3.93283\n",
      "epoch no.0 train no.59640  loss = 3.16926 avg_loss = 3.93626\n",
      "epoch no.0 train no.59650  loss = 2.56020 avg_loss = 3.92814\n",
      "epoch no.0 train no.59660  loss = 3.71654 avg_loss = 4.00485\n",
      "epoch no.0 train no.59670  loss = 3.81530 avg_loss = 3.98763\n",
      "epoch no.0 train no.59680  loss = 5.50148 avg_loss = 4.01150\n",
      "epoch no.0 train no.59690  loss = 1.87572 avg_loss = 3.97043\n",
      "epoch no.0 train no.59700  loss = 1.74651 avg_loss = 3.92540\n",
      "epoch no.0 train no.59710  loss = 2.60810 avg_loss = 3.90124\n",
      "epoch no.0 train no.59720  loss = 4.91662 avg_loss = 3.92133\n",
      "epoch no.0 train no.59730  loss = 3.02853 avg_loss = 3.84562\n",
      "epoch no.0 train no.59740  loss = 2.22935 avg_loss = 3.81558\n",
      "epoch no.0 train no.59750  loss = 4.97555 avg_loss = 3.82688\n",
      "epoch no.0 train no.59760  loss = 5.26300 avg_loss = 3.81307\n",
      "epoch no.0 train no.59770  loss = 3.86216 avg_loss = 3.79187\n",
      "epoch no.0 train no.59780  loss = 2.44758 avg_loss = 3.79123\n",
      "epoch no.0 train no.59790  loss = 3.99882 avg_loss = 3.77112\n",
      "epoch no.0 train no.59800  loss = 4.74536 avg_loss = 3.79163\n",
      "epoch no.0 train no.59810  loss = 3.65072 avg_loss = 3.79631\n",
      "epoch no.0 train no.59820  loss = 5.49621 avg_loss = 3.86976\n",
      "epoch no.0 train no.59830  loss = 2.14827 avg_loss = 3.83721\n",
      "epoch no.0 train no.59840  loss = 2.29619 avg_loss = 3.78734\n",
      "epoch no.0 train no.59850  loss = 4.44271 avg_loss = 3.80189\n",
      "epoch no.0 train no.59860  loss = 3.44170 avg_loss = 3.78164\n",
      "epoch no.0 train no.59870  loss = 3.36385 avg_loss = 3.77312\n",
      "epoch no.0 train no.59880  loss = 3.36913 avg_loss = 3.78045\n",
      "epoch no.0 train no.59890  loss = 4.14574 avg_loss = 3.79409\n",
      "epoch no.0 train no.59900  loss = 2.68965 avg_loss = 3.80552\n",
      "epoch no.0 train no.59910  loss = 3.78209 avg_loss = 3.74741\n",
      "epoch no.0 train no.59920  loss = 7.19888 avg_loss = 3.81522\n",
      "epoch no.0 train no.59930  loss = 3.40110 avg_loss = 3.75434\n",
      "epoch no.0 train no.59940  loss = 3.35651 avg_loss = 3.68616\n",
      "epoch no.0 train no.59950  loss = 3.16332 avg_loss = 3.71123\n",
      "epoch no.0 train no.59960  loss = 3.34024 avg_loss = 3.75506\n",
      "epoch no.0 train no.59970  loss = 6.28363 avg_loss = 3.77976\n",
      "epoch no.0 train no.59980  loss = 5.36582 avg_loss = 3.74655\n",
      "epoch no.0 train no.59990  loss = 2.91187 avg_loss = 3.68748\n",
      "epoch no.0 train no.60000  loss = 2.90757 avg_loss = 3.70059\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '곡', '▁팝', '집', '</s>']\n",
      "추억의 명곡 모음집</s>\n",
      "epoch no.0 train no.60010  loss = 6.37997 avg_loss = 3.78393\n",
      "epoch no.0 train no.60020  loss = 3.08811 avg_loss = 3.75342\n",
      "epoch no.0 train no.60030  loss = 3.41931 avg_loss = 3.76162\n",
      "epoch no.0 train no.60040  loss = 3.29309 avg_loss = 3.81864\n",
      "epoch no.0 train no.60050  loss = 4.65903 avg_loss = 3.77071\n",
      "epoch no.0 train no.60060  loss = 2.62959 avg_loss = 3.77128\n",
      "epoch no.0 train no.60070  loss = 3.50131 avg_loss = 3.74404\n",
      "epoch no.0 train no.60080  loss = 2.52529 avg_loss = 3.73338\n",
      "epoch no.0 train no.60090  loss = 3.79466 avg_loss = 3.70619\n",
      "epoch no.0 train no.60100  loss = 3.55646 avg_loss = 3.70459\n",
      "epoch no.0 train no.60110  loss = 2.38913 avg_loss = 3.71218\n",
      "epoch no.0 train no.60120  loss = 4.17656 avg_loss = 3.69997\n",
      "epoch no.0 train no.60130  loss = 3.78764 avg_loss = 3.65584\n",
      "epoch no.0 train no.60140  loss = 4.08810 avg_loss = 3.67566\n",
      "epoch no.0 train no.60150  loss = 3.71408 avg_loss = 3.64637\n",
      "epoch no.0 train no.60160  loss = 1.47934 avg_loss = 3.58068\n",
      "epoch no.0 train no.60170  loss = 3.27137 avg_loss = 3.56213\n",
      "epoch no.0 train no.60180  loss = 3.98719 avg_loss = 3.54887\n",
      "epoch no.0 train no.60190  loss = 2.62871 avg_loss = 3.55393\n",
      "epoch no.0 train no.60200  loss = 4.05378 avg_loss = 3.56709\n",
      "epoch no.0 train no.60210  loss = 4.91072 avg_loss = 3.68526\n",
      "epoch no.0 train no.60220  loss = 3.64910 avg_loss = 3.70989\n",
      "epoch no.0 train no.60230  loss = 3.23159 avg_loss = 3.66023\n",
      "epoch no.0 train no.60240  loss = 3.58699 avg_loss = 3.75164\n",
      "epoch no.0 train no.60250  loss = 6.28575 avg_loss = 3.79498\n",
      "epoch no.0 train no.60260  loss = 3.78164 avg_loss = 3.80952\n",
      "epoch no.0 train no.60270  loss = 3.03119 avg_loss = 3.78225\n",
      "epoch no.0 train no.60280  loss = 3.65494 avg_loss = 3.84542\n",
      "epoch no.0 train no.60290  loss = 3.49013 avg_loss = 3.88931\n",
      "epoch no.0 train no.60300  loss = 2.04691 avg_loss = 3.88452\n",
      "epoch no.0 train no.60310  loss = 2.52029 avg_loss = 3.84923\n",
      "epoch no.0 train no.60320  loss = 2.70520 avg_loss = 3.84087\n",
      "epoch no.0 train no.60330  loss = 5.80955 avg_loss = 3.89296\n",
      "epoch no.0 train no.60340  loss = 4.38236 avg_loss = 3.85193\n",
      "epoch no.0 train no.60350  loss = 5.16141 avg_loss = 3.86139\n",
      "epoch no.0 train no.60360  loss = 3.03942 avg_loss = 3.81951\n",
      "epoch no.0 train no.60370  loss = 2.21592 avg_loss = 3.79999\n",
      "epoch no.0 train no.60380  loss = 5.72894 avg_loss = 3.83672\n",
      "epoch no.0 train no.60390  loss = 3.55136 avg_loss = 3.79817\n",
      "epoch no.0 train no.60400  loss = 6.08433 avg_loss = 3.82191\n",
      "epoch no.0 train no.60410  loss = 5.17298 avg_loss = 3.80097\n",
      "epoch no.0 train no.60420  loss = 6.52450 avg_loss = 3.85245\n",
      "epoch no.0 train no.60430  loss = 2.64580 avg_loss = 3.82547\n",
      "epoch no.0 train no.60440  loss = 6.17745 avg_loss = 3.87489\n",
      "epoch no.0 train no.60450  loss = 3.68604 avg_loss = 3.87369\n",
      "epoch no.0 train no.60460  loss = 4.11486 avg_loss = 3.94792\n",
      "epoch no.0 train no.60470  loss = 4.61708 avg_loss = 3.93896\n",
      "epoch no.0 train no.60480  loss = 3.48396 avg_loss = 3.97650\n",
      "epoch no.0 train no.60490  loss = 2.78486 avg_loss = 3.89968\n",
      "epoch no.0 train no.60500  loss = 3.47047 avg_loss = 3.85015\n",
      "epoch no.0 train no.60510  loss = 3.53973 avg_loss = 3.88018\n",
      "epoch no.0 train no.60520  loss = 3.62001 avg_loss = 3.86165\n",
      "epoch no.0 train no.60530  loss = 3.09190 avg_loss = 3.82884\n",
      "epoch no.0 train no.60540  loss = 2.07995 avg_loss = 3.78904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.60550  loss = 4.92101 avg_loss = 3.78007\n",
      "epoch no.0 train no.60560  loss = 4.11550 avg_loss = 3.76782\n",
      "epoch no.0 train no.60570  loss = 6.08319 avg_loss = 3.80569\n",
      "epoch no.0 train no.60580  loss = 2.29169 avg_loss = 3.80743\n",
      "epoch no.0 train no.60590  loss = 4.24615 avg_loss = 3.76854\n",
      "epoch no.0 train no.60600  loss = 2.87471 avg_loss = 3.74219\n",
      "epoch no.0 train no.60610  loss = 4.34506 avg_loss = 3.82184\n",
      "epoch no.0 train no.60620  loss = 3.91209 avg_loss = 3.83168\n",
      "epoch no.0 train no.60630  loss = 5.50810 avg_loss = 3.85776\n",
      "epoch no.0 train no.60640  loss = 3.37001 avg_loss = 3.81153\n",
      "epoch no.0 train no.60650  loss = 4.34302 avg_loss = 3.81160\n",
      "epoch no.0 train no.60660  loss = 1.69037 avg_loss = 3.80333\n",
      "epoch no.0 train no.60670  loss = 5.78640 avg_loss = 3.81437\n",
      "epoch no.0 train no.60680  loss = 5.05951 avg_loss = 3.92070\n",
      "epoch no.0 train no.60690  loss = 2.50116 avg_loss = 3.83794\n",
      "epoch no.0 train no.60700  loss = 2.27010 avg_loss = 3.78862\n",
      "epoch no.0 train no.60710  loss = 4.85749 avg_loss = 3.83969\n",
      "epoch no.0 train no.60720  loss = 1.87790 avg_loss = 3.85356\n",
      "epoch no.0 train no.60730  loss = 5.46050 avg_loss = 3.87868\n",
      "epoch no.0 train no.60740  loss = 4.68759 avg_loss = 3.87492\n",
      "epoch no.0 train no.60750  loss = 2.91280 avg_loss = 3.84454\n",
      "epoch no.0 train no.60760  loss = 4.42904 avg_loss = 3.90083\n",
      "epoch no.0 train no.60770  loss = 2.28701 avg_loss = 3.88217\n",
      "epoch no.0 train no.60780  loss = 5.74281 avg_loss = 3.93597\n",
      "epoch no.0 train no.60790  loss = 6.80816 avg_loss = 3.92802\n",
      "epoch no.0 train no.60800  loss = 2.16377 avg_loss = 3.94103\n",
      "epoch no.0 train no.60810  loss = 5.15554 avg_loss = 3.93175\n",
      "epoch no.0 train no.60820  loss = 2.91476 avg_loss = 3.92170\n",
      "epoch no.0 train no.60830  loss = 5.32972 avg_loss = 3.92218\n",
      "epoch no.0 train no.60840  loss = 2.81364 avg_loss = 3.94276\n",
      "epoch no.0 train no.60850  loss = 4.17057 avg_loss = 3.92692\n",
      "epoch no.0 train no.60860  loss = 5.15084 avg_loss = 3.90341\n",
      "epoch no.0 train no.60870  loss = 3.62496 avg_loss = 3.88706\n",
      "epoch no.0 train no.60880  loss = 2.88718 avg_loss = 3.87893\n",
      "epoch no.0 train no.60890  loss = 6.37618 avg_loss = 3.87770\n",
      "epoch no.0 train no.60900  loss = 5.34827 avg_loss = 3.88690\n",
      "epoch no.0 train no.60910  loss = 3.81277 avg_loss = 3.88087\n",
      "epoch no.0 train no.60920  loss = 5.26008 avg_loss = 3.92898\n",
      "epoch no.0 train no.60930  loss = 2.92429 avg_loss = 3.92441\n",
      "epoch no.0 train no.60940  loss = 2.20813 avg_loss = 3.93402\n",
      "epoch no.0 train no.60950  loss = 4.87203 avg_loss = 3.98486\n",
      "epoch no.0 train no.60960  loss = 6.67541 avg_loss = 3.98012\n",
      "epoch no.0 train no.60970  loss = 3.39921 avg_loss = 3.97934\n",
      "epoch no.0 train no.60980  loss = 2.74049 avg_loss = 3.98653\n",
      "epoch no.0 train no.60990  loss = 2.76745 avg_loss = 4.04089\n",
      "epoch no.0 train no.61000  loss = 3.31278 avg_loss = 3.93360\n",
      "3\n",
      "to_tokens: ['▁비', '▁발라', '송', '▁모음', '</s>']\n",
      "추억의 팝송 베스트</s>\n",
      "epoch no.0 train no.61010  loss = 6.91156 avg_loss = 4.01143\n",
      "epoch no.0 train no.61020  loss = 2.77219 avg_loss = 3.97421\n",
      "epoch no.0 train no.61030  loss = 3.72932 avg_loss = 3.98324\n",
      "epoch no.0 train no.61040  loss = 3.96182 avg_loss = 3.97644\n",
      "epoch no.0 train no.61050  loss = 3.54166 avg_loss = 3.97641\n",
      "epoch no.0 train no.61060  loss = 3.38617 avg_loss = 3.98846\n",
      "epoch no.0 train no.61070  loss = 4.20253 avg_loss = 4.01912\n",
      "epoch no.0 train no.61080  loss = 5.94666 avg_loss = 3.99975\n",
      "epoch no.0 train no.61090  loss = 6.13171 avg_loss = 4.00418\n",
      "epoch no.0 train no.61100  loss = 2.42828 avg_loss = 3.94313\n",
      "epoch no.0 train no.61110  loss = 5.38192 avg_loss = 3.91569\n",
      "epoch no.0 train no.61120  loss = 4.65452 avg_loss = 3.97442\n",
      "epoch no.0 train no.61130  loss = 4.42785 avg_loss = 4.00419\n",
      "epoch no.0 train no.61140  loss = 2.57649 avg_loss = 4.01848\n",
      "epoch no.0 train no.61150  loss = 4.91620 avg_loss = 3.98576\n",
      "epoch no.0 train no.61160  loss = 4.35014 avg_loss = 3.99521\n",
      "epoch no.0 train no.61170  loss = 2.10019 avg_loss = 3.99784\n",
      "epoch no.0 train no.61180  loss = 3.30750 avg_loss = 3.94043\n",
      "epoch no.0 train no.61190  loss = 5.13272 avg_loss = 3.95307\n",
      "epoch no.0 train no.61200  loss = 4.05995 avg_loss = 3.91942\n",
      "epoch no.0 train no.61210  loss = 3.11289 avg_loss = 3.86194\n",
      "epoch no.0 train no.61220  loss = 3.74693 avg_loss = 3.90611\n",
      "epoch no.0 train no.61230  loss = 4.15782 avg_loss = 3.95807\n",
      "epoch no.0 train no.61240  loss = 4.38557 avg_loss = 3.98962\n",
      "epoch no.0 train no.61250  loss = 3.88959 avg_loss = 3.92702\n",
      "epoch no.0 train no.61260  loss = 5.96747 avg_loss = 3.99501\n",
      "epoch no.0 train no.61270  loss = 3.65221 avg_loss = 3.97361\n",
      "epoch no.0 train no.61280  loss = 4.64814 avg_loss = 3.95834\n",
      "epoch no.0 train no.61290  loss = 3.07031 avg_loss = 3.97081\n",
      "epoch no.0 train no.61300  loss = 2.13955 avg_loss = 3.90767\n",
      "epoch no.0 train no.61310  loss = 3.87652 avg_loss = 3.91915\n",
      "epoch no.0 train no.61320  loss = 5.45918 avg_loss = 3.91499\n",
      "epoch no.0 train no.61330  loss = 2.03875 avg_loss = 3.89431\n",
      "epoch no.0 train no.61340  loss = 2.89981 avg_loss = 3.85559\n",
      "epoch no.0 train no.61350  loss = 3.12907 avg_loss = 3.87159\n",
      "epoch no.0 train no.61360  loss = 4.40470 avg_loss = 3.92323\n",
      "epoch no.0 train no.61370  loss = 3.47330 avg_loss = 3.96001\n",
      "epoch no.0 train no.61380  loss = 3.92276 avg_loss = 3.94581\n",
      "epoch no.0 train no.61390  loss = 2.90571 avg_loss = 3.88431\n",
      "epoch no.0 train no.61400  loss = 2.58477 avg_loss = 3.84113\n",
      "epoch no.0 train no.61410  loss = 4.82310 avg_loss = 3.88653\n",
      "epoch no.0 train no.61420  loss = 5.09632 avg_loss = 3.90062\n",
      "epoch no.0 train no.61430  loss = 2.61966 avg_loss = 3.86543\n",
      "epoch no.0 train no.61440  loss = 4.12715 avg_loss = 3.86348\n",
      "epoch no.0 train no.61450  loss = 2.99716 avg_loss = 3.92763\n",
      "epoch no.0 train no.61460  loss = 5.07383 avg_loss = 3.91225\n",
      "epoch no.0 train no.61470  loss = 3.94133 avg_loss = 3.92288\n",
      "epoch no.0 train no.61480  loss = 3.41786 avg_loss = 3.87549\n",
      "epoch no.0 train no.61490  loss = 3.13759 avg_loss = 3.88618\n",
      "epoch no.0 train no.61500  loss = 4.58194 avg_loss = 3.90799\n",
      "epoch no.0 train no.61510  loss = 4.60682 avg_loss = 3.88939\n",
      "epoch no.0 train no.61520  loss = 3.01820 avg_loss = 3.87795\n",
      "epoch no.0 train no.61530  loss = 5.24928 avg_loss = 3.85786\n",
      "epoch no.0 train no.61540  loss = 3.41105 avg_loss = 3.85530\n",
      "epoch no.0 train no.61550  loss = 2.72256 avg_loss = 3.79730\n",
      "epoch no.0 train no.61560  loss = 2.40972 avg_loss = 3.85366\n",
      "epoch no.0 train no.61570  loss = 3.46010 avg_loss = 3.78054\n",
      "epoch no.0 train no.61580  loss = 3.75248 avg_loss = 3.81643\n",
      "epoch no.0 train no.61590  loss = 4.06547 avg_loss = 3.81149\n",
      "epoch no.0 train no.61600  loss = 2.58419 avg_loss = 3.77899\n",
      "epoch no.0 train no.61610  loss = 3.87510 avg_loss = 3.73297\n",
      "epoch no.0 train no.61620  loss = 4.94142 avg_loss = 3.74583\n",
      "epoch no.0 train no.61630  loss = 2.00500 avg_loss = 3.75263\n",
      "epoch no.0 train no.61640  loss = 5.62421 avg_loss = 3.84144\n",
      "epoch no.0 train no.61650  loss = 3.37671 avg_loss = 3.93183\n",
      "epoch no.0 train no.61660  loss = 5.13110 avg_loss = 3.93455\n",
      "epoch no.0 train no.61670  loss = 2.59326 avg_loss = 3.94501\n",
      "epoch no.0 train no.61680  loss = 2.87938 avg_loss = 3.89518\n",
      "epoch no.0 train no.61690  loss = 2.27556 avg_loss = 3.86561\n",
      "epoch no.0 train no.61700  loss = 5.59337 avg_loss = 3.91148\n",
      "epoch no.0 train no.61710  loss = 4.71287 avg_loss = 3.93912\n",
      "epoch no.0 train no.61720  loss = 6.48189 avg_loss = 3.92101\n",
      "epoch no.0 train no.61730  loss = 4.48359 avg_loss = 3.92254\n",
      "epoch no.0 train no.61740  loss = 3.52120 avg_loss = 3.94476\n",
      "epoch no.0 train no.61750  loss = 5.79203 avg_loss = 4.02172\n",
      "epoch no.0 train no.61760  loss = 3.20435 avg_loss = 4.01752\n",
      "epoch no.0 train no.61770  loss = 2.36070 avg_loss = 4.08901\n",
      "epoch no.0 train no.61780  loss = 3.12005 avg_loss = 4.07789\n",
      "epoch no.0 train no.61790  loss = 4.71279 avg_loss = 4.03297\n",
      "epoch no.0 train no.61800  loss = 3.02014 avg_loss = 4.02271\n",
      "epoch no.0 train no.61810  loss = 7.44546 avg_loss = 4.06574\n",
      "epoch no.0 train no.61820  loss = 1.94973 avg_loss = 4.05456\n",
      "epoch no.0 train no.61830  loss = 4.85085 avg_loss = 4.04876\n",
      "epoch no.0 train no.61840  loss = 2.01522 avg_loss = 4.01624\n",
      "epoch no.0 train no.61850  loss = 4.58848 avg_loss = 3.97708\n",
      "epoch no.0 train no.61860  loss = 4.35999 avg_loss = 3.94875\n",
      "epoch no.0 train no.61870  loss = 4.86087 avg_loss = 3.97747\n",
      "epoch no.0 train no.61880  loss = 5.01740 avg_loss = 3.97517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.61890  loss = 3.89229 avg_loss = 4.01799\n",
      "epoch no.0 train no.61900  loss = 4.57002 avg_loss = 4.02558\n",
      "epoch no.0 train no.61910  loss = 2.36580 avg_loss = 3.96060\n",
      "epoch no.0 train no.61920  loss = 3.32058 avg_loss = 3.92685\n",
      "epoch no.0 train no.61930  loss = 3.78727 avg_loss = 3.95392\n",
      "epoch no.0 train no.61940  loss = 4.38208 avg_loss = 3.91198\n",
      "epoch no.0 train no.61950  loss = 3.22539 avg_loss = 3.88481\n",
      "epoch no.0 train no.61960  loss = 4.68745 avg_loss = 3.94059\n",
      "epoch no.0 train no.61970  loss = 3.64652 avg_loss = 3.92995\n",
      "epoch no.0 train no.61980  loss = 4.20517 avg_loss = 3.96182\n",
      "epoch no.0 train no.61990  loss = 3.14098 avg_loss = 3.90855\n",
      "epoch no.0 train no.62000  loss = 4.07921 avg_loss = 3.95016\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁댄스', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.0 train no.62010  loss = 1.76887 avg_loss = 3.98286\n",
      "epoch no.0 train no.62020  loss = 2.63008 avg_loss = 3.96070\n",
      "epoch no.0 train no.62030  loss = 3.10513 avg_loss = 3.91279\n",
      "epoch no.0 train no.62040  loss = 3.50349 avg_loss = 3.93365\n",
      "epoch no.0 train no.62050  loss = 4.39213 avg_loss = 3.94930\n",
      "epoch no.0 train no.62060  loss = 5.80865 avg_loss = 4.00811\n",
      "epoch no.0 train no.62070  loss = 3.72939 avg_loss = 3.98630\n",
      "epoch no.0 train no.62080  loss = 1.78453 avg_loss = 3.94231\n",
      "epoch no.0 train no.62090  loss = 1.74334 avg_loss = 3.87548\n",
      "epoch no.0 train no.62100  loss = 2.06171 avg_loss = 3.84468\n",
      "epoch no.0 train no.62110  loss = 2.38832 avg_loss = 3.84724\n",
      "epoch no.0 train no.62120  loss = 3.60156 avg_loss = 3.81699\n",
      "epoch no.0 train no.62130  loss = 4.15732 avg_loss = 3.78984\n",
      "epoch no.0 train no.62140  loss = 4.90780 avg_loss = 3.81148\n",
      "epoch no.0 train no.62150  loss = 5.55367 avg_loss = 3.79702\n",
      "epoch no.0 train no.62160  loss = 2.56573 avg_loss = 3.80202\n",
      "epoch no.0 train no.62170  loss = 3.34173 avg_loss = 3.73644\n",
      "epoch no.0 train no.62180  loss = 2.32127 avg_loss = 3.75039\n",
      "epoch no.0 train no.62190  loss = 4.91455 avg_loss = 3.76708\n",
      "epoch no.0 train no.62200  loss = 2.73293 avg_loss = 3.76689\n",
      "epoch no.0 train no.62210  loss = 4.18530 avg_loss = 3.74595\n",
      "epoch no.0 train no.62220  loss = 4.19470 avg_loss = 3.76204\n",
      "epoch no.0 train no.62230  loss = 4.51817 avg_loss = 3.71777\n",
      "epoch no.0 train no.62240  loss = 2.27675 avg_loss = 3.71717\n",
      "epoch no.0 train no.62250  loss = 3.51082 avg_loss = 3.70450\n",
      "epoch no.0 train no.62260  loss = 4.22500 avg_loss = 3.71221\n",
      "epoch no.0 train no.62270  loss = 5.34083 avg_loss = 3.76534\n",
      "epoch no.0 train no.62280  loss = 3.12038 avg_loss = 3.79202\n",
      "epoch no.0 train no.62290  loss = 3.18174 avg_loss = 3.84030\n",
      "epoch no.0 train no.62300  loss = 2.38256 avg_loss = 3.78516\n",
      "epoch no.0 train no.62310  loss = 4.91478 avg_loss = 3.81406\n",
      "epoch no.0 train no.62320  loss = 3.38542 avg_loss = 3.83751\n",
      "epoch no.0 train no.62330  loss = 3.17705 avg_loss = 3.80935\n",
      "epoch no.0 train no.62340  loss = 4.56612 avg_loss = 3.91886\n",
      "epoch no.0 train no.62350  loss = 4.17103 avg_loss = 3.90757\n",
      "epoch no.0 train no.62360  loss = 3.24913 avg_loss = 3.87782\n",
      "epoch no.0 train no.62370  loss = 5.30043 avg_loss = 3.89080\n",
      "epoch no.0 train no.62380  loss = 4.16342 avg_loss = 3.85945\n",
      "epoch no.0 train no.62390  loss = 3.89666 avg_loss = 3.88472\n",
      "epoch no.0 train no.62400  loss = 2.76564 avg_loss = 3.84933\n",
      "epoch no.0 train no.62410  loss = 4.16952 avg_loss = 3.85723\n",
      "epoch no.0 train no.62420  loss = 3.00888 avg_loss = 3.80818\n",
      "epoch no.0 train no.62430  loss = 5.77202 avg_loss = 3.80858\n",
      "epoch no.0 train no.62440  loss = 5.35786 avg_loss = 3.79469\n",
      "epoch no.0 train no.62450  loss = 4.60313 avg_loss = 3.79324\n",
      "epoch no.0 train no.62460  loss = 2.94827 avg_loss = 3.79620\n",
      "epoch no.0 train no.62470  loss = 6.23994 avg_loss = 3.74801\n",
      "epoch no.0 train no.62480  loss = 2.41250 avg_loss = 3.77729\n",
      "epoch no.0 train no.62490  loss = 2.13501 avg_loss = 3.73459\n",
      "epoch no.0 train no.62500  loss = 4.23097 avg_loss = 3.74164\n",
      "epoch no.0 train no.62510  loss = 3.95661 avg_loss = 3.79987\n",
      "epoch no.0 train no.62520  loss = 4.52961 avg_loss = 3.86484\n",
      "epoch no.0 train no.62530  loss = 4.61046 avg_loss = 3.92135\n",
      "epoch no.0 train no.62540  loss = 7.36341 avg_loss = 3.97795\n",
      "epoch no.0 train no.62550  loss = 5.85014 avg_loss = 3.96024\n",
      "epoch no.0 train no.62560  loss = 5.90293 avg_loss = 3.98035\n",
      "epoch no.0 train no.62570  loss = 2.12707 avg_loss = 3.98395\n",
      "epoch no.0 train no.62580  loss = 5.39328 avg_loss = 3.99898\n",
      "epoch no.0 train no.62590  loss = 3.58748 avg_loss = 4.07276\n",
      "epoch no.0 train no.62600  loss = 5.17112 avg_loss = 4.05430\n",
      "epoch no.0 train no.62610  loss = 1.93735 avg_loss = 4.02741\n",
      "epoch no.0 train no.62620  loss = 5.08743 avg_loss = 4.02521\n",
      "epoch no.0 train no.62630  loss = 3.11975 avg_loss = 3.98633\n",
      "epoch no.0 train no.62640  loss = 4.32291 avg_loss = 3.94508\n",
      "epoch no.0 train no.62650  loss = 2.61157 avg_loss = 3.92481\n",
      "epoch no.0 train no.62660  loss = 5.10795 avg_loss = 3.96460\n",
      "epoch no.0 train no.62670  loss = 3.89319 avg_loss = 3.93488\n",
      "epoch no.0 train no.62680  loss = 4.75480 avg_loss = 3.97209\n",
      "epoch no.0 train no.62690  loss = 3.10451 avg_loss = 4.01326\n",
      "epoch no.0 train no.62700  loss = 3.16816 avg_loss = 3.99638\n",
      "epoch no.0 train no.62710  loss = 2.76492 avg_loss = 3.97359\n",
      "epoch no.0 train no.62720  loss = 5.99573 avg_loss = 3.98652\n",
      "epoch no.0 train no.62730  loss = 3.32495 avg_loss = 3.95595\n",
      "epoch no.0 train no.62740  loss = 4.09544 avg_loss = 3.94840\n",
      "epoch no.0 train no.62750  loss = 2.29947 avg_loss = 3.92695\n",
      "epoch no.0 train no.62760  loss = 3.77659 avg_loss = 3.88530\n",
      "epoch no.0 train no.62770  loss = 4.75219 avg_loss = 3.94715\n",
      "epoch no.0 train no.62780  loss = 4.18684 avg_loss = 3.93788\n",
      "epoch no.0 train no.62790  loss = 3.15861 avg_loss = 3.91094\n",
      "epoch no.0 train no.62800  loss = 2.90854 avg_loss = 3.90976\n",
      "epoch no.0 train no.62810  loss = 2.31748 avg_loss = 3.92404\n",
      "epoch no.0 train no.62820  loss = 2.49512 avg_loss = 3.91040\n",
      "epoch no.0 train no.62830  loss = 6.44392 avg_loss = 3.90735\n",
      "epoch no.0 train no.62840  loss = 6.38742 avg_loss = 3.87314\n",
      "epoch no.0 train no.62850  loss = 5.18107 avg_loss = 3.92401\n",
      "epoch no.0 train no.62860  loss = 2.20786 avg_loss = 3.92247\n",
      "epoch no.0 train no.62870  loss = 3.06604 avg_loss = 3.94446\n",
      "epoch no.0 train no.62880  loss = 4.58141 avg_loss = 3.96704\n",
      "epoch no.0 train no.62890  loss = 4.28126 avg_loss = 3.96076\n",
      "epoch no.0 train no.62900  loss = 6.57517 avg_loss = 3.91637\n",
      "epoch no.0 train no.62910  loss = 4.47273 avg_loss = 3.92252\n",
      "epoch no.0 train no.62920  loss = 4.51981 avg_loss = 3.91808\n",
      "epoch no.0 train no.62930  loss = 3.19952 avg_loss = 3.90576\n",
      "epoch no.0 train no.62940  loss = 4.00525 avg_loss = 3.88036\n",
      "epoch no.0 train no.62950  loss = 4.01789 avg_loss = 3.89085\n",
      "epoch no.0 train no.62960  loss = 4.28619 avg_loss = 3.85380\n",
      "epoch no.0 train no.62970  loss = 3.12736 avg_loss = 3.82070\n",
      "epoch no.0 train no.62980  loss = 2.26320 avg_loss = 3.81316\n",
      "epoch no.0 train no.62990  loss = 3.68849 avg_loss = 3.81950\n",
      "epoch no.0 train no.63000  loss = 4.61238 avg_loss = 3.80248\n",
      "5\n",
      "to_tokens: ['▁비', '▁발라', '곡', '들', '년대', '▁발라', '</s>']\n",
      "추억의 명곡 2000년대 가요</s>\n",
      "epoch no.0 train no.63010  loss = 3.46586 avg_loss = 3.80001\n",
      "epoch no.0 train no.63020  loss = 4.97830 avg_loss = 3.82076\n",
      "epoch no.0 train no.63030  loss = 3.54905 avg_loss = 3.86487\n",
      "epoch no.0 train no.63040  loss = 2.68022 avg_loss = 3.85667\n",
      "epoch no.0 train no.63050  loss = 6.26156 avg_loss = 3.89360\n",
      "epoch no.0 train no.63060  loss = 3.02592 avg_loss = 3.92896\n",
      "epoch no.0 train no.63070  loss = 2.58553 avg_loss = 3.92819\n",
      "epoch no.0 train no.63080  loss = 4.93921 avg_loss = 3.94621\n",
      "epoch no.0 train no.63090  loss = 3.33392 avg_loss = 3.89877\n",
      "epoch no.0 train no.63100  loss = 3.27618 avg_loss = 3.83852\n",
      "epoch no.0 train no.63110  loss = 5.17787 avg_loss = 3.92293\n",
      "epoch no.0 train no.63120  loss = 2.03184 avg_loss = 3.85326\n",
      "epoch no.0 train no.63130  loss = 2.22946 avg_loss = 3.86601\n",
      "epoch no.0 train no.63140  loss = 4.37209 avg_loss = 3.86624\n",
      "epoch no.0 train no.63150  loss = 3.95399 avg_loss = 3.85647\n",
      "epoch no.0 train no.63160  loss = 2.88341 avg_loss = 3.86680\n",
      "epoch no.0 train no.63170  loss = 4.97951 avg_loss = 3.84725\n",
      "epoch no.0 train no.63180  loss = 2.48357 avg_loss = 3.80341\n",
      "epoch no.0 train no.63190  loss = 4.07140 avg_loss = 3.74971\n",
      "epoch no.0 train no.63200  loss = 3.36162 avg_loss = 3.73598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.63210  loss = 3.64355 avg_loss = 3.75615\n",
      "epoch no.0 train no.63220  loss = 4.21919 avg_loss = 3.77605\n",
      "epoch no.0 train no.63230  loss = 3.69959 avg_loss = 3.81991\n",
      "epoch no.0 train no.63240  loss = 3.61519 avg_loss = 3.83812\n",
      "epoch no.0 train no.63250  loss = 5.31536 avg_loss = 3.85516\n",
      "epoch no.0 train no.63260  loss = 4.48240 avg_loss = 3.88495\n",
      "epoch no.0 train no.63270  loss = 3.93474 avg_loss = 3.85946\n",
      "epoch no.0 train no.63280  loss = 4.26685 avg_loss = 3.81999\n",
      "epoch no.0 train no.63290  loss = 2.52035 avg_loss = 3.82872\n",
      "epoch no.0 train no.63300  loss = 5.39141 avg_loss = 3.79472\n",
      "epoch no.0 train no.63310  loss = 6.55257 avg_loss = 3.82422\n",
      "epoch no.0 train no.63320  loss = 4.73867 avg_loss = 3.81992\n",
      "epoch no.0 train no.63330  loss = 2.44846 avg_loss = 3.77446\n",
      "epoch no.0 train no.63340  loss = 2.46243 avg_loss = 3.80390\n",
      "epoch no.0 train no.63350  loss = 4.93698 avg_loss = 3.81174\n",
      "epoch no.0 train no.63360  loss = 5.43112 avg_loss = 3.81682\n",
      "epoch no.0 train no.63370  loss = 2.70989 avg_loss = 3.75983\n",
      "epoch no.0 train no.63380  loss = 2.69697 avg_loss = 3.74600\n",
      "epoch no.0 train no.63390  loss = 4.18042 avg_loss = 3.76849\n",
      "epoch no.0 train no.63400  loss = 3.88117 avg_loss = 3.80948\n",
      "epoch no.0 train no.63410  loss = 3.18448 avg_loss = 3.78790\n",
      "epoch no.0 train no.63420  loss = 3.64919 avg_loss = 3.75169\n",
      "epoch no.0 train no.63430  loss = 2.82094 avg_loss = 3.74748\n",
      "epoch no.0 train no.63440  loss = 4.52414 avg_loss = 3.76539\n",
      "epoch no.0 train no.63450  loss = 5.05272 avg_loss = 3.80019\n",
      "epoch no.0 train no.63460  loss = 4.14623 avg_loss = 3.76611\n",
      "epoch no.0 train no.63470  loss = 4.70654 avg_loss = 3.81072\n",
      "epoch no.0 train no.63480  loss = 3.24156 avg_loss = 3.80976\n",
      "epoch no.0 train no.63490  loss = 4.31203 avg_loss = 3.80429\n",
      "epoch no.0 train no.63500  loss = 6.43516 avg_loss = 3.85021\n",
      "epoch no.0 train no.63510  loss = 2.10083 avg_loss = 3.82786\n",
      "epoch no.0 train no.63520  loss = 4.57243 avg_loss = 3.89446\n",
      "epoch no.0 train no.63530  loss = 6.32542 avg_loss = 3.97146\n",
      "epoch no.0 train no.63540  loss = 3.21415 avg_loss = 3.92789\n",
      "epoch no.0 train no.63550  loss = 4.45787 avg_loss = 3.94550\n",
      "epoch no.0 train no.63560  loss = 4.13953 avg_loss = 3.96878\n",
      "epoch no.0 train no.63570  loss = 1.79426 avg_loss = 3.95547\n",
      "epoch no.0 train no.63580  loss = 3.54818 avg_loss = 3.96287\n",
      "epoch no.0 train no.63590  loss = 4.12410 avg_loss = 3.95131\n",
      "epoch no.0 train no.63600  loss = 3.95828 avg_loss = 3.93335\n",
      "epoch no.0 train no.63610  loss = 6.05917 avg_loss = 3.95140\n",
      "epoch no.0 train no.63620  loss = 5.33371 avg_loss = 3.93851\n",
      "epoch no.0 train no.63630  loss = 5.70413 avg_loss = 4.00993\n",
      "epoch no.0 train no.63640  loss = 4.51099 avg_loss = 3.96630\n",
      "epoch no.0 train no.63650  loss = 2.55587 avg_loss = 4.01267\n",
      "epoch no.0 train no.63660  loss = 1.33950 avg_loss = 3.92177\n",
      "epoch no.0 train no.63670  loss = 5.76745 avg_loss = 3.93431\n",
      "epoch no.0 train no.63680  loss = 4.90065 avg_loss = 3.86741\n",
      "epoch no.0 train no.63690  loss = 5.62070 avg_loss = 3.90913\n",
      "epoch no.0 train no.63700  loss = 2.88230 avg_loss = 3.85035\n",
      "epoch no.0 train no.63710  loss = 4.76236 avg_loss = 3.90777\n",
      "epoch no.0 train no.63720  loss = 2.18769 avg_loss = 3.87604\n",
      "epoch no.0 train no.63730  loss = 2.87270 avg_loss = 3.87250\n",
      "epoch no.0 train no.63740  loss = 5.73073 avg_loss = 3.90910\n",
      "epoch no.0 train no.63750  loss = 2.38894 avg_loss = 3.89812\n",
      "epoch no.0 train no.63760  loss = 3.60728 avg_loss = 3.82499\n",
      "epoch no.0 train no.63770  loss = 6.42385 avg_loss = 3.81409\n",
      "epoch no.0 train no.63780  loss = 5.20947 avg_loss = 3.86100\n",
      "epoch no.0 train no.63790  loss = 3.02133 avg_loss = 3.81262\n",
      "epoch no.0 train no.63800  loss = 3.42019 avg_loss = 3.84290\n",
      "epoch no.0 train no.63810  loss = 3.17216 avg_loss = 3.81096\n",
      "epoch no.0 train no.63820  loss = 2.54825 avg_loss = 3.83958\n",
      "epoch no.0 train no.63830  loss = 3.45281 avg_loss = 3.80771\n",
      "epoch no.0 train no.63840  loss = 4.49755 avg_loss = 3.87638\n",
      "epoch no.0 train no.63850  loss = 2.19467 avg_loss = 3.80485\n",
      "epoch no.0 train no.63860  loss = 4.42766 avg_loss = 3.79263\n",
      "epoch no.0 train no.63870  loss = 4.65343 avg_loss = 3.85503\n",
      "epoch no.0 train no.63880  loss = 2.22652 avg_loss = 3.86317\n",
      "epoch no.0 train no.63890  loss = 4.68562 avg_loss = 3.92237\n",
      "epoch no.0 train no.63900  loss = 2.64745 avg_loss = 3.83624\n",
      "epoch no.0 train no.63910  loss = 4.27432 avg_loss = 3.91056\n",
      "epoch no.0 train no.63920  loss = 3.12734 avg_loss = 3.93539\n",
      "epoch no.0 train no.63930  loss = 4.07157 avg_loss = 3.92692\n",
      "epoch no.0 train no.63940  loss = 2.40521 avg_loss = 3.92127\n",
      "epoch no.0 train no.63950  loss = 3.61140 avg_loss = 3.98883\n",
      "epoch no.0 train no.63960  loss = 4.21175 avg_loss = 3.99982\n",
      "epoch no.0 train no.63970  loss = 5.10580 avg_loss = 3.98080\n",
      "epoch no.0 train no.63980  loss = 3.09304 avg_loss = 3.94178\n",
      "epoch no.0 train no.63990  loss = 4.67294 avg_loss = 4.00297\n",
      "epoch no.0 train no.64000  loss = 8.33665 avg_loss = 4.02331\n",
      "2\n",
      "to_tokens: ['▁비', '▁드라마', '드', '▁명']\n",
      "추억의 발라드</s>\n",
      "epoch no.0 train no.64010  loss = 2.76247 avg_loss = 3.98320\n",
      "epoch no.0 train no.64020  loss = 5.13547 avg_loss = 3.93869\n",
      "epoch no.0 train no.64030  loss = 2.49329 avg_loss = 3.99439\n",
      "epoch no.0 train no.64040  loss = 3.36180 avg_loss = 3.99243\n",
      "epoch no.0 train no.64050  loss = 4.05555 avg_loss = 3.95274\n",
      "epoch no.0 train no.64060  loss = 4.62297 avg_loss = 3.97407\n",
      "epoch no.0 train no.64070  loss = 3.28506 avg_loss = 3.95450\n",
      "epoch no.0 train no.64080  loss = 2.98709 avg_loss = 3.94111\n",
      "epoch no.0 train no.64090  loss = 3.23657 avg_loss = 3.98434\n",
      "epoch no.0 train no.64100  loss = 4.21434 avg_loss = 4.01764\n",
      "epoch no.0 train no.64110  loss = 2.85696 avg_loss = 4.00812\n",
      "epoch no.0 train no.64120  loss = 6.35646 avg_loss = 4.04379\n",
      "epoch no.0 train no.64130  loss = 3.81685 avg_loss = 4.02497\n",
      "epoch no.0 train no.64140  loss = 5.02960 avg_loss = 3.98096\n",
      "epoch no.0 train no.64150  loss = 3.43905 avg_loss = 4.06947\n",
      "epoch no.0 train no.64160  loss = 3.01647 avg_loss = 4.00137\n",
      "epoch no.0 train no.64170  loss = 5.00630 avg_loss = 3.94935\n",
      "epoch no.0 train no.64180  loss = 3.57138 avg_loss = 3.94356\n",
      "epoch no.0 train no.64190  loss = 2.70775 avg_loss = 3.87076\n",
      "epoch no.0 train no.64200  loss = 3.63276 avg_loss = 3.86795\n",
      "epoch no.0 train no.64210  loss = 3.90472 avg_loss = 3.84922\n",
      "epoch no.0 train no.64220  loss = 4.74048 avg_loss = 3.88022\n",
      "epoch no.0 train no.64230  loss = 2.14028 avg_loss = 3.85744\n",
      "epoch no.0 train no.64240  loss = 4.89734 avg_loss = 3.91098\n",
      "epoch no.0 train no.64250  loss = 4.23014 avg_loss = 3.93948\n",
      "epoch no.0 train no.64260  loss = 3.62141 avg_loss = 3.94337\n",
      "epoch no.0 train no.64270  loss = 2.65589 avg_loss = 3.88967\n",
      "epoch no.0 train no.64280  loss = 3.32122 avg_loss = 3.85751\n",
      "epoch no.0 train no.64290  loss = 3.69169 avg_loss = 3.87461\n",
      "epoch no.0 train no.64300  loss = 5.24803 avg_loss = 3.85103\n",
      "epoch no.0 train no.64310  loss = 1.49221 avg_loss = 3.80679\n",
      "epoch no.0 train no.64320  loss = 4.00248 avg_loss = 3.81232\n",
      "epoch no.0 train no.64330  loss = 2.46303 avg_loss = 3.75813\n",
      "epoch no.0 train no.64340  loss = 2.13789 avg_loss = 3.79824\n",
      "epoch no.0 train no.64350  loss = 4.37362 avg_loss = 3.75608\n",
      "epoch no.0 train no.64360  loss = 3.26931 avg_loss = 3.73362\n",
      "epoch no.0 train no.64370  loss = 2.24913 avg_loss = 3.69642\n",
      "epoch no.0 train no.64380  loss = 4.47647 avg_loss = 3.73490\n",
      "epoch no.0 train no.64390  loss = 5.24722 avg_loss = 3.73288\n",
      "epoch no.0 train no.64400  loss = 5.18439 avg_loss = 3.78171\n",
      "epoch no.0 train no.64410  loss = 3.05173 avg_loss = 3.80180\n",
      "epoch no.0 train no.64420  loss = 2.94716 avg_loss = 3.75177\n",
      "epoch no.0 train no.64430  loss = 2.75667 avg_loss = 3.77388\n",
      "epoch no.0 train no.64440  loss = 2.53861 avg_loss = 3.78482\n",
      "epoch no.0 train no.64450  loss = 3.91960 avg_loss = 3.74793\n",
      "epoch no.0 train no.64460  loss = 3.15020 avg_loss = 3.72218\n",
      "epoch no.0 train no.64470  loss = 1.86239 avg_loss = 3.73278\n",
      "epoch no.0 train no.64480  loss = 1.65930 avg_loss = 3.71380\n",
      "epoch no.0 train no.64490  loss = 4.00444 avg_loss = 3.68262\n",
      "epoch no.0 train no.64500  loss = 4.40172 avg_loss = 3.73412\n",
      "epoch no.0 train no.64510  loss = 3.96331 avg_loss = 3.74230\n",
      "epoch no.0 train no.64520  loss = 2.93321 avg_loss = 3.76380\n",
      "epoch no.0 train no.64530  loss = 2.53335 avg_loss = 3.75714\n",
      "epoch no.0 train no.64540  loss = 3.85511 avg_loss = 3.78090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.64550  loss = 4.43076 avg_loss = 3.77721\n",
      "epoch no.0 train no.64560  loss = 3.76481 avg_loss = 3.79462\n",
      "epoch no.0 train no.64570  loss = 5.41069 avg_loss = 3.77813\n",
      "epoch no.0 train no.64580  loss = 5.38906 avg_loss = 3.78903\n",
      "epoch no.0 train no.64590  loss = 3.45434 avg_loss = 3.83037\n",
      "epoch no.0 train no.64600  loss = 3.00882 avg_loss = 3.81489\n",
      "epoch no.0 train no.64610  loss = 4.73778 avg_loss = 3.84504\n",
      "epoch no.0 train no.64620  loss = 3.64732 avg_loss = 3.78660\n",
      "epoch no.0 train no.64630  loss = 2.00126 avg_loss = 3.74470\n",
      "epoch no.0 train no.64640  loss = 3.27442 avg_loss = 3.81180\n",
      "epoch no.0 train no.64650  loss = 7.27800 avg_loss = 3.89551\n",
      "epoch no.0 train no.64660  loss = 3.25378 avg_loss = 3.89451\n",
      "epoch no.0 train no.64670  loss = 2.20763 avg_loss = 3.82789\n",
      "epoch no.0 train no.64680  loss = 2.81586 avg_loss = 3.77094\n",
      "epoch no.0 train no.64690  loss = 2.62607 avg_loss = 3.73807\n",
      "epoch no.0 train no.64700  loss = 4.69222 avg_loss = 3.82997\n",
      "epoch no.0 train no.64710  loss = 3.61611 avg_loss = 3.84949\n",
      "epoch no.0 train no.64720  loss = 3.59936 avg_loss = 3.83520\n",
      "epoch no.0 train no.64730  loss = 4.01641 avg_loss = 3.86668\n",
      "epoch no.0 train no.64740  loss = 3.62626 avg_loss = 3.86626\n",
      "epoch no.0 train no.64750  loss = 3.72192 avg_loss = 3.82969\n",
      "epoch no.0 train no.64760  loss = 3.22653 avg_loss = 3.80900\n",
      "epoch no.0 train no.64770  loss = 3.47394 avg_loss = 3.78883\n",
      "epoch no.0 train no.64780  loss = 3.11730 avg_loss = 3.79696\n",
      "epoch no.0 train no.64790  loss = 3.52492 avg_loss = 3.79330\n",
      "epoch no.0 train no.64800  loss = 3.66006 avg_loss = 3.79745\n",
      "epoch no.0 train no.64810  loss = 6.20103 avg_loss = 3.83651\n",
      "epoch no.0 train no.64820  loss = 2.19392 avg_loss = 3.81623\n",
      "epoch no.0 train no.64830  loss = 3.52325 avg_loss = 3.86628\n",
      "epoch no.0 train no.64840  loss = 5.15906 avg_loss = 3.92369\n",
      "epoch no.0 train no.64850  loss = 7.02195 avg_loss = 3.97320\n",
      "epoch no.0 train no.64860  loss = 3.91902 avg_loss = 3.90662\n",
      "epoch no.0 train no.64870  loss = 5.83164 avg_loss = 3.88886\n",
      "epoch no.0 train no.64880  loss = 3.90687 avg_loss = 3.88587\n",
      "epoch no.0 train no.64890  loss = 4.47590 avg_loss = 3.88779\n",
      "epoch no.0 train no.64900  loss = 2.44011 avg_loss = 3.84529\n",
      "epoch no.0 train no.64910  loss = 3.14901 avg_loss = 3.81767\n",
      "epoch no.0 train no.64920  loss = 3.61323 avg_loss = 3.78151\n",
      "epoch no.0 train no.64930  loss = 6.88996 avg_loss = 3.83616\n",
      "epoch no.0 train no.64940  loss = 3.02966 avg_loss = 3.84149\n",
      "epoch no.0 train no.64950  loss = 3.03292 avg_loss = 3.81730\n",
      "epoch no.0 train no.64960  loss = 4.95040 avg_loss = 3.80228\n",
      "epoch no.0 train no.64970  loss = 4.53955 avg_loss = 3.78190\n",
      "epoch no.0 train no.64980  loss = 2.54305 avg_loss = 3.77829\n",
      "epoch no.0 train no.64990  loss = 3.59497 avg_loss = 3.77417\n",
      "epoch no.0 train no.65000  loss = 4.17517 avg_loss = 3.79396\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '곡', '들', '▁노래', '</s>']\n",
      "추억의 명곡 아이돌 노래</s>\n",
      "epoch no.0 train no.65010  loss = 2.08097 avg_loss = 3.79479\n",
      "epoch no.0 train no.65020  loss = 1.41261 avg_loss = 3.82717\n",
      "epoch no.0 train no.65030  loss = 2.94186 avg_loss = 3.79232\n",
      "epoch no.0 train no.65040  loss = 4.49100 avg_loss = 3.82567\n",
      "epoch no.0 train no.65050  loss = 3.59418 avg_loss = 3.84789\n",
      "epoch no.0 train no.65060  loss = 6.45954 avg_loss = 3.81796\n",
      "epoch no.0 train no.65070  loss = 2.90305 avg_loss = 3.78539\n",
      "epoch no.0 train no.65080  loss = 5.36896 avg_loss = 3.84252\n",
      "epoch no.0 train no.65090  loss = 3.70094 avg_loss = 3.82565\n",
      "epoch no.0 train no.65100  loss = 4.98086 avg_loss = 3.83353\n",
      "epoch no.0 train no.65110  loss = 2.51036 avg_loss = 3.76894\n",
      "epoch no.0 train no.65120  loss = 2.99459 avg_loss = 3.77215\n",
      "epoch no.0 train no.65130  loss = 6.34472 avg_loss = 3.81829\n",
      "epoch no.0 train no.65140  loss = 3.47416 avg_loss = 3.79820\n",
      "epoch no.0 train no.65150  loss = 3.49343 avg_loss = 3.83791\n",
      "epoch no.0 train no.65160  loss = 2.49436 avg_loss = 3.76756\n",
      "epoch no.0 train no.65170  loss = 6.23528 avg_loss = 3.80235\n",
      "epoch no.0 train no.65180  loss = 3.04365 avg_loss = 3.76104\n",
      "epoch no.0 train no.65190  loss = 5.02955 avg_loss = 3.75176\n",
      "epoch no.0 train no.65200  loss = 3.02838 avg_loss = 3.72294\n",
      "epoch no.0 train no.65210  loss = 4.91133 avg_loss = 3.72281\n",
      "epoch no.0 train no.65220  loss = 2.60763 avg_loss = 3.70707\n",
      "epoch no.0 train no.65230  loss = 4.22890 avg_loss = 3.77496\n",
      "epoch no.0 train no.65240  loss = 5.89750 avg_loss = 3.80741\n",
      "epoch no.0 train no.65250  loss = 6.27140 avg_loss = 3.77461\n",
      "epoch no.0 train no.65260  loss = 4.32396 avg_loss = 3.82492\n",
      "epoch no.0 train no.65270  loss = 5.27551 avg_loss = 3.87705\n",
      "epoch no.0 train no.65280  loss = 2.32918 avg_loss = 3.90152\n",
      "epoch no.0 train no.65290  loss = 3.72013 avg_loss = 3.91588\n",
      "epoch no.0 train no.65300  loss = 3.55455 avg_loss = 3.87285\n",
      "epoch no.0 train no.65310  loss = 2.83415 avg_loss = 3.84492\n",
      "epoch no.0 train no.65320  loss = 3.96171 avg_loss = 3.76912\n",
      "epoch no.0 train no.65330  loss = 3.63622 avg_loss = 3.81743\n",
      "epoch no.0 train no.65340  loss = 4.04253 avg_loss = 3.76528\n",
      "epoch no.0 train no.65350  loss = 4.89820 avg_loss = 3.76343\n",
      "epoch no.0 train no.65360  loss = 4.85043 avg_loss = 3.82581\n",
      "epoch no.0 train no.65370  loss = 6.78116 avg_loss = 3.84119\n",
      "epoch no.0 train no.65380  loss = 3.27449 avg_loss = 3.78645\n",
      "epoch no.0 train no.65390  loss = 4.92137 avg_loss = 3.83720\n",
      "epoch no.0 train no.65400  loss = 5.48754 avg_loss = 3.90957\n",
      "epoch no.0 train no.65410  loss = 2.29664 avg_loss = 3.87072\n",
      "epoch no.0 train no.65420  loss = 2.93090 avg_loss = 3.90038\n",
      "epoch no.0 train no.65430  loss = 4.36253 avg_loss = 3.91998\n",
      "epoch no.0 train no.65440  loss = 3.17563 avg_loss = 3.95438\n",
      "epoch no.0 train no.65450  loss = 4.52954 avg_loss = 3.96052\n",
      "epoch no.0 train no.65460  loss = 4.41672 avg_loss = 4.00626\n",
      "epoch no.0 train no.65470  loss = 3.26109 avg_loss = 3.95810\n",
      "epoch no.0 train no.65480  loss = 2.23048 avg_loss = 3.95798\n",
      "epoch no.0 train no.65490  loss = 2.27614 avg_loss = 3.91433\n",
      "epoch no.0 train no.65500  loss = 3.87116 avg_loss = 3.89127\n",
      "epoch no.0 train no.65510  loss = 3.65059 avg_loss = 3.88221\n",
      "epoch no.0 train no.65520  loss = 3.75556 avg_loss = 3.92743\n",
      "epoch no.0 train no.65530  loss = 1.57102 avg_loss = 3.82776\n",
      "epoch no.0 train no.65540  loss = 4.50549 avg_loss = 3.77944\n",
      "epoch no.0 train no.65550  loss = 4.14806 avg_loss = 3.86663\n",
      "epoch no.0 train no.65560  loss = 2.14078 avg_loss = 3.81373\n",
      "epoch no.0 train no.65570  loss = 3.38789 avg_loss = 3.85087\n",
      "epoch no.0 train no.65580  loss = 4.19337 avg_loss = 3.84560\n",
      "epoch no.0 train no.65590  loss = 3.55222 avg_loss = 3.84970\n",
      "epoch no.0 train no.65600  loss = 4.20459 avg_loss = 3.86650\n",
      "epoch no.0 train no.65610  loss = 4.12841 avg_loss = 3.85905\n",
      "epoch no.0 train no.65620  loss = 4.04152 avg_loss = 3.95572\n",
      "epoch no.0 train no.65630  loss = 4.56161 avg_loss = 3.94507\n",
      "epoch no.0 train no.65640  loss = 2.19465 avg_loss = 3.98490\n",
      "epoch no.0 train no.65650  loss = 3.00262 avg_loss = 3.95678\n",
      "epoch no.0 train no.65660  loss = 4.48192 avg_loss = 4.02122\n",
      "epoch no.0 train no.65670  loss = 4.80492 avg_loss = 4.04126\n",
      "epoch no.0 train no.65680  loss = 3.86827 avg_loss = 4.04239\n",
      "epoch no.0 train no.65690  loss = 3.59043 avg_loss = 4.05443\n",
      "epoch no.0 train no.65700  loss = 4.32436 avg_loss = 4.04493\n",
      "epoch no.0 train no.65710  loss = 2.04133 avg_loss = 4.01274\n",
      "epoch no.0 train no.65720  loss = 3.88602 avg_loss = 3.97695\n",
      "epoch no.0 train no.65730  loss = 3.46998 avg_loss = 3.90794\n",
      "epoch no.0 train no.65740  loss = 2.01160 avg_loss = 3.92435\n",
      "epoch no.0 train no.65750  loss = 1.67631 avg_loss = 3.85760\n",
      "epoch no.0 train no.65760  loss = 4.08227 avg_loss = 3.80133\n",
      "epoch no.0 train no.65770  loss = 3.36593 avg_loss = 3.77740\n",
      "epoch no.0 train no.65780  loss = 3.64051 avg_loss = 3.72361\n",
      "epoch no.0 train no.65790  loss = 3.74167 avg_loss = 3.66547\n",
      "epoch no.0 train no.65800  loss = 3.06092 avg_loss = 3.69822\n",
      "epoch no.0 train no.65810  loss = 5.92778 avg_loss = 3.78397\n",
      "epoch no.0 train no.65820  loss = 3.84044 avg_loss = 3.75777\n",
      "epoch no.0 train no.65830  loss = 3.83714 avg_loss = 3.77268\n",
      "epoch no.0 train no.65840  loss = 4.43822 avg_loss = 3.76659\n",
      "epoch no.0 train no.65850  loss = 2.57097 avg_loss = 3.79937\n",
      "epoch no.0 train no.65860  loss = 4.30826 avg_loss = 3.77423\n",
      "epoch no.0 train no.65870  loss = 4.24030 avg_loss = 3.80345\n",
      "epoch no.0 train no.65880  loss = 3.64554 avg_loss = 3.79980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.65890  loss = 1.82795 avg_loss = 3.82214\n",
      "epoch no.0 train no.65900  loss = 4.34742 avg_loss = 3.79850\n",
      "epoch no.0 train no.65910  loss = 3.05731 avg_loss = 3.82503\n",
      "epoch no.0 train no.65920  loss = 2.49708 avg_loss = 3.83499\n",
      "epoch no.0 train no.65930  loss = 5.49814 avg_loss = 3.84379\n",
      "epoch no.0 train no.65940  loss = 2.65458 avg_loss = 3.87693\n",
      "epoch no.0 train no.65950  loss = 3.48737 avg_loss = 3.83919\n",
      "epoch no.0 train no.65960  loss = 3.39618 avg_loss = 3.87316\n",
      "epoch no.0 train no.65970  loss = 4.25640 avg_loss = 3.84266\n",
      "epoch no.0 train no.65980  loss = 3.64463 avg_loss = 3.80716\n",
      "epoch no.0 train no.65990  loss = 3.51867 avg_loss = 3.86410\n",
      "epoch no.0 train no.66000  loss = 3.57676 avg_loss = 3.79308\n",
      "5\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁발라', '들', '음', '</s>']\n",
      "추억의 90년대 노래모음</s>\n",
      "epoch no.0 train no.66010  loss = 4.03004 avg_loss = 3.79950\n",
      "epoch no.0 train no.66020  loss = 2.15762 avg_loss = 3.76727\n",
      "epoch no.0 train no.66030  loss = 6.57922 avg_loss = 3.84465\n",
      "epoch no.0 train no.66040  loss = 4.20653 avg_loss = 3.83144\n",
      "epoch no.0 train no.66050  loss = 4.28253 avg_loss = 3.86784\n",
      "epoch no.0 train no.66060  loss = 4.15474 avg_loss = 3.88074\n",
      "epoch no.0 train no.66070  loss = 3.20213 avg_loss = 3.80960\n",
      "epoch no.0 train no.66080  loss = 2.44579 avg_loss = 3.82024\n",
      "epoch no.0 train no.66090  loss = 5.01998 avg_loss = 3.87171\n",
      "epoch no.0 train no.66100  loss = 5.44969 avg_loss = 3.93514\n",
      "epoch no.0 train no.66110  loss = 2.42107 avg_loss = 3.94226\n",
      "epoch no.0 train no.66120  loss = 5.49361 avg_loss = 3.96980\n",
      "epoch no.0 train no.66130  loss = 2.84403 avg_loss = 3.98505\n",
      "epoch no.0 train no.66140  loss = 3.46807 avg_loss = 3.99620\n",
      "epoch no.0 train no.66150  loss = 5.89212 avg_loss = 4.00699\n",
      "epoch no.0 train no.66160  loss = 3.79576 avg_loss = 4.05680\n",
      "epoch no.0 train no.66170  loss = 3.89460 avg_loss = 4.08619\n",
      "epoch no.0 train no.66180  loss = 5.35947 avg_loss = 4.04210\n",
      "epoch no.0 train no.66190  loss = 2.51664 avg_loss = 3.98601\n",
      "epoch no.0 train no.66200  loss = 3.49096 avg_loss = 3.97447\n",
      "epoch no.0 train no.66210  loss = 4.00140 avg_loss = 3.98934\n",
      "epoch no.0 train no.66220  loss = 3.59673 avg_loss = 3.91729\n",
      "epoch no.0 train no.66230  loss = 6.50801 avg_loss = 3.96263\n",
      "epoch no.0 train no.66240  loss = 3.83885 avg_loss = 3.91121\n",
      "epoch no.0 train no.66250  loss = 2.09739 avg_loss = 3.84477\n",
      "epoch no.0 train no.66260  loss = 2.96433 avg_loss = 3.81562\n",
      "epoch no.0 train no.66270  loss = 4.33671 avg_loss = 3.77213\n",
      "epoch no.0 train no.66280  loss = 2.94718 avg_loss = 3.74630\n",
      "epoch no.0 train no.66290  loss = 1.65079 avg_loss = 3.73913\n",
      "epoch no.0 train no.66300  loss = 2.65596 avg_loss = 3.80047\n",
      "epoch no.0 train no.66310  loss = 2.35551 avg_loss = 3.77747\n",
      "epoch no.0 train no.66320  loss = 4.73567 avg_loss = 3.75200\n",
      "epoch no.0 train no.66330  loss = 5.63523 avg_loss = 3.71792\n",
      "epoch no.0 train no.66340  loss = 2.00550 avg_loss = 3.66298\n",
      "epoch no.0 train no.66350  loss = 6.50770 avg_loss = 3.67503\n",
      "epoch no.0 train no.66360  loss = 3.71084 avg_loss = 3.61507\n",
      "epoch no.0 train no.66370  loss = 3.13831 avg_loss = 3.65641\n",
      "epoch no.0 train no.66380  loss = 3.87695 avg_loss = 3.66099\n",
      "epoch no.0 train no.66390  loss = 4.24243 avg_loss = 3.70877\n",
      "epoch no.0 train no.66400  loss = 4.79898 avg_loss = 3.73483\n",
      "epoch no.0 train no.66410  loss = 4.75213 avg_loss = 3.75094\n",
      "epoch no.0 train no.66420  loss = 2.68881 avg_loss = 3.79660\n",
      "epoch no.0 train no.66430  loss = 5.87224 avg_loss = 3.82562\n",
      "epoch no.0 train no.66440  loss = 3.31039 avg_loss = 3.90012\n",
      "epoch no.0 train no.66450  loss = 6.18830 avg_loss = 3.93230\n",
      "epoch no.0 train no.66460  loss = 4.45325 avg_loss = 3.95545\n",
      "epoch no.0 train no.66470  loss = 4.37195 avg_loss = 3.94837\n",
      "epoch no.0 train no.66480  loss = 2.28066 avg_loss = 3.90076\n",
      "epoch no.0 train no.66490  loss = 4.87525 avg_loss = 3.85493\n",
      "epoch no.0 train no.66500  loss = 5.21071 avg_loss = 3.92196\n",
      "epoch no.0 train no.66510  loss = 5.16630 avg_loss = 3.94173\n",
      "epoch no.0 train no.66520  loss = 4.84167 avg_loss = 3.93015\n",
      "epoch no.0 train no.66530  loss = 2.80302 avg_loss = 3.89129\n",
      "epoch no.0 train no.66540  loss = 3.17959 avg_loss = 3.85074\n",
      "epoch no.0 train no.66550  loss = 3.31123 avg_loss = 3.81086\n",
      "epoch no.0 train no.66560  loss = 2.80734 avg_loss = 3.80365\n",
      "epoch no.0 train no.66570  loss = 5.00378 avg_loss = 3.89650\n",
      "epoch no.0 train no.66580  loss = 6.85367 avg_loss = 3.95717\n",
      "epoch no.0 train no.66590  loss = 2.61728 avg_loss = 3.88919\n",
      "epoch no.0 train no.66600  loss = 4.15506 avg_loss = 3.84705\n",
      "epoch no.0 train no.66610  loss = 2.65645 avg_loss = 3.86252\n",
      "epoch no.0 train no.66620  loss = 3.87819 avg_loss = 3.85710\n",
      "epoch no.0 train no.66630  loss = 3.84383 avg_loss = 3.81484\n",
      "epoch no.0 train no.66640  loss = 3.12039 avg_loss = 3.82017\n",
      "epoch no.0 train no.66650  loss = 4.38612 avg_loss = 3.75568\n",
      "epoch no.0 train no.66660  loss = 3.06805 avg_loss = 3.77632\n",
      "epoch no.0 train no.66670  loss = 2.42627 avg_loss = 3.74094\n",
      "epoch no.0 train no.66680  loss = 4.62828 avg_loss = 3.81667\n",
      "epoch no.0 train no.66690  loss = 3.90096 avg_loss = 3.82720\n",
      "epoch no.0 train no.66700  loss = 1.88341 avg_loss = 3.81128\n",
      "epoch no.0 train no.66710  loss = 3.79138 avg_loss = 3.87534\n",
      "epoch no.0 train no.66720  loss = 3.64593 avg_loss = 3.84434\n",
      "epoch no.0 train no.66730  loss = 5.80107 avg_loss = 3.83219\n",
      "epoch no.0 train no.66740  loss = 5.65242 avg_loss = 3.84982\n",
      "epoch no.0 train no.66750  loss = 4.58574 avg_loss = 3.87015\n",
      "epoch no.0 train no.66760  loss = 5.64293 avg_loss = 3.90920\n",
      "epoch no.0 train no.66770  loss = 3.34184 avg_loss = 3.83076\n",
      "epoch no.0 train no.66780  loss = 2.21839 avg_loss = 3.87557\n",
      "epoch no.0 train no.66790  loss = 4.87034 avg_loss = 3.93015\n",
      "epoch no.0 train no.66800  loss = 5.56781 avg_loss = 3.95939\n",
      "epoch no.0 train no.66810  loss = 3.57312 avg_loss = 3.97085\n",
      "epoch no.0 train no.66820  loss = 6.68683 avg_loss = 3.93607\n",
      "epoch no.0 train no.66830  loss = 2.30647 avg_loss = 3.91196\n",
      "epoch no.0 train no.66840  loss = 5.91877 avg_loss = 3.96823\n",
      "epoch no.0 train no.66850  loss = 3.72108 avg_loss = 3.97522\n",
      "epoch no.0 train no.66860  loss = 3.79615 avg_loss = 3.92872\n",
      "epoch no.0 train no.66870  loss = 2.31377 avg_loss = 3.92197\n",
      "epoch no.0 train no.66880  loss = 1.84587 avg_loss = 3.92105\n",
      "epoch no.0 train no.66890  loss = 3.27808 avg_loss = 3.91083\n",
      "epoch no.0 train no.66900  loss = 4.05523 avg_loss = 3.93058\n",
      "epoch no.0 train no.66910  loss = 4.89333 avg_loss = 3.93661\n",
      "epoch no.0 train no.66920  loss = 4.39976 avg_loss = 3.91426\n",
      "epoch no.0 train no.66930  loss = 3.60060 avg_loss = 3.93503\n",
      "epoch no.0 train no.66940  loss = 4.68031 avg_loss = 3.96671\n",
      "epoch no.0 train no.66950  loss = 2.94793 avg_loss = 3.95211\n",
      "epoch no.0 train no.66960  loss = 3.68748 avg_loss = 3.94074\n",
      "epoch no.0 train no.66970  loss = 4.20763 avg_loss = 3.90150\n",
      "epoch no.0 train no.66980  loss = 3.26259 avg_loss = 3.90055\n",
      "epoch no.0 train no.66990  loss = 4.53889 avg_loss = 3.88984\n",
      "epoch no.0 train no.67000  loss = 5.86435 avg_loss = 3.98215\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.0 train no.67010  loss = 6.99761 avg_loss = 4.00934\n",
      "epoch no.0 train no.67020  loss = 2.99754 avg_loss = 4.06478\n",
      "epoch no.0 train no.67030  loss = 4.34620 avg_loss = 3.98940\n",
      "epoch no.0 train no.67040  loss = 2.82077 avg_loss = 3.97746\n",
      "epoch no.0 train no.67050  loss = 2.45670 avg_loss = 3.93276\n",
      "epoch no.0 train no.67060  loss = 3.23385 avg_loss = 3.88365\n",
      "epoch no.0 train no.67070  loss = 5.65149 avg_loss = 3.95361\n",
      "epoch no.0 train no.67080  loss = 3.64499 avg_loss = 4.02991\n",
      "epoch no.0 train no.67090  loss = 4.12072 avg_loss = 4.03133\n",
      "epoch no.0 train no.67100  loss = 3.23555 avg_loss = 3.94189\n",
      "epoch no.0 train no.67110  loss = 2.44147 avg_loss = 3.92867\n",
      "epoch no.0 train no.67120  loss = 4.49846 avg_loss = 3.93284\n",
      "epoch no.0 train no.67130  loss = 4.08481 avg_loss = 3.91426\n",
      "epoch no.0 train no.67140  loss = 5.23841 avg_loss = 3.86023\n",
      "epoch no.0 train no.67150  loss = 4.01290 avg_loss = 3.88571\n",
      "epoch no.0 train no.67160  loss = 3.67091 avg_loss = 3.91468\n",
      "epoch no.0 train no.67170  loss = 3.89378 avg_loss = 3.90747\n",
      "epoch no.0 train no.67180  loss = 4.79298 avg_loss = 3.87034\n",
      "epoch no.0 train no.67190  loss = 2.89231 avg_loss = 3.83013\n",
      "epoch no.0 train no.67200  loss = 4.33446 avg_loss = 3.87988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.67210  loss = 2.09445 avg_loss = 3.85666\n",
      "epoch no.0 train no.67220  loss = 2.60255 avg_loss = 3.88178\n",
      "epoch no.0 train no.67230  loss = 4.62824 avg_loss = 3.89457\n",
      "epoch no.0 train no.67240  loss = 3.43020 avg_loss = 3.89034\n",
      "epoch no.0 train no.67250  loss = 3.39905 avg_loss = 3.82763\n",
      "epoch no.0 train no.67260  loss = 3.14330 avg_loss = 3.80338\n",
      "epoch no.0 train no.67270  loss = 3.30881 avg_loss = 3.87584\n",
      "epoch no.0 train no.67280  loss = 2.65966 avg_loss = 3.82993\n",
      "epoch no.0 train no.67290  loss = 4.10995 avg_loss = 3.84785\n",
      "epoch no.0 train no.67300  loss = 2.64584 avg_loss = 3.84894\n",
      "epoch no.0 train no.67310  loss = 5.06141 avg_loss = 3.83626\n",
      "epoch no.0 train no.67320  loss = 4.02228 avg_loss = 3.89448\n",
      "epoch no.0 train no.67330  loss = 7.66793 avg_loss = 3.94224\n",
      "epoch no.0 train no.67340  loss = 5.32340 avg_loss = 3.95593\n",
      "epoch no.0 train no.67350  loss = 4.06361 avg_loss = 3.89271\n",
      "epoch no.0 train no.67360  loss = 3.94718 avg_loss = 3.92575\n",
      "epoch no.0 train no.67370  loss = 2.43179 avg_loss = 3.90886\n",
      "epoch no.0 train no.67380  loss = 4.00613 avg_loss = 3.87511\n",
      "epoch no.0 train no.67390  loss = 5.32093 avg_loss = 3.90401\n",
      "epoch no.0 train no.67400  loss = 3.66501 avg_loss = 3.93154\n",
      "epoch no.0 train no.67410  loss = 2.60243 avg_loss = 3.90202\n",
      "epoch no.0 train no.67420  loss = 4.50805 avg_loss = 3.92854\n",
      "epoch no.0 train no.67430  loss = 3.02309 avg_loss = 3.87411\n",
      "epoch no.0 train no.67440  loss = 2.25665 avg_loss = 3.85927\n",
      "epoch no.0 train no.67450  loss = 3.13145 avg_loss = 3.91779\n",
      "epoch no.0 train no.67460  loss = 3.50749 avg_loss = 3.91206\n",
      "epoch no.0 train no.67470  loss = 4.54562 avg_loss = 3.88755\n",
      "epoch no.0 train no.67480  loss = 3.36324 avg_loss = 3.90765\n",
      "epoch no.0 train no.67490  loss = 4.66847 avg_loss = 3.90922\n",
      "epoch no.0 train no.67500  loss = 4.11213 avg_loss = 3.89213\n",
      "epoch no.0 train no.67510  loss = 3.67862 avg_loss = 3.90749\n",
      "epoch no.0 train no.67520  loss = 2.95697 avg_loss = 3.85398\n",
      "epoch no.0 train no.67530  loss = 6.14285 avg_loss = 3.91229\n",
      "epoch no.0 train no.67540  loss = 2.90861 avg_loss = 3.89247\n",
      "epoch no.0 train no.67550  loss = 2.11338 avg_loss = 3.87891\n",
      "epoch no.0 train no.67560  loss = 3.11662 avg_loss = 3.83076\n",
      "epoch no.0 train no.67570  loss = 5.92553 avg_loss = 3.86244\n",
      "epoch no.0 train no.67580  loss = 6.73261 avg_loss = 3.88851\n",
      "epoch no.0 train no.67590  loss = 2.44753 avg_loss = 3.89257\n",
      "epoch no.0 train no.67600  loss = 4.11731 avg_loss = 3.89488\n",
      "epoch no.0 train no.67610  loss = 3.65176 avg_loss = 3.88795\n",
      "epoch no.0 train no.67620  loss = 3.10983 avg_loss = 3.83369\n",
      "epoch no.0 train no.67630  loss = 2.05658 avg_loss = 3.80178\n",
      "epoch no.0 train no.67640  loss = 4.30947 avg_loss = 3.78609\n",
      "epoch no.0 train no.67650  loss = 5.83125 avg_loss = 3.73681\n",
      "epoch no.0 train no.67660  loss = 2.47804 avg_loss = 3.69963\n",
      "epoch no.0 train no.67670  loss = 6.46773 avg_loss = 3.75990\n",
      "epoch no.0 train no.67680  loss = 2.66906 avg_loss = 3.78019\n",
      "epoch no.0 train no.67690  loss = 4.72508 avg_loss = 3.86429\n",
      "epoch no.0 train no.67700  loss = 4.78357 avg_loss = 3.94206\n",
      "epoch no.0 train no.67710  loss = 3.49104 avg_loss = 4.01298\n",
      "epoch no.0 train no.67720  loss = 4.09350 avg_loss = 4.04732\n",
      "epoch no.0 train no.67730  loss = 3.57220 avg_loss = 4.06287\n",
      "epoch no.0 train no.67740  loss = 3.42738 avg_loss = 4.05622\n",
      "epoch no.0 train no.67750  loss = 6.42573 avg_loss = 4.12102\n",
      "epoch no.0 train no.67760  loss = 4.93217 avg_loss = 4.07647\n",
      "epoch no.0 train no.67770  loss = 4.91740 avg_loss = 4.03639\n",
      "epoch no.0 train no.67780  loss = 4.90174 avg_loss = 4.02307\n",
      "epoch no.0 train no.67790  loss = 4.92037 avg_loss = 3.97103\n",
      "epoch no.0 train no.67800  loss = 3.13325 avg_loss = 3.92834\n",
      "epoch no.0 train no.67810  loss = 5.22155 avg_loss = 3.94549\n",
      "epoch no.0 train no.67820  loss = 3.16948 avg_loss = 3.96547\n",
      "epoch no.0 train no.67830  loss = 4.94945 avg_loss = 3.91877\n",
      "epoch no.0 train no.67840  loss = 2.83736 avg_loss = 3.90476\n",
      "epoch no.0 train no.67850  loss = 5.48416 avg_loss = 3.89158\n",
      "epoch no.0 train no.67860  loss = 7.41429 avg_loss = 3.93231\n",
      "epoch no.0 train no.67870  loss = 2.95621 avg_loss = 3.92094\n",
      "epoch no.0 train no.67880  loss = 2.75378 avg_loss = 3.89054\n",
      "epoch no.0 train no.67890  loss = 4.24398 avg_loss = 3.88639\n",
      "epoch no.0 train no.67900  loss = 3.46732 avg_loss = 3.91076\n",
      "epoch no.0 train no.67910  loss = 6.29151 avg_loss = 3.96993\n",
      "epoch no.0 train no.67920  loss = 3.43679 avg_loss = 3.93114\n",
      "epoch no.0 train no.67930  loss = 5.61489 avg_loss = 3.93738\n",
      "epoch no.0 train no.67940  loss = 2.54490 avg_loss = 3.95075\n",
      "epoch no.0 train no.67950  loss = 5.17050 avg_loss = 3.93382\n",
      "epoch no.0 train no.67960  loss = 4.24306 avg_loss = 3.93302\n",
      "epoch no.0 train no.67970  loss = 4.78929 avg_loss = 3.91634\n",
      "epoch no.0 train no.67980  loss = 2.33319 avg_loss = 3.86080\n",
      "epoch no.0 train no.67990  loss = 2.75111 avg_loss = 3.82466\n",
      "epoch no.0 train no.68000  loss = 4.53580 avg_loss = 3.80027\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '</s>', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.0 train no.68010  loss = 3.07686 avg_loss = 3.85545\n",
      "epoch no.0 train no.68020  loss = 2.55770 avg_loss = 3.90820\n",
      "epoch no.0 train no.68030  loss = 4.75901 avg_loss = 3.87418\n",
      "epoch no.0 train no.68040  loss = 2.72322 avg_loss = 3.83912\n",
      "epoch no.0 train no.68050  loss = 3.41853 avg_loss = 3.81840\n",
      "epoch no.0 train no.68060  loss = 2.72776 avg_loss = 3.85234\n",
      "epoch no.0 train no.68070  loss = 3.74484 avg_loss = 3.86458\n",
      "epoch no.0 train no.68080  loss = 4.50622 avg_loss = 3.90559\n",
      "epoch no.0 train no.68090  loss = 2.08352 avg_loss = 3.86139\n",
      "epoch no.0 train no.68100  loss = 1.93870 avg_loss = 3.81891\n",
      "epoch no.0 train no.68110  loss = 3.81439 avg_loss = 3.86656\n",
      "epoch no.0 train no.68120  loss = 4.48177 avg_loss = 3.93405\n",
      "epoch no.0 train no.68130  loss = 5.45831 avg_loss = 3.90433\n",
      "epoch no.0 train no.68140  loss = 2.92361 avg_loss = 3.87879\n",
      "epoch no.0 train no.68150  loss = 5.39169 avg_loss = 3.91168\n",
      "epoch no.0 train no.68160  loss = 7.25216 avg_loss = 4.00192\n",
      "epoch no.0 train no.68170  loss = 5.38147 avg_loss = 3.95523\n",
      "epoch no.0 train no.68180  loss = 4.46169 avg_loss = 3.99847\n",
      "epoch no.0 train no.68190  loss = 3.07367 avg_loss = 4.04232\n",
      "epoch no.0 train no.68200  loss = 4.82633 avg_loss = 4.05230\n",
      "epoch no.0 train no.68210  loss = 2.02905 avg_loss = 4.01569\n",
      "epoch no.0 train no.68220  loss = 2.22450 avg_loss = 4.04868\n",
      "epoch no.0 train no.68230  loss = 2.86690 avg_loss = 4.03479\n",
      "epoch no.0 train no.68240  loss = 3.59814 avg_loss = 4.03450\n",
      "epoch no.0 train no.68250  loss = 2.48008 avg_loss = 4.04287\n",
      "epoch no.0 train no.68260  loss = 1.88139 avg_loss = 4.01534\n",
      "epoch no.0 train no.68270  loss = 4.50307 avg_loss = 4.00645\n",
      "epoch no.0 train no.68280  loss = 6.04582 avg_loss = 4.05860\n",
      "epoch no.0 train no.68290  loss = 3.91681 avg_loss = 4.03762\n",
      "epoch no.0 train no.68300  loss = 5.01984 avg_loss = 4.01624\n",
      "epoch no.0 train no.68310  loss = 5.30556 avg_loss = 4.09405\n",
      "epoch no.0 train no.68320  loss = 5.57813 avg_loss = 4.04974\n",
      "epoch no.0 train no.68330  loss = 4.87054 avg_loss = 4.01255\n",
      "epoch no.0 train no.68340  loss = 2.10424 avg_loss = 4.04541\n",
      "epoch no.0 train no.68350  loss = 3.65081 avg_loss = 4.00296\n",
      "epoch no.0 train no.68360  loss = 2.99271 avg_loss = 4.05330\n",
      "epoch no.0 train no.68370  loss = 2.66207 avg_loss = 4.08071\n",
      "epoch no.0 train no.68380  loss = 3.40155 avg_loss = 4.01550\n",
      "epoch no.0 train no.68390  loss = 1.74505 avg_loss = 3.97119\n",
      "epoch no.0 train no.68400  loss = 2.85120 avg_loss = 4.01718\n",
      "epoch no.0 train no.68410  loss = 3.16621 avg_loss = 4.02546\n",
      "epoch no.0 train no.68420  loss = 3.29537 avg_loss = 4.00223\n",
      "epoch no.0 train no.68430  loss = 2.58667 avg_loss = 3.95725\n",
      "epoch no.0 train no.68440  loss = 4.03795 avg_loss = 3.96032\n",
      "epoch no.0 train no.68450  loss = 3.08682 avg_loss = 3.92372\n",
      "epoch no.0 train no.68460  loss = 5.49890 avg_loss = 3.88130\n",
      "epoch no.0 train no.68470  loss = 2.89855 avg_loss = 3.90061\n",
      "epoch no.0 train no.68480  loss = 5.42776 avg_loss = 3.90143\n",
      "epoch no.0 train no.68490  loss = 3.24334 avg_loss = 3.90312\n",
      "epoch no.0 train no.68500  loss = 2.28908 avg_loss = 3.86875\n",
      "epoch no.0 train no.68510  loss = 3.44432 avg_loss = 3.88084\n",
      "epoch no.0 train no.68520  loss = 3.84308 avg_loss = 3.86333\n",
      "epoch no.0 train no.68530  loss = 4.42909 avg_loss = 3.90864\n",
      "epoch no.0 train no.68540  loss = 3.25780 avg_loss = 3.87548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.68550  loss = 2.48662 avg_loss = 3.92208\n",
      "epoch no.0 train no.68560  loss = 3.06254 avg_loss = 3.94398\n",
      "epoch no.0 train no.68570  loss = 6.77897 avg_loss = 4.01180\n",
      "epoch no.0 train no.68580  loss = 2.52406 avg_loss = 4.03236\n",
      "epoch no.0 train no.68590  loss = 3.88666 avg_loss = 3.99499\n",
      "epoch no.0 train no.68600  loss = 4.74239 avg_loss = 4.04607\n",
      "epoch no.0 train no.68610  loss = 4.70769 avg_loss = 3.99563\n",
      "epoch no.0 train no.68620  loss = 3.18383 avg_loss = 3.97616\n",
      "epoch no.0 train no.68630  loss = 4.76871 avg_loss = 3.96048\n",
      "epoch no.0 train no.68640  loss = 2.03702 avg_loss = 3.91012\n",
      "epoch no.0 train no.68650  loss = 2.52425 avg_loss = 3.87097\n",
      "epoch no.0 train no.68660  loss = 4.44187 avg_loss = 3.89058\n",
      "epoch no.0 train no.68670  loss = 5.04566 avg_loss = 3.90262\n",
      "epoch no.0 train no.68680  loss = 5.31415 avg_loss = 3.91796\n",
      "epoch no.0 train no.68690  loss = 1.95949 avg_loss = 3.89590\n",
      "epoch no.0 train no.68700  loss = 5.90506 avg_loss = 3.94562\n",
      "epoch no.0 train no.68710  loss = 2.38676 avg_loss = 3.90377\n",
      "epoch no.0 train no.68720  loss = 4.35449 avg_loss = 3.93300\n",
      "epoch no.0 train no.68730  loss = 4.54505 avg_loss = 4.00099\n",
      "epoch no.0 train no.68740  loss = 2.84971 avg_loss = 4.01635\n",
      "epoch no.0 train no.68750  loss = 3.33964 avg_loss = 4.06036\n",
      "epoch no.0 train no.68760  loss = 2.63262 avg_loss = 3.99755\n",
      "epoch no.0 train no.68770  loss = 5.78594 avg_loss = 4.01530\n",
      "epoch no.0 train no.68780  loss = 4.50436 avg_loss = 4.04375\n",
      "epoch no.0 train no.68790  loss = 2.94725 avg_loss = 3.94348\n",
      "epoch no.0 train no.68800  loss = 4.14535 avg_loss = 3.93288\n",
      "epoch no.0 train no.68810  loss = 3.93050 avg_loss = 3.92160\n",
      "epoch no.0 train no.68820  loss = 3.73165 avg_loss = 3.90821\n",
      "epoch no.0 train no.68830  loss = 4.71309 avg_loss = 3.86127\n",
      "epoch no.0 train no.68840  loss = 2.12709 avg_loss = 3.87318\n",
      "epoch no.0 train no.68850  loss = 4.89567 avg_loss = 3.90832\n",
      "epoch no.0 train no.68860  loss = 3.35307 avg_loss = 3.89902\n",
      "epoch no.0 train no.68870  loss = 5.07957 avg_loss = 3.91690\n",
      "epoch no.0 train no.68880  loss = 6.01611 avg_loss = 3.91163\n",
      "epoch no.0 train no.68890  loss = 4.03101 avg_loss = 3.89193\n",
      "epoch no.0 train no.68900  loss = 2.11938 avg_loss = 3.84014\n",
      "epoch no.0 train no.68910  loss = 3.22639 avg_loss = 3.83314\n",
      "epoch no.0 train no.68920  loss = 4.18009 avg_loss = 3.88064\n",
      "epoch no.0 train no.68930  loss = 2.84661 avg_loss = 3.93463\n",
      "epoch no.0 train no.68940  loss = 3.60892 avg_loss = 3.87744\n",
      "epoch no.0 train no.68950  loss = 3.20502 avg_loss = 3.86917\n",
      "epoch no.0 train no.68960  loss = 3.14844 avg_loss = 3.85941\n",
      "epoch no.0 train no.68970  loss = 1.56819 avg_loss = 3.83632\n",
      "epoch no.0 train no.68980  loss = 3.16177 avg_loss = 3.83485\n",
      "epoch no.0 train no.68990  loss = 4.37695 avg_loss = 3.93875\n",
      "epoch no.0 train no.69000  loss = 3.02782 avg_loss = 3.88154\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '송', '▁모음', '음', '</s>']\n",
      "추억의 팝송모음</s>\n",
      "epoch no.0 train no.69010  loss = 4.17026 avg_loss = 3.83777\n",
      "epoch no.0 train no.69020  loss = 5.08355 avg_loss = 3.88785\n",
      "epoch no.0 train no.69030  loss = 2.19606 avg_loss = 3.89801\n",
      "epoch no.0 train no.69040  loss = 3.93471 avg_loss = 3.93688\n",
      "epoch no.0 train no.69050  loss = 3.75916 avg_loss = 3.92077\n",
      "epoch no.0 train no.69060  loss = 6.22138 avg_loss = 3.92454\n",
      "epoch no.0 train no.69070  loss = 4.53032 avg_loss = 3.93779\n",
      "epoch no.0 train no.69080  loss = 2.31455 avg_loss = 3.98121\n",
      "epoch no.0 train no.69090  loss = 2.68400 avg_loss = 3.95107\n",
      "epoch no.0 train no.69100  loss = 4.94583 avg_loss = 3.96541\n",
      "epoch no.0 train no.69110  loss = 3.70460 avg_loss = 3.97345\n",
      "epoch no.0 train no.69120  loss = 3.15486 avg_loss = 3.95563\n",
      "epoch no.0 train no.69130  loss = 3.07828 avg_loss = 4.01176\n",
      "epoch no.0 train no.69140  loss = 4.13837 avg_loss = 4.06638\n",
      "epoch no.0 train no.69150  loss = 5.29074 avg_loss = 4.04726\n",
      "epoch no.0 train no.69160  loss = 2.80726 avg_loss = 4.03447\n",
      "epoch no.0 train no.69170  loss = 5.09902 avg_loss = 4.03418\n",
      "epoch no.0 train no.69180  loss = 2.97902 avg_loss = 3.99937\n",
      "epoch no.0 train no.69190  loss = 3.77932 avg_loss = 3.97334\n",
      "epoch no.0 train no.69200  loss = 5.26397 avg_loss = 4.03107\n",
      "epoch no.0 train no.69210  loss = 2.60936 avg_loss = 3.98162\n",
      "epoch no.0 train no.69220  loss = 4.98198 avg_loss = 4.01354\n",
      "epoch no.0 train no.69230  loss = 3.53778 avg_loss = 3.97916\n",
      "epoch no.0 train no.69240  loss = 3.37172 avg_loss = 3.94179\n",
      "epoch no.0 train no.69250  loss = 2.52947 avg_loss = 3.94799\n",
      "epoch no.0 train no.69260  loss = 2.29111 avg_loss = 3.87631\n",
      "epoch no.0 train no.69270  loss = 3.40370 avg_loss = 3.88911\n",
      "epoch no.0 train no.69280  loss = 3.06132 avg_loss = 3.90377\n",
      "epoch no.0 train no.69290  loss = 4.02509 avg_loss = 3.94663\n",
      "epoch no.0 train no.69300  loss = 3.74604 avg_loss = 3.94470\n",
      "epoch no.0 train no.69310  loss = 3.22261 avg_loss = 3.97528\n",
      "epoch no.0 train no.69320  loss = 5.09859 avg_loss = 3.99370\n",
      "epoch no.0 train no.69330  loss = 4.00613 avg_loss = 3.99751\n",
      "epoch no.0 train no.69340  loss = 2.45121 avg_loss = 3.96873\n",
      "epoch no.0 train no.69350  loss = 2.55965 avg_loss = 3.98585\n",
      "epoch no.0 train no.69360  loss = 4.96974 avg_loss = 3.96214\n",
      "epoch no.0 train no.69370  loss = 4.55555 avg_loss = 3.97130\n",
      "epoch no.0 train no.69380  loss = 3.05824 avg_loss = 3.92966\n",
      "epoch no.0 train no.69390  loss = 4.57001 avg_loss = 3.91617\n",
      "epoch no.0 train no.69400  loss = 2.61487 avg_loss = 3.89888\n",
      "epoch no.0 train no.69410  loss = 3.52877 avg_loss = 3.89916\n",
      "epoch no.0 train no.69420  loss = 2.88664 avg_loss = 3.87998\n",
      "epoch no.0 train no.69430  loss = 6.97900 avg_loss = 3.92702\n",
      "epoch no.0 train no.69440  loss = 3.54484 avg_loss = 3.91234\n",
      "epoch no.0 train no.69450  loss = 3.94174 avg_loss = 3.92133\n",
      "epoch no.0 train no.69460  loss = 5.11948 avg_loss = 3.94653\n",
      "epoch no.0 train no.69470  loss = 3.24732 avg_loss = 3.94463\n",
      "epoch no.0 train no.69480  loss = 6.32588 avg_loss = 3.96545\n",
      "epoch no.0 train no.69490  loss = 2.92085 avg_loss = 3.95314\n",
      "epoch no.0 train no.69500  loss = 3.74387 avg_loss = 3.90926\n",
      "epoch no.0 train no.69510  loss = 6.15842 avg_loss = 3.87912\n",
      "epoch no.0 train no.69520  loss = 6.91931 avg_loss = 3.90183\n",
      "epoch no.0 train no.69530  loss = 3.30241 avg_loss = 3.93347\n",
      "epoch no.0 train no.69540  loss = 4.41850 avg_loss = 3.94648\n",
      "epoch no.0 train no.69550  loss = 1.90753 avg_loss = 3.88486\n",
      "epoch no.0 train no.69560  loss = 2.64170 avg_loss = 3.85633\n",
      "epoch no.0 train no.69570  loss = 3.15840 avg_loss = 3.84546\n",
      "epoch no.0 train no.69580  loss = 5.02532 avg_loss = 3.84642\n",
      "epoch no.0 train no.69590  loss = 3.28144 avg_loss = 3.81751\n",
      "epoch no.0 train no.69600  loss = 4.09529 avg_loss = 3.82889\n",
      "epoch no.0 train no.69610  loss = 2.55372 avg_loss = 3.86126\n",
      "epoch no.0 train no.69620  loss = 5.30396 avg_loss = 3.85168\n",
      "epoch no.0 train no.69630  loss = 2.55749 avg_loss = 3.85048\n",
      "epoch no.0 train no.69640  loss = 3.07555 avg_loss = 3.76369\n",
      "epoch no.0 train no.69650  loss = 4.89501 avg_loss = 3.76699\n",
      "epoch no.0 train no.69660  loss = 2.85543 avg_loss = 3.77640\n",
      "epoch no.0 train no.69670  loss = 1.93106 avg_loss = 3.76539\n",
      "epoch no.0 train no.69680  loss = 4.12996 avg_loss = 3.76368\n",
      "epoch no.0 train no.69690  loss = 4.28034 avg_loss = 3.78243\n",
      "epoch no.0 train no.69700  loss = 4.01203 avg_loss = 3.78737\n",
      "epoch no.0 train no.69710  loss = 5.42259 avg_loss = 3.87443\n",
      "epoch no.0 train no.69720  loss = 3.58402 avg_loss = 3.89388\n",
      "epoch no.0 train no.69730  loss = 5.27916 avg_loss = 3.93345\n",
      "epoch no.0 train no.69740  loss = 3.33832 avg_loss = 3.93306\n",
      "epoch no.0 train no.69750  loss = 3.73358 avg_loss = 3.93331\n",
      "epoch no.0 train no.69760  loss = 3.00042 avg_loss = 3.93766\n",
      "epoch no.0 train no.69770  loss = 2.38990 avg_loss = 3.94441\n",
      "epoch no.0 train no.69780  loss = 2.93155 avg_loss = 3.91081\n",
      "epoch no.0 train no.69790  loss = 3.23533 avg_loss = 3.89574\n",
      "epoch no.0 train no.69800  loss = 4.79355 avg_loss = 3.94112\n",
      "epoch no.0 train no.69810  loss = 4.40600 avg_loss = 3.94683\n",
      "epoch no.0 train no.69820  loss = 3.94165 avg_loss = 3.90815\n",
      "epoch no.0 train no.69830  loss = 6.00504 avg_loss = 3.96069\n",
      "epoch no.0 train no.69840  loss = 4.21211 avg_loss = 3.96018\n",
      "epoch no.0 train no.69850  loss = 3.22331 avg_loss = 3.96502\n",
      "epoch no.0 train no.69860  loss = 2.04311 avg_loss = 3.95969\n",
      "epoch no.0 train no.69870  loss = 3.06770 avg_loss = 3.97271\n",
      "epoch no.0 train no.69880  loss = 4.64797 avg_loss = 3.98707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.69890  loss = 3.12204 avg_loss = 3.97579\n",
      "epoch no.0 train no.69900  loss = 2.34520 avg_loss = 3.95142\n",
      "epoch no.0 train no.69910  loss = 3.08264 avg_loss = 3.98186\n",
      "epoch no.0 train no.69920  loss = 1.85690 avg_loss = 4.03496\n",
      "epoch no.0 train no.69930  loss = 4.89271 avg_loss = 3.96195\n",
      "epoch no.0 train no.69940  loss = 2.81381 avg_loss = 3.93693\n",
      "epoch no.0 train no.69950  loss = 3.62253 avg_loss = 3.88513\n",
      "epoch no.0 train no.69960  loss = 2.41183 avg_loss = 3.80830\n",
      "epoch no.0 train no.69970  loss = 3.48583 avg_loss = 3.79759\n",
      "epoch no.0 train no.69980  loss = 3.26350 avg_loss = 3.77954\n",
      "epoch no.0 train no.69990  loss = 3.42184 avg_loss = 3.79022\n",
      "epoch no.0 train no.70000  loss = 6.06840 avg_loss = 3.81153\n",
      "3\n",
      "to_tokens: ['▁잔잔', '▁발라', '▁o', 'st', '</s>']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.0 train no.70010  loss = 2.11280 avg_loss = 3.82820\n",
      "epoch no.0 train no.70020  loss = 4.28110 avg_loss = 3.87465\n",
      "epoch no.0 train no.70030  loss = 3.29598 avg_loss = 3.84827\n",
      "epoch no.0 train no.70040  loss = 3.54957 avg_loss = 3.82474\n",
      "epoch no.0 train no.70050  loss = 2.42265 avg_loss = 3.83779\n",
      "epoch no.0 train no.70060  loss = 3.09570 avg_loss = 3.86610\n",
      "epoch no.0 train no.70070  loss = 3.27778 avg_loss = 3.86866\n",
      "epoch no.0 train no.70080  loss = 7.08132 avg_loss = 3.87178\n",
      "epoch no.0 train no.70090  loss = 2.86372 avg_loss = 3.88658\n",
      "epoch no.0 train no.70100  loss = 3.23950 avg_loss = 3.94422\n",
      "epoch no.0 train no.70110  loss = 2.63969 avg_loss = 3.95347\n",
      "epoch no.0 train no.70120  loss = 4.42778 avg_loss = 3.95470\n",
      "epoch no.0 train no.70130  loss = 2.90118 avg_loss = 3.92959\n",
      "epoch no.0 train no.70140  loss = 3.67527 avg_loss = 3.86867\n",
      "epoch no.0 train no.70150  loss = 2.90117 avg_loss = 3.80693\n",
      "epoch no.0 train no.70160  loss = 5.33780 avg_loss = 3.79051\n",
      "epoch no.0 train no.70170  loss = 3.47234 avg_loss = 3.84620\n",
      "epoch no.0 train no.70180  loss = 4.12415 avg_loss = 3.83696\n",
      "epoch no.0 train no.70190  loss = 2.43030 avg_loss = 3.89901\n",
      "epoch no.0 train no.70200  loss = 5.71154 avg_loss = 3.94606\n",
      "epoch no.0 train no.70210  loss = 3.17628 avg_loss = 3.97199\n",
      "epoch no.0 train no.70220  loss = 2.96414 avg_loss = 3.94069\n",
      "epoch no.0 train no.70230  loss = 4.46537 avg_loss = 3.91937\n",
      "epoch no.0 train no.70240  loss = 4.18813 avg_loss = 3.92710\n",
      "epoch no.0 train no.70250  loss = 5.52797 avg_loss = 3.94826\n",
      "epoch no.0 train no.70260  loss = 4.46378 avg_loss = 3.93564\n",
      "epoch no.0 train no.70270  loss = 4.87017 avg_loss = 3.94198\n",
      "epoch no.0 train no.70280  loss = 6.78231 avg_loss = 3.96615\n",
      "epoch no.0 train no.70290  loss = 3.60802 avg_loss = 3.96482\n",
      "epoch no.0 train no.70300  loss = 3.50300 avg_loss = 3.92011\n",
      "epoch no.0 train no.70310  loss = 3.97924 avg_loss = 3.92499\n",
      "epoch no.0 train no.70320  loss = 2.72033 avg_loss = 3.93860\n",
      "epoch no.0 train no.70330  loss = 1.68552 avg_loss = 3.90371\n",
      "epoch no.0 train no.70340  loss = 4.51786 avg_loss = 3.91854\n",
      "epoch no.0 train no.70350  loss = 2.53631 avg_loss = 3.83075\n",
      "epoch no.0 train no.70360  loss = 4.85442 avg_loss = 3.82645\n",
      "epoch no.0 train no.70370  loss = 5.11046 avg_loss = 3.91062\n",
      "epoch no.0 train no.70380  loss = 3.67369 avg_loss = 3.89348\n",
      "epoch no.0 train no.70390  loss = 5.01842 avg_loss = 3.87658\n",
      "epoch no.0 train no.70400  loss = 5.44406 avg_loss = 3.87669\n",
      "epoch no.0 train no.70410  loss = 2.89838 avg_loss = 3.90650\n",
      "epoch no.0 train no.70420  loss = 3.31943 avg_loss = 3.89263\n",
      "epoch no.0 train no.70430  loss = 4.19295 avg_loss = 3.92517\n",
      "epoch no.0 train no.70440  loss = 6.22279 avg_loss = 3.98885\n",
      "epoch no.0 train no.70450  loss = 4.39568 avg_loss = 3.99113\n",
      "epoch no.0 train no.70460  loss = 4.73858 avg_loss = 3.98954\n",
      "epoch no.0 train no.70470  loss = 4.73798 avg_loss = 3.97428\n",
      "epoch no.0 train no.70480  loss = 2.69766 avg_loss = 3.97462\n",
      "epoch no.0 train no.70490  loss = 2.50660 avg_loss = 3.96384\n",
      "epoch no.0 train no.70500  loss = 2.27342 avg_loss = 3.97901\n",
      "epoch no.0 train no.70510  loss = 6.60576 avg_loss = 3.99140\n",
      "epoch no.0 train no.70520  loss = 3.21135 avg_loss = 4.00003\n",
      "epoch no.0 train no.70530  loss = 2.64777 avg_loss = 3.97555\n",
      "epoch no.0 train no.70540  loss = 4.05101 avg_loss = 3.97271\n",
      "epoch no.0 train no.70550  loss = 3.21923 avg_loss = 3.92400\n",
      "epoch no.0 train no.70560  loss = 2.85812 avg_loss = 3.88779\n",
      "epoch no.0 train no.70570  loss = 3.21961 avg_loss = 3.88473\n",
      "epoch no.0 train no.70580  loss = 4.01744 avg_loss = 3.92366\n",
      "epoch no.0 train no.70590  loss = 2.58608 avg_loss = 3.96946\n",
      "epoch no.0 train no.70600  loss = 2.75800 avg_loss = 3.88235\n",
      "epoch no.0 train no.70610  loss = 3.17043 avg_loss = 3.88225\n",
      "epoch no.0 train no.70620  loss = 4.17844 avg_loss = 3.86316\n",
      "epoch no.0 train no.70630  loss = 3.17126 avg_loss = 3.88937\n",
      "epoch no.0 train no.70640  loss = 3.19455 avg_loss = 3.92350\n",
      "epoch no.0 train no.70650  loss = 4.09288 avg_loss = 3.87027\n",
      "epoch no.0 train no.70660  loss = 2.51602 avg_loss = 3.89872\n",
      "epoch no.0 train no.70670  loss = 3.39250 avg_loss = 3.90731\n",
      "epoch no.0 train no.70680  loss = 2.45145 avg_loss = 3.92690\n",
      "epoch no.0 train no.70690  loss = 3.65553 avg_loss = 3.86591\n",
      "epoch no.0 train no.70700  loss = 2.46027 avg_loss = 3.87864\n",
      "epoch no.0 train no.70710  loss = 3.65318 avg_loss = 3.81361\n",
      "epoch no.0 train no.70720  loss = 3.13119 avg_loss = 3.82834\n",
      "epoch no.0 train no.70730  loss = 6.58542 avg_loss = 3.79632\n",
      "epoch no.0 train no.70740  loss = 1.92839 avg_loss = 3.77806\n",
      "epoch no.0 train no.70750  loss = 4.85506 avg_loss = 3.77830\n",
      "epoch no.0 train no.70760  loss = 2.40409 avg_loss = 3.75753\n",
      "epoch no.0 train no.70770  loss = 5.54347 avg_loss = 3.77283\n",
      "epoch no.0 train no.70780  loss = 4.30867 avg_loss = 3.84984\n",
      "epoch no.0 train no.70790  loss = 2.35978 avg_loss = 3.81869\n",
      "epoch no.0 train no.70800  loss = 4.08192 avg_loss = 3.86436\n",
      "epoch no.0 train no.70810  loss = 3.63009 avg_loss = 3.85248\n",
      "epoch no.0 train no.70820  loss = 2.29165 avg_loss = 3.82941\n",
      "epoch no.0 train no.70830  loss = 4.54150 avg_loss = 3.85529\n",
      "epoch no.0 train no.70840  loss = 2.16322 avg_loss = 3.85168\n",
      "epoch no.0 train no.70850  loss = 4.43092 avg_loss = 3.86175\n",
      "epoch no.0 train no.70860  loss = 4.31315 avg_loss = 3.85861\n",
      "epoch no.0 train no.70870  loss = 5.82704 avg_loss = 3.83471\n",
      "epoch no.0 train no.70880  loss = 2.34064 avg_loss = 3.85521\n",
      "epoch no.0 train no.70890  loss = 3.41191 avg_loss = 3.84434\n",
      "epoch no.0 train no.70900  loss = 3.19990 avg_loss = 3.81190\n",
      "epoch no.0 train no.70910  loss = 2.66313 avg_loss = 3.76173\n",
      "epoch no.0 train no.70920  loss = 2.94556 avg_loss = 3.76587\n",
      "epoch no.0 train no.70930  loss = 6.27040 avg_loss = 3.75969\n",
      "epoch no.0 train no.70940  loss = 4.05773 avg_loss = 3.71566\n",
      "epoch no.0 train no.70950  loss = 3.80784 avg_loss = 3.72714\n",
      "epoch no.0 train no.70960  loss = 5.85645 avg_loss = 3.79617\n",
      "epoch no.0 train no.70970  loss = 3.27925 avg_loss = 3.79907\n",
      "epoch no.0 train no.70980  loss = 4.78227 avg_loss = 3.83248\n",
      "epoch no.0 train no.70990  loss = 4.50370 avg_loss = 3.85812\n",
      "epoch no.0 train no.71000  loss = 2.36753 avg_loss = 3.83178\n",
      "5\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁댄스', '</s>', '곡', '</s>']\n",
      "추억의 90년대 가요 명곡</s>\n",
      "epoch no.0 train no.71010  loss = 2.03696 avg_loss = 3.84195\n",
      "epoch no.0 train no.71020  loss = 4.36777 avg_loss = 3.86755\n",
      "epoch no.0 train no.71030  loss = 2.36974 avg_loss = 3.90274\n",
      "epoch no.0 train no.71040  loss = 2.74588 avg_loss = 3.91126\n",
      "epoch no.0 train no.71050  loss = 2.41827 avg_loss = 3.89548\n",
      "epoch no.0 train no.71060  loss = 6.78013 avg_loss = 3.91466\n",
      "epoch no.0 train no.71070  loss = 2.46716 avg_loss = 3.89637\n",
      "epoch no.0 train no.71080  loss = 3.31108 avg_loss = 3.86778\n",
      "epoch no.0 train no.71090  loss = 3.38503 avg_loss = 3.85901\n",
      "epoch no.0 train no.71100  loss = 3.23522 avg_loss = 3.85514\n",
      "epoch no.0 train no.71110  loss = 3.46412 avg_loss = 3.85285\n",
      "epoch no.0 train no.71120  loss = 3.65363 avg_loss = 3.84320\n",
      "epoch no.0 train no.71130  loss = 3.51240 avg_loss = 3.84689\n",
      "epoch no.0 train no.71140  loss = 3.23449 avg_loss = 3.85533\n",
      "epoch no.0 train no.71150  loss = 5.08662 avg_loss = 3.87920\n",
      "epoch no.0 train no.71160  loss = 6.92028 avg_loss = 3.90690\n",
      "epoch no.0 train no.71170  loss = 3.44838 avg_loss = 3.84344\n",
      "epoch no.0 train no.71180  loss = 2.95606 avg_loss = 3.88397\n",
      "epoch no.0 train no.71190  loss = 5.28965 avg_loss = 3.82208\n",
      "epoch no.0 train no.71200  loss = 2.58691 avg_loss = 3.76333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.71210  loss = 3.38200 avg_loss = 3.73824\n",
      "epoch no.0 train no.71220  loss = 6.67625 avg_loss = 3.81467\n",
      "epoch no.0 train no.71230  loss = 3.24062 avg_loss = 3.86465\n",
      "epoch no.0 train no.71240  loss = 5.71936 avg_loss = 3.81775\n",
      "epoch no.0 train no.71250  loss = 4.21184 avg_loss = 3.82526\n",
      "epoch no.0 train no.71260  loss = 4.58325 avg_loss = 3.88109\n",
      "epoch no.0 train no.71270  loss = 4.78202 avg_loss = 3.81208\n",
      "epoch no.0 train no.71280  loss = 3.53022 avg_loss = 3.81265\n",
      "epoch no.0 train no.71290  loss = 6.82937 avg_loss = 3.82558\n",
      "epoch no.0 train no.71300  loss = 3.28189 avg_loss = 3.83673\n",
      "epoch no.0 train no.71310  loss = 3.77757 avg_loss = 3.85778\n",
      "epoch no.0 train no.71320  loss = 3.08052 avg_loss = 3.79272\n",
      "epoch no.0 train no.71330  loss = 4.67912 avg_loss = 3.80326\n",
      "epoch no.0 train no.71340  loss = 5.44039 avg_loss = 3.79750\n",
      "epoch no.0 train no.71350  loss = 3.97168 avg_loss = 3.81552\n",
      "epoch no.0 train no.71360  loss = 4.71333 avg_loss = 3.83551\n",
      "epoch no.0 train no.71370  loss = 4.08559 avg_loss = 3.78161\n",
      "epoch no.0 train no.71380  loss = 2.34678 avg_loss = 3.72553\n",
      "epoch no.0 train no.71390  loss = 3.16035 avg_loss = 3.75354\n",
      "epoch no.0 train no.71400  loss = 4.25737 avg_loss = 3.80177\n",
      "epoch no.0 train no.71410  loss = 2.63617 avg_loss = 3.77765\n",
      "epoch no.0 train no.71420  loss = 3.86465 avg_loss = 3.79708\n",
      "epoch no.0 train no.71430  loss = 3.71379 avg_loss = 3.80808\n",
      "epoch no.0 train no.71440  loss = 3.35392 avg_loss = 3.75879\n",
      "epoch no.0 train no.71450  loss = 3.01778 avg_loss = 3.71509\n",
      "epoch no.0 train no.71460  loss = 3.63544 avg_loss = 3.66754\n",
      "epoch no.0 train no.71470  loss = 2.18446 avg_loss = 3.63995\n",
      "epoch no.0 train no.71480  loss = 4.11566 avg_loss = 3.59812\n",
      "epoch no.0 train no.71490  loss = 4.22316 avg_loss = 3.67730\n",
      "epoch no.0 train no.71500  loss = 2.71344 avg_loss = 3.67869\n",
      "epoch no.0 train no.71510  loss = 4.39470 avg_loss = 3.69859\n",
      "epoch no.0 train no.71520  loss = 5.61457 avg_loss = 3.68812\n",
      "epoch no.0 train no.71530  loss = 3.48814 avg_loss = 3.78207\n",
      "epoch no.0 train no.71540  loss = 2.63262 avg_loss = 3.84871\n",
      "epoch no.0 train no.71550  loss = 4.04496 avg_loss = 3.83092\n",
      "epoch no.0 train no.71560  loss = 3.57051 avg_loss = 3.85818\n",
      "epoch no.0 train no.71570  loss = 2.46892 avg_loss = 3.85370\n",
      "epoch no.0 train no.71580  loss = 3.56914 avg_loss = 3.86257\n",
      "epoch no.0 train no.71590  loss = 6.83539 avg_loss = 3.82988\n",
      "epoch no.0 train no.71600  loss = 3.04915 avg_loss = 3.79156\n",
      "epoch no.0 train no.71610  loss = 3.27800 avg_loss = 3.79292\n",
      "epoch no.0 train no.71620  loss = 7.46731 avg_loss = 3.87810\n",
      "epoch no.0 train no.71630  loss = 2.81952 avg_loss = 3.79176\n",
      "epoch no.0 train no.71640  loss = 5.58057 avg_loss = 3.82828\n",
      "epoch no.0 train no.71650  loss = 3.80211 avg_loss = 3.84520\n",
      "epoch no.0 train no.71660  loss = 2.81249 avg_loss = 3.82539\n",
      "epoch no.0 train no.71670  loss = 3.07682 avg_loss = 3.86043\n",
      "epoch no.0 train no.71680  loss = 2.36857 avg_loss = 3.83141\n",
      "epoch no.0 train no.71690  loss = 3.56990 avg_loss = 3.82393\n",
      "epoch no.0 train no.71700  loss = 2.71683 avg_loss = 3.85969\n",
      "epoch no.0 train no.71710  loss = 2.09287 avg_loss = 3.89758\n",
      "epoch no.0 train no.71720  loss = 3.09037 avg_loss = 3.91319\n",
      "epoch no.0 train no.71730  loss = 4.06527 avg_loss = 3.88758\n",
      "epoch no.0 train no.71740  loss = 3.31838 avg_loss = 3.87596\n",
      "epoch no.0 train no.71750  loss = 3.94088 avg_loss = 3.89938\n",
      "epoch no.0 train no.71760  loss = 2.50737 avg_loss = 3.88854\n",
      "epoch no.0 train no.71770  loss = 3.80115 avg_loss = 3.87892\n",
      "epoch no.0 train no.71780  loss = 2.34736 avg_loss = 3.81747\n",
      "epoch no.0 train no.71790  loss = 3.67425 avg_loss = 3.86089\n",
      "epoch no.0 train no.71800  loss = 2.45030 avg_loss = 3.87447\n",
      "epoch no.0 train no.71810  loss = 3.69696 avg_loss = 3.86352\n",
      "epoch no.0 train no.71820  loss = 4.23873 avg_loss = 3.91478\n",
      "epoch no.0 train no.71830  loss = 4.40603 avg_loss = 3.92626\n",
      "epoch no.0 train no.71840  loss = 2.65824 avg_loss = 3.93139\n",
      "epoch no.0 train no.71850  loss = 2.83957 avg_loss = 3.84393\n",
      "epoch no.0 train no.71860  loss = 2.76688 avg_loss = 3.80064\n",
      "epoch no.0 train no.71870  loss = 2.26988 avg_loss = 3.68952\n",
      "epoch no.0 train no.71880  loss = 2.25342 avg_loss = 3.74391\n",
      "epoch no.0 train no.71890  loss = 4.84490 avg_loss = 3.76899\n",
      "epoch no.0 train no.71900  loss = 5.90313 avg_loss = 3.80372\n",
      "epoch no.0 train no.71910  loss = 5.57708 avg_loss = 3.83096\n",
      "epoch no.0 train no.71920  loss = 4.75112 avg_loss = 3.89483\n",
      "epoch no.0 train no.71930  loss = 3.50065 avg_loss = 3.94933\n",
      "epoch no.0 train no.71940  loss = 3.76013 avg_loss = 3.94112\n",
      "epoch no.0 train no.71950  loss = 2.65668 avg_loss = 3.95427\n",
      "epoch no.0 train no.71960  loss = 4.90192 avg_loss = 3.98219\n",
      "epoch no.0 train no.71970  loss = 4.48956 avg_loss = 4.02583\n",
      "epoch no.0 train no.71980  loss = 3.94885 avg_loss = 4.02070\n",
      "epoch no.0 train no.71990  loss = 4.72444 avg_loss = 4.06787\n",
      "epoch no.0 train no.72000  loss = 6.15375 avg_loss = 4.07847\n",
      "2\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명']\n",
      "추억의 발라드</s>\n",
      "epoch no.0 train no.72010  loss = 2.77867 avg_loss = 4.04796\n",
      "epoch no.0 train no.72020  loss = 5.59338 avg_loss = 4.00224\n",
      "epoch no.0 train no.72030  loss = 4.61544 avg_loss = 4.05717\n",
      "epoch no.0 train no.72040  loss = 5.08817 avg_loss = 4.07751\n",
      "epoch no.0 train no.72050  loss = 3.03913 avg_loss = 4.02974\n",
      "epoch no.0 train no.72060  loss = 3.52107 avg_loss = 4.04897\n",
      "epoch no.0 train no.72070  loss = 4.15702 avg_loss = 4.03443\n",
      "epoch no.0 train no.72080  loss = 6.11790 avg_loss = 3.97702\n",
      "epoch no.0 train no.72090  loss = 1.86605 avg_loss = 3.93691\n",
      "epoch no.0 train no.72100  loss = 5.31495 avg_loss = 3.94139\n",
      "epoch no.0 train no.72110  loss = 3.47455 avg_loss = 3.89650\n",
      "epoch no.0 train no.72120  loss = 3.82285 avg_loss = 3.81150\n",
      "epoch no.0 train no.72130  loss = 3.46218 avg_loss = 3.81446\n",
      "epoch no.0 train no.72140  loss = 3.44352 avg_loss = 3.88618\n",
      "epoch no.0 train no.72150  loss = 4.22924 avg_loss = 3.91154\n",
      "epoch no.0 train no.72160  loss = 2.65880 avg_loss = 3.90361\n",
      "epoch no.0 train no.72170  loss = 4.02161 avg_loss = 3.94531\n",
      "epoch no.0 train no.72180  loss = 4.49426 avg_loss = 3.99460\n",
      "epoch no.0 train no.72190  loss = 2.61455 avg_loss = 3.98437\n",
      "epoch no.0 train no.72200  loss = 2.95242 avg_loss = 3.93424\n",
      "epoch no.0 train no.72210  loss = 4.00101 avg_loss = 3.89922\n",
      "epoch no.0 train no.72220  loss = 2.39213 avg_loss = 3.84034\n",
      "epoch no.0 train no.72230  loss = 5.44334 avg_loss = 3.84487\n",
      "epoch no.0 train no.72240  loss = 6.08113 avg_loss = 3.94173\n",
      "epoch no.0 train no.72250  loss = 4.96806 avg_loss = 3.98472\n",
      "epoch no.0 train no.72260  loss = 4.08902 avg_loss = 3.90272\n",
      "epoch no.0 train no.72270  loss = 4.33105 avg_loss = 3.94788\n",
      "epoch no.0 train no.72280  loss = 2.66730 avg_loss = 3.86867\n",
      "epoch no.0 train no.72290  loss = 2.80018 avg_loss = 3.84593\n",
      "epoch no.0 train no.72300  loss = 2.41735 avg_loss = 3.84894\n",
      "epoch no.0 train no.72310  loss = 3.66094 avg_loss = 3.82605\n",
      "epoch no.0 train no.72320  loss = 1.78613 avg_loss = 3.81342\n",
      "epoch no.0 train no.72330  loss = 6.14786 avg_loss = 3.83822\n",
      "epoch no.0 train no.72340  loss = 4.11724 avg_loss = 3.83419\n",
      "epoch no.0 train no.72350  loss = 4.24606 avg_loss = 3.82293\n",
      "epoch no.0 train no.72360  loss = 6.85746 avg_loss = 3.87025\n",
      "epoch no.0 train no.72370  loss = 2.77479 avg_loss = 3.86364\n",
      "epoch no.0 train no.72380  loss = 4.52414 avg_loss = 3.90302\n",
      "epoch no.0 train no.72390  loss = 2.48541 avg_loss = 3.88555\n",
      "epoch no.0 train no.72400  loss = 2.84640 avg_loss = 3.87151\n",
      "epoch no.0 train no.72410  loss = 3.58896 avg_loss = 3.90162\n",
      "epoch no.0 train no.72420  loss = 3.06792 avg_loss = 3.86289\n",
      "epoch no.0 train no.72430  loss = 2.68853 avg_loss = 3.85674\n",
      "epoch no.0 train no.72440  loss = 4.51529 avg_loss = 3.86213\n",
      "epoch no.0 train no.72450  loss = 7.26305 avg_loss = 3.87079\n",
      "epoch no.0 train no.72460  loss = 5.04666 avg_loss = 3.90171\n",
      "epoch no.0 train no.72470  loss = 3.14697 avg_loss = 3.86438\n",
      "epoch no.0 train no.72480  loss = 2.21507 avg_loss = 3.86444\n",
      "epoch no.0 train no.72490  loss = 2.48275 avg_loss = 3.83066\n",
      "epoch no.0 train no.72500  loss = 4.31296 avg_loss = 3.83091\n",
      "epoch no.0 train no.72510  loss = 3.92473 avg_loss = 3.82372\n",
      "epoch no.0 train no.72520  loss = 3.71640 avg_loss = 3.80749\n",
      "epoch no.0 train no.72530  loss = 3.34529 avg_loss = 3.82182\n",
      "epoch no.0 train no.72540  loss = 3.00402 avg_loss = 3.79686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0 train no.72550  loss = 4.02167 avg_loss = 3.76452\n",
      "epoch no.0 train no.72560  loss = 2.65013 avg_loss = 3.79384\n",
      "epoch no.0 train no.72570  loss = 5.16944 avg_loss = 3.79029\n",
      "epoch no.0 train no.72580  loss = 2.72113 avg_loss = 3.83413\n",
      "epoch no.0 train no.72590  loss = 2.17372 avg_loss = 3.85801\n",
      "epoch no.0 train no.72600  loss = 3.73117 avg_loss = 3.89812\n",
      "epoch no.0 train no.72610  loss = 2.49358 avg_loss = 3.80500\n",
      "epoch no.0 train no.72620  loss = 4.23413 avg_loss = 3.80620\n",
      "epoch no.0 train no.72630  loss = 3.81555 avg_loss = 3.79166\n",
      "epoch no.0 train no.72640  loss = 6.32133 avg_loss = 3.87453\n",
      "epoch no.0 train no.72650  loss = 5.58835 avg_loss = 3.89069\n",
      "epoch no.0 train no.72660  loss = 4.81131 avg_loss = 3.93730\n",
      "epoch no.0 train no.72670  loss = 5.40914 avg_loss = 3.97017\n",
      "epoch no.0 train no.72680  loss = 3.75666 avg_loss = 4.00274\n",
      "epoch no.0 train no.72690  loss = 4.82645 avg_loss = 4.03152\n",
      "epoch no.0 train no.72700  loss = 4.58967 avg_loss = 4.05407\n",
      "epoch no.0 train no.72710  loss = 4.06670 avg_loss = 4.02565\n",
      "epoch no.0 train no.72720  loss = 4.17882 avg_loss = 4.05785\n",
      "epoch no.0 train no.72730  loss = 7.52667 avg_loss = 4.03102\n",
      "epoch no.0 train no.72740  loss = 2.21456 avg_loss = 3.98834\n",
      "epoch no.0 train no.72750  loss = 4.33221 avg_loss = 3.93908\n",
      "epoch no.0 train no.72760  loss = 5.68886 avg_loss = 3.95935\n",
      "epoch no.0 train no.72770  loss = 3.20734 avg_loss = 3.94789\n",
      "epoch no.0 train no.72780  loss = 4.92645 avg_loss = 3.98962\n",
      "epoch no.0 train no.72790  loss = 4.18909 avg_loss = 3.97991\n",
      "epoch no.0 train no.72800  loss = 3.36028 avg_loss = 3.92304\n",
      "epoch no.0 train no.72810  loss = 5.37224 avg_loss = 3.87393\n",
      "epoch no.0 train no.72820  loss = 5.20878 avg_loss = 3.88639\n",
      "epoch no.0 train no.72830  loss = 3.77460 avg_loss = 3.91479\n",
      "epoch no.0 train no.72840  loss = 3.78825 avg_loss = 3.86806\n",
      "epoch no.0 train no.72850  loss = 5.62269 avg_loss = 3.87141\n",
      "epoch no.0 train no.72860  loss = 3.47984 avg_loss = 3.86884\n",
      "epoch no.0 train no.72870  loss = 3.54277 avg_loss = 3.90455\n",
      "epoch no.0 train no.72880  loss = 3.26305 avg_loss = 3.86075\n",
      "epoch no.0 train no.72890  loss = 4.72448 avg_loss = 3.82573\n",
      "epoch no.0 train no.72900  loss = 6.29342 avg_loss = 3.81554\n",
      "epoch no.0 train no.72910  loss = 4.05372 avg_loss = 3.86604\n",
      "epoch no.0 train no.72920  loss = 3.58220 avg_loss = 3.80964\n",
      "epoch no.0 train no.72930  loss = 1.30599 avg_loss = 3.74447\n",
      "epoch no.0 train no.72940  loss = 3.98496 avg_loss = 3.70585\n",
      "epoch no.0 train no.72950  loss = 4.68985 avg_loss = 3.71732\n",
      "epoch no.0 train no.72960  loss = 3.69686 avg_loss = 3.75084\n",
      "epoch no.0 train no.72970  loss = 2.54980 avg_loss = 3.80014\n",
      "epoch no.0 train no.72980  loss = 3.77055 avg_loss = 3.75890\n",
      "epoch no.0 train no.72990  loss = 4.41389 avg_loss = 3.83011\n",
      "epoch no.0 train no.73000  loss = 2.45026 avg_loss = 3.76347\n",
      "3\n",
      "to_tokens: ['▁비', '▁가요', '▁o', 'st', '</s>']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.0 train no.73010  loss = 2.93152 avg_loss = 3.76382\n",
      "epoch no.0 train no.73020  loss = 6.30310 avg_loss = 3.79386\n",
      "epoch no.0 train no.73030  loss = 3.72970 avg_loss = 3.84523\n",
      "epoch no.0 train no.73040  loss = 3.41340 avg_loss = 3.85668\n",
      "epoch no.0 train no.73050  loss = 3.34018 avg_loss = 3.87725\n",
      "epoch no.0 train no.73060  loss = 3.75614 avg_loss = 3.84473\n",
      "epoch no.0 train no.73070  loss = 5.28638 avg_loss = 3.90827\n",
      "epoch no.0 train no.73080  loss = 2.78440 avg_loss = 3.81071\n",
      "epoch no.0 train no.73090  loss = 2.80330 avg_loss = 3.79399\n",
      "epoch no.0 train no.73100  loss = 4.17279 avg_loss = 3.79106\n",
      "epoch no.0 train no.73110  loss = 4.19913 avg_loss = 3.82259\n",
      "epoch no.0 train no.73120  loss = 3.48205 avg_loss = 3.81308\n",
      "epoch no.0 train no.73130  loss = 4.20055 avg_loss = 3.89504\n",
      "epoch no.0 train no.73140  loss = 4.52387 avg_loss = 3.90043\n",
      "epoch no.0 train no.73150  loss = 3.40985 avg_loss = 3.92929\n",
      "epoch no.0 train no.73160  loss = 6.13926 avg_loss = 3.92311\n",
      "epoch no.0 train no.73170  loss = 3.37923 avg_loss = 3.91772\n",
      "epoch no.0 train no.73180  loss = 3.57372 avg_loss = 3.89324\n",
      "epoch no.0 train no.73190  loss = 2.04545 avg_loss = 3.85171\n",
      "epoch no.0 train no.73200  loss = 2.77989 avg_loss = 3.84072\n",
      "epoch no.0 train no.73210  loss = 2.20718 avg_loss = 3.80293\n",
      "epoch no.0 train no.73220  loss = 2.79151 avg_loss = 3.77971\n",
      "epoch no.0 train no.73230  loss = 2.85445 avg_loss = 3.75064\n",
      "epoch no.0 train no.73240  loss = 3.27976 avg_loss = 3.72245\n",
      "epoch no.0 train no.73250  loss = 2.83623 avg_loss = 3.70210\n",
      "epoch no.0 train no.73260  loss = 3.33044 avg_loss = 3.73702\n",
      "epoch no.0 train no.73270  loss = 2.37266 avg_loss = 3.74676\n",
      "epoch no.0 train no.73280  loss = 3.94991 avg_loss = 3.76561\n",
      "epoch no.0 train no.73290  loss = 4.32412 avg_loss = 3.69865\n",
      "epoch no.0 train no.73300  loss = 5.71886 avg_loss = 3.71103\n",
      "epoch no.0 train no.73310  loss = 3.55605 avg_loss = 3.73029\n",
      "epoch no.0 train no.73320  loss = 4.61337 avg_loss = 3.84300\n",
      "epoch no.0 train no.73330  loss = 7.33769 avg_loss = 3.84346\n",
      "epoch no.0 train no.73340  loss = 4.27802 avg_loss = 3.85488\n",
      "epoch no.0 train no.73350  loss = 5.79589 avg_loss = 3.85111\n",
      "epoch no.1 train no.73360  loss = 4.53117 avg_loss = 3.85215\n",
      "epoch no.1 train no.73370  loss = 5.00956 avg_loss = 3.82926\n",
      "epoch no.1 train no.73380  loss = 3.19178 avg_loss = 3.78651\n",
      "epoch no.1 train no.73390  loss = 2.98261 avg_loss = 3.79736\n",
      "epoch no.1 train no.73400  loss = 2.04194 avg_loss = 3.74920\n",
      "epoch no.1 train no.73410  loss = 2.57529 avg_loss = 3.69568\n",
      "epoch no.1 train no.73420  loss = 3.03622 avg_loss = 3.71208\n",
      "epoch no.1 train no.73430  loss = 3.47684 avg_loss = 3.68807\n",
      "epoch no.1 train no.73440  loss = 5.48518 avg_loss = 3.69282\n",
      "epoch no.1 train no.73450  loss = 3.62054 avg_loss = 3.70511\n",
      "epoch no.1 train no.73460  loss = 2.97800 avg_loss = 3.70592\n",
      "epoch no.1 train no.73470  loss = 4.89056 avg_loss = 3.66245\n",
      "epoch no.1 train no.73480  loss = 2.87563 avg_loss = 3.66254\n",
      "epoch no.1 train no.73490  loss = 3.87096 avg_loss = 3.67802\n",
      "epoch no.1 train no.73500  loss = 4.89604 avg_loss = 3.70010\n",
      "epoch no.1 train no.73510  loss = 4.29128 avg_loss = 3.69725\n",
      "epoch no.1 train no.73520  loss = 2.10067 avg_loss = 3.68042\n",
      "epoch no.1 train no.73530  loss = 3.08863 avg_loss = 3.68737\n",
      "epoch no.1 train no.73540  loss = 5.79474 avg_loss = 3.67360\n",
      "epoch no.1 train no.73550  loss = 5.13626 avg_loss = 3.67623\n",
      "epoch no.1 train no.73560  loss = 3.00823 avg_loss = 3.68027\n",
      "epoch no.1 train no.73570  loss = 6.15104 avg_loss = 3.71024\n",
      "epoch no.1 train no.73580  loss = 2.86578 avg_loss = 3.65896\n",
      "epoch no.1 train no.73590  loss = 3.30484 avg_loss = 3.61069\n",
      "epoch no.1 train no.73600  loss = 3.24660 avg_loss = 3.66534\n",
      "epoch no.1 train no.73610  loss = 3.15721 avg_loss = 3.64321\n",
      "epoch no.1 train no.73620  loss = 3.63661 avg_loss = 3.70907\n",
      "epoch no.1 train no.73630  loss = 4.54700 avg_loss = 3.68608\n",
      "epoch no.1 train no.73640  loss = 3.96075 avg_loss = 3.67630\n",
      "epoch no.1 train no.73650  loss = 4.12136 avg_loss = 3.65338\n",
      "epoch no.1 train no.73660  loss = 2.93286 avg_loss = 3.60256\n",
      "epoch no.1 train no.73670  loss = 5.07989 avg_loss = 3.60753\n",
      "epoch no.1 train no.73680  loss = 5.33793 avg_loss = 3.61498\n",
      "epoch no.1 train no.73690  loss = 2.68595 avg_loss = 3.62741\n",
      "epoch no.1 train no.73700  loss = 2.61339 avg_loss = 3.59003\n",
      "epoch no.1 train no.73710  loss = 3.86259 avg_loss = 3.61170\n",
      "epoch no.1 train no.73720  loss = 4.07532 avg_loss = 3.64890\n",
      "epoch no.1 train no.73730  loss = 5.09150 avg_loss = 3.67496\n",
      "epoch no.1 train no.73740  loss = 3.65345 avg_loss = 3.66227\n",
      "epoch no.1 train no.73750  loss = 2.49316 avg_loss = 3.67888\n",
      "epoch no.1 train no.73760  loss = 4.54157 avg_loss = 3.63728\n",
      "epoch no.1 train no.73770  loss = 3.31615 avg_loss = 3.62227\n",
      "epoch no.1 train no.73780  loss = 4.45841 avg_loss = 3.62334\n",
      "epoch no.1 train no.73790  loss = 4.23240 avg_loss = 3.63310\n",
      "epoch no.1 train no.73800  loss = 3.63858 avg_loss = 3.61088\n",
      "epoch no.1 train no.73810  loss = 5.51170 avg_loss = 3.62352\n",
      "epoch no.1 train no.73820  loss = 5.34482 avg_loss = 3.62713\n",
      "epoch no.1 train no.73830  loss = 3.63197 avg_loss = 3.65566\n",
      "epoch no.1 train no.73840  loss = 5.84551 avg_loss = 3.63731\n",
      "epoch no.1 train no.73850  loss = 1.90466 avg_loss = 3.62970\n",
      "epoch no.1 train no.73860  loss = 4.96543 avg_loss = 3.68292\n",
      "epoch no.1 train no.73870  loss = 3.17592 avg_loss = 3.70448\n",
      "epoch no.1 train no.73880  loss = 2.57318 avg_loss = 3.65438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.73890  loss = 5.05783 avg_loss = 3.64845\n",
      "epoch no.1 train no.73900  loss = 3.82628 avg_loss = 3.58798\n",
      "epoch no.1 train no.73910  loss = 4.32371 avg_loss = 3.59798\n",
      "epoch no.1 train no.73920  loss = 3.18656 avg_loss = 3.54824\n",
      "epoch no.1 train no.73930  loss = 2.92691 avg_loss = 3.58533\n",
      "epoch no.1 train no.73940  loss = 3.40217 avg_loss = 3.60423\n",
      "epoch no.1 train no.73950  loss = 3.36277 avg_loss = 3.55917\n",
      "epoch no.1 train no.73960  loss = 2.76182 avg_loss = 3.59613\n",
      "epoch no.1 train no.73970  loss = 3.94482 avg_loss = 3.59931\n",
      "epoch no.1 train no.73980  loss = 2.12753 avg_loss = 3.58411\n",
      "epoch no.1 train no.73990  loss = 2.91906 avg_loss = 3.56021\n",
      "epoch no.1 train no.74000  loss = 2.14330 avg_loss = 3.49399\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '▁o', 'st', '▁명', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.1 train no.74010  loss = 3.03730 avg_loss = 3.50703\n",
      "epoch no.1 train no.74020  loss = 3.03422 avg_loss = 3.51425\n",
      "epoch no.1 train no.74030  loss = 3.36457 avg_loss = 3.53160\n",
      "epoch no.1 train no.74040  loss = 1.76400 avg_loss = 3.50846\n",
      "epoch no.1 train no.74050  loss = 3.59402 avg_loss = 3.51101\n",
      "epoch no.1 train no.74060  loss = 3.29969 avg_loss = 3.49546\n",
      "epoch no.1 train no.74070  loss = 4.34112 avg_loss = 3.51775\n",
      "epoch no.1 train no.74080  loss = 4.15497 avg_loss = 3.56503\n",
      "epoch no.1 train no.74090  loss = 3.77343 avg_loss = 3.60590\n",
      "epoch no.1 train no.74100  loss = 3.51540 avg_loss = 3.57767\n",
      "epoch no.1 train no.74110  loss = 4.54157 avg_loss = 3.56222\n",
      "epoch no.1 train no.74120  loss = 2.71217 avg_loss = 3.52853\n",
      "epoch no.1 train no.74130  loss = 3.90109 avg_loss = 3.57784\n",
      "epoch no.1 train no.74140  loss = 3.21662 avg_loss = 3.57319\n",
      "epoch no.1 train no.74150  loss = 4.61046 avg_loss = 3.58867\n",
      "epoch no.1 train no.74160  loss = 2.19135 avg_loss = 3.56272\n",
      "epoch no.1 train no.74170  loss = 4.26053 avg_loss = 3.62554\n",
      "epoch no.1 train no.74180  loss = 3.13608 avg_loss = 3.65663\n",
      "epoch no.1 train no.74190  loss = 6.71516 avg_loss = 3.78382\n",
      "epoch no.1 train no.74200  loss = 2.95672 avg_loss = 3.75060\n",
      "epoch no.1 train no.74210  loss = 3.67533 avg_loss = 3.67151\n",
      "epoch no.1 train no.74220  loss = 3.89303 avg_loss = 3.66973\n",
      "epoch no.1 train no.74230  loss = 2.07242 avg_loss = 3.69446\n",
      "epoch no.1 train no.74240  loss = 3.47378 avg_loss = 3.68175\n",
      "epoch no.1 train no.74250  loss = 4.28097 avg_loss = 3.68932\n",
      "epoch no.1 train no.74260  loss = 3.33356 avg_loss = 3.62530\n",
      "epoch no.1 train no.74270  loss = 2.79745 avg_loss = 3.59016\n",
      "epoch no.1 train no.74280  loss = 2.62302 avg_loss = 3.55745\n",
      "epoch no.1 train no.74290  loss = 4.95121 avg_loss = 3.57825\n",
      "epoch no.1 train no.74300  loss = 2.65535 avg_loss = 3.60324\n",
      "epoch no.1 train no.74310  loss = 3.18532 avg_loss = 3.67082\n",
      "epoch no.1 train no.74320  loss = 1.96193 avg_loss = 3.65912\n",
      "epoch no.1 train no.74330  loss = 4.98811 avg_loss = 3.63930\n",
      "epoch no.1 train no.74340  loss = 2.70160 avg_loss = 3.63800\n",
      "epoch no.1 train no.74350  loss = 4.02029 avg_loss = 3.64382\n",
      "epoch no.1 train no.74360  loss = 4.30307 avg_loss = 3.66631\n",
      "epoch no.1 train no.74370  loss = 3.51352 avg_loss = 3.60351\n",
      "epoch no.1 train no.74380  loss = 5.30047 avg_loss = 3.67555\n",
      "epoch no.1 train no.74390  loss = 5.10990 avg_loss = 3.68901\n",
      "epoch no.1 train no.74400  loss = 1.98488 avg_loss = 3.64410\n",
      "epoch no.1 train no.74410  loss = 4.02549 avg_loss = 3.59859\n",
      "epoch no.1 train no.74420  loss = 4.38228 avg_loss = 3.58370\n",
      "epoch no.1 train no.74430  loss = 3.16763 avg_loss = 3.51345\n",
      "epoch no.1 train no.74440  loss = 3.71373 avg_loss = 3.56745\n",
      "epoch no.1 train no.74450  loss = 3.89674 avg_loss = 3.54909\n",
      "epoch no.1 train no.74460  loss = 4.89646 avg_loss = 3.58011\n",
      "epoch no.1 train no.74470  loss = 4.73605 avg_loss = 3.59311\n",
      "epoch no.1 train no.74480  loss = 5.52047 avg_loss = 3.62969\n",
      "epoch no.1 train no.74490  loss = 3.17172 avg_loss = 3.63801\n",
      "epoch no.1 train no.74500  loss = 2.56223 avg_loss = 3.69049\n",
      "epoch no.1 train no.74510  loss = 2.18526 avg_loss = 3.63396\n",
      "epoch no.1 train no.74520  loss = 3.22275 avg_loss = 3.63577\n",
      "epoch no.1 train no.74530  loss = 2.14309 avg_loss = 3.63749\n",
      "epoch no.1 train no.74540  loss = 4.00442 avg_loss = 3.59965\n",
      "epoch no.1 train no.74550  loss = 2.97568 avg_loss = 3.58627\n",
      "epoch no.1 train no.74560  loss = 2.25598 avg_loss = 3.54447\n",
      "epoch no.1 train no.74570  loss = 4.17879 avg_loss = 3.56738\n",
      "epoch no.1 train no.74580  loss = 5.26495 avg_loss = 3.59320\n",
      "epoch no.1 train no.74590  loss = 3.97377 avg_loss = 3.60656\n",
      "epoch no.1 train no.74600  loss = 3.47414 avg_loss = 3.61089\n",
      "epoch no.1 train no.74610  loss = 4.24947 avg_loss = 3.63408\n",
      "epoch no.1 train no.74620  loss = 2.10214 avg_loss = 3.63718\n",
      "epoch no.1 train no.74630  loss = 4.68822 avg_loss = 3.65794\n",
      "epoch no.1 train no.74640  loss = 1.74814 avg_loss = 3.62153\n",
      "epoch no.1 train no.74650  loss = 3.51149 avg_loss = 3.63257\n",
      "epoch no.1 train no.74660  loss = 3.41574 avg_loss = 3.64256\n",
      "epoch no.1 train no.74670  loss = 5.96490 avg_loss = 3.71308\n",
      "epoch no.1 train no.74680  loss = 4.34015 avg_loss = 3.72769\n",
      "epoch no.1 train no.74690  loss = 2.74714 avg_loss = 3.68683\n",
      "epoch no.1 train no.74700  loss = 4.47276 avg_loss = 3.62756\n",
      "epoch no.1 train no.74710  loss = 4.22567 avg_loss = 3.59802\n",
      "epoch no.1 train no.74720  loss = 3.26548 avg_loss = 3.60449\n",
      "epoch no.1 train no.74730  loss = 1.98004 avg_loss = 3.54113\n",
      "epoch no.1 train no.74740  loss = 5.42198 avg_loss = 3.54701\n",
      "epoch no.1 train no.74750  loss = 4.50313 avg_loss = 3.57569\n",
      "epoch no.1 train no.74760  loss = 4.93862 avg_loss = 3.57820\n",
      "epoch no.1 train no.74770  loss = 2.86788 avg_loss = 3.61937\n",
      "epoch no.1 train no.74780  loss = 1.97362 avg_loss = 3.65584\n",
      "epoch no.1 train no.74790  loss = 3.88064 avg_loss = 3.59775\n",
      "epoch no.1 train no.74800  loss = 3.21081 avg_loss = 3.62433\n",
      "epoch no.1 train no.74810  loss = 4.77333 avg_loss = 3.64455\n",
      "epoch no.1 train no.74820  loss = 1.52209 avg_loss = 3.68147\n",
      "epoch no.1 train no.74830  loss = 3.41962 avg_loss = 3.68296\n",
      "epoch no.1 train no.74840  loss = 5.42786 avg_loss = 3.72338\n",
      "epoch no.1 train no.74850  loss = 2.34389 avg_loss = 3.69066\n",
      "epoch no.1 train no.74860  loss = 3.35668 avg_loss = 3.66924\n",
      "epoch no.1 train no.74870  loss = 2.51530 avg_loss = 3.62675\n",
      "epoch no.1 train no.74880  loss = 2.53943 avg_loss = 3.59867\n",
      "epoch no.1 train no.74890  loss = 4.91053 avg_loss = 3.64141\n",
      "epoch no.1 train no.74900  loss = 3.90006 avg_loss = 3.65751\n",
      "epoch no.1 train no.74910  loss = 5.55056 avg_loss = 3.68012\n",
      "epoch no.1 train no.74920  loss = 4.33474 avg_loss = 3.69502\n",
      "epoch no.1 train no.74930  loss = 2.94215 avg_loss = 3.71648\n",
      "epoch no.1 train no.74940  loss = 3.33912 avg_loss = 3.66888\n",
      "epoch no.1 train no.74950  loss = 5.65457 avg_loss = 3.64502\n",
      "epoch no.1 train no.74960  loss = 3.79690 avg_loss = 3.66498\n",
      "epoch no.1 train no.74970  loss = 2.55844 avg_loss = 3.68781\n",
      "epoch no.1 train no.74980  loss = 3.25221 avg_loss = 3.68454\n",
      "epoch no.1 train no.74990  loss = 2.21314 avg_loss = 3.66007\n",
      "epoch no.1 train no.75000  loss = 3.79063 avg_loss = 3.67947\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '송', '▁명', '</s>']\n",
      "추억의 팝송들</s>\n",
      "epoch no.1 train no.75010  loss = 3.29067 avg_loss = 3.63364\n",
      "epoch no.1 train no.75020  loss = 3.61660 avg_loss = 3.65648\n",
      "epoch no.1 train no.75030  loss = 5.21691 avg_loss = 3.71841\n",
      "epoch no.1 train no.75040  loss = 3.74296 avg_loss = 3.67211\n",
      "epoch no.1 train no.75050  loss = 5.84025 avg_loss = 3.69817\n",
      "epoch no.1 train no.75060  loss = 3.80876 avg_loss = 3.68152\n",
      "epoch no.1 train no.75070  loss = 3.85609 avg_loss = 3.65330\n",
      "epoch no.1 train no.75080  loss = 3.11081 avg_loss = 3.67760\n",
      "epoch no.1 train no.75090  loss = 3.03196 avg_loss = 3.65913\n",
      "epoch no.1 train no.75100  loss = 3.51830 avg_loss = 3.67910\n",
      "epoch no.1 train no.75110  loss = 2.60362 avg_loss = 3.61675\n",
      "epoch no.1 train no.75120  loss = 3.74063 avg_loss = 3.60547\n",
      "epoch no.1 train no.75130  loss = 3.06707 avg_loss = 3.57605\n",
      "epoch no.1 train no.75140  loss = 4.99717 avg_loss = 3.56497\n",
      "epoch no.1 train no.75150  loss = 4.54416 avg_loss = 3.58988\n",
      "epoch no.1 train no.75160  loss = 3.15406 avg_loss = 3.56913\n",
      "epoch no.1 train no.75170  loss = 2.37537 avg_loss = 3.53782\n",
      "epoch no.1 train no.75180  loss = 4.27066 avg_loss = 3.54609\n",
      "epoch no.1 train no.75190  loss = 4.28803 avg_loss = 3.55210\n",
      "epoch no.1 train no.75200  loss = 3.15903 avg_loss = 3.57857\n",
      "epoch no.1 train no.75210  loss = 3.05029 avg_loss = 3.55772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.75220  loss = 4.09584 avg_loss = 3.58662\n",
      "epoch no.1 train no.75230  loss = 2.45451 avg_loss = 3.55100\n",
      "epoch no.1 train no.75240  loss = 5.40860 avg_loss = 3.53011\n",
      "epoch no.1 train no.75250  loss = 2.94909 avg_loss = 3.54520\n",
      "epoch no.1 train no.75260  loss = 2.62979 avg_loss = 3.48536\n",
      "epoch no.1 train no.75270  loss = 2.43149 avg_loss = 3.51702\n",
      "epoch no.1 train no.75280  loss = 1.93599 avg_loss = 3.51436\n",
      "epoch no.1 train no.75290  loss = 5.41326 avg_loss = 3.49264\n",
      "epoch no.1 train no.75300  loss = 4.65472 avg_loss = 3.53637\n",
      "epoch no.1 train no.75310  loss = 4.21307 avg_loss = 3.56837\n",
      "epoch no.1 train no.75320  loss = 2.93895 avg_loss = 3.49927\n",
      "epoch no.1 train no.75330  loss = 4.99233 avg_loss = 3.48104\n",
      "epoch no.1 train no.75340  loss = 4.99449 avg_loss = 3.55598\n",
      "epoch no.1 train no.75350  loss = 4.88974 avg_loss = 3.60520\n",
      "epoch no.1 train no.75360  loss = 2.58387 avg_loss = 3.54755\n",
      "epoch no.1 train no.75370  loss = 2.06148 avg_loss = 3.55234\n",
      "epoch no.1 train no.75380  loss = 2.87986 avg_loss = 3.52813\n",
      "epoch no.1 train no.75390  loss = 4.32464 avg_loss = 3.56100\n",
      "epoch no.1 train no.75400  loss = 2.99284 avg_loss = 3.53283\n",
      "epoch no.1 train no.75410  loss = 3.78725 avg_loss = 3.54610\n",
      "epoch no.1 train no.75420  loss = 5.06354 avg_loss = 3.49032\n",
      "epoch no.1 train no.75430  loss = 2.56815 avg_loss = 3.48565\n",
      "epoch no.1 train no.75440  loss = 4.53847 avg_loss = 3.53678\n",
      "epoch no.1 train no.75450  loss = 3.47313 avg_loss = 3.48702\n",
      "epoch no.1 train no.75460  loss = 5.59793 avg_loss = 3.51323\n",
      "epoch no.1 train no.75470  loss = 6.07943 avg_loss = 3.51947\n",
      "epoch no.1 train no.75480  loss = 1.82003 avg_loss = 3.49686\n",
      "epoch no.1 train no.75490  loss = 2.76427 avg_loss = 3.49875\n",
      "epoch no.1 train no.75500  loss = 5.51971 avg_loss = 3.49901\n",
      "epoch no.1 train no.75510  loss = 4.92464 avg_loss = 3.49256\n",
      "epoch no.1 train no.75520  loss = 1.95621 avg_loss = 3.50227\n",
      "epoch no.1 train no.75530  loss = 2.37751 avg_loss = 3.47018\n",
      "epoch no.1 train no.75540  loss = 3.43820 avg_loss = 3.47077\n",
      "epoch no.1 train no.75550  loss = 4.48739 avg_loss = 3.45169\n",
      "epoch no.1 train no.75560  loss = 4.64443 avg_loss = 3.54918\n",
      "epoch no.1 train no.75570  loss = 2.64846 avg_loss = 3.57351\n",
      "epoch no.1 train no.75580  loss = 3.17054 avg_loss = 3.55941\n",
      "epoch no.1 train no.75590  loss = 1.95952 avg_loss = 3.54330\n",
      "epoch no.1 train no.75600  loss = 4.68690 avg_loss = 3.54910\n",
      "epoch no.1 train no.75610  loss = 4.15742 avg_loss = 3.55809\n",
      "epoch no.1 train no.75620  loss = 3.07070 avg_loss = 3.58611\n",
      "epoch no.1 train no.75630  loss = 2.81921 avg_loss = 3.56021\n",
      "epoch no.1 train no.75640  loss = 2.62625 avg_loss = 3.55437\n",
      "epoch no.1 train no.75650  loss = 2.24446 avg_loss = 3.56343\n",
      "epoch no.1 train no.75660  loss = 3.77299 avg_loss = 3.54661\n",
      "epoch no.1 train no.75670  loss = 3.68017 avg_loss = 3.53866\n",
      "epoch no.1 train no.75680  loss = 2.36983 avg_loss = 3.57425\n",
      "epoch no.1 train no.75690  loss = 3.43161 avg_loss = 3.53259\n",
      "epoch no.1 train no.75700  loss = 2.57900 avg_loss = 3.58107\n",
      "epoch no.1 train no.75710  loss = 2.73989 avg_loss = 3.57896\n",
      "epoch no.1 train no.75720  loss = 3.38299 avg_loss = 3.58849\n",
      "epoch no.1 train no.75730  loss = 3.56601 avg_loss = 3.58657\n",
      "epoch no.1 train no.75740  loss = 2.52762 avg_loss = 3.57342\n",
      "epoch no.1 train no.75750  loss = 4.59681 avg_loss = 3.56223\n",
      "epoch no.1 train no.75760  loss = 6.13826 avg_loss = 3.60681\n",
      "epoch no.1 train no.75770  loss = 2.90155 avg_loss = 3.63093\n",
      "epoch no.1 train no.75780  loss = 3.38310 avg_loss = 3.62711\n",
      "epoch no.1 train no.75790  loss = 2.57549 avg_loss = 3.62742\n",
      "epoch no.1 train no.75800  loss = 3.77031 avg_loss = 3.58894\n",
      "epoch no.1 train no.75810  loss = 4.80678 avg_loss = 3.62430\n",
      "epoch no.1 train no.75820  loss = 2.41586 avg_loss = 3.64816\n",
      "epoch no.1 train no.75830  loss = 6.09799 avg_loss = 3.60606\n",
      "epoch no.1 train no.75840  loss = 3.62790 avg_loss = 3.61383\n",
      "epoch no.1 train no.75850  loss = 3.16447 avg_loss = 3.56683\n",
      "epoch no.1 train no.75860  loss = 2.90634 avg_loss = 3.56983\n",
      "epoch no.1 train no.75870  loss = 4.20313 avg_loss = 3.56362\n",
      "epoch no.1 train no.75880  loss = 2.97083 avg_loss = 3.58297\n",
      "epoch no.1 train no.75890  loss = 2.78212 avg_loss = 3.63293\n",
      "epoch no.1 train no.75900  loss = 2.81901 avg_loss = 3.64109\n",
      "epoch no.1 train no.75910  loss = 1.76380 avg_loss = 3.62428\n",
      "epoch no.1 train no.75920  loss = 3.19321 avg_loss = 3.57774\n",
      "epoch no.1 train no.75930  loss = 4.02986 avg_loss = 3.57483\n",
      "epoch no.1 train no.75940  loss = 2.70709 avg_loss = 3.54346\n",
      "epoch no.1 train no.75950  loss = 3.51847 avg_loss = 3.55174\n",
      "epoch no.1 train no.75960  loss = 3.63498 avg_loss = 3.53748\n",
      "epoch no.1 train no.75970  loss = 5.97977 avg_loss = 3.58399\n",
      "epoch no.1 train no.75980  loss = 8.34934 avg_loss = 3.61865\n",
      "epoch no.1 train no.75990  loss = 3.50818 avg_loss = 3.59479\n",
      "epoch no.1 train no.76000  loss = 3.83901 avg_loss = 3.66457\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁명']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.1 train no.76010  loss = 4.81954 avg_loss = 3.67812\n",
      "epoch no.1 train no.76020  loss = 1.32126 avg_loss = 3.62496\n",
      "epoch no.1 train no.76030  loss = 3.43771 avg_loss = 3.62601\n",
      "epoch no.1 train no.76040  loss = 4.37432 avg_loss = 3.67429\n",
      "epoch no.1 train no.76050  loss = 4.85223 avg_loss = 3.72455\n",
      "epoch no.1 train no.76060  loss = 3.18157 avg_loss = 3.72812\n",
      "epoch no.1 train no.76070  loss = 4.83850 avg_loss = 3.72225\n",
      "epoch no.1 train no.76080  loss = 4.29015 avg_loss = 3.69092\n",
      "epoch no.1 train no.76090  loss = 2.45608 avg_loss = 3.69880\n",
      "epoch no.1 train no.76100  loss = 5.06328 avg_loss = 3.65300\n",
      "epoch no.1 train no.76110  loss = 3.00354 avg_loss = 3.60763\n",
      "epoch no.1 train no.76120  loss = 4.23177 avg_loss = 3.60887\n",
      "epoch no.1 train no.76130  loss = 3.35584 avg_loss = 3.66688\n",
      "epoch no.1 train no.76140  loss = 3.76226 avg_loss = 3.69988\n",
      "epoch no.1 train no.76150  loss = 3.05400 avg_loss = 3.71654\n",
      "epoch no.1 train no.76160  loss = 5.09832 avg_loss = 3.74562\n",
      "epoch no.1 train no.76170  loss = 3.33565 avg_loss = 3.72280\n",
      "epoch no.1 train no.76180  loss = 3.87834 avg_loss = 3.78848\n",
      "epoch no.1 train no.76190  loss = 5.57628 avg_loss = 3.76797\n",
      "epoch no.1 train no.76200  loss = 4.30656 avg_loss = 3.80324\n",
      "epoch no.1 train no.76210  loss = 2.91479 avg_loss = 3.74378\n",
      "epoch no.1 train no.76220  loss = 2.30442 avg_loss = 3.69336\n",
      "epoch no.1 train no.76230  loss = 4.14470 avg_loss = 3.74613\n",
      "epoch no.1 train no.76240  loss = 3.18208 avg_loss = 3.72142\n",
      "epoch no.1 train no.76250  loss = 4.76384 avg_loss = 3.74976\n",
      "epoch no.1 train no.76260  loss = 3.12622 avg_loss = 3.69775\n",
      "epoch no.1 train no.76270  loss = 1.99476 avg_loss = 3.66297\n",
      "epoch no.1 train no.76280  loss = 3.70972 avg_loss = 3.68647\n",
      "epoch no.1 train no.76290  loss = 5.37709 avg_loss = 3.70558\n",
      "epoch no.1 train no.76300  loss = 2.64624 avg_loss = 3.67061\n",
      "epoch no.1 train no.76310  loss = 3.09007 avg_loss = 3.69886\n",
      "epoch no.1 train no.76320  loss = 2.80533 avg_loss = 3.67907\n",
      "epoch no.1 train no.76330  loss = 3.71886 avg_loss = 3.71353\n",
      "epoch no.1 train no.76340  loss = 2.33199 avg_loss = 3.70284\n",
      "epoch no.1 train no.76350  loss = 2.95082 avg_loss = 3.67679\n",
      "epoch no.1 train no.76360  loss = 3.66208 avg_loss = 3.67550\n",
      "epoch no.1 train no.76370  loss = 3.01606 avg_loss = 3.66866\n",
      "epoch no.1 train no.76380  loss = 4.23933 avg_loss = 3.66426\n",
      "epoch no.1 train no.76390  loss = 4.47713 avg_loss = 3.64186\n",
      "epoch no.1 train no.76400  loss = 5.51345 avg_loss = 3.63373\n",
      "epoch no.1 train no.76410  loss = 3.25674 avg_loss = 3.64119\n",
      "epoch no.1 train no.76420  loss = 3.57724 avg_loss = 3.64247\n",
      "epoch no.1 train no.76430  loss = 2.10739 avg_loss = 3.62298\n",
      "epoch no.1 train no.76440  loss = 3.17492 avg_loss = 3.61762\n",
      "epoch no.1 train no.76450  loss = 5.02061 avg_loss = 3.65003\n",
      "epoch no.1 train no.76460  loss = 2.29979 avg_loss = 3.62603\n",
      "epoch no.1 train no.76470  loss = 4.22849 avg_loss = 3.62920\n",
      "epoch no.1 train no.76480  loss = 4.50114 avg_loss = 3.68874\n",
      "epoch no.1 train no.76490  loss = 3.89530 avg_loss = 3.69942\n",
      "epoch no.1 train no.76500  loss = 3.12517 avg_loss = 3.68395\n",
      "epoch no.1 train no.76510  loss = 5.38603 avg_loss = 3.72298\n",
      "epoch no.1 train no.76520  loss = 2.61789 avg_loss = 3.69445\n",
      "epoch no.1 train no.76530  loss = 2.49697 avg_loss = 3.67221\n",
      "epoch no.1 train no.76540  loss = 3.31705 avg_loss = 3.67864\n",
      "epoch no.1 train no.76550  loss = 5.10941 avg_loss = 3.70536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.76560  loss = 2.64836 avg_loss = 3.61816\n",
      "epoch no.1 train no.76570  loss = 3.63204 avg_loss = 3.59957\n",
      "epoch no.1 train no.76580  loss = 3.39590 avg_loss = 3.64795\n",
      "epoch no.1 train no.76590  loss = 5.29636 avg_loss = 3.63981\n",
      "epoch no.1 train no.76600  loss = 2.10968 avg_loss = 3.68531\n",
      "epoch no.1 train no.76610  loss = 3.56096 avg_loss = 3.68341\n",
      "epoch no.1 train no.76620  loss = 2.75940 avg_loss = 3.66775\n",
      "epoch no.1 train no.76630  loss = 4.16191 avg_loss = 3.69037\n",
      "epoch no.1 train no.76640  loss = 2.79221 avg_loss = 3.70518\n",
      "epoch no.1 train no.76650  loss = 1.80182 avg_loss = 3.62339\n",
      "epoch no.1 train no.76660  loss = 5.99918 avg_loss = 3.65928\n",
      "epoch no.1 train no.76670  loss = 3.97937 avg_loss = 3.67038\n",
      "epoch no.1 train no.76680  loss = 4.97261 avg_loss = 3.69494\n",
      "epoch no.1 train no.76690  loss = 3.92078 avg_loss = 3.69513\n",
      "epoch no.1 train no.76700  loss = 3.97255 avg_loss = 3.63994\n",
      "epoch no.1 train no.76710  loss = 2.60067 avg_loss = 3.67354\n",
      "epoch no.1 train no.76720  loss = 5.00963 avg_loss = 3.65802\n",
      "epoch no.1 train no.76730  loss = 2.69207 avg_loss = 3.62503\n",
      "epoch no.1 train no.76740  loss = 3.62845 avg_loss = 3.60142\n",
      "epoch no.1 train no.76750  loss = 4.40121 avg_loss = 3.60817\n",
      "epoch no.1 train no.76760  loss = 5.11527 avg_loss = 3.59857\n",
      "epoch no.1 train no.76770  loss = 3.16563 avg_loss = 3.57853\n",
      "epoch no.1 train no.76780  loss = 2.49828 avg_loss = 3.61745\n",
      "epoch no.1 train no.76790  loss = 4.73900 avg_loss = 3.63725\n",
      "epoch no.1 train no.76800  loss = 6.02169 avg_loss = 3.63252\n",
      "epoch no.1 train no.76810  loss = 3.04037 avg_loss = 3.63790\n",
      "epoch no.1 train no.76820  loss = 3.36586 avg_loss = 3.62323\n",
      "epoch no.1 train no.76830  loss = 3.30056 avg_loss = 3.62252\n",
      "epoch no.1 train no.76840  loss = 4.99401 avg_loss = 3.59496\n",
      "epoch no.1 train no.76850  loss = 2.92084 avg_loss = 3.57513\n",
      "epoch no.1 train no.76860  loss = 4.56922 avg_loss = 3.53583\n",
      "epoch no.1 train no.76870  loss = 2.76801 avg_loss = 3.58376\n",
      "epoch no.1 train no.76880  loss = 4.18270 avg_loss = 3.63500\n",
      "epoch no.1 train no.76890  loss = 2.43424 avg_loss = 3.59066\n",
      "epoch no.1 train no.76900  loss = 4.53395 avg_loss = 3.65992\n",
      "epoch no.1 train no.76910  loss = 2.97338 avg_loss = 3.61933\n",
      "epoch no.1 train no.76920  loss = 2.90115 avg_loss = 3.63950\n",
      "epoch no.1 train no.76930  loss = 4.94913 avg_loss = 3.59909\n",
      "epoch no.1 train no.76940  loss = 4.24867 avg_loss = 3.59965\n",
      "epoch no.1 train no.76950  loss = 2.54034 avg_loss = 3.61040\n",
      "epoch no.1 train no.76960  loss = 3.25523 avg_loss = 3.59520\n",
      "epoch no.1 train no.76970  loss = 2.57105 avg_loss = 3.59819\n",
      "epoch no.1 train no.76980  loss = 4.16969 avg_loss = 3.63561\n",
      "epoch no.1 train no.76990  loss = 1.43964 avg_loss = 3.60107\n",
      "epoch no.1 train no.77000  loss = 2.87029 avg_loss = 3.60375\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁댄스', '곡', '</s>']\n",
      "추억의 1990년대 댄스팝</s>\n",
      "epoch no.1 train no.77010  loss = 3.85599 avg_loss = 3.60329\n",
      "epoch no.1 train no.77020  loss = 3.70348 avg_loss = 3.63129\n",
      "epoch no.1 train no.77030  loss = 5.07143 avg_loss = 3.64197\n",
      "epoch no.1 train no.77040  loss = 3.23609 avg_loss = 3.67495\n",
      "epoch no.1 train no.77050  loss = 3.95373 avg_loss = 3.64935\n",
      "epoch no.1 train no.77060  loss = 4.04815 avg_loss = 3.66167\n",
      "epoch no.1 train no.77070  loss = 5.12456 avg_loss = 3.62214\n",
      "epoch no.1 train no.77080  loss = 4.62488 avg_loss = 3.60624\n",
      "epoch no.1 train no.77090  loss = 3.33073 avg_loss = 3.55218\n",
      "epoch no.1 train no.77100  loss = 2.32361 avg_loss = 3.48949\n",
      "epoch no.1 train no.77110  loss = 4.04240 avg_loss = 3.48711\n",
      "epoch no.1 train no.77120  loss = 3.20707 avg_loss = 3.50564\n",
      "epoch no.1 train no.77130  loss = 3.61148 avg_loss = 3.52065\n",
      "epoch no.1 train no.77140  loss = 3.46512 avg_loss = 3.52132\n",
      "epoch no.1 train no.77150  loss = 2.75706 avg_loss = 3.52611\n",
      "epoch no.1 train no.77160  loss = 3.35724 avg_loss = 3.45049\n",
      "epoch no.1 train no.77170  loss = 3.43661 avg_loss = 3.44648\n",
      "epoch no.1 train no.77180  loss = 1.60414 avg_loss = 3.43389\n",
      "epoch no.1 train no.77190  loss = 4.50646 avg_loss = 3.47452\n",
      "epoch no.1 train no.77200  loss = 3.49491 avg_loss = 3.48950\n",
      "epoch no.1 train no.77210  loss = 3.02587 avg_loss = 3.48961\n",
      "epoch no.1 train no.77220  loss = 4.17814 avg_loss = 3.48221\n",
      "epoch no.1 train no.77230  loss = 2.67411 avg_loss = 3.47179\n",
      "epoch no.1 train no.77240  loss = 4.44847 avg_loss = 3.50212\n",
      "epoch no.1 train no.77250  loss = 4.46546 avg_loss = 3.51557\n",
      "epoch no.1 train no.77260  loss = 3.40567 avg_loss = 3.53039\n",
      "epoch no.1 train no.77270  loss = 3.90822 avg_loss = 3.52569\n",
      "epoch no.1 train no.77280  loss = 3.13259 avg_loss = 3.53672\n",
      "epoch no.1 train no.77290  loss = 2.70612 avg_loss = 3.49987\n",
      "epoch no.1 train no.77300  loss = 3.33639 avg_loss = 3.49155\n",
      "epoch no.1 train no.77310  loss = 5.84179 avg_loss = 3.49027\n",
      "epoch no.1 train no.77320  loss = 2.98852 avg_loss = 3.53599\n",
      "epoch no.1 train no.77330  loss = 2.30303 avg_loss = 3.57529\n",
      "epoch no.1 train no.77340  loss = 2.12397 avg_loss = 3.56964\n",
      "epoch no.1 train no.77350  loss = 5.17319 avg_loss = 3.58729\n",
      "epoch no.1 train no.77360  loss = 3.67503 avg_loss = 3.63309\n",
      "epoch no.1 train no.77370  loss = 4.36086 avg_loss = 3.66793\n",
      "epoch no.1 train no.77380  loss = 4.26223 avg_loss = 3.65214\n",
      "epoch no.1 train no.77390  loss = 4.33922 avg_loss = 3.66117\n",
      "epoch no.1 train no.77400  loss = 3.18247 avg_loss = 3.66558\n",
      "epoch no.1 train no.77410  loss = 4.07728 avg_loss = 3.68126\n",
      "epoch no.1 train no.77420  loss = 2.55617 avg_loss = 3.68278\n",
      "epoch no.1 train no.77430  loss = 4.53439 avg_loss = 3.76948\n",
      "epoch no.1 train no.77440  loss = 6.46845 avg_loss = 3.81709\n",
      "epoch no.1 train no.77450  loss = 2.85135 avg_loss = 3.77021\n",
      "epoch no.1 train no.77460  loss = 5.22176 avg_loss = 3.85280\n",
      "epoch no.1 train no.77470  loss = 3.81095 avg_loss = 3.86230\n",
      "epoch no.1 train no.77480  loss = 4.46488 avg_loss = 3.79068\n",
      "epoch no.1 train no.77490  loss = 4.09153 avg_loss = 3.76707\n",
      "epoch no.1 train no.77500  loss = 4.29528 avg_loss = 3.71499\n",
      "epoch no.1 train no.77510  loss = 2.69097 avg_loss = 3.70648\n",
      "epoch no.1 train no.77520  loss = 3.73572 avg_loss = 3.69383\n",
      "epoch no.1 train no.77530  loss = 5.23604 avg_loss = 3.75366\n",
      "epoch no.1 train no.77540  loss = 4.55641 avg_loss = 3.74710\n",
      "epoch no.1 train no.77550  loss = 6.29886 avg_loss = 3.77754\n",
      "epoch no.1 train no.77560  loss = 2.71535 avg_loss = 3.78870\n",
      "epoch no.1 train no.77570  loss = 3.87609 avg_loss = 3.75546\n",
      "epoch no.1 train no.77580  loss = 5.27231 avg_loss = 3.77049\n",
      "epoch no.1 train no.77590  loss = 2.70291 avg_loss = 3.70894\n",
      "epoch no.1 train no.77600  loss = 3.29686 avg_loss = 3.68515\n",
      "epoch no.1 train no.77610  loss = 3.99151 avg_loss = 3.66703\n",
      "epoch no.1 train no.77620  loss = 3.97826 avg_loss = 3.63044\n",
      "epoch no.1 train no.77630  loss = 2.48153 avg_loss = 3.58698\n",
      "epoch no.1 train no.77640  loss = 1.98704 avg_loss = 3.55633\n",
      "epoch no.1 train no.77650  loss = 5.11359 avg_loss = 3.54395\n",
      "epoch no.1 train no.77660  loss = 4.32651 avg_loss = 3.48444\n",
      "epoch no.1 train no.77670  loss = 3.95779 avg_loss = 3.53232\n",
      "epoch no.1 train no.77680  loss = 5.06697 avg_loss = 3.56711\n",
      "epoch no.1 train no.77690  loss = 3.42384 avg_loss = 3.57939\n",
      "epoch no.1 train no.77700  loss = 1.99786 avg_loss = 3.59989\n",
      "epoch no.1 train no.77710  loss = 1.96208 avg_loss = 3.61168\n",
      "epoch no.1 train no.77720  loss = 2.66041 avg_loss = 3.58267\n",
      "epoch no.1 train no.77730  loss = 4.83946 avg_loss = 3.60887\n",
      "epoch no.1 train no.77740  loss = 2.48192 avg_loss = 3.55854\n",
      "epoch no.1 train no.77750  loss = 3.83877 avg_loss = 3.58832\n",
      "epoch no.1 train no.77760  loss = 3.33113 avg_loss = 3.56867\n",
      "epoch no.1 train no.77770  loss = 4.18142 avg_loss = 3.58081\n",
      "epoch no.1 train no.77780  loss = 2.48166 avg_loss = 3.55251\n",
      "epoch no.1 train no.77790  loss = 3.18942 avg_loss = 3.52632\n",
      "epoch no.1 train no.77800  loss = 1.92115 avg_loss = 3.53853\n",
      "epoch no.1 train no.77810  loss = 3.81290 avg_loss = 3.60933\n",
      "epoch no.1 train no.77820  loss = 3.54179 avg_loss = 3.65024\n",
      "epoch no.1 train no.77830  loss = 3.21381 avg_loss = 3.62728\n",
      "epoch no.1 train no.77840  loss = 1.50302 avg_loss = 3.63045\n",
      "epoch no.1 train no.77850  loss = 4.11268 avg_loss = 3.61366\n",
      "epoch no.1 train no.77860  loss = 5.06542 avg_loss = 3.60414\n",
      "epoch no.1 train no.77870  loss = 3.72518 avg_loss = 3.62783\n",
      "epoch no.1 train no.77880  loss = 2.97090 avg_loss = 3.66376\n",
      "epoch no.1 train no.77890  loss = 3.75991 avg_loss = 3.63749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.77900  loss = 2.29964 avg_loss = 3.65022\n",
      "epoch no.1 train no.77910  loss = 1.85452 avg_loss = 3.63124\n",
      "epoch no.1 train no.77920  loss = 4.32055 avg_loss = 3.66272\n",
      "epoch no.1 train no.77930  loss = 4.65886 avg_loss = 3.65742\n",
      "epoch no.1 train no.77940  loss = 4.33773 avg_loss = 3.68870\n",
      "epoch no.1 train no.77950  loss = 3.96812 avg_loss = 3.70767\n",
      "epoch no.1 train no.77960  loss = 3.72973 avg_loss = 3.68944\n",
      "epoch no.1 train no.77970  loss = 4.51389 avg_loss = 3.71926\n",
      "epoch no.1 train no.77980  loss = 3.51151 avg_loss = 3.75230\n",
      "epoch no.1 train no.77990  loss = 2.83103 avg_loss = 3.69760\n",
      "epoch no.1 train no.78000  loss = 6.64169 avg_loss = 3.70024\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.1 train no.78010  loss = 4.40560 avg_loss = 3.70489\n",
      "epoch no.1 train no.78020  loss = 3.20893 avg_loss = 3.70407\n",
      "epoch no.1 train no.78030  loss = 2.92013 avg_loss = 3.70481\n",
      "epoch no.1 train no.78040  loss = 3.58607 avg_loss = 3.70132\n",
      "epoch no.1 train no.78050  loss = 2.59802 avg_loss = 3.68829\n",
      "epoch no.1 train no.78060  loss = 3.48615 avg_loss = 3.69997\n",
      "epoch no.1 train no.78070  loss = 4.40225 avg_loss = 3.72432\n",
      "epoch no.1 train no.78080  loss = 5.75314 avg_loss = 3.70261\n",
      "epoch no.1 train no.78090  loss = 4.00849 avg_loss = 3.66994\n",
      "epoch no.1 train no.78100  loss = 5.36748 avg_loss = 3.71410\n",
      "epoch no.1 train no.78110  loss = 4.89039 avg_loss = 3.73623\n",
      "epoch no.1 train no.78120  loss = 3.43198 avg_loss = 3.71987\n",
      "epoch no.1 train no.78130  loss = 3.34353 avg_loss = 3.71416\n",
      "epoch no.1 train no.78140  loss = 3.23718 avg_loss = 3.69394\n",
      "epoch no.1 train no.78150  loss = 2.72353 avg_loss = 3.65664\n",
      "epoch no.1 train no.78160  loss = 5.05357 avg_loss = 3.66873\n",
      "epoch no.1 train no.78170  loss = 5.14540 avg_loss = 3.70634\n",
      "epoch no.1 train no.78180  loss = 3.47838 avg_loss = 3.73530\n",
      "epoch no.1 train no.78190  loss = 3.17153 avg_loss = 3.77445\n",
      "epoch no.1 train no.78200  loss = 3.28285 avg_loss = 3.78151\n",
      "epoch no.1 train no.78210  loss = 5.63803 avg_loss = 3.77145\n",
      "epoch no.1 train no.78220  loss = 3.78625 avg_loss = 3.73171\n",
      "epoch no.1 train no.78230  loss = 3.39012 avg_loss = 3.72211\n",
      "epoch no.1 train no.78240  loss = 2.46954 avg_loss = 3.68393\n",
      "epoch no.1 train no.78250  loss = 2.03880 avg_loss = 3.68730\n",
      "epoch no.1 train no.78260  loss = 4.40818 avg_loss = 3.64785\n",
      "epoch no.1 train no.78270  loss = 3.58302 avg_loss = 3.65176\n",
      "epoch no.1 train no.78280  loss = 3.50882 avg_loss = 3.65221\n",
      "epoch no.1 train no.78290  loss = 4.55496 avg_loss = 3.72371\n",
      "epoch no.1 train no.78300  loss = 2.03528 avg_loss = 3.72609\n",
      "epoch no.1 train no.78310  loss = 3.11124 avg_loss = 3.77782\n",
      "epoch no.1 train no.78320  loss = 2.98657 avg_loss = 3.69865\n",
      "epoch no.1 train no.78330  loss = 2.92487 avg_loss = 3.66911\n",
      "epoch no.1 train no.78340  loss = 3.86998 avg_loss = 3.69757\n",
      "epoch no.1 train no.78350  loss = 3.80541 avg_loss = 3.72009\n",
      "epoch no.1 train no.78360  loss = 3.95442 avg_loss = 3.68762\n",
      "epoch no.1 train no.78370  loss = 2.77904 avg_loss = 3.68924\n",
      "epoch no.1 train no.78380  loss = 2.23325 avg_loss = 3.69227\n",
      "epoch no.1 train no.78390  loss = 2.71229 avg_loss = 3.64649\n",
      "epoch no.1 train no.78400  loss = 3.62692 avg_loss = 3.63932\n",
      "epoch no.1 train no.78410  loss = 4.81436 avg_loss = 3.64961\n",
      "epoch no.1 train no.78420  loss = 1.92593 avg_loss = 3.63123\n",
      "epoch no.1 train no.78430  loss = 3.14741 avg_loss = 3.63875\n",
      "epoch no.1 train no.78440  loss = 3.30419 avg_loss = 3.63975\n",
      "epoch no.1 train no.78450  loss = 2.42639 avg_loss = 3.60049\n",
      "epoch no.1 train no.78460  loss = 4.83515 avg_loss = 3.64378\n",
      "epoch no.1 train no.78470  loss = 3.67985 avg_loss = 3.65354\n",
      "epoch no.1 train no.78480  loss = 3.33732 avg_loss = 3.69031\n",
      "epoch no.1 train no.78490  loss = 5.18269 avg_loss = 3.76475\n",
      "epoch no.1 train no.78500  loss = 4.15784 avg_loss = 3.75474\n",
      "epoch no.1 train no.78510  loss = 5.09297 avg_loss = 3.73281\n",
      "epoch no.1 train no.78520  loss = 3.47335 avg_loss = 3.72483\n",
      "epoch no.1 train no.78530  loss = 4.23628 avg_loss = 3.72601\n",
      "epoch no.1 train no.78540  loss = 4.58961 avg_loss = 3.68523\n",
      "epoch no.1 train no.78550  loss = 5.82766 avg_loss = 3.72530\n",
      "epoch no.1 train no.78560  loss = 2.91796 avg_loss = 3.71943\n",
      "epoch no.1 train no.78570  loss = 2.67139 avg_loss = 3.69156\n",
      "epoch no.1 train no.78580  loss = 1.64556 avg_loss = 3.67137\n",
      "epoch no.1 train no.78590  loss = 3.24581 avg_loss = 3.71930\n",
      "epoch no.1 train no.78600  loss = 3.35810 avg_loss = 3.73490\n",
      "epoch no.1 train no.78610  loss = 3.51981 avg_loss = 3.76178\n",
      "epoch no.1 train no.78620  loss = 5.61919 avg_loss = 3.72476\n",
      "epoch no.1 train no.78630  loss = 3.29423 avg_loss = 3.69855\n",
      "epoch no.1 train no.78640  loss = 3.89461 avg_loss = 3.68685\n",
      "epoch no.1 train no.78650  loss = 1.65358 avg_loss = 3.64909\n",
      "epoch no.1 train no.78660  loss = 2.85983 avg_loss = 3.66999\n",
      "epoch no.1 train no.78670  loss = 4.44071 avg_loss = 3.67110\n",
      "epoch no.1 train no.78680  loss = 2.57786 avg_loss = 3.64497\n",
      "epoch no.1 train no.78690  loss = 1.69745 avg_loss = 3.60214\n",
      "epoch no.1 train no.78700  loss = 2.59972 avg_loss = 3.59523\n",
      "epoch no.1 train no.78710  loss = 3.31620 avg_loss = 3.52665\n",
      "epoch no.1 train no.78720  loss = 3.22041 avg_loss = 3.53309\n",
      "epoch no.1 train no.78730  loss = 5.65759 avg_loss = 3.59298\n",
      "epoch no.1 train no.78740  loss = 5.54944 avg_loss = 3.56659\n",
      "epoch no.1 train no.78750  loss = 2.96413 avg_loss = 3.58250\n",
      "epoch no.1 train no.78760  loss = 2.87589 avg_loss = 3.56339\n",
      "epoch no.1 train no.78770  loss = 4.95391 avg_loss = 3.63159\n",
      "epoch no.1 train no.78780  loss = 4.46938 avg_loss = 3.60305\n",
      "epoch no.1 train no.78790  loss = 3.04840 avg_loss = 3.55743\n",
      "epoch no.1 train no.78800  loss = 3.13846 avg_loss = 3.58392\n",
      "epoch no.1 train no.78810  loss = 2.20108 avg_loss = 3.55589\n",
      "epoch no.1 train no.78820  loss = 3.82154 avg_loss = 3.60574\n",
      "epoch no.1 train no.78830  loss = 4.76409 avg_loss = 3.60373\n",
      "epoch no.1 train no.78840  loss = 4.30649 avg_loss = 3.58071\n",
      "epoch no.1 train no.78850  loss = 2.56059 avg_loss = 3.57753\n",
      "epoch no.1 train no.78860  loss = 4.27793 avg_loss = 3.53329\n",
      "epoch no.1 train no.78870  loss = 7.86416 avg_loss = 3.57389\n",
      "epoch no.1 train no.78880  loss = 3.40236 avg_loss = 3.52189\n",
      "epoch no.1 train no.78890  loss = 4.80610 avg_loss = 3.54121\n",
      "epoch no.1 train no.78900  loss = 5.83142 avg_loss = 3.49502\n",
      "epoch no.1 train no.78910  loss = 3.59791 avg_loss = 3.52373\n",
      "epoch no.1 train no.78920  loss = 1.55778 avg_loss = 3.52186\n",
      "epoch no.1 train no.78930  loss = 3.85018 avg_loss = 3.53366\n",
      "epoch no.1 train no.78940  loss = 2.49705 avg_loss = 3.53082\n",
      "epoch no.1 train no.78950  loss = 3.55950 avg_loss = 3.57168\n",
      "epoch no.1 train no.78960  loss = 3.55593 avg_loss = 3.60247\n",
      "epoch no.1 train no.78970  loss = 4.22504 avg_loss = 3.59460\n",
      "epoch no.1 train no.78980  loss = 5.93374 avg_loss = 3.67022\n",
      "epoch no.1 train no.78990  loss = 3.18665 avg_loss = 3.67624\n",
      "epoch no.1 train no.79000  loss = 5.13565 avg_loss = 3.69805\n",
      "4\n",
      "to_tokens: ['▁비', '▁팝', '팝', '▁명', '드', '</s>']\n",
      "추억의 올드팝 발라드</s>\n",
      "epoch no.1 train no.79010  loss = 3.66809 avg_loss = 3.69265\n",
      "epoch no.1 train no.79020  loss = 1.69142 avg_loss = 3.67955\n",
      "epoch no.1 train no.79030  loss = 2.72336 avg_loss = 3.65301\n",
      "epoch no.1 train no.79040  loss = 1.46519 avg_loss = 3.64004\n",
      "epoch no.1 train no.79050  loss = 3.81366 avg_loss = 3.63333\n",
      "epoch no.1 train no.79060  loss = 4.36427 avg_loss = 3.64955\n",
      "epoch no.1 train no.79070  loss = 3.78840 avg_loss = 3.59817\n",
      "epoch no.1 train no.79080  loss = 2.77608 avg_loss = 3.65054\n",
      "epoch no.1 train no.79090  loss = 3.66616 avg_loss = 3.61562\n",
      "epoch no.1 train no.79100  loss = 3.43387 avg_loss = 3.62164\n",
      "epoch no.1 train no.79110  loss = 2.55666 avg_loss = 3.63727\n",
      "epoch no.1 train no.79120  loss = 4.07801 avg_loss = 3.61825\n",
      "epoch no.1 train no.79130  loss = 2.43986 avg_loss = 3.60796\n",
      "epoch no.1 train no.79140  loss = 2.95494 avg_loss = 3.56510\n",
      "epoch no.1 train no.79150  loss = 5.40757 avg_loss = 3.54984\n",
      "epoch no.1 train no.79160  loss = 4.46612 avg_loss = 3.57926\n",
      "epoch no.1 train no.79170  loss = 4.87966 avg_loss = 3.57163\n",
      "epoch no.1 train no.79180  loss = 3.36568 avg_loss = 3.54488\n",
      "epoch no.1 train no.79190  loss = 2.81226 avg_loss = 3.58522\n",
      "epoch no.1 train no.79200  loss = 3.56095 avg_loss = 3.56437\n",
      "epoch no.1 train no.79210  loss = 3.43495 avg_loss = 3.55867\n",
      "epoch no.1 train no.79220  loss = 3.94250 avg_loss = 3.59472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.79230  loss = 4.16757 avg_loss = 3.64305\n",
      "epoch no.1 train no.79240  loss = 5.32866 avg_loss = 3.66017\n",
      "epoch no.1 train no.79250  loss = 2.79851 avg_loss = 3.64449\n",
      "epoch no.1 train no.79260  loss = 3.16069 avg_loss = 3.64921\n",
      "epoch no.1 train no.79270  loss = 2.38578 avg_loss = 3.63754\n",
      "epoch no.1 train no.79280  loss = 3.20048 avg_loss = 3.63414\n",
      "epoch no.1 train no.79290  loss = 4.84325 avg_loss = 3.66781\n",
      "epoch no.1 train no.79300  loss = 3.19644 avg_loss = 3.65814\n",
      "epoch no.1 train no.79310  loss = 6.40884 avg_loss = 3.65001\n",
      "epoch no.1 train no.79320  loss = 4.02856 avg_loss = 3.67342\n",
      "epoch no.1 train no.79330  loss = 5.88366 avg_loss = 3.61509\n",
      "epoch no.1 train no.79340  loss = 2.53201 avg_loss = 3.56909\n",
      "epoch no.1 train no.79350  loss = 3.48756 avg_loss = 3.59801\n",
      "epoch no.1 train no.79360  loss = 2.57676 avg_loss = 3.54439\n",
      "epoch no.1 train no.79370  loss = 2.72791 avg_loss = 3.59806\n",
      "epoch no.1 train no.79380  loss = 3.50931 avg_loss = 3.61343\n",
      "epoch no.1 train no.79390  loss = 4.74291 avg_loss = 3.61894\n",
      "epoch no.1 train no.79400  loss = 1.77618 avg_loss = 3.57452\n",
      "epoch no.1 train no.79410  loss = 4.55089 avg_loss = 3.58780\n",
      "epoch no.1 train no.79420  loss = 2.30891 avg_loss = 3.55907\n",
      "epoch no.1 train no.79430  loss = 5.48366 avg_loss = 3.62689\n",
      "epoch no.1 train no.79440  loss = 3.56725 avg_loss = 3.61590\n",
      "epoch no.1 train no.79450  loss = 1.35966 avg_loss = 3.60599\n",
      "epoch no.1 train no.79460  loss = 2.34630 avg_loss = 3.56151\n",
      "epoch no.1 train no.79470  loss = 2.87470 avg_loss = 3.52762\n",
      "epoch no.1 train no.79480  loss = 3.03105 avg_loss = 3.46089\n",
      "epoch no.1 train no.79490  loss = 4.25778 avg_loss = 3.47600\n",
      "epoch no.1 train no.79500  loss = 2.77399 avg_loss = 3.48271\n",
      "epoch no.1 train no.79510  loss = 4.60364 avg_loss = 3.53736\n",
      "epoch no.1 train no.79520  loss = 3.15995 avg_loss = 3.52594\n",
      "epoch no.1 train no.79530  loss = 4.26779 avg_loss = 3.51985\n",
      "epoch no.1 train no.79540  loss = 2.89651 avg_loss = 3.51865\n",
      "epoch no.1 train no.79550  loss = 3.62604 avg_loss = 3.50772\n",
      "epoch no.1 train no.79560  loss = 2.65089 avg_loss = 3.54848\n",
      "epoch no.1 train no.79570  loss = 4.29508 avg_loss = 3.54084\n",
      "epoch no.1 train no.79580  loss = 4.27905 avg_loss = 3.58192\n",
      "epoch no.1 train no.79590  loss = 2.53382 avg_loss = 3.58072\n",
      "epoch no.1 train no.79600  loss = 2.92440 avg_loss = 3.55490\n",
      "epoch no.1 train no.79610  loss = 4.60150 avg_loss = 3.56941\n",
      "epoch no.1 train no.79620  loss = 3.03216 avg_loss = 3.56363\n",
      "epoch no.1 train no.79630  loss = 3.88894 avg_loss = 3.53091\n",
      "epoch no.1 train no.79640  loss = 3.87655 avg_loss = 3.55142\n",
      "epoch no.1 train no.79650  loss = 2.81549 avg_loss = 3.58457\n",
      "epoch no.1 train no.79660  loss = 3.91581 avg_loss = 3.56843\n",
      "epoch no.1 train no.79670  loss = 2.74812 avg_loss = 3.53918\n",
      "epoch no.1 train no.79680  loss = 2.92466 avg_loss = 3.54874\n",
      "epoch no.1 train no.79690  loss = 5.75368 avg_loss = 3.59258\n",
      "epoch no.1 train no.79700  loss = 2.89010 avg_loss = 3.54103\n",
      "epoch no.1 train no.79710  loss = 2.82043 avg_loss = 3.55843\n",
      "epoch no.1 train no.79720  loss = 3.98334 avg_loss = 3.56287\n",
      "epoch no.1 train no.79730  loss = 2.27265 avg_loss = 3.51989\n",
      "epoch no.1 train no.79740  loss = 4.01727 avg_loss = 3.47494\n",
      "epoch no.1 train no.79750  loss = 2.92939 avg_loss = 3.44825\n",
      "epoch no.1 train no.79760  loss = 2.69959 avg_loss = 3.44454\n",
      "epoch no.1 train no.79770  loss = 3.45040 avg_loss = 3.49986\n",
      "epoch no.1 train no.79780  loss = 3.62734 avg_loss = 3.46622\n",
      "epoch no.1 train no.79790  loss = 4.00900 avg_loss = 3.47175\n",
      "epoch no.1 train no.79800  loss = 2.52864 avg_loss = 3.45033\n",
      "epoch no.1 train no.79810  loss = 3.00110 avg_loss = 3.47247\n",
      "epoch no.1 train no.79820  loss = 3.57449 avg_loss = 3.52757\n",
      "epoch no.1 train no.79830  loss = 3.64499 avg_loss = 3.57638\n",
      "epoch no.1 train no.79840  loss = 3.17523 avg_loss = 3.60796\n",
      "epoch no.1 train no.79850  loss = 2.86026 avg_loss = 3.61240\n",
      "epoch no.1 train no.79860  loss = 4.47011 avg_loss = 3.63419\n",
      "epoch no.1 train no.79870  loss = 4.33619 avg_loss = 3.60279\n",
      "epoch no.1 train no.79880  loss = 2.69889 avg_loss = 3.56963\n",
      "epoch no.1 train no.79890  loss = 2.27244 avg_loss = 3.57420\n",
      "epoch no.1 train no.79900  loss = 3.35398 avg_loss = 3.60425\n",
      "epoch no.1 train no.79910  loss = 5.36107 avg_loss = 3.65004\n",
      "epoch no.1 train no.79920  loss = 3.19514 avg_loss = 3.65085\n",
      "epoch no.1 train no.79930  loss = 3.18366 avg_loss = 3.64339\n",
      "epoch no.1 train no.79940  loss = 3.19858 avg_loss = 3.67636\n",
      "epoch no.1 train no.79950  loss = 2.19351 avg_loss = 3.72075\n",
      "epoch no.1 train no.79960  loss = 3.36874 avg_loss = 3.71852\n",
      "epoch no.1 train no.79970  loss = 4.14391 avg_loss = 3.71524\n",
      "epoch no.1 train no.79980  loss = 2.78793 avg_loss = 3.71817\n",
      "epoch no.1 train no.79990  loss = 4.16302 avg_loss = 3.71077\n",
      "epoch no.1 train no.80000  loss = 3.58768 avg_loss = 3.74337\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>', '</s>']\n",
      "추억의 발라드 명곡 모음</s>\n",
      "epoch no.1 train no.80010  loss = 3.83282 avg_loss = 3.76030\n",
      "epoch no.1 train no.80020  loss = 2.73880 avg_loss = 3.72974\n",
      "epoch no.1 train no.80030  loss = 3.97464 avg_loss = 3.69327\n",
      "epoch no.1 train no.80040  loss = 3.82863 avg_loss = 3.69033\n",
      "epoch no.1 train no.80050  loss = 3.47793 avg_loss = 3.62969\n",
      "epoch no.1 train no.80060  loss = 2.09685 avg_loss = 3.58833\n",
      "epoch no.1 train no.80070  loss = 3.26618 avg_loss = 3.61766\n",
      "epoch no.1 train no.80080  loss = 4.29740 avg_loss = 3.61789\n",
      "epoch no.1 train no.80090  loss = 4.30846 avg_loss = 3.65802\n",
      "epoch no.1 train no.80100  loss = 4.19388 avg_loss = 3.67336\n",
      "epoch no.1 train no.80110  loss = 3.25694 avg_loss = 3.65724\n",
      "epoch no.1 train no.80120  loss = 3.59320 avg_loss = 3.67566\n",
      "epoch no.1 train no.80130  loss = 3.63115 avg_loss = 3.66052\n",
      "epoch no.1 train no.80140  loss = 3.44828 avg_loss = 3.70751\n",
      "epoch no.1 train no.80150  loss = 4.65341 avg_loss = 3.69019\n",
      "epoch no.1 train no.80160  loss = 3.42304 avg_loss = 3.65326\n",
      "epoch no.1 train no.80170  loss = 2.66589 avg_loss = 3.62043\n",
      "epoch no.1 train no.80180  loss = 2.77878 avg_loss = 3.58259\n",
      "epoch no.1 train no.80190  loss = 6.02268 avg_loss = 3.62306\n",
      "epoch no.1 train no.80200  loss = 3.50720 avg_loss = 3.59659\n",
      "epoch no.1 train no.80210  loss = 2.75753 avg_loss = 3.54670\n",
      "epoch no.1 train no.80220  loss = 2.14934 avg_loss = 3.54607\n",
      "epoch no.1 train no.80230  loss = 3.52043 avg_loss = 3.52910\n",
      "epoch no.1 train no.80240  loss = 2.61969 avg_loss = 3.53175\n",
      "epoch no.1 train no.80250  loss = 3.41797 avg_loss = 3.54087\n",
      "epoch no.1 train no.80260  loss = 2.61447 avg_loss = 3.51915\n",
      "epoch no.1 train no.80270  loss = 2.89617 avg_loss = 3.54502\n",
      "epoch no.1 train no.80280  loss = 3.34889 avg_loss = 3.55325\n",
      "epoch no.1 train no.80290  loss = 3.73708 avg_loss = 3.58887\n",
      "epoch no.1 train no.80300  loss = 4.23246 avg_loss = 3.61905\n",
      "epoch no.1 train no.80310  loss = 6.72210 avg_loss = 3.65696\n",
      "epoch no.1 train no.80320  loss = 1.68071 avg_loss = 3.61209\n",
      "epoch no.1 train no.80330  loss = 4.72336 avg_loss = 3.60400\n",
      "epoch no.1 train no.80340  loss = 2.38138 avg_loss = 3.56112\n",
      "epoch no.1 train no.80350  loss = 4.17687 avg_loss = 3.54514\n",
      "epoch no.1 train no.80360  loss = 2.00712 avg_loss = 3.47910\n",
      "epoch no.1 train no.80370  loss = 3.72957 avg_loss = 3.50554\n",
      "epoch no.1 train no.80380  loss = 3.67380 avg_loss = 3.50794\n",
      "epoch no.1 train no.80390  loss = 1.74050 avg_loss = 3.47449\n",
      "epoch no.1 train no.80400  loss = 3.53985 avg_loss = 3.52497\n",
      "epoch no.1 train no.80410  loss = 6.18746 avg_loss = 3.56818\n",
      "epoch no.1 train no.80420  loss = 7.00915 avg_loss = 3.54110\n",
      "epoch no.1 train no.80430  loss = 4.45486 avg_loss = 3.53425\n",
      "epoch no.1 train no.80440  loss = 5.16793 avg_loss = 3.59566\n",
      "epoch no.1 train no.80450  loss = 4.93261 avg_loss = 3.58984\n",
      "epoch no.1 train no.80460  loss = 3.89736 avg_loss = 3.60477\n",
      "epoch no.1 train no.80470  loss = 3.44281 avg_loss = 3.63429\n",
      "epoch no.1 train no.80480  loss = 2.37647 avg_loss = 3.63333\n",
      "epoch no.1 train no.80490  loss = 3.75665 avg_loss = 3.60272\n",
      "epoch no.1 train no.80500  loss = 2.91651 avg_loss = 3.59730\n",
      "epoch no.1 train no.80510  loss = 3.21999 avg_loss = 3.58482\n",
      "epoch no.1 train no.80520  loss = 2.67332 avg_loss = 3.57107\n",
      "epoch no.1 train no.80530  loss = 4.04139 avg_loss = 3.62825\n",
      "epoch no.1 train no.80540  loss = 4.13598 avg_loss = 3.58878\n",
      "epoch no.1 train no.80550  loss = 4.35621 avg_loss = 3.62566\n",
      "epoch no.1 train no.80560  loss = 3.39996 avg_loss = 3.66514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.80570  loss = 4.17035 avg_loss = 3.67909\n",
      "epoch no.1 train no.80580  loss = 2.26732 avg_loss = 3.70462\n",
      "epoch no.1 train no.80590  loss = 3.25503 avg_loss = 3.69462\n",
      "epoch no.1 train no.80600  loss = 4.88355 avg_loss = 3.68555\n",
      "epoch no.1 train no.80610  loss = 5.30824 avg_loss = 3.74176\n",
      "epoch no.1 train no.80620  loss = 3.91703 avg_loss = 3.75846\n",
      "epoch no.1 train no.80630  loss = 2.79638 avg_loss = 3.73115\n",
      "epoch no.1 train no.80640  loss = 2.94150 avg_loss = 3.72192\n",
      "epoch no.1 train no.80650  loss = 2.17418 avg_loss = 3.72412\n",
      "epoch no.1 train no.80660  loss = 4.67777 avg_loss = 3.74502\n",
      "epoch no.1 train no.80670  loss = 3.96443 avg_loss = 3.73186\n",
      "epoch no.1 train no.80680  loss = 4.51721 avg_loss = 3.72866\n",
      "epoch no.1 train no.80690  loss = 2.70359 avg_loss = 3.68716\n",
      "epoch no.1 train no.80700  loss = 4.34703 avg_loss = 3.70292\n",
      "epoch no.1 train no.80710  loss = 4.20804 avg_loss = 3.63510\n",
      "epoch no.1 train no.80720  loss = 5.00382 avg_loss = 3.63709\n",
      "epoch no.1 train no.80730  loss = 2.85333 avg_loss = 3.64097\n",
      "epoch no.1 train no.80740  loss = 1.90990 avg_loss = 3.61935\n",
      "epoch no.1 train no.80750  loss = 3.10490 avg_loss = 3.59712\n",
      "epoch no.1 train no.80760  loss = 2.54045 avg_loss = 3.57060\n",
      "epoch no.1 train no.80770  loss = 5.36911 avg_loss = 3.56356\n",
      "epoch no.1 train no.80780  loss = 2.49666 avg_loss = 3.62306\n",
      "epoch no.1 train no.80790  loss = 2.08675 avg_loss = 3.60688\n",
      "epoch no.1 train no.80800  loss = 2.50715 avg_loss = 3.58336\n",
      "epoch no.1 train no.80810  loss = 1.63227 avg_loss = 3.52566\n",
      "epoch no.1 train no.80820  loss = 3.14965 avg_loss = 3.59688\n",
      "epoch no.1 train no.80830  loss = 2.65800 avg_loss = 3.62025\n",
      "epoch no.1 train no.80840  loss = 2.23309 avg_loss = 3.66364\n",
      "epoch no.1 train no.80850  loss = 3.97102 avg_loss = 3.68575\n",
      "epoch no.1 train no.80860  loss = 2.79669 avg_loss = 3.68710\n",
      "epoch no.1 train no.80870  loss = 3.93784 avg_loss = 3.64266\n",
      "epoch no.1 train no.80880  loss = 3.42544 avg_loss = 3.57930\n",
      "epoch no.1 train no.80890  loss = 2.21367 avg_loss = 3.59694\n",
      "epoch no.1 train no.80900  loss = 3.65651 avg_loss = 3.57221\n",
      "epoch no.1 train no.80910  loss = 5.05932 avg_loss = 3.59417\n",
      "epoch no.1 train no.80920  loss = 4.05576 avg_loss = 3.62031\n",
      "epoch no.1 train no.80930  loss = 3.35116 avg_loss = 3.60078\n",
      "epoch no.1 train no.80940  loss = 3.58486 avg_loss = 3.67125\n",
      "epoch no.1 train no.80950  loss = 3.79969 avg_loss = 3.68087\n",
      "epoch no.1 train no.80960  loss = 1.76792 avg_loss = 3.64417\n",
      "epoch no.1 train no.80970  loss = 2.56044 avg_loss = 3.66004\n",
      "epoch no.1 train no.80980  loss = 3.58075 avg_loss = 3.63180\n",
      "epoch no.1 train no.80990  loss = 2.81161 avg_loss = 3.56698\n",
      "epoch no.1 train no.81000  loss = 2.87960 avg_loss = 3.52742\n",
      "3\n",
      "to_tokens: ['▁비', '▁올드', '▁o', 'st', '▁모음']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.1 train no.81010  loss = 3.56824 avg_loss = 3.50825\n",
      "epoch no.1 train no.81020  loss = 2.14015 avg_loss = 3.50912\n",
      "epoch no.1 train no.81030  loss = 3.80684 avg_loss = 3.50223\n",
      "epoch no.1 train no.81040  loss = 3.76079 avg_loss = 3.51283\n",
      "epoch no.1 train no.81050  loss = 2.40833 avg_loss = 3.51937\n",
      "epoch no.1 train no.81060  loss = 4.19053 avg_loss = 3.53552\n",
      "epoch no.1 train no.81070  loss = 2.89641 avg_loss = 3.52902\n",
      "epoch no.1 train no.81080  loss = 3.10679 avg_loss = 3.50374\n",
      "epoch no.1 train no.81090  loss = 3.43180 avg_loss = 3.50924\n",
      "epoch no.1 train no.81100  loss = 3.98430 avg_loss = 3.49397\n",
      "epoch no.1 train no.81110  loss = 2.01621 avg_loss = 3.42562\n",
      "epoch no.1 train no.81120  loss = 4.75839 avg_loss = 3.42064\n",
      "epoch no.1 train no.81130  loss = 3.37772 avg_loss = 3.40422\n",
      "epoch no.1 train no.81140  loss = 5.38775 avg_loss = 3.44144\n",
      "epoch no.1 train no.81150  loss = 4.71246 avg_loss = 3.44749\n",
      "epoch no.1 train no.81160  loss = 4.75312 avg_loss = 3.46766\n",
      "epoch no.1 train no.81170  loss = 2.22849 avg_loss = 3.46224\n",
      "epoch no.1 train no.81180  loss = 4.13355 avg_loss = 3.46029\n",
      "epoch no.1 train no.81190  loss = 2.89683 avg_loss = 3.47193\n",
      "epoch no.1 train no.81200  loss = 3.58797 avg_loss = 3.45780\n",
      "epoch no.1 train no.81210  loss = 3.42019 avg_loss = 3.48842\n",
      "epoch no.1 train no.81220  loss = 2.57865 avg_loss = 3.45888\n",
      "epoch no.1 train no.81230  loss = 2.20485 avg_loss = 3.42818\n",
      "epoch no.1 train no.81240  loss = 3.56242 avg_loss = 3.53310\n",
      "epoch no.1 train no.81250  loss = 5.18403 avg_loss = 3.56230\n",
      "epoch no.1 train no.81260  loss = 2.85236 avg_loss = 3.53622\n",
      "epoch no.1 train no.81270  loss = 4.17897 avg_loss = 3.54215\n",
      "epoch no.1 train no.81280  loss = 2.31194 avg_loss = 3.50108\n",
      "epoch no.1 train no.81290  loss = 3.53085 avg_loss = 3.50786\n",
      "epoch no.1 train no.81300  loss = 2.51568 avg_loss = 3.45403\n",
      "epoch no.1 train no.81310  loss = 2.56813 avg_loss = 3.42906\n",
      "epoch no.1 train no.81320  loss = 5.27632 avg_loss = 3.44410\n",
      "epoch no.1 train no.81330  loss = 4.38542 avg_loss = 3.46313\n",
      "epoch no.1 train no.81340  loss = 3.31015 avg_loss = 3.45445\n",
      "epoch no.1 train no.81350  loss = 3.72919 avg_loss = 3.44035\n",
      "epoch no.1 train no.81360  loss = 3.42833 avg_loss = 3.48110\n",
      "epoch no.1 train no.81370  loss = 3.52337 avg_loss = 3.50577\n",
      "epoch no.1 train no.81380  loss = 7.00503 avg_loss = 3.54523\n",
      "epoch no.1 train no.81390  loss = 2.66159 avg_loss = 3.56522\n",
      "epoch no.1 train no.81400  loss = 4.08877 avg_loss = 3.59760\n",
      "epoch no.1 train no.81410  loss = 2.54388 avg_loss = 3.62649\n",
      "epoch no.1 train no.81420  loss = 2.86818 avg_loss = 3.65935\n",
      "epoch no.1 train no.81430  loss = 3.42353 avg_loss = 3.62395\n",
      "epoch no.1 train no.81440  loss = 2.68471 avg_loss = 3.63611\n",
      "epoch no.1 train no.81450  loss = 2.83966 avg_loss = 3.59064\n",
      "epoch no.1 train no.81460  loss = 2.92029 avg_loss = 3.55255\n",
      "epoch no.1 train no.81470  loss = 4.29127 avg_loss = 3.54865\n",
      "epoch no.1 train no.81480  loss = 3.85964 avg_loss = 3.57649\n",
      "epoch no.1 train no.81490  loss = 2.99086 avg_loss = 3.58329\n",
      "epoch no.1 train no.81500  loss = 2.60030 avg_loss = 3.54555\n",
      "epoch no.1 train no.81510  loss = 4.33705 avg_loss = 3.52844\n",
      "epoch no.1 train no.81520  loss = 2.45543 avg_loss = 3.54323\n",
      "epoch no.1 train no.81530  loss = 4.14199 avg_loss = 3.56824\n",
      "epoch no.1 train no.81540  loss = 2.50504 avg_loss = 3.53104\n",
      "epoch no.1 train no.81550  loss = 3.40229 avg_loss = 3.53053\n",
      "epoch no.1 train no.81560  loss = 4.83256 avg_loss = 3.51942\n",
      "epoch no.1 train no.81570  loss = 2.81838 avg_loss = 3.49415\n",
      "epoch no.1 train no.81580  loss = 2.79097 avg_loss = 3.51800\n",
      "epoch no.1 train no.81590  loss = 4.19782 avg_loss = 3.55070\n",
      "epoch no.1 train no.81600  loss = 4.59001 avg_loss = 3.53792\n",
      "epoch no.1 train no.81610  loss = 5.16604 avg_loss = 3.59406\n",
      "epoch no.1 train no.81620  loss = 4.15486 avg_loss = 3.59070\n",
      "epoch no.1 train no.81630  loss = 2.36532 avg_loss = 3.60711\n",
      "epoch no.1 train no.81640  loss = 4.39691 avg_loss = 3.65312\n",
      "epoch no.1 train no.81650  loss = 3.18988 avg_loss = 3.59299\n",
      "epoch no.1 train no.81660  loss = 3.02427 avg_loss = 3.61225\n",
      "epoch no.1 train no.81670  loss = 5.28467 avg_loss = 3.61281\n",
      "epoch no.1 train no.81680  loss = 5.93913 avg_loss = 3.64446\n",
      "epoch no.1 train no.81690  loss = 2.40520 avg_loss = 3.67743\n",
      "epoch no.1 train no.81700  loss = 4.93834 avg_loss = 3.70338\n",
      "epoch no.1 train no.81710  loss = 5.77324 avg_loss = 3.67297\n",
      "epoch no.1 train no.81720  loss = 2.22913 avg_loss = 3.65919\n",
      "epoch no.1 train no.81730  loss = 1.78645 avg_loss = 3.65279\n",
      "epoch no.1 train no.81740  loss = 3.50359 avg_loss = 3.74787\n",
      "epoch no.1 train no.81750  loss = 3.08803 avg_loss = 3.72599\n",
      "epoch no.1 train no.81760  loss = 5.16662 avg_loss = 3.73849\n",
      "epoch no.1 train no.81770  loss = 5.62737 avg_loss = 3.71254\n",
      "epoch no.1 train no.81780  loss = 3.80620 avg_loss = 3.68377\n",
      "epoch no.1 train no.81790  loss = 2.21461 avg_loss = 3.70617\n",
      "epoch no.1 train no.81800  loss = 2.39578 avg_loss = 3.69084\n",
      "epoch no.1 train no.81810  loss = 4.10303 avg_loss = 3.65405\n",
      "epoch no.1 train no.81820  loss = 4.44853 avg_loss = 3.64355\n",
      "epoch no.1 train no.81830  loss = 3.62356 avg_loss = 3.63761\n",
      "epoch no.1 train no.81840  loss = 5.82302 avg_loss = 3.66171\n",
      "epoch no.1 train no.81850  loss = 3.53785 avg_loss = 3.68819\n",
      "epoch no.1 train no.81860  loss = 3.93621 avg_loss = 3.68241\n",
      "epoch no.1 train no.81870  loss = 4.78927 avg_loss = 3.67359\n",
      "epoch no.1 train no.81880  loss = 4.05301 avg_loss = 3.69643\n",
      "epoch no.1 train no.81890  loss = 6.81128 avg_loss = 3.75172\n",
      "epoch no.1 train no.81900  loss = 2.69898 avg_loss = 3.71557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.81910  loss = 4.49193 avg_loss = 3.72066\n",
      "epoch no.1 train no.81920  loss = 3.84730 avg_loss = 3.72637\n",
      "epoch no.1 train no.81930  loss = 4.38620 avg_loss = 3.72066\n",
      "epoch no.1 train no.81940  loss = 3.96651 avg_loss = 3.71821\n",
      "epoch no.1 train no.81950  loss = 2.37714 avg_loss = 3.66858\n",
      "epoch no.1 train no.81960  loss = 4.01521 avg_loss = 3.67291\n",
      "epoch no.1 train no.81970  loss = 3.24362 avg_loss = 3.66921\n",
      "epoch no.1 train no.81980  loss = 5.20020 avg_loss = 3.69273\n",
      "epoch no.1 train no.81990  loss = 2.47904 avg_loss = 3.68508\n",
      "epoch no.1 train no.82000  loss = 2.91158 avg_loss = 3.64845\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁명', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.1 train no.82010  loss = 4.12376 avg_loss = 3.65364\n",
      "epoch no.1 train no.82020  loss = 3.03917 avg_loss = 3.65925\n",
      "epoch no.1 train no.82030  loss = 1.92054 avg_loss = 3.60809\n",
      "epoch no.1 train no.82040  loss = 4.20635 avg_loss = 3.60555\n",
      "epoch no.1 train no.82050  loss = 4.75097 avg_loss = 3.61353\n",
      "epoch no.1 train no.82060  loss = 5.18887 avg_loss = 3.64327\n",
      "epoch no.1 train no.82070  loss = 2.72720 avg_loss = 3.63124\n",
      "epoch no.1 train no.82080  loss = 2.56899 avg_loss = 3.63034\n",
      "epoch no.1 train no.82090  loss = 3.32806 avg_loss = 3.60361\n",
      "epoch no.1 train no.82100  loss = 6.25694 avg_loss = 3.64638\n",
      "epoch no.1 train no.82110  loss = 2.30759 avg_loss = 3.66764\n",
      "epoch no.1 train no.82120  loss = 4.33565 avg_loss = 3.66183\n",
      "epoch no.1 train no.82130  loss = 3.42684 avg_loss = 3.67537\n",
      "epoch no.1 train no.82140  loss = 3.44624 avg_loss = 3.69896\n",
      "epoch no.1 train no.82150  loss = 2.60758 avg_loss = 3.75098\n",
      "epoch no.1 train no.82160  loss = 2.82671 avg_loss = 3.78759\n",
      "epoch no.1 train no.82170  loss = 5.08043 avg_loss = 3.79251\n",
      "epoch no.1 train no.82180  loss = 4.33677 avg_loss = 3.81991\n",
      "epoch no.1 train no.82190  loss = 4.81569 avg_loss = 3.86431\n",
      "epoch no.1 train no.82200  loss = 5.37591 avg_loss = 3.81326\n",
      "epoch no.1 train no.82210  loss = 2.44650 avg_loss = 3.75899\n",
      "epoch no.1 train no.82220  loss = 4.09692 avg_loss = 3.74969\n",
      "epoch no.1 train no.82230  loss = 4.05634 avg_loss = 3.77772\n",
      "epoch no.1 train no.82240  loss = 3.42610 avg_loss = 3.74893\n",
      "epoch no.1 train no.82250  loss = 2.81587 avg_loss = 3.69564\n",
      "epoch no.1 train no.82260  loss = 3.19169 avg_loss = 3.72154\n",
      "epoch no.1 train no.82270  loss = 3.35603 avg_loss = 3.71068\n",
      "epoch no.1 train no.82280  loss = 4.44438 avg_loss = 3.73717\n",
      "epoch no.1 train no.82290  loss = 2.06603 avg_loss = 3.72882\n",
      "epoch no.1 train no.82300  loss = 2.08735 avg_loss = 3.74210\n",
      "epoch no.1 train no.82310  loss = 1.61824 avg_loss = 3.71692\n",
      "epoch no.1 train no.82320  loss = 2.80651 avg_loss = 3.74065\n",
      "epoch no.1 train no.82330  loss = 2.11974 avg_loss = 3.69643\n",
      "epoch no.1 train no.82340  loss = 4.64334 avg_loss = 3.68544\n",
      "epoch no.1 train no.82350  loss = 2.53718 avg_loss = 3.66206\n",
      "epoch no.1 train no.82360  loss = 4.08869 avg_loss = 3.67000\n",
      "epoch no.1 train no.82370  loss = 2.58462 avg_loss = 3.61103\n",
      "epoch no.1 train no.82380  loss = 5.09959 avg_loss = 3.62365\n",
      "epoch no.1 train no.82390  loss = 3.79974 avg_loss = 3.65731\n",
      "epoch no.1 train no.82400  loss = 5.29590 avg_loss = 3.63627\n",
      "epoch no.1 train no.82410  loss = 3.43230 avg_loss = 3.63559\n",
      "epoch no.1 train no.82420  loss = 2.50887 avg_loss = 3.66286\n",
      "epoch no.1 train no.82430  loss = 2.57939 avg_loss = 3.67017\n",
      "epoch no.1 train no.82440  loss = 4.13312 avg_loss = 3.65094\n",
      "epoch no.1 train no.82450  loss = 3.77961 avg_loss = 3.68965\n",
      "epoch no.1 train no.82460  loss = 2.85409 avg_loss = 3.66004\n",
      "epoch no.1 train no.82470  loss = 3.55961 avg_loss = 3.69347\n",
      "epoch no.1 train no.82480  loss = 2.62932 avg_loss = 3.64789\n",
      "epoch no.1 train no.82490  loss = 3.35584 avg_loss = 3.63051\n",
      "epoch no.1 train no.82500  loss = 3.99231 avg_loss = 3.63270\n",
      "epoch no.1 train no.82510  loss = 5.30345 avg_loss = 3.72127\n",
      "epoch no.1 train no.82520  loss = 1.77454 avg_loss = 3.70895\n",
      "epoch no.1 train no.82530  loss = 5.10522 avg_loss = 3.73662\n",
      "epoch no.1 train no.82540  loss = 3.44499 avg_loss = 3.70406\n",
      "epoch no.1 train no.82550  loss = 4.00511 avg_loss = 3.74774\n",
      "epoch no.1 train no.82560  loss = 2.57087 avg_loss = 3.69035\n",
      "epoch no.1 train no.82570  loss = 3.07963 avg_loss = 3.71254\n",
      "epoch no.1 train no.82580  loss = 2.61851 avg_loss = 3.72026\n",
      "epoch no.1 train no.82590  loss = 4.14743 avg_loss = 3.74283\n",
      "epoch no.1 train no.82600  loss = 2.47878 avg_loss = 3.75925\n",
      "epoch no.1 train no.82610  loss = 2.76161 avg_loss = 3.73849\n",
      "epoch no.1 train no.82620  loss = 5.17109 avg_loss = 3.71735\n",
      "epoch no.1 train no.82630  loss = 5.31276 avg_loss = 3.74037\n",
      "epoch no.1 train no.82640  loss = 2.46631 avg_loss = 3.64884\n",
      "epoch no.1 train no.82650  loss = 3.91386 avg_loss = 3.63477\n",
      "epoch no.1 train no.82660  loss = 4.47090 avg_loss = 3.64827\n",
      "epoch no.1 train no.82670  loss = 4.87346 avg_loss = 3.64333\n",
      "epoch no.1 train no.82680  loss = 2.56612 avg_loss = 3.62658\n",
      "epoch no.1 train no.82690  loss = 6.17140 avg_loss = 3.64652\n",
      "epoch no.1 train no.82700  loss = 4.50038 avg_loss = 3.62436\n",
      "epoch no.1 train no.82710  loss = 2.69770 avg_loss = 3.58337\n",
      "epoch no.1 train no.82720  loss = 4.48019 avg_loss = 3.54358\n",
      "epoch no.1 train no.82730  loss = 3.80225 avg_loss = 3.61333\n",
      "epoch no.1 train no.82740  loss = 3.91690 avg_loss = 3.72086\n",
      "epoch no.1 train no.82750  loss = 3.46252 avg_loss = 3.71137\n",
      "epoch no.1 train no.82760  loss = 2.64834 avg_loss = 3.68045\n",
      "epoch no.1 train no.82770  loss = 4.41980 avg_loss = 3.62557\n",
      "epoch no.1 train no.82780  loss = 3.82325 avg_loss = 3.62452\n",
      "epoch no.1 train no.82790  loss = 1.64297 avg_loss = 3.58295\n",
      "epoch no.1 train no.82800  loss = 2.33838 avg_loss = 3.56956\n",
      "epoch no.1 train no.82810  loss = 4.20587 avg_loss = 3.57318\n",
      "epoch no.1 train no.82820  loss = 2.72878 avg_loss = 3.56547\n",
      "epoch no.1 train no.82830  loss = 3.13531 avg_loss = 3.57370\n",
      "epoch no.1 train no.82840  loss = 3.24911 avg_loss = 3.61154\n",
      "epoch no.1 train no.82850  loss = 4.83040 avg_loss = 3.63936\n",
      "epoch no.1 train no.82860  loss = 4.38951 avg_loss = 3.61255\n",
      "epoch no.1 train no.82870  loss = 3.97833 avg_loss = 3.62774\n",
      "epoch no.1 train no.82880  loss = 3.32031 avg_loss = 3.62069\n",
      "epoch no.1 train no.82890  loss = 3.84626 avg_loss = 3.61279\n",
      "epoch no.1 train no.82900  loss = 2.95345 avg_loss = 3.50822\n",
      "epoch no.1 train no.82910  loss = 2.53175 avg_loss = 3.53562\n",
      "epoch no.1 train no.82920  loss = 3.23790 avg_loss = 3.54376\n",
      "epoch no.1 train no.82930  loss = 3.86311 avg_loss = 3.53818\n",
      "epoch no.1 train no.82940  loss = 2.97376 avg_loss = 3.55878\n",
      "epoch no.1 train no.82950  loss = 4.66679 avg_loss = 3.56467\n",
      "epoch no.1 train no.82960  loss = 2.76708 avg_loss = 3.55246\n",
      "epoch no.1 train no.82970  loss = 2.80974 avg_loss = 3.56679\n",
      "epoch no.1 train no.82980  loss = 3.23512 avg_loss = 3.59991\n",
      "epoch no.1 train no.82990  loss = 3.44462 avg_loss = 3.53428\n",
      "epoch no.1 train no.83000  loss = 3.44912 avg_loss = 3.54013\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '▁o', 'st', '</s>', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.1 train no.83010  loss = 2.88626 avg_loss = 3.56365\n",
      "epoch no.1 train no.83020  loss = 3.71234 avg_loss = 3.62874\n",
      "epoch no.1 train no.83030  loss = 3.83913 avg_loss = 3.63344\n",
      "epoch no.1 train no.83040  loss = 3.43936 avg_loss = 3.67898\n",
      "epoch no.1 train no.83050  loss = 2.97029 avg_loss = 3.67068\n",
      "epoch no.1 train no.83060  loss = 3.48323 avg_loss = 3.64412\n",
      "epoch no.1 train no.83070  loss = 5.11804 avg_loss = 3.68910\n",
      "epoch no.1 train no.83080  loss = 4.37660 avg_loss = 3.68629\n",
      "epoch no.1 train no.83090  loss = 2.92347 avg_loss = 3.68693\n",
      "epoch no.1 train no.83100  loss = 4.50451 avg_loss = 3.70337\n",
      "epoch no.1 train no.83110  loss = 4.09235 avg_loss = 3.67681\n",
      "epoch no.1 train no.83120  loss = 3.53099 avg_loss = 3.65008\n",
      "epoch no.1 train no.83130  loss = 2.26543 avg_loss = 3.64812\n",
      "epoch no.1 train no.83140  loss = 1.07507 avg_loss = 3.58458\n",
      "epoch no.1 train no.83150  loss = 2.32574 avg_loss = 3.56494\n",
      "epoch no.1 train no.83160  loss = 3.47334 avg_loss = 3.51408\n",
      "epoch no.1 train no.83170  loss = 2.52587 avg_loss = 3.47201\n",
      "epoch no.1 train no.83180  loss = 3.70051 avg_loss = 3.43483\n",
      "epoch no.1 train no.83190  loss = 3.43813 avg_loss = 3.44322\n",
      "epoch no.1 train no.83200  loss = 5.03505 avg_loss = 3.46337\n",
      "epoch no.1 train no.83210  loss = 3.80048 avg_loss = 3.51860\n",
      "epoch no.1 train no.83220  loss = 3.77263 avg_loss = 3.49247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.83230  loss = 3.06956 avg_loss = 3.46115\n",
      "epoch no.1 train no.83240  loss = 3.30539 avg_loss = 3.47604\n",
      "epoch no.1 train no.83250  loss = 3.95417 avg_loss = 3.46576\n",
      "epoch no.1 train no.83260  loss = 4.75019 avg_loss = 3.47123\n",
      "epoch no.1 train no.83270  loss = 3.30030 avg_loss = 3.44891\n",
      "epoch no.1 train no.83280  loss = 1.94761 avg_loss = 3.40325\n",
      "epoch no.1 train no.83290  loss = 3.95194 avg_loss = 3.43959\n",
      "epoch no.1 train no.83300  loss = 2.60729 avg_loss = 3.36124\n",
      "epoch no.1 train no.83310  loss = 3.89307 avg_loss = 3.34129\n",
      "epoch no.1 train no.83320  loss = 3.06829 avg_loss = 3.32423\n",
      "epoch no.1 train no.83330  loss = 3.67667 avg_loss = 3.33177\n",
      "epoch no.1 train no.83340  loss = 3.27054 avg_loss = 3.41273\n",
      "epoch no.1 train no.83350  loss = 3.09249 avg_loss = 3.41881\n",
      "epoch no.1 train no.83360  loss = 2.82229 avg_loss = 3.41424\n",
      "epoch no.1 train no.83370  loss = 2.55333 avg_loss = 3.39893\n",
      "epoch no.1 train no.83380  loss = 3.43436 avg_loss = 3.39689\n",
      "epoch no.1 train no.83390  loss = 2.93819 avg_loss = 3.40281\n",
      "epoch no.1 train no.83400  loss = 3.03156 avg_loss = 3.41498\n",
      "epoch no.1 train no.83410  loss = 4.82104 avg_loss = 3.46802\n",
      "epoch no.1 train no.83420  loss = 6.85226 avg_loss = 3.54653\n",
      "epoch no.1 train no.83430  loss = 2.66395 avg_loss = 3.51135\n",
      "epoch no.1 train no.83440  loss = 4.01810 avg_loss = 3.50654\n",
      "epoch no.1 train no.83450  loss = 3.05937 avg_loss = 3.49857\n",
      "epoch no.1 train no.83460  loss = 3.94398 avg_loss = 3.59176\n",
      "epoch no.1 train no.83470  loss = 3.16854 avg_loss = 3.64677\n",
      "epoch no.1 train no.83480  loss = 3.29199 avg_loss = 3.63315\n",
      "epoch no.1 train no.83490  loss = 2.46770 avg_loss = 3.61511\n",
      "epoch no.1 train no.83500  loss = 4.99190 avg_loss = 3.62264\n",
      "epoch no.1 train no.83510  loss = 2.47633 avg_loss = 3.60332\n",
      "epoch no.1 train no.83520  loss = 1.45186 avg_loss = 3.55078\n",
      "epoch no.1 train no.83530  loss = 4.82388 avg_loss = 3.58441\n",
      "epoch no.1 train no.83540  loss = 2.08571 avg_loss = 3.58951\n",
      "epoch no.1 train no.83550  loss = 3.27558 avg_loss = 3.61023\n",
      "epoch no.1 train no.83560  loss = 2.37418 avg_loss = 3.58055\n",
      "epoch no.1 train no.83570  loss = 1.72593 avg_loss = 3.55440\n",
      "epoch no.1 train no.83580  loss = 2.40387 avg_loss = 3.56408\n",
      "epoch no.1 train no.83590  loss = 3.15098 avg_loss = 3.55637\n",
      "epoch no.1 train no.83600  loss = 3.73095 avg_loss = 3.62366\n",
      "epoch no.1 train no.83610  loss = 2.28299 avg_loss = 3.67794\n",
      "epoch no.1 train no.83620  loss = 4.69832 avg_loss = 3.73980\n",
      "epoch no.1 train no.83630  loss = 2.72569 avg_loss = 3.76150\n",
      "epoch no.1 train no.83640  loss = 1.63247 avg_loss = 3.72554\n",
      "epoch no.1 train no.83650  loss = 3.63875 avg_loss = 3.70502\n",
      "epoch no.1 train no.83660  loss = 3.95410 avg_loss = 3.78354\n",
      "epoch no.1 train no.83670  loss = 3.56244 avg_loss = 3.78832\n",
      "epoch no.1 train no.83680  loss = 3.04251 avg_loss = 3.75107\n",
      "epoch no.1 train no.83690  loss = 6.12014 avg_loss = 3.75994\n",
      "epoch no.1 train no.83700  loss = 5.06799 avg_loss = 3.73126\n",
      "epoch no.1 train no.83710  loss = 2.82105 avg_loss = 3.70464\n",
      "epoch no.1 train no.83720  loss = 2.28092 avg_loss = 3.70768\n",
      "epoch no.1 train no.83730  loss = 3.71061 avg_loss = 3.71667\n",
      "epoch no.1 train no.83740  loss = 2.48978 avg_loss = 3.70701\n",
      "epoch no.1 train no.83750  loss = 3.64302 avg_loss = 3.71407\n",
      "epoch no.1 train no.83760  loss = 3.18090 avg_loss = 3.73560\n",
      "epoch no.1 train no.83770  loss = 3.50389 avg_loss = 3.76473\n",
      "epoch no.1 train no.83780  loss = 3.38323 avg_loss = 3.81376\n",
      "epoch no.1 train no.83790  loss = 5.89589 avg_loss = 3.77428\n",
      "epoch no.1 train no.83800  loss = 3.43001 avg_loss = 3.75554\n",
      "epoch no.1 train no.83810  loss = 4.78848 avg_loss = 3.75631\n",
      "epoch no.1 train no.83820  loss = 4.66319 avg_loss = 3.81711\n",
      "epoch no.1 train no.83830  loss = 3.70433 avg_loss = 3.82835\n",
      "epoch no.1 train no.83840  loss = 5.35373 avg_loss = 3.81696\n",
      "epoch no.1 train no.83850  loss = 5.16621 avg_loss = 3.87973\n",
      "epoch no.1 train no.83860  loss = 3.46435 avg_loss = 3.84202\n",
      "epoch no.1 train no.83870  loss = 4.37737 avg_loss = 3.79373\n",
      "epoch no.1 train no.83880  loss = 3.71091 avg_loss = 3.76812\n",
      "epoch no.1 train no.83890  loss = 4.30591 avg_loss = 3.71509\n",
      "epoch no.1 train no.83900  loss = 4.17767 avg_loss = 3.74909\n",
      "epoch no.1 train no.83910  loss = 4.57100 avg_loss = 3.70729\n",
      "epoch no.1 train no.83920  loss = 3.19117 avg_loss = 3.64071\n",
      "epoch no.1 train no.83930  loss = 5.45429 avg_loss = 3.63932\n",
      "epoch no.1 train no.83940  loss = 2.65078 avg_loss = 3.62694\n",
      "epoch no.1 train no.83950  loss = 3.26590 avg_loss = 3.60726\n",
      "epoch no.1 train no.83960  loss = 2.41016 avg_loss = 3.52898\n",
      "epoch no.1 train no.83970  loss = 4.14904 avg_loss = 3.50058\n",
      "epoch no.1 train no.83980  loss = 3.97949 avg_loss = 3.51493\n",
      "epoch no.1 train no.83990  loss = 3.81959 avg_loss = 3.51401\n",
      "epoch no.1 train no.84000  loss = 2.20690 avg_loss = 3.56245\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '▁o', 'st', '</s>', '집']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.1 train no.84010  loss = 4.21760 avg_loss = 3.58436\n",
      "epoch no.1 train no.84020  loss = 3.59196 avg_loss = 3.60734\n",
      "epoch no.1 train no.84030  loss = 4.26404 avg_loss = 3.61280\n",
      "epoch no.1 train no.84040  loss = 2.65262 avg_loss = 3.59539\n",
      "epoch no.1 train no.84050  loss = 3.59978 avg_loss = 3.62838\n",
      "epoch no.1 train no.84060  loss = 4.76519 avg_loss = 3.67243\n",
      "epoch no.1 train no.84070  loss = 4.47038 avg_loss = 3.66364\n",
      "epoch no.1 train no.84080  loss = 4.06088 avg_loss = 3.73279\n",
      "epoch no.1 train no.84090  loss = 2.65524 avg_loss = 3.69599\n",
      "epoch no.1 train no.84100  loss = 2.86512 avg_loss = 3.68658\n",
      "epoch no.1 train no.84110  loss = 3.12023 avg_loss = 3.68862\n",
      "epoch no.1 train no.84120  loss = 5.78847 avg_loss = 3.67261\n",
      "epoch no.1 train no.84130  loss = 3.55460 avg_loss = 3.65958\n",
      "epoch no.1 train no.84140  loss = 3.37003 avg_loss = 3.67892\n",
      "epoch no.1 train no.84150  loss = 3.04064 avg_loss = 3.66256\n",
      "epoch no.1 train no.84160  loss = 3.71836 avg_loss = 3.68333\n",
      "epoch no.1 train no.84170  loss = 2.76365 avg_loss = 3.63167\n",
      "epoch no.1 train no.84180  loss = 2.30925 avg_loss = 3.56687\n",
      "epoch no.1 train no.84190  loss = 4.35334 avg_loss = 3.52640\n",
      "epoch no.1 train no.84200  loss = 4.04422 avg_loss = 3.49518\n",
      "epoch no.1 train no.84210  loss = 3.93357 avg_loss = 3.54386\n",
      "epoch no.1 train no.84220  loss = 2.15339 avg_loss = 3.56428\n",
      "epoch no.1 train no.84230  loss = 6.52277 avg_loss = 3.57721\n",
      "epoch no.1 train no.84240  loss = 3.43629 avg_loss = 3.58701\n",
      "epoch no.1 train no.84250  loss = 4.03320 avg_loss = 3.58240\n",
      "epoch no.1 train no.84260  loss = 2.52930 avg_loss = 3.60271\n",
      "epoch no.1 train no.84270  loss = 3.35787 avg_loss = 3.56202\n",
      "epoch no.1 train no.84280  loss = 3.02585 avg_loss = 3.51095\n",
      "epoch no.1 train no.84290  loss = 4.47425 avg_loss = 3.55104\n",
      "epoch no.1 train no.84300  loss = 3.22717 avg_loss = 3.64468\n",
      "epoch no.1 train no.84310  loss = 4.63962 avg_loss = 3.66653\n",
      "epoch no.1 train no.84320  loss = 3.99937 avg_loss = 3.65287\n",
      "epoch no.1 train no.84330  loss = 2.63858 avg_loss = 3.65075\n",
      "epoch no.1 train no.84340  loss = 4.34246 avg_loss = 3.66720\n",
      "epoch no.1 train no.84350  loss = 3.24844 avg_loss = 3.65852\n",
      "epoch no.1 train no.84360  loss = 3.94952 avg_loss = 3.65496\n",
      "epoch no.1 train no.84370  loss = 3.80934 avg_loss = 3.59507\n",
      "epoch no.1 train no.84380  loss = 2.68238 avg_loss = 3.57240\n",
      "epoch no.1 train no.84390  loss = 2.88488 avg_loss = 3.57983\n",
      "epoch no.1 train no.84400  loss = 5.05636 avg_loss = 3.64065\n",
      "epoch no.1 train no.84410  loss = 2.99331 avg_loss = 3.64520\n",
      "epoch no.1 train no.84420  loss = 4.69074 avg_loss = 3.62148\n",
      "epoch no.1 train no.84430  loss = 2.25881 avg_loss = 3.60305\n",
      "epoch no.1 train no.84440  loss = 3.56331 avg_loss = 3.62199\n",
      "epoch no.1 train no.84450  loss = 2.55063 avg_loss = 3.60622\n",
      "epoch no.1 train no.84460  loss = 2.65235 avg_loss = 3.56832\n",
      "epoch no.1 train no.84470  loss = 3.71896 avg_loss = 3.59795\n",
      "epoch no.1 train no.84480  loss = 6.14416 avg_loss = 3.61499\n",
      "epoch no.1 train no.84490  loss = 2.53431 avg_loss = 3.57537\n",
      "epoch no.1 train no.84500  loss = 2.50507 avg_loss = 3.55405\n",
      "epoch no.1 train no.84510  loss = 1.92045 avg_loss = 3.51113\n",
      "epoch no.1 train no.84520  loss = 3.18278 avg_loss = 3.59698\n",
      "epoch no.1 train no.84530  loss = 3.66702 avg_loss = 3.59562\n",
      "epoch no.1 train no.84540  loss = 3.29716 avg_loss = 3.57157\n",
      "epoch no.1 train no.84550  loss = 2.22879 avg_loss = 3.57951\n",
      "epoch no.1 train no.84560  loss = 3.12420 avg_loss = 3.59188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.84570  loss = 3.22432 avg_loss = 3.59204\n",
      "epoch no.1 train no.84580  loss = 3.61475 avg_loss = 3.60596\n",
      "epoch no.1 train no.84590  loss = 1.94235 avg_loss = 3.54268\n",
      "epoch no.1 train no.84600  loss = 3.14655 avg_loss = 3.61649\n",
      "epoch no.1 train no.84610  loss = 4.07972 avg_loss = 3.60382\n",
      "epoch no.1 train no.84620  loss = 3.67921 avg_loss = 3.61783\n",
      "epoch no.1 train no.84630  loss = 2.02170 avg_loss = 3.57629\n",
      "epoch no.1 train no.84640  loss = 3.41811 avg_loss = 3.57072\n",
      "epoch no.1 train no.84650  loss = 3.92429 avg_loss = 3.58771\n",
      "epoch no.1 train no.84660  loss = 3.56521 avg_loss = 3.62990\n",
      "epoch no.1 train no.84670  loss = 3.11743 avg_loss = 3.55561\n",
      "epoch no.1 train no.84680  loss = 3.72871 avg_loss = 3.56045\n",
      "epoch no.1 train no.84690  loss = 3.72944 avg_loss = 3.59453\n",
      "epoch no.1 train no.84700  loss = 2.09754 avg_loss = 3.56219\n",
      "epoch no.1 train no.84710  loss = 4.20669 avg_loss = 3.57105\n",
      "epoch no.1 train no.84720  loss = 2.95074 avg_loss = 3.61131\n",
      "epoch no.1 train no.84730  loss = 4.06083 avg_loss = 3.62629\n",
      "epoch no.1 train no.84740  loss = 2.69720 avg_loss = 3.58917\n",
      "epoch no.1 train no.84750  loss = 4.84438 avg_loss = 3.56318\n",
      "epoch no.1 train no.84760  loss = 4.87682 avg_loss = 3.60457\n",
      "epoch no.1 train no.84770  loss = 4.44511 avg_loss = 3.62363\n",
      "epoch no.1 train no.84780  loss = 2.11978 avg_loss = 3.58718\n",
      "epoch no.1 train no.84790  loss = 5.39386 avg_loss = 3.63436\n",
      "epoch no.1 train no.84800  loss = 2.62823 avg_loss = 3.68655\n",
      "epoch no.1 train no.84810  loss = 3.93244 avg_loss = 3.64878\n",
      "epoch no.1 train no.84820  loss = 7.57088 avg_loss = 3.73329\n",
      "epoch no.1 train no.84830  loss = 2.20222 avg_loss = 3.70804\n",
      "epoch no.1 train no.84840  loss = 3.71705 avg_loss = 3.70187\n",
      "epoch no.1 train no.84850  loss = 5.30303 avg_loss = 3.74950\n",
      "epoch no.1 train no.84860  loss = 4.33647 avg_loss = 3.71289\n",
      "epoch no.1 train no.84870  loss = 3.56937 avg_loss = 3.67798\n",
      "epoch no.1 train no.84880  loss = 2.13482 avg_loss = 3.67456\n",
      "epoch no.1 train no.84890  loss = 2.90743 avg_loss = 3.62777\n",
      "epoch no.1 train no.84900  loss = 3.67408 avg_loss = 3.62468\n",
      "epoch no.1 train no.84910  loss = 3.58879 avg_loss = 3.69629\n",
      "epoch no.1 train no.84920  loss = 2.91974 avg_loss = 3.68178\n",
      "epoch no.1 train no.84930  loss = 4.29444 avg_loss = 3.68519\n",
      "epoch no.1 train no.84940  loss = 2.77971 avg_loss = 3.74380\n",
      "epoch no.1 train no.84950  loss = 4.08549 avg_loss = 3.74830\n",
      "epoch no.1 train no.84960  loss = 4.42960 avg_loss = 3.73679\n",
      "epoch no.1 train no.84970  loss = 2.60720 avg_loss = 3.71071\n",
      "epoch no.1 train no.84980  loss = 2.56732 avg_loss = 3.71461\n",
      "epoch no.1 train no.84990  loss = 3.52645 avg_loss = 3.71958\n",
      "epoch no.1 train no.85000  loss = 5.11662 avg_loss = 3.66696\n",
      "3\n",
      "to_tokens: ['▁가을', '▁90', '곡', '들', '</s>']\n",
      "추억의 댄스곡들</s>\n",
      "epoch no.1 train no.85010  loss = 4.34465 avg_loss = 3.74053\n",
      "epoch no.1 train no.85020  loss = 2.51840 avg_loss = 3.69618\n",
      "epoch no.1 train no.85030  loss = 4.67818 avg_loss = 3.72883\n",
      "epoch no.1 train no.85040  loss = 3.61679 avg_loss = 3.73461\n",
      "epoch no.1 train no.85050  loss = 2.28127 avg_loss = 3.77465\n",
      "epoch no.1 train no.85060  loss = 3.24519 avg_loss = 3.76214\n",
      "epoch no.1 train no.85070  loss = 2.90060 avg_loss = 3.74150\n",
      "epoch no.1 train no.85080  loss = 5.98933 avg_loss = 3.74701\n",
      "epoch no.1 train no.85090  loss = 3.03447 avg_loss = 3.70218\n",
      "epoch no.1 train no.85100  loss = 2.03120 avg_loss = 3.68403\n",
      "epoch no.1 train no.85110  loss = 3.76886 avg_loss = 3.66962\n",
      "epoch no.1 train no.85120  loss = 3.48062 avg_loss = 3.67785\n",
      "epoch no.1 train no.85130  loss = 3.24558 avg_loss = 3.70014\n",
      "epoch no.1 train no.85140  loss = 2.44263 avg_loss = 3.73075\n",
      "epoch no.1 train no.85150  loss = 3.31733 avg_loss = 3.64952\n",
      "epoch no.1 train no.85160  loss = 3.68209 avg_loss = 3.67683\n",
      "epoch no.1 train no.85170  loss = 2.09919 avg_loss = 3.66580\n",
      "epoch no.1 train no.85180  loss = 2.87792 avg_loss = 3.66455\n",
      "epoch no.1 train no.85190  loss = 2.89470 avg_loss = 3.64519\n",
      "epoch no.1 train no.85200  loss = 3.45136 avg_loss = 3.59537\n",
      "epoch no.1 train no.85210  loss = 2.72332 avg_loss = 3.55492\n",
      "epoch no.1 train no.85220  loss = 2.87432 avg_loss = 3.60001\n",
      "epoch no.1 train no.85230  loss = 2.33879 avg_loss = 3.57659\n",
      "epoch no.1 train no.85240  loss = 4.64343 avg_loss = 3.56621\n",
      "epoch no.1 train no.85250  loss = 5.07218 avg_loss = 3.63221\n",
      "epoch no.1 train no.85260  loss = 6.03195 avg_loss = 3.64748\n",
      "epoch no.1 train no.85270  loss = 2.19846 avg_loss = 3.68709\n",
      "epoch no.1 train no.85280  loss = 2.12856 avg_loss = 3.71912\n",
      "epoch no.1 train no.85290  loss = 2.68953 avg_loss = 3.69133\n",
      "epoch no.1 train no.85300  loss = 2.83597 avg_loss = 3.69259\n",
      "epoch no.1 train no.85310  loss = 4.23728 avg_loss = 3.66328\n",
      "epoch no.1 train no.85320  loss = 2.81789 avg_loss = 3.60404\n",
      "epoch no.1 train no.85330  loss = 1.41964 avg_loss = 3.59891\n",
      "epoch no.1 train no.85340  loss = 2.24564 avg_loss = 3.59128\n",
      "epoch no.1 train no.85350  loss = 2.95904 avg_loss = 3.58263\n",
      "epoch no.1 train no.85360  loss = 4.34344 avg_loss = 3.55529\n",
      "epoch no.1 train no.85370  loss = 2.92276 avg_loss = 3.53490\n",
      "epoch no.1 train no.85380  loss = 5.19320 avg_loss = 3.55180\n",
      "epoch no.1 train no.85390  loss = 3.41071 avg_loss = 3.54555\n",
      "epoch no.1 train no.85400  loss = 5.39236 avg_loss = 3.59977\n",
      "epoch no.1 train no.85410  loss = 2.17918 avg_loss = 3.56251\n",
      "epoch no.1 train no.85420  loss = 4.39567 avg_loss = 3.60856\n",
      "epoch no.1 train no.85430  loss = 3.77558 avg_loss = 3.66354\n",
      "epoch no.1 train no.85440  loss = 2.86176 avg_loss = 3.63703\n",
      "epoch no.1 train no.85450  loss = 4.38005 avg_loss = 3.69332\n",
      "epoch no.1 train no.85460  loss = 3.87714 avg_loss = 3.73497\n",
      "epoch no.1 train no.85470  loss = 5.33258 avg_loss = 3.71921\n",
      "epoch no.1 train no.85480  loss = 2.37616 avg_loss = 3.65680\n",
      "epoch no.1 train no.85490  loss = 1.99408 avg_loss = 3.62486\n",
      "epoch no.1 train no.85500  loss = 3.32108 avg_loss = 3.64323\n",
      "epoch no.1 train no.85510  loss = 6.36449 avg_loss = 3.67208\n",
      "epoch no.1 train no.85520  loss = 6.19192 avg_loss = 3.70618\n",
      "epoch no.1 train no.85530  loss = 2.99431 avg_loss = 3.67426\n",
      "epoch no.1 train no.85540  loss = 3.49705 avg_loss = 3.64350\n",
      "epoch no.1 train no.85550  loss = 3.51745 avg_loss = 3.63944\n",
      "epoch no.1 train no.85560  loss = 4.03885 avg_loss = 3.67422\n",
      "epoch no.1 train no.85570  loss = 3.09155 avg_loss = 3.64431\n",
      "epoch no.1 train no.85580  loss = 6.87596 avg_loss = 3.66865\n",
      "epoch no.1 train no.85590  loss = 2.34004 avg_loss = 3.61494\n",
      "epoch no.1 train no.85600  loss = 4.55614 avg_loss = 3.65038\n",
      "epoch no.1 train no.85610  loss = 2.22503 avg_loss = 3.65294\n",
      "epoch no.1 train no.85620  loss = 4.04269 avg_loss = 3.71424\n",
      "epoch no.1 train no.85630  loss = 4.38444 avg_loss = 3.67950\n",
      "epoch no.1 train no.85640  loss = 3.38006 avg_loss = 3.64394\n",
      "epoch no.1 train no.85650  loss = 5.00252 avg_loss = 3.71312\n",
      "epoch no.1 train no.85660  loss = 3.04323 avg_loss = 3.72073\n",
      "epoch no.1 train no.85670  loss = 6.09366 avg_loss = 3.76614\n",
      "epoch no.1 train no.85680  loss = 6.01534 avg_loss = 3.74157\n",
      "epoch no.1 train no.85690  loss = 2.25429 avg_loss = 3.68867\n",
      "epoch no.1 train no.85700  loss = 4.15696 avg_loss = 3.65755\n",
      "epoch no.1 train no.85710  loss = 4.26633 avg_loss = 3.62044\n",
      "epoch no.1 train no.85720  loss = 3.64512 avg_loss = 3.61971\n",
      "epoch no.1 train no.85730  loss = 4.07977 avg_loss = 3.63951\n",
      "epoch no.1 train no.85740  loss = 1.73644 avg_loss = 3.62849\n",
      "epoch no.1 train no.85750  loss = 3.68361 avg_loss = 3.64035\n",
      "epoch no.1 train no.85760  loss = 4.75889 avg_loss = 3.62951\n",
      "epoch no.1 train no.85770  loss = 3.34350 avg_loss = 3.71478\n",
      "epoch no.1 train no.85780  loss = 1.58826 avg_loss = 3.63507\n",
      "epoch no.1 train no.85790  loss = 3.63627 avg_loss = 3.64863\n",
      "epoch no.1 train no.85800  loss = 3.05646 avg_loss = 3.68425\n",
      "epoch no.1 train no.85810  loss = 3.21224 avg_loss = 3.66396\n",
      "epoch no.1 train no.85820  loss = 4.72670 avg_loss = 3.64853\n",
      "epoch no.1 train no.85830  loss = 1.27361 avg_loss = 3.63611\n",
      "epoch no.1 train no.85840  loss = 2.76586 avg_loss = 3.62693\n",
      "epoch no.1 train no.85850  loss = 2.50554 avg_loss = 3.59443\n",
      "epoch no.1 train no.85860  loss = 5.63950 avg_loss = 3.61114\n",
      "epoch no.1 train no.85870  loss = 3.38188 avg_loss = 3.62598\n",
      "epoch no.1 train no.85880  loss = 5.63246 avg_loss = 3.66519\n",
      "epoch no.1 train no.85890  loss = 2.82058 avg_loss = 3.63390\n",
      "epoch no.1 train no.85900  loss = 4.36769 avg_loss = 3.62301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.85910  loss = 2.68123 avg_loss = 3.63894\n",
      "epoch no.1 train no.85920  loss = 3.33006 avg_loss = 3.61547\n",
      "epoch no.1 train no.85930  loss = 5.04779 avg_loss = 3.64078\n",
      "epoch no.1 train no.85940  loss = 3.92444 avg_loss = 3.64201\n",
      "epoch no.1 train no.85950  loss = 3.69285 avg_loss = 3.61130\n",
      "epoch no.1 train no.85960  loss = 1.94015 avg_loss = 3.58323\n",
      "epoch no.1 train no.85970  loss = 2.11230 avg_loss = 3.58333\n",
      "epoch no.1 train no.85980  loss = 6.03399 avg_loss = 3.57420\n",
      "epoch no.1 train no.85990  loss = 4.94039 avg_loss = 3.57339\n",
      "epoch no.1 train no.86000  loss = 3.97789 avg_loss = 3.54796\n",
      "3\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁가요', '</s>']\n",
      "추억의 90년대 가요</s>\n",
      "epoch no.1 train no.86010  loss = 2.53370 avg_loss = 3.52793\n",
      "epoch no.1 train no.86020  loss = 3.82736 avg_loss = 3.52667\n",
      "epoch no.1 train no.86030  loss = 2.49460 avg_loss = 3.55054\n",
      "epoch no.1 train no.86040  loss = 6.11255 avg_loss = 3.53792\n",
      "epoch no.1 train no.86050  loss = 3.78697 avg_loss = 3.56603\n",
      "epoch no.1 train no.86060  loss = 5.68902 avg_loss = 3.59871\n",
      "epoch no.1 train no.86070  loss = 2.94790 avg_loss = 3.55115\n",
      "epoch no.1 train no.86080  loss = 4.44055 avg_loss = 3.53790\n",
      "epoch no.1 train no.86090  loss = 1.55425 avg_loss = 3.51186\n",
      "epoch no.1 train no.86100  loss = 4.40069 avg_loss = 3.52017\n",
      "epoch no.1 train no.86110  loss = 2.88928 avg_loss = 3.58926\n",
      "epoch no.1 train no.86120  loss = 2.84221 avg_loss = 3.50176\n",
      "epoch no.1 train no.86130  loss = 4.73110 avg_loss = 3.61148\n",
      "epoch no.1 train no.86140  loss = 3.46646 avg_loss = 3.59027\n",
      "epoch no.1 train no.86150  loss = 2.58548 avg_loss = 3.54618\n",
      "epoch no.1 train no.86160  loss = 6.07411 avg_loss = 3.60448\n",
      "epoch no.1 train no.86170  loss = 4.04299 avg_loss = 3.59788\n",
      "epoch no.1 train no.86180  loss = 2.46601 avg_loss = 3.61309\n",
      "epoch no.1 train no.86190  loss = 2.94418 avg_loss = 3.61710\n",
      "epoch no.1 train no.86200  loss = 3.89374 avg_loss = 3.58299\n",
      "epoch no.1 train no.86210  loss = 3.43916 avg_loss = 3.59373\n",
      "epoch no.1 train no.86220  loss = 4.35149 avg_loss = 3.56497\n",
      "epoch no.1 train no.86230  loss = 4.41327 avg_loss = 3.61687\n",
      "epoch no.1 train no.86240  loss = 3.53606 avg_loss = 3.70881\n",
      "epoch no.1 train no.86250  loss = 2.54982 avg_loss = 3.70027\n",
      "epoch no.1 train no.86260  loss = 2.71779 avg_loss = 3.69148\n",
      "epoch no.1 train no.86270  loss = 4.52007 avg_loss = 3.68305\n",
      "epoch no.1 train no.86280  loss = 4.88934 avg_loss = 3.68231\n",
      "epoch no.1 train no.86290  loss = 3.11540 avg_loss = 3.72305\n",
      "epoch no.1 train no.86300  loss = 3.06379 avg_loss = 3.69414\n",
      "epoch no.1 train no.86310  loss = 3.95469 avg_loss = 3.64848\n",
      "epoch no.1 train no.86320  loss = 2.25905 avg_loss = 3.65252\n",
      "epoch no.1 train no.86330  loss = 3.79687 avg_loss = 3.70504\n",
      "epoch no.1 train no.86340  loss = 4.33668 avg_loss = 3.72846\n",
      "epoch no.1 train no.86350  loss = 5.06652 avg_loss = 3.79799\n",
      "epoch no.1 train no.86360  loss = 4.58080 avg_loss = 3.80940\n",
      "epoch no.1 train no.86370  loss = 3.34893 avg_loss = 3.81841\n",
      "epoch no.1 train no.86380  loss = 3.42829 avg_loss = 3.86476\n",
      "epoch no.1 train no.86390  loss = 3.87700 avg_loss = 3.85270\n",
      "epoch no.1 train no.86400  loss = 2.55643 avg_loss = 3.77733\n",
      "epoch no.1 train no.86410  loss = 3.52649 avg_loss = 3.69270\n",
      "epoch no.1 train no.86420  loss = 3.73677 avg_loss = 3.67868\n",
      "epoch no.1 train no.86430  loss = 2.78396 avg_loss = 3.68118\n",
      "epoch no.1 train no.86440  loss = 3.66642 avg_loss = 3.73907\n",
      "epoch no.1 train no.86450  loss = 2.39329 avg_loss = 3.72942\n",
      "epoch no.1 train no.86460  loss = 6.56699 avg_loss = 3.71344\n",
      "epoch no.1 train no.86470  loss = 2.12966 avg_loss = 3.71143\n",
      "epoch no.1 train no.86480  loss = 4.19203 avg_loss = 3.67215\n",
      "epoch no.1 train no.86490  loss = 3.64793 avg_loss = 3.62913\n",
      "epoch no.1 train no.86500  loss = 5.16166 avg_loss = 3.66967\n",
      "epoch no.1 train no.86510  loss = 2.58711 avg_loss = 3.63823\n",
      "epoch no.1 train no.86520  loss = 5.29797 avg_loss = 3.72311\n",
      "epoch no.1 train no.86530  loss = 5.18265 avg_loss = 3.71043\n",
      "epoch no.1 train no.86540  loss = 2.14695 avg_loss = 3.66176\n",
      "epoch no.1 train no.86550  loss = 3.05394 avg_loss = 3.64683\n",
      "epoch no.1 train no.86560  loss = 3.00031 avg_loss = 3.59865\n",
      "epoch no.1 train no.86570  loss = 3.44503 avg_loss = 3.63289\n",
      "epoch no.1 train no.86580  loss = 2.85179 avg_loss = 3.58682\n",
      "epoch no.1 train no.86590  loss = 2.99176 avg_loss = 3.56844\n",
      "epoch no.1 train no.86600  loss = 3.40914 avg_loss = 3.55246\n",
      "epoch no.1 train no.86610  loss = 3.16190 avg_loss = 3.55554\n",
      "epoch no.1 train no.86620  loss = 3.42384 avg_loss = 3.57764\n",
      "epoch no.1 train no.86630  loss = 5.88471 avg_loss = 3.60428\n",
      "epoch no.1 train no.86640  loss = 4.49070 avg_loss = 3.61177\n",
      "epoch no.1 train no.86650  loss = 4.97704 avg_loss = 3.60235\n",
      "epoch no.1 train no.86660  loss = 3.50670 avg_loss = 3.64393\n",
      "epoch no.1 train no.86670  loss = 5.99480 avg_loss = 3.65776\n",
      "epoch no.1 train no.86680  loss = 3.37303 avg_loss = 3.70213\n",
      "epoch no.1 train no.86690  loss = 3.57636 avg_loss = 3.66657\n",
      "epoch no.1 train no.86700  loss = 4.25348 avg_loss = 3.62424\n",
      "epoch no.1 train no.86710  loss = 2.55218 avg_loss = 3.61826\n",
      "epoch no.1 train no.86720  loss = 2.77533 avg_loss = 3.63120\n",
      "epoch no.1 train no.86730  loss = 2.51371 avg_loss = 3.59769\n",
      "epoch no.1 train no.86740  loss = 5.17348 avg_loss = 3.60267\n",
      "epoch no.1 train no.86750  loss = 3.58995 avg_loss = 3.59655\n",
      "epoch no.1 train no.86760  loss = 2.85789 avg_loss = 3.59560\n",
      "epoch no.1 train no.86770  loss = 5.71496 avg_loss = 3.63217\n",
      "epoch no.1 train no.86780  loss = 2.93445 avg_loss = 3.59574\n",
      "epoch no.1 train no.86790  loss = 3.15373 avg_loss = 3.59696\n",
      "epoch no.1 train no.86800  loss = 2.14952 avg_loss = 3.58386\n",
      "epoch no.1 train no.86810  loss = 3.65135 avg_loss = 3.62761\n",
      "epoch no.1 train no.86820  loss = 3.97858 avg_loss = 3.57668\n",
      "epoch no.1 train no.86830  loss = 7.53106 avg_loss = 3.64155\n",
      "epoch no.1 train no.86840  loss = 2.44891 avg_loss = 3.59428\n",
      "epoch no.1 train no.86850  loss = 3.27884 avg_loss = 3.63753\n",
      "epoch no.1 train no.86860  loss = 2.11884 avg_loss = 3.56991\n",
      "epoch no.1 train no.86870  loss = 2.74859 avg_loss = 3.54196\n",
      "epoch no.1 train no.86880  loss = 2.31895 avg_loss = 3.52529\n",
      "epoch no.1 train no.86890  loss = 3.10199 avg_loss = 3.55045\n",
      "epoch no.1 train no.86900  loss = 2.19223 avg_loss = 3.53945\n",
      "epoch no.1 train no.86910  loss = 2.39183 avg_loss = 3.55113\n",
      "epoch no.1 train no.86920  loss = 3.37607 avg_loss = 3.54875\n",
      "epoch no.1 train no.86930  loss = 3.46729 avg_loss = 3.57964\n",
      "epoch no.1 train no.86940  loss = 4.42437 avg_loss = 3.61400\n",
      "epoch no.1 train no.86950  loss = 4.53230 avg_loss = 3.63179\n",
      "epoch no.1 train no.86960  loss = 5.21748 avg_loss = 3.61066\n",
      "epoch no.1 train no.86970  loss = 2.70115 avg_loss = 3.58825\n",
      "epoch no.1 train no.86980  loss = 3.27736 avg_loss = 3.61503\n",
      "epoch no.1 train no.86990  loss = 3.54812 avg_loss = 3.64100\n",
      "epoch no.1 train no.87000  loss = 3.88970 avg_loss = 3.61843\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁명', '곡', '들', '</s>']\n",
      "추억의 가요 명곡들</s>\n",
      "epoch no.1 train no.87010  loss = 2.41205 avg_loss = 3.60446\n",
      "epoch no.1 train no.87020  loss = 1.87533 avg_loss = 3.60730\n",
      "epoch no.1 train no.87030  loss = 3.44018 avg_loss = 3.61736\n",
      "epoch no.1 train no.87040  loss = 4.47814 avg_loss = 3.63242\n",
      "epoch no.1 train no.87050  loss = 3.88051 avg_loss = 3.60221\n",
      "epoch no.1 train no.87060  loss = 2.03060 avg_loss = 3.61710\n",
      "epoch no.1 train no.87070  loss = 2.92735 avg_loss = 3.60477\n",
      "epoch no.1 train no.87080  loss = 5.25132 avg_loss = 3.60370\n",
      "epoch no.1 train no.87090  loss = 4.96261 avg_loss = 3.60810\n",
      "epoch no.1 train no.87100  loss = 2.43283 avg_loss = 3.58130\n",
      "epoch no.1 train no.87110  loss = 5.26524 avg_loss = 3.66238\n",
      "epoch no.1 train no.87120  loss = 2.40014 avg_loss = 3.61665\n",
      "epoch no.1 train no.87130  loss = 4.54504 avg_loss = 3.59970\n",
      "epoch no.1 train no.87140  loss = 4.34518 avg_loss = 3.55601\n",
      "epoch no.1 train no.87150  loss = 3.00836 avg_loss = 3.61280\n",
      "epoch no.1 train no.87160  loss = 4.12351 avg_loss = 3.57829\n",
      "epoch no.1 train no.87170  loss = 2.69593 avg_loss = 3.53820\n",
      "epoch no.1 train no.87180  loss = 2.66456 avg_loss = 3.49440\n",
      "epoch no.1 train no.87190  loss = 4.16264 avg_loss = 3.53861\n",
      "epoch no.1 train no.87200  loss = 3.21696 avg_loss = 3.56919\n",
      "epoch no.1 train no.87210  loss = 4.11484 avg_loss = 3.52260\n",
      "epoch no.1 train no.87220  loss = 6.55798 avg_loss = 3.53916\n",
      "epoch no.1 train no.87230  loss = 4.34000 avg_loss = 3.55684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.87240  loss = 5.89314 avg_loss = 3.54544\n",
      "epoch no.1 train no.87250  loss = 2.48086 avg_loss = 3.54560\n",
      "epoch no.1 train no.87260  loss = 2.94130 avg_loss = 3.59797\n",
      "epoch no.1 train no.87270  loss = 4.16582 avg_loss = 3.59741\n",
      "epoch no.1 train no.87280  loss = 4.08933 avg_loss = 3.58526\n",
      "epoch no.1 train no.87290  loss = 4.37934 avg_loss = 3.60184\n",
      "epoch no.1 train no.87300  loss = 3.75521 avg_loss = 3.59420\n",
      "epoch no.1 train no.87310  loss = 4.40200 avg_loss = 3.61063\n",
      "epoch no.1 train no.87320  loss = 3.79969 avg_loss = 3.60070\n",
      "epoch no.1 train no.87330  loss = 3.40855 avg_loss = 3.65606\n",
      "epoch no.1 train no.87340  loss = 4.95027 avg_loss = 3.71994\n",
      "epoch no.1 train no.87350  loss = 2.13348 avg_loss = 3.68409\n",
      "epoch no.1 train no.87360  loss = 4.58751 avg_loss = 3.67964\n",
      "epoch no.1 train no.87370  loss = 4.00844 avg_loss = 3.66990\n",
      "epoch no.1 train no.87380  loss = 5.28198 avg_loss = 3.65681\n",
      "epoch no.1 train no.87390  loss = 3.88773 avg_loss = 3.65920\n",
      "epoch no.1 train no.87400  loss = 3.74919 avg_loss = 3.67890\n",
      "epoch no.1 train no.87410  loss = 4.03212 avg_loss = 3.67241\n",
      "epoch no.1 train no.87420  loss = 5.65734 avg_loss = 3.70215\n",
      "epoch no.1 train no.87430  loss = 3.66458 avg_loss = 3.67623\n",
      "epoch no.1 train no.87440  loss = 3.67863 avg_loss = 3.67102\n",
      "epoch no.1 train no.87450  loss = 2.58382 avg_loss = 3.68410\n",
      "epoch no.1 train no.87460  loss = 4.22628 avg_loss = 3.75036\n",
      "epoch no.1 train no.87470  loss = 4.92154 avg_loss = 3.75041\n",
      "epoch no.1 train no.87480  loss = 2.55083 avg_loss = 3.73174\n",
      "epoch no.1 train no.87490  loss = 1.76495 avg_loss = 3.74067\n",
      "epoch no.1 train no.87500  loss = 2.02586 avg_loss = 3.68267\n",
      "epoch no.1 train no.87510  loss = 5.67213 avg_loss = 3.73097\n",
      "epoch no.1 train no.87520  loss = 3.50893 avg_loss = 3.73711\n",
      "epoch no.1 train no.87530  loss = 3.61136 avg_loss = 3.72066\n",
      "epoch no.1 train no.87540  loss = 2.47250 avg_loss = 3.67573\n",
      "epoch no.1 train no.87550  loss = 4.12867 avg_loss = 3.68656\n",
      "epoch no.1 train no.87560  loss = 3.26232 avg_loss = 3.64038\n",
      "epoch no.1 train no.87570  loss = 6.56020 avg_loss = 3.66637\n",
      "epoch no.1 train no.87580  loss = 3.30815 avg_loss = 3.64589\n",
      "epoch no.1 train no.87590  loss = 2.25727 avg_loss = 3.66728\n",
      "epoch no.1 train no.87600  loss = 2.21565 avg_loss = 3.64374\n",
      "epoch no.1 train no.87610  loss = 3.41218 avg_loss = 3.69201\n",
      "epoch no.1 train no.87620  loss = 4.81322 avg_loss = 3.73005\n",
      "epoch no.1 train no.87630  loss = 2.49313 avg_loss = 3.73451\n",
      "epoch no.1 train no.87640  loss = 3.25168 avg_loss = 3.75129\n",
      "epoch no.1 train no.87650  loss = 3.69572 avg_loss = 3.70267\n",
      "epoch no.1 train no.87660  loss = 2.98877 avg_loss = 3.65878\n",
      "epoch no.1 train no.87670  loss = 2.96664 avg_loss = 3.63062\n",
      "epoch no.1 train no.87680  loss = 6.09277 avg_loss = 3.64761\n",
      "epoch no.1 train no.87690  loss = 2.88391 avg_loss = 3.68334\n",
      "epoch no.1 train no.87700  loss = 7.17646 avg_loss = 3.67229\n",
      "epoch no.1 train no.87710  loss = 3.36862 avg_loss = 3.61763\n",
      "epoch no.1 train no.87720  loss = 6.05759 avg_loss = 3.68533\n",
      "epoch no.1 train no.87730  loss = 3.71764 avg_loss = 3.68086\n",
      "epoch no.1 train no.87740  loss = 3.77557 avg_loss = 3.68438\n",
      "epoch no.1 train no.87750  loss = 2.98601 avg_loss = 3.68613\n",
      "epoch no.1 train no.87760  loss = 3.90855 avg_loss = 3.74679\n",
      "epoch no.1 train no.87770  loss = 2.94643 avg_loss = 3.77383\n",
      "epoch no.1 train no.87780  loss = 6.29234 avg_loss = 3.76880\n",
      "epoch no.1 train no.87790  loss = 3.99967 avg_loss = 3.78825\n",
      "epoch no.1 train no.87800  loss = 5.38981 avg_loss = 3.74017\n",
      "epoch no.1 train no.87810  loss = 2.90805 avg_loss = 3.73457\n",
      "epoch no.1 train no.87820  loss = 3.40891 avg_loss = 3.71189\n",
      "epoch no.1 train no.87830  loss = 3.01593 avg_loss = 3.68395\n",
      "epoch no.1 train no.87840  loss = 6.98223 avg_loss = 3.76686\n",
      "epoch no.1 train no.87850  loss = 5.38439 avg_loss = 3.75638\n",
      "epoch no.1 train no.87860  loss = 3.12848 avg_loss = 3.78577\n",
      "epoch no.1 train no.87870  loss = 3.70449 avg_loss = 3.74008\n",
      "epoch no.1 train no.87880  loss = 4.78100 avg_loss = 3.76889\n",
      "epoch no.1 train no.87890  loss = 5.14998 avg_loss = 3.74708\n",
      "epoch no.1 train no.87900  loss = 4.04120 avg_loss = 3.72137\n",
      "epoch no.1 train no.87910  loss = 6.42733 avg_loss = 3.74959\n",
      "epoch no.1 train no.87920  loss = 3.38808 avg_loss = 3.78461\n",
      "epoch no.1 train no.87930  loss = 2.64308 avg_loss = 3.70355\n",
      "epoch no.1 train no.87940  loss = 2.41254 avg_loss = 3.67203\n",
      "epoch no.1 train no.87950  loss = 2.69738 avg_loss = 3.62960\n",
      "epoch no.1 train no.87960  loss = 4.00050 avg_loss = 3.64551\n",
      "epoch no.1 train no.87970  loss = 2.85126 avg_loss = 3.63802\n",
      "epoch no.1 train no.87980  loss = 3.45277 avg_loss = 3.64925\n",
      "epoch no.1 train no.87990  loss = 3.93555 avg_loss = 3.63990\n",
      "epoch no.1 train no.88000  loss = 3.89232 avg_loss = 3.61894\n",
      "3\n",
      "to_tokens: ['▁가을', '▁90', '들', '음', '</s>']\n",
      "추억의노래모음</s>\n",
      "epoch no.1 train no.88010  loss = 4.45430 avg_loss = 3.59652\n",
      "epoch no.1 train no.88020  loss = 3.41543 avg_loss = 3.56699\n",
      "epoch no.1 train no.88030  loss = 3.43306 avg_loss = 3.55650\n",
      "epoch no.1 train no.88040  loss = 3.81341 avg_loss = 3.55145\n",
      "epoch no.1 train no.88050  loss = 4.01178 avg_loss = 3.59649\n",
      "epoch no.1 train no.88060  loss = 2.57938 avg_loss = 3.54429\n",
      "epoch no.1 train no.88070  loss = 2.21685 avg_loss = 3.54509\n",
      "epoch no.1 train no.88080  loss = 2.69424 avg_loss = 3.57954\n",
      "epoch no.1 train no.88090  loss = 2.88138 avg_loss = 3.52863\n",
      "epoch no.1 train no.88100  loss = 2.12210 avg_loss = 3.52356\n",
      "epoch no.1 train no.88110  loss = 4.87475 avg_loss = 3.52588\n",
      "epoch no.1 train no.88120  loss = 3.68582 avg_loss = 3.54616\n",
      "epoch no.1 train no.88130  loss = 3.63627 avg_loss = 3.61990\n",
      "epoch no.1 train no.88140  loss = 3.85224 avg_loss = 3.62355\n",
      "epoch no.1 train no.88150  loss = 5.74436 avg_loss = 3.62038\n",
      "epoch no.1 train no.88160  loss = 2.75745 avg_loss = 3.59389\n",
      "epoch no.1 train no.88170  loss = 3.17444 avg_loss = 3.62011\n",
      "epoch no.1 train no.88180  loss = 2.71143 avg_loss = 3.69540\n",
      "epoch no.1 train no.88190  loss = 6.53516 avg_loss = 3.74414\n",
      "epoch no.1 train no.88200  loss = 2.92419 avg_loss = 3.73876\n",
      "epoch no.1 train no.88210  loss = 2.76018 avg_loss = 3.72493\n",
      "epoch no.1 train no.88220  loss = 3.97762 avg_loss = 3.78310\n",
      "epoch no.1 train no.88230  loss = 4.34259 avg_loss = 3.77965\n",
      "epoch no.1 train no.88240  loss = 4.44595 avg_loss = 3.81588\n",
      "epoch no.1 train no.88250  loss = 4.70997 avg_loss = 3.81170\n",
      "epoch no.1 train no.88260  loss = 3.93731 avg_loss = 3.80668\n",
      "epoch no.1 train no.88270  loss = 6.37585 avg_loss = 3.81529\n",
      "epoch no.1 train no.88280  loss = 3.19224 avg_loss = 3.73955\n",
      "epoch no.1 train no.88290  loss = 4.99309 avg_loss = 3.74489\n",
      "epoch no.1 train no.88300  loss = 3.07653 avg_loss = 3.72602\n",
      "epoch no.1 train no.88310  loss = 4.05322 avg_loss = 3.70018\n",
      "epoch no.1 train no.88320  loss = 4.32432 avg_loss = 3.67121\n",
      "epoch no.1 train no.88330  loss = 2.88234 avg_loss = 3.68275\n",
      "epoch no.1 train no.88340  loss = 3.34664 avg_loss = 3.65139\n",
      "epoch no.1 train no.88350  loss = 4.19556 avg_loss = 3.67586\n",
      "epoch no.1 train no.88360  loss = 4.85537 avg_loss = 3.72837\n",
      "epoch no.1 train no.88370  loss = 2.60405 avg_loss = 3.68920\n",
      "epoch no.1 train no.88380  loss = 5.70090 avg_loss = 3.75365\n",
      "epoch no.1 train no.88390  loss = 2.76730 avg_loss = 3.72717\n",
      "epoch no.1 train no.88400  loss = 3.27104 avg_loss = 3.70075\n",
      "epoch no.1 train no.88410  loss = 3.35676 avg_loss = 3.69147\n",
      "epoch no.1 train no.88420  loss = 4.55299 avg_loss = 3.67072\n",
      "epoch no.1 train no.88430  loss = 2.87698 avg_loss = 3.65341\n",
      "epoch no.1 train no.88440  loss = 3.95503 avg_loss = 3.65357\n",
      "epoch no.1 train no.88450  loss = 3.12243 avg_loss = 3.64204\n",
      "epoch no.1 train no.88460  loss = 3.25798 avg_loss = 3.57577\n",
      "epoch no.1 train no.88470  loss = 3.75503 avg_loss = 3.61006\n",
      "epoch no.1 train no.88480  loss = 2.83100 avg_loss = 3.62167\n",
      "epoch no.1 train no.88490  loss = 4.08503 avg_loss = 3.60515\n",
      "epoch no.1 train no.88500  loss = 2.18669 avg_loss = 3.54325\n",
      "epoch no.1 train no.88510  loss = 2.67559 avg_loss = 3.59882\n",
      "epoch no.1 train no.88520  loss = 2.10742 avg_loss = 3.58469\n",
      "epoch no.1 train no.88530  loss = 2.88961 avg_loss = 3.60776\n",
      "epoch no.1 train no.88540  loss = 3.07326 avg_loss = 3.65534\n",
      "epoch no.1 train no.88550  loss = 2.43001 avg_loss = 3.63011\n",
      "epoch no.1 train no.88560  loss = 2.09680 avg_loss = 3.59946\n",
      "epoch no.1 train no.88570  loss = 3.47525 avg_loss = 3.58450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.88580  loss = 4.34254 avg_loss = 3.59057\n",
      "epoch no.1 train no.88590  loss = 4.39572 avg_loss = 3.63535\n",
      "epoch no.1 train no.88600  loss = 1.97464 avg_loss = 3.68011\n",
      "epoch no.1 train no.88610  loss = 2.01150 avg_loss = 3.71157\n",
      "epoch no.1 train no.88620  loss = 3.63678 avg_loss = 3.68589\n",
      "epoch no.1 train no.88630  loss = 5.07822 avg_loss = 3.67220\n",
      "epoch no.1 train no.88640  loss = 5.54372 avg_loss = 3.69906\n",
      "epoch no.1 train no.88650  loss = 1.73940 avg_loss = 3.69304\n",
      "epoch no.1 train no.88660  loss = 5.03703 avg_loss = 3.65307\n",
      "epoch no.1 train no.88670  loss = 4.74615 avg_loss = 3.66822\n",
      "epoch no.1 train no.88680  loss = 2.72972 avg_loss = 3.68006\n",
      "epoch no.1 train no.88690  loss = 6.55720 avg_loss = 3.70772\n",
      "epoch no.1 train no.88700  loss = 3.25543 avg_loss = 3.72075\n",
      "epoch no.1 train no.88710  loss = 3.13471 avg_loss = 3.71559\n",
      "epoch no.1 train no.88720  loss = 4.53287 avg_loss = 3.74900\n",
      "epoch no.1 train no.88730  loss = 4.18688 avg_loss = 3.79136\n",
      "epoch no.1 train no.88740  loss = 2.12164 avg_loss = 3.76412\n",
      "epoch no.1 train no.88750  loss = 3.43051 avg_loss = 3.71585\n",
      "epoch no.1 train no.88760  loss = 2.67626 avg_loss = 3.75393\n",
      "epoch no.1 train no.88770  loss = 6.09250 avg_loss = 3.73071\n",
      "epoch no.1 train no.88780  loss = 3.50308 avg_loss = 3.71764\n",
      "epoch no.1 train no.88790  loss = 2.43834 avg_loss = 3.64918\n",
      "epoch no.1 train no.88800  loss = 3.29637 avg_loss = 3.69022\n",
      "epoch no.1 train no.88810  loss = 2.10396 avg_loss = 3.69125\n",
      "epoch no.1 train no.88820  loss = 2.90017 avg_loss = 3.65339\n",
      "epoch no.1 train no.88830  loss = 4.83478 avg_loss = 3.68598\n",
      "epoch no.1 train no.88840  loss = 4.15538 avg_loss = 3.65232\n",
      "epoch no.1 train no.88850  loss = 4.23747 avg_loss = 3.66507\n",
      "epoch no.1 train no.88860  loss = 4.05981 avg_loss = 3.65015\n",
      "epoch no.1 train no.88870  loss = 3.69912 avg_loss = 3.61885\n",
      "epoch no.1 train no.88880  loss = 3.58047 avg_loss = 3.61419\n",
      "epoch no.1 train no.88890  loss = 3.69365 avg_loss = 3.67505\n",
      "epoch no.1 train no.88900  loss = 4.47494 avg_loss = 3.70035\n",
      "epoch no.1 train no.88910  loss = 3.07482 avg_loss = 3.66583\n",
      "epoch no.1 train no.88920  loss = 3.04586 avg_loss = 3.71265\n",
      "epoch no.1 train no.88930  loss = 3.54138 avg_loss = 3.72057\n",
      "epoch no.1 train no.88940  loss = 4.52588 avg_loss = 3.73232\n",
      "epoch no.1 train no.88950  loss = 2.78193 avg_loss = 3.76644\n",
      "epoch no.1 train no.88960  loss = 2.32945 avg_loss = 3.72250\n",
      "epoch no.1 train no.88970  loss = 3.95926 avg_loss = 3.69210\n",
      "epoch no.1 train no.88980  loss = 2.65478 avg_loss = 3.69707\n",
      "epoch no.1 train no.88990  loss = 3.18807 avg_loss = 3.70068\n",
      "epoch no.1 train no.89000  loss = 4.35768 avg_loss = 3.72216\n",
      "3\n",
      "to_tokens: ['▁비', '▁발라', '노래', '들', '</s>']\n",
      "추억의 그노래들</s>\n",
      "epoch no.1 train no.89010  loss = 2.30419 avg_loss = 3.68509\n",
      "epoch no.1 train no.89020  loss = 2.57965 avg_loss = 3.65452\n",
      "epoch no.1 train no.89030  loss = 3.89379 avg_loss = 3.67942\n",
      "epoch no.1 train no.89040  loss = 3.17602 avg_loss = 3.65427\n",
      "epoch no.1 train no.89050  loss = 4.25146 avg_loss = 3.68789\n",
      "epoch no.1 train no.89060  loss = 5.16169 avg_loss = 3.66832\n",
      "epoch no.1 train no.89070  loss = 2.46590 avg_loss = 3.61138\n",
      "epoch no.1 train no.89080  loss = 3.00354 avg_loss = 3.63437\n",
      "epoch no.1 train no.89090  loss = 3.72426 avg_loss = 3.64014\n",
      "epoch no.1 train no.89100  loss = 4.91893 avg_loss = 3.67185\n",
      "epoch no.1 train no.89110  loss = 4.51669 avg_loss = 3.69141\n",
      "epoch no.1 train no.89120  loss = 2.44828 avg_loss = 3.73493\n",
      "epoch no.1 train no.89130  loss = 2.93313 avg_loss = 3.70501\n",
      "epoch no.1 train no.89140  loss = 3.60015 avg_loss = 3.72353\n",
      "epoch no.1 train no.89150  loss = 6.67223 avg_loss = 3.73053\n",
      "epoch no.1 train no.89160  loss = 4.66511 avg_loss = 3.72729\n",
      "epoch no.1 train no.89170  loss = 2.18745 avg_loss = 3.70287\n",
      "epoch no.1 train no.89180  loss = 2.86930 avg_loss = 3.71192\n",
      "epoch no.1 train no.89190  loss = 3.37312 avg_loss = 3.66838\n",
      "epoch no.1 train no.89200  loss = 3.38693 avg_loss = 3.68266\n",
      "epoch no.1 train no.89210  loss = 4.78631 avg_loss = 3.72154\n",
      "epoch no.1 train no.89220  loss = 4.36757 avg_loss = 3.74058\n",
      "epoch no.1 train no.89230  loss = 5.25824 avg_loss = 3.76957\n",
      "epoch no.1 train no.89240  loss = 3.18065 avg_loss = 3.72971\n",
      "epoch no.1 train no.89250  loss = 2.75288 avg_loss = 3.69600\n",
      "epoch no.1 train no.89260  loss = 6.86700 avg_loss = 3.75979\n",
      "epoch no.1 train no.89270  loss = 3.10327 avg_loss = 3.77134\n",
      "epoch no.1 train no.89280  loss = 5.00484 avg_loss = 3.75534\n",
      "epoch no.1 train no.89290  loss = 2.97322 avg_loss = 3.69418\n",
      "epoch no.1 train no.89300  loss = 3.81303 avg_loss = 3.67597\n",
      "epoch no.1 train no.89310  loss = 2.29875 avg_loss = 3.67682\n",
      "epoch no.1 train no.89320  loss = 2.60365 avg_loss = 3.69529\n",
      "epoch no.1 train no.89330  loss = 4.95266 avg_loss = 3.68982\n",
      "epoch no.1 train no.89340  loss = 5.21732 avg_loss = 3.65341\n",
      "epoch no.1 train no.89350  loss = 4.58888 avg_loss = 3.70486\n",
      "epoch no.1 train no.89360  loss = 4.50743 avg_loss = 3.69814\n",
      "epoch no.1 train no.89370  loss = 2.88286 avg_loss = 3.63389\n",
      "epoch no.1 train no.89380  loss = 3.48773 avg_loss = 3.62682\n",
      "epoch no.1 train no.89390  loss = 3.22285 avg_loss = 3.63640\n",
      "epoch no.1 train no.89400  loss = 2.82971 avg_loss = 3.64901\n",
      "epoch no.1 train no.89410  loss = 5.66723 avg_loss = 3.72010\n",
      "epoch no.1 train no.89420  loss = 3.53160 avg_loss = 3.69878\n",
      "epoch no.1 train no.89430  loss = 3.23566 avg_loss = 3.71751\n",
      "epoch no.1 train no.89440  loss = 1.82984 avg_loss = 3.72733\n",
      "epoch no.1 train no.89450  loss = 4.90276 avg_loss = 3.75495\n",
      "epoch no.1 train no.89460  loss = 4.36255 avg_loss = 3.75691\n",
      "epoch no.1 train no.89470  loss = 4.69028 avg_loss = 3.74375\n",
      "epoch no.1 train no.89480  loss = 3.14684 avg_loss = 3.78215\n",
      "epoch no.1 train no.89490  loss = 3.65207 avg_loss = 3.74066\n",
      "epoch no.1 train no.89500  loss = 2.95889 avg_loss = 3.76793\n",
      "epoch no.1 train no.89510  loss = 2.20034 avg_loss = 3.73041\n",
      "epoch no.1 train no.89520  loss = 2.45586 avg_loss = 3.70361\n",
      "epoch no.1 train no.89530  loss = 3.22954 avg_loss = 3.65227\n",
      "epoch no.1 train no.89540  loss = 4.30681 avg_loss = 3.64429\n",
      "epoch no.1 train no.89550  loss = 4.53389 avg_loss = 3.64253\n",
      "epoch no.1 train no.89560  loss = 3.48883 avg_loss = 3.63642\n",
      "epoch no.1 train no.89570  loss = 3.70081 avg_loss = 3.64548\n",
      "epoch no.1 train no.89580  loss = 3.29472 avg_loss = 3.63837\n",
      "epoch no.1 train no.89590  loss = 3.09948 avg_loss = 3.66751\n",
      "epoch no.1 train no.89600  loss = 1.57649 avg_loss = 3.66115\n",
      "epoch no.1 train no.89610  loss = 4.20789 avg_loss = 3.63683\n",
      "epoch no.1 train no.89620  loss = 4.05168 avg_loss = 3.63853\n",
      "epoch no.1 train no.89630  loss = 3.09024 avg_loss = 3.67096\n",
      "epoch no.1 train no.89640  loss = 3.17050 avg_loss = 3.71481\n",
      "epoch no.1 train no.89650  loss = 2.60502 avg_loss = 3.68901\n",
      "epoch no.1 train no.89660  loss = 3.06742 avg_loss = 3.63300\n",
      "epoch no.1 train no.89670  loss = 5.73959 avg_loss = 3.65138\n",
      "epoch no.1 train no.89680  loss = 3.29804 avg_loss = 3.68452\n",
      "epoch no.1 train no.89690  loss = 4.03633 avg_loss = 3.68818\n",
      "epoch no.1 train no.89700  loss = 2.85039 avg_loss = 3.62151\n",
      "epoch no.1 train no.89710  loss = 3.54505 avg_loss = 3.66528\n",
      "epoch no.1 train no.89720  loss = 2.19520 avg_loss = 3.66669\n",
      "epoch no.1 train no.89730  loss = 5.53129 avg_loss = 3.68527\n",
      "epoch no.1 train no.89740  loss = 2.84789 avg_loss = 3.65377\n",
      "epoch no.1 train no.89750  loss = 4.18157 avg_loss = 3.67340\n",
      "epoch no.1 train no.89760  loss = 3.22361 avg_loss = 3.65084\n",
      "epoch no.1 train no.89770  loss = 2.42790 avg_loss = 3.60947\n",
      "epoch no.1 train no.89780  loss = 5.61059 avg_loss = 3.60235\n",
      "epoch no.1 train no.89790  loss = 3.53692 avg_loss = 3.60134\n",
      "epoch no.1 train no.89800  loss = 3.03839 avg_loss = 3.58059\n",
      "epoch no.1 train no.89810  loss = 3.67637 avg_loss = 3.65207\n",
      "epoch no.1 train no.89820  loss = 1.86490 avg_loss = 3.62046\n",
      "epoch no.1 train no.89830  loss = 3.68456 avg_loss = 3.59981\n",
      "epoch no.1 train no.89840  loss = 5.43106 avg_loss = 3.61163\n",
      "epoch no.1 train no.89850  loss = 2.42712 avg_loss = 3.59570\n",
      "epoch no.1 train no.89860  loss = 3.76036 avg_loss = 3.56671\n",
      "epoch no.1 train no.89870  loss = 3.35072 avg_loss = 3.55828\n",
      "epoch no.1 train no.89880  loss = 2.29179 avg_loss = 3.59297\n",
      "epoch no.1 train no.89890  loss = 2.13076 avg_loss = 3.56764\n",
      "epoch no.1 train no.89900  loss = 4.50346 avg_loss = 3.57077\n",
      "epoch no.1 train no.89910  loss = 3.91630 avg_loss = 3.51050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.89920  loss = 5.70295 avg_loss = 3.57553\n",
      "epoch no.1 train no.89930  loss = 3.57397 avg_loss = 3.55022\n",
      "epoch no.1 train no.89940  loss = 5.26414 avg_loss = 3.53607\n",
      "epoch no.1 train no.89950  loss = 3.35703 avg_loss = 3.50915\n",
      "epoch no.1 train no.89960  loss = 2.91104 avg_loss = 3.59615\n",
      "epoch no.1 train no.89970  loss = 3.99915 avg_loss = 3.66753\n",
      "epoch no.1 train no.89980  loss = 2.78932 avg_loss = 3.63640\n",
      "epoch no.1 train no.89990  loss = 4.24513 avg_loss = 3.64593\n",
      "epoch no.1 train no.90000  loss = 2.26150 avg_loss = 3.66299\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '팝', '▁명', '▁함께', '</s>']\n",
      "추억의 올드팝과 함께</s>\n",
      "epoch no.1 train no.90010  loss = 2.24583 avg_loss = 3.64444\n",
      "epoch no.1 train no.90020  loss = 2.98687 avg_loss = 3.60268\n",
      "epoch no.1 train no.90030  loss = 2.85853 avg_loss = 3.59931\n",
      "epoch no.1 train no.90040  loss = 2.72966 avg_loss = 3.57425\n",
      "epoch no.1 train no.90050  loss = 3.94113 avg_loss = 3.58025\n",
      "epoch no.1 train no.90060  loss = 2.31828 avg_loss = 3.60766\n",
      "epoch no.1 train no.90070  loss = 2.56081 avg_loss = 3.58740\n",
      "epoch no.1 train no.90080  loss = 3.60671 avg_loss = 3.60275\n",
      "epoch no.1 train no.90090  loss = 2.07146 avg_loss = 3.60580\n",
      "epoch no.1 train no.90100  loss = 3.21868 avg_loss = 3.62676\n",
      "epoch no.1 train no.90110  loss = 2.63854 avg_loss = 3.62820\n",
      "epoch no.1 train no.90120  loss = 2.47039 avg_loss = 3.64107\n",
      "epoch no.1 train no.90130  loss = 4.60963 avg_loss = 3.63230\n",
      "epoch no.1 train no.90140  loss = 4.26000 avg_loss = 3.70071\n",
      "epoch no.1 train no.90150  loss = 1.79830 avg_loss = 3.64056\n",
      "epoch no.1 train no.90160  loss = 3.82308 avg_loss = 3.67906\n",
      "epoch no.1 train no.90170  loss = 3.26036 avg_loss = 3.71961\n",
      "epoch no.1 train no.90180  loss = 5.17092 avg_loss = 3.71459\n",
      "epoch no.1 train no.90190  loss = 4.70122 avg_loss = 3.69147\n",
      "epoch no.1 train no.90200  loss = 3.87203 avg_loss = 3.69895\n",
      "epoch no.1 train no.90210  loss = 4.26583 avg_loss = 3.67977\n",
      "epoch no.1 train no.90220  loss = 4.62213 avg_loss = 3.73638\n",
      "epoch no.1 train no.90230  loss = 5.80523 avg_loss = 3.74741\n",
      "epoch no.1 train no.90240  loss = 3.34729 avg_loss = 3.72656\n",
      "epoch no.1 train no.90250  loss = 4.22652 avg_loss = 3.73439\n",
      "epoch no.1 train no.90260  loss = 4.55750 avg_loss = 3.75766\n",
      "epoch no.1 train no.90270  loss = 4.21083 avg_loss = 3.78464\n",
      "epoch no.1 train no.90280  loss = 5.72936 avg_loss = 3.81942\n",
      "epoch no.1 train no.90290  loss = 2.50480 avg_loss = 3.81401\n",
      "epoch no.1 train no.90300  loss = 4.71701 avg_loss = 3.78732\n",
      "epoch no.1 train no.90310  loss = 4.26281 avg_loss = 3.72206\n",
      "epoch no.1 train no.90320  loss = 2.05223 avg_loss = 3.68887\n",
      "epoch no.1 train no.90330  loss = 3.09110 avg_loss = 3.64927\n",
      "epoch no.1 train no.90340  loss = 4.04693 avg_loss = 3.64246\n",
      "epoch no.1 train no.90350  loss = 3.42004 avg_loss = 3.64385\n",
      "epoch no.1 train no.90360  loss = 5.26150 avg_loss = 3.63663\n",
      "epoch no.1 train no.90370  loss = 3.07843 avg_loss = 3.67687\n",
      "epoch no.1 train no.90380  loss = 3.04017 avg_loss = 3.69033\n",
      "epoch no.1 train no.90390  loss = 4.43528 avg_loss = 3.66784\n",
      "epoch no.1 train no.90400  loss = 3.13066 avg_loss = 3.70463\n",
      "epoch no.1 train no.90410  loss = 2.73199 avg_loss = 3.66154\n",
      "epoch no.1 train no.90420  loss = 2.96244 avg_loss = 3.64241\n",
      "epoch no.1 train no.90430  loss = 3.62083 avg_loss = 3.72688\n",
      "epoch no.1 train no.90440  loss = 7.18067 avg_loss = 3.74924\n",
      "epoch no.1 train no.90450  loss = 4.56730 avg_loss = 3.79370\n",
      "epoch no.1 train no.90460  loss = 6.79908 avg_loss = 3.82790\n",
      "epoch no.1 train no.90470  loss = 3.51499 avg_loss = 3.78345\n",
      "epoch no.1 train no.90480  loss = 2.17103 avg_loss = 3.79232\n",
      "epoch no.1 train no.90490  loss = 2.69591 avg_loss = 3.77600\n",
      "epoch no.1 train no.90500  loss = 4.48132 avg_loss = 3.73080\n",
      "epoch no.1 train no.90510  loss = 5.39520 avg_loss = 3.78106\n",
      "epoch no.1 train no.90520  loss = 4.13328 avg_loss = 3.79620\n",
      "epoch no.1 train no.90530  loss = 3.37974 avg_loss = 3.79204\n",
      "epoch no.1 train no.90540  loss = 2.75761 avg_loss = 3.75244\n",
      "epoch no.1 train no.90550  loss = 3.26284 avg_loss = 3.76787\n",
      "epoch no.1 train no.90560  loss = 4.45296 avg_loss = 3.77737\n",
      "epoch no.1 train no.90570  loss = 2.49339 avg_loss = 3.78324\n",
      "epoch no.1 train no.90580  loss = 2.91404 avg_loss = 3.81498\n",
      "epoch no.1 train no.90590  loss = 3.34300 avg_loss = 3.78926\n",
      "epoch no.1 train no.90600  loss = 3.09233 avg_loss = 3.74839\n",
      "epoch no.1 train no.90610  loss = 1.91748 avg_loss = 3.66450\n",
      "epoch no.1 train no.90620  loss = 4.41343 avg_loss = 3.63733\n",
      "epoch no.1 train no.90630  loss = 2.80437 avg_loss = 3.63889\n",
      "epoch no.1 train no.90640  loss = 2.60060 avg_loss = 3.65057\n",
      "epoch no.1 train no.90650  loss = 4.28451 avg_loss = 3.68229\n",
      "epoch no.1 train no.90660  loss = 5.71725 avg_loss = 3.65034\n",
      "epoch no.1 train no.90670  loss = 3.06335 avg_loss = 3.62698\n",
      "epoch no.1 train no.90680  loss = 5.12514 avg_loss = 3.67603\n",
      "epoch no.1 train no.90690  loss = 2.10694 avg_loss = 3.69767\n",
      "epoch no.1 train no.90700  loss = 4.25691 avg_loss = 3.70835\n",
      "epoch no.1 train no.90710  loss = 3.15681 avg_loss = 3.74606\n",
      "epoch no.1 train no.90720  loss = 3.27905 avg_loss = 3.73882\n",
      "epoch no.1 train no.90730  loss = 2.36360 avg_loss = 3.79785\n",
      "epoch no.1 train no.90740  loss = 6.43823 avg_loss = 3.79473\n",
      "epoch no.1 train no.90750  loss = 4.60252 avg_loss = 3.83689\n",
      "epoch no.1 train no.90760  loss = 2.10816 avg_loss = 3.76753\n",
      "epoch no.1 train no.90770  loss = 4.45062 avg_loss = 3.76645\n",
      "epoch no.1 train no.90780  loss = 3.29276 avg_loss = 3.76743\n",
      "epoch no.1 train no.90790  loss = 2.83582 avg_loss = 3.74728\n",
      "epoch no.1 train no.90800  loss = 4.24228 avg_loss = 3.75905\n",
      "epoch no.1 train no.90810  loss = 3.04187 avg_loss = 3.78753\n",
      "epoch no.1 train no.90820  loss = 6.59431 avg_loss = 3.83423\n",
      "epoch no.1 train no.90830  loss = 2.43381 avg_loss = 3.80879\n",
      "epoch no.1 train no.90840  loss = 3.34525 avg_loss = 3.79210\n",
      "epoch no.1 train no.90850  loss = 3.12064 avg_loss = 3.72736\n",
      "epoch no.1 train no.90860  loss = 5.59475 avg_loss = 3.77563\n",
      "epoch no.1 train no.90870  loss = 2.95723 avg_loss = 3.72379\n",
      "epoch no.1 train no.90880  loss = 3.02607 avg_loss = 3.70956\n",
      "epoch no.1 train no.90890  loss = 4.53589 avg_loss = 3.73685\n",
      "epoch no.1 train no.90900  loss = 3.35256 avg_loss = 3.71447\n",
      "epoch no.1 train no.90910  loss = 4.56162 avg_loss = 3.70418\n",
      "epoch no.1 train no.90920  loss = 5.61197 avg_loss = 3.63611\n",
      "epoch no.1 train no.90930  loss = 3.93765 avg_loss = 3.63502\n",
      "epoch no.1 train no.90940  loss = 2.91942 avg_loss = 3.64523\n",
      "epoch no.1 train no.90950  loss = 5.62872 avg_loss = 3.68627\n",
      "epoch no.1 train no.90960  loss = 2.86811 avg_loss = 3.65136\n",
      "epoch no.1 train no.90970  loss = 1.74116 avg_loss = 3.62677\n",
      "epoch no.1 train no.90980  loss = 2.80654 avg_loss = 3.57112\n",
      "epoch no.1 train no.90990  loss = 5.10787 avg_loss = 3.55135\n",
      "epoch no.1 train no.91000  loss = 2.73867 avg_loss = 3.47812\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '곡', '▁모음', 'est', '</s>']\n",
      "추억의 댄스곡 best</s>\n",
      "epoch no.1 train no.91010  loss = 4.01790 avg_loss = 3.51948\n",
      "epoch no.1 train no.91020  loss = 6.14362 avg_loss = 3.53212\n",
      "epoch no.1 train no.91030  loss = 3.54408 avg_loss = 3.62516\n",
      "epoch no.1 train no.91040  loss = 4.81676 avg_loss = 3.64044\n",
      "epoch no.1 train no.91050  loss = 3.70478 avg_loss = 3.63372\n",
      "epoch no.1 train no.91060  loss = 3.60934 avg_loss = 3.60626\n",
      "epoch no.1 train no.91070  loss = 4.16195 avg_loss = 3.64893\n",
      "epoch no.1 train no.91080  loss = 1.73458 avg_loss = 3.70726\n",
      "epoch no.1 train no.91090  loss = 3.68460 avg_loss = 3.66536\n",
      "epoch no.1 train no.91100  loss = 2.20735 avg_loss = 3.63356\n",
      "epoch no.1 train no.91110  loss = 4.55906 avg_loss = 3.63312\n",
      "epoch no.1 train no.91120  loss = 5.64823 avg_loss = 3.63385\n",
      "epoch no.1 train no.91130  loss = 4.28491 avg_loss = 3.64243\n",
      "epoch no.1 train no.91140  loss = 4.51206 avg_loss = 3.69786\n",
      "epoch no.1 train no.91150  loss = 2.08000 avg_loss = 3.68175\n",
      "epoch no.1 train no.91160  loss = 4.14059 avg_loss = 3.68253\n",
      "epoch no.1 train no.91170  loss = 2.94776 avg_loss = 3.69319\n",
      "epoch no.1 train no.91180  loss = 3.27366 avg_loss = 3.67663\n",
      "epoch no.1 train no.91190  loss = 5.67729 avg_loss = 3.64191\n",
      "epoch no.1 train no.91200  loss = 3.10302 avg_loss = 3.65908\n",
      "epoch no.1 train no.91210  loss = 3.36883 avg_loss = 3.65472\n",
      "epoch no.1 train no.91220  loss = 5.28824 avg_loss = 3.67294\n",
      "epoch no.1 train no.91230  loss = 2.40444 avg_loss = 3.66961\n",
      "epoch no.1 train no.91240  loss = 3.94949 avg_loss = 3.69539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.91250  loss = 5.32357 avg_loss = 3.72365\n",
      "epoch no.1 train no.91260  loss = 3.60165 avg_loss = 3.68303\n",
      "epoch no.1 train no.91270  loss = 3.57195 avg_loss = 3.66020\n",
      "epoch no.1 train no.91280  loss = 3.97386 avg_loss = 3.60837\n",
      "epoch no.1 train no.91290  loss = 2.44235 avg_loss = 3.57051\n",
      "epoch no.1 train no.91300  loss = 4.14830 avg_loss = 3.61321\n",
      "epoch no.1 train no.91310  loss = 3.43693 avg_loss = 3.61281\n",
      "epoch no.1 train no.91320  loss = 2.47185 avg_loss = 3.57788\n",
      "epoch no.1 train no.91330  loss = 4.92455 avg_loss = 3.59430\n",
      "epoch no.1 train no.91340  loss = 3.57189 avg_loss = 3.57069\n",
      "epoch no.1 train no.91350  loss = 4.06429 avg_loss = 3.62092\n",
      "epoch no.1 train no.91360  loss = 2.87877 avg_loss = 3.59197\n",
      "epoch no.1 train no.91370  loss = 2.46591 avg_loss = 3.57870\n",
      "epoch no.1 train no.91380  loss = 2.79874 avg_loss = 3.56181\n",
      "epoch no.1 train no.91390  loss = 3.98262 avg_loss = 3.60740\n",
      "epoch no.1 train no.91400  loss = 2.36246 avg_loss = 3.56234\n",
      "epoch no.1 train no.91410  loss = 4.86184 avg_loss = 3.55754\n",
      "epoch no.1 train no.91420  loss = 5.27623 avg_loss = 3.61827\n",
      "epoch no.1 train no.91430  loss = 3.61169 avg_loss = 3.65955\n",
      "epoch no.1 train no.91440  loss = 2.22901 avg_loss = 3.68468\n",
      "epoch no.1 train no.91450  loss = 2.33022 avg_loss = 3.67682\n",
      "epoch no.1 train no.91460  loss = 5.28393 avg_loss = 3.64368\n",
      "epoch no.1 train no.91470  loss = 3.15345 avg_loss = 3.62340\n",
      "epoch no.1 train no.91480  loss = 5.03660 avg_loss = 3.63338\n",
      "epoch no.1 train no.91490  loss = 2.75635 avg_loss = 3.64182\n",
      "epoch no.1 train no.91500  loss = 3.13522 avg_loss = 3.64032\n",
      "epoch no.1 train no.91510  loss = 3.01492 avg_loss = 3.60311\n",
      "epoch no.1 train no.91520  loss = 4.80130 avg_loss = 3.62748\n",
      "epoch no.1 train no.91530  loss = 6.44332 avg_loss = 3.63512\n",
      "epoch no.1 train no.91540  loss = 2.88790 avg_loss = 3.63810\n",
      "epoch no.1 train no.91550  loss = 4.87776 avg_loss = 3.69918\n",
      "epoch no.1 train no.91560  loss = 2.07082 avg_loss = 3.68589\n",
      "epoch no.1 train no.91570  loss = 2.38603 avg_loss = 3.68128\n",
      "epoch no.1 train no.91580  loss = 4.00489 avg_loss = 3.67100\n",
      "epoch no.1 train no.91590  loss = 4.52042 avg_loss = 3.68583\n",
      "epoch no.1 train no.91600  loss = 3.29239 avg_loss = 3.68901\n",
      "epoch no.1 train no.91610  loss = 2.91860 avg_loss = 3.71101\n",
      "epoch no.1 train no.91620  loss = 2.08991 avg_loss = 3.65077\n",
      "epoch no.1 train no.91630  loss = 5.00728 avg_loss = 3.68062\n",
      "epoch no.1 train no.91640  loss = 6.20540 avg_loss = 3.72384\n",
      "epoch no.1 train no.91650  loss = 2.52933 avg_loss = 3.69012\n",
      "epoch no.1 train no.91660  loss = 2.86964 avg_loss = 3.66783\n",
      "epoch no.1 train no.91670  loss = 3.64742 avg_loss = 3.67586\n",
      "epoch no.1 train no.91680  loss = 4.64687 avg_loss = 3.66695\n",
      "epoch no.1 train no.91690  loss = 3.93042 avg_loss = 3.66171\n",
      "epoch no.1 train no.91700  loss = 3.11835 avg_loss = 3.65938\n",
      "epoch no.1 train no.91710  loss = 2.92272 avg_loss = 3.62253\n",
      "epoch no.1 train no.91720  loss = 2.03241 avg_loss = 3.62817\n",
      "epoch no.1 train no.91730  loss = 2.37280 avg_loss = 3.60862\n",
      "epoch no.1 train no.91740  loss = 3.35176 avg_loss = 3.56245\n",
      "epoch no.1 train no.91750  loss = 2.78332 avg_loss = 3.54912\n",
      "epoch no.1 train no.91760  loss = 2.80458 avg_loss = 3.56880\n",
      "epoch no.1 train no.91770  loss = 4.78366 avg_loss = 3.59035\n",
      "epoch no.1 train no.91780  loss = 4.09838 avg_loss = 3.61431\n",
      "epoch no.1 train no.91790  loss = 2.84807 avg_loss = 3.57104\n",
      "epoch no.1 train no.91800  loss = 3.83520 avg_loss = 3.57575\n",
      "epoch no.1 train no.91810  loss = 1.86517 avg_loss = 3.61681\n",
      "epoch no.1 train no.91820  loss = 3.93086 avg_loss = 3.57887\n",
      "epoch no.1 train no.91830  loss = 3.12684 avg_loss = 3.56936\n",
      "epoch no.1 train no.91840  loss = 6.20377 avg_loss = 3.62100\n",
      "epoch no.1 train no.91850  loss = 4.30395 avg_loss = 3.59985\n",
      "epoch no.1 train no.91860  loss = 3.37007 avg_loss = 3.59774\n",
      "epoch no.1 train no.91870  loss = 3.33699 avg_loss = 3.58203\n",
      "epoch no.1 train no.91880  loss = 4.62567 avg_loss = 3.62468\n",
      "epoch no.1 train no.91890  loss = 5.94800 avg_loss = 3.63225\n",
      "epoch no.1 train no.91900  loss = 3.97846 avg_loss = 3.54339\n",
      "epoch no.1 train no.91910  loss = 3.05178 avg_loss = 3.54568\n",
      "epoch no.1 train no.91920  loss = 3.62693 avg_loss = 3.57956\n",
      "epoch no.1 train no.91930  loss = 3.38866 avg_loss = 3.58767\n",
      "epoch no.1 train no.91940  loss = 3.65010 avg_loss = 3.60663\n",
      "epoch no.1 train no.91950  loss = 2.71364 avg_loss = 3.63181\n",
      "epoch no.1 train no.91960  loss = 2.39352 avg_loss = 3.62177\n",
      "epoch no.1 train no.91970  loss = 4.38847 avg_loss = 3.62606\n",
      "epoch no.1 train no.91980  loss = 4.25288 avg_loss = 3.67765\n",
      "epoch no.1 train no.91990  loss = 2.13746 avg_loss = 3.66962\n",
      "epoch no.1 train no.92000  loss = 3.09902 avg_loss = 3.70200\n",
      "5\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '곡', '</s>', '</s>']\n",
      "추억의 발라드 명곡들</s>\n",
      "epoch no.1 train no.92010  loss = 3.01841 avg_loss = 3.64596\n",
      "epoch no.1 train no.92020  loss = 3.76705 avg_loss = 3.59287\n",
      "epoch no.1 train no.92030  loss = 2.20147 avg_loss = 3.53868\n",
      "epoch no.1 train no.92040  loss = 2.99950 avg_loss = 3.57716\n",
      "epoch no.1 train no.92050  loss = 3.39105 avg_loss = 3.56564\n",
      "epoch no.1 train no.92060  loss = 4.39721 avg_loss = 3.58310\n",
      "epoch no.1 train no.92070  loss = 3.21722 avg_loss = 3.51560\n",
      "epoch no.1 train no.92080  loss = 2.53481 avg_loss = 3.51971\n",
      "epoch no.1 train no.92090  loss = 4.34552 avg_loss = 3.53721\n",
      "epoch no.1 train no.92100  loss = 2.03633 avg_loss = 3.56915\n",
      "epoch no.1 train no.92110  loss = 3.31116 avg_loss = 3.60147\n",
      "epoch no.1 train no.92120  loss = 2.76257 avg_loss = 3.58458\n",
      "epoch no.1 train no.92130  loss = 3.84692 avg_loss = 3.59223\n",
      "epoch no.1 train no.92140  loss = 3.52261 avg_loss = 3.62538\n",
      "epoch no.1 train no.92150  loss = 5.56559 avg_loss = 3.60970\n",
      "epoch no.1 train no.92160  loss = 3.75373 avg_loss = 3.65791\n",
      "epoch no.1 train no.92170  loss = 3.19147 avg_loss = 3.61559\n",
      "epoch no.1 train no.92180  loss = 2.87700 avg_loss = 3.56588\n",
      "epoch no.1 train no.92190  loss = 2.28751 avg_loss = 3.57017\n",
      "epoch no.1 train no.92200  loss = 5.51747 avg_loss = 3.56775\n",
      "epoch no.1 train no.92210  loss = 3.30978 avg_loss = 3.55087\n",
      "epoch no.1 train no.92220  loss = 4.10696 avg_loss = 3.55968\n",
      "epoch no.1 train no.92230  loss = 2.73172 avg_loss = 3.60322\n",
      "epoch no.1 train no.92240  loss = 3.84823 avg_loss = 3.57623\n",
      "epoch no.1 train no.92250  loss = 5.76782 avg_loss = 3.65986\n",
      "epoch no.1 train no.92260  loss = 2.50377 avg_loss = 3.62572\n",
      "epoch no.1 train no.92270  loss = 5.00008 avg_loss = 3.66731\n",
      "epoch no.1 train no.92280  loss = 4.37945 avg_loss = 3.62839\n",
      "epoch no.1 train no.92290  loss = 4.46795 avg_loss = 3.63945\n",
      "epoch no.1 train no.92300  loss = 4.70578 avg_loss = 3.62123\n",
      "epoch no.1 train no.92310  loss = 2.92586 avg_loss = 3.64627\n",
      "epoch no.1 train no.92320  loss = 2.84635 avg_loss = 3.72747\n",
      "epoch no.1 train no.92330  loss = 5.70398 avg_loss = 3.76803\n",
      "epoch no.1 train no.92340  loss = 3.43699 avg_loss = 3.75234\n",
      "epoch no.1 train no.92350  loss = 3.59372 avg_loss = 3.80087\n",
      "epoch no.1 train no.92360  loss = 2.52213 avg_loss = 3.77544\n",
      "epoch no.1 train no.92370  loss = 2.79999 avg_loss = 3.78200\n",
      "epoch no.1 train no.92380  loss = 3.23664 avg_loss = 3.75247\n",
      "epoch no.1 train no.92390  loss = 3.11872 avg_loss = 3.75762\n",
      "epoch no.1 train no.92400  loss = 2.89422 avg_loss = 3.71767\n",
      "epoch no.1 train no.92410  loss = 3.18411 avg_loss = 3.72647\n",
      "epoch no.1 train no.92420  loss = 4.68728 avg_loss = 3.74147\n",
      "epoch no.1 train no.92430  loss = 3.69196 avg_loss = 3.74958\n",
      "epoch no.1 train no.92440  loss = 4.27671 avg_loss = 3.74326\n",
      "epoch no.1 train no.92450  loss = 2.89803 avg_loss = 3.80483\n",
      "epoch no.1 train no.92460  loss = 4.98679 avg_loss = 3.77400\n",
      "epoch no.1 train no.92470  loss = 2.19765 avg_loss = 3.70743\n",
      "epoch no.1 train no.92480  loss = 3.98356 avg_loss = 3.71544\n",
      "epoch no.1 train no.92490  loss = 2.19117 avg_loss = 3.67237\n",
      "epoch no.1 train no.92500  loss = 2.47414 avg_loss = 3.73742\n",
      "epoch no.1 train no.92510  loss = 4.30276 avg_loss = 3.74506\n",
      "epoch no.1 train no.92520  loss = 3.19943 avg_loss = 3.73320\n",
      "epoch no.1 train no.92530  loss = 2.52703 avg_loss = 3.80116\n",
      "epoch no.1 train no.92540  loss = 1.86921 avg_loss = 3.75131\n",
      "epoch no.1 train no.92550  loss = 3.69520 avg_loss = 3.73691\n",
      "epoch no.1 train no.92560  loss = 2.68246 avg_loss = 3.68144\n",
      "epoch no.1 train no.92570  loss = 2.83222 avg_loss = 3.70496\n",
      "epoch no.1 train no.92580  loss = 3.23479 avg_loss = 3.66628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.92590  loss = 4.14135 avg_loss = 3.71038\n",
      "epoch no.1 train no.92600  loss = 2.50817 avg_loss = 3.72896\n",
      "epoch no.1 train no.92610  loss = 2.05384 avg_loss = 3.67512\n",
      "epoch no.1 train no.92620  loss = 2.38356 avg_loss = 3.66944\n",
      "epoch no.1 train no.92630  loss = 3.59925 avg_loss = 3.68637\n",
      "epoch no.1 train no.92640  loss = 2.75105 avg_loss = 3.66730\n",
      "epoch no.1 train no.92650  loss = 4.72702 avg_loss = 3.64206\n",
      "epoch no.1 train no.92660  loss = 2.70028 avg_loss = 3.60904\n",
      "epoch no.1 train no.92670  loss = 2.87800 avg_loss = 3.58172\n",
      "epoch no.1 train no.92680  loss = 3.47564 avg_loss = 3.54154\n",
      "epoch no.1 train no.92690  loss = 2.69352 avg_loss = 3.52939\n",
      "epoch no.1 train no.92700  loss = 4.11633 avg_loss = 3.59474\n",
      "epoch no.1 train no.92710  loss = 3.58547 avg_loss = 3.60413\n",
      "epoch no.1 train no.92720  loss = 3.98075 avg_loss = 3.67621\n",
      "epoch no.1 train no.92730  loss = 3.76473 avg_loss = 3.73825\n",
      "epoch no.1 train no.92740  loss = 3.15245 avg_loss = 3.73215\n",
      "epoch no.1 train no.92750  loss = 3.81294 avg_loss = 3.70786\n",
      "epoch no.1 train no.92760  loss = 2.70815 avg_loss = 3.66055\n",
      "epoch no.1 train no.92770  loss = 2.17252 avg_loss = 3.60614\n",
      "epoch no.1 train no.92780  loss = 3.55768 avg_loss = 3.55941\n",
      "epoch no.1 train no.92790  loss = 4.11148 avg_loss = 3.57441\n",
      "epoch no.1 train no.92800  loss = 3.30795 avg_loss = 3.58635\n",
      "epoch no.1 train no.92810  loss = 2.90449 avg_loss = 3.56618\n",
      "epoch no.1 train no.92820  loss = 4.07158 avg_loss = 3.57858\n",
      "epoch no.1 train no.92830  loss = 4.88649 avg_loss = 3.59392\n",
      "epoch no.1 train no.92840  loss = 5.54432 avg_loss = 3.60668\n",
      "epoch no.1 train no.92850  loss = 1.50648 avg_loss = 3.63859\n",
      "epoch no.1 train no.92860  loss = 4.40462 avg_loss = 3.62216\n",
      "epoch no.1 train no.92870  loss = 2.86782 avg_loss = 3.57851\n",
      "epoch no.1 train no.92880  loss = 3.62087 avg_loss = 3.60147\n",
      "epoch no.1 train no.92890  loss = 3.11626 avg_loss = 3.50095\n",
      "epoch no.1 train no.92900  loss = 4.92234 avg_loss = 3.55737\n",
      "epoch no.1 train no.92910  loss = 5.59974 avg_loss = 3.56235\n",
      "epoch no.1 train no.92920  loss = 4.31328 avg_loss = 3.55893\n",
      "epoch no.1 train no.92930  loss = 4.99229 avg_loss = 3.62322\n",
      "epoch no.1 train no.92940  loss = 2.03518 avg_loss = 3.62813\n",
      "epoch no.1 train no.92950  loss = 2.54978 avg_loss = 3.62668\n",
      "epoch no.1 train no.92960  loss = 4.08513 avg_loss = 3.65421\n",
      "epoch no.1 train no.92970  loss = 4.84033 avg_loss = 3.67661\n",
      "epoch no.1 train no.92980  loss = 3.82729 avg_loss = 3.73606\n",
      "epoch no.1 train no.92990  loss = 1.90756 avg_loss = 3.70487\n",
      "epoch no.1 train no.93000  loss = 2.15417 avg_loss = 3.69955\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '집', '</s>']\n",
      "추억의 발라드 모음집</s>\n",
      "epoch no.1 train no.93010  loss = 3.47087 avg_loss = 3.72542\n",
      "epoch no.1 train no.93020  loss = 3.73354 avg_loss = 3.77581\n",
      "epoch no.1 train no.93030  loss = 1.82664 avg_loss = 3.73106\n",
      "epoch no.1 train no.93040  loss = 3.27512 avg_loss = 3.77680\n",
      "epoch no.1 train no.93050  loss = 5.99302 avg_loss = 3.79232\n",
      "epoch no.1 train no.93060  loss = 2.14128 avg_loss = 3.69265\n",
      "epoch no.1 train no.93070  loss = 2.20101 avg_loss = 3.66179\n",
      "epoch no.1 train no.93080  loss = 3.00264 avg_loss = 3.64785\n",
      "epoch no.1 train no.93090  loss = 3.11854 avg_loss = 3.61174\n",
      "epoch no.1 train no.93100  loss = 2.49036 avg_loss = 3.60330\n",
      "epoch no.1 train no.93110  loss = 3.73649 avg_loss = 3.60054\n",
      "epoch no.1 train no.93120  loss = 7.07971 avg_loss = 3.58710\n",
      "epoch no.1 train no.93130  loss = 4.20101 avg_loss = 3.64890\n",
      "epoch no.1 train no.93140  loss = 4.39337 avg_loss = 3.67744\n",
      "epoch no.1 train no.93150  loss = 4.09268 avg_loss = 3.66451\n",
      "epoch no.1 train no.93160  loss = 3.19051 avg_loss = 3.70113\n",
      "epoch no.1 train no.93170  loss = 2.07675 avg_loss = 3.65481\n",
      "epoch no.1 train no.93180  loss = 5.65310 avg_loss = 3.71342\n",
      "epoch no.1 train no.93190  loss = 2.95946 avg_loss = 3.62278\n",
      "epoch no.1 train no.93200  loss = 5.97351 avg_loss = 3.66894\n",
      "epoch no.1 train no.93210  loss = 4.67950 avg_loss = 3.64281\n",
      "epoch no.1 train no.93220  loss = 4.74880 avg_loss = 3.65670\n",
      "epoch no.1 train no.93230  loss = 3.51000 avg_loss = 3.60435\n",
      "epoch no.1 train no.93240  loss = 2.66051 avg_loss = 3.62063\n",
      "epoch no.1 train no.93250  loss = 4.31931 avg_loss = 3.60807\n",
      "epoch no.1 train no.93260  loss = 1.81883 avg_loss = 3.58146\n",
      "epoch no.1 train no.93270  loss = 2.97683 avg_loss = 3.55285\n",
      "epoch no.1 train no.93280  loss = 2.46671 avg_loss = 3.51280\n",
      "epoch no.1 train no.93290  loss = 4.26473 avg_loss = 3.57291\n",
      "epoch no.1 train no.93300  loss = 4.28150 avg_loss = 3.61915\n",
      "epoch no.1 train no.93310  loss = 2.74295 avg_loss = 3.65944\n",
      "epoch no.1 train no.93320  loss = 2.37526 avg_loss = 3.67196\n",
      "epoch no.1 train no.93330  loss = 4.86279 avg_loss = 3.64858\n",
      "epoch no.1 train no.93340  loss = 2.66345 avg_loss = 3.60055\n",
      "epoch no.1 train no.93350  loss = 4.49240 avg_loss = 3.66899\n",
      "epoch no.1 train no.93360  loss = 4.94421 avg_loss = 3.70107\n",
      "epoch no.1 train no.93370  loss = 6.15144 avg_loss = 3.71263\n",
      "epoch no.1 train no.93380  loss = 5.86742 avg_loss = 3.75354\n",
      "epoch no.1 train no.93390  loss = 3.02794 avg_loss = 3.71252\n",
      "epoch no.1 train no.93400  loss = 2.38368 avg_loss = 3.70131\n",
      "epoch no.1 train no.93410  loss = 4.48154 avg_loss = 3.72620\n",
      "epoch no.1 train no.93420  loss = 5.51411 avg_loss = 3.75928\n",
      "epoch no.1 train no.93430  loss = 2.52388 avg_loss = 3.78320\n",
      "epoch no.1 train no.93440  loss = 4.38909 avg_loss = 3.85356\n",
      "epoch no.1 train no.93450  loss = 3.58492 avg_loss = 3.83030\n",
      "epoch no.1 train no.93460  loss = 3.92317 avg_loss = 3.77699\n",
      "epoch no.1 train no.93470  loss = 4.47861 avg_loss = 3.77528\n",
      "epoch no.1 train no.93480  loss = 3.41675 avg_loss = 3.73076\n",
      "epoch no.1 train no.93490  loss = 4.92493 avg_loss = 3.73026\n",
      "epoch no.1 train no.93500  loss = 3.68152 avg_loss = 3.75796\n",
      "epoch no.1 train no.93510  loss = 4.19486 avg_loss = 3.80432\n",
      "epoch no.1 train no.93520  loss = 4.79428 avg_loss = 3.73487\n",
      "epoch no.1 train no.93530  loss = 4.87786 avg_loss = 3.80792\n",
      "epoch no.1 train no.93540  loss = 2.33005 avg_loss = 3.79311\n",
      "epoch no.1 train no.93550  loss = 4.62851 avg_loss = 3.83338\n",
      "epoch no.1 train no.93560  loss = 3.96138 avg_loss = 3.79692\n",
      "epoch no.1 train no.93570  loss = 4.62337 avg_loss = 3.85036\n",
      "epoch no.1 train no.93580  loss = 3.11138 avg_loss = 3.87601\n",
      "epoch no.1 train no.93590  loss = 3.11380 avg_loss = 3.86858\n",
      "epoch no.1 train no.93600  loss = 2.97054 avg_loss = 3.82190\n",
      "epoch no.1 train no.93610  loss = 2.32531 avg_loss = 3.76740\n",
      "epoch no.1 train no.93620  loss = 2.55698 avg_loss = 3.70775\n",
      "epoch no.1 train no.93630  loss = 3.00427 avg_loss = 3.67295\n",
      "epoch no.1 train no.93640  loss = 3.19004 avg_loss = 3.61755\n",
      "epoch no.1 train no.93650  loss = 4.24759 avg_loss = 3.58447\n",
      "epoch no.1 train no.93660  loss = 3.12091 avg_loss = 3.57337\n",
      "epoch no.1 train no.93670  loss = 3.20479 avg_loss = 3.54150\n",
      "epoch no.1 train no.93680  loss = 4.07742 avg_loss = 3.55934\n",
      "epoch no.1 train no.93690  loss = 3.24520 avg_loss = 3.56831\n",
      "epoch no.1 train no.93700  loss = 3.15653 avg_loss = 3.57605\n",
      "epoch no.1 train no.93710  loss = 3.97265 avg_loss = 3.52138\n",
      "epoch no.1 train no.93720  loss = 3.76240 avg_loss = 3.54410\n",
      "epoch no.1 train no.93730  loss = 3.25159 avg_loss = 3.55163\n",
      "epoch no.1 train no.93740  loss = 3.16437 avg_loss = 3.58244\n",
      "epoch no.1 train no.93750  loss = 3.55473 avg_loss = 3.62724\n",
      "epoch no.1 train no.93760  loss = 3.17884 avg_loss = 3.61623\n",
      "epoch no.1 train no.93770  loss = 3.51886 avg_loss = 3.66912\n",
      "epoch no.1 train no.93780  loss = 2.43107 avg_loss = 3.67449\n",
      "epoch no.1 train no.93790  loss = 3.82478 avg_loss = 3.67545\n",
      "epoch no.1 train no.93800  loss = 3.54403 avg_loss = 3.64368\n",
      "epoch no.1 train no.93810  loss = 2.64848 avg_loss = 3.63595\n",
      "epoch no.1 train no.93820  loss = 2.73390 avg_loss = 3.61693\n",
      "epoch no.1 train no.93830  loss = 3.64910 avg_loss = 3.63429\n",
      "epoch no.1 train no.93840  loss = 3.03147 avg_loss = 3.59259\n",
      "epoch no.1 train no.93850  loss = 3.78912 avg_loss = 3.59647\n",
      "epoch no.1 train no.93860  loss = 3.60624 avg_loss = 3.59065\n",
      "epoch no.1 train no.93870  loss = 1.58768 avg_loss = 3.56243\n",
      "epoch no.1 train no.93880  loss = 4.30872 avg_loss = 3.60616\n",
      "epoch no.1 train no.93890  loss = 4.65294 avg_loss = 3.59680\n",
      "epoch no.1 train no.93900  loss = 4.93098 avg_loss = 3.65472\n",
      "epoch no.1 train no.93910  loss = 3.69971 avg_loss = 3.64879\n",
      "epoch no.1 train no.93920  loss = 4.47443 avg_loss = 3.69945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.93930  loss = 4.59934 avg_loss = 3.73251\n",
      "epoch no.1 train no.93940  loss = 2.96089 avg_loss = 3.66800\n",
      "epoch no.1 train no.93950  loss = 2.86876 avg_loss = 3.64374\n",
      "epoch no.1 train no.93960  loss = 4.71643 avg_loss = 3.68514\n",
      "epoch no.1 train no.93970  loss = 5.45428 avg_loss = 3.73861\n",
      "epoch no.1 train no.93980  loss = 3.30089 avg_loss = 3.73584\n",
      "epoch no.1 train no.93990  loss = 2.58846 avg_loss = 3.73169\n",
      "epoch no.1 train no.94000  loss = 2.56499 avg_loss = 3.72784\n",
      "4\n",
      "to_tokens: ['▁가을', '▁노래', '드', '▁모음', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.1 train no.94010  loss = 3.88059 avg_loss = 3.69906\n",
      "epoch no.1 train no.94020  loss = 3.51675 avg_loss = 3.66931\n",
      "epoch no.1 train no.94030  loss = 4.68987 avg_loss = 3.68571\n",
      "epoch no.1 train no.94040  loss = 3.58197 avg_loss = 3.70216\n",
      "epoch no.1 train no.94050  loss = 3.01813 avg_loss = 3.76903\n",
      "epoch no.1 train no.94060  loss = 4.92211 avg_loss = 3.76206\n",
      "epoch no.1 train no.94070  loss = 3.34669 avg_loss = 3.74004\n",
      "epoch no.1 train no.94080  loss = 3.44071 avg_loss = 3.77172\n",
      "epoch no.1 train no.94090  loss = 3.35092 avg_loss = 3.79175\n",
      "epoch no.1 train no.94100  loss = 5.32105 avg_loss = 3.81389\n",
      "epoch no.1 train no.94110  loss = 2.60032 avg_loss = 3.77522\n",
      "epoch no.1 train no.94120  loss = 3.29731 avg_loss = 3.73155\n",
      "epoch no.1 train no.94130  loss = 4.39808 avg_loss = 3.75065\n",
      "epoch no.1 train no.94140  loss = 3.15826 avg_loss = 3.72422\n",
      "epoch no.1 train no.94150  loss = 4.66701 avg_loss = 3.79505\n",
      "epoch no.1 train no.94160  loss = 2.55522 avg_loss = 3.74156\n",
      "epoch no.1 train no.94170  loss = 4.59949 avg_loss = 3.80450\n",
      "epoch no.1 train no.94180  loss = 5.24647 avg_loss = 3.78822\n",
      "epoch no.1 train no.94190  loss = 3.71092 avg_loss = 3.79372\n",
      "epoch no.1 train no.94200  loss = 3.09991 avg_loss = 3.78587\n",
      "epoch no.1 train no.94210  loss = 4.37038 avg_loss = 3.77835\n",
      "epoch no.1 train no.94220  loss = 3.07312 avg_loss = 3.76846\n",
      "epoch no.1 train no.94230  loss = 1.75598 avg_loss = 3.76286\n",
      "epoch no.1 train no.94240  loss = 2.42076 avg_loss = 3.77133\n",
      "epoch no.1 train no.94250  loss = 4.90779 avg_loss = 3.77362\n",
      "epoch no.1 train no.94260  loss = 2.90960 avg_loss = 3.75507\n",
      "epoch no.1 train no.94270  loss = 4.83450 avg_loss = 3.78167\n",
      "epoch no.1 train no.94280  loss = 2.78488 avg_loss = 3.78147\n",
      "epoch no.1 train no.94290  loss = 2.46606 avg_loss = 3.72843\n",
      "epoch no.1 train no.94300  loss = 3.16144 avg_loss = 3.76593\n",
      "epoch no.1 train no.94310  loss = 3.33264 avg_loss = 3.78271\n",
      "epoch no.1 train no.94320  loss = 6.10380 avg_loss = 3.81689\n",
      "epoch no.1 train no.94330  loss = 2.92395 avg_loss = 3.76921\n",
      "epoch no.1 train no.94340  loss = 3.36584 avg_loss = 3.80792\n",
      "epoch no.1 train no.94350  loss = 4.91415 avg_loss = 3.83497\n",
      "epoch no.1 train no.94360  loss = 2.75520 avg_loss = 3.77733\n",
      "epoch no.1 train no.94370  loss = 2.90049 avg_loss = 3.80247\n",
      "epoch no.1 train no.94380  loss = 5.31969 avg_loss = 3.79418\n",
      "epoch no.1 train no.94390  loss = 3.55112 avg_loss = 3.75986\n",
      "epoch no.1 train no.94400  loss = 3.08143 avg_loss = 3.73450\n",
      "epoch no.1 train no.94410  loss = 3.72611 avg_loss = 3.71910\n",
      "epoch no.1 train no.94420  loss = 2.54416 avg_loss = 3.70782\n",
      "epoch no.1 train no.94430  loss = 3.99625 avg_loss = 3.66414\n",
      "epoch no.1 train no.94440  loss = 3.53399 avg_loss = 3.68906\n",
      "epoch no.1 train no.94450  loss = 3.67733 avg_loss = 3.65456\n",
      "epoch no.1 train no.94460  loss = 4.56340 avg_loss = 3.64477\n",
      "epoch no.1 train no.94470  loss = 3.04305 avg_loss = 3.67769\n",
      "epoch no.1 train no.94480  loss = 3.30241 avg_loss = 3.65326\n",
      "epoch no.1 train no.94490  loss = 3.84413 avg_loss = 3.69148\n",
      "epoch no.1 train no.94500  loss = 2.66518 avg_loss = 3.66905\n",
      "epoch no.1 train no.94510  loss = 3.79955 avg_loss = 3.65870\n",
      "epoch no.1 train no.94520  loss = 4.94151 avg_loss = 3.69061\n",
      "epoch no.1 train no.94530  loss = 3.08367 avg_loss = 3.65203\n",
      "epoch no.1 train no.94540  loss = 2.76730 avg_loss = 3.70905\n",
      "epoch no.1 train no.94550  loss = 3.96928 avg_loss = 3.74460\n",
      "epoch no.1 train no.94560  loss = 3.74848 avg_loss = 3.72827\n",
      "epoch no.1 train no.94570  loss = 3.07067 avg_loss = 3.70407\n",
      "epoch no.1 train no.94580  loss = 3.10900 avg_loss = 3.69083\n",
      "epoch no.1 train no.94590  loss = 2.18469 avg_loss = 3.68199\n",
      "epoch no.1 train no.94600  loss = 2.92896 avg_loss = 3.63675\n",
      "epoch no.1 train no.94610  loss = 1.53717 avg_loss = 3.60698\n",
      "epoch no.1 train no.94620  loss = 2.33281 avg_loss = 3.55419\n",
      "epoch no.1 train no.94630  loss = 2.87437 avg_loss = 3.57815\n",
      "epoch no.1 train no.94640  loss = 2.24525 avg_loss = 3.57291\n",
      "epoch no.1 train no.94650  loss = 2.24450 avg_loss = 3.59716\n",
      "epoch no.1 train no.94660  loss = 3.91241 avg_loss = 3.59606\n",
      "epoch no.1 train no.94670  loss = 4.02976 avg_loss = 3.61216\n",
      "epoch no.1 train no.94680  loss = 2.51175 avg_loss = 3.64108\n",
      "epoch no.1 train no.94690  loss = 3.89380 avg_loss = 3.63684\n",
      "epoch no.1 train no.94700  loss = 4.79294 avg_loss = 3.62963\n",
      "epoch no.1 train no.94710  loss = 2.86032 avg_loss = 3.58677\n",
      "epoch no.1 train no.94720  loss = 4.40619 avg_loss = 3.51556\n",
      "epoch no.1 train no.94730  loss = 4.67889 avg_loss = 3.52329\n",
      "epoch no.1 train no.94740  loss = 4.06118 avg_loss = 3.52558\n",
      "epoch no.1 train no.94750  loss = 5.77425 avg_loss = 3.53525\n",
      "epoch no.1 train no.94760  loss = 2.60758 avg_loss = 3.55008\n",
      "epoch no.1 train no.94770  loss = 4.24174 avg_loss = 3.56935\n",
      "epoch no.1 train no.94780  loss = 3.42220 avg_loss = 3.58877\n",
      "epoch no.1 train no.94790  loss = 3.81393 avg_loss = 3.60738\n",
      "epoch no.1 train no.94800  loss = 6.74882 avg_loss = 3.63900\n",
      "epoch no.1 train no.94810  loss = 2.78743 avg_loss = 3.65223\n",
      "epoch no.1 train no.94820  loss = 3.00169 avg_loss = 3.64625\n",
      "epoch no.1 train no.94830  loss = 5.54339 avg_loss = 3.65373\n",
      "epoch no.1 train no.94840  loss = 4.27508 avg_loss = 3.65567\n",
      "epoch no.1 train no.94850  loss = 3.69299 avg_loss = 3.60241\n",
      "epoch no.1 train no.94860  loss = 4.40578 avg_loss = 3.65172\n",
      "epoch no.1 train no.94870  loss = 2.67909 avg_loss = 3.60636\n",
      "epoch no.1 train no.94880  loss = 3.37992 avg_loss = 3.66160\n",
      "epoch no.1 train no.94890  loss = 5.41258 avg_loss = 3.68907\n",
      "epoch no.1 train no.94900  loss = 4.99589 avg_loss = 3.74558\n",
      "epoch no.1 train no.94910  loss = 7.51176 avg_loss = 3.79583\n",
      "epoch no.1 train no.94920  loss = 2.95696 avg_loss = 3.79988\n",
      "epoch no.1 train no.94930  loss = 2.90449 avg_loss = 3.79962\n",
      "epoch no.1 train no.94940  loss = 1.91334 avg_loss = 3.73792\n",
      "epoch no.1 train no.94950  loss = 2.32843 avg_loss = 3.69117\n",
      "epoch no.1 train no.94960  loss = 5.72555 avg_loss = 3.65463\n",
      "epoch no.1 train no.94970  loss = 2.10096 avg_loss = 3.63029\n",
      "epoch no.1 train no.94980  loss = 3.48404 avg_loss = 3.62677\n",
      "epoch no.1 train no.94990  loss = 4.40666 avg_loss = 3.61560\n",
      "epoch no.1 train no.95000  loss = 4.46572 avg_loss = 3.65696\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '시절', '▁그', '노래', '</s>']\n",
      "추억의 그시절 그노래</s>\n",
      "epoch no.1 train no.95010  loss = 5.14761 avg_loss = 3.66802\n",
      "epoch no.1 train no.95020  loss = 3.76391 avg_loss = 3.66433\n",
      "epoch no.1 train no.95030  loss = 4.15408 avg_loss = 3.65042\n",
      "epoch no.1 train no.95040  loss = 5.67305 avg_loss = 3.66285\n",
      "epoch no.1 train no.95050  loss = 3.50044 avg_loss = 3.67905\n",
      "epoch no.1 train no.95060  loss = 2.48596 avg_loss = 3.68116\n",
      "epoch no.1 train no.95070  loss = 2.75477 avg_loss = 3.67566\n",
      "epoch no.1 train no.95080  loss = 2.23450 avg_loss = 3.65935\n",
      "epoch no.1 train no.95090  loss = 2.51222 avg_loss = 3.67375\n",
      "epoch no.1 train no.95100  loss = 5.60322 avg_loss = 3.69489\n",
      "epoch no.1 train no.95110  loss = 6.17710 avg_loss = 3.71796\n",
      "epoch no.1 train no.95120  loss = 4.66311 avg_loss = 3.70389\n",
      "epoch no.1 train no.95130  loss = 6.52478 avg_loss = 3.70733\n",
      "epoch no.1 train no.95140  loss = 4.34072 avg_loss = 3.71662\n",
      "epoch no.1 train no.95150  loss = 5.10795 avg_loss = 3.67236\n",
      "epoch no.1 train no.95160  loss = 3.13692 avg_loss = 3.71526\n",
      "epoch no.1 train no.95170  loss = 4.61985 avg_loss = 3.66766\n",
      "epoch no.1 train no.95180  loss = 2.24577 avg_loss = 3.69247\n",
      "epoch no.1 train no.95190  loss = 3.00485 avg_loss = 3.73299\n",
      "epoch no.1 train no.95200  loss = 3.65990 avg_loss = 3.71130\n",
      "epoch no.1 train no.95210  loss = 6.14219 avg_loss = 3.76471\n",
      "epoch no.1 train no.95220  loss = 2.21622 avg_loss = 3.74925\n",
      "epoch no.1 train no.95230  loss = 4.29789 avg_loss = 3.74828\n",
      "epoch no.1 train no.95240  loss = 3.60013 avg_loss = 3.74619\n",
      "epoch no.1 train no.95250  loss = 2.50368 avg_loss = 3.73491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.95260  loss = 2.86715 avg_loss = 3.71253\n",
      "epoch no.1 train no.95270  loss = 2.86468 avg_loss = 3.72551\n",
      "epoch no.1 train no.95280  loss = 3.01007 avg_loss = 3.67124\n",
      "epoch no.1 train no.95290  loss = 2.73794 avg_loss = 3.64782\n",
      "epoch no.1 train no.95300  loss = 3.75306 avg_loss = 3.59746\n",
      "epoch no.1 train no.95310  loss = 2.10532 avg_loss = 3.63850\n",
      "epoch no.1 train no.95320  loss = 2.71457 avg_loss = 3.65958\n",
      "epoch no.1 train no.95330  loss = 2.93164 avg_loss = 3.67749\n",
      "epoch no.1 train no.95340  loss = 5.67658 avg_loss = 3.66444\n",
      "epoch no.1 train no.95350  loss = 3.67754 avg_loss = 3.65236\n",
      "epoch no.1 train no.95360  loss = 4.47873 avg_loss = 3.65014\n",
      "epoch no.1 train no.95370  loss = 2.86503 avg_loss = 3.70177\n",
      "epoch no.1 train no.95380  loss = 3.50020 avg_loss = 3.73507\n",
      "epoch no.1 train no.95390  loss = 4.74324 avg_loss = 3.74432\n",
      "epoch no.1 train no.95400  loss = 3.29970 avg_loss = 3.76820\n",
      "epoch no.1 train no.95410  loss = 3.56025 avg_loss = 3.77901\n",
      "epoch no.1 train no.95420  loss = 4.04523 avg_loss = 3.76575\n",
      "epoch no.1 train no.95430  loss = 2.64848 avg_loss = 3.70314\n",
      "epoch no.1 train no.95440  loss = 3.25838 avg_loss = 3.69672\n",
      "epoch no.1 train no.95450  loss = 5.21385 avg_loss = 3.68050\n",
      "epoch no.1 train no.95460  loss = 2.61962 avg_loss = 3.68994\n",
      "epoch no.1 train no.95470  loss = 2.69311 avg_loss = 3.65023\n",
      "epoch no.1 train no.95480  loss = 5.94834 avg_loss = 3.70443\n",
      "epoch no.1 train no.95490  loss = 4.18112 avg_loss = 3.69127\n",
      "epoch no.1 train no.95500  loss = 3.87074 avg_loss = 3.68463\n",
      "epoch no.1 train no.95510  loss = 3.71221 avg_loss = 3.73093\n",
      "epoch no.1 train no.95520  loss = 4.33006 avg_loss = 3.78088\n",
      "epoch no.1 train no.95530  loss = 4.24712 avg_loss = 3.78964\n",
      "epoch no.1 train no.95540  loss = 3.80030 avg_loss = 3.77497\n",
      "epoch no.1 train no.95550  loss = 3.05459 avg_loss = 3.76210\n",
      "epoch no.1 train no.95560  loss = 2.78876 avg_loss = 3.73841\n",
      "epoch no.1 train no.95570  loss = 4.18692 avg_loss = 3.78901\n",
      "epoch no.1 train no.95580  loss = 4.89561 avg_loss = 3.81677\n",
      "epoch no.1 train no.95590  loss = 4.40713 avg_loss = 3.81635\n",
      "epoch no.1 train no.95600  loss = 3.26681 avg_loss = 3.78790\n",
      "epoch no.1 train no.95610  loss = 3.38940 avg_loss = 3.78886\n",
      "epoch no.1 train no.95620  loss = 3.23786 avg_loss = 3.77579\n",
      "epoch no.1 train no.95630  loss = 3.75312 avg_loss = 3.72353\n",
      "epoch no.1 train no.95640  loss = 4.47604 avg_loss = 3.73923\n",
      "epoch no.1 train no.95650  loss = 4.07258 avg_loss = 3.75633\n",
      "epoch no.1 train no.95660  loss = 2.36144 avg_loss = 3.70070\n",
      "epoch no.1 train no.95670  loss = 2.38699 avg_loss = 3.67260\n",
      "epoch no.1 train no.95680  loss = 5.67300 avg_loss = 3.72090\n",
      "epoch no.1 train no.95690  loss = 4.37327 avg_loss = 3.75241\n",
      "epoch no.1 train no.95700  loss = 3.12305 avg_loss = 3.69016\n",
      "epoch no.1 train no.95710  loss = 3.53384 avg_loss = 3.64985\n",
      "epoch no.1 train no.95720  loss = 5.32014 avg_loss = 3.68353\n",
      "epoch no.1 train no.95730  loss = 3.41741 avg_loss = 3.67153\n",
      "epoch no.1 train no.95740  loss = 2.57938 avg_loss = 3.60861\n",
      "epoch no.1 train no.95750  loss = 3.04171 avg_loss = 3.63488\n",
      "epoch no.1 train no.95760  loss = 4.41992 avg_loss = 3.64936\n",
      "epoch no.1 train no.95770  loss = 4.75973 avg_loss = 3.69305\n",
      "epoch no.1 train no.95780  loss = 2.96136 avg_loss = 3.71289\n",
      "epoch no.1 train no.95790  loss = 3.58702 avg_loss = 3.73179\n",
      "epoch no.1 train no.95800  loss = 2.24858 avg_loss = 3.69575\n",
      "epoch no.1 train no.95810  loss = 2.23675 avg_loss = 3.65205\n",
      "epoch no.1 train no.95820  loss = 3.48370 avg_loss = 3.60943\n",
      "epoch no.1 train no.95830  loss = 2.39390 avg_loss = 3.57266\n",
      "epoch no.1 train no.95840  loss = 2.51325 avg_loss = 3.56792\n",
      "epoch no.1 train no.95850  loss = 4.03598 avg_loss = 3.55898\n",
      "epoch no.1 train no.95860  loss = 3.87049 avg_loss = 3.56808\n",
      "epoch no.1 train no.95870  loss = 2.78524 avg_loss = 3.54475\n",
      "epoch no.1 train no.95880  loss = 5.36465 avg_loss = 3.52511\n",
      "epoch no.1 train no.95890  loss = 2.38357 avg_loss = 3.49965\n",
      "epoch no.1 train no.95900  loss = 3.46266 avg_loss = 3.51060\n",
      "epoch no.1 train no.95910  loss = 2.40029 avg_loss = 3.51352\n",
      "epoch no.1 train no.95920  loss = 4.74788 avg_loss = 3.47346\n",
      "epoch no.1 train no.95930  loss = 4.08534 avg_loss = 3.47912\n",
      "epoch no.1 train no.95940  loss = 3.54949 avg_loss = 3.44875\n",
      "epoch no.1 train no.95950  loss = 3.02950 avg_loss = 3.52055\n",
      "epoch no.1 train no.95960  loss = 3.06366 avg_loss = 3.52077\n",
      "epoch no.1 train no.95970  loss = 2.65000 avg_loss = 3.53086\n",
      "epoch no.1 train no.95980  loss = 3.06414 avg_loss = 3.54830\n",
      "epoch no.1 train no.95990  loss = 4.37234 avg_loss = 3.55738\n",
      "epoch no.1 train no.96000  loss = 4.17597 avg_loss = 3.54475\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁가요', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.1 train no.96010  loss = 2.38420 avg_loss = 3.52820\n",
      "epoch no.1 train no.96020  loss = 5.39783 avg_loss = 3.54089\n",
      "epoch no.1 train no.96030  loss = 3.01454 avg_loss = 3.50913\n",
      "epoch no.1 train no.96040  loss = 4.93516 avg_loss = 3.48190\n",
      "epoch no.1 train no.96050  loss = 3.29739 avg_loss = 3.53231\n",
      "epoch no.1 train no.96060  loss = 3.30402 avg_loss = 3.47893\n",
      "epoch no.1 train no.96070  loss = 2.39868 avg_loss = 3.52865\n",
      "epoch no.1 train no.96080  loss = 5.59625 avg_loss = 3.54312\n",
      "epoch no.1 train no.96090  loss = 5.37840 avg_loss = 3.57319\n",
      "epoch no.1 train no.96100  loss = 4.54412 avg_loss = 3.55849\n",
      "epoch no.1 train no.96110  loss = 5.58716 avg_loss = 3.59852\n",
      "epoch no.1 train no.96120  loss = 4.60656 avg_loss = 3.60768\n",
      "epoch no.1 train no.96130  loss = 3.19804 avg_loss = 3.52561\n",
      "epoch no.1 train no.96140  loss = 3.39740 avg_loss = 3.57351\n",
      "epoch no.1 train no.96150  loss = 3.27522 avg_loss = 3.57490\n",
      "epoch no.1 train no.96160  loss = 4.04172 avg_loss = 3.58050\n",
      "epoch no.1 train no.96170  loss = 3.33826 avg_loss = 3.59223\n",
      "epoch no.1 train no.96180  loss = 4.97382 avg_loss = 3.63087\n",
      "epoch no.1 train no.96190  loss = 2.83033 avg_loss = 3.57508\n",
      "epoch no.1 train no.96200  loss = 2.89276 avg_loss = 3.54882\n",
      "epoch no.1 train no.96210  loss = 1.82012 avg_loss = 3.50570\n",
      "epoch no.1 train no.96220  loss = 3.51570 avg_loss = 3.51219\n",
      "epoch no.1 train no.96230  loss = 6.19793 avg_loss = 3.55258\n",
      "epoch no.1 train no.96240  loss = 4.26438 avg_loss = 3.56562\n",
      "epoch no.1 train no.96250  loss = 2.53888 avg_loss = 3.49188\n",
      "epoch no.1 train no.96260  loss = 4.86863 avg_loss = 3.47306\n",
      "epoch no.1 train no.96270  loss = 4.40539 avg_loss = 3.49758\n",
      "epoch no.1 train no.96280  loss = 3.03888 avg_loss = 3.55717\n",
      "epoch no.1 train no.96290  loss = 3.07238 avg_loss = 3.57844\n",
      "epoch no.1 train no.96300  loss = 3.97421 avg_loss = 3.58249\n",
      "epoch no.1 train no.96310  loss = 3.95687 avg_loss = 3.59045\n",
      "epoch no.1 train no.96320  loss = 2.47575 avg_loss = 3.69019\n",
      "epoch no.1 train no.96330  loss = 3.21920 avg_loss = 3.71020\n",
      "epoch no.1 train no.96340  loss = 2.17519 avg_loss = 3.65982\n",
      "epoch no.1 train no.96350  loss = 2.93540 avg_loss = 3.62696\n",
      "epoch no.1 train no.96360  loss = 2.31964 avg_loss = 3.64232\n",
      "epoch no.1 train no.96370  loss = 4.69409 avg_loss = 3.62342\n",
      "epoch no.1 train no.96380  loss = 4.65459 avg_loss = 3.70857\n",
      "epoch no.1 train no.96390  loss = 4.04719 avg_loss = 3.65850\n",
      "epoch no.1 train no.96400  loss = 5.40900 avg_loss = 3.70941\n",
      "epoch no.1 train no.96410  loss = 3.65071 avg_loss = 3.73638\n",
      "epoch no.1 train no.96420  loss = 4.25223 avg_loss = 3.69124\n",
      "epoch no.1 train no.96430  loss = 1.79580 avg_loss = 3.65745\n",
      "epoch no.1 train no.96440  loss = 4.35746 avg_loss = 3.64473\n",
      "epoch no.1 train no.96450  loss = 6.06460 avg_loss = 3.71057\n",
      "epoch no.1 train no.96460  loss = 3.07197 avg_loss = 3.63528\n",
      "epoch no.1 train no.96470  loss = 1.90070 avg_loss = 3.62168\n",
      "epoch no.1 train no.96480  loss = 2.74323 avg_loss = 3.67593\n",
      "epoch no.1 train no.96490  loss = 3.59783 avg_loss = 3.67498\n",
      "epoch no.1 train no.96500  loss = 5.37530 avg_loss = 3.69977\n",
      "epoch no.1 train no.96510  loss = 2.39990 avg_loss = 3.71473\n",
      "epoch no.1 train no.96520  loss = 4.25585 avg_loss = 3.71530\n",
      "epoch no.1 train no.96530  loss = 2.43993 avg_loss = 3.76003\n",
      "epoch no.1 train no.96540  loss = 2.75735 avg_loss = 3.74410\n",
      "epoch no.1 train no.96550  loss = 4.27880 avg_loss = 3.71553\n",
      "epoch no.1 train no.96560  loss = 2.21102 avg_loss = 3.64247\n",
      "epoch no.1 train no.96570  loss = 3.57160 avg_loss = 3.61911\n",
      "epoch no.1 train no.96580  loss = 3.10063 avg_loss = 3.58333\n",
      "epoch no.1 train no.96590  loss = 5.29344 avg_loss = 3.57401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.96600  loss = 5.29448 avg_loss = 3.62981\n",
      "epoch no.1 train no.96610  loss = 3.23678 avg_loss = 3.61837\n",
      "epoch no.1 train no.96620  loss = 4.50374 avg_loss = 3.57074\n",
      "epoch no.1 train no.96630  loss = 3.06112 avg_loss = 3.57255\n",
      "epoch no.1 train no.96640  loss = 3.80778 avg_loss = 3.62063\n",
      "epoch no.1 train no.96650  loss = 2.29555 avg_loss = 3.57222\n",
      "epoch no.1 train no.96660  loss = 3.14999 avg_loss = 3.60217\n",
      "epoch no.1 train no.96670  loss = 3.73127 avg_loss = 3.61100\n",
      "epoch no.1 train no.96680  loss = 2.25411 avg_loss = 3.55631\n",
      "epoch no.1 train no.96690  loss = 4.82544 avg_loss = 3.59219\n",
      "epoch no.1 train no.96700  loss = 2.49498 avg_loss = 3.58431\n",
      "epoch no.1 train no.96710  loss = 5.19524 avg_loss = 3.61769\n",
      "epoch no.1 train no.96720  loss = 2.31695 avg_loss = 3.59009\n",
      "epoch no.1 train no.96730  loss = 2.07757 avg_loss = 3.60835\n",
      "epoch no.1 train no.96740  loss = 3.03341 avg_loss = 3.61566\n",
      "epoch no.1 train no.96750  loss = 1.72050 avg_loss = 3.60405\n",
      "epoch no.1 train no.96760  loss = 3.33957 avg_loss = 3.57516\n",
      "epoch no.1 train no.96770  loss = 3.09341 avg_loss = 3.50198\n",
      "epoch no.1 train no.96780  loss = 3.70289 avg_loss = 3.50008\n",
      "epoch no.1 train no.96790  loss = 4.57056 avg_loss = 3.52718\n",
      "epoch no.1 train no.96800  loss = 2.98675 avg_loss = 3.54996\n",
      "epoch no.1 train no.96810  loss = 2.09128 avg_loss = 3.54203\n",
      "epoch no.1 train no.96820  loss = 3.81879 avg_loss = 3.58135\n",
      "epoch no.1 train no.96830  loss = 2.71883 avg_loss = 3.54275\n",
      "epoch no.1 train no.96840  loss = 3.81078 avg_loss = 3.53452\n",
      "epoch no.1 train no.96850  loss = 5.67948 avg_loss = 3.52603\n",
      "epoch no.1 train no.96860  loss = 5.07229 avg_loss = 3.59306\n",
      "epoch no.1 train no.96870  loss = 2.68371 avg_loss = 3.62792\n",
      "epoch no.1 train no.96880  loss = 3.61825 avg_loss = 3.58318\n",
      "epoch no.1 train no.96890  loss = 3.78177 avg_loss = 3.59268\n",
      "epoch no.1 train no.96900  loss = 2.47204 avg_loss = 3.56270\n",
      "epoch no.1 train no.96910  loss = 3.36643 avg_loss = 3.57800\n",
      "epoch no.1 train no.96920  loss = 2.23689 avg_loss = 3.61092\n",
      "epoch no.1 train no.96930  loss = 3.33505 avg_loss = 3.57756\n",
      "epoch no.1 train no.96940  loss = 3.36713 avg_loss = 3.60089\n",
      "epoch no.1 train no.96950  loss = 2.19237 avg_loss = 3.55225\n",
      "epoch no.1 train no.96960  loss = 2.50275 avg_loss = 3.56628\n",
      "epoch no.1 train no.96970  loss = 5.07375 avg_loss = 3.60332\n",
      "epoch no.1 train no.96980  loss = 5.56402 avg_loss = 3.60322\n",
      "epoch no.1 train no.96990  loss = 4.47058 avg_loss = 3.65265\n",
      "epoch no.1 train no.97000  loss = 3.51137 avg_loss = 3.64113\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.1 train no.97010  loss = 4.91875 avg_loss = 3.72085\n",
      "epoch no.1 train no.97020  loss = 3.33616 avg_loss = 3.74976\n",
      "epoch no.1 train no.97030  loss = 3.23843 avg_loss = 3.72708\n",
      "epoch no.1 train no.97040  loss = 4.70960 avg_loss = 3.66512\n",
      "epoch no.1 train no.97050  loss = 3.00539 avg_loss = 3.61237\n",
      "epoch no.1 train no.97060  loss = 2.37508 avg_loss = 3.63294\n",
      "epoch no.1 train no.97070  loss = 5.78032 avg_loss = 3.62973\n",
      "epoch no.1 train no.97080  loss = 6.13994 avg_loss = 3.63345\n",
      "epoch no.1 train no.97090  loss = 4.05815 avg_loss = 3.67706\n",
      "epoch no.1 train no.97100  loss = 4.31086 avg_loss = 3.71903\n",
      "epoch no.1 train no.97110  loss = 2.21457 avg_loss = 3.68202\n",
      "epoch no.1 train no.97120  loss = 7.68956 avg_loss = 3.73213\n",
      "epoch no.1 train no.97130  loss = 2.43065 avg_loss = 3.68604\n",
      "epoch no.1 train no.97140  loss = 2.90313 avg_loss = 3.72207\n",
      "epoch no.1 train no.97150  loss = 2.13918 avg_loss = 3.72744\n",
      "epoch no.1 train no.97160  loss = 3.67973 avg_loss = 3.71732\n",
      "epoch no.1 train no.97170  loss = 2.79340 avg_loss = 3.63910\n",
      "epoch no.1 train no.97180  loss = 5.72615 avg_loss = 3.67797\n",
      "epoch no.1 train no.97190  loss = 3.25171 avg_loss = 3.63107\n",
      "epoch no.1 train no.97200  loss = 2.75175 avg_loss = 3.66936\n",
      "epoch no.1 train no.97210  loss = 2.64469 avg_loss = 3.64021\n",
      "epoch no.1 train no.97220  loss = 4.57882 avg_loss = 3.60520\n",
      "epoch no.1 train no.97230  loss = 5.03505 avg_loss = 3.67573\n",
      "epoch no.1 train no.97240  loss = 5.38436 avg_loss = 3.67085\n",
      "epoch no.1 train no.97250  loss = 4.87284 avg_loss = 3.67567\n",
      "epoch no.1 train no.97260  loss = 3.27899 avg_loss = 3.70140\n",
      "epoch no.1 train no.97270  loss = 4.18721 avg_loss = 3.69183\n",
      "epoch no.1 train no.97280  loss = 3.94943 avg_loss = 3.67151\n",
      "epoch no.1 train no.97290  loss = 5.66856 avg_loss = 3.68480\n",
      "epoch no.1 train no.97300  loss = 1.69623 avg_loss = 3.72404\n",
      "epoch no.1 train no.97310  loss = 2.91470 avg_loss = 3.68030\n",
      "epoch no.1 train no.97320  loss = 3.72466 avg_loss = 3.63290\n",
      "epoch no.1 train no.97330  loss = 4.23036 avg_loss = 3.66414\n",
      "epoch no.1 train no.97340  loss = 4.21768 avg_loss = 3.64624\n",
      "epoch no.1 train no.97350  loss = 5.76439 avg_loss = 3.68697\n",
      "epoch no.1 train no.97360  loss = 2.52157 avg_loss = 3.62878\n",
      "epoch no.1 train no.97370  loss = 3.42452 avg_loss = 3.69256\n",
      "epoch no.1 train no.97380  loss = 3.57159 avg_loss = 3.65411\n",
      "epoch no.1 train no.97390  loss = 4.21148 avg_loss = 3.67183\n",
      "epoch no.1 train no.97400  loss = 2.58912 avg_loss = 3.67523\n",
      "epoch no.1 train no.97410  loss = 4.25246 avg_loss = 3.71227\n",
      "epoch no.1 train no.97420  loss = 2.27547 avg_loss = 3.68894\n",
      "epoch no.1 train no.97430  loss = 3.77976 avg_loss = 3.71072\n",
      "epoch no.1 train no.97440  loss = 3.72501 avg_loss = 3.71242\n",
      "epoch no.1 train no.97450  loss = 2.23706 avg_loss = 3.67440\n",
      "epoch no.1 train no.97460  loss = 3.46066 avg_loss = 3.69808\n",
      "epoch no.1 train no.97470  loss = 3.25825 avg_loss = 3.66305\n",
      "epoch no.1 train no.97480  loss = 2.37411 avg_loss = 3.67795\n",
      "epoch no.1 train no.97490  loss = 3.49827 avg_loss = 3.68277\n",
      "epoch no.1 train no.97500  loss = 3.90480 avg_loss = 3.71064\n",
      "epoch no.1 train no.97510  loss = 4.94362 avg_loss = 3.74732\n",
      "epoch no.1 train no.97520  loss = 2.65102 avg_loss = 3.72563\n",
      "epoch no.1 train no.97530  loss = 3.38245 avg_loss = 3.68285\n",
      "epoch no.1 train no.97540  loss = 3.43883 avg_loss = 3.71379\n",
      "epoch no.1 train no.97550  loss = 3.39252 avg_loss = 3.69230\n",
      "epoch no.1 train no.97560  loss = 2.60251 avg_loss = 3.69298\n",
      "epoch no.1 train no.97570  loss = 4.16080 avg_loss = 3.65767\n",
      "epoch no.1 train no.97580  loss = 3.92671 avg_loss = 3.60482\n",
      "epoch no.1 train no.97590  loss = 2.90303 avg_loss = 3.57252\n",
      "epoch no.1 train no.97600  loss = 6.35590 avg_loss = 3.61787\n",
      "epoch no.1 train no.97610  loss = 2.80169 avg_loss = 3.59149\n",
      "epoch no.1 train no.97620  loss = 4.13906 avg_loss = 3.58933\n",
      "epoch no.1 train no.97630  loss = 2.52717 avg_loss = 3.61793\n",
      "epoch no.1 train no.97640  loss = 2.33849 avg_loss = 3.56365\n",
      "epoch no.1 train no.97650  loss = 3.50197 avg_loss = 3.61288\n",
      "epoch no.1 train no.97660  loss = 4.63009 avg_loss = 3.65333\n",
      "epoch no.1 train no.97670  loss = 4.09042 avg_loss = 3.62879\n",
      "epoch no.1 train no.97680  loss = 1.92177 avg_loss = 3.60717\n",
      "epoch no.1 train no.97690  loss = 3.41296 avg_loss = 3.61169\n",
      "epoch no.1 train no.97700  loss = 5.00704 avg_loss = 3.68056\n",
      "epoch no.1 train no.97710  loss = 3.86401 avg_loss = 3.69437\n",
      "epoch no.1 train no.97720  loss = 1.17703 avg_loss = 3.64582\n",
      "epoch no.1 train no.97730  loss = 3.68573 avg_loss = 3.71411\n",
      "epoch no.1 train no.97740  loss = 4.18928 avg_loss = 3.72165\n",
      "epoch no.1 train no.97750  loss = 2.58978 avg_loss = 3.69993\n",
      "epoch no.1 train no.97760  loss = 2.50211 avg_loss = 3.66904\n",
      "epoch no.1 train no.97770  loss = 4.27874 avg_loss = 3.65434\n",
      "epoch no.1 train no.97780  loss = 3.17287 avg_loss = 3.60993\n",
      "epoch no.1 train no.97790  loss = 4.26678 avg_loss = 3.63789\n",
      "epoch no.1 train no.97800  loss = 2.48432 avg_loss = 3.62202\n",
      "epoch no.1 train no.97810  loss = 1.94648 avg_loss = 3.60715\n",
      "epoch no.1 train no.97820  loss = 3.39341 avg_loss = 3.60586\n",
      "epoch no.1 train no.97830  loss = 3.63307 avg_loss = 3.64862\n",
      "epoch no.1 train no.97840  loss = 2.51392 avg_loss = 3.66421\n",
      "epoch no.1 train no.97850  loss = 2.33736 avg_loss = 3.66982\n",
      "epoch no.1 train no.97860  loss = 2.89817 avg_loss = 3.67663\n",
      "epoch no.1 train no.97870  loss = 4.34024 avg_loss = 3.67924\n",
      "epoch no.1 train no.97880  loss = 3.22377 avg_loss = 3.63767\n",
      "epoch no.1 train no.97890  loss = 4.17514 avg_loss = 3.64933\n",
      "epoch no.1 train no.97900  loss = 3.61253 avg_loss = 3.64851\n",
      "epoch no.1 train no.97910  loss = 1.88414 avg_loss = 3.65655\n",
      "epoch no.1 train no.97920  loss = 3.34962 avg_loss = 3.62971\n",
      "epoch no.1 train no.97930  loss = 2.29458 avg_loss = 3.63962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.97940  loss = 4.37818 avg_loss = 3.66947\n",
      "epoch no.1 train no.97950  loss = 4.32147 avg_loss = 3.65396\n",
      "epoch no.1 train no.97960  loss = 2.16212 avg_loss = 3.65131\n",
      "epoch no.1 train no.97970  loss = 2.47463 avg_loss = 3.64123\n",
      "epoch no.1 train no.97980  loss = 3.52682 avg_loss = 3.62799\n",
      "epoch no.1 train no.97990  loss = 4.72476 avg_loss = 3.64410\n",
      "epoch no.1 train no.98000  loss = 6.64246 avg_loss = 3.68387\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁배경', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.1 train no.98010  loss = 3.14339 avg_loss = 3.69561\n",
      "epoch no.1 train no.98020  loss = 3.36725 avg_loss = 3.67004\n",
      "epoch no.1 train no.98030  loss = 7.74057 avg_loss = 3.76848\n",
      "epoch no.1 train no.98040  loss = 2.22661 avg_loss = 3.75325\n",
      "epoch no.1 train no.98050  loss = 4.71333 avg_loss = 3.75067\n",
      "epoch no.1 train no.98060  loss = 3.20418 avg_loss = 3.73535\n",
      "epoch no.1 train no.98070  loss = 3.78781 avg_loss = 3.68269\n",
      "epoch no.1 train no.98080  loss = 2.87000 avg_loss = 3.67969\n",
      "epoch no.1 train no.98090  loss = 4.97138 avg_loss = 3.69674\n",
      "epoch no.1 train no.98100  loss = 1.98630 avg_loss = 3.68462\n",
      "epoch no.1 train no.98110  loss = 3.96933 avg_loss = 3.65638\n",
      "epoch no.1 train no.98120  loss = 2.78724 avg_loss = 3.63105\n",
      "epoch no.1 train no.98130  loss = 2.87079 avg_loss = 3.61939\n",
      "epoch no.1 train no.98140  loss = 4.24204 avg_loss = 3.67938\n",
      "epoch no.1 train no.98150  loss = 3.46443 avg_loss = 3.66744\n",
      "epoch no.1 train no.98160  loss = 5.98585 avg_loss = 3.69398\n",
      "epoch no.1 train no.98170  loss = 3.11518 avg_loss = 3.69375\n",
      "epoch no.1 train no.98180  loss = 2.43596 avg_loss = 3.68830\n",
      "epoch no.1 train no.98190  loss = 3.07540 avg_loss = 3.68328\n",
      "epoch no.1 train no.98200  loss = 3.37498 avg_loss = 3.65797\n",
      "epoch no.1 train no.98210  loss = 2.64543 avg_loss = 3.63518\n",
      "epoch no.1 train no.98220  loss = 2.36063 avg_loss = 3.57730\n",
      "epoch no.1 train no.98230  loss = 4.71878 avg_loss = 3.60278\n",
      "epoch no.1 train no.98240  loss = 2.35641 avg_loss = 3.56876\n",
      "epoch no.1 train no.98250  loss = 2.90407 avg_loss = 3.58907\n",
      "epoch no.1 train no.98260  loss = 3.50794 avg_loss = 3.58743\n",
      "epoch no.1 train no.98270  loss = 1.74729 avg_loss = 3.51321\n",
      "epoch no.1 train no.98280  loss = 3.68672 avg_loss = 3.50044\n",
      "epoch no.1 train no.98290  loss = 2.09922 avg_loss = 3.48273\n",
      "epoch no.1 train no.98300  loss = 3.96202 avg_loss = 3.50118\n",
      "epoch no.1 train no.98310  loss = 4.32520 avg_loss = 3.50360\n",
      "epoch no.1 train no.98320  loss = 3.05747 avg_loss = 3.52660\n",
      "epoch no.1 train no.98330  loss = 5.15679 avg_loss = 3.53225\n",
      "epoch no.1 train no.98340  loss = 3.83017 avg_loss = 3.48139\n",
      "epoch no.1 train no.98350  loss = 6.25405 avg_loss = 3.51882\n",
      "epoch no.1 train no.98360  loss = 5.20836 avg_loss = 3.56333\n",
      "epoch no.1 train no.98370  loss = 6.31282 avg_loss = 3.60042\n",
      "epoch no.1 train no.98380  loss = 2.86534 avg_loss = 3.59295\n",
      "epoch no.1 train no.98390  loss = 3.69276 avg_loss = 3.57169\n",
      "epoch no.1 train no.98400  loss = 3.33680 avg_loss = 3.59002\n",
      "epoch no.1 train no.98410  loss = 4.18396 avg_loss = 3.62351\n",
      "epoch no.1 train no.98420  loss = 4.40855 avg_loss = 3.62631\n",
      "epoch no.1 train no.98430  loss = 3.57248 avg_loss = 3.58539\n",
      "epoch no.1 train no.98440  loss = 3.78639 avg_loss = 3.60420\n",
      "epoch no.1 train no.98450  loss = 3.44585 avg_loss = 3.57010\n",
      "epoch no.1 train no.98460  loss = 4.04483 avg_loss = 3.54978\n",
      "epoch no.1 train no.98470  loss = 3.49431 avg_loss = 3.58175\n",
      "epoch no.1 train no.98480  loss = 4.25644 avg_loss = 3.61203\n",
      "epoch no.1 train no.98490  loss = 4.10385 avg_loss = 3.57158\n",
      "epoch no.1 train no.98500  loss = 3.21112 avg_loss = 3.56859\n",
      "epoch no.1 train no.98510  loss = 2.48567 avg_loss = 3.58452\n",
      "epoch no.1 train no.98520  loss = 4.81576 avg_loss = 3.61192\n",
      "epoch no.1 train no.98530  loss = 5.32407 avg_loss = 3.62966\n",
      "epoch no.1 train no.98540  loss = 3.19685 avg_loss = 3.63013\n",
      "epoch no.1 train no.98550  loss = 3.92423 avg_loss = 3.65511\n",
      "epoch no.1 train no.98560  loss = 2.19591 avg_loss = 3.62546\n",
      "epoch no.1 train no.98570  loss = 2.62680 avg_loss = 3.64707\n",
      "epoch no.1 train no.98580  loss = 2.67464 avg_loss = 3.60386\n",
      "epoch no.1 train no.98590  loss = 4.20842 avg_loss = 3.63457\n",
      "epoch no.1 train no.98600  loss = 2.76084 avg_loss = 3.58380\n",
      "epoch no.1 train no.98610  loss = 3.84416 avg_loss = 3.57690\n",
      "epoch no.1 train no.98620  loss = 3.86413 avg_loss = 3.59886\n",
      "epoch no.1 train no.98630  loss = 2.21236 avg_loss = 3.60922\n",
      "epoch no.1 train no.98640  loss = 5.00949 avg_loss = 3.56103\n",
      "epoch no.1 train no.98650  loss = 3.92014 avg_loss = 3.58894\n",
      "epoch no.1 train no.98660  loss = 4.45942 avg_loss = 3.54413\n",
      "epoch no.1 train no.98670  loss = 4.15197 avg_loss = 3.58639\n",
      "epoch no.1 train no.98680  loss = 5.00237 avg_loss = 3.59931\n",
      "epoch no.1 train no.98690  loss = 4.12967 avg_loss = 3.53903\n",
      "epoch no.1 train no.98700  loss = 3.99448 avg_loss = 3.57616\n",
      "epoch no.1 train no.98710  loss = 3.77939 avg_loss = 3.56728\n",
      "epoch no.1 train no.98720  loss = 3.13903 avg_loss = 3.63131\n",
      "epoch no.1 train no.98730  loss = 3.43582 avg_loss = 3.59874\n",
      "epoch no.1 train no.98740  loss = 3.91510 avg_loss = 3.56977\n",
      "epoch no.1 train no.98750  loss = 2.32609 avg_loss = 3.54159\n",
      "epoch no.1 train no.98760  loss = 4.71586 avg_loss = 3.61294\n",
      "epoch no.1 train no.98770  loss = 3.86074 avg_loss = 3.64729\n",
      "epoch no.1 train no.98780  loss = 2.28354 avg_loss = 3.65516\n",
      "epoch no.1 train no.98790  loss = 5.33810 avg_loss = 3.69717\n",
      "epoch no.1 train no.98800  loss = 3.06157 avg_loss = 3.73703\n",
      "epoch no.1 train no.98810  loss = 4.27036 avg_loss = 3.66921\n",
      "epoch no.1 train no.98820  loss = 4.34256 avg_loss = 3.63282\n",
      "epoch no.1 train no.98830  loss = 4.05051 avg_loss = 3.67394\n",
      "epoch no.1 train no.98840  loss = 3.12575 avg_loss = 3.65235\n",
      "epoch no.1 train no.98850  loss = 2.25831 avg_loss = 3.62487\n",
      "epoch no.1 train no.98860  loss = 6.37021 avg_loss = 3.58847\n",
      "epoch no.1 train no.98870  loss = 4.65974 avg_loss = 3.57347\n",
      "epoch no.1 train no.98880  loss = 2.75928 avg_loss = 3.53194\n",
      "epoch no.1 train no.98890  loss = 2.80163 avg_loss = 3.55578\n",
      "epoch no.1 train no.98900  loss = 7.44690 avg_loss = 3.61562\n",
      "epoch no.1 train no.98910  loss = 5.56752 avg_loss = 3.64684\n",
      "epoch no.1 train no.98920  loss = 4.82300 avg_loss = 3.68446\n",
      "epoch no.1 train no.98930  loss = 2.81421 avg_loss = 3.65022\n",
      "epoch no.1 train no.98940  loss = 2.90832 avg_loss = 3.69566\n",
      "epoch no.1 train no.98950  loss = 5.16994 avg_loss = 3.69814\n",
      "epoch no.1 train no.98960  loss = 5.25780 avg_loss = 3.72867\n",
      "epoch no.1 train no.98970  loss = 3.74492 avg_loss = 3.72226\n",
      "epoch no.1 train no.98980  loss = 2.30300 avg_loss = 3.74993\n",
      "epoch no.1 train no.98990  loss = 3.50631 avg_loss = 3.75370\n",
      "epoch no.1 train no.99000  loss = 4.57806 avg_loss = 3.75884\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.1 train no.99010  loss = 4.01686 avg_loss = 3.79601\n",
      "epoch no.1 train no.99020  loss = 4.27994 avg_loss = 3.79482\n",
      "epoch no.1 train no.99030  loss = 3.65548 avg_loss = 3.78919\n",
      "epoch no.1 train no.99040  loss = 5.91659 avg_loss = 3.83307\n",
      "epoch no.1 train no.99050  loss = 3.68484 avg_loss = 3.73438\n",
      "epoch no.1 train no.99060  loss = 6.30878 avg_loss = 3.79079\n",
      "epoch no.1 train no.99070  loss = 7.49082 avg_loss = 3.81502\n",
      "epoch no.1 train no.99080  loss = 3.64382 avg_loss = 3.78421\n",
      "epoch no.1 train no.99090  loss = 5.17024 avg_loss = 3.78785\n",
      "epoch no.1 train no.99100  loss = 4.69591 avg_loss = 3.78479\n",
      "epoch no.1 train no.99110  loss = 3.22738 avg_loss = 3.70248\n",
      "epoch no.1 train no.99120  loss = 5.61478 avg_loss = 3.72757\n",
      "epoch no.1 train no.99130  loss = 3.81314 avg_loss = 3.67824\n",
      "epoch no.1 train no.99140  loss = 4.17483 avg_loss = 3.68528\n",
      "epoch no.1 train no.99150  loss = 1.86934 avg_loss = 3.68325\n",
      "epoch no.1 train no.99160  loss = 5.43838 avg_loss = 3.68685\n",
      "epoch no.1 train no.99170  loss = 4.32692 avg_loss = 3.69333\n",
      "epoch no.1 train no.99180  loss = 3.66332 avg_loss = 3.63509\n",
      "epoch no.1 train no.99190  loss = 4.16340 avg_loss = 3.62877\n",
      "epoch no.1 train no.99200  loss = 3.20444 avg_loss = 3.67240\n",
      "epoch no.1 train no.99210  loss = 3.06751 avg_loss = 3.75132\n",
      "epoch no.1 train no.99220  loss = 6.03127 avg_loss = 3.76319\n",
      "epoch no.1 train no.99230  loss = 5.46575 avg_loss = 3.77929\n",
      "epoch no.1 train no.99240  loss = 3.99033 avg_loss = 3.76668\n",
      "epoch no.1 train no.99250  loss = 6.60364 avg_loss = 3.77121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.99260  loss = 4.09824 avg_loss = 3.76041\n",
      "epoch no.1 train no.99270  loss = 3.15263 avg_loss = 3.72283\n",
      "epoch no.1 train no.99280  loss = 3.66614 avg_loss = 3.69590\n",
      "epoch no.1 train no.99290  loss = 2.38730 avg_loss = 3.63868\n",
      "epoch no.1 train no.99300  loss = 4.91055 avg_loss = 3.62496\n",
      "epoch no.1 train no.99310  loss = 4.49701 avg_loss = 3.62209\n",
      "epoch no.1 train no.99320  loss = 4.07511 avg_loss = 3.67499\n",
      "epoch no.1 train no.99330  loss = 4.12585 avg_loss = 3.65792\n",
      "epoch no.1 train no.99340  loss = 3.64631 avg_loss = 3.61393\n",
      "epoch no.1 train no.99350  loss = 3.79416 avg_loss = 3.59187\n",
      "epoch no.1 train no.99360  loss = 3.64361 avg_loss = 3.56154\n",
      "epoch no.1 train no.99370  loss = 3.31691 avg_loss = 3.58736\n",
      "epoch no.1 train no.99380  loss = 4.44275 avg_loss = 3.57581\n",
      "epoch no.1 train no.99390  loss = 2.85998 avg_loss = 3.58901\n",
      "epoch no.1 train no.99400  loss = 3.85954 avg_loss = 3.63101\n",
      "epoch no.1 train no.99410  loss = 5.16321 avg_loss = 3.63305\n",
      "epoch no.1 train no.99420  loss = 3.34834 avg_loss = 3.70390\n",
      "epoch no.1 train no.99430  loss = 3.65068 avg_loss = 3.63677\n",
      "epoch no.1 train no.99440  loss = 3.57650 avg_loss = 3.63112\n",
      "epoch no.1 train no.99450  loss = 3.79564 avg_loss = 3.59908\n",
      "epoch no.1 train no.99460  loss = 3.03556 avg_loss = 3.57415\n",
      "epoch no.1 train no.99470  loss = 3.37029 avg_loss = 3.58223\n",
      "epoch no.1 train no.99480  loss = 3.59120 avg_loss = 3.57761\n",
      "epoch no.1 train no.99490  loss = 4.89327 avg_loss = 3.60643\n",
      "epoch no.1 train no.99500  loss = 3.32076 avg_loss = 3.59670\n",
      "epoch no.1 train no.99510  loss = 2.81698 avg_loss = 3.62374\n",
      "epoch no.1 train no.99520  loss = 2.92749 avg_loss = 3.57089\n",
      "epoch no.1 train no.99530  loss = 5.29090 avg_loss = 3.57937\n",
      "epoch no.1 train no.99540  loss = 2.97211 avg_loss = 3.61920\n",
      "epoch no.1 train no.99550  loss = 3.99000 avg_loss = 3.63118\n",
      "epoch no.1 train no.99560  loss = 4.69106 avg_loss = 3.65128\n",
      "epoch no.1 train no.99570  loss = 4.04143 avg_loss = 3.62560\n",
      "epoch no.1 train no.99580  loss = 3.53007 avg_loss = 3.64208\n",
      "epoch no.1 train no.99590  loss = 2.05193 avg_loss = 3.62927\n",
      "epoch no.1 train no.99600  loss = 3.77225 avg_loss = 3.65400\n",
      "epoch no.1 train no.99610  loss = 2.55467 avg_loss = 3.60192\n",
      "epoch no.1 train no.99620  loss = 2.19567 avg_loss = 3.65679\n",
      "epoch no.1 train no.99630  loss = 2.73583 avg_loss = 3.63607\n",
      "epoch no.1 train no.99640  loss = 3.48511 avg_loss = 3.63427\n",
      "epoch no.1 train no.99650  loss = 5.68804 avg_loss = 3.59087\n",
      "epoch no.1 train no.99660  loss = 2.88598 avg_loss = 3.55301\n",
      "epoch no.1 train no.99670  loss = 2.96447 avg_loss = 3.56137\n",
      "epoch no.1 train no.99680  loss = 3.20162 avg_loss = 3.53017\n",
      "epoch no.1 train no.99690  loss = 3.78575 avg_loss = 3.51162\n",
      "epoch no.1 train no.99700  loss = 2.89135 avg_loss = 3.55760\n",
      "epoch no.1 train no.99710  loss = 3.44279 avg_loss = 3.54626\n",
      "epoch no.1 train no.99720  loss = 3.62985 avg_loss = 3.53856\n",
      "epoch no.1 train no.99730  loss = 3.91992 avg_loss = 3.53468\n",
      "epoch no.1 train no.99740  loss = 4.66795 avg_loss = 3.50773\n",
      "epoch no.1 train no.99750  loss = 5.95789 avg_loss = 3.52604\n",
      "epoch no.1 train no.99760  loss = 2.84589 avg_loss = 3.51225\n",
      "epoch no.1 train no.99770  loss = 5.51623 avg_loss = 3.58323\n",
      "epoch no.1 train no.99780  loss = 3.78394 avg_loss = 3.54033\n",
      "epoch no.1 train no.99790  loss = 3.08505 avg_loss = 3.55225\n",
      "epoch no.1 train no.99800  loss = 4.13021 avg_loss = 3.59672\n",
      "epoch no.1 train no.99810  loss = 3.74574 avg_loss = 3.56492\n",
      "epoch no.1 train no.99820  loss = 4.13089 avg_loss = 3.55143\n",
      "epoch no.1 train no.99830  loss = 6.60301 avg_loss = 3.57447\n",
      "epoch no.1 train no.99840  loss = 3.76264 avg_loss = 3.58723\n",
      "epoch no.1 train no.99850  loss = 3.58789 avg_loss = 3.62656\n",
      "epoch no.1 train no.99860  loss = 2.32465 avg_loss = 3.62171\n",
      "epoch no.1 train no.99870  loss = 4.30760 avg_loss = 3.65402\n",
      "epoch no.1 train no.99880  loss = 3.27222 avg_loss = 3.60537\n",
      "epoch no.1 train no.99890  loss = 5.95129 avg_loss = 3.65494\n",
      "epoch no.1 train no.99900  loss = 2.37915 avg_loss = 3.65826\n",
      "epoch no.1 train no.99910  loss = 3.76734 avg_loss = 3.65594\n",
      "epoch no.1 train no.99920  loss = 5.70900 avg_loss = 3.71018\n",
      "epoch no.1 train no.99930  loss = 3.69983 avg_loss = 3.67764\n",
      "epoch no.1 train no.99940  loss = 4.67651 avg_loss = 3.69377\n",
      "epoch no.1 train no.99950  loss = 5.12548 avg_loss = 3.67613\n",
      "epoch no.1 train no.99960  loss = 3.92792 avg_loss = 3.65113\n",
      "epoch no.1 train no.99970  loss = 1.72690 avg_loss = 3.64569\n",
      "epoch no.1 train no.99980  loss = 2.26059 avg_loss = 3.61546\n",
      "epoch no.1 train no.99990  loss = 4.10319 avg_loss = 3.61684\n",
      "epoch no.1 train no.100000  loss = 3.21920 avg_loss = 3.59002\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁발라', '▁모음', '</s>']\n",
      "추억의 90년대 가요 모음</s>\n",
      "epoch no.1 train no.100010  loss = 3.65701 avg_loss = 3.58270\n",
      "epoch no.1 train no.100020  loss = 3.72174 avg_loss = 3.63309\n",
      "epoch no.1 train no.100030  loss = 4.90199 avg_loss = 3.62859\n",
      "epoch no.1 train no.100040  loss = 2.22562 avg_loss = 3.62931\n",
      "epoch no.1 train no.100050  loss = 6.09959 avg_loss = 3.65922\n",
      "epoch no.1 train no.100060  loss = 3.85141 avg_loss = 3.63077\n",
      "epoch no.1 train no.100070  loss = 2.69637 avg_loss = 3.60277\n",
      "epoch no.1 train no.100080  loss = 3.67129 avg_loss = 3.66080\n",
      "epoch no.1 train no.100090  loss = 3.56395 avg_loss = 3.62216\n",
      "epoch no.1 train no.100100  loss = 2.47207 avg_loss = 3.56325\n",
      "epoch no.1 train no.100110  loss = 2.56637 avg_loss = 3.60085\n",
      "epoch no.1 train no.100120  loss = 2.89955 avg_loss = 3.59562\n",
      "epoch no.1 train no.100130  loss = 3.56364 avg_loss = 3.63068\n",
      "epoch no.1 train no.100140  loss = 4.63217 avg_loss = 3.61172\n",
      "epoch no.1 train no.100150  loss = 4.26927 avg_loss = 3.62110\n",
      "epoch no.1 train no.100160  loss = 1.51789 avg_loss = 3.58538\n",
      "epoch no.1 train no.100170  loss = 3.29801 avg_loss = 3.54283\n",
      "epoch no.1 train no.100180  loss = 1.61950 avg_loss = 3.57751\n",
      "epoch no.1 train no.100190  loss = 2.72040 avg_loss = 3.60735\n",
      "epoch no.1 train no.100200  loss = 1.87625 avg_loss = 3.57216\n",
      "epoch no.1 train no.100210  loss = 5.68765 avg_loss = 3.61983\n",
      "epoch no.1 train no.100220  loss = 5.79699 avg_loss = 3.64304\n",
      "epoch no.1 train no.100230  loss = 2.41495 avg_loss = 3.64130\n",
      "epoch no.1 train no.100240  loss = 4.22886 avg_loss = 3.63945\n",
      "epoch no.1 train no.100250  loss = 2.91477 avg_loss = 3.55025\n",
      "epoch no.1 train no.100260  loss = 5.46861 avg_loss = 3.52286\n",
      "epoch no.1 train no.100270  loss = 3.23285 avg_loss = 3.51253\n",
      "epoch no.1 train no.100280  loss = 3.17430 avg_loss = 3.56175\n",
      "epoch no.1 train no.100290  loss = 4.70574 avg_loss = 3.58290\n",
      "epoch no.1 train no.100300  loss = 3.79426 avg_loss = 3.62282\n",
      "epoch no.1 train no.100310  loss = 5.87972 avg_loss = 3.65137\n",
      "epoch no.1 train no.100320  loss = 3.72747 avg_loss = 3.61875\n",
      "epoch no.1 train no.100330  loss = 3.43851 avg_loss = 3.62715\n",
      "epoch no.1 train no.100340  loss = 4.46270 avg_loss = 3.64674\n",
      "epoch no.1 train no.100350  loss = 5.38771 avg_loss = 3.64951\n",
      "epoch no.1 train no.100360  loss = 3.41622 avg_loss = 3.62684\n",
      "epoch no.1 train no.100370  loss = 4.14832 avg_loss = 3.61209\n",
      "epoch no.1 train no.100380  loss = 5.61000 avg_loss = 3.63367\n",
      "epoch no.1 train no.100390  loss = 7.64057 avg_loss = 3.58278\n",
      "epoch no.1 train no.100400  loss = 4.42178 avg_loss = 3.62503\n",
      "epoch no.1 train no.100410  loss = 3.52700 avg_loss = 3.65230\n",
      "epoch no.1 train no.100420  loss = 3.74513 avg_loss = 3.68057\n",
      "epoch no.1 train no.100430  loss = 4.24166 avg_loss = 3.72800\n",
      "epoch no.1 train no.100440  loss = 2.13168 avg_loss = 3.70935\n",
      "epoch no.1 train no.100450  loss = 4.71599 avg_loss = 3.66287\n",
      "epoch no.1 train no.100460  loss = 3.39706 avg_loss = 3.69381\n",
      "epoch no.1 train no.100470  loss = 3.60495 avg_loss = 3.64849\n",
      "epoch no.1 train no.100480  loss = 2.89739 avg_loss = 3.67555\n",
      "epoch no.1 train no.100490  loss = 6.46261 avg_loss = 3.68407\n",
      "epoch no.1 train no.100500  loss = 2.39689 avg_loss = 3.70199\n",
      "epoch no.1 train no.100510  loss = 2.41022 avg_loss = 3.65586\n",
      "epoch no.1 train no.100520  loss = 3.86000 avg_loss = 3.63950\n",
      "epoch no.1 train no.100530  loss = 2.18860 avg_loss = 3.64388\n",
      "epoch no.1 train no.100540  loss = 4.74511 avg_loss = 3.66586\n",
      "epoch no.1 train no.100550  loss = 3.12815 avg_loss = 3.73901\n",
      "epoch no.1 train no.100560  loss = 3.78551 avg_loss = 3.77005\n",
      "epoch no.1 train no.100570  loss = 5.30690 avg_loss = 3.74688\n",
      "epoch no.1 train no.100580  loss = 3.55416 avg_loss = 3.79229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.100590  loss = 2.29724 avg_loss = 3.77395\n",
      "epoch no.1 train no.100600  loss = 3.54750 avg_loss = 3.80453\n",
      "epoch no.1 train no.100610  loss = 1.96158 avg_loss = 3.76445\n",
      "epoch no.1 train no.100620  loss = 2.08359 avg_loss = 3.81440\n",
      "epoch no.1 train no.100630  loss = 3.91566 avg_loss = 3.79256\n",
      "epoch no.1 train no.100640  loss = 7.12449 avg_loss = 3.81616\n",
      "epoch no.1 train no.100650  loss = 3.56850 avg_loss = 3.80340\n",
      "epoch no.1 train no.100660  loss = 3.03447 avg_loss = 3.73163\n",
      "epoch no.1 train no.100670  loss = 3.35040 avg_loss = 3.69685\n",
      "epoch no.1 train no.100680  loss = 4.11948 avg_loss = 3.72200\n",
      "epoch no.1 train no.100690  loss = 5.58865 avg_loss = 3.66890\n",
      "epoch no.1 train no.100700  loss = 3.88804 avg_loss = 3.64833\n",
      "epoch no.1 train no.100710  loss = 3.31784 avg_loss = 3.65264\n",
      "epoch no.1 train no.100720  loss = 6.65863 avg_loss = 3.70644\n",
      "epoch no.1 train no.100730  loss = 2.63539 avg_loss = 3.66112\n",
      "epoch no.1 train no.100740  loss = 5.00615 avg_loss = 3.72308\n",
      "epoch no.1 train no.100750  loss = 2.87485 avg_loss = 3.70966\n",
      "epoch no.1 train no.100760  loss = 2.24387 avg_loss = 3.68250\n",
      "epoch no.1 train no.100770  loss = 3.33348 avg_loss = 3.65917\n",
      "epoch no.1 train no.100780  loss = 3.39949 avg_loss = 3.65528\n",
      "epoch no.1 train no.100790  loss = 3.71723 avg_loss = 3.72356\n",
      "epoch no.1 train no.100800  loss = 2.95990 avg_loss = 3.75761\n",
      "epoch no.1 train no.100810  loss = 3.94473 avg_loss = 3.79715\n",
      "epoch no.1 train no.100820  loss = 4.04939 avg_loss = 3.80180\n",
      "epoch no.1 train no.100830  loss = 4.37637 avg_loss = 3.79280\n",
      "epoch no.1 train no.100840  loss = 3.34727 avg_loss = 3.84981\n",
      "epoch no.1 train no.100850  loss = 2.92538 avg_loss = 3.80798\n",
      "epoch no.1 train no.100860  loss = 3.32180 avg_loss = 3.79822\n",
      "epoch no.1 train no.100870  loss = 3.39386 avg_loss = 3.80684\n",
      "epoch no.1 train no.100880  loss = 4.07949 avg_loss = 3.78310\n",
      "epoch no.1 train no.100890  loss = 4.05197 avg_loss = 3.75328\n",
      "epoch no.1 train no.100900  loss = 3.56102 avg_loss = 3.68093\n",
      "epoch no.1 train no.100910  loss = 2.93663 avg_loss = 3.66284\n",
      "epoch no.1 train no.100920  loss = 2.71512 avg_loss = 3.61006\n",
      "epoch no.1 train no.100930  loss = 4.17781 avg_loss = 3.60611\n",
      "epoch no.1 train no.100940  loss = 4.17676 avg_loss = 3.61586\n",
      "epoch no.1 train no.100950  loss = 2.71304 avg_loss = 3.59701\n",
      "epoch no.1 train no.100960  loss = 2.74496 avg_loss = 3.56453\n",
      "epoch no.1 train no.100970  loss = 3.95302 avg_loss = 3.57615\n",
      "epoch no.1 train no.100980  loss = 2.32587 avg_loss = 3.57793\n",
      "epoch no.1 train no.100990  loss = 3.26828 avg_loss = 3.54134\n",
      "epoch no.1 train no.101000  loss = 4.73479 avg_loss = 3.55884\n",
      "3\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁모음', '</s>']\n",
      "추억의 발라드 모음</s>\n",
      "epoch no.1 train no.101010  loss = 3.90359 avg_loss = 3.61145\n",
      "epoch no.1 train no.101020  loss = 3.97824 avg_loss = 3.58544\n",
      "epoch no.1 train no.101030  loss = 6.83179 avg_loss = 3.65097\n",
      "epoch no.1 train no.101040  loss = 3.46375 avg_loss = 3.62446\n",
      "epoch no.1 train no.101050  loss = 4.13722 avg_loss = 3.63714\n",
      "epoch no.1 train no.101060  loss = 2.93411 avg_loss = 3.69869\n",
      "epoch no.1 train no.101070  loss = 2.93177 avg_loss = 3.73780\n",
      "epoch no.1 train no.101080  loss = 4.12487 avg_loss = 3.69196\n",
      "epoch no.1 train no.101090  loss = 2.66753 avg_loss = 3.65954\n",
      "epoch no.1 train no.101100  loss = 2.44512 avg_loss = 3.65368\n",
      "epoch no.1 train no.101110  loss = 3.69749 avg_loss = 3.69855\n",
      "epoch no.1 train no.101120  loss = 4.81974 avg_loss = 3.71824\n",
      "epoch no.1 train no.101130  loss = 3.19263 avg_loss = 3.70699\n",
      "epoch no.1 train no.101140  loss = 2.99262 avg_loss = 3.70046\n",
      "epoch no.1 train no.101150  loss = 2.91251 avg_loss = 3.68579\n",
      "epoch no.1 train no.101160  loss = 1.98153 avg_loss = 3.67750\n",
      "epoch no.1 train no.101170  loss = 4.87427 avg_loss = 3.69579\n",
      "epoch no.1 train no.101180  loss = 2.46830 avg_loss = 3.73755\n",
      "epoch no.1 train no.101190  loss = 3.96142 avg_loss = 3.68605\n",
      "epoch no.1 train no.101200  loss = 2.75803 avg_loss = 3.67045\n",
      "epoch no.1 train no.101210  loss = 3.96947 avg_loss = 3.68338\n",
      "epoch no.1 train no.101220  loss = 6.07441 avg_loss = 3.68639\n",
      "epoch no.1 train no.101230  loss = 3.94798 avg_loss = 3.66560\n",
      "epoch no.1 train no.101240  loss = 1.49786 avg_loss = 3.67887\n",
      "epoch no.1 train no.101250  loss = 3.22753 avg_loss = 3.70109\n",
      "epoch no.1 train no.101260  loss = 3.29607 avg_loss = 3.75357\n",
      "epoch no.1 train no.101270  loss = 3.87584 avg_loss = 3.79707\n",
      "epoch no.1 train no.101280  loss = 3.46517 avg_loss = 3.77624\n",
      "epoch no.1 train no.101290  loss = 2.36321 avg_loss = 3.72600\n",
      "epoch no.1 train no.101300  loss = 4.87418 avg_loss = 3.73791\n",
      "epoch no.1 train no.101310  loss = 4.11021 avg_loss = 3.73559\n",
      "epoch no.1 train no.101320  loss = 5.02592 avg_loss = 3.73662\n",
      "epoch no.1 train no.101330  loss = 3.75230 avg_loss = 3.75067\n",
      "epoch no.1 train no.101340  loss = 3.17440 avg_loss = 3.71258\n",
      "epoch no.1 train no.101350  loss = 2.79574 avg_loss = 3.71994\n",
      "epoch no.1 train no.101360  loss = 5.71541 avg_loss = 3.71344\n",
      "epoch no.1 train no.101370  loss = 5.58222 avg_loss = 3.73601\n",
      "epoch no.1 train no.101380  loss = 4.65362 avg_loss = 3.76405\n",
      "epoch no.1 train no.101390  loss = 4.79234 avg_loss = 3.77835\n",
      "epoch no.1 train no.101400  loss = 4.28405 avg_loss = 3.81121\n",
      "epoch no.1 train no.101410  loss = 2.42935 avg_loss = 3.75778\n",
      "epoch no.1 train no.101420  loss = 3.87576 avg_loss = 3.78038\n",
      "epoch no.1 train no.101430  loss = 4.34115 avg_loss = 3.74092\n",
      "epoch no.1 train no.101440  loss = 3.48703 avg_loss = 3.71687\n",
      "epoch no.1 train no.101450  loss = 4.53106 avg_loss = 3.69626\n",
      "epoch no.1 train no.101460  loss = 3.76289 avg_loss = 3.69271\n",
      "epoch no.1 train no.101470  loss = 4.87751 avg_loss = 3.78788\n",
      "epoch no.1 train no.101480  loss = 4.16118 avg_loss = 3.74131\n",
      "epoch no.1 train no.101490  loss = 2.21033 avg_loss = 3.75624\n",
      "epoch no.1 train no.101500  loss = 3.42227 avg_loss = 3.75077\n",
      "epoch no.1 train no.101510  loss = 2.23092 avg_loss = 3.74068\n",
      "epoch no.1 train no.101520  loss = 5.37797 avg_loss = 3.75902\n",
      "epoch no.1 train no.101530  loss = 3.35455 avg_loss = 3.76092\n",
      "epoch no.1 train no.101540  loss = 4.73692 avg_loss = 3.74545\n",
      "epoch no.1 train no.101550  loss = 2.50866 avg_loss = 3.69149\n",
      "epoch no.1 train no.101560  loss = 1.93233 avg_loss = 3.67094\n",
      "epoch no.1 train no.101570  loss = 5.07964 avg_loss = 3.69982\n",
      "epoch no.1 train no.101580  loss = 2.14113 avg_loss = 3.70078\n",
      "epoch no.1 train no.101590  loss = 2.72552 avg_loss = 3.70947\n",
      "epoch no.1 train no.101600  loss = 3.72860 avg_loss = 3.65248\n",
      "epoch no.1 train no.101610  loss = 2.42260 avg_loss = 3.61213\n",
      "epoch no.1 train no.101620  loss = 4.16295 avg_loss = 3.57552\n",
      "epoch no.1 train no.101630  loss = 3.75898 avg_loss = 3.56291\n",
      "epoch no.1 train no.101640  loss = 3.58274 avg_loss = 3.56730\n",
      "epoch no.1 train no.101650  loss = 3.38857 avg_loss = 3.58517\n",
      "epoch no.1 train no.101660  loss = 3.33211 avg_loss = 3.55490\n",
      "epoch no.1 train no.101670  loss = 3.96368 avg_loss = 3.52714\n",
      "epoch no.1 train no.101680  loss = 3.19179 avg_loss = 3.53663\n",
      "epoch no.1 train no.101690  loss = 2.90963 avg_loss = 3.60478\n",
      "epoch no.1 train no.101700  loss = 4.30221 avg_loss = 3.61346\n",
      "epoch no.1 train no.101710  loss = 3.45697 avg_loss = 3.63255\n",
      "epoch no.1 train no.101720  loss = 5.34406 avg_loss = 3.68039\n",
      "epoch no.1 train no.101730  loss = 3.76902 avg_loss = 3.69658\n",
      "epoch no.1 train no.101740  loss = 2.00820 avg_loss = 3.66899\n",
      "epoch no.1 train no.101750  loss = 4.55095 avg_loss = 3.64603\n",
      "epoch no.1 train no.101760  loss = 4.30460 avg_loss = 3.67663\n",
      "epoch no.1 train no.101770  loss = 1.37165 avg_loss = 3.63513\n",
      "epoch no.1 train no.101780  loss = 2.93031 avg_loss = 3.63908\n",
      "epoch no.1 train no.101790  loss = 4.71713 avg_loss = 3.64645\n",
      "epoch no.1 train no.101800  loss = 3.06205 avg_loss = 3.64204\n",
      "epoch no.1 train no.101810  loss = 4.67014 avg_loss = 3.62577\n",
      "epoch no.1 train no.101820  loss = 5.24434 avg_loss = 3.60355\n",
      "epoch no.1 train no.101830  loss = 5.46443 avg_loss = 3.57716\n",
      "epoch no.1 train no.101840  loss = 2.94329 avg_loss = 3.57281\n",
      "epoch no.1 train no.101850  loss = 2.80297 avg_loss = 3.63254\n",
      "epoch no.1 train no.101860  loss = 4.25216 avg_loss = 3.63825\n",
      "epoch no.1 train no.101870  loss = 3.33864 avg_loss = 3.67376\n",
      "epoch no.1 train no.101880  loss = 2.88137 avg_loss = 3.67113\n",
      "epoch no.1 train no.101890  loss = 2.21639 avg_loss = 3.59853\n",
      "epoch no.1 train no.101900  loss = 2.49237 avg_loss = 3.53957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.101910  loss = 3.08398 avg_loss = 3.52857\n",
      "epoch no.1 train no.101920  loss = 3.26006 avg_loss = 3.50165\n",
      "epoch no.1 train no.101930  loss = 2.83196 avg_loss = 3.48744\n",
      "epoch no.1 train no.101940  loss = 2.47055 avg_loss = 3.52855\n",
      "epoch no.1 train no.101950  loss = 3.96696 avg_loss = 3.50913\n",
      "epoch no.1 train no.101960  loss = 6.62572 avg_loss = 3.56855\n",
      "epoch no.1 train no.101970  loss = 5.77084 avg_loss = 3.59522\n",
      "epoch no.1 train no.101980  loss = 4.99945 avg_loss = 3.62286\n",
      "epoch no.1 train no.101990  loss = 4.35116 avg_loss = 3.60609\n",
      "epoch no.1 train no.102000  loss = 2.42198 avg_loss = 3.57504\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '곡', '들', '곡', '</s>']\n",
      "추억의 명곡 리메이크곡</s>\n",
      "epoch no.1 train no.102010  loss = 3.30670 avg_loss = 3.57605\n",
      "epoch no.1 train no.102020  loss = 4.97593 avg_loss = 3.64944\n",
      "epoch no.1 train no.102030  loss = 5.68038 avg_loss = 3.69123\n",
      "epoch no.1 train no.102040  loss = 6.18914 avg_loss = 3.81081\n",
      "epoch no.1 train no.102050  loss = 2.22508 avg_loss = 3.74107\n",
      "epoch no.1 train no.102060  loss = 5.52959 avg_loss = 3.74413\n",
      "epoch no.1 train no.102070  loss = 4.08096 avg_loss = 3.78295\n",
      "epoch no.1 train no.102080  loss = 3.33906 avg_loss = 3.79773\n",
      "epoch no.1 train no.102090  loss = 4.65107 avg_loss = 3.80182\n",
      "epoch no.1 train no.102100  loss = 3.40205 avg_loss = 3.76596\n",
      "epoch no.1 train no.102110  loss = 3.15874 avg_loss = 3.74200\n",
      "epoch no.1 train no.102120  loss = 3.14193 avg_loss = 3.72691\n",
      "epoch no.1 train no.102130  loss = 6.04084 avg_loss = 3.70625\n",
      "epoch no.1 train no.102140  loss = 4.35806 avg_loss = 3.76666\n",
      "epoch no.1 train no.102150  loss = 3.46922 avg_loss = 3.81604\n",
      "epoch no.1 train no.102160  loss = 4.26674 avg_loss = 3.83031\n",
      "epoch no.1 train no.102170  loss = 3.09109 avg_loss = 3.80448\n",
      "epoch no.1 train no.102180  loss = 2.66644 avg_loss = 3.72479\n",
      "epoch no.1 train no.102190  loss = 3.65260 avg_loss = 3.75643\n",
      "epoch no.1 train no.102200  loss = 4.96513 avg_loss = 3.80180\n",
      "epoch no.1 train no.102210  loss = 3.69286 avg_loss = 3.75915\n",
      "epoch no.1 train no.102220  loss = 3.72451 avg_loss = 3.74172\n",
      "epoch no.1 train no.102230  loss = 4.65845 avg_loss = 3.67680\n",
      "epoch no.1 train no.102240  loss = 3.84959 avg_loss = 3.67630\n",
      "epoch no.1 train no.102250  loss = 1.70272 avg_loss = 3.63703\n",
      "epoch no.1 train no.102260  loss = 3.31729 avg_loss = 3.66792\n",
      "epoch no.1 train no.102270  loss = 2.26711 avg_loss = 3.61938\n",
      "epoch no.1 train no.102280  loss = 3.92339 avg_loss = 3.63879\n",
      "epoch no.1 train no.102290  loss = 4.23902 avg_loss = 3.62251\n",
      "epoch no.1 train no.102300  loss = 3.35862 avg_loss = 3.60091\n",
      "epoch no.1 train no.102310  loss = 3.93991 avg_loss = 3.57943\n",
      "epoch no.1 train no.102320  loss = 3.25718 avg_loss = 3.58891\n",
      "epoch no.1 train no.102330  loss = 4.05880 avg_loss = 3.56928\n",
      "epoch no.1 train no.102340  loss = 4.27977 avg_loss = 3.58631\n",
      "epoch no.1 train no.102350  loss = 3.16475 avg_loss = 3.61336\n",
      "epoch no.1 train no.102360  loss = 3.57512 avg_loss = 3.63855\n",
      "epoch no.1 train no.102370  loss = 4.51417 avg_loss = 3.67099\n",
      "epoch no.1 train no.102380  loss = 5.35607 avg_loss = 3.67367\n",
      "epoch no.1 train no.102390  loss = 4.33437 avg_loss = 3.68788\n",
      "epoch no.1 train no.102400  loss = 6.54678 avg_loss = 3.71110\n",
      "epoch no.1 train no.102410  loss = 2.70723 avg_loss = 3.69673\n",
      "epoch no.1 train no.102420  loss = 2.94594 avg_loss = 3.74740\n",
      "epoch no.1 train no.102430  loss = 3.54690 avg_loss = 3.79487\n",
      "epoch no.1 train no.102440  loss = 4.26168 avg_loss = 3.83329\n",
      "epoch no.1 train no.102450  loss = 2.17081 avg_loss = 3.81614\n",
      "epoch no.1 train no.102460  loss = 3.28204 avg_loss = 3.82162\n",
      "epoch no.1 train no.102470  loss = 4.36096 avg_loss = 3.74091\n",
      "epoch no.1 train no.102480  loss = 5.18527 avg_loss = 3.81817\n",
      "epoch no.1 train no.102490  loss = 3.77875 avg_loss = 3.78171\n",
      "epoch no.1 train no.102500  loss = 2.92968 avg_loss = 3.74442\n",
      "epoch no.1 train no.102510  loss = 3.44668 avg_loss = 3.70595\n",
      "epoch no.1 train no.102520  loss = 4.24873 avg_loss = 3.69111\n",
      "epoch no.1 train no.102530  loss = 3.32824 avg_loss = 3.65481\n",
      "epoch no.1 train no.102540  loss = 3.24624 avg_loss = 3.63799\n",
      "epoch no.1 train no.102550  loss = 4.26736 avg_loss = 3.63668\n",
      "epoch no.1 train no.102560  loss = 5.22162 avg_loss = 3.68472\n",
      "epoch no.1 train no.102570  loss = 2.41627 avg_loss = 3.71186\n",
      "epoch no.1 train no.102580  loss = 3.26345 avg_loss = 3.67554\n",
      "epoch no.1 train no.102590  loss = 4.73524 avg_loss = 3.71290\n",
      "epoch no.1 train no.102600  loss = 3.48635 avg_loss = 3.70828\n",
      "epoch no.1 train no.102610  loss = 3.23964 avg_loss = 3.71032\n",
      "epoch no.1 train no.102620  loss = 5.61477 avg_loss = 3.71241\n",
      "epoch no.1 train no.102630  loss = 3.04470 avg_loss = 3.69939\n",
      "epoch no.1 train no.102640  loss = 2.23807 avg_loss = 3.68320\n",
      "epoch no.1 train no.102650  loss = 2.02513 avg_loss = 3.67253\n",
      "epoch no.1 train no.102660  loss = 3.73498 avg_loss = 3.64151\n",
      "epoch no.1 train no.102670  loss = 2.49954 avg_loss = 3.63601\n",
      "epoch no.1 train no.102680  loss = 3.39176 avg_loss = 3.59363\n",
      "epoch no.1 train no.102690  loss = 2.48201 avg_loss = 3.58818\n",
      "epoch no.1 train no.102700  loss = 5.60124 avg_loss = 3.67087\n",
      "epoch no.1 train no.102710  loss = 2.82602 avg_loss = 3.69801\n",
      "epoch no.1 train no.102720  loss = 2.94766 avg_loss = 3.67341\n",
      "epoch no.1 train no.102730  loss = 5.37633 avg_loss = 3.69705\n",
      "epoch no.1 train no.102740  loss = 3.50087 avg_loss = 3.67175\n",
      "epoch no.1 train no.102750  loss = 2.29821 avg_loss = 3.66878\n",
      "epoch no.1 train no.102760  loss = 2.69636 avg_loss = 3.63351\n",
      "epoch no.1 train no.102770  loss = 4.32747 avg_loss = 3.62132\n",
      "epoch no.1 train no.102780  loss = 2.63886 avg_loss = 3.57240\n",
      "epoch no.1 train no.102790  loss = 4.38186 avg_loss = 3.54568\n",
      "epoch no.1 train no.102800  loss = 2.21639 avg_loss = 3.55187\n",
      "epoch no.1 train no.102810  loss = 5.58376 avg_loss = 3.56987\n",
      "epoch no.1 train no.102820  loss = 3.88436 avg_loss = 3.59230\n",
      "epoch no.1 train no.102830  loss = 3.34064 avg_loss = 3.58371\n",
      "epoch no.1 train no.102840  loss = 3.58851 avg_loss = 3.54165\n",
      "epoch no.1 train no.102850  loss = 2.68590 avg_loss = 3.52307\n",
      "epoch no.1 train no.102860  loss = 4.79561 avg_loss = 3.52528\n",
      "epoch no.1 train no.102870  loss = 2.00562 avg_loss = 3.54450\n",
      "epoch no.1 train no.102880  loss = 2.36669 avg_loss = 3.54056\n",
      "epoch no.1 train no.102890  loss = 4.17116 avg_loss = 3.55304\n",
      "epoch no.1 train no.102900  loss = 2.46601 avg_loss = 3.56779\n",
      "epoch no.1 train no.102910  loss = 3.16020 avg_loss = 3.57261\n",
      "epoch no.1 train no.102920  loss = 3.32132 avg_loss = 3.59277\n",
      "epoch no.1 train no.102930  loss = 2.72929 avg_loss = 3.59926\n",
      "epoch no.1 train no.102940  loss = 3.12052 avg_loss = 3.65263\n",
      "epoch no.1 train no.102950  loss = 3.34347 avg_loss = 3.67071\n",
      "epoch no.1 train no.102960  loss = 4.08946 avg_loss = 3.67170\n",
      "epoch no.1 train no.102970  loss = 5.03061 avg_loss = 3.73251\n",
      "epoch no.1 train no.102980  loss = 2.92291 avg_loss = 3.75570\n",
      "epoch no.1 train no.102990  loss = 4.02607 avg_loss = 3.78852\n",
      "epoch no.1 train no.103000  loss = 3.46091 avg_loss = 3.78077\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '곡', '▁모음', '음', '</s>']\n",
      "추억의 댄스곡모음</s>\n",
      "epoch no.1 train no.103010  loss = 2.04196 avg_loss = 3.70639\n",
      "epoch no.1 train no.103020  loss = 4.85769 avg_loss = 3.72242\n",
      "epoch no.1 train no.103030  loss = 4.71116 avg_loss = 3.76041\n",
      "epoch no.1 train no.103040  loss = 5.12645 avg_loss = 3.75004\n",
      "epoch no.1 train no.103050  loss = 3.39199 avg_loss = 3.71708\n",
      "epoch no.1 train no.103060  loss = 2.05519 avg_loss = 3.70051\n",
      "epoch no.1 train no.103070  loss = 4.41045 avg_loss = 3.77369\n",
      "epoch no.1 train no.103080  loss = 2.83966 avg_loss = 3.74103\n",
      "epoch no.1 train no.103090  loss = 2.06061 avg_loss = 3.69025\n",
      "epoch no.1 train no.103100  loss = 2.94853 avg_loss = 3.72334\n",
      "epoch no.1 train no.103110  loss = 6.12526 avg_loss = 3.72277\n",
      "epoch no.1 train no.103120  loss = 2.17766 avg_loss = 3.70694\n",
      "epoch no.1 train no.103130  loss = 3.63768 avg_loss = 3.67976\n",
      "epoch no.1 train no.103140  loss = 3.12187 avg_loss = 3.68031\n",
      "epoch no.1 train no.103150  loss = 3.08775 avg_loss = 3.63878\n",
      "epoch no.1 train no.103160  loss = 6.26140 avg_loss = 3.66194\n",
      "epoch no.1 train no.103170  loss = 2.42980 avg_loss = 3.59062\n",
      "epoch no.1 train no.103180  loss = 2.29679 avg_loss = 3.61480\n",
      "epoch no.1 train no.103190  loss = 3.75439 avg_loss = 3.62348\n",
      "epoch no.1 train no.103200  loss = 3.39462 avg_loss = 3.61346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.103210  loss = 2.91414 avg_loss = 3.58247\n",
      "epoch no.1 train no.103220  loss = 3.04663 avg_loss = 3.53803\n",
      "epoch no.1 train no.103230  loss = 2.32604 avg_loss = 3.57494\n",
      "epoch no.1 train no.103240  loss = 5.13066 avg_loss = 3.61235\n",
      "epoch no.1 train no.103250  loss = 4.53252 avg_loss = 3.56890\n",
      "epoch no.1 train no.103260  loss = 2.23156 avg_loss = 3.58366\n",
      "epoch no.1 train no.103270  loss = 4.93167 avg_loss = 3.59424\n",
      "epoch no.1 train no.103280  loss = 4.45999 avg_loss = 3.58707\n",
      "epoch no.1 train no.103290  loss = 4.18958 avg_loss = 3.62749\n",
      "epoch no.1 train no.103300  loss = 5.40506 avg_loss = 3.68926\n",
      "epoch no.1 train no.103310  loss = 3.22316 avg_loss = 3.64316\n",
      "epoch no.1 train no.103320  loss = 3.15123 avg_loss = 3.63752\n",
      "epoch no.1 train no.103330  loss = 2.32852 avg_loss = 3.60959\n",
      "epoch no.1 train no.103340  loss = 2.68842 avg_loss = 3.57943\n",
      "epoch no.1 train no.103350  loss = 6.46385 avg_loss = 3.65345\n",
      "epoch no.1 train no.103360  loss = 4.19512 avg_loss = 3.67180\n",
      "epoch no.1 train no.103370  loss = 2.35806 avg_loss = 3.66307\n",
      "epoch no.1 train no.103380  loss = 5.43264 avg_loss = 3.64769\n",
      "epoch no.1 train no.103390  loss = 2.99659 avg_loss = 3.64081\n",
      "epoch no.1 train no.103400  loss = 2.16579 avg_loss = 3.68264\n",
      "epoch no.1 train no.103410  loss = 3.03345 avg_loss = 3.66507\n",
      "epoch no.1 train no.103420  loss = 5.97460 avg_loss = 3.66359\n",
      "epoch no.1 train no.103430  loss = 4.08456 avg_loss = 3.66933\n",
      "epoch no.1 train no.103440  loss = 6.46791 avg_loss = 3.69119\n",
      "epoch no.1 train no.103450  loss = 2.13817 avg_loss = 3.63339\n",
      "epoch no.1 train no.103460  loss = 4.01813 avg_loss = 3.63047\n",
      "epoch no.1 train no.103470  loss = 3.07425 avg_loss = 3.61629\n",
      "epoch no.1 train no.103480  loss = 2.80926 avg_loss = 3.63490\n",
      "epoch no.1 train no.103490  loss = 2.51845 avg_loss = 3.62426\n",
      "epoch no.1 train no.103500  loss = 4.12327 avg_loss = 3.61068\n",
      "epoch no.1 train no.103510  loss = 5.62275 avg_loss = 3.61520\n",
      "epoch no.1 train no.103520  loss = 2.98314 avg_loss = 3.60650\n",
      "epoch no.1 train no.103530  loss = 2.74464 avg_loss = 3.59727\n",
      "epoch no.1 train no.103540  loss = 3.81579 avg_loss = 3.62209\n",
      "epoch no.1 train no.103550  loss = 4.17708 avg_loss = 3.63448\n",
      "epoch no.1 train no.103560  loss = 3.76344 avg_loss = 3.62688\n",
      "epoch no.1 train no.103570  loss = 3.47465 avg_loss = 3.61506\n",
      "epoch no.1 train no.103580  loss = 3.18167 avg_loss = 3.63101\n",
      "epoch no.1 train no.103590  loss = 2.46581 avg_loss = 3.60128\n",
      "epoch no.1 train no.103600  loss = 2.33977 avg_loss = 3.59445\n",
      "epoch no.1 train no.103610  loss = 3.60207 avg_loss = 3.65974\n",
      "epoch no.1 train no.103620  loss = 5.47309 avg_loss = 3.69968\n",
      "epoch no.1 train no.103630  loss = 3.20764 avg_loss = 3.68377\n",
      "epoch no.1 train no.103640  loss = 3.44099 avg_loss = 3.70245\n",
      "epoch no.1 train no.103650  loss = 3.74771 avg_loss = 3.73468\n",
      "epoch no.1 train no.103660  loss = 2.96550 avg_loss = 3.69730\n",
      "epoch no.1 train no.103670  loss = 3.15670 avg_loss = 3.70295\n",
      "epoch no.1 train no.103680  loss = 3.98409 avg_loss = 3.65422\n",
      "epoch no.1 train no.103690  loss = 3.70417 avg_loss = 3.59143\n",
      "epoch no.1 train no.103700  loss = 5.73748 avg_loss = 3.61922\n",
      "epoch no.1 train no.103710  loss = 3.23269 avg_loss = 3.56526\n",
      "epoch no.1 train no.103720  loss = 3.27614 avg_loss = 3.50122\n",
      "epoch no.1 train no.103730  loss = 3.74799 avg_loss = 3.53304\n",
      "epoch no.1 train no.103740  loss = 3.28100 avg_loss = 3.56728\n",
      "epoch no.1 train no.103750  loss = 2.12009 avg_loss = 3.50068\n",
      "epoch no.1 train no.103760  loss = 2.51818 avg_loss = 3.47586\n",
      "epoch no.1 train no.103770  loss = 1.54279 avg_loss = 3.47522\n",
      "epoch no.1 train no.103780  loss = 1.50350 avg_loss = 3.45030\n",
      "epoch no.1 train no.103790  loss = 5.46972 avg_loss = 3.44372\n",
      "epoch no.1 train no.103800  loss = 2.31804 avg_loss = 3.44047\n",
      "epoch no.1 train no.103810  loss = 3.74567 avg_loss = 3.50651\n",
      "epoch no.1 train no.103820  loss = 3.71075 avg_loss = 3.49850\n",
      "epoch no.1 train no.103830  loss = 3.46917 avg_loss = 3.53726\n",
      "epoch no.1 train no.103840  loss = 2.29210 avg_loss = 3.52505\n",
      "epoch no.1 train no.103850  loss = 3.12813 avg_loss = 3.58671\n",
      "epoch no.1 train no.103860  loss = 2.96178 avg_loss = 3.55906\n",
      "epoch no.1 train no.103870  loss = 3.39769 avg_loss = 3.60507\n",
      "epoch no.1 train no.103880  loss = 2.37522 avg_loss = 3.55038\n",
      "epoch no.1 train no.103890  loss = 2.44794 avg_loss = 3.57678\n",
      "epoch no.1 train no.103900  loss = 3.85664 avg_loss = 3.54657\n",
      "epoch no.1 train no.103910  loss = 3.20939 avg_loss = 3.53430\n",
      "epoch no.1 train no.103920  loss = 2.43849 avg_loss = 3.59802\n",
      "epoch no.1 train no.103930  loss = 3.50674 avg_loss = 3.56343\n",
      "epoch no.1 train no.103940  loss = 2.50453 avg_loss = 3.53978\n",
      "epoch no.1 train no.103950  loss = 3.18593 avg_loss = 3.55493\n",
      "epoch no.1 train no.103960  loss = 3.27019 avg_loss = 3.53583\n",
      "epoch no.1 train no.103970  loss = 2.59503 avg_loss = 3.53686\n",
      "epoch no.1 train no.103980  loss = 4.41671 avg_loss = 3.54085\n",
      "epoch no.1 train no.103990  loss = 5.14384 avg_loss = 3.52795\n",
      "epoch no.1 train no.104000  loss = 3.03515 avg_loss = 3.52615\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '년대', '▁발라', '▁가요', '</s>']\n",
      "추억의 90년대 인기팝</s>\n",
      "epoch no.1 train no.104010  loss = 3.38703 avg_loss = 3.57008\n",
      "epoch no.1 train no.104020  loss = 2.25837 avg_loss = 3.55758\n",
      "epoch no.1 train no.104030  loss = 3.06008 avg_loss = 3.59402\n",
      "epoch no.1 train no.104040  loss = 4.35076 avg_loss = 3.60358\n",
      "epoch no.1 train no.104050  loss = 2.59152 avg_loss = 3.57847\n",
      "epoch no.1 train no.104060  loss = 4.39263 avg_loss = 3.60900\n",
      "epoch no.1 train no.104070  loss = 4.03997 avg_loss = 3.59861\n",
      "epoch no.1 train no.104080  loss = 3.24134 avg_loss = 3.64156\n",
      "epoch no.1 train no.104090  loss = 3.95296 avg_loss = 3.65240\n",
      "epoch no.1 train no.104100  loss = 4.83558 avg_loss = 3.67810\n",
      "epoch no.1 train no.104110  loss = 4.15388 avg_loss = 3.70017\n",
      "epoch no.1 train no.104120  loss = 2.39437 avg_loss = 3.66811\n",
      "epoch no.1 train no.104130  loss = 2.84847 avg_loss = 3.64261\n",
      "epoch no.1 train no.104140  loss = 3.89413 avg_loss = 3.62910\n",
      "epoch no.1 train no.104150  loss = 4.97438 avg_loss = 3.67674\n",
      "epoch no.1 train no.104160  loss = 5.02783 avg_loss = 3.66681\n",
      "epoch no.1 train no.104170  loss = 5.92339 avg_loss = 3.63799\n",
      "epoch no.1 train no.104180  loss = 2.34620 avg_loss = 3.62138\n",
      "epoch no.1 train no.104190  loss = 3.79532 avg_loss = 3.63227\n",
      "epoch no.1 train no.104200  loss = 4.39273 avg_loss = 3.63130\n",
      "epoch no.1 train no.104210  loss = 3.40043 avg_loss = 3.60410\n",
      "epoch no.1 train no.104220  loss = 1.98242 avg_loss = 3.60771\n",
      "epoch no.1 train no.104230  loss = 3.22318 avg_loss = 3.58343\n",
      "epoch no.1 train no.104240  loss = 4.62436 avg_loss = 3.63947\n",
      "epoch no.1 train no.104250  loss = 5.53454 avg_loss = 3.67174\n",
      "epoch no.1 train no.104260  loss = 3.38558 avg_loss = 3.64274\n",
      "epoch no.1 train no.104270  loss = 3.11086 avg_loss = 3.64003\n",
      "epoch no.1 train no.104280  loss = 2.60370 avg_loss = 3.62327\n",
      "epoch no.1 train no.104290  loss = 3.47934 avg_loss = 3.61265\n",
      "epoch no.1 train no.104300  loss = 3.08958 avg_loss = 3.60176\n",
      "epoch no.1 train no.104310  loss = 2.19600 avg_loss = 3.64636\n",
      "epoch no.1 train no.104320  loss = 2.46570 avg_loss = 3.58756\n",
      "epoch no.1 train no.104330  loss = 3.16257 avg_loss = 3.58848\n",
      "epoch no.1 train no.104340  loss = 3.47910 avg_loss = 3.56111\n",
      "epoch no.1 train no.104350  loss = 4.26639 avg_loss = 3.54529\n",
      "epoch no.1 train no.104360  loss = 3.15091 avg_loss = 3.52266\n",
      "epoch no.1 train no.104370  loss = 2.84715 avg_loss = 3.50260\n",
      "epoch no.1 train no.104380  loss = 3.85369 avg_loss = 3.51866\n",
      "epoch no.1 train no.104390  loss = 3.54966 avg_loss = 3.56881\n",
      "epoch no.1 train no.104400  loss = 2.77225 avg_loss = 3.51997\n",
      "epoch no.1 train no.104410  loss = 3.33779 avg_loss = 3.54968\n",
      "epoch no.1 train no.104420  loss = 3.91724 avg_loss = 3.60362\n",
      "epoch no.1 train no.104430  loss = 3.02916 avg_loss = 3.59674\n",
      "epoch no.1 train no.104440  loss = 4.38896 avg_loss = 3.63192\n",
      "epoch no.1 train no.104450  loss = 1.07007 avg_loss = 3.57390\n",
      "epoch no.1 train no.104460  loss = 1.98752 avg_loss = 3.55592\n",
      "epoch no.1 train no.104470  loss = 4.57445 avg_loss = 3.59169\n",
      "epoch no.1 train no.104480  loss = 2.12214 avg_loss = 3.55281\n",
      "epoch no.1 train no.104490  loss = 4.36628 avg_loss = 3.58303\n",
      "epoch no.1 train no.104500  loss = 2.54174 avg_loss = 3.53753\n",
      "epoch no.1 train no.104510  loss = 4.49777 avg_loss = 3.54597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.104520  loss = 6.52792 avg_loss = 3.62897\n",
      "epoch no.1 train no.104530  loss = 3.79752 avg_loss = 3.67710\n",
      "epoch no.1 train no.104540  loss = 4.93213 avg_loss = 3.65546\n",
      "epoch no.1 train no.104550  loss = 4.13773 avg_loss = 3.66655\n",
      "epoch no.1 train no.104560  loss = 5.06086 avg_loss = 3.62870\n",
      "epoch no.1 train no.104570  loss = 2.47068 avg_loss = 3.65219\n",
      "epoch no.1 train no.104580  loss = 3.85711 avg_loss = 3.66106\n",
      "epoch no.1 train no.104590  loss = 2.97454 avg_loss = 3.63731\n",
      "epoch no.1 train no.104600  loss = 2.61638 avg_loss = 3.59158\n",
      "epoch no.1 train no.104610  loss = 2.84998 avg_loss = 3.59924\n",
      "epoch no.1 train no.104620  loss = 5.86572 avg_loss = 3.62221\n",
      "epoch no.1 train no.104630  loss = 3.82130 avg_loss = 3.70313\n",
      "epoch no.1 train no.104640  loss = 5.44285 avg_loss = 3.70964\n",
      "epoch no.1 train no.104650  loss = 3.57396 avg_loss = 3.72961\n",
      "epoch no.1 train no.104660  loss = 4.97961 avg_loss = 3.74806\n",
      "epoch no.1 train no.104670  loss = 3.19174 avg_loss = 3.67743\n",
      "epoch no.1 train no.104680  loss = 3.07824 avg_loss = 3.64355\n",
      "epoch no.1 train no.104690  loss = 2.66787 avg_loss = 3.62703\n",
      "epoch no.1 train no.104700  loss = 3.87113 avg_loss = 3.60474\n",
      "epoch no.1 train no.104710  loss = 4.74643 avg_loss = 3.64480\n",
      "epoch no.1 train no.104720  loss = 3.30688 avg_loss = 3.69182\n",
      "epoch no.1 train no.104730  loss = 4.97007 avg_loss = 3.67632\n",
      "epoch no.1 train no.104740  loss = 2.87556 avg_loss = 3.71454\n",
      "epoch no.1 train no.104750  loss = 4.19542 avg_loss = 3.75597\n",
      "epoch no.1 train no.104760  loss = 3.97903 avg_loss = 3.77279\n",
      "epoch no.1 train no.104770  loss = 3.36088 avg_loss = 3.74944\n",
      "epoch no.1 train no.104780  loss = 4.40094 avg_loss = 3.74141\n",
      "epoch no.1 train no.104790  loss = 4.18438 avg_loss = 3.73711\n",
      "epoch no.1 train no.104800  loss = 3.08199 avg_loss = 3.72383\n",
      "epoch no.1 train no.104810  loss = 4.53486 avg_loss = 3.68739\n",
      "epoch no.1 train no.104820  loss = 3.64742 avg_loss = 3.72561\n",
      "epoch no.1 train no.104830  loss = 4.76984 avg_loss = 3.68855\n",
      "epoch no.1 train no.104840  loss = 2.74261 avg_loss = 3.67671\n",
      "epoch no.1 train no.104850  loss = 3.74969 avg_loss = 3.67505\n",
      "epoch no.1 train no.104860  loss = 3.06561 avg_loss = 3.66587\n",
      "epoch no.1 train no.104870  loss = 3.87907 avg_loss = 3.65458\n",
      "epoch no.1 train no.104880  loss = 2.90480 avg_loss = 3.61879\n",
      "epoch no.1 train no.104890  loss = 2.09896 avg_loss = 3.64083\n",
      "epoch no.1 train no.104900  loss = 3.77352 avg_loss = 3.64569\n",
      "epoch no.1 train no.104910  loss = 2.16295 avg_loss = 3.65062\n",
      "epoch no.1 train no.104920  loss = 4.99564 avg_loss = 3.64311\n",
      "epoch no.1 train no.104930  loss = 3.57975 avg_loss = 3.62572\n",
      "epoch no.1 train no.104940  loss = 2.91471 avg_loss = 3.55289\n",
      "epoch no.1 train no.104950  loss = 3.14749 avg_loss = 3.57764\n",
      "epoch no.1 train no.104960  loss = 3.39730 avg_loss = 3.62456\n",
      "epoch no.1 train no.104970  loss = 4.15387 avg_loss = 3.64459\n",
      "epoch no.1 train no.104980  loss = 3.50992 avg_loss = 3.63505\n",
      "epoch no.1 train no.104990  loss = 3.28286 avg_loss = 3.57178\n",
      "epoch no.1 train no.105000  loss = 2.79118 avg_loss = 3.53585\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁발라', '▁베스트', '</s>']\n",
      "추억의 90년대 가요 베스트</s>\n",
      "epoch no.1 train no.105010  loss = 4.52298 avg_loss = 3.53849\n",
      "epoch no.1 train no.105020  loss = 2.67990 avg_loss = 3.56526\n",
      "epoch no.1 train no.105030  loss = 3.65822 avg_loss = 3.57172\n",
      "epoch no.1 train no.105040  loss = 3.66755 avg_loss = 3.54110\n",
      "epoch no.1 train no.105050  loss = 3.50178 avg_loss = 3.61330\n",
      "epoch no.1 train no.105060  loss = 4.74051 avg_loss = 3.61165\n",
      "epoch no.1 train no.105070  loss = 3.58810 avg_loss = 3.61804\n",
      "epoch no.1 train no.105080  loss = 3.15222 avg_loss = 3.65962\n",
      "epoch no.1 train no.105090  loss = 3.27126 avg_loss = 3.63424\n",
      "epoch no.1 train no.105100  loss = 3.95680 avg_loss = 3.67630\n",
      "epoch no.1 train no.105110  loss = 3.56659 avg_loss = 3.64269\n",
      "epoch no.1 train no.105120  loss = 5.37541 avg_loss = 3.68475\n",
      "epoch no.1 train no.105130  loss = 2.14319 avg_loss = 3.69200\n",
      "epoch no.1 train no.105140  loss = 3.02723 avg_loss = 3.71149\n",
      "epoch no.1 train no.105150  loss = 3.05275 avg_loss = 3.67774\n",
      "epoch no.1 train no.105160  loss = 3.36077 avg_loss = 3.65154\n",
      "epoch no.1 train no.105170  loss = 3.39442 avg_loss = 3.60778\n",
      "epoch no.1 train no.105180  loss = 5.27535 avg_loss = 3.57483\n",
      "epoch no.1 train no.105190  loss = 4.45621 avg_loss = 3.59058\n",
      "epoch no.1 train no.105200  loss = 3.96461 avg_loss = 3.55577\n",
      "epoch no.1 train no.105210  loss = 4.07285 avg_loss = 3.52021\n",
      "epoch no.1 train no.105220  loss = 2.58961 avg_loss = 3.56531\n",
      "epoch no.1 train no.105230  loss = 3.04068 avg_loss = 3.60011\n",
      "epoch no.1 train no.105240  loss = 3.16453 avg_loss = 3.57001\n",
      "epoch no.1 train no.105250  loss = 3.12368 avg_loss = 3.52373\n",
      "epoch no.1 train no.105260  loss = 1.92954 avg_loss = 3.57483\n",
      "epoch no.1 train no.105270  loss = 4.08580 avg_loss = 3.56553\n",
      "epoch no.1 train no.105280  loss = 2.39071 avg_loss = 3.51894\n",
      "epoch no.1 train no.105290  loss = 1.91124 avg_loss = 3.51106\n",
      "epoch no.1 train no.105300  loss = 2.45729 avg_loss = 3.49793\n",
      "epoch no.1 train no.105310  loss = 6.02710 avg_loss = 3.46992\n",
      "epoch no.1 train no.105320  loss = 3.85313 avg_loss = 3.54347\n",
      "epoch no.1 train no.105330  loss = 3.04544 avg_loss = 3.54528\n",
      "epoch no.1 train no.105340  loss = 3.88117 avg_loss = 3.56318\n",
      "epoch no.1 train no.105350  loss = 3.32308 avg_loss = 3.57081\n",
      "epoch no.1 train no.105360  loss = 4.53431 avg_loss = 3.56381\n",
      "epoch no.1 train no.105370  loss = 5.02863 avg_loss = 3.55701\n",
      "epoch no.1 train no.105380  loss = 4.37985 avg_loss = 3.62568\n",
      "epoch no.1 train no.105390  loss = 2.90037 avg_loss = 3.60489\n",
      "epoch no.1 train no.105400  loss = 5.36249 avg_loss = 3.64032\n",
      "epoch no.1 train no.105410  loss = 2.50434 avg_loss = 3.63559\n",
      "epoch no.1 train no.105420  loss = 2.86588 avg_loss = 3.60089\n",
      "epoch no.1 train no.105430  loss = 3.55973 avg_loss = 3.61419\n",
      "epoch no.1 train no.105440  loss = 2.30827 avg_loss = 3.64963\n",
      "epoch no.1 train no.105450  loss = 5.37245 avg_loss = 3.68419\n",
      "epoch no.1 train no.105460  loss = 3.28382 avg_loss = 3.72579\n",
      "epoch no.1 train no.105470  loss = 3.88942 avg_loss = 3.70085\n",
      "epoch no.1 train no.105480  loss = 4.58675 avg_loss = 3.69554\n",
      "epoch no.1 train no.105490  loss = 3.33707 avg_loss = 3.71760\n",
      "epoch no.1 train no.105500  loss = 3.97988 avg_loss = 3.70278\n",
      "epoch no.1 train no.105510  loss = 2.16023 avg_loss = 3.69447\n",
      "epoch no.1 train no.105520  loss = 3.22260 avg_loss = 3.65245\n",
      "epoch no.1 train no.105530  loss = 3.47466 avg_loss = 3.64908\n",
      "epoch no.1 train no.105540  loss = 3.30400 avg_loss = 3.65436\n",
      "epoch no.1 train no.105550  loss = 2.25121 avg_loss = 3.62212\n",
      "epoch no.1 train no.105560  loss = 5.33289 avg_loss = 3.64171\n",
      "epoch no.1 train no.105570  loss = 3.03873 avg_loss = 3.61626\n",
      "epoch no.1 train no.105580  loss = 5.44418 avg_loss = 3.64655\n",
      "epoch no.1 train no.105590  loss = 3.95735 avg_loss = 3.62962\n",
      "epoch no.1 train no.105600  loss = 5.73460 avg_loss = 3.63389\n",
      "epoch no.1 train no.105610  loss = 4.74119 avg_loss = 3.65052\n",
      "epoch no.1 train no.105620  loss = 3.74038 avg_loss = 3.64808\n",
      "epoch no.1 train no.105630  loss = 5.05611 avg_loss = 3.64336\n",
      "epoch no.1 train no.105640  loss = 4.20036 avg_loss = 3.68569\n",
      "epoch no.1 train no.105650  loss = 3.20817 avg_loss = 3.69911\n",
      "epoch no.1 train no.105660  loss = 4.43774 avg_loss = 3.69126\n",
      "epoch no.1 train no.105670  loss = 1.96675 avg_loss = 3.70752\n",
      "epoch no.1 train no.105680  loss = 3.56679 avg_loss = 3.69959\n",
      "epoch no.1 train no.105690  loss = 2.19146 avg_loss = 3.68079\n",
      "epoch no.1 train no.105700  loss = 5.60584 avg_loss = 3.72199\n",
      "epoch no.1 train no.105710  loss = 3.60416 avg_loss = 3.69806\n",
      "epoch no.1 train no.105720  loss = 4.39838 avg_loss = 3.72685\n",
      "epoch no.1 train no.105730  loss = 4.31954 avg_loss = 3.74780\n",
      "epoch no.1 train no.105740  loss = 3.00839 avg_loss = 3.69788\n",
      "epoch no.1 train no.105750  loss = 3.31554 avg_loss = 3.69162\n",
      "epoch no.1 train no.105760  loss = 1.80124 avg_loss = 3.69157\n",
      "epoch no.1 train no.105770  loss = 2.64870 avg_loss = 3.66895\n",
      "epoch no.1 train no.105780  loss = 6.75824 avg_loss = 3.69024\n",
      "epoch no.1 train no.105790  loss = 2.27824 avg_loss = 3.63356\n",
      "epoch no.1 train no.105800  loss = 2.54165 avg_loss = 3.63020\n",
      "epoch no.1 train no.105810  loss = 7.02041 avg_loss = 3.66571\n",
      "epoch no.1 train no.105820  loss = 4.94834 avg_loss = 3.63878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.105830  loss = 4.26952 avg_loss = 3.65068\n",
      "epoch no.1 train no.105840  loss = 2.06022 avg_loss = 3.67845\n",
      "epoch no.1 train no.105850  loss = 4.53748 avg_loss = 3.70526\n",
      "epoch no.1 train no.105860  loss = 5.01494 avg_loss = 3.70870\n",
      "epoch no.1 train no.105870  loss = 4.69436 avg_loss = 3.70568\n",
      "epoch no.1 train no.105880  loss = 4.00916 avg_loss = 3.67937\n",
      "epoch no.1 train no.105890  loss = 2.79087 avg_loss = 3.69345\n",
      "epoch no.1 train no.105900  loss = 3.88260 avg_loss = 3.76996\n",
      "epoch no.1 train no.105910  loss = 3.36082 avg_loss = 3.74537\n",
      "epoch no.1 train no.105920  loss = 2.44175 avg_loss = 3.69445\n",
      "epoch no.1 train no.105930  loss = 3.16334 avg_loss = 3.70408\n",
      "epoch no.1 train no.105940  loss = 5.91813 avg_loss = 3.76013\n",
      "epoch no.1 train no.105950  loss = 3.65746 avg_loss = 3.80787\n",
      "epoch no.1 train no.105960  loss = 3.44476 avg_loss = 3.77585\n",
      "epoch no.1 train no.105970  loss = 3.62508 avg_loss = 3.74968\n",
      "epoch no.1 train no.105980  loss = 4.79809 avg_loss = 3.73771\n",
      "epoch no.1 train no.105990  loss = 6.25908 avg_loss = 3.75091\n",
      "epoch no.1 train no.106000  loss = 4.53263 avg_loss = 3.80590\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '년대', '▁댄스', '요', '</s>']\n",
      "추억의 90년대 인기가요</s>\n",
      "epoch no.1 train no.106010  loss = 2.34369 avg_loss = 3.76382\n",
      "epoch no.1 train no.106020  loss = 3.08026 avg_loss = 3.77906\n",
      "epoch no.1 train no.106030  loss = 4.58303 avg_loss = 3.78502\n",
      "epoch no.1 train no.106040  loss = 3.24498 avg_loss = 3.73272\n",
      "epoch no.1 train no.106050  loss = 3.64019 avg_loss = 3.71994\n",
      "epoch no.1 train no.106060  loss = 6.58621 avg_loss = 3.76918\n",
      "epoch no.1 train no.106070  loss = 3.51729 avg_loss = 3.80610\n",
      "epoch no.1 train no.106080  loss = 3.04545 avg_loss = 3.78516\n",
      "epoch no.1 train no.106090  loss = 3.57908 avg_loss = 3.72956\n",
      "epoch no.1 train no.106100  loss = 4.16007 avg_loss = 3.73859\n",
      "epoch no.1 train no.106110  loss = 3.06781 avg_loss = 3.68376\n",
      "epoch no.1 train no.106120  loss = 4.38519 avg_loss = 3.74553\n",
      "epoch no.1 train no.106130  loss = 4.14405 avg_loss = 3.73090\n",
      "epoch no.1 train no.106140  loss = 3.06167 avg_loss = 3.73993\n",
      "epoch no.1 train no.106150  loss = 3.71146 avg_loss = 3.69931\n",
      "epoch no.1 train no.106160  loss = 1.70801 avg_loss = 3.66984\n",
      "epoch no.1 train no.106170  loss = 3.84864 avg_loss = 3.63198\n",
      "epoch no.1 train no.106180  loss = 5.45474 avg_loss = 3.64059\n",
      "epoch no.1 train no.106190  loss = 4.66034 avg_loss = 3.64081\n",
      "epoch no.1 train no.106200  loss = 5.82130 avg_loss = 3.66863\n",
      "epoch no.1 train no.106210  loss = 2.80534 avg_loss = 3.63550\n",
      "epoch no.1 train no.106220  loss = 2.40177 avg_loss = 3.60261\n",
      "epoch no.1 train no.106230  loss = 3.48290 avg_loss = 3.61724\n",
      "epoch no.1 train no.106240  loss = 4.62396 avg_loss = 3.60605\n",
      "epoch no.1 train no.106250  loss = 4.00615 avg_loss = 3.63261\n",
      "epoch no.1 train no.106260  loss = 5.85639 avg_loss = 3.65933\n",
      "epoch no.1 train no.106270  loss = 3.11418 avg_loss = 3.60979\n",
      "epoch no.1 train no.106280  loss = 2.96711 avg_loss = 3.61473\n",
      "epoch no.1 train no.106290  loss = 3.49640 avg_loss = 3.59746\n",
      "epoch no.1 train no.106300  loss = 5.35964 avg_loss = 3.59896\n",
      "epoch no.1 train no.106310  loss = 2.70973 avg_loss = 3.64905\n",
      "epoch no.1 train no.106320  loss = 2.54300 avg_loss = 3.66732\n",
      "epoch no.1 train no.106330  loss = 3.40346 avg_loss = 3.65462\n",
      "epoch no.1 train no.106340  loss = 4.92312 avg_loss = 3.66069\n",
      "epoch no.1 train no.106350  loss = 2.23318 avg_loss = 3.60247\n",
      "epoch no.1 train no.106360  loss = 3.42371 avg_loss = 3.61154\n",
      "epoch no.1 train no.106370  loss = 3.59020 avg_loss = 3.66876\n",
      "epoch no.1 train no.106380  loss = 7.92537 avg_loss = 3.70537\n",
      "epoch no.1 train no.106390  loss = 3.52429 avg_loss = 3.72769\n",
      "epoch no.1 train no.106400  loss = 1.89090 avg_loss = 3.72312\n",
      "epoch no.1 train no.106410  loss = 2.75807 avg_loss = 3.67855\n",
      "epoch no.1 train no.106420  loss = 3.37041 avg_loss = 3.67310\n",
      "epoch no.1 train no.106430  loss = 4.86160 avg_loss = 3.70688\n",
      "epoch no.1 train no.106440  loss = 2.36683 avg_loss = 3.68196\n",
      "epoch no.1 train no.106450  loss = 4.79937 avg_loss = 3.71287\n",
      "epoch no.1 train no.106460  loss = 5.03369 avg_loss = 3.76939\n",
      "epoch no.1 train no.106470  loss = 3.57065 avg_loss = 3.73394\n",
      "epoch no.1 train no.106480  loss = 4.62635 avg_loss = 3.75978\n",
      "epoch no.1 train no.106490  loss = 5.74606 avg_loss = 3.73668\n",
      "epoch no.1 train no.106500  loss = 2.11025 avg_loss = 3.67088\n",
      "epoch no.1 train no.106510  loss = 4.08352 avg_loss = 3.69273\n",
      "epoch no.1 train no.106520  loss = 3.69311 avg_loss = 3.72518\n",
      "epoch no.1 train no.106530  loss = 4.91586 avg_loss = 3.72422\n",
      "epoch no.1 train no.106540  loss = 2.37662 avg_loss = 3.66816\n",
      "epoch no.1 train no.106550  loss = 3.03475 avg_loss = 3.70678\n",
      "epoch no.1 train no.106560  loss = 4.73106 avg_loss = 3.75421\n",
      "epoch no.1 train no.106570  loss = 4.03034 avg_loss = 3.70451\n",
      "epoch no.1 train no.106580  loss = 4.55008 avg_loss = 3.73670\n",
      "epoch no.1 train no.106590  loss = 4.53588 avg_loss = 3.76765\n",
      "epoch no.1 train no.106600  loss = 2.37227 avg_loss = 3.73559\n",
      "epoch no.1 train no.106610  loss = 3.30849 avg_loss = 3.73936\n",
      "epoch no.1 train no.106620  loss = 3.25305 avg_loss = 3.74466\n",
      "epoch no.1 train no.106630  loss = 1.41507 avg_loss = 3.69347\n",
      "epoch no.1 train no.106640  loss = 4.00616 avg_loss = 3.69772\n",
      "epoch no.1 train no.106650  loss = 2.55072 avg_loss = 3.62401\n",
      "epoch no.1 train no.106660  loss = 4.92384 avg_loss = 3.66649\n",
      "epoch no.1 train no.106670  loss = 5.33845 avg_loss = 3.69854\n",
      "epoch no.1 train no.106680  loss = 4.11192 avg_loss = 3.64803\n",
      "epoch no.1 train no.106690  loss = 3.61049 avg_loss = 3.61410\n",
      "epoch no.1 train no.106700  loss = 2.62041 avg_loss = 3.62629\n",
      "epoch no.1 train no.106710  loss = 2.94454 avg_loss = 3.61532\n",
      "epoch no.1 train no.106720  loss = 4.44569 avg_loss = 3.65749\n",
      "epoch no.1 train no.106730  loss = 3.71104 avg_loss = 3.69018\n",
      "epoch no.1 train no.106740  loss = 3.84315 avg_loss = 3.68422\n",
      "epoch no.1 train no.106750  loss = 3.84974 avg_loss = 3.67137\n",
      "epoch no.1 train no.106760  loss = 4.86092 avg_loss = 3.70197\n",
      "epoch no.1 train no.106770  loss = 3.98473 avg_loss = 3.66210\n",
      "epoch no.1 train no.106780  loss = 1.92926 avg_loss = 3.63101\n",
      "epoch no.1 train no.106790  loss = 2.75316 avg_loss = 3.64152\n",
      "epoch no.1 train no.106800  loss = 5.98699 avg_loss = 3.66634\n",
      "epoch no.1 train no.106810  loss = 3.67240 avg_loss = 3.67570\n",
      "epoch no.1 train no.106820  loss = 3.69036 avg_loss = 3.61401\n",
      "epoch no.1 train no.106830  loss = 2.38583 avg_loss = 3.59696\n",
      "epoch no.1 train no.106840  loss = 2.45206 avg_loss = 3.57187\n",
      "epoch no.1 train no.106850  loss = 3.22203 avg_loss = 3.52520\n",
      "epoch no.1 train no.106860  loss = 6.15752 avg_loss = 3.59322\n",
      "epoch no.1 train no.106870  loss = 4.07730 avg_loss = 3.59954\n",
      "epoch no.1 train no.106880  loss = 4.46861 avg_loss = 3.60513\n",
      "epoch no.1 train no.106890  loss = 3.30753 avg_loss = 3.61539\n",
      "epoch no.1 train no.106900  loss = 6.43658 avg_loss = 3.63104\n",
      "epoch no.1 train no.106910  loss = 2.88048 avg_loss = 3.64100\n",
      "epoch no.1 train no.106920  loss = 3.62652 avg_loss = 3.67580\n",
      "epoch no.1 train no.106930  loss = 2.54085 avg_loss = 3.64715\n",
      "epoch no.1 train no.106940  loss = 3.41930 avg_loss = 3.59878\n",
      "epoch no.1 train no.106950  loss = 5.68009 avg_loss = 3.66960\n",
      "epoch no.1 train no.106960  loss = 3.01522 avg_loss = 3.66851\n",
      "epoch no.1 train no.106970  loss = 2.96103 avg_loss = 3.72471\n",
      "epoch no.1 train no.106980  loss = 2.32755 avg_loss = 3.66074\n",
      "epoch no.1 train no.106990  loss = 5.66482 avg_loss = 3.67026\n",
      "epoch no.1 train no.107000  loss = 4.67375 avg_loss = 3.68030\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '▁모음', '</s>']\n",
      "추억의 발라드노래 모음</s>\n",
      "epoch no.1 train no.107010  loss = 4.57320 avg_loss = 3.68484\n",
      "epoch no.1 train no.107020  loss = 2.75271 avg_loss = 3.67534\n",
      "epoch no.1 train no.107030  loss = 1.75923 avg_loss = 3.63602\n",
      "epoch no.1 train no.107040  loss = 2.05274 avg_loss = 3.62760\n",
      "epoch no.1 train no.107050  loss = 1.84562 avg_loss = 3.62053\n",
      "epoch no.1 train no.107060  loss = 3.47050 avg_loss = 3.57462\n",
      "epoch no.1 train no.107070  loss = 3.39452 avg_loss = 3.60349\n",
      "epoch no.1 train no.107080  loss = 4.71384 avg_loss = 3.63816\n",
      "epoch no.1 train no.107090  loss = 3.41940 avg_loss = 3.63265\n",
      "epoch no.1 train no.107100  loss = 5.64824 avg_loss = 3.65131\n",
      "epoch no.1 train no.107110  loss = 4.21176 avg_loss = 3.60612\n",
      "epoch no.1 train no.107120  loss = 3.97965 avg_loss = 3.63369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.107130  loss = 4.88921 avg_loss = 3.68453\n",
      "epoch no.1 train no.107140  loss = 2.09625 avg_loss = 3.67077\n",
      "epoch no.1 train no.107150  loss = 3.28452 avg_loss = 3.63093\n",
      "epoch no.1 train no.107160  loss = 3.05397 avg_loss = 3.59615\n",
      "epoch no.1 train no.107170  loss = 3.02170 avg_loss = 3.60324\n",
      "epoch no.1 train no.107180  loss = 2.51306 avg_loss = 3.60581\n",
      "epoch no.1 train no.107190  loss = 3.85234 avg_loss = 3.59343\n",
      "epoch no.1 train no.107200  loss = 5.09974 avg_loss = 3.61463\n",
      "epoch no.1 train no.107210  loss = 3.56931 avg_loss = 3.58487\n",
      "epoch no.1 train no.107220  loss = 5.55833 avg_loss = 3.63519\n",
      "epoch no.1 train no.107230  loss = 5.16062 avg_loss = 3.67626\n",
      "epoch no.1 train no.107240  loss = 3.13690 avg_loss = 3.62218\n",
      "epoch no.1 train no.107250  loss = 3.79069 avg_loss = 3.61817\n",
      "epoch no.1 train no.107260  loss = 2.87530 avg_loss = 3.67530\n",
      "epoch no.1 train no.107270  loss = 3.45692 avg_loss = 3.64766\n",
      "epoch no.1 train no.107280  loss = 3.97328 avg_loss = 3.64994\n",
      "epoch no.1 train no.107290  loss = 2.97766 avg_loss = 3.65928\n",
      "epoch no.1 train no.107300  loss = 4.06839 avg_loss = 3.64996\n",
      "epoch no.1 train no.107310  loss = 4.13923 avg_loss = 3.65196\n",
      "epoch no.1 train no.107320  loss = 3.33482 avg_loss = 3.63289\n",
      "epoch no.1 train no.107330  loss = 3.78616 avg_loss = 3.61992\n",
      "epoch no.1 train no.107340  loss = 2.32388 avg_loss = 3.60875\n",
      "epoch no.1 train no.107350  loss = 2.99070 avg_loss = 3.59478\n",
      "epoch no.1 train no.107360  loss = 2.30656 avg_loss = 3.64861\n",
      "epoch no.1 train no.107370  loss = 4.26320 avg_loss = 3.65643\n",
      "epoch no.1 train no.107380  loss = 2.79402 avg_loss = 3.62142\n",
      "epoch no.1 train no.107390  loss = 4.41854 avg_loss = 3.61235\n",
      "epoch no.1 train no.107400  loss = 2.70560 avg_loss = 3.63317\n",
      "epoch no.1 train no.107410  loss = 5.74214 avg_loss = 3.72742\n",
      "epoch no.1 train no.107420  loss = 4.60552 avg_loss = 3.68885\n",
      "epoch no.1 train no.107430  loss = 3.12453 avg_loss = 3.67893\n",
      "epoch no.1 train no.107440  loss = 3.24618 avg_loss = 3.67269\n",
      "epoch no.1 train no.107450  loss = 2.41458 avg_loss = 3.73869\n",
      "epoch no.1 train no.107460  loss = 2.07510 avg_loss = 3.73054\n",
      "epoch no.1 train no.107470  loss = 2.34529 avg_loss = 3.71437\n",
      "epoch no.1 train no.107480  loss = 3.56876 avg_loss = 3.71021\n",
      "epoch no.1 train no.107490  loss = 2.63133 avg_loss = 3.82165\n",
      "epoch no.1 train no.107500  loss = 4.73090 avg_loss = 3.78789\n",
      "epoch no.1 train no.107510  loss = 5.52383 avg_loss = 3.78547\n",
      "epoch no.1 train no.107520  loss = 2.64428 avg_loss = 3.76800\n",
      "epoch no.1 train no.107530  loss = 4.59318 avg_loss = 3.80079\n",
      "epoch no.1 train no.107540  loss = 3.81339 avg_loss = 3.72607\n",
      "epoch no.1 train no.107550  loss = 4.59255 avg_loss = 3.67288\n",
      "epoch no.1 train no.107560  loss = 4.13984 avg_loss = 3.67791\n",
      "epoch no.1 train no.107570  loss = 3.08781 avg_loss = 3.64550\n",
      "epoch no.1 train no.107580  loss = 3.02399 avg_loss = 3.63334\n",
      "epoch no.1 train no.107590  loss = 4.89176 avg_loss = 3.65834\n",
      "epoch no.1 train no.107600  loss = 3.86831 avg_loss = 3.68461\n",
      "epoch no.1 train no.107610  loss = 2.50065 avg_loss = 3.67389\n",
      "epoch no.1 train no.107620  loss = 2.85855 avg_loss = 3.65219\n",
      "epoch no.1 train no.107630  loss = 2.21006 avg_loss = 3.57092\n",
      "epoch no.1 train no.107640  loss = 3.21758 avg_loss = 3.55518\n",
      "epoch no.1 train no.107650  loss = 2.65903 avg_loss = 3.57340\n",
      "epoch no.1 train no.107660  loss = 3.66921 avg_loss = 3.56657\n",
      "epoch no.1 train no.107670  loss = 5.40883 avg_loss = 3.65309\n",
      "epoch no.1 train no.107680  loss = 2.98905 avg_loss = 3.64274\n",
      "epoch no.1 train no.107690  loss = 2.63348 avg_loss = 3.61482\n",
      "epoch no.1 train no.107700  loss = 4.57602 avg_loss = 3.63546\n",
      "epoch no.1 train no.107710  loss = 3.63750 avg_loss = 3.68150\n",
      "epoch no.1 train no.107720  loss = 4.00883 avg_loss = 3.63037\n",
      "epoch no.1 train no.107730  loss = 2.00505 avg_loss = 3.59677\n",
      "epoch no.1 train no.107740  loss = 3.64208 avg_loss = 3.56848\n",
      "epoch no.1 train no.107750  loss = 2.82810 avg_loss = 3.61486\n",
      "epoch no.1 train no.107760  loss = 2.45916 avg_loss = 3.66226\n",
      "epoch no.1 train no.107770  loss = 4.00266 avg_loss = 3.63365\n",
      "epoch no.1 train no.107780  loss = 2.34246 avg_loss = 3.65113\n",
      "epoch no.1 train no.107790  loss = 4.44472 avg_loss = 3.66096\n",
      "epoch no.1 train no.107800  loss = 2.95400 avg_loss = 3.65834\n",
      "epoch no.1 train no.107810  loss = 2.11594 avg_loss = 3.61435\n",
      "epoch no.1 train no.107820  loss = 3.53419 avg_loss = 3.59821\n",
      "epoch no.1 train no.107830  loss = 4.12951 avg_loss = 3.62013\n",
      "epoch no.1 train no.107840  loss = 3.25027 avg_loss = 3.58577\n",
      "epoch no.1 train no.107850  loss = 1.52556 avg_loss = 3.55958\n",
      "epoch no.1 train no.107860  loss = 4.31618 avg_loss = 3.63088\n",
      "epoch no.1 train no.107870  loss = 3.05747 avg_loss = 3.58908\n",
      "epoch no.1 train no.107880  loss = 2.56386 avg_loss = 3.60922\n",
      "epoch no.1 train no.107890  loss = 5.28465 avg_loss = 3.63960\n",
      "epoch no.1 train no.107900  loss = 3.38966 avg_loss = 3.65998\n",
      "epoch no.1 train no.107910  loss = 3.22331 avg_loss = 3.65966\n",
      "epoch no.1 train no.107920  loss = 3.05973 avg_loss = 3.58640\n",
      "epoch no.1 train no.107930  loss = 4.30256 avg_loss = 3.54990\n",
      "epoch no.1 train no.107940  loss = 2.99224 avg_loss = 3.52214\n",
      "epoch no.1 train no.107950  loss = 2.75641 avg_loss = 3.55521\n",
      "epoch no.1 train no.107960  loss = 4.42224 avg_loss = 3.64696\n",
      "epoch no.1 train no.107970  loss = 4.01674 avg_loss = 3.73696\n",
      "epoch no.1 train no.107980  loss = 3.12351 avg_loss = 3.69604\n",
      "epoch no.1 train no.107990  loss = 3.61229 avg_loss = 3.74204\n",
      "epoch no.1 train no.108000  loss = 2.52035 avg_loss = 3.71834\n",
      "5\n",
      "to_tokens: ['▁가을', '▁발라', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.1 train no.108010  loss = 3.67885 avg_loss = 3.79069\n",
      "epoch no.1 train no.108020  loss = 4.63659 avg_loss = 3.79046\n",
      "epoch no.1 train no.108030  loss = 3.00957 avg_loss = 3.77701\n",
      "epoch no.1 train no.108040  loss = 3.45847 avg_loss = 3.77360\n",
      "epoch no.1 train no.108050  loss = 2.27747 avg_loss = 3.76198\n",
      "epoch no.1 train no.108060  loss = 3.20411 avg_loss = 3.76070\n",
      "epoch no.1 train no.108070  loss = 5.41559 avg_loss = 3.74303\n",
      "epoch no.1 train no.108080  loss = 4.22896 avg_loss = 3.73126\n",
      "epoch no.1 train no.108090  loss = 4.26336 avg_loss = 3.77119\n",
      "epoch no.1 train no.108100  loss = 2.23678 avg_loss = 3.73381\n",
      "epoch no.1 train no.108110  loss = 4.37369 avg_loss = 3.71797\n",
      "epoch no.1 train no.108120  loss = 3.18078 avg_loss = 3.66928\n",
      "epoch no.1 train no.108130  loss = 2.59783 avg_loss = 3.67142\n",
      "epoch no.1 train no.108140  loss = 2.44411 avg_loss = 3.68174\n",
      "epoch no.1 train no.108150  loss = 2.06223 avg_loss = 3.64904\n",
      "epoch no.1 train no.108160  loss = 3.96445 avg_loss = 3.63237\n",
      "epoch no.1 train no.108170  loss = 3.50231 avg_loss = 3.61248\n",
      "epoch no.1 train no.108180  loss = 3.88575 avg_loss = 3.70770\n",
      "epoch no.1 train no.108190  loss = 4.96996 avg_loss = 3.71579\n",
      "epoch no.1 train no.108200  loss = 4.63249 avg_loss = 3.71569\n",
      "epoch no.1 train no.108210  loss = 4.69115 avg_loss = 3.73565\n",
      "epoch no.1 train no.108220  loss = 6.89767 avg_loss = 3.75875\n",
      "epoch no.1 train no.108230  loss = 2.84894 avg_loss = 3.74431\n",
      "epoch no.1 train no.108240  loss = 3.59585 avg_loss = 3.72411\n",
      "epoch no.1 train no.108250  loss = 3.12326 avg_loss = 3.66767\n",
      "epoch no.1 train no.108260  loss = 3.73490 avg_loss = 3.63285\n",
      "epoch no.1 train no.108270  loss = 3.35440 avg_loss = 3.66735\n",
      "epoch no.1 train no.108280  loss = 4.44622 avg_loss = 3.67106\n",
      "epoch no.1 train no.108290  loss = 4.20835 avg_loss = 3.70167\n",
      "epoch no.1 train no.108300  loss = 2.41653 avg_loss = 3.68772\n",
      "epoch no.1 train no.108310  loss = 2.72577 avg_loss = 3.68120\n",
      "epoch no.1 train no.108320  loss = 2.68330 avg_loss = 3.67407\n",
      "epoch no.1 train no.108330  loss = 3.29816 avg_loss = 3.68258\n",
      "epoch no.1 train no.108340  loss = 2.43010 avg_loss = 3.66076\n",
      "epoch no.1 train no.108350  loss = 6.82505 avg_loss = 3.67772\n",
      "epoch no.1 train no.108360  loss = 4.46480 avg_loss = 3.66940\n",
      "epoch no.1 train no.108370  loss = 5.00196 avg_loss = 3.66127\n",
      "epoch no.1 train no.108380  loss = 3.54196 avg_loss = 3.67066\n",
      "epoch no.1 train no.108390  loss = 3.20987 avg_loss = 3.65554\n",
      "epoch no.1 train no.108400  loss = 4.38234 avg_loss = 3.64989\n",
      "epoch no.1 train no.108410  loss = 2.75574 avg_loss = 3.61826\n",
      "epoch no.1 train no.108420  loss = 2.32050 avg_loss = 3.58739\n",
      "epoch no.1 train no.108430  loss = 2.00076 avg_loss = 3.55871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.108440  loss = 4.35842 avg_loss = 3.58100\n",
      "epoch no.1 train no.108450  loss = 5.24146 avg_loss = 3.57275\n",
      "epoch no.1 train no.108460  loss = 4.63053 avg_loss = 3.61162\n",
      "epoch no.1 train no.108470  loss = 3.31276 avg_loss = 3.61854\n",
      "epoch no.1 train no.108480  loss = 5.03271 avg_loss = 3.63510\n",
      "epoch no.1 train no.108490  loss = 5.43644 avg_loss = 3.65819\n",
      "epoch no.1 train no.108500  loss = 3.16829 avg_loss = 3.64400\n",
      "epoch no.1 train no.108510  loss = 3.26389 avg_loss = 3.62996\n",
      "epoch no.1 train no.108520  loss = 3.83302 avg_loss = 3.63541\n",
      "epoch no.1 train no.108530  loss = 3.29906 avg_loss = 3.65020\n",
      "epoch no.1 train no.108540  loss = 3.22918 avg_loss = 3.61905\n",
      "epoch no.1 train no.108550  loss = 4.56797 avg_loss = 3.59907\n",
      "epoch no.1 train no.108560  loss = 4.43804 avg_loss = 3.58878\n",
      "epoch no.1 train no.108570  loss = 3.98649 avg_loss = 3.58059\n",
      "epoch no.1 train no.108580  loss = 2.79677 avg_loss = 3.60136\n",
      "epoch no.1 train no.108590  loss = 2.78252 avg_loss = 3.57848\n",
      "epoch no.1 train no.108600  loss = 2.55459 avg_loss = 3.54527\n",
      "epoch no.1 train no.108610  loss = 3.01797 avg_loss = 3.49670\n",
      "epoch no.1 train no.108620  loss = 6.10267 avg_loss = 3.54893\n",
      "epoch no.1 train no.108630  loss = 4.64228 avg_loss = 3.61508\n",
      "epoch no.1 train no.108640  loss = 1.79827 avg_loss = 3.60163\n",
      "epoch no.1 train no.108650  loss = 4.15303 avg_loss = 3.60336\n",
      "epoch no.1 train no.108660  loss = 2.86248 avg_loss = 3.58672\n",
      "epoch no.1 train no.108670  loss = 4.55437 avg_loss = 3.58680\n",
      "epoch no.1 train no.108680  loss = 4.26411 avg_loss = 3.57887\n",
      "epoch no.1 train no.108690  loss = 5.77531 avg_loss = 3.59907\n",
      "epoch no.1 train no.108700  loss = 2.58112 avg_loss = 3.61780\n",
      "epoch no.1 train no.108710  loss = 4.37953 avg_loss = 3.70541\n",
      "epoch no.1 train no.108720  loss = 4.96503 avg_loss = 3.68254\n",
      "epoch no.1 train no.108730  loss = 5.97298 avg_loss = 3.69147\n",
      "epoch no.1 train no.108740  loss = 2.17998 avg_loss = 3.68449\n",
      "epoch no.1 train no.108750  loss = 2.67785 avg_loss = 3.69076\n",
      "epoch no.1 train no.108760  loss = 2.98991 avg_loss = 3.66521\n",
      "epoch no.1 train no.108770  loss = 4.08849 avg_loss = 3.66311\n",
      "epoch no.1 train no.108780  loss = 3.50273 avg_loss = 3.63039\n",
      "epoch no.1 train no.108790  loss = 6.39922 avg_loss = 3.65950\n",
      "epoch no.1 train no.108800  loss = 4.40834 avg_loss = 3.61803\n",
      "epoch no.1 train no.108810  loss = 4.15944 avg_loss = 3.59076\n",
      "epoch no.1 train no.108820  loss = 1.93608 avg_loss = 3.61071\n",
      "epoch no.1 train no.108830  loss = 4.46031 avg_loss = 3.63017\n",
      "epoch no.1 train no.108840  loss = 3.49532 avg_loss = 3.67770\n",
      "epoch no.1 train no.108850  loss = 3.08691 avg_loss = 3.66060\n",
      "epoch no.1 train no.108860  loss = 3.16120 avg_loss = 3.66098\n",
      "epoch no.1 train no.108870  loss = 2.99126 avg_loss = 3.71695\n",
      "epoch no.1 train no.108880  loss = 2.67701 avg_loss = 3.68096\n",
      "epoch no.1 train no.108890  loss = 3.59384 avg_loss = 3.69433\n",
      "epoch no.1 train no.108900  loss = 2.74277 avg_loss = 3.66315\n",
      "epoch no.1 train no.108910  loss = 3.73263 avg_loss = 3.66782\n",
      "epoch no.1 train no.108920  loss = 5.37330 avg_loss = 3.65419\n",
      "epoch no.1 train no.108930  loss = 3.96169 avg_loss = 3.65375\n",
      "epoch no.1 train no.108940  loss = 1.86918 avg_loss = 3.68019\n",
      "epoch no.1 train no.108950  loss = 3.63336 avg_loss = 3.65655\n",
      "epoch no.1 train no.108960  loss = 5.03608 avg_loss = 3.68432\n",
      "epoch no.1 train no.108970  loss = 2.87756 avg_loss = 3.66764\n",
      "epoch no.1 train no.108980  loss = 4.43930 avg_loss = 3.66373\n",
      "epoch no.1 train no.108990  loss = 5.62813 avg_loss = 3.66137\n",
      "epoch no.1 train no.109000  loss = 6.07313 avg_loss = 3.65817\n",
      "4\n",
      "to_tokens: ['▁잔잔', '▁명', '드', '▁명', '집', '</s>']\n",
      "추억의 발라드 모음집</s>\n",
      "epoch no.1 train no.109010  loss = 2.44312 avg_loss = 3.66602\n",
      "epoch no.1 train no.109020  loss = 2.25678 avg_loss = 3.63535\n",
      "epoch no.1 train no.109030  loss = 4.34884 avg_loss = 3.67109\n",
      "epoch no.1 train no.109040  loss = 2.92084 avg_loss = 3.67760\n",
      "epoch no.1 train no.109050  loss = 2.31715 avg_loss = 3.63928\n",
      "epoch no.1 train no.109060  loss = 2.73599 avg_loss = 3.65253\n",
      "epoch no.1 train no.109070  loss = 4.42279 avg_loss = 3.69127\n",
      "epoch no.1 train no.109080  loss = 2.02242 avg_loss = 3.66642\n",
      "epoch no.1 train no.109090  loss = 5.12647 avg_loss = 3.74942\n",
      "epoch no.1 train no.109100  loss = 3.34014 avg_loss = 3.68540\n",
      "epoch no.1 train no.109110  loss = 4.63417 avg_loss = 3.68676\n",
      "epoch no.1 train no.109120  loss = 2.47734 avg_loss = 3.67739\n",
      "epoch no.1 train no.109130  loss = 3.86275 avg_loss = 3.68374\n",
      "epoch no.1 train no.109140  loss = 3.89709 avg_loss = 3.66200\n",
      "epoch no.1 train no.109150  loss = 2.55849 avg_loss = 3.66539\n",
      "epoch no.1 train no.109160  loss = 2.45965 avg_loss = 3.63761\n",
      "epoch no.1 train no.109170  loss = 3.86055 avg_loss = 3.60576\n",
      "epoch no.1 train no.109180  loss = 6.19132 avg_loss = 3.62090\n",
      "epoch no.1 train no.109190  loss = 2.97875 avg_loss = 3.63699\n",
      "epoch no.1 train no.109200  loss = 3.83447 avg_loss = 3.69396\n",
      "epoch no.1 train no.109210  loss = 3.59237 avg_loss = 3.71031\n",
      "epoch no.1 train no.109220  loss = 3.05781 avg_loss = 3.71388\n",
      "epoch no.1 train no.109230  loss = 2.52439 avg_loss = 3.70769\n",
      "epoch no.1 train no.109240  loss = 3.41244 avg_loss = 3.67660\n",
      "epoch no.1 train no.109250  loss = 5.94177 avg_loss = 3.74658\n",
      "epoch no.1 train no.109260  loss = 3.71569 avg_loss = 3.74029\n",
      "epoch no.1 train no.109270  loss = 2.40795 avg_loss = 3.76684\n",
      "epoch no.1 train no.109280  loss = 2.96771 avg_loss = 3.77286\n",
      "epoch no.1 train no.109290  loss = 3.83824 avg_loss = 3.76712\n",
      "epoch no.1 train no.109300  loss = 2.85252 avg_loss = 3.84507\n",
      "epoch no.1 train no.109310  loss = 3.13826 avg_loss = 3.79685\n",
      "epoch no.1 train no.109320  loss = 4.29194 avg_loss = 3.79816\n",
      "epoch no.1 train no.109330  loss = 5.59214 avg_loss = 3.82530\n",
      "epoch no.1 train no.109340  loss = 5.00868 avg_loss = 3.81503\n",
      "epoch no.1 train no.109350  loss = 4.53334 avg_loss = 3.79308\n",
      "epoch no.1 train no.109360  loss = 2.46900 avg_loss = 3.75208\n",
      "epoch no.1 train no.109370  loss = 3.45291 avg_loss = 3.73009\n",
      "epoch no.1 train no.109380  loss = 3.20487 avg_loss = 3.73865\n",
      "epoch no.1 train no.109390  loss = 3.45454 avg_loss = 3.74097\n",
      "epoch no.1 train no.109400  loss = 3.14643 avg_loss = 3.68370\n",
      "epoch no.1 train no.109410  loss = 3.67282 avg_loss = 3.69818\n",
      "epoch no.1 train no.109420  loss = 3.20083 avg_loss = 3.73955\n",
      "epoch no.1 train no.109430  loss = 4.73460 avg_loss = 3.70162\n",
      "epoch no.1 train no.109440  loss = 3.91854 avg_loss = 3.68040\n",
      "epoch no.1 train no.109450  loss = 5.00740 avg_loss = 3.71780\n",
      "epoch no.1 train no.109460  loss = 2.82292 avg_loss = 3.66257\n",
      "epoch no.1 train no.109470  loss = 3.70805 avg_loss = 3.67666\n",
      "epoch no.1 train no.109480  loss = 2.81691 avg_loss = 3.67152\n",
      "epoch no.1 train no.109490  loss = 3.47465 avg_loss = 3.64717\n",
      "epoch no.1 train no.109500  loss = 5.88252 avg_loss = 3.70754\n",
      "epoch no.1 train no.109510  loss = 2.50986 avg_loss = 3.66571\n",
      "epoch no.1 train no.109520  loss = 3.92817 avg_loss = 3.69295\n",
      "epoch no.1 train no.109530  loss = 3.94309 avg_loss = 3.64511\n",
      "epoch no.1 train no.109540  loss = 2.88386 avg_loss = 3.63327\n",
      "epoch no.1 train no.109550  loss = 4.29123 avg_loss = 3.55870\n",
      "epoch no.1 train no.109560  loss = 3.81585 avg_loss = 3.57065\n",
      "epoch no.1 train no.109570  loss = 2.55921 avg_loss = 3.52750\n",
      "epoch no.1 train no.109580  loss = 4.16968 avg_loss = 3.61940\n",
      "epoch no.1 train no.109590  loss = 3.54071 avg_loss = 3.61975\n",
      "epoch no.1 train no.109600  loss = 3.23658 avg_loss = 3.55976\n",
      "epoch no.1 train no.109610  loss = 2.35856 avg_loss = 3.49821\n",
      "epoch no.1 train no.109620  loss = 3.16115 avg_loss = 3.48251\n",
      "epoch no.1 train no.109630  loss = 3.27576 avg_loss = 3.45679\n",
      "epoch no.1 train no.109640  loss = 4.41433 avg_loss = 3.49228\n",
      "epoch no.1 train no.109650  loss = 1.28065 avg_loss = 3.50226\n",
      "epoch no.1 train no.109660  loss = 1.88925 avg_loss = 3.53942\n",
      "epoch no.1 train no.109670  loss = 3.75116 avg_loss = 3.54331\n",
      "epoch no.1 train no.109680  loss = 3.79673 avg_loss = 3.54773\n",
      "epoch no.1 train no.109690  loss = 5.71408 avg_loss = 3.62040\n",
      "epoch no.1 train no.109700  loss = 3.25230 avg_loss = 3.63551\n",
      "epoch no.1 train no.109710  loss = 4.37410 avg_loss = 3.60749\n",
      "epoch no.1 train no.109720  loss = 4.36258 avg_loss = 3.61629\n",
      "epoch no.1 train no.109730  loss = 2.92379 avg_loss = 3.61231\n",
      "epoch no.1 train no.109740  loss = 3.88337 avg_loss = 3.67985\n",
      "epoch no.1 train no.109750  loss = 2.65251 avg_loss = 3.68459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.109760  loss = 3.21425 avg_loss = 3.62520\n",
      "epoch no.1 train no.109770  loss = 4.32335 avg_loss = 3.69108\n",
      "epoch no.1 train no.109780  loss = 1.82753 avg_loss = 3.69878\n",
      "epoch no.1 train no.109790  loss = 4.00097 avg_loss = 3.68510\n",
      "epoch no.1 train no.109800  loss = 4.83806 avg_loss = 3.65642\n",
      "epoch no.1 train no.109810  loss = 4.30936 avg_loss = 3.64727\n",
      "epoch no.1 train no.109820  loss = 4.76391 avg_loss = 3.68197\n",
      "epoch no.1 train no.109830  loss = 3.57212 avg_loss = 3.61527\n",
      "epoch no.1 train no.109840  loss = 4.83064 avg_loss = 3.66266\n",
      "epoch no.1 train no.109850  loss = 4.77067 avg_loss = 3.64917\n",
      "epoch no.1 train no.109860  loss = 3.50787 avg_loss = 3.64800\n",
      "epoch no.1 train no.109870  loss = 3.60424 avg_loss = 3.66394\n",
      "epoch no.1 train no.109880  loss = 3.08191 avg_loss = 3.69428\n",
      "epoch no.1 train no.109890  loss = 4.43134 avg_loss = 3.65260\n",
      "epoch no.1 train no.109900  loss = 4.18714 avg_loss = 3.64059\n",
      "epoch no.1 train no.109910  loss = 3.64032 avg_loss = 3.63556\n",
      "epoch no.1 train no.109920  loss = 4.43671 avg_loss = 3.70270\n",
      "epoch no.1 train no.109930  loss = 2.47107 avg_loss = 3.66770\n",
      "epoch no.1 train no.109940  loss = 2.65766 avg_loss = 3.71450\n",
      "epoch no.1 train no.109950  loss = 5.18493 avg_loss = 3.71765\n",
      "epoch no.1 train no.109960  loss = 4.18738 avg_loss = 3.75669\n",
      "epoch no.1 train no.109970  loss = 2.46623 avg_loss = 3.73670\n",
      "epoch no.1 train no.109980  loss = 4.84420 avg_loss = 3.75166\n",
      "epoch no.1 train no.109990  loss = 3.33962 avg_loss = 3.71750\n",
      "epoch no.1 train no.110000  loss = 4.19550 avg_loss = 3.69466\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '곡', '▁모음', '집', '</s>']\n",
      "추억의 댄스곡 모음집</s>\n",
      "epoch no.1 train no.110010  loss = 6.31238 avg_loss = 3.77191\n",
      "epoch no.1 train no.110020  loss = 1.78104 avg_loss = 3.81154\n",
      "epoch no.1 train no.110030  loss = 2.64651 avg_loss = 3.80143\n",
      "epoch no.1 train no.110040  loss = 5.26438 avg_loss = 3.85532\n",
      "epoch no.1 train no.110050  loss = 2.37754 avg_loss = 3.81285\n",
      "epoch no.1 train no.110060  loss = 2.93852 avg_loss = 3.78804\n",
      "epoch no.1 train no.110070  loss = 3.40037 avg_loss = 3.77276\n",
      "epoch no.1 train no.110080  loss = 3.25183 avg_loss = 3.74601\n",
      "epoch no.1 train no.110090  loss = 2.13362 avg_loss = 3.72366\n",
      "epoch no.1 train no.110100  loss = 5.42091 avg_loss = 3.72384\n",
      "epoch no.1 train no.110110  loss = 1.86731 avg_loss = 3.75442\n",
      "epoch no.1 train no.110120  loss = 1.99287 avg_loss = 3.73816\n",
      "epoch no.1 train no.110130  loss = 3.21261 avg_loss = 3.74298\n",
      "epoch no.1 train no.110140  loss = 4.52111 avg_loss = 3.72385\n",
      "epoch no.1 train no.110150  loss = 4.06439 avg_loss = 3.75547\n",
      "epoch no.1 train no.110160  loss = 3.70117 avg_loss = 3.81528\n",
      "epoch no.1 train no.110170  loss = 1.74481 avg_loss = 3.79974\n",
      "epoch no.1 train no.110180  loss = 2.69388 avg_loss = 3.72316\n",
      "epoch no.1 train no.110190  loss = 2.71670 avg_loss = 3.70600\n",
      "epoch no.1 train no.110200  loss = 3.98313 avg_loss = 3.67705\n",
      "epoch no.1 train no.110210  loss = 3.01366 avg_loss = 3.62023\n",
      "epoch no.1 train no.110220  loss = 3.98109 avg_loss = 3.62304\n",
      "epoch no.1 train no.110230  loss = 2.93273 avg_loss = 3.59922\n",
      "epoch no.1 train no.110240  loss = 3.44162 avg_loss = 3.64876\n",
      "epoch no.1 train no.110250  loss = 2.23441 avg_loss = 3.65717\n",
      "epoch no.1 train no.110260  loss = 1.87648 avg_loss = 3.70764\n",
      "epoch no.1 train no.110270  loss = 4.88639 avg_loss = 3.72178\n",
      "epoch no.1 train no.110280  loss = 2.02618 avg_loss = 3.70766\n",
      "epoch no.1 train no.110290  loss = 3.52280 avg_loss = 3.80807\n",
      "epoch no.1 train no.110300  loss = 5.06841 avg_loss = 3.79355\n",
      "epoch no.1 train no.110310  loss = 2.40854 avg_loss = 3.80136\n",
      "epoch no.1 train no.110320  loss = 1.98590 avg_loss = 3.86211\n",
      "epoch no.1 train no.110330  loss = 5.81144 avg_loss = 3.83077\n",
      "epoch no.1 train no.110340  loss = 3.10907 avg_loss = 3.86509\n",
      "epoch no.1 train no.110350  loss = 3.97260 avg_loss = 3.83568\n",
      "epoch no.1 train no.110360  loss = 4.07138 avg_loss = 3.81282\n",
      "epoch no.1 train no.110370  loss = 5.12154 avg_loss = 3.82821\n",
      "epoch no.1 train no.110380  loss = 3.17848 avg_loss = 3.78454\n",
      "epoch no.1 train no.110390  loss = 4.26487 avg_loss = 3.82268\n",
      "epoch no.1 train no.110400  loss = 5.97551 avg_loss = 3.77421\n",
      "epoch no.1 train no.110410  loss = 4.80954 avg_loss = 3.81514\n",
      "epoch no.1 train no.110420  loss = 3.74556 avg_loss = 3.79028\n",
      "epoch no.1 train no.110430  loss = 4.28168 avg_loss = 3.74006\n",
      "epoch no.1 train no.110440  loss = 4.35700 avg_loss = 3.78701\n",
      "epoch no.1 train no.110450  loss = 3.58994 avg_loss = 3.78824\n",
      "epoch no.1 train no.110460  loss = 2.73078 avg_loss = 3.76188\n",
      "epoch no.1 train no.110470  loss = 3.51459 avg_loss = 3.80303\n",
      "epoch no.1 train no.110480  loss = 2.68762 avg_loss = 3.80093\n",
      "epoch no.1 train no.110490  loss = 2.14511 avg_loss = 3.85271\n",
      "epoch no.1 train no.110500  loss = 2.73838 avg_loss = 3.81797\n",
      "epoch no.1 train no.110510  loss = 3.30827 avg_loss = 3.79597\n",
      "epoch no.1 train no.110520  loss = 4.45140 avg_loss = 3.83049\n",
      "epoch no.1 train no.110530  loss = 4.71726 avg_loss = 3.84638\n",
      "epoch no.1 train no.110540  loss = 5.07947 avg_loss = 3.83633\n",
      "epoch no.1 train no.110550  loss = 3.11638 avg_loss = 3.78393\n",
      "epoch no.1 train no.110560  loss = 3.27752 avg_loss = 3.76896\n",
      "epoch no.1 train no.110570  loss = 4.06811 avg_loss = 3.74329\n",
      "epoch no.1 train no.110580  loss = 4.47259 avg_loss = 3.76456\n",
      "epoch no.1 train no.110590  loss = 2.71919 avg_loss = 3.68609\n",
      "epoch no.1 train no.110600  loss = 3.42592 avg_loss = 3.65577\n",
      "epoch no.1 train no.110610  loss = 3.48653 avg_loss = 3.71946\n",
      "epoch no.1 train no.110620  loss = 4.01174 avg_loss = 3.69767\n",
      "epoch no.1 train no.110630  loss = 3.18786 avg_loss = 3.70758\n",
      "epoch no.1 train no.110640  loss = 2.96199 avg_loss = 3.71698\n",
      "epoch no.1 train no.110650  loss = 3.63881 avg_loss = 3.63208\n",
      "epoch no.1 train no.110660  loss = 3.61958 avg_loss = 3.61627\n",
      "epoch no.1 train no.110670  loss = 5.43914 avg_loss = 3.66380\n",
      "epoch no.1 train no.110680  loss = 4.74110 avg_loss = 3.64037\n",
      "epoch no.1 train no.110690  loss = 2.74213 avg_loss = 3.64296\n",
      "epoch no.1 train no.110700  loss = 2.22562 avg_loss = 3.63257\n",
      "epoch no.1 train no.110710  loss = 1.95873 avg_loss = 3.55510\n",
      "epoch no.1 train no.110720  loss = 3.64469 avg_loss = 3.60972\n",
      "epoch no.1 train no.110730  loss = 4.12918 avg_loss = 3.56547\n",
      "epoch no.1 train no.110740  loss = 2.79043 avg_loss = 3.53065\n",
      "epoch no.1 train no.110750  loss = 3.54976 avg_loss = 3.51470\n",
      "epoch no.1 train no.110760  loss = 6.76086 avg_loss = 3.58551\n",
      "epoch no.1 train no.110770  loss = 3.71854 avg_loss = 3.58867\n",
      "epoch no.1 train no.110780  loss = 2.97695 avg_loss = 3.59711\n",
      "epoch no.1 train no.110790  loss = 4.03989 avg_loss = 3.61369\n",
      "epoch no.1 train no.110800  loss = 4.69499 avg_loss = 3.63110\n",
      "epoch no.1 train no.110810  loss = 3.52656 avg_loss = 3.63071\n",
      "epoch no.1 train no.110820  loss = 2.82076 avg_loss = 3.60865\n",
      "epoch no.1 train no.110830  loss = 4.44594 avg_loss = 3.64455\n",
      "epoch no.1 train no.110840  loss = 2.98776 avg_loss = 3.62737\n",
      "epoch no.1 train no.110850  loss = 2.30892 avg_loss = 3.60703\n",
      "epoch no.1 train no.110860  loss = 6.61269 avg_loss = 3.63713\n",
      "epoch no.1 train no.110870  loss = 2.42372 avg_loss = 3.62937\n",
      "epoch no.1 train no.110880  loss = 5.24694 avg_loss = 3.65184\n",
      "epoch no.1 train no.110890  loss = 3.74019 avg_loss = 3.63658\n",
      "epoch no.1 train no.110900  loss = 4.38696 avg_loss = 3.60578\n",
      "epoch no.1 train no.110910  loss = 3.01641 avg_loss = 3.58097\n",
      "epoch no.1 train no.110920  loss = 3.19717 avg_loss = 3.59805\n",
      "epoch no.1 train no.110930  loss = 2.26060 avg_loss = 3.55289\n",
      "epoch no.1 train no.110940  loss = 2.33943 avg_loss = 3.55306\n",
      "epoch no.1 train no.110950  loss = 5.57902 avg_loss = 3.54700\n",
      "epoch no.1 train no.110960  loss = 5.11572 avg_loss = 3.55302\n",
      "epoch no.1 train no.110970  loss = 2.96877 avg_loss = 3.47432\n",
      "epoch no.1 train no.110980  loss = 2.93934 avg_loss = 3.45385\n",
      "epoch no.1 train no.110990  loss = 3.52524 avg_loss = 3.51059\n",
      "epoch no.1 train no.111000  loss = 4.27331 avg_loss = 3.53345\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.1 train no.111010  loss = 2.72457 avg_loss = 3.52664\n",
      "epoch no.1 train no.111020  loss = 4.85451 avg_loss = 3.48691\n",
      "epoch no.1 train no.111030  loss = 2.29806 avg_loss = 3.47686\n",
      "epoch no.1 train no.111040  loss = 3.52725 avg_loss = 3.49930\n",
      "epoch no.1 train no.111050  loss = 4.30454 avg_loss = 3.50612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.111060  loss = 1.30069 avg_loss = 3.52881\n",
      "epoch no.1 train no.111070  loss = 5.49537 avg_loss = 3.59464\n",
      "epoch no.1 train no.111080  loss = 3.98385 avg_loss = 3.64169\n",
      "epoch no.1 train no.111090  loss = 3.28796 avg_loss = 3.60933\n",
      "epoch no.1 train no.111100  loss = 2.05696 avg_loss = 3.65540\n",
      "epoch no.1 train no.111110  loss = 4.08367 avg_loss = 3.62752\n",
      "epoch no.1 train no.111120  loss = 5.53767 avg_loss = 3.67746\n",
      "epoch no.1 train no.111130  loss = 4.09076 avg_loss = 3.64789\n",
      "epoch no.1 train no.111140  loss = 4.19539 avg_loss = 3.64746\n",
      "epoch no.1 train no.111150  loss = 5.07347 avg_loss = 3.71340\n",
      "epoch no.1 train no.111160  loss = 2.75473 avg_loss = 3.70302\n",
      "epoch no.1 train no.111170  loss = 3.58726 avg_loss = 3.75523\n",
      "epoch no.1 train no.111180  loss = 4.16892 avg_loss = 3.74981\n",
      "epoch no.1 train no.111190  loss = 4.16595 avg_loss = 3.68570\n",
      "epoch no.1 train no.111200  loss = 3.27782 avg_loss = 3.69880\n",
      "epoch no.1 train no.111210  loss = 3.06843 avg_loss = 3.69114\n",
      "epoch no.1 train no.111220  loss = 4.84316 avg_loss = 3.69759\n",
      "epoch no.1 train no.111230  loss = 2.66047 avg_loss = 3.64996\n",
      "epoch no.1 train no.111240  loss = 4.34556 avg_loss = 3.70616\n",
      "epoch no.1 train no.111250  loss = 3.44736 avg_loss = 3.65941\n",
      "epoch no.1 train no.111260  loss = 4.75944 avg_loss = 3.66624\n",
      "epoch no.1 train no.111270  loss = 2.14110 avg_loss = 3.63824\n",
      "epoch no.1 train no.111280  loss = 3.48161 avg_loss = 3.63176\n",
      "epoch no.1 train no.111290  loss = 4.07864 avg_loss = 3.63588\n",
      "epoch no.1 train no.111300  loss = 4.55663 avg_loss = 3.61048\n",
      "epoch no.1 train no.111310  loss = 3.27368 avg_loss = 3.61213\n",
      "epoch no.1 train no.111320  loss = 6.69232 avg_loss = 3.62699\n",
      "epoch no.1 train no.111330  loss = 2.81386 avg_loss = 3.63068\n",
      "epoch no.1 train no.111340  loss = 3.34263 avg_loss = 3.61470\n",
      "epoch no.1 train no.111350  loss = 4.40198 avg_loss = 3.64942\n",
      "epoch no.1 train no.111360  loss = 3.85319 avg_loss = 3.67664\n",
      "epoch no.1 train no.111370  loss = 4.26466 avg_loss = 3.67297\n",
      "epoch no.1 train no.111380  loss = 3.77709 avg_loss = 3.64879\n",
      "epoch no.1 train no.111390  loss = 3.84932 avg_loss = 3.65946\n",
      "epoch no.1 train no.111400  loss = 2.35190 avg_loss = 3.58258\n",
      "epoch no.1 train no.111410  loss = 3.15499 avg_loss = 3.60449\n",
      "epoch no.1 train no.111420  loss = 2.96203 avg_loss = 3.56480\n",
      "epoch no.1 train no.111430  loss = 4.41673 avg_loss = 3.55733\n",
      "epoch no.1 train no.111440  loss = 2.33751 avg_loss = 3.55048\n",
      "epoch no.1 train no.111450  loss = 2.79976 avg_loss = 3.49738\n",
      "epoch no.1 train no.111460  loss = 5.70050 avg_loss = 3.58135\n",
      "epoch no.1 train no.111470  loss = 2.88216 avg_loss = 3.57411\n",
      "epoch no.1 train no.111480  loss = 1.30863 avg_loss = 3.52163\n",
      "epoch no.1 train no.111490  loss = 3.21107 avg_loss = 3.48955\n",
      "epoch no.1 train no.111500  loss = 3.31344 avg_loss = 3.53124\n",
      "epoch no.1 train no.111510  loss = 5.99132 avg_loss = 3.50147\n",
      "epoch no.1 train no.111520  loss = 3.36924 avg_loss = 3.53277\n",
      "epoch no.1 train no.111530  loss = 3.95516 avg_loss = 3.51461\n",
      "epoch no.1 train no.111540  loss = 2.84320 avg_loss = 3.50682\n",
      "epoch no.1 train no.111550  loss = 4.53235 avg_loss = 3.50186\n",
      "epoch no.1 train no.111560  loss = 3.36511 avg_loss = 3.52066\n",
      "epoch no.1 train no.111570  loss = 3.93825 avg_loss = 3.51706\n",
      "epoch no.1 train no.111580  loss = 3.37204 avg_loss = 3.55042\n",
      "epoch no.1 train no.111590  loss = 4.56720 avg_loss = 3.57339\n",
      "epoch no.1 train no.111600  loss = 3.56898 avg_loss = 3.55180\n",
      "epoch no.1 train no.111610  loss = 3.04439 avg_loss = 3.55640\n",
      "epoch no.1 train no.111620  loss = 3.44526 avg_loss = 3.54297\n",
      "epoch no.1 train no.111630  loss = 2.46550 avg_loss = 3.54853\n",
      "epoch no.1 train no.111640  loss = 2.06284 avg_loss = 3.53246\n",
      "epoch no.1 train no.111650  loss = 4.48441 avg_loss = 3.52467\n",
      "epoch no.1 train no.111660  loss = 2.67457 avg_loss = 3.48570\n",
      "epoch no.1 train no.111670  loss = 3.73213 avg_loss = 3.53539\n",
      "epoch no.1 train no.111680  loss = 1.80868 avg_loss = 3.56441\n",
      "epoch no.1 train no.111690  loss = 2.40974 avg_loss = 3.57038\n",
      "epoch no.1 train no.111700  loss = 2.14227 avg_loss = 3.59573\n",
      "epoch no.1 train no.111710  loss = 2.37139 avg_loss = 3.55573\n",
      "epoch no.1 train no.111720  loss = 2.26677 avg_loss = 3.51315\n",
      "epoch no.1 train no.111730  loss = 3.22909 avg_loss = 3.49484\n",
      "epoch no.1 train no.111740  loss = 4.95179 avg_loss = 3.53841\n",
      "epoch no.1 train no.111750  loss = 4.17184 avg_loss = 3.58028\n",
      "epoch no.1 train no.111760  loss = 3.25467 avg_loss = 3.61629\n",
      "epoch no.1 train no.111770  loss = 3.25124 avg_loss = 3.62132\n",
      "epoch no.1 train no.111780  loss = 5.12285 avg_loss = 3.61994\n",
      "epoch no.1 train no.111790  loss = 2.83214 avg_loss = 3.64922\n",
      "epoch no.1 train no.111800  loss = 4.16295 avg_loss = 3.61610\n",
      "epoch no.1 train no.111810  loss = 3.35386 avg_loss = 3.57960\n",
      "epoch no.1 train no.111820  loss = 2.16290 avg_loss = 3.52576\n",
      "epoch no.1 train no.111830  loss = 5.61542 avg_loss = 3.55947\n",
      "epoch no.1 train no.111840  loss = 2.44235 avg_loss = 3.56573\n",
      "epoch no.1 train no.111850  loss = 2.27239 avg_loss = 3.56685\n",
      "epoch no.1 train no.111860  loss = 2.62013 avg_loss = 3.58369\n",
      "epoch no.1 train no.111870  loss = 4.51631 avg_loss = 3.57998\n",
      "epoch no.1 train no.111880  loss = 2.66883 avg_loss = 3.57581\n",
      "epoch no.1 train no.111890  loss = 5.25931 avg_loss = 3.57523\n",
      "epoch no.1 train no.111900  loss = 3.31847 avg_loss = 3.58605\n",
      "epoch no.1 train no.111910  loss = 1.85190 avg_loss = 3.61634\n",
      "epoch no.1 train no.111920  loss = 1.75973 avg_loss = 3.61283\n",
      "epoch no.1 train no.111930  loss = 1.93630 avg_loss = 3.58643\n",
      "epoch no.1 train no.111940  loss = 5.67871 avg_loss = 3.63363\n",
      "epoch no.1 train no.111950  loss = 3.54894 avg_loss = 3.62265\n",
      "epoch no.1 train no.111960  loss = 2.59612 avg_loss = 3.61234\n",
      "epoch no.1 train no.111970  loss = 2.99028 avg_loss = 3.61619\n",
      "epoch no.1 train no.111980  loss = 3.18872 avg_loss = 3.61514\n",
      "epoch no.1 train no.111990  loss = 2.99310 avg_loss = 3.59770\n",
      "epoch no.1 train no.112000  loss = 3.50375 avg_loss = 3.57340\n",
      "3\n",
      "to_tokens: ['▁비', '▁명', '들', '음', '</s>']\n",
      "추억의 노래모음</s>\n",
      "epoch no.1 train no.112010  loss = 3.69592 avg_loss = 3.59153\n",
      "epoch no.1 train no.112020  loss = 3.68235 avg_loss = 3.60637\n",
      "epoch no.1 train no.112030  loss = 4.02902 avg_loss = 3.67587\n",
      "epoch no.1 train no.112040  loss = 3.86772 avg_loss = 3.65841\n",
      "epoch no.1 train no.112050  loss = 3.40360 avg_loss = 3.64609\n",
      "epoch no.1 train no.112060  loss = 4.78243 avg_loss = 3.66423\n",
      "epoch no.1 train no.112070  loss = 3.03321 avg_loss = 3.68389\n",
      "epoch no.1 train no.112080  loss = 3.98658 avg_loss = 3.68394\n",
      "epoch no.1 train no.112090  loss = 2.36987 avg_loss = 3.66143\n",
      "epoch no.1 train no.112100  loss = 3.13868 avg_loss = 3.67823\n",
      "epoch no.1 train no.112110  loss = 3.33755 avg_loss = 3.65185\n",
      "epoch no.1 train no.112120  loss = 2.63881 avg_loss = 3.64520\n",
      "epoch no.1 train no.112130  loss = 2.99905 avg_loss = 3.58495\n",
      "epoch no.1 train no.112140  loss = 4.00870 avg_loss = 3.53795\n",
      "epoch no.1 train no.112150  loss = 3.51069 avg_loss = 3.52785\n",
      "epoch no.1 train no.112160  loss = 2.53030 avg_loss = 3.57596\n",
      "epoch no.1 train no.112170  loss = 6.02980 avg_loss = 3.66705\n",
      "epoch no.1 train no.112180  loss = 2.76202 avg_loss = 3.67897\n",
      "epoch no.1 train no.112190  loss = 4.08218 avg_loss = 3.66548\n",
      "epoch no.1 train no.112200  loss = 2.47265 avg_loss = 3.61110\n",
      "epoch no.1 train no.112210  loss = 3.62544 avg_loss = 3.66621\n",
      "epoch no.1 train no.112220  loss = 3.50884 avg_loss = 3.68263\n",
      "epoch no.1 train no.112230  loss = 3.14990 avg_loss = 3.66501\n",
      "epoch no.1 train no.112240  loss = 3.97597 avg_loss = 3.68773\n",
      "epoch no.1 train no.112250  loss = 4.06759 avg_loss = 3.67037\n",
      "epoch no.1 train no.112260  loss = 3.12495 avg_loss = 3.65953\n",
      "epoch no.1 train no.112270  loss = 1.97497 avg_loss = 3.61492\n",
      "epoch no.1 train no.112280  loss = 3.84416 avg_loss = 3.62458\n",
      "epoch no.1 train no.112290  loss = 2.05719 avg_loss = 3.61815\n",
      "epoch no.1 train no.112300  loss = 6.97966 avg_loss = 3.66212\n",
      "epoch no.1 train no.112310  loss = 4.07247 avg_loss = 3.66916\n",
      "epoch no.1 train no.112320  loss = 4.81875 avg_loss = 3.67414\n",
      "epoch no.1 train no.112330  loss = 3.37072 avg_loss = 3.66331\n",
      "epoch no.1 train no.112340  loss = 4.42493 avg_loss = 3.61823\n",
      "epoch no.1 train no.112350  loss = 2.34892 avg_loss = 3.63443\n",
      "epoch no.1 train no.112360  loss = 2.54882 avg_loss = 3.61646\n",
      "epoch no.1 train no.112370  loss = 2.62200 avg_loss = 3.59894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.112380  loss = 3.55704 avg_loss = 3.62354\n",
      "epoch no.1 train no.112390  loss = 3.74793 avg_loss = 3.60623\n",
      "epoch no.1 train no.112400  loss = 1.78977 avg_loss = 3.54784\n",
      "epoch no.1 train no.112410  loss = 3.06729 avg_loss = 3.57096\n",
      "epoch no.1 train no.112420  loss = 3.60080 avg_loss = 3.54829\n",
      "epoch no.1 train no.112430  loss = 3.36752 avg_loss = 3.58089\n",
      "epoch no.1 train no.112440  loss = 3.77822 avg_loss = 3.59859\n",
      "epoch no.1 train no.112450  loss = 4.62709 avg_loss = 3.59691\n",
      "epoch no.1 train no.112460  loss = 2.17648 avg_loss = 3.64787\n",
      "epoch no.1 train no.112470  loss = 3.62271 avg_loss = 3.68196\n",
      "epoch no.1 train no.112480  loss = 2.74839 avg_loss = 3.65399\n",
      "epoch no.1 train no.112490  loss = 2.73805 avg_loss = 3.61151\n",
      "epoch no.1 train no.112500  loss = 3.93734 avg_loss = 3.66477\n",
      "epoch no.1 train no.112510  loss = 4.89361 avg_loss = 3.68191\n",
      "epoch no.1 train no.112520  loss = 3.15238 avg_loss = 3.66427\n",
      "epoch no.1 train no.112530  loss = 2.07985 avg_loss = 3.63228\n",
      "epoch no.1 train no.112540  loss = 2.78558 avg_loss = 3.65826\n",
      "epoch no.1 train no.112550  loss = 3.79070 avg_loss = 3.65975\n",
      "epoch no.1 train no.112560  loss = 3.67172 avg_loss = 3.65299\n",
      "epoch no.1 train no.112570  loss = 4.02037 avg_loss = 3.66339\n",
      "epoch no.1 train no.112580  loss = 4.27717 avg_loss = 3.66097\n",
      "epoch no.1 train no.112590  loss = 6.24716 avg_loss = 3.72829\n",
      "epoch no.1 train no.112600  loss = 2.48827 avg_loss = 3.71952\n",
      "epoch no.1 train no.112610  loss = 3.42438 avg_loss = 3.76322\n",
      "epoch no.1 train no.112620  loss = 2.76904 avg_loss = 3.74275\n",
      "epoch no.1 train no.112630  loss = 3.43935 avg_loss = 3.75296\n",
      "epoch no.1 train no.112640  loss = 4.90733 avg_loss = 3.72364\n",
      "epoch no.1 train no.112650  loss = 2.48383 avg_loss = 3.80324\n",
      "epoch no.1 train no.112660  loss = 2.75800 avg_loss = 3.85101\n",
      "epoch no.1 train no.112670  loss = 4.42967 avg_loss = 3.83460\n",
      "epoch no.1 train no.112680  loss = 3.53344 avg_loss = 3.75374\n",
      "epoch no.1 train no.112690  loss = 4.23509 avg_loss = 3.77270\n",
      "epoch no.1 train no.112700  loss = 4.06996 avg_loss = 3.78732\n",
      "epoch no.1 train no.112710  loss = 5.38097 avg_loss = 3.78990\n",
      "epoch no.1 train no.112720  loss = 2.82308 avg_loss = 3.75560\n",
      "epoch no.1 train no.112730  loss = 2.85124 avg_loss = 3.75876\n",
      "epoch no.1 train no.112740  loss = 4.96990 avg_loss = 3.73211\n",
      "epoch no.1 train no.112750  loss = 2.37803 avg_loss = 3.69385\n",
      "epoch no.1 train no.112760  loss = 3.28808 avg_loss = 3.72110\n",
      "epoch no.1 train no.112770  loss = 3.38350 avg_loss = 3.72451\n",
      "epoch no.1 train no.112780  loss = 7.11928 avg_loss = 3.76280\n",
      "epoch no.1 train no.112790  loss = 2.86574 avg_loss = 3.75129\n",
      "epoch no.1 train no.112800  loss = 3.40281 avg_loss = 3.72218\n",
      "epoch no.1 train no.112810  loss = 3.18562 avg_loss = 3.65766\n",
      "epoch no.1 train no.112820  loss = 3.58458 avg_loss = 3.64147\n",
      "epoch no.1 train no.112830  loss = 4.31377 avg_loss = 3.65420\n",
      "epoch no.1 train no.112840  loss = 3.54005 avg_loss = 3.65034\n",
      "epoch no.1 train no.112850  loss = 4.64626 avg_loss = 3.71047\n",
      "epoch no.1 train no.112860  loss = 3.05489 avg_loss = 3.70171\n",
      "epoch no.1 train no.112870  loss = 4.03102 avg_loss = 3.70089\n",
      "epoch no.1 train no.112880  loss = 4.32819 avg_loss = 3.63303\n",
      "epoch no.1 train no.112890  loss = 2.76717 avg_loss = 3.60907\n",
      "epoch no.1 train no.112900  loss = 5.28738 avg_loss = 3.62611\n",
      "epoch no.1 train no.112910  loss = 4.73543 avg_loss = 3.63172\n",
      "epoch no.1 train no.112920  loss = 4.37582 avg_loss = 3.60772\n",
      "epoch no.1 train no.112930  loss = 3.30576 avg_loss = 3.60930\n",
      "epoch no.1 train no.112940  loss = 2.86019 avg_loss = 3.54067\n",
      "epoch no.1 train no.112950  loss = 6.20592 avg_loss = 3.57821\n",
      "epoch no.1 train no.112960  loss = 2.17684 avg_loss = 3.51224\n",
      "epoch no.1 train no.112970  loss = 2.90540 avg_loss = 3.54106\n",
      "epoch no.1 train no.112980  loss = 3.38991 avg_loss = 3.46504\n",
      "epoch no.1 train no.112990  loss = 5.43753 avg_loss = 3.54422\n",
      "epoch no.1 train no.113000  loss = 3.43793 avg_loss = 3.63315\n",
      "5\n",
      "to_tokens: ['▁가을', '▁명', '들', '▁모음', '▁듣는', '고', '</s>']\n",
      "추억의 노래들 다시듣기</s>\n",
      "epoch no.1 train no.113010  loss = 3.29626 avg_loss = 3.60282\n",
      "epoch no.1 train no.113020  loss = 2.89580 avg_loss = 3.60254\n",
      "epoch no.1 train no.113030  loss = 5.55433 avg_loss = 3.68939\n",
      "epoch no.1 train no.113040  loss = 1.94566 avg_loss = 3.66636\n",
      "epoch no.1 train no.113050  loss = 3.00328 avg_loss = 3.64502\n",
      "epoch no.1 train no.113060  loss = 2.55896 avg_loss = 3.64422\n",
      "epoch no.1 train no.113070  loss = 4.33397 avg_loss = 3.64946\n",
      "epoch no.1 train no.113080  loss = 4.01021 avg_loss = 3.65417\n",
      "epoch no.1 train no.113090  loss = 2.83897 avg_loss = 3.63481\n",
      "epoch no.1 train no.113100  loss = 2.98352 avg_loss = 3.56356\n",
      "epoch no.1 train no.113110  loss = 2.84636 avg_loss = 3.51621\n",
      "epoch no.1 train no.113120  loss = 2.48447 avg_loss = 3.51418\n",
      "epoch no.1 train no.113130  loss = 2.10880 avg_loss = 3.49700\n",
      "epoch no.1 train no.113140  loss = 5.19783 avg_loss = 3.54141\n",
      "epoch no.1 train no.113150  loss = 4.08682 avg_loss = 3.54982\n",
      "epoch no.1 train no.113160  loss = 4.34211 avg_loss = 3.56569\n",
      "epoch no.1 train no.113170  loss = 4.22536 avg_loss = 3.59558\n",
      "epoch no.1 train no.113180  loss = 2.57348 avg_loss = 3.60470\n",
      "epoch no.1 train no.113190  loss = 2.49795 avg_loss = 3.60617\n",
      "epoch no.1 train no.113200  loss = 2.66838 avg_loss = 3.56569\n",
      "epoch no.1 train no.113210  loss = 5.38755 avg_loss = 3.61323\n",
      "epoch no.1 train no.113220  loss = 5.31176 avg_loss = 3.62924\n",
      "epoch no.1 train no.113230  loss = 2.63632 avg_loss = 3.55133\n",
      "epoch no.1 train no.113240  loss = 2.94943 avg_loss = 3.58589\n",
      "epoch no.1 train no.113250  loss = 2.93747 avg_loss = 3.60281\n",
      "epoch no.1 train no.113260  loss = 3.65117 avg_loss = 3.60421\n",
      "epoch no.1 train no.113270  loss = 1.69257 avg_loss = 3.61299\n",
      "epoch no.1 train no.113280  loss = 2.67111 avg_loss = 3.60172\n",
      "epoch no.1 train no.113290  loss = 3.04729 avg_loss = 3.56191\n",
      "epoch no.1 train no.113300  loss = 4.30816 avg_loss = 3.54330\n",
      "epoch no.1 train no.113310  loss = 4.00861 avg_loss = 3.55776\n",
      "epoch no.1 train no.113320  loss = 2.50018 avg_loss = 3.62966\n",
      "epoch no.1 train no.113330  loss = 3.03161 avg_loss = 3.65440\n",
      "epoch no.1 train no.113340  loss = 3.32669 avg_loss = 3.62798\n",
      "epoch no.1 train no.113350  loss = 4.26857 avg_loss = 3.59959\n",
      "epoch no.1 train no.113360  loss = 1.90143 avg_loss = 3.56942\n",
      "epoch no.1 train no.113370  loss = 2.80945 avg_loss = 3.56069\n",
      "epoch no.1 train no.113380  loss = 3.53429 avg_loss = 3.55761\n",
      "epoch no.1 train no.113390  loss = 3.70962 avg_loss = 3.57544\n",
      "epoch no.1 train no.113400  loss = 3.47046 avg_loss = 3.61632\n",
      "epoch no.1 train no.113410  loss = 5.62761 avg_loss = 3.61174\n",
      "epoch no.1 train no.113420  loss = 3.68363 avg_loss = 3.61021\n",
      "epoch no.1 train no.113430  loss = 4.77546 avg_loss = 3.66702\n",
      "epoch no.1 train no.113440  loss = 2.63910 avg_loss = 3.65135\n",
      "epoch no.1 train no.113450  loss = 2.22790 avg_loss = 3.68617\n",
      "epoch no.1 train no.113460  loss = 2.73325 avg_loss = 3.65993\n",
      "epoch no.1 train no.113470  loss = 3.43681 avg_loss = 3.66399\n",
      "epoch no.1 train no.113480  loss = 2.85589 avg_loss = 3.60796\n",
      "epoch no.1 train no.113490  loss = 3.80993 avg_loss = 3.56498\n",
      "epoch no.1 train no.113500  loss = 2.67620 avg_loss = 3.60052\n",
      "epoch no.1 train no.113510  loss = 2.53347 avg_loss = 3.67067\n",
      "epoch no.1 train no.113520  loss = 2.63372 avg_loss = 3.69570\n",
      "epoch no.1 train no.113530  loss = 2.73015 avg_loss = 3.69773\n",
      "epoch no.1 train no.113540  loss = 3.72287 avg_loss = 3.69549\n",
      "epoch no.1 train no.113550  loss = 4.14006 avg_loss = 3.68491\n",
      "epoch no.1 train no.113560  loss = 2.95077 avg_loss = 3.69356\n",
      "epoch no.1 train no.113570  loss = 5.12830 avg_loss = 3.69429\n",
      "epoch no.1 train no.113580  loss = 2.46246 avg_loss = 3.65834\n",
      "epoch no.1 train no.113590  loss = 3.72965 avg_loss = 3.72667\n",
      "epoch no.1 train no.113600  loss = 5.51218 avg_loss = 3.73512\n",
      "epoch no.1 train no.113610  loss = 5.25636 avg_loss = 3.74356\n",
      "epoch no.1 train no.113620  loss = 3.72505 avg_loss = 3.70707\n",
      "epoch no.1 train no.113630  loss = 3.47244 avg_loss = 3.69824\n",
      "epoch no.1 train no.113640  loss = 4.00732 avg_loss = 3.65049\n",
      "epoch no.1 train no.113650  loss = 4.58210 avg_loss = 3.64896\n",
      "epoch no.1 train no.113660  loss = 5.10047 avg_loss = 3.60655\n",
      "epoch no.1 train no.113670  loss = 6.77737 avg_loss = 3.58870\n",
      "epoch no.1 train no.113680  loss = 3.29421 avg_loss = 3.62710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.113690  loss = 5.07442 avg_loss = 3.62831\n",
      "epoch no.1 train no.113700  loss = 4.18213 avg_loss = 3.63514\n",
      "epoch no.1 train no.113710  loss = 3.46698 avg_loss = 3.63683\n",
      "epoch no.1 train no.113720  loss = 5.19529 avg_loss = 3.65476\n",
      "epoch no.1 train no.113730  loss = 3.20780 avg_loss = 3.64902\n",
      "epoch no.1 train no.113740  loss = 2.82533 avg_loss = 3.62038\n",
      "epoch no.1 train no.113750  loss = 3.70492 avg_loss = 3.64202\n",
      "epoch no.1 train no.113760  loss = 3.16661 avg_loss = 3.67136\n",
      "epoch no.1 train no.113770  loss = 2.87948 avg_loss = 3.66017\n",
      "epoch no.1 train no.113780  loss = 2.16285 avg_loss = 3.67576\n",
      "epoch no.1 train no.113790  loss = 3.24828 avg_loss = 3.73070\n",
      "epoch no.1 train no.113800  loss = 3.44996 avg_loss = 3.67102\n",
      "epoch no.1 train no.113810  loss = 4.57558 avg_loss = 3.70811\n",
      "epoch no.1 train no.113820  loss = 3.87553 avg_loss = 3.69589\n",
      "epoch no.1 train no.113830  loss = 3.75824 avg_loss = 3.72512\n",
      "epoch no.1 train no.113840  loss = 2.28927 avg_loss = 3.72005\n",
      "epoch no.1 train no.113850  loss = 5.34796 avg_loss = 3.74219\n",
      "epoch no.1 train no.113860  loss = 2.16376 avg_loss = 3.72462\n",
      "epoch no.1 train no.113870  loss = 5.38287 avg_loss = 3.74253\n",
      "epoch no.1 train no.113880  loss = 4.20386 avg_loss = 3.76190\n",
      "epoch no.1 train no.113890  loss = 3.85229 avg_loss = 3.70399\n",
      "epoch no.1 train no.113900  loss = 4.10095 avg_loss = 3.69087\n",
      "epoch no.1 train no.113910  loss = 5.26310 avg_loss = 3.71899\n",
      "epoch no.1 train no.113920  loss = 4.85637 avg_loss = 3.80295\n",
      "epoch no.1 train no.113930  loss = 3.01490 avg_loss = 3.73163\n",
      "epoch no.1 train no.113940  loss = 3.01918 avg_loss = 3.77300\n",
      "epoch no.1 train no.113950  loss = 4.96378 avg_loss = 3.81192\n",
      "epoch no.1 train no.113960  loss = 3.66946 avg_loss = 3.80005\n",
      "epoch no.1 train no.113970  loss = 4.35668 avg_loss = 3.74921\n",
      "epoch no.1 train no.113980  loss = 5.94652 avg_loss = 3.77040\n",
      "epoch no.1 train no.113990  loss = 1.81120 avg_loss = 3.74536\n",
      "epoch no.1 train no.114000  loss = 3.94959 avg_loss = 3.71710\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.1 train no.114010  loss = 5.89568 avg_loss = 3.73105\n",
      "epoch no.1 train no.114020  loss = 4.26084 avg_loss = 3.72380\n",
      "epoch no.1 train no.114030  loss = 5.01086 avg_loss = 3.70601\n",
      "epoch no.1 train no.114040  loss = 4.96927 avg_loss = 3.68709\n",
      "epoch no.1 train no.114050  loss = 5.84523 avg_loss = 3.68068\n",
      "epoch no.1 train no.114060  loss = 5.66126 avg_loss = 3.71342\n",
      "epoch no.1 train no.114070  loss = 3.31595 avg_loss = 3.70402\n",
      "epoch no.1 train no.114080  loss = 2.89003 avg_loss = 3.71876\n",
      "epoch no.1 train no.114090  loss = 4.21356 avg_loss = 3.71445\n",
      "epoch no.1 train no.114100  loss = 3.40894 avg_loss = 3.75005\n",
      "epoch no.1 train no.114110  loss = 4.71556 avg_loss = 3.76546\n",
      "epoch no.1 train no.114120  loss = 2.48900 avg_loss = 3.76007\n",
      "epoch no.1 train no.114130  loss = 5.64866 avg_loss = 3.78006\n",
      "epoch no.1 train no.114140  loss = 4.53839 avg_loss = 3.75974\n",
      "epoch no.1 train no.114150  loss = 2.04001 avg_loss = 3.73114\n",
      "epoch no.1 train no.114160  loss = 3.11884 avg_loss = 3.65612\n",
      "epoch no.1 train no.114170  loss = 4.37164 avg_loss = 3.69384\n",
      "epoch no.1 train no.114180  loss = 5.91871 avg_loss = 3.70150\n",
      "epoch no.1 train no.114190  loss = 3.97011 avg_loss = 3.72049\n",
      "epoch no.1 train no.114200  loss = 5.16889 avg_loss = 3.72209\n",
      "epoch no.1 train no.114210  loss = 2.51008 avg_loss = 3.67965\n",
      "epoch no.1 train no.114220  loss = 3.59537 avg_loss = 3.71272\n",
      "epoch no.1 train no.114230  loss = 2.89283 avg_loss = 3.76530\n",
      "epoch no.1 train no.114240  loss = 5.57948 avg_loss = 3.78936\n",
      "epoch no.1 train no.114250  loss = 5.77889 avg_loss = 3.80760\n",
      "epoch no.1 train no.114260  loss = 3.39401 avg_loss = 3.77198\n",
      "epoch no.1 train no.114270  loss = 3.13277 avg_loss = 3.82488\n",
      "epoch no.1 train no.114280  loss = 2.21026 avg_loss = 3.78462\n",
      "epoch no.1 train no.114290  loss = 3.12314 avg_loss = 3.80975\n",
      "epoch no.1 train no.114300  loss = 6.40800 avg_loss = 3.82448\n",
      "epoch no.1 train no.114310  loss = 3.59839 avg_loss = 3.81639\n",
      "epoch no.1 train no.114320  loss = 1.71881 avg_loss = 3.78395\n",
      "epoch no.1 train no.114330  loss = 5.71802 avg_loss = 3.77742\n",
      "epoch no.1 train no.114340  loss = 3.14961 avg_loss = 3.77160\n",
      "epoch no.1 train no.114350  loss = 4.41116 avg_loss = 3.80357\n",
      "epoch no.1 train no.114360  loss = 1.91598 avg_loss = 3.82585\n",
      "epoch no.1 train no.114370  loss = 6.25607 avg_loss = 3.79889\n",
      "epoch no.1 train no.114380  loss = 3.25252 avg_loss = 3.79804\n",
      "epoch no.1 train no.114390  loss = 3.47414 avg_loss = 3.75637\n",
      "epoch no.1 train no.114400  loss = 3.96390 avg_loss = 3.73552\n",
      "epoch no.1 train no.114410  loss = 4.07686 avg_loss = 3.76287\n",
      "epoch no.1 train no.114420  loss = 5.28344 avg_loss = 3.75046\n",
      "epoch no.1 train no.114430  loss = 4.02203 avg_loss = 3.68978\n",
      "epoch no.1 train no.114440  loss = 3.25161 avg_loss = 3.67397\n",
      "epoch no.1 train no.114450  loss = 2.02809 avg_loss = 3.72111\n",
      "epoch no.1 train no.114460  loss = 2.69055 avg_loss = 3.68431\n",
      "epoch no.1 train no.114470  loss = 2.59709 avg_loss = 3.70315\n",
      "epoch no.1 train no.114480  loss = 3.91348 avg_loss = 3.71345\n",
      "epoch no.1 train no.114490  loss = 2.07914 avg_loss = 3.68530\n",
      "epoch no.1 train no.114500  loss = 5.61667 avg_loss = 3.70645\n",
      "epoch no.1 train no.114510  loss = 2.91120 avg_loss = 3.68355\n",
      "epoch no.1 train no.114520  loss = 3.12362 avg_loss = 3.70928\n",
      "epoch no.1 train no.114530  loss = 3.27321 avg_loss = 3.69624\n",
      "epoch no.1 train no.114540  loss = 2.74933 avg_loss = 3.61765\n",
      "epoch no.1 train no.114550  loss = 4.25563 avg_loss = 3.63130\n",
      "epoch no.1 train no.114560  loss = 3.53825 avg_loss = 3.59731\n",
      "epoch no.1 train no.114570  loss = 5.38703 avg_loss = 3.59194\n",
      "epoch no.1 train no.114580  loss = 2.85751 avg_loss = 3.56526\n",
      "epoch no.1 train no.114590  loss = 4.61855 avg_loss = 3.60308\n",
      "epoch no.1 train no.114600  loss = 2.90259 avg_loss = 3.60462\n",
      "epoch no.1 train no.114610  loss = 4.48958 avg_loss = 3.57664\n",
      "epoch no.1 train no.114620  loss = 3.89601 avg_loss = 3.59348\n",
      "epoch no.1 train no.114630  loss = 3.47355 avg_loss = 3.59825\n",
      "epoch no.1 train no.114640  loss = 5.16789 avg_loss = 3.66216\n",
      "epoch no.1 train no.114650  loss = 5.35655 avg_loss = 3.71266\n",
      "epoch no.1 train no.114660  loss = 2.53773 avg_loss = 3.68638\n",
      "epoch no.1 train no.114670  loss = 4.51505 avg_loss = 3.69496\n",
      "epoch no.1 train no.114680  loss = 1.99398 avg_loss = 3.66865\n",
      "epoch no.1 train no.114690  loss = 2.74655 avg_loss = 3.67070\n",
      "epoch no.1 train no.114700  loss = 2.83990 avg_loss = 3.65674\n",
      "epoch no.1 train no.114710  loss = 2.35168 avg_loss = 3.63450\n",
      "epoch no.1 train no.114720  loss = 2.81301 avg_loss = 3.59121\n",
      "epoch no.1 train no.114730  loss = 3.67627 avg_loss = 3.62666\n",
      "epoch no.1 train no.114740  loss = 3.92212 avg_loss = 3.63542\n",
      "epoch no.1 train no.114750  loss = 4.21702 avg_loss = 3.64934\n",
      "epoch no.1 train no.114760  loss = 2.21266 avg_loss = 3.69499\n",
      "epoch no.1 train no.114770  loss = 4.35053 avg_loss = 3.66980\n",
      "epoch no.1 train no.114780  loss = 3.50251 avg_loss = 3.66895\n",
      "epoch no.1 train no.114790  loss = 3.67436 avg_loss = 3.69109\n",
      "epoch no.1 train no.114800  loss = 3.07160 avg_loss = 3.66727\n",
      "epoch no.1 train no.114810  loss = 4.70969 avg_loss = 3.63898\n",
      "epoch no.1 train no.114820  loss = 2.05334 avg_loss = 3.63086\n",
      "epoch no.1 train no.114830  loss = 1.60606 avg_loss = 3.59099\n",
      "epoch no.1 train no.114840  loss = 2.87613 avg_loss = 3.60262\n",
      "epoch no.1 train no.114850  loss = 5.99075 avg_loss = 3.62931\n",
      "epoch no.1 train no.114860  loss = 2.67996 avg_loss = 3.61969\n",
      "epoch no.1 train no.114870  loss = 2.62585 avg_loss = 3.66707\n",
      "epoch no.1 train no.114880  loss = 3.30911 avg_loss = 3.62083\n",
      "epoch no.1 train no.114890  loss = 3.13584 avg_loss = 3.57153\n",
      "epoch no.1 train no.114900  loss = 2.22499 avg_loss = 3.53581\n",
      "epoch no.1 train no.114910  loss = 5.42601 avg_loss = 3.54549\n",
      "epoch no.1 train no.114920  loss = 5.68817 avg_loss = 3.60059\n",
      "epoch no.1 train no.114930  loss = 3.55544 avg_loss = 3.65529\n",
      "epoch no.1 train no.114940  loss = 4.78193 avg_loss = 3.65109\n",
      "epoch no.1 train no.114950  loss = 2.27787 avg_loss = 3.70496\n",
      "epoch no.1 train no.114960  loss = 3.81165 avg_loss = 3.73761\n",
      "epoch no.1 train no.114970  loss = 4.99408 avg_loss = 3.78669\n",
      "epoch no.1 train no.114980  loss = 3.41685 avg_loss = 3.80323\n",
      "epoch no.1 train no.114990  loss = 3.72121 avg_loss = 3.77734\n",
      "epoch no.1 train no.115000  loss = 3.86283 avg_loss = 3.82199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.1 train no.115010  loss = 3.17829 avg_loss = 3.80067\n",
      "epoch no.1 train no.115020  loss = 6.29896 avg_loss = 3.78222\n",
      "epoch no.1 train no.115030  loss = 4.67917 avg_loss = 3.80489\n",
      "epoch no.1 train no.115040  loss = 3.69583 avg_loss = 3.80271\n",
      "epoch no.1 train no.115050  loss = 4.89788 avg_loss = 3.83443\n",
      "epoch no.1 train no.115060  loss = 3.00964 avg_loss = 3.78808\n",
      "epoch no.1 train no.115070  loss = 4.71394 avg_loss = 3.74018\n",
      "epoch no.1 train no.115080  loss = 4.39125 avg_loss = 3.74054\n",
      "epoch no.1 train no.115090  loss = 3.55723 avg_loss = 3.76781\n",
      "epoch no.1 train no.115100  loss = 3.25590 avg_loss = 3.73835\n",
      "epoch no.1 train no.115110  loss = 2.78425 avg_loss = 3.71576\n",
      "epoch no.1 train no.115120  loss = 4.31546 avg_loss = 3.70528\n",
      "epoch no.1 train no.115130  loss = 4.77407 avg_loss = 3.70715\n",
      "epoch no.1 train no.115140  loss = 5.48545 avg_loss = 3.76264\n",
      "epoch no.1 train no.115150  loss = 3.14897 avg_loss = 3.74430\n",
      "epoch no.1 train no.115160  loss = 4.47817 avg_loss = 3.69680\n",
      "epoch no.1 train no.115170  loss = 2.95357 avg_loss = 3.64852\n",
      "epoch no.1 train no.115180  loss = 4.07466 avg_loss = 3.60217\n",
      "epoch no.1 train no.115190  loss = 3.60734 avg_loss = 3.57885\n",
      "epoch no.1 train no.115200  loss = 2.40733 avg_loss = 3.55178\n",
      "epoch no.1 train no.115210  loss = 3.00451 avg_loss = 3.57078\n",
      "epoch no.1 train no.115220  loss = 1.71847 avg_loss = 3.54429\n",
      "epoch no.1 train no.115230  loss = 3.46932 avg_loss = 3.56424\n",
      "epoch no.1 train no.115240  loss = 4.00249 avg_loss = 3.56298\n",
      "epoch no.1 train no.115250  loss = 3.48085 avg_loss = 3.57732\n",
      "epoch no.1 train no.115260  loss = 3.23866 avg_loss = 3.53492\n",
      "epoch no.1 train no.115270  loss = 4.00575 avg_loss = 3.59313\n",
      "epoch no.1 train no.115280  loss = 4.10892 avg_loss = 3.60464\n",
      "epoch no.1 train no.115290  loss = 2.08652 avg_loss = 3.59318\n",
      "epoch no.1 train no.115300  loss = 3.71236 avg_loss = 3.59084\n",
      "epoch no.1 train no.115310  loss = 2.77115 avg_loss = 3.60230\n",
      "epoch no.1 train no.115320  loss = 5.00998 avg_loss = 3.62873\n",
      "epoch no.1 train no.115330  loss = 2.87631 avg_loss = 3.62980\n",
      "epoch no.1 train no.115340  loss = 5.11140 avg_loss = 3.66274\n",
      "epoch no.1 train no.115350  loss = 4.29476 avg_loss = 3.67890\n",
      "epoch no.1 train no.115360  loss = 1.04861 avg_loss = 3.60578\n",
      "epoch no.1 train no.115370  loss = 5.78645 avg_loss = 3.63014\n",
      "epoch no.1 train no.115380  loss = 3.13061 avg_loss = 3.59519\n",
      "epoch no.1 train no.115390  loss = 3.13068 avg_loss = 3.55585\n",
      "epoch no.1 train no.115400  loss = 5.47113 avg_loss = 3.53954\n",
      "epoch no.1 train no.115410  loss = 3.89235 avg_loss = 3.51406\n",
      "epoch no.1 train no.115420  loss = 3.68514 avg_loss = 3.49858\n",
      "epoch no.1 train no.115430  loss = 2.20875 avg_loss = 3.48068\n",
      "epoch no.1 train no.115440  loss = 3.59805 avg_loss = 3.50119\n",
      "epoch no.1 train no.115450  loss = 5.05458 avg_loss = 3.51978\n",
      "epoch no.1 train no.115460  loss = 4.64232 avg_loss = 3.51419\n",
      "epoch no.1 train no.115470  loss = 1.98594 avg_loss = 3.55916\n",
      "epoch no.1 train no.115480  loss = 3.73876 avg_loss = 3.55860\n",
      "epoch no.1 train no.115490  loss = 3.46376 avg_loss = 3.55916\n",
      "epoch no.1 train no.115500  loss = 5.20372 avg_loss = 3.65128\n",
      "epoch no.1 train no.115510  loss = 4.75031 avg_loss = 3.67219\n",
      "epoch no.1 train no.115520  loss = 4.75828 avg_loss = 3.65855\n",
      "epoch no.1 train no.115530  loss = 2.68853 avg_loss = 3.69867\n",
      "epoch no.1 train no.115540  loss = 4.55136 avg_loss = 3.68253\n",
      "epoch no.1 train no.115550  loss = 3.50820 avg_loss = 3.70047\n",
      "epoch no.1 train no.115560  loss = 4.04708 avg_loss = 3.70601\n",
      "epoch no.1 train no.115570  loss = 4.52365 avg_loss = 3.71119\n",
      "epoch no.1 train no.115580  loss = 3.32614 avg_loss = 3.68648\n",
      "epoch no.1 train no.115590  loss = 3.15989 avg_loss = 3.73772\n",
      "epoch no.1 train no.115600  loss = 4.59149 avg_loss = 3.76621\n",
      "epoch no.1 train no.115610  loss = 2.38038 avg_loss = 3.77778\n",
      "epoch no.1 train no.115620  loss = 3.86533 avg_loss = 3.73260\n",
      "epoch no.1 train no.115630  loss = 4.35665 avg_loss = 3.72636\n",
      "epoch no.1 train no.115640  loss = 1.70249 avg_loss = 3.71535\n",
      "epoch no.1 train no.115650  loss = 2.87062 avg_loss = 3.65806\n",
      "epoch no.1 train no.115660  loss = 4.86061 avg_loss = 3.63651\n",
      "epoch no.1 train no.115670  loss = 3.22100 avg_loss = 3.59495\n",
      "epoch no.1 train no.115680  loss = 3.66624 avg_loss = 3.61376\n",
      "epoch no.1 train no.115690  loss = 5.41181 avg_loss = 3.64812\n",
      "epoch no.1 train no.115700  loss = 2.32213 avg_loss = 3.64503\n",
      "epoch no.1 train no.115710  loss = 2.99577 avg_loss = 3.68291\n",
      "epoch no.1 train no.115720  loss = 3.78223 avg_loss = 3.66245\n",
      "epoch no.1 train no.115730  loss = 4.19138 avg_loss = 3.61963\n",
      "epoch no.1 train no.115740  loss = 5.16233 avg_loss = 3.62167\n",
      "epoch no.1 train no.115750  loss = 6.03705 avg_loss = 3.61703\n",
      "epoch no.1 train no.115760  loss = 2.91634 avg_loss = 3.61106\n",
      "epoch no.1 train no.115770  loss = 2.31844 avg_loss = 3.54880\n",
      "epoch no.1 train no.115780  loss = 3.53024 avg_loss = 3.52369\n",
      "epoch no.1 train no.115790  loss = 4.70623 avg_loss = 3.52602\n",
      "epoch no.1 train no.115800  loss = 2.31826 avg_loss = 3.49374\n",
      "epoch no.1 train no.115810  loss = 2.34450 avg_loss = 3.50605\n",
      "epoch no.1 train no.115820  loss = 2.64110 avg_loss = 3.54317\n",
      "epoch no.1 train no.115830  loss = 3.50036 avg_loss = 3.56595\n",
      "epoch no.1 train no.115840  loss = 3.27904 avg_loss = 3.56324\n",
      "epoch no.1 train no.115850  loss = 5.61728 avg_loss = 3.59716\n",
      "epoch no.1 train no.115860  loss = 4.48717 avg_loss = 3.57045\n",
      "epoch no.1 train no.115870  loss = 2.23959 avg_loss = 3.57758\n",
      "epoch no.1 train no.115880  loss = 4.16429 avg_loss = 3.58929\n",
      "epoch no.1 train no.115890  loss = 5.11112 avg_loss = 3.58051\n",
      "epoch no.1 train no.115900  loss = 2.47499 avg_loss = 3.58161\n",
      "epoch no.1 train no.115910  loss = 4.21673 avg_loss = 3.69973\n",
      "epoch no.1 train no.115920  loss = 3.30952 avg_loss = 3.69122\n",
      "epoch no.1 train no.115930  loss = 3.29150 avg_loss = 3.74097\n",
      "epoch no.1 train no.115940  loss = 2.61489 avg_loss = 3.70075\n",
      "epoch no.1 train no.115950  loss = 5.59347 avg_loss = 3.70699\n",
      "epoch no.1 train no.115960  loss = 2.62960 avg_loss = 3.71506\n",
      "epoch no.1 train no.115970  loss = 2.95210 avg_loss = 3.71937\n",
      "epoch no.1 train no.115980  loss = 4.43350 avg_loss = 3.72253\n",
      "epoch no.1 train no.115990  loss = 5.49473 avg_loss = 3.74120\n",
      "epoch no.1 train no.116000  loss = 4.49539 avg_loss = 3.74918\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '들', '▁모음', '집', '</s>']\n",
      "추억의 노래들 모음집</s>\n",
      "epoch no.1 train no.116010  loss = 4.29857 avg_loss = 3.72883\n",
      "epoch no.1 train no.116020  loss = 2.61733 avg_loss = 3.70232\n",
      "epoch no.1 train no.116030  loss = 3.00161 avg_loss = 3.69358\n",
      "epoch no.1 train no.116040  loss = 3.61584 avg_loss = 3.67751\n",
      "epoch no.1 train no.116050  loss = 3.17141 avg_loss = 3.67819\n",
      "epoch no.1 train no.116060  loss = 2.19246 avg_loss = 3.65037\n",
      "epoch no.1 train no.116070  loss = 2.38524 avg_loss = 3.60034\n",
      "epoch no.1 train no.116080  loss = 3.50410 avg_loss = 3.52727\n",
      "epoch no.1 train no.116090  loss = 4.44337 avg_loss = 3.54845\n",
      "epoch no.1 train no.116100  loss = 3.55848 avg_loss = 3.55464\n",
      "epoch no.1 train no.116110  loss = 5.23244 avg_loss = 3.55763\n",
      "epoch no.1 train no.116120  loss = 3.49920 avg_loss = 3.53804\n",
      "epoch no.1 train no.116130  loss = 2.65502 avg_loss = 3.49472\n",
      "epoch no.1 train no.116140  loss = 2.59479 avg_loss = 3.48402\n",
      "epoch no.1 train no.116150  loss = 3.36264 avg_loss = 3.39647\n",
      "epoch no.1 train no.116160  loss = 3.99907 avg_loss = 3.49229\n",
      "epoch no.1 train no.116170  loss = 5.20962 avg_loss = 3.54661\n",
      "epoch no.1 train no.116180  loss = 1.74990 avg_loss = 3.51212\n",
      "epoch no.1 train no.116190  loss = 3.84072 avg_loss = 3.51447\n",
      "epoch no.1 train no.116200  loss = 2.32449 avg_loss = 3.48155\n",
      "epoch no.1 train no.116210  loss = 2.37741 avg_loss = 3.52348\n",
      "epoch no.1 train no.116220  loss = 3.71423 avg_loss = 3.55165\n",
      "epoch no.1 train no.116230  loss = 2.91573 avg_loss = 3.54346\n",
      "epoch no.1 train no.116240  loss = 3.45158 avg_loss = 3.59924\n",
      "epoch no.1 train no.116250  loss = 3.82653 avg_loss = 3.61336\n",
      "epoch no.1 train no.116260  loss = 2.44444 avg_loss = 3.55799\n",
      "epoch no.1 train no.116270  loss = 3.48683 avg_loss = 3.67871\n",
      "epoch no.1 train no.116280  loss = 5.65181 avg_loss = 3.68678\n",
      "epoch no.1 train no.116290  loss = 2.16693 avg_loss = 3.72617\n",
      "epoch no.1 train no.116300  loss = 2.40368 avg_loss = 3.73791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.116310  loss = 4.29090 avg_loss = 3.68225\n",
      "epoch no.1 train no.116320  loss = 3.01287 avg_loss = 3.67423\n",
      "epoch no.1 train no.116330  loss = 4.21393 avg_loss = 3.67723\n",
      "epoch no.1 train no.116340  loss = 2.41696 avg_loss = 3.68875\n",
      "epoch no.1 train no.116350  loss = 4.67472 avg_loss = 3.65495\n",
      "epoch no.1 train no.116360  loss = 4.66439 avg_loss = 3.67935\n",
      "epoch no.1 train no.116370  loss = 2.30297 avg_loss = 3.65292\n",
      "epoch no.1 train no.116380  loss = 5.34695 avg_loss = 3.62321\n",
      "epoch no.1 train no.116390  loss = 3.10866 avg_loss = 3.64299\n",
      "epoch no.1 train no.116400  loss = 3.00940 avg_loss = 3.65514\n",
      "epoch no.1 train no.116410  loss = 2.33468 avg_loss = 3.59538\n",
      "epoch no.1 train no.116420  loss = 4.34009 avg_loss = 3.62620\n",
      "epoch no.1 train no.116430  loss = 3.21906 avg_loss = 3.61934\n",
      "epoch no.1 train no.116440  loss = 3.11687 avg_loss = 3.69561\n",
      "epoch no.1 train no.116450  loss = 4.38036 avg_loss = 3.65057\n",
      "epoch no.1 train no.116460  loss = 3.35131 avg_loss = 3.62357\n",
      "epoch no.1 train no.116470  loss = 2.47594 avg_loss = 3.62891\n",
      "epoch no.1 train no.116480  loss = 4.14034 avg_loss = 3.63128\n",
      "epoch no.1 train no.116490  loss = 3.08906 avg_loss = 3.64240\n",
      "epoch no.1 train no.116500  loss = 4.40366 avg_loss = 3.64668\n",
      "epoch no.1 train no.116510  loss = 2.72000 avg_loss = 3.58439\n",
      "epoch no.1 train no.116520  loss = 3.46303 avg_loss = 3.62054\n",
      "epoch no.1 train no.116530  loss = 4.98607 avg_loss = 3.65798\n",
      "epoch no.1 train no.116540  loss = 5.95961 avg_loss = 3.61091\n",
      "epoch no.1 train no.116550  loss = 2.71484 avg_loss = 3.61865\n",
      "epoch no.1 train no.116560  loss = 3.66461 avg_loss = 3.64133\n",
      "epoch no.1 train no.116570  loss = 3.92301 avg_loss = 3.73291\n",
      "epoch no.1 train no.116580  loss = 3.98398 avg_loss = 3.71263\n",
      "epoch no.1 train no.116590  loss = 3.82101 avg_loss = 3.70657\n",
      "epoch no.1 train no.116600  loss = 3.04030 avg_loss = 3.68957\n",
      "epoch no.1 train no.116610  loss = 4.98279 avg_loss = 3.68610\n",
      "epoch no.1 train no.116620  loss = 2.85552 avg_loss = 3.71682\n",
      "epoch no.1 train no.116630  loss = 4.57244 avg_loss = 3.73286\n",
      "epoch no.1 train no.116640  loss = 2.58707 avg_loss = 3.70869\n",
      "epoch no.1 train no.116650  loss = 2.89371 avg_loss = 3.69686\n",
      "epoch no.1 train no.116660  loss = 5.39836 avg_loss = 3.75681\n",
      "epoch no.1 train no.116670  loss = 3.73252 avg_loss = 3.74500\n",
      "epoch no.1 train no.116680  loss = 3.60658 avg_loss = 3.74074\n",
      "epoch no.1 train no.116690  loss = 1.97937 avg_loss = 3.70306\n",
      "epoch no.1 train no.116700  loss = 2.25769 avg_loss = 3.62527\n",
      "epoch no.1 train no.116710  loss = 5.15460 avg_loss = 3.64008\n",
      "epoch no.1 train no.116720  loss = 5.53350 avg_loss = 3.59353\n",
      "epoch no.1 train no.116730  loss = 3.07748 avg_loss = 3.59737\n",
      "epoch no.1 train no.116740  loss = 3.57607 avg_loss = 3.59025\n",
      "epoch no.1 train no.116750  loss = 4.58461 avg_loss = 3.59027\n",
      "epoch no.1 train no.116760  loss = 4.71267 avg_loss = 3.58242\n",
      "epoch no.1 train no.116770  loss = 3.88003 avg_loss = 3.62085\n",
      "epoch no.1 train no.116780  loss = 2.69750 avg_loss = 3.65374\n",
      "epoch no.1 train no.116790  loss = 6.06639 avg_loss = 3.72388\n",
      "epoch no.1 train no.116800  loss = 1.91354 avg_loss = 3.78082\n",
      "epoch no.1 train no.116810  loss = 3.32206 avg_loss = 3.76619\n",
      "epoch no.1 train no.116820  loss = 2.01098 avg_loss = 3.73356\n",
      "epoch no.1 train no.116830  loss = 3.56614 avg_loss = 3.71211\n",
      "epoch no.1 train no.116840  loss = 3.93537 avg_loss = 3.66747\n",
      "epoch no.1 train no.116850  loss = 3.47088 avg_loss = 3.62222\n",
      "epoch no.1 train no.116860  loss = 2.55628 avg_loss = 3.57979\n",
      "epoch no.1 train no.116870  loss = 3.77223 avg_loss = 3.58430\n",
      "epoch no.1 train no.116880  loss = 2.41337 avg_loss = 3.52977\n",
      "epoch no.1 train no.116890  loss = 2.32929 avg_loss = 3.50004\n",
      "epoch no.1 train no.116900  loss = 3.03048 avg_loss = 3.50684\n",
      "epoch no.1 train no.116910  loss = 2.77833 avg_loss = 3.54435\n",
      "epoch no.1 train no.116920  loss = 3.24855 avg_loss = 3.53191\n",
      "epoch no.1 train no.116930  loss = 3.14196 avg_loss = 3.52222\n",
      "epoch no.1 train no.116940  loss = 3.83719 avg_loss = 3.53274\n",
      "epoch no.1 train no.116950  loss = 4.75999 avg_loss = 3.55110\n",
      "epoch no.1 train no.116960  loss = 4.07511 avg_loss = 3.56061\n",
      "epoch no.1 train no.116970  loss = 3.23838 avg_loss = 3.51570\n",
      "epoch no.1 train no.116980  loss = 2.65492 avg_loss = 3.52130\n",
      "epoch no.1 train no.116990  loss = 2.90667 avg_loss = 3.50806\n",
      "epoch no.1 train no.117000  loss = 3.42352 avg_loss = 3.50914\n",
      "3\n",
      "to_tokens: ['▁가을', '▁발라', '▁o', 'st', '▁모음']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.1 train no.117010  loss = 3.66736 avg_loss = 3.51596\n",
      "epoch no.1 train no.117020  loss = 4.55669 avg_loss = 3.55110\n",
      "epoch no.1 train no.117030  loss = 2.92714 avg_loss = 3.54176\n",
      "epoch no.1 train no.117040  loss = 3.31825 avg_loss = 3.56303\n",
      "epoch no.1 train no.117050  loss = 6.25170 avg_loss = 3.60773\n",
      "epoch no.1 train no.117060  loss = 2.35068 avg_loss = 3.59598\n",
      "epoch no.1 train no.117070  loss = 2.94847 avg_loss = 3.63735\n",
      "epoch no.1 train no.117080  loss = 3.19454 avg_loss = 3.58485\n",
      "epoch no.1 train no.117090  loss = 3.66123 avg_loss = 3.59906\n",
      "epoch no.1 train no.117100  loss = 2.17684 avg_loss = 3.59837\n",
      "epoch no.1 train no.117110  loss = 4.71649 avg_loss = 3.68715\n",
      "epoch no.1 train no.117120  loss = 3.36927 avg_loss = 3.70050\n",
      "epoch no.1 train no.117130  loss = 5.97991 avg_loss = 3.71706\n",
      "epoch no.1 train no.117140  loss = 4.47293 avg_loss = 3.70449\n",
      "epoch no.1 train no.117150  loss = 3.61805 avg_loss = 3.73595\n",
      "epoch no.1 train no.117160  loss = 3.09713 avg_loss = 3.69063\n",
      "epoch no.1 train no.117170  loss = 3.91230 avg_loss = 3.64985\n",
      "epoch no.1 train no.117180  loss = 5.02289 avg_loss = 3.66741\n",
      "epoch no.1 train no.117190  loss = 3.77667 avg_loss = 3.64357\n",
      "epoch no.1 train no.117200  loss = 3.23336 avg_loss = 3.62618\n",
      "epoch no.1 train no.117210  loss = 2.38337 avg_loss = 3.56460\n",
      "epoch no.1 train no.117220  loss = 2.36292 avg_loss = 3.60036\n",
      "epoch no.1 train no.117230  loss = 4.05034 avg_loss = 3.57366\n",
      "epoch no.1 train no.117240  loss = 4.59445 avg_loss = 3.58910\n",
      "epoch no.1 train no.117250  loss = 2.35986 avg_loss = 3.59689\n",
      "epoch no.1 train no.117260  loss = 3.92654 avg_loss = 3.56067\n",
      "epoch no.1 train no.117270  loss = 5.00390 avg_loss = 3.58755\n",
      "epoch no.1 train no.117280  loss = 2.62160 avg_loss = 3.52596\n",
      "epoch no.1 train no.117290  loss = 3.40850 avg_loss = 3.58864\n",
      "epoch no.1 train no.117300  loss = 3.18079 avg_loss = 3.56482\n",
      "epoch no.1 train no.117310  loss = 2.56987 avg_loss = 3.52781\n",
      "epoch no.1 train no.117320  loss = 2.73032 avg_loss = 3.53069\n",
      "epoch no.1 train no.117330  loss = 3.05572 avg_loss = 3.54361\n",
      "epoch no.1 train no.117340  loss = 1.87427 avg_loss = 3.48614\n",
      "epoch no.1 train no.117350  loss = 3.52278 avg_loss = 3.44715\n",
      "epoch no.1 train no.117360  loss = 4.99266 avg_loss = 3.45954\n",
      "epoch no.1 train no.117370  loss = 2.90596 avg_loss = 3.44483\n",
      "epoch no.1 train no.117380  loss = 4.09560 avg_loss = 3.45445\n",
      "epoch no.1 train no.117390  loss = 3.39127 avg_loss = 3.49229\n",
      "epoch no.1 train no.117400  loss = 2.23611 avg_loss = 3.48983\n",
      "epoch no.1 train no.117410  loss = 4.60568 avg_loss = 3.52899\n",
      "epoch no.1 train no.117420  loss = 2.58041 avg_loss = 3.53450\n",
      "epoch no.1 train no.117430  loss = 3.95905 avg_loss = 3.54948\n",
      "epoch no.1 train no.117440  loss = 4.53954 avg_loss = 3.56470\n",
      "epoch no.1 train no.117450  loss = 3.01796 avg_loss = 3.59271\n",
      "epoch no.1 train no.117460  loss = 4.33979 avg_loss = 3.60464\n",
      "epoch no.1 train no.117470  loss = 5.07142 avg_loss = 3.61660\n",
      "epoch no.1 train no.117480  loss = 4.18062 avg_loss = 3.58076\n",
      "epoch no.1 train no.117490  loss = 2.82149 avg_loss = 3.57138\n",
      "epoch no.1 train no.117500  loss = 3.90489 avg_loss = 3.57893\n",
      "epoch no.1 train no.117510  loss = 3.13054 avg_loss = 3.58883\n",
      "epoch no.1 train no.117520  loss = 3.69432 avg_loss = 3.58264\n",
      "epoch no.1 train no.117530  loss = 2.59775 avg_loss = 3.64518\n",
      "epoch no.1 train no.117540  loss = 4.14385 avg_loss = 3.67702\n",
      "epoch no.1 train no.117550  loss = 1.64747 avg_loss = 3.65244\n",
      "epoch no.1 train no.117560  loss = 2.93011 avg_loss = 3.65915\n",
      "epoch no.1 train no.117570  loss = 3.02840 avg_loss = 3.66713\n",
      "epoch no.1 train no.117580  loss = 5.87700 avg_loss = 3.71195\n",
      "epoch no.1 train no.117590  loss = 2.00181 avg_loss = 3.66050\n",
      "epoch no.1 train no.117600  loss = 3.78711 avg_loss = 3.65246\n",
      "epoch no.1 train no.117610  loss = 3.93677 avg_loss = 3.68153\n",
      "epoch no.1 train no.117620  loss = 3.80304 avg_loss = 3.72344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.117630  loss = 3.14213 avg_loss = 3.64573\n",
      "epoch no.1 train no.117640  loss = 3.31091 avg_loss = 3.63918\n",
      "epoch no.1 train no.117650  loss = 3.80739 avg_loss = 3.61159\n",
      "epoch no.1 train no.117660  loss = 4.17037 avg_loss = 3.62718\n",
      "epoch no.1 train no.117670  loss = 6.36134 avg_loss = 3.66208\n",
      "epoch no.1 train no.117680  loss = 2.67058 avg_loss = 3.66126\n",
      "epoch no.1 train no.117690  loss = 2.03917 avg_loss = 3.66883\n",
      "epoch no.1 train no.117700  loss = 3.09397 avg_loss = 3.66927\n",
      "epoch no.1 train no.117710  loss = 3.51293 avg_loss = 3.65028\n",
      "epoch no.1 train no.117720  loss = 4.11208 avg_loss = 3.68407\n",
      "epoch no.1 train no.117730  loss = 5.14039 avg_loss = 3.65597\n",
      "epoch no.1 train no.117740  loss = 4.17548 avg_loss = 3.66713\n",
      "epoch no.1 train no.117750  loss = 4.91069 avg_loss = 3.66800\n",
      "epoch no.1 train no.117760  loss = 4.78917 avg_loss = 3.65791\n",
      "epoch no.1 train no.117770  loss = 4.18549 avg_loss = 3.66246\n",
      "epoch no.1 train no.117780  loss = 5.00996 avg_loss = 3.71159\n",
      "epoch no.1 train no.117790  loss = 1.97033 avg_loss = 3.74014\n",
      "epoch no.1 train no.117800  loss = 3.34633 avg_loss = 3.73888\n",
      "epoch no.1 train no.117810  loss = 2.34869 avg_loss = 3.70849\n",
      "epoch no.1 train no.117820  loss = 4.89770 avg_loss = 3.70478\n",
      "epoch no.1 train no.117830  loss = 2.85240 avg_loss = 3.71282\n",
      "epoch no.1 train no.117840  loss = 3.42439 avg_loss = 3.67154\n",
      "epoch no.1 train no.117850  loss = 3.84118 avg_loss = 3.65546\n",
      "epoch no.1 train no.117860  loss = 3.67353 avg_loss = 3.66809\n",
      "epoch no.1 train no.117870  loss = 3.25900 avg_loss = 3.68641\n",
      "epoch no.1 train no.117880  loss = 2.80485 avg_loss = 3.70582\n",
      "epoch no.1 train no.117890  loss = 6.29326 avg_loss = 3.72612\n",
      "epoch no.1 train no.117900  loss = 3.73197 avg_loss = 3.68474\n",
      "epoch no.1 train no.117910  loss = 3.25745 avg_loss = 3.70750\n",
      "epoch no.1 train no.117920  loss = 2.03546 avg_loss = 3.72649\n",
      "epoch no.1 train no.117930  loss = 2.69193 avg_loss = 3.68919\n",
      "epoch no.1 train no.117940  loss = 7.01158 avg_loss = 3.74021\n",
      "epoch no.1 train no.117950  loss = 4.38500 avg_loss = 3.72480\n",
      "epoch no.1 train no.117960  loss = 2.77355 avg_loss = 3.73459\n",
      "epoch no.1 train no.117970  loss = 4.57009 avg_loss = 3.75120\n",
      "epoch no.1 train no.117980  loss = 2.23961 avg_loss = 3.68864\n",
      "epoch no.1 train no.117990  loss = 4.42373 avg_loss = 3.72581\n",
      "epoch no.1 train no.118000  loss = 6.59456 avg_loss = 3.77042\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '▁시절', '▁그', '▁노래', '</s>']\n",
      "추억의 그 시절 그 노래</s>\n",
      "epoch no.1 train no.118010  loss = 3.34284 avg_loss = 3.78872\n",
      "epoch no.1 train no.118020  loss = 4.62247 avg_loss = 3.75053\n",
      "epoch no.1 train no.118030  loss = 2.88036 avg_loss = 3.73281\n",
      "epoch no.1 train no.118040  loss = 2.19396 avg_loss = 3.68872\n",
      "epoch no.1 train no.118050  loss = 4.92985 avg_loss = 3.70585\n",
      "epoch no.1 train no.118060  loss = 3.79353 avg_loss = 3.70979\n",
      "epoch no.1 train no.118070  loss = 4.51859 avg_loss = 3.70522\n",
      "epoch no.1 train no.118080  loss = 3.35264 avg_loss = 3.70658\n",
      "epoch no.1 train no.118090  loss = 5.71336 avg_loss = 3.67930\n",
      "epoch no.1 train no.118100  loss = 2.65942 avg_loss = 3.67692\n",
      "epoch no.1 train no.118110  loss = 4.72871 avg_loss = 3.68779\n",
      "epoch no.1 train no.118120  loss = 3.32118 avg_loss = 3.76125\n",
      "epoch no.1 train no.118130  loss = 3.67487 avg_loss = 3.74456\n",
      "epoch no.1 train no.118140  loss = 4.29796 avg_loss = 3.71974\n",
      "epoch no.1 train no.118150  loss = 2.25016 avg_loss = 3.66075\n",
      "epoch no.1 train no.118160  loss = 4.09974 avg_loss = 3.68128\n",
      "epoch no.1 train no.118170  loss = 6.02289 avg_loss = 3.72270\n",
      "epoch no.1 train no.118180  loss = 3.89843 avg_loss = 3.71477\n",
      "epoch no.1 train no.118190  loss = 6.41201 avg_loss = 3.77701\n",
      "epoch no.1 train no.118200  loss = 3.66292 avg_loss = 3.84208\n",
      "epoch no.1 train no.118210  loss = 4.22676 avg_loss = 3.87398\n",
      "epoch no.1 train no.118220  loss = 3.67859 avg_loss = 3.83294\n",
      "epoch no.1 train no.118230  loss = 5.15759 avg_loss = 3.85501\n",
      "epoch no.1 train no.118240  loss = 3.39939 avg_loss = 3.84323\n",
      "epoch no.1 train no.118250  loss = 2.27490 avg_loss = 3.84845\n",
      "epoch no.1 train no.118260  loss = 2.35197 avg_loss = 3.81600\n",
      "epoch no.1 train no.118270  loss = 4.77613 avg_loss = 3.76043\n",
      "epoch no.1 train no.118280  loss = 6.20165 avg_loss = 3.82500\n",
      "epoch no.1 train no.118290  loss = 4.39061 avg_loss = 3.83848\n",
      "epoch no.1 train no.118300  loss = 3.77054 avg_loss = 3.78697\n",
      "epoch no.1 train no.118310  loss = 5.58767 avg_loss = 3.74915\n",
      "epoch no.1 train no.118320  loss = 2.94206 avg_loss = 3.73110\n",
      "epoch no.1 train no.118330  loss = 3.02500 avg_loss = 3.69918\n",
      "epoch no.1 train no.118340  loss = 3.58482 avg_loss = 3.65821\n",
      "epoch no.1 train no.118350  loss = 3.15993 avg_loss = 3.64096\n",
      "epoch no.1 train no.118360  loss = 4.44395 avg_loss = 3.65643\n",
      "epoch no.1 train no.118370  loss = 2.97104 avg_loss = 3.69277\n",
      "epoch no.1 train no.118380  loss = 3.76641 avg_loss = 3.69578\n",
      "epoch no.1 train no.118390  loss = 2.59371 avg_loss = 3.70358\n",
      "epoch no.1 train no.118400  loss = 3.89829 avg_loss = 3.76173\n",
      "epoch no.1 train no.118410  loss = 2.27352 avg_loss = 3.73364\n",
      "epoch no.1 train no.118420  loss = 3.59656 avg_loss = 3.72165\n",
      "epoch no.1 train no.118430  loss = 2.59985 avg_loss = 3.75383\n",
      "epoch no.1 train no.118440  loss = 5.01664 avg_loss = 3.75512\n",
      "epoch no.1 train no.118450  loss = 2.94886 avg_loss = 3.78579\n",
      "epoch no.1 train no.118460  loss = 4.45187 avg_loss = 3.77046\n",
      "epoch no.1 train no.118470  loss = 3.88357 avg_loss = 3.73876\n",
      "epoch no.1 train no.118480  loss = 2.72475 avg_loss = 3.74924\n",
      "epoch no.1 train no.118490  loss = 3.77164 avg_loss = 3.74581\n",
      "epoch no.1 train no.118500  loss = 4.09815 avg_loss = 3.76619\n",
      "epoch no.1 train no.118510  loss = 6.10861 avg_loss = 3.75038\n",
      "epoch no.1 train no.118520  loss = 4.68691 avg_loss = 3.74237\n",
      "epoch no.1 train no.118530  loss = 2.33284 avg_loss = 3.71759\n",
      "epoch no.1 train no.118540  loss = 3.25516 avg_loss = 3.74162\n",
      "epoch no.1 train no.118550  loss = 2.01638 avg_loss = 3.69705\n",
      "epoch no.1 train no.118560  loss = 2.06410 avg_loss = 3.70030\n",
      "epoch no.1 train no.118570  loss = 6.37049 avg_loss = 3.72687\n",
      "epoch no.1 train no.118580  loss = 2.58923 avg_loss = 3.79272\n",
      "epoch no.1 train no.118590  loss = 4.23721 avg_loss = 3.77441\n",
      "epoch no.1 train no.118600  loss = 2.83354 avg_loss = 3.75027\n",
      "epoch no.1 train no.118610  loss = 3.06755 avg_loss = 3.70487\n",
      "epoch no.1 train no.118620  loss = 4.05314 avg_loss = 3.66420\n",
      "epoch no.1 train no.118630  loss = 2.18586 avg_loss = 3.64621\n",
      "epoch no.1 train no.118640  loss = 2.89880 avg_loss = 3.62408\n",
      "epoch no.1 train no.118650  loss = 4.59846 avg_loss = 3.58175\n",
      "epoch no.1 train no.118660  loss = 8.56423 avg_loss = 3.60365\n",
      "epoch no.1 train no.118670  loss = 3.67216 avg_loss = 3.64060\n",
      "epoch no.1 train no.118680  loss = 3.70961 avg_loss = 3.68857\n",
      "epoch no.1 train no.118690  loss = 4.58681 avg_loss = 3.66522\n",
      "epoch no.1 train no.118700  loss = 5.02437 avg_loss = 3.71258\n",
      "epoch no.1 train no.118710  loss = 2.59043 avg_loss = 3.69033\n",
      "epoch no.1 train no.118720  loss = 3.80832 avg_loss = 3.67136\n",
      "epoch no.1 train no.118730  loss = 2.87812 avg_loss = 3.70655\n",
      "epoch no.1 train no.118740  loss = 3.55410 avg_loss = 3.66768\n",
      "epoch no.1 train no.118750  loss = 3.75042 avg_loss = 3.63078\n",
      "epoch no.1 train no.118760  loss = 3.42195 avg_loss = 3.65562\n",
      "epoch no.1 train no.118770  loss = 4.80321 avg_loss = 3.66086\n",
      "epoch no.1 train no.118780  loss = 3.18473 avg_loss = 3.65130\n",
      "epoch no.1 train no.118790  loss = 3.54673 avg_loss = 3.63142\n",
      "epoch no.1 train no.118800  loss = 3.09312 avg_loss = 3.65719\n",
      "epoch no.1 train no.118810  loss = 4.17758 avg_loss = 3.68892\n",
      "epoch no.1 train no.118820  loss = 3.78163 avg_loss = 3.68170\n",
      "epoch no.1 train no.118830  loss = 3.81313 avg_loss = 3.67239\n",
      "epoch no.1 train no.118840  loss = 4.66902 avg_loss = 3.63723\n",
      "epoch no.1 train no.118850  loss = 2.95952 avg_loss = 3.62644\n",
      "epoch no.1 train no.118860  loss = 3.66613 avg_loss = 3.65132\n",
      "epoch no.1 train no.118870  loss = 3.18147 avg_loss = 3.63645\n",
      "epoch no.1 train no.118880  loss = 3.73798 avg_loss = 3.63362\n",
      "epoch no.1 train no.118890  loss = 4.89507 avg_loss = 3.61459\n",
      "epoch no.1 train no.118900  loss = 4.13812 avg_loss = 3.58132\n",
      "epoch no.1 train no.118910  loss = 3.62526 avg_loss = 3.59577\n",
      "epoch no.1 train no.118920  loss = 3.78464 avg_loss = 3.64746\n",
      "epoch no.1 train no.118930  loss = 3.31264 avg_loss = 3.63952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.118940  loss = 2.76022 avg_loss = 3.61251\n",
      "epoch no.1 train no.118950  loss = 5.97464 avg_loss = 3.64043\n",
      "epoch no.1 train no.118960  loss = 2.10031 avg_loss = 3.60938\n",
      "epoch no.1 train no.118970  loss = 4.96716 avg_loss = 3.62970\n",
      "epoch no.1 train no.118980  loss = 2.00349 avg_loss = 3.65060\n",
      "epoch no.1 train no.118990  loss = 3.85723 avg_loss = 3.67027\n",
      "epoch no.1 train no.119000  loss = 5.30338 avg_loss = 3.68012\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.1 train no.119010  loss = 3.45401 avg_loss = 3.65854\n",
      "epoch no.1 train no.119020  loss = 2.66021 avg_loss = 3.63127\n",
      "epoch no.1 train no.119030  loss = 3.83027 avg_loss = 3.57467\n",
      "epoch no.1 train no.119040  loss = 3.02133 avg_loss = 3.54108\n",
      "epoch no.1 train no.119050  loss = 4.15693 avg_loss = 3.52886\n",
      "epoch no.1 train no.119060  loss = 2.28298 avg_loss = 3.54510\n",
      "epoch no.1 train no.119070  loss = 3.50931 avg_loss = 3.53054\n",
      "epoch no.1 train no.119080  loss = 3.31544 avg_loss = 3.55347\n",
      "epoch no.1 train no.119090  loss = 4.97279 avg_loss = 3.62786\n",
      "epoch no.1 train no.119100  loss = 5.68288 avg_loss = 3.66526\n",
      "epoch no.1 train no.119110  loss = 5.40728 avg_loss = 3.67670\n",
      "epoch no.1 train no.119120  loss = 2.07252 avg_loss = 3.63177\n",
      "epoch no.1 train no.119130  loss = 4.35775 avg_loss = 3.64058\n",
      "epoch no.1 train no.119140  loss = 3.57128 avg_loss = 3.60826\n",
      "epoch no.1 train no.119150  loss = 3.54093 avg_loss = 3.61147\n",
      "epoch no.1 train no.119160  loss = 2.97017 avg_loss = 3.60549\n",
      "epoch no.1 train no.119170  loss = 3.28688 avg_loss = 3.62005\n",
      "epoch no.1 train no.119180  loss = 2.46298 avg_loss = 3.59787\n",
      "epoch no.1 train no.119190  loss = 2.29490 avg_loss = 3.58503\n",
      "epoch no.1 train no.119200  loss = 3.34460 avg_loss = 3.59657\n",
      "epoch no.1 train no.119210  loss = 3.60053 avg_loss = 3.58702\n",
      "epoch no.1 train no.119220  loss = 5.03690 avg_loss = 3.60577\n",
      "epoch no.1 train no.119230  loss = 2.54930 avg_loss = 3.57120\n",
      "epoch no.1 train no.119240  loss = 4.11914 avg_loss = 3.52906\n",
      "epoch no.1 train no.119250  loss = 1.75532 avg_loss = 3.52911\n",
      "epoch no.1 train no.119260  loss = 5.83997 avg_loss = 3.58072\n",
      "epoch no.1 train no.119270  loss = 4.18903 avg_loss = 3.56367\n",
      "epoch no.1 train no.119280  loss = 3.99391 avg_loss = 3.53568\n",
      "epoch no.1 train no.119290  loss = 3.35914 avg_loss = 3.53029\n",
      "epoch no.1 train no.119300  loss = 2.66395 avg_loss = 3.54686\n",
      "epoch no.1 train no.119310  loss = 3.23637 avg_loss = 3.53724\n",
      "epoch no.1 train no.119320  loss = 1.79890 avg_loss = 3.51162\n",
      "epoch no.1 train no.119330  loss = 4.49473 avg_loss = 3.54882\n",
      "epoch no.1 train no.119340  loss = 4.15280 avg_loss = 3.58795\n",
      "epoch no.1 train no.119350  loss = 1.80714 avg_loss = 3.56197\n",
      "epoch no.1 train no.119360  loss = 2.07806 avg_loss = 3.52949\n",
      "epoch no.1 train no.119370  loss = 2.92663 avg_loss = 3.46854\n",
      "epoch no.1 train no.119380  loss = 4.12442 avg_loss = 3.58374\n",
      "epoch no.1 train no.119390  loss = 4.73124 avg_loss = 3.58804\n",
      "epoch no.1 train no.119400  loss = 3.21816 avg_loss = 3.59840\n",
      "epoch no.1 train no.119410  loss = 3.00415 avg_loss = 3.58845\n",
      "epoch no.1 train no.119420  loss = 3.27991 avg_loss = 3.63058\n",
      "epoch no.1 train no.119430  loss = 4.35198 avg_loss = 3.65476\n",
      "epoch no.1 train no.119440  loss = 3.38109 avg_loss = 3.76684\n",
      "epoch no.1 train no.119450  loss = 1.72268 avg_loss = 3.76673\n",
      "epoch no.1 train no.119460  loss = 3.47506 avg_loss = 3.71884\n",
      "epoch no.1 train no.119470  loss = 4.23765 avg_loss = 3.72294\n",
      "epoch no.1 train no.119480  loss = 2.86698 avg_loss = 3.79470\n",
      "epoch no.1 train no.119490  loss = 3.48736 avg_loss = 3.78484\n",
      "epoch no.1 train no.119500  loss = 2.29562 avg_loss = 3.77092\n",
      "epoch no.1 train no.119510  loss = 2.89346 avg_loss = 3.76190\n",
      "epoch no.1 train no.119520  loss = 1.98136 avg_loss = 3.76851\n",
      "epoch no.1 train no.119530  loss = 2.80590 avg_loss = 3.80717\n",
      "epoch no.1 train no.119540  loss = 3.59381 avg_loss = 3.82983\n",
      "epoch no.1 train no.119550  loss = 4.39131 avg_loss = 3.86400\n",
      "epoch no.1 train no.119560  loss = 3.51548 avg_loss = 3.81030\n",
      "epoch no.1 train no.119570  loss = 3.98572 avg_loss = 3.84200\n",
      "epoch no.1 train no.119580  loss = 3.32105 avg_loss = 3.82284\n",
      "epoch no.1 train no.119590  loss = 2.92982 avg_loss = 3.78507\n",
      "epoch no.1 train no.119600  loss = 3.73105 avg_loss = 3.77965\n",
      "epoch no.1 train no.119610  loss = 4.46734 avg_loss = 3.78708\n",
      "epoch no.1 train no.119620  loss = 3.79063 avg_loss = 3.74332\n",
      "epoch no.1 train no.119630  loss = 3.87012 avg_loss = 3.77848\n",
      "epoch no.1 train no.119640  loss = 3.57795 avg_loss = 3.77758\n",
      "epoch no.1 train no.119650  loss = 3.24203 avg_loss = 3.76028\n",
      "epoch no.1 train no.119660  loss = 3.52927 avg_loss = 3.74598\n",
      "epoch no.1 train no.119670  loss = 3.94566 avg_loss = 3.73166\n",
      "epoch no.1 train no.119680  loss = 2.07408 avg_loss = 3.69280\n",
      "epoch no.1 train no.119690  loss = 2.76495 avg_loss = 3.68060\n",
      "epoch no.1 train no.119700  loss = 3.10574 avg_loss = 3.70206\n",
      "epoch no.1 train no.119710  loss = 2.98749 avg_loss = 3.67796\n",
      "epoch no.1 train no.119720  loss = 2.56050 avg_loss = 3.66592\n",
      "epoch no.1 train no.119730  loss = 2.35345 avg_loss = 3.67374\n",
      "epoch no.1 train no.119740  loss = 5.40905 avg_loss = 3.71885\n",
      "epoch no.1 train no.119750  loss = 2.84438 avg_loss = 3.67682\n",
      "epoch no.1 train no.119760  loss = 4.56751 avg_loss = 3.63973\n",
      "epoch no.1 train no.119770  loss = 3.08870 avg_loss = 3.61146\n",
      "epoch no.1 train no.119780  loss = 4.11785 avg_loss = 3.66432\n",
      "epoch no.1 train no.119790  loss = 2.48370 avg_loss = 3.66890\n",
      "epoch no.1 train no.119800  loss = 2.97691 avg_loss = 3.66596\n",
      "epoch no.1 train no.119810  loss = 3.23685 avg_loss = 3.69705\n",
      "epoch no.1 train no.119820  loss = 2.44895 avg_loss = 3.65754\n",
      "epoch no.1 train no.119830  loss = 3.55675 avg_loss = 3.74660\n",
      "epoch no.1 train no.119840  loss = 3.75180 avg_loss = 3.76620\n",
      "epoch no.1 train no.119850  loss = 3.71706 avg_loss = 3.72669\n",
      "epoch no.1 train no.119860  loss = 4.04246 avg_loss = 3.69426\n",
      "epoch no.1 train no.119870  loss = 3.45641 avg_loss = 3.68943\n",
      "epoch no.1 train no.119880  loss = 1.83759 avg_loss = 3.67947\n",
      "epoch no.1 train no.119890  loss = 3.07059 avg_loss = 3.64828\n",
      "epoch no.1 train no.119900  loss = 2.50094 avg_loss = 3.69407\n",
      "epoch no.1 train no.119910  loss = 2.76765 avg_loss = 3.64704\n",
      "epoch no.1 train no.119920  loss = 2.69300 avg_loss = 3.61502\n",
      "epoch no.1 train no.119930  loss = 4.23263 avg_loss = 3.65956\n",
      "epoch no.1 train no.119940  loss = 2.61515 avg_loss = 3.63110\n",
      "epoch no.1 train no.119950  loss = 3.06691 avg_loss = 3.61576\n",
      "epoch no.1 train no.119960  loss = 4.00372 avg_loss = 3.60551\n",
      "epoch no.1 train no.119970  loss = 3.63699 avg_loss = 3.61323\n",
      "epoch no.1 train no.119980  loss = 2.76458 avg_loss = 3.63297\n",
      "epoch no.1 train no.119990  loss = 1.86325 avg_loss = 3.68729\n",
      "epoch no.1 train no.120000  loss = 2.61243 avg_loss = 3.69378\n",
      "4\n",
      "to_tokens: ['▁비', '▁팝', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.1 train no.120010  loss = 7.73524 avg_loss = 3.72455\n",
      "epoch no.1 train no.120020  loss = 3.12431 avg_loss = 3.69370\n",
      "epoch no.1 train no.120030  loss = 5.98215 avg_loss = 3.73053\n",
      "epoch no.1 train no.120040  loss = 5.22855 avg_loss = 3.73318\n",
      "epoch no.1 train no.120050  loss = 2.55056 avg_loss = 3.81082\n",
      "epoch no.1 train no.120060  loss = 2.65315 avg_loss = 3.83869\n",
      "epoch no.1 train no.120070  loss = 6.45963 avg_loss = 3.82604\n",
      "epoch no.1 train no.120080  loss = 2.11755 avg_loss = 3.79870\n",
      "epoch no.1 train no.120090  loss = 3.42804 avg_loss = 3.70162\n",
      "epoch no.1 train no.120100  loss = 2.64930 avg_loss = 3.72381\n",
      "epoch no.1 train no.120110  loss = 5.56627 avg_loss = 3.69508\n",
      "epoch no.1 train no.120120  loss = 4.07002 avg_loss = 3.73324\n",
      "epoch no.1 train no.120130  loss = 4.75813 avg_loss = 3.76569\n",
      "epoch no.1 train no.120140  loss = 3.41794 avg_loss = 3.76642\n",
      "epoch no.1 train no.120150  loss = 4.15536 avg_loss = 3.73821\n",
      "epoch no.1 train no.120160  loss = 5.59188 avg_loss = 3.75011\n",
      "epoch no.1 train no.120170  loss = 4.58292 avg_loss = 3.80745\n",
      "epoch no.1 train no.120180  loss = 1.68343 avg_loss = 3.79731\n",
      "epoch no.1 train no.120190  loss = 4.45829 avg_loss = 3.77092\n",
      "epoch no.1 train no.120200  loss = 4.96466 avg_loss = 3.78518\n",
      "epoch no.1 train no.120210  loss = 4.60234 avg_loss = 3.81299\n",
      "epoch no.1 train no.120220  loss = 1.93428 avg_loss = 3.71780\n",
      "epoch no.1 train no.120230  loss = 4.89128 avg_loss = 3.72537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.120240  loss = 3.38405 avg_loss = 3.68490\n",
      "epoch no.1 train no.120250  loss = 2.50203 avg_loss = 3.66477\n",
      "epoch no.1 train no.120260  loss = 4.23852 avg_loss = 3.60162\n",
      "epoch no.1 train no.120270  loss = 2.66031 avg_loss = 3.58046\n",
      "epoch no.1 train no.120280  loss = 2.41139 avg_loss = 3.60111\n",
      "epoch no.1 train no.120290  loss = 2.90393 avg_loss = 3.60450\n",
      "epoch no.1 train no.120300  loss = 4.31969 avg_loss = 3.64916\n",
      "epoch no.1 train no.120310  loss = 4.75282 avg_loss = 3.69381\n",
      "epoch no.1 train no.120320  loss = 5.03252 avg_loss = 3.72172\n",
      "epoch no.1 train no.120330  loss = 2.36804 avg_loss = 3.68598\n",
      "epoch no.1 train no.120340  loss = 2.66423 avg_loss = 3.72375\n",
      "epoch no.1 train no.120350  loss = 5.37540 avg_loss = 3.76015\n",
      "epoch no.1 train no.120360  loss = 1.76680 avg_loss = 3.70477\n",
      "epoch no.1 train no.120370  loss = 3.95812 avg_loss = 3.70663\n",
      "epoch no.1 train no.120380  loss = 3.02992 avg_loss = 3.73077\n",
      "epoch no.1 train no.120390  loss = 2.60148 avg_loss = 3.70403\n",
      "epoch no.1 train no.120400  loss = 2.67062 avg_loss = 3.64402\n",
      "epoch no.1 train no.120410  loss = 2.25033 avg_loss = 3.62485\n",
      "epoch no.1 train no.120420  loss = 6.37417 avg_loss = 3.65080\n",
      "epoch no.1 train no.120430  loss = 4.80716 avg_loss = 3.64861\n",
      "epoch no.1 train no.120440  loss = 2.41885 avg_loss = 3.63729\n",
      "epoch no.1 train no.120450  loss = 4.39641 avg_loss = 3.68224\n",
      "epoch no.1 train no.120460  loss = 3.09358 avg_loss = 3.65187\n",
      "epoch no.1 train no.120470  loss = 2.68646 avg_loss = 3.63731\n",
      "epoch no.1 train no.120480  loss = 2.73990 avg_loss = 3.64464\n",
      "epoch no.1 train no.120490  loss = 3.78930 avg_loss = 3.63509\n",
      "epoch no.1 train no.120500  loss = 2.87503 avg_loss = 3.61052\n",
      "epoch no.1 train no.120510  loss = 3.39891 avg_loss = 3.62770\n",
      "epoch no.1 train no.120520  loss = 3.35036 avg_loss = 3.58536\n",
      "epoch no.1 train no.120530  loss = 3.13196 avg_loss = 3.55393\n",
      "epoch no.1 train no.120540  loss = 4.60546 avg_loss = 3.54709\n",
      "epoch no.1 train no.120550  loss = 4.24623 avg_loss = 3.57937\n",
      "epoch no.1 train no.120560  loss = 3.17280 avg_loss = 3.55325\n",
      "epoch no.1 train no.120570  loss = 4.09608 avg_loss = 3.56846\n",
      "epoch no.1 train no.120580  loss = 2.34363 avg_loss = 3.63723\n",
      "epoch no.1 train no.120590  loss = 2.82897 avg_loss = 3.63588\n",
      "epoch no.1 train no.120600  loss = 4.57757 avg_loss = 3.60563\n",
      "epoch no.1 train no.120610  loss = 3.44545 avg_loss = 3.59733\n",
      "epoch no.1 train no.120620  loss = 7.74690 avg_loss = 3.61082\n",
      "epoch no.1 train no.120630  loss = 2.45267 avg_loss = 3.55979\n",
      "epoch no.1 train no.120640  loss = 3.64273 avg_loss = 3.56871\n",
      "epoch no.1 train no.120650  loss = 4.20822 avg_loss = 3.60528\n",
      "epoch no.1 train no.120660  loss = 3.55917 avg_loss = 3.58057\n",
      "epoch no.1 train no.120670  loss = 4.98471 avg_loss = 3.56544\n",
      "epoch no.1 train no.120680  loss = 3.37868 avg_loss = 3.60241\n",
      "epoch no.1 train no.120690  loss = 6.02098 avg_loss = 3.59080\n",
      "epoch no.1 train no.120700  loss = 3.34927 avg_loss = 3.57539\n",
      "epoch no.1 train no.120710  loss = 4.47575 avg_loss = 3.56933\n",
      "epoch no.1 train no.120720  loss = 2.24543 avg_loss = 3.64086\n",
      "epoch no.1 train no.120730  loss = 4.03161 avg_loss = 3.62917\n",
      "epoch no.1 train no.120740  loss = 2.94216 avg_loss = 3.57277\n",
      "epoch no.1 train no.120750  loss = 7.32435 avg_loss = 3.58214\n",
      "epoch no.1 train no.120760  loss = 4.30752 avg_loss = 3.57179\n",
      "epoch no.1 train no.120770  loss = 4.04205 avg_loss = 3.57435\n",
      "epoch no.1 train no.120780  loss = 5.68784 avg_loss = 3.60010\n",
      "epoch no.1 train no.120790  loss = 2.26456 avg_loss = 3.59490\n",
      "epoch no.1 train no.120800  loss = 5.28094 avg_loss = 3.67420\n",
      "epoch no.1 train no.120810  loss = 4.36314 avg_loss = 3.73713\n",
      "epoch no.1 train no.120820  loss = 4.41962 avg_loss = 3.71385\n",
      "epoch no.1 train no.120830  loss = 3.40627 avg_loss = 3.72259\n",
      "epoch no.1 train no.120840  loss = 3.80667 avg_loss = 3.71363\n",
      "epoch no.1 train no.120850  loss = 2.41340 avg_loss = 3.66566\n",
      "epoch no.1 train no.120860  loss = 5.26436 avg_loss = 3.70431\n",
      "epoch no.1 train no.120870  loss = 3.38103 avg_loss = 3.68379\n",
      "epoch no.1 train no.120880  loss = 2.83845 avg_loss = 3.66789\n",
      "epoch no.1 train no.120890  loss = 4.43599 avg_loss = 3.64276\n",
      "epoch no.1 train no.120900  loss = 4.85881 avg_loss = 3.63581\n",
      "epoch no.1 train no.120910  loss = 3.11406 avg_loss = 3.63860\n",
      "epoch no.1 train no.120920  loss = 3.94680 avg_loss = 3.62911\n",
      "epoch no.1 train no.120930  loss = 5.51707 avg_loss = 3.63975\n",
      "epoch no.1 train no.120940  loss = 2.39157 avg_loss = 3.57237\n",
      "epoch no.1 train no.120950  loss = 3.93253 avg_loss = 3.59547\n",
      "epoch no.1 train no.120960  loss = 1.77620 avg_loss = 3.56521\n",
      "epoch no.1 train no.120970  loss = 3.13379 avg_loss = 3.55211\n",
      "epoch no.1 train no.120980  loss = 2.42179 avg_loss = 3.52021\n",
      "epoch no.1 train no.120990  loss = 2.47866 avg_loss = 3.53165\n",
      "epoch no.1 train no.121000  loss = 3.65022 avg_loss = 3.56422\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '팝', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.1 train no.121010  loss = 4.44254 avg_loss = 3.55335\n",
      "epoch no.1 train no.121020  loss = 6.01409 avg_loss = 3.56316\n",
      "epoch no.1 train no.121030  loss = 5.49660 avg_loss = 3.54632\n",
      "epoch no.1 train no.121040  loss = 2.73914 avg_loss = 3.55967\n",
      "epoch no.1 train no.121050  loss = 3.21401 avg_loss = 3.50923\n",
      "epoch no.1 train no.121060  loss = 4.37779 avg_loss = 3.57526\n",
      "epoch no.1 train no.121070  loss = 3.40704 avg_loss = 3.57852\n",
      "epoch no.1 train no.121080  loss = 3.17145 avg_loss = 3.56889\n",
      "epoch no.1 train no.121090  loss = 2.93307 avg_loss = 3.59815\n",
      "epoch no.1 train no.121100  loss = 6.06782 avg_loss = 3.60731\n",
      "epoch no.1 train no.121110  loss = 3.55051 avg_loss = 3.63051\n",
      "epoch no.1 train no.121120  loss = 1.79285 avg_loss = 3.57202\n",
      "epoch no.1 train no.121130  loss = 5.11129 avg_loss = 3.62693\n",
      "epoch no.1 train no.121140  loss = 2.55904 avg_loss = 3.65614\n",
      "epoch no.1 train no.121150  loss = 3.75793 avg_loss = 3.68695\n",
      "epoch no.1 train no.121160  loss = 3.57753 avg_loss = 3.66762\n",
      "epoch no.1 train no.121170  loss = 2.71739 avg_loss = 3.69614\n",
      "epoch no.1 train no.121180  loss = 3.50246 avg_loss = 3.70405\n",
      "epoch no.1 train no.121190  loss = 3.75902 avg_loss = 3.70422\n",
      "epoch no.1 train no.121200  loss = 5.06375 avg_loss = 3.77136\n",
      "epoch no.1 train no.121210  loss = 4.76577 avg_loss = 3.78165\n",
      "epoch no.1 train no.121220  loss = 4.07631 avg_loss = 3.84208\n",
      "epoch no.1 train no.121230  loss = 2.74422 avg_loss = 3.83168\n",
      "epoch no.1 train no.121240  loss = 2.78951 avg_loss = 3.84387\n",
      "epoch no.1 train no.121250  loss = 5.30853 avg_loss = 3.82434\n",
      "epoch no.1 train no.121260  loss = 2.00990 avg_loss = 3.83548\n",
      "epoch no.1 train no.121270  loss = 2.61335 avg_loss = 3.83925\n",
      "epoch no.1 train no.121280  loss = 2.75987 avg_loss = 3.79818\n",
      "epoch no.1 train no.121290  loss = 2.97146 avg_loss = 3.77861\n",
      "epoch no.1 train no.121300  loss = 4.19936 avg_loss = 3.79927\n",
      "epoch no.1 train no.121310  loss = 2.86869 avg_loss = 3.83651\n",
      "epoch no.1 train no.121320  loss = 5.11130 avg_loss = 3.86167\n",
      "epoch no.1 train no.121330  loss = 2.53456 avg_loss = 3.82447\n",
      "epoch no.1 train no.121340  loss = 4.59228 avg_loss = 3.83016\n",
      "epoch no.1 train no.121350  loss = 5.67735 avg_loss = 3.82760\n",
      "epoch no.1 train no.121360  loss = 4.03005 avg_loss = 3.81178\n",
      "epoch no.1 train no.121370  loss = 3.46803 avg_loss = 3.83988\n",
      "epoch no.1 train no.121380  loss = 2.10058 avg_loss = 3.80447\n",
      "epoch no.1 train no.121390  loss = 5.86120 avg_loss = 3.83232\n",
      "epoch no.1 train no.121400  loss = 3.86016 avg_loss = 3.83059\n",
      "epoch no.1 train no.121410  loss = 6.33468 avg_loss = 3.85463\n",
      "epoch no.1 train no.121420  loss = 5.87481 avg_loss = 3.84603\n",
      "epoch no.1 train no.121430  loss = 4.10284 avg_loss = 3.82502\n",
      "epoch no.1 train no.121440  loss = 4.79402 avg_loss = 3.74435\n",
      "epoch no.1 train no.121450  loss = 4.33057 avg_loss = 3.71070\n",
      "epoch no.1 train no.121460  loss = 4.80105 avg_loss = 3.67840\n",
      "epoch no.1 train no.121470  loss = 4.26622 avg_loss = 3.65825\n",
      "epoch no.1 train no.121480  loss = 5.19584 avg_loss = 3.68843\n",
      "epoch no.1 train no.121490  loss = 2.18692 avg_loss = 3.61683\n",
      "epoch no.1 train no.121500  loss = 3.00540 avg_loss = 3.63538\n",
      "epoch no.1 train no.121510  loss = 3.23403 avg_loss = 3.61850\n",
      "epoch no.1 train no.121520  loss = 2.40824 avg_loss = 3.60658\n",
      "epoch no.1 train no.121530  loss = 3.97229 avg_loss = 3.62856\n",
      "epoch no.1 train no.121540  loss = 5.36696 avg_loss = 3.62209\n",
      "epoch no.1 train no.121550  loss = 3.07932 avg_loss = 3.64226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.121560  loss = 3.57508 avg_loss = 3.64503\n",
      "epoch no.1 train no.121570  loss = 4.36538 avg_loss = 3.67314\n",
      "epoch no.1 train no.121580  loss = 2.79376 avg_loss = 3.71695\n",
      "epoch no.1 train no.121590  loss = 3.07429 avg_loss = 3.71107\n",
      "epoch no.1 train no.121600  loss = 4.11199 avg_loss = 3.71992\n",
      "epoch no.1 train no.121610  loss = 3.94254 avg_loss = 3.77160\n",
      "epoch no.1 train no.121620  loss = 3.84860 avg_loss = 3.74192\n",
      "epoch no.1 train no.121630  loss = 4.22428 avg_loss = 3.77388\n",
      "epoch no.1 train no.121640  loss = 3.67432 avg_loss = 3.78656\n",
      "epoch no.1 train no.121650  loss = 3.03192 avg_loss = 3.77381\n",
      "epoch no.1 train no.121660  loss = 2.68232 avg_loss = 3.77075\n",
      "epoch no.1 train no.121670  loss = 5.85010 avg_loss = 3.74705\n",
      "epoch no.1 train no.121680  loss = 3.95816 avg_loss = 3.82425\n",
      "epoch no.1 train no.121690  loss = 2.78615 avg_loss = 3.75948\n",
      "epoch no.1 train no.121700  loss = 5.66350 avg_loss = 3.78225\n",
      "epoch no.1 train no.121710  loss = 2.98294 avg_loss = 3.73514\n",
      "epoch no.1 train no.121720  loss = 3.09141 avg_loss = 3.73030\n",
      "epoch no.1 train no.121730  loss = 2.98695 avg_loss = 3.71321\n",
      "epoch no.1 train no.121740  loss = 4.51965 avg_loss = 3.72936\n",
      "epoch no.1 train no.121750  loss = 2.66049 avg_loss = 3.64931\n",
      "epoch no.1 train no.121760  loss = 2.96519 avg_loss = 3.60750\n",
      "epoch no.1 train no.121770  loss = 4.42495 avg_loss = 3.59018\n",
      "epoch no.1 train no.121780  loss = 3.89721 avg_loss = 3.69027\n",
      "epoch no.1 train no.121790  loss = 1.63202 avg_loss = 3.68982\n",
      "epoch no.1 train no.121800  loss = 3.40671 avg_loss = 3.73457\n",
      "epoch no.1 train no.121810  loss = 3.06261 avg_loss = 3.78069\n",
      "epoch no.1 train no.121820  loss = 3.21209 avg_loss = 3.78275\n",
      "epoch no.1 train no.121830  loss = 4.26375 avg_loss = 3.81692\n",
      "epoch no.1 train no.121840  loss = 5.47742 avg_loss = 3.84107\n",
      "epoch no.1 train no.121850  loss = 4.30781 avg_loss = 3.82239\n",
      "epoch no.1 train no.121860  loss = 3.32072 avg_loss = 3.88377\n",
      "epoch no.1 train no.121870  loss = 4.42708 avg_loss = 3.82637\n",
      "epoch no.1 train no.121880  loss = 3.64731 avg_loss = 3.79630\n",
      "epoch no.1 train no.121890  loss = 3.47905 avg_loss = 3.75770\n",
      "epoch no.1 train no.121900  loss = 4.61352 avg_loss = 3.76259\n",
      "epoch no.1 train no.121910  loss = 1.99983 avg_loss = 3.72657\n",
      "epoch no.1 train no.121920  loss = 4.72394 avg_loss = 3.79023\n",
      "epoch no.1 train no.121930  loss = 2.11360 avg_loss = 3.78927\n",
      "epoch no.1 train no.121940  loss = 3.35545 avg_loss = 3.74828\n",
      "epoch no.1 train no.121950  loss = 3.68729 avg_loss = 3.76393\n",
      "epoch no.1 train no.121960  loss = 4.49965 avg_loss = 3.76827\n",
      "epoch no.1 train no.121970  loss = 4.45689 avg_loss = 3.74088\n",
      "epoch no.1 train no.121980  loss = 5.82679 avg_loss = 3.71359\n",
      "epoch no.1 train no.121990  loss = 4.86376 avg_loss = 3.70562\n",
      "epoch no.1 train no.122000  loss = 4.11203 avg_loss = 3.64460\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '시절', '▁그', '▁노래', '</s>']\n",
      "추억의 그 시절 그 노래</s>\n",
      "epoch no.1 train no.122010  loss = 3.99463 avg_loss = 3.70119\n",
      "epoch no.1 train no.122020  loss = 1.41885 avg_loss = 3.66943\n",
      "epoch no.1 train no.122030  loss = 3.10205 avg_loss = 3.64193\n",
      "epoch no.1 train no.122040  loss = 3.69060 avg_loss = 3.66095\n",
      "epoch no.1 train no.122050  loss = 2.70810 avg_loss = 3.66586\n",
      "epoch no.1 train no.122060  loss = 3.25091 avg_loss = 3.67224\n",
      "epoch no.1 train no.122070  loss = 4.46371 avg_loss = 3.69792\n",
      "epoch no.1 train no.122080  loss = 3.62571 avg_loss = 3.71372\n",
      "epoch no.1 train no.122090  loss = 4.69389 avg_loss = 3.68200\n",
      "epoch no.1 train no.122100  loss = 5.15854 avg_loss = 3.69286\n",
      "epoch no.1 train no.122110  loss = 6.59207 avg_loss = 3.69067\n",
      "epoch no.1 train no.122120  loss = 4.28641 avg_loss = 3.68334\n",
      "epoch no.1 train no.122130  loss = 2.79506 avg_loss = 3.71607\n",
      "epoch no.1 train no.122140  loss = 4.18407 avg_loss = 3.70558\n",
      "epoch no.1 train no.122150  loss = 3.15611 avg_loss = 3.70317\n",
      "epoch no.1 train no.122160  loss = 1.42702 avg_loss = 3.68902\n",
      "epoch no.1 train no.122170  loss = 4.04504 avg_loss = 3.68724\n",
      "epoch no.1 train no.122180  loss = 4.91805 avg_loss = 3.65614\n",
      "epoch no.1 train no.122190  loss = 1.36908 avg_loss = 3.63982\n",
      "epoch no.1 train no.122200  loss = 3.04713 avg_loss = 3.62059\n",
      "epoch no.1 train no.122210  loss = 4.38598 avg_loss = 3.67991\n",
      "epoch no.1 train no.122220  loss = 4.75850 avg_loss = 3.69995\n",
      "epoch no.1 train no.122230  loss = 6.05174 avg_loss = 3.81048\n",
      "epoch no.1 train no.122240  loss = 4.38976 avg_loss = 3.83085\n",
      "epoch no.1 train no.122250  loss = 3.65641 avg_loss = 3.76468\n",
      "epoch no.1 train no.122260  loss = 4.23679 avg_loss = 3.71702\n",
      "epoch no.1 train no.122270  loss = 4.09726 avg_loss = 3.75643\n",
      "epoch no.1 train no.122280  loss = 2.15437 avg_loss = 3.77123\n",
      "epoch no.1 train no.122290  loss = 2.62494 avg_loss = 3.75762\n",
      "epoch no.1 train no.122300  loss = 2.24769 avg_loss = 3.76701\n",
      "epoch no.1 train no.122310  loss = 3.24685 avg_loss = 3.72322\n",
      "epoch no.1 train no.122320  loss = 4.14068 avg_loss = 3.73192\n",
      "epoch no.1 train no.122330  loss = 3.24088 avg_loss = 3.74853\n",
      "epoch no.1 train no.122340  loss = 1.80236 avg_loss = 3.75042\n",
      "epoch no.1 train no.122350  loss = 2.04222 avg_loss = 3.85195\n",
      "epoch no.1 train no.122360  loss = 3.97305 avg_loss = 3.78447\n",
      "epoch no.1 train no.122370  loss = 2.58576 avg_loss = 3.80757\n",
      "epoch no.1 train no.122380  loss = 2.34043 avg_loss = 3.77588\n",
      "epoch no.1 train no.122390  loss = 3.25588 avg_loss = 3.76628\n",
      "epoch no.1 train no.122400  loss = 2.40030 avg_loss = 3.77933\n",
      "epoch no.1 train no.122410  loss = 3.21381 avg_loss = 3.72635\n",
      "epoch no.1 train no.122420  loss = 3.86128 avg_loss = 3.78391\n",
      "epoch no.1 train no.122430  loss = 1.56816 avg_loss = 3.76051\n",
      "epoch no.1 train no.122440  loss = 3.67675 avg_loss = 3.76603\n",
      "epoch no.1 train no.122450  loss = 2.41002 avg_loss = 3.80709\n",
      "epoch no.1 train no.122460  loss = 5.61293 avg_loss = 3.76746\n",
      "epoch no.1 train no.122470  loss = 3.36732 avg_loss = 3.81851\n",
      "epoch no.1 train no.122480  loss = 2.11190 avg_loss = 3.78397\n",
      "epoch no.1 train no.122490  loss = 3.11789 avg_loss = 3.77941\n",
      "epoch no.1 train no.122500  loss = 3.26217 avg_loss = 3.73699\n",
      "epoch no.1 train no.122510  loss = 3.36546 avg_loss = 3.69885\n",
      "epoch no.1 train no.122520  loss = 3.68997 avg_loss = 3.72659\n",
      "epoch no.1 train no.122530  loss = 3.93544 avg_loss = 3.72032\n",
      "epoch no.1 train no.122540  loss = 3.00794 avg_loss = 3.68513\n",
      "epoch no.1 train no.122550  loss = 5.12485 avg_loss = 3.70220\n",
      "epoch no.1 train no.122560  loss = 2.17899 avg_loss = 3.67849\n",
      "epoch no.1 train no.122570  loss = 2.14890 avg_loss = 3.70794\n",
      "epoch no.1 train no.122580  loss = 3.40616 avg_loss = 3.74059\n",
      "epoch no.1 train no.122590  loss = 5.47703 avg_loss = 3.71981\n",
      "epoch no.1 train no.122600  loss = 4.07091 avg_loss = 3.74177\n",
      "epoch no.1 train no.122610  loss = 2.63742 avg_loss = 3.72809\n",
      "epoch no.1 train no.122620  loss = 4.60080 avg_loss = 3.70036\n",
      "epoch no.1 train no.122630  loss = 4.60248 avg_loss = 3.69456\n",
      "epoch no.1 train no.122640  loss = 3.85724 avg_loss = 3.71745\n",
      "epoch no.1 train no.122650  loss = 4.18964 avg_loss = 3.66895\n",
      "epoch no.1 train no.122660  loss = 3.27468 avg_loss = 3.62926\n",
      "epoch no.1 train no.122670  loss = 4.70260 avg_loss = 3.62282\n",
      "epoch no.1 train no.122680  loss = 3.49434 avg_loss = 3.58432\n",
      "epoch no.1 train no.122690  loss = 6.57522 avg_loss = 3.63287\n",
      "epoch no.1 train no.122700  loss = 2.14150 avg_loss = 3.60540\n",
      "epoch no.1 train no.122710  loss = 3.64323 avg_loss = 3.63073\n",
      "epoch no.1 train no.122720  loss = 4.60034 avg_loss = 3.66368\n",
      "epoch no.1 train no.122730  loss = 2.49516 avg_loss = 3.63077\n",
      "epoch no.1 train no.122740  loss = 2.45087 avg_loss = 3.62360\n",
      "epoch no.1 train no.122750  loss = 5.59442 avg_loss = 3.65230\n",
      "epoch no.1 train no.122760  loss = 4.12825 avg_loss = 3.70821\n",
      "epoch no.1 train no.122770  loss = 4.50740 avg_loss = 3.75264\n",
      "epoch no.1 train no.122780  loss = 4.50463 avg_loss = 3.72904\n",
      "epoch no.1 train no.122790  loss = 4.59259 avg_loss = 3.73494\n",
      "epoch no.1 train no.122800  loss = 3.72084 avg_loss = 3.67696\n",
      "epoch no.1 train no.122810  loss = 4.10273 avg_loss = 3.66526\n",
      "epoch no.1 train no.122820  loss = 2.15962 avg_loss = 3.59878\n",
      "epoch no.1 train no.122830  loss = 5.37922 avg_loss = 3.65934\n",
      "epoch no.1 train no.122840  loss = 3.39317 avg_loss = 3.65484\n",
      "epoch no.1 train no.122850  loss = 3.79005 avg_loss = 3.69974\n",
      "epoch no.1 train no.122860  loss = 3.74185 avg_loss = 3.70236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.122870  loss = 2.87441 avg_loss = 3.68110\n",
      "epoch no.1 train no.122880  loss = 3.19405 avg_loss = 3.72423\n",
      "epoch no.1 train no.122890  loss = 7.16443 avg_loss = 3.71300\n",
      "epoch no.1 train no.122900  loss = 3.50819 avg_loss = 3.71827\n",
      "epoch no.1 train no.122910  loss = 3.95627 avg_loss = 3.69691\n",
      "epoch no.1 train no.122920  loss = 4.41417 avg_loss = 3.75070\n",
      "epoch no.1 train no.122930  loss = 4.96971 avg_loss = 3.76065\n",
      "epoch no.1 train no.122940  loss = 3.52129 avg_loss = 3.66948\n",
      "epoch no.1 train no.122950  loss = 2.22552 avg_loss = 3.71326\n",
      "epoch no.1 train no.122960  loss = 6.67414 avg_loss = 3.76231\n",
      "epoch no.1 train no.122970  loss = 2.22973 avg_loss = 3.73186\n",
      "epoch no.1 train no.122980  loss = 3.64405 avg_loss = 3.68924\n",
      "epoch no.1 train no.122990  loss = 3.95270 avg_loss = 3.64465\n",
      "epoch no.1 train no.123000  loss = 4.61221 avg_loss = 3.63268\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '</s>']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.1 train no.123010  loss = 4.80456 avg_loss = 3.59137\n",
      "epoch no.1 train no.123020  loss = 4.84548 avg_loss = 3.60790\n",
      "epoch no.1 train no.123030  loss = 3.06622 avg_loss = 3.62566\n",
      "epoch no.1 train no.123040  loss = 5.01106 avg_loss = 3.66384\n",
      "epoch no.1 train no.123050  loss = 4.61649 avg_loss = 3.65201\n",
      "epoch no.1 train no.123060  loss = 3.80003 avg_loss = 3.70307\n",
      "epoch no.1 train no.123070  loss = 3.19470 avg_loss = 3.69510\n",
      "epoch no.1 train no.123080  loss = 3.18966 avg_loss = 3.69105\n",
      "epoch no.1 train no.123090  loss = 4.71882 avg_loss = 3.75443\n",
      "epoch no.1 train no.123100  loss = 3.06662 avg_loss = 3.70347\n",
      "epoch no.1 train no.123110  loss = 3.02319 avg_loss = 3.71964\n",
      "epoch no.1 train no.123120  loss = 3.76013 avg_loss = 3.67333\n",
      "epoch no.1 train no.123130  loss = 5.32872 avg_loss = 3.65313\n",
      "epoch no.1 train no.123140  loss = 3.90486 avg_loss = 3.65128\n",
      "epoch no.1 train no.123150  loss = 4.77838 avg_loss = 3.67197\n",
      "epoch no.1 train no.123160  loss = 2.79058 avg_loss = 3.68544\n",
      "epoch no.1 train no.123170  loss = 2.75134 avg_loss = 3.66146\n",
      "epoch no.1 train no.123180  loss = 3.14890 avg_loss = 3.68268\n",
      "epoch no.1 train no.123190  loss = 4.18079 avg_loss = 3.64344\n",
      "epoch no.1 train no.123200  loss = 5.15950 avg_loss = 3.67944\n",
      "epoch no.1 train no.123210  loss = 3.87532 avg_loss = 3.62683\n",
      "epoch no.1 train no.123220  loss = 2.31212 avg_loss = 3.63532\n",
      "epoch no.1 train no.123230  loss = 4.75083 avg_loss = 3.66367\n",
      "epoch no.1 train no.123240  loss = 2.37795 avg_loss = 3.63244\n",
      "epoch no.1 train no.123250  loss = 3.97287 avg_loss = 3.67981\n",
      "epoch no.1 train no.123260  loss = 3.26868 avg_loss = 3.73079\n",
      "epoch no.1 train no.123270  loss = 3.77444 avg_loss = 3.72777\n",
      "epoch no.1 train no.123280  loss = 3.44032 avg_loss = 3.74684\n",
      "epoch no.1 train no.123290  loss = 4.66009 avg_loss = 3.72775\n",
      "epoch no.1 train no.123300  loss = 3.93531 avg_loss = 3.71254\n",
      "epoch no.1 train no.123310  loss = 3.51010 avg_loss = 3.72038\n",
      "epoch no.1 train no.123320  loss = 4.81986 avg_loss = 3.77759\n",
      "epoch no.1 train no.123330  loss = 3.91182 avg_loss = 3.82019\n",
      "epoch no.1 train no.123340  loss = 2.16925 avg_loss = 3.76559\n",
      "epoch no.1 train no.123350  loss = 2.51500 avg_loss = 3.76264\n",
      "epoch no.1 train no.123360  loss = 3.53807 avg_loss = 3.73524\n",
      "epoch no.1 train no.123370  loss = 3.37874 avg_loss = 3.69129\n",
      "epoch no.1 train no.123380  loss = 2.69783 avg_loss = 3.68327\n",
      "epoch no.1 train no.123390  loss = 3.04821 avg_loss = 3.64600\n",
      "epoch no.1 train no.123400  loss = 5.22040 avg_loss = 3.68185\n",
      "epoch no.1 train no.123410  loss = 3.82862 avg_loss = 3.68457\n",
      "epoch no.1 train no.123420  loss = 2.61750 avg_loss = 3.66959\n",
      "epoch no.1 train no.123430  loss = 4.81588 avg_loss = 3.69096\n",
      "epoch no.1 train no.123440  loss = 4.78644 avg_loss = 3.69694\n",
      "epoch no.1 train no.123450  loss = 5.35525 avg_loss = 3.68792\n",
      "epoch no.1 train no.123460  loss = 4.84041 avg_loss = 3.74668\n",
      "epoch no.1 train no.123470  loss = 1.68898 avg_loss = 3.70083\n",
      "epoch no.1 train no.123480  loss = 4.31270 avg_loss = 3.67459\n",
      "epoch no.1 train no.123490  loss = 3.79648 avg_loss = 3.65997\n",
      "epoch no.1 train no.123500  loss = 2.65110 avg_loss = 3.68714\n",
      "epoch no.1 train no.123510  loss = 6.41057 avg_loss = 3.68454\n",
      "epoch no.1 train no.123520  loss = 3.55655 avg_loss = 3.70277\n",
      "epoch no.1 train no.123530  loss = 2.54140 avg_loss = 3.70833\n",
      "epoch no.1 train no.123540  loss = 3.33709 avg_loss = 3.70288\n",
      "epoch no.1 train no.123550  loss = 2.73491 avg_loss = 3.66514\n",
      "epoch no.1 train no.123560  loss = 2.46789 avg_loss = 3.67634\n",
      "epoch no.1 train no.123570  loss = 4.80540 avg_loss = 3.71072\n",
      "epoch no.1 train no.123580  loss = 3.79035 avg_loss = 3.69145\n",
      "epoch no.1 train no.123590  loss = 3.02153 avg_loss = 3.64654\n",
      "epoch no.1 train no.123600  loss = 3.93737 avg_loss = 3.59922\n",
      "epoch no.1 train no.123610  loss = 3.13508 avg_loss = 3.64019\n",
      "epoch no.1 train no.123620  loss = 7.01946 avg_loss = 3.68458\n",
      "epoch no.1 train no.123630  loss = 4.74476 avg_loss = 3.68241\n",
      "epoch no.1 train no.123640  loss = 3.24751 avg_loss = 3.69365\n",
      "epoch no.1 train no.123650  loss = 4.12320 avg_loss = 3.64056\n",
      "epoch no.1 train no.123660  loss = 2.59706 avg_loss = 3.61257\n",
      "epoch no.1 train no.123670  loss = 2.55955 avg_loss = 3.55409\n",
      "epoch no.1 train no.123680  loss = 6.18786 avg_loss = 3.55029\n",
      "epoch no.1 train no.123690  loss = 2.87292 avg_loss = 3.56218\n",
      "epoch no.1 train no.123700  loss = 2.11509 avg_loss = 3.52647\n",
      "epoch no.1 train no.123710  loss = 4.24954 avg_loss = 3.60924\n",
      "epoch no.1 train no.123720  loss = 3.29099 avg_loss = 3.58501\n",
      "epoch no.1 train no.123730  loss = 3.66944 avg_loss = 3.57502\n",
      "epoch no.1 train no.123740  loss = 3.61229 avg_loss = 3.62270\n",
      "epoch no.1 train no.123750  loss = 4.20377 avg_loss = 3.63581\n",
      "epoch no.1 train no.123760  loss = 4.78311 avg_loss = 3.64531\n",
      "epoch no.1 train no.123770  loss = 3.87400 avg_loss = 3.64058\n",
      "epoch no.1 train no.123780  loss = 4.50066 avg_loss = 3.63677\n",
      "epoch no.1 train no.123790  loss = 2.50090 avg_loss = 3.61809\n",
      "epoch no.1 train no.123800  loss = 3.83697 avg_loss = 3.59699\n",
      "epoch no.1 train no.123810  loss = 3.27479 avg_loss = 3.58271\n",
      "epoch no.1 train no.123820  loss = 2.17991 avg_loss = 3.54103\n",
      "epoch no.1 train no.123830  loss = 2.39846 avg_loss = 3.52410\n",
      "epoch no.1 train no.123840  loss = 3.73392 avg_loss = 3.52424\n",
      "epoch no.1 train no.123850  loss = 3.28576 avg_loss = 3.51878\n",
      "epoch no.1 train no.123860  loss = 3.94542 avg_loss = 3.50862\n",
      "epoch no.1 train no.123870  loss = 4.20794 avg_loss = 3.50169\n",
      "epoch no.1 train no.123880  loss = 3.89141 avg_loss = 3.55367\n",
      "epoch no.1 train no.123890  loss = 4.38540 avg_loss = 3.56636\n",
      "epoch no.1 train no.123900  loss = 4.28581 avg_loss = 3.60221\n",
      "epoch no.1 train no.123910  loss = 1.86650 avg_loss = 3.55109\n",
      "epoch no.1 train no.123920  loss = 2.53660 avg_loss = 3.58479\n",
      "epoch no.1 train no.123930  loss = 3.05003 avg_loss = 3.64471\n",
      "epoch no.1 train no.123940  loss = 2.32618 avg_loss = 3.62840\n",
      "epoch no.1 train no.123950  loss = 3.22380 avg_loss = 3.69793\n",
      "epoch no.1 train no.123960  loss = 3.57251 avg_loss = 3.72053\n",
      "epoch no.1 train no.123970  loss = 5.66875 avg_loss = 3.76928\n",
      "epoch no.1 train no.123980  loss = 1.74952 avg_loss = 3.76451\n",
      "epoch no.1 train no.123990  loss = 4.05179 avg_loss = 3.80596\n",
      "epoch no.1 train no.124000  loss = 5.52648 avg_loss = 3.80972\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.1 train no.124010  loss = 2.61651 avg_loss = 3.77236\n",
      "epoch no.1 train no.124020  loss = 3.36292 avg_loss = 3.73309\n",
      "epoch no.1 train no.124030  loss = 2.65231 avg_loss = 3.72449\n",
      "epoch no.1 train no.124040  loss = 4.36624 avg_loss = 3.68578\n",
      "epoch no.1 train no.124050  loss = 4.10378 avg_loss = 3.72274\n",
      "epoch no.1 train no.124060  loss = 2.96123 avg_loss = 3.72593\n",
      "epoch no.1 train no.124070  loss = 3.93686 avg_loss = 3.76894\n",
      "epoch no.1 train no.124080  loss = 6.04136 avg_loss = 3.75797\n",
      "epoch no.1 train no.124090  loss = 3.44017 avg_loss = 3.84169\n",
      "epoch no.1 train no.124100  loss = 2.33749 avg_loss = 3.73835\n",
      "epoch no.1 train no.124110  loss = 2.94642 avg_loss = 3.73963\n",
      "epoch no.1 train no.124120  loss = 4.07615 avg_loss = 3.72499\n",
      "epoch no.1 train no.124130  loss = 6.42717 avg_loss = 3.77505\n",
      "epoch no.1 train no.124140  loss = 5.02488 avg_loss = 3.70923\n",
      "epoch no.1 train no.124150  loss = 6.06159 avg_loss = 3.73676\n",
      "epoch no.1 train no.124160  loss = 3.86388 avg_loss = 3.70585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.124170  loss = 4.03562 avg_loss = 3.74533\n",
      "epoch no.1 train no.124180  loss = 2.15417 avg_loss = 3.76153\n",
      "epoch no.1 train no.124190  loss = 3.61005 avg_loss = 3.75794\n",
      "epoch no.1 train no.124200  loss = 4.16084 avg_loss = 3.76289\n",
      "epoch no.1 train no.124210  loss = 2.41905 avg_loss = 3.81556\n",
      "epoch no.1 train no.124220  loss = 2.63530 avg_loss = 3.81079\n",
      "epoch no.1 train no.124230  loss = 3.91657 avg_loss = 3.85465\n",
      "epoch no.1 train no.124240  loss = 3.23826 avg_loss = 3.86663\n",
      "epoch no.1 train no.124250  loss = 2.27972 avg_loss = 3.86238\n",
      "epoch no.1 train no.124260  loss = 4.00331 avg_loss = 3.83820\n",
      "epoch no.1 train no.124270  loss = 4.22022 avg_loss = 3.81024\n",
      "epoch no.1 train no.124280  loss = 3.58372 avg_loss = 3.79757\n",
      "epoch no.1 train no.124290  loss = 3.31092 avg_loss = 3.78485\n",
      "epoch no.1 train no.124300  loss = 5.80287 avg_loss = 3.79437\n",
      "epoch no.1 train no.124310  loss = 4.92425 avg_loss = 3.78968\n",
      "epoch no.1 train no.124320  loss = 6.20059 avg_loss = 3.81921\n",
      "epoch no.1 train no.124330  loss = 4.97934 avg_loss = 3.83740\n",
      "epoch no.1 train no.124340  loss = 4.24005 avg_loss = 3.83172\n",
      "epoch no.1 train no.124350  loss = 4.93585 avg_loss = 3.85340\n",
      "epoch no.1 train no.124360  loss = 2.59027 avg_loss = 3.85707\n",
      "epoch no.1 train no.124370  loss = 4.73136 avg_loss = 3.82918\n",
      "epoch no.1 train no.124380  loss = 5.77200 avg_loss = 3.92805\n",
      "epoch no.1 train no.124390  loss = 3.10408 avg_loss = 3.89499\n",
      "epoch no.1 train no.124400  loss = 4.02722 avg_loss = 3.90104\n",
      "epoch no.1 train no.124410  loss = 3.88575 avg_loss = 3.85256\n",
      "epoch no.1 train no.124420  loss = 2.46512 avg_loss = 3.85201\n",
      "epoch no.1 train no.124430  loss = 5.54634 avg_loss = 3.82492\n",
      "epoch no.1 train no.124440  loss = 4.48793 avg_loss = 3.86282\n",
      "epoch no.1 train no.124450  loss = 4.18579 avg_loss = 3.83755\n",
      "epoch no.1 train no.124460  loss = 2.01314 avg_loss = 3.81540\n",
      "epoch no.1 train no.124470  loss = 3.85068 avg_loss = 3.84050\n",
      "epoch no.1 train no.124480  loss = 3.54118 avg_loss = 3.80327\n",
      "epoch no.1 train no.124490  loss = 4.67733 avg_loss = 3.81341\n",
      "epoch no.1 train no.124500  loss = 4.73748 avg_loss = 3.83047\n",
      "epoch no.1 train no.124510  loss = 3.87015 avg_loss = 3.82754\n",
      "epoch no.1 train no.124520  loss = 2.68585 avg_loss = 3.83452\n",
      "epoch no.1 train no.124530  loss = 1.91821 avg_loss = 3.82301\n",
      "epoch no.1 train no.124540  loss = 4.32249 avg_loss = 3.78151\n",
      "epoch no.1 train no.124550  loss = 2.62496 avg_loss = 3.73778\n",
      "epoch no.1 train no.124560  loss = 3.53550 avg_loss = 3.73862\n",
      "epoch no.1 train no.124570  loss = 2.62063 avg_loss = 3.77218\n",
      "epoch no.1 train no.124580  loss = 3.73343 avg_loss = 3.78577\n",
      "epoch no.1 train no.124590  loss = 2.58769 avg_loss = 3.72780\n",
      "epoch no.1 train no.124600  loss = 2.30842 avg_loss = 3.70704\n",
      "epoch no.1 train no.124610  loss = 2.54007 avg_loss = 3.67894\n",
      "epoch no.1 train no.124620  loss = 2.59516 avg_loss = 3.65270\n",
      "epoch no.1 train no.124630  loss = 2.96511 avg_loss = 3.62880\n",
      "epoch no.1 train no.124640  loss = 2.08770 avg_loss = 3.56545\n",
      "epoch no.1 train no.124650  loss = 4.71143 avg_loss = 3.63105\n",
      "epoch no.1 train no.124660  loss = 4.68322 avg_loss = 3.64826\n",
      "epoch no.1 train no.124670  loss = 6.65330 avg_loss = 3.67903\n",
      "epoch no.1 train no.124680  loss = 3.89418 avg_loss = 3.70799\n",
      "epoch no.1 train no.124690  loss = 4.35657 avg_loss = 3.71406\n",
      "epoch no.1 train no.124700  loss = 2.99526 avg_loss = 3.68250\n",
      "epoch no.1 train no.124710  loss = 4.93592 avg_loss = 3.65796\n",
      "epoch no.1 train no.124720  loss = 3.17583 avg_loss = 3.62596\n",
      "epoch no.1 train no.124730  loss = 2.99547 avg_loss = 3.58797\n",
      "epoch no.1 train no.124740  loss = 3.64836 avg_loss = 3.57382\n",
      "epoch no.1 train no.124750  loss = 2.30624 avg_loss = 3.56888\n",
      "epoch no.1 train no.124760  loss = 5.27485 avg_loss = 3.60144\n",
      "epoch no.1 train no.124770  loss = 5.76082 avg_loss = 3.57037\n",
      "epoch no.1 train no.124780  loss = 2.79298 avg_loss = 3.57306\n",
      "epoch no.1 train no.124790  loss = 2.39519 avg_loss = 3.54413\n",
      "epoch no.1 train no.124800  loss = 2.58364 avg_loss = 3.58597\n",
      "epoch no.1 train no.124810  loss = 2.70137 avg_loss = 3.52893\n",
      "epoch no.1 train no.124820  loss = 3.80988 avg_loss = 3.55527\n",
      "epoch no.1 train no.124830  loss = 3.68339 avg_loss = 3.52672\n",
      "epoch no.1 train no.124840  loss = 1.74306 avg_loss = 3.49996\n",
      "epoch no.1 train no.124850  loss = 3.19668 avg_loss = 3.49355\n",
      "epoch no.1 train no.124860  loss = 4.94490 avg_loss = 3.53053\n",
      "epoch no.1 train no.124870  loss = 2.82004 avg_loss = 3.52060\n",
      "epoch no.1 train no.124880  loss = 3.88821 avg_loss = 3.50354\n",
      "epoch no.1 train no.124890  loss = 6.02014 avg_loss = 3.46108\n",
      "epoch no.1 train no.124900  loss = 5.91712 avg_loss = 3.58322\n",
      "epoch no.1 train no.124910  loss = 4.36776 avg_loss = 3.56143\n",
      "epoch no.1 train no.124920  loss = 2.78127 avg_loss = 3.61280\n",
      "epoch no.1 train no.124930  loss = 3.49152 avg_loss = 3.68023\n",
      "epoch no.1 train no.124940  loss = 3.96045 avg_loss = 3.68063\n",
      "epoch no.1 train no.124950  loss = 4.00066 avg_loss = 3.64700\n",
      "epoch no.1 train no.124960  loss = 5.32971 avg_loss = 3.71189\n",
      "epoch no.1 train no.124970  loss = 3.02572 avg_loss = 3.67583\n",
      "epoch no.1 train no.124980  loss = 4.12563 avg_loss = 3.65902\n",
      "epoch no.1 train no.124990  loss = 3.22042 avg_loss = 3.63739\n",
      "epoch no.1 train no.125000  loss = 3.50478 avg_loss = 3.64727\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '음악', '</s>']\n",
      "추억의 2000년대 인디음악</s>\n",
      "epoch no.1 train no.125010  loss = 4.78354 avg_loss = 3.67275\n",
      "epoch no.1 train no.125020  loss = 5.16458 avg_loss = 3.76301\n",
      "epoch no.1 train no.125030  loss = 4.40693 avg_loss = 3.74409\n",
      "epoch no.1 train no.125040  loss = 3.79896 avg_loss = 3.74969\n",
      "epoch no.1 train no.125050  loss = 2.92799 avg_loss = 3.76902\n",
      "epoch no.1 train no.125060  loss = 2.43213 avg_loss = 3.76260\n",
      "epoch no.1 train no.125070  loss = 3.61771 avg_loss = 3.73201\n",
      "epoch no.1 train no.125080  loss = 3.67717 avg_loss = 3.69155\n",
      "epoch no.1 train no.125090  loss = 3.72546 avg_loss = 3.71306\n",
      "epoch no.1 train no.125100  loss = 2.32845 avg_loss = 3.75744\n",
      "epoch no.1 train no.125110  loss = 6.82164 avg_loss = 3.81068\n",
      "epoch no.1 train no.125120  loss = 3.36215 avg_loss = 3.73004\n",
      "epoch no.1 train no.125130  loss = 3.91032 avg_loss = 3.72443\n",
      "epoch no.1 train no.125140  loss = 3.76639 avg_loss = 3.75280\n",
      "epoch no.1 train no.125150  loss = 4.11637 avg_loss = 3.80155\n",
      "epoch no.1 train no.125160  loss = 3.90935 avg_loss = 3.83867\n",
      "epoch no.1 train no.125170  loss = 3.34629 avg_loss = 3.85660\n",
      "epoch no.1 train no.125180  loss = 3.86260 avg_loss = 3.81833\n",
      "epoch no.1 train no.125190  loss = 5.54300 avg_loss = 3.82709\n",
      "epoch no.1 train no.125200  loss = 3.77033 avg_loss = 3.84972\n",
      "epoch no.1 train no.125210  loss = 3.24331 avg_loss = 3.87884\n",
      "epoch no.1 train no.125220  loss = 4.16721 avg_loss = 3.88452\n",
      "epoch no.1 train no.125230  loss = 2.57018 avg_loss = 3.87178\n",
      "epoch no.1 train no.125240  loss = 2.23818 avg_loss = 3.85485\n",
      "epoch no.1 train no.125250  loss = 4.87972 avg_loss = 3.86862\n",
      "epoch no.1 train no.125260  loss = 2.46404 avg_loss = 3.79655\n",
      "epoch no.1 train no.125270  loss = 2.12775 avg_loss = 3.72580\n",
      "epoch no.1 train no.125280  loss = 4.50109 avg_loss = 3.67679\n",
      "epoch no.1 train no.125290  loss = 3.35766 avg_loss = 3.67950\n",
      "epoch no.1 train no.125300  loss = 1.90209 avg_loss = 3.65706\n",
      "epoch no.1 train no.125310  loss = 4.84966 avg_loss = 3.72106\n",
      "epoch no.1 train no.125320  loss = 4.34966 avg_loss = 3.73565\n",
      "epoch no.1 train no.125330  loss = 2.41661 avg_loss = 3.69869\n",
      "epoch no.1 train no.125340  loss = 4.54165 avg_loss = 3.71198\n",
      "epoch no.1 train no.125350  loss = 3.19378 avg_loss = 3.68446\n",
      "epoch no.1 train no.125360  loss = 3.41696 avg_loss = 3.65828\n",
      "epoch no.1 train no.125370  loss = 4.40888 avg_loss = 3.64805\n",
      "epoch no.1 train no.125380  loss = 2.90815 avg_loss = 3.59468\n",
      "epoch no.1 train no.125390  loss = 3.80220 avg_loss = 3.58798\n",
      "epoch no.1 train no.125400  loss = 2.35893 avg_loss = 3.61184\n",
      "epoch no.1 train no.125410  loss = 3.54165 avg_loss = 3.59687\n",
      "epoch no.1 train no.125420  loss = 3.67750 avg_loss = 3.65008\n",
      "epoch no.1 train no.125430  loss = 2.59297 avg_loss = 3.68571\n",
      "epoch no.1 train no.125440  loss = 6.06078 avg_loss = 3.73104\n",
      "epoch no.1 train no.125450  loss = 2.39569 avg_loss = 3.67444\n",
      "epoch no.1 train no.125460  loss = 3.04126 avg_loss = 3.64248\n",
      "epoch no.1 train no.125470  loss = 3.77325 avg_loss = 3.63000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.125480  loss = 3.18910 avg_loss = 3.64503\n",
      "epoch no.1 train no.125490  loss = 3.93798 avg_loss = 3.64136\n",
      "epoch no.1 train no.125500  loss = 4.79721 avg_loss = 3.61232\n",
      "epoch no.1 train no.125510  loss = 3.36393 avg_loss = 3.66179\n",
      "epoch no.1 train no.125520  loss = 3.05510 avg_loss = 3.65135\n",
      "epoch no.1 train no.125530  loss = 4.14012 avg_loss = 3.65523\n",
      "epoch no.1 train no.125540  loss = 3.39531 avg_loss = 3.70415\n",
      "epoch no.1 train no.125550  loss = 3.02159 avg_loss = 3.71372\n",
      "epoch no.1 train no.125560  loss = 3.12698 avg_loss = 3.66458\n",
      "epoch no.1 train no.125570  loss = 4.91105 avg_loss = 3.67276\n",
      "epoch no.1 train no.125580  loss = 2.47235 avg_loss = 3.67604\n",
      "epoch no.1 train no.125590  loss = 2.65148 avg_loss = 3.61459\n",
      "epoch no.1 train no.125600  loss = 4.59604 avg_loss = 3.63224\n",
      "epoch no.1 train no.125610  loss = 5.42221 avg_loss = 3.62958\n",
      "epoch no.1 train no.125620  loss = 3.45509 avg_loss = 3.64357\n",
      "epoch no.1 train no.125630  loss = 5.16983 avg_loss = 3.67196\n",
      "epoch no.1 train no.125640  loss = 2.83340 avg_loss = 3.66594\n",
      "epoch no.1 train no.125650  loss = 4.20263 avg_loss = 3.64016\n",
      "epoch no.1 train no.125660  loss = 3.35642 avg_loss = 3.59094\n",
      "epoch no.1 train no.125670  loss = 2.32810 avg_loss = 3.54873\n",
      "epoch no.1 train no.125680  loss = 3.02793 avg_loss = 3.54372\n",
      "epoch no.1 train no.125690  loss = 3.41111 avg_loss = 3.58743\n",
      "epoch no.1 train no.125700  loss = 3.53581 avg_loss = 3.60979\n",
      "epoch no.1 train no.125710  loss = 1.84893 avg_loss = 3.60274\n",
      "epoch no.1 train no.125720  loss = 1.61385 avg_loss = 3.56682\n",
      "epoch no.1 train no.125730  loss = 3.48528 avg_loss = 3.60285\n",
      "epoch no.1 train no.125740  loss = 3.09855 avg_loss = 3.62419\n",
      "epoch no.1 train no.125750  loss = 1.87802 avg_loss = 3.62608\n",
      "epoch no.1 train no.125760  loss = 2.83967 avg_loss = 3.59195\n",
      "epoch no.1 train no.125770  loss = 2.78302 avg_loss = 3.52717\n",
      "epoch no.1 train no.125780  loss = 2.62026 avg_loss = 3.54089\n",
      "epoch no.1 train no.125790  loss = 3.59940 avg_loss = 3.53844\n",
      "epoch no.1 train no.125800  loss = 2.63279 avg_loss = 3.51699\n",
      "epoch no.1 train no.125810  loss = 2.89508 avg_loss = 3.50773\n",
      "epoch no.1 train no.125820  loss = 7.71994 avg_loss = 3.53629\n",
      "epoch no.1 train no.125830  loss = 4.64653 avg_loss = 3.58614\n",
      "epoch no.1 train no.125840  loss = 4.04549 avg_loss = 3.56711\n",
      "epoch no.1 train no.125850  loss = 5.55978 avg_loss = 3.61838\n",
      "epoch no.1 train no.125860  loss = 2.98440 avg_loss = 3.57413\n",
      "epoch no.1 train no.125870  loss = 2.75795 avg_loss = 3.57639\n",
      "epoch no.1 train no.125880  loss = 4.04820 avg_loss = 3.63551\n",
      "epoch no.1 train no.125890  loss = 2.64933 avg_loss = 3.69038\n",
      "epoch no.1 train no.125900  loss = 5.16362 avg_loss = 3.64201\n",
      "epoch no.1 train no.125910  loss = 2.95960 avg_loss = 3.63333\n",
      "epoch no.1 train no.125920  loss = 2.51484 avg_loss = 3.67193\n",
      "epoch no.1 train no.125930  loss = 5.34049 avg_loss = 3.66265\n",
      "epoch no.1 train no.125940  loss = 4.61511 avg_loss = 3.68648\n",
      "epoch no.1 train no.125950  loss = 3.92156 avg_loss = 3.75009\n",
      "epoch no.1 train no.125960  loss = 5.48563 avg_loss = 3.75211\n",
      "epoch no.1 train no.125970  loss = 4.49502 avg_loss = 3.77739\n",
      "epoch no.1 train no.125980  loss = 3.24579 avg_loss = 3.79928\n",
      "epoch no.1 train no.125990  loss = 3.77957 avg_loss = 3.76602\n",
      "epoch no.1 train no.126000  loss = 2.26644 avg_loss = 3.71614\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁노래', '▁가요', '</s>']\n",
      "추억의 90년대 인기 가요</s>\n",
      "epoch no.1 train no.126010  loss = 2.79122 avg_loss = 3.73031\n",
      "epoch no.1 train no.126020  loss = 3.62106 avg_loss = 3.70233\n",
      "epoch no.1 train no.126030  loss = 5.04516 avg_loss = 3.73281\n",
      "epoch no.1 train no.126040  loss = 3.63743 avg_loss = 3.75099\n",
      "epoch no.1 train no.126050  loss = 4.60701 avg_loss = 3.80279\n",
      "epoch no.1 train no.126060  loss = 4.07895 avg_loss = 3.76841\n",
      "epoch no.1 train no.126070  loss = 3.82854 avg_loss = 3.79871\n",
      "epoch no.1 train no.126080  loss = 4.09396 avg_loss = 3.77675\n",
      "epoch no.1 train no.126090  loss = 5.85515 avg_loss = 3.77296\n",
      "epoch no.1 train no.126100  loss = 2.92452 avg_loss = 3.72494\n",
      "epoch no.1 train no.126110  loss = 5.01313 avg_loss = 3.70764\n",
      "epoch no.1 train no.126120  loss = 3.83249 avg_loss = 3.68010\n",
      "epoch no.1 train no.126130  loss = 3.89838 avg_loss = 3.73893\n",
      "epoch no.1 train no.126140  loss = 3.98914 avg_loss = 3.68977\n",
      "epoch no.1 train no.126150  loss = 2.94623 avg_loss = 3.68515\n",
      "epoch no.1 train no.126160  loss = 3.66919 avg_loss = 3.68165\n",
      "epoch no.1 train no.126170  loss = 2.99570 avg_loss = 3.66876\n",
      "epoch no.1 train no.126180  loss = 4.78066 avg_loss = 3.64548\n",
      "epoch no.1 train no.126190  loss = 3.68440 avg_loss = 3.67738\n",
      "epoch no.1 train no.126200  loss = 4.50726 avg_loss = 3.76466\n",
      "epoch no.1 train no.126210  loss = 5.30567 avg_loss = 3.77174\n",
      "epoch no.1 train no.126220  loss = 3.32023 avg_loss = 3.73587\n",
      "epoch no.1 train no.126230  loss = 3.40898 avg_loss = 3.76846\n",
      "epoch no.1 train no.126240  loss = 2.96832 avg_loss = 3.71873\n",
      "epoch no.1 train no.126250  loss = 3.47501 avg_loss = 3.67228\n",
      "epoch no.1 train no.126260  loss = 2.84255 avg_loss = 3.61404\n",
      "epoch no.1 train no.126270  loss = 4.33464 avg_loss = 3.66027\n",
      "epoch no.1 train no.126280  loss = 3.10204 avg_loss = 3.64879\n",
      "epoch no.1 train no.126290  loss = 2.80581 avg_loss = 3.64582\n",
      "epoch no.1 train no.126300  loss = 6.66977 avg_loss = 3.70479\n",
      "epoch no.1 train no.126310  loss = 4.71198 avg_loss = 3.70762\n",
      "epoch no.1 train no.126320  loss = 2.43291 avg_loss = 3.73005\n",
      "epoch no.1 train no.126330  loss = 2.22092 avg_loss = 3.69961\n",
      "epoch no.1 train no.126340  loss = 2.99870 avg_loss = 3.73196\n",
      "epoch no.1 train no.126350  loss = 2.54850 avg_loss = 3.71625\n",
      "epoch no.1 train no.126360  loss = 2.85635 avg_loss = 3.72642\n",
      "epoch no.1 train no.126370  loss = 2.04730 avg_loss = 3.75800\n",
      "epoch no.1 train no.126380  loss = 4.02511 avg_loss = 3.78920\n",
      "epoch no.1 train no.126390  loss = 3.11667 avg_loss = 3.82563\n",
      "epoch no.1 train no.126400  loss = 3.98108 avg_loss = 3.80416\n",
      "epoch no.1 train no.126410  loss = 4.81337 avg_loss = 3.80858\n",
      "epoch no.1 train no.126420  loss = 3.06504 avg_loss = 3.78908\n",
      "epoch no.1 train no.126430  loss = 5.57959 avg_loss = 3.73928\n",
      "epoch no.1 train no.126440  loss = 2.54726 avg_loss = 3.70365\n",
      "epoch no.1 train no.126450  loss = 2.58724 avg_loss = 3.65498\n",
      "epoch no.1 train no.126460  loss = 3.04179 avg_loss = 3.70961\n",
      "epoch no.1 train no.126470  loss = 2.04460 avg_loss = 3.70838\n",
      "epoch no.1 train no.126480  loss = 1.57571 avg_loss = 3.79042\n",
      "epoch no.1 train no.126490  loss = 5.61194 avg_loss = 3.82241\n",
      "epoch no.1 train no.126500  loss = 5.58174 avg_loss = 3.87928\n",
      "epoch no.1 train no.126510  loss = 3.51337 avg_loss = 3.86253\n",
      "epoch no.1 train no.126520  loss = 2.74045 avg_loss = 3.82473\n",
      "epoch no.1 train no.126530  loss = 3.63501 avg_loss = 3.79934\n",
      "epoch no.1 train no.126540  loss = 3.88001 avg_loss = 3.79653\n",
      "epoch no.1 train no.126550  loss = 5.22570 avg_loss = 3.76637\n",
      "epoch no.1 train no.126560  loss = 2.89921 avg_loss = 3.76678\n",
      "epoch no.1 train no.126570  loss = 3.23957 avg_loss = 3.76323\n",
      "epoch no.1 train no.126580  loss = 7.42604 avg_loss = 3.81985\n",
      "epoch no.1 train no.126590  loss = 3.46624 avg_loss = 3.82444\n",
      "epoch no.1 train no.126600  loss = 3.65474 avg_loss = 3.83021\n",
      "epoch no.1 train no.126610  loss = 3.49729 avg_loss = 3.79444\n",
      "epoch no.1 train no.126620  loss = 3.74437 avg_loss = 3.80447\n",
      "epoch no.1 train no.126630  loss = 1.34512 avg_loss = 3.74835\n",
      "epoch no.1 train no.126640  loss = 3.03225 avg_loss = 3.71401\n",
      "epoch no.1 train no.126650  loss = 1.81510 avg_loss = 3.72001\n",
      "epoch no.1 train no.126660  loss = 4.94122 avg_loss = 3.65263\n",
      "epoch no.1 train no.126670  loss = 4.68910 avg_loss = 3.69883\n",
      "epoch no.1 train no.126680  loss = 4.23486 avg_loss = 3.72740\n",
      "epoch no.1 train no.126690  loss = 2.47030 avg_loss = 3.76324\n",
      "epoch no.1 train no.126700  loss = 2.68109 avg_loss = 3.71219\n",
      "epoch no.1 train no.126710  loss = 3.32279 avg_loss = 3.73794\n",
      "epoch no.1 train no.126720  loss = 4.03677 avg_loss = 3.67125\n",
      "epoch no.1 train no.126730  loss = 4.28699 avg_loss = 3.65405\n",
      "epoch no.1 train no.126740  loss = 3.07931 avg_loss = 3.61240\n",
      "epoch no.1 train no.126750  loss = 2.37595 avg_loss = 3.60839\n",
      "epoch no.1 train no.126760  loss = 5.05529 avg_loss = 3.61325\n",
      "epoch no.1 train no.126770  loss = 5.35350 avg_loss = 3.63925\n",
      "epoch no.1 train no.126780  loss = 3.13433 avg_loss = 3.61450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.126790  loss = 3.54083 avg_loss = 3.61731\n",
      "epoch no.1 train no.126800  loss = 2.50628 avg_loss = 3.57901\n",
      "epoch no.1 train no.126810  loss = 4.66304 avg_loss = 3.60648\n",
      "epoch no.1 train no.126820  loss = 3.76602 avg_loss = 3.64703\n",
      "epoch no.1 train no.126830  loss = 3.90306 avg_loss = 3.62080\n",
      "epoch no.1 train no.126840  loss = 4.00155 avg_loss = 3.64442\n",
      "epoch no.1 train no.126850  loss = 4.72787 avg_loss = 3.63990\n",
      "epoch no.1 train no.126860  loss = 2.98798 avg_loss = 3.65602\n",
      "epoch no.1 train no.126870  loss = 3.94974 avg_loss = 3.67079\n",
      "epoch no.1 train no.126880  loss = 4.10105 avg_loss = 3.69416\n",
      "epoch no.1 train no.126890  loss = 3.12717 avg_loss = 3.68706\n",
      "epoch no.1 train no.126900  loss = 4.34944 avg_loss = 3.77674\n",
      "epoch no.1 train no.126910  loss = 5.27577 avg_loss = 3.79329\n",
      "epoch no.1 train no.126920  loss = 5.18000 avg_loss = 3.76979\n",
      "epoch no.1 train no.126930  loss = 2.52051 avg_loss = 3.73278\n",
      "epoch no.1 train no.126940  loss = 3.39962 avg_loss = 3.69164\n",
      "epoch no.1 train no.126950  loss = 3.01510 avg_loss = 3.65992\n",
      "epoch no.1 train no.126960  loss = 3.96538 avg_loss = 3.65677\n",
      "epoch no.1 train no.126970  loss = 4.20769 avg_loss = 3.65001\n",
      "epoch no.1 train no.126980  loss = 4.34817 avg_loss = 3.66038\n",
      "epoch no.1 train no.126990  loss = 2.48913 avg_loss = 3.65556\n",
      "epoch no.1 train no.127000  loss = 4.99022 avg_loss = 3.65007\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.1 train no.127010  loss = 4.60949 avg_loss = 3.62899\n",
      "epoch no.1 train no.127020  loss = 4.05896 avg_loss = 3.62267\n",
      "epoch no.1 train no.127030  loss = 3.56016 avg_loss = 3.59859\n",
      "epoch no.1 train no.127040  loss = 3.60160 avg_loss = 3.59402\n",
      "epoch no.1 train no.127050  loss = 4.50896 avg_loss = 3.65404\n",
      "epoch no.1 train no.127060  loss = 3.87593 avg_loss = 3.64576\n",
      "epoch no.1 train no.127070  loss = 1.84205 avg_loss = 3.63634\n",
      "epoch no.1 train no.127080  loss = 2.29883 avg_loss = 3.61760\n",
      "epoch no.1 train no.127090  loss = 4.05738 avg_loss = 3.67674\n",
      "epoch no.1 train no.127100  loss = 4.16212 avg_loss = 3.73881\n",
      "epoch no.1 train no.127110  loss = 4.38235 avg_loss = 3.75030\n",
      "epoch no.1 train no.127120  loss = 4.22448 avg_loss = 3.69452\n",
      "epoch no.1 train no.127130  loss = 4.35603 avg_loss = 3.67420\n",
      "epoch no.1 train no.127140  loss = 5.61305 avg_loss = 3.69305\n",
      "epoch no.1 train no.127150  loss = 3.55256 avg_loss = 3.68511\n",
      "epoch no.1 train no.127160  loss = 4.94305 avg_loss = 3.72712\n",
      "epoch no.1 train no.127170  loss = 2.79895 avg_loss = 3.69341\n",
      "epoch no.1 train no.127180  loss = 4.99061 avg_loss = 3.70396\n",
      "epoch no.1 train no.127190  loss = 3.86303 avg_loss = 3.74740\n",
      "epoch no.1 train no.127200  loss = 3.49288 avg_loss = 3.70788\n",
      "epoch no.1 train no.127210  loss = 2.94735 avg_loss = 3.64575\n",
      "epoch no.1 train no.127220  loss = 4.69697 avg_loss = 3.65630\n",
      "epoch no.1 train no.127230  loss = 3.44989 avg_loss = 3.61613\n",
      "epoch no.1 train no.127240  loss = 3.11531 avg_loss = 3.64344\n",
      "epoch no.1 train no.127250  loss = 3.30040 avg_loss = 3.67690\n",
      "epoch no.1 train no.127260  loss = 2.95962 avg_loss = 3.68368\n",
      "epoch no.1 train no.127270  loss = 2.80291 avg_loss = 3.67954\n",
      "epoch no.1 train no.127280  loss = 2.37113 avg_loss = 3.72764\n",
      "epoch no.1 train no.127290  loss = 3.58969 avg_loss = 3.68611\n",
      "epoch no.1 train no.127300  loss = 4.41466 avg_loss = 3.66229\n",
      "epoch no.1 train no.127310  loss = 2.54176 avg_loss = 3.67655\n",
      "epoch no.1 train no.127320  loss = 4.30467 avg_loss = 3.70333\n",
      "epoch no.1 train no.127330  loss = 3.41616 avg_loss = 3.72731\n",
      "epoch no.1 train no.127340  loss = 2.67257 avg_loss = 3.75264\n",
      "epoch no.1 train no.127350  loss = 3.55778 avg_loss = 3.72995\n",
      "epoch no.1 train no.127360  loss = 3.72960 avg_loss = 3.69292\n",
      "epoch no.1 train no.127370  loss = 4.68258 avg_loss = 3.69615\n",
      "epoch no.1 train no.127380  loss = 6.22818 avg_loss = 3.70997\n",
      "epoch no.1 train no.127390  loss = 3.36047 avg_loss = 3.68536\n",
      "epoch no.1 train no.127400  loss = 3.82452 avg_loss = 3.64350\n",
      "epoch no.1 train no.127410  loss = 3.66601 avg_loss = 3.64784\n",
      "epoch no.1 train no.127420  loss = 3.64399 avg_loss = 3.62196\n",
      "epoch no.1 train no.127430  loss = 2.92566 avg_loss = 3.59881\n",
      "epoch no.1 train no.127440  loss = 3.61271 avg_loss = 3.66286\n",
      "epoch no.1 train no.127450  loss = 3.23999 avg_loss = 3.69961\n",
      "epoch no.1 train no.127460  loss = 3.49658 avg_loss = 3.64862\n",
      "epoch no.1 train no.127470  loss = 5.52409 avg_loss = 3.70425\n",
      "epoch no.1 train no.127480  loss = 4.06432 avg_loss = 3.70765\n",
      "epoch no.1 train no.127490  loss = 2.00498 avg_loss = 3.70004\n",
      "epoch no.1 train no.127500  loss = 4.24044 avg_loss = 3.67396\n",
      "epoch no.1 train no.127510  loss = 3.07133 avg_loss = 3.66254\n",
      "epoch no.1 train no.127520  loss = 2.84751 avg_loss = 3.62366\n",
      "epoch no.1 train no.127530  loss = 2.23543 avg_loss = 3.60402\n",
      "epoch no.1 train no.127540  loss = 2.84579 avg_loss = 3.55833\n",
      "epoch no.1 train no.127550  loss = 3.91104 avg_loss = 3.53053\n",
      "epoch no.1 train no.127560  loss = 2.91427 avg_loss = 3.51211\n",
      "epoch no.1 train no.127570  loss = 3.25545 avg_loss = 3.51530\n",
      "epoch no.1 train no.127580  loss = 4.10538 avg_loss = 3.53451\n",
      "epoch no.1 train no.127590  loss = 4.55697 avg_loss = 3.56004\n",
      "epoch no.1 train no.127600  loss = 3.80289 avg_loss = 3.56994\n",
      "epoch no.1 train no.127610  loss = 5.16738 avg_loss = 3.63545\n",
      "epoch no.1 train no.127620  loss = 3.57689 avg_loss = 3.65783\n",
      "epoch no.1 train no.127630  loss = 3.33263 avg_loss = 3.66466\n",
      "epoch no.1 train no.127640  loss = 2.50771 avg_loss = 3.60975\n",
      "epoch no.1 train no.127650  loss = 4.15175 avg_loss = 3.62016\n",
      "epoch no.1 train no.127660  loss = 4.76029 avg_loss = 3.62725\n",
      "epoch no.1 train no.127670  loss = 4.74482 avg_loss = 3.59543\n",
      "epoch no.1 train no.127680  loss = 2.70025 avg_loss = 3.58322\n",
      "epoch no.1 train no.127690  loss = 4.16149 avg_loss = 3.58397\n",
      "epoch no.1 train no.127700  loss = 2.22868 avg_loss = 3.57777\n",
      "epoch no.1 train no.127710  loss = 2.92883 avg_loss = 3.56922\n",
      "epoch no.1 train no.127720  loss = 2.71676 avg_loss = 3.59286\n",
      "epoch no.1 train no.127730  loss = 3.14523 avg_loss = 3.57288\n",
      "epoch no.1 train no.127740  loss = 2.50382 avg_loss = 3.58403\n",
      "epoch no.1 train no.127750  loss = 3.18136 avg_loss = 3.58094\n",
      "epoch no.1 train no.127760  loss = 3.08449 avg_loss = 3.51266\n",
      "epoch no.1 train no.127770  loss = 3.42124 avg_loss = 3.45334\n",
      "epoch no.1 train no.127780  loss = 6.05070 avg_loss = 3.49064\n",
      "epoch no.1 train no.127790  loss = 3.48052 avg_loss = 3.54171\n",
      "epoch no.1 train no.127800  loss = 4.94744 avg_loss = 3.61673\n",
      "epoch no.1 train no.127810  loss = 5.07803 avg_loss = 3.61526\n",
      "epoch no.1 train no.127820  loss = 2.59251 avg_loss = 3.60261\n",
      "epoch no.1 train no.127830  loss = 2.36508 avg_loss = 3.55283\n",
      "epoch no.1 train no.127840  loss = 5.52838 avg_loss = 3.59279\n",
      "epoch no.1 train no.127850  loss = 4.91233 avg_loss = 3.57696\n",
      "epoch no.1 train no.127860  loss = 5.56515 avg_loss = 3.63639\n",
      "epoch no.1 train no.127870  loss = 3.46612 avg_loss = 3.62653\n",
      "epoch no.1 train no.127880  loss = 4.65187 avg_loss = 3.65456\n",
      "epoch no.1 train no.127890  loss = 4.11733 avg_loss = 3.62118\n",
      "epoch no.1 train no.127900  loss = 2.36892 avg_loss = 3.56186\n",
      "epoch no.1 train no.127910  loss = 2.44280 avg_loss = 3.59003\n",
      "epoch no.1 train no.127920  loss = 3.08853 avg_loss = 3.60160\n",
      "epoch no.1 train no.127930  loss = 5.78342 avg_loss = 3.63779\n",
      "epoch no.1 train no.127940  loss = 3.23792 avg_loss = 3.63386\n",
      "epoch no.1 train no.127950  loss = 3.59314 avg_loss = 3.63118\n",
      "epoch no.1 train no.127960  loss = 4.80898 avg_loss = 3.65424\n",
      "epoch no.1 train no.127970  loss = 3.22028 avg_loss = 3.66444\n",
      "epoch no.1 train no.127980  loss = 3.18563 avg_loss = 3.64607\n",
      "epoch no.1 train no.127990  loss = 6.53575 avg_loss = 3.64238\n",
      "epoch no.1 train no.128000  loss = 4.77136 avg_loss = 3.61136\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '곡', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.1 train no.128010  loss = 3.33098 avg_loss = 3.61024\n",
      "epoch no.1 train no.128020  loss = 1.93257 avg_loss = 3.62731\n",
      "epoch no.1 train no.128030  loss = 3.35745 avg_loss = 3.61759\n",
      "epoch no.1 train no.128040  loss = 4.07391 avg_loss = 3.54608\n",
      "epoch no.1 train no.128050  loss = 3.54530 avg_loss = 3.53125\n",
      "epoch no.1 train no.128060  loss = 4.86537 avg_loss = 3.53942\n",
      "epoch no.1 train no.128070  loss = 2.65176 avg_loss = 3.55594\n",
      "epoch no.1 train no.128080  loss = 2.56648 avg_loss = 3.59085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.128090  loss = 2.89939 avg_loss = 3.55050\n",
      "epoch no.1 train no.128100  loss = 3.17896 avg_loss = 3.52311\n",
      "epoch no.1 train no.128110  loss = 4.13136 avg_loss = 3.56805\n",
      "epoch no.1 train no.128120  loss = 4.42890 avg_loss = 3.64119\n",
      "epoch no.1 train no.128130  loss = 6.00850 avg_loss = 3.69859\n",
      "epoch no.1 train no.128140  loss = 1.09212 avg_loss = 3.66208\n",
      "epoch no.1 train no.128150  loss = 2.77088 avg_loss = 3.65092\n",
      "epoch no.1 train no.128160  loss = 3.23367 avg_loss = 3.65329\n",
      "epoch no.1 train no.128170  loss = 4.88882 avg_loss = 3.66734\n",
      "epoch no.1 train no.128180  loss = 2.34987 avg_loss = 3.66393\n",
      "epoch no.1 train no.128190  loss = 2.86830 avg_loss = 3.62362\n",
      "epoch no.1 train no.128200  loss = 3.45899 avg_loss = 3.67439\n",
      "epoch no.1 train no.128210  loss = 3.67883 avg_loss = 3.71732\n",
      "epoch no.1 train no.128220  loss = 3.43062 avg_loss = 3.72884\n",
      "epoch no.1 train no.128230  loss = 3.65934 avg_loss = 3.73721\n",
      "epoch no.1 train no.128240  loss = 5.74667 avg_loss = 3.75939\n",
      "epoch no.1 train no.128250  loss = 2.88439 avg_loss = 3.68306\n",
      "epoch no.1 train no.128260  loss = 3.23000 avg_loss = 3.66033\n",
      "epoch no.1 train no.128270  loss = 3.85953 avg_loss = 3.66228\n",
      "epoch no.1 train no.128280  loss = 2.06936 avg_loss = 3.69814\n",
      "epoch no.1 train no.128290  loss = 3.45160 avg_loss = 3.68377\n",
      "epoch no.1 train no.128300  loss = 2.45565 avg_loss = 3.70372\n",
      "epoch no.1 train no.128310  loss = 3.52173 avg_loss = 3.71616\n",
      "epoch no.1 train no.128320  loss = 3.43080 avg_loss = 3.70176\n",
      "epoch no.1 train no.128330  loss = 4.12202 avg_loss = 3.73012\n",
      "epoch no.1 train no.128340  loss = 3.26982 avg_loss = 3.71226\n",
      "epoch no.1 train no.128350  loss = 6.00396 avg_loss = 3.77837\n",
      "epoch no.1 train no.128360  loss = 3.11174 avg_loss = 3.74258\n",
      "epoch no.1 train no.128370  loss = 6.64942 avg_loss = 3.75570\n",
      "epoch no.1 train no.128380  loss = 6.01377 avg_loss = 3.86229\n",
      "epoch no.1 train no.128390  loss = 4.38224 avg_loss = 3.80596\n",
      "epoch no.1 train no.128400  loss = 4.54830 avg_loss = 3.80153\n",
      "epoch no.1 train no.128410  loss = 5.69786 avg_loss = 3.75949\n",
      "epoch no.1 train no.128420  loss = 2.77962 avg_loss = 3.78662\n",
      "epoch no.1 train no.128430  loss = 4.02643 avg_loss = 3.74863\n",
      "epoch no.1 train no.128440  loss = 2.56865 avg_loss = 3.68980\n",
      "epoch no.1 train no.128450  loss = 4.06809 avg_loss = 3.71606\n",
      "epoch no.1 train no.128460  loss = 4.30030 avg_loss = 3.70698\n",
      "epoch no.1 train no.128470  loss = 4.31670 avg_loss = 3.77635\n",
      "epoch no.1 train no.128480  loss = 3.85542 avg_loss = 3.86399\n",
      "epoch no.1 train no.128490  loss = 3.25322 avg_loss = 3.80968\n",
      "epoch no.1 train no.128500  loss = 2.66935 avg_loss = 3.81512\n",
      "epoch no.1 train no.128510  loss = 2.87667 avg_loss = 3.85850\n",
      "epoch no.1 train no.128520  loss = 3.66359 avg_loss = 3.89201\n",
      "epoch no.1 train no.128530  loss = 3.48053 avg_loss = 3.84909\n",
      "epoch no.1 train no.128540  loss = 5.10169 avg_loss = 3.79251\n",
      "epoch no.1 train no.128550  loss = 6.10036 avg_loss = 3.77363\n",
      "epoch no.1 train no.128560  loss = 4.58705 avg_loss = 3.79331\n",
      "epoch no.1 train no.128570  loss = 3.87352 avg_loss = 3.75765\n",
      "epoch no.1 train no.128580  loss = 2.55241 avg_loss = 3.70843\n",
      "epoch no.1 train no.128590  loss = 3.34589 avg_loss = 3.74949\n",
      "epoch no.1 train no.128600  loss = 3.72506 avg_loss = 3.68183\n",
      "epoch no.1 train no.128610  loss = 2.99838 avg_loss = 3.73680\n",
      "epoch no.1 train no.128620  loss = 7.43837 avg_loss = 3.77120\n",
      "epoch no.1 train no.128630  loss = 2.83604 avg_loss = 3.73511\n",
      "epoch no.1 train no.128640  loss = 4.51921 avg_loss = 3.73131\n",
      "epoch no.1 train no.128650  loss = 4.09460 avg_loss = 3.70599\n",
      "epoch no.1 train no.128660  loss = 2.17843 avg_loss = 3.72410\n",
      "epoch no.1 train no.128670  loss = 3.13638 avg_loss = 3.66896\n",
      "epoch no.1 train no.128680  loss = 3.07305 avg_loss = 3.61263\n",
      "epoch no.1 train no.128690  loss = 4.56247 avg_loss = 3.62837\n",
      "epoch no.1 train no.128700  loss = 4.05815 avg_loss = 3.65114\n",
      "epoch no.1 train no.128710  loss = 4.58564 avg_loss = 3.63772\n",
      "epoch no.1 train no.128720  loss = 2.44235 avg_loss = 3.63578\n",
      "epoch no.1 train no.128730  loss = 2.50096 avg_loss = 3.67512\n",
      "epoch no.1 train no.128740  loss = 1.97382 avg_loss = 3.64506\n",
      "epoch no.1 train no.128750  loss = 1.79540 avg_loss = 3.61549\n",
      "epoch no.1 train no.128760  loss = 2.74897 avg_loss = 3.62581\n",
      "epoch no.1 train no.128770  loss = 4.46465 avg_loss = 3.69587\n",
      "epoch no.1 train no.128780  loss = 2.70869 avg_loss = 3.71137\n",
      "epoch no.1 train no.128790  loss = 2.26374 avg_loss = 3.66844\n",
      "epoch no.1 train no.128800  loss = 2.11871 avg_loss = 3.64193\n",
      "epoch no.1 train no.128810  loss = 3.64210 avg_loss = 3.64236\n",
      "epoch no.1 train no.128820  loss = 4.94477 avg_loss = 3.62613\n",
      "epoch no.1 train no.128830  loss = 4.62146 avg_loss = 3.60482\n",
      "epoch no.1 train no.128840  loss = 2.11761 avg_loss = 3.54561\n",
      "epoch no.1 train no.128850  loss = 3.92675 avg_loss = 3.55721\n",
      "epoch no.1 train no.128860  loss = 3.20118 avg_loss = 3.59958\n",
      "epoch no.1 train no.128870  loss = 4.08394 avg_loss = 3.63915\n",
      "epoch no.1 train no.128880  loss = 3.04320 avg_loss = 3.61601\n",
      "epoch no.1 train no.128890  loss = 4.01946 avg_loss = 3.64898\n",
      "epoch no.1 train no.128900  loss = 3.84326 avg_loss = 3.68168\n",
      "epoch no.1 train no.128910  loss = 3.24573 avg_loss = 3.72830\n",
      "epoch no.1 train no.128920  loss = 3.93355 avg_loss = 3.70758\n",
      "epoch no.1 train no.128930  loss = 5.16653 avg_loss = 3.70852\n",
      "epoch no.1 train no.128940  loss = 2.91135 avg_loss = 3.68589\n",
      "epoch no.1 train no.128950  loss = 4.74352 avg_loss = 3.65381\n",
      "epoch no.1 train no.128960  loss = 2.12246 avg_loss = 3.67476\n",
      "epoch no.1 train no.128970  loss = 2.87434 avg_loss = 3.69578\n",
      "epoch no.1 train no.128980  loss = 2.80976 avg_loss = 3.66216\n",
      "epoch no.1 train no.128990  loss = 3.05799 avg_loss = 3.63995\n",
      "epoch no.1 train no.129000  loss = 3.04612 avg_loss = 3.63118\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '송', '</s>']\n",
      "추억의 2000년대 팝송</s>\n",
      "epoch no.1 train no.129010  loss = 2.10363 avg_loss = 3.60141\n",
      "epoch no.1 train no.129020  loss = 4.47729 avg_loss = 3.63879\n",
      "epoch no.1 train no.129030  loss = 5.77590 avg_loss = 3.69666\n",
      "epoch no.1 train no.129040  loss = 4.78271 avg_loss = 3.66141\n",
      "epoch no.1 train no.129050  loss = 5.66632 avg_loss = 3.72489\n",
      "epoch no.1 train no.129060  loss = 3.24486 avg_loss = 3.72817\n",
      "epoch no.1 train no.129070  loss = 1.89550 avg_loss = 3.63913\n",
      "epoch no.1 train no.129080  loss = 1.64992 avg_loss = 3.61616\n",
      "epoch no.1 train no.129090  loss = 3.67574 avg_loss = 3.62143\n",
      "epoch no.1 train no.129100  loss = 4.04170 avg_loss = 3.62099\n",
      "epoch no.1 train no.129110  loss = 3.32939 avg_loss = 3.59897\n",
      "epoch no.1 train no.129120  loss = 2.46999 avg_loss = 3.58369\n",
      "epoch no.1 train no.129130  loss = 3.17578 avg_loss = 3.61641\n",
      "epoch no.1 train no.129140  loss = 4.26701 avg_loss = 3.58821\n",
      "epoch no.1 train no.129150  loss = 4.31303 avg_loss = 3.64304\n",
      "epoch no.1 train no.129160  loss = 3.36046 avg_loss = 3.64876\n",
      "epoch no.1 train no.129170  loss = 1.62372 avg_loss = 3.67333\n",
      "epoch no.1 train no.129180  loss = 3.76849 avg_loss = 3.62953\n",
      "epoch no.1 train no.129190  loss = 2.62308 avg_loss = 3.67458\n",
      "epoch no.1 train no.129200  loss = 3.74388 avg_loss = 3.64610\n",
      "epoch no.1 train no.129210  loss = 3.35814 avg_loss = 3.65240\n",
      "epoch no.1 train no.129220  loss = 2.10737 avg_loss = 3.64683\n",
      "epoch no.1 train no.129230  loss = 4.57552 avg_loss = 3.64861\n",
      "epoch no.1 train no.129240  loss = 4.37161 avg_loss = 3.72534\n",
      "epoch no.1 train no.129250  loss = 1.67790 avg_loss = 3.66361\n",
      "epoch no.1 train no.129260  loss = 3.99687 avg_loss = 3.65699\n",
      "epoch no.1 train no.129270  loss = 1.42596 avg_loss = 3.61704\n",
      "epoch no.1 train no.129280  loss = 2.37246 avg_loss = 3.58887\n",
      "epoch no.1 train no.129290  loss = 2.45460 avg_loss = 3.57831\n",
      "epoch no.1 train no.129300  loss = 2.97778 avg_loss = 3.53035\n",
      "epoch no.1 train no.129310  loss = 2.49975 avg_loss = 3.52524\n",
      "epoch no.1 train no.129320  loss = 4.71996 avg_loss = 3.57287\n",
      "epoch no.1 train no.129330  loss = 4.82252 avg_loss = 3.59306\n",
      "epoch no.1 train no.129340  loss = 2.91884 avg_loss = 3.64868\n",
      "epoch no.1 train no.129350  loss = 2.58263 avg_loss = 3.62059\n",
      "epoch no.1 train no.129360  loss = 4.73395 avg_loss = 3.61167\n",
      "epoch no.1 train no.129370  loss = 2.93249 avg_loss = 3.61146\n",
      "epoch no.1 train no.129380  loss = 3.72135 avg_loss = 3.59022\n",
      "epoch no.1 train no.129390  loss = 4.05490 avg_loss = 3.56641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.129400  loss = 5.30756 avg_loss = 3.64184\n",
      "epoch no.1 train no.129410  loss = 3.85254 avg_loss = 3.62546\n",
      "epoch no.1 train no.129420  loss = 3.34056 avg_loss = 3.57709\n",
      "epoch no.1 train no.129430  loss = 3.05474 avg_loss = 3.58466\n",
      "epoch no.1 train no.129440  loss = 2.43808 avg_loss = 3.58037\n",
      "epoch no.1 train no.129450  loss = 2.41601 avg_loss = 3.58692\n",
      "epoch no.1 train no.129460  loss = 2.36507 avg_loss = 3.56058\n",
      "epoch no.1 train no.129470  loss = 3.10988 avg_loss = 3.61983\n",
      "epoch no.1 train no.129480  loss = 2.53967 avg_loss = 3.60175\n",
      "epoch no.1 train no.129490  loss = 3.39613 avg_loss = 3.60156\n",
      "epoch no.1 train no.129500  loss = 3.18762 avg_loss = 3.55652\n",
      "epoch no.1 train no.129510  loss = 2.90129 avg_loss = 3.57426\n",
      "epoch no.1 train no.129520  loss = 2.12257 avg_loss = 3.52875\n",
      "epoch no.1 train no.129530  loss = 7.70302 avg_loss = 3.65360\n",
      "epoch no.1 train no.129540  loss = 3.31348 avg_loss = 3.62046\n",
      "epoch no.1 train no.129550  loss = 3.57803 avg_loss = 3.62103\n",
      "epoch no.1 train no.129560  loss = 4.78128 avg_loss = 3.64486\n",
      "epoch no.1 train no.129570  loss = 1.96087 avg_loss = 3.63637\n",
      "epoch no.1 train no.129580  loss = 4.32806 avg_loss = 3.61492\n",
      "epoch no.1 train no.129590  loss = 3.94671 avg_loss = 3.63931\n",
      "epoch no.1 train no.129600  loss = 2.67233 avg_loss = 3.63722\n",
      "epoch no.1 train no.129610  loss = 3.30961 avg_loss = 3.62525\n",
      "epoch no.1 train no.129620  loss = 3.63239 avg_loss = 3.62944\n",
      "epoch no.1 train no.129630  loss = 3.40538 avg_loss = 3.58643\n",
      "epoch no.1 train no.129640  loss = 3.94314 avg_loss = 3.54547\n",
      "epoch no.1 train no.129650  loss = 2.98653 avg_loss = 3.50808\n",
      "epoch no.1 train no.129660  loss = 4.31241 avg_loss = 3.51561\n",
      "epoch no.1 train no.129670  loss = 3.56880 avg_loss = 3.51539\n",
      "epoch no.1 train no.129680  loss = 4.54167 avg_loss = 3.55589\n",
      "epoch no.1 train no.129690  loss = 3.18081 avg_loss = 3.57969\n",
      "epoch no.1 train no.129700  loss = 5.64995 avg_loss = 3.54515\n",
      "epoch no.1 train no.129710  loss = 2.80943 avg_loss = 3.52637\n",
      "epoch no.1 train no.129720  loss = 2.91899 avg_loss = 3.55343\n",
      "epoch no.1 train no.129730  loss = 3.93880 avg_loss = 3.53347\n",
      "epoch no.1 train no.129740  loss = 3.56120 avg_loss = 3.52085\n",
      "epoch no.1 train no.129750  loss = 3.07412 avg_loss = 3.50118\n",
      "epoch no.1 train no.129760  loss = 2.43852 avg_loss = 3.52046\n",
      "epoch no.1 train no.129770  loss = 3.34369 avg_loss = 3.52500\n",
      "epoch no.1 train no.129780  loss = 2.68721 avg_loss = 3.56741\n",
      "epoch no.1 train no.129790  loss = 2.86697 avg_loss = 3.52670\n",
      "epoch no.1 train no.129800  loss = 1.93706 avg_loss = 3.51061\n",
      "epoch no.1 train no.129810  loss = 3.33515 avg_loss = 3.55356\n",
      "epoch no.1 train no.129820  loss = 3.93111 avg_loss = 3.52106\n",
      "epoch no.1 train no.129830  loss = 6.29136 avg_loss = 3.57832\n",
      "epoch no.1 train no.129840  loss = 3.31336 avg_loss = 3.57087\n",
      "epoch no.1 train no.129850  loss = 2.46373 avg_loss = 3.53060\n",
      "epoch no.1 train no.129860  loss = 4.65198 avg_loss = 3.52628\n",
      "epoch no.1 train no.129870  loss = 4.00821 avg_loss = 3.55774\n",
      "epoch no.1 train no.129880  loss = 3.38032 avg_loss = 3.54840\n",
      "epoch no.1 train no.129890  loss = 4.37584 avg_loss = 3.54072\n",
      "epoch no.1 train no.129900  loss = 3.28489 avg_loss = 3.52584\n",
      "epoch no.1 train no.129910  loss = 4.73037 avg_loss = 3.54650\n",
      "epoch no.1 train no.129920  loss = 4.12746 avg_loss = 3.53198\n",
      "epoch no.1 train no.129930  loss = 4.01834 avg_loss = 3.59413\n",
      "epoch no.1 train no.129940  loss = 3.47608 avg_loss = 3.54223\n",
      "epoch no.1 train no.129950  loss = 3.13055 avg_loss = 3.52746\n",
      "epoch no.1 train no.129960  loss = 2.55338 avg_loss = 3.55349\n",
      "epoch no.1 train no.129970  loss = 3.18943 avg_loss = 3.56306\n",
      "epoch no.1 train no.129980  loss = 5.86633 avg_loss = 3.60920\n",
      "epoch no.1 train no.129990  loss = 4.22346 avg_loss = 3.59968\n",
      "epoch no.1 train no.130000  loss = 3.93487 avg_loss = 3.62228\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.1 train no.130010  loss = 3.43623 avg_loss = 3.59673\n",
      "epoch no.1 train no.130020  loss = 2.90459 avg_loss = 3.63622\n",
      "epoch no.1 train no.130030  loss = 2.99936 avg_loss = 3.67086\n",
      "epoch no.1 train no.130040  loss = 3.65801 avg_loss = 3.65137\n",
      "epoch no.1 train no.130050  loss = 3.31003 avg_loss = 3.66792\n",
      "epoch no.1 train no.130060  loss = 5.88256 avg_loss = 3.67974\n",
      "epoch no.1 train no.130070  loss = 4.68001 avg_loss = 3.69389\n",
      "epoch no.1 train no.130080  loss = 5.51058 avg_loss = 3.71009\n",
      "epoch no.1 train no.130090  loss = 3.17526 avg_loss = 3.65835\n",
      "epoch no.1 train no.130100  loss = 3.23014 avg_loss = 3.69310\n",
      "epoch no.1 train no.130110  loss = 3.66920 avg_loss = 3.72693\n",
      "epoch no.1 train no.130120  loss = 4.06898 avg_loss = 3.69764\n",
      "epoch no.1 train no.130130  loss = 3.75656 avg_loss = 3.74562\n",
      "epoch no.1 train no.130140  loss = 3.63887 avg_loss = 3.73750\n",
      "epoch no.1 train no.130150  loss = 2.39216 avg_loss = 3.70043\n",
      "epoch no.1 train no.130160  loss = 5.17632 avg_loss = 3.70800\n",
      "epoch no.1 train no.130170  loss = 3.43452 avg_loss = 3.65920\n",
      "epoch no.1 train no.130180  loss = 3.64822 avg_loss = 3.65399\n",
      "epoch no.1 train no.130190  loss = 5.90429 avg_loss = 3.62730\n",
      "epoch no.1 train no.130200  loss = 4.47025 avg_loss = 3.63433\n",
      "epoch no.1 train no.130210  loss = 2.15318 avg_loss = 3.59315\n",
      "epoch no.1 train no.130220  loss = 4.27743 avg_loss = 3.64155\n",
      "epoch no.1 train no.130230  loss = 3.08836 avg_loss = 3.65823\n",
      "epoch no.1 train no.130240  loss = 3.35300 avg_loss = 3.64686\n",
      "epoch no.1 train no.130250  loss = 3.65538 avg_loss = 3.65436\n",
      "epoch no.1 train no.130260  loss = 4.30993 avg_loss = 3.68455\n",
      "epoch no.1 train no.130270  loss = 3.27790 avg_loss = 3.66647\n",
      "epoch no.1 train no.130280  loss = 2.54368 avg_loss = 3.69041\n",
      "epoch no.1 train no.130290  loss = 3.44402 avg_loss = 3.67835\n",
      "epoch no.1 train no.130300  loss = 3.40596 avg_loss = 3.66014\n",
      "epoch no.1 train no.130310  loss = 4.28545 avg_loss = 3.67599\n",
      "epoch no.1 train no.130320  loss = 4.34174 avg_loss = 3.63543\n",
      "epoch no.1 train no.130330  loss = 1.92253 avg_loss = 3.63424\n",
      "epoch no.1 train no.130340  loss = 2.58813 avg_loss = 3.62131\n",
      "epoch no.1 train no.130350  loss = 3.43954 avg_loss = 3.66379\n",
      "epoch no.1 train no.130360  loss = 3.54056 avg_loss = 3.64805\n",
      "epoch no.1 train no.130370  loss = 4.92292 avg_loss = 3.65087\n",
      "epoch no.1 train no.130380  loss = 4.05003 avg_loss = 3.62841\n",
      "epoch no.1 train no.130390  loss = 5.39681 avg_loss = 3.66957\n",
      "epoch no.1 train no.130400  loss = 5.02795 avg_loss = 3.68610\n",
      "epoch no.1 train no.130410  loss = 4.27290 avg_loss = 3.68795\n",
      "epoch no.1 train no.130420  loss = 4.84767 avg_loss = 3.66683\n",
      "epoch no.1 train no.130430  loss = 5.54358 avg_loss = 3.67198\n",
      "epoch no.1 train no.130440  loss = 3.21374 avg_loss = 3.59179\n",
      "epoch no.1 train no.130450  loss = 3.29142 avg_loss = 3.58138\n",
      "epoch no.1 train no.130460  loss = 3.74332 avg_loss = 3.60089\n",
      "epoch no.1 train no.130470  loss = 3.21843 avg_loss = 3.59382\n",
      "epoch no.1 train no.130480  loss = 4.64134 avg_loss = 3.67295\n",
      "epoch no.1 train no.130490  loss = 3.91309 avg_loss = 3.64323\n",
      "epoch no.1 train no.130500  loss = 3.51497 avg_loss = 3.62652\n",
      "epoch no.1 train no.130510  loss = 5.16828 avg_loss = 3.59097\n",
      "epoch no.1 train no.130520  loss = 3.28384 avg_loss = 3.61258\n",
      "epoch no.1 train no.130530  loss = 3.07390 avg_loss = 3.63666\n",
      "epoch no.1 train no.130540  loss = 4.12845 avg_loss = 3.65863\n",
      "epoch no.1 train no.130550  loss = 4.56024 avg_loss = 3.71803\n",
      "epoch no.1 train no.130560  loss = 1.96038 avg_loss = 3.74853\n",
      "epoch no.1 train no.130570  loss = 3.18400 avg_loss = 3.76554\n",
      "epoch no.1 train no.130580  loss = 4.07298 avg_loss = 3.82818\n",
      "epoch no.1 train no.130590  loss = 3.75101 avg_loss = 3.77843\n",
      "epoch no.1 train no.130600  loss = 1.93577 avg_loss = 3.72458\n",
      "epoch no.1 train no.130610  loss = 2.59145 avg_loss = 3.70531\n",
      "epoch no.1 train no.130620  loss = 2.85959 avg_loss = 3.63594\n",
      "epoch no.1 train no.130630  loss = 6.72114 avg_loss = 3.62089\n",
      "epoch no.1 train no.130640  loss = 2.98900 avg_loss = 3.62753\n",
      "epoch no.1 train no.130650  loss = 2.57721 avg_loss = 3.60124\n",
      "epoch no.1 train no.130660  loss = 3.19698 avg_loss = 3.58262\n",
      "epoch no.1 train no.130670  loss = 3.46193 avg_loss = 3.60928\n",
      "epoch no.1 train no.130680  loss = 3.04609 avg_loss = 3.64689\n",
      "epoch no.1 train no.130690  loss = 3.20490 avg_loss = 3.58576\n",
      "epoch no.1 train no.130700  loss = 4.34913 avg_loss = 3.63816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.130710  loss = 2.78938 avg_loss = 3.62756\n",
      "epoch no.1 train no.130720  loss = 4.87439 avg_loss = 3.63903\n",
      "epoch no.1 train no.130730  loss = 3.81371 avg_loss = 3.65033\n",
      "epoch no.1 train no.130740  loss = 2.55612 avg_loss = 3.68543\n",
      "epoch no.1 train no.130750  loss = 3.29105 avg_loss = 3.73998\n",
      "epoch no.1 train no.130760  loss = 3.28434 avg_loss = 3.70731\n",
      "epoch no.1 train no.130770  loss = 1.41260 avg_loss = 3.70187\n",
      "epoch no.1 train no.130780  loss = 3.45050 avg_loss = 3.72887\n",
      "epoch no.1 train no.130790  loss = 4.11977 avg_loss = 3.74347\n",
      "epoch no.1 train no.130800  loss = 4.15056 avg_loss = 3.76412\n",
      "epoch no.1 train no.130810  loss = 3.51379 avg_loss = 3.76106\n",
      "epoch no.1 train no.130820  loss = 1.99920 avg_loss = 3.71890\n",
      "epoch no.1 train no.130830  loss = 3.86258 avg_loss = 3.70156\n",
      "epoch no.1 train no.130840  loss = 3.72927 avg_loss = 3.73256\n",
      "epoch no.1 train no.130850  loss = 3.65102 avg_loss = 3.73269\n",
      "epoch no.1 train no.130860  loss = 6.41882 avg_loss = 3.68810\n",
      "epoch no.1 train no.130870  loss = 4.04172 avg_loss = 3.70080\n",
      "epoch no.1 train no.130880  loss = 2.72727 avg_loss = 3.71643\n",
      "epoch no.1 train no.130890  loss = 3.78170 avg_loss = 3.73948\n",
      "epoch no.1 train no.130900  loss = 2.73653 avg_loss = 3.69497\n",
      "epoch no.1 train no.130910  loss = 6.34074 avg_loss = 3.72826\n",
      "epoch no.1 train no.130920  loss = 2.49088 avg_loss = 3.67424\n",
      "epoch no.1 train no.130930  loss = 3.43995 avg_loss = 3.66522\n",
      "epoch no.1 train no.130940  loss = 2.92670 avg_loss = 3.66649\n",
      "epoch no.1 train no.130950  loss = 7.08664 avg_loss = 3.74937\n",
      "epoch no.1 train no.130960  loss = 1.94160 avg_loss = 3.72397\n",
      "epoch no.1 train no.130970  loss = 3.65604 avg_loss = 3.73222\n",
      "epoch no.1 train no.130980  loss = 3.83557 avg_loss = 3.74414\n",
      "epoch no.1 train no.130990  loss = 2.40937 avg_loss = 3.70505\n",
      "epoch no.1 train no.131000  loss = 4.74024 avg_loss = 3.70634\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁모음', 'est', '</s>']\n",
      "추억의 발라드 best</s>\n",
      "epoch no.1 train no.131010  loss = 3.09747 avg_loss = 3.68314\n",
      "epoch no.1 train no.131020  loss = 3.20202 avg_loss = 3.74105\n",
      "epoch no.1 train no.131030  loss = 3.06781 avg_loss = 3.75648\n",
      "epoch no.1 train no.131040  loss = 2.90431 avg_loss = 3.76098\n",
      "epoch no.1 train no.131050  loss = 3.19314 avg_loss = 3.76652\n",
      "epoch no.1 train no.131060  loss = 4.27024 avg_loss = 3.77339\n",
      "epoch no.1 train no.131070  loss = 2.35148 avg_loss = 3.77074\n",
      "epoch no.1 train no.131080  loss = 3.97093 avg_loss = 3.74067\n",
      "epoch no.1 train no.131090  loss = 2.61506 avg_loss = 3.76150\n",
      "epoch no.1 train no.131100  loss = 2.55643 avg_loss = 3.68142\n",
      "epoch no.1 train no.131110  loss = 4.89274 avg_loss = 3.66424\n",
      "epoch no.1 train no.131120  loss = 3.44691 avg_loss = 3.62988\n",
      "epoch no.1 train no.131130  loss = 2.35030 avg_loss = 3.60384\n",
      "epoch no.1 train no.131140  loss = 4.50671 avg_loss = 3.62625\n",
      "epoch no.1 train no.131150  loss = 1.93888 avg_loss = 3.65043\n",
      "epoch no.1 train no.131160  loss = 4.21628 avg_loss = 3.68009\n",
      "epoch no.1 train no.131170  loss = 3.20892 avg_loss = 3.69216\n",
      "epoch no.1 train no.131180  loss = 4.64294 avg_loss = 3.66920\n",
      "epoch no.1 train no.131190  loss = 2.86019 avg_loss = 3.68143\n",
      "epoch no.1 train no.131200  loss = 2.64654 avg_loss = 3.58565\n",
      "epoch no.1 train no.131210  loss = 3.18031 avg_loss = 3.57738\n",
      "epoch no.1 train no.131220  loss = 3.09498 avg_loss = 3.54368\n",
      "epoch no.1 train no.131230  loss = 2.66295 avg_loss = 3.52879\n",
      "epoch no.1 train no.131240  loss = 5.77315 avg_loss = 3.53060\n",
      "epoch no.1 train no.131250  loss = 3.57171 avg_loss = 3.47048\n",
      "epoch no.1 train no.131260  loss = 3.25627 avg_loss = 3.42734\n",
      "epoch no.1 train no.131270  loss = 4.30781 avg_loss = 3.48019\n",
      "epoch no.1 train no.131280  loss = 4.00124 avg_loss = 3.47535\n",
      "epoch no.1 train no.131290  loss = 3.49188 avg_loss = 3.49806\n",
      "epoch no.1 train no.131300  loss = 5.04606 avg_loss = 3.49741\n",
      "epoch no.1 train no.131310  loss = 3.99966 avg_loss = 3.54853\n",
      "epoch no.1 train no.131320  loss = 4.03309 avg_loss = 3.57737\n",
      "epoch no.1 train no.131330  loss = 3.33006 avg_loss = 3.64995\n",
      "epoch no.1 train no.131340  loss = 4.33704 avg_loss = 3.65682\n",
      "epoch no.1 train no.131350  loss = 4.93631 avg_loss = 3.71226\n",
      "epoch no.1 train no.131360  loss = 3.28795 avg_loss = 3.67306\n",
      "epoch no.1 train no.131370  loss = 2.05815 avg_loss = 3.68659\n",
      "epoch no.1 train no.131380  loss = 5.31064 avg_loss = 3.65404\n",
      "epoch no.1 train no.131390  loss = 4.95402 avg_loss = 3.64465\n",
      "epoch no.1 train no.131400  loss = 3.43353 avg_loss = 3.64642\n",
      "epoch no.1 train no.131410  loss = 2.37675 avg_loss = 3.60008\n",
      "epoch no.1 train no.131420  loss = 2.37886 avg_loss = 3.63089\n",
      "epoch no.1 train no.131430  loss = 3.54086 avg_loss = 3.70090\n",
      "epoch no.1 train no.131440  loss = 7.74981 avg_loss = 3.70219\n",
      "epoch no.1 train no.131450  loss = 5.94187 avg_loss = 3.71348\n",
      "epoch no.1 train no.131460  loss = 3.56362 avg_loss = 3.66093\n",
      "epoch no.1 train no.131470  loss = 3.44460 avg_loss = 3.70389\n",
      "epoch no.1 train no.131480  loss = 2.95610 avg_loss = 3.75534\n",
      "epoch no.1 train no.131490  loss = 3.24991 avg_loss = 3.77093\n",
      "epoch no.1 train no.131500  loss = 3.10708 avg_loss = 3.70500\n",
      "epoch no.1 train no.131510  loss = 4.07772 avg_loss = 3.70688\n",
      "epoch no.1 train no.131520  loss = 2.71723 avg_loss = 3.64691\n",
      "epoch no.1 train no.131530  loss = 4.70350 avg_loss = 3.65552\n",
      "epoch no.1 train no.131540  loss = 5.84831 avg_loss = 3.62153\n",
      "epoch no.1 train no.131550  loss = 5.09528 avg_loss = 3.68597\n",
      "epoch no.1 train no.131560  loss = 3.93838 avg_loss = 3.73661\n",
      "epoch no.1 train no.131570  loss = 3.33302 avg_loss = 3.73979\n",
      "epoch no.1 train no.131580  loss = 2.20632 avg_loss = 3.67523\n",
      "epoch no.1 train no.131590  loss = 5.43197 avg_loss = 3.67306\n",
      "epoch no.1 train no.131600  loss = 3.60649 avg_loss = 3.67618\n",
      "epoch no.1 train no.131610  loss = 2.80256 avg_loss = 3.65797\n",
      "epoch no.1 train no.131620  loss = 3.69422 avg_loss = 3.66237\n",
      "epoch no.1 train no.131630  loss = 4.71630 avg_loss = 3.69731\n",
      "epoch no.1 train no.131640  loss = 3.09333 avg_loss = 3.71157\n",
      "epoch no.1 train no.131650  loss = 2.67770 avg_loss = 3.65042\n",
      "epoch no.1 train no.131660  loss = 4.95582 avg_loss = 3.69122\n",
      "epoch no.1 train no.131670  loss = 7.04142 avg_loss = 3.68807\n",
      "epoch no.1 train no.131680  loss = 3.04071 avg_loss = 3.68603\n",
      "epoch no.1 train no.131690  loss = 4.20209 avg_loss = 3.68534\n",
      "epoch no.1 train no.131700  loss = 4.67410 avg_loss = 3.73106\n",
      "epoch no.1 train no.131710  loss = 1.93686 avg_loss = 3.71639\n",
      "epoch no.1 train no.131720  loss = 3.51559 avg_loss = 3.76115\n",
      "epoch no.1 train no.131730  loss = 3.15709 avg_loss = 3.73157\n",
      "epoch no.1 train no.131740  loss = 4.39837 avg_loss = 3.72596\n",
      "epoch no.1 train no.131750  loss = 3.51150 avg_loss = 3.73810\n",
      "epoch no.1 train no.131760  loss = 2.72893 avg_loss = 3.73544\n",
      "epoch no.1 train no.131770  loss = 2.80408 avg_loss = 3.70282\n",
      "epoch no.1 train no.131780  loss = 2.49271 avg_loss = 3.72156\n",
      "epoch no.1 train no.131790  loss = 1.29030 avg_loss = 3.75453\n",
      "epoch no.1 train no.131800  loss = 3.62092 avg_loss = 3.72400\n",
      "epoch no.1 train no.131810  loss = 2.36930 avg_loss = 3.69249\n",
      "epoch no.1 train no.131820  loss = 3.46666 avg_loss = 3.66779\n",
      "epoch no.1 train no.131830  loss = 4.53780 avg_loss = 3.66608\n",
      "epoch no.1 train no.131840  loss = 2.61485 avg_loss = 3.65071\n",
      "epoch no.1 train no.131850  loss = 3.68028 avg_loss = 3.66629\n",
      "epoch no.1 train no.131860  loss = 1.82085 avg_loss = 3.60597\n",
      "epoch no.1 train no.131870  loss = 2.51655 avg_loss = 3.56602\n",
      "epoch no.1 train no.131880  loss = 2.79961 avg_loss = 3.57412\n",
      "epoch no.1 train no.131890  loss = 3.94841 avg_loss = 3.57904\n",
      "epoch no.1 train no.131900  loss = 5.22615 avg_loss = 3.62950\n",
      "epoch no.1 train no.131910  loss = 3.21122 avg_loss = 3.65907\n",
      "epoch no.1 train no.131920  loss = 4.12001 avg_loss = 3.63984\n",
      "epoch no.1 train no.131930  loss = 3.20846 avg_loss = 3.63087\n",
      "epoch no.1 train no.131940  loss = 4.95605 avg_loss = 3.60113\n",
      "epoch no.1 train no.131950  loss = 2.64096 avg_loss = 3.57907\n",
      "epoch no.1 train no.131960  loss = 3.07629 avg_loss = 3.59721\n",
      "epoch no.1 train no.131970  loss = 3.42849 avg_loss = 3.60182\n",
      "epoch no.1 train no.131980  loss = 4.94936 avg_loss = 3.64912\n",
      "epoch no.1 train no.131990  loss = 2.88767 avg_loss = 3.70896\n",
      "epoch no.1 train no.132000  loss = 4.07615 avg_loss = 3.76639\n",
      "6\n",
      "to_tokens: ['▁가을', '▁90', '들', '▁모음', '년대', '</s>', '드', '</s>']\n",
      "추억의 노래들 90년대 발라드</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.132010  loss = 4.25533 avg_loss = 3.76875\n",
      "epoch no.1 train no.132020  loss = 3.09846 avg_loss = 3.76731\n",
      "epoch no.1 train no.132030  loss = 6.73594 avg_loss = 3.75428\n",
      "epoch no.1 train no.132040  loss = 4.50024 avg_loss = 3.71611\n",
      "epoch no.1 train no.132050  loss = 5.72836 avg_loss = 3.72257\n",
      "epoch no.1 train no.132060  loss = 5.80159 avg_loss = 3.77448\n",
      "epoch no.1 train no.132070  loss = 4.12664 avg_loss = 3.69194\n",
      "epoch no.1 train no.132080  loss = 3.34014 avg_loss = 3.71279\n",
      "epoch no.1 train no.132090  loss = 3.93891 avg_loss = 3.73863\n",
      "epoch no.1 train no.132100  loss = 3.22259 avg_loss = 3.72077\n",
      "epoch no.1 train no.132110  loss = 3.48799 avg_loss = 3.73219\n",
      "epoch no.1 train no.132120  loss = 7.02513 avg_loss = 3.78137\n",
      "epoch no.1 train no.132130  loss = 4.92431 avg_loss = 3.82513\n",
      "epoch no.1 train no.132140  loss = 4.60624 avg_loss = 3.85800\n",
      "epoch no.1 train no.132150  loss = 3.74882 avg_loss = 3.81090\n",
      "epoch no.1 train no.132160  loss = 2.99276 avg_loss = 3.75711\n",
      "epoch no.1 train no.132170  loss = 4.37363 avg_loss = 3.73188\n",
      "epoch no.1 train no.132180  loss = 2.95641 avg_loss = 3.67919\n",
      "epoch no.1 train no.132190  loss = 3.98899 avg_loss = 3.61828\n",
      "epoch no.1 train no.132200  loss = 2.42847 avg_loss = 3.51948\n",
      "epoch no.1 train no.132210  loss = 4.58729 avg_loss = 3.51516\n",
      "epoch no.1 train no.132220  loss = 2.43295 avg_loss = 3.47791\n",
      "epoch no.1 train no.132230  loss = 2.95921 avg_loss = 3.47581\n",
      "epoch no.1 train no.132240  loss = 2.08624 avg_loss = 3.49013\n",
      "epoch no.1 train no.132250  loss = 5.36836 avg_loss = 3.53716\n",
      "epoch no.1 train no.132260  loss = 1.47072 avg_loss = 3.55104\n",
      "epoch no.1 train no.132270  loss = 2.60410 avg_loss = 3.54834\n",
      "epoch no.1 train no.132280  loss = 4.29721 avg_loss = 3.54674\n",
      "epoch no.1 train no.132290  loss = 4.47147 avg_loss = 3.56720\n",
      "epoch no.1 train no.132300  loss = 1.84835 avg_loss = 3.57626\n",
      "epoch no.1 train no.132310  loss = 4.72976 avg_loss = 3.64765\n",
      "epoch no.1 train no.132320  loss = 3.12856 avg_loss = 3.63240\n",
      "epoch no.1 train no.132330  loss = 4.57856 avg_loss = 3.67392\n",
      "epoch no.1 train no.132340  loss = 6.10997 avg_loss = 3.69425\n",
      "epoch no.1 train no.132350  loss = 2.90705 avg_loss = 3.65411\n",
      "epoch no.1 train no.132360  loss = 3.52977 avg_loss = 3.66341\n",
      "epoch no.1 train no.132370  loss = 3.95079 avg_loss = 3.67803\n",
      "epoch no.1 train no.132380  loss = 3.10157 avg_loss = 3.66032\n",
      "epoch no.1 train no.132390  loss = 2.72266 avg_loss = 3.62966\n",
      "epoch no.1 train no.132400  loss = 2.60229 avg_loss = 3.66837\n",
      "epoch no.1 train no.132410  loss = 3.88399 avg_loss = 3.71626\n",
      "epoch no.1 train no.132420  loss = 1.70640 avg_loss = 3.70763\n",
      "epoch no.1 train no.132430  loss = 3.43380 avg_loss = 3.77489\n",
      "epoch no.1 train no.132440  loss = 4.00885 avg_loss = 3.73460\n",
      "epoch no.1 train no.132450  loss = 3.10527 avg_loss = 3.78297\n",
      "epoch no.1 train no.132460  loss = 4.30120 avg_loss = 3.70460\n",
      "epoch no.1 train no.132470  loss = 4.58531 avg_loss = 3.74152\n",
      "epoch no.1 train no.132480  loss = 4.12523 avg_loss = 3.82723\n",
      "epoch no.1 train no.132490  loss = 4.73275 avg_loss = 3.77668\n",
      "epoch no.1 train no.132500  loss = 3.36548 avg_loss = 3.68894\n",
      "epoch no.1 train no.132510  loss = 2.55727 avg_loss = 3.66282\n",
      "epoch no.1 train no.132520  loss = 3.20407 avg_loss = 3.65086\n",
      "epoch no.1 train no.132530  loss = 5.89158 avg_loss = 3.63182\n",
      "epoch no.1 train no.132540  loss = 3.53133 avg_loss = 3.57368\n",
      "epoch no.1 train no.132550  loss = 3.10696 avg_loss = 3.57587\n",
      "epoch no.1 train no.132560  loss = 4.38557 avg_loss = 3.64431\n",
      "epoch no.1 train no.132570  loss = 5.58891 avg_loss = 3.71005\n",
      "epoch no.1 train no.132580  loss = 5.85392 avg_loss = 3.69559\n",
      "epoch no.1 train no.132590  loss = 4.96206 avg_loss = 3.71705\n",
      "epoch no.1 train no.132600  loss = 2.94166 avg_loss = 3.76181\n",
      "epoch no.1 train no.132610  loss = 2.36974 avg_loss = 3.71468\n",
      "epoch no.1 train no.132620  loss = 1.90977 avg_loss = 3.72712\n",
      "epoch no.1 train no.132630  loss = 2.94353 avg_loss = 3.76000\n",
      "epoch no.1 train no.132640  loss = 2.09738 avg_loss = 3.66861\n",
      "epoch no.1 train no.132650  loss = 4.04482 avg_loss = 3.65762\n",
      "epoch no.1 train no.132660  loss = 2.04824 avg_loss = 3.66735\n",
      "epoch no.1 train no.132670  loss = 5.80337 avg_loss = 3.61446\n",
      "epoch no.1 train no.132680  loss = 4.78031 avg_loss = 3.67103\n",
      "epoch no.1 train no.132690  loss = 2.69750 avg_loss = 3.68878\n",
      "epoch no.1 train no.132700  loss = 2.82152 avg_loss = 3.75600\n",
      "epoch no.1 train no.132710  loss = 3.60711 avg_loss = 3.74749\n",
      "epoch no.1 train no.132720  loss = 2.94146 avg_loss = 3.74968\n",
      "epoch no.1 train no.132730  loss = 3.55611 avg_loss = 3.73454\n",
      "epoch no.1 train no.132740  loss = 2.79054 avg_loss = 3.71426\n",
      "epoch no.1 train no.132750  loss = 2.79413 avg_loss = 3.67679\n",
      "epoch no.1 train no.132760  loss = 2.77212 avg_loss = 3.67501\n",
      "epoch no.1 train no.132770  loss = 6.32393 avg_loss = 3.70415\n",
      "epoch no.1 train no.132780  loss = 2.87180 avg_loss = 3.67082\n",
      "epoch no.1 train no.132790  loss = 3.28664 avg_loss = 3.72976\n",
      "epoch no.1 train no.132800  loss = 2.39085 avg_loss = 3.71725\n",
      "epoch no.1 train no.132810  loss = 4.01220 avg_loss = 3.72121\n",
      "epoch no.1 train no.132820  loss = 3.28258 avg_loss = 3.67039\n",
      "epoch no.1 train no.132830  loss = 2.73920 avg_loss = 3.70354\n",
      "epoch no.1 train no.132840  loss = 3.95151 avg_loss = 3.70327\n",
      "epoch no.1 train no.132850  loss = 5.72497 avg_loss = 3.77254\n",
      "epoch no.1 train no.132860  loss = 4.58661 avg_loss = 3.76941\n",
      "epoch no.1 train no.132870  loss = 1.90425 avg_loss = 3.75397\n",
      "epoch no.1 train no.132880  loss = 4.78890 avg_loss = 3.76874\n",
      "epoch no.1 train no.132890  loss = 2.49461 avg_loss = 3.72174\n",
      "epoch no.1 train no.132900  loss = 3.53586 avg_loss = 3.74123\n",
      "epoch no.1 train no.132910  loss = 3.17698 avg_loss = 3.71356\n",
      "epoch no.1 train no.132920  loss = 4.77330 avg_loss = 3.67674\n",
      "epoch no.1 train no.132930  loss = 2.35610 avg_loss = 3.65793\n",
      "epoch no.1 train no.132940  loss = 2.83416 avg_loss = 3.70003\n",
      "epoch no.1 train no.132950  loss = 2.91100 avg_loss = 3.70170\n",
      "epoch no.1 train no.132960  loss = 5.65817 avg_loss = 3.68116\n",
      "epoch no.1 train no.132970  loss = 1.60491 avg_loss = 3.68169\n",
      "epoch no.1 train no.132980  loss = 3.04658 avg_loss = 3.70493\n",
      "epoch no.1 train no.132990  loss = 5.32775 avg_loss = 3.71549\n",
      "epoch no.1 train no.133000  loss = 3.61360 avg_loss = 3.68507\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '송', '▁모음', '집', '</s>']\n",
      "추억의 팝송 모음 1</s>\n",
      "epoch no.1 train no.133010  loss = 2.37236 avg_loss = 3.67127\n",
      "epoch no.1 train no.133020  loss = 5.58347 avg_loss = 3.68265\n",
      "epoch no.1 train no.133030  loss = 3.77396 avg_loss = 3.71971\n",
      "epoch no.1 train no.133040  loss = 3.19546 avg_loss = 3.74073\n",
      "epoch no.1 train no.133050  loss = 2.49612 avg_loss = 3.74673\n",
      "epoch no.1 train no.133060  loss = 3.38157 avg_loss = 3.75059\n",
      "epoch no.1 train no.133070  loss = 1.87298 avg_loss = 3.76638\n",
      "epoch no.1 train no.133080  loss = 1.22007 avg_loss = 3.70756\n",
      "epoch no.1 train no.133090  loss = 2.15712 avg_loss = 3.76244\n",
      "epoch no.1 train no.133100  loss = 4.32422 avg_loss = 3.78537\n",
      "epoch no.1 train no.133110  loss = 2.52054 avg_loss = 3.73413\n",
      "epoch no.1 train no.133120  loss = 3.70570 avg_loss = 3.69678\n",
      "epoch no.1 train no.133130  loss = 2.59334 avg_loss = 3.70964\n",
      "epoch no.1 train no.133140  loss = 3.35696 avg_loss = 3.70874\n",
      "epoch no.1 train no.133150  loss = 2.78396 avg_loss = 3.74840\n",
      "epoch no.1 train no.133160  loss = 4.42224 avg_loss = 3.78040\n",
      "epoch no.1 train no.133170  loss = 3.79616 avg_loss = 3.75483\n",
      "epoch no.1 train no.133180  loss = 3.20641 avg_loss = 3.75979\n",
      "epoch no.1 train no.133190  loss = 4.10669 avg_loss = 3.72646\n",
      "epoch no.1 train no.133200  loss = 3.95136 avg_loss = 3.72174\n",
      "epoch no.1 train no.133210  loss = 1.90509 avg_loss = 3.73470\n",
      "epoch no.1 train no.133220  loss = 3.76319 avg_loss = 3.71653\n",
      "epoch no.1 train no.133230  loss = 6.31883 avg_loss = 3.75794\n",
      "epoch no.1 train no.133240  loss = 2.31122 avg_loss = 3.72184\n",
      "epoch no.1 train no.133250  loss = 2.10822 avg_loss = 3.64245\n",
      "epoch no.1 train no.133260  loss = 5.13387 avg_loss = 3.68030\n",
      "epoch no.1 train no.133270  loss = 4.64177 avg_loss = 3.69055\n",
      "epoch no.1 train no.133280  loss = 3.46645 avg_loss = 3.69519\n",
      "epoch no.1 train no.133290  loss = 3.35392 avg_loss = 3.66468\n",
      "epoch no.1 train no.133300  loss = 4.77576 avg_loss = 3.66256\n",
      "epoch no.1 train no.133310  loss = 2.91560 avg_loss = 3.66874\n",
      "epoch no.1 train no.133320  loss = 2.68900 avg_loss = 3.67640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.133330  loss = 4.77920 avg_loss = 3.67601\n",
      "epoch no.1 train no.133340  loss = 3.21789 avg_loss = 3.68611\n",
      "epoch no.1 train no.133350  loss = 3.56794 avg_loss = 3.63447\n",
      "epoch no.1 train no.133360  loss = 2.87090 avg_loss = 3.60486\n",
      "epoch no.1 train no.133370  loss = 2.61880 avg_loss = 3.61873\n",
      "epoch no.1 train no.133380  loss = 2.23498 avg_loss = 3.57928\n",
      "epoch no.1 train no.133390  loss = 5.25312 avg_loss = 3.55881\n",
      "epoch no.1 train no.133400  loss = 4.06851 avg_loss = 3.61946\n",
      "epoch no.1 train no.133410  loss = 5.91353 avg_loss = 3.69634\n",
      "epoch no.1 train no.133420  loss = 3.47163 avg_loss = 3.67968\n",
      "epoch no.1 train no.133430  loss = 2.62501 avg_loss = 3.68956\n",
      "epoch no.1 train no.133440  loss = 2.68132 avg_loss = 3.70067\n",
      "epoch no.1 train no.133450  loss = 3.85153 avg_loss = 3.71164\n",
      "epoch no.1 train no.133460  loss = 3.18029 avg_loss = 3.70783\n",
      "epoch no.1 train no.133470  loss = 3.51172 avg_loss = 3.67159\n",
      "epoch no.1 train no.133480  loss = 4.08595 avg_loss = 3.72535\n",
      "epoch no.1 train no.133490  loss = 3.07609 avg_loss = 3.67435\n",
      "epoch no.1 train no.133500  loss = 4.71373 avg_loss = 3.71333\n",
      "epoch no.1 train no.133510  loss = 3.46837 avg_loss = 3.76599\n",
      "epoch no.1 train no.133520  loss = 3.70564 avg_loss = 3.73269\n",
      "epoch no.1 train no.133530  loss = 2.64603 avg_loss = 3.66340\n",
      "epoch no.1 train no.133540  loss = 2.78925 avg_loss = 3.63064\n",
      "epoch no.1 train no.133550  loss = 4.19843 avg_loss = 3.70090\n",
      "epoch no.1 train no.133560  loss = 4.46865 avg_loss = 3.76253\n",
      "epoch no.1 train no.133570  loss = 3.37882 avg_loss = 3.80861\n",
      "epoch no.1 train no.133580  loss = 3.20604 avg_loss = 3.74979\n",
      "epoch no.1 train no.133590  loss = 8.18479 avg_loss = 3.86150\n",
      "epoch no.1 train no.133600  loss = 3.16054 avg_loss = 3.84007\n",
      "epoch no.1 train no.133610  loss = 2.24224 avg_loss = 3.76781\n",
      "epoch no.1 train no.133620  loss = 3.19996 avg_loss = 3.74704\n",
      "epoch no.1 train no.133630  loss = 2.52655 avg_loss = 3.72726\n",
      "epoch no.1 train no.133640  loss = 3.78738 avg_loss = 3.70224\n",
      "epoch no.1 train no.133650  loss = 1.92894 avg_loss = 3.69007\n",
      "epoch no.1 train no.133660  loss = 4.02826 avg_loss = 3.71541\n",
      "epoch no.1 train no.133670  loss = 3.46587 avg_loss = 3.66428\n",
      "epoch no.1 train no.133680  loss = 7.12065 avg_loss = 3.73093\n",
      "epoch no.1 train no.133690  loss = 6.11046 avg_loss = 3.74376\n",
      "epoch no.1 train no.133700  loss = 4.13025 avg_loss = 3.69519\n",
      "epoch no.1 train no.133710  loss = 3.69227 avg_loss = 3.69812\n",
      "epoch no.1 train no.133720  loss = 3.36125 avg_loss = 3.75446\n",
      "epoch no.1 train no.133730  loss = 1.93234 avg_loss = 3.71503\n",
      "epoch no.1 train no.133740  loss = 5.90755 avg_loss = 3.73907\n",
      "epoch no.1 train no.133750  loss = 4.02099 avg_loss = 3.76173\n",
      "epoch no.1 train no.133760  loss = 3.77991 avg_loss = 3.80868\n",
      "epoch no.1 train no.133770  loss = 5.22197 avg_loss = 3.82546\n",
      "epoch no.1 train no.133780  loss = 2.36051 avg_loss = 3.78068\n",
      "epoch no.1 train no.133790  loss = 3.27532 avg_loss = 3.74150\n",
      "epoch no.1 train no.133800  loss = 2.02248 avg_loss = 3.69835\n",
      "epoch no.1 train no.133810  loss = 3.18619 avg_loss = 3.63407\n",
      "epoch no.1 train no.133820  loss = 2.23658 avg_loss = 3.61799\n",
      "epoch no.1 train no.133830  loss = 3.74023 avg_loss = 3.58721\n",
      "epoch no.1 train no.133840  loss = 2.59971 avg_loss = 3.54307\n",
      "epoch no.1 train no.133850  loss = 3.85812 avg_loss = 3.50330\n",
      "epoch no.1 train no.133860  loss = 3.60646 avg_loss = 3.50772\n",
      "epoch no.1 train no.133870  loss = 4.01748 avg_loss = 3.55496\n",
      "epoch no.1 train no.133880  loss = 4.10831 avg_loss = 3.57334\n",
      "epoch no.1 train no.133890  loss = 3.69930 avg_loss = 3.58386\n",
      "epoch no.1 train no.133900  loss = 4.51944 avg_loss = 3.64300\n",
      "epoch no.1 train no.133910  loss = 2.04038 avg_loss = 3.56873\n",
      "epoch no.1 train no.133920  loss = 3.28822 avg_loss = 3.57234\n",
      "epoch no.1 train no.133930  loss = 4.68820 avg_loss = 3.57864\n",
      "epoch no.1 train no.133940  loss = 2.61993 avg_loss = 3.60118\n",
      "epoch no.1 train no.133950  loss = 8.09855 avg_loss = 3.70560\n",
      "epoch no.1 train no.133960  loss = 2.35801 avg_loss = 3.65238\n",
      "epoch no.1 train no.133970  loss = 2.87207 avg_loss = 3.67409\n",
      "epoch no.1 train no.133980  loss = 3.63164 avg_loss = 3.68221\n",
      "epoch no.1 train no.133990  loss = 3.94015 avg_loss = 3.70462\n",
      "epoch no.1 train no.134000  loss = 4.84337 avg_loss = 3.73575\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.1 train no.134010  loss = 2.51742 avg_loss = 3.71689\n",
      "epoch no.1 train no.134020  loss = 5.60276 avg_loss = 3.68362\n",
      "epoch no.1 train no.134030  loss = 3.84150 avg_loss = 3.65458\n",
      "epoch no.1 train no.134040  loss = 2.43938 avg_loss = 3.66972\n",
      "epoch no.1 train no.134050  loss = 4.68938 avg_loss = 3.70190\n",
      "epoch no.1 train no.134060  loss = 3.41824 avg_loss = 3.70102\n",
      "epoch no.1 train no.134070  loss = 3.86977 avg_loss = 3.75062\n",
      "epoch no.1 train no.134080  loss = 4.01795 avg_loss = 3.75503\n",
      "epoch no.1 train no.134090  loss = 3.09129 avg_loss = 3.74082\n",
      "epoch no.1 train no.134100  loss = 5.10955 avg_loss = 3.72329\n",
      "epoch no.1 train no.134110  loss = 5.11715 avg_loss = 3.75511\n",
      "epoch no.1 train no.134120  loss = 2.08478 avg_loss = 3.72743\n",
      "epoch no.1 train no.134130  loss = 2.58169 avg_loss = 3.74473\n",
      "epoch no.1 train no.134140  loss = 2.77835 avg_loss = 3.75580\n",
      "epoch no.1 train no.134150  loss = 5.17593 avg_loss = 3.78175\n",
      "epoch no.1 train no.134160  loss = 5.14672 avg_loss = 3.86060\n",
      "epoch no.1 train no.134170  loss = 3.82189 avg_loss = 3.82895\n",
      "epoch no.1 train no.134180  loss = 4.38071 avg_loss = 3.83618\n",
      "epoch no.1 train no.134190  loss = 4.90644 avg_loss = 3.86892\n",
      "epoch no.1 train no.134200  loss = 3.20304 avg_loss = 3.83959\n",
      "epoch no.1 train no.134210  loss = 3.00661 avg_loss = 3.80741\n",
      "epoch no.1 train no.134220  loss = 4.79882 avg_loss = 3.82759\n",
      "epoch no.1 train no.134230  loss = 4.58549 avg_loss = 3.79949\n",
      "epoch no.1 train no.134240  loss = 4.29294 avg_loss = 3.82976\n",
      "epoch no.1 train no.134250  loss = 4.51709 avg_loss = 3.87475\n",
      "epoch no.1 train no.134260  loss = 4.35185 avg_loss = 3.96439\n",
      "epoch no.1 train no.134270  loss = 3.57861 avg_loss = 3.95322\n",
      "epoch no.1 train no.134280  loss = 3.43756 avg_loss = 3.91834\n",
      "epoch no.1 train no.134290  loss = 4.40763 avg_loss = 3.88196\n",
      "epoch no.1 train no.134300  loss = 3.48877 avg_loss = 3.88460\n",
      "epoch no.1 train no.134310  loss = 2.68305 avg_loss = 3.88566\n",
      "epoch no.1 train no.134320  loss = 2.43675 avg_loss = 3.83365\n",
      "epoch no.1 train no.134330  loss = 4.63620 avg_loss = 3.82639\n",
      "epoch no.1 train no.134340  loss = 5.04171 avg_loss = 3.75697\n",
      "epoch no.1 train no.134350  loss = 4.09758 avg_loss = 3.74811\n",
      "epoch no.1 train no.134360  loss = 2.88051 avg_loss = 3.75784\n",
      "epoch no.1 train no.134370  loss = 3.79644 avg_loss = 3.75262\n",
      "epoch no.1 train no.134380  loss = 2.38725 avg_loss = 3.72573\n",
      "epoch no.1 train no.134390  loss = 4.20493 avg_loss = 3.74140\n",
      "epoch no.1 train no.134400  loss = 3.95345 avg_loss = 3.70998\n",
      "epoch no.1 train no.134410  loss = 3.42630 avg_loss = 3.69872\n",
      "epoch no.1 train no.134420  loss = 2.42446 avg_loss = 3.67517\n",
      "epoch no.1 train no.134430  loss = 2.54323 avg_loss = 3.63515\n",
      "epoch no.1 train no.134440  loss = 2.73044 avg_loss = 3.61993\n",
      "epoch no.1 train no.134450  loss = 3.00185 avg_loss = 3.64491\n",
      "epoch no.1 train no.134460  loss = 3.06056 avg_loss = 3.62672\n",
      "epoch no.1 train no.134470  loss = 3.25329 avg_loss = 3.65536\n",
      "epoch no.1 train no.134480  loss = 2.60326 avg_loss = 3.64782\n",
      "epoch no.1 train no.134490  loss = 4.87921 avg_loss = 3.68038\n",
      "epoch no.1 train no.134500  loss = 3.16529 avg_loss = 3.65504\n",
      "epoch no.1 train no.134510  loss = 4.34185 avg_loss = 3.61898\n",
      "epoch no.1 train no.134520  loss = 3.86429 avg_loss = 3.68037\n",
      "epoch no.1 train no.134530  loss = 7.31772 avg_loss = 3.75041\n",
      "epoch no.1 train no.134540  loss = 4.12507 avg_loss = 3.71321\n",
      "epoch no.1 train no.134550  loss = 4.71893 avg_loss = 3.80412\n",
      "epoch no.1 train no.134560  loss = 4.58349 avg_loss = 3.84893\n",
      "epoch no.1 train no.134570  loss = 3.35771 avg_loss = 3.80626\n",
      "epoch no.1 train no.134580  loss = 2.48401 avg_loss = 3.76295\n",
      "epoch no.1 train no.134590  loss = 2.37959 avg_loss = 3.69876\n",
      "epoch no.1 train no.134600  loss = 3.02832 avg_loss = 3.67996\n",
      "epoch no.1 train no.134610  loss = 2.43086 avg_loss = 3.59366\n",
      "epoch no.1 train no.134620  loss = 3.56436 avg_loss = 3.56376\n",
      "epoch no.1 train no.134630  loss = 2.78250 avg_loss = 3.58577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.134640  loss = 3.79259 avg_loss = 3.58645\n",
      "epoch no.1 train no.134650  loss = 3.17040 avg_loss = 3.63936\n",
      "epoch no.1 train no.134660  loss = 4.79057 avg_loss = 3.59615\n",
      "epoch no.1 train no.134670  loss = 4.22293 avg_loss = 3.55848\n",
      "epoch no.1 train no.134680  loss = 4.13999 avg_loss = 3.58308\n",
      "epoch no.1 train no.134690  loss = 4.05264 avg_loss = 3.62901\n",
      "epoch no.1 train no.134700  loss = 3.28675 avg_loss = 3.59576\n",
      "epoch no.1 train no.134710  loss = 4.94923 avg_loss = 3.61678\n",
      "epoch no.1 train no.134720  loss = 3.45556 avg_loss = 3.61178\n",
      "epoch no.1 train no.134730  loss = 2.70220 avg_loss = 3.66156\n",
      "epoch no.1 train no.134740  loss = 3.16398 avg_loss = 3.72110\n",
      "epoch no.1 train no.134750  loss = 1.92325 avg_loss = 3.68249\n",
      "epoch no.1 train no.134760  loss = 2.62116 avg_loss = 3.65724\n",
      "epoch no.1 train no.134770  loss = 3.49267 avg_loss = 3.68686\n",
      "epoch no.1 train no.134780  loss = 4.62445 avg_loss = 3.72498\n",
      "epoch no.1 train no.134790  loss = 3.34287 avg_loss = 3.70362\n",
      "epoch no.1 train no.134800  loss = 5.69448 avg_loss = 3.72184\n",
      "epoch no.1 train no.134810  loss = 4.30618 avg_loss = 3.73812\n",
      "epoch no.1 train no.134820  loss = 2.52594 avg_loss = 3.73932\n",
      "epoch no.1 train no.134830  loss = 4.88556 avg_loss = 3.74895\n",
      "epoch no.1 train no.134840  loss = 4.61228 avg_loss = 3.79409\n",
      "epoch no.1 train no.134850  loss = 4.64518 avg_loss = 3.80322\n",
      "epoch no.1 train no.134860  loss = 3.81288 avg_loss = 3.84168\n",
      "epoch no.1 train no.134870  loss = 3.90772 avg_loss = 3.83933\n",
      "epoch no.1 train no.134880  loss = 2.80363 avg_loss = 3.80090\n",
      "epoch no.1 train no.134890  loss = 5.34951 avg_loss = 3.82147\n",
      "epoch no.1 train no.134900  loss = 1.89627 avg_loss = 3.81476\n",
      "epoch no.1 train no.134910  loss = 2.45998 avg_loss = 3.83027\n",
      "epoch no.1 train no.134920  loss = 3.72278 avg_loss = 3.83378\n",
      "epoch no.1 train no.134930  loss = 2.81425 avg_loss = 3.79066\n",
      "epoch no.1 train no.134940  loss = 4.18003 avg_loss = 3.80652\n",
      "epoch no.1 train no.134950  loss = 2.85734 avg_loss = 3.80185\n",
      "epoch no.1 train no.134960  loss = 4.07606 avg_loss = 3.78060\n",
      "epoch no.1 train no.134970  loss = 2.63562 avg_loss = 3.73989\n",
      "epoch no.1 train no.134980  loss = 2.82999 avg_loss = 3.71323\n",
      "epoch no.1 train no.134990  loss = 3.38775 avg_loss = 3.71521\n",
      "epoch no.1 train no.135000  loss = 3.03974 avg_loss = 3.69230\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.1 train no.135010  loss = 2.67498 avg_loss = 3.67668\n",
      "epoch no.1 train no.135020  loss = 3.33548 avg_loss = 3.66384\n",
      "epoch no.1 train no.135030  loss = 4.34843 avg_loss = 3.68945\n",
      "epoch no.1 train no.135040  loss = 4.92260 avg_loss = 3.71591\n",
      "epoch no.1 train no.135050  loss = 5.25927 avg_loss = 3.76698\n",
      "epoch no.1 train no.135060  loss = 2.84380 avg_loss = 3.73408\n",
      "epoch no.1 train no.135070  loss = 3.97297 avg_loss = 3.75957\n",
      "epoch no.1 train no.135080  loss = 3.72911 avg_loss = 3.71737\n",
      "epoch no.1 train no.135090  loss = 4.74395 avg_loss = 3.71035\n",
      "epoch no.1 train no.135100  loss = 2.28493 avg_loss = 3.68981\n",
      "epoch no.1 train no.135110  loss = 5.60895 avg_loss = 3.75049\n",
      "epoch no.1 train no.135120  loss = 4.29050 avg_loss = 3.75291\n",
      "epoch no.1 train no.135130  loss = 4.51022 avg_loss = 3.77704\n",
      "epoch no.1 train no.135140  loss = 7.27018 avg_loss = 3.79567\n",
      "epoch no.1 train no.135150  loss = 4.10646 avg_loss = 3.81966\n",
      "epoch no.1 train no.135160  loss = 2.29228 avg_loss = 3.73868\n",
      "epoch no.1 train no.135170  loss = 2.11488 avg_loss = 3.67774\n",
      "epoch no.1 train no.135180  loss = 2.27971 avg_loss = 3.63048\n",
      "epoch no.1 train no.135190  loss = 3.48564 avg_loss = 3.65928\n",
      "epoch no.1 train no.135200  loss = 1.93293 avg_loss = 3.63089\n",
      "epoch no.1 train no.135210  loss = 3.20142 avg_loss = 3.61281\n",
      "epoch no.1 train no.135220  loss = 3.56015 avg_loss = 3.58998\n",
      "epoch no.1 train no.135230  loss = 5.67819 avg_loss = 3.64239\n",
      "epoch no.1 train no.135240  loss = 4.40763 avg_loss = 3.63375\n",
      "epoch no.1 train no.135250  loss = 2.00543 avg_loss = 3.60124\n",
      "epoch no.1 train no.135260  loss = 2.38931 avg_loss = 3.56150\n",
      "epoch no.1 train no.135270  loss = 3.96747 avg_loss = 3.60865\n",
      "epoch no.1 train no.135280  loss = 4.27349 avg_loss = 3.63737\n",
      "epoch no.1 train no.135290  loss = 3.47371 avg_loss = 3.64968\n",
      "epoch no.1 train no.135300  loss = 4.25722 avg_loss = 3.67317\n",
      "epoch no.1 train no.135310  loss = 5.04381 avg_loss = 3.70713\n",
      "epoch no.1 train no.135320  loss = 3.60459 avg_loss = 3.69735\n",
      "epoch no.1 train no.135330  loss = 3.19766 avg_loss = 3.69362\n",
      "epoch no.1 train no.135340  loss = 3.53417 avg_loss = 3.69758\n",
      "epoch no.1 train no.135350  loss = 3.68502 avg_loss = 3.64791\n",
      "epoch no.1 train no.135360  loss = 2.58003 avg_loss = 3.65857\n",
      "epoch no.1 train no.135370  loss = 5.86261 avg_loss = 3.64838\n",
      "epoch no.1 train no.135380  loss = 4.09619 avg_loss = 3.63420\n",
      "epoch no.1 train no.135390  loss = 4.69929 avg_loss = 3.68571\n",
      "epoch no.1 train no.135400  loss = 3.97867 avg_loss = 3.68852\n",
      "epoch no.1 train no.135410  loss = 1.47453 avg_loss = 3.66341\n",
      "epoch no.1 train no.135420  loss = 5.41884 avg_loss = 3.63460\n",
      "epoch no.1 train no.135430  loss = 5.34547 avg_loss = 3.61887\n",
      "epoch no.1 train no.135440  loss = 4.97466 avg_loss = 3.62596\n",
      "epoch no.1 train no.135450  loss = 4.56321 avg_loss = 3.67816\n",
      "epoch no.1 train no.135460  loss = 1.73283 avg_loss = 3.68499\n",
      "epoch no.1 train no.135470  loss = 2.68560 avg_loss = 3.65549\n",
      "epoch no.1 train no.135480  loss = 4.30089 avg_loss = 3.71248\n",
      "epoch no.1 train no.135490  loss = 5.60302 avg_loss = 3.75086\n",
      "epoch no.1 train no.135500  loss = 3.66499 avg_loss = 3.77203\n",
      "epoch no.1 train no.135510  loss = 6.26535 avg_loss = 3.79843\n",
      "epoch no.1 train no.135520  loss = 2.82270 avg_loss = 3.77681\n",
      "epoch no.1 train no.135530  loss = 3.73187 avg_loss = 3.75219\n",
      "epoch no.1 train no.135540  loss = 3.30205 avg_loss = 3.70001\n",
      "epoch no.1 train no.135550  loss = 2.40516 avg_loss = 3.66529\n",
      "epoch no.1 train no.135560  loss = 3.60467 avg_loss = 3.68228\n",
      "epoch no.1 train no.135570  loss = 3.82063 avg_loss = 3.67659\n",
      "epoch no.1 train no.135580  loss = 2.06841 avg_loss = 3.63257\n",
      "epoch no.1 train no.135590  loss = 2.33997 avg_loss = 3.66657\n",
      "epoch no.1 train no.135600  loss = 3.31999 avg_loss = 3.70834\n",
      "epoch no.1 train no.135610  loss = 3.21102 avg_loss = 3.71434\n",
      "epoch no.1 train no.135620  loss = 3.10947 avg_loss = 3.66040\n",
      "epoch no.1 train no.135630  loss = 4.74217 avg_loss = 3.65692\n",
      "epoch no.1 train no.135640  loss = 2.46475 avg_loss = 3.66102\n",
      "epoch no.1 train no.135650  loss = 2.76089 avg_loss = 3.65168\n",
      "epoch no.1 train no.135660  loss = 3.78446 avg_loss = 3.62885\n",
      "epoch no.1 train no.135670  loss = 3.17621 avg_loss = 3.69315\n",
      "epoch no.1 train no.135680  loss = 5.74873 avg_loss = 3.71516\n",
      "epoch no.1 train no.135690  loss = 2.98048 avg_loss = 3.68499\n",
      "epoch no.1 train no.135700  loss = 2.91209 avg_loss = 3.72545\n",
      "epoch no.1 train no.135710  loss = 4.34519 avg_loss = 3.72004\n",
      "epoch no.1 train no.135720  loss = 4.30170 avg_loss = 3.67413\n",
      "epoch no.1 train no.135730  loss = 4.17928 avg_loss = 3.67500\n",
      "epoch no.1 train no.135740  loss = 3.85767 avg_loss = 3.64325\n",
      "epoch no.1 train no.135750  loss = 3.16758 avg_loss = 3.66186\n",
      "epoch no.1 train no.135760  loss = 6.00274 avg_loss = 3.67052\n",
      "epoch no.1 train no.135770  loss = 2.92860 avg_loss = 3.65185\n",
      "epoch no.1 train no.135780  loss = 4.05686 avg_loss = 3.65577\n",
      "epoch no.1 train no.135790  loss = 2.48055 avg_loss = 3.71212\n",
      "epoch no.1 train no.135800  loss = 4.92022 avg_loss = 3.72937\n",
      "epoch no.1 train no.135810  loss = 3.48693 avg_loss = 3.73952\n",
      "epoch no.1 train no.135820  loss = 3.82831 avg_loss = 3.79111\n",
      "epoch no.1 train no.135830  loss = 3.63548 avg_loss = 3.81199\n",
      "epoch no.1 train no.135840  loss = 2.66238 avg_loss = 3.74799\n",
      "epoch no.1 train no.135850  loss = 5.06813 avg_loss = 3.71223\n",
      "epoch no.1 train no.135860  loss = 5.01712 avg_loss = 3.73824\n",
      "epoch no.1 train no.135870  loss = 4.04999 avg_loss = 3.72519\n",
      "epoch no.1 train no.135880  loss = 3.17508 avg_loss = 3.79082\n",
      "epoch no.1 train no.135890  loss = 4.64272 avg_loss = 3.76389\n",
      "epoch no.1 train no.135900  loss = 2.56445 avg_loss = 3.77843\n",
      "epoch no.1 train no.135910  loss = 4.02107 avg_loss = 3.76829\n",
      "epoch no.1 train no.135920  loss = 5.78757 avg_loss = 3.75540\n",
      "epoch no.1 train no.135930  loss = 4.26272 avg_loss = 3.71970\n",
      "epoch no.1 train no.135940  loss = 2.23588 avg_loss = 3.67448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.135950  loss = 3.66544 avg_loss = 3.73798\n",
      "epoch no.1 train no.135960  loss = 4.05326 avg_loss = 3.69352\n",
      "epoch no.1 train no.135970  loss = 4.08173 avg_loss = 3.66975\n",
      "epoch no.1 train no.135980  loss = 3.86575 avg_loss = 3.68560\n",
      "epoch no.1 train no.135990  loss = 3.70027 avg_loss = 3.72754\n",
      "epoch no.1 train no.136000  loss = 4.44764 avg_loss = 3.72422\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '▁o', 'st', '▁명', '</s>']\n",
      "추억의 영화 ost 모음</s>\n",
      "epoch no.1 train no.136010  loss = 2.59867 avg_loss = 3.74425\n",
      "epoch no.1 train no.136020  loss = 3.33895 avg_loss = 3.73012\n",
      "epoch no.1 train no.136030  loss = 3.90358 avg_loss = 3.74885\n",
      "epoch no.1 train no.136040  loss = 4.85477 avg_loss = 3.73254\n",
      "epoch no.1 train no.136050  loss = 3.47071 avg_loss = 3.78216\n",
      "epoch no.1 train no.136060  loss = 3.87404 avg_loss = 3.78327\n",
      "epoch no.1 train no.136070  loss = 2.49778 avg_loss = 3.79250\n",
      "epoch no.1 train no.136080  loss = 2.83026 avg_loss = 3.77040\n",
      "epoch no.1 train no.136090  loss = 2.79041 avg_loss = 3.78037\n",
      "epoch no.1 train no.136100  loss = 4.92456 avg_loss = 3.76399\n",
      "epoch no.1 train no.136110  loss = 2.98669 avg_loss = 3.75087\n",
      "epoch no.1 train no.136120  loss = 3.16459 avg_loss = 3.74788\n",
      "epoch no.1 train no.136130  loss = 2.62682 avg_loss = 3.72183\n",
      "epoch no.1 train no.136140  loss = 4.50583 avg_loss = 3.73549\n",
      "epoch no.1 train no.136150  loss = 3.66090 avg_loss = 3.74340\n",
      "epoch no.1 train no.136160  loss = 3.80635 avg_loss = 3.72841\n",
      "epoch no.1 train no.136170  loss = 4.62968 avg_loss = 3.76838\n",
      "epoch no.1 train no.136180  loss = 2.36239 avg_loss = 3.76909\n",
      "epoch no.1 train no.136190  loss = 5.73100 avg_loss = 3.78364\n",
      "epoch no.1 train no.136200  loss = 3.19065 avg_loss = 3.77163\n",
      "epoch no.1 train no.136210  loss = 5.08651 avg_loss = 3.79032\n",
      "epoch no.1 train no.136220  loss = 3.50008 avg_loss = 3.79757\n",
      "epoch no.1 train no.136230  loss = 2.28581 avg_loss = 3.78428\n",
      "epoch no.1 train no.136240  loss = 3.14586 avg_loss = 3.76273\n",
      "epoch no.1 train no.136250  loss = 5.78805 avg_loss = 3.76287\n",
      "epoch no.1 train no.136260  loss = 6.54221 avg_loss = 3.77123\n",
      "epoch no.1 train no.136270  loss = 5.06522 avg_loss = 3.73410\n",
      "epoch no.1 train no.136280  loss = 2.69710 avg_loss = 3.74546\n",
      "epoch no.1 train no.136290  loss = 4.41354 avg_loss = 3.81757\n",
      "epoch no.1 train no.136300  loss = 4.55636 avg_loss = 3.78950\n",
      "epoch no.1 train no.136310  loss = 4.79328 avg_loss = 3.76920\n",
      "epoch no.1 train no.136320  loss = 2.19151 avg_loss = 3.77718\n",
      "epoch no.1 train no.136330  loss = 2.75044 avg_loss = 3.75057\n",
      "epoch no.1 train no.136340  loss = 1.55522 avg_loss = 3.72144\n",
      "epoch no.1 train no.136350  loss = 3.68927 avg_loss = 3.78613\n",
      "epoch no.1 train no.136360  loss = 2.35355 avg_loss = 3.72874\n",
      "epoch no.1 train no.136370  loss = 3.23417 avg_loss = 3.76937\n",
      "epoch no.1 train no.136380  loss = 3.54746 avg_loss = 3.76780\n",
      "epoch no.1 train no.136390  loss = 3.84608 avg_loss = 3.76128\n",
      "epoch no.1 train no.136400  loss = 3.73019 avg_loss = 3.71040\n",
      "epoch no.1 train no.136410  loss = 2.46306 avg_loss = 3.67044\n",
      "epoch no.1 train no.136420  loss = 5.04179 avg_loss = 3.68643\n",
      "epoch no.1 train no.136430  loss = 5.37544 avg_loss = 3.71340\n",
      "epoch no.1 train no.136440  loss = 2.88140 avg_loss = 3.68844\n",
      "epoch no.1 train no.136450  loss = 2.47769 avg_loss = 3.65082\n",
      "epoch no.1 train no.136460  loss = 2.09435 avg_loss = 3.66907\n",
      "epoch no.1 train no.136470  loss = 4.85023 avg_loss = 3.66093\n",
      "epoch no.1 train no.136480  loss = 2.28213 avg_loss = 3.68801\n",
      "epoch no.1 train no.136490  loss = 2.75913 avg_loss = 3.64120\n",
      "epoch no.1 train no.136500  loss = 3.79067 avg_loss = 3.67368\n",
      "epoch no.1 train no.136510  loss = 3.73518 avg_loss = 3.71609\n",
      "epoch no.1 train no.136520  loss = 3.69467 avg_loss = 3.70981\n",
      "epoch no.1 train no.136530  loss = 4.60075 avg_loss = 3.64128\n",
      "epoch no.1 train no.136540  loss = 2.11256 avg_loss = 3.60947\n",
      "epoch no.1 train no.136550  loss = 5.06093 avg_loss = 3.60823\n",
      "epoch no.1 train no.136560  loss = 3.36325 avg_loss = 3.57895\n",
      "epoch no.1 train no.136570  loss = 2.66576 avg_loss = 3.53937\n",
      "epoch no.1 train no.136580  loss = 3.60552 avg_loss = 3.60378\n",
      "epoch no.1 train no.136590  loss = 5.37918 avg_loss = 3.61289\n",
      "epoch no.1 train no.136600  loss = 8.16236 avg_loss = 3.60533\n",
      "epoch no.1 train no.136610  loss = 1.46604 avg_loss = 3.64065\n",
      "epoch no.1 train no.136620  loss = 4.00028 avg_loss = 3.67250\n",
      "epoch no.1 train no.136630  loss = 3.33888 avg_loss = 3.69787\n",
      "epoch no.1 train no.136640  loss = 4.65202 avg_loss = 3.65041\n",
      "epoch no.1 train no.136650  loss = 3.22208 avg_loss = 3.65932\n",
      "epoch no.1 train no.136660  loss = 3.25677 avg_loss = 3.62440\n",
      "epoch no.1 train no.136670  loss = 3.65203 avg_loss = 3.57895\n",
      "epoch no.1 train no.136680  loss = 2.56926 avg_loss = 3.57575\n",
      "epoch no.1 train no.136690  loss = 2.64416 avg_loss = 3.54577\n",
      "epoch no.1 train no.136700  loss = 4.93763 avg_loss = 3.58391\n",
      "epoch no.1 train no.136710  loss = 3.03806 avg_loss = 3.57160\n",
      "epoch no.1 train no.136720  loss = 2.20015 avg_loss = 3.58054\n",
      "epoch no.1 train no.136730  loss = 2.94998 avg_loss = 3.65058\n",
      "epoch no.1 train no.136740  loss = 3.10888 avg_loss = 3.64454\n",
      "epoch no.1 train no.136750  loss = 2.58786 avg_loss = 3.58871\n",
      "epoch no.1 train no.136760  loss = 3.31320 avg_loss = 3.60100\n",
      "epoch no.1 train no.136770  loss = 2.72268 avg_loss = 3.70946\n",
      "epoch no.1 train no.136780  loss = 2.57427 avg_loss = 3.69545\n",
      "epoch no.1 train no.136790  loss = 3.88707 avg_loss = 3.70647\n",
      "epoch no.1 train no.136800  loss = 5.86106 avg_loss = 3.74328\n",
      "epoch no.1 train no.136810  loss = 2.96452 avg_loss = 3.68942\n",
      "epoch no.1 train no.136820  loss = 4.08698 avg_loss = 3.68495\n",
      "epoch no.1 train no.136830  loss = 3.21876 avg_loss = 3.68476\n",
      "epoch no.1 train no.136840  loss = 3.32302 avg_loss = 3.71448\n",
      "epoch no.1 train no.136850  loss = 2.81570 avg_loss = 3.70477\n",
      "epoch no.1 train no.136860  loss = 2.53322 avg_loss = 3.70952\n",
      "epoch no.1 train no.136870  loss = 3.88018 avg_loss = 3.75734\n",
      "epoch no.1 train no.136880  loss = 3.00158 avg_loss = 3.69231\n",
      "epoch no.1 train no.136890  loss = 3.70268 avg_loss = 3.72445\n",
      "epoch no.1 train no.136900  loss = 3.15594 avg_loss = 3.71237\n",
      "epoch no.1 train no.136910  loss = 2.38984 avg_loss = 3.76895\n",
      "epoch no.1 train no.136920  loss = 3.46560 avg_loss = 3.76601\n",
      "epoch no.1 train no.136930  loss = 3.65536 avg_loss = 3.73782\n",
      "epoch no.1 train no.136940  loss = 5.62802 avg_loss = 3.72144\n",
      "epoch no.1 train no.136950  loss = 2.19916 avg_loss = 3.69880\n",
      "epoch no.1 train no.136960  loss = 2.70708 avg_loss = 3.70318\n",
      "epoch no.1 train no.136970  loss = 3.30159 avg_loss = 3.63835\n",
      "epoch no.1 train no.136980  loss = 3.18332 avg_loss = 3.61414\n",
      "epoch no.1 train no.136990  loss = 4.65666 avg_loss = 3.58550\n",
      "epoch no.1 train no.137000  loss = 4.42385 avg_loss = 3.59629\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '드', '▁모음', '</s>', '</s>']\n",
      "추억의 발라드 모음집</s>\n",
      "epoch no.1 train no.137010  loss = 2.55517 avg_loss = 3.56853\n",
      "epoch no.1 train no.137020  loss = 3.68919 avg_loss = 3.56060\n",
      "epoch no.1 train no.137030  loss = 2.20393 avg_loss = 3.61830\n",
      "epoch no.1 train no.137040  loss = 4.02475 avg_loss = 3.60759\n",
      "epoch no.1 train no.137050  loss = 3.70616 avg_loss = 3.63436\n",
      "epoch no.1 train no.137060  loss = 4.54783 avg_loss = 3.71013\n",
      "epoch no.1 train no.137070  loss = 2.50007 avg_loss = 3.75915\n",
      "epoch no.1 train no.137080  loss = 3.65389 avg_loss = 3.73313\n",
      "epoch no.1 train no.137090  loss = 2.90119 avg_loss = 3.69321\n",
      "epoch no.1 train no.137100  loss = 2.75524 avg_loss = 3.70172\n",
      "epoch no.1 train no.137110  loss = 2.90350 avg_loss = 3.69979\n",
      "epoch no.1 train no.137120  loss = 3.36684 avg_loss = 3.66430\n",
      "epoch no.1 train no.137130  loss = 3.41999 avg_loss = 3.66548\n",
      "epoch no.1 train no.137140  loss = 4.22272 avg_loss = 3.63167\n",
      "epoch no.1 train no.137150  loss = 2.08674 avg_loss = 3.61635\n",
      "epoch no.1 train no.137160  loss = 2.64648 avg_loss = 3.56179\n",
      "epoch no.1 train no.137170  loss = 2.80847 avg_loss = 3.53836\n",
      "epoch no.1 train no.137180  loss = 2.30158 avg_loss = 3.50185\n",
      "epoch no.1 train no.137190  loss = 3.67105 avg_loss = 3.53472\n",
      "epoch no.1 train no.137200  loss = 2.87558 avg_loss = 3.55892\n",
      "epoch no.1 train no.137210  loss = 3.04538 avg_loss = 3.59794\n",
      "epoch no.1 train no.137220  loss = 5.00946 avg_loss = 3.64661\n",
      "epoch no.1 train no.137230  loss = 1.81561 avg_loss = 3.61905\n",
      "epoch no.1 train no.137240  loss = 5.08818 avg_loss = 3.61543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.137250  loss = 2.29377 avg_loss = 3.61522\n",
      "epoch no.1 train no.137260  loss = 3.25088 avg_loss = 3.62030\n",
      "epoch no.1 train no.137270  loss = 3.69236 avg_loss = 3.63236\n",
      "epoch no.1 train no.137280  loss = 5.87398 avg_loss = 3.63910\n",
      "epoch no.1 train no.137290  loss = 3.89000 avg_loss = 3.69467\n",
      "epoch no.1 train no.137300  loss = 2.60958 avg_loss = 3.63358\n",
      "epoch no.1 train no.137310  loss = 3.54531 avg_loss = 3.62683\n",
      "epoch no.1 train no.137320  loss = 5.82805 avg_loss = 3.64914\n",
      "epoch no.1 train no.137330  loss = 5.92299 avg_loss = 3.65684\n",
      "epoch no.1 train no.137340  loss = 3.43603 avg_loss = 3.64468\n",
      "epoch no.1 train no.137350  loss = 4.75537 avg_loss = 3.64563\n",
      "epoch no.1 train no.137360  loss = 3.60894 avg_loss = 3.64118\n",
      "epoch no.1 train no.137370  loss = 2.52560 avg_loss = 3.62340\n",
      "epoch no.1 train no.137380  loss = 2.73857 avg_loss = 3.62497\n",
      "epoch no.1 train no.137390  loss = 2.61257 avg_loss = 3.65161\n",
      "epoch no.1 train no.137400  loss = 2.60683 avg_loss = 3.66535\n",
      "epoch no.1 train no.137410  loss = 2.12755 avg_loss = 3.62965\n",
      "epoch no.1 train no.137420  loss = 2.56787 avg_loss = 3.61687\n",
      "epoch no.1 train no.137430  loss = 3.63966 avg_loss = 3.63966\n",
      "epoch no.1 train no.137440  loss = 4.22002 avg_loss = 3.57914\n",
      "epoch no.1 train no.137450  loss = 1.04647 avg_loss = 3.57704\n",
      "epoch no.1 train no.137460  loss = 3.30951 avg_loss = 3.56341\n",
      "epoch no.1 train no.137470  loss = 2.47325 avg_loss = 3.55781\n",
      "epoch no.1 train no.137480  loss = 3.20632 avg_loss = 3.55178\n",
      "epoch no.1 train no.137490  loss = 2.49227 avg_loss = 3.54570\n",
      "epoch no.1 train no.137500  loss = 5.95482 avg_loss = 3.57906\n",
      "epoch no.1 train no.137510  loss = 3.58046 avg_loss = 3.57230\n",
      "epoch no.1 train no.137520  loss = 3.89944 avg_loss = 3.57472\n",
      "epoch no.1 train no.137530  loss = 3.08762 avg_loss = 3.55136\n",
      "epoch no.1 train no.137540  loss = 3.73077 avg_loss = 3.50162\n",
      "epoch no.1 train no.137550  loss = 3.21403 avg_loss = 3.54209\n",
      "epoch no.1 train no.137560  loss = 3.03648 avg_loss = 3.55827\n",
      "epoch no.1 train no.137570  loss = 4.04280 avg_loss = 3.62141\n",
      "epoch no.1 train no.137580  loss = 5.86974 avg_loss = 3.63383\n",
      "epoch no.1 train no.137590  loss = 3.20561 avg_loss = 3.61307\n",
      "epoch no.1 train no.137600  loss = 2.59831 avg_loss = 3.63630\n",
      "epoch no.1 train no.137610  loss = 5.42278 avg_loss = 3.67645\n",
      "epoch no.1 train no.137620  loss = 4.49931 avg_loss = 3.64360\n",
      "epoch no.1 train no.137630  loss = 3.95923 avg_loss = 3.62592\n",
      "epoch no.1 train no.137640  loss = 4.79378 avg_loss = 3.60566\n",
      "epoch no.1 train no.137650  loss = 3.84028 avg_loss = 3.60746\n",
      "epoch no.1 train no.137660  loss = 3.57088 avg_loss = 3.62869\n",
      "epoch no.1 train no.137670  loss = 2.20974 avg_loss = 3.60760\n",
      "epoch no.1 train no.137680  loss = 2.70921 avg_loss = 3.65855\n",
      "epoch no.1 train no.137690  loss = 2.93348 avg_loss = 3.61067\n",
      "epoch no.1 train no.137700  loss = 2.77396 avg_loss = 3.59808\n",
      "epoch no.1 train no.137710  loss = 4.57278 avg_loss = 3.58917\n",
      "epoch no.1 train no.137720  loss = 4.65562 avg_loss = 3.61082\n",
      "epoch no.1 train no.137730  loss = 6.74352 avg_loss = 3.64167\n",
      "epoch no.1 train no.137740  loss = 5.63942 avg_loss = 3.67326\n",
      "epoch no.1 train no.137750  loss = 3.93209 avg_loss = 3.67982\n",
      "epoch no.1 train no.137760  loss = 4.41017 avg_loss = 3.63149\n",
      "epoch no.1 train no.137770  loss = 2.13068 avg_loss = 3.61058\n",
      "epoch no.1 train no.137780  loss = 3.47351 avg_loss = 3.62495\n",
      "epoch no.1 train no.137790  loss = 2.19727 avg_loss = 3.61286\n",
      "epoch no.1 train no.137800  loss = 2.99968 avg_loss = 3.58256\n",
      "epoch no.1 train no.137810  loss = 2.85127 avg_loss = 3.58545\n",
      "epoch no.1 train no.137820  loss = 3.37673 avg_loss = 3.58112\n",
      "epoch no.1 train no.137830  loss = 4.14844 avg_loss = 3.55107\n",
      "epoch no.1 train no.137840  loss = 3.14000 avg_loss = 3.54138\n",
      "epoch no.1 train no.137850  loss = 4.00473 avg_loss = 3.62240\n",
      "epoch no.1 train no.137860  loss = 3.48109 avg_loss = 3.69404\n",
      "epoch no.1 train no.137870  loss = 3.15068 avg_loss = 3.67888\n",
      "epoch no.1 train no.137880  loss = 2.21818 avg_loss = 3.70744\n",
      "epoch no.1 train no.137890  loss = 3.21082 avg_loss = 3.68173\n",
      "epoch no.1 train no.137900  loss = 2.87461 avg_loss = 3.68525\n",
      "epoch no.1 train no.137910  loss = 3.56790 avg_loss = 3.65278\n",
      "epoch no.1 train no.137920  loss = 2.91434 avg_loss = 3.65443\n",
      "epoch no.1 train no.137930  loss = 3.28214 avg_loss = 3.65465\n",
      "epoch no.1 train no.137940  loss = 2.85946 avg_loss = 3.68926\n",
      "epoch no.1 train no.137950  loss = 2.34465 avg_loss = 3.66254\n",
      "epoch no.1 train no.137960  loss = 3.95890 avg_loss = 3.68149\n",
      "epoch no.1 train no.137970  loss = 2.44295 avg_loss = 3.71299\n",
      "epoch no.1 train no.137980  loss = 2.09972 avg_loss = 3.69125\n",
      "epoch no.1 train no.137990  loss = 2.96956 avg_loss = 3.70803\n",
      "epoch no.1 train no.138000  loss = 2.63951 avg_loss = 3.67965\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.1 train no.138010  loss = 5.81490 avg_loss = 3.70068\n",
      "epoch no.1 train no.138020  loss = 2.33724 avg_loss = 3.67992\n",
      "epoch no.1 train no.138030  loss = 1.87943 avg_loss = 3.67358\n",
      "epoch no.1 train no.138040  loss = 3.21046 avg_loss = 3.65561\n",
      "epoch no.1 train no.138050  loss = 1.79048 avg_loss = 3.67315\n",
      "epoch no.1 train no.138060  loss = 2.95399 avg_loss = 3.64121\n",
      "epoch no.1 train no.138070  loss = 3.81963 avg_loss = 3.69601\n",
      "epoch no.1 train no.138080  loss = 3.13992 avg_loss = 3.63789\n",
      "epoch no.1 train no.138090  loss = 5.18523 avg_loss = 3.74092\n",
      "epoch no.1 train no.138100  loss = 5.47129 avg_loss = 3.79564\n",
      "epoch no.1 train no.138110  loss = 2.55816 avg_loss = 3.78643\n",
      "epoch no.1 train no.138120  loss = 2.77637 avg_loss = 3.77393\n",
      "epoch no.1 train no.138130  loss = 3.63855 avg_loss = 3.69370\n",
      "epoch no.1 train no.138140  loss = 3.62998 avg_loss = 3.66839\n",
      "epoch no.1 train no.138150  loss = 3.94480 avg_loss = 3.68591\n",
      "epoch no.1 train no.138160  loss = 3.29965 avg_loss = 3.65531\n",
      "epoch no.1 train no.138170  loss = 5.25733 avg_loss = 3.71268\n",
      "epoch no.1 train no.138180  loss = 2.45161 avg_loss = 3.69527\n",
      "epoch no.1 train no.138190  loss = 3.04406 avg_loss = 3.66531\n",
      "epoch no.1 train no.138200  loss = 1.79232 avg_loss = 3.67262\n",
      "epoch no.1 train no.138210  loss = 7.51236 avg_loss = 3.70063\n",
      "epoch no.1 train no.138220  loss = 2.47295 avg_loss = 3.63333\n",
      "epoch no.1 train no.138230  loss = 3.38177 avg_loss = 3.59484\n",
      "epoch no.1 train no.138240  loss = 3.70104 avg_loss = 3.57937\n",
      "epoch no.1 train no.138250  loss = 2.24229 avg_loss = 3.52897\n",
      "epoch no.1 train no.138260  loss = 1.98803 avg_loss = 3.55547\n",
      "epoch no.1 train no.138270  loss = 4.47879 avg_loss = 3.62362\n",
      "epoch no.1 train no.138280  loss = 3.96030 avg_loss = 3.62786\n",
      "epoch no.1 train no.138290  loss = 4.12669 avg_loss = 3.60987\n",
      "epoch no.1 train no.138300  loss = 3.17626 avg_loss = 3.62928\n",
      "epoch no.1 train no.138310  loss = 4.38038 avg_loss = 3.61018\n",
      "epoch no.1 train no.138320  loss = 2.45599 avg_loss = 3.59379\n",
      "epoch no.1 train no.138330  loss = 1.80676 avg_loss = 3.52837\n",
      "epoch no.1 train no.138340  loss = 6.73068 avg_loss = 3.61854\n",
      "epoch no.1 train no.138350  loss = 4.77905 avg_loss = 3.64298\n",
      "epoch no.1 train no.138360  loss = 2.31313 avg_loss = 3.60591\n",
      "epoch no.1 train no.138370  loss = 2.85591 avg_loss = 3.58004\n",
      "epoch no.1 train no.138380  loss = 4.61752 avg_loss = 3.57144\n",
      "epoch no.1 train no.138390  loss = 6.28266 avg_loss = 3.56475\n",
      "epoch no.1 train no.138400  loss = 4.11332 avg_loss = 3.59535\n",
      "epoch no.1 train no.138410  loss = 4.16235 avg_loss = 3.55751\n",
      "epoch no.1 train no.138420  loss = 5.12090 avg_loss = 3.61668\n",
      "epoch no.1 train no.138430  loss = 4.63222 avg_loss = 3.59730\n",
      "epoch no.1 train no.138440  loss = 2.53324 avg_loss = 3.59982\n",
      "epoch no.1 train no.138450  loss = 3.97409 avg_loss = 3.64658\n",
      "epoch no.1 train no.138460  loss = 3.50403 avg_loss = 3.65037\n",
      "epoch no.1 train no.138470  loss = 3.74000 avg_loss = 3.63030\n",
      "epoch no.1 train no.138480  loss = 2.86724 avg_loss = 3.64482\n",
      "epoch no.1 train no.138490  loss = 3.25603 avg_loss = 3.60333\n",
      "epoch no.1 train no.138500  loss = 4.26231 avg_loss = 3.64507\n",
      "epoch no.1 train no.138510  loss = 3.79251 avg_loss = 3.65792\n",
      "epoch no.1 train no.138520  loss = 4.02081 avg_loss = 3.64733\n",
      "epoch no.1 train no.138530  loss = 1.85356 avg_loss = 3.59758\n",
      "epoch no.1 train no.138540  loss = 1.84371 avg_loss = 3.64134\n",
      "epoch no.1 train no.138550  loss = 4.32752 avg_loss = 3.66318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.138560  loss = 3.52645 avg_loss = 3.66076\n",
      "epoch no.1 train no.138570  loss = 5.00703 avg_loss = 3.74582\n",
      "epoch no.1 train no.138580  loss = 3.84089 avg_loss = 3.75179\n",
      "epoch no.1 train no.138590  loss = 3.82555 avg_loss = 3.74703\n",
      "epoch no.1 train no.138600  loss = 2.31515 avg_loss = 3.72538\n",
      "epoch no.1 train no.138610  loss = 3.11314 avg_loss = 3.71960\n",
      "epoch no.1 train no.138620  loss = 1.92696 avg_loss = 3.71134\n",
      "epoch no.1 train no.138630  loss = 3.40165 avg_loss = 3.65680\n",
      "epoch no.1 train no.138640  loss = 2.60274 avg_loss = 3.59462\n",
      "epoch no.1 train no.138650  loss = 5.09239 avg_loss = 3.58760\n",
      "epoch no.1 train no.138660  loss = 3.50511 avg_loss = 3.63296\n",
      "epoch no.1 train no.138670  loss = 3.59592 avg_loss = 3.60844\n",
      "epoch no.1 train no.138680  loss = 4.58781 avg_loss = 3.58955\n",
      "epoch no.1 train no.138690  loss = 3.56158 avg_loss = 3.62524\n",
      "epoch no.1 train no.138700  loss = 5.53136 avg_loss = 3.63479\n",
      "epoch no.1 train no.138710  loss = 2.32077 avg_loss = 3.59529\n",
      "epoch no.1 train no.138720  loss = 1.68777 avg_loss = 3.58785\n",
      "epoch no.1 train no.138730  loss = 1.95194 avg_loss = 3.57288\n",
      "epoch no.1 train no.138740  loss = 3.58582 avg_loss = 3.51287\n",
      "epoch no.1 train no.138750  loss = 3.93325 avg_loss = 3.55340\n",
      "epoch no.1 train no.138760  loss = 4.74147 avg_loss = 3.61723\n",
      "epoch no.1 train no.138770  loss = 2.54807 avg_loss = 3.56567\n",
      "epoch no.1 train no.138780  loss = 3.87440 avg_loss = 3.58397\n",
      "epoch no.1 train no.138790  loss = 3.42863 avg_loss = 3.55228\n",
      "epoch no.1 train no.138800  loss = 3.52067 avg_loss = 3.59140\n",
      "epoch no.1 train no.138810  loss = 3.63257 avg_loss = 3.59152\n",
      "epoch no.1 train no.138820  loss = 1.74250 avg_loss = 3.51889\n",
      "epoch no.1 train no.138830  loss = 3.95913 avg_loss = 3.56657\n",
      "epoch no.1 train no.138840  loss = 4.64896 avg_loss = 3.59430\n",
      "epoch no.1 train no.138850  loss = 3.53908 avg_loss = 3.53134\n",
      "epoch no.1 train no.138860  loss = 4.39871 avg_loss = 3.55065\n",
      "epoch no.1 train no.138870  loss = 2.50897 avg_loss = 3.57300\n",
      "epoch no.1 train no.138880  loss = 4.00388 avg_loss = 3.62925\n",
      "epoch no.1 train no.138890  loss = 5.50878 avg_loss = 3.63767\n",
      "epoch no.1 train no.138900  loss = 4.04271 avg_loss = 3.59773\n",
      "epoch no.1 train no.138910  loss = 3.68988 avg_loss = 3.58556\n",
      "epoch no.1 train no.138920  loss = 2.52984 avg_loss = 3.61331\n",
      "epoch no.1 train no.138930  loss = 2.97370 avg_loss = 3.62196\n",
      "epoch no.1 train no.138940  loss = 2.16778 avg_loss = 3.61025\n",
      "epoch no.1 train no.138950  loss = 3.19973 avg_loss = 3.63798\n",
      "epoch no.1 train no.138960  loss = 4.74595 avg_loss = 3.64256\n",
      "epoch no.1 train no.138970  loss = 3.03546 avg_loss = 3.64452\n",
      "epoch no.1 train no.138980  loss = 5.94251 avg_loss = 3.69464\n",
      "epoch no.1 train no.138990  loss = 2.66128 avg_loss = 3.70811\n",
      "epoch no.1 train no.139000  loss = 2.80504 avg_loss = 3.71621\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '송', '들', '집', '</s>']\n",
      "추억의 팝송 모음집</s>\n",
      "epoch no.1 train no.139010  loss = 2.83795 avg_loss = 3.77565\n",
      "epoch no.1 train no.139020  loss = 4.15821 avg_loss = 3.74054\n",
      "epoch no.1 train no.139030  loss = 2.59121 avg_loss = 3.69256\n",
      "epoch no.1 train no.139040  loss = 3.45952 avg_loss = 3.67252\n",
      "epoch no.1 train no.139050  loss = 2.51907 avg_loss = 3.67799\n",
      "epoch no.1 train no.139060  loss = 6.86247 avg_loss = 3.77370\n",
      "epoch no.1 train no.139070  loss = 3.66301 avg_loss = 3.85439\n",
      "epoch no.1 train no.139080  loss = 2.15562 avg_loss = 3.78695\n",
      "epoch no.1 train no.139090  loss = 3.94016 avg_loss = 3.72743\n",
      "epoch no.1 train no.139100  loss = 1.52088 avg_loss = 3.70934\n",
      "epoch no.1 train no.139110  loss = 3.77639 avg_loss = 3.71471\n",
      "epoch no.1 train no.139120  loss = 2.42691 avg_loss = 3.69251\n",
      "epoch no.1 train no.139130  loss = 2.17769 avg_loss = 3.68070\n",
      "epoch no.1 train no.139140  loss = 5.13049 avg_loss = 3.81823\n",
      "epoch no.1 train no.139150  loss = 3.55199 avg_loss = 3.81738\n",
      "epoch no.1 train no.139160  loss = 3.62345 avg_loss = 3.83150\n",
      "epoch no.1 train no.139170  loss = 3.87145 avg_loss = 3.86971\n",
      "epoch no.1 train no.139180  loss = 3.30992 avg_loss = 3.85485\n",
      "epoch no.1 train no.139190  loss = 2.68291 avg_loss = 3.83929\n",
      "epoch no.1 train no.139200  loss = 3.36864 avg_loss = 3.82338\n",
      "epoch no.1 train no.139210  loss = 3.16537 avg_loss = 3.80573\n",
      "epoch no.1 train no.139220  loss = 3.17432 avg_loss = 3.76825\n",
      "epoch no.1 train no.139230  loss = 4.53475 avg_loss = 3.75226\n",
      "epoch no.1 train no.139240  loss = 2.77986 avg_loss = 3.71982\n",
      "epoch no.1 train no.139250  loss = 5.75745 avg_loss = 3.72514\n",
      "epoch no.1 train no.139260  loss = 3.13874 avg_loss = 3.68112\n",
      "epoch no.1 train no.139270  loss = 4.04670 avg_loss = 3.68022\n",
      "epoch no.1 train no.139280  loss = 2.51010 avg_loss = 3.67870\n",
      "epoch no.1 train no.139290  loss = 3.91726 avg_loss = 3.71824\n",
      "epoch no.1 train no.139300  loss = 3.55880 avg_loss = 3.69915\n",
      "epoch no.1 train no.139310  loss = 3.77832 avg_loss = 3.69835\n",
      "epoch no.1 train no.139320  loss = 4.83965 avg_loss = 3.68503\n",
      "epoch no.1 train no.139330  loss = 3.31573 avg_loss = 3.66514\n",
      "epoch no.1 train no.139340  loss = 3.23288 avg_loss = 3.64876\n",
      "epoch no.1 train no.139350  loss = 2.61475 avg_loss = 3.61647\n",
      "epoch no.1 train no.139360  loss = 4.11181 avg_loss = 3.65323\n",
      "epoch no.1 train no.139370  loss = 4.74354 avg_loss = 3.61901\n",
      "epoch no.1 train no.139380  loss = 2.33225 avg_loss = 3.57678\n",
      "epoch no.1 train no.139390  loss = 3.28539 avg_loss = 3.61654\n",
      "epoch no.1 train no.139400  loss = 2.51296 avg_loss = 3.59040\n",
      "epoch no.1 train no.139410  loss = 3.75469 avg_loss = 3.59270\n",
      "epoch no.1 train no.139420  loss = 3.56823 avg_loss = 3.52872\n",
      "epoch no.1 train no.139430  loss = 2.82418 avg_loss = 3.58793\n",
      "epoch no.1 train no.139440  loss = 2.12442 avg_loss = 3.63455\n",
      "epoch no.1 train no.139450  loss = 2.11110 avg_loss = 3.63533\n",
      "epoch no.1 train no.139460  loss = 5.64727 avg_loss = 3.67028\n",
      "epoch no.1 train no.139470  loss = 3.92870 avg_loss = 3.70087\n",
      "epoch no.1 train no.139480  loss = 4.66183 avg_loss = 3.73331\n",
      "epoch no.1 train no.139490  loss = 4.57113 avg_loss = 3.70979\n",
      "epoch no.1 train no.139500  loss = 5.93777 avg_loss = 3.72653\n",
      "epoch no.1 train no.139510  loss = 3.43993 avg_loss = 3.77083\n",
      "epoch no.1 train no.139520  loss = 3.13603 avg_loss = 3.76231\n",
      "epoch no.1 train no.139530  loss = 3.43838 avg_loss = 3.74179\n",
      "epoch no.1 train no.139540  loss = 3.43740 avg_loss = 3.70788\n",
      "epoch no.1 train no.139550  loss = 3.76946 avg_loss = 3.74324\n",
      "epoch no.1 train no.139560  loss = 3.83641 avg_loss = 3.71535\n",
      "epoch no.1 train no.139570  loss = 2.40379 avg_loss = 3.71452\n",
      "epoch no.1 train no.139580  loss = 3.70702 avg_loss = 3.74382\n",
      "epoch no.1 train no.139590  loss = 4.24118 avg_loss = 3.76203\n",
      "epoch no.1 train no.139600  loss = 4.95501 avg_loss = 3.71532\n",
      "epoch no.1 train no.139610  loss = 2.02676 avg_loss = 3.71075\n",
      "epoch no.1 train no.139620  loss = 4.63848 avg_loss = 3.72238\n",
      "epoch no.1 train no.139630  loss = 3.62456 avg_loss = 3.74724\n",
      "epoch no.1 train no.139640  loss = 2.83946 avg_loss = 3.71278\n",
      "epoch no.1 train no.139650  loss = 3.69271 avg_loss = 3.72038\n",
      "epoch no.1 train no.139660  loss = 3.73702 avg_loss = 3.64991\n",
      "epoch no.1 train no.139670  loss = 4.74337 avg_loss = 3.64570\n",
      "epoch no.1 train no.139680  loss = 2.65791 avg_loss = 3.64162\n",
      "epoch no.1 train no.139690  loss = 3.52085 avg_loss = 3.66649\n",
      "epoch no.1 train no.139700  loss = 4.90550 avg_loss = 3.64365\n",
      "epoch no.1 train no.139710  loss = 3.82800 avg_loss = 3.64381\n",
      "epoch no.1 train no.139720  loss = 4.44248 avg_loss = 3.70964\n",
      "epoch no.1 train no.139730  loss = 5.95111 avg_loss = 3.73994\n",
      "epoch no.1 train no.139740  loss = 3.80269 avg_loss = 3.72942\n",
      "epoch no.1 train no.139750  loss = 5.24725 avg_loss = 3.67820\n",
      "epoch no.1 train no.139760  loss = 2.21565 avg_loss = 3.65817\n",
      "epoch no.1 train no.139770  loss = 5.67093 avg_loss = 3.70714\n",
      "epoch no.1 train no.139780  loss = 2.93102 avg_loss = 3.69354\n",
      "epoch no.1 train no.139790  loss = 4.83689 avg_loss = 3.69517\n",
      "epoch no.1 train no.139800  loss = 2.22895 avg_loss = 3.68234\n",
      "epoch no.1 train no.139810  loss = 2.67783 avg_loss = 3.74691\n",
      "epoch no.1 train no.139820  loss = 3.34375 avg_loss = 3.81468\n",
      "epoch no.1 train no.139830  loss = 4.15697 avg_loss = 3.79802\n",
      "epoch no.1 train no.139840  loss = 4.91495 avg_loss = 3.73922\n",
      "epoch no.1 train no.139850  loss = 2.68527 avg_loss = 3.71522\n",
      "epoch no.1 train no.139860  loss = 3.69053 avg_loss = 3.67486\n",
      "epoch no.1 train no.139870  loss = 5.88043 avg_loss = 3.72455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.139880  loss = 3.85755 avg_loss = 3.77560\n",
      "epoch no.1 train no.139890  loss = 3.72703 avg_loss = 3.74050\n",
      "epoch no.1 train no.139900  loss = 5.54342 avg_loss = 3.75531\n",
      "epoch no.1 train no.139910  loss = 4.34458 avg_loss = 3.76292\n",
      "epoch no.1 train no.139920  loss = 3.82274 avg_loss = 3.73894\n",
      "epoch no.1 train no.139930  loss = 5.30489 avg_loss = 3.72269\n",
      "epoch no.1 train no.139940  loss = 2.68158 avg_loss = 3.73900\n",
      "epoch no.1 train no.139950  loss = 3.99610 avg_loss = 3.69913\n",
      "epoch no.1 train no.139960  loss = 2.30635 avg_loss = 3.71867\n",
      "epoch no.1 train no.139970  loss = 3.89060 avg_loss = 3.71564\n",
      "epoch no.1 train no.139980  loss = 4.30480 avg_loss = 3.74361\n",
      "epoch no.1 train no.139990  loss = 2.97423 avg_loss = 3.69555\n",
      "epoch no.1 train no.140000  loss = 4.51727 avg_loss = 3.69170\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.1 train no.140010  loss = 3.65846 avg_loss = 3.63797\n",
      "epoch no.1 train no.140020  loss = 3.70327 avg_loss = 3.64989\n",
      "epoch no.1 train no.140030  loss = 3.90985 avg_loss = 3.66458\n",
      "epoch no.1 train no.140040  loss = 3.43323 avg_loss = 3.64923\n",
      "epoch no.1 train no.140050  loss = 3.76110 avg_loss = 3.68497\n",
      "epoch no.1 train no.140060  loss = 4.36764 avg_loss = 3.68397\n",
      "epoch no.1 train no.140070  loss = 4.69734 avg_loss = 3.69557\n",
      "epoch no.1 train no.140080  loss = 1.98818 avg_loss = 3.63718\n",
      "epoch no.1 train no.140090  loss = 2.62175 avg_loss = 3.66900\n",
      "epoch no.1 train no.140100  loss = 2.43312 avg_loss = 3.64129\n",
      "epoch no.1 train no.140110  loss = 5.77349 avg_loss = 3.62197\n",
      "epoch no.1 train no.140120  loss = 3.69881 avg_loss = 3.61587\n",
      "epoch no.1 train no.140130  loss = 4.64393 avg_loss = 3.63231\n",
      "epoch no.1 train no.140140  loss = 3.70122 avg_loss = 3.60489\n",
      "epoch no.1 train no.140150  loss = 5.04746 avg_loss = 3.62851\n",
      "epoch no.1 train no.140160  loss = 4.43351 avg_loss = 3.58461\n",
      "epoch no.1 train no.140170  loss = 3.50147 avg_loss = 3.58207\n",
      "epoch no.1 train no.140180  loss = 4.23713 avg_loss = 3.60948\n",
      "epoch no.1 train no.140190  loss = 3.44948 avg_loss = 3.56675\n",
      "epoch no.1 train no.140200  loss = 3.39770 avg_loss = 3.59395\n",
      "epoch no.1 train no.140210  loss = 3.15949 avg_loss = 3.64346\n",
      "epoch no.1 train no.140220  loss = 3.62466 avg_loss = 3.62672\n",
      "epoch no.1 train no.140230  loss = 3.47456 avg_loss = 3.65962\n",
      "epoch no.1 train no.140240  loss = 2.01137 avg_loss = 3.66179\n",
      "epoch no.1 train no.140250  loss = 4.79917 avg_loss = 3.68254\n",
      "epoch no.1 train no.140260  loss = 3.30389 avg_loss = 3.67061\n",
      "epoch no.1 train no.140270  loss = 3.49623 avg_loss = 3.64895\n",
      "epoch no.1 train no.140280  loss = 3.15487 avg_loss = 3.66780\n",
      "epoch no.1 train no.140290  loss = 2.73641 avg_loss = 3.60522\n",
      "epoch no.1 train no.140300  loss = 2.34010 avg_loss = 3.57139\n",
      "epoch no.1 train no.140310  loss = 5.00222 avg_loss = 3.60992\n",
      "epoch no.1 train no.140320  loss = 3.59635 avg_loss = 3.58787\n",
      "epoch no.1 train no.140330  loss = 2.92555 avg_loss = 3.59201\n",
      "epoch no.1 train no.140340  loss = 1.98117 avg_loss = 3.60227\n",
      "epoch no.1 train no.140350  loss = 3.80355 avg_loss = 3.56297\n",
      "epoch no.1 train no.140360  loss = 2.30123 avg_loss = 3.55778\n",
      "epoch no.1 train no.140370  loss = 3.91835 avg_loss = 3.58697\n",
      "epoch no.1 train no.140380  loss = 3.08945 avg_loss = 3.59687\n",
      "epoch no.1 train no.140390  loss = 3.59721 avg_loss = 3.59087\n",
      "epoch no.1 train no.140400  loss = 2.92706 avg_loss = 3.58719\n",
      "epoch no.1 train no.140410  loss = 5.18804 avg_loss = 3.58349\n",
      "epoch no.1 train no.140420  loss = 4.56863 avg_loss = 3.62672\n",
      "epoch no.1 train no.140430  loss = 3.32808 avg_loss = 3.63106\n",
      "epoch no.1 train no.140440  loss = 4.41774 avg_loss = 3.63971\n",
      "epoch no.1 train no.140450  loss = 3.37073 avg_loss = 3.65395\n",
      "epoch no.1 train no.140460  loss = 3.47741 avg_loss = 3.64768\n",
      "epoch no.1 train no.140470  loss = 2.69719 avg_loss = 3.68499\n",
      "epoch no.1 train no.140480  loss = 3.66831 avg_loss = 3.67629\n",
      "epoch no.1 train no.140490  loss = 4.24401 avg_loss = 3.59308\n",
      "epoch no.1 train no.140500  loss = 4.23961 avg_loss = 3.59842\n",
      "epoch no.1 train no.140510  loss = 3.36063 avg_loss = 3.56755\n",
      "epoch no.1 train no.140520  loss = 2.20010 avg_loss = 3.58997\n",
      "epoch no.1 train no.140530  loss = 2.99202 avg_loss = 3.54972\n",
      "epoch no.1 train no.140540  loss = 3.17197 avg_loss = 3.57103\n",
      "epoch no.1 train no.140550  loss = 3.64122 avg_loss = 3.51593\n",
      "epoch no.1 train no.140560  loss = 4.42724 avg_loss = 3.60620\n",
      "epoch no.1 train no.140570  loss = 4.23823 avg_loss = 3.59461\n",
      "epoch no.1 train no.140580  loss = 4.33810 avg_loss = 3.56762\n",
      "epoch no.1 train no.140590  loss = 3.22748 avg_loss = 3.60797\n",
      "epoch no.1 train no.140600  loss = 2.94377 avg_loss = 3.64235\n",
      "epoch no.1 train no.140610  loss = 2.47056 avg_loss = 3.63173\n",
      "epoch no.1 train no.140620  loss = 2.24438 avg_loss = 3.62667\n",
      "epoch no.1 train no.140630  loss = 3.65411 avg_loss = 3.66345\n",
      "epoch no.1 train no.140640  loss = 6.28618 avg_loss = 3.65480\n",
      "epoch no.1 train no.140650  loss = 1.99759 avg_loss = 3.60527\n",
      "epoch no.1 train no.140660  loss = 4.99808 avg_loss = 3.65166\n",
      "epoch no.1 train no.140670  loss = 5.25356 avg_loss = 3.60191\n",
      "epoch no.1 train no.140680  loss = 3.69961 avg_loss = 3.65718\n",
      "epoch no.1 train no.140690  loss = 4.82476 avg_loss = 3.67873\n",
      "epoch no.1 train no.140700  loss = 2.65842 avg_loss = 3.62374\n",
      "epoch no.1 train no.140710  loss = 3.54126 avg_loss = 3.61200\n",
      "epoch no.1 train no.140720  loss = 5.77104 avg_loss = 3.63780\n",
      "epoch no.1 train no.140730  loss = 1.97737 avg_loss = 3.59996\n",
      "epoch no.1 train no.140740  loss = 4.44761 avg_loss = 3.59708\n",
      "epoch no.1 train no.140750  loss = 3.56765 avg_loss = 3.57557\n",
      "epoch no.1 train no.140760  loss = 5.28282 avg_loss = 3.65325\n",
      "epoch no.1 train no.140770  loss = 3.12307 avg_loss = 3.59467\n",
      "epoch no.1 train no.140780  loss = 4.61365 avg_loss = 3.57968\n",
      "epoch no.1 train no.140790  loss = 3.85056 avg_loss = 3.63087\n",
      "epoch no.1 train no.140800  loss = 2.88860 avg_loss = 3.56930\n",
      "epoch no.1 train no.140810  loss = 2.82173 avg_loss = 3.61791\n",
      "epoch no.1 train no.140820  loss = 2.78157 avg_loss = 3.67316\n",
      "epoch no.1 train no.140830  loss = 3.22812 avg_loss = 3.61415\n",
      "epoch no.1 train no.140840  loss = 3.88899 avg_loss = 3.55866\n",
      "epoch no.1 train no.140850  loss = 4.59088 avg_loss = 3.64342\n",
      "epoch no.1 train no.140860  loss = 6.18048 avg_loss = 3.67056\n",
      "epoch no.1 train no.140870  loss = 4.30283 avg_loss = 3.65096\n",
      "epoch no.1 train no.140880  loss = 5.84107 avg_loss = 3.69381\n",
      "epoch no.1 train no.140890  loss = 1.97676 avg_loss = 3.65044\n",
      "epoch no.1 train no.140900  loss = 1.92684 avg_loss = 3.63958\n",
      "epoch no.1 train no.140910  loss = 4.40587 avg_loss = 3.67461\n",
      "epoch no.1 train no.140920  loss = 5.79833 avg_loss = 3.69704\n",
      "epoch no.1 train no.140930  loss = 4.93420 avg_loss = 3.72087\n",
      "epoch no.1 train no.140940  loss = 2.55933 avg_loss = 3.69559\n",
      "epoch no.1 train no.140950  loss = 5.75303 avg_loss = 3.69840\n",
      "epoch no.1 train no.140960  loss = 4.15330 avg_loss = 3.71846\n",
      "epoch no.1 train no.140970  loss = 6.27707 avg_loss = 3.74776\n",
      "epoch no.1 train no.140980  loss = 4.25704 avg_loss = 3.74170\n",
      "epoch no.1 train no.140990  loss = 2.99224 avg_loss = 3.68966\n",
      "epoch no.1 train no.141000  loss = 3.50811 avg_loss = 3.66257\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '</s>']\n",
      "추억의 90년대 가요</s>\n",
      "epoch no.1 train no.141010  loss = 3.72227 avg_loss = 3.64620\n",
      "epoch no.1 train no.141020  loss = 2.47236 avg_loss = 3.63517\n",
      "epoch no.1 train no.141030  loss = 4.53935 avg_loss = 3.59250\n",
      "epoch no.1 train no.141040  loss = 2.44433 avg_loss = 3.60373\n",
      "epoch no.1 train no.141050  loss = 2.77786 avg_loss = 3.62164\n",
      "epoch no.1 train no.141060  loss = 2.44668 avg_loss = 3.58894\n",
      "epoch no.1 train no.141070  loss = 4.00934 avg_loss = 3.60422\n",
      "epoch no.1 train no.141080  loss = 2.50186 avg_loss = 3.56236\n",
      "epoch no.1 train no.141090  loss = 3.90881 avg_loss = 3.54671\n",
      "epoch no.1 train no.141100  loss = 5.42613 avg_loss = 3.61249\n",
      "epoch no.1 train no.141110  loss = 3.72026 avg_loss = 3.66462\n",
      "epoch no.1 train no.141120  loss = 3.73517 avg_loss = 3.70716\n",
      "epoch no.1 train no.141130  loss = 2.52744 avg_loss = 3.66392\n",
      "epoch no.1 train no.141140  loss = 5.22409 avg_loss = 3.69316\n",
      "epoch no.1 train no.141150  loss = 4.30422 avg_loss = 3.65412\n",
      "epoch no.1 train no.141160  loss = 3.19243 avg_loss = 3.62496\n",
      "epoch no.1 train no.141170  loss = 1.92485 avg_loss = 3.67382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.141180  loss = 4.96929 avg_loss = 3.66704\n",
      "epoch no.1 train no.141190  loss = 3.68141 avg_loss = 3.64537\n",
      "epoch no.1 train no.141200  loss = 3.93875 avg_loss = 3.67991\n",
      "epoch no.1 train no.141210  loss = 5.57557 avg_loss = 3.66281\n",
      "epoch no.1 train no.141220  loss = 5.30582 avg_loss = 3.64977\n",
      "epoch no.1 train no.141230  loss = 5.29116 avg_loss = 3.63007\n",
      "epoch no.1 train no.141240  loss = 2.43978 avg_loss = 3.63403\n",
      "epoch no.1 train no.141250  loss = 2.78176 avg_loss = 3.61689\n",
      "epoch no.1 train no.141260  loss = 3.33786 avg_loss = 3.54862\n",
      "epoch no.1 train no.141270  loss = 5.68916 avg_loss = 3.63234\n",
      "epoch no.1 train no.141280  loss = 3.06096 avg_loss = 3.60735\n",
      "epoch no.1 train no.141290  loss = 3.33009 avg_loss = 3.54929\n",
      "epoch no.1 train no.141300  loss = 2.91093 avg_loss = 3.54670\n",
      "epoch no.1 train no.141310  loss = 1.77252 avg_loss = 3.58015\n",
      "epoch no.1 train no.141320  loss = 4.21405 avg_loss = 3.55799\n",
      "epoch no.1 train no.141330  loss = 1.93013 avg_loss = 3.53145\n",
      "epoch no.1 train no.141340  loss = 3.87062 avg_loss = 3.51451\n",
      "epoch no.1 train no.141350  loss = 5.51089 avg_loss = 3.54328\n",
      "epoch no.1 train no.141360  loss = 5.01920 avg_loss = 3.55674\n",
      "epoch no.1 train no.141370  loss = 3.58408 avg_loss = 3.57221\n",
      "epoch no.1 train no.141380  loss = 1.90074 avg_loss = 3.57378\n",
      "epoch no.1 train no.141390  loss = 3.70200 avg_loss = 3.57350\n",
      "epoch no.1 train no.141400  loss = 3.01998 avg_loss = 3.61560\n",
      "epoch no.1 train no.141410  loss = 3.97158 avg_loss = 3.63591\n",
      "epoch no.1 train no.141420  loss = 4.48303 avg_loss = 3.65132\n",
      "epoch no.1 train no.141430  loss = 4.93647 avg_loss = 3.63820\n",
      "epoch no.1 train no.141440  loss = 2.11167 avg_loss = 3.54381\n",
      "epoch no.1 train no.141450  loss = 3.40200 avg_loss = 3.53853\n",
      "epoch no.1 train no.141460  loss = 3.73318 avg_loss = 3.62279\n",
      "epoch no.1 train no.141470  loss = 3.77971 avg_loss = 3.63121\n",
      "epoch no.1 train no.141480  loss = 4.04919 avg_loss = 3.62790\n",
      "epoch no.1 train no.141490  loss = 3.15558 avg_loss = 3.60262\n",
      "epoch no.1 train no.141500  loss = 4.67384 avg_loss = 3.61972\n",
      "epoch no.1 train no.141510  loss = 3.83103 avg_loss = 3.58538\n",
      "epoch no.1 train no.141520  loss = 5.03680 avg_loss = 3.62644\n",
      "epoch no.1 train no.141530  loss = 4.19751 avg_loss = 3.62401\n",
      "epoch no.1 train no.141540  loss = 3.89427 avg_loss = 3.56544\n",
      "epoch no.1 train no.141550  loss = 3.89628 avg_loss = 3.59480\n",
      "epoch no.1 train no.141560  loss = 3.88492 avg_loss = 3.58294\n",
      "epoch no.1 train no.141570  loss = 2.91401 avg_loss = 3.61118\n",
      "epoch no.1 train no.141580  loss = 3.79780 avg_loss = 3.57555\n",
      "epoch no.1 train no.141590  loss = 2.75983 avg_loss = 3.55979\n",
      "epoch no.1 train no.141600  loss = 3.37748 avg_loss = 3.58662\n",
      "epoch no.1 train no.141610  loss = 2.50372 avg_loss = 3.50573\n",
      "epoch no.1 train no.141620  loss = 4.32548 avg_loss = 3.52763\n",
      "epoch no.1 train no.141630  loss = 3.22304 avg_loss = 3.54021\n",
      "epoch no.1 train no.141640  loss = 2.86425 avg_loss = 3.56848\n",
      "epoch no.1 train no.141650  loss = 2.81181 avg_loss = 3.53127\n",
      "epoch no.1 train no.141660  loss = 3.31457 avg_loss = 3.48339\n",
      "epoch no.1 train no.141670  loss = 2.48907 avg_loss = 3.42719\n",
      "epoch no.1 train no.141680  loss = 3.10252 avg_loss = 3.42483\n",
      "epoch no.1 train no.141690  loss = 4.87197 avg_loss = 3.44528\n",
      "epoch no.1 train no.141700  loss = 2.20031 avg_loss = 3.53518\n",
      "epoch no.1 train no.141710  loss = 2.66620 avg_loss = 3.51502\n",
      "epoch no.1 train no.141720  loss = 2.62654 avg_loss = 3.47529\n",
      "epoch no.1 train no.141730  loss = 3.82683 avg_loss = 3.43846\n",
      "epoch no.1 train no.141740  loss = 3.16985 avg_loss = 3.44199\n",
      "epoch no.1 train no.141750  loss = 3.03307 avg_loss = 3.47485\n",
      "epoch no.1 train no.141760  loss = 4.12229 avg_loss = 3.48337\n",
      "epoch no.1 train no.141770  loss = 3.03171 avg_loss = 3.45393\n",
      "epoch no.1 train no.141780  loss = 4.27216 avg_loss = 3.49235\n",
      "epoch no.1 train no.141790  loss = 3.18579 avg_loss = 3.47143\n",
      "epoch no.1 train no.141800  loss = 2.46049 avg_loss = 3.46338\n",
      "epoch no.1 train no.141810  loss = 4.64489 avg_loss = 3.49678\n",
      "epoch no.1 train no.141820  loss = 4.15058 avg_loss = 3.53417\n",
      "epoch no.1 train no.141830  loss = 4.75031 avg_loss = 3.55847\n",
      "epoch no.1 train no.141840  loss = 3.07979 avg_loss = 3.57368\n",
      "epoch no.1 train no.141850  loss = 4.05682 avg_loss = 3.58141\n",
      "epoch no.1 train no.141860  loss = 2.65127 avg_loss = 3.62314\n",
      "epoch no.1 train no.141870  loss = 5.49850 avg_loss = 3.66435\n",
      "epoch no.1 train no.141880  loss = 1.82428 avg_loss = 3.64454\n",
      "epoch no.1 train no.141890  loss = 3.49691 avg_loss = 3.66452\n",
      "epoch no.1 train no.141900  loss = 4.23087 avg_loss = 3.74185\n",
      "epoch no.1 train no.141910  loss = 5.21164 avg_loss = 3.76871\n",
      "epoch no.1 train no.141920  loss = 3.24514 avg_loss = 3.75554\n",
      "epoch no.1 train no.141930  loss = 3.49793 avg_loss = 3.70686\n",
      "epoch no.1 train no.141940  loss = 4.75050 avg_loss = 3.73010\n",
      "epoch no.1 train no.141950  loss = 3.93347 avg_loss = 3.72001\n",
      "epoch no.1 train no.141960  loss = 6.07382 avg_loss = 3.77137\n",
      "epoch no.1 train no.141970  loss = 3.52089 avg_loss = 3.75374\n",
      "epoch no.1 train no.141980  loss = 4.20890 avg_loss = 3.76361\n",
      "epoch no.1 train no.141990  loss = 2.99157 avg_loss = 3.74203\n",
      "epoch no.1 train no.142000  loss = 2.75888 avg_loss = 3.75150\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁모음']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.1 train no.142010  loss = 4.15946 avg_loss = 3.72234\n",
      "epoch no.1 train no.142020  loss = 3.80015 avg_loss = 3.71991\n",
      "epoch no.1 train no.142030  loss = 4.06904 avg_loss = 3.80580\n",
      "epoch no.1 train no.142040  loss = 3.42046 avg_loss = 3.83118\n",
      "epoch no.1 train no.142050  loss = 2.74504 avg_loss = 3.78570\n",
      "epoch no.1 train no.142060  loss = 2.66811 avg_loss = 3.79957\n",
      "epoch no.1 train no.142070  loss = 2.74697 avg_loss = 3.79065\n",
      "epoch no.1 train no.142080  loss = 2.05394 avg_loss = 3.77425\n",
      "epoch no.1 train no.142090  loss = 4.79537 avg_loss = 3.75873\n",
      "epoch no.1 train no.142100  loss = 1.95765 avg_loss = 3.82136\n",
      "epoch no.1 train no.142110  loss = 3.17046 avg_loss = 3.81146\n",
      "epoch no.1 train no.142120  loss = 3.31396 avg_loss = 3.82701\n",
      "epoch no.1 train no.142130  loss = 2.78897 avg_loss = 3.83417\n",
      "epoch no.1 train no.142140  loss = 3.51890 avg_loss = 3.76081\n",
      "epoch no.1 train no.142150  loss = 3.07999 avg_loss = 3.72760\n",
      "epoch no.1 train no.142160  loss = 3.36927 avg_loss = 3.76564\n",
      "epoch no.1 train no.142170  loss = 3.49648 avg_loss = 3.72204\n",
      "epoch no.1 train no.142180  loss = 3.29795 avg_loss = 3.70212\n",
      "epoch no.1 train no.142190  loss = 3.13422 avg_loss = 3.74177\n",
      "epoch no.1 train no.142200  loss = 6.13199 avg_loss = 3.73351\n",
      "epoch no.1 train no.142210  loss = 2.38524 avg_loss = 3.69631\n",
      "epoch no.1 train no.142220  loss = 2.34386 avg_loss = 3.64150\n",
      "epoch no.1 train no.142230  loss = 2.92510 avg_loss = 3.63254\n",
      "epoch no.1 train no.142240  loss = 3.58499 avg_loss = 3.61744\n",
      "epoch no.1 train no.142250  loss = 3.15322 avg_loss = 3.64471\n",
      "epoch no.1 train no.142260  loss = 2.31049 avg_loss = 3.64589\n",
      "epoch no.1 train no.142270  loss = 3.72922 avg_loss = 3.66373\n",
      "epoch no.1 train no.142280  loss = 2.32319 avg_loss = 3.64771\n",
      "epoch no.1 train no.142290  loss = 3.22831 avg_loss = 3.64836\n",
      "epoch no.1 train no.142300  loss = 4.62283 avg_loss = 3.67621\n",
      "epoch no.1 train no.142310  loss = 5.08201 avg_loss = 3.71536\n",
      "epoch no.1 train no.142320  loss = 3.03661 avg_loss = 3.72551\n",
      "epoch no.1 train no.142330  loss = 2.28622 avg_loss = 3.70914\n",
      "epoch no.1 train no.142340  loss = 5.22476 avg_loss = 3.73333\n",
      "epoch no.1 train no.142350  loss = 3.08586 avg_loss = 3.70050\n",
      "epoch no.1 train no.142360  loss = 2.84076 avg_loss = 3.68276\n",
      "epoch no.1 train no.142370  loss = 5.79502 avg_loss = 3.68463\n",
      "epoch no.1 train no.142380  loss = 4.68448 avg_loss = 3.67529\n",
      "epoch no.1 train no.142390  loss = 6.12781 avg_loss = 3.66571\n",
      "epoch no.1 train no.142400  loss = 6.05963 avg_loss = 3.69889\n",
      "epoch no.1 train no.142410  loss = 5.41243 avg_loss = 3.74073\n",
      "epoch no.1 train no.142420  loss = 4.88911 avg_loss = 3.77028\n",
      "epoch no.1 train no.142430  loss = 2.51144 avg_loss = 3.78323\n",
      "epoch no.1 train no.142440  loss = 2.55490 avg_loss = 3.69931\n",
      "epoch no.1 train no.142450  loss = 3.36390 avg_loss = 3.67734\n",
      "epoch no.1 train no.142460  loss = 1.73578 avg_loss = 3.60765\n",
      "epoch no.1 train no.142470  loss = 2.58307 avg_loss = 3.56254\n",
      "epoch no.1 train no.142480  loss = 7.04388 avg_loss = 3.62223\n",
      "epoch no.1 train no.142490  loss = 4.34189 avg_loss = 3.62972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.142500  loss = 3.44033 avg_loss = 3.63932\n",
      "epoch no.1 train no.142510  loss = 3.09818 avg_loss = 3.69171\n",
      "epoch no.1 train no.142520  loss = 5.51493 avg_loss = 3.80415\n",
      "epoch no.1 train no.142530  loss = 3.05752 avg_loss = 3.79585\n",
      "epoch no.1 train no.142540  loss = 4.07324 avg_loss = 3.77690\n",
      "epoch no.1 train no.142550  loss = 5.82680 avg_loss = 3.81904\n",
      "epoch no.1 train no.142560  loss = 4.49937 avg_loss = 3.81206\n",
      "epoch no.1 train no.142570  loss = 3.76365 avg_loss = 3.75302\n",
      "epoch no.1 train no.142580  loss = 1.84094 avg_loss = 3.68724\n",
      "epoch no.1 train no.142590  loss = 3.87367 avg_loss = 3.66720\n",
      "epoch no.1 train no.142600  loss = 3.28329 avg_loss = 3.68909\n",
      "epoch no.1 train no.142610  loss = 4.68364 avg_loss = 3.71635\n",
      "epoch no.1 train no.142620  loss = 3.66998 avg_loss = 3.75755\n",
      "epoch no.1 train no.142630  loss = 3.54744 avg_loss = 3.68985\n",
      "epoch no.1 train no.142640  loss = 2.18226 avg_loss = 3.69961\n",
      "epoch no.1 train no.142650  loss = 2.51746 avg_loss = 3.65358\n",
      "epoch no.1 train no.142660  loss = 4.07238 avg_loss = 3.70624\n",
      "epoch no.1 train no.142670  loss = 4.57901 avg_loss = 3.71315\n",
      "epoch no.1 train no.142680  loss = 3.02437 avg_loss = 3.70137\n",
      "epoch no.1 train no.142690  loss = 3.02882 avg_loss = 3.64606\n",
      "epoch no.1 train no.142700  loss = 3.06758 avg_loss = 3.67913\n",
      "epoch no.1 train no.142710  loss = 3.83691 avg_loss = 3.65855\n",
      "epoch no.1 train no.142720  loss = 3.05031 avg_loss = 3.61559\n",
      "epoch no.1 train no.142730  loss = 2.49171 avg_loss = 3.59178\n",
      "epoch no.1 train no.142740  loss = 2.65185 avg_loss = 3.60849\n",
      "epoch no.1 train no.142750  loss = 3.26651 avg_loss = 3.59482\n",
      "epoch no.1 train no.142760  loss = 2.32147 avg_loss = 3.60037\n",
      "epoch no.1 train no.142770  loss = 3.56223 avg_loss = 3.58115\n",
      "epoch no.1 train no.142780  loss = 4.22651 avg_loss = 3.59093\n",
      "epoch no.1 train no.142790  loss = 3.64174 avg_loss = 3.67765\n",
      "epoch no.1 train no.142800  loss = 3.36811 avg_loss = 3.66327\n",
      "epoch no.1 train no.142810  loss = 3.35037 avg_loss = 3.63208\n",
      "epoch no.1 train no.142820  loss = 4.42768 avg_loss = 3.64829\n",
      "epoch no.1 train no.142830  loss = 5.00236 avg_loss = 3.72065\n",
      "epoch no.1 train no.142840  loss = 6.24374 avg_loss = 3.79357\n",
      "epoch no.1 train no.142850  loss = 5.55673 avg_loss = 3.80955\n",
      "epoch no.1 train no.142860  loss = 4.67245 avg_loss = 3.79944\n",
      "epoch no.1 train no.142870  loss = 3.57838 avg_loss = 3.78736\n",
      "epoch no.1 train no.142880  loss = 3.43561 avg_loss = 3.84037\n",
      "epoch no.1 train no.142890  loss = 3.32284 avg_loss = 3.78882\n",
      "epoch no.1 train no.142900  loss = 5.23797 avg_loss = 3.80823\n",
      "epoch no.1 train no.142910  loss = 2.59905 avg_loss = 3.78443\n",
      "epoch no.1 train no.142920  loss = 3.75270 avg_loss = 3.79874\n",
      "epoch no.1 train no.142930  loss = 3.64397 avg_loss = 3.81495\n",
      "epoch no.1 train no.142940  loss = 3.56760 avg_loss = 3.74148\n",
      "epoch no.1 train no.142950  loss = 4.23560 avg_loss = 3.73296\n",
      "epoch no.1 train no.142960  loss = 2.86090 avg_loss = 3.70193\n",
      "epoch no.1 train no.142970  loss = 4.59891 avg_loss = 3.73018\n",
      "epoch no.1 train no.142980  loss = 3.30894 avg_loss = 3.68285\n",
      "epoch no.1 train no.142990  loss = 3.31840 avg_loss = 3.64899\n",
      "epoch no.1 train no.143000  loss = 3.64558 avg_loss = 3.65009\n",
      "4\n",
      "to_tokens: ['▁가을', '▁드라마', 'st', '▁명', '</s>', '</s>']\n",
      "추억의 ost 모음집</s>\n",
      "epoch no.1 train no.143010  loss = 3.91481 avg_loss = 3.62213\n",
      "epoch no.1 train no.143020  loss = 2.92550 avg_loss = 3.61742\n",
      "epoch no.1 train no.143030  loss = 3.87182 avg_loss = 3.62794\n",
      "epoch no.1 train no.143040  loss = 4.12326 avg_loss = 3.65251\n",
      "epoch no.1 train no.143050  loss = 3.85782 avg_loss = 3.62671\n",
      "epoch no.1 train no.143060  loss = 2.83547 avg_loss = 3.59817\n",
      "epoch no.1 train no.143070  loss = 4.47958 avg_loss = 3.60358\n",
      "epoch no.1 train no.143080  loss = 1.98012 avg_loss = 3.62268\n",
      "epoch no.1 train no.143090  loss = 3.23266 avg_loss = 3.69204\n",
      "epoch no.1 train no.143100  loss = 3.59751 avg_loss = 3.71398\n",
      "epoch no.1 train no.143110  loss = 3.15466 avg_loss = 3.74591\n",
      "epoch no.1 train no.143120  loss = 3.75340 avg_loss = 3.77100\n",
      "epoch no.1 train no.143130  loss = 3.24540 avg_loss = 3.75847\n",
      "epoch no.1 train no.143140  loss = 2.83626 avg_loss = 3.73139\n",
      "epoch no.1 train no.143150  loss = 5.24272 avg_loss = 3.73539\n",
      "epoch no.1 train no.143160  loss = 2.29222 avg_loss = 3.71030\n",
      "epoch no.1 train no.143170  loss = 4.61864 avg_loss = 3.77850\n",
      "epoch no.1 train no.143180  loss = 4.89825 avg_loss = 3.81791\n",
      "epoch no.1 train no.143190  loss = 3.97122 avg_loss = 3.79585\n",
      "epoch no.1 train no.143200  loss = 3.38329 avg_loss = 3.78497\n",
      "epoch no.1 train no.143210  loss = 4.43105 avg_loss = 3.79625\n",
      "epoch no.1 train no.143220  loss = 4.84270 avg_loss = 3.80275\n",
      "epoch no.1 train no.143230  loss = 1.59118 avg_loss = 3.74677\n",
      "epoch no.1 train no.143240  loss = 5.77442 avg_loss = 3.72535\n",
      "epoch no.1 train no.143250  loss = 4.51665 avg_loss = 3.69165\n",
      "epoch no.1 train no.143260  loss = 1.73696 avg_loss = 3.69372\n",
      "epoch no.1 train no.143270  loss = 2.86285 avg_loss = 3.71847\n",
      "epoch no.1 train no.143280  loss = 3.09835 avg_loss = 3.72014\n",
      "epoch no.1 train no.143290  loss = 2.47940 avg_loss = 3.70859\n",
      "epoch no.1 train no.143300  loss = 3.02765 avg_loss = 3.69652\n",
      "epoch no.1 train no.143310  loss = 2.32240 avg_loss = 3.65407\n",
      "epoch no.1 train no.143320  loss = 1.79639 avg_loss = 3.61448\n",
      "epoch no.1 train no.143330  loss = 4.04143 avg_loss = 3.60232\n",
      "epoch no.1 train no.143340  loss = 4.21993 avg_loss = 3.63455\n",
      "epoch no.1 train no.143350  loss = 1.75522 avg_loss = 3.57530\n",
      "epoch no.1 train no.143360  loss = 5.77446 avg_loss = 3.61840\n",
      "epoch no.1 train no.143370  loss = 3.94995 avg_loss = 3.59783\n",
      "epoch no.1 train no.143380  loss = 3.30982 avg_loss = 3.65127\n",
      "epoch no.1 train no.143390  loss = 6.63539 avg_loss = 3.69469\n",
      "epoch no.1 train no.143400  loss = 2.52877 avg_loss = 3.64937\n",
      "epoch no.1 train no.143410  loss = 3.24215 avg_loss = 3.68102\n",
      "epoch no.1 train no.143420  loss = 4.06205 avg_loss = 3.70571\n",
      "epoch no.1 train no.143430  loss = 5.83096 avg_loss = 3.68665\n",
      "epoch no.1 train no.143440  loss = 3.78924 avg_loss = 3.63689\n",
      "epoch no.1 train no.143450  loss = 6.95151 avg_loss = 3.63404\n",
      "epoch no.1 train no.143460  loss = 3.41128 avg_loss = 3.63448\n",
      "epoch no.1 train no.143470  loss = 6.36904 avg_loss = 3.70627\n",
      "epoch no.1 train no.143480  loss = 3.46217 avg_loss = 3.71185\n",
      "epoch no.1 train no.143490  loss = 5.52346 avg_loss = 3.76040\n",
      "epoch no.1 train no.143500  loss = 3.45338 avg_loss = 3.75457\n",
      "epoch no.1 train no.143510  loss = 2.33216 avg_loss = 3.74975\n",
      "epoch no.1 train no.143520  loss = 2.40568 avg_loss = 3.77483\n",
      "epoch no.1 train no.143530  loss = 2.78838 avg_loss = 3.72770\n",
      "epoch no.1 train no.143540  loss = 4.74450 avg_loss = 3.73066\n",
      "epoch no.1 train no.143550  loss = 2.24984 avg_loss = 3.69479\n",
      "epoch no.1 train no.143560  loss = 3.95286 avg_loss = 3.72343\n",
      "epoch no.1 train no.143570  loss = 3.54897 avg_loss = 3.69100\n",
      "epoch no.1 train no.143580  loss = 3.08053 avg_loss = 3.64694\n",
      "epoch no.1 train no.143590  loss = 4.71641 avg_loss = 3.66928\n",
      "epoch no.1 train no.143600  loss = 4.16917 avg_loss = 3.64574\n",
      "epoch no.1 train no.143610  loss = 1.78535 avg_loss = 3.56724\n",
      "epoch no.1 train no.143620  loss = 6.20894 avg_loss = 3.58399\n",
      "epoch no.1 train no.143630  loss = 4.80335 avg_loss = 3.59864\n",
      "epoch no.1 train no.143640  loss = 3.13591 avg_loss = 3.60312\n",
      "epoch no.1 train no.143650  loss = 3.03277 avg_loss = 3.58470\n",
      "epoch no.1 train no.143660  loss = 5.00256 avg_loss = 3.58612\n",
      "epoch no.1 train no.143670  loss = 4.45714 avg_loss = 3.54517\n",
      "epoch no.1 train no.143680  loss = 2.52962 avg_loss = 3.51787\n",
      "epoch no.1 train no.143690  loss = 3.68992 avg_loss = 3.55035\n",
      "epoch no.1 train no.143700  loss = 3.40880 avg_loss = 3.58378\n",
      "epoch no.1 train no.143710  loss = 3.70862 avg_loss = 3.64606\n",
      "epoch no.1 train no.143720  loss = 3.27323 avg_loss = 3.68121\n",
      "epoch no.1 train no.143730  loss = 2.28148 avg_loss = 3.67229\n",
      "epoch no.1 train no.143740  loss = 4.78062 avg_loss = 3.63953\n",
      "epoch no.1 train no.143750  loss = 2.52668 avg_loss = 3.59842\n",
      "epoch no.1 train no.143760  loss = 5.29889 avg_loss = 3.62061\n",
      "epoch no.1 train no.143770  loss = 5.15065 avg_loss = 3.68627\n",
      "epoch no.1 train no.143780  loss = 3.13662 avg_loss = 3.66629\n",
      "epoch no.1 train no.143790  loss = 4.51322 avg_loss = 3.71516\n",
      "epoch no.1 train no.143800  loss = 2.14517 avg_loss = 3.69054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.143810  loss = 1.73428 avg_loss = 3.63976\n",
      "epoch no.1 train no.143820  loss = 4.72022 avg_loss = 3.60738\n",
      "epoch no.1 train no.143830  loss = 5.43857 avg_loss = 3.61773\n",
      "epoch no.1 train no.143840  loss = 2.78267 avg_loss = 3.66753\n",
      "epoch no.1 train no.143850  loss = 3.67007 avg_loss = 3.67429\n",
      "epoch no.1 train no.143860  loss = 5.06559 avg_loss = 3.75877\n",
      "epoch no.1 train no.143870  loss = 3.91061 avg_loss = 3.77033\n",
      "epoch no.1 train no.143880  loss = 4.59133 avg_loss = 3.83363\n",
      "epoch no.1 train no.143890  loss = 2.82311 avg_loss = 3.75192\n",
      "epoch no.1 train no.143900  loss = 2.54146 avg_loss = 3.72393\n",
      "epoch no.1 train no.143910  loss = 3.38555 avg_loss = 3.75870\n",
      "epoch no.1 train no.143920  loss = 5.13553 avg_loss = 3.75194\n",
      "epoch no.1 train no.143930  loss = 4.24419 avg_loss = 3.76876\n",
      "epoch no.1 train no.143940  loss = 3.34401 avg_loss = 3.77565\n",
      "epoch no.1 train no.143950  loss = 2.71678 avg_loss = 3.78950\n",
      "epoch no.1 train no.143960  loss = 3.41363 avg_loss = 3.79938\n",
      "epoch no.1 train no.143970  loss = 3.06519 avg_loss = 3.75880\n",
      "epoch no.1 train no.143980  loss = 2.26250 avg_loss = 3.74671\n",
      "epoch no.1 train no.143990  loss = 4.39641 avg_loss = 3.70201\n",
      "epoch no.1 train no.144000  loss = 6.01397 avg_loss = 3.67307\n",
      "4\n",
      "to_tokens: ['▁가을', '▁드라마', '년대', '▁발라', '송', '</s>']\n",
      "추억의 2000년대 팝송</s>\n",
      "epoch no.1 train no.144010  loss = 2.87702 avg_loss = 3.63810\n",
      "epoch no.1 train no.144020  loss = 2.32002 avg_loss = 3.60420\n",
      "epoch no.1 train no.144030  loss = 3.61657 avg_loss = 3.63243\n",
      "epoch no.1 train no.144040  loss = 5.75772 avg_loss = 3.69244\n",
      "epoch no.1 train no.144050  loss = 3.50472 avg_loss = 3.74259\n",
      "epoch no.1 train no.144060  loss = 3.25927 avg_loss = 3.74533\n",
      "epoch no.1 train no.144070  loss = 4.69091 avg_loss = 3.73945\n",
      "epoch no.1 train no.144080  loss = 2.70971 avg_loss = 3.76050\n",
      "epoch no.1 train no.144090  loss = 3.56146 avg_loss = 3.74561\n",
      "epoch no.1 train no.144100  loss = 5.03359 avg_loss = 3.75961\n",
      "epoch no.1 train no.144110  loss = 4.07294 avg_loss = 3.76452\n",
      "epoch no.1 train no.144120  loss = 3.61698 avg_loss = 3.75567\n",
      "epoch no.1 train no.144130  loss = 5.23622 avg_loss = 3.72746\n",
      "epoch no.1 train no.144140  loss = 3.66818 avg_loss = 3.67889\n",
      "epoch no.1 train no.144150  loss = 4.95834 avg_loss = 3.71621\n",
      "epoch no.1 train no.144160  loss = 2.53056 avg_loss = 3.74590\n",
      "epoch no.1 train no.144170  loss = 3.53391 avg_loss = 3.80795\n",
      "epoch no.1 train no.144180  loss = 2.96771 avg_loss = 3.81323\n",
      "epoch no.1 train no.144190  loss = 3.09102 avg_loss = 3.81127\n",
      "epoch no.1 train no.144200  loss = 6.29954 avg_loss = 3.79189\n",
      "epoch no.1 train no.144210  loss = 4.63795 avg_loss = 3.87908\n",
      "epoch no.1 train no.144220  loss = 3.10751 avg_loss = 3.86807\n",
      "epoch no.1 train no.144230  loss = 5.43063 avg_loss = 3.89514\n",
      "epoch no.1 train no.144240  loss = 3.41735 avg_loss = 3.86539\n",
      "epoch no.1 train no.144250  loss = 3.16063 avg_loss = 3.87329\n",
      "epoch no.1 train no.144260  loss = 3.14936 avg_loss = 3.80493\n",
      "epoch no.1 train no.144270  loss = 2.84331 avg_loss = 3.79570\n",
      "epoch no.1 train no.144280  loss = 2.63811 avg_loss = 3.81595\n",
      "epoch no.1 train no.144290  loss = 2.78597 avg_loss = 3.77048\n",
      "epoch no.1 train no.144300  loss = 5.26310 avg_loss = 3.82958\n",
      "epoch no.1 train no.144310  loss = 4.52264 avg_loss = 3.82875\n",
      "epoch no.1 train no.144320  loss = 2.80521 avg_loss = 3.76384\n",
      "epoch no.1 train no.144330  loss = 4.44520 avg_loss = 3.76288\n",
      "epoch no.1 train no.144340  loss = 2.64241 avg_loss = 3.73504\n",
      "epoch no.1 train no.144350  loss = 3.36359 avg_loss = 3.73850\n",
      "epoch no.1 train no.144360  loss = 4.75191 avg_loss = 3.78361\n",
      "epoch no.1 train no.144370  loss = 2.96361 avg_loss = 3.71568\n",
      "epoch no.1 train no.144380  loss = 2.56945 avg_loss = 3.69374\n",
      "epoch no.1 train no.144390  loss = 2.66555 avg_loss = 3.70118\n",
      "epoch no.1 train no.144400  loss = 4.15207 avg_loss = 3.69754\n",
      "epoch no.1 train no.144410  loss = 5.66985 avg_loss = 3.71624\n",
      "epoch no.1 train no.144420  loss = 4.67172 avg_loss = 3.72567\n",
      "epoch no.1 train no.144430  loss = 3.79327 avg_loss = 3.70162\n",
      "epoch no.1 train no.144440  loss = 4.21381 avg_loss = 3.74459\n",
      "epoch no.1 train no.144450  loss = 2.82196 avg_loss = 3.75573\n",
      "epoch no.1 train no.144460  loss = 2.48880 avg_loss = 3.75858\n",
      "epoch no.1 train no.144470  loss = 4.00952 avg_loss = 3.77894\n",
      "epoch no.1 train no.144480  loss = 4.59132 avg_loss = 3.78458\n",
      "epoch no.1 train no.144490  loss = 4.37968 avg_loss = 3.75478\n",
      "epoch no.1 train no.144500  loss = 3.16754 avg_loss = 3.77741\n",
      "epoch no.1 train no.144510  loss = 4.09862 avg_loss = 3.78484\n",
      "epoch no.1 train no.144520  loss = 4.53635 avg_loss = 3.75246\n",
      "epoch no.1 train no.144530  loss = 3.41846 avg_loss = 3.72813\n",
      "epoch no.1 train no.144540  loss = 4.71925 avg_loss = 3.75959\n",
      "epoch no.1 train no.144550  loss = 3.64698 avg_loss = 3.75314\n",
      "epoch no.1 train no.144560  loss = 3.90856 avg_loss = 3.71037\n",
      "epoch no.1 train no.144570  loss = 3.49597 avg_loss = 3.72752\n",
      "epoch no.1 train no.144580  loss = 2.60252 avg_loss = 3.76365\n",
      "epoch no.1 train no.144590  loss = 2.93804 avg_loss = 3.71981\n",
      "epoch no.1 train no.144600  loss = 3.13013 avg_loss = 3.74396\n",
      "epoch no.1 train no.144610  loss = 4.73818 avg_loss = 3.71301\n",
      "epoch no.1 train no.144620  loss = 3.97193 avg_loss = 3.67470\n",
      "epoch no.1 train no.144630  loss = 2.50343 avg_loss = 3.72513\n",
      "epoch no.1 train no.144640  loss = 1.70909 avg_loss = 3.69581\n",
      "epoch no.1 train no.144650  loss = 4.42052 avg_loss = 3.74919\n",
      "epoch no.1 train no.144660  loss = 4.01697 avg_loss = 3.74634\n",
      "epoch no.1 train no.144670  loss = 3.04887 avg_loss = 3.71188\n",
      "epoch no.1 train no.144680  loss = 4.08790 avg_loss = 3.68571\n",
      "epoch no.1 train no.144690  loss = 3.23928 avg_loss = 3.68109\n",
      "epoch no.1 train no.144700  loss = 3.63655 avg_loss = 3.69451\n",
      "epoch no.1 train no.144710  loss = 2.84505 avg_loss = 3.69216\n",
      "epoch no.1 train no.144720  loss = 3.55545 avg_loss = 3.71236\n",
      "epoch no.1 train no.144730  loss = 3.89354 avg_loss = 3.64995\n",
      "epoch no.1 train no.144740  loss = 3.48514 avg_loss = 3.61788\n",
      "epoch no.1 train no.144750  loss = 3.19460 avg_loss = 3.59195\n",
      "epoch no.1 train no.144760  loss = 4.36054 avg_loss = 3.58833\n",
      "epoch no.1 train no.144770  loss = 3.09130 avg_loss = 3.56733\n",
      "epoch no.1 train no.144780  loss = 1.65907 avg_loss = 3.55327\n",
      "epoch no.1 train no.144790  loss = 3.27790 avg_loss = 3.53800\n",
      "epoch no.1 train no.144800  loss = 3.48969 avg_loss = 3.59984\n",
      "epoch no.1 train no.144810  loss = 4.29324 avg_loss = 3.64224\n",
      "epoch no.1 train no.144820  loss = 2.69816 avg_loss = 3.57804\n",
      "epoch no.1 train no.144830  loss = 3.96577 avg_loss = 3.55845\n",
      "epoch no.1 train no.144840  loss = 4.13354 avg_loss = 3.60671\n",
      "epoch no.1 train no.144850  loss = 3.52658 avg_loss = 3.62811\n",
      "epoch no.1 train no.144860  loss = 2.99209 avg_loss = 3.63332\n",
      "epoch no.1 train no.144870  loss = 3.54060 avg_loss = 3.66077\n",
      "epoch no.1 train no.144880  loss = 6.71504 avg_loss = 3.66378\n",
      "epoch no.1 train no.144890  loss = 2.22701 avg_loss = 3.70424\n",
      "epoch no.1 train no.144900  loss = 2.73833 avg_loss = 3.67025\n",
      "epoch no.1 train no.144910  loss = 3.64799 avg_loss = 3.64911\n",
      "epoch no.1 train no.144920  loss = 3.84090 avg_loss = 3.64019\n",
      "epoch no.1 train no.144930  loss = 3.83546 avg_loss = 3.57778\n",
      "epoch no.1 train no.144940  loss = 4.01313 avg_loss = 3.60969\n",
      "epoch no.1 train no.144950  loss = 4.67660 avg_loss = 3.67569\n",
      "epoch no.1 train no.144960  loss = 4.53036 avg_loss = 3.64213\n",
      "epoch no.1 train no.144970  loss = 4.68441 avg_loss = 3.67057\n",
      "epoch no.1 train no.144980  loss = 2.64362 avg_loss = 3.69980\n",
      "epoch no.1 train no.144990  loss = 2.76785 avg_loss = 3.70806\n",
      "epoch no.1 train no.145000  loss = 2.62606 avg_loss = 3.72224\n",
      "5\n",
      "to_tokens: ['▁가을', '▁드라마', '▁명', '곡', '들', '음', '</s>']\n",
      "추억의 걸그룹 명곡모음</s>\n",
      "epoch no.1 train no.145010  loss = 4.83211 avg_loss = 3.72886\n",
      "epoch no.1 train no.145020  loss = 3.72525 avg_loss = 3.70118\n",
      "epoch no.1 train no.145030  loss = 2.02540 avg_loss = 3.64832\n",
      "epoch no.1 train no.145040  loss = 3.61342 avg_loss = 3.63625\n",
      "epoch no.1 train no.145050  loss = 2.55589 avg_loss = 3.59617\n",
      "epoch no.1 train no.145060  loss = 3.81745 avg_loss = 3.61032\n",
      "epoch no.1 train no.145070  loss = 2.86907 avg_loss = 3.60994\n",
      "epoch no.1 train no.145080  loss = 2.33596 avg_loss = 3.57314\n",
      "epoch no.1 train no.145090  loss = 4.06710 avg_loss = 3.59112\n",
      "epoch no.1 train no.145100  loss = 3.98301 avg_loss = 3.66475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.145110  loss = 3.90945 avg_loss = 3.63509\n",
      "epoch no.1 train no.145120  loss = 1.97220 avg_loss = 3.65887\n",
      "epoch no.1 train no.145130  loss = 4.66328 avg_loss = 3.67492\n",
      "epoch no.1 train no.145140  loss = 2.16376 avg_loss = 3.66082\n",
      "epoch no.1 train no.145150  loss = 2.62761 avg_loss = 3.64767\n",
      "epoch no.1 train no.145160  loss = 3.47186 avg_loss = 3.65596\n",
      "epoch no.1 train no.145170  loss = 3.35710 avg_loss = 3.64782\n",
      "epoch no.1 train no.145180  loss = 2.63487 avg_loss = 3.68359\n",
      "epoch no.1 train no.145190  loss = 2.17425 avg_loss = 3.60189\n",
      "epoch no.1 train no.145200  loss = 3.39774 avg_loss = 3.60034\n",
      "epoch no.1 train no.145210  loss = 2.48340 avg_loss = 3.59770\n",
      "epoch no.1 train no.145220  loss = 4.59617 avg_loss = 3.58779\n",
      "epoch no.1 train no.145230  loss = 5.25751 avg_loss = 3.59380\n",
      "epoch no.1 train no.145240  loss = 5.33564 avg_loss = 3.55600\n",
      "epoch no.1 train no.145250  loss = 4.78562 avg_loss = 3.62579\n",
      "epoch no.1 train no.145260  loss = 3.50288 avg_loss = 3.63919\n",
      "epoch no.1 train no.145270  loss = 1.34775 avg_loss = 3.65925\n",
      "epoch no.1 train no.145280  loss = 1.96506 avg_loss = 3.65550\n",
      "epoch no.1 train no.145290  loss = 4.63632 avg_loss = 3.66085\n",
      "epoch no.1 train no.145300  loss = 5.59023 avg_loss = 3.72830\n",
      "epoch no.1 train no.145310  loss = 2.88352 avg_loss = 3.75023\n",
      "epoch no.1 train no.145320  loss = 2.18337 avg_loss = 3.73050\n",
      "epoch no.1 train no.145330  loss = 4.63673 avg_loss = 3.69240\n",
      "epoch no.1 train no.145340  loss = 6.16236 avg_loss = 3.71114\n",
      "epoch no.1 train no.145350  loss = 3.31812 avg_loss = 3.72759\n",
      "epoch no.1 train no.145360  loss = 3.48533 avg_loss = 3.71387\n",
      "epoch no.1 train no.145370  loss = 3.00061 avg_loss = 3.72817\n",
      "epoch no.1 train no.145380  loss = 2.33805 avg_loss = 3.73853\n",
      "epoch no.1 train no.145390  loss = 3.08406 avg_loss = 3.78048\n",
      "epoch no.1 train no.145400  loss = 3.51566 avg_loss = 3.79257\n",
      "epoch no.1 train no.145410  loss = 5.28644 avg_loss = 3.78953\n",
      "epoch no.1 train no.145420  loss = 3.80028 avg_loss = 3.80312\n",
      "epoch no.1 train no.145430  loss = 2.01600 avg_loss = 3.75292\n",
      "epoch no.1 train no.145440  loss = 2.50485 avg_loss = 3.66546\n",
      "epoch no.1 train no.145450  loss = 2.62992 avg_loss = 3.64011\n",
      "epoch no.1 train no.145460  loss = 2.99014 avg_loss = 3.63301\n",
      "epoch no.1 train no.145470  loss = 2.40036 avg_loss = 3.60983\n",
      "epoch no.1 train no.145480  loss = 1.48570 avg_loss = 3.60178\n",
      "epoch no.1 train no.145490  loss = 5.15662 avg_loss = 3.63442\n",
      "epoch no.1 train no.145500  loss = 3.75688 avg_loss = 3.66702\n",
      "epoch no.1 train no.145510  loss = 4.63956 avg_loss = 3.72506\n",
      "epoch no.1 train no.145520  loss = 3.17840 avg_loss = 3.68828\n",
      "epoch no.1 train no.145530  loss = 1.79610 avg_loss = 3.66948\n",
      "epoch no.1 train no.145540  loss = 3.87096 avg_loss = 3.65070\n",
      "epoch no.1 train no.145550  loss = 2.84290 avg_loss = 3.60086\n",
      "epoch no.1 train no.145560  loss = 1.79280 avg_loss = 3.66017\n",
      "epoch no.1 train no.145570  loss = 4.74134 avg_loss = 3.63058\n",
      "epoch no.1 train no.145580  loss = 3.01440 avg_loss = 3.64983\n",
      "epoch no.1 train no.145590  loss = 4.05313 avg_loss = 3.61608\n",
      "epoch no.1 train no.145600  loss = 4.02546 avg_loss = 3.62476\n",
      "epoch no.1 train no.145610  loss = 2.93812 avg_loss = 3.61831\n",
      "epoch no.1 train no.145620  loss = 4.22541 avg_loss = 3.58563\n",
      "epoch no.1 train no.145630  loss = 1.80644 avg_loss = 3.58367\n",
      "epoch no.1 train no.145640  loss = 3.80141 avg_loss = 3.59872\n",
      "epoch no.1 train no.145650  loss = 2.99216 avg_loss = 3.60520\n",
      "epoch no.1 train no.145660  loss = 5.62039 avg_loss = 3.64661\n",
      "epoch no.1 train no.145670  loss = 2.56355 avg_loss = 3.63785\n",
      "epoch no.1 train no.145680  loss = 3.28192 avg_loss = 3.62769\n",
      "epoch no.1 train no.145690  loss = 4.45158 avg_loss = 3.63882\n",
      "epoch no.1 train no.145700  loss = 2.63132 avg_loss = 3.59678\n",
      "epoch no.1 train no.145710  loss = 3.41469 avg_loss = 3.61638\n",
      "epoch no.1 train no.145720  loss = 3.30274 avg_loss = 3.59675\n",
      "epoch no.1 train no.145730  loss = 3.65423 avg_loss = 3.58958\n",
      "epoch no.1 train no.145740  loss = 5.14022 avg_loss = 3.64512\n",
      "epoch no.1 train no.145750  loss = 2.67717 avg_loss = 3.67525\n",
      "epoch no.1 train no.145760  loss = 3.14543 avg_loss = 3.68077\n",
      "epoch no.1 train no.145770  loss = 2.11312 avg_loss = 3.70661\n",
      "epoch no.1 train no.145780  loss = 3.13390 avg_loss = 3.66510\n",
      "epoch no.1 train no.145790  loss = 2.71586 avg_loss = 3.63894\n",
      "epoch no.1 train no.145800  loss = 4.04309 avg_loss = 3.61403\n",
      "epoch no.1 train no.145810  loss = 4.24006 avg_loss = 3.61466\n",
      "epoch no.1 train no.145820  loss = 3.45126 avg_loss = 3.64855\n",
      "epoch no.1 train no.145830  loss = 3.05302 avg_loss = 3.64275\n",
      "epoch no.1 train no.145840  loss = 2.95474 avg_loss = 3.61777\n",
      "epoch no.1 train no.145850  loss = 6.42157 avg_loss = 3.66300\n",
      "epoch no.1 train no.145860  loss = 3.68232 avg_loss = 3.69827\n",
      "epoch no.1 train no.145870  loss = 4.16815 avg_loss = 3.67798\n",
      "epoch no.1 train no.145880  loss = 2.74009 avg_loss = 3.66883\n",
      "epoch no.1 train no.145890  loss = 3.48262 avg_loss = 3.69938\n",
      "epoch no.1 train no.145900  loss = 2.51498 avg_loss = 3.68111\n",
      "epoch no.1 train no.145910  loss = 4.27392 avg_loss = 3.72715\n",
      "epoch no.1 train no.145920  loss = 2.85125 avg_loss = 3.73634\n",
      "epoch no.1 train no.145930  loss = 3.11170 avg_loss = 3.76960\n",
      "epoch no.1 train no.145940  loss = 4.42196 avg_loss = 3.79160\n",
      "epoch no.1 train no.145950  loss = 3.73577 avg_loss = 3.75826\n",
      "epoch no.1 train no.145960  loss = 2.70126 avg_loss = 3.72911\n",
      "epoch no.1 train no.145970  loss = 4.56687 avg_loss = 3.74517\n",
      "epoch no.1 train no.145980  loss = 3.69971 avg_loss = 3.72740\n",
      "epoch no.1 train no.145990  loss = 2.77283 avg_loss = 3.68707\n",
      "epoch no.1 train no.146000  loss = 2.78820 avg_loss = 3.64684\n",
      "4\n",
      "to_tokens: ['▁가을', '▁드라마', '▁o', 'st', '</s>', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.1 train no.146010  loss = 3.80825 avg_loss = 3.71489\n",
      "epoch no.1 train no.146020  loss = 4.52763 avg_loss = 3.71813\n",
      "epoch no.1 train no.146030  loss = 2.80640 avg_loss = 3.66176\n",
      "epoch no.1 train no.146040  loss = 3.24713 avg_loss = 3.65002\n",
      "epoch no.1 train no.146050  loss = 3.41919 avg_loss = 3.63474\n",
      "epoch no.1 train no.146060  loss = 5.58456 avg_loss = 3.61185\n",
      "epoch no.1 train no.146070  loss = 3.08178 avg_loss = 3.60074\n",
      "epoch no.1 train no.146080  loss = 3.90922 avg_loss = 3.64906\n",
      "epoch no.1 train no.146090  loss = 5.28865 avg_loss = 3.64903\n",
      "epoch no.1 train no.146100  loss = 3.75630 avg_loss = 3.66933\n",
      "epoch no.1 train no.146110  loss = 3.95296 avg_loss = 3.72505\n",
      "epoch no.1 train no.146120  loss = 3.69093 avg_loss = 3.74758\n",
      "epoch no.1 train no.146130  loss = 4.73720 avg_loss = 3.74331\n",
      "epoch no.1 train no.146140  loss = 4.71001 avg_loss = 3.71060\n",
      "epoch no.1 train no.146150  loss = 2.59642 avg_loss = 3.68864\n",
      "epoch no.1 train no.146160  loss = 3.03659 avg_loss = 3.67137\n",
      "epoch no.1 train no.146170  loss = 3.84168 avg_loss = 3.67387\n",
      "epoch no.1 train no.146180  loss = 2.37772 avg_loss = 3.63620\n",
      "epoch no.1 train no.146190  loss = 3.26431 avg_loss = 3.65670\n",
      "epoch no.1 train no.146200  loss = 2.44591 avg_loss = 3.63719\n",
      "epoch no.1 train no.146210  loss = 4.55963 avg_loss = 3.61344\n",
      "epoch no.1 train no.146220  loss = 2.81869 avg_loss = 3.60887\n",
      "epoch no.1 train no.146230  loss = 3.67353 avg_loss = 3.56730\n",
      "epoch no.1 train no.146240  loss = 2.14827 avg_loss = 3.59613\n",
      "epoch no.1 train no.146250  loss = 3.24530 avg_loss = 3.60841\n",
      "epoch no.1 train no.146260  loss = 3.90798 avg_loss = 3.65682\n",
      "epoch no.1 train no.146270  loss = 3.54321 avg_loss = 3.65454\n",
      "epoch no.1 train no.146280  loss = 3.43360 avg_loss = 3.67659\n",
      "epoch no.1 train no.146290  loss = 2.65905 avg_loss = 3.73768\n",
      "epoch no.1 train no.146300  loss = 3.44264 avg_loss = 3.73507\n",
      "epoch no.1 train no.146310  loss = 3.12729 avg_loss = 3.74688\n",
      "epoch no.1 train no.146320  loss = 3.28963 avg_loss = 3.66802\n",
      "epoch no.1 train no.146330  loss = 2.62488 avg_loss = 3.63121\n",
      "epoch no.1 train no.146340  loss = 3.12344 avg_loss = 3.65152\n",
      "epoch no.1 train no.146350  loss = 3.32487 avg_loss = 3.65060\n",
      "epoch no.1 train no.146360  loss = 3.76228 avg_loss = 3.62239\n",
      "epoch no.1 train no.146370  loss = 5.75601 avg_loss = 3.63231\n",
      "epoch no.1 train no.146380  loss = 2.24909 avg_loss = 3.55706\n",
      "epoch no.1 train no.146390  loss = 3.76900 avg_loss = 3.54575\n",
      "epoch no.1 train no.146400  loss = 2.85893 avg_loss = 3.51911\n",
      "epoch no.1 train no.146410  loss = 2.45635 avg_loss = 3.57105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.146420  loss = 2.35549 avg_loss = 3.55663\n",
      "epoch no.1 train no.146430  loss = 3.49768 avg_loss = 3.58114\n",
      "epoch no.1 train no.146440  loss = 3.36405 avg_loss = 3.59787\n",
      "epoch no.1 train no.146450  loss = 3.63151 avg_loss = 3.62935\n",
      "epoch no.1 train no.146460  loss = 3.94820 avg_loss = 3.63255\n",
      "epoch no.1 train no.146470  loss = 3.24864 avg_loss = 3.66075\n",
      "epoch no.1 train no.146480  loss = 4.52522 avg_loss = 3.70703\n",
      "epoch no.1 train no.146490  loss = 3.80467 avg_loss = 3.66351\n",
      "epoch no.1 train no.146500  loss = 3.23956 avg_loss = 3.69346\n",
      "epoch no.1 train no.146510  loss = 3.21381 avg_loss = 3.74078\n",
      "epoch no.1 train no.146520  loss = 6.26100 avg_loss = 3.73418\n",
      "epoch no.1 train no.146530  loss = 3.63682 avg_loss = 3.76507\n",
      "epoch no.1 train no.146540  loss = 3.97088 avg_loss = 3.74409\n",
      "epoch no.1 train no.146550  loss = 4.92891 avg_loss = 3.73049\n",
      "epoch no.1 train no.146560  loss = 4.68583 avg_loss = 3.69750\n",
      "epoch no.1 train no.146570  loss = 2.62073 avg_loss = 3.64943\n",
      "epoch no.1 train no.146580  loss = 3.32437 avg_loss = 3.63578\n",
      "epoch no.1 train no.146590  loss = 3.89682 avg_loss = 3.65576\n",
      "epoch no.1 train no.146600  loss = 2.38632 avg_loss = 3.65784\n",
      "epoch no.1 train no.146610  loss = 2.23758 avg_loss = 3.65315\n",
      "epoch no.1 train no.146620  loss = 2.72452 avg_loss = 3.65858\n",
      "epoch no.1 train no.146630  loss = 1.60807 avg_loss = 3.61823\n",
      "epoch no.1 train no.146640  loss = 5.50837 avg_loss = 3.62742\n",
      "epoch no.1 train no.146650  loss = 1.99210 avg_loss = 3.72477\n",
      "epoch no.1 train no.146660  loss = 2.98688 avg_loss = 3.71659\n",
      "epoch no.1 train no.146670  loss = 4.66552 avg_loss = 3.74536\n",
      "epoch no.1 train no.146680  loss = 4.77997 avg_loss = 3.77341\n",
      "epoch no.1 train no.146690  loss = 2.16772 avg_loss = 3.79019\n",
      "epoch no.1 train no.146700  loss = 4.75797 avg_loss = 3.76723\n",
      "epoch no.2 train no.146710  loss = 3.40942 avg_loss = 3.71087\n",
      "epoch no.2 train no.146720  loss = 4.67764 avg_loss = 3.65549\n",
      "epoch no.2 train no.146730  loss = 2.55549 avg_loss = 3.60970\n",
      "epoch no.2 train no.146740  loss = 2.92233 avg_loss = 3.57416\n",
      "epoch no.2 train no.146750  loss = 2.50894 avg_loss = 3.51508\n",
      "epoch no.2 train no.146760  loss = 3.46626 avg_loss = 3.51023\n",
      "epoch no.2 train no.146770  loss = 4.20927 avg_loss = 3.50746\n",
      "epoch no.2 train no.146780  loss = 5.01794 avg_loss = 3.52393\n",
      "epoch no.2 train no.146790  loss = 2.57813 avg_loss = 3.49443\n",
      "epoch no.2 train no.146800  loss = 4.91245 avg_loss = 3.43630\n",
      "epoch no.2 train no.146810  loss = 3.13952 avg_loss = 3.38438\n",
      "epoch no.2 train no.146820  loss = 3.44009 avg_loss = 3.36911\n",
      "epoch no.2 train no.146830  loss = 3.21873 avg_loss = 3.38603\n",
      "epoch no.2 train no.146840  loss = 4.36638 avg_loss = 3.39853\n",
      "epoch no.2 train no.146850  loss = 3.49352 avg_loss = 3.43420\n",
      "epoch no.2 train no.146860  loss = 4.00507 avg_loss = 3.45214\n",
      "epoch no.2 train no.146870  loss = 2.51522 avg_loss = 3.42188\n",
      "epoch no.2 train no.146880  loss = 4.84801 avg_loss = 3.40727\n",
      "epoch no.2 train no.146890  loss = 2.76939 avg_loss = 3.41771\n",
      "epoch no.2 train no.146900  loss = 4.44346 avg_loss = 3.42655\n",
      "epoch no.2 train no.146910  loss = 2.20484 avg_loss = 3.38824\n",
      "epoch no.2 train no.146920  loss = 3.72672 avg_loss = 3.38443\n",
      "epoch no.2 train no.146930  loss = 3.38098 avg_loss = 3.41003\n",
      "epoch no.2 train no.146940  loss = 2.38672 avg_loss = 3.41047\n",
      "epoch no.2 train no.146950  loss = 2.13568 avg_loss = 3.41413\n",
      "epoch no.2 train no.146960  loss = 2.09242 avg_loss = 3.39704\n",
      "epoch no.2 train no.146970  loss = 4.20817 avg_loss = 3.33286\n",
      "epoch no.2 train no.146980  loss = 4.83181 avg_loss = 3.40569\n",
      "epoch no.2 train no.146990  loss = 3.64642 avg_loss = 3.40567\n",
      "epoch no.2 train no.147000  loss = 3.19725 avg_loss = 3.37640\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>', '</s>']\n",
      "추억의 발라드 명곡들</s>\n",
      "epoch no.2 train no.147010  loss = 2.58202 avg_loss = 3.37554\n",
      "epoch no.2 train no.147020  loss = 2.54582 avg_loss = 3.40214\n",
      "epoch no.2 train no.147030  loss = 4.41538 avg_loss = 3.50222\n",
      "epoch no.2 train no.147040  loss = 3.89682 avg_loss = 3.54147\n",
      "epoch no.2 train no.147050  loss = 4.31354 avg_loss = 3.54800\n",
      "epoch no.2 train no.147060  loss = 3.21390 avg_loss = 3.52638\n",
      "epoch no.2 train no.147070  loss = 2.77221 avg_loss = 3.54298\n",
      "epoch no.2 train no.147080  loss = 2.86412 avg_loss = 3.48786\n",
      "epoch no.2 train no.147090  loss = 2.50669 avg_loss = 3.45603\n",
      "epoch no.2 train no.147100  loss = 2.57188 avg_loss = 3.47641\n",
      "epoch no.2 train no.147110  loss = 3.46509 avg_loss = 3.44881\n",
      "epoch no.2 train no.147120  loss = 2.57451 avg_loss = 3.46829\n",
      "epoch no.2 train no.147130  loss = 4.80978 avg_loss = 3.42507\n",
      "epoch no.2 train no.147140  loss = 5.86307 avg_loss = 3.47281\n",
      "epoch no.2 train no.147150  loss = 3.87124 avg_loss = 3.46712\n",
      "epoch no.2 train no.147160  loss = 3.20559 avg_loss = 3.46946\n",
      "epoch no.2 train no.147170  loss = 4.71330 avg_loss = 3.45710\n",
      "epoch no.2 train no.147180  loss = 3.29689 avg_loss = 3.39715\n",
      "epoch no.2 train no.147190  loss = 5.77114 avg_loss = 3.37842\n",
      "epoch no.2 train no.147200  loss = 2.78919 avg_loss = 3.35351\n",
      "epoch no.2 train no.147210  loss = 4.54431 avg_loss = 3.33235\n",
      "epoch no.2 train no.147220  loss = 3.67896 avg_loss = 3.31695\n",
      "epoch no.2 train no.147230  loss = 2.64472 avg_loss = 3.32951\n",
      "epoch no.2 train no.147240  loss = 3.53888 avg_loss = 3.32927\n",
      "epoch no.2 train no.147250  loss = 2.36352 avg_loss = 3.36302\n",
      "epoch no.2 train no.147260  loss = 2.77039 avg_loss = 3.34789\n",
      "epoch no.2 train no.147270  loss = 5.68012 avg_loss = 3.39952\n",
      "epoch no.2 train no.147280  loss = 2.07412 avg_loss = 3.36786\n",
      "epoch no.2 train no.147290  loss = 3.02943 avg_loss = 3.37513\n",
      "epoch no.2 train no.147300  loss = 3.63703 avg_loss = 3.41587\n",
      "epoch no.2 train no.147310  loss = 5.92401 avg_loss = 3.44299\n",
      "epoch no.2 train no.147320  loss = 3.31143 avg_loss = 3.46333\n",
      "epoch no.2 train no.147330  loss = 3.12741 avg_loss = 3.43618\n",
      "epoch no.2 train no.147340  loss = 2.51418 avg_loss = 3.42921\n",
      "epoch no.2 train no.147350  loss = 3.66853 avg_loss = 3.44981\n",
      "epoch no.2 train no.147360  loss = 3.21167 avg_loss = 3.42360\n",
      "epoch no.2 train no.147370  loss = 2.73730 avg_loss = 3.39384\n",
      "epoch no.2 train no.147380  loss = 3.44793 avg_loss = 3.39422\n",
      "epoch no.2 train no.147390  loss = 1.70919 avg_loss = 3.34190\n",
      "epoch no.2 train no.147400  loss = 2.45202 avg_loss = 3.30685\n",
      "epoch no.2 train no.147410  loss = 2.25455 avg_loss = 3.29838\n",
      "epoch no.2 train no.147420  loss = 2.73633 avg_loss = 3.28288\n",
      "epoch no.2 train no.147430  loss = 6.56005 avg_loss = 3.32057\n",
      "epoch no.2 train no.147440  loss = 2.46430 avg_loss = 3.33735\n",
      "epoch no.2 train no.147450  loss = 3.12533 avg_loss = 3.31320\n",
      "epoch no.2 train no.147460  loss = 2.59784 avg_loss = 3.31100\n",
      "epoch no.2 train no.147470  loss = 3.81322 avg_loss = 3.34833\n",
      "epoch no.2 train no.147480  loss = 3.31345 avg_loss = 3.40037\n",
      "epoch no.2 train no.147490  loss = 3.35656 avg_loss = 3.35945\n",
      "epoch no.2 train no.147500  loss = 1.95819 avg_loss = 3.32369\n",
      "epoch no.2 train no.147510  loss = 4.65657 avg_loss = 3.31245\n",
      "epoch no.2 train no.147520  loss = 4.78412 avg_loss = 3.34221\n",
      "epoch no.2 train no.147530  loss = 4.52337 avg_loss = 3.36515\n",
      "epoch no.2 train no.147540  loss = 3.28868 avg_loss = 3.37065\n",
      "epoch no.2 train no.147550  loss = 4.40800 avg_loss = 3.37164\n",
      "epoch no.2 train no.147560  loss = 2.70550 avg_loss = 3.33699\n",
      "epoch no.2 train no.147570  loss = 3.63271 avg_loss = 3.36798\n",
      "epoch no.2 train no.147580  loss = 3.05291 avg_loss = 3.35455\n",
      "epoch no.2 train no.147590  loss = 6.78039 avg_loss = 3.34423\n",
      "epoch no.2 train no.147600  loss = 2.21668 avg_loss = 3.29615\n",
      "epoch no.2 train no.147610  loss = 4.47658 avg_loss = 3.32182\n",
      "epoch no.2 train no.147620  loss = 4.42049 avg_loss = 3.31533\n",
      "epoch no.2 train no.147630  loss = 2.32844 avg_loss = 3.29384\n",
      "epoch no.2 train no.147640  loss = 3.25522 avg_loss = 3.28354\n",
      "epoch no.2 train no.147650  loss = 3.53154 avg_loss = 3.29272\n",
      "epoch no.2 train no.147660  loss = 2.96610 avg_loss = 3.26122\n",
      "epoch no.2 train no.147670  loss = 6.04225 avg_loss = 3.24925\n",
      "epoch no.2 train no.147680  loss = 2.01562 avg_loss = 3.23543\n",
      "epoch no.2 train no.147690  loss = 4.39368 avg_loss = 3.21280\n",
      "epoch no.2 train no.147700  loss = 4.09419 avg_loss = 3.26491\n",
      "epoch no.2 train no.147710  loss = 2.66211 avg_loss = 3.25099\n",
      "epoch no.2 train no.147720  loss = 3.35340 avg_loss = 3.29486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.147730  loss = 3.55608 avg_loss = 3.28685\n",
      "epoch no.2 train no.147740  loss = 3.01671 avg_loss = 3.30301\n",
      "epoch no.2 train no.147750  loss = 2.90665 avg_loss = 3.26760\n",
      "epoch no.2 train no.147760  loss = 2.64989 avg_loss = 3.23073\n",
      "epoch no.2 train no.147770  loss = 2.25139 avg_loss = 3.23087\n",
      "epoch no.2 train no.147780  loss = 3.03622 avg_loss = 3.24799\n",
      "epoch no.2 train no.147790  loss = 4.67087 avg_loss = 3.29645\n",
      "epoch no.2 train no.147800  loss = 2.81987 avg_loss = 3.31086\n",
      "epoch no.2 train no.147810  loss = 4.28764 avg_loss = 3.30313\n",
      "epoch no.2 train no.147820  loss = 2.98785 avg_loss = 3.23619\n",
      "epoch no.2 train no.147830  loss = 4.55805 avg_loss = 3.25793\n",
      "epoch no.2 train no.147840  loss = 3.22587 avg_loss = 3.22865\n",
      "epoch no.2 train no.147850  loss = 4.78813 avg_loss = 3.27288\n",
      "epoch no.2 train no.147860  loss = 2.34126 avg_loss = 3.33763\n",
      "epoch no.2 train no.147870  loss = 2.05433 avg_loss = 3.27676\n",
      "epoch no.2 train no.147880  loss = 4.48879 avg_loss = 3.33728\n",
      "epoch no.2 train no.147890  loss = 4.88006 avg_loss = 3.33845\n",
      "epoch no.2 train no.147900  loss = 4.45152 avg_loss = 3.30870\n",
      "epoch no.2 train no.147910  loss = 4.46980 avg_loss = 3.28708\n",
      "epoch no.2 train no.147920  loss = 2.79080 avg_loss = 3.29938\n",
      "epoch no.2 train no.147930  loss = 3.22733 avg_loss = 3.27394\n",
      "epoch no.2 train no.147940  loss = 3.90211 avg_loss = 3.33482\n",
      "epoch no.2 train no.147950  loss = 1.43941 avg_loss = 3.31482\n",
      "epoch no.2 train no.147960  loss = 4.88888 avg_loss = 3.37294\n",
      "epoch no.2 train no.147970  loss = 2.42337 avg_loss = 3.35775\n",
      "epoch no.2 train no.147980  loss = 3.32294 avg_loss = 3.36107\n",
      "epoch no.2 train no.147990  loss = 2.80318 avg_loss = 3.35113\n",
      "epoch no.2 train no.148000  loss = 3.24212 avg_loss = 3.32600\n",
      "6\n",
      "to_tokens: ['▁가을', '▁2000', '년대', '대를', '▁주름', '한', '▁댄스', '</s>']\n",
      "추억의 90년대를 장식한 가요</s>\n",
      "epoch no.2 train no.148010  loss = 4.14196 avg_loss = 3.37362\n",
      "epoch no.2 train no.148020  loss = 3.53594 avg_loss = 3.36606\n",
      "epoch no.2 train no.148030  loss = 3.06497 avg_loss = 3.37368\n",
      "epoch no.2 train no.148040  loss = 3.97603 avg_loss = 3.35429\n",
      "epoch no.2 train no.148050  loss = 2.84412 avg_loss = 3.35486\n",
      "epoch no.2 train no.148060  loss = 2.99951 avg_loss = 3.40349\n",
      "epoch no.2 train no.148070  loss = 6.15598 avg_loss = 3.45778\n",
      "epoch no.2 train no.148080  loss = 6.40114 avg_loss = 3.53285\n",
      "epoch no.2 train no.148090  loss = 3.45214 avg_loss = 3.52064\n",
      "epoch no.2 train no.148100  loss = 3.03355 avg_loss = 3.48231\n",
      "epoch no.2 train no.148110  loss = 4.53310 avg_loss = 3.45496\n",
      "epoch no.2 train no.148120  loss = 3.72309 avg_loss = 3.46878\n",
      "epoch no.2 train no.148130  loss = 3.85436 avg_loss = 3.48847\n",
      "epoch no.2 train no.148140  loss = 3.72796 avg_loss = 3.45764\n",
      "epoch no.2 train no.148150  loss = 1.51007 avg_loss = 3.43206\n",
      "epoch no.2 train no.148160  loss = 3.62865 avg_loss = 3.41505\n",
      "epoch no.2 train no.148170  loss = 2.56249 avg_loss = 3.41189\n",
      "epoch no.2 train no.148180  loss = 3.30055 avg_loss = 3.43346\n",
      "epoch no.2 train no.148190  loss = 2.60333 avg_loss = 3.36721\n",
      "epoch no.2 train no.148200  loss = 2.12811 avg_loss = 3.33676\n",
      "epoch no.2 train no.148210  loss = 2.43476 avg_loss = 3.33646\n",
      "epoch no.2 train no.148220  loss = 4.42188 avg_loss = 3.35057\n",
      "epoch no.2 train no.148230  loss = 2.81725 avg_loss = 3.35731\n",
      "epoch no.2 train no.148240  loss = 2.75743 avg_loss = 3.37195\n",
      "epoch no.2 train no.148250  loss = 3.37542 avg_loss = 3.35620\n",
      "epoch no.2 train no.148260  loss = 2.41817 avg_loss = 3.26983\n",
      "epoch no.2 train no.148270  loss = 2.49997 avg_loss = 3.28028\n",
      "epoch no.2 train no.148280  loss = 2.50362 avg_loss = 3.24841\n",
      "epoch no.2 train no.148290  loss = 3.36698 avg_loss = 3.23681\n",
      "epoch no.2 train no.148300  loss = 3.53699 avg_loss = 3.27667\n",
      "epoch no.2 train no.148310  loss = 3.42762 avg_loss = 3.27888\n",
      "epoch no.2 train no.148320  loss = 3.66756 avg_loss = 3.33004\n",
      "epoch no.2 train no.148330  loss = 1.85884 avg_loss = 3.31543\n",
      "epoch no.2 train no.148340  loss = 3.51877 avg_loss = 3.32048\n",
      "epoch no.2 train no.148350  loss = 3.40240 avg_loss = 3.35464\n",
      "epoch no.2 train no.148360  loss = 2.68764 avg_loss = 3.31356\n",
      "epoch no.2 train no.148370  loss = 3.04587 avg_loss = 3.37149\n",
      "epoch no.2 train no.148380  loss = 2.52749 avg_loss = 3.34897\n",
      "epoch no.2 train no.148390  loss = 1.79598 avg_loss = 3.35829\n",
      "epoch no.2 train no.148400  loss = 3.17640 avg_loss = 3.33540\n",
      "epoch no.2 train no.148410  loss = 2.53088 avg_loss = 3.37825\n",
      "epoch no.2 train no.148420  loss = 3.00779 avg_loss = 3.35378\n",
      "epoch no.2 train no.148430  loss = 2.31296 avg_loss = 3.37570\n",
      "epoch no.2 train no.148440  loss = 4.15446 avg_loss = 3.40172\n",
      "epoch no.2 train no.148450  loss = 2.60824 avg_loss = 3.34898\n",
      "epoch no.2 train no.148460  loss = 2.81791 avg_loss = 3.32664\n",
      "epoch no.2 train no.148470  loss = 2.98771 avg_loss = 3.32970\n",
      "epoch no.2 train no.148480  loss = 2.32180 avg_loss = 3.35391\n",
      "epoch no.2 train no.148490  loss = 4.79221 avg_loss = 3.33838\n",
      "epoch no.2 train no.148500  loss = 2.78725 avg_loss = 3.31544\n",
      "epoch no.2 train no.148510  loss = 4.76522 avg_loss = 3.31642\n",
      "epoch no.2 train no.148520  loss = 3.38373 avg_loss = 3.34766\n",
      "epoch no.2 train no.148530  loss = 4.27387 avg_loss = 3.34790\n",
      "epoch no.2 train no.148540  loss = 3.07074 avg_loss = 3.42955\n",
      "epoch no.2 train no.148550  loss = 3.23631 avg_loss = 3.38893\n",
      "epoch no.2 train no.148560  loss = 3.24517 avg_loss = 3.44141\n",
      "epoch no.2 train no.148570  loss = 3.95705 avg_loss = 3.41998\n",
      "epoch no.2 train no.148580  loss = 2.85682 avg_loss = 3.35582\n",
      "epoch no.2 train no.148590  loss = 1.76997 avg_loss = 3.30552\n",
      "epoch no.2 train no.148600  loss = 3.25360 avg_loss = 3.28390\n",
      "epoch no.2 train no.148610  loss = 2.24095 avg_loss = 3.28392\n",
      "epoch no.2 train no.148620  loss = 3.91596 avg_loss = 3.28644\n",
      "epoch no.2 train no.148630  loss = 3.26760 avg_loss = 3.32560\n",
      "epoch no.2 train no.148640  loss = 1.88508 avg_loss = 3.34117\n",
      "epoch no.2 train no.148650  loss = 3.17414 avg_loss = 3.36739\n",
      "epoch no.2 train no.148660  loss = 4.06522 avg_loss = 3.44415\n",
      "epoch no.2 train no.148670  loss = 2.18219 avg_loss = 3.44703\n",
      "epoch no.2 train no.148680  loss = 2.64285 avg_loss = 3.41713\n",
      "epoch no.2 train no.148690  loss = 1.73241 avg_loss = 3.42114\n",
      "epoch no.2 train no.148700  loss = 2.85167 avg_loss = 3.40284\n",
      "epoch no.2 train no.148710  loss = 4.11147 avg_loss = 3.41852\n",
      "epoch no.2 train no.148720  loss = 2.72547 avg_loss = 3.37216\n",
      "epoch no.2 train no.148730  loss = 3.10789 avg_loss = 3.33515\n",
      "epoch no.2 train no.148740  loss = 3.24249 avg_loss = 3.38511\n",
      "epoch no.2 train no.148750  loss = 2.23659 avg_loss = 3.38878\n",
      "epoch no.2 train no.148760  loss = 5.11836 avg_loss = 3.40849\n",
      "epoch no.2 train no.148770  loss = 3.44061 avg_loss = 3.45886\n",
      "epoch no.2 train no.148780  loss = 2.90942 avg_loss = 3.42609\n",
      "epoch no.2 train no.148790  loss = 3.17853 avg_loss = 3.40329\n",
      "epoch no.2 train no.148800  loss = 2.69392 avg_loss = 3.48780\n",
      "epoch no.2 train no.148810  loss = 3.02958 avg_loss = 3.43868\n",
      "epoch no.2 train no.148820  loss = 2.88330 avg_loss = 3.46119\n",
      "epoch no.2 train no.148830  loss = 4.85650 avg_loss = 3.46361\n",
      "epoch no.2 train no.148840  loss = 3.58409 avg_loss = 3.43234\n",
      "epoch no.2 train no.148850  loss = 2.41880 avg_loss = 3.40797\n",
      "epoch no.2 train no.148860  loss = 2.60578 avg_loss = 3.37161\n",
      "epoch no.2 train no.148870  loss = 2.28728 avg_loss = 3.40482\n",
      "epoch no.2 train no.148880  loss = 2.96958 avg_loss = 3.39967\n",
      "epoch no.2 train no.148890  loss = 3.73872 avg_loss = 3.42660\n",
      "epoch no.2 train no.148900  loss = 3.67407 avg_loss = 3.40030\n",
      "epoch no.2 train no.148910  loss = 4.99808 avg_loss = 3.39007\n",
      "epoch no.2 train no.148920  loss = 2.49099 avg_loss = 3.37977\n",
      "epoch no.2 train no.148930  loss = 2.43810 avg_loss = 3.35539\n",
      "epoch no.2 train no.148940  loss = 1.30317 avg_loss = 3.26058\n",
      "epoch no.2 train no.148950  loss = 2.54024 avg_loss = 3.25017\n",
      "epoch no.2 train no.148960  loss = 2.68562 avg_loss = 3.23521\n",
      "epoch no.2 train no.148970  loss = 2.43650 avg_loss = 3.23904\n",
      "epoch no.2 train no.148980  loss = 2.59001 avg_loss = 3.22091\n",
      "epoch no.2 train no.148990  loss = 4.27906 avg_loss = 3.24259\n",
      "epoch no.2 train no.149000  loss = 4.03562 avg_loss = 3.30839\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '▁명', '곡', '</s>']\n",
      "추억의 90년대 가요 명곡</s>\n",
      "epoch no.2 train no.149010  loss = 2.59293 avg_loss = 3.26164\n",
      "epoch no.2 train no.149020  loss = 3.23649 avg_loss = 3.27493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.149030  loss = 3.73501 avg_loss = 3.30623\n",
      "epoch no.2 train no.149040  loss = 3.83827 avg_loss = 3.32137\n",
      "epoch no.2 train no.149050  loss = 3.30306 avg_loss = 3.29257\n",
      "epoch no.2 train no.149060  loss = 1.96990 avg_loss = 3.29825\n",
      "epoch no.2 train no.149070  loss = 2.75400 avg_loss = 3.31099\n",
      "epoch no.2 train no.149080  loss = 2.46364 avg_loss = 3.31521\n",
      "epoch no.2 train no.149090  loss = 1.82523 avg_loss = 3.28713\n",
      "epoch no.2 train no.149100  loss = 3.48368 avg_loss = 3.28211\n",
      "epoch no.2 train no.149110  loss = 1.92601 avg_loss = 3.28783\n",
      "epoch no.2 train no.149120  loss = 1.84087 avg_loss = 3.30055\n",
      "epoch no.2 train no.149130  loss = 2.32505 avg_loss = 3.28543\n",
      "epoch no.2 train no.149140  loss = 4.22437 avg_loss = 3.29559\n",
      "epoch no.2 train no.149150  loss = 3.36501 avg_loss = 3.31100\n",
      "epoch no.2 train no.149160  loss = 3.32659 avg_loss = 3.31434\n",
      "epoch no.2 train no.149170  loss = 3.52085 avg_loss = 3.31112\n",
      "epoch no.2 train no.149180  loss = 4.72594 avg_loss = 3.33593\n",
      "epoch no.2 train no.149190  loss = 3.12797 avg_loss = 3.33743\n",
      "epoch no.2 train no.149200  loss = 2.59246 avg_loss = 3.29739\n",
      "epoch no.2 train no.149210  loss = 3.60674 avg_loss = 3.36624\n",
      "epoch no.2 train no.149220  loss = 3.82105 avg_loss = 3.37005\n",
      "epoch no.2 train no.149230  loss = 4.10983 avg_loss = 3.34208\n",
      "epoch no.2 train no.149240  loss = 3.68827 avg_loss = 3.34100\n",
      "epoch no.2 train no.149250  loss = 3.88560 avg_loss = 3.31063\n",
      "epoch no.2 train no.149260  loss = 3.04782 avg_loss = 3.33034\n",
      "epoch no.2 train no.149270  loss = 3.11580 avg_loss = 3.32734\n",
      "epoch no.2 train no.149280  loss = 3.39890 avg_loss = 3.36929\n",
      "epoch no.2 train no.149290  loss = 1.83167 avg_loss = 3.38109\n",
      "epoch no.2 train no.149300  loss = 1.87933 avg_loss = 3.37718\n",
      "epoch no.2 train no.149310  loss = 3.11873 avg_loss = 3.33370\n",
      "epoch no.2 train no.149320  loss = 2.97838 avg_loss = 3.32413\n",
      "epoch no.2 train no.149330  loss = 2.46616 avg_loss = 3.31499\n",
      "epoch no.2 train no.149340  loss = 2.68876 avg_loss = 3.34697\n",
      "epoch no.2 train no.149350  loss = 3.23148 avg_loss = 3.33143\n",
      "epoch no.2 train no.149360  loss = 3.50752 avg_loss = 3.36508\n",
      "epoch no.2 train no.149370  loss = 2.80759 avg_loss = 3.35310\n",
      "epoch no.2 train no.149380  loss = 2.58370 avg_loss = 3.33783\n",
      "epoch no.2 train no.149390  loss = 3.46392 avg_loss = 3.40625\n",
      "epoch no.2 train no.149400  loss = 4.62763 avg_loss = 3.40196\n",
      "epoch no.2 train no.149410  loss = 2.72877 avg_loss = 3.40727\n",
      "epoch no.2 train no.149420  loss = 2.24599 avg_loss = 3.39956\n",
      "epoch no.2 train no.149430  loss = 4.09173 avg_loss = 3.42273\n",
      "epoch no.2 train no.149440  loss = 2.45815 avg_loss = 3.40936\n",
      "epoch no.2 train no.149450  loss = 2.27630 avg_loss = 3.38411\n",
      "epoch no.2 train no.149460  loss = 5.35789 avg_loss = 3.48079\n",
      "epoch no.2 train no.149470  loss = 2.92007 avg_loss = 3.45121\n",
      "epoch no.2 train no.149480  loss = 2.95054 avg_loss = 3.49016\n",
      "epoch no.2 train no.149490  loss = 2.99675 avg_loss = 3.48801\n",
      "epoch no.2 train no.149500  loss = 3.66940 avg_loss = 3.45588\n",
      "epoch no.2 train no.149510  loss = 4.91324 avg_loss = 3.52109\n",
      "epoch no.2 train no.149520  loss = 2.78375 avg_loss = 3.54875\n",
      "epoch no.2 train no.149530  loss = 3.96221 avg_loss = 3.51157\n",
      "epoch no.2 train no.149540  loss = 4.98688 avg_loss = 3.47895\n",
      "epoch no.2 train no.149550  loss = 5.36073 avg_loss = 3.49209\n",
      "epoch no.2 train no.149560  loss = 3.04245 avg_loss = 3.50769\n",
      "epoch no.2 train no.149570  loss = 1.81371 avg_loss = 3.48618\n",
      "epoch no.2 train no.149580  loss = 3.77105 avg_loss = 3.48815\n",
      "epoch no.2 train no.149590  loss = 4.00429 avg_loss = 3.55936\n",
      "epoch no.2 train no.149600  loss = 5.69844 avg_loss = 3.60889\n",
      "epoch no.2 train no.149610  loss = 2.79065 avg_loss = 3.56594\n",
      "epoch no.2 train no.149620  loss = 4.10863 avg_loss = 3.55434\n",
      "epoch no.2 train no.149630  loss = 2.30721 avg_loss = 3.50107\n",
      "epoch no.2 train no.149640  loss = 3.09341 avg_loss = 3.46006\n",
      "epoch no.2 train no.149650  loss = 3.37489 avg_loss = 3.44924\n",
      "epoch no.2 train no.149660  loss = 3.36499 avg_loss = 3.43603\n",
      "epoch no.2 train no.149670  loss = 3.78712 avg_loss = 3.42686\n",
      "epoch no.2 train no.149680  loss = 3.35264 avg_loss = 3.37607\n",
      "epoch no.2 train no.149690  loss = 3.07731 avg_loss = 3.34328\n",
      "epoch no.2 train no.149700  loss = 4.58499 avg_loss = 3.33341\n",
      "epoch no.2 train no.149710  loss = 2.94791 avg_loss = 3.31208\n",
      "epoch no.2 train no.149720  loss = 4.45319 avg_loss = 3.33532\n",
      "epoch no.2 train no.149730  loss = 4.06670 avg_loss = 3.32115\n",
      "epoch no.2 train no.149740  loss = 2.97231 avg_loss = 3.37869\n",
      "epoch no.2 train no.149750  loss = 3.42517 avg_loss = 3.39144\n",
      "epoch no.2 train no.149760  loss = 2.96240 avg_loss = 3.38098\n",
      "epoch no.2 train no.149770  loss = 3.13371 avg_loss = 3.35971\n",
      "epoch no.2 train no.149780  loss = 2.87455 avg_loss = 3.36696\n",
      "epoch no.2 train no.149790  loss = 3.06979 avg_loss = 3.37361\n",
      "epoch no.2 train no.149800  loss = 3.33098 avg_loss = 3.41917\n",
      "epoch no.2 train no.149810  loss = 3.67439 avg_loss = 3.42582\n",
      "epoch no.2 train no.149820  loss = 2.12561 avg_loss = 3.45271\n",
      "epoch no.2 train no.149830  loss = 2.87559 avg_loss = 3.47683\n",
      "epoch no.2 train no.149840  loss = 3.12321 avg_loss = 3.43448\n",
      "epoch no.2 train no.149850  loss = 2.04973 avg_loss = 3.47694\n",
      "epoch no.2 train no.149860  loss = 3.74471 avg_loss = 3.50171\n",
      "epoch no.2 train no.149870  loss = 3.72685 avg_loss = 3.53763\n",
      "epoch no.2 train no.149880  loss = 2.80022 avg_loss = 3.50100\n",
      "epoch no.2 train no.149890  loss = 2.93803 avg_loss = 3.52711\n",
      "epoch no.2 train no.149900  loss = 3.31111 avg_loss = 3.48700\n",
      "epoch no.2 train no.149910  loss = 3.90872 avg_loss = 3.48808\n",
      "epoch no.2 train no.149920  loss = 3.44185 avg_loss = 3.47338\n",
      "epoch no.2 train no.149930  loss = 3.55438 avg_loss = 3.49342\n",
      "epoch no.2 train no.149940  loss = 3.23740 avg_loss = 3.47655\n",
      "epoch no.2 train no.149950  loss = 2.07610 avg_loss = 3.52239\n",
      "epoch no.2 train no.149960  loss = 3.01249 avg_loss = 3.54531\n",
      "epoch no.2 train no.149970  loss = 3.59099 avg_loss = 3.51044\n",
      "epoch no.2 train no.149980  loss = 3.87706 avg_loss = 3.49277\n",
      "epoch no.2 train no.149990  loss = 3.58769 avg_loss = 3.48699\n",
      "epoch no.2 train no.150000  loss = 2.23371 avg_loss = 3.45524\n",
      "4\n",
      "to_tokens: ['▁가을', '▁2000', '팝', '송', '▁모음', '</s>']\n",
      "추억의 올드팝송 모음</s>\n",
      "epoch no.2 train no.150010  loss = 3.29406 avg_loss = 3.49362\n",
      "epoch no.2 train no.150020  loss = 2.20328 avg_loss = 3.47046\n",
      "epoch no.2 train no.150030  loss = 3.71627 avg_loss = 3.46638\n",
      "epoch no.2 train no.150040  loss = 3.69729 avg_loss = 3.46692\n",
      "epoch no.2 train no.150050  loss = 5.68945 avg_loss = 3.44981\n",
      "epoch no.2 train no.150060  loss = 4.95945 avg_loss = 3.44936\n",
      "epoch no.2 train no.150070  loss = 4.97885 avg_loss = 3.47359\n",
      "epoch no.2 train no.150080  loss = 3.69657 avg_loss = 3.46218\n",
      "epoch no.2 train no.150090  loss = 3.30274 avg_loss = 3.46656\n",
      "epoch no.2 train no.150100  loss = 3.84989 avg_loss = 3.47703\n",
      "epoch no.2 train no.150110  loss = 2.08588 avg_loss = 3.44883\n",
      "epoch no.2 train no.150120  loss = 4.78422 avg_loss = 3.45612\n",
      "epoch no.2 train no.150130  loss = 3.08989 avg_loss = 3.45858\n",
      "epoch no.2 train no.150140  loss = 3.83108 avg_loss = 3.42469\n",
      "epoch no.2 train no.150150  loss = 2.01344 avg_loss = 3.34270\n",
      "epoch no.2 train no.150160  loss = 4.70016 avg_loss = 3.36556\n",
      "epoch no.2 train no.150170  loss = 3.84088 avg_loss = 3.34891\n",
      "epoch no.2 train no.150180  loss = 3.74982 avg_loss = 3.30755\n",
      "epoch no.2 train no.150190  loss = 3.48066 avg_loss = 3.30879\n",
      "epoch no.2 train no.150200  loss = 4.54481 avg_loss = 3.27299\n",
      "epoch no.2 train no.150210  loss = 3.96734 avg_loss = 3.24961\n",
      "epoch no.2 train no.150220  loss = 4.87082 avg_loss = 3.26325\n",
      "epoch no.2 train no.150230  loss = 3.11376 avg_loss = 3.27962\n",
      "epoch no.2 train no.150240  loss = 3.81397 avg_loss = 3.21111\n",
      "epoch no.2 train no.150250  loss = 2.47927 avg_loss = 3.23934\n",
      "epoch no.2 train no.150260  loss = 2.15713 avg_loss = 3.22779\n",
      "epoch no.2 train no.150270  loss = 3.47886 avg_loss = 3.24057\n",
      "epoch no.2 train no.150280  loss = 2.19659 avg_loss = 3.23959\n",
      "epoch no.2 train no.150290  loss = 3.45084 avg_loss = 3.21500\n",
      "epoch no.2 train no.150300  loss = 4.59728 avg_loss = 3.30024\n",
      "epoch no.2 train no.150310  loss = 3.82972 avg_loss = 3.35164\n",
      "epoch no.2 train no.150320  loss = 2.57071 avg_loss = 3.32397\n",
      "epoch no.2 train no.150330  loss = 4.22966 avg_loss = 3.37328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.150340  loss = 2.35470 avg_loss = 3.38168\n",
      "epoch no.2 train no.150350  loss = 2.02626 avg_loss = 3.38258\n",
      "epoch no.2 train no.150360  loss = 2.11548 avg_loss = 3.36479\n",
      "epoch no.2 train no.150370  loss = 2.62371 avg_loss = 3.36049\n",
      "epoch no.2 train no.150380  loss = 3.91478 avg_loss = 3.40005\n",
      "epoch no.2 train no.150390  loss = 2.72383 avg_loss = 3.39271\n",
      "epoch no.2 train no.150400  loss = 3.35534 avg_loss = 3.39620\n",
      "epoch no.2 train no.150410  loss = 3.47187 avg_loss = 3.36663\n",
      "epoch no.2 train no.150420  loss = 1.98295 avg_loss = 3.34519\n",
      "epoch no.2 train no.150430  loss = 5.07516 avg_loss = 3.31738\n",
      "epoch no.2 train no.150440  loss = 2.60447 avg_loss = 3.33348\n",
      "epoch no.2 train no.150450  loss = 3.44057 avg_loss = 3.37012\n",
      "epoch no.2 train no.150460  loss = 7.86697 avg_loss = 3.44273\n",
      "epoch no.2 train no.150470  loss = 4.05044 avg_loss = 3.44277\n",
      "epoch no.2 train no.150480  loss = 3.27038 avg_loss = 3.41787\n",
      "epoch no.2 train no.150490  loss = 3.75335 avg_loss = 3.45277\n",
      "epoch no.2 train no.150500  loss = 2.11410 avg_loss = 3.45849\n",
      "epoch no.2 train no.150510  loss = 3.99660 avg_loss = 3.43437\n",
      "epoch no.2 train no.150520  loss = 4.32589 avg_loss = 3.48500\n",
      "epoch no.2 train no.150530  loss = 4.19376 avg_loss = 3.44259\n",
      "epoch no.2 train no.150540  loss = 4.80781 avg_loss = 3.43573\n",
      "epoch no.2 train no.150550  loss = 2.32498 avg_loss = 3.40690\n",
      "epoch no.2 train no.150560  loss = 4.33038 avg_loss = 3.41039\n",
      "epoch no.2 train no.150570  loss = 3.09134 avg_loss = 3.38374\n",
      "epoch no.2 train no.150580  loss = 3.01007 avg_loss = 3.40049\n",
      "epoch no.2 train no.150590  loss = 2.72525 avg_loss = 3.37300\n",
      "epoch no.2 train no.150600  loss = 4.47913 avg_loss = 3.42803\n",
      "epoch no.2 train no.150610  loss = 2.81207 avg_loss = 3.39313\n",
      "epoch no.2 train no.150620  loss = 2.98202 avg_loss = 3.43766\n",
      "epoch no.2 train no.150630  loss = 3.44975 avg_loss = 3.44677\n",
      "epoch no.2 train no.150640  loss = 4.77380 avg_loss = 3.36625\n",
      "epoch no.2 train no.150650  loss = 4.66009 avg_loss = 3.38739\n",
      "epoch no.2 train no.150660  loss = 2.43082 avg_loss = 3.40345\n",
      "epoch no.2 train no.150670  loss = 4.14862 avg_loss = 3.38823\n",
      "epoch no.2 train no.150680  loss = 4.06819 avg_loss = 3.40214\n",
      "epoch no.2 train no.150690  loss = 2.98257 avg_loss = 3.40111\n",
      "epoch no.2 train no.150700  loss = 3.22895 avg_loss = 3.38390\n",
      "epoch no.2 train no.150710  loss = 3.26628 avg_loss = 3.37250\n",
      "epoch no.2 train no.150720  loss = 2.36537 avg_loss = 3.32512\n",
      "epoch no.2 train no.150730  loss = 1.95498 avg_loss = 3.32122\n",
      "epoch no.2 train no.150740  loss = 2.26338 avg_loss = 3.31070\n",
      "epoch no.2 train no.150750  loss = 2.74458 avg_loss = 3.32031\n",
      "epoch no.2 train no.150760  loss = 3.51965 avg_loss = 3.28794\n",
      "epoch no.2 train no.150770  loss = 2.71062 avg_loss = 3.33974\n",
      "epoch no.2 train no.150780  loss = 2.76506 avg_loss = 3.32151\n",
      "epoch no.2 train no.150790  loss = 3.58505 avg_loss = 3.38092\n",
      "epoch no.2 train no.150800  loss = 5.36731 avg_loss = 3.39322\n",
      "epoch no.2 train no.150810  loss = 3.60746 avg_loss = 3.34857\n",
      "epoch no.2 train no.150820  loss = 4.64341 avg_loss = 3.31996\n",
      "epoch no.2 train no.150830  loss = 5.06752 avg_loss = 3.36674\n",
      "epoch no.2 train no.150840  loss = 2.93005 avg_loss = 3.37957\n",
      "epoch no.2 train no.150850  loss = 3.86782 avg_loss = 3.40909\n",
      "epoch no.2 train no.150860  loss = 2.41454 avg_loss = 3.46420\n",
      "epoch no.2 train no.150870  loss = 3.82844 avg_loss = 3.47172\n",
      "epoch no.2 train no.150880  loss = 3.41011 avg_loss = 3.39111\n",
      "epoch no.2 train no.150890  loss = 2.12781 avg_loss = 3.42707\n",
      "epoch no.2 train no.150900  loss = 3.39040 avg_loss = 3.41478\n",
      "epoch no.2 train no.150910  loss = 4.59501 avg_loss = 3.41819\n",
      "epoch no.2 train no.150920  loss = 3.95840 avg_loss = 3.42264\n",
      "epoch no.2 train no.150930  loss = 3.78626 avg_loss = 3.43030\n",
      "epoch no.2 train no.150940  loss = 2.58051 avg_loss = 3.41475\n",
      "epoch no.2 train no.150950  loss = 3.17514 avg_loss = 3.40441\n",
      "epoch no.2 train no.150960  loss = 4.86833 avg_loss = 3.44157\n",
      "epoch no.2 train no.150970  loss = 4.07179 avg_loss = 3.39658\n",
      "epoch no.2 train no.150980  loss = 2.85665 avg_loss = 3.37332\n",
      "epoch no.2 train no.150990  loss = 3.16456 avg_loss = 3.37733\n",
      "epoch no.2 train no.151000  loss = 1.87013 avg_loss = 3.39310\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '송', '▁모음', '▁모음', '</s>']\n",
      "추억의 팝송들 모음</s>\n",
      "epoch no.2 train no.151010  loss = 5.38541 avg_loss = 3.43010\n",
      "epoch no.2 train no.151020  loss = 3.57021 avg_loss = 3.40920\n",
      "epoch no.2 train no.151030  loss = 4.73554 avg_loss = 3.41605\n",
      "epoch no.2 train no.151040  loss = 3.10169 avg_loss = 3.42237\n",
      "epoch no.2 train no.151050  loss = 3.11992 avg_loss = 3.37739\n",
      "epoch no.2 train no.151060  loss = 4.74553 avg_loss = 3.38819\n",
      "epoch no.2 train no.151070  loss = 5.76577 avg_loss = 3.42428\n",
      "epoch no.2 train no.151080  loss = 2.34311 avg_loss = 3.40232\n",
      "epoch no.2 train no.151090  loss = 3.83907 avg_loss = 3.39394\n",
      "epoch no.2 train no.151100  loss = 4.04195 avg_loss = 3.40946\n",
      "epoch no.2 train no.151110  loss = 3.80440 avg_loss = 3.36845\n",
      "epoch no.2 train no.151120  loss = 2.97418 avg_loss = 3.35073\n",
      "epoch no.2 train no.151130  loss = 4.62855 avg_loss = 3.37670\n",
      "epoch no.2 train no.151140  loss = 4.26426 avg_loss = 3.37634\n",
      "epoch no.2 train no.151150  loss = 3.22821 avg_loss = 3.40063\n",
      "epoch no.2 train no.151160  loss = 2.65117 avg_loss = 3.39827\n",
      "epoch no.2 train no.151170  loss = 3.34608 avg_loss = 3.43694\n",
      "epoch no.2 train no.151180  loss = 2.82689 avg_loss = 3.45600\n",
      "epoch no.2 train no.151190  loss = 4.95533 avg_loss = 3.42964\n",
      "epoch no.2 train no.151200  loss = 5.45438 avg_loss = 3.44397\n",
      "epoch no.2 train no.151210  loss = 2.04095 avg_loss = 3.44822\n",
      "epoch no.2 train no.151220  loss = 5.14339 avg_loss = 3.40948\n",
      "epoch no.2 train no.151230  loss = 5.26300 avg_loss = 3.41106\n",
      "epoch no.2 train no.151240  loss = 2.12638 avg_loss = 3.36310\n",
      "epoch no.2 train no.151250  loss = 2.87127 avg_loss = 3.45382\n",
      "epoch no.2 train no.151260  loss = 4.04805 avg_loss = 3.49835\n",
      "epoch no.2 train no.151270  loss = 1.94698 avg_loss = 3.48053\n",
      "epoch no.2 train no.151280  loss = 3.09898 avg_loss = 3.48832\n",
      "epoch no.2 train no.151290  loss = 4.25735 avg_loss = 3.48373\n",
      "epoch no.2 train no.151300  loss = 3.53396 avg_loss = 3.48788\n",
      "epoch no.2 train no.151310  loss = 2.75367 avg_loss = 3.47948\n",
      "epoch no.2 train no.151320  loss = 3.45706 avg_loss = 3.50447\n",
      "epoch no.2 train no.151330  loss = 4.89625 avg_loss = 3.52892\n",
      "epoch no.2 train no.151340  loss = 4.48264 avg_loss = 3.54841\n",
      "epoch no.2 train no.151350  loss = 2.59805 avg_loss = 3.56383\n",
      "epoch no.2 train no.151360  loss = 2.84608 avg_loss = 3.56516\n",
      "epoch no.2 train no.151370  loss = 3.15285 avg_loss = 3.59851\n",
      "epoch no.2 train no.151380  loss = 2.87087 avg_loss = 3.55286\n",
      "epoch no.2 train no.151390  loss = 2.63317 avg_loss = 3.55422\n",
      "epoch no.2 train no.151400  loss = 2.34774 avg_loss = 3.58620\n",
      "epoch no.2 train no.151410  loss = 2.75718 avg_loss = 3.57011\n",
      "epoch no.2 train no.151420  loss = 2.82418 avg_loss = 3.51605\n",
      "epoch no.2 train no.151430  loss = 4.08277 avg_loss = 3.52690\n",
      "epoch no.2 train no.151440  loss = 2.83781 avg_loss = 3.48846\n",
      "epoch no.2 train no.151450  loss = 2.73160 avg_loss = 3.46728\n",
      "epoch no.2 train no.151460  loss = 2.79202 avg_loss = 3.49469\n",
      "epoch no.2 train no.151470  loss = 4.14916 avg_loss = 3.53394\n",
      "epoch no.2 train no.151480  loss = 3.32762 avg_loss = 3.55321\n",
      "epoch no.2 train no.151490  loss = 2.51541 avg_loss = 3.50538\n",
      "epoch no.2 train no.151500  loss = 4.17696 avg_loss = 3.49173\n",
      "epoch no.2 train no.151510  loss = 2.62431 avg_loss = 3.43914\n",
      "epoch no.2 train no.151520  loss = 1.97621 avg_loss = 3.39242\n",
      "epoch no.2 train no.151530  loss = 3.39909 avg_loss = 3.39979\n",
      "epoch no.2 train no.151540  loss = 5.48237 avg_loss = 3.47749\n",
      "epoch no.2 train no.151550  loss = 3.00060 avg_loss = 3.44252\n",
      "epoch no.2 train no.151560  loss = 3.16601 avg_loss = 3.45334\n",
      "epoch no.2 train no.151570  loss = 3.15213 avg_loss = 3.45394\n",
      "epoch no.2 train no.151580  loss = 3.76199 avg_loss = 3.44838\n",
      "epoch no.2 train no.151590  loss = 1.81980 avg_loss = 3.40985\n",
      "epoch no.2 train no.151600  loss = 2.75702 avg_loss = 3.41562\n",
      "epoch no.2 train no.151610  loss = 1.70592 avg_loss = 3.36692\n",
      "epoch no.2 train no.151620  loss = 4.78177 avg_loss = 3.35980\n",
      "epoch no.2 train no.151630  loss = 2.54030 avg_loss = 3.32711\n",
      "epoch no.2 train no.151640  loss = 3.63349 avg_loss = 3.29076\n",
      "epoch no.2 train no.151650  loss = 3.22394 avg_loss = 3.33123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.151660  loss = 2.66430 avg_loss = 3.29852\n",
      "epoch no.2 train no.151670  loss = 3.78896 avg_loss = 3.24836\n",
      "epoch no.2 train no.151680  loss = 2.60815 avg_loss = 3.27616\n",
      "epoch no.2 train no.151690  loss = 4.17419 avg_loss = 3.31104\n",
      "epoch no.2 train no.151700  loss = 3.26731 avg_loss = 3.27569\n",
      "epoch no.2 train no.151710  loss = 3.11993 avg_loss = 3.31716\n",
      "epoch no.2 train no.151720  loss = 3.91206 avg_loss = 3.29542\n",
      "epoch no.2 train no.151730  loss = 1.91432 avg_loss = 3.28531\n",
      "epoch no.2 train no.151740  loss = 3.41464 avg_loss = 3.29685\n",
      "epoch no.2 train no.151750  loss = 4.32890 avg_loss = 3.28297\n",
      "epoch no.2 train no.151760  loss = 2.01669 avg_loss = 3.26236\n",
      "epoch no.2 train no.151770  loss = 3.53812 avg_loss = 3.31230\n",
      "epoch no.2 train no.151780  loss = 2.55046 avg_loss = 3.31475\n",
      "epoch no.2 train no.151790  loss = 4.28384 avg_loss = 3.32268\n",
      "epoch no.2 train no.151800  loss = 3.32879 avg_loss = 3.36600\n",
      "epoch no.2 train no.151810  loss = 1.52452 avg_loss = 3.32971\n",
      "epoch no.2 train no.151820  loss = 1.48369 avg_loss = 3.28219\n",
      "epoch no.2 train no.151830  loss = 3.03288 avg_loss = 3.28244\n",
      "epoch no.2 train no.151840  loss = 2.05093 avg_loss = 3.28138\n",
      "epoch no.2 train no.151850  loss = 4.64345 avg_loss = 3.35091\n",
      "epoch no.2 train no.151860  loss = 2.12272 avg_loss = 3.34712\n",
      "epoch no.2 train no.151870  loss = 4.94434 avg_loss = 3.40850\n",
      "epoch no.2 train no.151880  loss = 4.75565 avg_loss = 3.44729\n",
      "epoch no.2 train no.151890  loss = 2.26652 avg_loss = 3.42047\n",
      "epoch no.2 train no.151900  loss = 5.08731 avg_loss = 3.46304\n",
      "epoch no.2 train no.151910  loss = 4.22859 avg_loss = 3.48503\n",
      "epoch no.2 train no.151920  loss = 5.40987 avg_loss = 3.51561\n",
      "epoch no.2 train no.151930  loss = 4.11334 avg_loss = 3.48337\n",
      "epoch no.2 train no.151940  loss = 1.95674 avg_loss = 3.49558\n",
      "epoch no.2 train no.151950  loss = 2.42376 avg_loss = 3.52797\n",
      "epoch no.2 train no.151960  loss = 3.87253 avg_loss = 3.56695\n",
      "epoch no.2 train no.151970  loss = 4.35205 avg_loss = 3.56464\n",
      "epoch no.2 train no.151980  loss = 3.42642 avg_loss = 3.53725\n",
      "epoch no.2 train no.151990  loss = 4.28046 avg_loss = 3.54423\n",
      "epoch no.2 train no.152000  loss = 2.70999 avg_loss = 3.49000\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.152010  loss = 3.89309 avg_loss = 3.48584\n",
      "epoch no.2 train no.152020  loss = 2.27015 avg_loss = 3.42739\n",
      "epoch no.2 train no.152030  loss = 2.13701 avg_loss = 3.37951\n",
      "epoch no.2 train no.152040  loss = 3.11857 avg_loss = 3.43458\n",
      "epoch no.2 train no.152050  loss = 3.40704 avg_loss = 3.40821\n",
      "epoch no.2 train no.152060  loss = 3.39252 avg_loss = 3.43884\n",
      "epoch no.2 train no.152070  loss = 4.83345 avg_loss = 3.45100\n",
      "epoch no.2 train no.152080  loss = 3.49829 avg_loss = 3.41566\n",
      "epoch no.2 train no.152090  loss = 4.11552 avg_loss = 3.39438\n",
      "epoch no.2 train no.152100  loss = 3.65468 avg_loss = 3.39634\n",
      "epoch no.2 train no.152110  loss = 3.87166 avg_loss = 3.41648\n",
      "epoch no.2 train no.152120  loss = 1.81768 avg_loss = 3.39295\n",
      "epoch no.2 train no.152130  loss = 4.60758 avg_loss = 3.38359\n",
      "epoch no.2 train no.152140  loss = 3.45863 avg_loss = 3.37938\n",
      "epoch no.2 train no.152150  loss = 3.21154 avg_loss = 3.37187\n",
      "epoch no.2 train no.152160  loss = 2.92646 avg_loss = 3.33384\n",
      "epoch no.2 train no.152170  loss = 3.76070 avg_loss = 3.33234\n",
      "epoch no.2 train no.152180  loss = 2.38639 avg_loss = 3.30767\n",
      "epoch no.2 train no.152190  loss = 4.10092 avg_loss = 3.34384\n",
      "epoch no.2 train no.152200  loss = 5.25932 avg_loss = 3.35999\n",
      "epoch no.2 train no.152210  loss = 3.59210 avg_loss = 3.34267\n",
      "epoch no.2 train no.152220  loss = 2.48609 avg_loss = 3.31503\n",
      "epoch no.2 train no.152230  loss = 3.68094 avg_loss = 3.31979\n",
      "epoch no.2 train no.152240  loss = 3.17943 avg_loss = 3.31290\n",
      "epoch no.2 train no.152250  loss = 2.60129 avg_loss = 3.32575\n",
      "epoch no.2 train no.152260  loss = 4.28398 avg_loss = 3.28062\n",
      "epoch no.2 train no.152270  loss = 2.57560 avg_loss = 3.24984\n",
      "epoch no.2 train no.152280  loss = 2.66089 avg_loss = 3.29529\n",
      "epoch no.2 train no.152290  loss = 4.79420 avg_loss = 3.25929\n",
      "epoch no.2 train no.152300  loss = 4.15742 avg_loss = 3.28539\n",
      "epoch no.2 train no.152310  loss = 4.33981 avg_loss = 3.33794\n",
      "epoch no.2 train no.152320  loss = 2.91532 avg_loss = 3.35934\n",
      "epoch no.2 train no.152330  loss = 3.15333 avg_loss = 3.36634\n",
      "epoch no.2 train no.152340  loss = 4.80808 avg_loss = 3.35586\n",
      "epoch no.2 train no.152350  loss = 1.93985 avg_loss = 3.34524\n",
      "epoch no.2 train no.152360  loss = 3.91519 avg_loss = 3.34323\n",
      "epoch no.2 train no.152370  loss = 3.38937 avg_loss = 3.34982\n",
      "epoch no.2 train no.152380  loss = 3.84243 avg_loss = 3.28198\n",
      "epoch no.2 train no.152390  loss = 3.24905 avg_loss = 3.25417\n",
      "epoch no.2 train no.152400  loss = 4.51440 avg_loss = 3.27796\n",
      "epoch no.2 train no.152410  loss = 2.37770 avg_loss = 3.30348\n",
      "epoch no.2 train no.152420  loss = 2.62364 avg_loss = 3.25716\n",
      "epoch no.2 train no.152430  loss = 3.20741 avg_loss = 3.25174\n",
      "epoch no.2 train no.152440  loss = 3.31704 avg_loss = 3.26460\n",
      "epoch no.2 train no.152450  loss = 4.76416 avg_loss = 3.24012\n",
      "epoch no.2 train no.152460  loss = 2.58683 avg_loss = 3.24419\n",
      "epoch no.2 train no.152470  loss = 2.97475 avg_loss = 3.29013\n",
      "epoch no.2 train no.152480  loss = 2.41377 avg_loss = 3.26733\n",
      "epoch no.2 train no.152490  loss = 3.79553 avg_loss = 3.26983\n",
      "epoch no.2 train no.152500  loss = 4.87454 avg_loss = 3.29087\n",
      "epoch no.2 train no.152510  loss = 3.68708 avg_loss = 3.31984\n",
      "epoch no.2 train no.152520  loss = 3.82366 avg_loss = 3.30672\n",
      "epoch no.2 train no.152530  loss = 2.12291 avg_loss = 3.34446\n",
      "epoch no.2 train no.152540  loss = 2.47639 avg_loss = 3.34362\n",
      "epoch no.2 train no.152550  loss = 4.20522 avg_loss = 3.36347\n",
      "epoch no.2 train no.152560  loss = 4.51656 avg_loss = 3.40092\n",
      "epoch no.2 train no.152570  loss = 5.01367 avg_loss = 3.41021\n",
      "epoch no.2 train no.152580  loss = 2.99262 avg_loss = 3.41096\n",
      "epoch no.2 train no.152590  loss = 2.71751 avg_loss = 3.38927\n",
      "epoch no.2 train no.152600  loss = 2.95773 avg_loss = 3.36450\n",
      "epoch no.2 train no.152610  loss = 1.90655 avg_loss = 3.34706\n",
      "epoch no.2 train no.152620  loss = 3.08734 avg_loss = 3.31914\n",
      "epoch no.2 train no.152630  loss = 2.32878 avg_loss = 3.31676\n",
      "epoch no.2 train no.152640  loss = 2.09364 avg_loss = 3.29273\n",
      "epoch no.2 train no.152650  loss = 4.61184 avg_loss = 3.30851\n",
      "epoch no.2 train no.152660  loss = 3.88148 avg_loss = 3.28892\n",
      "epoch no.2 train no.152670  loss = 2.86563 avg_loss = 3.32324\n",
      "epoch no.2 train no.152680  loss = 2.57571 avg_loss = 3.29978\n",
      "epoch no.2 train no.152690  loss = 1.54461 avg_loss = 3.27765\n",
      "epoch no.2 train no.152700  loss = 2.64476 avg_loss = 3.29464\n",
      "epoch no.2 train no.152710  loss = 4.39716 avg_loss = 3.33168\n",
      "epoch no.2 train no.152720  loss = 3.31059 avg_loss = 3.33934\n",
      "epoch no.2 train no.152730  loss = 3.41034 avg_loss = 3.35381\n",
      "epoch no.2 train no.152740  loss = 4.02267 avg_loss = 3.34327\n",
      "epoch no.2 train no.152750  loss = 3.59208 avg_loss = 3.40894\n",
      "epoch no.2 train no.152760  loss = 2.93969 avg_loss = 3.43435\n",
      "epoch no.2 train no.152770  loss = 3.83526 avg_loss = 3.46876\n",
      "epoch no.2 train no.152780  loss = 4.13465 avg_loss = 3.51350\n",
      "epoch no.2 train no.152790  loss = 1.86478 avg_loss = 3.52814\n",
      "epoch no.2 train no.152800  loss = 4.39358 avg_loss = 3.50695\n",
      "epoch no.2 train no.152810  loss = 3.47510 avg_loss = 3.53001\n",
      "epoch no.2 train no.152820  loss = 3.25059 avg_loss = 3.47523\n",
      "epoch no.2 train no.152830  loss = 2.11206 avg_loss = 3.41802\n",
      "epoch no.2 train no.152840  loss = 3.46453 avg_loss = 3.40427\n",
      "epoch no.2 train no.152850  loss = 3.39837 avg_loss = 3.42902\n",
      "epoch no.2 train no.152860  loss = 3.24448 avg_loss = 3.40974\n",
      "epoch no.2 train no.152870  loss = 3.17475 avg_loss = 3.41372\n",
      "epoch no.2 train no.152880  loss = 2.82386 avg_loss = 3.40065\n",
      "epoch no.2 train no.152890  loss = 2.36188 avg_loss = 3.38074\n",
      "epoch no.2 train no.152900  loss = 2.95925 avg_loss = 3.36905\n",
      "epoch no.2 train no.152910  loss = 4.57777 avg_loss = 3.36815\n",
      "epoch no.2 train no.152920  loss = 3.41085 avg_loss = 3.34964\n",
      "epoch no.2 train no.152930  loss = 3.28671 avg_loss = 3.34962\n",
      "epoch no.2 train no.152940  loss = 2.61969 avg_loss = 3.41116\n",
      "epoch no.2 train no.152950  loss = 2.68374 avg_loss = 3.38585\n",
      "epoch no.2 train no.152960  loss = 2.18825 avg_loss = 3.40938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.152970  loss = 3.47550 avg_loss = 3.36933\n",
      "epoch no.2 train no.152980  loss = 1.64601 avg_loss = 3.40570\n",
      "epoch no.2 train no.152990  loss = 2.57822 avg_loss = 3.41128\n",
      "epoch no.2 train no.153000  loss = 3.97083 avg_loss = 3.38078\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.153010  loss = 2.89799 avg_loss = 3.37126\n",
      "epoch no.2 train no.153020  loss = 6.17362 avg_loss = 3.39087\n",
      "epoch no.2 train no.153030  loss = 2.84189 avg_loss = 3.37523\n",
      "epoch no.2 train no.153040  loss = 4.79064 avg_loss = 3.42108\n",
      "epoch no.2 train no.153050  loss = 3.29182 avg_loss = 3.40069\n",
      "epoch no.2 train no.153060  loss = 3.14675 avg_loss = 3.38768\n",
      "epoch no.2 train no.153070  loss = 3.62073 avg_loss = 3.40675\n",
      "epoch no.2 train no.153080  loss = 2.87452 avg_loss = 3.40040\n",
      "epoch no.2 train no.153090  loss = 2.58504 avg_loss = 3.37304\n",
      "epoch no.2 train no.153100  loss = 1.92571 avg_loss = 3.39996\n",
      "epoch no.2 train no.153110  loss = 2.36708 avg_loss = 3.39158\n",
      "epoch no.2 train no.153120  loss = 3.32410 avg_loss = 3.41197\n",
      "epoch no.2 train no.153130  loss = 2.55857 avg_loss = 3.36394\n",
      "epoch no.2 train no.153140  loss = 4.00221 avg_loss = 3.33830\n",
      "epoch no.2 train no.153150  loss = 3.67035 avg_loss = 3.30560\n",
      "epoch no.2 train no.153160  loss = 3.66383 avg_loss = 3.37212\n",
      "epoch no.2 train no.153170  loss = 7.06331 avg_loss = 3.41478\n",
      "epoch no.2 train no.153180  loss = 1.68755 avg_loss = 3.40788\n",
      "epoch no.2 train no.153190  loss = 3.14685 avg_loss = 3.43613\n",
      "epoch no.2 train no.153200  loss = 3.07594 avg_loss = 3.42030\n",
      "epoch no.2 train no.153210  loss = 2.51975 avg_loss = 3.41974\n",
      "epoch no.2 train no.153220  loss = 4.69550 avg_loss = 3.45095\n",
      "epoch no.2 train no.153230  loss = 3.31025 avg_loss = 3.45772\n",
      "epoch no.2 train no.153240  loss = 3.29291 avg_loss = 3.42682\n",
      "epoch no.2 train no.153250  loss = 4.47211 avg_loss = 3.42428\n",
      "epoch no.2 train no.153260  loss = 4.18381 avg_loss = 3.43638\n",
      "epoch no.2 train no.153270  loss = 4.65445 avg_loss = 3.41073\n",
      "epoch no.2 train no.153280  loss = 3.84496 avg_loss = 3.37352\n",
      "epoch no.2 train no.153290  loss = 3.68049 avg_loss = 3.37354\n",
      "epoch no.2 train no.153300  loss = 3.70169 avg_loss = 3.37826\n",
      "epoch no.2 train no.153310  loss = 3.21636 avg_loss = 3.40645\n",
      "epoch no.2 train no.153320  loss = 2.34192 avg_loss = 3.37199\n",
      "epoch no.2 train no.153330  loss = 3.15497 avg_loss = 3.37740\n",
      "epoch no.2 train no.153340  loss = 1.92171 avg_loss = 3.33314\n",
      "epoch no.2 train no.153350  loss = 4.20157 avg_loss = 3.32543\n",
      "epoch no.2 train no.153360  loss = 2.60774 avg_loss = 3.33131\n",
      "epoch no.2 train no.153370  loss = 4.54425 avg_loss = 3.32659\n",
      "epoch no.2 train no.153380  loss = 2.93083 avg_loss = 3.35312\n",
      "epoch no.2 train no.153390  loss = 3.90357 avg_loss = 3.38346\n",
      "epoch no.2 train no.153400  loss = 3.29833 avg_loss = 3.42282\n",
      "epoch no.2 train no.153410  loss = 4.34246 avg_loss = 3.47113\n",
      "epoch no.2 train no.153420  loss = 3.76956 avg_loss = 3.51287\n",
      "epoch no.2 train no.153430  loss = 3.24931 avg_loss = 3.49787\n",
      "epoch no.2 train no.153440  loss = 3.72301 avg_loss = 3.48754\n",
      "epoch no.2 train no.153450  loss = 3.20863 avg_loss = 3.47586\n",
      "epoch no.2 train no.153460  loss = 2.53813 avg_loss = 3.51051\n",
      "epoch no.2 train no.153470  loss = 4.29380 avg_loss = 3.52105\n",
      "epoch no.2 train no.153480  loss = 3.40341 avg_loss = 3.50341\n",
      "epoch no.2 train no.153490  loss = 5.31841 avg_loss = 3.48509\n",
      "epoch no.2 train no.153500  loss = 3.65573 avg_loss = 3.47052\n",
      "epoch no.2 train no.153510  loss = 3.46165 avg_loss = 3.46717\n",
      "epoch no.2 train no.153520  loss = 4.11637 avg_loss = 3.42893\n",
      "epoch no.2 train no.153530  loss = 1.88675 avg_loss = 3.44682\n",
      "epoch no.2 train no.153540  loss = 2.84129 avg_loss = 3.42249\n",
      "epoch no.2 train no.153550  loss = 3.68780 avg_loss = 3.46940\n",
      "epoch no.2 train no.153560  loss = 3.60562 avg_loss = 3.43304\n",
      "epoch no.2 train no.153570  loss = 4.77776 avg_loss = 3.46372\n",
      "epoch no.2 train no.153580  loss = 3.11114 avg_loss = 3.46853\n",
      "epoch no.2 train no.153590  loss = 4.06259 avg_loss = 3.46182\n",
      "epoch no.2 train no.153600  loss = 3.81220 avg_loss = 3.46558\n",
      "epoch no.2 train no.153610  loss = 2.64619 avg_loss = 3.44166\n",
      "epoch no.2 train no.153620  loss = 5.06386 avg_loss = 3.44593\n",
      "epoch no.2 train no.153630  loss = 3.50401 avg_loss = 3.48594\n",
      "epoch no.2 train no.153640  loss = 3.76414 avg_loss = 3.47927\n",
      "epoch no.2 train no.153650  loss = 2.91397 avg_loss = 3.41391\n",
      "epoch no.2 train no.153660  loss = 4.08073 avg_loss = 3.41124\n",
      "epoch no.2 train no.153670  loss = 6.69381 avg_loss = 3.41792\n",
      "epoch no.2 train no.153680  loss = 2.70238 avg_loss = 3.44142\n",
      "epoch no.2 train no.153690  loss = 3.44752 avg_loss = 3.45728\n",
      "epoch no.2 train no.153700  loss = 3.57314 avg_loss = 3.41645\n",
      "epoch no.2 train no.153710  loss = 1.85363 avg_loss = 3.42668\n",
      "epoch no.2 train no.153720  loss = 3.31424 avg_loss = 3.39855\n",
      "epoch no.2 train no.153730  loss = 3.30251 avg_loss = 3.39331\n",
      "epoch no.2 train no.153740  loss = 2.80232 avg_loss = 3.39901\n",
      "epoch no.2 train no.153750  loss = 4.42169 avg_loss = 3.44174\n",
      "epoch no.2 train no.153760  loss = 3.55106 avg_loss = 3.39554\n",
      "epoch no.2 train no.153770  loss = 2.96467 avg_loss = 3.43591\n",
      "epoch no.2 train no.153780  loss = 2.28054 avg_loss = 3.45498\n",
      "epoch no.2 train no.153790  loss = 4.07490 avg_loss = 3.43992\n",
      "epoch no.2 train no.153800  loss = 2.72172 avg_loss = 3.40770\n",
      "epoch no.2 train no.153810  loss = 2.81680 avg_loss = 3.38952\n",
      "epoch no.2 train no.153820  loss = 2.51617 avg_loss = 3.35131\n",
      "epoch no.2 train no.153830  loss = 1.99465 avg_loss = 3.36562\n",
      "epoch no.2 train no.153840  loss = 1.72124 avg_loss = 3.31298\n",
      "epoch no.2 train no.153850  loss = 4.87236 avg_loss = 3.34110\n",
      "epoch no.2 train no.153860  loss = 3.05280 avg_loss = 3.36665\n",
      "epoch no.2 train no.153870  loss = 3.62696 avg_loss = 3.40243\n",
      "epoch no.2 train no.153880  loss = 4.78502 avg_loss = 3.44523\n",
      "epoch no.2 train no.153890  loss = 2.37504 avg_loss = 3.44850\n",
      "epoch no.2 train no.153900  loss = 4.11999 avg_loss = 3.45640\n",
      "epoch no.2 train no.153910  loss = 3.49580 avg_loss = 3.51661\n",
      "epoch no.2 train no.153920  loss = 4.68718 avg_loss = 3.48104\n",
      "epoch no.2 train no.153930  loss = 4.86046 avg_loss = 3.51639\n",
      "epoch no.2 train no.153940  loss = 4.97280 avg_loss = 3.56278\n",
      "epoch no.2 train no.153950  loss = 3.76494 avg_loss = 3.49764\n",
      "epoch no.2 train no.153960  loss = 3.84204 avg_loss = 3.47340\n",
      "epoch no.2 train no.153970  loss = 4.01089 avg_loss = 3.46892\n",
      "epoch no.2 train no.153980  loss = 5.57530 avg_loss = 3.48547\n",
      "epoch no.2 train no.153990  loss = 3.07687 avg_loss = 3.45502\n",
      "epoch no.2 train no.154000  loss = 2.36696 avg_loss = 3.45539\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁시절', '▁그', '들', '</s>']\n",
      "추억의 그시절 노래들</s>\n",
      "epoch no.2 train no.154010  loss = 2.03353 avg_loss = 3.43844\n",
      "epoch no.2 train no.154020  loss = 3.71952 avg_loss = 3.48826\n",
      "epoch no.2 train no.154030  loss = 2.19297 avg_loss = 3.50912\n",
      "epoch no.2 train no.154040  loss = 2.77340 avg_loss = 3.47681\n",
      "epoch no.2 train no.154050  loss = 3.46140 avg_loss = 3.41935\n",
      "epoch no.2 train no.154060  loss = 2.63359 avg_loss = 3.41027\n",
      "epoch no.2 train no.154070  loss = 2.59196 avg_loss = 3.38461\n",
      "epoch no.2 train no.154080  loss = 5.25534 avg_loss = 3.42780\n",
      "epoch no.2 train no.154090  loss = 1.92994 avg_loss = 3.42190\n",
      "epoch no.2 train no.154100  loss = 2.30389 avg_loss = 3.33756\n",
      "epoch no.2 train no.154110  loss = 3.86472 avg_loss = 3.34274\n",
      "epoch no.2 train no.154120  loss = 3.03401 avg_loss = 3.37162\n",
      "epoch no.2 train no.154130  loss = 2.77036 avg_loss = 3.33352\n",
      "epoch no.2 train no.154140  loss = 4.38330 avg_loss = 3.33432\n",
      "epoch no.2 train no.154150  loss = 5.36966 avg_loss = 3.36170\n",
      "epoch no.2 train no.154160  loss = 2.99262 avg_loss = 3.35743\n",
      "epoch no.2 train no.154170  loss = 5.74123 avg_loss = 3.42040\n",
      "epoch no.2 train no.154180  loss = 2.44179 avg_loss = 3.36433\n",
      "epoch no.2 train no.154190  loss = 2.63052 avg_loss = 3.35015\n",
      "epoch no.2 train no.154200  loss = 2.63228 avg_loss = 3.36494\n",
      "epoch no.2 train no.154210  loss = 3.34603 avg_loss = 3.35362\n",
      "epoch no.2 train no.154220  loss = 2.57056 avg_loss = 3.32773\n",
      "epoch no.2 train no.154230  loss = 2.61186 avg_loss = 3.32995\n",
      "epoch no.2 train no.154240  loss = 1.92717 avg_loss = 3.36793\n",
      "epoch no.2 train no.154250  loss = 1.95605 avg_loss = 3.36614\n",
      "epoch no.2 train no.154260  loss = 3.01598 avg_loss = 3.33603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.154270  loss = 2.83377 avg_loss = 3.34806\n",
      "epoch no.2 train no.154280  loss = 5.07972 avg_loss = 3.30531\n",
      "epoch no.2 train no.154290  loss = 2.08408 avg_loss = 3.34228\n",
      "epoch no.2 train no.154300  loss = 1.95084 avg_loss = 3.30951\n",
      "epoch no.2 train no.154310  loss = 3.15787 avg_loss = 3.37583\n",
      "epoch no.2 train no.154320  loss = 5.15559 avg_loss = 3.38311\n",
      "epoch no.2 train no.154330  loss = 3.20912 avg_loss = 3.38904\n",
      "epoch no.2 train no.154340  loss = 4.49378 avg_loss = 3.38773\n",
      "epoch no.2 train no.154350  loss = 5.37220 avg_loss = 3.37623\n",
      "epoch no.2 train no.154360  loss = 3.76861 avg_loss = 3.37916\n",
      "epoch no.2 train no.154370  loss = 2.82605 avg_loss = 3.32499\n",
      "epoch no.2 train no.154380  loss = 2.73704 avg_loss = 3.30844\n",
      "epoch no.2 train no.154390  loss = 3.41606 avg_loss = 3.30201\n",
      "epoch no.2 train no.154400  loss = 3.62399 avg_loss = 3.36258\n",
      "epoch no.2 train no.154410  loss = 6.57223 avg_loss = 3.40121\n",
      "epoch no.2 train no.154420  loss = 2.67964 avg_loss = 3.41343\n",
      "epoch no.2 train no.154430  loss = 2.84948 avg_loss = 3.39242\n",
      "epoch no.2 train no.154440  loss = 4.92177 avg_loss = 3.44067\n",
      "epoch no.2 train no.154450  loss = 4.72876 avg_loss = 3.47343\n",
      "epoch no.2 train no.154460  loss = 4.69751 avg_loss = 3.48166\n",
      "epoch no.2 train no.154470  loss = 3.65309 avg_loss = 3.47575\n",
      "epoch no.2 train no.154480  loss = 4.94527 avg_loss = 3.51308\n",
      "epoch no.2 train no.154490  loss = 2.81817 avg_loss = 3.52609\n",
      "epoch no.2 train no.154500  loss = 3.72448 avg_loss = 3.52205\n",
      "epoch no.2 train no.154510  loss = 3.85460 avg_loss = 3.54547\n",
      "epoch no.2 train no.154520  loss = 1.88958 avg_loss = 3.53827\n",
      "epoch no.2 train no.154530  loss = 3.60866 avg_loss = 3.53236\n",
      "epoch no.2 train no.154540  loss = 1.93771 avg_loss = 3.53636\n",
      "epoch no.2 train no.154550  loss = 4.09902 avg_loss = 3.57386\n",
      "epoch no.2 train no.154560  loss = 2.30946 avg_loss = 3.51006\n",
      "epoch no.2 train no.154570  loss = 3.92552 avg_loss = 3.54075\n",
      "epoch no.2 train no.154580  loss = 3.28557 avg_loss = 3.51913\n",
      "epoch no.2 train no.154590  loss = 2.78920 avg_loss = 3.49143\n",
      "epoch no.2 train no.154600  loss = 3.09414 avg_loss = 3.47131\n",
      "epoch no.2 train no.154610  loss = 2.91270 avg_loss = 3.43054\n",
      "epoch no.2 train no.154620  loss = 2.38958 avg_loss = 3.42580\n",
      "epoch no.2 train no.154630  loss = 2.19320 avg_loss = 3.46067\n",
      "epoch no.2 train no.154640  loss = 4.62251 avg_loss = 3.47233\n",
      "epoch no.2 train no.154650  loss = 3.65366 avg_loss = 3.45973\n",
      "epoch no.2 train no.154660  loss = 3.81534 avg_loss = 3.45193\n",
      "epoch no.2 train no.154670  loss = 2.33862 avg_loss = 3.44892\n",
      "epoch no.2 train no.154680  loss = 4.09260 avg_loss = 3.43232\n",
      "epoch no.2 train no.154690  loss = 4.16292 avg_loss = 3.44446\n",
      "epoch no.2 train no.154700  loss = 5.94482 avg_loss = 3.43229\n",
      "epoch no.2 train no.154710  loss = 2.90165 avg_loss = 3.46029\n",
      "epoch no.2 train no.154720  loss = 4.94826 avg_loss = 3.50272\n",
      "epoch no.2 train no.154730  loss = 3.31955 avg_loss = 3.46328\n",
      "epoch no.2 train no.154740  loss = 2.68891 avg_loss = 3.46881\n",
      "epoch no.2 train no.154750  loss = 3.26005 avg_loss = 3.46477\n",
      "epoch no.2 train no.154760  loss = 2.55093 avg_loss = 3.44181\n",
      "epoch no.2 train no.154770  loss = 3.10922 avg_loss = 3.43207\n",
      "epoch no.2 train no.154780  loss = 3.00217 avg_loss = 3.39247\n",
      "epoch no.2 train no.154790  loss = 4.39271 avg_loss = 3.43723\n",
      "epoch no.2 train no.154800  loss = 2.62754 avg_loss = 3.44255\n",
      "epoch no.2 train no.154810  loss = 2.87938 avg_loss = 3.47879\n",
      "epoch no.2 train no.154820  loss = 4.43201 avg_loss = 3.47968\n",
      "epoch no.2 train no.154830  loss = 2.98341 avg_loss = 3.47667\n",
      "epoch no.2 train no.154840  loss = 3.71052 avg_loss = 3.48434\n",
      "epoch no.2 train no.154850  loss = 4.33298 avg_loss = 3.51405\n",
      "epoch no.2 train no.154860  loss = 3.70297 avg_loss = 3.50218\n",
      "epoch no.2 train no.154870  loss = 3.20612 avg_loss = 3.49233\n",
      "epoch no.2 train no.154880  loss = 2.48102 avg_loss = 3.51329\n",
      "epoch no.2 train no.154890  loss = 2.95529 avg_loss = 3.49031\n",
      "epoch no.2 train no.154900  loss = 3.66615 avg_loss = 3.47052\n",
      "epoch no.2 train no.154910  loss = 2.38654 avg_loss = 3.45332\n",
      "epoch no.2 train no.154920  loss = 4.75422 avg_loss = 3.47712\n",
      "epoch no.2 train no.154930  loss = 2.31872 avg_loss = 3.48164\n",
      "epoch no.2 train no.154940  loss = 2.07294 avg_loss = 3.46521\n",
      "epoch no.2 train no.154950  loss = 2.53072 avg_loss = 3.47558\n",
      "epoch no.2 train no.154960  loss = 3.83876 avg_loss = 3.46827\n",
      "epoch no.2 train no.154970  loss = 4.04697 avg_loss = 3.48137\n",
      "epoch no.2 train no.154980  loss = 3.26908 avg_loss = 3.48869\n",
      "epoch no.2 train no.154990  loss = 3.80566 avg_loss = 3.53418\n",
      "epoch no.2 train no.155000  loss = 4.38356 avg_loss = 3.50593\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.2 train no.155010  loss = 5.23311 avg_loss = 3.51061\n",
      "epoch no.2 train no.155020  loss = 3.30690 avg_loss = 3.50755\n",
      "epoch no.2 train no.155030  loss = 4.06576 avg_loss = 3.52208\n",
      "epoch no.2 train no.155040  loss = 2.08254 avg_loss = 3.45451\n",
      "epoch no.2 train no.155050  loss = 4.69963 avg_loss = 3.46825\n",
      "epoch no.2 train no.155060  loss = 6.96210 avg_loss = 3.48353\n",
      "epoch no.2 train no.155070  loss = 3.60994 avg_loss = 3.46756\n",
      "epoch no.2 train no.155080  loss = 5.25276 avg_loss = 3.46713\n",
      "epoch no.2 train no.155090  loss = 4.04117 avg_loss = 3.46454\n",
      "epoch no.2 train no.155100  loss = 2.98731 avg_loss = 3.45625\n",
      "epoch no.2 train no.155110  loss = 2.90171 avg_loss = 3.46146\n",
      "epoch no.2 train no.155120  loss = 2.65092 avg_loss = 3.43031\n",
      "epoch no.2 train no.155130  loss = 2.71463 avg_loss = 3.44081\n",
      "epoch no.2 train no.155140  loss = 4.13884 avg_loss = 3.46725\n",
      "epoch no.2 train no.155150  loss = 2.77298 avg_loss = 3.45942\n",
      "epoch no.2 train no.155160  loss = 5.31092 avg_loss = 3.51782\n",
      "epoch no.2 train no.155170  loss = 2.52452 avg_loss = 3.44699\n",
      "epoch no.2 train no.155180  loss = 3.40377 avg_loss = 3.42676\n",
      "epoch no.2 train no.155190  loss = 2.66774 avg_loss = 3.39711\n",
      "epoch no.2 train no.155200  loss = 2.67922 avg_loss = 3.42539\n",
      "epoch no.2 train no.155210  loss = 3.73370 avg_loss = 3.44566\n",
      "epoch no.2 train no.155220  loss = 3.64329 avg_loss = 3.50754\n",
      "epoch no.2 train no.155230  loss = 3.62041 avg_loss = 3.50377\n",
      "epoch no.2 train no.155240  loss = 3.22834 avg_loss = 3.50675\n",
      "epoch no.2 train no.155250  loss = 3.99646 avg_loss = 3.54389\n",
      "epoch no.2 train no.155260  loss = 3.68716 avg_loss = 3.51743\n",
      "epoch no.2 train no.155270  loss = 2.70210 avg_loss = 3.48844\n",
      "epoch no.2 train no.155280  loss = 3.34714 avg_loss = 3.47055\n",
      "epoch no.2 train no.155290  loss = 2.80347 avg_loss = 3.49138\n",
      "epoch no.2 train no.155300  loss = 3.44023 avg_loss = 3.44045\n",
      "epoch no.2 train no.155310  loss = 2.61651 avg_loss = 3.45192\n",
      "epoch no.2 train no.155320  loss = 4.40806 avg_loss = 3.49448\n",
      "epoch no.2 train no.155330  loss = 2.50144 avg_loss = 3.46609\n",
      "epoch no.2 train no.155340  loss = 2.40953 avg_loss = 3.48628\n",
      "epoch no.2 train no.155350  loss = 3.29373 avg_loss = 3.46294\n",
      "epoch no.2 train no.155360  loss = 5.31272 avg_loss = 3.49810\n",
      "epoch no.2 train no.155370  loss = 2.95486 avg_loss = 3.51564\n",
      "epoch no.2 train no.155380  loss = 4.34267 avg_loss = 3.55740\n",
      "epoch no.2 train no.155390  loss = 3.91063 avg_loss = 3.55334\n",
      "epoch no.2 train no.155400  loss = 3.51795 avg_loss = 3.54252\n",
      "epoch no.2 train no.155410  loss = 2.65027 avg_loss = 3.49594\n",
      "epoch no.2 train no.155420  loss = 3.42209 avg_loss = 3.54634\n",
      "epoch no.2 train no.155430  loss = 3.44219 avg_loss = 3.50865\n",
      "epoch no.2 train no.155440  loss = 3.79802 avg_loss = 3.50498\n",
      "epoch no.2 train no.155450  loss = 2.45289 avg_loss = 3.47274\n",
      "epoch no.2 train no.155460  loss = 2.24882 avg_loss = 3.47110\n",
      "epoch no.2 train no.155470  loss = 4.90273 avg_loss = 3.47974\n",
      "epoch no.2 train no.155480  loss = 1.91106 avg_loss = 3.46638\n",
      "epoch no.2 train no.155490  loss = 4.39556 avg_loss = 3.49290\n",
      "epoch no.2 train no.155500  loss = 4.57510 avg_loss = 3.48245\n",
      "epoch no.2 train no.155510  loss = 3.32421 avg_loss = 3.45472\n",
      "epoch no.2 train no.155520  loss = 3.03593 avg_loss = 3.45078\n",
      "epoch no.2 train no.155530  loss = 3.66785 avg_loss = 3.41869\n",
      "epoch no.2 train no.155540  loss = 3.05567 avg_loss = 3.45619\n",
      "epoch no.2 train no.155550  loss = 4.39481 avg_loss = 3.48916\n",
      "epoch no.2 train no.155560  loss = 4.30973 avg_loss = 3.49290\n",
      "epoch no.2 train no.155570  loss = 2.86629 avg_loss = 3.45198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.155580  loss = 5.03959 avg_loss = 3.46102\n",
      "epoch no.2 train no.155590  loss = 3.39174 avg_loss = 3.46976\n",
      "epoch no.2 train no.155600  loss = 3.66883 avg_loss = 3.48599\n",
      "epoch no.2 train no.155610  loss = 3.76073 avg_loss = 3.42049\n",
      "epoch no.2 train no.155620  loss = 3.42183 avg_loss = 3.40317\n",
      "epoch no.2 train no.155630  loss = 2.84938 avg_loss = 3.48037\n",
      "epoch no.2 train no.155640  loss = 2.81669 avg_loss = 3.46623\n",
      "epoch no.2 train no.155650  loss = 3.89061 avg_loss = 3.49183\n",
      "epoch no.2 train no.155660  loss = 1.68771 avg_loss = 3.44446\n",
      "epoch no.2 train no.155670  loss = 3.58423 avg_loss = 3.44566\n",
      "epoch no.2 train no.155680  loss = 2.92792 avg_loss = 3.43667\n",
      "epoch no.2 train no.155690  loss = 2.12637 avg_loss = 3.40276\n",
      "epoch no.2 train no.155700  loss = 3.27465 avg_loss = 3.41652\n",
      "epoch no.2 train no.155710  loss = 2.91651 avg_loss = 3.39565\n",
      "epoch no.2 train no.155720  loss = 3.34215 avg_loss = 3.38688\n",
      "epoch no.2 train no.155730  loss = 2.84351 avg_loss = 3.39241\n",
      "epoch no.2 train no.155740  loss = 2.46404 avg_loss = 3.34627\n",
      "epoch no.2 train no.155750  loss = 2.76170 avg_loss = 3.31488\n",
      "epoch no.2 train no.155760  loss = 2.87299 avg_loss = 3.30469\n",
      "epoch no.2 train no.155770  loss = 2.95312 avg_loss = 3.33244\n",
      "epoch no.2 train no.155780  loss = 4.79416 avg_loss = 3.39335\n",
      "epoch no.2 train no.155790  loss = 2.81860 avg_loss = 3.39283\n",
      "epoch no.2 train no.155800  loss = 3.72325 avg_loss = 3.40288\n",
      "epoch no.2 train no.155810  loss = 2.10191 avg_loss = 3.37000\n",
      "epoch no.2 train no.155820  loss = 3.21423 avg_loss = 3.38334\n",
      "epoch no.2 train no.155830  loss = 2.81373 avg_loss = 3.43342\n",
      "epoch no.2 train no.155840  loss = 2.92254 avg_loss = 3.40523\n",
      "epoch no.2 train no.155850  loss = 2.37871 avg_loss = 3.42241\n",
      "epoch no.2 train no.155860  loss = 1.39652 avg_loss = 3.39706\n",
      "epoch no.2 train no.155870  loss = 3.73660 avg_loss = 3.39289\n",
      "epoch no.2 train no.155880  loss = 3.98777 avg_loss = 3.40991\n",
      "epoch no.2 train no.155890  loss = 1.28560 avg_loss = 3.36789\n",
      "epoch no.2 train no.155900  loss = 5.81046 avg_loss = 3.39751\n",
      "epoch no.2 train no.155910  loss = 3.66414 avg_loss = 3.38530\n",
      "epoch no.2 train no.155920  loss = 5.30833 avg_loss = 3.39850\n",
      "epoch no.2 train no.155930  loss = 2.70325 avg_loss = 3.40704\n",
      "epoch no.2 train no.155940  loss = 2.81361 avg_loss = 3.40635\n",
      "epoch no.2 train no.155950  loss = 2.79226 avg_loss = 3.40976\n",
      "epoch no.2 train no.155960  loss = 3.46217 avg_loss = 3.40201\n",
      "epoch no.2 train no.155970  loss = 3.54820 avg_loss = 3.38035\n",
      "epoch no.2 train no.155980  loss = 2.86633 avg_loss = 3.36433\n",
      "epoch no.2 train no.155990  loss = 4.85774 avg_loss = 3.34698\n",
      "epoch no.2 train no.156000  loss = 4.72960 avg_loss = 3.32928\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '▁o', 'st', '▁모음', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.2 train no.156010  loss = 2.52120 avg_loss = 3.33843\n",
      "epoch no.2 train no.156020  loss = 1.77147 avg_loss = 3.32946\n",
      "epoch no.2 train no.156030  loss = 3.29677 avg_loss = 3.36409\n",
      "epoch no.2 train no.156040  loss = 2.51595 avg_loss = 3.38858\n",
      "epoch no.2 train no.156050  loss = 5.66616 avg_loss = 3.44567\n",
      "epoch no.2 train no.156060  loss = 3.39076 avg_loss = 3.42840\n",
      "epoch no.2 train no.156070  loss = 3.88576 avg_loss = 3.39822\n",
      "epoch no.2 train no.156080  loss = 3.12149 avg_loss = 3.37940\n",
      "epoch no.2 train no.156090  loss = 5.63605 avg_loss = 3.41979\n",
      "epoch no.2 train no.156100  loss = 4.45443 avg_loss = 3.43435\n",
      "epoch no.2 train no.156110  loss = 3.49580 avg_loss = 3.49327\n",
      "epoch no.2 train no.156120  loss = 2.78199 avg_loss = 3.48301\n",
      "epoch no.2 train no.156130  loss = 3.65022 avg_loss = 3.50677\n",
      "epoch no.2 train no.156140  loss = 3.99311 avg_loss = 3.52392\n",
      "epoch no.2 train no.156150  loss = 3.27463 avg_loss = 3.48055\n",
      "epoch no.2 train no.156160  loss = 2.65530 avg_loss = 3.40639\n",
      "epoch no.2 train no.156170  loss = 2.59272 avg_loss = 3.39538\n",
      "epoch no.2 train no.156180  loss = 4.75989 avg_loss = 3.38223\n",
      "epoch no.2 train no.156190  loss = 3.39502 avg_loss = 3.38981\n",
      "epoch no.2 train no.156200  loss = 2.99023 avg_loss = 3.37059\n",
      "epoch no.2 train no.156210  loss = 1.99358 avg_loss = 3.33030\n",
      "epoch no.2 train no.156220  loss = 3.76757 avg_loss = 3.32196\n",
      "epoch no.2 train no.156230  loss = 3.33863 avg_loss = 3.33353\n",
      "epoch no.2 train no.156240  loss = 3.23284 avg_loss = 3.32517\n",
      "epoch no.2 train no.156250  loss = 3.47067 avg_loss = 3.34441\n",
      "epoch no.2 train no.156260  loss = 3.71981 avg_loss = 3.32961\n",
      "epoch no.2 train no.156270  loss = 1.53120 avg_loss = 3.32781\n",
      "epoch no.2 train no.156280  loss = 4.13319 avg_loss = 3.41703\n",
      "epoch no.2 train no.156290  loss = 3.48599 avg_loss = 3.44154\n",
      "epoch no.2 train no.156300  loss = 4.24570 avg_loss = 3.48848\n",
      "epoch no.2 train no.156310  loss = 5.07230 avg_loss = 3.48903\n",
      "epoch no.2 train no.156320  loss = 3.22007 avg_loss = 3.48914\n",
      "epoch no.2 train no.156330  loss = 1.94411 avg_loss = 3.45043\n",
      "epoch no.2 train no.156340  loss = 2.98286 avg_loss = 3.43808\n",
      "epoch no.2 train no.156350  loss = 4.84753 avg_loss = 3.43771\n",
      "epoch no.2 train no.156360  loss = 4.12067 avg_loss = 3.47027\n",
      "epoch no.2 train no.156370  loss = 3.51125 avg_loss = 3.49933\n",
      "epoch no.2 train no.156380  loss = 3.54957 avg_loss = 3.53980\n",
      "epoch no.2 train no.156390  loss = 3.51175 avg_loss = 3.52620\n",
      "epoch no.2 train no.156400  loss = 3.75135 avg_loss = 3.49854\n",
      "epoch no.2 train no.156410  loss = 3.39017 avg_loss = 3.50223\n",
      "epoch no.2 train no.156420  loss = 4.11566 avg_loss = 3.52433\n",
      "epoch no.2 train no.156430  loss = 2.48805 avg_loss = 3.50737\n",
      "epoch no.2 train no.156440  loss = 3.21475 avg_loss = 3.46805\n",
      "epoch no.2 train no.156450  loss = 2.71654 avg_loss = 3.46021\n",
      "epoch no.2 train no.156460  loss = 4.19847 avg_loss = 3.49062\n",
      "epoch no.2 train no.156470  loss = 1.81068 avg_loss = 3.45371\n",
      "epoch no.2 train no.156480  loss = 2.38605 avg_loss = 3.41437\n",
      "epoch no.2 train no.156490  loss = 3.34217 avg_loss = 3.34144\n",
      "epoch no.2 train no.156500  loss = 2.93456 avg_loss = 3.30929\n",
      "epoch no.2 train no.156510  loss = 3.25400 avg_loss = 3.31363\n",
      "epoch no.2 train no.156520  loss = 2.43030 avg_loss = 3.30196\n",
      "epoch no.2 train no.156530  loss = 3.88143 avg_loss = 3.32673\n",
      "epoch no.2 train no.156540  loss = 1.54011 avg_loss = 3.34511\n",
      "epoch no.2 train no.156550  loss = 3.92205 avg_loss = 3.33888\n",
      "epoch no.2 train no.156560  loss = 4.31079 avg_loss = 3.37858\n",
      "epoch no.2 train no.156570  loss = 1.56509 avg_loss = 3.36070\n",
      "epoch no.2 train no.156580  loss = 1.94351 avg_loss = 3.32455\n",
      "epoch no.2 train no.156590  loss = 2.68917 avg_loss = 3.31307\n",
      "epoch no.2 train no.156600  loss = 3.75680 avg_loss = 3.33841\n",
      "epoch no.2 train no.156610  loss = 3.53605 avg_loss = 3.33708\n",
      "epoch no.2 train no.156620  loss = 3.93484 avg_loss = 3.35154\n",
      "epoch no.2 train no.156630  loss = 3.22642 avg_loss = 3.37447\n",
      "epoch no.2 train no.156640  loss = 3.02961 avg_loss = 3.37651\n",
      "epoch no.2 train no.156650  loss = 2.03208 avg_loss = 3.39477\n",
      "epoch no.2 train no.156660  loss = 2.45411 avg_loss = 3.38448\n",
      "epoch no.2 train no.156670  loss = 2.04283 avg_loss = 3.36223\n",
      "epoch no.2 train no.156680  loss = 4.24274 avg_loss = 3.37934\n",
      "epoch no.2 train no.156690  loss = 3.93089 avg_loss = 3.36205\n",
      "epoch no.2 train no.156700  loss = 1.48705 avg_loss = 3.36859\n",
      "epoch no.2 train no.156710  loss = 3.31904 avg_loss = 3.34153\n",
      "epoch no.2 train no.156720  loss = 2.56549 avg_loss = 3.33376\n",
      "epoch no.2 train no.156730  loss = 4.37960 avg_loss = 3.35232\n",
      "epoch no.2 train no.156740  loss = 4.73298 avg_loss = 3.37281\n",
      "epoch no.2 train no.156750  loss = 3.39026 avg_loss = 3.40467\n",
      "epoch no.2 train no.156760  loss = 3.74936 avg_loss = 3.38376\n",
      "epoch no.2 train no.156770  loss = 2.71111 avg_loss = 3.38659\n",
      "epoch no.2 train no.156780  loss = 3.43886 avg_loss = 3.33789\n",
      "epoch no.2 train no.156790  loss = 3.05319 avg_loss = 3.33924\n",
      "epoch no.2 train no.156800  loss = 2.37030 avg_loss = 3.36025\n",
      "epoch no.2 train no.156810  loss = 4.28542 avg_loss = 3.41178\n",
      "epoch no.2 train no.156820  loss = 5.59996 avg_loss = 3.41768\n",
      "epoch no.2 train no.156830  loss = 3.99654 avg_loss = 3.44985\n",
      "epoch no.2 train no.156840  loss = 2.56540 avg_loss = 3.41473\n",
      "epoch no.2 train no.156850  loss = 3.39347 avg_loss = 3.42913\n",
      "epoch no.2 train no.156860  loss = 2.36052 avg_loss = 3.44027\n",
      "epoch no.2 train no.156870  loss = 4.57183 avg_loss = 3.39704\n",
      "epoch no.2 train no.156880  loss = 3.10344 avg_loss = 3.41261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.156890  loss = 4.83443 avg_loss = 3.42024\n",
      "epoch no.2 train no.156900  loss = 2.89473 avg_loss = 3.40866\n",
      "epoch no.2 train no.156910  loss = 2.25845 avg_loss = 3.36160\n",
      "epoch no.2 train no.156920  loss = 3.22662 avg_loss = 3.37393\n",
      "epoch no.2 train no.156930  loss = 5.72550 avg_loss = 3.40701\n",
      "epoch no.2 train no.156940  loss = 2.98904 avg_loss = 3.39852\n",
      "epoch no.2 train no.156950  loss = 2.98599 avg_loss = 3.38715\n",
      "epoch no.2 train no.156960  loss = 5.36521 avg_loss = 3.43000\n",
      "epoch no.2 train no.156970  loss = 3.64206 avg_loss = 3.44303\n",
      "epoch no.2 train no.156980  loss = 2.70564 avg_loss = 3.44545\n",
      "epoch no.2 train no.156990  loss = 5.58716 avg_loss = 3.46118\n",
      "epoch no.2 train no.157000  loss = 3.33167 avg_loss = 3.44676\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁가요', '모']\n",
      "추억의 90년대 가요</s>\n",
      "epoch no.2 train no.157010  loss = 3.23051 avg_loss = 3.47507\n",
      "epoch no.2 train no.157020  loss = 4.37234 avg_loss = 3.47090\n",
      "epoch no.2 train no.157030  loss = 3.01468 avg_loss = 3.40972\n",
      "epoch no.2 train no.157040  loss = 3.54120 avg_loss = 3.43205\n",
      "epoch no.2 train no.157050  loss = 3.17221 avg_loss = 3.43107\n",
      "epoch no.2 train no.157060  loss = 3.55664 avg_loss = 3.44280\n",
      "epoch no.2 train no.157070  loss = 3.49020 avg_loss = 3.45518\n",
      "epoch no.2 train no.157080  loss = 4.56225 avg_loss = 3.49028\n",
      "epoch no.2 train no.157090  loss = 1.85802 avg_loss = 3.49773\n",
      "epoch no.2 train no.157100  loss = 2.02023 avg_loss = 3.45405\n",
      "epoch no.2 train no.157110  loss = 5.09453 avg_loss = 3.51049\n",
      "epoch no.2 train no.157120  loss = 3.69757 avg_loss = 3.48914\n",
      "epoch no.2 train no.157130  loss = 4.75306 avg_loss = 3.48044\n",
      "epoch no.2 train no.157140  loss = 2.49227 avg_loss = 3.44682\n",
      "epoch no.2 train no.157150  loss = 3.43102 avg_loss = 3.45019\n",
      "epoch no.2 train no.157160  loss = 3.46684 avg_loss = 3.45494\n",
      "epoch no.2 train no.157170  loss = 4.77930 avg_loss = 3.48135\n",
      "epoch no.2 train no.157180  loss = 2.18345 avg_loss = 3.44102\n",
      "epoch no.2 train no.157190  loss = 2.57675 avg_loss = 3.40019\n",
      "epoch no.2 train no.157200  loss = 3.13774 avg_loss = 3.33854\n",
      "epoch no.2 train no.157210  loss = 3.20798 avg_loss = 3.34243\n",
      "epoch no.2 train no.157220  loss = 2.91518 avg_loss = 3.40571\n",
      "epoch no.2 train no.157230  loss = 2.67563 avg_loss = 3.41350\n",
      "epoch no.2 train no.157240  loss = 3.10701 avg_loss = 3.45541\n",
      "epoch no.2 train no.157250  loss = 4.83568 avg_loss = 3.48637\n",
      "epoch no.2 train no.157260  loss = 2.58169 avg_loss = 3.46384\n",
      "epoch no.2 train no.157270  loss = 4.60998 avg_loss = 3.41169\n",
      "epoch no.2 train no.157280  loss = 3.65559 avg_loss = 3.44067\n",
      "epoch no.2 train no.157290  loss = 3.74966 avg_loss = 3.39212\n",
      "epoch no.2 train no.157300  loss = 2.23288 avg_loss = 3.37476\n",
      "epoch no.2 train no.157310  loss = 2.84508 avg_loss = 3.40847\n",
      "epoch no.2 train no.157320  loss = 2.73264 avg_loss = 3.42449\n",
      "epoch no.2 train no.157330  loss = 4.19438 avg_loss = 3.40868\n",
      "epoch no.2 train no.157340  loss = 2.94434 avg_loss = 3.44230\n",
      "epoch no.2 train no.157350  loss = 4.21565 avg_loss = 3.41597\n",
      "epoch no.2 train no.157360  loss = 2.77156 avg_loss = 3.46330\n",
      "epoch no.2 train no.157370  loss = 3.15010 avg_loss = 3.49067\n",
      "epoch no.2 train no.157380  loss = 4.09079 avg_loss = 3.49819\n",
      "epoch no.2 train no.157390  loss = 3.36669 avg_loss = 3.46584\n",
      "epoch no.2 train no.157400  loss = 5.41904 avg_loss = 3.48453\n",
      "epoch no.2 train no.157410  loss = 3.38333 avg_loss = 3.47703\n",
      "epoch no.2 train no.157420  loss = 2.21658 avg_loss = 3.45763\n",
      "epoch no.2 train no.157430  loss = 2.63985 avg_loss = 3.43736\n",
      "epoch no.2 train no.157440  loss = 2.47318 avg_loss = 3.46793\n",
      "epoch no.2 train no.157450  loss = 3.34208 avg_loss = 3.44416\n",
      "epoch no.2 train no.157460  loss = 2.55292 avg_loss = 3.44920\n",
      "epoch no.2 train no.157470  loss = 4.30673 avg_loss = 3.40712\n",
      "epoch no.2 train no.157480  loss = 3.65791 avg_loss = 3.45033\n",
      "epoch no.2 train no.157490  loss = 3.65146 avg_loss = 3.44871\n",
      "epoch no.2 train no.157500  loss = 4.15982 avg_loss = 3.42611\n",
      "epoch no.2 train no.157510  loss = 2.81517 avg_loss = 3.41064\n",
      "epoch no.2 train no.157520  loss = 1.36578 avg_loss = 3.41777\n",
      "epoch no.2 train no.157530  loss = 3.13743 avg_loss = 3.38330\n",
      "epoch no.2 train no.157540  loss = 5.39511 avg_loss = 3.39809\n",
      "epoch no.2 train no.157550  loss = 7.17028 avg_loss = 3.41598\n",
      "epoch no.2 train no.157560  loss = 2.14232 avg_loss = 3.45300\n",
      "epoch no.2 train no.157570  loss = 3.09651 avg_loss = 3.38898\n",
      "epoch no.2 train no.157580  loss = 3.99399 avg_loss = 3.43372\n",
      "epoch no.2 train no.157590  loss = 3.01425 avg_loss = 3.39850\n",
      "epoch no.2 train no.157600  loss = 3.93014 avg_loss = 3.39743\n",
      "epoch no.2 train no.157610  loss = 2.82272 avg_loss = 3.36955\n",
      "epoch no.2 train no.157620  loss = 2.62138 avg_loss = 3.37347\n",
      "epoch no.2 train no.157630  loss = 3.87102 avg_loss = 3.41634\n",
      "epoch no.2 train no.157640  loss = 5.50701 avg_loss = 3.43003\n",
      "epoch no.2 train no.157650  loss = 4.05081 avg_loss = 3.44285\n",
      "epoch no.2 train no.157660  loss = 3.68807 avg_loss = 3.45638\n",
      "epoch no.2 train no.157670  loss = 2.72664 avg_loss = 3.47052\n",
      "epoch no.2 train no.157680  loss = 4.99041 avg_loss = 3.45478\n",
      "epoch no.2 train no.157690  loss = 4.76640 avg_loss = 3.46460\n",
      "epoch no.2 train no.157700  loss = 2.91044 avg_loss = 3.48708\n",
      "epoch no.2 train no.157710  loss = 2.74322 avg_loss = 3.52166\n",
      "epoch no.2 train no.157720  loss = 3.81683 avg_loss = 3.50734\n",
      "epoch no.2 train no.157730  loss = 3.48949 avg_loss = 3.51520\n",
      "epoch no.2 train no.157740  loss = 3.16609 avg_loss = 3.46031\n",
      "epoch no.2 train no.157750  loss = 4.85734 avg_loss = 3.46772\n",
      "epoch no.2 train no.157760  loss = 2.21790 avg_loss = 3.43701\n",
      "epoch no.2 train no.157770  loss = 2.87919 avg_loss = 3.39520\n",
      "epoch no.2 train no.157780  loss = 4.33795 avg_loss = 3.47053\n",
      "epoch no.2 train no.157790  loss = 3.41590 avg_loss = 3.45266\n",
      "epoch no.2 train no.157800  loss = 3.53329 avg_loss = 3.45046\n",
      "epoch no.2 train no.157810  loss = 4.84667 avg_loss = 3.44765\n",
      "epoch no.2 train no.157820  loss = 4.28865 avg_loss = 3.40402\n",
      "epoch no.2 train no.157830  loss = 4.38985 avg_loss = 3.39175\n",
      "epoch no.2 train no.157840  loss = 2.06739 avg_loss = 3.39354\n",
      "epoch no.2 train no.157850  loss = 3.91729 avg_loss = 3.40553\n",
      "epoch no.2 train no.157860  loss = 3.42043 avg_loss = 3.45178\n",
      "epoch no.2 train no.157870  loss = 2.60991 avg_loss = 3.45599\n",
      "epoch no.2 train no.157880  loss = 4.01401 avg_loss = 3.48240\n",
      "epoch no.2 train no.157890  loss = 5.29406 avg_loss = 3.48751\n",
      "epoch no.2 train no.157900  loss = 3.10522 avg_loss = 3.48108\n",
      "epoch no.2 train no.157910  loss = 2.36540 avg_loss = 3.43703\n",
      "epoch no.2 train no.157920  loss = 2.89953 avg_loss = 3.40666\n",
      "epoch no.2 train no.157930  loss = 2.70641 avg_loss = 3.39041\n",
      "epoch no.2 train no.157940  loss = 3.34371 avg_loss = 3.39978\n",
      "epoch no.2 train no.157950  loss = 4.26118 avg_loss = 3.40466\n",
      "epoch no.2 train no.157960  loss = 3.41886 avg_loss = 3.41860\n",
      "epoch no.2 train no.157970  loss = 3.33249 avg_loss = 3.40300\n",
      "epoch no.2 train no.157980  loss = 3.15032 avg_loss = 3.37602\n",
      "epoch no.2 train no.157990  loss = 2.33685 avg_loss = 3.38771\n",
      "epoch no.2 train no.158000  loss = 2.69521 avg_loss = 3.36904\n",
      "3\n",
      "to_tokens: ['▁비', '▁올드', '▁o', 'st', '▁모음']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.2 train no.158010  loss = 2.94266 avg_loss = 3.34392\n",
      "epoch no.2 train no.158020  loss = 4.43455 avg_loss = 3.34797\n",
      "epoch no.2 train no.158030  loss = 2.97931 avg_loss = 3.40949\n",
      "epoch no.2 train no.158040  loss = 2.02787 avg_loss = 3.39589\n",
      "epoch no.2 train no.158050  loss = 1.86932 avg_loss = 3.36833\n",
      "epoch no.2 train no.158060  loss = 2.78172 avg_loss = 3.34583\n",
      "epoch no.2 train no.158070  loss = 2.22289 avg_loss = 3.32203\n",
      "epoch no.2 train no.158080  loss = 3.16018 avg_loss = 3.32719\n",
      "epoch no.2 train no.158090  loss = 5.54692 avg_loss = 3.35188\n",
      "epoch no.2 train no.158100  loss = 3.55510 avg_loss = 3.36228\n",
      "epoch no.2 train no.158110  loss = 4.09273 avg_loss = 3.40761\n",
      "epoch no.2 train no.158120  loss = 4.87417 avg_loss = 3.39952\n",
      "epoch no.2 train no.158130  loss = 2.70065 avg_loss = 3.39243\n",
      "epoch no.2 train no.158140  loss = 2.56144 avg_loss = 3.36227\n",
      "epoch no.2 train no.158150  loss = 4.75313 avg_loss = 3.44257\n",
      "epoch no.2 train no.158160  loss = 2.78834 avg_loss = 3.44519\n",
      "epoch no.2 train no.158170  loss = 2.85763 avg_loss = 3.45655\n",
      "epoch no.2 train no.158180  loss = 2.67497 avg_loss = 3.40460\n",
      "epoch no.2 train no.158190  loss = 1.63694 avg_loss = 3.37204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.158200  loss = 3.26743 avg_loss = 3.37331\n",
      "epoch no.2 train no.158210  loss = 3.48513 avg_loss = 3.37683\n",
      "epoch no.2 train no.158220  loss = 4.72605 avg_loss = 3.37360\n",
      "epoch no.2 train no.158230  loss = 2.15628 avg_loss = 3.39493\n",
      "epoch no.2 train no.158240  loss = 3.23738 avg_loss = 3.38599\n",
      "epoch no.2 train no.158250  loss = 3.11772 avg_loss = 3.35259\n",
      "epoch no.2 train no.158260  loss = 1.64642 avg_loss = 3.36806\n",
      "epoch no.2 train no.158270  loss = 4.25096 avg_loss = 3.35789\n",
      "epoch no.2 train no.158280  loss = 3.53945 avg_loss = 3.33898\n",
      "epoch no.2 train no.158290  loss = 1.58299 avg_loss = 3.33538\n",
      "epoch no.2 train no.158300  loss = 1.95048 avg_loss = 3.37301\n",
      "epoch no.2 train no.158310  loss = 1.79938 avg_loss = 3.34534\n",
      "epoch no.2 train no.158320  loss = 2.87714 avg_loss = 3.35389\n",
      "epoch no.2 train no.158330  loss = 3.45472 avg_loss = 3.41646\n",
      "epoch no.2 train no.158340  loss = 5.04749 avg_loss = 3.42065\n",
      "epoch no.2 train no.158350  loss = 5.95613 avg_loss = 3.40039\n",
      "epoch no.2 train no.158360  loss = 4.36607 avg_loss = 3.40731\n",
      "epoch no.2 train no.158370  loss = 4.16230 avg_loss = 3.41519\n",
      "epoch no.2 train no.158380  loss = 4.42755 avg_loss = 3.41529\n",
      "epoch no.2 train no.158390  loss = 2.70543 avg_loss = 3.40667\n",
      "epoch no.2 train no.158400  loss = 2.76837 avg_loss = 3.38046\n",
      "epoch no.2 train no.158410  loss = 3.84297 avg_loss = 3.37082\n",
      "epoch no.2 train no.158420  loss = 1.67145 avg_loss = 3.37308\n",
      "epoch no.2 train no.158430  loss = 2.88170 avg_loss = 3.31816\n",
      "epoch no.2 train no.158440  loss = 2.98744 avg_loss = 3.31950\n",
      "epoch no.2 train no.158450  loss = 4.50832 avg_loss = 3.31597\n",
      "epoch no.2 train no.158460  loss = 3.92623 avg_loss = 3.34300\n",
      "epoch no.2 train no.158470  loss = 4.35997 avg_loss = 3.34380\n",
      "epoch no.2 train no.158480  loss = 2.98412 avg_loss = 3.32965\n",
      "epoch no.2 train no.158490  loss = 3.50065 avg_loss = 3.35272\n",
      "epoch no.2 train no.158500  loss = 4.78178 avg_loss = 3.32165\n",
      "epoch no.2 train no.158510  loss = 2.75358 avg_loss = 3.36543\n",
      "epoch no.2 train no.158520  loss = 3.91155 avg_loss = 3.39354\n",
      "epoch no.2 train no.158530  loss = 4.94484 avg_loss = 3.40572\n",
      "epoch no.2 train no.158540  loss = 2.62500 avg_loss = 3.41668\n",
      "epoch no.2 train no.158550  loss = 3.26874 avg_loss = 3.41533\n",
      "epoch no.2 train no.158560  loss = 3.85907 avg_loss = 3.45604\n",
      "epoch no.2 train no.158570  loss = 4.42061 avg_loss = 3.50943\n",
      "epoch no.2 train no.158580  loss = 4.17484 avg_loss = 3.49374\n",
      "epoch no.2 train no.158590  loss = 2.03924 avg_loss = 3.50857\n",
      "epoch no.2 train no.158600  loss = 2.78268 avg_loss = 3.48117\n",
      "epoch no.2 train no.158610  loss = 2.11857 avg_loss = 3.46368\n",
      "epoch no.2 train no.158620  loss = 3.30923 avg_loss = 3.51942\n",
      "epoch no.2 train no.158630  loss = 3.06418 avg_loss = 3.47731\n",
      "epoch no.2 train no.158640  loss = 3.83879 avg_loss = 3.47071\n",
      "epoch no.2 train no.158650  loss = 3.09380 avg_loss = 3.43799\n",
      "epoch no.2 train no.158660  loss = 3.03057 avg_loss = 3.39235\n",
      "epoch no.2 train no.158670  loss = 2.48419 avg_loss = 3.37825\n",
      "epoch no.2 train no.158680  loss = 2.69341 avg_loss = 3.38605\n",
      "epoch no.2 train no.158690  loss = 2.99474 avg_loss = 3.38735\n",
      "epoch no.2 train no.158700  loss = 3.13858 avg_loss = 3.40843\n",
      "epoch no.2 train no.158710  loss = 3.01663 avg_loss = 3.40992\n",
      "epoch no.2 train no.158720  loss = 4.53561 avg_loss = 3.38990\n",
      "epoch no.2 train no.158730  loss = 4.85067 avg_loss = 3.35793\n",
      "epoch no.2 train no.158740  loss = 3.67943 avg_loss = 3.32827\n",
      "epoch no.2 train no.158750  loss = 3.09523 avg_loss = 3.31811\n",
      "epoch no.2 train no.158760  loss = 4.55418 avg_loss = 3.30734\n",
      "epoch no.2 train no.158770  loss = 4.26870 avg_loss = 3.34754\n",
      "epoch no.2 train no.158780  loss = 3.31054 avg_loss = 3.38693\n",
      "epoch no.2 train no.158790  loss = 5.58076 avg_loss = 3.41329\n",
      "epoch no.2 train no.158800  loss = 3.48005 avg_loss = 3.43050\n",
      "epoch no.2 train no.158810  loss = 4.51305 avg_loss = 3.49571\n",
      "epoch no.2 train no.158820  loss = 3.22742 avg_loss = 3.51521\n",
      "epoch no.2 train no.158830  loss = 3.47510 avg_loss = 3.50653\n",
      "epoch no.2 train no.158840  loss = 3.45132 avg_loss = 3.48410\n",
      "epoch no.2 train no.158850  loss = 4.89970 avg_loss = 3.54631\n",
      "epoch no.2 train no.158860  loss = 4.59244 avg_loss = 3.54451\n",
      "epoch no.2 train no.158870  loss = 3.47176 avg_loss = 3.53130\n",
      "epoch no.2 train no.158880  loss = 2.58649 avg_loss = 3.53447\n",
      "epoch no.2 train no.158890  loss = 3.44806 avg_loss = 3.54743\n",
      "epoch no.2 train no.158900  loss = 4.04050 avg_loss = 3.55947\n",
      "epoch no.2 train no.158910  loss = 3.72512 avg_loss = 3.54009\n",
      "epoch no.2 train no.158920  loss = 4.50654 avg_loss = 3.51296\n",
      "epoch no.2 train no.158930  loss = 3.51480 avg_loss = 3.49307\n",
      "epoch no.2 train no.158940  loss = 3.74736 avg_loss = 3.49089\n",
      "epoch no.2 train no.158950  loss = 3.75098 avg_loss = 3.46434\n",
      "epoch no.2 train no.158960  loss = 3.61400 avg_loss = 3.41693\n",
      "epoch no.2 train no.158970  loss = 3.39596 avg_loss = 3.41393\n",
      "epoch no.2 train no.158980  loss = 2.57058 avg_loss = 3.43595\n",
      "epoch no.2 train no.158990  loss = 4.21176 avg_loss = 3.42515\n",
      "epoch no.2 train no.159000  loss = 3.64533 avg_loss = 3.43219\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁명']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.2 train no.159010  loss = 3.34452 avg_loss = 3.45584\n",
      "epoch no.2 train no.159020  loss = 3.86315 avg_loss = 3.47697\n",
      "epoch no.2 train no.159030  loss = 2.62738 avg_loss = 3.49482\n",
      "epoch no.2 train no.159040  loss = 3.01589 avg_loss = 3.46570\n",
      "epoch no.2 train no.159050  loss = 2.62368 avg_loss = 3.46379\n",
      "epoch no.2 train no.159060  loss = 3.93313 avg_loss = 3.42928\n",
      "epoch no.2 train no.159070  loss = 2.35703 avg_loss = 3.43255\n",
      "epoch no.2 train no.159080  loss = 1.89850 avg_loss = 3.37451\n",
      "epoch no.2 train no.159090  loss = 4.45488 avg_loss = 3.34209\n",
      "epoch no.2 train no.159100  loss = 4.45551 avg_loss = 3.33713\n",
      "epoch no.2 train no.159110  loss = 3.79078 avg_loss = 3.33266\n",
      "epoch no.2 train no.159120  loss = 2.51754 avg_loss = 3.31030\n",
      "epoch no.2 train no.159130  loss = 5.26611 avg_loss = 3.30482\n",
      "epoch no.2 train no.159140  loss = 2.94581 avg_loss = 3.30029\n",
      "epoch no.2 train no.159150  loss = 2.22117 avg_loss = 3.26258\n",
      "epoch no.2 train no.159160  loss = 1.94295 avg_loss = 3.24161\n",
      "epoch no.2 train no.159170  loss = 1.15007 avg_loss = 3.25385\n",
      "epoch no.2 train no.159180  loss = 2.30048 avg_loss = 3.30931\n",
      "epoch no.2 train no.159190  loss = 2.15128 avg_loss = 3.34080\n",
      "epoch no.2 train no.159200  loss = 3.55344 avg_loss = 3.33643\n",
      "epoch no.2 train no.159210  loss = 3.69895 avg_loss = 3.37146\n",
      "epoch no.2 train no.159220  loss = 3.70813 avg_loss = 3.38421\n",
      "epoch no.2 train no.159230  loss = 3.80944 avg_loss = 3.39536\n",
      "epoch no.2 train no.159240  loss = 3.33552 avg_loss = 3.37563\n",
      "epoch no.2 train no.159250  loss = 1.92912 avg_loss = 3.37969\n",
      "epoch no.2 train no.159260  loss = 4.43426 avg_loss = 3.36106\n",
      "epoch no.2 train no.159270  loss = 2.02858 avg_loss = 3.37409\n",
      "epoch no.2 train no.159280  loss = 3.44847 avg_loss = 3.37102\n",
      "epoch no.2 train no.159290  loss = 2.70527 avg_loss = 3.34970\n",
      "epoch no.2 train no.159300  loss = 3.42918 avg_loss = 3.36504\n",
      "epoch no.2 train no.159310  loss = 3.84550 avg_loss = 3.40817\n",
      "epoch no.2 train no.159320  loss = 3.57655 avg_loss = 3.44195\n",
      "epoch no.2 train no.159330  loss = 3.65088 avg_loss = 3.40020\n",
      "epoch no.2 train no.159340  loss = 4.00913 avg_loss = 3.39685\n",
      "epoch no.2 train no.159350  loss = 2.17415 avg_loss = 3.36956\n",
      "epoch no.2 train no.159360  loss = 2.86676 avg_loss = 3.38644\n",
      "epoch no.2 train no.159370  loss = 2.24113 avg_loss = 3.35464\n",
      "epoch no.2 train no.159380  loss = 4.19344 avg_loss = 3.36691\n",
      "epoch no.2 train no.159390  loss = 3.74674 avg_loss = 3.36769\n",
      "epoch no.2 train no.159400  loss = 2.21680 avg_loss = 3.37500\n",
      "epoch no.2 train no.159410  loss = 2.86652 avg_loss = 3.41678\n",
      "epoch no.2 train no.159420  loss = 3.50233 avg_loss = 3.42200\n",
      "epoch no.2 train no.159430  loss = 3.46219 avg_loss = 3.42140\n",
      "epoch no.2 train no.159440  loss = 3.89445 avg_loss = 3.45269\n",
      "epoch no.2 train no.159450  loss = 4.56484 avg_loss = 3.45337\n",
      "epoch no.2 train no.159460  loss = 2.26266 avg_loss = 3.40117\n",
      "epoch no.2 train no.159470  loss = 2.76642 avg_loss = 3.48918\n",
      "epoch no.2 train no.159480  loss = 2.91989 avg_loss = 3.51676\n",
      "epoch no.2 train no.159490  loss = 3.13052 avg_loss = 3.47928\n",
      "epoch no.2 train no.159500  loss = 2.37783 avg_loss = 3.49439\n",
      "epoch no.2 train no.159510  loss = 3.92212 avg_loss = 3.42284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.159520  loss = 2.65584 avg_loss = 3.38535\n",
      "epoch no.2 train no.159530  loss = 3.76477 avg_loss = 3.39310\n",
      "epoch no.2 train no.159540  loss = 3.51934 avg_loss = 3.38068\n",
      "epoch no.2 train no.159550  loss = 4.24518 avg_loss = 3.35884\n",
      "epoch no.2 train no.159560  loss = 4.82305 avg_loss = 3.37287\n",
      "epoch no.2 train no.159570  loss = 3.64072 avg_loss = 3.38728\n",
      "epoch no.2 train no.159580  loss = 3.41546 avg_loss = 3.38540\n",
      "epoch no.2 train no.159590  loss = 3.38252 avg_loss = 3.38502\n",
      "epoch no.2 train no.159600  loss = 4.32564 avg_loss = 3.39776\n",
      "epoch no.2 train no.159610  loss = 3.18601 avg_loss = 3.39719\n",
      "epoch no.2 train no.159620  loss = 4.12938 avg_loss = 3.38616\n",
      "epoch no.2 train no.159630  loss = 3.93086 avg_loss = 3.40749\n",
      "epoch no.2 train no.159640  loss = 2.20724 avg_loss = 3.38535\n",
      "epoch no.2 train no.159650  loss = 3.67060 avg_loss = 3.37424\n",
      "epoch no.2 train no.159660  loss = 3.02271 avg_loss = 3.31715\n",
      "epoch no.2 train no.159670  loss = 2.78905 avg_loss = 3.36504\n",
      "epoch no.2 train no.159680  loss = 7.27341 avg_loss = 3.45529\n",
      "epoch no.2 train no.159690  loss = 5.90721 avg_loss = 3.45491\n",
      "epoch no.2 train no.159700  loss = 3.69666 avg_loss = 3.46042\n",
      "epoch no.2 train no.159710  loss = 5.55521 avg_loss = 3.51754\n",
      "epoch no.2 train no.159720  loss = 2.55709 avg_loss = 3.53291\n",
      "epoch no.2 train no.159730  loss = 2.12585 avg_loss = 3.51260\n",
      "epoch no.2 train no.159740  loss = 4.32706 avg_loss = 3.47020\n",
      "epoch no.2 train no.159750  loss = 3.76398 avg_loss = 3.45001\n",
      "epoch no.2 train no.159760  loss = 3.88438 avg_loss = 3.41443\n",
      "epoch no.2 train no.159770  loss = 2.98456 avg_loss = 3.36330\n",
      "epoch no.2 train no.159780  loss = 1.33186 avg_loss = 3.32731\n",
      "epoch no.2 train no.159790  loss = 1.87498 avg_loss = 3.32757\n",
      "epoch no.2 train no.159800  loss = 4.16207 avg_loss = 3.33413\n",
      "epoch no.2 train no.159810  loss = 4.82378 avg_loss = 3.42484\n",
      "epoch no.2 train no.159820  loss = 4.92485 avg_loss = 3.41741\n",
      "epoch no.2 train no.159830  loss = 4.07423 avg_loss = 3.48369\n",
      "epoch no.2 train no.159840  loss = 2.78147 avg_loss = 3.51591\n",
      "epoch no.2 train no.159850  loss = 3.34909 avg_loss = 3.52120\n",
      "epoch no.2 train no.159860  loss = 2.80566 avg_loss = 3.54568\n",
      "epoch no.2 train no.159870  loss = 2.16094 avg_loss = 3.51309\n",
      "epoch no.2 train no.159880  loss = 4.55412 avg_loss = 3.53968\n",
      "epoch no.2 train no.159890  loss = 2.53441 avg_loss = 3.54014\n",
      "epoch no.2 train no.159900  loss = 3.49719 avg_loss = 3.51676\n",
      "epoch no.2 train no.159910  loss = 1.23977 avg_loss = 3.45111\n",
      "epoch no.2 train no.159920  loss = 3.04307 avg_loss = 3.44212\n",
      "epoch no.2 train no.159930  loss = 2.90988 avg_loss = 3.45284\n",
      "epoch no.2 train no.159940  loss = 3.41627 avg_loss = 3.46768\n",
      "epoch no.2 train no.159950  loss = 4.35497 avg_loss = 3.43370\n",
      "epoch no.2 train no.159960  loss = 2.66024 avg_loss = 3.44103\n",
      "epoch no.2 train no.159970  loss = 5.35158 avg_loss = 3.50357\n",
      "epoch no.2 train no.159980  loss = 4.44698 avg_loss = 3.53917\n",
      "epoch no.2 train no.159990  loss = 4.57403 avg_loss = 3.58659\n",
      "epoch no.2 train no.160000  loss = 2.85839 avg_loss = 3.55517\n",
      "3\n",
      "to_tokens: ['▁비', '▁올드', '팝', '송', '</s>']\n",
      "추억의 올드팝 베스트</s>\n",
      "epoch no.2 train no.160010  loss = 3.63898 avg_loss = 3.56790\n",
      "epoch no.2 train no.160020  loss = 3.50225 avg_loss = 3.56290\n",
      "epoch no.2 train no.160030  loss = 3.18903 avg_loss = 3.55733\n",
      "epoch no.2 train no.160040  loss = 2.01096 avg_loss = 3.51976\n",
      "epoch no.2 train no.160050  loss = 4.13453 avg_loss = 3.54297\n",
      "epoch no.2 train no.160060  loss = 2.20821 avg_loss = 3.53739\n",
      "epoch no.2 train no.160070  loss = 4.44358 avg_loss = 3.51775\n",
      "epoch no.2 train no.160080  loss = 1.96565 avg_loss = 3.47432\n",
      "epoch no.2 train no.160090  loss = 4.68519 avg_loss = 3.47048\n",
      "epoch no.2 train no.160100  loss = 2.53067 avg_loss = 3.43193\n",
      "epoch no.2 train no.160110  loss = 3.03414 avg_loss = 3.43032\n",
      "epoch no.2 train no.160120  loss = 3.72977 avg_loss = 3.42758\n",
      "epoch no.2 train no.160130  loss = 4.14440 avg_loss = 3.42259\n",
      "epoch no.2 train no.160140  loss = 3.79959 avg_loss = 3.40433\n",
      "epoch no.2 train no.160150  loss = 2.07889 avg_loss = 3.37624\n",
      "epoch no.2 train no.160160  loss = 4.57120 avg_loss = 3.39395\n",
      "epoch no.2 train no.160170  loss = 2.83516 avg_loss = 3.41876\n",
      "epoch no.2 train no.160180  loss = 1.66359 avg_loss = 3.40565\n",
      "epoch no.2 train no.160190  loss = 1.94701 avg_loss = 3.40522\n",
      "epoch no.2 train no.160200  loss = 3.07425 avg_loss = 3.40619\n",
      "epoch no.2 train no.160210  loss = 2.76285 avg_loss = 3.42068\n",
      "epoch no.2 train no.160220  loss = 4.44099 avg_loss = 3.40835\n",
      "epoch no.2 train no.160230  loss = 2.28219 avg_loss = 3.46403\n",
      "epoch no.2 train no.160240  loss = 2.76253 avg_loss = 3.44466\n",
      "epoch no.2 train no.160250  loss = 3.44732 avg_loss = 3.49474\n",
      "epoch no.2 train no.160260  loss = 1.78655 avg_loss = 3.48175\n",
      "epoch no.2 train no.160270  loss = 2.80310 avg_loss = 3.51079\n",
      "epoch no.2 train no.160280  loss = 3.81295 avg_loss = 3.50723\n",
      "epoch no.2 train no.160290  loss = 2.26187 avg_loss = 3.46237\n",
      "epoch no.2 train no.160300  loss = 4.18184 avg_loss = 3.50568\n",
      "epoch no.2 train no.160310  loss = 3.83010 avg_loss = 3.47693\n",
      "epoch no.2 train no.160320  loss = 3.60353 avg_loss = 3.49790\n",
      "epoch no.2 train no.160330  loss = 3.74757 avg_loss = 3.51759\n",
      "epoch no.2 train no.160340  loss = 4.43310 avg_loss = 3.47374\n",
      "epoch no.2 train no.160350  loss = 4.62017 avg_loss = 3.45913\n",
      "epoch no.2 train no.160360  loss = 5.12450 avg_loss = 3.45925\n",
      "epoch no.2 train no.160370  loss = 2.88091 avg_loss = 3.47894\n",
      "epoch no.2 train no.160380  loss = 2.62949 avg_loss = 3.42808\n",
      "epoch no.2 train no.160390  loss = 4.72724 avg_loss = 3.44980\n",
      "epoch no.2 train no.160400  loss = 2.95856 avg_loss = 3.41812\n",
      "epoch no.2 train no.160410  loss = 2.95183 avg_loss = 3.43598\n",
      "epoch no.2 train no.160420  loss = 3.33616 avg_loss = 3.42869\n",
      "epoch no.2 train no.160430  loss = 3.80464 avg_loss = 3.45489\n",
      "epoch no.2 train no.160440  loss = 3.60477 avg_loss = 3.47467\n",
      "epoch no.2 train no.160450  loss = 2.44449 avg_loss = 3.48544\n",
      "epoch no.2 train no.160460  loss = 2.28962 avg_loss = 3.42707\n",
      "epoch no.2 train no.160470  loss = 2.55431 avg_loss = 3.38659\n",
      "epoch no.2 train no.160480  loss = 3.12461 avg_loss = 3.38130\n",
      "epoch no.2 train no.160490  loss = 2.47746 avg_loss = 3.39150\n",
      "epoch no.2 train no.160500  loss = 3.90561 avg_loss = 3.38600\n",
      "epoch no.2 train no.160510  loss = 2.44382 avg_loss = 3.43400\n",
      "epoch no.2 train no.160520  loss = 3.55014 avg_loss = 3.48492\n",
      "epoch no.2 train no.160530  loss = 5.86661 avg_loss = 3.48570\n",
      "epoch no.2 train no.160540  loss = 4.94848 avg_loss = 3.51985\n",
      "epoch no.2 train no.160550  loss = 3.04050 avg_loss = 3.48407\n",
      "epoch no.2 train no.160560  loss = 2.06629 avg_loss = 3.42889\n",
      "epoch no.2 train no.160570  loss = 3.33458 avg_loss = 3.42043\n",
      "epoch no.2 train no.160580  loss = 3.36226 avg_loss = 3.36868\n",
      "epoch no.2 train no.160590  loss = 3.19058 avg_loss = 3.36737\n",
      "epoch no.2 train no.160600  loss = 3.48520 avg_loss = 3.36214\n",
      "epoch no.2 train no.160610  loss = 2.71237 avg_loss = 3.37574\n",
      "epoch no.2 train no.160620  loss = 4.16653 avg_loss = 3.44316\n",
      "epoch no.2 train no.160630  loss = 2.81839 avg_loss = 3.46206\n",
      "epoch no.2 train no.160640  loss = 2.47421 avg_loss = 3.44143\n",
      "epoch no.2 train no.160650  loss = 3.88302 avg_loss = 3.41657\n",
      "epoch no.2 train no.160660  loss = 3.46776 avg_loss = 3.41678\n",
      "epoch no.2 train no.160670  loss = 4.11079 avg_loss = 3.41581\n",
      "epoch no.2 train no.160680  loss = 2.73683 avg_loss = 3.39216\n",
      "epoch no.2 train no.160690  loss = 3.33714 avg_loss = 3.38546\n",
      "epoch no.2 train no.160700  loss = 2.43489 avg_loss = 3.36014\n",
      "epoch no.2 train no.160710  loss = 3.07909 avg_loss = 3.35063\n",
      "epoch no.2 train no.160720  loss = 2.13229 avg_loss = 3.33972\n",
      "epoch no.2 train no.160730  loss = 4.28493 avg_loss = 3.37418\n",
      "epoch no.2 train no.160740  loss = 2.95354 avg_loss = 3.35121\n",
      "epoch no.2 train no.160750  loss = 4.38856 avg_loss = 3.37898\n",
      "epoch no.2 train no.160760  loss = 4.31162 avg_loss = 3.36358\n",
      "epoch no.2 train no.160770  loss = 3.50958 avg_loss = 3.37945\n",
      "epoch no.2 train no.160780  loss = 3.14099 avg_loss = 3.35922\n",
      "epoch no.2 train no.160790  loss = 3.68791 avg_loss = 3.35473\n",
      "epoch no.2 train no.160800  loss = 3.79804 avg_loss = 3.41065\n",
      "epoch no.2 train no.160810  loss = 2.08754 avg_loss = 3.37530\n",
      "epoch no.2 train no.160820  loss = 1.99148 avg_loss = 3.33868\n",
      "epoch no.2 train no.160830  loss = 4.31573 avg_loss = 3.35421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.160840  loss = 2.94770 avg_loss = 3.32185\n",
      "epoch no.2 train no.160850  loss = 3.75733 avg_loss = 3.32775\n",
      "epoch no.2 train no.160860  loss = 6.97543 avg_loss = 3.32811\n",
      "epoch no.2 train no.160870  loss = 3.34154 avg_loss = 3.34872\n",
      "epoch no.2 train no.160880  loss = 3.37891 avg_loss = 3.35572\n",
      "epoch no.2 train no.160890  loss = 3.13750 avg_loss = 3.36387\n",
      "epoch no.2 train no.160900  loss = 5.98427 avg_loss = 3.40581\n",
      "epoch no.2 train no.160910  loss = 3.24164 avg_loss = 3.40713\n",
      "epoch no.2 train no.160920  loss = 1.99479 avg_loss = 3.35557\n",
      "epoch no.2 train no.160930  loss = 2.56658 avg_loss = 3.31661\n",
      "epoch no.2 train no.160940  loss = 2.67598 avg_loss = 3.32104\n",
      "epoch no.2 train no.160950  loss = 3.27164 avg_loss = 3.30005\n",
      "epoch no.2 train no.160960  loss = 2.46506 avg_loss = 3.28920\n",
      "epoch no.2 train no.160970  loss = 3.20398 avg_loss = 3.30221\n",
      "epoch no.2 train no.160980  loss = 3.33034 avg_loss = 3.26941\n",
      "epoch no.2 train no.160990  loss = 4.33433 avg_loss = 3.31427\n",
      "epoch no.2 train no.161000  loss = 4.87003 avg_loss = 3.31514\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '▁가요', '</s>']\n",
      "추억의 90년대 인기 가요</s>\n",
      "epoch no.2 train no.161010  loss = 2.00191 avg_loss = 3.31147\n",
      "epoch no.2 train no.161020  loss = 4.00355 avg_loss = 3.30834\n",
      "epoch no.2 train no.161030  loss = 4.77041 avg_loss = 3.34483\n",
      "epoch no.2 train no.161040  loss = 3.95698 avg_loss = 3.38889\n",
      "epoch no.2 train no.161050  loss = 4.08622 avg_loss = 3.39791\n",
      "epoch no.2 train no.161060  loss = 3.79463 avg_loss = 3.40432\n",
      "epoch no.2 train no.161070  loss = 3.72395 avg_loss = 3.38368\n",
      "epoch no.2 train no.161080  loss = 2.37024 avg_loss = 3.38393\n",
      "epoch no.2 train no.161090  loss = 3.62264 avg_loss = 3.41036\n",
      "epoch no.2 train no.161100  loss = 3.08652 avg_loss = 3.37350\n",
      "epoch no.2 train no.161110  loss = 5.71386 avg_loss = 3.45460\n",
      "epoch no.2 train no.161120  loss = 1.72930 avg_loss = 3.46034\n",
      "epoch no.2 train no.161130  loss = 3.04893 avg_loss = 3.46425\n",
      "epoch no.2 train no.161140  loss = 2.11965 avg_loss = 3.44707\n",
      "epoch no.2 train no.161150  loss = 3.37618 avg_loss = 3.44194\n",
      "epoch no.2 train no.161160  loss = 3.35681 avg_loss = 3.45513\n",
      "epoch no.2 train no.161170  loss = 3.74609 avg_loss = 3.48785\n",
      "epoch no.2 train no.161180  loss = 3.50284 avg_loss = 3.47414\n",
      "epoch no.2 train no.161190  loss = 3.61958 avg_loss = 3.49483\n",
      "epoch no.2 train no.161200  loss = 2.65806 avg_loss = 3.49890\n",
      "epoch no.2 train no.161210  loss = 4.65472 avg_loss = 3.52214\n",
      "epoch no.2 train no.161220  loss = 3.96801 avg_loss = 3.51258\n",
      "epoch no.2 train no.161230  loss = 3.78773 avg_loss = 3.47739\n",
      "epoch no.2 train no.161240  loss = 5.33949 avg_loss = 3.52030\n",
      "epoch no.2 train no.161250  loss = 2.32193 avg_loss = 3.52605\n",
      "epoch no.2 train no.161260  loss = 4.68421 avg_loss = 3.56374\n",
      "epoch no.2 train no.161270  loss = 3.07883 avg_loss = 3.54621\n",
      "epoch no.2 train no.161280  loss = 2.91172 avg_loss = 3.55183\n",
      "epoch no.2 train no.161290  loss = 4.48455 avg_loss = 3.55521\n",
      "epoch no.2 train no.161300  loss = 1.01797 avg_loss = 3.52937\n",
      "epoch no.2 train no.161310  loss = 3.52759 avg_loss = 3.51325\n",
      "epoch no.2 train no.161320  loss = 3.68432 avg_loss = 3.50119\n",
      "epoch no.2 train no.161330  loss = 1.73249 avg_loss = 3.48727\n",
      "epoch no.2 train no.161340  loss = 3.98297 avg_loss = 3.50244\n",
      "epoch no.2 train no.161350  loss = 2.31719 avg_loss = 3.50613\n",
      "epoch no.2 train no.161360  loss = 2.44868 avg_loss = 3.47655\n",
      "epoch no.2 train no.161370  loss = 3.17298 avg_loss = 3.46697\n",
      "epoch no.2 train no.161380  loss = 2.71831 avg_loss = 3.49091\n",
      "epoch no.2 train no.161390  loss = 4.24089 avg_loss = 3.45818\n",
      "epoch no.2 train no.161400  loss = 3.43580 avg_loss = 3.41554\n",
      "epoch no.2 train no.161410  loss = 4.26836 avg_loss = 3.40998\n",
      "epoch no.2 train no.161420  loss = 2.83517 avg_loss = 3.40426\n",
      "epoch no.2 train no.161430  loss = 4.22546 avg_loss = 3.41764\n",
      "epoch no.2 train no.161440  loss = 3.20165 avg_loss = 3.44776\n",
      "epoch no.2 train no.161450  loss = 4.73840 avg_loss = 3.45424\n",
      "epoch no.2 train no.161460  loss = 3.76917 avg_loss = 3.47650\n",
      "epoch no.2 train no.161470  loss = 4.84012 avg_loss = 3.44699\n",
      "epoch no.2 train no.161480  loss = 1.81269 avg_loss = 3.47405\n",
      "epoch no.2 train no.161490  loss = 3.38562 avg_loss = 3.46373\n",
      "epoch no.2 train no.161500  loss = 3.34151 avg_loss = 3.43860\n",
      "epoch no.2 train no.161510  loss = 2.41561 avg_loss = 3.43703\n",
      "epoch no.2 train no.161520  loss = 2.55876 avg_loss = 3.41519\n",
      "epoch no.2 train no.161530  loss = 3.72696 avg_loss = 3.46086\n",
      "epoch no.2 train no.161540  loss = 4.05001 avg_loss = 3.47700\n",
      "epoch no.2 train no.161550  loss = 3.33448 avg_loss = 3.49628\n",
      "epoch no.2 train no.161560  loss = 5.02177 avg_loss = 3.48675\n",
      "epoch no.2 train no.161570  loss = 4.48023 avg_loss = 3.48632\n",
      "epoch no.2 train no.161580  loss = 3.33829 avg_loss = 3.48308\n",
      "epoch no.2 train no.161590  loss = 2.95973 avg_loss = 3.47311\n",
      "epoch no.2 train no.161600  loss = 2.98517 avg_loss = 3.42763\n",
      "epoch no.2 train no.161610  loss = 3.01734 avg_loss = 3.43540\n",
      "epoch no.2 train no.161620  loss = 3.39055 avg_loss = 3.42192\n",
      "epoch no.2 train no.161630  loss = 1.48931 avg_loss = 3.38329\n",
      "epoch no.2 train no.161640  loss = 2.41597 avg_loss = 3.41896\n",
      "epoch no.2 train no.161650  loss = 4.62267 avg_loss = 3.44642\n",
      "epoch no.2 train no.161660  loss = 3.36370 avg_loss = 3.46516\n",
      "epoch no.2 train no.161670  loss = 3.50469 avg_loss = 3.48151\n",
      "epoch no.2 train no.161680  loss = 3.52528 avg_loss = 3.43129\n",
      "epoch no.2 train no.161690  loss = 3.24031 avg_loss = 3.47857\n",
      "epoch no.2 train no.161700  loss = 3.01565 avg_loss = 3.43489\n",
      "epoch no.2 train no.161710  loss = 3.22665 avg_loss = 3.50297\n",
      "epoch no.2 train no.161720  loss = 2.62288 avg_loss = 3.46764\n",
      "epoch no.2 train no.161730  loss = 2.01913 avg_loss = 3.47403\n",
      "epoch no.2 train no.161740  loss = 2.87125 avg_loss = 3.45519\n",
      "epoch no.2 train no.161750  loss = 3.58373 avg_loss = 3.44284\n",
      "epoch no.2 train no.161760  loss = 2.57449 avg_loss = 3.43667\n",
      "epoch no.2 train no.161770  loss = 3.42986 avg_loss = 3.51792\n",
      "epoch no.2 train no.161780  loss = 4.96054 avg_loss = 3.52126\n",
      "epoch no.2 train no.161790  loss = 3.72538 avg_loss = 3.51655\n",
      "epoch no.2 train no.161800  loss = 4.60469 avg_loss = 3.51694\n",
      "epoch no.2 train no.161810  loss = 4.55258 avg_loss = 3.58083\n",
      "epoch no.2 train no.161820  loss = 2.86727 avg_loss = 3.57530\n",
      "epoch no.2 train no.161830  loss = 4.51996 avg_loss = 3.55472\n",
      "epoch no.2 train no.161840  loss = 3.95318 avg_loss = 3.54471\n",
      "epoch no.2 train no.161850  loss = 3.07275 avg_loss = 3.52974\n",
      "epoch no.2 train no.161860  loss = 2.35633 avg_loss = 3.44417\n",
      "epoch no.2 train no.161870  loss = 3.03251 avg_loss = 3.43032\n",
      "epoch no.2 train no.161880  loss = 4.21439 avg_loss = 3.41517\n",
      "epoch no.2 train no.161890  loss = 2.42683 avg_loss = 3.39121\n",
      "epoch no.2 train no.161900  loss = 3.25862 avg_loss = 3.36670\n",
      "epoch no.2 train no.161910  loss = 5.30690 avg_loss = 3.35970\n",
      "epoch no.2 train no.161920  loss = 2.91858 avg_loss = 3.38513\n",
      "epoch no.2 train no.161930  loss = 1.86837 avg_loss = 3.34482\n",
      "epoch no.2 train no.161940  loss = 5.70301 avg_loss = 3.39217\n",
      "epoch no.2 train no.161950  loss = 3.46763 avg_loss = 3.36172\n",
      "epoch no.2 train no.161960  loss = 2.98795 avg_loss = 3.34732\n",
      "epoch no.2 train no.161970  loss = 2.37011 avg_loss = 3.31800\n",
      "epoch no.2 train no.161980  loss = 3.06250 avg_loss = 3.35609\n",
      "epoch no.2 train no.161990  loss = 4.26982 avg_loss = 3.33549\n",
      "epoch no.2 train no.162000  loss = 2.70291 avg_loss = 3.30215\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '곡', '들', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.2 train no.162010  loss = 4.01215 avg_loss = 3.35350\n",
      "epoch no.2 train no.162020  loss = 4.76948 avg_loss = 3.33870\n",
      "epoch no.2 train no.162030  loss = 3.47564 avg_loss = 3.35779\n",
      "epoch no.2 train no.162040  loss = 2.24938 avg_loss = 3.38202\n",
      "epoch no.2 train no.162050  loss = 2.73479 avg_loss = 3.37987\n",
      "epoch no.2 train no.162060  loss = 4.29324 avg_loss = 3.36736\n",
      "epoch no.2 train no.162070  loss = 3.38878 avg_loss = 3.41249\n",
      "epoch no.2 train no.162080  loss = 2.40656 avg_loss = 3.36634\n",
      "epoch no.2 train no.162090  loss = 2.82287 avg_loss = 3.37197\n",
      "epoch no.2 train no.162100  loss = 2.02172 avg_loss = 3.38253\n",
      "epoch no.2 train no.162110  loss = 3.20445 avg_loss = 3.42208\n",
      "epoch no.2 train no.162120  loss = 2.11727 avg_loss = 3.40790\n",
      "epoch no.2 train no.162130  loss = 3.46461 avg_loss = 3.39152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.162140  loss = 3.34693 avg_loss = 3.35470\n",
      "epoch no.2 train no.162150  loss = 3.98068 avg_loss = 3.33529\n",
      "epoch no.2 train no.162160  loss = 2.08255 avg_loss = 3.32332\n",
      "epoch no.2 train no.162170  loss = 2.85531 avg_loss = 3.32086\n",
      "epoch no.2 train no.162180  loss = 3.55079 avg_loss = 3.31818\n",
      "epoch no.2 train no.162190  loss = 2.78638 avg_loss = 3.28213\n",
      "epoch no.2 train no.162200  loss = 4.08746 avg_loss = 3.24620\n",
      "epoch no.2 train no.162210  loss = 3.94104 avg_loss = 3.29920\n",
      "epoch no.2 train no.162220  loss = 3.90251 avg_loss = 3.33389\n",
      "epoch no.2 train no.162230  loss = 2.74951 avg_loss = 3.33902\n",
      "epoch no.2 train no.162240  loss = 3.82721 avg_loss = 3.39461\n",
      "epoch no.2 train no.162250  loss = 3.37649 avg_loss = 3.43288\n",
      "epoch no.2 train no.162260  loss = 5.39229 avg_loss = 3.46999\n",
      "epoch no.2 train no.162270  loss = 3.79416 avg_loss = 3.49583\n",
      "epoch no.2 train no.162280  loss = 3.45941 avg_loss = 3.53174\n",
      "epoch no.2 train no.162290  loss = 3.36969 avg_loss = 3.53421\n",
      "epoch no.2 train no.162300  loss = 2.17918 avg_loss = 3.51502\n",
      "epoch no.2 train no.162310  loss = 3.19187 avg_loss = 3.49392\n",
      "epoch no.2 train no.162320  loss = 3.10556 avg_loss = 3.48007\n",
      "epoch no.2 train no.162330  loss = 4.23464 avg_loss = 3.49288\n",
      "epoch no.2 train no.162340  loss = 4.43161 avg_loss = 3.52625\n",
      "epoch no.2 train no.162350  loss = 3.32769 avg_loss = 3.47695\n",
      "epoch no.2 train no.162360  loss = 3.09248 avg_loss = 3.41804\n",
      "epoch no.2 train no.162370  loss = 2.60513 avg_loss = 3.43363\n",
      "epoch no.2 train no.162380  loss = 2.17292 avg_loss = 3.42211\n",
      "epoch no.2 train no.162390  loss = 3.80773 avg_loss = 3.43396\n",
      "epoch no.2 train no.162400  loss = 4.88156 avg_loss = 3.49149\n",
      "epoch no.2 train no.162410  loss = 2.33564 avg_loss = 3.44632\n",
      "epoch no.2 train no.162420  loss = 2.98036 avg_loss = 3.38899\n",
      "epoch no.2 train no.162430  loss = 3.58701 avg_loss = 3.42039\n",
      "epoch no.2 train no.162440  loss = 3.56749 avg_loss = 3.43196\n",
      "epoch no.2 train no.162450  loss = 1.88945 avg_loss = 3.41039\n",
      "epoch no.2 train no.162460  loss = 3.05266 avg_loss = 3.35971\n",
      "epoch no.2 train no.162470  loss = 3.55889 avg_loss = 3.33649\n",
      "epoch no.2 train no.162480  loss = 3.40123 avg_loss = 3.35996\n",
      "epoch no.2 train no.162490  loss = 2.64306 avg_loss = 3.39443\n",
      "epoch no.2 train no.162500  loss = 2.62896 avg_loss = 3.37895\n",
      "epoch no.2 train no.162510  loss = 3.85124 avg_loss = 3.38731\n",
      "epoch no.2 train no.162520  loss = 2.96373 avg_loss = 3.38964\n",
      "epoch no.2 train no.162530  loss = 3.98674 avg_loss = 3.39752\n",
      "epoch no.2 train no.162540  loss = 2.70325 avg_loss = 3.38105\n",
      "epoch no.2 train no.162550  loss = 2.25595 avg_loss = 3.34212\n",
      "epoch no.2 train no.162560  loss = 3.74416 avg_loss = 3.39289\n",
      "epoch no.2 train no.162570  loss = 3.07894 avg_loss = 3.41537\n",
      "epoch no.2 train no.162580  loss = 5.30749 avg_loss = 3.40661\n",
      "epoch no.2 train no.162590  loss = 5.02190 avg_loss = 3.38566\n",
      "epoch no.2 train no.162600  loss = 3.31148 avg_loss = 3.34538\n",
      "epoch no.2 train no.162610  loss = 1.86432 avg_loss = 3.34608\n",
      "epoch no.2 train no.162620  loss = 3.10008 avg_loss = 3.33112\n",
      "epoch no.2 train no.162630  loss = 2.87685 avg_loss = 3.32526\n",
      "epoch no.2 train no.162640  loss = 4.90338 avg_loss = 3.35842\n",
      "epoch no.2 train no.162650  loss = 2.44892 avg_loss = 3.34510\n",
      "epoch no.2 train no.162660  loss = 4.67804 avg_loss = 3.35467\n",
      "epoch no.2 train no.162670  loss = 3.35713 avg_loss = 3.36074\n",
      "epoch no.2 train no.162680  loss = 1.98723 avg_loss = 3.37265\n",
      "epoch no.2 train no.162690  loss = 3.58650 avg_loss = 3.38005\n",
      "epoch no.2 train no.162700  loss = 4.68764 avg_loss = 3.39621\n",
      "epoch no.2 train no.162710  loss = 2.91632 avg_loss = 3.33215\n",
      "epoch no.2 train no.162720  loss = 6.73131 avg_loss = 3.37866\n",
      "epoch no.2 train no.162730  loss = 3.66540 avg_loss = 3.38814\n",
      "epoch no.2 train no.162740  loss = 3.10611 avg_loss = 3.40079\n",
      "epoch no.2 train no.162750  loss = 3.34390 avg_loss = 3.35434\n",
      "epoch no.2 train no.162760  loss = 2.97434 avg_loss = 3.30286\n",
      "epoch no.2 train no.162770  loss = 3.19690 avg_loss = 3.29575\n",
      "epoch no.2 train no.162780  loss = 4.41369 avg_loss = 3.30206\n",
      "epoch no.2 train no.162790  loss = 2.51056 avg_loss = 3.27553\n",
      "epoch no.2 train no.162800  loss = 2.76991 avg_loss = 3.28944\n",
      "epoch no.2 train no.162810  loss = 4.32827 avg_loss = 3.30906\n",
      "epoch no.2 train no.162820  loss = 5.34716 avg_loss = 3.32101\n",
      "epoch no.2 train no.162830  loss = 3.65377 avg_loss = 3.30532\n",
      "epoch no.2 train no.162840  loss = 2.62339 avg_loss = 3.27363\n",
      "epoch no.2 train no.162850  loss = 2.99340 avg_loss = 3.33978\n",
      "epoch no.2 train no.162860  loss = 4.35873 avg_loss = 3.33640\n",
      "epoch no.2 train no.162870  loss = 3.67861 avg_loss = 3.31233\n",
      "epoch no.2 train no.162880  loss = 4.84385 avg_loss = 3.33874\n",
      "epoch no.2 train no.162890  loss = 4.34614 avg_loss = 3.37093\n",
      "epoch no.2 train no.162900  loss = 4.09431 avg_loss = 3.40123\n",
      "epoch no.2 train no.162910  loss = 3.97207 avg_loss = 3.38399\n",
      "epoch no.2 train no.162920  loss = 3.13252 avg_loss = 3.34934\n",
      "epoch no.2 train no.162930  loss = 3.54450 avg_loss = 3.35890\n",
      "epoch no.2 train no.162940  loss = 2.73109 avg_loss = 3.36539\n",
      "epoch no.2 train no.162950  loss = 4.70034 avg_loss = 3.36159\n",
      "epoch no.2 train no.162960  loss = 3.82810 avg_loss = 3.36952\n",
      "epoch no.2 train no.162970  loss = 3.25958 avg_loss = 3.34902\n",
      "epoch no.2 train no.162980  loss = 1.54673 avg_loss = 3.31295\n",
      "epoch no.2 train no.162990  loss = 2.81581 avg_loss = 3.32276\n",
      "epoch no.2 train no.163000  loss = 2.25130 avg_loss = 3.28247\n",
      "5\n",
      "to_tokens: ['▁비', '▁팝', '▁o', 'st', '▁명', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.2 train no.163010  loss = 2.95780 avg_loss = 3.29255\n",
      "epoch no.2 train no.163020  loss = 3.24718 avg_loss = 3.29274\n",
      "epoch no.2 train no.163030  loss = 4.67613 avg_loss = 3.30067\n",
      "epoch no.2 train no.163040  loss = 3.57891 avg_loss = 3.34841\n",
      "epoch no.2 train no.163050  loss = 3.80244 avg_loss = 3.39661\n",
      "epoch no.2 train no.163060  loss = 3.04904 avg_loss = 3.43495\n",
      "epoch no.2 train no.163070  loss = 3.69366 avg_loss = 3.43908\n",
      "epoch no.2 train no.163080  loss = 3.77463 avg_loss = 3.43353\n",
      "epoch no.2 train no.163090  loss = 4.24876 avg_loss = 3.42918\n",
      "epoch no.2 train no.163100  loss = 2.66991 avg_loss = 3.38231\n",
      "epoch no.2 train no.163110  loss = 2.69674 avg_loss = 3.42737\n",
      "epoch no.2 train no.163120  loss = 4.93376 avg_loss = 3.45630\n",
      "epoch no.2 train no.163130  loss = 3.70879 avg_loss = 3.45064\n",
      "epoch no.2 train no.163140  loss = 3.01891 avg_loss = 3.45299\n",
      "epoch no.2 train no.163150  loss = 1.79865 avg_loss = 3.44510\n",
      "epoch no.2 train no.163160  loss = 3.28928 avg_loss = 3.43203\n",
      "epoch no.2 train no.163170  loss = 2.96040 avg_loss = 3.47985\n",
      "epoch no.2 train no.163180  loss = 4.67821 avg_loss = 3.51874\n",
      "epoch no.2 train no.163190  loss = 2.86326 avg_loss = 3.51789\n",
      "epoch no.2 train no.163200  loss = 3.12803 avg_loss = 3.53676\n",
      "epoch no.2 train no.163210  loss = 3.62551 avg_loss = 3.53283\n",
      "epoch no.2 train no.163220  loss = 6.58093 avg_loss = 3.56013\n",
      "epoch no.2 train no.163230  loss = 3.53797 avg_loss = 3.53516\n",
      "epoch no.2 train no.163240  loss = 3.82406 avg_loss = 3.48885\n",
      "epoch no.2 train no.163250  loss = 2.09230 avg_loss = 3.49764\n",
      "epoch no.2 train no.163260  loss = 4.78783 avg_loss = 3.46510\n",
      "epoch no.2 train no.163270  loss = 3.22183 avg_loss = 3.44603\n",
      "epoch no.2 train no.163280  loss = 2.72634 avg_loss = 3.39258\n",
      "epoch no.2 train no.163290  loss = 1.83444 avg_loss = 3.41719\n",
      "epoch no.2 train no.163300  loss = 2.69042 avg_loss = 3.45212\n",
      "epoch no.2 train no.163310  loss = 3.59058 avg_loss = 3.45384\n",
      "epoch no.2 train no.163320  loss = 3.32920 avg_loss = 3.48041\n",
      "epoch no.2 train no.163330  loss = 2.47717 avg_loss = 3.47959\n",
      "epoch no.2 train no.163340  loss = 2.32245 avg_loss = 3.47489\n",
      "epoch no.2 train no.163350  loss = 3.21240 avg_loss = 3.46064\n",
      "epoch no.2 train no.163360  loss = 3.66413 avg_loss = 3.42414\n",
      "epoch no.2 train no.163370  loss = 3.38316 avg_loss = 3.42599\n",
      "epoch no.2 train no.163380  loss = 3.07668 avg_loss = 3.42278\n",
      "epoch no.2 train no.163390  loss = 3.81746 avg_loss = 3.36858\n",
      "epoch no.2 train no.163400  loss = 2.58912 avg_loss = 3.39413\n",
      "epoch no.2 train no.163410  loss = 4.49963 avg_loss = 3.44590\n",
      "epoch no.2 train no.163420  loss = 3.78741 avg_loss = 3.44938\n",
      "epoch no.2 train no.163430  loss = 5.10486 avg_loss = 3.53081\n",
      "epoch no.2 train no.163440  loss = 3.56509 avg_loss = 3.51414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.163450  loss = 2.93661 avg_loss = 3.48388\n",
      "epoch no.2 train no.163460  loss = 2.52269 avg_loss = 3.46056\n",
      "epoch no.2 train no.163470  loss = 4.21093 avg_loss = 3.41703\n",
      "epoch no.2 train no.163480  loss = 3.36753 avg_loss = 3.41588\n",
      "epoch no.2 train no.163490  loss = 4.01908 avg_loss = 3.42531\n",
      "epoch no.2 train no.163500  loss = 3.03215 avg_loss = 3.41574\n",
      "epoch no.2 train no.163510  loss = 4.59374 avg_loss = 3.46116\n",
      "epoch no.2 train no.163520  loss = 3.14450 avg_loss = 3.43915\n",
      "epoch no.2 train no.163530  loss = 2.71777 avg_loss = 3.43248\n",
      "epoch no.2 train no.163540  loss = 4.56685 avg_loss = 3.41773\n",
      "epoch no.2 train no.163550  loss = 3.78759 avg_loss = 3.39356\n",
      "epoch no.2 train no.163560  loss = 3.65105 avg_loss = 3.41374\n",
      "epoch no.2 train no.163570  loss = 1.71631 avg_loss = 3.36969\n",
      "epoch no.2 train no.163580  loss = 3.38730 avg_loss = 3.33162\n",
      "epoch no.2 train no.163590  loss = 2.92739 avg_loss = 3.32748\n",
      "epoch no.2 train no.163600  loss = 3.22646 avg_loss = 3.29205\n",
      "epoch no.2 train no.163610  loss = 2.41563 avg_loss = 3.26235\n",
      "epoch no.2 train no.163620  loss = 2.20376 avg_loss = 3.27220\n",
      "epoch no.2 train no.163630  loss = 2.10589 avg_loss = 3.24513\n",
      "epoch no.2 train no.163640  loss = 4.42670 avg_loss = 3.27076\n",
      "epoch no.2 train no.163650  loss = 3.90588 avg_loss = 3.26749\n",
      "epoch no.2 train no.163660  loss = 2.18146 avg_loss = 3.30528\n",
      "epoch no.2 train no.163670  loss = 3.59951 avg_loss = 3.39699\n",
      "epoch no.2 train no.163680  loss = 2.17848 avg_loss = 3.40716\n",
      "epoch no.2 train no.163690  loss = 2.55423 avg_loss = 3.39791\n",
      "epoch no.2 train no.163700  loss = 5.57032 avg_loss = 3.40023\n",
      "epoch no.2 train no.163710  loss = 3.71501 avg_loss = 3.44219\n",
      "epoch no.2 train no.163720  loss = 2.36785 avg_loss = 3.46157\n",
      "epoch no.2 train no.163730  loss = 2.70918 avg_loss = 3.41910\n",
      "epoch no.2 train no.163740  loss = 3.72048 avg_loss = 3.44378\n",
      "epoch no.2 train no.163750  loss = 2.09488 avg_loss = 3.36859\n",
      "epoch no.2 train no.163760  loss = 2.44665 avg_loss = 3.37224\n",
      "epoch no.2 train no.163770  loss = 2.65660 avg_loss = 3.36177\n",
      "epoch no.2 train no.163780  loss = 3.90241 avg_loss = 3.34588\n",
      "epoch no.2 train no.163790  loss = 4.56486 avg_loss = 3.36824\n",
      "epoch no.2 train no.163800  loss = 5.99954 avg_loss = 3.37420\n",
      "epoch no.2 train no.163810  loss = 1.69444 avg_loss = 3.39907\n",
      "epoch no.2 train no.163820  loss = 4.18371 avg_loss = 3.36134\n",
      "epoch no.2 train no.163830  loss = 3.53504 avg_loss = 3.36231\n",
      "epoch no.2 train no.163840  loss = 4.00527 avg_loss = 3.39301\n",
      "epoch no.2 train no.163850  loss = 3.72188 avg_loss = 3.37805\n",
      "epoch no.2 train no.163860  loss = 3.74482 avg_loss = 3.36479\n",
      "epoch no.2 train no.163870  loss = 4.88832 avg_loss = 3.40194\n",
      "epoch no.2 train no.163880  loss = 1.77842 avg_loss = 3.43551\n",
      "epoch no.2 train no.163890  loss = 2.48847 avg_loss = 3.42259\n",
      "epoch no.2 train no.163900  loss = 2.32806 avg_loss = 3.39552\n",
      "epoch no.2 train no.163910  loss = 2.70979 avg_loss = 3.43753\n",
      "epoch no.2 train no.163920  loss = 4.16026 avg_loss = 3.40246\n",
      "epoch no.2 train no.163930  loss = 4.69473 avg_loss = 3.38905\n",
      "epoch no.2 train no.163940  loss = 3.52426 avg_loss = 3.38572\n",
      "epoch no.2 train no.163950  loss = 2.74655 avg_loss = 3.39574\n",
      "epoch no.2 train no.163960  loss = 3.17424 avg_loss = 3.38300\n",
      "epoch no.2 train no.163970  loss = 4.56706 avg_loss = 3.37842\n",
      "epoch no.2 train no.163980  loss = 4.75563 avg_loss = 3.41728\n",
      "epoch no.2 train no.163990  loss = 4.18459 avg_loss = 3.48508\n",
      "epoch no.2 train no.164000  loss = 3.72572 avg_loss = 3.50121\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁가요', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.2 train no.164010  loss = 3.99109 avg_loss = 3.45998\n",
      "epoch no.2 train no.164020  loss = 4.00714 avg_loss = 3.46503\n",
      "epoch no.2 train no.164030  loss = 5.03412 avg_loss = 3.48211\n",
      "epoch no.2 train no.164040  loss = 2.80351 avg_loss = 3.45502\n",
      "epoch no.2 train no.164050  loss = 2.40419 avg_loss = 3.47711\n",
      "epoch no.2 train no.164060  loss = 3.23979 avg_loss = 3.43764\n",
      "epoch no.2 train no.164070  loss = 2.66221 avg_loss = 3.41343\n",
      "epoch no.2 train no.164080  loss = 3.45175 avg_loss = 3.38877\n",
      "epoch no.2 train no.164090  loss = 4.61127 avg_loss = 3.38282\n",
      "epoch no.2 train no.164100  loss = 2.37916 avg_loss = 3.40788\n",
      "epoch no.2 train no.164110  loss = 3.27566 avg_loss = 3.39309\n",
      "epoch no.2 train no.164120  loss = 2.06459 avg_loss = 3.41475\n",
      "epoch no.2 train no.164130  loss = 4.92421 avg_loss = 3.43796\n",
      "epoch no.2 train no.164140  loss = 1.66960 avg_loss = 3.41986\n",
      "epoch no.2 train no.164150  loss = 3.48914 avg_loss = 3.39835\n",
      "epoch no.2 train no.164160  loss = 2.93569 avg_loss = 3.39333\n",
      "epoch no.2 train no.164170  loss = 3.37803 avg_loss = 3.39988\n",
      "epoch no.2 train no.164180  loss = 5.29075 avg_loss = 3.37885\n",
      "epoch no.2 train no.164190  loss = 3.49005 avg_loss = 3.40834\n",
      "epoch no.2 train no.164200  loss = 3.44332 avg_loss = 3.41697\n",
      "epoch no.2 train no.164210  loss = 3.78572 avg_loss = 3.44594\n",
      "epoch no.2 train no.164220  loss = 2.76301 avg_loss = 3.39953\n",
      "epoch no.2 train no.164230  loss = 3.42209 avg_loss = 3.46268\n",
      "epoch no.2 train no.164240  loss = 3.39860 avg_loss = 3.42833\n",
      "epoch no.2 train no.164250  loss = 4.19936 avg_loss = 3.47282\n",
      "epoch no.2 train no.164260  loss = 5.14799 avg_loss = 3.48710\n",
      "epoch no.2 train no.164270  loss = 3.14958 avg_loss = 3.46445\n",
      "epoch no.2 train no.164280  loss = 3.45669 avg_loss = 3.42661\n",
      "epoch no.2 train no.164290  loss = 2.55879 avg_loss = 3.39690\n",
      "epoch no.2 train no.164300  loss = 2.22891 avg_loss = 3.42455\n",
      "epoch no.2 train no.164310  loss = 3.82395 avg_loss = 3.40650\n",
      "epoch no.2 train no.164320  loss = 4.66173 avg_loss = 3.42206\n",
      "epoch no.2 train no.164330  loss = 3.02274 avg_loss = 3.39138\n",
      "epoch no.2 train no.164340  loss = 2.49760 avg_loss = 3.36734\n",
      "epoch no.2 train no.164350  loss = 3.30602 avg_loss = 3.34454\n",
      "epoch no.2 train no.164360  loss = 2.94629 avg_loss = 3.33545\n",
      "epoch no.2 train no.164370  loss = 3.14591 avg_loss = 3.33504\n",
      "epoch no.2 train no.164380  loss = 5.38116 avg_loss = 3.30655\n",
      "epoch no.2 train no.164390  loss = 4.95753 avg_loss = 3.31556\n",
      "epoch no.2 train no.164400  loss = 3.06945 avg_loss = 3.31230\n",
      "epoch no.2 train no.164410  loss = 5.88683 avg_loss = 3.30902\n",
      "epoch no.2 train no.164420  loss = 2.31365 avg_loss = 3.29154\n",
      "epoch no.2 train no.164430  loss = 6.82890 avg_loss = 3.33163\n",
      "epoch no.2 train no.164440  loss = 3.30545 avg_loss = 3.35168\n",
      "epoch no.2 train no.164450  loss = 2.88586 avg_loss = 3.32964\n",
      "epoch no.2 train no.164460  loss = 2.62589 avg_loss = 3.30896\n",
      "epoch no.2 train no.164470  loss = 3.01135 avg_loss = 3.30643\n",
      "epoch no.2 train no.164480  loss = 3.77959 avg_loss = 3.34132\n",
      "epoch no.2 train no.164490  loss = 2.64209 avg_loss = 3.33913\n",
      "epoch no.2 train no.164500  loss = 2.69091 avg_loss = 3.29541\n",
      "epoch no.2 train no.164510  loss = 3.59251 avg_loss = 3.34318\n",
      "epoch no.2 train no.164520  loss = 5.00227 avg_loss = 3.36731\n",
      "epoch no.2 train no.164530  loss = 3.74912 avg_loss = 3.40185\n",
      "epoch no.2 train no.164540  loss = 3.35035 avg_loss = 3.37108\n",
      "epoch no.2 train no.164550  loss = 3.57138 avg_loss = 3.34762\n",
      "epoch no.2 train no.164560  loss = 3.25143 avg_loss = 3.35060\n",
      "epoch no.2 train no.164570  loss = 3.77112 avg_loss = 3.33490\n",
      "epoch no.2 train no.164580  loss = 2.05415 avg_loss = 3.33294\n",
      "epoch no.2 train no.164590  loss = 3.00270 avg_loss = 3.29432\n",
      "epoch no.2 train no.164600  loss = 4.64390 avg_loss = 3.34320\n",
      "epoch no.2 train no.164610  loss = 4.54537 avg_loss = 3.32689\n",
      "epoch no.2 train no.164620  loss = 2.06984 avg_loss = 3.36969\n",
      "epoch no.2 train no.164630  loss = 3.15667 avg_loss = 3.41241\n",
      "epoch no.2 train no.164640  loss = 2.38433 avg_loss = 3.37626\n",
      "epoch no.2 train no.164650  loss = 2.78109 avg_loss = 3.41564\n",
      "epoch no.2 train no.164660  loss = 4.35402 avg_loss = 3.40371\n",
      "epoch no.2 train no.164670  loss = 3.79434 avg_loss = 3.40507\n",
      "epoch no.2 train no.164680  loss = 2.41330 avg_loss = 3.36411\n",
      "epoch no.2 train no.164690  loss = 4.65607 avg_loss = 3.37498\n",
      "epoch no.2 train no.164700  loss = 4.40345 avg_loss = 3.40059\n",
      "epoch no.2 train no.164710  loss = 3.59538 avg_loss = 3.38669\n",
      "epoch no.2 train no.164720  loss = 4.66653 avg_loss = 3.41429\n",
      "epoch no.2 train no.164730  loss = 4.09125 avg_loss = 3.40500\n",
      "epoch no.2 train no.164740  loss = 4.53598 avg_loss = 3.43044\n",
      "epoch no.2 train no.164750  loss = 5.47780 avg_loss = 3.41657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.164760  loss = 3.94783 avg_loss = 3.45267\n",
      "epoch no.2 train no.164770  loss = 4.21589 avg_loss = 3.50859\n",
      "epoch no.2 train no.164780  loss = 3.91242 avg_loss = 3.47758\n",
      "epoch no.2 train no.164790  loss = 2.35338 avg_loss = 3.42905\n",
      "epoch no.2 train no.164800  loss = 3.07901 avg_loss = 3.41424\n",
      "epoch no.2 train no.164810  loss = 2.76209 avg_loss = 3.40395\n",
      "epoch no.2 train no.164820  loss = 2.88047 avg_loss = 3.40199\n",
      "epoch no.2 train no.164830  loss = 3.75399 avg_loss = 3.39774\n",
      "epoch no.2 train no.164840  loss = 3.01742 avg_loss = 3.36980\n",
      "epoch no.2 train no.164850  loss = 2.68484 avg_loss = 3.33475\n",
      "epoch no.2 train no.164860  loss = 4.05846 avg_loss = 3.30170\n",
      "epoch no.2 train no.164870  loss = 3.05121 avg_loss = 3.27640\n",
      "epoch no.2 train no.164880  loss = 2.68294 avg_loss = 3.28529\n",
      "epoch no.2 train no.164890  loss = 3.84180 avg_loss = 3.24857\n",
      "epoch no.2 train no.164900  loss = 2.64707 avg_loss = 3.29556\n",
      "epoch no.2 train no.164910  loss = 4.04358 avg_loss = 3.29610\n",
      "epoch no.2 train no.164920  loss = 3.22230 avg_loss = 3.34410\n",
      "epoch no.2 train no.164930  loss = 3.02024 avg_loss = 3.38028\n",
      "epoch no.2 train no.164940  loss = 2.43866 avg_loss = 3.44812\n",
      "epoch no.2 train no.164950  loss = 5.16108 avg_loss = 3.46341\n",
      "epoch no.2 train no.164960  loss = 2.53219 avg_loss = 3.44833\n",
      "epoch no.2 train no.164970  loss = 3.26469 avg_loss = 3.48414\n",
      "epoch no.2 train no.164980  loss = 5.76055 avg_loss = 3.52256\n",
      "epoch no.2 train no.164990  loss = 4.67546 avg_loss = 3.52880\n",
      "epoch no.2 train no.165000  loss = 3.17899 avg_loss = 3.47706\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>', '</s>']\n",
      "추억의 2000년대 발라드 모음</s>\n",
      "epoch no.2 train no.165010  loss = 4.41532 avg_loss = 3.48691\n",
      "epoch no.2 train no.165020  loss = 5.31790 avg_loss = 3.50473\n",
      "epoch no.2 train no.165030  loss = 2.44320 avg_loss = 3.47293\n",
      "epoch no.2 train no.165040  loss = 4.63731 avg_loss = 3.46051\n",
      "epoch no.2 train no.165050  loss = 3.44614 avg_loss = 3.45443\n",
      "epoch no.2 train no.165060  loss = 3.27116 avg_loss = 3.41248\n",
      "epoch no.2 train no.165070  loss = 5.03600 avg_loss = 3.40876\n",
      "epoch no.2 train no.165080  loss = 4.31238 avg_loss = 3.45653\n",
      "epoch no.2 train no.165090  loss = 2.60600 avg_loss = 3.44681\n",
      "epoch no.2 train no.165100  loss = 5.43927 avg_loss = 3.46960\n",
      "epoch no.2 train no.165110  loss = 1.84611 avg_loss = 3.46288\n",
      "epoch no.2 train no.165120  loss = 2.94771 avg_loss = 3.46427\n",
      "epoch no.2 train no.165130  loss = 3.84635 avg_loss = 3.46478\n",
      "epoch no.2 train no.165140  loss = 2.48234 avg_loss = 3.45188\n",
      "epoch no.2 train no.165150  loss = 2.11848 avg_loss = 3.45449\n",
      "epoch no.2 train no.165160  loss = 5.42984 avg_loss = 3.46592\n",
      "epoch no.2 train no.165170  loss = 3.19927 avg_loss = 3.48105\n",
      "epoch no.2 train no.165180  loss = 5.74627 avg_loss = 3.55326\n",
      "epoch no.2 train no.165190  loss = 4.19000 avg_loss = 3.54258\n",
      "epoch no.2 train no.165200  loss = 4.10641 avg_loss = 3.52269\n",
      "epoch no.2 train no.165210  loss = 4.50459 avg_loss = 3.50052\n",
      "epoch no.2 train no.165220  loss = 5.44861 avg_loss = 3.58563\n",
      "epoch no.2 train no.165230  loss = 3.58530 avg_loss = 3.54909\n",
      "epoch no.2 train no.165240  loss = 3.19675 avg_loss = 3.55036\n",
      "epoch no.2 train no.165250  loss = 4.12044 avg_loss = 3.56032\n",
      "epoch no.2 train no.165260  loss = 2.60610 avg_loss = 3.49548\n",
      "epoch no.2 train no.165270  loss = 3.44005 avg_loss = 3.50144\n",
      "epoch no.2 train no.165280  loss = 3.14905 avg_loss = 3.49462\n",
      "epoch no.2 train no.165290  loss = 3.50120 avg_loss = 3.52060\n",
      "epoch no.2 train no.165300  loss = 5.19847 avg_loss = 3.51991\n",
      "epoch no.2 train no.165310  loss = 3.17409 avg_loss = 3.54018\n",
      "epoch no.2 train no.165320  loss = 3.29436 avg_loss = 3.51485\n",
      "epoch no.2 train no.165330  loss = 5.01097 avg_loss = 3.56184\n",
      "epoch no.2 train no.165340  loss = 2.52389 avg_loss = 3.53766\n",
      "epoch no.2 train no.165350  loss = 4.06359 avg_loss = 3.51313\n",
      "epoch no.2 train no.165360  loss = 3.81185 avg_loss = 3.50633\n",
      "epoch no.2 train no.165370  loss = 2.42928 avg_loss = 3.56836\n",
      "epoch no.2 train no.165380  loss = 2.83356 avg_loss = 3.49912\n",
      "epoch no.2 train no.165390  loss = 2.49929 avg_loss = 3.46586\n",
      "epoch no.2 train no.165400  loss = 2.39556 avg_loss = 3.45894\n",
      "epoch no.2 train no.165410  loss = 3.80765 avg_loss = 3.47971\n",
      "epoch no.2 train no.165420  loss = 2.53643 avg_loss = 3.43542\n",
      "epoch no.2 train no.165430  loss = 4.71820 avg_loss = 3.41775\n",
      "epoch no.2 train no.165440  loss = 2.91947 avg_loss = 3.45003\n",
      "epoch no.2 train no.165450  loss = 2.62822 avg_loss = 3.43297\n",
      "epoch no.2 train no.165460  loss = 3.77199 avg_loss = 3.41830\n",
      "epoch no.2 train no.165470  loss = 3.91940 avg_loss = 3.41452\n",
      "epoch no.2 train no.165480  loss = 3.19619 avg_loss = 3.36678\n",
      "epoch no.2 train no.165490  loss = 3.64709 avg_loss = 3.39999\n",
      "epoch no.2 train no.165500  loss = 3.44575 avg_loss = 3.39593\n",
      "epoch no.2 train no.165510  loss = 3.97738 avg_loss = 3.38141\n",
      "epoch no.2 train no.165520  loss = 4.24172 avg_loss = 3.37256\n",
      "epoch no.2 train no.165530  loss = 4.37373 avg_loss = 3.44853\n",
      "epoch no.2 train no.165540  loss = 4.86821 avg_loss = 3.46469\n",
      "epoch no.2 train no.165550  loss = 4.68637 avg_loss = 3.47293\n",
      "epoch no.2 train no.165560  loss = 3.02441 avg_loss = 3.44115\n",
      "epoch no.2 train no.165570  loss = 4.16508 avg_loss = 3.50121\n",
      "epoch no.2 train no.165580  loss = 3.37396 avg_loss = 3.49238\n",
      "epoch no.2 train no.165590  loss = 2.44090 avg_loss = 3.50142\n",
      "epoch no.2 train no.165600  loss = 3.71787 avg_loss = 3.50218\n",
      "epoch no.2 train no.165610  loss = 3.95941 avg_loss = 3.44793\n",
      "epoch no.2 train no.165620  loss = 4.71048 avg_loss = 3.50114\n",
      "epoch no.2 train no.165630  loss = 2.76602 avg_loss = 3.50733\n",
      "epoch no.2 train no.165640  loss = 4.44281 avg_loss = 3.52042\n",
      "epoch no.2 train no.165650  loss = 4.68056 avg_loss = 3.54988\n",
      "epoch no.2 train no.165660  loss = 2.20828 avg_loss = 3.52329\n",
      "epoch no.2 train no.165670  loss = 2.71629 avg_loss = 3.57716\n",
      "epoch no.2 train no.165680  loss = 7.06973 avg_loss = 3.63527\n",
      "epoch no.2 train no.165690  loss = 4.94602 avg_loss = 3.63451\n",
      "epoch no.2 train no.165700  loss = 3.96699 avg_loss = 3.64963\n",
      "epoch no.2 train no.165710  loss = 3.25673 avg_loss = 3.62051\n",
      "epoch no.2 train no.165720  loss = 4.19652 avg_loss = 3.62749\n",
      "epoch no.2 train no.165730  loss = 3.21373 avg_loss = 3.63213\n",
      "epoch no.2 train no.165740  loss = 3.89124 avg_loss = 3.60544\n",
      "epoch no.2 train no.165750  loss = 3.29782 avg_loss = 3.55786\n",
      "epoch no.2 train no.165760  loss = 4.32854 avg_loss = 3.61247\n",
      "epoch no.2 train no.165770  loss = 3.09654 avg_loss = 3.60304\n",
      "epoch no.2 train no.165780  loss = 2.74272 avg_loss = 3.56798\n",
      "epoch no.2 train no.165790  loss = 2.98704 avg_loss = 3.51715\n",
      "epoch no.2 train no.165800  loss = 3.44832 avg_loss = 3.49642\n",
      "epoch no.2 train no.165810  loss = 4.19553 avg_loss = 3.53591\n",
      "epoch no.2 train no.165820  loss = 3.43526 avg_loss = 3.53343\n",
      "epoch no.2 train no.165830  loss = 2.59616 avg_loss = 3.51751\n",
      "epoch no.2 train no.165840  loss = 3.89930 avg_loss = 3.46073\n",
      "epoch no.2 train no.165850  loss = 3.50370 avg_loss = 3.46358\n",
      "epoch no.2 train no.165860  loss = 4.08176 avg_loss = 3.47606\n",
      "epoch no.2 train no.165870  loss = 1.93325 avg_loss = 3.45841\n",
      "epoch no.2 train no.165880  loss = 4.43631 avg_loss = 3.47664\n",
      "epoch no.2 train no.165890  loss = 2.83843 avg_loss = 3.48616\n",
      "epoch no.2 train no.165900  loss = 2.30626 avg_loss = 3.45948\n",
      "epoch no.2 train no.165910  loss = 2.73080 avg_loss = 3.49359\n",
      "epoch no.2 train no.165920  loss = 4.49606 avg_loss = 3.49950\n",
      "epoch no.2 train no.165930  loss = 5.50631 avg_loss = 3.50981\n",
      "epoch no.2 train no.165940  loss = 3.22774 avg_loss = 3.49064\n",
      "epoch no.2 train no.165950  loss = 2.62723 avg_loss = 3.48222\n",
      "epoch no.2 train no.165960  loss = 4.21510 avg_loss = 3.50902\n",
      "epoch no.2 train no.165970  loss = 3.63993 avg_loss = 3.50775\n",
      "epoch no.2 train no.165980  loss = 3.32028 avg_loss = 3.49967\n",
      "epoch no.2 train no.165990  loss = 2.76202 avg_loss = 3.52555\n",
      "epoch no.2 train no.166000  loss = 3.59557 avg_loss = 3.51815\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁가요', '모', '</s>']\n",
      "추억의 2000년대 가요 모음</s>\n",
      "epoch no.2 train no.166010  loss = 3.42477 avg_loss = 3.54740\n",
      "epoch no.2 train no.166020  loss = 3.79419 avg_loss = 3.52260\n",
      "epoch no.2 train no.166030  loss = 2.66497 avg_loss = 3.46863\n",
      "epoch no.2 train no.166040  loss = 2.48262 avg_loss = 3.44621\n",
      "epoch no.2 train no.166050  loss = 2.78492 avg_loss = 3.41411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.166060  loss = 3.25960 avg_loss = 3.41290\n",
      "epoch no.2 train no.166070  loss = 3.77703 avg_loss = 3.40548\n",
      "epoch no.2 train no.166080  loss = 2.61460 avg_loss = 3.43500\n",
      "epoch no.2 train no.166090  loss = 3.17640 avg_loss = 3.45655\n",
      "epoch no.2 train no.166100  loss = 3.20902 avg_loss = 3.49943\n",
      "epoch no.2 train no.166110  loss = 4.54639 avg_loss = 3.51795\n",
      "epoch no.2 train no.166120  loss = 2.28772 avg_loss = 3.49164\n",
      "epoch no.2 train no.166130  loss = 2.43171 avg_loss = 3.50599\n",
      "epoch no.2 train no.166140  loss = 4.92696 avg_loss = 3.52794\n",
      "epoch no.2 train no.166150  loss = 2.97371 avg_loss = 3.52359\n",
      "epoch no.2 train no.166160  loss = 3.33835 avg_loss = 3.58730\n",
      "epoch no.2 train no.166170  loss = 5.10138 avg_loss = 3.57953\n",
      "epoch no.2 train no.166180  loss = 4.42041 avg_loss = 3.56525\n",
      "epoch no.2 train no.166190  loss = 4.81721 avg_loss = 3.55923\n",
      "epoch no.2 train no.166200  loss = 3.74337 avg_loss = 3.59155\n",
      "epoch no.2 train no.166210  loss = 3.90442 avg_loss = 3.59561\n",
      "epoch no.2 train no.166220  loss = 3.07378 avg_loss = 3.60011\n",
      "epoch no.2 train no.166230  loss = 4.04687 avg_loss = 3.55497\n",
      "epoch no.2 train no.166240  loss = 2.98271 avg_loss = 3.57798\n",
      "epoch no.2 train no.166250  loss = 5.12718 avg_loss = 3.51327\n",
      "epoch no.2 train no.166260  loss = 2.66869 avg_loss = 3.55587\n",
      "epoch no.2 train no.166270  loss = 3.69347 avg_loss = 3.54358\n",
      "epoch no.2 train no.166280  loss = 4.14454 avg_loss = 3.57008\n",
      "epoch no.2 train no.166290  loss = 1.77576 avg_loss = 3.61368\n",
      "epoch no.2 train no.166300  loss = 1.86765 avg_loss = 3.57780\n",
      "epoch no.2 train no.166310  loss = 1.99887 avg_loss = 3.57099\n",
      "epoch no.2 train no.166320  loss = 2.52531 avg_loss = 3.58168\n",
      "epoch no.2 train no.166330  loss = 2.45938 avg_loss = 3.59224\n",
      "epoch no.2 train no.166340  loss = 2.35627 avg_loss = 3.56008\n",
      "epoch no.2 train no.166350  loss = 2.74111 avg_loss = 3.56360\n",
      "epoch no.2 train no.166360  loss = 3.42334 avg_loss = 3.57095\n",
      "epoch no.2 train no.166370  loss = 3.89421 avg_loss = 3.56574\n",
      "epoch no.2 train no.166380  loss = 4.11373 avg_loss = 3.57527\n",
      "epoch no.2 train no.166390  loss = 3.81499 avg_loss = 3.56259\n",
      "epoch no.2 train no.166400  loss = 5.12088 avg_loss = 3.53853\n",
      "epoch no.2 train no.166410  loss = 2.34918 avg_loss = 3.51340\n",
      "epoch no.2 train no.166420  loss = 4.18000 avg_loss = 3.55650\n",
      "epoch no.2 train no.166430  loss = 2.71403 avg_loss = 3.55035\n",
      "epoch no.2 train no.166440  loss = 4.50494 avg_loss = 3.53710\n",
      "epoch no.2 train no.166450  loss = 3.66984 avg_loss = 3.54612\n",
      "epoch no.2 train no.166460  loss = 3.77110 avg_loss = 3.50492\n",
      "epoch no.2 train no.166470  loss = 2.46650 avg_loss = 3.50629\n",
      "epoch no.2 train no.166480  loss = 3.00363 avg_loss = 3.47488\n",
      "epoch no.2 train no.166490  loss = 1.93337 avg_loss = 3.45299\n",
      "epoch no.2 train no.166500  loss = 3.48790 avg_loss = 3.48833\n",
      "epoch no.2 train no.166510  loss = 5.01387 avg_loss = 3.47595\n",
      "epoch no.2 train no.166520  loss = 3.09002 avg_loss = 3.43799\n",
      "epoch no.2 train no.166530  loss = 4.89801 avg_loss = 3.45644\n",
      "epoch no.2 train no.166540  loss = 2.77695 avg_loss = 3.43388\n",
      "epoch no.2 train no.166550  loss = 2.36956 avg_loss = 3.40590\n",
      "epoch no.2 train no.166560  loss = 3.80100 avg_loss = 3.40452\n",
      "epoch no.2 train no.166570  loss = 2.27587 avg_loss = 3.37987\n",
      "epoch no.2 train no.166580  loss = 2.53502 avg_loss = 3.39206\n",
      "epoch no.2 train no.166590  loss = 3.08376 avg_loss = 3.44196\n",
      "epoch no.2 train no.166600  loss = 4.80836 avg_loss = 3.45471\n",
      "epoch no.2 train no.166610  loss = 2.81492 avg_loss = 3.46384\n",
      "epoch no.2 train no.166620  loss = 1.49650 avg_loss = 3.42165\n",
      "epoch no.2 train no.166630  loss = 2.29598 avg_loss = 3.37408\n",
      "epoch no.2 train no.166640  loss = 3.89138 avg_loss = 3.36631\n",
      "epoch no.2 train no.166650  loss = 5.28738 avg_loss = 3.42944\n",
      "epoch no.2 train no.166660  loss = 2.42481 avg_loss = 3.42038\n",
      "epoch no.2 train no.166670  loss = 4.53739 avg_loss = 3.43644\n",
      "epoch no.2 train no.166680  loss = 2.41368 avg_loss = 3.46813\n",
      "epoch no.2 train no.166690  loss = 4.53532 avg_loss = 3.42909\n",
      "epoch no.2 train no.166700  loss = 4.32740 avg_loss = 3.43937\n",
      "epoch no.2 train no.166710  loss = 2.06620 avg_loss = 3.36428\n",
      "epoch no.2 train no.166720  loss = 2.54548 avg_loss = 3.34309\n",
      "epoch no.2 train no.166730  loss = 3.98830 avg_loss = 3.37116\n",
      "epoch no.2 train no.166740  loss = 2.53558 avg_loss = 3.41298\n",
      "epoch no.2 train no.166750  loss = 2.89454 avg_loss = 3.42942\n",
      "epoch no.2 train no.166760  loss = 2.38136 avg_loss = 3.43719\n",
      "epoch no.2 train no.166770  loss = 2.60712 avg_loss = 3.44983\n",
      "epoch no.2 train no.166780  loss = 2.65645 avg_loss = 3.44292\n",
      "epoch no.2 train no.166790  loss = 5.25943 avg_loss = 3.44350\n",
      "epoch no.2 train no.166800  loss = 3.98436 avg_loss = 3.43681\n",
      "epoch no.2 train no.166810  loss = 3.54333 avg_loss = 3.42784\n",
      "epoch no.2 train no.166820  loss = 2.66134 avg_loss = 3.42138\n",
      "epoch no.2 train no.166830  loss = 3.59558 avg_loss = 3.36298\n",
      "epoch no.2 train no.166840  loss = 2.72428 avg_loss = 3.39093\n",
      "epoch no.2 train no.166850  loss = 2.98968 avg_loss = 3.45956\n",
      "epoch no.2 train no.166860  loss = 3.20420 avg_loss = 3.51842\n",
      "epoch no.2 train no.166870  loss = 3.32403 avg_loss = 3.47784\n",
      "epoch no.2 train no.166880  loss = 1.68918 avg_loss = 3.42463\n",
      "epoch no.2 train no.166890  loss = 4.64181 avg_loss = 3.43633\n",
      "epoch no.2 train no.166900  loss = 3.59236 avg_loss = 3.51029\n",
      "epoch no.2 train no.166910  loss = 4.10004 avg_loss = 3.53981\n",
      "epoch no.2 train no.166920  loss = 4.30828 avg_loss = 3.54944\n",
      "epoch no.2 train no.166930  loss = 4.30794 avg_loss = 3.54975\n",
      "epoch no.2 train no.166940  loss = 2.40791 avg_loss = 3.51916\n",
      "epoch no.2 train no.166950  loss = 2.99107 avg_loss = 3.52387\n",
      "epoch no.2 train no.166960  loss = 2.75336 avg_loss = 3.49908\n",
      "epoch no.2 train no.166970  loss = 3.16190 avg_loss = 3.49111\n",
      "epoch no.2 train no.166980  loss = 2.20571 avg_loss = 3.45836\n",
      "epoch no.2 train no.166990  loss = 3.73371 avg_loss = 3.43954\n",
      "epoch no.2 train no.167000  loss = 2.83607 avg_loss = 3.44593\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '곡', '들', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.2 train no.167010  loss = 4.12712 avg_loss = 3.45074\n",
      "epoch no.2 train no.167020  loss = 3.42943 avg_loss = 3.37751\n",
      "epoch no.2 train no.167030  loss = 5.03876 avg_loss = 3.36424\n",
      "epoch no.2 train no.167040  loss = 4.44609 avg_loss = 3.41344\n",
      "epoch no.2 train no.167050  loss = 2.77755 avg_loss = 3.41982\n",
      "epoch no.2 train no.167060  loss = 3.41594 avg_loss = 3.44493\n",
      "epoch no.2 train no.167070  loss = 5.61585 avg_loss = 3.44586\n",
      "epoch no.2 train no.167080  loss = 4.06514 avg_loss = 3.41178\n",
      "epoch no.2 train no.167090  loss = 4.84299 avg_loss = 3.43533\n",
      "epoch no.2 train no.167100  loss = 3.54352 avg_loss = 3.40255\n",
      "epoch no.2 train no.167110  loss = 4.56259 avg_loss = 3.42420\n",
      "epoch no.2 train no.167120  loss = 6.99255 avg_loss = 3.44795\n",
      "epoch no.2 train no.167130  loss = 3.49221 avg_loss = 3.52643\n",
      "epoch no.2 train no.167140  loss = 2.60233 avg_loss = 3.51465\n",
      "epoch no.2 train no.167150  loss = 4.68157 avg_loss = 3.52089\n",
      "epoch no.2 train no.167160  loss = 3.57568 avg_loss = 3.56027\n",
      "epoch no.2 train no.167170  loss = 3.92395 avg_loss = 3.54899\n",
      "epoch no.2 train no.167180  loss = 4.30030 avg_loss = 3.53616\n",
      "epoch no.2 train no.167190  loss = 3.26538 avg_loss = 3.54130\n",
      "epoch no.2 train no.167200  loss = 3.20029 avg_loss = 3.53691\n",
      "epoch no.2 train no.167210  loss = 1.91799 avg_loss = 3.50899\n",
      "epoch no.2 train no.167220  loss = 4.07108 avg_loss = 3.52031\n",
      "epoch no.2 train no.167230  loss = 3.17900 avg_loss = 3.49164\n",
      "epoch no.2 train no.167240  loss = 4.58837 avg_loss = 3.47990\n",
      "epoch no.2 train no.167250  loss = 2.85781 avg_loss = 3.46519\n",
      "epoch no.2 train no.167260  loss = 4.54983 avg_loss = 3.47954\n",
      "epoch no.2 train no.167270  loss = 3.45757 avg_loss = 3.54029\n",
      "epoch no.2 train no.167280  loss = 4.59133 avg_loss = 3.54102\n",
      "epoch no.2 train no.167290  loss = 3.84658 avg_loss = 3.52524\n",
      "epoch no.2 train no.167300  loss = 2.69156 avg_loss = 3.53104\n",
      "epoch no.2 train no.167310  loss = 2.79288 avg_loss = 3.47145\n",
      "epoch no.2 train no.167320  loss = 2.94685 avg_loss = 3.44600\n",
      "epoch no.2 train no.167330  loss = 3.41096 avg_loss = 3.43814\n",
      "epoch no.2 train no.167340  loss = 5.27519 avg_loss = 3.43665\n",
      "epoch no.2 train no.167350  loss = 3.02429 avg_loss = 3.48482\n",
      "epoch no.2 train no.167360  loss = 4.90415 avg_loss = 3.57176\n",
      "epoch no.2 train no.167370  loss = 2.81378 avg_loss = 3.66163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.167380  loss = 3.61335 avg_loss = 3.66339\n",
      "epoch no.2 train no.167390  loss = 3.52359 avg_loss = 3.63321\n",
      "epoch no.2 train no.167400  loss = 3.58688 avg_loss = 3.61299\n",
      "epoch no.2 train no.167410  loss = 4.00950 avg_loss = 3.56646\n",
      "epoch no.2 train no.167420  loss = 3.18912 avg_loss = 3.52454\n",
      "epoch no.2 train no.167430  loss = 6.15752 avg_loss = 3.57523\n",
      "epoch no.2 train no.167440  loss = 5.04866 avg_loss = 3.56101\n",
      "epoch no.2 train no.167450  loss = 3.55356 avg_loss = 3.56210\n",
      "epoch no.2 train no.167460  loss = 5.37922 avg_loss = 3.56624\n",
      "epoch no.2 train no.167470  loss = 2.78884 avg_loss = 3.60091\n",
      "epoch no.2 train no.167480  loss = 3.66636 avg_loss = 3.58844\n",
      "epoch no.2 train no.167490  loss = 5.34799 avg_loss = 3.59213\n",
      "epoch no.2 train no.167500  loss = 4.00484 avg_loss = 3.62773\n",
      "epoch no.2 train no.167510  loss = 2.83836 avg_loss = 3.58792\n",
      "epoch no.2 train no.167520  loss = 5.56503 avg_loss = 3.58619\n",
      "epoch no.2 train no.167530  loss = 2.74304 avg_loss = 3.55087\n",
      "epoch no.2 train no.167540  loss = 3.10082 avg_loss = 3.50882\n",
      "epoch no.2 train no.167550  loss = 4.05833 avg_loss = 3.50772\n",
      "epoch no.2 train no.167560  loss = 3.96259 avg_loss = 3.54499\n",
      "epoch no.2 train no.167570  loss = 2.77386 avg_loss = 3.51557\n",
      "epoch no.2 train no.167580  loss = 2.61064 avg_loss = 3.49636\n",
      "epoch no.2 train no.167590  loss = 3.84828 avg_loss = 3.52090\n",
      "epoch no.2 train no.167600  loss = 2.45674 avg_loss = 3.47075\n",
      "epoch no.2 train no.167610  loss = 4.11890 avg_loss = 3.42994\n",
      "epoch no.2 train no.167620  loss = 3.25667 avg_loss = 3.43520\n",
      "epoch no.2 train no.167630  loss = 2.48633 avg_loss = 3.43249\n",
      "epoch no.2 train no.167640  loss = 4.32709 avg_loss = 3.45258\n",
      "epoch no.2 train no.167650  loss = 3.65100 avg_loss = 3.41903\n",
      "epoch no.2 train no.167660  loss = 3.30942 avg_loss = 3.42422\n",
      "epoch no.2 train no.167670  loss = 2.36762 avg_loss = 3.38811\n",
      "epoch no.2 train no.167680  loss = 1.30138 avg_loss = 3.34378\n",
      "epoch no.2 train no.167690  loss = 2.40470 avg_loss = 3.32639\n",
      "epoch no.2 train no.167700  loss = 2.95784 avg_loss = 3.35415\n",
      "epoch no.2 train no.167710  loss = 5.78102 avg_loss = 3.34949\n",
      "epoch no.2 train no.167720  loss = 3.76086 avg_loss = 3.32556\n",
      "epoch no.2 train no.167730  loss = 2.86676 avg_loss = 3.30787\n",
      "epoch no.2 train no.167740  loss = 3.88254 avg_loss = 3.33350\n",
      "epoch no.2 train no.167750  loss = 3.51688 avg_loss = 3.33895\n",
      "epoch no.2 train no.167760  loss = 4.09425 avg_loss = 3.36802\n",
      "epoch no.2 train no.167770  loss = 3.15627 avg_loss = 3.41293\n",
      "epoch no.2 train no.167780  loss = 2.89410 avg_loss = 3.47431\n",
      "epoch no.2 train no.167790  loss = 4.25643 avg_loss = 3.54897\n",
      "epoch no.2 train no.167800  loss = 2.81796 avg_loss = 3.48952\n",
      "epoch no.2 train no.167810  loss = 2.92526 avg_loss = 3.49430\n",
      "epoch no.2 train no.167820  loss = 3.10707 avg_loss = 3.45880\n",
      "epoch no.2 train no.167830  loss = 3.86678 avg_loss = 3.44274\n",
      "epoch no.2 train no.167840  loss = 3.09577 avg_loss = 3.46826\n",
      "epoch no.2 train no.167850  loss = 2.35003 avg_loss = 3.40304\n",
      "epoch no.2 train no.167860  loss = 2.48230 avg_loss = 3.39641\n",
      "epoch no.2 train no.167870  loss = 3.08605 avg_loss = 3.40401\n",
      "epoch no.2 train no.167880  loss = 3.19593 avg_loss = 3.38623\n",
      "epoch no.2 train no.167890  loss = 4.55930 avg_loss = 3.42231\n",
      "epoch no.2 train no.167900  loss = 3.62103 avg_loss = 3.37792\n",
      "epoch no.2 train no.167910  loss = 3.36017 avg_loss = 3.33539\n",
      "epoch no.2 train no.167920  loss = 3.07387 avg_loss = 3.37574\n",
      "epoch no.2 train no.167930  loss = 2.57043 avg_loss = 3.43087\n",
      "epoch no.2 train no.167940  loss = 2.46020 avg_loss = 3.40232\n",
      "epoch no.2 train no.167950  loss = 4.19800 avg_loss = 3.41254\n",
      "epoch no.2 train no.167960  loss = 1.69848 avg_loss = 3.38884\n",
      "epoch no.2 train no.167970  loss = 5.83179 avg_loss = 3.38369\n",
      "epoch no.2 train no.167980  loss = 3.28004 avg_loss = 3.38835\n",
      "epoch no.2 train no.167990  loss = 2.92191 avg_loss = 3.42816\n",
      "epoch no.2 train no.168000  loss = 3.01520 avg_loss = 3.48921\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.2 train no.168010  loss = 3.41729 avg_loss = 3.47739\n",
      "epoch no.2 train no.168020  loss = 4.74615 avg_loss = 3.49944\n",
      "epoch no.2 train no.168030  loss = 3.25643 avg_loss = 3.46168\n",
      "epoch no.2 train no.168040  loss = 3.80590 avg_loss = 3.41757\n",
      "epoch no.2 train no.168050  loss = 1.99435 avg_loss = 3.41849\n",
      "epoch no.2 train no.168060  loss = 1.72076 avg_loss = 3.37480\n",
      "epoch no.2 train no.168070  loss = 4.54310 avg_loss = 3.39374\n",
      "epoch no.2 train no.168080  loss = 3.83321 avg_loss = 3.37233\n",
      "epoch no.2 train no.168090  loss = 3.25995 avg_loss = 3.37933\n",
      "epoch no.2 train no.168100  loss = 1.95651 avg_loss = 3.39912\n",
      "epoch no.2 train no.168110  loss = 3.12115 avg_loss = 3.42312\n",
      "epoch no.2 train no.168120  loss = 2.93130 avg_loss = 3.45109\n",
      "epoch no.2 train no.168130  loss = 4.03450 avg_loss = 3.46564\n",
      "epoch no.2 train no.168140  loss = 3.47805 avg_loss = 3.51009\n",
      "epoch no.2 train no.168150  loss = 3.50153 avg_loss = 3.50085\n",
      "epoch no.2 train no.168160  loss = 4.67345 avg_loss = 3.52409\n",
      "epoch no.2 train no.168170  loss = 2.35746 avg_loss = 3.51113\n",
      "epoch no.2 train no.168180  loss = 5.08619 avg_loss = 3.59772\n",
      "epoch no.2 train no.168190  loss = 2.97802 avg_loss = 3.56712\n",
      "epoch no.2 train no.168200  loss = 3.17509 avg_loss = 3.56553\n",
      "epoch no.2 train no.168210  loss = 3.89413 avg_loss = 3.55292\n",
      "epoch no.2 train no.168220  loss = 4.11125 avg_loss = 3.52235\n",
      "epoch no.2 train no.168230  loss = 2.51671 avg_loss = 3.45809\n",
      "epoch no.2 train no.168240  loss = 4.17184 avg_loss = 3.47185\n",
      "epoch no.2 train no.168250  loss = 6.59106 avg_loss = 3.52513\n",
      "epoch no.2 train no.168260  loss = 3.72041 avg_loss = 3.47148\n",
      "epoch no.2 train no.168270  loss = 5.00327 avg_loss = 3.44483\n",
      "epoch no.2 train no.168280  loss = 3.09379 avg_loss = 3.42286\n",
      "epoch no.2 train no.168290  loss = 4.95529 avg_loss = 3.42314\n",
      "epoch no.2 train no.168300  loss = 2.27437 avg_loss = 3.41464\n",
      "epoch no.2 train no.168310  loss = 3.80251 avg_loss = 3.43889\n",
      "epoch no.2 train no.168320  loss = 4.91713 avg_loss = 3.47123\n",
      "epoch no.2 train no.168330  loss = 3.99792 avg_loss = 3.43714\n",
      "epoch no.2 train no.168340  loss = 2.42402 avg_loss = 3.45221\n",
      "epoch no.2 train no.168350  loss = 3.21158 avg_loss = 3.50432\n",
      "epoch no.2 train no.168360  loss = 4.49559 avg_loss = 3.53975\n",
      "epoch no.2 train no.168370  loss = 3.55797 avg_loss = 3.55447\n",
      "epoch no.2 train no.168380  loss = 3.32861 avg_loss = 3.51612\n",
      "epoch no.2 train no.168390  loss = 3.33746 avg_loss = 3.52962\n",
      "epoch no.2 train no.168400  loss = 3.34196 avg_loss = 3.50914\n",
      "epoch no.2 train no.168410  loss = 2.28606 avg_loss = 3.49779\n",
      "epoch no.2 train no.168420  loss = 3.23204 avg_loss = 3.46466\n",
      "epoch no.2 train no.168430  loss = 2.50282 avg_loss = 3.44729\n",
      "epoch no.2 train no.168440  loss = 1.88233 avg_loss = 3.43659\n",
      "epoch no.2 train no.168450  loss = 2.52014 avg_loss = 3.43658\n",
      "epoch no.2 train no.168460  loss = 2.03768 avg_loss = 3.47701\n",
      "epoch no.2 train no.168470  loss = 3.67198 avg_loss = 3.48813\n",
      "epoch no.2 train no.168480  loss = 4.63901 avg_loss = 3.48719\n",
      "epoch no.2 train no.168490  loss = 3.24295 avg_loss = 3.52627\n",
      "epoch no.2 train no.168500  loss = 3.42211 avg_loss = 3.54817\n",
      "epoch no.2 train no.168510  loss = 2.79159 avg_loss = 3.52049\n",
      "epoch no.2 train no.168520  loss = 3.92645 avg_loss = 3.46200\n",
      "epoch no.2 train no.168530  loss = 3.27046 avg_loss = 3.42511\n",
      "epoch no.2 train no.168540  loss = 4.03040 avg_loss = 3.43840\n",
      "epoch no.2 train no.168550  loss = 3.70774 avg_loss = 3.39946\n",
      "epoch no.2 train no.168560  loss = 4.40227 avg_loss = 3.42077\n",
      "epoch no.2 train no.168570  loss = 5.58659 avg_loss = 3.50229\n",
      "epoch no.2 train no.168580  loss = 5.70508 avg_loss = 3.50014\n",
      "epoch no.2 train no.168590  loss = 4.88743 avg_loss = 3.50531\n",
      "epoch no.2 train no.168600  loss = 3.99002 avg_loss = 3.50350\n",
      "epoch no.2 train no.168610  loss = 3.79833 avg_loss = 3.42679\n",
      "epoch no.2 train no.168620  loss = 2.24406 avg_loss = 3.46217\n",
      "epoch no.2 train no.168630  loss = 2.72723 avg_loss = 3.41679\n",
      "epoch no.2 train no.168640  loss = 2.31192 avg_loss = 3.41827\n",
      "epoch no.2 train no.168650  loss = 3.61065 avg_loss = 3.39458\n",
      "epoch no.2 train no.168660  loss = 3.00605 avg_loss = 3.39864\n",
      "epoch no.2 train no.168670  loss = 3.82494 avg_loss = 3.40087\n",
      "epoch no.2 train no.168680  loss = 4.31942 avg_loss = 3.44811\n",
      "epoch no.2 train no.168690  loss = 2.46944 avg_loss = 3.45488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.168700  loss = 3.18593 avg_loss = 3.43852\n",
      "epoch no.2 train no.168710  loss = 4.16667 avg_loss = 3.42535\n",
      "epoch no.2 train no.168720  loss = 2.69576 avg_loss = 3.41395\n",
      "epoch no.2 train no.168730  loss = 3.00737 avg_loss = 3.38408\n",
      "epoch no.2 train no.168740  loss = 3.98192 avg_loss = 3.35103\n",
      "epoch no.2 train no.168750  loss = 2.77763 avg_loss = 3.35011\n",
      "epoch no.2 train no.168760  loss = 4.44603 avg_loss = 3.35448\n",
      "epoch no.2 train no.168770  loss = 2.29955 avg_loss = 3.36988\n",
      "epoch no.2 train no.168780  loss = 2.43405 avg_loss = 3.36475\n",
      "epoch no.2 train no.168790  loss = 3.08381 avg_loss = 3.33638\n",
      "epoch no.2 train no.168800  loss = 3.21402 avg_loss = 3.38439\n",
      "epoch no.2 train no.168810  loss = 4.13970 avg_loss = 3.42531\n",
      "epoch no.2 train no.168820  loss = 3.51162 avg_loss = 3.40943\n",
      "epoch no.2 train no.168830  loss = 4.46643 avg_loss = 3.39977\n",
      "epoch no.2 train no.168840  loss = 4.51085 avg_loss = 3.44211\n",
      "epoch no.2 train no.168850  loss = 2.21627 avg_loss = 3.41315\n",
      "epoch no.2 train no.168860  loss = 2.15021 avg_loss = 3.44600\n",
      "epoch no.2 train no.168870  loss = 3.87707 avg_loss = 3.47975\n",
      "epoch no.2 train no.168880  loss = 3.19479 avg_loss = 3.49207\n",
      "epoch no.2 train no.168890  loss = 4.05913 avg_loss = 3.48589\n",
      "epoch no.2 train no.168900  loss = 4.62425 avg_loss = 3.50873\n",
      "epoch no.2 train no.168910  loss = 4.25322 avg_loss = 3.50217\n",
      "epoch no.2 train no.168920  loss = 2.55612 avg_loss = 3.56085\n",
      "epoch no.2 train no.168930  loss = 5.38442 avg_loss = 3.56424\n",
      "epoch no.2 train no.168940  loss = 2.00571 avg_loss = 3.53531\n",
      "epoch no.2 train no.168950  loss = 3.02667 avg_loss = 3.51590\n",
      "epoch no.2 train no.168960  loss = 7.44012 avg_loss = 3.56180\n",
      "epoch no.2 train no.168970  loss = 3.32052 avg_loss = 3.53246\n",
      "epoch no.2 train no.168980  loss = 2.30517 avg_loss = 3.54994\n",
      "epoch no.2 train no.168990  loss = 3.96952 avg_loss = 3.58297\n",
      "epoch no.2 train no.169000  loss = 2.41536 avg_loss = 3.63656\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '▁o', '▁o', '</s>', '</s>']\n",
      "추억의 영화음악들 모음</s>\n",
      "epoch no.2 train no.169010  loss = 3.94357 avg_loss = 3.64014\n",
      "epoch no.2 train no.169020  loss = 1.81580 avg_loss = 3.60704\n",
      "epoch no.2 train no.169030  loss = 3.05847 avg_loss = 3.57453\n",
      "epoch no.2 train no.169040  loss = 2.70620 avg_loss = 3.57447\n",
      "epoch no.2 train no.169050  loss = 2.75657 avg_loss = 3.60597\n",
      "epoch no.2 train no.169060  loss = 4.34092 avg_loss = 3.60045\n",
      "epoch no.2 train no.169070  loss = 3.92102 avg_loss = 3.58925\n",
      "epoch no.2 train no.169080  loss = 3.18285 avg_loss = 3.60045\n",
      "epoch no.2 train no.169090  loss = 4.66714 avg_loss = 3.63066\n",
      "epoch no.2 train no.169100  loss = 3.78458 avg_loss = 3.65355\n",
      "epoch no.2 train no.169110  loss = 2.44675 avg_loss = 3.58775\n",
      "epoch no.2 train no.169120  loss = 1.58284 avg_loss = 3.55524\n",
      "epoch no.2 train no.169130  loss = 2.34373 avg_loss = 3.53966\n",
      "epoch no.2 train no.169140  loss = 4.14929 avg_loss = 3.51938\n",
      "epoch no.2 train no.169150  loss = 4.93614 avg_loss = 3.53183\n",
      "epoch no.2 train no.169160  loss = 3.90062 avg_loss = 3.52781\n",
      "epoch no.2 train no.169170  loss = 4.36655 avg_loss = 3.55398\n",
      "epoch no.2 train no.169180  loss = 2.52510 avg_loss = 3.51598\n",
      "epoch no.2 train no.169190  loss = 3.15076 avg_loss = 3.49434\n",
      "epoch no.2 train no.169200  loss = 2.47437 avg_loss = 3.54580\n",
      "epoch no.2 train no.169210  loss = 2.54327 avg_loss = 3.57154\n",
      "epoch no.2 train no.169220  loss = 3.59351 avg_loss = 3.53050\n",
      "epoch no.2 train no.169230  loss = 4.06705 avg_loss = 3.55652\n",
      "epoch no.2 train no.169240  loss = 3.15147 avg_loss = 3.54743\n",
      "epoch no.2 train no.169250  loss = 3.12088 avg_loss = 3.55357\n",
      "epoch no.2 train no.169260  loss = 5.17908 avg_loss = 3.56064\n",
      "epoch no.2 train no.169270  loss = 2.29230 avg_loss = 3.53385\n",
      "epoch no.2 train no.169280  loss = 3.29332 avg_loss = 3.57687\n",
      "epoch no.2 train no.169290  loss = 3.62689 avg_loss = 3.57132\n",
      "epoch no.2 train no.169300  loss = 4.02113 avg_loss = 3.51116\n",
      "epoch no.2 train no.169310  loss = 3.83536 avg_loss = 3.50624\n",
      "epoch no.2 train no.169320  loss = 5.34095 avg_loss = 3.54377\n",
      "epoch no.2 train no.169330  loss = 3.81514 avg_loss = 3.54486\n",
      "epoch no.2 train no.169340  loss = 3.09472 avg_loss = 3.49729\n",
      "epoch no.2 train no.169350  loss = 3.09043 avg_loss = 3.57553\n",
      "epoch no.2 train no.169360  loss = 4.02142 avg_loss = 3.51754\n",
      "epoch no.2 train no.169370  loss = 3.14320 avg_loss = 3.50837\n",
      "epoch no.2 train no.169380  loss = 3.91102 avg_loss = 3.51261\n",
      "epoch no.2 train no.169390  loss = 1.55659 avg_loss = 3.49734\n",
      "epoch no.2 train no.169400  loss = 3.33768 avg_loss = 3.46086\n",
      "epoch no.2 train no.169410  loss = 3.21015 avg_loss = 3.45877\n",
      "epoch no.2 train no.169420  loss = 4.09073 avg_loss = 3.43071\n",
      "epoch no.2 train no.169430  loss = 4.08186 avg_loss = 3.37646\n",
      "epoch no.2 train no.169440  loss = 4.58256 avg_loss = 3.37919\n",
      "epoch no.2 train no.169450  loss = 3.47085 avg_loss = 3.38541\n",
      "epoch no.2 train no.169460  loss = 2.56899 avg_loss = 3.40339\n",
      "epoch no.2 train no.169470  loss = 3.64961 avg_loss = 3.44618\n",
      "epoch no.2 train no.169480  loss = 3.27738 avg_loss = 3.41873\n",
      "epoch no.2 train no.169490  loss = 3.67625 avg_loss = 3.40266\n",
      "epoch no.2 train no.169500  loss = 2.10240 avg_loss = 3.40405\n",
      "epoch no.2 train no.169510  loss = 3.74170 avg_loss = 3.40150\n",
      "epoch no.2 train no.169520  loss = 3.52421 avg_loss = 3.42530\n",
      "epoch no.2 train no.169530  loss = 4.66440 avg_loss = 3.42141\n",
      "epoch no.2 train no.169540  loss = 2.24166 avg_loss = 3.42889\n",
      "epoch no.2 train no.169550  loss = 4.06545 avg_loss = 3.47697\n",
      "epoch no.2 train no.169560  loss = 4.27532 avg_loss = 3.46894\n",
      "epoch no.2 train no.169570  loss = 2.54942 avg_loss = 3.44574\n",
      "epoch no.2 train no.169580  loss = 2.97003 avg_loss = 3.47340\n",
      "epoch no.2 train no.169590  loss = 4.66352 avg_loss = 3.47907\n",
      "epoch no.2 train no.169600  loss = 2.44881 avg_loss = 3.43287\n",
      "epoch no.2 train no.169610  loss = 4.20425 avg_loss = 3.43137\n",
      "epoch no.2 train no.169620  loss = 4.52110 avg_loss = 3.46754\n",
      "epoch no.2 train no.169630  loss = 4.59669 avg_loss = 3.50462\n",
      "epoch no.2 train no.169640  loss = 1.55320 avg_loss = 3.45920\n",
      "epoch no.2 train no.169650  loss = 3.75437 avg_loss = 3.49539\n",
      "epoch no.2 train no.169660  loss = 2.16770 avg_loss = 3.48210\n",
      "epoch no.2 train no.169670  loss = 5.04583 avg_loss = 3.50688\n",
      "epoch no.2 train no.169680  loss = 2.34050 avg_loss = 3.49098\n",
      "epoch no.2 train no.169690  loss = 1.50117 avg_loss = 3.40517\n",
      "epoch no.2 train no.169700  loss = 3.07902 avg_loss = 3.40039\n",
      "epoch no.2 train no.169710  loss = 5.69892 avg_loss = 3.41630\n",
      "epoch no.2 train no.169720  loss = 4.68517 avg_loss = 3.47824\n",
      "epoch no.2 train no.169730  loss = 3.70279 avg_loss = 3.47496\n",
      "epoch no.2 train no.169740  loss = 2.70623 avg_loss = 3.42107\n",
      "epoch no.2 train no.169750  loss = 3.19823 avg_loss = 3.46689\n",
      "epoch no.2 train no.169760  loss = 2.38437 avg_loss = 3.43614\n",
      "epoch no.2 train no.169770  loss = 3.26939 avg_loss = 3.45729\n",
      "epoch no.2 train no.169780  loss = 4.29232 avg_loss = 3.49551\n",
      "epoch no.2 train no.169790  loss = 4.23026 avg_loss = 3.51748\n",
      "epoch no.2 train no.169800  loss = 5.97681 avg_loss = 3.51996\n",
      "epoch no.2 train no.169810  loss = 2.75583 avg_loss = 3.51532\n",
      "epoch no.2 train no.169820  loss = 3.56645 avg_loss = 3.47640\n",
      "epoch no.2 train no.169830  loss = 3.12480 avg_loss = 3.50808\n",
      "epoch no.2 train no.169840  loss = 3.46311 avg_loss = 3.45114\n",
      "epoch no.2 train no.169850  loss = 5.59027 avg_loss = 3.50409\n",
      "epoch no.2 train no.169860  loss = 2.30641 avg_loss = 3.46648\n",
      "epoch no.2 train no.169870  loss = 2.95357 avg_loss = 3.46622\n",
      "epoch no.2 train no.169880  loss = 2.64418 avg_loss = 3.47283\n",
      "epoch no.2 train no.169890  loss = 6.67176 avg_loss = 3.53465\n",
      "epoch no.2 train no.169900  loss = 2.60420 avg_loss = 3.52513\n",
      "epoch no.2 train no.169910  loss = 2.68722 avg_loss = 3.48816\n",
      "epoch no.2 train no.169920  loss = 4.86717 avg_loss = 3.49538\n",
      "epoch no.2 train no.169930  loss = 5.60976 avg_loss = 3.54775\n",
      "epoch no.2 train no.169940  loss = 3.31748 avg_loss = 3.54366\n",
      "epoch no.2 train no.169950  loss = 3.50721 avg_loss = 3.55700\n",
      "epoch no.2 train no.169960  loss = 2.93275 avg_loss = 3.52032\n",
      "epoch no.2 train no.169970  loss = 3.94370 avg_loss = 3.49478\n",
      "epoch no.2 train no.169980  loss = 5.04456 avg_loss = 3.49397\n",
      "epoch no.2 train no.169990  loss = 3.85985 avg_loss = 3.47706\n",
      "epoch no.2 train no.170000  loss = 4.45766 avg_loss = 3.45644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "to_tokens: ['▁가을', '▁올드', '팝', '송', '▁모음', '곡', '</s>']\n",
      "추억의 올드 팝송 명곡</s>\n",
      "epoch no.2 train no.170010  loss = 2.52447 avg_loss = 3.43806\n",
      "epoch no.2 train no.170020  loss = 4.40130 avg_loss = 3.46597\n",
      "epoch no.2 train no.170030  loss = 4.84042 avg_loss = 3.50930\n",
      "epoch no.2 train no.170040  loss = 3.99345 avg_loss = 3.56949\n",
      "epoch no.2 train no.170050  loss = 3.47232 avg_loss = 3.57194\n",
      "epoch no.2 train no.170060  loss = 3.67342 avg_loss = 3.54097\n",
      "epoch no.2 train no.170070  loss = 3.29064 avg_loss = 3.49914\n",
      "epoch no.2 train no.170080  loss = 3.77866 avg_loss = 3.47843\n",
      "epoch no.2 train no.170090  loss = 2.50804 avg_loss = 3.45090\n",
      "epoch no.2 train no.170100  loss = 2.97374 avg_loss = 3.43901\n",
      "epoch no.2 train no.170110  loss = 2.40304 avg_loss = 3.42046\n",
      "epoch no.2 train no.170120  loss = 1.58390 avg_loss = 3.37844\n",
      "epoch no.2 train no.170130  loss = 3.48155 avg_loss = 3.43247\n",
      "epoch no.2 train no.170140  loss = 3.29404 avg_loss = 3.47458\n",
      "epoch no.2 train no.170150  loss = 5.46103 avg_loss = 3.49972\n",
      "epoch no.2 train no.170160  loss = 1.90519 avg_loss = 3.54257\n",
      "epoch no.2 train no.170170  loss = 5.76333 avg_loss = 3.56334\n",
      "epoch no.2 train no.170180  loss = 3.40977 avg_loss = 3.51181\n",
      "epoch no.2 train no.170190  loss = 2.51838 avg_loss = 3.44532\n",
      "epoch no.2 train no.170200  loss = 2.22512 avg_loss = 3.44857\n",
      "epoch no.2 train no.170210  loss = 2.93405 avg_loss = 3.44636\n",
      "epoch no.2 train no.170220  loss = 1.91153 avg_loss = 3.42260\n",
      "epoch no.2 train no.170230  loss = 2.28976 avg_loss = 3.42281\n",
      "epoch no.2 train no.170240  loss = 3.65187 avg_loss = 3.37224\n",
      "epoch no.2 train no.170250  loss = 4.50315 avg_loss = 3.33354\n",
      "epoch no.2 train no.170260  loss = 4.18229 avg_loss = 3.32894\n",
      "epoch no.2 train no.170270  loss = 2.58474 avg_loss = 3.33046\n",
      "epoch no.2 train no.170280  loss = 4.41768 avg_loss = 3.35589\n",
      "epoch no.2 train no.170290  loss = 2.31990 avg_loss = 3.36087\n",
      "epoch no.2 train no.170300  loss = 3.84920 avg_loss = 3.33257\n",
      "epoch no.2 train no.170310  loss = 4.32582 avg_loss = 3.40195\n",
      "epoch no.2 train no.170320  loss = 3.08109 avg_loss = 3.42794\n",
      "epoch no.2 train no.170330  loss = 2.80094 avg_loss = 3.46869\n",
      "epoch no.2 train no.170340  loss = 1.33718 avg_loss = 3.45892\n",
      "epoch no.2 train no.170350  loss = 4.56190 avg_loss = 3.47266\n",
      "epoch no.2 train no.170360  loss = 7.36956 avg_loss = 3.52085\n",
      "epoch no.2 train no.170370  loss = 3.14246 avg_loss = 3.50864\n",
      "epoch no.2 train no.170380  loss = 1.95862 avg_loss = 3.49974\n",
      "epoch no.2 train no.170390  loss = 2.54461 avg_loss = 3.44567\n",
      "epoch no.2 train no.170400  loss = 4.84083 avg_loss = 3.44120\n",
      "epoch no.2 train no.170410  loss = 4.14886 avg_loss = 3.46754\n",
      "epoch no.2 train no.170420  loss = 2.98837 avg_loss = 3.49266\n",
      "epoch no.2 train no.170430  loss = 3.98498 avg_loss = 3.46341\n",
      "epoch no.2 train no.170440  loss = 3.19731 avg_loss = 3.41246\n",
      "epoch no.2 train no.170450  loss = 4.88110 avg_loss = 3.40184\n",
      "epoch no.2 train no.170460  loss = 3.62290 avg_loss = 3.44266\n",
      "epoch no.2 train no.170470  loss = 2.84582 avg_loss = 3.44366\n",
      "epoch no.2 train no.170480  loss = 5.64854 avg_loss = 3.45912\n",
      "epoch no.2 train no.170490  loss = 2.30460 avg_loss = 3.41933\n",
      "epoch no.2 train no.170500  loss = 2.93497 avg_loss = 3.40295\n",
      "epoch no.2 train no.170510  loss = 4.32991 avg_loss = 3.44896\n",
      "epoch no.2 train no.170520  loss = 5.28006 avg_loss = 3.47472\n",
      "epoch no.2 train no.170530  loss = 2.27376 avg_loss = 3.46669\n",
      "epoch no.2 train no.170540  loss = 3.84510 avg_loss = 3.47008\n",
      "epoch no.2 train no.170550  loss = 3.17155 avg_loss = 3.46614\n",
      "epoch no.2 train no.170560  loss = 2.44286 avg_loss = 3.46646\n",
      "epoch no.2 train no.170570  loss = 2.90787 avg_loss = 3.49852\n",
      "epoch no.2 train no.170580  loss = 2.51653 avg_loss = 3.51293\n",
      "epoch no.2 train no.170590  loss = 2.91932 avg_loss = 3.50228\n",
      "epoch no.2 train no.170600  loss = 3.72672 avg_loss = 3.50373\n",
      "epoch no.2 train no.170610  loss = 3.51766 avg_loss = 3.52224\n",
      "epoch no.2 train no.170620  loss = 2.19130 avg_loss = 3.51532\n",
      "epoch no.2 train no.170630  loss = 4.39561 avg_loss = 3.48488\n",
      "epoch no.2 train no.170640  loss = 2.68366 avg_loss = 3.46409\n",
      "epoch no.2 train no.170650  loss = 5.12353 avg_loss = 3.45900\n",
      "epoch no.2 train no.170660  loss = 6.05601 avg_loss = 3.48051\n",
      "epoch no.2 train no.170670  loss = 2.43614 avg_loss = 3.48095\n",
      "epoch no.2 train no.170680  loss = 3.53812 avg_loss = 3.53315\n",
      "epoch no.2 train no.170690  loss = 3.55490 avg_loss = 3.50989\n",
      "epoch no.2 train no.170700  loss = 4.63294 avg_loss = 3.56252\n",
      "epoch no.2 train no.170710  loss = 2.22483 avg_loss = 3.54481\n",
      "epoch no.2 train no.170720  loss = 4.04806 avg_loss = 3.56860\n",
      "epoch no.2 train no.170730  loss = 2.44501 avg_loss = 3.54720\n",
      "epoch no.2 train no.170740  loss = 3.11507 avg_loss = 3.54741\n",
      "epoch no.2 train no.170750  loss = 3.54781 avg_loss = 3.54579\n",
      "epoch no.2 train no.170760  loss = 2.59647 avg_loss = 3.54800\n",
      "epoch no.2 train no.170770  loss = 2.69870 avg_loss = 3.53670\n",
      "epoch no.2 train no.170780  loss = 5.63527 avg_loss = 3.53624\n",
      "epoch no.2 train no.170790  loss = 2.69547 avg_loss = 3.51065\n",
      "epoch no.2 train no.170800  loss = 3.74567 avg_loss = 3.49879\n",
      "epoch no.2 train no.170810  loss = 2.99914 avg_loss = 3.52375\n",
      "epoch no.2 train no.170820  loss = 3.14471 avg_loss = 3.53450\n",
      "epoch no.2 train no.170830  loss = 4.30802 avg_loss = 3.59819\n",
      "epoch no.2 train no.170840  loss = 3.43073 avg_loss = 3.55410\n",
      "epoch no.2 train no.170850  loss = 5.10230 avg_loss = 3.60605\n",
      "epoch no.2 train no.170860  loss = 4.90898 avg_loss = 3.61945\n",
      "epoch no.2 train no.170870  loss = 2.44995 avg_loss = 3.60565\n",
      "epoch no.2 train no.170880  loss = 2.61087 avg_loss = 3.55866\n",
      "epoch no.2 train no.170890  loss = 4.80009 avg_loss = 3.57865\n",
      "epoch no.2 train no.170900  loss = 4.29405 avg_loss = 3.61732\n",
      "epoch no.2 train no.170910  loss = 4.48588 avg_loss = 3.57217\n",
      "epoch no.2 train no.170920  loss = 2.13881 avg_loss = 3.53644\n",
      "epoch no.2 train no.170930  loss = 2.23145 avg_loss = 3.51877\n",
      "epoch no.2 train no.170940  loss = 5.82706 avg_loss = 3.50060\n",
      "epoch no.2 train no.170950  loss = 2.79375 avg_loss = 3.49947\n",
      "epoch no.2 train no.170960  loss = 1.96572 avg_loss = 3.47260\n",
      "epoch no.2 train no.170970  loss = 3.22194 avg_loss = 3.41898\n",
      "epoch no.2 train no.170980  loss = 3.23265 avg_loss = 3.37361\n",
      "epoch no.2 train no.170990  loss = 5.30728 avg_loss = 3.39046\n",
      "epoch no.2 train no.171000  loss = 1.96489 avg_loss = 3.38558\n",
      "4\n",
      "to_tokens: ['▁가을', '▁o', '곡', '들', '드', '</s>']\n",
      "추억의 명곡 발라드</s>\n",
      "epoch no.2 train no.171010  loss = 3.36316 avg_loss = 3.39415\n",
      "epoch no.2 train no.171020  loss = 3.30511 avg_loss = 3.37971\n",
      "epoch no.2 train no.171030  loss = 2.40420 avg_loss = 3.39853\n",
      "epoch no.2 train no.171040  loss = 2.98143 avg_loss = 3.38100\n",
      "epoch no.2 train no.171050  loss = 2.98927 avg_loss = 3.40795\n",
      "epoch no.2 train no.171060  loss = 4.53276 avg_loss = 3.42813\n",
      "epoch no.2 train no.171070  loss = 5.11010 avg_loss = 3.45556\n",
      "epoch no.2 train no.171080  loss = 3.78697 avg_loss = 3.43561\n",
      "epoch no.2 train no.171090  loss = 3.70633 avg_loss = 3.38638\n",
      "epoch no.2 train no.171100  loss = 3.79319 avg_loss = 3.41910\n",
      "epoch no.2 train no.171110  loss = 2.00642 avg_loss = 3.41918\n",
      "epoch no.2 train no.171120  loss = 3.65587 avg_loss = 3.42095\n",
      "epoch no.2 train no.171130  loss = 2.48061 avg_loss = 3.40922\n",
      "epoch no.2 train no.171140  loss = 2.98163 avg_loss = 3.40647\n",
      "epoch no.2 train no.171150  loss = 2.73766 avg_loss = 3.39332\n",
      "epoch no.2 train no.171160  loss = 3.67263 avg_loss = 3.35746\n",
      "epoch no.2 train no.171170  loss = 3.56879 avg_loss = 3.34951\n",
      "epoch no.2 train no.171180  loss = 3.28643 avg_loss = 3.34236\n",
      "epoch no.2 train no.171190  loss = 2.81366 avg_loss = 3.30695\n",
      "epoch no.2 train no.171200  loss = 2.50197 avg_loss = 3.31762\n",
      "epoch no.2 train no.171210  loss = 2.70341 avg_loss = 3.32012\n",
      "epoch no.2 train no.171220  loss = 5.00277 avg_loss = 3.37398\n",
      "epoch no.2 train no.171230  loss = 3.84219 avg_loss = 3.33600\n",
      "epoch no.2 train no.171240  loss = 2.39813 avg_loss = 3.32963\n",
      "epoch no.2 train no.171250  loss = 4.32397 avg_loss = 3.35983\n",
      "epoch no.2 train no.171260  loss = 3.80668 avg_loss = 3.33741\n",
      "epoch no.2 train no.171270  loss = 2.77991 avg_loss = 3.29077\n",
      "epoch no.2 train no.171280  loss = 3.55670 avg_loss = 3.32151\n",
      "epoch no.2 train no.171290  loss = 4.58573 avg_loss = 3.33305\n",
      "epoch no.2 train no.171300  loss = 3.52406 avg_loss = 3.34490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.171310  loss = 4.49289 avg_loss = 3.38600\n",
      "epoch no.2 train no.171320  loss = 4.00390 avg_loss = 3.36806\n",
      "epoch no.2 train no.171330  loss = 3.67978 avg_loss = 3.37148\n",
      "epoch no.2 train no.171340  loss = 2.58253 avg_loss = 3.43711\n",
      "epoch no.2 train no.171350  loss = 2.70650 avg_loss = 3.41927\n",
      "epoch no.2 train no.171360  loss = 5.08390 avg_loss = 3.46426\n",
      "epoch no.2 train no.171370  loss = 2.92012 avg_loss = 3.45609\n",
      "epoch no.2 train no.171380  loss = 1.60084 avg_loss = 3.43560\n",
      "epoch no.2 train no.171390  loss = 1.66534 avg_loss = 3.44160\n",
      "epoch no.2 train no.171400  loss = 3.46097 avg_loss = 3.48138\n",
      "epoch no.2 train no.171410  loss = 3.41724 avg_loss = 3.47296\n",
      "epoch no.2 train no.171420  loss = 3.33871 avg_loss = 3.41234\n",
      "epoch no.2 train no.171430  loss = 4.30197 avg_loss = 3.36983\n",
      "epoch no.2 train no.171440  loss = 2.10747 avg_loss = 3.34765\n",
      "epoch no.2 train no.171450  loss = 1.81479 avg_loss = 3.35826\n",
      "epoch no.2 train no.171460  loss = 5.05238 avg_loss = 3.40600\n",
      "epoch no.2 train no.171470  loss = 2.84013 avg_loss = 3.46178\n",
      "epoch no.2 train no.171480  loss = 2.91283 avg_loss = 3.45304\n",
      "epoch no.2 train no.171490  loss = 3.43772 avg_loss = 3.42715\n",
      "epoch no.2 train no.171500  loss = 5.02056 avg_loss = 3.45341\n",
      "epoch no.2 train no.171510  loss = 3.78402 avg_loss = 3.39454\n",
      "epoch no.2 train no.171520  loss = 2.59098 avg_loss = 3.40984\n",
      "epoch no.2 train no.171530  loss = 2.72990 avg_loss = 3.39401\n",
      "epoch no.2 train no.171540  loss = 3.16237 avg_loss = 3.41968\n",
      "epoch no.2 train no.171550  loss = 2.28680 avg_loss = 3.36192\n",
      "epoch no.2 train no.171560  loss = 4.01455 avg_loss = 3.41109\n",
      "epoch no.2 train no.171570  loss = 3.25406 avg_loss = 3.42497\n",
      "epoch no.2 train no.171580  loss = 2.92386 avg_loss = 3.39899\n",
      "epoch no.2 train no.171590  loss = 4.21324 avg_loss = 3.40714\n",
      "epoch no.2 train no.171600  loss = 4.17343 avg_loss = 3.43030\n",
      "epoch no.2 train no.171610  loss = 3.75574 avg_loss = 3.43649\n",
      "epoch no.2 train no.171620  loss = 2.90534 avg_loss = 3.39589\n",
      "epoch no.2 train no.171630  loss = 5.56229 avg_loss = 3.33998\n",
      "epoch no.2 train no.171640  loss = 2.28084 avg_loss = 3.30967\n",
      "epoch no.2 train no.171650  loss = 2.33257 avg_loss = 3.26800\n",
      "epoch no.2 train no.171660  loss = 3.20042 avg_loss = 3.22573\n",
      "epoch no.2 train no.171670  loss = 3.87442 avg_loss = 3.27189\n",
      "epoch no.2 train no.171680  loss = 3.38358 avg_loss = 3.27884\n",
      "epoch no.2 train no.171690  loss = 2.50560 avg_loss = 3.29918\n",
      "epoch no.2 train no.171700  loss = 2.97974 avg_loss = 3.33843\n",
      "epoch no.2 train no.171710  loss = 2.63080 avg_loss = 3.31896\n",
      "epoch no.2 train no.171720  loss = 3.42759 avg_loss = 3.35190\n",
      "epoch no.2 train no.171730  loss = 2.46013 avg_loss = 3.37524\n",
      "epoch no.2 train no.171740  loss = 3.98847 avg_loss = 3.39026\n",
      "epoch no.2 train no.171750  loss = 2.84606 avg_loss = 3.33575\n",
      "epoch no.2 train no.171760  loss = 4.11065 avg_loss = 3.37894\n",
      "epoch no.2 train no.171770  loss = 2.89076 avg_loss = 3.40688\n",
      "epoch no.2 train no.171780  loss = 4.12447 avg_loss = 3.40850\n",
      "epoch no.2 train no.171790  loss = 1.93508 avg_loss = 3.43617\n",
      "epoch no.2 train no.171800  loss = 3.72812 avg_loss = 3.42689\n",
      "epoch no.2 train no.171810  loss = 3.46090 avg_loss = 3.46068\n",
      "epoch no.2 train no.171820  loss = 3.61646 avg_loss = 3.39562\n",
      "epoch no.2 train no.171830  loss = 2.92859 avg_loss = 3.35080\n",
      "epoch no.2 train no.171840  loss = 2.78925 avg_loss = 3.37183\n",
      "epoch no.2 train no.171850  loss = 2.54066 avg_loss = 3.35335\n",
      "epoch no.2 train no.171860  loss = 3.37364 avg_loss = 3.34710\n",
      "epoch no.2 train no.171870  loss = 2.56762 avg_loss = 3.32065\n",
      "epoch no.2 train no.171880  loss = 2.30831 avg_loss = 3.30562\n",
      "epoch no.2 train no.171890  loss = 3.96627 avg_loss = 3.33848\n",
      "epoch no.2 train no.171900  loss = 2.70841 avg_loss = 3.38736\n",
      "epoch no.2 train no.171910  loss = 4.41036 avg_loss = 3.42261\n",
      "epoch no.2 train no.171920  loss = 2.58655 avg_loss = 3.38643\n",
      "epoch no.2 train no.171930  loss = 3.98636 avg_loss = 3.40132\n",
      "epoch no.2 train no.171940  loss = 2.84004 avg_loss = 3.39017\n",
      "epoch no.2 train no.171950  loss = 2.85953 avg_loss = 3.35944\n",
      "epoch no.2 train no.171960  loss = 3.35200 avg_loss = 3.37383\n",
      "epoch no.2 train no.171970  loss = 3.87062 avg_loss = 3.38180\n",
      "epoch no.2 train no.171980  loss = 4.33433 avg_loss = 3.38850\n",
      "epoch no.2 train no.171990  loss = 4.59126 avg_loss = 3.34469\n",
      "epoch no.2 train no.172000  loss = 2.27881 avg_loss = 3.33535\n",
      "4\n",
      "to_tokens: ['▁가을', '▁드라마', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.2 train no.172010  loss = 2.95457 avg_loss = 3.42849\n",
      "epoch no.2 train no.172020  loss = 2.57704 avg_loss = 3.40237\n",
      "epoch no.2 train no.172030  loss = 3.54771 avg_loss = 3.37575\n",
      "epoch no.2 train no.172040  loss = 3.40906 avg_loss = 3.33923\n",
      "epoch no.2 train no.172050  loss = 3.70755 avg_loss = 3.34219\n",
      "epoch no.2 train no.172060  loss = 3.09422 avg_loss = 3.36181\n",
      "epoch no.2 train no.172070  loss = 2.46667 avg_loss = 3.34389\n",
      "epoch no.2 train no.172080  loss = 3.09740 avg_loss = 3.35913\n",
      "epoch no.2 train no.172090  loss = 4.66610 avg_loss = 3.44219\n",
      "epoch no.2 train no.172100  loss = 3.10009 avg_loss = 3.46652\n",
      "epoch no.2 train no.172110  loss = 4.54647 avg_loss = 3.51726\n",
      "epoch no.2 train no.172120  loss = 3.21112 avg_loss = 3.48035\n",
      "epoch no.2 train no.172130  loss = 2.30350 avg_loss = 3.48991\n",
      "epoch no.2 train no.172140  loss = 1.65015 avg_loss = 3.47706\n",
      "epoch no.2 train no.172150  loss = 3.72038 avg_loss = 3.43723\n",
      "epoch no.2 train no.172160  loss = 2.28997 avg_loss = 3.41377\n",
      "epoch no.2 train no.172170  loss = 5.55100 avg_loss = 3.42776\n",
      "epoch no.2 train no.172180  loss = 3.51092 avg_loss = 3.41247\n",
      "epoch no.2 train no.172190  loss = 5.90757 avg_loss = 3.44028\n",
      "epoch no.2 train no.172200  loss = 3.36044 avg_loss = 3.49344\n",
      "epoch no.2 train no.172210  loss = 4.61202 avg_loss = 3.47657\n",
      "epoch no.2 train no.172220  loss = 3.51282 avg_loss = 3.44948\n",
      "epoch no.2 train no.172230  loss = 3.95036 avg_loss = 3.45177\n",
      "epoch no.2 train no.172240  loss = 2.71058 avg_loss = 3.42290\n",
      "epoch no.2 train no.172250  loss = 2.11092 avg_loss = 3.37260\n",
      "epoch no.2 train no.172260  loss = 3.09788 avg_loss = 3.40379\n",
      "epoch no.2 train no.172270  loss = 3.77227 avg_loss = 3.43251\n",
      "epoch no.2 train no.172280  loss = 3.58906 avg_loss = 3.40603\n",
      "epoch no.2 train no.172290  loss = 2.69315 avg_loss = 3.39026\n",
      "epoch no.2 train no.172300  loss = 4.93757 avg_loss = 3.41376\n",
      "epoch no.2 train no.172310  loss = 3.07160 avg_loss = 3.40992\n",
      "epoch no.2 train no.172320  loss = 2.54373 avg_loss = 3.43337\n",
      "epoch no.2 train no.172330  loss = 1.37970 avg_loss = 3.38183\n",
      "epoch no.2 train no.172340  loss = 3.69433 avg_loss = 3.40352\n",
      "epoch no.2 train no.172350  loss = 1.47471 avg_loss = 3.34611\n",
      "epoch no.2 train no.172360  loss = 2.64094 avg_loss = 3.31100\n",
      "epoch no.2 train no.172370  loss = 3.99772 avg_loss = 3.31871\n",
      "epoch no.2 train no.172380  loss = 5.64632 avg_loss = 3.33569\n",
      "epoch no.2 train no.172390  loss = 2.65626 avg_loss = 3.31107\n",
      "epoch no.2 train no.172400  loss = 2.66613 avg_loss = 3.32876\n",
      "epoch no.2 train no.172410  loss = 4.16119 avg_loss = 3.35537\n",
      "epoch no.2 train no.172420  loss = 3.05545 avg_loss = 3.33909\n",
      "epoch no.2 train no.172430  loss = 3.37226 avg_loss = 3.34323\n",
      "epoch no.2 train no.172440  loss = 3.91581 avg_loss = 3.34303\n",
      "epoch no.2 train no.172450  loss = 3.76556 avg_loss = 3.44238\n",
      "epoch no.2 train no.172460  loss = 4.37356 avg_loss = 3.44003\n",
      "epoch no.2 train no.172470  loss = 2.06565 avg_loss = 3.39209\n",
      "epoch no.2 train no.172480  loss = 3.30349 avg_loss = 3.37561\n",
      "epoch no.2 train no.172490  loss = 3.61412 avg_loss = 3.43163\n",
      "epoch no.2 train no.172500  loss = 5.46834 avg_loss = 3.42481\n",
      "epoch no.2 train no.172510  loss = 2.86447 avg_loss = 3.45844\n",
      "epoch no.2 train no.172520  loss = 3.24622 avg_loss = 3.47260\n",
      "epoch no.2 train no.172530  loss = 3.03514 avg_loss = 3.44727\n",
      "epoch no.2 train no.172540  loss = 2.53764 avg_loss = 3.44321\n",
      "epoch no.2 train no.172550  loss = 3.06603 avg_loss = 3.44175\n",
      "epoch no.2 train no.172560  loss = 2.00708 avg_loss = 3.45871\n",
      "epoch no.2 train no.172570  loss = 2.35674 avg_loss = 3.41855\n",
      "epoch no.2 train no.172580  loss = 3.03380 avg_loss = 3.44765\n",
      "epoch no.2 train no.172590  loss = 5.08283 avg_loss = 3.47528\n",
      "epoch no.2 train no.172600  loss = 4.05872 avg_loss = 3.53446\n",
      "epoch no.2 train no.172610  loss = 3.61269 avg_loss = 3.53050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.172620  loss = 2.34887 avg_loss = 3.49629\n",
      "epoch no.2 train no.172630  loss = 1.79685 avg_loss = 3.50561\n",
      "epoch no.2 train no.172640  loss = 4.21169 avg_loss = 3.51513\n",
      "epoch no.2 train no.172650  loss = 2.71885 avg_loss = 3.49766\n",
      "epoch no.2 train no.172660  loss = 4.64350 avg_loss = 3.49003\n",
      "epoch no.2 train no.172670  loss = 3.35251 avg_loss = 3.48588\n",
      "epoch no.2 train no.172680  loss = 4.36097 avg_loss = 3.53247\n",
      "epoch no.2 train no.172690  loss = 3.20663 avg_loss = 3.57615\n",
      "epoch no.2 train no.172700  loss = 3.35526 avg_loss = 3.59752\n",
      "epoch no.2 train no.172710  loss = 2.98267 avg_loss = 3.58979\n",
      "epoch no.2 train no.172720  loss = 2.47783 avg_loss = 3.56247\n",
      "epoch no.2 train no.172730  loss = 3.65986 avg_loss = 3.54587\n",
      "epoch no.2 train no.172740  loss = 3.19588 avg_loss = 3.56188\n",
      "epoch no.2 train no.172750  loss = 5.91229 avg_loss = 3.53675\n",
      "epoch no.2 train no.172760  loss = 4.19161 avg_loss = 3.54746\n",
      "epoch no.2 train no.172770  loss = 2.55169 avg_loss = 3.53547\n",
      "epoch no.2 train no.172780  loss = 4.48349 avg_loss = 3.52223\n",
      "epoch no.2 train no.172790  loss = 5.00056 avg_loss = 3.56056\n",
      "epoch no.2 train no.172800  loss = 4.18720 avg_loss = 3.59520\n",
      "epoch no.2 train no.172810  loss = 3.69655 avg_loss = 3.55522\n",
      "epoch no.2 train no.172820  loss = 3.23477 avg_loss = 3.53072\n",
      "epoch no.2 train no.172830  loss = 3.09242 avg_loss = 3.48307\n",
      "epoch no.2 train no.172840  loss = 4.43950 avg_loss = 3.49686\n",
      "epoch no.2 train no.172850  loss = 2.63805 avg_loss = 3.46170\n",
      "epoch no.2 train no.172860  loss = 2.65837 avg_loss = 3.39965\n",
      "epoch no.2 train no.172870  loss = 2.93754 avg_loss = 3.36648\n",
      "epoch no.2 train no.172880  loss = 2.57291 avg_loss = 3.38124\n",
      "epoch no.2 train no.172890  loss = 2.27511 avg_loss = 3.35430\n",
      "epoch no.2 train no.172900  loss = 4.77846 avg_loss = 3.37371\n",
      "epoch no.2 train no.172910  loss = 2.26676 avg_loss = 3.36545\n",
      "epoch no.2 train no.172920  loss = 3.23687 avg_loss = 3.34476\n",
      "epoch no.2 train no.172930  loss = 2.48212 avg_loss = 3.38335\n",
      "epoch no.2 train no.172940  loss = 3.22118 avg_loss = 3.43657\n",
      "epoch no.2 train no.172950  loss = 3.18929 avg_loss = 3.46152\n",
      "epoch no.2 train no.172960  loss = 4.84449 avg_loss = 3.51366\n",
      "epoch no.2 train no.172970  loss = 3.72057 avg_loss = 3.47067\n",
      "epoch no.2 train no.172980  loss = 3.95726 avg_loss = 3.44340\n",
      "epoch no.2 train no.172990  loss = 3.45804 avg_loss = 3.41774\n",
      "epoch no.2 train no.173000  loss = 5.06843 avg_loss = 3.37640\n",
      "4\n",
      "to_tokens: ['▁비', '▁o', '년대', '▁노래', '곡', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.2 train no.173010  loss = 2.01187 avg_loss = 3.35368\n",
      "epoch no.2 train no.173020  loss = 3.79686 avg_loss = 3.37696\n",
      "epoch no.2 train no.173030  loss = 3.11754 avg_loss = 3.35385\n",
      "epoch no.2 train no.173040  loss = 3.87103 avg_loss = 3.38929\n",
      "epoch no.2 train no.173050  loss = 3.43858 avg_loss = 3.37275\n",
      "epoch no.2 train no.173060  loss = 3.53885 avg_loss = 3.40045\n",
      "epoch no.2 train no.173070  loss = 4.13926 avg_loss = 3.41454\n",
      "epoch no.2 train no.173080  loss = 4.40779 avg_loss = 3.41043\n",
      "epoch no.2 train no.173090  loss = 2.62153 avg_loss = 3.42355\n",
      "epoch no.2 train no.173100  loss = 3.30625 avg_loss = 3.42176\n",
      "epoch no.2 train no.173110  loss = 4.45168 avg_loss = 3.43851\n",
      "epoch no.2 train no.173120  loss = 1.30409 avg_loss = 3.40228\n",
      "epoch no.2 train no.173130  loss = 3.24696 avg_loss = 3.37991\n",
      "epoch no.2 train no.173140  loss = 3.40012 avg_loss = 3.34188\n",
      "epoch no.2 train no.173150  loss = 2.34684 avg_loss = 3.39792\n",
      "epoch no.2 train no.173160  loss = 3.18467 avg_loss = 3.38980\n",
      "epoch no.2 train no.173170  loss = 5.42075 avg_loss = 3.45380\n",
      "epoch no.2 train no.173180  loss = 3.12041 avg_loss = 3.47705\n",
      "epoch no.2 train no.173190  loss = 3.99475 avg_loss = 3.46735\n",
      "epoch no.2 train no.173200  loss = 3.19118 avg_loss = 3.46321\n",
      "epoch no.2 train no.173210  loss = 2.72622 avg_loss = 3.47160\n",
      "epoch no.2 train no.173220  loss = 2.68722 avg_loss = 3.42726\n",
      "epoch no.2 train no.173230  loss = 3.24927 avg_loss = 3.40955\n",
      "epoch no.2 train no.173240  loss = 3.68023 avg_loss = 3.32489\n",
      "epoch no.2 train no.173250  loss = 2.63679 avg_loss = 3.34118\n",
      "epoch no.2 train no.173260  loss = 3.25949 avg_loss = 3.34267\n",
      "epoch no.2 train no.173270  loss = 3.09399 avg_loss = 3.28880\n",
      "epoch no.2 train no.173280  loss = 4.57305 avg_loss = 3.32210\n",
      "epoch no.2 train no.173290  loss = 3.71599 avg_loss = 3.36341\n",
      "epoch no.2 train no.173300  loss = 2.93020 avg_loss = 3.32076\n",
      "epoch no.2 train no.173310  loss = 5.29170 avg_loss = 3.31762\n",
      "epoch no.2 train no.173320  loss = 2.40983 avg_loss = 3.31155\n",
      "epoch no.2 train no.173330  loss = 3.67884 avg_loss = 3.31283\n",
      "epoch no.2 train no.173340  loss = 3.58441 avg_loss = 3.28443\n",
      "epoch no.2 train no.173350  loss = 2.98381 avg_loss = 3.29465\n",
      "epoch no.2 train no.173360  loss = 2.93419 avg_loss = 3.27249\n",
      "epoch no.2 train no.173370  loss = 3.03133 avg_loss = 3.23164\n",
      "epoch no.2 train no.173380  loss = 3.68493 avg_loss = 3.25760\n",
      "epoch no.2 train no.173390  loss = 4.92481 avg_loss = 3.28847\n",
      "epoch no.2 train no.173400  loss = 3.21624 avg_loss = 3.29681\n",
      "epoch no.2 train no.173410  loss = 4.03116 avg_loss = 3.32952\n",
      "epoch no.2 train no.173420  loss = 4.52478 avg_loss = 3.34626\n",
      "epoch no.2 train no.173430  loss = 2.71960 avg_loss = 3.32793\n",
      "epoch no.2 train no.173440  loss = 3.48164 avg_loss = 3.38315\n",
      "epoch no.2 train no.173450  loss = 1.58939 avg_loss = 3.39440\n",
      "epoch no.2 train no.173460  loss = 2.66619 avg_loss = 3.34127\n",
      "epoch no.2 train no.173470  loss = 2.39822 avg_loss = 3.34883\n",
      "epoch no.2 train no.173480  loss = 2.82103 avg_loss = 3.36999\n",
      "epoch no.2 train no.173490  loss = 3.80866 avg_loss = 3.36321\n",
      "epoch no.2 train no.173500  loss = 3.01782 avg_loss = 3.35603\n",
      "epoch no.2 train no.173510  loss = 4.04323 avg_loss = 3.42352\n",
      "epoch no.2 train no.173520  loss = 2.44267 avg_loss = 3.42217\n",
      "epoch no.2 train no.173530  loss = 2.21700 avg_loss = 3.42708\n",
      "epoch no.2 train no.173540  loss = 4.17927 avg_loss = 3.39860\n",
      "epoch no.2 train no.173550  loss = 5.01070 avg_loss = 3.40092\n",
      "epoch no.2 train no.173560  loss = 2.14624 avg_loss = 3.48982\n",
      "epoch no.2 train no.173570  loss = 3.24252 avg_loss = 3.47856\n",
      "epoch no.2 train no.173580  loss = 5.17366 avg_loss = 3.51179\n",
      "epoch no.2 train no.173590  loss = 4.77884 avg_loss = 3.51418\n",
      "epoch no.2 train no.173600  loss = 5.41951 avg_loss = 3.53400\n",
      "epoch no.2 train no.173610  loss = 3.15274 avg_loss = 3.53206\n",
      "epoch no.2 train no.173620  loss = 3.17602 avg_loss = 3.49973\n",
      "epoch no.2 train no.173630  loss = 2.17198 avg_loss = 3.49040\n",
      "epoch no.2 train no.173640  loss = 3.22889 avg_loss = 3.49699\n",
      "epoch no.2 train no.173650  loss = 2.84255 avg_loss = 3.46513\n",
      "epoch no.2 train no.173660  loss = 4.28015 avg_loss = 3.50488\n",
      "epoch no.2 train no.173670  loss = 3.85849 avg_loss = 3.50181\n",
      "epoch no.2 train no.173680  loss = 5.97366 avg_loss = 3.48055\n",
      "epoch no.2 train no.173690  loss = 3.77055 avg_loss = 3.48624\n",
      "epoch no.2 train no.173700  loss = 2.37390 avg_loss = 3.48677\n",
      "epoch no.2 train no.173710  loss = 2.53042 avg_loss = 3.49339\n",
      "epoch no.2 train no.173720  loss = 5.57437 avg_loss = 3.52762\n",
      "epoch no.2 train no.173730  loss = 4.69012 avg_loss = 3.53319\n",
      "epoch no.2 train no.173740  loss = 3.61852 avg_loss = 3.57943\n",
      "epoch no.2 train no.173750  loss = 2.44036 avg_loss = 3.56439\n",
      "epoch no.2 train no.173760  loss = 2.16686 avg_loss = 3.56143\n",
      "epoch no.2 train no.173770  loss = 3.77010 avg_loss = 3.51752\n",
      "epoch no.2 train no.173780  loss = 3.17611 avg_loss = 3.53385\n",
      "epoch no.2 train no.173790  loss = 1.68827 avg_loss = 3.48868\n",
      "epoch no.2 train no.173800  loss = 3.07322 avg_loss = 3.49096\n",
      "epoch no.2 train no.173810  loss = 2.47944 avg_loss = 3.52353\n",
      "epoch no.2 train no.173820  loss = 2.31835 avg_loss = 3.46309\n",
      "epoch no.2 train no.173830  loss = 2.43581 avg_loss = 3.44878\n",
      "epoch no.2 train no.173840  loss = 4.58638 avg_loss = 3.47546\n",
      "epoch no.2 train no.173850  loss = 3.10276 avg_loss = 3.44742\n",
      "epoch no.2 train no.173860  loss = 5.25409 avg_loss = 3.52359\n",
      "epoch no.2 train no.173870  loss = 5.21000 avg_loss = 3.48722\n",
      "epoch no.2 train no.173880  loss = 4.03811 avg_loss = 3.51318\n",
      "epoch no.2 train no.173890  loss = 3.34953 avg_loss = 3.60364\n",
      "epoch no.2 train no.173900  loss = 3.19995 avg_loss = 3.61252\n",
      "epoch no.2 train no.173910  loss = 5.08601 avg_loss = 3.66652\n",
      "epoch no.2 train no.173920  loss = 3.28362 avg_loss = 3.68861\n",
      "epoch no.2 train no.173930  loss = 3.01594 avg_loss = 3.64731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.173940  loss = 5.24642 avg_loss = 3.63003\n",
      "epoch no.2 train no.173950  loss = 5.09577 avg_loss = 3.62833\n",
      "epoch no.2 train no.173960  loss = 3.34969 avg_loss = 3.57453\n",
      "epoch no.2 train no.173970  loss = 4.17188 avg_loss = 3.62181\n",
      "epoch no.2 train no.173980  loss = 3.95246 avg_loss = 3.61811\n",
      "epoch no.2 train no.173990  loss = 2.07468 avg_loss = 3.57485\n",
      "epoch no.2 train no.174000  loss = 2.97110 avg_loss = 3.60131\n",
      "5\n",
      "to_tokens: ['▁가을', '▁명', '▁o', 'st', '▁모음', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.2 train no.174010  loss = 2.97443 avg_loss = 3.59518\n",
      "epoch no.2 train no.174020  loss = 4.73481 avg_loss = 3.62081\n",
      "epoch no.2 train no.174030  loss = 4.32055 avg_loss = 3.60738\n",
      "epoch no.2 train no.174040  loss = 3.26087 avg_loss = 3.64367\n",
      "epoch no.2 train no.174050  loss = 2.22070 avg_loss = 3.60326\n",
      "epoch no.2 train no.174060  loss = 2.81248 avg_loss = 3.56100\n",
      "epoch no.2 train no.174070  loss = 1.95150 avg_loss = 3.60124\n",
      "epoch no.2 train no.174080  loss = 4.32216 avg_loss = 3.62827\n",
      "epoch no.2 train no.174090  loss = 5.62101 avg_loss = 3.63056\n",
      "epoch no.2 train no.174100  loss = 3.42079 avg_loss = 3.65115\n",
      "epoch no.2 train no.174110  loss = 2.49130 avg_loss = 3.65420\n",
      "epoch no.2 train no.174120  loss = 2.34404 avg_loss = 3.58308\n",
      "epoch no.2 train no.174130  loss = 3.12445 avg_loss = 3.54789\n",
      "epoch no.2 train no.174140  loss = 4.02839 avg_loss = 3.58246\n",
      "epoch no.2 train no.174150  loss = 3.24063 avg_loss = 3.61318\n",
      "epoch no.2 train no.174160  loss = 3.96363 avg_loss = 3.61604\n",
      "epoch no.2 train no.174170  loss = 2.20893 avg_loss = 3.57154\n",
      "epoch no.2 train no.174180  loss = 2.80894 avg_loss = 3.54519\n",
      "epoch no.2 train no.174190  loss = 3.06059 avg_loss = 3.50427\n",
      "epoch no.2 train no.174200  loss = 4.22572 avg_loss = 3.50039\n",
      "epoch no.2 train no.174210  loss = 1.94380 avg_loss = 3.45764\n",
      "epoch no.2 train no.174220  loss = 3.26723 avg_loss = 3.46664\n",
      "epoch no.2 train no.174230  loss = 2.34033 avg_loss = 3.40471\n",
      "epoch no.2 train no.174240  loss = 4.03051 avg_loss = 3.37515\n",
      "epoch no.2 train no.174250  loss = 3.90163 avg_loss = 3.39142\n",
      "epoch no.2 train no.174260  loss = 2.07419 avg_loss = 3.40640\n",
      "epoch no.2 train no.174270  loss = 3.96991 avg_loss = 3.42870\n",
      "epoch no.2 train no.174280  loss = 3.38713 avg_loss = 3.46655\n",
      "epoch no.2 train no.174290  loss = 2.51455 avg_loss = 3.48361\n",
      "epoch no.2 train no.174300  loss = 5.11237 avg_loss = 3.53001\n",
      "epoch no.2 train no.174310  loss = 2.95708 avg_loss = 3.55265\n",
      "epoch no.2 train no.174320  loss = 6.16400 avg_loss = 3.56633\n",
      "epoch no.2 train no.174330  loss = 5.02638 avg_loss = 3.58626\n",
      "epoch no.2 train no.174340  loss = 4.62172 avg_loss = 3.62820\n",
      "epoch no.2 train no.174350  loss = 2.99685 avg_loss = 3.64091\n",
      "epoch no.2 train no.174360  loss = 2.51655 avg_loss = 3.61942\n",
      "epoch no.2 train no.174370  loss = 4.88388 avg_loss = 3.69379\n",
      "epoch no.2 train no.174380  loss = 3.37482 avg_loss = 3.64276\n",
      "epoch no.2 train no.174390  loss = 3.41990 avg_loss = 3.70346\n",
      "epoch no.2 train no.174400  loss = 3.04323 avg_loss = 3.64469\n",
      "epoch no.2 train no.174410  loss = 3.28962 avg_loss = 3.61615\n",
      "epoch no.2 train no.174420  loss = 5.90688 avg_loss = 3.60841\n",
      "epoch no.2 train no.174430  loss = 4.19207 avg_loss = 3.56408\n",
      "epoch no.2 train no.174440  loss = 2.51474 avg_loss = 3.47985\n",
      "epoch no.2 train no.174450  loss = 2.38050 avg_loss = 3.41604\n",
      "epoch no.2 train no.174460  loss = 2.57689 avg_loss = 3.45039\n",
      "epoch no.2 train no.174470  loss = 3.30632 avg_loss = 3.47350\n",
      "epoch no.2 train no.174480  loss = 3.77332 avg_loss = 3.52094\n",
      "epoch no.2 train no.174490  loss = 6.10554 avg_loss = 3.49622\n",
      "epoch no.2 train no.174500  loss = 3.87456 avg_loss = 3.48344\n",
      "epoch no.2 train no.174510  loss = 3.39308 avg_loss = 3.41352\n",
      "epoch no.2 train no.174520  loss = 4.69208 avg_loss = 3.42252\n",
      "epoch no.2 train no.174530  loss = 3.71125 avg_loss = 3.45429\n",
      "epoch no.2 train no.174540  loss = 5.48329 avg_loss = 3.51653\n",
      "epoch no.2 train no.174550  loss = 3.11182 avg_loss = 3.50147\n",
      "epoch no.2 train no.174560  loss = 4.59759 avg_loss = 3.47773\n",
      "epoch no.2 train no.174570  loss = 2.56390 avg_loss = 3.43633\n",
      "epoch no.2 train no.174580  loss = 2.96826 avg_loss = 3.44472\n",
      "epoch no.2 train no.174590  loss = 3.89196 avg_loss = 3.41762\n",
      "epoch no.2 train no.174600  loss = 2.42395 avg_loss = 3.40201\n",
      "epoch no.2 train no.174610  loss = 3.14728 avg_loss = 3.42088\n",
      "epoch no.2 train no.174620  loss = 4.44424 avg_loss = 3.43574\n",
      "epoch no.2 train no.174630  loss = 2.18399 avg_loss = 3.41588\n",
      "epoch no.2 train no.174640  loss = 5.15957 avg_loss = 3.42012\n",
      "epoch no.2 train no.174650  loss = 4.13666 avg_loss = 3.40766\n",
      "epoch no.2 train no.174660  loss = 3.75241 avg_loss = 3.42801\n",
      "epoch no.2 train no.174670  loss = 3.54750 avg_loss = 3.42157\n",
      "epoch no.2 train no.174680  loss = 2.98805 avg_loss = 3.43746\n",
      "epoch no.2 train no.174690  loss = 3.20160 avg_loss = 3.39656\n",
      "epoch no.2 train no.174700  loss = 2.78822 avg_loss = 3.45085\n",
      "epoch no.2 train no.174710  loss = 3.35208 avg_loss = 3.41732\n",
      "epoch no.2 train no.174720  loss = 3.16438 avg_loss = 3.42838\n",
      "epoch no.2 train no.174730  loss = 4.83715 avg_loss = 3.43183\n",
      "epoch no.2 train no.174740  loss = 4.23201 avg_loss = 3.44472\n",
      "epoch no.2 train no.174750  loss = 3.65438 avg_loss = 3.46019\n",
      "epoch no.2 train no.174760  loss = 4.59463 avg_loss = 3.52908\n",
      "epoch no.2 train no.174770  loss = 5.12340 avg_loss = 3.51354\n",
      "epoch no.2 train no.174780  loss = 2.56621 avg_loss = 3.54648\n",
      "epoch no.2 train no.174790  loss = 4.58786 avg_loss = 3.51930\n",
      "epoch no.2 train no.174800  loss = 3.52798 avg_loss = 3.50364\n",
      "epoch no.2 train no.174810  loss = 4.51044 avg_loss = 3.49694\n",
      "epoch no.2 train no.174820  loss = 3.35168 avg_loss = 3.49405\n",
      "epoch no.2 train no.174830  loss = 3.75186 avg_loss = 3.52303\n",
      "epoch no.2 train no.174840  loss = 3.28308 avg_loss = 3.46728\n",
      "epoch no.2 train no.174850  loss = 2.71579 avg_loss = 3.47647\n",
      "epoch no.2 train no.174860  loss = 5.26980 avg_loss = 3.49468\n",
      "epoch no.2 train no.174870  loss = 2.58556 avg_loss = 3.50560\n",
      "epoch no.2 train no.174880  loss = 2.99436 avg_loss = 3.47534\n",
      "epoch no.2 train no.174890  loss = 4.14525 avg_loss = 3.48548\n",
      "epoch no.2 train no.174900  loss = 3.10486 avg_loss = 3.49771\n",
      "epoch no.2 train no.174910  loss = 3.70360 avg_loss = 3.50028\n",
      "epoch no.2 train no.174920  loss = 3.21834 avg_loss = 3.50190\n",
      "epoch no.2 train no.174930  loss = 1.99888 avg_loss = 3.49200\n",
      "epoch no.2 train no.174940  loss = 3.10421 avg_loss = 3.47547\n",
      "epoch no.2 train no.174950  loss = 2.74084 avg_loss = 3.48193\n",
      "epoch no.2 train no.174960  loss = 4.99654 avg_loss = 3.50943\n",
      "epoch no.2 train no.174970  loss = 3.80997 avg_loss = 3.47058\n",
      "epoch no.2 train no.174980  loss = 4.62784 avg_loss = 3.49131\n",
      "epoch no.2 train no.174990  loss = 1.67874 avg_loss = 3.44860\n",
      "epoch no.2 train no.175000  loss = 3.92102 avg_loss = 3.42571\n",
      "4\n",
      "to_tokens: ['▁비', '▁팝', '곡', '들', '집', '</s>']\n",
      "추억의 명곡 모음집</s>\n",
      "epoch no.2 train no.175010  loss = 3.93109 avg_loss = 3.43217\n",
      "epoch no.2 train no.175020  loss = 2.49621 avg_loss = 3.40542\n",
      "epoch no.2 train no.175030  loss = 2.15457 avg_loss = 3.37034\n",
      "epoch no.2 train no.175040  loss = 3.63845 avg_loss = 3.36077\n",
      "epoch no.2 train no.175050  loss = 3.70992 avg_loss = 3.38466\n",
      "epoch no.2 train no.175060  loss = 3.11804 avg_loss = 3.35383\n",
      "epoch no.2 train no.175070  loss = 4.09488 avg_loss = 3.39250\n",
      "epoch no.2 train no.175080  loss = 2.70489 avg_loss = 3.40758\n",
      "epoch no.2 train no.175090  loss = 2.35496 avg_loss = 3.42339\n",
      "epoch no.2 train no.175100  loss = 3.54632 avg_loss = 3.41747\n",
      "epoch no.2 train no.175110  loss = 2.73268 avg_loss = 3.40130\n",
      "epoch no.2 train no.175120  loss = 3.37935 avg_loss = 3.37575\n",
      "epoch no.2 train no.175130  loss = 3.37323 avg_loss = 3.39522\n",
      "epoch no.2 train no.175140  loss = 4.63757 avg_loss = 3.45425\n",
      "epoch no.2 train no.175150  loss = 2.92570 avg_loss = 3.40587\n",
      "epoch no.2 train no.175160  loss = 2.76533 avg_loss = 3.35733\n",
      "epoch no.2 train no.175170  loss = 3.74364 avg_loss = 3.35778\n",
      "epoch no.2 train no.175180  loss = 3.84038 avg_loss = 3.37620\n",
      "epoch no.2 train no.175190  loss = 4.43139 avg_loss = 3.38851\n",
      "epoch no.2 train no.175200  loss = 3.19616 avg_loss = 3.38328\n",
      "epoch no.2 train no.175210  loss = 7.32474 avg_loss = 3.41478\n",
      "epoch no.2 train no.175220  loss = 3.48678 avg_loss = 3.37001\n",
      "epoch no.2 train no.175230  loss = 3.51033 avg_loss = 3.34747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.175240  loss = 4.54574 avg_loss = 3.37866\n",
      "epoch no.2 train no.175250  loss = 2.32147 avg_loss = 3.36715\n",
      "epoch no.2 train no.175260  loss = 3.36761 avg_loss = 3.35912\n",
      "epoch no.2 train no.175270  loss = 3.39874 avg_loss = 3.37534\n",
      "epoch no.2 train no.175280  loss = 2.65237 avg_loss = 3.36216\n",
      "epoch no.2 train no.175290  loss = 3.36996 avg_loss = 3.37633\n",
      "epoch no.2 train no.175300  loss = 2.05512 avg_loss = 3.38740\n",
      "epoch no.2 train no.175310  loss = 3.23082 avg_loss = 3.44049\n",
      "epoch no.2 train no.175320  loss = 2.69041 avg_loss = 3.43637\n",
      "epoch no.2 train no.175330  loss = 3.23019 avg_loss = 3.45208\n",
      "epoch no.2 train no.175340  loss = 3.29219 avg_loss = 3.42448\n",
      "epoch no.2 train no.175350  loss = 3.57912 avg_loss = 3.41271\n",
      "epoch no.2 train no.175360  loss = 3.28984 avg_loss = 3.43161\n",
      "epoch no.2 train no.175370  loss = 3.78528 avg_loss = 3.49316\n",
      "epoch no.2 train no.175380  loss = 3.31612 avg_loss = 3.49716\n",
      "epoch no.2 train no.175390  loss = 2.21428 avg_loss = 3.46332\n",
      "epoch no.2 train no.175400  loss = 2.55145 avg_loss = 3.46851\n",
      "epoch no.2 train no.175410  loss = 2.58164 avg_loss = 3.41870\n",
      "epoch no.2 train no.175420  loss = 5.05454 avg_loss = 3.44074\n",
      "epoch no.2 train no.175430  loss = 3.34742 avg_loss = 3.43605\n",
      "epoch no.2 train no.175440  loss = 2.81182 avg_loss = 3.47952\n",
      "epoch no.2 train no.175450  loss = 3.01598 avg_loss = 3.47487\n",
      "epoch no.2 train no.175460  loss = 3.30053 avg_loss = 3.44226\n",
      "epoch no.2 train no.175470  loss = 3.43344 avg_loss = 3.43904\n",
      "epoch no.2 train no.175480  loss = 6.08244 avg_loss = 3.46417\n",
      "epoch no.2 train no.175490  loss = 1.98726 avg_loss = 3.48922\n",
      "epoch no.2 train no.175500  loss = 3.21189 avg_loss = 3.50411\n",
      "epoch no.2 train no.175510  loss = 5.52154 avg_loss = 3.54289\n",
      "epoch no.2 train no.175520  loss = 2.86175 avg_loss = 3.52663\n",
      "epoch no.2 train no.175530  loss = 3.58433 avg_loss = 3.54360\n",
      "epoch no.2 train no.175540  loss = 3.53463 avg_loss = 3.56444\n",
      "epoch no.2 train no.175550  loss = 2.08706 avg_loss = 3.57344\n",
      "epoch no.2 train no.175560  loss = 2.62793 avg_loss = 3.57889\n",
      "epoch no.2 train no.175570  loss = 3.68823 avg_loss = 3.60055\n",
      "epoch no.2 train no.175580  loss = 3.46087 avg_loss = 3.57069\n",
      "epoch no.2 train no.175590  loss = 2.99734 avg_loss = 3.56154\n",
      "epoch no.2 train no.175600  loss = 2.57474 avg_loss = 3.58566\n",
      "epoch no.2 train no.175610  loss = 5.64834 avg_loss = 3.57408\n",
      "epoch no.2 train no.175620  loss = 1.88617 avg_loss = 3.51598\n",
      "epoch no.2 train no.175630  loss = 1.99893 avg_loss = 3.50975\n",
      "epoch no.2 train no.175640  loss = 4.17909 avg_loss = 3.50789\n",
      "epoch no.2 train no.175650  loss = 4.54710 avg_loss = 3.51095\n",
      "epoch no.2 train no.175660  loss = 2.54211 avg_loss = 3.53375\n",
      "epoch no.2 train no.175670  loss = 2.64197 avg_loss = 3.52389\n",
      "epoch no.2 train no.175680  loss = 4.58434 avg_loss = 3.52990\n",
      "epoch no.2 train no.175690  loss = 2.37354 avg_loss = 3.53581\n",
      "epoch no.2 train no.175700  loss = 2.26619 avg_loss = 3.54360\n",
      "epoch no.2 train no.175710  loss = 4.08789 avg_loss = 3.52517\n",
      "epoch no.2 train no.175720  loss = 3.05402 avg_loss = 3.53457\n",
      "epoch no.2 train no.175730  loss = 3.11701 avg_loss = 3.51048\n",
      "epoch no.2 train no.175740  loss = 3.88267 avg_loss = 3.50317\n",
      "epoch no.2 train no.175750  loss = 2.38068 avg_loss = 3.48590\n",
      "epoch no.2 train no.175760  loss = 2.76504 avg_loss = 3.53081\n",
      "epoch no.2 train no.175770  loss = 4.38918 avg_loss = 3.53478\n",
      "epoch no.2 train no.175780  loss = 2.68118 avg_loss = 3.51865\n",
      "epoch no.2 train no.175790  loss = 4.95850 avg_loss = 3.51694\n",
      "epoch no.2 train no.175800  loss = 4.18836 avg_loss = 3.50419\n",
      "epoch no.2 train no.175810  loss = 3.58067 avg_loss = 3.44921\n",
      "epoch no.2 train no.175820  loss = 3.53114 avg_loss = 3.44113\n",
      "epoch no.2 train no.175830  loss = 5.50077 avg_loss = 3.44096\n",
      "epoch no.2 train no.175840  loss = 2.24619 avg_loss = 3.41080\n",
      "epoch no.2 train no.175850  loss = 1.71262 avg_loss = 3.42437\n",
      "epoch no.2 train no.175860  loss = 3.41770 avg_loss = 3.42642\n",
      "epoch no.2 train no.175870  loss = 2.79837 avg_loss = 3.41203\n",
      "epoch no.2 train no.175880  loss = 2.35708 avg_loss = 3.38685\n",
      "epoch no.2 train no.175890  loss = 4.29062 avg_loss = 3.42663\n",
      "epoch no.2 train no.175900  loss = 2.11165 avg_loss = 3.40467\n",
      "epoch no.2 train no.175910  loss = 3.52894 avg_loss = 3.35846\n",
      "epoch no.2 train no.175920  loss = 3.66009 avg_loss = 3.33697\n",
      "epoch no.2 train no.175930  loss = 2.34060 avg_loss = 3.32708\n",
      "epoch no.2 train no.175940  loss = 2.43078 avg_loss = 3.34635\n",
      "epoch no.2 train no.175950  loss = 1.99523 avg_loss = 3.33373\n",
      "epoch no.2 train no.175960  loss = 4.59181 avg_loss = 3.34933\n",
      "epoch no.2 train no.175970  loss = 3.45049 avg_loss = 3.33383\n",
      "epoch no.2 train no.175980  loss = 4.97224 avg_loss = 3.30757\n",
      "epoch no.2 train no.175990  loss = 4.32124 avg_loss = 3.37364\n",
      "epoch no.2 train no.176000  loss = 2.69744 avg_loss = 3.41092\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.2 train no.176010  loss = 3.21935 avg_loss = 3.39312\n",
      "epoch no.2 train no.176020  loss = 3.52601 avg_loss = 3.41070\n",
      "epoch no.2 train no.176030  loss = 3.31340 avg_loss = 3.45688\n",
      "epoch no.2 train no.176040  loss = 3.14348 avg_loss = 3.46268\n",
      "epoch no.2 train no.176050  loss = 5.49470 avg_loss = 3.43970\n",
      "epoch no.2 train no.176060  loss = 2.47223 avg_loss = 3.39932\n",
      "epoch no.2 train no.176070  loss = 4.73655 avg_loss = 3.37926\n",
      "epoch no.2 train no.176080  loss = 1.88452 avg_loss = 3.36774\n",
      "epoch no.2 train no.176090  loss = 2.32815 avg_loss = 3.34777\n",
      "epoch no.2 train no.176100  loss = 4.34021 avg_loss = 3.35469\n",
      "epoch no.2 train no.176110  loss = 5.16233 avg_loss = 3.31897\n",
      "epoch no.2 train no.176120  loss = 3.33799 avg_loss = 3.33004\n",
      "epoch no.2 train no.176130  loss = 4.00792 avg_loss = 3.36848\n",
      "epoch no.2 train no.176140  loss = 4.55230 avg_loss = 3.37759\n",
      "epoch no.2 train no.176150  loss = 3.10945 avg_loss = 3.38409\n",
      "epoch no.2 train no.176160  loss = 3.27238 avg_loss = 3.34772\n",
      "epoch no.2 train no.176170  loss = 2.93635 avg_loss = 3.32804\n",
      "epoch no.2 train no.176180  loss = 3.22251 avg_loss = 3.28972\n",
      "epoch no.2 train no.176190  loss = 2.29523 avg_loss = 3.29817\n",
      "epoch no.2 train no.176200  loss = 3.26628 avg_loss = 3.27716\n",
      "epoch no.2 train no.176210  loss = 4.43573 avg_loss = 3.38301\n",
      "epoch no.2 train no.176220  loss = 2.80365 avg_loss = 3.32978\n",
      "epoch no.2 train no.176230  loss = 2.83679 avg_loss = 3.37813\n",
      "epoch no.2 train no.176240  loss = 2.27859 avg_loss = 3.37769\n",
      "epoch no.2 train no.176250  loss = 3.63008 avg_loss = 3.40205\n",
      "epoch no.2 train no.176260  loss = 4.62400 avg_loss = 3.41426\n",
      "epoch no.2 train no.176270  loss = 2.79559 avg_loss = 3.42622\n",
      "epoch no.2 train no.176280  loss = 2.58209 avg_loss = 3.40883\n",
      "epoch no.2 train no.176290  loss = 3.08841 avg_loss = 3.43502\n",
      "epoch no.2 train no.176300  loss = 2.69044 avg_loss = 3.41639\n",
      "epoch no.2 train no.176310  loss = 3.50880 avg_loss = 3.44960\n",
      "epoch no.2 train no.176320  loss = 4.29524 avg_loss = 3.46908\n",
      "epoch no.2 train no.176330  loss = 4.01400 avg_loss = 3.44934\n",
      "epoch no.2 train no.176340  loss = 2.71261 avg_loss = 3.52128\n",
      "epoch no.2 train no.176350  loss = 4.14676 avg_loss = 3.51674\n",
      "epoch no.2 train no.176360  loss = 1.89172 avg_loss = 3.51672\n",
      "epoch no.2 train no.176370  loss = 4.57849 avg_loss = 3.52709\n",
      "epoch no.2 train no.176380  loss = 2.04829 avg_loss = 3.45796\n",
      "epoch no.2 train no.176390  loss = 4.24959 avg_loss = 3.48871\n",
      "epoch no.2 train no.176400  loss = 6.29101 avg_loss = 3.50474\n",
      "epoch no.2 train no.176410  loss = 2.62017 avg_loss = 3.53611\n",
      "epoch no.2 train no.176420  loss = 2.97690 avg_loss = 3.53189\n",
      "epoch no.2 train no.176430  loss = 3.42855 avg_loss = 3.54409\n",
      "epoch no.2 train no.176440  loss = 4.57851 avg_loss = 3.59510\n",
      "epoch no.2 train no.176450  loss = 2.47897 avg_loss = 3.62262\n",
      "epoch no.2 train no.176460  loss = 1.70262 avg_loss = 3.56831\n",
      "epoch no.2 train no.176470  loss = 2.76599 avg_loss = 3.54416\n",
      "epoch no.2 train no.176480  loss = 3.68445 avg_loss = 3.56519\n",
      "epoch no.2 train no.176490  loss = 3.61317 avg_loss = 3.51686\n",
      "epoch no.2 train no.176500  loss = 3.57302 avg_loss = 3.52464\n",
      "epoch no.2 train no.176510  loss = 2.48640 avg_loss = 3.57036\n",
      "epoch no.2 train no.176520  loss = 4.40454 avg_loss = 3.54069\n",
      "epoch no.2 train no.176530  loss = 1.93654 avg_loss = 3.52965\n",
      "epoch no.2 train no.176540  loss = 5.49857 avg_loss = 3.50635\n",
      "epoch no.2 train no.176550  loss = 1.98487 avg_loss = 3.55798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.176560  loss = 3.76950 avg_loss = 3.55576\n",
      "epoch no.2 train no.176570  loss = 2.69923 avg_loss = 3.53254\n",
      "epoch no.2 train no.176580  loss = 4.28103 avg_loss = 3.52407\n",
      "epoch no.2 train no.176590  loss = 4.64733 avg_loss = 3.53242\n",
      "epoch no.2 train no.176600  loss = 3.04085 avg_loss = 3.57841\n",
      "epoch no.2 train no.176610  loss = 2.69280 avg_loss = 3.54859\n",
      "epoch no.2 train no.176620  loss = 2.69596 avg_loss = 3.53983\n",
      "epoch no.2 train no.176630  loss = 2.85641 avg_loss = 3.53054\n",
      "epoch no.2 train no.176640  loss = 4.23767 avg_loss = 3.59282\n",
      "epoch no.2 train no.176650  loss = 4.61535 avg_loss = 3.62501\n",
      "epoch no.2 train no.176660  loss = 4.21641 avg_loss = 3.66285\n",
      "epoch no.2 train no.176670  loss = 2.57843 avg_loss = 3.64920\n",
      "epoch no.2 train no.176680  loss = 3.27089 avg_loss = 3.65613\n",
      "epoch no.2 train no.176690  loss = 3.42106 avg_loss = 3.64515\n",
      "epoch no.2 train no.176700  loss = 2.95077 avg_loss = 3.65922\n",
      "epoch no.2 train no.176710  loss = 3.32447 avg_loss = 3.63563\n",
      "epoch no.2 train no.176720  loss = 4.17325 avg_loss = 3.59088\n",
      "epoch no.2 train no.176730  loss = 2.25751 avg_loss = 3.63182\n",
      "epoch no.2 train no.176740  loss = 3.07251 avg_loss = 3.62616\n",
      "epoch no.2 train no.176750  loss = 2.96501 avg_loss = 3.64923\n",
      "epoch no.2 train no.176760  loss = 4.03292 avg_loss = 3.63812\n",
      "epoch no.2 train no.176770  loss = 4.18061 avg_loss = 3.60479\n",
      "epoch no.2 train no.176780  loss = 4.35999 avg_loss = 3.57911\n",
      "epoch no.2 train no.176790  loss = 4.56830 avg_loss = 3.57258\n",
      "epoch no.2 train no.176800  loss = 2.70367 avg_loss = 3.55423\n",
      "epoch no.2 train no.176810  loss = 2.71584 avg_loss = 3.57307\n",
      "epoch no.2 train no.176820  loss = 5.07485 avg_loss = 3.64449\n",
      "epoch no.2 train no.176830  loss = 3.00240 avg_loss = 3.56310\n",
      "epoch no.2 train no.176840  loss = 2.96336 avg_loss = 3.56303\n",
      "epoch no.2 train no.176850  loss = 3.31290 avg_loss = 3.56789\n",
      "epoch no.2 train no.176860  loss = 2.11352 avg_loss = 3.51867\n",
      "epoch no.2 train no.176870  loss = 3.18171 avg_loss = 3.54062\n",
      "epoch no.2 train no.176880  loss = 3.64162 avg_loss = 3.54347\n",
      "epoch no.2 train no.176890  loss = 4.55237 avg_loss = 3.57633\n",
      "epoch no.2 train no.176900  loss = 3.90585 avg_loss = 3.54108\n",
      "epoch no.2 train no.176910  loss = 3.51327 avg_loss = 3.56337\n",
      "epoch no.2 train no.176920  loss = 1.89293 avg_loss = 3.59047\n",
      "epoch no.2 train no.176930  loss = 3.86103 avg_loss = 3.59237\n",
      "epoch no.2 train no.176940  loss = 4.62647 avg_loss = 3.59796\n",
      "epoch no.2 train no.176950  loss = 4.44617 avg_loss = 3.56103\n",
      "epoch no.2 train no.176960  loss = 2.89788 avg_loss = 3.53853\n",
      "epoch no.2 train no.176970  loss = 3.77575 avg_loss = 3.54534\n",
      "epoch no.2 train no.176980  loss = 3.49938 avg_loss = 3.56105\n",
      "epoch no.2 train no.176990  loss = 2.61194 avg_loss = 3.51447\n",
      "epoch no.2 train no.177000  loss = 3.46289 avg_loss = 3.46901\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁모음', '집', '</s>']\n",
      "추억의 발라드 모음집</s>\n",
      "epoch no.2 train no.177010  loss = 3.40023 avg_loss = 3.45977\n",
      "epoch no.2 train no.177020  loss = 4.21545 avg_loss = 3.43993\n",
      "epoch no.2 train no.177030  loss = 4.95592 avg_loss = 3.48439\n",
      "epoch no.2 train no.177040  loss = 4.78927 avg_loss = 3.45652\n",
      "epoch no.2 train no.177050  loss = 4.20157 avg_loss = 3.41083\n",
      "epoch no.2 train no.177060  loss = 3.05971 avg_loss = 3.43846\n",
      "epoch no.2 train no.177070  loss = 3.17218 avg_loss = 3.49448\n",
      "epoch no.2 train no.177080  loss = 4.18582 avg_loss = 3.45599\n",
      "epoch no.2 train no.177090  loss = 4.10838 avg_loss = 3.49460\n",
      "epoch no.2 train no.177100  loss = 1.90143 avg_loss = 3.50133\n",
      "epoch no.2 train no.177110  loss = 5.92582 avg_loss = 3.50086\n",
      "epoch no.2 train no.177120  loss = 2.37411 avg_loss = 3.47426\n",
      "epoch no.2 train no.177130  loss = 3.28716 avg_loss = 3.44635\n",
      "epoch no.2 train no.177140  loss = 3.65081 avg_loss = 3.48123\n",
      "epoch no.2 train no.177150  loss = 2.67565 avg_loss = 3.48340\n",
      "epoch no.2 train no.177160  loss = 3.50462 avg_loss = 3.48352\n",
      "epoch no.2 train no.177170  loss = 4.39027 avg_loss = 3.51999\n",
      "epoch no.2 train no.177180  loss = 4.17987 avg_loss = 3.48577\n",
      "epoch no.2 train no.177190  loss = 3.03157 avg_loss = 3.43942\n",
      "epoch no.2 train no.177200  loss = 3.43631 avg_loss = 3.42109\n",
      "epoch no.2 train no.177210  loss = 3.78800 avg_loss = 3.42171\n",
      "epoch no.2 train no.177220  loss = 3.37995 avg_loss = 3.38666\n",
      "epoch no.2 train no.177230  loss = 3.19137 avg_loss = 3.38517\n",
      "epoch no.2 train no.177240  loss = 3.75438 avg_loss = 3.33193\n",
      "epoch no.2 train no.177250  loss = 2.74322 avg_loss = 3.33004\n",
      "epoch no.2 train no.177260  loss = 3.07330 avg_loss = 3.36330\n",
      "epoch no.2 train no.177270  loss = 4.67807 avg_loss = 3.34094\n",
      "epoch no.2 train no.177280  loss = 2.96977 avg_loss = 3.36268\n",
      "epoch no.2 train no.177290  loss = 3.31565 avg_loss = 3.38051\n",
      "epoch no.2 train no.177300  loss = 1.87009 avg_loss = 3.36135\n",
      "epoch no.2 train no.177310  loss = 3.46655 avg_loss = 3.40732\n",
      "epoch no.2 train no.177320  loss = 4.05064 avg_loss = 3.41120\n",
      "epoch no.2 train no.177330  loss = 5.63664 avg_loss = 3.45488\n",
      "epoch no.2 train no.177340  loss = 3.05245 avg_loss = 3.48126\n",
      "epoch no.2 train no.177350  loss = 2.60465 avg_loss = 3.46862\n",
      "epoch no.2 train no.177360  loss = 4.47460 avg_loss = 3.48364\n",
      "epoch no.2 train no.177370  loss = 2.17837 avg_loss = 3.51588\n",
      "epoch no.2 train no.177380  loss = 3.66987 avg_loss = 3.53396\n",
      "epoch no.2 train no.177390  loss = 3.53256 avg_loss = 3.53308\n",
      "epoch no.2 train no.177400  loss = 3.84957 avg_loss = 3.49094\n",
      "epoch no.2 train no.177410  loss = 3.48727 avg_loss = 3.48031\n",
      "epoch no.2 train no.177420  loss = 3.66588 avg_loss = 3.47271\n",
      "epoch no.2 train no.177430  loss = 4.48488 avg_loss = 3.47293\n",
      "epoch no.2 train no.177440  loss = 2.26752 avg_loss = 3.43420\n",
      "epoch no.2 train no.177450  loss = 3.30021 avg_loss = 3.44113\n",
      "epoch no.2 train no.177460  loss = 6.25745 avg_loss = 3.45838\n",
      "epoch no.2 train no.177470  loss = 2.31270 avg_loss = 3.42833\n",
      "epoch no.2 train no.177480  loss = 4.70514 avg_loss = 3.40679\n",
      "epoch no.2 train no.177490  loss = 2.92817 avg_loss = 3.41139\n",
      "epoch no.2 train no.177500  loss = 3.26637 avg_loss = 3.41486\n",
      "epoch no.2 train no.177510  loss = 3.56687 avg_loss = 3.43887\n",
      "epoch no.2 train no.177520  loss = 3.37581 avg_loss = 3.46695\n",
      "epoch no.2 train no.177530  loss = 2.82025 avg_loss = 3.42338\n",
      "epoch no.2 train no.177540  loss = 1.94594 avg_loss = 3.42320\n",
      "epoch no.2 train no.177550  loss = 4.47479 avg_loss = 3.37236\n",
      "epoch no.2 train no.177560  loss = 3.37749 avg_loss = 3.36607\n",
      "epoch no.2 train no.177570  loss = 2.11928 avg_loss = 3.37995\n",
      "epoch no.2 train no.177580  loss = 6.15593 avg_loss = 3.42789\n",
      "epoch no.2 train no.177590  loss = 1.83790 avg_loss = 3.41386\n",
      "epoch no.2 train no.177600  loss = 3.09499 avg_loss = 3.40128\n",
      "epoch no.2 train no.177610  loss = 2.73720 avg_loss = 3.41482\n",
      "epoch no.2 train no.177620  loss = 4.26028 avg_loss = 3.45517\n",
      "epoch no.2 train no.177630  loss = 3.04067 avg_loss = 3.48475\n",
      "epoch no.2 train no.177640  loss = 3.81617 avg_loss = 3.44904\n",
      "epoch no.2 train no.177650  loss = 5.70567 avg_loss = 3.51014\n",
      "epoch no.2 train no.177660  loss = 5.83417 avg_loss = 3.49754\n",
      "epoch no.2 train no.177670  loss = 2.01076 avg_loss = 3.48822\n",
      "epoch no.2 train no.177680  loss = 3.63519 avg_loss = 3.49691\n",
      "epoch no.2 train no.177690  loss = 4.63471 avg_loss = 3.45582\n",
      "epoch no.2 train no.177700  loss = 3.64078 avg_loss = 3.50184\n",
      "epoch no.2 train no.177710  loss = 2.15229 avg_loss = 3.46503\n",
      "epoch no.2 train no.177720  loss = 2.48200 avg_loss = 3.45484\n",
      "epoch no.2 train no.177730  loss = 3.05471 avg_loss = 3.43716\n",
      "epoch no.2 train no.177740  loss = 3.06331 avg_loss = 3.45171\n",
      "epoch no.2 train no.177750  loss = 3.88287 avg_loss = 3.48738\n",
      "epoch no.2 train no.177760  loss = 2.23331 avg_loss = 3.44512\n",
      "epoch no.2 train no.177770  loss = 4.36593 avg_loss = 3.46135\n",
      "epoch no.2 train no.177780  loss = 3.09054 avg_loss = 3.47841\n",
      "epoch no.2 train no.177790  loss = 4.67940 avg_loss = 3.49000\n",
      "epoch no.2 train no.177800  loss = 3.55829 avg_loss = 3.50265\n",
      "epoch no.2 train no.177810  loss = 3.93932 avg_loss = 3.50578\n",
      "epoch no.2 train no.177820  loss = 2.20173 avg_loss = 3.45578\n",
      "epoch no.2 train no.177830  loss = 1.76437 avg_loss = 3.44001\n",
      "epoch no.2 train no.177840  loss = 3.89430 avg_loss = 3.43684\n",
      "epoch no.2 train no.177850  loss = 3.82122 avg_loss = 3.41246\n",
      "epoch no.2 train no.177860  loss = 4.60353 avg_loss = 3.42927\n",
      "epoch no.2 train no.177870  loss = 3.01679 avg_loss = 3.45521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.177880  loss = 2.79020 avg_loss = 3.43891\n",
      "epoch no.2 train no.177890  loss = 3.25484 avg_loss = 3.41280\n",
      "epoch no.2 train no.177900  loss = 3.36896 avg_loss = 3.40049\n",
      "epoch no.2 train no.177910  loss = 3.02457 avg_loss = 3.39890\n",
      "epoch no.2 train no.177920  loss = 3.58002 avg_loss = 3.38717\n",
      "epoch no.2 train no.177930  loss = 3.38065 avg_loss = 3.41690\n",
      "epoch no.2 train no.177940  loss = 2.00345 avg_loss = 3.37689\n",
      "epoch no.2 train no.177950  loss = 2.27066 avg_loss = 3.37541\n",
      "epoch no.2 train no.177960  loss = 3.30382 avg_loss = 3.36829\n",
      "epoch no.2 train no.177970  loss = 4.83316 avg_loss = 3.40797\n",
      "epoch no.2 train no.177980  loss = 3.09963 avg_loss = 3.40377\n",
      "epoch no.2 train no.177990  loss = 4.13012 avg_loss = 3.41699\n",
      "epoch no.2 train no.178000  loss = 1.82382 avg_loss = 3.37526\n",
      "4\n",
      "to_tokens: ['▁카페에서', '▁90', '곡', '▁모음', '집', '</s>']\n",
      "추억의 댄스곡 모음곡</s>\n",
      "epoch no.2 train no.178010  loss = 2.73463 avg_loss = 3.32482\n",
      "epoch no.2 train no.178020  loss = 4.14478 avg_loss = 3.36944\n",
      "epoch no.2 train no.178030  loss = 2.13661 avg_loss = 3.37147\n",
      "epoch no.2 train no.178040  loss = 5.32309 avg_loss = 3.39015\n",
      "epoch no.2 train no.178050  loss = 3.48263 avg_loss = 3.41961\n",
      "epoch no.2 train no.178060  loss = 2.31308 avg_loss = 3.42003\n",
      "epoch no.2 train no.178070  loss = 2.94676 avg_loss = 3.43379\n",
      "epoch no.2 train no.178080  loss = 4.26677 avg_loss = 3.42577\n",
      "epoch no.2 train no.178090  loss = 3.97431 avg_loss = 3.45435\n",
      "epoch no.2 train no.178100  loss = 2.93492 avg_loss = 3.44866\n",
      "epoch no.2 train no.178110  loss = 3.56588 avg_loss = 3.48303\n",
      "epoch no.2 train no.178120  loss = 2.88675 avg_loss = 3.44758\n",
      "epoch no.2 train no.178130  loss = 3.42094 avg_loss = 3.42135\n",
      "epoch no.2 train no.178140  loss = 3.52339 avg_loss = 3.37870\n",
      "epoch no.2 train no.178150  loss = 2.13263 avg_loss = 3.31840\n",
      "epoch no.2 train no.178160  loss = 3.02438 avg_loss = 3.35449\n",
      "epoch no.2 train no.178170  loss = 4.21684 avg_loss = 3.34243\n",
      "epoch no.2 train no.178180  loss = 2.48798 avg_loss = 3.35226\n",
      "epoch no.2 train no.178190  loss = 3.51046 avg_loss = 3.35436\n",
      "epoch no.2 train no.178200  loss = 2.46642 avg_loss = 3.36024\n",
      "epoch no.2 train no.178210  loss = 2.60934 avg_loss = 3.34308\n",
      "epoch no.2 train no.178220  loss = 5.34456 avg_loss = 3.36597\n",
      "epoch no.2 train no.178230  loss = 3.77550 avg_loss = 3.38632\n",
      "epoch no.2 train no.178240  loss = 3.92017 avg_loss = 3.38275\n",
      "epoch no.2 train no.178250  loss = 3.92367 avg_loss = 3.37054\n",
      "epoch no.2 train no.178260  loss = 4.79594 avg_loss = 3.41179\n",
      "epoch no.2 train no.178270  loss = 3.03582 avg_loss = 3.37096\n",
      "epoch no.2 train no.178280  loss = 2.54284 avg_loss = 3.35120\n",
      "epoch no.2 train no.178290  loss = 3.37625 avg_loss = 3.36694\n",
      "epoch no.2 train no.178300  loss = 4.90552 avg_loss = 3.38033\n",
      "epoch no.2 train no.178310  loss = 1.97763 avg_loss = 3.37347\n",
      "epoch no.2 train no.178320  loss = 3.02239 avg_loss = 3.36114\n",
      "epoch no.2 train no.178330  loss = 4.69782 avg_loss = 3.41266\n",
      "epoch no.2 train no.178340  loss = 2.90261 avg_loss = 3.40398\n",
      "epoch no.2 train no.178350  loss = 3.75190 avg_loss = 3.48593\n",
      "epoch no.2 train no.178360  loss = 4.94772 avg_loss = 3.47700\n",
      "epoch no.2 train no.178370  loss = 1.82107 avg_loss = 3.48296\n",
      "epoch no.2 train no.178380  loss = 1.68595 avg_loss = 3.46859\n",
      "epoch no.2 train no.178390  loss = 4.38238 avg_loss = 3.47444\n",
      "epoch no.2 train no.178400  loss = 2.76509 avg_loss = 3.50246\n",
      "epoch no.2 train no.178410  loss = 4.25736 avg_loss = 3.52612\n",
      "epoch no.2 train no.178420  loss = 4.30341 avg_loss = 3.54485\n",
      "epoch no.2 train no.178430  loss = 5.16731 avg_loss = 3.54885\n",
      "epoch no.2 train no.178440  loss = 3.88728 avg_loss = 3.55894\n",
      "epoch no.2 train no.178450  loss = 1.79651 avg_loss = 3.53728\n",
      "epoch no.2 train no.178460  loss = 3.04017 avg_loss = 3.51463\n",
      "epoch no.2 train no.178470  loss = 4.43996 avg_loss = 3.53762\n",
      "epoch no.2 train no.178480  loss = 3.53462 avg_loss = 3.52057\n",
      "epoch no.2 train no.178490  loss = 3.81288 avg_loss = 3.52359\n",
      "epoch no.2 train no.178500  loss = 3.68400 avg_loss = 3.53373\n",
      "epoch no.2 train no.178510  loss = 6.28135 avg_loss = 3.56384\n",
      "epoch no.2 train no.178520  loss = 2.05213 avg_loss = 3.54962\n",
      "epoch no.2 train no.178530  loss = 2.74000 avg_loss = 3.50734\n",
      "epoch no.2 train no.178540  loss = 2.27487 avg_loss = 3.46901\n",
      "epoch no.2 train no.178550  loss = 3.20509 avg_loss = 3.46832\n",
      "epoch no.2 train no.178560  loss = 2.87010 avg_loss = 3.46904\n",
      "epoch no.2 train no.178570  loss = 4.02379 avg_loss = 3.52133\n",
      "epoch no.2 train no.178580  loss = 3.27187 avg_loss = 3.51756\n",
      "epoch no.2 train no.178590  loss = 1.76303 avg_loss = 3.50129\n",
      "epoch no.2 train no.178600  loss = 3.67962 avg_loss = 3.50251\n",
      "epoch no.2 train no.178610  loss = 2.17289 avg_loss = 3.47884\n",
      "epoch no.2 train no.178620  loss = 4.11327 avg_loss = 3.51202\n",
      "epoch no.2 train no.178630  loss = 2.47512 avg_loss = 3.47275\n",
      "epoch no.2 train no.178640  loss = 5.95664 avg_loss = 3.50814\n",
      "epoch no.2 train no.178650  loss = 2.44806 avg_loss = 3.54581\n",
      "epoch no.2 train no.178660  loss = 2.97384 avg_loss = 3.54904\n",
      "epoch no.2 train no.178670  loss = 3.30776 avg_loss = 3.54963\n",
      "epoch no.2 train no.178680  loss = 3.16197 avg_loss = 3.53955\n",
      "epoch no.2 train no.178690  loss = 4.28128 avg_loss = 3.52544\n",
      "epoch no.2 train no.178700  loss = 3.63345 avg_loss = 3.50015\n",
      "epoch no.2 train no.178710  loss = 3.13135 avg_loss = 3.50091\n",
      "epoch no.2 train no.178720  loss = 4.10259 avg_loss = 3.50769\n",
      "epoch no.2 train no.178730  loss = 4.51334 avg_loss = 3.48608\n",
      "epoch no.2 train no.178740  loss = 3.13405 avg_loss = 3.48272\n",
      "epoch no.2 train no.178750  loss = 2.47940 avg_loss = 3.49045\n",
      "epoch no.2 train no.178760  loss = 2.93197 avg_loss = 3.48782\n",
      "epoch no.2 train no.178770  loss = 5.00826 avg_loss = 3.48098\n",
      "epoch no.2 train no.178780  loss = 3.01063 avg_loss = 3.44965\n",
      "epoch no.2 train no.178790  loss = 2.70963 avg_loss = 3.46177\n",
      "epoch no.2 train no.178800  loss = 3.09865 avg_loss = 3.46774\n",
      "epoch no.2 train no.178810  loss = 2.93064 avg_loss = 3.46024\n",
      "epoch no.2 train no.178820  loss = 2.61145 avg_loss = 3.49340\n",
      "epoch no.2 train no.178830  loss = 3.61754 avg_loss = 3.53195\n",
      "epoch no.2 train no.178840  loss = 2.65673 avg_loss = 3.49379\n",
      "epoch no.2 train no.178850  loss = 2.89248 avg_loss = 3.55627\n",
      "epoch no.2 train no.178860  loss = 2.92326 avg_loss = 3.54221\n",
      "epoch no.2 train no.178870  loss = 5.09498 avg_loss = 3.54971\n",
      "epoch no.2 train no.178880  loss = 2.65860 avg_loss = 3.52641\n",
      "epoch no.2 train no.178890  loss = 2.31628 avg_loss = 3.48876\n",
      "epoch no.2 train no.178900  loss = 2.98321 avg_loss = 3.48503\n",
      "epoch no.2 train no.178910  loss = 4.84699 avg_loss = 3.48933\n",
      "epoch no.2 train no.178920  loss = 1.46836 avg_loss = 3.43581\n",
      "epoch no.2 train no.178930  loss = 2.79904 avg_loss = 3.43907\n",
      "epoch no.2 train no.178940  loss = 3.42258 avg_loss = 3.44625\n",
      "epoch no.2 train no.178950  loss = 2.25980 avg_loss = 3.44114\n",
      "epoch no.2 train no.178960  loss = 3.21510 avg_loss = 3.48284\n",
      "epoch no.2 train no.178970  loss = 3.43049 avg_loss = 3.45764\n",
      "epoch no.2 train no.178980  loss = 6.53642 avg_loss = 3.45081\n",
      "epoch no.2 train no.178990  loss = 4.42612 avg_loss = 3.48093\n",
      "epoch no.2 train no.179000  loss = 2.18975 avg_loss = 3.48552\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁댄스', '곡', '</s>']\n",
      "추억의 90년대 댄스 댄스</s>\n",
      "epoch no.2 train no.179010  loss = 4.66669 avg_loss = 3.51086\n",
      "epoch no.2 train no.179020  loss = 2.12705 avg_loss = 3.48426\n",
      "epoch no.2 train no.179030  loss = 2.86600 avg_loss = 3.46212\n",
      "epoch no.2 train no.179040  loss = 3.10818 avg_loss = 3.43021\n",
      "epoch no.2 train no.179050  loss = 3.25255 avg_loss = 3.43128\n",
      "epoch no.2 train no.179060  loss = 4.17689 avg_loss = 3.45350\n",
      "epoch no.2 train no.179070  loss = 3.33128 avg_loss = 3.49861\n",
      "epoch no.2 train no.179080  loss = 4.42726 avg_loss = 3.48579\n",
      "epoch no.2 train no.179090  loss = 4.66486 avg_loss = 3.47259\n",
      "epoch no.2 train no.179100  loss = 3.47657 avg_loss = 3.42862\n",
      "epoch no.2 train no.179110  loss = 5.21101 avg_loss = 3.47593\n",
      "epoch no.2 train no.179120  loss = 3.97459 avg_loss = 3.48820\n",
      "epoch no.2 train no.179130  loss = 3.47249 avg_loss = 3.44296\n",
      "epoch no.2 train no.179140  loss = 3.74024 avg_loss = 3.42891\n",
      "epoch no.2 train no.179150  loss = 4.97124 avg_loss = 3.47700\n",
      "epoch no.2 train no.179160  loss = 4.05509 avg_loss = 3.47292\n",
      "epoch no.2 train no.179170  loss = 6.11571 avg_loss = 3.54440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.179180  loss = 2.41569 avg_loss = 3.55282\n",
      "epoch no.2 train no.179190  loss = 3.04317 avg_loss = 3.53943\n",
      "epoch no.2 train no.179200  loss = 4.51592 avg_loss = 3.56664\n",
      "epoch no.2 train no.179210  loss = 2.67107 avg_loss = 3.54119\n",
      "epoch no.2 train no.179220  loss = 4.06737 avg_loss = 3.53816\n",
      "epoch no.2 train no.179230  loss = 4.36004 avg_loss = 3.61981\n",
      "epoch no.2 train no.179240  loss = 3.63382 avg_loss = 3.58677\n",
      "epoch no.2 train no.179250  loss = 3.93974 avg_loss = 3.56229\n",
      "epoch no.2 train no.179260  loss = 3.40115 avg_loss = 3.57227\n",
      "epoch no.2 train no.179270  loss = 3.39651 avg_loss = 3.59791\n",
      "epoch no.2 train no.179280  loss = 4.75285 avg_loss = 3.53173\n",
      "epoch no.2 train no.179290  loss = 3.68050 avg_loss = 3.46672\n",
      "epoch no.2 train no.179300  loss = 2.57919 avg_loss = 3.45160\n",
      "epoch no.2 train no.179310  loss = 3.40744 avg_loss = 3.45351\n",
      "epoch no.2 train no.179320  loss = 2.84244 avg_loss = 3.44868\n",
      "epoch no.2 train no.179330  loss = 2.46961 avg_loss = 3.42845\n",
      "epoch no.2 train no.179340  loss = 3.19673 avg_loss = 3.42258\n",
      "epoch no.2 train no.179350  loss = 2.43996 avg_loss = 3.42994\n",
      "epoch no.2 train no.179360  loss = 2.65796 avg_loss = 3.40113\n",
      "epoch no.2 train no.179370  loss = 1.84332 avg_loss = 3.37622\n",
      "epoch no.2 train no.179380  loss = 3.38689 avg_loss = 3.38059\n",
      "epoch no.2 train no.179390  loss = 2.60103 avg_loss = 3.36428\n",
      "epoch no.2 train no.179400  loss = 3.82028 avg_loss = 3.34692\n",
      "epoch no.2 train no.179410  loss = 4.88099 avg_loss = 3.38676\n",
      "epoch no.2 train no.179420  loss = 3.86003 avg_loss = 3.35196\n",
      "epoch no.2 train no.179430  loss = 2.85421 avg_loss = 3.38153\n",
      "epoch no.2 train no.179440  loss = 4.97586 avg_loss = 3.47127\n",
      "epoch no.2 train no.179450  loss = 3.90996 avg_loss = 3.49852\n",
      "epoch no.2 train no.179460  loss = 5.07783 avg_loss = 3.47384\n",
      "epoch no.2 train no.179470  loss = 3.94479 avg_loss = 3.47011\n",
      "epoch no.2 train no.179480  loss = 4.29667 avg_loss = 3.44149\n",
      "epoch no.2 train no.179490  loss = 3.11003 avg_loss = 3.41362\n",
      "epoch no.2 train no.179500  loss = 4.81446 avg_loss = 3.45567\n",
      "epoch no.2 train no.179510  loss = 3.15557 avg_loss = 3.45741\n",
      "epoch no.2 train no.179520  loss = 2.92922 avg_loss = 3.46337\n",
      "epoch no.2 train no.179530  loss = 4.79745 avg_loss = 3.51927\n",
      "epoch no.2 train no.179540  loss = 3.13361 avg_loss = 3.49574\n",
      "epoch no.2 train no.179550  loss = 3.71137 avg_loss = 3.45494\n",
      "epoch no.2 train no.179560  loss = 2.16145 avg_loss = 3.42417\n",
      "epoch no.2 train no.179570  loss = 3.73328 avg_loss = 3.41287\n",
      "epoch no.2 train no.179580  loss = 4.54801 avg_loss = 3.40352\n",
      "epoch no.2 train no.179590  loss = 4.36869 avg_loss = 3.39548\n",
      "epoch no.2 train no.179600  loss = 3.67242 avg_loss = 3.41285\n",
      "epoch no.2 train no.179610  loss = 4.08182 avg_loss = 3.41357\n",
      "epoch no.2 train no.179620  loss = 3.21751 avg_loss = 3.44729\n",
      "epoch no.2 train no.179630  loss = 3.78017 avg_loss = 3.42182\n",
      "epoch no.2 train no.179640  loss = 5.10895 avg_loss = 3.45420\n",
      "epoch no.2 train no.179650  loss = 4.23932 avg_loss = 3.51079\n",
      "epoch no.2 train no.179660  loss = 3.86233 avg_loss = 3.48464\n",
      "epoch no.2 train no.179670  loss = 3.96606 avg_loss = 3.52329\n",
      "epoch no.2 train no.179680  loss = 1.90972 avg_loss = 3.55539\n",
      "epoch no.2 train no.179690  loss = 3.33616 avg_loss = 3.56910\n",
      "epoch no.2 train no.179700  loss = 2.40662 avg_loss = 3.52643\n",
      "epoch no.2 train no.179710  loss = 2.42525 avg_loss = 3.51430\n",
      "epoch no.2 train no.179720  loss = 3.49590 avg_loss = 3.50133\n",
      "epoch no.2 train no.179730  loss = 4.19781 avg_loss = 3.51857\n",
      "epoch no.2 train no.179740  loss = 3.16491 avg_loss = 3.47292\n",
      "epoch no.2 train no.179750  loss = 4.01688 avg_loss = 3.52382\n",
      "epoch no.2 train no.179760  loss = 3.23551 avg_loss = 3.47405\n",
      "epoch no.2 train no.179770  loss = 3.90604 avg_loss = 3.47583\n",
      "epoch no.2 train no.179780  loss = 2.62336 avg_loss = 3.53208\n",
      "epoch no.2 train no.179790  loss = 3.22552 avg_loss = 3.48781\n",
      "epoch no.2 train no.179800  loss = 2.45261 avg_loss = 3.44950\n",
      "epoch no.2 train no.179810  loss = 3.56191 avg_loss = 3.47792\n",
      "epoch no.2 train no.179820  loss = 4.45026 avg_loss = 3.40197\n",
      "epoch no.2 train no.179830  loss = 7.30208 avg_loss = 3.48327\n",
      "epoch no.2 train no.179840  loss = 3.96373 avg_loss = 3.47953\n",
      "epoch no.2 train no.179850  loss = 4.13923 avg_loss = 3.48021\n",
      "epoch no.2 train no.179860  loss = 2.51129 avg_loss = 3.49991\n",
      "epoch no.2 train no.179870  loss = 3.84832 avg_loss = 3.51749\n",
      "epoch no.2 train no.179880  loss = 2.91054 avg_loss = 3.51052\n",
      "epoch no.2 train no.179890  loss = 4.07565 avg_loss = 3.51592\n",
      "epoch no.2 train no.179900  loss = 3.86614 avg_loss = 3.57940\n",
      "epoch no.2 train no.179910  loss = 5.36945 avg_loss = 3.53139\n",
      "epoch no.2 train no.179920  loss = 4.19962 avg_loss = 3.51271\n",
      "epoch no.2 train no.179930  loss = 3.91267 avg_loss = 3.53871\n",
      "epoch no.2 train no.179940  loss = 2.39443 avg_loss = 3.50474\n",
      "epoch no.2 train no.179950  loss = 3.10016 avg_loss = 3.52284\n",
      "epoch no.2 train no.179960  loss = 3.18890 avg_loss = 3.50152\n",
      "epoch no.2 train no.179970  loss = 1.51902 avg_loss = 3.49400\n",
      "epoch no.2 train no.179980  loss = 4.44895 avg_loss = 3.46535\n",
      "epoch no.2 train no.179990  loss = 4.90127 avg_loss = 3.48690\n",
      "epoch no.2 train no.180000  loss = 3.04701 avg_loss = 3.51760\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.2 train no.180010  loss = 2.13073 avg_loss = 3.47967\n",
      "epoch no.2 train no.180020  loss = 3.39844 avg_loss = 3.44622\n",
      "epoch no.2 train no.180030  loss = 3.09288 avg_loss = 3.40675\n",
      "epoch no.2 train no.180040  loss = 2.80831 avg_loss = 3.46131\n",
      "epoch no.2 train no.180050  loss = 3.63412 avg_loss = 3.49437\n",
      "epoch no.2 train no.180060  loss = 2.58474 avg_loss = 3.46617\n",
      "epoch no.2 train no.180070  loss = 4.24290 avg_loss = 3.46466\n",
      "epoch no.2 train no.180080  loss = 5.15663 avg_loss = 3.51576\n",
      "epoch no.2 train no.180090  loss = 3.19040 avg_loss = 3.53438\n",
      "epoch no.2 train no.180100  loss = 3.44515 avg_loss = 3.51487\n",
      "epoch no.2 train no.180110  loss = 3.78196 avg_loss = 3.52343\n",
      "epoch no.2 train no.180120  loss = 1.51245 avg_loss = 3.48160\n",
      "epoch no.2 train no.180130  loss = 3.36245 avg_loss = 3.48646\n",
      "epoch no.2 train no.180140  loss = 2.04531 avg_loss = 3.43318\n",
      "epoch no.2 train no.180150  loss = 2.03123 avg_loss = 3.43692\n",
      "epoch no.2 train no.180160  loss = 5.28681 avg_loss = 3.47702\n",
      "epoch no.2 train no.180170  loss = 6.96029 avg_loss = 3.50135\n",
      "epoch no.2 train no.180180  loss = 2.77813 avg_loss = 3.51041\n",
      "epoch no.2 train no.180190  loss = 3.61052 avg_loss = 3.48455\n",
      "epoch no.2 train no.180200  loss = 3.34950 avg_loss = 3.48530\n",
      "epoch no.2 train no.180210  loss = 5.25415 avg_loss = 3.54277\n",
      "epoch no.2 train no.180220  loss = 3.72787 avg_loss = 3.58675\n",
      "epoch no.2 train no.180230  loss = 2.88909 avg_loss = 3.59726\n",
      "epoch no.2 train no.180240  loss = 2.76848 avg_loss = 3.55140\n",
      "epoch no.2 train no.180250  loss = 4.40117 avg_loss = 3.52494\n",
      "epoch no.2 train no.180260  loss = 5.22141 avg_loss = 3.58169\n",
      "epoch no.2 train no.180270  loss = 3.39787 avg_loss = 3.55398\n",
      "epoch no.2 train no.180280  loss = 2.72675 avg_loss = 3.56099\n",
      "epoch no.2 train no.180290  loss = 2.74028 avg_loss = 3.54972\n",
      "epoch no.2 train no.180300  loss = 2.13818 avg_loss = 3.51035\n",
      "epoch no.2 train no.180310  loss = 4.43478 avg_loss = 3.58943\n",
      "epoch no.2 train no.180320  loss = 5.20354 avg_loss = 3.56441\n",
      "epoch no.2 train no.180330  loss = 2.37122 avg_loss = 3.54883\n",
      "epoch no.2 train no.180340  loss = 4.47571 avg_loss = 3.50326\n",
      "epoch no.2 train no.180350  loss = 2.92148 avg_loss = 3.49088\n",
      "epoch no.2 train no.180360  loss = 4.66950 avg_loss = 3.49956\n",
      "epoch no.2 train no.180370  loss = 1.68720 avg_loss = 3.48879\n",
      "epoch no.2 train no.180380  loss = 4.10273 avg_loss = 3.49265\n",
      "epoch no.2 train no.180390  loss = 3.36478 avg_loss = 3.51085\n",
      "epoch no.2 train no.180400  loss = 4.99219 avg_loss = 3.48941\n",
      "epoch no.2 train no.180410  loss = 3.56677 avg_loss = 3.48817\n",
      "epoch no.2 train no.180420  loss = 3.83299 avg_loss = 3.50299\n",
      "epoch no.2 train no.180430  loss = 2.43927 avg_loss = 3.49263\n",
      "epoch no.2 train no.180440  loss = 1.96130 avg_loss = 3.50659\n",
      "epoch no.2 train no.180450  loss = 5.33814 avg_loss = 3.49006\n",
      "epoch no.2 train no.180460  loss = 2.73373 avg_loss = 3.50291\n",
      "epoch no.2 train no.180470  loss = 5.15022 avg_loss = 3.53969\n",
      "epoch no.2 train no.180480  loss = 2.13992 avg_loss = 3.49454\n",
      "epoch no.2 train no.180490  loss = 4.21370 avg_loss = 3.49856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.180500  loss = 2.73711 avg_loss = 3.49038\n",
      "epoch no.2 train no.180510  loss = 3.19467 avg_loss = 3.48459\n",
      "epoch no.2 train no.180520  loss = 5.14750 avg_loss = 3.51077\n",
      "epoch no.2 train no.180530  loss = 3.84970 avg_loss = 3.48717\n",
      "epoch no.2 train no.180540  loss = 3.49853 avg_loss = 3.47786\n",
      "epoch no.2 train no.180550  loss = 5.21703 avg_loss = 3.54558\n",
      "epoch no.2 train no.180560  loss = 4.29705 avg_loss = 3.54412\n",
      "epoch no.2 train no.180570  loss = 5.91976 avg_loss = 3.56885\n",
      "epoch no.2 train no.180580  loss = 1.80198 avg_loss = 3.56713\n",
      "epoch no.2 train no.180590  loss = 2.20764 avg_loss = 3.51478\n",
      "epoch no.2 train no.180600  loss = 2.01125 avg_loss = 3.53186\n",
      "epoch no.2 train no.180610  loss = 4.53661 avg_loss = 3.58478\n",
      "epoch no.2 train no.180620  loss = 1.99874 avg_loss = 3.50921\n",
      "epoch no.2 train no.180630  loss = 3.63025 avg_loss = 3.48620\n",
      "epoch no.2 train no.180640  loss = 2.58896 avg_loss = 3.53220\n",
      "epoch no.2 train no.180650  loss = 3.22889 avg_loss = 3.50009\n",
      "epoch no.2 train no.180660  loss = 3.00013 avg_loss = 3.49197\n",
      "epoch no.2 train no.180670  loss = 2.74176 avg_loss = 3.47881\n",
      "epoch no.2 train no.180680  loss = 2.10714 avg_loss = 3.41947\n",
      "epoch no.2 train no.180690  loss = 2.33226 avg_loss = 3.41986\n",
      "epoch no.2 train no.180700  loss = 2.47833 avg_loss = 3.43218\n",
      "epoch no.2 train no.180710  loss = 2.36767 avg_loss = 3.41865\n",
      "epoch no.2 train no.180720  loss = 3.55592 avg_loss = 3.43194\n",
      "epoch no.2 train no.180730  loss = 6.12425 avg_loss = 3.41015\n",
      "epoch no.2 train no.180740  loss = 3.32036 avg_loss = 3.42415\n",
      "epoch no.2 train no.180750  loss = 4.31515 avg_loss = 3.48758\n",
      "epoch no.2 train no.180760  loss = 2.56067 avg_loss = 3.48700\n",
      "epoch no.2 train no.180770  loss = 4.79407 avg_loss = 3.46436\n",
      "epoch no.2 train no.180780  loss = 4.99352 avg_loss = 3.51372\n",
      "epoch no.2 train no.180790  loss = 2.86535 avg_loss = 3.51233\n",
      "epoch no.2 train no.180800  loss = 3.11289 avg_loss = 3.48897\n",
      "epoch no.2 train no.180810  loss = 3.62534 avg_loss = 3.52709\n",
      "epoch no.2 train no.180820  loss = 3.68959 avg_loss = 3.52477\n",
      "epoch no.2 train no.180830  loss = 3.63838 avg_loss = 3.51323\n",
      "epoch no.2 train no.180840  loss = 3.33742 avg_loss = 3.47998\n",
      "epoch no.2 train no.180850  loss = 2.72032 avg_loss = 3.49186\n",
      "epoch no.2 train no.180860  loss = 3.02820 avg_loss = 3.51018\n",
      "epoch no.2 train no.180870  loss = 3.42632 avg_loss = 3.50079\n",
      "epoch no.2 train no.180880  loss = 1.77721 avg_loss = 3.48507\n",
      "epoch no.2 train no.180890  loss = 2.77691 avg_loss = 3.42461\n",
      "epoch no.2 train no.180900  loss = 4.10214 avg_loss = 3.41311\n",
      "epoch no.2 train no.180910  loss = 3.00967 avg_loss = 3.42238\n",
      "epoch no.2 train no.180920  loss = 2.43407 avg_loss = 3.39167\n",
      "epoch no.2 train no.180930  loss = 2.14592 avg_loss = 3.38701\n",
      "epoch no.2 train no.180940  loss = 4.16332 avg_loss = 3.37077\n",
      "epoch no.2 train no.180950  loss = 3.47724 avg_loss = 3.43869\n",
      "epoch no.2 train no.180960  loss = 2.82094 avg_loss = 3.46212\n",
      "epoch no.2 train no.180970  loss = 3.04651 avg_loss = 3.44021\n",
      "epoch no.2 train no.180980  loss = 5.60230 avg_loss = 3.49652\n",
      "epoch no.2 train no.180990  loss = 2.82557 avg_loss = 3.48650\n",
      "epoch no.2 train no.181000  loss = 2.64753 avg_loss = 3.48220\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '송', '▁모음', '집', '</s>']\n",
      "추억의 팝송 모음집</s>\n",
      "epoch no.2 train no.181010  loss = 2.74780 avg_loss = 3.49734\n",
      "epoch no.2 train no.181020  loss = 4.92465 avg_loss = 3.49626\n",
      "epoch no.2 train no.181030  loss = 3.52628 avg_loss = 3.49897\n",
      "epoch no.2 train no.181040  loss = 2.84362 avg_loss = 3.54943\n",
      "epoch no.2 train no.181050  loss = 3.45113 avg_loss = 3.53063\n",
      "epoch no.2 train no.181060  loss = 3.52953 avg_loss = 3.57491\n",
      "epoch no.2 train no.181070  loss = 2.91106 avg_loss = 3.50662\n",
      "epoch no.2 train no.181080  loss = 2.90262 avg_loss = 3.50308\n",
      "epoch no.2 train no.181090  loss = 3.11510 avg_loss = 3.46847\n",
      "epoch no.2 train no.181100  loss = 5.12865 avg_loss = 3.53447\n",
      "epoch no.2 train no.181110  loss = 4.84196 avg_loss = 3.58162\n",
      "epoch no.2 train no.181120  loss = 3.45702 avg_loss = 3.53330\n",
      "epoch no.2 train no.181130  loss = 3.45268 avg_loss = 3.50458\n",
      "epoch no.2 train no.181140  loss = 2.61358 avg_loss = 3.46260\n",
      "epoch no.2 train no.181150  loss = 3.00266 avg_loss = 3.42332\n",
      "epoch no.2 train no.181160  loss = 3.05624 avg_loss = 3.47036\n",
      "epoch no.2 train no.181170  loss = 3.70420 avg_loss = 3.47030\n",
      "epoch no.2 train no.181180  loss = 4.56086 avg_loss = 3.50727\n",
      "epoch no.2 train no.181190  loss = 4.07150 avg_loss = 3.47120\n",
      "epoch no.2 train no.181200  loss = 4.50625 avg_loss = 3.42964\n",
      "epoch no.2 train no.181210  loss = 4.33661 avg_loss = 3.45615\n",
      "epoch no.2 train no.181220  loss = 4.03310 avg_loss = 3.41484\n",
      "epoch no.2 train no.181230  loss = 4.50508 avg_loss = 3.36699\n",
      "epoch no.2 train no.181240  loss = 4.38313 avg_loss = 3.43374\n",
      "epoch no.2 train no.181250  loss = 5.06101 avg_loss = 3.39748\n",
      "epoch no.2 train no.181260  loss = 3.90820 avg_loss = 3.36363\n",
      "epoch no.2 train no.181270  loss = 2.74095 avg_loss = 3.37361\n",
      "epoch no.2 train no.181280  loss = 3.20252 avg_loss = 3.35778\n",
      "epoch no.2 train no.181290  loss = 2.92378 avg_loss = 3.40303\n",
      "epoch no.2 train no.181300  loss = 1.97605 avg_loss = 3.46133\n",
      "epoch no.2 train no.181310  loss = 4.13862 avg_loss = 3.51634\n",
      "epoch no.2 train no.181320  loss = 3.45495 avg_loss = 3.49714\n",
      "epoch no.2 train no.181330  loss = 2.45401 avg_loss = 3.41325\n",
      "epoch no.2 train no.181340  loss = 4.79400 avg_loss = 3.46953\n",
      "epoch no.2 train no.181350  loss = 2.83344 avg_loss = 3.52642\n",
      "epoch no.2 train no.181360  loss = 3.12264 avg_loss = 3.55499\n",
      "epoch no.2 train no.181370  loss = 3.16004 avg_loss = 3.51283\n",
      "epoch no.2 train no.181380  loss = 2.94476 avg_loss = 3.49870\n",
      "epoch no.2 train no.181390  loss = 3.07017 avg_loss = 3.52438\n",
      "epoch no.2 train no.181400  loss = 1.96812 avg_loss = 3.46887\n",
      "epoch no.2 train no.181410  loss = 3.14785 avg_loss = 3.52136\n",
      "epoch no.2 train no.181420  loss = 3.07574 avg_loss = 3.53928\n",
      "epoch no.2 train no.181430  loss = 2.41418 avg_loss = 3.53244\n",
      "epoch no.2 train no.181440  loss = 4.09558 avg_loss = 3.55111\n",
      "epoch no.2 train no.181450  loss = 3.49486 avg_loss = 3.56148\n",
      "epoch no.2 train no.181460  loss = 2.36971 avg_loss = 3.52625\n",
      "epoch no.2 train no.181470  loss = 3.78794 avg_loss = 3.51535\n",
      "epoch no.2 train no.181480  loss = 2.08631 avg_loss = 3.52971\n",
      "epoch no.2 train no.181490  loss = 5.33047 avg_loss = 3.53596\n",
      "epoch no.2 train no.181500  loss = 2.34789 avg_loss = 3.51143\n",
      "epoch no.2 train no.181510  loss = 4.46713 avg_loss = 3.49902\n",
      "epoch no.2 train no.181520  loss = 5.78071 avg_loss = 3.54575\n",
      "epoch no.2 train no.181530  loss = 3.62704 avg_loss = 3.51596\n",
      "epoch no.2 train no.181540  loss = 5.74233 avg_loss = 3.53459\n",
      "epoch no.2 train no.181550  loss = 3.31106 avg_loss = 3.50424\n",
      "epoch no.2 train no.181560  loss = 3.62662 avg_loss = 3.48194\n",
      "epoch no.2 train no.181570  loss = 2.88030 avg_loss = 3.48717\n",
      "epoch no.2 train no.181580  loss = 2.36045 avg_loss = 3.43450\n",
      "epoch no.2 train no.181590  loss = 2.73612 avg_loss = 3.45738\n",
      "epoch no.2 train no.181600  loss = 5.30219 avg_loss = 3.53102\n",
      "epoch no.2 train no.181610  loss = 2.98509 avg_loss = 3.51931\n",
      "epoch no.2 train no.181620  loss = 3.21348 avg_loss = 3.51432\n",
      "epoch no.2 train no.181630  loss = 4.44593 avg_loss = 3.47378\n",
      "epoch no.2 train no.181640  loss = 3.38070 avg_loss = 3.46853\n",
      "epoch no.2 train no.181650  loss = 5.04516 avg_loss = 3.45994\n",
      "epoch no.2 train no.181660  loss = 2.82332 avg_loss = 3.44444\n",
      "epoch no.2 train no.181670  loss = 3.78326 avg_loss = 3.43427\n",
      "epoch no.2 train no.181680  loss = 2.66841 avg_loss = 3.40916\n",
      "epoch no.2 train no.181690  loss = 3.73176 avg_loss = 3.40155\n",
      "epoch no.2 train no.181700  loss = 4.22373 avg_loss = 3.40175\n",
      "epoch no.2 train no.181710  loss = 3.40605 avg_loss = 3.42562\n",
      "epoch no.2 train no.181720  loss = 2.72539 avg_loss = 3.40461\n",
      "epoch no.2 train no.181730  loss = 2.56193 avg_loss = 3.36681\n",
      "epoch no.2 train no.181740  loss = 4.17937 avg_loss = 3.41953\n",
      "epoch no.2 train no.181750  loss = 2.09481 avg_loss = 3.45738\n",
      "epoch no.2 train no.181760  loss = 3.27533 avg_loss = 3.48372\n",
      "epoch no.2 train no.181770  loss = 3.79545 avg_loss = 3.47241\n",
      "epoch no.2 train no.181780  loss = 2.53186 avg_loss = 3.47143\n",
      "epoch no.2 train no.181790  loss = 3.00813 avg_loss = 3.46286\n",
      "epoch no.2 train no.181800  loss = 3.05434 avg_loss = 3.47621\n",
      "epoch no.2 train no.181810  loss = 3.67999 avg_loss = 3.48923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.181820  loss = 5.31369 avg_loss = 3.47735\n",
      "epoch no.2 train no.181830  loss = 4.22563 avg_loss = 3.46542\n",
      "epoch no.2 train no.181840  loss = 2.07177 avg_loss = 3.46138\n",
      "epoch no.2 train no.181850  loss = 2.47383 avg_loss = 3.41840\n",
      "epoch no.2 train no.181860  loss = 1.80979 avg_loss = 3.40575\n",
      "epoch no.2 train no.181870  loss = 4.64318 avg_loss = 3.38156\n",
      "epoch no.2 train no.181880  loss = 4.72392 avg_loss = 3.39356\n",
      "epoch no.2 train no.181890  loss = 4.98843 avg_loss = 3.37343\n",
      "epoch no.2 train no.181900  loss = 3.20275 avg_loss = 3.32705\n",
      "epoch no.2 train no.181910  loss = 4.77625 avg_loss = 3.31843\n",
      "epoch no.2 train no.181920  loss = 4.94655 avg_loss = 3.36225\n",
      "epoch no.2 train no.181930  loss = 3.27325 avg_loss = 3.43916\n",
      "epoch no.2 train no.181940  loss = 3.04135 avg_loss = 3.40494\n",
      "epoch no.2 train no.181950  loss = 2.75245 avg_loss = 3.43500\n",
      "epoch no.2 train no.181960  loss = 2.37034 avg_loss = 3.45947\n",
      "epoch no.2 train no.181970  loss = 4.69623 avg_loss = 3.44427\n",
      "epoch no.2 train no.181980  loss = 3.18550 avg_loss = 3.47354\n",
      "epoch no.2 train no.181990  loss = 2.90754 avg_loss = 3.49101\n",
      "epoch no.2 train no.182000  loss = 4.22718 avg_loss = 3.50620\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.182010  loss = 4.21149 avg_loss = 3.50253\n",
      "epoch no.2 train no.182020  loss = 4.57554 avg_loss = 3.50511\n",
      "epoch no.2 train no.182030  loss = 6.10286 avg_loss = 3.56229\n",
      "epoch no.2 train no.182040  loss = 4.15579 avg_loss = 3.55627\n",
      "epoch no.2 train no.182050  loss = 2.92186 avg_loss = 3.53533\n",
      "epoch no.2 train no.182060  loss = 3.96805 avg_loss = 3.52157\n",
      "epoch no.2 train no.182070  loss = 4.25930 avg_loss = 3.52226\n",
      "epoch no.2 train no.182080  loss = 2.75426 avg_loss = 3.51169\n",
      "epoch no.2 train no.182090  loss = 4.85559 avg_loss = 3.55806\n",
      "epoch no.2 train no.182100  loss = 2.96649 avg_loss = 3.50734\n",
      "epoch no.2 train no.182110  loss = 3.83771 avg_loss = 3.50687\n",
      "epoch no.2 train no.182120  loss = 3.66631 avg_loss = 3.54658\n",
      "epoch no.2 train no.182130  loss = 1.90803 avg_loss = 3.55737\n",
      "epoch no.2 train no.182140  loss = 5.93288 avg_loss = 3.59903\n",
      "epoch no.2 train no.182150  loss = 4.17907 avg_loss = 3.62196\n",
      "epoch no.2 train no.182160  loss = 2.51709 avg_loss = 3.58229\n",
      "epoch no.2 train no.182170  loss = 3.04263 avg_loss = 3.55989\n",
      "epoch no.2 train no.182180  loss = 4.70063 avg_loss = 3.51710\n",
      "epoch no.2 train no.182190  loss = 2.73835 avg_loss = 3.48091\n",
      "epoch no.2 train no.182200  loss = 3.61279 avg_loss = 3.47598\n",
      "epoch no.2 train no.182210  loss = 3.34230 avg_loss = 3.50920\n",
      "epoch no.2 train no.182220  loss = 4.32453 avg_loss = 3.47111\n",
      "epoch no.2 train no.182230  loss = 3.06599 avg_loss = 3.43712\n",
      "epoch no.2 train no.182240  loss = 1.94893 avg_loss = 3.44934\n",
      "epoch no.2 train no.182250  loss = 5.71423 avg_loss = 3.44581\n",
      "epoch no.2 train no.182260  loss = 4.47703 avg_loss = 3.45237\n",
      "epoch no.2 train no.182270  loss = 3.27459 avg_loss = 3.54481\n",
      "epoch no.2 train no.182280  loss = 2.83418 avg_loss = 3.52721\n",
      "epoch no.2 train no.182290  loss = 3.24592 avg_loss = 3.55524\n",
      "epoch no.2 train no.182300  loss = 6.04685 avg_loss = 3.56774\n",
      "epoch no.2 train no.182310  loss = 3.31152 avg_loss = 3.51942\n",
      "epoch no.2 train no.182320  loss = 5.37469 avg_loss = 3.56705\n",
      "epoch no.2 train no.182330  loss = 4.02353 avg_loss = 3.60181\n",
      "epoch no.2 train no.182340  loss = 4.91158 avg_loss = 3.60487\n",
      "epoch no.2 train no.182350  loss = 4.84603 avg_loss = 3.63148\n",
      "epoch no.2 train no.182360  loss = 4.23855 avg_loss = 3.60594\n",
      "epoch no.2 train no.182370  loss = 3.05528 avg_loss = 3.56008\n",
      "epoch no.2 train no.182380  loss = 2.44776 avg_loss = 3.50368\n",
      "epoch no.2 train no.182390  loss = 5.14513 avg_loss = 3.55322\n",
      "epoch no.2 train no.182400  loss = 7.18152 avg_loss = 3.55057\n",
      "epoch no.2 train no.182410  loss = 4.91893 avg_loss = 3.53436\n",
      "epoch no.2 train no.182420  loss = 5.08343 avg_loss = 3.55951\n",
      "epoch no.2 train no.182430  loss = 2.86671 avg_loss = 3.52610\n",
      "epoch no.2 train no.182440  loss = 2.17749 avg_loss = 3.49580\n",
      "epoch no.2 train no.182450  loss = 4.43235 avg_loss = 3.49778\n",
      "epoch no.2 train no.182460  loss = 2.55435 avg_loss = 3.44455\n",
      "epoch no.2 train no.182470  loss = 2.35126 avg_loss = 3.40348\n",
      "epoch no.2 train no.182480  loss = 4.82295 avg_loss = 3.38879\n",
      "epoch no.2 train no.182490  loss = 2.89128 avg_loss = 3.38811\n",
      "epoch no.2 train no.182500  loss = 2.50308 avg_loss = 3.42257\n",
      "epoch no.2 train no.182510  loss = 4.61415 avg_loss = 3.42810\n",
      "epoch no.2 train no.182520  loss = 2.05795 avg_loss = 3.40942\n",
      "epoch no.2 train no.182530  loss = 3.57972 avg_loss = 3.45613\n",
      "epoch no.2 train no.182540  loss = 2.46583 avg_loss = 3.39438\n",
      "epoch no.2 train no.182550  loss = 3.14351 avg_loss = 3.36657\n",
      "epoch no.2 train no.182560  loss = 1.79801 avg_loss = 3.32699\n",
      "epoch no.2 train no.182570  loss = 2.49758 avg_loss = 3.32279\n",
      "epoch no.2 train no.182580  loss = 3.42163 avg_loss = 3.31322\n",
      "epoch no.2 train no.182590  loss = 3.21247 avg_loss = 3.39088\n",
      "epoch no.2 train no.182600  loss = 2.58577 avg_loss = 3.33405\n",
      "epoch no.2 train no.182610  loss = 3.75086 avg_loss = 3.37770\n",
      "epoch no.2 train no.182620  loss = 2.43546 avg_loss = 3.35581\n",
      "epoch no.2 train no.182630  loss = 2.64571 avg_loss = 3.45981\n",
      "epoch no.2 train no.182640  loss = 6.97316 avg_loss = 3.52119\n",
      "epoch no.2 train no.182650  loss = 3.22010 avg_loss = 3.55847\n",
      "epoch no.2 train no.182660  loss = 2.92086 avg_loss = 3.49064\n",
      "epoch no.2 train no.182670  loss = 3.13248 avg_loss = 3.54395\n",
      "epoch no.2 train no.182680  loss = 3.44538 avg_loss = 3.53473\n",
      "epoch no.2 train no.182690  loss = 4.46707 avg_loss = 3.55704\n",
      "epoch no.2 train no.182700  loss = 4.41409 avg_loss = 3.58912\n",
      "epoch no.2 train no.182710  loss = 4.13694 avg_loss = 3.58790\n",
      "epoch no.2 train no.182720  loss = 4.77101 avg_loss = 3.60635\n",
      "epoch no.2 train no.182730  loss = 4.23691 avg_loss = 3.60616\n",
      "epoch no.2 train no.182740  loss = 2.98117 avg_loss = 3.55489\n",
      "epoch no.2 train no.182750  loss = 5.66791 avg_loss = 3.56465\n",
      "epoch no.2 train no.182760  loss = 4.77585 avg_loss = 3.58902\n",
      "epoch no.2 train no.182770  loss = 2.51087 avg_loss = 3.54095\n",
      "epoch no.2 train no.182780  loss = 3.44963 avg_loss = 3.52570\n",
      "epoch no.2 train no.182790  loss = 6.59309 avg_loss = 3.53844\n",
      "epoch no.2 train no.182800  loss = 3.26352 avg_loss = 3.55563\n",
      "epoch no.2 train no.182810  loss = 3.02589 avg_loss = 3.55860\n",
      "epoch no.2 train no.182820  loss = 3.94884 avg_loss = 3.58450\n",
      "epoch no.2 train no.182830  loss = 5.02470 avg_loss = 3.60954\n",
      "epoch no.2 train no.182840  loss = 2.99472 avg_loss = 3.55081\n",
      "epoch no.2 train no.182850  loss = 4.27049 avg_loss = 3.55235\n",
      "epoch no.2 train no.182860  loss = 2.26612 avg_loss = 3.51771\n",
      "epoch no.2 train no.182870  loss = 4.34695 avg_loss = 3.58319\n",
      "epoch no.2 train no.182880  loss = 2.54215 avg_loss = 3.60822\n",
      "epoch no.2 train no.182890  loss = 2.46888 avg_loss = 3.63790\n",
      "epoch no.2 train no.182900  loss = 4.72734 avg_loss = 3.59769\n",
      "epoch no.2 train no.182910  loss = 1.44002 avg_loss = 3.52198\n",
      "epoch no.2 train no.182920  loss = 3.89550 avg_loss = 3.49029\n",
      "epoch no.2 train no.182930  loss = 1.92591 avg_loss = 3.49061\n",
      "epoch no.2 train no.182940  loss = 3.40613 avg_loss = 3.46987\n",
      "epoch no.2 train no.182950  loss = 5.12442 avg_loss = 3.54798\n",
      "epoch no.2 train no.182960  loss = 3.51466 avg_loss = 3.56831\n",
      "epoch no.2 train no.182970  loss = 2.74783 avg_loss = 3.54623\n",
      "epoch no.2 train no.182980  loss = 5.52002 avg_loss = 3.53714\n",
      "epoch no.2 train no.182990  loss = 4.94208 avg_loss = 3.56613\n",
      "epoch no.2 train no.183000  loss = 4.33143 avg_loss = 3.57487\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '송', '모', '음', '</s>']\n",
      "추억의 팝송모음</s>\n",
      "epoch no.2 train no.183010  loss = 3.34156 avg_loss = 3.55421\n",
      "epoch no.2 train no.183020  loss = 5.58283 avg_loss = 3.56365\n",
      "epoch no.2 train no.183030  loss = 2.66810 avg_loss = 3.62204\n",
      "epoch no.2 train no.183040  loss = 3.00747 avg_loss = 3.56857\n",
      "epoch no.2 train no.183050  loss = 4.29999 avg_loss = 3.55429\n",
      "epoch no.2 train no.183060  loss = 3.33730 avg_loss = 3.55427\n",
      "epoch no.2 train no.183070  loss = 3.81380 avg_loss = 3.53558\n",
      "epoch no.2 train no.183080  loss = 3.72259 avg_loss = 3.55118\n",
      "epoch no.2 train no.183090  loss = 4.91501 avg_loss = 3.54906\n",
      "epoch no.2 train no.183100  loss = 3.74371 avg_loss = 3.56482\n",
      "epoch no.2 train no.183110  loss = 2.27564 avg_loss = 3.58229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.183120  loss = 4.15117 avg_loss = 3.56622\n",
      "epoch no.2 train no.183130  loss = 3.10756 avg_loss = 3.56336\n",
      "epoch no.2 train no.183140  loss = 4.66536 avg_loss = 3.55233\n",
      "epoch no.2 train no.183150  loss = 3.57003 avg_loss = 3.58049\n",
      "epoch no.2 train no.183160  loss = 3.00064 avg_loss = 3.54392\n",
      "epoch no.2 train no.183170  loss = 3.29304 avg_loss = 3.58099\n",
      "epoch no.2 train no.183180  loss = 2.78531 avg_loss = 3.50942\n",
      "epoch no.2 train no.183190  loss = 2.16382 avg_loss = 3.45553\n",
      "epoch no.2 train no.183200  loss = 1.38656 avg_loss = 3.38252\n",
      "epoch no.2 train no.183210  loss = 2.44364 avg_loss = 3.39959\n",
      "epoch no.2 train no.183220  loss = 4.19947 avg_loss = 3.40442\n",
      "epoch no.2 train no.183230  loss = 3.45182 avg_loss = 3.46487\n",
      "epoch no.2 train no.183240  loss = 3.58937 avg_loss = 3.41424\n",
      "epoch no.2 train no.183250  loss = 3.64793 avg_loss = 3.37161\n",
      "epoch no.2 train no.183260  loss = 3.58674 avg_loss = 3.45369\n",
      "epoch no.2 train no.183270  loss = 3.70451 avg_loss = 3.40535\n",
      "epoch no.2 train no.183280  loss = 3.58160 avg_loss = 3.36623\n",
      "epoch no.2 train no.183290  loss = 2.38336 avg_loss = 3.34202\n",
      "epoch no.2 train no.183300  loss = 3.11086 avg_loss = 3.35310\n",
      "epoch no.2 train no.183310  loss = 2.11490 avg_loss = 3.34341\n",
      "epoch no.2 train no.183320  loss = 3.21668 avg_loss = 3.31828\n",
      "epoch no.2 train no.183330  loss = 3.97503 avg_loss = 3.36232\n",
      "epoch no.2 train no.183340  loss = 4.02731 avg_loss = 3.38108\n",
      "epoch no.2 train no.183350  loss = 4.30288 avg_loss = 3.39204\n",
      "epoch no.2 train no.183360  loss = 3.33327 avg_loss = 3.42902\n",
      "epoch no.2 train no.183370  loss = 2.38324 avg_loss = 3.43871\n",
      "epoch no.2 train no.183380  loss = 2.82295 avg_loss = 3.41768\n",
      "epoch no.2 train no.183390  loss = 2.48588 avg_loss = 3.46111\n",
      "epoch no.2 train no.183400  loss = 3.28993 avg_loss = 3.42154\n",
      "epoch no.2 train no.183410  loss = 5.69763 avg_loss = 3.45301\n",
      "epoch no.2 train no.183420  loss = 2.53827 avg_loss = 3.47486\n",
      "epoch no.2 train no.183430  loss = 3.54152 avg_loss = 3.47788\n",
      "epoch no.2 train no.183440  loss = 1.72822 avg_loss = 3.46185\n",
      "epoch no.2 train no.183450  loss = 4.22212 avg_loss = 3.47080\n",
      "epoch no.2 train no.183460  loss = 4.14414 avg_loss = 3.49204\n",
      "epoch no.2 train no.183470  loss = 3.73750 avg_loss = 3.48511\n",
      "epoch no.2 train no.183480  loss = 3.01637 avg_loss = 3.46279\n",
      "epoch no.2 train no.183490  loss = 4.35175 avg_loss = 3.50461\n",
      "epoch no.2 train no.183500  loss = 3.50650 avg_loss = 3.49622\n",
      "epoch no.2 train no.183510  loss = 4.06888 avg_loss = 3.52666\n",
      "epoch no.2 train no.183520  loss = 3.76800 avg_loss = 3.53994\n",
      "epoch no.2 train no.183530  loss = 5.50404 avg_loss = 3.55303\n",
      "epoch no.2 train no.183540  loss = 2.30204 avg_loss = 3.53236\n",
      "epoch no.2 train no.183550  loss = 2.75053 avg_loss = 3.50321\n",
      "epoch no.2 train no.183560  loss = 3.60838 avg_loss = 3.47886\n",
      "epoch no.2 train no.183570  loss = 3.16311 avg_loss = 3.46851\n",
      "epoch no.2 train no.183580  loss = 3.89476 avg_loss = 3.49623\n",
      "epoch no.2 train no.183590  loss = 2.60208 avg_loss = 3.50119\n",
      "epoch no.2 train no.183600  loss = 5.05285 avg_loss = 3.49444\n",
      "epoch no.2 train no.183610  loss = 4.59935 avg_loss = 3.46374\n",
      "epoch no.2 train no.183620  loss = 2.55807 avg_loss = 3.48195\n",
      "epoch no.2 train no.183630  loss = 3.42592 avg_loss = 3.51061\n",
      "epoch no.2 train no.183640  loss = 2.70057 avg_loss = 3.52673\n",
      "epoch no.2 train no.183650  loss = 3.71372 avg_loss = 3.53417\n",
      "epoch no.2 train no.183660  loss = 2.18847 avg_loss = 3.50249\n",
      "epoch no.2 train no.183670  loss = 3.11901 avg_loss = 3.48532\n",
      "epoch no.2 train no.183680  loss = 5.17884 avg_loss = 3.56922\n",
      "epoch no.2 train no.183690  loss = 4.35508 avg_loss = 3.60714\n",
      "epoch no.2 train no.183700  loss = 2.82973 avg_loss = 3.57610\n",
      "epoch no.2 train no.183710  loss = 3.85793 avg_loss = 3.61322\n",
      "epoch no.2 train no.183720  loss = 3.29954 avg_loss = 3.57310\n",
      "epoch no.2 train no.183730  loss = 3.23601 avg_loss = 3.51688\n",
      "epoch no.2 train no.183740  loss = 2.68631 avg_loss = 3.47529\n",
      "epoch no.2 train no.183750  loss = 4.66549 avg_loss = 3.48942\n",
      "epoch no.2 train no.183760  loss = 3.67373 avg_loss = 3.50490\n",
      "epoch no.2 train no.183770  loss = 4.67344 avg_loss = 3.50365\n",
      "epoch no.2 train no.183780  loss = 2.10883 avg_loss = 3.52420\n",
      "epoch no.2 train no.183790  loss = 5.68235 avg_loss = 3.50939\n",
      "epoch no.2 train no.183800  loss = 4.28743 avg_loss = 3.51473\n",
      "epoch no.2 train no.183810  loss = 2.19059 avg_loss = 3.47234\n",
      "epoch no.2 train no.183820  loss = 2.87004 avg_loss = 3.48287\n",
      "epoch no.2 train no.183830  loss = 3.82046 avg_loss = 3.50281\n",
      "epoch no.2 train no.183840  loss = 3.39431 avg_loss = 3.45851\n",
      "epoch no.2 train no.183850  loss = 3.43702 avg_loss = 3.43556\n",
      "epoch no.2 train no.183860  loss = 3.87756 avg_loss = 3.44554\n",
      "epoch no.2 train no.183870  loss = 4.94313 avg_loss = 3.49519\n",
      "epoch no.2 train no.183880  loss = 4.90102 avg_loss = 3.51681\n",
      "epoch no.2 train no.183890  loss = 2.12475 avg_loss = 3.48527\n",
      "epoch no.2 train no.183900  loss = 5.39832 avg_loss = 3.46511\n",
      "epoch no.2 train no.183910  loss = 1.64937 avg_loss = 3.43122\n",
      "epoch no.2 train no.183920  loss = 3.57578 avg_loss = 3.41278\n",
      "epoch no.2 train no.183930  loss = 2.00151 avg_loss = 3.39003\n",
      "epoch no.2 train no.183940  loss = 2.59607 avg_loss = 3.38326\n",
      "epoch no.2 train no.183950  loss = 3.80358 avg_loss = 3.44061\n",
      "epoch no.2 train no.183960  loss = 2.07983 avg_loss = 3.39273\n",
      "epoch no.2 train no.183970  loss = 2.47562 avg_loss = 3.38799\n",
      "epoch no.2 train no.183980  loss = 5.81952 avg_loss = 3.43546\n",
      "epoch no.2 train no.183990  loss = 4.54085 avg_loss = 3.42060\n",
      "epoch no.2 train no.184000  loss = 2.20887 avg_loss = 3.36039\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.184010  loss = 4.35803 avg_loss = 3.35680\n",
      "epoch no.2 train no.184020  loss = 4.41608 avg_loss = 3.39047\n",
      "epoch no.2 train no.184030  loss = 3.21648 avg_loss = 3.40930\n",
      "epoch no.2 train no.184040  loss = 4.10720 avg_loss = 3.43843\n",
      "epoch no.2 train no.184050  loss = 1.93381 avg_loss = 3.41323\n",
      "epoch no.2 train no.184060  loss = 2.25169 avg_loss = 3.39531\n",
      "epoch no.2 train no.184070  loss = 2.90598 avg_loss = 3.40469\n",
      "epoch no.2 train no.184080  loss = 4.95848 avg_loss = 3.43081\n",
      "epoch no.2 train no.184090  loss = 1.98731 avg_loss = 3.40118\n",
      "epoch no.2 train no.184100  loss = 2.82593 avg_loss = 3.40757\n",
      "epoch no.2 train no.184110  loss = 3.10024 avg_loss = 3.42362\n",
      "epoch no.2 train no.184120  loss = 3.75486 avg_loss = 3.42828\n",
      "epoch no.2 train no.184130  loss = 4.18081 avg_loss = 3.45703\n",
      "epoch no.2 train no.184140  loss = 5.95814 avg_loss = 3.45133\n",
      "epoch no.2 train no.184150  loss = 2.16986 avg_loss = 3.44924\n",
      "epoch no.2 train no.184160  loss = 2.80584 avg_loss = 3.41985\n",
      "epoch no.2 train no.184170  loss = 2.34353 avg_loss = 3.48295\n",
      "epoch no.2 train no.184180  loss = 2.94466 avg_loss = 3.50826\n",
      "epoch no.2 train no.184190  loss = 5.02296 avg_loss = 3.48839\n",
      "epoch no.2 train no.184200  loss = 4.16818 avg_loss = 3.48378\n",
      "epoch no.2 train no.184210  loss = 2.55150 avg_loss = 3.45289\n",
      "epoch no.2 train no.184220  loss = 3.64139 avg_loss = 3.44854\n",
      "epoch no.2 train no.184230  loss = 4.40773 avg_loss = 3.54157\n",
      "epoch no.2 train no.184240  loss = 3.53889 avg_loss = 3.57383\n",
      "epoch no.2 train no.184250  loss = 2.37844 avg_loss = 3.54111\n",
      "epoch no.2 train no.184260  loss = 3.04294 avg_loss = 3.47993\n",
      "epoch no.2 train no.184270  loss = 5.72588 avg_loss = 3.47049\n",
      "epoch no.2 train no.184280  loss = 3.68424 avg_loss = 3.53498\n",
      "epoch no.2 train no.184290  loss = 4.47404 avg_loss = 3.54152\n",
      "epoch no.2 train no.184300  loss = 3.80932 avg_loss = 3.58766\n",
      "epoch no.2 train no.184310  loss = 1.62192 avg_loss = 3.58409\n",
      "epoch no.2 train no.184320  loss = 2.99476 avg_loss = 3.57514\n",
      "epoch no.2 train no.184330  loss = 2.26551 avg_loss = 3.55303\n",
      "epoch no.2 train no.184340  loss = 2.86335 avg_loss = 3.57872\n",
      "epoch no.2 train no.184350  loss = 3.45013 avg_loss = 3.59410\n",
      "epoch no.2 train no.184360  loss = 3.48936 avg_loss = 3.54810\n",
      "epoch no.2 train no.184370  loss = 3.15866 avg_loss = 3.56109\n",
      "epoch no.2 train no.184380  loss = 5.49485 avg_loss = 3.57793\n",
      "epoch no.2 train no.184390  loss = 2.94862 avg_loss = 3.56057\n",
      "epoch no.2 train no.184400  loss = 5.33848 avg_loss = 3.58378\n",
      "epoch no.2 train no.184410  loss = 3.21437 avg_loss = 3.57584\n",
      "epoch no.2 train no.184420  loss = 3.71283 avg_loss = 3.56739\n",
      "epoch no.2 train no.184430  loss = 3.99404 avg_loss = 3.50738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.184440  loss = 3.69812 avg_loss = 3.53551\n",
      "epoch no.2 train no.184450  loss = 3.40291 avg_loss = 3.55989\n",
      "epoch no.2 train no.184460  loss = 4.04195 avg_loss = 3.52708\n",
      "epoch no.2 train no.184470  loss = 1.56725 avg_loss = 3.46213\n",
      "epoch no.2 train no.184480  loss = 3.48935 avg_loss = 3.42036\n",
      "epoch no.2 train no.184490  loss = 3.03462 avg_loss = 3.40774\n",
      "epoch no.2 train no.184500  loss = 3.36303 avg_loss = 3.37380\n",
      "epoch no.2 train no.184510  loss = 2.46410 avg_loss = 3.35509\n",
      "epoch no.2 train no.184520  loss = 3.79818 avg_loss = 3.33061\n",
      "epoch no.2 train no.184530  loss = 5.62122 avg_loss = 3.43361\n",
      "epoch no.2 train no.184540  loss = 5.03125 avg_loss = 3.42851\n",
      "epoch no.2 train no.184550  loss = 3.85379 avg_loss = 3.40773\n",
      "epoch no.2 train no.184560  loss = 4.03268 avg_loss = 3.41875\n",
      "epoch no.2 train no.184570  loss = 4.52256 avg_loss = 3.36710\n",
      "epoch no.2 train no.184580  loss = 3.33271 avg_loss = 3.38162\n",
      "epoch no.2 train no.184590  loss = 2.73232 avg_loss = 3.36586\n",
      "epoch no.2 train no.184600  loss = 2.71016 avg_loss = 3.37608\n",
      "epoch no.2 train no.184610  loss = 5.49344 avg_loss = 3.39411\n",
      "epoch no.2 train no.184620  loss = 4.04735 avg_loss = 3.39328\n",
      "epoch no.2 train no.184630  loss = 4.91619 avg_loss = 3.40270\n",
      "epoch no.2 train no.184640  loss = 2.63153 avg_loss = 3.41662\n",
      "epoch no.2 train no.184650  loss = 3.06411 avg_loss = 3.42219\n",
      "epoch no.2 train no.184660  loss = 3.99273 avg_loss = 3.44028\n",
      "epoch no.2 train no.184670  loss = 2.42156 avg_loss = 3.41306\n",
      "epoch no.2 train no.184680  loss = 2.25754 avg_loss = 3.42760\n",
      "epoch no.2 train no.184690  loss = 4.00326 avg_loss = 3.41233\n",
      "epoch no.2 train no.184700  loss = 2.45557 avg_loss = 3.42948\n",
      "epoch no.2 train no.184710  loss = 3.78955 avg_loss = 3.46352\n",
      "epoch no.2 train no.184720  loss = 2.70043 avg_loss = 3.44898\n",
      "epoch no.2 train no.184730  loss = 2.92770 avg_loss = 3.49232\n",
      "epoch no.2 train no.184740  loss = 3.51074 avg_loss = 3.47105\n",
      "epoch no.2 train no.184750  loss = 2.41068 avg_loss = 3.51042\n",
      "epoch no.2 train no.184760  loss = 3.22622 avg_loss = 3.50415\n",
      "epoch no.2 train no.184770  loss = 2.46534 avg_loss = 3.52430\n",
      "epoch no.2 train no.184780  loss = 4.08822 avg_loss = 3.49241\n",
      "epoch no.2 train no.184790  loss = 3.79343 avg_loss = 3.50634\n",
      "epoch no.2 train no.184800  loss = 3.14460 avg_loss = 3.48466\n",
      "epoch no.2 train no.184810  loss = 2.91134 avg_loss = 3.45606\n",
      "epoch no.2 train no.184820  loss = 3.66271 avg_loss = 3.47091\n",
      "epoch no.2 train no.184830  loss = 4.46620 avg_loss = 3.51987\n",
      "epoch no.2 train no.184840  loss = 2.79564 avg_loss = 3.48472\n",
      "epoch no.2 train no.184850  loss = 2.42491 avg_loss = 3.44545\n",
      "epoch no.2 train no.184860  loss = 3.66771 avg_loss = 3.46301\n",
      "epoch no.2 train no.184870  loss = 4.07386 avg_loss = 3.46956\n",
      "epoch no.2 train no.184880  loss = 2.07775 avg_loss = 3.47505\n",
      "epoch no.2 train no.184890  loss = 3.39422 avg_loss = 3.48652\n",
      "epoch no.2 train no.184900  loss = 3.96569 avg_loss = 3.53289\n",
      "epoch no.2 train no.184910  loss = 3.61338 avg_loss = 3.56209\n",
      "epoch no.2 train no.184920  loss = 4.15104 avg_loss = 3.61682\n",
      "epoch no.2 train no.184930  loss = 2.69505 avg_loss = 3.55865\n",
      "epoch no.2 train no.184940  loss = 1.98504 avg_loss = 3.51717\n",
      "epoch no.2 train no.184950  loss = 1.93595 avg_loss = 3.50992\n",
      "epoch no.2 train no.184960  loss = 5.36629 avg_loss = 3.51775\n",
      "epoch no.2 train no.184970  loss = 3.50495 avg_loss = 3.57701\n",
      "epoch no.2 train no.184980  loss = 4.76744 avg_loss = 3.52168\n",
      "epoch no.2 train no.184990  loss = 3.12379 avg_loss = 3.51981\n",
      "epoch no.2 train no.185000  loss = 4.00683 avg_loss = 3.52245\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '송', '</s>']\n",
      "추억의 90년대 팝송</s>\n",
      "epoch no.2 train no.185010  loss = 3.32673 avg_loss = 3.55208\n",
      "epoch no.2 train no.185020  loss = 2.07720 avg_loss = 3.51156\n",
      "epoch no.2 train no.185030  loss = 5.10460 avg_loss = 3.49056\n",
      "epoch no.2 train no.185040  loss = 3.50384 avg_loss = 3.46518\n",
      "epoch no.2 train no.185050  loss = 2.79243 avg_loss = 3.45236\n",
      "epoch no.2 train no.185060  loss = 3.66377 avg_loss = 3.46800\n",
      "epoch no.2 train no.185070  loss = 4.79157 avg_loss = 3.48488\n",
      "epoch no.2 train no.185080  loss = 2.03234 avg_loss = 3.41597\n",
      "epoch no.2 train no.185090  loss = 4.83559 avg_loss = 3.39505\n",
      "epoch no.2 train no.185100  loss = 3.40562 avg_loss = 3.38774\n",
      "epoch no.2 train no.185110  loss = 1.11310 avg_loss = 3.40397\n",
      "epoch no.2 train no.185120  loss = 3.11153 avg_loss = 3.40665\n",
      "epoch no.2 train no.185130  loss = 2.97469 avg_loss = 3.42530\n",
      "epoch no.2 train no.185140  loss = 3.39930 avg_loss = 3.42783\n",
      "epoch no.2 train no.185150  loss = 3.77199 avg_loss = 3.49314\n",
      "epoch no.2 train no.185160  loss = 3.30044 avg_loss = 3.55328\n",
      "epoch no.2 train no.185170  loss = 3.31830 avg_loss = 3.56619\n",
      "epoch no.2 train no.185180  loss = 4.03056 avg_loss = 3.61220\n",
      "epoch no.2 train no.185190  loss = 3.62929 avg_loss = 3.62881\n",
      "epoch no.2 train no.185200  loss = 3.49349 avg_loss = 3.60212\n",
      "epoch no.2 train no.185210  loss = 2.74719 avg_loss = 3.60599\n",
      "epoch no.2 train no.185220  loss = 3.57860 avg_loss = 3.55817\n",
      "epoch no.2 train no.185230  loss = 4.12579 avg_loss = 3.58013\n",
      "epoch no.2 train no.185240  loss = 3.08926 avg_loss = 3.54501\n",
      "epoch no.2 train no.185250  loss = 2.22278 avg_loss = 3.51625\n",
      "epoch no.2 train no.185260  loss = 3.24146 avg_loss = 3.57810\n",
      "epoch no.2 train no.185270  loss = 2.88326 avg_loss = 3.56432\n",
      "epoch no.2 train no.185280  loss = 2.89911 avg_loss = 3.54444\n",
      "epoch no.2 train no.185290  loss = 3.86469 avg_loss = 3.52686\n",
      "epoch no.2 train no.185300  loss = 2.95931 avg_loss = 3.47387\n",
      "epoch no.2 train no.185310  loss = 4.86744 avg_loss = 3.47962\n",
      "epoch no.2 train no.185320  loss = 3.25532 avg_loss = 3.46194\n",
      "epoch no.2 train no.185330  loss = 4.89081 avg_loss = 3.47065\n",
      "epoch no.2 train no.185340  loss = 4.24284 avg_loss = 3.43555\n",
      "epoch no.2 train no.185350  loss = 2.28068 avg_loss = 3.41742\n",
      "epoch no.2 train no.185360  loss = 2.19716 avg_loss = 3.45703\n",
      "epoch no.2 train no.185370  loss = 3.17768 avg_loss = 3.44358\n",
      "epoch no.2 train no.185380  loss = 3.71433 avg_loss = 3.41660\n",
      "epoch no.2 train no.185390  loss = 3.31862 avg_loss = 3.44473\n",
      "epoch no.2 train no.185400  loss = 3.90514 avg_loss = 3.44108\n",
      "epoch no.2 train no.185410  loss = 4.78384 avg_loss = 3.47555\n",
      "epoch no.2 train no.185420  loss = 2.47867 avg_loss = 3.50342\n",
      "epoch no.2 train no.185430  loss = 1.79536 avg_loss = 3.45690\n",
      "epoch no.2 train no.185440  loss = 4.12554 avg_loss = 3.41186\n",
      "epoch no.2 train no.185450  loss = 5.66428 avg_loss = 3.45692\n",
      "epoch no.2 train no.185460  loss = 2.63068 avg_loss = 3.45904\n",
      "epoch no.2 train no.185470  loss = 3.26875 avg_loss = 3.48758\n",
      "epoch no.2 train no.185480  loss = 4.53039 avg_loss = 3.53880\n",
      "epoch no.2 train no.185490  loss = 3.53904 avg_loss = 3.52361\n",
      "epoch no.2 train no.185500  loss = 3.28634 avg_loss = 3.51461\n",
      "epoch no.2 train no.185510  loss = 3.49667 avg_loss = 3.44788\n",
      "epoch no.2 train no.185520  loss = 3.37262 avg_loss = 3.45587\n",
      "epoch no.2 train no.185530  loss = 3.16006 avg_loss = 3.49435\n",
      "epoch no.2 train no.185540  loss = 4.52824 avg_loss = 3.48354\n",
      "epoch no.2 train no.185550  loss = 2.38663 avg_loss = 3.47416\n",
      "epoch no.2 train no.185560  loss = 1.90927 avg_loss = 3.44411\n",
      "epoch no.2 train no.185570  loss = 3.69681 avg_loss = 3.43498\n",
      "epoch no.2 train no.185580  loss = 4.59850 avg_loss = 3.47583\n",
      "epoch no.2 train no.185590  loss = 2.17813 avg_loss = 3.43890\n",
      "epoch no.2 train no.185600  loss = 3.96462 avg_loss = 3.45189\n",
      "epoch no.2 train no.185610  loss = 1.79704 avg_loss = 3.39978\n",
      "epoch no.2 train no.185620  loss = 5.03744 avg_loss = 3.40067\n",
      "epoch no.2 train no.185630  loss = 2.87216 avg_loss = 3.37856\n",
      "epoch no.2 train no.185640  loss = 2.06712 avg_loss = 3.38484\n",
      "epoch no.2 train no.185650  loss = 2.10344 avg_loss = 3.36586\n",
      "epoch no.2 train no.185660  loss = 4.22332 avg_loss = 3.36673\n",
      "epoch no.2 train no.185670  loss = 3.16609 avg_loss = 3.37169\n",
      "epoch no.2 train no.185680  loss = 2.72365 avg_loss = 3.38044\n",
      "epoch no.2 train no.185690  loss = 6.41502 avg_loss = 3.41455\n",
      "epoch no.2 train no.185700  loss = 5.15758 avg_loss = 3.44180\n",
      "epoch no.2 train no.185710  loss = 2.90158 avg_loss = 3.45239\n",
      "epoch no.2 train no.185720  loss = 2.91830 avg_loss = 3.47278\n",
      "epoch no.2 train no.185730  loss = 3.55009 avg_loss = 3.50400\n",
      "epoch no.2 train no.185740  loss = 3.69308 avg_loss = 3.51907\n",
      "epoch no.2 train no.185750  loss = 3.21659 avg_loss = 3.55460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.185760  loss = 2.61246 avg_loss = 3.57446\n",
      "epoch no.2 train no.185770  loss = 2.84834 avg_loss = 3.59039\n",
      "epoch no.2 train no.185780  loss = 3.62368 avg_loss = 3.55972\n",
      "epoch no.2 train no.185790  loss = 3.93915 avg_loss = 3.54112\n",
      "epoch no.2 train no.185800  loss = 3.18133 avg_loss = 3.52427\n",
      "epoch no.2 train no.185810  loss = 2.88644 avg_loss = 3.51140\n",
      "epoch no.2 train no.185820  loss = 1.47306 avg_loss = 3.52119\n",
      "epoch no.2 train no.185830  loss = 2.89339 avg_loss = 3.55954\n",
      "epoch no.2 train no.185840  loss = 4.65848 avg_loss = 3.57590\n",
      "epoch no.2 train no.185850  loss = 3.26525 avg_loss = 3.60171\n",
      "epoch no.2 train no.185860  loss = 3.11460 avg_loss = 3.60103\n",
      "epoch no.2 train no.185870  loss = 3.97385 avg_loss = 3.61007\n",
      "epoch no.2 train no.185880  loss = 1.55686 avg_loss = 3.57708\n",
      "epoch no.2 train no.185890  loss = 3.97138 avg_loss = 3.60336\n",
      "epoch no.2 train no.185900  loss = 3.52011 avg_loss = 3.64224\n",
      "epoch no.2 train no.185910  loss = 2.73787 avg_loss = 3.59904\n",
      "epoch no.2 train no.185920  loss = 2.64595 avg_loss = 3.60666\n",
      "epoch no.2 train no.185930  loss = 3.99310 avg_loss = 3.57899\n",
      "epoch no.2 train no.185940  loss = 2.91458 avg_loss = 3.58131\n",
      "epoch no.2 train no.185950  loss = 2.54330 avg_loss = 3.55725\n",
      "epoch no.2 train no.185960  loss = 2.20862 avg_loss = 3.52585\n",
      "epoch no.2 train no.185970  loss = 3.87502 avg_loss = 3.55062\n",
      "epoch no.2 train no.185980  loss = 4.41451 avg_loss = 3.55300\n",
      "epoch no.2 train no.185990  loss = 2.96960 avg_loss = 3.51308\n",
      "epoch no.2 train no.186000  loss = 2.78544 avg_loss = 3.51041\n",
      "3\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '▁베스트']\n",
      "추억의 90년대 가요</s>\n",
      "epoch no.2 train no.186010  loss = 3.13472 avg_loss = 3.52493\n",
      "epoch no.2 train no.186020  loss = 2.27603 avg_loss = 3.51166\n",
      "epoch no.2 train no.186030  loss = 5.28680 avg_loss = 3.56354\n",
      "epoch no.2 train no.186040  loss = 3.91253 avg_loss = 3.60992\n",
      "epoch no.2 train no.186050  loss = 4.51141 avg_loss = 3.60543\n",
      "epoch no.2 train no.186060  loss = 3.67606 avg_loss = 3.61165\n",
      "epoch no.2 train no.186070  loss = 2.84373 avg_loss = 3.59198\n",
      "epoch no.2 train no.186080  loss = 1.97813 avg_loss = 3.55535\n",
      "epoch no.2 train no.186090  loss = 2.78715 avg_loss = 3.55582\n",
      "epoch no.2 train no.186100  loss = 3.45967 avg_loss = 3.50156\n",
      "epoch no.2 train no.186110  loss = 3.65449 avg_loss = 3.49966\n",
      "epoch no.2 train no.186120  loss = 3.05721 avg_loss = 3.49604\n",
      "epoch no.2 train no.186130  loss = 3.75488 avg_loss = 3.52563\n",
      "epoch no.2 train no.186140  loss = 3.90927 avg_loss = 3.54157\n",
      "epoch no.2 train no.186150  loss = 3.84833 avg_loss = 3.53588\n",
      "epoch no.2 train no.186160  loss = 3.09183 avg_loss = 3.50726\n",
      "epoch no.2 train no.186170  loss = 4.41062 avg_loss = 3.55753\n",
      "epoch no.2 train no.186180  loss = 3.10858 avg_loss = 3.55577\n",
      "epoch no.2 train no.186190  loss = 2.08372 avg_loss = 3.58061\n",
      "epoch no.2 train no.186200  loss = 1.91219 avg_loss = 3.59672\n",
      "epoch no.2 train no.186210  loss = 3.34238 avg_loss = 3.54063\n",
      "epoch no.2 train no.186220  loss = 3.14879 avg_loss = 3.53331\n",
      "epoch no.2 train no.186230  loss = 2.58735 avg_loss = 3.47287\n",
      "epoch no.2 train no.186240  loss = 5.06633 avg_loss = 3.44924\n",
      "epoch no.2 train no.186250  loss = 4.56925 avg_loss = 3.50858\n",
      "epoch no.2 train no.186260  loss = 3.99063 avg_loss = 3.47043\n",
      "epoch no.2 train no.186270  loss = 3.57148 avg_loss = 3.47119\n",
      "epoch no.2 train no.186280  loss = 2.04639 avg_loss = 3.41676\n",
      "epoch no.2 train no.186290  loss = 6.57795 avg_loss = 3.48450\n",
      "epoch no.2 train no.186300  loss = 5.64756 avg_loss = 3.50463\n",
      "epoch no.2 train no.186310  loss = 3.28874 avg_loss = 3.47425\n",
      "epoch no.2 train no.186320  loss = 2.97211 avg_loss = 3.42128\n",
      "epoch no.2 train no.186330  loss = 3.98450 avg_loss = 3.42259\n",
      "epoch no.2 train no.186340  loss = 2.58507 avg_loss = 3.39247\n",
      "epoch no.2 train no.186350  loss = 4.72717 avg_loss = 3.40150\n",
      "epoch no.2 train no.186360  loss = 2.81048 avg_loss = 3.38047\n",
      "epoch no.2 train no.186370  loss = 3.59981 avg_loss = 3.39276\n",
      "epoch no.2 train no.186380  loss = 2.24599 avg_loss = 3.37671\n",
      "epoch no.2 train no.186390  loss = 3.36949 avg_loss = 3.42120\n",
      "epoch no.2 train no.186400  loss = 3.34436 avg_loss = 3.38878\n",
      "epoch no.2 train no.186410  loss = 4.22913 avg_loss = 3.39704\n",
      "epoch no.2 train no.186420  loss = 3.14739 avg_loss = 3.42395\n",
      "epoch no.2 train no.186430  loss = 1.76072 avg_loss = 3.42229\n",
      "epoch no.2 train no.186440  loss = 2.97334 avg_loss = 3.40606\n",
      "epoch no.2 train no.186450  loss = 2.21669 avg_loss = 3.35165\n",
      "epoch no.2 train no.186460  loss = 2.92746 avg_loss = 3.33694\n",
      "epoch no.2 train no.186470  loss = 3.63299 avg_loss = 3.36156\n",
      "epoch no.2 train no.186480  loss = 2.93825 avg_loss = 3.41294\n",
      "epoch no.2 train no.186490  loss = 2.90708 avg_loss = 3.45161\n",
      "epoch no.2 train no.186500  loss = 2.74879 avg_loss = 3.46440\n",
      "epoch no.2 train no.186510  loss = 2.61139 avg_loss = 3.44385\n",
      "epoch no.2 train no.186520  loss = 3.52535 avg_loss = 3.45074\n",
      "epoch no.2 train no.186530  loss = 3.42052 avg_loss = 3.48528\n",
      "epoch no.2 train no.186540  loss = 4.76972 avg_loss = 3.49033\n",
      "epoch no.2 train no.186550  loss = 3.21072 avg_loss = 3.45070\n",
      "epoch no.2 train no.186560  loss = 4.44426 avg_loss = 3.41947\n",
      "epoch no.2 train no.186570  loss = 4.22008 avg_loss = 3.43635\n",
      "epoch no.2 train no.186580  loss = 5.34742 avg_loss = 3.43697\n",
      "epoch no.2 train no.186590  loss = 2.60194 avg_loss = 3.41633\n",
      "epoch no.2 train no.186600  loss = 2.70693 avg_loss = 3.45554\n",
      "epoch no.2 train no.186610  loss = 4.60561 avg_loss = 3.44025\n",
      "epoch no.2 train no.186620  loss = 3.18696 avg_loss = 3.43204\n",
      "epoch no.2 train no.186630  loss = 4.55587 avg_loss = 3.43427\n",
      "epoch no.2 train no.186640  loss = 2.08468 avg_loss = 3.45348\n",
      "epoch no.2 train no.186650  loss = 4.01811 avg_loss = 3.46333\n",
      "epoch no.2 train no.186660  loss = 3.99625 avg_loss = 3.52678\n",
      "epoch no.2 train no.186670  loss = 3.53683 avg_loss = 3.47376\n",
      "epoch no.2 train no.186680  loss = 2.94982 avg_loss = 3.50169\n",
      "epoch no.2 train no.186690  loss = 2.61497 avg_loss = 3.50027\n",
      "epoch no.2 train no.186700  loss = 2.16698 avg_loss = 3.47435\n",
      "epoch no.2 train no.186710  loss = 3.34946 avg_loss = 3.49678\n",
      "epoch no.2 train no.186720  loss = 3.31192 avg_loss = 3.46858\n",
      "epoch no.2 train no.186730  loss = 3.33114 avg_loss = 3.48177\n",
      "epoch no.2 train no.186740  loss = 2.94230 avg_loss = 3.47975\n",
      "epoch no.2 train no.186750  loss = 1.76088 avg_loss = 3.48085\n",
      "epoch no.2 train no.186760  loss = 5.02235 avg_loss = 3.50325\n",
      "epoch no.2 train no.186770  loss = 2.76635 avg_loss = 3.48322\n",
      "epoch no.2 train no.186780  loss = 3.33470 avg_loss = 3.48607\n",
      "epoch no.2 train no.186790  loss = 4.03216 avg_loss = 3.48541\n",
      "epoch no.2 train no.186800  loss = 2.96143 avg_loss = 3.45469\n",
      "epoch no.2 train no.186810  loss = 4.30819 avg_loss = 3.50424\n",
      "epoch no.2 train no.186820  loss = 3.18848 avg_loss = 3.48906\n",
      "epoch no.2 train no.186830  loss = 5.00677 avg_loss = 3.48390\n",
      "epoch no.2 train no.186840  loss = 3.30051 avg_loss = 3.47871\n",
      "epoch no.2 train no.186850  loss = 3.03391 avg_loss = 3.46446\n",
      "epoch no.2 train no.186860  loss = 3.91903 avg_loss = 3.42419\n",
      "epoch no.2 train no.186870  loss = 2.33170 avg_loss = 3.43827\n",
      "epoch no.2 train no.186880  loss = 3.12128 avg_loss = 3.41268\n",
      "epoch no.2 train no.186890  loss = 3.07103 avg_loss = 3.42028\n",
      "epoch no.2 train no.186900  loss = 3.77773 avg_loss = 3.39986\n",
      "epoch no.2 train no.186910  loss = 5.00239 avg_loss = 3.42205\n",
      "epoch no.2 train no.186920  loss = 3.28978 avg_loss = 3.46128\n",
      "epoch no.2 train no.186930  loss = 1.91107 avg_loss = 3.44609\n",
      "epoch no.2 train no.186940  loss = 4.78980 avg_loss = 3.47113\n",
      "epoch no.2 train no.186950  loss = 3.67939 avg_loss = 3.53086\n",
      "epoch no.2 train no.186960  loss = 4.23911 avg_loss = 3.58049\n",
      "epoch no.2 train no.186970  loss = 1.88459 avg_loss = 3.58711\n",
      "epoch no.2 train no.186980  loss = 2.93758 avg_loss = 3.53332\n",
      "epoch no.2 train no.186990  loss = 3.13493 avg_loss = 3.53100\n",
      "epoch no.2 train no.187000  loss = 2.44881 avg_loss = 3.53776\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '곡', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.2 train no.187010  loss = 3.59435 avg_loss = 3.54248\n",
      "epoch no.2 train no.187020  loss = 2.74139 avg_loss = 3.57260\n",
      "epoch no.2 train no.187030  loss = 3.34445 avg_loss = 3.59568\n",
      "epoch no.2 train no.187040  loss = 2.17634 avg_loss = 3.57539\n",
      "epoch no.2 train no.187050  loss = 4.23842 avg_loss = 3.59316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.187060  loss = 2.61160 avg_loss = 3.61531\n",
      "epoch no.2 train no.187070  loss = 3.80919 avg_loss = 3.60224\n",
      "epoch no.2 train no.187080  loss = 3.22343 avg_loss = 3.54447\n",
      "epoch no.2 train no.187090  loss = 3.07751 avg_loss = 3.49732\n",
      "epoch no.2 train no.187100  loss = 3.76822 avg_loss = 3.49726\n",
      "epoch no.2 train no.187110  loss = 2.63655 avg_loss = 3.50667\n",
      "epoch no.2 train no.187120  loss = 5.18918 avg_loss = 3.46617\n",
      "epoch no.2 train no.187130  loss = 2.08738 avg_loss = 3.40583\n",
      "epoch no.2 train no.187140  loss = 3.90409 avg_loss = 3.42918\n",
      "epoch no.2 train no.187150  loss = 2.71821 avg_loss = 3.45268\n",
      "epoch no.2 train no.187160  loss = 3.47558 avg_loss = 3.48912\n",
      "epoch no.2 train no.187170  loss = 2.76875 avg_loss = 3.42404\n",
      "epoch no.2 train no.187180  loss = 5.06290 avg_loss = 3.45355\n",
      "epoch no.2 train no.187190  loss = 2.80145 avg_loss = 3.46306\n",
      "epoch no.2 train no.187200  loss = 3.95619 avg_loss = 3.47723\n",
      "epoch no.2 train no.187210  loss = 2.91449 avg_loss = 3.47430\n",
      "epoch no.2 train no.187220  loss = 3.52869 avg_loss = 3.45086\n",
      "epoch no.2 train no.187230  loss = 2.41497 avg_loss = 3.39330\n",
      "epoch no.2 train no.187240  loss = 2.89500 avg_loss = 3.40772\n",
      "epoch no.2 train no.187250  loss = 4.75945 avg_loss = 3.47024\n",
      "epoch no.2 train no.187260  loss = 2.71852 avg_loss = 3.47592\n",
      "epoch no.2 train no.187270  loss = 2.57839 avg_loss = 3.44459\n",
      "epoch no.2 train no.187280  loss = 4.72925 avg_loss = 3.45322\n",
      "epoch no.2 train no.187290  loss = 3.04927 avg_loss = 3.53884\n",
      "epoch no.2 train no.187300  loss = 4.75164 avg_loss = 3.56958\n",
      "epoch no.2 train no.187310  loss = 4.60591 avg_loss = 3.57695\n",
      "epoch no.2 train no.187320  loss = 3.26355 avg_loss = 3.60122\n",
      "epoch no.2 train no.187330  loss = 3.97234 avg_loss = 3.62565\n",
      "epoch no.2 train no.187340  loss = 2.39178 avg_loss = 3.55390\n",
      "epoch no.2 train no.187350  loss = 2.34915 avg_loss = 3.53713\n",
      "epoch no.2 train no.187360  loss = 4.42933 avg_loss = 3.48916\n",
      "epoch no.2 train no.187370  loss = 2.75770 avg_loss = 3.54923\n",
      "epoch no.2 train no.187380  loss = 2.27163 avg_loss = 3.52924\n",
      "epoch no.2 train no.187390  loss = 3.42401 avg_loss = 3.52550\n",
      "epoch no.2 train no.187400  loss = 2.95013 avg_loss = 3.51863\n",
      "epoch no.2 train no.187410  loss = 4.69572 avg_loss = 3.50656\n",
      "epoch no.2 train no.187420  loss = 4.89765 avg_loss = 3.50659\n",
      "epoch no.2 train no.187430  loss = 4.99476 avg_loss = 3.47010\n",
      "epoch no.2 train no.187440  loss = 2.81282 avg_loss = 3.46213\n",
      "epoch no.2 train no.187450  loss = 3.51780 avg_loss = 3.49518\n",
      "epoch no.2 train no.187460  loss = 3.96393 avg_loss = 3.47866\n",
      "epoch no.2 train no.187470  loss = 3.23660 avg_loss = 3.45427\n",
      "epoch no.2 train no.187480  loss = 2.82656 avg_loss = 3.40531\n",
      "epoch no.2 train no.187490  loss = 4.00951 avg_loss = 3.43224\n",
      "epoch no.2 train no.187500  loss = 3.38678 avg_loss = 3.44563\n",
      "epoch no.2 train no.187510  loss = 3.98084 avg_loss = 3.48465\n",
      "epoch no.2 train no.187520  loss = 3.41744 avg_loss = 3.46231\n",
      "epoch no.2 train no.187530  loss = 4.09259 avg_loss = 3.45719\n",
      "epoch no.2 train no.187540  loss = 3.65536 avg_loss = 3.48228\n",
      "epoch no.2 train no.187550  loss = 2.92756 avg_loss = 3.46260\n",
      "epoch no.2 train no.187560  loss = 2.90580 avg_loss = 3.47384\n",
      "epoch no.2 train no.187570  loss = 2.93150 avg_loss = 3.49454\n",
      "epoch no.2 train no.187580  loss = 2.32423 avg_loss = 3.42800\n",
      "epoch no.2 train no.187590  loss = 2.00435 avg_loss = 3.40681\n",
      "epoch no.2 train no.187600  loss = 4.52511 avg_loss = 3.44330\n",
      "epoch no.2 train no.187610  loss = 3.36287 avg_loss = 3.41863\n",
      "epoch no.2 train no.187620  loss = 2.43722 avg_loss = 3.36408\n",
      "epoch no.2 train no.187630  loss = 2.56285 avg_loss = 3.36431\n",
      "epoch no.2 train no.187640  loss = 3.16406 avg_loss = 3.36205\n",
      "epoch no.2 train no.187650  loss = 3.96060 avg_loss = 3.35803\n",
      "epoch no.2 train no.187660  loss = 4.10621 avg_loss = 3.42424\n",
      "epoch no.2 train no.187670  loss = 2.42493 avg_loss = 3.41520\n",
      "epoch no.2 train no.187680  loss = 5.31271 avg_loss = 3.45618\n",
      "epoch no.2 train no.187690  loss = 1.62252 avg_loss = 3.48423\n",
      "epoch no.2 train no.187700  loss = 1.78784 avg_loss = 3.51498\n",
      "epoch no.2 train no.187710  loss = 5.55907 avg_loss = 3.54524\n",
      "epoch no.2 train no.187720  loss = 2.31172 avg_loss = 3.54674\n",
      "epoch no.2 train no.187730  loss = 3.04793 avg_loss = 3.52981\n",
      "epoch no.2 train no.187740  loss = 2.43597 avg_loss = 3.49269\n",
      "epoch no.2 train no.187750  loss = 4.60840 avg_loss = 3.47303\n",
      "epoch no.2 train no.187760  loss = 5.88684 avg_loss = 3.43073\n",
      "epoch no.2 train no.187770  loss = 3.40786 avg_loss = 3.42882\n",
      "epoch no.2 train no.187780  loss = 2.27179 avg_loss = 3.44855\n",
      "epoch no.2 train no.187790  loss = 4.60247 avg_loss = 3.48654\n",
      "epoch no.2 train no.187800  loss = 2.51758 avg_loss = 3.47633\n",
      "epoch no.2 train no.187810  loss = 3.36376 avg_loss = 3.47872\n",
      "epoch no.2 train no.187820  loss = 2.61077 avg_loss = 3.43212\n",
      "epoch no.2 train no.187830  loss = 2.64420 avg_loss = 3.40483\n",
      "epoch no.2 train no.187840  loss = 4.26573 avg_loss = 3.42805\n",
      "epoch no.2 train no.187850  loss = 4.91375 avg_loss = 3.41497\n",
      "epoch no.2 train no.187860  loss = 4.32604 avg_loss = 3.44701\n",
      "epoch no.2 train no.187870  loss = 2.61947 avg_loss = 3.47274\n",
      "epoch no.2 train no.187880  loss = 2.11368 avg_loss = 3.45599\n",
      "epoch no.2 train no.187890  loss = 5.30080 avg_loss = 3.46268\n",
      "epoch no.2 train no.187900  loss = 3.09725 avg_loss = 3.43743\n",
      "epoch no.2 train no.187910  loss = 3.54238 avg_loss = 3.43353\n",
      "epoch no.2 train no.187920  loss = 2.25431 avg_loss = 3.45387\n",
      "epoch no.2 train no.187930  loss = 2.49476 avg_loss = 3.45363\n",
      "epoch no.2 train no.187940  loss = 2.60800 avg_loss = 3.43290\n",
      "epoch no.2 train no.187950  loss = 4.66329 avg_loss = 3.44480\n",
      "epoch no.2 train no.187960  loss = 3.77710 avg_loss = 3.41156\n",
      "epoch no.2 train no.187970  loss = 3.84017 avg_loss = 3.45846\n",
      "epoch no.2 train no.187980  loss = 3.02185 avg_loss = 3.44955\n",
      "epoch no.2 train no.187990  loss = 3.28839 avg_loss = 3.44819\n",
      "epoch no.2 train no.188000  loss = 2.48263 avg_loss = 3.42195\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.188010  loss = 3.18648 avg_loss = 3.40826\n",
      "epoch no.2 train no.188020  loss = 3.66323 avg_loss = 3.44488\n",
      "epoch no.2 train no.188030  loss = 2.32037 avg_loss = 3.40041\n",
      "epoch no.2 train no.188040  loss = 1.98467 avg_loss = 3.36277\n",
      "epoch no.2 train no.188050  loss = 3.74986 avg_loss = 3.41099\n",
      "epoch no.2 train no.188060  loss = 4.46845 avg_loss = 3.41317\n",
      "epoch no.2 train no.188070  loss = 4.22092 avg_loss = 3.42793\n",
      "epoch no.2 train no.188080  loss = 2.18448 avg_loss = 3.40869\n",
      "epoch no.2 train no.188090  loss = 2.93388 avg_loss = 3.41116\n",
      "epoch no.2 train no.188100  loss = 5.91306 avg_loss = 3.42207\n",
      "epoch no.2 train no.188110  loss = 4.54899 avg_loss = 3.42768\n",
      "epoch no.2 train no.188120  loss = 2.97578 avg_loss = 3.47037\n",
      "epoch no.2 train no.188130  loss = 4.52088 avg_loss = 3.57917\n",
      "epoch no.2 train no.188140  loss = 2.95223 avg_loss = 3.51486\n",
      "epoch no.2 train no.188150  loss = 5.40770 avg_loss = 3.53358\n",
      "epoch no.2 train no.188160  loss = 3.00957 avg_loss = 3.52722\n",
      "epoch no.2 train no.188170  loss = 2.87704 avg_loss = 3.47827\n",
      "epoch no.2 train no.188180  loss = 4.33499 avg_loss = 3.53162\n",
      "epoch no.2 train no.188190  loss = 5.73792 avg_loss = 3.53915\n",
      "epoch no.2 train no.188200  loss = 2.78112 avg_loss = 3.56117\n",
      "epoch no.2 train no.188210  loss = 6.02351 avg_loss = 3.53074\n",
      "epoch no.2 train no.188220  loss = 5.80564 avg_loss = 3.58577\n",
      "epoch no.2 train no.188230  loss = 3.22189 avg_loss = 3.60240\n",
      "epoch no.2 train no.188240  loss = 2.83835 avg_loss = 3.60855\n",
      "epoch no.2 train no.188250  loss = 4.28907 avg_loss = 3.69744\n",
      "epoch no.2 train no.188260  loss = 3.52785 avg_loss = 3.69394\n",
      "epoch no.2 train no.188270  loss = 4.18430 avg_loss = 3.70305\n",
      "epoch no.2 train no.188280  loss = 2.35851 avg_loss = 3.64439\n",
      "epoch no.2 train no.188290  loss = 2.19249 avg_loss = 3.59864\n",
      "epoch no.2 train no.188300  loss = 2.76225 avg_loss = 3.56647\n",
      "epoch no.2 train no.188310  loss = 2.74939 avg_loss = 3.50967\n",
      "epoch no.2 train no.188320  loss = 3.82002 avg_loss = 3.51588\n",
      "epoch no.2 train no.188330  loss = 2.02258 avg_loss = 3.53648\n",
      "epoch no.2 train no.188340  loss = 2.58981 avg_loss = 3.50396\n",
      "epoch no.2 train no.188350  loss = 2.37614 avg_loss = 3.50715\n",
      "epoch no.2 train no.188360  loss = 3.16741 avg_loss = 3.50272\n",
      "epoch no.2 train no.188370  loss = 2.80367 avg_loss = 3.49263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.188380  loss = 4.69596 avg_loss = 3.55411\n",
      "epoch no.2 train no.188390  loss = 2.62467 avg_loss = 3.53004\n",
      "epoch no.2 train no.188400  loss = 3.49744 avg_loss = 3.54286\n",
      "epoch no.2 train no.188410  loss = 4.00755 avg_loss = 3.52990\n",
      "epoch no.2 train no.188420  loss = 3.80718 avg_loss = 3.49203\n",
      "epoch no.2 train no.188430  loss = 2.85848 avg_loss = 3.50208\n",
      "epoch no.2 train no.188440  loss = 2.04772 avg_loss = 3.47465\n",
      "epoch no.2 train no.188450  loss = 2.97956 avg_loss = 3.47670\n",
      "epoch no.2 train no.188460  loss = 3.48388 avg_loss = 3.44839\n",
      "epoch no.2 train no.188470  loss = 5.29958 avg_loss = 3.42893\n",
      "epoch no.2 train no.188480  loss = 4.38830 avg_loss = 3.41981\n",
      "epoch no.2 train no.188490  loss = 3.78434 avg_loss = 3.50503\n",
      "epoch no.2 train no.188500  loss = 4.41968 avg_loss = 3.55406\n",
      "epoch no.2 train no.188510  loss = 2.45273 avg_loss = 3.51955\n",
      "epoch no.2 train no.188520  loss = 2.03415 avg_loss = 3.52869\n",
      "epoch no.2 train no.188530  loss = 2.75742 avg_loss = 3.52015\n",
      "epoch no.2 train no.188540  loss = 4.24831 avg_loss = 3.49484\n",
      "epoch no.2 train no.188550  loss = 2.20733 avg_loss = 3.49016\n",
      "epoch no.2 train no.188560  loss = 4.25600 avg_loss = 3.50923\n",
      "epoch no.2 train no.188570  loss = 4.02073 avg_loss = 3.52326\n",
      "epoch no.2 train no.188580  loss = 3.62910 avg_loss = 3.52628\n",
      "epoch no.2 train no.188590  loss = 3.40297 avg_loss = 3.52759\n",
      "epoch no.2 train no.188600  loss = 3.25005 avg_loss = 3.46989\n",
      "epoch no.2 train no.188610  loss = 3.72142 avg_loss = 3.48239\n",
      "epoch no.2 train no.188620  loss = 4.24714 avg_loss = 3.45820\n",
      "epoch no.2 train no.188630  loss = 3.89324 avg_loss = 3.46299\n",
      "epoch no.2 train no.188640  loss = 2.00559 avg_loss = 3.46842\n",
      "epoch no.2 train no.188650  loss = 3.82222 avg_loss = 3.47668\n",
      "epoch no.2 train no.188660  loss = 3.84823 avg_loss = 3.47694\n",
      "epoch no.2 train no.188670  loss = 4.40664 avg_loss = 3.46581\n",
      "epoch no.2 train no.188680  loss = 3.80747 avg_loss = 3.48272\n",
      "epoch no.2 train no.188690  loss = 3.51714 avg_loss = 3.47664\n",
      "epoch no.2 train no.188700  loss = 2.95279 avg_loss = 3.50450\n",
      "epoch no.2 train no.188710  loss = 3.87237 avg_loss = 3.48274\n",
      "epoch no.2 train no.188720  loss = 5.24244 avg_loss = 3.50886\n",
      "epoch no.2 train no.188730  loss = 3.64405 avg_loss = 3.51315\n",
      "epoch no.2 train no.188740  loss = 3.28015 avg_loss = 3.49657\n",
      "epoch no.2 train no.188750  loss = 4.95401 avg_loss = 3.51752\n",
      "epoch no.2 train no.188760  loss = 3.11488 avg_loss = 3.46731\n",
      "epoch no.2 train no.188770  loss = 3.01545 avg_loss = 3.44160\n",
      "epoch no.2 train no.188780  loss = 1.80805 avg_loss = 3.43913\n",
      "epoch no.2 train no.188790  loss = 3.59788 avg_loss = 3.45123\n",
      "epoch no.2 train no.188800  loss = 3.83187 avg_loss = 3.44690\n",
      "epoch no.2 train no.188810  loss = 3.54376 avg_loss = 3.42916\n",
      "epoch no.2 train no.188820  loss = 3.99978 avg_loss = 3.48857\n",
      "epoch no.2 train no.188830  loss = 4.93640 avg_loss = 3.49784\n",
      "epoch no.2 train no.188840  loss = 3.95426 avg_loss = 3.48931\n",
      "epoch no.2 train no.188850  loss = 2.75409 avg_loss = 3.47260\n",
      "epoch no.2 train no.188860  loss = 4.49543 avg_loss = 3.48208\n",
      "epoch no.2 train no.188870  loss = 2.65897 avg_loss = 3.43554\n",
      "epoch no.2 train no.188880  loss = 4.41221 avg_loss = 3.52429\n",
      "epoch no.2 train no.188890  loss = 2.85891 avg_loss = 3.52007\n",
      "epoch no.2 train no.188900  loss = 3.39723 avg_loss = 3.53335\n",
      "epoch no.2 train no.188910  loss = 1.87387 avg_loss = 3.50022\n",
      "epoch no.2 train no.188920  loss = 4.63815 avg_loss = 3.59860\n",
      "epoch no.2 train no.188930  loss = 3.12934 avg_loss = 3.56830\n",
      "epoch no.2 train no.188940  loss = 3.14128 avg_loss = 3.57163\n",
      "epoch no.2 train no.188950  loss = 4.29348 avg_loss = 3.55936\n",
      "epoch no.2 train no.188960  loss = 4.56066 avg_loss = 3.55408\n",
      "epoch no.2 train no.188970  loss = 3.69950 avg_loss = 3.51819\n",
      "epoch no.2 train no.188980  loss = 2.19082 avg_loss = 3.50534\n",
      "epoch no.2 train no.188990  loss = 2.50481 avg_loss = 3.41708\n",
      "epoch no.2 train no.189000  loss = 3.77985 avg_loss = 3.42335\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.2 train no.189010  loss = 2.76440 avg_loss = 3.39871\n",
      "epoch no.2 train no.189020  loss = 2.90606 avg_loss = 3.39403\n",
      "epoch no.2 train no.189030  loss = 2.30106 avg_loss = 3.42442\n",
      "epoch no.2 train no.189040  loss = 3.37021 avg_loss = 3.46702\n",
      "epoch no.2 train no.189050  loss = 4.23864 avg_loss = 3.44910\n",
      "epoch no.2 train no.189060  loss = 3.55711 avg_loss = 3.43346\n",
      "epoch no.2 train no.189070  loss = 2.87775 avg_loss = 3.41855\n",
      "epoch no.2 train no.189080  loss = 3.87716 avg_loss = 3.41838\n",
      "epoch no.2 train no.189090  loss = 2.18153 avg_loss = 3.42753\n",
      "epoch no.2 train no.189100  loss = 4.13176 avg_loss = 3.43314\n",
      "epoch no.2 train no.189110  loss = 5.61886 avg_loss = 3.43168\n",
      "epoch no.2 train no.189120  loss = 4.94573 avg_loss = 3.45758\n",
      "epoch no.2 train no.189130  loss = 2.04321 avg_loss = 3.43972\n",
      "epoch no.2 train no.189140  loss = 2.39744 avg_loss = 3.38192\n",
      "epoch no.2 train no.189150  loss = 5.51969 avg_loss = 3.35344\n",
      "epoch no.2 train no.189160  loss = 2.80079 avg_loss = 3.36043\n",
      "epoch no.2 train no.189170  loss = 5.08882 avg_loss = 3.41811\n",
      "epoch no.2 train no.189180  loss = 4.14830 avg_loss = 3.42824\n",
      "epoch no.2 train no.189190  loss = 2.53068 avg_loss = 3.47442\n",
      "epoch no.2 train no.189200  loss = 4.52522 avg_loss = 3.46528\n",
      "epoch no.2 train no.189210  loss = 4.21868 avg_loss = 3.47203\n",
      "epoch no.2 train no.189220  loss = 1.87495 avg_loss = 3.47478\n",
      "epoch no.2 train no.189230  loss = 3.34889 avg_loss = 3.44528\n",
      "epoch no.2 train no.189240  loss = 1.60952 avg_loss = 3.43269\n",
      "epoch no.2 train no.189250  loss = 4.60492 avg_loss = 3.39074\n",
      "epoch no.2 train no.189260  loss = 3.03602 avg_loss = 3.38244\n",
      "epoch no.2 train no.189270  loss = 2.57810 avg_loss = 3.37308\n",
      "epoch no.2 train no.189280  loss = 1.92125 avg_loss = 3.33870\n",
      "epoch no.2 train no.189290  loss = 2.23469 avg_loss = 3.33519\n",
      "epoch no.2 train no.189300  loss = 3.05125 avg_loss = 3.40800\n",
      "epoch no.2 train no.189310  loss = 2.14343 avg_loss = 3.44880\n",
      "epoch no.2 train no.189320  loss = 2.26669 avg_loss = 3.44673\n",
      "epoch no.2 train no.189330  loss = 4.66892 avg_loss = 3.54016\n",
      "epoch no.2 train no.189340  loss = 2.74563 avg_loss = 3.52970\n",
      "epoch no.2 train no.189350  loss = 4.40809 avg_loss = 3.52276\n",
      "epoch no.2 train no.189360  loss = 4.60325 avg_loss = 3.53859\n",
      "epoch no.2 train no.189370  loss = 3.34721 avg_loss = 3.54751\n",
      "epoch no.2 train no.189380  loss = 4.10486 avg_loss = 3.54748\n",
      "epoch no.2 train no.189390  loss = 2.09032 avg_loss = 3.48018\n",
      "epoch no.2 train no.189400  loss = 2.46859 avg_loss = 3.48554\n",
      "epoch no.2 train no.189410  loss = 3.94964 avg_loss = 3.51866\n",
      "epoch no.2 train no.189420  loss = 3.68683 avg_loss = 3.51672\n",
      "epoch no.2 train no.189430  loss = 4.53559 avg_loss = 3.51501\n",
      "epoch no.2 train no.189440  loss = 3.11099 avg_loss = 3.46249\n",
      "epoch no.2 train no.189450  loss = 2.19407 avg_loss = 3.51030\n",
      "epoch no.2 train no.189460  loss = 3.35895 avg_loss = 3.47325\n",
      "epoch no.2 train no.189470  loss = 3.89912 avg_loss = 3.46073\n",
      "epoch no.2 train no.189480  loss = 6.66633 avg_loss = 3.50948\n",
      "epoch no.2 train no.189490  loss = 3.16554 avg_loss = 3.50776\n",
      "epoch no.2 train no.189500  loss = 3.10975 avg_loss = 3.47428\n",
      "epoch no.2 train no.189510  loss = 5.12409 avg_loss = 3.49582\n",
      "epoch no.2 train no.189520  loss = 4.52687 avg_loss = 3.52607\n",
      "epoch no.2 train no.189530  loss = 3.88533 avg_loss = 3.57222\n",
      "epoch no.2 train no.189540  loss = 3.62268 avg_loss = 3.57007\n",
      "epoch no.2 train no.189550  loss = 4.38262 avg_loss = 3.63192\n",
      "epoch no.2 train no.189560  loss = 3.54514 avg_loss = 3.65358\n",
      "epoch no.2 train no.189570  loss = 2.75929 avg_loss = 3.63890\n",
      "epoch no.2 train no.189580  loss = 2.45396 avg_loss = 3.61636\n",
      "epoch no.2 train no.189590  loss = 2.97221 avg_loss = 3.56918\n",
      "epoch no.2 train no.189600  loss = 3.54871 avg_loss = 3.53724\n",
      "epoch no.2 train no.189610  loss = 3.05824 avg_loss = 3.53078\n",
      "epoch no.2 train no.189620  loss = 4.28850 avg_loss = 3.50672\n",
      "epoch no.2 train no.189630  loss = 2.77589 avg_loss = 3.48735\n",
      "epoch no.2 train no.189640  loss = 3.89923 avg_loss = 3.47351\n",
      "epoch no.2 train no.189650  loss = 3.09918 avg_loss = 3.43160\n",
      "epoch no.2 train no.189660  loss = 2.46589 avg_loss = 3.41132\n",
      "epoch no.2 train no.189670  loss = 2.66140 avg_loss = 3.47660\n",
      "epoch no.2 train no.189680  loss = 3.61213 avg_loss = 3.48901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.189690  loss = 2.02292 avg_loss = 3.47095\n",
      "epoch no.2 train no.189700  loss = 2.88980 avg_loss = 3.47622\n",
      "epoch no.2 train no.189710  loss = 3.39983 avg_loss = 3.43753\n",
      "epoch no.2 train no.189720  loss = 2.60554 avg_loss = 3.41835\n",
      "epoch no.2 train no.189730  loss = 2.54858 avg_loss = 3.44404\n",
      "epoch no.2 train no.189740  loss = 2.41022 avg_loss = 3.46279\n",
      "epoch no.2 train no.189750  loss = 3.39376 avg_loss = 3.52692\n",
      "epoch no.2 train no.189760  loss = 2.76446 avg_loss = 3.51657\n",
      "epoch no.2 train no.189770  loss = 5.51140 avg_loss = 3.53329\n",
      "epoch no.2 train no.189780  loss = 4.40142 avg_loss = 3.53890\n",
      "epoch no.2 train no.189790  loss = 2.39080 avg_loss = 3.44240\n",
      "epoch no.2 train no.189800  loss = 3.46952 avg_loss = 3.43683\n",
      "epoch no.2 train no.189810  loss = 6.33558 avg_loss = 3.45638\n",
      "epoch no.2 train no.189820  loss = 3.72017 avg_loss = 3.45507\n",
      "epoch no.2 train no.189830  loss = 3.61382 avg_loss = 3.40696\n",
      "epoch no.2 train no.189840  loss = 1.79220 avg_loss = 3.34568\n",
      "epoch no.2 train no.189850  loss = 3.66108 avg_loss = 3.37313\n",
      "epoch no.2 train no.189860  loss = 2.57650 avg_loss = 3.41623\n",
      "epoch no.2 train no.189870  loss = 1.50143 avg_loss = 3.44063\n",
      "epoch no.2 train no.189880  loss = 3.67372 avg_loss = 3.50140\n",
      "epoch no.2 train no.189890  loss = 3.43048 avg_loss = 3.53757\n",
      "epoch no.2 train no.189900  loss = 2.83605 avg_loss = 3.55707\n",
      "epoch no.2 train no.189910  loss = 2.71009 avg_loss = 3.52768\n",
      "epoch no.2 train no.189920  loss = 3.89792 avg_loss = 3.52345\n",
      "epoch no.2 train no.189930  loss = 2.24308 avg_loss = 3.49806\n",
      "epoch no.2 train no.189940  loss = 2.23330 avg_loss = 3.47932\n",
      "epoch no.2 train no.189950  loss = 4.60765 avg_loss = 3.42746\n",
      "epoch no.2 train no.189960  loss = 5.72363 avg_loss = 3.41525\n",
      "epoch no.2 train no.189970  loss = 4.10556 avg_loss = 3.46302\n",
      "epoch no.2 train no.189980  loss = 2.55447 avg_loss = 3.41821\n",
      "epoch no.2 train no.189990  loss = 3.14998 avg_loss = 3.45680\n",
      "epoch no.2 train no.190000  loss = 4.93152 avg_loss = 3.46959\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.2 train no.190010  loss = 3.49983 avg_loss = 3.44078\n",
      "epoch no.2 train no.190020  loss = 3.28545 avg_loss = 3.43971\n",
      "epoch no.2 train no.190030  loss = 2.13558 avg_loss = 3.42266\n",
      "epoch no.2 train no.190040  loss = 3.02841 avg_loss = 3.41829\n",
      "epoch no.2 train no.190050  loss = 2.39895 avg_loss = 3.37469\n",
      "epoch no.2 train no.190060  loss = 3.58409 avg_loss = 3.41087\n",
      "epoch no.2 train no.190070  loss = 3.00445 avg_loss = 3.40308\n",
      "epoch no.2 train no.190080  loss = 3.24742 avg_loss = 3.39245\n",
      "epoch no.2 train no.190090  loss = 3.67385 avg_loss = 3.36971\n",
      "epoch no.2 train no.190100  loss = 4.06642 avg_loss = 3.49397\n",
      "epoch no.2 train no.190110  loss = 3.41603 avg_loss = 3.48000\n",
      "epoch no.2 train no.190120  loss = 3.41341 avg_loss = 3.46175\n",
      "epoch no.2 train no.190130  loss = 2.34510 avg_loss = 3.45138\n",
      "epoch no.2 train no.190140  loss = 4.49033 avg_loss = 3.47812\n",
      "epoch no.2 train no.190150  loss = 2.11022 avg_loss = 3.49157\n",
      "epoch no.2 train no.190160  loss = 4.49392 avg_loss = 3.53324\n",
      "epoch no.2 train no.190170  loss = 2.61696 avg_loss = 3.54197\n",
      "epoch no.2 train no.190180  loss = 3.09677 avg_loss = 3.51034\n",
      "epoch no.2 train no.190190  loss = 3.56660 avg_loss = 3.51684\n",
      "epoch no.2 train no.190200  loss = 4.28103 avg_loss = 3.50378\n",
      "epoch no.2 train no.190210  loss = 3.39005 avg_loss = 3.49761\n",
      "epoch no.2 train no.190220  loss = 3.01728 avg_loss = 3.49845\n",
      "epoch no.2 train no.190230  loss = 4.57764 avg_loss = 3.55489\n",
      "epoch no.2 train no.190240  loss = 6.79201 avg_loss = 3.62025\n",
      "epoch no.2 train no.190250  loss = 2.00344 avg_loss = 3.60926\n",
      "epoch no.2 train no.190260  loss = 2.92395 avg_loss = 3.66097\n",
      "epoch no.2 train no.190270  loss = 3.48303 avg_loss = 3.62533\n",
      "epoch no.2 train no.190280  loss = 2.37179 avg_loss = 3.62993\n",
      "epoch no.2 train no.190290  loss = 4.28490 avg_loss = 3.65775\n",
      "epoch no.2 train no.190300  loss = 2.52811 avg_loss = 3.61149\n",
      "epoch no.2 train no.190310  loss = 3.40790 avg_loss = 3.57729\n",
      "epoch no.2 train no.190320  loss = 3.89072 avg_loss = 3.55682\n",
      "epoch no.2 train no.190330  loss = 1.71601 avg_loss = 3.52212\n",
      "epoch no.2 train no.190340  loss = 1.66478 avg_loss = 3.50871\n",
      "epoch no.2 train no.190350  loss = 3.54443 avg_loss = 3.56365\n",
      "epoch no.2 train no.190360  loss = 3.91601 avg_loss = 3.56694\n",
      "epoch no.2 train no.190370  loss = 2.57392 avg_loss = 3.53772\n",
      "epoch no.2 train no.190380  loss = 4.68061 avg_loss = 3.55588\n",
      "epoch no.2 train no.190390  loss = 3.66654 avg_loss = 3.53560\n",
      "epoch no.2 train no.190400  loss = 3.09847 avg_loss = 3.48396\n",
      "epoch no.2 train no.190410  loss = 3.73136 avg_loss = 3.46402\n",
      "epoch no.2 train no.190420  loss = 3.78065 avg_loss = 3.49325\n",
      "epoch no.2 train no.190430  loss = 3.47156 avg_loss = 3.54590\n",
      "epoch no.2 train no.190440  loss = 4.02063 avg_loss = 3.55940\n",
      "epoch no.2 train no.190450  loss = 2.45210 avg_loss = 3.52065\n",
      "epoch no.2 train no.190460  loss = 2.28487 avg_loss = 3.53375\n",
      "epoch no.2 train no.190470  loss = 3.99421 avg_loss = 3.56617\n",
      "epoch no.2 train no.190480  loss = 4.03770 avg_loss = 3.54529\n",
      "epoch no.2 train no.190490  loss = 3.96158 avg_loss = 3.56901\n",
      "epoch no.2 train no.190500  loss = 4.27367 avg_loss = 3.56511\n",
      "epoch no.2 train no.190510  loss = 5.00469 avg_loss = 3.56792\n",
      "epoch no.2 train no.190520  loss = 3.91154 avg_loss = 3.56552\n",
      "epoch no.2 train no.190530  loss = 4.47355 avg_loss = 3.61867\n",
      "epoch no.2 train no.190540  loss = 2.23113 avg_loss = 3.60691\n",
      "epoch no.2 train no.190550  loss = 4.65505 avg_loss = 3.58705\n",
      "epoch no.2 train no.190560  loss = 3.64370 avg_loss = 3.57342\n",
      "epoch no.2 train no.190570  loss = 2.54936 avg_loss = 3.54157\n",
      "epoch no.2 train no.190580  loss = 3.81751 avg_loss = 3.54562\n",
      "epoch no.2 train no.190590  loss = 2.22374 avg_loss = 3.47625\n",
      "epoch no.2 train no.190600  loss = 3.68944 avg_loss = 3.49018\n",
      "epoch no.2 train no.190610  loss = 3.61035 avg_loss = 3.46188\n",
      "epoch no.2 train no.190620  loss = 3.90066 avg_loss = 3.45857\n",
      "epoch no.2 train no.190630  loss = 3.22131 avg_loss = 3.44347\n",
      "epoch no.2 train no.190640  loss = 4.65449 avg_loss = 3.50929\n",
      "epoch no.2 train no.190650  loss = 1.89008 avg_loss = 3.48471\n",
      "epoch no.2 train no.190660  loss = 6.59679 avg_loss = 3.50840\n",
      "epoch no.2 train no.190670  loss = 3.82081 avg_loss = 3.52312\n",
      "epoch no.2 train no.190680  loss = 1.90937 avg_loss = 3.53963\n",
      "epoch no.2 train no.190690  loss = 3.09128 avg_loss = 3.55421\n",
      "epoch no.2 train no.190700  loss = 2.65038 avg_loss = 3.58209\n",
      "epoch no.2 train no.190710  loss = 2.84778 avg_loss = 3.53138\n",
      "epoch no.2 train no.190720  loss = 3.01334 avg_loss = 3.50914\n",
      "epoch no.2 train no.190730  loss = 3.08673 avg_loss = 3.54925\n",
      "epoch no.2 train no.190740  loss = 4.69507 avg_loss = 3.51500\n",
      "epoch no.2 train no.190750  loss = 3.66820 avg_loss = 3.47848\n",
      "epoch no.2 train no.190760  loss = 3.05546 avg_loss = 3.49085\n",
      "epoch no.2 train no.190770  loss = 3.46192 avg_loss = 3.49161\n",
      "epoch no.2 train no.190780  loss = 2.19542 avg_loss = 3.48421\n",
      "epoch no.2 train no.190790  loss = 2.26968 avg_loss = 3.47276\n",
      "epoch no.2 train no.190800  loss = 3.87315 avg_loss = 3.51330\n",
      "epoch no.2 train no.190810  loss = 4.77300 avg_loss = 3.52545\n",
      "epoch no.2 train no.190820  loss = 4.50780 avg_loss = 3.55272\n",
      "epoch no.2 train no.190830  loss = 2.28768 avg_loss = 3.57007\n",
      "epoch no.2 train no.190840  loss = 2.70629 avg_loss = 3.53276\n",
      "epoch no.2 train no.190850  loss = 3.83043 avg_loss = 3.50224\n",
      "epoch no.2 train no.190860  loss = 5.83883 avg_loss = 3.53349\n",
      "epoch no.2 train no.190870  loss = 3.18088 avg_loss = 3.53782\n",
      "epoch no.2 train no.190880  loss = 4.38612 avg_loss = 3.55996\n",
      "epoch no.2 train no.190890  loss = 5.12941 avg_loss = 3.51967\n",
      "epoch no.2 train no.190900  loss = 3.91379 avg_loss = 3.53397\n",
      "epoch no.2 train no.190910  loss = 5.79565 avg_loss = 3.51865\n",
      "epoch no.2 train no.190920  loss = 3.41962 avg_loss = 3.51930\n",
      "epoch no.2 train no.190930  loss = 2.06951 avg_loss = 3.47786\n",
      "epoch no.2 train no.190940  loss = 3.65710 avg_loss = 3.46443\n",
      "epoch no.2 train no.190950  loss = 2.33063 avg_loss = 3.45482\n",
      "epoch no.2 train no.190960  loss = 2.87544 avg_loss = 3.48496\n",
      "epoch no.2 train no.190970  loss = 2.91015 avg_loss = 3.47051\n",
      "epoch no.2 train no.190980  loss = 5.50439 avg_loss = 3.49967\n",
      "epoch no.2 train no.190990  loss = 3.71992 avg_loss = 3.47640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.191000  loss = 4.62100 avg_loss = 3.47528\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.2 train no.191010  loss = 2.99895 avg_loss = 3.43878\n",
      "epoch no.2 train no.191020  loss = 3.41955 avg_loss = 3.42836\n",
      "epoch no.2 train no.191030  loss = 3.27353 avg_loss = 3.40420\n",
      "epoch no.2 train no.191040  loss = 2.39787 avg_loss = 3.38561\n",
      "epoch no.2 train no.191050  loss = 4.06578 avg_loss = 3.43577\n",
      "epoch no.2 train no.191060  loss = 4.43660 avg_loss = 3.43713\n",
      "epoch no.2 train no.191070  loss = 4.07971 avg_loss = 3.45834\n",
      "epoch no.2 train no.191080  loss = 4.07072 avg_loss = 3.44981\n",
      "epoch no.2 train no.191090  loss = 2.63870 avg_loss = 3.47887\n",
      "epoch no.2 train no.191100  loss = 4.15054 avg_loss = 3.47528\n",
      "epoch no.2 train no.191110  loss = 3.66306 avg_loss = 3.51190\n",
      "epoch no.2 train no.191120  loss = 5.90771 avg_loss = 3.48361\n",
      "epoch no.2 train no.191130  loss = 4.06151 avg_loss = 3.47269\n",
      "epoch no.2 train no.191140  loss = 4.43952 avg_loss = 3.51293\n",
      "epoch no.2 train no.191150  loss = 5.43902 avg_loss = 3.54090\n",
      "epoch no.2 train no.191160  loss = 3.21095 avg_loss = 3.54331\n",
      "epoch no.2 train no.191170  loss = 1.95428 avg_loss = 3.48958\n",
      "epoch no.2 train no.191180  loss = 4.50610 avg_loss = 3.52403\n",
      "epoch no.2 train no.191190  loss = 3.05569 avg_loss = 3.49793\n",
      "epoch no.2 train no.191200  loss = 3.58728 avg_loss = 3.48237\n",
      "epoch no.2 train no.191210  loss = 3.09966 avg_loss = 3.46936\n",
      "epoch no.2 train no.191220  loss = 3.50494 avg_loss = 3.46354\n",
      "epoch no.2 train no.191230  loss = 2.22712 avg_loss = 3.45533\n",
      "epoch no.2 train no.191240  loss = 2.96343 avg_loss = 3.43274\n",
      "epoch no.2 train no.191250  loss = 3.33592 avg_loss = 3.40172\n",
      "epoch no.2 train no.191260  loss = 5.04225 avg_loss = 3.45363\n",
      "epoch no.2 train no.191270  loss = 4.38234 avg_loss = 3.46416\n",
      "epoch no.2 train no.191280  loss = 2.77427 avg_loss = 3.45996\n",
      "epoch no.2 train no.191290  loss = 2.68051 avg_loss = 3.48930\n",
      "epoch no.2 train no.191300  loss = 4.63571 avg_loss = 3.53537\n",
      "epoch no.2 train no.191310  loss = 3.06395 avg_loss = 3.47782\n",
      "epoch no.2 train no.191320  loss = 2.71354 avg_loss = 3.48582\n",
      "epoch no.2 train no.191330  loss = 5.54598 avg_loss = 3.50765\n",
      "epoch no.2 train no.191340  loss = 4.65517 avg_loss = 3.52599\n",
      "epoch no.2 train no.191350  loss = 3.37294 avg_loss = 3.51647\n",
      "epoch no.2 train no.191360  loss = 3.91446 avg_loss = 3.55854\n",
      "epoch no.2 train no.191370  loss = 2.48635 avg_loss = 3.50928\n",
      "epoch no.2 train no.191380  loss = 2.23487 avg_loss = 3.47174\n",
      "epoch no.2 train no.191390  loss = 2.25009 avg_loss = 3.45069\n",
      "epoch no.2 train no.191400  loss = 5.19447 avg_loss = 3.49970\n",
      "epoch no.2 train no.191410  loss = 4.33079 avg_loss = 3.50715\n",
      "epoch no.2 train no.191420  loss = 3.37706 avg_loss = 3.52849\n",
      "epoch no.2 train no.191430  loss = 3.27123 avg_loss = 3.50795\n",
      "epoch no.2 train no.191440  loss = 3.43473 avg_loss = 3.49318\n",
      "epoch no.2 train no.191450  loss = 3.56947 avg_loss = 3.51182\n",
      "epoch no.2 train no.191460  loss = 1.32753 avg_loss = 3.47542\n",
      "epoch no.2 train no.191470  loss = 3.52323 avg_loss = 3.45206\n",
      "epoch no.2 train no.191480  loss = 3.66405 avg_loss = 3.44219\n",
      "epoch no.2 train no.191490  loss = 2.92617 avg_loss = 3.45004\n",
      "epoch no.2 train no.191500  loss = 2.51062 avg_loss = 3.44081\n",
      "epoch no.2 train no.191510  loss = 4.36605 avg_loss = 3.44363\n",
      "epoch no.2 train no.191520  loss = 2.98115 avg_loss = 3.42126\n",
      "epoch no.2 train no.191530  loss = 5.00565 avg_loss = 3.45782\n",
      "epoch no.2 train no.191540  loss = 3.58718 avg_loss = 3.48034\n",
      "epoch no.2 train no.191550  loss = 2.78525 avg_loss = 3.48066\n",
      "epoch no.2 train no.191560  loss = 2.24002 avg_loss = 3.44435\n",
      "epoch no.2 train no.191570  loss = 2.58668 avg_loss = 3.40608\n",
      "epoch no.2 train no.191580  loss = 4.24163 avg_loss = 3.48583\n",
      "epoch no.2 train no.191590  loss = 3.17659 avg_loss = 3.45911\n",
      "epoch no.2 train no.191600  loss = 2.63980 avg_loss = 3.44524\n",
      "epoch no.2 train no.191610  loss = 3.00298 avg_loss = 3.40923\n",
      "epoch no.2 train no.191620  loss = 3.18946 avg_loss = 3.40966\n",
      "epoch no.2 train no.191630  loss = 3.29573 avg_loss = 3.37929\n",
      "epoch no.2 train no.191640  loss = 2.85255 avg_loss = 3.35611\n",
      "epoch no.2 train no.191650  loss = 4.48470 avg_loss = 3.36847\n",
      "epoch no.2 train no.191660  loss = 2.83234 avg_loss = 3.37085\n",
      "epoch no.2 train no.191670  loss = 3.80519 avg_loss = 3.36656\n",
      "epoch no.2 train no.191680  loss = 2.73317 avg_loss = 3.38139\n",
      "epoch no.2 train no.191690  loss = 1.76646 avg_loss = 3.37903\n",
      "epoch no.2 train no.191700  loss = 2.78369 avg_loss = 3.43056\n",
      "epoch no.2 train no.191710  loss = 2.13589 avg_loss = 3.35790\n",
      "epoch no.2 train no.191720  loss = 2.15210 avg_loss = 3.33956\n",
      "epoch no.2 train no.191730  loss = 3.91549 avg_loss = 3.35731\n",
      "epoch no.2 train no.191740  loss = 3.54421 avg_loss = 3.36520\n",
      "epoch no.2 train no.191750  loss = 3.99842 avg_loss = 3.37280\n",
      "epoch no.2 train no.191760  loss = 3.59493 avg_loss = 3.35689\n",
      "epoch no.2 train no.191770  loss = 4.55598 avg_loss = 3.32361\n",
      "epoch no.2 train no.191780  loss = 3.28525 avg_loss = 3.31991\n",
      "epoch no.2 train no.191790  loss = 4.79186 avg_loss = 3.38831\n",
      "epoch no.2 train no.191800  loss = 6.24662 avg_loss = 3.43599\n",
      "epoch no.2 train no.191810  loss = 4.72355 avg_loss = 3.44554\n",
      "epoch no.2 train no.191820  loss = 4.32614 avg_loss = 3.47199\n",
      "epoch no.2 train no.191830  loss = 2.92395 avg_loss = 3.46685\n",
      "epoch no.2 train no.191840  loss = 4.25409 avg_loss = 3.46407\n",
      "epoch no.2 train no.191850  loss = 3.51439 avg_loss = 3.47493\n",
      "epoch no.2 train no.191860  loss = 3.95156 avg_loss = 3.49633\n",
      "epoch no.2 train no.191870  loss = 2.37718 avg_loss = 3.50604\n",
      "epoch no.2 train no.191880  loss = 2.98595 avg_loss = 3.52462\n",
      "epoch no.2 train no.191890  loss = 3.32481 avg_loss = 3.55334\n",
      "epoch no.2 train no.191900  loss = 5.52153 avg_loss = 3.58334\n",
      "epoch no.2 train no.191910  loss = 3.27988 avg_loss = 3.55694\n",
      "epoch no.2 train no.191920  loss = 2.26540 avg_loss = 3.59169\n",
      "epoch no.2 train no.191930  loss = 2.65484 avg_loss = 3.61550\n",
      "epoch no.2 train no.191940  loss = 3.85072 avg_loss = 3.59782\n",
      "epoch no.2 train no.191950  loss = 3.28371 avg_loss = 3.60757\n",
      "epoch no.2 train no.191960  loss = 4.73600 avg_loss = 3.66330\n",
      "epoch no.2 train no.191970  loss = 3.03564 avg_loss = 3.65484\n",
      "epoch no.2 train no.191980  loss = 2.41207 avg_loss = 3.71433\n",
      "epoch no.2 train no.191990  loss = 3.72475 avg_loss = 3.70804\n",
      "epoch no.2 train no.192000  loss = 3.71006 avg_loss = 3.73906\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.192010  loss = 4.39973 avg_loss = 3.70302\n",
      "epoch no.2 train no.192020  loss = 3.48661 avg_loss = 3.68120\n",
      "epoch no.2 train no.192030  loss = 5.05768 avg_loss = 3.66888\n",
      "epoch no.2 train no.192040  loss = 4.07396 avg_loss = 3.66417\n",
      "epoch no.2 train no.192050  loss = 3.29158 avg_loss = 3.63593\n",
      "epoch no.2 train no.192060  loss = 2.66626 avg_loss = 3.64721\n",
      "epoch no.2 train no.192070  loss = 4.13251 avg_loss = 3.64613\n",
      "epoch no.2 train no.192080  loss = 3.37804 avg_loss = 3.61126\n",
      "epoch no.2 train no.192090  loss = 5.06588 avg_loss = 3.63521\n",
      "epoch no.2 train no.192100  loss = 3.73002 avg_loss = 3.60544\n",
      "epoch no.2 train no.192110  loss = 3.21844 avg_loss = 3.61577\n",
      "epoch no.2 train no.192120  loss = 4.29989 avg_loss = 3.57928\n",
      "epoch no.2 train no.192130  loss = 2.93872 avg_loss = 3.53533\n",
      "epoch no.2 train no.192140  loss = 2.71060 avg_loss = 3.52847\n",
      "epoch no.2 train no.192150  loss = 5.34395 avg_loss = 3.60683\n",
      "epoch no.2 train no.192160  loss = 2.28520 avg_loss = 3.55185\n",
      "epoch no.2 train no.192170  loss = 4.70719 avg_loss = 3.53844\n",
      "epoch no.2 train no.192180  loss = 3.69751 avg_loss = 3.51478\n",
      "epoch no.2 train no.192190  loss = 3.24481 avg_loss = 3.50945\n",
      "epoch no.2 train no.192200  loss = 4.63633 avg_loss = 3.54037\n",
      "epoch no.2 train no.192210  loss = 3.90563 avg_loss = 3.49880\n",
      "epoch no.2 train no.192220  loss = 3.90084 avg_loss = 3.45364\n",
      "epoch no.2 train no.192230  loss = 2.75143 avg_loss = 3.40994\n",
      "epoch no.2 train no.192240  loss = 2.21668 avg_loss = 3.38538\n",
      "epoch no.2 train no.192250  loss = 4.14467 avg_loss = 3.38672\n",
      "epoch no.2 train no.192260  loss = 4.47027 avg_loss = 3.37834\n",
      "epoch no.2 train no.192270  loss = 4.48730 avg_loss = 3.35442\n",
      "epoch no.2 train no.192280  loss = 4.62076 avg_loss = 3.39432\n",
      "epoch no.2 train no.192290  loss = 5.00344 avg_loss = 3.40970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.192300  loss = 1.68599 avg_loss = 3.41917\n",
      "epoch no.2 train no.192310  loss = 3.38597 avg_loss = 3.40298\n",
      "epoch no.2 train no.192320  loss = 2.87801 avg_loss = 3.42309\n",
      "epoch no.2 train no.192330  loss = 3.29273 avg_loss = 3.50366\n",
      "epoch no.2 train no.192340  loss = 3.99191 avg_loss = 3.55157\n",
      "epoch no.2 train no.192350  loss = 2.13424 avg_loss = 3.54399\n",
      "epoch no.2 train no.192360  loss = 4.49488 avg_loss = 3.55705\n",
      "epoch no.2 train no.192370  loss = 2.41229 avg_loss = 3.53280\n",
      "epoch no.2 train no.192380  loss = 2.71428 avg_loss = 3.56174\n",
      "epoch no.2 train no.192390  loss = 3.35639 avg_loss = 3.60151\n",
      "epoch no.2 train no.192400  loss = 1.85224 avg_loss = 3.61244\n",
      "epoch no.2 train no.192410  loss = 2.67657 avg_loss = 3.53656\n",
      "epoch no.2 train no.192420  loss = 3.00462 avg_loss = 3.49885\n",
      "epoch no.2 train no.192430  loss = 1.79701 avg_loss = 3.50823\n",
      "epoch no.2 train no.192440  loss = 4.09948 avg_loss = 3.44137\n",
      "epoch no.2 train no.192450  loss = 2.01559 avg_loss = 3.42741\n",
      "epoch no.2 train no.192460  loss = 1.50339 avg_loss = 3.40576\n",
      "epoch no.2 train no.192470  loss = 4.54809 avg_loss = 3.39114\n",
      "epoch no.2 train no.192480  loss = 4.31371 avg_loss = 3.38401\n",
      "epoch no.2 train no.192490  loss = 2.55199 avg_loss = 3.32172\n",
      "epoch no.2 train no.192500  loss = 1.66867 avg_loss = 3.34039\n",
      "epoch no.2 train no.192510  loss = 4.57933 avg_loss = 3.40129\n",
      "epoch no.2 train no.192520  loss = 1.68367 avg_loss = 3.36862\n",
      "epoch no.2 train no.192530  loss = 3.64163 avg_loss = 3.34092\n",
      "epoch no.2 train no.192540  loss = 3.37919 avg_loss = 3.31207\n",
      "epoch no.2 train no.192550  loss = 2.49920 avg_loss = 3.34602\n",
      "epoch no.2 train no.192560  loss = 4.41626 avg_loss = 3.38933\n",
      "epoch no.2 train no.192570  loss = 2.66928 avg_loss = 3.35656\n",
      "epoch no.2 train no.192580  loss = 4.30830 avg_loss = 3.37655\n",
      "epoch no.2 train no.192590  loss = 3.16115 avg_loss = 3.41722\n",
      "epoch no.2 train no.192600  loss = 4.08749 avg_loss = 3.45131\n",
      "epoch no.2 train no.192610  loss = 4.25344 avg_loss = 3.45625\n",
      "epoch no.2 train no.192620  loss = 2.73269 avg_loss = 3.49317\n",
      "epoch no.2 train no.192630  loss = 3.08157 avg_loss = 3.46327\n",
      "epoch no.2 train no.192640  loss = 5.32675 avg_loss = 3.53095\n",
      "epoch no.2 train no.192650  loss = 3.78727 avg_loss = 3.57382\n",
      "epoch no.2 train no.192660  loss = 2.84174 avg_loss = 3.57071\n",
      "epoch no.2 train no.192670  loss = 3.53071 avg_loss = 3.54226\n",
      "epoch no.2 train no.192680  loss = 2.88548 avg_loss = 3.48575\n",
      "epoch no.2 train no.192690  loss = 2.34148 avg_loss = 3.48931\n",
      "epoch no.2 train no.192700  loss = 3.35297 avg_loss = 3.46462\n",
      "epoch no.2 train no.192710  loss = 2.47182 avg_loss = 3.45659\n",
      "epoch no.2 train no.192720  loss = 2.52654 avg_loss = 3.41762\n",
      "epoch no.2 train no.192730  loss = 3.99134 avg_loss = 3.46398\n",
      "epoch no.2 train no.192740  loss = 4.37007 avg_loss = 3.52078\n",
      "epoch no.2 train no.192750  loss = 2.77159 avg_loss = 3.47621\n",
      "epoch no.2 train no.192760  loss = 3.17698 avg_loss = 3.49415\n",
      "epoch no.2 train no.192770  loss = 2.64187 avg_loss = 3.48289\n",
      "epoch no.2 train no.192780  loss = 3.60375 avg_loss = 3.50577\n",
      "epoch no.2 train no.192790  loss = 2.21085 avg_loss = 3.48637\n",
      "epoch no.2 train no.192800  loss = 2.64050 avg_loss = 3.46301\n",
      "epoch no.2 train no.192810  loss = 3.51676 avg_loss = 3.46049\n",
      "epoch no.2 train no.192820  loss = 2.62533 avg_loss = 3.44735\n",
      "epoch no.2 train no.192830  loss = 3.84616 avg_loss = 3.46883\n",
      "epoch no.2 train no.192840  loss = 2.52222 avg_loss = 3.45812\n",
      "epoch no.2 train no.192850  loss = 3.47305 avg_loss = 3.40369\n",
      "epoch no.2 train no.192860  loss = 2.76720 avg_loss = 3.37425\n",
      "epoch no.2 train no.192870  loss = 2.24824 avg_loss = 3.43103\n",
      "epoch no.2 train no.192880  loss = 2.56244 avg_loss = 3.46889\n",
      "epoch no.2 train no.192890  loss = 4.10378 avg_loss = 3.46128\n",
      "epoch no.2 train no.192900  loss = 2.02856 avg_loss = 3.47038\n",
      "epoch no.2 train no.192910  loss = 2.94895 avg_loss = 3.44888\n",
      "epoch no.2 train no.192920  loss = 3.67674 avg_loss = 3.45584\n",
      "epoch no.2 train no.192930  loss = 2.71290 avg_loss = 3.42903\n",
      "epoch no.2 train no.192940  loss = 4.30850 avg_loss = 3.45635\n",
      "epoch no.2 train no.192950  loss = 4.28946 avg_loss = 3.45003\n",
      "epoch no.2 train no.192960  loss = 2.79664 avg_loss = 3.44054\n",
      "epoch no.2 train no.192970  loss = 2.67663 avg_loss = 3.41898\n",
      "epoch no.2 train no.192980  loss = 3.29902 avg_loss = 3.42025\n",
      "epoch no.2 train no.192990  loss = 3.71665 avg_loss = 3.45144\n",
      "epoch no.2 train no.193000  loss = 3.64028 avg_loss = 3.46125\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '곡', '들', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.2 train no.193010  loss = 2.91599 avg_loss = 3.50576\n",
      "epoch no.2 train no.193020  loss = 3.18157 avg_loss = 3.50815\n",
      "epoch no.2 train no.193030  loss = 1.99622 avg_loss = 3.45676\n",
      "epoch no.2 train no.193040  loss = 1.80365 avg_loss = 3.41324\n",
      "epoch no.2 train no.193050  loss = 3.57899 avg_loss = 3.40688\n",
      "epoch no.2 train no.193060  loss = 4.35081 avg_loss = 3.40108\n",
      "epoch no.2 train no.193070  loss = 3.67974 avg_loss = 3.37726\n",
      "epoch no.2 train no.193080  loss = 7.08630 avg_loss = 3.42264\n",
      "epoch no.2 train no.193090  loss = 4.68611 avg_loss = 3.42956\n",
      "epoch no.2 train no.193100  loss = 2.54238 avg_loss = 3.42852\n",
      "epoch no.2 train no.193110  loss = 6.97866 avg_loss = 3.44324\n",
      "epoch no.2 train no.193120  loss = 3.88265 avg_loss = 3.44856\n",
      "epoch no.2 train no.193130  loss = 3.02358 avg_loss = 3.43132\n",
      "epoch no.2 train no.193140  loss = 2.81407 avg_loss = 3.45243\n",
      "epoch no.2 train no.193150  loss = 3.32838 avg_loss = 3.48776\n",
      "epoch no.2 train no.193160  loss = 3.00615 avg_loss = 3.49390\n",
      "epoch no.2 train no.193170  loss = 3.02019 avg_loss = 3.49717\n",
      "epoch no.2 train no.193180  loss = 2.23282 avg_loss = 3.55283\n",
      "epoch no.2 train no.193190  loss = 2.31878 avg_loss = 3.57368\n",
      "epoch no.2 train no.193200  loss = 3.17375 avg_loss = 3.54549\n",
      "epoch no.2 train no.193210  loss = 4.05007 avg_loss = 3.56795\n",
      "epoch no.2 train no.193220  loss = 5.14620 avg_loss = 3.57708\n",
      "epoch no.2 train no.193230  loss = 4.67592 avg_loss = 3.61066\n",
      "epoch no.2 train no.193240  loss = 1.98744 avg_loss = 3.60256\n",
      "epoch no.2 train no.193250  loss = 3.12208 avg_loss = 3.55982\n",
      "epoch no.2 train no.193260  loss = 4.06532 avg_loss = 3.59323\n",
      "epoch no.2 train no.193270  loss = 3.27578 avg_loss = 3.61169\n",
      "epoch no.2 train no.193280  loss = 5.51209 avg_loss = 3.65131\n",
      "epoch no.2 train no.193290  loss = 3.69802 avg_loss = 3.61480\n",
      "epoch no.2 train no.193300  loss = 2.54126 avg_loss = 3.58403\n",
      "epoch no.2 train no.193310  loss = 3.63480 avg_loss = 3.58661\n",
      "epoch no.2 train no.193320  loss = 2.46603 avg_loss = 3.58700\n",
      "epoch no.2 train no.193330  loss = 3.54211 avg_loss = 3.55247\n",
      "epoch no.2 train no.193340  loss = 2.66858 avg_loss = 3.53891\n",
      "epoch no.2 train no.193350  loss = 2.19559 avg_loss = 3.53249\n",
      "epoch no.2 train no.193360  loss = 3.25519 avg_loss = 3.52088\n",
      "epoch no.2 train no.193370  loss = 2.65683 avg_loss = 3.53750\n",
      "epoch no.2 train no.193380  loss = 2.19696 avg_loss = 3.47407\n",
      "epoch no.2 train no.193390  loss = 2.91154 avg_loss = 3.47293\n",
      "epoch no.2 train no.193400  loss = 4.45811 avg_loss = 3.48740\n",
      "epoch no.2 train no.193410  loss = 3.81775 avg_loss = 3.47769\n",
      "epoch no.2 train no.193420  loss = 4.20942 avg_loss = 3.44426\n",
      "epoch no.2 train no.193430  loss = 3.82588 avg_loss = 3.45468\n",
      "epoch no.2 train no.193440  loss = 5.73564 avg_loss = 3.43689\n",
      "epoch no.2 train no.193450  loss = 1.86077 avg_loss = 3.44795\n",
      "epoch no.2 train no.193460  loss = 4.10447 avg_loss = 3.46922\n",
      "epoch no.2 train no.193470  loss = 3.70472 avg_loss = 3.49194\n",
      "epoch no.2 train no.193480  loss = 3.26052 avg_loss = 3.53141\n",
      "epoch no.2 train no.193490  loss = 4.36724 avg_loss = 3.56319\n",
      "epoch no.2 train no.193500  loss = 3.81085 avg_loss = 3.57910\n",
      "epoch no.2 train no.193510  loss = 3.15132 avg_loss = 3.58346\n",
      "epoch no.2 train no.193520  loss = 3.29767 avg_loss = 3.56143\n",
      "epoch no.2 train no.193530  loss = 4.08093 avg_loss = 3.56000\n",
      "epoch no.2 train no.193540  loss = 2.23840 avg_loss = 3.52887\n",
      "epoch no.2 train no.193550  loss = 1.90834 avg_loss = 3.49191\n",
      "epoch no.2 train no.193560  loss = 2.64497 avg_loss = 3.43781\n",
      "epoch no.2 train no.193570  loss = 3.10253 avg_loss = 3.42660\n",
      "epoch no.2 train no.193580  loss = 2.74959 avg_loss = 3.39685\n",
      "epoch no.2 train no.193590  loss = 2.91207 avg_loss = 3.38309\n",
      "epoch no.2 train no.193600  loss = 3.45549 avg_loss = 3.39271\n",
      "epoch no.2 train no.193610  loss = 3.13370 avg_loss = 3.48030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.193620  loss = 2.66410 avg_loss = 3.45018\n",
      "epoch no.2 train no.193630  loss = 2.19818 avg_loss = 3.42619\n",
      "epoch no.2 train no.193640  loss = 3.51494 avg_loss = 3.39151\n",
      "epoch no.2 train no.193650  loss = 6.00095 avg_loss = 3.41888\n",
      "epoch no.2 train no.193660  loss = 5.07787 avg_loss = 3.37375\n",
      "epoch no.2 train no.193670  loss = 4.63195 avg_loss = 3.37791\n",
      "epoch no.2 train no.193680  loss = 3.20865 avg_loss = 3.39518\n",
      "epoch no.2 train no.193690  loss = 7.27233 avg_loss = 3.45855\n",
      "epoch no.2 train no.193700  loss = 7.66813 avg_loss = 3.49920\n",
      "epoch no.2 train no.193710  loss = 4.71505 avg_loss = 3.47653\n",
      "epoch no.2 train no.193720  loss = 2.60242 avg_loss = 3.50606\n",
      "epoch no.2 train no.193730  loss = 7.30674 avg_loss = 3.47615\n",
      "epoch no.2 train no.193740  loss = 3.34005 avg_loss = 3.43082\n",
      "epoch no.2 train no.193750  loss = 3.81344 avg_loss = 3.44079\n",
      "epoch no.2 train no.193760  loss = 3.88828 avg_loss = 3.39957\n",
      "epoch no.2 train no.193770  loss = 2.19849 avg_loss = 3.39874\n",
      "epoch no.2 train no.193780  loss = 2.84497 avg_loss = 3.37091\n",
      "epoch no.2 train no.193790  loss = 3.70888 avg_loss = 3.35954\n",
      "epoch no.2 train no.193800  loss = 2.42203 avg_loss = 3.36772\n",
      "epoch no.2 train no.193810  loss = 1.97712 avg_loss = 3.37450\n",
      "epoch no.2 train no.193820  loss = 2.96391 avg_loss = 3.39742\n",
      "epoch no.2 train no.193830  loss = 2.57630 avg_loss = 3.38247\n",
      "epoch no.2 train no.193840  loss = 3.14956 avg_loss = 3.32871\n",
      "epoch no.2 train no.193850  loss = 2.36770 avg_loss = 3.35591\n",
      "epoch no.2 train no.193860  loss = 3.95614 avg_loss = 3.33605\n",
      "epoch no.2 train no.193870  loss = 2.31106 avg_loss = 3.31748\n",
      "epoch no.2 train no.193880  loss = 3.94213 avg_loss = 3.33118\n",
      "epoch no.2 train no.193890  loss = 4.08730 avg_loss = 3.30249\n",
      "epoch no.2 train no.193900  loss = 2.59331 avg_loss = 3.31626\n",
      "epoch no.2 train no.193910  loss = 3.31363 avg_loss = 3.33141\n",
      "epoch no.2 train no.193920  loss = 5.47272 avg_loss = 3.35031\n",
      "epoch no.2 train no.193930  loss = 3.45101 avg_loss = 3.38882\n",
      "epoch no.2 train no.193940  loss = 4.15889 avg_loss = 3.44073\n",
      "epoch no.2 train no.193950  loss = 3.86350 avg_loss = 3.44147\n",
      "epoch no.2 train no.193960  loss = 3.99343 avg_loss = 3.50408\n",
      "epoch no.2 train no.193970  loss = 2.33191 avg_loss = 3.49047\n",
      "epoch no.2 train no.193980  loss = 4.05587 avg_loss = 3.49523\n",
      "epoch no.2 train no.193990  loss = 3.91645 avg_loss = 3.49455\n",
      "epoch no.2 train no.194000  loss = 4.37676 avg_loss = 3.45874\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.2 train no.194010  loss = 3.52946 avg_loss = 3.42724\n",
      "epoch no.2 train no.194020  loss = 2.45662 avg_loss = 3.45270\n",
      "epoch no.2 train no.194030  loss = 3.28926 avg_loss = 3.46722\n",
      "epoch no.2 train no.194040  loss = 4.23699 avg_loss = 3.43315\n",
      "epoch no.2 train no.194050  loss = 4.06056 avg_loss = 3.49917\n",
      "epoch no.2 train no.194060  loss = 3.70897 avg_loss = 3.44188\n",
      "epoch no.2 train no.194070  loss = 3.47663 avg_loss = 3.45293\n",
      "epoch no.2 train no.194080  loss = 4.11674 avg_loss = 3.41536\n",
      "epoch no.2 train no.194090  loss = 4.08058 avg_loss = 3.39177\n",
      "epoch no.2 train no.194100  loss = 2.83732 avg_loss = 3.41675\n",
      "epoch no.2 train no.194110  loss = 2.76863 avg_loss = 3.38924\n",
      "epoch no.2 train no.194120  loss = 4.11665 avg_loss = 3.40229\n",
      "epoch no.2 train no.194130  loss = 6.25622 avg_loss = 3.45970\n",
      "epoch no.2 train no.194140  loss = 5.72419 avg_loss = 3.52648\n",
      "epoch no.2 train no.194150  loss = 2.28284 avg_loss = 3.52597\n",
      "epoch no.2 train no.194160  loss = 1.80399 avg_loss = 3.48111\n",
      "epoch no.2 train no.194170  loss = 3.23330 avg_loss = 3.45624\n",
      "epoch no.2 train no.194180  loss = 3.81644 avg_loss = 3.47544\n",
      "epoch no.2 train no.194190  loss = 3.43226 avg_loss = 3.48949\n",
      "epoch no.2 train no.194200  loss = 4.48622 avg_loss = 3.44043\n",
      "epoch no.2 train no.194210  loss = 2.35142 avg_loss = 3.44761\n",
      "epoch no.2 train no.194220  loss = 5.95850 avg_loss = 3.47117\n",
      "epoch no.2 train no.194230  loss = 2.82798 avg_loss = 3.46391\n",
      "epoch no.2 train no.194240  loss = 3.80652 avg_loss = 3.49315\n",
      "epoch no.2 train no.194250  loss = 1.90175 avg_loss = 3.45119\n",
      "epoch no.2 train no.194260  loss = 4.77448 avg_loss = 3.42205\n",
      "epoch no.2 train no.194270  loss = 3.31079 avg_loss = 3.43874\n",
      "epoch no.2 train no.194280  loss = 3.68257 avg_loss = 3.38277\n",
      "epoch no.2 train no.194290  loss = 7.06538 avg_loss = 3.40489\n",
      "epoch no.2 train no.194300  loss = 5.19115 avg_loss = 3.47435\n",
      "epoch no.2 train no.194310  loss = 2.92441 avg_loss = 3.50133\n",
      "epoch no.2 train no.194320  loss = 2.39680 avg_loss = 3.47667\n",
      "epoch no.2 train no.194330  loss = 2.44298 avg_loss = 3.46522\n",
      "epoch no.2 train no.194340  loss = 3.34966 avg_loss = 3.50375\n",
      "epoch no.2 train no.194350  loss = 3.18644 avg_loss = 3.49560\n",
      "epoch no.2 train no.194360  loss = 2.42102 avg_loss = 3.44757\n",
      "epoch no.2 train no.194370  loss = 4.60251 avg_loss = 3.47288\n",
      "epoch no.2 train no.194380  loss = 2.84498 avg_loss = 3.44419\n",
      "epoch no.2 train no.194390  loss = 2.98445 avg_loss = 3.41589\n",
      "epoch no.2 train no.194400  loss = 3.53634 avg_loss = 3.44306\n",
      "epoch no.2 train no.194410  loss = 2.56829 avg_loss = 3.39469\n",
      "epoch no.2 train no.194420  loss = 1.24601 avg_loss = 3.38934\n",
      "epoch no.2 train no.194430  loss = 6.57794 avg_loss = 3.44512\n",
      "epoch no.2 train no.194440  loss = 3.10226 avg_loss = 3.47589\n",
      "epoch no.2 train no.194450  loss = 3.27977 avg_loss = 3.45295\n",
      "epoch no.2 train no.194460  loss = 2.66219 avg_loss = 3.47898\n",
      "epoch no.2 train no.194470  loss = 3.70305 avg_loss = 3.49070\n",
      "epoch no.2 train no.194480  loss = 2.80553 avg_loss = 3.44029\n",
      "epoch no.2 train no.194490  loss = 4.49607 avg_loss = 3.42720\n",
      "epoch no.2 train no.194500  loss = 3.23297 avg_loss = 3.39238\n",
      "epoch no.2 train no.194510  loss = 4.00272 avg_loss = 3.42297\n",
      "epoch no.2 train no.194520  loss = 3.93871 avg_loss = 3.39327\n",
      "epoch no.2 train no.194530  loss = 3.53903 avg_loss = 3.37133\n",
      "epoch no.2 train no.194540  loss = 1.97590 avg_loss = 3.38865\n",
      "epoch no.2 train no.194550  loss = 3.35195 avg_loss = 3.36303\n",
      "epoch no.2 train no.194560  loss = 3.90069 avg_loss = 3.43736\n",
      "epoch no.2 train no.194570  loss = 1.60127 avg_loss = 3.41160\n",
      "epoch no.2 train no.194580  loss = 3.37464 avg_loss = 3.43644\n",
      "epoch no.2 train no.194590  loss = 4.10679 avg_loss = 3.39026\n",
      "epoch no.2 train no.194600  loss = 2.86604 avg_loss = 3.36743\n",
      "epoch no.2 train no.194610  loss = 3.38612 avg_loss = 3.38855\n",
      "epoch no.2 train no.194620  loss = 6.10344 avg_loss = 3.35968\n",
      "epoch no.2 train no.194630  loss = 2.58674 avg_loss = 3.39086\n",
      "epoch no.2 train no.194640  loss = 2.75324 avg_loss = 3.36516\n",
      "epoch no.2 train no.194650  loss = 3.79074 avg_loss = 3.44368\n",
      "epoch no.2 train no.194660  loss = 4.09470 avg_loss = 3.41028\n",
      "epoch no.2 train no.194670  loss = 2.10882 avg_loss = 3.35469\n",
      "epoch no.2 train no.194680  loss = 5.34928 avg_loss = 3.37840\n",
      "epoch no.2 train no.194690  loss = 4.02431 avg_loss = 3.42020\n",
      "epoch no.2 train no.194700  loss = 4.39611 avg_loss = 3.39913\n",
      "epoch no.2 train no.194710  loss = 3.24300 avg_loss = 3.40672\n",
      "epoch no.2 train no.194720  loss = 4.95664 avg_loss = 3.39046\n",
      "epoch no.2 train no.194730  loss = 3.11339 avg_loss = 3.45236\n",
      "epoch no.2 train no.194740  loss = 3.95187 avg_loss = 3.48188\n",
      "epoch no.2 train no.194750  loss = 2.72885 avg_loss = 3.45392\n",
      "epoch no.2 train no.194760  loss = 3.15019 avg_loss = 3.41183\n",
      "epoch no.2 train no.194770  loss = 3.05350 avg_loss = 3.37821\n",
      "epoch no.2 train no.194780  loss = 3.22402 avg_loss = 3.35279\n",
      "epoch no.2 train no.194790  loss = 4.90279 avg_loss = 3.33739\n",
      "epoch no.2 train no.194800  loss = 4.71063 avg_loss = 3.32809\n",
      "epoch no.2 train no.194810  loss = 2.96516 avg_loss = 3.34705\n",
      "epoch no.2 train no.194820  loss = 3.75469 avg_loss = 3.37928\n",
      "epoch no.2 train no.194830  loss = 3.41533 avg_loss = 3.39483\n",
      "epoch no.2 train no.194840  loss = 5.08188 avg_loss = 3.40059\n",
      "epoch no.2 train no.194850  loss = 3.13569 avg_loss = 3.42565\n",
      "epoch no.2 train no.194860  loss = 4.10851 avg_loss = 3.40108\n",
      "epoch no.2 train no.194870  loss = 2.17063 avg_loss = 3.39675\n",
      "epoch no.2 train no.194880  loss = 1.91098 avg_loss = 3.33610\n",
      "epoch no.2 train no.194890  loss = 3.44022 avg_loss = 3.36466\n",
      "epoch no.2 train no.194900  loss = 2.25635 avg_loss = 3.36546\n",
      "epoch no.2 train no.194910  loss = 4.09124 avg_loss = 3.34869\n",
      "epoch no.2 train no.194920  loss = 2.59646 avg_loss = 3.37062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.194930  loss = 2.62719 avg_loss = 3.40661\n",
      "epoch no.2 train no.194940  loss = 5.59068 avg_loss = 3.39467\n",
      "epoch no.2 train no.194950  loss = 2.93285 avg_loss = 3.41643\n",
      "epoch no.2 train no.194960  loss = 3.78566 avg_loss = 3.43401\n",
      "epoch no.2 train no.194970  loss = 2.19226 avg_loss = 3.39800\n",
      "epoch no.2 train no.194980  loss = 3.46469 avg_loss = 3.42870\n",
      "epoch no.2 train no.194990  loss = 3.38620 avg_loss = 3.39546\n",
      "epoch no.2 train no.195000  loss = 3.66965 avg_loss = 3.45609\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁팝', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.195010  loss = 4.46521 avg_loss = 3.46518\n",
      "epoch no.2 train no.195020  loss = 3.75054 avg_loss = 3.41363\n",
      "epoch no.2 train no.195030  loss = 3.69951 avg_loss = 3.39650\n",
      "epoch no.2 train no.195040  loss = 4.19143 avg_loss = 3.38726\n",
      "epoch no.2 train no.195050  loss = 3.44790 avg_loss = 3.35839\n",
      "epoch no.2 train no.195060  loss = 3.78144 avg_loss = 3.39238\n",
      "epoch no.2 train no.195070  loss = 5.12879 avg_loss = 3.38783\n",
      "epoch no.2 train no.195080  loss = 3.30975 avg_loss = 3.40087\n",
      "epoch no.2 train no.195090  loss = 3.58660 avg_loss = 3.40934\n",
      "epoch no.2 train no.195100  loss = 2.75053 avg_loss = 3.43510\n",
      "epoch no.2 train no.195110  loss = 4.62124 avg_loss = 3.47060\n",
      "epoch no.2 train no.195120  loss = 3.57231 avg_loss = 3.51561\n",
      "epoch no.2 train no.195130  loss = 2.83214 avg_loss = 3.50239\n",
      "epoch no.2 train no.195140  loss = 4.61986 avg_loss = 3.51167\n",
      "epoch no.2 train no.195150  loss = 3.59580 avg_loss = 3.46223\n",
      "epoch no.2 train no.195160  loss = 5.40303 avg_loss = 3.53530\n",
      "epoch no.2 train no.195170  loss = 3.78727 avg_loss = 3.49339\n",
      "epoch no.2 train no.195180  loss = 6.03248 avg_loss = 3.57430\n",
      "epoch no.2 train no.195190  loss = 3.66185 avg_loss = 3.52974\n",
      "epoch no.2 train no.195200  loss = 7.22971 avg_loss = 3.53712\n",
      "epoch no.2 train no.195210  loss = 3.31070 avg_loss = 3.55296\n",
      "epoch no.2 train no.195220  loss = 4.12789 avg_loss = 3.59909\n",
      "epoch no.2 train no.195230  loss = 3.98368 avg_loss = 3.60316\n",
      "epoch no.2 train no.195240  loss = 3.45914 avg_loss = 3.62308\n",
      "epoch no.2 train no.195250  loss = 3.14152 avg_loss = 3.58937\n",
      "epoch no.2 train no.195260  loss = 4.27626 avg_loss = 3.62196\n",
      "epoch no.2 train no.195270  loss = 5.11783 avg_loss = 3.61792\n",
      "epoch no.2 train no.195280  loss = 5.29666 avg_loss = 3.56852\n",
      "epoch no.2 train no.195290  loss = 3.11416 avg_loss = 3.56502\n",
      "epoch no.2 train no.195300  loss = 3.57818 avg_loss = 3.58267\n",
      "epoch no.2 train no.195310  loss = 5.55765 avg_loss = 3.56799\n",
      "epoch no.2 train no.195320  loss = 3.72390 avg_loss = 3.57245\n",
      "epoch no.2 train no.195330  loss = 2.60173 avg_loss = 3.55513\n",
      "epoch no.2 train no.195340  loss = 3.15390 avg_loss = 3.54148\n",
      "epoch no.2 train no.195350  loss = 5.12135 avg_loss = 3.56063\n",
      "epoch no.2 train no.195360  loss = 3.10912 avg_loss = 3.50576\n",
      "epoch no.2 train no.195370  loss = 4.50494 avg_loss = 3.53859\n",
      "epoch no.2 train no.195380  loss = 3.81149 avg_loss = 3.57293\n",
      "epoch no.2 train no.195390  loss = 6.47305 avg_loss = 3.55752\n",
      "epoch no.2 train no.195400  loss = 3.13607 avg_loss = 3.53716\n",
      "epoch no.2 train no.195410  loss = 2.64482 avg_loss = 3.59134\n",
      "epoch no.2 train no.195420  loss = 4.57203 avg_loss = 3.64435\n",
      "epoch no.2 train no.195430  loss = 3.14237 avg_loss = 3.62536\n",
      "epoch no.2 train no.195440  loss = 3.17601 avg_loss = 3.57411\n",
      "epoch no.2 train no.195450  loss = 3.29224 avg_loss = 3.54347\n",
      "epoch no.2 train no.195460  loss = 2.52841 avg_loss = 3.54279\n",
      "epoch no.2 train no.195470  loss = 2.28874 avg_loss = 3.53365\n",
      "epoch no.2 train no.195480  loss = 3.88977 avg_loss = 3.55049\n",
      "epoch no.2 train no.195490  loss = 3.42964 avg_loss = 3.57997\n",
      "epoch no.2 train no.195500  loss = 1.70458 avg_loss = 3.57022\n",
      "epoch no.2 train no.195510  loss = 3.41073 avg_loss = 3.51350\n",
      "epoch no.2 train no.195520  loss = 2.17713 avg_loss = 3.44999\n",
      "epoch no.2 train no.195530  loss = 4.29232 avg_loss = 3.43942\n",
      "epoch no.2 train no.195540  loss = 3.97526 avg_loss = 3.48375\n",
      "epoch no.2 train no.195550  loss = 2.09299 avg_loss = 3.45301\n",
      "epoch no.2 train no.195560  loss = 2.89589 avg_loss = 3.44652\n",
      "epoch no.2 train no.195570  loss = 2.78882 avg_loss = 3.43748\n",
      "epoch no.2 train no.195580  loss = 5.67731 avg_loss = 3.49253\n",
      "epoch no.2 train no.195590  loss = 3.41893 avg_loss = 3.47603\n",
      "epoch no.2 train no.195600  loss = 2.75848 avg_loss = 3.47240\n",
      "epoch no.2 train no.195610  loss = 3.18182 avg_loss = 3.46085\n",
      "epoch no.2 train no.195620  loss = 2.64541 avg_loss = 3.46826\n",
      "epoch no.2 train no.195630  loss = 2.29409 avg_loss = 3.47383\n",
      "epoch no.2 train no.195640  loss = 3.17461 avg_loss = 3.51241\n",
      "epoch no.2 train no.195650  loss = 2.44482 avg_loss = 3.49516\n",
      "epoch no.2 train no.195660  loss = 2.79679 avg_loss = 3.49768\n",
      "epoch no.2 train no.195670  loss = 2.95280 avg_loss = 3.49068\n",
      "epoch no.2 train no.195680  loss = 1.64115 avg_loss = 3.46573\n",
      "epoch no.2 train no.195690  loss = 4.92177 avg_loss = 3.44867\n",
      "epoch no.2 train no.195700  loss = 3.37552 avg_loss = 3.42781\n",
      "epoch no.2 train no.195710  loss = 3.96121 avg_loss = 3.51065\n",
      "epoch no.2 train no.195720  loss = 4.63980 avg_loss = 3.50255\n",
      "epoch no.2 train no.195730  loss = 2.36446 avg_loss = 3.51220\n",
      "epoch no.2 train no.195740  loss = 2.46786 avg_loss = 3.56730\n",
      "epoch no.2 train no.195750  loss = 3.50891 avg_loss = 3.53231\n",
      "epoch no.2 train no.195760  loss = 6.06652 avg_loss = 3.58191\n",
      "epoch no.2 train no.195770  loss = 3.01489 avg_loss = 3.56751\n",
      "epoch no.2 train no.195780  loss = 3.01240 avg_loss = 3.50010\n",
      "epoch no.2 train no.195790  loss = 3.84203 avg_loss = 3.50642\n",
      "epoch no.2 train no.195800  loss = 3.35091 avg_loss = 3.46011\n",
      "epoch no.2 train no.195810  loss = 2.43294 avg_loss = 3.40091\n",
      "epoch no.2 train no.195820  loss = 4.21280 avg_loss = 3.45629\n",
      "epoch no.2 train no.195830  loss = 3.72108 avg_loss = 3.47224\n",
      "epoch no.2 train no.195840  loss = 4.67098 avg_loss = 3.52783\n",
      "epoch no.2 train no.195850  loss = 1.77010 avg_loss = 3.52516\n",
      "epoch no.2 train no.195860  loss = 3.19402 avg_loss = 3.46440\n",
      "epoch no.2 train no.195870  loss = 4.36682 avg_loss = 3.46137\n",
      "epoch no.2 train no.195880  loss = 5.33537 avg_loss = 3.46367\n",
      "epoch no.2 train no.195890  loss = 2.86993 avg_loss = 3.46339\n",
      "epoch no.2 train no.195900  loss = 2.86570 avg_loss = 3.42113\n",
      "epoch no.2 train no.195910  loss = 2.71351 avg_loss = 3.38898\n",
      "epoch no.2 train no.195920  loss = 1.69212 avg_loss = 3.38084\n",
      "epoch no.2 train no.195930  loss = 3.46291 avg_loss = 3.37774\n",
      "epoch no.2 train no.195940  loss = 3.15753 avg_loss = 3.44148\n",
      "epoch no.2 train no.195950  loss = 3.99435 avg_loss = 3.45271\n",
      "epoch no.2 train no.195960  loss = 4.76919 avg_loss = 3.42695\n",
      "epoch no.2 train no.195970  loss = 3.09175 avg_loss = 3.43101\n",
      "epoch no.2 train no.195980  loss = 2.38647 avg_loss = 3.43480\n",
      "epoch no.2 train no.195990  loss = 2.41231 avg_loss = 3.44143\n",
      "epoch no.2 train no.196000  loss = 3.31035 avg_loss = 3.42595\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.2 train no.196010  loss = 2.94960 avg_loss = 3.39395\n",
      "epoch no.2 train no.196020  loss = 3.10594 avg_loss = 3.41632\n",
      "epoch no.2 train no.196030  loss = 3.95678 avg_loss = 3.41576\n",
      "epoch no.2 train no.196040  loss = 4.24348 avg_loss = 3.49675\n",
      "epoch no.2 train no.196050  loss = 3.81031 avg_loss = 3.46176\n",
      "epoch no.2 train no.196060  loss = 1.87490 avg_loss = 3.44693\n",
      "epoch no.2 train no.196070  loss = 3.56982 avg_loss = 3.49563\n",
      "epoch no.2 train no.196080  loss = 2.14043 avg_loss = 3.49928\n",
      "epoch no.2 train no.196090  loss = 4.12249 avg_loss = 3.47930\n",
      "epoch no.2 train no.196100  loss = 3.49322 avg_loss = 3.50030\n",
      "epoch no.2 train no.196110  loss = 5.08383 avg_loss = 3.52934\n",
      "epoch no.2 train no.196120  loss = 1.81055 avg_loss = 3.47593\n",
      "epoch no.2 train no.196130  loss = 3.71016 avg_loss = 3.49365\n",
      "epoch no.2 train no.196140  loss = 5.25381 avg_loss = 3.55378\n",
      "epoch no.2 train no.196150  loss = 4.15143 avg_loss = 3.58220\n",
      "epoch no.2 train no.196160  loss = 3.04182 avg_loss = 3.57437\n",
      "epoch no.2 train no.196170  loss = 2.81185 avg_loss = 3.53387\n",
      "epoch no.2 train no.196180  loss = 3.27227 avg_loss = 3.55695\n",
      "epoch no.2 train no.196190  loss = 3.36199 avg_loss = 3.53256\n",
      "epoch no.2 train no.196200  loss = 4.25274 avg_loss = 3.52672\n",
      "epoch no.2 train no.196210  loss = 2.54047 avg_loss = 3.58611\n",
      "epoch no.2 train no.196220  loss = 3.70278 avg_loss = 3.58228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.196230  loss = 2.78082 avg_loss = 3.56316\n",
      "epoch no.2 train no.196240  loss = 4.72520 avg_loss = 3.63096\n",
      "epoch no.2 train no.196250  loss = 2.49022 avg_loss = 3.60085\n",
      "epoch no.2 train no.196260  loss = 5.58145 avg_loss = 3.56556\n",
      "epoch no.2 train no.196270  loss = 2.20305 avg_loss = 3.62907\n",
      "epoch no.2 train no.196280  loss = 2.87366 avg_loss = 3.62005\n",
      "epoch no.2 train no.196290  loss = 2.29476 avg_loss = 3.64201\n",
      "epoch no.2 train no.196300  loss = 2.52578 avg_loss = 3.59329\n",
      "epoch no.2 train no.196310  loss = 4.53267 avg_loss = 3.56905\n",
      "epoch no.2 train no.196320  loss = 2.23017 avg_loss = 3.55238\n",
      "epoch no.2 train no.196330  loss = 1.51007 avg_loss = 3.49062\n",
      "epoch no.2 train no.196340  loss = 2.79694 avg_loss = 3.53240\n",
      "epoch no.2 train no.196350  loss = 3.74483 avg_loss = 3.52318\n",
      "epoch no.2 train no.196360  loss = 2.65162 avg_loss = 3.51341\n",
      "epoch no.2 train no.196370  loss = 3.02025 avg_loss = 3.49601\n",
      "epoch no.2 train no.196380  loss = 4.40848 avg_loss = 3.56447\n",
      "epoch no.2 train no.196390  loss = 6.05786 avg_loss = 3.57911\n",
      "epoch no.2 train no.196400  loss = 2.75680 avg_loss = 3.57204\n",
      "epoch no.2 train no.196410  loss = 4.44976 avg_loss = 3.55964\n",
      "epoch no.2 train no.196420  loss = 2.31288 avg_loss = 3.58111\n",
      "epoch no.2 train no.196430  loss = 3.02865 avg_loss = 3.58380\n",
      "epoch no.2 train no.196440  loss = 2.77424 avg_loss = 3.53224\n",
      "epoch no.2 train no.196450  loss = 2.80978 avg_loss = 3.50175\n",
      "epoch no.2 train no.196460  loss = 6.32494 avg_loss = 3.54025\n",
      "epoch no.2 train no.196470  loss = 2.12007 avg_loss = 3.54322\n",
      "epoch no.2 train no.196480  loss = 3.80481 avg_loss = 3.55511\n",
      "epoch no.2 train no.196490  loss = 4.58482 avg_loss = 3.58580\n",
      "epoch no.2 train no.196500  loss = 4.04393 avg_loss = 3.56185\n",
      "epoch no.2 train no.196510  loss = 3.78542 avg_loss = 3.55755\n",
      "epoch no.2 train no.196520  loss = 2.95527 avg_loss = 3.54356\n",
      "epoch no.2 train no.196530  loss = 2.83767 avg_loss = 3.54497\n",
      "epoch no.2 train no.196540  loss = 2.45806 avg_loss = 3.54460\n",
      "epoch no.2 train no.196550  loss = 2.09747 avg_loss = 3.47886\n",
      "epoch no.2 train no.196560  loss = 3.10639 avg_loss = 3.47988\n",
      "epoch no.2 train no.196570  loss = 3.92121 avg_loss = 3.45126\n",
      "epoch no.2 train no.196580  loss = 4.20379 avg_loss = 3.46154\n",
      "epoch no.2 train no.196590  loss = 4.23585 avg_loss = 3.44779\n",
      "epoch no.2 train no.196600  loss = 2.93047 avg_loss = 3.45661\n",
      "epoch no.2 train no.196610  loss = 2.39716 avg_loss = 3.48937\n",
      "epoch no.2 train no.196620  loss = 2.90036 avg_loss = 3.53615\n",
      "epoch no.2 train no.196630  loss = 2.76165 avg_loss = 3.53801\n",
      "epoch no.2 train no.196640  loss = 3.39744 avg_loss = 3.54523\n",
      "epoch no.2 train no.196650  loss = 4.63350 avg_loss = 3.50832\n",
      "epoch no.2 train no.196660  loss = 3.72142 avg_loss = 3.52468\n",
      "epoch no.2 train no.196670  loss = 4.44788 avg_loss = 3.53321\n",
      "epoch no.2 train no.196680  loss = 5.41363 avg_loss = 3.55128\n",
      "epoch no.2 train no.196690  loss = 2.96338 avg_loss = 3.56709\n",
      "epoch no.2 train no.196700  loss = 2.65060 avg_loss = 3.62945\n",
      "epoch no.2 train no.196710  loss = 3.31187 avg_loss = 3.60514\n",
      "epoch no.2 train no.196720  loss = 3.86297 avg_loss = 3.56517\n",
      "epoch no.2 train no.196730  loss = 6.00835 avg_loss = 3.60460\n",
      "epoch no.2 train no.196740  loss = 5.28415 avg_loss = 3.62941\n",
      "epoch no.2 train no.196750  loss = 2.59854 avg_loss = 3.57272\n",
      "epoch no.2 train no.196760  loss = 2.17274 avg_loss = 3.58228\n",
      "epoch no.2 train no.196770  loss = 2.86000 avg_loss = 3.55277\n",
      "epoch no.2 train no.196780  loss = 4.00523 avg_loss = 3.53630\n",
      "epoch no.2 train no.196790  loss = 4.02818 avg_loss = 3.54703\n",
      "epoch no.2 train no.196800  loss = 2.27365 avg_loss = 3.51942\n",
      "epoch no.2 train no.196810  loss = 3.27849 avg_loss = 3.51714\n",
      "epoch no.2 train no.196820  loss = 3.22675 avg_loss = 3.51870\n",
      "epoch no.2 train no.196830  loss = 3.87823 avg_loss = 3.58062\n",
      "epoch no.2 train no.196840  loss = 3.69107 avg_loss = 3.58263\n",
      "epoch no.2 train no.196850  loss = 3.88808 avg_loss = 3.56132\n",
      "epoch no.2 train no.196860  loss = 2.55057 avg_loss = 3.49342\n",
      "epoch no.2 train no.196870  loss = 5.35967 avg_loss = 3.53072\n",
      "epoch no.2 train no.196880  loss = 4.09697 avg_loss = 3.51381\n",
      "epoch no.2 train no.196890  loss = 6.52218 avg_loss = 3.51316\n",
      "epoch no.2 train no.196900  loss = 4.21256 avg_loss = 3.52642\n",
      "epoch no.2 train no.196910  loss = 2.64000 avg_loss = 3.57260\n",
      "epoch no.2 train no.196920  loss = 2.81674 avg_loss = 3.60976\n",
      "epoch no.2 train no.196930  loss = 5.69690 avg_loss = 3.58789\n",
      "epoch no.2 train no.196940  loss = 4.98699 avg_loss = 3.57686\n",
      "epoch no.2 train no.196950  loss = 2.57960 avg_loss = 3.57584\n",
      "epoch no.2 train no.196960  loss = 3.96111 avg_loss = 3.60051\n",
      "epoch no.2 train no.196970  loss = 4.22133 avg_loss = 3.59965\n",
      "epoch no.2 train no.196980  loss = 5.06060 avg_loss = 3.61669\n",
      "epoch no.2 train no.196990  loss = 3.64597 avg_loss = 3.62004\n",
      "epoch no.2 train no.197000  loss = 3.42855 avg_loss = 3.62265\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '팝', '▁모음', '집', '</s>']\n",
      "추억의 댄스곡 모음집</s>\n",
      "epoch no.2 train no.197010  loss = 3.66220 avg_loss = 3.62165\n",
      "epoch no.2 train no.197020  loss = 3.79075 avg_loss = 3.56055\n",
      "epoch no.2 train no.197030  loss = 2.13278 avg_loss = 3.49738\n",
      "epoch no.2 train no.197040  loss = 3.43960 avg_loss = 3.45797\n",
      "epoch no.2 train no.197050  loss = 3.39055 avg_loss = 3.45576\n",
      "epoch no.2 train no.197060  loss = 4.30251 avg_loss = 3.43380\n",
      "epoch no.2 train no.197070  loss = 1.89279 avg_loss = 3.40689\n",
      "epoch no.2 train no.197080  loss = 4.61852 avg_loss = 3.35808\n",
      "epoch no.2 train no.197090  loss = 6.26275 avg_loss = 3.41482\n",
      "epoch no.2 train no.197100  loss = 4.73782 avg_loss = 3.42138\n",
      "epoch no.2 train no.197110  loss = 1.66843 avg_loss = 3.43113\n",
      "epoch no.2 train no.197120  loss = 4.13006 avg_loss = 3.41768\n",
      "epoch no.2 train no.197130  loss = 4.06861 avg_loss = 3.45765\n",
      "epoch no.2 train no.197140  loss = 1.54967 avg_loss = 3.45658\n",
      "epoch no.2 train no.197150  loss = 4.26596 avg_loss = 3.45144\n",
      "epoch no.2 train no.197160  loss = 3.06701 avg_loss = 3.49763\n",
      "epoch no.2 train no.197170  loss = 3.29182 avg_loss = 3.48319\n",
      "epoch no.2 train no.197180  loss = 5.16472 avg_loss = 3.53917\n",
      "epoch no.2 train no.197190  loss = 4.25061 avg_loss = 3.54393\n",
      "epoch no.2 train no.197200  loss = 6.87317 avg_loss = 3.59983\n",
      "epoch no.2 train no.197210  loss = 3.64120 avg_loss = 3.54797\n",
      "epoch no.2 train no.197220  loss = 2.95337 avg_loss = 3.52217\n",
      "epoch no.2 train no.197230  loss = 3.07460 avg_loss = 3.51125\n",
      "epoch no.2 train no.197240  loss = 3.73001 avg_loss = 3.51164\n",
      "epoch no.2 train no.197250  loss = 2.40676 avg_loss = 3.47036\n",
      "epoch no.2 train no.197260  loss = 3.21175 avg_loss = 3.46755\n",
      "epoch no.2 train no.197270  loss = 3.56083 avg_loss = 3.49665\n",
      "epoch no.2 train no.197280  loss = 4.00116 avg_loss = 3.50830\n",
      "epoch no.2 train no.197290  loss = 3.21898 avg_loss = 3.50490\n",
      "epoch no.2 train no.197300  loss = 3.50884 avg_loss = 3.51254\n",
      "epoch no.2 train no.197310  loss = 3.32867 avg_loss = 3.56679\n",
      "epoch no.2 train no.197320  loss = 2.87874 avg_loss = 3.58183\n",
      "epoch no.2 train no.197330  loss = 2.95515 avg_loss = 3.56269\n",
      "epoch no.2 train no.197340  loss = 4.93983 avg_loss = 3.53825\n",
      "epoch no.2 train no.197350  loss = 4.57177 avg_loss = 3.49650\n",
      "epoch no.2 train no.197360  loss = 3.14808 avg_loss = 3.51664\n",
      "epoch no.2 train no.197370  loss = 3.71985 avg_loss = 3.53152\n",
      "epoch no.2 train no.197380  loss = 2.90220 avg_loss = 3.52131\n",
      "epoch no.2 train no.197390  loss = 3.06873 avg_loss = 3.53792\n",
      "epoch no.2 train no.197400  loss = 3.53010 avg_loss = 3.52108\n",
      "epoch no.2 train no.197410  loss = 2.49777 avg_loss = 3.48318\n",
      "epoch no.2 train no.197420  loss = 6.12796 avg_loss = 3.55028\n",
      "epoch no.2 train no.197430  loss = 4.52679 avg_loss = 3.55060\n",
      "epoch no.2 train no.197440  loss = 2.43923 avg_loss = 3.51689\n",
      "epoch no.2 train no.197450  loss = 3.50558 avg_loss = 3.52642\n",
      "epoch no.2 train no.197460  loss = 3.05644 avg_loss = 3.49079\n",
      "epoch no.2 train no.197470  loss = 3.91585 avg_loss = 3.48045\n",
      "epoch no.2 train no.197480  loss = 3.58878 avg_loss = 3.45167\n",
      "epoch no.2 train no.197490  loss = 3.71779 avg_loss = 3.46335\n",
      "epoch no.2 train no.197500  loss = 2.72930 avg_loss = 3.44298\n",
      "epoch no.2 train no.197510  loss = 1.30563 avg_loss = 3.38327\n",
      "epoch no.2 train no.197520  loss = 2.96614 avg_loss = 3.43384\n",
      "epoch no.2 train no.197530  loss = 4.76389 avg_loss = 3.44732\n",
      "epoch no.2 train no.197540  loss = 1.25058 avg_loss = 3.45821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.197550  loss = 2.13557 avg_loss = 3.50825\n",
      "epoch no.2 train no.197560  loss = 1.92455 avg_loss = 3.53593\n",
      "epoch no.2 train no.197570  loss = 3.76045 avg_loss = 3.49947\n",
      "epoch no.2 train no.197580  loss = 2.00844 avg_loss = 3.42767\n",
      "epoch no.2 train no.197590  loss = 2.22158 avg_loss = 3.44802\n",
      "epoch no.2 train no.197600  loss = 2.57770 avg_loss = 3.43632\n",
      "epoch no.2 train no.197610  loss = 2.19316 avg_loss = 3.40205\n",
      "epoch no.2 train no.197620  loss = 3.60261 avg_loss = 3.41620\n",
      "epoch no.2 train no.197630  loss = 2.98945 avg_loss = 3.42398\n",
      "epoch no.2 train no.197640  loss = 3.49957 avg_loss = 3.39815\n",
      "epoch no.2 train no.197650  loss = 3.34760 avg_loss = 3.40387\n",
      "epoch no.2 train no.197660  loss = 3.24549 avg_loss = 3.39104\n",
      "epoch no.2 train no.197670  loss = 3.02528 avg_loss = 3.38263\n",
      "epoch no.2 train no.197680  loss = 5.17415 avg_loss = 3.41914\n",
      "epoch no.2 train no.197690  loss = 2.97076 avg_loss = 3.43635\n",
      "epoch no.2 train no.197700  loss = 2.30047 avg_loss = 3.44644\n",
      "epoch no.2 train no.197710  loss = 3.19973 avg_loss = 3.47362\n",
      "epoch no.2 train no.197720  loss = 3.12879 avg_loss = 3.42219\n",
      "epoch no.2 train no.197730  loss = 5.74564 avg_loss = 3.50251\n",
      "epoch no.2 train no.197740  loss = 3.92570 avg_loss = 3.54444\n",
      "epoch no.2 train no.197750  loss = 3.84633 avg_loss = 3.50491\n",
      "epoch no.2 train no.197760  loss = 3.39142 avg_loss = 3.53127\n",
      "epoch no.2 train no.197770  loss = 4.07450 avg_loss = 3.52143\n",
      "epoch no.2 train no.197780  loss = 4.18835 avg_loss = 3.55302\n",
      "epoch no.2 train no.197790  loss = 3.45038 avg_loss = 3.55877\n",
      "epoch no.2 train no.197800  loss = 3.11718 avg_loss = 3.52286\n",
      "epoch no.2 train no.197810  loss = 2.88648 avg_loss = 3.55287\n",
      "epoch no.2 train no.197820  loss = 2.76027 avg_loss = 3.54534\n",
      "epoch no.2 train no.197830  loss = 3.06318 avg_loss = 3.53850\n",
      "epoch no.2 train no.197840  loss = 3.19441 avg_loss = 3.53591\n",
      "epoch no.2 train no.197850  loss = 3.97987 avg_loss = 3.52159\n",
      "epoch no.2 train no.197860  loss = 3.91669 avg_loss = 3.49705\n",
      "epoch no.2 train no.197870  loss = 2.64775 avg_loss = 3.52037\n",
      "epoch no.2 train no.197880  loss = 6.19918 avg_loss = 3.52020\n",
      "epoch no.2 train no.197890  loss = 2.85242 avg_loss = 3.54135\n",
      "epoch no.2 train no.197900  loss = 4.05034 avg_loss = 3.58900\n",
      "epoch no.2 train no.197910  loss = 4.76331 avg_loss = 3.65144\n",
      "epoch no.2 train no.197920  loss = 2.58589 avg_loss = 3.60601\n",
      "epoch no.2 train no.197930  loss = 3.56640 avg_loss = 3.62903\n",
      "epoch no.2 train no.197940  loss = 2.87432 avg_loss = 3.59936\n",
      "epoch no.2 train no.197950  loss = 3.26901 avg_loss = 3.61825\n",
      "epoch no.2 train no.197960  loss = 3.86779 avg_loss = 3.62505\n",
      "epoch no.2 train no.197970  loss = 2.77488 avg_loss = 3.57567\n",
      "epoch no.2 train no.197980  loss = 1.96153 avg_loss = 3.53826\n",
      "epoch no.2 train no.197990  loss = 2.61150 avg_loss = 3.50597\n",
      "epoch no.2 train no.198000  loss = 3.41467 avg_loss = 3.56444\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.2 train no.198010  loss = 4.82938 avg_loss = 3.60640\n",
      "epoch no.2 train no.198020  loss = 4.15529 avg_loss = 3.62512\n",
      "epoch no.2 train no.198030  loss = 2.30882 avg_loss = 3.58496\n",
      "epoch no.2 train no.198040  loss = 2.39927 avg_loss = 3.54379\n",
      "epoch no.2 train no.198050  loss = 2.45976 avg_loss = 3.54856\n",
      "epoch no.2 train no.198060  loss = 3.50559 avg_loss = 3.58193\n",
      "epoch no.2 train no.198070  loss = 3.23782 avg_loss = 3.53530\n",
      "epoch no.2 train no.198080  loss = 3.42159 avg_loss = 3.52536\n",
      "epoch no.2 train no.198090  loss = 2.98465 avg_loss = 3.50215\n",
      "epoch no.2 train no.198100  loss = 2.24068 avg_loss = 3.54718\n",
      "epoch no.2 train no.198110  loss = 1.27186 avg_loss = 3.52369\n",
      "epoch no.2 train no.198120  loss = 2.91604 avg_loss = 3.52748\n",
      "epoch no.2 train no.198130  loss = 2.14338 avg_loss = 3.47624\n",
      "epoch no.2 train no.198140  loss = 4.14445 avg_loss = 3.47501\n",
      "epoch no.2 train no.198150  loss = 4.35671 avg_loss = 3.46848\n",
      "epoch no.2 train no.198160  loss = 3.05383 avg_loss = 3.43046\n",
      "epoch no.2 train no.198170  loss = 3.46851 avg_loss = 3.42886\n",
      "epoch no.2 train no.198180  loss = 3.79958 avg_loss = 3.46436\n",
      "epoch no.2 train no.198190  loss = 3.32304 avg_loss = 3.49759\n",
      "epoch no.2 train no.198200  loss = 2.69179 avg_loss = 3.46151\n",
      "epoch no.2 train no.198210  loss = 2.47869 avg_loss = 3.39227\n",
      "epoch no.2 train no.198220  loss = 3.40547 avg_loss = 3.40657\n",
      "epoch no.2 train no.198230  loss = 3.44298 avg_loss = 3.39775\n",
      "epoch no.2 train no.198240  loss = 3.14528 avg_loss = 3.38976\n",
      "epoch no.2 train no.198250  loss = 2.43703 avg_loss = 3.40938\n",
      "epoch no.2 train no.198260  loss = 4.77521 avg_loss = 3.41134\n",
      "epoch no.2 train no.198270  loss = 2.74121 avg_loss = 3.40718\n",
      "epoch no.2 train no.198280  loss = 2.89686 avg_loss = 3.38393\n",
      "epoch no.2 train no.198290  loss = 4.79618 avg_loss = 3.36835\n",
      "epoch no.2 train no.198300  loss = 2.45525 avg_loss = 3.35446\n",
      "epoch no.2 train no.198310  loss = 7.20811 avg_loss = 3.33633\n",
      "epoch no.2 train no.198320  loss = 3.19415 avg_loss = 3.28993\n",
      "epoch no.2 train no.198330  loss = 5.90985 avg_loss = 3.34373\n",
      "epoch no.2 train no.198340  loss = 2.73555 avg_loss = 3.37152\n",
      "epoch no.2 train no.198350  loss = 2.97339 avg_loss = 3.38600\n",
      "epoch no.2 train no.198360  loss = 3.47402 avg_loss = 3.45591\n",
      "epoch no.2 train no.198370  loss = 4.54484 avg_loss = 3.44555\n",
      "epoch no.2 train no.198380  loss = 3.60200 avg_loss = 3.44048\n",
      "epoch no.2 train no.198390  loss = 5.74101 avg_loss = 3.51242\n",
      "epoch no.2 train no.198400  loss = 2.88006 avg_loss = 3.45318\n",
      "epoch no.2 train no.198410  loss = 2.43561 avg_loss = 3.40638\n",
      "epoch no.2 train no.198420  loss = 1.34620 avg_loss = 3.36580\n",
      "epoch no.2 train no.198430  loss = 4.76138 avg_loss = 3.36756\n",
      "epoch no.2 train no.198440  loss = 3.32377 avg_loss = 3.36887\n",
      "epoch no.2 train no.198450  loss = 3.97854 avg_loss = 3.37354\n",
      "epoch no.2 train no.198460  loss = 5.49745 avg_loss = 3.39334\n",
      "epoch no.2 train no.198470  loss = 4.10295 avg_loss = 3.38196\n",
      "epoch no.2 train no.198480  loss = 2.78636 avg_loss = 3.37575\n",
      "epoch no.2 train no.198490  loss = 2.79994 avg_loss = 3.39558\n",
      "epoch no.2 train no.198500  loss = 3.22768 avg_loss = 3.35434\n",
      "epoch no.2 train no.198510  loss = 3.55145 avg_loss = 3.38055\n",
      "epoch no.2 train no.198520  loss = 2.88905 avg_loss = 3.37025\n",
      "epoch no.2 train no.198530  loss = 7.08650 avg_loss = 3.37396\n",
      "epoch no.2 train no.198540  loss = 1.92640 avg_loss = 3.36465\n",
      "epoch no.2 train no.198550  loss = 4.01498 avg_loss = 3.35536\n",
      "epoch no.2 train no.198560  loss = 4.28279 avg_loss = 3.35998\n",
      "epoch no.2 train no.198570  loss = 2.73261 avg_loss = 3.37757\n",
      "epoch no.2 train no.198580  loss = 4.32233 avg_loss = 3.41971\n",
      "epoch no.2 train no.198590  loss = 2.64051 avg_loss = 3.43451\n",
      "epoch no.2 train no.198600  loss = 3.20600 avg_loss = 3.43109\n",
      "epoch no.2 train no.198610  loss = 3.66554 avg_loss = 3.38052\n",
      "epoch no.2 train no.198620  loss = 1.86061 avg_loss = 3.32123\n",
      "epoch no.2 train no.198630  loss = 2.36707 avg_loss = 3.34185\n",
      "epoch no.2 train no.198640  loss = 2.39264 avg_loss = 3.38735\n",
      "epoch no.2 train no.198650  loss = 4.56508 avg_loss = 3.39724\n",
      "epoch no.2 train no.198660  loss = 3.47854 avg_loss = 3.41398\n",
      "epoch no.2 train no.198670  loss = 4.92811 avg_loss = 3.49350\n",
      "epoch no.2 train no.198680  loss = 4.50186 avg_loss = 3.52950\n",
      "epoch no.2 train no.198690  loss = 3.71997 avg_loss = 3.49850\n",
      "epoch no.2 train no.198700  loss = 3.44399 avg_loss = 3.48610\n",
      "epoch no.2 train no.198710  loss = 3.83092 avg_loss = 3.52849\n",
      "epoch no.2 train no.198720  loss = 4.17875 avg_loss = 3.51540\n",
      "epoch no.2 train no.198730  loss = 4.37079 avg_loss = 3.52671\n",
      "epoch no.2 train no.198740  loss = 4.61395 avg_loss = 3.57832\n",
      "epoch no.2 train no.198750  loss = 1.23289 avg_loss = 3.57798\n",
      "epoch no.2 train no.198760  loss = 4.39825 avg_loss = 3.58612\n",
      "epoch no.2 train no.198770  loss = 4.25060 avg_loss = 3.51689\n",
      "epoch no.2 train no.198780  loss = 3.88844 avg_loss = 3.53379\n",
      "epoch no.2 train no.198790  loss = 2.44973 avg_loss = 3.49165\n",
      "epoch no.2 train no.198800  loss = 4.24548 avg_loss = 3.44331\n",
      "epoch no.2 train no.198810  loss = 5.59765 avg_loss = 3.47939\n",
      "epoch no.2 train no.198820  loss = 2.47231 avg_loss = 3.49566\n",
      "epoch no.2 train no.198830  loss = 2.45017 avg_loss = 3.44479\n",
      "epoch no.2 train no.198840  loss = 2.28752 avg_loss = 3.45860\n",
      "epoch no.2 train no.198850  loss = 2.83753 avg_loss = 3.49100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.198860  loss = 3.89189 avg_loss = 3.50213\n",
      "epoch no.2 train no.198870  loss = 2.33169 avg_loss = 3.53062\n",
      "epoch no.2 train no.198880  loss = 2.41940 avg_loss = 3.53813\n",
      "epoch no.2 train no.198890  loss = 3.17562 avg_loss = 3.53123\n",
      "epoch no.2 train no.198900  loss = 2.13129 avg_loss = 3.49862\n",
      "epoch no.2 train no.198910  loss = 2.07704 avg_loss = 3.47705\n",
      "epoch no.2 train no.198920  loss = 2.27015 avg_loss = 3.51171\n",
      "epoch no.2 train no.198930  loss = 3.14541 avg_loss = 3.49770\n",
      "epoch no.2 train no.198940  loss = 3.74633 avg_loss = 3.54074\n",
      "epoch no.2 train no.198950  loss = 4.51812 avg_loss = 3.54401\n",
      "epoch no.2 train no.198960  loss = 2.82809 avg_loss = 3.53337\n",
      "epoch no.2 train no.198970  loss = 4.43686 avg_loss = 3.54714\n",
      "epoch no.2 train no.198980  loss = 2.21522 avg_loss = 3.62155\n",
      "epoch no.2 train no.198990  loss = 2.90130 avg_loss = 3.67309\n",
      "epoch no.2 train no.199000  loss = 6.65642 avg_loss = 3.63888\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '집', '</s>']\n",
      "추억의 발라드 모음집</s>\n",
      "epoch no.2 train no.199010  loss = 3.51084 avg_loss = 3.66259\n",
      "epoch no.2 train no.199020  loss = 3.44525 avg_loss = 3.64687\n",
      "epoch no.2 train no.199030  loss = 3.16311 avg_loss = 3.60655\n",
      "epoch no.2 train no.199040  loss = 2.41229 avg_loss = 3.58527\n",
      "epoch no.2 train no.199050  loss = 3.51373 avg_loss = 3.56941\n",
      "epoch no.2 train no.199060  loss = 5.33545 avg_loss = 3.64971\n",
      "epoch no.2 train no.199070  loss = 1.74112 avg_loss = 3.62436\n",
      "epoch no.2 train no.199080  loss = 2.01753 avg_loss = 3.63079\n",
      "epoch no.2 train no.199090  loss = 2.87636 avg_loss = 3.60771\n",
      "epoch no.2 train no.199100  loss = 3.39322 avg_loss = 3.58404\n",
      "epoch no.2 train no.199110  loss = 3.41352 avg_loss = 3.53122\n",
      "epoch no.2 train no.199120  loss = 2.09989 avg_loss = 3.45879\n",
      "epoch no.2 train no.199130  loss = 4.98089 avg_loss = 3.47168\n",
      "epoch no.2 train no.199140  loss = 3.22789 avg_loss = 3.39406\n",
      "epoch no.2 train no.199150  loss = 3.88889 avg_loss = 3.36646\n",
      "epoch no.2 train no.199160  loss = 2.75328 avg_loss = 3.40421\n",
      "epoch no.2 train no.199170  loss = 2.65643 avg_loss = 3.43056\n",
      "epoch no.2 train no.199180  loss = 2.96672 avg_loss = 3.46492\n",
      "epoch no.2 train no.199190  loss = 2.33573 avg_loss = 3.45112\n",
      "epoch no.2 train no.199200  loss = 4.44104 avg_loss = 3.44283\n",
      "epoch no.2 train no.199210  loss = 2.98347 avg_loss = 3.43176\n",
      "epoch no.2 train no.199220  loss = 3.46764 avg_loss = 3.46293\n",
      "epoch no.2 train no.199230  loss = 4.99309 avg_loss = 3.41993\n",
      "epoch no.2 train no.199240  loss = 3.57135 avg_loss = 3.42051\n",
      "epoch no.2 train no.199250  loss = 3.04077 avg_loss = 3.40271\n",
      "epoch no.2 train no.199260  loss = 2.20748 avg_loss = 3.34965\n",
      "epoch no.2 train no.199270  loss = 1.87868 avg_loss = 3.33478\n",
      "epoch no.2 train no.199280  loss = 2.57259 avg_loss = 3.35100\n",
      "epoch no.2 train no.199290  loss = 2.93757 avg_loss = 3.39838\n",
      "epoch no.2 train no.199300  loss = 5.12787 avg_loss = 3.39475\n",
      "epoch no.2 train no.199310  loss = 2.05658 avg_loss = 3.37034\n",
      "epoch no.2 train no.199320  loss = 3.01011 avg_loss = 3.36457\n",
      "epoch no.2 train no.199330  loss = 4.06276 avg_loss = 3.38017\n",
      "epoch no.2 train no.199340  loss = 3.00265 avg_loss = 3.42388\n",
      "epoch no.2 train no.199350  loss = 3.57716 avg_loss = 3.41401\n",
      "epoch no.2 train no.199360  loss = 2.81522 avg_loss = 3.42413\n",
      "epoch no.2 train no.199370  loss = 3.53340 avg_loss = 3.42927\n",
      "epoch no.2 train no.199380  loss = 2.88746 avg_loss = 3.42317\n",
      "epoch no.2 train no.199390  loss = 4.48987 avg_loss = 3.42879\n",
      "epoch no.2 train no.199400  loss = 2.13110 avg_loss = 3.40498\n",
      "epoch no.2 train no.199410  loss = 2.55092 avg_loss = 3.39706\n",
      "epoch no.2 train no.199420  loss = 3.78736 avg_loss = 3.41412\n",
      "epoch no.2 train no.199430  loss = 3.53633 avg_loss = 3.40107\n",
      "epoch no.2 train no.199440  loss = 1.89568 avg_loss = 3.42646\n",
      "epoch no.2 train no.199450  loss = 2.70742 avg_loss = 3.40749\n",
      "epoch no.2 train no.199460  loss = 4.72449 avg_loss = 3.43809\n",
      "epoch no.2 train no.199470  loss = 1.96724 avg_loss = 3.43667\n",
      "epoch no.2 train no.199480  loss = 4.67911 avg_loss = 3.46201\n",
      "epoch no.2 train no.199490  loss = 2.74861 avg_loss = 3.43873\n",
      "epoch no.2 train no.199500  loss = 4.45715 avg_loss = 3.48759\n",
      "epoch no.2 train no.199510  loss = 2.78871 avg_loss = 3.49302\n",
      "epoch no.2 train no.199520  loss = 4.90012 avg_loss = 3.51810\n",
      "epoch no.2 train no.199530  loss = 2.96216 avg_loss = 3.48672\n",
      "epoch no.2 train no.199540  loss = 3.17580 avg_loss = 3.50091\n",
      "epoch no.2 train no.199550  loss = 3.90306 avg_loss = 3.51242\n",
      "epoch no.2 train no.199560  loss = 3.25846 avg_loss = 3.52766\n",
      "epoch no.2 train no.199570  loss = 2.65851 avg_loss = 3.52307\n",
      "epoch no.2 train no.199580  loss = 3.43701 avg_loss = 3.55646\n",
      "epoch no.2 train no.199590  loss = 3.43961 avg_loss = 3.46801\n",
      "epoch no.2 train no.199600  loss = 4.47880 avg_loss = 3.46678\n",
      "epoch no.2 train no.199610  loss = 5.01817 avg_loss = 3.45561\n",
      "epoch no.2 train no.199620  loss = 3.09323 avg_loss = 3.40794\n",
      "epoch no.2 train no.199630  loss = 3.23012 avg_loss = 3.44232\n",
      "epoch no.2 train no.199640  loss = 2.67595 avg_loss = 3.42204\n",
      "epoch no.2 train no.199650  loss = 5.64968 avg_loss = 3.50037\n",
      "epoch no.2 train no.199660  loss = 3.58787 avg_loss = 3.46206\n",
      "epoch no.2 train no.199670  loss = 4.06991 avg_loss = 3.47613\n",
      "epoch no.2 train no.199680  loss = 5.08782 avg_loss = 3.53977\n",
      "epoch no.2 train no.199690  loss = 3.38216 avg_loss = 3.54534\n",
      "epoch no.2 train no.199700  loss = 4.71756 avg_loss = 3.57437\n",
      "epoch no.2 train no.199710  loss = 4.08220 avg_loss = 3.67825\n",
      "epoch no.2 train no.199720  loss = 2.18127 avg_loss = 3.63354\n",
      "epoch no.2 train no.199730  loss = 3.73210 avg_loss = 3.65609\n",
      "epoch no.2 train no.199740  loss = 5.00599 avg_loss = 3.64772\n",
      "epoch no.2 train no.199750  loss = 3.92825 avg_loss = 3.64740\n",
      "epoch no.2 train no.199760  loss = 2.09661 avg_loss = 3.58614\n",
      "epoch no.2 train no.199770  loss = 4.34689 avg_loss = 3.56042\n",
      "epoch no.2 train no.199780  loss = 5.14985 avg_loss = 3.53830\n",
      "epoch no.2 train no.199790  loss = 2.55182 avg_loss = 3.48545\n",
      "epoch no.2 train no.199800  loss = 3.76740 avg_loss = 3.47511\n",
      "epoch no.2 train no.199810  loss = 4.59373 avg_loss = 3.45875\n",
      "epoch no.2 train no.199820  loss = 4.31942 avg_loss = 3.50909\n",
      "epoch no.2 train no.199830  loss = 3.76023 avg_loss = 3.54280\n",
      "epoch no.2 train no.199840  loss = 3.28202 avg_loss = 3.55364\n",
      "epoch no.2 train no.199850  loss = 3.20221 avg_loss = 3.54633\n",
      "epoch no.2 train no.199860  loss = 2.89573 avg_loss = 3.51354\n",
      "epoch no.2 train no.199870  loss = 2.24067 avg_loss = 3.48391\n",
      "epoch no.2 train no.199880  loss = 2.40293 avg_loss = 3.44020\n",
      "epoch no.2 train no.199890  loss = 3.44760 avg_loss = 3.41729\n",
      "epoch no.2 train no.199900  loss = 1.84184 avg_loss = 3.34823\n",
      "epoch no.2 train no.199910  loss = 2.13413 avg_loss = 3.33214\n",
      "epoch no.2 train no.199920  loss = 4.64709 avg_loss = 3.32247\n",
      "epoch no.2 train no.199930  loss = 3.95487 avg_loss = 3.32740\n",
      "epoch no.2 train no.199940  loss = 3.38175 avg_loss = 3.31549\n",
      "epoch no.2 train no.199950  loss = 3.70725 avg_loss = 3.34101\n",
      "epoch no.2 train no.199960  loss = 2.34666 avg_loss = 3.38288\n",
      "epoch no.2 train no.199970  loss = 7.51629 avg_loss = 3.42583\n",
      "epoch no.2 train no.199980  loss = 3.00820 avg_loss = 3.46472\n",
      "epoch no.2 train no.199990  loss = 3.93557 avg_loss = 3.45079\n",
      "epoch no.2 train no.200000  loss = 2.60671 avg_loss = 3.47215\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '모', '</s>']\n",
      "추억의 2000년대 가요 모음</s>\n",
      "epoch no.2 train no.200010  loss = 4.68950 avg_loss = 3.50396\n",
      "epoch no.2 train no.200020  loss = 5.71120 avg_loss = 3.56703\n",
      "epoch no.2 train no.200030  loss = 2.75944 avg_loss = 3.59953\n",
      "epoch no.2 train no.200040  loss = 3.60060 avg_loss = 3.59160\n",
      "epoch no.2 train no.200050  loss = 4.25388 avg_loss = 3.57984\n",
      "epoch no.2 train no.200060  loss = 3.08882 avg_loss = 3.53279\n",
      "epoch no.2 train no.200070  loss = 2.71741 avg_loss = 3.57395\n",
      "epoch no.2 train no.200080  loss = 3.43277 avg_loss = 3.61460\n",
      "epoch no.2 train no.200090  loss = 4.93875 avg_loss = 3.62861\n",
      "epoch no.2 train no.200100  loss = 4.35971 avg_loss = 3.61605\n",
      "epoch no.2 train no.200110  loss = 2.72961 avg_loss = 3.54407\n",
      "epoch no.2 train no.200120  loss = 2.97459 avg_loss = 3.56696\n",
      "epoch no.2 train no.200130  loss = 3.61910 avg_loss = 3.58795\n",
      "epoch no.2 train no.200140  loss = 4.14708 avg_loss = 3.60807\n",
      "epoch no.2 train no.200150  loss = 2.84582 avg_loss = 3.64558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.200160  loss = 3.14124 avg_loss = 3.66178\n",
      "epoch no.2 train no.200170  loss = 2.93915 avg_loss = 3.62485\n",
      "epoch no.2 train no.200180  loss = 5.12927 avg_loss = 3.67567\n",
      "epoch no.2 train no.200190  loss = 4.99271 avg_loss = 3.66667\n",
      "epoch no.2 train no.200200  loss = 3.96226 avg_loss = 3.60847\n",
      "epoch no.2 train no.200210  loss = 2.35748 avg_loss = 3.59141\n",
      "epoch no.2 train no.200220  loss = 5.76766 avg_loss = 3.60689\n",
      "epoch no.2 train no.200230  loss = 2.41921 avg_loss = 3.58676\n",
      "epoch no.2 train no.200240  loss = 3.75886 avg_loss = 3.57953\n",
      "epoch no.2 train no.200250  loss = 4.00434 avg_loss = 3.54283\n",
      "epoch no.2 train no.200260  loss = 2.24827 avg_loss = 3.52855\n",
      "epoch no.2 train no.200270  loss = 2.81106 avg_loss = 3.51457\n",
      "epoch no.2 train no.200280  loss = 2.62607 avg_loss = 3.53474\n",
      "epoch no.2 train no.200290  loss = 2.77968 avg_loss = 3.53106\n",
      "epoch no.2 train no.200300  loss = 3.60023 avg_loss = 3.56888\n",
      "epoch no.2 train no.200310  loss = 2.50579 avg_loss = 3.55737\n",
      "epoch no.2 train no.200320  loss = 4.01825 avg_loss = 3.53389\n",
      "epoch no.2 train no.200330  loss = 3.35559 avg_loss = 3.56835\n",
      "epoch no.2 train no.200340  loss = 2.76470 avg_loss = 3.52472\n",
      "epoch no.2 train no.200350  loss = 2.11131 avg_loss = 3.49287\n",
      "epoch no.2 train no.200360  loss = 3.48165 avg_loss = 3.42621\n",
      "epoch no.2 train no.200370  loss = 3.28345 avg_loss = 3.44827\n",
      "epoch no.2 train no.200380  loss = 6.47019 avg_loss = 3.49986\n",
      "epoch no.2 train no.200390  loss = 4.22793 avg_loss = 3.48773\n",
      "epoch no.2 train no.200400  loss = 3.35618 avg_loss = 3.52334\n",
      "epoch no.2 train no.200410  loss = 3.82427 avg_loss = 3.53004\n",
      "epoch no.2 train no.200420  loss = 4.22845 avg_loss = 3.51466\n",
      "epoch no.2 train no.200430  loss = 3.29850 avg_loss = 3.56071\n",
      "epoch no.2 train no.200440  loss = 4.58511 avg_loss = 3.55458\n",
      "epoch no.2 train no.200450  loss = 3.11193 avg_loss = 3.56720\n",
      "epoch no.2 train no.200460  loss = 2.95197 avg_loss = 3.55540\n",
      "epoch no.2 train no.200470  loss = 2.37585 avg_loss = 3.55186\n",
      "epoch no.2 train no.200480  loss = 3.67796 avg_loss = 3.52230\n",
      "epoch no.2 train no.200490  loss = 3.28089 avg_loss = 3.48994\n",
      "epoch no.2 train no.200500  loss = 3.50560 avg_loss = 3.52263\n",
      "epoch no.2 train no.200510  loss = 3.11410 avg_loss = 3.48922\n",
      "epoch no.2 train no.200520  loss = 2.60519 avg_loss = 3.55612\n",
      "epoch no.2 train no.200530  loss = 2.77256 avg_loss = 3.54376\n",
      "epoch no.2 train no.200540  loss = 4.21594 avg_loss = 3.56916\n",
      "epoch no.2 train no.200550  loss = 3.52426 avg_loss = 3.59268\n",
      "epoch no.2 train no.200560  loss = 3.16100 avg_loss = 3.61809\n",
      "epoch no.2 train no.200570  loss = 3.26325 avg_loss = 3.60231\n",
      "epoch no.2 train no.200580  loss = 2.42975 avg_loss = 3.57867\n",
      "epoch no.2 train no.200590  loss = 3.57054 avg_loss = 3.58860\n",
      "epoch no.2 train no.200600  loss = 3.20591 avg_loss = 3.56431\n",
      "epoch no.2 train no.200610  loss = 2.52036 avg_loss = 3.54733\n",
      "epoch no.2 train no.200620  loss = 4.32983 avg_loss = 3.53982\n",
      "epoch no.2 train no.200630  loss = 4.09453 avg_loss = 3.48255\n",
      "epoch no.2 train no.200640  loss = 4.29037 avg_loss = 3.45921\n",
      "epoch no.2 train no.200650  loss = 4.78593 avg_loss = 3.42625\n",
      "epoch no.2 train no.200660  loss = 2.85171 avg_loss = 3.44289\n",
      "epoch no.2 train no.200670  loss = 1.92466 avg_loss = 3.41434\n",
      "epoch no.2 train no.200680  loss = 3.41417 avg_loss = 3.46257\n",
      "epoch no.2 train no.200690  loss = 3.01038 avg_loss = 3.50145\n",
      "epoch no.2 train no.200700  loss = 5.85066 avg_loss = 3.50861\n",
      "epoch no.2 train no.200710  loss = 4.17330 avg_loss = 3.47606\n",
      "epoch no.2 train no.200720  loss = 3.06030 avg_loss = 3.44448\n",
      "epoch no.2 train no.200730  loss = 3.23481 avg_loss = 3.37188\n",
      "epoch no.2 train no.200740  loss = 2.68368 avg_loss = 3.37574\n",
      "epoch no.2 train no.200750  loss = 2.17331 avg_loss = 3.38995\n",
      "epoch no.2 train no.200760  loss = 3.57144 avg_loss = 3.40803\n",
      "epoch no.2 train no.200770  loss = 4.17665 avg_loss = 3.44938\n",
      "epoch no.2 train no.200780  loss = 4.33955 avg_loss = 3.44654\n",
      "epoch no.2 train no.200790  loss = 2.97057 avg_loss = 3.45084\n",
      "epoch no.2 train no.200800  loss = 3.28146 avg_loss = 3.46154\n",
      "epoch no.2 train no.200810  loss = 2.44418 avg_loss = 3.41744\n",
      "epoch no.2 train no.200820  loss = 4.39049 avg_loss = 3.45065\n",
      "epoch no.2 train no.200830  loss = 2.69056 avg_loss = 3.43115\n",
      "epoch no.2 train no.200840  loss = 1.37786 avg_loss = 3.35780\n",
      "epoch no.2 train no.200850  loss = 4.36322 avg_loss = 3.40760\n",
      "epoch no.2 train no.200860  loss = 4.52254 avg_loss = 3.40438\n",
      "epoch no.2 train no.200870  loss = 3.46220 avg_loss = 3.37809\n",
      "epoch no.2 train no.200880  loss = 5.55136 avg_loss = 3.38083\n",
      "epoch no.2 train no.200890  loss = 4.18396 avg_loss = 3.41167\n",
      "epoch no.2 train no.200900  loss = 3.57877 avg_loss = 3.43456\n",
      "epoch no.2 train no.200910  loss = 2.90023 avg_loss = 3.39267\n",
      "epoch no.2 train no.200920  loss = 4.38119 avg_loss = 3.41419\n",
      "epoch no.2 train no.200930  loss = 1.97982 avg_loss = 3.44454\n",
      "epoch no.2 train no.200940  loss = 3.10978 avg_loss = 3.44055\n",
      "epoch no.2 train no.200950  loss = 3.59720 avg_loss = 3.43488\n",
      "epoch no.2 train no.200960  loss = 5.80497 avg_loss = 3.46242\n",
      "epoch no.2 train no.200970  loss = 4.42168 avg_loss = 3.44717\n",
      "epoch no.2 train no.200980  loss = 3.95663 avg_loss = 3.42149\n",
      "epoch no.2 train no.200990  loss = 4.23553 avg_loss = 3.40770\n",
      "epoch no.2 train no.201000  loss = 2.61254 avg_loss = 3.40394\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.2 train no.201010  loss = 2.67987 avg_loss = 3.41764\n",
      "epoch no.2 train no.201020  loss = 3.07361 avg_loss = 3.41359\n",
      "epoch no.2 train no.201030  loss = 3.34907 avg_loss = 3.37379\n",
      "epoch no.2 train no.201040  loss = 3.51640 avg_loss = 3.39895\n",
      "epoch no.2 train no.201050  loss = 2.64001 avg_loss = 3.40141\n",
      "epoch no.2 train no.201060  loss = 4.74322 avg_loss = 3.44308\n",
      "epoch no.2 train no.201070  loss = 4.70404 avg_loss = 3.48439\n",
      "epoch no.2 train no.201080  loss = 4.97082 avg_loss = 3.51928\n",
      "epoch no.2 train no.201090  loss = 3.62245 avg_loss = 3.53676\n",
      "epoch no.2 train no.201100  loss = 3.46000 avg_loss = 3.49747\n",
      "epoch no.2 train no.201110  loss = 2.08251 avg_loss = 3.47865\n",
      "epoch no.2 train no.201120  loss = 3.56976 avg_loss = 3.43318\n",
      "epoch no.2 train no.201130  loss = 2.71084 avg_loss = 3.42879\n",
      "epoch no.2 train no.201140  loss = 5.54174 avg_loss = 3.48445\n",
      "epoch no.2 train no.201150  loss = 2.81902 avg_loss = 3.48810\n",
      "epoch no.2 train no.201160  loss = 3.36061 avg_loss = 3.49926\n",
      "epoch no.2 train no.201170  loss = 1.73935 avg_loss = 3.46411\n",
      "epoch no.2 train no.201180  loss = 2.45142 avg_loss = 3.41591\n",
      "epoch no.2 train no.201190  loss = 4.44096 avg_loss = 3.45986\n",
      "epoch no.2 train no.201200  loss = 5.28038 avg_loss = 3.43702\n",
      "epoch no.2 train no.201210  loss = 3.60001 avg_loss = 3.43008\n",
      "epoch no.2 train no.201220  loss = 3.56005 avg_loss = 3.38775\n",
      "epoch no.2 train no.201230  loss = 3.99673 avg_loss = 3.44956\n",
      "epoch no.2 train no.201240  loss = 3.71188 avg_loss = 3.44182\n",
      "epoch no.2 train no.201250  loss = 2.46125 avg_loss = 3.42873\n",
      "epoch no.2 train no.201260  loss = 3.60155 avg_loss = 3.44021\n",
      "epoch no.2 train no.201270  loss = 3.49865 avg_loss = 3.39925\n",
      "epoch no.2 train no.201280  loss = 4.54019 avg_loss = 3.47310\n",
      "epoch no.2 train no.201290  loss = 3.34649 avg_loss = 3.42906\n",
      "epoch no.2 train no.201300  loss = 3.45914 avg_loss = 3.47499\n",
      "epoch no.2 train no.201310  loss = 2.99991 avg_loss = 3.42496\n",
      "epoch no.2 train no.201320  loss = 4.74357 avg_loss = 3.41560\n",
      "epoch no.2 train no.201330  loss = 4.25272 avg_loss = 3.41482\n",
      "epoch no.2 train no.201340  loss = 3.69542 avg_loss = 3.44337\n",
      "epoch no.2 train no.201350  loss = 3.91846 avg_loss = 3.48652\n",
      "epoch no.2 train no.201360  loss = 1.27164 avg_loss = 3.43686\n",
      "epoch no.2 train no.201370  loss = 4.87916 avg_loss = 3.42955\n",
      "epoch no.2 train no.201380  loss = 2.89601 avg_loss = 3.39608\n",
      "epoch no.2 train no.201390  loss = 2.84030 avg_loss = 3.39296\n",
      "epoch no.2 train no.201400  loss = 1.93040 avg_loss = 3.39125\n",
      "epoch no.2 train no.201410  loss = 6.17652 avg_loss = 3.44652\n",
      "epoch no.2 train no.201420  loss = 3.22151 avg_loss = 3.42635\n",
      "epoch no.2 train no.201430  loss = 5.48440 avg_loss = 3.47801\n",
      "epoch no.2 train no.201440  loss = 6.64800 avg_loss = 3.55992\n",
      "epoch no.2 train no.201450  loss = 4.26815 avg_loss = 3.57588\n",
      "epoch no.2 train no.201460  loss = 3.24505 avg_loss = 3.56450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.201470  loss = 2.02238 avg_loss = 3.55193\n",
      "epoch no.2 train no.201480  loss = 2.88782 avg_loss = 3.50947\n",
      "epoch no.2 train no.201490  loss = 3.13398 avg_loss = 3.47710\n",
      "epoch no.2 train no.201500  loss = 2.99036 avg_loss = 3.45051\n",
      "epoch no.2 train no.201510  loss = 4.14696 avg_loss = 3.43769\n",
      "epoch no.2 train no.201520  loss = 3.52905 avg_loss = 3.44680\n",
      "epoch no.2 train no.201530  loss = 5.03693 avg_loss = 3.47232\n",
      "epoch no.2 train no.201540  loss = 4.20786 avg_loss = 3.54121\n",
      "epoch no.2 train no.201550  loss = 3.30986 avg_loss = 3.58007\n",
      "epoch no.2 train no.201560  loss = 3.13377 avg_loss = 3.52291\n",
      "epoch no.2 train no.201570  loss = 5.86619 avg_loss = 3.53104\n",
      "epoch no.2 train no.201580  loss = 3.79904 avg_loss = 3.51830\n",
      "epoch no.2 train no.201590  loss = 4.48678 avg_loss = 3.51328\n",
      "epoch no.2 train no.201600  loss = 3.32029 avg_loss = 3.47780\n",
      "epoch no.2 train no.201610  loss = 2.88792 avg_loss = 3.45251\n",
      "epoch no.2 train no.201620  loss = 4.46560 avg_loss = 3.48855\n",
      "epoch no.2 train no.201630  loss = 2.85669 avg_loss = 3.47595\n",
      "epoch no.2 train no.201640  loss = 1.69292 avg_loss = 3.49627\n",
      "epoch no.2 train no.201650  loss = 5.20555 avg_loss = 3.51369\n",
      "epoch no.2 train no.201660  loss = 5.51399 avg_loss = 3.53073\n",
      "epoch no.2 train no.201670  loss = 4.20944 avg_loss = 3.53091\n",
      "epoch no.2 train no.201680  loss = 3.83083 avg_loss = 3.53058\n",
      "epoch no.2 train no.201690  loss = 3.41514 avg_loss = 3.51785\n",
      "epoch no.2 train no.201700  loss = 3.34181 avg_loss = 3.53112\n",
      "epoch no.2 train no.201710  loss = 3.11434 avg_loss = 3.49075\n",
      "epoch no.2 train no.201720  loss = 3.63528 avg_loss = 3.48645\n",
      "epoch no.2 train no.201730  loss = 3.93702 avg_loss = 3.48216\n",
      "epoch no.2 train no.201740  loss = 3.65548 avg_loss = 3.51444\n",
      "epoch no.2 train no.201750  loss = 3.76059 avg_loss = 3.49641\n",
      "epoch no.2 train no.201760  loss = 4.68748 avg_loss = 3.48464\n",
      "epoch no.2 train no.201770  loss = 1.99839 avg_loss = 3.47890\n",
      "epoch no.2 train no.201780  loss = 3.88549 avg_loss = 3.44384\n",
      "epoch no.2 train no.201790  loss = 4.79752 avg_loss = 3.47937\n",
      "epoch no.2 train no.201800  loss = 3.59667 avg_loss = 3.53248\n",
      "epoch no.2 train no.201810  loss = 2.70359 avg_loss = 3.48457\n",
      "epoch no.2 train no.201820  loss = 2.11170 avg_loss = 3.47453\n",
      "epoch no.2 train no.201830  loss = 4.84706 avg_loss = 3.46948\n",
      "epoch no.2 train no.201840  loss = 2.97403 avg_loss = 3.46082\n",
      "epoch no.2 train no.201850  loss = 2.77415 avg_loss = 3.46873\n",
      "epoch no.2 train no.201860  loss = 4.80246 avg_loss = 3.51010\n",
      "epoch no.2 train no.201870  loss = 2.05919 avg_loss = 3.49505\n",
      "epoch no.2 train no.201880  loss = 2.21952 avg_loss = 3.49044\n",
      "epoch no.2 train no.201890  loss = 3.86805 avg_loss = 3.49033\n",
      "epoch no.2 train no.201900  loss = 3.39066 avg_loss = 3.47311\n",
      "epoch no.2 train no.201910  loss = 2.42366 avg_loss = 3.50194\n",
      "epoch no.2 train no.201920  loss = 3.69185 avg_loss = 3.50606\n",
      "epoch no.2 train no.201930  loss = 2.86728 avg_loss = 3.58455\n",
      "epoch no.2 train no.201940  loss = 3.13597 avg_loss = 3.65164\n",
      "epoch no.2 train no.201950  loss = 3.13876 avg_loss = 3.58042\n",
      "epoch no.2 train no.201960  loss = 2.16587 avg_loss = 3.54848\n",
      "epoch no.2 train no.201970  loss = 2.57481 avg_loss = 3.55565\n",
      "epoch no.2 train no.201980  loss = 7.11490 avg_loss = 3.61779\n",
      "epoch no.2 train no.201990  loss = 3.81983 avg_loss = 3.62915\n",
      "epoch no.2 train no.202000  loss = 3.86540 avg_loss = 3.63582\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '년대', '▁가요', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.202010  loss = 3.70130 avg_loss = 3.64435\n",
      "epoch no.2 train no.202020  loss = 4.38215 avg_loss = 3.63219\n",
      "epoch no.2 train no.202030  loss = 5.01623 avg_loss = 3.62066\n",
      "epoch no.2 train no.202040  loss = 4.41615 avg_loss = 3.60504\n",
      "epoch no.2 train no.202050  loss = 3.30021 avg_loss = 3.60348\n",
      "epoch no.2 train no.202060  loss = 3.24561 avg_loss = 3.60022\n",
      "epoch no.2 train no.202070  loss = 3.33035 avg_loss = 3.60003\n",
      "epoch no.2 train no.202080  loss = 3.91716 avg_loss = 3.61693\n",
      "epoch no.2 train no.202090  loss = 2.63000 avg_loss = 3.62443\n",
      "epoch no.2 train no.202100  loss = 2.38135 avg_loss = 3.53397\n",
      "epoch no.2 train no.202110  loss = 3.68110 avg_loss = 3.52753\n",
      "epoch no.2 train no.202120  loss = 4.26505 avg_loss = 3.49443\n",
      "epoch no.2 train no.202130  loss = 5.57922 avg_loss = 3.50995\n",
      "epoch no.2 train no.202140  loss = 4.11236 avg_loss = 3.53402\n",
      "epoch no.2 train no.202150  loss = 2.70975 avg_loss = 3.50074\n",
      "epoch no.2 train no.202160  loss = 3.64855 avg_loss = 3.51450\n",
      "epoch no.2 train no.202170  loss = 2.45383 avg_loss = 3.49784\n",
      "epoch no.2 train no.202180  loss = 5.05396 avg_loss = 3.46233\n",
      "epoch no.2 train no.202190  loss = 1.88973 avg_loss = 3.44265\n",
      "epoch no.2 train no.202200  loss = 2.50767 avg_loss = 3.41499\n",
      "epoch no.2 train no.202210  loss = 3.48652 avg_loss = 3.42734\n",
      "epoch no.2 train no.202220  loss = 3.10544 avg_loss = 3.41162\n",
      "epoch no.2 train no.202230  loss = 3.34552 avg_loss = 3.34845\n",
      "epoch no.2 train no.202240  loss = 3.66635 avg_loss = 3.36094\n",
      "epoch no.2 train no.202250  loss = 4.18671 avg_loss = 3.36728\n",
      "epoch no.2 train no.202260  loss = 2.39021 avg_loss = 3.32682\n",
      "epoch no.2 train no.202270  loss = 5.79508 avg_loss = 3.35893\n",
      "epoch no.2 train no.202280  loss = 3.91511 avg_loss = 3.33642\n",
      "epoch no.2 train no.202290  loss = 1.53233 avg_loss = 3.29360\n",
      "epoch no.2 train no.202300  loss = 3.83496 avg_loss = 3.33808\n",
      "epoch no.2 train no.202310  loss = 3.89827 avg_loss = 3.32478\n",
      "epoch no.2 train no.202320  loss = 3.73924 avg_loss = 3.38625\n",
      "epoch no.2 train no.202330  loss = 5.84535 avg_loss = 3.44178\n",
      "epoch no.2 train no.202340  loss = 3.42250 avg_loss = 3.46806\n",
      "epoch no.2 train no.202350  loss = 2.36608 avg_loss = 3.49123\n",
      "epoch no.2 train no.202360  loss = 3.93516 avg_loss = 3.51830\n",
      "epoch no.2 train no.202370  loss = 3.65414 avg_loss = 3.55179\n",
      "epoch no.2 train no.202380  loss = 2.68814 avg_loss = 3.52909\n",
      "epoch no.2 train no.202390  loss = 3.73809 avg_loss = 3.57202\n",
      "epoch no.2 train no.202400  loss = 5.18484 avg_loss = 3.54879\n",
      "epoch no.2 train no.202410  loss = 2.64913 avg_loss = 3.53927\n",
      "epoch no.2 train no.202420  loss = 3.10089 avg_loss = 3.52178\n",
      "epoch no.2 train no.202430  loss = 4.73367 avg_loss = 3.51023\n",
      "epoch no.2 train no.202440  loss = 5.49189 avg_loss = 3.55148\n",
      "epoch no.2 train no.202450  loss = 6.26349 avg_loss = 3.57129\n",
      "epoch no.2 train no.202460  loss = 3.74381 avg_loss = 3.55637\n",
      "epoch no.2 train no.202470  loss = 5.87907 avg_loss = 3.60419\n",
      "epoch no.2 train no.202480  loss = 3.54808 avg_loss = 3.60119\n",
      "epoch no.2 train no.202490  loss = 2.16913 avg_loss = 3.54752\n",
      "epoch no.2 train no.202500  loss = 2.20732 avg_loss = 3.52139\n",
      "epoch no.2 train no.202510  loss = 5.66831 avg_loss = 3.52930\n",
      "epoch no.2 train no.202520  loss = 2.63988 avg_loss = 3.54122\n",
      "epoch no.2 train no.202530  loss = 3.95306 avg_loss = 3.52538\n",
      "epoch no.2 train no.202540  loss = 2.70867 avg_loss = 3.53021\n",
      "epoch no.2 train no.202550  loss = 5.12676 avg_loss = 3.58304\n",
      "epoch no.2 train no.202560  loss = 5.75064 avg_loss = 3.56059\n",
      "epoch no.2 train no.202570  loss = 2.52376 avg_loss = 3.55651\n",
      "epoch no.2 train no.202580  loss = 3.41578 avg_loss = 3.57557\n",
      "epoch no.2 train no.202590  loss = 4.62222 avg_loss = 3.59683\n",
      "epoch no.2 train no.202600  loss = 2.51485 avg_loss = 3.51417\n",
      "epoch no.2 train no.202610  loss = 4.57585 avg_loss = 3.53454\n",
      "epoch no.2 train no.202620  loss = 4.39371 avg_loss = 3.57164\n",
      "epoch no.2 train no.202630  loss = 4.42276 avg_loss = 3.57634\n",
      "epoch no.2 train no.202640  loss = 4.69681 avg_loss = 3.58940\n",
      "epoch no.2 train no.202650  loss = 1.73211 avg_loss = 3.58926\n",
      "epoch no.2 train no.202660  loss = 2.67596 avg_loss = 3.57481\n",
      "epoch no.2 train no.202670  loss = 1.76678 avg_loss = 3.56037\n",
      "epoch no.2 train no.202680  loss = 3.27842 avg_loss = 3.55128\n",
      "epoch no.2 train no.202690  loss = 2.59629 avg_loss = 3.52271\n",
      "epoch no.2 train no.202700  loss = 2.56796 avg_loss = 3.51989\n",
      "epoch no.2 train no.202710  loss = 3.10834 avg_loss = 3.49924\n",
      "epoch no.2 train no.202720  loss = 4.41280 avg_loss = 3.48847\n",
      "epoch no.2 train no.202730  loss = 3.47440 avg_loss = 3.54302\n",
      "epoch no.2 train no.202740  loss = 1.78297 avg_loss = 3.59966\n",
      "epoch no.2 train no.202750  loss = 2.77500 avg_loss = 3.55515\n",
      "epoch no.2 train no.202760  loss = 3.21413 avg_loss = 3.57105\n",
      "epoch no.2 train no.202770  loss = 2.65484 avg_loss = 3.57647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.202780  loss = 4.41990 avg_loss = 3.65670\n",
      "epoch no.2 train no.202790  loss = 4.95844 avg_loss = 3.62822\n",
      "epoch no.2 train no.202800  loss = 3.82925 avg_loss = 3.58744\n",
      "epoch no.2 train no.202810  loss = 4.50712 avg_loss = 3.64219\n",
      "epoch no.2 train no.202820  loss = 4.41027 avg_loss = 3.68066\n",
      "epoch no.2 train no.202830  loss = 3.37417 avg_loss = 3.65997\n",
      "epoch no.2 train no.202840  loss = 4.48422 avg_loss = 3.67369\n",
      "epoch no.2 train no.202850  loss = 4.31956 avg_loss = 3.62362\n",
      "epoch no.2 train no.202860  loss = 3.22858 avg_loss = 3.55759\n",
      "epoch no.2 train no.202870  loss = 5.05235 avg_loss = 3.55715\n",
      "epoch no.2 train no.202880  loss = 5.53188 avg_loss = 3.55346\n",
      "epoch no.2 train no.202890  loss = 3.27085 avg_loss = 3.61764\n",
      "epoch no.2 train no.202900  loss = 3.20433 avg_loss = 3.57321\n",
      "epoch no.2 train no.202910  loss = 5.31410 avg_loss = 3.60980\n",
      "epoch no.2 train no.202920  loss = 3.14217 avg_loss = 3.60878\n",
      "epoch no.2 train no.202930  loss = 2.60097 avg_loss = 3.65822\n",
      "epoch no.2 train no.202940  loss = 5.65243 avg_loss = 3.69073\n",
      "epoch no.2 train no.202950  loss = 6.99485 avg_loss = 3.70504\n",
      "epoch no.2 train no.202960  loss = 3.89761 avg_loss = 3.62296\n",
      "epoch no.2 train no.202970  loss = 2.72945 avg_loss = 3.52804\n",
      "epoch no.2 train no.202980  loss = 4.03425 avg_loss = 3.51735\n",
      "epoch no.2 train no.202990  loss = 2.04637 avg_loss = 3.50671\n",
      "epoch no.2 train no.203000  loss = 3.67859 avg_loss = 3.46587\n",
      "3\n",
      "to_tokens: ['▁비', '▁드라마', '요', '▁베스트', '</s>']\n",
      "추억의 인기가요 베스트</s>\n",
      "epoch no.2 train no.203010  loss = 4.62025 avg_loss = 3.48921\n",
      "epoch no.2 train no.203020  loss = 4.01582 avg_loss = 3.48412\n",
      "epoch no.2 train no.203030  loss = 3.71667 avg_loss = 3.45864\n",
      "epoch no.2 train no.203040  loss = 4.75911 avg_loss = 3.44393\n",
      "epoch no.2 train no.203050  loss = 5.20857 avg_loss = 3.45060\n",
      "epoch no.2 train no.203060  loss = 4.97555 avg_loss = 3.46539\n",
      "epoch no.2 train no.203070  loss = 2.96945 avg_loss = 3.48900\n",
      "epoch no.2 train no.203080  loss = 3.16804 avg_loss = 3.48939\n",
      "epoch no.2 train no.203090  loss = 3.55839 avg_loss = 3.46219\n",
      "epoch no.2 train no.203100  loss = 2.51804 avg_loss = 3.44356\n",
      "epoch no.2 train no.203110  loss = 4.09110 avg_loss = 3.43211\n",
      "epoch no.2 train no.203120  loss = 3.59402 avg_loss = 3.45530\n",
      "epoch no.2 train no.203130  loss = 6.06509 avg_loss = 3.49409\n",
      "epoch no.2 train no.203140  loss = 5.57518 avg_loss = 3.53704\n",
      "epoch no.2 train no.203150  loss = 3.03959 avg_loss = 3.57373\n",
      "epoch no.2 train no.203160  loss = 3.54219 avg_loss = 3.55405\n",
      "epoch no.2 train no.203170  loss = 3.39300 avg_loss = 3.61874\n",
      "epoch no.2 train no.203180  loss = 3.65878 avg_loss = 3.58053\n",
      "epoch no.2 train no.203190  loss = 2.67013 avg_loss = 3.56723\n",
      "epoch no.2 train no.203200  loss = 3.37740 avg_loss = 3.58891\n",
      "epoch no.2 train no.203210  loss = 3.96790 avg_loss = 3.57207\n",
      "epoch no.2 train no.203220  loss = 4.19300 avg_loss = 3.59220\n",
      "epoch no.2 train no.203230  loss = 2.50391 avg_loss = 3.58645\n",
      "epoch no.2 train no.203240  loss = 5.79126 avg_loss = 3.56204\n",
      "epoch no.2 train no.203250  loss = 4.34581 avg_loss = 3.53050\n",
      "epoch no.2 train no.203260  loss = 3.44312 avg_loss = 3.51975\n",
      "epoch no.2 train no.203270  loss = 4.66008 avg_loss = 3.53197\n",
      "epoch no.2 train no.203280  loss = 2.50334 avg_loss = 3.54799\n",
      "epoch no.2 train no.203290  loss = 2.29423 avg_loss = 3.53674\n",
      "epoch no.2 train no.203300  loss = 4.04636 avg_loss = 3.54856\n",
      "epoch no.2 train no.203310  loss = 3.67680 avg_loss = 3.52211\n",
      "epoch no.2 train no.203320  loss = 2.33216 avg_loss = 3.48158\n",
      "epoch no.2 train no.203330  loss = 2.92529 avg_loss = 3.45050\n",
      "epoch no.2 train no.203340  loss = 4.06605 avg_loss = 3.47603\n",
      "epoch no.2 train no.203350  loss = 3.88039 avg_loss = 3.48677\n",
      "epoch no.2 train no.203360  loss = 1.56408 avg_loss = 3.46381\n",
      "epoch no.2 train no.203370  loss = 3.11239 avg_loss = 3.49272\n",
      "epoch no.2 train no.203380  loss = 2.49022 avg_loss = 3.45111\n",
      "epoch no.2 train no.203390  loss = 3.28640 avg_loss = 3.43975\n",
      "epoch no.2 train no.203400  loss = 2.84348 avg_loss = 3.43074\n",
      "epoch no.2 train no.203410  loss = 2.33799 avg_loss = 3.43294\n",
      "epoch no.2 train no.203420  loss = 2.38855 avg_loss = 3.41852\n",
      "epoch no.2 train no.203430  loss = 3.78924 avg_loss = 3.41127\n",
      "epoch no.2 train no.203440  loss = 2.54928 avg_loss = 3.40470\n",
      "epoch no.2 train no.203450  loss = 3.00261 avg_loss = 3.36695\n",
      "epoch no.2 train no.203460  loss = 3.65817 avg_loss = 3.34008\n",
      "epoch no.2 train no.203470  loss = 3.05799 avg_loss = 3.31646\n",
      "epoch no.2 train no.203480  loss = 3.08802 avg_loss = 3.34266\n",
      "epoch no.2 train no.203490  loss = 2.19674 avg_loss = 3.33552\n",
      "epoch no.2 train no.203500  loss = 2.43010 avg_loss = 3.34139\n",
      "epoch no.2 train no.203510  loss = 2.41661 avg_loss = 3.40221\n",
      "epoch no.2 train no.203520  loss = 3.99815 avg_loss = 3.45435\n",
      "epoch no.2 train no.203530  loss = 3.23149 avg_loss = 3.48339\n",
      "epoch no.2 train no.203540  loss = 4.46552 avg_loss = 3.54822\n",
      "epoch no.2 train no.203550  loss = 2.95192 avg_loss = 3.48055\n",
      "epoch no.2 train no.203560  loss = 3.77419 avg_loss = 3.52274\n",
      "epoch no.2 train no.203570  loss = 2.01028 avg_loss = 3.50555\n",
      "epoch no.2 train no.203580  loss = 5.23879 avg_loss = 3.49753\n",
      "epoch no.2 train no.203590  loss = 3.52819 avg_loss = 3.50793\n",
      "epoch no.2 train no.203600  loss = 3.16610 avg_loss = 3.52178\n",
      "epoch no.2 train no.203610  loss = 4.40526 avg_loss = 3.52409\n",
      "epoch no.2 train no.203620  loss = 4.45143 avg_loss = 3.55081\n",
      "epoch no.2 train no.203630  loss = 2.45066 avg_loss = 3.54188\n",
      "epoch no.2 train no.203640  loss = 4.02097 avg_loss = 3.57745\n",
      "epoch no.2 train no.203650  loss = 5.18472 avg_loss = 3.59808\n",
      "epoch no.2 train no.203660  loss = 2.38459 avg_loss = 3.56232\n",
      "epoch no.2 train no.203670  loss = 2.26066 avg_loss = 3.53037\n",
      "epoch no.2 train no.203680  loss = 2.15056 avg_loss = 3.52434\n",
      "epoch no.2 train no.203690  loss = 2.61213 avg_loss = 3.53812\n",
      "epoch no.2 train no.203700  loss = 4.34449 avg_loss = 3.55627\n",
      "epoch no.2 train no.203710  loss = 2.93357 avg_loss = 3.52678\n",
      "epoch no.2 train no.203720  loss = 3.06903 avg_loss = 3.49781\n",
      "epoch no.2 train no.203730  loss = 4.41184 avg_loss = 3.59120\n",
      "epoch no.2 train no.203740  loss = 3.42538 avg_loss = 3.56718\n",
      "epoch no.2 train no.203750  loss = 6.51800 avg_loss = 3.56554\n",
      "epoch no.2 train no.203760  loss = 2.90654 avg_loss = 3.51672\n",
      "epoch no.2 train no.203770  loss = 3.58392 avg_loss = 3.49054\n",
      "epoch no.2 train no.203780  loss = 1.62818 avg_loss = 3.43866\n",
      "epoch no.2 train no.203790  loss = 2.53409 avg_loss = 3.49552\n",
      "epoch no.2 train no.203800  loss = 4.31301 avg_loss = 3.52562\n",
      "epoch no.2 train no.203810  loss = 2.65698 avg_loss = 3.48557\n",
      "epoch no.2 train no.203820  loss = 3.04573 avg_loss = 3.45900\n",
      "epoch no.2 train no.203830  loss = 3.43490 avg_loss = 3.46959\n",
      "epoch no.2 train no.203840  loss = 4.09083 avg_loss = 3.44240\n",
      "epoch no.2 train no.203850  loss = 2.36954 avg_loss = 3.47236\n",
      "epoch no.2 train no.203860  loss = 5.43176 avg_loss = 3.53382\n",
      "epoch no.2 train no.203870  loss = 4.01293 avg_loss = 3.48803\n",
      "epoch no.2 train no.203880  loss = 3.62927 avg_loss = 3.45667\n",
      "epoch no.2 train no.203890  loss = 2.50807 avg_loss = 3.46681\n",
      "epoch no.2 train no.203900  loss = 3.30958 avg_loss = 3.50325\n",
      "epoch no.2 train no.203910  loss = 4.44565 avg_loss = 3.50213\n",
      "epoch no.2 train no.203920  loss = 2.84120 avg_loss = 3.47652\n",
      "epoch no.2 train no.203930  loss = 3.14848 avg_loss = 3.50966\n",
      "epoch no.2 train no.203940  loss = 2.29569 avg_loss = 3.46385\n",
      "epoch no.2 train no.203950  loss = 2.64210 avg_loss = 3.45109\n",
      "epoch no.2 train no.203960  loss = 2.91705 avg_loss = 3.49548\n",
      "epoch no.2 train no.203970  loss = 2.46302 avg_loss = 3.44705\n",
      "epoch no.2 train no.203980  loss = 2.49150 avg_loss = 3.37828\n",
      "epoch no.2 train no.203990  loss = 3.25313 avg_loss = 3.38667\n",
      "epoch no.2 train no.204000  loss = 1.92073 avg_loss = 3.39706\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.2 train no.204010  loss = 1.56793 avg_loss = 3.35275\n",
      "epoch no.2 train no.204020  loss = 3.33718 avg_loss = 3.38488\n",
      "epoch no.2 train no.204030  loss = 3.20722 avg_loss = 3.39740\n",
      "epoch no.2 train no.204040  loss = 2.56705 avg_loss = 3.37940\n",
      "epoch no.2 train no.204050  loss = 3.07311 avg_loss = 3.37893\n",
      "epoch no.2 train no.204060  loss = 2.09213 avg_loss = 3.33856\n",
      "epoch no.2 train no.204070  loss = 2.27900 avg_loss = 3.32747\n",
      "epoch no.2 train no.204080  loss = 3.08423 avg_loss = 3.29559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.204090  loss = 4.06539 avg_loss = 3.34667\n",
      "epoch no.2 train no.204100  loss = 6.26198 avg_loss = 3.39940\n",
      "epoch no.2 train no.204110  loss = 4.40183 avg_loss = 3.39836\n",
      "epoch no.2 train no.204120  loss = 2.98671 avg_loss = 3.42244\n",
      "epoch no.2 train no.204130  loss = 4.93761 avg_loss = 3.45535\n",
      "epoch no.2 train no.204140  loss = 4.28136 avg_loss = 3.42281\n",
      "epoch no.2 train no.204150  loss = 2.96751 avg_loss = 3.49367\n",
      "epoch no.2 train no.204160  loss = 6.21070 avg_loss = 3.48893\n",
      "epoch no.2 train no.204170  loss = 5.45241 avg_loss = 3.48418\n",
      "epoch no.2 train no.204180  loss = 2.63888 avg_loss = 3.46624\n",
      "epoch no.2 train no.204190  loss = 3.44975 avg_loss = 3.44533\n",
      "epoch no.2 train no.204200  loss = 3.74976 avg_loss = 3.45733\n",
      "epoch no.2 train no.204210  loss = 4.16501 avg_loss = 3.44529\n",
      "epoch no.2 train no.204220  loss = 3.05221 avg_loss = 3.50421\n",
      "epoch no.2 train no.204230  loss = 3.64758 avg_loss = 3.50792\n",
      "epoch no.2 train no.204240  loss = 2.40859 avg_loss = 3.48496\n",
      "epoch no.2 train no.204250  loss = 2.45912 avg_loss = 3.48636\n",
      "epoch no.2 train no.204260  loss = 3.83566 avg_loss = 3.51435\n",
      "epoch no.2 train no.204270  loss = 3.88049 avg_loss = 3.48917\n",
      "epoch no.2 train no.204280  loss = 5.67033 avg_loss = 3.48241\n",
      "epoch no.2 train no.204290  loss = 2.30188 avg_loss = 3.43375\n",
      "epoch no.2 train no.204300  loss = 4.84019 avg_loss = 3.46140\n",
      "epoch no.2 train no.204310  loss = 2.86985 avg_loss = 3.47392\n",
      "epoch no.2 train no.204320  loss = 4.78316 avg_loss = 3.44454\n",
      "epoch no.2 train no.204330  loss = 3.75919 avg_loss = 3.47511\n",
      "epoch no.2 train no.204340  loss = 3.83340 avg_loss = 3.45791\n",
      "epoch no.2 train no.204350  loss = 2.41857 avg_loss = 3.47713\n",
      "epoch no.2 train no.204360  loss = 2.90143 avg_loss = 3.45250\n",
      "epoch no.2 train no.204370  loss = 4.99481 avg_loss = 3.46028\n",
      "epoch no.2 train no.204380  loss = 3.26242 avg_loss = 3.45302\n",
      "epoch no.2 train no.204390  loss = 3.20598 avg_loss = 3.51629\n",
      "epoch no.2 train no.204400  loss = 2.62387 avg_loss = 3.51525\n",
      "epoch no.2 train no.204410  loss = 3.89944 avg_loss = 3.51549\n",
      "epoch no.2 train no.204420  loss = 3.00005 avg_loss = 3.49204\n",
      "epoch no.2 train no.204430  loss = 2.99762 avg_loss = 3.51494\n",
      "epoch no.2 train no.204440  loss = 3.07554 avg_loss = 3.55663\n",
      "epoch no.2 train no.204450  loss = 4.21733 avg_loss = 3.58548\n",
      "epoch no.2 train no.204460  loss = 3.45284 avg_loss = 3.57925\n",
      "epoch no.2 train no.204470  loss = 3.03917 avg_loss = 3.52998\n",
      "epoch no.2 train no.204480  loss = 4.03874 avg_loss = 3.51449\n",
      "epoch no.2 train no.204490  loss = 2.89835 avg_loss = 3.49955\n",
      "epoch no.2 train no.204500  loss = 2.68022 avg_loss = 3.50410\n",
      "epoch no.2 train no.204510  loss = 4.00840 avg_loss = 3.52486\n",
      "epoch no.2 train no.204520  loss = 3.42969 avg_loss = 3.49928\n",
      "epoch no.2 train no.204530  loss = 2.57830 avg_loss = 3.54545\n",
      "epoch no.2 train no.204540  loss = 6.51037 avg_loss = 3.57287\n",
      "epoch no.2 train no.204550  loss = 4.22055 avg_loss = 3.55902\n",
      "epoch no.2 train no.204560  loss = 4.38941 avg_loss = 3.61648\n",
      "epoch no.2 train no.204570  loss = 2.76605 avg_loss = 3.58782\n",
      "epoch no.2 train no.204580  loss = 4.81168 avg_loss = 3.56063\n",
      "epoch no.2 train no.204590  loss = 4.66766 avg_loss = 3.59326\n",
      "epoch no.2 train no.204600  loss = 2.83235 avg_loss = 3.58216\n",
      "epoch no.2 train no.204610  loss = 3.87727 avg_loss = 3.51371\n",
      "epoch no.2 train no.204620  loss = 3.77790 avg_loss = 3.53466\n",
      "epoch no.2 train no.204630  loss = 2.12513 avg_loss = 3.49551\n",
      "epoch no.2 train no.204640  loss = 4.35994 avg_loss = 3.53056\n",
      "epoch no.2 train no.204650  loss = 2.70847 avg_loss = 3.54328\n",
      "epoch no.2 train no.204660  loss = 2.84688 avg_loss = 3.49010\n",
      "epoch no.2 train no.204670  loss = 2.88850 avg_loss = 3.44173\n",
      "epoch no.2 train no.204680  loss = 2.77315 avg_loss = 3.43893\n",
      "epoch no.2 train no.204690  loss = 2.47842 avg_loss = 3.44698\n",
      "epoch no.2 train no.204700  loss = 3.42711 avg_loss = 3.43465\n",
      "epoch no.2 train no.204710  loss = 2.71847 avg_loss = 3.40547\n",
      "epoch no.2 train no.204720  loss = 4.03412 avg_loss = 3.40447\n",
      "epoch no.2 train no.204730  loss = 4.59880 avg_loss = 3.38713\n",
      "epoch no.2 train no.204740  loss = 2.65426 avg_loss = 3.41690\n",
      "epoch no.2 train no.204750  loss = 2.63080 avg_loss = 3.41201\n",
      "epoch no.2 train no.204760  loss = 3.34160 avg_loss = 3.43644\n",
      "epoch no.2 train no.204770  loss = 4.16516 avg_loss = 3.48425\n",
      "epoch no.2 train no.204780  loss = 3.05827 avg_loss = 3.48893\n",
      "epoch no.2 train no.204790  loss = 2.24783 avg_loss = 3.49578\n",
      "epoch no.2 train no.204800  loss = 3.59705 avg_loss = 3.51687\n",
      "epoch no.2 train no.204810  loss = 1.45373 avg_loss = 3.52494\n",
      "epoch no.2 train no.204820  loss = 2.54842 avg_loss = 3.50867\n",
      "epoch no.2 train no.204830  loss = 4.99938 avg_loss = 3.51149\n",
      "epoch no.2 train no.204840  loss = 3.08168 avg_loss = 3.47606\n",
      "epoch no.2 train no.204850  loss = 2.77733 avg_loss = 3.45205\n",
      "epoch no.2 train no.204860  loss = 3.56735 avg_loss = 3.45048\n",
      "epoch no.2 train no.204870  loss = 4.87059 avg_loss = 3.41645\n",
      "epoch no.2 train no.204880  loss = 4.00738 avg_loss = 3.40949\n",
      "epoch no.2 train no.204890  loss = 2.86474 avg_loss = 3.41332\n",
      "epoch no.2 train no.204900  loss = 2.62001 avg_loss = 3.48300\n",
      "epoch no.2 train no.204910  loss = 4.90129 avg_loss = 3.43745\n",
      "epoch no.2 train no.204920  loss = 2.01631 avg_loss = 3.41354\n",
      "epoch no.2 train no.204930  loss = 3.71342 avg_loss = 3.42044\n",
      "epoch no.2 train no.204940  loss = 3.39880 avg_loss = 3.41802\n",
      "epoch no.2 train no.204950  loss = 2.84675 avg_loss = 3.44550\n",
      "epoch no.2 train no.204960  loss = 5.22777 avg_loss = 3.45091\n",
      "epoch no.2 train no.204970  loss = 5.02268 avg_loss = 3.46365\n",
      "epoch no.2 train no.204980  loss = 5.58458 avg_loss = 3.52129\n",
      "epoch no.2 train no.204990  loss = 3.61931 avg_loss = 3.45741\n",
      "epoch no.2 train no.205000  loss = 4.23044 avg_loss = 3.51840\n",
      "5\n",
      "to_tokens: ['▁가을', '▁2000', '년대', '▁발라', '▁노래', '드', '</s>']\n",
      "추억의 2000년대 아이돌 발라드</s>\n",
      "epoch no.2 train no.205010  loss = 2.78374 avg_loss = 3.49242\n",
      "epoch no.2 train no.205020  loss = 3.52549 avg_loss = 3.52537\n",
      "epoch no.2 train no.205030  loss = 3.37148 avg_loss = 3.53909\n",
      "epoch no.2 train no.205040  loss = 3.50492 avg_loss = 3.56159\n",
      "epoch no.2 train no.205050  loss = 2.25555 avg_loss = 3.50002\n",
      "epoch no.2 train no.205060  loss = 2.94414 avg_loss = 3.43986\n",
      "epoch no.2 train no.205070  loss = 5.22611 avg_loss = 3.40729\n",
      "epoch no.2 train no.205080  loss = 3.21018 avg_loss = 3.46624\n",
      "epoch no.2 train no.205090  loss = 4.16718 avg_loss = 3.44121\n",
      "epoch no.2 train no.205100  loss = 3.56352 avg_loss = 3.42022\n",
      "epoch no.2 train no.205110  loss = 4.09081 avg_loss = 3.45816\n",
      "epoch no.2 train no.205120  loss = 5.28264 avg_loss = 3.53434\n",
      "epoch no.2 train no.205130  loss = 2.73564 avg_loss = 3.55380\n",
      "epoch no.2 train no.205140  loss = 3.78889 avg_loss = 3.59519\n",
      "epoch no.2 train no.205150  loss = 3.91338 avg_loss = 3.61447\n",
      "epoch no.2 train no.205160  loss = 3.55828 avg_loss = 3.64324\n",
      "epoch no.2 train no.205170  loss = 2.60891 avg_loss = 3.61066\n",
      "epoch no.2 train no.205180  loss = 6.44344 avg_loss = 3.63142\n",
      "epoch no.2 train no.205190  loss = 2.59442 avg_loss = 3.53896\n",
      "epoch no.2 train no.205200  loss = 3.47114 avg_loss = 3.55788\n",
      "epoch no.2 train no.205210  loss = 3.77365 avg_loss = 3.55355\n",
      "epoch no.2 train no.205220  loss = 3.84826 avg_loss = 3.54998\n",
      "epoch no.2 train no.205230  loss = 3.55454 avg_loss = 3.56065\n",
      "epoch no.2 train no.205240  loss = 5.14916 avg_loss = 3.53511\n",
      "epoch no.2 train no.205250  loss = 2.18474 avg_loss = 3.51047\n",
      "epoch no.2 train no.205260  loss = 4.47163 avg_loss = 3.48450\n",
      "epoch no.2 train no.205270  loss = 3.01282 avg_loss = 3.48761\n",
      "epoch no.2 train no.205280  loss = 2.49577 avg_loss = 3.57003\n",
      "epoch no.2 train no.205290  loss = 3.55144 avg_loss = 3.58996\n",
      "epoch no.2 train no.205300  loss = 7.07008 avg_loss = 3.62336\n",
      "epoch no.2 train no.205310  loss = 3.24745 avg_loss = 3.59786\n",
      "epoch no.2 train no.205320  loss = 3.29654 avg_loss = 3.60222\n",
      "epoch no.2 train no.205330  loss = 2.21354 avg_loss = 3.55582\n",
      "epoch no.2 train no.205340  loss = 2.57692 avg_loss = 3.55391\n",
      "epoch no.2 train no.205350  loss = 2.14118 avg_loss = 3.52549\n",
      "epoch no.2 train no.205360  loss = 4.32741 avg_loss = 3.54690\n",
      "epoch no.2 train no.205370  loss = 2.39374 avg_loss = 3.55007\n",
      "epoch no.2 train no.205380  loss = 2.79815 avg_loss = 3.53191\n",
      "epoch no.2 train no.205390  loss = 2.57928 avg_loss = 3.50657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.205400  loss = 4.86547 avg_loss = 3.47927\n",
      "epoch no.2 train no.205410  loss = 2.87543 avg_loss = 3.47958\n",
      "epoch no.2 train no.205420  loss = 3.09543 avg_loss = 3.41889\n",
      "epoch no.2 train no.205430  loss = 5.21805 avg_loss = 3.43947\n",
      "epoch no.2 train no.205440  loss = 2.41831 avg_loss = 3.44083\n",
      "epoch no.2 train no.205450  loss = 1.42734 avg_loss = 3.39694\n",
      "epoch no.2 train no.205460  loss = 2.54527 avg_loss = 3.38695\n",
      "epoch no.2 train no.205470  loss = 3.52534 avg_loss = 3.45337\n",
      "epoch no.2 train no.205480  loss = 4.18741 avg_loss = 3.46671\n",
      "epoch no.2 train no.205490  loss = 6.56139 avg_loss = 3.53134\n",
      "epoch no.2 train no.205500  loss = 3.74531 avg_loss = 3.49279\n",
      "epoch no.2 train no.205510  loss = 4.67109 avg_loss = 3.55568\n",
      "epoch no.2 train no.205520  loss = 4.01869 avg_loss = 3.60983\n",
      "epoch no.2 train no.205530  loss = 2.28219 avg_loss = 3.60120\n",
      "epoch no.2 train no.205540  loss = 2.69295 avg_loss = 3.56550\n",
      "epoch no.2 train no.205550  loss = 3.62230 avg_loss = 3.56521\n",
      "epoch no.2 train no.205560  loss = 3.69396 avg_loss = 3.59170\n",
      "epoch no.2 train no.205570  loss = 2.57675 avg_loss = 3.56766\n",
      "epoch no.2 train no.205580  loss = 2.84718 avg_loss = 3.54363\n",
      "epoch no.2 train no.205590  loss = 4.41509 avg_loss = 3.52644\n",
      "epoch no.2 train no.205600  loss = 3.30709 avg_loss = 3.49233\n",
      "epoch no.2 train no.205610  loss = 2.81656 avg_loss = 3.55039\n",
      "epoch no.2 train no.205620  loss = 3.99564 avg_loss = 3.55297\n",
      "epoch no.2 train no.205630  loss = 2.46099 avg_loss = 3.51899\n",
      "epoch no.2 train no.205640  loss = 6.39534 avg_loss = 3.54914\n",
      "epoch no.2 train no.205650  loss = 2.30341 avg_loss = 3.53034\n",
      "epoch no.2 train no.205660  loss = 4.99918 avg_loss = 3.55613\n",
      "epoch no.2 train no.205670  loss = 3.97749 avg_loss = 3.54761\n",
      "epoch no.2 train no.205680  loss = 3.33497 avg_loss = 3.54741\n",
      "epoch no.2 train no.205690  loss = 7.66767 avg_loss = 3.58686\n",
      "epoch no.2 train no.205700  loss = 4.47369 avg_loss = 3.59965\n",
      "epoch no.2 train no.205710  loss = 3.52892 avg_loss = 3.58934\n",
      "epoch no.2 train no.205720  loss = 2.84395 avg_loss = 3.60065\n",
      "epoch no.2 train no.205730  loss = 3.84010 avg_loss = 3.58233\n",
      "epoch no.2 train no.205740  loss = 2.47627 avg_loss = 3.50525\n",
      "epoch no.2 train no.205750  loss = 5.03183 avg_loss = 3.50142\n",
      "epoch no.2 train no.205760  loss = 3.49301 avg_loss = 3.51028\n",
      "epoch no.2 train no.205770  loss = 3.32249 avg_loss = 3.50101\n",
      "epoch no.2 train no.205780  loss = 1.92412 avg_loss = 3.46444\n",
      "epoch no.2 train no.205790  loss = 3.95245 avg_loss = 3.48389\n",
      "epoch no.2 train no.205800  loss = 3.66486 avg_loss = 3.51835\n",
      "epoch no.2 train no.205810  loss = 5.91030 avg_loss = 3.53228\n",
      "epoch no.2 train no.205820  loss = 3.09611 avg_loss = 3.52109\n",
      "epoch no.2 train no.205830  loss = 4.18204 avg_loss = 3.47756\n",
      "epoch no.2 train no.205840  loss = 5.45609 avg_loss = 3.51569\n",
      "epoch no.2 train no.205850  loss = 4.17943 avg_loss = 3.59822\n",
      "epoch no.2 train no.205860  loss = 3.44602 avg_loss = 3.59704\n",
      "epoch no.2 train no.205870  loss = 3.63022 avg_loss = 3.59890\n",
      "epoch no.2 train no.205880  loss = 2.80930 avg_loss = 3.58186\n",
      "epoch no.2 train no.205890  loss = 2.16306 avg_loss = 3.58852\n",
      "epoch no.2 train no.205900  loss = 4.24015 avg_loss = 3.58455\n",
      "epoch no.2 train no.205910  loss = 2.90273 avg_loss = 3.58849\n",
      "epoch no.2 train no.205920  loss = 5.26187 avg_loss = 3.61370\n",
      "epoch no.2 train no.205930  loss = 2.24902 avg_loss = 3.59347\n",
      "epoch no.2 train no.205940  loss = 3.11387 avg_loss = 3.58869\n",
      "epoch no.2 train no.205950  loss = 2.78642 avg_loss = 3.54636\n",
      "epoch no.2 train no.205960  loss = 4.29681 avg_loss = 3.59774\n",
      "epoch no.2 train no.205970  loss = 2.65277 avg_loss = 3.57549\n",
      "epoch no.2 train no.205980  loss = 3.05523 avg_loss = 3.55829\n",
      "epoch no.2 train no.205990  loss = 2.77233 avg_loss = 3.56949\n",
      "epoch no.2 train no.206000  loss = 2.44108 avg_loss = 3.57001\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.2 train no.206010  loss = 3.39997 avg_loss = 3.58606\n",
      "epoch no.2 train no.206020  loss = 2.21641 avg_loss = 3.56923\n",
      "epoch no.2 train no.206030  loss = 2.06754 avg_loss = 3.56384\n",
      "epoch no.2 train no.206040  loss = 3.15137 avg_loss = 3.56285\n",
      "epoch no.2 train no.206050  loss = 4.07367 avg_loss = 3.56591\n",
      "epoch no.2 train no.206060  loss = 4.17681 avg_loss = 3.56681\n",
      "epoch no.2 train no.206070  loss = 2.21883 avg_loss = 3.57520\n",
      "epoch no.2 train no.206080  loss = 4.28498 avg_loss = 3.58426\n",
      "epoch no.2 train no.206090  loss = 2.12607 avg_loss = 3.55829\n",
      "epoch no.2 train no.206100  loss = 2.74279 avg_loss = 3.54827\n",
      "epoch no.2 train no.206110  loss = 3.36448 avg_loss = 3.52283\n",
      "epoch no.2 train no.206120  loss = 3.07732 avg_loss = 3.49620\n",
      "epoch no.2 train no.206130  loss = 3.89926 avg_loss = 3.51942\n",
      "epoch no.2 train no.206140  loss = 2.36975 avg_loss = 3.49296\n",
      "epoch no.2 train no.206150  loss = 2.66893 avg_loss = 3.52613\n",
      "epoch no.2 train no.206160  loss = 1.87473 avg_loss = 3.51836\n",
      "epoch no.2 train no.206170  loss = 4.35473 avg_loss = 3.53049\n",
      "epoch no.2 train no.206180  loss = 5.08960 avg_loss = 3.55981\n",
      "epoch no.2 train no.206190  loss = 3.06188 avg_loss = 3.57466\n",
      "epoch no.2 train no.206200  loss = 2.55903 avg_loss = 3.57126\n",
      "epoch no.2 train no.206210  loss = 3.94735 avg_loss = 3.57041\n",
      "epoch no.2 train no.206220  loss = 4.37725 avg_loss = 3.57893\n",
      "epoch no.2 train no.206230  loss = 1.80707 avg_loss = 3.56351\n",
      "epoch no.2 train no.206240  loss = 3.34510 avg_loss = 3.54091\n",
      "epoch no.2 train no.206250  loss = 2.20611 avg_loss = 3.58143\n",
      "epoch no.2 train no.206260  loss = 4.05236 avg_loss = 3.55238\n",
      "epoch no.2 train no.206270  loss = 1.72788 avg_loss = 3.55017\n",
      "epoch no.2 train no.206280  loss = 2.69167 avg_loss = 3.52125\n",
      "epoch no.2 train no.206290  loss = 3.85374 avg_loss = 3.53588\n",
      "epoch no.2 train no.206300  loss = 3.97372 avg_loss = 3.50536\n",
      "epoch no.2 train no.206310  loss = 3.83779 avg_loss = 3.52081\n",
      "epoch no.2 train no.206320  loss = 2.60329 avg_loss = 3.46965\n",
      "epoch no.2 train no.206330  loss = 5.93880 avg_loss = 3.52145\n",
      "epoch no.2 train no.206340  loss = 3.39707 avg_loss = 3.57216\n",
      "epoch no.2 train no.206350  loss = 1.73578 avg_loss = 3.52374\n",
      "epoch no.2 train no.206360  loss = 2.63810 avg_loss = 3.50553\n",
      "epoch no.2 train no.206370  loss = 2.36486 avg_loss = 3.50668\n",
      "epoch no.2 train no.206380  loss = 4.45196 avg_loss = 3.50535\n",
      "epoch no.2 train no.206390  loss = 3.76381 avg_loss = 3.51433\n",
      "epoch no.2 train no.206400  loss = 3.57382 avg_loss = 3.46994\n",
      "epoch no.2 train no.206410  loss = 2.73679 avg_loss = 3.44024\n",
      "epoch no.2 train no.206420  loss = 4.32400 avg_loss = 3.43337\n",
      "epoch no.2 train no.206430  loss = 4.26902 avg_loss = 3.43382\n",
      "epoch no.2 train no.206440  loss = 3.00476 avg_loss = 3.48006\n",
      "epoch no.2 train no.206450  loss = 2.60725 avg_loss = 3.48000\n",
      "epoch no.2 train no.206460  loss = 2.60884 avg_loss = 3.47605\n",
      "epoch no.2 train no.206470  loss = 3.07322 avg_loss = 3.49393\n",
      "epoch no.2 train no.206480  loss = 4.17501 avg_loss = 3.50220\n",
      "epoch no.2 train no.206490  loss = 1.88161 avg_loss = 3.48154\n",
      "epoch no.2 train no.206500  loss = 1.71176 avg_loss = 3.45437\n",
      "epoch no.2 train no.206510  loss = 3.37764 avg_loss = 3.39493\n",
      "epoch no.2 train no.206520  loss = 5.84680 avg_loss = 3.36742\n",
      "epoch no.2 train no.206530  loss = 2.83504 avg_loss = 3.35384\n",
      "epoch no.2 train no.206540  loss = 3.87355 avg_loss = 3.38501\n",
      "epoch no.2 train no.206550  loss = 4.38498 avg_loss = 3.37655\n",
      "epoch no.2 train no.206560  loss = 4.83241 avg_loss = 3.42317\n",
      "epoch no.2 train no.206570  loss = 4.75776 avg_loss = 3.48303\n",
      "epoch no.2 train no.206580  loss = 5.47580 avg_loss = 3.49069\n",
      "epoch no.2 train no.206590  loss = 3.85067 avg_loss = 3.52723\n",
      "epoch no.2 train no.206600  loss = 2.90411 avg_loss = 3.49673\n",
      "epoch no.2 train no.206610  loss = 3.22005 avg_loss = 3.51879\n",
      "epoch no.2 train no.206620  loss = 4.82549 avg_loss = 3.54206\n",
      "epoch no.2 train no.206630  loss = 2.53589 avg_loss = 3.52124\n",
      "epoch no.2 train no.206640  loss = 2.43325 avg_loss = 3.54976\n",
      "epoch no.2 train no.206650  loss = 3.11061 avg_loss = 3.56638\n",
      "epoch no.2 train no.206660  loss = 4.94009 avg_loss = 3.57736\n",
      "epoch no.2 train no.206670  loss = 4.47734 avg_loss = 3.53988\n",
      "epoch no.2 train no.206680  loss = 5.20310 avg_loss = 3.52917\n",
      "epoch no.2 train no.206690  loss = 2.46812 avg_loss = 3.51923\n",
      "epoch no.2 train no.206700  loss = 3.31278 avg_loss = 3.47239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.206710  loss = 2.88120 avg_loss = 3.42436\n",
      "epoch no.2 train no.206720  loss = 6.18079 avg_loss = 3.45560\n",
      "epoch no.2 train no.206730  loss = 4.75530 avg_loss = 3.43133\n",
      "epoch no.2 train no.206740  loss = 3.97319 avg_loss = 3.48251\n",
      "epoch no.2 train no.206750  loss = 3.16646 avg_loss = 3.47228\n",
      "epoch no.2 train no.206760  loss = 3.53739 avg_loss = 3.49099\n",
      "epoch no.2 train no.206770  loss = 2.48905 avg_loss = 3.49631\n",
      "epoch no.2 train no.206780  loss = 4.56657 avg_loss = 3.52815\n",
      "epoch no.2 train no.206790  loss = 2.36729 avg_loss = 3.49632\n",
      "epoch no.2 train no.206800  loss = 2.88701 avg_loss = 3.50964\n",
      "epoch no.2 train no.206810  loss = 4.32087 avg_loss = 3.51914\n",
      "epoch no.2 train no.206820  loss = 2.78082 avg_loss = 3.52286\n",
      "epoch no.2 train no.206830  loss = 3.86062 avg_loss = 3.50199\n",
      "epoch no.2 train no.206840  loss = 3.72949 avg_loss = 3.47732\n",
      "epoch no.2 train no.206850  loss = 2.56419 avg_loss = 3.47380\n",
      "epoch no.2 train no.206860  loss = 2.22923 avg_loss = 3.49895\n",
      "epoch no.2 train no.206870  loss = 3.24222 avg_loss = 3.45723\n",
      "epoch no.2 train no.206880  loss = 4.37299 avg_loss = 3.43463\n",
      "epoch no.2 train no.206890  loss = 5.85128 avg_loss = 3.41105\n",
      "epoch no.2 train no.206900  loss = 4.21738 avg_loss = 3.43338\n",
      "epoch no.2 train no.206910  loss = 3.97917 avg_loss = 3.45700\n",
      "epoch no.2 train no.206920  loss = 4.14585 avg_loss = 3.44345\n",
      "epoch no.2 train no.206930  loss = 5.33945 avg_loss = 3.50807\n",
      "epoch no.2 train no.206940  loss = 2.82396 avg_loss = 3.50291\n",
      "epoch no.2 train no.206950  loss = 3.48459 avg_loss = 3.48963\n",
      "epoch no.2 train no.206960  loss = 3.79439 avg_loss = 3.52936\n",
      "epoch no.2 train no.206970  loss = 2.43388 avg_loss = 3.45666\n",
      "epoch no.2 train no.206980  loss = 4.02422 avg_loss = 3.43993\n",
      "epoch no.2 train no.206990  loss = 2.57313 avg_loss = 3.37763\n",
      "epoch no.2 train no.207000  loss = 5.87204 avg_loss = 3.42946\n",
      "3\n",
      "to_tokens: ['▁비', '▁드라마', '들', '▁모음', '</s>']\n",
      "추억의 노래들 추천</s>\n",
      "epoch no.2 train no.207010  loss = 4.15523 avg_loss = 3.44981\n",
      "epoch no.2 train no.207020  loss = 2.80270 avg_loss = 3.45523\n",
      "epoch no.2 train no.207030  loss = 3.05914 avg_loss = 3.47431\n",
      "epoch no.2 train no.207040  loss = 2.77056 avg_loss = 3.51580\n",
      "epoch no.2 train no.207050  loss = 5.06111 avg_loss = 3.58248\n",
      "epoch no.2 train no.207060  loss = 3.10035 avg_loss = 3.53987\n",
      "epoch no.2 train no.207070  loss = 3.04389 avg_loss = 3.48923\n",
      "epoch no.2 train no.207080  loss = 2.68996 avg_loss = 3.46340\n",
      "epoch no.2 train no.207090  loss = 3.60168 avg_loss = 3.49288\n",
      "epoch no.2 train no.207100  loss = 4.57580 avg_loss = 3.51882\n",
      "epoch no.2 train no.207110  loss = 5.12975 avg_loss = 3.49763\n",
      "epoch no.2 train no.207120  loss = 4.64100 avg_loss = 3.49360\n",
      "epoch no.2 train no.207130  loss = 3.06344 avg_loss = 3.52590\n",
      "epoch no.2 train no.207140  loss = 4.70319 avg_loss = 3.55243\n",
      "epoch no.2 train no.207150  loss = 2.32070 avg_loss = 3.60839\n",
      "epoch no.2 train no.207160  loss = 4.01389 avg_loss = 3.58168\n",
      "epoch no.2 train no.207170  loss = 1.44440 avg_loss = 3.56518\n",
      "epoch no.2 train no.207180  loss = 3.77680 avg_loss = 3.54812\n",
      "epoch no.2 train no.207190  loss = 2.91998 avg_loss = 3.54777\n",
      "epoch no.2 train no.207200  loss = 5.12045 avg_loss = 3.52826\n",
      "epoch no.2 train no.207210  loss = 2.77754 avg_loss = 3.53383\n",
      "epoch no.2 train no.207220  loss = 4.40983 avg_loss = 3.49144\n",
      "epoch no.2 train no.207230  loss = 3.09140 avg_loss = 3.47434\n",
      "epoch no.2 train no.207240  loss = 2.34598 avg_loss = 3.46565\n",
      "epoch no.2 train no.207250  loss = 2.42837 avg_loss = 3.40416\n",
      "epoch no.2 train no.207260  loss = 3.30668 avg_loss = 3.37921\n",
      "epoch no.2 train no.207270  loss = 4.19426 avg_loss = 3.39431\n",
      "epoch no.2 train no.207280  loss = 4.74416 avg_loss = 3.45109\n",
      "epoch no.2 train no.207290  loss = 3.79077 avg_loss = 3.45617\n",
      "epoch no.2 train no.207300  loss = 3.93414 avg_loss = 3.49115\n",
      "epoch no.2 train no.207310  loss = 2.88711 avg_loss = 3.54450\n",
      "epoch no.2 train no.207320  loss = 2.15061 avg_loss = 3.51044\n",
      "epoch no.2 train no.207330  loss = 3.38737 avg_loss = 3.52541\n",
      "epoch no.2 train no.207340  loss = 1.90911 avg_loss = 3.53500\n",
      "epoch no.2 train no.207350  loss = 4.78557 avg_loss = 3.55873\n",
      "epoch no.2 train no.207360  loss = 5.21279 avg_loss = 3.58141\n",
      "epoch no.2 train no.207370  loss = 3.22725 avg_loss = 3.58101\n",
      "epoch no.2 train no.207380  loss = 2.08100 avg_loss = 3.50146\n",
      "epoch no.2 train no.207390  loss = 3.88755 avg_loss = 3.48663\n",
      "epoch no.2 train no.207400  loss = 2.92038 avg_loss = 3.48872\n",
      "epoch no.2 train no.207410  loss = 3.12070 avg_loss = 3.49177\n",
      "epoch no.2 train no.207420  loss = 4.80850 avg_loss = 3.51153\n",
      "epoch no.2 train no.207430  loss = 1.95911 avg_loss = 3.52304\n",
      "epoch no.2 train no.207440  loss = 3.58878 avg_loss = 3.49583\n",
      "epoch no.2 train no.207450  loss = 2.79705 avg_loss = 3.47964\n",
      "epoch no.2 train no.207460  loss = 5.31761 avg_loss = 3.54803\n",
      "epoch no.2 train no.207470  loss = 4.18227 avg_loss = 3.49436\n",
      "epoch no.2 train no.207480  loss = 3.06761 avg_loss = 3.50434\n",
      "epoch no.2 train no.207490  loss = 2.17022 avg_loss = 3.45933\n",
      "epoch no.2 train no.207500  loss = 5.37254 avg_loss = 3.44212\n",
      "epoch no.2 train no.207510  loss = 3.26769 avg_loss = 3.43474\n",
      "epoch no.2 train no.207520  loss = 3.13409 avg_loss = 3.42930\n",
      "epoch no.2 train no.207530  loss = 3.96857 avg_loss = 3.39324\n",
      "epoch no.2 train no.207540  loss = 2.78939 avg_loss = 3.42719\n",
      "epoch no.2 train no.207550  loss = 2.73300 avg_loss = 3.41626\n",
      "epoch no.2 train no.207560  loss = 1.96542 avg_loss = 3.41716\n",
      "epoch no.2 train no.207570  loss = 3.55825 avg_loss = 3.49751\n",
      "epoch no.2 train no.207580  loss = 3.51995 avg_loss = 3.48330\n",
      "epoch no.2 train no.207590  loss = 2.32777 avg_loss = 3.46060\n",
      "epoch no.2 train no.207600  loss = 3.62103 avg_loss = 3.48628\n",
      "epoch no.2 train no.207610  loss = 3.18031 avg_loss = 3.55858\n",
      "epoch no.2 train no.207620  loss = 4.02571 avg_loss = 3.59530\n",
      "epoch no.2 train no.207630  loss = 5.24363 avg_loss = 3.58726\n",
      "epoch no.2 train no.207640  loss = 2.58563 avg_loss = 3.61789\n",
      "epoch no.2 train no.207650  loss = 3.86397 avg_loss = 3.59450\n",
      "epoch no.2 train no.207660  loss = 5.82939 avg_loss = 3.64555\n",
      "epoch no.2 train no.207670  loss = 2.14705 avg_loss = 3.63865\n",
      "epoch no.2 train no.207680  loss = 6.65212 avg_loss = 3.67615\n",
      "epoch no.2 train no.207690  loss = 3.22024 avg_loss = 3.65964\n",
      "epoch no.2 train no.207700  loss = 3.19850 avg_loss = 3.65584\n",
      "epoch no.2 train no.207710  loss = 3.27167 avg_loss = 3.61798\n",
      "epoch no.2 train no.207720  loss = 3.73918 avg_loss = 3.61027\n",
      "epoch no.2 train no.207730  loss = 4.10331 avg_loss = 3.61215\n",
      "epoch no.2 train no.207740  loss = 3.46500 avg_loss = 3.63162\n",
      "epoch no.2 train no.207750  loss = 4.00630 avg_loss = 3.68565\n",
      "epoch no.2 train no.207760  loss = 3.03265 avg_loss = 3.67019\n",
      "epoch no.2 train no.207770  loss = 3.46108 avg_loss = 3.60715\n",
      "epoch no.2 train no.207780  loss = 2.75283 avg_loss = 3.59464\n",
      "epoch no.2 train no.207790  loss = 3.71391 avg_loss = 3.58494\n",
      "epoch no.2 train no.207800  loss = 2.75891 avg_loss = 3.55975\n",
      "epoch no.2 train no.207810  loss = 3.61930 avg_loss = 3.54894\n",
      "epoch no.2 train no.207820  loss = 2.76656 avg_loss = 3.55496\n",
      "epoch no.2 train no.207830  loss = 4.36878 avg_loss = 3.51195\n",
      "epoch no.2 train no.207840  loss = 4.65221 avg_loss = 3.47113\n",
      "epoch no.2 train no.207850  loss = 4.56931 avg_loss = 3.46322\n",
      "epoch no.2 train no.207860  loss = 3.12668 avg_loss = 3.43764\n",
      "epoch no.2 train no.207870  loss = 2.33957 avg_loss = 3.41049\n",
      "epoch no.2 train no.207880  loss = 2.43754 avg_loss = 3.42535\n",
      "epoch no.2 train no.207890  loss = 5.51674 avg_loss = 3.45369\n",
      "epoch no.2 train no.207900  loss = 4.42722 avg_loss = 3.45068\n",
      "epoch no.2 train no.207910  loss = 2.67631 avg_loss = 3.42244\n",
      "epoch no.2 train no.207920  loss = 3.27030 avg_loss = 3.40727\n",
      "epoch no.2 train no.207930  loss = 4.80825 avg_loss = 3.46594\n",
      "epoch no.2 train no.207940  loss = 3.63135 avg_loss = 3.45471\n",
      "epoch no.2 train no.207950  loss = 4.60480 avg_loss = 3.44780\n",
      "epoch no.2 train no.207960  loss = 3.67017 avg_loss = 3.44265\n",
      "epoch no.2 train no.207970  loss = 3.29944 avg_loss = 3.45649\n",
      "epoch no.2 train no.207980  loss = 5.95266 avg_loss = 3.47659\n",
      "epoch no.2 train no.207990  loss = 3.14179 avg_loss = 3.48711\n",
      "epoch no.2 train no.208000  loss = 2.41476 avg_loss = 3.43238\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.208010  loss = 2.91859 avg_loss = 3.40870\n",
      "epoch no.2 train no.208020  loss = 2.36783 avg_loss = 3.38931\n",
      "epoch no.2 train no.208030  loss = 3.76387 avg_loss = 3.35680\n",
      "epoch no.2 train no.208040  loss = 2.06678 avg_loss = 3.35030\n",
      "epoch no.2 train no.208050  loss = 3.51932 avg_loss = 3.31188\n",
      "epoch no.2 train no.208060  loss = 3.30993 avg_loss = 3.29143\n",
      "epoch no.2 train no.208070  loss = 4.20145 avg_loss = 3.25061\n",
      "epoch no.2 train no.208080  loss = 4.45328 avg_loss = 3.27839\n",
      "epoch no.2 train no.208090  loss = 3.63405 avg_loss = 3.30887\n",
      "epoch no.2 train no.208100  loss = 2.84556 avg_loss = 3.31683\n",
      "epoch no.2 train no.208110  loss = 2.28731 avg_loss = 3.30717\n",
      "epoch no.2 train no.208120  loss = 2.66038 avg_loss = 3.32543\n",
      "epoch no.2 train no.208130  loss = 3.84689 avg_loss = 3.32134\n",
      "epoch no.2 train no.208140  loss = 4.26301 avg_loss = 3.32897\n",
      "epoch no.2 train no.208150  loss = 4.06963 avg_loss = 3.35948\n",
      "epoch no.2 train no.208160  loss = 2.49268 avg_loss = 3.33490\n",
      "epoch no.2 train no.208170  loss = 3.63317 avg_loss = 3.33864\n",
      "epoch no.2 train no.208180  loss = 3.28296 avg_loss = 3.34018\n",
      "epoch no.2 train no.208190  loss = 4.98506 avg_loss = 3.39858\n",
      "epoch no.2 train no.208200  loss = 2.48485 avg_loss = 3.40816\n",
      "epoch no.2 train no.208210  loss = 4.33209 avg_loss = 3.44372\n",
      "epoch no.2 train no.208220  loss = 2.82274 avg_loss = 3.45351\n",
      "epoch no.2 train no.208230  loss = 1.54129 avg_loss = 3.45731\n",
      "epoch no.2 train no.208240  loss = 4.12289 avg_loss = 3.49672\n",
      "epoch no.2 train no.208250  loss = 4.42430 avg_loss = 3.52265\n",
      "epoch no.2 train no.208260  loss = 3.49368 avg_loss = 3.53418\n",
      "epoch no.2 train no.208270  loss = 1.98536 avg_loss = 3.53992\n",
      "epoch no.2 train no.208280  loss = 3.86090 avg_loss = 3.50902\n",
      "epoch no.2 train no.208290  loss = 2.03261 avg_loss = 3.48698\n",
      "epoch no.2 train no.208300  loss = 4.88012 avg_loss = 3.46598\n",
      "epoch no.2 train no.208310  loss = 4.99433 avg_loss = 3.53377\n",
      "epoch no.2 train no.208320  loss = 3.12307 avg_loss = 3.55548\n",
      "epoch no.2 train no.208330  loss = 4.04556 avg_loss = 3.55722\n",
      "epoch no.2 train no.208340  loss = 2.37313 avg_loss = 3.59154\n",
      "epoch no.2 train no.208350  loss = 2.72218 avg_loss = 3.59072\n",
      "epoch no.2 train no.208360  loss = 2.67181 avg_loss = 3.57950\n",
      "epoch no.2 train no.208370  loss = 3.70652 avg_loss = 3.59602\n",
      "epoch no.2 train no.208380  loss = 2.58990 avg_loss = 3.57065\n",
      "epoch no.2 train no.208390  loss = 5.40487 avg_loss = 3.56570\n",
      "epoch no.2 train no.208400  loss = 2.24644 avg_loss = 3.58555\n",
      "epoch no.2 train no.208410  loss = 4.94613 avg_loss = 3.56167\n",
      "epoch no.2 train no.208420  loss = 4.74658 avg_loss = 3.56941\n",
      "epoch no.2 train no.208430  loss = 2.67476 avg_loss = 3.55696\n",
      "epoch no.2 train no.208440  loss = 2.73928 avg_loss = 3.57360\n",
      "epoch no.2 train no.208450  loss = 2.87349 avg_loss = 3.58918\n",
      "epoch no.2 train no.208460  loss = 3.21089 avg_loss = 3.54318\n",
      "epoch no.2 train no.208470  loss = 3.19114 avg_loss = 3.52359\n",
      "epoch no.2 train no.208480  loss = 1.86889 avg_loss = 3.51209\n",
      "epoch no.2 train no.208490  loss = 2.40412 avg_loss = 3.53311\n",
      "epoch no.2 train no.208500  loss = 3.40062 avg_loss = 3.53994\n",
      "epoch no.2 train no.208510  loss = 2.61120 avg_loss = 3.55938\n",
      "epoch no.2 train no.208520  loss = 2.49953 avg_loss = 3.58084\n",
      "epoch no.2 train no.208530  loss = 1.95196 avg_loss = 3.54428\n",
      "epoch no.2 train no.208540  loss = 3.73609 avg_loss = 3.57141\n",
      "epoch no.2 train no.208550  loss = 4.88382 avg_loss = 3.60321\n",
      "epoch no.2 train no.208560  loss = 4.98249 avg_loss = 3.63489\n",
      "epoch no.2 train no.208570  loss = 3.12330 avg_loss = 3.62162\n",
      "epoch no.2 train no.208580  loss = 5.40131 avg_loss = 3.61143\n",
      "epoch no.2 train no.208590  loss = 6.34933 avg_loss = 3.64172\n",
      "epoch no.2 train no.208600  loss = 3.05037 avg_loss = 3.58817\n",
      "epoch no.2 train no.208610  loss = 3.81858 avg_loss = 3.64121\n",
      "epoch no.2 train no.208620  loss = 5.41678 avg_loss = 3.59763\n",
      "epoch no.2 train no.208630  loss = 4.88298 avg_loss = 3.62202\n",
      "epoch no.2 train no.208640  loss = 2.81998 avg_loss = 3.66637\n",
      "epoch no.2 train no.208650  loss = 4.68673 avg_loss = 3.64164\n",
      "epoch no.2 train no.208660  loss = 4.47933 avg_loss = 3.63573\n",
      "epoch no.2 train no.208670  loss = 4.71968 avg_loss = 3.67542\n",
      "epoch no.2 train no.208680  loss = 2.83789 avg_loss = 3.68037\n",
      "epoch no.2 train no.208690  loss = 2.31335 avg_loss = 3.67852\n",
      "epoch no.2 train no.208700  loss = 4.79544 avg_loss = 3.61658\n",
      "epoch no.2 train no.208710  loss = 3.85362 avg_loss = 3.55747\n",
      "epoch no.2 train no.208720  loss = 2.22944 avg_loss = 3.54112\n",
      "epoch no.2 train no.208730  loss = 2.68624 avg_loss = 3.56692\n",
      "epoch no.2 train no.208740  loss = 1.89322 avg_loss = 3.54734\n",
      "epoch no.2 train no.208750  loss = 3.94089 avg_loss = 3.51526\n",
      "epoch no.2 train no.208760  loss = 3.95190 avg_loss = 3.52055\n",
      "epoch no.2 train no.208770  loss = 4.89907 avg_loss = 3.57049\n",
      "epoch no.2 train no.208780  loss = 4.56819 avg_loss = 3.56520\n",
      "epoch no.2 train no.208790  loss = 4.67168 avg_loss = 3.53426\n",
      "epoch no.2 train no.208800  loss = 6.15885 avg_loss = 3.51591\n",
      "epoch no.2 train no.208810  loss = 2.82639 avg_loss = 3.54168\n",
      "epoch no.2 train no.208820  loss = 3.87952 avg_loss = 3.52868\n",
      "epoch no.2 train no.208830  loss = 2.49438 avg_loss = 3.53528\n",
      "epoch no.2 train no.208840  loss = 3.03815 avg_loss = 3.52681\n",
      "epoch no.2 train no.208850  loss = 2.88232 avg_loss = 3.53525\n",
      "epoch no.2 train no.208860  loss = 3.15835 avg_loss = 3.50680\n",
      "epoch no.2 train no.208870  loss = 3.10308 avg_loss = 3.51713\n",
      "epoch no.2 train no.208880  loss = 4.10411 avg_loss = 3.53133\n",
      "epoch no.2 train no.208890  loss = 3.96433 avg_loss = 3.48685\n",
      "epoch no.2 train no.208900  loss = 2.24600 avg_loss = 3.47029\n",
      "epoch no.2 train no.208910  loss = 2.79970 avg_loss = 3.48957\n",
      "epoch no.2 train no.208920  loss = 3.55218 avg_loss = 3.47596\n",
      "epoch no.2 train no.208930  loss = 3.43033 avg_loss = 3.48042\n",
      "epoch no.2 train no.208940  loss = 2.35769 avg_loss = 3.50161\n",
      "epoch no.2 train no.208950  loss = 4.58899 avg_loss = 3.59311\n",
      "epoch no.2 train no.208960  loss = 3.52819 avg_loss = 3.62576\n",
      "epoch no.2 train no.208970  loss = 1.71382 avg_loss = 3.64848\n",
      "epoch no.2 train no.208980  loss = 3.84778 avg_loss = 3.65860\n",
      "epoch no.2 train no.208990  loss = 2.11318 avg_loss = 3.64395\n",
      "epoch no.2 train no.209000  loss = 2.37276 avg_loss = 3.60706\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁댄스', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.209010  loss = 2.02674 avg_loss = 3.52335\n",
      "epoch no.2 train no.209020  loss = 4.02862 avg_loss = 3.50218\n",
      "epoch no.2 train no.209030  loss = 4.55526 avg_loss = 3.51593\n",
      "epoch no.2 train no.209040  loss = 2.17250 avg_loss = 3.43428\n",
      "epoch no.2 train no.209050  loss = 4.09754 avg_loss = 3.40491\n",
      "epoch no.2 train no.209060  loss = 4.34783 avg_loss = 3.39775\n",
      "epoch no.2 train no.209070  loss = 5.08654 avg_loss = 3.41602\n",
      "epoch no.2 train no.209080  loss = 4.57825 avg_loss = 3.44281\n",
      "epoch no.2 train no.209090  loss = 2.91813 avg_loss = 3.42343\n",
      "epoch no.2 train no.209100  loss = 4.34034 avg_loss = 3.40367\n",
      "epoch no.2 train no.209110  loss = 2.33034 avg_loss = 3.42526\n",
      "epoch no.2 train no.209120  loss = 3.67651 avg_loss = 3.53249\n",
      "epoch no.2 train no.209130  loss = 4.76084 avg_loss = 3.58698\n",
      "epoch no.2 train no.209140  loss = 3.59307 avg_loss = 3.58005\n",
      "epoch no.2 train no.209150  loss = 3.26884 avg_loss = 3.53668\n",
      "epoch no.2 train no.209160  loss = 3.59286 avg_loss = 3.52616\n",
      "epoch no.2 train no.209170  loss = 5.25116 avg_loss = 3.48526\n",
      "epoch no.2 train no.209180  loss = 3.09404 avg_loss = 3.50988\n",
      "epoch no.2 train no.209190  loss = 4.03289 avg_loss = 3.50934\n",
      "epoch no.2 train no.209200  loss = 2.53782 avg_loss = 3.50306\n",
      "epoch no.2 train no.209210  loss = 3.27090 avg_loss = 3.50900\n",
      "epoch no.2 train no.209220  loss = 3.34185 avg_loss = 3.51851\n",
      "epoch no.2 train no.209230  loss = 4.21411 avg_loss = 3.52916\n",
      "epoch no.2 train no.209240  loss = 3.59279 avg_loss = 3.55143\n",
      "epoch no.2 train no.209250  loss = 2.44069 avg_loss = 3.50648\n",
      "epoch no.2 train no.209260  loss = 2.92847 avg_loss = 3.55651\n",
      "epoch no.2 train no.209270  loss = 3.08698 avg_loss = 3.48751\n",
      "epoch no.2 train no.209280  loss = 2.99654 avg_loss = 3.44937\n",
      "epoch no.2 train no.209290  loss = 5.87066 avg_loss = 3.45486\n",
      "epoch no.2 train no.209300  loss = 3.22621 avg_loss = 3.42762\n",
      "epoch no.2 train no.209310  loss = 3.49915 avg_loss = 3.44350\n",
      "epoch no.2 train no.209320  loss = 4.17995 avg_loss = 3.48224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.209330  loss = 2.74249 avg_loss = 3.53361\n",
      "epoch no.2 train no.209340  loss = 3.08521 avg_loss = 3.52645\n",
      "epoch no.2 train no.209350  loss = 4.61511 avg_loss = 3.53445\n",
      "epoch no.2 train no.209360  loss = 4.25046 avg_loss = 3.54779\n",
      "epoch no.2 train no.209370  loss = 2.77573 avg_loss = 3.52838\n",
      "epoch no.2 train no.209380  loss = 4.14057 avg_loss = 3.59556\n",
      "epoch no.2 train no.209390  loss = 2.34013 avg_loss = 3.53812\n",
      "epoch no.2 train no.209400  loss = 3.58785 avg_loss = 3.54610\n",
      "epoch no.2 train no.209410  loss = 4.62838 avg_loss = 3.54057\n",
      "epoch no.2 train no.209420  loss = 1.79854 avg_loss = 3.52167\n",
      "epoch no.2 train no.209430  loss = 2.40877 avg_loss = 3.52321\n",
      "epoch no.2 train no.209440  loss = 1.10717 avg_loss = 3.50487\n",
      "epoch no.2 train no.209450  loss = 4.11802 avg_loss = 3.50620\n",
      "epoch no.2 train no.209460  loss = 5.42096 avg_loss = 3.54974\n",
      "epoch no.2 train no.209470  loss = 3.27834 avg_loss = 3.57256\n",
      "epoch no.2 train no.209480  loss = 3.10519 avg_loss = 3.55970\n",
      "epoch no.2 train no.209490  loss = 2.61754 avg_loss = 3.54859\n",
      "epoch no.2 train no.209500  loss = 2.93402 avg_loss = 3.55838\n",
      "epoch no.2 train no.209510  loss = 2.88970 avg_loss = 3.53048\n",
      "epoch no.2 train no.209520  loss = 2.73572 avg_loss = 3.47004\n",
      "epoch no.2 train no.209530  loss = 4.50055 avg_loss = 3.43604\n",
      "epoch no.2 train no.209540  loss = 2.09098 avg_loss = 3.45325\n",
      "epoch no.2 train no.209550  loss = 3.21864 avg_loss = 3.44361\n",
      "epoch no.2 train no.209560  loss = 3.27831 avg_loss = 3.46822\n",
      "epoch no.2 train no.209570  loss = 3.32043 avg_loss = 3.48598\n",
      "epoch no.2 train no.209580  loss = 2.38834 avg_loss = 3.50495\n",
      "epoch no.2 train no.209590  loss = 3.03547 avg_loss = 3.49068\n",
      "epoch no.2 train no.209600  loss = 3.14450 avg_loss = 3.58324\n",
      "epoch no.2 train no.209610  loss = 2.45853 avg_loss = 3.59881\n",
      "epoch no.2 train no.209620  loss = 3.46723 avg_loss = 3.58626\n",
      "epoch no.2 train no.209630  loss = 3.50628 avg_loss = 3.58291\n",
      "epoch no.2 train no.209640  loss = 2.56380 avg_loss = 3.54637\n",
      "epoch no.2 train no.209650  loss = 3.03700 avg_loss = 3.50937\n",
      "epoch no.2 train no.209660  loss = 4.03561 avg_loss = 3.52787\n",
      "epoch no.2 train no.209670  loss = 3.11015 avg_loss = 3.50805\n",
      "epoch no.2 train no.209680  loss = 3.01553 avg_loss = 3.54703\n",
      "epoch no.2 train no.209690  loss = 3.24287 avg_loss = 3.55775\n",
      "epoch no.2 train no.209700  loss = 4.06314 avg_loss = 3.52708\n",
      "epoch no.2 train no.209710  loss = 4.09071 avg_loss = 3.56723\n",
      "epoch no.2 train no.209720  loss = 2.45596 avg_loss = 3.59298\n",
      "epoch no.2 train no.209730  loss = 3.50454 avg_loss = 3.52665\n",
      "epoch no.2 train no.209740  loss = 3.63795 avg_loss = 3.51671\n",
      "epoch no.2 train no.209750  loss = 6.19436 avg_loss = 3.52974\n",
      "epoch no.2 train no.209760  loss = 3.65501 avg_loss = 3.53302\n",
      "epoch no.2 train no.209770  loss = 3.59158 avg_loss = 3.52333\n",
      "epoch no.2 train no.209780  loss = 3.05048 avg_loss = 3.51534\n",
      "epoch no.2 train no.209790  loss = 2.70930 avg_loss = 3.50995\n",
      "epoch no.2 train no.209800  loss = 6.41263 avg_loss = 3.55377\n",
      "epoch no.2 train no.209810  loss = 3.92282 avg_loss = 3.57222\n",
      "epoch no.2 train no.209820  loss = 4.81879 avg_loss = 3.57478\n",
      "epoch no.2 train no.209830  loss = 5.58016 avg_loss = 3.58519\n",
      "epoch no.2 train no.209840  loss = 2.98707 avg_loss = 3.60342\n",
      "epoch no.2 train no.209850  loss = 2.17115 avg_loss = 3.54911\n",
      "epoch no.2 train no.209860  loss = 2.40332 avg_loss = 3.52793\n",
      "epoch no.2 train no.209870  loss = 5.57906 avg_loss = 3.56696\n",
      "epoch no.2 train no.209880  loss = 3.13163 avg_loss = 3.50968\n",
      "epoch no.2 train no.209890  loss = 2.76403 avg_loss = 3.51211\n",
      "epoch no.2 train no.209900  loss = 5.19270 avg_loss = 3.52620\n",
      "epoch no.2 train no.209910  loss = 3.36972 avg_loss = 3.57555\n",
      "epoch no.2 train no.209920  loss = 2.78565 avg_loss = 3.52009\n",
      "epoch no.2 train no.209930  loss = 3.53523 avg_loss = 3.51616\n",
      "epoch no.2 train no.209940  loss = 3.65081 avg_loss = 3.48865\n",
      "epoch no.2 train no.209950  loss = 3.03207 avg_loss = 3.51700\n",
      "epoch no.2 train no.209960  loss = 3.58619 avg_loss = 3.48092\n",
      "epoch no.2 train no.209970  loss = 2.47253 avg_loss = 3.49448\n",
      "epoch no.2 train no.209980  loss = 2.28535 avg_loss = 3.48412\n",
      "epoch no.2 train no.209990  loss = 3.04571 avg_loss = 3.51735\n",
      "epoch no.2 train no.210000  loss = 4.99531 avg_loss = 3.53358\n",
      "5\n",
      "to_tokens: ['▁비', '▁드라마', '▁시절', '▁그', '▁시절', '▁그', '</s>']\n",
      "추억의 그 때 그 시절로</s>\n",
      "epoch no.2 train no.210010  loss = 3.43378 avg_loss = 3.51721\n",
      "epoch no.2 train no.210020  loss = 4.41583 avg_loss = 3.49844\n",
      "epoch no.2 train no.210030  loss = 2.72022 avg_loss = 3.46298\n",
      "epoch no.2 train no.210040  loss = 2.76447 avg_loss = 3.49235\n",
      "epoch no.2 train no.210050  loss = 1.30713 avg_loss = 3.52347\n",
      "epoch no.2 train no.210060  loss = 3.36854 avg_loss = 3.50122\n",
      "epoch no.2 train no.210070  loss = 2.96378 avg_loss = 3.53003\n",
      "epoch no.2 train no.210080  loss = 5.24077 avg_loss = 3.56243\n",
      "epoch no.2 train no.210090  loss = 4.18514 avg_loss = 3.59895\n",
      "epoch no.2 train no.210100  loss = 2.89874 avg_loss = 3.62863\n",
      "epoch no.2 train no.210110  loss = 3.15315 avg_loss = 3.58888\n",
      "epoch no.2 train no.210120  loss = 2.29851 avg_loss = 3.54656\n",
      "epoch no.2 train no.210130  loss = 1.70750 avg_loss = 3.52983\n",
      "epoch no.2 train no.210140  loss = 2.68202 avg_loss = 3.52994\n",
      "epoch no.2 train no.210150  loss = 3.92814 avg_loss = 3.50717\n",
      "epoch no.2 train no.210160  loss = 4.61482 avg_loss = 3.52284\n",
      "epoch no.2 train no.210170  loss = 2.46198 avg_loss = 3.46413\n",
      "epoch no.2 train no.210180  loss = 4.18661 avg_loss = 3.46411\n",
      "epoch no.2 train no.210190  loss = 4.74428 avg_loss = 3.50157\n",
      "epoch no.2 train no.210200  loss = 2.47462 avg_loss = 3.54114\n",
      "epoch no.2 train no.210210  loss = 6.59040 avg_loss = 3.53427\n",
      "epoch no.2 train no.210220  loss = 1.94494 avg_loss = 3.52303\n",
      "epoch no.2 train no.210230  loss = 3.00575 avg_loss = 3.49398\n",
      "epoch no.2 train no.210240  loss = 3.57217 avg_loss = 3.50377\n",
      "epoch no.2 train no.210250  loss = 5.36538 avg_loss = 3.47046\n",
      "epoch no.2 train no.210260  loss = 4.25808 avg_loss = 3.46229\n",
      "epoch no.2 train no.210270  loss = 2.80755 avg_loss = 3.41884\n",
      "epoch no.2 train no.210280  loss = 4.59420 avg_loss = 3.44310\n",
      "epoch no.2 train no.210290  loss = 4.34598 avg_loss = 3.44476\n",
      "epoch no.2 train no.210300  loss = 3.18645 avg_loss = 3.51680\n",
      "epoch no.2 train no.210310  loss = 2.37711 avg_loss = 3.50334\n",
      "epoch no.2 train no.210320  loss = 4.63107 avg_loss = 3.45770\n",
      "epoch no.2 train no.210330  loss = 3.51253 avg_loss = 3.44271\n",
      "epoch no.2 train no.210340  loss = 2.85382 avg_loss = 3.44574\n",
      "epoch no.2 train no.210350  loss = 4.83912 avg_loss = 3.43713\n",
      "epoch no.2 train no.210360  loss = 1.66331 avg_loss = 3.43212\n",
      "epoch no.2 train no.210370  loss = 2.65636 avg_loss = 3.43371\n",
      "epoch no.2 train no.210380  loss = 3.38582 avg_loss = 3.40164\n",
      "epoch no.2 train no.210390  loss = 4.14908 avg_loss = 3.41511\n",
      "epoch no.2 train no.210400  loss = 5.32235 avg_loss = 3.44782\n",
      "epoch no.2 train no.210410  loss = 2.93815 avg_loss = 3.52453\n",
      "epoch no.2 train no.210420  loss = 4.52653 avg_loss = 3.50759\n",
      "epoch no.2 train no.210430  loss = 4.68040 avg_loss = 3.55628\n",
      "epoch no.2 train no.210440  loss = 2.89640 avg_loss = 3.55812\n",
      "epoch no.2 train no.210450  loss = 5.24146 avg_loss = 3.57995\n",
      "epoch no.2 train no.210460  loss = 3.12349 avg_loss = 3.53357\n",
      "epoch no.2 train no.210470  loss = 2.94425 avg_loss = 3.56990\n",
      "epoch no.2 train no.210480  loss = 2.86412 avg_loss = 3.52843\n",
      "epoch no.2 train no.210490  loss = 4.12749 avg_loss = 3.55487\n",
      "epoch no.2 train no.210500  loss = 3.62335 avg_loss = 3.58123\n",
      "epoch no.2 train no.210510  loss = 5.90592 avg_loss = 3.57647\n",
      "epoch no.2 train no.210520  loss = 3.12728 avg_loss = 3.56619\n",
      "epoch no.2 train no.210530  loss = 5.05062 avg_loss = 3.57417\n",
      "epoch no.2 train no.210540  loss = 4.51155 avg_loss = 3.57429\n",
      "epoch no.2 train no.210550  loss = 2.69817 avg_loss = 3.56611\n",
      "epoch no.2 train no.210560  loss = 3.07661 avg_loss = 3.55694\n",
      "epoch no.2 train no.210570  loss = 4.73966 avg_loss = 3.55612\n",
      "epoch no.2 train no.210580  loss = 4.50705 avg_loss = 3.54047\n",
      "epoch no.2 train no.210590  loss = 3.13411 avg_loss = 3.53203\n",
      "epoch no.2 train no.210600  loss = 2.83903 avg_loss = 3.55643\n",
      "epoch no.2 train no.210610  loss = 2.88236 avg_loss = 3.54589\n",
      "epoch no.2 train no.210620  loss = 3.35094 avg_loss = 3.55399\n",
      "epoch no.2 train no.210630  loss = 5.87670 avg_loss = 3.52839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.210640  loss = 2.20237 avg_loss = 3.54678\n",
      "epoch no.2 train no.210650  loss = 4.35651 avg_loss = 3.55999\n",
      "epoch no.2 train no.210660  loss = 3.19827 avg_loss = 3.62770\n",
      "epoch no.2 train no.210670  loss = 3.90896 avg_loss = 3.68378\n",
      "epoch no.2 train no.210680  loss = 3.75665 avg_loss = 3.72184\n",
      "epoch no.2 train no.210690  loss = 3.09807 avg_loss = 3.68434\n",
      "epoch no.2 train no.210700  loss = 4.63210 avg_loss = 3.64943\n",
      "epoch no.2 train no.210710  loss = 2.74936 avg_loss = 3.64151\n",
      "epoch no.2 train no.210720  loss = 3.72027 avg_loss = 3.64071\n",
      "epoch no.2 train no.210730  loss = 3.93209 avg_loss = 3.64156\n",
      "epoch no.2 train no.210740  loss = 6.47448 avg_loss = 3.70769\n",
      "epoch no.2 train no.210750  loss = 3.60025 avg_loss = 3.70674\n",
      "epoch no.2 train no.210760  loss = 4.00852 avg_loss = 3.73090\n",
      "epoch no.2 train no.210770  loss = 3.52106 avg_loss = 3.68611\n",
      "epoch no.2 train no.210780  loss = 4.48524 avg_loss = 3.67412\n",
      "epoch no.2 train no.210790  loss = 3.73770 avg_loss = 3.67431\n",
      "epoch no.2 train no.210800  loss = 3.79943 avg_loss = 3.65423\n",
      "epoch no.2 train no.210810  loss = 2.77404 avg_loss = 3.65859\n",
      "epoch no.2 train no.210820  loss = 6.80502 avg_loss = 3.63041\n",
      "epoch no.2 train no.210830  loss = 2.74607 avg_loss = 3.57731\n",
      "epoch no.2 train no.210840  loss = 3.31935 avg_loss = 3.53972\n",
      "epoch no.2 train no.210850  loss = 3.04882 avg_loss = 3.54325\n",
      "epoch no.2 train no.210860  loss = 3.86055 avg_loss = 3.59873\n",
      "epoch no.2 train no.210870  loss = 4.85270 avg_loss = 3.57873\n",
      "epoch no.2 train no.210880  loss = 4.53601 avg_loss = 3.63398\n",
      "epoch no.2 train no.210890  loss = 4.23162 avg_loss = 3.64543\n",
      "epoch no.2 train no.210900  loss = 4.39918 avg_loss = 3.63194\n",
      "epoch no.2 train no.210910  loss = 3.13125 avg_loss = 3.57801\n",
      "epoch no.2 train no.210920  loss = 2.77044 avg_loss = 3.61720\n",
      "epoch no.2 train no.210930  loss = 1.95221 avg_loss = 3.60208\n",
      "epoch no.2 train no.210940  loss = 3.54852 avg_loss = 3.62111\n",
      "epoch no.2 train no.210950  loss = 3.78845 avg_loss = 3.67093\n",
      "epoch no.2 train no.210960  loss = 3.81447 avg_loss = 3.66944\n",
      "epoch no.2 train no.210970  loss = 4.30850 avg_loss = 3.64091\n",
      "epoch no.2 train no.210980  loss = 2.68235 avg_loss = 3.62799\n",
      "epoch no.2 train no.210990  loss = 4.11244 avg_loss = 3.63864\n",
      "epoch no.2 train no.211000  loss = 4.99528 avg_loss = 3.61934\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '▁때', '▁그', '▁노래', '</s>']\n",
      "추억의 그 시절 그 노래</s>\n",
      "epoch no.2 train no.211010  loss = 3.06522 avg_loss = 3.64092\n",
      "epoch no.2 train no.211020  loss = 4.36648 avg_loss = 3.64352\n",
      "epoch no.2 train no.211030  loss = 5.19885 avg_loss = 3.67910\n",
      "epoch no.2 train no.211040  loss = 2.99265 avg_loss = 3.64459\n",
      "epoch no.2 train no.211050  loss = 2.89642 avg_loss = 3.62252\n",
      "epoch no.2 train no.211060  loss = 4.49171 avg_loss = 3.61990\n",
      "epoch no.2 train no.211070  loss = 2.44162 avg_loss = 3.59435\n",
      "epoch no.2 train no.211080  loss = 2.62171 avg_loss = 3.52008\n",
      "epoch no.2 train no.211090  loss = 2.73579 avg_loss = 3.56380\n",
      "epoch no.2 train no.211100  loss = 1.78954 avg_loss = 3.54908\n",
      "epoch no.2 train no.211110  loss = 2.08754 avg_loss = 3.60735\n",
      "epoch no.2 train no.211120  loss = 3.04820 avg_loss = 3.60043\n",
      "epoch no.2 train no.211130  loss = 3.73126 avg_loss = 3.57475\n",
      "epoch no.2 train no.211140  loss = 4.33205 avg_loss = 3.54547\n",
      "epoch no.2 train no.211150  loss = 3.31092 avg_loss = 3.59740\n",
      "epoch no.2 train no.211160  loss = 3.55326 avg_loss = 3.58481\n",
      "epoch no.2 train no.211170  loss = 3.84951 avg_loss = 3.54724\n",
      "epoch no.2 train no.211180  loss = 3.54312 avg_loss = 3.50024\n",
      "epoch no.2 train no.211190  loss = 2.88421 avg_loss = 3.50196\n",
      "epoch no.2 train no.211200  loss = 2.70007 avg_loss = 3.51858\n",
      "epoch no.2 train no.211210  loss = 3.99465 avg_loss = 3.52502\n",
      "epoch no.2 train no.211220  loss = 4.85423 avg_loss = 3.53976\n",
      "epoch no.2 train no.211230  loss = 2.84985 avg_loss = 3.55624\n",
      "epoch no.2 train no.211240  loss = 4.71842 avg_loss = 3.60765\n",
      "epoch no.2 train no.211250  loss = 5.94572 avg_loss = 3.62910\n",
      "epoch no.2 train no.211260  loss = 4.42047 avg_loss = 3.61426\n",
      "epoch no.2 train no.211270  loss = 2.61944 avg_loss = 3.58482\n",
      "epoch no.2 train no.211280  loss = 1.85619 avg_loss = 3.57468\n",
      "epoch no.2 train no.211290  loss = 3.09543 avg_loss = 3.52419\n",
      "epoch no.2 train no.211300  loss = 4.33822 avg_loss = 3.49986\n",
      "epoch no.2 train no.211310  loss = 3.19419 avg_loss = 3.48092\n",
      "epoch no.2 train no.211320  loss = 6.43091 avg_loss = 3.51318\n",
      "epoch no.2 train no.211330  loss = 3.34143 avg_loss = 3.53160\n",
      "epoch no.2 train no.211340  loss = 3.92426 avg_loss = 3.50124\n",
      "epoch no.2 train no.211350  loss = 4.18548 avg_loss = 3.51467\n",
      "epoch no.2 train no.211360  loss = 2.08901 avg_loss = 3.49656\n",
      "epoch no.2 train no.211370  loss = 2.15155 avg_loss = 3.44159\n",
      "epoch no.2 train no.211380  loss = 2.03382 avg_loss = 3.44073\n",
      "epoch no.2 train no.211390  loss = 3.56096 avg_loss = 3.49016\n",
      "epoch no.2 train no.211400  loss = 2.43062 avg_loss = 3.46053\n",
      "epoch no.2 train no.211410  loss = 4.04977 avg_loss = 3.48615\n",
      "epoch no.2 train no.211420  loss = 3.13858 avg_loss = 3.48666\n",
      "epoch no.2 train no.211430  loss = 4.18523 avg_loss = 3.47252\n",
      "epoch no.2 train no.211440  loss = 3.22690 avg_loss = 3.48384\n",
      "epoch no.2 train no.211450  loss = 4.47386 avg_loss = 3.50692\n",
      "epoch no.2 train no.211460  loss = 2.41120 avg_loss = 3.47243\n",
      "epoch no.2 train no.211470  loss = 2.57746 avg_loss = 3.43424\n",
      "epoch no.2 train no.211480  loss = 5.38426 avg_loss = 3.45969\n",
      "epoch no.2 train no.211490  loss = 4.20524 avg_loss = 3.49610\n",
      "epoch no.2 train no.211500  loss = 2.60822 avg_loss = 3.58223\n",
      "epoch no.2 train no.211510  loss = 3.83761 avg_loss = 3.59082\n",
      "epoch no.2 train no.211520  loss = 2.24463 avg_loss = 3.55148\n",
      "epoch no.2 train no.211530  loss = 3.50359 avg_loss = 3.55619\n",
      "epoch no.2 train no.211540  loss = 4.51516 avg_loss = 3.54019\n",
      "epoch no.2 train no.211550  loss = 2.69367 avg_loss = 3.57124\n",
      "epoch no.2 train no.211560  loss = 4.41488 avg_loss = 3.64051\n",
      "epoch no.2 train no.211570  loss = 4.06799 avg_loss = 3.68905\n",
      "epoch no.2 train no.211580  loss = 2.35401 avg_loss = 3.65578\n",
      "epoch no.2 train no.211590  loss = 2.98917 avg_loss = 3.62750\n",
      "epoch no.2 train no.211600  loss = 3.85849 avg_loss = 3.60445\n",
      "epoch no.2 train no.211610  loss = 5.71513 avg_loss = 3.61939\n",
      "epoch no.2 train no.211620  loss = 3.78955 avg_loss = 3.62779\n",
      "epoch no.2 train no.211630  loss = 4.49241 avg_loss = 3.59848\n",
      "epoch no.2 train no.211640  loss = 4.11693 avg_loss = 3.57889\n",
      "epoch no.2 train no.211650  loss = 1.93251 avg_loss = 3.55961\n",
      "epoch no.2 train no.211660  loss = 2.89483 avg_loss = 3.53201\n",
      "epoch no.2 train no.211670  loss = 3.22145 avg_loss = 3.47960\n",
      "epoch no.2 train no.211680  loss = 3.58922 avg_loss = 3.50707\n",
      "epoch no.2 train no.211690  loss = 3.93390 avg_loss = 3.51568\n",
      "epoch no.2 train no.211700  loss = 1.94732 avg_loss = 3.50793\n",
      "epoch no.2 train no.211710  loss = 3.69712 avg_loss = 3.50672\n",
      "epoch no.2 train no.211720  loss = 3.49064 avg_loss = 3.46432\n",
      "epoch no.2 train no.211730  loss = 3.07688 avg_loss = 3.45332\n",
      "epoch no.2 train no.211740  loss = 2.88482 avg_loss = 3.43484\n",
      "epoch no.2 train no.211750  loss = 4.42345 avg_loss = 3.42222\n",
      "epoch no.2 train no.211760  loss = 6.93531 avg_loss = 3.48021\n",
      "epoch no.2 train no.211770  loss = 3.47864 avg_loss = 3.51984\n",
      "epoch no.2 train no.211780  loss = 3.08471 avg_loss = 3.46677\n",
      "epoch no.2 train no.211790  loss = 2.60040 avg_loss = 3.44675\n",
      "epoch no.2 train no.211800  loss = 2.99259 avg_loss = 3.44515\n",
      "epoch no.2 train no.211810  loss = 2.56737 avg_loss = 3.39437\n",
      "epoch no.2 train no.211820  loss = 3.25531 avg_loss = 3.38909\n",
      "epoch no.2 train no.211830  loss = 5.39474 avg_loss = 3.44783\n",
      "epoch no.2 train no.211840  loss = 3.68685 avg_loss = 3.43997\n",
      "epoch no.2 train no.211850  loss = 2.52813 avg_loss = 3.46381\n",
      "epoch no.2 train no.211860  loss = 3.17987 avg_loss = 3.49239\n",
      "epoch no.2 train no.211870  loss = 2.64733 avg_loss = 3.56299\n",
      "epoch no.2 train no.211880  loss = 2.35062 avg_loss = 3.58735\n",
      "epoch no.2 train no.211890  loss = 3.10081 avg_loss = 3.56810\n",
      "epoch no.2 train no.211900  loss = 3.45574 avg_loss = 3.57576\n",
      "epoch no.2 train no.211910  loss = 3.31706 avg_loss = 3.59213\n",
      "epoch no.2 train no.211920  loss = 1.48822 avg_loss = 3.54405\n",
      "epoch no.2 train no.211930  loss = 4.15934 avg_loss = 3.51624\n",
      "epoch no.2 train no.211940  loss = 3.12923 avg_loss = 3.49851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.211950  loss = 3.06386 avg_loss = 3.47490\n",
      "epoch no.2 train no.211960  loss = 4.27605 avg_loss = 3.47405\n",
      "epoch no.2 train no.211970  loss = 2.11413 avg_loss = 3.42312\n",
      "epoch no.2 train no.211980  loss = 1.51849 avg_loss = 3.40346\n",
      "epoch no.2 train no.211990  loss = 2.73134 avg_loss = 3.40201\n",
      "epoch no.2 train no.212000  loss = 3.26416 avg_loss = 3.41766\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.212010  loss = 4.00928 avg_loss = 3.47404\n",
      "epoch no.2 train no.212020  loss = 3.39864 avg_loss = 3.51457\n",
      "epoch no.2 train no.212030  loss = 6.05747 avg_loss = 3.56044\n",
      "epoch no.2 train no.212040  loss = 2.85560 avg_loss = 3.55806\n",
      "epoch no.2 train no.212050  loss = 3.38335 avg_loss = 3.52870\n",
      "epoch no.2 train no.212060  loss = 3.29426 avg_loss = 3.52249\n",
      "epoch no.2 train no.212070  loss = 5.71865 avg_loss = 3.58520\n",
      "epoch no.2 train no.212080  loss = 3.44522 avg_loss = 3.61604\n",
      "epoch no.2 train no.212090  loss = 4.11311 avg_loss = 3.59390\n",
      "epoch no.2 train no.212100  loss = 4.03183 avg_loss = 3.57307\n",
      "epoch no.2 train no.212110  loss = 3.51284 avg_loss = 3.58534\n",
      "epoch no.2 train no.212120  loss = 3.50450 avg_loss = 3.53170\n",
      "epoch no.2 train no.212130  loss = 2.18859 avg_loss = 3.50241\n",
      "epoch no.2 train no.212140  loss = 2.80779 avg_loss = 3.48908\n",
      "epoch no.2 train no.212150  loss = 3.19147 avg_loss = 3.45348\n",
      "epoch no.2 train no.212160  loss = 2.61819 avg_loss = 3.37504\n",
      "epoch no.2 train no.212170  loss = 2.32022 avg_loss = 3.36038\n",
      "epoch no.2 train no.212180  loss = 3.30398 avg_loss = 3.37580\n",
      "epoch no.2 train no.212190  loss = 3.16985 avg_loss = 3.39047\n",
      "epoch no.2 train no.212200  loss = 5.13927 avg_loss = 3.41549\n",
      "epoch no.2 train no.212210  loss = 3.68680 avg_loss = 3.41490\n",
      "epoch no.2 train no.212220  loss = 3.63754 avg_loss = 3.44140\n",
      "epoch no.2 train no.212230  loss = 4.46466 avg_loss = 3.47069\n",
      "epoch no.2 train no.212240  loss = 3.12762 avg_loss = 3.54163\n",
      "epoch no.2 train no.212250  loss = 4.45005 avg_loss = 3.57285\n",
      "epoch no.2 train no.212260  loss = 3.09494 avg_loss = 3.59237\n",
      "epoch no.2 train no.212270  loss = 5.76695 avg_loss = 3.65681\n",
      "epoch no.2 train no.212280  loss = 2.97383 avg_loss = 3.70105\n",
      "epoch no.2 train no.212290  loss = 3.07221 avg_loss = 3.68853\n",
      "epoch no.2 train no.212300  loss = 5.88208 avg_loss = 3.65788\n",
      "epoch no.2 train no.212310  loss = 2.28185 avg_loss = 3.63076\n",
      "epoch no.2 train no.212320  loss = 2.53216 avg_loss = 3.63936\n",
      "epoch no.2 train no.212330  loss = 3.61351 avg_loss = 3.64158\n",
      "epoch no.2 train no.212340  loss = 2.15787 avg_loss = 3.59907\n",
      "epoch no.2 train no.212350  loss = 2.76978 avg_loss = 3.59367\n",
      "epoch no.2 train no.212360  loss = 4.56008 avg_loss = 3.57836\n",
      "epoch no.2 train no.212370  loss = 3.70754 avg_loss = 3.54819\n",
      "epoch no.2 train no.212380  loss = 2.60639 avg_loss = 3.52447\n",
      "epoch no.2 train no.212390  loss = 2.18445 avg_loss = 3.48390\n",
      "epoch no.2 train no.212400  loss = 3.16229 avg_loss = 3.49904\n",
      "epoch no.2 train no.212410  loss = 3.87490 avg_loss = 3.53115\n",
      "epoch no.2 train no.212420  loss = 1.71709 avg_loss = 3.48891\n",
      "epoch no.2 train no.212430  loss = 3.31120 avg_loss = 3.45262\n",
      "epoch no.2 train no.212440  loss = 2.27330 avg_loss = 3.45114\n",
      "epoch no.2 train no.212450  loss = 5.81774 avg_loss = 3.52069\n",
      "epoch no.2 train no.212460  loss = 4.47678 avg_loss = 3.53785\n",
      "epoch no.2 train no.212470  loss = 3.44006 avg_loss = 3.52109\n",
      "epoch no.2 train no.212480  loss = 3.22645 avg_loss = 3.56775\n",
      "epoch no.2 train no.212490  loss = 3.78843 avg_loss = 3.58332\n",
      "epoch no.2 train no.212500  loss = 2.11146 avg_loss = 3.52475\n",
      "epoch no.2 train no.212510  loss = 5.17797 avg_loss = 3.58386\n",
      "epoch no.2 train no.212520  loss = 5.44333 avg_loss = 3.55053\n",
      "epoch no.2 train no.212530  loss = 4.00199 avg_loss = 3.56244\n",
      "epoch no.2 train no.212540  loss = 2.70705 avg_loss = 3.55652\n",
      "epoch no.2 train no.212550  loss = 3.99579 avg_loss = 3.55727\n",
      "epoch no.2 train no.212560  loss = 3.01476 avg_loss = 3.55294\n",
      "epoch no.2 train no.212570  loss = 4.78722 avg_loss = 3.58287\n",
      "epoch no.2 train no.212580  loss = 3.18279 avg_loss = 3.56690\n",
      "epoch no.2 train no.212590  loss = 3.29043 avg_loss = 3.57501\n",
      "epoch no.2 train no.212600  loss = 3.35680 avg_loss = 3.60939\n",
      "epoch no.2 train no.212610  loss = 2.69143 avg_loss = 3.62631\n",
      "epoch no.2 train no.212620  loss = 2.49123 avg_loss = 3.59000\n",
      "epoch no.2 train no.212630  loss = 3.63654 avg_loss = 3.54063\n",
      "epoch no.2 train no.212640  loss = 4.37284 avg_loss = 3.59887\n",
      "epoch no.2 train no.212650  loss = 4.37893 avg_loss = 3.56981\n",
      "epoch no.2 train no.212660  loss = 4.67675 avg_loss = 3.57003\n",
      "epoch no.2 train no.212670  loss = 4.23232 avg_loss = 3.57973\n",
      "epoch no.2 train no.212680  loss = 4.28014 avg_loss = 3.56926\n",
      "epoch no.2 train no.212690  loss = 2.86503 avg_loss = 3.54876\n",
      "epoch no.2 train no.212700  loss = 3.85971 avg_loss = 3.51218\n",
      "epoch no.2 train no.212710  loss = 3.99726 avg_loss = 3.50284\n",
      "epoch no.2 train no.212720  loss = 3.28746 avg_loss = 3.47960\n",
      "epoch no.2 train no.212730  loss = 2.03407 avg_loss = 3.46821\n",
      "epoch no.2 train no.212740  loss = 2.51991 avg_loss = 3.41153\n",
      "epoch no.2 train no.212750  loss = 3.34760 avg_loss = 3.39937\n",
      "epoch no.2 train no.212760  loss = 3.62865 avg_loss = 3.39963\n",
      "epoch no.2 train no.212770  loss = 3.37134 avg_loss = 3.39046\n",
      "epoch no.2 train no.212780  loss = 4.00462 avg_loss = 3.34281\n",
      "epoch no.2 train no.212790  loss = 2.48432 avg_loss = 3.35787\n",
      "epoch no.2 train no.212800  loss = 3.17382 avg_loss = 3.40160\n",
      "epoch no.2 train no.212810  loss = 3.49825 avg_loss = 3.42430\n",
      "epoch no.2 train no.212820  loss = 3.77669 avg_loss = 3.38560\n",
      "epoch no.2 train no.212830  loss = 3.57528 avg_loss = 3.43535\n",
      "epoch no.2 train no.212840  loss = 3.70618 avg_loss = 3.47882\n",
      "epoch no.2 train no.212850  loss = 3.19555 avg_loss = 3.46183\n",
      "epoch no.2 train no.212860  loss = 2.24228 avg_loss = 3.43253\n",
      "epoch no.2 train no.212870  loss = 5.58011 avg_loss = 3.42175\n",
      "epoch no.2 train no.212880  loss = 4.15286 avg_loss = 3.45167\n",
      "epoch no.2 train no.212890  loss = 5.89520 avg_loss = 3.44549\n",
      "epoch no.2 train no.212900  loss = 3.39061 avg_loss = 3.46674\n",
      "epoch no.2 train no.212910  loss = 2.03395 avg_loss = 3.46710\n",
      "epoch no.2 train no.212920  loss = 3.54498 avg_loss = 3.47353\n",
      "epoch no.2 train no.212930  loss = 2.15574 avg_loss = 3.51657\n",
      "epoch no.2 train no.212940  loss = 3.47595 avg_loss = 3.48523\n",
      "epoch no.2 train no.212950  loss = 3.08770 avg_loss = 3.51667\n",
      "epoch no.2 train no.212960  loss = 3.29406 avg_loss = 3.50521\n",
      "epoch no.2 train no.212970  loss = 2.77673 avg_loss = 3.50996\n",
      "epoch no.2 train no.212980  loss = 3.23431 avg_loss = 3.52389\n",
      "epoch no.2 train no.212990  loss = 2.42699 avg_loss = 3.53048\n",
      "epoch no.2 train no.213000  loss = 4.62603 avg_loss = 3.53987\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.2 train no.213010  loss = 2.15596 avg_loss = 3.51855\n",
      "epoch no.2 train no.213020  loss = 3.83392 avg_loss = 3.56619\n",
      "epoch no.2 train no.213030  loss = 3.78241 avg_loss = 3.57928\n",
      "epoch no.2 train no.213040  loss = 1.32201 avg_loss = 3.59958\n",
      "epoch no.2 train no.213050  loss = 3.40219 avg_loss = 3.58640\n",
      "epoch no.2 train no.213060  loss = 3.58184 avg_loss = 3.53722\n",
      "epoch no.2 train no.213070  loss = 2.46044 avg_loss = 3.49730\n",
      "epoch no.2 train no.213080  loss = 5.08291 avg_loss = 3.46230\n",
      "epoch no.2 train no.213090  loss = 3.43218 avg_loss = 3.50684\n",
      "epoch no.2 train no.213100  loss = 4.32986 avg_loss = 3.58653\n",
      "epoch no.2 train no.213110  loss = 2.02058 avg_loss = 3.54709\n",
      "epoch no.2 train no.213120  loss = 3.34229 avg_loss = 3.54997\n",
      "epoch no.2 train no.213130  loss = 6.90240 avg_loss = 3.58548\n",
      "epoch no.2 train no.213140  loss = 3.23909 avg_loss = 3.55742\n",
      "epoch no.2 train no.213150  loss = 2.56427 avg_loss = 3.62966\n",
      "epoch no.2 train no.213160  loss = 3.53500 avg_loss = 3.56809\n",
      "epoch no.2 train no.213170  loss = 2.89890 avg_loss = 3.53281\n",
      "epoch no.2 train no.213180  loss = 2.15177 avg_loss = 3.46818\n",
      "epoch no.2 train no.213190  loss = 3.88273 avg_loss = 3.45842\n",
      "epoch no.2 train no.213200  loss = 3.96756 avg_loss = 3.47662\n",
      "epoch no.2 train no.213210  loss = 1.21568 avg_loss = 3.42174\n",
      "epoch no.2 train no.213220  loss = 3.77319 avg_loss = 3.43254\n",
      "epoch no.2 train no.213230  loss = 2.22382 avg_loss = 3.49533\n",
      "epoch no.2 train no.213240  loss = 2.81790 avg_loss = 3.51082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.213250  loss = 6.30566 avg_loss = 3.57777\n",
      "epoch no.2 train no.213260  loss = 3.69029 avg_loss = 3.52079\n",
      "epoch no.2 train no.213270  loss = 4.67388 avg_loss = 3.55248\n",
      "epoch no.2 train no.213280  loss = 2.51946 avg_loss = 3.54370\n",
      "epoch no.2 train no.213290  loss = 4.63711 avg_loss = 3.54819\n",
      "epoch no.2 train no.213300  loss = 4.95472 avg_loss = 3.55726\n",
      "epoch no.2 train no.213310  loss = 3.80171 avg_loss = 3.56118\n",
      "epoch no.2 train no.213320  loss = 5.56329 avg_loss = 3.59618\n",
      "epoch no.2 train no.213330  loss = 4.82372 avg_loss = 3.54061\n",
      "epoch no.2 train no.213340  loss = 4.39800 avg_loss = 3.50440\n",
      "epoch no.2 train no.213350  loss = 2.96222 avg_loss = 3.55622\n",
      "epoch no.2 train no.213360  loss = 4.20671 avg_loss = 3.53046\n",
      "epoch no.2 train no.213370  loss = 4.61319 avg_loss = 3.55517\n",
      "epoch no.2 train no.213380  loss = 5.06822 avg_loss = 3.56188\n",
      "epoch no.2 train no.213390  loss = 2.76492 avg_loss = 3.52635\n",
      "epoch no.2 train no.213400  loss = 5.37728 avg_loss = 3.53047\n",
      "epoch no.2 train no.213410  loss = 2.60859 avg_loss = 3.55593\n",
      "epoch no.2 train no.213420  loss = 3.24092 avg_loss = 3.54951\n",
      "epoch no.2 train no.213430  loss = 2.64620 avg_loss = 3.52502\n",
      "epoch no.2 train no.213440  loss = 4.34582 avg_loss = 3.54398\n",
      "epoch no.2 train no.213450  loss = 2.97333 avg_loss = 3.52678\n",
      "epoch no.2 train no.213460  loss = 2.68806 avg_loss = 3.52222\n",
      "epoch no.2 train no.213470  loss = 3.10218 avg_loss = 3.46429\n",
      "epoch no.2 train no.213480  loss = 4.73256 avg_loss = 3.48383\n",
      "epoch no.2 train no.213490  loss = 3.49452 avg_loss = 3.42844\n",
      "epoch no.2 train no.213500  loss = 4.21542 avg_loss = 3.44304\n",
      "epoch no.2 train no.213510  loss = 1.91199 avg_loss = 3.39540\n",
      "epoch no.2 train no.213520  loss = 2.87652 avg_loss = 3.36620\n",
      "epoch no.2 train no.213530  loss = 2.62147 avg_loss = 3.38212\n",
      "epoch no.2 train no.213540  loss = 4.20044 avg_loss = 3.36368\n",
      "epoch no.2 train no.213550  loss = 2.49933 avg_loss = 3.34333\n",
      "epoch no.2 train no.213560  loss = 2.25980 avg_loss = 3.36733\n",
      "epoch no.2 train no.213570  loss = 3.70986 avg_loss = 3.35880\n",
      "epoch no.2 train no.213580  loss = 4.18106 avg_loss = 3.37399\n",
      "epoch no.2 train no.213590  loss = 3.03159 avg_loss = 3.40424\n",
      "epoch no.2 train no.213600  loss = 4.86366 avg_loss = 3.42929\n",
      "epoch no.2 train no.213610  loss = 3.89264 avg_loss = 3.42059\n",
      "epoch no.2 train no.213620  loss = 2.26363 avg_loss = 3.41008\n",
      "epoch no.2 train no.213630  loss = 2.86702 avg_loss = 3.40528\n",
      "epoch no.2 train no.213640  loss = 5.09243 avg_loss = 3.42303\n",
      "epoch no.2 train no.213650  loss = 2.65344 avg_loss = 3.42837\n",
      "epoch no.2 train no.213660  loss = 2.16163 avg_loss = 3.46762\n",
      "epoch no.2 train no.213670  loss = 4.76430 avg_loss = 3.49032\n",
      "epoch no.2 train no.213680  loss = 4.47571 avg_loss = 3.50779\n",
      "epoch no.2 train no.213690  loss = 3.54956 avg_loss = 3.46747\n",
      "epoch no.2 train no.213700  loss = 3.34125 avg_loss = 3.43566\n",
      "epoch no.2 train no.213710  loss = 5.96976 avg_loss = 3.52185\n",
      "epoch no.2 train no.213720  loss = 3.11463 avg_loss = 3.52806\n",
      "epoch no.2 train no.213730  loss = 5.71232 avg_loss = 3.52485\n",
      "epoch no.2 train no.213740  loss = 2.75556 avg_loss = 3.54005\n",
      "epoch no.2 train no.213750  loss = 1.45148 avg_loss = 3.49519\n",
      "epoch no.2 train no.213760  loss = 3.12243 avg_loss = 3.49128\n",
      "epoch no.2 train no.213770  loss = 3.93773 avg_loss = 3.47815\n",
      "epoch no.2 train no.213780  loss = 5.21325 avg_loss = 3.50584\n",
      "epoch no.2 train no.213790  loss = 3.20046 avg_loss = 3.52174\n",
      "epoch no.2 train no.213800  loss = 2.32038 avg_loss = 3.48357\n",
      "epoch no.2 train no.213810  loss = 2.72061 avg_loss = 3.47175\n",
      "epoch no.2 train no.213820  loss = 3.76495 avg_loss = 3.55798\n",
      "epoch no.2 train no.213830  loss = 2.44971 avg_loss = 3.50015\n",
      "epoch no.2 train no.213840  loss = 4.15355 avg_loss = 3.52688\n",
      "epoch no.2 train no.213850  loss = 2.71754 avg_loss = 3.50540\n",
      "epoch no.2 train no.213860  loss = 1.99575 avg_loss = 3.49764\n",
      "epoch no.2 train no.213870  loss = 4.19979 avg_loss = 3.53685\n",
      "epoch no.2 train no.213880  loss = 3.00871 avg_loss = 3.48769\n",
      "epoch no.2 train no.213890  loss = 3.02903 avg_loss = 3.44646\n",
      "epoch no.2 train no.213900  loss = 1.59051 avg_loss = 3.45036\n",
      "epoch no.2 train no.213910  loss = 2.75583 avg_loss = 3.48110\n",
      "epoch no.2 train no.213920  loss = 2.74511 avg_loss = 3.49939\n",
      "epoch no.2 train no.213930  loss = 4.50946 avg_loss = 3.49631\n",
      "epoch no.2 train no.213940  loss = 3.73954 avg_loss = 3.52389\n",
      "epoch no.2 train no.213950  loss = 3.40791 avg_loss = 3.45750\n",
      "epoch no.2 train no.213960  loss = 3.66344 avg_loss = 3.46344\n",
      "epoch no.2 train no.213970  loss = 2.59706 avg_loss = 3.47878\n",
      "epoch no.2 train no.213980  loss = 2.59425 avg_loss = 3.44379\n",
      "epoch no.2 train no.213990  loss = 4.27859 avg_loss = 3.48942\n",
      "epoch no.2 train no.214000  loss = 1.64615 avg_loss = 3.50728\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.2 train no.214010  loss = 2.25311 avg_loss = 3.49504\n",
      "epoch no.2 train no.214020  loss = 3.29742 avg_loss = 3.50042\n",
      "epoch no.2 train no.214030  loss = 2.46097 avg_loss = 3.46400\n",
      "epoch no.2 train no.214040  loss = 2.46778 avg_loss = 3.46076\n",
      "epoch no.2 train no.214050  loss = 4.15087 avg_loss = 3.49081\n",
      "epoch no.2 train no.214060  loss = 2.92266 avg_loss = 3.49013\n",
      "epoch no.2 train no.214070  loss = 4.29489 avg_loss = 3.54580\n",
      "epoch no.2 train no.214080  loss = 2.72035 avg_loss = 3.53601\n",
      "epoch no.2 train no.214090  loss = 3.28620 avg_loss = 3.52229\n",
      "epoch no.2 train no.214100  loss = 3.68390 avg_loss = 3.47811\n",
      "epoch no.2 train no.214110  loss = 4.86131 avg_loss = 3.51902\n",
      "epoch no.2 train no.214120  loss = 3.17681 avg_loss = 3.54587\n",
      "epoch no.2 train no.214130  loss = 2.63460 avg_loss = 3.54816\n",
      "epoch no.2 train no.214140  loss = 2.82623 avg_loss = 3.55941\n",
      "epoch no.2 train no.214150  loss = 2.80135 avg_loss = 3.60858\n",
      "epoch no.2 train no.214160  loss = 4.23318 avg_loss = 3.66765\n",
      "epoch no.2 train no.214170  loss = 2.74424 avg_loss = 3.69977\n",
      "epoch no.2 train no.214180  loss = 2.61575 avg_loss = 3.71582\n",
      "epoch no.2 train no.214190  loss = 5.01770 avg_loss = 3.76003\n",
      "epoch no.2 train no.214200  loss = 3.01726 avg_loss = 3.75943\n",
      "epoch no.2 train no.214210  loss = 3.42021 avg_loss = 3.71012\n",
      "epoch no.2 train no.214220  loss = 2.98742 avg_loss = 3.66807\n",
      "epoch no.2 train no.214230  loss = 4.36055 avg_loss = 3.64608\n",
      "epoch no.2 train no.214240  loss = 4.20266 avg_loss = 3.71992\n",
      "epoch no.2 train no.214250  loss = 5.89555 avg_loss = 3.72922\n",
      "epoch no.2 train no.214260  loss = 3.69563 avg_loss = 3.73443\n",
      "epoch no.2 train no.214270  loss = 4.58952 avg_loss = 3.74896\n",
      "epoch no.2 train no.214280  loss = 3.01681 avg_loss = 3.70014\n",
      "epoch no.2 train no.214290  loss = 3.20431 avg_loss = 3.68416\n",
      "epoch no.2 train no.214300  loss = 2.89377 avg_loss = 3.69491\n",
      "epoch no.2 train no.214310  loss = 5.45386 avg_loss = 3.67663\n",
      "epoch no.2 train no.214320  loss = 2.80707 avg_loss = 3.63614\n",
      "epoch no.2 train no.214330  loss = 2.01416 avg_loss = 3.69477\n",
      "epoch no.2 train no.214340  loss = 2.57694 avg_loss = 3.66990\n",
      "epoch no.2 train no.214350  loss = 2.52970 avg_loss = 3.58829\n",
      "epoch no.2 train no.214360  loss = 2.98866 avg_loss = 3.60508\n",
      "epoch no.2 train no.214370  loss = 3.14081 avg_loss = 3.56284\n",
      "epoch no.2 train no.214380  loss = 2.99769 avg_loss = 3.55883\n",
      "epoch no.2 train no.214390  loss = 3.88455 avg_loss = 3.51344\n",
      "epoch no.2 train no.214400  loss = 3.37514 avg_loss = 3.49221\n",
      "epoch no.2 train no.214410  loss = 2.69513 avg_loss = 3.52950\n",
      "epoch no.2 train no.214420  loss = 2.71173 avg_loss = 3.51696\n",
      "epoch no.2 train no.214430  loss = 4.96347 avg_loss = 3.59119\n",
      "epoch no.2 train no.214440  loss = 3.63597 avg_loss = 3.52869\n",
      "epoch no.2 train no.214450  loss = 3.87966 avg_loss = 3.51535\n",
      "epoch no.2 train no.214460  loss = 2.52192 avg_loss = 3.52445\n",
      "epoch no.2 train no.214470  loss = 5.26544 avg_loss = 3.49473\n",
      "epoch no.2 train no.214480  loss = 2.53086 avg_loss = 3.42364\n",
      "epoch no.2 train no.214490  loss = 4.00873 avg_loss = 3.43921\n",
      "epoch no.2 train no.214500  loss = 2.78168 avg_loss = 3.41533\n",
      "epoch no.2 train no.214510  loss = 3.55152 avg_loss = 3.42938\n",
      "epoch no.2 train no.214520  loss = 2.62940 avg_loss = 3.39251\n",
      "epoch no.2 train no.214530  loss = 2.01271 avg_loss = 3.40588\n",
      "epoch no.2 train no.214540  loss = 2.17778 avg_loss = 3.39563\n",
      "epoch no.2 train no.214550  loss = 3.11722 avg_loss = 3.42835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.214560  loss = 4.53362 avg_loss = 3.39237\n",
      "epoch no.2 train no.214570  loss = 4.75159 avg_loss = 3.37952\n",
      "epoch no.2 train no.214580  loss = 2.79069 avg_loss = 3.44479\n",
      "epoch no.2 train no.214590  loss = 4.22300 avg_loss = 3.43124\n",
      "epoch no.2 train no.214600  loss = 2.68129 avg_loss = 3.38947\n",
      "epoch no.2 train no.214610  loss = 3.85805 avg_loss = 3.35184\n",
      "epoch no.2 train no.214620  loss = 6.13082 avg_loss = 3.39077\n",
      "epoch no.2 train no.214630  loss = 2.39196 avg_loss = 3.33920\n",
      "epoch no.2 train no.214640  loss = 4.01972 avg_loss = 3.35734\n",
      "epoch no.2 train no.214650  loss = 3.12829 avg_loss = 3.37113\n",
      "epoch no.2 train no.214660  loss = 3.65734 avg_loss = 3.38123\n",
      "epoch no.2 train no.214670  loss = 2.65755 avg_loss = 3.42991\n",
      "epoch no.2 train no.214680  loss = 3.74416 avg_loss = 3.45468\n",
      "epoch no.2 train no.214690  loss = 3.71304 avg_loss = 3.51148\n",
      "epoch no.2 train no.214700  loss = 2.37061 avg_loss = 3.53936\n",
      "epoch no.2 train no.214710  loss = 2.11074 avg_loss = 3.51099\n",
      "epoch no.2 train no.214720  loss = 1.94391 avg_loss = 3.44877\n",
      "epoch no.2 train no.214730  loss = 3.58357 avg_loss = 3.45865\n",
      "epoch no.2 train no.214740  loss = 5.91575 avg_loss = 3.43404\n",
      "epoch no.2 train no.214750  loss = 3.50947 avg_loss = 3.47929\n",
      "epoch no.2 train no.214760  loss = 2.35510 avg_loss = 3.50436\n",
      "epoch no.2 train no.214770  loss = 3.57337 avg_loss = 3.50811\n",
      "epoch no.2 train no.214780  loss = 2.70720 avg_loss = 3.44408\n",
      "epoch no.2 train no.214790  loss = 3.87204 avg_loss = 3.44446\n",
      "epoch no.2 train no.214800  loss = 2.78042 avg_loss = 3.44110\n",
      "epoch no.2 train no.214810  loss = 6.08456 avg_loss = 3.46451\n",
      "epoch no.2 train no.214820  loss = 1.55685 avg_loss = 3.45727\n",
      "epoch no.2 train no.214830  loss = 3.59145 avg_loss = 3.44717\n",
      "epoch no.2 train no.214840  loss = 3.05056 avg_loss = 3.45135\n",
      "epoch no.2 train no.214850  loss = 2.36478 avg_loss = 3.39794\n",
      "epoch no.2 train no.214860  loss = 2.29905 avg_loss = 3.40786\n",
      "epoch no.2 train no.214870  loss = 4.57269 avg_loss = 3.42351\n",
      "epoch no.2 train no.214880  loss = 3.63004 avg_loss = 3.39687\n",
      "epoch no.2 train no.214890  loss = 2.68758 avg_loss = 3.41550\n",
      "epoch no.2 train no.214900  loss = 2.73828 avg_loss = 3.41873\n",
      "epoch no.2 train no.214910  loss = 3.22926 avg_loss = 3.44876\n",
      "epoch no.2 train no.214920  loss = 2.48736 avg_loss = 3.41521\n",
      "epoch no.2 train no.214930  loss = 3.57938 avg_loss = 3.43800\n",
      "epoch no.2 train no.214940  loss = 6.54727 avg_loss = 3.49855\n",
      "epoch no.2 train no.214950  loss = 2.18417 avg_loss = 3.44765\n",
      "epoch no.2 train no.214960  loss = 2.99005 avg_loss = 3.46685\n",
      "epoch no.2 train no.214970  loss = 3.07539 avg_loss = 3.47141\n",
      "epoch no.2 train no.214980  loss = 3.61415 avg_loss = 3.45304\n",
      "epoch no.2 train no.214990  loss = 4.29093 avg_loss = 3.47970\n",
      "epoch no.2 train no.215000  loss = 2.99254 avg_loss = 3.45763\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁모음', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.2 train no.215010  loss = 2.16951 avg_loss = 3.43670\n",
      "epoch no.2 train no.215020  loss = 3.32210 avg_loss = 3.47096\n",
      "epoch no.2 train no.215030  loss = 7.31729 avg_loss = 3.47850\n",
      "epoch no.2 train no.215040  loss = 2.64812 avg_loss = 3.49239\n",
      "epoch no.2 train no.215050  loss = 3.63185 avg_loss = 3.49769\n",
      "epoch no.2 train no.215060  loss = 2.96383 avg_loss = 3.49200\n",
      "epoch no.2 train no.215070  loss = 2.47714 avg_loss = 3.47686\n",
      "epoch no.2 train no.215080  loss = 3.20871 avg_loss = 3.45185\n",
      "epoch no.2 train no.215090  loss = 2.55629 avg_loss = 3.45384\n",
      "epoch no.2 train no.215100  loss = 2.29753 avg_loss = 3.51010\n",
      "epoch no.2 train no.215110  loss = 3.68547 avg_loss = 3.49770\n",
      "epoch no.2 train no.215120  loss = 4.62255 avg_loss = 3.52362\n",
      "epoch no.2 train no.215130  loss = 3.10428 avg_loss = 3.50869\n",
      "epoch no.2 train no.215140  loss = 4.11582 avg_loss = 3.49894\n",
      "epoch no.2 train no.215150  loss = 4.26932 avg_loss = 3.56934\n",
      "epoch no.2 train no.215160  loss = 5.38620 avg_loss = 3.59359\n",
      "epoch no.2 train no.215170  loss = 3.41768 avg_loss = 3.60752\n",
      "epoch no.2 train no.215180  loss = 1.61446 avg_loss = 3.52513\n",
      "epoch no.2 train no.215190  loss = 2.19445 avg_loss = 3.50342\n",
      "epoch no.2 train no.215200  loss = 2.65965 avg_loss = 3.46533\n",
      "epoch no.2 train no.215210  loss = 4.47410 avg_loss = 3.49084\n",
      "epoch no.2 train no.215220  loss = 5.78510 avg_loss = 3.54051\n",
      "epoch no.2 train no.215230  loss = 4.24903 avg_loss = 3.54694\n",
      "epoch no.2 train no.215240  loss = 3.47258 avg_loss = 3.49773\n",
      "epoch no.2 train no.215250  loss = 3.32118 avg_loss = 3.50609\n",
      "epoch no.2 train no.215260  loss = 5.61751 avg_loss = 3.56565\n",
      "epoch no.2 train no.215270  loss = 5.41900 avg_loss = 3.57807\n",
      "epoch no.2 train no.215280  loss = 2.76825 avg_loss = 3.60791\n",
      "epoch no.2 train no.215290  loss = 3.56042 avg_loss = 3.61266\n",
      "epoch no.2 train no.215300  loss = 4.08697 avg_loss = 3.58894\n",
      "epoch no.2 train no.215310  loss = 2.69802 avg_loss = 3.60584\n",
      "epoch no.2 train no.215320  loss = 3.46449 avg_loss = 3.57693\n",
      "epoch no.2 train no.215330  loss = 4.16933 avg_loss = 3.56418\n",
      "epoch no.2 train no.215340  loss = 3.69649 avg_loss = 3.60011\n",
      "epoch no.2 train no.215350  loss = 4.23847 avg_loss = 3.53236\n",
      "epoch no.2 train no.215360  loss = 2.13021 avg_loss = 3.49671\n",
      "epoch no.2 train no.215370  loss = 3.12694 avg_loss = 3.47755\n",
      "epoch no.2 train no.215380  loss = 2.46246 avg_loss = 3.45720\n",
      "epoch no.2 train no.215390  loss = 2.29107 avg_loss = 3.44265\n",
      "epoch no.2 train no.215400  loss = 2.88412 avg_loss = 3.42044\n",
      "epoch no.2 train no.215410  loss = 3.00222 avg_loss = 3.41522\n",
      "epoch no.2 train no.215420  loss = 3.97559 avg_loss = 3.42516\n",
      "epoch no.2 train no.215430  loss = 5.02054 avg_loss = 3.46272\n",
      "epoch no.2 train no.215440  loss = 4.93392 avg_loss = 3.55289\n",
      "epoch no.2 train no.215450  loss = 2.54932 avg_loss = 3.49223\n",
      "epoch no.2 train no.215460  loss = 5.92940 avg_loss = 3.52872\n",
      "epoch no.2 train no.215470  loss = 2.91259 avg_loss = 3.50475\n",
      "epoch no.2 train no.215480  loss = 3.71666 avg_loss = 3.51909\n",
      "epoch no.2 train no.215490  loss = 2.62731 avg_loss = 3.50524\n",
      "epoch no.2 train no.215500  loss = 3.13257 avg_loss = 3.58915\n",
      "epoch no.2 train no.215510  loss = 3.02145 avg_loss = 3.57145\n",
      "epoch no.2 train no.215520  loss = 1.92030 avg_loss = 3.58137\n",
      "epoch no.2 train no.215530  loss = 3.78400 avg_loss = 3.58726\n",
      "epoch no.2 train no.215540  loss = 2.99952 avg_loss = 3.61998\n",
      "epoch no.2 train no.215550  loss = 3.32831 avg_loss = 3.57105\n",
      "epoch no.2 train no.215560  loss = 5.40372 avg_loss = 3.62643\n",
      "epoch no.2 train no.215570  loss = 3.13293 avg_loss = 3.64083\n",
      "epoch no.2 train no.215580  loss = 2.67567 avg_loss = 3.57572\n",
      "epoch no.2 train no.215590  loss = 6.31014 avg_loss = 3.58653\n",
      "epoch no.2 train no.215600  loss = 3.19764 avg_loss = 3.56795\n",
      "epoch no.2 train no.215610  loss = 4.65055 avg_loss = 3.53186\n",
      "epoch no.2 train no.215620  loss = 2.49200 avg_loss = 3.45794\n",
      "epoch no.2 train no.215630  loss = 3.98309 avg_loss = 3.44127\n",
      "epoch no.2 train no.215640  loss = 3.35728 avg_loss = 3.41309\n",
      "epoch no.2 train no.215650  loss = 4.89268 avg_loss = 3.44465\n",
      "epoch no.2 train no.215660  loss = 2.86698 avg_loss = 3.46597\n",
      "epoch no.2 train no.215670  loss = 2.14558 avg_loss = 3.45428\n",
      "epoch no.2 train no.215680  loss = 2.89695 avg_loss = 3.46271\n",
      "epoch no.2 train no.215690  loss = 2.73015 avg_loss = 3.48248\n",
      "epoch no.2 train no.215700  loss = 3.74631 avg_loss = 3.52210\n",
      "epoch no.2 train no.215710  loss = 3.03988 avg_loss = 3.54628\n",
      "epoch no.2 train no.215720  loss = 2.37171 avg_loss = 3.50121\n",
      "epoch no.2 train no.215730  loss = 3.55817 avg_loss = 3.48596\n",
      "epoch no.2 train no.215740  loss = 3.78013 avg_loss = 3.54902\n",
      "epoch no.2 train no.215750  loss = 4.70055 avg_loss = 3.56424\n",
      "epoch no.2 train no.215760  loss = 1.89181 avg_loss = 3.55912\n",
      "epoch no.2 train no.215770  loss = 2.19264 avg_loss = 3.54434\n",
      "epoch no.2 train no.215780  loss = 3.62741 avg_loss = 3.55368\n",
      "epoch no.2 train no.215790  loss = 3.44474 avg_loss = 3.57240\n",
      "epoch no.2 train no.215800  loss = 3.01144 avg_loss = 3.58495\n",
      "epoch no.2 train no.215810  loss = 3.25843 avg_loss = 3.56988\n",
      "epoch no.2 train no.215820  loss = 4.08431 avg_loss = 3.55095\n",
      "epoch no.2 train no.215830  loss = 2.66932 avg_loss = 3.54276\n",
      "epoch no.2 train no.215840  loss = 2.76356 avg_loss = 3.50611\n",
      "epoch no.2 train no.215850  loss = 5.80712 avg_loss = 3.54453\n",
      "epoch no.2 train no.215860  loss = 3.95264 avg_loss = 3.54681\n",
      "epoch no.2 train no.215870  loss = 2.93785 avg_loss = 3.55468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.215880  loss = 4.03169 avg_loss = 3.54110\n",
      "epoch no.2 train no.215890  loss = 2.82215 avg_loss = 3.54257\n",
      "epoch no.2 train no.215900  loss = 4.09200 avg_loss = 3.55564\n",
      "epoch no.2 train no.215910  loss = 4.04819 avg_loss = 3.55749\n",
      "epoch no.2 train no.215920  loss = 1.68179 avg_loss = 3.53942\n",
      "epoch no.2 train no.215930  loss = 5.46707 avg_loss = 3.51323\n",
      "epoch no.2 train no.215940  loss = 5.14443 avg_loss = 3.57567\n",
      "epoch no.2 train no.215950  loss = 3.42206 avg_loss = 3.56285\n",
      "epoch no.2 train no.215960  loss = 4.76562 avg_loss = 3.57097\n",
      "epoch no.2 train no.215970  loss = 3.80471 avg_loss = 3.51716\n",
      "epoch no.2 train no.215980  loss = 2.05681 avg_loss = 3.46850\n",
      "epoch no.2 train no.215990  loss = 3.76803 avg_loss = 3.50223\n",
      "epoch no.2 train no.216000  loss = 4.13496 avg_loss = 3.54917\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.2 train no.216010  loss = 2.17306 avg_loss = 3.53977\n",
      "epoch no.2 train no.216020  loss = 3.33789 avg_loss = 3.48217\n",
      "epoch no.2 train no.216030  loss = 1.43434 avg_loss = 3.43879\n",
      "epoch no.2 train no.216040  loss = 3.82495 avg_loss = 3.41217\n",
      "epoch no.2 train no.216050  loss = 5.58853 avg_loss = 3.45289\n",
      "epoch no.2 train no.216060  loss = 3.75556 avg_loss = 3.44800\n",
      "epoch no.2 train no.216070  loss = 2.42359 avg_loss = 3.43244\n",
      "epoch no.2 train no.216080  loss = 5.28601 avg_loss = 3.42291\n",
      "epoch no.2 train no.216090  loss = 3.93133 avg_loss = 3.40877\n",
      "epoch no.2 train no.216100  loss = 3.45136 avg_loss = 3.43465\n",
      "epoch no.2 train no.216110  loss = 3.74450 avg_loss = 3.40989\n",
      "epoch no.2 train no.216120  loss = 5.99183 avg_loss = 3.45998\n",
      "epoch no.2 train no.216130  loss = 3.78766 avg_loss = 3.48528\n",
      "epoch no.2 train no.216140  loss = 2.55269 avg_loss = 3.48172\n",
      "epoch no.2 train no.216150  loss = 4.94478 avg_loss = 3.51840\n",
      "epoch no.2 train no.216160  loss = 3.57357 avg_loss = 3.53314\n",
      "epoch no.2 train no.216170  loss = 4.40530 avg_loss = 3.54191\n",
      "epoch no.2 train no.216180  loss = 6.47663 avg_loss = 3.57816\n",
      "epoch no.2 train no.216190  loss = 4.04132 avg_loss = 3.53518\n",
      "epoch no.2 train no.216200  loss = 4.77533 avg_loss = 3.56382\n",
      "epoch no.2 train no.216210  loss = 2.89377 avg_loss = 3.53806\n",
      "epoch no.2 train no.216220  loss = 4.41619 avg_loss = 3.57706\n",
      "epoch no.2 train no.216230  loss = 4.00490 avg_loss = 3.56680\n",
      "epoch no.2 train no.216240  loss = 4.39337 avg_loss = 3.53007\n",
      "epoch no.2 train no.216250  loss = 1.76335 avg_loss = 3.49710\n",
      "epoch no.2 train no.216260  loss = 2.51888 avg_loss = 3.43823\n",
      "epoch no.2 train no.216270  loss = 1.65814 avg_loss = 3.38349\n",
      "epoch no.2 train no.216280  loss = 2.86949 avg_loss = 3.36926\n",
      "epoch no.2 train no.216290  loss = 5.34874 avg_loss = 3.43031\n",
      "epoch no.2 train no.216300  loss = 3.28845 avg_loss = 3.43910\n",
      "epoch no.2 train no.216310  loss = 3.16349 avg_loss = 3.36861\n",
      "epoch no.2 train no.216320  loss = 3.55590 avg_loss = 3.40864\n",
      "epoch no.2 train no.216330  loss = 4.12539 avg_loss = 3.36232\n",
      "epoch no.2 train no.216340  loss = 4.32010 avg_loss = 3.33013\n",
      "epoch no.2 train no.216350  loss = 2.59064 avg_loss = 3.31494\n",
      "epoch no.2 train no.216360  loss = 2.28117 avg_loss = 3.35767\n",
      "epoch no.2 train no.216370  loss = 4.42308 avg_loss = 3.39567\n",
      "epoch no.2 train no.216380  loss = 4.46374 avg_loss = 3.39231\n",
      "epoch no.2 train no.216390  loss = 3.33586 avg_loss = 3.37957\n",
      "epoch no.2 train no.216400  loss = 3.85325 avg_loss = 3.44757\n",
      "epoch no.2 train no.216410  loss = 2.56084 avg_loss = 3.38357\n",
      "epoch no.2 train no.216420  loss = 3.70862 avg_loss = 3.39178\n",
      "epoch no.2 train no.216430  loss = 4.43126 avg_loss = 3.37352\n",
      "epoch no.2 train no.216440  loss = 2.24355 avg_loss = 3.37038\n",
      "epoch no.2 train no.216450  loss = 2.87917 avg_loss = 3.41422\n",
      "epoch no.2 train no.216460  loss = 4.14219 avg_loss = 3.39497\n",
      "epoch no.2 train no.216470  loss = 4.25135 avg_loss = 3.40336\n",
      "epoch no.2 train no.216480  loss = 4.42789 avg_loss = 3.46300\n",
      "epoch no.2 train no.216490  loss = 4.06230 avg_loss = 3.45817\n",
      "epoch no.2 train no.216500  loss = 3.05365 avg_loss = 3.47548\n",
      "epoch no.2 train no.216510  loss = 4.46037 avg_loss = 3.50696\n",
      "epoch no.2 train no.216520  loss = 3.25651 avg_loss = 3.52739\n",
      "epoch no.2 train no.216530  loss = 2.25249 avg_loss = 3.48579\n",
      "epoch no.2 train no.216540  loss = 2.43882 avg_loss = 3.45767\n",
      "epoch no.2 train no.216550  loss = 4.53585 avg_loss = 3.48081\n",
      "epoch no.2 train no.216560  loss = 3.52492 avg_loss = 3.49995\n",
      "epoch no.2 train no.216570  loss = 2.92635 avg_loss = 3.49682\n",
      "epoch no.2 train no.216580  loss = 4.61538 avg_loss = 3.52984\n",
      "epoch no.2 train no.216590  loss = 2.24757 avg_loss = 3.54772\n",
      "epoch no.2 train no.216600  loss = 4.80386 avg_loss = 3.57792\n",
      "epoch no.2 train no.216610  loss = 5.31737 avg_loss = 3.54492\n",
      "epoch no.2 train no.216620  loss = 3.40012 avg_loss = 3.60252\n",
      "epoch no.2 train no.216630  loss = 4.86595 avg_loss = 3.59738\n",
      "epoch no.2 train no.216640  loss = 5.15208 avg_loss = 3.57239\n",
      "epoch no.2 train no.216650  loss = 3.64306 avg_loss = 3.55736\n",
      "epoch no.2 train no.216660  loss = 4.68758 avg_loss = 3.57673\n",
      "epoch no.2 train no.216670  loss = 2.49242 avg_loss = 3.51958\n",
      "epoch no.2 train no.216680  loss = 4.53072 avg_loss = 3.53147\n",
      "epoch no.2 train no.216690  loss = 2.40555 avg_loss = 3.57956\n",
      "epoch no.2 train no.216700  loss = 4.13927 avg_loss = 3.59322\n",
      "epoch no.2 train no.216710  loss = 2.74643 avg_loss = 3.59161\n",
      "epoch no.2 train no.216720  loss = 2.42189 avg_loss = 3.55271\n",
      "epoch no.2 train no.216730  loss = 4.31441 avg_loss = 3.52079\n",
      "epoch no.2 train no.216740  loss = 3.83499 avg_loss = 3.52133\n",
      "epoch no.2 train no.216750  loss = 2.71259 avg_loss = 3.51379\n",
      "epoch no.2 train no.216760  loss = 3.40305 avg_loss = 3.55090\n",
      "epoch no.2 train no.216770  loss = 3.58538 avg_loss = 3.53260\n",
      "epoch no.2 train no.216780  loss = 4.13186 avg_loss = 3.45580\n",
      "epoch no.2 train no.216790  loss = 3.68331 avg_loss = 3.47797\n",
      "epoch no.2 train no.216800  loss = 4.12642 avg_loss = 3.49885\n",
      "epoch no.2 train no.216810  loss = 4.10395 avg_loss = 3.47984\n",
      "epoch no.2 train no.216820  loss = 5.28550 avg_loss = 3.49559\n",
      "epoch no.2 train no.216830  loss = 2.54059 avg_loss = 3.53039\n",
      "epoch no.2 train no.216840  loss = 6.79475 avg_loss = 3.51671\n",
      "epoch no.2 train no.216850  loss = 5.34512 avg_loss = 3.51291\n",
      "epoch no.2 train no.216860  loss = 4.43876 avg_loss = 3.53037\n",
      "epoch no.2 train no.216870  loss = 2.55813 avg_loss = 3.51962\n",
      "epoch no.2 train no.216880  loss = 3.78731 avg_loss = 3.53669\n",
      "epoch no.2 train no.216890  loss = 2.42430 avg_loss = 3.50596\n",
      "epoch no.2 train no.216900  loss = 3.41774 avg_loss = 3.46836\n",
      "epoch no.2 train no.216910  loss = 4.21600 avg_loss = 3.52444\n",
      "epoch no.2 train no.216920  loss = 3.14926 avg_loss = 3.48107\n",
      "epoch no.2 train no.216930  loss = 2.61285 avg_loss = 3.50745\n",
      "epoch no.2 train no.216940  loss = 3.32513 avg_loss = 3.46988\n",
      "epoch no.2 train no.216950  loss = 3.18144 avg_loss = 3.45382\n",
      "epoch no.2 train no.216960  loss = 3.16143 avg_loss = 3.46774\n",
      "epoch no.2 train no.216970  loss = 4.03664 avg_loss = 3.52602\n",
      "epoch no.2 train no.216980  loss = 4.75146 avg_loss = 3.54905\n",
      "epoch no.2 train no.216990  loss = 3.87226 avg_loss = 3.56948\n",
      "epoch no.2 train no.217000  loss = 4.90379 avg_loss = 3.63127\n",
      "4\n",
      "to_tokens: ['▁가을', '▁명', '▁시절', '▁그', '곡', '</s>']\n",
      "추억의 그시절 명곡</s>\n",
      "epoch no.2 train no.217010  loss = 2.73011 avg_loss = 3.54347\n",
      "epoch no.2 train no.217020  loss = 4.65210 avg_loss = 3.56388\n",
      "epoch no.2 train no.217030  loss = 3.34655 avg_loss = 3.57692\n",
      "epoch no.2 train no.217040  loss = 3.95657 avg_loss = 3.58424\n",
      "epoch no.2 train no.217050  loss = 4.51433 avg_loss = 3.55144\n",
      "epoch no.2 train no.217060  loss = 3.42074 avg_loss = 3.55644\n",
      "epoch no.2 train no.217070  loss = 2.38312 avg_loss = 3.51481\n",
      "epoch no.2 train no.217080  loss = 3.06940 avg_loss = 3.52606\n",
      "epoch no.2 train no.217090  loss = 2.74894 avg_loss = 3.51609\n",
      "epoch no.2 train no.217100  loss = 3.47659 avg_loss = 3.52789\n",
      "epoch no.2 train no.217110  loss = 3.84170 avg_loss = 3.57640\n",
      "epoch no.2 train no.217120  loss = 3.80177 avg_loss = 3.56212\n",
      "epoch no.2 train no.217130  loss = 3.58973 avg_loss = 3.56649\n",
      "epoch no.2 train no.217140  loss = 4.45701 avg_loss = 3.61111\n",
      "epoch no.2 train no.217150  loss = 4.29619 avg_loss = 3.57040\n",
      "epoch no.2 train no.217160  loss = 4.06044 avg_loss = 3.59821\n",
      "epoch no.2 train no.217170  loss = 2.16513 avg_loss = 3.54781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.217180  loss = 2.66529 avg_loss = 3.54175\n",
      "epoch no.2 train no.217190  loss = 3.12129 avg_loss = 3.57433\n",
      "epoch no.2 train no.217200  loss = 1.97466 avg_loss = 3.63344\n",
      "epoch no.2 train no.217210  loss = 5.93135 avg_loss = 3.68297\n",
      "epoch no.2 train no.217220  loss = 3.59863 avg_loss = 3.67186\n",
      "epoch no.2 train no.217230  loss = 3.35964 avg_loss = 3.63604\n",
      "epoch no.2 train no.217240  loss = 2.90796 avg_loss = 3.58847\n",
      "epoch no.2 train no.217250  loss = 4.13131 avg_loss = 3.56202\n",
      "epoch no.2 train no.217260  loss = 3.34352 avg_loss = 3.54910\n",
      "epoch no.2 train no.217270  loss = 2.99579 avg_loss = 3.52898\n",
      "epoch no.2 train no.217280  loss = 3.36097 avg_loss = 3.54543\n",
      "epoch no.2 train no.217290  loss = 3.80367 avg_loss = 3.49550\n",
      "epoch no.2 train no.217300  loss = 2.21831 avg_loss = 3.48210\n",
      "epoch no.2 train no.217310  loss = 4.00929 avg_loss = 3.48244\n",
      "epoch no.2 train no.217320  loss = 5.81415 avg_loss = 3.54459\n",
      "epoch no.2 train no.217330  loss = 3.04036 avg_loss = 3.51900\n",
      "epoch no.2 train no.217340  loss = 3.60445 avg_loss = 3.49431\n",
      "epoch no.2 train no.217350  loss = 3.19602 avg_loss = 3.52234\n",
      "epoch no.2 train no.217360  loss = 3.22809 avg_loss = 3.49741\n",
      "epoch no.2 train no.217370  loss = 5.98324 avg_loss = 3.61766\n",
      "epoch no.2 train no.217380  loss = 3.12106 avg_loss = 3.58374\n",
      "epoch no.2 train no.217390  loss = 5.84510 avg_loss = 3.58388\n",
      "epoch no.2 train no.217400  loss = 3.37627 avg_loss = 3.58766\n",
      "epoch no.2 train no.217410  loss = 2.75698 avg_loss = 3.58734\n",
      "epoch no.2 train no.217420  loss = 2.41295 avg_loss = 3.58513\n",
      "epoch no.2 train no.217430  loss = 5.34995 avg_loss = 3.57569\n",
      "epoch no.2 train no.217440  loss = 2.98311 avg_loss = 3.51435\n",
      "epoch no.2 train no.217450  loss = 3.96665 avg_loss = 3.46506\n",
      "epoch no.2 train no.217460  loss = 4.04047 avg_loss = 3.44304\n",
      "epoch no.2 train no.217470  loss = 2.31892 avg_loss = 3.41811\n",
      "epoch no.2 train no.217480  loss = 3.39468 avg_loss = 3.43286\n",
      "epoch no.2 train no.217490  loss = 3.96353 avg_loss = 3.44691\n",
      "epoch no.2 train no.217500  loss = 4.02607 avg_loss = 3.46005\n",
      "epoch no.2 train no.217510  loss = 4.13186 avg_loss = 3.44221\n",
      "epoch no.2 train no.217520  loss = 5.85585 avg_loss = 3.44137\n",
      "epoch no.2 train no.217530  loss = 4.20850 avg_loss = 3.48476\n",
      "epoch no.2 train no.217540  loss = 2.76562 avg_loss = 3.50499\n",
      "epoch no.2 train no.217550  loss = 2.80728 avg_loss = 3.50944\n",
      "epoch no.2 train no.217560  loss = 2.88495 avg_loss = 3.52897\n",
      "epoch no.2 train no.217570  loss = 3.36729 avg_loss = 3.53243\n",
      "epoch no.2 train no.217580  loss = 2.48621 avg_loss = 3.57903\n",
      "epoch no.2 train no.217590  loss = 3.37929 avg_loss = 3.58238\n",
      "epoch no.2 train no.217600  loss = 4.08638 avg_loss = 3.60016\n",
      "epoch no.2 train no.217610  loss = 2.62136 avg_loss = 3.53607\n",
      "epoch no.2 train no.217620  loss = 3.50268 avg_loss = 3.51999\n",
      "epoch no.2 train no.217630  loss = 5.01493 avg_loss = 3.56998\n",
      "epoch no.2 train no.217640  loss = 2.23207 avg_loss = 3.58846\n",
      "epoch no.2 train no.217650  loss = 5.66886 avg_loss = 3.61559\n",
      "epoch no.2 train no.217660  loss = 4.19628 avg_loss = 3.60505\n",
      "epoch no.2 train no.217670  loss = 2.63898 avg_loss = 3.58735\n",
      "epoch no.2 train no.217680  loss = 3.98932 avg_loss = 3.55740\n",
      "epoch no.2 train no.217690  loss = 3.45010 avg_loss = 3.59901\n",
      "epoch no.2 train no.217700  loss = 3.71803 avg_loss = 3.60856\n",
      "epoch no.2 train no.217710  loss = 2.55864 avg_loss = 3.63131\n",
      "epoch no.2 train no.217720  loss = 3.27006 avg_loss = 3.64929\n",
      "epoch no.2 train no.217730  loss = 2.94855 avg_loss = 3.64433\n",
      "epoch no.2 train no.217740  loss = 2.77624 avg_loss = 3.61862\n",
      "epoch no.2 train no.217750  loss = 2.66189 avg_loss = 3.63792\n",
      "epoch no.2 train no.217760  loss = 2.32848 avg_loss = 3.60528\n",
      "epoch no.2 train no.217770  loss = 4.34340 avg_loss = 3.60562\n",
      "epoch no.2 train no.217780  loss = 2.88896 avg_loss = 3.60138\n",
      "epoch no.2 train no.217790  loss = 3.68415 avg_loss = 3.59929\n",
      "epoch no.2 train no.217800  loss = 3.47082 avg_loss = 3.64337\n",
      "epoch no.2 train no.217810  loss = 2.44488 avg_loss = 3.62405\n",
      "epoch no.2 train no.217820  loss = 2.54107 avg_loss = 3.55821\n",
      "epoch no.2 train no.217830  loss = 3.16452 avg_loss = 3.58036\n",
      "epoch no.2 train no.217840  loss = 2.36027 avg_loss = 3.55768\n",
      "epoch no.2 train no.217850  loss = 4.57919 avg_loss = 3.54478\n",
      "epoch no.2 train no.217860  loss = 2.84155 avg_loss = 3.52206\n",
      "epoch no.2 train no.217870  loss = 3.26423 avg_loss = 3.55245\n",
      "epoch no.2 train no.217880  loss = 3.99541 avg_loss = 3.56807\n",
      "epoch no.2 train no.217890  loss = 4.11870 avg_loss = 3.63733\n",
      "epoch no.2 train no.217900  loss = 4.50151 avg_loss = 3.60529\n",
      "epoch no.2 train no.217910  loss = 4.59583 avg_loss = 3.63704\n",
      "epoch no.2 train no.217920  loss = 3.90149 avg_loss = 3.62716\n",
      "epoch no.2 train no.217930  loss = 4.18134 avg_loss = 3.62596\n",
      "epoch no.2 train no.217940  loss = 4.09702 avg_loss = 3.65293\n",
      "epoch no.2 train no.217950  loss = 2.13325 avg_loss = 3.66349\n",
      "epoch no.2 train no.217960  loss = 3.42719 avg_loss = 3.69341\n",
      "epoch no.2 train no.217970  loss = 3.60330 avg_loss = 3.68761\n",
      "epoch no.2 train no.217980  loss = 3.53178 avg_loss = 3.67826\n",
      "epoch no.2 train no.217990  loss = 3.75161 avg_loss = 3.69652\n",
      "epoch no.2 train no.218000  loss = 3.38059 avg_loss = 3.69186\n",
      "5\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁명', '곡', '</s>', '</s>']\n",
      "추억의 발라드 명곡들</s>\n",
      "epoch no.2 train no.218010  loss = 2.36232 avg_loss = 3.61781\n",
      "epoch no.2 train no.218020  loss = 3.00390 avg_loss = 3.57256\n",
      "epoch no.2 train no.218030  loss = 3.13514 avg_loss = 3.57730\n",
      "epoch no.2 train no.218040  loss = 3.23321 avg_loss = 3.60107\n",
      "epoch no.2 train no.218050  loss = 5.08675 avg_loss = 3.59504\n",
      "epoch no.2 train no.218060  loss = 3.79507 avg_loss = 3.57982\n",
      "epoch no.2 train no.218070  loss = 4.04753 avg_loss = 3.58018\n",
      "epoch no.2 train no.218080  loss = 3.49611 avg_loss = 3.51462\n",
      "epoch no.2 train no.218090  loss = 2.24333 avg_loss = 3.48068\n",
      "epoch no.2 train no.218100  loss = 3.47203 avg_loss = 3.47237\n",
      "epoch no.2 train no.218110  loss = 3.63040 avg_loss = 3.51102\n",
      "epoch no.2 train no.218120  loss = 4.04858 avg_loss = 3.48807\n",
      "epoch no.2 train no.218130  loss = 3.35934 avg_loss = 3.54515\n",
      "epoch no.2 train no.218140  loss = 1.87297 avg_loss = 3.48085\n",
      "epoch no.2 train no.218150  loss = 2.80043 avg_loss = 3.49360\n",
      "epoch no.2 train no.218160  loss = 4.18392 avg_loss = 3.53543\n",
      "epoch no.2 train no.218170  loss = 6.05703 avg_loss = 3.58168\n",
      "epoch no.2 train no.218180  loss = 4.37551 avg_loss = 3.57036\n",
      "epoch no.2 train no.218190  loss = 4.00327 avg_loss = 3.55418\n",
      "epoch no.2 train no.218200  loss = 3.95814 avg_loss = 3.53740\n",
      "epoch no.2 train no.218210  loss = 3.51992 avg_loss = 3.59950\n",
      "epoch no.2 train no.218220  loss = 3.32616 avg_loss = 3.61387\n",
      "epoch no.2 train no.218230  loss = 4.56925 avg_loss = 3.61170\n",
      "epoch no.2 train no.218240  loss = 4.74856 avg_loss = 3.69801\n",
      "epoch no.2 train no.218250  loss = 3.53998 avg_loss = 3.68085\n",
      "epoch no.2 train no.218260  loss = 2.86014 avg_loss = 3.69880\n",
      "epoch no.2 train no.218270  loss = 2.75974 avg_loss = 3.64334\n",
      "epoch no.2 train no.218280  loss = 3.35392 avg_loss = 3.70363\n",
      "epoch no.2 train no.218290  loss = 1.90868 avg_loss = 3.66360\n",
      "epoch no.2 train no.218300  loss = 3.96223 avg_loss = 3.65712\n",
      "epoch no.2 train no.218310  loss = 3.15963 avg_loss = 3.63254\n",
      "epoch no.2 train no.218320  loss = 3.76113 avg_loss = 3.66577\n",
      "epoch no.2 train no.218330  loss = 2.01483 avg_loss = 3.63618\n",
      "epoch no.2 train no.218340  loss = 3.06029 avg_loss = 3.63569\n",
      "epoch no.2 train no.218350  loss = 3.92113 avg_loss = 3.66509\n",
      "epoch no.2 train no.218360  loss = 4.69962 avg_loss = 3.65833\n",
      "epoch no.2 train no.218370  loss = 4.71832 avg_loss = 3.67915\n",
      "epoch no.2 train no.218380  loss = 2.58070 avg_loss = 3.65730\n",
      "epoch no.2 train no.218390  loss = 3.47442 avg_loss = 3.60103\n",
      "epoch no.2 train no.218400  loss = 2.49955 avg_loss = 3.59397\n",
      "epoch no.2 train no.218410  loss = 5.98897 avg_loss = 3.62135\n",
      "epoch no.2 train no.218420  loss = 1.80921 avg_loss = 3.60188\n",
      "epoch no.2 train no.218430  loss = 4.33623 avg_loss = 3.57160\n",
      "epoch no.2 train no.218440  loss = 4.50308 avg_loss = 3.56863\n",
      "epoch no.2 train no.218450  loss = 2.34852 avg_loss = 3.58318\n",
      "epoch no.2 train no.218460  loss = 3.96414 avg_loss = 3.54265\n",
      "epoch no.2 train no.218470  loss = 2.93893 avg_loss = 3.57468\n",
      "epoch no.2 train no.218480  loss = 1.70617 avg_loss = 3.54932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.218490  loss = 3.62575 avg_loss = 3.51646\n",
      "epoch no.2 train no.218500  loss = 4.53748 avg_loss = 3.48883\n",
      "epoch no.2 train no.218510  loss = 4.25118 avg_loss = 3.51606\n",
      "epoch no.2 train no.218520  loss = 4.22011 avg_loss = 3.52098\n",
      "epoch no.2 train no.218530  loss = 3.29025 avg_loss = 3.46027\n",
      "epoch no.2 train no.218540  loss = 3.28049 avg_loss = 3.42014\n",
      "epoch no.2 train no.218550  loss = 3.40200 avg_loss = 3.43970\n",
      "epoch no.2 train no.218560  loss = 3.16438 avg_loss = 3.45960\n",
      "epoch no.2 train no.218570  loss = 2.03127 avg_loss = 3.49156\n",
      "epoch no.2 train no.218580  loss = 3.75471 avg_loss = 3.52294\n",
      "epoch no.2 train no.218590  loss = 3.84087 avg_loss = 3.49509\n",
      "epoch no.2 train no.218600  loss = 2.69923 avg_loss = 3.46524\n",
      "epoch no.2 train no.218610  loss = 4.12701 avg_loss = 3.50548\n",
      "epoch no.2 train no.218620  loss = 4.65022 avg_loss = 3.49083\n",
      "epoch no.2 train no.218630  loss = 3.62251 avg_loss = 3.50440\n",
      "epoch no.2 train no.218640  loss = 4.11011 avg_loss = 3.51576\n",
      "epoch no.2 train no.218650  loss = 4.59375 avg_loss = 3.55184\n",
      "epoch no.2 train no.218660  loss = 4.63270 avg_loss = 3.56489\n",
      "epoch no.2 train no.218670  loss = 3.61525 avg_loss = 3.53918\n",
      "epoch no.2 train no.218680  loss = 2.40988 avg_loss = 3.53791\n",
      "epoch no.2 train no.218690  loss = 2.52733 avg_loss = 3.48686\n",
      "epoch no.2 train no.218700  loss = 2.29844 avg_loss = 3.49891\n",
      "epoch no.2 train no.218710  loss = 5.42744 avg_loss = 3.56369\n",
      "epoch no.2 train no.218720  loss = 2.67466 avg_loss = 3.53228\n",
      "epoch no.2 train no.218730  loss = 3.26228 avg_loss = 3.48816\n",
      "epoch no.2 train no.218740  loss = 3.19492 avg_loss = 3.46226\n",
      "epoch no.2 train no.218750  loss = 2.71406 avg_loss = 3.43208\n",
      "epoch no.2 train no.218760  loss = 4.87741 avg_loss = 3.42202\n",
      "epoch no.2 train no.218770  loss = 3.85189 avg_loss = 3.45325\n",
      "epoch no.2 train no.218780  loss = 6.32858 avg_loss = 3.48678\n",
      "epoch no.2 train no.218790  loss = 4.82473 avg_loss = 3.56071\n",
      "epoch no.2 train no.218800  loss = 2.70721 avg_loss = 3.52892\n",
      "epoch no.2 train no.218810  loss = 3.19706 avg_loss = 3.50673\n",
      "epoch no.2 train no.218820  loss = 2.62025 avg_loss = 3.49814\n",
      "epoch no.2 train no.218830  loss = 2.88039 avg_loss = 3.49612\n",
      "epoch no.2 train no.218840  loss = 4.72980 avg_loss = 3.54414\n",
      "epoch no.2 train no.218850  loss = 2.65836 avg_loss = 3.56156\n",
      "epoch no.2 train no.218860  loss = 2.41673 avg_loss = 3.57812\n",
      "epoch no.2 train no.218870  loss = 3.36164 avg_loss = 3.53429\n",
      "epoch no.2 train no.218880  loss = 4.81054 avg_loss = 3.49381\n",
      "epoch no.2 train no.218890  loss = 4.23064 avg_loss = 3.44999\n",
      "epoch no.2 train no.218900  loss = 2.47243 avg_loss = 3.37896\n",
      "epoch no.2 train no.218910  loss = 3.36319 avg_loss = 3.37221\n",
      "epoch no.2 train no.218920  loss = 4.04632 avg_loss = 3.43443\n",
      "epoch no.2 train no.218930  loss = 2.98338 avg_loss = 3.43949\n",
      "epoch no.2 train no.218940  loss = 1.65865 avg_loss = 3.48106\n",
      "epoch no.2 train no.218950  loss = 3.01005 avg_loss = 3.48010\n",
      "epoch no.2 train no.218960  loss = 3.31403 avg_loss = 3.45623\n",
      "epoch no.2 train no.218970  loss = 2.33707 avg_loss = 3.42140\n",
      "epoch no.2 train no.218980  loss = 2.59460 avg_loss = 3.38305\n",
      "epoch no.2 train no.218990  loss = 3.35742 avg_loss = 3.37678\n",
      "epoch no.2 train no.219000  loss = 5.44378 avg_loss = 3.39329\n",
      "3\n",
      "to_tokens: ['▁비', '▁2000', '▁o', 'st', '▁모음']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.2 train no.219010  loss = 3.87510 avg_loss = 3.48449\n",
      "epoch no.2 train no.219020  loss = 2.54627 avg_loss = 3.46710\n",
      "epoch no.2 train no.219030  loss = 4.22127 avg_loss = 3.50655\n",
      "epoch no.2 train no.219040  loss = 3.73269 avg_loss = 3.55936\n",
      "epoch no.2 train no.219050  loss = 2.52876 avg_loss = 3.55848\n",
      "epoch no.2 train no.219060  loss = 3.26103 avg_loss = 3.59335\n",
      "epoch no.2 train no.219070  loss = 2.62526 avg_loss = 3.57107\n",
      "epoch no.2 train no.219080  loss = 3.95713 avg_loss = 3.53214\n",
      "epoch no.2 train no.219090  loss = 3.62930 avg_loss = 3.51801\n",
      "epoch no.2 train no.219100  loss = 2.23782 avg_loss = 3.45232\n",
      "epoch no.2 train no.219110  loss = 3.70747 avg_loss = 3.47263\n",
      "epoch no.2 train no.219120  loss = 3.80220 avg_loss = 3.55241\n",
      "epoch no.2 train no.219130  loss = 2.11165 avg_loss = 3.49518\n",
      "epoch no.2 train no.219140  loss = 3.55328 avg_loss = 3.48285\n",
      "epoch no.2 train no.219150  loss = 2.52502 avg_loss = 3.48926\n",
      "epoch no.2 train no.219160  loss = 4.20090 avg_loss = 3.48377\n",
      "epoch no.2 train no.219170  loss = 3.88799 avg_loss = 3.49951\n",
      "epoch no.2 train no.219180  loss = 3.78291 avg_loss = 3.52068\n",
      "epoch no.2 train no.219190  loss = 2.08215 avg_loss = 3.47076\n",
      "epoch no.2 train no.219200  loss = 3.68154 avg_loss = 3.49164\n",
      "epoch no.2 train no.219210  loss = 1.31725 avg_loss = 3.46564\n",
      "epoch no.2 train no.219220  loss = 4.71653 avg_loss = 3.45772\n",
      "epoch no.2 train no.219230  loss = 2.67613 avg_loss = 3.46988\n",
      "epoch no.2 train no.219240  loss = 2.82276 avg_loss = 3.45475\n",
      "epoch no.2 train no.219250  loss = 3.04032 avg_loss = 3.47035\n",
      "epoch no.2 train no.219260  loss = 3.30588 avg_loss = 3.44804\n",
      "epoch no.2 train no.219270  loss = 4.42472 avg_loss = 3.46743\n",
      "epoch no.2 train no.219280  loss = 3.83896 avg_loss = 3.47661\n",
      "epoch no.2 train no.219290  loss = 3.58206 avg_loss = 3.46385\n",
      "epoch no.2 train no.219300  loss = 4.01784 avg_loss = 3.46181\n",
      "epoch no.2 train no.219310  loss = 6.06233 avg_loss = 3.49663\n",
      "epoch no.2 train no.219320  loss = 4.72097 avg_loss = 3.52527\n",
      "epoch no.2 train no.219330  loss = 2.30424 avg_loss = 3.51344\n",
      "epoch no.2 train no.219340  loss = 3.32107 avg_loss = 3.52839\n",
      "epoch no.2 train no.219350  loss = 3.40919 avg_loss = 3.53194\n",
      "epoch no.2 train no.219360  loss = 5.30389 avg_loss = 3.57916\n",
      "epoch no.2 train no.219370  loss = 4.87773 avg_loss = 3.60187\n",
      "epoch no.2 train no.219380  loss = 3.55043 avg_loss = 3.63578\n",
      "epoch no.2 train no.219390  loss = 2.90021 avg_loss = 3.63407\n",
      "epoch no.2 train no.219400  loss = 3.09294 avg_loss = 3.61508\n",
      "epoch no.2 train no.219410  loss = 6.07301 avg_loss = 3.66907\n",
      "epoch no.2 train no.219420  loss = 2.28246 avg_loss = 3.60246\n",
      "epoch no.2 train no.219430  loss = 2.03951 avg_loss = 3.55842\n",
      "epoch no.2 train no.219440  loss = 4.99844 avg_loss = 3.61871\n",
      "epoch no.2 train no.219450  loss = 5.10208 avg_loss = 3.64092\n",
      "epoch no.2 train no.219460  loss = 2.03761 avg_loss = 3.59301\n",
      "epoch no.2 train no.219470  loss = 3.37521 avg_loss = 3.59655\n",
      "epoch no.2 train no.219480  loss = 3.69248 avg_loss = 3.55573\n",
      "epoch no.2 train no.219490  loss = 3.47450 avg_loss = 3.55168\n",
      "epoch no.2 train no.219500  loss = 3.14387 avg_loss = 3.55581\n",
      "epoch no.2 train no.219510  loss = 3.18609 avg_loss = 3.55602\n",
      "epoch no.2 train no.219520  loss = 4.84265 avg_loss = 3.53404\n",
      "epoch no.2 train no.219530  loss = 2.77625 avg_loss = 3.54368\n",
      "epoch no.2 train no.219540  loss = 2.83728 avg_loss = 3.50638\n",
      "epoch no.2 train no.219550  loss = 4.03960 avg_loss = 3.49742\n",
      "epoch no.2 train no.219560  loss = 3.36952 avg_loss = 3.48713\n",
      "epoch no.2 train no.219570  loss = 3.92109 avg_loss = 3.50175\n",
      "epoch no.2 train no.219580  loss = 3.92522 avg_loss = 3.57137\n",
      "epoch no.2 train no.219590  loss = 3.77960 avg_loss = 3.56382\n",
      "epoch no.2 train no.219600  loss = 3.44896 avg_loss = 3.55231\n",
      "epoch no.2 train no.219610  loss = 2.30704 avg_loss = 3.56818\n",
      "epoch no.2 train no.219620  loss = 5.94396 avg_loss = 3.56389\n",
      "epoch no.2 train no.219630  loss = 4.12093 avg_loss = 3.55666\n",
      "epoch no.2 train no.219640  loss = 3.89140 avg_loss = 3.52793\n",
      "epoch no.2 train no.219650  loss = 5.63187 avg_loss = 3.54075\n",
      "epoch no.2 train no.219660  loss = 4.59223 avg_loss = 3.58890\n",
      "epoch no.2 train no.219670  loss = 3.23291 avg_loss = 3.62666\n",
      "epoch no.2 train no.219680  loss = 1.82165 avg_loss = 3.54167\n",
      "epoch no.2 train no.219690  loss = 4.00355 avg_loss = 3.52259\n",
      "epoch no.2 train no.219700  loss = 2.43486 avg_loss = 3.54743\n",
      "epoch no.2 train no.219710  loss = 2.81374 avg_loss = 3.56588\n",
      "epoch no.2 train no.219720  loss = 3.05831 avg_loss = 3.61342\n",
      "epoch no.2 train no.219730  loss = 2.48877 avg_loss = 3.60892\n",
      "epoch no.2 train no.219740  loss = 3.01283 avg_loss = 3.59464\n",
      "epoch no.2 train no.219750  loss = 3.37611 avg_loss = 3.56072\n",
      "epoch no.2 train no.219760  loss = 5.07790 avg_loss = 3.55460\n",
      "epoch no.2 train no.219770  loss = 3.70319 avg_loss = 3.53108\n",
      "epoch no.2 train no.219780  loss = 3.48110 avg_loss = 3.49382\n",
      "epoch no.2 train no.219790  loss = 2.98979 avg_loss = 3.50961\n",
      "epoch no.2 train no.219800  loss = 3.16136 avg_loss = 3.47039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.2 train no.219810  loss = 2.95625 avg_loss = 3.51098\n",
      "epoch no.2 train no.219820  loss = 4.92008 avg_loss = 3.51417\n",
      "epoch no.2 train no.219830  loss = 3.78353 avg_loss = 3.50004\n",
      "epoch no.2 train no.219840  loss = 2.19086 avg_loss = 3.51091\n",
      "epoch no.2 train no.219850  loss = 2.53317 avg_loss = 3.54646\n",
      "epoch no.2 train no.219860  loss = 2.55952 avg_loss = 3.53013\n",
      "epoch no.2 train no.219870  loss = 6.09462 avg_loss = 3.57818\n",
      "epoch no.2 train no.219880  loss = 4.04540 avg_loss = 3.59943\n",
      "epoch no.2 train no.219890  loss = 3.26228 avg_loss = 3.57282\n",
      "epoch no.2 train no.219900  loss = 4.37373 avg_loss = 3.57403\n",
      "epoch no.2 train no.219910  loss = 2.81855 avg_loss = 3.58914\n",
      "epoch no.2 train no.219920  loss = 5.91449 avg_loss = 3.63041\n",
      "epoch no.2 train no.219930  loss = 2.58764 avg_loss = 3.65838\n",
      "epoch no.2 train no.219940  loss = 3.41646 avg_loss = 3.63888\n",
      "epoch no.2 train no.219950  loss = 4.20704 avg_loss = 3.66604\n",
      "epoch no.2 train no.219960  loss = 4.08543 avg_loss = 3.70814\n",
      "epoch no.2 train no.219970  loss = 3.65654 avg_loss = 3.69759\n",
      "epoch no.2 train no.219980  loss = 4.68266 avg_loss = 3.68439\n",
      "epoch no.2 train no.219990  loss = 3.07034 avg_loss = 3.67622\n",
      "epoch no.2 train no.220000  loss = 3.01585 avg_loss = 3.69781\n",
      "4\n",
      "to_tokens: ['▁비', '▁댄스', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.2 train no.220010  loss = 3.12660 avg_loss = 3.70830\n",
      "epoch no.2 train no.220020  loss = 2.76312 avg_loss = 3.70214\n",
      "epoch no.2 train no.220030  loss = 4.68646 avg_loss = 3.76084\n",
      "epoch no.2 train no.220040  loss = 2.40677 avg_loss = 3.71495\n",
      "epoch no.2 train no.220050  loss = 2.34445 avg_loss = 3.66644\n",
      "epoch no.3 train no.220060  loss = 2.25441 avg_loss = 3.58363\n",
      "epoch no.3 train no.220070  loss = 3.49736 avg_loss = 3.53241\n",
      "epoch no.3 train no.220080  loss = 3.40856 avg_loss = 3.51145\n",
      "epoch no.3 train no.220090  loss = 2.14270 avg_loss = 3.45789\n",
      "epoch no.3 train no.220100  loss = 3.83120 avg_loss = 3.42901\n",
      "epoch no.3 train no.220110  loss = 5.28098 avg_loss = 3.39787\n",
      "epoch no.3 train no.220120  loss = 2.49418 avg_loss = 3.35801\n",
      "epoch no.3 train no.220130  loss = 1.87064 avg_loss = 3.33463\n",
      "epoch no.3 train no.220140  loss = 2.71963 avg_loss = 3.27361\n",
      "epoch no.3 train no.220150  loss = 2.88215 avg_loss = 3.31831\n",
      "epoch no.3 train no.220160  loss = 3.73504 avg_loss = 3.34469\n",
      "epoch no.3 train no.220170  loss = 4.81522 avg_loss = 3.32360\n",
      "epoch no.3 train no.220180  loss = 3.48272 avg_loss = 3.30443\n",
      "epoch no.3 train no.220190  loss = 2.50949 avg_loss = 3.29544\n",
      "epoch no.3 train no.220200  loss = 4.00083 avg_loss = 3.32057\n",
      "epoch no.3 train no.220210  loss = 2.48239 avg_loss = 3.26909\n",
      "epoch no.3 train no.220220  loss = 3.36113 avg_loss = 3.27757\n",
      "epoch no.3 train no.220230  loss = 4.59080 avg_loss = 3.28992\n",
      "epoch no.3 train no.220240  loss = 3.26133 avg_loss = 3.26579\n",
      "epoch no.3 train no.220250  loss = 3.11632 avg_loss = 3.26920\n",
      "epoch no.3 train no.220260  loss = 3.92082 avg_loss = 3.31803\n",
      "epoch no.3 train no.220270  loss = 3.07222 avg_loss = 3.29912\n",
      "epoch no.3 train no.220280  loss = 4.34782 avg_loss = 3.28114\n",
      "epoch no.3 train no.220290  loss = 3.42827 avg_loss = 3.29024\n",
      "epoch no.3 train no.220300  loss = 2.65300 avg_loss = 3.30619\n",
      "epoch no.3 train no.220310  loss = 2.92297 avg_loss = 3.29165\n",
      "epoch no.3 train no.220320  loss = 3.17267 avg_loss = 3.29567\n",
      "epoch no.3 train no.220330  loss = 3.20166 avg_loss = 3.31427\n",
      "epoch no.3 train no.220340  loss = 3.14982 avg_loss = 3.38434\n",
      "epoch no.3 train no.220350  loss = 2.25341 avg_loss = 3.34473\n",
      "epoch no.3 train no.220360  loss = 3.46154 avg_loss = 3.39121\n",
      "epoch no.3 train no.220370  loss = 3.52163 avg_loss = 3.32574\n",
      "epoch no.3 train no.220380  loss = 2.90929 avg_loss = 3.29118\n",
      "epoch no.3 train no.220390  loss = 4.11819 avg_loss = 3.29026\n",
      "epoch no.3 train no.220400  loss = 2.55283 avg_loss = 3.26879\n",
      "epoch no.3 train no.220410  loss = 4.97261 avg_loss = 3.26102\n",
      "epoch no.3 train no.220420  loss = 3.85482 avg_loss = 3.23381\n",
      "epoch no.3 train no.220430  loss = 1.99794 avg_loss = 3.18652\n",
      "epoch no.3 train no.220440  loss = 1.92205 avg_loss = 3.14235\n",
      "epoch no.3 train no.220450  loss = 3.07168 avg_loss = 3.11423\n",
      "epoch no.3 train no.220460  loss = 2.41381 avg_loss = 3.13881\n",
      "epoch no.3 train no.220470  loss = 2.77190 avg_loss = 3.13739\n",
      "epoch no.3 train no.220480  loss = 5.00159 avg_loss = 3.13654\n",
      "epoch no.3 train no.220490  loss = 3.17151 avg_loss = 3.12383\n",
      "epoch no.3 train no.220500  loss = 2.53266 avg_loss = 3.12115\n",
      "epoch no.3 train no.220510  loss = 4.55006 avg_loss = 3.15504\n",
      "epoch no.3 train no.220520  loss = 3.18366 avg_loss = 3.16299\n",
      "epoch no.3 train no.220530  loss = 3.65497 avg_loss = 3.20440\n",
      "epoch no.3 train no.220540  loss = 3.73061 avg_loss = 3.22048\n",
      "epoch no.3 train no.220550  loss = 4.39418 avg_loss = 3.21531\n",
      "epoch no.3 train no.220560  loss = 4.29633 avg_loss = 3.21122\n",
      "epoch no.3 train no.220570  loss = 3.01014 avg_loss = 3.20156\n",
      "epoch no.3 train no.220580  loss = 3.10341 avg_loss = 3.20746\n",
      "epoch no.3 train no.220590  loss = 4.12571 avg_loss = 3.21284\n",
      "epoch no.3 train no.220600  loss = 2.58584 avg_loss = 3.17271\n",
      "epoch no.3 train no.220610  loss = 3.32515 avg_loss = 3.21741\n",
      "epoch no.3 train no.220620  loss = 3.70035 avg_loss = 3.22138\n",
      "epoch no.3 train no.220630  loss = 3.52925 avg_loss = 3.17693\n",
      "epoch no.3 train no.220640  loss = 6.61794 avg_loss = 3.20665\n",
      "epoch no.3 train no.220650  loss = 3.68107 avg_loss = 3.17889\n",
      "epoch no.3 train no.220660  loss = 1.97069 avg_loss = 3.15702\n",
      "epoch no.3 train no.220670  loss = 3.21587 avg_loss = 3.14803\n",
      "epoch no.3 train no.220680  loss = 3.09240 avg_loss = 3.18464\n",
      "epoch no.3 train no.220690  loss = 2.86034 avg_loss = 3.22670\n",
      "epoch no.3 train no.220700  loss = 2.49433 avg_loss = 3.20843\n",
      "epoch no.3 train no.220710  loss = 4.40662 avg_loss = 3.19312\n",
      "epoch no.3 train no.220720  loss = 3.94778 avg_loss = 3.19416\n",
      "epoch no.3 train no.220730  loss = 5.85694 avg_loss = 3.20086\n",
      "epoch no.3 train no.220740  loss = 2.79107 avg_loss = 3.16992\n",
      "epoch no.3 train no.220750  loss = 1.82692 avg_loss = 3.15439\n",
      "epoch no.3 train no.220760  loss = 5.74254 avg_loss = 3.17125\n",
      "epoch no.3 train no.220770  loss = 3.62930 avg_loss = 3.18792\n",
      "epoch no.3 train no.220780  loss = 3.39304 avg_loss = 3.22239\n",
      "epoch no.3 train no.220790  loss = 2.40382 avg_loss = 3.16615\n",
      "epoch no.3 train no.220800  loss = 2.79891 avg_loss = 3.15558\n",
      "epoch no.3 train no.220810  loss = 2.42825 avg_loss = 3.16757\n",
      "epoch no.3 train no.220820  loss = 2.24208 avg_loss = 3.16469\n",
      "epoch no.3 train no.220830  loss = 3.34098 avg_loss = 3.17825\n",
      "epoch no.3 train no.220840  loss = 3.02136 avg_loss = 3.19196\n",
      "epoch no.3 train no.220850  loss = 3.11386 avg_loss = 3.18308\n",
      "epoch no.3 train no.220860  loss = 3.67339 avg_loss = 3.20067\n",
      "epoch no.3 train no.220870  loss = 4.50575 avg_loss = 3.19507\n",
      "epoch no.3 train no.220880  loss = 2.06680 avg_loss = 3.16351\n",
      "epoch no.3 train no.220890  loss = 2.01535 avg_loss = 3.17664\n",
      "epoch no.3 train no.220900  loss = 3.24093 avg_loss = 3.14346\n",
      "epoch no.3 train no.220910  loss = 3.72943 avg_loss = 3.14701\n",
      "epoch no.3 train no.220920  loss = 3.61952 avg_loss = 3.15494\n",
      "epoch no.3 train no.220930  loss = 2.67060 avg_loss = 3.14083\n",
      "epoch no.3 train no.220940  loss = 3.06702 avg_loss = 3.15912\n",
      "epoch no.3 train no.220950  loss = 3.23715 avg_loss = 3.12118\n",
      "epoch no.3 train no.220960  loss = 2.82607 avg_loss = 3.10660\n",
      "epoch no.3 train no.220970  loss = 3.03445 avg_loss = 3.10394\n",
      "epoch no.3 train no.220980  loss = 2.88758 avg_loss = 3.12080\n",
      "epoch no.3 train no.220990  loss = 5.39786 avg_loss = 3.14229\n",
      "epoch no.3 train no.221000  loss = 3.01797 avg_loss = 3.15482\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '곡', '만', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.3 train no.221010  loss = 3.35050 avg_loss = 3.14224\n",
      "epoch no.3 train no.221020  loss = 2.93816 avg_loss = 3.16493\n",
      "epoch no.3 train no.221030  loss = 4.22518 avg_loss = 3.20631\n",
      "epoch no.3 train no.221040  loss = 2.65474 avg_loss = 3.19407\n",
      "epoch no.3 train no.221050  loss = 2.68086 avg_loss = 3.22694\n",
      "epoch no.3 train no.221060  loss = 4.05943 avg_loss = 3.21162\n",
      "epoch no.3 train no.221070  loss = 1.83232 avg_loss = 3.17827\n",
      "epoch no.3 train no.221080  loss = 3.47326 avg_loss = 3.16262\n",
      "epoch no.3 train no.221090  loss = 2.91254 avg_loss = 3.18706\n",
      "epoch no.3 train no.221100  loss = 4.35361 avg_loss = 3.21018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.221110  loss = 3.03914 avg_loss = 3.15435\n",
      "epoch no.3 train no.221120  loss = 1.76593 avg_loss = 3.10158\n",
      "epoch no.3 train no.221130  loss = 5.28714 avg_loss = 3.10643\n",
      "epoch no.3 train no.221140  loss = 3.91512 avg_loss = 3.17235\n",
      "epoch no.3 train no.221150  loss = 2.99546 avg_loss = 3.18106\n",
      "epoch no.3 train no.221160  loss = 4.53180 avg_loss = 3.16839\n",
      "epoch no.3 train no.221170  loss = 3.39437 avg_loss = 3.15738\n",
      "epoch no.3 train no.221180  loss = 2.41024 avg_loss = 3.14021\n",
      "epoch no.3 train no.221190  loss = 4.05710 avg_loss = 3.21269\n",
      "epoch no.3 train no.221200  loss = 2.54334 avg_loss = 3.18938\n",
      "epoch no.3 train no.221210  loss = 1.83176 avg_loss = 3.19586\n",
      "epoch no.3 train no.221220  loss = 3.36814 avg_loss = 3.21020\n",
      "epoch no.3 train no.221230  loss = 2.67272 avg_loss = 3.21161\n",
      "epoch no.3 train no.221240  loss = 2.06410 avg_loss = 3.19536\n",
      "epoch no.3 train no.221250  loss = 3.05623 avg_loss = 3.18717\n",
      "epoch no.3 train no.221260  loss = 2.54122 avg_loss = 3.15945\n",
      "epoch no.3 train no.221270  loss = 2.91497 avg_loss = 3.16549\n",
      "epoch no.3 train no.221280  loss = 3.32976 avg_loss = 3.13039\n",
      "epoch no.3 train no.221290  loss = 3.43332 avg_loss = 3.12972\n",
      "epoch no.3 train no.221300  loss = 3.34342 avg_loss = 3.09703\n",
      "epoch no.3 train no.221310  loss = 2.85543 avg_loss = 3.09971\n",
      "epoch no.3 train no.221320  loss = 4.34181 avg_loss = 3.13713\n",
      "epoch no.3 train no.221330  loss = 2.47674 avg_loss = 3.11414\n",
      "epoch no.3 train no.221340  loss = 1.65737 avg_loss = 3.08336\n",
      "epoch no.3 train no.221350  loss = 3.68304 avg_loss = 3.10710\n",
      "epoch no.3 train no.221360  loss = 2.51835 avg_loss = 3.10145\n",
      "epoch no.3 train no.221370  loss = 3.82646 avg_loss = 3.09998\n",
      "epoch no.3 train no.221380  loss = 2.11113 avg_loss = 3.07223\n",
      "epoch no.3 train no.221390  loss = 2.37124 avg_loss = 3.08723\n",
      "epoch no.3 train no.221400  loss = 3.28262 avg_loss = 3.07904\n",
      "epoch no.3 train no.221410  loss = 2.52734 avg_loss = 3.09657\n",
      "epoch no.3 train no.221420  loss = 2.12270 avg_loss = 3.08044\n",
      "epoch no.3 train no.221430  loss = 4.64696 avg_loss = 3.11904\n",
      "epoch no.3 train no.221440  loss = 2.66854 avg_loss = 3.14787\n",
      "epoch no.3 train no.221450  loss = 1.90089 avg_loss = 3.08761\n",
      "epoch no.3 train no.221460  loss = 3.38535 avg_loss = 3.09336\n",
      "epoch no.3 train no.221470  loss = 3.78258 avg_loss = 3.13401\n",
      "epoch no.3 train no.221480  loss = 3.72864 avg_loss = 3.10081\n",
      "epoch no.3 train no.221490  loss = 4.88440 avg_loss = 3.11542\n",
      "epoch no.3 train no.221500  loss = 3.40560 avg_loss = 3.11820\n",
      "epoch no.3 train no.221510  loss = 2.51826 avg_loss = 3.04402\n",
      "epoch no.3 train no.221520  loss = 2.06737 avg_loss = 3.03099\n",
      "epoch no.3 train no.221530  loss = 3.19313 avg_loss = 3.01629\n",
      "epoch no.3 train no.221540  loss = 3.73735 avg_loss = 3.02336\n",
      "epoch no.3 train no.221550  loss = 3.56585 avg_loss = 3.04040\n",
      "epoch no.3 train no.221560  loss = 1.46655 avg_loss = 3.05424\n",
      "epoch no.3 train no.221570  loss = 2.28820 avg_loss = 3.06968\n",
      "epoch no.3 train no.221580  loss = 2.16851 avg_loss = 3.11997\n",
      "epoch no.3 train no.221590  loss = 2.50880 avg_loss = 3.16015\n",
      "epoch no.3 train no.221600  loss = 5.04140 avg_loss = 3.17186\n",
      "epoch no.3 train no.221610  loss = 5.07225 avg_loss = 3.22197\n",
      "epoch no.3 train no.221620  loss = 3.97356 avg_loss = 3.25074\n",
      "epoch no.3 train no.221630  loss = 3.07330 avg_loss = 3.22152\n",
      "epoch no.3 train no.221640  loss = 5.52108 avg_loss = 3.19847\n",
      "epoch no.3 train no.221650  loss = 3.77664 avg_loss = 3.16640\n",
      "epoch no.3 train no.221660  loss = 5.80740 avg_loss = 3.18700\n",
      "epoch no.3 train no.221670  loss = 4.01847 avg_loss = 3.17961\n",
      "epoch no.3 train no.221680  loss = 2.53614 avg_loss = 3.17295\n",
      "epoch no.3 train no.221690  loss = 2.28595 avg_loss = 3.18826\n",
      "epoch no.3 train no.221700  loss = 3.85920 avg_loss = 3.16791\n",
      "epoch no.3 train no.221710  loss = 2.48860 avg_loss = 3.19164\n",
      "epoch no.3 train no.221720  loss = 2.86737 avg_loss = 3.14304\n",
      "epoch no.3 train no.221730  loss = 3.27241 avg_loss = 3.11653\n",
      "epoch no.3 train no.221740  loss = 2.84584 avg_loss = 3.12342\n",
      "epoch no.3 train no.221750  loss = 3.78584 avg_loss = 3.08608\n",
      "epoch no.3 train no.221760  loss = 3.35884 avg_loss = 3.12593\n",
      "epoch no.3 train no.221770  loss = 3.76061 avg_loss = 3.09973\n",
      "epoch no.3 train no.221780  loss = 5.77627 avg_loss = 3.13142\n",
      "epoch no.3 train no.221790  loss = 3.81153 avg_loss = 3.12378\n",
      "epoch no.3 train no.221800  loss = 3.08402 avg_loss = 3.14715\n",
      "epoch no.3 train no.221810  loss = 2.62534 avg_loss = 3.13033\n",
      "epoch no.3 train no.221820  loss = 2.59112 avg_loss = 3.14020\n",
      "epoch no.3 train no.221830  loss = 3.03479 avg_loss = 3.13798\n",
      "epoch no.3 train no.221840  loss = 3.23963 avg_loss = 3.15373\n",
      "epoch no.3 train no.221850  loss = 2.30466 avg_loss = 3.14315\n",
      "epoch no.3 train no.221860  loss = 2.54106 avg_loss = 3.17772\n",
      "epoch no.3 train no.221870  loss = 3.02037 avg_loss = 3.18405\n",
      "epoch no.3 train no.221880  loss = 2.61251 avg_loss = 3.21007\n",
      "epoch no.3 train no.221890  loss = 3.01780 avg_loss = 3.20349\n",
      "epoch no.3 train no.221900  loss = 2.46429 avg_loss = 3.16787\n",
      "epoch no.3 train no.221910  loss = 3.56137 avg_loss = 3.12334\n",
      "epoch no.3 train no.221920  loss = 3.46630 avg_loss = 3.17988\n",
      "epoch no.3 train no.221930  loss = 2.77228 avg_loss = 3.16925\n",
      "epoch no.3 train no.221940  loss = 3.49139 avg_loss = 3.16152\n",
      "epoch no.3 train no.221950  loss = 2.50099 avg_loss = 3.19356\n",
      "epoch no.3 train no.221960  loss = 4.02786 avg_loss = 3.21315\n",
      "epoch no.3 train no.221970  loss = 3.24609 avg_loss = 3.25396\n",
      "epoch no.3 train no.221980  loss = 2.21902 avg_loss = 3.27685\n",
      "epoch no.3 train no.221990  loss = 5.87778 avg_loss = 3.27571\n",
      "epoch no.3 train no.222000  loss = 3.82978 avg_loss = 3.21577\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.3 train no.222010  loss = 3.31094 avg_loss = 3.21002\n",
      "epoch no.3 train no.222020  loss = 2.33341 avg_loss = 3.14269\n",
      "epoch no.3 train no.222030  loss = 2.98668 avg_loss = 3.17465\n",
      "epoch no.3 train no.222040  loss = 3.04866 avg_loss = 3.20275\n",
      "epoch no.3 train no.222050  loss = 4.68343 avg_loss = 3.20642\n",
      "epoch no.3 train no.222060  loss = 4.01864 avg_loss = 3.22875\n",
      "epoch no.3 train no.222070  loss = 2.41486 avg_loss = 3.22729\n",
      "epoch no.3 train no.222080  loss = 3.96004 avg_loss = 3.21618\n",
      "epoch no.3 train no.222090  loss = 3.67431 avg_loss = 3.23300\n",
      "epoch no.3 train no.222100  loss = 3.74137 avg_loss = 3.22123\n",
      "epoch no.3 train no.222110  loss = 2.75543 avg_loss = 3.23731\n",
      "epoch no.3 train no.222120  loss = 3.45581 avg_loss = 3.25772\n",
      "epoch no.3 train no.222130  loss = 2.34504 avg_loss = 3.22304\n",
      "epoch no.3 train no.222140  loss = 3.32883 avg_loss = 3.22199\n",
      "epoch no.3 train no.222150  loss = 2.91672 avg_loss = 3.19797\n",
      "epoch no.3 train no.222160  loss = 4.32226 avg_loss = 3.21137\n",
      "epoch no.3 train no.222170  loss = 5.41693 avg_loss = 3.22623\n",
      "epoch no.3 train no.222180  loss = 3.22890 avg_loss = 3.24563\n",
      "epoch no.3 train no.222190  loss = 2.07344 avg_loss = 3.24956\n",
      "epoch no.3 train no.222200  loss = 3.36996 avg_loss = 3.27535\n",
      "epoch no.3 train no.222210  loss = 2.14338 avg_loss = 3.29496\n",
      "epoch no.3 train no.222220  loss = 2.21026 avg_loss = 3.29768\n",
      "epoch no.3 train no.222230  loss = 2.63222 avg_loss = 3.26593\n",
      "epoch no.3 train no.222240  loss = 2.16162 avg_loss = 3.22585\n",
      "epoch no.3 train no.222250  loss = 2.20605 avg_loss = 3.21564\n",
      "epoch no.3 train no.222260  loss = 2.29107 avg_loss = 3.25458\n",
      "epoch no.3 train no.222270  loss = 3.23566 avg_loss = 3.27146\n",
      "epoch no.3 train no.222280  loss = 1.94951 avg_loss = 3.28162\n",
      "epoch no.3 train no.222290  loss = 5.17197 avg_loss = 3.28637\n",
      "epoch no.3 train no.222300  loss = 4.55836 avg_loss = 3.28439\n",
      "epoch no.3 train no.222310  loss = 2.64017 avg_loss = 3.30269\n",
      "epoch no.3 train no.222320  loss = 4.55158 avg_loss = 3.32851\n",
      "epoch no.3 train no.222330  loss = 1.84837 avg_loss = 3.32046\n",
      "epoch no.3 train no.222340  loss = 2.99738 avg_loss = 3.27655\n",
      "epoch no.3 train no.222350  loss = 5.78450 avg_loss = 3.32537\n",
      "epoch no.3 train no.222360  loss = 3.86172 avg_loss = 3.34004\n",
      "epoch no.3 train no.222370  loss = 2.49625 avg_loss = 3.33304\n",
      "epoch no.3 train no.222380  loss = 2.54839 avg_loss = 3.38204\n",
      "epoch no.3 train no.222390  loss = 1.98291 avg_loss = 3.35207\n",
      "epoch no.3 train no.222400  loss = 2.42651 avg_loss = 3.31508\n",
      "epoch no.3 train no.222410  loss = 2.86228 avg_loss = 3.31641\n",
      "epoch no.3 train no.222420  loss = 3.72945 avg_loss = 3.29074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.222430  loss = 3.00905 avg_loss = 3.30536\n",
      "epoch no.3 train no.222440  loss = 4.70538 avg_loss = 3.29654\n",
      "epoch no.3 train no.222450  loss = 3.51330 avg_loss = 3.28262\n",
      "epoch no.3 train no.222460  loss = 4.58726 avg_loss = 3.28583\n",
      "epoch no.3 train no.222470  loss = 2.11648 avg_loss = 3.26606\n",
      "epoch no.3 train no.222480  loss = 2.89045 avg_loss = 3.22546\n",
      "epoch no.3 train no.222490  loss = 3.47032 avg_loss = 3.24030\n",
      "epoch no.3 train no.222500  loss = 4.20399 avg_loss = 3.26974\n",
      "epoch no.3 train no.222510  loss = 5.40181 avg_loss = 3.25080\n",
      "epoch no.3 train no.222520  loss = 4.21899 avg_loss = 3.26099\n",
      "epoch no.3 train no.222530  loss = 4.80028 avg_loss = 3.26454\n",
      "epoch no.3 train no.222540  loss = 4.38249 avg_loss = 3.29599\n",
      "epoch no.3 train no.222550  loss = 2.24659 avg_loss = 3.27518\n",
      "epoch no.3 train no.222560  loss = 5.03460 avg_loss = 3.26449\n",
      "epoch no.3 train no.222570  loss = 2.72735 avg_loss = 3.25787\n",
      "epoch no.3 train no.222580  loss = 3.65913 avg_loss = 3.28316\n",
      "epoch no.3 train no.222590  loss = 4.40743 avg_loss = 3.24739\n",
      "epoch no.3 train no.222600  loss = 3.35084 avg_loss = 3.27062\n",
      "epoch no.3 train no.222610  loss = 2.69601 avg_loss = 3.24526\n",
      "epoch no.3 train no.222620  loss = 3.17913 avg_loss = 3.21172\n",
      "epoch no.3 train no.222630  loss = 3.12175 avg_loss = 3.24526\n",
      "epoch no.3 train no.222640  loss = 2.39762 avg_loss = 3.21328\n",
      "epoch no.3 train no.222650  loss = 2.93185 avg_loss = 3.24119\n",
      "epoch no.3 train no.222660  loss = 3.72540 avg_loss = 3.26979\n",
      "epoch no.3 train no.222670  loss = 2.41432 avg_loss = 3.24692\n",
      "epoch no.3 train no.222680  loss = 5.45700 avg_loss = 3.29073\n",
      "epoch no.3 train no.222690  loss = 3.72575 avg_loss = 3.28093\n",
      "epoch no.3 train no.222700  loss = 5.36599 avg_loss = 3.31467\n",
      "epoch no.3 train no.222710  loss = 2.30340 avg_loss = 3.27660\n",
      "epoch no.3 train no.222720  loss = 3.61680 avg_loss = 3.25247\n",
      "epoch no.3 train no.222730  loss = 2.96546 avg_loss = 3.21408\n",
      "epoch no.3 train no.222740  loss = 3.87257 avg_loss = 3.24914\n",
      "epoch no.3 train no.222750  loss = 2.28668 avg_loss = 3.25869\n",
      "epoch no.3 train no.222760  loss = 4.00787 avg_loss = 3.26458\n",
      "epoch no.3 train no.222770  loss = 3.85181 avg_loss = 3.25017\n",
      "epoch no.3 train no.222780  loss = 3.10070 avg_loss = 3.25777\n",
      "epoch no.3 train no.222790  loss = 2.95819 avg_loss = 3.27123\n",
      "epoch no.3 train no.222800  loss = 2.28864 avg_loss = 3.20671\n",
      "epoch no.3 train no.222810  loss = 2.87463 avg_loss = 3.19832\n",
      "epoch no.3 train no.222820  loss = 1.72731 avg_loss = 3.21682\n",
      "epoch no.3 train no.222830  loss = 2.54830 avg_loss = 3.21459\n",
      "epoch no.3 train no.222840  loss = 2.86302 avg_loss = 3.23247\n",
      "epoch no.3 train no.222850  loss = 3.24602 avg_loss = 3.27546\n",
      "epoch no.3 train no.222860  loss = 3.79650 avg_loss = 3.29565\n",
      "epoch no.3 train no.222870  loss = 3.80574 avg_loss = 3.34040\n",
      "epoch no.3 train no.222880  loss = 3.61107 avg_loss = 3.34619\n",
      "epoch no.3 train no.222890  loss = 3.56510 avg_loss = 3.32009\n",
      "epoch no.3 train no.222900  loss = 3.72119 avg_loss = 3.37704\n",
      "epoch no.3 train no.222910  loss = 1.65712 avg_loss = 3.35122\n",
      "epoch no.3 train no.222920  loss = 3.37855 avg_loss = 3.35503\n",
      "epoch no.3 train no.222930  loss = 2.49570 avg_loss = 3.33193\n",
      "epoch no.3 train no.222940  loss = 3.06619 avg_loss = 3.32001\n",
      "epoch no.3 train no.222950  loss = 2.18570 avg_loss = 3.30084\n",
      "epoch no.3 train no.222960  loss = 3.85340 avg_loss = 3.29223\n",
      "epoch no.3 train no.222970  loss = 2.54087 avg_loss = 3.25345\n",
      "epoch no.3 train no.222980  loss = 4.26041 avg_loss = 3.24971\n",
      "epoch no.3 train no.222990  loss = 2.96402 avg_loss = 3.23830\n",
      "epoch no.3 train no.223000  loss = 2.75357 avg_loss = 3.26531\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', '</s>', '합', '</s>']\n",
      "추억의 싸이월드 감성힙합</s>\n",
      "epoch no.3 train no.223010  loss = 4.61069 avg_loss = 3.27131\n",
      "epoch no.3 train no.223020  loss = 3.82275 avg_loss = 3.22314\n",
      "epoch no.3 train no.223030  loss = 2.44817 avg_loss = 3.17985\n",
      "epoch no.3 train no.223040  loss = 1.97130 avg_loss = 3.15169\n",
      "epoch no.3 train no.223050  loss = 2.36373 avg_loss = 3.15123\n",
      "epoch no.3 train no.223060  loss = 2.44476 avg_loss = 3.15246\n",
      "epoch no.3 train no.223070  loss = 2.52155 avg_loss = 3.20364\n",
      "epoch no.3 train no.223080  loss = 3.84547 avg_loss = 3.21093\n",
      "epoch no.3 train no.223090  loss = 2.26773 avg_loss = 3.20757\n",
      "epoch no.3 train no.223100  loss = 3.95308 avg_loss = 3.21764\n",
      "epoch no.3 train no.223110  loss = 2.81197 avg_loss = 3.25932\n",
      "epoch no.3 train no.223120  loss = 2.74475 avg_loss = 3.25657\n",
      "epoch no.3 train no.223130  loss = 2.65146 avg_loss = 3.23031\n",
      "epoch no.3 train no.223140  loss = 2.70772 avg_loss = 3.24783\n",
      "epoch no.3 train no.223150  loss = 4.78179 avg_loss = 3.23668\n",
      "epoch no.3 train no.223160  loss = 3.11941 avg_loss = 3.18083\n",
      "epoch no.3 train no.223170  loss = 4.98501 avg_loss = 3.18795\n",
      "epoch no.3 train no.223180  loss = 3.38218 avg_loss = 3.20195\n",
      "epoch no.3 train no.223190  loss = 3.83281 avg_loss = 3.20653\n",
      "epoch no.3 train no.223200  loss = 3.62297 avg_loss = 3.20299\n",
      "epoch no.3 train no.223210  loss = 4.00036 avg_loss = 3.19421\n",
      "epoch no.3 train no.223220  loss = 4.63186 avg_loss = 3.20454\n",
      "epoch no.3 train no.223230  loss = 2.39692 avg_loss = 3.24689\n",
      "epoch no.3 train no.223240  loss = 3.48463 avg_loss = 3.21054\n",
      "epoch no.3 train no.223250  loss = 4.18099 avg_loss = 3.16605\n",
      "epoch no.3 train no.223260  loss = 4.43968 avg_loss = 3.14769\n",
      "epoch no.3 train no.223270  loss = 3.63596 avg_loss = 3.22196\n",
      "epoch no.3 train no.223280  loss = 3.31290 avg_loss = 3.18951\n",
      "epoch no.3 train no.223290  loss = 3.38957 avg_loss = 3.21384\n",
      "epoch no.3 train no.223300  loss = 1.54041 avg_loss = 3.21800\n",
      "epoch no.3 train no.223310  loss = 2.29964 avg_loss = 3.23641\n",
      "epoch no.3 train no.223320  loss = 2.87255 avg_loss = 3.23012\n",
      "epoch no.3 train no.223330  loss = 2.30582 avg_loss = 3.18756\n",
      "epoch no.3 train no.223340  loss = 3.68366 avg_loss = 3.21993\n",
      "epoch no.3 train no.223350  loss = 2.47504 avg_loss = 3.20557\n",
      "epoch no.3 train no.223360  loss = 2.65398 avg_loss = 3.21520\n",
      "epoch no.3 train no.223370  loss = 3.71079 avg_loss = 3.23165\n",
      "epoch no.3 train no.223380  loss = 4.27449 avg_loss = 3.26487\n",
      "epoch no.3 train no.223390  loss = 3.67456 avg_loss = 3.27284\n",
      "epoch no.3 train no.223400  loss = 4.47098 avg_loss = 3.30835\n",
      "epoch no.3 train no.223410  loss = 1.76397 avg_loss = 3.28329\n",
      "epoch no.3 train no.223420  loss = 2.47041 avg_loss = 3.26522\n",
      "epoch no.3 train no.223430  loss = 3.47403 avg_loss = 3.22341\n",
      "epoch no.3 train no.223440  loss = 2.12763 avg_loss = 3.22239\n",
      "epoch no.3 train no.223450  loss = 4.63183 avg_loss = 3.24719\n",
      "epoch no.3 train no.223460  loss = 3.32712 avg_loss = 3.25434\n",
      "epoch no.3 train no.223470  loss = 4.52371 avg_loss = 3.27122\n",
      "epoch no.3 train no.223480  loss = 3.30544 avg_loss = 3.27983\n",
      "epoch no.3 train no.223490  loss = 2.35105 avg_loss = 3.30951\n",
      "epoch no.3 train no.223500  loss = 2.74033 avg_loss = 3.32348\n",
      "epoch no.3 train no.223510  loss = 4.26629 avg_loss = 3.28244\n",
      "epoch no.3 train no.223520  loss = 3.41606 avg_loss = 3.26434\n",
      "epoch no.3 train no.223530  loss = 4.22025 avg_loss = 3.21882\n",
      "epoch no.3 train no.223540  loss = 5.23243 avg_loss = 3.24845\n",
      "epoch no.3 train no.223550  loss = 2.27003 avg_loss = 3.28078\n",
      "epoch no.3 train no.223560  loss = 3.43028 avg_loss = 3.28617\n",
      "epoch no.3 train no.223570  loss = 2.60955 avg_loss = 3.26927\n",
      "epoch no.3 train no.223580  loss = 2.30744 avg_loss = 3.25845\n",
      "epoch no.3 train no.223590  loss = 2.66495 avg_loss = 3.25380\n",
      "epoch no.3 train no.223600  loss = 2.57052 avg_loss = 3.23958\n",
      "epoch no.3 train no.223610  loss = 2.71810 avg_loss = 3.24687\n",
      "epoch no.3 train no.223620  loss = 2.96658 avg_loss = 3.25722\n",
      "epoch no.3 train no.223630  loss = 2.36810 avg_loss = 3.23145\n",
      "epoch no.3 train no.223640  loss = 2.35213 avg_loss = 3.21740\n",
      "epoch no.3 train no.223650  loss = 3.66131 avg_loss = 3.20218\n",
      "epoch no.3 train no.223660  loss = 3.68092 avg_loss = 3.20312\n",
      "epoch no.3 train no.223670  loss = 2.48234 avg_loss = 3.20672\n",
      "epoch no.3 train no.223680  loss = 2.60866 avg_loss = 3.22308\n",
      "epoch no.3 train no.223690  loss = 3.40693 avg_loss = 3.22872\n",
      "epoch no.3 train no.223700  loss = 1.84805 avg_loss = 3.23436\n",
      "epoch no.3 train no.223710  loss = 3.71803 avg_loss = 3.28598\n",
      "epoch no.3 train no.223720  loss = 2.96966 avg_loss = 3.27944\n",
      "epoch no.3 train no.223730  loss = 1.74329 avg_loss = 3.28495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.223740  loss = 2.55775 avg_loss = 3.29227\n",
      "epoch no.3 train no.223750  loss = 2.30912 avg_loss = 3.29813\n",
      "epoch no.3 train no.223760  loss = 4.97503 avg_loss = 3.30869\n",
      "epoch no.3 train no.223770  loss = 2.99863 avg_loss = 3.33636\n",
      "epoch no.3 train no.223780  loss = 4.50636 avg_loss = 3.31951\n",
      "epoch no.3 train no.223790  loss = 2.61931 avg_loss = 3.28171\n",
      "epoch no.3 train no.223800  loss = 2.96815 avg_loss = 3.25059\n",
      "epoch no.3 train no.223810  loss = 3.65369 avg_loss = 3.25134\n",
      "epoch no.3 train no.223820  loss = 2.67066 avg_loss = 3.22902\n",
      "epoch no.3 train no.223830  loss = 3.01290 avg_loss = 3.26965\n",
      "epoch no.3 train no.223840  loss = 3.37032 avg_loss = 3.27055\n",
      "epoch no.3 train no.223850  loss = 2.73891 avg_loss = 3.25286\n",
      "epoch no.3 train no.223860  loss = 1.94098 avg_loss = 3.23493\n",
      "epoch no.3 train no.223870  loss = 4.14073 avg_loss = 3.27621\n",
      "epoch no.3 train no.223880  loss = 2.20990 avg_loss = 3.27542\n",
      "epoch no.3 train no.223890  loss = 1.87578 avg_loss = 3.25478\n",
      "epoch no.3 train no.223900  loss = 5.13987 avg_loss = 3.28937\n",
      "epoch no.3 train no.223910  loss = 2.76880 avg_loss = 3.25664\n",
      "epoch no.3 train no.223920  loss = 2.88102 avg_loss = 3.23279\n",
      "epoch no.3 train no.223930  loss = 4.35557 avg_loss = 3.27703\n",
      "epoch no.3 train no.223940  loss = 3.66343 avg_loss = 3.32300\n",
      "epoch no.3 train no.223950  loss = 3.59013 avg_loss = 3.30120\n",
      "epoch no.3 train no.223960  loss = 2.82863 avg_loss = 3.29047\n",
      "epoch no.3 train no.223970  loss = 4.85855 avg_loss = 3.31015\n",
      "epoch no.3 train no.223980  loss = 3.39493 avg_loss = 3.31991\n",
      "epoch no.3 train no.223990  loss = 2.98315 avg_loss = 3.26388\n",
      "epoch no.3 train no.224000  loss = 3.75349 avg_loss = 3.26737\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '송', '▁명', '집', '</s>']\n",
      "추억의 팝송 모음2</s>\n",
      "epoch no.3 train no.224010  loss = 2.82851 avg_loss = 3.26785\n",
      "epoch no.3 train no.224020  loss = 2.99913 avg_loss = 3.29231\n",
      "epoch no.3 train no.224030  loss = 3.96532 avg_loss = 3.28100\n",
      "epoch no.3 train no.224040  loss = 3.90618 avg_loss = 3.27179\n",
      "epoch no.3 train no.224050  loss = 2.10629 avg_loss = 3.23103\n",
      "epoch no.3 train no.224060  loss = 3.69830 avg_loss = 3.21128\n",
      "epoch no.3 train no.224070  loss = 3.06247 avg_loss = 3.20543\n",
      "epoch no.3 train no.224080  loss = 4.13601 avg_loss = 3.20723\n",
      "epoch no.3 train no.224090  loss = 2.82684 avg_loss = 3.19997\n",
      "epoch no.3 train no.224100  loss = 2.64715 avg_loss = 3.21075\n",
      "epoch no.3 train no.224110  loss = 2.64285 avg_loss = 3.22322\n",
      "epoch no.3 train no.224120  loss = 2.82106 avg_loss = 3.22238\n",
      "epoch no.3 train no.224130  loss = 3.15246 avg_loss = 3.22692\n",
      "epoch no.3 train no.224140  loss = 3.85543 avg_loss = 3.19863\n",
      "epoch no.3 train no.224150  loss = 2.40091 avg_loss = 3.22144\n",
      "epoch no.3 train no.224160  loss = 2.02011 avg_loss = 3.20975\n",
      "epoch no.3 train no.224170  loss = 3.65984 avg_loss = 3.23717\n",
      "epoch no.3 train no.224180  loss = 3.45116 avg_loss = 3.23183\n",
      "epoch no.3 train no.224190  loss = 3.53505 avg_loss = 3.21250\n",
      "epoch no.3 train no.224200  loss = 4.25053 avg_loss = 3.18902\n",
      "epoch no.3 train no.224210  loss = 3.78713 avg_loss = 3.19178\n",
      "epoch no.3 train no.224220  loss = 4.24847 avg_loss = 3.20365\n",
      "epoch no.3 train no.224230  loss = 1.95448 avg_loss = 3.20340\n",
      "epoch no.3 train no.224240  loss = 3.42071 avg_loss = 3.26940\n",
      "epoch no.3 train no.224250  loss = 1.70759 avg_loss = 3.26587\n",
      "epoch no.3 train no.224260  loss = 3.92955 avg_loss = 3.26726\n",
      "epoch no.3 train no.224270  loss = 2.75987 avg_loss = 3.28712\n",
      "epoch no.3 train no.224280  loss = 3.47149 avg_loss = 3.28619\n",
      "epoch no.3 train no.224290  loss = 3.25276 avg_loss = 3.25063\n",
      "epoch no.3 train no.224300  loss = 3.50450 avg_loss = 3.20138\n",
      "epoch no.3 train no.224310  loss = 3.86124 avg_loss = 3.19514\n",
      "epoch no.3 train no.224320  loss = 2.46426 avg_loss = 3.17614\n",
      "epoch no.3 train no.224330  loss = 3.34282 avg_loss = 3.16554\n",
      "epoch no.3 train no.224340  loss = 3.13828 avg_loss = 3.16571\n",
      "epoch no.3 train no.224350  loss = 2.74478 avg_loss = 3.12301\n",
      "epoch no.3 train no.224360  loss = 3.48250 avg_loss = 3.08877\n",
      "epoch no.3 train no.224370  loss = 3.84379 avg_loss = 3.13976\n",
      "epoch no.3 train no.224380  loss = 3.17034 avg_loss = 3.15523\n",
      "epoch no.3 train no.224390  loss = 2.28213 avg_loss = 3.11117\n",
      "epoch no.3 train no.224400  loss = 3.24880 avg_loss = 3.13605\n",
      "epoch no.3 train no.224410  loss = 2.89847 avg_loss = 3.15327\n",
      "epoch no.3 train no.224420  loss = 4.57955 avg_loss = 3.21797\n",
      "epoch no.3 train no.224430  loss = 3.77524 avg_loss = 3.20448\n",
      "epoch no.3 train no.224440  loss = 3.67494 avg_loss = 3.19560\n",
      "epoch no.3 train no.224450  loss = 4.09242 avg_loss = 3.21552\n",
      "epoch no.3 train no.224460  loss = 3.52967 avg_loss = 3.20594\n",
      "epoch no.3 train no.224470  loss = 1.41222 avg_loss = 3.13749\n",
      "epoch no.3 train no.224480  loss = 3.50193 avg_loss = 3.19358\n",
      "epoch no.3 train no.224490  loss = 2.97472 avg_loss = 3.23927\n",
      "epoch no.3 train no.224500  loss = 2.66961 avg_loss = 3.19768\n",
      "epoch no.3 train no.224510  loss = 3.79111 avg_loss = 3.20822\n",
      "epoch no.3 train no.224520  loss = 2.94625 avg_loss = 3.19997\n",
      "epoch no.3 train no.224530  loss = 4.12495 avg_loss = 3.24792\n",
      "epoch no.3 train no.224540  loss = 3.96990 avg_loss = 3.21376\n",
      "epoch no.3 train no.224550  loss = 4.11338 avg_loss = 3.19465\n",
      "epoch no.3 train no.224560  loss = 3.32500 avg_loss = 3.22887\n",
      "epoch no.3 train no.224570  loss = 2.88628 avg_loss = 3.18783\n",
      "epoch no.3 train no.224580  loss = 2.10161 avg_loss = 3.19061\n",
      "epoch no.3 train no.224590  loss = 2.92322 avg_loss = 3.17734\n",
      "epoch no.3 train no.224600  loss = 3.25403 avg_loss = 3.15682\n",
      "epoch no.3 train no.224610  loss = 4.93927 avg_loss = 3.17851\n",
      "epoch no.3 train no.224620  loss = 3.31917 avg_loss = 3.16776\n",
      "epoch no.3 train no.224630  loss = 4.05630 avg_loss = 3.21065\n",
      "epoch no.3 train no.224640  loss = 3.04472 avg_loss = 3.22821\n",
      "epoch no.3 train no.224650  loss = 4.46416 avg_loss = 3.25291\n",
      "epoch no.3 train no.224660  loss = 3.35791 avg_loss = 3.26602\n",
      "epoch no.3 train no.224670  loss = 2.11265 avg_loss = 3.23720\n",
      "epoch no.3 train no.224680  loss = 3.19067 avg_loss = 3.25560\n",
      "epoch no.3 train no.224690  loss = 2.62409 avg_loss = 3.24813\n",
      "epoch no.3 train no.224700  loss = 2.68819 avg_loss = 3.24330\n",
      "epoch no.3 train no.224710  loss = 2.48999 avg_loss = 3.22338\n",
      "epoch no.3 train no.224720  loss = 4.00069 avg_loss = 3.24640\n",
      "epoch no.3 train no.224730  loss = 2.32891 avg_loss = 3.21966\n",
      "epoch no.3 train no.224740  loss = 2.84676 avg_loss = 3.20767\n",
      "epoch no.3 train no.224750  loss = 2.86139 avg_loss = 3.21151\n",
      "epoch no.3 train no.224760  loss = 3.45385 avg_loss = 3.21238\n",
      "epoch no.3 train no.224770  loss = 5.42861 avg_loss = 3.23675\n",
      "epoch no.3 train no.224780  loss = 2.60886 avg_loss = 3.27268\n",
      "epoch no.3 train no.224790  loss = 2.46902 avg_loss = 3.28654\n",
      "epoch no.3 train no.224800  loss = 3.00547 avg_loss = 3.27902\n",
      "epoch no.3 train no.224810  loss = 2.90204 avg_loss = 3.25014\n",
      "epoch no.3 train no.224820  loss = 2.04489 avg_loss = 3.23642\n",
      "epoch no.3 train no.224830  loss = 2.74501 avg_loss = 3.24694\n",
      "epoch no.3 train no.224840  loss = 3.92985 avg_loss = 3.26870\n",
      "epoch no.3 train no.224850  loss = 3.99604 avg_loss = 3.28756\n",
      "epoch no.3 train no.224860  loss = 2.71435 avg_loss = 3.27855\n",
      "epoch no.3 train no.224870  loss = 1.86975 avg_loss = 3.26355\n",
      "epoch no.3 train no.224880  loss = 2.89596 avg_loss = 3.21685\n",
      "epoch no.3 train no.224890  loss = 4.24317 avg_loss = 3.26768\n",
      "epoch no.3 train no.224900  loss = 2.22934 avg_loss = 3.26094\n",
      "epoch no.3 train no.224910  loss = 3.00659 avg_loss = 3.22977\n",
      "epoch no.3 train no.224920  loss = 2.02585 avg_loss = 3.20752\n",
      "epoch no.3 train no.224930  loss = 2.16672 avg_loss = 3.20483\n",
      "epoch no.3 train no.224940  loss = 4.54655 avg_loss = 3.21696\n",
      "epoch no.3 train no.224950  loss = 4.08880 avg_loss = 3.18864\n",
      "epoch no.3 train no.224960  loss = 4.12221 avg_loss = 3.23070\n",
      "epoch no.3 train no.224970  loss = 3.25650 avg_loss = 3.31739\n",
      "epoch no.3 train no.224980  loss = 3.70320 avg_loss = 3.31030\n",
      "epoch no.3 train no.224990  loss = 2.86382 avg_loss = 3.32701\n",
      "epoch no.3 train no.225000  loss = 3.73743 avg_loss = 3.35503\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.3 train no.225010  loss = 4.43718 avg_loss = 3.38969\n",
      "epoch no.3 train no.225020  loss = 2.96473 avg_loss = 3.37207\n",
      "epoch no.3 train no.225030  loss = 3.69191 avg_loss = 3.38756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.225040  loss = 4.47105 avg_loss = 3.40175\n",
      "epoch no.3 train no.225050  loss = 3.03115 avg_loss = 3.35285\n",
      "epoch no.3 train no.225060  loss = 2.83978 avg_loss = 3.32559\n",
      "epoch no.3 train no.225070  loss = 2.74037 avg_loss = 3.37734\n",
      "epoch no.3 train no.225080  loss = 2.86783 avg_loss = 3.36407\n",
      "epoch no.3 train no.225090  loss = 4.28869 avg_loss = 3.31420\n",
      "epoch no.3 train no.225100  loss = 3.29853 avg_loss = 3.32353\n",
      "epoch no.3 train no.225110  loss = 3.66868 avg_loss = 3.34019\n",
      "epoch no.3 train no.225120  loss = 2.83962 avg_loss = 3.30906\n",
      "epoch no.3 train no.225130  loss = 3.75786 avg_loss = 3.28752\n",
      "epoch no.3 train no.225140  loss = 2.67488 avg_loss = 3.29577\n",
      "epoch no.3 train no.225150  loss = 3.70172 avg_loss = 3.30965\n",
      "epoch no.3 train no.225160  loss = 3.14878 avg_loss = 3.32857\n",
      "epoch no.3 train no.225170  loss = 4.04923 avg_loss = 3.33535\n",
      "epoch no.3 train no.225180  loss = 4.10021 avg_loss = 3.30544\n",
      "epoch no.3 train no.225190  loss = 2.29201 avg_loss = 3.32265\n",
      "epoch no.3 train no.225200  loss = 2.68897 avg_loss = 3.25682\n",
      "epoch no.3 train no.225210  loss = 3.31240 avg_loss = 3.27000\n",
      "epoch no.3 train no.225220  loss = 4.99256 avg_loss = 3.26838\n",
      "epoch no.3 train no.225230  loss = 4.66130 avg_loss = 3.32060\n",
      "epoch no.3 train no.225240  loss = 3.22958 avg_loss = 3.31321\n",
      "epoch no.3 train no.225250  loss = 1.79654 avg_loss = 3.31768\n",
      "epoch no.3 train no.225260  loss = 2.13202 avg_loss = 3.30755\n",
      "epoch no.3 train no.225270  loss = 3.39371 avg_loss = 3.32222\n",
      "epoch no.3 train no.225280  loss = 2.79130 avg_loss = 3.30272\n",
      "epoch no.3 train no.225290  loss = 3.47751 avg_loss = 3.31449\n",
      "epoch no.3 train no.225300  loss = 4.27685 avg_loss = 3.32553\n",
      "epoch no.3 train no.225310  loss = 2.21008 avg_loss = 3.31986\n",
      "epoch no.3 train no.225320  loss = 3.59835 avg_loss = 3.32669\n",
      "epoch no.3 train no.225330  loss = 2.70747 avg_loss = 3.29622\n",
      "epoch no.3 train no.225340  loss = 2.10554 avg_loss = 3.29010\n",
      "epoch no.3 train no.225350  loss = 3.02456 avg_loss = 3.31627\n",
      "epoch no.3 train no.225360  loss = 3.25611 avg_loss = 3.33727\n",
      "epoch no.3 train no.225370  loss = 2.78412 avg_loss = 3.31632\n",
      "epoch no.3 train no.225380  loss = 3.96187 avg_loss = 3.33431\n",
      "epoch no.3 train no.225390  loss = 3.81771 avg_loss = 3.25945\n",
      "epoch no.3 train no.225400  loss = 3.24916 avg_loss = 3.26809\n",
      "epoch no.3 train no.225410  loss = 4.04860 avg_loss = 3.27761\n",
      "epoch no.3 train no.225420  loss = 2.87836 avg_loss = 3.28823\n",
      "epoch no.3 train no.225430  loss = 4.29676 avg_loss = 3.27339\n",
      "epoch no.3 train no.225440  loss = 3.19846 avg_loss = 3.29934\n",
      "epoch no.3 train no.225450  loss = 3.93666 avg_loss = 3.33560\n",
      "epoch no.3 train no.225460  loss = 2.54511 avg_loss = 3.31673\n",
      "epoch no.3 train no.225470  loss = 3.29816 avg_loss = 3.30877\n",
      "epoch no.3 train no.225480  loss = 2.36897 avg_loss = 3.29469\n",
      "epoch no.3 train no.225490  loss = 3.18841 avg_loss = 3.24303\n",
      "epoch no.3 train no.225500  loss = 3.89697 avg_loss = 3.24244\n",
      "epoch no.3 train no.225510  loss = 3.17322 avg_loss = 3.20688\n",
      "epoch no.3 train no.225520  loss = 3.17566 avg_loss = 3.23245\n",
      "epoch no.3 train no.225530  loss = 2.92539 avg_loss = 3.20275\n",
      "epoch no.3 train no.225540  loss = 2.46830 avg_loss = 3.21202\n",
      "epoch no.3 train no.225550  loss = 3.42348 avg_loss = 3.26015\n",
      "epoch no.3 train no.225560  loss = 4.15781 avg_loss = 3.25141\n",
      "epoch no.3 train no.225570  loss = 3.79990 avg_loss = 3.25625\n",
      "epoch no.3 train no.225580  loss = 3.37834 avg_loss = 3.28331\n",
      "epoch no.3 train no.225590  loss = 2.65260 avg_loss = 3.26985\n",
      "epoch no.3 train no.225600  loss = 3.61814 avg_loss = 3.25326\n",
      "epoch no.3 train no.225610  loss = 4.10897 avg_loss = 3.28912\n",
      "epoch no.3 train no.225620  loss = 2.99220 avg_loss = 3.31679\n",
      "epoch no.3 train no.225630  loss = 4.17921 avg_loss = 3.28852\n",
      "epoch no.3 train no.225640  loss = 4.45457 avg_loss = 3.28955\n",
      "epoch no.3 train no.225650  loss = 1.99445 avg_loss = 3.28766\n",
      "epoch no.3 train no.225660  loss = 3.15055 avg_loss = 3.31669\n",
      "epoch no.3 train no.225670  loss = 2.74500 avg_loss = 3.26738\n",
      "epoch no.3 train no.225680  loss = 3.22735 avg_loss = 3.29864\n",
      "epoch no.3 train no.225690  loss = 2.78036 avg_loss = 3.26784\n",
      "epoch no.3 train no.225700  loss = 4.07423 avg_loss = 3.28029\n",
      "epoch no.3 train no.225710  loss = 2.87897 avg_loss = 3.22583\n",
      "epoch no.3 train no.225720  loss = 3.50948 avg_loss = 3.25628\n",
      "epoch no.3 train no.225730  loss = 3.25129 avg_loss = 3.27444\n",
      "epoch no.3 train no.225740  loss = 2.11867 avg_loss = 3.25444\n",
      "epoch no.3 train no.225750  loss = 3.64420 avg_loss = 3.22627\n",
      "epoch no.3 train no.225760  loss = 2.44076 avg_loss = 3.21416\n",
      "epoch no.3 train no.225770  loss = 5.26532 avg_loss = 3.24662\n",
      "epoch no.3 train no.225780  loss = 2.88106 avg_loss = 3.23297\n",
      "epoch no.3 train no.225790  loss = 2.10134 avg_loss = 3.21835\n",
      "epoch no.3 train no.225800  loss = 3.44362 avg_loss = 3.22489\n",
      "epoch no.3 train no.225810  loss = 3.67012 avg_loss = 3.26938\n",
      "epoch no.3 train no.225820  loss = 5.34028 avg_loss = 3.25493\n",
      "epoch no.3 train no.225830  loss = 4.12315 avg_loss = 3.21256\n",
      "epoch no.3 train no.225840  loss = 2.77856 avg_loss = 3.20221\n",
      "epoch no.3 train no.225850  loss = 4.08207 avg_loss = 3.16503\n",
      "epoch no.3 train no.225860  loss = 4.07700 avg_loss = 3.21359\n",
      "epoch no.3 train no.225870  loss = 5.70089 avg_loss = 3.27640\n",
      "epoch no.3 train no.225880  loss = 2.21182 avg_loss = 3.27465\n",
      "epoch no.3 train no.225890  loss = 3.11453 avg_loss = 3.30777\n",
      "epoch no.3 train no.225900  loss = 2.62541 avg_loss = 3.28359\n",
      "epoch no.3 train no.225910  loss = 5.44135 avg_loss = 3.28714\n",
      "epoch no.3 train no.225920  loss = 4.89476 avg_loss = 3.30741\n",
      "epoch no.3 train no.225930  loss = 2.77084 avg_loss = 3.30239\n",
      "epoch no.3 train no.225940  loss = 3.08346 avg_loss = 3.32817\n",
      "epoch no.3 train no.225950  loss = 3.55008 avg_loss = 3.29376\n",
      "epoch no.3 train no.225960  loss = 2.32692 avg_loss = 3.24259\n",
      "epoch no.3 train no.225970  loss = 2.23228 avg_loss = 3.19856\n",
      "epoch no.3 train no.225980  loss = 2.48264 avg_loss = 3.20710\n",
      "epoch no.3 train no.225990  loss = 3.60065 avg_loss = 3.15782\n",
      "epoch no.3 train no.226000  loss = 2.44140 avg_loss = 3.18624\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '곡', '발', '라드', '</s>']\n",
      "추억의 명곡발라드</s>\n",
      "epoch no.3 train no.226010  loss = 3.20640 avg_loss = 3.17963\n",
      "epoch no.3 train no.226020  loss = 3.84777 avg_loss = 3.17352\n",
      "epoch no.3 train no.226030  loss = 3.35793 avg_loss = 3.17002\n",
      "epoch no.3 train no.226040  loss = 3.95962 avg_loss = 3.13693\n",
      "epoch no.3 train no.226050  loss = 3.00682 avg_loss = 3.16544\n",
      "epoch no.3 train no.226060  loss = 2.65299 avg_loss = 3.20095\n",
      "epoch no.3 train no.226070  loss = 2.26751 avg_loss = 3.19274\n",
      "epoch no.3 train no.226080  loss = 2.44043 avg_loss = 3.22479\n",
      "epoch no.3 train no.226090  loss = 3.68593 avg_loss = 3.27491\n",
      "epoch no.3 train no.226100  loss = 3.70314 avg_loss = 3.23901\n",
      "epoch no.3 train no.226110  loss = 2.64094 avg_loss = 3.20163\n",
      "epoch no.3 train no.226120  loss = 3.20483 avg_loss = 3.21009\n",
      "epoch no.3 train no.226130  loss = 2.63818 avg_loss = 3.18154\n",
      "epoch no.3 train no.226140  loss = 3.60058 avg_loss = 3.17457\n",
      "epoch no.3 train no.226150  loss = 4.18761 avg_loss = 3.18483\n",
      "epoch no.3 train no.226160  loss = 2.20932 avg_loss = 3.17447\n",
      "epoch no.3 train no.226170  loss = 3.80585 avg_loss = 3.20476\n",
      "epoch no.3 train no.226180  loss = 2.48809 avg_loss = 3.17312\n",
      "epoch no.3 train no.226190  loss = 3.91364 avg_loss = 3.16979\n",
      "epoch no.3 train no.226200  loss = 5.90169 avg_loss = 3.20826\n",
      "epoch no.3 train no.226210  loss = 4.86543 avg_loss = 3.19353\n",
      "epoch no.3 train no.226220  loss = 4.08730 avg_loss = 3.19808\n",
      "epoch no.3 train no.226230  loss = 2.78416 avg_loss = 3.16522\n",
      "epoch no.3 train no.226240  loss = 3.80020 avg_loss = 3.16599\n",
      "epoch no.3 train no.226250  loss = 3.96634 avg_loss = 3.13669\n",
      "epoch no.3 train no.226260  loss = 2.47779 avg_loss = 3.10990\n",
      "epoch no.3 train no.226270  loss = 3.81673 avg_loss = 3.14001\n",
      "epoch no.3 train no.226280  loss = 3.91171 avg_loss = 3.15707\n",
      "epoch no.3 train no.226290  loss = 2.94149 avg_loss = 3.12765\n",
      "epoch no.3 train no.226300  loss = 2.49885 avg_loss = 3.15106\n",
      "epoch no.3 train no.226310  loss = 2.79403 avg_loss = 3.12154\n",
      "epoch no.3 train no.226320  loss = 3.12353 avg_loss = 3.08080\n",
      "epoch no.3 train no.226330  loss = 3.17905 avg_loss = 3.12501\n",
      "epoch no.3 train no.226340  loss = 3.81264 avg_loss = 3.20187\n",
      "epoch no.3 train no.226350  loss = 2.17400 avg_loss = 3.19095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.226360  loss = 2.71431 avg_loss = 3.16804\n",
      "epoch no.3 train no.226370  loss = 2.97465 avg_loss = 3.15974\n",
      "epoch no.3 train no.226380  loss = 2.82236 avg_loss = 3.18193\n",
      "epoch no.3 train no.226390  loss = 2.40078 avg_loss = 3.15655\n",
      "epoch no.3 train no.226400  loss = 2.59753 avg_loss = 3.16816\n",
      "epoch no.3 train no.226410  loss = 5.20108 avg_loss = 3.22844\n",
      "epoch no.3 train no.226420  loss = 3.06123 avg_loss = 3.23868\n",
      "epoch no.3 train no.226430  loss = 1.89311 avg_loss = 3.20840\n",
      "epoch no.3 train no.226440  loss = 3.37521 avg_loss = 3.22353\n",
      "epoch no.3 train no.226450  loss = 4.34507 avg_loss = 3.28611\n",
      "epoch no.3 train no.226460  loss = 2.87675 avg_loss = 3.25014\n",
      "epoch no.3 train no.226470  loss = 3.11045 avg_loss = 3.25649\n",
      "epoch no.3 train no.226480  loss = 2.77299 avg_loss = 3.22318\n",
      "epoch no.3 train no.226490  loss = 5.28822 avg_loss = 3.25073\n",
      "epoch no.3 train no.226500  loss = 4.62790 avg_loss = 3.29020\n",
      "epoch no.3 train no.226510  loss = 3.29474 avg_loss = 3.30084\n",
      "epoch no.3 train no.226520  loss = 2.86961 avg_loss = 3.26556\n",
      "epoch no.3 train no.226530  loss = 3.14612 avg_loss = 3.28288\n",
      "epoch no.3 train no.226540  loss = 5.70837 avg_loss = 3.30906\n",
      "epoch no.3 train no.226550  loss = 3.53488 avg_loss = 3.29486\n",
      "epoch no.3 train no.226560  loss = 2.07358 avg_loss = 3.26097\n",
      "epoch no.3 train no.226570  loss = 2.44347 avg_loss = 3.29453\n",
      "epoch no.3 train no.226580  loss = 2.52619 avg_loss = 3.26713\n",
      "epoch no.3 train no.226590  loss = 3.22984 avg_loss = 3.22564\n",
      "epoch no.3 train no.226600  loss = 2.45275 avg_loss = 3.21935\n",
      "epoch no.3 train no.226610  loss = 2.82567 avg_loss = 3.20958\n",
      "epoch no.3 train no.226620  loss = 3.01969 avg_loss = 3.21225\n",
      "epoch no.3 train no.226630  loss = 1.79598 avg_loss = 3.19748\n",
      "epoch no.3 train no.226640  loss = 3.16293 avg_loss = 3.16990\n",
      "epoch no.3 train no.226650  loss = 3.60009 avg_loss = 3.15561\n",
      "epoch no.3 train no.226660  loss = 3.98402 avg_loss = 3.11933\n",
      "epoch no.3 train no.226670  loss = 2.99194 avg_loss = 3.11102\n",
      "epoch no.3 train no.226680  loss = 3.38434 avg_loss = 3.12978\n",
      "epoch no.3 train no.226690  loss = 3.33439 avg_loss = 3.16086\n",
      "epoch no.3 train no.226700  loss = 3.81531 avg_loss = 3.21503\n",
      "epoch no.3 train no.226710  loss = 3.16574 avg_loss = 3.25440\n",
      "epoch no.3 train no.226720  loss = 4.13939 avg_loss = 3.24415\n",
      "epoch no.3 train no.226730  loss = 3.43519 avg_loss = 3.25529\n",
      "epoch no.3 train no.226740  loss = 1.86982 avg_loss = 3.20723\n",
      "epoch no.3 train no.226750  loss = 2.70291 avg_loss = 3.24347\n",
      "epoch no.3 train no.226760  loss = 2.39237 avg_loss = 3.28976\n",
      "epoch no.3 train no.226770  loss = 4.49071 avg_loss = 3.29821\n",
      "epoch no.3 train no.226780  loss = 2.02747 avg_loss = 3.28857\n",
      "epoch no.3 train no.226790  loss = 2.63874 avg_loss = 3.24081\n",
      "epoch no.3 train no.226800  loss = 4.38110 avg_loss = 3.23419\n",
      "epoch no.3 train no.226810  loss = 3.77631 avg_loss = 3.18206\n",
      "epoch no.3 train no.226820  loss = 2.72216 avg_loss = 3.20394\n",
      "epoch no.3 train no.226830  loss = 3.17954 avg_loss = 3.20990\n",
      "epoch no.3 train no.226840  loss = 2.49135 avg_loss = 3.24986\n",
      "epoch no.3 train no.226850  loss = 3.14816 avg_loss = 3.23160\n",
      "epoch no.3 train no.226860  loss = 2.29426 avg_loss = 3.18842\n",
      "epoch no.3 train no.226870  loss = 3.00310 avg_loss = 3.20774\n",
      "epoch no.3 train no.226880  loss = 2.40554 avg_loss = 3.17908\n",
      "epoch no.3 train no.226890  loss = 3.17901 avg_loss = 3.16968\n",
      "epoch no.3 train no.226900  loss = 3.08472 avg_loss = 3.18208\n",
      "epoch no.3 train no.226910  loss = 3.35327 avg_loss = 3.19240\n",
      "epoch no.3 train no.226920  loss = 2.26561 avg_loss = 3.19384\n",
      "epoch no.3 train no.226930  loss = 4.60986 avg_loss = 3.16094\n",
      "epoch no.3 train no.226940  loss = 3.96045 avg_loss = 3.16669\n",
      "epoch no.3 train no.226950  loss = 2.95722 avg_loss = 3.14113\n",
      "epoch no.3 train no.226960  loss = 4.30827 avg_loss = 3.15073\n",
      "epoch no.3 train no.226970  loss = 4.33790 avg_loss = 3.14531\n",
      "epoch no.3 train no.226980  loss = 3.52451 avg_loss = 3.14664\n",
      "epoch no.3 train no.226990  loss = 3.69746 avg_loss = 3.19972\n",
      "epoch no.3 train no.227000  loss = 2.78608 avg_loss = 3.19098\n",
      "5\n",
      "to_tokens: ['▁가을', '▁발라', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.227010  loss = 3.18988 avg_loss = 3.20123\n",
      "epoch no.3 train no.227020  loss = 3.42815 avg_loss = 3.21942\n",
      "epoch no.3 train no.227030  loss = 2.96355 avg_loss = 3.20298\n",
      "epoch no.3 train no.227040  loss = 2.10130 avg_loss = 3.22319\n",
      "epoch no.3 train no.227050  loss = 5.51104 avg_loss = 3.23653\n",
      "epoch no.3 train no.227060  loss = 2.87104 avg_loss = 3.21785\n",
      "epoch no.3 train no.227070  loss = 4.23545 avg_loss = 3.23369\n",
      "epoch no.3 train no.227080  loss = 4.54229 avg_loss = 3.25084\n",
      "epoch no.3 train no.227090  loss = 2.25096 avg_loss = 3.24050\n",
      "epoch no.3 train no.227100  loss = 3.45978 avg_loss = 3.24324\n",
      "epoch no.3 train no.227110  loss = 3.22648 avg_loss = 3.28417\n",
      "epoch no.3 train no.227120  loss = 3.50852 avg_loss = 3.34512\n",
      "epoch no.3 train no.227130  loss = 3.93947 avg_loss = 3.36366\n",
      "epoch no.3 train no.227140  loss = 3.01436 avg_loss = 3.34839\n",
      "epoch no.3 train no.227150  loss = 5.00343 avg_loss = 3.39148\n",
      "epoch no.3 train no.227160  loss = 2.93325 avg_loss = 3.37810\n",
      "epoch no.3 train no.227170  loss = 4.89052 avg_loss = 3.37967\n",
      "epoch no.3 train no.227180  loss = 2.99400 avg_loss = 3.40407\n",
      "epoch no.3 train no.227190  loss = 2.88335 avg_loss = 3.41579\n",
      "epoch no.3 train no.227200  loss = 2.83687 avg_loss = 3.42075\n",
      "epoch no.3 train no.227210  loss = 2.35112 avg_loss = 3.35969\n",
      "epoch no.3 train no.227220  loss = 2.41744 avg_loss = 3.29374\n",
      "epoch no.3 train no.227230  loss = 2.74465 avg_loss = 3.26762\n",
      "epoch no.3 train no.227240  loss = 3.63075 avg_loss = 3.26344\n",
      "epoch no.3 train no.227250  loss = 2.23864 avg_loss = 3.30361\n",
      "epoch no.3 train no.227260  loss = 2.41733 avg_loss = 3.25511\n",
      "epoch no.3 train no.227270  loss = 2.77817 avg_loss = 3.21671\n",
      "epoch no.3 train no.227280  loss = 3.90184 avg_loss = 3.25004\n",
      "epoch no.3 train no.227290  loss = 2.66421 avg_loss = 3.22700\n",
      "epoch no.3 train no.227300  loss = 4.36283 avg_loss = 3.20208\n",
      "epoch no.3 train no.227310  loss = 3.24703 avg_loss = 3.24189\n",
      "epoch no.3 train no.227320  loss = 2.64982 avg_loss = 3.26320\n",
      "epoch no.3 train no.227330  loss = 2.50535 avg_loss = 3.23749\n",
      "epoch no.3 train no.227340  loss = 2.99069 avg_loss = 3.21152\n",
      "epoch no.3 train no.227350  loss = 2.30028 avg_loss = 3.22674\n",
      "epoch no.3 train no.227360  loss = 3.64864 avg_loss = 3.22529\n",
      "epoch no.3 train no.227370  loss = 2.60558 avg_loss = 3.18504\n",
      "epoch no.3 train no.227380  loss = 2.77838 avg_loss = 3.20117\n",
      "epoch no.3 train no.227390  loss = 2.94108 avg_loss = 3.22771\n",
      "epoch no.3 train no.227400  loss = 2.30659 avg_loss = 3.17398\n",
      "epoch no.3 train no.227410  loss = 3.54821 avg_loss = 3.18709\n",
      "epoch no.3 train no.227420  loss = 2.93300 avg_loss = 3.16004\n",
      "epoch no.3 train no.227430  loss = 4.68729 avg_loss = 3.21255\n",
      "epoch no.3 train no.227440  loss = 2.52566 avg_loss = 3.18210\n",
      "epoch no.3 train no.227450  loss = 3.29072 avg_loss = 3.22860\n",
      "epoch no.3 train no.227460  loss = 2.69251 avg_loss = 3.24706\n",
      "epoch no.3 train no.227470  loss = 4.17551 avg_loss = 3.26470\n",
      "epoch no.3 train no.227480  loss = 4.27616 avg_loss = 3.25100\n",
      "epoch no.3 train no.227490  loss = 3.42251 avg_loss = 3.24165\n",
      "epoch no.3 train no.227500  loss = 3.80060 avg_loss = 3.31439\n",
      "epoch no.3 train no.227510  loss = 2.10605 avg_loss = 3.27316\n",
      "epoch no.3 train no.227520  loss = 3.06424 avg_loss = 3.27030\n",
      "epoch no.3 train no.227530  loss = 5.59268 avg_loss = 3.28807\n",
      "epoch no.3 train no.227540  loss = 3.80554 avg_loss = 3.34486\n",
      "epoch no.3 train no.227550  loss = 2.78327 avg_loss = 3.35241\n",
      "epoch no.3 train no.227560  loss = 3.27118 avg_loss = 3.32614\n",
      "epoch no.3 train no.227570  loss = 2.90785 avg_loss = 3.33899\n",
      "epoch no.3 train no.227580  loss = 3.33035 avg_loss = 3.34723\n",
      "epoch no.3 train no.227590  loss = 2.83991 avg_loss = 3.35361\n",
      "epoch no.3 train no.227600  loss = 2.70029 avg_loss = 3.31993\n",
      "epoch no.3 train no.227610  loss = 1.81410 avg_loss = 3.26492\n",
      "epoch no.3 train no.227620  loss = 3.12131 avg_loss = 3.25302\n",
      "epoch no.3 train no.227630  loss = 3.23162 avg_loss = 3.27761\n",
      "epoch no.3 train no.227640  loss = 3.76916 avg_loss = 3.22880\n",
      "epoch no.3 train no.227650  loss = 2.69790 avg_loss = 3.23566\n",
      "epoch no.3 train no.227660  loss = 3.03091 avg_loss = 3.22492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.227670  loss = 2.98025 avg_loss = 3.22628\n",
      "epoch no.3 train no.227680  loss = 3.39373 avg_loss = 3.24923\n",
      "epoch no.3 train no.227690  loss = 2.42841 avg_loss = 3.27902\n",
      "epoch no.3 train no.227700  loss = 4.79850 avg_loss = 3.29700\n",
      "epoch no.3 train no.227710  loss = 3.08937 avg_loss = 3.28247\n",
      "epoch no.3 train no.227720  loss = 2.72445 avg_loss = 3.26742\n",
      "epoch no.3 train no.227730  loss = 4.68439 avg_loss = 3.30047\n",
      "epoch no.3 train no.227740  loss = 4.33679 avg_loss = 3.31424\n",
      "epoch no.3 train no.227750  loss = 3.29205 avg_loss = 3.30566\n",
      "epoch no.3 train no.227760  loss = 2.14493 avg_loss = 3.28724\n",
      "epoch no.3 train no.227770  loss = 3.36221 avg_loss = 3.26357\n",
      "epoch no.3 train no.227780  loss = 3.64146 avg_loss = 3.30850\n",
      "epoch no.3 train no.227790  loss = 3.05693 avg_loss = 3.29977\n",
      "epoch no.3 train no.227800  loss = 6.56464 avg_loss = 3.38459\n",
      "epoch no.3 train no.227810  loss = 1.81658 avg_loss = 3.36494\n",
      "epoch no.3 train no.227820  loss = 2.61118 avg_loss = 3.37738\n",
      "epoch no.3 train no.227830  loss = 3.91452 avg_loss = 3.39311\n",
      "epoch no.3 train no.227840  loss = 2.65748 avg_loss = 3.36062\n",
      "epoch no.3 train no.227850  loss = 4.38868 avg_loss = 3.36223\n",
      "epoch no.3 train no.227860  loss = 2.82673 avg_loss = 3.32864\n",
      "epoch no.3 train no.227870  loss = 2.40872 avg_loss = 3.30735\n",
      "epoch no.3 train no.227880  loss = 2.86102 avg_loss = 3.31457\n",
      "epoch no.3 train no.227890  loss = 4.12587 avg_loss = 3.28626\n",
      "epoch no.3 train no.227900  loss = 2.73467 avg_loss = 3.25887\n",
      "epoch no.3 train no.227910  loss = 2.23118 avg_loss = 3.27646\n",
      "epoch no.3 train no.227920  loss = 2.92008 avg_loss = 3.26607\n",
      "epoch no.3 train no.227930  loss = 2.85142 avg_loss = 3.23463\n",
      "epoch no.3 train no.227940  loss = 3.37502 avg_loss = 3.22360\n",
      "epoch no.3 train no.227950  loss = 2.61506 avg_loss = 3.20454\n",
      "epoch no.3 train no.227960  loss = 2.57845 avg_loss = 3.17565\n",
      "epoch no.3 train no.227970  loss = 2.51580 avg_loss = 3.12272\n",
      "epoch no.3 train no.227980  loss = 5.41454 avg_loss = 3.12548\n",
      "epoch no.3 train no.227990  loss = 3.47566 avg_loss = 3.15268\n",
      "epoch no.3 train no.228000  loss = 2.73105 avg_loss = 3.17828\n",
      "7\n",
      "to_tokens: ['▁가을', '▁명', '송', '▁모음', '▁함께', '</s>', '드는', '어요', '</s>']\n",
      "추억의 팝송과 함께 잠들어요</s>\n",
      "epoch no.3 train no.228010  loss = 2.59936 avg_loss = 3.16064\n",
      "epoch no.3 train no.228020  loss = 2.89720 avg_loss = 3.15457\n",
      "epoch no.3 train no.228030  loss = 2.78780 avg_loss = 3.12770\n",
      "epoch no.3 train no.228040  loss = 3.43090 avg_loss = 3.14424\n",
      "epoch no.3 train no.228050  loss = 2.67182 avg_loss = 3.17953\n",
      "epoch no.3 train no.228060  loss = 2.93178 avg_loss = 3.12398\n",
      "epoch no.3 train no.228070  loss = 3.65723 avg_loss = 3.11091\n",
      "epoch no.3 train no.228080  loss = 3.25725 avg_loss = 3.11460\n",
      "epoch no.3 train no.228090  loss = 6.74958 avg_loss = 3.20022\n",
      "epoch no.3 train no.228100  loss = 3.39682 avg_loss = 3.18295\n",
      "epoch no.3 train no.228110  loss = 2.03652 avg_loss = 3.21594\n",
      "epoch no.3 train no.228120  loss = 5.06640 avg_loss = 3.18583\n",
      "epoch no.3 train no.228130  loss = 2.60469 avg_loss = 3.17054\n",
      "epoch no.3 train no.228140  loss = 3.06991 avg_loss = 3.17232\n",
      "epoch no.3 train no.228150  loss = 1.89019 avg_loss = 3.15364\n",
      "epoch no.3 train no.228160  loss = 2.76344 avg_loss = 3.15364\n",
      "epoch no.3 train no.228170  loss = 3.85817 avg_loss = 3.18085\n",
      "epoch no.3 train no.228180  loss = 2.61586 avg_loss = 3.15999\n",
      "epoch no.3 train no.228190  loss = 2.49802 avg_loss = 3.17051\n",
      "epoch no.3 train no.228200  loss = 3.22854 avg_loss = 3.17492\n",
      "epoch no.3 train no.228210  loss = 2.30376 avg_loss = 3.15533\n",
      "epoch no.3 train no.228220  loss = 2.85791 avg_loss = 3.20125\n",
      "epoch no.3 train no.228230  loss = 1.91654 avg_loss = 3.20177\n",
      "epoch no.3 train no.228240  loss = 2.12985 avg_loss = 3.17166\n",
      "epoch no.3 train no.228250  loss = 3.23694 avg_loss = 3.18535\n",
      "epoch no.3 train no.228260  loss = 2.09183 avg_loss = 3.19772\n",
      "epoch no.3 train no.228270  loss = 3.94422 avg_loss = 3.23323\n",
      "epoch no.3 train no.228280  loss = 2.40060 avg_loss = 3.22528\n",
      "epoch no.3 train no.228290  loss = 3.62074 avg_loss = 3.23073\n",
      "epoch no.3 train no.228300  loss = 1.95629 avg_loss = 3.16874\n",
      "epoch no.3 train no.228310  loss = 3.01086 avg_loss = 3.17156\n",
      "epoch no.3 train no.228320  loss = 6.01459 avg_loss = 3.21967\n",
      "epoch no.3 train no.228330  loss = 3.57024 avg_loss = 3.17245\n",
      "epoch no.3 train no.228340  loss = 3.85268 avg_loss = 3.18732\n",
      "epoch no.3 train no.228350  loss = 3.02478 avg_loss = 3.16021\n",
      "epoch no.3 train no.228360  loss = 4.45087 avg_loss = 3.19079\n",
      "epoch no.3 train no.228370  loss = 5.89039 avg_loss = 3.23166\n",
      "epoch no.3 train no.228380  loss = 3.50474 avg_loss = 3.24026\n",
      "epoch no.3 train no.228390  loss = 2.92716 avg_loss = 3.24726\n",
      "epoch no.3 train no.228400  loss = 6.53506 avg_loss = 3.28009\n",
      "epoch no.3 train no.228410  loss = 3.87158 avg_loss = 3.25084\n",
      "epoch no.3 train no.228420  loss = 2.56084 avg_loss = 3.24704\n",
      "epoch no.3 train no.228430  loss = 3.23566 avg_loss = 3.24740\n",
      "epoch no.3 train no.228440  loss = 3.92171 avg_loss = 3.27849\n",
      "epoch no.3 train no.228450  loss = 2.66437 avg_loss = 3.23174\n",
      "epoch no.3 train no.228460  loss = 4.05858 avg_loss = 3.24765\n",
      "epoch no.3 train no.228470  loss = 4.31017 avg_loss = 3.25326\n",
      "epoch no.3 train no.228480  loss = 4.25661 avg_loss = 3.29497\n",
      "epoch no.3 train no.228490  loss = 3.70318 avg_loss = 3.31316\n",
      "epoch no.3 train no.228500  loss = 3.61458 avg_loss = 3.34832\n",
      "epoch no.3 train no.228510  loss = 2.19644 avg_loss = 3.31719\n",
      "epoch no.3 train no.228520  loss = 3.94947 avg_loss = 3.32047\n",
      "epoch no.3 train no.228530  loss = 3.31379 avg_loss = 3.34582\n",
      "epoch no.3 train no.228540  loss = 2.13207 avg_loss = 3.31127\n",
      "epoch no.3 train no.228550  loss = 2.81517 avg_loss = 3.34470\n",
      "epoch no.3 train no.228560  loss = 2.46396 avg_loss = 3.34748\n",
      "epoch no.3 train no.228570  loss = 3.51062 avg_loss = 3.34693\n",
      "epoch no.3 train no.228580  loss = 2.52670 avg_loss = 3.42174\n",
      "epoch no.3 train no.228590  loss = 3.30663 avg_loss = 3.40865\n",
      "epoch no.3 train no.228600  loss = 3.62903 avg_loss = 3.39480\n",
      "epoch no.3 train no.228610  loss = 3.59899 avg_loss = 3.40712\n",
      "epoch no.3 train no.228620  loss = 4.13332 avg_loss = 3.37630\n",
      "epoch no.3 train no.228630  loss = 2.51858 avg_loss = 3.34710\n",
      "epoch no.3 train no.228640  loss = 4.91989 avg_loss = 3.39172\n",
      "epoch no.3 train no.228650  loss = 3.58263 avg_loss = 3.38989\n",
      "epoch no.3 train no.228660  loss = 2.36103 avg_loss = 3.35517\n",
      "epoch no.3 train no.228670  loss = 3.28294 avg_loss = 3.31784\n",
      "epoch no.3 train no.228680  loss = 3.29396 avg_loss = 3.29621\n",
      "epoch no.3 train no.228690  loss = 3.39403 avg_loss = 3.34731\n",
      "epoch no.3 train no.228700  loss = 2.98684 avg_loss = 3.40267\n",
      "epoch no.3 train no.228710  loss = 2.51520 avg_loss = 3.39222\n",
      "epoch no.3 train no.228720  loss = 2.02198 avg_loss = 3.36004\n",
      "epoch no.3 train no.228730  loss = 3.38405 avg_loss = 3.34789\n",
      "epoch no.3 train no.228740  loss = 2.69800 avg_loss = 3.33967\n",
      "epoch no.3 train no.228750  loss = 2.93913 avg_loss = 3.34226\n",
      "epoch no.3 train no.228760  loss = 2.44121 avg_loss = 3.37880\n",
      "epoch no.3 train no.228770  loss = 3.24493 avg_loss = 3.31603\n",
      "epoch no.3 train no.228780  loss = 2.27881 avg_loss = 3.30358\n",
      "epoch no.3 train no.228790  loss = 2.32413 avg_loss = 3.27289\n",
      "epoch no.3 train no.228800  loss = 2.65803 avg_loss = 3.28412\n",
      "epoch no.3 train no.228810  loss = 5.84509 avg_loss = 3.25949\n",
      "epoch no.3 train no.228820  loss = 4.03840 avg_loss = 3.29081\n",
      "epoch no.3 train no.228830  loss = 2.92090 avg_loss = 3.28064\n",
      "epoch no.3 train no.228840  loss = 2.54699 avg_loss = 3.28359\n",
      "epoch no.3 train no.228850  loss = 3.40372 avg_loss = 3.31150\n",
      "epoch no.3 train no.228860  loss = 3.04535 avg_loss = 3.34130\n",
      "epoch no.3 train no.228870  loss = 5.80240 avg_loss = 3.34736\n",
      "epoch no.3 train no.228880  loss = 2.64487 avg_loss = 3.36256\n",
      "epoch no.3 train no.228890  loss = 3.74912 avg_loss = 3.31619\n",
      "epoch no.3 train no.228900  loss = 3.36889 avg_loss = 3.27060\n",
      "epoch no.3 train no.228910  loss = 2.06997 avg_loss = 3.24069\n",
      "epoch no.3 train no.228920  loss = 3.48806 avg_loss = 3.23787\n",
      "epoch no.3 train no.228930  loss = 2.65269 avg_loss = 3.25054\n",
      "epoch no.3 train no.228940  loss = 2.15260 avg_loss = 3.23786\n",
      "epoch no.3 train no.228950  loss = 3.89358 avg_loss = 3.21216\n",
      "epoch no.3 train no.228960  loss = 3.08166 avg_loss = 3.19975\n",
      "epoch no.3 train no.228970  loss = 3.05621 avg_loss = 3.15816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.228980  loss = 3.77105 avg_loss = 3.12758\n",
      "epoch no.3 train no.228990  loss = 1.30776 avg_loss = 3.15280\n",
      "epoch no.3 train no.229000  loss = 5.64721 avg_loss = 3.14939\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.3 train no.229010  loss = 2.52488 avg_loss = 3.17891\n",
      "epoch no.3 train no.229020  loss = 2.71084 avg_loss = 3.21036\n",
      "epoch no.3 train no.229030  loss = 2.10484 avg_loss = 3.23710\n",
      "epoch no.3 train no.229040  loss = 3.56272 avg_loss = 3.20721\n",
      "epoch no.3 train no.229050  loss = 2.36770 avg_loss = 3.21007\n",
      "epoch no.3 train no.229060  loss = 4.41950 avg_loss = 3.22378\n",
      "epoch no.3 train no.229070  loss = 5.41490 avg_loss = 3.27221\n",
      "epoch no.3 train no.229080  loss = 3.50108 avg_loss = 3.23317\n",
      "epoch no.3 train no.229090  loss = 4.05457 avg_loss = 3.25122\n",
      "epoch no.3 train no.229100  loss = 2.34874 avg_loss = 3.19450\n",
      "epoch no.3 train no.229110  loss = 2.85601 avg_loss = 3.18895\n",
      "epoch no.3 train no.229120  loss = 2.79456 avg_loss = 3.19017\n",
      "epoch no.3 train no.229130  loss = 1.81867 avg_loss = 3.18824\n",
      "epoch no.3 train no.229140  loss = 4.41018 avg_loss = 3.19212\n",
      "epoch no.3 train no.229150  loss = 5.30272 avg_loss = 3.19198\n",
      "epoch no.3 train no.229160  loss = 2.31504 avg_loss = 3.18985\n",
      "epoch no.3 train no.229170  loss = 5.28949 avg_loss = 3.21622\n",
      "epoch no.3 train no.229180  loss = 2.28821 avg_loss = 3.21066\n",
      "epoch no.3 train no.229190  loss = 1.76829 avg_loss = 3.20017\n",
      "epoch no.3 train no.229200  loss = 2.83828 avg_loss = 3.20261\n",
      "epoch no.3 train no.229210  loss = 4.62590 avg_loss = 3.21085\n",
      "epoch no.3 train no.229220  loss = 1.92537 avg_loss = 3.16759\n",
      "epoch no.3 train no.229230  loss = 2.29243 avg_loss = 3.23218\n",
      "epoch no.3 train no.229240  loss = 2.52617 avg_loss = 3.23438\n",
      "epoch no.3 train no.229250  loss = 3.18863 avg_loss = 3.21555\n",
      "epoch no.3 train no.229260  loss = 2.53307 avg_loss = 3.19361\n",
      "epoch no.3 train no.229270  loss = 2.31480 avg_loss = 3.19290\n",
      "epoch no.3 train no.229280  loss = 5.18024 avg_loss = 3.22580\n",
      "epoch no.3 train no.229290  loss = 1.51026 avg_loss = 3.19338\n",
      "epoch no.3 train no.229300  loss = 4.38102 avg_loss = 3.19720\n",
      "epoch no.3 train no.229310  loss = 2.61757 avg_loss = 3.19506\n",
      "epoch no.3 train no.229320  loss = 2.53110 avg_loss = 3.19065\n",
      "epoch no.3 train no.229330  loss = 2.43019 avg_loss = 3.15312\n",
      "epoch no.3 train no.229340  loss = 1.93839 avg_loss = 3.14059\n",
      "epoch no.3 train no.229350  loss = 2.73567 avg_loss = 3.16101\n",
      "epoch no.3 train no.229360  loss = 3.68031 avg_loss = 3.19662\n",
      "epoch no.3 train no.229370  loss = 3.15544 avg_loss = 3.18405\n",
      "epoch no.3 train no.229380  loss = 2.39647 avg_loss = 3.20855\n",
      "epoch no.3 train no.229390  loss = 3.54224 avg_loss = 3.21166\n",
      "epoch no.3 train no.229400  loss = 2.86724 avg_loss = 3.22604\n",
      "epoch no.3 train no.229410  loss = 2.79404 avg_loss = 3.23935\n",
      "epoch no.3 train no.229420  loss = 2.27526 avg_loss = 3.22136\n",
      "epoch no.3 train no.229430  loss = 2.74924 avg_loss = 3.29623\n",
      "epoch no.3 train no.229440  loss = 2.82487 avg_loss = 3.31142\n",
      "epoch no.3 train no.229450  loss = 5.21374 avg_loss = 3.34554\n",
      "epoch no.3 train no.229460  loss = 4.61625 avg_loss = 3.34210\n",
      "epoch no.3 train no.229470  loss = 3.47823 avg_loss = 3.36453\n",
      "epoch no.3 train no.229480  loss = 3.65586 avg_loss = 3.33971\n",
      "epoch no.3 train no.229490  loss = 4.13691 avg_loss = 3.35652\n",
      "epoch no.3 train no.229500  loss = 3.04441 avg_loss = 3.30622\n",
      "epoch no.3 train no.229510  loss = 2.85696 avg_loss = 3.27255\n",
      "epoch no.3 train no.229520  loss = 4.22549 avg_loss = 3.24739\n",
      "epoch no.3 train no.229530  loss = 3.08570 avg_loss = 3.23777\n",
      "epoch no.3 train no.229540  loss = 2.08894 avg_loss = 3.18902\n",
      "epoch no.3 train no.229550  loss = 3.45808 avg_loss = 3.17136\n",
      "epoch no.3 train no.229560  loss = 4.36177 avg_loss = 3.17364\n",
      "epoch no.3 train no.229570  loss = 1.91184 avg_loss = 3.17538\n",
      "epoch no.3 train no.229580  loss = 5.60668 avg_loss = 3.19450\n",
      "epoch no.3 train no.229590  loss = 2.30569 avg_loss = 3.20742\n",
      "epoch no.3 train no.229600  loss = 3.73178 avg_loss = 3.20029\n",
      "epoch no.3 train no.229610  loss = 3.58773 avg_loss = 3.20031\n",
      "epoch no.3 train no.229620  loss = 3.68134 avg_loss = 3.17789\n",
      "epoch no.3 train no.229630  loss = 2.90943 avg_loss = 3.18801\n",
      "epoch no.3 train no.229640  loss = 4.32981 avg_loss = 3.19622\n",
      "epoch no.3 train no.229650  loss = 3.48722 avg_loss = 3.18583\n",
      "epoch no.3 train no.229660  loss = 2.32771 avg_loss = 3.21441\n",
      "epoch no.3 train no.229670  loss = 4.48405 avg_loss = 3.27756\n",
      "epoch no.3 train no.229680  loss = 2.44410 avg_loss = 3.29308\n",
      "epoch no.3 train no.229690  loss = 3.21805 avg_loss = 3.26734\n",
      "epoch no.3 train no.229700  loss = 5.02012 avg_loss = 3.27257\n",
      "epoch no.3 train no.229710  loss = 3.53260 avg_loss = 3.25926\n",
      "epoch no.3 train no.229720  loss = 4.74797 avg_loss = 3.26936\n",
      "epoch no.3 train no.229730  loss = 2.94907 avg_loss = 3.26127\n",
      "epoch no.3 train no.229740  loss = 3.00700 avg_loss = 3.26379\n",
      "epoch no.3 train no.229750  loss = 3.92406 avg_loss = 3.27571\n",
      "epoch no.3 train no.229760  loss = 1.81142 avg_loss = 3.25497\n",
      "epoch no.3 train no.229770  loss = 3.80277 avg_loss = 3.25131\n",
      "epoch no.3 train no.229780  loss = 5.48261 avg_loss = 3.26515\n",
      "epoch no.3 train no.229790  loss = 2.35249 avg_loss = 3.25545\n",
      "epoch no.3 train no.229800  loss = 2.51077 avg_loss = 3.24073\n",
      "epoch no.3 train no.229810  loss = 3.95515 avg_loss = 3.20989\n",
      "epoch no.3 train no.229820  loss = 3.14165 avg_loss = 3.21858\n",
      "epoch no.3 train no.229830  loss = 4.75829 avg_loss = 3.27325\n",
      "epoch no.3 train no.229840  loss = 2.90902 avg_loss = 3.28144\n",
      "epoch no.3 train no.229850  loss = 2.64254 avg_loss = 3.27365\n",
      "epoch no.3 train no.229860  loss = 2.06951 avg_loss = 3.30873\n",
      "epoch no.3 train no.229870  loss = 5.63515 avg_loss = 3.31605\n",
      "epoch no.3 train no.229880  loss = 3.49965 avg_loss = 3.29860\n",
      "epoch no.3 train no.229890  loss = 3.38874 avg_loss = 3.30962\n",
      "epoch no.3 train no.229900  loss = 2.97582 avg_loss = 3.31981\n",
      "epoch no.3 train no.229910  loss = 4.24319 avg_loss = 3.36966\n",
      "epoch no.3 train no.229920  loss = 4.91001 avg_loss = 3.35153\n",
      "epoch no.3 train no.229930  loss = 2.74628 avg_loss = 3.34608\n",
      "epoch no.3 train no.229940  loss = 3.07859 avg_loss = 3.35422\n",
      "epoch no.3 train no.229950  loss = 2.72633 avg_loss = 3.30720\n",
      "epoch no.3 train no.229960  loss = 1.63319 avg_loss = 3.25786\n",
      "epoch no.3 train no.229970  loss = 2.32513 avg_loss = 3.26127\n",
      "epoch no.3 train no.229980  loss = 4.12836 avg_loss = 3.23335\n",
      "epoch no.3 train no.229990  loss = 3.67337 avg_loss = 3.27150\n",
      "epoch no.3 train no.230000  loss = 2.76182 avg_loss = 3.26522\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '대를', '▁가요', '</s>']\n",
      "추억의 2000년 초반 가요</s>\n",
      "epoch no.3 train no.230010  loss = 2.20260 avg_loss = 3.25766\n",
      "epoch no.3 train no.230020  loss = 4.49966 avg_loss = 3.29119\n",
      "epoch no.3 train no.230030  loss = 3.25034 avg_loss = 3.31294\n",
      "epoch no.3 train no.230040  loss = 3.38116 avg_loss = 3.30840\n",
      "epoch no.3 train no.230050  loss = 5.37346 avg_loss = 3.28964\n",
      "epoch no.3 train no.230060  loss = 2.74182 avg_loss = 3.28003\n",
      "epoch no.3 train no.230070  loss = 3.57300 avg_loss = 3.25451\n",
      "epoch no.3 train no.230080  loss = 1.91853 avg_loss = 3.22265\n",
      "epoch no.3 train no.230090  loss = 1.78049 avg_loss = 3.22209\n",
      "epoch no.3 train no.230100  loss = 2.49556 avg_loss = 3.21309\n",
      "epoch no.3 train no.230110  loss = 3.01216 avg_loss = 3.22640\n",
      "epoch no.3 train no.230120  loss = 2.47475 avg_loss = 3.22779\n",
      "epoch no.3 train no.230130  loss = 4.51948 avg_loss = 3.23815\n",
      "epoch no.3 train no.230140  loss = 2.62691 avg_loss = 3.20233\n",
      "epoch no.3 train no.230150  loss = 4.65262 avg_loss = 3.21671\n",
      "epoch no.3 train no.230160  loss = 3.48608 avg_loss = 3.17296\n",
      "epoch no.3 train no.230170  loss = 2.73624 avg_loss = 3.13848\n",
      "epoch no.3 train no.230180  loss = 3.41355 avg_loss = 3.17242\n",
      "epoch no.3 train no.230190  loss = 2.29722 avg_loss = 3.17659\n",
      "epoch no.3 train no.230200  loss = 4.38737 avg_loss = 3.19655\n",
      "epoch no.3 train no.230210  loss = 1.83450 avg_loss = 3.18880\n",
      "epoch no.3 train no.230220  loss = 3.34170 avg_loss = 3.19191\n",
      "epoch no.3 train no.230230  loss = 5.01084 avg_loss = 3.23945\n",
      "epoch no.3 train no.230240  loss = 2.90125 avg_loss = 3.22478\n",
      "epoch no.3 train no.230250  loss = 2.98178 avg_loss = 3.29004\n",
      "epoch no.3 train no.230260  loss = 2.77696 avg_loss = 3.30334\n",
      "epoch no.3 train no.230270  loss = 6.35289 avg_loss = 3.32386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.230280  loss = 2.38319 avg_loss = 3.26180\n",
      "epoch no.3 train no.230290  loss = 4.16471 avg_loss = 3.24096\n",
      "epoch no.3 train no.230300  loss = 2.76720 avg_loss = 3.24231\n",
      "epoch no.3 train no.230310  loss = 2.58954 avg_loss = 3.20313\n",
      "epoch no.3 train no.230320  loss = 3.48439 avg_loss = 3.26170\n",
      "epoch no.3 train no.230330  loss = 4.19207 avg_loss = 3.26372\n",
      "epoch no.3 train no.230340  loss = 5.90615 avg_loss = 3.30990\n",
      "epoch no.3 train no.230350  loss = 4.01359 avg_loss = 3.24535\n",
      "epoch no.3 train no.230360  loss = 2.79935 avg_loss = 3.28562\n",
      "epoch no.3 train no.230370  loss = 3.14002 avg_loss = 3.25873\n",
      "epoch no.3 train no.230380  loss = 1.89735 avg_loss = 3.26751\n",
      "epoch no.3 train no.230390  loss = 3.27917 avg_loss = 3.21905\n",
      "epoch no.3 train no.230400  loss = 4.29925 avg_loss = 3.20139\n",
      "epoch no.3 train no.230410  loss = 3.71543 avg_loss = 3.22820\n",
      "epoch no.3 train no.230420  loss = 2.18050 avg_loss = 3.19725\n",
      "epoch no.3 train no.230430  loss = 3.64241 avg_loss = 3.17080\n",
      "epoch no.3 train no.230440  loss = 2.89804 avg_loss = 3.23649\n",
      "epoch no.3 train no.230450  loss = 2.00923 avg_loss = 3.22323\n",
      "epoch no.3 train no.230460  loss = 1.47872 avg_loss = 3.16226\n",
      "epoch no.3 train no.230470  loss = 3.17953 avg_loss = 3.15274\n",
      "epoch no.3 train no.230480  loss = 3.21300 avg_loss = 3.15099\n",
      "epoch no.3 train no.230490  loss = 3.62030 avg_loss = 3.16538\n",
      "epoch no.3 train no.230500  loss = 1.53039 avg_loss = 3.15769\n",
      "epoch no.3 train no.230510  loss = 4.40111 avg_loss = 3.14999\n",
      "epoch no.3 train no.230520  loss = 3.03633 avg_loss = 3.13556\n",
      "epoch no.3 train no.230530  loss = 3.25104 avg_loss = 3.12543\n",
      "epoch no.3 train no.230540  loss = 1.90578 avg_loss = 3.13680\n",
      "epoch no.3 train no.230550  loss = 3.96374 avg_loss = 3.15898\n",
      "epoch no.3 train no.230560  loss = 1.42358 avg_loss = 3.15536\n",
      "epoch no.3 train no.230570  loss = 2.89492 avg_loss = 3.16914\n",
      "epoch no.3 train no.230580  loss = 2.56284 avg_loss = 3.13724\n",
      "epoch no.3 train no.230590  loss = 2.49271 avg_loss = 3.12044\n",
      "epoch no.3 train no.230600  loss = 3.06913 avg_loss = 3.14003\n",
      "epoch no.3 train no.230610  loss = 3.32728 avg_loss = 3.16316\n",
      "epoch no.3 train no.230620  loss = 4.01424 avg_loss = 3.15823\n",
      "epoch no.3 train no.230630  loss = 3.42576 avg_loss = 3.11791\n",
      "epoch no.3 train no.230640  loss = 2.91835 avg_loss = 3.15840\n",
      "epoch no.3 train no.230650  loss = 2.30312 avg_loss = 3.19144\n",
      "epoch no.3 train no.230660  loss = 3.61923 avg_loss = 3.21395\n",
      "epoch no.3 train no.230670  loss = 4.24520 avg_loss = 3.17977\n",
      "epoch no.3 train no.230680  loss = 4.05193 avg_loss = 3.16721\n",
      "epoch no.3 train no.230690  loss = 2.62862 avg_loss = 3.13396\n",
      "epoch no.3 train no.230700  loss = 2.06740 avg_loss = 3.12705\n",
      "epoch no.3 train no.230710  loss = 1.48297 avg_loss = 3.12958\n",
      "epoch no.3 train no.230720  loss = 2.95109 avg_loss = 3.15108\n",
      "epoch no.3 train no.230730  loss = 2.52978 avg_loss = 3.13300\n",
      "epoch no.3 train no.230740  loss = 4.55932 avg_loss = 3.17765\n",
      "epoch no.3 train no.230750  loss = 2.94102 avg_loss = 3.19119\n",
      "epoch no.3 train no.230760  loss = 1.82077 avg_loss = 3.19419\n",
      "epoch no.3 train no.230770  loss = 4.40053 avg_loss = 3.19251\n",
      "epoch no.3 train no.230780  loss = 3.88295 avg_loss = 3.18652\n",
      "epoch no.3 train no.230790  loss = 2.50637 avg_loss = 3.21306\n",
      "epoch no.3 train no.230800  loss = 4.08580 avg_loss = 3.23609\n",
      "epoch no.3 train no.230810  loss = 3.26270 avg_loss = 3.24174\n",
      "epoch no.3 train no.230820  loss = 1.19860 avg_loss = 3.18536\n",
      "epoch no.3 train no.230830  loss = 3.61860 avg_loss = 3.15813\n",
      "epoch no.3 train no.230840  loss = 3.51428 avg_loss = 3.18041\n",
      "epoch no.3 train no.230850  loss = 3.44853 avg_loss = 3.18597\n",
      "epoch no.3 train no.230860  loss = 2.27351 avg_loss = 3.16869\n",
      "epoch no.3 train no.230870  loss = 2.95945 avg_loss = 3.14473\n",
      "epoch no.3 train no.230880  loss = 3.32295 avg_loss = 3.16683\n",
      "epoch no.3 train no.230890  loss = 2.73747 avg_loss = 3.17032\n",
      "epoch no.3 train no.230900  loss = 2.86732 avg_loss = 3.19650\n",
      "epoch no.3 train no.230910  loss = 4.36390 avg_loss = 3.21550\n",
      "epoch no.3 train no.230920  loss = 2.33934 avg_loss = 3.18371\n",
      "epoch no.3 train no.230930  loss = 5.19706 avg_loss = 3.22434\n",
      "epoch no.3 train no.230940  loss = 3.00688 avg_loss = 3.17984\n",
      "epoch no.3 train no.230950  loss = 2.43810 avg_loss = 3.22269\n",
      "epoch no.3 train no.230960  loss = 3.68447 avg_loss = 3.22957\n",
      "epoch no.3 train no.230970  loss = 2.65430 avg_loss = 3.20797\n",
      "epoch no.3 train no.230980  loss = 3.49650 avg_loss = 3.20585\n",
      "epoch no.3 train no.230990  loss = 2.97204 avg_loss = 3.20681\n",
      "epoch no.3 train no.231000  loss = 3.50864 avg_loss = 3.18141\n",
      "4\n",
      "to_tokens: ['▁가을', '▁명', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.3 train no.231010  loss = 2.12848 avg_loss = 3.16183\n",
      "epoch no.3 train no.231020  loss = 2.72017 avg_loss = 3.16329\n",
      "epoch no.3 train no.231030  loss = 2.44573 avg_loss = 3.11762\n",
      "epoch no.3 train no.231040  loss = 2.93052 avg_loss = 3.10864\n",
      "epoch no.3 train no.231050  loss = 4.36337 avg_loss = 3.13947\n",
      "epoch no.3 train no.231060  loss = 2.80205 avg_loss = 3.18146\n",
      "epoch no.3 train no.231070  loss = 2.46073 avg_loss = 3.17733\n",
      "epoch no.3 train no.231080  loss = 5.42025 avg_loss = 3.21284\n",
      "epoch no.3 train no.231090  loss = 2.74950 avg_loss = 3.23109\n",
      "epoch no.3 train no.231100  loss = 3.57736 avg_loss = 3.23841\n",
      "epoch no.3 train no.231110  loss = 4.63167 avg_loss = 3.18370\n",
      "epoch no.3 train no.231120  loss = 3.58858 avg_loss = 3.20421\n",
      "epoch no.3 train no.231130  loss = 2.93756 avg_loss = 3.19945\n",
      "epoch no.3 train no.231140  loss = 3.17449 avg_loss = 3.20212\n",
      "epoch no.3 train no.231150  loss = 2.81563 avg_loss = 3.21375\n",
      "epoch no.3 train no.231160  loss = 2.85284 avg_loss = 3.26895\n",
      "epoch no.3 train no.231170  loss = 4.40558 avg_loss = 3.32502\n",
      "epoch no.3 train no.231180  loss = 4.24597 avg_loss = 3.34621\n",
      "epoch no.3 train no.231190  loss = 2.64121 avg_loss = 3.31097\n",
      "epoch no.3 train no.231200  loss = 3.33264 avg_loss = 3.27274\n",
      "epoch no.3 train no.231210  loss = 3.77388 avg_loss = 3.23063\n",
      "epoch no.3 train no.231220  loss = 2.70691 avg_loss = 3.20290\n",
      "epoch no.3 train no.231230  loss = 3.00049 avg_loss = 3.23350\n",
      "epoch no.3 train no.231240  loss = 3.12594 avg_loss = 3.27868\n",
      "epoch no.3 train no.231250  loss = 2.61619 avg_loss = 3.25670\n",
      "epoch no.3 train no.231260  loss = 3.13897 avg_loss = 3.24688\n",
      "epoch no.3 train no.231270  loss = 2.43019 avg_loss = 3.22592\n",
      "epoch no.3 train no.231280  loss = 3.00022 avg_loss = 3.20491\n",
      "epoch no.3 train no.231290  loss = 2.46106 avg_loss = 3.20109\n",
      "epoch no.3 train no.231300  loss = 2.60615 avg_loss = 3.13929\n",
      "epoch no.3 train no.231310  loss = 2.34042 avg_loss = 3.17098\n",
      "epoch no.3 train no.231320  loss = 3.37373 avg_loss = 3.14424\n",
      "epoch no.3 train no.231330  loss = 4.49796 avg_loss = 3.16418\n",
      "epoch no.3 train no.231340  loss = 1.54550 avg_loss = 3.21427\n",
      "epoch no.3 train no.231350  loss = 1.34540 avg_loss = 3.16729\n",
      "epoch no.3 train no.231360  loss = 3.37788 avg_loss = 3.13715\n",
      "epoch no.3 train no.231370  loss = 2.13666 avg_loss = 3.14521\n",
      "epoch no.3 train no.231380  loss = 2.39111 avg_loss = 3.17312\n",
      "epoch no.3 train no.231390  loss = 1.72547 avg_loss = 3.16315\n",
      "epoch no.3 train no.231400  loss = 3.55339 avg_loss = 3.19055\n",
      "epoch no.3 train no.231410  loss = 2.72250 avg_loss = 3.17528\n",
      "epoch no.3 train no.231420  loss = 2.54959 avg_loss = 3.18871\n",
      "epoch no.3 train no.231430  loss = 2.78953 avg_loss = 3.17099\n",
      "epoch no.3 train no.231440  loss = 4.43271 avg_loss = 3.17758\n",
      "epoch no.3 train no.231450  loss = 2.18042 avg_loss = 3.20267\n",
      "epoch no.3 train no.231460  loss = 4.24271 avg_loss = 3.17943\n",
      "epoch no.3 train no.231470  loss = 2.94718 avg_loss = 3.18464\n",
      "epoch no.3 train no.231480  loss = 2.92911 avg_loss = 3.20573\n",
      "epoch no.3 train no.231490  loss = 3.92307 avg_loss = 3.25610\n",
      "epoch no.3 train no.231500  loss = 3.47627 avg_loss = 3.30677\n",
      "epoch no.3 train no.231510  loss = 3.42051 avg_loss = 3.28522\n",
      "epoch no.3 train no.231520  loss = 2.75746 avg_loss = 3.30830\n",
      "epoch no.3 train no.231530  loss = 2.69831 avg_loss = 3.35490\n",
      "epoch no.3 train no.231540  loss = 5.19229 avg_loss = 3.34880\n",
      "epoch no.3 train no.231550  loss = 5.76768 avg_loss = 3.34056\n",
      "epoch no.3 train no.231560  loss = 2.12517 avg_loss = 3.30774\n",
      "epoch no.3 train no.231570  loss = 2.20177 avg_loss = 3.31556\n",
      "epoch no.3 train no.231580  loss = 3.75424 avg_loss = 3.32981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.231590  loss = 3.34554 avg_loss = 3.32464\n",
      "epoch no.3 train no.231600  loss = 2.78101 avg_loss = 3.28467\n",
      "epoch no.3 train no.231610  loss = 2.79203 avg_loss = 3.25505\n",
      "epoch no.3 train no.231620  loss = 3.73506 avg_loss = 3.30358\n",
      "epoch no.3 train no.231630  loss = 2.52281 avg_loss = 3.31382\n",
      "epoch no.3 train no.231640  loss = 1.92255 avg_loss = 3.28773\n",
      "epoch no.3 train no.231650  loss = 2.66982 avg_loss = 3.21687\n",
      "epoch no.3 train no.231660  loss = 1.99916 avg_loss = 3.20930\n",
      "epoch no.3 train no.231670  loss = 2.04486 avg_loss = 3.20255\n",
      "epoch no.3 train no.231680  loss = 2.54098 avg_loss = 3.19827\n",
      "epoch no.3 train no.231690  loss = 3.63692 avg_loss = 3.20393\n",
      "epoch no.3 train no.231700  loss = 2.99964 avg_loss = 3.20938\n",
      "epoch no.3 train no.231710  loss = 2.62147 avg_loss = 3.22516\n",
      "epoch no.3 train no.231720  loss = 3.98492 avg_loss = 3.24098\n",
      "epoch no.3 train no.231730  loss = 3.21861 avg_loss = 3.23603\n",
      "epoch no.3 train no.231740  loss = 2.54216 avg_loss = 3.21437\n",
      "epoch no.3 train no.231750  loss = 2.48977 avg_loss = 3.20773\n",
      "epoch no.3 train no.231760  loss = 3.75570 avg_loss = 3.23100\n",
      "epoch no.3 train no.231770  loss = 4.13454 avg_loss = 3.27613\n",
      "epoch no.3 train no.231780  loss = 4.13270 avg_loss = 3.29376\n",
      "epoch no.3 train no.231790  loss = 3.84589 avg_loss = 3.30270\n",
      "epoch no.3 train no.231800  loss = 2.30422 avg_loss = 3.33788\n",
      "epoch no.3 train no.231810  loss = 2.65298 avg_loss = 3.32090\n",
      "epoch no.3 train no.231820  loss = 2.62349 avg_loss = 3.30779\n",
      "epoch no.3 train no.231830  loss = 2.34692 avg_loss = 3.28403\n",
      "epoch no.3 train no.231840  loss = 4.22787 avg_loss = 3.26511\n",
      "epoch no.3 train no.231850  loss = 2.94775 avg_loss = 3.24763\n",
      "epoch no.3 train no.231860  loss = 1.97750 avg_loss = 3.19234\n",
      "epoch no.3 train no.231870  loss = 3.26742 avg_loss = 3.17382\n",
      "epoch no.3 train no.231880  loss = 3.19859 avg_loss = 3.18449\n",
      "epoch no.3 train no.231890  loss = 3.16779 avg_loss = 3.23555\n",
      "epoch no.3 train no.231900  loss = 4.14685 avg_loss = 3.21348\n",
      "epoch no.3 train no.231910  loss = 2.46236 avg_loss = 3.23621\n",
      "epoch no.3 train no.231920  loss = 2.64920 avg_loss = 3.27938\n",
      "epoch no.3 train no.231930  loss = 2.99697 avg_loss = 3.27365\n",
      "epoch no.3 train no.231940  loss = 3.87749 avg_loss = 3.30415\n",
      "epoch no.3 train no.231950  loss = 4.05032 avg_loss = 3.29647\n",
      "epoch no.3 train no.231960  loss = 3.04007 avg_loss = 3.32918\n",
      "epoch no.3 train no.231970  loss = 3.45043 avg_loss = 3.32696\n",
      "epoch no.3 train no.231980  loss = 3.67551 avg_loss = 3.28206\n",
      "epoch no.3 train no.231990  loss = 1.93170 avg_loss = 3.27193\n",
      "epoch no.3 train no.232000  loss = 5.22558 avg_loss = 3.28290\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '팝', '송', '▁모음', '</s>']\n",
      "추억의 올드팝송 모음</s>\n",
      "epoch no.3 train no.232010  loss = 1.90111 avg_loss = 3.23123\n",
      "epoch no.3 train no.232020  loss = 3.67906 avg_loss = 3.27471\n",
      "epoch no.3 train no.232030  loss = 4.17157 avg_loss = 3.29731\n",
      "epoch no.3 train no.232040  loss = 2.63151 avg_loss = 3.25499\n",
      "epoch no.3 train no.232050  loss = 3.68353 avg_loss = 3.24988\n",
      "epoch no.3 train no.232060  loss = 2.22717 avg_loss = 3.22115\n",
      "epoch no.3 train no.232070  loss = 3.43691 avg_loss = 3.24125\n",
      "epoch no.3 train no.232080  loss = 4.09239 avg_loss = 3.25708\n",
      "epoch no.3 train no.232090  loss = 4.63900 avg_loss = 3.24522\n",
      "epoch no.3 train no.232100  loss = 2.69421 avg_loss = 3.23578\n",
      "epoch no.3 train no.232110  loss = 4.43766 avg_loss = 3.23696\n",
      "epoch no.3 train no.232120  loss = 3.16763 avg_loss = 3.22698\n",
      "epoch no.3 train no.232130  loss = 2.78914 avg_loss = 3.26095\n",
      "epoch no.3 train no.232140  loss = 2.43536 avg_loss = 3.23961\n",
      "epoch no.3 train no.232150  loss = 0.85346 avg_loss = 3.15377\n",
      "epoch no.3 train no.232160  loss = 2.84725 avg_loss = 3.14874\n",
      "epoch no.3 train no.232170  loss = 4.09551 avg_loss = 3.16457\n",
      "epoch no.3 train no.232180  loss = 2.29095 avg_loss = 3.17437\n",
      "epoch no.3 train no.232190  loss = 4.58206 avg_loss = 3.19213\n",
      "epoch no.3 train no.232200  loss = 2.95809 avg_loss = 3.20749\n",
      "epoch no.3 train no.232210  loss = 2.59677 avg_loss = 3.17867\n",
      "epoch no.3 train no.232220  loss = 2.74987 avg_loss = 3.15609\n",
      "epoch no.3 train no.232230  loss = 2.34434 avg_loss = 3.17145\n",
      "epoch no.3 train no.232240  loss = 3.45384 avg_loss = 3.17014\n",
      "epoch no.3 train no.232250  loss = 2.86091 avg_loss = 3.20498\n",
      "epoch no.3 train no.232260  loss = 2.71381 avg_loss = 3.19258\n",
      "epoch no.3 train no.232270  loss = 2.40371 avg_loss = 3.21572\n",
      "epoch no.3 train no.232280  loss = 2.92598 avg_loss = 3.20014\n",
      "epoch no.3 train no.232290  loss = 3.24111 avg_loss = 3.23407\n",
      "epoch no.3 train no.232300  loss = 3.47762 avg_loss = 3.21417\n",
      "epoch no.3 train no.232310  loss = 1.90755 avg_loss = 3.22229\n",
      "epoch no.3 train no.232320  loss = 1.96840 avg_loss = 3.21258\n",
      "epoch no.3 train no.232330  loss = 2.59576 avg_loss = 3.19893\n",
      "epoch no.3 train no.232340  loss = 2.93349 avg_loss = 3.17234\n",
      "epoch no.3 train no.232350  loss = 2.05089 avg_loss = 3.21538\n",
      "epoch no.3 train no.232360  loss = 3.61524 avg_loss = 3.20473\n",
      "epoch no.3 train no.232370  loss = 4.45294 avg_loss = 3.23236\n",
      "epoch no.3 train no.232380  loss = 2.52685 avg_loss = 3.25058\n",
      "epoch no.3 train no.232390  loss = 5.81171 avg_loss = 3.26835\n",
      "epoch no.3 train no.232400  loss = 3.31486 avg_loss = 3.24457\n",
      "epoch no.3 train no.232410  loss = 3.42268 avg_loss = 3.24559\n",
      "epoch no.3 train no.232420  loss = 3.30240 avg_loss = 3.26957\n",
      "epoch no.3 train no.232430  loss = 2.48785 avg_loss = 3.30005\n",
      "epoch no.3 train no.232440  loss = 3.90596 avg_loss = 3.27139\n",
      "epoch no.3 train no.232450  loss = 3.73415 avg_loss = 3.31536\n",
      "epoch no.3 train no.232460  loss = 4.52139 avg_loss = 3.32723\n",
      "epoch no.3 train no.232470  loss = 3.79529 avg_loss = 3.28901\n",
      "epoch no.3 train no.232480  loss = 2.87196 avg_loss = 3.25696\n",
      "epoch no.3 train no.232490  loss = 3.08068 avg_loss = 3.24035\n",
      "epoch no.3 train no.232500  loss = 3.45399 avg_loss = 3.28878\n",
      "epoch no.3 train no.232510  loss = 4.77608 avg_loss = 3.26682\n",
      "epoch no.3 train no.232520  loss = 3.15783 avg_loss = 3.26494\n",
      "epoch no.3 train no.232530  loss = 2.35461 avg_loss = 3.27146\n",
      "epoch no.3 train no.232540  loss = 3.46413 avg_loss = 3.25990\n",
      "epoch no.3 train no.232550  loss = 4.84309 avg_loss = 3.30799\n",
      "epoch no.3 train no.232560  loss = 2.76958 avg_loss = 3.32307\n",
      "epoch no.3 train no.232570  loss = 4.58221 avg_loss = 3.32552\n",
      "epoch no.3 train no.232580  loss = 2.72460 avg_loss = 3.27062\n",
      "epoch no.3 train no.232590  loss = 1.86282 avg_loss = 3.33742\n",
      "epoch no.3 train no.232600  loss = 3.47066 avg_loss = 3.36724\n",
      "epoch no.3 train no.232610  loss = 3.38906 avg_loss = 3.33065\n",
      "epoch no.3 train no.232620  loss = 3.67610 avg_loss = 3.28402\n",
      "epoch no.3 train no.232630  loss = 5.65561 avg_loss = 3.29044\n",
      "epoch no.3 train no.232640  loss = 3.25007 avg_loss = 3.28177\n",
      "epoch no.3 train no.232650  loss = 2.14301 avg_loss = 3.24985\n",
      "epoch no.3 train no.232660  loss = 3.82544 avg_loss = 3.24998\n",
      "epoch no.3 train no.232670  loss = 2.23099 avg_loss = 3.21679\n",
      "epoch no.3 train no.232680  loss = 4.12378 avg_loss = 3.24016\n",
      "epoch no.3 train no.232690  loss = 2.87168 avg_loss = 3.23484\n",
      "epoch no.3 train no.232700  loss = 2.55986 avg_loss = 3.20808\n",
      "epoch no.3 train no.232710  loss = 3.67345 avg_loss = 3.23461\n",
      "epoch no.3 train no.232720  loss = 3.25814 avg_loss = 3.19919\n",
      "epoch no.3 train no.232730  loss = 2.77146 avg_loss = 3.17889\n",
      "epoch no.3 train no.232740  loss = 5.47390 avg_loss = 3.25736\n",
      "epoch no.3 train no.232750  loss = 4.66654 avg_loss = 3.28802\n",
      "epoch no.3 train no.232760  loss = 2.81717 avg_loss = 3.30876\n",
      "epoch no.3 train no.232770  loss = 3.04379 avg_loss = 3.32383\n",
      "epoch no.3 train no.232780  loss = 3.52575 avg_loss = 3.35002\n",
      "epoch no.3 train no.232790  loss = 2.75131 avg_loss = 3.30754\n",
      "epoch no.3 train no.232800  loss = 2.58753 avg_loss = 3.30379\n",
      "epoch no.3 train no.232810  loss = 2.23995 avg_loss = 3.31785\n",
      "epoch no.3 train no.232820  loss = 2.29756 avg_loss = 3.31560\n",
      "epoch no.3 train no.232830  loss = 2.57029 avg_loss = 3.28047\n",
      "epoch no.3 train no.232840  loss = 4.62595 avg_loss = 3.32443\n",
      "epoch no.3 train no.232850  loss = 5.18015 avg_loss = 3.35357\n",
      "epoch no.3 train no.232860  loss = 3.86269 avg_loss = 3.38198\n",
      "epoch no.3 train no.232870  loss = 3.55827 avg_loss = 3.37770\n",
      "epoch no.3 train no.232880  loss = 4.73764 avg_loss = 3.37172\n",
      "epoch no.3 train no.232890  loss = 3.16870 avg_loss = 3.38122\n",
      "epoch no.3 train no.232900  loss = 3.99304 avg_loss = 3.33875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.232910  loss = 4.26198 avg_loss = 3.31247\n",
      "epoch no.3 train no.232920  loss = 4.54479 avg_loss = 3.39232\n",
      "epoch no.3 train no.232930  loss = 2.76171 avg_loss = 3.38673\n",
      "epoch no.3 train no.232940  loss = 2.70502 avg_loss = 3.37600\n",
      "epoch no.3 train no.232950  loss = 4.50355 avg_loss = 3.40113\n",
      "epoch no.3 train no.232960  loss = 2.32508 avg_loss = 3.36944\n",
      "epoch no.3 train no.232970  loss = 2.26679 avg_loss = 3.36238\n",
      "epoch no.3 train no.232980  loss = 3.77440 avg_loss = 3.38862\n",
      "epoch no.3 train no.232990  loss = 4.66847 avg_loss = 3.40025\n",
      "epoch no.3 train no.233000  loss = 2.85220 avg_loss = 3.36870\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.233010  loss = 3.11207 avg_loss = 3.38610\n",
      "epoch no.3 train no.233020  loss = 2.80683 avg_loss = 3.35323\n",
      "epoch no.3 train no.233030  loss = 4.35342 avg_loss = 3.36833\n",
      "epoch no.3 train no.233040  loss = 3.85862 avg_loss = 3.36145\n",
      "epoch no.3 train no.233050  loss = 4.66548 avg_loss = 3.38564\n",
      "epoch no.3 train no.233060  loss = 3.30016 avg_loss = 3.34348\n",
      "epoch no.3 train no.233070  loss = 2.25400 avg_loss = 3.29379\n",
      "epoch no.3 train no.233080  loss = 3.30783 avg_loss = 3.30386\n",
      "epoch no.3 train no.233090  loss = 4.54713 avg_loss = 3.27439\n",
      "epoch no.3 train no.233100  loss = 2.63502 avg_loss = 3.23933\n",
      "epoch no.3 train no.233110  loss = 3.49482 avg_loss = 3.21748\n",
      "epoch no.3 train no.233120  loss = 1.89473 avg_loss = 3.21711\n",
      "epoch no.3 train no.233130  loss = 3.37647 avg_loss = 3.23071\n",
      "epoch no.3 train no.233140  loss = 3.49593 avg_loss = 3.27010\n",
      "epoch no.3 train no.233150  loss = 4.13536 avg_loss = 3.31230\n",
      "epoch no.3 train no.233160  loss = 2.06477 avg_loss = 3.34359\n",
      "epoch no.3 train no.233170  loss = 2.21631 avg_loss = 3.31655\n",
      "epoch no.3 train no.233180  loss = 2.09899 avg_loss = 3.27788\n",
      "epoch no.3 train no.233190  loss = 3.71129 avg_loss = 3.27996\n",
      "epoch no.3 train no.233200  loss = 3.94513 avg_loss = 3.27212\n",
      "epoch no.3 train no.233210  loss = 3.39610 avg_loss = 3.27142\n",
      "epoch no.3 train no.233220  loss = 4.08318 avg_loss = 3.26814\n",
      "epoch no.3 train no.233230  loss = 4.25187 avg_loss = 3.23713\n",
      "epoch no.3 train no.233240  loss = 1.99559 avg_loss = 3.27889\n",
      "epoch no.3 train no.233250  loss = 4.10631 avg_loss = 3.28953\n",
      "epoch no.3 train no.233260  loss = 2.14801 avg_loss = 3.30197\n",
      "epoch no.3 train no.233270  loss = 4.74075 avg_loss = 3.30109\n",
      "epoch no.3 train no.233280  loss = 3.17207 avg_loss = 3.30529\n",
      "epoch no.3 train no.233290  loss = 3.61270 avg_loss = 3.29278\n",
      "epoch no.3 train no.233300  loss = 3.30744 avg_loss = 3.25812\n",
      "epoch no.3 train no.233310  loss = 3.01665 avg_loss = 3.21824\n",
      "epoch no.3 train no.233320  loss = 2.46255 avg_loss = 3.22326\n",
      "epoch no.3 train no.233330  loss = 3.33550 avg_loss = 3.24131\n",
      "epoch no.3 train no.233340  loss = 2.89112 avg_loss = 3.25130\n",
      "epoch no.3 train no.233350  loss = 3.32156 avg_loss = 3.27770\n",
      "epoch no.3 train no.233360  loss = 3.03174 avg_loss = 3.33854\n",
      "epoch no.3 train no.233370  loss = 4.68742 avg_loss = 3.32295\n",
      "epoch no.3 train no.233380  loss = 1.91641 avg_loss = 3.29626\n",
      "epoch no.3 train no.233390  loss = 4.59334 avg_loss = 3.32419\n",
      "epoch no.3 train no.233400  loss = 4.53474 avg_loss = 3.30766\n",
      "epoch no.3 train no.233410  loss = 2.93056 avg_loss = 3.28294\n",
      "epoch no.3 train no.233420  loss = 2.82349 avg_loss = 3.27687\n",
      "epoch no.3 train no.233430  loss = 4.22293 avg_loss = 3.24100\n",
      "epoch no.3 train no.233440  loss = 3.81872 avg_loss = 3.21987\n",
      "epoch no.3 train no.233450  loss = 2.96958 avg_loss = 3.17161\n",
      "epoch no.3 train no.233460  loss = 3.44530 avg_loss = 3.21035\n",
      "epoch no.3 train no.233470  loss = 1.68043 avg_loss = 3.18353\n",
      "epoch no.3 train no.233480  loss = 4.50671 avg_loss = 3.23720\n",
      "epoch no.3 train no.233490  loss = 2.56717 avg_loss = 3.20628\n",
      "epoch no.3 train no.233500  loss = 5.91793 avg_loss = 3.22265\n",
      "epoch no.3 train no.233510  loss = 5.73612 avg_loss = 3.26320\n",
      "epoch no.3 train no.233520  loss = 2.80532 avg_loss = 3.27390\n",
      "epoch no.3 train no.233530  loss = 3.66609 avg_loss = 3.28169\n",
      "epoch no.3 train no.233540  loss = 3.87625 avg_loss = 3.33629\n",
      "epoch no.3 train no.233550  loss = 4.07314 avg_loss = 3.34048\n",
      "epoch no.3 train no.233560  loss = 4.36765 avg_loss = 3.31628\n",
      "epoch no.3 train no.233570  loss = 5.15061 avg_loss = 3.32473\n",
      "epoch no.3 train no.233580  loss = 4.21906 avg_loss = 3.29889\n",
      "epoch no.3 train no.233590  loss = 2.40624 avg_loss = 3.28406\n",
      "epoch no.3 train no.233600  loss = 4.50109 avg_loss = 3.28570\n",
      "epoch no.3 train no.233610  loss = 2.75589 avg_loss = 3.26483\n",
      "epoch no.3 train no.233620  loss = 5.03630 avg_loss = 3.30772\n",
      "epoch no.3 train no.233630  loss = 2.33988 avg_loss = 3.30247\n",
      "epoch no.3 train no.233640  loss = 3.05211 avg_loss = 3.29331\n",
      "epoch no.3 train no.233650  loss = 3.93873 avg_loss = 3.31695\n",
      "epoch no.3 train no.233660  loss = 2.33711 avg_loss = 3.27264\n",
      "epoch no.3 train no.233670  loss = 3.61024 avg_loss = 3.27410\n",
      "epoch no.3 train no.233680  loss = 2.66476 avg_loss = 3.25219\n",
      "epoch no.3 train no.233690  loss = 3.67826 avg_loss = 3.22656\n",
      "epoch no.3 train no.233700  loss = 5.49349 avg_loss = 3.26340\n",
      "epoch no.3 train no.233710  loss = 2.91493 avg_loss = 3.22612\n",
      "epoch no.3 train no.233720  loss = 3.73877 avg_loss = 3.18373\n",
      "epoch no.3 train no.233730  loss = 1.63405 avg_loss = 3.20540\n",
      "epoch no.3 train no.233740  loss = 3.56587 avg_loss = 3.20865\n",
      "epoch no.3 train no.233750  loss = 3.31058 avg_loss = 3.19510\n",
      "epoch no.3 train no.233760  loss = 2.94348 avg_loss = 3.21395\n",
      "epoch no.3 train no.233770  loss = 3.53805 avg_loss = 3.26715\n",
      "epoch no.3 train no.233780  loss = 2.17413 avg_loss = 3.26357\n",
      "epoch no.3 train no.233790  loss = 3.69550 avg_loss = 3.23017\n",
      "epoch no.3 train no.233800  loss = 4.00144 avg_loss = 3.20910\n",
      "epoch no.3 train no.233810  loss = 4.07000 avg_loss = 3.20971\n",
      "epoch no.3 train no.233820  loss = 4.93721 avg_loss = 3.27351\n",
      "epoch no.3 train no.233830  loss = 1.44222 avg_loss = 3.22067\n",
      "epoch no.3 train no.233840  loss = 2.59414 avg_loss = 3.19268\n",
      "epoch no.3 train no.233850  loss = 2.22915 avg_loss = 3.18432\n",
      "epoch no.3 train no.233860  loss = 1.81511 avg_loss = 3.19179\n",
      "epoch no.3 train no.233870  loss = 2.65157 avg_loss = 3.20103\n",
      "epoch no.3 train no.233880  loss = 3.05361 avg_loss = 3.22811\n",
      "epoch no.3 train no.233890  loss = 2.33242 avg_loss = 3.28708\n",
      "epoch no.3 train no.233900  loss = 2.20643 avg_loss = 3.29874\n",
      "epoch no.3 train no.233910  loss = 3.70812 avg_loss = 3.28975\n",
      "epoch no.3 train no.233920  loss = 3.11871 avg_loss = 3.24927\n",
      "epoch no.3 train no.233930  loss = 3.28274 avg_loss = 3.24021\n",
      "epoch no.3 train no.233940  loss = 4.20545 avg_loss = 3.25046\n",
      "epoch no.3 train no.233950  loss = 3.72967 avg_loss = 3.27471\n",
      "epoch no.3 train no.233960  loss = 2.45390 avg_loss = 3.22775\n",
      "epoch no.3 train no.233970  loss = 3.47941 avg_loss = 3.23965\n",
      "epoch no.3 train no.233980  loss = 2.95619 avg_loss = 3.20742\n",
      "epoch no.3 train no.233990  loss = 3.65401 avg_loss = 3.19651\n",
      "epoch no.3 train no.234000  loss = 3.59829 avg_loss = 3.19862\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '송', '▁모음', '집', '</s>']\n",
      "추억의 팝송 모음집</s>\n",
      "epoch no.3 train no.234010  loss = 2.49295 avg_loss = 3.15336\n",
      "epoch no.3 train no.234020  loss = 3.77903 avg_loss = 3.24459\n",
      "epoch no.3 train no.234030  loss = 1.91580 avg_loss = 3.19452\n",
      "epoch no.3 train no.234040  loss = 1.89280 avg_loss = 3.21479\n",
      "epoch no.3 train no.234050  loss = 3.82305 avg_loss = 3.21792\n",
      "epoch no.3 train no.234060  loss = 1.90962 avg_loss = 3.16112\n",
      "epoch no.3 train no.234070  loss = 3.24582 avg_loss = 3.17301\n",
      "epoch no.3 train no.234080  loss = 2.40224 avg_loss = 3.18761\n",
      "epoch no.3 train no.234090  loss = 4.29861 avg_loss = 3.17473\n",
      "epoch no.3 train no.234100  loss = 3.31468 avg_loss = 3.19954\n",
      "epoch no.3 train no.234110  loss = 1.67147 avg_loss = 3.23357\n",
      "epoch no.3 train no.234120  loss = 3.37465 avg_loss = 3.21927\n",
      "epoch no.3 train no.234130  loss = 2.19078 avg_loss = 3.21353\n",
      "epoch no.3 train no.234140  loss = 3.71539 avg_loss = 3.18425\n",
      "epoch no.3 train no.234150  loss = 2.96005 avg_loss = 3.16722\n",
      "epoch no.3 train no.234160  loss = 2.51032 avg_loss = 3.16892\n",
      "epoch no.3 train no.234170  loss = 4.27535 avg_loss = 3.17758\n",
      "epoch no.3 train no.234180  loss = 3.46652 avg_loss = 3.20128\n",
      "epoch no.3 train no.234190  loss = 3.83034 avg_loss = 3.24694\n",
      "epoch no.3 train no.234200  loss = 4.11693 avg_loss = 3.26491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.234210  loss = 2.78677 avg_loss = 3.23791\n",
      "epoch no.3 train no.234220  loss = 3.75402 avg_loss = 3.19376\n",
      "epoch no.3 train no.234230  loss = 3.98551 avg_loss = 3.16839\n",
      "epoch no.3 train no.234240  loss = 3.33935 avg_loss = 3.22490\n",
      "epoch no.3 train no.234250  loss = 4.12896 avg_loss = 3.23204\n",
      "epoch no.3 train no.234260  loss = 2.75510 avg_loss = 3.26825\n",
      "epoch no.3 train no.234270  loss = 3.96246 avg_loss = 3.27933\n",
      "epoch no.3 train no.234280  loss = 3.18577 avg_loss = 3.27775\n",
      "epoch no.3 train no.234290  loss = 3.78362 avg_loss = 3.28302\n",
      "epoch no.3 train no.234300  loss = 3.66529 avg_loss = 3.30156\n",
      "epoch no.3 train no.234310  loss = 5.06697 avg_loss = 3.33154\n",
      "epoch no.3 train no.234320  loss = 4.14128 avg_loss = 3.32195\n",
      "epoch no.3 train no.234330  loss = 2.80702 avg_loss = 3.34459\n",
      "epoch no.3 train no.234340  loss = 2.41543 avg_loss = 3.29192\n",
      "epoch no.3 train no.234350  loss = 3.73422 avg_loss = 3.27481\n",
      "epoch no.3 train no.234360  loss = 4.43699 avg_loss = 3.28879\n",
      "epoch no.3 train no.234370  loss = 2.91247 avg_loss = 3.32987\n",
      "epoch no.3 train no.234380  loss = 3.09882 avg_loss = 3.30310\n",
      "epoch no.3 train no.234390  loss = 2.81834 avg_loss = 3.27120\n",
      "epoch no.3 train no.234400  loss = 3.38001 avg_loss = 3.30370\n",
      "epoch no.3 train no.234410  loss = 2.61614 avg_loss = 3.29487\n",
      "epoch no.3 train no.234420  loss = 4.57597 avg_loss = 3.32797\n",
      "epoch no.3 train no.234430  loss = 3.30945 avg_loss = 3.28898\n",
      "epoch no.3 train no.234440  loss = 3.55651 avg_loss = 3.22444\n",
      "epoch no.3 train no.234450  loss = 3.53241 avg_loss = 3.21518\n",
      "epoch no.3 train no.234460  loss = 3.63623 avg_loss = 3.22081\n",
      "epoch no.3 train no.234470  loss = 5.03152 avg_loss = 3.27252\n",
      "epoch no.3 train no.234480  loss = 3.50079 avg_loss = 3.32905\n",
      "epoch no.3 train no.234490  loss = 2.17796 avg_loss = 3.33495\n",
      "epoch no.3 train no.234500  loss = 4.42593 avg_loss = 3.32882\n",
      "epoch no.3 train no.234510  loss = 3.13069 avg_loss = 3.33695\n",
      "epoch no.3 train no.234520  loss = 2.88499 avg_loss = 3.30034\n",
      "epoch no.3 train no.234530  loss = 2.31991 avg_loss = 3.25963\n",
      "epoch no.3 train no.234540  loss = 3.29934 avg_loss = 3.24485\n",
      "epoch no.3 train no.234550  loss = 2.53272 avg_loss = 3.23614\n",
      "epoch no.3 train no.234560  loss = 2.32442 avg_loss = 3.23549\n",
      "epoch no.3 train no.234570  loss = 2.79094 avg_loss = 3.27231\n",
      "epoch no.3 train no.234580  loss = 3.85163 avg_loss = 3.28861\n",
      "epoch no.3 train no.234590  loss = 2.81177 avg_loss = 3.28509\n",
      "epoch no.3 train no.234600  loss = 1.49974 avg_loss = 3.28996\n",
      "epoch no.3 train no.234610  loss = 4.26997 avg_loss = 3.26960\n",
      "epoch no.3 train no.234620  loss = 3.33324 avg_loss = 3.30244\n",
      "epoch no.3 train no.234630  loss = 2.80521 avg_loss = 3.32722\n",
      "epoch no.3 train no.234640  loss = 3.00535 avg_loss = 3.34455\n",
      "epoch no.3 train no.234650  loss = 3.01559 avg_loss = 3.33668\n",
      "epoch no.3 train no.234660  loss = 4.11975 avg_loss = 3.35815\n",
      "epoch no.3 train no.234670  loss = 3.03100 avg_loss = 3.30550\n",
      "epoch no.3 train no.234680  loss = 2.42733 avg_loss = 3.33343\n",
      "epoch no.3 train no.234690  loss = 2.62430 avg_loss = 3.32013\n",
      "epoch no.3 train no.234700  loss = 2.29207 avg_loss = 3.29106\n",
      "epoch no.3 train no.234710  loss = 4.32583 avg_loss = 3.28830\n",
      "epoch no.3 train no.234720  loss = 3.76006 avg_loss = 3.29011\n",
      "epoch no.3 train no.234730  loss = 5.61221 avg_loss = 3.32044\n",
      "epoch no.3 train no.234740  loss = 2.31146 avg_loss = 3.29076\n",
      "epoch no.3 train no.234750  loss = 3.72943 avg_loss = 3.27850\n",
      "epoch no.3 train no.234760  loss = 3.35077 avg_loss = 3.25441\n",
      "epoch no.3 train no.234770  loss = 2.86519 avg_loss = 3.24994\n",
      "epoch no.3 train no.234780  loss = 3.97313 avg_loss = 3.24663\n",
      "epoch no.3 train no.234790  loss = 4.63715 avg_loss = 3.25695\n",
      "epoch no.3 train no.234800  loss = 1.98457 avg_loss = 3.22449\n",
      "epoch no.3 train no.234810  loss = 3.12417 avg_loss = 3.26845\n",
      "epoch no.3 train no.234820  loss = 1.99449 avg_loss = 3.22201\n",
      "epoch no.3 train no.234830  loss = 3.03174 avg_loss = 3.24521\n",
      "epoch no.3 train no.234840  loss = 1.95270 avg_loss = 3.25109\n",
      "epoch no.3 train no.234850  loss = 3.38207 avg_loss = 3.26953\n",
      "epoch no.3 train no.234860  loss = 2.11811 avg_loss = 3.25582\n",
      "epoch no.3 train no.234870  loss = 2.28667 avg_loss = 3.25848\n",
      "epoch no.3 train no.234880  loss = 5.06284 avg_loss = 3.25965\n",
      "epoch no.3 train no.234890  loss = 3.60920 avg_loss = 3.31787\n",
      "epoch no.3 train no.234900  loss = 3.15349 avg_loss = 3.32673\n",
      "epoch no.3 train no.234910  loss = 2.83728 avg_loss = 3.30946\n",
      "epoch no.3 train no.234920  loss = 3.52322 avg_loss = 3.33442\n",
      "epoch no.3 train no.234930  loss = 2.90577 avg_loss = 3.33428\n",
      "epoch no.3 train no.234940  loss = 5.05367 avg_loss = 3.37903\n",
      "epoch no.3 train no.234950  loss = 4.21323 avg_loss = 3.39592\n",
      "epoch no.3 train no.234960  loss = 2.84276 avg_loss = 3.36217\n",
      "epoch no.3 train no.234970  loss = 3.53344 avg_loss = 3.35564\n",
      "epoch no.3 train no.234980  loss = 4.65656 avg_loss = 3.32992\n",
      "epoch no.3 train no.234990  loss = 3.48375 avg_loss = 3.32826\n",
      "epoch no.3 train no.235000  loss = 4.33013 avg_loss = 3.35436\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.235010  loss = 2.53161 avg_loss = 3.32294\n",
      "epoch no.3 train no.235020  loss = 2.97920 avg_loss = 3.29239\n",
      "epoch no.3 train no.235030  loss = 2.36337 avg_loss = 3.28620\n",
      "epoch no.3 train no.235040  loss = 3.35170 avg_loss = 3.32545\n",
      "epoch no.3 train no.235050  loss = 3.63891 avg_loss = 3.30958\n",
      "epoch no.3 train no.235060  loss = 2.87689 avg_loss = 3.33817\n",
      "epoch no.3 train no.235070  loss = 2.50271 avg_loss = 3.33895\n",
      "epoch no.3 train no.235080  loss = 3.20014 avg_loss = 3.34000\n",
      "epoch no.3 train no.235090  loss = 2.46302 avg_loss = 3.32575\n",
      "epoch no.3 train no.235100  loss = 2.39223 avg_loss = 3.30879\n",
      "epoch no.3 train no.235110  loss = 3.09655 avg_loss = 3.33019\n",
      "epoch no.3 train no.235120  loss = 2.05435 avg_loss = 3.30866\n",
      "epoch no.3 train no.235130  loss = 3.61047 avg_loss = 3.29442\n",
      "epoch no.3 train no.235140  loss = 3.50802 avg_loss = 3.31650\n",
      "epoch no.3 train no.235150  loss = 2.43621 avg_loss = 3.29070\n",
      "epoch no.3 train no.235160  loss = 3.97182 avg_loss = 3.28711\n",
      "epoch no.3 train no.235170  loss = 3.89488 avg_loss = 3.27654\n",
      "epoch no.3 train no.235180  loss = 4.44788 avg_loss = 3.27506\n",
      "epoch no.3 train no.235190  loss = 3.19577 avg_loss = 3.26501\n",
      "epoch no.3 train no.235200  loss = 3.47110 avg_loss = 3.26838\n",
      "epoch no.3 train no.235210  loss = 3.37764 avg_loss = 3.25461\n",
      "epoch no.3 train no.235220  loss = 5.04767 avg_loss = 3.24422\n",
      "epoch no.3 train no.235230  loss = 4.72064 avg_loss = 3.28185\n",
      "epoch no.3 train no.235240  loss = 4.05280 avg_loss = 3.29326\n",
      "epoch no.3 train no.235250  loss = 4.10164 avg_loss = 3.30043\n",
      "epoch no.3 train no.235260  loss = 2.94562 avg_loss = 3.28518\n",
      "epoch no.3 train no.235270  loss = 4.19209 avg_loss = 3.27553\n",
      "epoch no.3 train no.235280  loss = 2.71816 avg_loss = 3.27827\n",
      "epoch no.3 train no.235290  loss = 2.83431 avg_loss = 3.26468\n",
      "epoch no.3 train no.235300  loss = 4.46141 avg_loss = 3.24973\n",
      "epoch no.3 train no.235310  loss = 3.56448 avg_loss = 3.24583\n",
      "epoch no.3 train no.235320  loss = 3.29778 avg_loss = 3.21841\n",
      "epoch no.3 train no.235330  loss = 2.78826 avg_loss = 3.20440\n",
      "epoch no.3 train no.235340  loss = 3.42749 avg_loss = 3.23337\n",
      "epoch no.3 train no.235350  loss = 1.94497 avg_loss = 3.26922\n",
      "epoch no.3 train no.235360  loss = 3.85345 avg_loss = 3.26198\n",
      "epoch no.3 train no.235370  loss = 4.57166 avg_loss = 3.27103\n",
      "epoch no.3 train no.235380  loss = 3.89664 avg_loss = 3.29629\n",
      "epoch no.3 train no.235390  loss = 3.82504 avg_loss = 3.29716\n",
      "epoch no.3 train no.235400  loss = 3.13614 avg_loss = 3.31542\n",
      "epoch no.3 train no.235410  loss = 3.35868 avg_loss = 3.33366\n",
      "epoch no.3 train no.235420  loss = 2.74379 avg_loss = 3.33266\n",
      "epoch no.3 train no.235430  loss = 5.04303 avg_loss = 3.32896\n",
      "epoch no.3 train no.235440  loss = 2.26724 avg_loss = 3.30251\n",
      "epoch no.3 train no.235450  loss = 2.90954 avg_loss = 3.27220\n",
      "epoch no.3 train no.235460  loss = 4.72198 avg_loss = 3.28229\n",
      "epoch no.3 train no.235470  loss = 3.92757 avg_loss = 3.24578\n",
      "epoch no.3 train no.235480  loss = 2.88207 avg_loss = 3.26252\n",
      "epoch no.3 train no.235490  loss = 2.96865 avg_loss = 3.26848\n",
      "epoch no.3 train no.235500  loss = 2.56327 avg_loss = 3.28867\n",
      "epoch no.3 train no.235510  loss = 3.22273 avg_loss = 3.27316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.235520  loss = 2.00835 avg_loss = 3.26324\n",
      "epoch no.3 train no.235530  loss = 3.35623 avg_loss = 3.35408\n",
      "epoch no.3 train no.235540  loss = 3.36039 avg_loss = 3.35218\n",
      "epoch no.3 train no.235550  loss = 2.29358 avg_loss = 3.28483\n",
      "epoch no.3 train no.235560  loss = 3.07456 avg_loss = 3.26178\n",
      "epoch no.3 train no.235570  loss = 3.26227 avg_loss = 3.28184\n",
      "epoch no.3 train no.235580  loss = 3.79106 avg_loss = 3.30887\n",
      "epoch no.3 train no.235590  loss = 3.21731 avg_loss = 3.28757\n",
      "epoch no.3 train no.235600  loss = 5.68625 avg_loss = 3.29306\n",
      "epoch no.3 train no.235610  loss = 3.97431 avg_loss = 3.28329\n",
      "epoch no.3 train no.235620  loss = 3.75552 avg_loss = 3.29620\n",
      "epoch no.3 train no.235630  loss = 1.60993 avg_loss = 3.30666\n",
      "epoch no.3 train no.235640  loss = 2.02718 avg_loss = 3.25288\n",
      "epoch no.3 train no.235650  loss = 3.25996 avg_loss = 3.25559\n",
      "epoch no.3 train no.235660  loss = 2.59805 avg_loss = 3.19936\n",
      "epoch no.3 train no.235670  loss = 4.49079 avg_loss = 3.23839\n",
      "epoch no.3 train no.235680  loss = 3.48537 avg_loss = 3.25184\n",
      "epoch no.3 train no.235690  loss = 2.17413 avg_loss = 3.23943\n",
      "epoch no.3 train no.235700  loss = 2.79796 avg_loss = 3.22575\n",
      "epoch no.3 train no.235710  loss = 3.23961 avg_loss = 3.24025\n",
      "epoch no.3 train no.235720  loss = 2.70156 avg_loss = 3.24809\n",
      "epoch no.3 train no.235730  loss = 4.12385 avg_loss = 3.24760\n",
      "epoch no.3 train no.235740  loss = 2.50528 avg_loss = 3.23160\n",
      "epoch no.3 train no.235750  loss = 3.93891 avg_loss = 3.19256\n",
      "epoch no.3 train no.235760  loss = 2.66222 avg_loss = 3.19363\n",
      "epoch no.3 train no.235770  loss = 3.53558 avg_loss = 3.16033\n",
      "epoch no.3 train no.235780  loss = 3.54075 avg_loss = 3.19274\n",
      "epoch no.3 train no.235790  loss = 2.90605 avg_loss = 3.14684\n",
      "epoch no.3 train no.235800  loss = 4.73084 avg_loss = 3.18195\n",
      "epoch no.3 train no.235810  loss = 3.52915 avg_loss = 3.19998\n",
      "epoch no.3 train no.235820  loss = 2.72626 avg_loss = 3.20394\n",
      "epoch no.3 train no.235830  loss = 3.70630 avg_loss = 3.17848\n",
      "epoch no.3 train no.235840  loss = 4.30358 avg_loss = 3.20228\n",
      "epoch no.3 train no.235850  loss = 2.61844 avg_loss = 3.22906\n",
      "epoch no.3 train no.235860  loss = 2.74392 avg_loss = 3.21030\n",
      "epoch no.3 train no.235870  loss = 2.68566 avg_loss = 3.22854\n",
      "epoch no.3 train no.235880  loss = 3.06729 avg_loss = 3.22284\n",
      "epoch no.3 train no.235890  loss = 2.63339 avg_loss = 3.19893\n",
      "epoch no.3 train no.235900  loss = 3.40217 avg_loss = 3.22487\n",
      "epoch no.3 train no.235910  loss = 3.32531 avg_loss = 3.20541\n",
      "epoch no.3 train no.235920  loss = 4.55482 avg_loss = 3.22200\n",
      "epoch no.3 train no.235930  loss = 3.26779 avg_loss = 3.23410\n",
      "epoch no.3 train no.235940  loss = 4.34567 avg_loss = 3.26933\n",
      "epoch no.3 train no.235950  loss = 3.50997 avg_loss = 3.28116\n",
      "epoch no.3 train no.235960  loss = 2.30884 avg_loss = 3.30451\n",
      "epoch no.3 train no.235970  loss = 3.16515 avg_loss = 3.32511\n",
      "epoch no.3 train no.235980  loss = 3.27741 avg_loss = 3.29933\n",
      "epoch no.3 train no.235990  loss = 3.21040 avg_loss = 3.26752\n",
      "epoch no.3 train no.236000  loss = 2.90531 avg_loss = 3.21834\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '집', '</s>']\n",
      "추억의 발라드 모음집</s>\n",
      "epoch no.3 train no.236010  loss = 2.37346 avg_loss = 3.17098\n",
      "epoch no.3 train no.236020  loss = 2.87890 avg_loss = 3.14628\n",
      "epoch no.3 train no.236030  loss = 5.16331 avg_loss = 3.21475\n",
      "epoch no.3 train no.236040  loss = 2.93666 avg_loss = 3.22368\n",
      "epoch no.3 train no.236050  loss = 2.95642 avg_loss = 3.19098\n",
      "epoch no.3 train no.236060  loss = 2.46519 avg_loss = 3.22427\n",
      "epoch no.3 train no.236070  loss = 2.37277 avg_loss = 3.21591\n",
      "epoch no.3 train no.236080  loss = 1.80635 avg_loss = 3.22988\n",
      "epoch no.3 train no.236090  loss = 3.80642 avg_loss = 3.21494\n",
      "epoch no.3 train no.236100  loss = 4.61244 avg_loss = 3.25808\n",
      "epoch no.3 train no.236110  loss = 4.16364 avg_loss = 3.30106\n",
      "epoch no.3 train no.236120  loss = 1.95925 avg_loss = 3.29178\n",
      "epoch no.3 train no.236130  loss = 3.63860 avg_loss = 3.28004\n",
      "epoch no.3 train no.236140  loss = 3.42903 avg_loss = 3.29348\n",
      "epoch no.3 train no.236150  loss = 3.68906 avg_loss = 3.26879\n",
      "epoch no.3 train no.236160  loss = 2.47733 avg_loss = 3.21532\n",
      "epoch no.3 train no.236170  loss = 2.52025 avg_loss = 3.22868\n",
      "epoch no.3 train no.236180  loss = 4.40971 avg_loss = 3.24931\n",
      "epoch no.3 train no.236190  loss = 3.29693 avg_loss = 3.28912\n",
      "epoch no.3 train no.236200  loss = 2.97942 avg_loss = 3.25557\n",
      "epoch no.3 train no.236210  loss = 3.33399 avg_loss = 3.26658\n",
      "epoch no.3 train no.236220  loss = 3.93365 avg_loss = 3.29554\n",
      "epoch no.3 train no.236230  loss = 2.16414 avg_loss = 3.32033\n",
      "epoch no.3 train no.236240  loss = 2.33440 avg_loss = 3.30467\n",
      "epoch no.3 train no.236250  loss = 3.63771 avg_loss = 3.30227\n",
      "epoch no.3 train no.236260  loss = 2.89465 avg_loss = 3.26912\n",
      "epoch no.3 train no.236270  loss = 3.14721 avg_loss = 3.26668\n",
      "epoch no.3 train no.236280  loss = 4.23933 avg_loss = 3.26266\n",
      "epoch no.3 train no.236290  loss = 4.61370 avg_loss = 3.26013\n",
      "epoch no.3 train no.236300  loss = 2.41077 avg_loss = 3.19771\n",
      "epoch no.3 train no.236310  loss = 3.32690 avg_loss = 3.21491\n",
      "epoch no.3 train no.236320  loss = 3.95408 avg_loss = 3.23197\n",
      "epoch no.3 train no.236330  loss = 3.62019 avg_loss = 3.19745\n",
      "epoch no.3 train no.236340  loss = 3.59498 avg_loss = 3.24068\n",
      "epoch no.3 train no.236350  loss = 3.59901 avg_loss = 3.20064\n",
      "epoch no.3 train no.236360  loss = 4.34446 avg_loss = 3.26994\n",
      "epoch no.3 train no.236370  loss = 3.39931 avg_loss = 3.26901\n",
      "epoch no.3 train no.236380  loss = 4.57349 avg_loss = 3.27923\n",
      "epoch no.3 train no.236390  loss = 3.45157 avg_loss = 3.23321\n",
      "epoch no.3 train no.236400  loss = 3.34076 avg_loss = 3.21841\n",
      "epoch no.3 train no.236410  loss = 2.95352 avg_loss = 3.16703\n",
      "epoch no.3 train no.236420  loss = 2.34682 avg_loss = 3.17958\n",
      "epoch no.3 train no.236430  loss = 2.52581 avg_loss = 3.16650\n",
      "epoch no.3 train no.236440  loss = 3.50254 avg_loss = 3.17520\n",
      "epoch no.3 train no.236450  loss = 2.49896 avg_loss = 3.15303\n",
      "epoch no.3 train no.236460  loss = 2.97374 avg_loss = 3.17246\n",
      "epoch no.3 train no.236470  loss = 2.81489 avg_loss = 3.17054\n",
      "epoch no.3 train no.236480  loss = 3.12933 avg_loss = 3.17315\n",
      "epoch no.3 train no.236490  loss = 2.92075 avg_loss = 3.24260\n",
      "epoch no.3 train no.236500  loss = 2.43461 avg_loss = 3.23038\n",
      "epoch no.3 train no.236510  loss = 2.14548 avg_loss = 3.21926\n",
      "epoch no.3 train no.236520  loss = 3.18126 avg_loss = 3.18515\n",
      "epoch no.3 train no.236530  loss = 4.37948 avg_loss = 3.19107\n",
      "epoch no.3 train no.236540  loss = 3.67171 avg_loss = 3.23777\n",
      "epoch no.3 train no.236550  loss = 3.42916 avg_loss = 3.24734\n",
      "epoch no.3 train no.236560  loss = 3.40546 avg_loss = 3.22884\n",
      "epoch no.3 train no.236570  loss = 3.89679 avg_loss = 3.25651\n",
      "epoch no.3 train no.236580  loss = 3.81680 avg_loss = 3.23100\n",
      "epoch no.3 train no.236590  loss = 2.50567 avg_loss = 3.19760\n",
      "epoch no.3 train no.236600  loss = 5.37333 avg_loss = 3.20820\n",
      "epoch no.3 train no.236610  loss = 3.77627 avg_loss = 3.21840\n",
      "epoch no.3 train no.236620  loss = 2.46437 avg_loss = 3.20633\n",
      "epoch no.3 train no.236630  loss = 5.19499 avg_loss = 3.23136\n",
      "epoch no.3 train no.236640  loss = 4.03566 avg_loss = 3.20320\n",
      "epoch no.3 train no.236650  loss = 3.82264 avg_loss = 3.18340\n",
      "epoch no.3 train no.236660  loss = 2.05074 avg_loss = 3.17373\n",
      "epoch no.3 train no.236670  loss = 3.14937 avg_loss = 3.18872\n",
      "epoch no.3 train no.236680  loss = 5.48560 avg_loss = 3.20378\n",
      "epoch no.3 train no.236690  loss = 3.72301 avg_loss = 3.24034\n",
      "epoch no.3 train no.236700  loss = 4.68192 avg_loss = 3.25233\n",
      "epoch no.3 train no.236710  loss = 1.99624 avg_loss = 3.19804\n",
      "epoch no.3 train no.236720  loss = 2.60283 avg_loss = 3.21995\n",
      "epoch no.3 train no.236730  loss = 1.39582 avg_loss = 3.18882\n",
      "epoch no.3 train no.236740  loss = 4.43940 avg_loss = 3.28963\n",
      "epoch no.3 train no.236750  loss = 3.17244 avg_loss = 3.28151\n",
      "epoch no.3 train no.236760  loss = 3.48379 avg_loss = 3.24274\n",
      "epoch no.3 train no.236770  loss = 2.46890 avg_loss = 3.24180\n",
      "epoch no.3 train no.236780  loss = 2.68240 avg_loss = 3.21018\n",
      "epoch no.3 train no.236790  loss = 2.36514 avg_loss = 3.21319\n",
      "epoch no.3 train no.236800  loss = 2.64436 avg_loss = 3.24138\n",
      "epoch no.3 train no.236810  loss = 3.66968 avg_loss = 3.25641\n",
      "epoch no.3 train no.236820  loss = 3.18497 avg_loss = 3.25311\n",
      "epoch no.3 train no.236830  loss = 3.98554 avg_loss = 3.27959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.236840  loss = 2.41162 avg_loss = 3.32108\n",
      "epoch no.3 train no.236850  loss = 3.42312 avg_loss = 3.32861\n",
      "epoch no.3 train no.236860  loss = 2.48093 avg_loss = 3.34908\n",
      "epoch no.3 train no.236870  loss = 2.71925 avg_loss = 3.31387\n",
      "epoch no.3 train no.236880  loss = 3.28444 avg_loss = 3.32425\n",
      "epoch no.3 train no.236890  loss = 1.89335 avg_loss = 3.31264\n",
      "epoch no.3 train no.236900  loss = 2.39550 avg_loss = 3.30323\n",
      "epoch no.3 train no.236910  loss = 3.39963 avg_loss = 3.28733\n",
      "epoch no.3 train no.236920  loss = 1.53624 avg_loss = 3.30406\n",
      "epoch no.3 train no.236930  loss = 3.00765 avg_loss = 3.25398\n",
      "epoch no.3 train no.236940  loss = 2.60834 avg_loss = 3.22985\n",
      "epoch no.3 train no.236950  loss = 3.83214 avg_loss = 3.19369\n",
      "epoch no.3 train no.236960  loss = 3.61777 avg_loss = 3.18304\n",
      "epoch no.3 train no.236970  loss = 3.37721 avg_loss = 3.20167\n",
      "epoch no.3 train no.236980  loss = 2.84036 avg_loss = 3.19823\n",
      "epoch no.3 train no.236990  loss = 3.15371 avg_loss = 3.22544\n",
      "epoch no.3 train no.237000  loss = 2.70061 avg_loss = 3.21277\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '▁o', 'st', '▁모음', '</s>', '</s>']\n",
      "추억의 드라마 ost 모음집</s>\n",
      "epoch no.3 train no.237010  loss = 1.17415 avg_loss = 3.18344\n",
      "epoch no.3 train no.237020  loss = 3.86601 avg_loss = 3.15351\n",
      "epoch no.3 train no.237030  loss = 1.84026 avg_loss = 3.19587\n",
      "epoch no.3 train no.237040  loss = 3.27455 avg_loss = 3.22116\n",
      "epoch no.3 train no.237050  loss = 4.65701 avg_loss = 3.27761\n",
      "epoch no.3 train no.237060  loss = 3.23380 avg_loss = 3.25676\n",
      "epoch no.3 train no.237070  loss = 2.70387 avg_loss = 3.32365\n",
      "epoch no.3 train no.237080  loss = 4.47691 avg_loss = 3.29534\n",
      "epoch no.3 train no.237090  loss = 2.93794 avg_loss = 3.29157\n",
      "epoch no.3 train no.237100  loss = 4.08639 avg_loss = 3.25711\n",
      "epoch no.3 train no.237110  loss = 3.26584 avg_loss = 3.23638\n",
      "epoch no.3 train no.237120  loss = 2.25585 avg_loss = 3.27126\n",
      "epoch no.3 train no.237130  loss = 4.62186 avg_loss = 3.29127\n",
      "epoch no.3 train no.237140  loss = 3.83626 avg_loss = 3.29683\n",
      "epoch no.3 train no.237150  loss = 3.06348 avg_loss = 3.23621\n",
      "epoch no.3 train no.237160  loss = 3.12228 avg_loss = 3.22798\n",
      "epoch no.3 train no.237170  loss = 2.84658 avg_loss = 3.22586\n",
      "epoch no.3 train no.237180  loss = 4.21286 avg_loss = 3.26778\n",
      "epoch no.3 train no.237190  loss = 2.50781 avg_loss = 3.22102\n",
      "epoch no.3 train no.237200  loss = 1.78707 avg_loss = 3.18224\n",
      "epoch no.3 train no.237210  loss = 4.87399 avg_loss = 3.23593\n",
      "epoch no.3 train no.237220  loss = 2.03898 avg_loss = 3.20756\n",
      "epoch no.3 train no.237230  loss = 4.30163 avg_loss = 3.23554\n",
      "epoch no.3 train no.237240  loss = 2.66894 avg_loss = 3.20861\n",
      "epoch no.3 train no.237250  loss = 2.77524 avg_loss = 3.21388\n",
      "epoch no.3 train no.237260  loss = 1.71421 avg_loss = 3.22582\n",
      "epoch no.3 train no.237270  loss = 3.89751 avg_loss = 3.18997\n",
      "epoch no.3 train no.237280  loss = 3.92994 avg_loss = 3.19889\n",
      "epoch no.3 train no.237290  loss = 4.51920 avg_loss = 3.20292\n",
      "epoch no.3 train no.237300  loss = 2.29403 avg_loss = 3.19054\n",
      "epoch no.3 train no.237310  loss = 2.23771 avg_loss = 3.18069\n",
      "epoch no.3 train no.237320  loss = 3.53866 avg_loss = 3.16933\n",
      "epoch no.3 train no.237330  loss = 4.33307 avg_loss = 3.16619\n",
      "epoch no.3 train no.237340  loss = 2.17391 avg_loss = 3.15242\n",
      "epoch no.3 train no.237350  loss = 2.84713 avg_loss = 3.15005\n",
      "epoch no.3 train no.237360  loss = 4.20135 avg_loss = 3.18017\n",
      "epoch no.3 train no.237370  loss = 4.03709 avg_loss = 3.21382\n",
      "epoch no.3 train no.237380  loss = 2.21594 avg_loss = 3.18251\n",
      "epoch no.3 train no.237390  loss = 3.83383 avg_loss = 3.18632\n",
      "epoch no.3 train no.237400  loss = 5.35367 avg_loss = 3.18636\n",
      "epoch no.3 train no.237410  loss = 3.07637 avg_loss = 3.20017\n",
      "epoch no.3 train no.237420  loss = 3.40195 avg_loss = 3.17733\n",
      "epoch no.3 train no.237430  loss = 4.34084 avg_loss = 3.16475\n",
      "epoch no.3 train no.237440  loss = 3.25072 avg_loss = 3.21805\n",
      "epoch no.3 train no.237450  loss = 2.70453 avg_loss = 3.17413\n",
      "epoch no.3 train no.237460  loss = 2.58838 avg_loss = 3.18662\n",
      "epoch no.3 train no.237470  loss = 4.87682 avg_loss = 3.22609\n",
      "epoch no.3 train no.237480  loss = 3.91425 avg_loss = 3.24162\n",
      "epoch no.3 train no.237490  loss = 2.92841 avg_loss = 3.24880\n",
      "epoch no.3 train no.237500  loss = 2.52360 avg_loss = 3.25419\n",
      "epoch no.3 train no.237510  loss = 3.84399 avg_loss = 3.30990\n",
      "epoch no.3 train no.237520  loss = 5.08497 avg_loss = 3.36374\n",
      "epoch no.3 train no.237530  loss = 1.42934 avg_loss = 3.29156\n",
      "epoch no.3 train no.237540  loss = 3.34089 avg_loss = 3.32012\n",
      "epoch no.3 train no.237550  loss = 3.22835 avg_loss = 3.33795\n",
      "epoch no.3 train no.237560  loss = 3.25794 avg_loss = 3.36815\n",
      "epoch no.3 train no.237570  loss = 3.96283 avg_loss = 3.38259\n",
      "epoch no.3 train no.237580  loss = 2.99037 avg_loss = 3.35394\n",
      "epoch no.3 train no.237590  loss = 2.45153 avg_loss = 3.37912\n",
      "epoch no.3 train no.237600  loss = 2.54339 avg_loss = 3.38467\n",
      "epoch no.3 train no.237610  loss = 3.73162 avg_loss = 3.38727\n",
      "epoch no.3 train no.237620  loss = 2.30986 avg_loss = 3.34523\n",
      "epoch no.3 train no.237630  loss = 2.52030 avg_loss = 3.31892\n",
      "epoch no.3 train no.237640  loss = 3.38481 avg_loss = 3.28030\n",
      "epoch no.3 train no.237650  loss = 2.65543 avg_loss = 3.34175\n",
      "epoch no.3 train no.237660  loss = 5.20519 avg_loss = 3.37692\n",
      "epoch no.3 train no.237670  loss = 3.32026 avg_loss = 3.34637\n",
      "epoch no.3 train no.237680  loss = 1.65953 avg_loss = 3.32511\n",
      "epoch no.3 train no.237690  loss = 4.86394 avg_loss = 3.33300\n",
      "epoch no.3 train no.237700  loss = 2.61225 avg_loss = 3.31405\n",
      "epoch no.3 train no.237710  loss = 2.37696 avg_loss = 3.32744\n",
      "epoch no.3 train no.237720  loss = 3.18477 avg_loss = 3.29154\n",
      "epoch no.3 train no.237730  loss = 4.34506 avg_loss = 3.30572\n",
      "epoch no.3 train no.237740  loss = 3.68740 avg_loss = 3.34790\n",
      "epoch no.3 train no.237750  loss = 2.75683 avg_loss = 3.33521\n",
      "epoch no.3 train no.237760  loss = 3.32543 avg_loss = 3.36210\n",
      "epoch no.3 train no.237770  loss = 2.16043 avg_loss = 3.30016\n",
      "epoch no.3 train no.237780  loss = 1.08882 avg_loss = 3.27198\n",
      "epoch no.3 train no.237790  loss = 3.22866 avg_loss = 3.25248\n",
      "epoch no.3 train no.237800  loss = 1.62828 avg_loss = 3.19889\n",
      "epoch no.3 train no.237810  loss = 4.23141 avg_loss = 3.22492\n",
      "epoch no.3 train no.237820  loss = 5.58917 avg_loss = 3.25245\n",
      "epoch no.3 train no.237830  loss = 3.39582 avg_loss = 3.23896\n",
      "epoch no.3 train no.237840  loss = 3.67002 avg_loss = 3.23696\n",
      "epoch no.3 train no.237850  loss = 2.65039 avg_loss = 3.21854\n",
      "epoch no.3 train no.237860  loss = 3.12219 avg_loss = 3.22054\n",
      "epoch no.3 train no.237870  loss = 2.71364 avg_loss = 3.22400\n",
      "epoch no.3 train no.237880  loss = 2.64960 avg_loss = 3.23238\n",
      "epoch no.3 train no.237890  loss = 4.80884 avg_loss = 3.23866\n",
      "epoch no.3 train no.237900  loss = 1.83409 avg_loss = 3.20979\n",
      "epoch no.3 train no.237910  loss = 5.08364 avg_loss = 3.23234\n",
      "epoch no.3 train no.237920  loss = 2.15291 avg_loss = 3.18932\n",
      "epoch no.3 train no.237930  loss = 4.23570 avg_loss = 3.22277\n",
      "epoch no.3 train no.237940  loss = 3.07862 avg_loss = 3.26349\n",
      "epoch no.3 train no.237950  loss = 2.72386 avg_loss = 3.25161\n",
      "epoch no.3 train no.237960  loss = 4.23059 avg_loss = 3.27616\n",
      "epoch no.3 train no.237970  loss = 2.95557 avg_loss = 3.22454\n",
      "epoch no.3 train no.237980  loss = 2.21628 avg_loss = 3.16381\n",
      "epoch no.3 train no.237990  loss = 2.11383 avg_loss = 3.16035\n",
      "epoch no.3 train no.238000  loss = 2.92063 avg_loss = 3.15281\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁발라', '곡', '</s>']\n",
      "추억의 90년대 댄스팝</s>\n",
      "epoch no.3 train no.238010  loss = 1.83790 avg_loss = 3.17042\n",
      "epoch no.3 train no.238020  loss = 1.77921 avg_loss = 3.16193\n",
      "epoch no.3 train no.238030  loss = 1.98296 avg_loss = 3.15311\n",
      "epoch no.3 train no.238040  loss = 5.56008 avg_loss = 3.15913\n",
      "epoch no.3 train no.238050  loss = 3.26561 avg_loss = 3.22020\n",
      "epoch no.3 train no.238060  loss = 2.63912 avg_loss = 3.23094\n",
      "epoch no.3 train no.238070  loss = 2.28611 avg_loss = 3.24100\n",
      "epoch no.3 train no.238080  loss = 2.19382 avg_loss = 3.19511\n",
      "epoch no.3 train no.238090  loss = 3.29994 avg_loss = 3.19579\n",
      "epoch no.3 train no.238100  loss = 2.66101 avg_loss = 3.21244\n",
      "epoch no.3 train no.238110  loss = 3.72608 avg_loss = 3.23799\n",
      "epoch no.3 train no.238120  loss = 3.80239 avg_loss = 3.18277\n",
      "epoch no.3 train no.238130  loss = 2.82195 avg_loss = 3.21988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.238140  loss = 1.55200 avg_loss = 3.23070\n",
      "epoch no.3 train no.238150  loss = 2.50590 avg_loss = 3.23779\n",
      "epoch no.3 train no.238160  loss = 2.50458 avg_loss = 3.23988\n",
      "epoch no.3 train no.238170  loss = 3.77420 avg_loss = 3.21340\n",
      "epoch no.3 train no.238180  loss = 2.67312 avg_loss = 3.18634\n",
      "epoch no.3 train no.238190  loss = 3.63204 avg_loss = 3.17145\n",
      "epoch no.3 train no.238200  loss = 2.31975 avg_loss = 3.16167\n",
      "epoch no.3 train no.238210  loss = 4.21361 avg_loss = 3.16485\n",
      "epoch no.3 train no.238220  loss = 2.10535 avg_loss = 3.14431\n",
      "epoch no.3 train no.238230  loss = 2.81467 avg_loss = 3.10450\n",
      "epoch no.3 train no.238240  loss = 3.03961 avg_loss = 3.09673\n",
      "epoch no.3 train no.238250  loss = 3.30445 avg_loss = 3.11912\n",
      "epoch no.3 train no.238260  loss = 2.81568 avg_loss = 3.11097\n",
      "epoch no.3 train no.238270  loss = 3.92859 avg_loss = 3.12955\n",
      "epoch no.3 train no.238280  loss = 3.30102 avg_loss = 3.19157\n",
      "epoch no.3 train no.238290  loss = 4.42802 avg_loss = 3.25188\n",
      "epoch no.3 train no.238300  loss = 2.84084 avg_loss = 3.20834\n",
      "epoch no.3 train no.238310  loss = 2.89793 avg_loss = 3.25567\n",
      "epoch no.3 train no.238320  loss = 5.23449 avg_loss = 3.24906\n",
      "epoch no.3 train no.238330  loss = 2.54713 avg_loss = 3.30803\n",
      "epoch no.3 train no.238340  loss = 4.64660 avg_loss = 3.30826\n",
      "epoch no.3 train no.238350  loss = 1.58700 avg_loss = 3.24919\n",
      "epoch no.3 train no.238360  loss = 3.56975 avg_loss = 3.23345\n",
      "epoch no.3 train no.238370  loss = 2.94178 avg_loss = 3.22575\n",
      "epoch no.3 train no.238380  loss = 3.99807 avg_loss = 3.22761\n",
      "epoch no.3 train no.238390  loss = 3.27035 avg_loss = 3.25004\n",
      "epoch no.3 train no.238400  loss = 4.22522 avg_loss = 3.29590\n",
      "epoch no.3 train no.238410  loss = 5.55060 avg_loss = 3.27225\n",
      "epoch no.3 train no.238420  loss = 5.75974 avg_loss = 3.30119\n",
      "epoch no.3 train no.238430  loss = 2.71205 avg_loss = 3.25754\n",
      "epoch no.3 train no.238440  loss = 3.37007 avg_loss = 3.24811\n",
      "epoch no.3 train no.238450  loss = 1.89130 avg_loss = 3.27484\n",
      "epoch no.3 train no.238460  loss = 2.08062 avg_loss = 3.28180\n",
      "epoch no.3 train no.238470  loss = 2.44421 avg_loss = 3.29771\n",
      "epoch no.3 train no.238480  loss = 3.39704 avg_loss = 3.31998\n",
      "epoch no.3 train no.238490  loss = 3.42215 avg_loss = 3.27793\n",
      "epoch no.3 train no.238500  loss = 3.04160 avg_loss = 3.25867\n",
      "epoch no.3 train no.238510  loss = 2.14102 avg_loss = 3.26049\n",
      "epoch no.3 train no.238520  loss = 5.71602 avg_loss = 3.28398\n",
      "epoch no.3 train no.238530  loss = 4.05233 avg_loss = 3.32695\n",
      "epoch no.3 train no.238540  loss = 2.96688 avg_loss = 3.32056\n",
      "epoch no.3 train no.238550  loss = 5.10604 avg_loss = 3.31984\n",
      "epoch no.3 train no.238560  loss = 3.15780 avg_loss = 3.32074\n",
      "epoch no.3 train no.238570  loss = 2.74431 avg_loss = 3.29667\n",
      "epoch no.3 train no.238580  loss = 2.79952 avg_loss = 3.30894\n",
      "epoch no.3 train no.238590  loss = 4.06461 avg_loss = 3.33215\n",
      "epoch no.3 train no.238600  loss = 4.57480 avg_loss = 3.35143\n",
      "epoch no.3 train no.238610  loss = 4.93215 avg_loss = 3.34869\n",
      "epoch no.3 train no.238620  loss = 2.15427 avg_loss = 3.32834\n",
      "epoch no.3 train no.238630  loss = 4.56909 avg_loss = 3.33809\n",
      "epoch no.3 train no.238640  loss = 3.38150 avg_loss = 3.40972\n",
      "epoch no.3 train no.238650  loss = 2.62365 avg_loss = 3.38741\n",
      "epoch no.3 train no.238660  loss = 3.90468 avg_loss = 3.36906\n",
      "epoch no.3 train no.238670  loss = 3.47885 avg_loss = 3.37780\n",
      "epoch no.3 train no.238680  loss = 2.40623 avg_loss = 3.36316\n",
      "epoch no.3 train no.238690  loss = 2.36583 avg_loss = 3.31917\n",
      "epoch no.3 train no.238700  loss = 3.34239 avg_loss = 3.26965\n",
      "epoch no.3 train no.238710  loss = 3.14119 avg_loss = 3.26306\n",
      "epoch no.3 train no.238720  loss = 3.64867 avg_loss = 3.27483\n",
      "epoch no.3 train no.238730  loss = 2.59557 avg_loss = 3.23467\n",
      "epoch no.3 train no.238740  loss = 3.94100 avg_loss = 3.19122\n",
      "epoch no.3 train no.238750  loss = 3.25588 avg_loss = 3.24043\n",
      "epoch no.3 train no.238760  loss = 4.62127 avg_loss = 3.29044\n",
      "epoch no.3 train no.238770  loss = 6.96183 avg_loss = 3.32267\n",
      "epoch no.3 train no.238780  loss = 2.82298 avg_loss = 3.30142\n",
      "epoch no.3 train no.238790  loss = 1.97153 avg_loss = 3.31534\n",
      "epoch no.3 train no.238800  loss = 4.49946 avg_loss = 3.31468\n",
      "epoch no.3 train no.238810  loss = 4.23676 avg_loss = 3.36626\n",
      "epoch no.3 train no.238820  loss = 3.20477 avg_loss = 3.33691\n",
      "epoch no.3 train no.238830  loss = 4.20770 avg_loss = 3.33231\n",
      "epoch no.3 train no.238840  loss = 2.73890 avg_loss = 3.30866\n",
      "epoch no.3 train no.238850  loss = 3.15317 avg_loss = 3.26670\n",
      "epoch no.3 train no.238860  loss = 3.30693 avg_loss = 3.22714\n",
      "epoch no.3 train no.238870  loss = 4.86426 avg_loss = 3.25064\n",
      "epoch no.3 train no.238880  loss = 3.81337 avg_loss = 3.24766\n",
      "epoch no.3 train no.238890  loss = 2.53127 avg_loss = 3.21070\n",
      "epoch no.3 train no.238900  loss = 3.63508 avg_loss = 3.22454\n",
      "epoch no.3 train no.238910  loss = 2.52880 avg_loss = 3.20158\n",
      "epoch no.3 train no.238920  loss = 5.33135 avg_loss = 3.20414\n",
      "epoch no.3 train no.238930  loss = 5.04655 avg_loss = 3.21550\n",
      "epoch no.3 train no.238940  loss = 3.34985 avg_loss = 3.18576\n",
      "epoch no.3 train no.238950  loss = 3.32123 avg_loss = 3.17146\n",
      "epoch no.3 train no.238960  loss = 2.51250 avg_loss = 3.12453\n",
      "epoch no.3 train no.238970  loss = 2.07932 avg_loss = 3.12933\n",
      "epoch no.3 train no.238980  loss = 2.36827 avg_loss = 3.13887\n",
      "epoch no.3 train no.238990  loss = 3.15018 avg_loss = 3.16003\n",
      "epoch no.3 train no.239000  loss = 3.62205 avg_loss = 3.22131\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.3 train no.239010  loss = 4.88605 avg_loss = 3.28694\n",
      "epoch no.3 train no.239020  loss = 5.41863 avg_loss = 3.35502\n",
      "epoch no.3 train no.239030  loss = 3.78438 avg_loss = 3.32598\n",
      "epoch no.3 train no.239040  loss = 2.75018 avg_loss = 3.33163\n",
      "epoch no.3 train no.239050  loss = 4.44432 avg_loss = 3.33598\n",
      "epoch no.3 train no.239060  loss = 2.53941 avg_loss = 3.31994\n",
      "epoch no.3 train no.239070  loss = 3.03673 avg_loss = 3.35304\n",
      "epoch no.3 train no.239080  loss = 2.36849 avg_loss = 3.31853\n",
      "epoch no.3 train no.239090  loss = 4.27362 avg_loss = 3.34409\n",
      "epoch no.3 train no.239100  loss = 3.68635 avg_loss = 3.30632\n",
      "epoch no.3 train no.239110  loss = 2.94975 avg_loss = 3.29683\n",
      "epoch no.3 train no.239120  loss = 2.44961 avg_loss = 3.25826\n",
      "epoch no.3 train no.239130  loss = 3.48540 avg_loss = 3.26707\n",
      "epoch no.3 train no.239140  loss = 3.73628 avg_loss = 3.28253\n",
      "epoch no.3 train no.239150  loss = 3.55377 avg_loss = 3.32630\n",
      "epoch no.3 train no.239160  loss = 2.88181 avg_loss = 3.32730\n",
      "epoch no.3 train no.239170  loss = 2.67325 avg_loss = 3.26639\n",
      "epoch no.3 train no.239180  loss = 3.88008 avg_loss = 3.23813\n",
      "epoch no.3 train no.239190  loss = 2.57888 avg_loss = 3.23358\n",
      "epoch no.3 train no.239200  loss = 4.45360 avg_loss = 3.21509\n",
      "epoch no.3 train no.239210  loss = 3.58953 avg_loss = 3.23197\n",
      "epoch no.3 train no.239220  loss = 4.79864 avg_loss = 3.25702\n",
      "epoch no.3 train no.239230  loss = 2.76634 avg_loss = 3.26562\n",
      "epoch no.3 train no.239240  loss = 2.69918 avg_loss = 3.22045\n",
      "epoch no.3 train no.239250  loss = 1.91182 avg_loss = 3.16686\n",
      "epoch no.3 train no.239260  loss = 4.43442 avg_loss = 3.20473\n",
      "epoch no.3 train no.239270  loss = 1.94450 avg_loss = 3.22424\n",
      "epoch no.3 train no.239280  loss = 1.31561 avg_loss = 3.24754\n",
      "epoch no.3 train no.239290  loss = 1.94993 avg_loss = 3.21813\n",
      "epoch no.3 train no.239300  loss = 3.78502 avg_loss = 3.27544\n",
      "epoch no.3 train no.239310  loss = 2.12219 avg_loss = 3.22854\n",
      "epoch no.3 train no.239320  loss = 3.53930 avg_loss = 3.26906\n",
      "epoch no.3 train no.239330  loss = 3.83468 avg_loss = 3.27875\n",
      "epoch no.3 train no.239340  loss = 3.09222 avg_loss = 3.27196\n",
      "epoch no.3 train no.239350  loss = 3.16710 avg_loss = 3.30432\n",
      "epoch no.3 train no.239360  loss = 2.73025 avg_loss = 3.28515\n",
      "epoch no.3 train no.239370  loss = 2.84106 avg_loss = 3.25506\n",
      "epoch no.3 train no.239380  loss = 3.04750 avg_loss = 3.25980\n",
      "epoch no.3 train no.239390  loss = 3.56536 avg_loss = 3.23112\n",
      "epoch no.3 train no.239400  loss = 2.69318 avg_loss = 3.22201\n",
      "epoch no.3 train no.239410  loss = 3.12073 avg_loss = 3.22421\n",
      "epoch no.3 train no.239420  loss = 2.50752 avg_loss = 3.25609\n",
      "epoch no.3 train no.239430  loss = 3.74304 avg_loss = 3.23569\n",
      "epoch no.3 train no.239440  loss = 2.03325 avg_loss = 3.23745\n",
      "epoch no.3 train no.239450  loss = 4.61025 avg_loss = 3.26927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.239460  loss = 3.25295 avg_loss = 3.25590\n",
      "epoch no.3 train no.239470  loss = 3.89157 avg_loss = 3.25510\n",
      "epoch no.3 train no.239480  loss = 3.90244 avg_loss = 3.26027\n",
      "epoch no.3 train no.239490  loss = 3.65707 avg_loss = 3.27239\n",
      "epoch no.3 train no.239500  loss = 3.10758 avg_loss = 3.27472\n",
      "epoch no.3 train no.239510  loss = 3.38312 avg_loss = 3.29245\n",
      "epoch no.3 train no.239520  loss = 2.14137 avg_loss = 3.31746\n",
      "epoch no.3 train no.239530  loss = 3.25493 avg_loss = 3.30966\n",
      "epoch no.3 train no.239540  loss = 2.35259 avg_loss = 3.32619\n",
      "epoch no.3 train no.239550  loss = 3.13955 avg_loss = 3.32250\n",
      "epoch no.3 train no.239560  loss = 3.89356 avg_loss = 3.36253\n",
      "epoch no.3 train no.239570  loss = 4.05868 avg_loss = 3.38001\n",
      "epoch no.3 train no.239580  loss = 6.21625 avg_loss = 3.37280\n",
      "epoch no.3 train no.239590  loss = 3.64134 avg_loss = 3.38885\n",
      "epoch no.3 train no.239600  loss = 2.85833 avg_loss = 3.37810\n",
      "epoch no.3 train no.239610  loss = 3.20015 avg_loss = 3.37396\n",
      "epoch no.3 train no.239620  loss = 3.75448 avg_loss = 3.35291\n",
      "epoch no.3 train no.239630  loss = 2.59263 avg_loss = 3.30303\n",
      "epoch no.3 train no.239640  loss = 2.18531 avg_loss = 3.31274\n",
      "epoch no.3 train no.239650  loss = 4.60863 avg_loss = 3.33492\n",
      "epoch no.3 train no.239660  loss = 2.60240 avg_loss = 3.33347\n",
      "epoch no.3 train no.239670  loss = 4.39203 avg_loss = 3.36861\n",
      "epoch no.3 train no.239680  loss = 3.12669 avg_loss = 3.31804\n",
      "epoch no.3 train no.239690  loss = 3.14377 avg_loss = 3.31883\n",
      "epoch no.3 train no.239700  loss = 1.47324 avg_loss = 3.35802\n",
      "epoch no.3 train no.239710  loss = 4.99873 avg_loss = 3.33467\n",
      "epoch no.3 train no.239720  loss = 3.44638 avg_loss = 3.37428\n",
      "epoch no.3 train no.239730  loss = 2.13969 avg_loss = 3.32989\n",
      "epoch no.3 train no.239740  loss = 2.46738 avg_loss = 3.32620\n",
      "epoch no.3 train no.239750  loss = 4.26290 avg_loss = 3.35778\n",
      "epoch no.3 train no.239760  loss = 2.57358 avg_loss = 3.33682\n",
      "epoch no.3 train no.239770  loss = 2.48004 avg_loss = 3.33127\n",
      "epoch no.3 train no.239780  loss = 3.83608 avg_loss = 3.34848\n",
      "epoch no.3 train no.239790  loss = 3.50130 avg_loss = 3.34015\n",
      "epoch no.3 train no.239800  loss = 3.34988 avg_loss = 3.31611\n",
      "epoch no.3 train no.239810  loss = 3.19306 avg_loss = 3.30446\n",
      "epoch no.3 train no.239820  loss = 2.42264 avg_loss = 3.26299\n",
      "epoch no.3 train no.239830  loss = 4.61256 avg_loss = 3.27751\n",
      "epoch no.3 train no.239840  loss = 3.87131 avg_loss = 3.29255\n",
      "epoch no.3 train no.239850  loss = 3.50885 avg_loss = 3.29221\n",
      "epoch no.3 train no.239860  loss = 2.33664 avg_loss = 3.28546\n",
      "epoch no.3 train no.239870  loss = 5.48072 avg_loss = 3.28471\n",
      "epoch no.3 train no.239880  loss = 3.66113 avg_loss = 3.28082\n",
      "epoch no.3 train no.239890  loss = 4.08769 avg_loss = 3.26845\n",
      "epoch no.3 train no.239900  loss = 3.06337 avg_loss = 3.26797\n",
      "epoch no.3 train no.239910  loss = 2.69705 avg_loss = 3.24316\n",
      "epoch no.3 train no.239920  loss = 1.73683 avg_loss = 3.26871\n",
      "epoch no.3 train no.239930  loss = 5.34417 avg_loss = 3.25583\n",
      "epoch no.3 train no.239940  loss = 2.68808 avg_loss = 3.27563\n",
      "epoch no.3 train no.239950  loss = 3.71080 avg_loss = 3.28592\n",
      "epoch no.3 train no.239960  loss = 2.13065 avg_loss = 3.27346\n",
      "epoch no.3 train no.239970  loss = 1.50135 avg_loss = 3.26442\n",
      "epoch no.3 train no.239980  loss = 2.60533 avg_loss = 3.20598\n",
      "epoch no.3 train no.239990  loss = 2.03535 avg_loss = 3.17800\n",
      "epoch no.3 train no.240000  loss = 5.67562 avg_loss = 3.21794\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.3 train no.240010  loss = 3.88484 avg_loss = 3.22303\n",
      "epoch no.3 train no.240020  loss = 3.88973 avg_loss = 3.22442\n",
      "epoch no.3 train no.240030  loss = 2.66092 avg_loss = 3.27863\n",
      "epoch no.3 train no.240040  loss = 3.61119 avg_loss = 3.28162\n",
      "epoch no.3 train no.240050  loss = 3.29102 avg_loss = 3.25094\n",
      "epoch no.3 train no.240060  loss = 2.61541 avg_loss = 3.26981\n",
      "epoch no.3 train no.240070  loss = 5.30560 avg_loss = 3.27535\n",
      "epoch no.3 train no.240080  loss = 3.28776 avg_loss = 3.28851\n",
      "epoch no.3 train no.240090  loss = 3.09039 avg_loss = 3.27652\n",
      "epoch no.3 train no.240100  loss = 2.87684 avg_loss = 3.25606\n",
      "epoch no.3 train no.240110  loss = 3.37464 avg_loss = 3.34363\n",
      "epoch no.3 train no.240120  loss = 2.52733 avg_loss = 3.34320\n",
      "epoch no.3 train no.240130  loss = 2.41349 avg_loss = 3.32482\n",
      "epoch no.3 train no.240140  loss = 5.62855 avg_loss = 3.38777\n",
      "epoch no.3 train no.240150  loss = 2.88152 avg_loss = 3.35816\n",
      "epoch no.3 train no.240160  loss = 2.68960 avg_loss = 3.31650\n",
      "epoch no.3 train no.240170  loss = 1.52357 avg_loss = 3.31489\n",
      "epoch no.3 train no.240180  loss = 5.06076 avg_loss = 3.31806\n",
      "epoch no.3 train no.240190  loss = 3.14681 avg_loss = 3.31202\n",
      "epoch no.3 train no.240200  loss = 2.44161 avg_loss = 3.26314\n",
      "epoch no.3 train no.240210  loss = 3.89420 avg_loss = 3.24679\n",
      "epoch no.3 train no.240220  loss = 2.73918 avg_loss = 3.23308\n",
      "epoch no.3 train no.240230  loss = 3.57798 avg_loss = 3.25697\n",
      "epoch no.3 train no.240240  loss = 5.78086 avg_loss = 3.30233\n",
      "epoch no.3 train no.240250  loss = 3.42583 avg_loss = 3.24214\n",
      "epoch no.3 train no.240260  loss = 3.36354 avg_loss = 3.24890\n",
      "epoch no.3 train no.240270  loss = 4.39722 avg_loss = 3.32409\n",
      "epoch no.3 train no.240280  loss = 2.38195 avg_loss = 3.26403\n",
      "epoch no.3 train no.240290  loss = 4.14644 avg_loss = 3.28351\n",
      "epoch no.3 train no.240300  loss = 2.68604 avg_loss = 3.29454\n",
      "epoch no.3 train no.240310  loss = 2.89287 avg_loss = 3.31131\n",
      "epoch no.3 train no.240320  loss = 4.02640 avg_loss = 3.27571\n",
      "epoch no.3 train no.240330  loss = 4.13707 avg_loss = 3.31685\n",
      "epoch no.3 train no.240340  loss = 3.74620 avg_loss = 3.29580\n",
      "epoch no.3 train no.240350  loss = 3.73435 avg_loss = 3.31397\n",
      "epoch no.3 train no.240360  loss = 3.74189 avg_loss = 3.33296\n",
      "epoch no.3 train no.240370  loss = 2.92901 avg_loss = 3.39998\n",
      "epoch no.3 train no.240380  loss = 2.46204 avg_loss = 3.35719\n",
      "epoch no.3 train no.240390  loss = 2.66560 avg_loss = 3.32404\n",
      "epoch no.3 train no.240400  loss = 2.67732 avg_loss = 3.33664\n",
      "epoch no.3 train no.240410  loss = 5.01468 avg_loss = 3.34661\n",
      "epoch no.3 train no.240420  loss = 3.44608 avg_loss = 3.32015\n",
      "epoch no.3 train no.240430  loss = 3.87510 avg_loss = 3.28691\n",
      "epoch no.3 train no.240440  loss = 3.74874 avg_loss = 3.30981\n",
      "epoch no.3 train no.240450  loss = 4.43748 avg_loss = 3.38295\n",
      "epoch no.3 train no.240460  loss = 2.22676 avg_loss = 3.34475\n",
      "epoch no.3 train no.240470  loss = 2.23658 avg_loss = 3.38801\n",
      "epoch no.3 train no.240480  loss = 3.45286 avg_loss = 3.38454\n",
      "epoch no.3 train no.240490  loss = 2.82236 avg_loss = 3.38520\n",
      "epoch no.3 train no.240500  loss = 2.74038 avg_loss = 3.40400\n",
      "epoch no.3 train no.240510  loss = 4.03267 avg_loss = 3.41131\n",
      "epoch no.3 train no.240520  loss = 2.69083 avg_loss = 3.38659\n",
      "epoch no.3 train no.240530  loss = 2.92797 avg_loss = 3.38648\n",
      "epoch no.3 train no.240540  loss = 4.04420 avg_loss = 3.39160\n",
      "epoch no.3 train no.240550  loss = 2.21626 avg_loss = 3.33174\n",
      "epoch no.3 train no.240560  loss = 4.52659 avg_loss = 3.32073\n",
      "epoch no.3 train no.240570  loss = 4.53412 avg_loss = 3.34620\n",
      "epoch no.3 train no.240580  loss = 4.29296 avg_loss = 3.34972\n",
      "epoch no.3 train no.240590  loss = 2.31248 avg_loss = 3.32898\n",
      "epoch no.3 train no.240600  loss = 5.43544 avg_loss = 3.33392\n",
      "epoch no.3 train no.240610  loss = 3.65045 avg_loss = 3.29092\n",
      "epoch no.3 train no.240620  loss = 2.46921 avg_loss = 3.30817\n",
      "epoch no.3 train no.240630  loss = 4.80750 avg_loss = 3.33226\n",
      "epoch no.3 train no.240640  loss = 4.70982 avg_loss = 3.38826\n",
      "epoch no.3 train no.240650  loss = 3.04562 avg_loss = 3.38567\n",
      "epoch no.3 train no.240660  loss = 3.35109 avg_loss = 3.34880\n",
      "epoch no.3 train no.240670  loss = 2.89648 avg_loss = 3.35039\n",
      "epoch no.3 train no.240680  loss = 3.21970 avg_loss = 3.31116\n",
      "epoch no.3 train no.240690  loss = 3.12967 avg_loss = 3.31434\n",
      "epoch no.3 train no.240700  loss = 3.82590 avg_loss = 3.30805\n",
      "epoch no.3 train no.240710  loss = 3.29690 avg_loss = 3.28018\n",
      "epoch no.3 train no.240720  loss = 4.79083 avg_loss = 3.31176\n",
      "epoch no.3 train no.240730  loss = 2.95238 avg_loss = 3.27729\n",
      "epoch no.3 train no.240740  loss = 3.28387 avg_loss = 3.26070\n",
      "epoch no.3 train no.240750  loss = 2.61759 avg_loss = 3.24392\n",
      "epoch no.3 train no.240760  loss = 4.32091 avg_loss = 3.28366\n",
      "epoch no.3 train no.240770  loss = 3.41458 avg_loss = 3.30565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.240780  loss = 3.16516 avg_loss = 3.27869\n",
      "epoch no.3 train no.240790  loss = 2.54485 avg_loss = 3.31742\n",
      "epoch no.3 train no.240800  loss = 3.61943 avg_loss = 3.28799\n",
      "epoch no.3 train no.240810  loss = 3.71609 avg_loss = 3.27765\n",
      "epoch no.3 train no.240820  loss = 3.39519 avg_loss = 3.28990\n",
      "epoch no.3 train no.240830  loss = 3.60826 avg_loss = 3.28844\n",
      "epoch no.3 train no.240840  loss = 3.10050 avg_loss = 3.31571\n",
      "epoch no.3 train no.240850  loss = 2.19395 avg_loss = 3.30233\n",
      "epoch no.3 train no.240860  loss = 3.09645 avg_loss = 3.31498\n",
      "epoch no.3 train no.240870  loss = 2.40046 avg_loss = 3.29118\n",
      "epoch no.3 train no.240880  loss = 4.28641 avg_loss = 3.32046\n",
      "epoch no.3 train no.240890  loss = 4.19997 avg_loss = 3.35084\n",
      "epoch no.3 train no.240900  loss = 2.40567 avg_loss = 3.31751\n",
      "epoch no.3 train no.240910  loss = 3.12213 avg_loss = 3.32196\n",
      "epoch no.3 train no.240920  loss = 2.03992 avg_loss = 3.28334\n",
      "epoch no.3 train no.240930  loss = 3.20861 avg_loss = 3.29136\n",
      "epoch no.3 train no.240940  loss = 2.87528 avg_loss = 3.26771\n",
      "epoch no.3 train no.240950  loss = 2.07574 avg_loss = 3.28028\n",
      "epoch no.3 train no.240960  loss = 4.17900 avg_loss = 3.27581\n",
      "epoch no.3 train no.240970  loss = 2.80310 avg_loss = 3.26390\n",
      "epoch no.3 train no.240980  loss = 1.86261 avg_loss = 3.24010\n",
      "epoch no.3 train no.240990  loss = 3.18591 avg_loss = 3.20743\n",
      "epoch no.3 train no.241000  loss = 3.62442 avg_loss = 3.19109\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', 'est', '</s>']\n",
      "추억의 발라드 best</s>\n",
      "epoch no.3 train no.241010  loss = 2.55228 avg_loss = 3.16914\n",
      "epoch no.3 train no.241020  loss = 5.01137 avg_loss = 3.19156\n",
      "epoch no.3 train no.241030  loss = 3.34511 avg_loss = 3.21748\n",
      "epoch no.3 train no.241040  loss = 2.87202 avg_loss = 3.21063\n",
      "epoch no.3 train no.241050  loss = 3.46757 avg_loss = 3.19616\n",
      "epoch no.3 train no.241060  loss = 3.92297 avg_loss = 3.23886\n",
      "epoch no.3 train no.241070  loss = 3.29586 avg_loss = 3.20207\n",
      "epoch no.3 train no.241080  loss = 2.83013 avg_loss = 3.18088\n",
      "epoch no.3 train no.241090  loss = 3.67090 avg_loss = 3.16023\n",
      "epoch no.3 train no.241100  loss = 3.20427 avg_loss = 3.19846\n",
      "epoch no.3 train no.241110  loss = 5.35571 avg_loss = 3.21808\n",
      "epoch no.3 train no.241120  loss = 2.00276 avg_loss = 3.23480\n",
      "epoch no.3 train no.241130  loss = 2.98193 avg_loss = 3.26375\n",
      "epoch no.3 train no.241140  loss = 3.51696 avg_loss = 3.23892\n",
      "epoch no.3 train no.241150  loss = 3.90346 avg_loss = 3.27029\n",
      "epoch no.3 train no.241160  loss = 3.40491 avg_loss = 3.27712\n",
      "epoch no.3 train no.241170  loss = 2.91465 avg_loss = 3.28195\n",
      "epoch no.3 train no.241180  loss = 1.68884 avg_loss = 3.26978\n",
      "epoch no.3 train no.241190  loss = 2.84539 avg_loss = 3.31044\n",
      "epoch no.3 train no.241200  loss = 4.20927 avg_loss = 3.33024\n",
      "epoch no.3 train no.241210  loss = 3.73306 avg_loss = 3.34790\n",
      "epoch no.3 train no.241220  loss = 3.56082 avg_loss = 3.34153\n",
      "epoch no.3 train no.241230  loss = 3.27845 avg_loss = 3.35920\n",
      "epoch no.3 train no.241240  loss = 3.61201 avg_loss = 3.39502\n",
      "epoch no.3 train no.241250  loss = 2.90056 avg_loss = 3.40269\n",
      "epoch no.3 train no.241260  loss = 2.41402 avg_loss = 3.37764\n",
      "epoch no.3 train no.241270  loss = 2.96332 avg_loss = 3.36121\n",
      "epoch no.3 train no.241280  loss = 4.04058 avg_loss = 3.34911\n",
      "epoch no.3 train no.241290  loss = 2.10793 avg_loss = 3.29912\n",
      "epoch no.3 train no.241300  loss = 3.10895 avg_loss = 3.28733\n",
      "epoch no.3 train no.241310  loss = 5.13799 avg_loss = 3.26550\n",
      "epoch no.3 train no.241320  loss = 5.15553 avg_loss = 3.32106\n",
      "epoch no.3 train no.241330  loss = 3.84995 avg_loss = 3.40454\n",
      "epoch no.3 train no.241340  loss = 3.04253 avg_loss = 3.38521\n",
      "epoch no.3 train no.241350  loss = 2.20562 avg_loss = 3.37402\n",
      "epoch no.3 train no.241360  loss = 2.90159 avg_loss = 3.35184\n",
      "epoch no.3 train no.241370  loss = 2.77900 avg_loss = 3.29733\n",
      "epoch no.3 train no.241380  loss = 3.04039 avg_loss = 3.24060\n",
      "epoch no.3 train no.241390  loss = 4.33967 avg_loss = 3.24496\n",
      "epoch no.3 train no.241400  loss = 3.91480 avg_loss = 3.27352\n",
      "epoch no.3 train no.241410  loss = 2.93223 avg_loss = 3.20401\n",
      "epoch no.3 train no.241420  loss = 3.92069 avg_loss = 3.20719\n",
      "epoch no.3 train no.241430  loss = 2.39341 avg_loss = 3.22391\n",
      "epoch no.3 train no.241440  loss = 5.93547 avg_loss = 3.27492\n",
      "epoch no.3 train no.241450  loss = 3.63803 avg_loss = 3.26607\n",
      "epoch no.3 train no.241460  loss = 5.74745 avg_loss = 3.24475\n",
      "epoch no.3 train no.241470  loss = 4.18571 avg_loss = 3.26900\n",
      "epoch no.3 train no.241480  loss = 4.29844 avg_loss = 3.25330\n",
      "epoch no.3 train no.241490  loss = 2.98306 avg_loss = 3.24030\n",
      "epoch no.3 train no.241500  loss = 3.25323 avg_loss = 3.27809\n",
      "epoch no.3 train no.241510  loss = 5.19225 avg_loss = 3.35553\n",
      "epoch no.3 train no.241520  loss = 2.99938 avg_loss = 3.33844\n",
      "epoch no.3 train no.241530  loss = 2.11959 avg_loss = 3.34045\n",
      "epoch no.3 train no.241540  loss = 3.62194 avg_loss = 3.34162\n",
      "epoch no.3 train no.241550  loss = 4.28178 avg_loss = 3.33154\n",
      "epoch no.3 train no.241560  loss = 3.94503 avg_loss = 3.28118\n",
      "epoch no.3 train no.241570  loss = 2.99599 avg_loss = 3.27975\n",
      "epoch no.3 train no.241580  loss = 4.31401 avg_loss = 3.27908\n",
      "epoch no.3 train no.241590  loss = 4.03790 avg_loss = 3.32160\n",
      "epoch no.3 train no.241600  loss = 3.98809 avg_loss = 3.33447\n",
      "epoch no.3 train no.241610  loss = 3.79606 avg_loss = 3.36394\n",
      "epoch no.3 train no.241620  loss = 2.27704 avg_loss = 3.30113\n",
      "epoch no.3 train no.241630  loss = 3.42843 avg_loss = 3.35748\n",
      "epoch no.3 train no.241640  loss = 3.17105 avg_loss = 3.31188\n",
      "epoch no.3 train no.241650  loss = 2.94353 avg_loss = 3.25941\n",
      "epoch no.3 train no.241660  loss = 3.63212 avg_loss = 3.25846\n",
      "epoch no.3 train no.241670  loss = 2.81491 avg_loss = 3.28404\n",
      "epoch no.3 train no.241680  loss = 2.92577 avg_loss = 3.26908\n",
      "epoch no.3 train no.241690  loss = 3.03648 avg_loss = 3.28363\n",
      "epoch no.3 train no.241700  loss = 3.02796 avg_loss = 3.33489\n",
      "epoch no.3 train no.241710  loss = 4.16291 avg_loss = 3.32363\n",
      "epoch no.3 train no.241720  loss = 4.95026 avg_loss = 3.34754\n",
      "epoch no.3 train no.241730  loss = 3.25847 avg_loss = 3.32911\n",
      "epoch no.3 train no.241740  loss = 2.98355 avg_loss = 3.36507\n",
      "epoch no.3 train no.241750  loss = 1.50195 avg_loss = 3.39604\n",
      "epoch no.3 train no.241760  loss = 2.59716 avg_loss = 3.34339\n",
      "epoch no.3 train no.241770  loss = 3.75376 avg_loss = 3.33177\n",
      "epoch no.3 train no.241780  loss = 2.80772 avg_loss = 3.30612\n",
      "epoch no.3 train no.241790  loss = 5.10343 avg_loss = 3.31873\n",
      "epoch no.3 train no.241800  loss = 1.45135 avg_loss = 3.26295\n",
      "epoch no.3 train no.241810  loss = 3.65742 avg_loss = 3.27700\n",
      "epoch no.3 train no.241820  loss = 1.96760 avg_loss = 3.31535\n",
      "epoch no.3 train no.241830  loss = 2.27965 avg_loss = 3.26599\n",
      "epoch no.3 train no.241840  loss = 2.67458 avg_loss = 3.24323\n",
      "epoch no.3 train no.241850  loss = 2.04098 avg_loss = 3.24278\n",
      "epoch no.3 train no.241860  loss = 3.14928 avg_loss = 3.22842\n",
      "epoch no.3 train no.241870  loss = 2.38504 avg_loss = 3.19801\n",
      "epoch no.3 train no.241880  loss = 3.44931 avg_loss = 3.19564\n",
      "epoch no.3 train no.241890  loss = 3.18904 avg_loss = 3.21492\n",
      "epoch no.3 train no.241900  loss = 3.71069 avg_loss = 3.23440\n",
      "epoch no.3 train no.241910  loss = 3.03437 avg_loss = 3.22541\n",
      "epoch no.3 train no.241920  loss = 2.77793 avg_loss = 3.25911\n",
      "epoch no.3 train no.241930  loss = 3.30710 avg_loss = 3.25680\n",
      "epoch no.3 train no.241940  loss = 6.42436 avg_loss = 3.28806\n",
      "epoch no.3 train no.241950  loss = 2.34597 avg_loss = 3.25508\n",
      "epoch no.3 train no.241960  loss = 3.57495 avg_loss = 3.22603\n",
      "epoch no.3 train no.241970  loss = 3.51612 avg_loss = 3.20482\n",
      "epoch no.3 train no.241980  loss = 2.73106 avg_loss = 3.24862\n",
      "epoch no.3 train no.241990  loss = 2.50032 avg_loss = 3.25140\n",
      "epoch no.3 train no.242000  loss = 2.42486 avg_loss = 3.28553\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.3 train no.242010  loss = 2.22237 avg_loss = 3.27628\n",
      "epoch no.3 train no.242020  loss = 2.96447 avg_loss = 3.25287\n",
      "epoch no.3 train no.242030  loss = 2.44219 avg_loss = 3.25036\n",
      "epoch no.3 train no.242040  loss = 2.99557 avg_loss = 3.27383\n",
      "epoch no.3 train no.242050  loss = 2.36833 avg_loss = 3.29559\n",
      "epoch no.3 train no.242060  loss = 3.17347 avg_loss = 3.34820\n",
      "epoch no.3 train no.242070  loss = 4.05753 avg_loss = 3.33990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.242080  loss = 3.26234 avg_loss = 3.34860\n",
      "epoch no.3 train no.242090  loss = 3.63764 avg_loss = 3.38124\n",
      "epoch no.3 train no.242100  loss = 2.36096 avg_loss = 3.34455\n",
      "epoch no.3 train no.242110  loss = 2.85751 avg_loss = 3.33614\n",
      "epoch no.3 train no.242120  loss = 3.79508 avg_loss = 3.32247\n",
      "epoch no.3 train no.242130  loss = 3.15738 avg_loss = 3.28919\n",
      "epoch no.3 train no.242140  loss = 5.82835 avg_loss = 3.33450\n",
      "epoch no.3 train no.242150  loss = 4.19444 avg_loss = 3.32760\n",
      "epoch no.3 train no.242160  loss = 5.46425 avg_loss = 3.29251\n",
      "epoch no.3 train no.242170  loss = 2.67982 avg_loss = 3.28410\n",
      "epoch no.3 train no.242180  loss = 3.95522 avg_loss = 3.32027\n",
      "epoch no.3 train no.242190  loss = 3.39382 avg_loss = 3.33802\n",
      "epoch no.3 train no.242200  loss = 2.11214 avg_loss = 3.36359\n",
      "epoch no.3 train no.242210  loss = 2.62296 avg_loss = 3.38080\n",
      "epoch no.3 train no.242220  loss = 2.69503 avg_loss = 3.37355\n",
      "epoch no.3 train no.242230  loss = 1.62996 avg_loss = 3.36467\n",
      "epoch no.3 train no.242240  loss = 3.47308 avg_loss = 3.35303\n",
      "epoch no.3 train no.242250  loss = 3.16839 avg_loss = 3.32622\n",
      "epoch no.3 train no.242260  loss = 1.96277 avg_loss = 3.29807\n",
      "epoch no.3 train no.242270  loss = 4.92505 avg_loss = 3.35297\n",
      "epoch no.3 train no.242280  loss = 4.81416 avg_loss = 3.36013\n",
      "epoch no.3 train no.242290  loss = 5.04486 avg_loss = 3.38518\n",
      "epoch no.3 train no.242300  loss = 3.60800 avg_loss = 3.40744\n",
      "epoch no.3 train no.242310  loss = 2.08569 avg_loss = 3.31895\n",
      "epoch no.3 train no.242320  loss = 2.47491 avg_loss = 3.30508\n",
      "epoch no.3 train no.242330  loss = 5.49919 avg_loss = 3.33415\n",
      "epoch no.3 train no.242340  loss = 2.40156 avg_loss = 3.35988\n",
      "epoch no.3 train no.242350  loss = 2.41303 avg_loss = 3.36305\n",
      "epoch no.3 train no.242360  loss = 6.08630 avg_loss = 3.37450\n",
      "epoch no.3 train no.242370  loss = 3.58610 avg_loss = 3.37438\n",
      "epoch no.3 train no.242380  loss = 3.09565 avg_loss = 3.40390\n",
      "epoch no.3 train no.242390  loss = 3.61451 avg_loss = 3.38513\n",
      "epoch no.3 train no.242400  loss = 4.55557 avg_loss = 3.39599\n",
      "epoch no.3 train no.242410  loss = 2.29908 avg_loss = 3.40187\n",
      "epoch no.3 train no.242420  loss = 2.26717 avg_loss = 3.39376\n",
      "epoch no.3 train no.242430  loss = 4.86747 avg_loss = 3.34559\n",
      "epoch no.3 train no.242440  loss = 2.66762 avg_loss = 3.38156\n",
      "epoch no.3 train no.242450  loss = 3.68866 avg_loss = 3.38172\n",
      "epoch no.3 train no.242460  loss = 2.41424 avg_loss = 3.43755\n",
      "epoch no.3 train no.242470  loss = 3.40829 avg_loss = 3.42155\n",
      "epoch no.3 train no.242480  loss = 2.10243 avg_loss = 3.37151\n",
      "epoch no.3 train no.242490  loss = 3.28697 avg_loss = 3.38444\n",
      "epoch no.3 train no.242500  loss = 3.10411 avg_loss = 3.39295\n",
      "epoch no.3 train no.242510  loss = 3.72005 avg_loss = 3.38788\n",
      "epoch no.3 train no.242520  loss = 3.94707 avg_loss = 3.37832\n",
      "epoch no.3 train no.242530  loss = 3.56119 avg_loss = 3.37567\n",
      "epoch no.3 train no.242540  loss = 2.98616 avg_loss = 3.31411\n",
      "epoch no.3 train no.242550  loss = 2.51517 avg_loss = 3.29382\n",
      "epoch no.3 train no.242560  loss = 4.17369 avg_loss = 3.29108\n",
      "epoch no.3 train no.242570  loss = 2.32218 avg_loss = 3.31364\n",
      "epoch no.3 train no.242580  loss = 5.57958 avg_loss = 3.34172\n",
      "epoch no.3 train no.242590  loss = 2.78611 avg_loss = 3.39028\n",
      "epoch no.3 train no.242600  loss = 2.22150 avg_loss = 3.36626\n",
      "epoch no.3 train no.242610  loss = 3.73717 avg_loss = 3.34239\n",
      "epoch no.3 train no.242620  loss = 2.58088 avg_loss = 3.28565\n",
      "epoch no.3 train no.242630  loss = 2.52775 avg_loss = 3.23688\n",
      "epoch no.3 train no.242640  loss = 3.04992 avg_loss = 3.21010\n",
      "epoch no.3 train no.242650  loss = 3.35388 avg_loss = 3.19241\n",
      "epoch no.3 train no.242660  loss = 2.70083 avg_loss = 3.19068\n",
      "epoch no.3 train no.242670  loss = 2.23541 avg_loss = 3.19206\n",
      "epoch no.3 train no.242680  loss = 2.25488 avg_loss = 3.21048\n",
      "epoch no.3 train no.242690  loss = 2.93931 avg_loss = 3.22162\n",
      "epoch no.3 train no.242700  loss = 1.77702 avg_loss = 3.22152\n",
      "epoch no.3 train no.242710  loss = 3.75973 avg_loss = 3.26442\n",
      "epoch no.3 train no.242720  loss = 3.01580 avg_loss = 3.29393\n",
      "epoch no.3 train no.242730  loss = 3.54483 avg_loss = 3.26835\n",
      "epoch no.3 train no.242740  loss = 4.27845 avg_loss = 3.30396\n",
      "epoch no.3 train no.242750  loss = 2.56106 avg_loss = 3.29547\n",
      "epoch no.3 train no.242760  loss = 3.04214 avg_loss = 3.26761\n",
      "epoch no.3 train no.242770  loss = 3.56259 avg_loss = 3.28402\n",
      "epoch no.3 train no.242780  loss = 3.49775 avg_loss = 3.25682\n",
      "epoch no.3 train no.242790  loss = 4.62314 avg_loss = 3.28704\n",
      "epoch no.3 train no.242800  loss = 2.16600 avg_loss = 3.23335\n",
      "epoch no.3 train no.242810  loss = 3.40750 avg_loss = 3.26915\n",
      "epoch no.3 train no.242820  loss = 4.69250 avg_loss = 3.28475\n",
      "epoch no.3 train no.242830  loss = 2.87857 avg_loss = 3.26502\n",
      "epoch no.3 train no.242840  loss = 2.28718 avg_loss = 3.23276\n",
      "epoch no.3 train no.242850  loss = 3.82131 avg_loss = 3.23214\n",
      "epoch no.3 train no.242860  loss = 3.47555 avg_loss = 3.28276\n",
      "epoch no.3 train no.242870  loss = 2.39514 avg_loss = 3.29225\n",
      "epoch no.3 train no.242880  loss = 2.08591 avg_loss = 3.25804\n",
      "epoch no.3 train no.242890  loss = 3.40897 avg_loss = 3.33889\n",
      "epoch no.3 train no.242900  loss = 2.47170 avg_loss = 3.27724\n",
      "epoch no.3 train no.242910  loss = 4.46293 avg_loss = 3.35155\n",
      "epoch no.3 train no.242920  loss = 4.90383 avg_loss = 3.35368\n",
      "epoch no.3 train no.242930  loss = 1.99163 avg_loss = 3.40148\n",
      "epoch no.3 train no.242940  loss = 1.99334 avg_loss = 3.41819\n",
      "epoch no.3 train no.242950  loss = 2.19015 avg_loss = 3.37694\n",
      "epoch no.3 train no.242960  loss = 2.35240 avg_loss = 3.36174\n",
      "epoch no.3 train no.242970  loss = 4.11852 avg_loss = 3.35216\n",
      "epoch no.3 train no.242980  loss = 2.83053 avg_loss = 3.31290\n",
      "epoch no.3 train no.242990  loss = 3.85165 avg_loss = 3.30556\n",
      "epoch no.3 train no.243000  loss = 1.93749 avg_loss = 3.31519\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁댄스', '▁모음', '</s>']\n",
      "추억의 90년대 가요 베스트</s>\n",
      "epoch no.3 train no.243010  loss = 4.10207 avg_loss = 3.30885\n",
      "epoch no.3 train no.243020  loss = 2.87516 avg_loss = 3.28085\n",
      "epoch no.3 train no.243030  loss = 3.90010 avg_loss = 3.28103\n",
      "epoch no.3 train no.243040  loss = 2.29699 avg_loss = 3.22700\n",
      "epoch no.3 train no.243050  loss = 3.77004 avg_loss = 3.23541\n",
      "epoch no.3 train no.243060  loss = 2.58450 avg_loss = 3.23010\n",
      "epoch no.3 train no.243070  loss = 3.81391 avg_loss = 3.26296\n",
      "epoch no.3 train no.243080  loss = 3.35473 avg_loss = 3.25501\n",
      "epoch no.3 train no.243090  loss = 3.48639 avg_loss = 3.23121\n",
      "epoch no.3 train no.243100  loss = 2.15873 avg_loss = 3.25297\n",
      "epoch no.3 train no.243110  loss = 3.06092 avg_loss = 3.24400\n",
      "epoch no.3 train no.243120  loss = 2.35680 avg_loss = 3.24865\n",
      "epoch no.3 train no.243130  loss = 3.50957 avg_loss = 3.24771\n",
      "epoch no.3 train no.243140  loss = 3.05614 avg_loss = 3.23101\n",
      "epoch no.3 train no.243150  loss = 2.67563 avg_loss = 3.23210\n",
      "epoch no.3 train no.243160  loss = 3.00557 avg_loss = 3.27100\n",
      "epoch no.3 train no.243170  loss = 3.18234 avg_loss = 3.28701\n",
      "epoch no.3 train no.243180  loss = 2.54919 avg_loss = 3.29327\n",
      "epoch no.3 train no.243190  loss = 2.08764 avg_loss = 3.29112\n",
      "epoch no.3 train no.243200  loss = 2.27909 avg_loss = 3.30351\n",
      "epoch no.3 train no.243210  loss = 5.48954 avg_loss = 3.27810\n",
      "epoch no.3 train no.243220  loss = 2.94105 avg_loss = 3.26584\n",
      "epoch no.3 train no.243230  loss = 3.18879 avg_loss = 3.27681\n",
      "epoch no.3 train no.243240  loss = 3.55902 avg_loss = 3.25900\n",
      "epoch no.3 train no.243250  loss = 3.18842 avg_loss = 3.24978\n",
      "epoch no.3 train no.243260  loss = 2.64692 avg_loss = 3.24050\n",
      "epoch no.3 train no.243270  loss = 1.87930 avg_loss = 3.22192\n",
      "epoch no.3 train no.243280  loss = 4.04747 avg_loss = 3.23196\n",
      "epoch no.3 train no.243290  loss = 3.23979 avg_loss = 3.31699\n",
      "epoch no.3 train no.243300  loss = 3.97418 avg_loss = 3.34384\n",
      "epoch no.3 train no.243310  loss = 2.51799 avg_loss = 3.30715\n",
      "epoch no.3 train no.243320  loss = 4.10555 avg_loss = 3.31290\n",
      "epoch no.3 train no.243330  loss = 3.73302 avg_loss = 3.39056\n",
      "epoch no.3 train no.243340  loss = 1.86309 avg_loss = 3.37289\n",
      "epoch no.3 train no.243350  loss = 2.33574 avg_loss = 3.37125\n",
      "epoch no.3 train no.243360  loss = 1.79735 avg_loss = 3.35485\n",
      "epoch no.3 train no.243370  loss = 2.46387 avg_loss = 3.36015\n",
      "epoch no.3 train no.243380  loss = 1.94169 avg_loss = 3.32154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.243390  loss = 2.67252 avg_loss = 3.30812\n",
      "epoch no.3 train no.243400  loss = 3.79763 avg_loss = 3.30891\n",
      "epoch no.3 train no.243410  loss = 2.57821 avg_loss = 3.28318\n",
      "epoch no.3 train no.243420  loss = 3.49610 avg_loss = 3.32290\n",
      "epoch no.3 train no.243430  loss = 3.16147 avg_loss = 3.35662\n",
      "epoch no.3 train no.243440  loss = 2.88599 avg_loss = 3.34993\n",
      "epoch no.3 train no.243450  loss = 1.55228 avg_loss = 3.36399\n",
      "epoch no.3 train no.243460  loss = 2.78769 avg_loss = 3.34850\n",
      "epoch no.3 train no.243470  loss = 3.08724 avg_loss = 3.34810\n",
      "epoch no.3 train no.243480  loss = 2.53363 avg_loss = 3.34505\n",
      "epoch no.3 train no.243490  loss = 2.59388 avg_loss = 3.31158\n",
      "epoch no.3 train no.243500  loss = 1.71995 avg_loss = 3.28318\n",
      "epoch no.3 train no.243510  loss = 3.48806 avg_loss = 3.32862\n",
      "epoch no.3 train no.243520  loss = 3.13848 avg_loss = 3.33869\n",
      "epoch no.3 train no.243530  loss = 2.69521 avg_loss = 3.34297\n",
      "epoch no.3 train no.243540  loss = 5.22932 avg_loss = 3.38210\n",
      "epoch no.3 train no.243550  loss = 2.60742 avg_loss = 3.34650\n",
      "epoch no.3 train no.243560  loss = 2.22180 avg_loss = 3.35207\n",
      "epoch no.3 train no.243570  loss = 2.26433 avg_loss = 3.32588\n",
      "epoch no.3 train no.243580  loss = 2.96099 avg_loss = 3.32491\n",
      "epoch no.3 train no.243590  loss = 3.78906 avg_loss = 3.29934\n",
      "epoch no.3 train no.243600  loss = 2.31524 avg_loss = 3.33321\n",
      "epoch no.3 train no.243610  loss = 2.71097 avg_loss = 3.32310\n",
      "epoch no.3 train no.243620  loss = 4.62076 avg_loss = 3.33205\n",
      "epoch no.3 train no.243630  loss = 3.04733 avg_loss = 3.34828\n",
      "epoch no.3 train no.243640  loss = 2.85312 avg_loss = 3.33797\n",
      "epoch no.3 train no.243650  loss = 3.99142 avg_loss = 3.34797\n",
      "epoch no.3 train no.243660  loss = 2.01052 avg_loss = 3.36309\n",
      "epoch no.3 train no.243670  loss = 2.71445 avg_loss = 3.33889\n",
      "epoch no.3 train no.243680  loss = 4.02126 avg_loss = 3.32418\n",
      "epoch no.3 train no.243690  loss = 3.34169 avg_loss = 3.37466\n",
      "epoch no.3 train no.243700  loss = 5.44154 avg_loss = 3.31071\n",
      "epoch no.3 train no.243710  loss = 1.44777 avg_loss = 3.30524\n",
      "epoch no.3 train no.243720  loss = 2.27154 avg_loss = 3.30516\n",
      "epoch no.3 train no.243730  loss = 3.21820 avg_loss = 3.33176\n",
      "epoch no.3 train no.243740  loss = 2.49044 avg_loss = 3.36517\n",
      "epoch no.3 train no.243750  loss = 3.62193 avg_loss = 3.35074\n",
      "epoch no.3 train no.243760  loss = 3.96702 avg_loss = 3.36883\n",
      "epoch no.3 train no.243770  loss = 3.96716 avg_loss = 3.33426\n",
      "epoch no.3 train no.243780  loss = 4.75470 avg_loss = 3.35972\n",
      "epoch no.3 train no.243790  loss = 3.34515 avg_loss = 3.33548\n",
      "epoch no.3 train no.243800  loss = 1.54508 avg_loss = 3.27224\n",
      "epoch no.3 train no.243810  loss = 4.02291 avg_loss = 3.25440\n",
      "epoch no.3 train no.243820  loss = 2.88309 avg_loss = 3.21404\n",
      "epoch no.3 train no.243830  loss = 2.81068 avg_loss = 3.22351\n",
      "epoch no.3 train no.243840  loss = 2.77686 avg_loss = 3.25342\n",
      "epoch no.3 train no.243850  loss = 3.82821 avg_loss = 3.28207\n",
      "epoch no.3 train no.243860  loss = 3.00493 avg_loss = 3.26477\n",
      "epoch no.3 train no.243870  loss = 2.77249 avg_loss = 3.27081\n",
      "epoch no.3 train no.243880  loss = 3.91248 avg_loss = 3.27387\n",
      "epoch no.3 train no.243890  loss = 3.95987 avg_loss = 3.26628\n",
      "epoch no.3 train no.243900  loss = 2.20120 avg_loss = 3.23944\n",
      "epoch no.3 train no.243910  loss = 4.15960 avg_loss = 3.24647\n",
      "epoch no.3 train no.243920  loss = 5.02659 avg_loss = 3.26769\n",
      "epoch no.3 train no.243930  loss = 4.72742 avg_loss = 3.28540\n",
      "epoch no.3 train no.243940  loss = 3.72316 avg_loss = 3.28454\n",
      "epoch no.3 train no.243950  loss = 3.10281 avg_loss = 3.26342\n",
      "epoch no.3 train no.243960  loss = 3.37668 avg_loss = 3.24450\n",
      "epoch no.3 train no.243970  loss = 4.31633 avg_loss = 3.23786\n",
      "epoch no.3 train no.243980  loss = 3.94226 avg_loss = 3.20921\n",
      "epoch no.3 train no.243990  loss = 3.68314 avg_loss = 3.21219\n",
      "epoch no.3 train no.244000  loss = 4.05707 avg_loss = 3.21535\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '▁모음', '음', '</s>']\n",
      "추억의 90년대 가요모음</s>\n",
      "epoch no.3 train no.244010  loss = 3.46646 avg_loss = 3.22503\n",
      "epoch no.3 train no.244020  loss = 3.27284 avg_loss = 3.23344\n",
      "epoch no.3 train no.244030  loss = 3.08884 avg_loss = 3.22569\n",
      "epoch no.3 train no.244040  loss = 3.37916 avg_loss = 3.23560\n",
      "epoch no.3 train no.244050  loss = 2.72087 avg_loss = 3.24398\n",
      "epoch no.3 train no.244060  loss = 3.23506 avg_loss = 3.25660\n",
      "epoch no.3 train no.244070  loss = 2.10135 avg_loss = 3.23068\n",
      "epoch no.3 train no.244080  loss = 4.12578 avg_loss = 3.27091\n",
      "epoch no.3 train no.244090  loss = 4.13725 avg_loss = 3.28716\n",
      "epoch no.3 train no.244100  loss = 2.59900 avg_loss = 3.26666\n",
      "epoch no.3 train no.244110  loss = 3.15527 avg_loss = 3.27613\n",
      "epoch no.3 train no.244120  loss = 2.52126 avg_loss = 3.27776\n",
      "epoch no.3 train no.244130  loss = 2.80344 avg_loss = 3.27181\n",
      "epoch no.3 train no.244140  loss = 3.40503 avg_loss = 3.29300\n",
      "epoch no.3 train no.244150  loss = 2.53314 avg_loss = 3.29562\n",
      "epoch no.3 train no.244160  loss = 3.37790 avg_loss = 3.27794\n",
      "epoch no.3 train no.244170  loss = 3.40525 avg_loss = 3.27902\n",
      "epoch no.3 train no.244180  loss = 3.04771 avg_loss = 3.22331\n",
      "epoch no.3 train no.244190  loss = 4.83443 avg_loss = 3.22704\n",
      "epoch no.3 train no.244200  loss = 2.94975 avg_loss = 3.26874\n",
      "epoch no.3 train no.244210  loss = 4.89914 avg_loss = 3.30218\n",
      "epoch no.3 train no.244220  loss = 2.92416 avg_loss = 3.27700\n",
      "epoch no.3 train no.244230  loss = 3.78301 avg_loss = 3.30752\n",
      "epoch no.3 train no.244240  loss = 2.73094 avg_loss = 3.25519\n",
      "epoch no.3 train no.244250  loss = 3.08721 avg_loss = 3.27423\n",
      "epoch no.3 train no.244260  loss = 3.21332 avg_loss = 3.30212\n",
      "epoch no.3 train no.244270  loss = 3.66329 avg_loss = 3.35043\n",
      "epoch no.3 train no.244280  loss = 1.92037 avg_loss = 3.32170\n",
      "epoch no.3 train no.244290  loss = 3.31134 avg_loss = 3.34409\n",
      "epoch no.3 train no.244300  loss = 2.34975 avg_loss = 3.32894\n",
      "epoch no.3 train no.244310  loss = 1.66366 avg_loss = 3.28823\n",
      "epoch no.3 train no.244320  loss = 2.63168 avg_loss = 3.29215\n",
      "epoch no.3 train no.244330  loss = 3.60865 avg_loss = 3.30133\n",
      "epoch no.3 train no.244340  loss = 4.04081 avg_loss = 3.31571\n",
      "epoch no.3 train no.244350  loss = 2.91470 avg_loss = 3.38967\n",
      "epoch no.3 train no.244360  loss = 2.79145 avg_loss = 3.36041\n",
      "epoch no.3 train no.244370  loss = 3.43424 avg_loss = 3.39473\n",
      "epoch no.3 train no.244380  loss = 2.57091 avg_loss = 3.43582\n",
      "epoch no.3 train no.244390  loss = 3.17860 avg_loss = 3.43838\n",
      "epoch no.3 train no.244400  loss = 2.20523 avg_loss = 3.41679\n",
      "epoch no.3 train no.244410  loss = 2.69860 avg_loss = 3.36890\n",
      "epoch no.3 train no.244420  loss = 2.48835 avg_loss = 3.31175\n",
      "epoch no.3 train no.244430  loss = 2.50643 avg_loss = 3.29986\n",
      "epoch no.3 train no.244440  loss = 3.36302 avg_loss = 3.35542\n",
      "epoch no.3 train no.244450  loss = 1.92729 avg_loss = 3.34157\n",
      "epoch no.3 train no.244460  loss = 3.58493 avg_loss = 3.34136\n",
      "epoch no.3 train no.244470  loss = 2.93184 avg_loss = 3.32049\n",
      "epoch no.3 train no.244480  loss = 3.57021 avg_loss = 3.29839\n",
      "epoch no.3 train no.244490  loss = 2.30742 avg_loss = 3.23547\n",
      "epoch no.3 train no.244500  loss = 5.62197 avg_loss = 3.24220\n",
      "epoch no.3 train no.244510  loss = 3.96901 avg_loss = 3.30256\n",
      "epoch no.3 train no.244520  loss = 2.82316 avg_loss = 3.30574\n",
      "epoch no.3 train no.244530  loss = 2.69329 avg_loss = 3.29882\n",
      "epoch no.3 train no.244540  loss = 3.75684 avg_loss = 3.32194\n",
      "epoch no.3 train no.244550  loss = 4.36283 avg_loss = 3.32838\n",
      "epoch no.3 train no.244560  loss = 3.68471 avg_loss = 3.35177\n",
      "epoch no.3 train no.244570  loss = 2.85682 avg_loss = 3.31235\n",
      "epoch no.3 train no.244580  loss = 5.37204 avg_loss = 3.31019\n",
      "epoch no.3 train no.244590  loss = 4.37123 avg_loss = 3.32687\n",
      "epoch no.3 train no.244600  loss = 2.66372 avg_loss = 3.32871\n",
      "epoch no.3 train no.244610  loss = 1.38217 avg_loss = 3.30713\n",
      "epoch no.3 train no.244620  loss = 1.26183 avg_loss = 3.29180\n",
      "epoch no.3 train no.244630  loss = 3.91226 avg_loss = 3.32420\n",
      "epoch no.3 train no.244640  loss = 2.24955 avg_loss = 3.33690\n",
      "epoch no.3 train no.244650  loss = 4.86470 avg_loss = 3.34847\n",
      "epoch no.3 train no.244660  loss = 2.56080 avg_loss = 3.33356\n",
      "epoch no.3 train no.244670  loss = 2.84202 avg_loss = 3.35148\n",
      "epoch no.3 train no.244680  loss = 2.69541 avg_loss = 3.34827\n",
      "epoch no.3 train no.244690  loss = 2.89491 avg_loss = 3.31792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.244700  loss = 3.18979 avg_loss = 3.31062\n",
      "epoch no.3 train no.244710  loss = 3.89960 avg_loss = 3.29474\n",
      "epoch no.3 train no.244720  loss = 2.21022 avg_loss = 3.27369\n",
      "epoch no.3 train no.244730  loss = 4.23395 avg_loss = 3.27931\n",
      "epoch no.3 train no.244740  loss = 3.49618 avg_loss = 3.29623\n",
      "epoch no.3 train no.244750  loss = 2.72025 avg_loss = 3.28728\n",
      "epoch no.3 train no.244760  loss = 2.12199 avg_loss = 3.28572\n",
      "epoch no.3 train no.244770  loss = 2.65286 avg_loss = 3.28420\n",
      "epoch no.3 train no.244780  loss = 2.85806 avg_loss = 3.28005\n",
      "epoch no.3 train no.244790  loss = 2.01151 avg_loss = 3.27734\n",
      "epoch no.3 train no.244800  loss = 2.63715 avg_loss = 3.26412\n",
      "epoch no.3 train no.244810  loss = 2.29228 avg_loss = 3.24779\n",
      "epoch no.3 train no.244820  loss = 1.49779 avg_loss = 3.24340\n",
      "epoch no.3 train no.244830  loss = 3.23809 avg_loss = 3.24209\n",
      "epoch no.3 train no.244840  loss = 5.55738 avg_loss = 3.25034\n",
      "epoch no.3 train no.244850  loss = 3.67078 avg_loss = 3.22901\n",
      "epoch no.3 train no.244860  loss = 2.34013 avg_loss = 3.21109\n",
      "epoch no.3 train no.244870  loss = 4.91461 avg_loss = 3.21690\n",
      "epoch no.3 train no.244880  loss = 3.42822 avg_loss = 3.21040\n",
      "epoch no.3 train no.244890  loss = 2.69157 avg_loss = 3.20049\n",
      "epoch no.3 train no.244900  loss = 3.90615 avg_loss = 3.19564\n",
      "epoch no.3 train no.244910  loss = 4.84950 avg_loss = 3.21031\n",
      "epoch no.3 train no.244920  loss = 3.75960 avg_loss = 3.24444\n",
      "epoch no.3 train no.244930  loss = 2.98175 avg_loss = 3.26012\n",
      "epoch no.3 train no.244940  loss = 4.00137 avg_loss = 3.27798\n",
      "epoch no.3 train no.244950  loss = 4.14250 avg_loss = 3.25587\n",
      "epoch no.3 train no.244960  loss = 2.95206 avg_loss = 3.24200\n",
      "epoch no.3 train no.244970  loss = 3.99568 avg_loss = 3.22725\n",
      "epoch no.3 train no.244980  loss = 3.06146 avg_loss = 3.28189\n",
      "epoch no.3 train no.244990  loss = 2.03601 avg_loss = 3.25358\n",
      "epoch no.3 train no.245000  loss = 3.82600 avg_loss = 3.31951\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.3 train no.245010  loss = 3.71474 avg_loss = 3.29785\n",
      "epoch no.3 train no.245020  loss = 6.16610 avg_loss = 3.32145\n",
      "epoch no.3 train no.245030  loss = 1.94822 avg_loss = 3.26626\n",
      "epoch no.3 train no.245040  loss = 5.03673 avg_loss = 3.29214\n",
      "epoch no.3 train no.245050  loss = 3.28552 avg_loss = 3.28477\n",
      "epoch no.3 train no.245060  loss = 5.35815 avg_loss = 3.31605\n",
      "epoch no.3 train no.245070  loss = 4.21678 avg_loss = 3.28119\n",
      "epoch no.3 train no.245080  loss = 3.35911 avg_loss = 3.26019\n",
      "epoch no.3 train no.245090  loss = 3.11660 avg_loss = 3.24407\n",
      "epoch no.3 train no.245100  loss = 4.07946 avg_loss = 3.25247\n",
      "epoch no.3 train no.245110  loss = 4.73342 avg_loss = 3.22933\n",
      "epoch no.3 train no.245120  loss = 2.00304 avg_loss = 3.17595\n",
      "epoch no.3 train no.245130  loss = 2.62459 avg_loss = 3.13863\n",
      "epoch no.3 train no.245140  loss = 3.50155 avg_loss = 3.20701\n",
      "epoch no.3 train no.245150  loss = 2.47230 avg_loss = 3.13208\n",
      "epoch no.3 train no.245160  loss = 2.24083 avg_loss = 3.12603\n",
      "epoch no.3 train no.245170  loss = 4.13319 avg_loss = 3.15800\n",
      "epoch no.3 train no.245180  loss = 3.62710 avg_loss = 3.22366\n",
      "epoch no.3 train no.245190  loss = 2.57551 avg_loss = 3.16033\n",
      "epoch no.3 train no.245200  loss = 3.51459 avg_loss = 3.22262\n",
      "epoch no.3 train no.245210  loss = 5.22185 avg_loss = 3.26425\n",
      "epoch no.3 train no.245220  loss = 2.31570 avg_loss = 3.26075\n",
      "epoch no.3 train no.245230  loss = 5.43877 avg_loss = 3.31547\n",
      "epoch no.3 train no.245240  loss = 4.13540 avg_loss = 3.28246\n",
      "epoch no.3 train no.245250  loss = 2.24339 avg_loss = 3.25046\n",
      "epoch no.3 train no.245260  loss = 4.99327 avg_loss = 3.28371\n",
      "epoch no.3 train no.245270  loss = 4.62178 avg_loss = 3.25100\n",
      "epoch no.3 train no.245280  loss = 2.25312 avg_loss = 3.20928\n",
      "epoch no.3 train no.245290  loss = 1.87300 avg_loss = 3.19653\n",
      "epoch no.3 train no.245300  loss = 2.86235 avg_loss = 3.21368\n",
      "epoch no.3 train no.245310  loss = 3.60174 avg_loss = 3.26483\n",
      "epoch no.3 train no.245320  loss = 3.08872 avg_loss = 3.22628\n",
      "epoch no.3 train no.245330  loss = 3.80430 avg_loss = 3.27235\n",
      "epoch no.3 train no.245340  loss = 4.46995 avg_loss = 3.26041\n",
      "epoch no.3 train no.245350  loss = 3.38890 avg_loss = 3.21975\n",
      "epoch no.3 train no.245360  loss = 3.86064 avg_loss = 3.21708\n",
      "epoch no.3 train no.245370  loss = 2.49298 avg_loss = 3.20582\n",
      "epoch no.3 train no.245380  loss = 3.64161 avg_loss = 3.24303\n",
      "epoch no.3 train no.245390  loss = 3.25953 avg_loss = 3.25751\n",
      "epoch no.3 train no.245400  loss = 6.73097 avg_loss = 3.23623\n",
      "epoch no.3 train no.245410  loss = 3.92851 avg_loss = 3.28996\n",
      "epoch no.3 train no.245420  loss = 2.41005 avg_loss = 3.26853\n",
      "epoch no.3 train no.245430  loss = 2.96522 avg_loss = 3.25030\n",
      "epoch no.3 train no.245440  loss = 3.99104 avg_loss = 3.30306\n",
      "epoch no.3 train no.245450  loss = 2.62246 avg_loss = 3.25750\n",
      "epoch no.3 train no.245460  loss = 2.45631 avg_loss = 3.26466\n",
      "epoch no.3 train no.245470  loss = 3.80985 avg_loss = 3.26320\n",
      "epoch no.3 train no.245480  loss = 3.68616 avg_loss = 3.27659\n",
      "epoch no.3 train no.245490  loss = 5.00381 avg_loss = 3.28616\n",
      "epoch no.3 train no.245500  loss = 3.21299 avg_loss = 3.26606\n",
      "epoch no.3 train no.245510  loss = 3.36301 avg_loss = 3.23723\n",
      "epoch no.3 train no.245520  loss = 2.13904 avg_loss = 3.19676\n",
      "epoch no.3 train no.245530  loss = 2.89423 avg_loss = 3.23597\n",
      "epoch no.3 train no.245540  loss = 4.05500 avg_loss = 3.27787\n",
      "epoch no.3 train no.245550  loss = 2.56563 avg_loss = 3.27524\n",
      "epoch no.3 train no.245560  loss = 3.69059 avg_loss = 3.28552\n",
      "epoch no.3 train no.245570  loss = 2.82071 avg_loss = 3.32041\n",
      "epoch no.3 train no.245580  loss = 3.59820 avg_loss = 3.36691\n",
      "epoch no.3 train no.245590  loss = 3.32933 avg_loss = 3.33812\n",
      "epoch no.3 train no.245600  loss = 3.18312 avg_loss = 3.35786\n",
      "epoch no.3 train no.245610  loss = 3.25054 avg_loss = 3.35987\n",
      "epoch no.3 train no.245620  loss = 2.83536 avg_loss = 3.38644\n",
      "epoch no.3 train no.245630  loss = 5.09413 avg_loss = 3.36111\n",
      "epoch no.3 train no.245640  loss = 2.98594 avg_loss = 3.32565\n",
      "epoch no.3 train no.245650  loss = 4.85450 avg_loss = 3.29929\n",
      "epoch no.3 train no.245660  loss = 3.23985 avg_loss = 3.26370\n",
      "epoch no.3 train no.245670  loss = 4.56474 avg_loss = 3.27312\n",
      "epoch no.3 train no.245680  loss = 2.94168 avg_loss = 3.26038\n",
      "epoch no.3 train no.245690  loss = 1.60659 avg_loss = 3.26169\n",
      "epoch no.3 train no.245700  loss = 3.10713 avg_loss = 3.26874\n",
      "epoch no.3 train no.245710  loss = 4.40761 avg_loss = 3.32177\n",
      "epoch no.3 train no.245720  loss = 3.06931 avg_loss = 3.29517\n",
      "epoch no.3 train no.245730  loss = 1.96126 avg_loss = 3.28609\n",
      "epoch no.3 train no.245740  loss = 2.41178 avg_loss = 3.25953\n",
      "epoch no.3 train no.245750  loss = 3.00686 avg_loss = 3.25725\n",
      "epoch no.3 train no.245760  loss = 2.86339 avg_loss = 3.28278\n",
      "epoch no.3 train no.245770  loss = 3.35202 avg_loss = 3.29310\n",
      "epoch no.3 train no.245780  loss = 3.29476 avg_loss = 3.34858\n",
      "epoch no.3 train no.245790  loss = 3.84176 avg_loss = 3.35895\n",
      "epoch no.3 train no.245800  loss = 3.42537 avg_loss = 3.33739\n",
      "epoch no.3 train no.245810  loss = 3.43048 avg_loss = 3.36578\n",
      "epoch no.3 train no.245820  loss = 3.88220 avg_loss = 3.38341\n",
      "epoch no.3 train no.245830  loss = 2.52096 avg_loss = 3.35932\n",
      "epoch no.3 train no.245840  loss = 3.01365 avg_loss = 3.34898\n",
      "epoch no.3 train no.245850  loss = 3.67485 avg_loss = 3.31656\n",
      "epoch no.3 train no.245860  loss = 3.77731 avg_loss = 3.28566\n",
      "epoch no.3 train no.245870  loss = 3.22866 avg_loss = 3.28637\n",
      "epoch no.3 train no.245880  loss = 1.74213 avg_loss = 3.29945\n",
      "epoch no.3 train no.245890  loss = 3.41033 avg_loss = 3.30473\n",
      "epoch no.3 train no.245900  loss = 2.55757 avg_loss = 3.29140\n",
      "epoch no.3 train no.245910  loss = 4.26751 avg_loss = 3.31989\n",
      "epoch no.3 train no.245920  loss = 5.17630 avg_loss = 3.32509\n",
      "epoch no.3 train no.245930  loss = 4.00820 avg_loss = 3.34534\n",
      "epoch no.3 train no.245940  loss = 2.74971 avg_loss = 3.32225\n",
      "epoch no.3 train no.245950  loss = 3.80740 avg_loss = 3.34021\n",
      "epoch no.3 train no.245960  loss = 2.95638 avg_loss = 3.32065\n",
      "epoch no.3 train no.245970  loss = 2.75403 avg_loss = 3.33655\n",
      "epoch no.3 train no.245980  loss = 4.97536 avg_loss = 3.31988\n",
      "epoch no.3 train no.245990  loss = 2.68545 avg_loss = 3.29340\n",
      "epoch no.3 train no.246000  loss = 3.56734 avg_loss = 3.31925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '월드', 'g', 'm', '</s>', '</s>']\n",
      "추억의 싸이 bgm 모음</s>\n",
      "epoch no.3 train no.246010  loss = 5.75641 avg_loss = 3.33636\n",
      "epoch no.3 train no.246020  loss = 3.03996 avg_loss = 3.31067\n",
      "epoch no.3 train no.246030  loss = 3.65588 avg_loss = 3.31493\n",
      "epoch no.3 train no.246040  loss = 2.68815 avg_loss = 3.39289\n",
      "epoch no.3 train no.246050  loss = 3.44059 avg_loss = 3.38125\n",
      "epoch no.3 train no.246060  loss = 5.55268 avg_loss = 3.39044\n",
      "epoch no.3 train no.246070  loss = 2.46012 avg_loss = 3.37062\n",
      "epoch no.3 train no.246080  loss = 4.77577 avg_loss = 3.37983\n",
      "epoch no.3 train no.246090  loss = 2.89841 avg_loss = 3.38516\n",
      "epoch no.3 train no.246100  loss = 3.30358 avg_loss = 3.34193\n",
      "epoch no.3 train no.246110  loss = 4.20899 avg_loss = 3.30341\n",
      "epoch no.3 train no.246120  loss = 2.24006 avg_loss = 3.29609\n",
      "epoch no.3 train no.246130  loss = 4.47189 avg_loss = 3.31997\n",
      "epoch no.3 train no.246140  loss = 2.92172 avg_loss = 3.32547\n",
      "epoch no.3 train no.246150  loss = 2.79923 avg_loss = 3.32900\n",
      "epoch no.3 train no.246160  loss = 2.50433 avg_loss = 3.34074\n",
      "epoch no.3 train no.246170  loss = 2.81230 avg_loss = 3.31029\n",
      "epoch no.3 train no.246180  loss = 2.72862 avg_loss = 3.27944\n",
      "epoch no.3 train no.246190  loss = 4.06662 avg_loss = 3.29862\n",
      "epoch no.3 train no.246200  loss = 4.76627 avg_loss = 3.29395\n",
      "epoch no.3 train no.246210  loss = 3.26456 avg_loss = 3.26240\n",
      "epoch no.3 train no.246220  loss = 5.55793 avg_loss = 3.29393\n",
      "epoch no.3 train no.246230  loss = 3.05130 avg_loss = 3.26695\n",
      "epoch no.3 train no.246240  loss = 4.20855 avg_loss = 3.28914\n",
      "epoch no.3 train no.246250  loss = 6.25864 avg_loss = 3.28502\n",
      "epoch no.3 train no.246260  loss = 3.10486 avg_loss = 3.30914\n",
      "epoch no.3 train no.246270  loss = 3.43941 avg_loss = 3.29561\n",
      "epoch no.3 train no.246280  loss = 3.80729 avg_loss = 3.31371\n",
      "epoch no.3 train no.246290  loss = 3.18828 avg_loss = 3.31235\n",
      "epoch no.3 train no.246300  loss = 3.20603 avg_loss = 3.33626\n",
      "epoch no.3 train no.246310  loss = 4.14025 avg_loss = 3.30889\n",
      "epoch no.3 train no.246320  loss = 2.07406 avg_loss = 3.32063\n",
      "epoch no.3 train no.246330  loss = 4.13367 avg_loss = 3.32199\n",
      "epoch no.3 train no.246340  loss = 2.70889 avg_loss = 3.32802\n",
      "epoch no.3 train no.246350  loss = 5.26756 avg_loss = 3.32869\n",
      "epoch no.3 train no.246360  loss = 3.14195 avg_loss = 3.36692\n",
      "epoch no.3 train no.246370  loss = 3.04769 avg_loss = 3.37281\n",
      "epoch no.3 train no.246380  loss = 3.73650 avg_loss = 3.35375\n",
      "epoch no.3 train no.246390  loss = 2.28938 avg_loss = 3.29676\n",
      "epoch no.3 train no.246400  loss = 3.41233 avg_loss = 3.29447\n",
      "epoch no.3 train no.246410  loss = 3.65791 avg_loss = 3.30400\n",
      "epoch no.3 train no.246420  loss = 2.12263 avg_loss = 3.25351\n",
      "epoch no.3 train no.246430  loss = 2.85817 avg_loss = 3.21967\n",
      "epoch no.3 train no.246440  loss = 5.20154 avg_loss = 3.29995\n",
      "epoch no.3 train no.246450  loss = 5.99757 avg_loss = 3.34905\n",
      "epoch no.3 train no.246460  loss = 5.36844 avg_loss = 3.36296\n",
      "epoch no.3 train no.246470  loss = 2.96228 avg_loss = 3.35391\n",
      "epoch no.3 train no.246480  loss = 3.79273 avg_loss = 3.35744\n",
      "epoch no.3 train no.246490  loss = 3.28781 avg_loss = 3.34997\n",
      "epoch no.3 train no.246500  loss = 2.35677 avg_loss = 3.30554\n",
      "epoch no.3 train no.246510  loss = 2.08439 avg_loss = 3.33637\n",
      "epoch no.3 train no.246520  loss = 3.61108 avg_loss = 3.37264\n",
      "epoch no.3 train no.246530  loss = 3.70150 avg_loss = 3.35569\n",
      "epoch no.3 train no.246540  loss = 2.76571 avg_loss = 3.32087\n",
      "epoch no.3 train no.246550  loss = 2.40390 avg_loss = 3.31150\n",
      "epoch no.3 train no.246560  loss = 2.59085 avg_loss = 3.36496\n",
      "epoch no.3 train no.246570  loss = 1.75045 avg_loss = 3.36126\n",
      "epoch no.3 train no.246580  loss = 2.14517 avg_loss = 3.34554\n",
      "epoch no.3 train no.246590  loss = 3.77528 avg_loss = 3.34902\n",
      "epoch no.3 train no.246600  loss = 3.71982 avg_loss = 3.29101\n",
      "epoch no.3 train no.246610  loss = 3.68600 avg_loss = 3.31179\n",
      "epoch no.3 train no.246620  loss = 3.78721 avg_loss = 3.32978\n",
      "epoch no.3 train no.246630  loss = 3.10684 avg_loss = 3.32772\n",
      "epoch no.3 train no.246640  loss = 2.02862 avg_loss = 3.31425\n",
      "epoch no.3 train no.246650  loss = 3.41132 avg_loss = 3.34674\n",
      "epoch no.3 train no.246660  loss = 2.10255 avg_loss = 3.35280\n",
      "epoch no.3 train no.246670  loss = 1.89797 avg_loss = 3.34810\n",
      "epoch no.3 train no.246680  loss = 4.00228 avg_loss = 3.36533\n",
      "epoch no.3 train no.246690  loss = 3.47484 avg_loss = 3.37110\n",
      "epoch no.3 train no.246700  loss = 2.92797 avg_loss = 3.34623\n",
      "epoch no.3 train no.246710  loss = 2.16353 avg_loss = 3.34488\n",
      "epoch no.3 train no.246720  loss = 3.16567 avg_loss = 3.31638\n",
      "epoch no.3 train no.246730  loss = 3.12179 avg_loss = 3.30656\n",
      "epoch no.3 train no.246740  loss = 3.42587 avg_loss = 3.27970\n",
      "epoch no.3 train no.246750  loss = 3.20443 avg_loss = 3.23674\n",
      "epoch no.3 train no.246760  loss = 3.83420 avg_loss = 3.25747\n",
      "epoch no.3 train no.246770  loss = 2.29397 avg_loss = 3.25114\n",
      "epoch no.3 train no.246780  loss = 5.17051 avg_loss = 3.28880\n",
      "epoch no.3 train no.246790  loss = 5.60430 avg_loss = 3.31386\n",
      "epoch no.3 train no.246800  loss = 2.90801 avg_loss = 3.34337\n",
      "epoch no.3 train no.246810  loss = 2.94947 avg_loss = 3.36367\n",
      "epoch no.3 train no.246820  loss = 4.57148 avg_loss = 3.35847\n",
      "epoch no.3 train no.246830  loss = 3.53219 avg_loss = 3.43144\n",
      "epoch no.3 train no.246840  loss = 5.09827 avg_loss = 3.40201\n",
      "epoch no.3 train no.246850  loss = 4.46789 avg_loss = 3.36960\n",
      "epoch no.3 train no.246860  loss = 1.63789 avg_loss = 3.32688\n",
      "epoch no.3 train no.246870  loss = 3.82178 avg_loss = 3.31156\n",
      "epoch no.3 train no.246880  loss = 4.26996 avg_loss = 3.37177\n",
      "epoch no.3 train no.246890  loss = 1.79102 avg_loss = 3.34951\n",
      "epoch no.3 train no.246900  loss = 4.10906 avg_loss = 3.36589\n",
      "epoch no.3 train no.246910  loss = 2.68716 avg_loss = 3.37983\n",
      "epoch no.3 train no.246920  loss = 3.46039 avg_loss = 3.31444\n",
      "epoch no.3 train no.246930  loss = 3.12622 avg_loss = 3.28863\n",
      "epoch no.3 train no.246940  loss = 2.27854 avg_loss = 3.27981\n",
      "epoch no.3 train no.246950  loss = 3.00748 avg_loss = 3.31648\n",
      "epoch no.3 train no.246960  loss = 2.72675 avg_loss = 3.28139\n",
      "epoch no.3 train no.246970  loss = 3.89905 avg_loss = 3.30871\n",
      "epoch no.3 train no.246980  loss = 3.00631 avg_loss = 3.36612\n",
      "epoch no.3 train no.246990  loss = 2.96563 avg_loss = 3.37631\n",
      "epoch no.3 train no.247000  loss = 4.65935 avg_loss = 3.34315\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.3 train no.247010  loss = 2.53374 avg_loss = 3.29857\n",
      "epoch no.3 train no.247020  loss = 4.85484 avg_loss = 3.28248\n",
      "epoch no.3 train no.247030  loss = 2.85624 avg_loss = 3.26280\n",
      "epoch no.3 train no.247040  loss = 3.09497 avg_loss = 3.25950\n",
      "epoch no.3 train no.247050  loss = 2.65221 avg_loss = 3.28638\n",
      "epoch no.3 train no.247060  loss = 3.98747 avg_loss = 3.26962\n",
      "epoch no.3 train no.247070  loss = 5.25406 avg_loss = 3.29079\n",
      "epoch no.3 train no.247080  loss = 3.75014 avg_loss = 3.28855\n",
      "epoch no.3 train no.247090  loss = 3.84609 avg_loss = 3.26739\n",
      "epoch no.3 train no.247100  loss = 3.55482 avg_loss = 3.29826\n",
      "epoch no.3 train no.247110  loss = 5.34798 avg_loss = 3.36657\n",
      "epoch no.3 train no.247120  loss = 4.59679 avg_loss = 3.34508\n",
      "epoch no.3 train no.247130  loss = 4.41434 avg_loss = 3.36190\n",
      "epoch no.3 train no.247140  loss = 3.26537 avg_loss = 3.38525\n",
      "epoch no.3 train no.247150  loss = 4.56617 avg_loss = 3.38633\n",
      "epoch no.3 train no.247160  loss = 4.71629 avg_loss = 3.35568\n",
      "epoch no.3 train no.247170  loss = 1.80488 avg_loss = 3.34838\n",
      "epoch no.3 train no.247180  loss = 2.55968 avg_loss = 3.37097\n",
      "epoch no.3 train no.247190  loss = 4.13096 avg_loss = 3.37147\n",
      "epoch no.3 train no.247200  loss = 3.95041 avg_loss = 3.34705\n",
      "epoch no.3 train no.247210  loss = 3.46692 avg_loss = 3.32697\n",
      "epoch no.3 train no.247220  loss = 2.52205 avg_loss = 3.29724\n",
      "epoch no.3 train no.247230  loss = 2.96050 avg_loss = 3.26124\n",
      "epoch no.3 train no.247240  loss = 2.80516 avg_loss = 3.23134\n",
      "epoch no.3 train no.247250  loss = 4.91934 avg_loss = 3.23203\n",
      "epoch no.3 train no.247260  loss = 4.58520 avg_loss = 3.26341\n",
      "epoch no.3 train no.247270  loss = 5.03680 avg_loss = 3.28132\n",
      "epoch no.3 train no.247280  loss = 2.67659 avg_loss = 3.28287\n",
      "epoch no.3 train no.247290  loss = 4.22344 avg_loss = 3.25742\n",
      "epoch no.3 train no.247300  loss = 2.97246 avg_loss = 3.25950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.247310  loss = 1.50557 avg_loss = 3.26624\n",
      "epoch no.3 train no.247320  loss = 4.65083 avg_loss = 3.25394\n",
      "epoch no.3 train no.247330  loss = 2.47424 avg_loss = 3.26781\n",
      "epoch no.3 train no.247340  loss = 2.28163 avg_loss = 3.23932\n",
      "epoch no.3 train no.247350  loss = 2.79455 avg_loss = 3.24072\n",
      "epoch no.3 train no.247360  loss = 2.24599 avg_loss = 3.28348\n",
      "epoch no.3 train no.247370  loss = 3.10075 avg_loss = 3.26842\n",
      "epoch no.3 train no.247380  loss = 3.04344 avg_loss = 3.23665\n",
      "epoch no.3 train no.247390  loss = 2.78954 avg_loss = 3.23367\n",
      "epoch no.3 train no.247400  loss = 2.08184 avg_loss = 3.24972\n",
      "epoch no.3 train no.247410  loss = 2.41722 avg_loss = 3.18427\n",
      "epoch no.3 train no.247420  loss = 2.12669 avg_loss = 3.15746\n",
      "epoch no.3 train no.247430  loss = 3.89513 avg_loss = 3.19463\n",
      "epoch no.3 train no.247440  loss = 2.69377 avg_loss = 3.25681\n",
      "epoch no.3 train no.247450  loss = 4.12764 avg_loss = 3.28913\n",
      "epoch no.3 train no.247460  loss = 3.22629 avg_loss = 3.27541\n",
      "epoch no.3 train no.247470  loss = 3.18466 avg_loss = 3.29686\n",
      "epoch no.3 train no.247480  loss = 2.51849 avg_loss = 3.26308\n",
      "epoch no.3 train no.247490  loss = 3.41052 avg_loss = 3.26080\n",
      "epoch no.3 train no.247500  loss = 2.60387 avg_loss = 3.29465\n",
      "epoch no.3 train no.247510  loss = 2.54670 avg_loss = 3.27528\n",
      "epoch no.3 train no.247520  loss = 2.19428 avg_loss = 3.24442\n",
      "epoch no.3 train no.247530  loss = 3.10712 avg_loss = 3.24640\n",
      "epoch no.3 train no.247540  loss = 3.56340 avg_loss = 3.19850\n",
      "epoch no.3 train no.247550  loss = 3.09969 avg_loss = 3.15952\n",
      "epoch no.3 train no.247560  loss = 4.77696 avg_loss = 3.18879\n",
      "epoch no.3 train no.247570  loss = 2.94145 avg_loss = 3.19638\n",
      "epoch no.3 train no.247580  loss = 3.71207 avg_loss = 3.20636\n",
      "epoch no.3 train no.247590  loss = 4.01499 avg_loss = 3.28766\n",
      "epoch no.3 train no.247600  loss = 4.16645 avg_loss = 3.28923\n",
      "epoch no.3 train no.247610  loss = 3.59251 avg_loss = 3.25515\n",
      "epoch no.3 train no.247620  loss = 4.14468 avg_loss = 3.26765\n",
      "epoch no.3 train no.247630  loss = 3.47583 avg_loss = 3.25487\n",
      "epoch no.3 train no.247640  loss = 4.55006 avg_loss = 3.24186\n",
      "epoch no.3 train no.247650  loss = 2.34520 avg_loss = 3.25166\n",
      "epoch no.3 train no.247660  loss = 2.58394 avg_loss = 3.23352\n",
      "epoch no.3 train no.247670  loss = 4.66838 avg_loss = 3.25608\n",
      "epoch no.3 train no.247680  loss = 4.83301 avg_loss = 3.25963\n",
      "epoch no.3 train no.247690  loss = 3.69589 avg_loss = 3.28211\n",
      "epoch no.3 train no.247700  loss = 3.55669 avg_loss = 3.31464\n",
      "epoch no.3 train no.247710  loss = 3.02684 avg_loss = 3.33646\n",
      "epoch no.3 train no.247720  loss = 2.97440 avg_loss = 3.30413\n",
      "epoch no.3 train no.247730  loss = 3.49384 avg_loss = 3.29479\n",
      "epoch no.3 train no.247740  loss = 4.73381 avg_loss = 3.29741\n",
      "epoch no.3 train no.247750  loss = 2.33835 avg_loss = 3.31585\n",
      "epoch no.3 train no.247760  loss = 2.69350 avg_loss = 3.28167\n",
      "epoch no.3 train no.247770  loss = 4.20992 avg_loss = 3.29195\n",
      "epoch no.3 train no.247780  loss = 2.12716 avg_loss = 3.27958\n",
      "epoch no.3 train no.247790  loss = 3.79356 avg_loss = 3.28692\n",
      "epoch no.3 train no.247800  loss = 4.27460 avg_loss = 3.23433\n",
      "epoch no.3 train no.247810  loss = 4.25871 avg_loss = 3.29276\n",
      "epoch no.3 train no.247820  loss = 4.24670 avg_loss = 3.31301\n",
      "epoch no.3 train no.247830  loss = 4.47564 avg_loss = 3.31694\n",
      "epoch no.3 train no.247840  loss = 3.66612 avg_loss = 3.31055\n",
      "epoch no.3 train no.247850  loss = 2.64094 avg_loss = 3.27993\n",
      "epoch no.3 train no.247860  loss = 4.40459 avg_loss = 3.31007\n",
      "epoch no.3 train no.247870  loss = 3.31569 avg_loss = 3.30401\n",
      "epoch no.3 train no.247880  loss = 3.88523 avg_loss = 3.34533\n",
      "epoch no.3 train no.247890  loss = 2.95546 avg_loss = 3.33055\n",
      "epoch no.3 train no.247900  loss = 2.89060 avg_loss = 3.26779\n",
      "epoch no.3 train no.247910  loss = 2.93044 avg_loss = 3.28305\n",
      "epoch no.3 train no.247920  loss = 4.26108 avg_loss = 3.28109\n",
      "epoch no.3 train no.247930  loss = 2.44220 avg_loss = 3.26211\n",
      "epoch no.3 train no.247940  loss = 2.73704 avg_loss = 3.22897\n",
      "epoch no.3 train no.247950  loss = 3.30528 avg_loss = 3.20919\n",
      "epoch no.3 train no.247960  loss = 3.25037 avg_loss = 3.24861\n",
      "epoch no.3 train no.247970  loss = 3.44745 avg_loss = 3.26404\n",
      "epoch no.3 train no.247980  loss = 3.86478 avg_loss = 3.29204\n",
      "epoch no.3 train no.247990  loss = 4.20492 avg_loss = 3.29837\n",
      "epoch no.3 train no.248000  loss = 2.96087 avg_loss = 3.30645\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '팝', '과', '▁함께', '</s>']\n",
      "추억의 올드팝과 함께</s>\n",
      "epoch no.3 train no.248010  loss = 2.75845 avg_loss = 3.26558\n",
      "epoch no.3 train no.248020  loss = 5.19497 avg_loss = 3.28850\n",
      "epoch no.3 train no.248030  loss = 3.81411 avg_loss = 3.30384\n",
      "epoch no.3 train no.248040  loss = 2.49508 avg_loss = 3.29991\n",
      "epoch no.3 train no.248050  loss = 3.75733 avg_loss = 3.25476\n",
      "epoch no.3 train no.248060  loss = 2.60883 avg_loss = 3.22850\n",
      "epoch no.3 train no.248070  loss = 2.78205 avg_loss = 3.19539\n",
      "epoch no.3 train no.248080  loss = 2.96021 avg_loss = 3.18171\n",
      "epoch no.3 train no.248090  loss = 4.50277 avg_loss = 3.23081\n",
      "epoch no.3 train no.248100  loss = 3.83865 avg_loss = 3.23953\n",
      "epoch no.3 train no.248110  loss = 5.65702 avg_loss = 3.22645\n",
      "epoch no.3 train no.248120  loss = 2.12736 avg_loss = 3.24363\n",
      "epoch no.3 train no.248130  loss = 4.72594 avg_loss = 3.22116\n",
      "epoch no.3 train no.248140  loss = 3.46188 avg_loss = 3.24832\n",
      "epoch no.3 train no.248150  loss = 2.53298 avg_loss = 3.27582\n",
      "epoch no.3 train no.248160  loss = 2.92921 avg_loss = 3.26612\n",
      "epoch no.3 train no.248170  loss = 3.65179 avg_loss = 3.23827\n",
      "epoch no.3 train no.248180  loss = 5.25805 avg_loss = 3.23319\n",
      "epoch no.3 train no.248190  loss = 5.23283 avg_loss = 3.28064\n",
      "epoch no.3 train no.248200  loss = 3.24213 avg_loss = 3.21269\n",
      "epoch no.3 train no.248210  loss = 2.72154 avg_loss = 3.19602\n",
      "epoch no.3 train no.248220  loss = 2.92456 avg_loss = 3.19924\n",
      "epoch no.3 train no.248230  loss = 3.06572 avg_loss = 3.20447\n",
      "epoch no.3 train no.248240  loss = 4.79327 avg_loss = 3.25495\n",
      "epoch no.3 train no.248250  loss = 2.46550 avg_loss = 3.26985\n",
      "epoch no.3 train no.248260  loss = 3.77132 avg_loss = 3.31079\n",
      "epoch no.3 train no.248270  loss = 4.43868 avg_loss = 3.31453\n",
      "epoch no.3 train no.248280  loss = 4.58333 avg_loss = 3.35329\n",
      "epoch no.3 train no.248290  loss = 3.51430 avg_loss = 3.34105\n",
      "epoch no.3 train no.248300  loss = 2.01203 avg_loss = 3.27477\n",
      "epoch no.3 train no.248310  loss = 2.40511 avg_loss = 3.25175\n",
      "epoch no.3 train no.248320  loss = 3.38515 avg_loss = 3.25344\n",
      "epoch no.3 train no.248330  loss = 6.62615 avg_loss = 3.33267\n",
      "epoch no.3 train no.248340  loss = 2.61889 avg_loss = 3.31405\n",
      "epoch no.3 train no.248350  loss = 3.19696 avg_loss = 3.28861\n",
      "epoch no.3 train no.248360  loss = 4.30603 avg_loss = 3.32737\n",
      "epoch no.3 train no.248370  loss = 3.43579 avg_loss = 3.29247\n",
      "epoch no.3 train no.248380  loss = 3.50970 avg_loss = 3.31413\n",
      "epoch no.3 train no.248390  loss = 3.74161 avg_loss = 3.31211\n",
      "epoch no.3 train no.248400  loss = 2.64283 avg_loss = 3.30585\n",
      "epoch no.3 train no.248410  loss = 2.18706 avg_loss = 3.25687\n",
      "epoch no.3 train no.248420  loss = 2.96983 avg_loss = 3.19712\n",
      "epoch no.3 train no.248430  loss = 1.74799 avg_loss = 3.18110\n",
      "epoch no.3 train no.248440  loss = 3.57446 avg_loss = 3.20147\n",
      "epoch no.3 train no.248450  loss = 2.30489 avg_loss = 3.20452\n",
      "epoch no.3 train no.248460  loss = 3.37723 avg_loss = 3.23352\n",
      "epoch no.3 train no.248470  loss = 3.78844 avg_loss = 3.22315\n",
      "epoch no.3 train no.248480  loss = 4.02639 avg_loss = 3.26047\n",
      "epoch no.3 train no.248490  loss = 3.90032 avg_loss = 3.29052\n",
      "epoch no.3 train no.248500  loss = 2.50011 avg_loss = 3.26200\n",
      "epoch no.3 train no.248510  loss = 2.93719 avg_loss = 3.22656\n",
      "epoch no.3 train no.248520  loss = 2.91048 avg_loss = 3.19001\n",
      "epoch no.3 train no.248530  loss = 2.58509 avg_loss = 3.21191\n",
      "epoch no.3 train no.248540  loss = 2.69622 avg_loss = 3.18364\n",
      "epoch no.3 train no.248550  loss = 3.19721 avg_loss = 3.24008\n",
      "epoch no.3 train no.248560  loss = 2.09908 avg_loss = 3.24757\n",
      "epoch no.3 train no.248570  loss = 2.98432 avg_loss = 3.21790\n",
      "epoch no.3 train no.248580  loss = 2.43855 avg_loss = 3.24384\n",
      "epoch no.3 train no.248590  loss = 3.38520 avg_loss = 3.22746\n",
      "epoch no.3 train no.248600  loss = 2.98534 avg_loss = 3.24086\n",
      "epoch no.3 train no.248610  loss = 3.32419 avg_loss = 3.29968\n",
      "epoch no.3 train no.248620  loss = 2.70669 avg_loss = 3.25904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.248630  loss = 2.75534 avg_loss = 3.24304\n",
      "epoch no.3 train no.248640  loss = 3.09705 avg_loss = 3.24619\n",
      "epoch no.3 train no.248650  loss = 2.62823 avg_loss = 3.25168\n",
      "epoch no.3 train no.248660  loss = 2.72933 avg_loss = 3.25118\n",
      "epoch no.3 train no.248670  loss = 2.44377 avg_loss = 3.23754\n",
      "epoch no.3 train no.248680  loss = 4.06829 avg_loss = 3.22798\n",
      "epoch no.3 train no.248690  loss = 4.51630 avg_loss = 3.24377\n",
      "epoch no.3 train no.248700  loss = 4.34146 avg_loss = 3.30975\n",
      "epoch no.3 train no.248710  loss = 3.09325 avg_loss = 3.28084\n",
      "epoch no.3 train no.248720  loss = 2.46066 avg_loss = 3.26491\n",
      "epoch no.3 train no.248730  loss = 3.50155 avg_loss = 3.27761\n",
      "epoch no.3 train no.248740  loss = 3.15289 avg_loss = 3.25262\n",
      "epoch no.3 train no.248750  loss = 4.46591 avg_loss = 3.27373\n",
      "epoch no.3 train no.248760  loss = 3.78812 avg_loss = 3.30215\n",
      "epoch no.3 train no.248770  loss = 4.37739 avg_loss = 3.29351\n",
      "epoch no.3 train no.248780  loss = 3.22322 avg_loss = 3.28782\n",
      "epoch no.3 train no.248790  loss = 4.22740 avg_loss = 3.30042\n",
      "epoch no.3 train no.248800  loss = 4.19344 avg_loss = 3.31614\n",
      "epoch no.3 train no.248810  loss = 2.37750 avg_loss = 3.34197\n",
      "epoch no.3 train no.248820  loss = 2.82561 avg_loss = 3.29463\n",
      "epoch no.3 train no.248830  loss = 1.84809 avg_loss = 3.25964\n",
      "epoch no.3 train no.248840  loss = 2.68645 avg_loss = 3.22614\n",
      "epoch no.3 train no.248850  loss = 3.54987 avg_loss = 3.21858\n",
      "epoch no.3 train no.248860  loss = 3.58078 avg_loss = 3.21048\n",
      "epoch no.3 train no.248870  loss = 5.23099 avg_loss = 3.27859\n",
      "epoch no.3 train no.248880  loss = 3.81889 avg_loss = 3.30259\n",
      "epoch no.3 train no.248890  loss = 4.58743 avg_loss = 3.34246\n",
      "epoch no.3 train no.248900  loss = 6.44763 avg_loss = 3.44694\n",
      "epoch no.3 train no.248910  loss = 2.71313 avg_loss = 3.38796\n",
      "epoch no.3 train no.248920  loss = 2.41915 avg_loss = 3.35314\n",
      "epoch no.3 train no.248930  loss = 3.54649 avg_loss = 3.37915\n",
      "epoch no.3 train no.248940  loss = 3.10812 avg_loss = 3.36228\n",
      "epoch no.3 train no.248950  loss = 4.54003 avg_loss = 3.39651\n",
      "epoch no.3 train no.248960  loss = 3.15765 avg_loss = 3.46770\n",
      "epoch no.3 train no.248970  loss = 4.12192 avg_loss = 3.48845\n",
      "epoch no.3 train no.248980  loss = 2.41057 avg_loss = 3.44883\n",
      "epoch no.3 train no.248990  loss = 2.15023 avg_loss = 3.40987\n",
      "epoch no.3 train no.249000  loss = 3.18240 avg_loss = 3.36393\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '년대', '▁노래', '곡', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.3 train no.249010  loss = 2.37226 avg_loss = 3.33213\n",
      "epoch no.3 train no.249020  loss = 2.73237 avg_loss = 3.33153\n",
      "epoch no.3 train no.249030  loss = 2.04474 avg_loss = 3.33120\n",
      "epoch no.3 train no.249040  loss = 2.50944 avg_loss = 3.33482\n",
      "epoch no.3 train no.249050  loss = 3.65775 avg_loss = 3.30117\n",
      "epoch no.3 train no.249060  loss = 3.14745 avg_loss = 3.28741\n",
      "epoch no.3 train no.249070  loss = 2.91350 avg_loss = 3.28438\n",
      "epoch no.3 train no.249080  loss = 2.19051 avg_loss = 3.26849\n",
      "epoch no.3 train no.249090  loss = 3.35002 avg_loss = 3.28818\n",
      "epoch no.3 train no.249100  loss = 3.67881 avg_loss = 3.27534\n",
      "epoch no.3 train no.249110  loss = 3.27335 avg_loss = 3.31233\n",
      "epoch no.3 train no.249120  loss = 1.63642 avg_loss = 3.29213\n",
      "epoch no.3 train no.249130  loss = 3.85164 avg_loss = 3.31018\n",
      "epoch no.3 train no.249140  loss = 3.38317 avg_loss = 3.32304\n",
      "epoch no.3 train no.249150  loss = 3.02308 avg_loss = 3.33293\n",
      "epoch no.3 train no.249160  loss = 2.04174 avg_loss = 3.28422\n",
      "epoch no.3 train no.249170  loss = 2.04441 avg_loss = 3.25210\n",
      "epoch no.3 train no.249180  loss = 2.95156 avg_loss = 3.27006\n",
      "epoch no.3 train no.249190  loss = 2.22242 avg_loss = 3.22660\n",
      "epoch no.3 train no.249200  loss = 3.93975 avg_loss = 3.30406\n",
      "epoch no.3 train no.249210  loss = 3.38917 avg_loss = 3.32046\n",
      "epoch no.3 train no.249220  loss = 3.25615 avg_loss = 3.32753\n",
      "epoch no.3 train no.249230  loss = 3.00276 avg_loss = 3.30250\n",
      "epoch no.3 train no.249240  loss = 2.95246 avg_loss = 3.37777\n",
      "epoch no.3 train no.249250  loss = 3.04431 avg_loss = 3.36181\n",
      "epoch no.3 train no.249260  loss = 3.83505 avg_loss = 3.35462\n",
      "epoch no.3 train no.249270  loss = 2.90662 avg_loss = 3.30111\n",
      "epoch no.3 train no.249280  loss = 3.56611 avg_loss = 3.31740\n",
      "epoch no.3 train no.249290  loss = 1.92793 avg_loss = 3.32780\n",
      "epoch no.3 train no.249300  loss = 2.13991 avg_loss = 3.31281\n",
      "epoch no.3 train no.249310  loss = 2.87417 avg_loss = 3.33572\n",
      "epoch no.3 train no.249320  loss = 4.39805 avg_loss = 3.34758\n",
      "epoch no.3 train no.249330  loss = 3.23137 avg_loss = 3.31414\n",
      "epoch no.3 train no.249340  loss = 4.13959 avg_loss = 3.29567\n",
      "epoch no.3 train no.249350  loss = 4.67230 avg_loss = 3.29763\n",
      "epoch no.3 train no.249360  loss = 3.33899 avg_loss = 3.28738\n",
      "epoch no.3 train no.249370  loss = 3.11192 avg_loss = 3.27544\n",
      "epoch no.3 train no.249380  loss = 3.58109 avg_loss = 3.30235\n",
      "epoch no.3 train no.249390  loss = 2.95215 avg_loss = 3.34689\n",
      "epoch no.3 train no.249400  loss = 2.82547 avg_loss = 3.34819\n",
      "epoch no.3 train no.249410  loss = 3.46903 avg_loss = 3.34393\n",
      "epoch no.3 train no.249420  loss = 4.24482 avg_loss = 3.32983\n",
      "epoch no.3 train no.249430  loss = 4.09198 avg_loss = 3.31805\n",
      "epoch no.3 train no.249440  loss = 3.28605 avg_loss = 3.33813\n",
      "epoch no.3 train no.249450  loss = 4.03366 avg_loss = 3.33566\n",
      "epoch no.3 train no.249460  loss = 3.71407 avg_loss = 3.29187\n",
      "epoch no.3 train no.249470  loss = 4.75325 avg_loss = 3.33441\n",
      "epoch no.3 train no.249480  loss = 5.39758 avg_loss = 3.37822\n",
      "epoch no.3 train no.249490  loss = 1.88891 avg_loss = 3.31781\n",
      "epoch no.3 train no.249500  loss = 2.74901 avg_loss = 3.29909\n",
      "epoch no.3 train no.249510  loss = 2.52554 avg_loss = 3.27078\n",
      "epoch no.3 train no.249520  loss = 5.04617 avg_loss = 3.36215\n",
      "epoch no.3 train no.249530  loss = 4.12926 avg_loss = 3.36354\n",
      "epoch no.3 train no.249540  loss = 2.56269 avg_loss = 3.31788\n",
      "epoch no.3 train no.249550  loss = 3.14334 avg_loss = 3.29314\n",
      "epoch no.3 train no.249560  loss = 3.37245 avg_loss = 3.28179\n",
      "epoch no.3 train no.249570  loss = 3.08755 avg_loss = 3.26829\n",
      "epoch no.3 train no.249580  loss = 2.46607 avg_loss = 3.23124\n",
      "epoch no.3 train no.249590  loss = 3.36679 avg_loss = 3.24332\n",
      "epoch no.3 train no.249600  loss = 4.23628 avg_loss = 3.27760\n",
      "epoch no.3 train no.249610  loss = 3.69315 avg_loss = 3.27971\n",
      "epoch no.3 train no.249620  loss = 4.84768 avg_loss = 3.30294\n",
      "epoch no.3 train no.249630  loss = 2.43548 avg_loss = 3.26425\n",
      "epoch no.3 train no.249640  loss = 4.38727 avg_loss = 3.29812\n",
      "epoch no.3 train no.249650  loss = 2.71058 avg_loss = 3.30625\n",
      "epoch no.3 train no.249660  loss = 2.94721 avg_loss = 3.28992\n",
      "epoch no.3 train no.249670  loss = 4.26550 avg_loss = 3.29916\n",
      "epoch no.3 train no.249680  loss = 2.66848 avg_loss = 3.25671\n",
      "epoch no.3 train no.249690  loss = 4.93785 avg_loss = 3.29007\n",
      "epoch no.3 train no.249700  loss = 3.68062 avg_loss = 3.28517\n",
      "epoch no.3 train no.249710  loss = 2.87551 avg_loss = 3.32285\n",
      "epoch no.3 train no.249720  loss = 3.40707 avg_loss = 3.31615\n",
      "epoch no.3 train no.249730  loss = 3.76473 avg_loss = 3.29740\n",
      "epoch no.3 train no.249740  loss = 2.84804 avg_loss = 3.26304\n",
      "epoch no.3 train no.249750  loss = 3.46306 avg_loss = 3.25256\n",
      "epoch no.3 train no.249760  loss = 2.77051 avg_loss = 3.22596\n",
      "epoch no.3 train no.249770  loss = 2.67392 avg_loss = 3.24261\n",
      "epoch no.3 train no.249780  loss = 3.62156 avg_loss = 3.23078\n",
      "epoch no.3 train no.249790  loss = 1.60406 avg_loss = 3.22838\n",
      "epoch no.3 train no.249800  loss = 2.26669 avg_loss = 3.20720\n",
      "epoch no.3 train no.249810  loss = 4.33688 avg_loss = 3.24489\n",
      "epoch no.3 train no.249820  loss = 3.11632 avg_loss = 3.26256\n",
      "epoch no.3 train no.249830  loss = 2.65116 avg_loss = 3.26237\n",
      "epoch no.3 train no.249840  loss = 3.45298 avg_loss = 3.29869\n",
      "epoch no.3 train no.249850  loss = 2.79404 avg_loss = 3.36282\n",
      "epoch no.3 train no.249860  loss = 2.27493 avg_loss = 3.35752\n",
      "epoch no.3 train no.249870  loss = 3.50676 avg_loss = 3.35473\n",
      "epoch no.3 train no.249880  loss = 3.74240 avg_loss = 3.33582\n",
      "epoch no.3 train no.249890  loss = 3.61317 avg_loss = 3.37908\n",
      "epoch no.3 train no.249900  loss = 2.64751 avg_loss = 3.33570\n",
      "epoch no.3 train no.249910  loss = 3.07706 avg_loss = 3.33130\n",
      "epoch no.3 train no.249920  loss = 5.84031 avg_loss = 3.36960\n",
      "epoch no.3 train no.249930  loss = 2.56295 avg_loss = 3.38213\n",
      "epoch no.3 train no.249940  loss = 6.71027 avg_loss = 3.41094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.249950  loss = 2.60106 avg_loss = 3.38597\n",
      "epoch no.3 train no.249960  loss = 2.80254 avg_loss = 3.34892\n",
      "epoch no.3 train no.249970  loss = 1.48343 avg_loss = 3.28934\n",
      "epoch no.3 train no.249980  loss = 1.98479 avg_loss = 3.23685\n",
      "epoch no.3 train no.249990  loss = 4.02377 avg_loss = 3.22230\n",
      "epoch no.3 train no.250000  loss = 3.04889 avg_loss = 3.23917\n",
      "4\n",
      "to_tokens: ['▁가을', '▁올드', '곡', '들', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.3 train no.250010  loss = 1.97293 avg_loss = 3.22412\n",
      "epoch no.3 train no.250020  loss = 2.81024 avg_loss = 3.25944\n",
      "epoch no.3 train no.250030  loss = 2.34262 avg_loss = 3.30647\n",
      "epoch no.3 train no.250040  loss = 2.54855 avg_loss = 3.29864\n",
      "epoch no.3 train no.250050  loss = 4.02118 avg_loss = 3.36153\n",
      "epoch no.3 train no.250060  loss = 4.91891 avg_loss = 3.33166\n",
      "epoch no.3 train no.250070  loss = 3.68213 avg_loss = 3.33229\n",
      "epoch no.3 train no.250080  loss = 2.38109 avg_loss = 3.32160\n",
      "epoch no.3 train no.250090  loss = 2.36935 avg_loss = 3.35996\n",
      "epoch no.3 train no.250100  loss = 3.26016 avg_loss = 3.37035\n",
      "epoch no.3 train no.250110  loss = 3.54920 avg_loss = 3.35947\n",
      "epoch no.3 train no.250120  loss = 2.56863 avg_loss = 3.32799\n",
      "epoch no.3 train no.250130  loss = 3.33030 avg_loss = 3.29965\n",
      "epoch no.3 train no.250140  loss = 3.50890 avg_loss = 3.27706\n",
      "epoch no.3 train no.250150  loss = 3.53113 avg_loss = 3.28196\n",
      "epoch no.3 train no.250160  loss = 2.43088 avg_loss = 3.25243\n",
      "epoch no.3 train no.250170  loss = 1.64557 avg_loss = 3.19547\n",
      "epoch no.3 train no.250180  loss = 2.18516 avg_loss = 3.25254\n",
      "epoch no.3 train no.250190  loss = 4.75527 avg_loss = 3.26308\n",
      "epoch no.3 train no.250200  loss = 3.24441 avg_loss = 3.25904\n",
      "epoch no.3 train no.250210  loss = 3.19946 avg_loss = 3.23577\n",
      "epoch no.3 train no.250220  loss = 3.84418 avg_loss = 3.26215\n",
      "epoch no.3 train no.250230  loss = 3.13911 avg_loss = 3.27558\n",
      "epoch no.3 train no.250240  loss = 2.47891 avg_loss = 3.24199\n",
      "epoch no.3 train no.250250  loss = 2.32876 avg_loss = 3.22943\n",
      "epoch no.3 train no.250260  loss = 2.42792 avg_loss = 3.22329\n",
      "epoch no.3 train no.250270  loss = 3.30847 avg_loss = 3.28828\n",
      "epoch no.3 train no.250280  loss = 2.83822 avg_loss = 3.32897\n",
      "epoch no.3 train no.250290  loss = 2.33858 avg_loss = 3.35141\n",
      "epoch no.3 train no.250300  loss = 3.59294 avg_loss = 3.34775\n",
      "epoch no.3 train no.250310  loss = 1.43661 avg_loss = 3.32371\n",
      "epoch no.3 train no.250320  loss = 3.49491 avg_loss = 3.35588\n",
      "epoch no.3 train no.250330  loss = 4.12403 avg_loss = 3.36633\n",
      "epoch no.3 train no.250340  loss = 3.68723 avg_loss = 3.36945\n",
      "epoch no.3 train no.250350  loss = 2.52978 avg_loss = 3.39213\n",
      "epoch no.3 train no.250360  loss = 2.38506 avg_loss = 3.37105\n",
      "epoch no.3 train no.250370  loss = 2.08330 avg_loss = 3.37232\n",
      "epoch no.3 train no.250380  loss = 3.63760 avg_loss = 3.38416\n",
      "epoch no.3 train no.250390  loss = 1.78091 avg_loss = 3.37827\n",
      "epoch no.3 train no.250400  loss = 2.17476 avg_loss = 3.38014\n",
      "epoch no.3 train no.250410  loss = 2.05479 avg_loss = 3.36736\n",
      "epoch no.3 train no.250420  loss = 3.05682 avg_loss = 3.31520\n",
      "epoch no.3 train no.250430  loss = 4.33062 avg_loss = 3.35242\n",
      "epoch no.3 train no.250440  loss = 4.17645 avg_loss = 3.32761\n",
      "epoch no.3 train no.250450  loss = 2.78770 avg_loss = 3.33287\n",
      "epoch no.3 train no.250460  loss = 3.13650 avg_loss = 3.30648\n",
      "epoch no.3 train no.250470  loss = 2.10371 avg_loss = 3.28843\n",
      "epoch no.3 train no.250480  loss = 2.60811 avg_loss = 3.29749\n",
      "epoch no.3 train no.250490  loss = 3.38942 avg_loss = 3.29714\n",
      "epoch no.3 train no.250500  loss = 3.75605 avg_loss = 3.29222\n",
      "epoch no.3 train no.250510  loss = 2.31789 avg_loss = 3.27403\n",
      "epoch no.3 train no.250520  loss = 3.74888 avg_loss = 3.27297\n",
      "epoch no.3 train no.250530  loss = 2.40483 avg_loss = 3.28019\n",
      "epoch no.3 train no.250540  loss = 4.64650 avg_loss = 3.32102\n",
      "epoch no.3 train no.250550  loss = 4.11550 avg_loss = 3.37084\n",
      "epoch no.3 train no.250560  loss = 2.79704 avg_loss = 3.37309\n",
      "epoch no.3 train no.250570  loss = 3.68881 avg_loss = 3.37375\n",
      "epoch no.3 train no.250580  loss = 2.35958 avg_loss = 3.33075\n",
      "epoch no.3 train no.250590  loss = 3.49577 avg_loss = 3.29823\n",
      "epoch no.3 train no.250600  loss = 2.10681 avg_loss = 3.27147\n",
      "epoch no.3 train no.250610  loss = 2.13836 avg_loss = 3.23301\n",
      "epoch no.3 train no.250620  loss = 3.32807 avg_loss = 3.26813\n",
      "epoch no.3 train no.250630  loss = 3.01974 avg_loss = 3.23068\n",
      "epoch no.3 train no.250640  loss = 4.31900 avg_loss = 3.25442\n",
      "epoch no.3 train no.250650  loss = 3.23023 avg_loss = 3.27818\n",
      "epoch no.3 train no.250660  loss = 2.90131 avg_loss = 3.30722\n",
      "epoch no.3 train no.250670  loss = 2.28913 avg_loss = 3.23225\n",
      "epoch no.3 train no.250680  loss = 2.35254 avg_loss = 3.25549\n",
      "epoch no.3 train no.250690  loss = 2.93603 avg_loss = 3.28919\n",
      "epoch no.3 train no.250700  loss = 3.67832 avg_loss = 3.26635\n",
      "epoch no.3 train no.250710  loss = 4.64416 avg_loss = 3.30388\n",
      "epoch no.3 train no.250720  loss = 3.90126 avg_loss = 3.34651\n",
      "epoch no.3 train no.250730  loss = 3.09975 avg_loss = 3.36105\n",
      "epoch no.3 train no.250740  loss = 5.01355 avg_loss = 3.36671\n",
      "epoch no.3 train no.250750  loss = 2.29872 avg_loss = 3.36965\n",
      "epoch no.3 train no.250760  loss = 5.62254 avg_loss = 3.36624\n",
      "epoch no.3 train no.250770  loss = 3.34035 avg_loss = 3.34485\n",
      "epoch no.3 train no.250780  loss = 3.29428 avg_loss = 3.32974\n",
      "epoch no.3 train no.250790  loss = 3.09362 avg_loss = 3.31658\n",
      "epoch no.3 train no.250800  loss = 3.19991 avg_loss = 3.29019\n",
      "epoch no.3 train no.250810  loss = 2.73690 avg_loss = 3.29198\n",
      "epoch no.3 train no.250820  loss = 2.71311 avg_loss = 3.30946\n",
      "epoch no.3 train no.250830  loss = 5.00178 avg_loss = 3.28963\n",
      "epoch no.3 train no.250840  loss = 4.85260 avg_loss = 3.27333\n",
      "epoch no.3 train no.250850  loss = 2.95892 avg_loss = 3.29227\n",
      "epoch no.3 train no.250860  loss = 4.74409 avg_loss = 3.36027\n",
      "epoch no.3 train no.250870  loss = 2.23632 avg_loss = 3.34830\n",
      "epoch no.3 train no.250880  loss = 3.54945 avg_loss = 3.32610\n",
      "epoch no.3 train no.250890  loss = 1.37823 avg_loss = 3.29460\n",
      "epoch no.3 train no.250900  loss = 3.36598 avg_loss = 3.31371\n",
      "epoch no.3 train no.250910  loss = 2.74298 avg_loss = 3.26651\n",
      "epoch no.3 train no.250920  loss = 1.86693 avg_loss = 3.28703\n",
      "epoch no.3 train no.250930  loss = 2.76204 avg_loss = 3.29373\n",
      "epoch no.3 train no.250940  loss = 2.27048 avg_loss = 3.27565\n",
      "epoch no.3 train no.250950  loss = 2.21330 avg_loss = 3.26944\n",
      "epoch no.3 train no.250960  loss = 2.29697 avg_loss = 3.28712\n",
      "epoch no.3 train no.250970  loss = 3.72513 avg_loss = 3.25432\n",
      "epoch no.3 train no.250980  loss = 2.89411 avg_loss = 3.30295\n",
      "epoch no.3 train no.250990  loss = 2.58369 avg_loss = 3.29683\n",
      "epoch no.3 train no.251000  loss = 4.69258 avg_loss = 3.31839\n",
      "6\n",
      "to_tokens: ['▁가을', '▁올드', '▁o', 'st', '▁모음', '곡', '</s>', '</s>']\n",
      "추억의 드라마 ost 명곡 모음</s>\n",
      "epoch no.3 train no.251010  loss = 3.42246 avg_loss = 3.29178\n",
      "epoch no.3 train no.251020  loss = 2.71498 avg_loss = 3.32176\n",
      "epoch no.3 train no.251030  loss = 2.84048 avg_loss = 3.33170\n",
      "epoch no.3 train no.251040  loss = 1.90579 avg_loss = 3.27648\n",
      "epoch no.3 train no.251050  loss = 4.55694 avg_loss = 3.31618\n",
      "epoch no.3 train no.251060  loss = 4.38746 avg_loss = 3.35697\n",
      "epoch no.3 train no.251070  loss = 3.83556 avg_loss = 3.34570\n",
      "epoch no.3 train no.251080  loss = 2.59962 avg_loss = 3.30515\n",
      "epoch no.3 train no.251090  loss = 3.48832 avg_loss = 3.29021\n",
      "epoch no.3 train no.251100  loss = 2.11485 avg_loss = 3.22327\n",
      "epoch no.3 train no.251110  loss = 3.52608 avg_loss = 3.24523\n",
      "epoch no.3 train no.251120  loss = 1.92227 avg_loss = 3.22784\n",
      "epoch no.3 train no.251130  loss = 4.58990 avg_loss = 3.21696\n",
      "epoch no.3 train no.251140  loss = 4.27677 avg_loss = 3.18071\n",
      "epoch no.3 train no.251150  loss = 3.87123 avg_loss = 3.17353\n",
      "epoch no.3 train no.251160  loss = 2.91605 avg_loss = 3.16704\n",
      "epoch no.3 train no.251170  loss = 3.78855 avg_loss = 3.18238\n",
      "epoch no.3 train no.251180  loss = 3.74498 avg_loss = 3.17842\n",
      "epoch no.3 train no.251190  loss = 2.17509 avg_loss = 3.20752\n",
      "epoch no.3 train no.251200  loss = 5.14298 avg_loss = 3.21567\n",
      "epoch no.3 train no.251210  loss = 4.12954 avg_loss = 3.22686\n",
      "epoch no.3 train no.251220  loss = 4.61699 avg_loss = 3.20902\n",
      "epoch no.3 train no.251230  loss = 4.07154 avg_loss = 3.18482\n",
      "epoch no.3 train no.251240  loss = 3.61345 avg_loss = 3.24676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.251250  loss = 3.22182 avg_loss = 3.22463\n",
      "epoch no.3 train no.251260  loss = 4.51619 avg_loss = 3.27372\n",
      "epoch no.3 train no.251270  loss = 2.28958 avg_loss = 3.31576\n",
      "epoch no.3 train no.251280  loss = 2.65190 avg_loss = 3.32000\n",
      "epoch no.3 train no.251290  loss = 2.74442 avg_loss = 3.29814\n",
      "epoch no.3 train no.251300  loss = 2.93902 avg_loss = 3.26674\n",
      "epoch no.3 train no.251310  loss = 3.92457 avg_loss = 3.27075\n",
      "epoch no.3 train no.251320  loss = 2.35030 avg_loss = 3.28060\n",
      "epoch no.3 train no.251330  loss = 4.70573 avg_loss = 3.28917\n",
      "epoch no.3 train no.251340  loss = 3.08289 avg_loss = 3.34181\n",
      "epoch no.3 train no.251350  loss = 5.11937 avg_loss = 3.34569\n",
      "epoch no.3 train no.251360  loss = 3.83688 avg_loss = 3.31663\n",
      "epoch no.3 train no.251370  loss = 3.34702 avg_loss = 3.31620\n",
      "epoch no.3 train no.251380  loss = 2.36232 avg_loss = 3.29246\n",
      "epoch no.3 train no.251390  loss = 3.04825 avg_loss = 3.32245\n",
      "epoch no.3 train no.251400  loss = 2.55126 avg_loss = 3.34576\n",
      "epoch no.3 train no.251410  loss = 2.70822 avg_loss = 3.35667\n",
      "epoch no.3 train no.251420  loss = 2.75872 avg_loss = 3.31574\n",
      "epoch no.3 train no.251430  loss = 1.91560 avg_loss = 3.30577\n",
      "epoch no.3 train no.251440  loss = 2.53667 avg_loss = 3.28931\n",
      "epoch no.3 train no.251450  loss = 3.62254 avg_loss = 3.29677\n",
      "epoch no.3 train no.251460  loss = 2.95356 avg_loss = 3.28897\n",
      "epoch no.3 train no.251470  loss = 3.49748 avg_loss = 3.27812\n",
      "epoch no.3 train no.251480  loss = 4.96948 avg_loss = 3.31170\n",
      "epoch no.3 train no.251490  loss = 3.00067 avg_loss = 3.30184\n",
      "epoch no.3 train no.251500  loss = 2.52119 avg_loss = 3.24930\n",
      "epoch no.3 train no.251510  loss = 3.39324 avg_loss = 3.23921\n",
      "epoch no.3 train no.251520  loss = 3.21509 avg_loss = 3.19954\n",
      "epoch no.3 train no.251530  loss = 5.19070 avg_loss = 3.23491\n",
      "epoch no.3 train no.251540  loss = 2.77608 avg_loss = 3.24543\n",
      "epoch no.3 train no.251550  loss = 4.56113 avg_loss = 3.25484\n",
      "epoch no.3 train no.251560  loss = 2.02729 avg_loss = 3.26942\n",
      "epoch no.3 train no.251570  loss = 3.74130 avg_loss = 3.30852\n",
      "epoch no.3 train no.251580  loss = 3.07376 avg_loss = 3.33073\n",
      "epoch no.3 train no.251590  loss = 3.94013 avg_loss = 3.31511\n",
      "epoch no.3 train no.251600  loss = 2.60286 avg_loss = 3.29757\n",
      "epoch no.3 train no.251610  loss = 3.21155 avg_loss = 3.31022\n",
      "epoch no.3 train no.251620  loss = 1.58970 avg_loss = 3.32160\n",
      "epoch no.3 train no.251630  loss = 2.84838 avg_loss = 3.32993\n",
      "epoch no.3 train no.251640  loss = 3.33591 avg_loss = 3.32900\n",
      "epoch no.3 train no.251650  loss = 3.13797 avg_loss = 3.36738\n",
      "epoch no.3 train no.251660  loss = 3.35947 avg_loss = 3.33080\n",
      "epoch no.3 train no.251670  loss = 2.38356 avg_loss = 3.32709\n",
      "epoch no.3 train no.251680  loss = 4.85147 avg_loss = 3.38097\n",
      "epoch no.3 train no.251690  loss = 5.34794 avg_loss = 3.37942\n",
      "epoch no.3 train no.251700  loss = 4.57296 avg_loss = 3.39373\n",
      "epoch no.3 train no.251710  loss = 2.81232 avg_loss = 3.39778\n",
      "epoch no.3 train no.251720  loss = 4.14479 avg_loss = 3.36545\n",
      "epoch no.3 train no.251730  loss = 4.02742 avg_loss = 3.35174\n",
      "epoch no.3 train no.251740  loss = 3.55043 avg_loss = 3.36002\n",
      "epoch no.3 train no.251750  loss = 3.40682 avg_loss = 3.33378\n",
      "epoch no.3 train no.251760  loss = 2.12185 avg_loss = 3.32702\n",
      "epoch no.3 train no.251770  loss = 2.31439 avg_loss = 3.33884\n",
      "epoch no.3 train no.251780  loss = 4.16661 avg_loss = 3.33258\n",
      "epoch no.3 train no.251790  loss = 3.28045 avg_loss = 3.37397\n",
      "epoch no.3 train no.251800  loss = 4.50410 avg_loss = 3.38298\n",
      "epoch no.3 train no.251810  loss = 3.24515 avg_loss = 3.37588\n",
      "epoch no.3 train no.251820  loss = 1.62902 avg_loss = 3.30447\n",
      "epoch no.3 train no.251830  loss = 1.60048 avg_loss = 3.29987\n",
      "epoch no.3 train no.251840  loss = 4.15955 avg_loss = 3.36860\n",
      "epoch no.3 train no.251850  loss = 2.81639 avg_loss = 3.32612\n",
      "epoch no.3 train no.251860  loss = 3.04472 avg_loss = 3.31710\n",
      "epoch no.3 train no.251870  loss = 4.50531 avg_loss = 3.33265\n",
      "epoch no.3 train no.251880  loss = 2.62328 avg_loss = 3.34887\n",
      "epoch no.3 train no.251890  loss = 2.95743 avg_loss = 3.34812\n",
      "epoch no.3 train no.251900  loss = 1.88640 avg_loss = 3.27236\n",
      "epoch no.3 train no.251910  loss = 5.87561 avg_loss = 3.31425\n",
      "epoch no.3 train no.251920  loss = 4.19053 avg_loss = 3.34839\n",
      "epoch no.3 train no.251930  loss = 2.77815 avg_loss = 3.38289\n",
      "epoch no.3 train no.251940  loss = 3.12246 avg_loss = 3.36745\n",
      "epoch no.3 train no.251950  loss = 2.37125 avg_loss = 3.33860\n",
      "epoch no.3 train no.251960  loss = 3.54618 avg_loss = 3.33981\n",
      "epoch no.3 train no.251970  loss = 2.42568 avg_loss = 3.36057\n",
      "epoch no.3 train no.251980  loss = 2.30250 avg_loss = 3.30223\n",
      "epoch no.3 train no.251990  loss = 2.97757 avg_loss = 3.27689\n",
      "epoch no.3 train no.252000  loss = 3.61013 avg_loss = 3.32654\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '팝', '▁명', '▁모음', '</s>']\n",
      "추억의 올드팝송 모음</s>\n",
      "epoch no.3 train no.252010  loss = 2.61044 avg_loss = 3.30477\n",
      "epoch no.3 train no.252020  loss = 3.79448 avg_loss = 3.31226\n",
      "epoch no.3 train no.252030  loss = 4.54431 avg_loss = 3.35441\n",
      "epoch no.3 train no.252040  loss = 5.39302 avg_loss = 3.35927\n",
      "epoch no.3 train no.252050  loss = 5.46708 avg_loss = 3.38489\n",
      "epoch no.3 train no.252060  loss = 2.14768 avg_loss = 3.34062\n",
      "epoch no.3 train no.252070  loss = 2.66896 avg_loss = 3.30756\n",
      "epoch no.3 train no.252080  loss = 4.10437 avg_loss = 3.26769\n",
      "epoch no.3 train no.252090  loss = 3.75868 avg_loss = 3.28582\n",
      "epoch no.3 train no.252100  loss = 3.89246 avg_loss = 3.28365\n",
      "epoch no.3 train no.252110  loss = 3.62307 avg_loss = 3.27282\n",
      "epoch no.3 train no.252120  loss = 3.37315 avg_loss = 3.25487\n",
      "epoch no.3 train no.252130  loss = 1.88222 avg_loss = 3.19998\n",
      "epoch no.3 train no.252140  loss = 4.43547 avg_loss = 3.21217\n",
      "epoch no.3 train no.252150  loss = 5.17066 avg_loss = 3.24187\n",
      "epoch no.3 train no.252160  loss = 3.93540 avg_loss = 3.23421\n",
      "epoch no.3 train no.252170  loss = 2.71314 avg_loss = 3.19978\n",
      "epoch no.3 train no.252180  loss = 4.46188 avg_loss = 3.24427\n",
      "epoch no.3 train no.252190  loss = 2.43928 avg_loss = 3.23997\n",
      "epoch no.3 train no.252200  loss = 4.40327 avg_loss = 3.25469\n",
      "epoch no.3 train no.252210  loss = 4.32323 avg_loss = 3.28229\n",
      "epoch no.3 train no.252220  loss = 1.94756 avg_loss = 3.27500\n",
      "epoch no.3 train no.252230  loss = 6.02735 avg_loss = 3.24551\n",
      "epoch no.3 train no.252240  loss = 2.40409 avg_loss = 3.20443\n",
      "epoch no.3 train no.252250  loss = 3.93395 avg_loss = 3.24993\n",
      "epoch no.3 train no.252260  loss = 4.93018 avg_loss = 3.31401\n",
      "epoch no.3 train no.252270  loss = 3.59019 avg_loss = 3.31697\n",
      "epoch no.3 train no.252280  loss = 2.96404 avg_loss = 3.29911\n",
      "epoch no.3 train no.252290  loss = 4.25955 avg_loss = 3.28400\n",
      "epoch no.3 train no.252300  loss = 1.86212 avg_loss = 3.27101\n",
      "epoch no.3 train no.252310  loss = 3.29416 avg_loss = 3.28466\n",
      "epoch no.3 train no.252320  loss = 2.08048 avg_loss = 3.31049\n",
      "epoch no.3 train no.252330  loss = 3.31235 avg_loss = 3.31176\n",
      "epoch no.3 train no.252340  loss = 2.31333 avg_loss = 3.29250\n",
      "epoch no.3 train no.252350  loss = 3.27125 avg_loss = 3.28522\n",
      "epoch no.3 train no.252360  loss = 3.36467 avg_loss = 3.25230\n",
      "epoch no.3 train no.252370  loss = 2.61506 avg_loss = 3.24465\n",
      "epoch no.3 train no.252380  loss = 2.40528 avg_loss = 3.21822\n",
      "epoch no.3 train no.252390  loss = 3.87786 avg_loss = 3.23659\n",
      "epoch no.3 train no.252400  loss = 2.48282 avg_loss = 3.24161\n",
      "epoch no.3 train no.252410  loss = 2.78653 avg_loss = 3.24779\n",
      "epoch no.3 train no.252420  loss = 2.56650 avg_loss = 3.20761\n",
      "epoch no.3 train no.252430  loss = 3.71257 avg_loss = 3.19092\n",
      "epoch no.3 train no.252440  loss = 3.60396 avg_loss = 3.25962\n",
      "epoch no.3 train no.252450  loss = 3.79271 avg_loss = 3.26197\n",
      "epoch no.3 train no.252460  loss = 2.80939 avg_loss = 3.25687\n",
      "epoch no.3 train no.252470  loss = 3.03260 avg_loss = 3.25871\n",
      "epoch no.3 train no.252480  loss = 2.67610 avg_loss = 3.24579\n",
      "epoch no.3 train no.252490  loss = 3.66494 avg_loss = 3.24721\n",
      "epoch no.3 train no.252500  loss = 3.10965 avg_loss = 3.33478\n",
      "epoch no.3 train no.252510  loss = 2.63784 avg_loss = 3.28192\n",
      "epoch no.3 train no.252520  loss = 1.70250 avg_loss = 3.29233\n",
      "epoch no.3 train no.252530  loss = 3.67838 avg_loss = 3.31043\n",
      "epoch no.3 train no.252540  loss = 2.89256 avg_loss = 3.29628\n",
      "epoch no.3 train no.252550  loss = 1.89035 avg_loss = 3.35612\n",
      "epoch no.3 train no.252560  loss = 2.81804 avg_loss = 3.32320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.252570  loss = 5.38229 avg_loss = 3.28137\n",
      "epoch no.3 train no.252580  loss = 3.83204 avg_loss = 3.31312\n",
      "epoch no.3 train no.252590  loss = 2.25149 avg_loss = 3.30950\n",
      "epoch no.3 train no.252600  loss = 3.11400 avg_loss = 3.29126\n",
      "epoch no.3 train no.252610  loss = 2.66329 avg_loss = 3.27185\n",
      "epoch no.3 train no.252620  loss = 3.61415 avg_loss = 3.25586\n",
      "epoch no.3 train no.252630  loss = 5.33980 avg_loss = 3.26283\n",
      "epoch no.3 train no.252640  loss = 3.04455 avg_loss = 3.25334\n",
      "epoch no.3 train no.252650  loss = 2.82932 avg_loss = 3.28482\n",
      "epoch no.3 train no.252660  loss = 4.15724 avg_loss = 3.33345\n",
      "epoch no.3 train no.252670  loss = 3.23653 avg_loss = 3.30634\n",
      "epoch no.3 train no.252680  loss = 2.90686 avg_loss = 3.32196\n",
      "epoch no.3 train no.252690  loss = 3.04392 avg_loss = 3.29225\n",
      "epoch no.3 train no.252700  loss = 3.08414 avg_loss = 3.24159\n",
      "epoch no.3 train no.252710  loss = 3.52099 avg_loss = 3.22965\n",
      "epoch no.3 train no.252720  loss = 3.31908 avg_loss = 3.22861\n",
      "epoch no.3 train no.252730  loss = 3.83100 avg_loss = 3.23785\n",
      "epoch no.3 train no.252740  loss = 3.80835 avg_loss = 3.27901\n",
      "epoch no.3 train no.252750  loss = 3.98516 avg_loss = 3.26919\n",
      "epoch no.3 train no.252760  loss = 3.03615 avg_loss = 3.29149\n",
      "epoch no.3 train no.252770  loss = 2.57493 avg_loss = 3.26297\n",
      "epoch no.3 train no.252780  loss = 3.23312 avg_loss = 3.27567\n",
      "epoch no.3 train no.252790  loss = 3.37624 avg_loss = 3.29232\n",
      "epoch no.3 train no.252800  loss = 2.36586 avg_loss = 3.33046\n",
      "epoch no.3 train no.252810  loss = 4.49516 avg_loss = 3.33884\n",
      "epoch no.3 train no.252820  loss = 3.65619 avg_loss = 3.41675\n",
      "epoch no.3 train no.252830  loss = 3.37239 avg_loss = 3.42108\n",
      "epoch no.3 train no.252840  loss = 4.74279 avg_loss = 3.42186\n",
      "epoch no.3 train no.252850  loss = 2.55399 avg_loss = 3.38235\n",
      "epoch no.3 train no.252860  loss = 2.21642 avg_loss = 3.44674\n",
      "epoch no.3 train no.252870  loss = 3.72949 avg_loss = 3.44907\n",
      "epoch no.3 train no.252880  loss = 2.44736 avg_loss = 3.44070\n",
      "epoch no.3 train no.252890  loss = 4.07008 avg_loss = 3.51533\n",
      "epoch no.3 train no.252900  loss = 2.90101 avg_loss = 3.51930\n",
      "epoch no.3 train no.252910  loss = 4.15256 avg_loss = 3.47602\n",
      "epoch no.3 train no.252920  loss = 3.85460 avg_loss = 3.48528\n",
      "epoch no.3 train no.252930  loss = 2.26608 avg_loss = 3.48877\n",
      "epoch no.3 train no.252940  loss = 3.10344 avg_loss = 3.51332\n",
      "epoch no.3 train no.252950  loss = 3.89926 avg_loss = 3.53522\n",
      "epoch no.3 train no.252960  loss = 4.03432 avg_loss = 3.48504\n",
      "epoch no.3 train no.252970  loss = 2.42362 avg_loss = 3.44801\n",
      "epoch no.3 train no.252980  loss = 2.29373 avg_loss = 3.40526\n",
      "epoch no.3 train no.252990  loss = 2.46620 avg_loss = 3.39985\n",
      "epoch no.3 train no.253000  loss = 1.79110 avg_loss = 3.36527\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.3 train no.253010  loss = 4.07473 avg_loss = 3.37383\n",
      "epoch no.3 train no.253020  loss = 3.34038 avg_loss = 3.34298\n",
      "epoch no.3 train no.253030  loss = 3.81032 avg_loss = 3.32688\n",
      "epoch no.3 train no.253040  loss = 3.07550 avg_loss = 3.31630\n",
      "epoch no.3 train no.253050  loss = 2.81599 avg_loss = 3.33208\n",
      "epoch no.3 train no.253060  loss = 2.44050 avg_loss = 3.30785\n",
      "epoch no.3 train no.253070  loss = 3.62881 avg_loss = 3.28277\n",
      "epoch no.3 train no.253080  loss = 2.76646 avg_loss = 3.26912\n",
      "epoch no.3 train no.253090  loss = 4.60532 avg_loss = 3.30138\n",
      "epoch no.3 train no.253100  loss = 2.85987 avg_loss = 3.29571\n",
      "epoch no.3 train no.253110  loss = 3.47389 avg_loss = 3.30661\n",
      "epoch no.3 train no.253120  loss = 2.05741 avg_loss = 3.27392\n",
      "epoch no.3 train no.253130  loss = 3.67544 avg_loss = 3.27159\n",
      "epoch no.3 train no.253140  loss = 3.38567 avg_loss = 3.29063\n",
      "epoch no.3 train no.253150  loss = 2.76043 avg_loss = 3.32853\n",
      "epoch no.3 train no.253160  loss = 2.27949 avg_loss = 3.28906\n",
      "epoch no.3 train no.253170  loss = 2.99517 avg_loss = 3.33001\n",
      "epoch no.3 train no.253180  loss = 2.27587 avg_loss = 3.30689\n",
      "epoch no.3 train no.253190  loss = 3.71408 avg_loss = 3.29758\n",
      "epoch no.3 train no.253200  loss = 2.49359 avg_loss = 3.26261\n",
      "epoch no.3 train no.253210  loss = 1.48633 avg_loss = 3.22217\n",
      "epoch no.3 train no.253220  loss = 3.84718 avg_loss = 3.25361\n",
      "epoch no.3 train no.253230  loss = 1.87705 avg_loss = 3.23785\n",
      "epoch no.3 train no.253240  loss = 2.65127 avg_loss = 3.22946\n",
      "epoch no.3 train no.253250  loss = 3.08282 avg_loss = 3.25922\n",
      "epoch no.3 train no.253260  loss = 3.78835 avg_loss = 3.30579\n",
      "epoch no.3 train no.253270  loss = 2.82849 avg_loss = 3.29834\n",
      "epoch no.3 train no.253280  loss = 2.66660 avg_loss = 3.27381\n",
      "epoch no.3 train no.253290  loss = 2.45440 avg_loss = 3.25001\n",
      "epoch no.3 train no.253300  loss = 3.87260 avg_loss = 3.26779\n",
      "epoch no.3 train no.253310  loss = 2.14035 avg_loss = 3.28962\n",
      "epoch no.3 train no.253320  loss = 3.92916 avg_loss = 3.28018\n",
      "epoch no.3 train no.253330  loss = 3.29402 avg_loss = 3.30627\n",
      "epoch no.3 train no.253340  loss = 2.92413 avg_loss = 3.25899\n",
      "epoch no.3 train no.253350  loss = 3.55814 avg_loss = 3.24525\n",
      "epoch no.3 train no.253360  loss = 2.87619 avg_loss = 3.23350\n",
      "epoch no.3 train no.253370  loss = 3.31646 avg_loss = 3.25211\n",
      "epoch no.3 train no.253380  loss = 3.03686 avg_loss = 3.28785\n",
      "epoch no.3 train no.253390  loss = 3.82241 avg_loss = 3.26192\n",
      "epoch no.3 train no.253400  loss = 2.51350 avg_loss = 3.28215\n",
      "epoch no.3 train no.253410  loss = 3.58075 avg_loss = 3.32101\n",
      "epoch no.3 train no.253420  loss = 3.06441 avg_loss = 3.27251\n",
      "epoch no.3 train no.253430  loss = 4.80005 avg_loss = 3.32305\n",
      "epoch no.3 train no.253440  loss = 3.06783 avg_loss = 3.29281\n",
      "epoch no.3 train no.253450  loss = 3.77476 avg_loss = 3.33579\n",
      "epoch no.3 train no.253460  loss = 2.52696 avg_loss = 3.31927\n",
      "epoch no.3 train no.253470  loss = 3.69419 avg_loss = 3.33319\n",
      "epoch no.3 train no.253480  loss = 2.94527 avg_loss = 3.33870\n",
      "epoch no.3 train no.253490  loss = 3.99030 avg_loss = 3.33673\n",
      "epoch no.3 train no.253500  loss = 3.79975 avg_loss = 3.31961\n",
      "epoch no.3 train no.253510  loss = 4.37142 avg_loss = 3.26493\n",
      "epoch no.3 train no.253520  loss = 4.27142 avg_loss = 3.30453\n",
      "epoch no.3 train no.253530  loss = 4.30377 avg_loss = 3.32738\n",
      "epoch no.3 train no.253540  loss = 4.00589 avg_loss = 3.35567\n",
      "epoch no.3 train no.253550  loss = 2.97365 avg_loss = 3.33703\n",
      "epoch no.3 train no.253560  loss = 5.83452 avg_loss = 3.30228\n",
      "epoch no.3 train no.253570  loss = 3.55881 avg_loss = 3.38787\n",
      "epoch no.3 train no.253580  loss = 3.46636 avg_loss = 3.38558\n",
      "epoch no.3 train no.253590  loss = 2.06341 avg_loss = 3.41982\n",
      "epoch no.3 train no.253600  loss = 3.85012 avg_loss = 3.40239\n",
      "epoch no.3 train no.253610  loss = 3.61235 avg_loss = 3.42806\n",
      "epoch no.3 train no.253620  loss = 3.27971 avg_loss = 3.42802\n",
      "epoch no.3 train no.253630  loss = 2.69896 avg_loss = 3.40356\n",
      "epoch no.3 train no.253640  loss = 2.18326 avg_loss = 3.37581\n",
      "epoch no.3 train no.253650  loss = 3.12213 avg_loss = 3.40597\n",
      "epoch no.3 train no.253660  loss = 3.30368 avg_loss = 3.37836\n",
      "epoch no.3 train no.253670  loss = 4.20891 avg_loss = 3.42748\n",
      "epoch no.3 train no.253680  loss = 2.63547 avg_loss = 3.41217\n",
      "epoch no.3 train no.253690  loss = 3.25050 avg_loss = 3.42436\n",
      "epoch no.3 train no.253700  loss = 2.13701 avg_loss = 3.44089\n",
      "epoch no.3 train no.253710  loss = 3.15540 avg_loss = 3.42673\n",
      "epoch no.3 train no.253720  loss = 2.67115 avg_loss = 3.41647\n",
      "epoch no.3 train no.253730  loss = 4.12217 avg_loss = 3.41498\n",
      "epoch no.3 train no.253740  loss = 1.86237 avg_loss = 3.43588\n",
      "epoch no.3 train no.253750  loss = 3.39407 avg_loss = 3.46743\n",
      "epoch no.3 train no.253760  loss = 3.58819 avg_loss = 3.48690\n",
      "epoch no.3 train no.253770  loss = 2.25316 avg_loss = 3.44221\n",
      "epoch no.3 train no.253780  loss = 4.04108 avg_loss = 3.40655\n",
      "epoch no.3 train no.253790  loss = 4.17547 avg_loss = 3.39933\n",
      "epoch no.3 train no.253800  loss = 3.67932 avg_loss = 3.36486\n",
      "epoch no.3 train no.253810  loss = 2.77509 avg_loss = 3.36846\n",
      "epoch no.3 train no.253820  loss = 2.83047 avg_loss = 3.33054\n",
      "epoch no.3 train no.253830  loss = 6.67174 avg_loss = 3.37683\n",
      "epoch no.3 train no.253840  loss = 2.55225 avg_loss = 3.34195\n",
      "epoch no.3 train no.253850  loss = 3.90567 avg_loss = 3.30794\n",
      "epoch no.3 train no.253860  loss = 4.51540 avg_loss = 3.35057\n",
      "epoch no.3 train no.253870  loss = 4.13608 avg_loss = 3.41554\n",
      "epoch no.3 train no.253880  loss = 3.03964 avg_loss = 3.40920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.253890  loss = 3.16384 avg_loss = 3.40067\n",
      "epoch no.3 train no.253900  loss = 2.72555 avg_loss = 3.35139\n",
      "epoch no.3 train no.253910  loss = 3.80628 avg_loss = 3.36552\n",
      "epoch no.3 train no.253920  loss = 3.07210 avg_loss = 3.36780\n",
      "epoch no.3 train no.253930  loss = 4.70156 avg_loss = 3.33086\n",
      "epoch no.3 train no.253940  loss = 3.53004 avg_loss = 3.29465\n",
      "epoch no.3 train no.253950  loss = 4.15199 avg_loss = 3.28599\n",
      "epoch no.3 train no.253960  loss = 3.48914 avg_loss = 3.26784\n",
      "epoch no.3 train no.253970  loss = 3.28669 avg_loss = 3.24375\n",
      "epoch no.3 train no.253980  loss = 2.02647 avg_loss = 3.24888\n",
      "epoch no.3 train no.253990  loss = 3.23936 avg_loss = 3.26280\n",
      "epoch no.3 train no.254000  loss = 3.37858 avg_loss = 3.26599\n",
      "4\n",
      "to_tokens: ['▁가을', '▁드라마', '송', '▁모음', '집', '</s>']\n",
      "추억의 팝송 모음집</s>\n",
      "epoch no.3 train no.254010  loss = 2.70330 avg_loss = 3.27346\n",
      "epoch no.3 train no.254020  loss = 3.50658 avg_loss = 3.30915\n",
      "epoch no.3 train no.254030  loss = 2.68701 avg_loss = 3.30761\n",
      "epoch no.3 train no.254040  loss = 2.63224 avg_loss = 3.30437\n",
      "epoch no.3 train no.254050  loss = 2.67483 avg_loss = 3.30773\n",
      "epoch no.3 train no.254060  loss = 3.36383 avg_loss = 3.29285\n",
      "epoch no.3 train no.254070  loss = 3.73798 avg_loss = 3.29220\n",
      "epoch no.3 train no.254080  loss = 2.91047 avg_loss = 3.30128\n",
      "epoch no.3 train no.254090  loss = 2.08094 avg_loss = 3.24561\n",
      "epoch no.3 train no.254100  loss = 3.61089 avg_loss = 3.23666\n",
      "epoch no.3 train no.254110  loss = 4.11136 avg_loss = 3.25226\n",
      "epoch no.3 train no.254120  loss = 2.40709 avg_loss = 3.28311\n",
      "epoch no.3 train no.254130  loss = 4.35444 avg_loss = 3.26062\n",
      "epoch no.3 train no.254140  loss = 4.16881 avg_loss = 3.30867\n",
      "epoch no.3 train no.254150  loss = 2.86079 avg_loss = 3.27392\n",
      "epoch no.3 train no.254160  loss = 3.52949 avg_loss = 3.29330\n",
      "epoch no.3 train no.254170  loss = 2.25806 avg_loss = 3.24125\n",
      "epoch no.3 train no.254180  loss = 2.95266 avg_loss = 3.16528\n",
      "epoch no.3 train no.254190  loss = 4.40555 avg_loss = 3.22259\n",
      "epoch no.3 train no.254200  loss = 3.94387 avg_loss = 3.22449\n",
      "epoch no.3 train no.254210  loss = 4.62108 avg_loss = 3.23050\n",
      "epoch no.3 train no.254220  loss = 2.91242 avg_loss = 3.21896\n",
      "epoch no.3 train no.254230  loss = 2.88941 avg_loss = 3.23428\n",
      "epoch no.3 train no.254240  loss = 6.06109 avg_loss = 3.28107\n",
      "epoch no.3 train no.254250  loss = 5.10840 avg_loss = 3.28003\n",
      "epoch no.3 train no.254260  loss = 2.95421 avg_loss = 3.29411\n",
      "epoch no.3 train no.254270  loss = 2.84061 avg_loss = 3.33604\n",
      "epoch no.3 train no.254280  loss = 4.32461 avg_loss = 3.38088\n",
      "epoch no.3 train no.254290  loss = 4.83124 avg_loss = 3.42351\n",
      "epoch no.3 train no.254300  loss = 4.25726 avg_loss = 3.42472\n",
      "epoch no.3 train no.254310  loss = 2.69654 avg_loss = 3.43336\n",
      "epoch no.3 train no.254320  loss = 4.07835 avg_loss = 3.41307\n",
      "epoch no.3 train no.254330  loss = 3.13236 avg_loss = 3.41017\n",
      "epoch no.3 train no.254340  loss = 3.95287 avg_loss = 3.40125\n",
      "epoch no.3 train no.254350  loss = 4.01396 avg_loss = 3.44761\n",
      "epoch no.3 train no.254360  loss = 2.26279 avg_loss = 3.49775\n",
      "epoch no.3 train no.254370  loss = 4.19356 avg_loss = 3.46050\n",
      "epoch no.3 train no.254380  loss = 3.40945 avg_loss = 3.45678\n",
      "epoch no.3 train no.254390  loss = 1.98531 avg_loss = 3.45200\n",
      "epoch no.3 train no.254400  loss = 3.39952 avg_loss = 3.42567\n",
      "epoch no.3 train no.254410  loss = 2.86845 avg_loss = 3.41853\n",
      "epoch no.3 train no.254420  loss = 3.46973 avg_loss = 3.45154\n",
      "epoch no.3 train no.254430  loss = 2.44319 avg_loss = 3.43097\n",
      "epoch no.3 train no.254440  loss = 4.20878 avg_loss = 3.45494\n",
      "epoch no.3 train no.254450  loss = 1.29184 avg_loss = 3.45087\n",
      "epoch no.3 train no.254460  loss = 4.28009 avg_loss = 3.41632\n",
      "epoch no.3 train no.254470  loss = 4.89969 avg_loss = 3.38846\n",
      "epoch no.3 train no.254480  loss = 4.93117 avg_loss = 3.41525\n",
      "epoch no.3 train no.254490  loss = 2.73896 avg_loss = 3.41081\n",
      "epoch no.3 train no.254500  loss = 4.03597 avg_loss = 3.40703\n",
      "epoch no.3 train no.254510  loss = 2.14836 avg_loss = 3.36449\n",
      "epoch no.3 train no.254520  loss = 4.00077 avg_loss = 3.36903\n",
      "epoch no.3 train no.254530  loss = 4.81885 avg_loss = 3.37270\n",
      "epoch no.3 train no.254540  loss = 2.65211 avg_loss = 3.37232\n",
      "epoch no.3 train no.254550  loss = 3.97119 avg_loss = 3.37015\n",
      "epoch no.3 train no.254560  loss = 3.21317 avg_loss = 3.34383\n",
      "epoch no.3 train no.254570  loss = 2.44006 avg_loss = 3.34114\n",
      "epoch no.3 train no.254580  loss = 3.19037 avg_loss = 3.34413\n",
      "epoch no.3 train no.254590  loss = 2.78257 avg_loss = 3.29124\n",
      "epoch no.3 train no.254600  loss = 3.63741 avg_loss = 3.28305\n",
      "epoch no.3 train no.254610  loss = 3.89880 avg_loss = 3.28718\n",
      "epoch no.3 train no.254620  loss = 3.64849 avg_loss = 3.25882\n",
      "epoch no.3 train no.254630  loss = 4.41065 avg_loss = 3.28759\n",
      "epoch no.3 train no.254640  loss = 2.55807 avg_loss = 3.28693\n",
      "epoch no.3 train no.254650  loss = 2.17075 avg_loss = 3.24365\n",
      "epoch no.3 train no.254660  loss = 4.10077 avg_loss = 3.27066\n",
      "epoch no.3 train no.254670  loss = 4.03421 avg_loss = 3.24733\n",
      "epoch no.3 train no.254680  loss = 3.32973 avg_loss = 3.26034\n",
      "epoch no.3 train no.254690  loss = 3.98742 avg_loss = 3.28517\n",
      "epoch no.3 train no.254700  loss = 2.01810 avg_loss = 3.32600\n",
      "epoch no.3 train no.254710  loss = 5.22012 avg_loss = 3.34941\n",
      "epoch no.3 train no.254720  loss = 3.57783 avg_loss = 3.37066\n",
      "epoch no.3 train no.254730  loss = 2.68019 avg_loss = 3.38655\n",
      "epoch no.3 train no.254740  loss = 3.62530 avg_loss = 3.41467\n",
      "epoch no.3 train no.254750  loss = 2.33060 avg_loss = 3.37155\n",
      "epoch no.3 train no.254760  loss = 3.04694 avg_loss = 3.37631\n",
      "epoch no.3 train no.254770  loss = 3.38996 avg_loss = 3.40906\n",
      "epoch no.3 train no.254780  loss = 4.07083 avg_loss = 3.37850\n",
      "epoch no.3 train no.254790  loss = 4.81909 avg_loss = 3.41100\n",
      "epoch no.3 train no.254800  loss = 2.25888 avg_loss = 3.38021\n",
      "epoch no.3 train no.254810  loss = 2.73352 avg_loss = 3.35563\n",
      "epoch no.3 train no.254820  loss = 3.77931 avg_loss = 3.33964\n",
      "epoch no.3 train no.254830  loss = 3.17101 avg_loss = 3.38227\n",
      "epoch no.3 train no.254840  loss = 3.25575 avg_loss = 3.36881\n",
      "epoch no.3 train no.254850  loss = 3.02203 avg_loss = 3.33373\n",
      "epoch no.3 train no.254860  loss = 2.66439 avg_loss = 3.32032\n",
      "epoch no.3 train no.254870  loss = 3.72016 avg_loss = 3.35394\n",
      "epoch no.3 train no.254880  loss = 2.13587 avg_loss = 3.32222\n",
      "epoch no.3 train no.254890  loss = 3.04135 avg_loss = 3.34352\n",
      "epoch no.3 train no.254900  loss = 2.56979 avg_loss = 3.30510\n",
      "epoch no.3 train no.254910  loss = 2.54430 avg_loss = 3.31032\n",
      "epoch no.3 train no.254920  loss = 5.30029 avg_loss = 3.32130\n",
      "epoch no.3 train no.254930  loss = 4.48703 avg_loss = 3.30882\n",
      "epoch no.3 train no.254940  loss = 3.20325 avg_loss = 3.31416\n",
      "epoch no.3 train no.254950  loss = 2.78522 avg_loss = 3.33222\n",
      "epoch no.3 train no.254960  loss = 3.85668 avg_loss = 3.33170\n",
      "epoch no.3 train no.254970  loss = 2.27253 avg_loss = 3.29766\n",
      "epoch no.3 train no.254980  loss = 4.50467 avg_loss = 3.29687\n",
      "epoch no.3 train no.254990  loss = 3.87190 avg_loss = 3.25699\n",
      "epoch no.3 train no.255000  loss = 3.69751 avg_loss = 3.24429\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁가요', '드', '▁모음']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.3 train no.255010  loss = 3.35602 avg_loss = 3.24780\n",
      "epoch no.3 train no.255020  loss = 4.85939 avg_loss = 3.26655\n",
      "epoch no.3 train no.255030  loss = 1.08902 avg_loss = 3.23771\n",
      "epoch no.3 train no.255040  loss = 3.91315 avg_loss = 3.25896\n",
      "epoch no.3 train no.255050  loss = 2.02768 avg_loss = 3.26776\n",
      "epoch no.3 train no.255060  loss = 2.88386 avg_loss = 3.26844\n",
      "epoch no.3 train no.255070  loss = 2.20646 avg_loss = 3.24730\n",
      "epoch no.3 train no.255080  loss = 3.55066 avg_loss = 3.26432\n",
      "epoch no.3 train no.255090  loss = 3.83925 avg_loss = 3.27647\n",
      "epoch no.3 train no.255100  loss = 5.86767 avg_loss = 3.31558\n",
      "epoch no.3 train no.255110  loss = 4.61472 avg_loss = 3.38827\n",
      "epoch no.3 train no.255120  loss = 4.14766 avg_loss = 3.42086\n",
      "epoch no.3 train no.255130  loss = 3.01084 avg_loss = 3.40416\n",
      "epoch no.3 train no.255140  loss = 3.78774 avg_loss = 3.38992\n",
      "epoch no.3 train no.255150  loss = 2.67270 avg_loss = 3.34070\n",
      "epoch no.3 train no.255160  loss = 3.99079 avg_loss = 3.30177\n",
      "epoch no.3 train no.255170  loss = 2.21202 avg_loss = 3.27430\n",
      "epoch no.3 train no.255180  loss = 2.36215 avg_loss = 3.23753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.255190  loss = 3.35206 avg_loss = 3.22322\n",
      "epoch no.3 train no.255200  loss = 2.29563 avg_loss = 3.20235\n",
      "epoch no.3 train no.255210  loss = 4.12734 avg_loss = 3.21998\n",
      "epoch no.3 train no.255220  loss = 5.05387 avg_loss = 3.27480\n",
      "epoch no.3 train no.255230  loss = 1.73470 avg_loss = 3.26682\n",
      "epoch no.3 train no.255240  loss = 2.23053 avg_loss = 3.22481\n",
      "epoch no.3 train no.255250  loss = 3.43328 avg_loss = 3.23876\n",
      "epoch no.3 train no.255260  loss = 4.38764 avg_loss = 3.24782\n",
      "epoch no.3 train no.255270  loss = 3.14471 avg_loss = 3.26993\n",
      "epoch no.3 train no.255280  loss = 2.53685 avg_loss = 3.25175\n",
      "epoch no.3 train no.255290  loss = 3.44536 avg_loss = 3.24785\n",
      "epoch no.3 train no.255300  loss = 3.29710 avg_loss = 3.25801\n",
      "epoch no.3 train no.255310  loss = 3.54655 avg_loss = 3.17943\n",
      "epoch no.3 train no.255320  loss = 1.96799 avg_loss = 3.21457\n",
      "epoch no.3 train no.255330  loss = 2.58280 avg_loss = 3.20202\n",
      "epoch no.3 train no.255340  loss = 2.93762 avg_loss = 3.24585\n",
      "epoch no.3 train no.255350  loss = 2.75150 avg_loss = 3.29446\n",
      "epoch no.3 train no.255360  loss = 4.82530 avg_loss = 3.30702\n",
      "epoch no.3 train no.255370  loss = 3.14722 avg_loss = 3.29772\n",
      "epoch no.3 train no.255380  loss = 2.31529 avg_loss = 3.23364\n",
      "epoch no.3 train no.255390  loss = 2.79753 avg_loss = 3.23374\n",
      "epoch no.3 train no.255400  loss = 3.22567 avg_loss = 3.19022\n",
      "epoch no.3 train no.255410  loss = 3.60728 avg_loss = 3.18501\n",
      "epoch no.3 train no.255420  loss = 2.44666 avg_loss = 3.19617\n",
      "epoch no.3 train no.255430  loss = 2.04343 avg_loss = 3.20101\n",
      "epoch no.3 train no.255440  loss = 3.60745 avg_loss = 3.22744\n",
      "epoch no.3 train no.255450  loss = 4.40193 avg_loss = 3.21555\n",
      "epoch no.3 train no.255460  loss = 3.13827 avg_loss = 3.21240\n",
      "epoch no.3 train no.255470  loss = 2.17325 avg_loss = 3.23073\n",
      "epoch no.3 train no.255480  loss = 5.00598 avg_loss = 3.24411\n",
      "epoch no.3 train no.255490  loss = 4.55174 avg_loss = 3.27287\n",
      "epoch no.3 train no.255500  loss = 4.36548 avg_loss = 3.28741\n",
      "epoch no.3 train no.255510  loss = 3.32921 avg_loss = 3.25041\n",
      "epoch no.3 train no.255520  loss = 3.85904 avg_loss = 3.26845\n",
      "epoch no.3 train no.255530  loss = 3.72324 avg_loss = 3.28773\n",
      "epoch no.3 train no.255540  loss = 3.12732 avg_loss = 3.27425\n",
      "epoch no.3 train no.255550  loss = 1.94191 avg_loss = 3.24186\n",
      "epoch no.3 train no.255560  loss = 3.65187 avg_loss = 3.21907\n",
      "epoch no.3 train no.255570  loss = 2.76732 avg_loss = 3.22643\n",
      "epoch no.3 train no.255580  loss = 2.43822 avg_loss = 3.24797\n",
      "epoch no.3 train no.255590  loss = 4.18447 avg_loss = 3.23278\n",
      "epoch no.3 train no.255600  loss = 2.11254 avg_loss = 3.24101\n",
      "epoch no.3 train no.255610  loss = 3.14330 avg_loss = 3.23414\n",
      "epoch no.3 train no.255620  loss = 2.00256 avg_loss = 3.25862\n",
      "epoch no.3 train no.255630  loss = 3.32894 avg_loss = 3.26958\n",
      "epoch no.3 train no.255640  loss = 3.16528 avg_loss = 3.29636\n",
      "epoch no.3 train no.255650  loss = 2.09319 avg_loss = 3.26781\n",
      "epoch no.3 train no.255660  loss = 2.86890 avg_loss = 3.28241\n",
      "epoch no.3 train no.255670  loss = 2.55286 avg_loss = 3.25049\n",
      "epoch no.3 train no.255680  loss = 4.11097 avg_loss = 3.25023\n",
      "epoch no.3 train no.255690  loss = 3.01956 avg_loss = 3.23251\n",
      "epoch no.3 train no.255700  loss = 2.96711 avg_loss = 3.22636\n",
      "epoch no.3 train no.255710  loss = 3.45514 avg_loss = 3.24768\n",
      "epoch no.3 train no.255720  loss = 2.36866 avg_loss = 3.21554\n",
      "epoch no.3 train no.255730  loss = 3.82314 avg_loss = 3.23651\n",
      "epoch no.3 train no.255740  loss = 3.86934 avg_loss = 3.23324\n",
      "epoch no.3 train no.255750  loss = 3.92090 avg_loss = 3.23279\n",
      "epoch no.3 train no.255760  loss = 3.38146 avg_loss = 3.26998\n",
      "epoch no.3 train no.255770  loss = 2.66199 avg_loss = 3.26504\n",
      "epoch no.3 train no.255780  loss = 3.77054 avg_loss = 3.27843\n",
      "epoch no.3 train no.255790  loss = 1.83595 avg_loss = 3.21701\n",
      "epoch no.3 train no.255800  loss = 1.59963 avg_loss = 3.20835\n",
      "epoch no.3 train no.255810  loss = 3.56746 avg_loss = 3.25884\n",
      "epoch no.3 train no.255820  loss = 1.67086 avg_loss = 3.23696\n",
      "epoch no.3 train no.255830  loss = 3.19398 avg_loss = 3.25087\n",
      "epoch no.3 train no.255840  loss = 3.72294 avg_loss = 3.27825\n",
      "epoch no.3 train no.255850  loss = 2.38000 avg_loss = 3.24954\n",
      "epoch no.3 train no.255860  loss = 3.84876 avg_loss = 3.22555\n",
      "epoch no.3 train no.255870  loss = 5.18512 avg_loss = 3.23982\n",
      "epoch no.3 train no.255880  loss = 3.42962 avg_loss = 3.28937\n",
      "epoch no.3 train no.255890  loss = 4.44377 avg_loss = 3.31215\n",
      "epoch no.3 train no.255900  loss = 3.80837 avg_loss = 3.30122\n",
      "epoch no.3 train no.255910  loss = 2.40043 avg_loss = 3.32957\n",
      "epoch no.3 train no.255920  loss = 2.68400 avg_loss = 3.33691\n",
      "epoch no.3 train no.255930  loss = 2.85418 avg_loss = 3.31018\n",
      "epoch no.3 train no.255940  loss = 2.57918 avg_loss = 3.28580\n",
      "epoch no.3 train no.255950  loss = 2.23131 avg_loss = 3.26515\n",
      "epoch no.3 train no.255960  loss = 4.03290 avg_loss = 3.28474\n",
      "epoch no.3 train no.255970  loss = 2.94391 avg_loss = 3.32237\n",
      "epoch no.3 train no.255980  loss = 5.58097 avg_loss = 3.32289\n",
      "epoch no.3 train no.255990  loss = 3.16588 avg_loss = 3.32293\n",
      "epoch no.3 train no.256000  loss = 3.86231 avg_loss = 3.34279\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '팝', '▁명', '곡', '</s>']\n",
      "추억의 올드팝 명곡</s>\n",
      "epoch no.3 train no.256010  loss = 3.47970 avg_loss = 3.29018\n",
      "epoch no.3 train no.256020  loss = 3.11846 avg_loss = 3.30711\n",
      "epoch no.3 train no.256030  loss = 3.59986 avg_loss = 3.31599\n",
      "epoch no.3 train no.256040  loss = 3.47755 avg_loss = 3.35458\n",
      "epoch no.3 train no.256050  loss = 3.95345 avg_loss = 3.34957\n",
      "epoch no.3 train no.256060  loss = 3.48160 avg_loss = 3.37242\n",
      "epoch no.3 train no.256070  loss = 2.81701 avg_loss = 3.34691\n",
      "epoch no.3 train no.256080  loss = 2.34038 avg_loss = 3.33734\n",
      "epoch no.3 train no.256090  loss = 3.12726 avg_loss = 3.34673\n",
      "epoch no.3 train no.256100  loss = 4.13275 avg_loss = 3.34219\n",
      "epoch no.3 train no.256110  loss = 2.71741 avg_loss = 3.33913\n",
      "epoch no.3 train no.256120  loss = 2.05973 avg_loss = 3.33096\n",
      "epoch no.3 train no.256130  loss = 3.40089 avg_loss = 3.36064\n",
      "epoch no.3 train no.256140  loss = 4.49896 avg_loss = 3.38636\n",
      "epoch no.3 train no.256150  loss = 3.15992 avg_loss = 3.38103\n",
      "epoch no.3 train no.256160  loss = 4.27720 avg_loss = 3.40879\n",
      "epoch no.3 train no.256170  loss = 2.84995 avg_loss = 3.44448\n",
      "epoch no.3 train no.256180  loss = 1.43254 avg_loss = 3.42195\n",
      "epoch no.3 train no.256190  loss = 2.65651 avg_loss = 3.44556\n",
      "epoch no.3 train no.256200  loss = 2.48266 avg_loss = 3.43168\n",
      "epoch no.3 train no.256210  loss = 3.93016 avg_loss = 3.43722\n",
      "epoch no.3 train no.256220  loss = 3.98042 avg_loss = 3.44837\n",
      "epoch no.3 train no.256230  loss = 3.83206 avg_loss = 3.46523\n",
      "epoch no.3 train no.256240  loss = 2.46438 avg_loss = 3.47223\n",
      "epoch no.3 train no.256250  loss = 3.66745 avg_loss = 3.49978\n",
      "epoch no.3 train no.256260  loss = 3.60821 avg_loss = 3.48790\n",
      "epoch no.3 train no.256270  loss = 1.36917 avg_loss = 3.48440\n",
      "epoch no.3 train no.256280  loss = 3.24223 avg_loss = 3.49171\n",
      "epoch no.3 train no.256290  loss = 3.98934 avg_loss = 3.48399\n",
      "epoch no.3 train no.256300  loss = 1.59929 avg_loss = 3.42951\n",
      "epoch no.3 train no.256310  loss = 4.59106 avg_loss = 3.41848\n",
      "epoch no.3 train no.256320  loss = 4.65353 avg_loss = 3.40483\n",
      "epoch no.3 train no.256330  loss = 2.82559 avg_loss = 3.44441\n",
      "epoch no.3 train no.256340  loss = 3.26055 avg_loss = 3.44113\n",
      "epoch no.3 train no.256350  loss = 4.15034 avg_loss = 3.46279\n",
      "epoch no.3 train no.256360  loss = 2.52682 avg_loss = 3.43791\n",
      "epoch no.3 train no.256370  loss = 4.19839 avg_loss = 3.40800\n",
      "epoch no.3 train no.256380  loss = 2.73706 avg_loss = 3.45271\n",
      "epoch no.3 train no.256390  loss = 2.99071 avg_loss = 3.43047\n",
      "epoch no.3 train no.256400  loss = 2.09528 avg_loss = 3.45114\n",
      "epoch no.3 train no.256410  loss = 3.50462 avg_loss = 3.42743\n",
      "epoch no.3 train no.256420  loss = 4.96665 avg_loss = 3.43149\n",
      "epoch no.3 train no.256430  loss = 4.64985 avg_loss = 3.46713\n",
      "epoch no.3 train no.256440  loss = 2.91206 avg_loss = 3.43764\n",
      "epoch no.3 train no.256450  loss = 2.41352 avg_loss = 3.36038\n",
      "epoch no.3 train no.256460  loss = 2.98622 avg_loss = 3.33701\n",
      "epoch no.3 train no.256470  loss = 2.08620 avg_loss = 3.33094\n",
      "epoch no.3 train no.256480  loss = 3.93854 avg_loss = 3.33417\n",
      "epoch no.3 train no.256490  loss = 3.40281 avg_loss = 3.32679\n",
      "epoch no.3 train no.256500  loss = 3.73215 avg_loss = 3.32238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.256510  loss = 2.92387 avg_loss = 3.32025\n",
      "epoch no.3 train no.256520  loss = 1.89723 avg_loss = 3.29905\n",
      "epoch no.3 train no.256530  loss = 2.03562 avg_loss = 3.29638\n",
      "epoch no.3 train no.256540  loss = 4.32551 avg_loss = 3.40562\n",
      "epoch no.3 train no.256550  loss = 2.38361 avg_loss = 3.38161\n",
      "epoch no.3 train no.256560  loss = 3.69706 avg_loss = 3.40564\n",
      "epoch no.3 train no.256570  loss = 3.57856 avg_loss = 3.38545\n",
      "epoch no.3 train no.256580  loss = 4.04519 avg_loss = 3.41389\n",
      "epoch no.3 train no.256590  loss = 1.91201 avg_loss = 3.36722\n",
      "epoch no.3 train no.256600  loss = 2.84907 avg_loss = 3.39092\n",
      "epoch no.3 train no.256610  loss = 4.02421 avg_loss = 3.38962\n",
      "epoch no.3 train no.256620  loss = 2.13697 avg_loss = 3.35751\n",
      "epoch no.3 train no.256630  loss = 3.17230 avg_loss = 3.33888\n",
      "epoch no.3 train no.256640  loss = 3.07339 avg_loss = 3.35311\n",
      "epoch no.3 train no.256650  loss = 2.97523 avg_loss = 3.32272\n",
      "epoch no.3 train no.256660  loss = 2.65834 avg_loss = 3.36014\n",
      "epoch no.3 train no.256670  loss = 3.45201 avg_loss = 3.35598\n",
      "epoch no.3 train no.256680  loss = 3.56373 avg_loss = 3.38441\n",
      "epoch no.3 train no.256690  loss = 1.96930 avg_loss = 3.35269\n",
      "epoch no.3 train no.256700  loss = 3.21186 avg_loss = 3.30357\n",
      "epoch no.3 train no.256710  loss = 4.91256 avg_loss = 3.30981\n",
      "epoch no.3 train no.256720  loss = 3.24566 avg_loss = 3.30438\n",
      "epoch no.3 train no.256730  loss = 2.73144 avg_loss = 3.29049\n",
      "epoch no.3 train no.256740  loss = 3.82063 avg_loss = 3.31181\n",
      "epoch no.3 train no.256750  loss = 3.28897 avg_loss = 3.27218\n",
      "epoch no.3 train no.256760  loss = 4.03192 avg_loss = 3.26469\n",
      "epoch no.3 train no.256770  loss = 4.56209 avg_loss = 3.27811\n",
      "epoch no.3 train no.256780  loss = 4.55787 avg_loss = 3.31951\n",
      "epoch no.3 train no.256790  loss = 4.44126 avg_loss = 3.33408\n",
      "epoch no.3 train no.256800  loss = 2.00167 avg_loss = 3.30502\n",
      "epoch no.3 train no.256810  loss = 3.89061 avg_loss = 3.35133\n",
      "epoch no.3 train no.256820  loss = 3.94662 avg_loss = 3.36772\n",
      "epoch no.3 train no.256830  loss = 3.36864 avg_loss = 3.37267\n",
      "epoch no.3 train no.256840  loss = 4.55426 avg_loss = 3.36931\n",
      "epoch no.3 train no.256850  loss = 2.94685 avg_loss = 3.34019\n",
      "epoch no.3 train no.256860  loss = 5.14843 avg_loss = 3.36914\n",
      "epoch no.3 train no.256870  loss = 2.59355 avg_loss = 3.34374\n",
      "epoch no.3 train no.256880  loss = 4.32953 avg_loss = 3.37703\n",
      "epoch no.3 train no.256890  loss = 3.04928 avg_loss = 3.36178\n",
      "epoch no.3 train no.256900  loss = 5.40145 avg_loss = 3.37315\n",
      "epoch no.3 train no.256910  loss = 4.54724 avg_loss = 3.36481\n",
      "epoch no.3 train no.256920  loss = 3.53537 avg_loss = 3.37295\n",
      "epoch no.3 train no.256930  loss = 4.61222 avg_loss = 3.39321\n",
      "epoch no.3 train no.256940  loss = 3.62850 avg_loss = 3.39266\n",
      "epoch no.3 train no.256950  loss = 3.24093 avg_loss = 3.38851\n",
      "epoch no.3 train no.256960  loss = 2.96530 avg_loss = 3.36802\n",
      "epoch no.3 train no.256970  loss = 1.95902 avg_loss = 3.32859\n",
      "epoch no.3 train no.256980  loss = 3.62896 avg_loss = 3.34407\n",
      "epoch no.3 train no.256990  loss = 2.23617 avg_loss = 3.34927\n",
      "epoch no.3 train no.257000  loss = 2.91236 avg_loss = 3.33550\n",
      "5\n",
      "to_tokens: ['▁가을', '▁발라', '▁o', 'st', '▁모음', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.3 train no.257010  loss = 2.57399 avg_loss = 3.34820\n",
      "epoch no.3 train no.257020  loss = 3.05042 avg_loss = 3.33936\n",
      "epoch no.3 train no.257030  loss = 4.20903 avg_loss = 3.35722\n",
      "epoch no.3 train no.257040  loss = 4.42198 avg_loss = 3.35725\n",
      "epoch no.3 train no.257050  loss = 2.26319 avg_loss = 3.32884\n",
      "epoch no.3 train no.257060  loss = 4.78427 avg_loss = 3.35235\n",
      "epoch no.3 train no.257070  loss = 3.19585 avg_loss = 3.33387\n",
      "epoch no.3 train no.257080  loss = 3.64931 avg_loss = 3.34708\n",
      "epoch no.3 train no.257090  loss = 5.72472 avg_loss = 3.35130\n",
      "epoch no.3 train no.257100  loss = 4.75843 avg_loss = 3.39291\n",
      "epoch no.3 train no.257110  loss = 3.63168 avg_loss = 3.38904\n",
      "epoch no.3 train no.257120  loss = 3.17063 avg_loss = 3.40868\n",
      "epoch no.3 train no.257130  loss = 4.87762 avg_loss = 3.41250\n",
      "epoch no.3 train no.257140  loss = 3.44368 avg_loss = 3.37370\n",
      "epoch no.3 train no.257150  loss = 3.17486 avg_loss = 3.41025\n",
      "epoch no.3 train no.257160  loss = 2.09373 avg_loss = 3.38479\n",
      "epoch no.3 train no.257170  loss = 2.89185 avg_loss = 3.37560\n",
      "epoch no.3 train no.257180  loss = 2.58169 avg_loss = 3.36040\n",
      "epoch no.3 train no.257190  loss = 2.76106 avg_loss = 3.39826\n",
      "epoch no.3 train no.257200  loss = 2.14700 avg_loss = 3.36033\n",
      "epoch no.3 train no.257210  loss = 1.91599 avg_loss = 3.32817\n",
      "epoch no.3 train no.257220  loss = 1.56533 avg_loss = 3.28014\n",
      "epoch no.3 train no.257230  loss = 3.46048 avg_loss = 3.33111\n",
      "epoch no.3 train no.257240  loss = 4.28640 avg_loss = 3.38906\n",
      "epoch no.3 train no.257250  loss = 2.62616 avg_loss = 3.35552\n",
      "epoch no.3 train no.257260  loss = 4.32870 avg_loss = 3.32166\n",
      "epoch no.3 train no.257270  loss = 4.36562 avg_loss = 3.38029\n",
      "epoch no.3 train no.257280  loss = 3.09396 avg_loss = 3.35945\n",
      "epoch no.3 train no.257290  loss = 1.73149 avg_loss = 3.37463\n",
      "epoch no.3 train no.257300  loss = 3.31777 avg_loss = 3.36577\n",
      "epoch no.3 train no.257310  loss = 2.87736 avg_loss = 3.34605\n",
      "epoch no.3 train no.257320  loss = 2.78904 avg_loss = 3.34851\n",
      "epoch no.3 train no.257330  loss = 4.71511 avg_loss = 3.37473\n",
      "epoch no.3 train no.257340  loss = 2.96891 avg_loss = 3.33969\n",
      "epoch no.3 train no.257350  loss = 3.29716 avg_loss = 3.32698\n",
      "epoch no.3 train no.257360  loss = 2.39285 avg_loss = 3.31810\n",
      "epoch no.3 train no.257370  loss = 4.91378 avg_loss = 3.33282\n",
      "epoch no.3 train no.257380  loss = 2.74620 avg_loss = 3.33290\n",
      "epoch no.3 train no.257390  loss = 3.28795 avg_loss = 3.30506\n",
      "epoch no.3 train no.257400  loss = 2.42730 avg_loss = 3.30163\n",
      "epoch no.3 train no.257410  loss = 2.39705 avg_loss = 3.28158\n",
      "epoch no.3 train no.257420  loss = 2.37015 avg_loss = 3.27614\n",
      "epoch no.3 train no.257430  loss = 2.64622 avg_loss = 3.33132\n",
      "epoch no.3 train no.257440  loss = 2.41229 avg_loss = 3.28000\n",
      "epoch no.3 train no.257450  loss = 4.86685 avg_loss = 3.29230\n",
      "epoch no.3 train no.257460  loss = 3.38111 avg_loss = 3.30394\n",
      "epoch no.3 train no.257470  loss = 3.96137 avg_loss = 3.30058\n",
      "epoch no.3 train no.257480  loss = 3.30445 avg_loss = 3.23447\n",
      "epoch no.3 train no.257490  loss = 2.92049 avg_loss = 3.23686\n",
      "epoch no.3 train no.257500  loss = 3.24444 avg_loss = 3.25404\n",
      "epoch no.3 train no.257510  loss = 2.05718 avg_loss = 3.24156\n",
      "epoch no.3 train no.257520  loss = 3.11473 avg_loss = 3.27155\n",
      "epoch no.3 train no.257530  loss = 2.59387 avg_loss = 3.24485\n",
      "epoch no.3 train no.257540  loss = 1.97813 avg_loss = 3.23118\n",
      "epoch no.3 train no.257550  loss = 2.62271 avg_loss = 3.22072\n",
      "epoch no.3 train no.257560  loss = 2.33093 avg_loss = 3.17010\n",
      "epoch no.3 train no.257570  loss = 2.01376 avg_loss = 3.15336\n",
      "epoch no.3 train no.257580  loss = 2.55787 avg_loss = 3.17071\n",
      "epoch no.3 train no.257590  loss = 1.93733 avg_loss = 3.17800\n",
      "epoch no.3 train no.257600  loss = 5.32972 avg_loss = 3.24780\n",
      "epoch no.3 train no.257610  loss = 3.93064 avg_loss = 3.22511\n",
      "epoch no.3 train no.257620  loss = 3.53565 avg_loss = 3.22536\n",
      "epoch no.3 train no.257630  loss = 2.82340 avg_loss = 3.20191\n",
      "epoch no.3 train no.257640  loss = 4.01183 avg_loss = 3.22156\n",
      "epoch no.3 train no.257650  loss = 3.56189 avg_loss = 3.22809\n",
      "epoch no.3 train no.257660  loss = 4.51733 avg_loss = 3.20268\n",
      "epoch no.3 train no.257670  loss = 4.59091 avg_loss = 3.19636\n",
      "epoch no.3 train no.257680  loss = 4.03452 avg_loss = 3.17972\n",
      "epoch no.3 train no.257690  loss = 2.70643 avg_loss = 3.22843\n",
      "epoch no.3 train no.257700  loss = 3.11565 avg_loss = 3.21623\n",
      "epoch no.3 train no.257710  loss = 2.55866 avg_loss = 3.20364\n",
      "epoch no.3 train no.257720  loss = 3.04689 avg_loss = 3.20906\n",
      "epoch no.3 train no.257730  loss = 4.36791 avg_loss = 3.27762\n",
      "epoch no.3 train no.257740  loss = 2.51087 avg_loss = 3.28166\n",
      "epoch no.3 train no.257750  loss = 3.21558 avg_loss = 3.25647\n",
      "epoch no.3 train no.257760  loss = 3.76494 avg_loss = 3.28117\n",
      "epoch no.3 train no.257770  loss = 3.63806 avg_loss = 3.25867\n",
      "epoch no.3 train no.257780  loss = 2.97927 avg_loss = 3.27205\n",
      "epoch no.3 train no.257790  loss = 4.22549 avg_loss = 3.36581\n",
      "epoch no.3 train no.257800  loss = 2.50503 avg_loss = 3.33342\n",
      "epoch no.3 train no.257810  loss = 2.71640 avg_loss = 3.34251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.257820  loss = 5.58922 avg_loss = 3.32605\n",
      "epoch no.3 train no.257830  loss = 3.25167 avg_loss = 3.34119\n",
      "epoch no.3 train no.257840  loss = 2.31827 avg_loss = 3.41048\n",
      "epoch no.3 train no.257850  loss = 3.40008 avg_loss = 3.42571\n",
      "epoch no.3 train no.257860  loss = 2.52154 avg_loss = 3.43559\n",
      "epoch no.3 train no.257870  loss = 4.08193 avg_loss = 3.50090\n",
      "epoch no.3 train no.257880  loss = 3.30623 avg_loss = 3.54108\n",
      "epoch no.3 train no.257890  loss = 3.80249 avg_loss = 3.50123\n",
      "epoch no.3 train no.257900  loss = 2.53692 avg_loss = 3.49225\n",
      "epoch no.3 train no.257910  loss = 5.38529 avg_loss = 3.47412\n",
      "epoch no.3 train no.257920  loss = 4.38200 avg_loss = 3.45449\n",
      "epoch no.3 train no.257930  loss = 3.62297 avg_loss = 3.47313\n",
      "epoch no.3 train no.257940  loss = 5.80452 avg_loss = 3.54194\n",
      "epoch no.3 train no.257950  loss = 2.02718 avg_loss = 3.50825\n",
      "epoch no.3 train no.257960  loss = 2.31316 avg_loss = 3.51024\n",
      "epoch no.3 train no.257970  loss = 2.30676 avg_loss = 3.44513\n",
      "epoch no.3 train no.257980  loss = 3.90764 avg_loss = 3.41592\n",
      "epoch no.3 train no.257990  loss = 4.62803 avg_loss = 3.40929\n",
      "epoch no.3 train no.258000  loss = 2.46963 avg_loss = 3.45435\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 80년대 발라드</s>\n",
      "epoch no.3 train no.258010  loss = 3.40478 avg_loss = 3.48613\n",
      "epoch no.3 train no.258020  loss = 2.58752 avg_loss = 3.41940\n",
      "epoch no.3 train no.258030  loss = 4.51962 avg_loss = 3.40564\n",
      "epoch no.3 train no.258040  loss = 1.85143 avg_loss = 3.33997\n",
      "epoch no.3 train no.258050  loss = 4.50334 avg_loss = 3.35140\n",
      "epoch no.3 train no.258060  loss = 2.25765 avg_loss = 3.39257\n",
      "epoch no.3 train no.258070  loss = 3.09883 avg_loss = 3.40104\n",
      "epoch no.3 train no.258080  loss = 3.40827 avg_loss = 3.36329\n",
      "epoch no.3 train no.258090  loss = 3.23453 avg_loss = 3.38060\n",
      "epoch no.3 train no.258100  loss = 3.53307 avg_loss = 3.36423\n",
      "epoch no.3 train no.258110  loss = 4.93317 avg_loss = 3.35268\n",
      "epoch no.3 train no.258120  loss = 3.70117 avg_loss = 3.28857\n",
      "epoch no.3 train no.258130  loss = 3.06775 avg_loss = 3.27352\n",
      "epoch no.3 train no.258140  loss = 2.62934 avg_loss = 3.25903\n",
      "epoch no.3 train no.258150  loss = 4.48159 avg_loss = 3.27146\n",
      "epoch no.3 train no.258160  loss = 2.42930 avg_loss = 3.24901\n",
      "epoch no.3 train no.258170  loss = 2.53234 avg_loss = 3.24599\n",
      "epoch no.3 train no.258180  loss = 3.42524 avg_loss = 3.27152\n",
      "epoch no.3 train no.258190  loss = 4.06243 avg_loss = 3.28254\n",
      "epoch no.3 train no.258200  loss = 6.39738 avg_loss = 3.28800\n",
      "epoch no.3 train no.258210  loss = 1.48384 avg_loss = 3.31014\n",
      "epoch no.3 train no.258220  loss = 3.03940 avg_loss = 3.33810\n",
      "epoch no.3 train no.258230  loss = 3.11341 avg_loss = 3.36579\n",
      "epoch no.3 train no.258240  loss = 4.50094 avg_loss = 3.40630\n",
      "epoch no.3 train no.258250  loss = 3.26468 avg_loss = 3.38192\n",
      "epoch no.3 train no.258260  loss = 3.17079 avg_loss = 3.41531\n",
      "epoch no.3 train no.258270  loss = 3.35216 avg_loss = 3.38864\n",
      "epoch no.3 train no.258280  loss = 3.76208 avg_loss = 3.35754\n",
      "epoch no.3 train no.258290  loss = 3.46573 avg_loss = 3.32513\n",
      "epoch no.3 train no.258300  loss = 3.18739 avg_loss = 3.34848\n",
      "epoch no.3 train no.258310  loss = 2.00601 avg_loss = 3.29878\n",
      "epoch no.3 train no.258320  loss = 2.82918 avg_loss = 3.34269\n",
      "epoch no.3 train no.258330  loss = 3.79370 avg_loss = 3.35863\n",
      "epoch no.3 train no.258340  loss = 2.79051 avg_loss = 3.33832\n",
      "epoch no.3 train no.258350  loss = 4.00375 avg_loss = 3.29157\n",
      "epoch no.3 train no.258360  loss = 2.35849 avg_loss = 3.25283\n",
      "epoch no.3 train no.258370  loss = 2.86122 avg_loss = 3.24610\n",
      "epoch no.3 train no.258380  loss = 2.48772 avg_loss = 3.23463\n",
      "epoch no.3 train no.258390  loss = 4.26567 avg_loss = 3.23594\n",
      "epoch no.3 train no.258400  loss = 3.23840 avg_loss = 3.24412\n",
      "epoch no.3 train no.258410  loss = 2.37178 avg_loss = 3.24513\n",
      "epoch no.3 train no.258420  loss = 1.98813 avg_loss = 3.21865\n",
      "epoch no.3 train no.258430  loss = 2.12304 avg_loss = 3.19328\n",
      "epoch no.3 train no.258440  loss = 3.84676 avg_loss = 3.22922\n",
      "epoch no.3 train no.258450  loss = 3.14593 avg_loss = 3.23883\n",
      "epoch no.3 train no.258460  loss = 2.04742 avg_loss = 3.23507\n",
      "epoch no.3 train no.258470  loss = 2.86869 avg_loss = 3.22824\n",
      "epoch no.3 train no.258480  loss = 4.32536 avg_loss = 3.27718\n",
      "epoch no.3 train no.258490  loss = 2.74813 avg_loss = 3.25282\n",
      "epoch no.3 train no.258500  loss = 4.95516 avg_loss = 3.25980\n",
      "epoch no.3 train no.258510  loss = 3.09848 avg_loss = 3.32078\n",
      "epoch no.3 train no.258520  loss = 3.36719 avg_loss = 3.33418\n",
      "epoch no.3 train no.258530  loss = 2.99184 avg_loss = 3.31786\n",
      "epoch no.3 train no.258540  loss = 2.73854 avg_loss = 3.33934\n",
      "epoch no.3 train no.258550  loss = 5.15416 avg_loss = 3.37982\n",
      "epoch no.3 train no.258560  loss = 2.48697 avg_loss = 3.37067\n",
      "epoch no.3 train no.258570  loss = 4.63420 avg_loss = 3.41691\n",
      "epoch no.3 train no.258580  loss = 2.22756 avg_loss = 3.38038\n",
      "epoch no.3 train no.258590  loss = 4.86382 avg_loss = 3.41019\n",
      "epoch no.3 train no.258600  loss = 3.81404 avg_loss = 3.40467\n",
      "epoch no.3 train no.258610  loss = 2.98823 avg_loss = 3.41584\n",
      "epoch no.3 train no.258620  loss = 1.45912 avg_loss = 3.41256\n",
      "epoch no.3 train no.258630  loss = 4.57380 avg_loss = 3.41766\n",
      "epoch no.3 train no.258640  loss = 2.86848 avg_loss = 3.36991\n",
      "epoch no.3 train no.258650  loss = 4.00508 avg_loss = 3.37306\n",
      "epoch no.3 train no.258660  loss = 2.68074 avg_loss = 3.31507\n",
      "epoch no.3 train no.258670  loss = 1.85335 avg_loss = 3.30494\n",
      "epoch no.3 train no.258680  loss = 3.99188 avg_loss = 3.34152\n",
      "epoch no.3 train no.258690  loss = 2.45801 avg_loss = 3.35261\n",
      "epoch no.3 train no.258700  loss = 4.64379 avg_loss = 3.33274\n",
      "epoch no.3 train no.258710  loss = 2.83556 avg_loss = 3.27352\n",
      "epoch no.3 train no.258720  loss = 3.31103 avg_loss = 3.27905\n",
      "epoch no.3 train no.258730  loss = 2.32640 avg_loss = 3.27334\n",
      "epoch no.3 train no.258740  loss = 4.58614 avg_loss = 3.31970\n",
      "epoch no.3 train no.258750  loss = 2.70994 avg_loss = 3.28111\n",
      "epoch no.3 train no.258760  loss = 3.77450 avg_loss = 3.32999\n",
      "epoch no.3 train no.258770  loss = 3.91648 avg_loss = 3.36956\n",
      "epoch no.3 train no.258780  loss = 1.83974 avg_loss = 3.38164\n",
      "epoch no.3 train no.258790  loss = 2.27855 avg_loss = 3.37614\n",
      "epoch no.3 train no.258800  loss = 4.21719 avg_loss = 3.41945\n",
      "epoch no.3 train no.258810  loss = 5.16314 avg_loss = 3.41071\n",
      "epoch no.3 train no.258820  loss = 3.67926 avg_loss = 3.43769\n",
      "epoch no.3 train no.258830  loss = 5.28316 avg_loss = 3.47543\n",
      "epoch no.3 train no.258840  loss = 3.62393 avg_loss = 3.43921\n",
      "epoch no.3 train no.258850  loss = 2.66534 avg_loss = 3.40455\n",
      "epoch no.3 train no.258860  loss = 3.64349 avg_loss = 3.35375\n",
      "epoch no.3 train no.258870  loss = 3.96581 avg_loss = 3.34276\n",
      "epoch no.3 train no.258880  loss = 2.84648 avg_loss = 3.26090\n",
      "epoch no.3 train no.258890  loss = 2.08051 avg_loss = 3.24331\n",
      "epoch no.3 train no.258900  loss = 1.87115 avg_loss = 3.24703\n",
      "epoch no.3 train no.258910  loss = 4.13896 avg_loss = 3.23378\n",
      "epoch no.3 train no.258920  loss = 3.37765 avg_loss = 3.24053\n",
      "epoch no.3 train no.258930  loss = 3.51312 avg_loss = 3.28430\n",
      "epoch no.3 train no.258940  loss = 3.83767 avg_loss = 3.30803\n",
      "epoch no.3 train no.258950  loss = 5.69311 avg_loss = 3.36993\n",
      "epoch no.3 train no.258960  loss = 2.70868 avg_loss = 3.35054\n",
      "epoch no.3 train no.258970  loss = 4.78284 avg_loss = 3.38792\n",
      "epoch no.3 train no.258980  loss = 3.65991 avg_loss = 3.43194\n",
      "epoch no.3 train no.258990  loss = 2.33783 avg_loss = 3.41986\n",
      "epoch no.3 train no.259000  loss = 3.22179 avg_loss = 3.39652\n",
      "4\n",
      "to_tokens: ['▁가을', '▁올드', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.3 train no.259010  loss = 2.50229 avg_loss = 3.41639\n",
      "epoch no.3 train no.259020  loss = 1.96870 avg_loss = 3.32332\n",
      "epoch no.3 train no.259030  loss = 5.11935 avg_loss = 3.37519\n",
      "epoch no.3 train no.259040  loss = 3.22140 avg_loss = 3.33050\n",
      "epoch no.3 train no.259050  loss = 4.46060 avg_loss = 3.36114\n",
      "epoch no.3 train no.259060  loss = 3.47683 avg_loss = 3.39242\n",
      "epoch no.3 train no.259070  loss = 2.65639 avg_loss = 3.38163\n",
      "epoch no.3 train no.259080  loss = 1.68183 avg_loss = 3.36432\n",
      "epoch no.3 train no.259090  loss = 3.66113 avg_loss = 3.38698\n",
      "epoch no.3 train no.259100  loss = 2.25182 avg_loss = 3.43161\n",
      "epoch no.3 train no.259110  loss = 3.53979 avg_loss = 3.40942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.259120  loss = 3.80600 avg_loss = 3.39335\n",
      "epoch no.3 train no.259130  loss = 1.76414 avg_loss = 3.37977\n",
      "epoch no.3 train no.259140  loss = 3.21245 avg_loss = 3.33827\n",
      "epoch no.3 train no.259150  loss = 3.51839 avg_loss = 3.29643\n",
      "epoch no.3 train no.259160  loss = 2.73433 avg_loss = 3.30320\n",
      "epoch no.3 train no.259170  loss = 3.20296 avg_loss = 3.28438\n",
      "epoch no.3 train no.259180  loss = 2.96615 avg_loss = 3.28885\n",
      "epoch no.3 train no.259190  loss = 2.38336 avg_loss = 3.28667\n",
      "epoch no.3 train no.259200  loss = 2.56412 avg_loss = 3.30656\n",
      "epoch no.3 train no.259210  loss = 2.77069 avg_loss = 3.29352\n",
      "epoch no.3 train no.259220  loss = 4.86899 avg_loss = 3.30862\n",
      "epoch no.3 train no.259230  loss = 3.46755 avg_loss = 3.29593\n",
      "epoch no.3 train no.259240  loss = 2.78108 avg_loss = 3.28454\n",
      "epoch no.3 train no.259250  loss = 4.85485 avg_loss = 3.35314\n",
      "epoch no.3 train no.259260  loss = 3.26971 avg_loss = 3.37060\n",
      "epoch no.3 train no.259270  loss = 2.68241 avg_loss = 3.38629\n",
      "epoch no.3 train no.259280  loss = 4.26638 avg_loss = 3.42818\n",
      "epoch no.3 train no.259290  loss = 1.72616 avg_loss = 3.39626\n",
      "epoch no.3 train no.259300  loss = 2.84840 avg_loss = 3.40307\n",
      "epoch no.3 train no.259310  loss = 2.66139 avg_loss = 3.39118\n",
      "epoch no.3 train no.259320  loss = 3.77058 avg_loss = 3.39824\n",
      "epoch no.3 train no.259330  loss = 5.20696 avg_loss = 3.44773\n",
      "epoch no.3 train no.259340  loss = 3.86018 avg_loss = 3.40037\n",
      "epoch no.3 train no.259350  loss = 3.80657 avg_loss = 3.39006\n",
      "epoch no.3 train no.259360  loss = 3.10734 avg_loss = 3.39153\n",
      "epoch no.3 train no.259370  loss = 3.73084 avg_loss = 3.37979\n",
      "epoch no.3 train no.259380  loss = 3.30828 avg_loss = 3.32620\n",
      "epoch no.3 train no.259390  loss = 3.57181 avg_loss = 3.28891\n",
      "epoch no.3 train no.259400  loss = 4.37193 avg_loss = 3.30841\n",
      "epoch no.3 train no.259410  loss = 4.03473 avg_loss = 3.34961\n",
      "epoch no.3 train no.259420  loss = 5.08414 avg_loss = 3.39745\n",
      "epoch no.3 train no.259430  loss = 5.52943 avg_loss = 3.40454\n",
      "epoch no.3 train no.259440  loss = 2.56817 avg_loss = 3.44188\n",
      "epoch no.3 train no.259450  loss = 2.56255 avg_loss = 3.48471\n",
      "epoch no.3 train no.259460  loss = 4.17709 avg_loss = 3.47472\n",
      "epoch no.3 train no.259470  loss = 3.04109 avg_loss = 3.46016\n",
      "epoch no.3 train no.259480  loss = 4.06092 avg_loss = 3.46216\n",
      "epoch no.3 train no.259490  loss = 3.44258 avg_loss = 3.46020\n",
      "epoch no.3 train no.259500  loss = 2.46585 avg_loss = 3.44135\n",
      "epoch no.3 train no.259510  loss = 2.59075 avg_loss = 3.41307\n",
      "epoch no.3 train no.259520  loss = 2.49411 avg_loss = 3.42102\n",
      "epoch no.3 train no.259530  loss = 4.98312 avg_loss = 3.46855\n",
      "epoch no.3 train no.259540  loss = 2.47273 avg_loss = 3.45129\n",
      "epoch no.3 train no.259550  loss = 4.66489 avg_loss = 3.47006\n",
      "epoch no.3 train no.259560  loss = 2.97247 avg_loss = 3.46124\n",
      "epoch no.3 train no.259570  loss = 4.49920 avg_loss = 3.42823\n",
      "epoch no.3 train no.259580  loss = 3.45050 avg_loss = 3.42059\n",
      "epoch no.3 train no.259590  loss = 1.40523 avg_loss = 3.43221\n",
      "epoch no.3 train no.259600  loss = 3.68290 avg_loss = 3.40583\n",
      "epoch no.3 train no.259610  loss = 3.81403 avg_loss = 3.40603\n",
      "epoch no.3 train no.259620  loss = 2.89643 avg_loss = 3.39156\n",
      "epoch no.3 train no.259630  loss = 3.32231 avg_loss = 3.42729\n",
      "epoch no.3 train no.259640  loss = 3.05964 avg_loss = 3.41007\n",
      "epoch no.3 train no.259650  loss = 2.59148 avg_loss = 3.37697\n",
      "epoch no.3 train no.259660  loss = 2.61826 avg_loss = 3.36555\n",
      "epoch no.3 train no.259670  loss = 3.97297 avg_loss = 3.38788\n",
      "epoch no.3 train no.259680  loss = 3.42500 avg_loss = 3.35501\n",
      "epoch no.3 train no.259690  loss = 3.79749 avg_loss = 3.35406\n",
      "epoch no.3 train no.259700  loss = 5.75215 avg_loss = 3.37991\n",
      "epoch no.3 train no.259710  loss = 2.31267 avg_loss = 3.37974\n",
      "epoch no.3 train no.259720  loss = 2.66043 avg_loss = 3.34810\n",
      "epoch no.3 train no.259730  loss = 1.86010 avg_loss = 3.31091\n",
      "epoch no.3 train no.259740  loss = 5.83580 avg_loss = 3.33878\n",
      "epoch no.3 train no.259750  loss = 1.37243 avg_loss = 3.30336\n",
      "epoch no.3 train no.259760  loss = 2.42774 avg_loss = 3.33198\n",
      "epoch no.3 train no.259770  loss = 3.77039 avg_loss = 3.34028\n",
      "epoch no.3 train no.259780  loss = 2.65690 avg_loss = 3.32657\n",
      "epoch no.3 train no.259790  loss = 3.72860 avg_loss = 3.32818\n",
      "epoch no.3 train no.259800  loss = 3.45226 avg_loss = 3.30559\n",
      "epoch no.3 train no.259810  loss = 3.57024 avg_loss = 3.32940\n",
      "epoch no.3 train no.259820  loss = 3.36606 avg_loss = 3.38191\n",
      "epoch no.3 train no.259830  loss = 3.46942 avg_loss = 3.38948\n",
      "epoch no.3 train no.259840  loss = 3.19580 avg_loss = 3.39984\n",
      "epoch no.3 train no.259850  loss = 4.04805 avg_loss = 3.39486\n",
      "epoch no.3 train no.259860  loss = 2.85840 avg_loss = 3.35134\n",
      "epoch no.3 train no.259870  loss = 3.99817 avg_loss = 3.39923\n",
      "epoch no.3 train no.259880  loss = 4.27827 avg_loss = 3.39085\n",
      "epoch no.3 train no.259890  loss = 3.45458 avg_loss = 3.39946\n",
      "epoch no.3 train no.259900  loss = 3.01072 avg_loss = 3.39672\n",
      "epoch no.3 train no.259910  loss = 6.37370 avg_loss = 3.39502\n",
      "epoch no.3 train no.259920  loss = 3.23618 avg_loss = 3.36952\n",
      "epoch no.3 train no.259930  loss = 2.16879 avg_loss = 3.38510\n",
      "epoch no.3 train no.259940  loss = 2.40518 avg_loss = 3.36803\n",
      "epoch no.3 train no.259950  loss = 3.58626 avg_loss = 3.39224\n",
      "epoch no.3 train no.259960  loss = 3.03759 avg_loss = 3.39037\n",
      "epoch no.3 train no.259970  loss = 2.70766 avg_loss = 3.41459\n",
      "epoch no.3 train no.259980  loss = 4.00955 avg_loss = 3.37657\n",
      "epoch no.3 train no.259990  loss = 3.83300 avg_loss = 3.41453\n",
      "epoch no.3 train no.260000  loss = 3.09358 avg_loss = 3.41361\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.3 train no.260010  loss = 1.83055 avg_loss = 3.38088\n",
      "epoch no.3 train no.260020  loss = 3.48346 avg_loss = 3.40030\n",
      "epoch no.3 train no.260030  loss = 3.28594 avg_loss = 3.38572\n",
      "epoch no.3 train no.260040  loss = 2.45856 avg_loss = 3.34584\n",
      "epoch no.3 train no.260050  loss = 3.23500 avg_loss = 3.37656\n",
      "epoch no.3 train no.260060  loss = 2.82062 avg_loss = 3.36949\n",
      "epoch no.3 train no.260070  loss = 4.35194 avg_loss = 3.41167\n",
      "epoch no.3 train no.260080  loss = 3.00658 avg_loss = 3.38770\n",
      "epoch no.3 train no.260090  loss = 2.28386 avg_loss = 3.39033\n",
      "epoch no.3 train no.260100  loss = 2.51529 avg_loss = 3.43192\n",
      "epoch no.3 train no.260110  loss = 3.83295 avg_loss = 3.47731\n",
      "epoch no.3 train no.260120  loss = 1.91112 avg_loss = 3.46056\n",
      "epoch no.3 train no.260130  loss = 2.15303 avg_loss = 3.41824\n",
      "epoch no.3 train no.260140  loss = 3.13743 avg_loss = 3.38724\n",
      "epoch no.3 train no.260150  loss = 2.12985 avg_loss = 3.44067\n",
      "epoch no.3 train no.260160  loss = 2.96364 avg_loss = 3.43168\n",
      "epoch no.3 train no.260170  loss = 2.13660 avg_loss = 3.41077\n",
      "epoch no.3 train no.260180  loss = 3.05901 avg_loss = 3.34699\n",
      "epoch no.3 train no.260190  loss = 2.63846 avg_loss = 3.33226\n",
      "epoch no.3 train no.260200  loss = 2.67242 avg_loss = 3.34503\n",
      "epoch no.3 train no.260210  loss = 2.99560 avg_loss = 3.35295\n",
      "epoch no.3 train no.260220  loss = 2.82158 avg_loss = 3.31152\n",
      "epoch no.3 train no.260230  loss = 2.79845 avg_loss = 3.28673\n",
      "epoch no.3 train no.260240  loss = 3.30088 avg_loss = 3.27679\n",
      "epoch no.3 train no.260250  loss = 3.94587 avg_loss = 3.25220\n",
      "epoch no.3 train no.260260  loss = 2.01644 avg_loss = 3.24574\n",
      "epoch no.3 train no.260270  loss = 4.11329 avg_loss = 3.23782\n",
      "epoch no.3 train no.260280  loss = 3.00751 avg_loss = 3.25465\n",
      "epoch no.3 train no.260290  loss = 1.95708 avg_loss = 3.23649\n",
      "epoch no.3 train no.260300  loss = 4.14924 avg_loss = 3.21599\n",
      "epoch no.3 train no.260310  loss = 2.41030 avg_loss = 3.24382\n",
      "epoch no.3 train no.260320  loss = 4.79757 avg_loss = 3.23942\n",
      "epoch no.3 train no.260330  loss = 4.83016 avg_loss = 3.25912\n",
      "epoch no.3 train no.260340  loss = 3.75669 avg_loss = 3.28918\n",
      "epoch no.3 train no.260350  loss = 3.97498 avg_loss = 3.32556\n",
      "epoch no.3 train no.260360  loss = 3.34893 avg_loss = 3.38336\n",
      "epoch no.3 train no.260370  loss = 3.65668 avg_loss = 3.41188\n",
      "epoch no.3 train no.260380  loss = 6.73424 avg_loss = 3.45202\n",
      "epoch no.3 train no.260390  loss = 2.12564 avg_loss = 3.37246\n",
      "epoch no.3 train no.260400  loss = 5.33369 avg_loss = 3.35588\n",
      "epoch no.3 train no.260410  loss = 2.12764 avg_loss = 3.30922\n",
      "epoch no.3 train no.260420  loss = 2.89191 avg_loss = 3.34531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.260430  loss = 3.14116 avg_loss = 3.40708\n",
      "epoch no.3 train no.260440  loss = 5.17732 avg_loss = 3.40618\n",
      "epoch no.3 train no.260450  loss = 3.10105 avg_loss = 3.38949\n",
      "epoch no.3 train no.260460  loss = 2.21776 avg_loss = 3.41038\n",
      "epoch no.3 train no.260470  loss = 4.05962 avg_loss = 3.46527\n",
      "epoch no.3 train no.260480  loss = 3.50418 avg_loss = 3.50482\n",
      "epoch no.3 train no.260490  loss = 4.61908 avg_loss = 3.53347\n",
      "epoch no.3 train no.260500  loss = 4.63412 avg_loss = 3.56088\n",
      "epoch no.3 train no.260510  loss = 2.83170 avg_loss = 3.48649\n",
      "epoch no.3 train no.260520  loss = 3.37070 avg_loss = 3.46923\n",
      "epoch no.3 train no.260530  loss = 2.79492 avg_loss = 3.45522\n",
      "epoch no.3 train no.260540  loss = 3.22136 avg_loss = 3.43943\n",
      "epoch no.3 train no.260550  loss = 2.21199 avg_loss = 3.39523\n",
      "epoch no.3 train no.260560  loss = 5.14884 avg_loss = 3.38598\n",
      "epoch no.3 train no.260570  loss = 2.12647 avg_loss = 3.38877\n",
      "epoch no.3 train no.260580  loss = 2.22503 avg_loss = 3.38194\n",
      "epoch no.3 train no.260590  loss = 2.23596 avg_loss = 3.36036\n",
      "epoch no.3 train no.260600  loss = 5.66652 avg_loss = 3.37261\n",
      "epoch no.3 train no.260610  loss = 4.57092 avg_loss = 3.37025\n",
      "epoch no.3 train no.260620  loss = 2.56613 avg_loss = 3.38255\n",
      "epoch no.3 train no.260630  loss = 4.07502 avg_loss = 3.37828\n",
      "epoch no.3 train no.260640  loss = 3.10532 avg_loss = 3.33117\n",
      "epoch no.3 train no.260650  loss = 2.70360 avg_loss = 3.32248\n",
      "epoch no.3 train no.260660  loss = 2.79128 avg_loss = 3.31297\n",
      "epoch no.3 train no.260670  loss = 3.02887 avg_loss = 3.30244\n",
      "epoch no.3 train no.260680  loss = 2.74853 avg_loss = 3.30804\n",
      "epoch no.3 train no.260690  loss = 3.48069 avg_loss = 3.28641\n",
      "epoch no.3 train no.260700  loss = 3.38165 avg_loss = 3.29766\n",
      "epoch no.3 train no.260710  loss = 2.92693 avg_loss = 3.30037\n",
      "epoch no.3 train no.260720  loss = 1.94693 avg_loss = 3.30806\n",
      "epoch no.3 train no.260730  loss = 4.95218 avg_loss = 3.37951\n",
      "epoch no.3 train no.260740  loss = 3.80215 avg_loss = 3.39141\n",
      "epoch no.3 train no.260750  loss = 3.04081 avg_loss = 3.40697\n",
      "epoch no.3 train no.260760  loss = 2.23800 avg_loss = 3.38211\n",
      "epoch no.3 train no.260770  loss = 3.93519 avg_loss = 3.37626\n",
      "epoch no.3 train no.260780  loss = 3.20327 avg_loss = 3.35300\n",
      "epoch no.3 train no.260790  loss = 1.86269 avg_loss = 3.30790\n",
      "epoch no.3 train no.260800  loss = 4.29033 avg_loss = 3.29391\n",
      "epoch no.3 train no.260810  loss = 3.47415 avg_loss = 3.27723\n",
      "epoch no.3 train no.260820  loss = 2.29964 avg_loss = 3.28938\n",
      "epoch no.3 train no.260830  loss = 2.33786 avg_loss = 3.30952\n",
      "epoch no.3 train no.260840  loss = 3.37539 avg_loss = 3.31515\n",
      "epoch no.3 train no.260850  loss = 2.69185 avg_loss = 3.32270\n",
      "epoch no.3 train no.260860  loss = 3.70629 avg_loss = 3.30575\n",
      "epoch no.3 train no.260870  loss = 3.03972 avg_loss = 3.31508\n",
      "epoch no.3 train no.260880  loss = 3.90538 avg_loss = 3.34462\n",
      "epoch no.3 train no.260890  loss = 4.19385 avg_loss = 3.34898\n",
      "epoch no.3 train no.260900  loss = 4.97972 avg_loss = 3.37097\n",
      "epoch no.3 train no.260910  loss = 1.74004 avg_loss = 3.32691\n",
      "epoch no.3 train no.260920  loss = 2.82316 avg_loss = 3.27762\n",
      "epoch no.3 train no.260930  loss = 3.07177 avg_loss = 3.32847\n",
      "epoch no.3 train no.260940  loss = 2.44327 avg_loss = 3.33743\n",
      "epoch no.3 train no.260950  loss = 3.60919 avg_loss = 3.39087\n",
      "epoch no.3 train no.260960  loss = 2.78349 avg_loss = 3.35447\n",
      "epoch no.3 train no.260970  loss = 3.74893 avg_loss = 3.35117\n",
      "epoch no.3 train no.260980  loss = 3.43142 avg_loss = 3.32920\n",
      "epoch no.3 train no.260990  loss = 4.74873 avg_loss = 3.33429\n",
      "epoch no.3 train no.261000  loss = 3.63336 avg_loss = 3.30678\n",
      "5\n",
      "to_tokens: ['▁가을', '▁발라', '팝', '▁명', '▁모음', '음', '</s>']\n",
      "추억의 올드팝송모음</s>\n",
      "epoch no.3 train no.261010  loss = 4.04399 avg_loss = 3.29675\n",
      "epoch no.3 train no.261020  loss = 3.67709 avg_loss = 3.32722\n",
      "epoch no.3 train no.261030  loss = 2.69777 avg_loss = 3.34975\n",
      "epoch no.3 train no.261040  loss = 3.04098 avg_loss = 3.34951\n",
      "epoch no.3 train no.261050  loss = 4.05080 avg_loss = 3.32840\n",
      "epoch no.3 train no.261060  loss = 4.26558 avg_loss = 3.34253\n",
      "epoch no.3 train no.261070  loss = 2.71534 avg_loss = 3.30016\n",
      "epoch no.3 train no.261080  loss = 3.10329 avg_loss = 3.29942\n",
      "epoch no.3 train no.261090  loss = 4.28583 avg_loss = 3.40067\n",
      "epoch no.3 train no.261100  loss = 3.56287 avg_loss = 3.42268\n",
      "epoch no.3 train no.261110  loss = 3.98408 avg_loss = 3.44563\n",
      "epoch no.3 train no.261120  loss = 4.50380 avg_loss = 3.45341\n",
      "epoch no.3 train no.261130  loss = 4.68660 avg_loss = 3.48512\n",
      "epoch no.3 train no.261140  loss = 3.82168 avg_loss = 3.48510\n",
      "epoch no.3 train no.261150  loss = 3.62580 avg_loss = 3.44028\n",
      "epoch no.3 train no.261160  loss = 2.22649 avg_loss = 3.42136\n",
      "epoch no.3 train no.261170  loss = 2.79984 avg_loss = 3.42307\n",
      "epoch no.3 train no.261180  loss = 4.63946 avg_loss = 3.39869\n",
      "epoch no.3 train no.261190  loss = 2.52173 avg_loss = 3.40975\n",
      "epoch no.3 train no.261200  loss = 2.99502 avg_loss = 3.35871\n",
      "epoch no.3 train no.261210  loss = 3.85960 avg_loss = 3.32889\n",
      "epoch no.3 train no.261220  loss = 4.27745 avg_loss = 3.30832\n",
      "epoch no.3 train no.261230  loss = 2.56130 avg_loss = 3.27127\n",
      "epoch no.3 train no.261240  loss = 3.38243 avg_loss = 3.31399\n",
      "epoch no.3 train no.261250  loss = 1.34188 avg_loss = 3.25444\n",
      "epoch no.3 train no.261260  loss = 3.42932 avg_loss = 3.32019\n",
      "epoch no.3 train no.261270  loss = 3.39695 avg_loss = 3.33725\n",
      "epoch no.3 train no.261280  loss = 3.23760 avg_loss = 3.33874\n",
      "epoch no.3 train no.261290  loss = 2.76124 avg_loss = 3.35468\n",
      "epoch no.3 train no.261300  loss = 3.51798 avg_loss = 3.34385\n",
      "epoch no.3 train no.261310  loss = 2.51732 avg_loss = 3.33172\n",
      "epoch no.3 train no.261320  loss = 2.43653 avg_loss = 3.37146\n",
      "epoch no.3 train no.261330  loss = 4.04252 avg_loss = 3.41988\n",
      "epoch no.3 train no.261340  loss = 3.73638 avg_loss = 3.40775\n",
      "epoch no.3 train no.261350  loss = 4.07565 avg_loss = 3.40626\n",
      "epoch no.3 train no.261360  loss = 3.38424 avg_loss = 3.40963\n",
      "epoch no.3 train no.261370  loss = 3.11624 avg_loss = 3.42404\n",
      "epoch no.3 train no.261380  loss = 1.26160 avg_loss = 3.42404\n",
      "epoch no.3 train no.261390  loss = 3.19122 avg_loss = 3.44130\n",
      "epoch no.3 train no.261400  loss = 4.49426 avg_loss = 3.45672\n",
      "epoch no.3 train no.261410  loss = 2.11794 avg_loss = 3.44352\n",
      "epoch no.3 train no.261420  loss = 3.19130 avg_loss = 3.48920\n",
      "epoch no.3 train no.261430  loss = 4.69345 avg_loss = 3.49760\n",
      "epoch no.3 train no.261440  loss = 3.86708 avg_loss = 3.50508\n",
      "epoch no.3 train no.261450  loss = 1.98901 avg_loss = 3.51479\n",
      "epoch no.3 train no.261460  loss = 3.08683 avg_loss = 3.48192\n",
      "epoch no.3 train no.261470  loss = 2.86217 avg_loss = 3.45589\n",
      "epoch no.3 train no.261480  loss = 3.52352 avg_loss = 3.42883\n",
      "epoch no.3 train no.261490  loss = 2.98643 avg_loss = 3.42643\n",
      "epoch no.3 train no.261500  loss = 3.30356 avg_loss = 3.37614\n",
      "epoch no.3 train no.261510  loss = 3.08022 avg_loss = 3.37496\n",
      "epoch no.3 train no.261520  loss = 1.83566 avg_loss = 3.39437\n",
      "epoch no.3 train no.261530  loss = 3.29525 avg_loss = 3.35962\n",
      "epoch no.3 train no.261540  loss = 5.10969 avg_loss = 3.36147\n",
      "epoch no.3 train no.261550  loss = 5.04404 avg_loss = 3.42169\n",
      "epoch no.3 train no.261560  loss = 5.73952 avg_loss = 3.50430\n",
      "epoch no.3 train no.261570  loss = 2.83044 avg_loss = 3.42369\n",
      "epoch no.3 train no.261580  loss = 4.16650 avg_loss = 3.46215\n",
      "epoch no.3 train no.261590  loss = 3.08534 avg_loss = 3.49067\n",
      "epoch no.3 train no.261600  loss = 1.90745 avg_loss = 3.46488\n",
      "epoch no.3 train no.261610  loss = 3.22829 avg_loss = 3.44990\n",
      "epoch no.3 train no.261620  loss = 2.77232 avg_loss = 3.44143\n",
      "epoch no.3 train no.261630  loss = 2.44434 avg_loss = 3.40479\n",
      "epoch no.3 train no.261640  loss = 3.49668 avg_loss = 3.42582\n",
      "epoch no.3 train no.261650  loss = 2.03355 avg_loss = 3.40769\n",
      "epoch no.3 train no.261660  loss = 2.46812 avg_loss = 3.35383\n",
      "epoch no.3 train no.261670  loss = 3.76626 avg_loss = 3.35809\n",
      "epoch no.3 train no.261680  loss = 5.05518 avg_loss = 3.36755\n",
      "epoch no.3 train no.261690  loss = 2.77439 avg_loss = 3.36977\n",
      "epoch no.3 train no.261700  loss = 1.63196 avg_loss = 3.33421\n",
      "epoch no.3 train no.261710  loss = 2.80791 avg_loss = 3.33545\n",
      "epoch no.3 train no.261720  loss = 4.33785 avg_loss = 3.35508\n",
      "epoch no.3 train no.261730  loss = 2.38366 avg_loss = 3.36825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.261740  loss = 4.80048 avg_loss = 3.40764\n",
      "epoch no.3 train no.261750  loss = 4.69022 avg_loss = 3.41276\n",
      "epoch no.3 train no.261760  loss = 3.04298 avg_loss = 3.43438\n",
      "epoch no.3 train no.261770  loss = 3.34262 avg_loss = 3.40374\n",
      "epoch no.3 train no.261780  loss = 3.82914 avg_loss = 3.41782\n",
      "epoch no.3 train no.261790  loss = 2.63604 avg_loss = 3.35197\n",
      "epoch no.3 train no.261800  loss = 4.76736 avg_loss = 3.38206\n",
      "epoch no.3 train no.261810  loss = 6.57539 avg_loss = 3.38173\n",
      "epoch no.3 train no.261820  loss = 3.90979 avg_loss = 3.39273\n",
      "epoch no.3 train no.261830  loss = 3.23264 avg_loss = 3.41473\n",
      "epoch no.3 train no.261840  loss = 2.87724 avg_loss = 3.44866\n",
      "epoch no.3 train no.261850  loss = 4.42796 avg_loss = 3.46896\n",
      "epoch no.3 train no.261860  loss = 3.13855 avg_loss = 3.43537\n",
      "epoch no.3 train no.261870  loss = 6.33428 avg_loss = 3.42708\n",
      "epoch no.3 train no.261880  loss = 2.54610 avg_loss = 3.44471\n",
      "epoch no.3 train no.261890  loss = 4.03223 avg_loss = 3.44452\n",
      "epoch no.3 train no.261900  loss = 2.52950 avg_loss = 3.40616\n",
      "epoch no.3 train no.261910  loss = 2.72458 avg_loss = 3.37977\n",
      "epoch no.3 train no.261920  loss = 2.48919 avg_loss = 3.40243\n",
      "epoch no.3 train no.261930  loss = 2.54570 avg_loss = 3.38590\n",
      "epoch no.3 train no.261940  loss = 2.79010 avg_loss = 3.42001\n",
      "epoch no.3 train no.261950  loss = 1.99189 avg_loss = 3.40152\n",
      "epoch no.3 train no.261960  loss = 3.50657 avg_loss = 3.40452\n",
      "epoch no.3 train no.261970  loss = 2.65733 avg_loss = 3.40546\n",
      "epoch no.3 train no.261980  loss = 3.70824 avg_loss = 3.39790\n",
      "epoch no.3 train no.261990  loss = 3.11636 avg_loss = 3.37518\n",
      "epoch no.3 train no.262000  loss = 2.66454 avg_loss = 3.32641\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.3 train no.262010  loss = 3.52485 avg_loss = 3.34170\n",
      "epoch no.3 train no.262020  loss = 2.73740 avg_loss = 3.30710\n",
      "epoch no.3 train no.262030  loss = 4.50275 avg_loss = 3.37239\n",
      "epoch no.3 train no.262040  loss = 3.74040 avg_loss = 3.36752\n",
      "epoch no.3 train no.262050  loss = 2.81847 avg_loss = 3.35999\n",
      "epoch no.3 train no.262060  loss = 4.40864 avg_loss = 3.38545\n",
      "epoch no.3 train no.262070  loss = 3.09653 avg_loss = 3.41047\n",
      "epoch no.3 train no.262080  loss = 2.78579 avg_loss = 3.42441\n",
      "epoch no.3 train no.262090  loss = 2.86339 avg_loss = 3.40539\n",
      "epoch no.3 train no.262100  loss = 6.30600 avg_loss = 3.46638\n",
      "epoch no.3 train no.262110  loss = 3.25140 avg_loss = 3.43181\n",
      "epoch no.3 train no.262120  loss = 3.40997 avg_loss = 3.42419\n",
      "epoch no.3 train no.262130  loss = 3.08203 avg_loss = 3.43101\n",
      "epoch no.3 train no.262140  loss = 2.41730 avg_loss = 3.41216\n",
      "epoch no.3 train no.262150  loss = 1.97762 avg_loss = 3.33285\n",
      "epoch no.3 train no.262160  loss = 4.02603 avg_loss = 3.34789\n",
      "epoch no.3 train no.262170  loss = 3.97168 avg_loss = 3.36331\n",
      "epoch no.3 train no.262180  loss = 1.94192 avg_loss = 3.31680\n",
      "epoch no.3 train no.262190  loss = 3.03753 avg_loss = 3.29932\n",
      "epoch no.3 train no.262200  loss = 4.81381 avg_loss = 3.27099\n",
      "epoch no.3 train no.262210  loss = 3.77420 avg_loss = 3.26972\n",
      "epoch no.3 train no.262220  loss = 4.64170 avg_loss = 3.28202\n",
      "epoch no.3 train no.262230  loss = 4.87748 avg_loss = 3.29912\n",
      "epoch no.3 train no.262240  loss = 3.57171 avg_loss = 3.31611\n",
      "epoch no.3 train no.262250  loss = 2.94696 avg_loss = 3.31690\n",
      "epoch no.3 train no.262260  loss = 3.47549 avg_loss = 3.32055\n",
      "epoch no.3 train no.262270  loss = 1.40613 avg_loss = 3.26963\n",
      "epoch no.3 train no.262280  loss = 2.21010 avg_loss = 3.26328\n",
      "epoch no.3 train no.262290  loss = 2.41329 avg_loss = 3.21841\n",
      "epoch no.3 train no.262300  loss = 2.16985 avg_loss = 3.20318\n",
      "epoch no.3 train no.262310  loss = 2.91566 avg_loss = 3.20524\n",
      "epoch no.3 train no.262320  loss = 2.63914 avg_loss = 3.26455\n",
      "epoch no.3 train no.262330  loss = 1.89553 avg_loss = 3.22065\n",
      "epoch no.3 train no.262340  loss = 2.08537 avg_loss = 3.21934\n",
      "epoch no.3 train no.262350  loss = 3.17264 avg_loss = 3.21592\n",
      "epoch no.3 train no.262360  loss = 3.75953 avg_loss = 3.24533\n",
      "epoch no.3 train no.262370  loss = 3.13391 avg_loss = 3.28079\n",
      "epoch no.3 train no.262380  loss = 3.51840 avg_loss = 3.31153\n",
      "epoch no.3 train no.262390  loss = 2.65785 avg_loss = 3.29810\n",
      "epoch no.3 train no.262400  loss = 2.25689 avg_loss = 3.32125\n",
      "epoch no.3 train no.262410  loss = 2.73525 avg_loss = 3.29296\n",
      "epoch no.3 train no.262420  loss = 3.68369 avg_loss = 3.30716\n",
      "epoch no.3 train no.262430  loss = 2.25097 avg_loss = 3.29604\n",
      "epoch no.3 train no.262440  loss = 1.91309 avg_loss = 3.30819\n",
      "epoch no.3 train no.262450  loss = 2.56902 avg_loss = 3.30387\n",
      "epoch no.3 train no.262460  loss = 2.79638 avg_loss = 3.28602\n",
      "epoch no.3 train no.262470  loss = 4.00914 avg_loss = 3.34322\n",
      "epoch no.3 train no.262480  loss = 2.98316 avg_loss = 3.33580\n",
      "epoch no.3 train no.262490  loss = 1.69919 avg_loss = 3.33841\n",
      "epoch no.3 train no.262500  loss = 2.62215 avg_loss = 3.34652\n",
      "epoch no.3 train no.262510  loss = 3.91426 avg_loss = 3.35951\n",
      "epoch no.3 train no.262520  loss = 4.39453 avg_loss = 3.31247\n",
      "epoch no.3 train no.262530  loss = 5.04358 avg_loss = 3.31643\n",
      "epoch no.3 train no.262540  loss = 2.48047 avg_loss = 3.27694\n",
      "epoch no.3 train no.262550  loss = 3.84396 avg_loss = 3.31784\n",
      "epoch no.3 train no.262560  loss = 2.91162 avg_loss = 3.30380\n",
      "epoch no.3 train no.262570  loss = 4.51243 avg_loss = 3.35794\n",
      "epoch no.3 train no.262580  loss = 3.32001 avg_loss = 3.35028\n",
      "epoch no.3 train no.262590  loss = 4.18076 avg_loss = 3.37162\n",
      "epoch no.3 train no.262600  loss = 3.73269 avg_loss = 3.38735\n",
      "epoch no.3 train no.262610  loss = 3.35894 avg_loss = 3.40988\n",
      "epoch no.3 train no.262620  loss = 4.22675 avg_loss = 3.45144\n",
      "epoch no.3 train no.262630  loss = 3.34036 avg_loss = 3.48488\n",
      "epoch no.3 train no.262640  loss = 3.87162 avg_loss = 3.45250\n",
      "epoch no.3 train no.262650  loss = 1.68899 avg_loss = 3.41407\n",
      "epoch no.3 train no.262660  loss = 3.74851 avg_loss = 3.38261\n",
      "epoch no.3 train no.262670  loss = 3.66729 avg_loss = 3.35632\n",
      "epoch no.3 train no.262680  loss = 3.30938 avg_loss = 3.35351\n",
      "epoch no.3 train no.262690  loss = 3.55492 avg_loss = 3.35513\n",
      "epoch no.3 train no.262700  loss = 2.68928 avg_loss = 3.36626\n",
      "epoch no.3 train no.262710  loss = 2.28038 avg_loss = 3.32661\n",
      "epoch no.3 train no.262720  loss = 3.74086 avg_loss = 3.31877\n",
      "epoch no.3 train no.262730  loss = 2.44156 avg_loss = 3.31223\n",
      "epoch no.3 train no.262740  loss = 3.05680 avg_loss = 3.30365\n",
      "epoch no.3 train no.262750  loss = 2.29264 avg_loss = 3.28402\n",
      "epoch no.3 train no.262760  loss = 2.92143 avg_loss = 3.29916\n",
      "epoch no.3 train no.262770  loss = 2.07734 avg_loss = 3.30813\n",
      "epoch no.3 train no.262780  loss = 2.84960 avg_loss = 3.33385\n",
      "epoch no.3 train no.262790  loss = 3.92639 avg_loss = 3.31963\n",
      "epoch no.3 train no.262800  loss = 3.25612 avg_loss = 3.31473\n",
      "epoch no.3 train no.262810  loss = 2.32725 avg_loss = 3.30199\n",
      "epoch no.3 train no.262820  loss = 3.05402 avg_loss = 3.35988\n",
      "epoch no.3 train no.262830  loss = 4.57356 avg_loss = 3.37340\n",
      "epoch no.3 train no.262840  loss = 4.24553 avg_loss = 3.35869\n",
      "epoch no.3 train no.262850  loss = 3.26691 avg_loss = 3.36306\n",
      "epoch no.3 train no.262860  loss = 3.29096 avg_loss = 3.34792\n",
      "epoch no.3 train no.262870  loss = 4.28800 avg_loss = 3.36607\n",
      "epoch no.3 train no.262880  loss = 2.29940 avg_loss = 3.38984\n",
      "epoch no.3 train no.262890  loss = 2.36582 avg_loss = 3.40034\n",
      "epoch no.3 train no.262900  loss = 3.16186 avg_loss = 3.39729\n",
      "epoch no.3 train no.262910  loss = 2.74269 avg_loss = 3.35048\n",
      "epoch no.3 train no.262920  loss = 3.94424 avg_loss = 3.38324\n",
      "epoch no.3 train no.262930  loss = 3.95573 avg_loss = 3.37632\n",
      "epoch no.3 train no.262940  loss = 2.61697 avg_loss = 3.38033\n",
      "epoch no.3 train no.262950  loss = 2.68600 avg_loss = 3.40540\n",
      "epoch no.3 train no.262960  loss = 1.66836 avg_loss = 3.37537\n",
      "epoch no.3 train no.262970  loss = 2.58774 avg_loss = 3.40591\n",
      "epoch no.3 train no.262980  loss = 3.95382 avg_loss = 3.35110\n",
      "epoch no.3 train no.262990  loss = 2.72533 avg_loss = 3.35724\n",
      "epoch no.3 train no.263000  loss = 1.90175 avg_loss = 3.32333\n",
      "6\n",
      "to_tokens: ['▁가을', '▁그', '▁노래', '드', '▁모음', '곡', '</s>', '</s>']\n",
      "추억의 아이돌 발라드 명곡 모음</s>\n",
      "epoch no.3 train no.263010  loss = 2.90715 avg_loss = 3.28761\n",
      "epoch no.3 train no.263020  loss = 2.21600 avg_loss = 3.26287\n",
      "epoch no.3 train no.263030  loss = 3.30991 avg_loss = 3.27421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.263040  loss = 3.66831 avg_loss = 3.20839\n",
      "epoch no.3 train no.263050  loss = 3.74581 avg_loss = 3.23609\n",
      "epoch no.3 train no.263060  loss = 3.26845 avg_loss = 3.21835\n",
      "epoch no.3 train no.263070  loss = 2.46843 avg_loss = 3.24561\n",
      "epoch no.3 train no.263080  loss = 2.00873 avg_loss = 3.24010\n",
      "epoch no.3 train no.263090  loss = 4.86998 avg_loss = 3.27124\n",
      "epoch no.3 train no.263100  loss = 3.10155 avg_loss = 3.30139\n",
      "epoch no.3 train no.263110  loss = 3.73186 avg_loss = 3.31212\n",
      "epoch no.3 train no.263120  loss = 3.32206 avg_loss = 3.28057\n",
      "epoch no.3 train no.263130  loss = 4.55702 avg_loss = 3.28535\n",
      "epoch no.3 train no.263140  loss = 2.84640 avg_loss = 3.29558\n",
      "epoch no.3 train no.263150  loss = 4.03892 avg_loss = 3.30521\n",
      "epoch no.3 train no.263160  loss = 2.50625 avg_loss = 3.29545\n",
      "epoch no.3 train no.263170  loss = 4.22755 avg_loss = 3.31511\n",
      "epoch no.3 train no.263180  loss = 3.12839 avg_loss = 3.31277\n",
      "epoch no.3 train no.263190  loss = 3.20098 avg_loss = 3.35245\n",
      "epoch no.3 train no.263200  loss = 4.43922 avg_loss = 3.37763\n",
      "epoch no.3 train no.263210  loss = 2.96916 avg_loss = 3.40197\n",
      "epoch no.3 train no.263220  loss = 3.47212 avg_loss = 3.42249\n",
      "epoch no.3 train no.263230  loss = 2.59942 avg_loss = 3.40385\n",
      "epoch no.3 train no.263240  loss = 3.43896 avg_loss = 3.38018\n",
      "epoch no.3 train no.263250  loss = 2.89423 avg_loss = 3.42696\n",
      "epoch no.3 train no.263260  loss = 3.09984 avg_loss = 3.47145\n",
      "epoch no.3 train no.263270  loss = 3.44987 avg_loss = 3.46960\n",
      "epoch no.3 train no.263280  loss = 3.74188 avg_loss = 3.42504\n",
      "epoch no.3 train no.263290  loss = 2.60021 avg_loss = 3.42169\n",
      "epoch no.3 train no.263300  loss = 3.40745 avg_loss = 3.45227\n",
      "epoch no.3 train no.263310  loss = 3.13476 avg_loss = 3.40023\n",
      "epoch no.3 train no.263320  loss = 3.15228 avg_loss = 3.40323\n",
      "epoch no.3 train no.263330  loss = 4.95002 avg_loss = 3.37890\n",
      "epoch no.3 train no.263340  loss = 3.71666 avg_loss = 3.41451\n",
      "epoch no.3 train no.263350  loss = 3.37219 avg_loss = 3.44130\n",
      "epoch no.3 train no.263360  loss = 2.93989 avg_loss = 3.37845\n",
      "epoch no.3 train no.263370  loss = 1.64365 avg_loss = 3.32026\n",
      "epoch no.3 train no.263380  loss = 3.15411 avg_loss = 3.30153\n",
      "epoch no.3 train no.263390  loss = 2.72914 avg_loss = 3.30045\n",
      "epoch no.3 train no.263400  loss = 2.79880 avg_loss = 3.31142\n",
      "epoch no.3 train no.263410  loss = 3.29047 avg_loss = 3.29986\n",
      "epoch no.3 train no.263420  loss = 4.01207 avg_loss = 3.32534\n",
      "epoch no.3 train no.263430  loss = 2.62221 avg_loss = 3.32098\n",
      "epoch no.3 train no.263440  loss = 2.36464 avg_loss = 3.32076\n",
      "epoch no.3 train no.263450  loss = 2.54415 avg_loss = 3.27172\n",
      "epoch no.3 train no.263460  loss = 2.31174 avg_loss = 3.25601\n",
      "epoch no.3 train no.263470  loss = 2.69079 avg_loss = 3.31062\n",
      "epoch no.3 train no.263480  loss = 2.93592 avg_loss = 3.32582\n",
      "epoch no.3 train no.263490  loss = 2.37720 avg_loss = 3.34087\n",
      "epoch no.3 train no.263500  loss = 3.16535 avg_loss = 3.29355\n",
      "epoch no.3 train no.263510  loss = 4.19684 avg_loss = 3.31958\n",
      "epoch no.3 train no.263520  loss = 4.52598 avg_loss = 3.31668\n",
      "epoch no.3 train no.263530  loss = 4.32397 avg_loss = 3.31951\n",
      "epoch no.3 train no.263540  loss = 3.06997 avg_loss = 3.32238\n",
      "epoch no.3 train no.263550  loss = 2.96726 avg_loss = 3.33910\n",
      "epoch no.3 train no.263560  loss = 3.62774 avg_loss = 3.33835\n",
      "epoch no.3 train no.263570  loss = 2.72451 avg_loss = 3.34510\n",
      "epoch no.3 train no.263580  loss = 2.70113 avg_loss = 3.37786\n",
      "epoch no.3 train no.263590  loss = 3.63110 avg_loss = 3.36201\n",
      "epoch no.3 train no.263600  loss = 2.01844 avg_loss = 3.37304\n",
      "epoch no.3 train no.263610  loss = 4.01479 avg_loss = 3.41809\n",
      "epoch no.3 train no.263620  loss = 3.29625 avg_loss = 3.40171\n",
      "epoch no.3 train no.263630  loss = 2.42853 avg_loss = 3.36199\n",
      "epoch no.3 train no.263640  loss = 2.76967 avg_loss = 3.33970\n",
      "epoch no.3 train no.263650  loss = 3.76491 avg_loss = 3.35152\n",
      "epoch no.3 train no.263660  loss = 2.55200 avg_loss = 3.33905\n",
      "epoch no.3 train no.263670  loss = 3.98484 avg_loss = 3.33881\n",
      "epoch no.3 train no.263680  loss = 5.67792 avg_loss = 3.33662\n",
      "epoch no.3 train no.263690  loss = 3.49357 avg_loss = 3.30484\n",
      "epoch no.3 train no.263700  loss = 2.70875 avg_loss = 3.31223\n",
      "epoch no.3 train no.263710  loss = 3.99139 avg_loss = 3.34094\n",
      "epoch no.3 train no.263720  loss = 3.09691 avg_loss = 3.36192\n",
      "epoch no.3 train no.263730  loss = 1.75736 avg_loss = 3.30118\n",
      "epoch no.3 train no.263740  loss = 2.82778 avg_loss = 3.33135\n",
      "epoch no.3 train no.263750  loss = 3.24597 avg_loss = 3.35041\n",
      "epoch no.3 train no.263760  loss = 4.67348 avg_loss = 3.40452\n",
      "epoch no.3 train no.263770  loss = 3.10728 avg_loss = 3.45096\n",
      "epoch no.3 train no.263780  loss = 4.05375 avg_loss = 3.44482\n",
      "epoch no.3 train no.263790  loss = 3.41596 avg_loss = 3.41739\n",
      "epoch no.3 train no.263800  loss = 4.18922 avg_loss = 3.42737\n",
      "epoch no.3 train no.263810  loss = 4.24476 avg_loss = 3.48325\n",
      "epoch no.3 train no.263820  loss = 3.59624 avg_loss = 3.41898\n",
      "epoch no.3 train no.263830  loss = 4.70432 avg_loss = 3.40078\n",
      "epoch no.3 train no.263840  loss = 2.65270 avg_loss = 3.40968\n",
      "epoch no.3 train no.263850  loss = 3.27854 avg_loss = 3.45275\n",
      "epoch no.3 train no.263860  loss = 1.68925 avg_loss = 3.39193\n",
      "epoch no.3 train no.263870  loss = 5.05537 avg_loss = 3.40897\n",
      "epoch no.3 train no.263880  loss = 3.07537 avg_loss = 3.41590\n",
      "epoch no.3 train no.263890  loss = 2.03170 avg_loss = 3.43088\n",
      "epoch no.3 train no.263900  loss = 3.00315 avg_loss = 3.42011\n",
      "epoch no.3 train no.263910  loss = 4.47612 avg_loss = 3.46391\n",
      "epoch no.3 train no.263920  loss = 2.64380 avg_loss = 3.42154\n",
      "epoch no.3 train no.263930  loss = 2.31428 avg_loss = 3.41161\n",
      "epoch no.3 train no.263940  loss = 3.54509 avg_loss = 3.41580\n",
      "epoch no.3 train no.263950  loss = 3.00370 avg_loss = 3.39001\n",
      "epoch no.3 train no.263960  loss = 3.13966 avg_loss = 3.37802\n",
      "epoch no.3 train no.263970  loss = 2.53504 avg_loss = 3.35384\n",
      "epoch no.3 train no.263980  loss = 3.76838 avg_loss = 3.35607\n",
      "epoch no.3 train no.263990  loss = 2.78602 avg_loss = 3.31412\n",
      "epoch no.3 train no.264000  loss = 3.63537 avg_loss = 3.33707\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.264010  loss = 3.81318 avg_loss = 3.32168\n",
      "epoch no.3 train no.264020  loss = 3.78851 avg_loss = 3.33823\n",
      "epoch no.3 train no.264030  loss = 4.05550 avg_loss = 3.37110\n",
      "epoch no.3 train no.264040  loss = 3.98436 avg_loss = 3.38497\n",
      "epoch no.3 train no.264050  loss = 5.51980 avg_loss = 3.47501\n",
      "epoch no.3 train no.264060  loss = 2.44405 avg_loss = 3.43271\n",
      "epoch no.3 train no.264070  loss = 1.90365 avg_loss = 3.41817\n",
      "epoch no.3 train no.264080  loss = 2.67150 avg_loss = 3.40315\n",
      "epoch no.3 train no.264090  loss = 5.25062 avg_loss = 3.42391\n",
      "epoch no.3 train no.264100  loss = 3.02168 avg_loss = 3.43899\n",
      "epoch no.3 train no.264110  loss = 2.91739 avg_loss = 3.43867\n",
      "epoch no.3 train no.264120  loss = 1.77076 avg_loss = 3.36751\n",
      "epoch no.3 train no.264130  loss = 2.33320 avg_loss = 3.34988\n",
      "epoch no.3 train no.264140  loss = 3.75749 avg_loss = 3.37433\n",
      "epoch no.3 train no.264150  loss = 2.78452 avg_loss = 3.33910\n",
      "epoch no.3 train no.264160  loss = 2.61204 avg_loss = 3.32358\n",
      "epoch no.3 train no.264170  loss = 7.53524 avg_loss = 3.33275\n",
      "epoch no.3 train no.264180  loss = 2.56710 avg_loss = 3.29850\n",
      "epoch no.3 train no.264190  loss = 2.94011 avg_loss = 3.24929\n",
      "epoch no.3 train no.264200  loss = 2.87282 avg_loss = 3.25052\n",
      "epoch no.3 train no.264210  loss = 3.04152 avg_loss = 3.25572\n",
      "epoch no.3 train no.264220  loss = 2.52522 avg_loss = 3.30539\n",
      "epoch no.3 train no.264230  loss = 2.69307 avg_loss = 3.28419\n",
      "epoch no.3 train no.264240  loss = 4.41875 avg_loss = 3.28613\n",
      "epoch no.3 train no.264250  loss = 4.35275 avg_loss = 3.30911\n",
      "epoch no.3 train no.264260  loss = 6.21335 avg_loss = 3.36961\n",
      "epoch no.3 train no.264270  loss = 2.50569 avg_loss = 3.32947\n",
      "epoch no.3 train no.264280  loss = 2.25876 avg_loss = 3.30622\n",
      "epoch no.3 train no.264290  loss = 2.53282 avg_loss = 3.30745\n",
      "epoch no.3 train no.264300  loss = 2.83611 avg_loss = 3.31507\n",
      "epoch no.3 train no.264310  loss = 2.57844 avg_loss = 3.31093\n",
      "epoch no.3 train no.264320  loss = 3.20174 avg_loss = 3.35685\n",
      "epoch no.3 train no.264330  loss = 4.71996 avg_loss = 3.36501\n",
      "epoch no.3 train no.264340  loss = 5.16905 avg_loss = 3.38059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.264350  loss = 2.76741 avg_loss = 3.38363\n",
      "epoch no.3 train no.264360  loss = 2.85588 avg_loss = 3.39840\n",
      "epoch no.3 train no.264370  loss = 4.43515 avg_loss = 3.39586\n",
      "epoch no.3 train no.264380  loss = 2.70385 avg_loss = 3.36486\n",
      "epoch no.3 train no.264390  loss = 2.32057 avg_loss = 3.35955\n",
      "epoch no.3 train no.264400  loss = 4.24725 avg_loss = 3.33374\n",
      "epoch no.3 train no.264410  loss = 4.07012 avg_loss = 3.30471\n",
      "epoch no.3 train no.264420  loss = 4.45041 avg_loss = 3.34251\n",
      "epoch no.3 train no.264430  loss = 3.75564 avg_loss = 3.31058\n",
      "epoch no.3 train no.264440  loss = 4.51275 avg_loss = 3.30458\n",
      "epoch no.3 train no.264450  loss = 3.43196 avg_loss = 3.29159\n",
      "epoch no.3 train no.264460  loss = 2.81397 avg_loss = 3.30322\n",
      "epoch no.3 train no.264470  loss = 3.45576 avg_loss = 3.33626\n",
      "epoch no.3 train no.264480  loss = 3.39017 avg_loss = 3.33702\n",
      "epoch no.3 train no.264490  loss = 4.99359 avg_loss = 3.33198\n",
      "epoch no.3 train no.264500  loss = 3.75408 avg_loss = 3.37948\n",
      "epoch no.3 train no.264510  loss = 3.35952 avg_loss = 3.40955\n",
      "epoch no.3 train no.264520  loss = 3.24125 avg_loss = 3.37255\n",
      "epoch no.3 train no.264530  loss = 3.15294 avg_loss = 3.40006\n",
      "epoch no.3 train no.264540  loss = 4.27022 avg_loss = 3.34041\n",
      "epoch no.3 train no.264550  loss = 2.09198 avg_loss = 3.32785\n",
      "epoch no.3 train no.264560  loss = 4.22228 avg_loss = 3.36509\n",
      "epoch no.3 train no.264570  loss = 4.36250 avg_loss = 3.36031\n",
      "epoch no.3 train no.264580  loss = 2.17541 avg_loss = 3.32487\n",
      "epoch no.3 train no.264590  loss = 2.67224 avg_loss = 3.33258\n",
      "epoch no.3 train no.264600  loss = 3.05160 avg_loss = 3.34863\n",
      "epoch no.3 train no.264610  loss = 3.80364 avg_loss = 3.37301\n",
      "epoch no.3 train no.264620  loss = 3.27591 avg_loss = 3.37338\n",
      "epoch no.3 train no.264630  loss = 5.57288 avg_loss = 3.35685\n",
      "epoch no.3 train no.264640  loss = 2.78494 avg_loss = 3.35189\n",
      "epoch no.3 train no.264650  loss = 3.89992 avg_loss = 3.37761\n",
      "epoch no.3 train no.264660  loss = 4.83773 avg_loss = 3.39374\n",
      "epoch no.3 train no.264670  loss = 3.74355 avg_loss = 3.37283\n",
      "epoch no.3 train no.264680  loss = 2.34306 avg_loss = 3.37461\n",
      "epoch no.3 train no.264690  loss = 4.94712 avg_loss = 3.41188\n",
      "epoch no.3 train no.264700  loss = 3.11848 avg_loss = 3.40855\n",
      "epoch no.3 train no.264710  loss = 3.07547 avg_loss = 3.36729\n",
      "epoch no.3 train no.264720  loss = 2.85781 avg_loss = 3.37589\n",
      "epoch no.3 train no.264730  loss = 3.26007 avg_loss = 3.35116\n",
      "epoch no.3 train no.264740  loss = 2.66129 avg_loss = 3.35489\n",
      "epoch no.3 train no.264750  loss = 4.49582 avg_loss = 3.36552\n",
      "epoch no.3 train no.264760  loss = 2.41988 avg_loss = 3.36594\n",
      "epoch no.3 train no.264770  loss = 2.23507 avg_loss = 3.35039\n",
      "epoch no.3 train no.264780  loss = 2.53687 avg_loss = 3.28066\n",
      "epoch no.3 train no.264790  loss = 2.28591 avg_loss = 3.29230\n",
      "epoch no.3 train no.264800  loss = 2.17680 avg_loss = 3.31411\n",
      "epoch no.3 train no.264810  loss = 3.63488 avg_loss = 3.28340\n",
      "epoch no.3 train no.264820  loss = 4.12113 avg_loss = 3.29752\n",
      "epoch no.3 train no.264830  loss = 2.90977 avg_loss = 3.31431\n",
      "epoch no.3 train no.264840  loss = 3.62430 avg_loss = 3.31307\n",
      "epoch no.3 train no.264850  loss = 3.29587 avg_loss = 3.29858\n",
      "epoch no.3 train no.264860  loss = 2.19810 avg_loss = 3.32797\n",
      "epoch no.3 train no.264870  loss = 2.99750 avg_loss = 3.30871\n",
      "epoch no.3 train no.264880  loss = 4.16892 avg_loss = 3.31575\n",
      "epoch no.3 train no.264890  loss = 5.51546 avg_loss = 3.30936\n",
      "epoch no.3 train no.264900  loss = 4.45848 avg_loss = 3.30863\n",
      "epoch no.3 train no.264910  loss = 2.75390 avg_loss = 3.34498\n",
      "epoch no.3 train no.264920  loss = 2.44956 avg_loss = 3.32993\n",
      "epoch no.3 train no.264930  loss = 2.60547 avg_loss = 3.33617\n",
      "epoch no.3 train no.264940  loss = 2.44642 avg_loss = 3.30604\n",
      "epoch no.3 train no.264950  loss = 3.13865 avg_loss = 3.30908\n",
      "epoch no.3 train no.264960  loss = 3.77374 avg_loss = 3.25751\n",
      "epoch no.3 train no.264970  loss = 2.92344 avg_loss = 3.25194\n",
      "epoch no.3 train no.264980  loss = 2.76792 avg_loss = 3.27261\n",
      "epoch no.3 train no.264990  loss = 6.13688 avg_loss = 3.26159\n",
      "epoch no.3 train no.265000  loss = 2.06504 avg_loss = 3.26902\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '드', '▁모음', '집', '</s>']\n",
      "추억의 발라드  모음집</s>\n",
      "epoch no.3 train no.265010  loss = 1.72510 avg_loss = 3.27082\n",
      "epoch no.3 train no.265020  loss = 4.46755 avg_loss = 3.28575\n",
      "epoch no.3 train no.265030  loss = 2.47744 avg_loss = 3.33322\n",
      "epoch no.3 train no.265040  loss = 2.96606 avg_loss = 3.32067\n",
      "epoch no.3 train no.265050  loss = 2.54514 avg_loss = 3.28871\n",
      "epoch no.3 train no.265060  loss = 2.42612 avg_loss = 3.28171\n",
      "epoch no.3 train no.265070  loss = 3.17790 avg_loss = 3.25406\n",
      "epoch no.3 train no.265080  loss = 2.78198 avg_loss = 3.28999\n",
      "epoch no.3 train no.265090  loss = 3.99772 avg_loss = 3.27346\n",
      "epoch no.3 train no.265100  loss = 3.18100 avg_loss = 3.32484\n",
      "epoch no.3 train no.265110  loss = 2.80690 avg_loss = 3.29228\n",
      "epoch no.3 train no.265120  loss = 3.60932 avg_loss = 3.27741\n",
      "epoch no.3 train no.265130  loss = 2.53332 avg_loss = 3.28098\n",
      "epoch no.3 train no.265140  loss = 3.41950 avg_loss = 3.27466\n",
      "epoch no.3 train no.265150  loss = 5.50366 avg_loss = 3.30535\n",
      "epoch no.3 train no.265160  loss = 2.96971 avg_loss = 3.34510\n",
      "epoch no.3 train no.265170  loss = 5.44656 avg_loss = 3.33974\n",
      "epoch no.3 train no.265180  loss = 3.28870 avg_loss = 3.36510\n",
      "epoch no.3 train no.265190  loss = 1.90629 avg_loss = 3.34167\n",
      "epoch no.3 train no.265200  loss = 2.54582 avg_loss = 3.30365\n",
      "epoch no.3 train no.265210  loss = 3.77671 avg_loss = 3.31755\n",
      "epoch no.3 train no.265220  loss = 3.56365 avg_loss = 3.30353\n",
      "epoch no.3 train no.265230  loss = 4.13575 avg_loss = 3.31691\n",
      "epoch no.3 train no.265240  loss = 3.50102 avg_loss = 3.31725\n",
      "epoch no.3 train no.265250  loss = 2.48131 avg_loss = 3.31010\n",
      "epoch no.3 train no.265260  loss = 2.21723 avg_loss = 3.29853\n",
      "epoch no.3 train no.265270  loss = 2.65667 avg_loss = 3.29548\n",
      "epoch no.3 train no.265280  loss = 4.15899 avg_loss = 3.31529\n",
      "epoch no.3 train no.265290  loss = 3.27053 avg_loss = 3.29227\n",
      "epoch no.3 train no.265300  loss = 2.46925 avg_loss = 3.27422\n",
      "epoch no.3 train no.265310  loss = 3.29153 avg_loss = 3.24849\n",
      "epoch no.3 train no.265320  loss = 3.01226 avg_loss = 3.24027\n",
      "epoch no.3 train no.265330  loss = 4.08783 avg_loss = 3.20368\n",
      "epoch no.3 train no.265340  loss = 4.80202 avg_loss = 3.25166\n",
      "epoch no.3 train no.265350  loss = 2.99816 avg_loss = 3.23140\n",
      "epoch no.3 train no.265360  loss = 3.58494 avg_loss = 3.25010\n",
      "epoch no.3 train no.265370  loss = 4.98731 avg_loss = 3.28285\n",
      "epoch no.3 train no.265380  loss = 5.56227 avg_loss = 3.31276\n",
      "epoch no.3 train no.265390  loss = 4.68458 avg_loss = 3.31841\n",
      "epoch no.3 train no.265400  loss = 3.18937 avg_loss = 3.33882\n",
      "epoch no.3 train no.265410  loss = 2.38762 avg_loss = 3.33177\n",
      "epoch no.3 train no.265420  loss = 2.73449 avg_loss = 3.31591\n",
      "epoch no.3 train no.265430  loss = 3.95062 avg_loss = 3.33775\n",
      "epoch no.3 train no.265440  loss = 4.83543 avg_loss = 3.35858\n",
      "epoch no.3 train no.265450  loss = 6.30092 avg_loss = 3.37228\n",
      "epoch no.3 train no.265460  loss = 2.89275 avg_loss = 3.32430\n",
      "epoch no.3 train no.265470  loss = 3.22242 avg_loss = 3.32658\n",
      "epoch no.3 train no.265480  loss = 2.60742 avg_loss = 3.29742\n",
      "epoch no.3 train no.265490  loss = 5.81838 avg_loss = 3.37035\n",
      "epoch no.3 train no.265500  loss = 4.67225 avg_loss = 3.33215\n",
      "epoch no.3 train no.265510  loss = 2.20941 avg_loss = 3.34527\n",
      "epoch no.3 train no.265520  loss = 3.39314 avg_loss = 3.33303\n",
      "epoch no.3 train no.265530  loss = 4.33242 avg_loss = 3.34339\n",
      "epoch no.3 train no.265540  loss = 3.80382 avg_loss = 3.33225\n",
      "epoch no.3 train no.265550  loss = 3.05783 avg_loss = 3.27662\n",
      "epoch no.3 train no.265560  loss = 3.29715 avg_loss = 3.26861\n",
      "epoch no.3 train no.265570  loss = 1.96453 avg_loss = 3.25713\n",
      "epoch no.3 train no.265580  loss = 2.70732 avg_loss = 3.26388\n",
      "epoch no.3 train no.265590  loss = 4.21665 avg_loss = 3.33007\n",
      "epoch no.3 train no.265600  loss = 4.39094 avg_loss = 3.31284\n",
      "epoch no.3 train no.265610  loss = 2.76280 avg_loss = 3.29536\n",
      "epoch no.3 train no.265620  loss = 2.93150 avg_loss = 3.34806\n",
      "epoch no.3 train no.265630  loss = 3.53069 avg_loss = 3.36009\n",
      "epoch no.3 train no.265640  loss = 1.91147 avg_loss = 3.34472\n",
      "epoch no.3 train no.265650  loss = 3.54314 avg_loss = 3.31136\n",
      "epoch no.3 train no.265660  loss = 1.94101 avg_loss = 3.30909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.265670  loss = 3.96388 avg_loss = 3.27655\n",
      "epoch no.3 train no.265680  loss = 4.72011 avg_loss = 3.33715\n",
      "epoch no.3 train no.265690  loss = 4.02852 avg_loss = 3.35476\n",
      "epoch no.3 train no.265700  loss = 3.15015 avg_loss = 3.34556\n",
      "epoch no.3 train no.265710  loss = 4.65393 avg_loss = 3.36383\n",
      "epoch no.3 train no.265720  loss = 3.20312 avg_loss = 3.35667\n",
      "epoch no.3 train no.265730  loss = 3.40290 avg_loss = 3.35217\n",
      "epoch no.3 train no.265740  loss = 3.05282 avg_loss = 3.38436\n",
      "epoch no.3 train no.265750  loss = 3.63187 avg_loss = 3.40228\n",
      "epoch no.3 train no.265760  loss = 3.49506 avg_loss = 3.42462\n",
      "epoch no.3 train no.265770  loss = 4.10357 avg_loss = 3.42145\n",
      "epoch no.3 train no.265780  loss = 1.81629 avg_loss = 3.41238\n",
      "epoch no.3 train no.265790  loss = 3.20511 avg_loss = 3.39667\n",
      "epoch no.3 train no.265800  loss = 2.66376 avg_loss = 3.40402\n",
      "epoch no.3 train no.265810  loss = 3.15736 avg_loss = 3.38824\n",
      "epoch no.3 train no.265820  loss = 4.21313 avg_loss = 3.38571\n",
      "epoch no.3 train no.265830  loss = 3.64835 avg_loss = 3.35924\n",
      "epoch no.3 train no.265840  loss = 3.09621 avg_loss = 3.35162\n",
      "epoch no.3 train no.265850  loss = 3.62394 avg_loss = 3.35474\n",
      "epoch no.3 train no.265860  loss = 4.41049 avg_loss = 3.38670\n",
      "epoch no.3 train no.265870  loss = 2.53228 avg_loss = 3.40004\n",
      "epoch no.3 train no.265880  loss = 1.99770 avg_loss = 3.33245\n",
      "epoch no.3 train no.265890  loss = 2.60655 avg_loss = 3.33432\n",
      "epoch no.3 train no.265900  loss = 2.37282 avg_loss = 3.28201\n",
      "epoch no.3 train no.265910  loss = 4.03568 avg_loss = 3.32077\n",
      "epoch no.3 train no.265920  loss = 3.43988 avg_loss = 3.31238\n",
      "epoch no.3 train no.265930  loss = 2.05936 avg_loss = 3.30668\n",
      "epoch no.3 train no.265940  loss = 2.59744 avg_loss = 3.32926\n",
      "epoch no.3 train no.265950  loss = 3.73248 avg_loss = 3.33155\n",
      "epoch no.3 train no.265960  loss = 2.51287 avg_loss = 3.32420\n",
      "epoch no.3 train no.265970  loss = 3.22418 avg_loss = 3.31908\n",
      "epoch no.3 train no.265980  loss = 2.74364 avg_loss = 3.35156\n",
      "epoch no.3 train no.265990  loss = 4.28518 avg_loss = 3.35365\n",
      "epoch no.3 train no.266000  loss = 3.27374 avg_loss = 3.32302\n",
      "6\n",
      "to_tokens: ['▁비', '▁그', '월드', '▁b', 'g', 'm', '</s>', '</s>']\n",
      "추억의 싸이월드 bgm 모음</s>\n",
      "epoch no.3 train no.266010  loss = 4.14602 avg_loss = 3.33414\n",
      "epoch no.3 train no.266020  loss = 3.33686 avg_loss = 3.36678\n",
      "epoch no.3 train no.266030  loss = 2.52236 avg_loss = 3.34662\n",
      "epoch no.3 train no.266040  loss = 3.33758 avg_loss = 3.33361\n",
      "epoch no.3 train no.266050  loss = 4.39580 avg_loss = 3.33022\n",
      "epoch no.3 train no.266060  loss = 2.31470 avg_loss = 3.29031\n",
      "epoch no.3 train no.266070  loss = 2.77121 avg_loss = 3.30758\n",
      "epoch no.3 train no.266080  loss = 3.58741 avg_loss = 3.35617\n",
      "epoch no.3 train no.266090  loss = 4.13257 avg_loss = 3.43031\n",
      "epoch no.3 train no.266100  loss = 3.51494 avg_loss = 3.43074\n",
      "epoch no.3 train no.266110  loss = 2.57372 avg_loss = 3.48062\n",
      "epoch no.3 train no.266120  loss = 2.21916 avg_loss = 3.44539\n",
      "epoch no.3 train no.266130  loss = 2.76059 avg_loss = 3.42672\n",
      "epoch no.3 train no.266140  loss = 2.82612 avg_loss = 3.41476\n",
      "epoch no.3 train no.266150  loss = 2.68258 avg_loss = 3.39864\n",
      "epoch no.3 train no.266160  loss = 2.46203 avg_loss = 3.38843\n",
      "epoch no.3 train no.266170  loss = 4.01485 avg_loss = 3.39962\n",
      "epoch no.3 train no.266180  loss = 2.41322 avg_loss = 3.34635\n",
      "epoch no.3 train no.266190  loss = 2.58409 avg_loss = 3.34794\n",
      "epoch no.3 train no.266200  loss = 4.18821 avg_loss = 3.35468\n",
      "epoch no.3 train no.266210  loss = 4.37472 avg_loss = 3.40294\n",
      "epoch no.3 train no.266220  loss = 3.09838 avg_loss = 3.39746\n",
      "epoch no.3 train no.266230  loss = 2.64852 avg_loss = 3.40568\n",
      "epoch no.3 train no.266240  loss = 3.24043 avg_loss = 3.34586\n",
      "epoch no.3 train no.266250  loss = 4.57199 avg_loss = 3.29874\n",
      "epoch no.3 train no.266260  loss = 2.51237 avg_loss = 3.31199\n",
      "epoch no.3 train no.266270  loss = 3.81902 avg_loss = 3.34294\n",
      "epoch no.3 train no.266280  loss = 3.39089 avg_loss = 3.34992\n",
      "epoch no.3 train no.266290  loss = 2.70340 avg_loss = 3.33598\n",
      "epoch no.3 train no.266300  loss = 3.53661 avg_loss = 3.33841\n",
      "epoch no.3 train no.266310  loss = 3.07019 avg_loss = 3.34744\n",
      "epoch no.3 train no.266320  loss = 1.45237 avg_loss = 3.36428\n",
      "epoch no.3 train no.266330  loss = 3.44184 avg_loss = 3.37505\n",
      "epoch no.3 train no.266340  loss = 3.19054 avg_loss = 3.33202\n",
      "epoch no.3 train no.266350  loss = 2.51794 avg_loss = 3.32047\n",
      "epoch no.3 train no.266360  loss = 3.16249 avg_loss = 3.29847\n",
      "epoch no.3 train no.266370  loss = 3.84244 avg_loss = 3.33116\n",
      "epoch no.3 train no.266380  loss = 3.00522 avg_loss = 3.28813\n",
      "epoch no.3 train no.266390  loss = 3.49454 avg_loss = 3.25843\n",
      "epoch no.3 train no.266400  loss = 1.79847 avg_loss = 3.26620\n",
      "epoch no.3 train no.266410  loss = 3.28847 avg_loss = 3.29885\n",
      "epoch no.3 train no.266420  loss = 2.55597 avg_loss = 3.28869\n",
      "epoch no.3 train no.266430  loss = 2.94906 avg_loss = 3.28870\n",
      "epoch no.3 train no.266440  loss = 4.50452 avg_loss = 3.28872\n",
      "epoch no.3 train no.266450  loss = 2.45746 avg_loss = 3.24809\n",
      "epoch no.3 train no.266460  loss = 2.81348 avg_loss = 3.24743\n",
      "epoch no.3 train no.266470  loss = 3.20915 avg_loss = 3.21563\n",
      "epoch no.3 train no.266480  loss = 4.30026 avg_loss = 3.27267\n",
      "epoch no.3 train no.266490  loss = 3.28763 avg_loss = 3.26052\n",
      "epoch no.3 train no.266500  loss = 2.63945 avg_loss = 3.24332\n",
      "epoch no.3 train no.266510  loss = 2.22546 avg_loss = 3.26393\n",
      "epoch no.3 train no.266520  loss = 2.02035 avg_loss = 3.23850\n",
      "epoch no.3 train no.266530  loss = 3.41505 avg_loss = 3.26390\n",
      "epoch no.3 train no.266540  loss = 2.65606 avg_loss = 3.22031\n",
      "epoch no.3 train no.266550  loss = 1.73415 avg_loss = 3.18112\n",
      "epoch no.3 train no.266560  loss = 3.49779 avg_loss = 3.20880\n",
      "epoch no.3 train no.266570  loss = 2.87826 avg_loss = 3.21858\n",
      "epoch no.3 train no.266580  loss = 4.51148 avg_loss = 3.24690\n",
      "epoch no.3 train no.266590  loss = 3.90065 avg_loss = 3.24344\n",
      "epoch no.3 train no.266600  loss = 3.40006 avg_loss = 3.21441\n",
      "epoch no.3 train no.266610  loss = 2.54736 avg_loss = 3.26004\n",
      "epoch no.3 train no.266620  loss = 2.92397 avg_loss = 3.26092\n",
      "epoch no.3 train no.266630  loss = 2.83784 avg_loss = 3.25080\n",
      "epoch no.3 train no.266640  loss = 3.41203 avg_loss = 3.27460\n",
      "epoch no.3 train no.266650  loss = 2.94983 avg_loss = 3.26091\n",
      "epoch no.3 train no.266660  loss = 2.29528 avg_loss = 3.25882\n",
      "epoch no.3 train no.266670  loss = 2.19168 avg_loss = 3.26525\n",
      "epoch no.3 train no.266680  loss = 3.24809 avg_loss = 3.24954\n",
      "epoch no.3 train no.266690  loss = 2.89327 avg_loss = 3.28112\n",
      "epoch no.3 train no.266700  loss = 3.25929 avg_loss = 3.26219\n",
      "epoch no.3 train no.266710  loss = 2.35482 avg_loss = 3.24811\n",
      "epoch no.3 train no.266720  loss = 1.93871 avg_loss = 3.23116\n",
      "epoch no.3 train no.266730  loss = 4.90862 avg_loss = 3.24955\n",
      "epoch no.3 train no.266740  loss = 1.57228 avg_loss = 3.18773\n",
      "epoch no.3 train no.266750  loss = 3.50316 avg_loss = 3.16556\n",
      "epoch no.3 train no.266760  loss = 4.75698 avg_loss = 3.19127\n",
      "epoch no.3 train no.266770  loss = 6.55936 avg_loss = 3.18673\n",
      "epoch no.3 train no.266780  loss = 1.85155 avg_loss = 3.17056\n",
      "epoch no.3 train no.266790  loss = 4.57138 avg_loss = 3.20588\n",
      "epoch no.3 train no.266800  loss = 4.42342 avg_loss = 3.23653\n",
      "epoch no.3 train no.266810  loss = 1.26252 avg_loss = 3.19191\n",
      "epoch no.3 train no.266820  loss = 4.79896 avg_loss = 3.19384\n",
      "epoch no.3 train no.266830  loss = 2.67462 avg_loss = 3.19438\n",
      "epoch no.3 train no.266840  loss = 2.39605 avg_loss = 3.21499\n",
      "epoch no.3 train no.266850  loss = 4.72906 avg_loss = 3.19079\n",
      "epoch no.3 train no.266860  loss = 2.89582 avg_loss = 3.19391\n",
      "epoch no.3 train no.266870  loss = 3.69919 avg_loss = 3.16841\n",
      "epoch no.3 train no.266880  loss = 3.64420 avg_loss = 3.24784\n",
      "epoch no.3 train no.266890  loss = 3.05176 avg_loss = 3.25866\n",
      "epoch no.3 train no.266900  loss = 3.87348 avg_loss = 3.29895\n",
      "epoch no.3 train no.266910  loss = 2.89217 avg_loss = 3.27836\n",
      "epoch no.3 train no.266920  loss = 4.20543 avg_loss = 3.27060\n",
      "epoch no.3 train no.266930  loss = 3.97024 avg_loss = 3.32265\n",
      "epoch no.3 train no.266940  loss = 4.52388 avg_loss = 3.32349\n",
      "epoch no.3 train no.266950  loss = 3.12433 avg_loss = 3.35365\n",
      "epoch no.3 train no.266960  loss = 3.01603 avg_loss = 3.34727\n",
      "epoch no.3 train no.266970  loss = 1.94708 avg_loss = 3.32959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.266980  loss = 4.01971 avg_loss = 3.34427\n",
      "epoch no.3 train no.266990  loss = 4.12078 avg_loss = 3.39331\n",
      "epoch no.3 train no.267000  loss = 2.47993 avg_loss = 3.41744\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '송', '▁모음', '집', '</s>']\n",
      "추억의 팝송 모음집</s>\n",
      "epoch no.3 train no.267010  loss = 3.92992 avg_loss = 3.39267\n",
      "epoch no.3 train no.267020  loss = 2.11455 avg_loss = 3.36028\n",
      "epoch no.3 train no.267030  loss = 3.53312 avg_loss = 3.28796\n",
      "epoch no.3 train no.267040  loss = 3.19938 avg_loss = 3.30391\n",
      "epoch no.3 train no.267050  loss = 2.97252 avg_loss = 3.26993\n",
      "epoch no.3 train no.267060  loss = 3.37318 avg_loss = 3.28614\n",
      "epoch no.3 train no.267070  loss = 4.30446 avg_loss = 3.29172\n",
      "epoch no.3 train no.267080  loss = 2.36470 avg_loss = 3.27667\n",
      "epoch no.3 train no.267090  loss = 4.36378 avg_loss = 3.27131\n",
      "epoch no.3 train no.267100  loss = 2.64825 avg_loss = 3.26036\n",
      "epoch no.3 train no.267110  loss = 1.83425 avg_loss = 3.25446\n",
      "epoch no.3 train no.267120  loss = 2.72159 avg_loss = 3.25106\n",
      "epoch no.3 train no.267130  loss = 2.95730 avg_loss = 3.25270\n",
      "epoch no.3 train no.267140  loss = 2.59714 avg_loss = 3.28043\n",
      "epoch no.3 train no.267150  loss = 3.94853 avg_loss = 3.27448\n",
      "epoch no.3 train no.267160  loss = 3.01017 avg_loss = 3.25457\n",
      "epoch no.3 train no.267170  loss = 2.19541 avg_loss = 3.29151\n",
      "epoch no.3 train no.267180  loss = 2.53029 avg_loss = 3.27882\n",
      "epoch no.3 train no.267190  loss = 3.12497 avg_loss = 3.26190\n",
      "epoch no.3 train no.267200  loss = 3.41126 avg_loss = 3.24774\n",
      "epoch no.3 train no.267210  loss = 7.02766 avg_loss = 3.33165\n",
      "epoch no.3 train no.267220  loss = 2.64059 avg_loss = 3.34650\n",
      "epoch no.3 train no.267230  loss = 1.59748 avg_loss = 3.32110\n",
      "epoch no.3 train no.267240  loss = 2.08031 avg_loss = 3.29061\n",
      "epoch no.3 train no.267250  loss = 3.28306 avg_loss = 3.25169\n",
      "epoch no.3 train no.267260  loss = 2.55132 avg_loss = 3.29228\n",
      "epoch no.3 train no.267270  loss = 3.12618 avg_loss = 3.28920\n",
      "epoch no.3 train no.267280  loss = 2.56248 avg_loss = 3.26880\n",
      "epoch no.3 train no.267290  loss = 3.49129 avg_loss = 3.27809\n",
      "epoch no.3 train no.267300  loss = 2.85807 avg_loss = 3.30930\n",
      "epoch no.3 train no.267310  loss = 3.18349 avg_loss = 3.32979\n",
      "epoch no.3 train no.267320  loss = 4.24066 avg_loss = 3.33590\n",
      "epoch no.3 train no.267330  loss = 2.47304 avg_loss = 3.29651\n",
      "epoch no.3 train no.267340  loss = 3.19391 avg_loss = 3.30275\n",
      "epoch no.3 train no.267350  loss = 3.59446 avg_loss = 3.32482\n",
      "epoch no.3 train no.267360  loss = 3.41220 avg_loss = 3.28594\n",
      "epoch no.3 train no.267370  loss = 2.63768 avg_loss = 3.26361\n",
      "epoch no.3 train no.267380  loss = 2.55559 avg_loss = 3.26790\n",
      "epoch no.3 train no.267390  loss = 2.44224 avg_loss = 3.28438\n",
      "epoch no.3 train no.267400  loss = 2.62613 avg_loss = 3.28230\n",
      "epoch no.3 train no.267410  loss = 2.69871 avg_loss = 3.24644\n",
      "epoch no.3 train no.267420  loss = 3.55267 avg_loss = 3.26438\n",
      "epoch no.3 train no.267430  loss = 3.26757 avg_loss = 3.23937\n",
      "epoch no.3 train no.267440  loss = 3.56981 avg_loss = 3.24965\n",
      "epoch no.3 train no.267450  loss = 3.53968 avg_loss = 3.21338\n",
      "epoch no.3 train no.267460  loss = 6.17300 avg_loss = 3.21098\n",
      "epoch no.3 train no.267470  loss = 2.57027 avg_loss = 3.19093\n",
      "epoch no.3 train no.267480  loss = 2.70401 avg_loss = 3.17443\n",
      "epoch no.3 train no.267490  loss = 4.04445 avg_loss = 3.18719\n",
      "epoch no.3 train no.267500  loss = 2.74957 avg_loss = 3.21812\n",
      "epoch no.3 train no.267510  loss = 2.99918 avg_loss = 3.21413\n",
      "epoch no.3 train no.267520  loss = 4.68088 avg_loss = 3.28987\n",
      "epoch no.3 train no.267530  loss = 4.64994 avg_loss = 3.31953\n",
      "epoch no.3 train no.267540  loss = 3.66462 avg_loss = 3.31400\n",
      "epoch no.3 train no.267550  loss = 1.66453 avg_loss = 3.27362\n",
      "epoch no.3 train no.267560  loss = 2.76584 avg_loss = 3.26959\n",
      "epoch no.3 train no.267570  loss = 3.93473 avg_loss = 3.24492\n",
      "epoch no.3 train no.267580  loss = 2.19820 avg_loss = 3.23831\n",
      "epoch no.3 train no.267590  loss = 2.52714 avg_loss = 3.24406\n",
      "epoch no.3 train no.267600  loss = 2.69844 avg_loss = 3.25773\n",
      "epoch no.3 train no.267610  loss = 1.86613 avg_loss = 3.24594\n",
      "epoch no.3 train no.267620  loss = 2.87094 avg_loss = 3.30649\n",
      "epoch no.3 train no.267630  loss = 4.26614 avg_loss = 3.39121\n",
      "epoch no.3 train no.267640  loss = 4.18644 avg_loss = 3.43019\n",
      "epoch no.3 train no.267650  loss = 3.78421 avg_loss = 3.37318\n",
      "epoch no.3 train no.267660  loss = 6.08847 avg_loss = 3.40078\n",
      "epoch no.3 train no.267670  loss = 3.37011 avg_loss = 3.34048\n",
      "epoch no.3 train no.267680  loss = 4.86885 avg_loss = 3.35842\n",
      "epoch no.3 train no.267690  loss = 2.67417 avg_loss = 3.33253\n",
      "epoch no.3 train no.267700  loss = 4.34287 avg_loss = 3.34118\n",
      "epoch no.3 train no.267710  loss = 2.74349 avg_loss = 3.39605\n",
      "epoch no.3 train no.267720  loss = 3.67184 avg_loss = 3.39080\n",
      "epoch no.3 train no.267730  loss = 2.91158 avg_loss = 3.35912\n",
      "epoch no.3 train no.267740  loss = 5.09333 avg_loss = 3.37032\n",
      "epoch no.3 train no.267750  loss = 2.87245 avg_loss = 3.32454\n",
      "epoch no.3 train no.267760  loss = 2.19574 avg_loss = 3.36030\n",
      "epoch no.3 train no.267770  loss = 4.16694 avg_loss = 3.37707\n",
      "epoch no.3 train no.267780  loss = 2.83315 avg_loss = 3.38983\n",
      "epoch no.3 train no.267790  loss = 1.52340 avg_loss = 3.38156\n",
      "epoch no.3 train no.267800  loss = 4.23556 avg_loss = 3.38973\n",
      "epoch no.3 train no.267810  loss = 3.44182 avg_loss = 3.39367\n",
      "epoch no.3 train no.267820  loss = 3.97431 avg_loss = 3.43416\n",
      "epoch no.3 train no.267830  loss = 3.01958 avg_loss = 3.45430\n",
      "epoch no.3 train no.267840  loss = 3.32466 avg_loss = 3.40802\n",
      "epoch no.3 train no.267850  loss = 3.26535 avg_loss = 3.45070\n",
      "epoch no.3 train no.267860  loss = 3.85352 avg_loss = 3.45197\n",
      "epoch no.3 train no.267870  loss = 2.88594 avg_loss = 3.42799\n",
      "epoch no.3 train no.267880  loss = 4.34997 avg_loss = 3.45876\n",
      "epoch no.3 train no.267890  loss = 2.57407 avg_loss = 3.46519\n",
      "epoch no.3 train no.267900  loss = 4.22103 avg_loss = 3.48841\n",
      "epoch no.3 train no.267910  loss = 4.37437 avg_loss = 3.47694\n",
      "epoch no.3 train no.267920  loss = 3.15936 avg_loss = 3.45720\n",
      "epoch no.3 train no.267930  loss = 2.31294 avg_loss = 3.38313\n",
      "epoch no.3 train no.267940  loss = 5.85574 avg_loss = 3.39499\n",
      "epoch no.3 train no.267950  loss = 5.75649 avg_loss = 3.46170\n",
      "epoch no.3 train no.267960  loss = 2.41640 avg_loss = 3.42536\n",
      "epoch no.3 train no.267970  loss = 3.91007 avg_loss = 3.43956\n",
      "epoch no.3 train no.267980  loss = 2.45284 avg_loss = 3.40217\n",
      "epoch no.3 train no.267990  loss = 2.30869 avg_loss = 3.34166\n",
      "epoch no.3 train no.268000  loss = 2.34162 avg_loss = 3.31258\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.3 train no.268010  loss = 2.65572 avg_loss = 3.28928\n",
      "epoch no.3 train no.268020  loss = 3.92187 avg_loss = 3.29433\n",
      "epoch no.3 train no.268030  loss = 3.95216 avg_loss = 3.25369\n",
      "epoch no.3 train no.268040  loss = 3.98858 avg_loss = 3.27823\n",
      "epoch no.3 train no.268050  loss = 3.70477 avg_loss = 3.29707\n",
      "epoch no.3 train no.268060  loss = 3.54850 avg_loss = 3.34670\n",
      "epoch no.3 train no.268070  loss = 1.75781 avg_loss = 3.38332\n",
      "epoch no.3 train no.268080  loss = 4.28396 avg_loss = 3.36666\n",
      "epoch no.3 train no.268090  loss = 3.47676 avg_loss = 3.33951\n",
      "epoch no.3 train no.268100  loss = 3.17788 avg_loss = 3.37783\n",
      "epoch no.3 train no.268110  loss = 2.56312 avg_loss = 3.34719\n",
      "epoch no.3 train no.268120  loss = 4.14660 avg_loss = 3.32867\n",
      "epoch no.3 train no.268130  loss = 3.61815 avg_loss = 3.30911\n",
      "epoch no.3 train no.268140  loss = 3.48962 avg_loss = 3.37094\n",
      "epoch no.3 train no.268150  loss = 3.79635 avg_loss = 3.38865\n",
      "epoch no.3 train no.268160  loss = 2.40595 avg_loss = 3.41481\n",
      "epoch no.3 train no.268170  loss = 3.42027 avg_loss = 3.38275\n",
      "epoch no.3 train no.268180  loss = 2.82259 avg_loss = 3.36583\n",
      "epoch no.3 train no.268190  loss = 3.81579 avg_loss = 3.34842\n",
      "epoch no.3 train no.268200  loss = 4.28029 avg_loss = 3.36366\n",
      "epoch no.3 train no.268210  loss = 4.58480 avg_loss = 3.35049\n",
      "epoch no.3 train no.268220  loss = 3.86784 avg_loss = 3.32119\n",
      "epoch no.3 train no.268230  loss = 4.74054 avg_loss = 3.30833\n",
      "epoch no.3 train no.268240  loss = 5.01956 avg_loss = 3.30707\n",
      "epoch no.3 train no.268250  loss = 4.76229 avg_loss = 3.31429\n",
      "epoch no.3 train no.268260  loss = 3.66617 avg_loss = 3.35749\n",
      "epoch no.3 train no.268270  loss = 2.23886 avg_loss = 3.31414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.268280  loss = 2.91370 avg_loss = 3.29184\n",
      "epoch no.3 train no.268290  loss = 3.99092 avg_loss = 3.30258\n",
      "epoch no.3 train no.268300  loss = 2.77208 avg_loss = 3.29433\n",
      "epoch no.3 train no.268310  loss = 4.04374 avg_loss = 3.30073\n",
      "epoch no.3 train no.268320  loss = 2.73673 avg_loss = 3.27105\n",
      "epoch no.3 train no.268330  loss = 3.05619 avg_loss = 3.26713\n",
      "epoch no.3 train no.268340  loss = 3.92186 avg_loss = 3.28050\n",
      "epoch no.3 train no.268350  loss = 2.89598 avg_loss = 3.26915\n",
      "epoch no.3 train no.268360  loss = 2.38526 avg_loss = 3.26904\n",
      "epoch no.3 train no.268370  loss = 2.97496 avg_loss = 3.26770\n",
      "epoch no.3 train no.268380  loss = 3.44701 avg_loss = 3.31645\n",
      "epoch no.3 train no.268390  loss = 5.25327 avg_loss = 3.40136\n",
      "epoch no.3 train no.268400  loss = 2.16200 avg_loss = 3.40336\n",
      "epoch no.3 train no.268410  loss = 4.61103 avg_loss = 3.45723\n",
      "epoch no.3 train no.268420  loss = 2.94184 avg_loss = 3.45331\n",
      "epoch no.3 train no.268430  loss = 3.88878 avg_loss = 3.49306\n",
      "epoch no.3 train no.268440  loss = 2.68372 avg_loss = 3.48721\n",
      "epoch no.3 train no.268450  loss = 2.22145 avg_loss = 3.45704\n",
      "epoch no.3 train no.268460  loss = 4.23907 avg_loss = 3.45139\n",
      "epoch no.3 train no.268470  loss = 3.46003 avg_loss = 3.43764\n",
      "epoch no.3 train no.268480  loss = 3.81492 avg_loss = 3.41925\n",
      "epoch no.3 train no.268490  loss = 3.24019 avg_loss = 3.42909\n",
      "epoch no.3 train no.268500  loss = 5.97203 avg_loss = 3.45188\n",
      "epoch no.3 train no.268510  loss = 3.29321 avg_loss = 3.41331\n",
      "epoch no.3 train no.268520  loss = 5.24015 avg_loss = 3.45683\n",
      "epoch no.3 train no.268530  loss = 2.76611 avg_loss = 3.43591\n",
      "epoch no.3 train no.268540  loss = 3.17825 avg_loss = 3.44784\n",
      "epoch no.3 train no.268550  loss = 3.39193 avg_loss = 3.43014\n",
      "epoch no.3 train no.268560  loss = 3.44578 avg_loss = 3.41830\n",
      "epoch no.3 train no.268570  loss = 5.29424 avg_loss = 3.41671\n",
      "epoch no.3 train no.268580  loss = 4.48642 avg_loss = 3.41343\n",
      "epoch no.3 train no.268590  loss = 2.59299 avg_loss = 3.41110\n",
      "epoch no.3 train no.268600  loss = 3.28046 avg_loss = 3.42373\n",
      "epoch no.3 train no.268610  loss = 3.04987 avg_loss = 3.38806\n",
      "epoch no.3 train no.268620  loss = 2.57135 avg_loss = 3.36392\n",
      "epoch no.3 train no.268630  loss = 2.32452 avg_loss = 3.30624\n",
      "epoch no.3 train no.268640  loss = 2.89760 avg_loss = 3.30033\n",
      "epoch no.3 train no.268650  loss = 3.46725 avg_loss = 3.33460\n",
      "epoch no.3 train no.268660  loss = 2.17340 avg_loss = 3.26964\n",
      "epoch no.3 train no.268670  loss = 2.87519 avg_loss = 3.24311\n",
      "epoch no.3 train no.268680  loss = 2.62471 avg_loss = 3.23528\n",
      "epoch no.3 train no.268690  loss = 4.47045 avg_loss = 3.27620\n",
      "epoch no.3 train no.268700  loss = 3.42872 avg_loss = 3.31671\n",
      "epoch no.3 train no.268710  loss = 3.51711 avg_loss = 3.31520\n",
      "epoch no.3 train no.268720  loss = 4.76606 avg_loss = 3.34313\n",
      "epoch no.3 train no.268730  loss = 3.61723 avg_loss = 3.35767\n",
      "epoch no.3 train no.268740  loss = 2.58851 avg_loss = 3.30674\n",
      "epoch no.3 train no.268750  loss = 4.19904 avg_loss = 3.28825\n",
      "epoch no.3 train no.268760  loss = 3.65357 avg_loss = 3.27865\n",
      "epoch no.3 train no.268770  loss = 4.34229 avg_loss = 3.27978\n",
      "epoch no.3 train no.268780  loss = 2.94890 avg_loss = 3.27453\n",
      "epoch no.3 train no.268790  loss = 5.24693 avg_loss = 3.27750\n",
      "epoch no.3 train no.268800  loss = 2.00658 avg_loss = 3.28696\n",
      "epoch no.3 train no.268810  loss = 3.12127 avg_loss = 3.29730\n",
      "epoch no.3 train no.268820  loss = 2.30603 avg_loss = 3.28941\n",
      "epoch no.3 train no.268830  loss = 3.00078 avg_loss = 3.23533\n",
      "epoch no.3 train no.268840  loss = 3.83276 avg_loss = 3.21166\n",
      "epoch no.3 train no.268850  loss = 2.19455 avg_loss = 3.19555\n",
      "epoch no.3 train no.268860  loss = 3.26644 avg_loss = 3.23040\n",
      "epoch no.3 train no.268870  loss = 1.57617 avg_loss = 3.23678\n",
      "epoch no.3 train no.268880  loss = 3.75566 avg_loss = 3.23417\n",
      "epoch no.3 train no.268890  loss = 4.93910 avg_loss = 3.34976\n",
      "epoch no.3 train no.268900  loss = 4.24375 avg_loss = 3.40353\n",
      "epoch no.3 train no.268910  loss = 4.36599 avg_loss = 3.38238\n",
      "epoch no.3 train no.268920  loss = 3.74252 avg_loss = 3.45371\n",
      "epoch no.3 train no.268930  loss = 3.56823 avg_loss = 3.51256\n",
      "epoch no.3 train no.268940  loss = 2.52194 avg_loss = 3.48503\n",
      "epoch no.3 train no.268950  loss = 2.56324 avg_loss = 3.48291\n",
      "epoch no.3 train no.268960  loss = 2.93672 avg_loss = 3.42867\n",
      "epoch no.3 train no.268970  loss = 4.40680 avg_loss = 3.45450\n",
      "epoch no.3 train no.268980  loss = 2.68683 avg_loss = 3.42816\n",
      "epoch no.3 train no.268990  loss = 4.15390 avg_loss = 3.39530\n",
      "epoch no.3 train no.269000  loss = 2.55655 avg_loss = 3.41851\n",
      "4\n",
      "to_tokens: ['▁가을', '▁인기가', 'ld', '▁p', 'op', '</s>']\n",
      "추억의 old pop</s>\n",
      "epoch no.3 train no.269010  loss = 1.63969 avg_loss = 3.36456\n",
      "epoch no.3 train no.269020  loss = 4.30662 avg_loss = 3.39399\n",
      "epoch no.3 train no.269030  loss = 2.34919 avg_loss = 3.39810\n",
      "epoch no.3 train no.269040  loss = 2.75285 avg_loss = 3.37884\n",
      "epoch no.3 train no.269050  loss = 2.43513 avg_loss = 3.34646\n",
      "epoch no.3 train no.269060  loss = 2.37234 avg_loss = 3.30672\n",
      "epoch no.3 train no.269070  loss = 2.90398 avg_loss = 3.29022\n",
      "epoch no.3 train no.269080  loss = 2.63875 avg_loss = 3.31163\n",
      "epoch no.3 train no.269090  loss = 4.18556 avg_loss = 3.32124\n",
      "epoch no.3 train no.269100  loss = 5.41710 avg_loss = 3.33002\n",
      "epoch no.3 train no.269110  loss = 1.76602 avg_loss = 3.28933\n",
      "epoch no.3 train no.269120  loss = 4.63314 avg_loss = 3.31465\n",
      "epoch no.3 train no.269130  loss = 3.97334 avg_loss = 3.30887\n",
      "epoch no.3 train no.269140  loss = 4.17173 avg_loss = 3.31963\n",
      "epoch no.3 train no.269150  loss = 2.27964 avg_loss = 3.30056\n",
      "epoch no.3 train no.269160  loss = 2.81108 avg_loss = 3.31544\n",
      "epoch no.3 train no.269170  loss = 3.55653 avg_loss = 3.30165\n",
      "epoch no.3 train no.269180  loss = 2.19642 avg_loss = 3.24786\n",
      "epoch no.3 train no.269190  loss = 2.39676 avg_loss = 3.20326\n",
      "epoch no.3 train no.269200  loss = 2.85862 avg_loss = 3.18322\n",
      "epoch no.3 train no.269210  loss = 2.16100 avg_loss = 3.19332\n",
      "epoch no.3 train no.269220  loss = 3.69850 avg_loss = 3.18857\n",
      "epoch no.3 train no.269230  loss = 2.31745 avg_loss = 3.16185\n",
      "epoch no.3 train no.269240  loss = 1.99918 avg_loss = 3.14595\n",
      "epoch no.3 train no.269250  loss = 2.25179 avg_loss = 3.13053\n",
      "epoch no.3 train no.269260  loss = 2.75000 avg_loss = 3.17470\n",
      "epoch no.3 train no.269270  loss = 3.68844 avg_loss = 3.19359\n",
      "epoch no.3 train no.269280  loss = 3.96514 avg_loss = 3.18760\n",
      "epoch no.3 train no.269290  loss = 5.35527 avg_loss = 3.18967\n",
      "epoch no.3 train no.269300  loss = 2.26280 avg_loss = 3.17242\n",
      "epoch no.3 train no.269310  loss = 3.45500 avg_loss = 3.21841\n",
      "epoch no.3 train no.269320  loss = 3.24953 avg_loss = 3.19677\n",
      "epoch no.3 train no.269330  loss = 5.04176 avg_loss = 3.26058\n",
      "epoch no.3 train no.269340  loss = 3.02982 avg_loss = 3.26636\n",
      "epoch no.3 train no.269350  loss = 5.39347 avg_loss = 3.30395\n",
      "epoch no.3 train no.269360  loss = 2.91541 avg_loss = 3.31790\n",
      "epoch no.3 train no.269370  loss = 3.93849 avg_loss = 3.38201\n",
      "epoch no.3 train no.269380  loss = 2.95176 avg_loss = 3.34459\n",
      "epoch no.3 train no.269390  loss = 3.57055 avg_loss = 3.35851\n",
      "epoch no.3 train no.269400  loss = 2.21495 avg_loss = 3.33380\n",
      "epoch no.3 train no.269410  loss = 2.47794 avg_loss = 3.32428\n",
      "epoch no.3 train no.269420  loss = 2.90000 avg_loss = 3.31458\n",
      "epoch no.3 train no.269430  loss = 2.84513 avg_loss = 3.32427\n",
      "epoch no.3 train no.269440  loss = 2.36693 avg_loss = 3.30622\n",
      "epoch no.3 train no.269450  loss = 2.74869 avg_loss = 3.32462\n",
      "epoch no.3 train no.269460  loss = 5.34897 avg_loss = 3.30519\n",
      "epoch no.3 train no.269470  loss = 2.63715 avg_loss = 3.30693\n",
      "epoch no.3 train no.269480  loss = 4.60913 avg_loss = 3.34965\n",
      "epoch no.3 train no.269490  loss = 3.33695 avg_loss = 3.31808\n",
      "epoch no.3 train no.269500  loss = 4.33780 avg_loss = 3.35166\n",
      "epoch no.3 train no.269510  loss = 3.14189 avg_loss = 3.39132\n",
      "epoch no.3 train no.269520  loss = 4.61316 avg_loss = 3.37103\n",
      "epoch no.3 train no.269530  loss = 3.22416 avg_loss = 3.34885\n",
      "epoch no.3 train no.269540  loss = 3.33195 avg_loss = 3.38211\n",
      "epoch no.3 train no.269550  loss = 3.25208 avg_loss = 3.38571\n",
      "epoch no.3 train no.269560  loss = 3.53296 avg_loss = 3.37905\n",
      "epoch no.3 train no.269570  loss = 2.54802 avg_loss = 3.34615\n",
      "epoch no.3 train no.269580  loss = 2.74862 avg_loss = 3.39676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.269590  loss = 2.20118 avg_loss = 3.43051\n",
      "epoch no.3 train no.269600  loss = 2.48014 avg_loss = 3.40264\n",
      "epoch no.3 train no.269610  loss = 1.89133 avg_loss = 3.40233\n",
      "epoch no.3 train no.269620  loss = 2.70360 avg_loss = 3.39675\n",
      "epoch no.3 train no.269630  loss = 1.91268 avg_loss = 3.36738\n",
      "epoch no.3 train no.269640  loss = 5.27725 avg_loss = 3.36475\n",
      "epoch no.3 train no.269650  loss = 2.18757 avg_loss = 3.34965\n",
      "epoch no.3 train no.269660  loss = 2.10962 avg_loss = 3.33299\n",
      "epoch no.3 train no.269670  loss = 3.61474 avg_loss = 3.33338\n",
      "epoch no.3 train no.269680  loss = 2.54656 avg_loss = 3.34460\n",
      "epoch no.3 train no.269690  loss = 4.85214 avg_loss = 3.35536\n",
      "epoch no.3 train no.269700  loss = 3.01737 avg_loss = 3.33367\n",
      "epoch no.3 train no.269710  loss = 3.77642 avg_loss = 3.36405\n",
      "epoch no.3 train no.269720  loss = 3.55059 avg_loss = 3.34322\n",
      "epoch no.3 train no.269730  loss = 2.13588 avg_loss = 3.35455\n",
      "epoch no.3 train no.269740  loss = 3.89952 avg_loss = 3.31495\n",
      "epoch no.3 train no.269750  loss = 3.99050 avg_loss = 3.30186\n",
      "epoch no.3 train no.269760  loss = 4.42692 avg_loss = 3.31578\n",
      "epoch no.3 train no.269770  loss = 2.63301 avg_loss = 3.36016\n",
      "epoch no.3 train no.269780  loss = 3.34561 avg_loss = 3.38772\n",
      "epoch no.3 train no.269790  loss = 3.48382 avg_loss = 3.33782\n",
      "epoch no.3 train no.269800  loss = 2.37157 avg_loss = 3.29214\n",
      "epoch no.3 train no.269810  loss = 1.99329 avg_loss = 3.27940\n",
      "epoch no.3 train no.269820  loss = 2.92698 avg_loss = 3.26653\n",
      "epoch no.3 train no.269830  loss = 3.01340 avg_loss = 3.31606\n",
      "epoch no.3 train no.269840  loss = 2.70152 avg_loss = 3.30476\n",
      "epoch no.3 train no.269850  loss = 3.02487 avg_loss = 3.28490\n",
      "epoch no.3 train no.269860  loss = 2.67555 avg_loss = 3.29691\n",
      "epoch no.3 train no.269870  loss = 5.04747 avg_loss = 3.30089\n",
      "epoch no.3 train no.269880  loss = 2.92712 avg_loss = 3.28010\n",
      "epoch no.3 train no.269890  loss = 2.63140 avg_loss = 3.30814\n",
      "epoch no.3 train no.269900  loss = 2.84720 avg_loss = 3.33359\n",
      "epoch no.3 train no.269910  loss = 3.42740 avg_loss = 3.33620\n",
      "epoch no.3 train no.269920  loss = 2.56459 avg_loss = 3.31738\n",
      "epoch no.3 train no.269930  loss = 2.40294 avg_loss = 3.38407\n",
      "epoch no.3 train no.269940  loss = 2.95805 avg_loss = 3.42432\n",
      "epoch no.3 train no.269950  loss = 2.73267 avg_loss = 3.41219\n",
      "epoch no.3 train no.269960  loss = 3.28327 avg_loss = 3.43163\n",
      "epoch no.3 train no.269970  loss = 2.63162 avg_loss = 3.43892\n",
      "epoch no.3 train no.269980  loss = 3.33447 avg_loss = 3.38289\n",
      "epoch no.3 train no.269990  loss = 3.89788 avg_loss = 3.36289\n",
      "epoch no.3 train no.270000  loss = 3.79159 avg_loss = 3.35707\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁발라', '송', '</s>']\n",
      "추억의 2000년대 팝송</s>\n",
      "epoch no.3 train no.270010  loss = 5.14739 avg_loss = 3.35916\n",
      "epoch no.3 train no.270020  loss = 3.09647 avg_loss = 3.36287\n",
      "epoch no.3 train no.270030  loss = 5.62623 avg_loss = 3.38944\n",
      "epoch no.3 train no.270040  loss = 3.70725 avg_loss = 3.38525\n",
      "epoch no.3 train no.270050  loss = 2.65467 avg_loss = 3.35789\n",
      "epoch no.3 train no.270060  loss = 3.86121 avg_loss = 3.36449\n",
      "epoch no.3 train no.270070  loss = 1.96326 avg_loss = 3.32447\n",
      "epoch no.3 train no.270080  loss = 2.08030 avg_loss = 3.32801\n",
      "epoch no.3 train no.270090  loss = 3.64349 avg_loss = 3.31966\n",
      "epoch no.3 train no.270100  loss = 3.72973 avg_loss = 3.32895\n",
      "epoch no.3 train no.270110  loss = 4.55880 avg_loss = 3.31246\n",
      "epoch no.3 train no.270120  loss = 3.99685 avg_loss = 3.32360\n",
      "epoch no.3 train no.270130  loss = 2.79580 avg_loss = 3.35079\n",
      "epoch no.3 train no.270140  loss = 4.35003 avg_loss = 3.39089\n",
      "epoch no.3 train no.270150  loss = 2.75684 avg_loss = 3.38465\n",
      "epoch no.3 train no.270160  loss = 3.27978 avg_loss = 3.41646\n",
      "epoch no.3 train no.270170  loss = 3.00313 avg_loss = 3.42240\n",
      "epoch no.3 train no.270180  loss = 5.70985 avg_loss = 3.45776\n",
      "epoch no.3 train no.270190  loss = 2.96060 avg_loss = 3.45236\n",
      "epoch no.3 train no.270200  loss = 3.44596 avg_loss = 3.42918\n",
      "epoch no.3 train no.270210  loss = 4.39892 avg_loss = 3.44462\n",
      "epoch no.3 train no.270220  loss = 3.04764 avg_loss = 3.40292\n",
      "epoch no.3 train no.270230  loss = 2.91471 avg_loss = 3.42937\n",
      "epoch no.3 train no.270240  loss = 3.24986 avg_loss = 3.42106\n",
      "epoch no.3 train no.270250  loss = 2.75568 avg_loss = 3.44050\n",
      "epoch no.3 train no.270260  loss = 2.73031 avg_loss = 3.44117\n",
      "epoch no.3 train no.270270  loss = 1.99708 avg_loss = 3.39996\n",
      "epoch no.3 train no.270280  loss = 3.58323 avg_loss = 3.38411\n",
      "epoch no.3 train no.270290  loss = 3.08329 avg_loss = 3.35792\n",
      "epoch no.3 train no.270300  loss = 2.78600 avg_loss = 3.39218\n",
      "epoch no.3 train no.270310  loss = 3.44872 avg_loss = 3.33566\n",
      "epoch no.3 train no.270320  loss = 2.73031 avg_loss = 3.32573\n",
      "epoch no.3 train no.270330  loss = 3.37806 avg_loss = 3.35923\n",
      "epoch no.3 train no.270340  loss = 4.25310 avg_loss = 3.31312\n",
      "epoch no.3 train no.270350  loss = 5.37416 avg_loss = 3.32387\n",
      "epoch no.3 train no.270360  loss = 2.23970 avg_loss = 3.24095\n",
      "epoch no.3 train no.270370  loss = 4.03874 avg_loss = 3.24699\n",
      "epoch no.3 train no.270380  loss = 6.33592 avg_loss = 3.30121\n",
      "epoch no.3 train no.270390  loss = 3.15073 avg_loss = 3.29581\n",
      "epoch no.3 train no.270400  loss = 1.85522 avg_loss = 3.29862\n",
      "epoch no.3 train no.270410  loss = 4.24618 avg_loss = 3.27906\n",
      "epoch no.3 train no.270420  loss = 3.19303 avg_loss = 3.30209\n",
      "epoch no.3 train no.270430  loss = 2.64537 avg_loss = 3.27555\n",
      "epoch no.3 train no.270440  loss = 2.68294 avg_loss = 3.25049\n",
      "epoch no.3 train no.270450  loss = 3.66211 avg_loss = 3.27286\n",
      "epoch no.3 train no.270460  loss = 2.55666 avg_loss = 3.32758\n",
      "epoch no.3 train no.270470  loss = 3.65220 avg_loss = 3.30848\n",
      "epoch no.3 train no.270480  loss = 4.81332 avg_loss = 3.36113\n",
      "epoch no.3 train no.270490  loss = 3.31314 avg_loss = 3.34143\n",
      "epoch no.3 train no.270500  loss = 3.92575 avg_loss = 3.33895\n",
      "epoch no.3 train no.270510  loss = 2.01558 avg_loss = 3.28354\n",
      "epoch no.3 train no.270520  loss = 2.22142 avg_loss = 3.23787\n",
      "epoch no.3 train no.270530  loss = 2.76271 avg_loss = 3.23650\n",
      "epoch no.3 train no.270540  loss = 4.91667 avg_loss = 3.25346\n",
      "epoch no.3 train no.270550  loss = 4.09657 avg_loss = 3.25356\n",
      "epoch no.3 train no.270560  loss = 3.23326 avg_loss = 3.26965\n",
      "epoch no.3 train no.270570  loss = 2.14910 avg_loss = 3.25483\n",
      "epoch no.3 train no.270580  loss = 3.63391 avg_loss = 3.26597\n",
      "epoch no.3 train no.270590  loss = 1.41548 avg_loss = 3.28338\n",
      "epoch no.3 train no.270600  loss = 2.22932 avg_loss = 3.27036\n",
      "epoch no.3 train no.270610  loss = 3.22535 avg_loss = 3.30088\n",
      "epoch no.3 train no.270620  loss = 2.90418 avg_loss = 3.30971\n",
      "epoch no.3 train no.270630  loss = 2.74794 avg_loss = 3.31351\n",
      "epoch no.3 train no.270640  loss = 4.08469 avg_loss = 3.35589\n",
      "epoch no.3 train no.270650  loss = 3.03817 avg_loss = 3.31787\n",
      "epoch no.3 train no.270660  loss = 2.88376 avg_loss = 3.28296\n",
      "epoch no.3 train no.270670  loss = 3.64450 avg_loss = 3.28483\n",
      "epoch no.3 train no.270680  loss = 3.80431 avg_loss = 3.32549\n",
      "epoch no.3 train no.270690  loss = 3.63649 avg_loss = 3.34731\n",
      "epoch no.3 train no.270700  loss = 2.28253 avg_loss = 3.29772\n",
      "epoch no.3 train no.270710  loss = 3.13947 avg_loss = 3.27506\n",
      "epoch no.3 train no.270720  loss = 2.84004 avg_loss = 3.26760\n",
      "epoch no.3 train no.270730  loss = 3.38938 avg_loss = 3.23478\n",
      "epoch no.3 train no.270740  loss = 2.14357 avg_loss = 3.24987\n",
      "epoch no.3 train no.270750  loss = 3.68346 avg_loss = 3.28749\n",
      "epoch no.3 train no.270760  loss = 4.34166 avg_loss = 3.26687\n",
      "epoch no.3 train no.270770  loss = 2.79417 avg_loss = 3.26684\n",
      "epoch no.3 train no.270780  loss = 2.97185 avg_loss = 3.27276\n",
      "epoch no.3 train no.270790  loss = 3.62394 avg_loss = 3.28993\n",
      "epoch no.3 train no.270800  loss = 4.50830 avg_loss = 3.29435\n",
      "epoch no.3 train no.270810  loss = 2.50726 avg_loss = 3.32515\n",
      "epoch no.3 train no.270820  loss = 3.05532 avg_loss = 3.30230\n",
      "epoch no.3 train no.270830  loss = 5.10403 avg_loss = 3.31113\n",
      "epoch no.3 train no.270840  loss = 5.36573 avg_loss = 3.41196\n",
      "epoch no.3 train no.270850  loss = 3.49952 avg_loss = 3.40135\n",
      "epoch no.3 train no.270860  loss = 3.19386 avg_loss = 3.41943\n",
      "epoch no.3 train no.270870  loss = 2.29628 avg_loss = 3.39356\n",
      "epoch no.3 train no.270880  loss = 2.91556 avg_loss = 3.38088\n",
      "epoch no.3 train no.270890  loss = 2.26606 avg_loss = 3.36006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.270900  loss = 2.56720 avg_loss = 3.32410\n",
      "epoch no.3 train no.270910  loss = 3.38397 avg_loss = 3.32973\n",
      "epoch no.3 train no.270920  loss = 4.01926 avg_loss = 3.29453\n",
      "epoch no.3 train no.270930  loss = 3.05998 avg_loss = 3.26377\n",
      "epoch no.3 train no.270940  loss = 3.29825 avg_loss = 3.30113\n",
      "epoch no.3 train no.270950  loss = 2.44254 avg_loss = 3.31569\n",
      "epoch no.3 train no.270960  loss = 1.94549 avg_loss = 3.25664\n",
      "epoch no.3 train no.270970  loss = 4.05198 avg_loss = 3.30308\n",
      "epoch no.3 train no.270980  loss = 3.73108 avg_loss = 3.28690\n",
      "epoch no.3 train no.270990  loss = 4.49906 avg_loss = 3.33558\n",
      "epoch no.3 train no.271000  loss = 2.42761 avg_loss = 3.30440\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.3 train no.271010  loss = 2.16645 avg_loss = 3.24952\n",
      "epoch no.3 train no.271020  loss = 2.14415 avg_loss = 3.22405\n",
      "epoch no.3 train no.271030  loss = 3.65104 avg_loss = 3.24163\n",
      "epoch no.3 train no.271040  loss = 3.06917 avg_loss = 3.23162\n",
      "epoch no.3 train no.271050  loss = 1.59217 avg_loss = 3.24385\n",
      "epoch no.3 train no.271060  loss = 4.13759 avg_loss = 3.26442\n",
      "epoch no.3 train no.271070  loss = 4.17849 avg_loss = 3.26751\n",
      "epoch no.3 train no.271080  loss = 3.89407 avg_loss = 3.25659\n",
      "epoch no.3 train no.271090  loss = 3.07987 avg_loss = 3.24674\n",
      "epoch no.3 train no.271100  loss = 3.43353 avg_loss = 3.28708\n",
      "epoch no.3 train no.271110  loss = 4.44255 avg_loss = 3.26175\n",
      "epoch no.3 train no.271120  loss = 3.11133 avg_loss = 3.26178\n",
      "epoch no.3 train no.271130  loss = 2.74139 avg_loss = 3.23513\n",
      "epoch no.3 train no.271140  loss = 4.53482 avg_loss = 3.25075\n",
      "epoch no.3 train no.271150  loss = 2.55106 avg_loss = 3.26828\n",
      "epoch no.3 train no.271160  loss = 4.24991 avg_loss = 3.26858\n",
      "epoch no.3 train no.271170  loss = 3.16722 avg_loss = 3.24464\n",
      "epoch no.3 train no.271180  loss = 2.56276 avg_loss = 3.21481\n",
      "epoch no.3 train no.271190  loss = 4.79114 avg_loss = 3.26696\n",
      "epoch no.3 train no.271200  loss = 2.60394 avg_loss = 3.25599\n",
      "epoch no.3 train no.271210  loss = 3.84614 avg_loss = 3.22483\n",
      "epoch no.3 train no.271220  loss = 3.43033 avg_loss = 3.21876\n",
      "epoch no.3 train no.271230  loss = 4.78897 avg_loss = 3.30954\n",
      "epoch no.3 train no.271240  loss = 3.78546 avg_loss = 3.29358\n",
      "epoch no.3 train no.271250  loss = 3.26086 avg_loss = 3.29062\n",
      "epoch no.3 train no.271260  loss = 4.02925 avg_loss = 3.32081\n",
      "epoch no.3 train no.271270  loss = 4.06486 avg_loss = 3.31150\n",
      "epoch no.3 train no.271280  loss = 2.56045 avg_loss = 3.30750\n",
      "epoch no.3 train no.271290  loss = 3.90882 avg_loss = 3.30152\n",
      "epoch no.3 train no.271300  loss = 2.97725 avg_loss = 3.32317\n",
      "epoch no.3 train no.271310  loss = 4.24760 avg_loss = 3.30136\n",
      "epoch no.3 train no.271320  loss = 2.60235 avg_loss = 3.26030\n",
      "epoch no.3 train no.271330  loss = 2.11782 avg_loss = 3.24647\n",
      "epoch no.3 train no.271340  loss = 5.36097 avg_loss = 3.23553\n",
      "epoch no.3 train no.271350  loss = 2.85681 avg_loss = 3.23488\n",
      "epoch no.3 train no.271360  loss = 3.07856 avg_loss = 3.21980\n",
      "epoch no.3 train no.271370  loss = 3.01874 avg_loss = 3.18928\n",
      "epoch no.3 train no.271380  loss = 4.23659 avg_loss = 3.16709\n",
      "epoch no.3 train no.271390  loss = 2.99288 avg_loss = 3.16270\n",
      "epoch no.3 train no.271400  loss = 3.33940 avg_loss = 3.13448\n",
      "epoch no.3 train no.271410  loss = 3.60046 avg_loss = 3.18040\n",
      "epoch no.3 train no.271420  loss = 3.20409 avg_loss = 3.21423\n",
      "epoch no.3 train no.271430  loss = 3.55677 avg_loss = 3.21090\n",
      "epoch no.3 train no.271440  loss = 2.72444 avg_loss = 3.20720\n",
      "epoch no.3 train no.271450  loss = 3.45196 avg_loss = 3.23424\n",
      "epoch no.3 train no.271460  loss = 2.32124 avg_loss = 3.23835\n",
      "epoch no.3 train no.271470  loss = 4.16254 avg_loss = 3.26328\n",
      "epoch no.3 train no.271480  loss = 3.22282 avg_loss = 3.27413\n",
      "epoch no.3 train no.271490  loss = 3.86114 avg_loss = 3.27874\n",
      "epoch no.3 train no.271500  loss = 1.88071 avg_loss = 3.27682\n",
      "epoch no.3 train no.271510  loss = 2.34462 avg_loss = 3.29889\n",
      "epoch no.3 train no.271520  loss = 1.33444 avg_loss = 3.26510\n",
      "epoch no.3 train no.271530  loss = 4.28706 avg_loss = 3.30967\n",
      "epoch no.3 train no.271540  loss = 3.08046 avg_loss = 3.28020\n",
      "epoch no.3 train no.271550  loss = 2.07673 avg_loss = 3.26867\n",
      "epoch no.3 train no.271560  loss = 3.60182 avg_loss = 3.22719\n",
      "epoch no.3 train no.271570  loss = 4.32517 avg_loss = 3.28566\n",
      "epoch no.3 train no.271580  loss = 3.12665 avg_loss = 3.24408\n",
      "epoch no.3 train no.271590  loss = 3.36528 avg_loss = 3.25179\n",
      "epoch no.3 train no.271600  loss = 3.18685 avg_loss = 3.26086\n",
      "epoch no.3 train no.271610  loss = 4.59371 avg_loss = 3.34462\n",
      "epoch no.3 train no.271620  loss = 3.29837 avg_loss = 3.32842\n",
      "epoch no.3 train no.271630  loss = 2.08221 avg_loss = 3.29958\n",
      "epoch no.3 train no.271640  loss = 2.37675 avg_loss = 3.30927\n",
      "epoch no.3 train no.271650  loss = 3.32591 avg_loss = 3.28548\n",
      "epoch no.3 train no.271660  loss = 2.89119 avg_loss = 3.28048\n",
      "epoch no.3 train no.271670  loss = 2.93995 avg_loss = 3.30038\n",
      "epoch no.3 train no.271680  loss = 3.98497 avg_loss = 3.29984\n",
      "epoch no.3 train no.271690  loss = 3.69614 avg_loss = 3.29272\n",
      "epoch no.3 train no.271700  loss = 1.89804 avg_loss = 3.29905\n",
      "epoch no.3 train no.271710  loss = 3.62957 avg_loss = 3.32042\n",
      "epoch no.3 train no.271720  loss = 2.62247 avg_loss = 3.33859\n",
      "epoch no.3 train no.271730  loss = 4.24305 avg_loss = 3.37153\n",
      "epoch no.3 train no.271740  loss = 4.10688 avg_loss = 3.40027\n",
      "epoch no.3 train no.271750  loss = 3.89732 avg_loss = 3.40068\n",
      "epoch no.3 train no.271760  loss = 2.64744 avg_loss = 3.37328\n",
      "epoch no.3 train no.271770  loss = 4.12304 avg_loss = 3.43635\n",
      "epoch no.3 train no.271780  loss = 5.15014 avg_loss = 3.46884\n",
      "epoch no.3 train no.271790  loss = 2.61912 avg_loss = 3.43972\n",
      "epoch no.3 train no.271800  loss = 2.48669 avg_loss = 3.42212\n",
      "epoch no.3 train no.271810  loss = 3.41230 avg_loss = 3.39285\n",
      "epoch no.3 train no.271820  loss = 1.78323 avg_loss = 3.39064\n",
      "epoch no.3 train no.271830  loss = 3.24961 avg_loss = 3.39963\n",
      "epoch no.3 train no.271840  loss = 3.71851 avg_loss = 3.41896\n",
      "epoch no.3 train no.271850  loss = 2.93648 avg_loss = 3.34723\n",
      "epoch no.3 train no.271860  loss = 6.42302 avg_loss = 3.40012\n",
      "epoch no.3 train no.271870  loss = 3.74765 avg_loss = 3.41801\n",
      "epoch no.3 train no.271880  loss = 1.71461 avg_loss = 3.39102\n",
      "epoch no.3 train no.271890  loss = 4.56429 avg_loss = 3.40184\n",
      "epoch no.3 train no.271900  loss = 3.24310 avg_loss = 3.41327\n",
      "epoch no.3 train no.271910  loss = 4.89853 avg_loss = 3.42493\n",
      "epoch no.3 train no.271920  loss = 2.73802 avg_loss = 3.40086\n",
      "epoch no.3 train no.271930  loss = 3.68450 avg_loss = 3.46785\n",
      "epoch no.3 train no.271940  loss = 2.37063 avg_loss = 3.46255\n",
      "epoch no.3 train no.271950  loss = 3.57883 avg_loss = 3.41484\n",
      "epoch no.3 train no.271960  loss = 3.71402 avg_loss = 3.36403\n",
      "epoch no.3 train no.271970  loss = 2.96117 avg_loss = 3.34383\n",
      "epoch no.3 train no.271980  loss = 3.54321 avg_loss = 3.37381\n",
      "epoch no.3 train no.271990  loss = 3.82890 avg_loss = 3.39490\n",
      "epoch no.3 train no.272000  loss = 3.47085 avg_loss = 3.39438\n",
      "6\n",
      "to_tokens: ['▁비', '▁명', '▁때', '▁그', '▁시절', '▁그', '송', '</s>']\n",
      "추억의 그 때 그 시절 팝송</s>\n",
      "epoch no.3 train no.272010  loss = 3.77435 avg_loss = 3.44965\n",
      "epoch no.3 train no.272020  loss = 2.14737 avg_loss = 3.42357\n",
      "epoch no.3 train no.272030  loss = 2.86529 avg_loss = 3.38569\n",
      "epoch no.3 train no.272040  loss = 3.94941 avg_loss = 3.38550\n",
      "epoch no.3 train no.272050  loss = 4.82113 avg_loss = 3.47599\n",
      "epoch no.3 train no.272060  loss = 3.40051 avg_loss = 3.52346\n",
      "epoch no.3 train no.272070  loss = 1.72891 avg_loss = 3.43870\n",
      "epoch no.3 train no.272080  loss = 4.19322 avg_loss = 3.44604\n",
      "epoch no.3 train no.272090  loss = 4.19482 avg_loss = 3.45455\n",
      "epoch no.3 train no.272100  loss = 3.45702 avg_loss = 3.42183\n",
      "epoch no.3 train no.272110  loss = 3.34004 avg_loss = 3.42970\n",
      "epoch no.3 train no.272120  loss = 3.14746 avg_loss = 3.38250\n",
      "epoch no.3 train no.272130  loss = 3.60539 avg_loss = 3.39818\n",
      "epoch no.3 train no.272140  loss = 2.54204 avg_loss = 3.40082\n",
      "epoch no.3 train no.272150  loss = 2.76203 avg_loss = 3.40347\n",
      "epoch no.3 train no.272160  loss = 3.63076 avg_loss = 3.41075\n",
      "epoch no.3 train no.272170  loss = 4.82831 avg_loss = 3.41968\n",
      "epoch no.3 train no.272180  loss = 3.36502 avg_loss = 3.42643\n",
      "epoch no.3 train no.272190  loss = 4.29979 avg_loss = 3.46759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.272200  loss = 4.60455 avg_loss = 3.46568\n",
      "epoch no.3 train no.272210  loss = 3.41727 avg_loss = 3.41220\n",
      "epoch no.3 train no.272220  loss = 3.10435 avg_loss = 3.40577\n",
      "epoch no.3 train no.272230  loss = 1.82936 avg_loss = 3.40074\n",
      "epoch no.3 train no.272240  loss = 3.16714 avg_loss = 3.37905\n",
      "epoch no.3 train no.272250  loss = 1.54474 avg_loss = 3.40828\n",
      "epoch no.3 train no.272260  loss = 2.84424 avg_loss = 3.36395\n",
      "epoch no.3 train no.272270  loss = 5.16356 avg_loss = 3.32954\n",
      "epoch no.3 train no.272280  loss = 5.10805 avg_loss = 3.36032\n",
      "epoch no.3 train no.272290  loss = 2.41189 avg_loss = 3.30603\n",
      "epoch no.3 train no.272300  loss = 2.98631 avg_loss = 3.32099\n",
      "epoch no.3 train no.272310  loss = 2.99068 avg_loss = 3.31033\n",
      "epoch no.3 train no.272320  loss = 4.47053 avg_loss = 3.34337\n",
      "epoch no.3 train no.272330  loss = 3.91511 avg_loss = 3.34677\n",
      "epoch no.3 train no.272340  loss = 2.33198 avg_loss = 3.36864\n",
      "epoch no.3 train no.272350  loss = 2.26870 avg_loss = 3.37899\n",
      "epoch no.3 train no.272360  loss = 2.49040 avg_loss = 3.36636\n",
      "epoch no.3 train no.272370  loss = 3.98370 avg_loss = 3.39784\n",
      "epoch no.3 train no.272380  loss = 4.51476 avg_loss = 3.40088\n",
      "epoch no.3 train no.272390  loss = 3.86300 avg_loss = 3.38833\n",
      "epoch no.3 train no.272400  loss = 2.58733 avg_loss = 3.37211\n",
      "epoch no.3 train no.272410  loss = 2.02390 avg_loss = 3.33680\n",
      "epoch no.3 train no.272420  loss = 2.65684 avg_loss = 3.34605\n",
      "epoch no.3 train no.272430  loss = 3.95869 avg_loss = 3.35178\n",
      "epoch no.3 train no.272440  loss = 3.17002 avg_loss = 3.40116\n",
      "epoch no.3 train no.272450  loss = 3.92704 avg_loss = 3.38252\n",
      "epoch no.3 train no.272460  loss = 5.70774 avg_loss = 3.43070\n",
      "epoch no.3 train no.272470  loss = 2.66723 avg_loss = 3.43436\n",
      "epoch no.3 train no.272480  loss = 6.13900 avg_loss = 3.46194\n",
      "epoch no.3 train no.272490  loss = 3.47496 avg_loss = 3.45123\n",
      "epoch no.3 train no.272500  loss = 3.31324 avg_loss = 3.45445\n",
      "epoch no.3 train no.272510  loss = 4.40013 avg_loss = 3.47547\n",
      "epoch no.3 train no.272520  loss = 3.34552 avg_loss = 3.44084\n",
      "epoch no.3 train no.272530  loss = 3.94909 avg_loss = 3.46576\n",
      "epoch no.3 train no.272540  loss = 3.09345 avg_loss = 3.43291\n",
      "epoch no.3 train no.272550  loss = 2.48870 avg_loss = 3.50689\n",
      "epoch no.3 train no.272560  loss = 2.15399 avg_loss = 3.47639\n",
      "epoch no.3 train no.272570  loss = 2.59904 avg_loss = 3.44836\n",
      "epoch no.3 train no.272580  loss = 2.33710 avg_loss = 3.42949\n",
      "epoch no.3 train no.272590  loss = 3.93154 avg_loss = 3.41642\n",
      "epoch no.3 train no.272600  loss = 3.66604 avg_loss = 3.40123\n",
      "epoch no.3 train no.272610  loss = 2.00916 avg_loss = 3.35145\n",
      "epoch no.3 train no.272620  loss = 3.49146 avg_loss = 3.33457\n",
      "epoch no.3 train no.272630  loss = 2.78197 avg_loss = 3.32517\n",
      "epoch no.3 train no.272640  loss = 3.93374 avg_loss = 3.29588\n",
      "epoch no.3 train no.272650  loss = 1.26163 avg_loss = 3.26796\n",
      "epoch no.3 train no.272660  loss = 2.90390 avg_loss = 3.25742\n",
      "epoch no.3 train no.272670  loss = 4.09951 avg_loss = 3.25587\n",
      "epoch no.3 train no.272680  loss = 4.86276 avg_loss = 3.29367\n",
      "epoch no.3 train no.272690  loss = 2.55275 avg_loss = 3.30307\n",
      "epoch no.3 train no.272700  loss = 2.61814 avg_loss = 3.24003\n",
      "epoch no.3 train no.272710  loss = 4.69903 avg_loss = 3.30264\n",
      "epoch no.3 train no.272720  loss = 3.45952 avg_loss = 3.31593\n",
      "epoch no.3 train no.272730  loss = 3.11581 avg_loss = 3.34733\n",
      "epoch no.3 train no.272740  loss = 1.96618 avg_loss = 3.36272\n",
      "epoch no.3 train no.272750  loss = 4.00387 avg_loss = 3.40365\n",
      "epoch no.3 train no.272760  loss = 1.50647 avg_loss = 3.40880\n",
      "epoch no.3 train no.272770  loss = 1.84316 avg_loss = 3.34634\n",
      "epoch no.3 train no.272780  loss = 4.42346 avg_loss = 3.37269\n",
      "epoch no.3 train no.272790  loss = 3.10192 avg_loss = 3.41316\n",
      "epoch no.3 train no.272800  loss = 5.10935 avg_loss = 3.45314\n",
      "epoch no.3 train no.272810  loss = 2.31365 avg_loss = 3.48862\n",
      "epoch no.3 train no.272820  loss = 4.13205 avg_loss = 3.52704\n",
      "epoch no.3 train no.272830  loss = 2.78757 avg_loss = 3.51386\n",
      "epoch no.3 train no.272840  loss = 2.67257 avg_loss = 3.48776\n",
      "epoch no.3 train no.272850  loss = 3.73122 avg_loss = 3.49757\n",
      "epoch no.3 train no.272860  loss = 4.58542 avg_loss = 3.46563\n",
      "epoch no.3 train no.272870  loss = 2.21159 avg_loss = 3.47039\n",
      "epoch no.3 train no.272880  loss = 2.97688 avg_loss = 3.43706\n",
      "epoch no.3 train no.272890  loss = 3.30694 avg_loss = 3.43891\n",
      "epoch no.3 train no.272900  loss = 3.36470 avg_loss = 3.41155\n",
      "epoch no.3 train no.272910  loss = 2.79838 avg_loss = 3.42133\n",
      "epoch no.3 train no.272920  loss = 3.98498 avg_loss = 3.41013\n",
      "epoch no.3 train no.272930  loss = 1.81659 avg_loss = 3.35224\n",
      "epoch no.3 train no.272940  loss = 2.61660 avg_loss = 3.34896\n",
      "epoch no.3 train no.272950  loss = 4.23898 avg_loss = 3.35199\n",
      "epoch no.3 train no.272960  loss = 3.73418 avg_loss = 3.33264\n",
      "epoch no.3 train no.272970  loss = 2.30964 avg_loss = 3.28900\n",
      "epoch no.3 train no.272980  loss = 2.74950 avg_loss = 3.27636\n",
      "epoch no.3 train no.272990  loss = 2.37374 avg_loss = 3.24466\n",
      "epoch no.3 train no.273000  loss = 2.86364 avg_loss = 3.28148\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.3 train no.273010  loss = 2.61313 avg_loss = 3.30990\n",
      "epoch no.3 train no.273020  loss = 4.25509 avg_loss = 3.40330\n",
      "epoch no.3 train no.273030  loss = 3.48097 avg_loss = 3.40383\n",
      "epoch no.3 train no.273040  loss = 2.69996 avg_loss = 3.39895\n",
      "epoch no.3 train no.273050  loss = 3.82872 avg_loss = 3.41760\n",
      "epoch no.3 train no.273060  loss = 3.38987 avg_loss = 3.41853\n",
      "epoch no.3 train no.273070  loss = 4.36149 avg_loss = 3.46674\n",
      "epoch no.3 train no.273080  loss = 3.33635 avg_loss = 3.46051\n",
      "epoch no.3 train no.273090  loss = 3.00243 avg_loss = 3.44925\n",
      "epoch no.3 train no.273100  loss = 4.22478 avg_loss = 3.47170\n",
      "epoch no.3 train no.273110  loss = 4.37245 avg_loss = 3.49023\n",
      "epoch no.3 train no.273120  loss = 3.52052 avg_loss = 3.46502\n",
      "epoch no.3 train no.273130  loss = 3.60638 avg_loss = 3.47614\n",
      "epoch no.3 train no.273140  loss = 2.44524 avg_loss = 3.46333\n",
      "epoch no.3 train no.273150  loss = 2.97172 avg_loss = 3.44461\n",
      "epoch no.3 train no.273160  loss = 2.26728 avg_loss = 3.43198\n",
      "epoch no.3 train no.273170  loss = 4.52469 avg_loss = 3.40642\n",
      "epoch no.3 train no.273180  loss = 2.65657 avg_loss = 3.40101\n",
      "epoch no.3 train no.273190  loss = 2.75125 avg_loss = 3.38148\n",
      "epoch no.3 train no.273200  loss = 3.82152 avg_loss = 3.36949\n",
      "epoch no.3 train no.273210  loss = 2.75817 avg_loss = 3.40142\n",
      "epoch no.3 train no.273220  loss = 3.95846 avg_loss = 3.40635\n",
      "epoch no.3 train no.273230  loss = 2.38672 avg_loss = 3.36341\n",
      "epoch no.3 train no.273240  loss = 4.81335 avg_loss = 3.39043\n",
      "epoch no.3 train no.273250  loss = 5.47411 avg_loss = 3.43176\n",
      "epoch no.3 train no.273260  loss = 2.48420 avg_loss = 3.36959\n",
      "epoch no.3 train no.273270  loss = 2.00594 avg_loss = 3.36732\n",
      "epoch no.3 train no.273280  loss = 5.78369 avg_loss = 3.37224\n",
      "epoch no.3 train no.273290  loss = 3.32567 avg_loss = 3.38514\n",
      "epoch no.3 train no.273300  loss = 2.99160 avg_loss = 3.41662\n",
      "epoch no.3 train no.273310  loss = 3.00227 avg_loss = 3.40903\n",
      "epoch no.3 train no.273320  loss = 3.36978 avg_loss = 3.39168\n",
      "epoch no.3 train no.273330  loss = 2.23087 avg_loss = 3.35251\n",
      "epoch no.3 train no.273340  loss = 5.14751 avg_loss = 3.35607\n",
      "epoch no.3 train no.273350  loss = 4.27877 avg_loss = 3.39474\n",
      "epoch no.3 train no.273360  loss = 3.70870 avg_loss = 3.35742\n",
      "epoch no.3 train no.273370  loss = 3.01268 avg_loss = 3.41188\n",
      "epoch no.3 train no.273380  loss = 4.46532 avg_loss = 3.47199\n",
      "epoch no.3 train no.273390  loss = 2.84543 avg_loss = 3.49294\n",
      "epoch no.3 train no.273400  loss = 3.73221 avg_loss = 3.48332\n",
      "epoch no.3 train no.273410  loss = 3.48406 avg_loss = 3.48511\n",
      "epoch no.3 train no.273420  loss = 2.73211 avg_loss = 3.43845\n",
      "epoch no.3 train no.273430  loss = 2.39770 avg_loss = 3.46415\n",
      "epoch no.3 train no.273440  loss = 3.23145 avg_loss = 3.42281\n",
      "epoch no.3 train no.273450  loss = 5.08333 avg_loss = 3.41596\n",
      "epoch no.3 train no.273460  loss = 4.60125 avg_loss = 3.38901\n",
      "epoch no.3 train no.273470  loss = 3.06958 avg_loss = 3.35425\n",
      "epoch no.3 train no.273480  loss = 2.93521 avg_loss = 3.40407\n",
      "epoch no.3 train no.273490  loss = 2.86354 avg_loss = 3.34456\n",
      "epoch no.3 train no.273500  loss = 3.39552 avg_loss = 3.30228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.273510  loss = 4.51664 avg_loss = 3.31032\n",
      "epoch no.3 train no.273520  loss = 3.37448 avg_loss = 3.31945\n",
      "epoch no.3 train no.273530  loss = 2.15268 avg_loss = 3.30663\n",
      "epoch no.3 train no.273540  loss = 2.38115 avg_loss = 3.26057\n",
      "epoch no.3 train no.273550  loss = 2.00472 avg_loss = 3.25535\n",
      "epoch no.3 train no.273560  loss = 3.75623 avg_loss = 3.26192\n",
      "epoch no.3 train no.273570  loss = 4.72525 avg_loss = 3.28046\n",
      "epoch no.3 train no.273580  loss = 2.26982 avg_loss = 3.29766\n",
      "epoch no.3 train no.273590  loss = 4.71138 avg_loss = 3.25927\n",
      "epoch no.3 train no.273600  loss = 2.53961 avg_loss = 3.31727\n",
      "epoch no.3 train no.273610  loss = 2.98094 avg_loss = 3.33380\n",
      "epoch no.3 train no.273620  loss = 3.39481 avg_loss = 3.32720\n",
      "epoch no.3 train no.273630  loss = 5.84146 avg_loss = 3.39520\n",
      "epoch no.3 train no.273640  loss = 1.82422 avg_loss = 3.38513\n",
      "epoch no.3 train no.273650  loss = 1.43799 avg_loss = 3.39877\n",
      "epoch no.3 train no.273660  loss = 4.24607 avg_loss = 3.43427\n",
      "epoch no.3 train no.273670  loss = 4.36979 avg_loss = 3.44490\n",
      "epoch no.3 train no.273680  loss = 3.06853 avg_loss = 3.45295\n",
      "epoch no.3 train no.273690  loss = 2.32303 avg_loss = 3.43819\n",
      "epoch no.3 train no.273700  loss = 3.04157 avg_loss = 3.42949\n",
      "epoch no.3 train no.273710  loss = 2.31332 avg_loss = 3.39953\n",
      "epoch no.3 train no.273720  loss = 4.02446 avg_loss = 3.35383\n",
      "epoch no.3 train no.273730  loss = 3.27006 avg_loss = 3.31347\n",
      "epoch no.3 train no.273740  loss = 3.06284 avg_loss = 3.28118\n",
      "epoch no.3 train no.273750  loss = 4.16942 avg_loss = 3.25479\n",
      "epoch no.3 train no.273760  loss = 3.34892 avg_loss = 3.22329\n",
      "epoch no.3 train no.273770  loss = 2.26775 avg_loss = 3.23904\n",
      "epoch no.3 train no.273780  loss = 4.31552 avg_loss = 3.20675\n",
      "epoch no.3 train no.273790  loss = 4.11947 avg_loss = 3.20290\n",
      "epoch no.3 train no.273800  loss = 3.15762 avg_loss = 3.18820\n",
      "epoch no.3 train no.273810  loss = 2.81507 avg_loss = 3.16681\n",
      "epoch no.3 train no.273820  loss = 4.14482 avg_loss = 3.21196\n",
      "epoch no.3 train no.273830  loss = 6.43660 avg_loss = 3.27622\n",
      "epoch no.3 train no.273840  loss = 3.92670 avg_loss = 3.31871\n",
      "epoch no.3 train no.273850  loss = 2.33101 avg_loss = 3.29887\n",
      "epoch no.3 train no.273860  loss = 2.26866 avg_loss = 3.34951\n",
      "epoch no.3 train no.273870  loss = 2.24263 avg_loss = 3.31619\n",
      "epoch no.3 train no.273880  loss = 3.56990 avg_loss = 3.32623\n",
      "epoch no.3 train no.273890  loss = 3.35380 avg_loss = 3.32466\n",
      "epoch no.3 train no.273900  loss = 2.57018 avg_loss = 3.37611\n",
      "epoch no.3 train no.273910  loss = 4.03139 avg_loss = 3.34883\n",
      "epoch no.3 train no.273920  loss = 3.43213 avg_loss = 3.40390\n",
      "epoch no.3 train no.273930  loss = 3.52157 avg_loss = 3.40331\n",
      "epoch no.3 train no.273940  loss = 2.37647 avg_loss = 3.42450\n",
      "epoch no.3 train no.273950  loss = 2.94689 avg_loss = 3.40383\n",
      "epoch no.3 train no.273960  loss = 2.92669 avg_loss = 3.36909\n",
      "epoch no.3 train no.273970  loss = 3.23218 avg_loss = 3.37272\n",
      "epoch no.3 train no.273980  loss = 5.17023 avg_loss = 3.40119\n",
      "epoch no.3 train no.273990  loss = 3.28196 avg_loss = 3.38226\n",
      "epoch no.3 train no.274000  loss = 1.74607 avg_loss = 3.39486\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '송', '모', '곡', '</s>']\n",
      "추억의 팝송 명곡</s>\n",
      "epoch no.3 train no.274010  loss = 4.58706 avg_loss = 3.40283\n",
      "epoch no.3 train no.274020  loss = 3.90812 avg_loss = 3.37464\n",
      "epoch no.3 train no.274030  loss = 2.63244 avg_loss = 3.38108\n",
      "epoch no.3 train no.274040  loss = 5.31566 avg_loss = 3.41024\n",
      "epoch no.3 train no.274050  loss = 2.69989 avg_loss = 3.38256\n",
      "epoch no.3 train no.274060  loss = 4.11650 avg_loss = 3.35415\n",
      "epoch no.3 train no.274070  loss = 1.65384 avg_loss = 3.37489\n",
      "epoch no.3 train no.274080  loss = 2.39505 avg_loss = 3.35393\n",
      "epoch no.3 train no.274090  loss = 3.35723 avg_loss = 3.33179\n",
      "epoch no.3 train no.274100  loss = 3.11504 avg_loss = 3.32739\n",
      "epoch no.3 train no.274110  loss = 4.33750 avg_loss = 3.32876\n",
      "epoch no.3 train no.274120  loss = 2.53888 avg_loss = 3.33668\n",
      "epoch no.3 train no.274130  loss = 2.50654 avg_loss = 3.39148\n",
      "epoch no.3 train no.274140  loss = 3.07271 avg_loss = 3.38623\n",
      "epoch no.3 train no.274150  loss = 4.41088 avg_loss = 3.38780\n",
      "epoch no.3 train no.274160  loss = 2.97924 avg_loss = 3.36357\n",
      "epoch no.3 train no.274170  loss = 2.95660 avg_loss = 3.34213\n",
      "epoch no.3 train no.274180  loss = 3.42906 avg_loss = 3.36172\n",
      "epoch no.3 train no.274190  loss = 2.41320 avg_loss = 3.40977\n",
      "epoch no.3 train no.274200  loss = 3.62202 avg_loss = 3.42189\n",
      "epoch no.3 train no.274210  loss = 2.17727 avg_loss = 3.42938\n",
      "epoch no.3 train no.274220  loss = 4.57725 avg_loss = 3.39769\n",
      "epoch no.3 train no.274230  loss = 4.61831 avg_loss = 3.42015\n",
      "epoch no.3 train no.274240  loss = 2.12810 avg_loss = 3.40315\n",
      "epoch no.3 train no.274250  loss = 3.45453 avg_loss = 3.44117\n",
      "epoch no.3 train no.274260  loss = 4.19426 avg_loss = 3.45260\n",
      "epoch no.3 train no.274270  loss = 5.72797 avg_loss = 3.45609\n",
      "epoch no.3 train no.274280  loss = 4.41082 avg_loss = 3.46442\n",
      "epoch no.3 train no.274290  loss = 1.94810 avg_loss = 3.42088\n",
      "epoch no.3 train no.274300  loss = 3.09726 avg_loss = 3.43348\n",
      "epoch no.3 train no.274310  loss = 2.22817 avg_loss = 3.42365\n",
      "epoch no.3 train no.274320  loss = 2.28133 avg_loss = 3.38418\n",
      "epoch no.3 train no.274330  loss = 2.14985 avg_loss = 3.38219\n",
      "epoch no.3 train no.274340  loss = 3.12993 avg_loss = 3.41676\n",
      "epoch no.3 train no.274350  loss = 2.59609 avg_loss = 3.38671\n",
      "epoch no.3 train no.274360  loss = 2.20601 avg_loss = 3.38540\n",
      "epoch no.3 train no.274370  loss = 4.49865 avg_loss = 3.41745\n",
      "epoch no.3 train no.274380  loss = 1.59270 avg_loss = 3.40689\n",
      "epoch no.3 train no.274390  loss = 4.36199 avg_loss = 3.42717\n",
      "epoch no.3 train no.274400  loss = 3.94199 avg_loss = 3.39425\n",
      "epoch no.3 train no.274410  loss = 2.18301 avg_loss = 3.37377\n",
      "epoch no.3 train no.274420  loss = 2.86156 avg_loss = 3.34082\n",
      "epoch no.3 train no.274430  loss = 3.23401 avg_loss = 3.34011\n",
      "epoch no.3 train no.274440  loss = 2.85769 avg_loss = 3.37212\n",
      "epoch no.3 train no.274450  loss = 3.46978 avg_loss = 3.34575\n",
      "epoch no.3 train no.274460  loss = 2.90731 avg_loss = 3.33662\n",
      "epoch no.3 train no.274470  loss = 2.21825 avg_loss = 3.35284\n",
      "epoch no.3 train no.274480  loss = 4.21544 avg_loss = 3.37791\n",
      "epoch no.3 train no.274490  loss = 3.07531 avg_loss = 3.35149\n",
      "epoch no.3 train no.274500  loss = 2.60251 avg_loss = 3.30223\n",
      "epoch no.3 train no.274510  loss = 1.97661 avg_loss = 3.30668\n",
      "epoch no.3 train no.274520  loss = 3.60188 avg_loss = 3.33932\n",
      "epoch no.3 train no.274530  loss = 4.18583 avg_loss = 3.35745\n",
      "epoch no.3 train no.274540  loss = 2.27064 avg_loss = 3.34456\n",
      "epoch no.3 train no.274550  loss = 2.12392 avg_loss = 3.37867\n",
      "epoch no.3 train no.274560  loss = 3.81545 avg_loss = 3.35326\n",
      "epoch no.3 train no.274570  loss = 3.66024 avg_loss = 3.38408\n",
      "epoch no.3 train no.274580  loss = 3.40128 avg_loss = 3.39937\n",
      "epoch no.3 train no.274590  loss = 3.40158 avg_loss = 3.37981\n",
      "epoch no.3 train no.274600  loss = 4.50780 avg_loss = 3.36525\n",
      "epoch no.3 train no.274610  loss = 3.39517 avg_loss = 3.33971\n",
      "epoch no.3 train no.274620  loss = 4.37446 avg_loss = 3.35478\n",
      "epoch no.3 train no.274630  loss = 4.28026 avg_loss = 3.37903\n",
      "epoch no.3 train no.274640  loss = 2.75240 avg_loss = 3.38109\n",
      "epoch no.3 train no.274650  loss = 3.38992 avg_loss = 3.34964\n",
      "epoch no.3 train no.274660  loss = 2.76698 avg_loss = 3.28201\n",
      "epoch no.3 train no.274670  loss = 4.93331 avg_loss = 3.33048\n",
      "epoch no.3 train no.274680  loss = 3.65291 avg_loss = 3.29275\n",
      "epoch no.3 train no.274690  loss = 2.99729 avg_loss = 3.25668\n",
      "epoch no.3 train no.274700  loss = 3.01904 avg_loss = 3.25433\n",
      "epoch no.3 train no.274710  loss = 2.12802 avg_loss = 3.26820\n",
      "epoch no.3 train no.274720  loss = 2.20968 avg_loss = 3.24164\n",
      "epoch no.3 train no.274730  loss = 4.77975 avg_loss = 3.27987\n",
      "epoch no.3 train no.274740  loss = 2.48222 avg_loss = 3.27921\n",
      "epoch no.3 train no.274750  loss = 3.90883 avg_loss = 3.29307\n",
      "epoch no.3 train no.274760  loss = 2.54337 avg_loss = 3.29978\n",
      "epoch no.3 train no.274770  loss = 4.51890 avg_loss = 3.34118\n",
      "epoch no.3 train no.274780  loss = 4.19952 avg_loss = 3.35690\n",
      "epoch no.3 train no.274790  loss = 4.09904 avg_loss = 3.39146\n",
      "epoch no.3 train no.274800  loss = 2.57995 avg_loss = 3.38896\n",
      "epoch no.3 train no.274810  loss = 2.67721 avg_loss = 3.41673\n",
      "epoch no.3 train no.274820  loss = 3.85024 avg_loss = 3.42874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.274830  loss = 2.48406 avg_loss = 3.41298\n",
      "epoch no.3 train no.274840  loss = 3.70281 avg_loss = 3.40800\n",
      "epoch no.3 train no.274850  loss = 2.47114 avg_loss = 3.38348\n",
      "epoch no.3 train no.274860  loss = 3.11860 avg_loss = 3.39475\n",
      "epoch no.3 train no.274870  loss = 3.39820 avg_loss = 3.38391\n",
      "epoch no.3 train no.274880  loss = 3.75347 avg_loss = 3.33314\n",
      "epoch no.3 train no.274890  loss = 4.62537 avg_loss = 3.35615\n",
      "epoch no.3 train no.274900  loss = 4.52560 avg_loss = 3.38949\n",
      "epoch no.3 train no.274910  loss = 2.88215 avg_loss = 3.35348\n",
      "epoch no.3 train no.274920  loss = 2.67622 avg_loss = 3.32678\n",
      "epoch no.3 train no.274930  loss = 2.42000 avg_loss = 3.27911\n",
      "epoch no.3 train no.274940  loss = 3.19207 avg_loss = 3.30508\n",
      "epoch no.3 train no.274950  loss = 3.53786 avg_loss = 3.34498\n",
      "epoch no.3 train no.274960  loss = 3.22630 avg_loss = 3.35618\n",
      "epoch no.3 train no.274970  loss = 3.43035 avg_loss = 3.39301\n",
      "epoch no.3 train no.274980  loss = 2.72400 avg_loss = 3.33905\n",
      "epoch no.3 train no.274990  loss = 4.18444 avg_loss = 3.37405\n",
      "epoch no.3 train no.275000  loss = 2.91558 avg_loss = 3.34297\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.3 train no.275010  loss = 4.07704 avg_loss = 3.35417\n",
      "epoch no.3 train no.275020  loss = 5.30249 avg_loss = 3.36637\n",
      "epoch no.3 train no.275030  loss = 2.57319 avg_loss = 3.36762\n",
      "epoch no.3 train no.275040  loss = 4.73078 avg_loss = 3.37989\n",
      "epoch no.3 train no.275050  loss = 3.98620 avg_loss = 3.36612\n",
      "epoch no.3 train no.275060  loss = 2.59778 avg_loss = 3.37926\n",
      "epoch no.3 train no.275070  loss = 4.55201 avg_loss = 3.38448\n",
      "epoch no.3 train no.275080  loss = 3.06384 avg_loss = 3.39775\n",
      "epoch no.3 train no.275090  loss = 3.92782 avg_loss = 3.40009\n",
      "epoch no.3 train no.275100  loss = 3.23037 avg_loss = 3.38844\n",
      "epoch no.3 train no.275110  loss = 3.22045 avg_loss = 3.37497\n",
      "epoch no.3 train no.275120  loss = 4.54624 avg_loss = 3.37433\n",
      "epoch no.3 train no.275130  loss = 3.40039 avg_loss = 3.36607\n",
      "epoch no.3 train no.275140  loss = 4.85147 avg_loss = 3.42892\n",
      "epoch no.3 train no.275150  loss = 2.51678 avg_loss = 3.41526\n",
      "epoch no.3 train no.275160  loss = 2.73615 avg_loss = 3.40800\n",
      "epoch no.3 train no.275170  loss = 3.01027 avg_loss = 3.39795\n",
      "epoch no.3 train no.275180  loss = 4.00408 avg_loss = 3.38828\n",
      "epoch no.3 train no.275190  loss = 4.75823 avg_loss = 3.39026\n",
      "epoch no.3 train no.275200  loss = 3.02194 avg_loss = 3.40460\n",
      "epoch no.3 train no.275210  loss = 4.57729 avg_loss = 3.40453\n",
      "epoch no.3 train no.275220  loss = 2.94213 avg_loss = 3.38624\n",
      "epoch no.3 train no.275230  loss = 4.50282 avg_loss = 3.41743\n",
      "epoch no.3 train no.275240  loss = 2.94158 avg_loss = 3.36126\n",
      "epoch no.3 train no.275250  loss = 3.99288 avg_loss = 3.39918\n",
      "epoch no.3 train no.275260  loss = 2.34783 avg_loss = 3.33752\n",
      "epoch no.3 train no.275270  loss = 2.88913 avg_loss = 3.36606\n",
      "epoch no.3 train no.275280  loss = 5.37315 avg_loss = 3.36692\n",
      "epoch no.3 train no.275290  loss = 2.91428 avg_loss = 3.33316\n",
      "epoch no.3 train no.275300  loss = 2.53028 avg_loss = 3.31309\n",
      "epoch no.3 train no.275310  loss = 3.37978 avg_loss = 3.35887\n",
      "epoch no.3 train no.275320  loss = 5.49172 avg_loss = 3.38070\n",
      "epoch no.3 train no.275330  loss = 2.56193 avg_loss = 3.32985\n",
      "epoch no.3 train no.275340  loss = 5.13302 avg_loss = 3.31314\n",
      "epoch no.3 train no.275350  loss = 4.39391 avg_loss = 3.31876\n",
      "epoch no.3 train no.275360  loss = 4.27400 avg_loss = 3.36030\n",
      "epoch no.3 train no.275370  loss = 2.33693 avg_loss = 3.34375\n",
      "epoch no.3 train no.275380  loss = 2.62463 avg_loss = 3.37458\n",
      "epoch no.3 train no.275390  loss = 4.15165 avg_loss = 3.37780\n",
      "epoch no.3 train no.275400  loss = 3.20722 avg_loss = 3.42410\n",
      "epoch no.3 train no.275410  loss = 2.36759 avg_loss = 3.39758\n",
      "epoch no.3 train no.275420  loss = 3.50928 avg_loss = 3.45995\n",
      "epoch no.3 train no.275430  loss = 2.68409 avg_loss = 3.42171\n",
      "epoch no.3 train no.275440  loss = 3.24371 avg_loss = 3.38175\n",
      "epoch no.3 train no.275450  loss = 3.79135 avg_loss = 3.34434\n",
      "epoch no.3 train no.275460  loss = 3.80240 avg_loss = 3.36922\n",
      "epoch no.3 train no.275470  loss = 2.45886 avg_loss = 3.40704\n",
      "epoch no.3 train no.275480  loss = 2.52089 avg_loss = 3.41955\n",
      "epoch no.3 train no.275490  loss = 4.42155 avg_loss = 3.45807\n",
      "epoch no.3 train no.275500  loss = 3.26359 avg_loss = 3.46683\n",
      "epoch no.3 train no.275510  loss = 1.47306 avg_loss = 3.42636\n",
      "epoch no.3 train no.275520  loss = 2.74035 avg_loss = 3.39864\n",
      "epoch no.3 train no.275530  loss = 2.80532 avg_loss = 3.43402\n",
      "epoch no.3 train no.275540  loss = 2.10613 avg_loss = 3.42448\n",
      "epoch no.3 train no.275550  loss = 3.14518 avg_loss = 3.36109\n",
      "epoch no.3 train no.275560  loss = 3.98064 avg_loss = 3.36197\n",
      "epoch no.3 train no.275570  loss = 3.32641 avg_loss = 3.31580\n",
      "epoch no.3 train no.275580  loss = 3.27693 avg_loss = 3.33183\n",
      "epoch no.3 train no.275590  loss = 3.95485 avg_loss = 3.29820\n",
      "epoch no.3 train no.275600  loss = 3.26957 avg_loss = 3.38198\n",
      "epoch no.3 train no.275610  loss = 2.22480 avg_loss = 3.35252\n",
      "epoch no.3 train no.275620  loss = 1.69130 avg_loss = 3.40820\n",
      "epoch no.3 train no.275630  loss = 2.14400 avg_loss = 3.41196\n",
      "epoch no.3 train no.275640  loss = 2.89462 avg_loss = 3.42373\n",
      "epoch no.3 train no.275650  loss = 3.50884 avg_loss = 3.49329\n",
      "epoch no.3 train no.275660  loss = 3.59231 avg_loss = 3.50105\n",
      "epoch no.3 train no.275670  loss = 3.08048 avg_loss = 3.45159\n",
      "epoch no.3 train no.275680  loss = 2.69660 avg_loss = 3.45483\n",
      "epoch no.3 train no.275690  loss = 1.87756 avg_loss = 3.40087\n",
      "epoch no.3 train no.275700  loss = 3.19060 avg_loss = 3.41874\n",
      "epoch no.3 train no.275710  loss = 2.89576 avg_loss = 3.36780\n",
      "epoch no.3 train no.275720  loss = 3.67101 avg_loss = 3.39022\n",
      "epoch no.3 train no.275730  loss = 3.57422 avg_loss = 3.39896\n",
      "epoch no.3 train no.275740  loss = 2.15278 avg_loss = 3.37612\n",
      "epoch no.3 train no.275750  loss = 4.30867 avg_loss = 3.39086\n",
      "epoch no.3 train no.275760  loss = 2.96294 avg_loss = 3.38864\n",
      "epoch no.3 train no.275770  loss = 3.32601 avg_loss = 3.37381\n",
      "epoch no.3 train no.275780  loss = 4.04465 avg_loss = 3.37704\n",
      "epoch no.3 train no.275790  loss = 2.90744 avg_loss = 3.34903\n",
      "epoch no.3 train no.275800  loss = 3.30056 avg_loss = 3.35140\n",
      "epoch no.3 train no.275810  loss = 2.51153 avg_loss = 3.31809\n",
      "epoch no.3 train no.275820  loss = 6.82457 avg_loss = 3.39794\n",
      "epoch no.3 train no.275830  loss = 1.42553 avg_loss = 3.38232\n",
      "epoch no.3 train no.275840  loss = 2.35439 avg_loss = 3.39187\n",
      "epoch no.3 train no.275850  loss = 1.23447 avg_loss = 3.36518\n",
      "epoch no.3 train no.275860  loss = 3.56663 avg_loss = 3.38237\n",
      "epoch no.3 train no.275870  loss = 3.90856 avg_loss = 3.40783\n",
      "epoch no.3 train no.275880  loss = 5.92388 avg_loss = 3.43247\n",
      "epoch no.3 train no.275890  loss = 4.54200 avg_loss = 3.39444\n",
      "epoch no.3 train no.275900  loss = 5.01243 avg_loss = 3.40517\n",
      "epoch no.3 train no.275910  loss = 2.96162 avg_loss = 3.39828\n",
      "epoch no.3 train no.275920  loss = 3.43883 avg_loss = 3.38392\n",
      "epoch no.3 train no.275930  loss = 4.23549 avg_loss = 3.34443\n",
      "epoch no.3 train no.275940  loss = 4.14097 avg_loss = 3.37795\n",
      "epoch no.3 train no.275950  loss = 2.77622 avg_loss = 3.40186\n",
      "epoch no.3 train no.275960  loss = 3.02838 avg_loss = 3.41745\n",
      "epoch no.3 train no.275970  loss = 3.54711 avg_loss = 3.42339\n",
      "epoch no.3 train no.275980  loss = 2.64894 avg_loss = 3.34015\n",
      "epoch no.3 train no.275990  loss = 4.25120 avg_loss = 3.33149\n",
      "epoch no.3 train no.276000  loss = 4.42494 avg_loss = 3.36844\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁때', '▁그', 'st', '</s>']\n",
      "추억의 그 시절 ost</s>\n",
      "epoch no.3 train no.276010  loss = 2.45846 avg_loss = 3.33286\n",
      "epoch no.3 train no.276020  loss = 3.81413 avg_loss = 3.40128\n",
      "epoch no.3 train no.276030  loss = 3.04759 avg_loss = 3.38314\n",
      "epoch no.3 train no.276040  loss = 3.12916 avg_loss = 3.40899\n",
      "epoch no.3 train no.276050  loss = 2.90664 avg_loss = 3.42932\n",
      "epoch no.3 train no.276060  loss = 1.78368 avg_loss = 3.46640\n",
      "epoch no.3 train no.276070  loss = 4.31524 avg_loss = 3.46915\n",
      "epoch no.3 train no.276080  loss = 3.25643 avg_loss = 3.38714\n",
      "epoch no.3 train no.276090  loss = 2.42807 avg_loss = 3.36743\n",
      "epoch no.3 train no.276100  loss = 2.39821 avg_loss = 3.33894\n",
      "epoch no.3 train no.276110  loss = 3.33157 avg_loss = 3.33304\n",
      "epoch no.3 train no.276120  loss = 3.97378 avg_loss = 3.35617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.276130  loss = 2.85599 avg_loss = 3.38877\n",
      "epoch no.3 train no.276140  loss = 2.39757 avg_loss = 3.35975\n",
      "epoch no.3 train no.276150  loss = 2.07548 avg_loss = 3.38925\n",
      "epoch no.3 train no.276160  loss = 2.98465 avg_loss = 3.46667\n",
      "epoch no.3 train no.276170  loss = 3.69675 avg_loss = 3.42245\n",
      "epoch no.3 train no.276180  loss = 6.10902 avg_loss = 3.42868\n",
      "epoch no.3 train no.276190  loss = 2.12220 avg_loss = 3.42204\n",
      "epoch no.3 train no.276200  loss = 1.60436 avg_loss = 3.37777\n",
      "epoch no.3 train no.276210  loss = 4.06976 avg_loss = 3.39622\n",
      "epoch no.3 train no.276220  loss = 3.27370 avg_loss = 3.39826\n",
      "epoch no.3 train no.276230  loss = 2.78535 avg_loss = 3.40372\n",
      "epoch no.3 train no.276240  loss = 3.73434 avg_loss = 3.41353\n",
      "epoch no.3 train no.276250  loss = 6.38462 avg_loss = 3.47224\n",
      "epoch no.3 train no.276260  loss = 2.59929 avg_loss = 3.41804\n",
      "epoch no.3 train no.276270  loss = 2.36335 avg_loss = 3.37208\n",
      "epoch no.3 train no.276280  loss = 6.17028 avg_loss = 3.40458\n",
      "epoch no.3 train no.276290  loss = 3.92984 avg_loss = 3.40163\n",
      "epoch no.3 train no.276300  loss = 5.13691 avg_loss = 3.39474\n",
      "epoch no.3 train no.276310  loss = 3.52393 avg_loss = 3.40151\n",
      "epoch no.3 train no.276320  loss = 3.82408 avg_loss = 3.42922\n",
      "epoch no.3 train no.276330  loss = 2.47505 avg_loss = 3.43271\n",
      "epoch no.3 train no.276340  loss = 3.64861 avg_loss = 3.38355\n",
      "epoch no.3 train no.276350  loss = 5.19408 avg_loss = 3.41457\n",
      "epoch no.3 train no.276360  loss = 2.74094 avg_loss = 3.43405\n",
      "epoch no.3 train no.276370  loss = 3.31745 avg_loss = 3.45907\n",
      "epoch no.3 train no.276380  loss = 1.95058 avg_loss = 3.44005\n",
      "epoch no.3 train no.276390  loss = 3.89177 avg_loss = 3.40499\n",
      "epoch no.3 train no.276400  loss = 3.75058 avg_loss = 3.35749\n",
      "epoch no.3 train no.276410  loss = 3.62654 avg_loss = 3.36572\n",
      "epoch no.3 train no.276420  loss = 4.46043 avg_loss = 3.39880\n",
      "epoch no.3 train no.276430  loss = 3.51298 avg_loss = 3.41437\n",
      "epoch no.3 train no.276440  loss = 3.49945 avg_loss = 3.42321\n",
      "epoch no.3 train no.276450  loss = 4.92184 avg_loss = 3.40646\n",
      "epoch no.3 train no.276460  loss = 3.09982 avg_loss = 3.38621\n",
      "epoch no.3 train no.276470  loss = 2.41919 avg_loss = 3.40888\n",
      "epoch no.3 train no.276480  loss = 2.97372 avg_loss = 3.36705\n",
      "epoch no.3 train no.276490  loss = 3.07506 avg_loss = 3.35902\n",
      "epoch no.3 train no.276500  loss = 3.62253 avg_loss = 3.39139\n",
      "epoch no.3 train no.276510  loss = 3.37022 avg_loss = 3.36330\n",
      "epoch no.3 train no.276520  loss = 7.64694 avg_loss = 3.37185\n",
      "epoch no.3 train no.276530  loss = 4.31597 avg_loss = 3.38091\n",
      "epoch no.3 train no.276540  loss = 3.11351 avg_loss = 3.36148\n",
      "epoch no.3 train no.276550  loss = 2.73085 avg_loss = 3.36375\n",
      "epoch no.3 train no.276560  loss = 4.11778 avg_loss = 3.33643\n",
      "epoch no.3 train no.276570  loss = 2.90136 avg_loss = 3.36247\n",
      "epoch no.3 train no.276580  loss = 3.73296 avg_loss = 3.36045\n",
      "epoch no.3 train no.276590  loss = 4.75716 avg_loss = 3.35149\n",
      "epoch no.3 train no.276600  loss = 2.65566 avg_loss = 3.29911\n",
      "epoch no.3 train no.276610  loss = 2.42210 avg_loss = 3.31539\n",
      "epoch no.3 train no.276620  loss = 3.53119 avg_loss = 3.36941\n",
      "epoch no.3 train no.276630  loss = 1.97107 avg_loss = 3.41602\n",
      "epoch no.3 train no.276640  loss = 4.20580 avg_loss = 3.44303\n",
      "epoch no.3 train no.276650  loss = 4.66102 avg_loss = 3.43440\n",
      "epoch no.3 train no.276660  loss = 2.98132 avg_loss = 3.42134\n",
      "epoch no.3 train no.276670  loss = 4.69582 avg_loss = 3.41234\n",
      "epoch no.3 train no.276680  loss = 2.74099 avg_loss = 3.35760\n",
      "epoch no.3 train no.276690  loss = 1.83053 avg_loss = 3.33789\n",
      "epoch no.3 train no.276700  loss = 4.52118 avg_loss = 3.34191\n",
      "epoch no.3 train no.276710  loss = 3.94875 avg_loss = 3.32771\n",
      "epoch no.3 train no.276720  loss = 1.71063 avg_loss = 3.33168\n",
      "epoch no.3 train no.276730  loss = 3.79030 avg_loss = 3.31888\n",
      "epoch no.3 train no.276740  loss = 2.78286 avg_loss = 3.30905\n",
      "epoch no.3 train no.276750  loss = 2.88532 avg_loss = 3.31031\n",
      "epoch no.3 train no.276760  loss = 4.33673 avg_loss = 3.33836\n",
      "epoch no.3 train no.276770  loss = 3.79679 avg_loss = 3.35287\n",
      "epoch no.3 train no.276780  loss = 4.60746 avg_loss = 3.39010\n",
      "epoch no.3 train no.276790  loss = 1.40504 avg_loss = 3.40542\n",
      "epoch no.3 train no.276800  loss = 2.38894 avg_loss = 3.38827\n",
      "epoch no.3 train no.276810  loss = 4.47138 avg_loss = 3.38586\n",
      "epoch no.3 train no.276820  loss = 2.78206 avg_loss = 3.42826\n",
      "epoch no.3 train no.276830  loss = 2.43738 avg_loss = 3.42446\n",
      "epoch no.3 train no.276840  loss = 1.91763 avg_loss = 3.34414\n",
      "epoch no.3 train no.276850  loss = 3.26366 avg_loss = 3.34581\n",
      "epoch no.3 train no.276860  loss = 3.11663 avg_loss = 3.33479\n",
      "epoch no.3 train no.276870  loss = 3.60776 avg_loss = 3.34396\n",
      "epoch no.3 train no.276880  loss = 3.08076 avg_loss = 3.29746\n",
      "epoch no.3 train no.276890  loss = 4.51448 avg_loss = 3.34884\n",
      "epoch no.3 train no.276900  loss = 3.34687 avg_loss = 3.31995\n",
      "epoch no.3 train no.276910  loss = 3.04180 avg_loss = 3.27624\n",
      "epoch no.3 train no.276920  loss = 4.16109 avg_loss = 3.28717\n",
      "epoch no.3 train no.276930  loss = 3.83705 avg_loss = 3.28580\n",
      "epoch no.3 train no.276940  loss = 2.96441 avg_loss = 3.30803\n",
      "epoch no.3 train no.276950  loss = 2.78306 avg_loss = 3.31700\n",
      "epoch no.3 train no.276960  loss = 1.65878 avg_loss = 3.32288\n",
      "epoch no.3 train no.276970  loss = 3.27403 avg_loss = 3.33998\n",
      "epoch no.3 train no.276980  loss = 3.20858 avg_loss = 3.36100\n",
      "epoch no.3 train no.276990  loss = 3.25522 avg_loss = 3.40951\n",
      "epoch no.3 train no.277000  loss = 2.36135 avg_loss = 3.38859\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '▁o', 'st', '▁모음', '집', '</s>']\n",
      "추억의 드라마 ost 모음집</s>\n",
      "epoch no.3 train no.277010  loss = 3.27436 avg_loss = 3.38266\n",
      "epoch no.3 train no.277020  loss = 3.76255 avg_loss = 3.38766\n",
      "epoch no.3 train no.277030  loss = 2.17405 avg_loss = 3.33847\n",
      "epoch no.3 train no.277040  loss = 4.46643 avg_loss = 3.35897\n",
      "epoch no.3 train no.277050  loss = 2.74096 avg_loss = 3.34968\n",
      "epoch no.3 train no.277060  loss = 4.23703 avg_loss = 3.37615\n",
      "epoch no.3 train no.277070  loss = 2.15618 avg_loss = 3.38352\n",
      "epoch no.3 train no.277080  loss = 2.56605 avg_loss = 3.37598\n",
      "epoch no.3 train no.277090  loss = 2.58887 avg_loss = 3.38032\n",
      "epoch no.3 train no.277100  loss = 2.97375 avg_loss = 3.41766\n",
      "epoch no.3 train no.277110  loss = 3.71925 avg_loss = 3.43015\n",
      "epoch no.3 train no.277120  loss = 4.19209 avg_loss = 3.48205\n",
      "epoch no.3 train no.277130  loss = 3.92298 avg_loss = 3.43944\n",
      "epoch no.3 train no.277140  loss = 3.69292 avg_loss = 3.46642\n",
      "epoch no.3 train no.277150  loss = 4.52950 avg_loss = 3.43584\n",
      "epoch no.3 train no.277160  loss = 2.05970 avg_loss = 3.41174\n",
      "epoch no.3 train no.277170  loss = 2.86247 avg_loss = 3.40506\n",
      "epoch no.3 train no.277180  loss = 2.89644 avg_loss = 3.38688\n",
      "epoch no.3 train no.277190  loss = 2.40236 avg_loss = 3.36321\n",
      "epoch no.3 train no.277200  loss = 3.06512 avg_loss = 3.32193\n",
      "epoch no.3 train no.277210  loss = 1.90037 avg_loss = 3.29743\n",
      "epoch no.3 train no.277220  loss = 3.04269 avg_loss = 3.29761\n",
      "epoch no.3 train no.277230  loss = 2.49616 avg_loss = 3.31510\n",
      "epoch no.3 train no.277240  loss = 3.69277 avg_loss = 3.28539\n",
      "epoch no.3 train no.277250  loss = 2.17330 avg_loss = 3.25818\n",
      "epoch no.3 train no.277260  loss = 3.99959 avg_loss = 3.25277\n",
      "epoch no.3 train no.277270  loss = 2.29819 avg_loss = 3.25877\n",
      "epoch no.3 train no.277280  loss = 2.33647 avg_loss = 3.26032\n",
      "epoch no.3 train no.277290  loss = 3.23085 avg_loss = 3.27018\n",
      "epoch no.3 train no.277300  loss = 2.54728 avg_loss = 3.29693\n",
      "epoch no.3 train no.277310  loss = 3.85069 avg_loss = 3.32470\n",
      "epoch no.3 train no.277320  loss = 3.08174 avg_loss = 3.34132\n",
      "epoch no.3 train no.277330  loss = 2.16873 avg_loss = 3.36951\n",
      "epoch no.3 train no.277340  loss = 3.60195 avg_loss = 3.36453\n",
      "epoch no.3 train no.277350  loss = 3.84515 avg_loss = 3.37215\n",
      "epoch no.3 train no.277360  loss = 3.22724 avg_loss = 3.32773\n",
      "epoch no.3 train no.277370  loss = 3.18800 avg_loss = 3.33447\n",
      "epoch no.3 train no.277380  loss = 2.14946 avg_loss = 3.34068\n",
      "epoch no.3 train no.277390  loss = 4.87656 avg_loss = 3.31930\n",
      "epoch no.3 train no.277400  loss = 5.00650 avg_loss = 3.37399\n",
      "epoch no.3 train no.277410  loss = 3.27709 avg_loss = 3.38022\n",
      "epoch no.3 train no.277420  loss = 5.10643 avg_loss = 3.36976\n",
      "epoch no.3 train no.277430  loss = 3.51642 avg_loss = 3.38250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.277440  loss = 2.61843 avg_loss = 3.31408\n",
      "epoch no.3 train no.277450  loss = 3.52093 avg_loss = 3.32626\n",
      "epoch no.3 train no.277460  loss = 3.60358 avg_loss = 3.31317\n",
      "epoch no.3 train no.277470  loss = 2.85022 avg_loss = 3.33183\n",
      "epoch no.3 train no.277480  loss = 2.52383 avg_loss = 3.33244\n",
      "epoch no.3 train no.277490  loss = 3.14646 avg_loss = 3.34064\n",
      "epoch no.3 train no.277500  loss = 5.28177 avg_loss = 3.43687\n",
      "epoch no.3 train no.277510  loss = 4.14746 avg_loss = 3.45648\n",
      "epoch no.3 train no.277520  loss = 3.63687 avg_loss = 3.41409\n",
      "epoch no.3 train no.277530  loss = 4.19750 avg_loss = 3.43460\n",
      "epoch no.3 train no.277540  loss = 4.65429 avg_loss = 3.41751\n",
      "epoch no.3 train no.277550  loss = 3.16683 avg_loss = 3.42536\n",
      "epoch no.3 train no.277560  loss = 3.56599 avg_loss = 3.42426\n",
      "epoch no.3 train no.277570  loss = 2.84317 avg_loss = 3.42820\n",
      "epoch no.3 train no.277580  loss = 3.21462 avg_loss = 3.42811\n",
      "epoch no.3 train no.277590  loss = 2.35909 avg_loss = 3.38666\n",
      "epoch no.3 train no.277600  loss = 5.16572 avg_loss = 3.35487\n",
      "epoch no.3 train no.277610  loss = 2.06280 avg_loss = 3.33039\n",
      "epoch no.3 train no.277620  loss = 2.67199 avg_loss = 3.32208\n",
      "epoch no.3 train no.277630  loss = 1.79657 avg_loss = 3.28475\n",
      "epoch no.3 train no.277640  loss = 2.38422 avg_loss = 3.27979\n",
      "epoch no.3 train no.277650  loss = 3.39036 avg_loss = 3.31203\n",
      "epoch no.3 train no.277660  loss = 3.65158 avg_loss = 3.30771\n",
      "epoch no.3 train no.277670  loss = 2.68789 avg_loss = 3.27208\n",
      "epoch no.3 train no.277680  loss = 4.25420 avg_loss = 3.28836\n",
      "epoch no.3 train no.277690  loss = 1.86796 avg_loss = 3.29908\n",
      "epoch no.3 train no.277700  loss = 5.71710 avg_loss = 3.30503\n",
      "epoch no.3 train no.277710  loss = 3.72342 avg_loss = 3.32250\n",
      "epoch no.3 train no.277720  loss = 2.31847 avg_loss = 3.33605\n",
      "epoch no.3 train no.277730  loss = 2.89694 avg_loss = 3.34025\n",
      "epoch no.3 train no.277740  loss = 3.46832 avg_loss = 3.35818\n",
      "epoch no.3 train no.277750  loss = 3.74453 avg_loss = 3.33376\n",
      "epoch no.3 train no.277760  loss = 2.44444 avg_loss = 3.32321\n",
      "epoch no.3 train no.277770  loss = 2.97906 avg_loss = 3.28996\n",
      "epoch no.3 train no.277780  loss = 3.53959 avg_loss = 3.26254\n",
      "epoch no.3 train no.277790  loss = 1.81142 avg_loss = 3.31342\n",
      "epoch no.3 train no.277800  loss = 3.20994 avg_loss = 3.30708\n",
      "epoch no.3 train no.277810  loss = 3.33986 avg_loss = 3.35361\n",
      "epoch no.3 train no.277820  loss = 2.61695 avg_loss = 3.32255\n",
      "epoch no.3 train no.277830  loss = 5.42201 avg_loss = 3.34676\n",
      "epoch no.3 train no.277840  loss = 3.92415 avg_loss = 3.34382\n",
      "epoch no.3 train no.277850  loss = 3.07536 avg_loss = 3.34875\n",
      "epoch no.3 train no.277860  loss = 2.50518 avg_loss = 3.33165\n",
      "epoch no.3 train no.277870  loss = 2.73557 avg_loss = 3.32192\n",
      "epoch no.3 train no.277880  loss = 2.52164 avg_loss = 3.29119\n",
      "epoch no.3 train no.277890  loss = 2.71083 avg_loss = 3.28467\n",
      "epoch no.3 train no.277900  loss = 4.33195 avg_loss = 3.38761\n",
      "epoch no.3 train no.277910  loss = 2.31387 avg_loss = 3.39338\n",
      "epoch no.3 train no.277920  loss = 4.92740 avg_loss = 3.41465\n",
      "epoch no.3 train no.277930  loss = 3.61015 avg_loss = 3.38571\n",
      "epoch no.3 train no.277940  loss = 3.71913 avg_loss = 3.36785\n",
      "epoch no.3 train no.277950  loss = 3.82291 avg_loss = 3.42614\n",
      "epoch no.3 train no.277960  loss = 3.56763 avg_loss = 3.36906\n",
      "epoch no.3 train no.277970  loss = 2.53794 avg_loss = 3.35491\n",
      "epoch no.3 train no.277980  loss = 3.58748 avg_loss = 3.32482\n",
      "epoch no.3 train no.277990  loss = 2.81436 avg_loss = 3.38626\n",
      "epoch no.3 train no.278000  loss = 2.20673 avg_loss = 3.38608\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.3 train no.278010  loss = 3.30531 avg_loss = 3.36144\n",
      "epoch no.3 train no.278020  loss = 2.96706 avg_loss = 3.35595\n",
      "epoch no.3 train no.278030  loss = 2.30329 avg_loss = 3.31972\n",
      "epoch no.3 train no.278040  loss = 5.36275 avg_loss = 3.37120\n",
      "epoch no.3 train no.278050  loss = 3.34263 avg_loss = 3.37931\n",
      "epoch no.3 train no.278060  loss = 3.75477 avg_loss = 3.36044\n",
      "epoch no.3 train no.278070  loss = 4.62864 avg_loss = 3.38702\n",
      "epoch no.3 train no.278080  loss = 4.66111 avg_loss = 3.39257\n",
      "epoch no.3 train no.278090  loss = 3.00868 avg_loss = 3.34175\n",
      "epoch no.3 train no.278100  loss = 2.32704 avg_loss = 3.33972\n",
      "epoch no.3 train no.278110  loss = 2.17501 avg_loss = 3.37982\n",
      "epoch no.3 train no.278120  loss = 1.59060 avg_loss = 3.32122\n",
      "epoch no.3 train no.278130  loss = 2.19076 avg_loss = 3.32126\n",
      "epoch no.3 train no.278140  loss = 2.25215 avg_loss = 3.33368\n",
      "epoch no.3 train no.278150  loss = 1.74708 avg_loss = 3.32926\n",
      "epoch no.3 train no.278160  loss = 3.42753 avg_loss = 3.32958\n",
      "epoch no.3 train no.278170  loss = 3.49135 avg_loss = 3.34336\n",
      "epoch no.3 train no.278180  loss = 2.59409 avg_loss = 3.33611\n",
      "epoch no.3 train no.278190  loss = 3.58211 avg_loss = 3.36126\n",
      "epoch no.3 train no.278200  loss = 2.23607 avg_loss = 3.30891\n",
      "epoch no.3 train no.278210  loss = 1.55795 avg_loss = 3.27898\n",
      "epoch no.3 train no.278220  loss = 3.34315 avg_loss = 3.30549\n",
      "epoch no.3 train no.278230  loss = 4.16409 avg_loss = 3.35728\n",
      "epoch no.3 train no.278240  loss = 3.01113 avg_loss = 3.36054\n",
      "epoch no.3 train no.278250  loss = 3.21528 avg_loss = 3.32946\n",
      "epoch no.3 train no.278260  loss = 3.25900 avg_loss = 3.34766\n",
      "epoch no.3 train no.278270  loss = 1.59943 avg_loss = 3.36174\n",
      "epoch no.3 train no.278280  loss = 3.06709 avg_loss = 3.38319\n",
      "epoch no.3 train no.278290  loss = 3.65734 avg_loss = 3.41426\n",
      "epoch no.3 train no.278300  loss = 3.11613 avg_loss = 3.36574\n",
      "epoch no.3 train no.278310  loss = 3.31531 avg_loss = 3.37919\n",
      "epoch no.3 train no.278320  loss = 3.46378 avg_loss = 3.38249\n",
      "epoch no.3 train no.278330  loss = 3.35415 avg_loss = 3.35212\n",
      "epoch no.3 train no.278340  loss = 2.23995 avg_loss = 3.33236\n",
      "epoch no.3 train no.278350  loss = 3.53257 avg_loss = 3.33171\n",
      "epoch no.3 train no.278360  loss = 2.15310 avg_loss = 3.30438\n",
      "epoch no.3 train no.278370  loss = 2.76627 avg_loss = 3.30270\n",
      "epoch no.3 train no.278380  loss = 3.30642 avg_loss = 3.28555\n",
      "epoch no.3 train no.278390  loss = 2.61901 avg_loss = 3.30598\n",
      "epoch no.3 train no.278400  loss = 2.75212 avg_loss = 3.30902\n",
      "epoch no.3 train no.278410  loss = 3.49500 avg_loss = 3.30159\n",
      "epoch no.3 train no.278420  loss = 3.20508 avg_loss = 3.26846\n",
      "epoch no.3 train no.278430  loss = 2.23354 avg_loss = 3.24041\n",
      "epoch no.3 train no.278440  loss = 2.68319 avg_loss = 3.23331\n",
      "epoch no.3 train no.278450  loss = 3.47605 avg_loss = 3.23899\n",
      "epoch no.3 train no.278460  loss = 2.88476 avg_loss = 3.22330\n",
      "epoch no.3 train no.278470  loss = 6.54649 avg_loss = 3.26493\n",
      "epoch no.3 train no.278480  loss = 4.37664 avg_loss = 3.28085\n",
      "epoch no.3 train no.278490  loss = 3.80490 avg_loss = 3.25789\n",
      "epoch no.3 train no.278500  loss = 1.71291 avg_loss = 3.24416\n",
      "epoch no.3 train no.278510  loss = 7.58057 avg_loss = 3.35196\n",
      "epoch no.3 train no.278520  loss = 4.27996 avg_loss = 3.36520\n",
      "epoch no.3 train no.278530  loss = 3.04576 avg_loss = 3.39488\n",
      "epoch no.3 train no.278540  loss = 2.72155 avg_loss = 3.37403\n",
      "epoch no.3 train no.278550  loss = 2.20399 avg_loss = 3.36234\n",
      "epoch no.3 train no.278560  loss = 3.60818 avg_loss = 3.37078\n",
      "epoch no.3 train no.278570  loss = 3.67720 avg_loss = 3.42174\n",
      "epoch no.3 train no.278580  loss = 2.61960 avg_loss = 3.42816\n",
      "epoch no.3 train no.278590  loss = 3.07784 avg_loss = 3.41855\n",
      "epoch no.3 train no.278600  loss = 1.93359 avg_loss = 3.38157\n",
      "epoch no.3 train no.278610  loss = 4.23577 avg_loss = 3.36117\n",
      "epoch no.3 train no.278620  loss = 2.70360 avg_loss = 3.33286\n",
      "epoch no.3 train no.278630  loss = 5.32998 avg_loss = 3.43894\n",
      "epoch no.3 train no.278640  loss = 3.25171 avg_loss = 3.43098\n",
      "epoch no.3 train no.278650  loss = 4.17241 avg_loss = 3.41966\n",
      "epoch no.3 train no.278660  loss = 3.72031 avg_loss = 3.41401\n",
      "epoch no.3 train no.278670  loss = 3.65063 avg_loss = 3.33330\n",
      "epoch no.3 train no.278680  loss = 4.24888 avg_loss = 3.33670\n",
      "epoch no.3 train no.278690  loss = 2.82029 avg_loss = 3.39624\n",
      "epoch no.3 train no.278700  loss = 2.58457 avg_loss = 3.38746\n",
      "epoch no.3 train no.278710  loss = 2.57497 avg_loss = 3.36279\n",
      "epoch no.3 train no.278720  loss = 3.70648 avg_loss = 3.36889\n",
      "epoch no.3 train no.278730  loss = 3.38169 avg_loss = 3.35310\n",
      "epoch no.3 train no.278740  loss = 4.06900 avg_loss = 3.40128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.278750  loss = 3.35282 avg_loss = 3.40836\n",
      "epoch no.3 train no.278760  loss = 4.76747 avg_loss = 3.39702\n",
      "epoch no.3 train no.278770  loss = 3.89490 avg_loss = 3.44988\n",
      "epoch no.3 train no.278780  loss = 2.34310 avg_loss = 3.44370\n",
      "epoch no.3 train no.278790  loss = 4.24487 avg_loss = 3.43435\n",
      "epoch no.3 train no.278800  loss = 4.23910 avg_loss = 3.43145\n",
      "epoch no.3 train no.278810  loss = 4.37969 avg_loss = 3.44127\n",
      "epoch no.3 train no.278820  loss = 2.15900 avg_loss = 3.42635\n",
      "epoch no.3 train no.278830  loss = 3.04097 avg_loss = 3.41717\n",
      "epoch no.3 train no.278840  loss = 3.23506 avg_loss = 3.40918\n",
      "epoch no.3 train no.278850  loss = 3.52405 avg_loss = 3.40922\n",
      "epoch no.3 train no.278860  loss = 3.19674 avg_loss = 3.40955\n",
      "epoch no.3 train no.278870  loss = 2.86818 avg_loss = 3.43025\n",
      "epoch no.3 train no.278880  loss = 3.94687 avg_loss = 3.43274\n",
      "epoch no.3 train no.278890  loss = 4.31148 avg_loss = 3.48715\n",
      "epoch no.3 train no.278900  loss = 3.30521 avg_loss = 3.44097\n",
      "epoch no.3 train no.278910  loss = 3.77298 avg_loss = 3.40802\n",
      "epoch no.3 train no.278920  loss = 1.42049 avg_loss = 3.39253\n",
      "epoch no.3 train no.278930  loss = 2.88471 avg_loss = 3.37998\n",
      "epoch no.3 train no.278940  loss = 3.65519 avg_loss = 3.34502\n",
      "epoch no.3 train no.278950  loss = 3.09208 avg_loss = 3.32632\n",
      "epoch no.3 train no.278960  loss = 2.56476 avg_loss = 3.33822\n",
      "epoch no.3 train no.278970  loss = 5.79965 avg_loss = 3.39501\n",
      "epoch no.3 train no.278980  loss = 2.99563 avg_loss = 3.38319\n",
      "epoch no.3 train no.278990  loss = 4.52939 avg_loss = 3.37048\n",
      "epoch no.3 train no.279000  loss = 3.81307 avg_loss = 3.37791\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁명', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.3 train no.279010  loss = 2.84244 avg_loss = 3.36172\n",
      "epoch no.3 train no.279020  loss = 2.64790 avg_loss = 3.35351\n",
      "epoch no.3 train no.279030  loss = 3.26972 avg_loss = 3.39001\n",
      "epoch no.3 train no.279040  loss = 2.03278 avg_loss = 3.39395\n",
      "epoch no.3 train no.279050  loss = 3.47769 avg_loss = 3.36849\n",
      "epoch no.3 train no.279060  loss = 3.37558 avg_loss = 3.38734\n",
      "epoch no.3 train no.279070  loss = 4.46749 avg_loss = 3.35583\n",
      "epoch no.3 train no.279080  loss = 1.94828 avg_loss = 3.30170\n",
      "epoch no.3 train no.279090  loss = 5.50122 avg_loss = 3.29226\n",
      "epoch no.3 train no.279100  loss = 2.48119 avg_loss = 3.28877\n",
      "epoch no.3 train no.279110  loss = 5.33236 avg_loss = 3.30259\n",
      "epoch no.3 train no.279120  loss = 2.10976 avg_loss = 3.28737\n",
      "epoch no.3 train no.279130  loss = 3.11524 avg_loss = 3.30397\n",
      "epoch no.3 train no.279140  loss = 3.34811 avg_loss = 3.33235\n",
      "epoch no.3 train no.279150  loss = 2.66917 avg_loss = 3.28160\n",
      "epoch no.3 train no.279160  loss = 4.10589 avg_loss = 3.29091\n",
      "epoch no.3 train no.279170  loss = 2.71786 avg_loss = 3.24155\n",
      "epoch no.3 train no.279180  loss = 1.56919 avg_loss = 3.20722\n",
      "epoch no.3 train no.279190  loss = 2.39408 avg_loss = 3.20289\n",
      "epoch no.3 train no.279200  loss = 2.01208 avg_loss = 3.23727\n",
      "epoch no.3 train no.279210  loss = 2.17010 avg_loss = 3.24793\n",
      "epoch no.3 train no.279220  loss = 3.18005 avg_loss = 3.25085\n",
      "epoch no.3 train no.279230  loss = 2.93707 avg_loss = 3.24055\n",
      "epoch no.3 train no.279240  loss = 3.65074 avg_loss = 3.25699\n",
      "epoch no.3 train no.279250  loss = 2.68336 avg_loss = 3.25206\n",
      "epoch no.3 train no.279260  loss = 4.21393 avg_loss = 3.27303\n",
      "epoch no.3 train no.279270  loss = 3.22377 avg_loss = 3.24908\n",
      "epoch no.3 train no.279280  loss = 2.76198 avg_loss = 3.24032\n",
      "epoch no.3 train no.279290  loss = 2.99623 avg_loss = 3.25031\n",
      "epoch no.3 train no.279300  loss = 4.15172 avg_loss = 3.26130\n",
      "epoch no.3 train no.279310  loss = 4.97758 avg_loss = 3.27102\n",
      "epoch no.3 train no.279320  loss = 3.02582 avg_loss = 3.29861\n",
      "epoch no.3 train no.279330  loss = 2.36071 avg_loss = 3.29262\n",
      "epoch no.3 train no.279340  loss = 2.59745 avg_loss = 3.27405\n",
      "epoch no.3 train no.279350  loss = 2.50974 avg_loss = 3.30065\n",
      "epoch no.3 train no.279360  loss = 2.71225 avg_loss = 3.24515\n",
      "epoch no.3 train no.279370  loss = 2.44376 avg_loss = 3.30688\n",
      "epoch no.3 train no.279380  loss = 4.51890 avg_loss = 3.32874\n",
      "epoch no.3 train no.279390  loss = 4.26194 avg_loss = 3.38492\n",
      "epoch no.3 train no.279400  loss = 2.31590 avg_loss = 3.39647\n",
      "epoch no.3 train no.279410  loss = 3.58038 avg_loss = 3.40509\n",
      "epoch no.3 train no.279420  loss = 4.30770 avg_loss = 3.40621\n",
      "epoch no.3 train no.279430  loss = 3.14206 avg_loss = 3.37984\n",
      "epoch no.3 train no.279440  loss = 2.94951 avg_loss = 3.35333\n",
      "epoch no.3 train no.279450  loss = 2.25970 avg_loss = 3.31609\n",
      "epoch no.3 train no.279460  loss = 2.08560 avg_loss = 3.31653\n",
      "epoch no.3 train no.279470  loss = 4.67905 avg_loss = 3.29925\n",
      "epoch no.3 train no.279480  loss = 3.09559 avg_loss = 3.38617\n",
      "epoch no.3 train no.279490  loss = 3.93340 avg_loss = 3.38903\n",
      "epoch no.3 train no.279500  loss = 3.05923 avg_loss = 3.38297\n",
      "epoch no.3 train no.279510  loss = 3.74455 avg_loss = 3.38646\n",
      "epoch no.3 train no.279520  loss = 4.48850 avg_loss = 3.41208\n",
      "epoch no.3 train no.279530  loss = 3.42755 avg_loss = 3.37825\n",
      "epoch no.3 train no.279540  loss = 4.36028 avg_loss = 3.42882\n",
      "epoch no.3 train no.279550  loss = 2.54496 avg_loss = 3.38571\n",
      "epoch no.3 train no.279560  loss = 4.91880 avg_loss = 3.37033\n",
      "epoch no.3 train no.279570  loss = 1.85163 avg_loss = 3.39756\n",
      "epoch no.3 train no.279580  loss = 3.25718 avg_loss = 3.38291\n",
      "epoch no.3 train no.279590  loss = 3.06292 avg_loss = 3.36667\n",
      "epoch no.3 train no.279600  loss = 4.99337 avg_loss = 3.39850\n",
      "epoch no.3 train no.279610  loss = 3.01294 avg_loss = 3.33866\n",
      "epoch no.3 train no.279620  loss = 4.75015 avg_loss = 3.36180\n",
      "epoch no.3 train no.279630  loss = 4.11114 avg_loss = 3.37870\n",
      "epoch no.3 train no.279640  loss = 5.07258 avg_loss = 3.39744\n",
      "epoch no.3 train no.279650  loss = 3.88328 avg_loss = 3.36687\n",
      "epoch no.3 train no.279660  loss = 4.47322 avg_loss = 3.37789\n",
      "epoch no.3 train no.279670  loss = 3.93399 avg_loss = 3.35288\n",
      "epoch no.3 train no.279680  loss = 3.55532 avg_loss = 3.34517\n",
      "epoch no.3 train no.279690  loss = 5.35822 avg_loss = 3.32975\n",
      "epoch no.3 train no.279700  loss = 3.64937 avg_loss = 3.33401\n",
      "epoch no.3 train no.279710  loss = 4.40593 avg_loss = 3.30563\n",
      "epoch no.3 train no.279720  loss = 4.34656 avg_loss = 3.32466\n",
      "epoch no.3 train no.279730  loss = 4.07100 avg_loss = 3.32966\n",
      "epoch no.3 train no.279740  loss = 3.99284 avg_loss = 3.28872\n",
      "epoch no.3 train no.279750  loss = 3.80712 avg_loss = 3.29374\n",
      "epoch no.3 train no.279760  loss = 3.92793 avg_loss = 3.29829\n",
      "epoch no.3 train no.279770  loss = 2.51686 avg_loss = 3.31033\n",
      "epoch no.3 train no.279780  loss = 2.73809 avg_loss = 3.26332\n",
      "epoch no.3 train no.279790  loss = 2.58281 avg_loss = 3.23451\n",
      "epoch no.3 train no.279800  loss = 3.79249 avg_loss = 3.20850\n",
      "epoch no.3 train no.279810  loss = 3.48347 avg_loss = 3.22120\n",
      "epoch no.3 train no.279820  loss = 3.54216 avg_loss = 3.20914\n",
      "epoch no.3 train no.279830  loss = 2.44175 avg_loss = 3.20024\n",
      "epoch no.3 train no.279840  loss = 6.18380 avg_loss = 3.22910\n",
      "epoch no.3 train no.279850  loss = 3.18836 avg_loss = 3.22321\n",
      "epoch no.3 train no.279860  loss = 3.42623 avg_loss = 3.21987\n",
      "epoch no.3 train no.279870  loss = 2.64085 avg_loss = 3.25182\n",
      "epoch no.3 train no.279880  loss = 2.40409 avg_loss = 3.27386\n",
      "epoch no.3 train no.279890  loss = 2.83185 avg_loss = 3.29014\n",
      "epoch no.3 train no.279900  loss = 2.71578 avg_loss = 3.33274\n",
      "epoch no.3 train no.279910  loss = 3.02538 avg_loss = 3.32867\n",
      "epoch no.3 train no.279920  loss = 5.04965 avg_loss = 3.40898\n",
      "epoch no.3 train no.279930  loss = 3.60732 avg_loss = 3.42800\n",
      "epoch no.3 train no.279940  loss = 4.26481 avg_loss = 3.44179\n",
      "epoch no.3 train no.279950  loss = 5.43313 avg_loss = 3.45859\n",
      "epoch no.3 train no.279960  loss = 2.64587 avg_loss = 3.49343\n",
      "epoch no.3 train no.279970  loss = 1.97491 avg_loss = 3.50843\n",
      "epoch no.3 train no.279980  loss = 2.08113 avg_loss = 3.48223\n",
      "epoch no.3 train no.279990  loss = 2.61212 avg_loss = 3.49255\n",
      "epoch no.3 train no.280000  loss = 4.56040 avg_loss = 3.46878\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.280010  loss = 2.52247 avg_loss = 3.41122\n",
      "epoch no.3 train no.280020  loss = 3.52825 avg_loss = 3.38836\n",
      "epoch no.3 train no.280030  loss = 2.81695 avg_loss = 3.37361\n",
      "epoch no.3 train no.280040  loss = 5.24081 avg_loss = 3.38800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.280050  loss = 2.48208 avg_loss = 3.31824\n",
      "epoch no.3 train no.280060  loss = 4.06525 avg_loss = 3.33196\n",
      "epoch no.3 train no.280070  loss = 6.12695 avg_loss = 3.35311\n",
      "epoch no.3 train no.280080  loss = 3.43422 avg_loss = 3.33982\n",
      "epoch no.3 train no.280090  loss = 1.85811 avg_loss = 3.35422\n",
      "epoch no.3 train no.280100  loss = 4.03229 avg_loss = 3.33512\n",
      "epoch no.3 train no.280110  loss = 4.85458 avg_loss = 3.38434\n",
      "epoch no.3 train no.280120  loss = 2.39238 avg_loss = 3.38618\n",
      "epoch no.3 train no.280130  loss = 3.28493 avg_loss = 3.36598\n",
      "epoch no.3 train no.280140  loss = 4.16004 avg_loss = 3.42272\n",
      "epoch no.3 train no.280150  loss = 1.78642 avg_loss = 3.38319\n",
      "epoch no.3 train no.280160  loss = 3.48381 avg_loss = 3.36992\n",
      "epoch no.3 train no.280170  loss = 2.25972 avg_loss = 3.33263\n",
      "epoch no.3 train no.280180  loss = 7.20517 avg_loss = 3.34566\n",
      "epoch no.3 train no.280190  loss = 3.52066 avg_loss = 3.32368\n",
      "epoch no.3 train no.280200  loss = 3.32501 avg_loss = 3.30678\n",
      "epoch no.3 train no.280210  loss = 2.42574 avg_loss = 3.29230\n",
      "epoch no.3 train no.280220  loss = 3.58926 avg_loss = 3.25119\n",
      "epoch no.3 train no.280230  loss = 2.58356 avg_loss = 3.20293\n",
      "epoch no.3 train no.280240  loss = 2.45100 avg_loss = 3.18234\n",
      "epoch no.3 train no.280250  loss = 1.94348 avg_loss = 3.23536\n",
      "epoch no.3 train no.280260  loss = 4.07589 avg_loss = 3.30504\n",
      "epoch no.3 train no.280270  loss = 4.32563 avg_loss = 3.31988\n",
      "epoch no.3 train no.280280  loss = 2.95163 avg_loss = 3.32504\n",
      "epoch no.3 train no.280290  loss = 3.46004 avg_loss = 3.30381\n",
      "epoch no.3 train no.280300  loss = 6.16501 avg_loss = 3.33549\n",
      "epoch no.3 train no.280310  loss = 2.55193 avg_loss = 3.30473\n",
      "epoch no.3 train no.280320  loss = 2.21617 avg_loss = 3.25341\n",
      "epoch no.3 train no.280330  loss = 3.56152 avg_loss = 3.23117\n",
      "epoch no.3 train no.280340  loss = 3.15415 avg_loss = 3.20789\n",
      "epoch no.3 train no.280350  loss = 2.24801 avg_loss = 3.24112\n",
      "epoch no.3 train no.280360  loss = 2.66164 avg_loss = 3.24216\n",
      "epoch no.3 train no.280370  loss = 4.62105 avg_loss = 3.29767\n",
      "epoch no.3 train no.280380  loss = 2.83909 avg_loss = 3.27982\n",
      "epoch no.3 train no.280390  loss = 6.37461 avg_loss = 3.32327\n",
      "epoch no.3 train no.280400  loss = 2.69659 avg_loss = 3.31678\n",
      "epoch no.3 train no.280410  loss = 3.52980 avg_loss = 3.32644\n",
      "epoch no.3 train no.280420  loss = 2.54500 avg_loss = 3.36638\n",
      "epoch no.3 train no.280430  loss = 2.85328 avg_loss = 3.32918\n",
      "epoch no.3 train no.280440  loss = 5.40902 avg_loss = 3.39591\n",
      "epoch no.3 train no.280450  loss = 2.21386 avg_loss = 3.39584\n",
      "epoch no.3 train no.280460  loss = 2.68568 avg_loss = 3.32921\n",
      "epoch no.3 train no.280470  loss = 2.85200 avg_loss = 3.32017\n",
      "epoch no.3 train no.280480  loss = 3.16516 avg_loss = 3.31970\n",
      "epoch no.3 train no.280490  loss = 2.42763 avg_loss = 3.30890\n",
      "epoch no.3 train no.280500  loss = 2.83209 avg_loss = 3.27714\n",
      "epoch no.3 train no.280510  loss = 2.79131 avg_loss = 3.24781\n",
      "epoch no.3 train no.280520  loss = 2.93270 avg_loss = 3.21443\n",
      "epoch no.3 train no.280530  loss = 2.77377 avg_loss = 3.23378\n",
      "epoch no.3 train no.280540  loss = 4.90621 avg_loss = 3.23523\n",
      "epoch no.3 train no.280550  loss = 6.19463 avg_loss = 3.25261\n",
      "epoch no.3 train no.280560  loss = 5.04122 avg_loss = 3.27305\n",
      "epoch no.3 train no.280570  loss = 3.56276 avg_loss = 3.30990\n",
      "epoch no.3 train no.280580  loss = 3.47292 avg_loss = 3.30912\n",
      "epoch no.3 train no.280590  loss = 2.28384 avg_loss = 3.30724\n",
      "epoch no.3 train no.280600  loss = 2.85586 avg_loss = 3.31514\n",
      "epoch no.3 train no.280610  loss = 3.77167 avg_loss = 3.33084\n",
      "epoch no.3 train no.280620  loss = 4.91554 avg_loss = 3.30482\n",
      "epoch no.3 train no.280630  loss = 4.54468 avg_loss = 3.32963\n",
      "epoch no.3 train no.280640  loss = 2.90673 avg_loss = 3.31186\n",
      "epoch no.3 train no.280650  loss = 3.35605 avg_loss = 3.37045\n",
      "epoch no.3 train no.280660  loss = 2.93699 avg_loss = 3.35107\n",
      "epoch no.3 train no.280670  loss = 3.40632 avg_loss = 3.40521\n",
      "epoch no.3 train no.280680  loss = 2.22127 avg_loss = 3.42154\n",
      "epoch no.3 train no.280690  loss = 1.76302 avg_loss = 3.39744\n",
      "epoch no.3 train no.280700  loss = 2.13098 avg_loss = 3.35241\n",
      "epoch no.3 train no.280710  loss = 2.59957 avg_loss = 3.36351\n",
      "epoch no.3 train no.280720  loss = 4.98268 avg_loss = 3.40319\n",
      "epoch no.3 train no.280730  loss = 3.19958 avg_loss = 3.37986\n",
      "epoch no.3 train no.280740  loss = 3.25714 avg_loss = 3.35870\n",
      "epoch no.3 train no.280750  loss = 2.51756 avg_loss = 3.32790\n",
      "epoch no.3 train no.280760  loss = 1.76421 avg_loss = 3.33887\n",
      "epoch no.3 train no.280770  loss = 3.26745 avg_loss = 3.30664\n",
      "epoch no.3 train no.280780  loss = 3.39796 avg_loss = 3.30694\n",
      "epoch no.3 train no.280790  loss = 4.72158 avg_loss = 3.35972\n",
      "epoch no.3 train no.280800  loss = 2.68916 avg_loss = 3.31697\n",
      "epoch no.3 train no.280810  loss = 2.06810 avg_loss = 3.29594\n",
      "epoch no.3 train no.280820  loss = 5.52837 avg_loss = 3.31435\n",
      "epoch no.3 train no.280830  loss = 2.79589 avg_loss = 3.29477\n",
      "epoch no.3 train no.280840  loss = 2.51485 avg_loss = 3.29276\n",
      "epoch no.3 train no.280850  loss = 2.15316 avg_loss = 3.25173\n",
      "epoch no.3 train no.280860  loss = 4.94210 avg_loss = 3.28510\n",
      "epoch no.3 train no.280870  loss = 2.83650 avg_loss = 3.23539\n",
      "epoch no.3 train no.280880  loss = 3.55346 avg_loss = 3.24919\n",
      "epoch no.3 train no.280890  loss = 3.40516 avg_loss = 3.26890\n",
      "epoch no.3 train no.280900  loss = 3.70335 avg_loss = 3.29469\n",
      "epoch no.3 train no.280910  loss = 2.34748 avg_loss = 3.29268\n",
      "epoch no.3 train no.280920  loss = 3.01722 avg_loss = 3.33225\n",
      "epoch no.3 train no.280930  loss = 2.93368 avg_loss = 3.36806\n",
      "epoch no.3 train no.280940  loss = 5.11565 avg_loss = 3.43531\n",
      "epoch no.3 train no.280950  loss = 3.98330 avg_loss = 3.48069\n",
      "epoch no.3 train no.280960  loss = 4.28692 avg_loss = 3.46068\n",
      "epoch no.3 train no.280970  loss = 4.66735 avg_loss = 3.52180\n",
      "epoch no.3 train no.280980  loss = 4.16234 avg_loss = 3.49953\n",
      "epoch no.3 train no.280990  loss = 3.82986 avg_loss = 3.49591\n",
      "epoch no.3 train no.281000  loss = 4.01253 avg_loss = 3.50505\n",
      "5\n",
      "to_tokens: ['▁비', '▁명', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.281010  loss = 3.00815 avg_loss = 3.42314\n",
      "epoch no.3 train no.281020  loss = 1.94916 avg_loss = 3.39860\n",
      "epoch no.3 train no.281030  loss = 3.18086 avg_loss = 3.40863\n",
      "epoch no.3 train no.281040  loss = 3.76443 avg_loss = 3.44906\n",
      "epoch no.3 train no.281050  loss = 4.85945 avg_loss = 3.47358\n",
      "epoch no.3 train no.281060  loss = 3.47476 avg_loss = 3.43914\n",
      "epoch no.3 train no.281070  loss = 3.38504 avg_loss = 3.43369\n",
      "epoch no.3 train no.281080  loss = 3.53371 avg_loss = 3.44194\n",
      "epoch no.3 train no.281090  loss = 5.14161 avg_loss = 3.41861\n",
      "epoch no.3 train no.281100  loss = 2.64924 avg_loss = 3.36324\n",
      "epoch no.3 train no.281110  loss = 4.10004 avg_loss = 3.34418\n",
      "epoch no.3 train no.281120  loss = 5.31644 avg_loss = 3.34282\n",
      "epoch no.3 train no.281130  loss = 3.96363 avg_loss = 3.32519\n",
      "epoch no.3 train no.281140  loss = 3.80158 avg_loss = 3.33544\n",
      "epoch no.3 train no.281150  loss = 3.10683 avg_loss = 3.36997\n",
      "epoch no.3 train no.281160  loss = 2.66564 avg_loss = 3.40577\n",
      "epoch no.3 train no.281170  loss = 3.08466 avg_loss = 3.40284\n",
      "epoch no.3 train no.281180  loss = 2.06920 avg_loss = 3.40219\n",
      "epoch no.3 train no.281190  loss = 2.67932 avg_loss = 3.45861\n",
      "epoch no.3 train no.281200  loss = 4.87100 avg_loss = 3.44284\n",
      "epoch no.3 train no.281210  loss = 3.86571 avg_loss = 3.46577\n",
      "epoch no.3 train no.281220  loss = 4.61255 avg_loss = 3.48023\n",
      "epoch no.3 train no.281230  loss = 1.70512 avg_loss = 3.41555\n",
      "epoch no.3 train no.281240  loss = 2.45439 avg_loss = 3.43074\n",
      "epoch no.3 train no.281250  loss = 2.24985 avg_loss = 3.38669\n",
      "epoch no.3 train no.281260  loss = 4.68120 avg_loss = 3.40735\n",
      "epoch no.3 train no.281270  loss = 4.67638 avg_loss = 3.43954\n",
      "epoch no.3 train no.281280  loss = 3.82337 avg_loss = 3.45198\n",
      "epoch no.3 train no.281290  loss = 3.37810 avg_loss = 3.49185\n",
      "epoch no.3 train no.281300  loss = 3.25945 avg_loss = 3.49735\n",
      "epoch no.3 train no.281310  loss = 4.68034 avg_loss = 3.52207\n",
      "epoch no.3 train no.281320  loss = 5.56442 avg_loss = 3.53521\n",
      "epoch no.3 train no.281330  loss = 2.27350 avg_loss = 3.51711\n",
      "epoch no.3 train no.281340  loss = 3.62091 avg_loss = 3.54311\n",
      "epoch no.3 train no.281350  loss = 3.20372 avg_loss = 3.50222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.281360  loss = 3.32470 avg_loss = 3.48103\n",
      "epoch no.3 train no.281370  loss = 4.36675 avg_loss = 3.44635\n",
      "epoch no.3 train no.281380  loss = 5.41202 avg_loss = 3.45155\n",
      "epoch no.3 train no.281390  loss = 3.11992 avg_loss = 3.45061\n",
      "epoch no.3 train no.281400  loss = 3.62448 avg_loss = 3.40281\n",
      "epoch no.3 train no.281410  loss = 3.29990 avg_loss = 3.38974\n",
      "epoch no.3 train no.281420  loss = 2.88161 avg_loss = 3.36267\n",
      "epoch no.3 train no.281430  loss = 4.28401 avg_loss = 3.36652\n",
      "epoch no.3 train no.281440  loss = 3.71246 avg_loss = 3.39343\n",
      "epoch no.3 train no.281450  loss = 2.12412 avg_loss = 3.39618\n",
      "epoch no.3 train no.281460  loss = 5.04234 avg_loss = 3.40212\n",
      "epoch no.3 train no.281470  loss = 2.93388 avg_loss = 3.36828\n",
      "epoch no.3 train no.281480  loss = 4.18429 avg_loss = 3.39120\n",
      "epoch no.3 train no.281490  loss = 2.88929 avg_loss = 3.37320\n",
      "epoch no.3 train no.281500  loss = 3.05799 avg_loss = 3.41554\n",
      "epoch no.3 train no.281510  loss = 3.03722 avg_loss = 3.38164\n",
      "epoch no.3 train no.281520  loss = 2.27503 avg_loss = 3.35912\n",
      "epoch no.3 train no.281530  loss = 5.08151 avg_loss = 3.36784\n",
      "epoch no.3 train no.281540  loss = 3.40797 avg_loss = 3.36802\n",
      "epoch no.3 train no.281550  loss = 4.58173 avg_loss = 3.41686\n",
      "epoch no.3 train no.281560  loss = 3.03127 avg_loss = 3.42834\n",
      "epoch no.3 train no.281570  loss = 4.18866 avg_loss = 3.43591\n",
      "epoch no.3 train no.281580  loss = 2.67461 avg_loss = 3.36167\n",
      "epoch no.3 train no.281590  loss = 2.54884 avg_loss = 3.31758\n",
      "epoch no.3 train no.281600  loss = 3.94252 avg_loss = 3.33001\n",
      "epoch no.3 train no.281610  loss = 2.18098 avg_loss = 3.34994\n",
      "epoch no.3 train no.281620  loss = 3.48566 avg_loss = 3.38981\n",
      "epoch no.3 train no.281630  loss = 2.66766 avg_loss = 3.34060\n",
      "epoch no.3 train no.281640  loss = 3.22371 avg_loss = 3.30406\n",
      "epoch no.3 train no.281650  loss = 2.87320 avg_loss = 3.29241\n",
      "epoch no.3 train no.281660  loss = 4.99794 avg_loss = 3.28429\n",
      "epoch no.3 train no.281670  loss = 2.06095 avg_loss = 3.26448\n",
      "epoch no.3 train no.281680  loss = 3.62342 avg_loss = 3.22499\n",
      "epoch no.3 train no.281690  loss = 4.01272 avg_loss = 3.19755\n",
      "epoch no.3 train no.281700  loss = 2.49264 avg_loss = 3.19283\n",
      "epoch no.3 train no.281710  loss = 3.43715 avg_loss = 3.18044\n",
      "epoch no.3 train no.281720  loss = 3.16105 avg_loss = 3.17236\n",
      "epoch no.3 train no.281730  loss = 3.50779 avg_loss = 3.16539\n",
      "epoch no.3 train no.281740  loss = 2.07747 avg_loss = 3.18424\n",
      "epoch no.3 train no.281750  loss = 2.44191 avg_loss = 3.21655\n",
      "epoch no.3 train no.281760  loss = 2.88497 avg_loss = 3.25083\n",
      "epoch no.3 train no.281770  loss = 4.87681 avg_loss = 3.27453\n",
      "epoch no.3 train no.281780  loss = 2.92167 avg_loss = 3.26140\n",
      "epoch no.3 train no.281790  loss = 2.77645 avg_loss = 3.22611\n",
      "epoch no.3 train no.281800  loss = 3.05235 avg_loss = 3.23952\n",
      "epoch no.3 train no.281810  loss = 3.28225 avg_loss = 3.24022\n",
      "epoch no.3 train no.281820  loss = 3.91230 avg_loss = 3.28689\n",
      "epoch no.3 train no.281830  loss = 3.87373 avg_loss = 3.31354\n",
      "epoch no.3 train no.281840  loss = 3.53534 avg_loss = 3.34130\n",
      "epoch no.3 train no.281850  loss = 4.71891 avg_loss = 3.39876\n",
      "epoch no.3 train no.281860  loss = 3.19695 avg_loss = 3.40034\n",
      "epoch no.3 train no.281870  loss = 3.63499 avg_loss = 3.35815\n",
      "epoch no.3 train no.281880  loss = 3.82372 avg_loss = 3.34944\n",
      "epoch no.3 train no.281890  loss = 3.30343 avg_loss = 3.36258\n",
      "epoch no.3 train no.281900  loss = 3.57396 avg_loss = 3.38712\n",
      "epoch no.3 train no.281910  loss = 3.44611 avg_loss = 3.39994\n",
      "epoch no.3 train no.281920  loss = 3.15558 avg_loss = 3.40156\n",
      "epoch no.3 train no.281930  loss = 4.32108 avg_loss = 3.42811\n",
      "epoch no.3 train no.281940  loss = 5.66033 avg_loss = 3.45769\n",
      "epoch no.3 train no.281950  loss = 2.42903 avg_loss = 3.40447\n",
      "epoch no.3 train no.281960  loss = 4.98203 avg_loss = 3.43743\n",
      "epoch no.3 train no.281970  loss = 3.40296 avg_loss = 3.39127\n",
      "epoch no.3 train no.281980  loss = 5.89360 avg_loss = 3.36604\n",
      "epoch no.3 train no.281990  loss = 2.35710 avg_loss = 3.38053\n",
      "epoch no.3 train no.282000  loss = 3.03369 avg_loss = 3.37422\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 감성 발라드</s>\n",
      "epoch no.3 train no.282010  loss = 3.85096 avg_loss = 3.35252\n",
      "epoch no.3 train no.282020  loss = 3.28800 avg_loss = 3.33659\n",
      "epoch no.3 train no.282030  loss = 2.39068 avg_loss = 3.33320\n",
      "epoch no.3 train no.282040  loss = 2.68699 avg_loss = 3.34060\n",
      "epoch no.3 train no.282050  loss = 4.72587 avg_loss = 3.36868\n",
      "epoch no.3 train no.282060  loss = 3.53000 avg_loss = 3.38768\n",
      "epoch no.3 train no.282070  loss = 3.33327 avg_loss = 3.40273\n",
      "epoch no.3 train no.282080  loss = 2.72851 avg_loss = 3.42847\n",
      "epoch no.3 train no.282090  loss = 2.92087 avg_loss = 3.45901\n",
      "epoch no.3 train no.282100  loss = 4.95306 avg_loss = 3.38886\n",
      "epoch no.3 train no.282110  loss = 1.16544 avg_loss = 3.38149\n",
      "epoch no.3 train no.282120  loss = 3.67109 avg_loss = 3.32790\n",
      "epoch no.3 train no.282130  loss = 2.54327 avg_loss = 3.28170\n",
      "epoch no.3 train no.282140  loss = 2.53788 avg_loss = 3.30254\n",
      "epoch no.3 train no.282150  loss = 2.74391 avg_loss = 3.29868\n",
      "epoch no.3 train no.282160  loss = 2.95301 avg_loss = 3.26987\n",
      "epoch no.3 train no.282170  loss = 2.55191 avg_loss = 3.29763\n",
      "epoch no.3 train no.282180  loss = 2.59999 avg_loss = 3.26739\n",
      "epoch no.3 train no.282190  loss = 2.68170 avg_loss = 3.26512\n",
      "epoch no.3 train no.282200  loss = 3.45012 avg_loss = 3.22462\n",
      "epoch no.3 train no.282210  loss = 4.18255 avg_loss = 3.23863\n",
      "epoch no.3 train no.282220  loss = 3.97090 avg_loss = 3.23135\n",
      "epoch no.3 train no.282230  loss = 3.51212 avg_loss = 3.23595\n",
      "epoch no.3 train no.282240  loss = 5.36884 avg_loss = 3.23664\n",
      "epoch no.3 train no.282250  loss = 4.01502 avg_loss = 3.18702\n",
      "epoch no.3 train no.282260  loss = 5.35738 avg_loss = 3.23446\n",
      "epoch no.3 train no.282270  loss = 4.74989 avg_loss = 3.23825\n",
      "epoch no.3 train no.282280  loss = 4.45351 avg_loss = 3.24808\n",
      "epoch no.3 train no.282290  loss = 3.95543 avg_loss = 3.26650\n",
      "epoch no.3 train no.282300  loss = 3.32980 avg_loss = 3.27375\n",
      "epoch no.3 train no.282310  loss = 3.19612 avg_loss = 3.23533\n",
      "epoch no.3 train no.282320  loss = 2.16680 avg_loss = 3.25774\n",
      "epoch no.3 train no.282330  loss = 2.94680 avg_loss = 3.21919\n",
      "epoch no.3 train no.282340  loss = 2.38608 avg_loss = 3.21060\n",
      "epoch no.3 train no.282350  loss = 4.26087 avg_loss = 3.19812\n",
      "epoch no.3 train no.282360  loss = 1.99857 avg_loss = 3.17189\n",
      "epoch no.3 train no.282370  loss = 2.88449 avg_loss = 3.18310\n",
      "epoch no.3 train no.282380  loss = 1.95107 avg_loss = 3.22758\n",
      "epoch no.3 train no.282390  loss = 3.91707 avg_loss = 3.24816\n",
      "epoch no.3 train no.282400  loss = 2.62731 avg_loss = 3.22171\n",
      "epoch no.3 train no.282410  loss = 3.84216 avg_loss = 3.25653\n",
      "epoch no.3 train no.282420  loss = 2.77567 avg_loss = 3.25744\n",
      "epoch no.3 train no.282430  loss = 3.56570 avg_loss = 3.29700\n",
      "epoch no.3 train no.282440  loss = 2.32276 avg_loss = 3.23531\n",
      "epoch no.3 train no.282450  loss = 2.70326 avg_loss = 3.24917\n",
      "epoch no.3 train no.282460  loss = 2.84714 avg_loss = 3.25410\n",
      "epoch no.3 train no.282470  loss = 3.31795 avg_loss = 3.28262\n",
      "epoch no.3 train no.282480  loss = 3.48814 avg_loss = 3.29406\n",
      "epoch no.3 train no.282490  loss = 2.82471 avg_loss = 3.37826\n",
      "epoch no.3 train no.282500  loss = 1.75200 avg_loss = 3.32458\n",
      "epoch no.3 train no.282510  loss = 5.12862 avg_loss = 3.37107\n",
      "epoch no.3 train no.282520  loss = 3.61783 avg_loss = 3.36443\n",
      "epoch no.3 train no.282530  loss = 2.56851 avg_loss = 3.32046\n",
      "epoch no.3 train no.282540  loss = 3.24163 avg_loss = 3.29961\n",
      "epoch no.3 train no.282550  loss = 4.62115 avg_loss = 3.38518\n",
      "epoch no.3 train no.282560  loss = 4.06612 avg_loss = 3.38298\n",
      "epoch no.3 train no.282570  loss = 2.49884 avg_loss = 3.35327\n",
      "epoch no.3 train no.282580  loss = 2.37487 avg_loss = 3.35304\n",
      "epoch no.3 train no.282590  loss = 3.31085 avg_loss = 3.31868\n",
      "epoch no.3 train no.282600  loss = 5.84134 avg_loss = 3.36382\n",
      "epoch no.3 train no.282610  loss = 4.00749 avg_loss = 3.40677\n",
      "epoch no.3 train no.282620  loss = 2.53583 avg_loss = 3.43561\n",
      "epoch no.3 train no.282630  loss = 3.60187 avg_loss = 3.43835\n",
      "epoch no.3 train no.282640  loss = 4.54090 avg_loss = 3.49162\n",
      "epoch no.3 train no.282650  loss = 2.86147 avg_loss = 3.48903\n",
      "epoch no.3 train no.282660  loss = 5.07261 avg_loss = 3.47733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.282670  loss = 3.96027 avg_loss = 3.48857\n",
      "epoch no.3 train no.282680  loss = 3.14006 avg_loss = 3.45555\n",
      "epoch no.3 train no.282690  loss = 4.01365 avg_loss = 3.48362\n",
      "epoch no.3 train no.282700  loss = 2.54402 avg_loss = 3.47590\n",
      "epoch no.3 train no.282710  loss = 2.81527 avg_loss = 3.48512\n",
      "epoch no.3 train no.282720  loss = 4.15589 avg_loss = 3.52571\n",
      "epoch no.3 train no.282730  loss = 2.59794 avg_loss = 3.50539\n",
      "epoch no.3 train no.282740  loss = 3.32410 avg_loss = 3.46754\n",
      "epoch no.3 train no.282750  loss = 5.09442 avg_loss = 3.48490\n",
      "epoch no.3 train no.282760  loss = 1.97464 avg_loss = 3.46756\n",
      "epoch no.3 train no.282770  loss = 4.06425 avg_loss = 3.48775\n",
      "epoch no.3 train no.282780  loss = 2.56128 avg_loss = 3.49795\n",
      "epoch no.3 train no.282790  loss = 3.06380 avg_loss = 3.47963\n",
      "epoch no.3 train no.282800  loss = 5.14182 avg_loss = 3.52853\n",
      "epoch no.3 train no.282810  loss = 3.60700 avg_loss = 3.55912\n",
      "epoch no.3 train no.282820  loss = 4.66349 avg_loss = 3.59229\n",
      "epoch no.3 train no.282830  loss = 2.45858 avg_loss = 3.55123\n",
      "epoch no.3 train no.282840  loss = 3.29761 avg_loss = 3.59405\n",
      "epoch no.3 train no.282850  loss = 5.02774 avg_loss = 3.56010\n",
      "epoch no.3 train no.282860  loss = 3.62997 avg_loss = 3.50198\n",
      "epoch no.3 train no.282870  loss = 4.66283 avg_loss = 3.49254\n",
      "epoch no.3 train no.282880  loss = 3.17878 avg_loss = 3.48859\n",
      "epoch no.3 train no.282890  loss = 3.54907 avg_loss = 3.46218\n",
      "epoch no.3 train no.282900  loss = 3.59500 avg_loss = 3.45937\n",
      "epoch no.3 train no.282910  loss = 2.45272 avg_loss = 3.43230\n",
      "epoch no.3 train no.282920  loss = 3.53745 avg_loss = 3.40967\n",
      "epoch no.3 train no.282930  loss = 2.40524 avg_loss = 3.42324\n",
      "epoch no.3 train no.282940  loss = 2.51896 avg_loss = 3.39555\n",
      "epoch no.3 train no.282950  loss = 3.25552 avg_loss = 3.45566\n",
      "epoch no.3 train no.282960  loss = 2.91300 avg_loss = 3.44289\n",
      "epoch no.3 train no.282970  loss = 4.34007 avg_loss = 3.47978\n",
      "epoch no.3 train no.282980  loss = 2.80801 avg_loss = 3.41243\n",
      "epoch no.3 train no.282990  loss = 3.59928 avg_loss = 3.38351\n",
      "epoch no.3 train no.283000  loss = 3.14532 avg_loss = 3.34469\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '▁시절', '▁그', '드', '</s>']\n",
      "추억의  그 시절 발라드</s>\n",
      "epoch no.3 train no.283010  loss = 2.22277 avg_loss = 3.33119\n",
      "epoch no.3 train no.283020  loss = 2.80189 avg_loss = 3.29197\n",
      "epoch no.3 train no.283030  loss = 2.17932 avg_loss = 3.27087\n",
      "epoch no.3 train no.283040  loss = 4.81405 avg_loss = 3.35026\n",
      "epoch no.3 train no.283050  loss = 2.07693 avg_loss = 3.29764\n",
      "epoch no.3 train no.283060  loss = 7.51776 avg_loss = 3.35858\n",
      "epoch no.3 train no.283070  loss = 5.48560 avg_loss = 3.42158\n",
      "epoch no.3 train no.283080  loss = 3.97287 avg_loss = 3.40480\n",
      "epoch no.3 train no.283090  loss = 3.07257 avg_loss = 3.36933\n",
      "epoch no.3 train no.283100  loss = 2.42258 avg_loss = 3.33065\n",
      "epoch no.3 train no.283110  loss = 4.97142 avg_loss = 3.36180\n",
      "epoch no.3 train no.283120  loss = 2.59092 avg_loss = 3.31840\n",
      "epoch no.3 train no.283130  loss = 2.65785 avg_loss = 3.30942\n",
      "epoch no.3 train no.283140  loss = 4.52321 avg_loss = 3.31591\n",
      "epoch no.3 train no.283150  loss = 3.01399 avg_loss = 3.27753\n",
      "epoch no.3 train no.283160  loss = 3.48816 avg_loss = 3.25122\n",
      "epoch no.3 train no.283170  loss = 2.85430 avg_loss = 3.25610\n",
      "epoch no.3 train no.283180  loss = 4.19650 avg_loss = 3.24820\n",
      "epoch no.3 train no.283190  loss = 3.86918 avg_loss = 3.23882\n",
      "epoch no.3 train no.283200  loss = 3.93455 avg_loss = 3.28488\n",
      "epoch no.3 train no.283210  loss = 2.14667 avg_loss = 3.29734\n",
      "epoch no.3 train no.283220  loss = 4.29657 avg_loss = 3.28028\n",
      "epoch no.3 train no.283230  loss = 3.48968 avg_loss = 3.27513\n",
      "epoch no.3 train no.283240  loss = 3.82340 avg_loss = 3.33237\n",
      "epoch no.3 train no.283250  loss = 2.12033 avg_loss = 3.35490\n",
      "epoch no.3 train no.283260  loss = 3.33541 avg_loss = 3.36157\n",
      "epoch no.3 train no.283270  loss = 3.97370 avg_loss = 3.42620\n",
      "epoch no.3 train no.283280  loss = 2.70859 avg_loss = 3.41252\n",
      "epoch no.3 train no.283290  loss = 1.92351 avg_loss = 3.35693\n",
      "epoch no.3 train no.283300  loss = 4.45231 avg_loss = 3.35372\n",
      "epoch no.3 train no.283310  loss = 3.06861 avg_loss = 3.32714\n",
      "epoch no.3 train no.283320  loss = 2.24117 avg_loss = 3.33616\n",
      "epoch no.3 train no.283330  loss = 3.73753 avg_loss = 3.33298\n",
      "epoch no.3 train no.283340  loss = 5.06312 avg_loss = 3.31803\n",
      "epoch no.3 train no.283350  loss = 3.53041 avg_loss = 3.35113\n",
      "epoch no.3 train no.283360  loss = 4.12933 avg_loss = 3.40354\n",
      "epoch no.3 train no.283370  loss = 5.59974 avg_loss = 3.42406\n",
      "epoch no.3 train no.283380  loss = 3.12530 avg_loss = 3.47317\n",
      "epoch no.3 train no.283390  loss = 1.73524 avg_loss = 3.46080\n",
      "epoch no.3 train no.283400  loss = 4.15026 avg_loss = 3.46469\n",
      "epoch no.3 train no.283410  loss = 4.83088 avg_loss = 3.47009\n",
      "epoch no.3 train no.283420  loss = 2.38419 avg_loss = 3.46209\n",
      "epoch no.3 train no.283430  loss = 3.73583 avg_loss = 3.42480\n",
      "epoch no.3 train no.283440  loss = 3.62528 avg_loss = 3.39169\n",
      "epoch no.3 train no.283450  loss = 3.44809 avg_loss = 3.35359\n",
      "epoch no.3 train no.283460  loss = 3.53500 avg_loss = 3.35510\n",
      "epoch no.3 train no.283470  loss = 2.54093 avg_loss = 3.32106\n",
      "epoch no.3 train no.283480  loss = 4.33298 avg_loss = 3.31588\n",
      "epoch no.3 train no.283490  loss = 3.48580 avg_loss = 3.35759\n",
      "epoch no.3 train no.283500  loss = 2.81408 avg_loss = 3.33164\n",
      "epoch no.3 train no.283510  loss = 6.22307 avg_loss = 3.38537\n",
      "epoch no.3 train no.283520  loss = 2.90419 avg_loss = 3.38110\n",
      "epoch no.3 train no.283530  loss = 4.11405 avg_loss = 3.36680\n",
      "epoch no.3 train no.283540  loss = 5.79792 avg_loss = 3.37075\n",
      "epoch no.3 train no.283550  loss = 2.72847 avg_loss = 3.31844\n",
      "epoch no.3 train no.283560  loss = 2.69413 avg_loss = 3.31914\n",
      "epoch no.3 train no.283570  loss = 5.82902 avg_loss = 3.34855\n",
      "epoch no.3 train no.283580  loss = 3.09922 avg_loss = 3.35650\n",
      "epoch no.3 train no.283590  loss = 3.59205 avg_loss = 3.35147\n",
      "epoch no.3 train no.283600  loss = 3.40441 avg_loss = 3.33975\n",
      "epoch no.3 train no.283610  loss = 3.73962 avg_loss = 3.37318\n",
      "epoch no.3 train no.283620  loss = 1.81799 avg_loss = 3.34701\n",
      "epoch no.3 train no.283630  loss = 1.94186 avg_loss = 3.31111\n",
      "epoch no.3 train no.283640  loss = 2.42091 avg_loss = 3.33587\n",
      "epoch no.3 train no.283650  loss = 3.00661 avg_loss = 3.31962\n",
      "epoch no.3 train no.283660  loss = 3.00497 avg_loss = 3.32405\n",
      "epoch no.3 train no.283670  loss = 4.68607 avg_loss = 3.36150\n",
      "epoch no.3 train no.283680  loss = 5.16690 avg_loss = 3.42185\n",
      "epoch no.3 train no.283690  loss = 3.57510 avg_loss = 3.39753\n",
      "epoch no.3 train no.283700  loss = 4.56226 avg_loss = 3.36454\n",
      "epoch no.3 train no.283710  loss = 3.22044 avg_loss = 3.37048\n",
      "epoch no.3 train no.283720  loss = 3.74238 avg_loss = 3.35236\n",
      "epoch no.3 train no.283730  loss = 2.56503 avg_loss = 3.32876\n",
      "epoch no.3 train no.283740  loss = 4.59415 avg_loss = 3.30924\n",
      "epoch no.3 train no.283750  loss = 4.60796 avg_loss = 3.27294\n",
      "epoch no.3 train no.283760  loss = 3.27854 avg_loss = 3.26034\n",
      "epoch no.3 train no.283770  loss = 2.40678 avg_loss = 3.30143\n",
      "epoch no.3 train no.283780  loss = 2.71373 avg_loss = 3.28425\n",
      "epoch no.3 train no.283790  loss = 3.34533 avg_loss = 3.30812\n",
      "epoch no.3 train no.283800  loss = 3.56294 avg_loss = 3.31960\n",
      "epoch no.3 train no.283810  loss = 2.31284 avg_loss = 3.28650\n",
      "epoch no.3 train no.283820  loss = 2.19264 avg_loss = 3.31496\n",
      "epoch no.3 train no.283830  loss = 2.59399 avg_loss = 3.28312\n",
      "epoch no.3 train no.283840  loss = 1.95205 avg_loss = 3.25439\n",
      "epoch no.3 train no.283850  loss = 3.25033 avg_loss = 3.31276\n",
      "epoch no.3 train no.283860  loss = 5.64099 avg_loss = 3.30114\n",
      "epoch no.3 train no.283870  loss = 2.60973 avg_loss = 3.32829\n",
      "epoch no.3 train no.283880  loss = 2.56683 avg_loss = 3.31817\n",
      "epoch no.3 train no.283890  loss = 4.88435 avg_loss = 3.34474\n",
      "epoch no.3 train no.283900  loss = 3.63114 avg_loss = 3.33736\n",
      "epoch no.3 train no.283910  loss = 2.80476 avg_loss = 3.33447\n",
      "epoch no.3 train no.283920  loss = 2.67053 avg_loss = 3.30825\n",
      "epoch no.3 train no.283930  loss = 2.44162 avg_loss = 3.24259\n",
      "epoch no.3 train no.283940  loss = 2.67767 avg_loss = 3.24633\n",
      "epoch no.3 train no.283950  loss = 2.81403 avg_loss = 3.23389\n",
      "epoch no.3 train no.283960  loss = 2.32169 avg_loss = 3.21514\n",
      "epoch no.3 train no.283970  loss = 2.13177 avg_loss = 3.20318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.283980  loss = 2.16533 avg_loss = 3.18705\n",
      "epoch no.3 train no.283990  loss = 3.61259 avg_loss = 3.20621\n",
      "epoch no.3 train no.284000  loss = 1.97614 avg_loss = 3.20429\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.3 train no.284010  loss = 3.88502 avg_loss = 3.22045\n",
      "epoch no.3 train no.284020  loss = 2.74861 avg_loss = 3.20512\n",
      "epoch no.3 train no.284030  loss = 2.55677 avg_loss = 3.16561\n",
      "epoch no.3 train no.284040  loss = 3.27643 avg_loss = 3.22470\n",
      "epoch no.3 train no.284050  loss = 1.63451 avg_loss = 3.22329\n",
      "epoch no.3 train no.284060  loss = 3.61755 avg_loss = 3.24534\n",
      "epoch no.3 train no.284070  loss = 2.70314 avg_loss = 3.27191\n",
      "epoch no.3 train no.284080  loss = 3.81987 avg_loss = 3.27052\n",
      "epoch no.3 train no.284090  loss = 2.96097 avg_loss = 3.27772\n",
      "epoch no.3 train no.284100  loss = 5.43954 avg_loss = 3.29299\n",
      "epoch no.3 train no.284110  loss = 2.40758 avg_loss = 3.24701\n",
      "epoch no.3 train no.284120  loss = 2.27036 avg_loss = 3.27051\n",
      "epoch no.3 train no.284130  loss = 3.69601 avg_loss = 3.34354\n",
      "epoch no.3 train no.284140  loss = 4.87188 avg_loss = 3.37447\n",
      "epoch no.3 train no.284150  loss = 3.29699 avg_loss = 3.39575\n",
      "epoch no.3 train no.284160  loss = 3.79546 avg_loss = 3.38744\n",
      "epoch no.3 train no.284170  loss = 2.80903 avg_loss = 3.33714\n",
      "epoch no.3 train no.284180  loss = 2.01601 avg_loss = 3.34884\n",
      "epoch no.3 train no.284190  loss = 3.64488 avg_loss = 3.32018\n",
      "epoch no.3 train no.284200  loss = 4.74535 avg_loss = 3.40595\n",
      "epoch no.3 train no.284210  loss = 2.79022 avg_loss = 3.41149\n",
      "epoch no.3 train no.284220  loss = 4.57583 avg_loss = 3.45693\n",
      "epoch no.3 train no.284230  loss = 5.22981 avg_loss = 3.49301\n",
      "epoch no.3 train no.284240  loss = 4.11286 avg_loss = 3.49566\n",
      "epoch no.3 train no.284250  loss = 3.95085 avg_loss = 3.49201\n",
      "epoch no.3 train no.284260  loss = 4.02625 avg_loss = 3.54122\n",
      "epoch no.3 train no.284270  loss = 4.15376 avg_loss = 3.54163\n",
      "epoch no.3 train no.284280  loss = 3.83085 avg_loss = 3.55497\n",
      "epoch no.3 train no.284290  loss = 5.56740 avg_loss = 3.52551\n",
      "epoch no.3 train no.284300  loss = 3.01681 avg_loss = 3.53523\n",
      "epoch no.3 train no.284310  loss = 3.91132 avg_loss = 3.53061\n",
      "epoch no.3 train no.284320  loss = 4.14296 avg_loss = 3.52168\n",
      "epoch no.3 train no.284330  loss = 3.40770 avg_loss = 3.49977\n",
      "epoch no.3 train no.284340  loss = 3.59587 avg_loss = 3.47923\n",
      "epoch no.3 train no.284350  loss = 4.83518 avg_loss = 3.47079\n",
      "epoch no.3 train no.284360  loss = 3.66106 avg_loss = 3.45938\n",
      "epoch no.3 train no.284370  loss = 2.96082 avg_loss = 3.41093\n",
      "epoch no.3 train no.284380  loss = 2.80562 avg_loss = 3.36903\n",
      "epoch no.3 train no.284390  loss = 2.71021 avg_loss = 3.31924\n",
      "epoch no.3 train no.284400  loss = 2.80761 avg_loss = 3.34135\n",
      "epoch no.3 train no.284410  loss = 3.16775 avg_loss = 3.34100\n",
      "epoch no.3 train no.284420  loss = 3.96586 avg_loss = 3.34480\n",
      "epoch no.3 train no.284430  loss = 3.60912 avg_loss = 3.32945\n",
      "epoch no.3 train no.284440  loss = 4.74136 avg_loss = 3.34611\n",
      "epoch no.3 train no.284450  loss = 2.72820 avg_loss = 3.34528\n",
      "epoch no.3 train no.284460  loss = 1.98063 avg_loss = 3.32142\n",
      "epoch no.3 train no.284470  loss = 4.81613 avg_loss = 3.32459\n",
      "epoch no.3 train no.284480  loss = 2.37294 avg_loss = 3.37990\n",
      "epoch no.3 train no.284490  loss = 2.46231 avg_loss = 3.35769\n",
      "epoch no.3 train no.284500  loss = 3.23399 avg_loss = 3.29811\n",
      "epoch no.3 train no.284510  loss = 3.29987 avg_loss = 3.26270\n",
      "epoch no.3 train no.284520  loss = 3.44951 avg_loss = 3.24449\n",
      "epoch no.3 train no.284530  loss = 3.29742 avg_loss = 3.25457\n",
      "epoch no.3 train no.284540  loss = 2.08087 avg_loss = 3.25769\n",
      "epoch no.3 train no.284550  loss = 3.29378 avg_loss = 3.27230\n",
      "epoch no.3 train no.284560  loss = 4.60476 avg_loss = 3.27288\n",
      "epoch no.3 train no.284570  loss = 4.55677 avg_loss = 3.32374\n",
      "epoch no.3 train no.284580  loss = 2.36518 avg_loss = 3.32409\n",
      "epoch no.3 train no.284590  loss = 2.61105 avg_loss = 3.29769\n",
      "epoch no.3 train no.284600  loss = 4.66616 avg_loss = 3.35761\n",
      "epoch no.3 train no.284610  loss = 5.00834 avg_loss = 3.37403\n",
      "epoch no.3 train no.284620  loss = 2.20675 avg_loss = 3.37277\n",
      "epoch no.3 train no.284630  loss = 6.25515 avg_loss = 3.37067\n",
      "epoch no.3 train no.284640  loss = 2.67314 avg_loss = 3.38221\n",
      "epoch no.3 train no.284650  loss = 4.48880 avg_loss = 3.41249\n",
      "epoch no.3 train no.284660  loss = 1.71819 avg_loss = 3.34885\n",
      "epoch no.3 train no.284670  loss = 3.65542 avg_loss = 3.35700\n",
      "epoch no.3 train no.284680  loss = 3.73825 avg_loss = 3.32957\n",
      "epoch no.3 train no.284690  loss = 3.32662 avg_loss = 3.32771\n",
      "epoch no.3 train no.284700  loss = 3.69587 avg_loss = 3.35526\n",
      "epoch no.3 train no.284710  loss = 2.03707 avg_loss = 3.36541\n",
      "epoch no.3 train no.284720  loss = 5.25602 avg_loss = 3.38012\n",
      "epoch no.3 train no.284730  loss = 2.96084 avg_loss = 3.33377\n",
      "epoch no.3 train no.284740  loss = 2.31724 avg_loss = 3.28089\n",
      "epoch no.3 train no.284750  loss = 4.15190 avg_loss = 3.29853\n",
      "epoch no.3 train no.284760  loss = 2.80933 avg_loss = 3.28863\n",
      "epoch no.3 train no.284770  loss = 3.84615 avg_loss = 3.29559\n",
      "epoch no.3 train no.284780  loss = 3.63069 avg_loss = 3.27589\n",
      "epoch no.3 train no.284790  loss = 2.16399 avg_loss = 3.31239\n",
      "epoch no.3 train no.284800  loss = 3.27360 avg_loss = 3.31962\n",
      "epoch no.3 train no.284810  loss = 5.75637 avg_loss = 3.34582\n",
      "epoch no.3 train no.284820  loss = 2.66793 avg_loss = 3.31101\n",
      "epoch no.3 train no.284830  loss = 4.05479 avg_loss = 3.34928\n",
      "epoch no.3 train no.284840  loss = 3.18591 avg_loss = 3.35172\n",
      "epoch no.3 train no.284850  loss = 4.32751 avg_loss = 3.30433\n",
      "epoch no.3 train no.284860  loss = 3.53021 avg_loss = 3.30505\n",
      "epoch no.3 train no.284870  loss = 2.95326 avg_loss = 3.30396\n",
      "epoch no.3 train no.284880  loss = 3.08873 avg_loss = 3.35517\n",
      "epoch no.3 train no.284890  loss = 3.58441 avg_loss = 3.35088\n",
      "epoch no.3 train no.284900  loss = 3.27022 avg_loss = 3.32650\n",
      "epoch no.3 train no.284910  loss = 5.57233 avg_loss = 3.33023\n",
      "epoch no.3 train no.284920  loss = 2.92024 avg_loss = 3.33359\n",
      "epoch no.3 train no.284930  loss = 4.55636 avg_loss = 3.33342\n",
      "epoch no.3 train no.284940  loss = 2.78558 avg_loss = 3.31518\n",
      "epoch no.3 train no.284950  loss = 3.56073 avg_loss = 3.30870\n",
      "epoch no.3 train no.284960  loss = 3.09700 avg_loss = 3.29866\n",
      "epoch no.3 train no.284970  loss = 4.41579 avg_loss = 3.32484\n",
      "epoch no.3 train no.284980  loss = 2.79242 avg_loss = 3.35143\n",
      "epoch no.3 train no.284990  loss = 3.28640 avg_loss = 3.31075\n",
      "epoch no.3 train no.285000  loss = 2.79509 avg_loss = 3.30514\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '▁o', '▁o', 'st', '</s>']\n",
      "추억의 인기 드라마 ost</s>\n",
      "epoch no.3 train no.285010  loss = 2.63752 avg_loss = 3.32971\n",
      "epoch no.3 train no.285020  loss = 2.14117 avg_loss = 3.33528\n",
      "epoch no.3 train no.285030  loss = 2.72239 avg_loss = 3.32461\n",
      "epoch no.3 train no.285040  loss = 4.98564 avg_loss = 3.36091\n",
      "epoch no.3 train no.285050  loss = 2.80763 avg_loss = 3.34375\n",
      "epoch no.3 train no.285060  loss = 3.82819 avg_loss = 3.30211\n",
      "epoch no.3 train no.285070  loss = 3.18932 avg_loss = 3.29326\n",
      "epoch no.3 train no.285080  loss = 3.11454 avg_loss = 3.34040\n",
      "epoch no.3 train no.285090  loss = 2.17461 avg_loss = 3.36853\n",
      "epoch no.3 train no.285100  loss = 2.80214 avg_loss = 3.34954\n",
      "epoch no.3 train no.285110  loss = 2.96657 avg_loss = 3.35861\n",
      "epoch no.3 train no.285120  loss = 3.40741 avg_loss = 3.38755\n",
      "epoch no.3 train no.285130  loss = 2.35327 avg_loss = 3.40541\n",
      "epoch no.3 train no.285140  loss = 3.27608 avg_loss = 3.39483\n",
      "epoch no.3 train no.285150  loss = 4.35190 avg_loss = 3.41526\n",
      "epoch no.3 train no.285160  loss = 2.67042 avg_loss = 3.43745\n",
      "epoch no.3 train no.285170  loss = 4.19902 avg_loss = 3.40534\n",
      "epoch no.3 train no.285180  loss = 2.80071 avg_loss = 3.40526\n",
      "epoch no.3 train no.285190  loss = 4.34365 avg_loss = 3.38951\n",
      "epoch no.3 train no.285200  loss = 3.06102 avg_loss = 3.39139\n",
      "epoch no.3 train no.285210  loss = 3.31340 avg_loss = 3.35945\n",
      "epoch no.3 train no.285220  loss = 2.74658 avg_loss = 3.39264\n",
      "epoch no.3 train no.285230  loss = 4.58130 avg_loss = 3.43235\n",
      "epoch no.3 train no.285240  loss = 4.12549 avg_loss = 3.43571\n",
      "epoch no.3 train no.285250  loss = 3.27024 avg_loss = 3.39367\n",
      "epoch no.3 train no.285260  loss = 1.82345 avg_loss = 3.37090\n",
      "epoch no.3 train no.285270  loss = 4.97243 avg_loss = 3.37049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.285280  loss = 4.82298 avg_loss = 3.36856\n",
      "epoch no.3 train no.285290  loss = 2.32253 avg_loss = 3.35361\n",
      "epoch no.3 train no.285300  loss = 2.64781 avg_loss = 3.27717\n",
      "epoch no.3 train no.285310  loss = 2.53770 avg_loss = 3.33605\n",
      "epoch no.3 train no.285320  loss = 3.46726 avg_loss = 3.32566\n",
      "epoch no.3 train no.285330  loss = 2.80815 avg_loss = 3.31808\n",
      "epoch no.3 train no.285340  loss = 1.77046 avg_loss = 3.29705\n",
      "epoch no.3 train no.285350  loss = 2.32910 avg_loss = 3.28491\n",
      "epoch no.3 train no.285360  loss = 2.26286 avg_loss = 3.28232\n",
      "epoch no.3 train no.285370  loss = 2.91480 avg_loss = 3.25004\n",
      "epoch no.3 train no.285380  loss = 2.86583 avg_loss = 3.24387\n",
      "epoch no.3 train no.285390  loss = 2.24676 avg_loss = 3.28445\n",
      "epoch no.3 train no.285400  loss = 3.65699 avg_loss = 3.29474\n",
      "epoch no.3 train no.285410  loss = 3.14117 avg_loss = 3.29590\n",
      "epoch no.3 train no.285420  loss = 3.90518 avg_loss = 3.31060\n",
      "epoch no.3 train no.285430  loss = 3.30972 avg_loss = 3.33009\n",
      "epoch no.3 train no.285440  loss = 2.17671 avg_loss = 3.28524\n",
      "epoch no.3 train no.285450  loss = 4.59114 avg_loss = 3.27493\n",
      "epoch no.3 train no.285460  loss = 2.06132 avg_loss = 3.23883\n",
      "epoch no.3 train no.285470  loss = 1.92452 avg_loss = 3.29801\n",
      "epoch no.3 train no.285480  loss = 3.72523 avg_loss = 3.30457\n",
      "epoch no.3 train no.285490  loss = 4.28666 avg_loss = 3.30202\n",
      "epoch no.3 train no.285500  loss = 2.93100 avg_loss = 3.30055\n",
      "epoch no.3 train no.285510  loss = 2.80738 avg_loss = 3.29178\n",
      "epoch no.3 train no.285520  loss = 2.32177 avg_loss = 3.27064\n",
      "epoch no.3 train no.285530  loss = 2.22947 avg_loss = 3.24709\n",
      "epoch no.3 train no.285540  loss = 3.47102 avg_loss = 3.28381\n",
      "epoch no.3 train no.285550  loss = 2.64461 avg_loss = 3.26567\n",
      "epoch no.3 train no.285560  loss = 2.45940 avg_loss = 3.28042\n",
      "epoch no.3 train no.285570  loss = 3.53414 avg_loss = 3.26507\n",
      "epoch no.3 train no.285580  loss = 2.77880 avg_loss = 3.25196\n",
      "epoch no.3 train no.285590  loss = 4.10260 avg_loss = 3.21084\n",
      "epoch no.3 train no.285600  loss = 4.80625 avg_loss = 3.24869\n",
      "epoch no.3 train no.285610  loss = 2.16302 avg_loss = 3.18977\n",
      "epoch no.3 train no.285620  loss = 4.67575 avg_loss = 3.24665\n",
      "epoch no.3 train no.285630  loss = 3.60607 avg_loss = 3.30276\n",
      "epoch no.3 train no.285640  loss = 4.05437 avg_loss = 3.25734\n",
      "epoch no.3 train no.285650  loss = 4.22615 avg_loss = 3.28962\n",
      "epoch no.3 train no.285660  loss = 2.04185 avg_loss = 3.24405\n",
      "epoch no.3 train no.285670  loss = 2.77766 avg_loss = 3.25084\n",
      "epoch no.3 train no.285680  loss = 4.26255 avg_loss = 3.26680\n",
      "epoch no.3 train no.285690  loss = 3.20838 avg_loss = 3.24711\n",
      "epoch no.3 train no.285700  loss = 2.30305 avg_loss = 3.18965\n",
      "epoch no.3 train no.285710  loss = 2.05640 avg_loss = 3.20671\n",
      "epoch no.3 train no.285720  loss = 4.03102 avg_loss = 3.22990\n",
      "epoch no.3 train no.285730  loss = 5.25300 avg_loss = 3.24792\n",
      "epoch no.3 train no.285740  loss = 4.89122 avg_loss = 3.32614\n",
      "epoch no.3 train no.285750  loss = 3.37293 avg_loss = 3.30783\n",
      "epoch no.3 train no.285760  loss = 2.21855 avg_loss = 3.30910\n",
      "epoch no.3 train no.285770  loss = 3.33969 avg_loss = 3.32692\n",
      "epoch no.3 train no.285780  loss = 3.61089 avg_loss = 3.37970\n",
      "epoch no.3 train no.285790  loss = 3.35396 avg_loss = 3.36893\n",
      "epoch no.3 train no.285800  loss = 4.14005 avg_loss = 3.37817\n",
      "epoch no.3 train no.285810  loss = 2.28630 avg_loss = 3.35752\n",
      "epoch no.3 train no.285820  loss = 2.65644 avg_loss = 3.34362\n",
      "epoch no.3 train no.285830  loss = 2.41976 avg_loss = 3.31657\n",
      "epoch no.3 train no.285840  loss = 4.63822 avg_loss = 3.31424\n",
      "epoch no.3 train no.285850  loss = 3.65206 avg_loss = 3.30136\n",
      "epoch no.3 train no.285860  loss = 4.27663 avg_loss = 3.32279\n",
      "epoch no.3 train no.285870  loss = 3.94712 avg_loss = 3.32413\n",
      "epoch no.3 train no.285880  loss = 2.68806 avg_loss = 3.32519\n",
      "epoch no.3 train no.285890  loss = 4.71430 avg_loss = 3.34191\n",
      "epoch no.3 train no.285900  loss = 1.90302 avg_loss = 3.34204\n",
      "epoch no.3 train no.285910  loss = 3.08061 avg_loss = 3.32644\n",
      "epoch no.3 train no.285920  loss = 5.88451 avg_loss = 3.37755\n",
      "epoch no.3 train no.285930  loss = 5.19733 avg_loss = 3.40762\n",
      "epoch no.3 train no.285940  loss = 3.56599 avg_loss = 3.38286\n",
      "epoch no.3 train no.285950  loss = 2.72473 avg_loss = 3.36232\n",
      "epoch no.3 train no.285960  loss = 2.83517 avg_loss = 3.36016\n",
      "epoch no.3 train no.285970  loss = 3.86984 avg_loss = 3.34145\n",
      "epoch no.3 train no.285980  loss = 3.78991 avg_loss = 3.36668\n",
      "epoch no.3 train no.285990  loss = 2.15281 avg_loss = 3.37290\n",
      "epoch no.3 train no.286000  loss = 4.83352 avg_loss = 3.38977\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.3 train no.286010  loss = 3.69561 avg_loss = 3.38299\n",
      "epoch no.3 train no.286020  loss = 2.31173 avg_loss = 3.45870\n",
      "epoch no.3 train no.286030  loss = 3.38408 avg_loss = 3.49683\n",
      "epoch no.3 train no.286040  loss = 3.62209 avg_loss = 3.46670\n",
      "epoch no.3 train no.286050  loss = 3.28137 avg_loss = 3.52156\n",
      "epoch no.3 train no.286060  loss = 3.25125 avg_loss = 3.49919\n",
      "epoch no.3 train no.286070  loss = 2.39246 avg_loss = 3.43374\n",
      "epoch no.3 train no.286080  loss = 4.29928 avg_loss = 3.40728\n",
      "epoch no.3 train no.286090  loss = 2.53272 avg_loss = 3.36139\n",
      "epoch no.3 train no.286100  loss = 5.12790 avg_loss = 3.33376\n",
      "epoch no.3 train no.286110  loss = 3.80181 avg_loss = 3.42665\n",
      "epoch no.3 train no.286120  loss = 2.63028 avg_loss = 3.42437\n",
      "epoch no.3 train no.286130  loss = 5.21020 avg_loss = 3.43717\n",
      "epoch no.3 train no.286140  loss = 4.53253 avg_loss = 3.46682\n",
      "epoch no.3 train no.286150  loss = 4.11803 avg_loss = 3.46893\n",
      "epoch no.3 train no.286160  loss = 5.04251 avg_loss = 3.47832\n",
      "epoch no.3 train no.286170  loss = 3.61959 avg_loss = 3.51501\n",
      "epoch no.3 train no.286180  loss = 3.03591 avg_loss = 3.46955\n",
      "epoch no.3 train no.286190  loss = 2.18632 avg_loss = 3.48263\n",
      "epoch no.3 train no.286200  loss = 2.31876 avg_loss = 3.49388\n",
      "epoch no.3 train no.286210  loss = 2.76327 avg_loss = 3.44097\n",
      "epoch no.3 train no.286220  loss = 4.10759 avg_loss = 3.42850\n",
      "epoch no.3 train no.286230  loss = 4.93713 avg_loss = 3.43718\n",
      "epoch no.3 train no.286240  loss = 1.73090 avg_loss = 3.41206\n",
      "epoch no.3 train no.286250  loss = 2.75565 avg_loss = 3.41547\n",
      "epoch no.3 train no.286260  loss = 2.79197 avg_loss = 3.40108\n",
      "epoch no.3 train no.286270  loss = 3.04711 avg_loss = 3.38785\n",
      "epoch no.3 train no.286280  loss = 2.63549 avg_loss = 3.40952\n",
      "epoch no.3 train no.286290  loss = 3.24484 avg_loss = 3.39356\n",
      "epoch no.3 train no.286300  loss = 2.42111 avg_loss = 3.39008\n",
      "epoch no.3 train no.286310  loss = 3.91023 avg_loss = 3.39190\n",
      "epoch no.3 train no.286320  loss = 4.45507 avg_loss = 3.39679\n",
      "epoch no.3 train no.286330  loss = 2.17313 avg_loss = 3.47831\n",
      "epoch no.3 train no.286340  loss = 3.55600 avg_loss = 3.50266\n",
      "epoch no.3 train no.286350  loss = 3.78039 avg_loss = 3.47736\n",
      "epoch no.3 train no.286360  loss = 2.82404 avg_loss = 3.46192\n",
      "epoch no.3 train no.286370  loss = 1.56968 avg_loss = 3.42703\n",
      "epoch no.3 train no.286380  loss = 3.78096 avg_loss = 3.41160\n",
      "epoch no.3 train no.286390  loss = 3.84735 avg_loss = 3.43874\n",
      "epoch no.3 train no.286400  loss = 4.27735 avg_loss = 3.44675\n",
      "epoch no.3 train no.286410  loss = 2.50801 avg_loss = 3.41936\n",
      "epoch no.3 train no.286420  loss = 3.93099 avg_loss = 3.43110\n",
      "epoch no.3 train no.286430  loss = 1.89249 avg_loss = 3.40599\n",
      "epoch no.3 train no.286440  loss = 3.90454 avg_loss = 3.43900\n",
      "epoch no.3 train no.286450  loss = 2.29560 avg_loss = 3.42926\n",
      "epoch no.3 train no.286460  loss = 2.54984 avg_loss = 3.40869\n",
      "epoch no.3 train no.286470  loss = 3.02053 avg_loss = 3.46414\n",
      "epoch no.3 train no.286480  loss = 2.39605 avg_loss = 3.50853\n",
      "epoch no.3 train no.286490  loss = 3.63622 avg_loss = 3.49345\n",
      "epoch no.3 train no.286500  loss = 3.01105 avg_loss = 3.51124\n",
      "epoch no.3 train no.286510  loss = 3.38023 avg_loss = 3.47288\n",
      "epoch no.3 train no.286520  loss = 5.78391 avg_loss = 3.49174\n",
      "epoch no.3 train no.286530  loss = 5.45310 avg_loss = 3.45430\n",
      "epoch no.3 train no.286540  loss = 2.67674 avg_loss = 3.47801\n",
      "epoch no.3 train no.286550  loss = 3.60798 avg_loss = 3.47135\n",
      "epoch no.3 train no.286560  loss = 3.77325 avg_loss = 3.46238\n",
      "epoch no.3 train no.286570  loss = 4.29928 avg_loss = 3.41881\n",
      "epoch no.3 train no.286580  loss = 3.32493 avg_loss = 3.40046\n",
      "epoch no.3 train no.286590  loss = 5.25281 avg_loss = 3.41441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.286600  loss = 3.98790 avg_loss = 3.44441\n",
      "epoch no.3 train no.286610  loss = 3.05656 avg_loss = 3.43842\n",
      "epoch no.3 train no.286620  loss = 4.17345 avg_loss = 3.39518\n",
      "epoch no.3 train no.286630  loss = 2.11946 avg_loss = 3.37621\n",
      "epoch no.3 train no.286640  loss = 3.11890 avg_loss = 3.34691\n",
      "epoch no.3 train no.286650  loss = 2.48044 avg_loss = 3.36091\n",
      "epoch no.3 train no.286660  loss = 2.58290 avg_loss = 3.34929\n",
      "epoch no.3 train no.286670  loss = 3.59894 avg_loss = 3.39397\n",
      "epoch no.3 train no.286680  loss = 2.85689 avg_loss = 3.37918\n",
      "epoch no.3 train no.286690  loss = 2.05191 avg_loss = 3.37257\n",
      "epoch no.3 train no.286700  loss = 4.46354 avg_loss = 3.34179\n",
      "epoch no.3 train no.286710  loss = 1.68046 avg_loss = 3.38389\n",
      "epoch no.3 train no.286720  loss = 2.65839 avg_loss = 3.31378\n",
      "epoch no.3 train no.286730  loss = 2.45523 avg_loss = 3.33665\n",
      "epoch no.3 train no.286740  loss = 3.58734 avg_loss = 3.38326\n",
      "epoch no.3 train no.286750  loss = 3.57714 avg_loss = 3.38899\n",
      "epoch no.3 train no.286760  loss = 2.86592 avg_loss = 3.35498\n",
      "epoch no.3 train no.286770  loss = 2.06807 avg_loss = 3.33155\n",
      "epoch no.3 train no.286780  loss = 2.55681 avg_loss = 3.29340\n",
      "epoch no.3 train no.286790  loss = 4.24013 avg_loss = 3.29113\n",
      "epoch no.3 train no.286800  loss = 5.05211 avg_loss = 3.32196\n",
      "epoch no.3 train no.286810  loss = 5.19136 avg_loss = 3.34225\n",
      "epoch no.3 train no.286820  loss = 2.32972 avg_loss = 3.33972\n",
      "epoch no.3 train no.286830  loss = 2.76073 avg_loss = 3.32270\n",
      "epoch no.3 train no.286840  loss = 3.15725 avg_loss = 3.34281\n",
      "epoch no.3 train no.286850  loss = 2.42991 avg_loss = 3.34601\n",
      "epoch no.3 train no.286860  loss = 2.61845 avg_loss = 3.32889\n",
      "epoch no.3 train no.286870  loss = 2.70588 avg_loss = 3.31634\n",
      "epoch no.3 train no.286880  loss = 3.37630 avg_loss = 3.35036\n",
      "epoch no.3 train no.286890  loss = 3.75619 avg_loss = 3.31645\n",
      "epoch no.3 train no.286900  loss = 3.84597 avg_loss = 3.32160\n",
      "epoch no.3 train no.286910  loss = 2.89182 avg_loss = 3.29442\n",
      "epoch no.3 train no.286920  loss = 4.27577 avg_loss = 3.29898\n",
      "epoch no.3 train no.286930  loss = 2.67570 avg_loss = 3.26435\n",
      "epoch no.3 train no.286940  loss = 3.18581 avg_loss = 3.24535\n",
      "epoch no.3 train no.286950  loss = 1.52590 avg_loss = 3.23500\n",
      "epoch no.3 train no.286960  loss = 3.48177 avg_loss = 3.26637\n",
      "epoch no.3 train no.286970  loss = 2.89274 avg_loss = 3.28461\n",
      "epoch no.3 train no.286980  loss = 2.71844 avg_loss = 3.25428\n",
      "epoch no.3 train no.286990  loss = 4.36800 avg_loss = 3.20927\n",
      "epoch no.3 train no.287000  loss = 2.46680 avg_loss = 3.21742\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '팝', '송', '▁모음', '</s>']\n",
      "추억의 올드팝송 모음</s>\n",
      "epoch no.3 train no.287010  loss = 4.58780 avg_loss = 3.25094\n",
      "epoch no.3 train no.287020  loss = 2.40269 avg_loss = 3.26493\n",
      "epoch no.3 train no.287030  loss = 2.82008 avg_loss = 3.22550\n",
      "epoch no.3 train no.287040  loss = 2.41293 avg_loss = 3.20580\n",
      "epoch no.3 train no.287050  loss = 2.76877 avg_loss = 3.17920\n",
      "epoch no.3 train no.287060  loss = 2.23584 avg_loss = 3.21916\n",
      "epoch no.3 train no.287070  loss = 3.93486 avg_loss = 3.25338\n",
      "epoch no.3 train no.287080  loss = 4.68938 avg_loss = 3.23291\n",
      "epoch no.3 train no.287090  loss = 3.48456 avg_loss = 3.29028\n",
      "epoch no.3 train no.287100  loss = 2.69882 avg_loss = 3.29954\n",
      "epoch no.3 train no.287110  loss = 3.58486 avg_loss = 3.27936\n",
      "epoch no.3 train no.287120  loss = 3.60188 avg_loss = 3.29913\n",
      "epoch no.3 train no.287130  loss = 2.97620 avg_loss = 3.30043\n",
      "epoch no.3 train no.287140  loss = 3.94101 avg_loss = 3.32925\n",
      "epoch no.3 train no.287150  loss = 3.72546 avg_loss = 3.34439\n",
      "epoch no.3 train no.287160  loss = 2.96630 avg_loss = 3.37226\n",
      "epoch no.3 train no.287170  loss = 3.55917 avg_loss = 3.38180\n",
      "epoch no.3 train no.287180  loss = 2.47978 avg_loss = 3.42607\n",
      "epoch no.3 train no.287190  loss = 3.38622 avg_loss = 3.46059\n",
      "epoch no.3 train no.287200  loss = 5.26382 avg_loss = 3.46331\n",
      "epoch no.3 train no.287210  loss = 2.26866 avg_loss = 3.43909\n",
      "epoch no.3 train no.287220  loss = 2.79967 avg_loss = 3.42120\n",
      "epoch no.3 train no.287230  loss = 3.63356 avg_loss = 3.39532\n",
      "epoch no.3 train no.287240  loss = 2.93001 avg_loss = 3.38764\n",
      "epoch no.3 train no.287250  loss = 3.08066 avg_loss = 3.42227\n",
      "epoch no.3 train no.287260  loss = 1.34812 avg_loss = 3.38652\n",
      "epoch no.3 train no.287270  loss = 3.88538 avg_loss = 3.38759\n",
      "epoch no.3 train no.287280  loss = 2.34193 avg_loss = 3.35552\n",
      "epoch no.3 train no.287290  loss = 2.08209 avg_loss = 3.34948\n",
      "epoch no.3 train no.287300  loss = 5.07827 avg_loss = 3.38319\n",
      "epoch no.3 train no.287310  loss = 3.67626 avg_loss = 3.36555\n",
      "epoch no.3 train no.287320  loss = 3.92600 avg_loss = 3.41302\n",
      "epoch no.3 train no.287330  loss = 3.32894 avg_loss = 3.36430\n",
      "epoch no.3 train no.287340  loss = 3.45001 avg_loss = 3.36832\n",
      "epoch no.3 train no.287350  loss = 3.71585 avg_loss = 3.38018\n",
      "epoch no.3 train no.287360  loss = 3.25558 avg_loss = 3.39714\n",
      "epoch no.3 train no.287370  loss = 2.16849 avg_loss = 3.39978\n",
      "epoch no.3 train no.287380  loss = 4.77178 avg_loss = 3.37599\n",
      "epoch no.3 train no.287390  loss = 2.39402 avg_loss = 3.35453\n",
      "epoch no.3 train no.287400  loss = 4.83041 avg_loss = 3.40742\n",
      "epoch no.3 train no.287410  loss = 2.19420 avg_loss = 3.39822\n",
      "epoch no.3 train no.287420  loss = 5.53951 avg_loss = 3.41870\n",
      "epoch no.3 train no.287430  loss = 3.61548 avg_loss = 3.41947\n",
      "epoch no.3 train no.287440  loss = 3.58490 avg_loss = 3.40540\n",
      "epoch no.3 train no.287450  loss = 3.05544 avg_loss = 3.39656\n",
      "epoch no.3 train no.287460  loss = 3.21326 avg_loss = 3.37623\n",
      "epoch no.3 train no.287470  loss = 1.95463 avg_loss = 3.40243\n",
      "epoch no.3 train no.287480  loss = 5.10138 avg_loss = 3.39978\n",
      "epoch no.3 train no.287490  loss = 3.16050 avg_loss = 3.39243\n",
      "epoch no.3 train no.287500  loss = 3.42180 avg_loss = 3.36463\n",
      "epoch no.3 train no.287510  loss = 2.54699 avg_loss = 3.34140\n",
      "epoch no.3 train no.287520  loss = 5.20128 avg_loss = 3.32897\n",
      "epoch no.3 train no.287530  loss = 3.61140 avg_loss = 3.36722\n",
      "epoch no.3 train no.287540  loss = 3.61876 avg_loss = 3.36060\n",
      "epoch no.3 train no.287550  loss = 2.18781 avg_loss = 3.31405\n",
      "epoch no.3 train no.287560  loss = 2.84521 avg_loss = 3.29364\n",
      "epoch no.3 train no.287570  loss = 4.12247 avg_loss = 3.30499\n",
      "epoch no.3 train no.287580  loss = 5.99631 avg_loss = 3.35370\n",
      "epoch no.3 train no.287590  loss = 2.48980 avg_loss = 3.32759\n",
      "epoch no.3 train no.287600  loss = 2.82031 avg_loss = 3.31207\n",
      "epoch no.3 train no.287610  loss = 2.64281 avg_loss = 3.33941\n",
      "epoch no.3 train no.287620  loss = 3.50459 avg_loss = 3.34190\n",
      "epoch no.3 train no.287630  loss = 4.37809 avg_loss = 3.31091\n",
      "epoch no.3 train no.287640  loss = 2.10487 avg_loss = 3.36519\n",
      "epoch no.3 train no.287650  loss = 2.93109 avg_loss = 3.35917\n",
      "epoch no.3 train no.287660  loss = 3.33327 avg_loss = 3.32624\n",
      "epoch no.3 train no.287670  loss = 3.98912 avg_loss = 3.34929\n",
      "epoch no.3 train no.287680  loss = 3.22775 avg_loss = 3.33612\n",
      "epoch no.3 train no.287690  loss = 4.35573 avg_loss = 3.35806\n",
      "epoch no.3 train no.287700  loss = 2.66621 avg_loss = 3.39476\n",
      "epoch no.3 train no.287710  loss = 3.82922 avg_loss = 3.42880\n",
      "epoch no.3 train no.287720  loss = 4.41934 avg_loss = 3.44545\n",
      "epoch no.3 train no.287730  loss = 4.06628 avg_loss = 3.44061\n",
      "epoch no.3 train no.287740  loss = 2.79597 avg_loss = 3.42419\n",
      "epoch no.3 train no.287750  loss = 5.18211 avg_loss = 3.40736\n",
      "epoch no.3 train no.287760  loss = 2.46997 avg_loss = 3.47659\n",
      "epoch no.3 train no.287770  loss = 4.34941 avg_loss = 3.50898\n",
      "epoch no.3 train no.287780  loss = 2.68045 avg_loss = 3.54168\n",
      "epoch no.3 train no.287790  loss = 1.98813 avg_loss = 3.51413\n",
      "epoch no.3 train no.287800  loss = 4.68999 avg_loss = 3.52633\n",
      "epoch no.3 train no.287810  loss = 2.65993 avg_loss = 3.51890\n",
      "epoch no.3 train no.287820  loss = 3.69485 avg_loss = 3.51713\n",
      "epoch no.3 train no.287830  loss = 4.77052 avg_loss = 3.55293\n",
      "epoch no.3 train no.287840  loss = 2.50104 avg_loss = 3.53716\n",
      "epoch no.3 train no.287850  loss = 2.62227 avg_loss = 3.50311\n",
      "epoch no.3 train no.287860  loss = 2.11941 avg_loss = 3.50657\n",
      "epoch no.3 train no.287870  loss = 3.39503 avg_loss = 3.48608\n",
      "epoch no.3 train no.287880  loss = 2.65042 avg_loss = 3.40160\n",
      "epoch no.3 train no.287890  loss = 4.16032 avg_loss = 3.39769\n",
      "epoch no.3 train no.287900  loss = 4.97663 avg_loss = 3.36200\n",
      "epoch no.3 train no.287910  loss = 2.52207 avg_loss = 3.34336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.287920  loss = 4.09053 avg_loss = 3.38431\n",
      "epoch no.3 train no.287930  loss = 4.77472 avg_loss = 3.42144\n",
      "epoch no.3 train no.287940  loss = 2.21961 avg_loss = 3.41776\n",
      "epoch no.3 train no.287950  loss = 3.15221 avg_loss = 3.41655\n",
      "epoch no.3 train no.287960  loss = 3.00247 avg_loss = 3.43639\n",
      "epoch no.3 train no.287970  loss = 3.20773 avg_loss = 3.40262\n",
      "epoch no.3 train no.287980  loss = 2.71433 avg_loss = 3.35131\n",
      "epoch no.3 train no.287990  loss = 2.52579 avg_loss = 3.36957\n",
      "epoch no.3 train no.288000  loss = 2.05594 avg_loss = 3.36198\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.288010  loss = 2.83339 avg_loss = 3.35243\n",
      "epoch no.3 train no.288020  loss = 2.75461 avg_loss = 3.31497\n",
      "epoch no.3 train no.288030  loss = 2.87460 avg_loss = 3.31080\n",
      "epoch no.3 train no.288040  loss = 3.41241 avg_loss = 3.29302\n",
      "epoch no.3 train no.288050  loss = 3.25024 avg_loss = 3.30806\n",
      "epoch no.3 train no.288060  loss = 2.20928 avg_loss = 3.27599\n",
      "epoch no.3 train no.288070  loss = 4.76101 avg_loss = 3.23450\n",
      "epoch no.3 train no.288080  loss = 3.40127 avg_loss = 3.25983\n",
      "epoch no.3 train no.288090  loss = 2.40260 avg_loss = 3.27928\n",
      "epoch no.3 train no.288100  loss = 3.25904 avg_loss = 3.32637\n",
      "epoch no.3 train no.288110  loss = 3.31373 avg_loss = 3.33591\n",
      "epoch no.3 train no.288120  loss = 3.79137 avg_loss = 3.32437\n",
      "epoch no.3 train no.288130  loss = 2.77996 avg_loss = 3.34050\n",
      "epoch no.3 train no.288140  loss = 3.43858 avg_loss = 3.31884\n",
      "epoch no.3 train no.288150  loss = 3.65796 avg_loss = 3.31426\n",
      "epoch no.3 train no.288160  loss = 2.70329 avg_loss = 3.34136\n",
      "epoch no.3 train no.288170  loss = 5.94486 avg_loss = 3.33169\n",
      "epoch no.3 train no.288180  loss = 4.51577 avg_loss = 3.38299\n",
      "epoch no.3 train no.288190  loss = 4.11082 avg_loss = 3.38500\n",
      "epoch no.3 train no.288200  loss = 3.12206 avg_loss = 3.34978\n",
      "epoch no.3 train no.288210  loss = 3.69984 avg_loss = 3.32168\n",
      "epoch no.3 train no.288220  loss = 4.51416 avg_loss = 3.30800\n",
      "epoch no.3 train no.288230  loss = 2.30989 avg_loss = 3.31373\n",
      "epoch no.3 train no.288240  loss = 3.53582 avg_loss = 3.29932\n",
      "epoch no.3 train no.288250  loss = 3.83491 avg_loss = 3.33230\n",
      "epoch no.3 train no.288260  loss = 2.51537 avg_loss = 3.33119\n",
      "epoch no.3 train no.288270  loss = 3.89578 avg_loss = 3.38370\n",
      "epoch no.3 train no.288280  loss = 1.99751 avg_loss = 3.38715\n",
      "epoch no.3 train no.288290  loss = 4.77052 avg_loss = 3.42498\n",
      "epoch no.3 train no.288300  loss = 3.77041 avg_loss = 3.40757\n",
      "epoch no.3 train no.288310  loss = 2.78136 avg_loss = 3.39584\n",
      "epoch no.3 train no.288320  loss = 4.23562 avg_loss = 3.42287\n",
      "epoch no.3 train no.288330  loss = 3.69431 avg_loss = 3.41394\n",
      "epoch no.3 train no.288340  loss = 3.27334 avg_loss = 3.34767\n",
      "epoch no.3 train no.288350  loss = 3.46902 avg_loss = 3.37532\n",
      "epoch no.3 train no.288360  loss = 3.39885 avg_loss = 3.38124\n",
      "epoch no.3 train no.288370  loss = 5.82105 avg_loss = 3.38282\n",
      "epoch no.3 train no.288380  loss = 2.64230 avg_loss = 3.32468\n",
      "epoch no.3 train no.288390  loss = 3.25719 avg_loss = 3.36207\n",
      "epoch no.3 train no.288400  loss = 2.98801 avg_loss = 3.34082\n",
      "epoch no.3 train no.288410  loss = 2.28390 avg_loss = 3.30196\n",
      "epoch no.3 train no.288420  loss = 3.46698 avg_loss = 3.31541\n",
      "epoch no.3 train no.288430  loss = 4.39899 avg_loss = 3.32030\n",
      "epoch no.3 train no.288440  loss = 3.99265 avg_loss = 3.27755\n",
      "epoch no.3 train no.288450  loss = 4.18193 avg_loss = 3.30473\n",
      "epoch no.3 train no.288460  loss = 2.35858 avg_loss = 3.25444\n",
      "epoch no.3 train no.288470  loss = 3.02885 avg_loss = 3.27080\n",
      "epoch no.3 train no.288480  loss = 2.41965 avg_loss = 3.21697\n",
      "epoch no.3 train no.288490  loss = 3.97441 avg_loss = 3.17822\n",
      "epoch no.3 train no.288500  loss = 3.66785 avg_loss = 3.21659\n",
      "epoch no.3 train no.288510  loss = 4.87194 avg_loss = 3.24495\n",
      "epoch no.3 train no.288520  loss = 3.82426 avg_loss = 3.22165\n",
      "epoch no.3 train no.288530  loss = 3.56140 avg_loss = 3.25261\n",
      "epoch no.3 train no.288540  loss = 2.57346 avg_loss = 3.30589\n",
      "epoch no.3 train no.288550  loss = 3.31132 avg_loss = 3.29293\n",
      "epoch no.3 train no.288560  loss = 2.02548 avg_loss = 3.29959\n",
      "epoch no.3 train no.288570  loss = 2.01215 avg_loss = 3.27908\n",
      "epoch no.3 train no.288580  loss = 3.67612 avg_loss = 3.30762\n",
      "epoch no.3 train no.288590  loss = 4.31218 avg_loss = 3.34164\n",
      "epoch no.3 train no.288600  loss = 3.53971 avg_loss = 3.37919\n",
      "epoch no.3 train no.288610  loss = 5.29631 avg_loss = 3.45110\n",
      "epoch no.3 train no.288620  loss = 4.32826 avg_loss = 3.47510\n",
      "epoch no.3 train no.288630  loss = 2.04917 avg_loss = 3.45843\n",
      "epoch no.3 train no.288640  loss = 2.62837 avg_loss = 3.45037\n",
      "epoch no.3 train no.288650  loss = 2.53341 avg_loss = 3.39933\n",
      "epoch no.3 train no.288660  loss = 2.85889 avg_loss = 3.48691\n",
      "epoch no.3 train no.288670  loss = 4.33114 avg_loss = 3.51167\n",
      "epoch no.3 train no.288680  loss = 4.36965 avg_loss = 3.49781\n",
      "epoch no.3 train no.288690  loss = 3.36797 avg_loss = 3.45835\n",
      "epoch no.3 train no.288700  loss = 2.23696 avg_loss = 3.43257\n",
      "epoch no.3 train no.288710  loss = 5.28733 avg_loss = 3.41409\n",
      "epoch no.3 train no.288720  loss = 3.71251 avg_loss = 3.42721\n",
      "epoch no.3 train no.288730  loss = 4.17799 avg_loss = 3.40692\n",
      "epoch no.3 train no.288740  loss = 2.85294 avg_loss = 3.37726\n",
      "epoch no.3 train no.288750  loss = 4.11480 avg_loss = 3.36241\n",
      "epoch no.3 train no.288760  loss = 3.46980 avg_loss = 3.36778\n",
      "epoch no.3 train no.288770  loss = 2.85589 avg_loss = 3.33924\n",
      "epoch no.3 train no.288780  loss = 5.65604 avg_loss = 3.35922\n",
      "epoch no.3 train no.288790  loss = 4.28051 avg_loss = 3.41913\n",
      "epoch no.3 train no.288800  loss = 2.44211 avg_loss = 3.39369\n",
      "epoch no.3 train no.288810  loss = 2.80061 avg_loss = 3.38029\n",
      "epoch no.3 train no.288820  loss = 2.86997 avg_loss = 3.35079\n",
      "epoch no.3 train no.288830  loss = 3.70248 avg_loss = 3.36005\n",
      "epoch no.3 train no.288840  loss = 2.19094 avg_loss = 3.35560\n",
      "epoch no.3 train no.288850  loss = 4.01216 avg_loss = 3.34082\n",
      "epoch no.3 train no.288860  loss = 4.25030 avg_loss = 3.39309\n",
      "epoch no.3 train no.288870  loss = 3.24500 avg_loss = 3.32496\n",
      "epoch no.3 train no.288880  loss = 4.00680 avg_loss = 3.30982\n",
      "epoch no.3 train no.288890  loss = 1.83967 avg_loss = 3.32009\n",
      "epoch no.3 train no.288900  loss = 3.52111 avg_loss = 3.32581\n",
      "epoch no.3 train no.288910  loss = 4.39547 avg_loss = 3.36428\n",
      "epoch no.3 train no.288920  loss = 4.20365 avg_loss = 3.38397\n",
      "epoch no.3 train no.288930  loss = 3.77616 avg_loss = 3.42160\n",
      "epoch no.3 train no.288940  loss = 1.82519 avg_loss = 3.40296\n",
      "epoch no.3 train no.288950  loss = 4.07607 avg_loss = 3.40864\n",
      "epoch no.3 train no.288960  loss = 2.52759 avg_loss = 3.40644\n",
      "epoch no.3 train no.288970  loss = 3.11175 avg_loss = 3.34762\n",
      "epoch no.3 train no.288980  loss = 2.82305 avg_loss = 3.37191\n",
      "epoch no.3 train no.288990  loss = 2.78521 avg_loss = 3.35250\n",
      "epoch no.3 train no.289000  loss = 2.49908 avg_loss = 3.37766\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '팝', '</s>']\n",
      "추억의 2000년대 댄스곡</s>\n",
      "epoch no.3 train no.289010  loss = 3.19015 avg_loss = 3.38215\n",
      "epoch no.3 train no.289020  loss = 3.93331 avg_loss = 3.37459\n",
      "epoch no.3 train no.289030  loss = 4.13650 avg_loss = 3.40239\n",
      "epoch no.3 train no.289040  loss = 3.87536 avg_loss = 3.37071\n",
      "epoch no.3 train no.289050  loss = 3.37644 avg_loss = 3.37812\n",
      "epoch no.3 train no.289060  loss = 5.01877 avg_loss = 3.40256\n",
      "epoch no.3 train no.289070  loss = 1.77725 avg_loss = 3.36234\n",
      "epoch no.3 train no.289080  loss = 3.24838 avg_loss = 3.33848\n",
      "epoch no.3 train no.289090  loss = 3.41999 avg_loss = 3.32310\n",
      "epoch no.3 train no.289100  loss = 3.80084 avg_loss = 3.29961\n",
      "epoch no.3 train no.289110  loss = 2.69626 avg_loss = 3.28278\n",
      "epoch no.3 train no.289120  loss = 2.21583 avg_loss = 3.24020\n",
      "epoch no.3 train no.289130  loss = 3.85553 avg_loss = 3.24386\n",
      "epoch no.3 train no.289140  loss = 2.50012 avg_loss = 3.25287\n",
      "epoch no.3 train no.289150  loss = 4.59947 avg_loss = 3.31413\n",
      "epoch no.3 train no.289160  loss = 2.94628 avg_loss = 3.28508\n",
      "epoch no.3 train no.289170  loss = 3.12034 avg_loss = 3.28308\n",
      "epoch no.3 train no.289180  loss = 4.82821 avg_loss = 3.27208\n",
      "epoch no.3 train no.289190  loss = 4.84243 avg_loss = 3.31810\n",
      "epoch no.3 train no.289200  loss = 2.94702 avg_loss = 3.30886\n",
      "epoch no.3 train no.289210  loss = 3.56871 avg_loss = 3.34915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.289220  loss = 2.80624 avg_loss = 3.34162\n",
      "epoch no.3 train no.289230  loss = 4.54103 avg_loss = 3.37199\n",
      "epoch no.3 train no.289240  loss = 4.37961 avg_loss = 3.40525\n",
      "epoch no.3 train no.289250  loss = 2.98139 avg_loss = 3.32577\n",
      "epoch no.3 train no.289260  loss = 3.79216 avg_loss = 3.31346\n",
      "epoch no.3 train no.289270  loss = 3.24953 avg_loss = 3.30861\n",
      "epoch no.3 train no.289280  loss = 2.15821 avg_loss = 3.36630\n",
      "epoch no.3 train no.289290  loss = 2.59160 avg_loss = 3.36907\n",
      "epoch no.3 train no.289300  loss = 2.91701 avg_loss = 3.35695\n",
      "epoch no.3 train no.289310  loss = 3.09617 avg_loss = 3.38165\n",
      "epoch no.3 train no.289320  loss = 3.12995 avg_loss = 3.36512\n",
      "epoch no.3 train no.289330  loss = 2.16817 avg_loss = 3.32682\n",
      "epoch no.3 train no.289340  loss = 3.16899 avg_loss = 3.29944\n",
      "epoch no.3 train no.289350  loss = 5.30902 avg_loss = 3.27434\n",
      "epoch no.3 train no.289360  loss = 2.01053 avg_loss = 3.32372\n",
      "epoch no.3 train no.289370  loss = 1.96103 avg_loss = 3.31983\n",
      "epoch no.3 train no.289380  loss = 3.60213 avg_loss = 3.30998\n",
      "epoch no.3 train no.289390  loss = 2.37356 avg_loss = 3.29026\n",
      "epoch no.3 train no.289400  loss = 4.59793 avg_loss = 3.29932\n",
      "epoch no.3 train no.289410  loss = 3.37174 avg_loss = 3.23081\n",
      "epoch no.3 train no.289420  loss = 3.18174 avg_loss = 3.19360\n",
      "epoch no.3 train no.289430  loss = 3.99219 avg_loss = 3.19028\n",
      "epoch no.3 train no.289440  loss = 4.05689 avg_loss = 3.23816\n",
      "epoch no.3 train no.289450  loss = 1.30568 avg_loss = 3.21892\n",
      "epoch no.3 train no.289460  loss = 3.98024 avg_loss = 3.23750\n",
      "epoch no.3 train no.289470  loss = 3.74299 avg_loss = 3.28191\n",
      "epoch no.3 train no.289480  loss = 2.54240 avg_loss = 3.30733\n",
      "epoch no.3 train no.289490  loss = 3.10276 avg_loss = 3.28596\n",
      "epoch no.3 train no.289500  loss = 4.57688 avg_loss = 3.34327\n",
      "epoch no.3 train no.289510  loss = 3.48785 avg_loss = 3.37854\n",
      "epoch no.3 train no.289520  loss = 3.77069 avg_loss = 3.37137\n",
      "epoch no.3 train no.289530  loss = 2.49811 avg_loss = 3.35162\n",
      "epoch no.3 train no.289540  loss = 3.00975 avg_loss = 3.33895\n",
      "epoch no.3 train no.289550  loss = 3.61522 avg_loss = 3.30530\n",
      "epoch no.3 train no.289560  loss = 4.45289 avg_loss = 3.30304\n",
      "epoch no.3 train no.289570  loss = 4.80485 avg_loss = 3.26618\n",
      "epoch no.3 train no.289580  loss = 4.01622 avg_loss = 3.30492\n",
      "epoch no.3 train no.289590  loss = 3.81900 avg_loss = 3.31965\n",
      "epoch no.3 train no.289600  loss = 4.54994 avg_loss = 3.30048\n",
      "epoch no.3 train no.289610  loss = 1.77138 avg_loss = 3.26454\n",
      "epoch no.3 train no.289620  loss = 2.65838 avg_loss = 3.25584\n",
      "epoch no.3 train no.289630  loss = 1.12027 avg_loss = 3.22738\n",
      "epoch no.3 train no.289640  loss = 2.46388 avg_loss = 3.23156\n",
      "epoch no.3 train no.289650  loss = 2.54866 avg_loss = 3.26634\n",
      "epoch no.3 train no.289660  loss = 1.86199 avg_loss = 3.32773\n",
      "epoch no.3 train no.289670  loss = 3.67220 avg_loss = 3.35025\n",
      "epoch no.3 train no.289680  loss = 3.11055 avg_loss = 3.34705\n",
      "epoch no.3 train no.289690  loss = 1.92849 avg_loss = 3.35380\n",
      "epoch no.3 train no.289700  loss = 3.85716 avg_loss = 3.38579\n",
      "epoch no.3 train no.289710  loss = 3.43888 avg_loss = 3.42257\n",
      "epoch no.3 train no.289720  loss = 3.24009 avg_loss = 3.40861\n",
      "epoch no.3 train no.289730  loss = 3.54234 avg_loss = 3.36735\n",
      "epoch no.3 train no.289740  loss = 2.41844 avg_loss = 3.37968\n",
      "epoch no.3 train no.289750  loss = 3.20463 avg_loss = 3.34043\n",
      "epoch no.3 train no.289760  loss = 2.53970 avg_loss = 3.36368\n",
      "epoch no.3 train no.289770  loss = 3.30234 avg_loss = 3.34655\n",
      "epoch no.3 train no.289780  loss = 2.10638 avg_loss = 3.41011\n",
      "epoch no.3 train no.289790  loss = 2.39962 avg_loss = 3.40655\n",
      "epoch no.3 train no.289800  loss = 3.59141 avg_loss = 3.41219\n",
      "epoch no.3 train no.289810  loss = 2.27360 avg_loss = 3.37783\n",
      "epoch no.3 train no.289820  loss = 2.23450 avg_loss = 3.34692\n",
      "epoch no.3 train no.289830  loss = 4.26056 avg_loss = 3.36701\n",
      "epoch no.3 train no.289840  loss = 3.31306 avg_loss = 3.38019\n",
      "epoch no.3 train no.289850  loss = 2.98543 avg_loss = 3.38492\n",
      "epoch no.3 train no.289860  loss = 1.69204 avg_loss = 3.36953\n",
      "epoch no.3 train no.289870  loss = 2.87562 avg_loss = 3.38265\n",
      "epoch no.3 train no.289880  loss = 2.46568 avg_loss = 3.38366\n",
      "epoch no.3 train no.289890  loss = 4.22450 avg_loss = 3.38791\n",
      "epoch no.3 train no.289900  loss = 2.40224 avg_loss = 3.38283\n",
      "epoch no.3 train no.289910  loss = 3.25627 avg_loss = 3.34913\n",
      "epoch no.3 train no.289920  loss = 3.98129 avg_loss = 3.37624\n",
      "epoch no.3 train no.289930  loss = 2.91688 avg_loss = 3.42213\n",
      "epoch no.3 train no.289940  loss = 2.85474 avg_loss = 3.39973\n",
      "epoch no.3 train no.289950  loss = 4.13946 avg_loss = 3.38319\n",
      "epoch no.3 train no.289960  loss = 3.44548 avg_loss = 3.34891\n",
      "epoch no.3 train no.289970  loss = 2.51891 avg_loss = 3.33036\n",
      "epoch no.3 train no.289980  loss = 2.70973 avg_loss = 3.34416\n",
      "epoch no.3 train no.289990  loss = 4.83911 avg_loss = 3.36008\n",
      "epoch no.3 train no.290000  loss = 4.89823 avg_loss = 3.38553\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '곡', '</s>', '</s>']\n",
      "추억의 90년대 댄스팝송</s>\n",
      "epoch no.3 train no.290010  loss = 2.59586 avg_loss = 3.41969\n",
      "epoch no.3 train no.290020  loss = 5.31346 avg_loss = 3.43122\n",
      "epoch no.3 train no.290030  loss = 2.15503 avg_loss = 3.42229\n",
      "epoch no.3 train no.290040  loss = 3.66940 avg_loss = 3.42388\n",
      "epoch no.3 train no.290050  loss = 4.22998 avg_loss = 3.38575\n",
      "epoch no.3 train no.290060  loss = 3.15563 avg_loss = 3.33026\n",
      "epoch no.3 train no.290070  loss = 3.79009 avg_loss = 3.32457\n",
      "epoch no.3 train no.290080  loss = 3.03956 avg_loss = 3.28112\n",
      "epoch no.3 train no.290090  loss = 2.95815 avg_loss = 3.35717\n",
      "epoch no.3 train no.290100  loss = 3.07021 avg_loss = 3.37307\n",
      "epoch no.3 train no.290110  loss = 4.24790 avg_loss = 3.43450\n",
      "epoch no.3 train no.290120  loss = 2.36123 avg_loss = 3.44017\n",
      "epoch no.3 train no.290130  loss = 2.50616 avg_loss = 3.42752\n",
      "epoch no.3 train no.290140  loss = 1.75333 avg_loss = 3.39342\n",
      "epoch no.3 train no.290150  loss = 3.46217 avg_loss = 3.35899\n",
      "epoch no.3 train no.290160  loss = 3.07807 avg_loss = 3.35721\n",
      "epoch no.3 train no.290170  loss = 2.34332 avg_loss = 3.34163\n",
      "epoch no.3 train no.290180  loss = 4.92936 avg_loss = 3.36722\n",
      "epoch no.3 train no.290190  loss = 2.96986 avg_loss = 3.32177\n",
      "epoch no.3 train no.290200  loss = 4.16467 avg_loss = 3.28451\n",
      "epoch no.3 train no.290210  loss = 2.22444 avg_loss = 3.29395\n",
      "epoch no.3 train no.290220  loss = 5.01681 avg_loss = 3.30161\n",
      "epoch no.3 train no.290230  loss = 4.23563 avg_loss = 3.33421\n",
      "epoch no.3 train no.290240  loss = 1.37357 avg_loss = 3.31302\n",
      "epoch no.3 train no.290250  loss = 2.94322 avg_loss = 3.32497\n",
      "epoch no.3 train no.290260  loss = 2.86278 avg_loss = 3.32189\n",
      "epoch no.3 train no.290270  loss = 2.20080 avg_loss = 3.30848\n",
      "epoch no.3 train no.290280  loss = 3.88843 avg_loss = 3.26758\n",
      "epoch no.3 train no.290290  loss = 2.98937 avg_loss = 3.29714\n",
      "epoch no.3 train no.290300  loss = 3.29866 avg_loss = 3.32315\n",
      "epoch no.3 train no.290310  loss = 3.97210 avg_loss = 3.34385\n",
      "epoch no.3 train no.290320  loss = 5.09466 avg_loss = 3.36701\n",
      "epoch no.3 train no.290330  loss = 3.03055 avg_loss = 3.36646\n",
      "epoch no.3 train no.290340  loss = 2.68118 avg_loss = 3.31771\n",
      "epoch no.3 train no.290350  loss = 2.77225 avg_loss = 3.31203\n",
      "epoch no.3 train no.290360  loss = 3.35208 avg_loss = 3.27326\n",
      "epoch no.3 train no.290370  loss = 2.28990 avg_loss = 3.25795\n",
      "epoch no.3 train no.290380  loss = 2.02902 avg_loss = 3.27225\n",
      "epoch no.3 train no.290390  loss = 3.02141 avg_loss = 3.28006\n",
      "epoch no.3 train no.290400  loss = 3.38841 avg_loss = 3.34895\n",
      "epoch no.3 train no.290410  loss = 2.58529 avg_loss = 3.30271\n",
      "epoch no.3 train no.290420  loss = 4.70196 avg_loss = 3.38439\n",
      "epoch no.3 train no.290430  loss = 3.61266 avg_loss = 3.41370\n",
      "epoch no.3 train no.290440  loss = 5.01604 avg_loss = 3.43738\n",
      "epoch no.3 train no.290450  loss = 2.75834 avg_loss = 3.38732\n",
      "epoch no.3 train no.290460  loss = 2.55157 avg_loss = 3.39601\n",
      "epoch no.3 train no.290470  loss = 3.18625 avg_loss = 3.38706\n",
      "epoch no.3 train no.290480  loss = 1.60686 avg_loss = 3.36694\n",
      "epoch no.3 train no.290490  loss = 5.69234 avg_loss = 3.37529\n",
      "epoch no.3 train no.290500  loss = 4.15680 avg_loss = 3.36760\n",
      "epoch no.3 train no.290510  loss = 4.09563 avg_loss = 3.35196\n",
      "epoch no.3 train no.290520  loss = 4.49017 avg_loss = 3.33767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.290530  loss = 2.91421 avg_loss = 3.29270\n",
      "epoch no.3 train no.290540  loss = 2.51069 avg_loss = 3.24303\n",
      "epoch no.3 train no.290550  loss = 4.94952 avg_loss = 3.31316\n",
      "epoch no.3 train no.290560  loss = 3.78954 avg_loss = 3.31179\n",
      "epoch no.3 train no.290570  loss = 2.35630 avg_loss = 3.28842\n",
      "epoch no.3 train no.290580  loss = 3.52580 avg_loss = 3.33247\n",
      "epoch no.3 train no.290590  loss = 4.42390 avg_loss = 3.35110\n",
      "epoch no.3 train no.290600  loss = 2.07550 avg_loss = 3.33287\n",
      "epoch no.3 train no.290610  loss = 3.94351 avg_loss = 3.34629\n",
      "epoch no.3 train no.290620  loss = 2.88874 avg_loss = 3.37951\n",
      "epoch no.3 train no.290630  loss = 3.99972 avg_loss = 3.36057\n",
      "epoch no.3 train no.290640  loss = 3.01270 avg_loss = 3.35020\n",
      "epoch no.3 train no.290650  loss = 2.42036 avg_loss = 3.32903\n",
      "epoch no.3 train no.290660  loss = 2.72372 avg_loss = 3.33413\n",
      "epoch no.3 train no.290670  loss = 2.91745 avg_loss = 3.31922\n",
      "epoch no.3 train no.290680  loss = 2.22943 avg_loss = 3.30502\n",
      "epoch no.3 train no.290690  loss = 2.19688 avg_loss = 3.35600\n",
      "epoch no.3 train no.290700  loss = 2.98937 avg_loss = 3.35701\n",
      "epoch no.3 train no.290710  loss = 5.05796 avg_loss = 3.41817\n",
      "epoch no.3 train no.290720  loss = 4.44989 avg_loss = 3.43441\n",
      "epoch no.3 train no.290730  loss = 2.52971 avg_loss = 3.42723\n",
      "epoch no.3 train no.290740  loss = 2.67436 avg_loss = 3.40085\n",
      "epoch no.3 train no.290750  loss = 2.95150 avg_loss = 3.42388\n",
      "epoch no.3 train no.290760  loss = 3.93848 avg_loss = 3.42721\n",
      "epoch no.3 train no.290770  loss = 3.75255 avg_loss = 3.45490\n",
      "epoch no.3 train no.290780  loss = 2.65029 avg_loss = 3.44418\n",
      "epoch no.3 train no.290790  loss = 3.52500 avg_loss = 3.43546\n",
      "epoch no.3 train no.290800  loss = 2.58863 avg_loss = 3.44890\n",
      "epoch no.3 train no.290810  loss = 2.95004 avg_loss = 3.43258\n",
      "epoch no.3 train no.290820  loss = 4.51272 avg_loss = 3.45027\n",
      "epoch no.3 train no.290830  loss = 3.93821 avg_loss = 3.50751\n",
      "epoch no.3 train no.290840  loss = 3.22031 avg_loss = 3.47848\n",
      "epoch no.3 train no.290850  loss = 2.34762 avg_loss = 3.43118\n",
      "epoch no.3 train no.290860  loss = 3.63037 avg_loss = 3.45125\n",
      "epoch no.3 train no.290870  loss = 3.32458 avg_loss = 3.49071\n",
      "epoch no.3 train no.290880  loss = 3.62230 avg_loss = 3.47917\n",
      "epoch no.3 train no.290890  loss = 2.96508 avg_loss = 3.43745\n",
      "epoch no.3 train no.290900  loss = 3.04330 avg_loss = 3.41583\n",
      "epoch no.3 train no.290910  loss = 3.21977 avg_loss = 3.39021\n",
      "epoch no.3 train no.290920  loss = 3.74026 avg_loss = 3.36276\n",
      "epoch no.3 train no.290930  loss = 3.79746 avg_loss = 3.39064\n",
      "epoch no.3 train no.290940  loss = 3.70845 avg_loss = 3.43185\n",
      "epoch no.3 train no.290950  loss = 3.01658 avg_loss = 3.40754\n",
      "epoch no.3 train no.290960  loss = 3.06478 avg_loss = 3.36705\n",
      "epoch no.3 train no.290970  loss = 2.95273 avg_loss = 3.33377\n",
      "epoch no.3 train no.290980  loss = 2.93877 avg_loss = 3.28019\n",
      "epoch no.3 train no.290990  loss = 3.81804 avg_loss = 3.29886\n",
      "epoch no.3 train no.291000  loss = 6.16950 avg_loss = 3.33590\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.291010  loss = 2.24506 avg_loss = 3.33111\n",
      "epoch no.3 train no.291020  loss = 2.29260 avg_loss = 3.33841\n",
      "epoch no.3 train no.291030  loss = 2.29936 avg_loss = 3.31876\n",
      "epoch no.3 train no.291040  loss = 2.63587 avg_loss = 3.32344\n",
      "epoch no.3 train no.291050  loss = 4.20831 avg_loss = 3.34932\n",
      "epoch no.3 train no.291060  loss = 7.38777 avg_loss = 3.37471\n",
      "epoch no.3 train no.291070  loss = 3.26661 avg_loss = 3.32390\n",
      "epoch no.3 train no.291080  loss = 3.85751 avg_loss = 3.35025\n",
      "epoch no.3 train no.291090  loss = 5.38197 avg_loss = 3.35892\n",
      "epoch no.3 train no.291100  loss = 2.09840 avg_loss = 3.35833\n",
      "epoch no.3 train no.291110  loss = 4.20536 avg_loss = 3.34594\n",
      "epoch no.3 train no.291120  loss = 3.13500 avg_loss = 3.32852\n",
      "epoch no.3 train no.291130  loss = 3.45374 avg_loss = 3.34592\n",
      "epoch no.3 train no.291140  loss = 2.84155 avg_loss = 3.35973\n",
      "epoch no.3 train no.291150  loss = 5.12694 avg_loss = 3.37608\n",
      "epoch no.3 train no.291160  loss = 5.43494 avg_loss = 3.45564\n",
      "epoch no.3 train no.291170  loss = 3.15574 avg_loss = 3.47142\n",
      "epoch no.3 train no.291180  loss = 2.84193 avg_loss = 3.43547\n",
      "epoch no.3 train no.291190  loss = 2.82918 avg_loss = 3.40017\n",
      "epoch no.3 train no.291200  loss = 3.08557 avg_loss = 3.34516\n",
      "epoch no.3 train no.291210  loss = 2.38331 avg_loss = 3.33913\n",
      "epoch no.3 train no.291220  loss = 2.94914 avg_loss = 3.32195\n",
      "epoch no.3 train no.291230  loss = 2.89685 avg_loss = 3.37101\n",
      "epoch no.3 train no.291240  loss = 2.08744 avg_loss = 3.34176\n",
      "epoch no.3 train no.291250  loss = 2.72406 avg_loss = 3.29230\n",
      "epoch no.3 train no.291260  loss = 2.12245 avg_loss = 3.36847\n",
      "epoch no.3 train no.291270  loss = 2.42685 avg_loss = 3.32693\n",
      "epoch no.3 train no.291280  loss = 2.39870 avg_loss = 3.32066\n",
      "epoch no.3 train no.291290  loss = 4.33187 avg_loss = 3.34838\n",
      "epoch no.3 train no.291300  loss = 3.93813 avg_loss = 3.31109\n",
      "epoch no.3 train no.291310  loss = 2.92278 avg_loss = 3.33324\n",
      "epoch no.3 train no.291320  loss = 2.07111 avg_loss = 3.33295\n",
      "epoch no.3 train no.291330  loss = 3.04108 avg_loss = 3.35039\n",
      "epoch no.3 train no.291340  loss = 4.13199 avg_loss = 3.37360\n",
      "epoch no.3 train no.291350  loss = 5.01294 avg_loss = 3.39226\n",
      "epoch no.3 train no.291360  loss = 3.15242 avg_loss = 3.38618\n",
      "epoch no.3 train no.291370  loss = 3.95320 avg_loss = 3.35598\n",
      "epoch no.3 train no.291380  loss = 1.78386 avg_loss = 3.35706\n",
      "epoch no.3 train no.291390  loss = 5.54019 avg_loss = 3.42627\n",
      "epoch no.3 train no.291400  loss = 1.99536 avg_loss = 3.34959\n",
      "epoch no.3 train no.291410  loss = 5.93126 avg_loss = 3.36338\n",
      "epoch no.3 train no.291420  loss = 2.51749 avg_loss = 3.38305\n",
      "epoch no.3 train no.291430  loss = 4.27053 avg_loss = 3.44658\n",
      "epoch no.3 train no.291440  loss = 4.52710 avg_loss = 3.47812\n",
      "epoch no.3 train no.291450  loss = 4.80283 avg_loss = 3.48381\n",
      "epoch no.3 train no.291460  loss = 2.36069 avg_loss = 3.46656\n",
      "epoch no.3 train no.291470  loss = 3.75054 avg_loss = 3.44181\n",
      "epoch no.3 train no.291480  loss = 3.39392 avg_loss = 3.45836\n",
      "epoch no.3 train no.291490  loss = 4.43246 avg_loss = 3.47906\n",
      "epoch no.3 train no.291500  loss = 3.28019 avg_loss = 3.42897\n",
      "epoch no.3 train no.291510  loss = 3.57411 avg_loss = 3.42776\n",
      "epoch no.3 train no.291520  loss = 2.40041 avg_loss = 3.38599\n",
      "epoch no.3 train no.291530  loss = 3.70254 avg_loss = 3.42577\n",
      "epoch no.3 train no.291540  loss = 2.44305 avg_loss = 3.36805\n",
      "epoch no.3 train no.291550  loss = 3.39060 avg_loss = 3.38855\n",
      "epoch no.3 train no.291560  loss = 4.74118 avg_loss = 3.36145\n",
      "epoch no.3 train no.291570  loss = 5.98253 avg_loss = 3.36619\n",
      "epoch no.3 train no.291580  loss = 2.67717 avg_loss = 3.36254\n",
      "epoch no.3 train no.291590  loss = 4.46382 avg_loss = 3.37188\n",
      "epoch no.3 train no.291600  loss = 4.34967 avg_loss = 3.36798\n",
      "epoch no.3 train no.291610  loss = 4.51136 avg_loss = 3.34950\n",
      "epoch no.3 train no.291620  loss = 2.28773 avg_loss = 3.36791\n",
      "epoch no.3 train no.291630  loss = 3.58921 avg_loss = 3.34451\n",
      "epoch no.3 train no.291640  loss = 3.58803 avg_loss = 3.39082\n",
      "epoch no.3 train no.291650  loss = 3.36321 avg_loss = 3.41370\n",
      "epoch no.3 train no.291660  loss = 3.58275 avg_loss = 3.37980\n",
      "epoch no.3 train no.291670  loss = 3.40758 avg_loss = 3.33396\n",
      "epoch no.3 train no.291680  loss = 4.23022 avg_loss = 3.36503\n",
      "epoch no.3 train no.291690  loss = 2.36433 avg_loss = 3.36503\n",
      "epoch no.3 train no.291700  loss = 3.78185 avg_loss = 3.36025\n",
      "epoch no.3 train no.291710  loss = 3.77251 avg_loss = 3.38944\n",
      "epoch no.3 train no.291720  loss = 3.29516 avg_loss = 3.39881\n",
      "epoch no.3 train no.291730  loss = 2.65885 avg_loss = 3.38837\n",
      "epoch no.3 train no.291740  loss = 2.73393 avg_loss = 3.38084\n",
      "epoch no.3 train no.291750  loss = 2.46901 avg_loss = 3.35633\n",
      "epoch no.3 train no.291760  loss = 3.12116 avg_loss = 3.34122\n",
      "epoch no.3 train no.291770  loss = 4.17554 avg_loss = 3.34632\n",
      "epoch no.3 train no.291780  loss = 3.32205 avg_loss = 3.36479\n",
      "epoch no.3 train no.291790  loss = 1.70092 avg_loss = 3.29834\n",
      "epoch no.3 train no.291800  loss = 4.48278 avg_loss = 3.34053\n",
      "epoch no.3 train no.291810  loss = 3.41755 avg_loss = 3.35213\n",
      "epoch no.3 train no.291820  loss = 2.26447 avg_loss = 3.35457\n",
      "epoch no.3 train no.291830  loss = 4.51255 avg_loss = 3.42260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.291840  loss = 3.53389 avg_loss = 3.41471\n",
      "epoch no.3 train no.291850  loss = 3.35411 avg_loss = 3.38990\n",
      "epoch no.3 train no.291860  loss = 4.43923 avg_loss = 3.40515\n",
      "epoch no.3 train no.291870  loss = 3.65371 avg_loss = 3.42921\n",
      "epoch no.3 train no.291880  loss = 4.82489 avg_loss = 3.45339\n",
      "epoch no.3 train no.291890  loss = 2.45666 avg_loss = 3.43865\n",
      "epoch no.3 train no.291900  loss = 4.40040 avg_loss = 3.44815\n",
      "epoch no.3 train no.291910  loss = 3.58084 avg_loss = 3.44324\n",
      "epoch no.3 train no.291920  loss = 2.27056 avg_loss = 3.41237\n",
      "epoch no.3 train no.291930  loss = 4.38465 avg_loss = 3.43113\n",
      "epoch no.3 train no.291940  loss = 2.38352 avg_loss = 3.44867\n",
      "epoch no.3 train no.291950  loss = 3.22388 avg_loss = 3.44730\n",
      "epoch no.3 train no.291960  loss = 3.00675 avg_loss = 3.44009\n",
      "epoch no.3 train no.291970  loss = 3.81672 avg_loss = 3.50163\n",
      "epoch no.3 train no.291980  loss = 3.83578 avg_loss = 3.46768\n",
      "epoch no.3 train no.291990  loss = 4.77290 avg_loss = 3.44633\n",
      "epoch no.3 train no.292000  loss = 3.43341 avg_loss = 3.44834\n",
      "4\n",
      "to_tokens: ['▁가을', '▁명', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.3 train no.292010  loss = 1.96015 avg_loss = 3.43586\n",
      "epoch no.3 train no.292020  loss = 2.23436 avg_loss = 3.42555\n",
      "epoch no.3 train no.292030  loss = 3.53890 avg_loss = 3.46663\n",
      "epoch no.3 train no.292040  loss = 4.92997 avg_loss = 3.45857\n",
      "epoch no.3 train no.292050  loss = 4.15847 avg_loss = 3.44183\n",
      "epoch no.3 train no.292060  loss = 3.63505 avg_loss = 3.44052\n",
      "epoch no.3 train no.292070  loss = 5.36930 avg_loss = 3.43816\n",
      "epoch no.3 train no.292080  loss = 2.19289 avg_loss = 3.39020\n",
      "epoch no.3 train no.292090  loss = 3.74763 avg_loss = 3.41509\n",
      "epoch no.3 train no.292100  loss = 3.52509 avg_loss = 3.47039\n",
      "epoch no.3 train no.292110  loss = 1.61209 avg_loss = 3.42912\n",
      "epoch no.3 train no.292120  loss = 3.69149 avg_loss = 3.44846\n",
      "epoch no.3 train no.292130  loss = 4.19364 avg_loss = 3.47768\n",
      "epoch no.3 train no.292140  loss = 2.68558 avg_loss = 3.47791\n",
      "epoch no.3 train no.292150  loss = 3.72063 avg_loss = 3.47802\n",
      "epoch no.3 train no.292160  loss = 3.18102 avg_loss = 3.46419\n",
      "epoch no.3 train no.292170  loss = 4.69746 avg_loss = 3.43819\n",
      "epoch no.3 train no.292180  loss = 2.48150 avg_loss = 3.40794\n",
      "epoch no.3 train no.292190  loss = 3.05953 avg_loss = 3.37330\n",
      "epoch no.3 train no.292200  loss = 2.63889 avg_loss = 3.35388\n",
      "epoch no.3 train no.292210  loss = 4.09333 avg_loss = 3.30457\n",
      "epoch no.3 train no.292220  loss = 3.61057 avg_loss = 3.30756\n",
      "epoch no.3 train no.292230  loss = 3.70619 avg_loss = 3.34569\n",
      "epoch no.3 train no.292240  loss = 2.46312 avg_loss = 3.34532\n",
      "epoch no.3 train no.292250  loss = 1.90511 avg_loss = 3.27486\n",
      "epoch no.3 train no.292260  loss = 4.13987 avg_loss = 3.31550\n",
      "epoch no.3 train no.292270  loss = 3.67132 avg_loss = 3.28814\n",
      "epoch no.3 train no.292280  loss = 4.30647 avg_loss = 3.34638\n",
      "epoch no.3 train no.292290  loss = 3.31441 avg_loss = 3.31084\n",
      "epoch no.3 train no.292300  loss = 2.87657 avg_loss = 3.29383\n",
      "epoch no.3 train no.292310  loss = 2.67075 avg_loss = 3.34776\n",
      "epoch no.3 train no.292320  loss = 1.94616 avg_loss = 3.35604\n",
      "epoch no.3 train no.292330  loss = 3.56105 avg_loss = 3.37472\n",
      "epoch no.3 train no.292340  loss = 5.61096 avg_loss = 3.42875\n",
      "epoch no.3 train no.292350  loss = 2.79577 avg_loss = 3.39825\n",
      "epoch no.3 train no.292360  loss = 4.50975 avg_loss = 3.43065\n",
      "epoch no.3 train no.292370  loss = 2.36816 avg_loss = 3.43069\n",
      "epoch no.3 train no.292380  loss = 2.44339 avg_loss = 3.40850\n",
      "epoch no.3 train no.292390  loss = 4.10393 avg_loss = 3.40861\n",
      "epoch no.3 train no.292400  loss = 4.68275 avg_loss = 3.39928\n",
      "epoch no.3 train no.292410  loss = 5.15615 avg_loss = 3.41138\n",
      "epoch no.3 train no.292420  loss = 3.12076 avg_loss = 3.40029\n",
      "epoch no.3 train no.292430  loss = 2.67713 avg_loss = 3.36009\n",
      "epoch no.3 train no.292440  loss = 2.92286 avg_loss = 3.36673\n",
      "epoch no.3 train no.292450  loss = 3.30038 avg_loss = 3.36640\n",
      "epoch no.3 train no.292460  loss = 1.52288 avg_loss = 3.29908\n",
      "epoch no.3 train no.292470  loss = 5.17814 avg_loss = 3.32992\n",
      "epoch no.3 train no.292480  loss = 2.53440 avg_loss = 3.33403\n",
      "epoch no.3 train no.292490  loss = 5.55424 avg_loss = 3.34433\n",
      "epoch no.3 train no.292500  loss = 4.11309 avg_loss = 3.35795\n",
      "epoch no.3 train no.292510  loss = 1.72566 avg_loss = 3.31952\n",
      "epoch no.3 train no.292520  loss = 2.58350 avg_loss = 3.31199\n",
      "epoch no.3 train no.292530  loss = 3.47818 avg_loss = 3.32061\n",
      "epoch no.3 train no.292540  loss = 2.82207 avg_loss = 3.32200\n",
      "epoch no.3 train no.292550  loss = 2.25140 avg_loss = 3.29790\n",
      "epoch no.3 train no.292560  loss = 2.22407 avg_loss = 3.28691\n",
      "epoch no.3 train no.292570  loss = 3.25466 avg_loss = 3.29416\n",
      "epoch no.3 train no.292580  loss = 4.07285 avg_loss = 3.30293\n",
      "epoch no.3 train no.292590  loss = 3.60526 avg_loss = 3.29278\n",
      "epoch no.3 train no.292600  loss = 3.61242 avg_loss = 3.30063\n",
      "epoch no.3 train no.292610  loss = 3.71727 avg_loss = 3.30874\n",
      "epoch no.3 train no.292620  loss = 3.20925 avg_loss = 3.31467\n",
      "epoch no.3 train no.292630  loss = 2.78933 avg_loss = 3.33539\n",
      "epoch no.3 train no.292640  loss = 4.67171 avg_loss = 3.34624\n",
      "epoch no.3 train no.292650  loss = 2.20035 avg_loss = 3.32306\n",
      "epoch no.3 train no.292660  loss = 5.70465 avg_loss = 3.32855\n",
      "epoch no.3 train no.292670  loss = 2.84088 avg_loss = 3.34189\n",
      "epoch no.3 train no.292680  loss = 3.48126 avg_loss = 3.30001\n",
      "epoch no.3 train no.292690  loss = 2.29756 avg_loss = 3.25789\n",
      "epoch no.3 train no.292700  loss = 2.55019 avg_loss = 3.31892\n",
      "epoch no.3 train no.292710  loss = 5.24515 avg_loss = 3.33768\n",
      "epoch no.3 train no.292720  loss = 3.11301 avg_loss = 3.36107\n",
      "epoch no.3 train no.292730  loss = 3.27227 avg_loss = 3.36240\n",
      "epoch no.3 train no.292740  loss = 3.89909 avg_loss = 3.35910\n",
      "epoch no.3 train no.292750  loss = 4.46330 avg_loss = 3.31671\n",
      "epoch no.3 train no.292760  loss = 3.32530 avg_loss = 3.27388\n",
      "epoch no.3 train no.292770  loss = 3.83939 avg_loss = 3.31572\n",
      "epoch no.3 train no.292780  loss = 2.42083 avg_loss = 3.28725\n",
      "epoch no.3 train no.292790  loss = 3.43953 avg_loss = 3.24938\n",
      "epoch no.3 train no.292800  loss = 2.03767 avg_loss = 3.25448\n",
      "epoch no.3 train no.292810  loss = 4.16394 avg_loss = 3.34062\n",
      "epoch no.3 train no.292820  loss = 2.82945 avg_loss = 3.34407\n",
      "epoch no.3 train no.292830  loss = 3.96667 avg_loss = 3.36464\n",
      "epoch no.3 train no.292840  loss = 2.70367 avg_loss = 3.39933\n",
      "epoch no.3 train no.292850  loss = 4.62590 avg_loss = 3.41454\n",
      "epoch no.3 train no.292860  loss = 2.57172 avg_loss = 3.42536\n",
      "epoch no.3 train no.292870  loss = 3.17703 avg_loss = 3.37612\n",
      "epoch no.3 train no.292880  loss = 3.14427 avg_loss = 3.38759\n",
      "epoch no.3 train no.292890  loss = 3.80674 avg_loss = 3.46026\n",
      "epoch no.3 train no.292900  loss = 5.42007 avg_loss = 3.46740\n",
      "epoch no.3 train no.292910  loss = 3.93572 avg_loss = 3.45943\n",
      "epoch no.3 train no.292920  loss = 2.58605 avg_loss = 3.43986\n",
      "epoch no.3 train no.292930  loss = 2.09592 avg_loss = 3.41831\n",
      "epoch no.3 train no.292940  loss = 5.96902 avg_loss = 3.40246\n",
      "epoch no.3 train no.292950  loss = 3.39230 avg_loss = 3.40414\n",
      "epoch no.3 train no.292960  loss = 3.76140 avg_loss = 3.37456\n",
      "epoch no.3 train no.292970  loss = 3.96851 avg_loss = 3.38387\n",
      "epoch no.3 train no.292980  loss = 4.11016 avg_loss = 3.40690\n",
      "epoch no.3 train no.292990  loss = 3.39503 avg_loss = 3.44715\n",
      "epoch no.3 train no.293000  loss = 4.04338 avg_loss = 3.41074\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.3 train no.293010  loss = 2.50244 avg_loss = 3.37183\n",
      "epoch no.3 train no.293020  loss = 3.13034 avg_loss = 3.42855\n",
      "epoch no.3 train no.293030  loss = 3.32650 avg_loss = 3.41342\n",
      "epoch no.3 train no.293040  loss = 2.33116 avg_loss = 3.38804\n",
      "epoch no.3 train no.293050  loss = 2.94472 avg_loss = 3.40738\n",
      "epoch no.3 train no.293060  loss = 3.12971 avg_loss = 3.39969\n",
      "epoch no.3 train no.293070  loss = 3.26836 avg_loss = 3.38372\n",
      "epoch no.3 train no.293080  loss = 2.83287 avg_loss = 3.35980\n",
      "epoch no.3 train no.293090  loss = 4.73089 avg_loss = 3.36620\n",
      "epoch no.3 train no.293100  loss = 2.23148 avg_loss = 3.35460\n",
      "epoch no.3 train no.293110  loss = 5.04678 avg_loss = 3.41054\n",
      "epoch no.3 train no.293120  loss = 3.28332 avg_loss = 3.38788\n",
      "epoch no.3 train no.293130  loss = 3.96502 avg_loss = 3.37607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.293140  loss = 3.76209 avg_loss = 3.35414\n",
      "epoch no.3 train no.293150  loss = 4.60468 avg_loss = 3.36025\n",
      "epoch no.3 train no.293160  loss = 2.72791 avg_loss = 3.38646\n",
      "epoch no.3 train no.293170  loss = 2.95390 avg_loss = 3.36807\n",
      "epoch no.3 train no.293180  loss = 2.96276 avg_loss = 3.38978\n",
      "epoch no.3 train no.293190  loss = 2.52619 avg_loss = 3.34070\n",
      "epoch no.3 train no.293200  loss = 6.45545 avg_loss = 3.36452\n",
      "epoch no.3 train no.293210  loss = 3.67405 avg_loss = 3.38578\n",
      "epoch no.3 train no.293220  loss = 2.91492 avg_loss = 3.39837\n",
      "epoch no.3 train no.293230  loss = 3.65107 avg_loss = 3.40098\n",
      "epoch no.3 train no.293240  loss = 4.07563 avg_loss = 3.35594\n",
      "epoch no.3 train no.293250  loss = 3.95708 avg_loss = 3.38452\n",
      "epoch no.3 train no.293260  loss = 2.01433 avg_loss = 3.35344\n",
      "epoch no.3 train no.293270  loss = 2.47877 avg_loss = 3.35491\n",
      "epoch no.3 train no.293280  loss = 2.80576 avg_loss = 3.38228\n",
      "epoch no.3 train no.293290  loss = 3.10429 avg_loss = 3.32080\n",
      "epoch no.3 train no.293300  loss = 3.34546 avg_loss = 3.34203\n",
      "epoch no.3 train no.293310  loss = 2.62553 avg_loss = 3.33407\n",
      "epoch no.3 train no.293320  loss = 2.98767 avg_loss = 3.34546\n",
      "epoch no.3 train no.293330  loss = 5.09169 avg_loss = 3.34456\n",
      "epoch no.3 train no.293340  loss = 2.37492 avg_loss = 3.30758\n",
      "epoch no.3 train no.293350  loss = 4.20012 avg_loss = 3.34742\n",
      "epoch no.3 train no.293360  loss = 2.31430 avg_loss = 3.31583\n",
      "epoch no.3 train no.293370  loss = 3.71345 avg_loss = 3.32588\n",
      "epoch no.3 train no.293380  loss = 3.43840 avg_loss = 3.32406\n",
      "epoch no.3 train no.293390  loss = 3.23082 avg_loss = 3.31201\n",
      "epoch no.3 train no.293400  loss = 3.80158 avg_loss = 3.36330\n",
      "epoch no.4 train no.293410  loss = 3.94454 avg_loss = 3.38353\n",
      "epoch no.4 train no.293420  loss = 2.02120 avg_loss = 3.34028\n",
      "epoch no.4 train no.293430  loss = 5.39387 avg_loss = 3.33421\n",
      "epoch no.4 train no.293440  loss = 3.36683 avg_loss = 3.29153\n",
      "epoch no.4 train no.293450  loss = 1.66292 avg_loss = 3.26275\n",
      "epoch no.4 train no.293460  loss = 2.19799 avg_loss = 3.21308\n",
      "epoch no.4 train no.293470  loss = 4.98860 avg_loss = 3.21860\n",
      "epoch no.4 train no.293480  loss = 2.86382 avg_loss = 3.21517\n",
      "epoch no.4 train no.293490  loss = 2.31978 avg_loss = 3.21519\n",
      "epoch no.4 train no.293500  loss = 1.39595 avg_loss = 3.17271\n",
      "epoch no.4 train no.293510  loss = 3.30518 avg_loss = 3.15314\n",
      "epoch no.4 train no.293520  loss = 2.26795 avg_loss = 3.16651\n",
      "epoch no.4 train no.293530  loss = 3.85420 avg_loss = 3.16032\n",
      "epoch no.4 train no.293540  loss = 2.52804 avg_loss = 3.13256\n",
      "epoch no.4 train no.293550  loss = 2.05577 avg_loss = 3.10706\n",
      "epoch no.4 train no.293560  loss = 3.63670 avg_loss = 3.12246\n",
      "epoch no.4 train no.293570  loss = 4.47598 avg_loss = 3.11786\n",
      "epoch no.4 train no.293580  loss = 3.10837 avg_loss = 3.10191\n",
      "epoch no.4 train no.293590  loss = 2.67490 avg_loss = 3.07044\n",
      "epoch no.4 train no.293600  loss = 3.17494 avg_loss = 3.05728\n",
      "epoch no.4 train no.293610  loss = 3.24188 avg_loss = 3.05714\n",
      "epoch no.4 train no.293620  loss = 3.66999 avg_loss = 3.08854\n",
      "epoch no.4 train no.293630  loss = 3.63782 avg_loss = 3.04379\n",
      "epoch no.4 train no.293640  loss = 3.76343 avg_loss = 3.03320\n",
      "epoch no.4 train no.293650  loss = 3.57181 avg_loss = 3.02208\n",
      "epoch no.4 train no.293660  loss = 3.22590 avg_loss = 2.97981\n",
      "epoch no.4 train no.293670  loss = 3.64802 avg_loss = 2.98622\n",
      "epoch no.4 train no.293680  loss = 3.36253 avg_loss = 2.98325\n",
      "epoch no.4 train no.293690  loss = 2.44022 avg_loss = 2.98035\n",
      "epoch no.4 train no.293700  loss = 2.74605 avg_loss = 2.98524\n",
      "epoch no.4 train no.293710  loss = 2.57466 avg_loss = 2.98712\n",
      "epoch no.4 train no.293720  loss = 5.28610 avg_loss = 2.99124\n",
      "epoch no.4 train no.293730  loss = 4.01865 avg_loss = 2.97721\n",
      "epoch no.4 train no.293740  loss = 3.22205 avg_loss = 2.98517\n",
      "epoch no.4 train no.293750  loss = 2.40266 avg_loss = 3.01657\n",
      "epoch no.4 train no.293760  loss = 3.74060 avg_loss = 3.05143\n",
      "epoch no.4 train no.293770  loss = 2.38530 avg_loss = 2.99743\n",
      "epoch no.4 train no.293780  loss = 2.88164 avg_loss = 2.96334\n",
      "epoch no.4 train no.293790  loss = 1.66368 avg_loss = 2.94870\n",
      "epoch no.4 train no.293800  loss = 4.20074 avg_loss = 3.00746\n",
      "epoch no.4 train no.293810  loss = 1.69676 avg_loss = 3.02122\n",
      "epoch no.4 train no.293820  loss = 3.25965 avg_loss = 3.01599\n",
      "epoch no.4 train no.293830  loss = 2.53407 avg_loss = 3.03856\n",
      "epoch no.4 train no.293840  loss = 3.22269 avg_loss = 3.02112\n",
      "epoch no.4 train no.293850  loss = 2.84794 avg_loss = 3.05426\n",
      "epoch no.4 train no.293860  loss = 2.80619 avg_loss = 3.06915\n",
      "epoch no.4 train no.293870  loss = 2.77778 avg_loss = 3.06213\n",
      "epoch no.4 train no.293880  loss = 2.71821 avg_loss = 3.07973\n",
      "epoch no.4 train no.293890  loss = 3.03184 avg_loss = 3.08794\n",
      "epoch no.4 train no.293900  loss = 4.39202 avg_loss = 3.07921\n",
      "epoch no.4 train no.293910  loss = 2.91636 avg_loss = 3.07610\n",
      "epoch no.4 train no.293920  loss = 3.07041 avg_loss = 3.05677\n",
      "epoch no.4 train no.293930  loss = 3.89208 avg_loss = 3.06439\n",
      "epoch no.4 train no.293940  loss = 3.13355 avg_loss = 3.06035\n",
      "epoch no.4 train no.293950  loss = 4.00609 avg_loss = 3.02708\n",
      "epoch no.4 train no.293960  loss = 2.34253 avg_loss = 2.99683\n",
      "epoch no.4 train no.293970  loss = 3.99684 avg_loss = 3.05162\n",
      "epoch no.4 train no.293980  loss = 2.28660 avg_loss = 3.02512\n",
      "epoch no.4 train no.293990  loss = 2.27429 avg_loss = 3.02609\n",
      "epoch no.4 train no.294000  loss = 3.44715 avg_loss = 3.05474\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '들', '▁그', '년대', '</s>']\n",
      "추억의 노래들 90년대</s>\n",
      "epoch no.4 train no.294010  loss = 3.75468 avg_loss = 3.04709\n",
      "epoch no.4 train no.294020  loss = 3.10366 avg_loss = 3.05725\n",
      "epoch no.4 train no.294030  loss = 3.48693 avg_loss = 3.04068\n",
      "epoch no.4 train no.294040  loss = 2.79682 avg_loss = 3.06803\n",
      "epoch no.4 train no.294050  loss = 3.30886 avg_loss = 3.04067\n",
      "epoch no.4 train no.294060  loss = 4.69432 avg_loss = 3.06547\n",
      "epoch no.4 train no.294070  loss = 3.61510 avg_loss = 3.02500\n",
      "epoch no.4 train no.294080  loss = 3.24450 avg_loss = 3.01979\n",
      "epoch no.4 train no.294090  loss = 5.25422 avg_loss = 3.00075\n",
      "epoch no.4 train no.294100  loss = 2.69454 avg_loss = 3.00450\n",
      "epoch no.4 train no.294110  loss = 4.06287 avg_loss = 2.99539\n",
      "epoch no.4 train no.294120  loss = 2.32619 avg_loss = 2.99525\n",
      "epoch no.4 train no.294130  loss = 2.69338 avg_loss = 2.96699\n",
      "epoch no.4 train no.294140  loss = 4.46583 avg_loss = 2.97606\n",
      "epoch no.4 train no.294150  loss = 1.93319 avg_loss = 3.00475\n",
      "epoch no.4 train no.294160  loss = 1.82489 avg_loss = 3.02414\n",
      "epoch no.4 train no.294170  loss = 2.55399 avg_loss = 3.03742\n",
      "epoch no.4 train no.294180  loss = 2.34175 avg_loss = 3.01640\n",
      "epoch no.4 train no.294190  loss = 3.54115 avg_loss = 3.00523\n",
      "epoch no.4 train no.294200  loss = 3.48391 avg_loss = 3.06550\n",
      "epoch no.4 train no.294210  loss = 2.13035 avg_loss = 3.06462\n",
      "epoch no.4 train no.294220  loss = 2.52257 avg_loss = 3.04342\n",
      "epoch no.4 train no.294230  loss = 2.27214 avg_loss = 3.02611\n",
      "epoch no.4 train no.294240  loss = 2.39929 avg_loss = 3.05766\n",
      "epoch no.4 train no.294250  loss = 2.60493 avg_loss = 3.06907\n",
      "epoch no.4 train no.294260  loss = 4.34620 avg_loss = 3.09542\n",
      "epoch no.4 train no.294270  loss = 3.74341 avg_loss = 3.10257\n",
      "epoch no.4 train no.294280  loss = 4.24326 avg_loss = 3.09101\n",
      "epoch no.4 train no.294290  loss = 4.34109 avg_loss = 3.09508\n",
      "epoch no.4 train no.294300  loss = 3.78663 avg_loss = 3.10393\n",
      "epoch no.4 train no.294310  loss = 2.65589 avg_loss = 3.09237\n",
      "epoch no.4 train no.294320  loss = 3.52394 avg_loss = 3.10730\n",
      "epoch no.4 train no.294330  loss = 2.10098 avg_loss = 3.16432\n",
      "epoch no.4 train no.294340  loss = 2.42326 avg_loss = 3.18189\n",
      "epoch no.4 train no.294350  loss = 2.65212 avg_loss = 3.14985\n",
      "epoch no.4 train no.294360  loss = 3.47074 avg_loss = 3.16086\n",
      "epoch no.4 train no.294370  loss = 2.59894 avg_loss = 3.12341\n",
      "epoch no.4 train no.294380  loss = 3.16592 avg_loss = 3.10062\n",
      "epoch no.4 train no.294390  loss = 2.86699 avg_loss = 3.09749\n",
      "epoch no.4 train no.294400  loss = 3.92006 avg_loss = 3.15798\n",
      "epoch no.4 train no.294410  loss = 3.72596 avg_loss = 3.14841\n",
      "epoch no.4 train no.294420  loss = 3.54386 avg_loss = 3.12709\n",
      "epoch no.4 train no.294430  loss = 4.50900 avg_loss = 3.16323\n",
      "epoch no.4 train no.294440  loss = 2.31768 avg_loss = 3.17162\n",
      "epoch no.4 train no.294450  loss = 4.36974 avg_loss = 3.17249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.294460  loss = 1.42841 avg_loss = 3.18014\n",
      "epoch no.4 train no.294470  loss = 2.83228 avg_loss = 3.15689\n",
      "epoch no.4 train no.294480  loss = 3.13116 avg_loss = 3.17158\n",
      "epoch no.4 train no.294490  loss = 3.03368 avg_loss = 3.13079\n",
      "epoch no.4 train no.294500  loss = 3.13164 avg_loss = 3.14125\n",
      "epoch no.4 train no.294510  loss = 1.86030 avg_loss = 3.11314\n",
      "epoch no.4 train no.294520  loss = 2.55463 avg_loss = 3.11384\n",
      "epoch no.4 train no.294530  loss = 4.94963 avg_loss = 3.09810\n",
      "epoch no.4 train no.294540  loss = 3.26152 avg_loss = 3.07675\n",
      "epoch no.4 train no.294550  loss = 1.68245 avg_loss = 3.05927\n",
      "epoch no.4 train no.294560  loss = 2.77191 avg_loss = 3.04883\n",
      "epoch no.4 train no.294570  loss = 2.85138 avg_loss = 3.03465\n",
      "epoch no.4 train no.294580  loss = 3.30592 avg_loss = 3.04223\n",
      "epoch no.4 train no.294590  loss = 2.27331 avg_loss = 3.04814\n",
      "epoch no.4 train no.294600  loss = 2.31212 avg_loss = 3.04981\n",
      "epoch no.4 train no.294610  loss = 2.05259 avg_loss = 3.08530\n",
      "epoch no.4 train no.294620  loss = 3.53592 avg_loss = 3.07802\n",
      "epoch no.4 train no.294630  loss = 3.56037 avg_loss = 3.07824\n",
      "epoch no.4 train no.294640  loss = 1.98308 avg_loss = 3.04284\n",
      "epoch no.4 train no.294650  loss = 2.95878 avg_loss = 3.05216\n",
      "epoch no.4 train no.294660  loss = 3.96663 avg_loss = 3.08557\n",
      "epoch no.4 train no.294670  loss = 2.37061 avg_loss = 3.08576\n",
      "epoch no.4 train no.294680  loss = 2.60907 avg_loss = 3.03011\n",
      "epoch no.4 train no.294690  loss = 3.26679 avg_loss = 3.03193\n",
      "epoch no.4 train no.294700  loss = 2.06333 avg_loss = 3.03456\n",
      "epoch no.4 train no.294710  loss = 2.81125 avg_loss = 3.02097\n",
      "epoch no.4 train no.294720  loss = 2.73982 avg_loss = 2.99079\n",
      "epoch no.4 train no.294730  loss = 1.99610 avg_loss = 2.96885\n",
      "epoch no.4 train no.294740  loss = 3.93608 avg_loss = 2.98064\n",
      "epoch no.4 train no.294750  loss = 2.27116 avg_loss = 2.95249\n",
      "epoch no.4 train no.294760  loss = 3.02491 avg_loss = 2.98933\n",
      "epoch no.4 train no.294770  loss = 3.46350 avg_loss = 3.02893\n",
      "epoch no.4 train no.294780  loss = 4.44623 avg_loss = 3.02387\n",
      "epoch no.4 train no.294790  loss = 2.70559 avg_loss = 2.99363\n",
      "epoch no.4 train no.294800  loss = 4.15754 avg_loss = 3.02611\n",
      "epoch no.4 train no.294810  loss = 4.05090 avg_loss = 3.03565\n",
      "epoch no.4 train no.294820  loss = 4.36357 avg_loss = 3.07732\n",
      "epoch no.4 train no.294830  loss = 3.43588 avg_loss = 3.04839\n",
      "epoch no.4 train no.294840  loss = 2.23340 avg_loss = 2.99387\n",
      "epoch no.4 train no.294850  loss = 3.59783 avg_loss = 3.00829\n",
      "epoch no.4 train no.294860  loss = 2.81392 avg_loss = 3.03517\n",
      "epoch no.4 train no.294870  loss = 3.05375 avg_loss = 3.03975\n",
      "epoch no.4 train no.294880  loss = 2.61067 avg_loss = 3.00617\n",
      "epoch no.4 train no.294890  loss = 2.52034 avg_loss = 2.98208\n",
      "epoch no.4 train no.294900  loss = 3.25675 avg_loss = 2.97242\n",
      "epoch no.4 train no.294910  loss = 2.96517 avg_loss = 2.98894\n",
      "epoch no.4 train no.294920  loss = 2.94987 avg_loss = 2.98494\n",
      "epoch no.4 train no.294930  loss = 2.50191 avg_loss = 2.95317\n",
      "epoch no.4 train no.294940  loss = 2.10997 avg_loss = 2.96525\n",
      "epoch no.4 train no.294950  loss = 4.27754 avg_loss = 2.98515\n",
      "epoch no.4 train no.294960  loss = 4.40034 avg_loss = 3.03187\n",
      "epoch no.4 train no.294970  loss = 3.89254 avg_loss = 3.01633\n",
      "epoch no.4 train no.294980  loss = 3.92084 avg_loss = 3.04157\n",
      "epoch no.4 train no.294990  loss = 2.32824 avg_loss = 3.04644\n",
      "epoch no.4 train no.295000  loss = 2.61627 avg_loss = 3.01267\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁발라', '모', '음', '</s>']\n",
      "추억의 90년대 가요모음</s>\n",
      "epoch no.4 train no.295010  loss = 2.90737 avg_loss = 3.01789\n",
      "epoch no.4 train no.295020  loss = 3.17241 avg_loss = 3.05098\n",
      "epoch no.4 train no.295030  loss = 2.66553 avg_loss = 3.05490\n",
      "epoch no.4 train no.295040  loss = 2.82383 avg_loss = 3.12315\n",
      "epoch no.4 train no.295050  loss = 3.58587 avg_loss = 3.15446\n",
      "epoch no.4 train no.295060  loss = 3.39525 avg_loss = 3.15201\n",
      "epoch no.4 train no.295070  loss = 2.55761 avg_loss = 3.14703\n",
      "epoch no.4 train no.295080  loss = 5.02963 avg_loss = 3.14935\n",
      "epoch no.4 train no.295090  loss = 5.33455 avg_loss = 3.15807\n",
      "epoch no.4 train no.295100  loss = 3.28033 avg_loss = 3.16944\n",
      "epoch no.4 train no.295110  loss = 3.43582 avg_loss = 3.17753\n",
      "epoch no.4 train no.295120  loss = 4.57531 avg_loss = 3.15675\n",
      "epoch no.4 train no.295130  loss = 2.07770 avg_loss = 3.14940\n",
      "epoch no.4 train no.295140  loss = 2.42116 avg_loss = 3.10209\n",
      "epoch no.4 train no.295150  loss = 3.65569 avg_loss = 3.05167\n",
      "epoch no.4 train no.295160  loss = 2.74346 avg_loss = 3.05825\n",
      "epoch no.4 train no.295170  loss = 3.51762 avg_loss = 3.08355\n",
      "epoch no.4 train no.295180  loss = 3.63090 avg_loss = 3.04540\n",
      "epoch no.4 train no.295190  loss = 1.35379 avg_loss = 3.03841\n",
      "epoch no.4 train no.295200  loss = 3.80285 avg_loss = 3.04770\n",
      "epoch no.4 train no.295210  loss = 2.65924 avg_loss = 3.06861\n",
      "epoch no.4 train no.295220  loss = 4.31554 avg_loss = 3.12283\n",
      "epoch no.4 train no.295230  loss = 1.99665 avg_loss = 3.07711\n",
      "epoch no.4 train no.295240  loss = 2.73206 avg_loss = 3.03793\n",
      "epoch no.4 train no.295250  loss = 1.83576 avg_loss = 3.04993\n",
      "epoch no.4 train no.295260  loss = 2.26137 avg_loss = 3.04663\n",
      "epoch no.4 train no.295270  loss = 2.02452 avg_loss = 3.01971\n",
      "epoch no.4 train no.295280  loss = 2.88828 avg_loss = 3.02436\n",
      "epoch no.4 train no.295290  loss = 2.91053 avg_loss = 2.98981\n",
      "epoch no.4 train no.295300  loss = 1.57418 avg_loss = 2.95459\n",
      "epoch no.4 train no.295310  loss = 2.68947 avg_loss = 2.90693\n",
      "epoch no.4 train no.295320  loss = 4.46433 avg_loss = 2.95670\n",
      "epoch no.4 train no.295330  loss = 3.29091 avg_loss = 2.94036\n",
      "epoch no.4 train no.295340  loss = 3.54833 avg_loss = 2.94013\n",
      "epoch no.4 train no.295350  loss = 1.92023 avg_loss = 2.96478\n",
      "epoch no.4 train no.295360  loss = 3.02232 avg_loss = 2.98804\n",
      "epoch no.4 train no.295370  loss = 3.31045 avg_loss = 2.99015\n",
      "epoch no.4 train no.295380  loss = 2.02403 avg_loss = 2.96040\n",
      "epoch no.4 train no.295390  loss = 3.66869 avg_loss = 2.98431\n",
      "epoch no.4 train no.295400  loss = 4.18246 avg_loss = 3.01526\n",
      "epoch no.4 train no.295410  loss = 4.21459 avg_loss = 3.01733\n",
      "epoch no.4 train no.295420  loss = 4.02951 avg_loss = 3.03661\n",
      "epoch no.4 train no.295430  loss = 3.82391 avg_loss = 3.02588\n",
      "epoch no.4 train no.295440  loss = 2.14453 avg_loss = 2.98259\n",
      "epoch no.4 train no.295450  loss = 3.81536 avg_loss = 2.99712\n",
      "epoch no.4 train no.295460  loss = 3.87593 avg_loss = 3.02327\n",
      "epoch no.4 train no.295470  loss = 1.96265 avg_loss = 2.98823\n",
      "epoch no.4 train no.295480  loss = 3.39883 avg_loss = 2.97329\n",
      "epoch no.4 train no.295490  loss = 3.42595 avg_loss = 3.04020\n",
      "epoch no.4 train no.295500  loss = 5.22150 avg_loss = 3.02622\n",
      "epoch no.4 train no.295510  loss = 3.42487 avg_loss = 2.98758\n",
      "epoch no.4 train no.295520  loss = 4.08750 avg_loss = 3.04206\n",
      "epoch no.4 train no.295530  loss = 1.98211 avg_loss = 3.04358\n",
      "epoch no.4 train no.295540  loss = 4.27334 avg_loss = 3.04090\n",
      "epoch no.4 train no.295550  loss = 2.82745 avg_loss = 3.04681\n",
      "epoch no.4 train no.295560  loss = 3.33424 avg_loss = 3.02899\n",
      "epoch no.4 train no.295570  loss = 2.66740 avg_loss = 3.03654\n",
      "epoch no.4 train no.295580  loss = 3.19200 avg_loss = 3.01155\n",
      "epoch no.4 train no.295590  loss = 3.39821 avg_loss = 3.03005\n",
      "epoch no.4 train no.295600  loss = 2.82434 avg_loss = 3.00722\n",
      "epoch no.4 train no.295610  loss = 1.97459 avg_loss = 2.97063\n",
      "epoch no.4 train no.295620  loss = 3.00454 avg_loss = 3.02012\n",
      "epoch no.4 train no.295630  loss = 3.04245 avg_loss = 3.01066\n",
      "epoch no.4 train no.295640  loss = 2.67988 avg_loss = 2.97796\n",
      "epoch no.4 train no.295650  loss = 1.83514 avg_loss = 2.92081\n",
      "epoch no.4 train no.295660  loss = 2.94982 avg_loss = 2.96805\n",
      "epoch no.4 train no.295670  loss = 4.17117 avg_loss = 3.01837\n",
      "epoch no.4 train no.295680  loss = 2.63346 avg_loss = 3.03424\n",
      "epoch no.4 train no.295690  loss = 3.02681 avg_loss = 3.06482\n",
      "epoch no.4 train no.295700  loss = 2.56940 avg_loss = 3.04637\n",
      "epoch no.4 train no.295710  loss = 3.81084 avg_loss = 3.03285\n",
      "epoch no.4 train no.295720  loss = 2.59558 avg_loss = 3.05928\n",
      "epoch no.4 train no.295730  loss = 3.50295 avg_loss = 3.07327\n",
      "epoch no.4 train no.295740  loss = 3.13902 avg_loss = 3.04922\n",
      "epoch no.4 train no.295750  loss = 3.26107 avg_loss = 3.06767\n",
      "epoch no.4 train no.295760  loss = 3.53637 avg_loss = 3.08197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.295770  loss = 2.75327 avg_loss = 3.09070\n",
      "epoch no.4 train no.295780  loss = 3.27857 avg_loss = 3.06304\n",
      "epoch no.4 train no.295790  loss = 3.14366 avg_loss = 3.02277\n",
      "epoch no.4 train no.295800  loss = 2.21007 avg_loss = 3.03334\n",
      "epoch no.4 train no.295810  loss = 2.08871 avg_loss = 3.05071\n",
      "epoch no.4 train no.295820  loss = 3.96217 avg_loss = 3.06955\n",
      "epoch no.4 train no.295830  loss = 2.78276 avg_loss = 3.06874\n",
      "epoch no.4 train no.295840  loss = 2.46275 avg_loss = 3.06035\n",
      "epoch no.4 train no.295850  loss = 3.67547 avg_loss = 3.03327\n",
      "epoch no.4 train no.295860  loss = 3.79107 avg_loss = 3.07740\n",
      "epoch no.4 train no.295870  loss = 3.29787 avg_loss = 3.11994\n",
      "epoch no.4 train no.295880  loss = 2.31361 avg_loss = 3.08483\n",
      "epoch no.4 train no.295890  loss = 2.29120 avg_loss = 3.10597\n",
      "epoch no.4 train no.295900  loss = 2.30390 avg_loss = 3.07843\n",
      "epoch no.4 train no.295910  loss = 3.68175 avg_loss = 3.07083\n",
      "epoch no.4 train no.295920  loss = 2.16652 avg_loss = 3.03046\n",
      "epoch no.4 train no.295930  loss = 2.55596 avg_loss = 3.01779\n",
      "epoch no.4 train no.295940  loss = 3.11465 avg_loss = 3.03411\n",
      "epoch no.4 train no.295950  loss = 2.15853 avg_loss = 3.04341\n",
      "epoch no.4 train no.295960  loss = 2.66365 avg_loss = 3.02719\n",
      "epoch no.4 train no.295970  loss = 2.34596 avg_loss = 2.98362\n",
      "epoch no.4 train no.295980  loss = 2.26306 avg_loss = 3.04574\n",
      "epoch no.4 train no.295990  loss = 1.81529 avg_loss = 3.03201\n",
      "epoch no.4 train no.296000  loss = 3.91541 avg_loss = 3.04796\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁발라', '▁모음', '</s>']\n",
      "추억의 2000년대 가요 베스트</s>\n",
      "epoch no.4 train no.296010  loss = 4.88827 avg_loss = 3.10939\n",
      "epoch no.4 train no.296020  loss = 4.03014 avg_loss = 3.08680\n",
      "epoch no.4 train no.296030  loss = 2.82765 avg_loss = 3.09932\n",
      "epoch no.4 train no.296040  loss = 2.40508 avg_loss = 3.07958\n",
      "epoch no.4 train no.296050  loss = 3.54708 avg_loss = 3.11581\n",
      "epoch no.4 train no.296060  loss = 3.01964 avg_loss = 3.10444\n",
      "epoch no.4 train no.296070  loss = 3.33679 avg_loss = 3.08704\n",
      "epoch no.4 train no.296080  loss = 4.06446 avg_loss = 3.10482\n",
      "epoch no.4 train no.296090  loss = 1.25489 avg_loss = 3.06395\n",
      "epoch no.4 train no.296100  loss = 3.51416 avg_loss = 3.09031\n",
      "epoch no.4 train no.296110  loss = 4.38159 avg_loss = 3.11370\n",
      "epoch no.4 train no.296120  loss = 3.57520 avg_loss = 3.08790\n",
      "epoch no.4 train no.296130  loss = 4.82610 avg_loss = 3.12316\n",
      "epoch no.4 train no.296140  loss = 3.54696 avg_loss = 3.13215\n",
      "epoch no.4 train no.296150  loss = 3.95846 avg_loss = 3.12756\n",
      "epoch no.4 train no.296160  loss = 3.08229 avg_loss = 3.13865\n",
      "epoch no.4 train no.296170  loss = 3.06582 avg_loss = 3.12270\n",
      "epoch no.4 train no.296180  loss = 4.73618 avg_loss = 3.14503\n",
      "epoch no.4 train no.296190  loss = 2.73053 avg_loss = 3.15022\n",
      "epoch no.4 train no.296200  loss = 3.34115 avg_loss = 3.12652\n",
      "epoch no.4 train no.296210  loss = 2.87000 avg_loss = 3.13152\n",
      "epoch no.4 train no.296220  loss = 1.87840 avg_loss = 3.14368\n",
      "epoch no.4 train no.296230  loss = 2.75621 avg_loss = 3.14204\n",
      "epoch no.4 train no.296240  loss = 1.46855 avg_loss = 3.16596\n",
      "epoch no.4 train no.296250  loss = 3.87169 avg_loss = 3.17709\n",
      "epoch no.4 train no.296260  loss = 2.26151 avg_loss = 3.15458\n",
      "epoch no.4 train no.296270  loss = 3.55403 avg_loss = 3.15266\n",
      "epoch no.4 train no.296280  loss = 3.01536 avg_loss = 3.16854\n",
      "epoch no.4 train no.296290  loss = 4.74041 avg_loss = 3.18498\n",
      "epoch no.4 train no.296300  loss = 3.12702 avg_loss = 3.19225\n",
      "epoch no.4 train no.296310  loss = 2.61949 avg_loss = 3.16624\n",
      "epoch no.4 train no.296320  loss = 3.02302 avg_loss = 3.14571\n",
      "epoch no.4 train no.296330  loss = 2.81357 avg_loss = 3.21903\n",
      "epoch no.4 train no.296340  loss = 3.02955 avg_loss = 3.20607\n",
      "epoch no.4 train no.296350  loss = 4.12630 avg_loss = 3.19153\n",
      "epoch no.4 train no.296360  loss = 2.67464 avg_loss = 3.15835\n",
      "epoch no.4 train no.296370  loss = 3.72324 avg_loss = 3.15841\n",
      "epoch no.4 train no.296380  loss = 3.12130 avg_loss = 3.16349\n",
      "epoch no.4 train no.296390  loss = 3.51410 avg_loss = 3.11723\n",
      "epoch no.4 train no.296400  loss = 3.75766 avg_loss = 3.09733\n",
      "epoch no.4 train no.296410  loss = 2.13220 avg_loss = 3.09237\n",
      "epoch no.4 train no.296420  loss = 2.62066 avg_loss = 3.08377\n",
      "epoch no.4 train no.296430  loss = 2.43742 avg_loss = 3.04397\n",
      "epoch no.4 train no.296440  loss = 3.91166 avg_loss = 3.02488\n",
      "epoch no.4 train no.296450  loss = 3.21473 avg_loss = 3.00663\n",
      "epoch no.4 train no.296460  loss = 4.48134 avg_loss = 3.03251\n",
      "epoch no.4 train no.296470  loss = 2.52141 avg_loss = 3.02155\n",
      "epoch no.4 train no.296480  loss = 2.29411 avg_loss = 2.99586\n",
      "epoch no.4 train no.296490  loss = 3.40635 avg_loss = 2.98372\n",
      "epoch no.4 train no.296500  loss = 2.90622 avg_loss = 2.99652\n",
      "epoch no.4 train no.296510  loss = 3.72071 avg_loss = 3.02227\n",
      "epoch no.4 train no.296520  loss = 1.69511 avg_loss = 3.00952\n",
      "epoch no.4 train no.296530  loss = 2.44463 avg_loss = 3.07329\n",
      "epoch no.4 train no.296540  loss = 2.45193 avg_loss = 3.04637\n",
      "epoch no.4 train no.296550  loss = 2.38828 avg_loss = 3.08353\n",
      "epoch no.4 train no.296560  loss = 2.91113 avg_loss = 3.04260\n",
      "epoch no.4 train no.296570  loss = 3.63236 avg_loss = 3.04433\n",
      "epoch no.4 train no.296580  loss = 1.54782 avg_loss = 3.02052\n",
      "epoch no.4 train no.296590  loss = 2.68821 avg_loss = 3.00034\n",
      "epoch no.4 train no.296600  loss = 2.00005 avg_loss = 3.01209\n",
      "epoch no.4 train no.296610  loss = 2.86495 avg_loss = 3.06315\n",
      "epoch no.4 train no.296620  loss = 2.01460 avg_loss = 3.07442\n",
      "epoch no.4 train no.296630  loss = 4.86482 avg_loss = 3.08009\n",
      "epoch no.4 train no.296640  loss = 3.60339 avg_loss = 3.10673\n",
      "epoch no.4 train no.296650  loss = 3.44138 avg_loss = 3.09848\n",
      "epoch no.4 train no.296660  loss = 2.32158 avg_loss = 3.15843\n",
      "epoch no.4 train no.296670  loss = 3.02607 avg_loss = 3.12227\n",
      "epoch no.4 train no.296680  loss = 3.25120 avg_loss = 3.08411\n",
      "epoch no.4 train no.296690  loss = 2.43778 avg_loss = 3.04559\n",
      "epoch no.4 train no.296700  loss = 4.06303 avg_loss = 3.05466\n",
      "epoch no.4 train no.296710  loss = 2.92315 avg_loss = 3.02544\n",
      "epoch no.4 train no.296720  loss = 2.08897 avg_loss = 3.03356\n",
      "epoch no.4 train no.296730  loss = 1.35409 avg_loss = 3.00383\n",
      "epoch no.4 train no.296740  loss = 2.25411 avg_loss = 3.01911\n",
      "epoch no.4 train no.296750  loss = 2.24538 avg_loss = 3.03151\n",
      "epoch no.4 train no.296760  loss = 5.42133 avg_loss = 3.06086\n",
      "epoch no.4 train no.296770  loss = 1.47126 avg_loss = 2.99818\n",
      "epoch no.4 train no.296780  loss = 4.75442 avg_loss = 2.99092\n",
      "epoch no.4 train no.296790  loss = 1.82675 avg_loss = 2.99463\n",
      "epoch no.4 train no.296800  loss = 2.64629 avg_loss = 2.97132\n",
      "epoch no.4 train no.296810  loss = 3.03718 avg_loss = 2.97231\n",
      "epoch no.4 train no.296820  loss = 2.88944 avg_loss = 2.97618\n",
      "epoch no.4 train no.296830  loss = 2.23483 avg_loss = 2.97049\n",
      "epoch no.4 train no.296840  loss = 2.47846 avg_loss = 2.98448\n",
      "epoch no.4 train no.296850  loss = 2.40199 avg_loss = 2.98753\n",
      "epoch no.4 train no.296860  loss = 1.65762 avg_loss = 2.95602\n",
      "epoch no.4 train no.296870  loss = 2.18728 avg_loss = 2.96125\n",
      "epoch no.4 train no.296880  loss = 3.68196 avg_loss = 2.99048\n",
      "epoch no.4 train no.296890  loss = 2.39618 avg_loss = 3.01131\n",
      "epoch no.4 train no.296900  loss = 3.56895 avg_loss = 3.01793\n",
      "epoch no.4 train no.296910  loss = 1.21872 avg_loss = 3.01173\n",
      "epoch no.4 train no.296920  loss = 3.22777 avg_loss = 3.02390\n",
      "epoch no.4 train no.296930  loss = 2.74305 avg_loss = 3.00340\n",
      "epoch no.4 train no.296940  loss = 3.42060 avg_loss = 3.02455\n",
      "epoch no.4 train no.296950  loss = 2.80648 avg_loss = 2.98757\n",
      "epoch no.4 train no.296960  loss = 2.00508 avg_loss = 2.98438\n",
      "epoch no.4 train no.296970  loss = 3.38094 avg_loss = 3.05665\n",
      "epoch no.4 train no.296980  loss = 1.92945 avg_loss = 3.05154\n",
      "epoch no.4 train no.296990  loss = 4.43100 avg_loss = 3.05281\n",
      "epoch no.4 train no.297000  loss = 3.59939 avg_loss = 3.06527\n",
      "4\n",
      "to_tokens: ['▁가을', '▁싸이', '곡', '발', '▁노래', '곡']\n",
      "추억의 명곡 아이돌 댄스</s>\n",
      "epoch no.4 train no.297010  loss = 3.42327 avg_loss = 3.07973\n",
      "epoch no.4 train no.297020  loss = 2.75435 avg_loss = 3.06279\n",
      "epoch no.4 train no.297030  loss = 3.58022 avg_loss = 3.04718\n",
      "epoch no.4 train no.297040  loss = 3.91989 avg_loss = 3.06223\n",
      "epoch no.4 train no.297050  loss = 4.36199 avg_loss = 3.09447\n",
      "epoch no.4 train no.297060  loss = 2.61016 avg_loss = 3.11727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.297070  loss = 6.74300 avg_loss = 3.16967\n",
      "epoch no.4 train no.297080  loss = 0.89716 avg_loss = 3.09358\n",
      "epoch no.4 train no.297090  loss = 3.12737 avg_loss = 3.12493\n",
      "epoch no.4 train no.297100  loss = 3.52222 avg_loss = 3.15841\n",
      "epoch no.4 train no.297110  loss = 2.63234 avg_loss = 3.14611\n",
      "epoch no.4 train no.297120  loss = 1.95656 avg_loss = 3.08940\n",
      "epoch no.4 train no.297130  loss = 2.85874 avg_loss = 3.09101\n",
      "epoch no.4 train no.297140  loss = 2.34596 avg_loss = 3.07922\n",
      "epoch no.4 train no.297150  loss = 4.65775 avg_loss = 3.09958\n",
      "epoch no.4 train no.297160  loss = 3.20718 avg_loss = 3.05358\n",
      "epoch no.4 train no.297170  loss = 4.24885 avg_loss = 3.05558\n",
      "epoch no.4 train no.297180  loss = 3.34109 avg_loss = 3.06436\n",
      "epoch no.4 train no.297190  loss = 4.19712 avg_loss = 3.10544\n",
      "epoch no.4 train no.297200  loss = 2.00669 avg_loss = 3.13236\n",
      "epoch no.4 train no.297210  loss = 3.70933 avg_loss = 3.12666\n",
      "epoch no.4 train no.297220  loss = 3.25962 avg_loss = 3.13762\n",
      "epoch no.4 train no.297230  loss = 3.06560 avg_loss = 3.10179\n",
      "epoch no.4 train no.297240  loss = 2.44712 avg_loss = 3.10233\n",
      "epoch no.4 train no.297250  loss = 3.75931 avg_loss = 3.10810\n",
      "epoch no.4 train no.297260  loss = 2.61908 avg_loss = 3.10342\n",
      "epoch no.4 train no.297270  loss = 3.04745 avg_loss = 3.09357\n",
      "epoch no.4 train no.297280  loss = 4.69432 avg_loss = 3.08145\n",
      "epoch no.4 train no.297290  loss = 3.87407 avg_loss = 3.06399\n",
      "epoch no.4 train no.297300  loss = 2.56450 avg_loss = 3.04843\n",
      "epoch no.4 train no.297310  loss = 3.98577 avg_loss = 3.04013\n",
      "epoch no.4 train no.297320  loss = 2.80455 avg_loss = 3.06714\n",
      "epoch no.4 train no.297330  loss = 1.84785 avg_loss = 3.07038\n",
      "epoch no.4 train no.297340  loss = 2.22828 avg_loss = 3.05938\n",
      "epoch no.4 train no.297350  loss = 2.25208 avg_loss = 3.05052\n",
      "epoch no.4 train no.297360  loss = 2.60777 avg_loss = 3.05447\n",
      "epoch no.4 train no.297370  loss = 2.91400 avg_loss = 3.04746\n",
      "epoch no.4 train no.297380  loss = 3.91774 avg_loss = 3.10625\n",
      "epoch no.4 train no.297390  loss = 2.06928 avg_loss = 3.09859\n",
      "epoch no.4 train no.297400  loss = 4.43914 avg_loss = 3.11012\n",
      "epoch no.4 train no.297410  loss = 2.10920 avg_loss = 3.10389\n",
      "epoch no.4 train no.297420  loss = 3.84234 avg_loss = 3.09717\n",
      "epoch no.4 train no.297430  loss = 3.42759 avg_loss = 3.05593\n",
      "epoch no.4 train no.297440  loss = 4.73721 avg_loss = 3.06818\n",
      "epoch no.4 train no.297450  loss = 3.06770 avg_loss = 3.06028\n",
      "epoch no.4 train no.297460  loss = 2.66713 avg_loss = 3.02115\n",
      "epoch no.4 train no.297470  loss = 4.15033 avg_loss = 3.04514\n",
      "epoch no.4 train no.297480  loss = 3.11850 avg_loss = 3.05541\n",
      "epoch no.4 train no.297490  loss = 3.41388 avg_loss = 3.02786\n",
      "epoch no.4 train no.297500  loss = 2.48467 avg_loss = 3.01380\n",
      "epoch no.4 train no.297510  loss = 2.13287 avg_loss = 3.04659\n",
      "epoch no.4 train no.297520  loss = 3.65420 avg_loss = 3.07065\n",
      "epoch no.4 train no.297530  loss = 4.18539 avg_loss = 3.09852\n",
      "epoch no.4 train no.297540  loss = 2.71410 avg_loss = 3.07568\n",
      "epoch no.4 train no.297550  loss = 3.82589 avg_loss = 3.05479\n",
      "epoch no.4 train no.297560  loss = 3.14553 avg_loss = 3.06662\n",
      "epoch no.4 train no.297570  loss = 3.00458 avg_loss = 3.05016\n",
      "epoch no.4 train no.297580  loss = 2.45578 avg_loss = 3.07223\n",
      "epoch no.4 train no.297590  loss = 3.83062 avg_loss = 3.05920\n",
      "epoch no.4 train no.297600  loss = 2.87231 avg_loss = 3.08597\n",
      "epoch no.4 train no.297610  loss = 2.56794 avg_loss = 3.11059\n",
      "epoch no.4 train no.297620  loss = 4.02859 avg_loss = 3.16219\n",
      "epoch no.4 train no.297630  loss = 2.33792 avg_loss = 3.14510\n",
      "epoch no.4 train no.297640  loss = 2.43946 avg_loss = 3.10410\n",
      "epoch no.4 train no.297650  loss = 2.75642 avg_loss = 3.12297\n",
      "epoch no.4 train no.297660  loss = 2.27284 avg_loss = 3.12099\n",
      "epoch no.4 train no.297670  loss = 2.74680 avg_loss = 3.11879\n",
      "epoch no.4 train no.297680  loss = 4.00869 avg_loss = 3.07430\n",
      "epoch no.4 train no.297690  loss = 1.92392 avg_loss = 3.06160\n",
      "epoch no.4 train no.297700  loss = 2.18284 avg_loss = 3.05458\n",
      "epoch no.4 train no.297710  loss = 6.03598 avg_loss = 3.06645\n",
      "epoch no.4 train no.297720  loss = 3.07502 avg_loss = 3.05072\n",
      "epoch no.4 train no.297730  loss = 1.87392 avg_loss = 3.06077\n",
      "epoch no.4 train no.297740  loss = 3.50371 avg_loss = 3.05650\n",
      "epoch no.4 train no.297750  loss = 3.41707 avg_loss = 3.04078\n",
      "epoch no.4 train no.297760  loss = 2.70063 avg_loss = 3.02041\n",
      "epoch no.4 train no.297770  loss = 2.27569 avg_loss = 2.95769\n",
      "epoch no.4 train no.297780  loss = 2.43860 avg_loss = 3.00647\n",
      "epoch no.4 train no.297790  loss = 3.23435 avg_loss = 3.00801\n",
      "epoch no.4 train no.297800  loss = 3.06629 avg_loss = 3.03210\n",
      "epoch no.4 train no.297810  loss = 3.04860 avg_loss = 3.08837\n",
      "epoch no.4 train no.297820  loss = 3.34446 avg_loss = 3.08909\n",
      "epoch no.4 train no.297830  loss = 1.52018 avg_loss = 3.10462\n",
      "epoch no.4 train no.297840  loss = 3.74302 avg_loss = 3.05380\n",
      "epoch no.4 train no.297850  loss = 2.46894 avg_loss = 3.03514\n",
      "epoch no.4 train no.297860  loss = 3.20658 avg_loss = 3.05633\n",
      "epoch no.4 train no.297870  loss = 2.86305 avg_loss = 3.03321\n",
      "epoch no.4 train no.297880  loss = 2.56566 avg_loss = 3.04785\n",
      "epoch no.4 train no.297890  loss = 2.62232 avg_loss = 3.06113\n",
      "epoch no.4 train no.297900  loss = 2.64268 avg_loss = 3.05045\n",
      "epoch no.4 train no.297910  loss = 2.69655 avg_loss = 3.06934\n",
      "epoch no.4 train no.297920  loss = 2.94852 avg_loss = 3.09134\n",
      "epoch no.4 train no.297930  loss = 2.96512 avg_loss = 3.07240\n",
      "epoch no.4 train no.297940  loss = 3.22941 avg_loss = 3.08736\n",
      "epoch no.4 train no.297950  loss = 4.27973 avg_loss = 3.06664\n",
      "epoch no.4 train no.297960  loss = 2.30225 avg_loss = 3.07100\n",
      "epoch no.4 train no.297970  loss = 4.06569 avg_loss = 3.06452\n",
      "epoch no.4 train no.297980  loss = 1.80646 avg_loss = 3.01170\n",
      "epoch no.4 train no.297990  loss = 2.60338 avg_loss = 3.02683\n",
      "epoch no.4 train no.298000  loss = 4.15694 avg_loss = 3.03682\n",
      "4\n",
      "to_tokens: ['▁가을', '▁싸이', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.298010  loss = 3.36400 avg_loss = 3.03502\n",
      "epoch no.4 train no.298020  loss = 1.95168 avg_loss = 3.03342\n",
      "epoch no.4 train no.298030  loss = 2.97555 avg_loss = 3.07543\n",
      "epoch no.4 train no.298040  loss = 4.09885 avg_loss = 3.05431\n",
      "epoch no.4 train no.298050  loss = 2.26505 avg_loss = 3.04832\n",
      "epoch no.4 train no.298060  loss = 2.52606 avg_loss = 3.01324\n",
      "epoch no.4 train no.298070  loss = 1.90327 avg_loss = 3.03895\n",
      "epoch no.4 train no.298080  loss = 3.97530 avg_loss = 3.04391\n",
      "epoch no.4 train no.298090  loss = 3.20519 avg_loss = 3.02768\n",
      "epoch no.4 train no.298100  loss = 2.14472 avg_loss = 2.99721\n",
      "epoch no.4 train no.298110  loss = 2.25772 avg_loss = 3.00327\n",
      "epoch no.4 train no.298120  loss = 1.77298 avg_loss = 2.99597\n",
      "epoch no.4 train no.298130  loss = 2.23058 avg_loss = 3.01679\n",
      "epoch no.4 train no.298140  loss = 3.09887 avg_loss = 3.03818\n",
      "epoch no.4 train no.298150  loss = 4.19276 avg_loss = 3.07335\n",
      "epoch no.4 train no.298160  loss = 5.32262 avg_loss = 3.10063\n",
      "epoch no.4 train no.298170  loss = 3.52796 avg_loss = 3.12407\n",
      "epoch no.4 train no.298180  loss = 3.26317 avg_loss = 3.09021\n",
      "epoch no.4 train no.298190  loss = 3.89910 avg_loss = 3.11225\n",
      "epoch no.4 train no.298200  loss = 2.51952 avg_loss = 3.09167\n",
      "epoch no.4 train no.298210  loss = 2.60853 avg_loss = 3.10176\n",
      "epoch no.4 train no.298220  loss = 4.15715 avg_loss = 3.10324\n",
      "epoch no.4 train no.298230  loss = 2.09977 avg_loss = 3.10864\n",
      "epoch no.4 train no.298240  loss = 2.35572 avg_loss = 3.11794\n",
      "epoch no.4 train no.298250  loss = 2.59521 avg_loss = 3.08617\n",
      "epoch no.4 train no.298260  loss = 3.01542 avg_loss = 3.08503\n",
      "epoch no.4 train no.298270  loss = 2.70995 avg_loss = 3.07234\n",
      "epoch no.4 train no.298280  loss = 1.97099 avg_loss = 3.02304\n",
      "epoch no.4 train no.298290  loss = 2.85999 avg_loss = 3.02769\n",
      "epoch no.4 train no.298300  loss = 2.63318 avg_loss = 2.99111\n",
      "epoch no.4 train no.298310  loss = 1.81517 avg_loss = 3.01138\n",
      "epoch no.4 train no.298320  loss = 3.02609 avg_loss = 3.01483\n",
      "epoch no.4 train no.298330  loss = 5.43292 avg_loss = 3.02393\n",
      "epoch no.4 train no.298340  loss = 4.98330 avg_loss = 3.02076\n",
      "epoch no.4 train no.298350  loss = 2.97224 avg_loss = 3.03571\n",
      "epoch no.4 train no.298360  loss = 2.53446 avg_loss = 3.01663\n",
      "epoch no.4 train no.298370  loss = 5.26773 avg_loss = 3.02494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.298380  loss = 2.69163 avg_loss = 3.00221\n",
      "epoch no.4 train no.298390  loss = 3.28199 avg_loss = 3.01232\n",
      "epoch no.4 train no.298400  loss = 2.25802 avg_loss = 2.98930\n",
      "epoch no.4 train no.298410  loss = 4.41964 avg_loss = 2.97527\n",
      "epoch no.4 train no.298420  loss = 1.78590 avg_loss = 2.97051\n",
      "epoch no.4 train no.298430  loss = 3.08708 avg_loss = 2.97255\n",
      "epoch no.4 train no.298440  loss = 2.34595 avg_loss = 2.95178\n",
      "epoch no.4 train no.298450  loss = 2.91042 avg_loss = 2.91274\n",
      "epoch no.4 train no.298460  loss = 2.61650 avg_loss = 2.91658\n",
      "epoch no.4 train no.298470  loss = 2.08743 avg_loss = 2.92223\n",
      "epoch no.4 train no.298480  loss = 2.66556 avg_loss = 2.92318\n",
      "epoch no.4 train no.298490  loss = 3.19449 avg_loss = 2.92889\n",
      "epoch no.4 train no.298500  loss = 3.89380 avg_loss = 2.93478\n",
      "epoch no.4 train no.298510  loss = 3.05213 avg_loss = 2.93432\n",
      "epoch no.4 train no.298520  loss = 2.84205 avg_loss = 2.93115\n",
      "epoch no.4 train no.298530  loss = 4.09848 avg_loss = 2.95999\n",
      "epoch no.4 train no.298540  loss = 3.09763 avg_loss = 2.95066\n",
      "epoch no.4 train no.298550  loss = 2.14053 avg_loss = 2.95094\n",
      "epoch no.4 train no.298560  loss = 3.76177 avg_loss = 2.99195\n",
      "epoch no.4 train no.298570  loss = 3.27220 avg_loss = 3.06391\n",
      "epoch no.4 train no.298580  loss = 4.14220 avg_loss = 3.07386\n",
      "epoch no.4 train no.298590  loss = 2.51588 avg_loss = 3.09989\n",
      "epoch no.4 train no.298600  loss = 4.79643 avg_loss = 3.11248\n",
      "epoch no.4 train no.298610  loss = 2.66514 avg_loss = 3.11355\n",
      "epoch no.4 train no.298620  loss = 2.92104 avg_loss = 3.08318\n",
      "epoch no.4 train no.298630  loss = 1.29288 avg_loss = 3.03439\n",
      "epoch no.4 train no.298640  loss = 1.89870 avg_loss = 3.01073\n",
      "epoch no.4 train no.298650  loss = 2.35702 avg_loss = 3.00911\n",
      "epoch no.4 train no.298660  loss = 3.88159 avg_loss = 2.99114\n",
      "epoch no.4 train no.298670  loss = 2.09485 avg_loss = 3.04756\n",
      "epoch no.4 train no.298680  loss = 3.21125 avg_loss = 3.03888\n",
      "epoch no.4 train no.298690  loss = 4.30257 avg_loss = 3.03554\n",
      "epoch no.4 train no.298700  loss = 2.08971 avg_loss = 3.02916\n",
      "epoch no.4 train no.298710  loss = 5.60301 avg_loss = 3.03394\n",
      "epoch no.4 train no.298720  loss = 1.87455 avg_loss = 3.01208\n",
      "epoch no.4 train no.298730  loss = 2.30273 avg_loss = 3.04329\n",
      "epoch no.4 train no.298740  loss = 3.75657 avg_loss = 3.10163\n",
      "epoch no.4 train no.298750  loss = 2.31540 avg_loss = 3.09937\n",
      "epoch no.4 train no.298760  loss = 1.86187 avg_loss = 3.10966\n",
      "epoch no.4 train no.298770  loss = 3.99844 avg_loss = 3.11737\n",
      "epoch no.4 train no.298780  loss = 2.68986 avg_loss = 3.12133\n",
      "epoch no.4 train no.298790  loss = 2.39599 avg_loss = 3.05663\n",
      "epoch no.4 train no.298800  loss = 2.93489 avg_loss = 3.06842\n",
      "epoch no.4 train no.298810  loss = 1.98532 avg_loss = 3.03934\n",
      "epoch no.4 train no.298820  loss = 2.65030 avg_loss = 3.05437\n",
      "epoch no.4 train no.298830  loss = 3.72001 avg_loss = 3.01272\n",
      "epoch no.4 train no.298840  loss = 2.88219 avg_loss = 3.00381\n",
      "epoch no.4 train no.298850  loss = 2.70865 avg_loss = 3.02784\n",
      "epoch no.4 train no.298860  loss = 2.01661 avg_loss = 3.03502\n",
      "epoch no.4 train no.298870  loss = 1.41465 avg_loss = 3.08195\n",
      "epoch no.4 train no.298880  loss = 2.52601 avg_loss = 3.03639\n",
      "epoch no.4 train no.298890  loss = 1.70106 avg_loss = 3.02748\n",
      "epoch no.4 train no.298900  loss = 2.67304 avg_loss = 2.99351\n",
      "epoch no.4 train no.298910  loss = 4.37318 avg_loss = 3.01676\n",
      "epoch no.4 train no.298920  loss = 5.14479 avg_loss = 3.01857\n",
      "epoch no.4 train no.298930  loss = 3.24338 avg_loss = 3.03399\n",
      "epoch no.4 train no.298940  loss = 3.77291 avg_loss = 3.02795\n",
      "epoch no.4 train no.298950  loss = 1.35680 avg_loss = 2.99811\n",
      "epoch no.4 train no.298960  loss = 2.20329 avg_loss = 3.04052\n",
      "epoch no.4 train no.298970  loss = 3.26147 avg_loss = 3.00857\n",
      "epoch no.4 train no.298980  loss = 3.54608 avg_loss = 2.99186\n",
      "epoch no.4 train no.298990  loss = 2.63270 avg_loss = 2.97390\n",
      "epoch no.4 train no.299000  loss = 2.54267 avg_loss = 2.98010\n",
      "4\n",
      "to_tokens: ['▁비', '▁인기가', '요', '▁70', '80', '</s>']\n",
      "추억의 인기가요 7080</s>\n",
      "epoch no.4 train no.299010  loss = 2.93282 avg_loss = 2.99135\n",
      "epoch no.4 train no.299020  loss = 2.55741 avg_loss = 2.98649\n",
      "epoch no.4 train no.299030  loss = 2.81714 avg_loss = 3.01041\n",
      "epoch no.4 train no.299040  loss = 3.49270 avg_loss = 3.03485\n",
      "epoch no.4 train no.299050  loss = 2.03889 avg_loss = 3.01704\n",
      "epoch no.4 train no.299060  loss = 3.46134 avg_loss = 3.01469\n",
      "epoch no.4 train no.299070  loss = 2.07691 avg_loss = 3.06210\n",
      "epoch no.4 train no.299080  loss = 2.28219 avg_loss = 3.00236\n",
      "epoch no.4 train no.299090  loss = 3.69565 avg_loss = 3.01246\n",
      "epoch no.4 train no.299100  loss = 2.20936 avg_loss = 3.03466\n",
      "epoch no.4 train no.299110  loss = 5.21930 avg_loss = 3.03845\n",
      "epoch no.4 train no.299120  loss = 2.17279 avg_loss = 3.01156\n",
      "epoch no.4 train no.299130  loss = 4.60777 avg_loss = 3.03808\n",
      "epoch no.4 train no.299140  loss = 2.17225 avg_loss = 3.04134\n",
      "epoch no.4 train no.299150  loss = 3.03345 avg_loss = 3.00274\n",
      "epoch no.4 train no.299160  loss = 2.82402 avg_loss = 3.02098\n",
      "epoch no.4 train no.299170  loss = 2.48286 avg_loss = 2.98492\n",
      "epoch no.4 train no.299180  loss = 2.49667 avg_loss = 2.97237\n",
      "epoch no.4 train no.299190  loss = 3.92945 avg_loss = 3.05242\n",
      "epoch no.4 train no.299200  loss = 3.69722 avg_loss = 3.06098\n",
      "epoch no.4 train no.299210  loss = 3.99215 avg_loss = 3.13033\n",
      "epoch no.4 train no.299220  loss = 2.43027 avg_loss = 3.09445\n",
      "epoch no.4 train no.299230  loss = 2.16335 avg_loss = 3.09667\n",
      "epoch no.4 train no.299240  loss = 2.14586 avg_loss = 3.09452\n",
      "epoch no.4 train no.299250  loss = 2.51365 avg_loss = 3.08476\n",
      "epoch no.4 train no.299260  loss = 3.04327 avg_loss = 3.08890\n",
      "epoch no.4 train no.299270  loss = 3.65161 avg_loss = 3.11285\n",
      "epoch no.4 train no.299280  loss = 4.99190 avg_loss = 3.11877\n",
      "epoch no.4 train no.299290  loss = 3.17681 avg_loss = 3.13498\n",
      "epoch no.4 train no.299300  loss = 4.25401 avg_loss = 3.14411\n",
      "epoch no.4 train no.299310  loss = 3.77977 avg_loss = 3.20029\n",
      "epoch no.4 train no.299320  loss = 2.45411 avg_loss = 3.20599\n",
      "epoch no.4 train no.299330  loss = 2.50870 avg_loss = 3.19762\n",
      "epoch no.4 train no.299340  loss = 4.02247 avg_loss = 3.15122\n",
      "epoch no.4 train no.299350  loss = 4.48212 avg_loss = 3.17875\n",
      "epoch no.4 train no.299360  loss = 2.14069 avg_loss = 3.21742\n",
      "epoch no.4 train no.299370  loss = 3.71876 avg_loss = 3.20490\n",
      "epoch no.4 train no.299380  loss = 4.23593 avg_loss = 3.20447\n",
      "epoch no.4 train no.299390  loss = 4.68998 avg_loss = 3.19348\n",
      "epoch no.4 train no.299400  loss = 1.84395 avg_loss = 3.16169\n",
      "epoch no.4 train no.299410  loss = 2.16602 avg_loss = 3.15518\n",
      "epoch no.4 train no.299420  loss = 2.44312 avg_loss = 3.15499\n",
      "epoch no.4 train no.299430  loss = 2.62948 avg_loss = 3.14799\n",
      "epoch no.4 train no.299440  loss = 2.57090 avg_loss = 3.14407\n",
      "epoch no.4 train no.299450  loss = 2.85891 avg_loss = 3.12349\n",
      "epoch no.4 train no.299460  loss = 3.51867 avg_loss = 3.13202\n",
      "epoch no.4 train no.299470  loss = 2.79747 avg_loss = 3.14476\n",
      "epoch no.4 train no.299480  loss = 3.97655 avg_loss = 3.13986\n",
      "epoch no.4 train no.299490  loss = 4.72453 avg_loss = 3.15202\n",
      "epoch no.4 train no.299500  loss = 4.11721 avg_loss = 3.16717\n",
      "epoch no.4 train no.299510  loss = 4.00067 avg_loss = 3.16488\n",
      "epoch no.4 train no.299520  loss = 4.38159 avg_loss = 3.17780\n",
      "epoch no.4 train no.299530  loss = 1.77955 avg_loss = 3.16540\n",
      "epoch no.4 train no.299540  loss = 4.15791 avg_loss = 3.20132\n",
      "epoch no.4 train no.299550  loss = 2.69444 avg_loss = 3.17631\n",
      "epoch no.4 train no.299560  loss = 4.44865 avg_loss = 3.20686\n",
      "epoch no.4 train no.299570  loss = 3.76401 avg_loss = 3.22096\n",
      "epoch no.4 train no.299580  loss = 3.26860 avg_loss = 3.17482\n",
      "epoch no.4 train no.299590  loss = 3.57615 avg_loss = 3.17400\n",
      "epoch no.4 train no.299600  loss = 1.77767 avg_loss = 3.15431\n",
      "epoch no.4 train no.299610  loss = 2.13456 avg_loss = 3.16903\n",
      "epoch no.4 train no.299620  loss = 3.24051 avg_loss = 3.21335\n",
      "epoch no.4 train no.299630  loss = 2.09919 avg_loss = 3.16139\n",
      "epoch no.4 train no.299640  loss = 2.84067 avg_loss = 3.14742\n",
      "epoch no.4 train no.299650  loss = 2.40047 avg_loss = 3.13455\n",
      "epoch no.4 train no.299660  loss = 4.12875 avg_loss = 3.13350\n",
      "epoch no.4 train no.299670  loss = 3.99968 avg_loss = 3.12040\n",
      "epoch no.4 train no.299680  loss = 2.16945 avg_loss = 3.10719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.299690  loss = 2.79803 avg_loss = 3.11011\n",
      "epoch no.4 train no.299700  loss = 4.58567 avg_loss = 3.11202\n",
      "epoch no.4 train no.299710  loss = 2.36004 avg_loss = 3.09043\n",
      "epoch no.4 train no.299720  loss = 3.09553 avg_loss = 3.08432\n",
      "epoch no.4 train no.299730  loss = 2.80597 avg_loss = 3.06761\n",
      "epoch no.4 train no.299740  loss = 1.65044 avg_loss = 3.06081\n",
      "epoch no.4 train no.299750  loss = 2.58606 avg_loss = 3.03017\n",
      "epoch no.4 train no.299760  loss = 3.53407 avg_loss = 2.99403\n",
      "epoch no.4 train no.299770  loss = 2.35591 avg_loss = 2.99646\n",
      "epoch no.4 train no.299780  loss = 1.98133 avg_loss = 2.99060\n",
      "epoch no.4 train no.299790  loss = 2.81223 avg_loss = 3.01011\n",
      "epoch no.4 train no.299800  loss = 2.58659 avg_loss = 2.97601\n",
      "epoch no.4 train no.299810  loss = 2.39814 avg_loss = 2.96011\n",
      "epoch no.4 train no.299820  loss = 3.19808 avg_loss = 2.98837\n",
      "epoch no.4 train no.299830  loss = 4.12305 avg_loss = 2.97430\n",
      "epoch no.4 train no.299840  loss = 2.62244 avg_loss = 2.99164\n",
      "epoch no.4 train no.299850  loss = 3.98435 avg_loss = 3.00398\n",
      "epoch no.4 train no.299860  loss = 4.04510 avg_loss = 3.03134\n",
      "epoch no.4 train no.299870  loss = 4.58447 avg_loss = 3.05774\n",
      "epoch no.4 train no.299880  loss = 1.65317 avg_loss = 2.99668\n",
      "epoch no.4 train no.299890  loss = 2.70988 avg_loss = 2.97203\n",
      "epoch no.4 train no.299900  loss = 3.27604 avg_loss = 2.97969\n",
      "epoch no.4 train no.299910  loss = 3.15801 avg_loss = 2.97940\n",
      "epoch no.4 train no.299920  loss = 2.71102 avg_loss = 2.99511\n",
      "epoch no.4 train no.299930  loss = 2.54099 avg_loss = 2.98723\n",
      "epoch no.4 train no.299940  loss = 3.62302 avg_loss = 3.00759\n",
      "epoch no.4 train no.299950  loss = 2.60417 avg_loss = 3.00759\n",
      "epoch no.4 train no.299960  loss = 3.05655 avg_loss = 3.01940\n",
      "epoch no.4 train no.299970  loss = 3.89522 avg_loss = 3.03233\n",
      "epoch no.4 train no.299980  loss = 2.08442 avg_loss = 3.04530\n",
      "epoch no.4 train no.299990  loss = 1.73168 avg_loss = 3.08533\n",
      "epoch no.4 train no.300000  loss = 2.77201 avg_loss = 3.07468\n",
      "6\n",
      "to_tokens: ['▁비', '▁2000', '팝', 'st', '▁모음', '곡', '들', '</s>']\n",
      "추억의 인기 ost 명곡 모음</s>\n",
      "epoch no.4 train no.300010  loss = 2.86617 avg_loss = 3.03551\n",
      "epoch no.4 train no.300020  loss = 5.49167 avg_loss = 3.05482\n",
      "epoch no.4 train no.300030  loss = 3.85483 avg_loss = 3.11042\n",
      "epoch no.4 train no.300040  loss = 3.73414 avg_loss = 3.08648\n",
      "epoch no.4 train no.300050  loss = 3.56667 avg_loss = 3.08917\n",
      "epoch no.4 train no.300060  loss = 4.03161 avg_loss = 3.08557\n",
      "epoch no.4 train no.300070  loss = 2.65883 avg_loss = 3.07737\n",
      "epoch no.4 train no.300080  loss = 1.73908 avg_loss = 3.06459\n",
      "epoch no.4 train no.300090  loss = 3.50960 avg_loss = 3.07967\n",
      "epoch no.4 train no.300100  loss = 3.00354 avg_loss = 3.09510\n",
      "epoch no.4 train no.300110  loss = 3.52719 avg_loss = 3.06052\n",
      "epoch no.4 train no.300120  loss = 3.20647 avg_loss = 3.04725\n",
      "epoch no.4 train no.300130  loss = 2.05891 avg_loss = 3.06500\n",
      "epoch no.4 train no.300140  loss = 1.86428 avg_loss = 3.04899\n",
      "epoch no.4 train no.300150  loss = 5.00837 avg_loss = 3.06800\n",
      "epoch no.4 train no.300160  loss = 2.46572 avg_loss = 3.06573\n",
      "epoch no.4 train no.300170  loss = 4.19561 avg_loss = 3.07167\n",
      "epoch no.4 train no.300180  loss = 3.53149 avg_loss = 3.09089\n",
      "epoch no.4 train no.300190  loss = 1.96493 avg_loss = 3.07737\n",
      "epoch no.4 train no.300200  loss = 1.98736 avg_loss = 3.04177\n",
      "epoch no.4 train no.300210  loss = 2.51573 avg_loss = 3.07977\n",
      "epoch no.4 train no.300220  loss = 2.59712 avg_loss = 3.07249\n",
      "epoch no.4 train no.300230  loss = 3.58382 avg_loss = 3.09124\n",
      "epoch no.4 train no.300240  loss = 2.16529 avg_loss = 3.05510\n",
      "epoch no.4 train no.300250  loss = 4.45209 avg_loss = 3.06890\n",
      "epoch no.4 train no.300260  loss = 4.59612 avg_loss = 3.04693\n",
      "epoch no.4 train no.300270  loss = 4.38822 avg_loss = 3.06970\n",
      "epoch no.4 train no.300280  loss = 3.72411 avg_loss = 3.07984\n",
      "epoch no.4 train no.300290  loss = 2.23659 avg_loss = 3.10142\n",
      "epoch no.4 train no.300300  loss = 2.43346 avg_loss = 3.13419\n",
      "epoch no.4 train no.300310  loss = 2.92158 avg_loss = 3.07988\n",
      "epoch no.4 train no.300320  loss = 3.09404 avg_loss = 3.06756\n",
      "epoch no.4 train no.300330  loss = 2.35452 avg_loss = 3.04053\n",
      "epoch no.4 train no.300340  loss = 2.76592 avg_loss = 3.01471\n",
      "epoch no.4 train no.300350  loss = 3.79919 avg_loss = 3.04406\n",
      "epoch no.4 train no.300360  loss = 2.31685 avg_loss = 3.01306\n",
      "epoch no.4 train no.300370  loss = 4.18697 avg_loss = 3.04211\n",
      "epoch no.4 train no.300380  loss = 2.21289 avg_loss = 3.04541\n",
      "epoch no.4 train no.300390  loss = 2.80058 avg_loss = 3.06072\n",
      "epoch no.4 train no.300400  loss = 2.49296 avg_loss = 3.04760\n",
      "epoch no.4 train no.300410  loss = 2.68919 avg_loss = 3.06357\n",
      "epoch no.4 train no.300420  loss = 3.03945 avg_loss = 3.08195\n",
      "epoch no.4 train no.300430  loss = 2.28001 avg_loss = 3.07512\n",
      "epoch no.4 train no.300440  loss = 2.13108 avg_loss = 3.06699\n",
      "epoch no.4 train no.300450  loss = 2.19633 avg_loss = 3.03461\n",
      "epoch no.4 train no.300460  loss = 3.09836 avg_loss = 2.98272\n",
      "epoch no.4 train no.300470  loss = 3.04439 avg_loss = 2.99252\n",
      "epoch no.4 train no.300480  loss = 2.70650 avg_loss = 2.99098\n",
      "epoch no.4 train no.300490  loss = 2.05569 avg_loss = 3.01949\n",
      "epoch no.4 train no.300500  loss = 3.41225 avg_loss = 2.99059\n",
      "epoch no.4 train no.300510  loss = 2.41981 avg_loss = 2.95880\n",
      "epoch no.4 train no.300520  loss = 1.98517 avg_loss = 2.96314\n",
      "epoch no.4 train no.300530  loss = 4.33110 avg_loss = 3.01364\n",
      "epoch no.4 train no.300540  loss = 4.23869 avg_loss = 3.04169\n",
      "epoch no.4 train no.300550  loss = 3.73201 avg_loss = 3.04313\n",
      "epoch no.4 train no.300560  loss = 2.51709 avg_loss = 3.04761\n",
      "epoch no.4 train no.300570  loss = 2.24465 avg_loss = 3.08202\n",
      "epoch no.4 train no.300580  loss = 3.20412 avg_loss = 3.11500\n",
      "epoch no.4 train no.300590  loss = 3.04724 avg_loss = 3.06210\n",
      "epoch no.4 train no.300600  loss = 3.33780 avg_loss = 3.02231\n",
      "epoch no.4 train no.300610  loss = 2.01431 avg_loss = 3.00841\n",
      "epoch no.4 train no.300620  loss = 2.66678 avg_loss = 3.01379\n",
      "epoch no.4 train no.300630  loss = 2.75221 avg_loss = 3.01768\n",
      "epoch no.4 train no.300640  loss = 1.94177 avg_loss = 3.02812\n",
      "epoch no.4 train no.300650  loss = 3.18038 avg_loss = 3.02679\n",
      "epoch no.4 train no.300660  loss = 4.97355 avg_loss = 3.03222\n",
      "epoch no.4 train no.300670  loss = 2.75936 avg_loss = 3.00772\n",
      "epoch no.4 train no.300680  loss = 4.55268 avg_loss = 3.04247\n",
      "epoch no.4 train no.300690  loss = 2.02378 avg_loss = 3.06363\n",
      "epoch no.4 train no.300700  loss = 2.30369 avg_loss = 3.06628\n",
      "epoch no.4 train no.300710  loss = 3.21918 avg_loss = 3.07846\n",
      "epoch no.4 train no.300720  loss = 4.47175 avg_loss = 3.13331\n",
      "epoch no.4 train no.300730  loss = 4.03865 avg_loss = 3.14528\n",
      "epoch no.4 train no.300740  loss = 3.55240 avg_loss = 3.12867\n",
      "epoch no.4 train no.300750  loss = 3.23093 avg_loss = 3.09316\n",
      "epoch no.4 train no.300760  loss = 2.00265 avg_loss = 3.08389\n",
      "epoch no.4 train no.300770  loss = 2.83841 avg_loss = 3.09121\n",
      "epoch no.4 train no.300780  loss = 3.66403 avg_loss = 3.10466\n",
      "epoch no.4 train no.300790  loss = 3.28740 avg_loss = 3.09579\n",
      "epoch no.4 train no.300800  loss = 3.66924 avg_loss = 3.11505\n",
      "epoch no.4 train no.300810  loss = 2.55808 avg_loss = 3.09784\n",
      "epoch no.4 train no.300820  loss = 3.10948 avg_loss = 3.09368\n",
      "epoch no.4 train no.300830  loss = 3.48703 avg_loss = 3.08835\n",
      "epoch no.4 train no.300840  loss = 2.37314 avg_loss = 3.05860\n",
      "epoch no.4 train no.300850  loss = 2.82614 avg_loss = 3.05093\n",
      "epoch no.4 train no.300860  loss = 2.83118 avg_loss = 3.07638\n",
      "epoch no.4 train no.300870  loss = 2.61857 avg_loss = 3.07982\n",
      "epoch no.4 train no.300880  loss = 2.99148 avg_loss = 3.06522\n",
      "epoch no.4 train no.300890  loss = 1.65527 avg_loss = 3.03344\n",
      "epoch no.4 train no.300900  loss = 2.40771 avg_loss = 2.99405\n",
      "epoch no.4 train no.300910  loss = 2.29430 avg_loss = 2.99206\n",
      "epoch no.4 train no.300920  loss = 1.66189 avg_loss = 2.98529\n",
      "epoch no.4 train no.300930  loss = 2.89882 avg_loss = 3.00008\n",
      "epoch no.4 train no.300940  loss = 3.09993 avg_loss = 3.06329\n",
      "epoch no.4 train no.300950  loss = 3.78940 avg_loss = 3.03390\n",
      "epoch no.4 train no.300960  loss = 3.25377 avg_loss = 3.02940\n",
      "epoch no.4 train no.300970  loss = 3.77963 avg_loss = 3.06803\n",
      "epoch no.4 train no.300980  loss = 2.97908 avg_loss = 3.09788\n",
      "epoch no.4 train no.300990  loss = 2.36982 avg_loss = 3.10770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.301000  loss = 2.54364 avg_loss = 3.08766\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁팝', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.301010  loss = 2.29696 avg_loss = 3.08204\n",
      "epoch no.4 train no.301020  loss = 3.95526 avg_loss = 3.06133\n",
      "epoch no.4 train no.301030  loss = 4.02992 avg_loss = 3.04225\n",
      "epoch no.4 train no.301040  loss = 2.74284 avg_loss = 3.02931\n",
      "epoch no.4 train no.301050  loss = 3.07175 avg_loss = 3.04322\n",
      "epoch no.4 train no.301060  loss = 3.09854 avg_loss = 3.03573\n",
      "epoch no.4 train no.301070  loss = 3.41373 avg_loss = 3.03369\n",
      "epoch no.4 train no.301080  loss = 3.03097 avg_loss = 3.02409\n",
      "epoch no.4 train no.301090  loss = 2.98595 avg_loss = 3.08409\n",
      "epoch no.4 train no.301100  loss = 3.09499 avg_loss = 3.11217\n",
      "epoch no.4 train no.301110  loss = 2.80934 avg_loss = 3.12549\n",
      "epoch no.4 train no.301120  loss = 3.19026 avg_loss = 3.12753\n",
      "epoch no.4 train no.301130  loss = 2.52324 avg_loss = 3.11976\n",
      "epoch no.4 train no.301140  loss = 2.03037 avg_loss = 3.12394\n",
      "epoch no.4 train no.301150  loss = 2.95104 avg_loss = 3.17393\n",
      "epoch no.4 train no.301160  loss = 3.25624 avg_loss = 3.15778\n",
      "epoch no.4 train no.301170  loss = 2.11857 avg_loss = 3.11324\n",
      "epoch no.4 train no.301180  loss = 2.55506 avg_loss = 3.09267\n",
      "epoch no.4 train no.301190  loss = 2.93809 avg_loss = 3.10490\n",
      "epoch no.4 train no.301200  loss = 2.84856 avg_loss = 3.10937\n",
      "epoch no.4 train no.301210  loss = 1.82375 avg_loss = 3.10906\n",
      "epoch no.4 train no.301220  loss = 2.08410 avg_loss = 3.12240\n",
      "epoch no.4 train no.301230  loss = 3.14122 avg_loss = 3.12521\n",
      "epoch no.4 train no.301240  loss = 2.60259 avg_loss = 3.08579\n",
      "epoch no.4 train no.301250  loss = 1.84025 avg_loss = 3.04854\n",
      "epoch no.4 train no.301260  loss = 3.62025 avg_loss = 3.04081\n",
      "epoch no.4 train no.301270  loss = 2.78221 avg_loss = 3.06007\n",
      "epoch no.4 train no.301280  loss = 3.12458 avg_loss = 3.14072\n",
      "epoch no.4 train no.301290  loss = 1.94077 avg_loss = 3.12131\n",
      "epoch no.4 train no.301300  loss = 1.68461 avg_loss = 3.11893\n",
      "epoch no.4 train no.301310  loss = 1.96778 avg_loss = 3.08641\n",
      "epoch no.4 train no.301320  loss = 3.28841 avg_loss = 3.07768\n",
      "epoch no.4 train no.301330  loss = 3.02689 avg_loss = 3.10805\n",
      "epoch no.4 train no.301340  loss = 4.72716 avg_loss = 3.10331\n",
      "epoch no.4 train no.301350  loss = 3.24342 avg_loss = 3.07465\n",
      "epoch no.4 train no.301360  loss = 2.08071 avg_loss = 3.07336\n",
      "epoch no.4 train no.301370  loss = 2.63580 avg_loss = 3.08585\n",
      "epoch no.4 train no.301380  loss = 4.52220 avg_loss = 3.14671\n",
      "epoch no.4 train no.301390  loss = 3.28149 avg_loss = 3.12620\n",
      "epoch no.4 train no.301400  loss = 2.92120 avg_loss = 3.13599\n",
      "epoch no.4 train no.301410  loss = 2.80510 avg_loss = 3.13644\n",
      "epoch no.4 train no.301420  loss = 3.20745 avg_loss = 3.10249\n",
      "epoch no.4 train no.301430  loss = 3.97048 avg_loss = 3.14478\n",
      "epoch no.4 train no.301440  loss = 1.99005 avg_loss = 3.12064\n",
      "epoch no.4 train no.301450  loss = 3.23054 avg_loss = 3.16050\n",
      "epoch no.4 train no.301460  loss = 1.79406 avg_loss = 3.16344\n",
      "epoch no.4 train no.301470  loss = 2.26091 avg_loss = 3.12509\n",
      "epoch no.4 train no.301480  loss = 3.17450 avg_loss = 3.14009\n",
      "epoch no.4 train no.301490  loss = 2.51127 avg_loss = 3.15187\n",
      "epoch no.4 train no.301500  loss = 2.82893 avg_loss = 3.14326\n",
      "epoch no.4 train no.301510  loss = 2.83343 avg_loss = 3.10340\n",
      "epoch no.4 train no.301520  loss = 1.85210 avg_loss = 3.07462\n",
      "epoch no.4 train no.301530  loss = 4.75439 avg_loss = 3.08462\n",
      "epoch no.4 train no.301540  loss = 3.17993 avg_loss = 3.06210\n",
      "epoch no.4 train no.301550  loss = 2.37005 avg_loss = 3.09715\n",
      "epoch no.4 train no.301560  loss = 2.46138 avg_loss = 3.14034\n",
      "epoch no.4 train no.301570  loss = 3.77945 avg_loss = 3.14934\n",
      "epoch no.4 train no.301580  loss = 2.32718 avg_loss = 3.11273\n",
      "epoch no.4 train no.301590  loss = 2.52366 avg_loss = 3.08885\n",
      "epoch no.4 train no.301600  loss = 3.81601 avg_loss = 3.10301\n",
      "epoch no.4 train no.301610  loss = 3.42956 avg_loss = 3.10007\n",
      "epoch no.4 train no.301620  loss = 3.42657 avg_loss = 3.09864\n",
      "epoch no.4 train no.301630  loss = 1.46591 avg_loss = 3.05725\n",
      "epoch no.4 train no.301640  loss = 5.19605 avg_loss = 3.09831\n",
      "epoch no.4 train no.301650  loss = 3.80053 avg_loss = 3.15561\n",
      "epoch no.4 train no.301660  loss = 3.26781 avg_loss = 3.11807\n",
      "epoch no.4 train no.301670  loss = 2.01829 avg_loss = 3.09326\n",
      "epoch no.4 train no.301680  loss = 2.54220 avg_loss = 3.08381\n",
      "epoch no.4 train no.301690  loss = 2.76773 avg_loss = 3.10748\n",
      "epoch no.4 train no.301700  loss = 2.31108 avg_loss = 3.09595\n",
      "epoch no.4 train no.301710  loss = 3.14858 avg_loss = 3.07073\n",
      "epoch no.4 train no.301720  loss = 2.56892 avg_loss = 3.06100\n",
      "epoch no.4 train no.301730  loss = 4.32425 avg_loss = 3.05917\n",
      "epoch no.4 train no.301740  loss = 3.30000 avg_loss = 3.03353\n",
      "epoch no.4 train no.301750  loss = 3.76754 avg_loss = 3.05666\n",
      "epoch no.4 train no.301760  loss = 3.30329 avg_loss = 3.04910\n",
      "epoch no.4 train no.301770  loss = 3.28598 avg_loss = 3.06844\n",
      "epoch no.4 train no.301780  loss = 2.79013 avg_loss = 3.03142\n",
      "epoch no.4 train no.301790  loss = 3.78515 avg_loss = 3.05796\n",
      "epoch no.4 train no.301800  loss = 4.26199 avg_loss = 3.09675\n",
      "epoch no.4 train no.301810  loss = 3.53525 avg_loss = 3.07068\n",
      "epoch no.4 train no.301820  loss = 4.13106 avg_loss = 3.09155\n",
      "epoch no.4 train no.301830  loss = 2.62813 avg_loss = 3.11331\n",
      "epoch no.4 train no.301840  loss = 1.74115 avg_loss = 3.11586\n",
      "epoch no.4 train no.301850  loss = 2.30814 avg_loss = 3.11510\n",
      "epoch no.4 train no.301860  loss = 2.59981 avg_loss = 3.09503\n",
      "epoch no.4 train no.301870  loss = 2.91352 avg_loss = 3.07109\n",
      "epoch no.4 train no.301880  loss = 3.11647 avg_loss = 3.01183\n",
      "epoch no.4 train no.301890  loss = 2.26028 avg_loss = 3.01072\n",
      "epoch no.4 train no.301900  loss = 3.15146 avg_loss = 3.00353\n",
      "epoch no.4 train no.301910  loss = 2.48810 avg_loss = 2.98817\n",
      "epoch no.4 train no.301920  loss = 2.65728 avg_loss = 3.06575\n",
      "epoch no.4 train no.301930  loss = 2.35724 avg_loss = 3.05209\n",
      "epoch no.4 train no.301940  loss = 2.92779 avg_loss = 3.01116\n",
      "epoch no.4 train no.301950  loss = 1.97680 avg_loss = 2.97426\n",
      "epoch no.4 train no.301960  loss = 3.62914 avg_loss = 2.99764\n",
      "epoch no.4 train no.301970  loss = 3.14014 avg_loss = 3.01658\n",
      "epoch no.4 train no.301980  loss = 2.90046 avg_loss = 3.04765\n",
      "epoch no.4 train no.301990  loss = 2.88465 avg_loss = 3.04512\n",
      "epoch no.4 train no.302000  loss = 2.08437 avg_loss = 3.04366\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.302010  loss = 2.48425 avg_loss = 3.03730\n",
      "epoch no.4 train no.302020  loss = 2.75729 avg_loss = 3.01181\n",
      "epoch no.4 train no.302030  loss = 3.71736 avg_loss = 3.02166\n",
      "epoch no.4 train no.302040  loss = 2.91174 avg_loss = 3.05654\n",
      "epoch no.4 train no.302050  loss = 2.44331 avg_loss = 3.07442\n",
      "epoch no.4 train no.302060  loss = 3.22239 avg_loss = 3.12311\n",
      "epoch no.4 train no.302070  loss = 2.95849 avg_loss = 3.12277\n",
      "epoch no.4 train no.302080  loss = 3.46017 avg_loss = 3.15374\n",
      "epoch no.4 train no.302090  loss = 3.73547 avg_loss = 3.13916\n",
      "epoch no.4 train no.302100  loss = 2.49378 avg_loss = 3.09272\n",
      "epoch no.4 train no.302110  loss = 2.57070 avg_loss = 3.06583\n",
      "epoch no.4 train no.302120  loss = 2.92107 avg_loss = 3.05935\n",
      "epoch no.4 train no.302130  loss = 4.60352 avg_loss = 3.05131\n",
      "epoch no.4 train no.302140  loss = 3.40701 avg_loss = 3.07267\n",
      "epoch no.4 train no.302150  loss = 2.86150 avg_loss = 3.06983\n",
      "epoch no.4 train no.302160  loss = 4.06218 avg_loss = 3.05427\n",
      "epoch no.4 train no.302170  loss = 2.76031 avg_loss = 3.08667\n",
      "epoch no.4 train no.302180  loss = 3.44053 avg_loss = 3.10314\n",
      "epoch no.4 train no.302190  loss = 4.86605 avg_loss = 3.14426\n",
      "epoch no.4 train no.302200  loss = 4.95905 avg_loss = 3.15731\n",
      "epoch no.4 train no.302210  loss = 4.66072 avg_loss = 3.18046\n",
      "epoch no.4 train no.302220  loss = 3.85091 avg_loss = 3.23498\n",
      "epoch no.4 train no.302230  loss = 3.81157 avg_loss = 3.24660\n",
      "epoch no.4 train no.302240  loss = 2.61919 avg_loss = 3.23158\n",
      "epoch no.4 train no.302250  loss = 2.25820 avg_loss = 3.21130\n",
      "epoch no.4 train no.302260  loss = 2.63011 avg_loss = 3.18826\n",
      "epoch no.4 train no.302270  loss = 3.10517 avg_loss = 3.17324\n",
      "epoch no.4 train no.302280  loss = 3.52889 avg_loss = 3.16209\n",
      "epoch no.4 train no.302290  loss = 4.56976 avg_loss = 3.17502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.302300  loss = 3.28934 avg_loss = 3.16330\n",
      "epoch no.4 train no.302310  loss = 3.61069 avg_loss = 3.17262\n",
      "epoch no.4 train no.302320  loss = 4.00752 avg_loss = 3.16985\n",
      "epoch no.4 train no.302330  loss = 2.47104 avg_loss = 3.12065\n",
      "epoch no.4 train no.302340  loss = 2.32081 avg_loss = 3.11204\n",
      "epoch no.4 train no.302350  loss = 3.13802 avg_loss = 3.13425\n",
      "epoch no.4 train no.302360  loss = 2.88440 avg_loss = 3.10561\n",
      "epoch no.4 train no.302370  loss = 3.09171 avg_loss = 3.13330\n",
      "epoch no.4 train no.302380  loss = 3.00581 avg_loss = 3.10228\n",
      "epoch no.4 train no.302390  loss = 2.62667 avg_loss = 3.17753\n",
      "epoch no.4 train no.302400  loss = 3.32588 avg_loss = 3.14639\n",
      "epoch no.4 train no.302410  loss = 3.86798 avg_loss = 3.12821\n",
      "epoch no.4 train no.302420  loss = 3.52369 avg_loss = 3.09915\n",
      "epoch no.4 train no.302430  loss = 2.35684 avg_loss = 3.08633\n",
      "epoch no.4 train no.302440  loss = 2.11516 avg_loss = 3.06167\n",
      "epoch no.4 train no.302450  loss = 4.62721 avg_loss = 3.10102\n",
      "epoch no.4 train no.302460  loss = 3.18919 avg_loss = 3.11331\n",
      "epoch no.4 train no.302470  loss = 1.78060 avg_loss = 3.10149\n",
      "epoch no.4 train no.302480  loss = 2.10106 avg_loss = 3.09353\n",
      "epoch no.4 train no.302490  loss = 3.28379 avg_loss = 3.11835\n",
      "epoch no.4 train no.302500  loss = 2.79631 avg_loss = 3.08523\n",
      "epoch no.4 train no.302510  loss = 3.79524 avg_loss = 3.05449\n",
      "epoch no.4 train no.302520  loss = 3.56084 avg_loss = 3.06330\n",
      "epoch no.4 train no.302530  loss = 1.94711 avg_loss = 3.06417\n",
      "epoch no.4 train no.302540  loss = 3.82577 avg_loss = 3.07736\n",
      "epoch no.4 train no.302550  loss = 2.91568 avg_loss = 3.08911\n",
      "epoch no.4 train no.302560  loss = 4.06976 avg_loss = 3.07099\n",
      "epoch no.4 train no.302570  loss = 4.43687 avg_loss = 3.09495\n",
      "epoch no.4 train no.302580  loss = 5.49601 avg_loss = 3.11620\n",
      "epoch no.4 train no.302590  loss = 2.72624 avg_loss = 3.06472\n",
      "epoch no.4 train no.302600  loss = 2.06359 avg_loss = 3.04639\n",
      "epoch no.4 train no.302610  loss = 3.63922 avg_loss = 3.01773\n",
      "epoch no.4 train no.302620  loss = 2.93168 avg_loss = 3.05057\n",
      "epoch no.4 train no.302630  loss = 4.19171 avg_loss = 3.10586\n",
      "epoch no.4 train no.302640  loss = 2.32723 avg_loss = 3.07461\n",
      "epoch no.4 train no.302650  loss = 3.01167 avg_loss = 3.10528\n",
      "epoch no.4 train no.302660  loss = 2.51339 avg_loss = 3.09861\n",
      "epoch no.4 train no.302670  loss = 3.37963 avg_loss = 3.11168\n",
      "epoch no.4 train no.302680  loss = 2.52668 avg_loss = 3.08847\n",
      "epoch no.4 train no.302690  loss = 2.83403 avg_loss = 3.10158\n",
      "epoch no.4 train no.302700  loss = 3.26150 avg_loss = 3.09238\n",
      "epoch no.4 train no.302710  loss = 3.05779 avg_loss = 3.07369\n",
      "epoch no.4 train no.302720  loss = 2.32677 avg_loss = 3.06944\n",
      "epoch no.4 train no.302730  loss = 3.85883 avg_loss = 3.08107\n",
      "epoch no.4 train no.302740  loss = 2.75046 avg_loss = 3.07746\n",
      "epoch no.4 train no.302750  loss = 4.40911 avg_loss = 3.05133\n",
      "epoch no.4 train no.302760  loss = 2.46134 avg_loss = 3.01983\n",
      "epoch no.4 train no.302770  loss = 2.72427 avg_loss = 3.00263\n",
      "epoch no.4 train no.302780  loss = 3.38786 avg_loss = 3.00143\n",
      "epoch no.4 train no.302790  loss = 3.79744 avg_loss = 3.03484\n",
      "epoch no.4 train no.302800  loss = 1.11406 avg_loss = 3.02375\n",
      "epoch no.4 train no.302810  loss = 3.83121 avg_loss = 3.06308\n",
      "epoch no.4 train no.302820  loss = 4.39612 avg_loss = 3.06296\n",
      "epoch no.4 train no.302830  loss = 4.68373 avg_loss = 3.05455\n",
      "epoch no.4 train no.302840  loss = 3.02902 avg_loss = 3.07398\n",
      "epoch no.4 train no.302850  loss = 3.40953 avg_loss = 3.04225\n",
      "epoch no.4 train no.302860  loss = 2.00739 avg_loss = 3.00249\n",
      "epoch no.4 train no.302870  loss = 2.28394 avg_loss = 2.99376\n",
      "epoch no.4 train no.302880  loss = 3.62694 avg_loss = 2.99127\n",
      "epoch no.4 train no.302890  loss = 3.90692 avg_loss = 3.03559\n",
      "epoch no.4 train no.302900  loss = 2.15452 avg_loss = 3.03393\n",
      "epoch no.4 train no.302910  loss = 2.21420 avg_loss = 3.02255\n",
      "epoch no.4 train no.302920  loss = 3.60780 avg_loss = 3.00221\n",
      "epoch no.4 train no.302930  loss = 1.95102 avg_loss = 2.98855\n",
      "epoch no.4 train no.302940  loss = 2.30289 avg_loss = 2.97799\n",
      "epoch no.4 train no.302950  loss = 3.04699 avg_loss = 3.00666\n",
      "epoch no.4 train no.302960  loss = 2.76129 avg_loss = 2.99181\n",
      "epoch no.4 train no.302970  loss = 3.72307 avg_loss = 3.02557\n",
      "epoch no.4 train no.302980  loss = 3.75374 avg_loss = 3.02544\n",
      "epoch no.4 train no.302990  loss = 4.36719 avg_loss = 3.03170\n",
      "epoch no.4 train no.303000  loss = 3.51325 avg_loss = 3.07206\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.303010  loss = 2.05658 avg_loss = 3.07356\n",
      "epoch no.4 train no.303020  loss = 3.02655 avg_loss = 3.07963\n",
      "epoch no.4 train no.303030  loss = 2.18313 avg_loss = 3.05917\n",
      "epoch no.4 train no.303040  loss = 3.29445 avg_loss = 3.03566\n",
      "epoch no.4 train no.303050  loss = 2.55012 avg_loss = 3.07027\n",
      "epoch no.4 train no.303060  loss = 2.86627 avg_loss = 3.04653\n",
      "epoch no.4 train no.303070  loss = 2.30333 avg_loss = 3.05382\n",
      "epoch no.4 train no.303080  loss = 2.49713 avg_loss = 3.05853\n",
      "epoch no.4 train no.303090  loss = 3.14769 avg_loss = 3.07478\n",
      "epoch no.4 train no.303100  loss = 3.71804 avg_loss = 3.07333\n",
      "epoch no.4 train no.303110  loss = 1.77166 avg_loss = 3.08266\n",
      "epoch no.4 train no.303120  loss = 6.57836 avg_loss = 3.12895\n",
      "epoch no.4 train no.303130  loss = 3.46614 avg_loss = 3.09819\n",
      "epoch no.4 train no.303140  loss = 3.65903 avg_loss = 3.06542\n",
      "epoch no.4 train no.303150  loss = 3.32613 avg_loss = 3.04068\n",
      "epoch no.4 train no.303160  loss = 1.97839 avg_loss = 3.03569\n",
      "epoch no.4 train no.303170  loss = 2.37448 avg_loss = 3.01666\n",
      "epoch no.4 train no.303180  loss = 3.55046 avg_loss = 3.06439\n",
      "epoch no.4 train no.303190  loss = 2.95763 avg_loss = 3.03522\n",
      "epoch no.4 train no.303200  loss = 3.45103 avg_loss = 3.08106\n",
      "epoch no.4 train no.303210  loss = 2.21368 avg_loss = 3.07787\n",
      "epoch no.4 train no.303220  loss = 2.98752 avg_loss = 3.06699\n",
      "epoch no.4 train no.303230  loss = 3.16285 avg_loss = 3.05011\n",
      "epoch no.4 train no.303240  loss = 1.94584 avg_loss = 3.06870\n",
      "epoch no.4 train no.303250  loss = 1.72016 avg_loss = 3.06323\n",
      "epoch no.4 train no.303260  loss = 4.07834 avg_loss = 3.11440\n",
      "epoch no.4 train no.303270  loss = 2.66963 avg_loss = 3.10967\n",
      "epoch no.4 train no.303280  loss = 2.91608 avg_loss = 3.14350\n",
      "epoch no.4 train no.303290  loss = 3.25033 avg_loss = 3.11399\n",
      "epoch no.4 train no.303300  loss = 2.94132 avg_loss = 3.12444\n",
      "epoch no.4 train no.303310  loss = 2.43587 avg_loss = 3.10530\n",
      "epoch no.4 train no.303320  loss = 3.38603 avg_loss = 3.15084\n",
      "epoch no.4 train no.303330  loss = 5.51350 avg_loss = 3.17755\n",
      "epoch no.4 train no.303340  loss = 2.25474 avg_loss = 3.18027\n",
      "epoch no.4 train no.303350  loss = 3.96926 avg_loss = 3.16392\n",
      "epoch no.4 train no.303360  loss = 2.31201 avg_loss = 3.13686\n",
      "epoch no.4 train no.303370  loss = 2.72166 avg_loss = 3.12856\n",
      "epoch no.4 train no.303380  loss = 2.17250 avg_loss = 3.08576\n",
      "epoch no.4 train no.303390  loss = 2.10939 avg_loss = 3.08404\n",
      "epoch no.4 train no.303400  loss = 3.16318 avg_loss = 3.09221\n",
      "epoch no.4 train no.303410  loss = 2.81502 avg_loss = 3.08895\n",
      "epoch no.4 train no.303420  loss = 5.01884 avg_loss = 3.13390\n",
      "epoch no.4 train no.303430  loss = 2.68370 avg_loss = 3.14083\n",
      "epoch no.4 train no.303440  loss = 2.04495 avg_loss = 3.10179\n",
      "epoch no.4 train no.303450  loss = 2.03582 avg_loss = 3.09859\n",
      "epoch no.4 train no.303460  loss = 2.50381 avg_loss = 3.10887\n",
      "epoch no.4 train no.303470  loss = 1.83251 avg_loss = 3.11533\n",
      "epoch no.4 train no.303480  loss = 3.80476 avg_loss = 3.12126\n",
      "epoch no.4 train no.303490  loss = 2.85790 avg_loss = 3.10726\n",
      "epoch no.4 train no.303500  loss = 3.58130 avg_loss = 3.08546\n",
      "epoch no.4 train no.303510  loss = 2.94278 avg_loss = 3.10869\n",
      "epoch no.4 train no.303520  loss = 3.16118 avg_loss = 3.11223\n",
      "epoch no.4 train no.303530  loss = 4.31276 avg_loss = 3.14935\n",
      "epoch no.4 train no.303540  loss = 2.72114 avg_loss = 3.12688\n",
      "epoch no.4 train no.303550  loss = 2.07351 avg_loss = 3.12053\n",
      "epoch no.4 train no.303560  loss = 2.86195 avg_loss = 3.15708\n",
      "epoch no.4 train no.303570  loss = 2.72545 avg_loss = 3.12747\n",
      "epoch no.4 train no.303580  loss = 3.92136 avg_loss = 3.13301\n",
      "epoch no.4 train no.303590  loss = 2.85840 avg_loss = 3.12951\n",
      "epoch no.4 train no.303600  loss = 2.34223 avg_loss = 3.11186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.303610  loss = 3.30526 avg_loss = 3.10169\n",
      "epoch no.4 train no.303620  loss = 2.27776 avg_loss = 3.09963\n",
      "epoch no.4 train no.303630  loss = 3.47326 avg_loss = 3.10101\n",
      "epoch no.4 train no.303640  loss = 4.31190 avg_loss = 3.06528\n",
      "epoch no.4 train no.303650  loss = 2.53806 avg_loss = 3.07993\n",
      "epoch no.4 train no.303660  loss = 3.12119 avg_loss = 3.07600\n",
      "epoch no.4 train no.303670  loss = 2.43051 avg_loss = 3.07767\n",
      "epoch no.4 train no.303680  loss = 2.49592 avg_loss = 3.08086\n",
      "epoch no.4 train no.303690  loss = 2.37547 avg_loss = 3.05967\n",
      "epoch no.4 train no.303700  loss = 2.72036 avg_loss = 3.04910\n",
      "epoch no.4 train no.303710  loss = 2.21929 avg_loss = 3.07323\n",
      "epoch no.4 train no.303720  loss = 3.29077 avg_loss = 3.08704\n",
      "epoch no.4 train no.303730  loss = 2.47809 avg_loss = 3.10075\n",
      "epoch no.4 train no.303740  loss = 2.85650 avg_loss = 3.07793\n",
      "epoch no.4 train no.303750  loss = 3.64321 avg_loss = 3.09682\n",
      "epoch no.4 train no.303760  loss = 2.71808 avg_loss = 3.10978\n",
      "epoch no.4 train no.303770  loss = 1.89578 avg_loss = 3.07448\n",
      "epoch no.4 train no.303780  loss = 3.61879 avg_loss = 3.04249\n",
      "epoch no.4 train no.303790  loss = 3.07859 avg_loss = 3.01477\n",
      "epoch no.4 train no.303800  loss = 3.03241 avg_loss = 3.01555\n",
      "epoch no.4 train no.303810  loss = 2.58337 avg_loss = 3.04347\n",
      "epoch no.4 train no.303820  loss = 2.67446 avg_loss = 3.05039\n",
      "epoch no.4 train no.303830  loss = 1.37939 avg_loss = 3.03067\n",
      "epoch no.4 train no.303840  loss = 2.18880 avg_loss = 3.00488\n",
      "epoch no.4 train no.303850  loss = 1.52295 avg_loss = 2.99958\n",
      "epoch no.4 train no.303860  loss = 1.82080 avg_loss = 2.99564\n",
      "epoch no.4 train no.303870  loss = 3.01447 avg_loss = 3.01298\n",
      "epoch no.4 train no.303880  loss = 3.22894 avg_loss = 3.01618\n",
      "epoch no.4 train no.303890  loss = 2.76846 avg_loss = 3.05177\n",
      "epoch no.4 train no.303900  loss = 2.76378 avg_loss = 3.06323\n",
      "epoch no.4 train no.303910  loss = 3.55652 avg_loss = 3.03234\n",
      "epoch no.4 train no.303920  loss = 2.14331 avg_loss = 3.03760\n",
      "epoch no.4 train no.303930  loss = 3.17507 avg_loss = 3.02971\n",
      "epoch no.4 train no.303940  loss = 2.20953 avg_loss = 3.03923\n",
      "epoch no.4 train no.303950  loss = 3.64551 avg_loss = 3.06847\n",
      "epoch no.4 train no.303960  loss = 3.02857 avg_loss = 3.05203\n",
      "epoch no.4 train no.303970  loss = 2.26327 avg_loss = 3.10199\n",
      "epoch no.4 train no.303980  loss = 3.05760 avg_loss = 3.09751\n",
      "epoch no.4 train no.303990  loss = 4.43112 avg_loss = 3.11326\n",
      "epoch no.4 train no.304000  loss = 3.03376 avg_loss = 3.10888\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '송', '▁모음', '곡', '</s>', '</s>']\n",
      "추억의 팝송 명곡들</s>\n",
      "epoch no.4 train no.304010  loss = 3.06551 avg_loss = 3.14888\n",
      "epoch no.4 train no.304020  loss = 3.89691 avg_loss = 3.20072\n",
      "epoch no.4 train no.304030  loss = 2.09713 avg_loss = 3.16695\n",
      "epoch no.4 train no.304040  loss = 3.13397 avg_loss = 3.16027\n",
      "epoch no.4 train no.304050  loss = 2.75325 avg_loss = 3.09710\n",
      "epoch no.4 train no.304060  loss = 3.74434 avg_loss = 3.08702\n",
      "epoch no.4 train no.304070  loss = 2.95999 avg_loss = 3.08296\n",
      "epoch no.4 train no.304080  loss = 3.34111 avg_loss = 3.09413\n",
      "epoch no.4 train no.304090  loss = 2.04925 avg_loss = 3.12023\n",
      "epoch no.4 train no.304100  loss = 1.92486 avg_loss = 3.10131\n",
      "epoch no.4 train no.304110  loss = 2.79149 avg_loss = 3.11252\n",
      "epoch no.4 train no.304120  loss = 3.25345 avg_loss = 3.12857\n",
      "epoch no.4 train no.304130  loss = 5.15692 avg_loss = 3.19808\n",
      "epoch no.4 train no.304140  loss = 1.69449 avg_loss = 3.19767\n",
      "epoch no.4 train no.304150  loss = 2.30024 avg_loss = 3.18902\n",
      "epoch no.4 train no.304160  loss = 3.06801 avg_loss = 3.17972\n",
      "epoch no.4 train no.304170  loss = 2.40375 avg_loss = 3.15429\n",
      "epoch no.4 train no.304180  loss = 3.57127 avg_loss = 3.18463\n",
      "epoch no.4 train no.304190  loss = 2.98800 avg_loss = 3.19452\n",
      "epoch no.4 train no.304200  loss = 2.83296 avg_loss = 3.14772\n",
      "epoch no.4 train no.304210  loss = 2.00850 avg_loss = 3.12203\n",
      "epoch no.4 train no.304220  loss = 2.42554 avg_loss = 3.10540\n",
      "epoch no.4 train no.304230  loss = 4.09397 avg_loss = 3.08987\n",
      "epoch no.4 train no.304240  loss = 3.06576 avg_loss = 3.06900\n",
      "epoch no.4 train no.304250  loss = 2.16977 avg_loss = 3.05375\n",
      "epoch no.4 train no.304260  loss = 2.35540 avg_loss = 3.06133\n",
      "epoch no.4 train no.304270  loss = 2.69328 avg_loss = 3.06399\n",
      "epoch no.4 train no.304280  loss = 2.93982 avg_loss = 3.05048\n",
      "epoch no.4 train no.304290  loss = 5.16063 avg_loss = 3.09306\n",
      "epoch no.4 train no.304300  loss = 4.18041 avg_loss = 3.07861\n",
      "epoch no.4 train no.304310  loss = 2.18626 avg_loss = 3.10326\n",
      "epoch no.4 train no.304320  loss = 3.40273 avg_loss = 3.10801\n",
      "epoch no.4 train no.304330  loss = 2.13844 avg_loss = 3.13982\n",
      "epoch no.4 train no.304340  loss = 2.18925 avg_loss = 3.11315\n",
      "epoch no.4 train no.304350  loss = 3.65064 avg_loss = 3.12378\n",
      "epoch no.4 train no.304360  loss = 4.14068 avg_loss = 3.21042\n",
      "epoch no.4 train no.304370  loss = 3.38911 avg_loss = 3.21191\n",
      "epoch no.4 train no.304380  loss = 2.05741 avg_loss = 3.15790\n",
      "epoch no.4 train no.304390  loss = 3.64184 avg_loss = 3.16298\n",
      "epoch no.4 train no.304400  loss = 2.66421 avg_loss = 3.16292\n",
      "epoch no.4 train no.304410  loss = 3.53210 avg_loss = 3.18817\n",
      "epoch no.4 train no.304420  loss = 3.40392 avg_loss = 3.23461\n",
      "epoch no.4 train no.304430  loss = 4.26807 avg_loss = 3.20686\n",
      "epoch no.4 train no.304440  loss = 3.01344 avg_loss = 3.22243\n",
      "epoch no.4 train no.304450  loss = 4.34684 avg_loss = 3.19551\n",
      "epoch no.4 train no.304460  loss = 3.65649 avg_loss = 3.19489\n",
      "epoch no.4 train no.304470  loss = 4.49298 avg_loss = 3.21501\n",
      "epoch no.4 train no.304480  loss = 3.50455 avg_loss = 3.17974\n",
      "epoch no.4 train no.304490  loss = 4.24484 avg_loss = 3.18163\n",
      "epoch no.4 train no.304500  loss = 5.43487 avg_loss = 3.16362\n",
      "epoch no.4 train no.304510  loss = 1.82797 avg_loss = 3.11182\n",
      "epoch no.4 train no.304520  loss = 2.59950 avg_loss = 3.09221\n",
      "epoch no.4 train no.304530  loss = 2.19005 avg_loss = 3.05360\n",
      "epoch no.4 train no.304540  loss = 2.21668 avg_loss = 3.05605\n",
      "epoch no.4 train no.304550  loss = 3.77193 avg_loss = 3.06515\n",
      "epoch no.4 train no.304560  loss = 2.53412 avg_loss = 3.11268\n",
      "epoch no.4 train no.304570  loss = 2.88647 avg_loss = 3.07760\n",
      "epoch no.4 train no.304580  loss = 2.55299 avg_loss = 3.11146\n",
      "epoch no.4 train no.304590  loss = 2.98323 avg_loss = 3.13752\n",
      "epoch no.4 train no.304600  loss = 4.76497 avg_loss = 3.13217\n",
      "epoch no.4 train no.304610  loss = 1.83630 avg_loss = 3.10409\n",
      "epoch no.4 train no.304620  loss = 2.40037 avg_loss = 3.06225\n",
      "epoch no.4 train no.304630  loss = 3.95032 avg_loss = 3.07335\n",
      "epoch no.4 train no.304640  loss = 2.34982 avg_loss = 3.09774\n",
      "epoch no.4 train no.304650  loss = 2.19771 avg_loss = 3.14255\n",
      "epoch no.4 train no.304660  loss = 3.87774 avg_loss = 3.18533\n",
      "epoch no.4 train no.304670  loss = 2.61845 avg_loss = 3.19783\n",
      "epoch no.4 train no.304680  loss = 2.55207 avg_loss = 3.19352\n",
      "epoch no.4 train no.304690  loss = 2.69847 avg_loss = 3.17387\n",
      "epoch no.4 train no.304700  loss = 2.12260 avg_loss = 3.19796\n",
      "epoch no.4 train no.304710  loss = 4.14134 avg_loss = 3.17830\n",
      "epoch no.4 train no.304720  loss = 1.64162 avg_loss = 3.16992\n",
      "epoch no.4 train no.304730  loss = 2.94771 avg_loss = 3.16771\n",
      "epoch no.4 train no.304740  loss = 2.53304 avg_loss = 3.12588\n",
      "epoch no.4 train no.304750  loss = 4.31568 avg_loss = 3.11542\n",
      "epoch no.4 train no.304760  loss = 2.38486 avg_loss = 3.07167\n",
      "epoch no.4 train no.304770  loss = 3.99289 avg_loss = 3.09444\n",
      "epoch no.4 train no.304780  loss = 3.45891 avg_loss = 3.08884\n",
      "epoch no.4 train no.304790  loss = 2.70342 avg_loss = 3.10334\n",
      "epoch no.4 train no.304800  loss = 3.65917 avg_loss = 3.10855\n",
      "epoch no.4 train no.304810  loss = 4.42066 avg_loss = 3.11249\n",
      "epoch no.4 train no.304820  loss = 3.30182 avg_loss = 3.11577\n",
      "epoch no.4 train no.304830  loss = 3.09235 avg_loss = 3.17784\n",
      "epoch no.4 train no.304840  loss = 3.09866 avg_loss = 3.12218\n",
      "epoch no.4 train no.304850  loss = 2.48754 avg_loss = 3.10302\n",
      "epoch no.4 train no.304860  loss = 4.04676 avg_loss = 3.09256\n",
      "epoch no.4 train no.304870  loss = 3.16435 avg_loss = 3.10379\n",
      "epoch no.4 train no.304880  loss = 3.91706 avg_loss = 3.07767\n",
      "epoch no.4 train no.304890  loss = 3.56235 avg_loss = 3.06457\n",
      "epoch no.4 train no.304900  loss = 3.69071 avg_loss = 3.04199\n",
      "epoch no.4 train no.304910  loss = 3.54519 avg_loss = 3.01685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.304920  loss = 3.01099 avg_loss = 2.99973\n",
      "epoch no.4 train no.304930  loss = 2.44873 avg_loss = 3.01006\n",
      "epoch no.4 train no.304940  loss = 4.45149 avg_loss = 3.02331\n",
      "epoch no.4 train no.304950  loss = 2.32945 avg_loss = 3.00360\n",
      "epoch no.4 train no.304960  loss = 2.90346 avg_loss = 3.00281\n",
      "epoch no.4 train no.304970  loss = 3.04025 avg_loss = 3.01240\n",
      "epoch no.4 train no.304980  loss = 6.45372 avg_loss = 3.05778\n",
      "epoch no.4 train no.304990  loss = 4.62513 avg_loss = 3.08072\n",
      "epoch no.4 train no.305000  loss = 3.34188 avg_loss = 3.04678\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁발라', '팝', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.4 train no.305010  loss = 3.30107 avg_loss = 3.04873\n",
      "epoch no.4 train no.305020  loss = 3.11492 avg_loss = 3.05883\n",
      "epoch no.4 train no.305030  loss = 2.60112 avg_loss = 3.09439\n",
      "epoch no.4 train no.305040  loss = 2.01475 avg_loss = 3.10269\n",
      "epoch no.4 train no.305050  loss = 4.57480 avg_loss = 3.09113\n",
      "epoch no.4 train no.305060  loss = 3.18189 avg_loss = 3.06919\n",
      "epoch no.4 train no.305070  loss = 4.02484 avg_loss = 3.13093\n",
      "epoch no.4 train no.305080  loss = 3.26581 avg_loss = 3.12493\n",
      "epoch no.4 train no.305090  loss = 3.07751 avg_loss = 3.12698\n",
      "epoch no.4 train no.305100  loss = 3.08133 avg_loss = 3.13324\n",
      "epoch no.4 train no.305110  loss = 4.29155 avg_loss = 3.17933\n",
      "epoch no.4 train no.305120  loss = 2.12585 avg_loss = 3.15783\n",
      "epoch no.4 train no.305130  loss = 3.18956 avg_loss = 3.10080\n",
      "epoch no.4 train no.305140  loss = 2.62132 avg_loss = 3.10381\n",
      "epoch no.4 train no.305150  loss = 3.10803 avg_loss = 3.12352\n",
      "epoch no.4 train no.305160  loss = 3.14504 avg_loss = 3.09938\n",
      "epoch no.4 train no.305170  loss = 2.74751 avg_loss = 3.08139\n",
      "epoch no.4 train no.305180  loss = 2.82796 avg_loss = 3.07377\n",
      "epoch no.4 train no.305190  loss = 3.24952 avg_loss = 3.09106\n",
      "epoch no.4 train no.305200  loss = 3.08421 avg_loss = 3.07006\n",
      "epoch no.4 train no.305210  loss = 4.07528 avg_loss = 3.04859\n",
      "epoch no.4 train no.305220  loss = 3.96723 avg_loss = 3.06329\n",
      "epoch no.4 train no.305230  loss = 4.56856 avg_loss = 3.04617\n",
      "epoch no.4 train no.305240  loss = 1.92898 avg_loss = 3.01976\n",
      "epoch no.4 train no.305250  loss = 3.59579 avg_loss = 2.99854\n",
      "epoch no.4 train no.305260  loss = 2.35504 avg_loss = 2.99464\n",
      "epoch no.4 train no.305270  loss = 1.96185 avg_loss = 3.00423\n",
      "epoch no.4 train no.305280  loss = 2.92347 avg_loss = 3.02784\n",
      "epoch no.4 train no.305290  loss = 4.29052 avg_loss = 3.05253\n",
      "epoch no.4 train no.305300  loss = 1.82479 avg_loss = 3.02394\n",
      "epoch no.4 train no.305310  loss = 2.47214 avg_loss = 3.05746\n",
      "epoch no.4 train no.305320  loss = 1.51913 avg_loss = 3.02733\n",
      "epoch no.4 train no.305330  loss = 1.73979 avg_loss = 3.05190\n",
      "epoch no.4 train no.305340  loss = 1.56523 avg_loss = 3.07087\n",
      "epoch no.4 train no.305350  loss = 1.88322 avg_loss = 3.09297\n",
      "epoch no.4 train no.305360  loss = 2.92805 avg_loss = 3.06810\n",
      "epoch no.4 train no.305370  loss = 2.64887 avg_loss = 3.04194\n",
      "epoch no.4 train no.305380  loss = 3.96398 avg_loss = 3.00186\n",
      "epoch no.4 train no.305390  loss = 2.58676 avg_loss = 3.01214\n",
      "epoch no.4 train no.305400  loss = 3.42731 avg_loss = 3.11986\n",
      "epoch no.4 train no.305410  loss = 2.82652 avg_loss = 3.09772\n",
      "epoch no.4 train no.305420  loss = 4.08437 avg_loss = 3.08505\n",
      "epoch no.4 train no.305430  loss = 2.74883 avg_loss = 3.09802\n",
      "epoch no.4 train no.305440  loss = 3.10568 avg_loss = 3.12597\n",
      "epoch no.4 train no.305450  loss = 2.75700 avg_loss = 3.13801\n",
      "epoch no.4 train no.305460  loss = 3.50998 avg_loss = 3.13944\n",
      "epoch no.4 train no.305470  loss = 4.42256 avg_loss = 3.11839\n",
      "epoch no.4 train no.305480  loss = 2.93387 avg_loss = 3.09747\n",
      "epoch no.4 train no.305490  loss = 3.25316 avg_loss = 3.06729\n",
      "epoch no.4 train no.305500  loss = 2.80503 avg_loss = 3.06253\n",
      "epoch no.4 train no.305510  loss = 4.52383 avg_loss = 3.06392\n",
      "epoch no.4 train no.305520  loss = 4.02267 avg_loss = 3.07339\n",
      "epoch no.4 train no.305530  loss = 3.98834 avg_loss = 3.08360\n",
      "epoch no.4 train no.305540  loss = 3.56899 avg_loss = 3.08214\n",
      "epoch no.4 train no.305550  loss = 3.86720 avg_loss = 3.05839\n",
      "epoch no.4 train no.305560  loss = 3.77970 avg_loss = 3.01487\n",
      "epoch no.4 train no.305570  loss = 2.47501 avg_loss = 2.97786\n",
      "epoch no.4 train no.305580  loss = 2.69740 avg_loss = 2.99085\n",
      "epoch no.4 train no.305590  loss = 2.60704 avg_loss = 3.01645\n",
      "epoch no.4 train no.305600  loss = 3.31387 avg_loss = 3.01982\n",
      "epoch no.4 train no.305610  loss = 3.18461 avg_loss = 3.04893\n",
      "epoch no.4 train no.305620  loss = 2.79962 avg_loss = 3.08433\n",
      "epoch no.4 train no.305630  loss = 3.89160 avg_loss = 3.11485\n",
      "epoch no.4 train no.305640  loss = 2.19539 avg_loss = 3.11495\n",
      "epoch no.4 train no.305650  loss = 2.09573 avg_loss = 3.08466\n",
      "epoch no.4 train no.305660  loss = 3.48884 avg_loss = 3.05624\n",
      "epoch no.4 train no.305670  loss = 3.96890 avg_loss = 3.03239\n",
      "epoch no.4 train no.305680  loss = 4.06343 avg_loss = 3.01876\n",
      "epoch no.4 train no.305690  loss = 3.18535 avg_loss = 3.00450\n",
      "epoch no.4 train no.305700  loss = 3.28419 avg_loss = 2.99389\n",
      "epoch no.4 train no.305710  loss = 3.84698 avg_loss = 2.98827\n",
      "epoch no.4 train no.305720  loss = 2.23371 avg_loss = 2.98391\n",
      "epoch no.4 train no.305730  loss = 4.24841 avg_loss = 2.98670\n",
      "epoch no.4 train no.305740  loss = 3.42180 avg_loss = 2.96162\n",
      "epoch no.4 train no.305750  loss = 4.10555 avg_loss = 2.97442\n",
      "epoch no.4 train no.305760  loss = 2.49934 avg_loss = 2.99114\n",
      "epoch no.4 train no.305770  loss = 1.79801 avg_loss = 3.01295\n",
      "epoch no.4 train no.305780  loss = 3.03655 avg_loss = 3.02844\n",
      "epoch no.4 train no.305790  loss = 2.35185 avg_loss = 3.01724\n",
      "epoch no.4 train no.305800  loss = 1.92957 avg_loss = 3.02246\n",
      "epoch no.4 train no.305810  loss = 3.17355 avg_loss = 3.02526\n",
      "epoch no.4 train no.305820  loss = 2.45801 avg_loss = 3.05163\n",
      "epoch no.4 train no.305830  loss = 3.83838 avg_loss = 3.00993\n",
      "epoch no.4 train no.305840  loss = 0.99152 avg_loss = 2.98766\n",
      "epoch no.4 train no.305850  loss = 4.97953 avg_loss = 3.03730\n",
      "epoch no.4 train no.305860  loss = 1.93663 avg_loss = 3.03360\n",
      "epoch no.4 train no.305870  loss = 3.59036 avg_loss = 3.05914\n",
      "epoch no.4 train no.305880  loss = 2.31847 avg_loss = 3.06040\n",
      "epoch no.4 train no.305890  loss = 3.40378 avg_loss = 3.10616\n",
      "epoch no.4 train no.305900  loss = 3.90578 avg_loss = 3.13535\n",
      "epoch no.4 train no.305910  loss = 4.65221 avg_loss = 3.12707\n",
      "epoch no.4 train no.305920  loss = 3.52042 avg_loss = 3.11758\n",
      "epoch no.4 train no.305930  loss = 1.89053 avg_loss = 3.07632\n",
      "epoch no.4 train no.305940  loss = 2.05810 avg_loss = 3.02920\n",
      "epoch no.4 train no.305950  loss = 2.86390 avg_loss = 3.07597\n",
      "epoch no.4 train no.305960  loss = 2.94483 avg_loss = 3.07236\n",
      "epoch no.4 train no.305970  loss = 5.18276 avg_loss = 3.11625\n",
      "epoch no.4 train no.305980  loss = 1.91700 avg_loss = 3.09624\n",
      "epoch no.4 train no.305990  loss = 2.56764 avg_loss = 3.11695\n",
      "epoch no.4 train no.306000  loss = 2.24644 avg_loss = 3.11988\n",
      "6\n",
      "to_tokens: ['▁비', '▁90', '팝', '▁모음', '▁모음', '듣', '봐', '</s>']\n",
      "추억의 댄스음악들 다시 들어보자</s>\n",
      "epoch no.4 train no.306010  loss = 3.09401 avg_loss = 3.10498\n",
      "epoch no.4 train no.306020  loss = 2.63559 avg_loss = 3.10516\n",
      "epoch no.4 train no.306030  loss = 3.09380 avg_loss = 3.06746\n",
      "epoch no.4 train no.306040  loss = 3.62166 avg_loss = 3.07580\n",
      "epoch no.4 train no.306050  loss = 3.03188 avg_loss = 3.06918\n",
      "epoch no.4 train no.306060  loss = 1.73217 avg_loss = 3.04447\n",
      "epoch no.4 train no.306070  loss = 4.57530 avg_loss = 3.08620\n",
      "epoch no.4 train no.306080  loss = 3.81007 avg_loss = 3.08756\n",
      "epoch no.4 train no.306090  loss = 3.17792 avg_loss = 3.10681\n",
      "epoch no.4 train no.306100  loss = 2.63824 avg_loss = 3.06967\n",
      "epoch no.4 train no.306110  loss = 2.40695 avg_loss = 3.06192\n",
      "epoch no.4 train no.306120  loss = 2.67179 avg_loss = 3.08372\n",
      "epoch no.4 train no.306130  loss = 3.27021 avg_loss = 3.07398\n",
      "epoch no.4 train no.306140  loss = 2.89174 avg_loss = 3.05559\n",
      "epoch no.4 train no.306150  loss = 2.39695 avg_loss = 3.02142\n",
      "epoch no.4 train no.306160  loss = 3.51640 avg_loss = 3.00241\n",
      "epoch no.4 train no.306170  loss = 2.33396 avg_loss = 2.94989\n",
      "epoch no.4 train no.306180  loss = 2.72128 avg_loss = 2.98196\n",
      "epoch no.4 train no.306190  loss = 3.13196 avg_loss = 2.97731\n",
      "epoch no.4 train no.306200  loss = 1.76870 avg_loss = 2.95507\n",
      "epoch no.4 train no.306210  loss = 2.67079 avg_loss = 2.95676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.306220  loss = 2.77771 avg_loss = 2.96170\n",
      "epoch no.4 train no.306230  loss = 4.11949 avg_loss = 2.97208\n",
      "epoch no.4 train no.306240  loss = 3.08931 avg_loss = 2.98259\n",
      "epoch no.4 train no.306250  loss = 2.81400 avg_loss = 2.95911\n",
      "epoch no.4 train no.306260  loss = 2.86074 avg_loss = 2.98492\n",
      "epoch no.4 train no.306270  loss = 3.41905 avg_loss = 3.03377\n",
      "epoch no.4 train no.306280  loss = 3.04081 avg_loss = 3.06062\n",
      "epoch no.4 train no.306290  loss = 2.91871 avg_loss = 3.01664\n",
      "epoch no.4 train no.306300  loss = 3.65061 avg_loss = 3.04489\n",
      "epoch no.4 train no.306310  loss = 4.85168 avg_loss = 3.06548\n",
      "epoch no.4 train no.306320  loss = 2.62292 avg_loss = 3.05938\n",
      "epoch no.4 train no.306330  loss = 2.42184 avg_loss = 3.02810\n",
      "epoch no.4 train no.306340  loss = 2.64031 avg_loss = 3.02188\n",
      "epoch no.4 train no.306350  loss = 2.66336 avg_loss = 3.03363\n",
      "epoch no.4 train no.306360  loss = 3.04806 avg_loss = 3.08539\n",
      "epoch no.4 train no.306370  loss = 3.79017 avg_loss = 3.11249\n",
      "epoch no.4 train no.306380  loss = 3.82322 avg_loss = 3.09420\n",
      "epoch no.4 train no.306390  loss = 2.50455 avg_loss = 3.06507\n",
      "epoch no.4 train no.306400  loss = 3.82667 avg_loss = 3.07780\n",
      "epoch no.4 train no.306410  loss = 2.80958 avg_loss = 3.08331\n",
      "epoch no.4 train no.306420  loss = 1.28485 avg_loss = 3.05615\n",
      "epoch no.4 train no.306430  loss = 3.53685 avg_loss = 3.08468\n",
      "epoch no.4 train no.306440  loss = 1.90906 avg_loss = 3.09945\n",
      "epoch no.4 train no.306450  loss = 3.32888 avg_loss = 3.11679\n",
      "epoch no.4 train no.306460  loss = 2.97567 avg_loss = 3.10503\n",
      "epoch no.4 train no.306470  loss = 2.90937 avg_loss = 3.12701\n",
      "epoch no.4 train no.306480  loss = 2.94140 avg_loss = 3.13326\n",
      "epoch no.4 train no.306490  loss = 2.02652 avg_loss = 3.11782\n",
      "epoch no.4 train no.306500  loss = 2.76660 avg_loss = 3.11835\n",
      "epoch no.4 train no.306510  loss = 1.98433 avg_loss = 3.13196\n",
      "epoch no.4 train no.306520  loss = 3.19236 avg_loss = 3.10055\n",
      "epoch no.4 train no.306530  loss = 4.21900 avg_loss = 3.08338\n",
      "epoch no.4 train no.306540  loss = 3.67725 avg_loss = 3.05680\n",
      "epoch no.4 train no.306550  loss = 5.45287 avg_loss = 3.06612\n",
      "epoch no.4 train no.306560  loss = 1.84033 avg_loss = 3.03581\n",
      "epoch no.4 train no.306570  loss = 3.27953 avg_loss = 3.02281\n",
      "epoch no.4 train no.306580  loss = 3.14999 avg_loss = 3.02165\n",
      "epoch no.4 train no.306590  loss = 2.22490 avg_loss = 3.02784\n",
      "epoch no.4 train no.306600  loss = 4.50625 avg_loss = 3.06403\n",
      "epoch no.4 train no.306610  loss = 3.17079 avg_loss = 3.06085\n",
      "epoch no.4 train no.306620  loss = 3.45480 avg_loss = 3.09032\n",
      "epoch no.4 train no.306630  loss = 3.00513 avg_loss = 3.08569\n",
      "epoch no.4 train no.306640  loss = 3.61974 avg_loss = 3.05436\n",
      "epoch no.4 train no.306650  loss = 2.42394 avg_loss = 3.07733\n",
      "epoch no.4 train no.306660  loss = 3.06338 avg_loss = 3.10244\n",
      "epoch no.4 train no.306670  loss = 4.16557 avg_loss = 3.11942\n",
      "epoch no.4 train no.306680  loss = 4.08547 avg_loss = 3.10715\n",
      "epoch no.4 train no.306690  loss = 3.17775 avg_loss = 3.07853\n",
      "epoch no.4 train no.306700  loss = 2.42656 avg_loss = 3.08945\n",
      "epoch no.4 train no.306710  loss = 6.10798 avg_loss = 3.12896\n",
      "epoch no.4 train no.306720  loss = 1.68238 avg_loss = 3.09697\n",
      "epoch no.4 train no.306730  loss = 3.42368 avg_loss = 3.09673\n",
      "epoch no.4 train no.306740  loss = 2.52753 avg_loss = 3.11397\n",
      "epoch no.4 train no.306750  loss = 2.97062 avg_loss = 3.08269\n",
      "epoch no.4 train no.306760  loss = 2.87014 avg_loss = 3.10414\n",
      "epoch no.4 train no.306770  loss = 3.01153 avg_loss = 3.15686\n",
      "epoch no.4 train no.306780  loss = 5.50253 avg_loss = 3.16926\n",
      "epoch no.4 train no.306790  loss = 3.93605 avg_loss = 3.20326\n",
      "epoch no.4 train no.306800  loss = 3.86133 avg_loss = 3.20764\n",
      "epoch no.4 train no.306810  loss = 3.96398 avg_loss = 3.16428\n",
      "epoch no.4 train no.306820  loss = 4.62729 avg_loss = 3.19758\n",
      "epoch no.4 train no.306830  loss = 1.42218 avg_loss = 3.16919\n",
      "epoch no.4 train no.306840  loss = 3.16844 avg_loss = 3.14685\n",
      "epoch no.4 train no.306850  loss = 2.83527 avg_loss = 3.12545\n",
      "epoch no.4 train no.306860  loss = 5.37499 avg_loss = 3.16383\n",
      "epoch no.4 train no.306870  loss = 1.86576 avg_loss = 3.17012\n",
      "epoch no.4 train no.306880  loss = 3.92396 avg_loss = 3.17653\n",
      "epoch no.4 train no.306890  loss = 2.71178 avg_loss = 3.15493\n",
      "epoch no.4 train no.306900  loss = 2.47098 avg_loss = 3.18715\n",
      "epoch no.4 train no.306910  loss = 2.42699 avg_loss = 3.17391\n",
      "epoch no.4 train no.306920  loss = 4.22692 avg_loss = 3.20528\n",
      "epoch no.4 train no.306930  loss = 3.55146 avg_loss = 3.18095\n",
      "epoch no.4 train no.306940  loss = 3.42706 avg_loss = 3.19132\n",
      "epoch no.4 train no.306950  loss = 3.29948 avg_loss = 3.18272\n",
      "epoch no.4 train no.306960  loss = 2.82720 avg_loss = 3.17467\n",
      "epoch no.4 train no.306970  loss = 2.51555 avg_loss = 3.13521\n",
      "epoch no.4 train no.306980  loss = 2.28017 avg_loss = 3.07259\n",
      "epoch no.4 train no.306990  loss = 3.69159 avg_loss = 3.07183\n",
      "epoch no.4 train no.307000  loss = 3.71426 avg_loss = 3.07298\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.4 train no.307010  loss = 2.72958 avg_loss = 3.09038\n",
      "epoch no.4 train no.307020  loss = 3.17218 avg_loss = 3.14111\n",
      "epoch no.4 train no.307030  loss = 3.19905 avg_loss = 3.10831\n",
      "epoch no.4 train no.307040  loss = 2.21888 avg_loss = 3.14643\n",
      "epoch no.4 train no.307050  loss = 4.55247 avg_loss = 3.18032\n",
      "epoch no.4 train no.307060  loss = 3.45573 avg_loss = 3.14715\n",
      "epoch no.4 train no.307070  loss = 4.47729 avg_loss = 3.15190\n",
      "epoch no.4 train no.307080  loss = 2.17000 avg_loss = 3.13508\n",
      "epoch no.4 train no.307090  loss = 2.16379 avg_loss = 3.12131\n",
      "epoch no.4 train no.307100  loss = 3.82261 avg_loss = 3.08351\n",
      "epoch no.4 train no.307110  loss = 1.97776 avg_loss = 3.04052\n",
      "epoch no.4 train no.307120  loss = 4.00095 avg_loss = 3.11732\n",
      "epoch no.4 train no.307130  loss = 3.91384 avg_loss = 3.13826\n",
      "epoch no.4 train no.307140  loss = 4.90403 avg_loss = 3.14820\n",
      "epoch no.4 train no.307150  loss = 4.11263 avg_loss = 3.16237\n",
      "epoch no.4 train no.307160  loss = 3.33810 avg_loss = 3.16166\n",
      "epoch no.4 train no.307170  loss = 2.28392 avg_loss = 3.16191\n",
      "epoch no.4 train no.307180  loss = 2.93376 avg_loss = 3.10239\n",
      "epoch no.4 train no.307190  loss = 2.16538 avg_loss = 3.11326\n",
      "epoch no.4 train no.307200  loss = 2.71813 avg_loss = 3.08298\n",
      "epoch no.4 train no.307210  loss = 3.12651 avg_loss = 3.09979\n",
      "epoch no.4 train no.307220  loss = 3.15661 avg_loss = 3.12058\n",
      "epoch no.4 train no.307230  loss = 2.01760 avg_loss = 3.14003\n",
      "epoch no.4 train no.307240  loss = 3.69994 avg_loss = 3.15610\n",
      "epoch no.4 train no.307250  loss = 2.47759 avg_loss = 3.12796\n",
      "epoch no.4 train no.307260  loss = 2.55252 avg_loss = 3.11387\n",
      "epoch no.4 train no.307270  loss = 2.24430 avg_loss = 3.09032\n",
      "epoch no.4 train no.307280  loss = 3.43294 avg_loss = 3.11735\n",
      "epoch no.4 train no.307290  loss = 2.24246 avg_loss = 3.13226\n",
      "epoch no.4 train no.307300  loss = 3.10302 avg_loss = 3.11374\n",
      "epoch no.4 train no.307310  loss = 2.37977 avg_loss = 3.07704\n",
      "epoch no.4 train no.307320  loss = 2.16290 avg_loss = 3.06844\n",
      "epoch no.4 train no.307330  loss = 2.37122 avg_loss = 3.05562\n",
      "epoch no.4 train no.307340  loss = 4.82682 avg_loss = 3.05106\n",
      "epoch no.4 train no.307350  loss = 2.76057 avg_loss = 3.04959\n",
      "epoch no.4 train no.307360  loss = 1.95740 avg_loss = 3.03817\n",
      "epoch no.4 train no.307370  loss = 4.83454 avg_loss = 3.05560\n",
      "epoch no.4 train no.307380  loss = 2.42246 avg_loss = 3.06266\n",
      "epoch no.4 train no.307390  loss = 3.42426 avg_loss = 3.06966\n",
      "epoch no.4 train no.307400  loss = 2.27314 avg_loss = 3.05745\n",
      "epoch no.4 train no.307410  loss = 2.13840 avg_loss = 3.04613\n",
      "epoch no.4 train no.307420  loss = 1.90458 avg_loss = 3.04307\n",
      "epoch no.4 train no.307430  loss = 3.68965 avg_loss = 3.07900\n",
      "epoch no.4 train no.307440  loss = 4.00236 avg_loss = 3.13676\n",
      "epoch no.4 train no.307450  loss = 3.02833 avg_loss = 3.12312\n",
      "epoch no.4 train no.307460  loss = 3.66915 avg_loss = 3.06786\n",
      "epoch no.4 train no.307470  loss = 3.04328 avg_loss = 3.07010\n",
      "epoch no.4 train no.307480  loss = 1.60419 avg_loss = 3.05934\n",
      "epoch no.4 train no.307490  loss = 2.81820 avg_loss = 3.04120\n",
      "epoch no.4 train no.307500  loss = 3.97609 avg_loss = 3.10345\n",
      "epoch no.4 train no.307510  loss = 2.79546 avg_loss = 3.06149\n",
      "epoch no.4 train no.307520  loss = 4.75401 avg_loss = 3.06967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.307530  loss = 1.14656 avg_loss = 3.04422\n",
      "epoch no.4 train no.307540  loss = 3.73621 avg_loss = 3.03035\n",
      "epoch no.4 train no.307550  loss = 3.01620 avg_loss = 3.02085\n",
      "epoch no.4 train no.307560  loss = 2.85956 avg_loss = 3.00803\n",
      "epoch no.4 train no.307570  loss = 3.13797 avg_loss = 2.99792\n",
      "epoch no.4 train no.307580  loss = 3.30687 avg_loss = 3.01378\n",
      "epoch no.4 train no.307590  loss = 2.06410 avg_loss = 3.01389\n",
      "epoch no.4 train no.307600  loss = 1.93952 avg_loss = 3.00063\n",
      "epoch no.4 train no.307610  loss = 4.22559 avg_loss = 2.99830\n",
      "epoch no.4 train no.307620  loss = 3.19075 avg_loss = 2.95174\n",
      "epoch no.4 train no.307630  loss = 2.92391 avg_loss = 2.98034\n",
      "epoch no.4 train no.307640  loss = 2.94272 avg_loss = 2.98879\n",
      "epoch no.4 train no.307650  loss = 5.11923 avg_loss = 3.04349\n",
      "epoch no.4 train no.307660  loss = 2.69595 avg_loss = 3.02358\n",
      "epoch no.4 train no.307670  loss = 2.23006 avg_loss = 3.00836\n",
      "epoch no.4 train no.307680  loss = 2.10264 avg_loss = 2.98894\n",
      "epoch no.4 train no.307690  loss = 3.52148 avg_loss = 3.05170\n",
      "epoch no.4 train no.307700  loss = 2.69818 avg_loss = 3.02460\n",
      "epoch no.4 train no.307710  loss = 2.70853 avg_loss = 3.01617\n",
      "epoch no.4 train no.307720  loss = 2.35161 avg_loss = 3.03782\n",
      "epoch no.4 train no.307730  loss = 3.11471 avg_loss = 3.02595\n",
      "epoch no.4 train no.307740  loss = 3.70568 avg_loss = 3.04392\n",
      "epoch no.4 train no.307750  loss = 3.77850 avg_loss = 3.04651\n",
      "epoch no.4 train no.307760  loss = 3.09145 avg_loss = 3.06617\n",
      "epoch no.4 train no.307770  loss = 3.47974 avg_loss = 3.07463\n",
      "epoch no.4 train no.307780  loss = 2.21935 avg_loss = 3.07497\n",
      "epoch no.4 train no.307790  loss = 2.40702 avg_loss = 3.10956\n",
      "epoch no.4 train no.307800  loss = 2.52368 avg_loss = 3.12937\n",
      "epoch no.4 train no.307810  loss = 3.01070 avg_loss = 3.14520\n",
      "epoch no.4 train no.307820  loss = 1.75768 avg_loss = 3.15352\n",
      "epoch no.4 train no.307830  loss = 4.10748 avg_loss = 3.15422\n",
      "epoch no.4 train no.307840  loss = 5.20356 avg_loss = 3.19176\n",
      "epoch no.4 train no.307850  loss = 3.48013 avg_loss = 3.18666\n",
      "epoch no.4 train no.307860  loss = 2.91081 avg_loss = 3.17609\n",
      "epoch no.4 train no.307870  loss = 3.15875 avg_loss = 3.22509\n",
      "epoch no.4 train no.307880  loss = 1.82793 avg_loss = 3.20078\n",
      "epoch no.4 train no.307890  loss = 2.42227 avg_loss = 3.19283\n",
      "epoch no.4 train no.307900  loss = 3.30522 avg_loss = 3.16589\n",
      "epoch no.4 train no.307910  loss = 4.64112 avg_loss = 3.18193\n",
      "epoch no.4 train no.307920  loss = 2.27865 avg_loss = 3.18231\n",
      "epoch no.4 train no.307930  loss = 2.97920 avg_loss = 3.16025\n",
      "epoch no.4 train no.307940  loss = 1.98469 avg_loss = 3.11918\n",
      "epoch no.4 train no.307950  loss = 3.33103 avg_loss = 3.16327\n",
      "epoch no.4 train no.307960  loss = 3.10432 avg_loss = 3.17327\n",
      "epoch no.4 train no.307970  loss = 3.87409 avg_loss = 3.14357\n",
      "epoch no.4 train no.307980  loss = 2.86233 avg_loss = 3.14868\n",
      "epoch no.4 train no.307990  loss = 3.40718 avg_loss = 3.13528\n",
      "epoch no.4 train no.308000  loss = 2.37830 avg_loss = 3.10266\n",
      "4\n",
      "to_tokens: ['▁비', '▁인기가', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.4 train no.308010  loss = 2.98311 avg_loss = 3.12629\n",
      "epoch no.4 train no.308020  loss = 3.00758 avg_loss = 3.12325\n",
      "epoch no.4 train no.308030  loss = 4.91589 avg_loss = 3.12806\n",
      "epoch no.4 train no.308040  loss = 2.48401 avg_loss = 3.15953\n",
      "epoch no.4 train no.308050  loss = 2.35117 avg_loss = 3.11388\n",
      "epoch no.4 train no.308060  loss = 1.84940 avg_loss = 3.12010\n",
      "epoch no.4 train no.308070  loss = 2.51784 avg_loss = 3.07174\n",
      "epoch no.4 train no.308080  loss = 3.51144 avg_loss = 3.06004\n",
      "epoch no.4 train no.308090  loss = 2.99347 avg_loss = 3.07750\n",
      "epoch no.4 train no.308100  loss = 2.49165 avg_loss = 3.10213\n",
      "epoch no.4 train no.308110  loss = 1.95153 avg_loss = 3.06145\n",
      "epoch no.4 train no.308120  loss = 3.43046 avg_loss = 3.09574\n",
      "epoch no.4 train no.308130  loss = 3.60423 avg_loss = 3.06128\n",
      "epoch no.4 train no.308140  loss = 2.65293 avg_loss = 3.08426\n",
      "epoch no.4 train no.308150  loss = 2.81196 avg_loss = 3.07886\n",
      "epoch no.4 train no.308160  loss = 3.33005 avg_loss = 3.10584\n",
      "epoch no.4 train no.308170  loss = 3.54167 avg_loss = 3.09552\n",
      "epoch no.4 train no.308180  loss = 3.34674 avg_loss = 3.10194\n",
      "epoch no.4 train no.308190  loss = 4.02626 avg_loss = 3.11194\n",
      "epoch no.4 train no.308200  loss = 2.98645 avg_loss = 3.12761\n",
      "epoch no.4 train no.308210  loss = 2.65625 avg_loss = 3.15295\n",
      "epoch no.4 train no.308220  loss = 4.34705 avg_loss = 3.14114\n",
      "epoch no.4 train no.308230  loss = 3.44841 avg_loss = 3.15296\n",
      "epoch no.4 train no.308240  loss = 1.88608 avg_loss = 3.11910\n",
      "epoch no.4 train no.308250  loss = 1.87631 avg_loss = 3.15338\n",
      "epoch no.4 train no.308260  loss = 3.30133 avg_loss = 3.12226\n",
      "epoch no.4 train no.308270  loss = 3.83427 avg_loss = 3.11948\n",
      "epoch no.4 train no.308280  loss = 2.92929 avg_loss = 3.11991\n",
      "epoch no.4 train no.308290  loss = 2.63154 avg_loss = 3.09021\n",
      "epoch no.4 train no.308300  loss = 2.74993 avg_loss = 3.13646\n",
      "epoch no.4 train no.308310  loss = 3.01409 avg_loss = 3.13874\n",
      "epoch no.4 train no.308320  loss = 3.56478 avg_loss = 3.14427\n",
      "epoch no.4 train no.308330  loss = 2.16270 avg_loss = 3.13913\n",
      "epoch no.4 train no.308340  loss = 2.36571 avg_loss = 3.13740\n",
      "epoch no.4 train no.308350  loss = 4.66676 avg_loss = 3.15666\n",
      "epoch no.4 train no.308360  loss = 2.70180 avg_loss = 3.20704\n",
      "epoch no.4 train no.308370  loss = 5.07889 avg_loss = 3.22500\n",
      "epoch no.4 train no.308380  loss = 2.19947 avg_loss = 3.24384\n",
      "epoch no.4 train no.308390  loss = 2.89141 avg_loss = 3.26050\n",
      "epoch no.4 train no.308400  loss = 3.48698 avg_loss = 3.24509\n",
      "epoch no.4 train no.308410  loss = 3.89564 avg_loss = 3.21892\n",
      "epoch no.4 train no.308420  loss = 2.88719 avg_loss = 3.23700\n",
      "epoch no.4 train no.308430  loss = 2.38932 avg_loss = 3.23081\n",
      "epoch no.4 train no.308440  loss = 2.90374 avg_loss = 3.19412\n",
      "epoch no.4 train no.308450  loss = 2.93607 avg_loss = 3.20347\n",
      "epoch no.4 train no.308460  loss = 2.53683 avg_loss = 3.22890\n",
      "epoch no.4 train no.308470  loss = 2.78581 avg_loss = 3.20834\n",
      "epoch no.4 train no.308480  loss = 2.43169 avg_loss = 3.21449\n",
      "epoch no.4 train no.308490  loss = 3.43460 avg_loss = 3.19260\n",
      "epoch no.4 train no.308500  loss = 2.00900 avg_loss = 3.17206\n",
      "epoch no.4 train no.308510  loss = 2.82632 avg_loss = 3.18769\n",
      "epoch no.4 train no.308520  loss = 2.02231 avg_loss = 3.18517\n",
      "epoch no.4 train no.308530  loss = 3.45399 avg_loss = 3.19793\n",
      "epoch no.4 train no.308540  loss = 2.07914 avg_loss = 3.13117\n",
      "epoch no.4 train no.308550  loss = 3.39605 avg_loss = 3.12430\n",
      "epoch no.4 train no.308560  loss = 3.21759 avg_loss = 3.11037\n",
      "epoch no.4 train no.308570  loss = 3.14956 avg_loss = 3.14617\n",
      "epoch no.4 train no.308580  loss = 3.10729 avg_loss = 3.16039\n",
      "epoch no.4 train no.308590  loss = 3.57954 avg_loss = 3.16932\n",
      "epoch no.4 train no.308600  loss = 2.63396 avg_loss = 3.16714\n",
      "epoch no.4 train no.308610  loss = 3.20053 avg_loss = 3.15098\n",
      "epoch no.4 train no.308620  loss = 2.05634 avg_loss = 3.14917\n",
      "epoch no.4 train no.308630  loss = 2.46012 avg_loss = 3.13263\n",
      "epoch no.4 train no.308640  loss = 2.31659 avg_loss = 3.13331\n",
      "epoch no.4 train no.308650  loss = 3.81655 avg_loss = 3.14492\n",
      "epoch no.4 train no.308660  loss = 1.98652 avg_loss = 3.07641\n",
      "epoch no.4 train no.308670  loss = 2.54209 avg_loss = 3.06458\n",
      "epoch no.4 train no.308680  loss = 3.46074 avg_loss = 3.04829\n",
      "epoch no.4 train no.308690  loss = 3.24478 avg_loss = 3.02579\n",
      "epoch no.4 train no.308700  loss = 3.48046 avg_loss = 3.02983\n",
      "epoch no.4 train no.308710  loss = 2.11873 avg_loss = 3.02883\n",
      "epoch no.4 train no.308720  loss = 2.19761 avg_loss = 3.08703\n",
      "epoch no.4 train no.308730  loss = 3.24557 avg_loss = 3.06539\n",
      "epoch no.4 train no.308740  loss = 3.46355 avg_loss = 3.06990\n",
      "epoch no.4 train no.308750  loss = 2.94357 avg_loss = 3.08609\n",
      "epoch no.4 train no.308760  loss = 4.22956 avg_loss = 3.09185\n",
      "epoch no.4 train no.308770  loss = 2.37635 avg_loss = 3.09651\n",
      "epoch no.4 train no.308780  loss = 2.70440 avg_loss = 3.07383\n",
      "epoch no.4 train no.308790  loss = 2.41098 avg_loss = 3.06603\n",
      "epoch no.4 train no.308800  loss = 4.02615 avg_loss = 3.05938\n",
      "epoch no.4 train no.308810  loss = 1.96475 avg_loss = 3.03528\n",
      "epoch no.4 train no.308820  loss = 3.45237 avg_loss = 3.04417\n",
      "epoch no.4 train no.308830  loss = 2.36615 avg_loss = 3.04964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.308840  loss = 2.45100 avg_loss = 3.03792\n",
      "epoch no.4 train no.308850  loss = 3.48397 avg_loss = 3.02054\n",
      "epoch no.4 train no.308860  loss = 2.04789 avg_loss = 2.99117\n",
      "epoch no.4 train no.308870  loss = 3.05918 avg_loss = 2.97419\n",
      "epoch no.4 train no.308880  loss = 2.84995 avg_loss = 3.00466\n",
      "epoch no.4 train no.308890  loss = 3.67627 avg_loss = 2.97905\n",
      "epoch no.4 train no.308900  loss = 2.62231 avg_loss = 2.98105\n",
      "epoch no.4 train no.308910  loss = 2.14278 avg_loss = 2.98141\n",
      "epoch no.4 train no.308920  loss = 2.24621 avg_loss = 2.98189\n",
      "epoch no.4 train no.308930  loss = 2.92816 avg_loss = 2.94090\n",
      "epoch no.4 train no.308940  loss = 5.00695 avg_loss = 2.96798\n",
      "epoch no.4 train no.308950  loss = 4.26217 avg_loss = 2.94302\n",
      "epoch no.4 train no.308960  loss = 1.99081 avg_loss = 2.93484\n",
      "epoch no.4 train no.308970  loss = 2.51082 avg_loss = 2.95827\n",
      "epoch no.4 train no.308980  loss = 2.47515 avg_loss = 2.96163\n",
      "epoch no.4 train no.308990  loss = 3.29048 avg_loss = 2.99647\n",
      "epoch no.4 train no.309000  loss = 2.19421 avg_loss = 3.01727\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.4 train no.309010  loss = 3.00518 avg_loss = 3.04933\n",
      "epoch no.4 train no.309020  loss = 3.23124 avg_loss = 3.07377\n",
      "epoch no.4 train no.309030  loss = 1.68655 avg_loss = 3.03840\n",
      "epoch no.4 train no.309040  loss = 2.70516 avg_loss = 3.01187\n",
      "epoch no.4 train no.309050  loss = 2.93392 avg_loss = 2.99020\n",
      "epoch no.4 train no.309060  loss = 3.95230 avg_loss = 2.98530\n",
      "epoch no.4 train no.309070  loss = 2.46249 avg_loss = 2.95496\n",
      "epoch no.4 train no.309080  loss = 5.14957 avg_loss = 2.94720\n",
      "epoch no.4 train no.309090  loss = 3.39877 avg_loss = 2.97936\n",
      "epoch no.4 train no.309100  loss = 3.14630 avg_loss = 2.97443\n",
      "epoch no.4 train no.309110  loss = 3.60003 avg_loss = 2.95768\n",
      "epoch no.4 train no.309120  loss = 4.03729 avg_loss = 3.00892\n",
      "epoch no.4 train no.309130  loss = 2.58822 avg_loss = 3.04134\n",
      "epoch no.4 train no.309140  loss = 2.29127 avg_loss = 3.06271\n",
      "epoch no.4 train no.309150  loss = 3.76596 avg_loss = 3.06243\n",
      "epoch no.4 train no.309160  loss = 3.12770 avg_loss = 3.09528\n",
      "epoch no.4 train no.309170  loss = 2.58561 avg_loss = 3.06407\n",
      "epoch no.4 train no.309180  loss = 4.19172 avg_loss = 3.07882\n",
      "epoch no.4 train no.309190  loss = 3.62702 avg_loss = 3.10501\n",
      "epoch no.4 train no.309200  loss = 2.59117 avg_loss = 3.06497\n",
      "epoch no.4 train no.309210  loss = 3.16888 avg_loss = 3.11803\n",
      "epoch no.4 train no.309220  loss = 3.32873 avg_loss = 3.14893\n",
      "epoch no.4 train no.309230  loss = 4.31005 avg_loss = 3.15093\n",
      "epoch no.4 train no.309240  loss = 3.80886 avg_loss = 3.14551\n",
      "epoch no.4 train no.309250  loss = 3.02252 avg_loss = 3.13490\n",
      "epoch no.4 train no.309260  loss = 2.13492 avg_loss = 3.13379\n",
      "epoch no.4 train no.309270  loss = 1.49133 avg_loss = 3.15415\n",
      "epoch no.4 train no.309280  loss = 2.10821 avg_loss = 3.10599\n",
      "epoch no.4 train no.309290  loss = 3.07362 avg_loss = 3.07730\n",
      "epoch no.4 train no.309300  loss = 3.55030 avg_loss = 3.06945\n",
      "epoch no.4 train no.309310  loss = 5.22339 avg_loss = 3.08602\n",
      "epoch no.4 train no.309320  loss = 4.24833 avg_loss = 3.09847\n",
      "epoch no.4 train no.309330  loss = 2.18033 avg_loss = 3.05355\n",
      "epoch no.4 train no.309340  loss = 3.37342 avg_loss = 3.06445\n",
      "epoch no.4 train no.309350  loss = 4.06617 avg_loss = 3.05658\n",
      "epoch no.4 train no.309360  loss = 2.43114 avg_loss = 3.06887\n",
      "epoch no.4 train no.309370  loss = 4.86772 avg_loss = 3.06387\n",
      "epoch no.4 train no.309380  loss = 2.21340 avg_loss = 3.09579\n",
      "epoch no.4 train no.309390  loss = 3.69410 avg_loss = 3.08384\n",
      "epoch no.4 train no.309400  loss = 1.87741 avg_loss = 3.10852\n",
      "epoch no.4 train no.309410  loss = 3.18686 avg_loss = 3.09621\n",
      "epoch no.4 train no.309420  loss = 2.34558 avg_loss = 3.07394\n",
      "epoch no.4 train no.309430  loss = 3.12576 avg_loss = 3.04305\n",
      "epoch no.4 train no.309440  loss = 2.04269 avg_loss = 3.04790\n",
      "epoch no.4 train no.309450  loss = 3.58758 avg_loss = 3.06033\n",
      "epoch no.4 train no.309460  loss = 2.91190 avg_loss = 3.07528\n",
      "epoch no.4 train no.309470  loss = 3.28061 avg_loss = 3.11027\n",
      "epoch no.4 train no.309480  loss = 2.82118 avg_loss = 3.09316\n",
      "epoch no.4 train no.309490  loss = 2.61274 avg_loss = 3.07471\n",
      "epoch no.4 train no.309500  loss = 4.62198 avg_loss = 3.11258\n",
      "epoch no.4 train no.309510  loss = 2.86627 avg_loss = 3.13284\n",
      "epoch no.4 train no.309520  loss = 3.93734 avg_loss = 3.13409\n",
      "epoch no.4 train no.309530  loss = 3.29336 avg_loss = 3.15670\n",
      "epoch no.4 train no.309540  loss = 2.92606 avg_loss = 3.14652\n",
      "epoch no.4 train no.309550  loss = 2.90109 avg_loss = 3.13215\n",
      "epoch no.4 train no.309560  loss = 3.46322 avg_loss = 3.16533\n",
      "epoch no.4 train no.309570  loss = 1.86318 avg_loss = 3.16040\n",
      "epoch no.4 train no.309580  loss = 1.96777 avg_loss = 3.13192\n",
      "epoch no.4 train no.309590  loss = 2.19022 avg_loss = 3.10938\n",
      "epoch no.4 train no.309600  loss = 3.01063 avg_loss = 3.07090\n",
      "epoch no.4 train no.309610  loss = 2.08956 avg_loss = 3.05973\n",
      "epoch no.4 train no.309620  loss = 2.62744 avg_loss = 3.06813\n",
      "epoch no.4 train no.309630  loss = 3.42102 avg_loss = 3.04766\n",
      "epoch no.4 train no.309640  loss = 3.20985 avg_loss = 3.05688\n",
      "epoch no.4 train no.309650  loss = 3.86918 avg_loss = 3.05049\n",
      "epoch no.4 train no.309660  loss = 2.53764 avg_loss = 3.04159\n",
      "epoch no.4 train no.309670  loss = 2.86622 avg_loss = 3.04782\n",
      "epoch no.4 train no.309680  loss = 3.58876 avg_loss = 3.05967\n",
      "epoch no.4 train no.309690  loss = 2.15864 avg_loss = 3.04955\n",
      "epoch no.4 train no.309700  loss = 2.62224 avg_loss = 3.02806\n",
      "epoch no.4 train no.309710  loss = 3.97479 avg_loss = 3.03283\n",
      "epoch no.4 train no.309720  loss = 4.54205 avg_loss = 3.07647\n",
      "epoch no.4 train no.309730  loss = 2.35850 avg_loss = 3.09665\n",
      "epoch no.4 train no.309740  loss = 3.72928 avg_loss = 3.10443\n",
      "epoch no.4 train no.309750  loss = 3.91873 avg_loss = 3.10778\n",
      "epoch no.4 train no.309760  loss = 3.68479 avg_loss = 3.14393\n",
      "epoch no.4 train no.309770  loss = 2.73770 avg_loss = 3.14174\n",
      "epoch no.4 train no.309780  loss = 2.43452 avg_loss = 3.14311\n",
      "epoch no.4 train no.309790  loss = 3.61846 avg_loss = 3.13646\n",
      "epoch no.4 train no.309800  loss = 3.37114 avg_loss = 3.11778\n",
      "epoch no.4 train no.309810  loss = 3.21172 avg_loss = 3.13485\n",
      "epoch no.4 train no.309820  loss = 3.41719 avg_loss = 3.11449\n",
      "epoch no.4 train no.309830  loss = 1.90849 avg_loss = 3.11200\n",
      "epoch no.4 train no.309840  loss = 3.07817 avg_loss = 3.07919\n",
      "epoch no.4 train no.309850  loss = 2.26481 avg_loss = 3.06868\n",
      "epoch no.4 train no.309860  loss = 3.52541 avg_loss = 3.14397\n",
      "epoch no.4 train no.309870  loss = 2.20223 avg_loss = 3.14849\n",
      "epoch no.4 train no.309880  loss = 2.24391 avg_loss = 3.10010\n",
      "epoch no.4 train no.309890  loss = 2.90530 avg_loss = 3.10045\n",
      "epoch no.4 train no.309900  loss = 2.41559 avg_loss = 3.06809\n",
      "epoch no.4 train no.309910  loss = 2.36987 avg_loss = 3.09744\n",
      "epoch no.4 train no.309920  loss = 3.08979 avg_loss = 3.10129\n",
      "epoch no.4 train no.309930  loss = 3.98470 avg_loss = 3.11009\n",
      "epoch no.4 train no.309940  loss = 3.91328 avg_loss = 3.11716\n",
      "epoch no.4 train no.309950  loss = 2.35016 avg_loss = 3.12181\n",
      "epoch no.4 train no.309960  loss = 3.18462 avg_loss = 3.10014\n",
      "epoch no.4 train no.309970  loss = 3.24236 avg_loss = 3.13312\n",
      "epoch no.4 train no.309980  loss = 5.36491 avg_loss = 3.14866\n",
      "epoch no.4 train no.309990  loss = 3.84162 avg_loss = 3.18885\n",
      "epoch no.4 train no.310000  loss = 3.21709 avg_loss = 3.12005\n",
      "10\n",
      "to_tokens: ['▁비', '▁겨울', '▁때', '▁그', '▁시절', '</s>', '▁떠나는', '▁여행', '▁여행', '팝', '▁여행', '</s>']\n",
      "추억의 그 때 그 시절로 떠나는 추억의 올드팝 여행</s>\n",
      "epoch no.4 train no.310010  loss = 3.30785 avg_loss = 3.15499\n",
      "epoch no.4 train no.310020  loss = 3.46374 avg_loss = 3.10601\n",
      "epoch no.4 train no.310030  loss = 4.53613 avg_loss = 3.12321\n",
      "epoch no.4 train no.310040  loss = 2.92727 avg_loss = 3.12207\n",
      "epoch no.4 train no.310050  loss = 4.12363 avg_loss = 3.16092\n",
      "epoch no.4 train no.310060  loss = 3.96627 avg_loss = 3.13001\n",
      "epoch no.4 train no.310070  loss = 2.34150 avg_loss = 3.13001\n",
      "epoch no.4 train no.310080  loss = 4.28429 avg_loss = 3.13673\n",
      "epoch no.4 train no.310090  loss = 2.13314 avg_loss = 3.13216\n",
      "epoch no.4 train no.310100  loss = 3.63351 avg_loss = 3.12939\n",
      "epoch no.4 train no.310110  loss = 2.66817 avg_loss = 3.11837\n",
      "epoch no.4 train no.310120  loss = 1.91441 avg_loss = 3.08172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.310130  loss = 2.39902 avg_loss = 3.09309\n",
      "epoch no.4 train no.310140  loss = 3.17356 avg_loss = 3.09331\n",
      "epoch no.4 train no.310150  loss = 3.43536 avg_loss = 3.06863\n",
      "epoch no.4 train no.310160  loss = 3.60652 avg_loss = 3.14459\n",
      "epoch no.4 train no.310170  loss = 2.11944 avg_loss = 3.18113\n",
      "epoch no.4 train no.310180  loss = 2.43709 avg_loss = 3.15389\n",
      "epoch no.4 train no.310190  loss = 2.79126 avg_loss = 3.13939\n",
      "epoch no.4 train no.310200  loss = 3.21522 avg_loss = 3.18335\n",
      "epoch no.4 train no.310210  loss = 3.08148 avg_loss = 3.13313\n",
      "epoch no.4 train no.310220  loss = 2.47073 avg_loss = 3.10970\n",
      "epoch no.4 train no.310230  loss = 3.10141 avg_loss = 3.12336\n",
      "epoch no.4 train no.310240  loss = 3.80974 avg_loss = 3.14440\n",
      "epoch no.4 train no.310250  loss = 1.93414 avg_loss = 3.10973\n",
      "epoch no.4 train no.310260  loss = 2.08517 avg_loss = 3.11237\n",
      "epoch no.4 train no.310270  loss = 1.78905 avg_loss = 3.11269\n",
      "epoch no.4 train no.310280  loss = 2.60829 avg_loss = 3.08664\n",
      "epoch no.4 train no.310290  loss = 2.02728 avg_loss = 3.05199\n",
      "epoch no.4 train no.310300  loss = 3.40530 avg_loss = 3.04756\n",
      "epoch no.4 train no.310310  loss = 3.45686 avg_loss = 3.09859\n",
      "epoch no.4 train no.310320  loss = 3.03587 avg_loss = 3.08689\n",
      "epoch no.4 train no.310330  loss = 1.13533 avg_loss = 3.07194\n",
      "epoch no.4 train no.310340  loss = 2.94290 avg_loss = 3.07317\n",
      "epoch no.4 train no.310350  loss = 3.76374 avg_loss = 3.06487\n",
      "epoch no.4 train no.310360  loss = 2.51237 avg_loss = 3.06286\n",
      "epoch no.4 train no.310370  loss = 2.64443 avg_loss = 3.04674\n",
      "epoch no.4 train no.310380  loss = 3.02095 avg_loss = 3.10742\n",
      "epoch no.4 train no.310390  loss = 2.26088 avg_loss = 3.06306\n",
      "epoch no.4 train no.310400  loss = 5.00172 avg_loss = 3.10141\n",
      "epoch no.4 train no.310410  loss = 2.49979 avg_loss = 3.10104\n",
      "epoch no.4 train no.310420  loss = 2.65175 avg_loss = 3.09041\n",
      "epoch no.4 train no.310430  loss = 3.09429 avg_loss = 3.07345\n",
      "epoch no.4 train no.310440  loss = 2.23543 avg_loss = 3.03611\n",
      "epoch no.4 train no.310450  loss = 4.52692 avg_loss = 3.06381\n",
      "epoch no.4 train no.310460  loss = 2.37576 avg_loss = 3.09939\n",
      "epoch no.4 train no.310470  loss = 2.94982 avg_loss = 3.07561\n",
      "epoch no.4 train no.310480  loss = 2.66757 avg_loss = 3.06627\n",
      "epoch no.4 train no.310490  loss = 2.90643 avg_loss = 3.08001\n",
      "epoch no.4 train no.310500  loss = 4.04634 avg_loss = 3.10488\n",
      "epoch no.4 train no.310510  loss = 2.56490 avg_loss = 3.14166\n",
      "epoch no.4 train no.310520  loss = 4.59228 avg_loss = 3.15410\n",
      "epoch no.4 train no.310530  loss = 3.19800 avg_loss = 3.18128\n",
      "epoch no.4 train no.310540  loss = 3.90897 avg_loss = 3.17539\n",
      "epoch no.4 train no.310550  loss = 2.72888 avg_loss = 3.18378\n",
      "epoch no.4 train no.310560  loss = 1.94066 avg_loss = 3.16327\n",
      "epoch no.4 train no.310570  loss = 3.89052 avg_loss = 3.13151\n",
      "epoch no.4 train no.310580  loss = 2.30860 avg_loss = 3.11931\n",
      "epoch no.4 train no.310590  loss = 4.19831 avg_loss = 3.15481\n",
      "epoch no.4 train no.310600  loss = 3.05598 avg_loss = 3.13951\n",
      "epoch no.4 train no.310610  loss = 3.73116 avg_loss = 3.15142\n",
      "epoch no.4 train no.310620  loss = 1.01258 avg_loss = 3.14762\n",
      "epoch no.4 train no.310630  loss = 3.29718 avg_loss = 3.15994\n",
      "epoch no.4 train no.310640  loss = 4.25460 avg_loss = 3.16515\n",
      "epoch no.4 train no.310650  loss = 3.05031 avg_loss = 3.17030\n",
      "epoch no.4 train no.310660  loss = 5.44990 avg_loss = 3.23360\n",
      "epoch no.4 train no.310670  loss = 4.17257 avg_loss = 3.22256\n",
      "epoch no.4 train no.310680  loss = 3.50229 avg_loss = 3.25940\n",
      "epoch no.4 train no.310690  loss = 3.75517 avg_loss = 3.24591\n",
      "epoch no.4 train no.310700  loss = 2.98875 avg_loss = 3.23449\n",
      "epoch no.4 train no.310710  loss = 3.62807 avg_loss = 3.25628\n",
      "epoch no.4 train no.310720  loss = 3.90489 avg_loss = 3.23634\n",
      "epoch no.4 train no.310730  loss = 3.42797 avg_loss = 3.23728\n",
      "epoch no.4 train no.310740  loss = 2.41010 avg_loss = 3.24630\n",
      "epoch no.4 train no.310750  loss = 2.40240 avg_loss = 3.19261\n",
      "epoch no.4 train no.310760  loss = 2.72092 avg_loss = 3.20669\n",
      "epoch no.4 train no.310770  loss = 4.44553 avg_loss = 3.19768\n",
      "epoch no.4 train no.310780  loss = 2.19513 avg_loss = 3.20483\n",
      "epoch no.4 train no.310790  loss = 3.91149 avg_loss = 3.23037\n",
      "epoch no.4 train no.310800  loss = 2.49888 avg_loss = 3.25175\n",
      "epoch no.4 train no.310810  loss = 2.47468 avg_loss = 3.21961\n",
      "epoch no.4 train no.310820  loss = 2.20859 avg_loss = 3.19758\n",
      "epoch no.4 train no.310830  loss = 2.85651 avg_loss = 3.19991\n",
      "epoch no.4 train no.310840  loss = 2.97743 avg_loss = 3.21162\n",
      "epoch no.4 train no.310850  loss = 3.66542 avg_loss = 3.20568\n",
      "epoch no.4 train no.310860  loss = 3.51480 avg_loss = 3.17963\n",
      "epoch no.4 train no.310870  loss = 3.09933 avg_loss = 3.16240\n",
      "epoch no.4 train no.310880  loss = 4.55415 avg_loss = 3.18335\n",
      "epoch no.4 train no.310890  loss = 4.01048 avg_loss = 3.16711\n",
      "epoch no.4 train no.310900  loss = 3.07290 avg_loss = 3.18469\n",
      "epoch no.4 train no.310910  loss = 2.99314 avg_loss = 3.14472\n",
      "epoch no.4 train no.310920  loss = 2.19541 avg_loss = 3.13022\n",
      "epoch no.4 train no.310930  loss = 1.99247 avg_loss = 3.16803\n",
      "epoch no.4 train no.310940  loss = 3.31027 avg_loss = 3.18608\n",
      "epoch no.4 train no.310950  loss = 2.61146 avg_loss = 3.19068\n",
      "epoch no.4 train no.310960  loss = 3.96113 avg_loss = 3.24561\n",
      "epoch no.4 train no.310970  loss = 2.64407 avg_loss = 3.23841\n",
      "epoch no.4 train no.310980  loss = 3.09705 avg_loss = 3.23298\n",
      "epoch no.4 train no.310990  loss = 2.80191 avg_loss = 3.21710\n",
      "epoch no.4 train no.311000  loss = 4.88486 avg_loss = 3.21236\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '곡', '</s>']\n",
      "추억의 90년대 댄스팝</s>\n",
      "epoch no.4 train no.311010  loss = 1.58973 avg_loss = 3.19517\n",
      "epoch no.4 train no.311020  loss = 4.05636 avg_loss = 3.16618\n",
      "epoch no.4 train no.311030  loss = 3.20098 avg_loss = 3.20732\n",
      "epoch no.4 train no.311040  loss = 5.59099 avg_loss = 3.20980\n",
      "epoch no.4 train no.311050  loss = 2.61147 avg_loss = 3.22320\n",
      "epoch no.4 train no.311060  loss = 3.23936 avg_loss = 3.21115\n",
      "epoch no.4 train no.311070  loss = 3.16704 avg_loss = 3.18962\n",
      "epoch no.4 train no.311080  loss = 3.66413 avg_loss = 3.16043\n",
      "epoch no.4 train no.311090  loss = 4.20194 avg_loss = 3.16020\n",
      "epoch no.4 train no.311100  loss = 2.26333 avg_loss = 3.13795\n",
      "epoch no.4 train no.311110  loss = 2.63649 avg_loss = 3.15532\n",
      "epoch no.4 train no.311120  loss = 2.63656 avg_loss = 3.13776\n",
      "epoch no.4 train no.311130  loss = 2.88284 avg_loss = 3.19682\n",
      "epoch no.4 train no.311140  loss = 2.49925 avg_loss = 3.20689\n",
      "epoch no.4 train no.311150  loss = 2.71809 avg_loss = 3.17467\n",
      "epoch no.4 train no.311160  loss = 2.51729 avg_loss = 3.17845\n",
      "epoch no.4 train no.311170  loss = 4.41805 avg_loss = 3.18736\n",
      "epoch no.4 train no.311180  loss = 2.50758 avg_loss = 3.17367\n",
      "epoch no.4 train no.311190  loss = 3.01729 avg_loss = 3.16408\n",
      "epoch no.4 train no.311200  loss = 3.11761 avg_loss = 3.13223\n",
      "epoch no.4 train no.311210  loss = 2.87563 avg_loss = 3.10736\n",
      "epoch no.4 train no.311220  loss = 3.58528 avg_loss = 3.09548\n",
      "epoch no.4 train no.311230  loss = 3.03620 avg_loss = 3.08887\n",
      "epoch no.4 train no.311240  loss = 2.27499 avg_loss = 3.10501\n",
      "epoch no.4 train no.311250  loss = 2.26598 avg_loss = 3.11216\n",
      "epoch no.4 train no.311260  loss = 2.70167 avg_loss = 3.15294\n",
      "epoch no.4 train no.311270  loss = 2.49886 avg_loss = 3.12554\n",
      "epoch no.4 train no.311280  loss = 3.60088 avg_loss = 3.12561\n",
      "epoch no.4 train no.311290  loss = 3.57654 avg_loss = 3.12534\n",
      "epoch no.4 train no.311300  loss = 3.99953 avg_loss = 3.14064\n",
      "epoch no.4 train no.311310  loss = 4.17676 avg_loss = 3.14418\n",
      "epoch no.4 train no.311320  loss = 1.90851 avg_loss = 3.14577\n",
      "epoch no.4 train no.311330  loss = 2.30686 avg_loss = 3.12058\n",
      "epoch no.4 train no.311340  loss = 2.38744 avg_loss = 3.12900\n",
      "epoch no.4 train no.311350  loss = 2.61584 avg_loss = 3.10959\n",
      "epoch no.4 train no.311360  loss = 2.29109 avg_loss = 3.08843\n",
      "epoch no.4 train no.311370  loss = 3.58350 avg_loss = 3.13341\n",
      "epoch no.4 train no.311380  loss = 2.77209 avg_loss = 3.09486\n",
      "epoch no.4 train no.311390  loss = 4.21398 avg_loss = 3.14133\n",
      "epoch no.4 train no.311400  loss = 3.17152 avg_loss = 3.14869\n",
      "epoch no.4 train no.311410  loss = 2.88326 avg_loss = 3.12415\n",
      "epoch no.4 train no.311420  loss = 6.22087 avg_loss = 3.13542\n",
      "epoch no.4 train no.311430  loss = 3.10256 avg_loss = 3.12189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.311440  loss = 1.70522 avg_loss = 3.13377\n",
      "epoch no.4 train no.311450  loss = 2.54020 avg_loss = 3.11184\n",
      "epoch no.4 train no.311460  loss = 2.21279 avg_loss = 3.11713\n",
      "epoch no.4 train no.311470  loss = 1.58852 avg_loss = 3.06358\n",
      "epoch no.4 train no.311480  loss = 3.07826 avg_loss = 3.05046\n",
      "epoch no.4 train no.311490  loss = 2.66003 avg_loss = 3.03536\n",
      "epoch no.4 train no.311500  loss = 4.11989 avg_loss = 3.06695\n",
      "epoch no.4 train no.311510  loss = 2.96480 avg_loss = 3.08124\n",
      "epoch no.4 train no.311520  loss = 4.38703 avg_loss = 3.10724\n",
      "epoch no.4 train no.311530  loss = 2.34831 avg_loss = 3.10973\n",
      "epoch no.4 train no.311540  loss = 3.38948 avg_loss = 3.09239\n",
      "epoch no.4 train no.311550  loss = 1.97582 avg_loss = 3.06480\n",
      "epoch no.4 train no.311560  loss = 2.79900 avg_loss = 3.06403\n",
      "epoch no.4 train no.311570  loss = 4.36524 avg_loss = 3.05491\n",
      "epoch no.4 train no.311580  loss = 3.12515 avg_loss = 3.04242\n",
      "epoch no.4 train no.311590  loss = 3.61958 avg_loss = 3.06044\n",
      "epoch no.4 train no.311600  loss = 4.79217 avg_loss = 3.07131\n",
      "epoch no.4 train no.311610  loss = 2.34329 avg_loss = 3.07541\n",
      "epoch no.4 train no.311620  loss = 2.48577 avg_loss = 3.07159\n",
      "epoch no.4 train no.311630  loss = 1.56926 avg_loss = 3.04494\n",
      "epoch no.4 train no.311640  loss = 4.07011 avg_loss = 3.10745\n",
      "epoch no.4 train no.311650  loss = 3.33003 avg_loss = 3.12754\n",
      "epoch no.4 train no.311660  loss = 2.60196 avg_loss = 3.11906\n",
      "epoch no.4 train no.311670  loss = 2.96851 avg_loss = 3.15635\n",
      "epoch no.4 train no.311680  loss = 3.71142 avg_loss = 3.18865\n",
      "epoch no.4 train no.311690  loss = 3.73302 avg_loss = 3.17309\n",
      "epoch no.4 train no.311700  loss = 2.59501 avg_loss = 3.12849\n",
      "epoch no.4 train no.311710  loss = 3.50524 avg_loss = 3.16538\n",
      "epoch no.4 train no.311720  loss = 4.31506 avg_loss = 3.16003\n",
      "epoch no.4 train no.311730  loss = 4.36153 avg_loss = 3.18016\n",
      "epoch no.4 train no.311740  loss = 2.74376 avg_loss = 3.17340\n",
      "epoch no.4 train no.311750  loss = 3.98515 avg_loss = 3.19554\n",
      "epoch no.4 train no.311760  loss = 2.58101 avg_loss = 3.15500\n",
      "epoch no.4 train no.311770  loss = 3.58172 avg_loss = 3.13337\n",
      "epoch no.4 train no.311780  loss = 2.86700 avg_loss = 3.07455\n",
      "epoch no.4 train no.311790  loss = 2.51261 avg_loss = 3.07031\n",
      "epoch no.4 train no.311800  loss = 4.38218 avg_loss = 3.06520\n",
      "epoch no.4 train no.311810  loss = 1.95940 avg_loss = 3.06018\n",
      "epoch no.4 train no.311820  loss = 3.77115 avg_loss = 3.07448\n",
      "epoch no.4 train no.311830  loss = 3.50158 avg_loss = 3.13299\n",
      "epoch no.4 train no.311840  loss = 4.26611 avg_loss = 3.17393\n",
      "epoch no.4 train no.311850  loss = 3.02741 avg_loss = 3.15170\n",
      "epoch no.4 train no.311860  loss = 2.03698 avg_loss = 3.14840\n",
      "epoch no.4 train no.311870  loss = 4.23156 avg_loss = 3.19673\n",
      "epoch no.4 train no.311880  loss = 2.39477 avg_loss = 3.15493\n",
      "epoch no.4 train no.311890  loss = 4.17103 avg_loss = 3.18830\n",
      "epoch no.4 train no.311900  loss = 3.46618 avg_loss = 3.18373\n",
      "epoch no.4 train no.311910  loss = 2.88418 avg_loss = 3.18332\n",
      "epoch no.4 train no.311920  loss = 2.85210 avg_loss = 3.17378\n",
      "epoch no.4 train no.311930  loss = 3.87572 avg_loss = 3.19079\n",
      "epoch no.4 train no.311940  loss = 2.51698 avg_loss = 3.20153\n",
      "epoch no.4 train no.311950  loss = 1.85910 avg_loss = 3.19287\n",
      "epoch no.4 train no.311960  loss = 2.97206 avg_loss = 3.19747\n",
      "epoch no.4 train no.311970  loss = 3.21322 avg_loss = 3.17455\n",
      "epoch no.4 train no.311980  loss = 4.17411 avg_loss = 3.17078\n",
      "epoch no.4 train no.311990  loss = 3.37115 avg_loss = 3.18125\n",
      "epoch no.4 train no.312000  loss = 2.28549 avg_loss = 3.17117\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '80', '년대', '모', '</s>']\n",
      "추억의 7080 가요 모음</s>\n",
      "epoch no.4 train no.312010  loss = 2.79996 avg_loss = 3.13009\n",
      "epoch no.4 train no.312020  loss = 3.64537 avg_loss = 3.14933\n",
      "epoch no.4 train no.312030  loss = 3.17865 avg_loss = 3.15689\n",
      "epoch no.4 train no.312040  loss = 2.75714 avg_loss = 3.16609\n",
      "epoch no.4 train no.312050  loss = 4.08453 avg_loss = 3.18205\n",
      "epoch no.4 train no.312060  loss = 4.44764 avg_loss = 3.23749\n",
      "epoch no.4 train no.312070  loss = 3.78453 avg_loss = 3.19482\n",
      "epoch no.4 train no.312080  loss = 4.76518 avg_loss = 3.20282\n",
      "epoch no.4 train no.312090  loss = 2.69187 avg_loss = 3.13449\n",
      "epoch no.4 train no.312100  loss = 3.41552 avg_loss = 3.14145\n",
      "epoch no.4 train no.312110  loss = 4.63427 avg_loss = 3.13956\n",
      "epoch no.4 train no.312120  loss = 2.23189 avg_loss = 3.10714\n",
      "epoch no.4 train no.312130  loss = 3.82056 avg_loss = 3.11472\n",
      "epoch no.4 train no.312140  loss = 2.24575 avg_loss = 3.10846\n",
      "epoch no.4 train no.312150  loss = 2.63759 avg_loss = 3.13241\n",
      "epoch no.4 train no.312160  loss = 3.64855 avg_loss = 3.08601\n",
      "epoch no.4 train no.312170  loss = 2.64141 avg_loss = 3.09150\n",
      "epoch no.4 train no.312180  loss = 3.31594 avg_loss = 3.09777\n",
      "epoch no.4 train no.312190  loss = 2.07283 avg_loss = 3.10958\n",
      "epoch no.4 train no.312200  loss = 2.63197 avg_loss = 3.07167\n",
      "epoch no.4 train no.312210  loss = 2.91680 avg_loss = 3.03351\n",
      "epoch no.4 train no.312220  loss = 3.96603 avg_loss = 3.03971\n",
      "epoch no.4 train no.312230  loss = 2.87398 avg_loss = 3.04418\n",
      "epoch no.4 train no.312240  loss = 3.06363 avg_loss = 3.01831\n",
      "epoch no.4 train no.312250  loss = 2.20106 avg_loss = 2.98630\n",
      "epoch no.4 train no.312260  loss = 2.70272 avg_loss = 2.97183\n",
      "epoch no.4 train no.312270  loss = 1.83798 avg_loss = 2.98339\n",
      "epoch no.4 train no.312280  loss = 3.84741 avg_loss = 2.99255\n",
      "epoch no.4 train no.312290  loss = 2.94704 avg_loss = 2.98160\n",
      "epoch no.4 train no.312300  loss = 2.82709 avg_loss = 2.98155\n",
      "epoch no.4 train no.312310  loss = 4.26489 avg_loss = 3.03028\n",
      "epoch no.4 train no.312320  loss = 3.55253 avg_loss = 3.04250\n",
      "epoch no.4 train no.312330  loss = 2.70437 avg_loss = 3.05921\n",
      "epoch no.4 train no.312340  loss = 2.59476 avg_loss = 3.09911\n",
      "epoch no.4 train no.312350  loss = 3.33597 avg_loss = 3.11694\n",
      "epoch no.4 train no.312360  loss = 2.97643 avg_loss = 3.10208\n",
      "epoch no.4 train no.312370  loss = 2.69529 avg_loss = 3.13382\n",
      "epoch no.4 train no.312380  loss = 3.69862 avg_loss = 3.11841\n",
      "epoch no.4 train no.312390  loss = 2.70784 avg_loss = 3.12023\n",
      "epoch no.4 train no.312400  loss = 4.19501 avg_loss = 3.12457\n",
      "epoch no.4 train no.312410  loss = 3.43829 avg_loss = 3.13973\n",
      "epoch no.4 train no.312420  loss = 3.53579 avg_loss = 3.13297\n",
      "epoch no.4 train no.312430  loss = 2.63042 avg_loss = 3.13308\n",
      "epoch no.4 train no.312440  loss = 4.63477 avg_loss = 3.10799\n",
      "epoch no.4 train no.312450  loss = 2.13067 avg_loss = 3.05120\n",
      "epoch no.4 train no.312460  loss = 5.28703 avg_loss = 3.10328\n",
      "epoch no.4 train no.312470  loss = 4.11229 avg_loss = 3.11407\n",
      "epoch no.4 train no.312480  loss = 1.19040 avg_loss = 3.09610\n",
      "epoch no.4 train no.312490  loss = 2.26137 avg_loss = 3.07457\n",
      "epoch no.4 train no.312500  loss = 2.32856 avg_loss = 3.04997\n",
      "epoch no.4 train no.312510  loss = 3.05513 avg_loss = 3.04030\n",
      "epoch no.4 train no.312520  loss = 2.99004 avg_loss = 3.04820\n",
      "epoch no.4 train no.312530  loss = 1.63919 avg_loss = 3.01496\n",
      "epoch no.4 train no.312540  loss = 2.74997 avg_loss = 2.99130\n",
      "epoch no.4 train no.312550  loss = 5.71479 avg_loss = 3.04125\n",
      "epoch no.4 train no.312560  loss = 3.26432 avg_loss = 3.03847\n",
      "epoch no.4 train no.312570  loss = 1.50857 avg_loss = 3.04286\n",
      "epoch no.4 train no.312580  loss = 2.88835 avg_loss = 3.06536\n",
      "epoch no.4 train no.312590  loss = 1.83368 avg_loss = 3.05448\n",
      "epoch no.4 train no.312600  loss = 2.09113 avg_loss = 3.04809\n",
      "epoch no.4 train no.312610  loss = 3.12602 avg_loss = 3.04006\n",
      "epoch no.4 train no.312620  loss = 4.19928 avg_loss = 3.05137\n",
      "epoch no.4 train no.312630  loss = 1.49849 avg_loss = 3.04122\n",
      "epoch no.4 train no.312640  loss = 2.88326 avg_loss = 3.05092\n",
      "epoch no.4 train no.312650  loss = 4.14463 avg_loss = 3.05012\n",
      "epoch no.4 train no.312660  loss = 4.06075 avg_loss = 3.08128\n",
      "epoch no.4 train no.312670  loss = 1.82133 avg_loss = 3.06567\n",
      "epoch no.4 train no.312680  loss = 4.99212 avg_loss = 3.06225\n",
      "epoch no.4 train no.312690  loss = 3.00744 avg_loss = 3.10479\n",
      "epoch no.4 train no.312700  loss = 4.26157 avg_loss = 3.12441\n",
      "epoch no.4 train no.312710  loss = 4.50315 avg_loss = 3.12167\n",
      "epoch no.4 train no.312720  loss = 3.77792 avg_loss = 3.11644\n",
      "epoch no.4 train no.312730  loss = 2.88837 avg_loss = 3.12825\n",
      "epoch no.4 train no.312740  loss = 3.26873 avg_loss = 3.14676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.312750  loss = 2.96866 avg_loss = 3.16578\n",
      "epoch no.4 train no.312760  loss = 2.93039 avg_loss = 3.14995\n",
      "epoch no.4 train no.312770  loss = 2.76887 avg_loss = 3.14884\n",
      "epoch no.4 train no.312780  loss = 2.84068 avg_loss = 3.15459\n",
      "epoch no.4 train no.312790  loss = 2.38785 avg_loss = 3.11361\n",
      "epoch no.4 train no.312800  loss = 3.87654 avg_loss = 3.11754\n",
      "epoch no.4 train no.312810  loss = 3.12245 avg_loss = 3.12436\n",
      "epoch no.4 train no.312820  loss = 2.90028 avg_loss = 3.15164\n",
      "epoch no.4 train no.312830  loss = 3.11798 avg_loss = 3.14297\n",
      "epoch no.4 train no.312840  loss = 3.48857 avg_loss = 3.13829\n",
      "epoch no.4 train no.312850  loss = 4.00571 avg_loss = 3.17006\n",
      "epoch no.4 train no.312860  loss = 3.82094 avg_loss = 3.15316\n",
      "epoch no.4 train no.312870  loss = 3.10098 avg_loss = 3.15118\n",
      "epoch no.4 train no.312880  loss = 3.06335 avg_loss = 3.10431\n",
      "epoch no.4 train no.312890  loss = 4.74014 avg_loss = 3.13862\n",
      "epoch no.4 train no.312900  loss = 3.43871 avg_loss = 3.15704\n",
      "epoch no.4 train no.312910  loss = 3.76943 avg_loss = 3.14263\n",
      "epoch no.4 train no.312920  loss = 2.84599 avg_loss = 3.15243\n",
      "epoch no.4 train no.312930  loss = 3.49509 avg_loss = 3.11970\n",
      "epoch no.4 train no.312940  loss = 1.99253 avg_loss = 3.08773\n",
      "epoch no.4 train no.312950  loss = 3.18862 avg_loss = 3.08176\n",
      "epoch no.4 train no.312960  loss = 3.13516 avg_loss = 3.14609\n",
      "epoch no.4 train no.312970  loss = 4.43340 avg_loss = 3.17022\n",
      "epoch no.4 train no.312980  loss = 5.07189 avg_loss = 3.16110\n",
      "epoch no.4 train no.312990  loss = 3.63619 avg_loss = 3.17931\n",
      "epoch no.4 train no.313000  loss = 2.92970 avg_loss = 3.17281\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '곡', '▁모음', '집', '</s>']\n",
      "추억의 댄스곡 모음집</s>\n",
      "epoch no.4 train no.313010  loss = 2.96796 avg_loss = 3.18500\n",
      "epoch no.4 train no.313020  loss = 3.65148 avg_loss = 3.17008\n",
      "epoch no.4 train no.313030  loss = 3.49775 avg_loss = 3.13058\n",
      "epoch no.4 train no.313040  loss = 2.70636 avg_loss = 3.14859\n",
      "epoch no.4 train no.313050  loss = 3.64261 avg_loss = 3.11772\n",
      "epoch no.4 train no.313060  loss = 3.06245 avg_loss = 3.11298\n",
      "epoch no.4 train no.313070  loss = 2.13312 avg_loss = 3.10450\n",
      "epoch no.4 train no.313080  loss = 3.13327 avg_loss = 3.13021\n",
      "epoch no.4 train no.313090  loss = 3.52237 avg_loss = 3.15454\n",
      "epoch no.4 train no.313100  loss = 3.66022 avg_loss = 3.17683\n",
      "epoch no.4 train no.313110  loss = 2.18742 avg_loss = 3.19543\n",
      "epoch no.4 train no.313120  loss = 3.19835 avg_loss = 3.19630\n",
      "epoch no.4 train no.313130  loss = 2.99891 avg_loss = 3.20779\n",
      "epoch no.4 train no.313140  loss = 2.68427 avg_loss = 3.17433\n",
      "epoch no.4 train no.313150  loss = 2.87051 avg_loss = 3.16321\n",
      "epoch no.4 train no.313160  loss = 2.30183 avg_loss = 3.13337\n",
      "epoch no.4 train no.313170  loss = 3.13987 avg_loss = 3.13038\n",
      "epoch no.4 train no.313180  loss = 3.44520 avg_loss = 3.14875\n",
      "epoch no.4 train no.313190  loss = 2.41061 avg_loss = 3.16092\n",
      "epoch no.4 train no.313200  loss = 1.90718 avg_loss = 3.16583\n",
      "epoch no.4 train no.313210  loss = 2.27520 avg_loss = 3.15814\n",
      "epoch no.4 train no.313220  loss = 3.49767 avg_loss = 3.13982\n",
      "epoch no.4 train no.313230  loss = 1.86362 avg_loss = 3.10589\n",
      "epoch no.4 train no.313240  loss = 3.20028 avg_loss = 3.12743\n",
      "epoch no.4 train no.313250  loss = 2.79749 avg_loss = 3.09970\n",
      "epoch no.4 train no.313260  loss = 3.45661 avg_loss = 3.08171\n",
      "epoch no.4 train no.313270  loss = 2.64228 avg_loss = 3.05205\n",
      "epoch no.4 train no.313280  loss = 2.66963 avg_loss = 3.09027\n",
      "epoch no.4 train no.313290  loss = 2.15241 avg_loss = 3.06783\n",
      "epoch no.4 train no.313300  loss = 3.07892 avg_loss = 3.08266\n",
      "epoch no.4 train no.313310  loss = 2.92656 avg_loss = 3.05754\n",
      "epoch no.4 train no.313320  loss = 2.89547 avg_loss = 3.06830\n",
      "epoch no.4 train no.313330  loss = 2.83181 avg_loss = 3.10117\n",
      "epoch no.4 train no.313340  loss = 2.80846 avg_loss = 3.16198\n",
      "epoch no.4 train no.313350  loss = 2.87900 avg_loss = 3.16576\n",
      "epoch no.4 train no.313360  loss = 2.94062 avg_loss = 3.09758\n",
      "epoch no.4 train no.313370  loss = 2.71745 avg_loss = 3.08252\n",
      "epoch no.4 train no.313380  loss = 4.22131 avg_loss = 3.06465\n",
      "epoch no.4 train no.313390  loss = 6.28687 avg_loss = 3.10963\n",
      "epoch no.4 train no.313400  loss = 2.32286 avg_loss = 3.11632\n",
      "epoch no.4 train no.313410  loss = 4.01912 avg_loss = 3.18181\n",
      "epoch no.4 train no.313420  loss = 2.12713 avg_loss = 3.22967\n",
      "epoch no.4 train no.313430  loss = 2.03932 avg_loss = 3.17922\n",
      "epoch no.4 train no.313440  loss = 3.02156 avg_loss = 3.17670\n",
      "epoch no.4 train no.313450  loss = 1.68137 avg_loss = 3.15878\n",
      "epoch no.4 train no.313460  loss = 3.45784 avg_loss = 3.12841\n",
      "epoch no.4 train no.313470  loss = 2.29172 avg_loss = 3.15283\n",
      "epoch no.4 train no.313480  loss = 2.58485 avg_loss = 3.17161\n",
      "epoch no.4 train no.313490  loss = 2.88398 avg_loss = 3.13732\n",
      "epoch no.4 train no.313500  loss = 2.35533 avg_loss = 3.11506\n",
      "epoch no.4 train no.313510  loss = 1.51930 avg_loss = 3.09383\n",
      "epoch no.4 train no.313520  loss = 2.55810 avg_loss = 3.13793\n",
      "epoch no.4 train no.313530  loss = 3.83919 avg_loss = 3.15475\n",
      "epoch no.4 train no.313540  loss = 5.30660 avg_loss = 3.14487\n",
      "epoch no.4 train no.313550  loss = 3.11755 avg_loss = 3.17810\n",
      "epoch no.4 train no.313560  loss = 3.06347 avg_loss = 3.15271\n",
      "epoch no.4 train no.313570  loss = 3.20392 avg_loss = 3.16454\n",
      "epoch no.4 train no.313580  loss = 2.88057 avg_loss = 3.13867\n",
      "epoch no.4 train no.313590  loss = 3.95536 avg_loss = 3.11837\n",
      "epoch no.4 train no.313600  loss = 3.70679 avg_loss = 3.11185\n",
      "epoch no.4 train no.313610  loss = 2.75175 avg_loss = 3.11984\n",
      "epoch no.4 train no.313620  loss = 3.19162 avg_loss = 3.12810\n",
      "epoch no.4 train no.313630  loss = 2.53133 avg_loss = 3.15589\n",
      "epoch no.4 train no.313640  loss = 1.92271 avg_loss = 3.16635\n",
      "epoch no.4 train no.313650  loss = 2.18684 avg_loss = 3.11431\n",
      "epoch no.4 train no.313660  loss = 2.23273 avg_loss = 3.09604\n",
      "epoch no.4 train no.313670  loss = 4.11778 avg_loss = 3.10123\n",
      "epoch no.4 train no.313680  loss = 3.30568 avg_loss = 3.07187\n",
      "epoch no.4 train no.313690  loss = 3.10987 avg_loss = 3.04618\n",
      "epoch no.4 train no.313700  loss = 2.14772 avg_loss = 3.00259\n",
      "epoch no.4 train no.313710  loss = 4.87041 avg_loss = 3.03604\n",
      "epoch no.4 train no.313720  loss = 3.15124 avg_loss = 3.04964\n",
      "epoch no.4 train no.313730  loss = 3.21336 avg_loss = 3.06119\n",
      "epoch no.4 train no.313740  loss = 2.20921 avg_loss = 3.04647\n",
      "epoch no.4 train no.313750  loss = 2.91110 avg_loss = 3.04052\n",
      "epoch no.4 train no.313760  loss = 6.84827 avg_loss = 3.09604\n",
      "epoch no.4 train no.313770  loss = 2.26632 avg_loss = 3.13261\n",
      "epoch no.4 train no.313780  loss = 2.27018 avg_loss = 3.11446\n",
      "epoch no.4 train no.313790  loss = 2.24007 avg_loss = 3.08837\n",
      "epoch no.4 train no.313800  loss = 2.54224 avg_loss = 3.07839\n",
      "epoch no.4 train no.313810  loss = 1.66522 avg_loss = 3.08000\n",
      "epoch no.4 train no.313820  loss = 2.06916 avg_loss = 3.07155\n",
      "epoch no.4 train no.313830  loss = 3.81082 avg_loss = 3.12343\n",
      "epoch no.4 train no.313840  loss = 2.10625 avg_loss = 3.11754\n",
      "epoch no.4 train no.313850  loss = 2.64874 avg_loss = 3.12907\n",
      "epoch no.4 train no.313860  loss = 3.37747 avg_loss = 3.07208\n",
      "epoch no.4 train no.313870  loss = 3.28027 avg_loss = 3.03986\n",
      "epoch no.4 train no.313880  loss = 2.19548 avg_loss = 2.98873\n",
      "epoch no.4 train no.313890  loss = 2.09692 avg_loss = 2.98702\n",
      "epoch no.4 train no.313900  loss = 3.41572 avg_loss = 3.01821\n",
      "epoch no.4 train no.313910  loss = 5.18390 avg_loss = 3.07269\n",
      "epoch no.4 train no.313920  loss = 2.26380 avg_loss = 3.07640\n",
      "epoch no.4 train no.313930  loss = 2.73190 avg_loss = 3.12028\n",
      "epoch no.4 train no.313940  loss = 5.94927 avg_loss = 3.16874\n",
      "epoch no.4 train no.313950  loss = 2.87478 avg_loss = 3.18912\n",
      "epoch no.4 train no.313960  loss = 1.96204 avg_loss = 3.16812\n",
      "epoch no.4 train no.313970  loss = 3.45293 avg_loss = 3.17161\n",
      "epoch no.4 train no.313980  loss = 3.18766 avg_loss = 3.22455\n",
      "epoch no.4 train no.313990  loss = 4.40618 avg_loss = 3.24218\n",
      "epoch no.4 train no.314000  loss = 4.19074 avg_loss = 3.25243\n",
      "4\n",
      "to_tokens: ['▁가을', '▁명', '년대', '▁발라', '요', '</s>']\n",
      "추억의 90년대 인기가요</s>\n",
      "epoch no.4 train no.314010  loss = 3.35613 avg_loss = 3.24145\n",
      "epoch no.4 train no.314020  loss = 3.30338 avg_loss = 3.23790\n",
      "epoch no.4 train no.314030  loss = 4.63614 avg_loss = 3.28870\n",
      "epoch no.4 train no.314040  loss = 3.54947 avg_loss = 3.30878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.314050  loss = 3.08244 avg_loss = 3.31161\n",
      "epoch no.4 train no.314060  loss = 2.95949 avg_loss = 3.28647\n",
      "epoch no.4 train no.314070  loss = 4.53355 avg_loss = 3.28706\n",
      "epoch no.4 train no.314080  loss = 1.84935 avg_loss = 3.28128\n",
      "epoch no.4 train no.314090  loss = 2.91026 avg_loss = 3.26633\n",
      "epoch no.4 train no.314100  loss = 3.45471 avg_loss = 3.22869\n",
      "epoch no.4 train no.314110  loss = 2.46635 avg_loss = 3.17928\n",
      "epoch no.4 train no.314120  loss = 2.74192 avg_loss = 3.13065\n",
      "epoch no.4 train no.314130  loss = 3.98987 avg_loss = 3.13069\n",
      "epoch no.4 train no.314140  loss = 3.99413 avg_loss = 3.13353\n",
      "epoch no.4 train no.314150  loss = 3.21918 avg_loss = 3.11495\n",
      "epoch no.4 train no.314160  loss = 2.19731 avg_loss = 3.10867\n",
      "epoch no.4 train no.314170  loss = 2.56607 avg_loss = 3.08312\n",
      "epoch no.4 train no.314180  loss = 2.68944 avg_loss = 3.09580\n",
      "epoch no.4 train no.314190  loss = 4.37647 avg_loss = 3.08829\n",
      "epoch no.4 train no.314200  loss = 2.87316 avg_loss = 3.13121\n",
      "epoch no.4 train no.314210  loss = 2.76310 avg_loss = 3.10616\n",
      "epoch no.4 train no.314220  loss = 2.62826 avg_loss = 3.08628\n",
      "epoch no.4 train no.314230  loss = 3.32479 avg_loss = 3.11199\n",
      "epoch no.4 train no.314240  loss = 4.51097 avg_loss = 3.11958\n",
      "epoch no.4 train no.314250  loss = 2.05240 avg_loss = 3.16795\n",
      "epoch no.4 train no.314260  loss = 3.40481 avg_loss = 3.14107\n",
      "epoch no.4 train no.314270  loss = 1.39329 avg_loss = 3.10764\n",
      "epoch no.4 train no.314280  loss = 4.07449 avg_loss = 3.07605\n",
      "epoch no.4 train no.314290  loss = 3.28810 avg_loss = 3.09468\n",
      "epoch no.4 train no.314300  loss = 2.54198 avg_loss = 3.06465\n",
      "epoch no.4 train no.314310  loss = 1.76391 avg_loss = 3.07405\n",
      "epoch no.4 train no.314320  loss = 4.49839 avg_loss = 3.14972\n",
      "epoch no.4 train no.314330  loss = 2.87778 avg_loss = 3.14843\n",
      "epoch no.4 train no.314340  loss = 3.24863 avg_loss = 3.11345\n",
      "epoch no.4 train no.314350  loss = 3.29596 avg_loss = 3.11398\n",
      "epoch no.4 train no.314360  loss = 2.46956 avg_loss = 3.12001\n",
      "epoch no.4 train no.314370  loss = 2.23690 avg_loss = 3.09624\n",
      "epoch no.4 train no.314380  loss = 1.97168 avg_loss = 3.09916\n",
      "epoch no.4 train no.314390  loss = 5.06945 avg_loss = 3.08418\n",
      "epoch no.4 train no.314400  loss = 2.39538 avg_loss = 3.08669\n",
      "epoch no.4 train no.314410  loss = 1.62700 avg_loss = 3.08204\n",
      "epoch no.4 train no.314420  loss = 2.36735 avg_loss = 3.10474\n",
      "epoch no.4 train no.314430  loss = 2.16136 avg_loss = 3.09977\n",
      "epoch no.4 train no.314440  loss = 3.78145 avg_loss = 3.15136\n",
      "epoch no.4 train no.314450  loss = 4.27482 avg_loss = 3.15247\n",
      "epoch no.4 train no.314460  loss = 2.06722 avg_loss = 3.16465\n",
      "epoch no.4 train no.314470  loss = 2.19450 avg_loss = 3.14945\n",
      "epoch no.4 train no.314480  loss = 2.62773 avg_loss = 3.11984\n",
      "epoch no.4 train no.314490  loss = 3.04776 avg_loss = 3.12644\n",
      "epoch no.4 train no.314500  loss = 1.99990 avg_loss = 3.11335\n",
      "epoch no.4 train no.314510  loss = 3.04960 avg_loss = 3.08487\n",
      "epoch no.4 train no.314520  loss = 2.91075 avg_loss = 3.05318\n",
      "epoch no.4 train no.314530  loss = 2.82088 avg_loss = 3.06052\n",
      "epoch no.4 train no.314540  loss = 1.95012 avg_loss = 3.09538\n",
      "epoch no.4 train no.314550  loss = 3.57829 avg_loss = 3.09555\n",
      "epoch no.4 train no.314560  loss = 2.57149 avg_loss = 3.12400\n",
      "epoch no.4 train no.314570  loss = 2.94353 avg_loss = 3.13271\n",
      "epoch no.4 train no.314580  loss = 4.11476 avg_loss = 3.14806\n",
      "epoch no.4 train no.314590  loss = 4.20361 avg_loss = 3.18560\n",
      "epoch no.4 train no.314600  loss = 1.91646 avg_loss = 3.18801\n",
      "epoch no.4 train no.314610  loss = 3.73840 avg_loss = 3.18061\n",
      "epoch no.4 train no.314620  loss = 3.50800 avg_loss = 3.14670\n",
      "epoch no.4 train no.314630  loss = 3.37143 avg_loss = 3.14575\n",
      "epoch no.4 train no.314640  loss = 3.09849 avg_loss = 3.17296\n",
      "epoch no.4 train no.314650  loss = 4.91952 avg_loss = 3.19612\n",
      "epoch no.4 train no.314660  loss = 1.71560 avg_loss = 3.15750\n",
      "epoch no.4 train no.314670  loss = 2.58666 avg_loss = 3.12805\n",
      "epoch no.4 train no.314680  loss = 3.31808 avg_loss = 3.07638\n",
      "epoch no.4 train no.314690  loss = 3.91799 avg_loss = 3.07794\n",
      "epoch no.4 train no.314700  loss = 2.81470 avg_loss = 3.08405\n",
      "epoch no.4 train no.314710  loss = 6.02885 avg_loss = 3.10015\n",
      "epoch no.4 train no.314720  loss = 3.35229 avg_loss = 3.11872\n",
      "epoch no.4 train no.314730  loss = 4.69327 avg_loss = 3.17042\n",
      "epoch no.4 train no.314740  loss = 3.18346 avg_loss = 3.16837\n",
      "epoch no.4 train no.314750  loss = 2.27175 avg_loss = 3.16234\n",
      "epoch no.4 train no.314760  loss = 2.27411 avg_loss = 3.14326\n",
      "epoch no.4 train no.314770  loss = 3.35293 avg_loss = 3.12862\n",
      "epoch no.4 train no.314780  loss = 1.86313 avg_loss = 3.15748\n",
      "epoch no.4 train no.314790  loss = 2.06541 avg_loss = 3.16607\n",
      "epoch no.4 train no.314800  loss = 3.58998 avg_loss = 3.15777\n",
      "epoch no.4 train no.314810  loss = 2.84059 avg_loss = 3.15751\n",
      "epoch no.4 train no.314820  loss = 3.98906 avg_loss = 3.13860\n",
      "epoch no.4 train no.314830  loss = 2.75937 avg_loss = 3.12708\n",
      "epoch no.4 train no.314840  loss = 3.50028 avg_loss = 3.13347\n",
      "epoch no.4 train no.314850  loss = 2.75468 avg_loss = 3.16250\n",
      "epoch no.4 train no.314860  loss = 1.47357 avg_loss = 3.13788\n",
      "epoch no.4 train no.314870  loss = 1.88502 avg_loss = 3.11023\n",
      "epoch no.4 train no.314880  loss = 5.62407 avg_loss = 3.20937\n",
      "epoch no.4 train no.314890  loss = 3.01076 avg_loss = 3.20396\n",
      "epoch no.4 train no.314900  loss = 4.69805 avg_loss = 3.21396\n",
      "epoch no.4 train no.314910  loss = 2.20616 avg_loss = 3.21260\n",
      "epoch no.4 train no.314920  loss = 2.82997 avg_loss = 3.19558\n",
      "epoch no.4 train no.314930  loss = 3.14651 avg_loss = 3.19394\n",
      "epoch no.4 train no.314940  loss = 2.78158 avg_loss = 3.16883\n",
      "epoch no.4 train no.314950  loss = 3.26161 avg_loss = 3.16366\n",
      "epoch no.4 train no.314960  loss = 3.19838 avg_loss = 3.21174\n",
      "epoch no.4 train no.314970  loss = 3.36505 avg_loss = 3.21073\n",
      "epoch no.4 train no.314980  loss = 2.57252 avg_loss = 3.17504\n",
      "epoch no.4 train no.314990  loss = 2.83873 avg_loss = 3.13087\n",
      "epoch no.4 train no.315000  loss = 3.22214 avg_loss = 3.10402\n",
      "5\n",
      "to_tokens: ['▁비', '▁드라마', '년대', '▁댄스', '▁가요', '곡', '</s>']\n",
      "추억의 90년대 인기 댄스곡</s>\n",
      "epoch no.4 train no.315010  loss = 1.93642 avg_loss = 3.09030\n",
      "epoch no.4 train no.315020  loss = 2.30945 avg_loss = 3.09520\n",
      "epoch no.4 train no.315030  loss = 3.93117 avg_loss = 3.10036\n",
      "epoch no.4 train no.315040  loss = 2.31309 avg_loss = 3.13516\n",
      "epoch no.4 train no.315050  loss = 4.18109 avg_loss = 3.15983\n",
      "epoch no.4 train no.315060  loss = 2.37550 avg_loss = 3.17785\n",
      "epoch no.4 train no.315070  loss = 3.92704 avg_loss = 3.18458\n",
      "epoch no.4 train no.315080  loss = 3.06146 avg_loss = 3.18712\n",
      "epoch no.4 train no.315090  loss = 2.06797 avg_loss = 3.11967\n",
      "epoch no.4 train no.315100  loss = 1.96595 avg_loss = 3.09587\n",
      "epoch no.4 train no.315110  loss = 2.48492 avg_loss = 3.12298\n",
      "epoch no.4 train no.315120  loss = 3.64000 avg_loss = 3.13872\n",
      "epoch no.4 train no.315130  loss = 4.16870 avg_loss = 3.13814\n",
      "epoch no.4 train no.315140  loss = 2.51897 avg_loss = 3.16609\n",
      "epoch no.4 train no.315150  loss = 4.12893 avg_loss = 3.15155\n",
      "epoch no.4 train no.315160  loss = 3.26955 avg_loss = 3.15723\n",
      "epoch no.4 train no.315170  loss = 3.06783 avg_loss = 3.17232\n",
      "epoch no.4 train no.315180  loss = 1.90726 avg_loss = 3.15550\n",
      "epoch no.4 train no.315190  loss = 4.29573 avg_loss = 3.19057\n",
      "epoch no.4 train no.315200  loss = 3.19174 avg_loss = 3.19400\n",
      "epoch no.4 train no.315210  loss = 2.99972 avg_loss = 3.20709\n",
      "epoch no.4 train no.315220  loss = 1.92013 avg_loss = 3.17010\n",
      "epoch no.4 train no.315230  loss = 2.45235 avg_loss = 3.14647\n",
      "epoch no.4 train no.315240  loss = 4.53801 avg_loss = 3.14706\n",
      "epoch no.4 train no.315250  loss = 3.53962 avg_loss = 3.14405\n",
      "epoch no.4 train no.315260  loss = 3.04348 avg_loss = 3.12577\n",
      "epoch no.4 train no.315270  loss = 2.64893 avg_loss = 3.12806\n",
      "epoch no.4 train no.315280  loss = 2.78255 avg_loss = 3.12312\n",
      "epoch no.4 train no.315290  loss = 3.42595 avg_loss = 3.15663\n",
      "epoch no.4 train no.315300  loss = 3.41427 avg_loss = 3.14393\n",
      "epoch no.4 train no.315310  loss = 2.61015 avg_loss = 3.16547\n",
      "epoch no.4 train no.315320  loss = 3.47633 avg_loss = 3.16551\n",
      "epoch no.4 train no.315330  loss = 2.78371 avg_loss = 3.13981\n",
      "epoch no.4 train no.315340  loss = 3.40033 avg_loss = 3.17701\n",
      "epoch no.4 train no.315350  loss = 2.68144 avg_loss = 3.18079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.315360  loss = 3.19725 avg_loss = 3.18720\n",
      "epoch no.4 train no.315370  loss = 2.73476 avg_loss = 3.17420\n",
      "epoch no.4 train no.315380  loss = 2.74916 avg_loss = 3.14871\n",
      "epoch no.4 train no.315390  loss = 2.54728 avg_loss = 3.13210\n",
      "epoch no.4 train no.315400  loss = 4.04417 avg_loss = 3.17263\n",
      "epoch no.4 train no.315410  loss = 3.12045 avg_loss = 3.14644\n",
      "epoch no.4 train no.315420  loss = 2.99553 avg_loss = 3.12996\n",
      "epoch no.4 train no.315430  loss = 3.58138 avg_loss = 3.08795\n",
      "epoch no.4 train no.315440  loss = 1.96344 avg_loss = 3.08396\n",
      "epoch no.4 train no.315450  loss = 4.07380 avg_loss = 3.09563\n",
      "epoch no.4 train no.315460  loss = 2.87835 avg_loss = 3.08545\n",
      "epoch no.4 train no.315470  loss = 4.81129 avg_loss = 3.10852\n",
      "epoch no.4 train no.315480  loss = 2.09837 avg_loss = 3.12041\n",
      "epoch no.4 train no.315490  loss = 6.23195 avg_loss = 3.15967\n",
      "epoch no.4 train no.315500  loss = 3.85435 avg_loss = 3.16938\n",
      "epoch no.4 train no.315510  loss = 3.67432 avg_loss = 3.18083\n",
      "epoch no.4 train no.315520  loss = 2.75016 avg_loss = 3.15018\n",
      "epoch no.4 train no.315530  loss = 2.34063 avg_loss = 3.13295\n",
      "epoch no.4 train no.315540  loss = 2.21655 avg_loss = 3.08339\n",
      "epoch no.4 train no.315550  loss = 3.42954 avg_loss = 3.10115\n",
      "epoch no.4 train no.315560  loss = 2.47453 avg_loss = 3.09015\n",
      "epoch no.4 train no.315570  loss = 4.00985 avg_loss = 3.12611\n",
      "epoch no.4 train no.315580  loss = 3.41351 avg_loss = 3.12409\n",
      "epoch no.4 train no.315590  loss = 2.90550 avg_loss = 3.11784\n",
      "epoch no.4 train no.315600  loss = 2.23564 avg_loss = 3.13191\n",
      "epoch no.4 train no.315610  loss = 2.52756 avg_loss = 3.15814\n",
      "epoch no.4 train no.315620  loss = 3.01865 avg_loss = 3.12742\n",
      "epoch no.4 train no.315630  loss = 2.58411 avg_loss = 3.10708\n",
      "epoch no.4 train no.315640  loss = 2.14757 avg_loss = 3.09242\n",
      "epoch no.4 train no.315650  loss = 4.55384 avg_loss = 3.09843\n",
      "epoch no.4 train no.315660  loss = 2.66682 avg_loss = 3.10051\n",
      "epoch no.4 train no.315670  loss = 3.03308 avg_loss = 3.08994\n",
      "epoch no.4 train no.315680  loss = 2.25884 avg_loss = 3.09484\n",
      "epoch no.4 train no.315690  loss = 2.43985 avg_loss = 3.10812\n",
      "epoch no.4 train no.315700  loss = 3.98286 avg_loss = 3.11230\n",
      "epoch no.4 train no.315710  loss = 2.82584 avg_loss = 3.10907\n",
      "epoch no.4 train no.315720  loss = 4.35851 avg_loss = 3.13250\n",
      "epoch no.4 train no.315730  loss = 3.39416 avg_loss = 3.13344\n",
      "epoch no.4 train no.315740  loss = 3.56053 avg_loss = 3.17051\n",
      "epoch no.4 train no.315750  loss = 3.65485 avg_loss = 3.15267\n",
      "epoch no.4 train no.315760  loss = 3.65192 avg_loss = 3.14005\n",
      "epoch no.4 train no.315770  loss = 2.92144 avg_loss = 3.12895\n",
      "epoch no.4 train no.315780  loss = 2.98156 avg_loss = 3.14346\n",
      "epoch no.4 train no.315790  loss = 2.78595 avg_loss = 3.10698\n",
      "epoch no.4 train no.315800  loss = 2.00445 avg_loss = 3.09731\n",
      "epoch no.4 train no.315810  loss = 3.09713 avg_loss = 3.09539\n",
      "epoch no.4 train no.315820  loss = 3.19839 avg_loss = 3.12193\n",
      "epoch no.4 train no.315830  loss = 2.45143 avg_loss = 3.12082\n",
      "epoch no.4 train no.315840  loss = 2.57507 avg_loss = 3.08753\n",
      "epoch no.4 train no.315850  loss = 3.57134 avg_loss = 3.06055\n",
      "epoch no.4 train no.315860  loss = 3.21267 avg_loss = 3.07807\n",
      "epoch no.4 train no.315870  loss = 3.70933 avg_loss = 3.06210\n",
      "epoch no.4 train no.315880  loss = 3.76334 avg_loss = 3.10270\n",
      "epoch no.4 train no.315890  loss = 2.30782 avg_loss = 3.10214\n",
      "epoch no.4 train no.315900  loss = 3.30030 avg_loss = 3.10463\n",
      "epoch no.4 train no.315910  loss = 3.06275 avg_loss = 3.10739\n",
      "epoch no.4 train no.315920  loss = 3.26438 avg_loss = 3.10899\n",
      "epoch no.4 train no.315930  loss = 2.50181 avg_loss = 3.10214\n",
      "epoch no.4 train no.315940  loss = 4.18651 avg_loss = 3.11018\n",
      "epoch no.4 train no.315950  loss = 4.16805 avg_loss = 3.12322\n",
      "epoch no.4 train no.315960  loss = 1.91844 avg_loss = 3.07535\n",
      "epoch no.4 train no.315970  loss = 2.32241 avg_loss = 3.06985\n",
      "epoch no.4 train no.315980  loss = 4.68674 avg_loss = 3.13377\n",
      "epoch no.4 train no.315990  loss = 3.01630 avg_loss = 3.11989\n",
      "epoch no.4 train no.316000  loss = 3.29310 avg_loss = 3.12106\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '요', '▁70', 'est', '</s>']\n",
      "추억의 인기가요 best</s>\n",
      "epoch no.4 train no.316010  loss = 5.05343 avg_loss = 3.13758\n",
      "epoch no.4 train no.316020  loss = 3.70237 avg_loss = 3.13982\n",
      "epoch no.4 train no.316030  loss = 3.34854 avg_loss = 3.08471\n",
      "epoch no.4 train no.316040  loss = 5.43508 avg_loss = 3.11763\n",
      "epoch no.4 train no.316050  loss = 3.60397 avg_loss = 3.10543\n",
      "epoch no.4 train no.316060  loss = 3.13231 avg_loss = 3.09993\n",
      "epoch no.4 train no.316070  loss = 2.46269 avg_loss = 3.07550\n",
      "epoch no.4 train no.316080  loss = 4.35592 avg_loss = 3.05808\n",
      "epoch no.4 train no.316090  loss = 2.73177 avg_loss = 3.01538\n",
      "epoch no.4 train no.316100  loss = 2.52424 avg_loss = 3.05545\n",
      "epoch no.4 train no.316110  loss = 4.18044 avg_loss = 3.08215\n",
      "epoch no.4 train no.316120  loss = 3.09661 avg_loss = 3.05225\n",
      "epoch no.4 train no.316130  loss = 2.50559 avg_loss = 3.06139\n",
      "epoch no.4 train no.316140  loss = 4.51640 avg_loss = 3.10644\n",
      "epoch no.4 train no.316150  loss = 2.51984 avg_loss = 3.11502\n",
      "epoch no.4 train no.316160  loss = 3.15443 avg_loss = 3.07965\n",
      "epoch no.4 train no.316170  loss = 3.94857 avg_loss = 3.04717\n",
      "epoch no.4 train no.316180  loss = 3.45746 avg_loss = 3.04364\n",
      "epoch no.4 train no.316190  loss = 5.25599 avg_loss = 3.04358\n",
      "epoch no.4 train no.316200  loss = 4.11724 avg_loss = 3.07191\n",
      "epoch no.4 train no.316210  loss = 4.06911 avg_loss = 3.06438\n",
      "epoch no.4 train no.316220  loss = 2.77820 avg_loss = 3.09682\n",
      "epoch no.4 train no.316230  loss = 2.45230 avg_loss = 3.05012\n",
      "epoch no.4 train no.316240  loss = 4.19221 avg_loss = 3.05533\n",
      "epoch no.4 train no.316250  loss = 2.06979 avg_loss = 3.05725\n",
      "epoch no.4 train no.316260  loss = 3.85461 avg_loss = 3.06854\n",
      "epoch no.4 train no.316270  loss = 2.37385 avg_loss = 3.04668\n",
      "epoch no.4 train no.316280  loss = 3.60193 avg_loss = 3.09348\n",
      "epoch no.4 train no.316290  loss = 3.25576 avg_loss = 3.05730\n",
      "epoch no.4 train no.316300  loss = 2.70450 avg_loss = 3.07376\n",
      "epoch no.4 train no.316310  loss = 3.40643 avg_loss = 3.10051\n",
      "epoch no.4 train no.316320  loss = 4.10863 avg_loss = 3.11731\n",
      "epoch no.4 train no.316330  loss = 3.95553 avg_loss = 3.16071\n",
      "epoch no.4 train no.316340  loss = 3.21198 avg_loss = 3.11901\n",
      "epoch no.4 train no.316350  loss = 1.88273 avg_loss = 3.10438\n",
      "epoch no.4 train no.316360  loss = 3.38271 avg_loss = 3.08877\n",
      "epoch no.4 train no.316370  loss = 2.42578 avg_loss = 3.06796\n",
      "epoch no.4 train no.316380  loss = 2.46315 avg_loss = 3.08936\n",
      "epoch no.4 train no.316390  loss = 4.19003 avg_loss = 3.08124\n",
      "epoch no.4 train no.316400  loss = 2.56260 avg_loss = 3.08243\n",
      "epoch no.4 train no.316410  loss = 2.21926 avg_loss = 3.10752\n",
      "epoch no.4 train no.316420  loss = 4.96614 avg_loss = 3.10966\n",
      "epoch no.4 train no.316430  loss = 3.75418 avg_loss = 3.17279\n",
      "epoch no.4 train no.316440  loss = 2.87277 avg_loss = 3.14245\n",
      "epoch no.4 train no.316450  loss = 3.05758 avg_loss = 3.19776\n",
      "epoch no.4 train no.316460  loss = 5.35252 avg_loss = 3.28048\n",
      "epoch no.4 train no.316470  loss = 4.53881 avg_loss = 3.25448\n",
      "epoch no.4 train no.316480  loss = 2.73744 avg_loss = 3.27991\n",
      "epoch no.4 train no.316490  loss = 2.20424 avg_loss = 3.23877\n",
      "epoch no.4 train no.316500  loss = 3.37538 avg_loss = 3.22395\n",
      "epoch no.4 train no.316510  loss = 3.01040 avg_loss = 3.19894\n",
      "epoch no.4 train no.316520  loss = 2.75084 avg_loss = 3.21539\n",
      "epoch no.4 train no.316530  loss = 3.16810 avg_loss = 3.18594\n",
      "epoch no.4 train no.316540  loss = 2.45004 avg_loss = 3.17474\n",
      "epoch no.4 train no.316550  loss = 2.24930 avg_loss = 3.14158\n",
      "epoch no.4 train no.316560  loss = 3.83260 avg_loss = 3.15206\n",
      "epoch no.4 train no.316570  loss = 1.99866 avg_loss = 3.15862\n",
      "epoch no.4 train no.316580  loss = 1.70765 avg_loss = 3.15661\n",
      "epoch no.4 train no.316590  loss = 4.06367 avg_loss = 3.13437\n",
      "epoch no.4 train no.316600  loss = 4.22673 avg_loss = 3.10430\n",
      "epoch no.4 train no.316610  loss = 2.96362 avg_loss = 3.13221\n",
      "epoch no.4 train no.316620  loss = 1.82257 avg_loss = 3.11110\n",
      "epoch no.4 train no.316630  loss = 3.17420 avg_loss = 3.09814\n",
      "epoch no.4 train no.316640  loss = 2.39663 avg_loss = 3.09639\n",
      "epoch no.4 train no.316650  loss = 2.57375 avg_loss = 3.08545\n",
      "epoch no.4 train no.316660  loss = 4.85705 avg_loss = 3.10798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.316670  loss = 2.97168 avg_loss = 3.12671\n",
      "epoch no.4 train no.316680  loss = 3.30330 avg_loss = 3.13128\n",
      "epoch no.4 train no.316690  loss = 1.14881 avg_loss = 3.10054\n",
      "epoch no.4 train no.316700  loss = 4.52612 avg_loss = 3.11340\n",
      "epoch no.4 train no.316710  loss = 3.75653 avg_loss = 3.12360\n",
      "epoch no.4 train no.316720  loss = 2.58542 avg_loss = 3.13272\n",
      "epoch no.4 train no.316730  loss = 2.39520 avg_loss = 3.14506\n",
      "epoch no.4 train no.316740  loss = 3.97535 avg_loss = 3.09587\n",
      "epoch no.4 train no.316750  loss = 3.98958 avg_loss = 3.10958\n",
      "epoch no.4 train no.316760  loss = 3.87941 avg_loss = 3.08841\n",
      "epoch no.4 train no.316770  loss = 0.99650 avg_loss = 3.10085\n",
      "epoch no.4 train no.316780  loss = 2.99110 avg_loss = 3.11609\n",
      "epoch no.4 train no.316790  loss = 2.84587 avg_loss = 3.12039\n",
      "epoch no.4 train no.316800  loss = 2.88448 avg_loss = 3.12206\n",
      "epoch no.4 train no.316810  loss = 1.94645 avg_loss = 3.08591\n",
      "epoch no.4 train no.316820  loss = 4.67871 avg_loss = 3.09739\n",
      "epoch no.4 train no.316830  loss = 2.59650 avg_loss = 3.12773\n",
      "epoch no.4 train no.316840  loss = 2.56359 avg_loss = 3.13870\n",
      "epoch no.4 train no.316850  loss = 4.70748 avg_loss = 3.14962\n",
      "epoch no.4 train no.316860  loss = 2.13940 avg_loss = 3.09373\n",
      "epoch no.4 train no.316870  loss = 3.06134 avg_loss = 3.07594\n",
      "epoch no.4 train no.316880  loss = 5.56989 avg_loss = 3.11827\n",
      "epoch no.4 train no.316890  loss = 3.59176 avg_loss = 3.10317\n",
      "epoch no.4 train no.316900  loss = 2.09785 avg_loss = 3.08290\n",
      "epoch no.4 train no.316910  loss = 3.41371 avg_loss = 3.07842\n",
      "epoch no.4 train no.316920  loss = 1.74341 avg_loss = 3.10539\n",
      "epoch no.4 train no.316930  loss = 2.68725 avg_loss = 3.12307\n",
      "epoch no.4 train no.316940  loss = 2.48431 avg_loss = 3.14716\n",
      "epoch no.4 train no.316950  loss = 3.65289 avg_loss = 3.17072\n",
      "epoch no.4 train no.316960  loss = 3.81116 avg_loss = 3.19544\n",
      "epoch no.4 train no.316970  loss = 3.64624 avg_loss = 3.17433\n",
      "epoch no.4 train no.316980  loss = 2.85775 avg_loss = 3.15029\n",
      "epoch no.4 train no.316990  loss = 1.99401 avg_loss = 3.17239\n",
      "epoch no.4 train no.317000  loss = 2.33218 avg_loss = 3.14506\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '▁o', 'st', '▁모음', '음', '</s>']\n",
      "추억의 드라마 ost모음</s>\n",
      "epoch no.4 train no.317010  loss = 2.74187 avg_loss = 3.13935\n",
      "epoch no.4 train no.317020  loss = 3.19325 avg_loss = 3.14783\n",
      "epoch no.4 train no.317030  loss = 2.91957 avg_loss = 3.14478\n",
      "epoch no.4 train no.317040  loss = 4.34248 avg_loss = 3.15448\n",
      "epoch no.4 train no.317050  loss = 5.32221 avg_loss = 3.20118\n",
      "epoch no.4 train no.317060  loss = 3.12669 avg_loss = 3.22572\n",
      "epoch no.4 train no.317070  loss = 1.58997 avg_loss = 3.25574\n",
      "epoch no.4 train no.317080  loss = 2.68124 avg_loss = 3.18516\n",
      "epoch no.4 train no.317090  loss = 2.02800 avg_loss = 3.15236\n",
      "epoch no.4 train no.317100  loss = 2.24078 avg_loss = 3.12780\n",
      "epoch no.4 train no.317110  loss = 2.80303 avg_loss = 3.14250\n",
      "epoch no.4 train no.317120  loss = 3.40610 avg_loss = 3.11033\n",
      "epoch no.4 train no.317130  loss = 2.04400 avg_loss = 3.15980\n",
      "epoch no.4 train no.317140  loss = 1.91201 avg_loss = 3.14987\n",
      "epoch no.4 train no.317150  loss = 2.61372 avg_loss = 3.14251\n",
      "epoch no.4 train no.317160  loss = 2.77670 avg_loss = 3.13813\n",
      "epoch no.4 train no.317170  loss = 3.82034 avg_loss = 3.16785\n",
      "epoch no.4 train no.317180  loss = 4.12249 avg_loss = 3.14548\n",
      "epoch no.4 train no.317190  loss = 3.65398 avg_loss = 3.18241\n",
      "epoch no.4 train no.317200  loss = 3.26920 avg_loss = 3.15822\n",
      "epoch no.4 train no.317210  loss = 3.28053 avg_loss = 3.13096\n",
      "epoch no.4 train no.317220  loss = 2.69241 avg_loss = 3.07412\n",
      "epoch no.4 train no.317230  loss = 2.08191 avg_loss = 3.04398\n",
      "epoch no.4 train no.317240  loss = 3.51516 avg_loss = 3.04983\n",
      "epoch no.4 train no.317250  loss = 3.24431 avg_loss = 3.08077\n",
      "epoch no.4 train no.317260  loss = 2.64751 avg_loss = 3.09418\n",
      "epoch no.4 train no.317270  loss = 3.97764 avg_loss = 3.11227\n",
      "epoch no.4 train no.317280  loss = 1.63311 avg_loss = 3.09488\n",
      "epoch no.4 train no.317290  loss = 3.23311 avg_loss = 3.09711\n",
      "epoch no.4 train no.317300  loss = 3.13418 avg_loss = 3.12679\n",
      "epoch no.4 train no.317310  loss = 3.55276 avg_loss = 3.13637\n",
      "epoch no.4 train no.317320  loss = 2.84046 avg_loss = 3.15030\n",
      "epoch no.4 train no.317330  loss = 4.81435 avg_loss = 3.17215\n",
      "epoch no.4 train no.317340  loss = 4.51192 avg_loss = 3.18347\n",
      "epoch no.4 train no.317350  loss = 2.24861 avg_loss = 3.14620\n",
      "epoch no.4 train no.317360  loss = 4.86145 avg_loss = 3.15992\n",
      "epoch no.4 train no.317370  loss = 3.08641 avg_loss = 3.16683\n",
      "epoch no.4 train no.317380  loss = 2.32346 avg_loss = 3.13264\n",
      "epoch no.4 train no.317390  loss = 2.85874 avg_loss = 3.17116\n",
      "epoch no.4 train no.317400  loss = 2.31024 avg_loss = 3.20315\n",
      "epoch no.4 train no.317410  loss = 3.25113 avg_loss = 3.21177\n",
      "epoch no.4 train no.317420  loss = 4.12287 avg_loss = 3.23760\n",
      "epoch no.4 train no.317430  loss = 2.72290 avg_loss = 3.23766\n",
      "epoch no.4 train no.317440  loss = 3.67911 avg_loss = 3.22606\n",
      "epoch no.4 train no.317450  loss = 2.23471 avg_loss = 3.21432\n",
      "epoch no.4 train no.317460  loss = 4.34035 avg_loss = 3.19679\n",
      "epoch no.4 train no.317470  loss = 3.53394 avg_loss = 3.21841\n",
      "epoch no.4 train no.317480  loss = 1.30128 avg_loss = 3.15355\n",
      "epoch no.4 train no.317490  loss = 2.38880 avg_loss = 3.15899\n",
      "epoch no.4 train no.317500  loss = 3.21522 avg_loss = 3.15700\n",
      "epoch no.4 train no.317510  loss = 2.05670 avg_loss = 3.16276\n",
      "epoch no.4 train no.317520  loss = 2.24838 avg_loss = 3.14669\n",
      "epoch no.4 train no.317530  loss = 3.05362 avg_loss = 3.10654\n",
      "epoch no.4 train no.317540  loss = 2.44891 avg_loss = 3.08211\n",
      "epoch no.4 train no.317550  loss = 4.25549 avg_loss = 3.08942\n",
      "epoch no.4 train no.317560  loss = 2.73307 avg_loss = 3.03258\n",
      "epoch no.4 train no.317570  loss = 3.74760 avg_loss = 3.03563\n",
      "epoch no.4 train no.317580  loss = 4.79823 avg_loss = 3.03887\n",
      "epoch no.4 train no.317590  loss = 2.78434 avg_loss = 3.08489\n",
      "epoch no.4 train no.317600  loss = 2.71890 avg_loss = 3.09185\n",
      "epoch no.4 train no.317610  loss = 2.24051 avg_loss = 3.05115\n",
      "epoch no.4 train no.317620  loss = 3.01908 avg_loss = 3.07047\n",
      "epoch no.4 train no.317630  loss = 4.93322 avg_loss = 3.10899\n",
      "epoch no.4 train no.317640  loss = 3.88355 avg_loss = 3.12689\n",
      "epoch no.4 train no.317650  loss = 2.84360 avg_loss = 3.10811\n",
      "epoch no.4 train no.317660  loss = 4.31385 avg_loss = 3.14096\n",
      "epoch no.4 train no.317670  loss = 3.68506 avg_loss = 3.13836\n",
      "epoch no.4 train no.317680  loss = 2.99680 avg_loss = 3.15244\n",
      "epoch no.4 train no.317690  loss = 2.22829 avg_loss = 3.13633\n",
      "epoch no.4 train no.317700  loss = 2.32689 avg_loss = 3.11186\n",
      "epoch no.4 train no.317710  loss = 2.79816 avg_loss = 3.14590\n",
      "epoch no.4 train no.317720  loss = 5.21890 avg_loss = 3.19693\n",
      "epoch no.4 train no.317730  loss = 2.71070 avg_loss = 3.18157\n",
      "epoch no.4 train no.317740  loss = 1.94189 avg_loss = 3.16634\n",
      "epoch no.4 train no.317750  loss = 3.83586 avg_loss = 3.15515\n",
      "epoch no.4 train no.317760  loss = 3.15729 avg_loss = 3.09007\n",
      "epoch no.4 train no.317770  loss = 2.23176 avg_loss = 3.09767\n",
      "epoch no.4 train no.317780  loss = 1.88984 avg_loss = 3.07624\n",
      "epoch no.4 train no.317790  loss = 2.59847 avg_loss = 3.09396\n",
      "epoch no.4 train no.317800  loss = 3.75350 avg_loss = 3.04832\n",
      "epoch no.4 train no.317810  loss = 2.23288 avg_loss = 3.05641\n",
      "epoch no.4 train no.317820  loss = 4.79141 avg_loss = 3.08069\n",
      "epoch no.4 train no.317830  loss = 2.89919 avg_loss = 3.07061\n",
      "epoch no.4 train no.317840  loss = 3.26474 avg_loss = 3.09498\n",
      "epoch no.4 train no.317850  loss = 2.93966 avg_loss = 3.13861\n",
      "epoch no.4 train no.317860  loss = 3.66857 avg_loss = 3.13113\n",
      "epoch no.4 train no.317870  loss = 2.27483 avg_loss = 3.10126\n",
      "epoch no.4 train no.317880  loss = 3.55330 avg_loss = 3.13059\n",
      "epoch no.4 train no.317890  loss = 3.20763 avg_loss = 3.12643\n",
      "epoch no.4 train no.317900  loss = 0.97149 avg_loss = 3.06277\n",
      "epoch no.4 train no.317910  loss = 2.24762 avg_loss = 3.03221\n",
      "epoch no.4 train no.317920  loss = 2.70481 avg_loss = 3.06322\n",
      "epoch no.4 train no.317930  loss = 3.70780 avg_loss = 3.05391\n",
      "epoch no.4 train no.317940  loss = 3.44396 avg_loss = 3.06848\n",
      "epoch no.4 train no.317950  loss = 2.49656 avg_loss = 3.09771\n",
      "epoch no.4 train no.317960  loss = 2.53489 avg_loss = 3.09536\n",
      "epoch no.4 train no.317970  loss = 3.15834 avg_loss = 3.11465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.317980  loss = 4.40590 avg_loss = 3.16473\n",
      "epoch no.4 train no.317990  loss = 2.50485 avg_loss = 3.15119\n",
      "epoch no.4 train no.318000  loss = 2.86687 avg_loss = 3.13500\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁시절', '이', '</s>', '송']\n",
      "추억의 그 시절 감성 팝</s>\n",
      "epoch no.4 train no.318010  loss = 2.52053 avg_loss = 3.10690\n",
      "epoch no.4 train no.318020  loss = 2.99245 avg_loss = 3.07543\n",
      "epoch no.4 train no.318030  loss = 2.91272 avg_loss = 3.07398\n",
      "epoch no.4 train no.318040  loss = 2.46355 avg_loss = 3.08861\n",
      "epoch no.4 train no.318050  loss = 2.65319 avg_loss = 3.12871\n",
      "epoch no.4 train no.318060  loss = 2.55433 avg_loss = 3.13950\n",
      "epoch no.4 train no.318070  loss = 3.85492 avg_loss = 3.16171\n",
      "epoch no.4 train no.318080  loss = 2.73444 avg_loss = 3.13639\n",
      "epoch no.4 train no.318090  loss = 2.84589 avg_loss = 3.13333\n",
      "epoch no.4 train no.318100  loss = 2.03578 avg_loss = 3.09900\n",
      "epoch no.4 train no.318110  loss = 3.15809 avg_loss = 3.08455\n",
      "epoch no.4 train no.318120  loss = 3.00057 avg_loss = 3.08964\n",
      "epoch no.4 train no.318130  loss = 4.40718 avg_loss = 3.12623\n",
      "epoch no.4 train no.318140  loss = 2.37531 avg_loss = 3.10423\n",
      "epoch no.4 train no.318150  loss = 2.13744 avg_loss = 3.07052\n",
      "epoch no.4 train no.318160  loss = 4.09501 avg_loss = 3.10526\n",
      "epoch no.4 train no.318170  loss = 2.94927 avg_loss = 3.14252\n",
      "epoch no.4 train no.318180  loss = 1.96579 avg_loss = 3.15017\n",
      "epoch no.4 train no.318190  loss = 1.63966 avg_loss = 3.08779\n",
      "epoch no.4 train no.318200  loss = 4.57471 avg_loss = 3.14519\n",
      "epoch no.4 train no.318210  loss = 3.96025 avg_loss = 3.14099\n",
      "epoch no.4 train no.318220  loss = 3.19562 avg_loss = 3.10003\n",
      "epoch no.4 train no.318230  loss = 1.57367 avg_loss = 3.08093\n",
      "epoch no.4 train no.318240  loss = 3.89389 avg_loss = 3.11226\n",
      "epoch no.4 train no.318250  loss = 1.39253 avg_loss = 3.10234\n",
      "epoch no.4 train no.318260  loss = 2.83574 avg_loss = 3.08975\n",
      "epoch no.4 train no.318270  loss = 2.85263 avg_loss = 3.09815\n",
      "epoch no.4 train no.318280  loss = 3.18523 avg_loss = 3.10442\n",
      "epoch no.4 train no.318290  loss = 2.46623 avg_loss = 3.15701\n",
      "epoch no.4 train no.318300  loss = 3.43954 avg_loss = 3.13091\n",
      "epoch no.4 train no.318310  loss = 1.41649 avg_loss = 3.18323\n",
      "epoch no.4 train no.318320  loss = 2.63600 avg_loss = 3.22823\n",
      "epoch no.4 train no.318330  loss = 1.98049 avg_loss = 3.21333\n",
      "epoch no.4 train no.318340  loss = 2.32209 avg_loss = 3.17103\n",
      "epoch no.4 train no.318350  loss = 3.02404 avg_loss = 3.17140\n",
      "epoch no.4 train no.318360  loss = 2.56711 avg_loss = 3.10538\n",
      "epoch no.4 train no.318370  loss = 4.31463 avg_loss = 3.10183\n",
      "epoch no.4 train no.318380  loss = 3.52860 avg_loss = 3.11626\n",
      "epoch no.4 train no.318390  loss = 3.80167 avg_loss = 3.11784\n",
      "epoch no.4 train no.318400  loss = 3.89113 avg_loss = 3.10369\n",
      "epoch no.4 train no.318410  loss = 3.24775 avg_loss = 3.12750\n",
      "epoch no.4 train no.318420  loss = 3.06401 avg_loss = 3.12971\n",
      "epoch no.4 train no.318430  loss = 3.61517 avg_loss = 3.10123\n",
      "epoch no.4 train no.318440  loss = 2.80686 avg_loss = 3.15277\n",
      "epoch no.4 train no.318450  loss = 2.59332 avg_loss = 3.12898\n",
      "epoch no.4 train no.318460  loss = 3.35068 avg_loss = 3.11925\n",
      "epoch no.4 train no.318470  loss = 2.72050 avg_loss = 3.13417\n",
      "epoch no.4 train no.318480  loss = 4.42143 avg_loss = 3.09215\n",
      "epoch no.4 train no.318490  loss = 4.34742 avg_loss = 3.11592\n",
      "epoch no.4 train no.318500  loss = 3.67652 avg_loss = 3.11552\n",
      "epoch no.4 train no.318510  loss = 3.07605 avg_loss = 3.12735\n",
      "epoch no.4 train no.318520  loss = 2.71699 avg_loss = 3.12292\n",
      "epoch no.4 train no.318530  loss = 2.35847 avg_loss = 3.11199\n",
      "epoch no.4 train no.318540  loss = 2.36815 avg_loss = 3.08974\n",
      "epoch no.4 train no.318550  loss = 3.35799 avg_loss = 3.12520\n",
      "epoch no.4 train no.318560  loss = 3.02590 avg_loss = 3.12700\n",
      "epoch no.4 train no.318570  loss = 4.72185 avg_loss = 3.13431\n",
      "epoch no.4 train no.318580  loss = 2.08793 avg_loss = 3.09169\n",
      "epoch no.4 train no.318590  loss = 1.78951 avg_loss = 3.04067\n",
      "epoch no.4 train no.318600  loss = 3.12177 avg_loss = 3.07364\n",
      "epoch no.4 train no.318610  loss = 2.27267 avg_loss = 3.06063\n",
      "epoch no.4 train no.318620  loss = 4.04328 avg_loss = 3.07556\n",
      "epoch no.4 train no.318630  loss = 4.33349 avg_loss = 3.09741\n",
      "epoch no.4 train no.318640  loss = 5.43583 avg_loss = 3.15024\n",
      "epoch no.4 train no.318650  loss = 3.24864 avg_loss = 3.16678\n",
      "epoch no.4 train no.318660  loss = 4.21206 avg_loss = 3.20162\n",
      "epoch no.4 train no.318670  loss = 2.80889 avg_loss = 3.21679\n",
      "epoch no.4 train no.318680  loss = 5.27359 avg_loss = 3.23691\n",
      "epoch no.4 train no.318690  loss = 3.06144 avg_loss = 3.27325\n",
      "epoch no.4 train no.318700  loss = 3.97209 avg_loss = 3.27477\n",
      "epoch no.4 train no.318710  loss = 3.92554 avg_loss = 3.25276\n",
      "epoch no.4 train no.318720  loss = 2.14334 avg_loss = 3.23758\n",
      "epoch no.4 train no.318730  loss = 4.72871 avg_loss = 3.22226\n",
      "epoch no.4 train no.318740  loss = 2.39265 avg_loss = 3.21200\n",
      "epoch no.4 train no.318750  loss = 1.99808 avg_loss = 3.19615\n",
      "epoch no.4 train no.318760  loss = 3.33147 avg_loss = 3.20308\n",
      "epoch no.4 train no.318770  loss = 1.90360 avg_loss = 3.16273\n",
      "epoch no.4 train no.318780  loss = 2.55424 avg_loss = 3.14200\n",
      "epoch no.4 train no.318790  loss = 3.48972 avg_loss = 3.13034\n",
      "epoch no.4 train no.318800  loss = 3.60674 avg_loss = 3.11219\n",
      "epoch no.4 train no.318810  loss = 2.06585 avg_loss = 3.10821\n",
      "epoch no.4 train no.318820  loss = 3.11828 avg_loss = 3.12335\n",
      "epoch no.4 train no.318830  loss = 2.38382 avg_loss = 3.08527\n",
      "epoch no.4 train no.318840  loss = 2.53955 avg_loss = 3.10660\n",
      "epoch no.4 train no.318850  loss = 2.31114 avg_loss = 3.08397\n",
      "epoch no.4 train no.318860  loss = 5.34146 avg_loss = 3.13071\n",
      "epoch no.4 train no.318870  loss = 2.86587 avg_loss = 3.20323\n",
      "epoch no.4 train no.318880  loss = 2.85943 avg_loss = 3.13637\n",
      "epoch no.4 train no.318890  loss = 4.31994 avg_loss = 3.14602\n",
      "epoch no.4 train no.318900  loss = 1.91101 avg_loss = 3.13297\n",
      "epoch no.4 train no.318910  loss = 2.05411 avg_loss = 3.11938\n",
      "epoch no.4 train no.318920  loss = 3.64361 avg_loss = 3.15027\n",
      "epoch no.4 train no.318930  loss = 3.39730 avg_loss = 3.17054\n",
      "epoch no.4 train no.318940  loss = 2.98760 avg_loss = 3.15386\n",
      "epoch no.4 train no.318950  loss = 2.77897 avg_loss = 3.14581\n",
      "epoch no.4 train no.318960  loss = 2.43579 avg_loss = 3.13743\n",
      "epoch no.4 train no.318970  loss = 2.28217 avg_loss = 3.13245\n",
      "epoch no.4 train no.318980  loss = 3.73608 avg_loss = 3.15444\n",
      "epoch no.4 train no.318990  loss = 2.21172 avg_loss = 3.15590\n",
      "epoch no.4 train no.319000  loss = 3.05955 avg_loss = 3.14087\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁노래', '들', '</s>']\n",
      "추억의 90년대 노래들</s>\n",
      "epoch no.4 train no.319010  loss = 2.81016 avg_loss = 3.13032\n",
      "epoch no.4 train no.319020  loss = 3.17698 avg_loss = 3.11370\n",
      "epoch no.4 train no.319030  loss = 2.94974 avg_loss = 3.09334\n",
      "epoch no.4 train no.319040  loss = 4.45707 avg_loss = 3.11233\n",
      "epoch no.4 train no.319050  loss = 3.81994 avg_loss = 3.12658\n",
      "epoch no.4 train no.319060  loss = 3.71558 avg_loss = 3.13785\n",
      "epoch no.4 train no.319070  loss = 2.09693 avg_loss = 3.13261\n",
      "epoch no.4 train no.319080  loss = 2.84242 avg_loss = 3.08910\n",
      "epoch no.4 train no.319090  loss = 2.38087 avg_loss = 3.11066\n",
      "epoch no.4 train no.319100  loss = 2.30506 avg_loss = 3.10436\n",
      "epoch no.4 train no.319110  loss = 3.88608 avg_loss = 3.09941\n",
      "epoch no.4 train no.319120  loss = 2.38123 avg_loss = 3.12204\n",
      "epoch no.4 train no.319130  loss = 3.13607 avg_loss = 3.11931\n",
      "epoch no.4 train no.319140  loss = 4.34695 avg_loss = 3.11284\n",
      "epoch no.4 train no.319150  loss = 2.44310 avg_loss = 3.07720\n",
      "epoch no.4 train no.319160  loss = 2.83361 avg_loss = 3.10583\n",
      "epoch no.4 train no.319170  loss = 3.02848 avg_loss = 3.13081\n",
      "epoch no.4 train no.319180  loss = 2.11804 avg_loss = 3.11107\n",
      "epoch no.4 train no.319190  loss = 3.85980 avg_loss = 3.13024\n",
      "epoch no.4 train no.319200  loss = 3.06546 avg_loss = 3.14147\n",
      "epoch no.4 train no.319210  loss = 3.33211 avg_loss = 3.17371\n",
      "epoch no.4 train no.319220  loss = 2.72809 avg_loss = 3.16064\n",
      "epoch no.4 train no.319230  loss = 2.10329 avg_loss = 3.13668\n",
      "epoch no.4 train no.319240  loss = 2.28445 avg_loss = 3.10257\n",
      "epoch no.4 train no.319250  loss = 3.28668 avg_loss = 3.06606\n",
      "epoch no.4 train no.319260  loss = 4.46236 avg_loss = 3.13533\n",
      "epoch no.4 train no.319270  loss = 2.83729 avg_loss = 3.15952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.319280  loss = 2.07492 avg_loss = 3.17038\n",
      "epoch no.4 train no.319290  loss = 2.27076 avg_loss = 3.18032\n",
      "epoch no.4 train no.319300  loss = 2.68113 avg_loss = 3.14017\n",
      "epoch no.4 train no.319310  loss = 4.39651 avg_loss = 3.15445\n",
      "epoch no.4 train no.319320  loss = 3.11015 avg_loss = 3.15168\n",
      "epoch no.4 train no.319330  loss = 2.94203 avg_loss = 3.16014\n",
      "epoch no.4 train no.319340  loss = 4.05550 avg_loss = 3.14747\n",
      "epoch no.4 train no.319350  loss = 3.34560 avg_loss = 3.17861\n",
      "epoch no.4 train no.319360  loss = 2.87971 avg_loss = 3.13625\n",
      "epoch no.4 train no.319370  loss = 3.76757 avg_loss = 3.13004\n",
      "epoch no.4 train no.319380  loss = 2.99492 avg_loss = 3.12194\n",
      "epoch no.4 train no.319390  loss = 2.70200 avg_loss = 3.08571\n",
      "epoch no.4 train no.319400  loss = 3.83512 avg_loss = 3.12381\n",
      "epoch no.4 train no.319410  loss = 2.30504 avg_loss = 3.08935\n",
      "epoch no.4 train no.319420  loss = 3.25147 avg_loss = 3.13534\n",
      "epoch no.4 train no.319430  loss = 2.77688 avg_loss = 3.16226\n",
      "epoch no.4 train no.319440  loss = 2.55609 avg_loss = 3.16364\n",
      "epoch no.4 train no.319450  loss = 3.84404 avg_loss = 3.19319\n",
      "epoch no.4 train no.319460  loss = 2.37408 avg_loss = 3.17384\n",
      "epoch no.4 train no.319470  loss = 3.63641 avg_loss = 3.16893\n",
      "epoch no.4 train no.319480  loss = 4.14605 avg_loss = 3.17308\n",
      "epoch no.4 train no.319490  loss = 2.16754 avg_loss = 3.19176\n",
      "epoch no.4 train no.319500  loss = 3.75190 avg_loss = 3.16054\n",
      "epoch no.4 train no.319510  loss = 2.49352 avg_loss = 3.15247\n",
      "epoch no.4 train no.319520  loss = 4.49278 avg_loss = 3.18851\n",
      "epoch no.4 train no.319530  loss = 2.70195 avg_loss = 3.19918\n",
      "epoch no.4 train no.319540  loss = 2.48861 avg_loss = 3.19196\n",
      "epoch no.4 train no.319550  loss = 2.48954 avg_loss = 3.21511\n",
      "epoch no.4 train no.319560  loss = 4.19256 avg_loss = 3.24505\n",
      "epoch no.4 train no.319570  loss = 2.58678 avg_loss = 3.20868\n",
      "epoch no.4 train no.319580  loss = 3.60745 avg_loss = 3.22741\n",
      "epoch no.4 train no.319590  loss = 4.26903 avg_loss = 3.27124\n",
      "epoch no.4 train no.319600  loss = 2.97205 avg_loss = 3.22800\n",
      "epoch no.4 train no.319610  loss = 2.87323 avg_loss = 3.21046\n",
      "epoch no.4 train no.319620  loss = 3.30262 avg_loss = 3.20148\n",
      "epoch no.4 train no.319630  loss = 2.94979 avg_loss = 3.17857\n",
      "epoch no.4 train no.319640  loss = 4.49138 avg_loss = 3.20939\n",
      "epoch no.4 train no.319650  loss = 2.70113 avg_loss = 3.20924\n",
      "epoch no.4 train no.319660  loss = 3.76259 avg_loss = 3.18042\n",
      "epoch no.4 train no.319670  loss = 5.02302 avg_loss = 3.19888\n",
      "epoch no.4 train no.319680  loss = 2.96667 avg_loss = 3.21984\n",
      "epoch no.4 train no.319690  loss = 3.60063 avg_loss = 3.24373\n",
      "epoch no.4 train no.319700  loss = 2.50165 avg_loss = 3.21559\n",
      "epoch no.4 train no.319710  loss = 1.70926 avg_loss = 3.20628\n",
      "epoch no.4 train no.319720  loss = 2.21775 avg_loss = 3.22625\n",
      "epoch no.4 train no.319730  loss = 2.24277 avg_loss = 3.19195\n",
      "epoch no.4 train no.319740  loss = 2.07589 avg_loss = 3.18636\n",
      "epoch no.4 train no.319750  loss = 4.34030 avg_loss = 3.20567\n",
      "epoch no.4 train no.319760  loss = 3.72150 avg_loss = 3.18102\n",
      "epoch no.4 train no.319770  loss = 3.26714 avg_loss = 3.19185\n",
      "epoch no.4 train no.319780  loss = 3.12706 avg_loss = 3.18355\n",
      "epoch no.4 train no.319790  loss = 2.29312 avg_loss = 3.18677\n",
      "epoch no.4 train no.319800  loss = 3.30651 avg_loss = 3.19153\n",
      "epoch no.4 train no.319810  loss = 2.21875 avg_loss = 3.13006\n",
      "epoch no.4 train no.319820  loss = 3.13548 avg_loss = 3.15897\n",
      "epoch no.4 train no.319830  loss = 3.49613 avg_loss = 3.15269\n",
      "epoch no.4 train no.319840  loss = 3.40298 avg_loss = 3.15554\n",
      "epoch no.4 train no.319850  loss = 2.71287 avg_loss = 3.16977\n",
      "epoch no.4 train no.319860  loss = 4.73365 avg_loss = 3.19936\n",
      "epoch no.4 train no.319870  loss = 5.06097 avg_loss = 3.20882\n",
      "epoch no.4 train no.319880  loss = 4.38185 avg_loss = 3.18784\n",
      "epoch no.4 train no.319890  loss = 2.32371 avg_loss = 3.18301\n",
      "epoch no.4 train no.319900  loss = 3.01122 avg_loss = 3.16390\n",
      "epoch no.4 train no.319910  loss = 2.92151 avg_loss = 3.15677\n",
      "epoch no.4 train no.319920  loss = 2.18586 avg_loss = 3.14621\n",
      "epoch no.4 train no.319930  loss = 2.37965 avg_loss = 3.12860\n",
      "epoch no.4 train no.319940  loss = 3.38383 avg_loss = 3.12496\n",
      "epoch no.4 train no.319950  loss = 2.62907 avg_loss = 3.14108\n",
      "epoch no.4 train no.319960  loss = 2.98321 avg_loss = 3.17259\n",
      "epoch no.4 train no.319970  loss = 3.56391 avg_loss = 3.17475\n",
      "epoch no.4 train no.319980  loss = 4.20452 avg_loss = 3.16673\n",
      "epoch no.4 train no.319990  loss = 2.80158 avg_loss = 3.13975\n",
      "epoch no.4 train no.320000  loss = 2.92072 avg_loss = 3.13188\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '곡', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.4 train no.320010  loss = 2.04988 avg_loss = 3.08165\n",
      "epoch no.4 train no.320020  loss = 2.35745 avg_loss = 3.08276\n",
      "epoch no.4 train no.320030  loss = 2.69119 avg_loss = 3.06537\n",
      "epoch no.4 train no.320040  loss = 2.96350 avg_loss = 3.10303\n",
      "epoch no.4 train no.320050  loss = 3.57804 avg_loss = 3.12787\n",
      "epoch no.4 train no.320060  loss = 2.47221 avg_loss = 3.14526\n",
      "epoch no.4 train no.320070  loss = 3.45573 avg_loss = 3.13817\n",
      "epoch no.4 train no.320080  loss = 2.04732 avg_loss = 3.12193\n",
      "epoch no.4 train no.320090  loss = 3.33526 avg_loss = 3.14189\n",
      "epoch no.4 train no.320100  loss = 2.21401 avg_loss = 3.20127\n",
      "epoch no.4 train no.320110  loss = 4.27502 avg_loss = 3.23767\n",
      "epoch no.4 train no.320120  loss = 3.69710 avg_loss = 3.22076\n",
      "epoch no.4 train no.320130  loss = 4.17794 avg_loss = 3.22482\n",
      "epoch no.4 train no.320140  loss = 2.03296 avg_loss = 3.19013\n",
      "epoch no.4 train no.320150  loss = 2.42172 avg_loss = 3.17005\n",
      "epoch no.4 train no.320160  loss = 3.27082 avg_loss = 3.15236\n",
      "epoch no.4 train no.320170  loss = 4.78553 avg_loss = 3.19869\n",
      "epoch no.4 train no.320180  loss = 3.58387 avg_loss = 3.22839\n",
      "epoch no.4 train no.320190  loss = 3.29454 avg_loss = 3.24326\n",
      "epoch no.4 train no.320200  loss = 4.09950 avg_loss = 3.23659\n",
      "epoch no.4 train no.320210  loss = 1.14283 avg_loss = 3.20057\n",
      "epoch no.4 train no.320220  loss = 2.73679 avg_loss = 3.19031\n",
      "epoch no.4 train no.320230  loss = 2.92500 avg_loss = 3.20615\n",
      "epoch no.4 train no.320240  loss = 3.97797 avg_loss = 3.22108\n",
      "epoch no.4 train no.320250  loss = 2.10963 avg_loss = 3.18426\n",
      "epoch no.4 train no.320260  loss = 3.86442 avg_loss = 3.18365\n",
      "epoch no.4 train no.320270  loss = 2.24787 avg_loss = 3.19386\n",
      "epoch no.4 train no.320280  loss = 2.75918 avg_loss = 3.18481\n",
      "epoch no.4 train no.320290  loss = 2.26555 avg_loss = 3.16931\n",
      "epoch no.4 train no.320300  loss = 1.93487 avg_loss = 3.17905\n",
      "epoch no.4 train no.320310  loss = 5.66437 avg_loss = 3.20602\n",
      "epoch no.4 train no.320320  loss = 2.38139 avg_loss = 3.18041\n",
      "epoch no.4 train no.320330  loss = 2.92274 avg_loss = 3.19942\n",
      "epoch no.4 train no.320340  loss = 2.19594 avg_loss = 3.21870\n",
      "epoch no.4 train no.320350  loss = 3.85860 avg_loss = 3.20950\n",
      "epoch no.4 train no.320360  loss = 3.04925 avg_loss = 3.19545\n",
      "epoch no.4 train no.320370  loss = 2.73438 avg_loss = 3.17477\n",
      "epoch no.4 train no.320380  loss = 3.62263 avg_loss = 3.22882\n",
      "epoch no.4 train no.320390  loss = 2.37207 avg_loss = 3.21184\n",
      "epoch no.4 train no.320400  loss = 4.06472 avg_loss = 3.23324\n",
      "epoch no.4 train no.320410  loss = 3.25602 avg_loss = 3.22384\n",
      "epoch no.4 train no.320420  loss = 2.92976 avg_loss = 3.22368\n",
      "epoch no.4 train no.320430  loss = 2.56474 avg_loss = 3.21288\n",
      "epoch no.4 train no.320440  loss = 2.56944 avg_loss = 3.19927\n",
      "epoch no.4 train no.320450  loss = 3.41194 avg_loss = 3.17058\n",
      "epoch no.4 train no.320460  loss = 2.34480 avg_loss = 3.12958\n",
      "epoch no.4 train no.320470  loss = 2.78045 avg_loss = 3.18446\n",
      "epoch no.4 train no.320480  loss = 2.57203 avg_loss = 3.15448\n",
      "epoch no.4 train no.320490  loss = 2.41093 avg_loss = 3.13573\n",
      "epoch no.4 train no.320500  loss = 2.04189 avg_loss = 3.07377\n",
      "epoch no.4 train no.320510  loss = 3.09331 avg_loss = 3.05293\n",
      "epoch no.4 train no.320520  loss = 3.39137 avg_loss = 3.05866\n",
      "epoch no.4 train no.320530  loss = 3.47237 avg_loss = 3.06380\n",
      "epoch no.4 train no.320540  loss = 3.00493 avg_loss = 3.03934\n",
      "epoch no.4 train no.320550  loss = 4.59790 avg_loss = 3.09501\n",
      "epoch no.4 train no.320560  loss = 2.57982 avg_loss = 3.11611\n",
      "epoch no.4 train no.320570  loss = 2.99721 avg_loss = 3.12606\n",
      "epoch no.4 train no.320580  loss = 3.16668 avg_loss = 3.07922\n",
      "epoch no.4 train no.320590  loss = 2.91459 avg_loss = 3.09802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.320600  loss = 3.57725 avg_loss = 3.06914\n",
      "epoch no.4 train no.320610  loss = 2.72392 avg_loss = 3.04484\n",
      "epoch no.4 train no.320620  loss = 3.57150 avg_loss = 3.05887\n",
      "epoch no.4 train no.320630  loss = 2.59999 avg_loss = 3.04258\n",
      "epoch no.4 train no.320640  loss = 2.49935 avg_loss = 3.02330\n",
      "epoch no.4 train no.320650  loss = 2.81641 avg_loss = 3.05422\n",
      "epoch no.4 train no.320660  loss = 2.01525 avg_loss = 3.06196\n",
      "epoch no.4 train no.320670  loss = 2.04681 avg_loss = 3.06247\n",
      "epoch no.4 train no.320680  loss = 3.57458 avg_loss = 3.08170\n",
      "epoch no.4 train no.320690  loss = 3.94588 avg_loss = 3.10211\n",
      "epoch no.4 train no.320700  loss = 3.54487 avg_loss = 3.06626\n",
      "epoch no.4 train no.320710  loss = 2.40111 avg_loss = 3.09974\n",
      "epoch no.4 train no.320720  loss = 3.00577 avg_loss = 3.08699\n",
      "epoch no.4 train no.320730  loss = 2.19163 avg_loss = 3.07435\n",
      "epoch no.4 train no.320740  loss = 2.46163 avg_loss = 3.11449\n",
      "epoch no.4 train no.320750  loss = 2.80849 avg_loss = 3.10267\n",
      "epoch no.4 train no.320760  loss = 2.32712 avg_loss = 3.07273\n",
      "epoch no.4 train no.320770  loss = 2.52897 avg_loss = 3.08612\n",
      "epoch no.4 train no.320780  loss = 3.52032 avg_loss = 3.12409\n",
      "epoch no.4 train no.320790  loss = 2.60723 avg_loss = 3.11394\n",
      "epoch no.4 train no.320800  loss = 4.39426 avg_loss = 3.07554\n",
      "epoch no.4 train no.320810  loss = 3.60283 avg_loss = 3.03539\n",
      "epoch no.4 train no.320820  loss = 3.11450 avg_loss = 3.06338\n",
      "epoch no.4 train no.320830  loss = 3.85156 avg_loss = 3.06620\n",
      "epoch no.4 train no.320840  loss = 2.57792 avg_loss = 3.03657\n",
      "epoch no.4 train no.320850  loss = 5.09716 avg_loss = 3.06908\n",
      "epoch no.4 train no.320860  loss = 1.70124 avg_loss = 3.05471\n",
      "epoch no.4 train no.320870  loss = 6.11265 avg_loss = 3.11790\n",
      "epoch no.4 train no.320880  loss = 3.88442 avg_loss = 3.14569\n",
      "epoch no.4 train no.320890  loss = 2.36857 avg_loss = 3.17830\n",
      "epoch no.4 train no.320900  loss = 3.77187 avg_loss = 3.16913\n",
      "epoch no.4 train no.320910  loss = 4.10931 avg_loss = 3.19308\n",
      "epoch no.4 train no.320920  loss = 3.91207 avg_loss = 3.16369\n",
      "epoch no.4 train no.320930  loss = 3.53013 avg_loss = 3.14241\n",
      "epoch no.4 train no.320940  loss = 3.64631 avg_loss = 3.15563\n",
      "epoch no.4 train no.320950  loss = 5.16324 avg_loss = 3.18193\n",
      "epoch no.4 train no.320960  loss = 3.84614 avg_loss = 3.16945\n",
      "epoch no.4 train no.320970  loss = 4.02830 avg_loss = 3.17057\n",
      "epoch no.4 train no.320980  loss = 2.84443 avg_loss = 3.15478\n",
      "epoch no.4 train no.320990  loss = 4.57472 avg_loss = 3.21188\n",
      "epoch no.4 train no.321000  loss = 2.67304 avg_loss = 3.18546\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '들', '▁모음', '노래', '</s>']\n",
      "추억의 노래들 옛날노래</s>\n",
      "epoch no.4 train no.321010  loss = 2.81933 avg_loss = 3.16990\n",
      "epoch no.4 train no.321020  loss = 2.53313 avg_loss = 3.16104\n",
      "epoch no.4 train no.321030  loss = 3.47574 avg_loss = 3.17142\n",
      "epoch no.4 train no.321040  loss = 3.36310 avg_loss = 3.19927\n",
      "epoch no.4 train no.321050  loss = 3.27392 avg_loss = 3.18455\n",
      "epoch no.4 train no.321060  loss = 3.53242 avg_loss = 3.15050\n",
      "epoch no.4 train no.321070  loss = 3.76846 avg_loss = 3.13271\n",
      "epoch no.4 train no.321080  loss = 2.31933 avg_loss = 3.17989\n",
      "epoch no.4 train no.321090  loss = 2.64187 avg_loss = 3.13820\n",
      "epoch no.4 train no.321100  loss = 2.10246 avg_loss = 3.11647\n",
      "epoch no.4 train no.321110  loss = 2.09518 avg_loss = 3.13149\n",
      "epoch no.4 train no.321120  loss = 3.03879 avg_loss = 3.11597\n",
      "epoch no.4 train no.321130  loss = 4.88447 avg_loss = 3.18752\n",
      "epoch no.4 train no.321140  loss = 2.59875 avg_loss = 3.18313\n",
      "epoch no.4 train no.321150  loss = 2.04203 avg_loss = 3.17068\n",
      "epoch no.4 train no.321160  loss = 4.35026 avg_loss = 3.15088\n",
      "epoch no.4 train no.321170  loss = 3.21161 avg_loss = 3.13624\n",
      "epoch no.4 train no.321180  loss = 3.43524 avg_loss = 3.16240\n",
      "epoch no.4 train no.321190  loss = 3.70309 avg_loss = 3.20465\n",
      "epoch no.4 train no.321200  loss = 3.54003 avg_loss = 3.18297\n",
      "epoch no.4 train no.321210  loss = 2.09114 avg_loss = 3.16496\n",
      "epoch no.4 train no.321220  loss = 3.71209 avg_loss = 3.15381\n",
      "epoch no.4 train no.321230  loss = 4.55336 avg_loss = 3.14862\n",
      "epoch no.4 train no.321240  loss = 2.95442 avg_loss = 3.13472\n",
      "epoch no.4 train no.321250  loss = 2.14749 avg_loss = 3.16909\n",
      "epoch no.4 train no.321260  loss = 3.93658 avg_loss = 3.20055\n",
      "epoch no.4 train no.321270  loss = 2.98397 avg_loss = 3.16263\n",
      "epoch no.4 train no.321280  loss = 2.91432 avg_loss = 3.17659\n",
      "epoch no.4 train no.321290  loss = 1.76686 avg_loss = 3.16212\n",
      "epoch no.4 train no.321300  loss = 2.71561 avg_loss = 3.17495\n",
      "epoch no.4 train no.321310  loss = 1.31090 avg_loss = 3.14548\n",
      "epoch no.4 train no.321320  loss = 5.05972 avg_loss = 3.13072\n",
      "epoch no.4 train no.321330  loss = 3.59652 avg_loss = 3.13380\n",
      "epoch no.4 train no.321340  loss = 3.23710 avg_loss = 3.11528\n",
      "epoch no.4 train no.321350  loss = 2.70457 avg_loss = 3.10746\n",
      "epoch no.4 train no.321360  loss = 4.42669 avg_loss = 3.08727\n",
      "epoch no.4 train no.321370  loss = 3.43930 avg_loss = 3.08257\n",
      "epoch no.4 train no.321380  loss = 3.57558 avg_loss = 3.12870\n",
      "epoch no.4 train no.321390  loss = 2.83786 avg_loss = 3.09920\n",
      "epoch no.4 train no.321400  loss = 4.76731 avg_loss = 3.13531\n",
      "epoch no.4 train no.321410  loss = 3.10360 avg_loss = 3.12322\n",
      "epoch no.4 train no.321420  loss = 2.72878 avg_loss = 3.11752\n",
      "epoch no.4 train no.321430  loss = 2.25328 avg_loss = 3.13639\n",
      "epoch no.4 train no.321440  loss = 3.63115 avg_loss = 3.16160\n",
      "epoch no.4 train no.321450  loss = 2.14508 avg_loss = 3.14555\n",
      "epoch no.4 train no.321460  loss = 2.17821 avg_loss = 3.18716\n",
      "epoch no.4 train no.321470  loss = 2.70371 avg_loss = 3.17724\n",
      "epoch no.4 train no.321480  loss = 3.32635 avg_loss = 3.16476\n",
      "epoch no.4 train no.321490  loss = 2.81068 avg_loss = 3.17438\n",
      "epoch no.4 train no.321500  loss = 2.49628 avg_loss = 3.15364\n",
      "epoch no.4 train no.321510  loss = 4.81680 avg_loss = 3.19011\n",
      "epoch no.4 train no.321520  loss = 4.99542 avg_loss = 3.21170\n",
      "epoch no.4 train no.321530  loss = 2.32273 avg_loss = 3.19062\n",
      "epoch no.4 train no.321540  loss = 3.61157 avg_loss = 3.16066\n",
      "epoch no.4 train no.321550  loss = 2.68898 avg_loss = 3.14120\n",
      "epoch no.4 train no.321560  loss = 3.92678 avg_loss = 3.18402\n",
      "epoch no.4 train no.321570  loss = 2.59022 avg_loss = 3.18389\n",
      "epoch no.4 train no.321580  loss = 3.49501 avg_loss = 3.19486\n",
      "epoch no.4 train no.321590  loss = 2.37158 avg_loss = 3.15924\n",
      "epoch no.4 train no.321600  loss = 1.66371 avg_loss = 3.15324\n",
      "epoch no.4 train no.321610  loss = 2.61144 avg_loss = 3.14847\n",
      "epoch no.4 train no.321620  loss = 3.96578 avg_loss = 3.19293\n",
      "epoch no.4 train no.321630  loss = 3.99370 avg_loss = 3.15666\n",
      "epoch no.4 train no.321640  loss = 3.52041 avg_loss = 3.15809\n",
      "epoch no.4 train no.321650  loss = 2.30400 avg_loss = 3.13321\n",
      "epoch no.4 train no.321660  loss = 3.30941 avg_loss = 3.13094\n",
      "epoch no.4 train no.321670  loss = 2.68124 avg_loss = 3.15246\n",
      "epoch no.4 train no.321680  loss = 2.30180 avg_loss = 3.16098\n",
      "epoch no.4 train no.321690  loss = 2.40206 avg_loss = 3.18003\n",
      "epoch no.4 train no.321700  loss = 3.40979 avg_loss = 3.18132\n",
      "epoch no.4 train no.321710  loss = 1.91617 avg_loss = 3.16128\n",
      "epoch no.4 train no.321720  loss = 3.28763 avg_loss = 3.16069\n",
      "epoch no.4 train no.321730  loss = 2.16711 avg_loss = 3.13431\n",
      "epoch no.4 train no.321740  loss = 4.25091 avg_loss = 3.21860\n",
      "epoch no.4 train no.321750  loss = 3.17761 avg_loss = 3.19600\n",
      "epoch no.4 train no.321760  loss = 3.93843 avg_loss = 3.23878\n",
      "epoch no.4 train no.321770  loss = 3.84263 avg_loss = 3.21329\n",
      "epoch no.4 train no.321780  loss = 2.14488 avg_loss = 3.20442\n",
      "epoch no.4 train no.321790  loss = 2.85717 avg_loss = 3.18379\n",
      "epoch no.4 train no.321800  loss = 2.55520 avg_loss = 3.17084\n",
      "epoch no.4 train no.321810  loss = 3.17717 avg_loss = 3.14486\n",
      "epoch no.4 train no.321820  loss = 2.09581 avg_loss = 3.14276\n",
      "epoch no.4 train no.321830  loss = 2.88178 avg_loss = 3.14123\n",
      "epoch no.4 train no.321840  loss = 2.29031 avg_loss = 3.14639\n",
      "epoch no.4 train no.321850  loss = 2.50851 avg_loss = 3.15198\n",
      "epoch no.4 train no.321860  loss = 2.71247 avg_loss = 3.17341\n",
      "epoch no.4 train no.321870  loss = 2.29062 avg_loss = 3.17582\n",
      "epoch no.4 train no.321880  loss = 3.45786 avg_loss = 3.21314\n",
      "epoch no.4 train no.321890  loss = 2.75825 avg_loss = 3.16969\n",
      "epoch no.4 train no.321900  loss = 2.67363 avg_loss = 3.12542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.321910  loss = 3.06419 avg_loss = 3.12155\n",
      "epoch no.4 train no.321920  loss = 1.99981 avg_loss = 3.12696\n",
      "epoch no.4 train no.321930  loss = 3.10799 avg_loss = 3.12695\n",
      "epoch no.4 train no.321940  loss = 3.19059 avg_loss = 3.15342\n",
      "epoch no.4 train no.321950  loss = 3.32394 avg_loss = 3.17893\n",
      "epoch no.4 train no.321960  loss = 4.28923 avg_loss = 3.21057\n",
      "epoch no.4 train no.321970  loss = 3.75822 avg_loss = 3.22282\n",
      "epoch no.4 train no.321980  loss = 3.98991 avg_loss = 3.21186\n",
      "epoch no.4 train no.321990  loss = 2.75182 avg_loss = 3.20578\n",
      "epoch no.4 train no.322000  loss = 2.73442 avg_loss = 3.24216\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.4 train no.322010  loss = 2.55764 avg_loss = 3.20375\n",
      "epoch no.4 train no.322020  loss = 2.65104 avg_loss = 3.20441\n",
      "epoch no.4 train no.322030  loss = 2.85783 avg_loss = 3.21733\n",
      "epoch no.4 train no.322040  loss = 3.60960 avg_loss = 3.19542\n",
      "epoch no.4 train no.322050  loss = 2.37387 avg_loss = 3.14741\n",
      "epoch no.4 train no.322060  loss = 3.28528 avg_loss = 3.20050\n",
      "epoch no.4 train no.322070  loss = 2.80722 avg_loss = 3.27939\n",
      "epoch no.4 train no.322080  loss = 5.60562 avg_loss = 3.31821\n",
      "epoch no.4 train no.322090  loss = 5.39437 avg_loss = 3.31808\n",
      "epoch no.4 train no.322100  loss = 2.36957 avg_loss = 3.29753\n",
      "epoch no.4 train no.322110  loss = 2.50699 avg_loss = 3.28692\n",
      "epoch no.4 train no.322120  loss = 2.65540 avg_loss = 3.25827\n",
      "epoch no.4 train no.322130  loss = 4.99910 avg_loss = 3.29863\n",
      "epoch no.4 train no.322140  loss = 3.27052 avg_loss = 3.26748\n",
      "epoch no.4 train no.322150  loss = 3.05010 avg_loss = 3.23538\n",
      "epoch no.4 train no.322160  loss = 2.90234 avg_loss = 3.25112\n",
      "epoch no.4 train no.322170  loss = 2.91016 avg_loss = 3.21771\n",
      "epoch no.4 train no.322180  loss = 2.37074 avg_loss = 3.23492\n",
      "epoch no.4 train no.322190  loss = 3.37577 avg_loss = 3.26029\n",
      "epoch no.4 train no.322200  loss = 2.57033 avg_loss = 3.24491\n",
      "epoch no.4 train no.322210  loss = 2.94259 avg_loss = 3.28582\n",
      "epoch no.4 train no.322220  loss = 4.39784 avg_loss = 3.27100\n",
      "epoch no.4 train no.322230  loss = 5.01612 avg_loss = 3.31076\n",
      "epoch no.4 train no.322240  loss = 2.49742 avg_loss = 3.26663\n",
      "epoch no.4 train no.322250  loss = 2.51252 avg_loss = 3.21444\n",
      "epoch no.4 train no.322260  loss = 3.65848 avg_loss = 3.20092\n",
      "epoch no.4 train no.322270  loss = 2.69982 avg_loss = 3.17507\n",
      "epoch no.4 train no.322280  loss = 2.82018 avg_loss = 3.14038\n",
      "epoch no.4 train no.322290  loss = 2.01695 avg_loss = 3.13265\n",
      "epoch no.4 train no.322300  loss = 2.93797 avg_loss = 3.14875\n",
      "epoch no.4 train no.322310  loss = 2.90432 avg_loss = 3.14227\n",
      "epoch no.4 train no.322320  loss = 2.65522 avg_loss = 3.09459\n",
      "epoch no.4 train no.322330  loss = 3.92361 avg_loss = 3.09257\n",
      "epoch no.4 train no.322340  loss = 3.06227 avg_loss = 3.06939\n",
      "epoch no.4 train no.322350  loss = 4.10241 avg_loss = 3.07699\n",
      "epoch no.4 train no.322360  loss = 2.61216 avg_loss = 3.05889\n",
      "epoch no.4 train no.322370  loss = 3.17697 avg_loss = 3.09172\n",
      "epoch no.4 train no.322380  loss = 1.58354 avg_loss = 3.06849\n",
      "epoch no.4 train no.322390  loss = 2.31956 avg_loss = 3.08859\n",
      "epoch no.4 train no.322400  loss = 4.15394 avg_loss = 3.07586\n",
      "epoch no.4 train no.322410  loss = 3.27690 avg_loss = 3.09888\n",
      "epoch no.4 train no.322420  loss = 1.87038 avg_loss = 3.07234\n",
      "epoch no.4 train no.322430  loss = 2.45222 avg_loss = 3.09561\n",
      "epoch no.4 train no.322440  loss = 2.56264 avg_loss = 3.08911\n",
      "epoch no.4 train no.322450  loss = 3.88614 avg_loss = 3.09565\n",
      "epoch no.4 train no.322460  loss = 2.02139 avg_loss = 3.06291\n",
      "epoch no.4 train no.322470  loss = 1.65653 avg_loss = 3.07301\n",
      "epoch no.4 train no.322480  loss = 2.66833 avg_loss = 3.08000\n",
      "epoch no.4 train no.322490  loss = 3.47434 avg_loss = 3.10005\n",
      "epoch no.4 train no.322500  loss = 5.22580 avg_loss = 3.10848\n",
      "epoch no.4 train no.322510  loss = 4.82253 avg_loss = 3.14344\n",
      "epoch no.4 train no.322520  loss = 3.84074 avg_loss = 3.16876\n",
      "epoch no.4 train no.322530  loss = 2.17983 avg_loss = 3.17563\n",
      "epoch no.4 train no.322540  loss = 3.70299 avg_loss = 3.17432\n",
      "epoch no.4 train no.322550  loss = 2.58058 avg_loss = 3.17063\n",
      "epoch no.4 train no.322560  loss = 4.12693 avg_loss = 3.17201\n",
      "epoch no.4 train no.322570  loss = 2.62676 avg_loss = 3.18165\n",
      "epoch no.4 train no.322580  loss = 3.25185 avg_loss = 3.21954\n",
      "epoch no.4 train no.322590  loss = 3.37364 avg_loss = 3.18649\n",
      "epoch no.4 train no.322600  loss = 2.89535 avg_loss = 3.18903\n",
      "epoch no.4 train no.322610  loss = 2.80948 avg_loss = 3.20962\n",
      "epoch no.4 train no.322620  loss = 3.94169 avg_loss = 3.20099\n",
      "epoch no.4 train no.322630  loss = 3.04890 avg_loss = 3.18815\n",
      "epoch no.4 train no.322640  loss = 2.22365 avg_loss = 3.18902\n",
      "epoch no.4 train no.322650  loss = 3.21599 avg_loss = 3.16640\n",
      "epoch no.4 train no.322660  loss = 3.70746 avg_loss = 3.18880\n",
      "epoch no.4 train no.322670  loss = 4.50217 avg_loss = 3.20261\n",
      "epoch no.4 train no.322680  loss = 2.35374 avg_loss = 3.21097\n",
      "epoch no.4 train no.322690  loss = 3.34396 avg_loss = 3.17112\n",
      "epoch no.4 train no.322700  loss = 3.36556 avg_loss = 3.18865\n",
      "epoch no.4 train no.322710  loss = 2.95482 avg_loss = 3.17844\n",
      "epoch no.4 train no.322720  loss = 2.66675 avg_loss = 3.12679\n",
      "epoch no.4 train no.322730  loss = 2.67606 avg_loss = 3.12466\n",
      "epoch no.4 train no.322740  loss = 2.82737 avg_loss = 3.09506\n",
      "epoch no.4 train no.322750  loss = 3.18417 avg_loss = 3.07132\n",
      "epoch no.4 train no.322760  loss = 3.23042 avg_loss = 3.07027\n",
      "epoch no.4 train no.322770  loss = 2.86085 avg_loss = 3.07774\n",
      "epoch no.4 train no.322780  loss = 2.22874 avg_loss = 3.10464\n",
      "epoch no.4 train no.322790  loss = 2.44233 avg_loss = 3.11742\n",
      "epoch no.4 train no.322800  loss = 3.69608 avg_loss = 3.13124\n",
      "epoch no.4 train no.322810  loss = 2.12609 avg_loss = 3.13106\n",
      "epoch no.4 train no.322820  loss = 3.30583 avg_loss = 3.12402\n",
      "epoch no.4 train no.322830  loss = 3.90895 avg_loss = 3.11636\n",
      "epoch no.4 train no.322840  loss = 3.75865 avg_loss = 3.12535\n",
      "epoch no.4 train no.322850  loss = 4.42931 avg_loss = 3.11860\n",
      "epoch no.4 train no.322860  loss = 1.79383 avg_loss = 3.04423\n",
      "epoch no.4 train no.322870  loss = 2.70390 avg_loss = 3.03815\n",
      "epoch no.4 train no.322880  loss = 2.27627 avg_loss = 3.05870\n",
      "epoch no.4 train no.322890  loss = 3.01909 avg_loss = 3.06190\n",
      "epoch no.4 train no.322900  loss = 2.15469 avg_loss = 3.05118\n",
      "epoch no.4 train no.322910  loss = 3.06336 avg_loss = 3.03228\n",
      "epoch no.4 train no.322920  loss = 2.57851 avg_loss = 3.03263\n",
      "epoch no.4 train no.322930  loss = 3.84551 avg_loss = 3.01070\n",
      "epoch no.4 train no.322940  loss = 2.50162 avg_loss = 3.01796\n",
      "epoch no.4 train no.322950  loss = 4.07845 avg_loss = 2.97908\n",
      "epoch no.4 train no.322960  loss = 2.61163 avg_loss = 2.96387\n",
      "epoch no.4 train no.322970  loss = 3.55532 avg_loss = 2.99168\n",
      "epoch no.4 train no.322980  loss = 3.79685 avg_loss = 3.05355\n",
      "epoch no.4 train no.322990  loss = 1.91518 avg_loss = 3.01037\n",
      "epoch no.4 train no.323000  loss = 1.80407 avg_loss = 2.98380\n",
      "5\n",
      "to_tokens: ['▁비', '▁명', '▁드라마', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 인기 드라마 ost 모음</s>\n",
      "epoch no.4 train no.323010  loss = 4.22296 avg_loss = 3.00167\n",
      "epoch no.4 train no.323020  loss = 3.07044 avg_loss = 3.02576\n",
      "epoch no.4 train no.323030  loss = 1.89075 avg_loss = 3.03535\n",
      "epoch no.4 train no.323040  loss = 3.45877 avg_loss = 3.12246\n",
      "epoch no.4 train no.323050  loss = 3.24323 avg_loss = 3.14231\n",
      "epoch no.4 train no.323060  loss = 3.14368 avg_loss = 3.13793\n",
      "epoch no.4 train no.323070  loss = 4.02207 avg_loss = 3.16723\n",
      "epoch no.4 train no.323080  loss = 1.82957 avg_loss = 3.13268\n",
      "epoch no.4 train no.323090  loss = 2.14855 avg_loss = 3.15576\n",
      "epoch no.4 train no.323100  loss = 2.72783 avg_loss = 3.16992\n",
      "epoch no.4 train no.323110  loss = 2.36978 avg_loss = 3.15349\n",
      "epoch no.4 train no.323120  loss = 3.44114 avg_loss = 3.14158\n",
      "epoch no.4 train no.323130  loss = 2.83029 avg_loss = 3.14017\n",
      "epoch no.4 train no.323140  loss = 2.03641 avg_loss = 3.15581\n",
      "epoch no.4 train no.323150  loss = 1.90358 avg_loss = 3.14161\n",
      "epoch no.4 train no.323160  loss = 2.67260 avg_loss = 3.16719\n",
      "epoch no.4 train no.323170  loss = 3.79909 avg_loss = 3.15382\n",
      "epoch no.4 train no.323180  loss = 3.20531 avg_loss = 3.18895\n",
      "epoch no.4 train no.323190  loss = 2.75406 avg_loss = 3.16271\n",
      "epoch no.4 train no.323200  loss = 3.32381 avg_loss = 3.14848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.323210  loss = 2.30640 avg_loss = 3.18496\n",
      "epoch no.4 train no.323220  loss = 2.68096 avg_loss = 3.19646\n",
      "epoch no.4 train no.323230  loss = 3.54513 avg_loss = 3.17224\n",
      "epoch no.4 train no.323240  loss = 2.55849 avg_loss = 3.18182\n",
      "epoch no.4 train no.323250  loss = 3.16178 avg_loss = 3.14197\n",
      "epoch no.4 train no.323260  loss = 2.97314 avg_loss = 3.12332\n",
      "epoch no.4 train no.323270  loss = 4.20037 avg_loss = 3.17122\n",
      "epoch no.4 train no.323280  loss = 2.93288 avg_loss = 3.18548\n",
      "epoch no.4 train no.323290  loss = 3.32966 avg_loss = 3.17084\n",
      "epoch no.4 train no.323300  loss = 4.33312 avg_loss = 3.23955\n",
      "epoch no.4 train no.323310  loss = 4.27597 avg_loss = 3.26855\n",
      "epoch no.4 train no.323320  loss = 2.55689 avg_loss = 3.27502\n",
      "epoch no.4 train no.323330  loss = 3.01619 avg_loss = 3.26187\n",
      "epoch no.4 train no.323340  loss = 3.52960 avg_loss = 3.26856\n",
      "epoch no.4 train no.323350  loss = 3.09227 avg_loss = 3.28182\n",
      "epoch no.4 train no.323360  loss = 3.33504 avg_loss = 3.23223\n",
      "epoch no.4 train no.323370  loss = 4.04702 avg_loss = 3.19420\n",
      "epoch no.4 train no.323380  loss = 3.51944 avg_loss = 3.19634\n",
      "epoch no.4 train no.323390  loss = 3.26183 avg_loss = 3.21847\n",
      "epoch no.4 train no.323400  loss = 2.53606 avg_loss = 3.22585\n",
      "epoch no.4 train no.323410  loss = 2.89787 avg_loss = 3.19254\n",
      "epoch no.4 train no.323420  loss = 2.16459 avg_loss = 3.17066\n",
      "epoch no.4 train no.323430  loss = 3.01415 avg_loss = 3.16465\n",
      "epoch no.4 train no.323440  loss = 2.31063 avg_loss = 3.11603\n",
      "epoch no.4 train no.323450  loss = 3.74784 avg_loss = 3.11601\n",
      "epoch no.4 train no.323460  loss = 4.03421 avg_loss = 3.14549\n",
      "epoch no.4 train no.323470  loss = 1.94237 avg_loss = 3.09667\n",
      "epoch no.4 train no.323480  loss = 3.86818 avg_loss = 3.07568\n",
      "epoch no.4 train no.323490  loss = 2.13729 avg_loss = 3.09327\n",
      "epoch no.4 train no.323500  loss = 2.40862 avg_loss = 3.11351\n",
      "epoch no.4 train no.323510  loss = 3.44716 avg_loss = 3.13360\n",
      "epoch no.4 train no.323520  loss = 3.69840 avg_loss = 3.13057\n",
      "epoch no.4 train no.323530  loss = 2.83292 avg_loss = 3.14051\n",
      "epoch no.4 train no.323540  loss = 3.62303 avg_loss = 3.12544\n",
      "epoch no.4 train no.323550  loss = 2.17456 avg_loss = 3.10216\n",
      "epoch no.4 train no.323560  loss = 1.86404 avg_loss = 3.13568\n",
      "epoch no.4 train no.323570  loss = 3.20279 avg_loss = 3.16848\n",
      "epoch no.4 train no.323580  loss = 4.12974 avg_loss = 3.18155\n",
      "epoch no.4 train no.323590  loss = 4.17523 avg_loss = 3.22152\n",
      "epoch no.4 train no.323600  loss = 1.73905 avg_loss = 3.15310\n",
      "epoch no.4 train no.323610  loss = 4.22474 avg_loss = 3.15421\n",
      "epoch no.4 train no.323620  loss = 2.00106 avg_loss = 3.12700\n",
      "epoch no.4 train no.323630  loss = 2.76166 avg_loss = 3.13407\n",
      "epoch no.4 train no.323640  loss = 3.22218 avg_loss = 3.14812\n",
      "epoch no.4 train no.323650  loss = 2.68073 avg_loss = 3.14298\n",
      "epoch no.4 train no.323660  loss = 3.62410 avg_loss = 3.16874\n",
      "epoch no.4 train no.323670  loss = 2.33796 avg_loss = 3.19605\n",
      "epoch no.4 train no.323680  loss = 2.98497 avg_loss = 3.21445\n",
      "epoch no.4 train no.323690  loss = 2.90529 avg_loss = 3.21688\n",
      "epoch no.4 train no.323700  loss = 2.81019 avg_loss = 3.22469\n",
      "epoch no.4 train no.323710  loss = 2.24085 avg_loss = 3.20144\n",
      "epoch no.4 train no.323720  loss = 2.03998 avg_loss = 3.21346\n",
      "epoch no.4 train no.323730  loss = 3.47860 avg_loss = 3.23220\n",
      "epoch no.4 train no.323740  loss = 2.93960 avg_loss = 3.25238\n",
      "epoch no.4 train no.323750  loss = 2.51149 avg_loss = 3.24655\n",
      "epoch no.4 train no.323760  loss = 3.97589 avg_loss = 3.25370\n",
      "epoch no.4 train no.323770  loss = 3.01920 avg_loss = 3.18615\n",
      "epoch no.4 train no.323780  loss = 4.10104 avg_loss = 3.15867\n",
      "epoch no.4 train no.323790  loss = 4.94281 avg_loss = 3.18606\n",
      "epoch no.4 train no.323800  loss = 3.10486 avg_loss = 3.17142\n",
      "epoch no.4 train no.323810  loss = 4.23858 avg_loss = 3.15722\n",
      "epoch no.4 train no.323820  loss = 2.97182 avg_loss = 3.12283\n",
      "epoch no.4 train no.323830  loss = 1.83106 avg_loss = 3.10492\n",
      "epoch no.4 train no.323840  loss = 2.02663 avg_loss = 3.08687\n",
      "epoch no.4 train no.323850  loss = 1.63973 avg_loss = 3.09509\n",
      "epoch no.4 train no.323860  loss = 3.67212 avg_loss = 3.11744\n",
      "epoch no.4 train no.323870  loss = 4.69642 avg_loss = 3.11672\n",
      "epoch no.4 train no.323880  loss = 1.71712 avg_loss = 3.10161\n",
      "epoch no.4 train no.323890  loss = 2.12766 avg_loss = 3.12410\n",
      "epoch no.4 train no.323900  loss = 4.52347 avg_loss = 3.12171\n",
      "epoch no.4 train no.323910  loss = 3.51103 avg_loss = 3.16034\n",
      "epoch no.4 train no.323920  loss = 2.70685 avg_loss = 3.15925\n",
      "epoch no.4 train no.323930  loss = 2.89657 avg_loss = 3.15543\n",
      "epoch no.4 train no.323940  loss = 2.37151 avg_loss = 3.11835\n",
      "epoch no.4 train no.323950  loss = 3.65074 avg_loss = 3.17097\n",
      "epoch no.4 train no.323960  loss = 2.17418 avg_loss = 3.16388\n",
      "epoch no.4 train no.323970  loss = 2.25729 avg_loss = 3.12891\n",
      "epoch no.4 train no.323980  loss = 4.08555 avg_loss = 3.17307\n",
      "epoch no.4 train no.323990  loss = 4.35862 avg_loss = 3.22195\n",
      "epoch no.4 train no.324000  loss = 3.51052 avg_loss = 3.26029\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '▁가요', '</s>', '</s>']\n",
      "추억의 90년대 인기 팝송</s>\n",
      "epoch no.4 train no.324010  loss = 3.53938 avg_loss = 3.26879\n",
      "epoch no.4 train no.324020  loss = 3.65105 avg_loss = 3.23156\n",
      "epoch no.4 train no.324030  loss = 3.48233 avg_loss = 3.19942\n",
      "epoch no.4 train no.324040  loss = 3.62263 avg_loss = 3.21941\n",
      "epoch no.4 train no.324050  loss = 4.21689 avg_loss = 3.19752\n",
      "epoch no.4 train no.324060  loss = 2.44361 avg_loss = 3.14098\n",
      "epoch no.4 train no.324070  loss = 3.86130 avg_loss = 3.15432\n",
      "epoch no.4 train no.324080  loss = 3.22793 avg_loss = 3.14796\n",
      "epoch no.4 train no.324090  loss = 4.12932 avg_loss = 3.17159\n",
      "epoch no.4 train no.324100  loss = 1.82741 avg_loss = 3.11747\n",
      "epoch no.4 train no.324110  loss = 3.51452 avg_loss = 3.13739\n",
      "epoch no.4 train no.324120  loss = 2.80530 avg_loss = 3.11232\n",
      "epoch no.4 train no.324130  loss = 2.38408 avg_loss = 3.09670\n",
      "epoch no.4 train no.324140  loss = 3.21432 avg_loss = 3.10083\n",
      "epoch no.4 train no.324150  loss = 3.84802 avg_loss = 3.11826\n",
      "epoch no.4 train no.324160  loss = 4.62751 avg_loss = 3.12003\n",
      "epoch no.4 train no.324170  loss = 4.82473 avg_loss = 3.10123\n",
      "epoch no.4 train no.324180  loss = 4.93322 avg_loss = 3.15941\n",
      "epoch no.4 train no.324190  loss = 5.32828 avg_loss = 3.17153\n",
      "epoch no.4 train no.324200  loss = 3.63583 avg_loss = 3.18682\n",
      "epoch no.4 train no.324210  loss = 2.73989 avg_loss = 3.20152\n",
      "epoch no.4 train no.324220  loss = 3.38443 avg_loss = 3.20342\n",
      "epoch no.4 train no.324230  loss = 2.30781 avg_loss = 3.23145\n",
      "epoch no.4 train no.324240  loss = 5.32153 avg_loss = 3.24953\n",
      "epoch no.4 train no.324250  loss = 3.69350 avg_loss = 3.28113\n",
      "epoch no.4 train no.324260  loss = 3.20588 avg_loss = 3.26170\n",
      "epoch no.4 train no.324270  loss = 2.45707 avg_loss = 3.28106\n",
      "epoch no.4 train no.324280  loss = 3.95776 avg_loss = 3.27207\n",
      "epoch no.4 train no.324290  loss = 3.09567 avg_loss = 3.26134\n",
      "epoch no.4 train no.324300  loss = 2.90106 avg_loss = 3.28237\n",
      "epoch no.4 train no.324310  loss = 2.95015 avg_loss = 3.25032\n",
      "epoch no.4 train no.324320  loss = 2.85307 avg_loss = 3.22421\n",
      "epoch no.4 train no.324330  loss = 3.07175 avg_loss = 3.21802\n",
      "epoch no.4 train no.324340  loss = 3.37613 avg_loss = 3.23258\n",
      "epoch no.4 train no.324350  loss = 2.88321 avg_loss = 3.21929\n",
      "epoch no.4 train no.324360  loss = 3.90487 avg_loss = 3.18752\n",
      "epoch no.4 train no.324370  loss = 2.23724 avg_loss = 3.15748\n",
      "epoch no.4 train no.324380  loss = 3.44272 avg_loss = 3.14589\n",
      "epoch no.4 train no.324390  loss = 1.71755 avg_loss = 3.14944\n",
      "epoch no.4 train no.324400  loss = 3.10268 avg_loss = 3.10544\n",
      "epoch no.4 train no.324410  loss = 2.53644 avg_loss = 3.12636\n",
      "epoch no.4 train no.324420  loss = 2.37198 avg_loss = 3.13646\n",
      "epoch no.4 train no.324430  loss = 3.33616 avg_loss = 3.10981\n",
      "epoch no.4 train no.324440  loss = 2.48334 avg_loss = 3.11273\n",
      "epoch no.4 train no.324450  loss = 2.98713 avg_loss = 3.10864\n",
      "epoch no.4 train no.324460  loss = 3.52685 avg_loss = 3.08978\n",
      "epoch no.4 train no.324470  loss = 3.83147 avg_loss = 3.10796\n",
      "epoch no.4 train no.324480  loss = 2.26490 avg_loss = 3.12196\n",
      "epoch no.4 train no.324490  loss = 2.53056 avg_loss = 3.13333\n",
      "epoch no.4 train no.324500  loss = 2.57894 avg_loss = 3.11259\n",
      "epoch no.4 train no.324510  loss = 3.58315 avg_loss = 3.17226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.324520  loss = 4.43335 avg_loss = 3.18044\n",
      "epoch no.4 train no.324530  loss = 3.00134 avg_loss = 3.19218\n",
      "epoch no.4 train no.324540  loss = 4.47034 avg_loss = 3.21939\n",
      "epoch no.4 train no.324550  loss = 3.17903 avg_loss = 3.24372\n",
      "epoch no.4 train no.324560  loss = 2.72463 avg_loss = 3.24976\n",
      "epoch no.4 train no.324570  loss = 3.59686 avg_loss = 3.23949\n",
      "epoch no.4 train no.324580  loss = 2.85336 avg_loss = 3.23094\n",
      "epoch no.4 train no.324590  loss = 3.37124 avg_loss = 3.21084\n",
      "epoch no.4 train no.324600  loss = 2.71888 avg_loss = 3.24590\n",
      "epoch no.4 train no.324610  loss = 3.67967 avg_loss = 3.23380\n",
      "epoch no.4 train no.324620  loss = 2.37215 avg_loss = 3.18064\n",
      "epoch no.4 train no.324630  loss = 3.25063 avg_loss = 3.16781\n",
      "epoch no.4 train no.324640  loss = 4.26561 avg_loss = 3.19681\n",
      "epoch no.4 train no.324650  loss = 3.18526 avg_loss = 3.19021\n",
      "epoch no.4 train no.324660  loss = 2.24194 avg_loss = 3.17709\n",
      "epoch no.4 train no.324670  loss = 3.05531 avg_loss = 3.16985\n",
      "epoch no.4 train no.324680  loss = 2.39293 avg_loss = 3.17184\n",
      "epoch no.4 train no.324690  loss = 2.88678 avg_loss = 3.17212\n",
      "epoch no.4 train no.324700  loss = 2.91572 avg_loss = 3.15547\n",
      "epoch no.4 train no.324710  loss = 2.64371 avg_loss = 3.12089\n",
      "epoch no.4 train no.324720  loss = 2.38946 avg_loss = 3.12677\n",
      "epoch no.4 train no.324730  loss = 2.20315 avg_loss = 3.11005\n",
      "epoch no.4 train no.324740  loss = 2.52533 avg_loss = 3.12558\n",
      "epoch no.4 train no.324750  loss = 4.45377 avg_loss = 3.10759\n",
      "epoch no.4 train no.324760  loss = 1.75913 avg_loss = 3.08052\n",
      "epoch no.4 train no.324770  loss = 3.35007 avg_loss = 3.09865\n",
      "epoch no.4 train no.324780  loss = 2.91345 avg_loss = 3.06517\n",
      "epoch no.4 train no.324790  loss = 3.87037 avg_loss = 3.09917\n",
      "epoch no.4 train no.324800  loss = 2.74172 avg_loss = 3.07846\n",
      "epoch no.4 train no.324810  loss = 3.48018 avg_loss = 3.11650\n",
      "epoch no.4 train no.324820  loss = 4.05099 avg_loss = 3.10477\n",
      "epoch no.4 train no.324830  loss = 4.15442 avg_loss = 3.09264\n",
      "epoch no.4 train no.324840  loss = 2.99749 avg_loss = 3.04664\n",
      "epoch no.4 train no.324850  loss = 3.12888 avg_loss = 3.10026\n",
      "epoch no.4 train no.324860  loss = 3.52462 avg_loss = 3.13290\n",
      "epoch no.4 train no.324870  loss = 3.06761 avg_loss = 3.11544\n",
      "epoch no.4 train no.324880  loss = 1.80780 avg_loss = 3.02674\n",
      "epoch no.4 train no.324890  loss = 2.41416 avg_loss = 3.00047\n",
      "epoch no.4 train no.324900  loss = 3.88276 avg_loss = 3.02436\n",
      "epoch no.4 train no.324910  loss = 3.11949 avg_loss = 3.02157\n",
      "epoch no.4 train no.324920  loss = 2.72311 avg_loss = 3.04317\n",
      "epoch no.4 train no.324930  loss = 2.79102 avg_loss = 3.03333\n",
      "epoch no.4 train no.324940  loss = 6.08905 avg_loss = 3.02883\n",
      "epoch no.4 train no.324950  loss = 5.17907 avg_loss = 3.05033\n",
      "epoch no.4 train no.324960  loss = 3.35104 avg_loss = 3.03639\n",
      "epoch no.4 train no.324970  loss = 4.03371 avg_loss = 3.08447\n",
      "epoch no.4 train no.324980  loss = 3.38424 avg_loss = 3.08205\n",
      "epoch no.4 train no.324990  loss = 4.66998 avg_loss = 3.10725\n",
      "epoch no.4 train no.325000  loss = 2.56523 avg_loss = 3.09639\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.4 train no.325010  loss = 3.46846 avg_loss = 3.11446\n",
      "epoch no.4 train no.325020  loss = 3.12058 avg_loss = 3.07612\n",
      "epoch no.4 train no.325030  loss = 3.36549 avg_loss = 3.04442\n",
      "epoch no.4 train no.325040  loss = 3.73574 avg_loss = 3.10014\n",
      "epoch no.4 train no.325050  loss = 1.54057 avg_loss = 3.09288\n",
      "epoch no.4 train no.325060  loss = 2.97290 avg_loss = 3.09133\n",
      "epoch no.4 train no.325070  loss = 2.53136 avg_loss = 3.10235\n",
      "epoch no.4 train no.325080  loss = 1.65071 avg_loss = 3.09794\n",
      "epoch no.4 train no.325090  loss = 2.48078 avg_loss = 3.07105\n",
      "epoch no.4 train no.325100  loss = 2.67148 avg_loss = 3.08155\n",
      "epoch no.4 train no.325110  loss = 2.72979 avg_loss = 3.09052\n",
      "epoch no.4 train no.325120  loss = 2.60592 avg_loss = 3.09344\n",
      "epoch no.4 train no.325130  loss = 4.20870 avg_loss = 3.09303\n",
      "epoch no.4 train no.325140  loss = 3.39399 avg_loss = 3.07834\n",
      "epoch no.4 train no.325150  loss = 2.73692 avg_loss = 3.09400\n",
      "epoch no.4 train no.325160  loss = 3.44068 avg_loss = 3.10090\n",
      "epoch no.4 train no.325170  loss = 1.82200 avg_loss = 3.10512\n",
      "epoch no.4 train no.325180  loss = 1.88538 avg_loss = 3.11710\n",
      "epoch no.4 train no.325190  loss = 2.93791 avg_loss = 3.08975\n",
      "epoch no.4 train no.325200  loss = 2.98338 avg_loss = 3.13845\n",
      "epoch no.4 train no.325210  loss = 4.62832 avg_loss = 3.12916\n",
      "epoch no.4 train no.325220  loss = 3.39121 avg_loss = 3.11435\n",
      "epoch no.4 train no.325230  loss = 3.81359 avg_loss = 3.15305\n",
      "epoch no.4 train no.325240  loss = 2.39124 avg_loss = 3.15682\n",
      "epoch no.4 train no.325250  loss = 3.88755 avg_loss = 3.15774\n",
      "epoch no.4 train no.325260  loss = 4.27179 avg_loss = 3.16931\n",
      "epoch no.4 train no.325270  loss = 3.20751 avg_loss = 3.18126\n",
      "epoch no.4 train no.325280  loss = 1.79431 avg_loss = 3.18293\n",
      "epoch no.4 train no.325290  loss = 3.21541 avg_loss = 3.22559\n",
      "epoch no.4 train no.325300  loss = 3.58332 avg_loss = 3.24883\n",
      "epoch no.4 train no.325310  loss = 2.39095 avg_loss = 3.23410\n",
      "epoch no.4 train no.325320  loss = 5.24457 avg_loss = 3.27440\n",
      "epoch no.4 train no.325330  loss = 2.30260 avg_loss = 3.24282\n",
      "epoch no.4 train no.325340  loss = 3.19476 avg_loss = 3.24649\n",
      "epoch no.4 train no.325350  loss = 4.58567 avg_loss = 3.26385\n",
      "epoch no.4 train no.325360  loss = 1.98857 avg_loss = 3.26643\n",
      "epoch no.4 train no.325370  loss = 3.94085 avg_loss = 3.26509\n",
      "epoch no.4 train no.325380  loss = 3.09351 avg_loss = 3.27759\n",
      "epoch no.4 train no.325390  loss = 3.86176 avg_loss = 3.24734\n",
      "epoch no.4 train no.325400  loss = 3.19278 avg_loss = 3.23846\n",
      "epoch no.4 train no.325410  loss = 3.03290 avg_loss = 3.20234\n",
      "epoch no.4 train no.325420  loss = 2.36566 avg_loss = 3.20638\n",
      "epoch no.4 train no.325430  loss = 3.86190 avg_loss = 3.21084\n",
      "epoch no.4 train no.325440  loss = 2.57260 avg_loss = 3.21167\n",
      "epoch no.4 train no.325450  loss = 3.04400 avg_loss = 3.26454\n",
      "epoch no.4 train no.325460  loss = 3.40319 avg_loss = 3.25382\n",
      "epoch no.4 train no.325470  loss = 2.50899 avg_loss = 3.22314\n",
      "epoch no.4 train no.325480  loss = 2.86159 avg_loss = 3.19712\n",
      "epoch no.4 train no.325490  loss = 2.56139 avg_loss = 3.17408\n",
      "epoch no.4 train no.325500  loss = 2.26353 avg_loss = 3.17242\n",
      "epoch no.4 train no.325510  loss = 2.49507 avg_loss = 3.14878\n",
      "epoch no.4 train no.325520  loss = 1.83964 avg_loss = 3.10070\n",
      "epoch no.4 train no.325530  loss = 3.62974 avg_loss = 3.11375\n",
      "epoch no.4 train no.325540  loss = 4.55325 avg_loss = 3.18299\n",
      "epoch no.4 train no.325550  loss = 4.23566 avg_loss = 3.20041\n",
      "epoch no.4 train no.325560  loss = 3.37029 avg_loss = 3.23456\n",
      "epoch no.4 train no.325570  loss = 3.97171 avg_loss = 3.25174\n",
      "epoch no.4 train no.325580  loss = 2.35614 avg_loss = 3.22646\n",
      "epoch no.4 train no.325590  loss = 2.54480 avg_loss = 3.24674\n",
      "epoch no.4 train no.325600  loss = 3.90001 avg_loss = 3.20466\n",
      "epoch no.4 train no.325610  loss = 2.38459 avg_loss = 3.19606\n",
      "epoch no.4 train no.325620  loss = 2.37468 avg_loss = 3.20627\n",
      "epoch no.4 train no.325630  loss = 1.63732 avg_loss = 3.17800\n",
      "epoch no.4 train no.325640  loss = 3.75004 avg_loss = 3.17676\n",
      "epoch no.4 train no.325650  loss = 2.80132 avg_loss = 3.16117\n",
      "epoch no.4 train no.325660  loss = 1.63540 avg_loss = 3.17630\n",
      "epoch no.4 train no.325670  loss = 1.15477 avg_loss = 3.18470\n",
      "epoch no.4 train no.325680  loss = 3.07853 avg_loss = 3.14483\n",
      "epoch no.4 train no.325690  loss = 1.72207 avg_loss = 3.16439\n",
      "epoch no.4 train no.325700  loss = 2.81132 avg_loss = 3.14967\n",
      "epoch no.4 train no.325710  loss = 4.53495 avg_loss = 3.13763\n",
      "epoch no.4 train no.325720  loss = 1.83712 avg_loss = 3.14859\n",
      "epoch no.4 train no.325730  loss = 2.50224 avg_loss = 3.16840\n",
      "epoch no.4 train no.325740  loss = 2.71443 avg_loss = 3.14961\n",
      "epoch no.4 train no.325750  loss = 4.49891 avg_loss = 3.17248\n",
      "epoch no.4 train no.325760  loss = 2.52023 avg_loss = 3.12636\n",
      "epoch no.4 train no.325770  loss = 4.88585 avg_loss = 3.13956\n",
      "epoch no.4 train no.325780  loss = 3.48148 avg_loss = 3.14392\n",
      "epoch no.4 train no.325790  loss = 2.97681 avg_loss = 3.17718\n",
      "epoch no.4 train no.325800  loss = 3.42847 avg_loss = 3.17097\n",
      "epoch no.4 train no.325810  loss = 4.27168 avg_loss = 3.20777\n",
      "epoch no.4 train no.325820  loss = 3.89545 avg_loss = 3.23055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.325830  loss = 2.39018 avg_loss = 3.20947\n",
      "epoch no.4 train no.325840  loss = 2.11560 avg_loss = 3.20544\n",
      "epoch no.4 train no.325850  loss = 3.50976 avg_loss = 3.18425\n",
      "epoch no.4 train no.325860  loss = 3.21875 avg_loss = 3.18402\n",
      "epoch no.4 train no.325870  loss = 4.48498 avg_loss = 3.20099\n",
      "epoch no.4 train no.325880  loss = 2.57512 avg_loss = 3.21514\n",
      "epoch no.4 train no.325890  loss = 2.68742 avg_loss = 3.24108\n",
      "epoch no.4 train no.325900  loss = 3.90168 avg_loss = 3.25735\n",
      "epoch no.4 train no.325910  loss = 4.05704 avg_loss = 3.22506\n",
      "epoch no.4 train no.325920  loss = 2.25661 avg_loss = 3.23050\n",
      "epoch no.4 train no.325930  loss = 2.95480 avg_loss = 3.20972\n",
      "epoch no.4 train no.325940  loss = 3.07059 avg_loss = 3.18289\n",
      "epoch no.4 train no.325950  loss = 3.50636 avg_loss = 3.22338\n",
      "epoch no.4 train no.325960  loss = 2.65284 avg_loss = 3.27122\n",
      "epoch no.4 train no.325970  loss = 2.25670 avg_loss = 3.23988\n",
      "epoch no.4 train no.325980  loss = 3.58511 avg_loss = 3.25409\n",
      "epoch no.4 train no.325990  loss = 3.01011 avg_loss = 3.27231\n",
      "epoch no.4 train no.326000  loss = 4.11156 avg_loss = 3.24084\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대  발라드</s>\n",
      "epoch no.4 train no.326010  loss = 3.29820 avg_loss = 3.24263\n",
      "epoch no.4 train no.326020  loss = 4.38868 avg_loss = 3.20114\n",
      "epoch no.4 train no.326030  loss = 3.69330 avg_loss = 3.20890\n",
      "epoch no.4 train no.326040  loss = 2.54838 avg_loss = 3.18669\n",
      "epoch no.4 train no.326050  loss = 3.67113 avg_loss = 3.19067\n",
      "epoch no.4 train no.326060  loss = 2.62925 avg_loss = 3.17926\n",
      "epoch no.4 train no.326070  loss = 2.55016 avg_loss = 3.20781\n",
      "epoch no.4 train no.326080  loss = 2.03212 avg_loss = 3.18392\n",
      "epoch no.4 train no.326090  loss = 2.26107 avg_loss = 3.16338\n",
      "epoch no.4 train no.326100  loss = 2.92302 avg_loss = 3.18445\n",
      "epoch no.4 train no.326110  loss = 2.88989 avg_loss = 3.20090\n",
      "epoch no.4 train no.326120  loss = 2.42900 avg_loss = 3.19736\n",
      "epoch no.4 train no.326130  loss = 3.82778 avg_loss = 3.23581\n",
      "epoch no.4 train no.326140  loss = 2.18410 avg_loss = 3.21944\n",
      "epoch no.4 train no.326150  loss = 3.14511 avg_loss = 3.22454\n",
      "epoch no.4 train no.326160  loss = 2.57096 avg_loss = 3.22366\n",
      "epoch no.4 train no.326170  loss = 3.19994 avg_loss = 3.25376\n",
      "epoch no.4 train no.326180  loss = 2.62193 avg_loss = 3.21779\n",
      "epoch no.4 train no.326190  loss = 2.08013 avg_loss = 3.18666\n",
      "epoch no.4 train no.326200  loss = 1.28180 avg_loss = 3.14380\n",
      "epoch no.4 train no.326210  loss = 3.93329 avg_loss = 3.15006\n",
      "epoch no.4 train no.326220  loss = 2.12178 avg_loss = 3.19177\n",
      "epoch no.4 train no.326230  loss = 2.60618 avg_loss = 3.17007\n",
      "epoch no.4 train no.326240  loss = 2.76086 avg_loss = 3.13545\n",
      "epoch no.4 train no.326250  loss = 1.80331 avg_loss = 3.13578\n",
      "epoch no.4 train no.326260  loss = 3.49790 avg_loss = 3.08880\n",
      "epoch no.4 train no.326270  loss = 2.65047 avg_loss = 3.08222\n",
      "epoch no.4 train no.326280  loss = 2.44767 avg_loss = 3.08099\n",
      "epoch no.4 train no.326290  loss = 2.71988 avg_loss = 3.08437\n",
      "epoch no.4 train no.326300  loss = 2.46001 avg_loss = 3.08805\n",
      "epoch no.4 train no.326310  loss = 2.23207 avg_loss = 3.07959\n",
      "epoch no.4 train no.326320  loss = 2.94540 avg_loss = 3.06942\n",
      "epoch no.4 train no.326330  loss = 3.39073 avg_loss = 3.06986\n",
      "epoch no.4 train no.326340  loss = 3.11600 avg_loss = 3.07493\n",
      "epoch no.4 train no.326350  loss = 3.37322 avg_loss = 3.06765\n",
      "epoch no.4 train no.326360  loss = 3.17680 avg_loss = 3.06141\n",
      "epoch no.4 train no.326370  loss = 3.11818 avg_loss = 3.12836\n",
      "epoch no.4 train no.326380  loss = 3.68397 avg_loss = 3.12531\n",
      "epoch no.4 train no.326390  loss = 2.19429 avg_loss = 3.08114\n",
      "epoch no.4 train no.326400  loss = 4.50812 avg_loss = 3.07117\n",
      "epoch no.4 train no.326410  loss = 3.53327 avg_loss = 3.04097\n",
      "epoch no.4 train no.326420  loss = 3.20992 avg_loss = 3.06501\n",
      "epoch no.4 train no.326430  loss = 2.45745 avg_loss = 3.05496\n",
      "epoch no.4 train no.326440  loss = 3.68188 avg_loss = 3.07599\n",
      "epoch no.4 train no.326450  loss = 4.83138 avg_loss = 3.07329\n",
      "epoch no.4 train no.326460  loss = 2.57075 avg_loss = 3.04403\n",
      "epoch no.4 train no.326470  loss = 3.92465 avg_loss = 3.11205\n",
      "epoch no.4 train no.326480  loss = 2.96368 avg_loss = 3.11702\n",
      "epoch no.4 train no.326490  loss = 3.74564 avg_loss = 3.13472\n",
      "epoch no.4 train no.326500  loss = 3.71453 avg_loss = 3.16531\n",
      "epoch no.4 train no.326510  loss = 3.12744 avg_loss = 3.21041\n",
      "epoch no.4 train no.326520  loss = 2.13155 avg_loss = 3.16919\n",
      "epoch no.4 train no.326530  loss = 1.30067 avg_loss = 3.14474\n",
      "epoch no.4 train no.326540  loss = 4.04028 avg_loss = 3.12488\n",
      "epoch no.4 train no.326550  loss = 2.86475 avg_loss = 3.17725\n",
      "epoch no.4 train no.326560  loss = 3.53777 avg_loss = 3.18087\n",
      "epoch no.4 train no.326570  loss = 2.41940 avg_loss = 3.14810\n",
      "epoch no.4 train no.326580  loss = 1.87280 avg_loss = 3.11521\n",
      "epoch no.4 train no.326590  loss = 3.92027 avg_loss = 3.16895\n",
      "epoch no.4 train no.326600  loss = 2.93620 avg_loss = 3.11912\n",
      "epoch no.4 train no.326610  loss = 3.82748 avg_loss = 3.05443\n",
      "epoch no.4 train no.326620  loss = 2.27553 avg_loss = 3.05964\n",
      "epoch no.4 train no.326630  loss = 2.75162 avg_loss = 3.06547\n",
      "epoch no.4 train no.326640  loss = 1.93704 avg_loss = 3.07594\n",
      "epoch no.4 train no.326650  loss = 1.70125 avg_loss = 3.10567\n",
      "epoch no.4 train no.326660  loss = 3.44853 avg_loss = 3.12818\n",
      "epoch no.4 train no.326670  loss = 3.31539 avg_loss = 3.17894\n",
      "epoch no.4 train no.326680  loss = 3.34721 avg_loss = 3.19997\n",
      "epoch no.4 train no.326690  loss = 3.76715 avg_loss = 3.21650\n",
      "epoch no.4 train no.326700  loss = 2.47149 avg_loss = 3.22397\n",
      "epoch no.4 train no.326710  loss = 1.59017 avg_loss = 3.20313\n",
      "epoch no.4 train no.326720  loss = 1.64894 avg_loss = 3.16555\n",
      "epoch no.4 train no.326730  loss = 2.52625 avg_loss = 3.12776\n",
      "epoch no.4 train no.326740  loss = 3.70356 avg_loss = 3.17001\n",
      "epoch no.4 train no.326750  loss = 3.01247 avg_loss = 3.16004\n",
      "epoch no.4 train no.326760  loss = 4.88315 avg_loss = 3.14805\n",
      "epoch no.4 train no.326770  loss = 2.87105 avg_loss = 3.15598\n",
      "epoch no.4 train no.326780  loss = 3.06588 avg_loss = 3.14121\n",
      "epoch no.4 train no.326790  loss = 2.20226 avg_loss = 3.17019\n",
      "epoch no.4 train no.326800  loss = 3.89944 avg_loss = 3.18849\n",
      "epoch no.4 train no.326810  loss = 4.15699 avg_loss = 3.14672\n",
      "epoch no.4 train no.326820  loss = 2.89979 avg_loss = 3.16564\n",
      "epoch no.4 train no.326830  loss = 2.79143 avg_loss = 3.17187\n",
      "epoch no.4 train no.326840  loss = 2.00954 avg_loss = 3.14741\n",
      "epoch no.4 train no.326850  loss = 2.44201 avg_loss = 3.17287\n",
      "epoch no.4 train no.326860  loss = 2.73938 avg_loss = 3.17108\n",
      "epoch no.4 train no.326870  loss = 4.53783 avg_loss = 3.20113\n",
      "epoch no.4 train no.326880  loss = 2.87088 avg_loss = 3.20376\n",
      "epoch no.4 train no.326890  loss = 1.74631 avg_loss = 3.17914\n",
      "epoch no.4 train no.326900  loss = 1.61569 avg_loss = 3.15548\n",
      "epoch no.4 train no.326910  loss = 2.23335 avg_loss = 3.13788\n",
      "epoch no.4 train no.326920  loss = 2.49221 avg_loss = 3.11061\n",
      "epoch no.4 train no.326930  loss = 3.27976 avg_loss = 3.10594\n",
      "epoch no.4 train no.326940  loss = 3.88405 avg_loss = 3.10720\n",
      "epoch no.4 train no.326950  loss = 1.86665 avg_loss = 3.08482\n",
      "epoch no.4 train no.326960  loss = 3.82186 avg_loss = 3.12957\n",
      "epoch no.4 train no.326970  loss = 4.94757 avg_loss = 3.13450\n",
      "epoch no.4 train no.326980  loss = 4.05232 avg_loss = 3.10008\n",
      "epoch no.4 train no.326990  loss = 2.60741 avg_loss = 3.11387\n",
      "epoch no.4 train no.327000  loss = 3.36209 avg_loss = 3.12558\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '곡', '▁발라', '라드', '</s>']\n",
      "추억의 명곡발라드</s>\n",
      "epoch no.4 train no.327010  loss = 2.12482 avg_loss = 3.11822\n",
      "epoch no.4 train no.327020  loss = 2.97092 avg_loss = 3.10563\n",
      "epoch no.4 train no.327030  loss = 6.09208 avg_loss = 3.11966\n",
      "epoch no.4 train no.327040  loss = 3.76729 avg_loss = 3.13076\n",
      "epoch no.4 train no.327050  loss = 4.26661 avg_loss = 3.11641\n",
      "epoch no.4 train no.327060  loss = 3.39932 avg_loss = 3.16837\n",
      "epoch no.4 train no.327070  loss = 3.23788 avg_loss = 3.17269\n",
      "epoch no.4 train no.327080  loss = 3.53748 avg_loss = 3.17070\n",
      "epoch no.4 train no.327090  loss = 3.22800 avg_loss = 3.15846\n",
      "epoch no.4 train no.327100  loss = 2.96982 avg_loss = 3.14734\n",
      "epoch no.4 train no.327110  loss = 3.06393 avg_loss = 3.14096\n",
      "epoch no.4 train no.327120  loss = 3.85551 avg_loss = 3.11200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.327130  loss = 3.92563 avg_loss = 3.16593\n",
      "epoch no.4 train no.327140  loss = 3.53938 avg_loss = 3.13092\n",
      "epoch no.4 train no.327150  loss = 4.47660 avg_loss = 3.19157\n",
      "epoch no.4 train no.327160  loss = 2.34818 avg_loss = 3.18745\n",
      "epoch no.4 train no.327170  loss = 2.88911 avg_loss = 3.20286\n",
      "epoch no.4 train no.327180  loss = 3.02968 avg_loss = 3.18487\n",
      "epoch no.4 train no.327190  loss = 2.32956 avg_loss = 3.19161\n",
      "epoch no.4 train no.327200  loss = 3.27534 avg_loss = 3.22296\n",
      "epoch no.4 train no.327210  loss = 2.55294 avg_loss = 3.20774\n",
      "epoch no.4 train no.327220  loss = 1.75111 avg_loss = 3.18447\n",
      "epoch no.4 train no.327230  loss = 2.83017 avg_loss = 3.17655\n",
      "epoch no.4 train no.327240  loss = 2.68245 avg_loss = 3.21131\n",
      "epoch no.4 train no.327250  loss = 4.09958 avg_loss = 3.17480\n",
      "epoch no.4 train no.327260  loss = 3.27936 avg_loss = 3.15380\n",
      "epoch no.4 train no.327270  loss = 4.41539 avg_loss = 3.20781\n",
      "epoch no.4 train no.327280  loss = 2.52655 avg_loss = 3.19285\n",
      "epoch no.4 train no.327290  loss = 4.61601 avg_loss = 3.20702\n",
      "epoch no.4 train no.327300  loss = 2.63293 avg_loss = 3.16888\n",
      "epoch no.4 train no.327310  loss = 1.54938 avg_loss = 3.13731\n",
      "epoch no.4 train no.327320  loss = 4.27977 avg_loss = 3.13342\n",
      "epoch no.4 train no.327330  loss = 2.68092 avg_loss = 3.14740\n",
      "epoch no.4 train no.327340  loss = 3.37424 avg_loss = 3.16677\n",
      "epoch no.4 train no.327350  loss = 4.44607 avg_loss = 3.18072\n",
      "epoch no.4 train no.327360  loss = 2.42463 avg_loss = 3.19012\n",
      "epoch no.4 train no.327370  loss = 4.06657 avg_loss = 3.21472\n",
      "epoch no.4 train no.327380  loss = 3.35245 avg_loss = 3.21536\n",
      "epoch no.4 train no.327390  loss = 2.68688 avg_loss = 3.19656\n",
      "epoch no.4 train no.327400  loss = 2.63888 avg_loss = 3.20247\n",
      "epoch no.4 train no.327410  loss = 3.07063 avg_loss = 3.21911\n",
      "epoch no.4 train no.327420  loss = 3.98159 avg_loss = 3.24378\n",
      "epoch no.4 train no.327430  loss = 3.62306 avg_loss = 3.24385\n",
      "epoch no.4 train no.327440  loss = 2.85648 avg_loss = 3.23705\n",
      "epoch no.4 train no.327450  loss = 3.28968 avg_loss = 3.26563\n",
      "epoch no.4 train no.327460  loss = 2.38606 avg_loss = 3.21942\n",
      "epoch no.4 train no.327470  loss = 3.72162 avg_loss = 3.21340\n",
      "epoch no.4 train no.327480  loss = 1.91910 avg_loss = 3.21606\n",
      "epoch no.4 train no.327490  loss = 1.90802 avg_loss = 3.18221\n",
      "epoch no.4 train no.327500  loss = 3.97431 avg_loss = 3.20899\n",
      "epoch no.4 train no.327510  loss = 3.60354 avg_loss = 3.22649\n",
      "epoch no.4 train no.327520  loss = 3.52301 avg_loss = 3.21579\n",
      "epoch no.4 train no.327530  loss = 2.15001 avg_loss = 3.16846\n",
      "epoch no.4 train no.327540  loss = 2.04198 avg_loss = 3.20192\n",
      "epoch no.4 train no.327550  loss = 2.34063 avg_loss = 3.15942\n",
      "epoch no.4 train no.327560  loss = 2.21926 avg_loss = 3.15203\n",
      "epoch no.4 train no.327570  loss = 1.87118 avg_loss = 3.15828\n",
      "epoch no.4 train no.327580  loss = 3.51731 avg_loss = 3.18086\n",
      "epoch no.4 train no.327590  loss = 2.41971 avg_loss = 3.15856\n",
      "epoch no.4 train no.327600  loss = 2.44431 avg_loss = 3.11878\n",
      "epoch no.4 train no.327610  loss = 2.40248 avg_loss = 3.10711\n",
      "epoch no.4 train no.327620  loss = 3.10479 avg_loss = 3.09909\n",
      "epoch no.4 train no.327630  loss = 5.02517 avg_loss = 3.13104\n",
      "epoch no.4 train no.327640  loss = 3.19411 avg_loss = 3.12115\n",
      "epoch no.4 train no.327650  loss = 4.18703 avg_loss = 3.11011\n",
      "epoch no.4 train no.327660  loss = 1.60225 avg_loss = 3.07728\n",
      "epoch no.4 train no.327670  loss = 3.55883 avg_loss = 3.07574\n",
      "epoch no.4 train no.327680  loss = 2.50029 avg_loss = 3.04452\n",
      "epoch no.4 train no.327690  loss = 1.34612 avg_loss = 3.07880\n",
      "epoch no.4 train no.327700  loss = 2.81712 avg_loss = 3.09793\n",
      "epoch no.4 train no.327710  loss = 3.90902 avg_loss = 3.11928\n",
      "epoch no.4 train no.327720  loss = 2.94945 avg_loss = 3.09105\n",
      "epoch no.4 train no.327730  loss = 1.87502 avg_loss = 3.08675\n",
      "epoch no.4 train no.327740  loss = 3.55073 avg_loss = 3.10286\n",
      "epoch no.4 train no.327750  loss = 2.55913 avg_loss = 3.14458\n",
      "epoch no.4 train no.327760  loss = 3.81197 avg_loss = 3.16464\n",
      "epoch no.4 train no.327770  loss = 2.39764 avg_loss = 3.19326\n",
      "epoch no.4 train no.327780  loss = 3.74652 avg_loss = 3.20236\n",
      "epoch no.4 train no.327790  loss = 4.53844 avg_loss = 3.19821\n",
      "epoch no.4 train no.327800  loss = 2.38498 avg_loss = 3.21658\n",
      "epoch no.4 train no.327810  loss = 2.55670 avg_loss = 3.20491\n",
      "epoch no.4 train no.327820  loss = 3.18828 avg_loss = 3.23512\n",
      "epoch no.4 train no.327830  loss = 2.55608 avg_loss = 3.21500\n",
      "epoch no.4 train no.327840  loss = 3.57394 avg_loss = 3.18733\n",
      "epoch no.4 train no.327850  loss = 3.13401 avg_loss = 3.17288\n",
      "epoch no.4 train no.327860  loss = 3.66179 avg_loss = 3.16278\n",
      "epoch no.4 train no.327870  loss = 3.46236 avg_loss = 3.16027\n",
      "epoch no.4 train no.327880  loss = 3.44357 avg_loss = 3.13821\n",
      "epoch no.4 train no.327890  loss = 4.26631 avg_loss = 3.12099\n",
      "epoch no.4 train no.327900  loss = 2.14007 avg_loss = 3.11066\n",
      "epoch no.4 train no.327910  loss = 3.28508 avg_loss = 3.15009\n",
      "epoch no.4 train no.327920  loss = 3.18143 avg_loss = 3.16162\n",
      "epoch no.4 train no.327930  loss = 2.27896 avg_loss = 3.13184\n",
      "epoch no.4 train no.327940  loss = 2.97976 avg_loss = 3.13698\n",
      "epoch no.4 train no.327950  loss = 2.27525 avg_loss = 3.15848\n",
      "epoch no.4 train no.327960  loss = 3.99732 avg_loss = 3.21663\n",
      "epoch no.4 train no.327970  loss = 2.86799 avg_loss = 3.18500\n",
      "epoch no.4 train no.327980  loss = 4.11330 avg_loss = 3.22065\n",
      "epoch no.4 train no.327990  loss = 2.52299 avg_loss = 3.22956\n",
      "epoch no.4 train no.328000  loss = 4.02563 avg_loss = 3.24501\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.4 train no.328010  loss = 3.21650 avg_loss = 3.28372\n",
      "epoch no.4 train no.328020  loss = 2.25824 avg_loss = 3.25255\n",
      "epoch no.4 train no.328030  loss = 3.51819 avg_loss = 3.23805\n",
      "epoch no.4 train no.328040  loss = 2.81306 avg_loss = 3.24173\n",
      "epoch no.4 train no.328050  loss = 2.35993 avg_loss = 3.21140\n",
      "epoch no.4 train no.328060  loss = 2.20786 avg_loss = 3.20048\n",
      "epoch no.4 train no.328070  loss = 4.32831 avg_loss = 3.22132\n",
      "epoch no.4 train no.328080  loss = 2.97170 avg_loss = 3.22458\n",
      "epoch no.4 train no.328090  loss = 4.21570 avg_loss = 3.21922\n",
      "epoch no.4 train no.328100  loss = 2.95266 avg_loss = 3.24078\n",
      "epoch no.4 train no.328110  loss = 2.31263 avg_loss = 3.19359\n",
      "epoch no.4 train no.328120  loss = 3.64547 avg_loss = 3.15473\n",
      "epoch no.4 train no.328130  loss = 3.07606 avg_loss = 3.17247\n",
      "epoch no.4 train no.328140  loss = 3.22267 avg_loss = 3.19584\n",
      "epoch no.4 train no.328150  loss = 2.11583 avg_loss = 3.18478\n",
      "epoch no.4 train no.328160  loss = 4.58346 avg_loss = 3.19095\n",
      "epoch no.4 train no.328170  loss = 3.32359 avg_loss = 3.15942\n",
      "epoch no.4 train no.328180  loss = 2.93702 avg_loss = 3.17792\n",
      "epoch no.4 train no.328190  loss = 3.04703 avg_loss = 3.19343\n",
      "epoch no.4 train no.328200  loss = 3.54884 avg_loss = 3.19213\n",
      "epoch no.4 train no.328210  loss = 2.64712 avg_loss = 3.20588\n",
      "epoch no.4 train no.328220  loss = 4.14717 avg_loss = 3.20401\n",
      "epoch no.4 train no.328230  loss = 2.94353 avg_loss = 3.16100\n",
      "epoch no.4 train no.328240  loss = 2.87195 avg_loss = 3.17768\n",
      "epoch no.4 train no.328250  loss = 1.35790 avg_loss = 3.12291\n",
      "epoch no.4 train no.328260  loss = 4.07307 avg_loss = 3.13513\n",
      "epoch no.4 train no.328270  loss = 4.76399 avg_loss = 3.15025\n",
      "epoch no.4 train no.328280  loss = 2.80785 avg_loss = 3.12532\n",
      "epoch no.4 train no.328290  loss = 2.92682 avg_loss = 3.11644\n",
      "epoch no.4 train no.328300  loss = 2.97254 avg_loss = 3.10482\n",
      "epoch no.4 train no.328310  loss = 3.64260 avg_loss = 3.10170\n",
      "epoch no.4 train no.328320  loss = 3.55504 avg_loss = 3.10643\n",
      "epoch no.4 train no.328330  loss = 5.79748 avg_loss = 3.14631\n",
      "epoch no.4 train no.328340  loss = 3.28484 avg_loss = 3.21303\n",
      "epoch no.4 train no.328350  loss = 2.50567 avg_loss = 3.17710\n",
      "epoch no.4 train no.328360  loss = 3.24357 avg_loss = 3.14101\n",
      "epoch no.4 train no.328370  loss = 2.65884 avg_loss = 3.13253\n",
      "epoch no.4 train no.328380  loss = 4.33146 avg_loss = 3.12108\n",
      "epoch no.4 train no.328390  loss = 1.77847 avg_loss = 3.13266\n",
      "epoch no.4 train no.328400  loss = 3.45868 avg_loss = 3.10642\n",
      "epoch no.4 train no.328410  loss = 2.65259 avg_loss = 3.14618\n",
      "epoch no.4 train no.328420  loss = 3.81646 avg_loss = 3.16158\n",
      "epoch no.4 train no.328430  loss = 2.80362 avg_loss = 3.14700\n",
      "epoch no.4 train no.328440  loss = 3.59883 avg_loss = 3.15286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.328450  loss = 2.10584 avg_loss = 3.14960\n",
      "epoch no.4 train no.328460  loss = 3.66501 avg_loss = 3.16408\n",
      "epoch no.4 train no.328470  loss = 3.00068 avg_loss = 3.15000\n",
      "epoch no.4 train no.328480  loss = 3.15373 avg_loss = 3.12199\n",
      "epoch no.4 train no.328490  loss = 4.48281 avg_loss = 3.12456\n",
      "epoch no.4 train no.328500  loss = 2.68976 avg_loss = 3.10790\n",
      "epoch no.4 train no.328510  loss = 1.94045 avg_loss = 3.11941\n",
      "epoch no.4 train no.328520  loss = 1.83355 avg_loss = 3.08648\n",
      "epoch no.4 train no.328530  loss = 5.06329 avg_loss = 3.10550\n",
      "epoch no.4 train no.328540  loss = 3.21537 avg_loss = 3.09100\n",
      "epoch no.4 train no.328550  loss = 3.36939 avg_loss = 3.08212\n",
      "epoch no.4 train no.328560  loss = 2.22057 avg_loss = 3.05791\n",
      "epoch no.4 train no.328570  loss = 2.66968 avg_loss = 3.06678\n",
      "epoch no.4 train no.328580  loss = 2.61318 avg_loss = 3.03973\n",
      "epoch no.4 train no.328590  loss = 3.45967 avg_loss = 3.09554\n",
      "epoch no.4 train no.328600  loss = 2.71109 avg_loss = 3.12927\n",
      "epoch no.4 train no.328610  loss = 3.89142 avg_loss = 3.13661\n",
      "epoch no.4 train no.328620  loss = 3.09647 avg_loss = 3.15428\n",
      "epoch no.4 train no.328630  loss = 1.19629 avg_loss = 3.11043\n",
      "epoch no.4 train no.328640  loss = 3.38402 avg_loss = 3.11085\n",
      "epoch no.4 train no.328650  loss = 3.38040 avg_loss = 3.15144\n",
      "epoch no.4 train no.328660  loss = 1.77405 avg_loss = 3.12774\n",
      "epoch no.4 train no.328670  loss = 3.01360 avg_loss = 3.14429\n",
      "epoch no.4 train no.328680  loss = 4.37790 avg_loss = 3.15030\n",
      "epoch no.4 train no.328690  loss = 4.43386 avg_loss = 3.17299\n",
      "epoch no.4 train no.328700  loss = 2.75137 avg_loss = 3.17574\n",
      "epoch no.4 train no.328710  loss = 3.46477 avg_loss = 3.20068\n",
      "epoch no.4 train no.328720  loss = 3.92825 avg_loss = 3.22898\n",
      "epoch no.4 train no.328730  loss = 2.70672 avg_loss = 3.23140\n",
      "epoch no.4 train no.328740  loss = 3.43246 avg_loss = 3.22662\n",
      "epoch no.4 train no.328750  loss = 3.08459 avg_loss = 3.18120\n",
      "epoch no.4 train no.328760  loss = 3.53447 avg_loss = 3.17374\n",
      "epoch no.4 train no.328770  loss = 2.63485 avg_loss = 3.16778\n",
      "epoch no.4 train no.328780  loss = 2.39025 avg_loss = 3.17906\n",
      "epoch no.4 train no.328790  loss = 2.51280 avg_loss = 3.17378\n",
      "epoch no.4 train no.328800  loss = 4.15441 avg_loss = 3.17994\n",
      "epoch no.4 train no.328810  loss = 3.64152 avg_loss = 3.14110\n",
      "epoch no.4 train no.328820  loss = 1.79254 avg_loss = 3.10766\n",
      "epoch no.4 train no.328830  loss = 5.59176 avg_loss = 3.17609\n",
      "epoch no.4 train no.328840  loss = 2.99677 avg_loss = 3.19250\n",
      "epoch no.4 train no.328850  loss = 2.43053 avg_loss = 3.17679\n",
      "epoch no.4 train no.328860  loss = 2.60770 avg_loss = 3.20515\n",
      "epoch no.4 train no.328870  loss = 3.47177 avg_loss = 3.18989\n",
      "epoch no.4 train no.328880  loss = 2.44268 avg_loss = 3.19978\n",
      "epoch no.4 train no.328890  loss = 2.41953 avg_loss = 3.24464\n",
      "epoch no.4 train no.328900  loss = 2.10942 avg_loss = 3.21249\n",
      "epoch no.4 train no.328910  loss = 5.00901 avg_loss = 3.18918\n",
      "epoch no.4 train no.328920  loss = 1.94927 avg_loss = 3.20672\n",
      "epoch no.4 train no.328930  loss = 3.87460 avg_loss = 3.18340\n",
      "epoch no.4 train no.328940  loss = 4.65863 avg_loss = 3.20633\n",
      "epoch no.4 train no.328950  loss = 2.52192 avg_loss = 3.16959\n",
      "epoch no.4 train no.328960  loss = 2.28802 avg_loss = 3.19313\n",
      "epoch no.4 train no.328970  loss = 4.71392 avg_loss = 3.24412\n",
      "epoch no.4 train no.328980  loss = 2.46847 avg_loss = 3.21728\n",
      "epoch no.4 train no.328990  loss = 2.53880 avg_loss = 3.19737\n",
      "epoch no.4 train no.329000  loss = 3.19126 avg_loss = 3.23780\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', '음악', '▁모음', '</s>']\n",
      "추억의 싸이월드 배경음악 모음</s>\n",
      "epoch no.4 train no.329010  loss = 3.42307 avg_loss = 3.21694\n",
      "epoch no.4 train no.329020  loss = 2.26288 avg_loss = 3.22804\n",
      "epoch no.4 train no.329030  loss = 3.24735 avg_loss = 3.26167\n",
      "epoch no.4 train no.329040  loss = 2.70284 avg_loss = 3.21765\n",
      "epoch no.4 train no.329050  loss = 2.48986 avg_loss = 3.23178\n",
      "epoch no.4 train no.329060  loss = 3.03283 avg_loss = 3.21801\n",
      "epoch no.4 train no.329070  loss = 2.77073 avg_loss = 3.19267\n",
      "epoch no.4 train no.329080  loss = 2.95656 avg_loss = 3.20708\n",
      "epoch no.4 train no.329090  loss = 2.82543 avg_loss = 3.23363\n",
      "epoch no.4 train no.329100  loss = 2.39220 avg_loss = 3.22143\n",
      "epoch no.4 train no.329110  loss = 2.19106 avg_loss = 3.19830\n",
      "epoch no.4 train no.329120  loss = 2.45735 avg_loss = 3.26575\n",
      "epoch no.4 train no.329130  loss = 4.45714 avg_loss = 3.29216\n",
      "epoch no.4 train no.329140  loss = 3.07079 avg_loss = 3.23974\n",
      "epoch no.4 train no.329150  loss = 3.83213 avg_loss = 3.25012\n",
      "epoch no.4 train no.329160  loss = 2.91350 avg_loss = 3.23003\n",
      "epoch no.4 train no.329170  loss = 2.44710 avg_loss = 3.22231\n",
      "epoch no.4 train no.329180  loss = 2.32916 avg_loss = 3.17523\n",
      "epoch no.4 train no.329190  loss = 4.78334 avg_loss = 3.19518\n",
      "epoch no.4 train no.329200  loss = 1.91784 avg_loss = 3.17624\n",
      "epoch no.4 train no.329210  loss = 3.41003 avg_loss = 3.15409\n",
      "epoch no.4 train no.329220  loss = 1.43495 avg_loss = 3.13469\n",
      "epoch no.4 train no.329230  loss = 4.10537 avg_loss = 3.18268\n",
      "epoch no.4 train no.329240  loss = 2.83680 avg_loss = 3.18245\n",
      "epoch no.4 train no.329250  loss = 2.28329 avg_loss = 3.16263\n",
      "epoch no.4 train no.329260  loss = 2.37541 avg_loss = 3.12205\n",
      "epoch no.4 train no.329270  loss = 2.36730 avg_loss = 3.14493\n",
      "epoch no.4 train no.329280  loss = 4.18204 avg_loss = 3.15914\n",
      "epoch no.4 train no.329290  loss = 3.05664 avg_loss = 3.13250\n",
      "epoch no.4 train no.329300  loss = 3.35775 avg_loss = 3.13431\n",
      "epoch no.4 train no.329310  loss = 3.82003 avg_loss = 3.11389\n",
      "epoch no.4 train no.329320  loss = 4.11506 avg_loss = 3.11797\n",
      "epoch no.4 train no.329330  loss = 4.80636 avg_loss = 3.14176\n",
      "epoch no.4 train no.329340  loss = 3.72404 avg_loss = 3.16709\n",
      "epoch no.4 train no.329350  loss = 4.26034 avg_loss = 3.15133\n",
      "epoch no.4 train no.329360  loss = 3.93992 avg_loss = 3.17052\n",
      "epoch no.4 train no.329370  loss = 3.59190 avg_loss = 3.19588\n",
      "epoch no.4 train no.329380  loss = 3.62770 avg_loss = 3.20772\n",
      "epoch no.4 train no.329390  loss = 3.88166 avg_loss = 3.19767\n",
      "epoch no.4 train no.329400  loss = 3.66789 avg_loss = 3.20785\n",
      "epoch no.4 train no.329410  loss = 3.15228 avg_loss = 3.18638\n",
      "epoch no.4 train no.329420  loss = 2.13059 avg_loss = 3.16762\n",
      "epoch no.4 train no.329430  loss = 2.71119 avg_loss = 3.11924\n",
      "epoch no.4 train no.329440  loss = 3.82709 avg_loss = 3.12526\n",
      "epoch no.4 train no.329450  loss = 2.82527 avg_loss = 3.13664\n",
      "epoch no.4 train no.329460  loss = 2.91592 avg_loss = 3.14788\n",
      "epoch no.4 train no.329470  loss = 2.92655 avg_loss = 3.11235\n",
      "epoch no.4 train no.329480  loss = 1.90389 avg_loss = 3.09677\n",
      "epoch no.4 train no.329490  loss = 2.33328 avg_loss = 3.11268\n",
      "epoch no.4 train no.329500  loss = 3.95010 avg_loss = 3.09348\n",
      "epoch no.4 train no.329510  loss = 3.50273 avg_loss = 3.06881\n",
      "epoch no.4 train no.329520  loss = 4.83582 avg_loss = 3.07577\n",
      "epoch no.4 train no.329530  loss = 2.19114 avg_loss = 3.09979\n",
      "epoch no.4 train no.329540  loss = 4.30439 avg_loss = 3.11596\n",
      "epoch no.4 train no.329550  loss = 3.35334 avg_loss = 3.13293\n",
      "epoch no.4 train no.329560  loss = 3.51978 avg_loss = 3.11327\n",
      "epoch no.4 train no.329570  loss = 2.96659 avg_loss = 3.13610\n",
      "epoch no.4 train no.329580  loss = 4.72694 avg_loss = 3.15850\n",
      "epoch no.4 train no.329590  loss = 3.48017 avg_loss = 3.15241\n",
      "epoch no.4 train no.329600  loss = 4.39413 avg_loss = 3.19560\n",
      "epoch no.4 train no.329610  loss = 4.11417 avg_loss = 3.24496\n",
      "epoch no.4 train no.329620  loss = 2.37929 avg_loss = 3.22936\n",
      "epoch no.4 train no.329630  loss = 3.11961 avg_loss = 3.24337\n",
      "epoch no.4 train no.329640  loss = 1.99328 avg_loss = 3.19069\n",
      "epoch no.4 train no.329650  loss = 5.41834 avg_loss = 3.21714\n",
      "epoch no.4 train no.329660  loss = 3.00701 avg_loss = 3.18903\n",
      "epoch no.4 train no.329670  loss = 3.46895 avg_loss = 3.17344\n",
      "epoch no.4 train no.329680  loss = 5.13551 avg_loss = 3.20605\n",
      "epoch no.4 train no.329690  loss = 2.95517 avg_loss = 3.23782\n",
      "epoch no.4 train no.329700  loss = 3.98449 avg_loss = 3.20954\n",
      "epoch no.4 train no.329710  loss = 2.26797 avg_loss = 3.16140\n",
      "epoch no.4 train no.329720  loss = 2.88758 avg_loss = 3.17005\n",
      "epoch no.4 train no.329730  loss = 2.61465 avg_loss = 3.17001\n",
      "epoch no.4 train no.329740  loss = 2.97094 avg_loss = 3.16760\n",
      "epoch no.4 train no.329750  loss = 2.82238 avg_loss = 3.13561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.329760  loss = 2.97441 avg_loss = 3.13079\n",
      "epoch no.4 train no.329770  loss = 1.93925 avg_loss = 3.10631\n",
      "epoch no.4 train no.329780  loss = 2.80463 avg_loss = 3.13022\n",
      "epoch no.4 train no.329790  loss = 3.97497 avg_loss = 3.12899\n",
      "epoch no.4 train no.329800  loss = 2.65770 avg_loss = 3.12989\n",
      "epoch no.4 train no.329810  loss = 3.47311 avg_loss = 3.13548\n",
      "epoch no.4 train no.329820  loss = 3.10542 avg_loss = 3.11750\n",
      "epoch no.4 train no.329830  loss = 3.42670 avg_loss = 3.10986\n",
      "epoch no.4 train no.329840  loss = 1.91250 avg_loss = 3.10610\n",
      "epoch no.4 train no.329850  loss = 1.76375 avg_loss = 3.15066\n",
      "epoch no.4 train no.329860  loss = 4.53810 avg_loss = 3.12403\n",
      "epoch no.4 train no.329870  loss = 3.64455 avg_loss = 3.15006\n",
      "epoch no.4 train no.329880  loss = 2.45783 avg_loss = 3.17689\n",
      "epoch no.4 train no.329890  loss = 1.78850 avg_loss = 3.17107\n",
      "epoch no.4 train no.329900  loss = 4.15847 avg_loss = 3.16460\n",
      "epoch no.4 train no.329910  loss = 3.37409 avg_loss = 3.14841\n",
      "epoch no.4 train no.329920  loss = 3.42784 avg_loss = 3.12306\n",
      "epoch no.4 train no.329930  loss = 3.72203 avg_loss = 3.13264\n",
      "epoch no.4 train no.329940  loss = 3.62675 avg_loss = 3.19337\n",
      "epoch no.4 train no.329950  loss = 3.42501 avg_loss = 3.20240\n",
      "epoch no.4 train no.329960  loss = 3.42982 avg_loss = 3.20507\n",
      "epoch no.4 train no.329970  loss = 2.18662 avg_loss = 3.20833\n",
      "epoch no.4 train no.329980  loss = 3.20055 avg_loss = 3.22236\n",
      "epoch no.4 train no.329990  loss = 3.03668 avg_loss = 3.18672\n",
      "epoch no.4 train no.330000  loss = 2.28171 avg_loss = 3.14947\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁팝', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.4 train no.330010  loss = 2.04444 avg_loss = 3.12787\n",
      "epoch no.4 train no.330020  loss = 3.45660 avg_loss = 3.08452\n",
      "epoch no.4 train no.330030  loss = 2.85135 avg_loss = 3.08700\n",
      "epoch no.4 train no.330040  loss = 2.34428 avg_loss = 3.07379\n",
      "epoch no.4 train no.330050  loss = 3.92286 avg_loss = 3.08258\n",
      "epoch no.4 train no.330060  loss = 3.95555 avg_loss = 3.06671\n",
      "epoch no.4 train no.330070  loss = 2.20064 avg_loss = 3.01786\n",
      "epoch no.4 train no.330080  loss = 3.74541 avg_loss = 3.02404\n",
      "epoch no.4 train no.330090  loss = 4.48348 avg_loss = 3.07564\n",
      "epoch no.4 train no.330100  loss = 5.14328 avg_loss = 3.13085\n",
      "epoch no.4 train no.330110  loss = 2.98685 avg_loss = 3.14079\n",
      "epoch no.4 train no.330120  loss = 2.23044 avg_loss = 3.11486\n",
      "epoch no.4 train no.330130  loss = 5.95411 avg_loss = 3.19474\n",
      "epoch no.4 train no.330140  loss = 1.51997 avg_loss = 3.14505\n",
      "epoch no.4 train no.330150  loss = 2.97719 avg_loss = 3.15780\n",
      "epoch no.4 train no.330160  loss = 3.88157 avg_loss = 3.15491\n",
      "epoch no.4 train no.330170  loss = 2.59141 avg_loss = 3.16706\n",
      "epoch no.4 train no.330180  loss = 2.57794 avg_loss = 3.16412\n",
      "epoch no.4 train no.330190  loss = 2.34563 avg_loss = 3.11601\n",
      "epoch no.4 train no.330200  loss = 3.69892 avg_loss = 3.16801\n",
      "epoch no.4 train no.330210  loss = 5.49217 avg_loss = 3.17691\n",
      "epoch no.4 train no.330220  loss = 2.85959 avg_loss = 3.18574\n",
      "epoch no.4 train no.330230  loss = 3.86398 avg_loss = 3.17875\n",
      "epoch no.4 train no.330240  loss = 2.88711 avg_loss = 3.18173\n",
      "epoch no.4 train no.330250  loss = 3.14403 avg_loss = 3.20043\n",
      "epoch no.4 train no.330260  loss = 3.66137 avg_loss = 3.20198\n",
      "epoch no.4 train no.330270  loss = 6.39042 avg_loss = 3.19433\n",
      "epoch no.4 train no.330280  loss = 2.48061 avg_loss = 3.22776\n",
      "epoch no.4 train no.330290  loss = 2.31100 avg_loss = 3.22969\n",
      "epoch no.4 train no.330300  loss = 3.08778 avg_loss = 3.19604\n",
      "epoch no.4 train no.330310  loss = 3.42571 avg_loss = 3.24984\n",
      "epoch no.4 train no.330320  loss = 3.98151 avg_loss = 3.29784\n",
      "epoch no.4 train no.330330  loss = 3.06717 avg_loss = 3.25823\n",
      "epoch no.4 train no.330340  loss = 2.85513 avg_loss = 3.25554\n",
      "epoch no.4 train no.330350  loss = 2.11266 avg_loss = 3.23122\n",
      "epoch no.4 train no.330360  loss = 2.88009 avg_loss = 3.25369\n",
      "epoch no.4 train no.330370  loss = 4.54932 avg_loss = 3.26318\n",
      "epoch no.4 train no.330380  loss = 1.87221 avg_loss = 3.19532\n",
      "epoch no.4 train no.330390  loss = 2.97479 avg_loss = 3.20162\n",
      "epoch no.4 train no.330400  loss = 5.04970 avg_loss = 3.21785\n",
      "epoch no.4 train no.330410  loss = 3.11143 avg_loss = 3.25050\n",
      "epoch no.4 train no.330420  loss = 3.44004 avg_loss = 3.23204\n",
      "epoch no.4 train no.330430  loss = 2.89412 avg_loss = 3.21869\n",
      "epoch no.4 train no.330440  loss = 2.07942 avg_loss = 3.20230\n",
      "epoch no.4 train no.330450  loss = 3.64513 avg_loss = 3.15981\n",
      "epoch no.4 train no.330460  loss = 2.71277 avg_loss = 3.12719\n",
      "epoch no.4 train no.330470  loss = 2.89958 avg_loss = 3.10698\n",
      "epoch no.4 train no.330480  loss = 2.52889 avg_loss = 3.09368\n",
      "epoch no.4 train no.330490  loss = 3.21770 avg_loss = 3.08999\n",
      "epoch no.4 train no.330500  loss = 2.03700 avg_loss = 3.03756\n",
      "epoch no.4 train no.330510  loss = 2.57830 avg_loss = 3.07295\n",
      "epoch no.4 train no.330520  loss = 2.98518 avg_loss = 3.12366\n",
      "epoch no.4 train no.330530  loss = 3.84053 avg_loss = 3.15605\n",
      "epoch no.4 train no.330540  loss = 4.67315 avg_loss = 3.18081\n",
      "epoch no.4 train no.330550  loss = 2.15579 avg_loss = 3.14732\n",
      "epoch no.4 train no.330560  loss = 2.24601 avg_loss = 3.13354\n",
      "epoch no.4 train no.330570  loss = 3.32862 avg_loss = 3.12059\n",
      "epoch no.4 train no.330580  loss = 4.62426 avg_loss = 3.17914\n",
      "epoch no.4 train no.330590  loss = 1.53937 avg_loss = 3.13773\n",
      "epoch no.4 train no.330600  loss = 2.13666 avg_loss = 3.09876\n",
      "epoch no.4 train no.330610  loss = 3.63793 avg_loss = 3.14059\n",
      "epoch no.4 train no.330620  loss = 2.44165 avg_loss = 3.14577\n",
      "epoch no.4 train no.330630  loss = 2.12155 avg_loss = 3.14690\n",
      "epoch no.4 train no.330640  loss = 2.04959 avg_loss = 3.14810\n",
      "epoch no.4 train no.330650  loss = 2.99522 avg_loss = 3.15237\n",
      "epoch no.4 train no.330660  loss = 3.74506 avg_loss = 3.16583\n",
      "epoch no.4 train no.330670  loss = 3.33671 avg_loss = 3.14256\n",
      "epoch no.4 train no.330680  loss = 3.67579 avg_loss = 3.14751\n",
      "epoch no.4 train no.330690  loss = 3.32486 avg_loss = 3.12250\n",
      "epoch no.4 train no.330700  loss = 1.97426 avg_loss = 3.14109\n",
      "epoch no.4 train no.330710  loss = 2.18994 avg_loss = 3.15488\n",
      "epoch no.4 train no.330720  loss = 2.73757 avg_loss = 3.17128\n",
      "epoch no.4 train no.330730  loss = 4.54920 avg_loss = 3.18166\n",
      "epoch no.4 train no.330740  loss = 2.99548 avg_loss = 3.15552\n",
      "epoch no.4 train no.330750  loss = 3.03954 avg_loss = 3.15139\n",
      "epoch no.4 train no.330760  loss = 4.29580 avg_loss = 3.18103\n",
      "epoch no.4 train no.330770  loss = 3.65152 avg_loss = 3.19579\n",
      "epoch no.4 train no.330780  loss = 3.30671 avg_loss = 3.23141\n",
      "epoch no.4 train no.330790  loss = 3.09430 avg_loss = 3.23162\n",
      "epoch no.4 train no.330800  loss = 1.72221 avg_loss = 3.19245\n",
      "epoch no.4 train no.330810  loss = 2.05271 avg_loss = 3.21162\n",
      "epoch no.4 train no.330820  loss = 3.27705 avg_loss = 3.18571\n",
      "epoch no.4 train no.330830  loss = 1.93109 avg_loss = 3.20496\n",
      "epoch no.4 train no.330840  loss = 3.18886 avg_loss = 3.19071\n",
      "epoch no.4 train no.330850  loss = 3.15648 avg_loss = 3.17867\n",
      "epoch no.4 train no.330860  loss = 3.25391 avg_loss = 3.20455\n",
      "epoch no.4 train no.330870  loss = 2.57500 avg_loss = 3.16737\n",
      "epoch no.4 train no.330880  loss = 2.00554 avg_loss = 3.16253\n",
      "epoch no.4 train no.330890  loss = 3.96309 avg_loss = 3.15711\n",
      "epoch no.4 train no.330900  loss = 4.21994 avg_loss = 3.15905\n",
      "epoch no.4 train no.330910  loss = 4.40705 avg_loss = 3.21314\n",
      "epoch no.4 train no.330920  loss = 2.71258 avg_loss = 3.22531\n",
      "epoch no.4 train no.330930  loss = 3.80742 avg_loss = 3.27117\n",
      "epoch no.4 train no.330940  loss = 3.39987 avg_loss = 3.28586\n",
      "epoch no.4 train no.330950  loss = 3.21816 avg_loss = 3.28198\n",
      "epoch no.4 train no.330960  loss = 2.45838 avg_loss = 3.27791\n",
      "epoch no.4 train no.330970  loss = 3.18347 avg_loss = 3.26395\n",
      "epoch no.4 train no.330980  loss = 3.55887 avg_loss = 3.22597\n",
      "epoch no.4 train no.330990  loss = 3.70748 avg_loss = 3.24046\n",
      "epoch no.4 train no.331000  loss = 4.06690 avg_loss = 3.20247\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '들', '▁그', '년대', '▁추억', '</s>']\n",
      "추억의 노래와 90년대 가요</s>\n",
      "epoch no.4 train no.331010  loss = 3.55806 avg_loss = 3.20200\n",
      "epoch no.4 train no.331020  loss = 3.77803 avg_loss = 3.19829\n",
      "epoch no.4 train no.331030  loss = 2.91602 avg_loss = 3.17752\n",
      "epoch no.4 train no.331040  loss = 4.27902 avg_loss = 3.13092\n",
      "epoch no.4 train no.331050  loss = 3.36278 avg_loss = 3.15429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.331060  loss = 5.04739 avg_loss = 3.18494\n",
      "epoch no.4 train no.331070  loss = 2.57078 avg_loss = 3.12802\n",
      "epoch no.4 train no.331080  loss = 1.51975 avg_loss = 3.10287\n",
      "epoch no.4 train no.331090  loss = 2.83849 avg_loss = 3.11357\n",
      "epoch no.4 train no.331100  loss = 2.65419 avg_loss = 3.13914\n",
      "epoch no.4 train no.331110  loss = 3.11118 avg_loss = 3.11817\n",
      "epoch no.4 train no.331120  loss = 3.06466 avg_loss = 3.14176\n",
      "epoch no.4 train no.331130  loss = 2.67515 avg_loss = 3.18410\n",
      "epoch no.4 train no.331140  loss = 2.90965 avg_loss = 3.21046\n",
      "epoch no.4 train no.331150  loss = 2.55894 avg_loss = 3.23841\n",
      "epoch no.4 train no.331160  loss = 3.71141 avg_loss = 3.23940\n",
      "epoch no.4 train no.331170  loss = 3.67496 avg_loss = 3.21547\n",
      "epoch no.4 train no.331180  loss = 3.74667 avg_loss = 3.26844\n",
      "epoch no.4 train no.331190  loss = 3.42744 avg_loss = 3.31122\n",
      "epoch no.4 train no.331200  loss = 3.16185 avg_loss = 3.29211\n",
      "epoch no.4 train no.331210  loss = 3.70061 avg_loss = 3.33659\n",
      "epoch no.4 train no.331220  loss = 2.40486 avg_loss = 3.32931\n",
      "epoch no.4 train no.331230  loss = 1.80151 avg_loss = 3.32615\n",
      "epoch no.4 train no.331240  loss = 2.86738 avg_loss = 3.28758\n",
      "epoch no.4 train no.331250  loss = 2.52376 avg_loss = 3.31336\n",
      "epoch no.4 train no.331260  loss = 1.70441 avg_loss = 3.31514\n",
      "epoch no.4 train no.331270  loss = 3.56517 avg_loss = 3.30134\n",
      "epoch no.4 train no.331280  loss = 3.93146 avg_loss = 3.27505\n",
      "epoch no.4 train no.331290  loss = 2.62633 avg_loss = 3.26769\n",
      "epoch no.4 train no.331300  loss = 2.40789 avg_loss = 3.24168\n",
      "epoch no.4 train no.331310  loss = 3.04917 avg_loss = 3.23351\n",
      "epoch no.4 train no.331320  loss = 1.91121 avg_loss = 3.24748\n",
      "epoch no.4 train no.331330  loss = 2.67913 avg_loss = 3.24662\n",
      "epoch no.4 train no.331340  loss = 3.38317 avg_loss = 3.24692\n",
      "epoch no.4 train no.331350  loss = 3.39193 avg_loss = 3.24283\n",
      "epoch no.4 train no.331360  loss = 3.14578 avg_loss = 3.25235\n",
      "epoch no.4 train no.331370  loss = 3.60937 avg_loss = 3.23926\n",
      "epoch no.4 train no.331380  loss = 2.50222 avg_loss = 3.18253\n",
      "epoch no.4 train no.331390  loss = 2.02157 avg_loss = 3.14552\n",
      "epoch no.4 train no.331400  loss = 2.63143 avg_loss = 3.14498\n",
      "epoch no.4 train no.331410  loss = 2.43926 avg_loss = 3.15935\n",
      "epoch no.4 train no.331420  loss = 3.23866 avg_loss = 3.17127\n",
      "epoch no.4 train no.331430  loss = 4.68161 avg_loss = 3.16712\n",
      "epoch no.4 train no.331440  loss = 4.31586 avg_loss = 3.18983\n",
      "epoch no.4 train no.331450  loss = 4.00899 avg_loss = 3.16767\n",
      "epoch no.4 train no.331460  loss = 3.74658 avg_loss = 3.20580\n",
      "epoch no.4 train no.331470  loss = 3.46800 avg_loss = 3.18480\n",
      "epoch no.4 train no.331480  loss = 3.75036 avg_loss = 3.20495\n",
      "epoch no.4 train no.331490  loss = 2.67997 avg_loss = 3.17307\n",
      "epoch no.4 train no.331500  loss = 4.07652 avg_loss = 3.18843\n",
      "epoch no.4 train no.331510  loss = 2.97359 avg_loss = 3.21139\n",
      "epoch no.4 train no.331520  loss = 4.26039 avg_loss = 3.19597\n",
      "epoch no.4 train no.331530  loss = 2.40962 avg_loss = 3.17623\n",
      "epoch no.4 train no.331540  loss = 3.43653 avg_loss = 3.17811\n",
      "epoch no.4 train no.331550  loss = 1.92097 avg_loss = 3.13957\n",
      "epoch no.4 train no.331560  loss = 2.49459 avg_loss = 3.16581\n",
      "epoch no.4 train no.331570  loss = 6.75030 avg_loss = 3.18882\n",
      "epoch no.4 train no.331580  loss = 2.03316 avg_loss = 3.16987\n",
      "epoch no.4 train no.331590  loss = 2.89139 avg_loss = 3.17423\n",
      "epoch no.4 train no.331600  loss = 3.29921 avg_loss = 3.19099\n",
      "epoch no.4 train no.331610  loss = 2.43997 avg_loss = 3.16974\n",
      "epoch no.4 train no.331620  loss = 3.51017 avg_loss = 3.15892\n",
      "epoch no.4 train no.331630  loss = 2.76469 avg_loss = 3.13489\n",
      "epoch no.4 train no.331640  loss = 1.40490 avg_loss = 3.16206\n",
      "epoch no.4 train no.331650  loss = 2.55044 avg_loss = 3.10946\n",
      "epoch no.4 train no.331660  loss = 3.01543 avg_loss = 3.11300\n",
      "epoch no.4 train no.331670  loss = 4.07293 avg_loss = 3.10378\n",
      "epoch no.4 train no.331680  loss = 2.52049 avg_loss = 3.09920\n",
      "epoch no.4 train no.331690  loss = 2.49779 avg_loss = 3.05084\n",
      "epoch no.4 train no.331700  loss = 2.53747 avg_loss = 3.02577\n",
      "epoch no.4 train no.331710  loss = 2.46232 avg_loss = 3.03223\n",
      "epoch no.4 train no.331720  loss = 3.15053 avg_loss = 3.02215\n",
      "epoch no.4 train no.331730  loss = 2.19572 avg_loss = 3.04997\n",
      "epoch no.4 train no.331740  loss = 3.73079 avg_loss = 3.05071\n",
      "epoch no.4 train no.331750  loss = 2.63698 avg_loss = 3.05546\n",
      "epoch no.4 train no.331760  loss = 3.63285 avg_loss = 3.05547\n",
      "epoch no.4 train no.331770  loss = 3.26084 avg_loss = 3.13539\n",
      "epoch no.4 train no.331780  loss = 4.11892 avg_loss = 3.12468\n",
      "epoch no.4 train no.331790  loss = 4.14632 avg_loss = 3.15164\n",
      "epoch no.4 train no.331800  loss = 2.14605 avg_loss = 3.11129\n",
      "epoch no.4 train no.331810  loss = 4.21121 avg_loss = 3.10215\n",
      "epoch no.4 train no.331820  loss = 5.28879 avg_loss = 3.16050\n",
      "epoch no.4 train no.331830  loss = 2.14007 avg_loss = 3.15921\n",
      "epoch no.4 train no.331840  loss = 2.82237 avg_loss = 3.14200\n",
      "epoch no.4 train no.331850  loss = 4.06694 avg_loss = 3.14571\n",
      "epoch no.4 train no.331860  loss = 4.05016 avg_loss = 3.17934\n",
      "epoch no.4 train no.331870  loss = 3.70334 avg_loss = 3.17935\n",
      "epoch no.4 train no.331880  loss = 2.67774 avg_loss = 3.20932\n",
      "epoch no.4 train no.331890  loss = 1.51886 avg_loss = 3.21105\n",
      "epoch no.4 train no.331900  loss = 3.22237 avg_loss = 3.25953\n",
      "epoch no.4 train no.331910  loss = 3.03396 avg_loss = 3.21741\n",
      "epoch no.4 train no.331920  loss = 4.22181 avg_loss = 3.26319\n",
      "epoch no.4 train no.331930  loss = 3.46330 avg_loss = 3.23526\n",
      "epoch no.4 train no.331940  loss = 3.33349 avg_loss = 3.28013\n",
      "epoch no.4 train no.331950  loss = 2.48438 avg_loss = 3.29613\n",
      "epoch no.4 train no.331960  loss = 1.85825 avg_loss = 3.24454\n",
      "epoch no.4 train no.331970  loss = 3.68522 avg_loss = 3.21104\n",
      "epoch no.4 train no.331980  loss = 2.93784 avg_loss = 3.19212\n",
      "epoch no.4 train no.331990  loss = 3.81035 avg_loss = 3.21561\n",
      "epoch no.4 train no.332000  loss = 4.31806 avg_loss = 3.19483\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '송', '</s>']\n",
      "추억의 90년대 팝송</s>\n",
      "epoch no.4 train no.332010  loss = 2.84079 avg_loss = 3.19592\n",
      "epoch no.4 train no.332020  loss = 3.69445 avg_loss = 3.19794\n",
      "epoch no.4 train no.332030  loss = 4.43735 avg_loss = 3.20313\n",
      "epoch no.4 train no.332040  loss = 2.88180 avg_loss = 3.19023\n",
      "epoch no.4 train no.332050  loss = 4.00021 avg_loss = 3.20301\n",
      "epoch no.4 train no.332060  loss = 1.57836 avg_loss = 3.20500\n",
      "epoch no.4 train no.332070  loss = 3.28953 avg_loss = 3.20862\n",
      "epoch no.4 train no.332080  loss = 3.19428 avg_loss = 3.15269\n",
      "epoch no.4 train no.332090  loss = 1.54074 avg_loss = 3.11985\n",
      "epoch no.4 train no.332100  loss = 2.88966 avg_loss = 3.11034\n",
      "epoch no.4 train no.332110  loss = 3.35745 avg_loss = 3.11546\n",
      "epoch no.4 train no.332120  loss = 3.16321 avg_loss = 3.12770\n",
      "epoch no.4 train no.332130  loss = 1.70068 avg_loss = 3.09287\n",
      "epoch no.4 train no.332140  loss = 3.12248 avg_loss = 3.11905\n",
      "epoch no.4 train no.332150  loss = 4.06185 avg_loss = 3.13488\n",
      "epoch no.4 train no.332160  loss = 1.93174 avg_loss = 3.14806\n",
      "epoch no.4 train no.332170  loss = 3.66861 avg_loss = 3.16398\n",
      "epoch no.4 train no.332180  loss = 2.06441 avg_loss = 3.16996\n",
      "epoch no.4 train no.332190  loss = 3.20779 avg_loss = 3.15327\n",
      "epoch no.4 train no.332200  loss = 3.74681 avg_loss = 3.20489\n",
      "epoch no.4 train no.332210  loss = 2.67125 avg_loss = 3.18108\n",
      "epoch no.4 train no.332220  loss = 2.53161 avg_loss = 3.15065\n",
      "epoch no.4 train no.332230  loss = 4.93119 avg_loss = 3.16596\n",
      "epoch no.4 train no.332240  loss = 3.91577 avg_loss = 3.21714\n",
      "epoch no.4 train no.332250  loss = 2.47368 avg_loss = 3.26446\n",
      "epoch no.4 train no.332260  loss = 4.13214 avg_loss = 3.29906\n",
      "epoch no.4 train no.332270  loss = 3.04372 avg_loss = 3.25734\n",
      "epoch no.4 train no.332280  loss = 1.95450 avg_loss = 3.25409\n",
      "epoch no.4 train no.332290  loss = 2.55839 avg_loss = 3.26610\n",
      "epoch no.4 train no.332300  loss = 2.79894 avg_loss = 3.22390\n",
      "epoch no.4 train no.332310  loss = 3.12418 avg_loss = 3.19169\n",
      "epoch no.4 train no.332320  loss = 1.89494 avg_loss = 3.17566\n",
      "epoch no.4 train no.332330  loss = 2.20534 avg_loss = 3.13843\n",
      "epoch no.4 train no.332340  loss = 2.60519 avg_loss = 3.16122\n",
      "epoch no.4 train no.332350  loss = 2.89811 avg_loss = 3.16393\n",
      "epoch no.4 train no.332360  loss = 2.46988 avg_loss = 3.13640\n",
      "epoch no.4 train no.332370  loss = 3.29301 avg_loss = 3.11620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.332380  loss = 3.58693 avg_loss = 3.11067\n",
      "epoch no.4 train no.332390  loss = 3.62444 avg_loss = 3.08994\n",
      "epoch no.4 train no.332400  loss = 3.87456 avg_loss = 3.08742\n",
      "epoch no.4 train no.332410  loss = 3.21388 avg_loss = 3.12174\n",
      "epoch no.4 train no.332420  loss = 3.59992 avg_loss = 3.15322\n",
      "epoch no.4 train no.332430  loss = 3.67286 avg_loss = 3.18878\n",
      "epoch no.4 train no.332440  loss = 4.36358 avg_loss = 3.18576\n",
      "epoch no.4 train no.332450  loss = 2.97373 avg_loss = 3.17291\n",
      "epoch no.4 train no.332460  loss = 2.88364 avg_loss = 3.14464\n",
      "epoch no.4 train no.332470  loss = 2.74795 avg_loss = 3.15212\n",
      "epoch no.4 train no.332480  loss = 3.63875 avg_loss = 3.12431\n",
      "epoch no.4 train no.332490  loss = 3.40497 avg_loss = 3.13924\n",
      "epoch no.4 train no.332500  loss = 2.21743 avg_loss = 3.12473\n",
      "epoch no.4 train no.332510  loss = 3.60986 avg_loss = 3.15090\n",
      "epoch no.4 train no.332520  loss = 2.87499 avg_loss = 3.20167\n",
      "epoch no.4 train no.332530  loss = 4.19336 avg_loss = 3.19889\n",
      "epoch no.4 train no.332540  loss = 3.38506 avg_loss = 3.20131\n",
      "epoch no.4 train no.332550  loss = 4.86633 avg_loss = 3.22444\n",
      "epoch no.4 train no.332560  loss = 3.89556 avg_loss = 3.21001\n",
      "epoch no.4 train no.332570  loss = 2.63325 avg_loss = 3.17515\n",
      "epoch no.4 train no.332580  loss = 2.41569 avg_loss = 3.15359\n",
      "epoch no.4 train no.332590  loss = 2.77638 avg_loss = 3.12235\n",
      "epoch no.4 train no.332600  loss = 4.68657 avg_loss = 3.14205\n",
      "epoch no.4 train no.332610  loss = 3.07996 avg_loss = 3.11672\n",
      "epoch no.4 train no.332620  loss = 3.12234 avg_loss = 3.09800\n",
      "epoch no.4 train no.332630  loss = 2.52203 avg_loss = 3.12941\n",
      "epoch no.4 train no.332640  loss = 1.66979 avg_loss = 3.14269\n",
      "epoch no.4 train no.332650  loss = 3.76104 avg_loss = 3.17534\n",
      "epoch no.4 train no.332660  loss = 3.74682 avg_loss = 3.12446\n",
      "epoch no.4 train no.332670  loss = 2.30638 avg_loss = 3.18627\n",
      "epoch no.4 train no.332680  loss = 4.71144 avg_loss = 3.16693\n",
      "epoch no.4 train no.332690  loss = 1.42873 avg_loss = 3.18854\n",
      "epoch no.4 train no.332700  loss = 3.77163 avg_loss = 3.19561\n",
      "epoch no.4 train no.332710  loss = 2.41028 avg_loss = 3.17971\n",
      "epoch no.4 train no.332720  loss = 2.50864 avg_loss = 3.15360\n",
      "epoch no.4 train no.332730  loss = 3.66360 avg_loss = 3.17065\n",
      "epoch no.4 train no.332740  loss = 3.85082 avg_loss = 3.16119\n",
      "epoch no.4 train no.332750  loss = 3.89844 avg_loss = 3.16890\n",
      "epoch no.4 train no.332760  loss = 2.18972 avg_loss = 3.18278\n",
      "epoch no.4 train no.332770  loss = 3.88737 avg_loss = 3.20741\n",
      "epoch no.4 train no.332780  loss = 3.32660 avg_loss = 3.20317\n",
      "epoch no.4 train no.332790  loss = 6.20259 avg_loss = 3.24714\n",
      "epoch no.4 train no.332800  loss = 3.03676 avg_loss = 3.21364\n",
      "epoch no.4 train no.332810  loss = 2.20669 avg_loss = 3.18420\n",
      "epoch no.4 train no.332820  loss = 1.90820 avg_loss = 3.16108\n",
      "epoch no.4 train no.332830  loss = 2.49298 avg_loss = 3.11741\n",
      "epoch no.4 train no.332840  loss = 4.57822 avg_loss = 3.17895\n",
      "epoch no.4 train no.332850  loss = 2.34012 avg_loss = 3.16440\n",
      "epoch no.4 train no.332860  loss = 2.54351 avg_loss = 3.19829\n",
      "epoch no.4 train no.332870  loss = 3.63729 avg_loss = 3.23549\n",
      "epoch no.4 train no.332880  loss = 2.93344 avg_loss = 3.20766\n",
      "epoch no.4 train no.332890  loss = 2.32321 avg_loss = 3.18396\n",
      "epoch no.4 train no.332900  loss = 2.10185 avg_loss = 3.19767\n",
      "epoch no.4 train no.332910  loss = 2.93723 avg_loss = 3.15659\n",
      "epoch no.4 train no.332920  loss = 4.95471 avg_loss = 3.14126\n",
      "epoch no.4 train no.332930  loss = 3.41457 avg_loss = 3.15153\n",
      "epoch no.4 train no.332940  loss = 3.05568 avg_loss = 3.14985\n",
      "epoch no.4 train no.332950  loss = 2.98483 avg_loss = 3.18211\n",
      "epoch no.4 train no.332960  loss = 3.74119 avg_loss = 3.17928\n",
      "epoch no.4 train no.332970  loss = 2.78446 avg_loss = 3.18261\n",
      "epoch no.4 train no.332980  loss = 4.94195 avg_loss = 3.21253\n",
      "epoch no.4 train no.332990  loss = 2.60588 avg_loss = 3.16927\n",
      "epoch no.4 train no.333000  loss = 4.31733 avg_loss = 3.17115\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁가요', '▁모음', '</s>']\n",
      "추억의 90년대 노래들</s>\n",
      "epoch no.4 train no.333010  loss = 7.05859 avg_loss = 3.23168\n",
      "epoch no.4 train no.333020  loss = 2.16090 avg_loss = 3.17166\n",
      "epoch no.4 train no.333030  loss = 3.02535 avg_loss = 3.18284\n",
      "epoch no.4 train no.333040  loss = 3.71087 avg_loss = 3.18279\n",
      "epoch no.4 train no.333050  loss = 3.53318 avg_loss = 3.17022\n",
      "epoch no.4 train no.333060  loss = 1.70863 avg_loss = 3.14514\n",
      "epoch no.4 train no.333070  loss = 2.23943 avg_loss = 3.12846\n",
      "epoch no.4 train no.333080  loss = 2.43125 avg_loss = 3.09787\n",
      "epoch no.4 train no.333090  loss = 2.60819 avg_loss = 3.08195\n",
      "epoch no.4 train no.333100  loss = 2.72428 avg_loss = 3.13417\n",
      "epoch no.4 train no.333110  loss = 2.55795 avg_loss = 3.22034\n",
      "epoch no.4 train no.333120  loss = 2.92381 avg_loss = 3.20126\n",
      "epoch no.4 train no.333130  loss = 3.57898 avg_loss = 3.21851\n",
      "epoch no.4 train no.333140  loss = 2.59518 avg_loss = 3.21961\n",
      "epoch no.4 train no.333150  loss = 3.36516 avg_loss = 3.23188\n",
      "epoch no.4 train no.333160  loss = 1.80332 avg_loss = 3.20406\n",
      "epoch no.4 train no.333170  loss = 3.49872 avg_loss = 3.26823\n",
      "epoch no.4 train no.333180  loss = 2.67604 avg_loss = 3.26174\n",
      "epoch no.4 train no.333190  loss = 3.25094 avg_loss = 3.26249\n",
      "epoch no.4 train no.333200  loss = 3.50094 avg_loss = 3.26398\n",
      "epoch no.4 train no.333210  loss = 3.59824 avg_loss = 3.34932\n",
      "epoch no.4 train no.333220  loss = 3.06511 avg_loss = 3.33475\n",
      "epoch no.4 train no.333230  loss = 2.66132 avg_loss = 3.31662\n",
      "epoch no.4 train no.333240  loss = 4.07908 avg_loss = 3.30392\n",
      "epoch no.4 train no.333250  loss = 2.14939 avg_loss = 3.29560\n",
      "epoch no.4 train no.333260  loss = 2.69544 avg_loss = 3.32202\n",
      "epoch no.4 train no.333270  loss = 2.65429 avg_loss = 3.29852\n",
      "epoch no.4 train no.333280  loss = 3.11232 avg_loss = 3.25803\n",
      "epoch no.4 train no.333290  loss = 2.23215 avg_loss = 3.24378\n",
      "epoch no.4 train no.333300  loss = 2.91191 avg_loss = 3.24270\n",
      "epoch no.4 train no.333310  loss = 3.18727 avg_loss = 3.22594\n",
      "epoch no.4 train no.333320  loss = 2.20109 avg_loss = 3.23074\n",
      "epoch no.4 train no.333330  loss = 2.59430 avg_loss = 3.22484\n",
      "epoch no.4 train no.333340  loss = 2.26351 avg_loss = 3.23931\n",
      "epoch no.4 train no.333350  loss = 2.57687 avg_loss = 3.25107\n",
      "epoch no.4 train no.333360  loss = 3.25668 avg_loss = 3.29294\n",
      "epoch no.4 train no.333370  loss = 3.69120 avg_loss = 3.27636\n",
      "epoch no.4 train no.333380  loss = 3.11662 avg_loss = 3.24532\n",
      "epoch no.4 train no.333390  loss = 2.61762 avg_loss = 3.27153\n",
      "epoch no.4 train no.333400  loss = 2.39080 avg_loss = 3.25606\n",
      "epoch no.4 train no.333410  loss = 2.76249 avg_loss = 3.24444\n",
      "epoch no.4 train no.333420  loss = 3.24364 avg_loss = 3.27528\n",
      "epoch no.4 train no.333430  loss = 4.67318 avg_loss = 3.28091\n",
      "epoch no.4 train no.333440  loss = 2.54623 avg_loss = 3.26629\n",
      "epoch no.4 train no.333450  loss = 2.06171 avg_loss = 3.27015\n",
      "epoch no.4 train no.333460  loss = 3.38283 avg_loss = 3.23340\n",
      "epoch no.4 train no.333470  loss = 2.72746 avg_loss = 3.20573\n",
      "epoch no.4 train no.333480  loss = 3.83717 avg_loss = 3.23441\n",
      "epoch no.4 train no.333490  loss = 3.32258 avg_loss = 3.24080\n",
      "epoch no.4 train no.333500  loss = 4.33226 avg_loss = 3.25940\n",
      "epoch no.4 train no.333510  loss = 3.30083 avg_loss = 3.24233\n",
      "epoch no.4 train no.333520  loss = 2.83872 avg_loss = 3.25889\n",
      "epoch no.4 train no.333530  loss = 3.74811 avg_loss = 3.24987\n",
      "epoch no.4 train no.333540  loss = 2.58847 avg_loss = 3.19862\n",
      "epoch no.4 train no.333550  loss = 2.58130 avg_loss = 3.19213\n",
      "epoch no.4 train no.333560  loss = 3.78870 avg_loss = 3.19377\n",
      "epoch no.4 train no.333570  loss = 3.77862 avg_loss = 3.20722\n",
      "epoch no.4 train no.333580  loss = 2.37349 avg_loss = 3.18678\n",
      "epoch no.4 train no.333590  loss = 3.08277 avg_loss = 3.19857\n",
      "epoch no.4 train no.333600  loss = 5.65378 avg_loss = 3.17907\n",
      "epoch no.4 train no.333610  loss = 2.48162 avg_loss = 3.16297\n",
      "epoch no.4 train no.333620  loss = 2.78785 avg_loss = 3.16093\n",
      "epoch no.4 train no.333630  loss = 2.33576 avg_loss = 3.13467\n",
      "epoch no.4 train no.333640  loss = 4.47187 avg_loss = 3.15482\n",
      "epoch no.4 train no.333650  loss = 2.65297 avg_loss = 3.11828\n",
      "epoch no.4 train no.333660  loss = 3.36161 avg_loss = 3.16842\n",
      "epoch no.4 train no.333670  loss = 3.00515 avg_loss = 3.15681\n",
      "epoch no.4 train no.333680  loss = 3.30312 avg_loss = 3.17699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.333690  loss = 3.07001 avg_loss = 3.16345\n",
      "epoch no.4 train no.333700  loss = 4.20777 avg_loss = 3.17782\n",
      "epoch no.4 train no.333710  loss = 4.43737 avg_loss = 3.19349\n",
      "epoch no.4 train no.333720  loss = 2.95554 avg_loss = 3.19866\n",
      "epoch no.4 train no.333730  loss = 2.52033 avg_loss = 3.20788\n",
      "epoch no.4 train no.333740  loss = 3.16660 avg_loss = 3.21337\n",
      "epoch no.4 train no.333750  loss = 3.76710 avg_loss = 3.19238\n",
      "epoch no.4 train no.333760  loss = 3.51008 avg_loss = 3.20916\n",
      "epoch no.4 train no.333770  loss = 1.67773 avg_loss = 3.18727\n",
      "epoch no.4 train no.333780  loss = 3.58697 avg_loss = 3.19099\n",
      "epoch no.4 train no.333790  loss = 3.14205 avg_loss = 3.20231\n",
      "epoch no.4 train no.333800  loss = 2.44380 avg_loss = 3.20504\n",
      "epoch no.4 train no.333810  loss = 3.30962 avg_loss = 3.21900\n",
      "epoch no.4 train no.333820  loss = 2.93799 avg_loss = 3.21356\n",
      "epoch no.4 train no.333830  loss = 4.25170 avg_loss = 3.20694\n",
      "epoch no.4 train no.333840  loss = 3.10006 avg_loss = 3.14617\n",
      "epoch no.4 train no.333850  loss = 2.77893 avg_loss = 3.14031\n",
      "epoch no.4 train no.333860  loss = 3.13041 avg_loss = 3.18210\n",
      "epoch no.4 train no.333870  loss = 3.02820 avg_loss = 3.14553\n",
      "epoch no.4 train no.333880  loss = 3.41395 avg_loss = 3.15111\n",
      "epoch no.4 train no.333890  loss = 2.61638 avg_loss = 3.14477\n",
      "epoch no.4 train no.333900  loss = 5.22694 avg_loss = 3.16357\n",
      "epoch no.4 train no.333910  loss = 2.94110 avg_loss = 3.18846\n",
      "epoch no.4 train no.333920  loss = 3.09622 avg_loss = 3.21213\n",
      "epoch no.4 train no.333930  loss = 2.38364 avg_loss = 3.22693\n",
      "epoch no.4 train no.333940  loss = 3.80085 avg_loss = 3.22968\n",
      "epoch no.4 train no.333950  loss = 2.55223 avg_loss = 3.23208\n",
      "epoch no.4 train no.333960  loss = 7.68399 avg_loss = 3.29894\n",
      "epoch no.4 train no.333970  loss = 1.41270 avg_loss = 3.26169\n",
      "epoch no.4 train no.333980  loss = 2.15278 avg_loss = 3.23699\n",
      "epoch no.4 train no.333990  loss = 4.15310 avg_loss = 3.26038\n",
      "epoch no.4 train no.334000  loss = 5.10673 avg_loss = 3.28309\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.334010  loss = 6.20975 avg_loss = 3.30264\n",
      "epoch no.4 train no.334020  loss = 2.99371 avg_loss = 3.31398\n",
      "epoch no.4 train no.334030  loss = 3.48813 avg_loss = 3.29488\n",
      "epoch no.4 train no.334040  loss = 3.82076 avg_loss = 3.29562\n",
      "epoch no.4 train no.334050  loss = 3.60168 avg_loss = 3.33495\n",
      "epoch no.4 train no.334060  loss = 2.99929 avg_loss = 3.29097\n",
      "epoch no.4 train no.334070  loss = 2.21107 avg_loss = 3.25824\n",
      "epoch no.4 train no.334080  loss = 2.22034 avg_loss = 3.22727\n",
      "epoch no.4 train no.334090  loss = 3.52493 avg_loss = 3.21818\n",
      "epoch no.4 train no.334100  loss = 1.65871 avg_loss = 3.23748\n",
      "epoch no.4 train no.334110  loss = 4.02906 avg_loss = 3.25170\n",
      "epoch no.4 train no.334120  loss = 3.81535 avg_loss = 3.27752\n",
      "epoch no.4 train no.334130  loss = 2.44266 avg_loss = 3.27245\n",
      "epoch no.4 train no.334140  loss = 2.62828 avg_loss = 3.26386\n",
      "epoch no.4 train no.334150  loss = 2.63970 avg_loss = 3.22971\n",
      "epoch no.4 train no.334160  loss = 3.07128 avg_loss = 3.22480\n",
      "epoch no.4 train no.334170  loss = 2.35364 avg_loss = 3.21575\n",
      "epoch no.4 train no.334180  loss = 3.09989 avg_loss = 3.18543\n",
      "epoch no.4 train no.334190  loss = 2.12952 avg_loss = 3.17960\n",
      "epoch no.4 train no.334200  loss = 3.19533 avg_loss = 3.15908\n",
      "epoch no.4 train no.334210  loss = 4.87137 avg_loss = 3.21557\n",
      "epoch no.4 train no.334220  loss = 4.49495 avg_loss = 3.24600\n",
      "epoch no.4 train no.334230  loss = 2.97397 avg_loss = 3.25817\n",
      "epoch no.4 train no.334240  loss = 2.85824 avg_loss = 3.22294\n",
      "epoch no.4 train no.334250  loss = 2.66866 avg_loss = 3.22410\n",
      "epoch no.4 train no.334260  loss = 2.24202 avg_loss = 3.20356\n",
      "epoch no.4 train no.334270  loss = 4.77078 avg_loss = 3.24828\n",
      "epoch no.4 train no.334280  loss = 3.34949 avg_loss = 3.20409\n",
      "epoch no.4 train no.334290  loss = 3.45672 avg_loss = 3.18950\n",
      "epoch no.4 train no.334300  loss = 3.19991 avg_loss = 3.22042\n",
      "epoch no.4 train no.334310  loss = 2.45537 avg_loss = 3.22356\n",
      "epoch no.4 train no.334320  loss = 3.04152 avg_loss = 3.21207\n",
      "epoch no.4 train no.334330  loss = 3.49639 avg_loss = 3.20172\n",
      "epoch no.4 train no.334340  loss = 2.50398 avg_loss = 3.21454\n",
      "epoch no.4 train no.334350  loss = 2.38646 avg_loss = 3.23225\n",
      "epoch no.4 train no.334360  loss = 3.07618 avg_loss = 3.19942\n",
      "epoch no.4 train no.334370  loss = 3.31143 avg_loss = 3.15516\n",
      "epoch no.4 train no.334380  loss = 2.82470 avg_loss = 3.20311\n",
      "epoch no.4 train no.334390  loss = 3.74277 avg_loss = 3.20481\n",
      "epoch no.4 train no.334400  loss = 3.91606 avg_loss = 3.18468\n",
      "epoch no.4 train no.334410  loss = 3.45312 avg_loss = 3.23034\n",
      "epoch no.4 train no.334420  loss = 2.68397 avg_loss = 3.25879\n",
      "epoch no.4 train no.334430  loss = 2.83500 avg_loss = 3.23730\n",
      "epoch no.4 train no.334440  loss = 4.60617 avg_loss = 3.20438\n",
      "epoch no.4 train no.334450  loss = 3.25423 avg_loss = 3.20843\n",
      "epoch no.4 train no.334460  loss = 2.11949 avg_loss = 3.21162\n",
      "epoch no.4 train no.334470  loss = 4.37988 avg_loss = 3.21484\n",
      "epoch no.4 train no.334480  loss = 2.15495 avg_loss = 3.20530\n",
      "epoch no.4 train no.334490  loss = 4.69643 avg_loss = 3.21406\n",
      "epoch no.4 train no.334500  loss = 3.30243 avg_loss = 3.22383\n",
      "epoch no.4 train no.334510  loss = 2.84953 avg_loss = 3.24836\n",
      "epoch no.4 train no.334520  loss = 2.43838 avg_loss = 3.20332\n",
      "epoch no.4 train no.334530  loss = 3.77839 avg_loss = 3.25124\n",
      "epoch no.4 train no.334540  loss = 2.62572 avg_loss = 3.25338\n",
      "epoch no.4 train no.334550  loss = 2.43631 avg_loss = 3.26900\n",
      "epoch no.4 train no.334560  loss = 2.23386 avg_loss = 3.24300\n",
      "epoch no.4 train no.334570  loss = 1.25148 avg_loss = 3.18356\n",
      "epoch no.4 train no.334580  loss = 3.11043 avg_loss = 3.23891\n",
      "epoch no.4 train no.334590  loss = 3.41194 avg_loss = 3.22803\n",
      "epoch no.4 train no.334600  loss = 2.64735 avg_loss = 3.23839\n",
      "epoch no.4 train no.334610  loss = 3.91130 avg_loss = 3.20489\n",
      "epoch no.4 train no.334620  loss = 2.54236 avg_loss = 3.18952\n",
      "epoch no.4 train no.334630  loss = 3.14513 avg_loss = 3.23355\n",
      "epoch no.4 train no.334640  loss = 2.14116 avg_loss = 3.16793\n",
      "epoch no.4 train no.334650  loss = 4.69602 avg_loss = 3.17408\n",
      "epoch no.4 train no.334660  loss = 3.50132 avg_loss = 3.20332\n",
      "epoch no.4 train no.334670  loss = 3.28619 avg_loss = 3.21973\n",
      "epoch no.4 train no.334680  loss = 3.54035 avg_loss = 3.18820\n",
      "epoch no.4 train no.334690  loss = 3.81033 avg_loss = 3.19737\n",
      "epoch no.4 train no.334700  loss = 3.09977 avg_loss = 3.24750\n",
      "epoch no.4 train no.334710  loss = 5.18434 avg_loss = 3.26414\n",
      "epoch no.4 train no.334720  loss = 3.40615 avg_loss = 3.23404\n",
      "epoch no.4 train no.334730  loss = 4.86420 avg_loss = 3.26602\n",
      "epoch no.4 train no.334740  loss = 3.86869 avg_loss = 3.29498\n",
      "epoch no.4 train no.334750  loss = 4.62734 avg_loss = 3.26792\n",
      "epoch no.4 train no.334760  loss = 3.71591 avg_loss = 3.29480\n",
      "epoch no.4 train no.334770  loss = 4.02043 avg_loss = 3.35051\n",
      "epoch no.4 train no.334780  loss = 2.99038 avg_loss = 3.34125\n",
      "epoch no.4 train no.334790  loss = 3.32460 avg_loss = 3.36051\n",
      "epoch no.4 train no.334800  loss = 2.85366 avg_loss = 3.33032\n",
      "epoch no.4 train no.334810  loss = 3.11160 avg_loss = 3.28791\n",
      "epoch no.4 train no.334820  loss = 3.35834 avg_loss = 3.30198\n",
      "epoch no.4 train no.334830  loss = 3.11800 avg_loss = 3.28373\n",
      "epoch no.4 train no.334840  loss = 3.24364 avg_loss = 3.26864\n",
      "epoch no.4 train no.334850  loss = 3.02560 avg_loss = 3.26827\n",
      "epoch no.4 train no.334860  loss = 1.75494 avg_loss = 3.27147\n",
      "epoch no.4 train no.334870  loss = 1.95070 avg_loss = 3.23046\n",
      "epoch no.4 train no.334880  loss = 2.84184 avg_loss = 3.18820\n",
      "epoch no.4 train no.334890  loss = 3.14934 avg_loss = 3.23927\n",
      "epoch no.4 train no.334900  loss = 1.94794 avg_loss = 3.25591\n",
      "epoch no.4 train no.334910  loss = 2.48159 avg_loss = 3.23732\n",
      "epoch no.4 train no.334920  loss = 3.39921 avg_loss = 3.22600\n",
      "epoch no.4 train no.334930  loss = 3.47708 avg_loss = 3.24543\n",
      "epoch no.4 train no.334940  loss = 1.94449 avg_loss = 3.25573\n",
      "epoch no.4 train no.334950  loss = 3.74593 avg_loss = 3.26634\n",
      "epoch no.4 train no.334960  loss = 2.15519 avg_loss = 3.24383\n",
      "epoch no.4 train no.334970  loss = 3.98394 avg_loss = 3.22601\n",
      "epoch no.4 train no.334980  loss = 3.35861 avg_loss = 3.28182\n",
      "epoch no.4 train no.334990  loss = 2.60295 avg_loss = 3.23854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.335000  loss = 3.30548 avg_loss = 3.25333\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.4 train no.335010  loss = 3.02400 avg_loss = 3.23018\n",
      "epoch no.4 train no.335020  loss = 4.43675 avg_loss = 3.25348\n",
      "epoch no.4 train no.335030  loss = 2.68213 avg_loss = 3.26133\n",
      "epoch no.4 train no.335040  loss = 3.63685 avg_loss = 3.29280\n",
      "epoch no.4 train no.335050  loss = 3.76641 avg_loss = 3.31920\n",
      "epoch no.4 train no.335060  loss = 1.68092 avg_loss = 3.26352\n",
      "epoch no.4 train no.335070  loss = 3.07955 avg_loss = 3.28170\n",
      "epoch no.4 train no.335080  loss = 3.16402 avg_loss = 3.23359\n",
      "epoch no.4 train no.335090  loss = 2.93287 avg_loss = 3.20922\n",
      "epoch no.4 train no.335100  loss = 2.22624 avg_loss = 3.20296\n",
      "epoch no.4 train no.335110  loss = 3.05315 avg_loss = 3.21752\n",
      "epoch no.4 train no.335120  loss = 1.95480 avg_loss = 3.20296\n",
      "epoch no.4 train no.335130  loss = 3.71136 avg_loss = 3.14603\n",
      "epoch no.4 train no.335140  loss = 4.34047 avg_loss = 3.18789\n",
      "epoch no.4 train no.335150  loss = 2.20926 avg_loss = 3.21686\n",
      "epoch no.4 train no.335160  loss = 3.46263 avg_loss = 3.22980\n",
      "epoch no.4 train no.335170  loss = 2.90518 avg_loss = 3.18979\n",
      "epoch no.4 train no.335180  loss = 4.09653 avg_loss = 3.22490\n",
      "epoch no.4 train no.335190  loss = 5.20209 avg_loss = 3.25832\n",
      "epoch no.4 train no.335200  loss = 3.80324 avg_loss = 3.27033\n",
      "epoch no.4 train no.335210  loss = 3.09096 avg_loss = 3.21376\n",
      "epoch no.4 train no.335220  loss = 3.09746 avg_loss = 3.21638\n",
      "epoch no.4 train no.335230  loss = 2.57077 avg_loss = 3.22128\n",
      "epoch no.4 train no.335240  loss = 4.05025 avg_loss = 3.25916\n",
      "epoch no.4 train no.335250  loss = 3.30492 avg_loss = 3.31706\n",
      "epoch no.4 train no.335260  loss = 4.33152 avg_loss = 3.32153\n",
      "epoch no.4 train no.335270  loss = 2.55836 avg_loss = 3.30803\n",
      "epoch no.4 train no.335280  loss = 4.10416 avg_loss = 3.32436\n",
      "epoch no.4 train no.335290  loss = 2.57686 avg_loss = 3.34918\n",
      "epoch no.4 train no.335300  loss = 4.23560 avg_loss = 3.33824\n",
      "epoch no.4 train no.335310  loss = 4.42950 avg_loss = 3.32655\n",
      "epoch no.4 train no.335320  loss = 2.94067 avg_loss = 3.29220\n",
      "epoch no.4 train no.335330  loss = 2.67494 avg_loss = 3.25256\n",
      "epoch no.4 train no.335340  loss = 3.33489 avg_loss = 3.21853\n",
      "epoch no.4 train no.335350  loss = 2.90553 avg_loss = 3.23043\n",
      "epoch no.4 train no.335360  loss = 3.69251 avg_loss = 3.25283\n",
      "epoch no.4 train no.335370  loss = 3.44756 avg_loss = 3.27043\n",
      "epoch no.4 train no.335380  loss = 2.13201 avg_loss = 3.26740\n",
      "epoch no.4 train no.335390  loss = 4.43022 avg_loss = 3.26534\n",
      "epoch no.4 train no.335400  loss = 3.68510 avg_loss = 3.26631\n",
      "epoch no.4 train no.335410  loss = 3.54906 avg_loss = 3.25983\n",
      "epoch no.4 train no.335420  loss = 4.05718 avg_loss = 3.24329\n",
      "epoch no.4 train no.335430  loss = 3.25588 avg_loss = 3.21323\n",
      "epoch no.4 train no.335440  loss = 3.76066 avg_loss = 3.21245\n",
      "epoch no.4 train no.335450  loss = 3.17831 avg_loss = 3.25851\n",
      "epoch no.4 train no.335460  loss = 3.63452 avg_loss = 3.28400\n",
      "epoch no.4 train no.335470  loss = 1.62495 avg_loss = 3.26360\n",
      "epoch no.4 train no.335480  loss = 4.19969 avg_loss = 3.27843\n",
      "epoch no.4 train no.335490  loss = 4.74819 avg_loss = 3.30899\n",
      "epoch no.4 train no.335500  loss = 3.47135 avg_loss = 3.35157\n",
      "epoch no.4 train no.335510  loss = 3.75153 avg_loss = 3.39801\n",
      "epoch no.4 train no.335520  loss = 5.56504 avg_loss = 3.37376\n",
      "epoch no.4 train no.335530  loss = 3.32587 avg_loss = 3.34859\n",
      "epoch no.4 train no.335540  loss = 4.70127 avg_loss = 3.35062\n",
      "epoch no.4 train no.335550  loss = 4.71137 avg_loss = 3.36299\n",
      "epoch no.4 train no.335560  loss = 3.55187 avg_loss = 3.33787\n",
      "epoch no.4 train no.335570  loss = 3.36293 avg_loss = 3.37335\n",
      "epoch no.4 train no.335580  loss = 2.77869 avg_loss = 3.33272\n",
      "epoch no.4 train no.335590  loss = 2.68441 avg_loss = 3.28854\n",
      "epoch no.4 train no.335600  loss = 2.99143 avg_loss = 3.27511\n",
      "epoch no.4 train no.335610  loss = 4.49981 avg_loss = 3.27115\n",
      "epoch no.4 train no.335620  loss = 2.40216 avg_loss = 3.32127\n",
      "epoch no.4 train no.335630  loss = 5.70525 avg_loss = 3.35010\n",
      "epoch no.4 train no.335640  loss = 2.52148 avg_loss = 3.30929\n",
      "epoch no.4 train no.335650  loss = 3.13910 avg_loss = 3.34501\n",
      "epoch no.4 train no.335660  loss = 3.22659 avg_loss = 3.35450\n",
      "epoch no.4 train no.335670  loss = 2.72444 avg_loss = 3.31677\n",
      "epoch no.4 train no.335680  loss = 2.19602 avg_loss = 3.34052\n",
      "epoch no.4 train no.335690  loss = 3.59111 avg_loss = 3.33401\n",
      "epoch no.4 train no.335700  loss = 2.27326 avg_loss = 3.31559\n",
      "epoch no.4 train no.335710  loss = 3.47244 avg_loss = 3.31774\n",
      "epoch no.4 train no.335720  loss = 3.31741 avg_loss = 3.24970\n",
      "epoch no.4 train no.335730  loss = 3.12694 avg_loss = 3.23267\n",
      "epoch no.4 train no.335740  loss = 3.00427 avg_loss = 3.27367\n",
      "epoch no.4 train no.335750  loss = 2.58078 avg_loss = 3.27475\n",
      "epoch no.4 train no.335760  loss = 2.54788 avg_loss = 3.29666\n",
      "epoch no.4 train no.335770  loss = 3.03084 avg_loss = 3.30092\n",
      "epoch no.4 train no.335780  loss = 2.41190 avg_loss = 3.32430\n",
      "epoch no.4 train no.335790  loss = 3.62229 avg_loss = 3.31926\n",
      "epoch no.4 train no.335800  loss = 2.33421 avg_loss = 3.30349\n",
      "epoch no.4 train no.335810  loss = 2.90963 avg_loss = 3.28210\n",
      "epoch no.4 train no.335820  loss = 3.31588 avg_loss = 3.27969\n",
      "epoch no.4 train no.335830  loss = 3.19785 avg_loss = 3.25174\n",
      "epoch no.4 train no.335840  loss = 2.45525 avg_loss = 3.19315\n",
      "epoch no.4 train no.335850  loss = 2.80678 avg_loss = 3.18886\n",
      "epoch no.4 train no.335860  loss = 3.90240 avg_loss = 3.20305\n",
      "epoch no.4 train no.335870  loss = 3.28606 avg_loss = 3.21001\n",
      "epoch no.4 train no.335880  loss = 2.30875 avg_loss = 3.20334\n",
      "epoch no.4 train no.335890  loss = 3.77242 avg_loss = 3.18482\n",
      "epoch no.4 train no.335900  loss = 4.06592 avg_loss = 3.22239\n",
      "epoch no.4 train no.335910  loss = 3.46914 avg_loss = 3.21445\n",
      "epoch no.4 train no.335920  loss = 4.00802 avg_loss = 3.22450\n",
      "epoch no.4 train no.335930  loss = 5.87092 avg_loss = 3.28033\n",
      "epoch no.4 train no.335940  loss = 3.40818 avg_loss = 3.25348\n",
      "epoch no.4 train no.335950  loss = 3.83738 avg_loss = 3.26129\n",
      "epoch no.4 train no.335960  loss = 2.47915 avg_loss = 3.29954\n",
      "epoch no.4 train no.335970  loss = 2.02805 avg_loss = 3.27639\n",
      "epoch no.4 train no.335980  loss = 5.78185 avg_loss = 3.34424\n",
      "epoch no.4 train no.335990  loss = 2.66579 avg_loss = 3.35220\n",
      "epoch no.4 train no.336000  loss = 3.98258 avg_loss = 3.38249\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '들', '으며', '올', '듣', '</s>']\n",
      "추억의 노래들아 이거들어</s>\n",
      "epoch no.4 train no.336010  loss = 2.30227 avg_loss = 3.35921\n",
      "epoch no.4 train no.336020  loss = 3.81342 avg_loss = 3.35848\n",
      "epoch no.4 train no.336030  loss = 3.08321 avg_loss = 3.33752\n",
      "epoch no.4 train no.336040  loss = 2.74568 avg_loss = 3.30830\n",
      "epoch no.4 train no.336050  loss = 4.70229 avg_loss = 3.32664\n",
      "epoch no.4 train no.336060  loss = 3.54489 avg_loss = 3.29876\n",
      "epoch no.4 train no.336070  loss = 3.46052 avg_loss = 3.30749\n",
      "epoch no.4 train no.336080  loss = 3.03713 avg_loss = 3.28805\n",
      "epoch no.4 train no.336090  loss = 2.09191 avg_loss = 3.23192\n",
      "epoch no.4 train no.336100  loss = 3.38711 avg_loss = 3.22841\n",
      "epoch no.4 train no.336110  loss = 2.14199 avg_loss = 3.28007\n",
      "epoch no.4 train no.336120  loss = 1.76079 avg_loss = 3.22720\n",
      "epoch no.4 train no.336130  loss = 2.94270 avg_loss = 3.18961\n",
      "epoch no.4 train no.336140  loss = 3.89008 avg_loss = 3.25804\n",
      "epoch no.4 train no.336150  loss = 3.81803 avg_loss = 3.23833\n",
      "epoch no.4 train no.336160  loss = 3.30601 avg_loss = 3.29774\n",
      "epoch no.4 train no.336170  loss = 2.20565 avg_loss = 3.33709\n",
      "epoch no.4 train no.336180  loss = 4.07205 avg_loss = 3.34846\n",
      "epoch no.4 train no.336190  loss = 2.28287 avg_loss = 3.31033\n",
      "epoch no.4 train no.336200  loss = 1.99105 avg_loss = 3.28253\n",
      "epoch no.4 train no.336210  loss = 2.66613 avg_loss = 3.26005\n",
      "epoch no.4 train no.336220  loss = 3.08354 avg_loss = 3.26462\n",
      "epoch no.4 train no.336230  loss = 5.44705 avg_loss = 3.27062\n",
      "epoch no.4 train no.336240  loss = 2.68109 avg_loss = 3.22491\n",
      "epoch no.4 train no.336250  loss = 3.29296 avg_loss = 3.18987\n",
      "epoch no.4 train no.336260  loss = 3.45371 avg_loss = 3.16925\n",
      "epoch no.4 train no.336270  loss = 4.77159 avg_loss = 3.17691\n",
      "epoch no.4 train no.336280  loss = 2.49880 avg_loss = 3.20911\n",
      "epoch no.4 train no.336290  loss = 4.62373 avg_loss = 3.25048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.336300  loss = 2.50054 avg_loss = 3.24026\n",
      "epoch no.4 train no.336310  loss = 2.92371 avg_loss = 3.22922\n",
      "epoch no.4 train no.336320  loss = 2.94185 avg_loss = 3.20820\n",
      "epoch no.4 train no.336330  loss = 2.60810 avg_loss = 3.20188\n",
      "epoch no.4 train no.336340  loss = 4.05480 avg_loss = 3.19530\n",
      "epoch no.4 train no.336350  loss = 4.01396 avg_loss = 3.21311\n",
      "epoch no.4 train no.336360  loss = 3.77336 avg_loss = 3.20407\n",
      "epoch no.4 train no.336370  loss = 2.09449 avg_loss = 3.14538\n",
      "epoch no.4 train no.336380  loss = 3.78498 avg_loss = 3.18910\n",
      "epoch no.4 train no.336390  loss = 5.41013 avg_loss = 3.20642\n",
      "epoch no.4 train no.336400  loss = 3.23583 avg_loss = 3.23603\n",
      "epoch no.4 train no.336410  loss = 5.65720 avg_loss = 3.26282\n",
      "epoch no.4 train no.336420  loss = 2.41119 avg_loss = 3.20980\n",
      "epoch no.4 train no.336430  loss = 3.18207 avg_loss = 3.22654\n",
      "epoch no.4 train no.336440  loss = 2.91286 avg_loss = 3.22997\n",
      "epoch no.4 train no.336450  loss = 2.68588 avg_loss = 3.22748\n",
      "epoch no.4 train no.336460  loss = 3.60338 avg_loss = 3.23358\n",
      "epoch no.4 train no.336470  loss = 3.48690 avg_loss = 3.19389\n",
      "epoch no.4 train no.336480  loss = 2.57409 avg_loss = 3.20480\n",
      "epoch no.4 train no.336490  loss = 2.32012 avg_loss = 3.18129\n",
      "epoch no.4 train no.336500  loss = 3.06965 avg_loss = 3.18767\n",
      "epoch no.4 train no.336510  loss = 3.63355 avg_loss = 3.18941\n",
      "epoch no.4 train no.336520  loss = 2.23736 avg_loss = 3.15068\n",
      "epoch no.4 train no.336530  loss = 2.96248 avg_loss = 3.13803\n",
      "epoch no.4 train no.336540  loss = 1.84630 avg_loss = 3.15967\n",
      "epoch no.4 train no.336550  loss = 2.21715 avg_loss = 3.12178\n",
      "epoch no.4 train no.336560  loss = 3.26109 avg_loss = 3.13442\n",
      "epoch no.4 train no.336570  loss = 3.21217 avg_loss = 3.12949\n",
      "epoch no.4 train no.336580  loss = 2.95034 avg_loss = 3.13433\n",
      "epoch no.4 train no.336590  loss = 1.85836 avg_loss = 3.12581\n",
      "epoch no.4 train no.336600  loss = 2.83759 avg_loss = 3.11184\n",
      "epoch no.4 train no.336610  loss = 3.26941 avg_loss = 3.15113\n",
      "epoch no.4 train no.336620  loss = 1.68009 avg_loss = 3.11552\n",
      "epoch no.4 train no.336630  loss = 3.87929 avg_loss = 3.12196\n",
      "epoch no.4 train no.336640  loss = 3.58085 avg_loss = 3.12546\n",
      "epoch no.4 train no.336650  loss = 2.93498 avg_loss = 3.13937\n",
      "epoch no.4 train no.336660  loss = 2.24555 avg_loss = 3.16902\n",
      "epoch no.4 train no.336670  loss = 3.19072 avg_loss = 3.13162\n",
      "epoch no.4 train no.336680  loss = 3.92282 avg_loss = 3.12539\n",
      "epoch no.4 train no.336690  loss = 3.29992 avg_loss = 3.18050\n",
      "epoch no.4 train no.336700  loss = 3.27502 avg_loss = 3.19731\n",
      "epoch no.4 train no.336710  loss = 2.73100 avg_loss = 3.22640\n",
      "epoch no.4 train no.336720  loss = 4.40226 avg_loss = 3.21368\n",
      "epoch no.4 train no.336730  loss = 3.00318 avg_loss = 3.16436\n",
      "epoch no.4 train no.336740  loss = 2.17944 avg_loss = 3.16714\n",
      "epoch no.4 train no.336750  loss = 5.11687 avg_loss = 3.13897\n",
      "epoch no.4 train no.336760  loss = 3.79673 avg_loss = 3.11855\n",
      "epoch no.4 train no.336770  loss = 2.70954 avg_loss = 3.13311\n",
      "epoch no.4 train no.336780  loss = 4.38203 avg_loss = 3.16502\n",
      "epoch no.4 train no.336790  loss = 2.25816 avg_loss = 3.16514\n",
      "epoch no.4 train no.336800  loss = 2.50422 avg_loss = 3.16096\n",
      "epoch no.4 train no.336810  loss = 3.44133 avg_loss = 3.12267\n",
      "epoch no.4 train no.336820  loss = 3.02439 avg_loss = 3.16546\n",
      "epoch no.4 train no.336830  loss = 2.25455 avg_loss = 3.13441\n",
      "epoch no.4 train no.336840  loss = 3.56237 avg_loss = 3.15910\n",
      "epoch no.4 train no.336850  loss = 1.93956 avg_loss = 3.15171\n",
      "epoch no.4 train no.336860  loss = 2.87481 avg_loss = 3.12874\n",
      "epoch no.4 train no.336870  loss = 2.58543 avg_loss = 3.16789\n",
      "epoch no.4 train no.336880  loss = 4.18939 avg_loss = 3.16665\n",
      "epoch no.4 train no.336890  loss = 2.78927 avg_loss = 3.22323\n",
      "epoch no.4 train no.336900  loss = 2.91252 avg_loss = 3.25803\n",
      "epoch no.4 train no.336910  loss = 2.55645 avg_loss = 3.21050\n",
      "epoch no.4 train no.336920  loss = 1.55310 avg_loss = 3.18302\n",
      "epoch no.4 train no.336930  loss = 4.75338 avg_loss = 3.23007\n",
      "epoch no.4 train no.336940  loss = 3.38626 avg_loss = 3.27399\n",
      "epoch no.4 train no.336950  loss = 4.18158 avg_loss = 3.25015\n",
      "epoch no.4 train no.336960  loss = 3.31959 avg_loss = 3.26581\n",
      "epoch no.4 train no.336970  loss = 3.17288 avg_loss = 3.25111\n",
      "epoch no.4 train no.336980  loss = 3.00793 avg_loss = 3.24535\n",
      "epoch no.4 train no.336990  loss = 3.35902 avg_loss = 3.23919\n",
      "epoch no.4 train no.337000  loss = 4.77145 avg_loss = 3.26903\n",
      "4\n",
      "to_tokens: ['▁가을', '▁싸이', '팝', '▁모음', '음', '</s>']\n",
      "추억의 댄스음악모음</s>\n",
      "epoch no.4 train no.337010  loss = 3.11269 avg_loss = 3.25777\n",
      "epoch no.4 train no.337020  loss = 2.11498 avg_loss = 3.20696\n",
      "epoch no.4 train no.337030  loss = 2.45153 avg_loss = 3.18460\n",
      "epoch no.4 train no.337040  loss = 2.80056 avg_loss = 3.21914\n",
      "epoch no.4 train no.337050  loss = 3.97236 avg_loss = 3.23507\n",
      "epoch no.4 train no.337060  loss = 4.55385 avg_loss = 3.25993\n",
      "epoch no.4 train no.337070  loss = 4.91291 avg_loss = 3.24976\n",
      "epoch no.4 train no.337080  loss = 5.59746 avg_loss = 3.26975\n",
      "epoch no.4 train no.337090  loss = 3.19560 avg_loss = 3.23684\n",
      "epoch no.4 train no.337100  loss = 3.78172 avg_loss = 3.22448\n",
      "epoch no.4 train no.337110  loss = 2.04413 avg_loss = 3.18652\n",
      "epoch no.4 train no.337120  loss = 2.16702 avg_loss = 3.14635\n",
      "epoch no.4 train no.337130  loss = 3.70579 avg_loss = 3.14876\n",
      "epoch no.4 train no.337140  loss = 3.84350 avg_loss = 3.18436\n",
      "epoch no.4 train no.337150  loss = 3.14240 avg_loss = 3.17866\n",
      "epoch no.4 train no.337160  loss = 6.15120 avg_loss = 3.17488\n",
      "epoch no.4 train no.337170  loss = 3.81673 avg_loss = 3.14630\n",
      "epoch no.4 train no.337180  loss = 2.28685 avg_loss = 3.09099\n",
      "epoch no.4 train no.337190  loss = 3.02292 avg_loss = 3.06637\n",
      "epoch no.4 train no.337200  loss = 3.20774 avg_loss = 3.07702\n",
      "epoch no.4 train no.337210  loss = 1.75214 avg_loss = 3.04912\n",
      "epoch no.4 train no.337220  loss = 3.02491 avg_loss = 3.07610\n",
      "epoch no.4 train no.337230  loss = 3.14153 avg_loss = 3.03591\n",
      "epoch no.4 train no.337240  loss = 4.50843 avg_loss = 3.04982\n",
      "epoch no.4 train no.337250  loss = 3.94223 avg_loss = 3.05741\n",
      "epoch no.4 train no.337260  loss = 1.97763 avg_loss = 3.03856\n",
      "epoch no.4 train no.337270  loss = 4.08676 avg_loss = 3.08171\n",
      "epoch no.4 train no.337280  loss = 2.01695 avg_loss = 3.07111\n",
      "epoch no.4 train no.337290  loss = 4.87548 avg_loss = 3.15956\n",
      "epoch no.4 train no.337300  loss = 3.26823 avg_loss = 3.13718\n",
      "epoch no.4 train no.337310  loss = 3.38188 avg_loss = 3.11697\n",
      "epoch no.4 train no.337320  loss = 3.24984 avg_loss = 3.13450\n",
      "epoch no.4 train no.337330  loss = 4.19422 avg_loss = 3.12679\n",
      "epoch no.4 train no.337340  loss = 2.48155 avg_loss = 3.12503\n",
      "epoch no.4 train no.337350  loss = 2.31458 avg_loss = 3.09697\n",
      "epoch no.4 train no.337360  loss = 4.14347 avg_loss = 3.10303\n",
      "epoch no.4 train no.337370  loss = 1.93703 avg_loss = 3.09667\n",
      "epoch no.4 train no.337380  loss = 2.54726 avg_loss = 3.12931\n",
      "epoch no.4 train no.337390  loss = 3.14958 avg_loss = 3.10481\n",
      "epoch no.4 train no.337400  loss = 2.80131 avg_loss = 3.09735\n",
      "epoch no.4 train no.337410  loss = 4.21781 avg_loss = 3.07790\n",
      "epoch no.4 train no.337420  loss = 2.95990 avg_loss = 3.13773\n",
      "epoch no.4 train no.337430  loss = 3.01739 avg_loss = 3.14404\n",
      "epoch no.4 train no.337440  loss = 3.16689 avg_loss = 3.15077\n",
      "epoch no.4 train no.337450  loss = 1.50527 avg_loss = 3.16579\n",
      "epoch no.4 train no.337460  loss = 2.13375 avg_loss = 3.18512\n",
      "epoch no.4 train no.337470  loss = 2.46565 avg_loss = 3.17994\n",
      "epoch no.4 train no.337480  loss = 4.02273 avg_loss = 3.20904\n",
      "epoch no.4 train no.337490  loss = 2.45954 avg_loss = 3.25149\n",
      "epoch no.4 train no.337500  loss = 2.51054 avg_loss = 3.25293\n",
      "epoch no.4 train no.337510  loss = 3.68421 avg_loss = 3.22083\n",
      "epoch no.4 train no.337520  loss = 3.03982 avg_loss = 3.20033\n",
      "epoch no.4 train no.337530  loss = 2.59782 avg_loss = 3.21592\n",
      "epoch no.4 train no.337540  loss = 2.60715 avg_loss = 3.22987\n",
      "epoch no.4 train no.337550  loss = 4.47056 avg_loss = 3.21974\n",
      "epoch no.4 train no.337560  loss = 2.03062 avg_loss = 3.23082\n",
      "epoch no.4 train no.337570  loss = 2.98208 avg_loss = 3.22663\n",
      "epoch no.4 train no.337580  loss = 2.96324 avg_loss = 3.22411\n",
      "epoch no.4 train no.337590  loss = 2.44532 avg_loss = 3.19332\n",
      "epoch no.4 train no.337600  loss = 2.93091 avg_loss = 3.18973\n",
      "epoch no.4 train no.337610  loss = 2.71600 avg_loss = 3.18968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.337620  loss = 2.01282 avg_loss = 3.18701\n",
      "epoch no.4 train no.337630  loss = 5.06227 avg_loss = 3.24184\n",
      "epoch no.4 train no.337640  loss = 3.93758 avg_loss = 3.22279\n",
      "epoch no.4 train no.337650  loss = 4.06444 avg_loss = 3.22157\n",
      "epoch no.4 train no.337660  loss = 3.47458 avg_loss = 3.24716\n",
      "epoch no.4 train no.337670  loss = 3.30038 avg_loss = 3.25144\n",
      "epoch no.4 train no.337680  loss = 2.35798 avg_loss = 3.24766\n",
      "epoch no.4 train no.337690  loss = 2.92366 avg_loss = 3.24688\n",
      "epoch no.4 train no.337700  loss = 2.74163 avg_loss = 3.22514\n",
      "epoch no.4 train no.337710  loss = 3.25486 avg_loss = 3.22783\n",
      "epoch no.4 train no.337720  loss = 3.05656 avg_loss = 3.20862\n",
      "epoch no.4 train no.337730  loss = 2.52281 avg_loss = 3.15325\n",
      "epoch no.4 train no.337740  loss = 2.81460 avg_loss = 3.16953\n",
      "epoch no.4 train no.337750  loss = 3.08261 avg_loss = 3.15966\n",
      "epoch no.4 train no.337760  loss = 3.18393 avg_loss = 3.15477\n",
      "epoch no.4 train no.337770  loss = 3.49433 avg_loss = 3.12662\n",
      "epoch no.4 train no.337780  loss = 2.47293 avg_loss = 3.12222\n",
      "epoch no.4 train no.337790  loss = 4.24697 avg_loss = 3.10400\n",
      "epoch no.4 train no.337800  loss = 2.14162 avg_loss = 3.11250\n",
      "epoch no.4 train no.337810  loss = 2.17466 avg_loss = 3.11035\n",
      "epoch no.4 train no.337820  loss = 3.43174 avg_loss = 3.16865\n",
      "epoch no.4 train no.337830  loss = 2.66169 avg_loss = 3.18266\n",
      "epoch no.4 train no.337840  loss = 2.46302 avg_loss = 3.16989\n",
      "epoch no.4 train no.337850  loss = 4.03878 avg_loss = 3.12632\n",
      "epoch no.4 train no.337860  loss = 2.66015 avg_loss = 3.08682\n",
      "epoch no.4 train no.337870  loss = 3.43660 avg_loss = 3.12498\n",
      "epoch no.4 train no.337880  loss = 4.27709 avg_loss = 3.16380\n",
      "epoch no.4 train no.337890  loss = 2.82422 avg_loss = 3.15231\n",
      "epoch no.4 train no.337900  loss = 2.15725 avg_loss = 3.14886\n",
      "epoch no.4 train no.337910  loss = 5.11037 avg_loss = 3.22460\n",
      "epoch no.4 train no.337920  loss = 5.53259 avg_loss = 3.26605\n",
      "epoch no.4 train no.337930  loss = 2.40240 avg_loss = 3.31570\n",
      "epoch no.4 train no.337940  loss = 2.09175 avg_loss = 3.29623\n",
      "epoch no.4 train no.337950  loss = 4.02874 avg_loss = 3.30491\n",
      "epoch no.4 train no.337960  loss = 3.21721 avg_loss = 3.31968\n",
      "epoch no.4 train no.337970  loss = 3.72051 avg_loss = 3.32187\n",
      "epoch no.4 train no.337980  loss = 2.56617 avg_loss = 3.32038\n",
      "epoch no.4 train no.337990  loss = 4.18756 avg_loss = 3.29969\n",
      "epoch no.4 train no.338000  loss = 2.77007 avg_loss = 3.31657\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '팝', '이', '▁모음', '</s>']\n",
      "추억의 댄스팝송 모음</s>\n",
      "epoch no.4 train no.338010  loss = 3.91141 avg_loss = 3.28932\n",
      "epoch no.4 train no.338020  loss = 2.78818 avg_loss = 3.29011\n",
      "epoch no.4 train no.338030  loss = 3.21933 avg_loss = 3.27405\n",
      "epoch no.4 train no.338040  loss = 3.46149 avg_loss = 3.26981\n",
      "epoch no.4 train no.338050  loss = 5.51695 avg_loss = 3.26247\n",
      "epoch no.4 train no.338060  loss = 3.26690 avg_loss = 3.32533\n",
      "epoch no.4 train no.338070  loss = 3.28709 avg_loss = 3.32562\n",
      "epoch no.4 train no.338080  loss = 4.04836 avg_loss = 3.29537\n",
      "epoch no.4 train no.338090  loss = 1.92843 avg_loss = 3.27321\n",
      "epoch no.4 train no.338100  loss = 1.78999 avg_loss = 3.29073\n",
      "epoch no.4 train no.338110  loss = 3.63261 avg_loss = 3.29233\n",
      "epoch no.4 train no.338120  loss = 2.93185 avg_loss = 3.27936\n",
      "epoch no.4 train no.338130  loss = 3.83761 avg_loss = 3.29599\n",
      "epoch no.4 train no.338140  loss = 4.51218 avg_loss = 3.28890\n",
      "epoch no.4 train no.338150  loss = 3.77253 avg_loss = 3.25387\n",
      "epoch no.4 train no.338160  loss = 1.56244 avg_loss = 3.25210\n",
      "epoch no.4 train no.338170  loss = 2.98525 avg_loss = 3.24586\n",
      "epoch no.4 train no.338180  loss = 4.93578 avg_loss = 3.24831\n",
      "epoch no.4 train no.338190  loss = 3.59141 avg_loss = 3.23057\n",
      "epoch no.4 train no.338200  loss = 3.79088 avg_loss = 3.22770\n",
      "epoch no.4 train no.338210  loss = 4.07391 avg_loss = 3.21170\n",
      "epoch no.4 train no.338220  loss = 3.12110 avg_loss = 3.16841\n",
      "epoch no.4 train no.338230  loss = 4.18742 avg_loss = 3.19967\n",
      "epoch no.4 train no.338240  loss = 3.84298 avg_loss = 3.20148\n",
      "epoch no.4 train no.338250  loss = 3.57611 avg_loss = 3.20437\n",
      "epoch no.4 train no.338260  loss = 2.38122 avg_loss = 3.19758\n",
      "epoch no.4 train no.338270  loss = 3.39059 avg_loss = 3.17513\n",
      "epoch no.4 train no.338280  loss = 1.85955 avg_loss = 3.15978\n",
      "epoch no.4 train no.338290  loss = 3.78374 avg_loss = 3.14832\n",
      "epoch no.4 train no.338300  loss = 3.25990 avg_loss = 3.17270\n",
      "epoch no.4 train no.338310  loss = 2.41051 avg_loss = 3.16912\n",
      "epoch no.4 train no.338320  loss = 2.71613 avg_loss = 3.17831\n",
      "epoch no.4 train no.338330  loss = 2.54349 avg_loss = 3.16001\n",
      "epoch no.4 train no.338340  loss = 2.03386 avg_loss = 3.12219\n",
      "epoch no.4 train no.338350  loss = 2.53024 avg_loss = 3.10239\n",
      "epoch no.4 train no.338360  loss = 3.38098 avg_loss = 3.12968\n",
      "epoch no.4 train no.338370  loss = 3.30098 avg_loss = 3.12708\n",
      "epoch no.4 train no.338380  loss = 3.67605 avg_loss = 3.10455\n",
      "epoch no.4 train no.338390  loss = 2.85876 avg_loss = 3.10739\n",
      "epoch no.4 train no.338400  loss = 2.71418 avg_loss = 3.12316\n",
      "epoch no.4 train no.338410  loss = 4.20482 avg_loss = 3.13040\n",
      "epoch no.4 train no.338420  loss = 3.55716 avg_loss = 3.17696\n",
      "epoch no.4 train no.338430  loss = 3.64909 avg_loss = 3.21174\n",
      "epoch no.4 train no.338440  loss = 2.77761 avg_loss = 3.22348\n",
      "epoch no.4 train no.338450  loss = 3.05802 avg_loss = 3.15373\n",
      "epoch no.4 train no.338460  loss = 2.96498 avg_loss = 3.16287\n",
      "epoch no.4 train no.338470  loss = 3.40015 avg_loss = 3.16534\n",
      "epoch no.4 train no.338480  loss = 2.06551 avg_loss = 3.15949\n",
      "epoch no.4 train no.338490  loss = 1.25514 avg_loss = 3.12958\n",
      "epoch no.4 train no.338500  loss = 2.68277 avg_loss = 3.15063\n",
      "epoch no.4 train no.338510  loss = 3.20808 avg_loss = 3.14382\n",
      "epoch no.4 train no.338520  loss = 4.81562 avg_loss = 3.17592\n",
      "epoch no.4 train no.338530  loss = 1.88690 avg_loss = 3.19105\n",
      "epoch no.4 train no.338540  loss = 2.08280 avg_loss = 3.18470\n",
      "epoch no.4 train no.338550  loss = 2.01408 avg_loss = 3.18096\n",
      "epoch no.4 train no.338560  loss = 2.94440 avg_loss = 3.15426\n",
      "epoch no.4 train no.338570  loss = 2.66037 avg_loss = 3.17216\n",
      "epoch no.4 train no.338580  loss = 4.87909 avg_loss = 3.20908\n",
      "epoch no.4 train no.338590  loss = 2.57812 avg_loss = 3.17368\n",
      "epoch no.4 train no.338600  loss = 3.57672 avg_loss = 3.17285\n",
      "epoch no.4 train no.338610  loss = 1.78682 avg_loss = 3.15149\n",
      "epoch no.4 train no.338620  loss = 4.57323 avg_loss = 3.21148\n",
      "epoch no.4 train no.338630  loss = 4.23151 avg_loss = 3.27387\n",
      "epoch no.4 train no.338640  loss = 2.79389 avg_loss = 3.29004\n",
      "epoch no.4 train no.338650  loss = 2.61605 avg_loss = 3.30364\n",
      "epoch no.4 train no.338660  loss = 4.16890 avg_loss = 3.34851\n",
      "epoch no.4 train no.338670  loss = 2.98475 avg_loss = 3.35097\n",
      "epoch no.4 train no.338680  loss = 2.82016 avg_loss = 3.35405\n",
      "epoch no.4 train no.338690  loss = 3.11447 avg_loss = 3.32994\n",
      "epoch no.4 train no.338700  loss = 2.29126 avg_loss = 3.31308\n",
      "epoch no.4 train no.338710  loss = 5.09079 avg_loss = 3.36189\n",
      "epoch no.4 train no.338720  loss = 4.01992 avg_loss = 3.38171\n",
      "epoch no.4 train no.338730  loss = 2.50325 avg_loss = 3.32444\n",
      "epoch no.4 train no.338740  loss = 3.43271 avg_loss = 3.32006\n",
      "epoch no.4 train no.338750  loss = 3.33283 avg_loss = 3.30704\n",
      "epoch no.4 train no.338760  loss = 4.47059 avg_loss = 3.30570\n",
      "epoch no.4 train no.338770  loss = 2.14994 avg_loss = 3.29230\n",
      "epoch no.4 train no.338780  loss = 4.34308 avg_loss = 3.28474\n",
      "epoch no.4 train no.338790  loss = 2.74583 avg_loss = 3.27777\n",
      "epoch no.4 train no.338800  loss = 3.91877 avg_loss = 3.25184\n",
      "epoch no.4 train no.338810  loss = 2.85624 avg_loss = 3.21838\n",
      "epoch no.4 train no.338820  loss = 4.75060 avg_loss = 3.24108\n",
      "epoch no.4 train no.338830  loss = 4.28773 avg_loss = 3.25604\n",
      "epoch no.4 train no.338840  loss = 2.03570 avg_loss = 3.24873\n",
      "epoch no.4 train no.338850  loss = 2.77619 avg_loss = 3.27272\n",
      "epoch no.4 train no.338860  loss = 4.38534 avg_loss = 3.24154\n",
      "epoch no.4 train no.338870  loss = 2.45411 avg_loss = 3.24415\n",
      "epoch no.4 train no.338880  loss = 3.19866 avg_loss = 3.24681\n",
      "epoch no.4 train no.338890  loss = 1.91350 avg_loss = 3.23446\n",
      "epoch no.4 train no.338900  loss = 2.20461 avg_loss = 3.19355\n",
      "epoch no.4 train no.338910  loss = 2.53785 avg_loss = 3.19313\n",
      "epoch no.4 train no.338920  loss = 3.04741 avg_loss = 3.16702\n",
      "epoch no.4 train no.338930  loss = 3.54278 avg_loss = 3.17027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.338940  loss = 3.65834 avg_loss = 3.18126\n",
      "epoch no.4 train no.338950  loss = 5.31822 avg_loss = 3.21313\n",
      "epoch no.4 train no.338960  loss = 1.84137 avg_loss = 3.18688\n",
      "epoch no.4 train no.338970  loss = 1.58924 avg_loss = 3.18241\n",
      "epoch no.4 train no.338980  loss = 2.01918 avg_loss = 3.12532\n",
      "epoch no.4 train no.338990  loss = 2.37628 avg_loss = 3.14673\n",
      "epoch no.4 train no.339000  loss = 4.37956 avg_loss = 3.16463\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁노래', '▁모음', '드', '</s>']\n",
      "추억의 90년대 가요 발라드</s>\n",
      "epoch no.4 train no.339010  loss = 3.73320 avg_loss = 3.15343\n",
      "epoch no.4 train no.339020  loss = 3.35634 avg_loss = 3.18180\n",
      "epoch no.4 train no.339030  loss = 3.20074 avg_loss = 3.20555\n",
      "epoch no.4 train no.339040  loss = 3.33404 avg_loss = 3.19739\n",
      "epoch no.4 train no.339050  loss = 2.15623 avg_loss = 3.19675\n",
      "epoch no.4 train no.339060  loss = 2.55351 avg_loss = 3.17473\n",
      "epoch no.4 train no.339070  loss = 3.34416 avg_loss = 3.16263\n",
      "epoch no.4 train no.339080  loss = 2.76558 avg_loss = 3.17194\n",
      "epoch no.4 train no.339090  loss = 2.71808 avg_loss = 3.12965\n",
      "epoch no.4 train no.339100  loss = 2.81607 avg_loss = 3.11114\n",
      "epoch no.4 train no.339110  loss = 1.80874 avg_loss = 3.11819\n",
      "epoch no.4 train no.339120  loss = 2.84686 avg_loss = 3.11685\n",
      "epoch no.4 train no.339130  loss = 2.90112 avg_loss = 3.09509\n",
      "epoch no.4 train no.339140  loss = 3.12728 avg_loss = 3.14744\n",
      "epoch no.4 train no.339150  loss = 3.72771 avg_loss = 3.17728\n",
      "epoch no.4 train no.339160  loss = 2.61094 avg_loss = 3.16745\n",
      "epoch no.4 train no.339170  loss = 4.70806 avg_loss = 3.18901\n",
      "epoch no.4 train no.339180  loss = 4.14618 avg_loss = 3.21301\n",
      "epoch no.4 train no.339190  loss = 3.03122 avg_loss = 3.19438\n",
      "epoch no.4 train no.339200  loss = 2.79894 avg_loss = 3.20884\n",
      "epoch no.4 train no.339210  loss = 2.13405 avg_loss = 3.21750\n",
      "epoch no.4 train no.339220  loss = 2.81436 avg_loss = 3.24168\n",
      "epoch no.4 train no.339230  loss = 3.50463 avg_loss = 3.24790\n",
      "epoch no.4 train no.339240  loss = 2.85524 avg_loss = 3.23089\n",
      "epoch no.4 train no.339250  loss = 3.47680 avg_loss = 3.20354\n",
      "epoch no.4 train no.339260  loss = 2.56693 avg_loss = 3.20576\n",
      "epoch no.4 train no.339270  loss = 4.03452 avg_loss = 3.16584\n",
      "epoch no.4 train no.339280  loss = 3.17561 avg_loss = 3.17902\n",
      "epoch no.4 train no.339290  loss = 3.19463 avg_loss = 3.18068\n",
      "epoch no.4 train no.339300  loss = 4.62935 avg_loss = 3.17094\n",
      "epoch no.4 train no.339310  loss = 3.14772 avg_loss = 3.15969\n",
      "epoch no.4 train no.339320  loss = 2.24870 avg_loss = 3.18578\n",
      "epoch no.4 train no.339330  loss = 2.83331 avg_loss = 3.15611\n",
      "epoch no.4 train no.339340  loss = 3.07940 avg_loss = 3.18466\n",
      "epoch no.4 train no.339350  loss = 3.44710 avg_loss = 3.17841\n",
      "epoch no.4 train no.339360  loss = 2.13332 avg_loss = 3.13060\n",
      "epoch no.4 train no.339370  loss = 3.03347 avg_loss = 3.17034\n",
      "epoch no.4 train no.339380  loss = 2.45892 avg_loss = 3.14668\n",
      "epoch no.4 train no.339390  loss = 3.27695 avg_loss = 3.16996\n",
      "epoch no.4 train no.339400  loss = 2.84174 avg_loss = 3.20632\n",
      "epoch no.4 train no.339410  loss = 3.60779 avg_loss = 3.16820\n",
      "epoch no.4 train no.339420  loss = 2.00067 avg_loss = 3.12203\n",
      "epoch no.4 train no.339430  loss = 3.31111 avg_loss = 3.11499\n",
      "epoch no.4 train no.339440  loss = 2.61852 avg_loss = 3.11955\n",
      "epoch no.4 train no.339450  loss = 2.60800 avg_loss = 3.18440\n",
      "epoch no.4 train no.339460  loss = 2.73148 avg_loss = 3.13705\n",
      "epoch no.4 train no.339470  loss = 2.80037 avg_loss = 3.14609\n",
      "epoch no.4 train no.339480  loss = 2.05159 avg_loss = 3.12934\n",
      "epoch no.4 train no.339490  loss = 2.77403 avg_loss = 3.17983\n",
      "epoch no.4 train no.339500  loss = 3.40400 avg_loss = 3.12352\n",
      "epoch no.4 train no.339510  loss = 2.88714 avg_loss = 3.19341\n",
      "epoch no.4 train no.339520  loss = 3.45503 avg_loss = 3.18863\n",
      "epoch no.4 train no.339530  loss = 3.65350 avg_loss = 3.16394\n",
      "epoch no.4 train no.339540  loss = 4.61457 avg_loss = 3.19962\n",
      "epoch no.4 train no.339550  loss = 2.23201 avg_loss = 3.18530\n",
      "epoch no.4 train no.339560  loss = 3.01996 avg_loss = 3.17325\n",
      "epoch no.4 train no.339570  loss = 4.67055 avg_loss = 3.18968\n",
      "epoch no.4 train no.339580  loss = 2.65746 avg_loss = 3.24037\n",
      "epoch no.4 train no.339590  loss = 3.02307 avg_loss = 3.26300\n",
      "epoch no.4 train no.339600  loss = 4.67195 avg_loss = 3.27562\n",
      "epoch no.4 train no.339610  loss = 2.49196 avg_loss = 3.28025\n",
      "epoch no.4 train no.339620  loss = 3.68658 avg_loss = 3.24815\n",
      "epoch no.4 train no.339630  loss = 2.67631 avg_loss = 3.27811\n",
      "epoch no.4 train no.339640  loss = 4.16800 avg_loss = 3.27094\n",
      "epoch no.4 train no.339650  loss = 3.85392 avg_loss = 3.27726\n",
      "epoch no.4 train no.339660  loss = 3.14828 avg_loss = 3.27796\n",
      "epoch no.4 train no.339670  loss = 2.61340 avg_loss = 3.25456\n",
      "epoch no.4 train no.339680  loss = 2.61467 avg_loss = 3.20339\n",
      "epoch no.4 train no.339690  loss = 2.19249 avg_loss = 3.21668\n",
      "epoch no.4 train no.339700  loss = 2.85613 avg_loss = 3.20523\n",
      "epoch no.4 train no.339710  loss = 2.69609 avg_loss = 3.19873\n",
      "epoch no.4 train no.339720  loss = 2.81980 avg_loss = 3.18756\n",
      "epoch no.4 train no.339730  loss = 4.65257 avg_loss = 3.18223\n",
      "epoch no.4 train no.339740  loss = 3.30529 avg_loss = 3.16577\n",
      "epoch no.4 train no.339750  loss = 3.23392 avg_loss = 3.16604\n",
      "epoch no.4 train no.339760  loss = 3.82227 avg_loss = 3.17324\n",
      "epoch no.4 train no.339770  loss = 2.74293 avg_loss = 3.18047\n",
      "epoch no.4 train no.339780  loss = 3.20209 avg_loss = 3.14470\n",
      "epoch no.4 train no.339790  loss = 2.97213 avg_loss = 3.12856\n",
      "epoch no.4 train no.339800  loss = 4.32632 avg_loss = 3.14742\n",
      "epoch no.4 train no.339810  loss = 2.78922 avg_loss = 3.12904\n",
      "epoch no.4 train no.339820  loss = 2.41892 avg_loss = 3.11167\n",
      "epoch no.4 train no.339830  loss = 3.04213 avg_loss = 3.11252\n",
      "epoch no.4 train no.339840  loss = 3.73555 avg_loss = 3.14321\n",
      "epoch no.4 train no.339850  loss = 3.40836 avg_loss = 3.11201\n",
      "epoch no.4 train no.339860  loss = 3.15084 avg_loss = 3.13483\n",
      "epoch no.4 train no.339870  loss = 4.23779 avg_loss = 3.17844\n",
      "epoch no.4 train no.339880  loss = 3.96485 avg_loss = 3.19758\n",
      "epoch no.4 train no.339890  loss = 3.06207 avg_loss = 3.19415\n",
      "epoch no.4 train no.339900  loss = 2.93995 avg_loss = 3.20061\n",
      "epoch no.4 train no.339910  loss = 2.51272 avg_loss = 3.20443\n",
      "epoch no.4 train no.339920  loss = 2.87732 avg_loss = 3.19855\n",
      "epoch no.4 train no.339930  loss = 2.90365 avg_loss = 3.19913\n",
      "epoch no.4 train no.339940  loss = 2.53053 avg_loss = 3.22556\n",
      "epoch no.4 train no.339950  loss = 3.25095 avg_loss = 3.20520\n",
      "epoch no.4 train no.339960  loss = 2.85406 avg_loss = 3.18319\n",
      "epoch no.4 train no.339970  loss = 3.51677 avg_loss = 3.15556\n",
      "epoch no.4 train no.339980  loss = 2.67031 avg_loss = 3.15349\n",
      "epoch no.4 train no.339990  loss = 2.92569 avg_loss = 3.16893\n",
      "epoch no.4 train no.340000  loss = 3.25878 avg_loss = 3.18309\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '▁시절', '▁그', '▁떠오르는', '▁노래', '</s>']\n",
      "추억의 그 시절이 떠오르는 노래</s>\n",
      "epoch no.4 train no.340010  loss = 1.42960 avg_loss = 3.15046\n",
      "epoch no.4 train no.340020  loss = 3.97656 avg_loss = 3.18448\n",
      "epoch no.4 train no.340030  loss = 3.88303 avg_loss = 3.26788\n",
      "epoch no.4 train no.340040  loss = 3.27754 avg_loss = 3.25552\n",
      "epoch no.4 train no.340050  loss = 2.42019 avg_loss = 3.23899\n",
      "epoch no.4 train no.340060  loss = 4.17553 avg_loss = 3.21961\n",
      "epoch no.4 train no.340070  loss = 3.39056 avg_loss = 3.21897\n",
      "epoch no.4 train no.340080  loss = 2.54690 avg_loss = 3.18832\n",
      "epoch no.4 train no.340090  loss = 5.18026 avg_loss = 3.25512\n",
      "epoch no.4 train no.340100  loss = 2.26286 avg_loss = 3.28887\n",
      "epoch no.4 train no.340110  loss = 3.58946 avg_loss = 3.28942\n",
      "epoch no.4 train no.340120  loss = 3.33830 avg_loss = 3.33849\n",
      "epoch no.4 train no.340130  loss = 3.86417 avg_loss = 3.35925\n",
      "epoch no.4 train no.340140  loss = 3.58307 avg_loss = 3.34661\n",
      "epoch no.4 train no.340150  loss = 3.38020 avg_loss = 3.32229\n",
      "epoch no.4 train no.340160  loss = 3.34400 avg_loss = 3.28402\n",
      "epoch no.4 train no.340170  loss = 2.98283 avg_loss = 3.28777\n",
      "epoch no.4 train no.340180  loss = 1.92486 avg_loss = 3.28215\n",
      "epoch no.4 train no.340190  loss = 6.62481 avg_loss = 3.27888\n",
      "epoch no.4 train no.340200  loss = 3.71838 avg_loss = 3.29229\n",
      "epoch no.4 train no.340210  loss = 3.62807 avg_loss = 3.28399\n",
      "epoch no.4 train no.340220  loss = 2.88951 avg_loss = 3.30938\n",
      "epoch no.4 train no.340230  loss = 2.63713 avg_loss = 3.32151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.340240  loss = 3.62125 avg_loss = 3.31563\n",
      "epoch no.4 train no.340250  loss = 2.87966 avg_loss = 3.24448\n",
      "epoch no.4 train no.340260  loss = 3.48823 avg_loss = 3.24989\n",
      "epoch no.4 train no.340270  loss = 2.82466 avg_loss = 3.21840\n",
      "epoch no.4 train no.340280  loss = 3.22698 avg_loss = 3.18433\n",
      "epoch no.4 train no.340290  loss = 3.83794 avg_loss = 3.24622\n",
      "epoch no.4 train no.340300  loss = 3.06497 avg_loss = 3.27197\n",
      "epoch no.4 train no.340310  loss = 3.46327 avg_loss = 3.29346\n",
      "epoch no.4 train no.340320  loss = 3.39999 avg_loss = 3.26065\n",
      "epoch no.4 train no.340330  loss = 2.32871 avg_loss = 3.25047\n",
      "epoch no.4 train no.340340  loss = 2.06142 avg_loss = 3.19180\n",
      "epoch no.4 train no.340350  loss = 3.08123 avg_loss = 3.13547\n",
      "epoch no.4 train no.340360  loss = 2.74368 avg_loss = 3.13409\n",
      "epoch no.4 train no.340370  loss = 2.46897 avg_loss = 3.12504\n",
      "epoch no.4 train no.340380  loss = 2.99571 avg_loss = 3.12849\n",
      "epoch no.4 train no.340390  loss = 2.38946 avg_loss = 3.11573\n",
      "epoch no.4 train no.340400  loss = 2.10307 avg_loss = 3.14347\n",
      "epoch no.4 train no.340410  loss = 1.90943 avg_loss = 3.15071\n",
      "epoch no.4 train no.340420  loss = 2.69842 avg_loss = 3.15327\n",
      "epoch no.4 train no.340430  loss = 2.26052 avg_loss = 3.16060\n",
      "epoch no.4 train no.340440  loss = 2.23739 avg_loss = 3.16797\n",
      "epoch no.4 train no.340450  loss = 2.53667 avg_loss = 3.13203\n",
      "epoch no.4 train no.340460  loss = 3.14235 avg_loss = 3.13568\n",
      "epoch no.4 train no.340470  loss = 3.41896 avg_loss = 3.15370\n",
      "epoch no.4 train no.340480  loss = 4.00315 avg_loss = 3.14603\n",
      "epoch no.4 train no.340490  loss = 2.46018 avg_loss = 3.14552\n",
      "epoch no.4 train no.340500  loss = 3.24027 avg_loss = 3.18331\n",
      "epoch no.4 train no.340510  loss = 2.82385 avg_loss = 3.23198\n",
      "epoch no.4 train no.340520  loss = 4.18661 avg_loss = 3.21916\n",
      "epoch no.4 train no.340530  loss = 2.17696 avg_loss = 3.21667\n",
      "epoch no.4 train no.340540  loss = 2.23386 avg_loss = 3.22513\n",
      "epoch no.4 train no.340550  loss = 3.20745 avg_loss = 3.24236\n",
      "epoch no.4 train no.340560  loss = 4.00248 avg_loss = 3.24602\n",
      "epoch no.4 train no.340570  loss = 1.76586 avg_loss = 3.22575\n",
      "epoch no.4 train no.340580  loss = 2.47996 avg_loss = 3.25181\n",
      "epoch no.4 train no.340590  loss = 3.57637 avg_loss = 3.21977\n",
      "epoch no.4 train no.340600  loss = 5.39518 avg_loss = 3.25195\n",
      "epoch no.4 train no.340610  loss = 2.85036 avg_loss = 3.28990\n",
      "epoch no.4 train no.340620  loss = 4.23683 avg_loss = 3.28640\n",
      "epoch no.4 train no.340630  loss = 2.60040 avg_loss = 3.26225\n",
      "epoch no.4 train no.340640  loss = 3.67972 avg_loss = 3.26028\n",
      "epoch no.4 train no.340650  loss = 2.41499 avg_loss = 3.22708\n",
      "epoch no.4 train no.340660  loss = 2.29662 avg_loss = 3.22078\n",
      "epoch no.4 train no.340670  loss = 3.05988 avg_loss = 3.23698\n",
      "epoch no.4 train no.340680  loss = 5.76488 avg_loss = 3.28254\n",
      "epoch no.4 train no.340690  loss = 2.52637 avg_loss = 3.26221\n",
      "epoch no.4 train no.340700  loss = 3.08987 avg_loss = 3.28745\n",
      "epoch no.4 train no.340710  loss = 3.35759 avg_loss = 3.28933\n",
      "epoch no.4 train no.340720  loss = 3.47441 avg_loss = 3.30545\n",
      "epoch no.4 train no.340730  loss = 3.14642 avg_loss = 3.25973\n",
      "epoch no.4 train no.340740  loss = 3.23525 avg_loss = 3.28523\n",
      "epoch no.4 train no.340750  loss = 4.14695 avg_loss = 3.30262\n",
      "epoch no.4 train no.340760  loss = 2.68762 avg_loss = 3.29547\n",
      "epoch no.4 train no.340770  loss = 3.31382 avg_loss = 3.27624\n",
      "epoch no.4 train no.340780  loss = 3.50682 avg_loss = 3.27780\n",
      "epoch no.4 train no.340790  loss = 3.15058 avg_loss = 3.31127\n",
      "epoch no.4 train no.340800  loss = 2.92269 avg_loss = 3.30436\n",
      "epoch no.4 train no.340810  loss = 3.49146 avg_loss = 3.31230\n",
      "epoch no.4 train no.340820  loss = 2.32726 avg_loss = 3.32993\n",
      "epoch no.4 train no.340830  loss = 5.16833 avg_loss = 3.33087\n",
      "epoch no.4 train no.340840  loss = 2.19697 avg_loss = 3.26346\n",
      "epoch no.4 train no.340850  loss = 3.86404 avg_loss = 3.26887\n",
      "epoch no.4 train no.340860  loss = 5.47388 avg_loss = 3.26415\n",
      "epoch no.4 train no.340870  loss = 4.85708 avg_loss = 3.25951\n",
      "epoch no.4 train no.340880  loss = 2.99370 avg_loss = 3.26900\n",
      "epoch no.4 train no.340890  loss = 2.72067 avg_loss = 3.24279\n",
      "epoch no.4 train no.340900  loss = 2.19220 avg_loss = 3.21018\n",
      "epoch no.4 train no.340910  loss = 3.49623 avg_loss = 3.20457\n",
      "epoch no.4 train no.340920  loss = 2.88995 avg_loss = 3.23625\n",
      "epoch no.4 train no.340930  loss = 3.89112 avg_loss = 3.29001\n",
      "epoch no.4 train no.340940  loss = 2.46050 avg_loss = 3.31161\n",
      "epoch no.4 train no.340950  loss = 2.84200 avg_loss = 3.30447\n",
      "epoch no.4 train no.340960  loss = 2.47435 avg_loss = 3.26062\n",
      "epoch no.4 train no.340970  loss = 2.96068 avg_loss = 3.23662\n",
      "epoch no.4 train no.340980  loss = 3.37346 avg_loss = 3.26845\n",
      "epoch no.4 train no.340990  loss = 2.92522 avg_loss = 3.23954\n",
      "epoch no.4 train no.341000  loss = 3.39169 avg_loss = 3.24454\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁노래', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.4 train no.341010  loss = 1.86318 avg_loss = 3.22356\n",
      "epoch no.4 train no.341020  loss = 3.57232 avg_loss = 3.21492\n",
      "epoch no.4 train no.341030  loss = 2.73755 avg_loss = 3.22219\n",
      "epoch no.4 train no.341040  loss = 2.55387 avg_loss = 3.22414\n",
      "epoch no.4 train no.341050  loss = 2.68285 avg_loss = 3.18562\n",
      "epoch no.4 train no.341060  loss = 2.74010 avg_loss = 3.16610\n",
      "epoch no.4 train no.341070  loss = 2.09907 avg_loss = 3.17496\n",
      "epoch no.4 train no.341080  loss = 2.56295 avg_loss = 3.17749\n",
      "epoch no.4 train no.341090  loss = 2.62658 avg_loss = 3.17786\n",
      "epoch no.4 train no.341100  loss = 2.52686 avg_loss = 3.14816\n",
      "epoch no.4 train no.341110  loss = 2.77688 avg_loss = 3.18613\n",
      "epoch no.4 train no.341120  loss = 2.90928 avg_loss = 3.15013\n",
      "epoch no.4 train no.341130  loss = 3.94433 avg_loss = 3.13418\n",
      "epoch no.4 train no.341140  loss = 2.31119 avg_loss = 3.16254\n",
      "epoch no.4 train no.341150  loss = 2.15921 avg_loss = 3.17303\n",
      "epoch no.4 train no.341160  loss = 2.97502 avg_loss = 3.16765\n",
      "epoch no.4 train no.341170  loss = 2.93608 avg_loss = 3.16334\n",
      "epoch no.4 train no.341180  loss = 2.26320 avg_loss = 3.13668\n",
      "epoch no.4 train no.341190  loss = 2.89400 avg_loss = 3.11340\n",
      "epoch no.4 train no.341200  loss = 3.25556 avg_loss = 3.08905\n",
      "epoch no.4 train no.341210  loss = 3.04424 avg_loss = 3.10010\n",
      "epoch no.4 train no.341220  loss = 2.75160 avg_loss = 3.09440\n",
      "epoch no.4 train no.341230  loss = 4.01567 avg_loss = 3.17703\n",
      "epoch no.4 train no.341240  loss = 3.46366 avg_loss = 3.17363\n",
      "epoch no.4 train no.341250  loss = 1.90542 avg_loss = 3.14443\n",
      "epoch no.4 train no.341260  loss = 4.33421 avg_loss = 3.14025\n",
      "epoch no.4 train no.341270  loss = 3.96077 avg_loss = 3.17698\n",
      "epoch no.4 train no.341280  loss = 5.24892 avg_loss = 3.20430\n",
      "epoch no.4 train no.341290  loss = 3.56395 avg_loss = 3.25339\n",
      "epoch no.4 train no.341300  loss = 3.36548 avg_loss = 3.29942\n",
      "epoch no.4 train no.341310  loss = 2.69628 avg_loss = 3.24981\n",
      "epoch no.4 train no.341320  loss = 2.22114 avg_loss = 3.23923\n",
      "epoch no.4 train no.341330  loss = 3.39427 avg_loss = 3.21307\n",
      "epoch no.4 train no.341340  loss = 2.51072 avg_loss = 3.19668\n",
      "epoch no.4 train no.341350  loss = 4.29519 avg_loss = 3.18804\n",
      "epoch no.4 train no.341360  loss = 3.69917 avg_loss = 3.22628\n",
      "epoch no.4 train no.341370  loss = 2.50924 avg_loss = 3.20435\n",
      "epoch no.4 train no.341380  loss = 3.18203 avg_loss = 3.21166\n",
      "epoch no.4 train no.341390  loss = 3.13107 avg_loss = 3.22021\n",
      "epoch no.4 train no.341400  loss = 4.09022 avg_loss = 3.22633\n",
      "epoch no.4 train no.341410  loss = 2.70643 avg_loss = 3.20592\n",
      "epoch no.4 train no.341420  loss = 2.14052 avg_loss = 3.18202\n",
      "epoch no.4 train no.341430  loss = 4.11986 avg_loss = 3.18853\n",
      "epoch no.4 train no.341440  loss = 5.31000 avg_loss = 3.22472\n",
      "epoch no.4 train no.341450  loss = 3.76029 avg_loss = 3.18042\n",
      "epoch no.4 train no.341460  loss = 3.51240 avg_loss = 3.17084\n",
      "epoch no.4 train no.341470  loss = 4.19609 avg_loss = 3.21768\n",
      "epoch no.4 train no.341480  loss = 4.21406 avg_loss = 3.25418\n",
      "epoch no.4 train no.341490  loss = 3.83590 avg_loss = 3.29434\n",
      "epoch no.4 train no.341500  loss = 3.99882 avg_loss = 3.30341\n",
      "epoch no.4 train no.341510  loss = 2.86264 avg_loss = 3.27930\n",
      "epoch no.4 train no.341520  loss = 3.94449 avg_loss = 3.27840\n",
      "epoch no.4 train no.341530  loss = 2.23908 avg_loss = 3.28497\n",
      "epoch no.4 train no.341540  loss = 2.56483 avg_loss = 3.25381\n",
      "epoch no.4 train no.341550  loss = 2.48096 avg_loss = 3.23811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.341560  loss = 2.78855 avg_loss = 3.25523\n",
      "epoch no.4 train no.341570  loss = 2.18241 avg_loss = 3.26934\n",
      "epoch no.4 train no.341580  loss = 1.38141 avg_loss = 3.27022\n",
      "epoch no.4 train no.341590  loss = 3.02019 avg_loss = 3.21082\n",
      "epoch no.4 train no.341600  loss = 2.82028 avg_loss = 3.18995\n",
      "epoch no.4 train no.341610  loss = 1.78846 avg_loss = 3.16062\n",
      "epoch no.4 train no.341620  loss = 2.31481 avg_loss = 3.16488\n",
      "epoch no.4 train no.341630  loss = 2.25689 avg_loss = 3.15512\n",
      "epoch no.4 train no.341640  loss = 2.57845 avg_loss = 3.16478\n",
      "epoch no.4 train no.341650  loss = 3.09020 avg_loss = 3.16258\n",
      "epoch no.4 train no.341660  loss = 2.77534 avg_loss = 3.20134\n",
      "epoch no.4 train no.341670  loss = 2.27062 avg_loss = 3.20550\n",
      "epoch no.4 train no.341680  loss = 3.26392 avg_loss = 3.16422\n",
      "epoch no.4 train no.341690  loss = 2.99928 avg_loss = 3.18566\n",
      "epoch no.4 train no.341700  loss = 3.41297 avg_loss = 3.18232\n",
      "epoch no.4 train no.341710  loss = 3.45945 avg_loss = 3.14739\n",
      "epoch no.4 train no.341720  loss = 2.30132 avg_loss = 3.17007\n",
      "epoch no.4 train no.341730  loss = 3.02942 avg_loss = 3.15261\n",
      "epoch no.4 train no.341740  loss = 2.18958 avg_loss = 3.12141\n",
      "epoch no.4 train no.341750  loss = 1.95781 avg_loss = 3.15478\n",
      "epoch no.4 train no.341760  loss = 3.28580 avg_loss = 3.16698\n",
      "epoch no.4 train no.341770  loss = 2.39877 avg_loss = 3.16605\n",
      "epoch no.4 train no.341780  loss = 3.62837 avg_loss = 3.15656\n",
      "epoch no.4 train no.341790  loss = 2.78617 avg_loss = 3.18169\n",
      "epoch no.4 train no.341800  loss = 3.24573 avg_loss = 3.14791\n",
      "epoch no.4 train no.341810  loss = 2.07144 avg_loss = 3.16727\n",
      "epoch no.4 train no.341820  loss = 3.60844 avg_loss = 3.15413\n",
      "epoch no.4 train no.341830  loss = 3.87570 avg_loss = 3.15226\n",
      "epoch no.4 train no.341840  loss = 4.46762 avg_loss = 3.17320\n",
      "epoch no.4 train no.341850  loss = 3.53586 avg_loss = 3.17657\n",
      "epoch no.4 train no.341860  loss = 2.74439 avg_loss = 3.19547\n",
      "epoch no.4 train no.341870  loss = 3.39788 avg_loss = 3.13805\n",
      "epoch no.4 train no.341880  loss = 5.09658 avg_loss = 3.17721\n",
      "epoch no.4 train no.341890  loss = 2.36417 avg_loss = 3.17993\n",
      "epoch no.4 train no.341900  loss = 3.41433 avg_loss = 3.19357\n",
      "epoch no.4 train no.341910  loss = 2.72096 avg_loss = 3.16096\n",
      "epoch no.4 train no.341920  loss = 4.08357 avg_loss = 3.17287\n",
      "epoch no.4 train no.341930  loss = 2.41748 avg_loss = 3.17023\n",
      "epoch no.4 train no.341940  loss = 2.97329 avg_loss = 3.18559\n",
      "epoch no.4 train no.341950  loss = 3.81535 avg_loss = 3.15833\n",
      "epoch no.4 train no.341960  loss = 3.37034 avg_loss = 3.16529\n",
      "epoch no.4 train no.341970  loss = 4.08651 avg_loss = 3.13101\n",
      "epoch no.4 train no.341980  loss = 3.10426 avg_loss = 3.14127\n",
      "epoch no.4 train no.341990  loss = 4.80279 avg_loss = 3.13348\n",
      "epoch no.4 train no.342000  loss = 2.70294 avg_loss = 3.08992\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁', '▁노래', '</s>']\n",
      "추억의 90 00년대 노래</s>\n",
      "epoch no.4 train no.342010  loss = 1.81952 avg_loss = 3.07880\n",
      "epoch no.4 train no.342020  loss = 4.30481 avg_loss = 3.14411\n",
      "epoch no.4 train no.342030  loss = 4.12033 avg_loss = 3.16396\n",
      "epoch no.4 train no.342040  loss = 3.09625 avg_loss = 3.20899\n",
      "epoch no.4 train no.342050  loss = 2.94809 avg_loss = 3.19868\n",
      "epoch no.4 train no.342060  loss = 4.04807 avg_loss = 3.19488\n",
      "epoch no.4 train no.342070  loss = 2.47249 avg_loss = 3.20405\n",
      "epoch no.4 train no.342080  loss = 3.67058 avg_loss = 3.16998\n",
      "epoch no.4 train no.342090  loss = 2.69955 avg_loss = 3.17495\n",
      "epoch no.4 train no.342100  loss = 2.68057 avg_loss = 3.18775\n",
      "epoch no.4 train no.342110  loss = 4.23319 avg_loss = 3.22751\n",
      "epoch no.4 train no.342120  loss = 3.13608 avg_loss = 3.25756\n",
      "epoch no.4 train no.342130  loss = 2.85970 avg_loss = 3.24980\n",
      "epoch no.4 train no.342140  loss = 3.11313 avg_loss = 3.27009\n",
      "epoch no.4 train no.342150  loss = 1.85966 avg_loss = 3.27514\n",
      "epoch no.4 train no.342160  loss = 2.84026 avg_loss = 3.28430\n",
      "epoch no.4 train no.342170  loss = 2.41326 avg_loss = 3.24659\n",
      "epoch no.4 train no.342180  loss = 3.28980 avg_loss = 3.28157\n",
      "epoch no.4 train no.342190  loss = 3.99662 avg_loss = 3.24853\n",
      "epoch no.4 train no.342200  loss = 2.71950 avg_loss = 3.31200\n",
      "epoch no.4 train no.342210  loss = 3.39119 avg_loss = 3.27069\n",
      "epoch no.4 train no.342220  loss = 2.06204 avg_loss = 3.25838\n",
      "epoch no.4 train no.342230  loss = 4.21560 avg_loss = 3.26306\n",
      "epoch no.4 train no.342240  loss = 2.16034 avg_loss = 3.20484\n",
      "epoch no.4 train no.342250  loss = 1.37761 avg_loss = 3.14821\n",
      "epoch no.4 train no.342260  loss = 2.74184 avg_loss = 3.14114\n",
      "epoch no.4 train no.342270  loss = 3.06162 avg_loss = 3.14991\n",
      "epoch no.4 train no.342280  loss = 2.05452 avg_loss = 3.14392\n",
      "epoch no.4 train no.342290  loss = 5.58549 avg_loss = 3.19037\n",
      "epoch no.4 train no.342300  loss = 2.68763 avg_loss = 3.21170\n",
      "epoch no.4 train no.342310  loss = 4.33259 avg_loss = 3.20074\n",
      "epoch no.4 train no.342320  loss = 2.99284 avg_loss = 3.19802\n",
      "epoch no.4 train no.342330  loss = 3.30491 avg_loss = 3.26293\n",
      "epoch no.4 train no.342340  loss = 3.31082 avg_loss = 3.27402\n",
      "epoch no.4 train no.342350  loss = 2.50020 avg_loss = 3.20308\n",
      "epoch no.4 train no.342360  loss = 3.01382 avg_loss = 3.18267\n",
      "epoch no.4 train no.342370  loss = 3.61797 avg_loss = 3.17021\n",
      "epoch no.4 train no.342380  loss = 4.51793 avg_loss = 3.19010\n",
      "epoch no.4 train no.342390  loss = 3.46488 avg_loss = 3.13039\n",
      "epoch no.4 train no.342400  loss = 3.12248 avg_loss = 3.15659\n",
      "epoch no.4 train no.342410  loss = 3.31859 avg_loss = 3.16626\n",
      "epoch no.4 train no.342420  loss = 3.29114 avg_loss = 3.17911\n",
      "epoch no.4 train no.342430  loss = 3.13209 avg_loss = 3.14924\n",
      "epoch no.4 train no.342440  loss = 2.13998 avg_loss = 3.10859\n",
      "epoch no.4 train no.342450  loss = 2.37772 avg_loss = 3.07749\n",
      "epoch no.4 train no.342460  loss = 3.99239 avg_loss = 3.05398\n",
      "epoch no.4 train no.342470  loss = 2.07447 avg_loss = 3.05140\n",
      "epoch no.4 train no.342480  loss = 4.56501 avg_loss = 3.05640\n",
      "epoch no.4 train no.342490  loss = 3.84511 avg_loss = 3.12997\n",
      "epoch no.4 train no.342500  loss = 3.37446 avg_loss = 3.12372\n",
      "epoch no.4 train no.342510  loss = 1.97664 avg_loss = 3.11028\n",
      "epoch no.4 train no.342520  loss = 3.36031 avg_loss = 3.09113\n",
      "epoch no.4 train no.342530  loss = 2.79962 avg_loss = 3.11075\n",
      "epoch no.4 train no.342540  loss = 3.20118 avg_loss = 3.11906\n",
      "epoch no.4 train no.342550  loss = 3.61403 avg_loss = 3.11545\n",
      "epoch no.4 train no.342560  loss = 3.12342 avg_loss = 3.10257\n",
      "epoch no.4 train no.342570  loss = 2.48059 avg_loss = 3.11334\n",
      "epoch no.4 train no.342580  loss = 3.95823 avg_loss = 3.12411\n",
      "epoch no.4 train no.342590  loss = 2.10574 avg_loss = 3.10188\n",
      "epoch no.4 train no.342600  loss = 3.22379 avg_loss = 3.14637\n",
      "epoch no.4 train no.342610  loss = 2.56830 avg_loss = 3.17985\n",
      "epoch no.4 train no.342620  loss = 2.72320 avg_loss = 3.18523\n",
      "epoch no.4 train no.342630  loss = 2.25909 avg_loss = 3.13765\n",
      "epoch no.4 train no.342640  loss = 2.71212 avg_loss = 3.13490\n",
      "epoch no.4 train no.342650  loss = 2.76132 avg_loss = 3.12334\n",
      "epoch no.4 train no.342660  loss = 2.01789 avg_loss = 3.10267\n",
      "epoch no.4 train no.342670  loss = 2.10997 avg_loss = 3.14018\n",
      "epoch no.4 train no.342680  loss = 2.05844 avg_loss = 3.09970\n",
      "epoch no.4 train no.342690  loss = 2.07754 avg_loss = 3.10213\n",
      "epoch no.4 train no.342700  loss = 4.26398 avg_loss = 3.13715\n",
      "epoch no.4 train no.342710  loss = 2.35824 avg_loss = 3.13905\n",
      "epoch no.4 train no.342720  loss = 3.53684 avg_loss = 3.13939\n",
      "epoch no.4 train no.342730  loss = 3.15238 avg_loss = 3.14362\n",
      "epoch no.4 train no.342740  loss = 3.59594 avg_loss = 3.20640\n",
      "epoch no.4 train no.342750  loss = 3.74101 avg_loss = 3.16586\n",
      "epoch no.4 train no.342760  loss = 2.63438 avg_loss = 3.17664\n",
      "epoch no.4 train no.342770  loss = 2.74958 avg_loss = 3.18429\n",
      "epoch no.4 train no.342780  loss = 3.05411 avg_loss = 3.21284\n",
      "epoch no.4 train no.342790  loss = 4.27674 avg_loss = 3.23631\n",
      "epoch no.4 train no.342800  loss = 3.77368 avg_loss = 3.18672\n",
      "epoch no.4 train no.342810  loss = 3.89064 avg_loss = 3.16844\n",
      "epoch no.4 train no.342820  loss = 2.61983 avg_loss = 3.17874\n",
      "epoch no.4 train no.342830  loss = 2.44813 avg_loss = 3.17308\n",
      "epoch no.4 train no.342840  loss = 2.68072 avg_loss = 3.19670\n",
      "epoch no.4 train no.342850  loss = 2.79662 avg_loss = 3.16684\n",
      "epoch no.4 train no.342860  loss = 1.96530 avg_loss = 3.13567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.342870  loss = 3.63094 avg_loss = 3.10683\n",
      "epoch no.4 train no.342880  loss = 3.46558 avg_loss = 3.15543\n",
      "epoch no.4 train no.342890  loss = 3.48510 avg_loss = 3.15927\n",
      "epoch no.4 train no.342900  loss = 2.02300 avg_loss = 3.10259\n",
      "epoch no.4 train no.342910  loss = 4.38873 avg_loss = 3.12352\n",
      "epoch no.4 train no.342920  loss = 2.93199 avg_loss = 3.10943\n",
      "epoch no.4 train no.342930  loss = 2.67447 avg_loss = 3.15839\n",
      "epoch no.4 train no.342940  loss = 2.42036 avg_loss = 3.09360\n",
      "epoch no.4 train no.342950  loss = 6.17676 avg_loss = 3.14397\n",
      "epoch no.4 train no.342960  loss = 2.63160 avg_loss = 3.16816\n",
      "epoch no.4 train no.342970  loss = 2.30169 avg_loss = 3.15259\n",
      "epoch no.4 train no.342980  loss = 3.15926 avg_loss = 3.14775\n",
      "epoch no.4 train no.342990  loss = 4.41524 avg_loss = 3.19793\n",
      "epoch no.4 train no.343000  loss = 2.30685 avg_loss = 3.16374\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.343010  loss = 3.57773 avg_loss = 3.16008\n",
      "epoch no.4 train no.343020  loss = 2.50472 avg_loss = 3.16253\n",
      "epoch no.4 train no.343030  loss = 3.78171 avg_loss = 3.18931\n",
      "epoch no.4 train no.343040  loss = 3.10523 avg_loss = 3.20325\n",
      "epoch no.4 train no.343050  loss = 4.17519 avg_loss = 3.22290\n",
      "epoch no.4 train no.343060  loss = 2.50980 avg_loss = 3.20554\n",
      "epoch no.4 train no.343070  loss = 2.54193 avg_loss = 3.19203\n",
      "epoch no.4 train no.343080  loss = 3.46043 avg_loss = 3.22984\n",
      "epoch no.4 train no.343090  loss = 2.57490 avg_loss = 3.22113\n",
      "epoch no.4 train no.343100  loss = 2.67167 avg_loss = 3.22672\n",
      "epoch no.4 train no.343110  loss = 3.10359 avg_loss = 3.24355\n",
      "epoch no.4 train no.343120  loss = 2.38146 avg_loss = 3.21779\n",
      "epoch no.4 train no.343130  loss = 4.40924 avg_loss = 3.22420\n",
      "epoch no.4 train no.343140  loss = 4.37208 avg_loss = 3.20595\n",
      "epoch no.4 train no.343150  loss = 5.45284 avg_loss = 3.22118\n",
      "epoch no.4 train no.343160  loss = 2.75361 avg_loss = 3.18975\n",
      "epoch no.4 train no.343170  loss = 3.31497 avg_loss = 3.18622\n",
      "epoch no.4 train no.343180  loss = 2.10516 avg_loss = 3.17567\n",
      "epoch no.4 train no.343190  loss = 2.18294 avg_loss = 3.15866\n",
      "epoch no.4 train no.343200  loss = 3.07220 avg_loss = 3.11490\n",
      "epoch no.4 train no.343210  loss = 2.57683 avg_loss = 3.14648\n",
      "epoch no.4 train no.343220  loss = 3.90559 avg_loss = 3.14780\n",
      "epoch no.4 train no.343230  loss = 2.15112 avg_loss = 3.13926\n",
      "epoch no.4 train no.343240  loss = 2.02749 avg_loss = 3.13242\n",
      "epoch no.4 train no.343250  loss = 4.61245 avg_loss = 3.16158\n",
      "epoch no.4 train no.343260  loss = 3.30468 avg_loss = 3.19621\n",
      "epoch no.4 train no.343270  loss = 2.02321 avg_loss = 3.14897\n",
      "epoch no.4 train no.343280  loss = 3.16786 avg_loss = 3.18516\n",
      "epoch no.4 train no.343290  loss = 3.76882 avg_loss = 3.16877\n",
      "epoch no.4 train no.343300  loss = 1.69022 avg_loss = 3.18314\n",
      "epoch no.4 train no.343310  loss = 2.19266 avg_loss = 3.19887\n",
      "epoch no.4 train no.343320  loss = 2.45379 avg_loss = 3.15694\n",
      "epoch no.4 train no.343330  loss = 2.62629 avg_loss = 3.15929\n",
      "epoch no.4 train no.343340  loss = 2.14883 avg_loss = 3.14605\n",
      "epoch no.4 train no.343350  loss = 2.77717 avg_loss = 3.15727\n",
      "epoch no.4 train no.343360  loss = 2.87910 avg_loss = 3.16207\n",
      "epoch no.4 train no.343370  loss = 3.87258 avg_loss = 3.20203\n",
      "epoch no.4 train no.343380  loss = 3.15788 avg_loss = 3.21146\n",
      "epoch no.4 train no.343390  loss = 3.67564 avg_loss = 3.18469\n",
      "epoch no.4 train no.343400  loss = 2.17381 avg_loss = 3.16068\n",
      "epoch no.4 train no.343410  loss = 2.73029 avg_loss = 3.18568\n",
      "epoch no.4 train no.343420  loss = 4.25609 avg_loss = 3.23643\n",
      "epoch no.4 train no.343430  loss = 3.14853 avg_loss = 3.27058\n",
      "epoch no.4 train no.343440  loss = 3.21555 avg_loss = 3.27506\n",
      "epoch no.4 train no.343450  loss = 1.88514 avg_loss = 3.23085\n",
      "epoch no.4 train no.343460  loss = 3.34539 avg_loss = 3.23526\n",
      "epoch no.4 train no.343470  loss = 2.45681 avg_loss = 3.17551\n",
      "epoch no.4 train no.343480  loss = 4.14029 avg_loss = 3.19764\n",
      "epoch no.4 train no.343490  loss = 2.08083 avg_loss = 3.17477\n",
      "epoch no.4 train no.343500  loss = 2.94749 avg_loss = 3.18905\n",
      "epoch no.4 train no.343510  loss = 2.85821 avg_loss = 3.19772\n",
      "epoch no.4 train no.343520  loss = 2.28355 avg_loss = 3.17129\n",
      "epoch no.4 train no.343530  loss = 2.82284 avg_loss = 3.19385\n",
      "epoch no.4 train no.343540  loss = 3.53965 avg_loss = 3.20043\n",
      "epoch no.4 train no.343550  loss = 2.47351 avg_loss = 3.26640\n",
      "epoch no.4 train no.343560  loss = 3.30947 avg_loss = 3.26184\n",
      "epoch no.4 train no.343570  loss = 3.17833 avg_loss = 3.22938\n",
      "epoch no.4 train no.343580  loss = 3.94988 avg_loss = 3.19570\n",
      "epoch no.4 train no.343590  loss = 1.58245 avg_loss = 3.15246\n",
      "epoch no.4 train no.343600  loss = 2.22393 avg_loss = 3.15016\n",
      "epoch no.4 train no.343610  loss = 2.67358 avg_loss = 3.12556\n",
      "epoch no.4 train no.343620  loss = 3.66700 avg_loss = 3.12972\n",
      "epoch no.4 train no.343630  loss = 3.92785 avg_loss = 3.20377\n",
      "epoch no.4 train no.343640  loss = 4.00848 avg_loss = 3.21464\n",
      "epoch no.4 train no.343650  loss = 2.97327 avg_loss = 3.18364\n",
      "epoch no.4 train no.343660  loss = 1.80499 avg_loss = 3.16570\n",
      "epoch no.4 train no.343670  loss = 2.46603 avg_loss = 3.20380\n",
      "epoch no.4 train no.343680  loss = 3.62208 avg_loss = 3.21890\n",
      "epoch no.4 train no.343690  loss = 2.97597 avg_loss = 3.15542\n",
      "epoch no.4 train no.343700  loss = 3.40877 avg_loss = 3.18747\n",
      "epoch no.4 train no.343710  loss = 3.03312 avg_loss = 3.20283\n",
      "epoch no.4 train no.343720  loss = 2.77278 avg_loss = 3.23464\n",
      "epoch no.4 train no.343730  loss = 2.62251 avg_loss = 3.25076\n",
      "epoch no.4 train no.343740  loss = 2.15189 avg_loss = 3.23170\n",
      "epoch no.4 train no.343750  loss = 3.23168 avg_loss = 3.20478\n",
      "epoch no.4 train no.343760  loss = 4.30395 avg_loss = 3.23158\n",
      "epoch no.4 train no.343770  loss = 2.59826 avg_loss = 3.22356\n",
      "epoch no.4 train no.343780  loss = 2.74394 avg_loss = 3.23115\n",
      "epoch no.4 train no.343790  loss = 3.27511 avg_loss = 3.26787\n",
      "epoch no.4 train no.343800  loss = 3.57563 avg_loss = 3.26576\n",
      "epoch no.4 train no.343810  loss = 2.62726 avg_loss = 3.21210\n",
      "epoch no.4 train no.343820  loss = 3.47301 avg_loss = 3.23429\n",
      "epoch no.4 train no.343830  loss = 2.41920 avg_loss = 3.24743\n",
      "epoch no.4 train no.343840  loss = 2.92664 avg_loss = 3.22384\n",
      "epoch no.4 train no.343850  loss = 3.23444 avg_loss = 3.22897\n",
      "epoch no.4 train no.343860  loss = 4.47024 avg_loss = 3.26898\n",
      "epoch no.4 train no.343870  loss = 3.58227 avg_loss = 3.25096\n",
      "epoch no.4 train no.343880  loss = 3.26901 avg_loss = 3.23538\n",
      "epoch no.4 train no.343890  loss = 3.32556 avg_loss = 3.19088\n",
      "epoch no.4 train no.343900  loss = 4.24795 avg_loss = 3.19577\n",
      "epoch no.4 train no.343910  loss = 3.38278 avg_loss = 3.17373\n",
      "epoch no.4 train no.343920  loss = 4.10464 avg_loss = 3.23158\n",
      "epoch no.4 train no.343930  loss = 3.12520 avg_loss = 3.21610\n",
      "epoch no.4 train no.343940  loss = 2.89663 avg_loss = 3.21208\n",
      "epoch no.4 train no.343950  loss = 5.39689 avg_loss = 3.25282\n",
      "epoch no.4 train no.343960  loss = 2.69362 avg_loss = 3.23416\n",
      "epoch no.4 train no.343970  loss = 3.57843 avg_loss = 3.25001\n",
      "epoch no.4 train no.343980  loss = 3.70588 avg_loss = 3.26339\n",
      "epoch no.4 train no.343990  loss = 4.45373 avg_loss = 3.24219\n",
      "epoch no.4 train no.344000  loss = 5.18754 avg_loss = 3.27117\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁모음', '▁90', '</s>']\n",
      "추억의 발라드 모음곡</s>\n",
      "epoch no.4 train no.344010  loss = 2.18594 avg_loss = 3.26452\n",
      "epoch no.4 train no.344020  loss = 3.94132 avg_loss = 3.24526\n",
      "epoch no.4 train no.344030  loss = 2.16516 avg_loss = 3.23029\n",
      "epoch no.4 train no.344040  loss = 2.29740 avg_loss = 3.23399\n",
      "epoch no.4 train no.344050  loss = 2.67464 avg_loss = 3.19985\n",
      "epoch no.4 train no.344060  loss = 2.25283 avg_loss = 3.16884\n",
      "epoch no.4 train no.344070  loss = 2.77846 avg_loss = 3.17406\n",
      "epoch no.4 train no.344080  loss = 3.39284 avg_loss = 3.16087\n",
      "epoch no.4 train no.344090  loss = 3.24743 avg_loss = 3.18531\n",
      "epoch no.4 train no.344100  loss = 2.87331 avg_loss = 3.19666\n",
      "epoch no.4 train no.344110  loss = 2.50508 avg_loss = 3.18002\n",
      "epoch no.4 train no.344120  loss = 3.04307 avg_loss = 3.18363\n",
      "epoch no.4 train no.344130  loss = 3.42762 avg_loss = 3.17416\n",
      "epoch no.4 train no.344140  loss = 3.02105 avg_loss = 3.17471\n",
      "epoch no.4 train no.344150  loss = 1.89946 avg_loss = 3.18407\n",
      "epoch no.4 train no.344160  loss = 3.14156 avg_loss = 3.18102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.344170  loss = 2.70411 avg_loss = 3.17558\n",
      "epoch no.4 train no.344180  loss = 2.46943 avg_loss = 3.18234\n",
      "epoch no.4 train no.344190  loss = 4.95454 avg_loss = 3.21763\n",
      "epoch no.4 train no.344200  loss = 3.62063 avg_loss = 3.23553\n",
      "epoch no.4 train no.344210  loss = 1.79499 avg_loss = 3.21875\n",
      "epoch no.4 train no.344220  loss = 2.14710 avg_loss = 3.19882\n",
      "epoch no.4 train no.344230  loss = 4.71511 avg_loss = 3.22241\n",
      "epoch no.4 train no.344240  loss = 4.06348 avg_loss = 3.27411\n",
      "epoch no.4 train no.344250  loss = 5.38744 avg_loss = 3.25983\n",
      "epoch no.4 train no.344260  loss = 2.69464 avg_loss = 3.20428\n",
      "epoch no.4 train no.344270  loss = 4.30875 avg_loss = 3.24348\n",
      "epoch no.4 train no.344280  loss = 3.97168 avg_loss = 3.23555\n",
      "epoch no.4 train no.344290  loss = 2.40413 avg_loss = 3.19921\n",
      "epoch no.4 train no.344300  loss = 2.97631 avg_loss = 3.17321\n",
      "epoch no.4 train no.344310  loss = 3.63916 avg_loss = 3.11368\n",
      "epoch no.4 train no.344320  loss = 3.07598 avg_loss = 3.18944\n",
      "epoch no.4 train no.344330  loss = 3.36358 avg_loss = 3.20337\n",
      "epoch no.4 train no.344340  loss = 1.74464 avg_loss = 3.19820\n",
      "epoch no.4 train no.344350  loss = 3.01746 avg_loss = 3.20827\n",
      "epoch no.4 train no.344360  loss = 3.94510 avg_loss = 3.23738\n",
      "epoch no.4 train no.344370  loss = 2.60760 avg_loss = 3.25013\n",
      "epoch no.4 train no.344380  loss = 1.90185 avg_loss = 3.23410\n",
      "epoch no.4 train no.344390  loss = 3.68293 avg_loss = 3.24933\n",
      "epoch no.4 train no.344400  loss = 2.58953 avg_loss = 3.30057\n",
      "epoch no.4 train no.344410  loss = 3.10141 avg_loss = 3.31455\n",
      "epoch no.4 train no.344420  loss = 2.78252 avg_loss = 3.24983\n",
      "epoch no.4 train no.344430  loss = 2.87298 avg_loss = 3.24436\n",
      "epoch no.4 train no.344440  loss = 3.53074 avg_loss = 3.24363\n",
      "epoch no.4 train no.344450  loss = 2.12011 avg_loss = 3.17400\n",
      "epoch no.4 train no.344460  loss = 5.04309 avg_loss = 3.23029\n",
      "epoch no.4 train no.344470  loss = 4.97821 avg_loss = 3.22161\n",
      "epoch no.4 train no.344480  loss = 3.09887 avg_loss = 3.22758\n",
      "epoch no.4 train no.344490  loss = 4.38295 avg_loss = 3.24799\n",
      "epoch no.4 train no.344500  loss = 2.42727 avg_loss = 3.20714\n",
      "epoch no.4 train no.344510  loss = 1.56790 avg_loss = 3.19348\n",
      "epoch no.4 train no.344520  loss = 4.18520 avg_loss = 3.21450\n",
      "epoch no.4 train no.344530  loss = 2.22498 avg_loss = 3.21470\n",
      "epoch no.4 train no.344540  loss = 2.76604 avg_loss = 3.16470\n",
      "epoch no.4 train no.344550  loss = 2.54298 avg_loss = 3.14856\n",
      "epoch no.4 train no.344560  loss = 3.33238 avg_loss = 3.14010\n",
      "epoch no.4 train no.344570  loss = 3.59457 avg_loss = 3.18089\n",
      "epoch no.4 train no.344580  loss = 3.00489 avg_loss = 3.14568\n",
      "epoch no.4 train no.344590  loss = 4.79174 avg_loss = 3.17581\n",
      "epoch no.4 train no.344600  loss = 4.11405 avg_loss = 3.22960\n",
      "epoch no.4 train no.344610  loss = 2.95077 avg_loss = 3.23087\n",
      "epoch no.4 train no.344620  loss = 2.90337 avg_loss = 3.21718\n",
      "epoch no.4 train no.344630  loss = 4.74925 avg_loss = 3.26458\n",
      "epoch no.4 train no.344640  loss = 1.94231 avg_loss = 3.27393\n",
      "epoch no.4 train no.344650  loss = 1.98924 avg_loss = 3.24322\n",
      "epoch no.4 train no.344660  loss = 1.33465 avg_loss = 3.24682\n",
      "epoch no.4 train no.344670  loss = 2.31099 avg_loss = 3.22373\n",
      "epoch no.4 train no.344680  loss = 4.82230 avg_loss = 3.26125\n",
      "epoch no.4 train no.344690  loss = 4.47350 avg_loss = 3.31241\n",
      "epoch no.4 train no.344700  loss = 3.52570 avg_loss = 3.30151\n",
      "epoch no.4 train no.344710  loss = 2.45605 avg_loss = 3.28407\n",
      "epoch no.4 train no.344720  loss = 3.56889 avg_loss = 3.31585\n",
      "epoch no.4 train no.344730  loss = 3.01278 avg_loss = 3.27294\n",
      "epoch no.4 train no.344740  loss = 2.24938 avg_loss = 3.24596\n",
      "epoch no.4 train no.344750  loss = 2.16103 avg_loss = 3.18578\n",
      "epoch no.4 train no.344760  loss = 2.65822 avg_loss = 3.17623\n",
      "epoch no.4 train no.344770  loss = 3.26765 avg_loss = 3.18752\n",
      "epoch no.4 train no.344780  loss = 2.31644 avg_loss = 3.18010\n",
      "epoch no.4 train no.344790  loss = 4.93175 avg_loss = 3.18405\n",
      "epoch no.4 train no.344800  loss = 1.80081 avg_loss = 3.19028\n",
      "epoch no.4 train no.344810  loss = 2.33489 avg_loss = 3.18827\n",
      "epoch no.4 train no.344820  loss = 2.92418 avg_loss = 3.18583\n",
      "epoch no.4 train no.344830  loss = 3.66526 avg_loss = 3.21365\n",
      "epoch no.4 train no.344840  loss = 2.69593 avg_loss = 3.22307\n",
      "epoch no.4 train no.344850  loss = 3.01371 avg_loss = 3.24012\n",
      "epoch no.4 train no.344860  loss = 2.79428 avg_loss = 3.21331\n",
      "epoch no.4 train no.344870  loss = 2.70308 avg_loss = 3.18212\n",
      "epoch no.4 train no.344880  loss = 3.04336 avg_loss = 3.20161\n",
      "epoch no.4 train no.344890  loss = 3.35115 avg_loss = 3.21881\n",
      "epoch no.4 train no.344900  loss = 2.33226 avg_loss = 3.21550\n",
      "epoch no.4 train no.344910  loss = 2.27266 avg_loss = 3.20019\n",
      "epoch no.4 train no.344920  loss = 2.55128 avg_loss = 3.15198\n",
      "epoch no.4 train no.344930  loss = 4.14742 avg_loss = 3.17480\n",
      "epoch no.4 train no.344940  loss = 2.81084 avg_loss = 3.13084\n",
      "epoch no.4 train no.344950  loss = 2.32739 avg_loss = 3.11657\n",
      "epoch no.4 train no.344960  loss = 2.58613 avg_loss = 3.10805\n",
      "epoch no.4 train no.344970  loss = 2.88502 avg_loss = 3.10812\n",
      "epoch no.4 train no.344980  loss = 3.73415 avg_loss = 3.10446\n",
      "epoch no.4 train no.344990  loss = 3.79633 avg_loss = 3.14845\n",
      "epoch no.4 train no.345000  loss = 3.40352 avg_loss = 3.16116\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁가요', '곡', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.4 train no.345010  loss = 2.84981 avg_loss = 3.20376\n",
      "epoch no.4 train no.345020  loss = 2.98224 avg_loss = 3.17457\n",
      "epoch no.4 train no.345030  loss = 3.21493 avg_loss = 3.21493\n",
      "epoch no.4 train no.345040  loss = 2.75486 avg_loss = 3.19022\n",
      "epoch no.4 train no.345050  loss = 2.72192 avg_loss = 3.17565\n",
      "epoch no.4 train no.345060  loss = 3.52293 avg_loss = 3.16754\n",
      "epoch no.4 train no.345070  loss = 4.65077 avg_loss = 3.14600\n",
      "epoch no.4 train no.345080  loss = 3.50175 avg_loss = 3.13435\n",
      "epoch no.4 train no.345090  loss = 2.98048 avg_loss = 3.16568\n",
      "epoch no.4 train no.345100  loss = 3.60093 avg_loss = 3.18277\n",
      "epoch no.4 train no.345110  loss = 3.95773 avg_loss = 3.14311\n",
      "epoch no.4 train no.345120  loss = 2.63326 avg_loss = 3.11784\n",
      "epoch no.4 train no.345130  loss = 3.46176 avg_loss = 3.11277\n",
      "epoch no.4 train no.345140  loss = 2.50137 avg_loss = 3.15234\n",
      "epoch no.4 train no.345150  loss = 4.06024 avg_loss = 3.17624\n",
      "epoch no.4 train no.345160  loss = 3.11441 avg_loss = 3.12397\n",
      "epoch no.4 train no.345170  loss = 1.83864 avg_loss = 3.11358\n",
      "epoch no.4 train no.345180  loss = 3.04167 avg_loss = 3.10892\n",
      "epoch no.4 train no.345190  loss = 2.45387 avg_loss = 3.09608\n",
      "epoch no.4 train no.345200  loss = 5.51068 avg_loss = 3.11418\n",
      "epoch no.4 train no.345210  loss = 1.73380 avg_loss = 3.08432\n",
      "epoch no.4 train no.345220  loss = 3.63543 avg_loss = 3.13163\n",
      "epoch no.4 train no.345230  loss = 4.18543 avg_loss = 3.16713\n",
      "epoch no.4 train no.345240  loss = 4.66280 avg_loss = 3.20951\n",
      "epoch no.4 train no.345250  loss = 2.34916 avg_loss = 3.16046\n",
      "epoch no.4 train no.345260  loss = 5.08260 avg_loss = 3.19029\n",
      "epoch no.4 train no.345270  loss = 4.40874 avg_loss = 3.24251\n",
      "epoch no.4 train no.345280  loss = 2.63327 avg_loss = 3.26829\n",
      "epoch no.4 train no.345290  loss = 2.31201 avg_loss = 3.24508\n",
      "epoch no.4 train no.345300  loss = 3.00341 avg_loss = 3.26955\n",
      "epoch no.4 train no.345310  loss = 4.16882 avg_loss = 3.29903\n",
      "epoch no.4 train no.345320  loss = 3.63555 avg_loss = 3.26353\n",
      "epoch no.4 train no.345330  loss = 3.39812 avg_loss = 3.22746\n",
      "epoch no.4 train no.345340  loss = 3.21617 avg_loss = 3.30222\n",
      "epoch no.4 train no.345350  loss = 3.14131 avg_loss = 3.33032\n",
      "epoch no.4 train no.345360  loss = 2.54882 avg_loss = 3.29488\n",
      "epoch no.4 train no.345370  loss = 4.40559 avg_loss = 3.29731\n",
      "epoch no.4 train no.345380  loss = 2.11687 avg_loss = 3.28812\n",
      "epoch no.4 train no.345390  loss = 2.32439 avg_loss = 3.23675\n",
      "epoch no.4 train no.345400  loss = 3.84314 avg_loss = 3.27763\n",
      "epoch no.4 train no.345410  loss = 3.06180 avg_loss = 3.30939\n",
      "epoch no.4 train no.345420  loss = 4.00747 avg_loss = 3.34657\n",
      "epoch no.4 train no.345430  loss = 4.32008 avg_loss = 3.33000\n",
      "epoch no.4 train no.345440  loss = 4.26663 avg_loss = 3.34202\n",
      "epoch no.4 train no.345450  loss = 2.45064 avg_loss = 3.31685\n",
      "epoch no.4 train no.345460  loss = 2.60529 avg_loss = 3.29224\n",
      "epoch no.4 train no.345470  loss = 3.91869 avg_loss = 3.30820\n",
      "epoch no.4 train no.345480  loss = 5.27457 avg_loss = 3.36107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.345490  loss = 2.51596 avg_loss = 3.31837\n",
      "epoch no.4 train no.345500  loss = 3.02433 avg_loss = 3.27673\n",
      "epoch no.4 train no.345510  loss = 2.23942 avg_loss = 3.27313\n",
      "epoch no.4 train no.345520  loss = 2.51132 avg_loss = 3.27338\n",
      "epoch no.4 train no.345530  loss = 4.63093 avg_loss = 3.29171\n",
      "epoch no.4 train no.345540  loss = 3.66342 avg_loss = 3.27306\n",
      "epoch no.4 train no.345550  loss = 3.72060 avg_loss = 3.29862\n",
      "epoch no.4 train no.345560  loss = 4.29207 avg_loss = 3.34426\n",
      "epoch no.4 train no.345570  loss = 3.05803 avg_loss = 3.26162\n",
      "epoch no.4 train no.345580  loss = 3.78334 avg_loss = 3.23623\n",
      "epoch no.4 train no.345590  loss = 4.26600 avg_loss = 3.21007\n",
      "epoch no.4 train no.345600  loss = 3.38846 avg_loss = 3.21949\n",
      "epoch no.4 train no.345610  loss = 1.89958 avg_loss = 3.22830\n",
      "epoch no.4 train no.345620  loss = 2.31051 avg_loss = 3.27162\n",
      "epoch no.4 train no.345630  loss = 2.68191 avg_loss = 3.27760\n",
      "epoch no.4 train no.345640  loss = 3.14729 avg_loss = 3.27849\n",
      "epoch no.4 train no.345650  loss = 4.51368 avg_loss = 3.26474\n",
      "epoch no.4 train no.345660  loss = 3.20720 avg_loss = 3.29789\n",
      "epoch no.4 train no.345670  loss = 3.24995 avg_loss = 3.31572\n",
      "epoch no.4 train no.345680  loss = 3.28989 avg_loss = 3.30389\n",
      "epoch no.4 train no.345690  loss = 2.68090 avg_loss = 3.34406\n",
      "epoch no.4 train no.345700  loss = 3.05970 avg_loss = 3.37591\n",
      "epoch no.4 train no.345710  loss = 3.08312 avg_loss = 3.35137\n",
      "epoch no.4 train no.345720  loss = 3.11063 avg_loss = 3.34193\n",
      "epoch no.4 train no.345730  loss = 2.43317 avg_loss = 3.36488\n",
      "epoch no.4 train no.345740  loss = 2.72795 avg_loss = 3.33942\n",
      "epoch no.4 train no.345750  loss = 2.93628 avg_loss = 3.25370\n",
      "epoch no.4 train no.345760  loss = 3.02404 avg_loss = 3.28661\n",
      "epoch no.4 train no.345770  loss = 3.19486 avg_loss = 3.28256\n",
      "epoch no.4 train no.345780  loss = 2.11714 avg_loss = 3.24721\n",
      "epoch no.4 train no.345790  loss = 3.45254 avg_loss = 3.25209\n",
      "epoch no.4 train no.345800  loss = 2.79346 avg_loss = 3.27508\n",
      "epoch no.4 train no.345810  loss = 3.91788 avg_loss = 3.28951\n",
      "epoch no.4 train no.345820  loss = 4.28349 avg_loss = 3.29316\n",
      "epoch no.4 train no.345830  loss = 3.09609 avg_loss = 3.32839\n",
      "epoch no.4 train no.345840  loss = 2.59348 avg_loss = 3.36792\n",
      "epoch no.4 train no.345850  loss = 3.81848 avg_loss = 3.33845\n",
      "epoch no.4 train no.345860  loss = 3.60724 avg_loss = 3.34117\n",
      "epoch no.4 train no.345870  loss = 4.83157 avg_loss = 3.34290\n",
      "epoch no.4 train no.345880  loss = 2.40062 avg_loss = 3.28086\n",
      "epoch no.4 train no.345890  loss = 2.13884 avg_loss = 3.27862\n",
      "epoch no.4 train no.345900  loss = 4.06432 avg_loss = 3.27906\n",
      "epoch no.4 train no.345910  loss = 3.59470 avg_loss = 3.24589\n",
      "epoch no.4 train no.345920  loss = 3.36624 avg_loss = 3.27215\n",
      "epoch no.4 train no.345930  loss = 2.28585 avg_loss = 3.27241\n",
      "epoch no.4 train no.345940  loss = 2.80040 avg_loss = 3.28146\n",
      "epoch no.4 train no.345950  loss = 5.47735 avg_loss = 3.34275\n",
      "epoch no.4 train no.345960  loss = 2.05097 avg_loss = 3.30389\n",
      "epoch no.4 train no.345970  loss = 2.86026 avg_loss = 3.31401\n",
      "epoch no.4 train no.345980  loss = 1.97840 avg_loss = 3.28306\n",
      "epoch no.4 train no.345990  loss = 3.06637 avg_loss = 3.27623\n",
      "epoch no.4 train no.346000  loss = 5.15073 avg_loss = 3.27292\n",
      "4\n",
      "to_tokens: ['▁가을', '▁그', '시절', '이', '드', '</s>']\n",
      "추억의 그 시절 발라드</s>\n",
      "epoch no.4 train no.346010  loss = 4.53769 avg_loss = 3.28705\n",
      "epoch no.4 train no.346020  loss = 2.57215 avg_loss = 3.30429\n",
      "epoch no.4 train no.346030  loss = 1.88001 avg_loss = 3.30404\n",
      "epoch no.4 train no.346040  loss = 3.25196 avg_loss = 3.26137\n",
      "epoch no.4 train no.346050  loss = 3.30499 avg_loss = 3.26933\n",
      "epoch no.4 train no.346060  loss = 2.84808 avg_loss = 3.25519\n",
      "epoch no.4 train no.346070  loss = 2.61350 avg_loss = 3.30366\n",
      "epoch no.4 train no.346080  loss = 3.85936 avg_loss = 3.28502\n",
      "epoch no.4 train no.346090  loss = 2.70516 avg_loss = 3.24804\n",
      "epoch no.4 train no.346100  loss = 2.13032 avg_loss = 3.27887\n",
      "epoch no.4 train no.346110  loss = 1.81891 avg_loss = 3.23356\n",
      "epoch no.4 train no.346120  loss = 2.17241 avg_loss = 3.24588\n",
      "epoch no.4 train no.346130  loss = 3.42916 avg_loss = 3.24988\n",
      "epoch no.4 train no.346140  loss = 4.35297 avg_loss = 3.22734\n",
      "epoch no.4 train no.346150  loss = 2.39531 avg_loss = 3.27277\n",
      "epoch no.4 train no.346160  loss = 3.90998 avg_loss = 3.26107\n",
      "epoch no.4 train no.346170  loss = 3.33377 avg_loss = 3.24021\n",
      "epoch no.4 train no.346180  loss = 2.24826 avg_loss = 3.21123\n",
      "epoch no.4 train no.346190  loss = 3.31601 avg_loss = 3.20931\n",
      "epoch no.4 train no.346200  loss = 2.86080 avg_loss = 3.15589\n",
      "epoch no.4 train no.346210  loss = 2.21979 avg_loss = 3.16110\n",
      "epoch no.4 train no.346220  loss = 2.68654 avg_loss = 3.13493\n",
      "epoch no.4 train no.346230  loss = 2.27045 avg_loss = 3.10602\n",
      "epoch no.4 train no.346240  loss = 3.57731 avg_loss = 3.11393\n",
      "epoch no.4 train no.346250  loss = 4.36622 avg_loss = 3.09805\n",
      "epoch no.4 train no.346260  loss = 3.07451 avg_loss = 3.09517\n",
      "epoch no.4 train no.346270  loss = 3.28795 avg_loss = 3.08623\n",
      "epoch no.4 train no.346280  loss = 2.99531 avg_loss = 3.09817\n",
      "epoch no.4 train no.346290  loss = 2.89093 avg_loss = 3.09151\n",
      "epoch no.4 train no.346300  loss = 4.16010 avg_loss = 3.10166\n",
      "epoch no.4 train no.346310  loss = 2.45578 avg_loss = 3.05040\n",
      "epoch no.4 train no.346320  loss = 2.36749 avg_loss = 3.11355\n",
      "epoch no.4 train no.346330  loss = 3.48137 avg_loss = 3.12315\n",
      "epoch no.4 train no.346340  loss = 3.41156 avg_loss = 3.15251\n",
      "epoch no.4 train no.346350  loss = 3.83203 avg_loss = 3.16870\n",
      "epoch no.4 train no.346360  loss = 2.66981 avg_loss = 3.20439\n",
      "epoch no.4 train no.346370  loss = 3.16385 avg_loss = 3.20754\n",
      "epoch no.4 train no.346380  loss = 3.58185 avg_loss = 3.24054\n",
      "epoch no.4 train no.346390  loss = 2.40416 avg_loss = 3.25975\n",
      "epoch no.4 train no.346400  loss = 3.40042 avg_loss = 3.24918\n",
      "epoch no.4 train no.346410  loss = 3.66139 avg_loss = 3.22086\n",
      "epoch no.4 train no.346420  loss = 2.89749 avg_loss = 3.21067\n",
      "epoch no.4 train no.346430  loss = 3.29857 avg_loss = 3.22334\n",
      "epoch no.4 train no.346440  loss = 3.45243 avg_loss = 3.19032\n",
      "epoch no.4 train no.346450  loss = 3.90138 avg_loss = 3.22392\n",
      "epoch no.4 train no.346460  loss = 3.01769 avg_loss = 3.18338\n",
      "epoch no.4 train no.346470  loss = 3.57962 avg_loss = 3.15819\n",
      "epoch no.4 train no.346480  loss = 4.51607 avg_loss = 3.21011\n",
      "epoch no.4 train no.346490  loss = 4.34306 avg_loss = 3.22901\n",
      "epoch no.4 train no.346500  loss = 3.30583 avg_loss = 3.17869\n",
      "epoch no.4 train no.346510  loss = 3.93977 avg_loss = 3.19431\n",
      "epoch no.4 train no.346520  loss = 5.27018 avg_loss = 3.15997\n",
      "epoch no.4 train no.346530  loss = 3.78144 avg_loss = 3.21298\n",
      "epoch no.4 train no.346540  loss = 3.20941 avg_loss = 3.23577\n",
      "epoch no.4 train no.346550  loss = 2.32577 avg_loss = 3.21737\n",
      "epoch no.4 train no.346560  loss = 2.86721 avg_loss = 3.18499\n",
      "epoch no.4 train no.346570  loss = 3.70743 avg_loss = 3.23326\n",
      "epoch no.4 train no.346580  loss = 3.73916 avg_loss = 3.20176\n",
      "epoch no.4 train no.346590  loss = 2.53411 avg_loss = 3.15242\n",
      "epoch no.4 train no.346600  loss = 2.90673 avg_loss = 3.13978\n",
      "epoch no.4 train no.346610  loss = 2.85533 avg_loss = 3.12971\n",
      "epoch no.4 train no.346620  loss = 1.83833 avg_loss = 3.13673\n",
      "epoch no.4 train no.346630  loss = 2.54286 avg_loss = 3.16046\n",
      "epoch no.4 train no.346640  loss = 2.55226 avg_loss = 3.16856\n",
      "epoch no.4 train no.346650  loss = 3.79944 avg_loss = 3.16332\n",
      "epoch no.4 train no.346660  loss = 2.56824 avg_loss = 3.13803\n",
      "epoch no.4 train no.346670  loss = 2.42952 avg_loss = 3.16960\n",
      "epoch no.4 train no.346680  loss = 2.84009 avg_loss = 3.15001\n",
      "epoch no.4 train no.346690  loss = 3.56887 avg_loss = 3.14714\n",
      "epoch no.4 train no.346700  loss = 4.21717 avg_loss = 3.15991\n",
      "epoch no.4 train no.346710  loss = 3.45358 avg_loss = 3.18256\n",
      "epoch no.4 train no.346720  loss = 2.62443 avg_loss = 3.14231\n",
      "epoch no.4 train no.346730  loss = 4.65241 avg_loss = 3.18187\n",
      "epoch no.4 train no.346740  loss = 2.78391 avg_loss = 3.16264\n",
      "epoch no.4 train no.346750  loss = 2.97074 avg_loss = 3.15794\n",
      "epoch no.4 train no.346760  loss = 3.00672 avg_loss = 3.15198\n",
      "epoch no.4 train no.346770  loss = 3.10064 avg_loss = 3.15217\n",
      "epoch no.4 train no.346780  loss = 3.14659 avg_loss = 3.14191\n",
      "epoch no.4 train no.346790  loss = 3.62755 avg_loss = 3.12693\n",
      "epoch no.4 train no.346800  loss = 3.37449 avg_loss = 3.12541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.346810  loss = 4.25233 avg_loss = 3.13175\n",
      "epoch no.4 train no.346820  loss = 2.29249 avg_loss = 3.11024\n",
      "epoch no.4 train no.346830  loss = 2.69438 avg_loss = 3.11756\n",
      "epoch no.4 train no.346840  loss = 3.88166 avg_loss = 3.14397\n",
      "epoch no.4 train no.346850  loss = 3.21377 avg_loss = 3.18932\n",
      "epoch no.4 train no.346860  loss = 3.59319 avg_loss = 3.19303\n",
      "epoch no.4 train no.346870  loss = 2.45201 avg_loss = 3.24095\n",
      "epoch no.4 train no.346880  loss = 2.17454 avg_loss = 3.23309\n",
      "epoch no.4 train no.346890  loss = 3.89701 avg_loss = 3.19404\n",
      "epoch no.4 train no.346900  loss = 2.99468 avg_loss = 3.21421\n",
      "epoch no.4 train no.346910  loss = 4.23392 avg_loss = 3.22679\n",
      "epoch no.4 train no.346920  loss = 2.53308 avg_loss = 3.19128\n",
      "epoch no.4 train no.346930  loss = 2.17166 avg_loss = 3.15558\n",
      "epoch no.4 train no.346940  loss = 4.96288 avg_loss = 3.16597\n",
      "epoch no.4 train no.346950  loss = 3.54024 avg_loss = 3.18407\n",
      "epoch no.4 train no.346960  loss = 3.23306 avg_loss = 3.19486\n",
      "epoch no.4 train no.346970  loss = 3.54011 avg_loss = 3.21396\n",
      "epoch no.4 train no.346980  loss = 4.63562 avg_loss = 3.20553\n",
      "epoch no.4 train no.346990  loss = 2.46272 avg_loss = 3.19849\n",
      "epoch no.4 train no.347000  loss = 2.40087 avg_loss = 3.20777\n",
      "6\n",
      "to_tokens: ['▁비', '▁90', '80', '▁가요', '▁포크', '</s>', '곡', '</s>']\n",
      "추억의 7080 한국 락 명곡</s>\n",
      "epoch no.4 train no.347010  loss = 2.36529 avg_loss = 3.23652\n",
      "epoch no.4 train no.347020  loss = 4.81204 avg_loss = 3.31644\n",
      "epoch no.4 train no.347030  loss = 3.35983 avg_loss = 3.31713\n",
      "epoch no.4 train no.347040  loss = 4.88847 avg_loss = 3.29398\n",
      "epoch no.4 train no.347050  loss = 2.42532 avg_loss = 3.28570\n",
      "epoch no.4 train no.347060  loss = 2.38903 avg_loss = 3.27393\n",
      "epoch no.4 train no.347070  loss = 2.70372 avg_loss = 3.28052\n",
      "epoch no.4 train no.347080  loss = 3.71909 avg_loss = 3.28623\n",
      "epoch no.4 train no.347090  loss = 4.36203 avg_loss = 3.30580\n",
      "epoch no.4 train no.347100  loss = 4.16227 avg_loss = 3.32202\n",
      "epoch no.4 train no.347110  loss = 3.22727 avg_loss = 3.31211\n",
      "epoch no.4 train no.347120  loss = 2.73637 avg_loss = 3.32708\n",
      "epoch no.4 train no.347130  loss = 2.97216 avg_loss = 3.28941\n",
      "epoch no.4 train no.347140  loss = 2.24040 avg_loss = 3.27013\n",
      "epoch no.4 train no.347150  loss = 2.62841 avg_loss = 3.26353\n",
      "epoch no.4 train no.347160  loss = 2.95363 avg_loss = 3.25313\n",
      "epoch no.4 train no.347170  loss = 1.66500 avg_loss = 3.20797\n",
      "epoch no.4 train no.347180  loss = 2.44732 avg_loss = 3.19766\n",
      "epoch no.4 train no.347190  loss = 4.39714 avg_loss = 3.21158\n",
      "epoch no.4 train no.347200  loss = 2.16401 avg_loss = 3.20269\n",
      "epoch no.4 train no.347210  loss = 3.98645 avg_loss = 3.20303\n",
      "epoch no.4 train no.347220  loss = 3.36544 avg_loss = 3.19067\n",
      "epoch no.4 train no.347230  loss = 3.15354 avg_loss = 3.17862\n",
      "epoch no.4 train no.347240  loss = 2.53696 avg_loss = 3.17549\n",
      "epoch no.4 train no.347250  loss = 3.26820 avg_loss = 3.16562\n",
      "epoch no.4 train no.347260  loss = 3.19982 avg_loss = 3.18779\n",
      "epoch no.4 train no.347270  loss = 2.79571 avg_loss = 3.19660\n",
      "epoch no.4 train no.347280  loss = 3.89577 avg_loss = 3.23940\n",
      "epoch no.4 train no.347290  loss = 3.96858 avg_loss = 3.20902\n",
      "epoch no.4 train no.347300  loss = 2.87940 avg_loss = 3.20647\n",
      "epoch no.4 train no.347310  loss = 3.05782 avg_loss = 3.21422\n",
      "epoch no.4 train no.347320  loss = 3.08090 avg_loss = 3.23283\n",
      "epoch no.4 train no.347330  loss = 3.21467 avg_loss = 3.20339\n",
      "epoch no.4 train no.347340  loss = 3.68131 avg_loss = 3.23441\n",
      "epoch no.4 train no.347350  loss = 2.06642 avg_loss = 3.19409\n",
      "epoch no.4 train no.347360  loss = 4.08192 avg_loss = 3.22381\n",
      "epoch no.4 train no.347370  loss = 5.00106 avg_loss = 3.27929\n",
      "epoch no.4 train no.347380  loss = 2.67054 avg_loss = 3.26647\n",
      "epoch no.4 train no.347390  loss = 3.51921 avg_loss = 3.24639\n",
      "epoch no.4 train no.347400  loss = 2.91780 avg_loss = 3.25409\n",
      "epoch no.4 train no.347410  loss = 3.52804 avg_loss = 3.24625\n",
      "epoch no.4 train no.347420  loss = 2.38299 avg_loss = 3.22106\n",
      "epoch no.4 train no.347430  loss = 3.77468 avg_loss = 3.22047\n",
      "epoch no.4 train no.347440  loss = 1.64845 avg_loss = 3.21326\n",
      "epoch no.4 train no.347450  loss = 3.93394 avg_loss = 3.19583\n",
      "epoch no.4 train no.347460  loss = 2.39177 avg_loss = 3.19482\n",
      "epoch no.4 train no.347470  loss = 1.73581 avg_loss = 3.23210\n",
      "epoch no.4 train no.347480  loss = 3.14472 avg_loss = 3.24343\n",
      "epoch no.4 train no.347490  loss = 2.59292 avg_loss = 3.22228\n",
      "epoch no.4 train no.347500  loss = 3.71883 avg_loss = 3.20058\n",
      "epoch no.4 train no.347510  loss = 3.91762 avg_loss = 3.19409\n",
      "epoch no.4 train no.347520  loss = 3.26075 avg_loss = 3.17528\n",
      "epoch no.4 train no.347530  loss = 3.57544 avg_loss = 3.14952\n",
      "epoch no.4 train no.347540  loss = 2.73061 avg_loss = 3.12934\n",
      "epoch no.4 train no.347550  loss = 3.34823 avg_loss = 3.11968\n",
      "epoch no.4 train no.347560  loss = 2.54219 avg_loss = 3.13337\n",
      "epoch no.4 train no.347570  loss = 4.15790 avg_loss = 3.12099\n",
      "epoch no.4 train no.347580  loss = 2.17073 avg_loss = 3.16630\n",
      "epoch no.4 train no.347590  loss = 2.18133 avg_loss = 3.13301\n",
      "epoch no.4 train no.347600  loss = 3.17134 avg_loss = 3.15678\n",
      "epoch no.4 train no.347610  loss = 1.98251 avg_loss = 3.16568\n",
      "epoch no.4 train no.347620  loss = 3.36345 avg_loss = 3.13513\n",
      "epoch no.4 train no.347630  loss = 2.98504 avg_loss = 3.10960\n",
      "epoch no.4 train no.347640  loss = 3.77865 avg_loss = 3.13668\n",
      "epoch no.4 train no.347650  loss = 2.24517 avg_loss = 3.09609\n",
      "epoch no.4 train no.347660  loss = 2.68946 avg_loss = 3.12075\n",
      "epoch no.4 train no.347670  loss = 5.14585 avg_loss = 3.11828\n",
      "epoch no.4 train no.347680  loss = 3.58776 avg_loss = 3.12347\n",
      "epoch no.4 train no.347690  loss = 3.77745 avg_loss = 3.15447\n",
      "epoch no.4 train no.347700  loss = 3.69341 avg_loss = 3.16660\n",
      "epoch no.4 train no.347710  loss = 5.20436 avg_loss = 3.16830\n",
      "epoch no.4 train no.347720  loss = 3.32549 avg_loss = 3.20208\n",
      "epoch no.4 train no.347730  loss = 3.44402 avg_loss = 3.19038\n",
      "epoch no.4 train no.347740  loss = 3.09653 avg_loss = 3.17815\n",
      "epoch no.4 train no.347750  loss = 3.60964 avg_loss = 3.15738\n",
      "epoch no.4 train no.347760  loss = 4.86591 avg_loss = 3.15969\n",
      "epoch no.4 train no.347770  loss = 2.73475 avg_loss = 3.12237\n",
      "epoch no.4 train no.347780  loss = 3.81110 avg_loss = 3.15392\n",
      "epoch no.4 train no.347790  loss = 3.35548 avg_loss = 3.19925\n",
      "epoch no.4 train no.347800  loss = 2.74546 avg_loss = 3.19572\n",
      "epoch no.4 train no.347810  loss = 2.17224 avg_loss = 3.16446\n",
      "epoch no.4 train no.347820  loss = 2.75815 avg_loss = 3.21381\n",
      "epoch no.4 train no.347830  loss = 1.40734 avg_loss = 3.21575\n",
      "epoch no.4 train no.347840  loss = 2.91053 avg_loss = 3.27357\n",
      "epoch no.4 train no.347850  loss = 3.75487 avg_loss = 3.28385\n",
      "epoch no.4 train no.347860  loss = 3.03609 avg_loss = 3.28076\n",
      "epoch no.4 train no.347870  loss = 3.31984 avg_loss = 3.25302\n",
      "epoch no.4 train no.347880  loss = 3.90472 avg_loss = 3.22733\n",
      "epoch no.4 train no.347890  loss = 2.44980 avg_loss = 3.19195\n",
      "epoch no.4 train no.347900  loss = 3.54365 avg_loss = 3.23654\n",
      "epoch no.4 train no.347910  loss = 3.74319 avg_loss = 3.23491\n",
      "epoch no.4 train no.347920  loss = 3.14788 avg_loss = 3.25721\n",
      "epoch no.4 train no.347930  loss = 3.37174 avg_loss = 3.25583\n",
      "epoch no.4 train no.347940  loss = 3.40539 avg_loss = 3.26491\n",
      "epoch no.4 train no.347950  loss = 5.07408 avg_loss = 3.30234\n",
      "epoch no.4 train no.347960  loss = 3.06748 avg_loss = 3.31378\n",
      "epoch no.4 train no.347970  loss = 3.38378 avg_loss = 3.29194\n",
      "epoch no.4 train no.347980  loss = 2.74114 avg_loss = 3.30006\n",
      "epoch no.4 train no.347990  loss = 3.57535 avg_loss = 3.27957\n",
      "epoch no.4 train no.348000  loss = 2.88023 avg_loss = 3.25216\n",
      "3\n",
      "to_tokens: ['▁가을', '▁2000', '년대', '▁발라', '</s>']\n",
      "추억의 90년대 가요</s>\n",
      "epoch no.4 train no.348010  loss = 4.34094 avg_loss = 3.20922\n",
      "epoch no.4 train no.348020  loss = 3.71788 avg_loss = 3.23274\n",
      "epoch no.4 train no.348030  loss = 2.13229 avg_loss = 3.26009\n",
      "epoch no.4 train no.348040  loss = 3.75755 avg_loss = 3.20653\n",
      "epoch no.4 train no.348050  loss = 3.12665 avg_loss = 3.16962\n",
      "epoch no.4 train no.348060  loss = 2.47768 avg_loss = 3.13308\n",
      "epoch no.4 train no.348070  loss = 1.94532 avg_loss = 3.13175\n",
      "epoch no.4 train no.348080  loss = 3.55322 avg_loss = 3.20462\n",
      "epoch no.4 train no.348090  loss = 2.92125 avg_loss = 3.21599\n",
      "epoch no.4 train no.348100  loss = 4.10829 avg_loss = 3.21098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.348110  loss = 3.14645 avg_loss = 3.18099\n",
      "epoch no.4 train no.348120  loss = 2.03099 avg_loss = 3.16734\n",
      "epoch no.4 train no.348130  loss = 2.65700 avg_loss = 3.12759\n",
      "epoch no.4 train no.348140  loss = 1.51002 avg_loss = 3.13394\n",
      "epoch no.4 train no.348150  loss = 4.16045 avg_loss = 3.16473\n",
      "epoch no.4 train no.348160  loss = 2.24842 avg_loss = 3.17868\n",
      "epoch no.4 train no.348170  loss = 2.60965 avg_loss = 3.17147\n",
      "epoch no.4 train no.348180  loss = 3.46045 avg_loss = 3.19595\n",
      "epoch no.4 train no.348190  loss = 3.30603 avg_loss = 3.16774\n",
      "epoch no.4 train no.348200  loss = 2.99716 avg_loss = 3.19738\n",
      "epoch no.4 train no.348210  loss = 3.34525 avg_loss = 3.18430\n",
      "epoch no.4 train no.348220  loss = 2.66462 avg_loss = 3.20820\n",
      "epoch no.4 train no.348230  loss = 3.51826 avg_loss = 3.19055\n",
      "epoch no.4 train no.348240  loss = 2.82206 avg_loss = 3.19723\n",
      "epoch no.4 train no.348250  loss = 2.66811 avg_loss = 3.18877\n",
      "epoch no.4 train no.348260  loss = 5.49093 avg_loss = 3.22437\n",
      "epoch no.4 train no.348270  loss = 3.04155 avg_loss = 3.19525\n",
      "epoch no.4 train no.348280  loss = 2.51878 avg_loss = 3.18550\n",
      "epoch no.4 train no.348290  loss = 3.34893 avg_loss = 3.17578\n",
      "epoch no.4 train no.348300  loss = 3.50695 avg_loss = 3.21492\n",
      "epoch no.4 train no.348310  loss = 4.43191 avg_loss = 3.24546\n",
      "epoch no.4 train no.348320  loss = 3.10157 avg_loss = 3.25622\n",
      "epoch no.4 train no.348330  loss = 2.33641 avg_loss = 3.19180\n",
      "epoch no.4 train no.348340  loss = 2.59651 avg_loss = 3.15803\n",
      "epoch no.4 train no.348350  loss = 4.91131 avg_loss = 3.15328\n",
      "epoch no.4 train no.348360  loss = 4.44141 avg_loss = 3.15711\n",
      "epoch no.4 train no.348370  loss = 2.92944 avg_loss = 3.20918\n",
      "epoch no.4 train no.348380  loss = 2.82473 avg_loss = 3.19023\n",
      "epoch no.4 train no.348390  loss = 4.03629 avg_loss = 3.19830\n",
      "epoch no.4 train no.348400  loss = 4.62067 avg_loss = 3.19718\n",
      "epoch no.4 train no.348410  loss = 4.67146 avg_loss = 3.17220\n",
      "epoch no.4 train no.348420  loss = 3.37943 avg_loss = 3.14494\n",
      "epoch no.4 train no.348430  loss = 3.35228 avg_loss = 3.13399\n",
      "epoch no.4 train no.348440  loss = 3.50985 avg_loss = 3.17602\n",
      "epoch no.4 train no.348450  loss = 2.94310 avg_loss = 3.16072\n",
      "epoch no.4 train no.348460  loss = 4.33864 avg_loss = 3.14944\n",
      "epoch no.4 train no.348470  loss = 3.76893 avg_loss = 3.15890\n",
      "epoch no.4 train no.348480  loss = 3.16821 avg_loss = 3.18340\n",
      "epoch no.4 train no.348490  loss = 1.85860 avg_loss = 3.20494\n",
      "epoch no.4 train no.348500  loss = 2.49744 avg_loss = 3.18862\n",
      "epoch no.4 train no.348510  loss = 3.80446 avg_loss = 3.17380\n",
      "epoch no.4 train no.348520  loss = 2.50804 avg_loss = 3.17398\n",
      "epoch no.4 train no.348530  loss = 3.91923 avg_loss = 3.23621\n",
      "epoch no.4 train no.348540  loss = 2.89896 avg_loss = 3.21489\n",
      "epoch no.4 train no.348550  loss = 2.67694 avg_loss = 3.19940\n",
      "epoch no.4 train no.348560  loss = 2.90912 avg_loss = 3.21354\n",
      "epoch no.4 train no.348570  loss = 2.86134 avg_loss = 3.19658\n",
      "epoch no.4 train no.348580  loss = 2.96486 avg_loss = 3.24047\n",
      "epoch no.4 train no.348590  loss = 2.71984 avg_loss = 3.27605\n",
      "epoch no.4 train no.348600  loss = 2.20588 avg_loss = 3.29282\n",
      "epoch no.4 train no.348610  loss = 2.00470 avg_loss = 3.27443\n",
      "epoch no.4 train no.348620  loss = 3.54647 avg_loss = 3.25728\n",
      "epoch no.4 train no.348630  loss = 3.87073 avg_loss = 3.30790\n",
      "epoch no.4 train no.348640  loss = 0.95932 avg_loss = 3.24583\n",
      "epoch no.4 train no.348650  loss = 3.82049 avg_loss = 3.24124\n",
      "epoch no.4 train no.348660  loss = 4.87899 avg_loss = 3.27792\n",
      "epoch no.4 train no.348670  loss = 3.35218 avg_loss = 3.28392\n",
      "epoch no.4 train no.348680  loss = 4.62278 avg_loss = 3.29755\n",
      "epoch no.4 train no.348690  loss = 2.48517 avg_loss = 3.23299\n",
      "epoch no.4 train no.348700  loss = 2.68168 avg_loss = 3.21176\n",
      "epoch no.4 train no.348710  loss = 4.39201 avg_loss = 3.24426\n",
      "epoch no.4 train no.348720  loss = 2.83709 avg_loss = 3.23641\n",
      "epoch no.4 train no.348730  loss = 1.86637 avg_loss = 3.19379\n",
      "epoch no.4 train no.348740  loss = 3.09702 avg_loss = 3.15894\n",
      "epoch no.4 train no.348750  loss = 2.54296 avg_loss = 3.12116\n",
      "epoch no.4 train no.348760  loss = 2.48565 avg_loss = 3.11536\n",
      "epoch no.4 train no.348770  loss = 3.41290 avg_loss = 3.11153\n",
      "epoch no.4 train no.348780  loss = 4.09115 avg_loss = 3.15278\n",
      "epoch no.4 train no.348790  loss = 4.15286 avg_loss = 3.18181\n",
      "epoch no.4 train no.348800  loss = 3.26416 avg_loss = 3.18338\n",
      "epoch no.4 train no.348810  loss = 3.22693 avg_loss = 3.18683\n",
      "epoch no.4 train no.348820  loss = 3.69075 avg_loss = 3.20734\n",
      "epoch no.4 train no.348830  loss = 2.87631 avg_loss = 3.20561\n",
      "epoch no.4 train no.348840  loss = 5.33466 avg_loss = 3.22906\n",
      "epoch no.4 train no.348850  loss = 2.00422 avg_loss = 3.21833\n",
      "epoch no.4 train no.348860  loss = 2.17116 avg_loss = 3.20825\n",
      "epoch no.4 train no.348870  loss = 2.36466 avg_loss = 3.21783\n",
      "epoch no.4 train no.348880  loss = 3.52634 avg_loss = 3.21419\n",
      "epoch no.4 train no.348890  loss = 3.17238 avg_loss = 3.20291\n",
      "epoch no.4 train no.348900  loss = 5.47434 avg_loss = 3.24133\n",
      "epoch no.4 train no.348910  loss = 3.06367 avg_loss = 3.24973\n",
      "epoch no.4 train no.348920  loss = 2.53326 avg_loss = 3.22237\n",
      "epoch no.4 train no.348930  loss = 5.42012 avg_loss = 3.28486\n",
      "epoch no.4 train no.348940  loss = 2.45053 avg_loss = 3.28372\n",
      "epoch no.4 train no.348950  loss = 3.92337 avg_loss = 3.29829\n",
      "epoch no.4 train no.348960  loss = 2.97569 avg_loss = 3.27075\n",
      "epoch no.4 train no.348970  loss = 1.92859 avg_loss = 3.25466\n",
      "epoch no.4 train no.348980  loss = 2.94016 avg_loss = 3.26016\n",
      "epoch no.4 train no.348990  loss = 4.43606 avg_loss = 3.25995\n",
      "epoch no.4 train no.349000  loss = 3.88180 avg_loss = 3.25517\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '</s>', '</s>']\n",
      "추억의 2000년대 가요 베스트</s>\n",
      "epoch no.4 train no.349010  loss = 3.27326 avg_loss = 3.22786\n",
      "epoch no.4 train no.349020  loss = 4.28528 avg_loss = 3.24262\n",
      "epoch no.4 train no.349030  loss = 1.23790 avg_loss = 3.21695\n",
      "epoch no.4 train no.349040  loss = 4.99597 avg_loss = 3.25278\n",
      "epoch no.4 train no.349050  loss = 2.74521 avg_loss = 3.23293\n",
      "epoch no.4 train no.349060  loss = 2.68052 avg_loss = 3.23311\n",
      "epoch no.4 train no.349070  loss = 3.53328 avg_loss = 3.20691\n",
      "epoch no.4 train no.349080  loss = 5.43292 avg_loss = 3.21389\n",
      "epoch no.4 train no.349090  loss = 3.93999 avg_loss = 3.19838\n",
      "epoch no.4 train no.349100  loss = 5.50569 avg_loss = 3.21994\n",
      "epoch no.4 train no.349110  loss = 4.24254 avg_loss = 3.22615\n",
      "epoch no.4 train no.349120  loss = 3.97863 avg_loss = 3.18043\n",
      "epoch no.4 train no.349130  loss = 3.22696 avg_loss = 3.20745\n",
      "epoch no.4 train no.349140  loss = 2.65821 avg_loss = 3.19678\n",
      "epoch no.4 train no.349150  loss = 4.14086 avg_loss = 3.19538\n",
      "epoch no.4 train no.349160  loss = 3.95031 avg_loss = 3.21805\n",
      "epoch no.4 train no.349170  loss = 3.02212 avg_loss = 3.19430\n",
      "epoch no.4 train no.349180  loss = 2.68677 avg_loss = 3.23224\n",
      "epoch no.4 train no.349190  loss = 2.38029 avg_loss = 3.22013\n",
      "epoch no.4 train no.349200  loss = 4.81065 avg_loss = 3.24194\n",
      "epoch no.4 train no.349210  loss = 4.08808 avg_loss = 3.30521\n",
      "epoch no.4 train no.349220  loss = 3.24127 avg_loss = 3.31956\n",
      "epoch no.4 train no.349230  loss = 4.20397 avg_loss = 3.30874\n",
      "epoch no.4 train no.349240  loss = 2.64653 avg_loss = 3.28981\n",
      "epoch no.4 train no.349250  loss = 2.91523 avg_loss = 3.31323\n",
      "epoch no.4 train no.349260  loss = 2.41570 avg_loss = 3.30667\n",
      "epoch no.4 train no.349270  loss = 2.56140 avg_loss = 3.28862\n",
      "epoch no.4 train no.349280  loss = 2.91725 avg_loss = 3.28994\n",
      "epoch no.4 train no.349290  loss = 3.24093 avg_loss = 3.29477\n",
      "epoch no.4 train no.349300  loss = 3.34752 avg_loss = 3.28139\n",
      "epoch no.4 train no.349310  loss = 2.34987 avg_loss = 3.25860\n",
      "epoch no.4 train no.349320  loss = 2.26539 avg_loss = 3.23197\n",
      "epoch no.4 train no.349330  loss = 3.50556 avg_loss = 3.21039\n",
      "epoch no.4 train no.349340  loss = 6.48202 avg_loss = 3.26171\n",
      "epoch no.4 train no.349350  loss = 3.78346 avg_loss = 3.21799\n",
      "epoch no.4 train no.349360  loss = 3.58528 avg_loss = 3.18843\n",
      "epoch no.4 train no.349370  loss = 3.43256 avg_loss = 3.21389\n",
      "epoch no.4 train no.349380  loss = 1.90090 avg_loss = 3.20989\n",
      "epoch no.4 train no.349390  loss = 2.95735 avg_loss = 3.21490\n",
      "epoch no.4 train no.349400  loss = 2.03209 avg_loss = 3.22039\n",
      "epoch no.4 train no.349410  loss = 2.92780 avg_loss = 3.22955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.349420  loss = 3.02699 avg_loss = 3.22497\n",
      "epoch no.4 train no.349430  loss = 3.00485 avg_loss = 3.23596\n",
      "epoch no.4 train no.349440  loss = 4.27616 avg_loss = 3.28594\n",
      "epoch no.4 train no.349450  loss = 5.03806 avg_loss = 3.30145\n",
      "epoch no.4 train no.349460  loss = 2.50255 avg_loss = 3.27566\n",
      "epoch no.4 train no.349470  loss = 2.17814 avg_loss = 3.29910\n",
      "epoch no.4 train no.349480  loss = 2.51542 avg_loss = 3.31606\n",
      "epoch no.4 train no.349490  loss = 2.38522 avg_loss = 3.26075\n",
      "epoch no.4 train no.349500  loss = 2.35444 avg_loss = 3.21889\n",
      "epoch no.4 train no.349510  loss = 1.69944 avg_loss = 3.21497\n",
      "epoch no.4 train no.349520  loss = 3.07978 avg_loss = 3.20435\n",
      "epoch no.4 train no.349530  loss = 4.48923 avg_loss = 3.21013\n",
      "epoch no.4 train no.349540  loss = 2.63835 avg_loss = 3.16886\n",
      "epoch no.4 train no.349550  loss = 3.17566 avg_loss = 3.17633\n",
      "epoch no.4 train no.349560  loss = 5.63874 avg_loss = 3.19346\n",
      "epoch no.4 train no.349570  loss = 3.50680 avg_loss = 3.20312\n",
      "epoch no.4 train no.349580  loss = 3.25689 avg_loss = 3.24798\n",
      "epoch no.4 train no.349590  loss = 3.21826 avg_loss = 3.22025\n",
      "epoch no.4 train no.349600  loss = 1.74302 avg_loss = 3.21852\n",
      "epoch no.4 train no.349610  loss = 1.92688 avg_loss = 3.22654\n",
      "epoch no.4 train no.349620  loss = 4.53911 avg_loss = 3.25827\n",
      "epoch no.4 train no.349630  loss = 2.48142 avg_loss = 3.23537\n",
      "epoch no.4 train no.349640  loss = 4.84048 avg_loss = 3.21668\n",
      "epoch no.4 train no.349650  loss = 3.95014 avg_loss = 3.19100\n",
      "epoch no.4 train no.349660  loss = 5.20457 avg_loss = 3.21581\n",
      "epoch no.4 train no.349670  loss = 2.35322 avg_loss = 3.23699\n",
      "epoch no.4 train no.349680  loss = 1.96602 avg_loss = 3.20724\n",
      "epoch no.4 train no.349690  loss = 3.08047 avg_loss = 3.18732\n",
      "epoch no.4 train no.349700  loss = 2.68053 avg_loss = 3.21295\n",
      "epoch no.4 train no.349710  loss = 1.75298 avg_loss = 3.19276\n",
      "epoch no.4 train no.349720  loss = 4.97808 avg_loss = 3.19745\n",
      "epoch no.4 train no.349730  loss = 4.62621 avg_loss = 3.23787\n",
      "epoch no.4 train no.349740  loss = 2.27978 avg_loss = 3.22778\n",
      "epoch no.4 train no.349750  loss = 3.30372 avg_loss = 3.27337\n",
      "epoch no.4 train no.349760  loss = 4.67501 avg_loss = 3.28362\n",
      "epoch no.4 train no.349770  loss = 2.80251 avg_loss = 3.29573\n",
      "epoch no.4 train no.349780  loss = 4.76165 avg_loss = 3.31942\n",
      "epoch no.4 train no.349790  loss = 2.04011 avg_loss = 3.31555\n",
      "epoch no.4 train no.349800  loss = 2.79961 avg_loss = 3.28235\n",
      "epoch no.4 train no.349810  loss = 1.73978 avg_loss = 3.26093\n",
      "epoch no.4 train no.349820  loss = 2.77174 avg_loss = 3.25145\n",
      "epoch no.4 train no.349830  loss = 1.86979 avg_loss = 3.23650\n",
      "epoch no.4 train no.349840  loss = 3.99019 avg_loss = 3.23769\n",
      "epoch no.4 train no.349850  loss = 3.22285 avg_loss = 3.27272\n",
      "epoch no.4 train no.349860  loss = 2.50715 avg_loss = 3.24150\n",
      "epoch no.4 train no.349870  loss = 3.13725 avg_loss = 3.26166\n",
      "epoch no.4 train no.349880  loss = 2.59809 avg_loss = 3.24218\n",
      "epoch no.4 train no.349890  loss = 3.68741 avg_loss = 3.24834\n",
      "epoch no.4 train no.349900  loss = 3.40155 avg_loss = 3.22668\n",
      "epoch no.4 train no.349910  loss = 2.50680 avg_loss = 3.21714\n",
      "epoch no.4 train no.349920  loss = 3.59621 avg_loss = 3.23816\n",
      "epoch no.4 train no.349930  loss = 2.74623 avg_loss = 3.18137\n",
      "epoch no.4 train no.349940  loss = 2.61672 avg_loss = 3.15275\n",
      "epoch no.4 train no.349950  loss = 4.88140 avg_loss = 3.16632\n",
      "epoch no.4 train no.349960  loss = 2.37656 avg_loss = 3.12244\n",
      "epoch no.4 train no.349970  loss = 3.40783 avg_loss = 3.15171\n",
      "epoch no.4 train no.349980  loss = 3.62188 avg_loss = 3.14114\n",
      "epoch no.4 train no.349990  loss = 3.91428 avg_loss = 3.18831\n",
      "epoch no.4 train no.350000  loss = 4.31925 avg_loss = 3.19100\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '▁베스트', '</s>']\n",
      "추억의 2000년대 가요 베스트</s>\n",
      "epoch no.4 train no.350010  loss = 4.62700 avg_loss = 3.19899\n",
      "epoch no.4 train no.350020  loss = 3.49104 avg_loss = 3.17215\n",
      "epoch no.4 train no.350030  loss = 3.81033 avg_loss = 3.18273\n",
      "epoch no.4 train no.350040  loss = 3.55420 avg_loss = 3.19266\n",
      "epoch no.4 train no.350050  loss = 3.21796 avg_loss = 3.23409\n",
      "epoch no.4 train no.350060  loss = 3.05749 avg_loss = 3.23032\n",
      "epoch no.4 train no.350070  loss = 3.27711 avg_loss = 3.22806\n",
      "epoch no.4 train no.350080  loss = 3.24365 avg_loss = 3.26142\n",
      "epoch no.4 train no.350090  loss = 2.97907 avg_loss = 3.21695\n",
      "epoch no.4 train no.350100  loss = 2.79060 avg_loss = 3.21835\n",
      "epoch no.4 train no.350110  loss = 1.82185 avg_loss = 3.15873\n",
      "epoch no.4 train no.350120  loss = 4.49347 avg_loss = 3.17713\n",
      "epoch no.4 train no.350130  loss = 3.78082 avg_loss = 3.21987\n",
      "epoch no.4 train no.350140  loss = 2.74719 avg_loss = 3.18464\n",
      "epoch no.4 train no.350150  loss = 2.50202 avg_loss = 3.18764\n",
      "epoch no.4 train no.350160  loss = 1.74521 avg_loss = 3.18280\n",
      "epoch no.4 train no.350170  loss = 3.23069 avg_loss = 3.15940\n",
      "epoch no.4 train no.350180  loss = 2.42521 avg_loss = 3.11871\n",
      "epoch no.4 train no.350190  loss = 2.03847 avg_loss = 3.12843\n",
      "epoch no.4 train no.350200  loss = 3.04284 avg_loss = 3.15762\n",
      "epoch no.4 train no.350210  loss = 4.08174 avg_loss = 3.15744\n",
      "epoch no.4 train no.350220  loss = 2.12426 avg_loss = 3.10046\n",
      "epoch no.4 train no.350230  loss = 4.89076 avg_loss = 3.08291\n",
      "epoch no.4 train no.350240  loss = 2.30269 avg_loss = 3.07283\n",
      "epoch no.4 train no.350250  loss = 1.87751 avg_loss = 3.02543\n",
      "epoch no.4 train no.350260  loss = 2.41122 avg_loss = 3.07863\n",
      "epoch no.4 train no.350270  loss = 2.49801 avg_loss = 3.09309\n",
      "epoch no.4 train no.350280  loss = 3.37926 avg_loss = 3.05118\n",
      "epoch no.4 train no.350290  loss = 4.13800 avg_loss = 3.09870\n",
      "epoch no.4 train no.350300  loss = 4.20326 avg_loss = 3.08181\n",
      "epoch no.4 train no.350310  loss = 2.16434 avg_loss = 3.07771\n",
      "epoch no.4 train no.350320  loss = 3.50228 avg_loss = 3.07776\n",
      "epoch no.4 train no.350330  loss = 2.88372 avg_loss = 3.09790\n",
      "epoch no.4 train no.350340  loss = 2.77382 avg_loss = 3.09894\n",
      "epoch no.4 train no.350350  loss = 3.47639 avg_loss = 3.15278\n",
      "epoch no.4 train no.350360  loss = 3.97448 avg_loss = 3.17795\n",
      "epoch no.4 train no.350370  loss = 3.18134 avg_loss = 3.15784\n",
      "epoch no.4 train no.350380  loss = 3.63299 avg_loss = 3.13475\n",
      "epoch no.4 train no.350390  loss = 4.91024 avg_loss = 3.19189\n",
      "epoch no.4 train no.350400  loss = 4.80087 avg_loss = 3.19797\n",
      "epoch no.4 train no.350410  loss = 4.19292 avg_loss = 3.22356\n",
      "epoch no.4 train no.350420  loss = 5.40119 avg_loss = 3.25468\n",
      "epoch no.4 train no.350430  loss = 1.68311 avg_loss = 3.22335\n",
      "epoch no.4 train no.350440  loss = 3.89642 avg_loss = 3.20700\n",
      "epoch no.4 train no.350450  loss = 4.89167 avg_loss = 3.17007\n",
      "epoch no.4 train no.350460  loss = 3.02850 avg_loss = 3.16210\n",
      "epoch no.4 train no.350470  loss = 2.96499 avg_loss = 3.19975\n",
      "epoch no.4 train no.350480  loss = 3.86615 avg_loss = 3.24566\n",
      "epoch no.4 train no.350490  loss = 4.50402 avg_loss = 3.21423\n",
      "epoch no.4 train no.350500  loss = 3.04054 avg_loss = 3.25002\n",
      "epoch no.4 train no.350510  loss = 2.83151 avg_loss = 3.25434\n",
      "epoch no.4 train no.350520  loss = 3.80339 avg_loss = 3.26970\n",
      "epoch no.4 train no.350530  loss = 4.06618 avg_loss = 3.30119\n",
      "epoch no.4 train no.350540  loss = 3.28253 avg_loss = 3.33532\n",
      "epoch no.4 train no.350550  loss = 2.20373 avg_loss = 3.33140\n",
      "epoch no.4 train no.350560  loss = 1.72628 avg_loss = 3.25919\n",
      "epoch no.4 train no.350570  loss = 2.34485 avg_loss = 3.23629\n",
      "epoch no.4 train no.350580  loss = 3.74639 avg_loss = 3.28296\n",
      "epoch no.4 train no.350590  loss = 3.07964 avg_loss = 3.25353\n",
      "epoch no.4 train no.350600  loss = 3.16652 avg_loss = 3.21964\n",
      "epoch no.4 train no.350610  loss = 4.80674 avg_loss = 3.24083\n",
      "epoch no.4 train no.350620  loss = 3.87699 avg_loss = 3.22819\n",
      "epoch no.4 train no.350630  loss = 2.63211 avg_loss = 3.19679\n",
      "epoch no.4 train no.350640  loss = 3.13493 avg_loss = 3.16048\n",
      "epoch no.4 train no.350650  loss = 2.46058 avg_loss = 3.12525\n",
      "epoch no.4 train no.350660  loss = 4.46986 avg_loss = 3.13674\n",
      "epoch no.4 train no.350670  loss = 1.53967 avg_loss = 3.19538\n",
      "epoch no.4 train no.350680  loss = 2.61085 avg_loss = 3.19233\n",
      "epoch no.4 train no.350690  loss = 4.16597 avg_loss = 3.20054\n",
      "epoch no.4 train no.350700  loss = 2.55524 avg_loss = 3.16681\n",
      "epoch no.4 train no.350710  loss = 3.48538 avg_loss = 3.19912\n",
      "epoch no.4 train no.350720  loss = 2.43246 avg_loss = 3.17948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.350730  loss = 4.32197 avg_loss = 3.20340\n",
      "epoch no.4 train no.350740  loss = 2.12688 avg_loss = 3.18916\n",
      "epoch no.4 train no.350750  loss = 4.42389 avg_loss = 3.23368\n",
      "epoch no.4 train no.350760  loss = 3.41893 avg_loss = 3.26369\n",
      "epoch no.4 train no.350770  loss = 2.49960 avg_loss = 3.26457\n",
      "epoch no.4 train no.350780  loss = 2.60954 avg_loss = 3.26492\n",
      "epoch no.4 train no.350790  loss = 3.31649 avg_loss = 3.25181\n",
      "epoch no.4 train no.350800  loss = 3.33731 avg_loss = 3.22630\n",
      "epoch no.4 train no.350810  loss = 2.09224 avg_loss = 3.19373\n",
      "epoch no.4 train no.350820  loss = 5.48923 avg_loss = 3.21996\n",
      "epoch no.4 train no.350830  loss = 2.95161 avg_loss = 3.19029\n",
      "epoch no.4 train no.350840  loss = 3.53642 avg_loss = 3.19902\n",
      "epoch no.4 train no.350850  loss = 2.74670 avg_loss = 3.18960\n",
      "epoch no.4 train no.350860  loss = 2.83582 avg_loss = 3.18035\n",
      "epoch no.4 train no.350870  loss = 3.24966 avg_loss = 3.14966\n",
      "epoch no.4 train no.350880  loss = 3.09959 avg_loss = 3.15976\n",
      "epoch no.4 train no.350890  loss = 4.13942 avg_loss = 3.17053\n",
      "epoch no.4 train no.350900  loss = 2.73194 avg_loss = 3.19225\n",
      "epoch no.4 train no.350910  loss = 2.01141 avg_loss = 3.17236\n",
      "epoch no.4 train no.350920  loss = 3.09163 avg_loss = 3.15406\n",
      "epoch no.4 train no.350930  loss = 4.88824 avg_loss = 3.17301\n",
      "epoch no.4 train no.350940  loss = 3.96551 avg_loss = 3.19828\n",
      "epoch no.4 train no.350950  loss = 3.31506 avg_loss = 3.20150\n",
      "epoch no.4 train no.350960  loss = 2.05684 avg_loss = 3.21351\n",
      "epoch no.4 train no.350970  loss = 3.53283 avg_loss = 3.20546\n",
      "epoch no.4 train no.350980  loss = 2.15221 avg_loss = 3.22754\n",
      "epoch no.4 train no.350990  loss = 3.27136 avg_loss = 3.19587\n",
      "epoch no.4 train no.351000  loss = 2.48535 avg_loss = 3.19245\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.351010  loss = 2.06937 avg_loss = 3.19885\n",
      "epoch no.4 train no.351020  loss = 2.66079 avg_loss = 3.21976\n",
      "epoch no.4 train no.351030  loss = 3.22307 avg_loss = 3.23528\n",
      "epoch no.4 train no.351040  loss = 3.85400 avg_loss = 3.25377\n",
      "epoch no.4 train no.351050  loss = 2.45247 avg_loss = 3.24131\n",
      "epoch no.4 train no.351060  loss = 3.44238 avg_loss = 3.19549\n",
      "epoch no.4 train no.351070  loss = 2.49412 avg_loss = 3.18660\n",
      "epoch no.4 train no.351080  loss = 3.96934 avg_loss = 3.19800\n",
      "epoch no.4 train no.351090  loss = 3.91426 avg_loss = 3.22521\n",
      "epoch no.4 train no.351100  loss = 3.93452 avg_loss = 3.22187\n",
      "epoch no.4 train no.351110  loss = 2.20268 avg_loss = 3.18850\n",
      "epoch no.4 train no.351120  loss = 2.33334 avg_loss = 3.15936\n",
      "epoch no.4 train no.351130  loss = 3.89864 avg_loss = 3.15819\n",
      "epoch no.4 train no.351140  loss = 4.35284 avg_loss = 3.18917\n",
      "epoch no.4 train no.351150  loss = 3.53826 avg_loss = 3.17771\n",
      "epoch no.4 train no.351160  loss = 2.24734 avg_loss = 3.16674\n",
      "epoch no.4 train no.351170  loss = 2.58523 avg_loss = 3.15548\n",
      "epoch no.4 train no.351180  loss = 3.23042 avg_loss = 3.16790\n",
      "epoch no.4 train no.351190  loss = 2.96445 avg_loss = 3.15958\n",
      "epoch no.4 train no.351200  loss = 3.58508 avg_loss = 3.15604\n",
      "epoch no.4 train no.351210  loss = 3.09646 avg_loss = 3.13331\n",
      "epoch no.4 train no.351220  loss = 2.38998 avg_loss = 3.13757\n",
      "epoch no.4 train no.351230  loss = 5.14770 avg_loss = 3.13419\n",
      "epoch no.4 train no.351240  loss = 1.90005 avg_loss = 3.13351\n",
      "epoch no.4 train no.351250  loss = 3.92788 avg_loss = 3.16607\n",
      "epoch no.4 train no.351260  loss = 3.05606 avg_loss = 3.20124\n",
      "epoch no.4 train no.351270  loss = 3.85347 avg_loss = 3.20193\n",
      "epoch no.4 train no.351280  loss = 1.80453 avg_loss = 3.16698\n",
      "epoch no.4 train no.351290  loss = 3.53517 avg_loss = 3.13192\n",
      "epoch no.4 train no.351300  loss = 4.29174 avg_loss = 3.18026\n",
      "epoch no.4 train no.351310  loss = 4.80230 avg_loss = 3.16187\n",
      "epoch no.4 train no.351320  loss = 3.03241 avg_loss = 3.13371\n",
      "epoch no.4 train no.351330  loss = 2.58837 avg_loss = 3.10954\n",
      "epoch no.4 train no.351340  loss = 2.74716 avg_loss = 3.09345\n",
      "epoch no.4 train no.351350  loss = 4.55835 avg_loss = 3.08156\n",
      "epoch no.4 train no.351360  loss = 1.88601 avg_loss = 3.09421\n",
      "epoch no.4 train no.351370  loss = 3.78041 avg_loss = 3.10396\n",
      "epoch no.4 train no.351380  loss = 2.64439 avg_loss = 3.09313\n",
      "epoch no.4 train no.351390  loss = 3.01787 avg_loss = 3.09244\n",
      "epoch no.4 train no.351400  loss = 1.94503 avg_loss = 3.12684\n",
      "epoch no.4 train no.351410  loss = 1.09500 avg_loss = 3.15183\n",
      "epoch no.4 train no.351420  loss = 2.91557 avg_loss = 3.14737\n",
      "epoch no.4 train no.351430  loss = 4.65960 avg_loss = 3.14886\n",
      "epoch no.4 train no.351440  loss = 3.13480 avg_loss = 3.18165\n",
      "epoch no.4 train no.351450  loss = 2.95367 avg_loss = 3.20858\n",
      "epoch no.4 train no.351460  loss = 4.80316 avg_loss = 3.22048\n",
      "epoch no.4 train no.351470  loss = 4.79076 avg_loss = 3.25553\n",
      "epoch no.4 train no.351480  loss = 3.64887 avg_loss = 3.21025\n",
      "epoch no.4 train no.351490  loss = 3.77277 avg_loss = 3.21582\n",
      "epoch no.4 train no.351500  loss = 2.01035 avg_loss = 3.21131\n",
      "epoch no.4 train no.351510  loss = 4.13442 avg_loss = 3.20756\n",
      "epoch no.4 train no.351520  loss = 2.78627 avg_loss = 3.21115\n",
      "epoch no.4 train no.351530  loss = 2.96095 avg_loss = 3.18851\n",
      "epoch no.4 train no.351540  loss = 2.54325 avg_loss = 3.17601\n",
      "epoch no.4 train no.351550  loss = 1.80533 avg_loss = 3.17781\n",
      "epoch no.4 train no.351560  loss = 2.40544 avg_loss = 3.14968\n",
      "epoch no.4 train no.351570  loss = 2.67419 avg_loss = 3.17855\n",
      "epoch no.4 train no.351580  loss = 3.51961 avg_loss = 3.15681\n",
      "epoch no.4 train no.351590  loss = 3.56341 avg_loss = 3.14175\n",
      "epoch no.4 train no.351600  loss = 2.46559 avg_loss = 3.12905\n",
      "epoch no.4 train no.351610  loss = 2.22720 avg_loss = 3.06976\n",
      "epoch no.4 train no.351620  loss = 2.42091 avg_loss = 3.04892\n",
      "epoch no.4 train no.351630  loss = 4.53282 avg_loss = 3.03818\n",
      "epoch no.4 train no.351640  loss = 2.68588 avg_loss = 3.03173\n",
      "epoch no.4 train no.351650  loss = 3.03713 avg_loss = 3.05701\n",
      "epoch no.4 train no.351660  loss = 3.52292 avg_loss = 3.06461\n",
      "epoch no.4 train no.351670  loss = 1.95910 avg_loss = 3.07601\n",
      "epoch no.4 train no.351680  loss = 3.60139 avg_loss = 3.06141\n",
      "epoch no.4 train no.351690  loss = 3.38453 avg_loss = 3.09876\n",
      "epoch no.4 train no.351700  loss = 1.65093 avg_loss = 3.11964\n",
      "epoch no.4 train no.351710  loss = 3.95468 avg_loss = 3.14794\n",
      "epoch no.4 train no.351720  loss = 2.53814 avg_loss = 3.13114\n",
      "epoch no.4 train no.351730  loss = 2.97113 avg_loss = 3.14785\n",
      "epoch no.4 train no.351740  loss = 2.49436 avg_loss = 3.12010\n",
      "epoch no.4 train no.351750  loss = 3.63035 avg_loss = 3.16017\n",
      "epoch no.4 train no.351760  loss = 2.01714 avg_loss = 3.18555\n",
      "epoch no.4 train no.351770  loss = 3.62097 avg_loss = 3.22588\n",
      "epoch no.4 train no.351780  loss = 5.22955 avg_loss = 3.26398\n",
      "epoch no.4 train no.351790  loss = 2.80108 avg_loss = 3.28841\n",
      "epoch no.4 train no.351800  loss = 4.09935 avg_loss = 3.23814\n",
      "epoch no.4 train no.351810  loss = 2.27791 avg_loss = 3.20697\n",
      "epoch no.4 train no.351820  loss = 3.63050 avg_loss = 3.18889\n",
      "epoch no.4 train no.351830  loss = 2.60064 avg_loss = 3.18849\n",
      "epoch no.4 train no.351840  loss = 3.62726 avg_loss = 3.22087\n",
      "epoch no.4 train no.351850  loss = 3.51216 avg_loss = 3.24953\n",
      "epoch no.4 train no.351860  loss = 2.75489 avg_loss = 3.25324\n",
      "epoch no.4 train no.351870  loss = 2.37627 avg_loss = 3.26141\n",
      "epoch no.4 train no.351880  loss = 3.19911 avg_loss = 3.25683\n",
      "epoch no.4 train no.351890  loss = 2.29736 avg_loss = 3.26221\n",
      "epoch no.4 train no.351900  loss = 3.35393 avg_loss = 3.23213\n",
      "epoch no.4 train no.351910  loss = 2.86019 avg_loss = 3.23541\n",
      "epoch no.4 train no.351920  loss = 2.79435 avg_loss = 3.26521\n",
      "epoch no.4 train no.351930  loss = 2.80254 avg_loss = 3.27850\n",
      "epoch no.4 train no.351940  loss = 3.69096 avg_loss = 3.30363\n",
      "epoch no.4 train no.351950  loss = 3.20082 avg_loss = 3.30633\n",
      "epoch no.4 train no.351960  loss = 4.04939 avg_loss = 3.34692\n",
      "epoch no.4 train no.351970  loss = 2.52937 avg_loss = 3.30074\n",
      "epoch no.4 train no.351980  loss = 5.78725 avg_loss = 3.30527\n",
      "epoch no.4 train no.351990  loss = 2.24143 avg_loss = 3.24778\n",
      "epoch no.4 train no.352000  loss = 2.07119 avg_loss = 3.23982\n",
      "4\n",
      "to_tokens: ['▁가을', '▁2000', '년대', '▁발라', '▁발라', '</s>']\n",
      "추억의 2000년대 가요 베스트</s>\n",
      "epoch no.4 train no.352010  loss = 4.67915 avg_loss = 3.28952\n",
      "epoch no.4 train no.352020  loss = 3.26654 avg_loss = 3.28963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.352030  loss = 2.44141 avg_loss = 3.30546\n",
      "epoch no.4 train no.352040  loss = 2.57852 avg_loss = 3.30586\n",
      "epoch no.4 train no.352050  loss = 3.76215 avg_loss = 3.29435\n",
      "epoch no.4 train no.352060  loss = 2.76834 avg_loss = 3.25986\n",
      "epoch no.4 train no.352070  loss = 4.15404 avg_loss = 3.22870\n",
      "epoch no.4 train no.352080  loss = 1.86880 avg_loss = 3.22206\n",
      "epoch no.4 train no.352090  loss = 3.68677 avg_loss = 3.20057\n",
      "epoch no.4 train no.352100  loss = 3.20425 avg_loss = 3.18612\n",
      "epoch no.4 train no.352110  loss = 2.53812 avg_loss = 3.19225\n",
      "epoch no.4 train no.352120  loss = 2.92902 avg_loss = 3.22405\n",
      "epoch no.4 train no.352130  loss = 2.67423 avg_loss = 3.19102\n",
      "epoch no.4 train no.352140  loss = 2.94870 avg_loss = 3.18103\n",
      "epoch no.4 train no.352150  loss = 2.09114 avg_loss = 3.13814\n",
      "epoch no.4 train no.352160  loss = 2.62937 avg_loss = 3.10808\n",
      "epoch no.4 train no.352170  loss = 3.07600 avg_loss = 3.09791\n",
      "epoch no.4 train no.352180  loss = 1.95557 avg_loss = 3.07586\n",
      "epoch no.4 train no.352190  loss = 3.23174 avg_loss = 3.09404\n",
      "epoch no.4 train no.352200  loss = 4.59334 avg_loss = 3.12235\n",
      "epoch no.4 train no.352210  loss = 2.77913 avg_loss = 3.11070\n",
      "epoch no.4 train no.352220  loss = 2.77472 avg_loss = 3.12430\n",
      "epoch no.4 train no.352230  loss = 3.55723 avg_loss = 3.14827\n",
      "epoch no.4 train no.352240  loss = 2.23432 avg_loss = 3.14847\n",
      "epoch no.4 train no.352250  loss = 2.72437 avg_loss = 3.15544\n",
      "epoch no.4 train no.352260  loss = 2.83077 avg_loss = 3.11811\n",
      "epoch no.4 train no.352270  loss = 3.12982 avg_loss = 3.14623\n",
      "epoch no.4 train no.352280  loss = 4.27195 avg_loss = 3.19263\n",
      "epoch no.4 train no.352290  loss = 3.04257 avg_loss = 3.20464\n",
      "epoch no.4 train no.352300  loss = 2.16201 avg_loss = 3.17212\n",
      "epoch no.4 train no.352310  loss = 2.94818 avg_loss = 3.18734\n",
      "epoch no.4 train no.352320  loss = 5.64116 avg_loss = 3.20518\n",
      "epoch no.4 train no.352330  loss = 2.72220 avg_loss = 3.19345\n",
      "epoch no.4 train no.352340  loss = 2.99030 avg_loss = 3.20243\n",
      "epoch no.4 train no.352350  loss = 2.69480 avg_loss = 3.20547\n",
      "epoch no.4 train no.352360  loss = 2.84042 avg_loss = 3.22464\n",
      "epoch no.4 train no.352370  loss = 3.88790 avg_loss = 3.23555\n",
      "epoch no.4 train no.352380  loss = 3.18139 avg_loss = 3.25751\n",
      "epoch no.4 train no.352390  loss = 2.26752 avg_loss = 3.29387\n",
      "epoch no.4 train no.352400  loss = 4.05757 avg_loss = 3.28484\n",
      "epoch no.4 train no.352410  loss = 2.67128 avg_loss = 3.31035\n",
      "epoch no.4 train no.352420  loss = 2.27829 avg_loss = 3.27754\n",
      "epoch no.4 train no.352430  loss = 2.51071 avg_loss = 3.27839\n",
      "epoch no.4 train no.352440  loss = 4.67936 avg_loss = 3.23543\n",
      "epoch no.4 train no.352450  loss = 1.91645 avg_loss = 3.22102\n",
      "epoch no.4 train no.352460  loss = 3.41560 avg_loss = 3.25547\n",
      "epoch no.4 train no.352470  loss = 1.75500 avg_loss = 3.22749\n",
      "epoch no.4 train no.352480  loss = 2.62110 avg_loss = 3.27324\n",
      "epoch no.4 train no.352490  loss = 3.57794 avg_loss = 3.22119\n",
      "epoch no.4 train no.352500  loss = 2.63357 avg_loss = 3.22481\n",
      "epoch no.4 train no.352510  loss = 2.89974 avg_loss = 3.22586\n",
      "epoch no.4 train no.352520  loss = 2.78975 avg_loss = 3.25881\n",
      "epoch no.4 train no.352530  loss = 2.78186 avg_loss = 3.23748\n",
      "epoch no.4 train no.352540  loss = 3.08719 avg_loss = 3.25458\n",
      "epoch no.4 train no.352550  loss = 3.72051 avg_loss = 3.24856\n",
      "epoch no.4 train no.352560  loss = 1.75255 avg_loss = 3.21571\n",
      "epoch no.4 train no.352570  loss = 2.12920 avg_loss = 3.23837\n",
      "epoch no.4 train no.352580  loss = 2.09254 avg_loss = 3.27914\n",
      "epoch no.4 train no.352590  loss = 2.45669 avg_loss = 3.27096\n",
      "epoch no.4 train no.352600  loss = 2.46615 avg_loss = 3.25926\n",
      "epoch no.4 train no.352610  loss = 2.66898 avg_loss = 3.23895\n",
      "epoch no.4 train no.352620  loss = 1.80621 avg_loss = 3.24795\n",
      "epoch no.4 train no.352630  loss = 4.84885 avg_loss = 3.28508\n",
      "epoch no.4 train no.352640  loss = 1.98256 avg_loss = 3.24104\n",
      "epoch no.4 train no.352650  loss = 2.86987 avg_loss = 3.23190\n",
      "epoch no.4 train no.352660  loss = 3.99335 avg_loss = 3.29124\n",
      "epoch no.4 train no.352670  loss = 3.73600 avg_loss = 3.27867\n",
      "epoch no.4 train no.352680  loss = 2.26787 avg_loss = 3.31558\n",
      "epoch no.4 train no.352690  loss = 2.91119 avg_loss = 3.32168\n",
      "epoch no.4 train no.352700  loss = 3.61175 avg_loss = 3.32326\n",
      "epoch no.4 train no.352710  loss = 3.70170 avg_loss = 3.37365\n",
      "epoch no.4 train no.352720  loss = 3.78866 avg_loss = 3.33842\n",
      "epoch no.4 train no.352730  loss = 2.06621 avg_loss = 3.32522\n",
      "epoch no.4 train no.352740  loss = 4.00553 avg_loss = 3.37331\n",
      "epoch no.4 train no.352750  loss = 1.90486 avg_loss = 3.36992\n",
      "epoch no.4 train no.352760  loss = 4.27049 avg_loss = 3.36913\n",
      "epoch no.4 train no.352770  loss = 4.00317 avg_loss = 3.30640\n",
      "epoch no.4 train no.352780  loss = 3.16215 avg_loss = 3.27416\n",
      "epoch no.4 train no.352790  loss = 3.65723 avg_loss = 3.25230\n",
      "epoch no.4 train no.352800  loss = 2.77691 avg_loss = 3.21633\n",
      "epoch no.4 train no.352810  loss = 3.53336 avg_loss = 3.24183\n",
      "epoch no.4 train no.352820  loss = 4.65777 avg_loss = 3.25641\n",
      "epoch no.4 train no.352830  loss = 2.92033 avg_loss = 3.26019\n",
      "epoch no.4 train no.352840  loss = 2.21603 avg_loss = 3.21517\n",
      "epoch no.4 train no.352850  loss = 4.66174 avg_loss = 3.26571\n",
      "epoch no.4 train no.352860  loss = 3.63571 avg_loss = 3.28520\n",
      "epoch no.4 train no.352870  loss = 4.30059 avg_loss = 3.29664\n",
      "epoch no.4 train no.352880  loss = 2.46747 avg_loss = 3.31077\n",
      "epoch no.4 train no.352890  loss = 3.01832 avg_loss = 3.28532\n",
      "epoch no.4 train no.352900  loss = 2.12941 avg_loss = 3.25449\n",
      "epoch no.4 train no.352910  loss = 2.91482 avg_loss = 3.25129\n",
      "epoch no.4 train no.352920  loss = 4.84262 avg_loss = 3.25790\n",
      "epoch no.4 train no.352930  loss = 4.10750 avg_loss = 3.23678\n",
      "epoch no.4 train no.352940  loss = 3.99753 avg_loss = 3.25273\n",
      "epoch no.4 train no.352950  loss = 2.84742 avg_loss = 3.22185\n",
      "epoch no.4 train no.352960  loss = 3.71951 avg_loss = 3.25353\n",
      "epoch no.4 train no.352970  loss = 3.12693 avg_loss = 3.28460\n",
      "epoch no.4 train no.352980  loss = 3.81141 avg_loss = 3.30372\n",
      "epoch no.4 train no.352990  loss = 2.89582 avg_loss = 3.27028\n",
      "epoch no.4 train no.353000  loss = 2.79710 avg_loss = 3.25094\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '팝', '합', '모', '음', '</s>']\n",
      "추억의 올드힙합모음</s>\n",
      "epoch no.4 train no.353010  loss = 2.71921 avg_loss = 3.28118\n",
      "epoch no.4 train no.353020  loss = 2.34634 avg_loss = 3.25795\n",
      "epoch no.4 train no.353030  loss = 3.07248 avg_loss = 3.25898\n",
      "epoch no.4 train no.353040  loss = 2.57753 avg_loss = 3.25053\n",
      "epoch no.4 train no.353050  loss = 3.17121 avg_loss = 3.25678\n",
      "epoch no.4 train no.353060  loss = 2.22936 avg_loss = 3.29640\n",
      "epoch no.4 train no.353070  loss = 2.69465 avg_loss = 3.26147\n",
      "epoch no.4 train no.353080  loss = 2.64259 avg_loss = 3.25772\n",
      "epoch no.4 train no.353090  loss = 3.95816 avg_loss = 3.23749\n",
      "epoch no.4 train no.353100  loss = 2.53169 avg_loss = 3.27270\n",
      "epoch no.4 train no.353110  loss = 6.69743 avg_loss = 3.31732\n",
      "epoch no.4 train no.353120  loss = 4.10959 avg_loss = 3.32722\n",
      "epoch no.4 train no.353130  loss = 3.10059 avg_loss = 3.31976\n",
      "epoch no.4 train no.353140  loss = 2.63989 avg_loss = 3.30174\n",
      "epoch no.4 train no.353150  loss = 3.12016 avg_loss = 3.27120\n",
      "epoch no.4 train no.353160  loss = 4.59962 avg_loss = 3.31039\n",
      "epoch no.4 train no.353170  loss = 6.77877 avg_loss = 3.39632\n",
      "epoch no.4 train no.353180  loss = 3.02452 avg_loss = 3.35504\n",
      "epoch no.4 train no.353190  loss = 2.48089 avg_loss = 3.32718\n",
      "epoch no.4 train no.353200  loss = 3.66064 avg_loss = 3.32800\n",
      "epoch no.4 train no.353210  loss = 2.04197 avg_loss = 3.31211\n",
      "epoch no.4 train no.353220  loss = 3.51296 avg_loss = 3.27542\n",
      "epoch no.4 train no.353230  loss = 3.70070 avg_loss = 3.26867\n",
      "epoch no.4 train no.353240  loss = 2.60035 avg_loss = 3.26279\n",
      "epoch no.4 train no.353250  loss = 3.42833 avg_loss = 3.21049\n",
      "epoch no.4 train no.353260  loss = 2.32522 avg_loss = 3.19408\n",
      "epoch no.4 train no.353270  loss = 4.07480 avg_loss = 3.17307\n",
      "epoch no.4 train no.353280  loss = 2.49790 avg_loss = 3.18741\n",
      "epoch no.4 train no.353290  loss = 2.50490 avg_loss = 3.18946\n",
      "epoch no.4 train no.353300  loss = 2.77883 avg_loss = 3.16226\n",
      "epoch no.4 train no.353310  loss = 3.71053 avg_loss = 3.15473\n",
      "epoch no.4 train no.353320  loss = 2.76629 avg_loss = 3.15665\n",
      "epoch no.4 train no.353330  loss = 1.99833 avg_loss = 3.16133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.353340  loss = 3.66976 avg_loss = 3.15538\n",
      "epoch no.4 train no.353350  loss = 2.38821 avg_loss = 3.15534\n",
      "epoch no.4 train no.353360  loss = 2.75451 avg_loss = 3.12885\n",
      "epoch no.4 train no.353370  loss = 1.97164 avg_loss = 3.12003\n",
      "epoch no.4 train no.353380  loss = 4.47944 avg_loss = 3.15205\n",
      "epoch no.4 train no.353390  loss = 4.01807 avg_loss = 3.18977\n",
      "epoch no.4 train no.353400  loss = 3.71219 avg_loss = 3.15662\n",
      "epoch no.4 train no.353410  loss = 2.32694 avg_loss = 3.17802\n",
      "epoch no.4 train no.353420  loss = 3.19641 avg_loss = 3.19558\n",
      "epoch no.4 train no.353430  loss = 2.50515 avg_loss = 3.20098\n",
      "epoch no.4 train no.353440  loss = 3.97426 avg_loss = 3.17903\n",
      "epoch no.4 train no.353450  loss = 2.37760 avg_loss = 3.16292\n",
      "epoch no.4 train no.353460  loss = 3.21528 avg_loss = 3.15691\n",
      "epoch no.4 train no.353470  loss = 4.16422 avg_loss = 3.13098\n",
      "epoch no.4 train no.353480  loss = 2.95761 avg_loss = 3.15339\n",
      "epoch no.4 train no.353490  loss = 4.03059 avg_loss = 3.17271\n",
      "epoch no.4 train no.353500  loss = 2.62423 avg_loss = 3.12512\n",
      "epoch no.4 train no.353510  loss = 2.11858 avg_loss = 3.13200\n",
      "epoch no.4 train no.353520  loss = 2.71661 avg_loss = 3.17442\n",
      "epoch no.4 train no.353530  loss = 2.45093 avg_loss = 3.20095\n",
      "epoch no.4 train no.353540  loss = 1.87877 avg_loss = 3.14945\n",
      "epoch no.4 train no.353550  loss = 3.25061 avg_loss = 3.12073\n",
      "epoch no.4 train no.353560  loss = 3.40505 avg_loss = 3.13129\n",
      "epoch no.4 train no.353570  loss = 1.75234 avg_loss = 3.11471\n",
      "epoch no.4 train no.353580  loss = 2.26417 avg_loss = 3.13927\n",
      "epoch no.4 train no.353590  loss = 2.40766 avg_loss = 3.16280\n",
      "epoch no.4 train no.353600  loss = 4.23503 avg_loss = 3.18446\n",
      "epoch no.4 train no.353610  loss = 2.97629 avg_loss = 3.18152\n",
      "epoch no.4 train no.353620  loss = 2.75245 avg_loss = 3.15781\n",
      "epoch no.4 train no.353630  loss = 5.13918 avg_loss = 3.18301\n",
      "epoch no.4 train no.353640  loss = 1.98831 avg_loss = 3.18042\n",
      "epoch no.4 train no.353650  loss = 2.99545 avg_loss = 3.19307\n",
      "epoch no.4 train no.353660  loss = 2.75181 avg_loss = 3.15713\n",
      "epoch no.4 train no.353670  loss = 2.84202 avg_loss = 3.14230\n",
      "epoch no.4 train no.353680  loss = 5.17906 avg_loss = 3.21054\n",
      "epoch no.4 train no.353690  loss = 1.85336 avg_loss = 3.17123\n",
      "epoch no.4 train no.353700  loss = 1.74135 avg_loss = 3.14733\n",
      "epoch no.4 train no.353710  loss = 3.23343 avg_loss = 3.19610\n",
      "epoch no.4 train no.353720  loss = 3.46868 avg_loss = 3.20723\n",
      "epoch no.4 train no.353730  loss = 2.26706 avg_loss = 3.15250\n",
      "epoch no.4 train no.353740  loss = 3.60791 avg_loss = 3.16821\n",
      "epoch no.4 train no.353750  loss = 3.74207 avg_loss = 3.12141\n",
      "epoch no.4 train no.353760  loss = 3.06647 avg_loss = 3.12860\n",
      "epoch no.4 train no.353770  loss = 3.12052 avg_loss = 3.09539\n",
      "epoch no.4 train no.353780  loss = 4.08114 avg_loss = 3.12070\n",
      "epoch no.4 train no.353790  loss = 4.14491 avg_loss = 3.13306\n",
      "epoch no.4 train no.353800  loss = 2.90428 avg_loss = 3.14100\n",
      "epoch no.4 train no.353810  loss = 3.06592 avg_loss = 3.18726\n",
      "epoch no.4 train no.353820  loss = 3.09312 avg_loss = 3.16689\n",
      "epoch no.4 train no.353830  loss = 2.92546 avg_loss = 3.17652\n",
      "epoch no.4 train no.353840  loss = 2.15721 avg_loss = 3.11534\n",
      "epoch no.4 train no.353850  loss = 2.81429 avg_loss = 3.17345\n",
      "epoch no.4 train no.353860  loss = 2.30104 avg_loss = 3.19274\n",
      "epoch no.4 train no.353870  loss = 2.55056 avg_loss = 3.16250\n",
      "epoch no.4 train no.353880  loss = 2.41538 avg_loss = 3.14833\n",
      "epoch no.4 train no.353890  loss = 2.63850 avg_loss = 3.16394\n",
      "epoch no.4 train no.353900  loss = 3.16283 avg_loss = 3.19389\n",
      "epoch no.4 train no.353910  loss = 1.99873 avg_loss = 3.17455\n",
      "epoch no.4 train no.353920  loss = 3.93171 avg_loss = 3.22246\n",
      "epoch no.4 train no.353930  loss = 3.11442 avg_loss = 3.22905\n",
      "epoch no.4 train no.353940  loss = 4.53217 avg_loss = 3.27296\n",
      "epoch no.4 train no.353950  loss = 4.42349 avg_loss = 3.27575\n",
      "epoch no.4 train no.353960  loss = 2.14262 avg_loss = 3.24151\n",
      "epoch no.4 train no.353970  loss = 2.26457 avg_loss = 3.20771\n",
      "epoch no.4 train no.353980  loss = 4.16388 avg_loss = 3.21679\n",
      "epoch no.4 train no.353990  loss = 2.10278 avg_loss = 3.22488\n",
      "epoch no.4 train no.354000  loss = 2.29742 avg_loss = 3.17696\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '곡', '들', '드', '</s>']\n",
      "추억의 명곡 발라드</s>\n",
      "epoch no.4 train no.354010  loss = 2.91121 avg_loss = 3.17384\n",
      "epoch no.4 train no.354020  loss = 2.81615 avg_loss = 3.19635\n",
      "epoch no.4 train no.354030  loss = 5.26109 avg_loss = 3.17696\n",
      "epoch no.4 train no.354040  loss = 1.98417 avg_loss = 3.18124\n",
      "epoch no.4 train no.354050  loss = 3.50299 avg_loss = 3.17316\n",
      "epoch no.4 train no.354060  loss = 2.16130 avg_loss = 3.25767\n",
      "epoch no.4 train no.354070  loss = 1.90043 avg_loss = 3.22769\n",
      "epoch no.4 train no.354080  loss = 4.25867 avg_loss = 3.20356\n",
      "epoch no.4 train no.354090  loss = 3.04870 avg_loss = 3.20985\n",
      "epoch no.4 train no.354100  loss = 2.76788 avg_loss = 3.21676\n",
      "epoch no.4 train no.354110  loss = 3.94355 avg_loss = 3.27199\n",
      "epoch no.4 train no.354120  loss = 5.10101 avg_loss = 3.26326\n",
      "epoch no.4 train no.354130  loss = 2.96286 avg_loss = 3.22900\n",
      "epoch no.4 train no.354140  loss = 3.72337 avg_loss = 3.22856\n",
      "epoch no.4 train no.354150  loss = 2.34786 avg_loss = 3.21217\n",
      "epoch no.4 train no.354160  loss = 2.33669 avg_loss = 3.21755\n",
      "epoch no.4 train no.354170  loss = 4.18088 avg_loss = 3.25340\n",
      "epoch no.4 train no.354180  loss = 4.83989 avg_loss = 3.21534\n",
      "epoch no.4 train no.354190  loss = 2.78477 avg_loss = 3.23287\n",
      "epoch no.4 train no.354200  loss = 2.96067 avg_loss = 3.23094\n",
      "epoch no.4 train no.354210  loss = 4.30842 avg_loss = 3.23899\n",
      "epoch no.4 train no.354220  loss = 3.03480 avg_loss = 3.19936\n",
      "epoch no.4 train no.354230  loss = 3.48958 avg_loss = 3.15605\n",
      "epoch no.4 train no.354240  loss = 3.17407 avg_loss = 3.16980\n",
      "epoch no.4 train no.354250  loss = 3.11214 avg_loss = 3.16601\n",
      "epoch no.4 train no.354260  loss = 3.24858 avg_loss = 3.17341\n",
      "epoch no.4 train no.354270  loss = 3.11797 avg_loss = 3.17486\n",
      "epoch no.4 train no.354280  loss = 2.18654 avg_loss = 3.16569\n",
      "epoch no.4 train no.354290  loss = 4.54298 avg_loss = 3.20758\n",
      "epoch no.4 train no.354300  loss = 3.47948 avg_loss = 3.19652\n",
      "epoch no.4 train no.354310  loss = 2.42681 avg_loss = 3.20391\n",
      "epoch no.4 train no.354320  loss = 3.11589 avg_loss = 3.17863\n",
      "epoch no.4 train no.354330  loss = 2.41271 avg_loss = 3.15167\n",
      "epoch no.4 train no.354340  loss = 2.77853 avg_loss = 3.15021\n",
      "epoch no.4 train no.354350  loss = 3.06125 avg_loss = 3.20172\n",
      "epoch no.4 train no.354360  loss = 2.66819 avg_loss = 3.21893\n",
      "epoch no.4 train no.354370  loss = 2.98990 avg_loss = 3.22207\n",
      "epoch no.4 train no.354380  loss = 6.62194 avg_loss = 3.26756\n",
      "epoch no.4 train no.354390  loss = 2.29103 avg_loss = 3.28254\n",
      "epoch no.4 train no.354400  loss = 2.71690 avg_loss = 3.23315\n",
      "epoch no.4 train no.354410  loss = 1.94416 avg_loss = 3.22165\n",
      "epoch no.4 train no.354420  loss = 3.58359 avg_loss = 3.20003\n",
      "epoch no.4 train no.354430  loss = 3.33385 avg_loss = 3.22418\n",
      "epoch no.4 train no.354440  loss = 2.88112 avg_loss = 3.24356\n",
      "epoch no.4 train no.354450  loss = 4.31220 avg_loss = 3.27276\n",
      "epoch no.4 train no.354460  loss = 5.17250 avg_loss = 3.31363\n",
      "epoch no.4 train no.354470  loss = 4.33286 avg_loss = 3.30239\n",
      "epoch no.4 train no.354480  loss = 2.74314 avg_loss = 3.29315\n",
      "epoch no.4 train no.354490  loss = 3.05730 avg_loss = 3.27482\n",
      "epoch no.4 train no.354500  loss = 3.60676 avg_loss = 3.24325\n",
      "epoch no.4 train no.354510  loss = 3.85056 avg_loss = 3.20847\n",
      "epoch no.4 train no.354520  loss = 3.45119 avg_loss = 3.26239\n",
      "epoch no.4 train no.354530  loss = 3.86761 avg_loss = 3.26757\n",
      "epoch no.4 train no.354540  loss = 2.10417 avg_loss = 3.24801\n",
      "epoch no.4 train no.354550  loss = 3.68047 avg_loss = 3.23641\n",
      "epoch no.4 train no.354560  loss = 2.76497 avg_loss = 3.23815\n",
      "epoch no.4 train no.354570  loss = 2.80021 avg_loss = 3.23237\n",
      "epoch no.4 train no.354580  loss = 3.74363 avg_loss = 3.21614\n",
      "epoch no.4 train no.354590  loss = 2.52245 avg_loss = 3.20474\n",
      "epoch no.4 train no.354600  loss = 4.00312 avg_loss = 3.15888\n",
      "epoch no.4 train no.354610  loss = 3.24553 avg_loss = 3.15334\n",
      "epoch no.4 train no.354620  loss = 3.28145 avg_loss = 3.19753\n",
      "epoch no.4 train no.354630  loss = 3.69851 avg_loss = 3.21402\n",
      "epoch no.4 train no.354640  loss = 2.21337 avg_loss = 3.17069\n",
      "epoch no.4 train no.354650  loss = 4.49322 avg_loss = 3.26586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.354660  loss = 2.80700 avg_loss = 3.23213\n",
      "epoch no.4 train no.354670  loss = 2.56101 avg_loss = 3.20811\n",
      "epoch no.4 train no.354680  loss = 3.41393 avg_loss = 3.19549\n",
      "epoch no.4 train no.354690  loss = 2.15053 avg_loss = 3.16118\n",
      "epoch no.4 train no.354700  loss = 2.21960 avg_loss = 3.15667\n",
      "epoch no.4 train no.354710  loss = 2.68701 avg_loss = 3.14801\n",
      "epoch no.4 train no.354720  loss = 4.27382 avg_loss = 3.20050\n",
      "epoch no.4 train no.354730  loss = 2.46745 avg_loss = 3.16227\n",
      "epoch no.4 train no.354740  loss = 2.43846 avg_loss = 3.13535\n",
      "epoch no.4 train no.354750  loss = 1.96792 avg_loss = 3.13477\n",
      "epoch no.4 train no.354760  loss = 5.62269 avg_loss = 3.15716\n",
      "epoch no.4 train no.354770  loss = 4.35482 avg_loss = 3.21373\n",
      "epoch no.4 train no.354780  loss = 2.56905 avg_loss = 3.18954\n",
      "epoch no.4 train no.354790  loss = 2.27878 avg_loss = 3.16727\n",
      "epoch no.4 train no.354800  loss = 4.03678 avg_loss = 3.18605\n",
      "epoch no.4 train no.354810  loss = 2.85435 avg_loss = 3.20097\n",
      "epoch no.4 train no.354820  loss = 3.92198 avg_loss = 3.27952\n",
      "epoch no.4 train no.354830  loss = 2.24736 avg_loss = 3.28273\n",
      "epoch no.4 train no.354840  loss = 2.52931 avg_loss = 3.27968\n",
      "epoch no.4 train no.354850  loss = 2.32257 avg_loss = 3.28358\n",
      "epoch no.4 train no.354860  loss = 2.55996 avg_loss = 3.23300\n",
      "epoch no.4 train no.354870  loss = 2.32822 avg_loss = 3.25523\n",
      "epoch no.4 train no.354880  loss = 3.37987 avg_loss = 3.24267\n",
      "epoch no.4 train no.354890  loss = 4.07326 avg_loss = 3.26242\n",
      "epoch no.4 train no.354900  loss = 4.03840 avg_loss = 3.28993\n",
      "epoch no.4 train no.354910  loss = 3.98692 avg_loss = 3.29366\n",
      "epoch no.4 train no.354920  loss = 3.80538 avg_loss = 3.28689\n",
      "epoch no.4 train no.354930  loss = 3.15116 avg_loss = 3.22400\n",
      "epoch no.4 train no.354940  loss = 6.76859 avg_loss = 3.25525\n",
      "epoch no.4 train no.354950  loss = 4.34271 avg_loss = 3.25583\n",
      "epoch no.4 train no.354960  loss = 5.01159 avg_loss = 3.24498\n",
      "epoch no.4 train no.354970  loss = 2.01507 avg_loss = 3.20367\n",
      "epoch no.4 train no.354980  loss = 4.22589 avg_loss = 3.24030\n",
      "epoch no.4 train no.354990  loss = 3.52217 avg_loss = 3.25455\n",
      "epoch no.4 train no.355000  loss = 3.27904 avg_loss = 3.23355\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.4 train no.355010  loss = 4.04305 avg_loss = 3.22507\n",
      "epoch no.4 train no.355020  loss = 2.08293 avg_loss = 3.25985\n",
      "epoch no.4 train no.355030  loss = 3.43637 avg_loss = 3.29186\n",
      "epoch no.4 train no.355040  loss = 2.44809 avg_loss = 3.26706\n",
      "epoch no.4 train no.355050  loss = 2.93568 avg_loss = 3.27290\n",
      "epoch no.4 train no.355060  loss = 1.71171 avg_loss = 3.29122\n",
      "epoch no.4 train no.355070  loss = 2.05098 avg_loss = 3.24626\n",
      "epoch no.4 train no.355080  loss = 2.97826 avg_loss = 3.26459\n",
      "epoch no.4 train no.355090  loss = 3.70543 avg_loss = 3.23922\n",
      "epoch no.4 train no.355100  loss = 2.88125 avg_loss = 3.26141\n",
      "epoch no.4 train no.355110  loss = 3.39546 avg_loss = 3.22699\n",
      "epoch no.4 train no.355120  loss = 4.44687 avg_loss = 3.24956\n",
      "epoch no.4 train no.355130  loss = 2.56378 avg_loss = 3.24187\n",
      "epoch no.4 train no.355140  loss = 3.76852 avg_loss = 3.21622\n",
      "epoch no.4 train no.355150  loss = 2.74413 avg_loss = 3.18555\n",
      "epoch no.4 train no.355160  loss = 2.85901 avg_loss = 3.16661\n",
      "epoch no.4 train no.355170  loss = 4.13548 avg_loss = 3.17684\n",
      "epoch no.4 train no.355180  loss = 1.95812 avg_loss = 3.15552\n",
      "epoch no.4 train no.355190  loss = 5.40880 avg_loss = 3.18371\n",
      "epoch no.4 train no.355200  loss = 3.01709 avg_loss = 3.23077\n",
      "epoch no.4 train no.355210  loss = 1.65525 avg_loss = 3.18551\n",
      "epoch no.4 train no.355220  loss = 2.21730 avg_loss = 3.20039\n",
      "epoch no.4 train no.355230  loss = 4.66938 avg_loss = 3.20237\n",
      "epoch no.4 train no.355240  loss = 1.82110 avg_loss = 3.24045\n",
      "epoch no.4 train no.355250  loss = 4.75261 avg_loss = 3.23146\n",
      "epoch no.4 train no.355260  loss = 3.83662 avg_loss = 3.21822\n",
      "epoch no.4 train no.355270  loss = 3.27510 avg_loss = 3.21521\n",
      "epoch no.4 train no.355280  loss = 1.71897 avg_loss = 3.23453\n",
      "epoch no.4 train no.355290  loss = 4.33599 avg_loss = 3.27868\n",
      "epoch no.4 train no.355300  loss = 4.47939 avg_loss = 3.24817\n",
      "epoch no.4 train no.355310  loss = 3.30748 avg_loss = 3.27849\n",
      "epoch no.4 train no.355320  loss = 2.31305 avg_loss = 3.27490\n",
      "epoch no.4 train no.355330  loss = 3.62028 avg_loss = 3.26545\n",
      "epoch no.4 train no.355340  loss = 4.42342 avg_loss = 3.22314\n",
      "epoch no.4 train no.355350  loss = 2.29534 avg_loss = 3.23842\n",
      "epoch no.4 train no.355360  loss = 2.51730 avg_loss = 3.19745\n",
      "epoch no.4 train no.355370  loss = 1.83003 avg_loss = 3.22148\n",
      "epoch no.4 train no.355380  loss = 3.08087 avg_loss = 3.18854\n",
      "epoch no.4 train no.355390  loss = 2.44347 avg_loss = 3.17751\n",
      "epoch no.4 train no.355400  loss = 2.33700 avg_loss = 3.15160\n",
      "epoch no.4 train no.355410  loss = 3.29399 avg_loss = 3.15555\n",
      "epoch no.4 train no.355420  loss = 2.76261 avg_loss = 3.16150\n",
      "epoch no.4 train no.355430  loss = 3.75554 avg_loss = 3.15852\n",
      "epoch no.4 train no.355440  loss = 4.58360 avg_loss = 3.22231\n",
      "epoch no.4 train no.355450  loss = 4.57392 avg_loss = 3.20791\n",
      "epoch no.4 train no.355460  loss = 2.81916 avg_loss = 3.17039\n",
      "epoch no.4 train no.355470  loss = 4.18226 avg_loss = 3.16779\n",
      "epoch no.4 train no.355480  loss = 3.77307 avg_loss = 3.17258\n",
      "epoch no.4 train no.355490  loss = 3.05631 avg_loss = 3.14192\n",
      "epoch no.4 train no.355500  loss = 3.77899 avg_loss = 3.16391\n",
      "epoch no.4 train no.355510  loss = 2.50373 avg_loss = 3.12731\n",
      "epoch no.4 train no.355520  loss = 4.19228 avg_loss = 3.15318\n",
      "epoch no.4 train no.355530  loss = 3.68349 avg_loss = 3.16182\n",
      "epoch no.4 train no.355540  loss = 4.51201 avg_loss = 3.18569\n",
      "epoch no.4 train no.355550  loss = 2.97684 avg_loss = 3.22474\n",
      "epoch no.4 train no.355560  loss = 4.02558 avg_loss = 3.20422\n",
      "epoch no.4 train no.355570  loss = 3.96626 avg_loss = 3.21267\n",
      "epoch no.4 train no.355580  loss = 4.24600 avg_loss = 3.26472\n",
      "epoch no.4 train no.355590  loss = 5.36662 avg_loss = 3.27125\n",
      "epoch no.4 train no.355600  loss = 1.75456 avg_loss = 3.28680\n",
      "epoch no.4 train no.355610  loss = 2.83798 avg_loss = 3.27041\n",
      "epoch no.4 train no.355620  loss = 3.82547 avg_loss = 3.29589\n",
      "epoch no.4 train no.355630  loss = 3.49105 avg_loss = 3.26040\n",
      "epoch no.4 train no.355640  loss = 4.82020 avg_loss = 3.27796\n",
      "epoch no.4 train no.355650  loss = 2.89082 avg_loss = 3.28012\n",
      "epoch no.4 train no.355660  loss = 3.58154 avg_loss = 3.24797\n",
      "epoch no.4 train no.355670  loss = 2.08088 avg_loss = 3.24236\n",
      "epoch no.4 train no.355680  loss = 2.71339 avg_loss = 3.27854\n",
      "epoch no.4 train no.355690  loss = 2.38783 avg_loss = 3.26030\n",
      "epoch no.4 train no.355700  loss = 4.18899 avg_loss = 3.23969\n",
      "epoch no.4 train no.355710  loss = 3.29793 avg_loss = 3.21296\n",
      "epoch no.4 train no.355720  loss = 2.29435 avg_loss = 3.16646\n",
      "epoch no.4 train no.355730  loss = 2.68863 avg_loss = 3.19788\n",
      "epoch no.4 train no.355740  loss = 2.83037 avg_loss = 3.18901\n",
      "epoch no.4 train no.355750  loss = 2.48933 avg_loss = 3.19675\n",
      "epoch no.4 train no.355760  loss = 4.10164 avg_loss = 3.23504\n",
      "epoch no.4 train no.355770  loss = 2.79512 avg_loss = 3.23177\n",
      "epoch no.4 train no.355780  loss = 3.76862 avg_loss = 3.23400\n",
      "epoch no.4 train no.355790  loss = 3.92031 avg_loss = 3.25715\n",
      "epoch no.4 train no.355800  loss = 3.48952 avg_loss = 3.25648\n",
      "epoch no.4 train no.355810  loss = 4.61793 avg_loss = 3.26652\n",
      "epoch no.4 train no.355820  loss = 4.50600 avg_loss = 3.31438\n",
      "epoch no.4 train no.355830  loss = 2.85973 avg_loss = 3.29076\n",
      "epoch no.4 train no.355840  loss = 2.93293 avg_loss = 3.28006\n",
      "epoch no.4 train no.355850  loss = 3.33307 avg_loss = 3.30953\n",
      "epoch no.4 train no.355860  loss = 3.98828 avg_loss = 3.26900\n",
      "epoch no.4 train no.355870  loss = 3.22271 avg_loss = 3.26570\n",
      "epoch no.4 train no.355880  loss = 4.25503 avg_loss = 3.27894\n",
      "epoch no.4 train no.355890  loss = 3.42490 avg_loss = 3.23678\n",
      "epoch no.4 train no.355900  loss = 2.32819 avg_loss = 3.20825\n",
      "epoch no.4 train no.355910  loss = 3.00029 avg_loss = 3.18704\n",
      "epoch no.4 train no.355920  loss = 3.23562 avg_loss = 3.21788\n",
      "epoch no.4 train no.355930  loss = 3.91899 avg_loss = 3.21550\n",
      "epoch no.4 train no.355940  loss = 3.10011 avg_loss = 3.19170\n",
      "epoch no.4 train no.355950  loss = 3.60735 avg_loss = 3.20825\n",
      "epoch no.4 train no.355960  loss = 1.74375 avg_loss = 3.15133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.355970  loss = 5.40132 avg_loss = 3.17267\n",
      "epoch no.4 train no.355980  loss = 2.74052 avg_loss = 3.15581\n",
      "epoch no.4 train no.355990  loss = 4.71221 avg_loss = 3.15041\n",
      "epoch no.4 train no.356000  loss = 4.40417 avg_loss = 3.14464\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁가요', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.356010  loss = 2.45267 avg_loss = 3.16742\n",
      "epoch no.4 train no.356020  loss = 4.49275 avg_loss = 3.19102\n",
      "epoch no.4 train no.356030  loss = 3.21987 avg_loss = 3.14715\n",
      "epoch no.4 train no.356040  loss = 2.56107 avg_loss = 3.21946\n",
      "epoch no.4 train no.356050  loss = 3.81443 avg_loss = 3.27310\n",
      "epoch no.4 train no.356060  loss = 3.11400 avg_loss = 3.25449\n",
      "epoch no.4 train no.356070  loss = 2.97167 avg_loss = 3.24191\n",
      "epoch no.4 train no.356080  loss = 2.99962 avg_loss = 3.25084\n",
      "epoch no.4 train no.356090  loss = 2.63613 avg_loss = 3.29391\n",
      "epoch no.4 train no.356100  loss = 2.03781 avg_loss = 3.28368\n",
      "epoch no.4 train no.356110  loss = 2.30829 avg_loss = 3.31544\n",
      "epoch no.4 train no.356120  loss = 4.37530 avg_loss = 3.32679\n",
      "epoch no.4 train no.356130  loss = 3.27957 avg_loss = 3.35059\n",
      "epoch no.4 train no.356140  loss = 2.08852 avg_loss = 3.31988\n",
      "epoch no.4 train no.356150  loss = 2.50185 avg_loss = 3.32096\n",
      "epoch no.4 train no.356160  loss = 4.08140 avg_loss = 3.29147\n",
      "epoch no.4 train no.356170  loss = 5.45755 avg_loss = 3.30104\n",
      "epoch no.4 train no.356180  loss = 2.41803 avg_loss = 3.27620\n",
      "epoch no.4 train no.356190  loss = 3.14879 avg_loss = 3.30928\n",
      "epoch no.4 train no.356200  loss = 3.29238 avg_loss = 3.28090\n",
      "epoch no.4 train no.356210  loss = 3.39214 avg_loss = 3.27742\n",
      "epoch no.4 train no.356220  loss = 3.59408 avg_loss = 3.32287\n",
      "epoch no.4 train no.356230  loss = 2.61398 avg_loss = 3.28556\n",
      "epoch no.4 train no.356240  loss = 3.37055 avg_loss = 3.26607\n",
      "epoch no.4 train no.356250  loss = 2.92701 avg_loss = 3.25780\n",
      "epoch no.4 train no.356260  loss = 2.61303 avg_loss = 3.26890\n",
      "epoch no.4 train no.356270  loss = 2.97429 avg_loss = 3.29309\n",
      "epoch no.4 train no.356280  loss = 2.17714 avg_loss = 3.26244\n",
      "epoch no.4 train no.356290  loss = 2.74677 avg_loss = 3.23352\n",
      "epoch no.4 train no.356300  loss = 3.48992 avg_loss = 3.23536\n",
      "epoch no.4 train no.356310  loss = 2.45906 avg_loss = 3.24803\n",
      "epoch no.4 train no.356320  loss = 3.66974 avg_loss = 3.24735\n",
      "epoch no.4 train no.356330  loss = 3.52558 avg_loss = 3.25023\n",
      "epoch no.4 train no.356340  loss = 2.56384 avg_loss = 3.22017\n",
      "epoch no.4 train no.356350  loss = 2.82647 avg_loss = 3.22542\n",
      "epoch no.4 train no.356360  loss = 4.78415 avg_loss = 3.22384\n",
      "epoch no.4 train no.356370  loss = 4.76588 avg_loss = 3.22624\n",
      "epoch no.4 train no.356380  loss = 2.50908 avg_loss = 3.20836\n",
      "epoch no.4 train no.356390  loss = 5.02547 avg_loss = 3.20939\n",
      "epoch no.4 train no.356400  loss = 2.58186 avg_loss = 3.20389\n",
      "epoch no.4 train no.356410  loss = 3.35705 avg_loss = 3.18973\n",
      "epoch no.4 train no.356420  loss = 2.77429 avg_loss = 3.20599\n",
      "epoch no.4 train no.356430  loss = 3.36142 avg_loss = 3.20630\n",
      "epoch no.4 train no.356440  loss = 4.51436 avg_loss = 3.24263\n",
      "epoch no.4 train no.356450  loss = 3.46031 avg_loss = 3.27708\n",
      "epoch no.4 train no.356460  loss = 4.34359 avg_loss = 3.27048\n",
      "epoch no.4 train no.356470  loss = 4.88239 avg_loss = 3.27853\n",
      "epoch no.4 train no.356480  loss = 2.61047 avg_loss = 3.27544\n",
      "epoch no.4 train no.356490  loss = 3.19795 avg_loss = 3.25092\n",
      "epoch no.4 train no.356500  loss = 3.78688 avg_loss = 3.25609\n",
      "epoch no.4 train no.356510  loss = 2.89326 avg_loss = 3.22327\n",
      "epoch no.4 train no.356520  loss = 3.76526 avg_loss = 3.26685\n",
      "epoch no.4 train no.356530  loss = 2.99518 avg_loss = 3.26973\n",
      "epoch no.4 train no.356540  loss = 4.42743 avg_loss = 3.29442\n",
      "epoch no.4 train no.356550  loss = 3.38162 avg_loss = 3.24964\n",
      "epoch no.4 train no.356560  loss = 3.14514 avg_loss = 3.25358\n",
      "epoch no.4 train no.356570  loss = 2.42531 avg_loss = 3.25021\n",
      "epoch no.4 train no.356580  loss = 3.99622 avg_loss = 3.31365\n",
      "epoch no.4 train no.356590  loss = 3.62941 avg_loss = 3.26444\n",
      "epoch no.4 train no.356600  loss = 3.61032 avg_loss = 3.23623\n",
      "epoch no.4 train no.356610  loss = 2.57573 avg_loss = 3.23505\n",
      "epoch no.4 train no.356620  loss = 5.38623 avg_loss = 3.23654\n",
      "epoch no.4 train no.356630  loss = 3.20566 avg_loss = 3.28343\n",
      "epoch no.4 train no.356640  loss = 3.24009 avg_loss = 3.26469\n",
      "epoch no.4 train no.356650  loss = 2.73772 avg_loss = 3.23900\n",
      "epoch no.4 train no.356660  loss = 1.53870 avg_loss = 3.22848\n",
      "epoch no.4 train no.356670  loss = 3.32310 avg_loss = 3.18475\n",
      "epoch no.4 train no.356680  loss = 3.01216 avg_loss = 3.15302\n",
      "epoch no.4 train no.356690  loss = 3.00085 avg_loss = 3.14837\n",
      "epoch no.4 train no.356700  loss = 2.69694 avg_loss = 3.13950\n",
      "epoch no.4 train no.356710  loss = 3.33689 avg_loss = 3.09982\n",
      "epoch no.4 train no.356720  loss = 4.09003 avg_loss = 3.11546\n",
      "epoch no.4 train no.356730  loss = 3.66422 avg_loss = 3.13348\n",
      "epoch no.4 train no.356740  loss = 2.84389 avg_loss = 3.15070\n",
      "epoch no.4 train no.356750  loss = 3.25294 avg_loss = 3.16994\n",
      "epoch no.4 train no.356760  loss = 2.71720 avg_loss = 3.18447\n",
      "epoch no.4 train no.356770  loss = 3.14203 avg_loss = 3.18869\n",
      "epoch no.4 train no.356780  loss = 3.68061 avg_loss = 3.20462\n",
      "epoch no.4 train no.356790  loss = 2.61914 avg_loss = 3.20619\n",
      "epoch no.4 train no.356800  loss = 3.09438 avg_loss = 3.21931\n",
      "epoch no.4 train no.356810  loss = 4.08890 avg_loss = 3.22880\n",
      "epoch no.4 train no.356820  loss = 5.13269 avg_loss = 3.25189\n",
      "epoch no.4 train no.356830  loss = 2.91989 avg_loss = 3.24769\n",
      "epoch no.4 train no.356840  loss = 3.89145 avg_loss = 3.24429\n",
      "epoch no.4 train no.356850  loss = 5.37408 avg_loss = 3.30034\n",
      "epoch no.4 train no.356860  loss = 2.91345 avg_loss = 3.23818\n",
      "epoch no.4 train no.356870  loss = 2.17645 avg_loss = 3.24113\n",
      "epoch no.4 train no.356880  loss = 3.75994 avg_loss = 3.25377\n",
      "epoch no.4 train no.356890  loss = 2.48101 avg_loss = 3.26417\n",
      "epoch no.4 train no.356900  loss = 4.15404 avg_loss = 3.29260\n",
      "epoch no.4 train no.356910  loss = 3.23378 avg_loss = 3.28592\n",
      "epoch no.4 train no.356920  loss = 3.49385 avg_loss = 3.26933\n",
      "epoch no.4 train no.356930  loss = 2.98997 avg_loss = 3.30362\n",
      "epoch no.4 train no.356940  loss = 2.98364 avg_loss = 3.33715\n",
      "epoch no.4 train no.356950  loss = 2.69183 avg_loss = 3.38577\n",
      "epoch no.4 train no.356960  loss = 4.04278 avg_loss = 3.39408\n",
      "epoch no.4 train no.356970  loss = 7.21731 avg_loss = 3.42157\n",
      "epoch no.4 train no.356980  loss = 3.35597 avg_loss = 3.43115\n",
      "epoch no.4 train no.356990  loss = 5.32280 avg_loss = 3.43738\n",
      "epoch no.4 train no.357000  loss = 2.11048 avg_loss = 3.43069\n",
      "4\n",
      "to_tokens: ['▁가을', '▁2000', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.357010  loss = 3.31402 avg_loss = 3.41441\n",
      "epoch no.4 train no.357020  loss = 4.41567 avg_loss = 3.42064\n",
      "epoch no.4 train no.357030  loss = 3.12150 avg_loss = 3.36648\n",
      "epoch no.4 train no.357040  loss = 2.15736 avg_loss = 3.39525\n",
      "epoch no.4 train no.357050  loss = 3.29609 avg_loss = 3.39471\n",
      "epoch no.4 train no.357060  loss = 3.96450 avg_loss = 3.38168\n",
      "epoch no.4 train no.357070  loss = 2.93602 avg_loss = 3.39847\n",
      "epoch no.4 train no.357080  loss = 2.81662 avg_loss = 3.34987\n",
      "epoch no.4 train no.357090  loss = 2.80615 avg_loss = 3.31696\n",
      "epoch no.4 train no.357100  loss = 4.12173 avg_loss = 3.30861\n",
      "epoch no.4 train no.357110  loss = 3.39469 avg_loss = 3.27783\n",
      "epoch no.4 train no.357120  loss = 5.69912 avg_loss = 3.33693\n",
      "epoch no.4 train no.357130  loss = 2.86460 avg_loss = 3.31294\n",
      "epoch no.4 train no.357140  loss = 2.80062 avg_loss = 3.30372\n",
      "epoch no.4 train no.357150  loss = 2.93375 avg_loss = 3.26416\n",
      "epoch no.4 train no.357160  loss = 2.56884 avg_loss = 3.23517\n",
      "epoch no.4 train no.357170  loss = 3.69537 avg_loss = 3.25985\n",
      "epoch no.4 train no.357180  loss = 2.25037 avg_loss = 3.24409\n",
      "epoch no.4 train no.357190  loss = 2.43624 avg_loss = 3.27131\n",
      "epoch no.4 train no.357200  loss = 4.74550 avg_loss = 3.26479\n",
      "epoch no.4 train no.357210  loss = 3.47796 avg_loss = 3.22961\n",
      "epoch no.4 train no.357220  loss = 2.70008 avg_loss = 3.18449\n",
      "epoch no.4 train no.357230  loss = 4.52601 avg_loss = 3.20906\n",
      "epoch no.4 train no.357240  loss = 2.97703 avg_loss = 3.17685\n",
      "epoch no.4 train no.357250  loss = 3.35037 avg_loss = 3.18908\n",
      "epoch no.4 train no.357260  loss = 3.03953 avg_loss = 3.18823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.357270  loss = 3.64697 avg_loss = 3.20186\n",
      "epoch no.4 train no.357280  loss = 4.56632 avg_loss = 3.18703\n",
      "epoch no.4 train no.357290  loss = 3.82816 avg_loss = 3.20895\n",
      "epoch no.4 train no.357300  loss = 2.71850 avg_loss = 3.26597\n",
      "epoch no.4 train no.357310  loss = 4.24567 avg_loss = 3.31272\n",
      "epoch no.4 train no.357320  loss = 4.27429 avg_loss = 3.29044\n",
      "epoch no.4 train no.357330  loss = 3.28410 avg_loss = 3.29422\n",
      "epoch no.4 train no.357340  loss = 3.28169 avg_loss = 3.27137\n",
      "epoch no.4 train no.357350  loss = 3.74557 avg_loss = 3.25457\n",
      "epoch no.4 train no.357360  loss = 2.36534 avg_loss = 3.23434\n",
      "epoch no.4 train no.357370  loss = 2.34265 avg_loss = 3.23959\n",
      "epoch no.4 train no.357380  loss = 3.45837 avg_loss = 3.24856\n",
      "epoch no.4 train no.357390  loss = 3.28963 avg_loss = 3.22384\n",
      "epoch no.4 train no.357400  loss = 2.31554 avg_loss = 3.19363\n",
      "epoch no.4 train no.357410  loss = 2.64073 avg_loss = 3.22620\n",
      "epoch no.4 train no.357420  loss = 4.25197 avg_loss = 3.23188\n",
      "epoch no.4 train no.357430  loss = 2.69054 avg_loss = 3.23113\n",
      "epoch no.4 train no.357440  loss = 2.37339 avg_loss = 3.25651\n",
      "epoch no.4 train no.357450  loss = 3.09203 avg_loss = 3.28100\n",
      "epoch no.4 train no.357460  loss = 3.29627 avg_loss = 3.26976\n",
      "epoch no.4 train no.357470  loss = 3.81544 avg_loss = 3.31196\n",
      "epoch no.4 train no.357480  loss = 2.91378 avg_loss = 3.32949\n",
      "epoch no.4 train no.357490  loss = 3.18996 avg_loss = 3.30707\n",
      "epoch no.4 train no.357500  loss = 3.30735 avg_loss = 3.34046\n",
      "epoch no.4 train no.357510  loss = 1.98158 avg_loss = 3.32071\n",
      "epoch no.4 train no.357520  loss = 1.65224 avg_loss = 3.32591\n",
      "epoch no.4 train no.357530  loss = 2.40462 avg_loss = 3.32006\n",
      "epoch no.4 train no.357540  loss = 2.37723 avg_loss = 3.33566\n",
      "epoch no.4 train no.357550  loss = 2.40553 avg_loss = 3.27582\n",
      "epoch no.4 train no.357560  loss = 3.57517 avg_loss = 3.26499\n",
      "epoch no.4 train no.357570  loss = 2.86771 avg_loss = 3.26370\n",
      "epoch no.4 train no.357580  loss = 2.94432 avg_loss = 3.26793\n",
      "epoch no.4 train no.357590  loss = 2.41042 avg_loss = 3.22685\n",
      "epoch no.4 train no.357600  loss = 2.48010 avg_loss = 3.18041\n",
      "epoch no.4 train no.357610  loss = 4.84751 avg_loss = 3.23402\n",
      "epoch no.4 train no.357620  loss = 3.79672 avg_loss = 3.21873\n",
      "epoch no.4 train no.357630  loss = 2.56560 avg_loss = 3.21045\n",
      "epoch no.4 train no.357640  loss = 3.18051 avg_loss = 3.21120\n",
      "epoch no.4 train no.357650  loss = 2.47942 avg_loss = 3.20941\n",
      "epoch no.4 train no.357660  loss = 2.25986 avg_loss = 3.19706\n",
      "epoch no.4 train no.357670  loss = 3.56109 avg_loss = 3.19997\n",
      "epoch no.4 train no.357680  loss = 2.50643 avg_loss = 3.13133\n",
      "epoch no.4 train no.357690  loss = 4.40696 avg_loss = 3.15392\n",
      "epoch no.4 train no.357700  loss = 2.34017 avg_loss = 3.17524\n",
      "epoch no.4 train no.357710  loss = 3.83899 avg_loss = 3.20297\n",
      "epoch no.4 train no.357720  loss = 4.39042 avg_loss = 3.22044\n",
      "epoch no.4 train no.357730  loss = 3.06358 avg_loss = 3.20303\n",
      "epoch no.4 train no.357740  loss = 2.64584 avg_loss = 3.19431\n",
      "epoch no.4 train no.357750  loss = 3.02744 avg_loss = 3.18436\n",
      "epoch no.4 train no.357760  loss = 2.20337 avg_loss = 3.15281\n",
      "epoch no.4 train no.357770  loss = 2.98341 avg_loss = 3.13140\n",
      "epoch no.4 train no.357780  loss = 2.85244 avg_loss = 3.16752\n",
      "epoch no.4 train no.357790  loss = 3.99926 avg_loss = 3.19410\n",
      "epoch no.4 train no.357800  loss = 3.55176 avg_loss = 3.18584\n",
      "epoch no.4 train no.357810  loss = 2.46641 avg_loss = 3.17788\n",
      "epoch no.4 train no.357820  loss = 3.11555 avg_loss = 3.19500\n",
      "epoch no.4 train no.357830  loss = 6.39948 avg_loss = 3.24126\n",
      "epoch no.4 train no.357840  loss = 1.83021 avg_loss = 3.23605\n",
      "epoch no.4 train no.357850  loss = 3.09473 avg_loss = 3.25272\n",
      "epoch no.4 train no.357860  loss = 3.31863 avg_loss = 3.24595\n",
      "epoch no.4 train no.357870  loss = 2.15277 avg_loss = 3.28384\n",
      "epoch no.4 train no.357880  loss = 4.72391 avg_loss = 3.33276\n",
      "epoch no.4 train no.357890  loss = 2.78397 avg_loss = 3.31471\n",
      "epoch no.4 train no.357900  loss = 3.28617 avg_loss = 3.28298\n",
      "epoch no.4 train no.357910  loss = 4.33526 avg_loss = 3.35376\n",
      "epoch no.4 train no.357920  loss = 3.15781 avg_loss = 3.36375\n",
      "epoch no.4 train no.357930  loss = 3.13974 avg_loss = 3.31915\n",
      "epoch no.4 train no.357940  loss = 5.09840 avg_loss = 3.33091\n",
      "epoch no.4 train no.357950  loss = 3.33676 avg_loss = 3.32985\n",
      "epoch no.4 train no.357960  loss = 2.86548 avg_loss = 3.29662\n",
      "epoch no.4 train no.357970  loss = 2.10325 avg_loss = 3.26696\n",
      "epoch no.4 train no.357980  loss = 2.01289 avg_loss = 3.26048\n",
      "epoch no.4 train no.357990  loss = 4.83384 avg_loss = 3.26970\n",
      "epoch no.4 train no.358000  loss = 4.62133 avg_loss = 3.26859\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '드', '</s>', '</s>']\n",
      "추억의 2000년대 발라드 베스트</s>\n",
      "epoch no.4 train no.358010  loss = 4.18916 avg_loss = 3.27628\n",
      "epoch no.4 train no.358020  loss = 2.96604 avg_loss = 3.26021\n",
      "epoch no.4 train no.358030  loss = 4.03759 avg_loss = 3.30765\n",
      "epoch no.4 train no.358040  loss = 3.82903 avg_loss = 3.26228\n",
      "epoch no.4 train no.358050  loss = 3.60824 avg_loss = 3.28745\n",
      "epoch no.4 train no.358060  loss = 3.88729 avg_loss = 3.25431\n",
      "epoch no.4 train no.358070  loss = 3.80954 avg_loss = 3.25555\n",
      "epoch no.4 train no.358080  loss = 3.95560 avg_loss = 3.23545\n",
      "epoch no.4 train no.358090  loss = 3.47786 avg_loss = 3.24794\n",
      "epoch no.4 train no.358100  loss = 3.25977 avg_loss = 3.25395\n",
      "epoch no.4 train no.358110  loss = 2.88260 avg_loss = 3.23266\n",
      "epoch no.4 train no.358120  loss = 5.60720 avg_loss = 3.26226\n",
      "epoch no.4 train no.358130  loss = 4.96120 avg_loss = 3.27878\n",
      "epoch no.4 train no.358140  loss = 2.31069 avg_loss = 3.27363\n",
      "epoch no.4 train no.358150  loss = 3.22780 avg_loss = 3.26777\n",
      "epoch no.4 train no.358160  loss = 3.36550 avg_loss = 3.29001\n",
      "epoch no.4 train no.358170  loss = 4.25902 avg_loss = 3.29655\n",
      "epoch no.4 train no.358180  loss = 4.07652 avg_loss = 3.29507\n",
      "epoch no.4 train no.358190  loss = 4.77924 avg_loss = 3.28045\n",
      "epoch no.4 train no.358200  loss = 2.63131 avg_loss = 3.25538\n",
      "epoch no.4 train no.358210  loss = 2.75235 avg_loss = 3.24939\n",
      "epoch no.4 train no.358220  loss = 3.31353 avg_loss = 3.21872\n",
      "epoch no.4 train no.358230  loss = 3.36177 avg_loss = 3.21820\n",
      "epoch no.4 train no.358240  loss = 2.57414 avg_loss = 3.24048\n",
      "epoch no.4 train no.358250  loss = 3.65494 avg_loss = 3.27785\n",
      "epoch no.4 train no.358260  loss = 2.52893 avg_loss = 3.26066\n",
      "epoch no.4 train no.358270  loss = 3.34449 avg_loss = 3.24978\n",
      "epoch no.4 train no.358280  loss = 2.92326 avg_loss = 3.22079\n",
      "epoch no.4 train no.358290  loss = 3.63319 avg_loss = 3.20180\n",
      "epoch no.4 train no.358300  loss = 3.21428 avg_loss = 3.20210\n",
      "epoch no.4 train no.358310  loss = 3.23293 avg_loss = 3.22216\n",
      "epoch no.4 train no.358320  loss = 3.08944 avg_loss = 3.23370\n",
      "epoch no.4 train no.358330  loss = 3.31642 avg_loss = 3.18376\n",
      "epoch no.4 train no.358340  loss = 3.41343 avg_loss = 3.19059\n",
      "epoch no.4 train no.358350  loss = 3.66485 avg_loss = 3.22374\n",
      "epoch no.4 train no.358360  loss = 3.56507 avg_loss = 3.22905\n",
      "epoch no.4 train no.358370  loss = 1.72005 avg_loss = 3.18465\n",
      "epoch no.4 train no.358380  loss = 3.27370 avg_loss = 3.19304\n",
      "epoch no.4 train no.358390  loss = 4.17202 avg_loss = 3.21373\n",
      "epoch no.4 train no.358400  loss = 2.46185 avg_loss = 3.23148\n",
      "epoch no.4 train no.358410  loss = 3.21851 avg_loss = 3.25789\n",
      "epoch no.4 train no.358420  loss = 2.68123 avg_loss = 3.25395\n",
      "epoch no.4 train no.358430  loss = 4.61573 avg_loss = 3.27743\n",
      "epoch no.4 train no.358440  loss = 1.83020 avg_loss = 3.31303\n",
      "epoch no.4 train no.358450  loss = 3.40049 avg_loss = 3.28776\n",
      "epoch no.4 train no.358460  loss = 3.99435 avg_loss = 3.30037\n",
      "epoch no.4 train no.358470  loss = 2.24777 avg_loss = 3.29880\n",
      "epoch no.4 train no.358480  loss = 2.98371 avg_loss = 3.26469\n",
      "epoch no.4 train no.358490  loss = 3.60625 avg_loss = 3.24521\n",
      "epoch no.4 train no.358500  loss = 3.52938 avg_loss = 3.21331\n",
      "epoch no.4 train no.358510  loss = 3.41194 avg_loss = 3.22501\n",
      "epoch no.4 train no.358520  loss = 4.19613 avg_loss = 3.21853\n",
      "epoch no.4 train no.358530  loss = 2.27674 avg_loss = 3.18590\n",
      "epoch no.4 train no.358540  loss = 4.45330 avg_loss = 3.23748\n",
      "epoch no.4 train no.358550  loss = 1.98092 avg_loss = 3.20954\n",
      "epoch no.4 train no.358560  loss = 3.22897 avg_loss = 3.18717\n",
      "epoch no.4 train no.358570  loss = 2.98654 avg_loss = 3.23843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.358580  loss = 3.89032 avg_loss = 3.24302\n",
      "epoch no.4 train no.358590  loss = 4.25770 avg_loss = 3.26231\n",
      "epoch no.4 train no.358600  loss = 3.96793 avg_loss = 3.26820\n",
      "epoch no.4 train no.358610  loss = 2.78616 avg_loss = 3.28338\n",
      "epoch no.4 train no.358620  loss = 5.02816 avg_loss = 3.27457\n",
      "epoch no.4 train no.358630  loss = 3.61667 avg_loss = 3.29732\n",
      "epoch no.4 train no.358640  loss = 1.98041 avg_loss = 3.25731\n",
      "epoch no.4 train no.358650  loss = 3.59023 avg_loss = 3.29333\n",
      "epoch no.4 train no.358660  loss = 5.79276 avg_loss = 3.33643\n",
      "epoch no.4 train no.358670  loss = 2.80205 avg_loss = 3.31584\n",
      "epoch no.4 train no.358680  loss = 2.51674 avg_loss = 3.27870\n",
      "epoch no.4 train no.358690  loss = 3.15506 avg_loss = 3.28272\n",
      "epoch no.4 train no.358700  loss = 4.77355 avg_loss = 3.24362\n",
      "epoch no.4 train no.358710  loss = 2.08684 avg_loss = 3.22818\n",
      "epoch no.4 train no.358720  loss = 3.64943 avg_loss = 3.25424\n",
      "epoch no.4 train no.358730  loss = 2.17638 avg_loss = 3.26528\n",
      "epoch no.4 train no.358740  loss = 3.30591 avg_loss = 3.22433\n",
      "epoch no.4 train no.358750  loss = 3.11129 avg_loss = 3.27457\n",
      "epoch no.4 train no.358760  loss = 2.59743 avg_loss = 3.23400\n",
      "epoch no.4 train no.358770  loss = 2.46693 avg_loss = 3.25196\n",
      "epoch no.4 train no.358780  loss = 2.95164 avg_loss = 3.22580\n",
      "epoch no.4 train no.358790  loss = 3.91270 avg_loss = 3.17203\n",
      "epoch no.4 train no.358800  loss = 3.18839 avg_loss = 3.20803\n",
      "epoch no.4 train no.358810  loss = 4.90029 avg_loss = 3.22308\n",
      "epoch no.4 train no.358820  loss = 2.32278 avg_loss = 3.21089\n",
      "epoch no.4 train no.358830  loss = 4.57780 avg_loss = 3.21191\n",
      "epoch no.4 train no.358840  loss = 3.79147 avg_loss = 3.23103\n",
      "epoch no.4 train no.358850  loss = 1.60606 avg_loss = 3.19252\n",
      "epoch no.4 train no.358860  loss = 6.48322 avg_loss = 3.24749\n",
      "epoch no.4 train no.358870  loss = 3.70357 avg_loss = 3.25968\n",
      "epoch no.4 train no.358880  loss = 2.66189 avg_loss = 3.23327\n",
      "epoch no.4 train no.358890  loss = 4.41174 avg_loss = 3.23148\n",
      "epoch no.4 train no.358900  loss = 3.88171 avg_loss = 3.22050\n",
      "epoch no.4 train no.358910  loss = 3.90996 avg_loss = 3.24476\n",
      "epoch no.4 train no.358920  loss = 4.42810 avg_loss = 3.28606\n",
      "epoch no.4 train no.358930  loss = 3.35076 avg_loss = 3.25702\n",
      "epoch no.4 train no.358940  loss = 2.62959 avg_loss = 3.20292\n",
      "epoch no.4 train no.358950  loss = 2.67890 avg_loss = 3.21707\n",
      "epoch no.4 train no.358960  loss = 2.56237 avg_loss = 3.19923\n",
      "epoch no.4 train no.358970  loss = 2.35513 avg_loss = 3.16988\n",
      "epoch no.4 train no.358980  loss = 2.30887 avg_loss = 3.19922\n",
      "epoch no.4 train no.358990  loss = 3.39044 avg_loss = 3.19281\n",
      "epoch no.4 train no.359000  loss = 3.91512 avg_loss = 3.17953\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '들', '</s>']\n",
      "추억의 2000년대 노래들</s>\n",
      "epoch no.4 train no.359010  loss = 3.49410 avg_loss = 3.22169\n",
      "epoch no.4 train no.359020  loss = 4.78058 avg_loss = 3.22289\n",
      "epoch no.4 train no.359030  loss = 3.84144 avg_loss = 3.24680\n",
      "epoch no.4 train no.359040  loss = 4.09419 avg_loss = 3.24945\n",
      "epoch no.4 train no.359050  loss = 4.09978 avg_loss = 3.24078\n",
      "epoch no.4 train no.359060  loss = 3.32102 avg_loss = 3.23654\n",
      "epoch no.4 train no.359070  loss = 3.26711 avg_loss = 3.30026\n",
      "epoch no.4 train no.359080  loss = 3.06681 avg_loss = 3.30474\n",
      "epoch no.4 train no.359090  loss = 2.75324 avg_loss = 3.30457\n",
      "epoch no.4 train no.359100  loss = 5.84209 avg_loss = 3.29540\n",
      "epoch no.4 train no.359110  loss = 3.96972 avg_loss = 3.29579\n",
      "epoch no.4 train no.359120  loss = 4.50368 avg_loss = 3.30241\n",
      "epoch no.4 train no.359130  loss = 5.29896 avg_loss = 3.28686\n",
      "epoch no.4 train no.359140  loss = 3.77396 avg_loss = 3.28301\n",
      "epoch no.4 train no.359150  loss = 2.91576 avg_loss = 3.25918\n",
      "epoch no.4 train no.359160  loss = 4.24058 avg_loss = 3.23465\n",
      "epoch no.4 train no.359170  loss = 1.94126 avg_loss = 3.23442\n",
      "epoch no.4 train no.359180  loss = 2.87181 avg_loss = 3.26921\n",
      "epoch no.4 train no.359190  loss = 2.46790 avg_loss = 3.28721\n",
      "epoch no.4 train no.359200  loss = 2.90398 avg_loss = 3.24573\n",
      "epoch no.4 train no.359210  loss = 4.12210 avg_loss = 3.31361\n",
      "epoch no.4 train no.359220  loss = 2.91625 avg_loss = 3.29935\n",
      "epoch no.4 train no.359230  loss = 3.01384 avg_loss = 3.28337\n",
      "epoch no.4 train no.359240  loss = 2.55362 avg_loss = 3.31151\n",
      "epoch no.4 train no.359250  loss = 4.00169 avg_loss = 3.35984\n",
      "epoch no.4 train no.359260  loss = 2.06832 avg_loss = 3.30105\n",
      "epoch no.4 train no.359270  loss = 2.53639 avg_loss = 3.26471\n",
      "epoch no.4 train no.359280  loss = 2.12117 avg_loss = 3.28187\n",
      "epoch no.4 train no.359290  loss = 3.38812 avg_loss = 3.23737\n",
      "epoch no.4 train no.359300  loss = 2.83156 avg_loss = 3.27562\n",
      "epoch no.4 train no.359310  loss = 4.81189 avg_loss = 3.28492\n",
      "epoch no.4 train no.359320  loss = 3.36832 avg_loss = 3.27355\n",
      "epoch no.4 train no.359330  loss = 6.07043 avg_loss = 3.26724\n",
      "epoch no.4 train no.359340  loss = 2.97099 avg_loss = 3.26349\n",
      "epoch no.4 train no.359350  loss = 2.39927 avg_loss = 3.22491\n",
      "epoch no.4 train no.359360  loss = 3.96099 avg_loss = 3.21978\n",
      "epoch no.4 train no.359370  loss = 3.00523 avg_loss = 3.21454\n",
      "epoch no.4 train no.359380  loss = 4.09066 avg_loss = 3.24062\n",
      "epoch no.4 train no.359390  loss = 3.43864 avg_loss = 3.26556\n",
      "epoch no.4 train no.359400  loss = 1.75838 avg_loss = 3.26164\n",
      "epoch no.4 train no.359410  loss = 4.19731 avg_loss = 3.24281\n",
      "epoch no.4 train no.359420  loss = 3.03290 avg_loss = 3.23533\n",
      "epoch no.4 train no.359430  loss = 4.50129 avg_loss = 3.24262\n",
      "epoch no.4 train no.359440  loss = 3.78837 avg_loss = 3.25358\n",
      "epoch no.4 train no.359450  loss = 2.70554 avg_loss = 3.22892\n",
      "epoch no.4 train no.359460  loss = 3.41328 avg_loss = 3.24935\n",
      "epoch no.4 train no.359470  loss = 2.41646 avg_loss = 3.23228\n",
      "epoch no.4 train no.359480  loss = 2.16051 avg_loss = 3.24124\n",
      "epoch no.4 train no.359490  loss = 3.52158 avg_loss = 3.26761\n",
      "epoch no.4 train no.359500  loss = 4.54289 avg_loss = 3.29941\n",
      "epoch no.4 train no.359510  loss = 5.11759 avg_loss = 3.29544\n",
      "epoch no.4 train no.359520  loss = 2.99651 avg_loss = 3.32706\n",
      "epoch no.4 train no.359530  loss = 3.26053 avg_loss = 3.35148\n",
      "epoch no.4 train no.359540  loss = 3.36420 avg_loss = 3.36556\n",
      "epoch no.4 train no.359550  loss = 2.08053 avg_loss = 3.35803\n",
      "epoch no.4 train no.359560  loss = 2.88333 avg_loss = 3.32473\n",
      "epoch no.4 train no.359570  loss = 3.35385 avg_loss = 3.36586\n",
      "epoch no.4 train no.359580  loss = 3.34274 avg_loss = 3.35428\n",
      "epoch no.4 train no.359590  loss = 3.86600 avg_loss = 3.39507\n",
      "epoch no.4 train no.359600  loss = 2.20977 avg_loss = 3.32571\n",
      "epoch no.4 train no.359610  loss = 3.80677 avg_loss = 3.32010\n",
      "epoch no.4 train no.359620  loss = 2.15161 avg_loss = 3.27220\n",
      "epoch no.4 train no.359630  loss = 2.79482 avg_loss = 3.28304\n",
      "epoch no.4 train no.359640  loss = 2.68943 avg_loss = 3.28677\n",
      "epoch no.4 train no.359650  loss = 3.78585 avg_loss = 3.30236\n",
      "epoch no.4 train no.359660  loss = 2.37424 avg_loss = 3.32129\n",
      "epoch no.4 train no.359670  loss = 2.21343 avg_loss = 3.28016\n",
      "epoch no.4 train no.359680  loss = 1.98652 avg_loss = 3.26905\n",
      "epoch no.4 train no.359690  loss = 2.88555 avg_loss = 3.29967\n",
      "epoch no.4 train no.359700  loss = 2.49721 avg_loss = 3.24876\n",
      "epoch no.4 train no.359710  loss = 1.78841 avg_loss = 3.20674\n",
      "epoch no.4 train no.359720  loss = 4.20287 avg_loss = 3.24377\n",
      "epoch no.4 train no.359730  loss = 3.22100 avg_loss = 3.28141\n",
      "epoch no.4 train no.359740  loss = 4.80919 avg_loss = 3.29801\n",
      "epoch no.4 train no.359750  loss = 3.05899 avg_loss = 3.25955\n",
      "epoch no.4 train no.359760  loss = 5.54323 avg_loss = 3.30066\n",
      "epoch no.4 train no.359770  loss = 3.38218 avg_loss = 3.31934\n",
      "epoch no.4 train no.359780  loss = 3.30064 avg_loss = 3.31864\n",
      "epoch no.4 train no.359790  loss = 3.60764 avg_loss = 3.38375\n",
      "epoch no.4 train no.359800  loss = 3.25563 avg_loss = 3.42242\n",
      "epoch no.4 train no.359810  loss = 3.47695 avg_loss = 3.41067\n",
      "epoch no.4 train no.359820  loss = 3.05254 avg_loss = 3.40088\n",
      "epoch no.4 train no.359830  loss = 3.02732 avg_loss = 3.39126\n",
      "epoch no.4 train no.359840  loss = 2.33769 avg_loss = 3.37147\n",
      "epoch no.4 train no.359850  loss = 2.42512 avg_loss = 3.39860\n",
      "epoch no.4 train no.359860  loss = 2.15384 avg_loss = 3.37536\n",
      "epoch no.4 train no.359870  loss = 2.13387 avg_loss = 3.37589\n",
      "epoch no.4 train no.359880  loss = 2.89126 avg_loss = 3.34208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.359890  loss = 3.78073 avg_loss = 3.33364\n",
      "epoch no.4 train no.359900  loss = 2.53542 avg_loss = 3.31405\n",
      "epoch no.4 train no.359910  loss = 4.11169 avg_loss = 3.33015\n",
      "epoch no.4 train no.359920  loss = 2.42346 avg_loss = 3.33405\n",
      "epoch no.4 train no.359930  loss = 2.97882 avg_loss = 3.33186\n",
      "epoch no.4 train no.359940  loss = 3.65518 avg_loss = 3.32198\n",
      "epoch no.4 train no.359950  loss = 1.75075 avg_loss = 3.27373\n",
      "epoch no.4 train no.359960  loss = 3.45291 avg_loss = 3.28364\n",
      "epoch no.4 train no.359970  loss = 3.48758 avg_loss = 3.26903\n",
      "epoch no.4 train no.359980  loss = 1.53114 avg_loss = 3.26710\n",
      "epoch no.4 train no.359990  loss = 3.57672 avg_loss = 3.27641\n",
      "epoch no.4 train no.360000  loss = 3.13715 avg_loss = 3.24885\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '▁o', 'st', '모', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.4 train no.360010  loss = 1.51157 avg_loss = 3.22602\n",
      "epoch no.4 train no.360020  loss = 3.54133 avg_loss = 3.25019\n",
      "epoch no.4 train no.360030  loss = 3.20165 avg_loss = 3.27964\n",
      "epoch no.4 train no.360040  loss = 4.80910 avg_loss = 3.32732\n",
      "epoch no.4 train no.360050  loss = 2.81741 avg_loss = 3.29131\n",
      "epoch no.4 train no.360060  loss = 1.98182 avg_loss = 3.27437\n",
      "epoch no.4 train no.360070  loss = 3.18476 avg_loss = 3.28712\n",
      "epoch no.4 train no.360080  loss = 2.75407 avg_loss = 3.32625\n",
      "epoch no.4 train no.360090  loss = 3.41640 avg_loss = 3.31711\n",
      "epoch no.4 train no.360100  loss = 5.61709 avg_loss = 3.38629\n",
      "epoch no.4 train no.360110  loss = 3.01534 avg_loss = 3.35236\n",
      "epoch no.4 train no.360120  loss = 4.64075 avg_loss = 3.40519\n",
      "epoch no.4 train no.360130  loss = 4.32293 avg_loss = 3.42236\n",
      "epoch no.4 train no.360140  loss = 2.27725 avg_loss = 3.34298\n",
      "epoch no.4 train no.360150  loss = 3.02758 avg_loss = 3.35807\n",
      "epoch no.4 train no.360160  loss = 3.12707 avg_loss = 3.31316\n",
      "epoch no.4 train no.360170  loss = 2.24143 avg_loss = 3.26902\n",
      "epoch no.4 train no.360180  loss = 6.67017 avg_loss = 3.26001\n",
      "epoch no.4 train no.360190  loss = 3.74853 avg_loss = 3.23945\n",
      "epoch no.4 train no.360200  loss = 3.46212 avg_loss = 3.27982\n",
      "epoch no.4 train no.360210  loss = 2.88953 avg_loss = 3.28658\n",
      "epoch no.4 train no.360220  loss = 2.88098 avg_loss = 3.28923\n",
      "epoch no.4 train no.360230  loss = 3.24224 avg_loss = 3.27746\n",
      "epoch no.4 train no.360240  loss = 2.67945 avg_loss = 3.25138\n",
      "epoch no.4 train no.360250  loss = 3.59001 avg_loss = 3.21734\n",
      "epoch no.4 train no.360260  loss = 2.63020 avg_loss = 3.20349\n",
      "epoch no.4 train no.360270  loss = 3.32646 avg_loss = 3.20807\n",
      "epoch no.4 train no.360280  loss = 3.57570 avg_loss = 3.18093\n",
      "epoch no.4 train no.360290  loss = 2.78847 avg_loss = 3.21260\n",
      "epoch no.4 train no.360300  loss = 1.86936 avg_loss = 3.19703\n",
      "epoch no.4 train no.360310  loss = 3.77683 avg_loss = 3.18722\n",
      "epoch no.4 train no.360320  loss = 1.78369 avg_loss = 3.16268\n",
      "epoch no.4 train no.360330  loss = 2.37973 avg_loss = 3.14302\n",
      "epoch no.4 train no.360340  loss = 3.81960 avg_loss = 3.14456\n",
      "epoch no.4 train no.360350  loss = 3.46833 avg_loss = 3.15399\n",
      "epoch no.4 train no.360360  loss = 2.44473 avg_loss = 3.11888\n",
      "epoch no.4 train no.360370  loss = 4.43554 avg_loss = 3.19037\n",
      "epoch no.4 train no.360380  loss = 3.49004 avg_loss = 3.18268\n",
      "epoch no.4 train no.360390  loss = 4.56680 avg_loss = 3.19090\n",
      "epoch no.4 train no.360400  loss = 2.51209 avg_loss = 3.17291\n",
      "epoch no.4 train no.360410  loss = 4.40641 avg_loss = 3.20108\n",
      "epoch no.4 train no.360420  loss = 2.00189 avg_loss = 3.19778\n",
      "epoch no.4 train no.360430  loss = 2.60467 avg_loss = 3.22407\n",
      "epoch no.4 train no.360440  loss = 2.43699 avg_loss = 3.19503\n",
      "epoch no.4 train no.360450  loss = 3.35495 avg_loss = 3.16556\n",
      "epoch no.4 train no.360460  loss = 2.94646 avg_loss = 3.16367\n",
      "epoch no.4 train no.360470  loss = 3.97397 avg_loss = 3.17682\n",
      "epoch no.4 train no.360480  loss = 3.93547 avg_loss = 3.24034\n",
      "epoch no.4 train no.360490  loss = 3.43730 avg_loss = 3.24167\n",
      "epoch no.4 train no.360500  loss = 2.57176 avg_loss = 3.21416\n",
      "epoch no.4 train no.360510  loss = 2.99199 avg_loss = 3.21479\n",
      "epoch no.4 train no.360520  loss = 2.29250 avg_loss = 3.22030\n",
      "epoch no.4 train no.360530  loss = 5.27993 avg_loss = 3.25282\n",
      "epoch no.4 train no.360540  loss = 2.18531 avg_loss = 3.25340\n",
      "epoch no.4 train no.360550  loss = 4.27576 avg_loss = 3.26353\n",
      "epoch no.4 train no.360560  loss = 2.63240 avg_loss = 3.24666\n",
      "epoch no.4 train no.360570  loss = 2.77781 avg_loss = 3.25146\n",
      "epoch no.4 train no.360580  loss = 3.47515 avg_loss = 3.30501\n",
      "epoch no.4 train no.360590  loss = 3.06661 avg_loss = 3.27008\n",
      "epoch no.4 train no.360600  loss = 3.28407 avg_loss = 3.27710\n",
      "epoch no.4 train no.360610  loss = 3.44964 avg_loss = 3.25558\n",
      "epoch no.4 train no.360620  loss = 2.88861 avg_loss = 3.21337\n",
      "epoch no.4 train no.360630  loss = 3.42058 avg_loss = 3.20692\n",
      "epoch no.4 train no.360640  loss = 3.39603 avg_loss = 3.21368\n",
      "epoch no.4 train no.360650  loss = 3.79921 avg_loss = 3.25703\n",
      "epoch no.4 train no.360660  loss = 4.33576 avg_loss = 3.27299\n",
      "epoch no.4 train no.360670  loss = 3.29412 avg_loss = 3.27842\n",
      "epoch no.4 train no.360680  loss = 2.88936 avg_loss = 3.33305\n",
      "epoch no.4 train no.360690  loss = 3.02680 avg_loss = 3.30670\n",
      "epoch no.4 train no.360700  loss = 2.11057 avg_loss = 3.30692\n",
      "epoch no.4 train no.360710  loss = 2.87306 avg_loss = 3.30331\n",
      "epoch no.4 train no.360720  loss = 3.66796 avg_loss = 3.31509\n",
      "epoch no.4 train no.360730  loss = 3.34142 avg_loss = 3.34204\n",
      "epoch no.4 train no.360740  loss = 4.28105 avg_loss = 3.40089\n",
      "epoch no.4 train no.360750  loss = 4.81630 avg_loss = 3.40973\n",
      "epoch no.4 train no.360760  loss = 1.99416 avg_loss = 3.38600\n",
      "epoch no.4 train no.360770  loss = 3.27715 avg_loss = 3.34056\n",
      "epoch no.4 train no.360780  loss = 2.62208 avg_loss = 3.28854\n",
      "epoch no.4 train no.360790  loss = 4.00698 avg_loss = 3.26092\n",
      "epoch no.4 train no.360800  loss = 2.72730 avg_loss = 3.25799\n",
      "epoch no.4 train no.360810  loss = 2.89832 avg_loss = 3.28386\n",
      "epoch no.4 train no.360820  loss = 3.56618 avg_loss = 3.28850\n",
      "epoch no.4 train no.360830  loss = 2.79468 avg_loss = 3.27993\n",
      "epoch no.4 train no.360840  loss = 2.18632 avg_loss = 3.25328\n",
      "epoch no.4 train no.360850  loss = 2.74596 avg_loss = 3.22298\n",
      "epoch no.4 train no.360860  loss = 4.26780 avg_loss = 3.20296\n",
      "epoch no.4 train no.360870  loss = 4.52850 avg_loss = 3.20730\n",
      "epoch no.4 train no.360880  loss = 4.45516 avg_loss = 3.22906\n",
      "epoch no.4 train no.360890  loss = 4.85563 avg_loss = 3.22581\n",
      "epoch no.4 train no.360900  loss = 2.61768 avg_loss = 3.22384\n",
      "epoch no.4 train no.360910  loss = 3.64053 avg_loss = 3.24562\n",
      "epoch no.4 train no.360920  loss = 4.98247 avg_loss = 3.27886\n",
      "epoch no.4 train no.360930  loss = 3.89010 avg_loss = 3.28176\n",
      "epoch no.4 train no.360940  loss = 3.53591 avg_loss = 3.27916\n",
      "epoch no.4 train no.360950  loss = 3.33753 avg_loss = 3.22691\n",
      "epoch no.4 train no.360960  loss = 3.69299 avg_loss = 3.18861\n",
      "epoch no.4 train no.360970  loss = 2.72413 avg_loss = 3.18937\n",
      "epoch no.4 train no.360980  loss = 4.51947 avg_loss = 3.23568\n",
      "epoch no.4 train no.360990  loss = 2.69247 avg_loss = 3.29573\n",
      "epoch no.4 train no.361000  loss = 3.12466 avg_loss = 3.30828\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '곡', '들', '라드', '</s>']\n",
      "추억의 명곡발라드</s>\n",
      "epoch no.4 train no.361010  loss = 2.07424 avg_loss = 3.35105\n",
      "epoch no.4 train no.361020  loss = 2.70336 avg_loss = 3.29568\n",
      "epoch no.4 train no.361030  loss = 3.88805 avg_loss = 3.30794\n",
      "epoch no.4 train no.361040  loss = 3.36743 avg_loss = 3.26020\n",
      "epoch no.4 train no.361050  loss = 2.42092 avg_loss = 3.21194\n",
      "epoch no.4 train no.361060  loss = 4.82105 avg_loss = 3.19425\n",
      "epoch no.4 train no.361070  loss = 3.13047 avg_loss = 3.18995\n",
      "epoch no.4 train no.361080  loss = 1.82676 avg_loss = 3.14316\n",
      "epoch no.4 train no.361090  loss = 2.24474 avg_loss = 3.12764\n",
      "epoch no.4 train no.361100  loss = 2.69610 avg_loss = 3.16589\n",
      "epoch no.4 train no.361110  loss = 2.72310 avg_loss = 3.18529\n",
      "epoch no.4 train no.361120  loss = 1.97216 avg_loss = 3.19502\n",
      "epoch no.4 train no.361130  loss = 5.18844 avg_loss = 3.22982\n",
      "epoch no.4 train no.361140  loss = 3.88387 avg_loss = 3.20391\n",
      "epoch no.4 train no.361150  loss = 3.28087 avg_loss = 3.19612\n",
      "epoch no.4 train no.361160  loss = 6.01798 avg_loss = 3.21612\n",
      "epoch no.4 train no.361170  loss = 4.07334 avg_loss = 3.22485\n",
      "epoch no.4 train no.361180  loss = 5.57353 avg_loss = 3.26240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.361190  loss = 3.24232 avg_loss = 3.28607\n",
      "epoch no.4 train no.361200  loss = 2.41616 avg_loss = 3.28567\n",
      "epoch no.4 train no.361210  loss = 3.04267 avg_loss = 3.29314\n",
      "epoch no.4 train no.361220  loss = 2.42419 avg_loss = 3.28377\n",
      "epoch no.4 train no.361230  loss = 2.79449 avg_loss = 3.28480\n",
      "epoch no.4 train no.361240  loss = 4.97457 avg_loss = 3.32018\n",
      "epoch no.4 train no.361250  loss = 1.80416 avg_loss = 3.27135\n",
      "epoch no.4 train no.361260  loss = 1.25096 avg_loss = 3.24938\n",
      "epoch no.4 train no.361270  loss = 3.29263 avg_loss = 3.22714\n",
      "epoch no.4 train no.361280  loss = 3.75993 avg_loss = 3.24143\n",
      "epoch no.4 train no.361290  loss = 4.75166 avg_loss = 3.24885\n",
      "epoch no.4 train no.361300  loss = 4.53193 avg_loss = 3.24772\n",
      "epoch no.4 train no.361310  loss = 3.36972 avg_loss = 3.26782\n",
      "epoch no.4 train no.361320  loss = 3.61483 avg_loss = 3.33023\n",
      "epoch no.4 train no.361330  loss = 2.49472 avg_loss = 3.31570\n",
      "epoch no.4 train no.361340  loss = 2.38382 avg_loss = 3.28677\n",
      "epoch no.4 train no.361350  loss = 2.19364 avg_loss = 3.27885\n",
      "epoch no.4 train no.361360  loss = 2.48817 avg_loss = 3.25014\n",
      "epoch no.4 train no.361370  loss = 4.88633 avg_loss = 3.24131\n",
      "epoch no.4 train no.361380  loss = 3.01531 avg_loss = 3.22390\n",
      "epoch no.4 train no.361390  loss = 3.09863 avg_loss = 3.24075\n",
      "epoch no.4 train no.361400  loss = 2.56187 avg_loss = 3.22871\n",
      "epoch no.4 train no.361410  loss = 3.57254 avg_loss = 3.19210\n",
      "epoch no.4 train no.361420  loss = 3.26432 avg_loss = 3.19670\n",
      "epoch no.4 train no.361430  loss = 3.02120 avg_loss = 3.19216\n",
      "epoch no.4 train no.361440  loss = 2.77465 avg_loss = 3.21640\n",
      "epoch no.4 train no.361450  loss = 3.66679 avg_loss = 3.21401\n",
      "epoch no.4 train no.361460  loss = 3.97805 avg_loss = 3.22425\n",
      "epoch no.4 train no.361470  loss = 3.46973 avg_loss = 3.26134\n",
      "epoch no.4 train no.361480  loss = 2.56092 avg_loss = 3.24397\n",
      "epoch no.4 train no.361490  loss = 2.92196 avg_loss = 3.24337\n",
      "epoch no.4 train no.361500  loss = 3.67517 avg_loss = 3.22130\n",
      "epoch no.4 train no.361510  loss = 3.02558 avg_loss = 3.20863\n",
      "epoch no.4 train no.361520  loss = 3.53710 avg_loss = 3.21620\n",
      "epoch no.4 train no.361530  loss = 2.18628 avg_loss = 3.18200\n",
      "epoch no.4 train no.361540  loss = 4.72481 avg_loss = 3.17931\n",
      "epoch no.4 train no.361550  loss = 6.28911 avg_loss = 3.24237\n",
      "epoch no.4 train no.361560  loss = 3.53562 avg_loss = 3.24735\n",
      "epoch no.4 train no.361570  loss = 4.09806 avg_loss = 3.24073\n",
      "epoch no.4 train no.361580  loss = 2.67694 avg_loss = 3.26588\n",
      "epoch no.4 train no.361590  loss = 3.85891 avg_loss = 3.22917\n",
      "epoch no.4 train no.361600  loss = 2.08867 avg_loss = 3.22724\n",
      "epoch no.4 train no.361610  loss = 3.12733 avg_loss = 3.22580\n",
      "epoch no.4 train no.361620  loss = 3.43274 avg_loss = 3.27253\n",
      "epoch no.4 train no.361630  loss = 3.09963 avg_loss = 3.27208\n",
      "epoch no.4 train no.361640  loss = 2.92694 avg_loss = 3.26393\n",
      "epoch no.4 train no.361650  loss = 2.88975 avg_loss = 3.31661\n",
      "epoch no.4 train no.361660  loss = 4.17549 avg_loss = 3.30656\n",
      "epoch no.4 train no.361670  loss = 3.52000 avg_loss = 3.33012\n",
      "epoch no.4 train no.361680  loss = 1.23890 avg_loss = 3.22951\n",
      "epoch no.4 train no.361690  loss = 4.21651 avg_loss = 3.26429\n",
      "epoch no.4 train no.361700  loss = 4.26013 avg_loss = 3.27752\n",
      "epoch no.4 train no.361710  loss = 2.41272 avg_loss = 3.28127\n",
      "epoch no.4 train no.361720  loss = 1.70376 avg_loss = 3.24049\n",
      "epoch no.4 train no.361730  loss = 2.59638 avg_loss = 3.32272\n",
      "epoch no.4 train no.361740  loss = 2.81653 avg_loss = 3.26889\n",
      "epoch no.4 train no.361750  loss = 4.36606 avg_loss = 3.27280\n",
      "epoch no.4 train no.361760  loss = 3.08946 avg_loss = 3.29031\n",
      "epoch no.4 train no.361770  loss = 2.71613 avg_loss = 3.26575\n",
      "epoch no.4 train no.361780  loss = 3.83120 avg_loss = 3.26737\n",
      "epoch no.4 train no.361790  loss = 3.91016 avg_loss = 3.20223\n",
      "epoch no.4 train no.361800  loss = 3.37358 avg_loss = 3.18064\n",
      "epoch no.4 train no.361810  loss = 3.02666 avg_loss = 3.19160\n",
      "epoch no.4 train no.361820  loss = 3.24587 avg_loss = 3.18743\n",
      "epoch no.4 train no.361830  loss = 2.89483 avg_loss = 3.16315\n",
      "epoch no.4 train no.361840  loss = 3.50138 avg_loss = 3.12754\n",
      "epoch no.4 train no.361850  loss = 3.90799 avg_loss = 3.12197\n",
      "epoch no.4 train no.361860  loss = 1.52186 avg_loss = 3.08743\n",
      "epoch no.4 train no.361870  loss = 1.89435 avg_loss = 3.03831\n",
      "epoch no.4 train no.361880  loss = 2.27110 avg_loss = 3.02509\n",
      "epoch no.4 train no.361890  loss = 2.57531 avg_loss = 3.06005\n",
      "epoch no.4 train no.361900  loss = 2.48673 avg_loss = 3.04278\n",
      "epoch no.4 train no.361910  loss = 3.54666 avg_loss = 3.07858\n",
      "epoch no.4 train no.361920  loss = 2.58811 avg_loss = 3.09415\n",
      "epoch no.4 train no.361930  loss = 4.36641 avg_loss = 3.14009\n",
      "epoch no.4 train no.361940  loss = 1.92450 avg_loss = 3.09712\n",
      "epoch no.4 train no.361950  loss = 4.60353 avg_loss = 3.15994\n",
      "epoch no.4 train no.361960  loss = 3.99390 avg_loss = 3.15436\n",
      "epoch no.4 train no.361970  loss = 2.89864 avg_loss = 3.15374\n",
      "epoch no.4 train no.361980  loss = 2.01836 avg_loss = 3.19460\n",
      "epoch no.4 train no.361990  loss = 2.62556 avg_loss = 3.20540\n",
      "epoch no.4 train no.362000  loss = 4.46526 avg_loss = 3.20312\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '곡', '▁발라', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.4 train no.362010  loss = 5.09046 avg_loss = 3.24650\n",
      "epoch no.4 train no.362020  loss = 4.49329 avg_loss = 3.25333\n",
      "epoch no.4 train no.362030  loss = 3.43679 avg_loss = 3.25158\n",
      "epoch no.4 train no.362040  loss = 2.60286 avg_loss = 3.26805\n",
      "epoch no.4 train no.362050  loss = 4.23844 avg_loss = 3.33246\n",
      "epoch no.4 train no.362060  loss = 2.37134 avg_loss = 3.29370\n",
      "epoch no.4 train no.362070  loss = 2.57203 avg_loss = 3.33846\n",
      "epoch no.4 train no.362080  loss = 2.04369 avg_loss = 3.30386\n",
      "epoch no.4 train no.362090  loss = 3.52464 avg_loss = 3.30610\n",
      "epoch no.4 train no.362100  loss = 1.86830 avg_loss = 3.29662\n",
      "epoch no.4 train no.362110  loss = 3.07876 avg_loss = 3.29634\n",
      "epoch no.4 train no.362120  loss = 3.62563 avg_loss = 3.30651\n",
      "epoch no.4 train no.362130  loss = 2.03313 avg_loss = 3.30063\n",
      "epoch no.4 train no.362140  loss = 4.59749 avg_loss = 3.30545\n",
      "epoch no.4 train no.362150  loss = 3.53743 avg_loss = 3.29064\n",
      "epoch no.4 train no.362160  loss = 5.05432 avg_loss = 3.29124\n",
      "epoch no.4 train no.362170  loss = 2.59023 avg_loss = 3.28227\n",
      "epoch no.4 train no.362180  loss = 3.42364 avg_loss = 3.28816\n",
      "epoch no.4 train no.362190  loss = 3.13553 avg_loss = 3.26410\n",
      "epoch no.4 train no.362200  loss = 2.80475 avg_loss = 3.25003\n",
      "epoch no.4 train no.362210  loss = 5.30971 avg_loss = 3.25495\n",
      "epoch no.4 train no.362220  loss = 1.93255 avg_loss = 3.21671\n",
      "epoch no.4 train no.362230  loss = 2.89161 avg_loss = 3.20839\n",
      "epoch no.4 train no.362240  loss = 2.08765 avg_loss = 3.18309\n",
      "epoch no.4 train no.362250  loss = 3.70152 avg_loss = 3.22548\n",
      "epoch no.4 train no.362260  loss = 5.11856 avg_loss = 3.21954\n",
      "epoch no.4 train no.362270  loss = 2.54005 avg_loss = 3.21004\n",
      "epoch no.4 train no.362280  loss = 3.76347 avg_loss = 3.23149\n",
      "epoch no.4 train no.362290  loss = 2.44560 avg_loss = 3.23799\n",
      "epoch no.4 train no.362300  loss = 3.42068 avg_loss = 3.23474\n",
      "epoch no.4 train no.362310  loss = 3.15067 avg_loss = 3.24362\n",
      "epoch no.4 train no.362320  loss = 3.94410 avg_loss = 3.26533\n",
      "epoch no.4 train no.362330  loss = 2.77679 avg_loss = 3.23078\n",
      "epoch no.4 train no.362340  loss = 2.70543 avg_loss = 3.23148\n",
      "epoch no.4 train no.362350  loss = 2.48217 avg_loss = 3.21134\n",
      "epoch no.4 train no.362360  loss = 3.49111 avg_loss = 3.27288\n",
      "epoch no.4 train no.362370  loss = 3.60857 avg_loss = 3.28481\n",
      "epoch no.4 train no.362380  loss = 1.89700 avg_loss = 3.26710\n",
      "epoch no.4 train no.362390  loss = 4.84081 avg_loss = 3.32063\n",
      "epoch no.4 train no.362400  loss = 3.17468 avg_loss = 3.36063\n",
      "epoch no.4 train no.362410  loss = 4.05369 avg_loss = 3.39420\n",
      "epoch no.4 train no.362420  loss = 4.32816 avg_loss = 3.39530\n",
      "epoch no.4 train no.362430  loss = 4.03859 avg_loss = 3.34898\n",
      "epoch no.4 train no.362440  loss = 2.88403 avg_loss = 3.37106\n",
      "epoch no.4 train no.362450  loss = 4.02142 avg_loss = 3.33810\n",
      "epoch no.4 train no.362460  loss = 3.90943 avg_loss = 3.31256\n",
      "epoch no.4 train no.362470  loss = 2.96494 avg_loss = 3.28819\n",
      "epoch no.4 train no.362480  loss = 2.59564 avg_loss = 3.33015\n",
      "epoch no.4 train no.362490  loss = 2.33294 avg_loss = 3.38605\n",
      "epoch no.4 train no.362500  loss = 3.88534 avg_loss = 3.37256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.362510  loss = 2.83781 avg_loss = 3.35763\n",
      "epoch no.4 train no.362520  loss = 2.24823 avg_loss = 3.34992\n",
      "epoch no.4 train no.362530  loss = 4.91313 avg_loss = 3.35935\n",
      "epoch no.4 train no.362540  loss = 3.09285 avg_loss = 3.36262\n",
      "epoch no.4 train no.362550  loss = 2.73802 avg_loss = 3.34357\n",
      "epoch no.4 train no.362560  loss = 3.18234 avg_loss = 3.33560\n",
      "epoch no.4 train no.362570  loss = 2.27196 avg_loss = 3.30385\n",
      "epoch no.4 train no.362580  loss = 3.87259 avg_loss = 3.27600\n",
      "epoch no.4 train no.362590  loss = 2.66062 avg_loss = 3.28761\n",
      "epoch no.4 train no.362600  loss = 3.46882 avg_loss = 3.24697\n",
      "epoch no.4 train no.362610  loss = 2.61468 avg_loss = 3.21756\n",
      "epoch no.4 train no.362620  loss = 4.11004 avg_loss = 3.20261\n",
      "epoch no.4 train no.362630  loss = 2.62942 avg_loss = 3.15433\n",
      "epoch no.4 train no.362640  loss = 3.22218 avg_loss = 3.16693\n",
      "epoch no.4 train no.362650  loss = 2.98861 avg_loss = 3.19064\n",
      "epoch no.4 train no.362660  loss = 3.91608 avg_loss = 3.17723\n",
      "epoch no.4 train no.362670  loss = 2.20847 avg_loss = 3.20312\n",
      "epoch no.4 train no.362680  loss = 3.99602 avg_loss = 3.18914\n",
      "epoch no.4 train no.362690  loss = 3.87053 avg_loss = 3.17340\n",
      "epoch no.4 train no.362700  loss = 5.62431 avg_loss = 3.22219\n",
      "epoch no.4 train no.362710  loss = 2.65896 avg_loss = 3.23273\n",
      "epoch no.4 train no.362720  loss = 3.21338 avg_loss = 3.27120\n",
      "epoch no.4 train no.362730  loss = 2.76307 avg_loss = 3.27436\n",
      "epoch no.4 train no.362740  loss = 3.37202 avg_loss = 3.30428\n",
      "epoch no.4 train no.362750  loss = 3.56780 avg_loss = 3.29968\n",
      "epoch no.4 train no.362760  loss = 3.63929 avg_loss = 3.27960\n",
      "epoch no.4 train no.362770  loss = 2.34711 avg_loss = 3.31038\n",
      "epoch no.4 train no.362780  loss = 3.13706 avg_loss = 3.31371\n",
      "epoch no.4 train no.362790  loss = 4.28349 avg_loss = 3.32771\n",
      "epoch no.4 train no.362800  loss = 3.77017 avg_loss = 3.31455\n",
      "epoch no.4 train no.362810  loss = 3.07349 avg_loss = 3.28610\n",
      "epoch no.4 train no.362820  loss = 4.60482 avg_loss = 3.28877\n",
      "epoch no.4 train no.362830  loss = 2.43347 avg_loss = 3.34380\n",
      "epoch no.4 train no.362840  loss = 4.26650 avg_loss = 3.32795\n",
      "epoch no.4 train no.362850  loss = 4.18409 avg_loss = 3.33247\n",
      "epoch no.4 train no.362860  loss = 3.00599 avg_loss = 3.34696\n",
      "epoch no.4 train no.362870  loss = 4.30552 avg_loss = 3.30747\n",
      "epoch no.4 train no.362880  loss = 4.06878 avg_loss = 3.30136\n",
      "epoch no.4 train no.362890  loss = 2.73283 avg_loss = 3.29351\n",
      "epoch no.4 train no.362900  loss = 4.29025 avg_loss = 3.25124\n",
      "epoch no.4 train no.362910  loss = 2.75876 avg_loss = 3.19447\n",
      "epoch no.4 train no.362920  loss = 2.51566 avg_loss = 3.22470\n",
      "epoch no.4 train no.362930  loss = 3.43192 avg_loss = 3.21130\n",
      "epoch no.4 train no.362940  loss = 4.89986 avg_loss = 3.25110\n",
      "epoch no.4 train no.362950  loss = 3.76402 avg_loss = 3.25423\n",
      "epoch no.4 train no.362960  loss = 1.98706 avg_loss = 3.19679\n",
      "epoch no.4 train no.362970  loss = 3.56187 avg_loss = 3.20036\n",
      "epoch no.4 train no.362980  loss = 3.00021 avg_loss = 3.20353\n",
      "epoch no.4 train no.362990  loss = 2.80421 avg_loss = 3.22321\n",
      "epoch no.4 train no.363000  loss = 3.04603 avg_loss = 3.24742\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.4 train no.363010  loss = 3.29880 avg_loss = 3.29098\n",
      "epoch no.4 train no.363020  loss = 2.66619 avg_loss = 3.29868\n",
      "epoch no.4 train no.363030  loss = 3.31645 avg_loss = 3.29526\n",
      "epoch no.4 train no.363040  loss = 4.29829 avg_loss = 3.29798\n",
      "epoch no.4 train no.363050  loss = 2.36059 avg_loss = 3.28199\n",
      "epoch no.4 train no.363060  loss = 2.25527 avg_loss = 3.30086\n",
      "epoch no.4 train no.363070  loss = 3.28472 avg_loss = 3.26654\n",
      "epoch no.4 train no.363080  loss = 3.22422 avg_loss = 3.25283\n",
      "epoch no.4 train no.363090  loss = 2.46969 avg_loss = 3.20883\n",
      "epoch no.4 train no.363100  loss = 3.74733 avg_loss = 3.23920\n",
      "epoch no.4 train no.363110  loss = 4.04793 avg_loss = 3.24328\n",
      "epoch no.4 train no.363120  loss = 4.52792 avg_loss = 3.25768\n",
      "epoch no.4 train no.363130  loss = 2.18533 avg_loss = 3.24215\n",
      "epoch no.4 train no.363140  loss = 4.15439 avg_loss = 3.22191\n",
      "epoch no.4 train no.363150  loss = 3.14882 avg_loss = 3.18109\n",
      "epoch no.4 train no.363160  loss = 2.15149 avg_loss = 3.21339\n",
      "epoch no.4 train no.363170  loss = 2.93404 avg_loss = 3.22752\n",
      "epoch no.4 train no.363180  loss = 3.94677 avg_loss = 3.24893\n",
      "epoch no.4 train no.363190  loss = 4.25385 avg_loss = 3.21951\n",
      "epoch no.4 train no.363200  loss = 4.17376 avg_loss = 3.22453\n",
      "epoch no.4 train no.363210  loss = 3.11677 avg_loss = 3.22464\n",
      "epoch no.4 train no.363220  loss = 1.99922 avg_loss = 3.23334\n",
      "epoch no.4 train no.363230  loss = 6.39346 avg_loss = 3.28537\n",
      "epoch no.4 train no.363240  loss = 3.34603 avg_loss = 3.25243\n",
      "epoch no.4 train no.363250  loss = 2.60473 avg_loss = 3.23348\n",
      "epoch no.4 train no.363260  loss = 2.38328 avg_loss = 3.18078\n",
      "epoch no.4 train no.363270  loss = 2.97490 avg_loss = 3.23939\n",
      "epoch no.4 train no.363280  loss = 3.02328 avg_loss = 3.21423\n",
      "epoch no.4 train no.363290  loss = 2.64958 avg_loss = 3.23567\n",
      "epoch no.4 train no.363300  loss = 4.68153 avg_loss = 3.27865\n",
      "epoch no.4 train no.363310  loss = 2.16595 avg_loss = 3.27968\n",
      "epoch no.4 train no.363320  loss = 3.23887 avg_loss = 3.25401\n",
      "epoch no.4 train no.363330  loss = 4.18808 avg_loss = 3.29357\n",
      "epoch no.4 train no.363340  loss = 5.37295 avg_loss = 3.29924\n",
      "epoch no.4 train no.363350  loss = 2.49862 avg_loss = 3.28948\n",
      "epoch no.4 train no.363360  loss = 2.58411 avg_loss = 3.25576\n",
      "epoch no.4 train no.363370  loss = 4.06016 avg_loss = 3.25677\n",
      "epoch no.4 train no.363380  loss = 3.75969 avg_loss = 3.25125\n",
      "epoch no.4 train no.363390  loss = 3.82147 avg_loss = 3.24955\n",
      "epoch no.4 train no.363400  loss = 2.91035 avg_loss = 3.24765\n",
      "epoch no.4 train no.363410  loss = 4.37044 avg_loss = 3.26470\n",
      "epoch no.4 train no.363420  loss = 2.41353 avg_loss = 3.23736\n",
      "epoch no.4 train no.363430  loss = 2.32111 avg_loss = 3.21115\n",
      "epoch no.4 train no.363440  loss = 2.23254 avg_loss = 3.19684\n",
      "epoch no.4 train no.363450  loss = 2.13625 avg_loss = 3.23527\n",
      "epoch no.4 train no.363460  loss = 2.29470 avg_loss = 3.23597\n",
      "epoch no.4 train no.363470  loss = 2.93630 avg_loss = 3.27879\n",
      "epoch no.4 train no.363480  loss = 2.30031 avg_loss = 3.24584\n",
      "epoch no.4 train no.363490  loss = 3.14543 avg_loss = 3.26517\n",
      "epoch no.4 train no.363500  loss = 2.33667 avg_loss = 3.29085\n",
      "epoch no.4 train no.363510  loss = 5.08416 avg_loss = 3.31324\n",
      "epoch no.4 train no.363520  loss = 1.88475 avg_loss = 3.29719\n",
      "epoch no.4 train no.363530  loss = 3.94305 avg_loss = 3.35764\n",
      "epoch no.4 train no.363540  loss = 2.68247 avg_loss = 3.35804\n",
      "epoch no.4 train no.363550  loss = 1.85712 avg_loss = 3.34044\n",
      "epoch no.4 train no.363560  loss = 4.03685 avg_loss = 3.33460\n",
      "epoch no.4 train no.363570  loss = 3.63308 avg_loss = 3.30075\n",
      "epoch no.4 train no.363580  loss = 4.27125 avg_loss = 3.29282\n",
      "epoch no.4 train no.363590  loss = 3.75810 avg_loss = 3.29608\n",
      "epoch no.4 train no.363600  loss = 4.56181 avg_loss = 3.29224\n",
      "epoch no.4 train no.363610  loss = 3.10241 avg_loss = 3.24893\n",
      "epoch no.4 train no.363620  loss = 4.06607 avg_loss = 3.24102\n",
      "epoch no.4 train no.363630  loss = 4.60710 avg_loss = 3.24155\n",
      "epoch no.4 train no.363640  loss = 3.40895 avg_loss = 3.29111\n",
      "epoch no.4 train no.363650  loss = 4.15049 avg_loss = 3.33018\n",
      "epoch no.4 train no.363660  loss = 2.37033 avg_loss = 3.29288\n",
      "epoch no.4 train no.363670  loss = 4.44379 avg_loss = 3.28749\n",
      "epoch no.4 train no.363680  loss = 4.54179 avg_loss = 3.28958\n",
      "epoch no.4 train no.363690  loss = 2.06778 avg_loss = 3.26465\n",
      "epoch no.4 train no.363700  loss = 3.49834 avg_loss = 3.24848\n",
      "epoch no.4 train no.363710  loss = 3.78636 avg_loss = 3.27694\n",
      "epoch no.4 train no.363720  loss = 1.60926 avg_loss = 3.28586\n",
      "epoch no.4 train no.363730  loss = 3.12398 avg_loss = 3.25929\n",
      "epoch no.4 train no.363740  loss = 3.12211 avg_loss = 3.24526\n",
      "epoch no.4 train no.363750  loss = 4.36147 avg_loss = 3.23233\n",
      "epoch no.4 train no.363760  loss = 5.67018 avg_loss = 3.26502\n",
      "epoch no.4 train no.363770  loss = 3.55613 avg_loss = 3.25189\n",
      "epoch no.4 train no.363780  loss = 2.83324 avg_loss = 3.25850\n",
      "epoch no.4 train no.363790  loss = 3.13285 avg_loss = 3.24262\n",
      "epoch no.4 train no.363800  loss = 3.49260 avg_loss = 3.27367\n",
      "epoch no.4 train no.363810  loss = 2.54114 avg_loss = 3.26581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.363820  loss = 3.05215 avg_loss = 3.22054\n",
      "epoch no.4 train no.363830  loss = 1.88191 avg_loss = 3.24438\n",
      "epoch no.4 train no.363840  loss = 5.86447 avg_loss = 3.25670\n",
      "epoch no.4 train no.363850  loss = 4.74747 avg_loss = 3.21597\n",
      "epoch no.4 train no.363860  loss = 3.11627 avg_loss = 3.19708\n",
      "epoch no.4 train no.363870  loss = 4.78166 avg_loss = 3.25185\n",
      "epoch no.4 train no.363880  loss = 3.13806 avg_loss = 3.26223\n",
      "epoch no.4 train no.363890  loss = 4.43383 avg_loss = 3.25701\n",
      "epoch no.4 train no.363900  loss = 5.25885 avg_loss = 3.24301\n",
      "epoch no.4 train no.363910  loss = 2.25623 avg_loss = 3.19735\n",
      "epoch no.4 train no.363920  loss = 2.53789 avg_loss = 3.20493\n",
      "epoch no.4 train no.363930  loss = 3.19301 avg_loss = 3.17695\n",
      "epoch no.4 train no.363940  loss = 1.50522 avg_loss = 3.13365\n",
      "epoch no.4 train no.363950  loss = 3.11782 avg_loss = 3.14195\n",
      "epoch no.4 train no.363960  loss = 2.81561 avg_loss = 3.13890\n",
      "epoch no.4 train no.363970  loss = 2.53437 avg_loss = 3.13010\n",
      "epoch no.4 train no.363980  loss = 3.20556 avg_loss = 3.12818\n",
      "epoch no.4 train no.363990  loss = 2.63793 avg_loss = 3.13962\n",
      "epoch no.4 train no.364000  loss = 3.67943 avg_loss = 3.13842\n",
      "4\n",
      "to_tokens: ['▁가을', '▁드라마', '▁때', '▁그', '들', '</s>']\n",
      "추억의 그 시절 노래들</s>\n",
      "epoch no.4 train no.364010  loss = 2.85420 avg_loss = 3.13812\n",
      "epoch no.4 train no.364020  loss = 2.81145 avg_loss = 3.13834\n",
      "epoch no.4 train no.364030  loss = 3.56053 avg_loss = 3.13607\n",
      "epoch no.4 train no.364040  loss = 2.97551 avg_loss = 3.15958\n",
      "epoch no.4 train no.364050  loss = 3.75986 avg_loss = 3.17223\n",
      "epoch no.4 train no.364060  loss = 4.19154 avg_loss = 3.17718\n",
      "epoch no.4 train no.364070  loss = 2.28098 avg_loss = 3.16897\n",
      "epoch no.4 train no.364080  loss = 4.03179 avg_loss = 3.18748\n",
      "epoch no.4 train no.364090  loss = 2.01863 avg_loss = 3.15941\n",
      "epoch no.4 train no.364100  loss = 3.25627 avg_loss = 3.17148\n",
      "epoch no.4 train no.364110  loss = 3.00032 avg_loss = 3.17224\n",
      "epoch no.4 train no.364120  loss = 3.05748 avg_loss = 3.21563\n",
      "epoch no.4 train no.364130  loss = 2.26346 avg_loss = 3.22196\n",
      "epoch no.4 train no.364140  loss = 2.22015 avg_loss = 3.17213\n",
      "epoch no.4 train no.364150  loss = 3.02609 avg_loss = 3.18803\n",
      "epoch no.4 train no.364160  loss = 4.55379 avg_loss = 3.18603\n",
      "epoch no.4 train no.364170  loss = 2.60417 avg_loss = 3.18439\n",
      "epoch no.4 train no.364180  loss = 3.63633 avg_loss = 3.15966\n",
      "epoch no.4 train no.364190  loss = 2.22748 avg_loss = 3.15006\n",
      "epoch no.4 train no.364200  loss = 2.30576 avg_loss = 3.09226\n",
      "epoch no.4 train no.364210  loss = 5.35734 avg_loss = 3.13750\n",
      "epoch no.4 train no.364220  loss = 4.52125 avg_loss = 3.20402\n",
      "epoch no.4 train no.364230  loss = 2.70018 avg_loss = 3.21005\n",
      "epoch no.4 train no.364240  loss = 1.98234 avg_loss = 3.20315\n",
      "epoch no.4 train no.364250  loss = 3.17410 avg_loss = 3.23325\n",
      "epoch no.4 train no.364260  loss = 4.37622 avg_loss = 3.21629\n",
      "epoch no.4 train no.364270  loss = 3.25238 avg_loss = 3.21788\n",
      "epoch no.4 train no.364280  loss = 4.91762 avg_loss = 3.20801\n",
      "epoch no.4 train no.364290  loss = 3.72593 avg_loss = 3.24391\n",
      "epoch no.4 train no.364300  loss = 4.55935 avg_loss = 3.24493\n",
      "epoch no.4 train no.364310  loss = 2.33303 avg_loss = 3.21608\n",
      "epoch no.4 train no.364320  loss = 3.24036 avg_loss = 3.19455\n",
      "epoch no.4 train no.364330  loss = 3.13678 avg_loss = 3.22059\n",
      "epoch no.4 train no.364340  loss = 3.65805 avg_loss = 3.21626\n",
      "epoch no.4 train no.364350  loss = 4.03781 avg_loss = 3.22426\n",
      "epoch no.4 train no.364360  loss = 4.20355 avg_loss = 3.21980\n",
      "epoch no.4 train no.364370  loss = 2.70534 avg_loss = 3.21110\n",
      "epoch no.4 train no.364380  loss = 2.57943 avg_loss = 3.18701\n",
      "epoch no.4 train no.364390  loss = 3.06463 avg_loss = 3.22219\n",
      "epoch no.4 train no.364400  loss = 1.96840 avg_loss = 3.27323\n",
      "epoch no.4 train no.364410  loss = 1.91649 avg_loss = 3.24677\n",
      "epoch no.4 train no.364420  loss = 3.32354 avg_loss = 3.21274\n",
      "epoch no.4 train no.364430  loss = 2.85408 avg_loss = 3.22772\n",
      "epoch no.4 train no.364440  loss = 3.29373 avg_loss = 3.24035\n",
      "epoch no.4 train no.364450  loss = 3.33734 avg_loss = 3.27829\n",
      "epoch no.4 train no.364460  loss = 3.70254 avg_loss = 3.30256\n",
      "epoch no.4 train no.364470  loss = 2.94852 avg_loss = 3.29218\n",
      "epoch no.4 train no.364480  loss = 3.38901 avg_loss = 3.26966\n",
      "epoch no.4 train no.364490  loss = 2.61460 avg_loss = 3.27242\n",
      "epoch no.4 train no.364500  loss = 2.07884 avg_loss = 3.31082\n",
      "epoch no.4 train no.364510  loss = 2.73456 avg_loss = 3.26507\n",
      "epoch no.4 train no.364520  loss = 5.75165 avg_loss = 3.32383\n",
      "epoch no.4 train no.364530  loss = 1.92839 avg_loss = 3.31115\n",
      "epoch no.4 train no.364540  loss = 2.47128 avg_loss = 3.26924\n",
      "epoch no.4 train no.364550  loss = 2.23107 avg_loss = 3.25911\n",
      "epoch no.4 train no.364560  loss = 3.09156 avg_loss = 3.22843\n",
      "epoch no.4 train no.364570  loss = 3.87098 avg_loss = 3.22596\n",
      "epoch no.4 train no.364580  loss = 3.48024 avg_loss = 3.23648\n",
      "epoch no.4 train no.364590  loss = 4.66769 avg_loss = 3.23156\n",
      "epoch no.4 train no.364600  loss = 3.44975 avg_loss = 3.25372\n",
      "epoch no.4 train no.364610  loss = 1.78708 avg_loss = 3.19207\n",
      "epoch no.4 train no.364620  loss = 5.32899 avg_loss = 3.22255\n",
      "epoch no.4 train no.364630  loss = 2.85939 avg_loss = 3.22300\n",
      "epoch no.4 train no.364640  loss = 5.13469 avg_loss = 3.24332\n",
      "epoch no.4 train no.364650  loss = 2.34519 avg_loss = 3.25284\n",
      "epoch no.4 train no.364660  loss = 2.97399 avg_loss = 3.24546\n",
      "epoch no.4 train no.364670  loss = 3.20472 avg_loss = 3.25244\n",
      "epoch no.4 train no.364680  loss = 3.82261 avg_loss = 3.23535\n",
      "epoch no.4 train no.364690  loss = 3.38910 avg_loss = 3.21939\n",
      "epoch no.4 train no.364700  loss = 2.57641 avg_loss = 3.23423\n",
      "epoch no.4 train no.364710  loss = 2.38402 avg_loss = 3.25136\n",
      "epoch no.4 train no.364720  loss = 3.79896 avg_loss = 3.25635\n",
      "epoch no.4 train no.364730  loss = 2.78645 avg_loss = 3.21521\n",
      "epoch no.4 train no.364740  loss = 2.42652 avg_loss = 3.22741\n",
      "epoch no.4 train no.364750  loss = 2.43196 avg_loss = 3.23574\n",
      "epoch no.4 train no.364760  loss = 3.52625 avg_loss = 3.21382\n",
      "epoch no.4 train no.364770  loss = 3.19676 avg_loss = 3.30247\n",
      "epoch no.4 train no.364780  loss = 3.19265 avg_loss = 3.31002\n",
      "epoch no.4 train no.364790  loss = 2.92778 avg_loss = 3.31349\n",
      "epoch no.4 train no.364800  loss = 2.02733 avg_loss = 3.28847\n",
      "epoch no.4 train no.364810  loss = 2.94425 avg_loss = 3.27888\n",
      "epoch no.4 train no.364820  loss = 4.04002 avg_loss = 3.26725\n",
      "epoch no.4 train no.364830  loss = 3.16278 avg_loss = 3.26978\n",
      "epoch no.4 train no.364840  loss = 3.10140 avg_loss = 3.32130\n",
      "epoch no.4 train no.364850  loss = 4.26679 avg_loss = 3.34103\n",
      "epoch no.4 train no.364860  loss = 3.03477 avg_loss = 3.32307\n",
      "epoch no.4 train no.364870  loss = 1.39151 avg_loss = 3.30388\n",
      "epoch no.4 train no.364880  loss = 2.28183 avg_loss = 3.33157\n",
      "epoch no.4 train no.364890  loss = 2.84843 avg_loss = 3.32684\n",
      "epoch no.4 train no.364900  loss = 2.47109 avg_loss = 3.33324\n",
      "epoch no.4 train no.364910  loss = 4.57496 avg_loss = 3.36121\n",
      "epoch no.4 train no.364920  loss = 2.66023 avg_loss = 3.33388\n",
      "epoch no.4 train no.364930  loss = 3.78698 avg_loss = 3.30398\n",
      "epoch no.4 train no.364940  loss = 2.02502 avg_loss = 3.28962\n",
      "epoch no.4 train no.364950  loss = 4.40864 avg_loss = 3.27824\n",
      "epoch no.4 train no.364960  loss = 3.83258 avg_loss = 3.23509\n",
      "epoch no.4 train no.364970  loss = 2.13677 avg_loss = 3.22980\n",
      "epoch no.4 train no.364980  loss = 3.78931 avg_loss = 3.23439\n",
      "epoch no.4 train no.364990  loss = 2.04481 avg_loss = 3.24308\n",
      "epoch no.4 train no.365000  loss = 2.85772 avg_loss = 3.22547\n",
      "5\n",
      "to_tokens: ['▁비', '▁드라마', '▁b', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.4 train no.365010  loss = 3.25457 avg_loss = 3.21817\n",
      "epoch no.4 train no.365020  loss = 3.20894 avg_loss = 3.23980\n",
      "epoch no.4 train no.365030  loss = 3.47461 avg_loss = 3.25264\n",
      "epoch no.4 train no.365040  loss = 3.45239 avg_loss = 3.24463\n",
      "epoch no.4 train no.365050  loss = 3.96945 avg_loss = 3.24836\n",
      "epoch no.4 train no.365060  loss = 3.29161 avg_loss = 3.23884\n",
      "epoch no.4 train no.365070  loss = 1.85459 avg_loss = 3.29116\n",
      "epoch no.4 train no.365080  loss = 3.85153 avg_loss = 3.28845\n",
      "epoch no.4 train no.365090  loss = 3.73846 avg_loss = 3.28663\n",
      "epoch no.4 train no.365100  loss = 5.00131 avg_loss = 3.31780\n",
      "epoch no.4 train no.365110  loss = 4.57689 avg_loss = 3.32304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.365120  loss = 2.21549 avg_loss = 3.26865\n",
      "epoch no.4 train no.365130  loss = 3.46751 avg_loss = 3.32205\n",
      "epoch no.4 train no.365140  loss = 3.14468 avg_loss = 3.32099\n",
      "epoch no.4 train no.365150  loss = 3.76934 avg_loss = 3.31768\n",
      "epoch no.4 train no.365160  loss = 3.23868 avg_loss = 3.31497\n",
      "epoch no.4 train no.365170  loss = 2.91561 avg_loss = 3.29284\n",
      "epoch no.4 train no.365180  loss = 2.79771 avg_loss = 3.29283\n",
      "epoch no.4 train no.365190  loss = 3.73711 avg_loss = 3.30806\n",
      "epoch no.4 train no.365200  loss = 2.39649 avg_loss = 3.30279\n",
      "epoch no.4 train no.365210  loss = 2.77596 avg_loss = 3.28203\n",
      "epoch no.4 train no.365220  loss = 3.67377 avg_loss = 3.29119\n",
      "epoch no.4 train no.365230  loss = 5.86859 avg_loss = 3.29573\n",
      "epoch no.4 train no.365240  loss = 2.48881 avg_loss = 3.29495\n",
      "epoch no.4 train no.365250  loss = 3.49025 avg_loss = 3.26708\n",
      "epoch no.4 train no.365260  loss = 3.50498 avg_loss = 3.32247\n",
      "epoch no.4 train no.365270  loss = 2.19583 avg_loss = 3.28776\n",
      "epoch no.4 train no.365280  loss = 2.88005 avg_loss = 3.24336\n",
      "epoch no.4 train no.365290  loss = 3.22666 avg_loss = 3.22265\n",
      "epoch no.4 train no.365300  loss = 3.47513 avg_loss = 3.22870\n",
      "epoch no.4 train no.365310  loss = 3.09707 avg_loss = 3.20758\n",
      "epoch no.4 train no.365320  loss = 2.14766 avg_loss = 3.21594\n",
      "epoch no.4 train no.365330  loss = 2.04512 avg_loss = 3.18729\n",
      "epoch no.4 train no.365340  loss = 3.74291 avg_loss = 3.16257\n",
      "epoch no.4 train no.365350  loss = 3.52951 avg_loss = 3.17978\n",
      "epoch no.4 train no.365360  loss = 3.72967 avg_loss = 3.16106\n",
      "epoch no.4 train no.365370  loss = 2.40589 avg_loss = 3.16383\n",
      "epoch no.4 train no.365380  loss = 2.45789 avg_loss = 3.18043\n",
      "epoch no.4 train no.365390  loss = 4.13896 avg_loss = 3.20123\n",
      "epoch no.4 train no.365400  loss = 4.04786 avg_loss = 3.13777\n",
      "epoch no.4 train no.365410  loss = 1.52847 avg_loss = 3.14280\n",
      "epoch no.4 train no.365420  loss = 2.93162 avg_loss = 3.18217\n",
      "epoch no.4 train no.365430  loss = 4.09188 avg_loss = 3.18256\n",
      "epoch no.4 train no.365440  loss = 3.77017 avg_loss = 3.18464\n",
      "epoch no.4 train no.365450  loss = 2.59126 avg_loss = 3.14978\n",
      "epoch no.4 train no.365460  loss = 5.23904 avg_loss = 3.19043\n",
      "epoch no.4 train no.365470  loss = 3.95239 avg_loss = 3.22551\n",
      "epoch no.4 train no.365480  loss = 2.55396 avg_loss = 3.25497\n",
      "epoch no.4 train no.365490  loss = 2.34851 avg_loss = 3.25267\n",
      "epoch no.4 train no.365500  loss = 4.03945 avg_loss = 3.25825\n",
      "epoch no.4 train no.365510  loss = 4.19926 avg_loss = 3.30417\n",
      "epoch no.4 train no.365520  loss = 2.42063 avg_loss = 3.28877\n",
      "epoch no.4 train no.365530  loss = 2.48491 avg_loss = 3.25412\n",
      "epoch no.4 train no.365540  loss = 3.78027 avg_loss = 3.26352\n",
      "epoch no.4 train no.365550  loss = 2.95776 avg_loss = 3.27493\n",
      "epoch no.4 train no.365560  loss = 2.77089 avg_loss = 3.26125\n",
      "epoch no.4 train no.365570  loss = 2.73671 avg_loss = 3.24337\n",
      "epoch no.4 train no.365580  loss = 3.12541 avg_loss = 3.22746\n",
      "epoch no.4 train no.365590  loss = 2.91015 avg_loss = 3.23796\n",
      "epoch no.4 train no.365600  loss = 2.40307 avg_loss = 3.20769\n",
      "epoch no.4 train no.365610  loss = 3.23001 avg_loss = 3.23690\n",
      "epoch no.4 train no.365620  loss = 2.71879 avg_loss = 3.27408\n",
      "epoch no.4 train no.365630  loss = 2.72800 avg_loss = 3.30408\n",
      "epoch no.4 train no.365640  loss = 1.53425 avg_loss = 3.27945\n",
      "epoch no.4 train no.365650  loss = 5.42891 avg_loss = 3.32324\n",
      "epoch no.4 train no.365660  loss = 3.30085 avg_loss = 3.32113\n",
      "epoch no.4 train no.365670  loss = 3.69258 avg_loss = 3.31062\n",
      "epoch no.4 train no.365680  loss = 3.00119 avg_loss = 3.30775\n",
      "epoch no.4 train no.365690  loss = 3.39622 avg_loss = 3.35684\n",
      "epoch no.4 train no.365700  loss = 3.11107 avg_loss = 3.30972\n",
      "epoch no.4 train no.365710  loss = 2.89845 avg_loss = 3.28011\n",
      "epoch no.4 train no.365720  loss = 3.05950 avg_loss = 3.25954\n",
      "epoch no.4 train no.365730  loss = 5.05038 avg_loss = 3.25402\n",
      "epoch no.4 train no.365740  loss = 2.83484 avg_loss = 3.24174\n",
      "epoch no.4 train no.365750  loss = 3.69443 avg_loss = 3.23584\n",
      "epoch no.4 train no.365760  loss = 2.74733 avg_loss = 3.25724\n",
      "epoch no.4 train no.365770  loss = 3.77804 avg_loss = 3.24108\n",
      "epoch no.4 train no.365780  loss = 2.67175 avg_loss = 3.24201\n",
      "epoch no.4 train no.365790  loss = 2.91324 avg_loss = 3.24147\n",
      "epoch no.4 train no.365800  loss = 2.92121 avg_loss = 3.21728\n",
      "epoch no.4 train no.365810  loss = 3.09184 avg_loss = 3.21140\n",
      "epoch no.4 train no.365820  loss = 5.64057 avg_loss = 3.24500\n",
      "epoch no.4 train no.365830  loss = 2.51795 avg_loss = 3.22688\n",
      "epoch no.4 train no.365840  loss = 2.37644 avg_loss = 3.22699\n",
      "epoch no.4 train no.365850  loss = 3.68478 avg_loss = 3.22073\n",
      "epoch no.4 train no.365860  loss = 3.91677 avg_loss = 3.21482\n",
      "epoch no.4 train no.365870  loss = 3.71546 avg_loss = 3.28923\n",
      "epoch no.4 train no.365880  loss = 3.30437 avg_loss = 3.28687\n",
      "epoch no.4 train no.365890  loss = 3.74234 avg_loss = 3.32765\n",
      "epoch no.4 train no.365900  loss = 4.69787 avg_loss = 3.37645\n",
      "epoch no.4 train no.365910  loss = 4.11846 avg_loss = 3.42270\n",
      "epoch no.4 train no.365920  loss = 3.34051 avg_loss = 3.45135\n",
      "epoch no.4 train no.365930  loss = 4.21711 avg_loss = 3.47463\n",
      "epoch no.4 train no.365940  loss = 2.03594 avg_loss = 3.43441\n",
      "epoch no.4 train no.365950  loss = 3.55124 avg_loss = 3.37602\n",
      "epoch no.4 train no.365960  loss = 4.02710 avg_loss = 3.38277\n",
      "epoch no.4 train no.365970  loss = 2.41793 avg_loss = 3.35038\n",
      "epoch no.4 train no.365980  loss = 2.35849 avg_loss = 3.39034\n",
      "epoch no.4 train no.365990  loss = 4.74181 avg_loss = 3.39197\n",
      "epoch no.4 train no.366000  loss = 2.69478 avg_loss = 3.33895\n",
      "4\n",
      "to_tokens: ['▁가을', '▁명', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.4 train no.366010  loss = 3.34189 avg_loss = 3.33568\n",
      "epoch no.4 train no.366020  loss = 2.87240 avg_loss = 3.33219\n",
      "epoch no.4 train no.366030  loss = 2.33574 avg_loss = 3.28883\n",
      "epoch no.4 train no.366040  loss = 4.71575 avg_loss = 3.26674\n",
      "epoch no.4 train no.366050  loss = 1.85917 avg_loss = 3.22482\n",
      "epoch no.4 train no.366060  loss = 2.72672 avg_loss = 3.25384\n",
      "epoch no.4 train no.366070  loss = 1.31156 avg_loss = 3.23873\n",
      "epoch no.4 train no.366080  loss = 1.81432 avg_loss = 3.20222\n",
      "epoch no.4 train no.366090  loss = 3.52627 avg_loss = 3.19004\n",
      "epoch no.4 train no.366100  loss = 3.94188 avg_loss = 3.20955\n",
      "epoch no.4 train no.366110  loss = 3.39026 avg_loss = 3.19884\n",
      "epoch no.4 train no.366120  loss = 2.62730 avg_loss = 3.23688\n",
      "epoch no.4 train no.366130  loss = 4.16442 avg_loss = 3.23578\n",
      "epoch no.4 train no.366140  loss = 2.52900 avg_loss = 3.23384\n",
      "epoch no.4 train no.366150  loss = 3.16517 avg_loss = 3.23201\n",
      "epoch no.4 train no.366160  loss = 2.96066 avg_loss = 3.19898\n",
      "epoch no.4 train no.366170  loss = 2.64062 avg_loss = 3.15623\n",
      "epoch no.4 train no.366180  loss = 5.41433 avg_loss = 3.17063\n",
      "epoch no.4 train no.366190  loss = 2.89173 avg_loss = 3.15823\n",
      "epoch no.4 train no.366200  loss = 3.53582 avg_loss = 3.17469\n",
      "epoch no.4 train no.366210  loss = 3.61487 avg_loss = 3.17204\n",
      "epoch no.4 train no.366220  loss = 3.02768 avg_loss = 3.19009\n",
      "epoch no.4 train no.366230  loss = 3.01314 avg_loss = 3.16253\n",
      "epoch no.4 train no.366240  loss = 3.07693 avg_loss = 3.17044\n",
      "epoch no.4 train no.366250  loss = 2.54073 avg_loss = 3.16247\n",
      "epoch no.4 train no.366260  loss = 3.30631 avg_loss = 3.17855\n",
      "epoch no.4 train no.366270  loss = 6.43159 avg_loss = 3.23252\n",
      "epoch no.4 train no.366280  loss = 4.20339 avg_loss = 3.25655\n",
      "epoch no.4 train no.366290  loss = 3.47752 avg_loss = 3.27856\n",
      "epoch no.4 train no.366300  loss = 2.05524 avg_loss = 3.26837\n",
      "epoch no.4 train no.366310  loss = 4.30926 avg_loss = 3.27279\n",
      "epoch no.4 train no.366320  loss = 4.65013 avg_loss = 3.29138\n",
      "epoch no.4 train no.366330  loss = 2.23378 avg_loss = 3.29031\n",
      "epoch no.4 train no.366340  loss = 3.17716 avg_loss = 3.26006\n",
      "epoch no.4 train no.366350  loss = 2.66434 avg_loss = 3.24323\n",
      "epoch no.4 train no.366360  loss = 3.42440 avg_loss = 3.21732\n",
      "epoch no.4 train no.366370  loss = 2.43935 avg_loss = 3.17768\n",
      "epoch no.4 train no.366380  loss = 5.25733 avg_loss = 3.18900\n",
      "epoch no.4 train no.366390  loss = 4.97742 avg_loss = 3.18308\n",
      "epoch no.4 train no.366400  loss = 3.42603 avg_loss = 3.16258\n",
      "epoch no.4 train no.366410  loss = 2.87203 avg_loss = 3.12357\n",
      "epoch no.4 train no.366420  loss = 3.61931 avg_loss = 3.12420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.366430  loss = 2.96004 avg_loss = 3.14760\n",
      "epoch no.4 train no.366440  loss = 2.85508 avg_loss = 3.12005\n",
      "epoch no.4 train no.366450  loss = 3.05884 avg_loss = 3.12369\n",
      "epoch no.4 train no.366460  loss = 2.78001 avg_loss = 3.13216\n",
      "epoch no.4 train no.366470  loss = 2.39977 avg_loss = 3.12984\n",
      "epoch no.4 train no.366480  loss = 2.80492 avg_loss = 3.11402\n",
      "epoch no.4 train no.366490  loss = 2.91956 avg_loss = 3.11596\n",
      "epoch no.4 train no.366500  loss = 3.10216 avg_loss = 3.13749\n",
      "epoch no.4 train no.366510  loss = 3.81879 avg_loss = 3.14765\n",
      "epoch no.4 train no.366520  loss = 3.97904 avg_loss = 3.13497\n",
      "epoch no.4 train no.366530  loss = 2.53102 avg_loss = 3.10261\n",
      "epoch no.4 train no.366540  loss = 3.78128 avg_loss = 3.09635\n",
      "epoch no.4 train no.366550  loss = 2.04415 avg_loss = 3.13419\n",
      "epoch no.4 train no.366560  loss = 5.65328 avg_loss = 3.18773\n",
      "epoch no.4 train no.366570  loss = 2.48975 avg_loss = 3.19662\n",
      "epoch no.4 train no.366580  loss = 4.45399 avg_loss = 3.17594\n",
      "epoch no.4 train no.366590  loss = 5.73636 avg_loss = 3.20449\n",
      "epoch no.4 train no.366600  loss = 5.67267 avg_loss = 3.20529\n",
      "epoch no.4 train no.366610  loss = 3.20009 avg_loss = 3.25108\n",
      "epoch no.4 train no.366620  loss = 4.21795 avg_loss = 3.21844\n",
      "epoch no.4 train no.366630  loss = 2.22524 avg_loss = 3.23127\n",
      "epoch no.4 train no.366640  loss = 3.32735 avg_loss = 3.23278\n",
      "epoch no.4 train no.366650  loss = 2.05670 avg_loss = 3.26557\n",
      "epoch no.4 train no.366660  loss = 4.73760 avg_loss = 3.24443\n",
      "epoch no.4 train no.366670  loss = 2.75998 avg_loss = 3.26533\n",
      "epoch no.4 train no.366680  loss = 1.84967 avg_loss = 3.28743\n",
      "epoch no.4 train no.366690  loss = 2.75933 avg_loss = 3.28479\n",
      "epoch no.4 train no.366700  loss = 3.60196 avg_loss = 3.27794\n",
      "epoch no.4 train no.366710  loss = 2.46689 avg_loss = 3.22837\n",
      "epoch no.4 train no.366720  loss = 3.38806 avg_loss = 3.23743\n",
      "epoch no.4 train no.366730  loss = 3.29675 avg_loss = 3.26513\n",
      "epoch no.4 train no.366740  loss = 4.17029 avg_loss = 3.25561\n",
      "epoch no.4 train no.366750  loss = 3.19120 avg_loss = 3.29394\n",
      "epoch no.5 train no.366760  loss = 3.29391 avg_loss = 3.28109\n",
      "epoch no.5 train no.366770  loss = 2.35695 avg_loss = 3.22912\n",
      "epoch no.5 train no.366780  loss = 3.51903 avg_loss = 3.18540\n",
      "epoch no.5 train no.366790  loss = 3.48915 avg_loss = 3.15236\n",
      "epoch no.5 train no.366800  loss = 2.45752 avg_loss = 3.12868\n",
      "epoch no.5 train no.366810  loss = 3.10614 avg_loss = 3.15970\n",
      "epoch no.5 train no.366820  loss = 2.24388 avg_loss = 3.15359\n",
      "epoch no.5 train no.366830  loss = 2.96544 avg_loss = 3.12728\n",
      "epoch no.5 train no.366840  loss = 3.64568 avg_loss = 3.11754\n",
      "epoch no.5 train no.366850  loss = 3.09737 avg_loss = 3.09950\n",
      "epoch no.5 train no.366860  loss = 3.38760 avg_loss = 3.09240\n",
      "epoch no.5 train no.366870  loss = 1.98805 avg_loss = 3.02366\n",
      "epoch no.5 train no.366880  loss = 1.42439 avg_loss = 3.00152\n",
      "epoch no.5 train no.366890  loss = 3.59406 avg_loss = 3.00652\n",
      "epoch no.5 train no.366900  loss = 5.54799 avg_loss = 3.00430\n",
      "epoch no.5 train no.366910  loss = 2.35638 avg_loss = 3.00619\n",
      "epoch no.5 train no.366920  loss = 2.54115 avg_loss = 2.99747\n",
      "epoch no.5 train no.366930  loss = 2.91454 avg_loss = 2.96015\n",
      "epoch no.5 train no.366940  loss = 3.21992 avg_loss = 2.96993\n",
      "epoch no.5 train no.366950  loss = 3.86709 avg_loss = 3.01528\n",
      "epoch no.5 train no.366960  loss = 2.83857 avg_loss = 3.00369\n",
      "epoch no.5 train no.366970  loss = 3.38980 avg_loss = 2.98613\n",
      "epoch no.5 train no.366980  loss = 3.56823 avg_loss = 3.00120\n",
      "epoch no.5 train no.366990  loss = 4.75900 avg_loss = 3.02073\n",
      "epoch no.5 train no.367000  loss = 2.72798 avg_loss = 3.02367\n",
      "4\n",
      "to_tokens: ['▁가을', '▁드라마', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.5 train no.367010  loss = 2.63869 avg_loss = 2.99306\n",
      "epoch no.5 train no.367020  loss = 2.04253 avg_loss = 2.98524\n",
      "epoch no.5 train no.367030  loss = 3.21065 avg_loss = 3.01154\n",
      "epoch no.5 train no.367040  loss = 4.48575 avg_loss = 3.01106\n",
      "epoch no.5 train no.367050  loss = 5.34529 avg_loss = 2.98678\n",
      "epoch no.5 train no.367060  loss = 3.32324 avg_loss = 2.94691\n",
      "epoch no.5 train no.367070  loss = 2.16930 avg_loss = 2.93778\n",
      "epoch no.5 train no.367080  loss = 3.19285 avg_loss = 2.94140\n",
      "epoch no.5 train no.367090  loss = 2.46084 avg_loss = 2.96072\n",
      "epoch no.5 train no.367100  loss = 2.29169 avg_loss = 2.94855\n",
      "epoch no.5 train no.367110  loss = 2.37438 avg_loss = 2.93767\n",
      "epoch no.5 train no.367120  loss = 2.04957 avg_loss = 2.94792\n",
      "epoch no.5 train no.367130  loss = 2.42110 avg_loss = 2.91919\n",
      "epoch no.5 train no.367140  loss = 3.22717 avg_loss = 2.93258\n",
      "epoch no.5 train no.367150  loss = 2.21144 avg_loss = 2.92342\n",
      "epoch no.5 train no.367160  loss = 2.76890 avg_loss = 2.91182\n",
      "epoch no.5 train no.367170  loss = 2.84402 avg_loss = 2.93041\n",
      "epoch no.5 train no.367180  loss = 2.42376 avg_loss = 2.90396\n",
      "epoch no.5 train no.367190  loss = 2.80018 avg_loss = 2.93178\n",
      "epoch no.5 train no.367200  loss = 4.08810 avg_loss = 2.96535\n",
      "epoch no.5 train no.367210  loss = 2.02208 avg_loss = 2.94437\n",
      "epoch no.5 train no.367220  loss = 3.00846 avg_loss = 2.92954\n",
      "epoch no.5 train no.367230  loss = 3.32955 avg_loss = 2.95192\n",
      "epoch no.5 train no.367240  loss = 1.96882 avg_loss = 2.92960\n",
      "epoch no.5 train no.367250  loss = 2.82228 avg_loss = 2.91732\n",
      "epoch no.5 train no.367260  loss = 2.27114 avg_loss = 2.89840\n",
      "epoch no.5 train no.367270  loss = 2.30559 avg_loss = 2.86785\n",
      "epoch no.5 train no.367280  loss = 3.14869 avg_loss = 2.86793\n",
      "epoch no.5 train no.367290  loss = 3.35213 avg_loss = 2.89287\n",
      "epoch no.5 train no.367300  loss = 1.61683 avg_loss = 2.90428\n",
      "epoch no.5 train no.367310  loss = 2.87944 avg_loss = 2.91173\n",
      "epoch no.5 train no.367320  loss = 2.95200 avg_loss = 2.95362\n",
      "epoch no.5 train no.367330  loss = 2.72487 avg_loss = 2.96730\n",
      "epoch no.5 train no.367340  loss = 2.49888 avg_loss = 2.94627\n",
      "epoch no.5 train no.367350  loss = 3.05025 avg_loss = 2.93225\n",
      "epoch no.5 train no.367360  loss = 1.99333 avg_loss = 2.87074\n",
      "epoch no.5 train no.367370  loss = 3.00354 avg_loss = 2.83884\n",
      "epoch no.5 train no.367380  loss = 3.16082 avg_loss = 2.84795\n",
      "epoch no.5 train no.367390  loss = 2.37163 avg_loss = 2.88490\n",
      "epoch no.5 train no.367400  loss = 2.07486 avg_loss = 2.88725\n",
      "epoch no.5 train no.367410  loss = 2.36599 avg_loss = 2.88276\n",
      "epoch no.5 train no.367420  loss = 2.60435 avg_loss = 2.92510\n",
      "epoch no.5 train no.367430  loss = 3.09733 avg_loss = 2.98689\n",
      "epoch no.5 train no.367440  loss = 2.72747 avg_loss = 2.96122\n",
      "epoch no.5 train no.367450  loss = 3.90550 avg_loss = 2.98019\n",
      "epoch no.5 train no.367460  loss = 2.05372 avg_loss = 2.97853\n",
      "epoch no.5 train no.367470  loss = 1.91797 avg_loss = 2.95441\n",
      "epoch no.5 train no.367480  loss = 2.63251 avg_loss = 2.93199\n",
      "epoch no.5 train no.367490  loss = 3.43125 avg_loss = 2.95628\n",
      "epoch no.5 train no.367500  loss = 3.81804 avg_loss = 2.95934\n",
      "epoch no.5 train no.367510  loss = 2.25324 avg_loss = 2.96806\n",
      "epoch no.5 train no.367520  loss = 2.78621 avg_loss = 2.94531\n",
      "epoch no.5 train no.367530  loss = 4.77516 avg_loss = 2.98495\n",
      "epoch no.5 train no.367540  loss = 2.61538 avg_loss = 2.99142\n",
      "epoch no.5 train no.367550  loss = 4.08025 avg_loss = 3.02349\n",
      "epoch no.5 train no.367560  loss = 2.84252 avg_loss = 2.99114\n",
      "epoch no.5 train no.367570  loss = 2.99409 avg_loss = 2.99944\n",
      "epoch no.5 train no.367580  loss = 4.16959 avg_loss = 2.99931\n",
      "epoch no.5 train no.367590  loss = 1.89577 avg_loss = 2.98515\n",
      "epoch no.5 train no.367600  loss = 2.65988 avg_loss = 3.02053\n",
      "epoch no.5 train no.367610  loss = 2.55725 avg_loss = 2.97259\n",
      "epoch no.5 train no.367620  loss = 4.05885 avg_loss = 2.94936\n",
      "epoch no.5 train no.367630  loss = 5.49604 avg_loss = 2.95226\n",
      "epoch no.5 train no.367640  loss = 2.63720 avg_loss = 2.90595\n",
      "epoch no.5 train no.367650  loss = 2.90865 avg_loss = 2.91627\n",
      "epoch no.5 train no.367660  loss = 2.38814 avg_loss = 2.88478\n",
      "epoch no.5 train no.367670  loss = 2.24878 avg_loss = 2.88000\n",
      "epoch no.5 train no.367680  loss = 3.20030 avg_loss = 2.86758\n",
      "epoch no.5 train no.367690  loss = 1.60159 avg_loss = 2.82242\n",
      "epoch no.5 train no.367700  loss = 0.84735 avg_loss = 2.81586\n",
      "epoch no.5 train no.367710  loss = 1.69957 avg_loss = 2.81336\n",
      "epoch no.5 train no.367720  loss = 2.55829 avg_loss = 2.82277\n",
      "epoch no.5 train no.367730  loss = 2.51838 avg_loss = 2.87411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.367740  loss = 3.89638 avg_loss = 2.87657\n",
      "epoch no.5 train no.367750  loss = 3.30124 avg_loss = 2.89819\n",
      "epoch no.5 train no.367760  loss = 3.04420 avg_loss = 2.88659\n",
      "epoch no.5 train no.367770  loss = 3.12166 avg_loss = 2.88832\n",
      "epoch no.5 train no.367780  loss = 2.53866 avg_loss = 2.85711\n",
      "epoch no.5 train no.367790  loss = 2.41861 avg_loss = 2.85458\n",
      "epoch no.5 train no.367800  loss = 2.93648 avg_loss = 2.88096\n",
      "epoch no.5 train no.367810  loss = 3.11397 avg_loss = 2.88096\n",
      "epoch no.5 train no.367820  loss = 2.67772 avg_loss = 2.90533\n",
      "epoch no.5 train no.367830  loss = 2.90627 avg_loss = 2.89956\n",
      "epoch no.5 train no.367840  loss = 1.07086 avg_loss = 2.90102\n",
      "epoch no.5 train no.367850  loss = 3.04672 avg_loss = 2.91858\n",
      "epoch no.5 train no.367860  loss = 2.53805 avg_loss = 2.89241\n",
      "epoch no.5 train no.367870  loss = 1.51504 avg_loss = 2.93839\n",
      "epoch no.5 train no.367880  loss = 2.73818 avg_loss = 2.94037\n",
      "epoch no.5 train no.367890  loss = 2.61136 avg_loss = 2.92148\n",
      "epoch no.5 train no.367900  loss = 2.45739 avg_loss = 2.91055\n",
      "epoch no.5 train no.367910  loss = 2.61774 avg_loss = 2.86079\n",
      "epoch no.5 train no.367920  loss = 2.35645 avg_loss = 2.88992\n",
      "epoch no.5 train no.367930  loss = 2.01551 avg_loss = 2.86709\n",
      "epoch no.5 train no.367940  loss = 2.45330 avg_loss = 2.84855\n",
      "epoch no.5 train no.367950  loss = 1.68427 avg_loss = 2.85278\n",
      "epoch no.5 train no.367960  loss = 1.87877 avg_loss = 2.80450\n",
      "epoch no.5 train no.367970  loss = 2.56443 avg_loss = 2.79272\n",
      "epoch no.5 train no.367980  loss = 2.66259 avg_loss = 2.82806\n",
      "epoch no.5 train no.367990  loss = 3.57570 avg_loss = 2.84217\n",
      "epoch no.5 train no.368000  loss = 4.05786 avg_loss = 2.88916\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.5 train no.368010  loss = 3.54806 avg_loss = 2.89829\n",
      "epoch no.5 train no.368020  loss = 2.74607 avg_loss = 2.89389\n",
      "epoch no.5 train no.368030  loss = 3.69489 avg_loss = 2.90385\n",
      "epoch no.5 train no.368040  loss = 3.59560 avg_loss = 2.91207\n",
      "epoch no.5 train no.368050  loss = 2.92082 avg_loss = 2.93544\n",
      "epoch no.5 train no.368060  loss = 3.09434 avg_loss = 2.93120\n",
      "epoch no.5 train no.368070  loss = 2.57456 avg_loss = 2.89478\n",
      "epoch no.5 train no.368080  loss = 2.23864 avg_loss = 2.88025\n",
      "epoch no.5 train no.368090  loss = 2.44978 avg_loss = 2.85650\n",
      "epoch no.5 train no.368100  loss = 2.36502 avg_loss = 2.85414\n",
      "epoch no.5 train no.368110  loss = 3.96650 avg_loss = 2.86815\n",
      "epoch no.5 train no.368120  loss = 3.84236 avg_loss = 2.86768\n",
      "epoch no.5 train no.368130  loss = 2.88624 avg_loss = 2.87718\n",
      "epoch no.5 train no.368140  loss = 2.23610 avg_loss = 2.87525\n",
      "epoch no.5 train no.368150  loss = 2.53158 avg_loss = 2.87872\n",
      "epoch no.5 train no.368160  loss = 3.05982 avg_loss = 2.86237\n",
      "epoch no.5 train no.368170  loss = 3.72584 avg_loss = 2.88220\n",
      "epoch no.5 train no.368180  loss = 3.25309 avg_loss = 2.85864\n",
      "epoch no.5 train no.368190  loss = 3.90254 avg_loss = 2.87387\n",
      "epoch no.5 train no.368200  loss = 2.00129 avg_loss = 2.88824\n",
      "epoch no.5 train no.368210  loss = 3.02492 avg_loss = 2.89293\n",
      "epoch no.5 train no.368220  loss = 3.29645 avg_loss = 2.88512\n",
      "epoch no.5 train no.368230  loss = 2.76425 avg_loss = 2.89231\n",
      "epoch no.5 train no.368240  loss = 2.84336 avg_loss = 2.88688\n",
      "epoch no.5 train no.368250  loss = 2.50669 avg_loss = 2.86857\n",
      "epoch no.5 train no.368260  loss = 2.41871 avg_loss = 2.85220\n",
      "epoch no.5 train no.368270  loss = 2.28653 avg_loss = 2.85890\n",
      "epoch no.5 train no.368280  loss = 3.35833 avg_loss = 2.86776\n",
      "epoch no.5 train no.368290  loss = 2.72391 avg_loss = 2.81951\n",
      "epoch no.5 train no.368300  loss = 2.03263 avg_loss = 2.82102\n",
      "epoch no.5 train no.368310  loss = 2.72035 avg_loss = 2.84592\n",
      "epoch no.5 train no.368320  loss = 2.53464 avg_loss = 2.84859\n",
      "epoch no.5 train no.368330  loss = 3.22467 avg_loss = 2.84990\n",
      "epoch no.5 train no.368340  loss = 1.83381 avg_loss = 2.84897\n",
      "epoch no.5 train no.368350  loss = 2.17987 avg_loss = 2.87710\n",
      "epoch no.5 train no.368360  loss = 2.59323 avg_loss = 2.84596\n",
      "epoch no.5 train no.368370  loss = 2.10399 avg_loss = 2.87547\n",
      "epoch no.5 train no.368380  loss = 4.18614 avg_loss = 2.90343\n",
      "epoch no.5 train no.368390  loss = 2.15366 avg_loss = 2.92090\n",
      "epoch no.5 train no.368400  loss = 0.86529 avg_loss = 2.88778\n",
      "epoch no.5 train no.368410  loss = 3.84023 avg_loss = 2.85394\n",
      "epoch no.5 train no.368420  loss = 3.59195 avg_loss = 2.89939\n",
      "epoch no.5 train no.368430  loss = 2.08650 avg_loss = 2.85922\n",
      "epoch no.5 train no.368440  loss = 1.25256 avg_loss = 2.83731\n",
      "epoch no.5 train no.368450  loss = 3.31459 avg_loss = 2.83836\n",
      "epoch no.5 train no.368460  loss = 2.72876 avg_loss = 2.84329\n",
      "epoch no.5 train no.368470  loss = 1.91889 avg_loss = 2.83554\n",
      "epoch no.5 train no.368480  loss = 2.49535 avg_loss = 2.82278\n",
      "epoch no.5 train no.368490  loss = 4.06708 avg_loss = 2.85808\n",
      "epoch no.5 train no.368500  loss = 2.03642 avg_loss = 2.89211\n",
      "epoch no.5 train no.368510  loss = 2.46032 avg_loss = 2.88087\n",
      "epoch no.5 train no.368520  loss = 2.38432 avg_loss = 2.89900\n",
      "epoch no.5 train no.368530  loss = 2.25757 avg_loss = 2.88464\n",
      "epoch no.5 train no.368540  loss = 2.83332 avg_loss = 2.89589\n",
      "epoch no.5 train no.368550  loss = 3.13972 avg_loss = 2.91525\n",
      "epoch no.5 train no.368560  loss = 2.80871 avg_loss = 2.90288\n",
      "epoch no.5 train no.368570  loss = 3.54211 avg_loss = 2.89036\n",
      "epoch no.5 train no.368580  loss = 2.95930 avg_loss = 2.88762\n",
      "epoch no.5 train no.368590  loss = 3.10912 avg_loss = 2.87530\n",
      "epoch no.5 train no.368600  loss = 3.24854 avg_loss = 2.89518\n",
      "epoch no.5 train no.368610  loss = 3.51966 avg_loss = 2.90241\n",
      "epoch no.5 train no.368620  loss = 3.40241 avg_loss = 2.94102\n",
      "epoch no.5 train no.368630  loss = 3.27129 avg_loss = 2.94140\n",
      "epoch no.5 train no.368640  loss = 3.87377 avg_loss = 2.98463\n",
      "epoch no.5 train no.368650  loss = 2.54547 avg_loss = 3.00946\n",
      "epoch no.5 train no.368660  loss = 3.01306 avg_loss = 2.98857\n",
      "epoch no.5 train no.368670  loss = 3.24563 avg_loss = 2.97224\n",
      "epoch no.5 train no.368680  loss = 2.94840 avg_loss = 2.97313\n",
      "epoch no.5 train no.368690  loss = 3.51588 avg_loss = 2.95925\n",
      "epoch no.5 train no.368700  loss = 3.60985 avg_loss = 2.94731\n",
      "epoch no.5 train no.368710  loss = 3.01412 avg_loss = 2.91198\n",
      "epoch no.5 train no.368720  loss = 3.38108 avg_loss = 2.91861\n",
      "epoch no.5 train no.368730  loss = 3.37959 avg_loss = 2.93219\n",
      "epoch no.5 train no.368740  loss = 2.65204 avg_loss = 2.95240\n",
      "epoch no.5 train no.368750  loss = 2.33475 avg_loss = 2.95684\n",
      "epoch no.5 train no.368760  loss = 2.25079 avg_loss = 2.95733\n",
      "epoch no.5 train no.368770  loss = 1.94540 avg_loss = 2.95230\n",
      "epoch no.5 train no.368780  loss = 2.79336 avg_loss = 2.94284\n",
      "epoch no.5 train no.368790  loss = 4.61754 avg_loss = 2.95924\n",
      "epoch no.5 train no.368800  loss = 1.95596 avg_loss = 2.94905\n",
      "epoch no.5 train no.368810  loss = 3.75783 avg_loss = 2.95408\n",
      "epoch no.5 train no.368820  loss = 2.12044 avg_loss = 2.92551\n",
      "epoch no.5 train no.368830  loss = 2.13623 avg_loss = 2.93553\n",
      "epoch no.5 train no.368840  loss = 2.88743 avg_loss = 2.90995\n",
      "epoch no.5 train no.368850  loss = 2.72186 avg_loss = 2.89505\n",
      "epoch no.5 train no.368860  loss = 3.66039 avg_loss = 2.92891\n",
      "epoch no.5 train no.368870  loss = 2.55159 avg_loss = 2.91160\n",
      "epoch no.5 train no.368880  loss = 3.55935 avg_loss = 2.91653\n",
      "epoch no.5 train no.368890  loss = 3.46695 avg_loss = 2.89073\n",
      "epoch no.5 train no.368900  loss = 2.78407 avg_loss = 2.87233\n",
      "epoch no.5 train no.368910  loss = 2.30785 avg_loss = 2.86037\n",
      "epoch no.5 train no.368920  loss = 2.69328 avg_loss = 2.88428\n",
      "epoch no.5 train no.368930  loss = 4.64161 avg_loss = 2.88376\n",
      "epoch no.5 train no.368940  loss = 3.35318 avg_loss = 2.87387\n",
      "epoch no.5 train no.368950  loss = 1.98198 avg_loss = 2.89699\n",
      "epoch no.5 train no.368960  loss = 2.74897 avg_loss = 2.89356\n",
      "epoch no.5 train no.368970  loss = 3.02339 avg_loss = 2.91286\n",
      "epoch no.5 train no.368980  loss = 2.15134 avg_loss = 2.87281\n",
      "epoch no.5 train no.368990  loss = 3.42667 avg_loss = 2.92811\n",
      "epoch no.5 train no.369000  loss = 2.24284 avg_loss = 2.91362\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.5 train no.369010  loss = 2.68765 avg_loss = 2.94896\n",
      "epoch no.5 train no.369020  loss = 2.49105 avg_loss = 2.93044\n",
      "epoch no.5 train no.369030  loss = 2.81376 avg_loss = 2.93971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.369040  loss = 1.92461 avg_loss = 2.94593\n",
      "epoch no.5 train no.369050  loss = 2.24639 avg_loss = 2.94697\n",
      "epoch no.5 train no.369060  loss = 3.36794 avg_loss = 2.97019\n",
      "epoch no.5 train no.369070  loss = 2.11547 avg_loss = 2.94901\n",
      "epoch no.5 train no.369080  loss = 2.88946 avg_loss = 2.97384\n",
      "epoch no.5 train no.369090  loss = 3.34278 avg_loss = 2.96682\n",
      "epoch no.5 train no.369100  loss = 2.64514 avg_loss = 2.97039\n",
      "epoch no.5 train no.369110  loss = 2.30954 avg_loss = 2.96632\n",
      "epoch no.5 train no.369120  loss = 2.45361 avg_loss = 2.99086\n",
      "epoch no.5 train no.369130  loss = 2.22067 avg_loss = 2.95944\n",
      "epoch no.5 train no.369140  loss = 1.87478 avg_loss = 2.93239\n",
      "epoch no.5 train no.369150  loss = 3.12752 avg_loss = 2.93149\n",
      "epoch no.5 train no.369160  loss = 1.79723 avg_loss = 2.89885\n",
      "epoch no.5 train no.369170  loss = 3.16068 avg_loss = 2.92553\n",
      "epoch no.5 train no.369180  loss = 2.16734 avg_loss = 2.91112\n",
      "epoch no.5 train no.369190  loss = 2.19901 avg_loss = 2.88717\n",
      "epoch no.5 train no.369200  loss = 2.86946 avg_loss = 2.89010\n",
      "epoch no.5 train no.369210  loss = 4.59034 avg_loss = 2.87761\n",
      "epoch no.5 train no.369220  loss = 3.64948 avg_loss = 2.88497\n",
      "epoch no.5 train no.369230  loss = 3.02054 avg_loss = 2.88153\n",
      "epoch no.5 train no.369240  loss = 2.27706 avg_loss = 2.86147\n",
      "epoch no.5 train no.369250  loss = 3.42061 avg_loss = 2.89057\n",
      "epoch no.5 train no.369260  loss = 3.00412 avg_loss = 2.89974\n",
      "epoch no.5 train no.369270  loss = 2.34176 avg_loss = 2.90701\n",
      "epoch no.5 train no.369280  loss = 2.58881 avg_loss = 2.86785\n",
      "epoch no.5 train no.369290  loss = 3.73855 avg_loss = 2.88453\n",
      "epoch no.5 train no.369300  loss = 2.94132 avg_loss = 2.86579\n",
      "epoch no.5 train no.369310  loss = 2.52998 avg_loss = 2.86547\n",
      "epoch no.5 train no.369320  loss = 2.28524 avg_loss = 2.83997\n",
      "epoch no.5 train no.369330  loss = 2.37493 avg_loss = 2.86769\n",
      "epoch no.5 train no.369340  loss = 3.04480 avg_loss = 2.87044\n",
      "epoch no.5 train no.369350  loss = 3.32630 avg_loss = 2.86632\n",
      "epoch no.5 train no.369360  loss = 4.24568 avg_loss = 2.86667\n",
      "epoch no.5 train no.369370  loss = 2.96577 avg_loss = 2.82869\n",
      "epoch no.5 train no.369380  loss = 2.57105 avg_loss = 2.84563\n",
      "epoch no.5 train no.369390  loss = 2.74278 avg_loss = 2.87159\n",
      "epoch no.5 train no.369400  loss = 1.67689 avg_loss = 2.84558\n",
      "epoch no.5 train no.369410  loss = 2.33969 avg_loss = 2.89803\n",
      "epoch no.5 train no.369420  loss = 3.42927 avg_loss = 2.95470\n",
      "epoch no.5 train no.369430  loss = 4.29414 avg_loss = 2.96247\n",
      "epoch no.5 train no.369440  loss = 4.20343 avg_loss = 2.98258\n",
      "epoch no.5 train no.369450  loss = 3.00011 avg_loss = 2.96854\n",
      "epoch no.5 train no.369460  loss = 2.17394 avg_loss = 2.95536\n",
      "epoch no.5 train no.369470  loss = 2.98418 avg_loss = 2.96620\n",
      "epoch no.5 train no.369480  loss = 3.50841 avg_loss = 2.97788\n",
      "epoch no.5 train no.369490  loss = 1.85264 avg_loss = 2.97890\n",
      "epoch no.5 train no.369500  loss = 2.53341 avg_loss = 2.97866\n",
      "epoch no.5 train no.369510  loss = 2.82608 avg_loss = 2.95059\n",
      "epoch no.5 train no.369520  loss = 3.28311 avg_loss = 2.94540\n",
      "epoch no.5 train no.369530  loss = 2.42627 avg_loss = 2.91924\n",
      "epoch no.5 train no.369540  loss = 2.82449 avg_loss = 2.90127\n",
      "epoch no.5 train no.369550  loss = 2.84591 avg_loss = 2.91418\n",
      "epoch no.5 train no.369560  loss = 3.77806 avg_loss = 2.93917\n",
      "epoch no.5 train no.369570  loss = 2.89770 avg_loss = 2.96665\n",
      "epoch no.5 train no.369580  loss = 3.33983 avg_loss = 2.95186\n",
      "epoch no.5 train no.369590  loss = 3.59237 avg_loss = 2.93842\n",
      "epoch no.5 train no.369600  loss = 2.63831 avg_loss = 2.93837\n",
      "epoch no.5 train no.369610  loss = 2.36692 avg_loss = 2.94074\n",
      "epoch no.5 train no.369620  loss = 2.18749 avg_loss = 2.90629\n",
      "epoch no.5 train no.369630  loss = 4.22838 avg_loss = 2.91432\n",
      "epoch no.5 train no.369640  loss = 2.83003 avg_loss = 2.89291\n",
      "epoch no.5 train no.369650  loss = 3.43747 avg_loss = 2.89466\n",
      "epoch no.5 train no.369660  loss = 2.89476 avg_loss = 2.88085\n",
      "epoch no.5 train no.369670  loss = 2.49511 avg_loss = 2.93495\n",
      "epoch no.5 train no.369680  loss = 3.48822 avg_loss = 2.93909\n",
      "epoch no.5 train no.369690  loss = 4.40605 avg_loss = 2.93060\n",
      "epoch no.5 train no.369700  loss = 2.01128 avg_loss = 2.92269\n",
      "epoch no.5 train no.369710  loss = 3.09292 avg_loss = 2.92379\n",
      "epoch no.5 train no.369720  loss = 2.00169 avg_loss = 2.91702\n",
      "epoch no.5 train no.369730  loss = 3.04671 avg_loss = 2.91847\n",
      "epoch no.5 train no.369740  loss = 2.94243 avg_loss = 2.89141\n",
      "epoch no.5 train no.369750  loss = 1.06317 avg_loss = 2.87211\n",
      "epoch no.5 train no.369760  loss = 2.34987 avg_loss = 2.89568\n",
      "epoch no.5 train no.369770  loss = 3.56518 avg_loss = 2.89609\n",
      "epoch no.5 train no.369780  loss = 3.52339 avg_loss = 2.88141\n",
      "epoch no.5 train no.369790  loss = 2.55989 avg_loss = 2.89218\n",
      "epoch no.5 train no.369800  loss = 2.15565 avg_loss = 2.89091\n",
      "epoch no.5 train no.369810  loss = 3.03269 avg_loss = 2.86966\n",
      "epoch no.5 train no.369820  loss = 1.96312 avg_loss = 2.84423\n",
      "epoch no.5 train no.369830  loss = 3.74540 avg_loss = 2.87397\n",
      "epoch no.5 train no.369840  loss = 1.40287 avg_loss = 2.90082\n",
      "epoch no.5 train no.369850  loss = 2.49640 avg_loss = 2.86337\n",
      "epoch no.5 train no.369860  loss = 4.40526 avg_loss = 2.88430\n",
      "epoch no.5 train no.369870  loss = 2.56713 avg_loss = 2.87990\n",
      "epoch no.5 train no.369880  loss = 1.37667 avg_loss = 2.86836\n",
      "epoch no.5 train no.369890  loss = 1.96189 avg_loss = 2.85466\n",
      "epoch no.5 train no.369900  loss = 3.81328 avg_loss = 2.88202\n",
      "epoch no.5 train no.369910  loss = 2.19175 avg_loss = 2.87590\n",
      "epoch no.5 train no.369920  loss = 4.05859 avg_loss = 2.90242\n",
      "epoch no.5 train no.369930  loss = 3.43272 avg_loss = 2.92025\n",
      "epoch no.5 train no.369940  loss = 3.04146 avg_loss = 2.95382\n",
      "epoch no.5 train no.369950  loss = 2.26818 avg_loss = 2.93820\n",
      "epoch no.5 train no.369960  loss = 4.27605 avg_loss = 2.92514\n",
      "epoch no.5 train no.369970  loss = 2.77968 avg_loss = 2.89101\n",
      "epoch no.5 train no.369980  loss = 3.59504 avg_loss = 2.86456\n",
      "epoch no.5 train no.369990  loss = 2.65428 avg_loss = 2.84072\n",
      "epoch no.5 train no.370000  loss = 2.95766 avg_loss = 2.85838\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '▁00', '년도', '▁노래', '</s>']\n",
      "추억의 90 00년도 노래</s>\n",
      "epoch no.5 train no.370010  loss = 3.57838 avg_loss = 2.89983\n",
      "epoch no.5 train no.370020  loss = 3.21589 avg_loss = 2.87736\n",
      "epoch no.5 train no.370030  loss = 3.19533 avg_loss = 2.89987\n",
      "epoch no.5 train no.370040  loss = 2.44283 avg_loss = 2.91536\n",
      "epoch no.5 train no.370050  loss = 3.61860 avg_loss = 2.89530\n",
      "epoch no.5 train no.370060  loss = 2.66786 avg_loss = 2.89854\n",
      "epoch no.5 train no.370070  loss = 3.21766 avg_loss = 2.89744\n",
      "epoch no.5 train no.370080  loss = 2.41808 avg_loss = 2.92375\n",
      "epoch no.5 train no.370090  loss = 2.99945 avg_loss = 2.97298\n",
      "epoch no.5 train no.370100  loss = 3.25821 avg_loss = 2.97990\n",
      "epoch no.5 train no.370110  loss = 3.48063 avg_loss = 2.94835\n",
      "epoch no.5 train no.370120  loss = 2.98012 avg_loss = 2.96520\n",
      "epoch no.5 train no.370130  loss = 4.08042 avg_loss = 2.96824\n",
      "epoch no.5 train no.370140  loss = 1.97887 avg_loss = 2.98115\n",
      "epoch no.5 train no.370150  loss = 3.43280 avg_loss = 2.97513\n",
      "epoch no.5 train no.370160  loss = 4.86467 avg_loss = 2.96046\n",
      "epoch no.5 train no.370170  loss = 4.38149 avg_loss = 2.96474\n",
      "epoch no.5 train no.370180  loss = 1.97266 avg_loss = 2.96141\n",
      "epoch no.5 train no.370190  loss = 3.48520 avg_loss = 2.93068\n",
      "epoch no.5 train no.370200  loss = 3.40478 avg_loss = 2.96466\n",
      "epoch no.5 train no.370210  loss = 3.24931 avg_loss = 2.96827\n",
      "epoch no.5 train no.370220  loss = 2.37134 avg_loss = 2.94402\n",
      "epoch no.5 train no.370230  loss = 3.31136 avg_loss = 2.92879\n",
      "epoch no.5 train no.370240  loss = 2.76101 avg_loss = 2.94017\n",
      "epoch no.5 train no.370250  loss = 2.71953 avg_loss = 2.95210\n",
      "epoch no.5 train no.370260  loss = 3.36825 avg_loss = 2.97359\n",
      "epoch no.5 train no.370270  loss = 2.36102 avg_loss = 2.95839\n",
      "epoch no.5 train no.370280  loss = 2.98709 avg_loss = 2.93548\n",
      "epoch no.5 train no.370290  loss = 2.87816 avg_loss = 2.90583\n",
      "epoch no.5 train no.370300  loss = 1.70351 avg_loss = 2.90126\n",
      "epoch no.5 train no.370310  loss = 3.95233 avg_loss = 2.95931\n",
      "epoch no.5 train no.370320  loss = 2.14554 avg_loss = 2.94499\n",
      "epoch no.5 train no.370330  loss = 2.28064 avg_loss = 2.90805\n",
      "epoch no.5 train no.370340  loss = 3.05038 avg_loss = 2.88121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.370350  loss = 1.96965 avg_loss = 2.86019\n",
      "epoch no.5 train no.370360  loss = 2.15922 avg_loss = 2.85284\n",
      "epoch no.5 train no.370370  loss = 2.03220 avg_loss = 2.85627\n",
      "epoch no.5 train no.370380  loss = 4.94036 avg_loss = 2.88367\n",
      "epoch no.5 train no.370390  loss = 3.81293 avg_loss = 2.87946\n",
      "epoch no.5 train no.370400  loss = 3.54463 avg_loss = 2.86882\n",
      "epoch no.5 train no.370410  loss = 1.98264 avg_loss = 2.90725\n",
      "epoch no.5 train no.370420  loss = 3.30613 avg_loss = 2.94622\n",
      "epoch no.5 train no.370430  loss = 2.24057 avg_loss = 2.90870\n",
      "epoch no.5 train no.370440  loss = 3.60448 avg_loss = 2.96450\n",
      "epoch no.5 train no.370450  loss = 2.99039 avg_loss = 2.99945\n",
      "epoch no.5 train no.370460  loss = 3.18385 avg_loss = 3.02561\n",
      "epoch no.5 train no.370470  loss = 2.83462 avg_loss = 3.02811\n",
      "epoch no.5 train no.370480  loss = 3.10575 avg_loss = 2.98352\n",
      "epoch no.5 train no.370490  loss = 2.48695 avg_loss = 2.98862\n",
      "epoch no.5 train no.370500  loss = 3.08371 avg_loss = 2.97230\n",
      "epoch no.5 train no.370510  loss = 3.20202 avg_loss = 2.97659\n",
      "epoch no.5 train no.370520  loss = 2.16586 avg_loss = 2.99793\n",
      "epoch no.5 train no.370530  loss = 2.67270 avg_loss = 3.01704\n",
      "epoch no.5 train no.370540  loss = 2.74829 avg_loss = 3.02056\n",
      "epoch no.5 train no.370550  loss = 1.93519 avg_loss = 3.00202\n",
      "epoch no.5 train no.370560  loss = 2.91425 avg_loss = 3.00158\n",
      "epoch no.5 train no.370570  loss = 1.97954 avg_loss = 2.99260\n",
      "epoch no.5 train no.370580  loss = 4.27226 avg_loss = 3.00398\n",
      "epoch no.5 train no.370590  loss = 3.73816 avg_loss = 2.99739\n",
      "epoch no.5 train no.370600  loss = 1.97594 avg_loss = 3.00560\n",
      "epoch no.5 train no.370610  loss = 3.60433 avg_loss = 3.02124\n",
      "epoch no.5 train no.370620  loss = 3.02327 avg_loss = 3.04990\n",
      "epoch no.5 train no.370630  loss = 2.69747 avg_loss = 3.05006\n",
      "epoch no.5 train no.370640  loss = 3.77524 avg_loss = 3.06278\n",
      "epoch no.5 train no.370650  loss = 3.98021 avg_loss = 3.03809\n",
      "epoch no.5 train no.370660  loss = 2.04177 avg_loss = 3.02147\n",
      "epoch no.5 train no.370670  loss = 3.18688 avg_loss = 2.99794\n",
      "epoch no.5 train no.370680  loss = 2.65550 avg_loss = 2.99061\n",
      "epoch no.5 train no.370690  loss = 4.15434 avg_loss = 3.00865\n",
      "epoch no.5 train no.370700  loss = 4.07681 avg_loss = 3.02172\n",
      "epoch no.5 train no.370710  loss = 3.51266 avg_loss = 3.01047\n",
      "epoch no.5 train no.370720  loss = 2.31030 avg_loss = 2.97246\n",
      "epoch no.5 train no.370730  loss = 3.94195 avg_loss = 2.97376\n",
      "epoch no.5 train no.370740  loss = 2.37268 avg_loss = 3.00682\n",
      "epoch no.5 train no.370750  loss = 3.72199 avg_loss = 2.99407\n",
      "epoch no.5 train no.370760  loss = 2.06935 avg_loss = 2.96527\n",
      "epoch no.5 train no.370770  loss = 3.51304 avg_loss = 2.99706\n",
      "epoch no.5 train no.370780  loss = 3.42311 avg_loss = 3.01016\n",
      "epoch no.5 train no.370790  loss = 2.53842 avg_loss = 3.00453\n",
      "epoch no.5 train no.370800  loss = 2.14404 avg_loss = 3.01483\n",
      "epoch no.5 train no.370810  loss = 3.96417 avg_loss = 2.98566\n",
      "epoch no.5 train no.370820  loss = 2.50753 avg_loss = 2.96555\n",
      "epoch no.5 train no.370830  loss = 6.51319 avg_loss = 3.02876\n",
      "epoch no.5 train no.370840  loss = 3.82428 avg_loss = 3.00034\n",
      "epoch no.5 train no.370850  loss = 2.13834 avg_loss = 2.96478\n",
      "epoch no.5 train no.370860  loss = 2.54470 avg_loss = 2.96175\n",
      "epoch no.5 train no.370870  loss = 3.19546 avg_loss = 2.96613\n",
      "epoch no.5 train no.370880  loss = 3.49197 avg_loss = 2.97315\n",
      "epoch no.5 train no.370890  loss = 3.09856 avg_loss = 2.95713\n",
      "epoch no.5 train no.370900  loss = 2.34630 avg_loss = 2.96300\n",
      "epoch no.5 train no.370910  loss = 3.49285 avg_loss = 2.96054\n",
      "epoch no.5 train no.370920  loss = 4.79230 avg_loss = 3.01334\n",
      "epoch no.5 train no.370930  loss = 3.30266 avg_loss = 2.97884\n",
      "epoch no.5 train no.370940  loss = 2.15744 avg_loss = 2.97967\n",
      "epoch no.5 train no.370950  loss = 3.99659 avg_loss = 2.98126\n",
      "epoch no.5 train no.370960  loss = 4.29768 avg_loss = 3.00428\n",
      "epoch no.5 train no.370970  loss = 3.01228 avg_loss = 2.99698\n",
      "epoch no.5 train no.370980  loss = 2.34980 avg_loss = 2.95051\n",
      "epoch no.5 train no.370990  loss = 4.09970 avg_loss = 2.99747\n",
      "epoch no.5 train no.371000  loss = 4.01330 avg_loss = 3.00306\n",
      "7\n",
      "to_tokens: ['▁비', '▁2000', '▁팝', '▁명', '▁함께', '▁하는', '▁추억', '팝', '</s>']\n",
      "추억의 올드팝과 함께 하는 올드스쿨</s>\n",
      "epoch no.5 train no.371010  loss = 2.19198 avg_loss = 2.97590\n",
      "epoch no.5 train no.371020  loss = 3.24217 avg_loss = 2.97342\n",
      "epoch no.5 train no.371030  loss = 2.36232 avg_loss = 2.98110\n",
      "epoch no.5 train no.371040  loss = 3.52860 avg_loss = 3.00247\n",
      "epoch no.5 train no.371050  loss = 3.86753 avg_loss = 3.00755\n",
      "epoch no.5 train no.371060  loss = 2.77307 avg_loss = 3.01069\n",
      "epoch no.5 train no.371070  loss = 3.03466 avg_loss = 3.03490\n",
      "epoch no.5 train no.371080  loss = 3.44124 avg_loss = 3.02596\n",
      "epoch no.5 train no.371090  loss = 2.70610 avg_loss = 3.02925\n",
      "epoch no.5 train no.371100  loss = 2.50012 avg_loss = 3.03753\n",
      "epoch no.5 train no.371110  loss = 2.95239 avg_loss = 3.05624\n",
      "epoch no.5 train no.371120  loss = 2.28192 avg_loss = 3.04178\n",
      "epoch no.5 train no.371130  loss = 3.02784 avg_loss = 3.03966\n",
      "epoch no.5 train no.371140  loss = 3.28323 avg_loss = 3.01180\n",
      "epoch no.5 train no.371150  loss = 3.66006 avg_loss = 2.98162\n",
      "epoch no.5 train no.371160  loss = 3.89557 avg_loss = 2.99653\n",
      "epoch no.5 train no.371170  loss = 2.22149 avg_loss = 2.98748\n",
      "epoch no.5 train no.371180  loss = 3.23186 avg_loss = 3.00654\n",
      "epoch no.5 train no.371190  loss = 3.54669 avg_loss = 3.02306\n",
      "epoch no.5 train no.371200  loss = 2.85385 avg_loss = 3.01706\n",
      "epoch no.5 train no.371210  loss = 4.17615 avg_loss = 2.98990\n",
      "epoch no.5 train no.371220  loss = 2.90314 avg_loss = 2.97643\n",
      "epoch no.5 train no.371230  loss = 1.47439 avg_loss = 2.92690\n",
      "epoch no.5 train no.371240  loss = 2.57890 avg_loss = 2.86668\n",
      "epoch no.5 train no.371250  loss = 2.35674 avg_loss = 2.86273\n",
      "epoch no.5 train no.371260  loss = 2.21877 avg_loss = 2.87830\n",
      "epoch no.5 train no.371270  loss = 1.58426 avg_loss = 2.84551\n",
      "epoch no.5 train no.371280  loss = 2.55000 avg_loss = 2.87445\n",
      "epoch no.5 train no.371290  loss = 2.81229 avg_loss = 2.87150\n",
      "epoch no.5 train no.371300  loss = 2.69942 avg_loss = 2.84867\n",
      "epoch no.5 train no.371310  loss = 5.05467 avg_loss = 2.85233\n",
      "epoch no.5 train no.371320  loss = 2.76404 avg_loss = 2.84570\n",
      "epoch no.5 train no.371330  loss = 3.16132 avg_loss = 2.88558\n",
      "epoch no.5 train no.371340  loss = 2.54191 avg_loss = 2.87529\n",
      "epoch no.5 train no.371350  loss = 3.23066 avg_loss = 2.91012\n",
      "epoch no.5 train no.371360  loss = 2.66937 avg_loss = 2.92584\n",
      "epoch no.5 train no.371370  loss = 3.63782 avg_loss = 2.91827\n",
      "epoch no.5 train no.371380  loss = 3.14758 avg_loss = 2.91411\n",
      "epoch no.5 train no.371390  loss = 1.52992 avg_loss = 2.88772\n",
      "epoch no.5 train no.371400  loss = 3.91746 avg_loss = 2.90212\n",
      "epoch no.5 train no.371410  loss = 2.29372 avg_loss = 2.92610\n",
      "epoch no.5 train no.371420  loss = 2.79781 avg_loss = 2.90701\n",
      "epoch no.5 train no.371430  loss = 2.43535 avg_loss = 2.92485\n",
      "epoch no.5 train no.371440  loss = 3.66445 avg_loss = 2.94487\n",
      "epoch no.5 train no.371450  loss = 4.33880 avg_loss = 2.99201\n",
      "epoch no.5 train no.371460  loss = 2.16399 avg_loss = 2.95886\n",
      "epoch no.5 train no.371470  loss = 2.83787 avg_loss = 2.95335\n",
      "epoch no.5 train no.371480  loss = 3.80348 avg_loss = 2.92852\n",
      "epoch no.5 train no.371490  loss = 1.83299 avg_loss = 2.87638\n",
      "epoch no.5 train no.371500  loss = 2.52138 avg_loss = 2.87126\n",
      "epoch no.5 train no.371510  loss = 2.52985 avg_loss = 2.88621\n",
      "epoch no.5 train no.371520  loss = 4.05644 avg_loss = 2.90997\n",
      "epoch no.5 train no.371530  loss = 2.35811 avg_loss = 2.96585\n",
      "epoch no.5 train no.371540  loss = 2.03130 avg_loss = 2.97310\n",
      "epoch no.5 train no.371550  loss = 2.32114 avg_loss = 2.93587\n",
      "epoch no.5 train no.371560  loss = 2.39187 avg_loss = 2.86550\n",
      "epoch no.5 train no.371570  loss = 4.31730 avg_loss = 2.85909\n",
      "epoch no.5 train no.371580  loss = 3.14912 avg_loss = 2.87843\n",
      "epoch no.5 train no.371590  loss = 2.96888 avg_loss = 2.89646\n",
      "epoch no.5 train no.371600  loss = 1.91998 avg_loss = 2.86870\n",
      "epoch no.5 train no.371610  loss = 2.64309 avg_loss = 2.87843\n",
      "epoch no.5 train no.371620  loss = 3.07716 avg_loss = 2.90200\n",
      "epoch no.5 train no.371630  loss = 3.35962 avg_loss = 2.93659\n",
      "epoch no.5 train no.371640  loss = 3.82572 avg_loss = 2.91767\n",
      "epoch no.5 train no.371650  loss = 3.74877 avg_loss = 2.91289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.371660  loss = 3.47889 avg_loss = 2.94351\n",
      "epoch no.5 train no.371670  loss = 3.05366 avg_loss = 2.94035\n",
      "epoch no.5 train no.371680  loss = 2.39843 avg_loss = 2.94724\n",
      "epoch no.5 train no.371690  loss = 2.88416 avg_loss = 2.96318\n",
      "epoch no.5 train no.371700  loss = 2.12347 avg_loss = 2.95963\n",
      "epoch no.5 train no.371710  loss = 3.04853 avg_loss = 2.98489\n",
      "epoch no.5 train no.371720  loss = 2.29936 avg_loss = 2.97151\n",
      "epoch no.5 train no.371730  loss = 2.26388 avg_loss = 2.97652\n",
      "epoch no.5 train no.371740  loss = 3.31223 avg_loss = 3.00454\n",
      "epoch no.5 train no.371750  loss = 2.04936 avg_loss = 2.96399\n",
      "epoch no.5 train no.371760  loss = 3.61337 avg_loss = 2.99206\n",
      "epoch no.5 train no.371770  loss = 3.89386 avg_loss = 3.00193\n",
      "epoch no.5 train no.371780  loss = 1.63407 avg_loss = 3.00572\n",
      "epoch no.5 train no.371790  loss = 2.39703 avg_loss = 3.00833\n",
      "epoch no.5 train no.371800  loss = 3.13335 avg_loss = 3.00386\n",
      "epoch no.5 train no.371810  loss = 2.67427 avg_loss = 2.99128\n",
      "epoch no.5 train no.371820  loss = 2.66602 avg_loss = 2.99682\n",
      "epoch no.5 train no.371830  loss = 3.82568 avg_loss = 3.01386\n",
      "epoch no.5 train no.371840  loss = 2.71266 avg_loss = 2.99501\n",
      "epoch no.5 train no.371850  loss = 1.91607 avg_loss = 2.99727\n",
      "epoch no.5 train no.371860  loss = 2.56673 avg_loss = 2.99552\n",
      "epoch no.5 train no.371870  loss = 3.81757 avg_loss = 2.98494\n",
      "epoch no.5 train no.371880  loss = 2.65937 avg_loss = 2.95137\n",
      "epoch no.5 train no.371890  loss = 2.78987 avg_loss = 2.92762\n",
      "epoch no.5 train no.371900  loss = 2.79281 avg_loss = 2.90796\n",
      "epoch no.5 train no.371910  loss = 3.00872 avg_loss = 2.91046\n",
      "epoch no.5 train no.371920  loss = 2.98191 avg_loss = 2.90347\n",
      "epoch no.5 train no.371930  loss = 2.02356 avg_loss = 2.90000\n",
      "epoch no.5 train no.371940  loss = 2.99259 avg_loss = 2.86326\n",
      "epoch no.5 train no.371950  loss = 1.50479 avg_loss = 2.84311\n",
      "epoch no.5 train no.371960  loss = 2.60357 avg_loss = 2.87581\n",
      "epoch no.5 train no.371970  loss = 2.21875 avg_loss = 2.87010\n",
      "epoch no.5 train no.371980  loss = 2.56590 avg_loss = 2.86233\n",
      "epoch no.5 train no.371990  loss = 1.74178 avg_loss = 2.86787\n",
      "epoch no.5 train no.372000  loss = 3.82778 avg_loss = 2.85502\n",
      "5\n",
      "to_tokens: ['▁가을', '▁2000', '년대', '대를', '▁담은', '하다', '</s>']\n",
      "추억의 2000년대를 추억하다</s>\n",
      "epoch no.5 train no.372010  loss = 2.10787 avg_loss = 2.86019\n",
      "epoch no.5 train no.372020  loss = 2.38802 avg_loss = 2.87394\n",
      "epoch no.5 train no.372030  loss = 3.84657 avg_loss = 2.88987\n",
      "epoch no.5 train no.372040  loss = 3.40033 avg_loss = 2.91599\n",
      "epoch no.5 train no.372050  loss = 1.97165 avg_loss = 2.92382\n",
      "epoch no.5 train no.372060  loss = 2.72149 avg_loss = 2.90359\n",
      "epoch no.5 train no.372070  loss = 2.90643 avg_loss = 2.87168\n",
      "epoch no.5 train no.372080  loss = 2.15932 avg_loss = 2.87874\n",
      "epoch no.5 train no.372090  loss = 2.02812 avg_loss = 2.94801\n",
      "epoch no.5 train no.372100  loss = 3.72261 avg_loss = 2.94098\n",
      "epoch no.5 train no.372110  loss = 2.36328 avg_loss = 2.94037\n",
      "epoch no.5 train no.372120  loss = 2.74023 avg_loss = 2.97057\n",
      "epoch no.5 train no.372130  loss = 2.77623 avg_loss = 2.99119\n",
      "epoch no.5 train no.372140  loss = 1.74343 avg_loss = 2.95568\n",
      "epoch no.5 train no.372150  loss = 3.32162 avg_loss = 2.93185\n",
      "epoch no.5 train no.372160  loss = 1.86922 avg_loss = 2.91728\n",
      "epoch no.5 train no.372170  loss = 2.29957 avg_loss = 2.94573\n",
      "epoch no.5 train no.372180  loss = 1.70070 avg_loss = 2.93885\n",
      "epoch no.5 train no.372190  loss = 4.61323 avg_loss = 2.95384\n",
      "epoch no.5 train no.372200  loss = 2.10448 avg_loss = 2.95710\n",
      "epoch no.5 train no.372210  loss = 2.48239 avg_loss = 2.97722\n",
      "epoch no.5 train no.372220  loss = 2.46186 avg_loss = 2.97121\n",
      "epoch no.5 train no.372230  loss = 2.25999 avg_loss = 2.99781\n",
      "epoch no.5 train no.372240  loss = 3.47053 avg_loss = 3.00198\n",
      "epoch no.5 train no.372250  loss = 3.13107 avg_loss = 2.99905\n",
      "epoch no.5 train no.372260  loss = 3.13154 avg_loss = 2.98432\n",
      "epoch no.5 train no.372270  loss = 1.85609 avg_loss = 3.01021\n",
      "epoch no.5 train no.372280  loss = 2.70093 avg_loss = 2.95955\n",
      "epoch no.5 train no.372290  loss = 3.41759 avg_loss = 2.98903\n",
      "epoch no.5 train no.372300  loss = 2.95821 avg_loss = 2.97956\n",
      "epoch no.5 train no.372310  loss = 2.53646 avg_loss = 2.96110\n",
      "epoch no.5 train no.372320  loss = 4.02536 avg_loss = 2.96291\n",
      "epoch no.5 train no.372330  loss = 3.33986 avg_loss = 2.99453\n",
      "epoch no.5 train no.372340  loss = 3.29992 avg_loss = 2.98851\n",
      "epoch no.5 train no.372350  loss = 3.40499 avg_loss = 2.97022\n",
      "epoch no.5 train no.372360  loss = 3.43716 avg_loss = 2.95016\n",
      "epoch no.5 train no.372370  loss = 2.89666 avg_loss = 2.92597\n",
      "epoch no.5 train no.372380  loss = 3.91736 avg_loss = 2.93759\n",
      "epoch no.5 train no.372390  loss = 2.50427 avg_loss = 2.93839\n",
      "epoch no.5 train no.372400  loss = 2.83662 avg_loss = 2.96416\n",
      "epoch no.5 train no.372410  loss = 2.65409 avg_loss = 2.94620\n",
      "epoch no.5 train no.372420  loss = 2.39849 avg_loss = 2.93297\n",
      "epoch no.5 train no.372430  loss = 3.29036 avg_loss = 2.93468\n",
      "epoch no.5 train no.372440  loss = 2.59493 avg_loss = 2.92021\n",
      "epoch no.5 train no.372450  loss = 2.44106 avg_loss = 2.90105\n",
      "epoch no.5 train no.372460  loss = 3.34237 avg_loss = 2.90813\n",
      "epoch no.5 train no.372470  loss = 3.08812 avg_loss = 2.95290\n",
      "epoch no.5 train no.372480  loss = 2.90898 avg_loss = 2.94195\n",
      "epoch no.5 train no.372490  loss = 3.66804 avg_loss = 2.96157\n",
      "epoch no.5 train no.372500  loss = 2.74793 avg_loss = 2.93532\n",
      "epoch no.5 train no.372510  loss = 3.00977 avg_loss = 2.90075\n",
      "epoch no.5 train no.372520  loss = 1.87537 avg_loss = 2.93268\n",
      "epoch no.5 train no.372530  loss = 2.13314 avg_loss = 2.89248\n",
      "epoch no.5 train no.372540  loss = 3.17191 avg_loss = 2.98065\n",
      "epoch no.5 train no.372550  loss = 3.79815 avg_loss = 2.97146\n",
      "epoch no.5 train no.372560  loss = 3.04448 avg_loss = 2.96016\n",
      "epoch no.5 train no.372570  loss = 3.49986 avg_loss = 2.94367\n",
      "epoch no.5 train no.372580  loss = 4.42750 avg_loss = 2.94150\n",
      "epoch no.5 train no.372590  loss = 3.50406 avg_loss = 2.92825\n",
      "epoch no.5 train no.372600  loss = 2.65666 avg_loss = 2.92775\n",
      "epoch no.5 train no.372610  loss = 3.96205 avg_loss = 2.94778\n",
      "epoch no.5 train no.372620  loss = 4.85397 avg_loss = 2.95630\n",
      "epoch no.5 train no.372630  loss = 2.96648 avg_loss = 2.93885\n",
      "epoch no.5 train no.372640  loss = 3.81153 avg_loss = 2.92464\n",
      "epoch no.5 train no.372650  loss = 3.00555 avg_loss = 2.91483\n",
      "epoch no.5 train no.372660  loss = 1.94172 avg_loss = 2.91933\n",
      "epoch no.5 train no.372670  loss = 2.75162 avg_loss = 2.91812\n",
      "epoch no.5 train no.372680  loss = 2.64365 avg_loss = 2.88381\n",
      "epoch no.5 train no.372690  loss = 3.89299 avg_loss = 2.87588\n",
      "epoch no.5 train no.372700  loss = 2.52469 avg_loss = 2.89906\n",
      "epoch no.5 train no.372710  loss = 3.29286 avg_loss = 2.92125\n",
      "epoch no.5 train no.372720  loss = 3.01982 avg_loss = 2.91097\n",
      "epoch no.5 train no.372730  loss = 1.69597 avg_loss = 2.93422\n",
      "epoch no.5 train no.372740  loss = 3.07004 avg_loss = 2.91348\n",
      "epoch no.5 train no.372750  loss = 3.17708 avg_loss = 2.91473\n",
      "epoch no.5 train no.372760  loss = 1.33273 avg_loss = 2.93000\n",
      "epoch no.5 train no.372770  loss = 2.75355 avg_loss = 2.93437\n",
      "epoch no.5 train no.372780  loss = 4.20303 avg_loss = 2.92945\n",
      "epoch no.5 train no.372790  loss = 3.46680 avg_loss = 2.93516\n",
      "epoch no.5 train no.372800  loss = 0.91715 avg_loss = 2.88834\n",
      "epoch no.5 train no.372810  loss = 2.34521 avg_loss = 2.89308\n",
      "epoch no.5 train no.372820  loss = 3.02940 avg_loss = 2.85497\n",
      "epoch no.5 train no.372830  loss = 2.15574 avg_loss = 2.83909\n",
      "epoch no.5 train no.372840  loss = 1.81994 avg_loss = 2.85663\n",
      "epoch no.5 train no.372850  loss = 2.30236 avg_loss = 2.86403\n",
      "epoch no.5 train no.372860  loss = 2.74732 avg_loss = 2.84004\n",
      "epoch no.5 train no.372870  loss = 2.92145 avg_loss = 2.84200\n",
      "epoch no.5 train no.372880  loss = 3.57631 avg_loss = 2.84750\n",
      "epoch no.5 train no.372890  loss = 3.37603 avg_loss = 2.88459\n",
      "epoch no.5 train no.372900  loss = 3.58095 avg_loss = 2.91166\n",
      "epoch no.5 train no.372910  loss = 5.79073 avg_loss = 2.93344\n",
      "epoch no.5 train no.372920  loss = 2.71574 avg_loss = 2.93092\n",
      "epoch no.5 train no.372930  loss = 2.74403 avg_loss = 2.93605\n",
      "epoch no.5 train no.372940  loss = 2.35000 avg_loss = 2.89160\n",
      "epoch no.5 train no.372950  loss = 4.57871 avg_loss = 2.93204\n",
      "epoch no.5 train no.372960  loss = 3.26858 avg_loss = 2.94504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.372970  loss = 1.68432 avg_loss = 2.92805\n",
      "epoch no.5 train no.372980  loss = 3.33521 avg_loss = 2.91345\n",
      "epoch no.5 train no.372990  loss = 2.41292 avg_loss = 2.93070\n",
      "epoch no.5 train no.373000  loss = 2.74510 avg_loss = 2.96736\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁노래', '</s>', '</s>']\n",
      "추억의 90년대 가요 모음</s>\n",
      "epoch no.5 train no.373010  loss = 1.89812 avg_loss = 2.95518\n",
      "epoch no.5 train no.373020  loss = 3.32108 avg_loss = 2.92085\n",
      "epoch no.5 train no.373030  loss = 3.33777 avg_loss = 2.96691\n",
      "epoch no.5 train no.373040  loss = 3.27860 avg_loss = 2.96681\n",
      "epoch no.5 train no.373050  loss = 2.80389 avg_loss = 3.02787\n",
      "epoch no.5 train no.373060  loss = 2.32506 avg_loss = 3.02025\n",
      "epoch no.5 train no.373070  loss = 3.70749 avg_loss = 3.01721\n",
      "epoch no.5 train no.373080  loss = 2.91790 avg_loss = 3.01227\n",
      "epoch no.5 train no.373090  loss = 3.71343 avg_loss = 3.06332\n",
      "epoch no.5 train no.373100  loss = 3.08153 avg_loss = 3.06727\n",
      "epoch no.5 train no.373110  loss = 2.66614 avg_loss = 3.07748\n",
      "epoch no.5 train no.373120  loss = 2.72902 avg_loss = 3.03386\n",
      "epoch no.5 train no.373130  loss = 1.79487 avg_loss = 3.01185\n",
      "epoch no.5 train no.373140  loss = 2.95030 avg_loss = 3.01079\n",
      "epoch no.5 train no.373150  loss = 2.38101 avg_loss = 2.99948\n",
      "epoch no.5 train no.373160  loss = 3.73300 avg_loss = 3.02871\n",
      "epoch no.5 train no.373170  loss = 2.70355 avg_loss = 3.04277\n",
      "epoch no.5 train no.373180  loss = 2.76358 avg_loss = 3.05699\n",
      "epoch no.5 train no.373190  loss = 2.98784 avg_loss = 3.04767\n",
      "epoch no.5 train no.373200  loss = 2.62133 avg_loss = 3.02999\n",
      "epoch no.5 train no.373210  loss = 2.96662 avg_loss = 3.00833\n",
      "epoch no.5 train no.373220  loss = 3.31444 avg_loss = 3.00562\n",
      "epoch no.5 train no.373230  loss = 3.11403 avg_loss = 2.99685\n",
      "epoch no.5 train no.373240  loss = 3.52690 avg_loss = 3.01162\n",
      "epoch no.5 train no.373250  loss = 2.57273 avg_loss = 2.98392\n",
      "epoch no.5 train no.373260  loss = 1.83809 avg_loss = 2.95948\n",
      "epoch no.5 train no.373270  loss = 2.17349 avg_loss = 2.95885\n",
      "epoch no.5 train no.373280  loss = 2.72159 avg_loss = 2.93730\n",
      "epoch no.5 train no.373290  loss = 2.90350 avg_loss = 2.95080\n",
      "epoch no.5 train no.373300  loss = 2.22921 avg_loss = 2.95027\n",
      "epoch no.5 train no.373310  loss = 3.80782 avg_loss = 2.97719\n",
      "epoch no.5 train no.373320  loss = 1.68564 avg_loss = 2.91965\n",
      "epoch no.5 train no.373330  loss = 2.89478 avg_loss = 2.90645\n",
      "epoch no.5 train no.373340  loss = 2.80566 avg_loss = 2.90224\n",
      "epoch no.5 train no.373350  loss = 2.04701 avg_loss = 2.89431\n",
      "epoch no.5 train no.373360  loss = 3.36197 avg_loss = 2.92078\n",
      "epoch no.5 train no.373370  loss = 2.82036 avg_loss = 2.91934\n",
      "epoch no.5 train no.373380  loss = 2.97366 avg_loss = 2.90702\n",
      "epoch no.5 train no.373390  loss = 2.40047 avg_loss = 2.89074\n",
      "epoch no.5 train no.373400  loss = 4.27034 avg_loss = 2.93709\n",
      "epoch no.5 train no.373410  loss = 2.23262 avg_loss = 2.93369\n",
      "epoch no.5 train no.373420  loss = 3.25613 avg_loss = 2.94337\n",
      "epoch no.5 train no.373430  loss = 2.27374 avg_loss = 2.95404\n",
      "epoch no.5 train no.373440  loss = 2.81029 avg_loss = 2.97674\n",
      "epoch no.5 train no.373450  loss = 2.26401 avg_loss = 2.98371\n",
      "epoch no.5 train no.373460  loss = 4.14440 avg_loss = 3.01277\n",
      "epoch no.5 train no.373470  loss = 2.85780 avg_loss = 2.97922\n",
      "epoch no.5 train no.373480  loss = 2.62403 avg_loss = 2.99580\n",
      "epoch no.5 train no.373490  loss = 2.21962 avg_loss = 2.95577\n",
      "epoch no.5 train no.373500  loss = 2.55944 avg_loss = 2.96755\n",
      "epoch no.5 train no.373510  loss = 3.18353 avg_loss = 2.97296\n",
      "epoch no.5 train no.373520  loss = 1.94345 avg_loss = 2.95848\n",
      "epoch no.5 train no.373530  loss = 2.11001 avg_loss = 2.97126\n",
      "epoch no.5 train no.373540  loss = 2.32053 avg_loss = 2.98242\n",
      "epoch no.5 train no.373550  loss = 2.17641 avg_loss = 2.98669\n",
      "epoch no.5 train no.373560  loss = 3.79015 avg_loss = 2.99918\n",
      "epoch no.5 train no.373570  loss = 2.40030 avg_loss = 2.97043\n",
      "epoch no.5 train no.373580  loss = 4.64070 avg_loss = 3.00358\n",
      "epoch no.5 train no.373590  loss = 2.76591 avg_loss = 2.98549\n",
      "epoch no.5 train no.373600  loss = 2.77454 avg_loss = 3.01250\n",
      "epoch no.5 train no.373610  loss = 2.81247 avg_loss = 3.05848\n",
      "epoch no.5 train no.373620  loss = 4.84691 avg_loss = 3.06309\n",
      "epoch no.5 train no.373630  loss = 2.77103 avg_loss = 3.09466\n",
      "epoch no.5 train no.373640  loss = 1.68727 avg_loss = 3.04397\n",
      "epoch no.5 train no.373650  loss = 2.84775 avg_loss = 2.99612\n",
      "epoch no.5 train no.373660  loss = 3.79100 avg_loss = 2.98371\n",
      "epoch no.5 train no.373670  loss = 2.86405 avg_loss = 2.98702\n",
      "epoch no.5 train no.373680  loss = 2.68447 avg_loss = 2.98297\n",
      "epoch no.5 train no.373690  loss = 2.59295 avg_loss = 2.98934\n",
      "epoch no.5 train no.373700  loss = 2.27937 avg_loss = 2.98767\n",
      "epoch no.5 train no.373710  loss = 4.07085 avg_loss = 2.99046\n",
      "epoch no.5 train no.373720  loss = 2.05000 avg_loss = 2.95216\n",
      "epoch no.5 train no.373730  loss = 3.14565 avg_loss = 2.93566\n",
      "epoch no.5 train no.373740  loss = 2.63594 avg_loss = 2.96301\n",
      "epoch no.5 train no.373750  loss = 2.15779 avg_loss = 2.95731\n",
      "epoch no.5 train no.373760  loss = 2.87260 avg_loss = 2.95065\n",
      "epoch no.5 train no.373770  loss = 2.94962 avg_loss = 2.97300\n",
      "epoch no.5 train no.373780  loss = 3.59304 avg_loss = 3.01221\n",
      "epoch no.5 train no.373790  loss = 3.98879 avg_loss = 3.03748\n",
      "epoch no.5 train no.373800  loss = 3.67134 avg_loss = 3.04680\n",
      "epoch no.5 train no.373810  loss = 2.66750 avg_loss = 3.00774\n",
      "epoch no.5 train no.373820  loss = 2.56825 avg_loss = 2.97079\n",
      "epoch no.5 train no.373830  loss = 3.29960 avg_loss = 2.98123\n",
      "epoch no.5 train no.373840  loss = 3.55342 avg_loss = 2.99935\n",
      "epoch no.5 train no.373850  loss = 2.61042 avg_loss = 2.96608\n",
      "epoch no.5 train no.373860  loss = 4.31625 avg_loss = 2.99318\n",
      "epoch no.5 train no.373870  loss = 1.93582 avg_loss = 2.96446\n",
      "epoch no.5 train no.373880  loss = 2.64862 avg_loss = 2.99430\n",
      "epoch no.5 train no.373890  loss = 2.61559 avg_loss = 2.98930\n",
      "epoch no.5 train no.373900  loss = 3.11920 avg_loss = 3.02020\n",
      "epoch no.5 train no.373910  loss = 2.92402 avg_loss = 3.00801\n",
      "epoch no.5 train no.373920  loss = 2.74263 avg_loss = 3.00951\n",
      "epoch no.5 train no.373930  loss = 1.68093 avg_loss = 2.98650\n",
      "epoch no.5 train no.373940  loss = 3.54573 avg_loss = 2.96907\n",
      "epoch no.5 train no.373950  loss = 3.27243 avg_loss = 3.00323\n",
      "epoch no.5 train no.373960  loss = 2.65624 avg_loss = 3.05615\n",
      "epoch no.5 train no.373970  loss = 4.15011 avg_loss = 3.03681\n",
      "epoch no.5 train no.373980  loss = 2.96746 avg_loss = 3.00197\n",
      "epoch no.5 train no.373990  loss = 2.97185 avg_loss = 2.99261\n",
      "epoch no.5 train no.374000  loss = 2.51061 avg_loss = 2.98186\n",
      "5\n",
      "to_tokens: ['▁가을', '▁2000', '▁o', 'st', '▁모음', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.5 train no.374010  loss = 4.19605 avg_loss = 2.97147\n",
      "epoch no.5 train no.374020  loss = 2.01736 avg_loss = 2.93409\n",
      "epoch no.5 train no.374030  loss = 3.06299 avg_loss = 2.88755\n",
      "epoch no.5 train no.374040  loss = 1.84710 avg_loss = 2.86619\n",
      "epoch no.5 train no.374050  loss = 2.96126 avg_loss = 2.86728\n",
      "epoch no.5 train no.374060  loss = 2.28622 avg_loss = 2.86918\n",
      "epoch no.5 train no.374070  loss = 3.06783 avg_loss = 2.89114\n",
      "epoch no.5 train no.374080  loss = 2.27461 avg_loss = 2.88881\n",
      "epoch no.5 train no.374090  loss = 3.12501 avg_loss = 2.91971\n",
      "epoch no.5 train no.374100  loss = 2.63600 avg_loss = 2.90091\n",
      "epoch no.5 train no.374110  loss = 2.64729 avg_loss = 2.87473\n",
      "epoch no.5 train no.374120  loss = 2.68216 avg_loss = 2.90150\n",
      "epoch no.5 train no.374130  loss = 2.33156 avg_loss = 2.88714\n",
      "epoch no.5 train no.374140  loss = 3.66960 avg_loss = 2.88538\n",
      "epoch no.5 train no.374150  loss = 3.08159 avg_loss = 2.87340\n",
      "epoch no.5 train no.374160  loss = 1.88798 avg_loss = 2.90154\n",
      "epoch no.5 train no.374170  loss = 2.71667 avg_loss = 2.90428\n",
      "epoch no.5 train no.374180  loss = 2.82199 avg_loss = 2.92485\n",
      "epoch no.5 train no.374190  loss = 3.28568 avg_loss = 2.93202\n",
      "epoch no.5 train no.374200  loss = 3.12007 avg_loss = 2.91010\n",
      "epoch no.5 train no.374210  loss = 2.21869 avg_loss = 2.93874\n",
      "epoch no.5 train no.374220  loss = 2.35590 avg_loss = 2.91283\n",
      "epoch no.5 train no.374230  loss = 2.41393 avg_loss = 2.92225\n",
      "epoch no.5 train no.374240  loss = 3.12436 avg_loss = 2.94618\n",
      "epoch no.5 train no.374250  loss = 3.51797 avg_loss = 2.96694\n",
      "epoch no.5 train no.374260  loss = 2.74599 avg_loss = 2.94448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.374270  loss = 3.65078 avg_loss = 2.93540\n",
      "epoch no.5 train no.374280  loss = 3.76395 avg_loss = 2.91589\n",
      "epoch no.5 train no.374290  loss = 5.14696 avg_loss = 2.94133\n",
      "epoch no.5 train no.374300  loss = 2.34727 avg_loss = 2.97069\n",
      "epoch no.5 train no.374310  loss = 3.31358 avg_loss = 2.92539\n",
      "epoch no.5 train no.374320  loss = 2.49763 avg_loss = 2.87997\n",
      "epoch no.5 train no.374330  loss = 3.93796 avg_loss = 2.92823\n",
      "epoch no.5 train no.374340  loss = 3.43319 avg_loss = 2.93975\n",
      "epoch no.5 train no.374350  loss = 2.43981 avg_loss = 2.96306\n",
      "epoch no.5 train no.374360  loss = 1.83824 avg_loss = 3.00593\n",
      "epoch no.5 train no.374370  loss = 3.93239 avg_loss = 3.00714\n",
      "epoch no.5 train no.374380  loss = 2.75861 avg_loss = 3.01948\n",
      "epoch no.5 train no.374390  loss = 2.65152 avg_loss = 3.03668\n",
      "epoch no.5 train no.374400  loss = 2.53530 avg_loss = 2.98946\n",
      "epoch no.5 train no.374410  loss = 2.29962 avg_loss = 2.97397\n",
      "epoch no.5 train no.374420  loss = 2.16228 avg_loss = 2.94993\n",
      "epoch no.5 train no.374430  loss = 2.92770 avg_loss = 2.95244\n",
      "epoch no.5 train no.374440  loss = 3.06610 avg_loss = 2.96087\n",
      "epoch no.5 train no.374450  loss = 2.74582 avg_loss = 2.97535\n",
      "epoch no.5 train no.374460  loss = 3.33018 avg_loss = 2.98836\n",
      "epoch no.5 train no.374470  loss = 3.40560 avg_loss = 3.02272\n",
      "epoch no.5 train no.374480  loss = 2.07387 avg_loss = 3.07729\n",
      "epoch no.5 train no.374490  loss = 2.57555 avg_loss = 3.07410\n",
      "epoch no.5 train no.374500  loss = 3.45283 avg_loss = 3.07765\n",
      "epoch no.5 train no.374510  loss = 2.38122 avg_loss = 3.07075\n",
      "epoch no.5 train no.374520  loss = 2.38311 avg_loss = 3.04747\n",
      "epoch no.5 train no.374530  loss = 2.54860 avg_loss = 3.01556\n",
      "epoch no.5 train no.374540  loss = 2.84215 avg_loss = 3.03337\n",
      "epoch no.5 train no.374550  loss = 2.81961 avg_loss = 3.01675\n",
      "epoch no.5 train no.374560  loss = 2.54032 avg_loss = 2.98906\n",
      "epoch no.5 train no.374570  loss = 1.50003 avg_loss = 2.96812\n",
      "epoch no.5 train no.374580  loss = 2.62565 avg_loss = 2.96729\n",
      "epoch no.5 train no.374590  loss = 3.48675 avg_loss = 2.98741\n",
      "epoch no.5 train no.374600  loss = 2.71695 avg_loss = 2.97730\n",
      "epoch no.5 train no.374610  loss = 2.93777 avg_loss = 2.95398\n",
      "epoch no.5 train no.374620  loss = 1.86971 avg_loss = 2.93941\n",
      "epoch no.5 train no.374630  loss = 2.22795 avg_loss = 2.93979\n",
      "epoch no.5 train no.374640  loss = 1.76564 avg_loss = 2.91060\n",
      "epoch no.5 train no.374650  loss = 2.66472 avg_loss = 2.93720\n",
      "epoch no.5 train no.374660  loss = 2.75014 avg_loss = 2.94712\n",
      "epoch no.5 train no.374670  loss = 2.76198 avg_loss = 2.90365\n",
      "epoch no.5 train no.374680  loss = 3.36363 avg_loss = 2.93049\n",
      "epoch no.5 train no.374690  loss = 2.77164 avg_loss = 2.95774\n",
      "epoch no.5 train no.374700  loss = 3.02011 avg_loss = 2.95938\n",
      "epoch no.5 train no.374710  loss = 2.88140 avg_loss = 2.93518\n",
      "epoch no.5 train no.374720  loss = 2.02524 avg_loss = 2.92038\n",
      "epoch no.5 train no.374730  loss = 2.52712 avg_loss = 2.92296\n",
      "epoch no.5 train no.374740  loss = 3.05464 avg_loss = 2.95402\n",
      "epoch no.5 train no.374750  loss = 4.07041 avg_loss = 2.96513\n",
      "epoch no.5 train no.374760  loss = 2.96963 avg_loss = 2.95634\n",
      "epoch no.5 train no.374770  loss = 2.20283 avg_loss = 2.92758\n",
      "epoch no.5 train no.374780  loss = 1.93341 avg_loss = 2.94678\n",
      "epoch no.5 train no.374790  loss = 3.58775 avg_loss = 2.92065\n",
      "epoch no.5 train no.374800  loss = 2.97208 avg_loss = 2.89749\n",
      "epoch no.5 train no.374810  loss = 2.88424 avg_loss = 2.87727\n",
      "epoch no.5 train no.374820  loss = 3.05250 avg_loss = 2.87321\n",
      "epoch no.5 train no.374830  loss = 2.14545 avg_loss = 2.84680\n",
      "epoch no.5 train no.374840  loss = 3.58258 avg_loss = 2.82034\n",
      "epoch no.5 train no.374850  loss = 3.44871 avg_loss = 2.79868\n",
      "epoch no.5 train no.374860  loss = 3.49250 avg_loss = 2.80015\n",
      "epoch no.5 train no.374870  loss = 2.81707 avg_loss = 2.83590\n",
      "epoch no.5 train no.374880  loss = 2.54778 avg_loss = 2.82259\n",
      "epoch no.5 train no.374890  loss = 3.32883 avg_loss = 2.81215\n",
      "epoch no.5 train no.374900  loss = 3.60873 avg_loss = 2.82207\n",
      "epoch no.5 train no.374910  loss = 2.54063 avg_loss = 2.79357\n",
      "epoch no.5 train no.374920  loss = 2.86802 avg_loss = 2.81490\n",
      "epoch no.5 train no.374930  loss = 3.62014 avg_loss = 2.82376\n",
      "epoch no.5 train no.374940  loss = 3.24054 avg_loss = 2.83922\n",
      "epoch no.5 train no.374950  loss = 3.40462 avg_loss = 2.87853\n",
      "epoch no.5 train no.374960  loss = 2.44973 avg_loss = 2.89935\n",
      "epoch no.5 train no.374970  loss = 3.67023 avg_loss = 2.90959\n",
      "epoch no.5 train no.374980  loss = 3.46034 avg_loss = 2.93354\n",
      "epoch no.5 train no.374990  loss = 2.96271 avg_loss = 2.94336\n",
      "epoch no.5 train no.375000  loss = 4.28218 avg_loss = 2.94831\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', 'nb', '</s>']\n",
      "추억의 2000년대 rnb</s>\n",
      "epoch no.5 train no.375010  loss = 2.80501 avg_loss = 2.93398\n",
      "epoch no.5 train no.375020  loss = 2.59080 avg_loss = 2.93060\n",
      "epoch no.5 train no.375030  loss = 4.84267 avg_loss = 2.94440\n",
      "epoch no.5 train no.375040  loss = 3.03258 avg_loss = 2.96776\n",
      "epoch no.5 train no.375050  loss = 3.50206 avg_loss = 2.97229\n",
      "epoch no.5 train no.375060  loss = 4.27773 avg_loss = 2.95828\n",
      "epoch no.5 train no.375070  loss = 4.25016 avg_loss = 2.98446\n",
      "epoch no.5 train no.375080  loss = 3.60581 avg_loss = 3.01717\n",
      "epoch no.5 train no.375090  loss = 1.95642 avg_loss = 3.01471\n",
      "epoch no.5 train no.375100  loss = 2.20625 avg_loss = 2.99197\n",
      "epoch no.5 train no.375110  loss = 2.14254 avg_loss = 2.99495\n",
      "epoch no.5 train no.375120  loss = 2.18700 avg_loss = 3.01346\n",
      "epoch no.5 train no.375130  loss = 3.63801 avg_loss = 3.01122\n",
      "epoch no.5 train no.375140  loss = 2.31998 avg_loss = 2.99946\n",
      "epoch no.5 train no.375150  loss = 1.84540 avg_loss = 2.94623\n",
      "epoch no.5 train no.375160  loss = 2.16952 avg_loss = 2.94002\n",
      "epoch no.5 train no.375170  loss = 2.89256 avg_loss = 2.94752\n",
      "epoch no.5 train no.375180  loss = 3.07665 avg_loss = 2.94847\n",
      "epoch no.5 train no.375190  loss = 2.31672 avg_loss = 2.94675\n",
      "epoch no.5 train no.375200  loss = 2.13884 avg_loss = 2.93689\n",
      "epoch no.5 train no.375210  loss = 2.18696 avg_loss = 2.92734\n",
      "epoch no.5 train no.375220  loss = 1.75432 avg_loss = 2.90906\n",
      "epoch no.5 train no.375230  loss = 2.45162 avg_loss = 2.89098\n",
      "epoch no.5 train no.375240  loss = 2.47838 avg_loss = 2.89586\n",
      "epoch no.5 train no.375250  loss = 2.63824 avg_loss = 2.88216\n",
      "epoch no.5 train no.375260  loss = 2.55722 avg_loss = 2.86622\n",
      "epoch no.5 train no.375270  loss = 4.11601 avg_loss = 2.88080\n",
      "epoch no.5 train no.375280  loss = 2.98408 avg_loss = 2.86309\n",
      "epoch no.5 train no.375290  loss = 1.51382 avg_loss = 2.86302\n",
      "epoch no.5 train no.375300  loss = 2.95678 avg_loss = 2.87065\n",
      "epoch no.5 train no.375310  loss = 1.89730 avg_loss = 2.83005\n",
      "epoch no.5 train no.375320  loss = 3.06276 avg_loss = 2.85831\n",
      "epoch no.5 train no.375330  loss = 1.85022 avg_loss = 2.85700\n",
      "epoch no.5 train no.375340  loss = 3.79442 avg_loss = 2.84970\n",
      "epoch no.5 train no.375350  loss = 3.02511 avg_loss = 2.87373\n",
      "epoch no.5 train no.375360  loss = 2.95421 avg_loss = 2.85593\n",
      "epoch no.5 train no.375370  loss = 3.58751 avg_loss = 2.91382\n",
      "epoch no.5 train no.375380  loss = 2.65839 avg_loss = 2.95910\n",
      "epoch no.5 train no.375390  loss = 3.81612 avg_loss = 2.94932\n",
      "epoch no.5 train no.375400  loss = 1.94183 avg_loss = 2.92345\n",
      "epoch no.5 train no.375410  loss = 4.17810 avg_loss = 2.94605\n",
      "epoch no.5 train no.375420  loss = 1.82028 avg_loss = 2.92200\n",
      "epoch no.5 train no.375430  loss = 4.31513 avg_loss = 2.96369\n",
      "epoch no.5 train no.375440  loss = 2.06974 avg_loss = 2.92420\n",
      "epoch no.5 train no.375450  loss = 3.83809 avg_loss = 2.87592\n",
      "epoch no.5 train no.375460  loss = 3.08235 avg_loss = 2.90803\n",
      "epoch no.5 train no.375470  loss = 3.17926 avg_loss = 2.91037\n",
      "epoch no.5 train no.375480  loss = 4.60782 avg_loss = 2.93658\n",
      "epoch no.5 train no.375490  loss = 3.28241 avg_loss = 2.98667\n",
      "epoch no.5 train no.375500  loss = 1.87017 avg_loss = 2.96472\n",
      "epoch no.5 train no.375510  loss = 2.98814 avg_loss = 2.91314\n",
      "epoch no.5 train no.375520  loss = 3.27521 avg_loss = 2.94975\n",
      "epoch no.5 train no.375530  loss = 2.84954 avg_loss = 2.92645\n",
      "epoch no.5 train no.375540  loss = 1.34700 avg_loss = 2.92729\n",
      "epoch no.5 train no.375550  loss = 3.18236 avg_loss = 2.94880\n",
      "epoch no.5 train no.375560  loss = 3.37785 avg_loss = 2.99482\n",
      "epoch no.5 train no.375570  loss = 3.50065 avg_loss = 3.00947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.375580  loss = 3.75472 avg_loss = 2.96654\n",
      "epoch no.5 train no.375590  loss = 4.52727 avg_loss = 2.98438\n",
      "epoch no.5 train no.375600  loss = 2.51960 avg_loss = 2.95517\n",
      "epoch no.5 train no.375610  loss = 1.73481 avg_loss = 2.96746\n",
      "epoch no.5 train no.375620  loss = 3.27237 avg_loss = 2.97275\n",
      "epoch no.5 train no.375630  loss = 3.75880 avg_loss = 2.98349\n",
      "epoch no.5 train no.375640  loss = 1.79072 avg_loss = 2.95874\n",
      "epoch no.5 train no.375650  loss = 2.03999 avg_loss = 2.98557\n",
      "epoch no.5 train no.375660  loss = 2.87245 avg_loss = 2.96820\n",
      "epoch no.5 train no.375670  loss = 4.05370 avg_loss = 2.96143\n",
      "epoch no.5 train no.375680  loss = 1.94839 avg_loss = 2.94222\n",
      "epoch no.5 train no.375690  loss = 3.18020 avg_loss = 2.94895\n",
      "epoch no.5 train no.375700  loss = 3.48038 avg_loss = 2.94062\n",
      "epoch no.5 train no.375710  loss = 3.22545 avg_loss = 2.95081\n",
      "epoch no.5 train no.375720  loss = 2.69424 avg_loss = 2.89485\n",
      "epoch no.5 train no.375730  loss = 3.51921 avg_loss = 2.86717\n",
      "epoch no.5 train no.375740  loss = 5.29615 avg_loss = 2.91527\n",
      "epoch no.5 train no.375750  loss = 2.54093 avg_loss = 2.94774\n",
      "epoch no.5 train no.375760  loss = 3.05716 avg_loss = 2.90270\n",
      "epoch no.5 train no.375770  loss = 3.22998 avg_loss = 2.91061\n",
      "epoch no.5 train no.375780  loss = 2.52318 avg_loss = 2.87385\n",
      "epoch no.5 train no.375790  loss = 3.18521 avg_loss = 2.88107\n",
      "epoch no.5 train no.375800  loss = 3.13790 avg_loss = 2.92427\n",
      "epoch no.5 train no.375810  loss = 2.33403 avg_loss = 2.90225\n",
      "epoch no.5 train no.375820  loss = 3.73564 avg_loss = 2.91163\n",
      "epoch no.5 train no.375830  loss = 2.91460 avg_loss = 2.93294\n",
      "epoch no.5 train no.375840  loss = 3.37384 avg_loss = 2.97277\n",
      "epoch no.5 train no.375850  loss = 4.88766 avg_loss = 3.00987\n",
      "epoch no.5 train no.375860  loss = 3.18133 avg_loss = 2.97950\n",
      "epoch no.5 train no.375870  loss = 2.27526 avg_loss = 2.94304\n",
      "epoch no.5 train no.375880  loss = 2.99206 avg_loss = 2.93865\n",
      "epoch no.5 train no.375890  loss = 2.93224 avg_loss = 2.91446\n",
      "epoch no.5 train no.375900  loss = 3.02770 avg_loss = 2.92169\n",
      "epoch no.5 train no.375910  loss = 2.12539 avg_loss = 2.93642\n",
      "epoch no.5 train no.375920  loss = 3.78440 avg_loss = 2.95371\n",
      "epoch no.5 train no.375930  loss = 3.89399 avg_loss = 2.96108\n",
      "epoch no.5 train no.375940  loss = 3.16677 avg_loss = 2.94698\n",
      "epoch no.5 train no.375950  loss = 1.64436 avg_loss = 2.95318\n",
      "epoch no.5 train no.375960  loss = 2.42030 avg_loss = 2.92281\n",
      "epoch no.5 train no.375970  loss = 3.90456 avg_loss = 2.97113\n",
      "epoch no.5 train no.375980  loss = 4.51901 avg_loss = 3.01685\n",
      "epoch no.5 train no.375990  loss = 3.18109 avg_loss = 2.99661\n",
      "epoch no.5 train no.376000  loss = 2.91875 avg_loss = 2.99782\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '▁b', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.376010  loss = 3.31726 avg_loss = 2.96582\n",
      "epoch no.5 train no.376020  loss = 1.99394 avg_loss = 2.91495\n",
      "epoch no.5 train no.376030  loss = 2.76581 avg_loss = 2.92589\n",
      "epoch no.5 train no.376040  loss = 4.14187 avg_loss = 2.94502\n",
      "epoch no.5 train no.376050  loss = 3.57113 avg_loss = 2.96128\n",
      "epoch no.5 train no.376060  loss = 1.86651 avg_loss = 2.91693\n",
      "epoch no.5 train no.376070  loss = 3.98043 avg_loss = 2.93044\n",
      "epoch no.5 train no.376080  loss = 3.29016 avg_loss = 2.93787\n",
      "epoch no.5 train no.376090  loss = 3.68743 avg_loss = 2.96716\n",
      "epoch no.5 train no.376100  loss = 3.15062 avg_loss = 2.95638\n",
      "epoch no.5 train no.376110  loss = 3.53883 avg_loss = 2.92654\n",
      "epoch no.5 train no.376120  loss = 1.67754 avg_loss = 2.91383\n",
      "epoch no.5 train no.376130  loss = 3.70548 avg_loss = 2.93281\n",
      "epoch no.5 train no.376140  loss = 2.20556 avg_loss = 2.95815\n",
      "epoch no.5 train no.376150  loss = 3.14598 avg_loss = 2.96440\n",
      "epoch no.5 train no.376160  loss = 2.59363 avg_loss = 2.94515\n",
      "epoch no.5 train no.376170  loss = 2.25845 avg_loss = 2.91834\n",
      "epoch no.5 train no.376180  loss = 2.88589 avg_loss = 2.90979\n",
      "epoch no.5 train no.376190  loss = 5.25987 avg_loss = 2.96818\n",
      "epoch no.5 train no.376200  loss = 4.40290 avg_loss = 2.96706\n",
      "epoch no.5 train no.376210  loss = 3.19346 avg_loss = 2.95770\n",
      "epoch no.5 train no.376220  loss = 3.38337 avg_loss = 2.95772\n",
      "epoch no.5 train no.376230  loss = 4.15959 avg_loss = 2.93732\n",
      "epoch no.5 train no.376240  loss = 2.20658 avg_loss = 2.89380\n",
      "epoch no.5 train no.376250  loss = 1.94741 avg_loss = 2.89730\n",
      "epoch no.5 train no.376260  loss = 3.85557 avg_loss = 2.90953\n",
      "epoch no.5 train no.376270  loss = 4.35535 avg_loss = 2.90025\n",
      "epoch no.5 train no.376280  loss = 2.30518 avg_loss = 2.89877\n",
      "epoch no.5 train no.376290  loss = 3.64167 avg_loss = 2.88190\n",
      "epoch no.5 train no.376300  loss = 3.33003 avg_loss = 2.91362\n",
      "epoch no.5 train no.376310  loss = 2.10627 avg_loss = 2.88707\n",
      "epoch no.5 train no.376320  loss = 1.66189 avg_loss = 2.87011\n",
      "epoch no.5 train no.376330  loss = 2.94985 avg_loss = 2.90611\n",
      "epoch no.5 train no.376340  loss = 3.60494 avg_loss = 2.88461\n",
      "epoch no.5 train no.376350  loss = 2.88747 avg_loss = 2.87468\n",
      "epoch no.5 train no.376360  loss = 3.58400 avg_loss = 2.92848\n",
      "epoch no.5 train no.376370  loss = 2.73541 avg_loss = 2.95004\n",
      "epoch no.5 train no.376380  loss = 3.45016 avg_loss = 3.00116\n",
      "epoch no.5 train no.376390  loss = 3.73245 avg_loss = 2.96037\n",
      "epoch no.5 train no.376400  loss = 3.82997 avg_loss = 2.99577\n",
      "epoch no.5 train no.376410  loss = 2.46920 avg_loss = 2.96471\n",
      "epoch no.5 train no.376420  loss = 3.21641 avg_loss = 2.98748\n",
      "epoch no.5 train no.376430  loss = 2.68648 avg_loss = 2.97956\n",
      "epoch no.5 train no.376440  loss = 3.49945 avg_loss = 2.97318\n",
      "epoch no.5 train no.376450  loss = 3.95855 avg_loss = 2.96802\n",
      "epoch no.5 train no.376460  loss = 3.18877 avg_loss = 2.93699\n",
      "epoch no.5 train no.376470  loss = 3.94948 avg_loss = 2.96739\n",
      "epoch no.5 train no.376480  loss = 2.48519 avg_loss = 2.95780\n",
      "epoch no.5 train no.376490  loss = 3.72157 avg_loss = 2.96714\n",
      "epoch no.5 train no.376500  loss = 1.85252 avg_loss = 2.95400\n",
      "epoch no.5 train no.376510  loss = 2.54552 avg_loss = 2.94847\n",
      "epoch no.5 train no.376520  loss = 3.75294 avg_loss = 2.92003\n",
      "epoch no.5 train no.376530  loss = 1.89493 avg_loss = 2.91867\n",
      "epoch no.5 train no.376540  loss = 2.87438 avg_loss = 2.89473\n",
      "epoch no.5 train no.376550  loss = 2.20459 avg_loss = 2.89410\n",
      "epoch no.5 train no.376560  loss = 2.57365 avg_loss = 2.92147\n",
      "epoch no.5 train no.376570  loss = 3.03428 avg_loss = 2.91669\n",
      "epoch no.5 train no.376580  loss = 3.88407 avg_loss = 2.89326\n",
      "epoch no.5 train no.376590  loss = 3.30263 avg_loss = 2.92210\n",
      "epoch no.5 train no.376600  loss = 2.48136 avg_loss = 2.93164\n",
      "epoch no.5 train no.376610  loss = 4.80210 avg_loss = 2.93869\n",
      "epoch no.5 train no.376620  loss = 2.12218 avg_loss = 2.95947\n",
      "epoch no.5 train no.376630  loss = 4.17740 avg_loss = 2.99945\n",
      "epoch no.5 train no.376640  loss = 2.40187 avg_loss = 2.96039\n",
      "epoch no.5 train no.376650  loss = 2.60563 avg_loss = 2.99438\n",
      "epoch no.5 train no.376660  loss = 3.11697 avg_loss = 2.98166\n",
      "epoch no.5 train no.376670  loss = 1.89553 avg_loss = 2.98544\n",
      "epoch no.5 train no.376680  loss = 2.75485 avg_loss = 2.99191\n",
      "epoch no.5 train no.376690  loss = 2.83665 avg_loss = 2.96947\n",
      "epoch no.5 train no.376700  loss = 3.59808 avg_loss = 2.95778\n",
      "epoch no.5 train no.376710  loss = 3.04477 avg_loss = 2.93710\n",
      "epoch no.5 train no.376720  loss = 3.75067 avg_loss = 2.97946\n",
      "epoch no.5 train no.376730  loss = 2.61500 avg_loss = 2.96503\n",
      "epoch no.5 train no.376740  loss = 1.80521 avg_loss = 2.93234\n",
      "epoch no.5 train no.376750  loss = 3.19136 avg_loss = 2.94299\n",
      "epoch no.5 train no.376760  loss = 2.75532 avg_loss = 2.91698\n",
      "epoch no.5 train no.376770  loss = 2.76635 avg_loss = 2.96376\n",
      "epoch no.5 train no.376780  loss = 2.42354 avg_loss = 2.96371\n",
      "epoch no.5 train no.376790  loss = 2.12596 avg_loss = 2.94854\n",
      "epoch no.5 train no.376800  loss = 3.09631 avg_loss = 2.92887\n",
      "epoch no.5 train no.376810  loss = 3.31884 avg_loss = 2.94295\n",
      "epoch no.5 train no.376820  loss = 2.69824 avg_loss = 2.94325\n",
      "epoch no.5 train no.376830  loss = 3.20566 avg_loss = 2.93695\n",
      "epoch no.5 train no.376840  loss = 2.84040 avg_loss = 2.93125\n",
      "epoch no.5 train no.376850  loss = 2.36084 avg_loss = 2.95536\n",
      "epoch no.5 train no.376860  loss = 3.62044 avg_loss = 2.96091\n",
      "epoch no.5 train no.376870  loss = 3.06922 avg_loss = 2.95441\n",
      "epoch no.5 train no.376880  loss = 3.12490 avg_loss = 2.98494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.376890  loss = 1.93851 avg_loss = 2.95736\n",
      "epoch no.5 train no.376900  loss = 4.63436 avg_loss = 2.98997\n",
      "epoch no.5 train no.376910  loss = 2.16317 avg_loss = 2.96231\n",
      "epoch no.5 train no.376920  loss = 2.60528 avg_loss = 2.97886\n",
      "epoch no.5 train no.376930  loss = 4.44353 avg_loss = 2.99585\n",
      "epoch no.5 train no.376940  loss = 2.57623 avg_loss = 3.02069\n",
      "epoch no.5 train no.376950  loss = 2.06931 avg_loss = 2.96997\n",
      "epoch no.5 train no.376960  loss = 2.67680 avg_loss = 2.98414\n",
      "epoch no.5 train no.376970  loss = 3.70199 avg_loss = 3.02840\n",
      "epoch no.5 train no.376980  loss = 2.06001 avg_loss = 3.00915\n",
      "epoch no.5 train no.376990  loss = 3.30834 avg_loss = 2.98663\n",
      "epoch no.5 train no.377000  loss = 2.24151 avg_loss = 2.99071\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '곡', '▁발라', '드', '</s>', '</s>']\n",
      "추억의 명곡 발라드 모음</s>\n",
      "epoch no.5 train no.377010  loss = 3.67495 avg_loss = 2.98214\n",
      "epoch no.5 train no.377020  loss = 2.06737 avg_loss = 2.95806\n",
      "epoch no.5 train no.377030  loss = 1.93877 avg_loss = 2.95403\n",
      "epoch no.5 train no.377040  loss = 3.31381 avg_loss = 2.96328\n",
      "epoch no.5 train no.377050  loss = 1.99516 avg_loss = 2.94420\n",
      "epoch no.5 train no.377060  loss = 2.70954 avg_loss = 2.96772\n",
      "epoch no.5 train no.377070  loss = 1.66232 avg_loss = 3.01347\n",
      "epoch no.5 train no.377080  loss = 2.61361 avg_loss = 3.03121\n",
      "epoch no.5 train no.377090  loss = 3.56206 avg_loss = 3.03424\n",
      "epoch no.5 train no.377100  loss = 3.85767 avg_loss = 3.01942\n",
      "epoch no.5 train no.377110  loss = 3.30608 avg_loss = 3.00643\n",
      "epoch no.5 train no.377120  loss = 4.07344 avg_loss = 2.99231\n",
      "epoch no.5 train no.377130  loss = 3.92198 avg_loss = 2.99489\n",
      "epoch no.5 train no.377140  loss = 2.93066 avg_loss = 2.98837\n",
      "epoch no.5 train no.377150  loss = 2.90042 avg_loss = 2.99561\n",
      "epoch no.5 train no.377160  loss = 2.24092 avg_loss = 2.97591\n",
      "epoch no.5 train no.377170  loss = 1.95496 avg_loss = 2.92875\n",
      "epoch no.5 train no.377180  loss = 3.39006 avg_loss = 2.95945\n",
      "epoch no.5 train no.377190  loss = 2.08756 avg_loss = 2.95259\n",
      "epoch no.5 train no.377200  loss = 3.08618 avg_loss = 2.94669\n",
      "epoch no.5 train no.377210  loss = 3.51313 avg_loss = 3.00892\n",
      "epoch no.5 train no.377220  loss = 2.06471 avg_loss = 2.99592\n",
      "epoch no.5 train no.377230  loss = 2.64624 avg_loss = 2.99377\n",
      "epoch no.5 train no.377240  loss = 3.58443 avg_loss = 3.02604\n",
      "epoch no.5 train no.377250  loss = 2.99174 avg_loss = 3.02557\n",
      "epoch no.5 train no.377260  loss = 3.15660 avg_loss = 3.03016\n",
      "epoch no.5 train no.377270  loss = 3.24278 avg_loss = 3.05416\n",
      "epoch no.5 train no.377280  loss = 4.72891 avg_loss = 3.06096\n",
      "epoch no.5 train no.377290  loss = 2.67403 avg_loss = 3.06678\n",
      "epoch no.5 train no.377300  loss = 2.97935 avg_loss = 3.03947\n",
      "epoch no.5 train no.377310  loss = 4.29193 avg_loss = 3.05120\n",
      "epoch no.5 train no.377320  loss = 2.79689 avg_loss = 3.00536\n",
      "epoch no.5 train no.377330  loss = 3.72197 avg_loss = 2.99453\n",
      "epoch no.5 train no.377340  loss = 2.01723 avg_loss = 2.96986\n",
      "epoch no.5 train no.377350  loss = 2.74353 avg_loss = 2.95280\n",
      "epoch no.5 train no.377360  loss = 3.28582 avg_loss = 2.98874\n",
      "epoch no.5 train no.377370  loss = 2.67056 avg_loss = 2.98132\n",
      "epoch no.5 train no.377380  loss = 1.93831 avg_loss = 2.94988\n",
      "epoch no.5 train no.377390  loss = 4.42927 avg_loss = 2.94662\n",
      "epoch no.5 train no.377400  loss = 2.75909 avg_loss = 2.96376\n",
      "epoch no.5 train no.377410  loss = 2.20739 avg_loss = 2.94844\n",
      "epoch no.5 train no.377420  loss = 4.75143 avg_loss = 2.96859\n",
      "epoch no.5 train no.377430  loss = 2.39985 avg_loss = 2.98116\n",
      "epoch no.5 train no.377440  loss = 3.37571 avg_loss = 2.99515\n",
      "epoch no.5 train no.377450  loss = 3.24586 avg_loss = 2.98275\n",
      "epoch no.5 train no.377460  loss = 1.76149 avg_loss = 2.97400\n",
      "epoch no.5 train no.377470  loss = 2.59072 avg_loss = 2.96099\n",
      "epoch no.5 train no.377480  loss = 2.42443 avg_loss = 2.98406\n",
      "epoch no.5 train no.377490  loss = 1.92006 avg_loss = 2.98192\n",
      "epoch no.5 train no.377500  loss = 4.49801 avg_loss = 3.02868\n",
      "epoch no.5 train no.377510  loss = 2.90408 avg_loss = 3.00397\n",
      "epoch no.5 train no.377520  loss = 3.30977 avg_loss = 3.02526\n",
      "epoch no.5 train no.377530  loss = 2.15443 avg_loss = 3.01760\n",
      "epoch no.5 train no.377540  loss = 3.67713 avg_loss = 3.03237\n",
      "epoch no.5 train no.377550  loss = 4.79889 avg_loss = 3.07306\n",
      "epoch no.5 train no.377560  loss = 3.00466 avg_loss = 3.04043\n",
      "epoch no.5 train no.377570  loss = 3.81905 avg_loss = 3.03539\n",
      "epoch no.5 train no.377580  loss = 2.80530 avg_loss = 3.00918\n",
      "epoch no.5 train no.377590  loss = 2.02753 avg_loss = 3.00645\n",
      "epoch no.5 train no.377600  loss = 2.81072 avg_loss = 3.02100\n",
      "epoch no.5 train no.377610  loss = 2.63228 avg_loss = 3.01918\n",
      "epoch no.5 train no.377620  loss = 2.48577 avg_loss = 3.00435\n",
      "epoch no.5 train no.377630  loss = 4.26185 avg_loss = 3.04846\n",
      "epoch no.5 train no.377640  loss = 2.78271 avg_loss = 3.02793\n",
      "epoch no.5 train no.377650  loss = 1.77930 avg_loss = 2.99924\n",
      "epoch no.5 train no.377660  loss = 3.37338 avg_loss = 3.02082\n",
      "epoch no.5 train no.377670  loss = 2.32441 avg_loss = 2.97997\n",
      "epoch no.5 train no.377680  loss = 2.78425 avg_loss = 2.94483\n",
      "epoch no.5 train no.377690  loss = 2.06896 avg_loss = 2.93121\n",
      "epoch no.5 train no.377700  loss = 2.55910 avg_loss = 2.92673\n",
      "epoch no.5 train no.377710  loss = 3.90822 avg_loss = 2.93634\n",
      "epoch no.5 train no.377720  loss = 3.55837 avg_loss = 2.93388\n",
      "epoch no.5 train no.377730  loss = 3.11538 avg_loss = 2.94577\n",
      "epoch no.5 train no.377740  loss = 2.78407 avg_loss = 2.94801\n",
      "epoch no.5 train no.377750  loss = 2.97253 avg_loss = 2.94827\n",
      "epoch no.5 train no.377760  loss = 3.07654 avg_loss = 2.94089\n",
      "epoch no.5 train no.377770  loss = 3.03349 avg_loss = 2.94351\n",
      "epoch no.5 train no.377780  loss = 3.90896 avg_loss = 2.96003\n",
      "epoch no.5 train no.377790  loss = 3.59330 avg_loss = 2.95232\n",
      "epoch no.5 train no.377800  loss = 2.98194 avg_loss = 2.98097\n",
      "epoch no.5 train no.377810  loss = 3.27137 avg_loss = 2.94830\n",
      "epoch no.5 train no.377820  loss = 3.19460 avg_loss = 2.93461\n",
      "epoch no.5 train no.377830  loss = 3.35542 avg_loss = 2.93987\n",
      "epoch no.5 train no.377840  loss = 2.35132 avg_loss = 2.95908\n",
      "epoch no.5 train no.377850  loss = 2.18841 avg_loss = 2.91990\n",
      "epoch no.5 train no.377860  loss = 3.07051 avg_loss = 2.93448\n",
      "epoch no.5 train no.377870  loss = 2.85601 avg_loss = 2.94222\n",
      "epoch no.5 train no.377880  loss = 3.46363 avg_loss = 2.94081\n",
      "epoch no.5 train no.377890  loss = 2.22859 avg_loss = 2.94875\n",
      "epoch no.5 train no.377900  loss = 3.18791 avg_loss = 2.94889\n",
      "epoch no.5 train no.377910  loss = 3.52627 avg_loss = 2.98900\n",
      "epoch no.5 train no.377920  loss = 2.69982 avg_loss = 3.01951\n",
      "epoch no.5 train no.377930  loss = 3.24518 avg_loss = 3.02642\n",
      "epoch no.5 train no.377940  loss = 3.69880 avg_loss = 3.04440\n",
      "epoch no.5 train no.377950  loss = 2.44552 avg_loss = 3.02169\n",
      "epoch no.5 train no.377960  loss = 3.97673 avg_loss = 3.03856\n",
      "epoch no.5 train no.377970  loss = 2.35894 avg_loss = 3.02731\n",
      "epoch no.5 train no.377980  loss = 3.16068 avg_loss = 3.03930\n",
      "epoch no.5 train no.377990  loss = 2.66572 avg_loss = 3.03934\n",
      "epoch no.5 train no.378000  loss = 3.73222 avg_loss = 2.99402\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '년대', '▁노래', '들', '</s>']\n",
      "추억의 90년대 노래들</s>\n",
      "epoch no.5 train no.378010  loss = 2.81598 avg_loss = 3.00954\n",
      "epoch no.5 train no.378020  loss = 2.97539 avg_loss = 2.97799\n",
      "epoch no.5 train no.378030  loss = 2.67412 avg_loss = 2.97004\n",
      "epoch no.5 train no.378040  loss = 2.14750 avg_loss = 2.98819\n",
      "epoch no.5 train no.378050  loss = 3.18327 avg_loss = 2.94881\n",
      "epoch no.5 train no.378060  loss = 3.78669 avg_loss = 2.94892\n",
      "epoch no.5 train no.378070  loss = 2.26198 avg_loss = 2.96648\n",
      "epoch no.5 train no.378080  loss = 2.86871 avg_loss = 2.95966\n",
      "epoch no.5 train no.378090  loss = 2.73058 avg_loss = 2.92032\n",
      "epoch no.5 train no.378100  loss = 1.63478 avg_loss = 2.93820\n",
      "epoch no.5 train no.378110  loss = 2.26345 avg_loss = 2.89939\n",
      "epoch no.5 train no.378120  loss = 3.54103 avg_loss = 2.93825\n",
      "epoch no.5 train no.378130  loss = 3.53696 avg_loss = 2.93618\n",
      "epoch no.5 train no.378140  loss = 3.50086 avg_loss = 2.91799\n",
      "epoch no.5 train no.378150  loss = 3.09012 avg_loss = 2.94834\n",
      "epoch no.5 train no.378160  loss = 2.84762 avg_loss = 2.92939\n",
      "epoch no.5 train no.378170  loss = 1.94017 avg_loss = 2.93735\n",
      "epoch no.5 train no.378180  loss = 2.17385 avg_loss = 2.93184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.378190  loss = 3.51465 avg_loss = 2.94665\n",
      "epoch no.5 train no.378200  loss = 3.39108 avg_loss = 2.94373\n",
      "epoch no.5 train no.378210  loss = 3.14870 avg_loss = 2.92795\n",
      "epoch no.5 train no.378220  loss = 3.07836 avg_loss = 2.94991\n",
      "epoch no.5 train no.378230  loss = 3.98579 avg_loss = 2.95452\n",
      "epoch no.5 train no.378240  loss = 3.58199 avg_loss = 2.95551\n",
      "epoch no.5 train no.378250  loss = 3.76335 avg_loss = 2.98895\n",
      "epoch no.5 train no.378260  loss = 3.12854 avg_loss = 2.99123\n",
      "epoch no.5 train no.378270  loss = 2.78255 avg_loss = 2.98051\n",
      "epoch no.5 train no.378280  loss = 2.13334 avg_loss = 2.98232\n",
      "epoch no.5 train no.378290  loss = 3.97925 avg_loss = 2.99803\n",
      "epoch no.5 train no.378300  loss = 3.57655 avg_loss = 2.95934\n",
      "epoch no.5 train no.378310  loss = 2.96115 avg_loss = 2.97147\n",
      "epoch no.5 train no.378320  loss = 3.58320 avg_loss = 2.95684\n",
      "epoch no.5 train no.378330  loss = 3.36335 avg_loss = 2.95762\n",
      "epoch no.5 train no.378340  loss = 3.86643 avg_loss = 2.94925\n",
      "epoch no.5 train no.378350  loss = 2.99863 avg_loss = 2.96711\n",
      "epoch no.5 train no.378360  loss = 2.27944 avg_loss = 2.95927\n",
      "epoch no.5 train no.378370  loss = 2.11679 avg_loss = 2.94014\n",
      "epoch no.5 train no.378380  loss = 4.10002 avg_loss = 2.99667\n",
      "epoch no.5 train no.378390  loss = 4.27114 avg_loss = 2.99601\n",
      "epoch no.5 train no.378400  loss = 3.67590 avg_loss = 2.99987\n",
      "epoch no.5 train no.378410  loss = 3.11430 avg_loss = 2.95202\n",
      "epoch no.5 train no.378420  loss = 1.42267 avg_loss = 2.96799\n",
      "epoch no.5 train no.378430  loss = 3.18995 avg_loss = 2.95943\n",
      "epoch no.5 train no.378440  loss = 3.46157 avg_loss = 2.98599\n",
      "epoch no.5 train no.378450  loss = 4.00793 avg_loss = 3.01407\n",
      "epoch no.5 train no.378460  loss = 3.20171 avg_loss = 3.00464\n",
      "epoch no.5 train no.378470  loss = 2.33312 avg_loss = 3.00980\n",
      "epoch no.5 train no.378480  loss = 2.46572 avg_loss = 3.04897\n",
      "epoch no.5 train no.378490  loss = 3.46765 avg_loss = 3.07438\n",
      "epoch no.5 train no.378500  loss = 2.25604 avg_loss = 3.04869\n",
      "epoch no.5 train no.378510  loss = 2.49229 avg_loss = 3.06255\n",
      "epoch no.5 train no.378520  loss = 2.79867 avg_loss = 3.04959\n",
      "epoch no.5 train no.378530  loss = 2.06861 avg_loss = 3.07714\n",
      "epoch no.5 train no.378540  loss = 3.18541 avg_loss = 3.07422\n",
      "epoch no.5 train no.378550  loss = 4.56736 avg_loss = 3.10525\n",
      "epoch no.5 train no.378560  loss = 3.00943 avg_loss = 3.08758\n",
      "epoch no.5 train no.378570  loss = 2.63651 avg_loss = 3.05321\n",
      "epoch no.5 train no.378580  loss = 1.87582 avg_loss = 3.03514\n",
      "epoch no.5 train no.378590  loss = 2.50067 avg_loss = 3.05000\n",
      "epoch no.5 train no.378600  loss = 3.06341 avg_loss = 3.01196\n",
      "epoch no.5 train no.378610  loss = 2.03664 avg_loss = 3.00595\n",
      "epoch no.5 train no.378620  loss = 3.30172 avg_loss = 3.04259\n",
      "epoch no.5 train no.378630  loss = 2.36358 avg_loss = 3.00397\n",
      "epoch no.5 train no.378640  loss = 2.25374 avg_loss = 2.99135\n",
      "epoch no.5 train no.378650  loss = 4.08065 avg_loss = 3.04113\n",
      "epoch no.5 train no.378660  loss = 4.11092 avg_loss = 3.02466\n",
      "epoch no.5 train no.378670  loss = 2.19523 avg_loss = 2.99487\n",
      "epoch no.5 train no.378680  loss = 3.11759 avg_loss = 2.97491\n",
      "epoch no.5 train no.378690  loss = 3.26498 avg_loss = 3.00694\n",
      "epoch no.5 train no.378700  loss = 2.37376 avg_loss = 2.99439\n",
      "epoch no.5 train no.378710  loss = 4.11570 avg_loss = 3.00780\n",
      "epoch no.5 train no.378720  loss = 3.17145 avg_loss = 3.02970\n",
      "epoch no.5 train no.378730  loss = 2.86538 avg_loss = 2.99354\n",
      "epoch no.5 train no.378740  loss = 3.00639 avg_loss = 3.02489\n",
      "epoch no.5 train no.378750  loss = 2.63771 avg_loss = 2.99449\n",
      "epoch no.5 train no.378760  loss = 2.70570 avg_loss = 2.95767\n",
      "epoch no.5 train no.378770  loss = 2.47772 avg_loss = 2.92084\n",
      "epoch no.5 train no.378780  loss = 2.12787 avg_loss = 2.92716\n",
      "epoch no.5 train no.378790  loss = 2.01531 avg_loss = 2.86979\n",
      "epoch no.5 train no.378800  loss = 1.73413 avg_loss = 2.88337\n",
      "epoch no.5 train no.378810  loss = 4.43186 avg_loss = 2.91532\n",
      "epoch no.5 train no.378820  loss = 2.53727 avg_loss = 2.88791\n",
      "epoch no.5 train no.378830  loss = 1.78572 avg_loss = 2.87358\n",
      "epoch no.5 train no.378840  loss = 2.16448 avg_loss = 2.88737\n",
      "epoch no.5 train no.378850  loss = 2.97126 avg_loss = 2.89744\n",
      "epoch no.5 train no.378860  loss = 3.58872 avg_loss = 2.91407\n",
      "epoch no.5 train no.378870  loss = 2.50226 avg_loss = 2.90190\n",
      "epoch no.5 train no.378880  loss = 2.28729 avg_loss = 2.87219\n",
      "epoch no.5 train no.378890  loss = 1.89141 avg_loss = 2.86950\n",
      "epoch no.5 train no.378900  loss = 2.60606 avg_loss = 2.89852\n",
      "epoch no.5 train no.378910  loss = 4.51069 avg_loss = 2.89623\n",
      "epoch no.5 train no.378920  loss = 4.05071 avg_loss = 2.89521\n",
      "epoch no.5 train no.378930  loss = 2.11417 avg_loss = 2.94974\n",
      "epoch no.5 train no.378940  loss = 2.80920 avg_loss = 2.93519\n",
      "epoch no.5 train no.378950  loss = 3.36634 avg_loss = 2.96119\n",
      "epoch no.5 train no.378960  loss = 2.42203 avg_loss = 2.90999\n",
      "epoch no.5 train no.378970  loss = 2.82925 avg_loss = 2.88988\n",
      "epoch no.5 train no.378980  loss = 2.90271 avg_loss = 2.92138\n",
      "epoch no.5 train no.378990  loss = 3.93548 avg_loss = 2.92033\n",
      "epoch no.5 train no.379000  loss = 2.10505 avg_loss = 2.94537\n",
      "4\n",
      "to_tokens: ['▁비', '▁노래', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.5 train no.379010  loss = 3.16609 avg_loss = 2.97501\n",
      "epoch no.5 train no.379020  loss = 3.25389 avg_loss = 2.99421\n",
      "epoch no.5 train no.379030  loss = 3.94283 avg_loss = 3.00546\n",
      "epoch no.5 train no.379040  loss = 2.07110 avg_loss = 2.98480\n",
      "epoch no.5 train no.379050  loss = 3.11765 avg_loss = 3.01534\n",
      "epoch no.5 train no.379060  loss = 3.27903 avg_loss = 3.03899\n",
      "epoch no.5 train no.379070  loss = 3.07270 avg_loss = 3.03227\n",
      "epoch no.5 train no.379080  loss = 4.56440 avg_loss = 3.05474\n",
      "epoch no.5 train no.379090  loss = 2.37152 avg_loss = 3.06006\n",
      "epoch no.5 train no.379100  loss = 2.83519 avg_loss = 3.03191\n",
      "epoch no.5 train no.379110  loss = 3.20904 avg_loss = 3.00880\n",
      "epoch no.5 train no.379120  loss = 2.65856 avg_loss = 2.97478\n",
      "epoch no.5 train no.379130  loss = 4.10854 avg_loss = 2.95995\n",
      "epoch no.5 train no.379140  loss = 3.20749 avg_loss = 2.98099\n",
      "epoch no.5 train no.379150  loss = 3.27174 avg_loss = 2.98950\n",
      "epoch no.5 train no.379160  loss = 2.38984 avg_loss = 2.96843\n",
      "epoch no.5 train no.379170  loss = 4.98595 avg_loss = 2.98727\n",
      "epoch no.5 train no.379180  loss = 2.96136 avg_loss = 2.98796\n",
      "epoch no.5 train no.379190  loss = 2.58708 avg_loss = 2.99395\n",
      "epoch no.5 train no.379200  loss = 3.52340 avg_loss = 2.95645\n",
      "epoch no.5 train no.379210  loss = 2.27515 avg_loss = 2.98667\n",
      "epoch no.5 train no.379220  loss = 3.85564 avg_loss = 2.97831\n",
      "epoch no.5 train no.379230  loss = 2.78198 avg_loss = 2.97800\n",
      "epoch no.5 train no.379240  loss = 2.60540 avg_loss = 2.96209\n",
      "epoch no.5 train no.379250  loss = 1.76224 avg_loss = 2.94446\n",
      "epoch no.5 train no.379260  loss = 2.76022 avg_loss = 2.94982\n",
      "epoch no.5 train no.379270  loss = 2.75099 avg_loss = 2.96937\n",
      "epoch no.5 train no.379280  loss = 2.44320 avg_loss = 2.95592\n",
      "epoch no.5 train no.379290  loss = 2.87415 avg_loss = 2.96212\n",
      "epoch no.5 train no.379300  loss = 2.97363 avg_loss = 2.96206\n",
      "epoch no.5 train no.379310  loss = 2.94738 avg_loss = 2.95064\n",
      "epoch no.5 train no.379320  loss = 1.39571 avg_loss = 2.91993\n",
      "epoch no.5 train no.379330  loss = 3.02847 avg_loss = 2.92723\n",
      "epoch no.5 train no.379340  loss = 4.17230 avg_loss = 2.89914\n",
      "epoch no.5 train no.379350  loss = 2.71637 avg_loss = 2.92934\n",
      "epoch no.5 train no.379360  loss = 2.90211 avg_loss = 2.92621\n",
      "epoch no.5 train no.379370  loss = 2.15199 avg_loss = 2.93957\n",
      "epoch no.5 train no.379380  loss = 2.96472 avg_loss = 2.98542\n",
      "epoch no.5 train no.379390  loss = 3.60366 avg_loss = 2.98555\n",
      "epoch no.5 train no.379400  loss = 3.74493 avg_loss = 2.95257\n",
      "epoch no.5 train no.379410  loss = 2.84147 avg_loss = 2.99557\n",
      "epoch no.5 train no.379420  loss = 1.75529 avg_loss = 2.99371\n",
      "epoch no.5 train no.379430  loss = 3.62521 avg_loss = 3.02060\n",
      "epoch no.5 train no.379440  loss = 2.95012 avg_loss = 3.01308\n",
      "epoch no.5 train no.379450  loss = 3.73499 avg_loss = 2.98134\n",
      "epoch no.5 train no.379460  loss = 2.28638 avg_loss = 2.97566\n",
      "epoch no.5 train no.379470  loss = 3.63061 avg_loss = 2.95656\n",
      "epoch no.5 train no.379480  loss = 3.23034 avg_loss = 2.96468\n",
      "epoch no.5 train no.379490  loss = 3.66801 avg_loss = 3.05428\n",
      "epoch no.5 train no.379500  loss = 2.66798 avg_loss = 3.05004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.379510  loss = 2.46471 avg_loss = 3.04499\n",
      "epoch no.5 train no.379520  loss = 2.57034 avg_loss = 3.02594\n",
      "epoch no.5 train no.379530  loss = 2.52930 avg_loss = 3.00999\n",
      "epoch no.5 train no.379540  loss = 2.61253 avg_loss = 2.99661\n",
      "epoch no.5 train no.379550  loss = 2.41210 avg_loss = 2.98617\n",
      "epoch no.5 train no.379560  loss = 4.62565 avg_loss = 2.98597\n",
      "epoch no.5 train no.379570  loss = 2.95922 avg_loss = 3.00679\n",
      "epoch no.5 train no.379580  loss = 2.91934 avg_loss = 3.00996\n",
      "epoch no.5 train no.379590  loss = 3.34707 avg_loss = 2.97976\n",
      "epoch no.5 train no.379600  loss = 2.20874 avg_loss = 2.97153\n",
      "epoch no.5 train no.379610  loss = 2.37680 avg_loss = 2.97976\n",
      "epoch no.5 train no.379620  loss = 3.39688 avg_loss = 2.97088\n",
      "epoch no.5 train no.379630  loss = 4.89644 avg_loss = 3.00533\n",
      "epoch no.5 train no.379640  loss = 3.86471 avg_loss = 3.04030\n",
      "epoch no.5 train no.379650  loss = 2.93422 avg_loss = 3.02798\n",
      "epoch no.5 train no.379660  loss = 4.17396 avg_loss = 3.03450\n",
      "epoch no.5 train no.379670  loss = 1.85686 avg_loss = 3.05320\n",
      "epoch no.5 train no.379680  loss = 3.89199 avg_loss = 3.04026\n",
      "epoch no.5 train no.379690  loss = 3.13798 avg_loss = 3.06633\n",
      "epoch no.5 train no.379700  loss = 3.24977 avg_loss = 3.02911\n",
      "epoch no.5 train no.379710  loss = 3.15840 avg_loss = 3.02163\n",
      "epoch no.5 train no.379720  loss = 2.88852 avg_loss = 3.02916\n",
      "epoch no.5 train no.379730  loss = 4.18970 avg_loss = 3.07749\n",
      "epoch no.5 train no.379740  loss = 3.88007 avg_loss = 3.04272\n",
      "epoch no.5 train no.379750  loss = 2.63390 avg_loss = 3.08490\n",
      "epoch no.5 train no.379760  loss = 1.95887 avg_loss = 3.11392\n",
      "epoch no.5 train no.379770  loss = 2.42930 avg_loss = 3.06819\n",
      "epoch no.5 train no.379780  loss = 2.25566 avg_loss = 3.04714\n",
      "epoch no.5 train no.379790  loss = 2.74596 avg_loss = 3.03617\n",
      "epoch no.5 train no.379800  loss = 1.98719 avg_loss = 3.03662\n",
      "epoch no.5 train no.379810  loss = 2.90716 avg_loss = 3.00350\n",
      "epoch no.5 train no.379820  loss = 2.80276 avg_loss = 2.97970\n",
      "epoch no.5 train no.379830  loss = 3.82089 avg_loss = 2.96704\n",
      "epoch no.5 train no.379840  loss = 5.14311 avg_loss = 2.99826\n",
      "epoch no.5 train no.379850  loss = 2.96905 avg_loss = 2.97542\n",
      "epoch no.5 train no.379860  loss = 4.76055 avg_loss = 2.98367\n",
      "epoch no.5 train no.379870  loss = 5.19055 avg_loss = 3.02790\n",
      "epoch no.5 train no.379880  loss = 2.38886 avg_loss = 3.02603\n",
      "epoch no.5 train no.379890  loss = 3.28096 avg_loss = 2.98179\n",
      "epoch no.5 train no.379900  loss = 4.09907 avg_loss = 3.00701\n",
      "epoch no.5 train no.379910  loss = 3.52699 avg_loss = 2.98442\n",
      "epoch no.5 train no.379920  loss = 3.40324 avg_loss = 2.97843\n",
      "epoch no.5 train no.379930  loss = 1.95450 avg_loss = 2.98984\n",
      "epoch no.5 train no.379940  loss = 2.70221 avg_loss = 3.01659\n",
      "epoch no.5 train no.379950  loss = 2.29033 avg_loss = 3.02799\n",
      "epoch no.5 train no.379960  loss = 2.52948 avg_loss = 3.02906\n",
      "epoch no.5 train no.379970  loss = 2.98401 avg_loss = 3.00669\n",
      "epoch no.5 train no.379980  loss = 3.27236 avg_loss = 2.97776\n",
      "epoch no.5 train no.379990  loss = 3.83719 avg_loss = 2.99946\n",
      "epoch no.5 train no.380000  loss = 2.93960 avg_loss = 2.98111\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 남자 발라드</s>\n",
      "epoch no.5 train no.380010  loss = 3.23672 avg_loss = 3.00145\n",
      "epoch no.5 train no.380020  loss = 2.27144 avg_loss = 2.98518\n",
      "epoch no.5 train no.380030  loss = 3.21327 avg_loss = 3.03636\n",
      "epoch no.5 train no.380040  loss = 4.44063 avg_loss = 3.05158\n",
      "epoch no.5 train no.380050  loss = 3.03895 avg_loss = 3.02494\n",
      "epoch no.5 train no.380060  loss = 1.45858 avg_loss = 3.00232\n",
      "epoch no.5 train no.380070  loss = 2.42289 avg_loss = 2.98415\n",
      "epoch no.5 train no.380080  loss = 2.85852 avg_loss = 2.95388\n",
      "epoch no.5 train no.380090  loss = 2.65539 avg_loss = 2.99411\n",
      "epoch no.5 train no.380100  loss = 4.39423 avg_loss = 2.99754\n",
      "epoch no.5 train no.380110  loss = 3.20910 avg_loss = 3.00255\n",
      "epoch no.5 train no.380120  loss = 5.76037 avg_loss = 3.08594\n",
      "epoch no.5 train no.380130  loss = 3.61453 avg_loss = 3.08211\n",
      "epoch no.5 train no.380140  loss = 2.29009 avg_loss = 3.04091\n",
      "epoch no.5 train no.380150  loss = 3.78843 avg_loss = 3.04820\n",
      "epoch no.5 train no.380160  loss = 3.80530 avg_loss = 3.04511\n",
      "epoch no.5 train no.380170  loss = 2.16439 avg_loss = 3.00750\n",
      "epoch no.5 train no.380180  loss = 2.77998 avg_loss = 3.00110\n",
      "epoch no.5 train no.380190  loss = 3.01258 avg_loss = 2.99941\n",
      "epoch no.5 train no.380200  loss = 2.51049 avg_loss = 2.99441\n",
      "epoch no.5 train no.380210  loss = 4.20169 avg_loss = 2.96699\n",
      "epoch no.5 train no.380220  loss = 1.86847 avg_loss = 2.92093\n",
      "epoch no.5 train no.380230  loss = 2.71076 avg_loss = 2.90287\n",
      "epoch no.5 train no.380240  loss = 2.66573 avg_loss = 2.90810\n",
      "epoch no.5 train no.380250  loss = 2.43551 avg_loss = 2.90292\n",
      "epoch no.5 train no.380260  loss = 3.08752 avg_loss = 2.90821\n",
      "epoch no.5 train no.380270  loss = 2.13810 avg_loss = 2.91079\n",
      "epoch no.5 train no.380280  loss = 4.88005 avg_loss = 2.91578\n",
      "epoch no.5 train no.380290  loss = 3.62772 avg_loss = 2.93498\n",
      "epoch no.5 train no.380300  loss = 2.97421 avg_loss = 2.92818\n",
      "epoch no.5 train no.380310  loss = 3.91295 avg_loss = 2.96515\n",
      "epoch no.5 train no.380320  loss = 2.17265 avg_loss = 2.99272\n",
      "epoch no.5 train no.380330  loss = 2.45658 avg_loss = 2.99352\n",
      "epoch no.5 train no.380340  loss = 3.01368 avg_loss = 2.99869\n",
      "epoch no.5 train no.380350  loss = 2.35982 avg_loss = 2.97125\n",
      "epoch no.5 train no.380360  loss = 2.95960 avg_loss = 2.95086\n",
      "epoch no.5 train no.380370  loss = 3.00531 avg_loss = 2.93199\n",
      "epoch no.5 train no.380380  loss = 2.72089 avg_loss = 2.91674\n",
      "epoch no.5 train no.380390  loss = 5.84818 avg_loss = 2.97840\n",
      "epoch no.5 train no.380400  loss = 2.96200 avg_loss = 2.96632\n",
      "epoch no.5 train no.380410  loss = 2.10059 avg_loss = 2.94740\n",
      "epoch no.5 train no.380420  loss = 1.97526 avg_loss = 2.96445\n",
      "epoch no.5 train no.380430  loss = 2.31160 avg_loss = 2.94287\n",
      "epoch no.5 train no.380440  loss = 3.05753 avg_loss = 2.95718\n",
      "epoch no.5 train no.380450  loss = 2.87125 avg_loss = 2.92694\n",
      "epoch no.5 train no.380460  loss = 2.67824 avg_loss = 2.91501\n",
      "epoch no.5 train no.380470  loss = 2.81432 avg_loss = 2.93688\n",
      "epoch no.5 train no.380480  loss = 3.39292 avg_loss = 2.95794\n",
      "epoch no.5 train no.380490  loss = 3.16993 avg_loss = 2.95523\n",
      "epoch no.5 train no.380500  loss = 3.88088 avg_loss = 2.94823\n",
      "epoch no.5 train no.380510  loss = 2.94256 avg_loss = 2.92978\n",
      "epoch no.5 train no.380520  loss = 3.02434 avg_loss = 2.94695\n",
      "epoch no.5 train no.380530  loss = 2.50246 avg_loss = 2.94436\n",
      "epoch no.5 train no.380540  loss = 3.30946 avg_loss = 2.95394\n",
      "epoch no.5 train no.380550  loss = 2.86309 avg_loss = 2.95578\n",
      "epoch no.5 train no.380560  loss = 2.22184 avg_loss = 2.95789\n",
      "epoch no.5 train no.380570  loss = 2.96766 avg_loss = 2.92281\n",
      "epoch no.5 train no.380580  loss = 2.83368 avg_loss = 2.93185\n",
      "epoch no.5 train no.380590  loss = 2.87381 avg_loss = 2.92444\n",
      "epoch no.5 train no.380600  loss = 4.39222 avg_loss = 2.93178\n",
      "epoch no.5 train no.380610  loss = 4.29100 avg_loss = 2.97749\n",
      "epoch no.5 train no.380620  loss = 3.30674 avg_loss = 3.00390\n",
      "epoch no.5 train no.380630  loss = 4.35808 avg_loss = 3.04914\n",
      "epoch no.5 train no.380640  loss = 2.62197 avg_loss = 3.04737\n",
      "epoch no.5 train no.380650  loss = 2.72884 avg_loss = 3.00496\n",
      "epoch no.5 train no.380660  loss = 4.94591 avg_loss = 3.02853\n",
      "epoch no.5 train no.380670  loss = 2.52953 avg_loss = 3.01471\n",
      "epoch no.5 train no.380680  loss = 2.89714 avg_loss = 3.00428\n",
      "epoch no.5 train no.380690  loss = 2.58383 avg_loss = 2.95498\n",
      "epoch no.5 train no.380700  loss = 2.51337 avg_loss = 2.93517\n",
      "epoch no.5 train no.380710  loss = 3.46599 avg_loss = 2.96854\n",
      "epoch no.5 train no.380720  loss = 2.61491 avg_loss = 3.00395\n",
      "epoch no.5 train no.380730  loss = 3.25604 avg_loss = 2.98960\n",
      "epoch no.5 train no.380740  loss = 2.53638 avg_loss = 2.96309\n",
      "epoch no.5 train no.380750  loss = 2.20184 avg_loss = 2.94417\n",
      "epoch no.5 train no.380760  loss = 4.19342 avg_loss = 2.94440\n",
      "epoch no.5 train no.380770  loss = 2.80432 avg_loss = 2.92335\n",
      "epoch no.5 train no.380780  loss = 1.57356 avg_loss = 2.97102\n",
      "epoch no.5 train no.380790  loss = 1.92269 avg_loss = 2.94050\n",
      "epoch no.5 train no.380800  loss = 3.00324 avg_loss = 2.92969\n",
      "epoch no.5 train no.380810  loss = 3.23428 avg_loss = 2.92843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.380820  loss = 2.90033 avg_loss = 2.92540\n",
      "epoch no.5 train no.380830  loss = 3.45288 avg_loss = 2.92060\n",
      "epoch no.5 train no.380840  loss = 2.49740 avg_loss = 2.89304\n",
      "epoch no.5 train no.380850  loss = 3.01985 avg_loss = 2.88754\n",
      "epoch no.5 train no.380860  loss = 3.01198 avg_loss = 2.92142\n",
      "epoch no.5 train no.380870  loss = 3.27017 avg_loss = 2.92969\n",
      "epoch no.5 train no.380880  loss = 2.56620 avg_loss = 2.91954\n",
      "epoch no.5 train no.380890  loss = 2.72566 avg_loss = 2.90181\n",
      "epoch no.5 train no.380900  loss = 2.79131 avg_loss = 2.89838\n",
      "epoch no.5 train no.380910  loss = 3.45187 avg_loss = 2.94056\n",
      "epoch no.5 train no.380920  loss = 2.15291 avg_loss = 2.90626\n",
      "epoch no.5 train no.380930  loss = 3.61386 avg_loss = 2.91360\n",
      "epoch no.5 train no.380940  loss = 2.84341 avg_loss = 2.91867\n",
      "epoch no.5 train no.380950  loss = 3.10146 avg_loss = 2.95168\n",
      "epoch no.5 train no.380960  loss = 2.60752 avg_loss = 2.95273\n",
      "epoch no.5 train no.380970  loss = 2.15591 avg_loss = 2.95336\n",
      "epoch no.5 train no.380980  loss = 2.71580 avg_loss = 2.92490\n",
      "epoch no.5 train no.380990  loss = 4.35920 avg_loss = 2.93372\n",
      "epoch no.5 train no.381000  loss = 2.99544 avg_loss = 2.92922\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '곡', '▁리메이크', '드', '</s>']\n",
      "추억의 명곡 발라드</s>\n",
      "epoch no.5 train no.381010  loss = 3.28843 avg_loss = 2.95932\n",
      "epoch no.5 train no.381020  loss = 2.01591 avg_loss = 2.99198\n",
      "epoch no.5 train no.381030  loss = 3.10524 avg_loss = 2.99723\n",
      "epoch no.5 train no.381040  loss = 2.77068 avg_loss = 3.01617\n",
      "epoch no.5 train no.381050  loss = 3.08659 avg_loss = 3.04244\n",
      "epoch no.5 train no.381060  loss = 2.12835 avg_loss = 3.03074\n",
      "epoch no.5 train no.381070  loss = 3.89325 avg_loss = 3.03281\n",
      "epoch no.5 train no.381080  loss = 2.54168 avg_loss = 3.05952\n",
      "epoch no.5 train no.381090  loss = 2.04920 avg_loss = 3.04350\n",
      "epoch no.5 train no.381100  loss = 3.79911 avg_loss = 3.02933\n",
      "epoch no.5 train no.381110  loss = 2.37950 avg_loss = 3.04297\n",
      "epoch no.5 train no.381120  loss = 2.33816 avg_loss = 3.03161\n",
      "epoch no.5 train no.381130  loss = 2.82695 avg_loss = 3.05441\n",
      "epoch no.5 train no.381140  loss = 4.99502 avg_loss = 3.04987\n",
      "epoch no.5 train no.381150  loss = 2.90071 avg_loss = 3.03009\n",
      "epoch no.5 train no.381160  loss = 3.50221 avg_loss = 3.07414\n",
      "epoch no.5 train no.381170  loss = 3.04918 avg_loss = 3.08541\n",
      "epoch no.5 train no.381180  loss = 2.52381 avg_loss = 3.04382\n",
      "epoch no.5 train no.381190  loss = 4.11327 avg_loss = 3.03320\n",
      "epoch no.5 train no.381200  loss = 2.62328 avg_loss = 3.00944\n",
      "epoch no.5 train no.381210  loss = 2.98931 avg_loss = 3.01335\n",
      "epoch no.5 train no.381220  loss = 1.58975 avg_loss = 3.00865\n",
      "epoch no.5 train no.381230  loss = 2.91238 avg_loss = 3.01725\n",
      "epoch no.5 train no.381240  loss = 2.90555 avg_loss = 3.03083\n",
      "epoch no.5 train no.381250  loss = 2.30532 avg_loss = 3.04756\n",
      "epoch no.5 train no.381260  loss = 3.88649 avg_loss = 3.02639\n",
      "epoch no.5 train no.381270  loss = 2.53041 avg_loss = 3.02579\n",
      "epoch no.5 train no.381280  loss = 2.98174 avg_loss = 3.04188\n",
      "epoch no.5 train no.381290  loss = 2.74020 avg_loss = 3.04517\n",
      "epoch no.5 train no.381300  loss = 2.82783 avg_loss = 3.05254\n",
      "epoch no.5 train no.381310  loss = 2.19079 avg_loss = 3.01930\n",
      "epoch no.5 train no.381320  loss = 2.44556 avg_loss = 3.00083\n",
      "epoch no.5 train no.381330  loss = 2.59780 avg_loss = 2.98512\n",
      "epoch no.5 train no.381340  loss = 2.55424 avg_loss = 2.95656\n",
      "epoch no.5 train no.381350  loss = 3.89832 avg_loss = 3.04482\n",
      "epoch no.5 train no.381360  loss = 3.93222 avg_loss = 3.03315\n",
      "epoch no.5 train no.381370  loss = 2.89585 avg_loss = 2.98433\n",
      "epoch no.5 train no.381380  loss = 1.74136 avg_loss = 2.98145\n",
      "epoch no.5 train no.381390  loss = 3.23041 avg_loss = 2.97702\n",
      "epoch no.5 train no.381400  loss = 2.78589 avg_loss = 2.99449\n",
      "epoch no.5 train no.381410  loss = 3.66841 avg_loss = 2.99509\n",
      "epoch no.5 train no.381420  loss = 2.36119 avg_loss = 2.98563\n",
      "epoch no.5 train no.381430  loss = 2.56984 avg_loss = 2.97904\n",
      "epoch no.5 train no.381440  loss = 2.20099 avg_loss = 2.94352\n",
      "epoch no.5 train no.381450  loss = 2.90526 avg_loss = 2.96328\n",
      "epoch no.5 train no.381460  loss = 3.21910 avg_loss = 2.94097\n",
      "epoch no.5 train no.381470  loss = 2.84731 avg_loss = 2.92927\n",
      "epoch no.5 train no.381480  loss = 1.68237 avg_loss = 2.95253\n",
      "epoch no.5 train no.381490  loss = 1.28188 avg_loss = 2.90883\n",
      "epoch no.5 train no.381500  loss = 2.51393 avg_loss = 2.92798\n",
      "epoch no.5 train no.381510  loss = 2.85262 avg_loss = 2.96545\n",
      "epoch no.5 train no.381520  loss = 2.90306 avg_loss = 2.94736\n",
      "epoch no.5 train no.381530  loss = 1.92469 avg_loss = 2.93546\n",
      "epoch no.5 train no.381540  loss = 3.02630 avg_loss = 2.94052\n",
      "epoch no.5 train no.381550  loss = 4.02312 avg_loss = 2.94600\n",
      "epoch no.5 train no.381560  loss = 2.48027 avg_loss = 2.95704\n",
      "epoch no.5 train no.381570  loss = 2.75572 avg_loss = 2.94493\n",
      "epoch no.5 train no.381580  loss = 2.14687 avg_loss = 2.93412\n",
      "epoch no.5 train no.381590  loss = 3.97282 avg_loss = 2.95889\n",
      "epoch no.5 train no.381600  loss = 2.32475 avg_loss = 2.97029\n",
      "epoch no.5 train no.381610  loss = 2.64329 avg_loss = 2.95650\n",
      "epoch no.5 train no.381620  loss = 4.34855 avg_loss = 2.98627\n",
      "epoch no.5 train no.381630  loss = 3.16059 avg_loss = 2.99067\n",
      "epoch no.5 train no.381640  loss = 3.92496 avg_loss = 3.02631\n",
      "epoch no.5 train no.381650  loss = 2.78801 avg_loss = 3.02507\n",
      "epoch no.5 train no.381660  loss = 2.23990 avg_loss = 3.00959\n",
      "epoch no.5 train no.381670  loss = 3.50308 avg_loss = 3.04212\n",
      "epoch no.5 train no.381680  loss = 2.32768 avg_loss = 3.01387\n",
      "epoch no.5 train no.381690  loss = 2.03178 avg_loss = 3.02458\n",
      "epoch no.5 train no.381700  loss = 3.06999 avg_loss = 2.98796\n",
      "epoch no.5 train no.381710  loss = 1.85818 avg_loss = 2.95800\n",
      "epoch no.5 train no.381720  loss = 2.24380 avg_loss = 2.93714\n",
      "epoch no.5 train no.381730  loss = 3.13515 avg_loss = 2.90414\n",
      "epoch no.5 train no.381740  loss = 1.57624 avg_loss = 2.85320\n",
      "epoch no.5 train no.381750  loss = 3.06929 avg_loss = 2.88971\n",
      "epoch no.5 train no.381760  loss = 3.16763 avg_loss = 2.88837\n",
      "epoch no.5 train no.381770  loss = 1.29650 avg_loss = 2.87978\n",
      "epoch no.5 train no.381780  loss = 2.97481 avg_loss = 2.87889\n",
      "epoch no.5 train no.381790  loss = 2.79962 avg_loss = 2.88707\n",
      "epoch no.5 train no.381800  loss = 3.25272 avg_loss = 2.89202\n",
      "epoch no.5 train no.381810  loss = 4.26554 avg_loss = 2.90164\n",
      "epoch no.5 train no.381820  loss = 2.05763 avg_loss = 2.90491\n",
      "epoch no.5 train no.381830  loss = 2.17885 avg_loss = 2.89859\n",
      "epoch no.5 train no.381840  loss = 2.46275 avg_loss = 2.89470\n",
      "epoch no.5 train no.381850  loss = 3.20925 avg_loss = 2.89124\n",
      "epoch no.5 train no.381860  loss = 2.88658 avg_loss = 2.91609\n",
      "epoch no.5 train no.381870  loss = 4.29992 avg_loss = 2.92306\n",
      "epoch no.5 train no.381880  loss = 3.38784 avg_loss = 2.92634\n",
      "epoch no.5 train no.381890  loss = 3.26191 avg_loss = 2.93913\n",
      "epoch no.5 train no.381900  loss = 3.16551 avg_loss = 2.96096\n",
      "epoch no.5 train no.381910  loss = 3.35076 avg_loss = 2.91958\n",
      "epoch no.5 train no.381920  loss = 2.60198 avg_loss = 2.90842\n",
      "epoch no.5 train no.381930  loss = 2.63518 avg_loss = 2.93871\n",
      "epoch no.5 train no.381940  loss = 2.28093 avg_loss = 2.93852\n",
      "epoch no.5 train no.381950  loss = 2.84540 avg_loss = 2.89123\n",
      "epoch no.5 train no.381960  loss = 2.58149 avg_loss = 2.90661\n",
      "epoch no.5 train no.381970  loss = 2.19507 avg_loss = 2.92409\n",
      "epoch no.5 train no.381980  loss = 2.93713 avg_loss = 2.90821\n",
      "epoch no.5 train no.381990  loss = 3.66218 avg_loss = 2.90522\n",
      "epoch no.5 train no.382000  loss = 2.47828 avg_loss = 2.92789\n",
      "4\n",
      "to_tokens: ['▁비', '▁팝', 'ld', '▁p', 'op', '</s>']\n",
      "추억의 old pop</s>\n",
      "epoch no.5 train no.382010  loss = 2.49596 avg_loss = 2.91989\n",
      "epoch no.5 train no.382020  loss = 2.81995 avg_loss = 2.94758\n",
      "epoch no.5 train no.382030  loss = 2.53505 avg_loss = 2.92439\n",
      "epoch no.5 train no.382040  loss = 2.89898 avg_loss = 2.91925\n",
      "epoch no.5 train no.382050  loss = 3.79823 avg_loss = 2.95841\n",
      "epoch no.5 train no.382060  loss = 3.52329 avg_loss = 2.95832\n",
      "epoch no.5 train no.382070  loss = 2.82598 avg_loss = 2.95171\n",
      "epoch no.5 train no.382080  loss = 1.95213 avg_loss = 2.94903\n",
      "epoch no.5 train no.382090  loss = 2.82840 avg_loss = 2.96011\n",
      "epoch no.5 train no.382100  loss = 2.82732 avg_loss = 2.95951\n",
      "epoch no.5 train no.382110  loss = 1.66540 avg_loss = 2.96360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.382120  loss = 1.36651 avg_loss = 2.92671\n",
      "epoch no.5 train no.382130  loss = 2.68335 avg_loss = 2.90610\n",
      "epoch no.5 train no.382140  loss = 1.99217 avg_loss = 2.90093\n",
      "epoch no.5 train no.382150  loss = 3.57768 avg_loss = 2.91328\n",
      "epoch no.5 train no.382160  loss = 3.81225 avg_loss = 2.90032\n",
      "epoch no.5 train no.382170  loss = 3.57359 avg_loss = 2.87713\n",
      "epoch no.5 train no.382180  loss = 4.21800 avg_loss = 2.87464\n",
      "epoch no.5 train no.382190  loss = 2.76819 avg_loss = 2.88591\n",
      "epoch no.5 train no.382200  loss = 2.91475 avg_loss = 2.86101\n",
      "epoch no.5 train no.382210  loss = 2.61990 avg_loss = 2.88606\n",
      "epoch no.5 train no.382220  loss = 3.06018 avg_loss = 2.89696\n",
      "epoch no.5 train no.382230  loss = 3.94497 avg_loss = 2.90863\n",
      "epoch no.5 train no.382240  loss = 4.81100 avg_loss = 2.93260\n",
      "epoch no.5 train no.382250  loss = 4.03137 avg_loss = 2.95337\n",
      "epoch no.5 train no.382260  loss = 3.53971 avg_loss = 2.94786\n",
      "epoch no.5 train no.382270  loss = 2.50620 avg_loss = 2.94435\n",
      "epoch no.5 train no.382280  loss = 4.26911 avg_loss = 2.94842\n",
      "epoch no.5 train no.382290  loss = 4.30709 avg_loss = 2.99505\n",
      "epoch no.5 train no.382300  loss = 2.74537 avg_loss = 2.99599\n",
      "epoch no.5 train no.382310  loss = 2.35787 avg_loss = 3.02227\n",
      "epoch no.5 train no.382320  loss = 1.77504 avg_loss = 2.98584\n",
      "epoch no.5 train no.382330  loss = 2.20754 avg_loss = 2.96704\n",
      "epoch no.5 train no.382340  loss = 3.49714 avg_loss = 2.99911\n",
      "epoch no.5 train no.382350  loss = 3.67769 avg_loss = 3.02174\n",
      "epoch no.5 train no.382360  loss = 3.48533 avg_loss = 3.02633\n",
      "epoch no.5 train no.382370  loss = 2.37209 avg_loss = 3.03223\n",
      "epoch no.5 train no.382380  loss = 2.90155 avg_loss = 3.01735\n",
      "epoch no.5 train no.382390  loss = 2.70415 avg_loss = 2.97621\n",
      "epoch no.5 train no.382400  loss = 2.78882 avg_loss = 2.95860\n",
      "epoch no.5 train no.382410  loss = 2.83232 avg_loss = 2.95254\n",
      "epoch no.5 train no.382420  loss = 4.38829 avg_loss = 2.99066\n",
      "epoch no.5 train no.382430  loss = 3.09919 avg_loss = 2.96757\n",
      "epoch no.5 train no.382440  loss = 1.81666 avg_loss = 2.97157\n",
      "epoch no.5 train no.382450  loss = 4.37359 avg_loss = 3.00523\n",
      "epoch no.5 train no.382460  loss = 3.85698 avg_loss = 3.00120\n",
      "epoch no.5 train no.382470  loss = 3.43972 avg_loss = 2.99471\n",
      "epoch no.5 train no.382480  loss = 2.93594 avg_loss = 2.98097\n",
      "epoch no.5 train no.382490  loss = 1.57072 avg_loss = 2.97021\n",
      "epoch no.5 train no.382500  loss = 3.38022 avg_loss = 2.97661\n",
      "epoch no.5 train no.382510  loss = 2.94306 avg_loss = 3.00130\n",
      "epoch no.5 train no.382520  loss = 3.75145 avg_loss = 2.99025\n",
      "epoch no.5 train no.382530  loss = 3.97981 avg_loss = 2.99187\n",
      "epoch no.5 train no.382540  loss = 3.48423 avg_loss = 3.01782\n",
      "epoch no.5 train no.382550  loss = 2.33614 avg_loss = 2.98788\n",
      "epoch no.5 train no.382560  loss = 2.35566 avg_loss = 2.97406\n",
      "epoch no.5 train no.382570  loss = 2.34422 avg_loss = 2.97044\n",
      "epoch no.5 train no.382580  loss = 3.23950 avg_loss = 3.01286\n",
      "epoch no.5 train no.382590  loss = 3.08470 avg_loss = 2.98798\n",
      "epoch no.5 train no.382600  loss = 2.45213 avg_loss = 2.98065\n",
      "epoch no.5 train no.382610  loss = 2.60264 avg_loss = 2.99477\n",
      "epoch no.5 train no.382620  loss = 2.06745 avg_loss = 2.96524\n",
      "epoch no.5 train no.382630  loss = 4.83179 avg_loss = 2.97211\n",
      "epoch no.5 train no.382640  loss = 2.41337 avg_loss = 2.99457\n",
      "epoch no.5 train no.382650  loss = 3.00381 avg_loss = 2.97393\n",
      "epoch no.5 train no.382660  loss = 2.01992 avg_loss = 2.97052\n",
      "epoch no.5 train no.382670  loss = 2.86142 avg_loss = 2.95168\n",
      "epoch no.5 train no.382680  loss = 2.51030 avg_loss = 2.93279\n",
      "epoch no.5 train no.382690  loss = 3.36980 avg_loss = 2.94828\n",
      "epoch no.5 train no.382700  loss = 3.17985 avg_loss = 2.97982\n",
      "epoch no.5 train no.382710  loss = 2.51744 avg_loss = 2.98165\n",
      "epoch no.5 train no.382720  loss = 2.68593 avg_loss = 2.96786\n",
      "epoch no.5 train no.382730  loss = 3.40036 avg_loss = 3.03780\n",
      "epoch no.5 train no.382740  loss = 3.54020 avg_loss = 3.01531\n",
      "epoch no.5 train no.382750  loss = 2.60083 avg_loss = 2.99625\n",
      "epoch no.5 train no.382760  loss = 2.43581 avg_loss = 2.96535\n",
      "epoch no.5 train no.382770  loss = 3.22442 avg_loss = 2.96584\n",
      "epoch no.5 train no.382780  loss = 2.21343 avg_loss = 2.99100\n",
      "epoch no.5 train no.382790  loss = 3.40451 avg_loss = 2.98303\n",
      "epoch no.5 train no.382800  loss = 3.56349 avg_loss = 2.99438\n",
      "epoch no.5 train no.382810  loss = 2.73876 avg_loss = 2.96215\n",
      "epoch no.5 train no.382820  loss = 2.96154 avg_loss = 3.00099\n",
      "epoch no.5 train no.382830  loss = 2.62291 avg_loss = 2.98830\n",
      "epoch no.5 train no.382840  loss = 3.01456 avg_loss = 2.95177\n",
      "epoch no.5 train no.382850  loss = 1.89529 avg_loss = 2.91634\n",
      "epoch no.5 train no.382860  loss = 3.24163 avg_loss = 2.90185\n",
      "epoch no.5 train no.382870  loss = 2.04692 avg_loss = 2.86722\n",
      "epoch no.5 train no.382880  loss = 2.35241 avg_loss = 2.93241\n",
      "epoch no.5 train no.382890  loss = 2.78867 avg_loss = 2.91990\n",
      "epoch no.5 train no.382900  loss = 2.99718 avg_loss = 2.89724\n",
      "epoch no.5 train no.382910  loss = 2.04166 avg_loss = 2.92773\n",
      "epoch no.5 train no.382920  loss = 2.93852 avg_loss = 2.94654\n",
      "epoch no.5 train no.382930  loss = 2.56532 avg_loss = 2.96972\n",
      "epoch no.5 train no.382940  loss = 4.84674 avg_loss = 3.02097\n",
      "epoch no.5 train no.382950  loss = 2.69215 avg_loss = 3.02608\n",
      "epoch no.5 train no.382960  loss = 4.50987 avg_loss = 3.01464\n",
      "epoch no.5 train no.382970  loss = 5.18410 avg_loss = 3.07081\n",
      "epoch no.5 train no.382980  loss = 1.86775 avg_loss = 3.02548\n",
      "epoch no.5 train no.382990  loss = 2.69819 avg_loss = 3.02696\n",
      "epoch no.5 train no.383000  loss = 3.61137 avg_loss = 3.01518\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '곡', '▁발라', '▁노래', '</s>']\n",
      "추억의 명곡 리메이크 노래</s>\n",
      "epoch no.5 train no.383010  loss = 2.87043 avg_loss = 3.01292\n",
      "epoch no.5 train no.383020  loss = 2.65393 avg_loss = 2.97784\n",
      "epoch no.5 train no.383030  loss = 2.48102 avg_loss = 2.96555\n",
      "epoch no.5 train no.383040  loss = 4.63899 avg_loss = 3.00055\n",
      "epoch no.5 train no.383050  loss = 2.52129 avg_loss = 2.99768\n",
      "epoch no.5 train no.383060  loss = 2.31171 avg_loss = 2.97551\n",
      "epoch no.5 train no.383070  loss = 3.47233 avg_loss = 2.99351\n",
      "epoch no.5 train no.383080  loss = 3.74441 avg_loss = 3.00701\n",
      "epoch no.5 train no.383090  loss = 2.53077 avg_loss = 3.01328\n",
      "epoch no.5 train no.383100  loss = 4.54562 avg_loss = 3.01043\n",
      "epoch no.5 train no.383110  loss = 2.40175 avg_loss = 2.96553\n",
      "epoch no.5 train no.383120  loss = 3.18123 avg_loss = 2.96671\n",
      "epoch no.5 train no.383130  loss = 2.27153 avg_loss = 2.96220\n",
      "epoch no.5 train no.383140  loss = 1.96344 avg_loss = 2.95064\n",
      "epoch no.5 train no.383150  loss = 4.38796 avg_loss = 3.00568\n",
      "epoch no.5 train no.383160  loss = 3.91532 avg_loss = 2.96327\n",
      "epoch no.5 train no.383170  loss = 5.65844 avg_loss = 2.99775\n",
      "epoch no.5 train no.383180  loss = 4.19444 avg_loss = 3.04604\n",
      "epoch no.5 train no.383190  loss = 1.74908 avg_loss = 3.07290\n",
      "epoch no.5 train no.383200  loss = 3.12822 avg_loss = 3.05378\n",
      "epoch no.5 train no.383210  loss = 1.86080 avg_loss = 3.02478\n",
      "epoch no.5 train no.383220  loss = 2.71474 avg_loss = 3.03449\n",
      "epoch no.5 train no.383230  loss = 3.40733 avg_loss = 3.03672\n",
      "epoch no.5 train no.383240  loss = 2.75460 avg_loss = 3.03886\n",
      "epoch no.5 train no.383250  loss = 3.18820 avg_loss = 3.06702\n",
      "epoch no.5 train no.383260  loss = 3.02694 avg_loss = 3.04169\n",
      "epoch no.5 train no.383270  loss = 2.16869 avg_loss = 3.05393\n",
      "epoch no.5 train no.383280  loss = 3.20766 avg_loss = 3.07108\n",
      "epoch no.5 train no.383290  loss = 2.21676 avg_loss = 3.03241\n",
      "epoch no.5 train no.383300  loss = 2.13981 avg_loss = 2.98768\n",
      "epoch no.5 train no.383310  loss = 3.26746 avg_loss = 2.97994\n",
      "epoch no.5 train no.383320  loss = 2.48408 avg_loss = 2.91745\n",
      "epoch no.5 train no.383330  loss = 2.80788 avg_loss = 2.93525\n",
      "epoch no.5 train no.383340  loss = 2.97563 avg_loss = 2.97270\n",
      "epoch no.5 train no.383350  loss = 1.89132 avg_loss = 2.98641\n",
      "epoch no.5 train no.383360  loss = 1.56254 avg_loss = 2.95397\n",
      "epoch no.5 train no.383370  loss = 2.55336 avg_loss = 2.96405\n",
      "epoch no.5 train no.383380  loss = 2.67819 avg_loss = 2.97753\n",
      "epoch no.5 train no.383390  loss = 3.75847 avg_loss = 2.96322\n",
      "epoch no.5 train no.383400  loss = 2.02414 avg_loss = 2.94420\n",
      "epoch no.5 train no.383410  loss = 2.26736 avg_loss = 2.92501\n",
      "epoch no.5 train no.383420  loss = 3.29416 avg_loss = 2.92117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.383430  loss = 2.05565 avg_loss = 2.92581\n",
      "epoch no.5 train no.383440  loss = 2.10278 avg_loss = 2.91982\n",
      "epoch no.5 train no.383450  loss = 3.50214 avg_loss = 2.97412\n",
      "epoch no.5 train no.383460  loss = 4.50209 avg_loss = 2.98452\n",
      "epoch no.5 train no.383470  loss = 3.96761 avg_loss = 2.99739\n",
      "epoch no.5 train no.383480  loss = 2.87788 avg_loss = 2.98633\n",
      "epoch no.5 train no.383490  loss = 2.33891 avg_loss = 2.98047\n",
      "epoch no.5 train no.383500  loss = 3.67865 avg_loss = 3.00109\n",
      "epoch no.5 train no.383510  loss = 2.60873 avg_loss = 3.01717\n",
      "epoch no.5 train no.383520  loss = 2.82748 avg_loss = 3.06891\n",
      "epoch no.5 train no.383530  loss = 3.31777 avg_loss = 3.03898\n",
      "epoch no.5 train no.383540  loss = 2.89987 avg_loss = 3.02319\n",
      "epoch no.5 train no.383550  loss = 2.83375 avg_loss = 3.00578\n",
      "epoch no.5 train no.383560  loss = 2.87646 avg_loss = 2.99620\n",
      "epoch no.5 train no.383570  loss = 2.50957 avg_loss = 2.99459\n",
      "epoch no.5 train no.383580  loss = 4.21299 avg_loss = 3.05937\n",
      "epoch no.5 train no.383590  loss = 2.75675 avg_loss = 3.05108\n",
      "epoch no.5 train no.383600  loss = 2.39996 avg_loss = 3.06552\n",
      "epoch no.5 train no.383610  loss = 2.24651 avg_loss = 2.99807\n",
      "epoch no.5 train no.383620  loss = 2.95986 avg_loss = 3.01948\n",
      "epoch no.5 train no.383630  loss = 3.43069 avg_loss = 3.03507\n",
      "epoch no.5 train no.383640  loss = 2.44702 avg_loss = 3.03076\n",
      "epoch no.5 train no.383650  loss = 2.10965 avg_loss = 3.04913\n",
      "epoch no.5 train no.383660  loss = 3.14389 avg_loss = 3.06222\n",
      "epoch no.5 train no.383670  loss = 2.54522 avg_loss = 2.99958\n",
      "epoch no.5 train no.383680  loss = 2.62464 avg_loss = 2.98354\n",
      "epoch no.5 train no.383690  loss = 2.34808 avg_loss = 2.99471\n",
      "epoch no.5 train no.383700  loss = 3.31199 avg_loss = 3.04093\n",
      "epoch no.5 train no.383710  loss = 2.55831 avg_loss = 3.02009\n",
      "epoch no.5 train no.383720  loss = 1.58999 avg_loss = 2.99385\n",
      "epoch no.5 train no.383730  loss = 2.31990 avg_loss = 2.98210\n",
      "epoch no.5 train no.383740  loss = 2.74084 avg_loss = 3.02445\n",
      "epoch no.5 train no.383750  loss = 3.50043 avg_loss = 3.03696\n",
      "epoch no.5 train no.383760  loss = 2.44429 avg_loss = 3.01819\n",
      "epoch no.5 train no.383770  loss = 2.76647 avg_loss = 3.01812\n",
      "epoch no.5 train no.383780  loss = 2.35453 avg_loss = 2.98296\n",
      "epoch no.5 train no.383790  loss = 4.10048 avg_loss = 2.98472\n",
      "epoch no.5 train no.383800  loss = 2.04169 avg_loss = 2.99606\n",
      "epoch no.5 train no.383810  loss = 2.42781 avg_loss = 3.00952\n",
      "epoch no.5 train no.383820  loss = 3.84362 avg_loss = 3.01224\n",
      "epoch no.5 train no.383830  loss = 2.08545 avg_loss = 3.00192\n",
      "epoch no.5 train no.383840  loss = 3.49421 avg_loss = 3.02031\n",
      "epoch no.5 train no.383850  loss = 2.84530 avg_loss = 2.97647\n",
      "epoch no.5 train no.383860  loss = 1.89382 avg_loss = 2.96935\n",
      "epoch no.5 train no.383870  loss = 2.01694 avg_loss = 2.94000\n",
      "epoch no.5 train no.383880  loss = 2.04667 avg_loss = 2.92327\n",
      "epoch no.5 train no.383890  loss = 3.51851 avg_loss = 2.94377\n",
      "epoch no.5 train no.383900  loss = 2.78159 avg_loss = 2.96171\n",
      "epoch no.5 train no.383910  loss = 3.75192 avg_loss = 2.95741\n",
      "epoch no.5 train no.383920  loss = 2.45554 avg_loss = 2.97689\n",
      "epoch no.5 train no.383930  loss = 2.91939 avg_loss = 2.94255\n",
      "epoch no.5 train no.383940  loss = 1.93524 avg_loss = 2.93204\n",
      "epoch no.5 train no.383950  loss = 2.40918 avg_loss = 2.93654\n",
      "epoch no.5 train no.383960  loss = 2.26375 avg_loss = 2.94226\n",
      "epoch no.5 train no.383970  loss = 2.88589 avg_loss = 2.94613\n",
      "epoch no.5 train no.383980  loss = 3.09335 avg_loss = 2.97223\n",
      "epoch no.5 train no.383990  loss = 2.91104 avg_loss = 2.98928\n",
      "epoch no.5 train no.384000  loss = 2.17692 avg_loss = 2.98610\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', 'st', '모', '곡', '</s>']\n",
      "추억의 ost 명곡</s>\n",
      "epoch no.5 train no.384010  loss = 3.77845 avg_loss = 2.95179\n",
      "epoch no.5 train no.384020  loss = 2.77052 avg_loss = 2.95921\n",
      "epoch no.5 train no.384030  loss = 3.25388 avg_loss = 2.96488\n",
      "epoch no.5 train no.384040  loss = 2.00277 avg_loss = 2.89927\n",
      "epoch no.5 train no.384050  loss = 2.28773 avg_loss = 2.93479\n",
      "epoch no.5 train no.384060  loss = 3.40514 avg_loss = 2.95085\n",
      "epoch no.5 train no.384070  loss = 3.23932 avg_loss = 2.93834\n",
      "epoch no.5 train no.384080  loss = 2.13577 avg_loss = 2.94872\n",
      "epoch no.5 train no.384090  loss = 3.53441 avg_loss = 2.91054\n",
      "epoch no.5 train no.384100  loss = 5.20214 avg_loss = 2.94629\n",
      "epoch no.5 train no.384110  loss = 2.92832 avg_loss = 2.94783\n",
      "epoch no.5 train no.384120  loss = 3.57131 avg_loss = 2.94302\n",
      "epoch no.5 train no.384130  loss = 2.86917 avg_loss = 2.92533\n",
      "epoch no.5 train no.384140  loss = 3.48328 avg_loss = 2.92008\n",
      "epoch no.5 train no.384150  loss = 2.68517 avg_loss = 2.94038\n",
      "epoch no.5 train no.384160  loss = 2.55691 avg_loss = 2.94820\n",
      "epoch no.5 train no.384170  loss = 4.93846 avg_loss = 2.95928\n",
      "epoch no.5 train no.384180  loss = 3.86396 avg_loss = 2.97013\n",
      "epoch no.5 train no.384190  loss = 3.65106 avg_loss = 2.97801\n",
      "epoch no.5 train no.384200  loss = 2.59216 avg_loss = 2.97137\n",
      "epoch no.5 train no.384210  loss = 2.69894 avg_loss = 2.98167\n",
      "epoch no.5 train no.384220  loss = 2.52346 avg_loss = 2.96220\n",
      "epoch no.5 train no.384230  loss = 3.33430 avg_loss = 2.99643\n",
      "epoch no.5 train no.384240  loss = 2.68468 avg_loss = 2.97408\n",
      "epoch no.5 train no.384250  loss = 2.10893 avg_loss = 2.95353\n",
      "epoch no.5 train no.384260  loss = 2.33231 avg_loss = 2.96529\n",
      "epoch no.5 train no.384270  loss = 1.82826 avg_loss = 2.96964\n",
      "epoch no.5 train no.384280  loss = 1.72263 avg_loss = 2.97343\n",
      "epoch no.5 train no.384290  loss = 2.96639 avg_loss = 2.97946\n",
      "epoch no.5 train no.384300  loss = 2.17984 avg_loss = 2.98525\n",
      "epoch no.5 train no.384310  loss = 2.85684 avg_loss = 3.00572\n",
      "epoch no.5 train no.384320  loss = 2.82850 avg_loss = 3.00517\n",
      "epoch no.5 train no.384330  loss = 3.12646 avg_loss = 2.95809\n",
      "epoch no.5 train no.384340  loss = 2.85387 avg_loss = 2.96792\n",
      "epoch no.5 train no.384350  loss = 2.40929 avg_loss = 2.96128\n",
      "epoch no.5 train no.384360  loss = 2.54077 avg_loss = 2.96901\n",
      "epoch no.5 train no.384370  loss = 2.35657 avg_loss = 2.97192\n",
      "epoch no.5 train no.384380  loss = 3.18193 avg_loss = 2.97835\n",
      "epoch no.5 train no.384390  loss = 3.95208 avg_loss = 3.04148\n",
      "epoch no.5 train no.384400  loss = 4.00187 avg_loss = 3.06236\n",
      "epoch no.5 train no.384410  loss = 2.27992 avg_loss = 3.05172\n",
      "epoch no.5 train no.384420  loss = 2.80597 avg_loss = 3.05588\n",
      "epoch no.5 train no.384430  loss = 3.05869 avg_loss = 3.03300\n",
      "epoch no.5 train no.384440  loss = 3.47678 avg_loss = 3.04761\n",
      "epoch no.5 train no.384450  loss = 3.55306 avg_loss = 3.02859\n",
      "epoch no.5 train no.384460  loss = 3.13976 avg_loss = 3.00714\n",
      "epoch no.5 train no.384470  loss = 2.58040 avg_loss = 2.96343\n",
      "epoch no.5 train no.384480  loss = 3.51238 avg_loss = 2.97121\n",
      "epoch no.5 train no.384490  loss = 2.58673 avg_loss = 2.95554\n",
      "epoch no.5 train no.384500  loss = 4.19741 avg_loss = 2.97837\n",
      "epoch no.5 train no.384510  loss = 2.66327 avg_loss = 2.92685\n",
      "epoch no.5 train no.384520  loss = 2.81391 avg_loss = 2.92988\n",
      "epoch no.5 train no.384530  loss = 2.94136 avg_loss = 2.98435\n",
      "epoch no.5 train no.384540  loss = 5.56264 avg_loss = 2.97119\n",
      "epoch no.5 train no.384550  loss = 3.60469 avg_loss = 2.98986\n",
      "epoch no.5 train no.384560  loss = 2.67749 avg_loss = 3.01087\n",
      "epoch no.5 train no.384570  loss = 2.53289 avg_loss = 2.98556\n",
      "epoch no.5 train no.384580  loss = 2.28891 avg_loss = 2.99507\n",
      "epoch no.5 train no.384590  loss = 4.67871 avg_loss = 2.99463\n",
      "epoch no.5 train no.384600  loss = 2.77759 avg_loss = 2.97800\n",
      "epoch no.5 train no.384610  loss = 4.01928 avg_loss = 2.98967\n",
      "epoch no.5 train no.384620  loss = 3.91653 avg_loss = 3.00083\n",
      "epoch no.5 train no.384630  loss = 4.28458 avg_loss = 3.03812\n",
      "epoch no.5 train no.384640  loss = 3.10682 avg_loss = 3.04794\n",
      "epoch no.5 train no.384650  loss = 2.67136 avg_loss = 3.03411\n",
      "epoch no.5 train no.384660  loss = 4.01344 avg_loss = 3.01480\n",
      "epoch no.5 train no.384670  loss = 2.46430 avg_loss = 2.98097\n",
      "epoch no.5 train no.384680  loss = 3.53035 avg_loss = 2.98067\n",
      "epoch no.5 train no.384690  loss = 3.40164 avg_loss = 2.97852\n",
      "epoch no.5 train no.384700  loss = 1.72698 avg_loss = 2.95295\n",
      "epoch no.5 train no.384710  loss = 3.23061 avg_loss = 2.94867\n",
      "epoch no.5 train no.384720  loss = 4.92268 avg_loss = 2.96121\n",
      "epoch no.5 train no.384730  loss = 4.04172 avg_loss = 3.01262\n",
      "epoch no.5 train no.384740  loss = 3.01455 avg_loss = 3.02908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.384750  loss = 2.84481 avg_loss = 3.01645\n",
      "epoch no.5 train no.384760  loss = 1.86230 avg_loss = 3.05535\n",
      "epoch no.5 train no.384770  loss = 3.20855 avg_loss = 3.06246\n",
      "epoch no.5 train no.384780  loss = 2.56850 avg_loss = 3.06287\n",
      "epoch no.5 train no.384790  loss = 3.66587 avg_loss = 3.04339\n",
      "epoch no.5 train no.384800  loss = 3.82422 avg_loss = 3.05124\n",
      "epoch no.5 train no.384810  loss = 2.98837 avg_loss = 3.06862\n",
      "epoch no.5 train no.384820  loss = 2.41904 avg_loss = 3.05781\n",
      "epoch no.5 train no.384830  loss = 2.09928 avg_loss = 3.04918\n",
      "epoch no.5 train no.384840  loss = 3.88366 avg_loss = 3.06294\n",
      "epoch no.5 train no.384850  loss = 3.69515 avg_loss = 3.07810\n",
      "epoch no.5 train no.384860  loss = 3.68302 avg_loss = 3.10464\n",
      "epoch no.5 train no.384870  loss = 3.00345 avg_loss = 3.11918\n",
      "epoch no.5 train no.384880  loss = 2.35634 avg_loss = 3.09166\n",
      "epoch no.5 train no.384890  loss = 3.87207 avg_loss = 3.09332\n",
      "epoch no.5 train no.384900  loss = 2.68583 avg_loss = 3.09432\n",
      "epoch no.5 train no.384910  loss = 5.53717 avg_loss = 3.11994\n",
      "epoch no.5 train no.384920  loss = 4.70698 avg_loss = 3.11640\n",
      "epoch no.5 train no.384930  loss = 2.89173 avg_loss = 3.10608\n",
      "epoch no.5 train no.384940  loss = 2.11101 avg_loss = 3.06868\n",
      "epoch no.5 train no.384950  loss = 2.37058 avg_loss = 3.04331\n",
      "epoch no.5 train no.384960  loss = 3.97971 avg_loss = 3.04501\n",
      "epoch no.5 train no.384970  loss = 1.96811 avg_loss = 3.00049\n",
      "epoch no.5 train no.384980  loss = 3.87299 avg_loss = 3.03192\n",
      "epoch no.5 train no.384990  loss = 1.94302 avg_loss = 2.97747\n",
      "epoch no.5 train no.385000  loss = 4.21434 avg_loss = 3.00688\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁노래', '곡', '</s>']\n",
      "추억의 90년대 댄스팝</s>\n",
      "epoch no.5 train no.385010  loss = 5.14275 avg_loss = 3.05931\n",
      "epoch no.5 train no.385020  loss = 3.30583 avg_loss = 3.07533\n",
      "epoch no.5 train no.385030  loss = 3.30698 avg_loss = 3.06705\n",
      "epoch no.5 train no.385040  loss = 2.65783 avg_loss = 3.05536\n",
      "epoch no.5 train no.385050  loss = 4.99366 avg_loss = 3.03380\n",
      "epoch no.5 train no.385060  loss = 3.13581 avg_loss = 3.02135\n",
      "epoch no.5 train no.385070  loss = 1.89840 avg_loss = 2.98917\n",
      "epoch no.5 train no.385080  loss = 3.81096 avg_loss = 2.98009\n",
      "epoch no.5 train no.385090  loss = 2.44470 avg_loss = 2.98655\n",
      "epoch no.5 train no.385100  loss = 2.16651 avg_loss = 2.94118\n",
      "epoch no.5 train no.385110  loss = 2.75280 avg_loss = 2.94830\n",
      "epoch no.5 train no.385120  loss = 3.21433 avg_loss = 2.94056\n",
      "epoch no.5 train no.385130  loss = 2.22242 avg_loss = 2.93646\n",
      "epoch no.5 train no.385140  loss = 2.88003 avg_loss = 2.96336\n",
      "epoch no.5 train no.385150  loss = 4.04717 avg_loss = 2.96647\n",
      "epoch no.5 train no.385160  loss = 2.41825 avg_loss = 2.95664\n",
      "epoch no.5 train no.385170  loss = 3.15249 avg_loss = 2.95251\n",
      "epoch no.5 train no.385180  loss = 4.54266 avg_loss = 2.96869\n",
      "epoch no.5 train no.385190  loss = 3.21023 avg_loss = 3.00722\n",
      "epoch no.5 train no.385200  loss = 2.28814 avg_loss = 3.02223\n",
      "epoch no.5 train no.385210  loss = 4.02815 avg_loss = 3.02052\n",
      "epoch no.5 train no.385220  loss = 3.36345 avg_loss = 3.00668\n",
      "epoch no.5 train no.385230  loss = 2.81082 avg_loss = 3.03655\n",
      "epoch no.5 train no.385240  loss = 2.46390 avg_loss = 3.03158\n",
      "epoch no.5 train no.385250  loss = 2.40047 avg_loss = 3.00474\n",
      "epoch no.5 train no.385260  loss = 4.10374 avg_loss = 3.01261\n",
      "epoch no.5 train no.385270  loss = 2.85460 avg_loss = 3.08357\n",
      "epoch no.5 train no.385280  loss = 3.93480 avg_loss = 3.07459\n",
      "epoch no.5 train no.385290  loss = 3.02035 avg_loss = 3.05902\n",
      "epoch no.5 train no.385300  loss = 1.63252 avg_loss = 3.03671\n",
      "epoch no.5 train no.385310  loss = 1.82417 avg_loss = 2.98989\n",
      "epoch no.5 train no.385320  loss = 3.94573 avg_loss = 3.01124\n",
      "epoch no.5 train no.385330  loss = 2.75410 avg_loss = 2.98341\n",
      "epoch no.5 train no.385340  loss = 2.12083 avg_loss = 2.98290\n",
      "epoch no.5 train no.385350  loss = 3.49285 avg_loss = 3.04971\n",
      "epoch no.5 train no.385360  loss = 2.30792 avg_loss = 3.03189\n",
      "epoch no.5 train no.385370  loss = 5.02705 avg_loss = 3.07205\n",
      "epoch no.5 train no.385380  loss = 3.48135 avg_loss = 3.08819\n",
      "epoch no.5 train no.385390  loss = 3.05988 avg_loss = 3.07875\n",
      "epoch no.5 train no.385400  loss = 2.74266 avg_loss = 3.03139\n",
      "epoch no.5 train no.385410  loss = 2.19262 avg_loss = 3.01159\n",
      "epoch no.5 train no.385420  loss = 3.67429 avg_loss = 2.99195\n",
      "epoch no.5 train no.385430  loss = 2.60505 avg_loss = 2.96094\n",
      "epoch no.5 train no.385440  loss = 2.51571 avg_loss = 2.95359\n",
      "epoch no.5 train no.385450  loss = 2.98318 avg_loss = 2.95850\n",
      "epoch no.5 train no.385460  loss = 3.59059 avg_loss = 2.93832\n",
      "epoch no.5 train no.385470  loss = 2.24156 avg_loss = 2.95697\n",
      "epoch no.5 train no.385480  loss = 2.25274 avg_loss = 2.98604\n",
      "epoch no.5 train no.385490  loss = 1.49565 avg_loss = 2.97157\n",
      "epoch no.5 train no.385500  loss = 3.64422 avg_loss = 3.01206\n",
      "epoch no.5 train no.385510  loss = 2.90327 avg_loss = 3.00174\n",
      "epoch no.5 train no.385520  loss = 2.41017 avg_loss = 3.00078\n",
      "epoch no.5 train no.385530  loss = 2.37632 avg_loss = 3.01402\n",
      "epoch no.5 train no.385540  loss = 1.76045 avg_loss = 3.00795\n",
      "epoch no.5 train no.385550  loss = 2.34921 avg_loss = 2.99896\n",
      "epoch no.5 train no.385560  loss = 2.21597 avg_loss = 2.96828\n",
      "epoch no.5 train no.385570  loss = 3.54197 avg_loss = 2.99539\n",
      "epoch no.5 train no.385580  loss = 2.04350 avg_loss = 2.97706\n",
      "epoch no.5 train no.385590  loss = 3.99958 avg_loss = 2.97270\n",
      "epoch no.5 train no.385600  loss = 2.27961 avg_loss = 2.94018\n",
      "epoch no.5 train no.385610  loss = 3.85191 avg_loss = 2.96812\n",
      "epoch no.5 train no.385620  loss = 2.53265 avg_loss = 2.94052\n",
      "epoch no.5 train no.385630  loss = 3.25137 avg_loss = 2.95070\n",
      "epoch no.5 train no.385640  loss = 2.94300 avg_loss = 2.94651\n",
      "epoch no.5 train no.385650  loss = 3.72798 avg_loss = 2.96446\n",
      "epoch no.5 train no.385660  loss = 3.01034 avg_loss = 2.95296\n",
      "epoch no.5 train no.385670  loss = 3.69245 avg_loss = 2.96215\n",
      "epoch no.5 train no.385680  loss = 3.09299 avg_loss = 2.96438\n",
      "epoch no.5 train no.385690  loss = 3.05437 avg_loss = 2.95261\n",
      "epoch no.5 train no.385700  loss = 5.55177 avg_loss = 3.00105\n",
      "epoch no.5 train no.385710  loss = 2.82355 avg_loss = 2.97012\n",
      "epoch no.5 train no.385720  loss = 2.53559 avg_loss = 2.98191\n",
      "epoch no.5 train no.385730  loss = 3.06173 avg_loss = 3.00264\n",
      "epoch no.5 train no.385740  loss = 2.87164 avg_loss = 2.99391\n",
      "epoch no.5 train no.385750  loss = 2.66273 avg_loss = 3.01033\n",
      "epoch no.5 train no.385760  loss = 2.04187 avg_loss = 2.97835\n",
      "epoch no.5 train no.385770  loss = 3.70837 avg_loss = 3.01417\n",
      "epoch no.5 train no.385780  loss = 2.68385 avg_loss = 2.99437\n",
      "epoch no.5 train no.385790  loss = 3.34684 avg_loss = 2.94678\n",
      "epoch no.5 train no.385800  loss = 3.45166 avg_loss = 2.95576\n",
      "epoch no.5 train no.385810  loss = 3.07635 avg_loss = 2.91629\n",
      "epoch no.5 train no.385820  loss = 2.58379 avg_loss = 2.93373\n",
      "epoch no.5 train no.385830  loss = 3.26368 avg_loss = 2.93539\n",
      "epoch no.5 train no.385840  loss = 3.75891 avg_loss = 2.97831\n",
      "epoch no.5 train no.385850  loss = 2.24577 avg_loss = 2.98678\n",
      "epoch no.5 train no.385860  loss = 1.95503 avg_loss = 2.97469\n",
      "epoch no.5 train no.385870  loss = 2.42620 avg_loss = 2.99046\n",
      "epoch no.5 train no.385880  loss = 2.10885 avg_loss = 3.01744\n",
      "epoch no.5 train no.385890  loss = 5.22808 avg_loss = 3.00807\n",
      "epoch no.5 train no.385900  loss = 2.60177 avg_loss = 3.03352\n",
      "epoch no.5 train no.385910  loss = 3.49664 avg_loss = 2.99636\n",
      "epoch no.5 train no.385920  loss = 2.65714 avg_loss = 3.02385\n",
      "epoch no.5 train no.385930  loss = 3.32400 avg_loss = 3.00496\n",
      "epoch no.5 train no.385940  loss = 3.77168 avg_loss = 2.96670\n",
      "epoch no.5 train no.385950  loss = 3.98868 avg_loss = 2.97586\n",
      "epoch no.5 train no.385960  loss = 2.34248 avg_loss = 3.00424\n",
      "epoch no.5 train no.385970  loss = 2.57413 avg_loss = 2.98692\n",
      "epoch no.5 train no.385980  loss = 2.58857 avg_loss = 2.95885\n",
      "epoch no.5 train no.385990  loss = 2.03691 avg_loss = 2.94319\n",
      "epoch no.5 train no.386000  loss = 3.47588 avg_loss = 2.97576\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.386010  loss = 4.79510 avg_loss = 3.03165\n",
      "epoch no.5 train no.386020  loss = 3.03934 avg_loss = 3.06169\n",
      "epoch no.5 train no.386030  loss = 2.06544 avg_loss = 3.09409\n",
      "epoch no.5 train no.386040  loss = 2.00410 avg_loss = 3.07325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.386050  loss = 4.41397 avg_loss = 3.10404\n",
      "epoch no.5 train no.386060  loss = 2.68178 avg_loss = 3.07460\n",
      "epoch no.5 train no.386070  loss = 3.43951 avg_loss = 3.06348\n",
      "epoch no.5 train no.386080  loss = 2.53797 avg_loss = 3.08232\n",
      "epoch no.5 train no.386090  loss = 4.27860 avg_loss = 3.07500\n",
      "epoch no.5 train no.386100  loss = 2.53189 avg_loss = 3.03559\n",
      "epoch no.5 train no.386110  loss = 2.95183 avg_loss = 3.05138\n",
      "epoch no.5 train no.386120  loss = 2.25856 avg_loss = 3.02848\n",
      "epoch no.5 train no.386130  loss = 3.73147 avg_loss = 3.02240\n",
      "epoch no.5 train no.386140  loss = 2.90282 avg_loss = 3.01048\n",
      "epoch no.5 train no.386150  loss = 2.74148 avg_loss = 2.94643\n",
      "epoch no.5 train no.386160  loss = 3.44232 avg_loss = 2.96757\n",
      "epoch no.5 train no.386170  loss = 1.88327 avg_loss = 2.98907\n",
      "epoch no.5 train no.386180  loss = 2.88507 avg_loss = 3.01161\n",
      "epoch no.5 train no.386190  loss = 2.44373 avg_loss = 3.00611\n",
      "epoch no.5 train no.386200  loss = 1.76968 avg_loss = 2.98771\n",
      "epoch no.5 train no.386210  loss = 2.55820 avg_loss = 2.97168\n",
      "epoch no.5 train no.386220  loss = 3.14603 avg_loss = 2.96891\n",
      "epoch no.5 train no.386230  loss = 2.88336 avg_loss = 2.98699\n",
      "epoch no.5 train no.386240  loss = 2.62448 avg_loss = 2.92671\n",
      "epoch no.5 train no.386250  loss = 2.89470 avg_loss = 2.91310\n",
      "epoch no.5 train no.386260  loss = 4.07996 avg_loss = 2.94200\n",
      "epoch no.5 train no.386270  loss = 2.79793 avg_loss = 2.97945\n",
      "epoch no.5 train no.386280  loss = 2.13678 avg_loss = 2.95138\n",
      "epoch no.5 train no.386290  loss = 2.79320 avg_loss = 2.91864\n",
      "epoch no.5 train no.386300  loss = 1.93181 avg_loss = 2.89425\n",
      "epoch no.5 train no.386310  loss = 2.87942 avg_loss = 2.88978\n",
      "epoch no.5 train no.386320  loss = 3.90879 avg_loss = 2.91382\n",
      "epoch no.5 train no.386330  loss = 3.63283 avg_loss = 2.93287\n",
      "epoch no.5 train no.386340  loss = 2.42799 avg_loss = 2.91397\n",
      "epoch no.5 train no.386350  loss = 3.20803 avg_loss = 2.91411\n",
      "epoch no.5 train no.386360  loss = 3.01689 avg_loss = 2.93169\n",
      "epoch no.5 train no.386370  loss = 3.29118 avg_loss = 2.96150\n",
      "epoch no.5 train no.386380  loss = 2.69469 avg_loss = 2.97044\n",
      "epoch no.5 train no.386390  loss = 3.03163 avg_loss = 2.96961\n",
      "epoch no.5 train no.386400  loss = 2.47095 avg_loss = 2.98493\n",
      "epoch no.5 train no.386410  loss = 3.08930 avg_loss = 2.99777\n",
      "epoch no.5 train no.386420  loss = 3.37736 avg_loss = 2.99776\n",
      "epoch no.5 train no.386430  loss = 2.65107 avg_loss = 3.03657\n",
      "epoch no.5 train no.386440  loss = 3.57255 avg_loss = 3.04200\n",
      "epoch no.5 train no.386450  loss = 2.72412 avg_loss = 3.06940\n",
      "epoch no.5 train no.386460  loss = 2.86409 avg_loss = 3.06753\n",
      "epoch no.5 train no.386470  loss = 3.10128 avg_loss = 3.06307\n",
      "epoch no.5 train no.386480  loss = 2.99466 avg_loss = 3.06948\n",
      "epoch no.5 train no.386490  loss = 2.09552 avg_loss = 3.07570\n",
      "epoch no.5 train no.386500  loss = 2.87852 avg_loss = 3.03938\n",
      "epoch no.5 train no.386510  loss = 3.35019 avg_loss = 3.06009\n",
      "epoch no.5 train no.386520  loss = 2.04944 avg_loss = 3.06192\n",
      "epoch no.5 train no.386530  loss = 3.19226 avg_loss = 3.10537\n",
      "epoch no.5 train no.386540  loss = 3.34115 avg_loss = 3.10916\n",
      "epoch no.5 train no.386550  loss = 3.64746 avg_loss = 3.06800\n",
      "epoch no.5 train no.386560  loss = 2.62137 avg_loss = 3.03662\n",
      "epoch no.5 train no.386570  loss = 3.47145 avg_loss = 3.05349\n",
      "epoch no.5 train no.386580  loss = 2.51219 avg_loss = 3.04391\n",
      "epoch no.5 train no.386590  loss = 2.47259 avg_loss = 3.06596\n",
      "epoch no.5 train no.386600  loss = 3.38967 avg_loss = 3.06900\n",
      "epoch no.5 train no.386610  loss = 2.58487 avg_loss = 3.05509\n",
      "epoch no.5 train no.386620  loss = 3.08116 avg_loss = 3.07609\n",
      "epoch no.5 train no.386630  loss = 4.61663 avg_loss = 3.12030\n",
      "epoch no.5 train no.386640  loss = 4.44099 avg_loss = 3.09802\n",
      "epoch no.5 train no.386650  loss = 2.22548 avg_loss = 3.09046\n",
      "epoch no.5 train no.386660  loss = 3.48563 avg_loss = 3.08881\n",
      "epoch no.5 train no.386670  loss = 5.10887 avg_loss = 3.07787\n",
      "epoch no.5 train no.386680  loss = 3.79195 avg_loss = 3.04753\n",
      "epoch no.5 train no.386690  loss = 2.40327 avg_loss = 3.02645\n",
      "epoch no.5 train no.386700  loss = 1.89160 avg_loss = 2.99812\n",
      "epoch no.5 train no.386710  loss = 3.21908 avg_loss = 2.97292\n",
      "epoch no.5 train no.386720  loss = 3.25068 avg_loss = 2.97334\n",
      "epoch no.5 train no.386730  loss = 3.86274 avg_loss = 2.99319\n",
      "epoch no.5 train no.386740  loss = 3.08974 avg_loss = 2.99431\n",
      "epoch no.5 train no.386750  loss = 3.02437 avg_loss = 2.99430\n",
      "epoch no.5 train no.386760  loss = 3.39691 avg_loss = 3.00945\n",
      "epoch no.5 train no.386770  loss = 4.68837 avg_loss = 3.03338\n",
      "epoch no.5 train no.386780  loss = 2.57617 avg_loss = 3.04682\n",
      "epoch no.5 train no.386790  loss = 2.23601 avg_loss = 3.02918\n",
      "epoch no.5 train no.386800  loss = 2.53615 avg_loss = 3.05093\n",
      "epoch no.5 train no.386810  loss = 3.80646 avg_loss = 3.03527\n",
      "epoch no.5 train no.386820  loss = 2.26684 avg_loss = 3.05232\n",
      "epoch no.5 train no.386830  loss = 3.49395 avg_loss = 3.05281\n",
      "epoch no.5 train no.386840  loss = 3.33901 avg_loss = 3.07812\n",
      "epoch no.5 train no.386850  loss = 3.32979 avg_loss = 3.08147\n",
      "epoch no.5 train no.386860  loss = 1.75561 avg_loss = 3.07370\n",
      "epoch no.5 train no.386870  loss = 2.02826 avg_loss = 3.05369\n",
      "epoch no.5 train no.386880  loss = 3.89039 avg_loss = 3.04601\n",
      "epoch no.5 train no.386890  loss = 1.77911 avg_loss = 3.00877\n",
      "epoch no.5 train no.386900  loss = 2.82619 avg_loss = 3.00144\n",
      "epoch no.5 train no.386910  loss = 3.81016 avg_loss = 3.00822\n",
      "epoch no.5 train no.386920  loss = 2.49414 avg_loss = 2.98715\n",
      "epoch no.5 train no.386930  loss = 3.84491 avg_loss = 2.99143\n",
      "epoch no.5 train no.386940  loss = 3.36813 avg_loss = 2.99455\n",
      "epoch no.5 train no.386950  loss = 3.56668 avg_loss = 2.98197\n",
      "epoch no.5 train no.386960  loss = 3.26937 avg_loss = 2.97666\n",
      "epoch no.5 train no.386970  loss = 1.84444 avg_loss = 2.95687\n",
      "epoch no.5 train no.386980  loss = 3.30590 avg_loss = 3.00876\n",
      "epoch no.5 train no.386990  loss = 3.69056 avg_loss = 3.02785\n",
      "epoch no.5 train no.387000  loss = 2.12295 avg_loss = 3.02221\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.5 train no.387010  loss = 2.94282 avg_loss = 3.00436\n",
      "epoch no.5 train no.387020  loss = 2.63581 avg_loss = 2.97703\n",
      "epoch no.5 train no.387030  loss = 1.87865 avg_loss = 2.92109\n",
      "epoch no.5 train no.387040  loss = 3.21204 avg_loss = 2.91886\n",
      "epoch no.5 train no.387050  loss = 3.94901 avg_loss = 2.92787\n",
      "epoch no.5 train no.387060  loss = 5.00561 avg_loss = 2.92737\n",
      "epoch no.5 train no.387070  loss = 3.05348 avg_loss = 2.94124\n",
      "epoch no.5 train no.387080  loss = 3.26383 avg_loss = 2.96655\n",
      "epoch no.5 train no.387090  loss = 3.81435 avg_loss = 2.95044\n",
      "epoch no.5 train no.387100  loss = 4.06530 avg_loss = 2.94140\n",
      "epoch no.5 train no.387110  loss = 2.93471 avg_loss = 2.92717\n",
      "epoch no.5 train no.387120  loss = 2.60983 avg_loss = 2.90023\n",
      "epoch no.5 train no.387130  loss = 2.48635 avg_loss = 2.90086\n",
      "epoch no.5 train no.387140  loss = 3.24082 avg_loss = 2.92829\n",
      "epoch no.5 train no.387150  loss = 3.09309 avg_loss = 2.92865\n",
      "epoch no.5 train no.387160  loss = 3.15040 avg_loss = 2.87446\n",
      "epoch no.5 train no.387170  loss = 1.52065 avg_loss = 2.88363\n",
      "epoch no.5 train no.387180  loss = 5.41484 avg_loss = 2.90837\n",
      "epoch no.5 train no.387190  loss = 2.19773 avg_loss = 2.91570\n",
      "epoch no.5 train no.387200  loss = 2.47998 avg_loss = 2.94523\n",
      "epoch no.5 train no.387210  loss = 1.74299 avg_loss = 2.93847\n",
      "epoch no.5 train no.387220  loss = 2.61543 avg_loss = 2.94542\n",
      "epoch no.5 train no.387230  loss = 3.48406 avg_loss = 2.94465\n",
      "epoch no.5 train no.387240  loss = 2.94136 avg_loss = 2.94588\n",
      "epoch no.5 train no.387250  loss = 3.00837 avg_loss = 2.94939\n",
      "epoch no.5 train no.387260  loss = 3.86279 avg_loss = 2.96294\n",
      "epoch no.5 train no.387270  loss = 2.56195 avg_loss = 2.92160\n",
      "epoch no.5 train no.387280  loss = 2.70579 avg_loss = 2.89001\n",
      "epoch no.5 train no.387290  loss = 3.78148 avg_loss = 2.87660\n",
      "epoch no.5 train no.387300  loss = 3.27805 avg_loss = 2.82545\n",
      "epoch no.5 train no.387310  loss = 2.86544 avg_loss = 2.88237\n",
      "epoch no.5 train no.387320  loss = 3.26031 avg_loss = 2.90175\n",
      "epoch no.5 train no.387330  loss = 3.14011 avg_loss = 2.87724\n",
      "epoch no.5 train no.387340  loss = 4.41265 avg_loss = 2.91686\n",
      "epoch no.5 train no.387350  loss = 2.30693 avg_loss = 2.93395\n",
      "epoch no.5 train no.387360  loss = 2.78180 avg_loss = 2.92673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.387370  loss = 0.89220 avg_loss = 2.94422\n",
      "epoch no.5 train no.387380  loss = 5.08407 avg_loss = 3.00081\n",
      "epoch no.5 train no.387390  loss = 2.67610 avg_loss = 2.99980\n",
      "epoch no.5 train no.387400  loss = 3.57454 avg_loss = 2.99153\n",
      "epoch no.5 train no.387410  loss = 4.31126 avg_loss = 3.00522\n",
      "epoch no.5 train no.387420  loss = 2.93109 avg_loss = 2.98419\n",
      "epoch no.5 train no.387430  loss = 4.32541 avg_loss = 2.96903\n",
      "epoch no.5 train no.387440  loss = 2.06124 avg_loss = 3.01177\n",
      "epoch no.5 train no.387450  loss = 2.71648 avg_loss = 3.01792\n",
      "epoch no.5 train no.387460  loss = 2.10155 avg_loss = 3.03099\n",
      "epoch no.5 train no.387470  loss = 2.98156 avg_loss = 3.04365\n",
      "epoch no.5 train no.387480  loss = 2.11439 avg_loss = 3.05947\n",
      "epoch no.5 train no.387490  loss = 2.69715 avg_loss = 3.06522\n",
      "epoch no.5 train no.387500  loss = 2.41220 avg_loss = 3.04252\n",
      "epoch no.5 train no.387510  loss = 3.85229 avg_loss = 3.03824\n",
      "epoch no.5 train no.387520  loss = 2.85097 avg_loss = 3.01022\n",
      "epoch no.5 train no.387530  loss = 2.56189 avg_loss = 3.00954\n",
      "epoch no.5 train no.387540  loss = 3.38651 avg_loss = 2.99706\n",
      "epoch no.5 train no.387550  loss = 3.80399 avg_loss = 2.98160\n",
      "epoch no.5 train no.387560  loss = 5.19898 avg_loss = 2.98510\n",
      "epoch no.5 train no.387570  loss = 2.81400 avg_loss = 2.97762\n",
      "epoch no.5 train no.387580  loss = 2.19727 avg_loss = 2.95387\n",
      "epoch no.5 train no.387590  loss = 3.31001 avg_loss = 2.95435\n",
      "epoch no.5 train no.387600  loss = 3.51924 avg_loss = 2.93030\n",
      "epoch no.5 train no.387610  loss = 2.48676 avg_loss = 2.94541\n",
      "epoch no.5 train no.387620  loss = 1.87623 avg_loss = 2.94560\n",
      "epoch no.5 train no.387630  loss = 3.38185 avg_loss = 2.93966\n",
      "epoch no.5 train no.387640  loss = 2.93481 avg_loss = 2.97713\n",
      "epoch no.5 train no.387650  loss = 4.14083 avg_loss = 2.97976\n",
      "epoch no.5 train no.387660  loss = 2.05989 avg_loss = 2.98360\n",
      "epoch no.5 train no.387670  loss = 2.09791 avg_loss = 2.98646\n",
      "epoch no.5 train no.387680  loss = 4.69725 avg_loss = 2.99455\n",
      "epoch no.5 train no.387690  loss = 1.87109 avg_loss = 3.00909\n",
      "epoch no.5 train no.387700  loss = 4.22256 avg_loss = 3.04569\n",
      "epoch no.5 train no.387710  loss = 3.53989 avg_loss = 3.05156\n",
      "epoch no.5 train no.387720  loss = 3.67573 avg_loss = 3.07580\n",
      "epoch no.5 train no.387730  loss = 2.36739 avg_loss = 3.07842\n",
      "epoch no.5 train no.387740  loss = 3.05005 avg_loss = 3.09789\n",
      "epoch no.5 train no.387750  loss = 3.52402 avg_loss = 3.08466\n",
      "epoch no.5 train no.387760  loss = 2.57106 avg_loss = 3.08536\n",
      "epoch no.5 train no.387770  loss = 2.67475 avg_loss = 3.05950\n",
      "epoch no.5 train no.387780  loss = 3.81978 avg_loss = 3.10705\n",
      "epoch no.5 train no.387790  loss = 3.28908 avg_loss = 3.11167\n",
      "epoch no.5 train no.387800  loss = 2.87682 avg_loss = 3.10392\n",
      "epoch no.5 train no.387810  loss = 3.01049 avg_loss = 3.07657\n",
      "epoch no.5 train no.387820  loss = 1.77806 avg_loss = 3.03472\n",
      "epoch no.5 train no.387830  loss = 2.28621 avg_loss = 3.02584\n",
      "epoch no.5 train no.387840  loss = 3.86696 avg_loss = 3.05667\n",
      "epoch no.5 train no.387850  loss = 3.70551 avg_loss = 3.07859\n",
      "epoch no.5 train no.387860  loss = 3.59563 avg_loss = 3.06713\n",
      "epoch no.5 train no.387870  loss = 3.70904 avg_loss = 3.08496\n",
      "epoch no.5 train no.387880  loss = 2.28652 avg_loss = 3.03351\n",
      "epoch no.5 train no.387890  loss = 2.43884 avg_loss = 3.07381\n",
      "epoch no.5 train no.387900  loss = 4.29292 avg_loss = 3.09747\n",
      "epoch no.5 train no.387910  loss = 2.03137 avg_loss = 3.08174\n",
      "epoch no.5 train no.387920  loss = 3.42179 avg_loss = 3.03704\n",
      "epoch no.5 train no.387930  loss = 2.41098 avg_loss = 3.00797\n",
      "epoch no.5 train no.387940  loss = 3.72340 avg_loss = 2.99315\n",
      "epoch no.5 train no.387950  loss = 2.84025 avg_loss = 3.00192\n",
      "epoch no.5 train no.387960  loss = 3.58689 avg_loss = 3.00338\n",
      "epoch no.5 train no.387970  loss = 4.26534 avg_loss = 3.07753\n",
      "epoch no.5 train no.387980  loss = 3.62861 avg_loss = 3.06020\n",
      "epoch no.5 train no.387990  loss = 3.11478 avg_loss = 3.05928\n",
      "epoch no.5 train no.388000  loss = 3.93016 avg_loss = 3.04714\n",
      "4\n",
      "to_tokens: ['▁가을', '▁발라', '드', '▁명', '집', '</s>']\n",
      "추억의 발라드 모음집</s>\n",
      "epoch no.5 train no.388010  loss = 4.15416 avg_loss = 3.04195\n",
      "epoch no.5 train no.388020  loss = 3.08014 avg_loss = 3.05487\n",
      "epoch no.5 train no.388030  loss = 3.08779 avg_loss = 3.08391\n",
      "epoch no.5 train no.388040  loss = 4.01098 avg_loss = 3.07522\n",
      "epoch no.5 train no.388050  loss = 3.32043 avg_loss = 3.07524\n",
      "epoch no.5 train no.388060  loss = 3.36943 avg_loss = 3.08102\n",
      "epoch no.5 train no.388070  loss = 3.62774 avg_loss = 3.10364\n",
      "epoch no.5 train no.388080  loss = 2.84632 avg_loss = 3.09434\n",
      "epoch no.5 train no.388090  loss = 5.06189 avg_loss = 3.12170\n",
      "epoch no.5 train no.388100  loss = 3.48162 avg_loss = 3.09989\n",
      "epoch no.5 train no.388110  loss = 2.62044 avg_loss = 3.07672\n",
      "epoch no.5 train no.388120  loss = 3.62667 avg_loss = 3.09121\n",
      "epoch no.5 train no.388130  loss = 2.65046 avg_loss = 3.07671\n",
      "epoch no.5 train no.388140  loss = 1.90518 avg_loss = 3.01098\n",
      "epoch no.5 train no.388150  loss = 1.81740 avg_loss = 2.99395\n",
      "epoch no.5 train no.388160  loss = 3.15854 avg_loss = 2.99075\n",
      "epoch no.5 train no.388170  loss = 2.89290 avg_loss = 2.97827\n",
      "epoch no.5 train no.388180  loss = 1.91719 avg_loss = 2.95155\n",
      "epoch no.5 train no.388190  loss = 3.25800 avg_loss = 2.95657\n",
      "epoch no.5 train no.388200  loss = 3.37075 avg_loss = 2.94549\n",
      "epoch no.5 train no.388210  loss = 2.36427 avg_loss = 2.93369\n",
      "epoch no.5 train no.388220  loss = 1.76949 avg_loss = 2.91929\n",
      "epoch no.5 train no.388230  loss = 2.52630 avg_loss = 2.97156\n",
      "epoch no.5 train no.388240  loss = 2.93224 avg_loss = 2.97315\n",
      "epoch no.5 train no.388250  loss = 3.10472 avg_loss = 2.95851\n",
      "epoch no.5 train no.388260  loss = 2.81529 avg_loss = 2.96346\n",
      "epoch no.5 train no.388270  loss = 2.67882 avg_loss = 2.96316\n",
      "epoch no.5 train no.388280  loss = 3.29739 avg_loss = 2.99484\n",
      "epoch no.5 train no.388290  loss = 2.43731 avg_loss = 3.01216\n",
      "epoch no.5 train no.388300  loss = 1.97080 avg_loss = 2.97830\n",
      "epoch no.5 train no.388310  loss = 4.07801 avg_loss = 2.97981\n",
      "epoch no.5 train no.388320  loss = 3.06461 avg_loss = 2.95023\n",
      "epoch no.5 train no.388330  loss = 1.37167 avg_loss = 2.95987\n",
      "epoch no.5 train no.388340  loss = 3.22683 avg_loss = 2.98476\n",
      "epoch no.5 train no.388350  loss = 2.37713 avg_loss = 3.00721\n",
      "epoch no.5 train no.388360  loss = 3.67116 avg_loss = 3.02540\n",
      "epoch no.5 train no.388370  loss = 1.69428 avg_loss = 3.01407\n",
      "epoch no.5 train no.388380  loss = 3.47841 avg_loss = 3.03554\n",
      "epoch no.5 train no.388390  loss = 2.69471 avg_loss = 3.02337\n",
      "epoch no.5 train no.388400  loss = 1.60206 avg_loss = 2.98400\n",
      "epoch no.5 train no.388410  loss = 2.27260 avg_loss = 2.97379\n",
      "epoch no.5 train no.388420  loss = 3.30622 avg_loss = 3.00384\n",
      "epoch no.5 train no.388430  loss = 3.60257 avg_loss = 3.02451\n",
      "epoch no.5 train no.388440  loss = 3.84373 avg_loss = 3.02406\n",
      "epoch no.5 train no.388450  loss = 2.43960 avg_loss = 3.02162\n",
      "epoch no.5 train no.388460  loss = 2.83551 avg_loss = 2.99938\n",
      "epoch no.5 train no.388470  loss = 1.94141 avg_loss = 3.00625\n",
      "epoch no.5 train no.388480  loss = 2.43691 avg_loss = 2.94350\n",
      "epoch no.5 train no.388490  loss = 3.34945 avg_loss = 2.97100\n",
      "epoch no.5 train no.388500  loss = 3.71333 avg_loss = 2.97403\n",
      "epoch no.5 train no.388510  loss = 2.77136 avg_loss = 2.97438\n",
      "epoch no.5 train no.388520  loss = 2.27048 avg_loss = 2.95950\n",
      "epoch no.5 train no.388530  loss = 4.97283 avg_loss = 2.97401\n",
      "epoch no.5 train no.388540  loss = 3.18931 avg_loss = 2.96982\n",
      "epoch no.5 train no.388550  loss = 2.68735 avg_loss = 2.98230\n",
      "epoch no.5 train no.388560  loss = 1.61004 avg_loss = 2.99441\n",
      "epoch no.5 train no.388570  loss = 3.79958 avg_loss = 3.01449\n",
      "epoch no.5 train no.388580  loss = 3.05476 avg_loss = 3.05954\n",
      "epoch no.5 train no.388590  loss = 2.86022 avg_loss = 3.01921\n",
      "epoch no.5 train no.388600  loss = 2.18760 avg_loss = 2.99456\n",
      "epoch no.5 train no.388610  loss = 3.73269 avg_loss = 3.00491\n",
      "epoch no.5 train no.388620  loss = 4.06501 avg_loss = 2.96118\n",
      "epoch no.5 train no.388630  loss = 2.67260 avg_loss = 2.96166\n",
      "epoch no.5 train no.388640  loss = 2.64569 avg_loss = 2.97438\n",
      "epoch no.5 train no.388650  loss = 2.84985 avg_loss = 2.95950\n",
      "epoch no.5 train no.388660  loss = 2.61422 avg_loss = 2.93172\n",
      "epoch no.5 train no.388670  loss = 2.81919 avg_loss = 2.93717\n",
      "epoch no.5 train no.388680  loss = 4.44554 avg_loss = 2.95399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.388690  loss = 2.27843 avg_loss = 2.91569\n",
      "epoch no.5 train no.388700  loss = 2.18729 avg_loss = 2.92730\n",
      "epoch no.5 train no.388710  loss = 3.85055 avg_loss = 2.95207\n",
      "epoch no.5 train no.388720  loss = 3.38082 avg_loss = 2.97818\n",
      "epoch no.5 train no.388730  loss = 3.35160 avg_loss = 2.99651\n",
      "epoch no.5 train no.388740  loss = 2.60634 avg_loss = 2.97010\n",
      "epoch no.5 train no.388750  loss = 1.88073 avg_loss = 2.94038\n",
      "epoch no.5 train no.388760  loss = 3.19075 avg_loss = 2.95167\n",
      "epoch no.5 train no.388770  loss = 2.58151 avg_loss = 2.95700\n",
      "epoch no.5 train no.388780  loss = 4.11060 avg_loss = 2.98383\n",
      "epoch no.5 train no.388790  loss = 3.22411 avg_loss = 2.98028\n",
      "epoch no.5 train no.388800  loss = 3.06877 avg_loss = 2.93747\n",
      "epoch no.5 train no.388810  loss = 0.90292 avg_loss = 2.91731\n",
      "epoch no.5 train no.388820  loss = 2.66459 avg_loss = 2.92306\n",
      "epoch no.5 train no.388830  loss = 3.78146 avg_loss = 2.94746\n",
      "epoch no.5 train no.388840  loss = 1.97026 avg_loss = 2.95401\n",
      "epoch no.5 train no.388850  loss = 3.48064 avg_loss = 2.92102\n",
      "epoch no.5 train no.388860  loss = 2.82554 avg_loss = 2.89009\n",
      "epoch no.5 train no.388870  loss = 3.92176 avg_loss = 2.90717\n",
      "epoch no.5 train no.388880  loss = 2.70889 avg_loss = 2.93600\n",
      "epoch no.5 train no.388890  loss = 4.54510 avg_loss = 2.92919\n",
      "epoch no.5 train no.388900  loss = 3.48372 avg_loss = 2.95790\n",
      "epoch no.5 train no.388910  loss = 3.14998 avg_loss = 2.97875\n",
      "epoch no.5 train no.388920  loss = 3.23640 avg_loss = 3.01814\n",
      "epoch no.5 train no.388930  loss = 2.01932 avg_loss = 3.02022\n",
      "epoch no.5 train no.388940  loss = 4.07072 avg_loss = 3.01854\n",
      "epoch no.5 train no.388950  loss = 4.79421 avg_loss = 3.05722\n",
      "epoch no.5 train no.388960  loss = 4.02839 avg_loss = 3.10223\n",
      "epoch no.5 train no.388970  loss = 2.63005 avg_loss = 3.07380\n",
      "epoch no.5 train no.388980  loss = 3.20971 avg_loss = 3.01864\n",
      "epoch no.5 train no.388990  loss = 3.30374 avg_loss = 2.99997\n",
      "epoch no.5 train no.389000  loss = 3.74592 avg_loss = 2.98428\n",
      "6\n",
      "to_tokens: ['▁비', '▁팝', '송', '▁모음', '▁함께', '▁하는', '▁추억', '밤']\n",
      "추억의 팝송과 함께 하는 가을</s>\n",
      "epoch no.5 train no.389010  loss = 3.88988 avg_loss = 2.99877\n",
      "epoch no.5 train no.389020  loss = 2.61589 avg_loss = 3.00462\n",
      "epoch no.5 train no.389030  loss = 2.98372 avg_loss = 3.00250\n",
      "epoch no.5 train no.389040  loss = 3.55271 avg_loss = 3.04357\n",
      "epoch no.5 train no.389050  loss = 2.26127 avg_loss = 2.98617\n",
      "epoch no.5 train no.389060  loss = 2.98993 avg_loss = 2.96165\n",
      "epoch no.5 train no.389070  loss = 3.24810 avg_loss = 2.97964\n",
      "epoch no.5 train no.389080  loss = 2.96567 avg_loss = 3.00087\n",
      "epoch no.5 train no.389090  loss = 4.94123 avg_loss = 3.03255\n",
      "epoch no.5 train no.389100  loss = 3.12578 avg_loss = 3.02954\n",
      "epoch no.5 train no.389110  loss = 4.38084 avg_loss = 3.03627\n",
      "epoch no.5 train no.389120  loss = 4.12022 avg_loss = 3.06461\n",
      "epoch no.5 train no.389130  loss = 2.73384 avg_loss = 3.04273\n",
      "epoch no.5 train no.389140  loss = 3.34645 avg_loss = 3.03313\n",
      "epoch no.5 train no.389150  loss = 2.88323 avg_loss = 3.02345\n",
      "epoch no.5 train no.389160  loss = 5.08177 avg_loss = 3.04816\n",
      "epoch no.5 train no.389170  loss = 3.28354 avg_loss = 3.03411\n",
      "epoch no.5 train no.389180  loss = 2.85728 avg_loss = 3.00907\n",
      "epoch no.5 train no.389190  loss = 2.99097 avg_loss = 3.01760\n",
      "epoch no.5 train no.389200  loss = 3.75337 avg_loss = 3.03532\n",
      "epoch no.5 train no.389210  loss = 1.68702 avg_loss = 3.04253\n",
      "epoch no.5 train no.389220  loss = 1.86486 avg_loss = 2.98065\n",
      "epoch no.5 train no.389230  loss = 3.54808 avg_loss = 2.92511\n",
      "epoch no.5 train no.389240  loss = 3.39559 avg_loss = 2.94914\n",
      "epoch no.5 train no.389250  loss = 2.73919 avg_loss = 2.96724\n",
      "epoch no.5 train no.389260  loss = 2.82391 avg_loss = 2.94817\n",
      "epoch no.5 train no.389270  loss = 3.34853 avg_loss = 2.95313\n",
      "epoch no.5 train no.389280  loss = 3.89566 avg_loss = 2.95846\n",
      "epoch no.5 train no.389290  loss = 3.48513 avg_loss = 2.94530\n",
      "epoch no.5 train no.389300  loss = 2.37804 avg_loss = 2.93865\n",
      "epoch no.5 train no.389310  loss = 4.85771 avg_loss = 2.97388\n",
      "epoch no.5 train no.389320  loss = 2.13504 avg_loss = 2.94956\n",
      "epoch no.5 train no.389330  loss = 3.30735 avg_loss = 2.95706\n",
      "epoch no.5 train no.389340  loss = 2.61782 avg_loss = 2.96003\n",
      "epoch no.5 train no.389350  loss = 2.86172 avg_loss = 2.94317\n",
      "epoch no.5 train no.389360  loss = 3.57029 avg_loss = 2.94083\n",
      "epoch no.5 train no.389370  loss = 2.68267 avg_loss = 2.93611\n",
      "epoch no.5 train no.389380  loss = 2.46387 avg_loss = 2.92085\n",
      "epoch no.5 train no.389390  loss = 3.05837 avg_loss = 2.91606\n",
      "epoch no.5 train no.389400  loss = 2.82985 avg_loss = 2.90687\n",
      "epoch no.5 train no.389410  loss = 4.02748 avg_loss = 2.96172\n",
      "epoch no.5 train no.389420  loss = 2.76959 avg_loss = 2.94620\n",
      "epoch no.5 train no.389430  loss = 2.68924 avg_loss = 2.94119\n",
      "epoch no.5 train no.389440  loss = 2.30576 avg_loss = 2.92072\n",
      "epoch no.5 train no.389450  loss = 3.58981 avg_loss = 2.97192\n",
      "epoch no.5 train no.389460  loss = 2.38385 avg_loss = 2.94391\n",
      "epoch no.5 train no.389470  loss = 4.11233 avg_loss = 2.92968\n",
      "epoch no.5 train no.389480  loss = 3.84249 avg_loss = 2.95627\n",
      "epoch no.5 train no.389490  loss = 1.98836 avg_loss = 2.97117\n",
      "epoch no.5 train no.389500  loss = 3.52327 avg_loss = 2.99123\n",
      "epoch no.5 train no.389510  loss = 3.52817 avg_loss = 2.98957\n",
      "epoch no.5 train no.389520  loss = 1.77398 avg_loss = 2.99325\n",
      "epoch no.5 train no.389530  loss = 2.67306 avg_loss = 2.99333\n",
      "epoch no.5 train no.389540  loss = 1.83170 avg_loss = 2.98691\n",
      "epoch no.5 train no.389550  loss = 3.56351 avg_loss = 3.00890\n",
      "epoch no.5 train no.389560  loss = 2.35729 avg_loss = 2.98473\n",
      "epoch no.5 train no.389570  loss = 3.13174 avg_loss = 3.01642\n",
      "epoch no.5 train no.389580  loss = 1.97158 avg_loss = 2.99704\n",
      "epoch no.5 train no.389590  loss = 2.31245 avg_loss = 2.98644\n",
      "epoch no.5 train no.389600  loss = 1.89746 avg_loss = 2.94602\n",
      "epoch no.5 train no.389610  loss = 1.25664 avg_loss = 2.91444\n",
      "epoch no.5 train no.389620  loss = 2.69905 avg_loss = 2.95471\n",
      "epoch no.5 train no.389630  loss = 3.33300 avg_loss = 2.94283\n",
      "epoch no.5 train no.389640  loss = 2.18483 avg_loss = 2.90218\n",
      "epoch no.5 train no.389650  loss = 2.60749 avg_loss = 2.90188\n",
      "epoch no.5 train no.389660  loss = 3.64597 avg_loss = 2.91881\n",
      "epoch no.5 train no.389670  loss = 3.46580 avg_loss = 2.96125\n",
      "epoch no.5 train no.389680  loss = 3.57179 avg_loss = 2.99970\n",
      "epoch no.5 train no.389690  loss = 2.56684 avg_loss = 3.04273\n",
      "epoch no.5 train no.389700  loss = 2.16863 avg_loss = 3.02079\n",
      "epoch no.5 train no.389710  loss = 2.34246 avg_loss = 3.02612\n",
      "epoch no.5 train no.389720  loss = 3.60400 avg_loss = 3.01773\n",
      "epoch no.5 train no.389730  loss = 3.19998 avg_loss = 3.02074\n",
      "epoch no.5 train no.389740  loss = 4.44309 avg_loss = 3.02243\n",
      "epoch no.5 train no.389750  loss = 4.20697 avg_loss = 3.02451\n",
      "epoch no.5 train no.389760  loss = 2.00534 avg_loss = 3.01708\n",
      "epoch no.5 train no.389770  loss = 3.69056 avg_loss = 3.08995\n",
      "epoch no.5 train no.389780  loss = 1.42419 avg_loss = 3.07487\n",
      "epoch no.5 train no.389790  loss = 3.24790 avg_loss = 3.07856\n",
      "epoch no.5 train no.389800  loss = 1.54944 avg_loss = 3.06507\n",
      "epoch no.5 train no.389810  loss = 2.80090 avg_loss = 3.01899\n",
      "epoch no.5 train no.389820  loss = 1.81106 avg_loss = 2.98930\n",
      "epoch no.5 train no.389830  loss = 5.01070 avg_loss = 3.03547\n",
      "epoch no.5 train no.389840  loss = 4.80967 avg_loss = 3.03503\n",
      "epoch no.5 train no.389850  loss = 3.12244 avg_loss = 3.04523\n",
      "epoch no.5 train no.389860  loss = 3.28871 avg_loss = 3.02133\n",
      "epoch no.5 train no.389870  loss = 3.16354 avg_loss = 3.01408\n",
      "epoch no.5 train no.389880  loss = 3.18868 avg_loss = 3.02003\n",
      "epoch no.5 train no.389890  loss = 2.39900 avg_loss = 3.00604\n",
      "epoch no.5 train no.389900  loss = 3.91533 avg_loss = 3.01952\n",
      "epoch no.5 train no.389910  loss = 2.45968 avg_loss = 3.01108\n",
      "epoch no.5 train no.389920  loss = 3.17529 avg_loss = 3.03121\n",
      "epoch no.5 train no.389930  loss = 2.80616 avg_loss = 3.02502\n",
      "epoch no.5 train no.389940  loss = 2.20390 avg_loss = 3.01907\n",
      "epoch no.5 train no.389950  loss = 2.56689 avg_loss = 2.98971\n",
      "epoch no.5 train no.389960  loss = 2.43553 avg_loss = 2.97001\n",
      "epoch no.5 train no.389970  loss = 2.03738 avg_loss = 2.95950\n",
      "epoch no.5 train no.389980  loss = 3.59261 avg_loss = 3.02389\n",
      "epoch no.5 train no.389990  loss = 2.18633 avg_loss = 3.05878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.390000  loss = 2.31371 avg_loss = 3.09132\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.5 train no.390010  loss = 3.82523 avg_loss = 3.10181\n",
      "epoch no.5 train no.390020  loss = 3.50144 avg_loss = 3.05866\n",
      "epoch no.5 train no.390030  loss = 2.90528 avg_loss = 3.08996\n",
      "epoch no.5 train no.390040  loss = 3.41038 avg_loss = 3.12167\n",
      "epoch no.5 train no.390050  loss = 3.00435 avg_loss = 3.12238\n",
      "epoch no.5 train no.390060  loss = 2.39225 avg_loss = 3.10809\n",
      "epoch no.5 train no.390070  loss = 1.98081 avg_loss = 3.08924\n",
      "epoch no.5 train no.390080  loss = 4.35821 avg_loss = 3.07143\n",
      "epoch no.5 train no.390090  loss = 3.45586 avg_loss = 3.02139\n",
      "epoch no.5 train no.390100  loss = 2.47929 avg_loss = 3.01734\n",
      "epoch no.5 train no.390110  loss = 1.50245 avg_loss = 3.02361\n",
      "epoch no.5 train no.390120  loss = 3.97214 avg_loss = 3.00831\n",
      "epoch no.5 train no.390130  loss = 2.68182 avg_loss = 2.99050\n",
      "epoch no.5 train no.390140  loss = 3.18950 avg_loss = 2.98784\n",
      "epoch no.5 train no.390150  loss = 2.48671 avg_loss = 2.98251\n",
      "epoch no.5 train no.390160  loss = 1.93158 avg_loss = 2.98364\n",
      "epoch no.5 train no.390170  loss = 2.22072 avg_loss = 2.97451\n",
      "epoch no.5 train no.390180  loss = 3.17616 avg_loss = 2.98855\n",
      "epoch no.5 train no.390190  loss = 2.93763 avg_loss = 2.97580\n",
      "epoch no.5 train no.390200  loss = 3.41915 avg_loss = 2.97704\n",
      "epoch no.5 train no.390210  loss = 3.91426 avg_loss = 2.96935\n",
      "epoch no.5 train no.390220  loss = 3.09171 avg_loss = 3.00822\n",
      "epoch no.5 train no.390230  loss = 3.07641 avg_loss = 3.02435\n",
      "epoch no.5 train no.390240  loss = 3.11615 avg_loss = 2.99226\n",
      "epoch no.5 train no.390250  loss = 2.31041 avg_loss = 3.01625\n",
      "epoch no.5 train no.390260  loss = 3.63746 avg_loss = 3.00186\n",
      "epoch no.5 train no.390270  loss = 4.46115 avg_loss = 3.02430\n",
      "epoch no.5 train no.390280  loss = 3.30027 avg_loss = 3.01766\n",
      "epoch no.5 train no.390290  loss = 2.08248 avg_loss = 3.04267\n",
      "epoch no.5 train no.390300  loss = 3.40882 avg_loss = 3.02592\n",
      "epoch no.5 train no.390310  loss = 3.13961 avg_loss = 3.04346\n",
      "epoch no.5 train no.390320  loss = 3.44163 avg_loss = 3.10278\n",
      "epoch no.5 train no.390330  loss = 3.65791 avg_loss = 3.10928\n",
      "epoch no.5 train no.390340  loss = 2.90778 avg_loss = 3.12828\n",
      "epoch no.5 train no.390350  loss = 4.22271 avg_loss = 3.12990\n",
      "epoch no.5 train no.390360  loss = 2.47884 avg_loss = 3.13068\n",
      "epoch no.5 train no.390370  loss = 2.93252 avg_loss = 3.10076\n",
      "epoch no.5 train no.390380  loss = 1.75231 avg_loss = 3.04407\n",
      "epoch no.5 train no.390390  loss = 1.63445 avg_loss = 2.99155\n",
      "epoch no.5 train no.390400  loss = 2.68631 avg_loss = 3.00423\n",
      "epoch no.5 train no.390410  loss = 3.16016 avg_loss = 3.00614\n",
      "epoch no.5 train no.390420  loss = 1.97946 avg_loss = 2.96736\n",
      "epoch no.5 train no.390430  loss = 3.35677 avg_loss = 2.99005\n",
      "epoch no.5 train no.390440  loss = 1.61451 avg_loss = 2.98385\n",
      "epoch no.5 train no.390450  loss = 3.13166 avg_loss = 2.98113\n",
      "epoch no.5 train no.390460  loss = 3.47223 avg_loss = 2.94381\n",
      "epoch no.5 train no.390470  loss = 1.74826 avg_loss = 2.95352\n",
      "epoch no.5 train no.390480  loss = 2.96764 avg_loss = 2.93540\n",
      "epoch no.5 train no.390490  loss = 2.58458 avg_loss = 2.94489\n",
      "epoch no.5 train no.390500  loss = 3.14479 avg_loss = 2.93439\n",
      "epoch no.5 train no.390510  loss = 3.81406 avg_loss = 2.98874\n",
      "epoch no.5 train no.390520  loss = 2.66888 avg_loss = 2.98563\n",
      "epoch no.5 train no.390530  loss = 3.06915 avg_loss = 2.95133\n",
      "epoch no.5 train no.390540  loss = 4.47271 avg_loss = 3.00941\n",
      "epoch no.5 train no.390550  loss = 1.35586 avg_loss = 3.03038\n",
      "epoch no.5 train no.390560  loss = 3.31028 avg_loss = 3.04931\n",
      "epoch no.5 train no.390570  loss = 5.28900 avg_loss = 3.08693\n",
      "epoch no.5 train no.390580  loss = 4.54854 avg_loss = 3.09413\n",
      "epoch no.5 train no.390590  loss = 2.49350 avg_loss = 3.05124\n",
      "epoch no.5 train no.390600  loss = 2.64865 avg_loss = 3.07984\n",
      "epoch no.5 train no.390610  loss = 4.20870 avg_loss = 3.08579\n",
      "epoch no.5 train no.390620  loss = 4.55860 avg_loss = 3.10624\n",
      "epoch no.5 train no.390630  loss = 3.91430 avg_loss = 3.08023\n",
      "epoch no.5 train no.390640  loss = 2.50478 avg_loss = 3.05220\n",
      "epoch no.5 train no.390650  loss = 2.41663 avg_loss = 3.08946\n",
      "epoch no.5 train no.390660  loss = 3.66981 avg_loss = 3.08894\n",
      "epoch no.5 train no.390670  loss = 3.08417 avg_loss = 3.14444\n",
      "epoch no.5 train no.390680  loss = 2.95859 avg_loss = 3.16504\n",
      "epoch no.5 train no.390690  loss = 2.41598 avg_loss = 3.15802\n",
      "epoch no.5 train no.390700  loss = 3.99988 avg_loss = 3.17429\n",
      "epoch no.5 train no.390710  loss = 2.51469 avg_loss = 3.13384\n",
      "epoch no.5 train no.390720  loss = 4.84058 avg_loss = 3.13512\n",
      "epoch no.5 train no.390730  loss = 2.10293 avg_loss = 3.10074\n",
      "epoch no.5 train no.390740  loss = 3.03819 avg_loss = 3.08538\n",
      "epoch no.5 train no.390750  loss = 4.21035 avg_loss = 3.06619\n",
      "epoch no.5 train no.390760  loss = 4.67305 avg_loss = 3.13598\n",
      "epoch no.5 train no.390770  loss = 3.48186 avg_loss = 3.14182\n",
      "epoch no.5 train no.390780  loss = 2.56340 avg_loss = 3.15085\n",
      "epoch no.5 train no.390790  loss = 2.60381 avg_loss = 3.12700\n",
      "epoch no.5 train no.390800  loss = 3.90593 avg_loss = 3.11725\n",
      "epoch no.5 train no.390810  loss = 4.04569 avg_loss = 3.11279\n",
      "epoch no.5 train no.390820  loss = 1.63835 avg_loss = 3.09365\n",
      "epoch no.5 train no.390830  loss = 2.68428 avg_loss = 3.10584\n",
      "epoch no.5 train no.390840  loss = 2.65775 avg_loss = 3.09860\n",
      "epoch no.5 train no.390850  loss = 3.90334 avg_loss = 3.13443\n",
      "epoch no.5 train no.390860  loss = 3.43358 avg_loss = 3.07227\n",
      "epoch no.5 train no.390870  loss = 3.35742 avg_loss = 3.04130\n",
      "epoch no.5 train no.390880  loss = 4.33827 avg_loss = 3.06650\n",
      "epoch no.5 train no.390890  loss = 4.92937 avg_loss = 3.07401\n",
      "epoch no.5 train no.390900  loss = 2.93815 avg_loss = 3.09490\n",
      "epoch no.5 train no.390910  loss = 4.11678 avg_loss = 3.06666\n",
      "epoch no.5 train no.390920  loss = 3.27038 avg_loss = 3.08423\n",
      "epoch no.5 train no.390930  loss = 2.95753 avg_loss = 3.06602\n",
      "epoch no.5 train no.390940  loss = 2.64883 avg_loss = 3.07762\n",
      "epoch no.5 train no.390950  loss = 2.70103 avg_loss = 3.09534\n",
      "epoch no.5 train no.390960  loss = 1.84796 avg_loss = 3.07226\n",
      "epoch no.5 train no.390970  loss = 4.20620 avg_loss = 3.08443\n",
      "epoch no.5 train no.390980  loss = 2.35897 avg_loss = 3.07343\n",
      "epoch no.5 train no.390990  loss = 3.76338 avg_loss = 3.06038\n",
      "epoch no.5 train no.391000  loss = 2.66173 avg_loss = 3.02449\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁노래', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.5 train no.391010  loss = 1.79744 avg_loss = 3.02861\n",
      "epoch no.5 train no.391020  loss = 3.27509 avg_loss = 2.98895\n",
      "epoch no.5 train no.391030  loss = 2.65902 avg_loss = 3.00453\n",
      "epoch no.5 train no.391040  loss = 2.24650 avg_loss = 3.02971\n",
      "epoch no.5 train no.391050  loss = 4.57697 avg_loss = 3.03010\n",
      "epoch no.5 train no.391060  loss = 1.58789 avg_loss = 3.03905\n",
      "epoch no.5 train no.391070  loss = 3.56669 avg_loss = 3.05949\n",
      "epoch no.5 train no.391080  loss = 2.68098 avg_loss = 3.03511\n",
      "epoch no.5 train no.391090  loss = 4.16975 avg_loss = 3.06770\n",
      "epoch no.5 train no.391100  loss = 4.53149 avg_loss = 3.08604\n",
      "epoch no.5 train no.391110  loss = 2.88057 avg_loss = 3.10670\n",
      "epoch no.5 train no.391120  loss = 2.91836 avg_loss = 3.13268\n",
      "epoch no.5 train no.391130  loss = 2.84276 avg_loss = 3.11355\n",
      "epoch no.5 train no.391140  loss = 3.22035 avg_loss = 3.11526\n",
      "epoch no.5 train no.391150  loss = 3.50848 avg_loss = 3.11128\n",
      "epoch no.5 train no.391160  loss = 2.87056 avg_loss = 3.09065\n",
      "epoch no.5 train no.391170  loss = 2.58328 avg_loss = 3.07623\n",
      "epoch no.5 train no.391180  loss = 3.11762 avg_loss = 3.05408\n",
      "epoch no.5 train no.391190  loss = 3.89306 avg_loss = 3.06771\n",
      "epoch no.5 train no.391200  loss = 2.57708 avg_loss = 3.04992\n",
      "epoch no.5 train no.391210  loss = 3.18474 avg_loss = 3.03813\n",
      "epoch no.5 train no.391220  loss = 3.30551 avg_loss = 3.03797\n",
      "epoch no.5 train no.391230  loss = 3.07880 avg_loss = 3.01422\n",
      "epoch no.5 train no.391240  loss = 3.24384 avg_loss = 3.03842\n",
      "epoch no.5 train no.391250  loss = 4.32541 avg_loss = 3.07663\n",
      "epoch no.5 train no.391260  loss = 1.46928 avg_loss = 3.03830\n",
      "epoch no.5 train no.391270  loss = 4.07670 avg_loss = 3.07257\n",
      "epoch no.5 train no.391280  loss = 2.36567 avg_loss = 3.06737\n",
      "epoch no.5 train no.391290  loss = 2.71874 avg_loss = 3.05152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.391300  loss = 2.54412 avg_loss = 3.01236\n",
      "epoch no.5 train no.391310  loss = 3.03802 avg_loss = 2.99480\n",
      "epoch no.5 train no.391320  loss = 3.25641 avg_loss = 3.02072\n",
      "epoch no.5 train no.391330  loss = 3.77957 avg_loss = 3.03583\n",
      "epoch no.5 train no.391340  loss = 3.88948 avg_loss = 3.03316\n",
      "epoch no.5 train no.391350  loss = 3.24884 avg_loss = 2.99062\n",
      "epoch no.5 train no.391360  loss = 2.93385 avg_loss = 3.01836\n",
      "epoch no.5 train no.391370  loss = 2.73100 avg_loss = 2.98226\n",
      "epoch no.5 train no.391380  loss = 2.74846 avg_loss = 3.01696\n",
      "epoch no.5 train no.391390  loss = 4.17866 avg_loss = 3.01802\n",
      "epoch no.5 train no.391400  loss = 3.17079 avg_loss = 3.02464\n",
      "epoch no.5 train no.391410  loss = 4.84048 avg_loss = 3.02086\n",
      "epoch no.5 train no.391420  loss = 2.50801 avg_loss = 3.05848\n",
      "epoch no.5 train no.391430  loss = 1.78652 avg_loss = 3.00601\n",
      "epoch no.5 train no.391440  loss = 3.08904 avg_loss = 3.00502\n",
      "epoch no.5 train no.391450  loss = 3.21007 avg_loss = 3.06834\n",
      "epoch no.5 train no.391460  loss = 2.50068 avg_loss = 3.08578\n",
      "epoch no.5 train no.391470  loss = 2.19511 avg_loss = 3.03926\n",
      "epoch no.5 train no.391480  loss = 1.80822 avg_loss = 3.03979\n",
      "epoch no.5 train no.391490  loss = 3.19866 avg_loss = 3.05994\n",
      "epoch no.5 train no.391500  loss = 1.64017 avg_loss = 3.02184\n",
      "epoch no.5 train no.391510  loss = 2.44993 avg_loss = 3.02709\n",
      "epoch no.5 train no.391520  loss = 3.66605 avg_loss = 3.02979\n",
      "epoch no.5 train no.391530  loss = 2.73669 avg_loss = 3.02051\n",
      "epoch no.5 train no.391540  loss = 2.28217 avg_loss = 2.98750\n",
      "epoch no.5 train no.391550  loss = 2.90416 avg_loss = 2.98011\n",
      "epoch no.5 train no.391560  loss = 3.08310 avg_loss = 2.99716\n",
      "epoch no.5 train no.391570  loss = 3.50177 avg_loss = 2.99957\n",
      "epoch no.5 train no.391580  loss = 2.97079 avg_loss = 3.03108\n",
      "epoch no.5 train no.391590  loss = 2.68517 avg_loss = 3.05818\n",
      "epoch no.5 train no.391600  loss = 2.30007 avg_loss = 3.08957\n",
      "epoch no.5 train no.391610  loss = 2.09887 avg_loss = 3.09102\n",
      "epoch no.5 train no.391620  loss = 3.67452 avg_loss = 3.10929\n",
      "epoch no.5 train no.391630  loss = 2.10636 avg_loss = 3.13961\n",
      "epoch no.5 train no.391640  loss = 2.62413 avg_loss = 3.11195\n",
      "epoch no.5 train no.391650  loss = 3.87246 avg_loss = 3.12350\n",
      "epoch no.5 train no.391660  loss = 3.81059 avg_loss = 3.10954\n",
      "epoch no.5 train no.391670  loss = 3.08250 avg_loss = 3.06181\n",
      "epoch no.5 train no.391680  loss = 2.72131 avg_loss = 3.06922\n",
      "epoch no.5 train no.391690  loss = 3.04325 avg_loss = 3.05871\n",
      "epoch no.5 train no.391700  loss = 2.56218 avg_loss = 3.05787\n",
      "epoch no.5 train no.391710  loss = 2.49344 avg_loss = 3.05827\n",
      "epoch no.5 train no.391720  loss = 4.22580 avg_loss = 3.06690\n",
      "epoch no.5 train no.391730  loss = 1.91605 avg_loss = 3.01970\n",
      "epoch no.5 train no.391740  loss = 2.20966 avg_loss = 3.01651\n",
      "epoch no.5 train no.391750  loss = 1.58789 avg_loss = 2.99537\n",
      "epoch no.5 train no.391760  loss = 2.11345 avg_loss = 2.96848\n",
      "epoch no.5 train no.391770  loss = 0.93880 avg_loss = 2.94415\n",
      "epoch no.5 train no.391780  loss = 2.72707 avg_loss = 2.91504\n",
      "epoch no.5 train no.391790  loss = 3.58754 avg_loss = 2.93867\n",
      "epoch no.5 train no.391800  loss = 2.74717 avg_loss = 2.92974\n",
      "epoch no.5 train no.391810  loss = 1.99962 avg_loss = 2.97575\n",
      "epoch no.5 train no.391820  loss = 4.01333 avg_loss = 3.02764\n",
      "epoch no.5 train no.391830  loss = 2.36194 avg_loss = 3.06293\n",
      "epoch no.5 train no.391840  loss = 2.68540 avg_loss = 3.07254\n",
      "epoch no.5 train no.391850  loss = 3.37001 avg_loss = 3.08795\n",
      "epoch no.5 train no.391860  loss = 3.62237 avg_loss = 3.10773\n",
      "epoch no.5 train no.391870  loss = 3.02607 avg_loss = 3.10955\n",
      "epoch no.5 train no.391880  loss = 2.99010 avg_loss = 3.11080\n",
      "epoch no.5 train no.391890  loss = 3.34162 avg_loss = 3.14401\n",
      "epoch no.5 train no.391900  loss = 1.84735 avg_loss = 3.07128\n",
      "epoch no.5 train no.391910  loss = 1.81997 avg_loss = 3.06093\n",
      "epoch no.5 train no.391920  loss = 1.97102 avg_loss = 3.08386\n",
      "epoch no.5 train no.391930  loss = 2.87590 avg_loss = 3.10701\n",
      "epoch no.5 train no.391940  loss = 2.81353 avg_loss = 3.05944\n",
      "epoch no.5 train no.391950  loss = 1.82266 avg_loss = 3.02838\n",
      "epoch no.5 train no.391960  loss = 3.97777 avg_loss = 3.01197\n",
      "epoch no.5 train no.391970  loss = 3.09104 avg_loss = 3.02692\n",
      "epoch no.5 train no.391980  loss = 3.67745 avg_loss = 2.99654\n",
      "epoch no.5 train no.391990  loss = 4.17073 avg_loss = 2.99506\n",
      "epoch no.5 train no.392000  loss = 2.35063 avg_loss = 2.94448\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '들', '</s>']\n",
      "추억의 90년대 노래 모음</s>\n",
      "epoch no.5 train no.392010  loss = 3.78871 avg_loss = 2.93725\n",
      "epoch no.5 train no.392020  loss = 3.77799 avg_loss = 2.95541\n",
      "epoch no.5 train no.392030  loss = 1.71671 avg_loss = 2.95335\n",
      "epoch no.5 train no.392040  loss = 2.76653 avg_loss = 2.96511\n",
      "epoch no.5 train no.392050  loss = 2.75848 avg_loss = 2.97188\n",
      "epoch no.5 train no.392060  loss = 3.76804 avg_loss = 2.98901\n",
      "epoch no.5 train no.392070  loss = 2.14763 avg_loss = 2.99191\n",
      "epoch no.5 train no.392080  loss = 2.74725 avg_loss = 2.99178\n",
      "epoch no.5 train no.392090  loss = 3.19304 avg_loss = 3.03509\n",
      "epoch no.5 train no.392100  loss = 4.03456 avg_loss = 3.04332\n",
      "epoch no.5 train no.392110  loss = 2.64823 avg_loss = 3.07702\n",
      "epoch no.5 train no.392120  loss = 2.99163 avg_loss = 3.09868\n",
      "epoch no.5 train no.392130  loss = 3.39866 avg_loss = 3.08049\n",
      "epoch no.5 train no.392140  loss = 2.54452 avg_loss = 3.08541\n",
      "epoch no.5 train no.392150  loss = 2.31167 avg_loss = 3.08510\n",
      "epoch no.5 train no.392160  loss = 2.84394 avg_loss = 3.07384\n",
      "epoch no.5 train no.392170  loss = 2.72442 avg_loss = 3.04995\n",
      "epoch no.5 train no.392180  loss = 2.38951 avg_loss = 3.04004\n",
      "epoch no.5 train no.392190  loss = 2.20997 avg_loss = 3.01693\n",
      "epoch no.5 train no.392200  loss = 2.51785 avg_loss = 3.02716\n",
      "epoch no.5 train no.392210  loss = 4.72164 avg_loss = 3.04360\n",
      "epoch no.5 train no.392220  loss = 2.35262 avg_loss = 3.02759\n",
      "epoch no.5 train no.392230  loss = 3.89450 avg_loss = 3.01920\n",
      "epoch no.5 train no.392240  loss = 4.47841 avg_loss = 3.01468\n",
      "epoch no.5 train no.392250  loss = 2.31196 avg_loss = 3.03513\n",
      "epoch no.5 train no.392260  loss = 4.56766 avg_loss = 3.04314\n",
      "epoch no.5 train no.392270  loss = 2.40698 avg_loss = 3.02868\n",
      "epoch no.5 train no.392280  loss = 3.08408 avg_loss = 3.01533\n",
      "epoch no.5 train no.392290  loss = 2.52123 avg_loss = 3.00048\n",
      "epoch no.5 train no.392300  loss = 3.38885 avg_loss = 2.98988\n",
      "epoch no.5 train no.392310  loss = 4.42114 avg_loss = 3.00675\n",
      "epoch no.5 train no.392320  loss = 3.54637 avg_loss = 2.99257\n",
      "epoch no.5 train no.392330  loss = 2.29096 avg_loss = 2.99994\n",
      "epoch no.5 train no.392340  loss = 2.82518 avg_loss = 3.00961\n",
      "epoch no.5 train no.392350  loss = 2.09600 avg_loss = 2.98528\n",
      "epoch no.5 train no.392360  loss = 3.31996 avg_loss = 3.01863\n",
      "epoch no.5 train no.392370  loss = 3.95644 avg_loss = 3.01030\n",
      "epoch no.5 train no.392380  loss = 2.54863 avg_loss = 2.99468\n",
      "epoch no.5 train no.392390  loss = 2.14138 avg_loss = 2.95787\n",
      "epoch no.5 train no.392400  loss = 3.80107 avg_loss = 2.92615\n",
      "epoch no.5 train no.392410  loss = 2.32446 avg_loss = 2.98051\n",
      "epoch no.5 train no.392420  loss = 3.06981 avg_loss = 3.03254\n",
      "epoch no.5 train no.392430  loss = 3.52942 avg_loss = 3.02468\n",
      "epoch no.5 train no.392440  loss = 3.04471 avg_loss = 3.00442\n",
      "epoch no.5 train no.392450  loss = 3.27625 avg_loss = 3.03199\n",
      "epoch no.5 train no.392460  loss = 3.67811 avg_loss = 3.03398\n",
      "epoch no.5 train no.392470  loss = 2.61841 avg_loss = 3.02304\n",
      "epoch no.5 train no.392480  loss = 4.04359 avg_loss = 3.06897\n",
      "epoch no.5 train no.392490  loss = 3.53496 avg_loss = 3.06424\n",
      "epoch no.5 train no.392500  loss = 3.25555 avg_loss = 3.07426\n",
      "epoch no.5 train no.392510  loss = 2.45178 avg_loss = 3.04355\n",
      "epoch no.5 train no.392520  loss = 3.42965 avg_loss = 3.01742\n",
      "epoch no.5 train no.392530  loss = 3.83796 avg_loss = 3.01901\n",
      "epoch no.5 train no.392540  loss = 2.62299 avg_loss = 3.04205\n",
      "epoch no.5 train no.392550  loss = 2.11526 avg_loss = 3.02582\n",
      "epoch no.5 train no.392560  loss = 2.46424 avg_loss = 3.04027\n",
      "epoch no.5 train no.392570  loss = 3.41838 avg_loss = 3.05592\n",
      "epoch no.5 train no.392580  loss = 2.62344 avg_loss = 3.04819\n",
      "epoch no.5 train no.392590  loss = 2.32751 avg_loss = 3.04286\n",
      "epoch no.5 train no.392600  loss = 3.23683 avg_loss = 3.02278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.392610  loss = 3.95640 avg_loss = 3.01548\n",
      "epoch no.5 train no.392620  loss = 2.97554 avg_loss = 2.98619\n",
      "epoch no.5 train no.392630  loss = 2.67260 avg_loss = 3.01227\n",
      "epoch no.5 train no.392640  loss = 3.17125 avg_loss = 3.04267\n",
      "epoch no.5 train no.392650  loss = 2.82264 avg_loss = 3.00011\n",
      "epoch no.5 train no.392660  loss = 4.86399 avg_loss = 2.98299\n",
      "epoch no.5 train no.392670  loss = 3.84941 avg_loss = 2.97598\n",
      "epoch no.5 train no.392680  loss = 2.98978 avg_loss = 2.95562\n",
      "epoch no.5 train no.392690  loss = 3.14299 avg_loss = 2.97767\n",
      "epoch no.5 train no.392700  loss = 3.32176 avg_loss = 2.99124\n",
      "epoch no.5 train no.392710  loss = 3.62610 avg_loss = 3.01093\n",
      "epoch no.5 train no.392720  loss = 2.07570 avg_loss = 3.06748\n",
      "epoch no.5 train no.392730  loss = 2.86056 avg_loss = 3.03811\n",
      "epoch no.5 train no.392740  loss = 2.43622 avg_loss = 3.02278\n",
      "epoch no.5 train no.392750  loss = 3.24440 avg_loss = 3.02641\n",
      "epoch no.5 train no.392760  loss = 2.69683 avg_loss = 3.02349\n",
      "epoch no.5 train no.392770  loss = 3.50551 avg_loss = 3.03706\n",
      "epoch no.5 train no.392780  loss = 3.63983 avg_loss = 3.05072\n",
      "epoch no.5 train no.392790  loss = 2.38521 avg_loss = 3.02537\n",
      "epoch no.5 train no.392800  loss = 3.25687 avg_loss = 2.98923\n",
      "epoch no.5 train no.392810  loss = 2.61642 avg_loss = 2.93722\n",
      "epoch no.5 train no.392820  loss = 2.31580 avg_loss = 2.91678\n",
      "epoch no.5 train no.392830  loss = 3.03793 avg_loss = 2.92555\n",
      "epoch no.5 train no.392840  loss = 2.52026 avg_loss = 2.93903\n",
      "epoch no.5 train no.392850  loss = 3.08926 avg_loss = 2.93244\n",
      "epoch no.5 train no.392860  loss = 2.77859 avg_loss = 2.91673\n",
      "epoch no.5 train no.392870  loss = 2.05905 avg_loss = 2.93540\n",
      "epoch no.5 train no.392880  loss = 2.36835 avg_loss = 2.94247\n",
      "epoch no.5 train no.392890  loss = 2.34208 avg_loss = 2.95686\n",
      "epoch no.5 train no.392900  loss = 3.74054 avg_loss = 3.01850\n",
      "epoch no.5 train no.392910  loss = 1.94129 avg_loss = 3.01922\n",
      "epoch no.5 train no.392920  loss = 1.91525 avg_loss = 3.00531\n",
      "epoch no.5 train no.392930  loss = 2.90342 avg_loss = 3.00474\n",
      "epoch no.5 train no.392940  loss = 2.17092 avg_loss = 2.98854\n",
      "epoch no.5 train no.392950  loss = 2.76656 avg_loss = 3.01340\n",
      "epoch no.5 train no.392960  loss = 2.60149 avg_loss = 3.00869\n",
      "epoch no.5 train no.392970  loss = 3.05280 avg_loss = 3.03353\n",
      "epoch no.5 train no.392980  loss = 1.72539 avg_loss = 3.00303\n",
      "epoch no.5 train no.392990  loss = 2.54198 avg_loss = 2.97148\n",
      "epoch no.5 train no.393000  loss = 3.17118 avg_loss = 3.00509\n",
      "6\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁발라', '아이돌', '▁노래', '곡', '</s>']\n",
      "추억의 2000년대 남자아이돌 댄스곡</s>\n",
      "epoch no.5 train no.393010  loss = 2.90927 avg_loss = 3.01169\n",
      "epoch no.5 train no.393020  loss = 2.49249 avg_loss = 2.98632\n",
      "epoch no.5 train no.393030  loss = 1.65159 avg_loss = 2.94369\n",
      "epoch no.5 train no.393040  loss = 3.70964 avg_loss = 2.93341\n",
      "epoch no.5 train no.393050  loss = 3.96876 avg_loss = 2.97179\n",
      "epoch no.5 train no.393060  loss = 3.10596 avg_loss = 2.97044\n",
      "epoch no.5 train no.393070  loss = 3.65226 avg_loss = 2.97731\n",
      "epoch no.5 train no.393080  loss = 2.08935 avg_loss = 2.99925\n",
      "epoch no.5 train no.393090  loss = 3.00821 avg_loss = 3.00661\n",
      "epoch no.5 train no.393100  loss = 2.17027 avg_loss = 3.02794\n",
      "epoch no.5 train no.393110  loss = 2.53370 avg_loss = 3.03759\n",
      "epoch no.5 train no.393120  loss = 2.84430 avg_loss = 3.02305\n",
      "epoch no.5 train no.393130  loss = 2.43683 avg_loss = 3.01434\n",
      "epoch no.5 train no.393140  loss = 3.38399 avg_loss = 2.98679\n",
      "epoch no.5 train no.393150  loss = 3.74753 avg_loss = 3.01955\n",
      "epoch no.5 train no.393160  loss = 2.70138 avg_loss = 3.01636\n",
      "epoch no.5 train no.393170  loss = 1.47579 avg_loss = 2.98180\n",
      "epoch no.5 train no.393180  loss = 2.15994 avg_loss = 2.94973\n",
      "epoch no.5 train no.393190  loss = 1.77556 avg_loss = 2.96762\n",
      "epoch no.5 train no.393200  loss = 3.35650 avg_loss = 2.92605\n",
      "epoch no.5 train no.393210  loss = 4.00000 avg_loss = 2.96006\n",
      "epoch no.5 train no.393220  loss = 4.37241 avg_loss = 3.00553\n",
      "epoch no.5 train no.393230  loss = 3.17545 avg_loss = 3.01514\n",
      "epoch no.5 train no.393240  loss = 1.93084 avg_loss = 2.99008\n",
      "epoch no.5 train no.393250  loss = 1.96086 avg_loss = 2.99009\n",
      "epoch no.5 train no.393260  loss = 5.24501 avg_loss = 3.02820\n",
      "epoch no.5 train no.393270  loss = 2.80529 avg_loss = 3.00433\n",
      "epoch no.5 train no.393280  loss = 1.90923 avg_loss = 2.99914\n",
      "epoch no.5 train no.393290  loss = 3.50642 avg_loss = 2.98395\n",
      "epoch no.5 train no.393300  loss = 2.06141 avg_loss = 3.01776\n",
      "epoch no.5 train no.393310  loss = 2.11134 avg_loss = 2.98749\n",
      "epoch no.5 train no.393320  loss = 3.11272 avg_loss = 2.96511\n",
      "epoch no.5 train no.393330  loss = 5.13972 avg_loss = 2.99488\n",
      "epoch no.5 train no.393340  loss = 2.68858 avg_loss = 3.02422\n",
      "epoch no.5 train no.393350  loss = 3.60791 avg_loss = 3.04234\n",
      "epoch no.5 train no.393360  loss = 1.79775 avg_loss = 3.02458\n",
      "epoch no.5 train no.393370  loss = 2.42782 avg_loss = 3.00605\n",
      "epoch no.5 train no.393380  loss = 2.66594 avg_loss = 3.00197\n",
      "epoch no.5 train no.393390  loss = 2.55002 avg_loss = 3.02078\n",
      "epoch no.5 train no.393400  loss = 1.26000 avg_loss = 2.98291\n",
      "epoch no.5 train no.393410  loss = 2.43887 avg_loss = 3.00034\n",
      "epoch no.5 train no.393420  loss = 2.05875 avg_loss = 2.99898\n",
      "epoch no.5 train no.393430  loss = 3.07609 avg_loss = 3.01138\n",
      "epoch no.5 train no.393440  loss = 2.90452 avg_loss = 3.02852\n",
      "epoch no.5 train no.393450  loss = 3.21579 avg_loss = 3.00721\n",
      "epoch no.5 train no.393460  loss = 2.23497 avg_loss = 3.01991\n",
      "epoch no.5 train no.393470  loss = 2.56550 avg_loss = 3.03103\n",
      "epoch no.5 train no.393480  loss = 2.29718 avg_loss = 3.03260\n",
      "epoch no.5 train no.393490  loss = 1.73451 avg_loss = 2.98845\n",
      "epoch no.5 train no.393500  loss = 2.73400 avg_loss = 2.98892\n",
      "epoch no.5 train no.393510  loss = 3.30308 avg_loss = 2.97859\n",
      "epoch no.5 train no.393520  loss = 3.46441 avg_loss = 2.98868\n",
      "epoch no.5 train no.393530  loss = 2.07789 avg_loss = 2.95410\n",
      "epoch no.5 train no.393540  loss = 3.35275 avg_loss = 2.90709\n",
      "epoch no.5 train no.393550  loss = 1.79980 avg_loss = 2.90712\n",
      "epoch no.5 train no.393560  loss = 1.08454 avg_loss = 2.88849\n",
      "epoch no.5 train no.393570  loss = 2.00399 avg_loss = 2.93200\n",
      "epoch no.5 train no.393580  loss = 2.84855 avg_loss = 2.92359\n",
      "epoch no.5 train no.393590  loss = 4.94076 avg_loss = 2.93637\n",
      "epoch no.5 train no.393600  loss = 3.38904 avg_loss = 2.95026\n",
      "epoch no.5 train no.393610  loss = 2.65547 avg_loss = 2.96139\n",
      "epoch no.5 train no.393620  loss = 3.90859 avg_loss = 3.00639\n",
      "epoch no.5 train no.393630  loss = 2.44382 avg_loss = 2.98207\n",
      "epoch no.5 train no.393640  loss = 2.11387 avg_loss = 2.97761\n",
      "epoch no.5 train no.393650  loss = 4.21531 avg_loss = 2.98686\n",
      "epoch no.5 train no.393660  loss = 2.28483 avg_loss = 2.98469\n",
      "epoch no.5 train no.393670  loss = 3.31914 avg_loss = 3.02672\n",
      "epoch no.5 train no.393680  loss = 1.80454 avg_loss = 3.02226\n",
      "epoch no.5 train no.393690  loss = 3.19990 avg_loss = 3.01661\n",
      "epoch no.5 train no.393700  loss = 1.94499 avg_loss = 3.00415\n",
      "epoch no.5 train no.393710  loss = 2.02716 avg_loss = 3.03210\n",
      "epoch no.5 train no.393720  loss = 3.32796 avg_loss = 3.00656\n",
      "epoch no.5 train no.393730  loss = 1.43555 avg_loss = 2.98228\n",
      "epoch no.5 train no.393740  loss = 2.66919 avg_loss = 2.97945\n",
      "epoch no.5 train no.393750  loss = 3.56643 avg_loss = 3.03355\n",
      "epoch no.5 train no.393760  loss = 2.85189 avg_loss = 3.02800\n",
      "epoch no.5 train no.393770  loss = 3.38751 avg_loss = 3.01692\n",
      "epoch no.5 train no.393780  loss = 2.19320 avg_loss = 3.00265\n",
      "epoch no.5 train no.393790  loss = 4.03812 avg_loss = 2.99634\n",
      "epoch no.5 train no.393800  loss = 2.63725 avg_loss = 2.97516\n",
      "epoch no.5 train no.393810  loss = 2.44028 avg_loss = 2.95801\n",
      "epoch no.5 train no.393820  loss = 3.86623 avg_loss = 2.98842\n",
      "epoch no.5 train no.393830  loss = 2.43357 avg_loss = 2.94754\n",
      "epoch no.5 train no.393840  loss = 3.79507 avg_loss = 2.93343\n",
      "epoch no.5 train no.393850  loss = 4.15679 avg_loss = 2.97311\n",
      "epoch no.5 train no.393860  loss = 2.30316 avg_loss = 2.96679\n",
      "epoch no.5 train no.393870  loss = 1.75587 avg_loss = 2.94896\n",
      "epoch no.5 train no.393880  loss = 2.73392 avg_loss = 2.99517\n",
      "epoch no.5 train no.393890  loss = 3.06768 avg_loss = 2.98121\n",
      "epoch no.5 train no.393900  loss = 1.55828 avg_loss = 2.93336\n",
      "epoch no.5 train no.393910  loss = 3.34855 avg_loss = 2.96012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.393920  loss = 4.26554 avg_loss = 2.96420\n",
      "epoch no.5 train no.393930  loss = 2.59130 avg_loss = 2.95606\n",
      "epoch no.5 train no.393940  loss = 2.47626 avg_loss = 2.92246\n",
      "epoch no.5 train no.393950  loss = 3.95784 avg_loss = 2.95873\n",
      "epoch no.5 train no.393960  loss = 2.54750 avg_loss = 2.99240\n",
      "epoch no.5 train no.393970  loss = 2.34012 avg_loss = 3.00977\n",
      "epoch no.5 train no.393980  loss = 3.19155 avg_loss = 3.00908\n",
      "epoch no.5 train no.393990  loss = 3.81704 avg_loss = 2.97542\n",
      "epoch no.5 train no.394000  loss = 5.33041 avg_loss = 3.03360\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '▁모음', '</s>']\n",
      "추억의 90년대 가요 모음</s>\n",
      "epoch no.5 train no.394010  loss = 1.80149 avg_loss = 3.01995\n",
      "epoch no.5 train no.394020  loss = 2.35858 avg_loss = 3.01294\n",
      "epoch no.5 train no.394030  loss = 3.06404 avg_loss = 3.01160\n",
      "epoch no.5 train no.394040  loss = 2.14542 avg_loss = 3.00016\n",
      "epoch no.5 train no.394050  loss = 5.13526 avg_loss = 3.02122\n",
      "epoch no.5 train no.394060  loss = 2.46727 avg_loss = 3.04991\n",
      "epoch no.5 train no.394070  loss = 4.70149 avg_loss = 3.06901\n",
      "epoch no.5 train no.394080  loss = 2.23235 avg_loss = 3.00782\n",
      "epoch no.5 train no.394090  loss = 2.70883 avg_loss = 3.03842\n",
      "epoch no.5 train no.394100  loss = 4.44031 avg_loss = 3.03668\n",
      "epoch no.5 train no.394110  loss = 2.75437 avg_loss = 3.04175\n",
      "epoch no.5 train no.394120  loss = 1.92312 avg_loss = 3.01342\n",
      "epoch no.5 train no.394130  loss = 2.32868 avg_loss = 3.01072\n",
      "epoch no.5 train no.394140  loss = 4.50201 avg_loss = 3.04330\n",
      "epoch no.5 train no.394150  loss = 2.02678 avg_loss = 3.00797\n",
      "epoch no.5 train no.394160  loss = 3.82036 avg_loss = 3.01776\n",
      "epoch no.5 train no.394170  loss = 2.44917 avg_loss = 3.01489\n",
      "epoch no.5 train no.394180  loss = 2.41640 avg_loss = 3.00647\n",
      "epoch no.5 train no.394190  loss = 1.43555 avg_loss = 2.99722\n",
      "epoch no.5 train no.394200  loss = 2.01011 avg_loss = 2.97814\n",
      "epoch no.5 train no.394210  loss = 2.26711 avg_loss = 2.94038\n",
      "epoch no.5 train no.394220  loss = 2.12293 avg_loss = 2.92344\n",
      "epoch no.5 train no.394230  loss = 3.26050 avg_loss = 2.94320\n",
      "epoch no.5 train no.394240  loss = 2.24185 avg_loss = 2.94824\n",
      "epoch no.5 train no.394250  loss = 5.57740 avg_loss = 3.02451\n",
      "epoch no.5 train no.394260  loss = 2.71027 avg_loss = 3.05333\n",
      "epoch no.5 train no.394270  loss = 2.97241 avg_loss = 3.04068\n",
      "epoch no.5 train no.394280  loss = 2.81315 avg_loss = 3.02921\n",
      "epoch no.5 train no.394290  loss = 2.21030 avg_loss = 3.02856\n",
      "epoch no.5 train no.394300  loss = 2.73417 avg_loss = 3.02835\n",
      "epoch no.5 train no.394310  loss = 2.85850 avg_loss = 3.01670\n",
      "epoch no.5 train no.394320  loss = 3.25609 avg_loss = 3.01261\n",
      "epoch no.5 train no.394330  loss = 3.75443 avg_loss = 3.02121\n",
      "epoch no.5 train no.394340  loss = 2.21448 avg_loss = 2.99870\n",
      "epoch no.5 train no.394350  loss = 3.14752 avg_loss = 3.00441\n",
      "epoch no.5 train no.394360  loss = 4.26369 avg_loss = 3.01206\n",
      "epoch no.5 train no.394370  loss = 2.81217 avg_loss = 3.02813\n",
      "epoch no.5 train no.394380  loss = 2.70692 avg_loss = 2.98977\n",
      "epoch no.5 train no.394390  loss = 4.43304 avg_loss = 3.01994\n",
      "epoch no.5 train no.394400  loss = 3.36983 avg_loss = 3.01968\n",
      "epoch no.5 train no.394410  loss = 3.08399 avg_loss = 3.01536\n",
      "epoch no.5 train no.394420  loss = 2.17060 avg_loss = 3.03722\n",
      "epoch no.5 train no.394430  loss = 3.20115 avg_loss = 3.04608\n",
      "epoch no.5 train no.394440  loss = 2.78020 avg_loss = 3.02372\n",
      "epoch no.5 train no.394450  loss = 3.80611 avg_loss = 3.04982\n",
      "epoch no.5 train no.394460  loss = 2.74087 avg_loss = 3.05287\n",
      "epoch no.5 train no.394470  loss = 2.77939 avg_loss = 3.02916\n",
      "epoch no.5 train no.394480  loss = 2.87142 avg_loss = 3.02070\n",
      "epoch no.5 train no.394490  loss = 1.76427 avg_loss = 3.00639\n",
      "epoch no.5 train no.394500  loss = 3.41189 avg_loss = 3.00618\n",
      "epoch no.5 train no.394510  loss = 2.75413 avg_loss = 2.98695\n",
      "epoch no.5 train no.394520  loss = 2.13862 avg_loss = 2.93309\n",
      "epoch no.5 train no.394530  loss = 3.36123 avg_loss = 2.94598\n",
      "epoch no.5 train no.394540  loss = 2.61288 avg_loss = 2.95658\n",
      "epoch no.5 train no.394550  loss = 1.50320 avg_loss = 2.93869\n",
      "epoch no.5 train no.394560  loss = 3.08824 avg_loss = 2.90048\n",
      "epoch no.5 train no.394570  loss = 2.37391 avg_loss = 2.92327\n",
      "epoch no.5 train no.394580  loss = 2.31346 avg_loss = 2.92330\n",
      "epoch no.5 train no.394590  loss = 3.45561 avg_loss = 2.90829\n",
      "epoch no.5 train no.394600  loss = 5.82840 avg_loss = 2.94300\n",
      "epoch no.5 train no.394610  loss = 2.87370 avg_loss = 2.97370\n",
      "epoch no.5 train no.394620  loss = 2.00442 avg_loss = 2.95746\n",
      "epoch no.5 train no.394630  loss = 3.17017 avg_loss = 2.94528\n",
      "epoch no.5 train no.394640  loss = 3.66189 avg_loss = 3.01606\n",
      "epoch no.5 train no.394650  loss = 2.30376 avg_loss = 3.02027\n",
      "epoch no.5 train no.394660  loss = 4.69260 avg_loss = 3.00703\n",
      "epoch no.5 train no.394670  loss = 4.83127 avg_loss = 3.04118\n",
      "epoch no.5 train no.394680  loss = 3.14307 avg_loss = 2.99075\n",
      "epoch no.5 train no.394690  loss = 2.43330 avg_loss = 2.98113\n",
      "epoch no.5 train no.394700  loss = 1.96618 avg_loss = 2.96020\n",
      "epoch no.5 train no.394710  loss = 2.91391 avg_loss = 2.98542\n",
      "epoch no.5 train no.394720  loss = 2.37330 avg_loss = 3.00073\n",
      "epoch no.5 train no.394730  loss = 1.35289 avg_loss = 3.02173\n",
      "epoch no.5 train no.394740  loss = 4.56550 avg_loss = 3.06212\n",
      "epoch no.5 train no.394750  loss = 3.39803 avg_loss = 3.06660\n",
      "epoch no.5 train no.394760  loss = 1.86451 avg_loss = 3.04719\n",
      "epoch no.5 train no.394770  loss = 2.56982 avg_loss = 3.05836\n",
      "epoch no.5 train no.394780  loss = 3.24838 avg_loss = 3.02230\n",
      "epoch no.5 train no.394790  loss = 2.52745 avg_loss = 3.00208\n",
      "epoch no.5 train no.394800  loss = 2.38458 avg_loss = 2.97897\n",
      "epoch no.5 train no.394810  loss = 2.06526 avg_loss = 2.98587\n",
      "epoch no.5 train no.394820  loss = 3.36259 avg_loss = 2.98936\n",
      "epoch no.5 train no.394830  loss = 2.09487 avg_loss = 2.97969\n",
      "epoch no.5 train no.394840  loss = 2.75403 avg_loss = 3.01040\n",
      "epoch no.5 train no.394850  loss = 3.14070 avg_loss = 2.97158\n",
      "epoch no.5 train no.394860  loss = 3.92635 avg_loss = 3.00976\n",
      "epoch no.5 train no.394870  loss = 3.56098 avg_loss = 2.99917\n",
      "epoch no.5 train no.394880  loss = 2.55720 avg_loss = 2.99659\n",
      "epoch no.5 train no.394890  loss = 3.06551 avg_loss = 2.99500\n",
      "epoch no.5 train no.394900  loss = 2.80900 avg_loss = 2.94721\n",
      "epoch no.5 train no.394910  loss = 3.36231 avg_loss = 3.01087\n",
      "epoch no.5 train no.394920  loss = 4.18978 avg_loss = 2.98546\n",
      "epoch no.5 train no.394930  loss = 1.58992 avg_loss = 3.00575\n",
      "epoch no.5 train no.394940  loss = 1.58487 avg_loss = 3.00443\n",
      "epoch no.5 train no.394950  loss = 2.65681 avg_loss = 3.00990\n",
      "epoch no.5 train no.394960  loss = 2.78019 avg_loss = 3.00154\n",
      "epoch no.5 train no.394970  loss = 2.90317 avg_loss = 2.97867\n",
      "epoch no.5 train no.394980  loss = 3.32016 avg_loss = 2.98165\n",
      "epoch no.5 train no.394990  loss = 2.05355 avg_loss = 2.97199\n",
      "epoch no.5 train no.395000  loss = 2.06697 avg_loss = 2.97794\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.395010  loss = 1.90656 avg_loss = 2.98564\n",
      "epoch no.5 train no.395020  loss = 3.79224 avg_loss = 3.02409\n",
      "epoch no.5 train no.395030  loss = 3.58735 avg_loss = 2.99390\n",
      "epoch no.5 train no.395040  loss = 3.59876 avg_loss = 2.97931\n",
      "epoch no.5 train no.395050  loss = 3.74235 avg_loss = 2.99473\n",
      "epoch no.5 train no.395060  loss = 4.94619 avg_loss = 3.01393\n",
      "epoch no.5 train no.395070  loss = 4.27152 avg_loss = 3.02338\n",
      "epoch no.5 train no.395080  loss = 3.42251 avg_loss = 3.00305\n",
      "epoch no.5 train no.395090  loss = 1.20995 avg_loss = 3.00800\n",
      "epoch no.5 train no.395100  loss = 3.15913 avg_loss = 3.01208\n",
      "epoch no.5 train no.395110  loss = 4.06160 avg_loss = 2.99342\n",
      "epoch no.5 train no.395120  loss = 3.76010 avg_loss = 3.00895\n",
      "epoch no.5 train no.395130  loss = 2.80393 avg_loss = 3.02306\n",
      "epoch no.5 train no.395140  loss = 2.64417 avg_loss = 3.08106\n",
      "epoch no.5 train no.395150  loss = 3.04472 avg_loss = 3.12697\n",
      "epoch no.5 train no.395160  loss = 3.63850 avg_loss = 3.10544\n",
      "epoch no.5 train no.395170  loss = 2.43394 avg_loss = 3.10003\n",
      "epoch no.5 train no.395180  loss = 2.72703 avg_loss = 3.11333\n",
      "epoch no.5 train no.395190  loss = 2.55016 avg_loss = 3.08894\n",
      "epoch no.5 train no.395200  loss = 4.52187 avg_loss = 3.10968\n",
      "epoch no.5 train no.395210  loss = 4.64105 avg_loss = 3.11756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.395220  loss = 3.67707 avg_loss = 3.08942\n",
      "epoch no.5 train no.395230  loss = 2.76677 avg_loss = 3.05902\n",
      "epoch no.5 train no.395240  loss = 2.66888 avg_loss = 3.06171\n",
      "epoch no.5 train no.395250  loss = 2.19172 avg_loss = 3.06167\n",
      "epoch no.5 train no.395260  loss = 1.78057 avg_loss = 3.07314\n",
      "epoch no.5 train no.395270  loss = 1.91506 avg_loss = 3.09586\n",
      "epoch no.5 train no.395280  loss = 3.09953 avg_loss = 3.09111\n",
      "epoch no.5 train no.395290  loss = 2.61952 avg_loss = 3.10067\n",
      "epoch no.5 train no.395300  loss = 4.05234 avg_loss = 3.13198\n",
      "epoch no.5 train no.395310  loss = 2.19591 avg_loss = 3.13492\n",
      "epoch no.5 train no.395320  loss = 4.28826 avg_loss = 3.15308\n",
      "epoch no.5 train no.395330  loss = 3.02589 avg_loss = 3.16718\n",
      "epoch no.5 train no.395340  loss = 2.29160 avg_loss = 3.16335\n",
      "epoch no.5 train no.395350  loss = 3.13625 avg_loss = 3.16738\n",
      "epoch no.5 train no.395360  loss = 2.54980 avg_loss = 3.13318\n",
      "epoch no.5 train no.395370  loss = 2.30373 avg_loss = 3.14571\n",
      "epoch no.5 train no.395380  loss = 2.72922 avg_loss = 3.12650\n",
      "epoch no.5 train no.395390  loss = 3.38601 avg_loss = 3.15019\n",
      "epoch no.5 train no.395400  loss = 3.34792 avg_loss = 3.14492\n",
      "epoch no.5 train no.395410  loss = 2.53897 avg_loss = 3.09175\n",
      "epoch no.5 train no.395420  loss = 3.51465 avg_loss = 3.05717\n",
      "epoch no.5 train no.395430  loss = 2.10149 avg_loss = 3.02354\n",
      "epoch no.5 train no.395440  loss = 1.91750 avg_loss = 3.00245\n",
      "epoch no.5 train no.395450  loss = 2.11976 avg_loss = 3.02197\n",
      "epoch no.5 train no.395460  loss = 2.11787 avg_loss = 2.97888\n",
      "epoch no.5 train no.395470  loss = 2.25241 avg_loss = 2.98404\n",
      "epoch no.5 train no.395480  loss = 2.70672 avg_loss = 3.00000\n",
      "epoch no.5 train no.395490  loss = 1.72901 avg_loss = 2.96576\n",
      "epoch no.5 train no.395500  loss = 2.48911 avg_loss = 2.94103\n",
      "epoch no.5 train no.395510  loss = 4.30413 avg_loss = 2.94651\n",
      "epoch no.5 train no.395520  loss = 4.18135 avg_loss = 3.01268\n",
      "epoch no.5 train no.395530  loss = 2.38409 avg_loss = 2.99434\n",
      "epoch no.5 train no.395540  loss = 3.26337 avg_loss = 2.97810\n",
      "epoch no.5 train no.395550  loss = 3.09268 avg_loss = 2.98518\n",
      "epoch no.5 train no.395560  loss = 3.01259 avg_loss = 2.95957\n",
      "epoch no.5 train no.395570  loss = 2.42910 avg_loss = 2.97187\n",
      "epoch no.5 train no.395580  loss = 1.73762 avg_loss = 2.97860\n",
      "epoch no.5 train no.395590  loss = 2.65519 avg_loss = 2.97840\n",
      "epoch no.5 train no.395600  loss = 3.32233 avg_loss = 2.98587\n",
      "epoch no.5 train no.395610  loss = 2.28364 avg_loss = 2.96729\n",
      "epoch no.5 train no.395620  loss = 3.69499 avg_loss = 2.98855\n",
      "epoch no.5 train no.395630  loss = 2.56371 avg_loss = 3.00262\n",
      "epoch no.5 train no.395640  loss = 2.47724 avg_loss = 2.97514\n",
      "epoch no.5 train no.395650  loss = 3.39405 avg_loss = 2.98531\n",
      "epoch no.5 train no.395660  loss = 3.27800 avg_loss = 2.99063\n",
      "epoch no.5 train no.395670  loss = 3.14802 avg_loss = 3.07399\n",
      "epoch no.5 train no.395680  loss = 2.54320 avg_loss = 3.04874\n",
      "epoch no.5 train no.395690  loss = 3.03461 avg_loss = 3.02828\n",
      "epoch no.5 train no.395700  loss = 3.36742 avg_loss = 3.06088\n",
      "epoch no.5 train no.395710  loss = 2.54580 avg_loss = 3.07203\n",
      "epoch no.5 train no.395720  loss = 2.63791 avg_loss = 3.05534\n",
      "epoch no.5 train no.395730  loss = 2.33814 avg_loss = 3.04778\n",
      "epoch no.5 train no.395740  loss = 2.37605 avg_loss = 3.04357\n",
      "epoch no.5 train no.395750  loss = 2.83580 avg_loss = 3.04607\n",
      "epoch no.5 train no.395760  loss = 2.58681 avg_loss = 3.05863\n",
      "epoch no.5 train no.395770  loss = 1.89632 avg_loss = 3.06790\n",
      "epoch no.5 train no.395780  loss = 2.17471 avg_loss = 3.06351\n",
      "epoch no.5 train no.395790  loss = 2.84764 avg_loss = 3.05735\n",
      "epoch no.5 train no.395800  loss = 4.80267 avg_loss = 3.08378\n",
      "epoch no.5 train no.395810  loss = 2.50289 avg_loss = 3.08724\n",
      "epoch no.5 train no.395820  loss = 4.87373 avg_loss = 3.06390\n",
      "epoch no.5 train no.395830  loss = 3.64478 avg_loss = 3.06837\n",
      "epoch no.5 train no.395840  loss = 3.86529 avg_loss = 3.07182\n",
      "epoch no.5 train no.395850  loss = 2.77022 avg_loss = 3.11374\n",
      "epoch no.5 train no.395860  loss = 2.77774 avg_loss = 3.09255\n",
      "epoch no.5 train no.395870  loss = 3.74310 avg_loss = 3.11710\n",
      "epoch no.5 train no.395880  loss = 2.08502 avg_loss = 3.06285\n",
      "epoch no.5 train no.395890  loss = 3.19485 avg_loss = 3.09675\n",
      "epoch no.5 train no.395900  loss = 2.35386 avg_loss = 3.10935\n",
      "epoch no.5 train no.395910  loss = 3.13084 avg_loss = 3.06876\n",
      "epoch no.5 train no.395920  loss = 2.25167 avg_loss = 3.04037\n",
      "epoch no.5 train no.395930  loss = 1.90872 avg_loss = 3.06879\n",
      "epoch no.5 train no.395940  loss = 1.91652 avg_loss = 3.06890\n",
      "epoch no.5 train no.395950  loss = 3.82252 avg_loss = 3.06555\n",
      "epoch no.5 train no.395960  loss = 2.03704 avg_loss = 3.05044\n",
      "epoch no.5 train no.395970  loss = 4.12447 avg_loss = 3.05263\n",
      "epoch no.5 train no.395980  loss = 2.86694 avg_loss = 3.04671\n",
      "epoch no.5 train no.395990  loss = 3.67662 avg_loss = 3.08091\n",
      "epoch no.5 train no.396000  loss = 2.38210 avg_loss = 3.07459\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.396010  loss = 3.13289 avg_loss = 3.03145\n",
      "epoch no.5 train no.396020  loss = 2.48029 avg_loss = 3.04987\n",
      "epoch no.5 train no.396030  loss = 2.43754 avg_loss = 3.06194\n",
      "epoch no.5 train no.396040  loss = 3.49473 avg_loss = 3.03001\n",
      "epoch no.5 train no.396050  loss = 3.93733 avg_loss = 3.03203\n",
      "epoch no.5 train no.396060  loss = 1.99725 avg_loss = 3.02154\n",
      "epoch no.5 train no.396070  loss = 4.86421 avg_loss = 3.03751\n",
      "epoch no.5 train no.396080  loss = 5.53208 avg_loss = 3.06818\n",
      "epoch no.5 train no.396090  loss = 1.83884 avg_loss = 3.05745\n",
      "epoch no.5 train no.396100  loss = 3.10081 avg_loss = 3.07724\n",
      "epoch no.5 train no.396110  loss = 2.20417 avg_loss = 3.03613\n",
      "epoch no.5 train no.396120  loss = 3.03694 avg_loss = 3.04557\n",
      "epoch no.5 train no.396130  loss = 1.97495 avg_loss = 3.02329\n",
      "epoch no.5 train no.396140  loss = 3.79233 avg_loss = 3.01582\n",
      "epoch no.5 train no.396150  loss = 3.44250 avg_loss = 3.04078\n",
      "epoch no.5 train no.396160  loss = 3.84755 avg_loss = 3.02970\n",
      "epoch no.5 train no.396170  loss = 2.67994 avg_loss = 3.01365\n",
      "epoch no.5 train no.396180  loss = 2.25442 avg_loss = 3.01247\n",
      "epoch no.5 train no.396190  loss = 2.79969 avg_loss = 3.01616\n",
      "epoch no.5 train no.396200  loss = 3.60601 avg_loss = 2.98473\n",
      "epoch no.5 train no.396210  loss = 2.66204 avg_loss = 3.01210\n",
      "epoch no.5 train no.396220  loss = 1.99487 avg_loss = 2.98999\n",
      "epoch no.5 train no.396230  loss = 2.47342 avg_loss = 2.98176\n",
      "epoch no.5 train no.396240  loss = 1.96632 avg_loss = 3.01313\n",
      "epoch no.5 train no.396250  loss = 3.88530 avg_loss = 3.04947\n",
      "epoch no.5 train no.396260  loss = 3.49300 avg_loss = 3.06128\n",
      "epoch no.5 train no.396270  loss = 2.10009 avg_loss = 3.04413\n",
      "epoch no.5 train no.396280  loss = 1.91605 avg_loss = 3.03362\n",
      "epoch no.5 train no.396290  loss = 2.46130 avg_loss = 3.04021\n",
      "epoch no.5 train no.396300  loss = 2.67376 avg_loss = 3.04348\n",
      "epoch no.5 train no.396310  loss = 3.61997 avg_loss = 3.01016\n",
      "epoch no.5 train no.396320  loss = 3.24982 avg_loss = 3.00908\n",
      "epoch no.5 train no.396330  loss = 2.34468 avg_loss = 2.97489\n",
      "epoch no.5 train no.396340  loss = 3.10162 avg_loss = 2.97371\n",
      "epoch no.5 train no.396350  loss = 3.40514 avg_loss = 2.98010\n",
      "epoch no.5 train no.396360  loss = 2.46637 avg_loss = 3.05966\n",
      "epoch no.5 train no.396370  loss = 3.78241 avg_loss = 3.08488\n",
      "epoch no.5 train no.396380  loss = 2.26128 avg_loss = 3.04063\n",
      "epoch no.5 train no.396390  loss = 3.41301 avg_loss = 3.07102\n",
      "epoch no.5 train no.396400  loss = 5.95276 avg_loss = 3.07633\n",
      "epoch no.5 train no.396410  loss = 2.77882 avg_loss = 3.03924\n",
      "epoch no.5 train no.396420  loss = 4.09114 avg_loss = 3.04661\n",
      "epoch no.5 train no.396430  loss = 2.65908 avg_loss = 3.04236\n",
      "epoch no.5 train no.396440  loss = 4.91057 avg_loss = 3.06387\n",
      "epoch no.5 train no.396450  loss = 3.72935 avg_loss = 3.03969\n",
      "epoch no.5 train no.396460  loss = 3.63189 avg_loss = 3.00125\n",
      "epoch no.5 train no.396470  loss = 2.85729 avg_loss = 3.01722\n",
      "epoch no.5 train no.396480  loss = 2.97428 avg_loss = 3.01274\n",
      "epoch no.5 train no.396490  loss = 4.40068 avg_loss = 3.04764\n",
      "epoch no.5 train no.396500  loss = 2.72021 avg_loss = 3.01639\n",
      "epoch no.5 train no.396510  loss = 3.64320 avg_loss = 3.00062\n",
      "epoch no.5 train no.396520  loss = 3.27806 avg_loss = 3.01382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.396530  loss = 2.62260 avg_loss = 3.00078\n",
      "epoch no.5 train no.396540  loss = 2.41644 avg_loss = 3.03616\n",
      "epoch no.5 train no.396550  loss = 2.35445 avg_loss = 3.00096\n",
      "epoch no.5 train no.396560  loss = 2.86152 avg_loss = 2.98968\n",
      "epoch no.5 train no.396570  loss = 2.15826 avg_loss = 2.98264\n",
      "epoch no.5 train no.396580  loss = 5.07907 avg_loss = 2.99195\n",
      "epoch no.5 train no.396590  loss = 3.21882 avg_loss = 2.98944\n",
      "epoch no.5 train no.396600  loss = 2.26898 avg_loss = 3.03319\n",
      "epoch no.5 train no.396610  loss = 2.83763 avg_loss = 3.03021\n",
      "epoch no.5 train no.396620  loss = 2.53545 avg_loss = 3.01022\n",
      "epoch no.5 train no.396630  loss = 3.42572 avg_loss = 3.02172\n",
      "epoch no.5 train no.396640  loss = 1.75031 avg_loss = 3.03946\n",
      "epoch no.5 train no.396650  loss = 3.83971 avg_loss = 3.03517\n",
      "epoch no.5 train no.396660  loss = 3.93936 avg_loss = 3.04570\n",
      "epoch no.5 train no.396670  loss = 3.47158 avg_loss = 3.08861\n",
      "epoch no.5 train no.396680  loss = 1.35393 avg_loss = 3.07946\n",
      "epoch no.5 train no.396690  loss = 3.72995 avg_loss = 3.09750\n",
      "epoch no.5 train no.396700  loss = 1.83477 avg_loss = 3.09002\n",
      "epoch no.5 train no.396710  loss = 4.51130 avg_loss = 3.08760\n",
      "epoch no.5 train no.396720  loss = 4.25998 avg_loss = 3.10283\n",
      "epoch no.5 train no.396730  loss = 3.81769 avg_loss = 3.13617\n",
      "epoch no.5 train no.396740  loss = 2.29296 avg_loss = 3.12356\n",
      "epoch no.5 train no.396750  loss = 2.47387 avg_loss = 3.08438\n",
      "epoch no.5 train no.396760  loss = 2.91193 avg_loss = 3.10011\n",
      "epoch no.5 train no.396770  loss = 3.09098 avg_loss = 3.11251\n",
      "epoch no.5 train no.396780  loss = 3.20675 avg_loss = 3.10630\n",
      "epoch no.5 train no.396790  loss = 2.75000 avg_loss = 3.11512\n",
      "epoch no.5 train no.396800  loss = 2.71172 avg_loss = 3.10116\n",
      "epoch no.5 train no.396810  loss = 3.44942 avg_loss = 3.13568\n",
      "epoch no.5 train no.396820  loss = 3.06249 avg_loss = 3.12296\n",
      "epoch no.5 train no.396830  loss = 2.12147 avg_loss = 3.11564\n",
      "epoch no.5 train no.396840  loss = 2.75766 avg_loss = 3.08109\n",
      "epoch no.5 train no.396850  loss = 3.75074 avg_loss = 3.10434\n",
      "epoch no.5 train no.396860  loss = 3.88228 avg_loss = 3.13616\n",
      "epoch no.5 train no.396870  loss = 3.61579 avg_loss = 3.18347\n",
      "epoch no.5 train no.396880  loss = 3.20329 avg_loss = 3.15537\n",
      "epoch no.5 train no.396890  loss = 3.54977 avg_loss = 3.13892\n",
      "epoch no.5 train no.396900  loss = 5.04931 avg_loss = 3.12002\n",
      "epoch no.5 train no.396910  loss = 3.51725 avg_loss = 3.09691\n",
      "epoch no.5 train no.396920  loss = 2.92667 avg_loss = 3.07268\n",
      "epoch no.5 train no.396930  loss = 1.75753 avg_loss = 3.11697\n",
      "epoch no.5 train no.396940  loss = 3.48311 avg_loss = 3.07343\n",
      "epoch no.5 train no.396950  loss = 2.42568 avg_loss = 3.09308\n",
      "epoch no.5 train no.396960  loss = 2.89157 avg_loss = 3.06170\n",
      "epoch no.5 train no.396970  loss = 3.29156 avg_loss = 3.05757\n",
      "epoch no.5 train no.396980  loss = 3.07649 avg_loss = 3.04931\n",
      "epoch no.5 train no.396990  loss = 3.43555 avg_loss = 3.04041\n",
      "epoch no.5 train no.397000  loss = 2.85839 avg_loss = 2.99649\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '▁때', '이', 'op', '</s>']\n",
      "추억의 그 시절 pop</s>\n",
      "epoch no.5 train no.397010  loss = 4.34459 avg_loss = 3.02271\n",
      "epoch no.5 train no.397020  loss = 2.30008 avg_loss = 3.04439\n",
      "epoch no.5 train no.397030  loss = 1.88970 avg_loss = 3.05318\n",
      "epoch no.5 train no.397040  loss = 3.20665 avg_loss = 3.08311\n",
      "epoch no.5 train no.397050  loss = 3.91251 avg_loss = 3.07000\n",
      "epoch no.5 train no.397060  loss = 4.76124 avg_loss = 3.07043\n",
      "epoch no.5 train no.397070  loss = 3.02085 avg_loss = 3.04114\n",
      "epoch no.5 train no.397080  loss = 2.76900 avg_loss = 3.03835\n",
      "epoch no.5 train no.397090  loss = 3.69263 avg_loss = 3.04052\n",
      "epoch no.5 train no.397100  loss = 3.41157 avg_loss = 3.05043\n",
      "epoch no.5 train no.397110  loss = 2.61394 avg_loss = 2.99916\n",
      "epoch no.5 train no.397120  loss = 2.40515 avg_loss = 2.97750\n",
      "epoch no.5 train no.397130  loss = 2.53131 avg_loss = 2.99433\n",
      "epoch no.5 train no.397140  loss = 5.02174 avg_loss = 3.00169\n",
      "epoch no.5 train no.397150  loss = 3.52973 avg_loss = 3.01824\n",
      "epoch no.5 train no.397160  loss = 2.95049 avg_loss = 3.01678\n",
      "epoch no.5 train no.397170  loss = 4.02701 avg_loss = 3.03159\n",
      "epoch no.5 train no.397180  loss = 3.08076 avg_loss = 3.02003\n",
      "epoch no.5 train no.397190  loss = 3.52223 avg_loss = 2.99756\n",
      "epoch no.5 train no.397200  loss = 2.00033 avg_loss = 3.06529\n",
      "epoch no.5 train no.397210  loss = 4.40962 avg_loss = 3.06516\n",
      "epoch no.5 train no.397220  loss = 2.64399 avg_loss = 3.04382\n",
      "epoch no.5 train no.397230  loss = 2.94292 avg_loss = 3.03075\n",
      "epoch no.5 train no.397240  loss = 2.24036 avg_loss = 3.04490\n",
      "epoch no.5 train no.397250  loss = 2.92353 avg_loss = 3.03225\n",
      "epoch no.5 train no.397260  loss = 2.52923 avg_loss = 3.04000\n",
      "epoch no.5 train no.397270  loss = 2.11633 avg_loss = 3.03805\n",
      "epoch no.5 train no.397280  loss = 1.21808 avg_loss = 3.01525\n",
      "epoch no.5 train no.397290  loss = 2.50644 avg_loss = 2.97240\n",
      "epoch no.5 train no.397300  loss = 3.60090 avg_loss = 3.02180\n",
      "epoch no.5 train no.397310  loss = 2.08199 avg_loss = 3.00627\n",
      "epoch no.5 train no.397320  loss = 2.22818 avg_loss = 2.99674\n",
      "epoch no.5 train no.397330  loss = 3.28078 avg_loss = 3.02496\n",
      "epoch no.5 train no.397340  loss = 3.50684 avg_loss = 3.01164\n",
      "epoch no.5 train no.397350  loss = 2.84137 avg_loss = 2.99407\n",
      "epoch no.5 train no.397360  loss = 3.08587 avg_loss = 3.01131\n",
      "epoch no.5 train no.397370  loss = 1.81310 avg_loss = 3.01046\n",
      "epoch no.5 train no.397380  loss = 2.51059 avg_loss = 2.97482\n",
      "epoch no.5 train no.397390  loss = 3.82383 avg_loss = 2.96924\n",
      "epoch no.5 train no.397400  loss = 2.51286 avg_loss = 2.96174\n",
      "epoch no.5 train no.397410  loss = 2.74358 avg_loss = 2.95699\n",
      "epoch no.5 train no.397420  loss = 3.40810 avg_loss = 2.97540\n",
      "epoch no.5 train no.397430  loss = 2.42548 avg_loss = 2.95217\n",
      "epoch no.5 train no.397440  loss = 1.97675 avg_loss = 2.95062\n",
      "epoch no.5 train no.397450  loss = 4.26737 avg_loss = 2.95765\n",
      "epoch no.5 train no.397460  loss = 1.78250 avg_loss = 2.96313\n",
      "epoch no.5 train no.397470  loss = 2.51666 avg_loss = 2.95250\n",
      "epoch no.5 train no.397480  loss = 3.33121 avg_loss = 2.94313\n",
      "epoch no.5 train no.397490  loss = 2.64953 avg_loss = 2.97601\n",
      "epoch no.5 train no.397500  loss = 2.18568 avg_loss = 2.96857\n",
      "epoch no.5 train no.397510  loss = 2.32154 avg_loss = 2.96016\n",
      "epoch no.5 train no.397520  loss = 2.38148 avg_loss = 2.97187\n",
      "epoch no.5 train no.397530  loss = 2.81851 avg_loss = 2.97634\n",
      "epoch no.5 train no.397540  loss = 3.70937 avg_loss = 2.94543\n",
      "epoch no.5 train no.397550  loss = 3.27748 avg_loss = 2.97566\n",
      "epoch no.5 train no.397560  loss = 3.64068 avg_loss = 2.97130\n",
      "epoch no.5 train no.397570  loss = 2.41357 avg_loss = 2.97052\n",
      "epoch no.5 train no.397580  loss = 2.54475 avg_loss = 2.99283\n",
      "epoch no.5 train no.397590  loss = 2.95785 avg_loss = 3.00949\n",
      "epoch no.5 train no.397600  loss = 2.62191 avg_loss = 3.04108\n",
      "epoch no.5 train no.397610  loss = 2.61466 avg_loss = 3.04281\n",
      "epoch no.5 train no.397620  loss = 3.12064 avg_loss = 3.03593\n",
      "epoch no.5 train no.397630  loss = 2.10771 avg_loss = 3.02957\n",
      "epoch no.5 train no.397640  loss = 3.82888 avg_loss = 3.02938\n",
      "epoch no.5 train no.397650  loss = 2.09466 avg_loss = 2.99901\n",
      "epoch no.5 train no.397660  loss = 3.24304 avg_loss = 3.01201\n",
      "epoch no.5 train no.397670  loss = 3.13896 avg_loss = 2.99402\n",
      "epoch no.5 train no.397680  loss = 2.94710 avg_loss = 3.01312\n",
      "epoch no.5 train no.397690  loss = 2.45843 avg_loss = 2.99792\n",
      "epoch no.5 train no.397700  loss = 2.15283 avg_loss = 2.99811\n",
      "epoch no.5 train no.397710  loss = 3.25346 avg_loss = 3.02284\n",
      "epoch no.5 train no.397720  loss = 3.56008 avg_loss = 3.02734\n",
      "epoch no.5 train no.397730  loss = 2.64800 avg_loss = 3.02386\n",
      "epoch no.5 train no.397740  loss = 2.29527 avg_loss = 2.99472\n",
      "epoch no.5 train no.397750  loss = 3.41740 avg_loss = 2.99800\n",
      "epoch no.5 train no.397760  loss = 2.43251 avg_loss = 2.99724\n",
      "epoch no.5 train no.397770  loss = 5.29803 avg_loss = 2.97834\n",
      "epoch no.5 train no.397780  loss = 3.92062 avg_loss = 2.98894\n",
      "epoch no.5 train no.397790  loss = 2.82167 avg_loss = 2.99326\n",
      "epoch no.5 train no.397800  loss = 3.40963 avg_loss = 2.97555\n",
      "epoch no.5 train no.397810  loss = 1.90732 avg_loss = 2.96608\n",
      "epoch no.5 train no.397820  loss = 2.75057 avg_loss = 2.98109\n",
      "epoch no.5 train no.397830  loss = 2.59698 avg_loss = 2.98066\n",
      "epoch no.5 train no.397840  loss = 2.82107 avg_loss = 2.99419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.397850  loss = 2.76720 avg_loss = 2.99188\n",
      "epoch no.5 train no.397860  loss = 4.02562 avg_loss = 2.99355\n",
      "epoch no.5 train no.397870  loss = 4.19007 avg_loss = 3.03589\n",
      "epoch no.5 train no.397880  loss = 3.39156 avg_loss = 3.04003\n",
      "epoch no.5 train no.397890  loss = 3.55371 avg_loss = 3.04465\n",
      "epoch no.5 train no.397900  loss = 5.45451 avg_loss = 3.05413\n",
      "epoch no.5 train no.397910  loss = 2.18253 avg_loss = 3.03098\n",
      "epoch no.5 train no.397920  loss = 3.55019 avg_loss = 2.97538\n",
      "epoch no.5 train no.397930  loss = 3.88623 avg_loss = 3.00484\n",
      "epoch no.5 train no.397940  loss = 3.06382 avg_loss = 3.00026\n",
      "epoch no.5 train no.397950  loss = 3.16394 avg_loss = 3.03671\n",
      "epoch no.5 train no.397960  loss = 2.55375 avg_loss = 3.02215\n",
      "epoch no.5 train no.397970  loss = 2.47378 avg_loss = 2.99009\n",
      "epoch no.5 train no.397980  loss = 2.42223 avg_loss = 2.99786\n",
      "epoch no.5 train no.397990  loss = 3.11269 avg_loss = 2.99039\n",
      "epoch no.5 train no.398000  loss = 3.37175 avg_loss = 2.98626\n",
      "5\n",
      "to_tokens: ['▁비', '▁드라마', '드', '▁명', '곡', '▁모음', '</s>']\n",
      "추억의 발라드 명곡 모음</s>\n",
      "epoch no.5 train no.398010  loss = 4.04974 avg_loss = 3.04081\n",
      "epoch no.5 train no.398020  loss = 3.54788 avg_loss = 3.03999\n",
      "epoch no.5 train no.398030  loss = 2.38721 avg_loss = 3.01503\n",
      "epoch no.5 train no.398040  loss = 1.95036 avg_loss = 2.99928\n",
      "epoch no.5 train no.398050  loss = 2.35196 avg_loss = 2.98855\n",
      "epoch no.5 train no.398060  loss = 2.77015 avg_loss = 2.95672\n",
      "epoch no.5 train no.398070  loss = 1.93648 avg_loss = 2.91446\n",
      "epoch no.5 train no.398080  loss = 3.38766 avg_loss = 2.95594\n",
      "epoch no.5 train no.398090  loss = 2.67858 avg_loss = 2.95662\n",
      "epoch no.5 train no.398100  loss = 2.27403 avg_loss = 2.96416\n",
      "epoch no.5 train no.398110  loss = 2.07755 avg_loss = 2.97159\n",
      "epoch no.5 train no.398120  loss = 3.30319 avg_loss = 2.95498\n",
      "epoch no.5 train no.398130  loss = 2.34405 avg_loss = 2.94153\n",
      "epoch no.5 train no.398140  loss = 4.31827 avg_loss = 2.99216\n",
      "epoch no.5 train no.398150  loss = 3.96120 avg_loss = 3.03291\n",
      "epoch no.5 train no.398160  loss = 3.17148 avg_loss = 3.02671\n",
      "epoch no.5 train no.398170  loss = 3.34182 avg_loss = 3.04774\n",
      "epoch no.5 train no.398180  loss = 2.97718 avg_loss = 3.04285\n",
      "epoch no.5 train no.398190  loss = 2.66721 avg_loss = 3.03093\n",
      "epoch no.5 train no.398200  loss = 2.78871 avg_loss = 3.04554\n",
      "epoch no.5 train no.398210  loss = 3.13615 avg_loss = 3.06971\n",
      "epoch no.5 train no.398220  loss = 3.84011 avg_loss = 3.04423\n",
      "epoch no.5 train no.398230  loss = 4.10517 avg_loss = 3.09068\n",
      "epoch no.5 train no.398240  loss = 2.60636 avg_loss = 3.08428\n",
      "epoch no.5 train no.398250  loss = 4.11439 avg_loss = 3.06692\n",
      "epoch no.5 train no.398260  loss = 2.34240 avg_loss = 3.01620\n",
      "epoch no.5 train no.398270  loss = 2.12703 avg_loss = 3.03810\n",
      "epoch no.5 train no.398280  loss = 2.35461 avg_loss = 3.03495\n",
      "epoch no.5 train no.398290  loss = 2.48190 avg_loss = 2.99432\n",
      "epoch no.5 train no.398300  loss = 3.88007 avg_loss = 3.07563\n",
      "epoch no.5 train no.398310  loss = 2.84065 avg_loss = 3.06462\n",
      "epoch no.5 train no.398320  loss = 2.27852 avg_loss = 3.02006\n",
      "epoch no.5 train no.398330  loss = 1.91883 avg_loss = 3.01085\n",
      "epoch no.5 train no.398340  loss = 2.75505 avg_loss = 3.04383\n",
      "epoch no.5 train no.398350  loss = 3.06478 avg_loss = 3.02294\n",
      "epoch no.5 train no.398360  loss = 2.77324 avg_loss = 3.03570\n",
      "epoch no.5 train no.398370  loss = 2.05626 avg_loss = 3.03502\n",
      "epoch no.5 train no.398380  loss = 3.40502 avg_loss = 3.00039\n",
      "epoch no.5 train no.398390  loss = 2.96660 avg_loss = 2.97426\n",
      "epoch no.5 train no.398400  loss = 3.52273 avg_loss = 3.03263\n",
      "epoch no.5 train no.398410  loss = 4.10700 avg_loss = 3.03586\n",
      "epoch no.5 train no.398420  loss = 2.98131 avg_loss = 3.04447\n",
      "epoch no.5 train no.398430  loss = 3.15763 avg_loss = 3.04721\n",
      "epoch no.5 train no.398440  loss = 3.12007 avg_loss = 3.03998\n",
      "epoch no.5 train no.398450  loss = 2.73714 avg_loss = 3.08778\n",
      "epoch no.5 train no.398460  loss = 3.74657 avg_loss = 3.12541\n",
      "epoch no.5 train no.398470  loss = 2.20665 avg_loss = 3.07174\n",
      "epoch no.5 train no.398480  loss = 3.27913 avg_loss = 3.07809\n",
      "epoch no.5 train no.398490  loss = 2.02546 avg_loss = 3.02431\n",
      "epoch no.5 train no.398500  loss = 2.78137 avg_loss = 3.00251\n",
      "epoch no.5 train no.398510  loss = 3.47903 avg_loss = 3.03182\n",
      "epoch no.5 train no.398520  loss = 3.12900 avg_loss = 3.01403\n",
      "epoch no.5 train no.398530  loss = 2.47671 avg_loss = 3.00445\n",
      "epoch no.5 train no.398540  loss = 4.11828 avg_loss = 3.00035\n",
      "epoch no.5 train no.398550  loss = 1.49385 avg_loss = 2.98029\n",
      "epoch no.5 train no.398560  loss = 3.44529 avg_loss = 2.98213\n",
      "epoch no.5 train no.398570  loss = 3.75228 avg_loss = 2.98848\n",
      "epoch no.5 train no.398580  loss = 4.21980 avg_loss = 3.03116\n",
      "epoch no.5 train no.398590  loss = 2.82626 avg_loss = 3.02772\n",
      "epoch no.5 train no.398600  loss = 2.60429 avg_loss = 3.04836\n",
      "epoch no.5 train no.398610  loss = 3.59048 avg_loss = 3.07009\n",
      "epoch no.5 train no.398620  loss = 3.94144 avg_loss = 3.08114\n",
      "epoch no.5 train no.398630  loss = 3.09621 avg_loss = 3.09421\n",
      "epoch no.5 train no.398640  loss = 3.27507 avg_loss = 3.10480\n",
      "epoch no.5 train no.398650  loss = 4.08822 avg_loss = 3.08992\n",
      "epoch no.5 train no.398660  loss = 2.96080 avg_loss = 3.06722\n",
      "epoch no.5 train no.398670  loss = 2.28505 avg_loss = 3.06649\n",
      "epoch no.5 train no.398680  loss = 2.96813 avg_loss = 3.11080\n",
      "epoch no.5 train no.398690  loss = 2.70397 avg_loss = 3.14711\n",
      "epoch no.5 train no.398700  loss = 1.94573 avg_loss = 3.09220\n",
      "epoch no.5 train no.398710  loss = 2.74094 avg_loss = 3.10061\n",
      "epoch no.5 train no.398720  loss = 3.22755 avg_loss = 3.08992\n",
      "epoch no.5 train no.398730  loss = 2.20508 avg_loss = 3.07753\n",
      "epoch no.5 train no.398740  loss = 3.25405 avg_loss = 3.01852\n",
      "epoch no.5 train no.398750  loss = 2.42316 avg_loss = 3.03062\n",
      "epoch no.5 train no.398760  loss = 3.99658 avg_loss = 2.99594\n",
      "epoch no.5 train no.398770  loss = 2.41110 avg_loss = 2.95724\n",
      "epoch no.5 train no.398780  loss = 3.53538 avg_loss = 2.95401\n",
      "epoch no.5 train no.398790  loss = 1.95595 avg_loss = 2.95815\n",
      "epoch no.5 train no.398800  loss = 2.69787 avg_loss = 2.94386\n",
      "epoch no.5 train no.398810  loss = 3.02169 avg_loss = 2.94336\n",
      "epoch no.5 train no.398820  loss = 3.26786 avg_loss = 2.95712\n",
      "epoch no.5 train no.398830  loss = 3.10068 avg_loss = 2.98136\n",
      "epoch no.5 train no.398840  loss = 2.05646 avg_loss = 3.00395\n",
      "epoch no.5 train no.398850  loss = 1.58385 avg_loss = 2.98156\n",
      "epoch no.5 train no.398860  loss = 2.42267 avg_loss = 2.99223\n",
      "epoch no.5 train no.398870  loss = 2.49084 avg_loss = 2.98084\n",
      "epoch no.5 train no.398880  loss = 3.45967 avg_loss = 3.01992\n",
      "epoch no.5 train no.398890  loss = 2.67301 avg_loss = 3.00812\n",
      "epoch no.5 train no.398900  loss = 2.54654 avg_loss = 2.98793\n",
      "epoch no.5 train no.398910  loss = 2.45711 avg_loss = 3.00042\n",
      "epoch no.5 train no.398920  loss = 2.50277 avg_loss = 3.02884\n",
      "epoch no.5 train no.398930  loss = 3.55896 avg_loss = 2.99948\n",
      "epoch no.5 train no.398940  loss = 1.79800 avg_loss = 2.97274\n",
      "epoch no.5 train no.398950  loss = 2.99425 avg_loss = 2.98643\n",
      "epoch no.5 train no.398960  loss = 2.65527 avg_loss = 2.97844\n",
      "epoch no.5 train no.398970  loss = 3.40512 avg_loss = 2.94856\n",
      "epoch no.5 train no.398980  loss = 2.90781 avg_loss = 2.95709\n",
      "epoch no.5 train no.398990  loss = 1.83414 avg_loss = 2.99324\n",
      "epoch no.5 train no.399000  loss = 3.02823 avg_loss = 2.97366\n",
      "6\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁여자', '▁솔로', '▁노래', '곡', '</s>']\n",
      "추억의 2000년대 여자그룹 댄스곡</s>\n",
      "epoch no.5 train no.399010  loss = 2.75108 avg_loss = 2.99691\n",
      "epoch no.5 train no.399020  loss = 2.34631 avg_loss = 3.01187\n",
      "epoch no.5 train no.399030  loss = 1.93708 avg_loss = 3.03834\n",
      "epoch no.5 train no.399040  loss = 3.17595 avg_loss = 2.99592\n",
      "epoch no.5 train no.399050  loss = 3.95477 avg_loss = 3.02602\n",
      "epoch no.5 train no.399060  loss = 2.77097 avg_loss = 3.03740\n",
      "epoch no.5 train no.399070  loss = 3.12092 avg_loss = 3.01900\n",
      "epoch no.5 train no.399080  loss = 4.09455 avg_loss = 3.03048\n",
      "epoch no.5 train no.399090  loss = 2.31172 avg_loss = 3.01330\n",
      "epoch no.5 train no.399100  loss = 3.32945 avg_loss = 3.01367\n",
      "epoch no.5 train no.399110  loss = 1.91926 avg_loss = 3.00240\n",
      "epoch no.5 train no.399120  loss = 3.92042 avg_loss = 3.02108\n",
      "epoch no.5 train no.399130  loss = 3.96310 avg_loss = 3.04218\n",
      "epoch no.5 train no.399140  loss = 3.25930 avg_loss = 3.01746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.399150  loss = 2.70514 avg_loss = 3.01036\n",
      "epoch no.5 train no.399160  loss = 4.64101 avg_loss = 3.01725\n",
      "epoch no.5 train no.399170  loss = 2.02202 avg_loss = 2.98600\n",
      "epoch no.5 train no.399180  loss = 3.86754 avg_loss = 2.97071\n",
      "epoch no.5 train no.399190  loss = 2.50008 avg_loss = 2.95833\n",
      "epoch no.5 train no.399200  loss = 4.55099 avg_loss = 2.98367\n",
      "epoch no.5 train no.399210  loss = 3.21792 avg_loss = 2.98280\n",
      "epoch no.5 train no.399220  loss = 1.83339 avg_loss = 2.98065\n",
      "epoch no.5 train no.399230  loss = 1.30579 avg_loss = 2.95577\n",
      "epoch no.5 train no.399240  loss = 3.53584 avg_loss = 2.98227\n",
      "epoch no.5 train no.399250  loss = 2.10750 avg_loss = 3.01181\n",
      "epoch no.5 train no.399260  loss = 1.68548 avg_loss = 3.00735\n",
      "epoch no.5 train no.399270  loss = 3.00535 avg_loss = 3.00773\n",
      "epoch no.5 train no.399280  loss = 3.85704 avg_loss = 2.98524\n",
      "epoch no.5 train no.399290  loss = 4.91058 avg_loss = 3.04001\n",
      "epoch no.5 train no.399300  loss = 4.62543 avg_loss = 3.03188\n",
      "epoch no.5 train no.399310  loss = 2.42157 avg_loss = 3.00926\n",
      "epoch no.5 train no.399320  loss = 5.50815 avg_loss = 3.05226\n",
      "epoch no.5 train no.399330  loss = 3.89634 avg_loss = 3.05577\n",
      "epoch no.5 train no.399340  loss = 3.07113 avg_loss = 3.08182\n",
      "epoch no.5 train no.399350  loss = 2.85669 avg_loss = 3.06898\n",
      "epoch no.5 train no.399360  loss = 2.37391 avg_loss = 3.04224\n",
      "epoch no.5 train no.399370  loss = 2.20856 avg_loss = 3.02581\n",
      "epoch no.5 train no.399380  loss = 2.76564 avg_loss = 3.00864\n",
      "epoch no.5 train no.399390  loss = 3.73069 avg_loss = 3.04388\n",
      "epoch no.5 train no.399400  loss = 3.11486 avg_loss = 3.04625\n",
      "epoch no.5 train no.399410  loss = 2.13147 avg_loss = 3.00950\n",
      "epoch no.5 train no.399420  loss = 1.74701 avg_loss = 3.01860\n",
      "epoch no.5 train no.399430  loss = 3.88882 avg_loss = 3.04777\n",
      "epoch no.5 train no.399440  loss = 3.86664 avg_loss = 3.04840\n",
      "epoch no.5 train no.399450  loss = 4.11647 avg_loss = 3.03893\n",
      "epoch no.5 train no.399460  loss = 2.76696 avg_loss = 3.01914\n",
      "epoch no.5 train no.399470  loss = 2.10437 avg_loss = 2.97051\n",
      "epoch no.5 train no.399480  loss = 2.89040 avg_loss = 2.97575\n",
      "epoch no.5 train no.399490  loss = 3.81243 avg_loss = 3.04126\n",
      "epoch no.5 train no.399500  loss = 3.75099 avg_loss = 3.03256\n",
      "epoch no.5 train no.399510  loss = 3.54909 avg_loss = 3.04325\n",
      "epoch no.5 train no.399520  loss = 3.56616 avg_loss = 3.08860\n",
      "epoch no.5 train no.399530  loss = 3.01491 avg_loss = 3.05643\n",
      "epoch no.5 train no.399540  loss = 3.57207 avg_loss = 3.04287\n",
      "epoch no.5 train no.399550  loss = 2.25853 avg_loss = 3.03172\n",
      "epoch no.5 train no.399560  loss = 2.24115 avg_loss = 3.00424\n",
      "epoch no.5 train no.399570  loss = 4.00626 avg_loss = 3.04285\n",
      "epoch no.5 train no.399580  loss = 2.93615 avg_loss = 3.02205\n",
      "epoch no.5 train no.399590  loss = 3.20441 avg_loss = 3.02904\n",
      "epoch no.5 train no.399600  loss = 2.54324 avg_loss = 2.99507\n",
      "epoch no.5 train no.399610  loss = 2.75698 avg_loss = 2.99485\n",
      "epoch no.5 train no.399620  loss = 2.11787 avg_loss = 2.97138\n",
      "epoch no.5 train no.399630  loss = 4.12364 avg_loss = 2.96193\n",
      "epoch no.5 train no.399640  loss = 3.49822 avg_loss = 2.99735\n",
      "epoch no.5 train no.399650  loss = 2.61012 avg_loss = 2.99749\n",
      "epoch no.5 train no.399660  loss = 4.13188 avg_loss = 2.99419\n",
      "epoch no.5 train no.399670  loss = 3.53316 avg_loss = 2.98896\n",
      "epoch no.5 train no.399680  loss = 2.35181 avg_loss = 2.97511\n",
      "epoch no.5 train no.399690  loss = 3.45244 avg_loss = 2.97739\n",
      "epoch no.5 train no.399700  loss = 2.12892 avg_loss = 2.96986\n",
      "epoch no.5 train no.399710  loss = 3.69421 avg_loss = 2.95486\n",
      "epoch no.5 train no.399720  loss = 2.20917 avg_loss = 2.95136\n",
      "epoch no.5 train no.399730  loss = 3.53050 avg_loss = 2.97960\n",
      "epoch no.5 train no.399740  loss = 1.84613 avg_loss = 2.99149\n",
      "epoch no.5 train no.399750  loss = 3.29541 avg_loss = 3.03538\n",
      "epoch no.5 train no.399760  loss = 5.66875 avg_loss = 3.09820\n",
      "epoch no.5 train no.399770  loss = 3.24947 avg_loss = 3.13376\n",
      "epoch no.5 train no.399780  loss = 2.22716 avg_loss = 3.10835\n",
      "epoch no.5 train no.399790  loss = 3.95762 avg_loss = 3.08380\n",
      "epoch no.5 train no.399800  loss = 3.00337 avg_loss = 3.07288\n",
      "epoch no.5 train no.399810  loss = 4.57776 avg_loss = 3.06590\n",
      "epoch no.5 train no.399820  loss = 3.13118 avg_loss = 3.05953\n",
      "epoch no.5 train no.399830  loss = 2.46069 avg_loss = 3.00295\n",
      "epoch no.5 train no.399840  loss = 2.01216 avg_loss = 3.01919\n",
      "epoch no.5 train no.399850  loss = 3.15479 avg_loss = 3.04170\n",
      "epoch no.5 train no.399860  loss = 3.25488 avg_loss = 3.02139\n",
      "epoch no.5 train no.399870  loss = 3.03790 avg_loss = 2.98475\n",
      "epoch no.5 train no.399880  loss = 2.25957 avg_loss = 2.97953\n",
      "epoch no.5 train no.399890  loss = 2.36814 avg_loss = 2.99427\n",
      "epoch no.5 train no.399900  loss = 4.28591 avg_loss = 2.99571\n",
      "epoch no.5 train no.399910  loss = 2.56448 avg_loss = 2.96588\n",
      "epoch no.5 train no.399920  loss = 2.88602 avg_loss = 2.96655\n",
      "epoch no.5 train no.399930  loss = 2.11429 avg_loss = 2.94621\n",
      "epoch no.5 train no.399940  loss = 6.67456 avg_loss = 2.99737\n",
      "epoch no.5 train no.399950  loss = 3.37452 avg_loss = 3.01488\n",
      "epoch no.5 train no.399960  loss = 3.02233 avg_loss = 2.99695\n",
      "epoch no.5 train no.399970  loss = 2.06479 avg_loss = 3.00984\n",
      "epoch no.5 train no.399980  loss = 2.35780 avg_loss = 2.98702\n",
      "epoch no.5 train no.399990  loss = 3.67841 avg_loss = 2.98474\n",
      "epoch no.5 train no.400000  loss = 4.35120 avg_loss = 3.01106\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '아이돌', '</s>']\n",
      "추억의 2000년대 여자아이돌</s>\n",
      "epoch no.5 train no.400010  loss = 3.04904 avg_loss = 3.10144\n",
      "epoch no.5 train no.400020  loss = 1.89351 avg_loss = 3.08528\n",
      "epoch no.5 train no.400030  loss = 4.47157 avg_loss = 3.10345\n",
      "epoch no.5 train no.400040  loss = 3.34702 avg_loss = 3.08349\n",
      "epoch no.5 train no.400050  loss = 3.42544 avg_loss = 3.11431\n",
      "epoch no.5 train no.400060  loss = 4.03696 avg_loss = 3.09977\n",
      "epoch no.5 train no.400070  loss = 2.29112 avg_loss = 3.09140\n",
      "epoch no.5 train no.400080  loss = 2.94595 avg_loss = 3.07357\n",
      "epoch no.5 train no.400090  loss = 3.33499 avg_loss = 3.01907\n",
      "epoch no.5 train no.400100  loss = 3.27431 avg_loss = 3.03516\n",
      "epoch no.5 train no.400110  loss = 2.13292 avg_loss = 3.02892\n",
      "epoch no.5 train no.400120  loss = 2.46529 avg_loss = 2.99929\n",
      "epoch no.5 train no.400130  loss = 4.54042 avg_loss = 3.00564\n",
      "epoch no.5 train no.400140  loss = 4.29371 avg_loss = 3.03337\n",
      "epoch no.5 train no.400150  loss = 5.02674 avg_loss = 3.02804\n",
      "epoch no.5 train no.400160  loss = 2.85650 avg_loss = 2.98709\n",
      "epoch no.5 train no.400170  loss = 2.69464 avg_loss = 2.96900\n",
      "epoch no.5 train no.400180  loss = 3.88451 avg_loss = 3.03871\n",
      "epoch no.5 train no.400190  loss = 1.49792 avg_loss = 3.06187\n",
      "epoch no.5 train no.400200  loss = 4.98365 avg_loss = 3.05835\n",
      "epoch no.5 train no.400210  loss = 3.16648 avg_loss = 3.05135\n",
      "epoch no.5 train no.400220  loss = 2.20551 avg_loss = 3.04835\n",
      "epoch no.5 train no.400230  loss = 2.35412 avg_loss = 3.07015\n",
      "epoch no.5 train no.400240  loss = 3.30802 avg_loss = 3.06464\n",
      "epoch no.5 train no.400250  loss = 2.21196 avg_loss = 3.02616\n",
      "epoch no.5 train no.400260  loss = 3.73157 avg_loss = 3.02488\n",
      "epoch no.5 train no.400270  loss = 4.63070 avg_loss = 3.01428\n",
      "epoch no.5 train no.400280  loss = 2.57780 avg_loss = 3.01157\n",
      "epoch no.5 train no.400290  loss = 2.74161 avg_loss = 2.99540\n",
      "epoch no.5 train no.400300  loss = 3.13436 avg_loss = 3.01696\n",
      "epoch no.5 train no.400310  loss = 4.25227 avg_loss = 3.00882\n",
      "epoch no.5 train no.400320  loss = 3.91735 avg_loss = 2.99166\n",
      "epoch no.5 train no.400330  loss = 3.14048 avg_loss = 2.99011\n",
      "epoch no.5 train no.400340  loss = 3.79435 avg_loss = 3.03659\n",
      "epoch no.5 train no.400350  loss = 2.15277 avg_loss = 2.99588\n",
      "epoch no.5 train no.400360  loss = 2.18278 avg_loss = 2.99848\n",
      "epoch no.5 train no.400370  loss = 3.95509 avg_loss = 3.00569\n",
      "epoch no.5 train no.400380  loss = 3.27091 avg_loss = 3.00273\n",
      "epoch no.5 train no.400390  loss = 2.41026 avg_loss = 2.97481\n",
      "epoch no.5 train no.400400  loss = 2.17293 avg_loss = 2.97213\n",
      "epoch no.5 train no.400410  loss = 2.12337 avg_loss = 2.97934\n",
      "epoch no.5 train no.400420  loss = 2.61982 avg_loss = 2.94053\n",
      "epoch no.5 train no.400430  loss = 3.77719 avg_loss = 2.97736\n",
      "epoch no.5 train no.400440  loss = 2.30789 avg_loss = 2.99400\n",
      "epoch no.5 train no.400450  loss = 2.18919 avg_loss = 2.97615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.400460  loss = 3.03343 avg_loss = 2.97818\n",
      "epoch no.5 train no.400470  loss = 2.77544 avg_loss = 2.95243\n",
      "epoch no.5 train no.400480  loss = 3.03322 avg_loss = 2.93905\n",
      "epoch no.5 train no.400490  loss = 2.02553 avg_loss = 2.93138\n",
      "epoch no.5 train no.400500  loss = 3.24587 avg_loss = 2.91508\n",
      "epoch no.5 train no.400510  loss = 2.24828 avg_loss = 2.91936\n",
      "epoch no.5 train no.400520  loss = 2.00250 avg_loss = 2.93481\n",
      "epoch no.5 train no.400530  loss = 3.68185 avg_loss = 2.92073\n",
      "epoch no.5 train no.400540  loss = 5.20574 avg_loss = 2.95338\n",
      "epoch no.5 train no.400550  loss = 2.24098 avg_loss = 2.91123\n",
      "epoch no.5 train no.400560  loss = 2.46639 avg_loss = 2.91215\n",
      "epoch no.5 train no.400570  loss = 2.83661 avg_loss = 2.93513\n",
      "epoch no.5 train no.400580  loss = 2.65627 avg_loss = 2.90904\n",
      "epoch no.5 train no.400590  loss = 3.08054 avg_loss = 2.88189\n",
      "epoch no.5 train no.400600  loss = 3.74363 avg_loss = 2.93898\n",
      "epoch no.5 train no.400610  loss = 2.75328 avg_loss = 2.93228\n",
      "epoch no.5 train no.400620  loss = 2.70100 avg_loss = 2.92337\n",
      "epoch no.5 train no.400630  loss = 2.10916 avg_loss = 2.95063\n",
      "epoch no.5 train no.400640  loss = 2.98754 avg_loss = 2.93775\n",
      "epoch no.5 train no.400650  loss = 3.44356 avg_loss = 2.95187\n",
      "epoch no.5 train no.400660  loss = 2.25275 avg_loss = 2.94726\n",
      "epoch no.5 train no.400670  loss = 2.98886 avg_loss = 2.96577\n",
      "epoch no.5 train no.400680  loss = 4.06279 avg_loss = 2.98745\n",
      "epoch no.5 train no.400690  loss = 3.28764 avg_loss = 2.96509\n",
      "epoch no.5 train no.400700  loss = 2.37615 avg_loss = 2.96683\n",
      "epoch no.5 train no.400710  loss = 3.28159 avg_loss = 2.96311\n",
      "epoch no.5 train no.400720  loss = 3.17625 avg_loss = 2.95578\n",
      "epoch no.5 train no.400730  loss = 2.84864 avg_loss = 2.93640\n",
      "epoch no.5 train no.400740  loss = 3.47374 avg_loss = 2.91862\n",
      "epoch no.5 train no.400750  loss = 2.83903 avg_loss = 2.89588\n",
      "epoch no.5 train no.400760  loss = 2.69048 avg_loss = 2.87551\n",
      "epoch no.5 train no.400770  loss = 2.74144 avg_loss = 2.89237\n",
      "epoch no.5 train no.400780  loss = 3.98661 avg_loss = 2.90973\n",
      "epoch no.5 train no.400790  loss = 2.46922 avg_loss = 2.91368\n",
      "epoch no.5 train no.400800  loss = 2.18011 avg_loss = 2.91018\n",
      "epoch no.5 train no.400810  loss = 4.37898 avg_loss = 2.93568\n",
      "epoch no.5 train no.400820  loss = 5.02312 avg_loss = 2.96000\n",
      "epoch no.5 train no.400830  loss = 3.29316 avg_loss = 2.94554\n",
      "epoch no.5 train no.400840  loss = 1.94710 avg_loss = 2.92595\n",
      "epoch no.5 train no.400850  loss = 2.33530 avg_loss = 2.93536\n",
      "epoch no.5 train no.400860  loss = 3.85071 avg_loss = 2.95861\n",
      "epoch no.5 train no.400870  loss = 3.54517 avg_loss = 2.97846\n",
      "epoch no.5 train no.400880  loss = 2.64251 avg_loss = 3.00890\n",
      "epoch no.5 train no.400890  loss = 2.06723 avg_loss = 2.99646\n",
      "epoch no.5 train no.400900  loss = 4.05038 avg_loss = 3.00483\n",
      "epoch no.5 train no.400910  loss = 4.94584 avg_loss = 3.03972\n",
      "epoch no.5 train no.400920  loss = 3.63867 avg_loss = 3.05898\n",
      "epoch no.5 train no.400930  loss = 2.55715 avg_loss = 3.02568\n",
      "epoch no.5 train no.400940  loss = 4.43236 avg_loss = 3.02543\n",
      "epoch no.5 train no.400950  loss = 1.74452 avg_loss = 3.01398\n",
      "epoch no.5 train no.400960  loss = 2.90354 avg_loss = 3.02108\n",
      "epoch no.5 train no.400970  loss = 2.45394 avg_loss = 3.02873\n",
      "epoch no.5 train no.400980  loss = 3.78557 avg_loss = 3.04027\n",
      "epoch no.5 train no.400990  loss = 2.66509 avg_loss = 3.02430\n",
      "epoch no.5 train no.401000  loss = 1.88663 avg_loss = 3.00594\n",
      "6\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', 'g', 'm', '▁모음', '</s>']\n",
      "추억의 싸이월드 bgm 모음</s>\n",
      "epoch no.5 train no.401010  loss = 2.03524 avg_loss = 3.02776\n",
      "epoch no.5 train no.401020  loss = 3.85665 avg_loss = 3.02072\n",
      "epoch no.5 train no.401030  loss = 2.50709 avg_loss = 3.01388\n",
      "epoch no.5 train no.401040  loss = 2.38236 avg_loss = 3.00232\n",
      "epoch no.5 train no.401050  loss = 3.16796 avg_loss = 2.97992\n",
      "epoch no.5 train no.401060  loss = 2.62853 avg_loss = 3.00992\n",
      "epoch no.5 train no.401070  loss = 2.67475 avg_loss = 2.98182\n",
      "epoch no.5 train no.401080  loss = 4.32873 avg_loss = 3.00116\n",
      "epoch no.5 train no.401090  loss = 2.75544 avg_loss = 2.99351\n",
      "epoch no.5 train no.401100  loss = 3.73637 avg_loss = 3.02519\n",
      "epoch no.5 train no.401110  loss = 2.39887 avg_loss = 3.06101\n",
      "epoch no.5 train no.401120  loss = 1.53027 avg_loss = 3.00477\n",
      "epoch no.5 train no.401130  loss = 2.49094 avg_loss = 3.02922\n",
      "epoch no.5 train no.401140  loss = 1.94608 avg_loss = 3.00365\n",
      "epoch no.5 train no.401150  loss = 2.16749 avg_loss = 2.99496\n",
      "epoch no.5 train no.401160  loss = 2.01471 avg_loss = 2.98367\n",
      "epoch no.5 train no.401170  loss = 3.09546 avg_loss = 3.00307\n",
      "epoch no.5 train no.401180  loss = 3.61684 avg_loss = 3.02181\n",
      "epoch no.5 train no.401190  loss = 3.56863 avg_loss = 3.04256\n",
      "epoch no.5 train no.401200  loss = 2.71881 avg_loss = 3.05215\n",
      "epoch no.5 train no.401210  loss = 3.79041 avg_loss = 3.04847\n",
      "epoch no.5 train no.401220  loss = 2.82430 avg_loss = 3.03701\n",
      "epoch no.5 train no.401230  loss = 3.15801 avg_loss = 3.02647\n",
      "epoch no.5 train no.401240  loss = 2.71579 avg_loss = 2.98864\n",
      "epoch no.5 train no.401250  loss = 4.32385 avg_loss = 3.02264\n",
      "epoch no.5 train no.401260  loss = 3.20417 avg_loss = 3.01360\n",
      "epoch no.5 train no.401270  loss = 2.47751 avg_loss = 2.98905\n",
      "epoch no.5 train no.401280  loss = 3.49120 avg_loss = 2.99099\n",
      "epoch no.5 train no.401290  loss = 2.23562 avg_loss = 2.96420\n",
      "epoch no.5 train no.401300  loss = 2.59714 avg_loss = 3.00365\n",
      "epoch no.5 train no.401310  loss = 4.02792 avg_loss = 2.97456\n",
      "epoch no.5 train no.401320  loss = 2.33213 avg_loss = 2.99006\n",
      "epoch no.5 train no.401330  loss = 2.83629 avg_loss = 2.95579\n",
      "epoch no.5 train no.401340  loss = 4.04178 avg_loss = 2.95357\n",
      "epoch no.5 train no.401350  loss = 2.96921 avg_loss = 2.95983\n",
      "epoch no.5 train no.401360  loss = 2.45510 avg_loss = 2.93193\n",
      "epoch no.5 train no.401370  loss = 2.50939 avg_loss = 2.98358\n",
      "epoch no.5 train no.401380  loss = 1.90222 avg_loss = 2.95649\n",
      "epoch no.5 train no.401390  loss = 1.91805 avg_loss = 3.02911\n",
      "epoch no.5 train no.401400  loss = 4.26604 avg_loss = 3.04682\n",
      "epoch no.5 train no.401410  loss = 2.31437 avg_loss = 3.07408\n",
      "epoch no.5 train no.401420  loss = 4.40400 avg_loss = 3.08369\n",
      "epoch no.5 train no.401430  loss = 4.27194 avg_loss = 3.11332\n",
      "epoch no.5 train no.401440  loss = 2.28004 avg_loss = 3.08206\n",
      "epoch no.5 train no.401450  loss = 1.91552 avg_loss = 3.09599\n",
      "epoch no.5 train no.401460  loss = 2.91227 avg_loss = 3.08335\n",
      "epoch no.5 train no.401470  loss = 3.43358 avg_loss = 3.10007\n",
      "epoch no.5 train no.401480  loss = 3.96621 avg_loss = 3.10630\n",
      "epoch no.5 train no.401490  loss = 3.43678 avg_loss = 3.07693\n",
      "epoch no.5 train no.401500  loss = 3.26542 avg_loss = 3.07424\n",
      "epoch no.5 train no.401510  loss = 4.08351 avg_loss = 3.07505\n",
      "epoch no.5 train no.401520  loss = 2.04421 avg_loss = 3.09761\n",
      "epoch no.5 train no.401530  loss = 3.19753 avg_loss = 3.04944\n",
      "epoch no.5 train no.401540  loss = 2.53750 avg_loss = 3.01227\n",
      "epoch no.5 train no.401550  loss = 3.62414 avg_loss = 3.04876\n",
      "epoch no.5 train no.401560  loss = 2.96366 avg_loss = 3.01326\n",
      "epoch no.5 train no.401570  loss = 5.39748 avg_loss = 3.02237\n",
      "epoch no.5 train no.401580  loss = 2.48703 avg_loss = 3.03352\n",
      "epoch no.5 train no.401590  loss = 3.69997 avg_loss = 3.01288\n",
      "epoch no.5 train no.401600  loss = 4.43150 avg_loss = 3.01907\n",
      "epoch no.5 train no.401610  loss = 2.91030 avg_loss = 3.02171\n",
      "epoch no.5 train no.401620  loss = 2.74965 avg_loss = 3.02403\n",
      "epoch no.5 train no.401630  loss = 3.51539 avg_loss = 2.99372\n",
      "epoch no.5 train no.401640  loss = 3.30647 avg_loss = 3.06589\n",
      "epoch no.5 train no.401650  loss = 2.48119 avg_loss = 3.03971\n",
      "epoch no.5 train no.401660  loss = 2.82224 avg_loss = 3.06227\n",
      "epoch no.5 train no.401670  loss = 2.99729 avg_loss = 3.05133\n",
      "epoch no.5 train no.401680  loss = 3.08769 avg_loss = 3.06948\n",
      "epoch no.5 train no.401690  loss = 4.16766 avg_loss = 3.10700\n",
      "epoch no.5 train no.401700  loss = 3.16171 avg_loss = 3.11051\n",
      "epoch no.5 train no.401710  loss = 2.65071 avg_loss = 3.09149\n",
      "epoch no.5 train no.401720  loss = 2.21553 avg_loss = 3.04895\n",
      "epoch no.5 train no.401730  loss = 3.13438 avg_loss = 3.08070\n",
      "epoch no.5 train no.401740  loss = 3.06073 avg_loss = 3.09139\n",
      "epoch no.5 train no.401750  loss = 2.94563 avg_loss = 3.06085\n",
      "epoch no.5 train no.401760  loss = 3.06518 avg_loss = 3.08260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.401770  loss = 2.44269 avg_loss = 3.06331\n",
      "epoch no.5 train no.401780  loss = 2.92744 avg_loss = 3.06229\n",
      "epoch no.5 train no.401790  loss = 2.97350 avg_loss = 3.08422\n",
      "epoch no.5 train no.401800  loss = 2.58780 avg_loss = 3.07746\n",
      "epoch no.5 train no.401810  loss = 2.83038 avg_loss = 3.09567\n",
      "epoch no.5 train no.401820  loss = 4.47376 avg_loss = 3.08073\n",
      "epoch no.5 train no.401830  loss = 3.26501 avg_loss = 3.10692\n",
      "epoch no.5 train no.401840  loss = 2.33006 avg_loss = 3.11719\n",
      "epoch no.5 train no.401850  loss = 1.74255 avg_loss = 3.13154\n",
      "epoch no.5 train no.401860  loss = 3.60771 avg_loss = 3.16496\n",
      "epoch no.5 train no.401870  loss = 3.29044 avg_loss = 3.16837\n",
      "epoch no.5 train no.401880  loss = 3.33913 avg_loss = 3.13662\n",
      "epoch no.5 train no.401890  loss = 3.45210 avg_loss = 3.12601\n",
      "epoch no.5 train no.401900  loss = 3.39525 avg_loss = 3.10405\n",
      "epoch no.5 train no.401910  loss = 3.55356 avg_loss = 3.08735\n",
      "epoch no.5 train no.401920  loss = 3.69313 avg_loss = 3.09510\n",
      "epoch no.5 train no.401930  loss = 3.44058 avg_loss = 3.11120\n",
      "epoch no.5 train no.401940  loss = 2.84860 avg_loss = 3.11957\n",
      "epoch no.5 train no.401950  loss = 2.37112 avg_loss = 3.12374\n",
      "epoch no.5 train no.401960  loss = 3.35727 avg_loss = 3.13878\n",
      "epoch no.5 train no.401970  loss = 3.53599 avg_loss = 3.16568\n",
      "epoch no.5 train no.401980  loss = 2.36950 avg_loss = 3.18401\n",
      "epoch no.5 train no.401990  loss = 3.72959 avg_loss = 3.15342\n",
      "epoch no.5 train no.402000  loss = 3.30616 avg_loss = 3.15253\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.5 train no.402010  loss = 3.74746 avg_loss = 3.14105\n",
      "epoch no.5 train no.402020  loss = 3.05263 avg_loss = 3.08847\n",
      "epoch no.5 train no.402030  loss = 3.31641 avg_loss = 3.10863\n",
      "epoch no.5 train no.402040  loss = 3.20700 avg_loss = 3.10176\n",
      "epoch no.5 train no.402050  loss = 2.41882 avg_loss = 3.08629\n",
      "epoch no.5 train no.402060  loss = 7.01168 avg_loss = 3.15324\n",
      "epoch no.5 train no.402070  loss = 2.84622 avg_loss = 3.15395\n",
      "epoch no.5 train no.402080  loss = 3.61145 avg_loss = 3.12206\n",
      "epoch no.5 train no.402090  loss = 3.51850 avg_loss = 3.10216\n",
      "epoch no.5 train no.402100  loss = 2.95819 avg_loss = 3.11302\n",
      "epoch no.5 train no.402110  loss = 2.63081 avg_loss = 3.10303\n",
      "epoch no.5 train no.402120  loss = 2.89629 avg_loss = 3.06098\n",
      "epoch no.5 train no.402130  loss = 4.04946 avg_loss = 3.08638\n",
      "epoch no.5 train no.402140  loss = 4.32504 avg_loss = 3.10927\n",
      "epoch no.5 train no.402150  loss = 2.82163 avg_loss = 3.08738\n",
      "epoch no.5 train no.402160  loss = 2.75842 avg_loss = 3.12121\n",
      "epoch no.5 train no.402170  loss = 2.96741 avg_loss = 3.08157\n",
      "epoch no.5 train no.402180  loss = 2.83594 avg_loss = 3.07706\n",
      "epoch no.5 train no.402190  loss = 3.23748 avg_loss = 3.06851\n",
      "epoch no.5 train no.402200  loss = 2.05382 avg_loss = 3.04799\n",
      "epoch no.5 train no.402210  loss = 4.56238 avg_loss = 3.05324\n",
      "epoch no.5 train no.402220  loss = 3.46320 avg_loss = 3.06012\n",
      "epoch no.5 train no.402230  loss = 1.98988 avg_loss = 3.05881\n",
      "epoch no.5 train no.402240  loss = 3.36536 avg_loss = 3.03250\n",
      "epoch no.5 train no.402250  loss = 3.63448 avg_loss = 3.02300\n",
      "epoch no.5 train no.402260  loss = 3.30919 avg_loss = 3.03815\n",
      "epoch no.5 train no.402270  loss = 3.55278 avg_loss = 3.05577\n",
      "epoch no.5 train no.402280  loss = 3.32233 avg_loss = 3.04742\n",
      "epoch no.5 train no.402290  loss = 2.85548 avg_loss = 3.05081\n",
      "epoch no.5 train no.402300  loss = 2.86869 avg_loss = 3.00729\n",
      "epoch no.5 train no.402310  loss = 2.67427 avg_loss = 3.00153\n",
      "epoch no.5 train no.402320  loss = 3.79985 avg_loss = 2.99982\n",
      "epoch no.5 train no.402330  loss = 3.06849 avg_loss = 2.98060\n",
      "epoch no.5 train no.402340  loss = 2.91123 avg_loss = 2.99787\n",
      "epoch no.5 train no.402350  loss = 3.50999 avg_loss = 3.00500\n",
      "epoch no.5 train no.402360  loss = 2.70976 avg_loss = 2.99566\n",
      "epoch no.5 train no.402370  loss = 5.45373 avg_loss = 3.01553\n",
      "epoch no.5 train no.402380  loss = 2.29156 avg_loss = 3.01673\n",
      "epoch no.5 train no.402390  loss = 3.37006 avg_loss = 3.04286\n",
      "epoch no.5 train no.402400  loss = 2.58998 avg_loss = 3.05979\n",
      "epoch no.5 train no.402410  loss = 3.12317 avg_loss = 3.10069\n",
      "epoch no.5 train no.402420  loss = 2.23549 avg_loss = 3.06231\n",
      "epoch no.5 train no.402430  loss = 3.10713 avg_loss = 3.09026\n",
      "epoch no.5 train no.402440  loss = 3.81891 avg_loss = 3.12848\n",
      "epoch no.5 train no.402450  loss = 2.49770 avg_loss = 3.12540\n",
      "epoch no.5 train no.402460  loss = 2.31358 avg_loss = 3.09446\n",
      "epoch no.5 train no.402470  loss = 2.57935 avg_loss = 3.09073\n",
      "epoch no.5 train no.402480  loss = 3.44705 avg_loss = 3.10033\n",
      "epoch no.5 train no.402490  loss = 2.62200 avg_loss = 3.13667\n",
      "epoch no.5 train no.402500  loss = 4.17726 avg_loss = 3.13359\n",
      "epoch no.5 train no.402510  loss = 2.70018 avg_loss = 3.07567\n",
      "epoch no.5 train no.402520  loss = 3.79130 avg_loss = 3.06887\n",
      "epoch no.5 train no.402530  loss = 4.03381 avg_loss = 3.06291\n",
      "epoch no.5 train no.402540  loss = 2.35860 avg_loss = 3.07650\n",
      "epoch no.5 train no.402550  loss = 2.51734 avg_loss = 3.05833\n",
      "epoch no.5 train no.402560  loss = 3.06327 avg_loss = 3.09106\n",
      "epoch no.5 train no.402570  loss = 2.71831 avg_loss = 3.04900\n",
      "epoch no.5 train no.402580  loss = 2.26216 avg_loss = 3.00975\n",
      "epoch no.5 train no.402590  loss = 1.71780 avg_loss = 3.01276\n",
      "epoch no.5 train no.402600  loss = 2.26541 avg_loss = 2.99601\n",
      "epoch no.5 train no.402610  loss = 2.18828 avg_loss = 3.00447\n",
      "epoch no.5 train no.402620  loss = 2.35979 avg_loss = 2.99401\n",
      "epoch no.5 train no.402630  loss = 2.44394 avg_loss = 2.98762\n",
      "epoch no.5 train no.402640  loss = 2.91728 avg_loss = 2.96361\n",
      "epoch no.5 train no.402650  loss = 3.44682 avg_loss = 2.92419\n",
      "epoch no.5 train no.402660  loss = 1.79855 avg_loss = 2.94924\n",
      "epoch no.5 train no.402670  loss = 2.91005 avg_loss = 2.95758\n",
      "epoch no.5 train no.402680  loss = 2.94443 avg_loss = 2.97057\n",
      "epoch no.5 train no.402690  loss = 3.15454 avg_loss = 2.97123\n",
      "epoch no.5 train no.402700  loss = 2.07038 avg_loss = 2.95462\n",
      "epoch no.5 train no.402710  loss = 2.79134 avg_loss = 2.90705\n",
      "epoch no.5 train no.402720  loss = 2.45214 avg_loss = 2.91328\n",
      "epoch no.5 train no.402730  loss = 1.73868 avg_loss = 2.89796\n",
      "epoch no.5 train no.402740  loss = 3.70252 avg_loss = 2.91684\n",
      "epoch no.5 train no.402750  loss = 1.75044 avg_loss = 2.90554\n",
      "epoch no.5 train no.402760  loss = 2.84801 avg_loss = 2.89573\n",
      "epoch no.5 train no.402770  loss = 4.17232 avg_loss = 2.92892\n",
      "epoch no.5 train no.402780  loss = 4.64147 avg_loss = 2.98533\n",
      "epoch no.5 train no.402790  loss = 3.82338 avg_loss = 2.98338\n",
      "epoch no.5 train no.402800  loss = 3.18725 avg_loss = 2.97111\n",
      "epoch no.5 train no.402810  loss = 2.98295 avg_loss = 2.97328\n",
      "epoch no.5 train no.402820  loss = 3.04596 avg_loss = 2.99801\n",
      "epoch no.5 train no.402830  loss = 2.96958 avg_loss = 2.97734\n",
      "epoch no.5 train no.402840  loss = 2.74216 avg_loss = 3.03730\n",
      "epoch no.5 train no.402850  loss = 2.59821 avg_loss = 3.02923\n",
      "epoch no.5 train no.402860  loss = 3.51627 avg_loss = 3.03471\n",
      "epoch no.5 train no.402870  loss = 3.11078 avg_loss = 3.01555\n",
      "epoch no.5 train no.402880  loss = 3.19329 avg_loss = 3.01115\n",
      "epoch no.5 train no.402890  loss = 3.11479 avg_loss = 3.03421\n",
      "epoch no.5 train no.402900  loss = 3.76246 avg_loss = 3.04272\n",
      "epoch no.5 train no.402910  loss = 2.85334 avg_loss = 3.06863\n",
      "epoch no.5 train no.402920  loss = 3.13538 avg_loss = 3.08506\n",
      "epoch no.5 train no.402930  loss = 3.47918 avg_loss = 3.09174\n",
      "epoch no.5 train no.402940  loss = 2.49649 avg_loss = 3.08647\n",
      "epoch no.5 train no.402950  loss = 3.76501 avg_loss = 3.06340\n",
      "epoch no.5 train no.402960  loss = 2.72694 avg_loss = 3.08158\n",
      "epoch no.5 train no.402970  loss = 2.87329 avg_loss = 3.07196\n",
      "epoch no.5 train no.402980  loss = 1.77401 avg_loss = 3.08622\n",
      "epoch no.5 train no.402990  loss = 4.10253 avg_loss = 3.08140\n",
      "epoch no.5 train no.403000  loss = 2.74166 avg_loss = 3.07290\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '▁o', 'st', '▁모음', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.5 train no.403010  loss = 4.13785 avg_loss = 3.07192\n",
      "epoch no.5 train no.403020  loss = 1.74857 avg_loss = 3.03145\n",
      "epoch no.5 train no.403030  loss = 4.79414 avg_loss = 3.03210\n",
      "epoch no.5 train no.403040  loss = 2.22485 avg_loss = 3.02359\n",
      "epoch no.5 train no.403050  loss = 3.46010 avg_loss = 3.02573\n",
      "epoch no.5 train no.403060  loss = 3.38928 avg_loss = 2.99660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.403070  loss = 2.59926 avg_loss = 3.01037\n",
      "epoch no.5 train no.403080  loss = 3.12898 avg_loss = 3.02330\n",
      "epoch no.5 train no.403090  loss = 3.15397 avg_loss = 3.02115\n",
      "epoch no.5 train no.403100  loss = 2.36575 avg_loss = 3.00064\n",
      "epoch no.5 train no.403110  loss = 3.32945 avg_loss = 2.99006\n",
      "epoch no.5 train no.403120  loss = 3.30650 avg_loss = 3.01061\n",
      "epoch no.5 train no.403130  loss = 2.63961 avg_loss = 3.00003\n",
      "epoch no.5 train no.403140  loss = 3.74651 avg_loss = 3.00679\n",
      "epoch no.5 train no.403150  loss = 4.29629 avg_loss = 3.05367\n",
      "epoch no.5 train no.403160  loss = 3.67670 avg_loss = 3.03150\n",
      "epoch no.5 train no.403170  loss = 3.84353 avg_loss = 3.04881\n",
      "epoch no.5 train no.403180  loss = 1.58799 avg_loss = 3.06238\n",
      "epoch no.5 train no.403190  loss = 3.00223 avg_loss = 3.08025\n",
      "epoch no.5 train no.403200  loss = 1.61681 avg_loss = 3.04078\n",
      "epoch no.5 train no.403210  loss = 2.18546 avg_loss = 3.02038\n",
      "epoch no.5 train no.403220  loss = 3.15597 avg_loss = 2.99271\n",
      "epoch no.5 train no.403230  loss = 2.77128 avg_loss = 2.94704\n",
      "epoch no.5 train no.403240  loss = 3.16977 avg_loss = 2.93430\n",
      "epoch no.5 train no.403250  loss = 4.18314 avg_loss = 2.97378\n",
      "epoch no.5 train no.403260  loss = 3.37856 avg_loss = 2.95759\n",
      "epoch no.5 train no.403270  loss = 3.28449 avg_loss = 2.97264\n",
      "epoch no.5 train no.403280  loss = 2.40083 avg_loss = 2.98218\n",
      "epoch no.5 train no.403290  loss = 3.65238 avg_loss = 3.03271\n",
      "epoch no.5 train no.403300  loss = 3.58506 avg_loss = 3.02610\n",
      "epoch no.5 train no.403310  loss = 2.46357 avg_loss = 3.02342\n",
      "epoch no.5 train no.403320  loss = 1.65647 avg_loss = 3.00435\n",
      "epoch no.5 train no.403330  loss = 3.23865 avg_loss = 3.02134\n",
      "epoch no.5 train no.403340  loss = 3.65536 avg_loss = 3.02686\n",
      "epoch no.5 train no.403350  loss = 4.35532 avg_loss = 3.00750\n",
      "epoch no.5 train no.403360  loss = 2.16490 avg_loss = 3.00757\n",
      "epoch no.5 train no.403370  loss = 4.18766 avg_loss = 2.98433\n",
      "epoch no.5 train no.403380  loss = 2.51232 avg_loss = 2.98876\n",
      "epoch no.5 train no.403390  loss = 3.30635 avg_loss = 3.00822\n",
      "epoch no.5 train no.403400  loss = 2.27098 avg_loss = 2.96871\n",
      "epoch no.5 train no.403410  loss = 1.93215 avg_loss = 2.93899\n",
      "epoch no.5 train no.403420  loss = 4.30949 avg_loss = 2.92966\n",
      "epoch no.5 train no.403430  loss = 4.32944 avg_loss = 2.95199\n",
      "epoch no.5 train no.403440  loss = 2.82967 avg_loss = 2.98637\n",
      "epoch no.5 train no.403450  loss = 1.91256 avg_loss = 2.95495\n",
      "epoch no.5 train no.403460  loss = 2.10997 avg_loss = 2.95043\n",
      "epoch no.5 train no.403470  loss = 3.05728 avg_loss = 2.94074\n",
      "epoch no.5 train no.403480  loss = 4.05571 avg_loss = 2.93990\n",
      "epoch no.5 train no.403490  loss = 2.86977 avg_loss = 2.94701\n",
      "epoch no.5 train no.403500  loss = 3.10071 avg_loss = 2.95489\n",
      "epoch no.5 train no.403510  loss = 5.75048 avg_loss = 3.00015\n",
      "epoch no.5 train no.403520  loss = 2.54684 avg_loss = 3.00347\n",
      "epoch no.5 train no.403530  loss = 2.55972 avg_loss = 3.03946\n",
      "epoch no.5 train no.403540  loss = 2.73196 avg_loss = 3.07729\n",
      "epoch no.5 train no.403550  loss = 2.36400 avg_loss = 3.07238\n",
      "epoch no.5 train no.403560  loss = 2.17030 avg_loss = 3.03458\n",
      "epoch no.5 train no.403570  loss = 3.24301 avg_loss = 2.99650\n",
      "epoch no.5 train no.403580  loss = 2.37278 avg_loss = 2.98390\n",
      "epoch no.5 train no.403590  loss = 1.70163 avg_loss = 2.99601\n",
      "epoch no.5 train no.403600  loss = 3.33461 avg_loss = 2.98968\n",
      "epoch no.5 train no.403610  loss = 1.80738 avg_loss = 2.96675\n",
      "epoch no.5 train no.403620  loss = 3.84913 avg_loss = 2.97202\n",
      "epoch no.5 train no.403630  loss = 2.41386 avg_loss = 2.97402\n",
      "epoch no.5 train no.403640  loss = 2.97110 avg_loss = 2.97417\n",
      "epoch no.5 train no.403650  loss = 3.36718 avg_loss = 2.97727\n",
      "epoch no.5 train no.403660  loss = 2.88771 avg_loss = 2.98804\n",
      "epoch no.5 train no.403670  loss = 4.41729 avg_loss = 3.06726\n",
      "epoch no.5 train no.403680  loss = 4.12677 avg_loss = 3.08264\n",
      "epoch no.5 train no.403690  loss = 3.07600 avg_loss = 3.04113\n",
      "epoch no.5 train no.403700  loss = 2.66085 avg_loss = 3.03869\n",
      "epoch no.5 train no.403710  loss = 2.26340 avg_loss = 3.01323\n",
      "epoch no.5 train no.403720  loss = 2.34680 avg_loss = 3.01776\n",
      "epoch no.5 train no.403730  loss = 4.86873 avg_loss = 3.02415\n",
      "epoch no.5 train no.403740  loss = 5.09724 avg_loss = 3.05271\n",
      "epoch no.5 train no.403750  loss = 3.13612 avg_loss = 3.07640\n",
      "epoch no.5 train no.403760  loss = 2.15374 avg_loss = 3.07196\n",
      "epoch no.5 train no.403770  loss = 6.14800 avg_loss = 3.09390\n",
      "epoch no.5 train no.403780  loss = 2.57252 avg_loss = 3.08230\n",
      "epoch no.5 train no.403790  loss = 1.74399 avg_loss = 3.03818\n",
      "epoch no.5 train no.403800  loss = 2.55182 avg_loss = 3.02295\n",
      "epoch no.5 train no.403810  loss = 2.48198 avg_loss = 3.00361\n",
      "epoch no.5 train no.403820  loss = 1.91280 avg_loss = 2.95896\n",
      "epoch no.5 train no.403830  loss = 4.53550 avg_loss = 2.97447\n",
      "epoch no.5 train no.403840  loss = 2.92609 avg_loss = 2.95854\n",
      "epoch no.5 train no.403850  loss = 2.00988 avg_loss = 2.93886\n",
      "epoch no.5 train no.403860  loss = 3.50856 avg_loss = 2.98108\n",
      "epoch no.5 train no.403870  loss = 2.58422 avg_loss = 2.94725\n",
      "epoch no.5 train no.403880  loss = 2.77951 avg_loss = 2.94874\n",
      "epoch no.5 train no.403890  loss = 3.24661 avg_loss = 3.00568\n",
      "epoch no.5 train no.403900  loss = 3.36014 avg_loss = 3.00828\n",
      "epoch no.5 train no.403910  loss = 2.52504 avg_loss = 2.99211\n",
      "epoch no.5 train no.403920  loss = 2.51852 avg_loss = 2.97977\n",
      "epoch no.5 train no.403930  loss = 1.44913 avg_loss = 2.95116\n",
      "epoch no.5 train no.403940  loss = 2.05401 avg_loss = 2.92970\n",
      "epoch no.5 train no.403950  loss = 3.17031 avg_loss = 2.93332\n",
      "epoch no.5 train no.403960  loss = 4.64060 avg_loss = 2.95345\n",
      "epoch no.5 train no.403970  loss = 3.27972 avg_loss = 2.94932\n",
      "epoch no.5 train no.403980  loss = 3.17337 avg_loss = 2.91280\n",
      "epoch no.5 train no.403990  loss = 2.22518 avg_loss = 2.89369\n",
      "epoch no.5 train no.404000  loss = 5.30983 avg_loss = 2.93789\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '월드', '▁b', '▁b', '</s>']\n",
      "추억의 싸이월드 인기곡</s>\n",
      "epoch no.5 train no.404010  loss = 4.12021 avg_loss = 2.98758\n",
      "epoch no.5 train no.404020  loss = 3.12936 avg_loss = 2.98321\n",
      "epoch no.5 train no.404030  loss = 3.35802 avg_loss = 3.00385\n",
      "epoch no.5 train no.404040  loss = 2.64252 avg_loss = 3.03398\n",
      "epoch no.5 train no.404050  loss = 3.34948 avg_loss = 3.06991\n",
      "epoch no.5 train no.404060  loss = 3.52283 avg_loss = 3.03809\n",
      "epoch no.5 train no.404070  loss = 3.64954 avg_loss = 3.03555\n",
      "epoch no.5 train no.404080  loss = 2.90515 avg_loss = 3.04295\n",
      "epoch no.5 train no.404090  loss = 2.11475 avg_loss = 3.03460\n",
      "epoch no.5 train no.404100  loss = 3.52526 avg_loss = 3.06107\n",
      "epoch no.5 train no.404110  loss = 2.15288 avg_loss = 3.05354\n",
      "epoch no.5 train no.404120  loss = 2.74023 avg_loss = 3.05092\n",
      "epoch no.5 train no.404130  loss = 4.98240 avg_loss = 3.06917\n",
      "epoch no.5 train no.404140  loss = 3.38751 avg_loss = 3.12630\n",
      "epoch no.5 train no.404150  loss = 3.52991 avg_loss = 3.12715\n",
      "epoch no.5 train no.404160  loss = 2.98118 avg_loss = 3.14066\n",
      "epoch no.5 train no.404170  loss = 4.68996 avg_loss = 3.16830\n",
      "epoch no.5 train no.404180  loss = 2.99072 avg_loss = 3.16074\n",
      "epoch no.5 train no.404190  loss = 2.39999 avg_loss = 3.11709\n",
      "epoch no.5 train no.404200  loss = 3.13241 avg_loss = 3.09408\n",
      "epoch no.5 train no.404210  loss = 3.49206 avg_loss = 3.06345\n",
      "epoch no.5 train no.404220  loss = 2.02773 avg_loss = 3.00415\n",
      "epoch no.5 train no.404230  loss = 3.77656 avg_loss = 3.02077\n",
      "epoch no.5 train no.404240  loss = 2.04048 avg_loss = 3.01844\n",
      "epoch no.5 train no.404250  loss = 2.05565 avg_loss = 3.04557\n",
      "epoch no.5 train no.404260  loss = 2.93549 avg_loss = 3.02969\n",
      "epoch no.5 train no.404270  loss = 3.52416 avg_loss = 3.06495\n",
      "epoch no.5 train no.404280  loss = 2.89165 avg_loss = 3.07850\n",
      "epoch no.5 train no.404290  loss = 3.46928 avg_loss = 3.08124\n",
      "epoch no.5 train no.404300  loss = 1.41642 avg_loss = 3.08404\n",
      "epoch no.5 train no.404310  loss = 3.73777 avg_loss = 3.09825\n",
      "epoch no.5 train no.404320  loss = 2.57778 avg_loss = 3.09022\n",
      "epoch no.5 train no.404330  loss = 3.14914 avg_loss = 3.06698\n",
      "epoch no.5 train no.404340  loss = 2.63279 avg_loss = 3.06032\n",
      "epoch no.5 train no.404350  loss = 2.22845 avg_loss = 3.06198\n",
      "epoch no.5 train no.404360  loss = 4.64271 avg_loss = 3.05387\n",
      "epoch no.5 train no.404370  loss = 2.33965 avg_loss = 3.05933\n",
      "epoch no.5 train no.404380  loss = 5.05184 avg_loss = 3.05453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.404390  loss = 3.46392 avg_loss = 3.04887\n",
      "epoch no.5 train no.404400  loss = 2.84868 avg_loss = 3.03338\n",
      "epoch no.5 train no.404410  loss = 3.26369 avg_loss = 3.05161\n",
      "epoch no.5 train no.404420  loss = 2.71270 avg_loss = 3.00782\n",
      "epoch no.5 train no.404430  loss = 2.60743 avg_loss = 2.96829\n",
      "epoch no.5 train no.404440  loss = 3.39940 avg_loss = 2.95138\n",
      "epoch no.5 train no.404450  loss = 4.88753 avg_loss = 2.98750\n",
      "epoch no.5 train no.404460  loss = 3.59855 avg_loss = 3.01635\n",
      "epoch no.5 train no.404470  loss = 2.62799 avg_loss = 3.01028\n",
      "epoch no.5 train no.404480  loss = 1.85078 avg_loss = 3.03715\n",
      "epoch no.5 train no.404490  loss = 2.70665 avg_loss = 3.03104\n",
      "epoch no.5 train no.404500  loss = 3.52893 avg_loss = 3.06204\n",
      "epoch no.5 train no.404510  loss = 2.05570 avg_loss = 3.06442\n",
      "epoch no.5 train no.404520  loss = 3.30500 avg_loss = 3.07784\n",
      "epoch no.5 train no.404530  loss = 2.55754 avg_loss = 3.05716\n",
      "epoch no.5 train no.404540  loss = 3.67431 avg_loss = 3.01650\n",
      "epoch no.5 train no.404550  loss = 3.36308 avg_loss = 3.03798\n",
      "epoch no.5 train no.404560  loss = 3.22789 avg_loss = 3.02558\n",
      "epoch no.5 train no.404570  loss = 3.52761 avg_loss = 3.01883\n",
      "epoch no.5 train no.404580  loss = 2.06410 avg_loss = 3.04566\n",
      "epoch no.5 train no.404590  loss = 2.76631 avg_loss = 3.02415\n",
      "epoch no.5 train no.404600  loss = 2.61689 avg_loss = 3.01569\n",
      "epoch no.5 train no.404610  loss = 3.94976 avg_loss = 2.99020\n",
      "epoch no.5 train no.404620  loss = 2.50139 avg_loss = 2.97167\n",
      "epoch no.5 train no.404630  loss = 2.50932 avg_loss = 2.99614\n",
      "epoch no.5 train no.404640  loss = 3.16008 avg_loss = 2.99544\n",
      "epoch no.5 train no.404650  loss = 2.33240 avg_loss = 2.98217\n",
      "epoch no.5 train no.404660  loss = 4.92912 avg_loss = 2.99404\n",
      "epoch no.5 train no.404670  loss = 2.38339 avg_loss = 2.95276\n",
      "epoch no.5 train no.404680  loss = 3.13314 avg_loss = 3.00055\n",
      "epoch no.5 train no.404690  loss = 3.49375 avg_loss = 2.99508\n",
      "epoch no.5 train no.404700  loss = 2.91521 avg_loss = 2.98746\n",
      "epoch no.5 train no.404710  loss = 2.89846 avg_loss = 3.00743\n",
      "epoch no.5 train no.404720  loss = 3.56526 avg_loss = 2.99370\n",
      "epoch no.5 train no.404730  loss = 3.37519 avg_loss = 3.02196\n",
      "epoch no.5 train no.404740  loss = 3.41879 avg_loss = 3.02880\n",
      "epoch no.5 train no.404750  loss = 3.00801 avg_loss = 3.01391\n",
      "epoch no.5 train no.404760  loss = 1.72933 avg_loss = 3.00177\n",
      "epoch no.5 train no.404770  loss = 3.05672 avg_loss = 3.03725\n",
      "epoch no.5 train no.404780  loss = 4.21233 avg_loss = 3.05661\n",
      "epoch no.5 train no.404790  loss = 3.90837 avg_loss = 3.11021\n",
      "epoch no.5 train no.404800  loss = 3.39744 avg_loss = 3.09899\n",
      "epoch no.5 train no.404810  loss = 2.87278 avg_loss = 3.09212\n",
      "epoch no.5 train no.404820  loss = 2.40383 avg_loss = 3.06981\n",
      "epoch no.5 train no.404830  loss = 3.28537 avg_loss = 3.10154\n",
      "epoch no.5 train no.404840  loss = 3.15151 avg_loss = 3.08088\n",
      "epoch no.5 train no.404850  loss = 1.99138 avg_loss = 3.06520\n",
      "epoch no.5 train no.404860  loss = 3.37732 avg_loss = 3.11887\n",
      "epoch no.5 train no.404870  loss = 4.20744 avg_loss = 3.16024\n",
      "epoch no.5 train no.404880  loss = 3.12558 avg_loss = 3.17982\n",
      "epoch no.5 train no.404890  loss = 2.35136 avg_loss = 3.13513\n",
      "epoch no.5 train no.404900  loss = 3.48077 avg_loss = 3.12080\n",
      "epoch no.5 train no.404910  loss = 2.42254 avg_loss = 3.09512\n",
      "epoch no.5 train no.404920  loss = 3.85305 avg_loss = 3.06491\n",
      "epoch no.5 train no.404930  loss = 3.87995 avg_loss = 3.05525\n",
      "epoch no.5 train no.404940  loss = 2.39562 avg_loss = 3.06614\n",
      "epoch no.5 train no.404950  loss = 3.12541 avg_loss = 3.07986\n",
      "epoch no.5 train no.404960  loss = 4.91595 avg_loss = 3.13571\n",
      "epoch no.5 train no.404970  loss = 3.15003 avg_loss = 3.12651\n",
      "epoch no.5 train no.404980  loss = 3.10414 avg_loss = 3.12207\n",
      "epoch no.5 train no.404990  loss = 3.03274 avg_loss = 3.13589\n",
      "epoch no.5 train no.405000  loss = 3.21198 avg_loss = 3.11766\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '송', '▁모음', '음', '</s>']\n",
      "추억의 팝송모음</s>\n",
      "epoch no.5 train no.405010  loss = 2.11056 avg_loss = 3.11497\n",
      "epoch no.5 train no.405020  loss = 3.19473 avg_loss = 3.09333\n",
      "epoch no.5 train no.405030  loss = 4.67274 avg_loss = 3.07863\n",
      "epoch no.5 train no.405040  loss = 3.53600 avg_loss = 3.06436\n",
      "epoch no.5 train no.405050  loss = 2.86215 avg_loss = 3.04346\n",
      "epoch no.5 train no.405060  loss = 2.77366 avg_loss = 3.04094\n",
      "epoch no.5 train no.405070  loss = 2.71447 avg_loss = 3.02541\n",
      "epoch no.5 train no.405080  loss = 2.30767 avg_loss = 3.03368\n",
      "epoch no.5 train no.405090  loss = 1.89880 avg_loss = 3.04081\n",
      "epoch no.5 train no.405100  loss = 2.86963 avg_loss = 3.02639\n",
      "epoch no.5 train no.405110  loss = 2.29231 avg_loss = 3.02502\n",
      "epoch no.5 train no.405120  loss = 1.99545 avg_loss = 2.97826\n",
      "epoch no.5 train no.405130  loss = 2.54594 avg_loss = 3.00731\n",
      "epoch no.5 train no.405140  loss = 3.15856 avg_loss = 3.00756\n",
      "epoch no.5 train no.405150  loss = 3.49653 avg_loss = 3.02564\n",
      "epoch no.5 train no.405160  loss = 3.62562 avg_loss = 3.00705\n",
      "epoch no.5 train no.405170  loss = 2.51169 avg_loss = 2.97710\n",
      "epoch no.5 train no.405180  loss = 2.83867 avg_loss = 2.99513\n",
      "epoch no.5 train no.405190  loss = 3.97786 avg_loss = 3.07151\n",
      "epoch no.5 train no.405200  loss = 2.78872 avg_loss = 3.08246\n",
      "epoch no.5 train no.405210  loss = 2.35360 avg_loss = 3.05859\n",
      "epoch no.5 train no.405220  loss = 1.98920 avg_loss = 3.03889\n",
      "epoch no.5 train no.405230  loss = 2.00557 avg_loss = 3.03217\n",
      "epoch no.5 train no.405240  loss = 1.95104 avg_loss = 2.99859\n",
      "epoch no.5 train no.405250  loss = 2.09100 avg_loss = 2.98936\n",
      "epoch no.5 train no.405260  loss = 3.01695 avg_loss = 2.98854\n",
      "epoch no.5 train no.405270  loss = 2.09220 avg_loss = 2.94915\n",
      "epoch no.5 train no.405280  loss = 2.69587 avg_loss = 2.98426\n",
      "epoch no.5 train no.405290  loss = 3.10960 avg_loss = 2.97286\n",
      "epoch no.5 train no.405300  loss = 4.92430 avg_loss = 2.97740\n",
      "epoch no.5 train no.405310  loss = 3.35543 avg_loss = 2.95846\n",
      "epoch no.5 train no.405320  loss = 3.88998 avg_loss = 2.97059\n",
      "epoch no.5 train no.405330  loss = 2.67423 avg_loss = 2.96436\n",
      "epoch no.5 train no.405340  loss = 2.46731 avg_loss = 2.98146\n",
      "epoch no.5 train no.405350  loss = 3.34187 avg_loss = 2.98580\n",
      "epoch no.5 train no.405360  loss = 3.01807 avg_loss = 2.99157\n",
      "epoch no.5 train no.405370  loss = 3.52489 avg_loss = 2.98479\n",
      "epoch no.5 train no.405380  loss = 4.75464 avg_loss = 3.02344\n",
      "epoch no.5 train no.405390  loss = 2.60767 avg_loss = 3.04082\n",
      "epoch no.5 train no.405400  loss = 2.68198 avg_loss = 3.03291\n",
      "epoch no.5 train no.405410  loss = 4.73145 avg_loss = 3.03919\n",
      "epoch no.5 train no.405420  loss = 3.62951 avg_loss = 3.06612\n",
      "epoch no.5 train no.405430  loss = 3.71045 avg_loss = 3.08084\n",
      "epoch no.5 train no.405440  loss = 3.17860 avg_loss = 3.06914\n",
      "epoch no.5 train no.405450  loss = 4.40670 avg_loss = 3.08270\n",
      "epoch no.5 train no.405460  loss = 2.62038 avg_loss = 3.11905\n",
      "epoch no.5 train no.405470  loss = 2.25635 avg_loss = 3.11797\n",
      "epoch no.5 train no.405480  loss = 4.23885 avg_loss = 3.15650\n",
      "epoch no.5 train no.405490  loss = 5.25282 avg_loss = 3.17855\n",
      "epoch no.5 train no.405500  loss = 3.40096 avg_loss = 3.22299\n",
      "epoch no.5 train no.405510  loss = 2.50951 avg_loss = 3.20460\n",
      "epoch no.5 train no.405520  loss = 3.43016 avg_loss = 3.18739\n",
      "epoch no.5 train no.405530  loss = 2.36520 avg_loss = 3.13638\n",
      "epoch no.5 train no.405540  loss = 2.84698 avg_loss = 3.10217\n",
      "epoch no.5 train no.405550  loss = 4.55795 avg_loss = 3.11605\n",
      "epoch no.5 train no.405560  loss = 3.20528 avg_loss = 3.15390\n",
      "epoch no.5 train no.405570  loss = 2.75514 avg_loss = 3.13814\n",
      "epoch no.5 train no.405580  loss = 3.99984 avg_loss = 3.14215\n",
      "epoch no.5 train no.405590  loss = 4.10757 avg_loss = 3.13077\n",
      "epoch no.5 train no.405600  loss = 3.80788 avg_loss = 3.15061\n",
      "epoch no.5 train no.405610  loss = 1.79786 avg_loss = 3.12643\n",
      "epoch no.5 train no.405620  loss = 3.44918 avg_loss = 3.14138\n",
      "epoch no.5 train no.405630  loss = 2.33715 avg_loss = 3.10453\n",
      "epoch no.5 train no.405640  loss = 2.94496 avg_loss = 3.07862\n",
      "epoch no.5 train no.405650  loss = 4.03598 avg_loss = 3.07622\n",
      "epoch no.5 train no.405660  loss = 2.69444 avg_loss = 3.03356\n",
      "epoch no.5 train no.405670  loss = 4.04432 avg_loss = 3.08113\n",
      "epoch no.5 train no.405680  loss = 1.83987 avg_loss = 3.04723\n",
      "epoch no.5 train no.405690  loss = 1.94844 avg_loss = 3.00588\n",
      "epoch no.5 train no.405700  loss = 3.21116 avg_loss = 3.03521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.405710  loss = 3.23257 avg_loss = 3.02552\n",
      "epoch no.5 train no.405720  loss = 2.17534 avg_loss = 3.03110\n",
      "epoch no.5 train no.405730  loss = 4.05716 avg_loss = 3.00921\n",
      "epoch no.5 train no.405740  loss = 3.33521 avg_loss = 3.00732\n",
      "epoch no.5 train no.405750  loss = 2.18314 avg_loss = 2.96654\n",
      "epoch no.5 train no.405760  loss = 2.29553 avg_loss = 2.99574\n",
      "epoch no.5 train no.405770  loss = 2.30547 avg_loss = 2.99275\n",
      "epoch no.5 train no.405780  loss = 2.64634 avg_loss = 3.00815\n",
      "epoch no.5 train no.405790  loss = 2.80228 avg_loss = 3.00602\n",
      "epoch no.5 train no.405800  loss = 3.98489 avg_loss = 3.00504\n",
      "epoch no.5 train no.405810  loss = 3.43243 avg_loss = 3.01482\n",
      "epoch no.5 train no.405820  loss = 2.01287 avg_loss = 2.98154\n",
      "epoch no.5 train no.405830  loss = 3.55435 avg_loss = 2.98334\n",
      "epoch no.5 train no.405840  loss = 2.59442 avg_loss = 3.02518\n",
      "epoch no.5 train no.405850  loss = 5.00872 avg_loss = 3.00405\n",
      "epoch no.5 train no.405860  loss = 2.42826 avg_loss = 3.03452\n",
      "epoch no.5 train no.405870  loss = 2.16604 avg_loss = 3.02328\n",
      "epoch no.5 train no.405880  loss = 3.09326 avg_loss = 2.99242\n",
      "epoch no.5 train no.405890  loss = 5.00616 avg_loss = 3.03707\n",
      "epoch no.5 train no.405900  loss = 4.08474 avg_loss = 3.04166\n",
      "epoch no.5 train no.405910  loss = 4.99075 avg_loss = 3.07505\n",
      "epoch no.5 train no.405920  loss = 3.11878 avg_loss = 3.05012\n",
      "epoch no.5 train no.405930  loss = 1.73492 avg_loss = 3.01651\n",
      "epoch no.5 train no.405940  loss = 3.66763 avg_loss = 3.04905\n",
      "epoch no.5 train no.405950  loss = 1.52583 avg_loss = 3.02244\n",
      "epoch no.5 train no.405960  loss = 2.17521 avg_loss = 2.97592\n",
      "epoch no.5 train no.405970  loss = 3.41307 avg_loss = 2.99313\n",
      "epoch no.5 train no.405980  loss = 2.84561 avg_loss = 2.95385\n",
      "epoch no.5 train no.405990  loss = 2.71810 avg_loss = 2.96249\n",
      "epoch no.5 train no.406000  loss = 4.51005 avg_loss = 2.94671\n",
      "3\n",
      "to_tokens: ['▁비', '▁싸이', '팝', '송', '</s>']\n",
      "추억의 올드팝 2</s>\n",
      "epoch no.5 train no.406010  loss = 4.00097 avg_loss = 2.96218\n",
      "epoch no.5 train no.406020  loss = 2.03885 avg_loss = 2.94090\n",
      "epoch no.5 train no.406030  loss = 2.08144 avg_loss = 2.92013\n",
      "epoch no.5 train no.406040  loss = 1.85775 avg_loss = 2.92583\n",
      "epoch no.5 train no.406050  loss = 3.28191 avg_loss = 2.95696\n",
      "epoch no.5 train no.406060  loss = 4.81878 avg_loss = 3.01230\n",
      "epoch no.5 train no.406070  loss = 2.93652 avg_loss = 3.00833\n",
      "epoch no.5 train no.406080  loss = 1.67302 avg_loss = 2.96749\n",
      "epoch no.5 train no.406090  loss = 3.40981 avg_loss = 2.95927\n",
      "epoch no.5 train no.406100  loss = 2.53401 avg_loss = 2.95903\n",
      "epoch no.5 train no.406110  loss = 2.84374 avg_loss = 2.95109\n",
      "epoch no.5 train no.406120  loss = 4.57296 avg_loss = 2.99968\n",
      "epoch no.5 train no.406130  loss = 2.40868 avg_loss = 2.98155\n",
      "epoch no.5 train no.406140  loss = 4.29205 avg_loss = 2.98071\n",
      "epoch no.5 train no.406150  loss = 3.67024 avg_loss = 2.97344\n",
      "epoch no.5 train no.406160  loss = 3.83215 avg_loss = 3.01788\n",
      "epoch no.5 train no.406170  loss = 2.37989 avg_loss = 2.98534\n",
      "epoch no.5 train no.406180  loss = 2.43543 avg_loss = 2.99446\n",
      "epoch no.5 train no.406190  loss = 3.59242 avg_loss = 2.98240\n",
      "epoch no.5 train no.406200  loss = 3.40827 avg_loss = 2.97424\n",
      "epoch no.5 train no.406210  loss = 2.15770 avg_loss = 2.94766\n",
      "epoch no.5 train no.406220  loss = 3.66273 avg_loss = 3.01514\n",
      "epoch no.5 train no.406230  loss = 2.51280 avg_loss = 2.97608\n",
      "epoch no.5 train no.406240  loss = 2.31618 avg_loss = 2.94690\n",
      "epoch no.5 train no.406250  loss = 1.79568 avg_loss = 2.93807\n",
      "epoch no.5 train no.406260  loss = 2.75268 avg_loss = 2.94421\n",
      "epoch no.5 train no.406270  loss = 2.40066 avg_loss = 2.94567\n",
      "epoch no.5 train no.406280  loss = 3.81657 avg_loss = 2.94287\n",
      "epoch no.5 train no.406290  loss = 1.56253 avg_loss = 2.95240\n",
      "epoch no.5 train no.406300  loss = 3.76292 avg_loss = 2.95758\n",
      "epoch no.5 train no.406310  loss = 2.76941 avg_loss = 2.96101\n",
      "epoch no.5 train no.406320  loss = 3.35293 avg_loss = 2.97124\n",
      "epoch no.5 train no.406330  loss = 4.05826 avg_loss = 2.94269\n",
      "epoch no.5 train no.406340  loss = 3.48653 avg_loss = 2.94668\n",
      "epoch no.5 train no.406350  loss = 3.42021 avg_loss = 2.95549\n",
      "epoch no.5 train no.406360  loss = 3.18623 avg_loss = 2.95674\n",
      "epoch no.5 train no.406370  loss = 1.99786 avg_loss = 2.97574\n",
      "epoch no.5 train no.406380  loss = 1.74016 avg_loss = 2.97426\n",
      "epoch no.5 train no.406390  loss = 2.28916 avg_loss = 2.95940\n",
      "epoch no.5 train no.406400  loss = 3.04357 avg_loss = 2.96159\n",
      "epoch no.5 train no.406410  loss = 3.00553 avg_loss = 2.97817\n",
      "epoch no.5 train no.406420  loss = 2.32066 avg_loss = 3.01209\n",
      "epoch no.5 train no.406430  loss = 2.16792 avg_loss = 2.99053\n",
      "epoch no.5 train no.406440  loss = 4.94102 avg_loss = 3.06755\n",
      "epoch no.5 train no.406450  loss = 2.44823 avg_loss = 3.06158\n",
      "epoch no.5 train no.406460  loss = 1.93178 avg_loss = 3.04583\n",
      "epoch no.5 train no.406470  loss = 2.70105 avg_loss = 3.04056\n",
      "epoch no.5 train no.406480  loss = 2.09945 avg_loss = 3.02157\n",
      "epoch no.5 train no.406490  loss = 2.32523 avg_loss = 2.99691\n",
      "epoch no.5 train no.406500  loss = 2.90107 avg_loss = 3.01385\n",
      "epoch no.5 train no.406510  loss = 2.44726 avg_loss = 3.01498\n",
      "epoch no.5 train no.406520  loss = 2.39013 avg_loss = 2.99949\n",
      "epoch no.5 train no.406530  loss = 3.51674 avg_loss = 3.01920\n",
      "epoch no.5 train no.406540  loss = 3.09987 avg_loss = 3.01743\n",
      "epoch no.5 train no.406550  loss = 4.15780 avg_loss = 3.06761\n",
      "epoch no.5 train no.406560  loss = 3.03022 avg_loss = 3.06695\n",
      "epoch no.5 train no.406570  loss = 2.91788 avg_loss = 3.05522\n",
      "epoch no.5 train no.406580  loss = 2.78598 avg_loss = 3.05063\n",
      "epoch no.5 train no.406590  loss = 3.05527 avg_loss = 3.01864\n",
      "epoch no.5 train no.406600  loss = 3.70117 avg_loss = 2.99864\n",
      "epoch no.5 train no.406610  loss = 5.39764 avg_loss = 3.06543\n",
      "epoch no.5 train no.406620  loss = 2.19367 avg_loss = 3.09833\n",
      "epoch no.5 train no.406630  loss = 2.43508 avg_loss = 3.07036\n",
      "epoch no.5 train no.406640  loss = 4.36281 avg_loss = 3.04593\n",
      "epoch no.5 train no.406650  loss = 2.49560 avg_loss = 3.04277\n",
      "epoch no.5 train no.406660  loss = 3.42922 avg_loss = 3.01866\n",
      "epoch no.5 train no.406670  loss = 3.79742 avg_loss = 3.05808\n",
      "epoch no.5 train no.406680  loss = 3.03747 avg_loss = 3.12370\n",
      "epoch no.5 train no.406690  loss = 2.99864 avg_loss = 3.12942\n",
      "epoch no.5 train no.406700  loss = 2.99143 avg_loss = 3.11478\n",
      "epoch no.5 train no.406710  loss = 3.94524 avg_loss = 3.10437\n",
      "epoch no.5 train no.406720  loss = 3.58950 avg_loss = 3.09073\n",
      "epoch no.5 train no.406730  loss = 3.88274 avg_loss = 3.05047\n",
      "epoch no.5 train no.406740  loss = 2.18611 avg_loss = 3.06903\n",
      "epoch no.5 train no.406750  loss = 1.82017 avg_loss = 3.05741\n",
      "epoch no.5 train no.406760  loss = 3.31505 avg_loss = 3.04760\n",
      "epoch no.5 train no.406770  loss = 2.55570 avg_loss = 3.04135\n",
      "epoch no.5 train no.406780  loss = 3.85315 avg_loss = 3.03175\n",
      "epoch no.5 train no.406790  loss = 3.56198 avg_loss = 3.04059\n",
      "epoch no.5 train no.406800  loss = 1.96673 avg_loss = 3.08498\n",
      "epoch no.5 train no.406810  loss = 2.00132 avg_loss = 3.08131\n",
      "epoch no.5 train no.406820  loss = 2.59528 avg_loss = 3.07038\n",
      "epoch no.5 train no.406830  loss = 3.86522 avg_loss = 3.09982\n",
      "epoch no.5 train no.406840  loss = 1.64995 avg_loss = 3.05905\n",
      "epoch no.5 train no.406850  loss = 2.99617 avg_loss = 3.05612\n",
      "epoch no.5 train no.406860  loss = 3.93913 avg_loss = 3.05078\n",
      "epoch no.5 train no.406870  loss = 2.58844 avg_loss = 3.05555\n",
      "epoch no.5 train no.406880  loss = 2.27016 avg_loss = 3.03862\n",
      "epoch no.5 train no.406890  loss = 2.66564 avg_loss = 3.03377\n",
      "epoch no.5 train no.406900  loss = 2.92363 avg_loss = 3.03296\n",
      "epoch no.5 train no.406910  loss = 3.28197 avg_loss = 3.06133\n",
      "epoch no.5 train no.406920  loss = 4.80323 avg_loss = 3.08138\n",
      "epoch no.5 train no.406930  loss = 2.33684 avg_loss = 3.07493\n",
      "epoch no.5 train no.406940  loss = 2.19955 avg_loss = 3.07715\n",
      "epoch no.5 train no.406950  loss = 2.61508 avg_loss = 3.07235\n",
      "epoch no.5 train no.406960  loss = 3.02273 avg_loss = 3.05584\n",
      "epoch no.5 train no.406970  loss = 1.60071 avg_loss = 3.03573\n",
      "epoch no.5 train no.406980  loss = 2.67785 avg_loss = 3.04961\n",
      "epoch no.5 train no.406990  loss = 2.95145 avg_loss = 3.05424\n",
      "epoch no.5 train no.407000  loss = 2.46731 avg_loss = 3.01123\n",
      "5\n",
      "to_tokens: ['▁비', '▁싸이', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.407010  loss = 2.68865 avg_loss = 2.99060\n",
      "epoch no.5 train no.407020  loss = 2.59221 avg_loss = 3.05761\n",
      "epoch no.5 train no.407030  loss = 3.06685 avg_loss = 3.04496\n",
      "epoch no.5 train no.407040  loss = 3.37937 avg_loss = 2.99657\n",
      "epoch no.5 train no.407050  loss = 3.43827 avg_loss = 3.01077\n",
      "epoch no.5 train no.407060  loss = 3.63059 avg_loss = 3.02888\n",
      "epoch no.5 train no.407070  loss = 3.68039 avg_loss = 3.06521\n",
      "epoch no.5 train no.407080  loss = 3.91812 avg_loss = 3.05707\n",
      "epoch no.5 train no.407090  loss = 3.33039 avg_loss = 3.04204\n",
      "epoch no.5 train no.407100  loss = 2.71758 avg_loss = 3.02563\n",
      "epoch no.5 train no.407110  loss = 1.27392 avg_loss = 3.00406\n",
      "epoch no.5 train no.407120  loss = 4.04726 avg_loss = 2.98283\n",
      "epoch no.5 train no.407130  loss = 2.50929 avg_loss = 2.96088\n",
      "epoch no.5 train no.407140  loss = 2.92263 avg_loss = 2.99723\n",
      "epoch no.5 train no.407150  loss = 2.79252 avg_loss = 3.00475\n",
      "epoch no.5 train no.407160  loss = 3.50430 avg_loss = 3.02069\n",
      "epoch no.5 train no.407170  loss = 3.23029 avg_loss = 3.02072\n",
      "epoch no.5 train no.407180  loss = 2.55172 avg_loss = 2.99499\n",
      "epoch no.5 train no.407190  loss = 2.93189 avg_loss = 3.01306\n",
      "epoch no.5 train no.407200  loss = 2.67081 avg_loss = 2.98228\n",
      "epoch no.5 train no.407210  loss = 3.22530 avg_loss = 2.97565\n",
      "epoch no.5 train no.407220  loss = 3.42215 avg_loss = 3.00116\n",
      "epoch no.5 train no.407230  loss = 3.91779 avg_loss = 2.98508\n",
      "epoch no.5 train no.407240  loss = 3.03517 avg_loss = 3.02105\n",
      "epoch no.5 train no.407250  loss = 4.00842 avg_loss = 3.02672\n",
      "epoch no.5 train no.407260  loss = 2.26931 avg_loss = 3.06742\n",
      "epoch no.5 train no.407270  loss = 3.52883 avg_loss = 3.04179\n",
      "epoch no.5 train no.407280  loss = 2.95045 avg_loss = 3.05453\n",
      "epoch no.5 train no.407290  loss = 2.84134 avg_loss = 3.05140\n",
      "epoch no.5 train no.407300  loss = 5.11350 avg_loss = 3.06153\n",
      "epoch no.5 train no.407310  loss = 2.41398 avg_loss = 3.00529\n",
      "epoch no.5 train no.407320  loss = 3.47523 avg_loss = 3.00332\n",
      "epoch no.5 train no.407330  loss = 2.47293 avg_loss = 3.00416\n",
      "epoch no.5 train no.407340  loss = 1.81217 avg_loss = 2.97851\n",
      "epoch no.5 train no.407350  loss = 1.48229 avg_loss = 2.94794\n",
      "epoch no.5 train no.407360  loss = 2.78448 avg_loss = 2.95694\n",
      "epoch no.5 train no.407370  loss = 2.38106 avg_loss = 2.98713\n",
      "epoch no.5 train no.407380  loss = 2.84596 avg_loss = 2.98225\n",
      "epoch no.5 train no.407390  loss = 2.19405 avg_loss = 2.94603\n",
      "epoch no.5 train no.407400  loss = 1.82766 avg_loss = 2.97081\n",
      "epoch no.5 train no.407410  loss = 3.80119 avg_loss = 2.96178\n",
      "epoch no.5 train no.407420  loss = 4.07803 avg_loss = 2.97670\n",
      "epoch no.5 train no.407430  loss = 3.45186 avg_loss = 2.98595\n",
      "epoch no.5 train no.407440  loss = 2.75073 avg_loss = 2.99845\n",
      "epoch no.5 train no.407450  loss = 3.62327 avg_loss = 2.98148\n",
      "epoch no.5 train no.407460  loss = 3.92919 avg_loss = 2.97713\n",
      "epoch no.5 train no.407470  loss = 3.06514 avg_loss = 2.98615\n",
      "epoch no.5 train no.407480  loss = 3.70392 avg_loss = 3.00263\n",
      "epoch no.5 train no.407490  loss = 3.56336 avg_loss = 3.02550\n",
      "epoch no.5 train no.407500  loss = 2.46141 avg_loss = 3.02281\n",
      "epoch no.5 train no.407510  loss = 2.98794 avg_loss = 3.02125\n",
      "epoch no.5 train no.407520  loss = 2.28223 avg_loss = 3.02378\n",
      "epoch no.5 train no.407530  loss = 1.78159 avg_loss = 3.04399\n",
      "epoch no.5 train no.407540  loss = 3.29983 avg_loss = 3.04004\n",
      "epoch no.5 train no.407550  loss = 3.66574 avg_loss = 3.03326\n",
      "epoch no.5 train no.407560  loss = 2.79270 avg_loss = 3.02645\n",
      "epoch no.5 train no.407570  loss = 3.28968 avg_loss = 3.04249\n",
      "epoch no.5 train no.407580  loss = 3.14953 avg_loss = 3.05273\n",
      "epoch no.5 train no.407590  loss = 2.64220 avg_loss = 3.07587\n",
      "epoch no.5 train no.407600  loss = 3.37203 avg_loss = 3.09700\n",
      "epoch no.5 train no.407610  loss = 3.79455 avg_loss = 3.10526\n",
      "epoch no.5 train no.407620  loss = 1.82018 avg_loss = 3.08810\n",
      "epoch no.5 train no.407630  loss = 4.13358 avg_loss = 3.12285\n",
      "epoch no.5 train no.407640  loss = 4.46237 avg_loss = 3.14927\n",
      "epoch no.5 train no.407650  loss = 4.06126 avg_loss = 3.14190\n",
      "epoch no.5 train no.407660  loss = 2.26039 avg_loss = 3.10225\n",
      "epoch no.5 train no.407670  loss = 2.48384 avg_loss = 3.09642\n",
      "epoch no.5 train no.407680  loss = 3.94028 avg_loss = 3.10027\n",
      "epoch no.5 train no.407690  loss = 4.94685 avg_loss = 3.13635\n",
      "epoch no.5 train no.407700  loss = 2.39686 avg_loss = 3.13553\n",
      "epoch no.5 train no.407710  loss = 2.21192 avg_loss = 3.11783\n",
      "epoch no.5 train no.407720  loss = 4.03107 avg_loss = 3.13630\n",
      "epoch no.5 train no.407730  loss = 2.94157 avg_loss = 3.14747\n",
      "epoch no.5 train no.407740  loss = 3.53476 avg_loss = 3.11979\n",
      "epoch no.5 train no.407750  loss = 3.46911 avg_loss = 3.11966\n",
      "epoch no.5 train no.407760  loss = 2.62261 avg_loss = 3.09319\n",
      "epoch no.5 train no.407770  loss = 3.08345 avg_loss = 3.06172\n",
      "epoch no.5 train no.407780  loss = 2.88178 avg_loss = 3.04658\n",
      "epoch no.5 train no.407790  loss = 4.20597 avg_loss = 3.07648\n",
      "epoch no.5 train no.407800  loss = 2.32337 avg_loss = 3.07089\n",
      "epoch no.5 train no.407810  loss = 5.70942 avg_loss = 3.07886\n",
      "epoch no.5 train no.407820  loss = 2.97970 avg_loss = 3.11407\n",
      "epoch no.5 train no.407830  loss = 2.36416 avg_loss = 3.13756\n",
      "epoch no.5 train no.407840  loss = 2.60761 avg_loss = 3.12036\n",
      "epoch no.5 train no.407850  loss = 3.17642 avg_loss = 3.10006\n",
      "epoch no.5 train no.407860  loss = 3.21889 avg_loss = 3.11479\n",
      "epoch no.5 train no.407870  loss = 3.99655 avg_loss = 3.16053\n",
      "epoch no.5 train no.407880  loss = 3.48797 avg_loss = 3.16397\n",
      "epoch no.5 train no.407890  loss = 3.76312 avg_loss = 3.16165\n",
      "epoch no.5 train no.407900  loss = 1.96277 avg_loss = 3.12838\n",
      "epoch no.5 train no.407910  loss = 3.92056 avg_loss = 3.14078\n",
      "epoch no.5 train no.407920  loss = 4.88141 avg_loss = 3.18206\n",
      "epoch no.5 train no.407930  loss = 3.36166 avg_loss = 3.17002\n",
      "epoch no.5 train no.407940  loss = 2.56475 avg_loss = 3.12444\n",
      "epoch no.5 train no.407950  loss = 2.03801 avg_loss = 3.09179\n",
      "epoch no.5 train no.407960  loss = 2.60090 avg_loss = 3.12683\n",
      "epoch no.5 train no.407970  loss = 2.39400 avg_loss = 3.13339\n",
      "epoch no.5 train no.407980  loss = 2.90865 avg_loss = 3.12782\n",
      "epoch no.5 train no.407990  loss = 2.60392 avg_loss = 3.16633\n",
      "epoch no.5 train no.408000  loss = 2.88402 avg_loss = 3.13947\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.5 train no.408010  loss = 2.21264 avg_loss = 3.09483\n",
      "epoch no.5 train no.408020  loss = 2.73674 avg_loss = 3.08154\n",
      "epoch no.5 train no.408030  loss = 2.45534 avg_loss = 3.07557\n",
      "epoch no.5 train no.408040  loss = 2.31261 avg_loss = 3.05438\n",
      "epoch no.5 train no.408050  loss = 2.08972 avg_loss = 3.02517\n",
      "epoch no.5 train no.408060  loss = 2.72578 avg_loss = 3.04477\n",
      "epoch no.5 train no.408070  loss = 3.23171 avg_loss = 3.06246\n",
      "epoch no.5 train no.408080  loss = 1.76906 avg_loss = 3.03877\n",
      "epoch no.5 train no.408090  loss = 2.08546 avg_loss = 3.04383\n",
      "epoch no.5 train no.408100  loss = 3.11495 avg_loss = 3.07270\n",
      "epoch no.5 train no.408110  loss = 4.65533 avg_loss = 3.07766\n",
      "epoch no.5 train no.408120  loss = 2.14403 avg_loss = 3.02197\n",
      "epoch no.5 train no.408130  loss = 4.69652 avg_loss = 3.01523\n",
      "epoch no.5 train no.408140  loss = 2.84483 avg_loss = 3.06799\n",
      "epoch no.5 train no.408150  loss = 2.90844 avg_loss = 3.08043\n",
      "epoch no.5 train no.408160  loss = 2.42418 avg_loss = 3.04754\n",
      "epoch no.5 train no.408170  loss = 2.43679 avg_loss = 3.04948\n",
      "epoch no.5 train no.408180  loss = 3.79112 avg_loss = 3.06213\n",
      "epoch no.5 train no.408190  loss = 2.99916 avg_loss = 3.04972\n",
      "epoch no.5 train no.408200  loss = 3.12312 avg_loss = 3.05150\n",
      "epoch no.5 train no.408210  loss = 2.84122 avg_loss = 3.06372\n",
      "epoch no.5 train no.408220  loss = 1.85800 avg_loss = 3.09025\n",
      "epoch no.5 train no.408230  loss = 3.22150 avg_loss = 3.08296\n",
      "epoch no.5 train no.408240  loss = 3.27462 avg_loss = 3.06537\n",
      "epoch no.5 train no.408250  loss = 4.58158 avg_loss = 3.05338\n",
      "epoch no.5 train no.408260  loss = 2.63684 avg_loss = 3.05771\n",
      "epoch no.5 train no.408270  loss = 1.48794 avg_loss = 3.03784\n",
      "epoch no.5 train no.408280  loss = 3.26966 avg_loss = 3.01357\n",
      "epoch no.5 train no.408290  loss = 2.54920 avg_loss = 2.96394\n",
      "epoch no.5 train no.408300  loss = 3.47467 avg_loss = 2.98946\n",
      "epoch no.5 train no.408310  loss = 1.80854 avg_loss = 2.95309\n",
      "epoch no.5 train no.408320  loss = 2.30735 avg_loss = 2.96983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.408330  loss = 4.73041 avg_loss = 2.99133\n",
      "epoch no.5 train no.408340  loss = 3.18159 avg_loss = 3.00075\n",
      "epoch no.5 train no.408350  loss = 3.85062 avg_loss = 3.03777\n",
      "epoch no.5 train no.408360  loss = 3.25924 avg_loss = 3.03901\n",
      "epoch no.5 train no.408370  loss = 4.42098 avg_loss = 3.08298\n",
      "epoch no.5 train no.408380  loss = 1.94931 avg_loss = 3.06120\n",
      "epoch no.5 train no.408390  loss = 3.68897 avg_loss = 3.09952\n",
      "epoch no.5 train no.408400  loss = 2.99361 avg_loss = 3.08582\n",
      "epoch no.5 train no.408410  loss = 4.25017 avg_loss = 3.08076\n",
      "epoch no.5 train no.408420  loss = 3.80869 avg_loss = 3.10686\n",
      "epoch no.5 train no.408430  loss = 2.09974 avg_loss = 3.09956\n",
      "epoch no.5 train no.408440  loss = 3.30049 avg_loss = 3.10201\n",
      "epoch no.5 train no.408450  loss = 3.37489 avg_loss = 3.04510\n",
      "epoch no.5 train no.408460  loss = 3.03999 avg_loss = 3.04617\n",
      "epoch no.5 train no.408470  loss = 3.02362 avg_loss = 3.03557\n",
      "epoch no.5 train no.408480  loss = 3.40085 avg_loss = 3.03711\n",
      "epoch no.5 train no.408490  loss = 3.57616 avg_loss = 3.11989\n",
      "epoch no.5 train no.408500  loss = 3.61341 avg_loss = 3.17180\n",
      "epoch no.5 train no.408510  loss = 2.60760 avg_loss = 3.16213\n",
      "epoch no.5 train no.408520  loss = 2.97653 avg_loss = 3.10953\n",
      "epoch no.5 train no.408530  loss = 2.72900 avg_loss = 3.08882\n",
      "epoch no.5 train no.408540  loss = 4.35701 avg_loss = 3.07431\n",
      "epoch no.5 train no.408550  loss = 2.07838 avg_loss = 3.06433\n",
      "epoch no.5 train no.408560  loss = 4.17089 avg_loss = 3.06094\n",
      "epoch no.5 train no.408570  loss = 3.01972 avg_loss = 3.05803\n",
      "epoch no.5 train no.408580  loss = 3.01546 avg_loss = 3.03632\n",
      "epoch no.5 train no.408590  loss = 2.67850 avg_loss = 3.06235\n",
      "epoch no.5 train no.408600  loss = 3.88106 avg_loss = 3.13203\n",
      "epoch no.5 train no.408610  loss = 1.81568 avg_loss = 3.07332\n",
      "epoch no.5 train no.408620  loss = 4.07025 avg_loss = 3.05474\n",
      "epoch no.5 train no.408630  loss = 2.91114 avg_loss = 3.02795\n",
      "epoch no.5 train no.408640  loss = 3.64730 avg_loss = 3.05424\n",
      "epoch no.5 train no.408650  loss = 2.51871 avg_loss = 3.02879\n",
      "epoch no.5 train no.408660  loss = 3.17869 avg_loss = 3.03345\n",
      "epoch no.5 train no.408670  loss = 2.42907 avg_loss = 3.04797\n",
      "epoch no.5 train no.408680  loss = 2.31477 avg_loss = 3.05395\n",
      "epoch no.5 train no.408690  loss = 3.73123 avg_loss = 3.06162\n",
      "epoch no.5 train no.408700  loss = 5.31001 avg_loss = 3.09142\n",
      "epoch no.5 train no.408710  loss = 3.71087 avg_loss = 3.12367\n",
      "epoch no.5 train no.408720  loss = 3.67715 avg_loss = 3.10709\n",
      "epoch no.5 train no.408730  loss = 2.52428 avg_loss = 3.11488\n",
      "epoch no.5 train no.408740  loss = 3.50274 avg_loss = 3.13537\n",
      "epoch no.5 train no.408750  loss = 3.84632 avg_loss = 3.12025\n",
      "epoch no.5 train no.408760  loss = 2.83833 avg_loss = 3.13253\n",
      "epoch no.5 train no.408770  loss = 2.51622 avg_loss = 3.13493\n",
      "epoch no.5 train no.408780  loss = 2.65260 avg_loss = 3.12685\n",
      "epoch no.5 train no.408790  loss = 3.16215 avg_loss = 3.12437\n",
      "epoch no.5 train no.408800  loss = 1.93268 avg_loss = 3.07217\n",
      "epoch no.5 train no.408810  loss = 3.05489 avg_loss = 3.07961\n",
      "epoch no.5 train no.408820  loss = 2.03751 avg_loss = 3.03215\n",
      "epoch no.5 train no.408830  loss = 2.96464 avg_loss = 2.99721\n",
      "epoch no.5 train no.408840  loss = 1.86553 avg_loss = 2.98193\n",
      "epoch no.5 train no.408850  loss = 2.51798 avg_loss = 2.98188\n",
      "epoch no.5 train no.408860  loss = 3.39981 avg_loss = 3.03497\n",
      "epoch no.5 train no.408870  loss = 2.63143 avg_loss = 3.00106\n",
      "epoch no.5 train no.408880  loss = 3.12692 avg_loss = 3.03503\n",
      "epoch no.5 train no.408890  loss = 2.73508 avg_loss = 3.08630\n",
      "epoch no.5 train no.408900  loss = 2.17183 avg_loss = 3.07160\n",
      "epoch no.5 train no.408910  loss = 4.34710 avg_loss = 3.04524\n",
      "epoch no.5 train no.408920  loss = 4.78211 avg_loss = 3.04738\n",
      "epoch no.5 train no.408930  loss = 3.15556 avg_loss = 3.06724\n",
      "epoch no.5 train no.408940  loss = 3.39969 avg_loss = 3.09004\n",
      "epoch no.5 train no.408950  loss = 2.09961 avg_loss = 3.12317\n",
      "epoch no.5 train no.408960  loss = 3.05290 avg_loss = 3.09681\n",
      "epoch no.5 train no.408970  loss = 1.41986 avg_loss = 3.08905\n",
      "epoch no.5 train no.408980  loss = 2.91176 avg_loss = 3.05775\n",
      "epoch no.5 train no.408990  loss = 4.29428 avg_loss = 3.04893\n",
      "epoch no.5 train no.409000  loss = 3.14525 avg_loss = 3.04660\n",
      "5\n",
      "to_tokens: ['▁비', '▁올드', '▁팝', '▁명', '▁함께', '에이지', '</s>']\n",
      "추억의 올드팝과 뉴에이지</s>\n",
      "epoch no.5 train no.409010  loss = 3.63991 avg_loss = 3.08993\n",
      "epoch no.5 train no.409020  loss = 3.40589 avg_loss = 3.09331\n",
      "epoch no.5 train no.409030  loss = 4.02423 avg_loss = 3.09579\n",
      "epoch no.5 train no.409040  loss = 2.67960 avg_loss = 3.11953\n",
      "epoch no.5 train no.409050  loss = 3.66552 avg_loss = 3.15117\n",
      "epoch no.5 train no.409060  loss = 2.41773 avg_loss = 3.17202\n",
      "epoch no.5 train no.409070  loss = 2.03538 avg_loss = 3.11511\n",
      "epoch no.5 train no.409080  loss = 2.70701 avg_loss = 3.09857\n",
      "epoch no.5 train no.409090  loss = 2.98410 avg_loss = 3.14122\n",
      "epoch no.5 train no.409100  loss = 3.25450 avg_loss = 3.13352\n",
      "epoch no.5 train no.409110  loss = 2.69523 avg_loss = 3.08780\n",
      "epoch no.5 train no.409120  loss = 3.31708 avg_loss = 3.11005\n",
      "epoch no.5 train no.409130  loss = 2.81004 avg_loss = 3.07872\n",
      "epoch no.5 train no.409140  loss = 3.26240 avg_loss = 3.14907\n",
      "epoch no.5 train no.409150  loss = 3.86536 avg_loss = 3.16408\n",
      "epoch no.5 train no.409160  loss = 3.97015 avg_loss = 3.14964\n",
      "epoch no.5 train no.409170  loss = 2.42370 avg_loss = 3.11479\n",
      "epoch no.5 train no.409180  loss = 1.86666 avg_loss = 3.08552\n",
      "epoch no.5 train no.409190  loss = 1.96923 avg_loss = 3.10548\n",
      "epoch no.5 train no.409200  loss = 2.25649 avg_loss = 3.07583\n",
      "epoch no.5 train no.409210  loss = 3.52729 avg_loss = 3.08086\n",
      "epoch no.5 train no.409220  loss = 2.13834 avg_loss = 3.06205\n",
      "epoch no.5 train no.409230  loss = 3.52728 avg_loss = 3.06413\n",
      "epoch no.5 train no.409240  loss = 2.19660 avg_loss = 3.05244\n",
      "epoch no.5 train no.409250  loss = 3.43416 avg_loss = 3.04286\n",
      "epoch no.5 train no.409260  loss = 2.24252 avg_loss = 3.07995\n",
      "epoch no.5 train no.409270  loss = 5.11451 avg_loss = 3.09752\n",
      "epoch no.5 train no.409280  loss = 2.29174 avg_loss = 3.08887\n",
      "epoch no.5 train no.409290  loss = 3.74539 avg_loss = 3.15177\n",
      "epoch no.5 train no.409300  loss = 1.98003 avg_loss = 3.15071\n",
      "epoch no.5 train no.409310  loss = 4.42144 avg_loss = 3.12918\n",
      "epoch no.5 train no.409320  loss = 3.01800 avg_loss = 3.11520\n",
      "epoch no.5 train no.409330  loss = 3.76356 avg_loss = 3.09741\n",
      "epoch no.5 train no.409340  loss = 3.37957 avg_loss = 3.09203\n",
      "epoch no.5 train no.409350  loss = 4.94444 avg_loss = 3.12738\n",
      "epoch no.5 train no.409360  loss = 4.34827 avg_loss = 3.10857\n",
      "epoch no.5 train no.409370  loss = 2.23610 avg_loss = 3.10857\n",
      "epoch no.5 train no.409380  loss = 2.49272 avg_loss = 3.06087\n",
      "epoch no.5 train no.409390  loss = 2.21247 avg_loss = 3.06325\n",
      "epoch no.5 train no.409400  loss = 2.06418 avg_loss = 3.06668\n",
      "epoch no.5 train no.409410  loss = 3.28836 avg_loss = 3.00183\n",
      "epoch no.5 train no.409420  loss = 2.21172 avg_loss = 2.97060\n",
      "epoch no.5 train no.409430  loss = 3.34205 avg_loss = 2.95549\n",
      "epoch no.5 train no.409440  loss = 3.69162 avg_loss = 2.95672\n",
      "epoch no.5 train no.409450  loss = 2.99739 avg_loss = 2.97761\n",
      "epoch no.5 train no.409460  loss = 3.99560 avg_loss = 3.04469\n",
      "epoch no.5 train no.409470  loss = 4.24358 avg_loss = 3.04926\n",
      "epoch no.5 train no.409480  loss = 4.13114 avg_loss = 3.04373\n",
      "epoch no.5 train no.409490  loss = 2.71002 avg_loss = 3.04006\n",
      "epoch no.5 train no.409500  loss = 3.84214 avg_loss = 3.02557\n",
      "epoch no.5 train no.409510  loss = 2.18694 avg_loss = 3.02215\n",
      "epoch no.5 train no.409520  loss = 4.05341 avg_loss = 3.04773\n",
      "epoch no.5 train no.409530  loss = 3.35592 avg_loss = 3.03138\n",
      "epoch no.5 train no.409540  loss = 2.66027 avg_loss = 3.04218\n",
      "epoch no.5 train no.409550  loss = 2.27689 avg_loss = 3.04553\n",
      "epoch no.5 train no.409560  loss = 3.70439 avg_loss = 3.05548\n",
      "epoch no.5 train no.409570  loss = 2.89496 avg_loss = 3.05619\n",
      "epoch no.5 train no.409580  loss = 2.60155 avg_loss = 3.06794\n",
      "epoch no.5 train no.409590  loss = 2.90463 avg_loss = 3.01487\n",
      "epoch no.5 train no.409600  loss = 3.68383 avg_loss = 3.00943\n",
      "epoch no.5 train no.409610  loss = 3.89896 avg_loss = 2.98594\n",
      "epoch no.5 train no.409620  loss = 2.81302 avg_loss = 2.98736\n",
      "epoch no.5 train no.409630  loss = 3.28107 avg_loss = 3.00500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.409640  loss = 4.48392 avg_loss = 3.00121\n",
      "epoch no.5 train no.409650  loss = 3.66264 avg_loss = 2.98004\n",
      "epoch no.5 train no.409660  loss = 1.69457 avg_loss = 2.98798\n",
      "epoch no.5 train no.409670  loss = 2.84862 avg_loss = 3.02684\n",
      "epoch no.5 train no.409680  loss = 4.35650 avg_loss = 3.01361\n",
      "epoch no.5 train no.409690  loss = 2.92289 avg_loss = 3.03172\n",
      "epoch no.5 train no.409700  loss = 3.07097 avg_loss = 3.01954\n",
      "epoch no.5 train no.409710  loss = 3.19031 avg_loss = 3.07086\n",
      "epoch no.5 train no.409720  loss = 3.19216 avg_loss = 3.05834\n",
      "epoch no.5 train no.409730  loss = 2.35644 avg_loss = 3.03360\n",
      "epoch no.5 train no.409740  loss = 3.16736 avg_loss = 3.06680\n",
      "epoch no.5 train no.409750  loss = 3.94999 avg_loss = 3.06149\n",
      "epoch no.5 train no.409760  loss = 4.44598 avg_loss = 3.09315\n",
      "epoch no.5 train no.409770  loss = 3.32430 avg_loss = 3.09769\n",
      "epoch no.5 train no.409780  loss = 1.96923 avg_loss = 3.05948\n",
      "epoch no.5 train no.409790  loss = 2.81045 avg_loss = 3.05947\n",
      "epoch no.5 train no.409800  loss = 2.28138 avg_loss = 3.04659\n",
      "epoch no.5 train no.409810  loss = 3.39482 avg_loss = 3.05814\n",
      "epoch no.5 train no.409820  loss = 3.44638 avg_loss = 3.05092\n",
      "epoch no.5 train no.409830  loss = 3.06609 avg_loss = 3.05955\n",
      "epoch no.5 train no.409840  loss = 3.42380 avg_loss = 3.07517\n",
      "epoch no.5 train no.409850  loss = 3.29909 avg_loss = 3.11920\n",
      "epoch no.5 train no.409860  loss = 3.20468 avg_loss = 3.09508\n",
      "epoch no.5 train no.409870  loss = 4.13399 avg_loss = 3.11102\n",
      "epoch no.5 train no.409880  loss = 2.26489 avg_loss = 3.06674\n",
      "epoch no.5 train no.409890  loss = 2.27436 avg_loss = 3.08985\n",
      "epoch no.5 train no.409900  loss = 3.20849 avg_loss = 3.11873\n",
      "epoch no.5 train no.409910  loss = 2.08612 avg_loss = 3.08840\n",
      "epoch no.5 train no.409920  loss = 4.00349 avg_loss = 3.06904\n",
      "epoch no.5 train no.409930  loss = 4.68286 avg_loss = 3.08711\n",
      "epoch no.5 train no.409940  loss = 2.15380 avg_loss = 3.06704\n",
      "epoch no.5 train no.409950  loss = 3.07277 avg_loss = 3.06829\n",
      "epoch no.5 train no.409960  loss = 2.52899 avg_loss = 3.02021\n",
      "epoch no.5 train no.409970  loss = 2.33737 avg_loss = 3.01439\n",
      "epoch no.5 train no.409980  loss = 2.82486 avg_loss = 3.01000\n",
      "epoch no.5 train no.409990  loss = 2.93602 avg_loss = 3.02782\n",
      "epoch no.5 train no.410000  loss = 3.05016 avg_loss = 3.01809\n",
      "5\n",
      "to_tokens: ['▁가을', '▁드라마', '년대', '▁가요', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 가요 발라드</s>\n",
      "epoch no.5 train no.410010  loss = 3.00602 avg_loss = 3.02745\n",
      "epoch no.5 train no.410020  loss = 3.12311 avg_loss = 2.99991\n",
      "epoch no.5 train no.410030  loss = 2.75333 avg_loss = 2.98812\n",
      "epoch no.5 train no.410040  loss = 3.36849 avg_loss = 2.98463\n",
      "epoch no.5 train no.410050  loss = 2.09408 avg_loss = 3.03460\n",
      "epoch no.5 train no.410060  loss = 1.47930 avg_loss = 3.00066\n",
      "epoch no.5 train no.410070  loss = 2.56395 avg_loss = 2.99906\n",
      "epoch no.5 train no.410080  loss = 5.99623 avg_loss = 3.03897\n",
      "epoch no.5 train no.410090  loss = 2.24801 avg_loss = 2.99701\n",
      "epoch no.5 train no.410100  loss = 1.97347 avg_loss = 2.99631\n",
      "epoch no.5 train no.410110  loss = 2.78388 avg_loss = 2.99678\n",
      "epoch no.5 train no.410120  loss = 3.46422 avg_loss = 3.03705\n",
      "epoch no.5 train no.410130  loss = 2.04220 avg_loss = 3.00867\n",
      "epoch no.5 train no.410140  loss = 2.90794 avg_loss = 2.99298\n",
      "epoch no.5 train no.410150  loss = 3.61870 avg_loss = 3.03116\n",
      "epoch no.5 train no.410160  loss = 4.32409 avg_loss = 3.06881\n",
      "epoch no.5 train no.410170  loss = 3.25590 avg_loss = 3.03619\n",
      "epoch no.5 train no.410180  loss = 3.32430 avg_loss = 3.02431\n",
      "epoch no.5 train no.410190  loss = 4.32732 avg_loss = 3.07302\n",
      "epoch no.5 train no.410200  loss = 4.25044 avg_loss = 3.10496\n",
      "epoch no.5 train no.410210  loss = 2.26661 avg_loss = 3.07703\n",
      "epoch no.5 train no.410220  loss = 2.27743 avg_loss = 3.06962\n",
      "epoch no.5 train no.410230  loss = 4.88902 avg_loss = 3.05578\n",
      "epoch no.5 train no.410240  loss = 3.06427 avg_loss = 3.07179\n",
      "epoch no.5 train no.410250  loss = 3.30316 avg_loss = 3.08589\n",
      "epoch no.5 train no.410260  loss = 3.30167 avg_loss = 3.07785\n",
      "epoch no.5 train no.410270  loss = 3.19410 avg_loss = 3.07016\n",
      "epoch no.5 train no.410280  loss = 3.77565 avg_loss = 3.09751\n",
      "epoch no.5 train no.410290  loss = 3.28097 avg_loss = 3.10428\n",
      "epoch no.5 train no.410300  loss = 2.23742 avg_loss = 3.13555\n",
      "epoch no.5 train no.410310  loss = 2.73113 avg_loss = 3.13041\n",
      "epoch no.5 train no.410320  loss = 1.60454 avg_loss = 3.14375\n",
      "epoch no.5 train no.410330  loss = 3.25917 avg_loss = 3.15935\n",
      "epoch no.5 train no.410340  loss = 1.31497 avg_loss = 3.12066\n",
      "epoch no.5 train no.410350  loss = 2.27773 avg_loss = 3.12551\n",
      "epoch no.5 train no.410360  loss = 2.96668 avg_loss = 3.09773\n",
      "epoch no.5 train no.410370  loss = 4.51096 avg_loss = 3.08237\n",
      "epoch no.5 train no.410380  loss = 2.97814 avg_loss = 3.07419\n",
      "epoch no.5 train no.410390  loss = 2.83449 avg_loss = 3.07672\n",
      "epoch no.5 train no.410400  loss = 2.97483 avg_loss = 3.08209\n",
      "epoch no.5 train no.410410  loss = 2.06365 avg_loss = 3.10892\n",
      "epoch no.5 train no.410420  loss = 3.50105 avg_loss = 3.09213\n",
      "epoch no.5 train no.410430  loss = 2.53818 avg_loss = 3.10724\n",
      "epoch no.5 train no.410440  loss = 1.92190 avg_loss = 3.09626\n",
      "epoch no.5 train no.410450  loss = 2.92242 avg_loss = 3.05312\n",
      "epoch no.5 train no.410460  loss = 1.85486 avg_loss = 3.04830\n",
      "epoch no.5 train no.410470  loss = 2.54705 avg_loss = 3.06534\n",
      "epoch no.5 train no.410480  loss = 1.49369 avg_loss = 3.06376\n",
      "epoch no.5 train no.410490  loss = 3.71306 avg_loss = 3.07771\n",
      "epoch no.5 train no.410500  loss = 2.89695 avg_loss = 3.10734\n",
      "epoch no.5 train no.410510  loss = 2.01025 avg_loss = 3.09101\n",
      "epoch no.5 train no.410520  loss = 2.36257 avg_loss = 3.08382\n",
      "epoch no.5 train no.410530  loss = 3.40685 avg_loss = 3.08763\n",
      "epoch no.5 train no.410540  loss = 3.19100 avg_loss = 3.09423\n",
      "epoch no.5 train no.410550  loss = 2.93388 avg_loss = 3.12258\n",
      "epoch no.5 train no.410560  loss = 2.32971 avg_loss = 3.11135\n",
      "epoch no.5 train no.410570  loss = 2.76543 avg_loss = 3.13800\n",
      "epoch no.5 train no.410580  loss = 2.60322 avg_loss = 3.14436\n",
      "epoch no.5 train no.410590  loss = 3.23266 avg_loss = 3.13620\n",
      "epoch no.5 train no.410600  loss = 2.97759 avg_loss = 3.14624\n",
      "epoch no.5 train no.410610  loss = 3.36252 avg_loss = 3.10725\n",
      "epoch no.5 train no.410620  loss = 3.88178 avg_loss = 3.09769\n",
      "epoch no.5 train no.410630  loss = 4.15166 avg_loss = 3.13175\n",
      "epoch no.5 train no.410640  loss = 3.70065 avg_loss = 3.13026\n",
      "epoch no.5 train no.410650  loss = 2.84078 avg_loss = 3.14235\n",
      "epoch no.5 train no.410660  loss = 3.42259 avg_loss = 3.12768\n",
      "epoch no.5 train no.410670  loss = 2.53871 avg_loss = 3.09130\n",
      "epoch no.5 train no.410680  loss = 3.48197 avg_loss = 3.10820\n",
      "epoch no.5 train no.410690  loss = 2.79698 avg_loss = 3.06793\n",
      "epoch no.5 train no.410700  loss = 4.70091 avg_loss = 3.07129\n",
      "epoch no.5 train no.410710  loss = 3.08996 avg_loss = 3.03184\n",
      "epoch no.5 train no.410720  loss = 4.50365 avg_loss = 3.07936\n",
      "epoch no.5 train no.410730  loss = 2.23078 avg_loss = 3.04722\n",
      "epoch no.5 train no.410740  loss = 1.75771 avg_loss = 2.99447\n",
      "epoch no.5 train no.410750  loss = 1.24031 avg_loss = 3.00797\n",
      "epoch no.5 train no.410760  loss = 3.91391 avg_loss = 3.03172\n",
      "epoch no.5 train no.410770  loss = 4.49192 avg_loss = 3.07250\n",
      "epoch no.5 train no.410780  loss = 3.34954 avg_loss = 3.10307\n",
      "epoch no.5 train no.410790  loss = 2.75097 avg_loss = 3.09292\n",
      "epoch no.5 train no.410800  loss = 3.33015 avg_loss = 3.07244\n",
      "epoch no.5 train no.410810  loss = 4.53119 avg_loss = 3.09044\n",
      "epoch no.5 train no.410820  loss = 2.86023 avg_loss = 3.07806\n",
      "epoch no.5 train no.410830  loss = 2.57307 avg_loss = 3.08940\n",
      "epoch no.5 train no.410840  loss = 2.62375 avg_loss = 3.09165\n",
      "epoch no.5 train no.410850  loss = 2.81345 avg_loss = 3.08200\n",
      "epoch no.5 train no.410860  loss = 2.55304 avg_loss = 3.10608\n",
      "epoch no.5 train no.410870  loss = 2.19034 avg_loss = 3.10142\n",
      "epoch no.5 train no.410880  loss = 2.87738 avg_loss = 3.11037\n",
      "epoch no.5 train no.410890  loss = 3.17809 avg_loss = 3.12221\n",
      "epoch no.5 train no.410900  loss = 2.35023 avg_loss = 3.07863\n",
      "epoch no.5 train no.410910  loss = 2.79884 avg_loss = 3.07978\n",
      "epoch no.5 train no.410920  loss = 2.41380 avg_loss = 3.11894\n",
      "epoch no.5 train no.410930  loss = 2.37990 avg_loss = 3.08531\n",
      "epoch no.5 train no.410940  loss = 2.43861 avg_loss = 3.12774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.410950  loss = 3.67278 avg_loss = 3.12411\n",
      "epoch no.5 train no.410960  loss = 2.71982 avg_loss = 3.13395\n",
      "epoch no.5 train no.410970  loss = 3.12299 avg_loss = 3.12154\n",
      "epoch no.5 train no.410980  loss = 3.54323 avg_loss = 3.10423\n",
      "epoch no.5 train no.410990  loss = 3.85457 avg_loss = 3.13211\n",
      "epoch no.5 train no.411000  loss = 4.22159 avg_loss = 3.12828\n",
      "8\n",
      "to_tokens: ['▁가을', '▁올드', 'p', '▁명', '▁함께', '▁기분', '▁가을', '시간', '▁걷기', '</s>']\n",
      "추억의 올드팝과 함께 하는 1시간 걷기</s>\n",
      "epoch no.5 train no.411010  loss = 3.14713 avg_loss = 3.11590\n",
      "epoch no.5 train no.411020  loss = 4.23329 avg_loss = 3.11713\n",
      "epoch no.5 train no.411030  loss = 3.90052 avg_loss = 3.13861\n",
      "epoch no.5 train no.411040  loss = 2.13790 avg_loss = 3.14128\n",
      "epoch no.5 train no.411050  loss = 3.48322 avg_loss = 3.13268\n",
      "epoch no.5 train no.411060  loss = 2.95038 avg_loss = 3.07443\n",
      "epoch no.5 train no.411070  loss = 2.84173 avg_loss = 3.08442\n",
      "epoch no.5 train no.411080  loss = 4.12725 avg_loss = 3.06278\n",
      "epoch no.5 train no.411090  loss = 2.53088 avg_loss = 3.05483\n",
      "epoch no.5 train no.411100  loss = 3.36421 avg_loss = 3.05502\n",
      "epoch no.5 train no.411110  loss = 3.76895 avg_loss = 3.10732\n",
      "epoch no.5 train no.411120  loss = 2.06061 avg_loss = 3.07109\n",
      "epoch no.5 train no.411130  loss = 2.07126 avg_loss = 3.07594\n",
      "epoch no.5 train no.411140  loss = 2.13361 avg_loss = 3.08773\n",
      "epoch no.5 train no.411150  loss = 2.05478 avg_loss = 3.05279\n",
      "epoch no.5 train no.411160  loss = 3.70541 avg_loss = 3.03217\n",
      "epoch no.5 train no.411170  loss = 2.66231 avg_loss = 3.03398\n",
      "epoch no.5 train no.411180  loss = 2.17907 avg_loss = 2.99862\n",
      "epoch no.5 train no.411190  loss = 2.46894 avg_loss = 3.01424\n",
      "epoch no.5 train no.411200  loss = 4.42196 avg_loss = 3.01577\n",
      "epoch no.5 train no.411210  loss = 2.58580 avg_loss = 3.06334\n",
      "epoch no.5 train no.411220  loss = 2.57574 avg_loss = 3.05699\n",
      "epoch no.5 train no.411230  loss = 2.01682 avg_loss = 3.01976\n",
      "epoch no.5 train no.411240  loss = 3.25821 avg_loss = 3.01720\n",
      "epoch no.5 train no.411250  loss = 3.40330 avg_loss = 2.99313\n",
      "epoch no.5 train no.411260  loss = 3.09547 avg_loss = 2.97537\n",
      "epoch no.5 train no.411270  loss = 1.51922 avg_loss = 2.99936\n",
      "epoch no.5 train no.411280  loss = 3.58983 avg_loss = 3.01931\n",
      "epoch no.5 train no.411290  loss = 2.52247 avg_loss = 3.00396\n",
      "epoch no.5 train no.411300  loss = 4.04092 avg_loss = 3.02043\n",
      "epoch no.5 train no.411310  loss = 2.41702 avg_loss = 3.05195\n",
      "epoch no.5 train no.411320  loss = 2.36438 avg_loss = 3.03027\n",
      "epoch no.5 train no.411330  loss = 3.91422 avg_loss = 3.01295\n",
      "epoch no.5 train no.411340  loss = 2.38967 avg_loss = 2.99765\n",
      "epoch no.5 train no.411350  loss = 3.58902 avg_loss = 3.03258\n",
      "epoch no.5 train no.411360  loss = 3.13210 avg_loss = 3.03020\n",
      "epoch no.5 train no.411370  loss = 4.25077 avg_loss = 3.04273\n",
      "epoch no.5 train no.411380  loss = 3.66129 avg_loss = 3.08931\n",
      "epoch no.5 train no.411390  loss = 3.94441 avg_loss = 3.08502\n",
      "epoch no.5 train no.411400  loss = 1.48539 avg_loss = 3.03936\n",
      "epoch no.5 train no.411410  loss = 4.13759 avg_loss = 3.06295\n",
      "epoch no.5 train no.411420  loss = 2.42645 avg_loss = 3.05798\n",
      "epoch no.5 train no.411430  loss = 3.30262 avg_loss = 3.07329\n",
      "epoch no.5 train no.411440  loss = 2.75365 avg_loss = 3.07024\n",
      "epoch no.5 train no.411450  loss = 1.74123 avg_loss = 3.02630\n",
      "epoch no.5 train no.411460  loss = 4.05738 avg_loss = 3.06092\n",
      "epoch no.5 train no.411470  loss = 2.67109 avg_loss = 3.05226\n",
      "epoch no.5 train no.411480  loss = 2.94870 avg_loss = 3.08553\n",
      "epoch no.5 train no.411490  loss = 2.88816 avg_loss = 3.05185\n",
      "epoch no.5 train no.411500  loss = 3.10337 avg_loss = 3.04252\n",
      "epoch no.5 train no.411510  loss = 2.16354 avg_loss = 3.04914\n",
      "epoch no.5 train no.411520  loss = 2.42501 avg_loss = 3.08881\n",
      "epoch no.5 train no.411530  loss = 3.03216 avg_loss = 3.11655\n",
      "epoch no.5 train no.411540  loss = 2.44644 avg_loss = 3.10736\n",
      "epoch no.5 train no.411550  loss = 4.50720 avg_loss = 3.11596\n",
      "epoch no.5 train no.411560  loss = 2.07313 avg_loss = 3.08258\n",
      "epoch no.5 train no.411570  loss = 2.85626 avg_loss = 3.11683\n",
      "epoch no.5 train no.411580  loss = 3.17178 avg_loss = 3.12595\n",
      "epoch no.5 train no.411590  loss = 2.66871 avg_loss = 3.09446\n",
      "epoch no.5 train no.411600  loss = 3.16557 avg_loss = 3.07996\n",
      "epoch no.5 train no.411610  loss = 1.94279 avg_loss = 3.10218\n",
      "epoch no.5 train no.411620  loss = 3.23014 avg_loss = 3.06141\n",
      "epoch no.5 train no.411630  loss = 3.74033 avg_loss = 3.09739\n",
      "epoch no.5 train no.411640  loss = 3.26779 avg_loss = 3.07740\n",
      "epoch no.5 train no.411650  loss = 2.53996 avg_loss = 3.05617\n",
      "epoch no.5 train no.411660  loss = 3.77791 avg_loss = 3.05016\n",
      "epoch no.5 train no.411670  loss = 3.03610 avg_loss = 3.04045\n",
      "epoch no.5 train no.411680  loss = 3.39669 avg_loss = 3.05323\n",
      "epoch no.5 train no.411690  loss = 2.32702 avg_loss = 3.12016\n",
      "epoch no.5 train no.411700  loss = 4.42389 avg_loss = 3.16337\n",
      "epoch no.5 train no.411710  loss = 3.00382 avg_loss = 3.15037\n",
      "epoch no.5 train no.411720  loss = 4.56868 avg_loss = 3.15976\n",
      "epoch no.5 train no.411730  loss = 2.96459 avg_loss = 3.16115\n",
      "epoch no.5 train no.411740  loss = 2.34204 avg_loss = 3.14626\n",
      "epoch no.5 train no.411750  loss = 3.87733 avg_loss = 3.14064\n",
      "epoch no.5 train no.411760  loss = 3.20969 avg_loss = 3.14055\n",
      "epoch no.5 train no.411770  loss = 2.57038 avg_loss = 3.10052\n",
      "epoch no.5 train no.411780  loss = 3.62236 avg_loss = 3.07087\n",
      "epoch no.5 train no.411790  loss = 2.86415 avg_loss = 3.05688\n",
      "epoch no.5 train no.411800  loss = 3.83807 avg_loss = 3.04246\n",
      "epoch no.5 train no.411810  loss = 2.24845 avg_loss = 3.03427\n",
      "epoch no.5 train no.411820  loss = 2.56206 avg_loss = 3.01572\n",
      "epoch no.5 train no.411830  loss = 2.18411 avg_loss = 2.98703\n",
      "epoch no.5 train no.411840  loss = 3.49073 avg_loss = 2.97845\n",
      "epoch no.5 train no.411850  loss = 2.86621 avg_loss = 2.98484\n",
      "epoch no.5 train no.411860  loss = 2.17876 avg_loss = 2.97916\n",
      "epoch no.5 train no.411870  loss = 5.86120 avg_loss = 2.99067\n",
      "epoch no.5 train no.411880  loss = 2.77016 avg_loss = 2.98701\n",
      "epoch no.5 train no.411890  loss = 2.33506 avg_loss = 2.99961\n",
      "epoch no.5 train no.411900  loss = 2.87638 avg_loss = 2.96537\n",
      "epoch no.5 train no.411910  loss = 2.98032 avg_loss = 2.95894\n",
      "epoch no.5 train no.411920  loss = 2.24585 avg_loss = 2.95830\n",
      "epoch no.5 train no.411930  loss = 3.49290 avg_loss = 3.02579\n",
      "epoch no.5 train no.411940  loss = 3.50369 avg_loss = 3.02880\n",
      "epoch no.5 train no.411950  loss = 4.23882 avg_loss = 3.06878\n",
      "epoch no.5 train no.411960  loss = 2.51002 avg_loss = 3.10514\n",
      "epoch no.5 train no.411970  loss = 2.68803 avg_loss = 3.09208\n",
      "epoch no.5 train no.411980  loss = 3.33384 avg_loss = 3.07251\n",
      "epoch no.5 train no.411990  loss = 2.83844 avg_loss = 3.11136\n",
      "epoch no.5 train no.412000  loss = 2.66229 avg_loss = 3.17452\n",
      "6\n",
      "to_tokens: ['▁비', '▁싸이', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드  bgm</s>\n",
      "epoch no.5 train no.412010  loss = 2.44370 avg_loss = 3.14037\n",
      "epoch no.5 train no.412020  loss = 2.09345 avg_loss = 3.11975\n",
      "epoch no.5 train no.412030  loss = 2.64521 avg_loss = 3.11266\n",
      "epoch no.5 train no.412040  loss = 2.09710 avg_loss = 3.09882\n",
      "epoch no.5 train no.412050  loss = 2.81421 avg_loss = 3.11061\n",
      "epoch no.5 train no.412060  loss = 5.37161 avg_loss = 3.12623\n",
      "epoch no.5 train no.412070  loss = 3.23096 avg_loss = 3.08938\n",
      "epoch no.5 train no.412080  loss = 4.35915 avg_loss = 3.12542\n",
      "epoch no.5 train no.412090  loss = 4.64207 avg_loss = 3.16872\n",
      "epoch no.5 train no.412100  loss = 2.18132 avg_loss = 3.14889\n",
      "epoch no.5 train no.412110  loss = 4.14606 avg_loss = 3.13279\n",
      "epoch no.5 train no.412120  loss = 1.43034 avg_loss = 3.13136\n",
      "epoch no.5 train no.412130  loss = 3.85578 avg_loss = 3.12659\n",
      "epoch no.5 train no.412140  loss = 2.76055 avg_loss = 3.11199\n",
      "epoch no.5 train no.412150  loss = 4.28478 avg_loss = 3.12335\n",
      "epoch no.5 train no.412160  loss = 3.17725 avg_loss = 3.08274\n",
      "epoch no.5 train no.412170  loss = 5.26191 avg_loss = 3.11417\n",
      "epoch no.5 train no.412180  loss = 2.15809 avg_loss = 3.08804\n",
      "epoch no.5 train no.412190  loss = 2.69370 avg_loss = 3.04519\n",
      "epoch no.5 train no.412200  loss = 3.89624 avg_loss = 3.02740\n",
      "epoch no.5 train no.412210  loss = 3.03072 avg_loss = 3.00742\n",
      "epoch no.5 train no.412220  loss = 3.40108 avg_loss = 3.03456\n",
      "epoch no.5 train no.412230  loss = 6.11084 avg_loss = 3.08141\n",
      "epoch no.5 train no.412240  loss = 1.97068 avg_loss = 3.08481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.412250  loss = 5.03528 avg_loss = 3.09179\n",
      "epoch no.5 train no.412260  loss = 1.35222 avg_loss = 3.06384\n",
      "epoch no.5 train no.412270  loss = 4.17494 avg_loss = 3.03603\n",
      "epoch no.5 train no.412280  loss = 2.18841 avg_loss = 3.01026\n",
      "epoch no.5 train no.412290  loss = 2.91404 avg_loss = 3.01706\n",
      "epoch no.5 train no.412300  loss = 3.14561 avg_loss = 3.05793\n",
      "epoch no.5 train no.412310  loss = 2.47105 avg_loss = 3.07858\n",
      "epoch no.5 train no.412320  loss = 2.78053 avg_loss = 3.07516\n",
      "epoch no.5 train no.412330  loss = 2.26317 avg_loss = 3.09105\n",
      "epoch no.5 train no.412340  loss = 2.81525 avg_loss = 3.14160\n",
      "epoch no.5 train no.412350  loss = 3.11439 avg_loss = 3.14314\n",
      "epoch no.5 train no.412360  loss = 2.87860 avg_loss = 3.10249\n",
      "epoch no.5 train no.412370  loss = 2.08398 avg_loss = 3.08932\n",
      "epoch no.5 train no.412380  loss = 2.63941 avg_loss = 3.06892\n",
      "epoch no.5 train no.412390  loss = 2.70286 avg_loss = 3.05442\n",
      "epoch no.5 train no.412400  loss = 2.17417 avg_loss = 3.02821\n",
      "epoch no.5 train no.412410  loss = 2.90786 avg_loss = 3.03839\n",
      "epoch no.5 train no.412420  loss = 2.28018 avg_loss = 3.02999\n",
      "epoch no.5 train no.412430  loss = 2.90660 avg_loss = 3.04016\n",
      "epoch no.5 train no.412440  loss = 2.82980 avg_loss = 3.01934\n",
      "epoch no.5 train no.412450  loss = 2.48288 avg_loss = 3.00355\n",
      "epoch no.5 train no.412460  loss = 3.55207 avg_loss = 3.02995\n",
      "epoch no.5 train no.412470  loss = 3.34951 avg_loss = 3.04039\n",
      "epoch no.5 train no.412480  loss = 1.55504 avg_loss = 3.01269\n",
      "epoch no.5 train no.412490  loss = 2.70364 avg_loss = 2.99922\n",
      "epoch no.5 train no.412500  loss = 3.01907 avg_loss = 3.01101\n",
      "epoch no.5 train no.412510  loss = 2.58505 avg_loss = 2.99146\n",
      "epoch no.5 train no.412520  loss = 1.84838 avg_loss = 2.97162\n",
      "epoch no.5 train no.412530  loss = 2.77150 avg_loss = 3.03779\n",
      "epoch no.5 train no.412540  loss = 3.16051 avg_loss = 3.09566\n",
      "epoch no.5 train no.412550  loss = 4.41241 avg_loss = 3.10551\n",
      "epoch no.5 train no.412560  loss = 3.58074 avg_loss = 3.12616\n",
      "epoch no.5 train no.412570  loss = 2.57312 avg_loss = 3.11090\n",
      "epoch no.5 train no.412580  loss = 3.93127 avg_loss = 3.10984\n",
      "epoch no.5 train no.412590  loss = 3.18779 avg_loss = 3.10092\n",
      "epoch no.5 train no.412600  loss = 3.17774 avg_loss = 3.09017\n",
      "epoch no.5 train no.412610  loss = 3.37783 avg_loss = 3.07023\n",
      "epoch no.5 train no.412620  loss = 1.99255 avg_loss = 3.01910\n",
      "epoch no.5 train no.412630  loss = 3.52346 avg_loss = 3.03052\n",
      "epoch no.5 train no.412640  loss = 2.57233 avg_loss = 3.04534\n",
      "epoch no.5 train no.412650  loss = 3.74424 avg_loss = 3.05096\n",
      "epoch no.5 train no.412660  loss = 3.60743 avg_loss = 3.07694\n",
      "epoch no.5 train no.412670  loss = 2.65095 avg_loss = 3.06104\n",
      "epoch no.5 train no.412680  loss = 1.73323 avg_loss = 3.04918\n",
      "epoch no.5 train no.412690  loss = 1.81213 avg_loss = 3.05102\n",
      "epoch no.5 train no.412700  loss = 4.14296 avg_loss = 3.06478\n",
      "epoch no.5 train no.412710  loss = 3.23151 avg_loss = 3.08655\n",
      "epoch no.5 train no.412720  loss = 2.24654 avg_loss = 3.05999\n",
      "epoch no.5 train no.412730  loss = 4.97202 avg_loss = 3.10124\n",
      "epoch no.5 train no.412740  loss = 2.65152 avg_loss = 3.07092\n",
      "epoch no.5 train no.412750  loss = 4.58516 avg_loss = 3.08421\n",
      "epoch no.5 train no.412760  loss = 3.29493 avg_loss = 3.05843\n",
      "epoch no.5 train no.412770  loss = 1.26761 avg_loss = 3.07844\n",
      "epoch no.5 train no.412780  loss = 3.45885 avg_loss = 3.11989\n",
      "epoch no.5 train no.412790  loss = 4.04762 avg_loss = 3.09614\n",
      "epoch no.5 train no.412800  loss = 4.31499 avg_loss = 3.10512\n",
      "epoch no.5 train no.412810  loss = 2.34136 avg_loss = 3.11487\n",
      "epoch no.5 train no.412820  loss = 3.31719 avg_loss = 3.15047\n",
      "epoch no.5 train no.412830  loss = 2.12014 avg_loss = 3.12945\n",
      "epoch no.5 train no.412840  loss = 2.87314 avg_loss = 3.11599\n",
      "epoch no.5 train no.412850  loss = 2.13447 avg_loss = 3.13062\n",
      "epoch no.5 train no.412860  loss = 3.22665 avg_loss = 3.09676\n",
      "epoch no.5 train no.412870  loss = 3.38317 avg_loss = 3.15422\n",
      "epoch no.5 train no.412880  loss = 2.49759 avg_loss = 3.13491\n",
      "epoch no.5 train no.412890  loss = 1.74151 avg_loss = 3.13660\n",
      "epoch no.5 train no.412900  loss = 1.62892 avg_loss = 3.10336\n",
      "epoch no.5 train no.412910  loss = 2.43918 avg_loss = 3.10446\n",
      "epoch no.5 train no.412920  loss = 4.27706 avg_loss = 3.11294\n",
      "epoch no.5 train no.412930  loss = 2.82922 avg_loss = 3.07876\n",
      "epoch no.5 train no.412940  loss = 4.40588 avg_loss = 3.05599\n",
      "epoch no.5 train no.412950  loss = 1.56936 avg_loss = 3.06329\n",
      "epoch no.5 train no.412960  loss = 1.89340 avg_loss = 3.08719\n",
      "epoch no.5 train no.412970  loss = 3.46502 avg_loss = 3.10025\n",
      "epoch no.5 train no.412980  loss = 2.43484 avg_loss = 3.05720\n",
      "epoch no.5 train no.412990  loss = 2.79404 avg_loss = 3.01661\n",
      "epoch no.5 train no.413000  loss = 2.25859 avg_loss = 2.99750\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁가요', '송', '</s>']\n",
      "추억의 90년대 팝송</s>\n",
      "epoch no.5 train no.413010  loss = 3.62211 avg_loss = 2.99364\n",
      "epoch no.5 train no.413020  loss = 3.45042 avg_loss = 3.00612\n",
      "epoch no.5 train no.413030  loss = 2.71875 avg_loss = 3.00623\n",
      "epoch no.5 train no.413040  loss = 1.85482 avg_loss = 2.98471\n",
      "epoch no.5 train no.413050  loss = 1.64109 avg_loss = 2.97790\n",
      "epoch no.5 train no.413060  loss = 3.16070 avg_loss = 2.98808\n",
      "epoch no.5 train no.413070  loss = 3.35589 avg_loss = 3.03742\n",
      "epoch no.5 train no.413080  loss = 2.75163 avg_loss = 3.02072\n",
      "epoch no.5 train no.413090  loss = 2.49361 avg_loss = 2.97463\n",
      "epoch no.5 train no.413100  loss = 3.23034 avg_loss = 2.99412\n",
      "epoch no.5 train no.413110  loss = 2.64658 avg_loss = 2.96512\n",
      "epoch no.5 train no.413120  loss = 3.37552 avg_loss = 3.00971\n",
      "epoch no.5 train no.413130  loss = 2.61825 avg_loss = 2.97904\n",
      "epoch no.5 train no.413140  loss = 1.75483 avg_loss = 2.92606\n",
      "epoch no.5 train no.413150  loss = 2.40649 avg_loss = 2.93606\n",
      "epoch no.5 train no.413160  loss = 2.88493 avg_loss = 2.92541\n",
      "epoch no.5 train no.413170  loss = 2.47173 avg_loss = 2.92170\n",
      "epoch no.5 train no.413180  loss = 3.57697 avg_loss = 2.95454\n",
      "epoch no.5 train no.413190  loss = 2.27689 avg_loss = 2.97880\n",
      "epoch no.5 train no.413200  loss = 2.92666 avg_loss = 3.00135\n",
      "epoch no.5 train no.413210  loss = 2.82649 avg_loss = 3.01260\n",
      "epoch no.5 train no.413220  loss = 2.41159 avg_loss = 2.98843\n",
      "epoch no.5 train no.413230  loss = 4.56646 avg_loss = 3.01813\n",
      "epoch no.5 train no.413240  loss = 4.46034 avg_loss = 3.04798\n",
      "epoch no.5 train no.413250  loss = 2.58291 avg_loss = 3.06553\n",
      "epoch no.5 train no.413260  loss = 3.12243 avg_loss = 3.10995\n",
      "epoch no.5 train no.413270  loss = 2.10542 avg_loss = 3.08694\n",
      "epoch no.5 train no.413280  loss = 3.00367 avg_loss = 3.06885\n",
      "epoch no.5 train no.413290  loss = 4.25495 avg_loss = 3.07864\n",
      "epoch no.5 train no.413300  loss = 1.75131 avg_loss = 3.09201\n",
      "epoch no.5 train no.413310  loss = 2.19003 avg_loss = 3.08655\n",
      "epoch no.5 train no.413320  loss = 2.61372 avg_loss = 3.11195\n",
      "epoch no.5 train no.413330  loss = 2.52376 avg_loss = 3.10093\n",
      "epoch no.5 train no.413340  loss = 2.60423 avg_loss = 3.11105\n",
      "epoch no.5 train no.413350  loss = 3.29129 avg_loss = 3.08406\n",
      "epoch no.5 train no.413360  loss = 1.59952 avg_loss = 3.06516\n",
      "epoch no.5 train no.413370  loss = 2.64062 avg_loss = 3.06387\n",
      "epoch no.5 train no.413380  loss = 3.87720 avg_loss = 3.12588\n",
      "epoch no.5 train no.413390  loss = 3.26873 avg_loss = 3.13985\n",
      "epoch no.5 train no.413400  loss = 2.67132 avg_loss = 3.10605\n",
      "epoch no.5 train no.413410  loss = 3.36022 avg_loss = 3.14162\n",
      "epoch no.5 train no.413420  loss = 2.38058 avg_loss = 3.11811\n",
      "epoch no.5 train no.413430  loss = 2.42172 avg_loss = 3.07975\n",
      "epoch no.5 train no.413440  loss = 2.01062 avg_loss = 3.00176\n",
      "epoch no.5 train no.413450  loss = 3.57889 avg_loss = 3.04451\n",
      "epoch no.5 train no.413460  loss = 4.05626 avg_loss = 3.07899\n",
      "epoch no.5 train no.413470  loss = 2.43438 avg_loss = 3.04092\n",
      "epoch no.5 train no.413480  loss = 3.23828 avg_loss = 3.05944\n",
      "epoch no.5 train no.413490  loss = 3.82656 avg_loss = 3.06623\n",
      "epoch no.5 train no.413500  loss = 2.76924 avg_loss = 3.05937\n",
      "epoch no.5 train no.413510  loss = 2.77760 avg_loss = 3.03435\n",
      "epoch no.5 train no.413520  loss = 2.84446 avg_loss = 3.01575\n",
      "epoch no.5 train no.413530  loss = 2.10050 avg_loss = 3.01099\n",
      "epoch no.5 train no.413540  loss = 2.95809 avg_loss = 3.01693\n",
      "epoch no.5 train no.413550  loss = 2.28714 avg_loss = 3.01424\n",
      "epoch no.5 train no.413560  loss = 3.07160 avg_loss = 3.03671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.413570  loss = 3.11923 avg_loss = 3.00784\n",
      "epoch no.5 train no.413580  loss = 3.64714 avg_loss = 3.00778\n",
      "epoch no.5 train no.413590  loss = 2.68461 avg_loss = 3.00124\n",
      "epoch no.5 train no.413600  loss = 2.17313 avg_loss = 2.96806\n",
      "epoch no.5 train no.413610  loss = 3.17269 avg_loss = 3.02459\n",
      "epoch no.5 train no.413620  loss = 4.56877 avg_loss = 3.05676\n",
      "epoch no.5 train no.413630  loss = 2.95997 avg_loss = 3.02742\n",
      "epoch no.5 train no.413640  loss = 2.31874 avg_loss = 3.00728\n",
      "epoch no.5 train no.413650  loss = 3.28864 avg_loss = 2.99604\n",
      "epoch no.5 train no.413660  loss = 2.97295 avg_loss = 2.99081\n",
      "epoch no.5 train no.413670  loss = 2.02864 avg_loss = 2.96148\n",
      "epoch no.5 train no.413680  loss = 1.83967 avg_loss = 2.97432\n",
      "epoch no.5 train no.413690  loss = 2.30097 avg_loss = 2.97515\n",
      "epoch no.5 train no.413700  loss = 2.34505 avg_loss = 2.97956\n",
      "epoch no.5 train no.413710  loss = 4.56762 avg_loss = 2.98569\n",
      "epoch no.5 train no.413720  loss = 3.64932 avg_loss = 2.95384\n",
      "epoch no.5 train no.413730  loss = 2.08717 avg_loss = 2.96551\n",
      "epoch no.5 train no.413740  loss = 1.96437 avg_loss = 2.97198\n",
      "epoch no.5 train no.413750  loss = 2.54587 avg_loss = 2.93190\n",
      "epoch no.5 train no.413760  loss = 2.96998 avg_loss = 2.97808\n",
      "epoch no.5 train no.413770  loss = 3.67273 avg_loss = 2.97845\n",
      "epoch no.5 train no.413780  loss = 2.72017 avg_loss = 3.02260\n",
      "epoch no.5 train no.413790  loss = 2.95234 avg_loss = 3.05087\n",
      "epoch no.5 train no.413800  loss = 3.76212 avg_loss = 3.05748\n",
      "epoch no.5 train no.413810  loss = 3.96220 avg_loss = 3.03632\n",
      "epoch no.5 train no.413820  loss = 2.44666 avg_loss = 3.03679\n",
      "epoch no.5 train no.413830  loss = 2.96221 avg_loss = 3.00118\n",
      "epoch no.5 train no.413840  loss = 3.52341 avg_loss = 2.99848\n",
      "epoch no.5 train no.413850  loss = 2.41605 avg_loss = 3.01336\n",
      "epoch no.5 train no.413860  loss = 1.98989 avg_loss = 2.97076\n",
      "epoch no.5 train no.413870  loss = 2.49930 avg_loss = 2.99026\n",
      "epoch no.5 train no.413880  loss = 1.72771 avg_loss = 3.01431\n",
      "epoch no.5 train no.413890  loss = 3.47476 avg_loss = 3.07891\n",
      "epoch no.5 train no.413900  loss = 4.70038 avg_loss = 3.10651\n",
      "epoch no.5 train no.413910  loss = 2.46809 avg_loss = 3.09314\n",
      "epoch no.5 train no.413920  loss = 2.91562 avg_loss = 3.07671\n",
      "epoch no.5 train no.413930  loss = 1.74009 avg_loss = 3.03951\n",
      "epoch no.5 train no.413940  loss = 3.07639 avg_loss = 3.05481\n",
      "epoch no.5 train no.413950  loss = 1.94005 avg_loss = 3.04175\n",
      "epoch no.5 train no.413960  loss = 2.26259 avg_loss = 3.01675\n",
      "epoch no.5 train no.413970  loss = 2.51886 avg_loss = 3.05193\n",
      "epoch no.5 train no.413980  loss = 2.76066 avg_loss = 3.04452\n",
      "epoch no.5 train no.413990  loss = 3.07612 avg_loss = 3.04806\n",
      "epoch no.5 train no.414000  loss = 2.77005 avg_loss = 3.09586\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.5 train no.414010  loss = 5.06533 avg_loss = 3.11047\n",
      "epoch no.5 train no.414020  loss = 5.30530 avg_loss = 3.09946\n",
      "epoch no.5 train no.414030  loss = 2.77974 avg_loss = 3.06834\n",
      "epoch no.5 train no.414040  loss = 2.89230 avg_loss = 3.02262\n",
      "epoch no.5 train no.414050  loss = 3.19632 avg_loss = 3.00433\n",
      "epoch no.5 train no.414060  loss = 2.50208 avg_loss = 3.01825\n",
      "epoch no.5 train no.414070  loss = 2.82098 avg_loss = 3.01102\n",
      "epoch no.5 train no.414080  loss = 3.45154 avg_loss = 2.99984\n",
      "epoch no.5 train no.414090  loss = 2.70400 avg_loss = 3.01473\n",
      "epoch no.5 train no.414100  loss = 1.97287 avg_loss = 2.99169\n",
      "epoch no.5 train no.414110  loss = 4.75836 avg_loss = 3.03625\n",
      "epoch no.5 train no.414120  loss = 2.11235 avg_loss = 2.99802\n",
      "epoch no.5 train no.414130  loss = 2.67311 avg_loss = 3.00437\n",
      "epoch no.5 train no.414140  loss = 2.87428 avg_loss = 2.99637\n",
      "epoch no.5 train no.414150  loss = 4.64247 avg_loss = 3.06822\n",
      "epoch no.5 train no.414160  loss = 4.70891 avg_loss = 3.07985\n",
      "epoch no.5 train no.414170  loss = 2.50124 avg_loss = 3.03219\n",
      "epoch no.5 train no.414180  loss = 3.71371 avg_loss = 3.06416\n",
      "epoch no.5 train no.414190  loss = 3.10550 avg_loss = 3.04575\n",
      "epoch no.5 train no.414200  loss = 2.99459 avg_loss = 2.98477\n",
      "epoch no.5 train no.414210  loss = 2.37717 avg_loss = 3.01325\n",
      "epoch no.5 train no.414220  loss = 3.44647 avg_loss = 2.97987\n",
      "epoch no.5 train no.414230  loss = 2.10067 avg_loss = 2.97326\n",
      "epoch no.5 train no.414240  loss = 2.16111 avg_loss = 2.95235\n",
      "epoch no.5 train no.414250  loss = 3.20280 avg_loss = 2.98572\n",
      "epoch no.5 train no.414260  loss = 2.02809 avg_loss = 3.01017\n",
      "epoch no.5 train no.414270  loss = 2.75910 avg_loss = 3.01541\n",
      "epoch no.5 train no.414280  loss = 2.52164 avg_loss = 3.02112\n",
      "epoch no.5 train no.414290  loss = 2.08936 avg_loss = 3.04396\n",
      "epoch no.5 train no.414300  loss = 4.56280 avg_loss = 3.06088\n",
      "epoch no.5 train no.414310  loss = 4.33717 avg_loss = 3.09252\n",
      "epoch no.5 train no.414320  loss = 2.78520 avg_loss = 3.07304\n",
      "epoch no.5 train no.414330  loss = 3.62744 avg_loss = 3.10459\n",
      "epoch no.5 train no.414340  loss = 3.21724 avg_loss = 3.09213\n",
      "epoch no.5 train no.414350  loss = 3.90918 avg_loss = 3.08694\n",
      "epoch no.5 train no.414360  loss = 3.44058 avg_loss = 3.07597\n",
      "epoch no.5 train no.414370  loss = 3.12112 avg_loss = 3.09323\n",
      "epoch no.5 train no.414380  loss = 5.08953 avg_loss = 3.12057\n",
      "epoch no.5 train no.414390  loss = 3.80125 avg_loss = 3.11164\n",
      "epoch no.5 train no.414400  loss = 2.22748 avg_loss = 3.12428\n",
      "epoch no.5 train no.414410  loss = 1.93455 avg_loss = 3.08758\n",
      "epoch no.5 train no.414420  loss = 1.90829 avg_loss = 3.07205\n",
      "epoch no.5 train no.414430  loss = 3.02229 avg_loss = 3.06159\n",
      "epoch no.5 train no.414440  loss = 4.09749 avg_loss = 3.05140\n",
      "epoch no.5 train no.414450  loss = 1.89408 avg_loss = 3.03538\n",
      "epoch no.5 train no.414460  loss = 3.19045 avg_loss = 3.02582\n",
      "epoch no.5 train no.414470  loss = 4.02627 avg_loss = 3.00764\n",
      "epoch no.5 train no.414480  loss = 2.21228 avg_loss = 3.05235\n",
      "epoch no.5 train no.414490  loss = 3.01614 avg_loss = 3.05220\n",
      "epoch no.5 train no.414500  loss = 2.02517 avg_loss = 3.05901\n",
      "epoch no.5 train no.414510  loss = 2.48488 avg_loss = 3.07374\n",
      "epoch no.5 train no.414520  loss = 2.24628 avg_loss = 3.06629\n",
      "epoch no.5 train no.414530  loss = 4.14002 avg_loss = 3.05860\n",
      "epoch no.5 train no.414540  loss = 3.51982 avg_loss = 3.06237\n",
      "epoch no.5 train no.414550  loss = 3.55495 avg_loss = 3.08470\n",
      "epoch no.5 train no.414560  loss = 4.15162 avg_loss = 3.11721\n",
      "epoch no.5 train no.414570  loss = 2.95229 avg_loss = 3.12885\n",
      "epoch no.5 train no.414580  loss = 2.32493 avg_loss = 3.13746\n",
      "epoch no.5 train no.414590  loss = 2.65746 avg_loss = 3.12460\n",
      "epoch no.5 train no.414600  loss = 3.37909 avg_loss = 3.12440\n",
      "epoch no.5 train no.414610  loss = 1.73994 avg_loss = 3.12278\n",
      "epoch no.5 train no.414620  loss = 3.22622 avg_loss = 3.16140\n",
      "epoch no.5 train no.414630  loss = 3.62680 avg_loss = 3.16062\n",
      "epoch no.5 train no.414640  loss = 2.72030 avg_loss = 3.11246\n",
      "epoch no.5 train no.414650  loss = 3.14104 avg_loss = 3.16085\n",
      "epoch no.5 train no.414660  loss = 3.04156 avg_loss = 3.13492\n",
      "epoch no.5 train no.414670  loss = 1.50163 avg_loss = 3.14747\n",
      "epoch no.5 train no.414680  loss = 3.96489 avg_loss = 3.13007\n",
      "epoch no.5 train no.414690  loss = 3.16156 avg_loss = 3.12477\n",
      "epoch no.5 train no.414700  loss = 2.92787 avg_loss = 3.12476\n",
      "epoch no.5 train no.414710  loss = 2.96850 avg_loss = 3.09772\n",
      "epoch no.5 train no.414720  loss = 2.71125 avg_loss = 3.08526\n",
      "epoch no.5 train no.414730  loss = 2.32195 avg_loss = 3.10854\n",
      "epoch no.5 train no.414740  loss = 2.40779 avg_loss = 3.11821\n",
      "epoch no.5 train no.414750  loss = 4.32212 avg_loss = 3.11721\n",
      "epoch no.5 train no.414760  loss = 2.81168 avg_loss = 3.12273\n",
      "epoch no.5 train no.414770  loss = 2.45134 avg_loss = 3.08648\n",
      "epoch no.5 train no.414780  loss = 1.76597 avg_loss = 3.06173\n",
      "epoch no.5 train no.414790  loss = 2.17585 avg_loss = 3.05786\n",
      "epoch no.5 train no.414800  loss = 3.39558 avg_loss = 3.03784\n",
      "epoch no.5 train no.414810  loss = 3.99188 avg_loss = 3.08444\n",
      "epoch no.5 train no.414820  loss = 1.93068 avg_loss = 3.06469\n",
      "epoch no.5 train no.414830  loss = 3.15030 avg_loss = 3.02639\n",
      "epoch no.5 train no.414840  loss = 2.45719 avg_loss = 3.04296\n",
      "epoch no.5 train no.414850  loss = 3.37113 avg_loss = 3.07151\n",
      "epoch no.5 train no.414860  loss = 2.51598 avg_loss = 3.04570\n",
      "epoch no.5 train no.414870  loss = 2.34232 avg_loss = 3.00539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.414880  loss = 2.85334 avg_loss = 2.99645\n",
      "epoch no.5 train no.414890  loss = 2.63660 avg_loss = 2.96260\n",
      "epoch no.5 train no.414900  loss = 3.76704 avg_loss = 2.97505\n",
      "epoch no.5 train no.414910  loss = 2.27019 avg_loss = 2.97994\n",
      "epoch no.5 train no.414920  loss = 3.17257 avg_loss = 3.03130\n",
      "epoch no.5 train no.414930  loss = 3.67413 avg_loss = 3.00142\n",
      "epoch no.5 train no.414940  loss = 5.20562 avg_loss = 3.01797\n",
      "epoch no.5 train no.414950  loss = 3.68903 avg_loss = 3.04645\n",
      "epoch no.5 train no.414960  loss = 2.61273 avg_loss = 3.02046\n",
      "epoch no.5 train no.414970  loss = 3.15233 avg_loss = 3.01382\n",
      "epoch no.5 train no.414980  loss = 3.64082 avg_loss = 3.06736\n",
      "epoch no.5 train no.414990  loss = 2.35944 avg_loss = 3.04342\n",
      "epoch no.5 train no.415000  loss = 2.61107 avg_loss = 3.08906\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '팝', '▁모음', '곡', '</s>']\n",
      "추억의 올드팝 명곡</s>\n",
      "epoch no.5 train no.415010  loss = 2.52040 avg_loss = 3.08327\n",
      "epoch no.5 train no.415020  loss = 2.91513 avg_loss = 3.08754\n",
      "epoch no.5 train no.415030  loss = 4.33226 avg_loss = 3.08976\n",
      "epoch no.5 train no.415040  loss = 3.63142 avg_loss = 3.08207\n",
      "epoch no.5 train no.415050  loss = 2.79012 avg_loss = 3.12226\n",
      "epoch no.5 train no.415060  loss = 2.90216 avg_loss = 3.09957\n",
      "epoch no.5 train no.415070  loss = 2.35309 avg_loss = 3.09425\n",
      "epoch no.5 train no.415080  loss = 2.85957 avg_loss = 3.08304\n",
      "epoch no.5 train no.415090  loss = 3.72610 avg_loss = 3.11384\n",
      "epoch no.5 train no.415100  loss = 2.21952 avg_loss = 3.11415\n",
      "epoch no.5 train no.415110  loss = 2.96028 avg_loss = 3.12394\n",
      "epoch no.5 train no.415120  loss = 3.29014 avg_loss = 3.16303\n",
      "epoch no.5 train no.415130  loss = 1.42499 avg_loss = 3.11405\n",
      "epoch no.5 train no.415140  loss = 2.59203 avg_loss = 3.10608\n",
      "epoch no.5 train no.415150  loss = 2.95890 avg_loss = 3.09087\n",
      "epoch no.5 train no.415160  loss = 3.48696 avg_loss = 3.12368\n",
      "epoch no.5 train no.415170  loss = 2.29725 avg_loss = 3.08296\n",
      "epoch no.5 train no.415180  loss = 2.88188 avg_loss = 3.05502\n",
      "epoch no.5 train no.415190  loss = 3.73498 avg_loss = 3.08823\n",
      "epoch no.5 train no.415200  loss = 2.31132 avg_loss = 3.06964\n",
      "epoch no.5 train no.415210  loss = 3.36375 avg_loss = 3.05075\n",
      "epoch no.5 train no.415220  loss = 2.17027 avg_loss = 3.09451\n",
      "epoch no.5 train no.415230  loss = 2.04841 avg_loss = 3.08861\n",
      "epoch no.5 train no.415240  loss = 1.60418 avg_loss = 3.05309\n",
      "epoch no.5 train no.415250  loss = 3.60248 avg_loss = 3.05931\n",
      "epoch no.5 train no.415260  loss = 1.95116 avg_loss = 3.07878\n",
      "epoch no.5 train no.415270  loss = 3.05512 avg_loss = 3.08855\n",
      "epoch no.5 train no.415280  loss = 3.78367 avg_loss = 3.06618\n",
      "epoch no.5 train no.415290  loss = 4.23482 avg_loss = 3.09385\n",
      "epoch no.5 train no.415300  loss = 4.30609 avg_loss = 3.13799\n",
      "epoch no.5 train no.415310  loss = 3.66565 avg_loss = 3.13769\n",
      "epoch no.5 train no.415320  loss = 2.26853 avg_loss = 3.11327\n",
      "epoch no.5 train no.415330  loss = 2.91447 avg_loss = 3.13749\n",
      "epoch no.5 train no.415340  loss = 3.59837 avg_loss = 3.12813\n",
      "epoch no.5 train no.415350  loss = 2.84708 avg_loss = 3.11948\n",
      "epoch no.5 train no.415360  loss = 3.05271 avg_loss = 3.09904\n",
      "epoch no.5 train no.415370  loss = 3.09135 avg_loss = 3.08116\n",
      "epoch no.5 train no.415380  loss = 4.15529 avg_loss = 3.06749\n",
      "epoch no.5 train no.415390  loss = 3.70186 avg_loss = 3.09256\n",
      "epoch no.5 train no.415400  loss = 2.21526 avg_loss = 3.09923\n",
      "epoch no.5 train no.415410  loss = 4.08605 avg_loss = 3.13346\n",
      "epoch no.5 train no.415420  loss = 2.20696 avg_loss = 3.13799\n",
      "epoch no.5 train no.415430  loss = 2.13628 avg_loss = 3.09818\n",
      "epoch no.5 train no.415440  loss = 3.54184 avg_loss = 3.07638\n",
      "epoch no.5 train no.415450  loss = 3.33516 avg_loss = 3.06739\n",
      "epoch no.5 train no.415460  loss = 2.95789 avg_loss = 3.08663\n",
      "epoch no.5 train no.415470  loss = 3.28060 avg_loss = 3.08292\n",
      "epoch no.5 train no.415480  loss = 2.05895 avg_loss = 3.07055\n",
      "epoch no.5 train no.415490  loss = 2.33107 avg_loss = 3.08884\n",
      "epoch no.5 train no.415500  loss = 1.63717 avg_loss = 3.07288\n",
      "epoch no.5 train no.415510  loss = 1.82740 avg_loss = 3.03625\n",
      "epoch no.5 train no.415520  loss = 4.10979 avg_loss = 3.01454\n",
      "epoch no.5 train no.415530  loss = 3.38250 avg_loss = 3.07782\n",
      "epoch no.5 train no.415540  loss = 3.16983 avg_loss = 3.07844\n",
      "epoch no.5 train no.415550  loss = 3.80961 avg_loss = 3.08179\n",
      "epoch no.5 train no.415560  loss = 5.36489 avg_loss = 3.07821\n",
      "epoch no.5 train no.415570  loss = 3.37119 avg_loss = 3.09834\n",
      "epoch no.5 train no.415580  loss = 4.43782 avg_loss = 3.07061\n",
      "epoch no.5 train no.415590  loss = 3.21115 avg_loss = 3.05730\n",
      "epoch no.5 train no.415600  loss = 3.04129 avg_loss = 3.08015\n",
      "epoch no.5 train no.415610  loss = 3.79232 avg_loss = 3.10201\n",
      "epoch no.5 train no.415620  loss = 3.61444 avg_loss = 3.06853\n",
      "epoch no.5 train no.415630  loss = 2.36283 avg_loss = 3.04459\n",
      "epoch no.5 train no.415640  loss = 2.96384 avg_loss = 3.03937\n",
      "epoch no.5 train no.415650  loss = 4.10708 avg_loss = 3.04906\n",
      "epoch no.5 train no.415660  loss = 2.45446 avg_loss = 3.04032\n",
      "epoch no.5 train no.415670  loss = 3.18697 avg_loss = 3.09884\n",
      "epoch no.5 train no.415680  loss = 3.35498 avg_loss = 3.02673\n",
      "epoch no.5 train no.415690  loss = 4.17940 avg_loss = 3.07081\n",
      "epoch no.5 train no.415700  loss = 2.93934 avg_loss = 3.07433\n",
      "epoch no.5 train no.415710  loss = 4.15088 avg_loss = 3.05116\n",
      "epoch no.5 train no.415720  loss = 2.98230 avg_loss = 3.05005\n",
      "epoch no.5 train no.415730  loss = 3.00489 avg_loss = 3.05480\n",
      "epoch no.5 train no.415740  loss = 2.48401 avg_loss = 3.03555\n",
      "epoch no.5 train no.415750  loss = 2.78786 avg_loss = 3.06493\n",
      "epoch no.5 train no.415760  loss = 3.76842 avg_loss = 3.03957\n",
      "epoch no.5 train no.415770  loss = 3.63184 avg_loss = 3.06844\n",
      "epoch no.5 train no.415780  loss = 5.45024 avg_loss = 3.11800\n",
      "epoch no.5 train no.415790  loss = 2.90255 avg_loss = 3.07158\n",
      "epoch no.5 train no.415800  loss = 2.51567 avg_loss = 3.07279\n",
      "epoch no.5 train no.415810  loss = 2.62469 avg_loss = 3.04682\n",
      "epoch no.5 train no.415820  loss = 2.43553 avg_loss = 3.08736\n",
      "epoch no.5 train no.415830  loss = 1.87690 avg_loss = 3.11499\n",
      "epoch no.5 train no.415840  loss = 2.31985 avg_loss = 3.08404\n",
      "epoch no.5 train no.415850  loss = 4.88080 avg_loss = 3.13972\n",
      "epoch no.5 train no.415860  loss = 2.29907 avg_loss = 3.11215\n",
      "epoch no.5 train no.415870  loss = 4.54623 avg_loss = 3.10741\n",
      "epoch no.5 train no.415880  loss = 4.13437 avg_loss = 3.10896\n",
      "epoch no.5 train no.415890  loss = 3.68444 avg_loss = 3.10840\n",
      "epoch no.5 train no.415900  loss = 2.57126 avg_loss = 3.06046\n",
      "epoch no.5 train no.415910  loss = 2.19259 avg_loss = 3.06797\n",
      "epoch no.5 train no.415920  loss = 2.47324 avg_loss = 3.02595\n",
      "epoch no.5 train no.415930  loss = 3.34964 avg_loss = 3.06398\n",
      "epoch no.5 train no.415940  loss = 2.92298 avg_loss = 3.06370\n",
      "epoch no.5 train no.415950  loss = 1.49329 avg_loss = 3.07027\n",
      "epoch no.5 train no.415960  loss = 3.68218 avg_loss = 3.09030\n",
      "epoch no.5 train no.415970  loss = 3.39179 avg_loss = 3.07621\n",
      "epoch no.5 train no.415980  loss = 2.41255 avg_loss = 3.06711\n",
      "epoch no.5 train no.415990  loss = 3.49737 avg_loss = 3.10668\n",
      "epoch no.5 train no.416000  loss = 4.26744 avg_loss = 3.11385\n",
      "6\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '▁발라', '드', '</s>']\n",
      "추억의 2000년대  남자 발라드</s>\n",
      "epoch no.5 train no.416010  loss = 3.03614 avg_loss = 3.14085\n",
      "epoch no.5 train no.416020  loss = 3.13255 avg_loss = 3.11067\n",
      "epoch no.5 train no.416030  loss = 2.18017 avg_loss = 3.08042\n",
      "epoch no.5 train no.416040  loss = 1.52381 avg_loss = 3.02681\n",
      "epoch no.5 train no.416050  loss = 2.08119 avg_loss = 3.05616\n",
      "epoch no.5 train no.416060  loss = 2.20372 avg_loss = 3.05644\n",
      "epoch no.5 train no.416070  loss = 4.33614 avg_loss = 3.08335\n",
      "epoch no.5 train no.416080  loss = 4.21582 avg_loss = 3.12082\n",
      "epoch no.5 train no.416090  loss = 3.15208 avg_loss = 3.11745\n",
      "epoch no.5 train no.416100  loss = 2.54553 avg_loss = 3.08688\n",
      "epoch no.5 train no.416110  loss = 4.03406 avg_loss = 3.07143\n",
      "epoch no.5 train no.416120  loss = 3.02593 avg_loss = 3.07626\n",
      "epoch no.5 train no.416130  loss = 1.97124 avg_loss = 3.01835\n",
      "epoch no.5 train no.416140  loss = 2.70095 avg_loss = 3.04756\n",
      "epoch no.5 train no.416150  loss = 2.53073 avg_loss = 3.06130\n",
      "epoch no.5 train no.416160  loss = 3.68379 avg_loss = 3.06420\n",
      "epoch no.5 train no.416170  loss = 4.08220 avg_loss = 3.08264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.416180  loss = 3.25738 avg_loss = 3.06217\n",
      "epoch no.5 train no.416190  loss = 2.72383 avg_loss = 3.01764\n",
      "epoch no.5 train no.416200  loss = 4.28025 avg_loss = 3.05287\n",
      "epoch no.5 train no.416210  loss = 4.46995 avg_loss = 3.11203\n",
      "epoch no.5 train no.416220  loss = 3.35510 avg_loss = 3.14064\n",
      "epoch no.5 train no.416230  loss = 3.21781 avg_loss = 3.13090\n",
      "epoch no.5 train no.416240  loss = 4.24234 avg_loss = 3.09902\n",
      "epoch no.5 train no.416250  loss = 2.71842 avg_loss = 3.09090\n",
      "epoch no.5 train no.416260  loss = 3.14021 avg_loss = 3.13491\n",
      "epoch no.5 train no.416270  loss = 3.76210 avg_loss = 3.10688\n",
      "epoch no.5 train no.416280  loss = 4.23401 avg_loss = 3.11509\n",
      "epoch no.5 train no.416290  loss = 2.67669 avg_loss = 3.15187\n",
      "epoch no.5 train no.416300  loss = 3.80853 avg_loss = 3.12403\n",
      "epoch no.5 train no.416310  loss = 4.94637 avg_loss = 3.12800\n",
      "epoch no.5 train no.416320  loss = 2.88851 avg_loss = 3.10198\n",
      "epoch no.5 train no.416330  loss = 3.48960 avg_loss = 3.10251\n",
      "epoch no.5 train no.416340  loss = 3.40925 avg_loss = 3.13196\n",
      "epoch no.5 train no.416350  loss = 3.04617 avg_loss = 3.12242\n",
      "epoch no.5 train no.416360  loss = 3.28070 avg_loss = 3.16506\n",
      "epoch no.5 train no.416370  loss = 1.87112 avg_loss = 3.13702\n",
      "epoch no.5 train no.416380  loss = 1.70545 avg_loss = 3.09429\n",
      "epoch no.5 train no.416390  loss = 5.43883 avg_loss = 3.08375\n",
      "epoch no.5 train no.416400  loss = 3.40030 avg_loss = 3.09840\n",
      "epoch no.5 train no.416410  loss = 3.69134 avg_loss = 3.08558\n",
      "epoch no.5 train no.416420  loss = 2.72262 avg_loss = 3.04330\n",
      "epoch no.5 train no.416430  loss = 2.69827 avg_loss = 3.00825\n",
      "epoch no.5 train no.416440  loss = 5.09302 avg_loss = 3.02686\n",
      "epoch no.5 train no.416450  loss = 3.57223 avg_loss = 3.02872\n",
      "epoch no.5 train no.416460  loss = 3.64306 avg_loss = 3.02854\n",
      "epoch no.5 train no.416470  loss = 4.31509 avg_loss = 3.03359\n",
      "epoch no.5 train no.416480  loss = 3.57177 avg_loss = 3.03500\n",
      "epoch no.5 train no.416490  loss = 1.65315 avg_loss = 2.97489\n",
      "epoch no.5 train no.416500  loss = 1.75564 avg_loss = 2.96674\n",
      "epoch no.5 train no.416510  loss = 3.06474 avg_loss = 2.99906\n",
      "epoch no.5 train no.416520  loss = 2.52577 avg_loss = 3.00121\n",
      "epoch no.5 train no.416530  loss = 3.39628 avg_loss = 2.98055\n",
      "epoch no.5 train no.416540  loss = 3.37075 avg_loss = 2.99593\n",
      "epoch no.5 train no.416550  loss = 3.58643 avg_loss = 2.98846\n",
      "epoch no.5 train no.416560  loss = 2.23272 avg_loss = 3.01967\n",
      "epoch no.5 train no.416570  loss = 3.09436 avg_loss = 3.03624\n",
      "epoch no.5 train no.416580  loss = 1.97413 avg_loss = 3.03018\n",
      "epoch no.5 train no.416590  loss = 3.63959 avg_loss = 3.05719\n",
      "epoch no.5 train no.416600  loss = 2.18800 avg_loss = 3.06040\n",
      "epoch no.5 train no.416610  loss = 3.08202 avg_loss = 3.06252\n",
      "epoch no.5 train no.416620  loss = 3.72920 avg_loss = 3.12452\n",
      "epoch no.5 train no.416630  loss = 2.51709 avg_loss = 3.10162\n",
      "epoch no.5 train no.416640  loss = 3.14556 avg_loss = 3.08481\n",
      "epoch no.5 train no.416650  loss = 3.55450 avg_loss = 3.05102\n",
      "epoch no.5 train no.416660  loss = 3.00384 avg_loss = 3.07784\n",
      "epoch no.5 train no.416670  loss = 2.44589 avg_loss = 3.09595\n",
      "epoch no.5 train no.416680  loss = 4.47528 avg_loss = 3.07648\n",
      "epoch no.5 train no.416690  loss = 2.74276 avg_loss = 3.06646\n",
      "epoch no.5 train no.416700  loss = 2.05719 avg_loss = 3.04057\n",
      "epoch no.5 train no.416710  loss = 2.68044 avg_loss = 3.08514\n",
      "epoch no.5 train no.416720  loss = 3.54773 avg_loss = 3.08939\n",
      "epoch no.5 train no.416730  loss = 2.57227 avg_loss = 3.08594\n",
      "epoch no.5 train no.416740  loss = 1.83030 avg_loss = 3.10620\n",
      "epoch no.5 train no.416750  loss = 3.66691 avg_loss = 3.16314\n",
      "epoch no.5 train no.416760  loss = 3.41877 avg_loss = 3.17438\n",
      "epoch no.5 train no.416770  loss = 2.86588 avg_loss = 3.20022\n",
      "epoch no.5 train no.416780  loss = 3.08060 avg_loss = 3.23415\n",
      "epoch no.5 train no.416790  loss = 3.20871 avg_loss = 3.19252\n",
      "epoch no.5 train no.416800  loss = 2.74553 avg_loss = 3.15482\n",
      "epoch no.5 train no.416810  loss = 3.61318 avg_loss = 3.13132\n",
      "epoch no.5 train no.416820  loss = 4.07224 avg_loss = 3.11671\n",
      "epoch no.5 train no.416830  loss = 3.38276 avg_loss = 3.14663\n",
      "epoch no.5 train no.416840  loss = 3.36936 avg_loss = 3.13551\n",
      "epoch no.5 train no.416850  loss = 2.96012 avg_loss = 3.12637\n",
      "epoch no.5 train no.416860  loss = 3.08755 avg_loss = 3.14030\n",
      "epoch no.5 train no.416870  loss = 3.13348 avg_loss = 3.09819\n",
      "epoch no.5 train no.416880  loss = 4.01163 avg_loss = 3.14224\n",
      "epoch no.5 train no.416890  loss = 4.13078 avg_loss = 3.14166\n",
      "epoch no.5 train no.416900  loss = 2.48793 avg_loss = 3.12862\n",
      "epoch no.5 train no.416910  loss = 2.24604 avg_loss = 3.12127\n",
      "epoch no.5 train no.416920  loss = 1.86762 avg_loss = 3.06539\n",
      "epoch no.5 train no.416930  loss = 3.78290 avg_loss = 3.08163\n",
      "epoch no.5 train no.416940  loss = 2.41583 avg_loss = 3.09162\n",
      "epoch no.5 train no.416950  loss = 1.14711 avg_loss = 3.06004\n",
      "epoch no.5 train no.416960  loss = 3.99894 avg_loss = 3.10827\n",
      "epoch no.5 train no.416970  loss = 2.49104 avg_loss = 3.05807\n",
      "epoch no.5 train no.416980  loss = 3.25615 avg_loss = 3.03925\n",
      "epoch no.5 train no.416990  loss = 3.55567 avg_loss = 3.00245\n",
      "epoch no.5 train no.417000  loss = 3.85593 avg_loss = 3.00399\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '년대', '▁댄스', '▁모음', '</s>']\n",
      "추억의 90년대 노래 모음</s>\n",
      "epoch no.5 train no.417010  loss = 3.46056 avg_loss = 3.03900\n",
      "epoch no.5 train no.417020  loss = 3.42122 avg_loss = 3.04655\n",
      "epoch no.5 train no.417030  loss = 3.93638 avg_loss = 3.07681\n",
      "epoch no.5 train no.417040  loss = 4.42883 avg_loss = 3.09037\n",
      "epoch no.5 train no.417050  loss = 2.14360 avg_loss = 3.09383\n",
      "epoch no.5 train no.417060  loss = 2.54205 avg_loss = 3.06532\n",
      "epoch no.5 train no.417070  loss = 2.37212 avg_loss = 3.08982\n",
      "epoch no.5 train no.417080  loss = 2.91843 avg_loss = 3.08534\n",
      "epoch no.5 train no.417090  loss = 3.33580 avg_loss = 3.06996\n",
      "epoch no.5 train no.417100  loss = 3.28165 avg_loss = 3.07712\n",
      "epoch no.5 train no.417110  loss = 1.45383 avg_loss = 3.07160\n",
      "epoch no.5 train no.417120  loss = 2.96460 avg_loss = 3.08529\n",
      "epoch no.5 train no.417130  loss = 4.01373 avg_loss = 3.09932\n",
      "epoch no.5 train no.417140  loss = 4.29379 avg_loss = 3.11347\n",
      "epoch no.5 train no.417150  loss = 2.53744 avg_loss = 3.16564\n",
      "epoch no.5 train no.417160  loss = 2.61417 avg_loss = 3.10753\n",
      "epoch no.5 train no.417170  loss = 3.48849 avg_loss = 3.10051\n",
      "epoch no.5 train no.417180  loss = 3.08610 avg_loss = 3.11995\n",
      "epoch no.5 train no.417190  loss = 2.98547 avg_loss = 3.10052\n",
      "epoch no.5 train no.417200  loss = 4.31197 avg_loss = 3.08383\n",
      "epoch no.5 train no.417210  loss = 2.74434 avg_loss = 3.10514\n",
      "epoch no.5 train no.417220  loss = 4.64505 avg_loss = 3.13740\n",
      "epoch no.5 train no.417230  loss = 3.63985 avg_loss = 3.11199\n",
      "epoch no.5 train no.417240  loss = 2.81428 avg_loss = 3.10378\n",
      "epoch no.5 train no.417250  loss = 4.07716 avg_loss = 3.09682\n",
      "epoch no.5 train no.417260  loss = 3.48077 avg_loss = 3.10519\n",
      "epoch no.5 train no.417270  loss = 2.53606 avg_loss = 3.14094\n",
      "epoch no.5 train no.417280  loss = 1.99832 avg_loss = 3.13808\n",
      "epoch no.5 train no.417290  loss = 2.67664 avg_loss = 3.12937\n",
      "epoch no.5 train no.417300  loss = 3.26497 avg_loss = 3.10688\n",
      "epoch no.5 train no.417310  loss = 3.26169 avg_loss = 3.10797\n",
      "epoch no.5 train no.417320  loss = 2.80928 avg_loss = 3.08515\n",
      "epoch no.5 train no.417330  loss = 3.71796 avg_loss = 3.09870\n",
      "epoch no.5 train no.417340  loss = 1.97241 avg_loss = 3.06738\n",
      "epoch no.5 train no.417350  loss = 3.97851 avg_loss = 3.07309\n",
      "epoch no.5 train no.417360  loss = 2.23983 avg_loss = 3.03938\n",
      "epoch no.5 train no.417370  loss = 3.67243 avg_loss = 3.02063\n",
      "epoch no.5 train no.417380  loss = 3.39818 avg_loss = 3.04706\n",
      "epoch no.5 train no.417390  loss = 2.96170 avg_loss = 3.03806\n",
      "epoch no.5 train no.417400  loss = 2.84768 avg_loss = 3.07802\n",
      "epoch no.5 train no.417410  loss = 2.57606 avg_loss = 3.05191\n",
      "epoch no.5 train no.417420  loss = 3.15017 avg_loss = 3.05739\n",
      "epoch no.5 train no.417430  loss = 4.74958 avg_loss = 3.03865\n",
      "epoch no.5 train no.417440  loss = 3.88014 avg_loss = 2.99224\n",
      "epoch no.5 train no.417450  loss = 2.35778 avg_loss = 3.02462\n",
      "epoch no.5 train no.417460  loss = 2.47333 avg_loss = 3.03223\n",
      "epoch no.5 train no.417470  loss = 2.74328 avg_loss = 3.03137\n",
      "epoch no.5 train no.417480  loss = 2.05706 avg_loss = 3.00982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.417490  loss = 2.94569 avg_loss = 3.03909\n",
      "epoch no.5 train no.417500  loss = 3.54741 avg_loss = 3.01858\n",
      "epoch no.5 train no.417510  loss = 3.00941 avg_loss = 3.03558\n",
      "epoch no.5 train no.417520  loss = 2.86161 avg_loss = 3.05752\n",
      "epoch no.5 train no.417530  loss = 3.22988 avg_loss = 3.07402\n",
      "epoch no.5 train no.417540  loss = 3.05701 avg_loss = 3.03855\n",
      "epoch no.5 train no.417550  loss = 2.38774 avg_loss = 3.05090\n",
      "epoch no.5 train no.417560  loss = 3.69631 avg_loss = 3.03834\n",
      "epoch no.5 train no.417570  loss = 2.54095 avg_loss = 3.02502\n",
      "epoch no.5 train no.417580  loss = 3.19516 avg_loss = 3.05212\n",
      "epoch no.5 train no.417590  loss = 2.66550 avg_loss = 3.05579\n",
      "epoch no.5 train no.417600  loss = 2.29696 avg_loss = 3.05369\n",
      "epoch no.5 train no.417610  loss = 2.18483 avg_loss = 3.05125\n",
      "epoch no.5 train no.417620  loss = 4.70281 avg_loss = 3.08756\n",
      "epoch no.5 train no.417630  loss = 2.84040 avg_loss = 3.08417\n",
      "epoch no.5 train no.417640  loss = 3.81427 avg_loss = 3.09849\n",
      "epoch no.5 train no.417650  loss = 3.32820 avg_loss = 3.07493\n",
      "epoch no.5 train no.417660  loss = 3.21501 avg_loss = 3.04720\n",
      "epoch no.5 train no.417670  loss = 2.67578 avg_loss = 3.04655\n",
      "epoch no.5 train no.417680  loss = 3.23912 avg_loss = 3.05788\n",
      "epoch no.5 train no.417690  loss = 2.69359 avg_loss = 3.05248\n",
      "epoch no.5 train no.417700  loss = 3.99792 avg_loss = 3.06492\n",
      "epoch no.5 train no.417710  loss = 2.54167 avg_loss = 3.11301\n",
      "epoch no.5 train no.417720  loss = 2.04925 avg_loss = 3.04623\n",
      "epoch no.5 train no.417730  loss = 2.52725 avg_loss = 3.02695\n",
      "epoch no.5 train no.417740  loss = 3.27093 avg_loss = 3.01417\n",
      "epoch no.5 train no.417750  loss = 3.07537 avg_loss = 2.99661\n",
      "epoch no.5 train no.417760  loss = 3.48903 avg_loss = 3.01947\n",
      "epoch no.5 train no.417770  loss = 3.68633 avg_loss = 3.05227\n",
      "epoch no.5 train no.417780  loss = 3.44878 avg_loss = 3.10821\n",
      "epoch no.5 train no.417790  loss = 5.08693 avg_loss = 3.12215\n",
      "epoch no.5 train no.417800  loss = 1.26296 avg_loss = 3.11593\n",
      "epoch no.5 train no.417810  loss = 1.92928 avg_loss = 3.12195\n",
      "epoch no.5 train no.417820  loss = 3.75948 avg_loss = 3.10854\n",
      "epoch no.5 train no.417830  loss = 2.70567 avg_loss = 3.06576\n",
      "epoch no.5 train no.417840  loss = 6.45978 avg_loss = 3.09812\n",
      "epoch no.5 train no.417850  loss = 2.29123 avg_loss = 3.04887\n",
      "epoch no.5 train no.417860  loss = 2.40286 avg_loss = 3.07758\n",
      "epoch no.5 train no.417870  loss = 3.52909 avg_loss = 3.07558\n",
      "epoch no.5 train no.417880  loss = 3.72890 avg_loss = 3.10597\n",
      "epoch no.5 train no.417890  loss = 2.40795 avg_loss = 3.14454\n",
      "epoch no.5 train no.417900  loss = 3.74178 avg_loss = 3.15839\n",
      "epoch no.5 train no.417910  loss = 3.61344 avg_loss = 3.17694\n",
      "epoch no.5 train no.417920  loss = 2.36359 avg_loss = 3.14590\n",
      "epoch no.5 train no.417930  loss = 2.30181 avg_loss = 3.14428\n",
      "epoch no.5 train no.417940  loss = 2.00738 avg_loss = 3.11535\n",
      "epoch no.5 train no.417950  loss = 4.16295 avg_loss = 3.16237\n",
      "epoch no.5 train no.417960  loss = 4.77327 avg_loss = 3.18561\n",
      "epoch no.5 train no.417970  loss = 4.14289 avg_loss = 3.16420\n",
      "epoch no.5 train no.417980  loss = 2.85125 avg_loss = 3.16089\n",
      "epoch no.5 train no.417990  loss = 2.09122 avg_loss = 3.15538\n",
      "epoch no.5 train no.418000  loss = 2.57496 avg_loss = 3.14710\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '년대', '▁가요', '송', '</s>']\n",
      "추억의 90년대 팝송</s>\n",
      "epoch no.5 train no.418010  loss = 3.24593 avg_loss = 3.14311\n",
      "epoch no.5 train no.418020  loss = 2.53937 avg_loss = 3.15224\n",
      "epoch no.5 train no.418030  loss = 2.81951 avg_loss = 3.10649\n",
      "epoch no.5 train no.418040  loss = 2.35869 avg_loss = 3.09706\n",
      "epoch no.5 train no.418050  loss = 3.11872 avg_loss = 3.10311\n",
      "epoch no.5 train no.418060  loss = 2.37288 avg_loss = 3.09140\n",
      "epoch no.5 train no.418070  loss = 2.92760 avg_loss = 3.12284\n",
      "epoch no.5 train no.418080  loss = 3.03008 avg_loss = 3.10407\n",
      "epoch no.5 train no.418090  loss = 2.19160 avg_loss = 3.11379\n",
      "epoch no.5 train no.418100  loss = 2.47983 avg_loss = 3.11957\n",
      "epoch no.5 train no.418110  loss = 4.02608 avg_loss = 3.10162\n",
      "epoch no.5 train no.418120  loss = 2.80692 avg_loss = 3.10817\n",
      "epoch no.5 train no.418130  loss = 4.00045 avg_loss = 3.13452\n",
      "epoch no.5 train no.418140  loss = 3.44391 avg_loss = 3.15252\n",
      "epoch no.5 train no.418150  loss = 2.79005 avg_loss = 3.18145\n",
      "epoch no.5 train no.418160  loss = 2.14945 avg_loss = 3.15183\n",
      "epoch no.5 train no.418170  loss = 2.38179 avg_loss = 3.08586\n",
      "epoch no.5 train no.418180  loss = 5.51358 avg_loss = 3.07130\n",
      "epoch no.5 train no.418190  loss = 2.48831 avg_loss = 3.07530\n",
      "epoch no.5 train no.418200  loss = 4.94959 avg_loss = 3.12818\n",
      "epoch no.5 train no.418210  loss = 2.45507 avg_loss = 3.16080\n",
      "epoch no.5 train no.418220  loss = 3.39435 avg_loss = 3.18995\n",
      "epoch no.5 train no.418230  loss = 3.63139 avg_loss = 3.19285\n",
      "epoch no.5 train no.418240  loss = 4.29436 avg_loss = 3.18670\n",
      "epoch no.5 train no.418250  loss = 2.58763 avg_loss = 3.15469\n",
      "epoch no.5 train no.418260  loss = 2.62777 avg_loss = 3.13045\n",
      "epoch no.5 train no.418270  loss = 1.98797 avg_loss = 3.13734\n",
      "epoch no.5 train no.418280  loss = 2.55728 avg_loss = 3.14735\n",
      "epoch no.5 train no.418290  loss = 2.57787 avg_loss = 3.15075\n",
      "epoch no.5 train no.418300  loss = 4.07499 avg_loss = 3.17461\n",
      "epoch no.5 train no.418310  loss = 3.03922 avg_loss = 3.14686\n",
      "epoch no.5 train no.418320  loss = 4.20712 avg_loss = 3.14521\n",
      "epoch no.5 train no.418330  loss = 2.47825 avg_loss = 3.10518\n",
      "epoch no.5 train no.418340  loss = 4.21858 avg_loss = 3.13502\n",
      "epoch no.5 train no.418350  loss = 3.03959 avg_loss = 3.10143\n",
      "epoch no.5 train no.418360  loss = 3.14753 avg_loss = 3.10990\n",
      "epoch no.5 train no.418370  loss = 2.19078 avg_loss = 3.10677\n",
      "epoch no.5 train no.418380  loss = 2.70774 avg_loss = 3.11181\n",
      "epoch no.5 train no.418390  loss = 2.40860 avg_loss = 3.12807\n",
      "epoch no.5 train no.418400  loss = 2.48932 avg_loss = 3.11690\n",
      "epoch no.5 train no.418410  loss = 1.93419 avg_loss = 3.06373\n",
      "epoch no.5 train no.418420  loss = 2.51327 avg_loss = 3.04791\n",
      "epoch no.5 train no.418430  loss = 3.71726 avg_loss = 3.07549\n",
      "epoch no.5 train no.418440  loss = 2.73600 avg_loss = 3.09544\n",
      "epoch no.5 train no.418450  loss = 2.05639 avg_loss = 3.09030\n",
      "epoch no.5 train no.418460  loss = 2.15392 avg_loss = 3.09433\n",
      "epoch no.5 train no.418470  loss = 1.66089 avg_loss = 3.06935\n",
      "epoch no.5 train no.418480  loss = 3.26636 avg_loss = 3.04300\n",
      "epoch no.5 train no.418490  loss = 3.38318 avg_loss = 3.02216\n",
      "epoch no.5 train no.418500  loss = 3.14657 avg_loss = 3.04669\n",
      "epoch no.5 train no.418510  loss = 3.13415 avg_loss = 3.05943\n",
      "epoch no.5 train no.418520  loss = 3.05501 avg_loss = 3.05009\n",
      "epoch no.5 train no.418530  loss = 5.13518 avg_loss = 3.08127\n",
      "epoch no.5 train no.418540  loss = 3.69559 avg_loss = 3.09643\n",
      "epoch no.5 train no.418550  loss = 4.12090 avg_loss = 3.10173\n",
      "epoch no.5 train no.418560  loss = 2.62310 avg_loss = 3.10830\n",
      "epoch no.5 train no.418570  loss = 1.36745 avg_loss = 3.08585\n",
      "epoch no.5 train no.418580  loss = 2.22265 avg_loss = 3.07444\n",
      "epoch no.5 train no.418590  loss = 3.06937 avg_loss = 3.05553\n",
      "epoch no.5 train no.418600  loss = 2.70109 avg_loss = 3.03802\n",
      "epoch no.5 train no.418610  loss = 2.50209 avg_loss = 3.06258\n",
      "epoch no.5 train no.418620  loss = 5.78241 avg_loss = 3.10730\n",
      "epoch no.5 train no.418630  loss = 2.65298 avg_loss = 3.10181\n",
      "epoch no.5 train no.418640  loss = 2.79135 avg_loss = 3.16800\n",
      "epoch no.5 train no.418650  loss = 2.49950 avg_loss = 3.16096\n",
      "epoch no.5 train no.418660  loss = 3.32851 avg_loss = 3.16158\n",
      "epoch no.5 train no.418670  loss = 3.48431 avg_loss = 3.16873\n",
      "epoch no.5 train no.418680  loss = 2.79936 avg_loss = 3.13819\n",
      "epoch no.5 train no.418690  loss = 2.83054 avg_loss = 3.13733\n",
      "epoch no.5 train no.418700  loss = 2.21240 avg_loss = 3.09890\n",
      "epoch no.5 train no.418710  loss = 3.29774 avg_loss = 3.07956\n",
      "epoch no.5 train no.418720  loss = 1.84964 avg_loss = 3.10241\n",
      "epoch no.5 train no.418730  loss = 3.27828 avg_loss = 3.11284\n",
      "epoch no.5 train no.418740  loss = 2.91411 avg_loss = 3.10243\n",
      "epoch no.5 train no.418750  loss = 3.32599 avg_loss = 3.08770\n",
      "epoch no.5 train no.418760  loss = 2.76621 avg_loss = 3.06270\n",
      "epoch no.5 train no.418770  loss = 3.05399 avg_loss = 3.11120\n",
      "epoch no.5 train no.418780  loss = 2.69779 avg_loss = 3.08637\n",
      "epoch no.5 train no.418790  loss = 2.82820 avg_loss = 3.07261\n",
      "epoch no.5 train no.418800  loss = 2.65150 avg_loss = 3.07242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.418810  loss = 3.62900 avg_loss = 3.11495\n",
      "epoch no.5 train no.418820  loss = 2.43891 avg_loss = 3.10382\n",
      "epoch no.5 train no.418830  loss = 3.78109 avg_loss = 3.13059\n",
      "epoch no.5 train no.418840  loss = 3.51264 avg_loss = 3.13187\n",
      "epoch no.5 train no.418850  loss = 3.28441 avg_loss = 3.13008\n",
      "epoch no.5 train no.418860  loss = 2.98194 avg_loss = 3.12002\n",
      "epoch no.5 train no.418870  loss = 2.98049 avg_loss = 3.15873\n",
      "epoch no.5 train no.418880  loss = 4.26744 avg_loss = 3.13386\n",
      "epoch no.5 train no.418890  loss = 2.72477 avg_loss = 3.10720\n",
      "epoch no.5 train no.418900  loss = 3.17843 avg_loss = 3.09732\n",
      "epoch no.5 train no.418910  loss = 2.67312 avg_loss = 3.13791\n",
      "epoch no.5 train no.418920  loss = 2.55851 avg_loss = 3.08044\n",
      "epoch no.5 train no.418930  loss = 3.19216 avg_loss = 3.08458\n",
      "epoch no.5 train no.418940  loss = 3.08577 avg_loss = 3.05924\n",
      "epoch no.5 train no.418950  loss = 3.55516 avg_loss = 3.06344\n",
      "epoch no.5 train no.418960  loss = 1.12746 avg_loss = 3.01509\n",
      "epoch no.5 train no.418970  loss = 3.45059 avg_loss = 3.04868\n",
      "epoch no.5 train no.418980  loss = 2.55714 avg_loss = 3.07213\n",
      "epoch no.5 train no.418990  loss = 3.19643 avg_loss = 3.11336\n",
      "epoch no.5 train no.419000  loss = 2.87030 avg_loss = 3.10895\n",
      "5\n",
      "to_tokens: ['▁비', '▁올드', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.419010  loss = 3.49335 avg_loss = 3.09181\n",
      "epoch no.5 train no.419020  loss = 2.20423 avg_loss = 3.06884\n",
      "epoch no.5 train no.419030  loss = 2.69368 avg_loss = 3.06891\n",
      "epoch no.5 train no.419040  loss = 3.22121 avg_loss = 3.08925\n",
      "epoch no.5 train no.419050  loss = 3.93799 avg_loss = 3.10046\n",
      "epoch no.5 train no.419060  loss = 2.59690 avg_loss = 3.11965\n",
      "epoch no.5 train no.419070  loss = 3.77245 avg_loss = 3.11357\n",
      "epoch no.5 train no.419080  loss = 2.73915 avg_loss = 3.12317\n",
      "epoch no.5 train no.419090  loss = 2.46725 avg_loss = 3.10062\n",
      "epoch no.5 train no.419100  loss = 3.27428 avg_loss = 3.06264\n",
      "epoch no.5 train no.419110  loss = 2.91516 avg_loss = 3.08177\n",
      "epoch no.5 train no.419120  loss = 2.20067 avg_loss = 3.07054\n",
      "epoch no.5 train no.419130  loss = 2.49164 avg_loss = 3.05701\n",
      "epoch no.5 train no.419140  loss = 4.19357 avg_loss = 3.04515\n",
      "epoch no.5 train no.419150  loss = 3.62890 avg_loss = 3.03928\n",
      "epoch no.5 train no.419160  loss = 2.45105 avg_loss = 3.07560\n",
      "epoch no.5 train no.419170  loss = 2.57961 avg_loss = 3.02809\n",
      "epoch no.5 train no.419180  loss = 3.44727 avg_loss = 3.06964\n",
      "epoch no.5 train no.419190  loss = 4.38045 avg_loss = 3.05281\n",
      "epoch no.5 train no.419200  loss = 2.75658 avg_loss = 3.03868\n",
      "epoch no.5 train no.419210  loss = 3.89674 avg_loss = 3.02532\n",
      "epoch no.5 train no.419220  loss = 2.19783 avg_loss = 3.00842\n",
      "epoch no.5 train no.419230  loss = 3.44301 avg_loss = 3.05341\n",
      "epoch no.5 train no.419240  loss = 3.75055 avg_loss = 3.07715\n",
      "epoch no.5 train no.419250  loss = 4.69603 avg_loss = 3.09153\n",
      "epoch no.5 train no.419260  loss = 2.41072 avg_loss = 3.03672\n",
      "epoch no.5 train no.419270  loss = 4.02171 avg_loss = 3.04615\n",
      "epoch no.5 train no.419280  loss = 1.86735 avg_loss = 3.02212\n",
      "epoch no.5 train no.419290  loss = 3.03854 avg_loss = 3.02344\n",
      "epoch no.5 train no.419300  loss = 3.11383 avg_loss = 3.02955\n",
      "epoch no.5 train no.419310  loss = 3.02770 avg_loss = 3.03857\n",
      "epoch no.5 train no.419320  loss = 2.25198 avg_loss = 3.01463\n",
      "epoch no.5 train no.419330  loss = 3.07411 avg_loss = 3.02130\n",
      "epoch no.5 train no.419340  loss = 4.54924 avg_loss = 3.04336\n",
      "epoch no.5 train no.419350  loss = 3.25502 avg_loss = 3.04801\n",
      "epoch no.5 train no.419360  loss = 3.42873 avg_loss = 3.04813\n",
      "epoch no.5 train no.419370  loss = 2.42145 avg_loss = 3.03799\n",
      "epoch no.5 train no.419380  loss = 2.52534 avg_loss = 3.05295\n",
      "epoch no.5 train no.419390  loss = 3.12550 avg_loss = 3.04428\n",
      "epoch no.5 train no.419400  loss = 2.57096 avg_loss = 3.03433\n",
      "epoch no.5 train no.419410  loss = 3.47204 avg_loss = 3.04524\n",
      "epoch no.5 train no.419420  loss = 3.26943 avg_loss = 3.05038\n",
      "epoch no.5 train no.419430  loss = 2.96444 avg_loss = 3.04667\n",
      "epoch no.5 train no.419440  loss = 3.55929 avg_loss = 3.06245\n",
      "epoch no.5 train no.419450  loss = 4.26307 avg_loss = 3.09665\n",
      "epoch no.5 train no.419460  loss = 3.73078 avg_loss = 3.14500\n",
      "epoch no.5 train no.419470  loss = 2.53424 avg_loss = 3.16952\n",
      "epoch no.5 train no.419480  loss = 2.55368 avg_loss = 3.16606\n",
      "epoch no.5 train no.419490  loss = 3.94203 avg_loss = 3.15634\n",
      "epoch no.5 train no.419500  loss = 2.45362 avg_loss = 3.14553\n",
      "epoch no.5 train no.419510  loss = 2.12444 avg_loss = 3.14446\n",
      "epoch no.5 train no.419520  loss = 2.64520 avg_loss = 3.14511\n",
      "epoch no.5 train no.419530  loss = 3.02423 avg_loss = 3.16395\n",
      "epoch no.5 train no.419540  loss = 4.15784 avg_loss = 3.17280\n",
      "epoch no.5 train no.419550  loss = 2.35212 avg_loss = 3.20089\n",
      "epoch no.5 train no.419560  loss = 2.49453 avg_loss = 3.19046\n",
      "epoch no.5 train no.419570  loss = 2.83240 avg_loss = 3.17617\n",
      "epoch no.5 train no.419580  loss = 4.04353 avg_loss = 3.15008\n",
      "epoch no.5 train no.419590  loss = 3.11449 avg_loss = 3.15075\n",
      "epoch no.5 train no.419600  loss = 3.25567 avg_loss = 3.20258\n",
      "epoch no.5 train no.419610  loss = 2.88642 avg_loss = 3.17221\n",
      "epoch no.5 train no.419620  loss = 3.02407 avg_loss = 3.16918\n",
      "epoch no.5 train no.419630  loss = 1.94691 avg_loss = 3.13310\n",
      "epoch no.5 train no.419640  loss = 2.27405 avg_loss = 3.13464\n",
      "epoch no.5 train no.419650  loss = 4.01097 avg_loss = 3.12737\n",
      "epoch no.5 train no.419660  loss = 1.27719 avg_loss = 3.11560\n",
      "epoch no.5 train no.419670  loss = 3.68854 avg_loss = 3.12308\n",
      "epoch no.5 train no.419680  loss = 3.12984 avg_loss = 3.18040\n",
      "epoch no.5 train no.419690  loss = 2.84521 avg_loss = 3.13582\n",
      "epoch no.5 train no.419700  loss = 3.66611 avg_loss = 3.11944\n",
      "epoch no.5 train no.419710  loss = 2.66908 avg_loss = 3.11286\n",
      "epoch no.5 train no.419720  loss = 3.03110 avg_loss = 3.09679\n",
      "epoch no.5 train no.419730  loss = 2.69844 avg_loss = 3.12915\n",
      "epoch no.5 train no.419740  loss = 3.64757 avg_loss = 3.12289\n",
      "epoch no.5 train no.419750  loss = 2.15329 avg_loss = 3.09690\n",
      "epoch no.5 train no.419760  loss = 4.33258 avg_loss = 3.07837\n",
      "epoch no.5 train no.419770  loss = 1.82542 avg_loss = 3.01316\n",
      "epoch no.5 train no.419780  loss = 2.10749 avg_loss = 3.03641\n",
      "epoch no.5 train no.419790  loss = 2.53852 avg_loss = 3.02351\n",
      "epoch no.5 train no.419800  loss = 2.15300 avg_loss = 3.00151\n",
      "epoch no.5 train no.419810  loss = 3.19451 avg_loss = 2.99926\n",
      "epoch no.5 train no.419820  loss = 3.03108 avg_loss = 3.01566\n",
      "epoch no.5 train no.419830  loss = 3.48214 avg_loss = 3.06975\n",
      "epoch no.5 train no.419840  loss = 2.78428 avg_loss = 3.04489\n",
      "epoch no.5 train no.419850  loss = 2.74108 avg_loss = 3.02694\n",
      "epoch no.5 train no.419860  loss = 1.87721 avg_loss = 3.02278\n",
      "epoch no.5 train no.419870  loss = 2.37694 avg_loss = 3.04863\n",
      "epoch no.5 train no.419880  loss = 3.05136 avg_loss = 3.08146\n",
      "epoch no.5 train no.419890  loss = 2.29571 avg_loss = 3.09112\n",
      "epoch no.5 train no.419900  loss = 2.90325 avg_loss = 3.10403\n",
      "epoch no.5 train no.419910  loss = 2.06053 avg_loss = 3.08000\n",
      "epoch no.5 train no.419920  loss = 2.41631 avg_loss = 3.05587\n",
      "epoch no.5 train no.419930  loss = 2.25601 avg_loss = 3.01938\n",
      "epoch no.5 train no.419940  loss = 4.13458 avg_loss = 3.02863\n",
      "epoch no.5 train no.419950  loss = 2.00581 avg_loss = 3.02342\n",
      "epoch no.5 train no.419960  loss = 4.65562 avg_loss = 3.04032\n",
      "epoch no.5 train no.419970  loss = 3.28274 avg_loss = 3.01149\n",
      "epoch no.5 train no.419980  loss = 2.63887 avg_loss = 3.00893\n",
      "epoch no.5 train no.419990  loss = 3.77976 avg_loss = 3.01912\n",
      "epoch no.5 train no.420000  loss = 2.71579 avg_loss = 3.01575\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.5 train no.420010  loss = 3.21858 avg_loss = 3.05659\n",
      "epoch no.5 train no.420020  loss = 3.21174 avg_loss = 3.03930\n",
      "epoch no.5 train no.420030  loss = 2.12288 avg_loss = 2.98975\n",
      "epoch no.5 train no.420040  loss = 5.06709 avg_loss = 3.01709\n",
      "epoch no.5 train no.420050  loss = 3.39583 avg_loss = 3.02400\n",
      "epoch no.5 train no.420060  loss = 3.26695 avg_loss = 2.99755\n",
      "epoch no.5 train no.420070  loss = 3.64620 avg_loss = 3.05285\n",
      "epoch no.5 train no.420080  loss = 1.63485 avg_loss = 3.03733\n",
      "epoch no.5 train no.420090  loss = 2.07452 avg_loss = 3.09112\n",
      "epoch no.5 train no.420100  loss = 3.44487 avg_loss = 3.10340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.420110  loss = 2.98560 avg_loss = 3.11589\n",
      "epoch no.5 train no.420120  loss = 2.57948 avg_loss = 3.11581\n",
      "epoch no.5 train no.420130  loss = 3.45017 avg_loss = 3.07847\n",
      "epoch no.5 train no.420140  loss = 2.16240 avg_loss = 3.07364\n",
      "epoch no.5 train no.420150  loss = 3.19312 avg_loss = 3.07333\n",
      "epoch no.5 train no.420160  loss = 3.08967 avg_loss = 3.12223\n",
      "epoch no.5 train no.420170  loss = 3.46015 avg_loss = 3.11661\n",
      "epoch no.5 train no.420180  loss = 2.23904 avg_loss = 3.07867\n",
      "epoch no.5 train no.420190  loss = 2.96795 avg_loss = 3.15565\n",
      "epoch no.5 train no.420200  loss = 3.25356 avg_loss = 3.14153\n",
      "epoch no.5 train no.420210  loss = 2.88122 avg_loss = 3.13936\n",
      "epoch no.5 train no.420220  loss = 1.80081 avg_loss = 3.08659\n",
      "epoch no.5 train no.420230  loss = 3.56283 avg_loss = 3.07930\n",
      "epoch no.5 train no.420240  loss = 4.36060 avg_loss = 3.08149\n",
      "epoch no.5 train no.420250  loss = 3.85504 avg_loss = 3.08773\n",
      "epoch no.5 train no.420260  loss = 3.14731 avg_loss = 3.09744\n",
      "epoch no.5 train no.420270  loss = 2.05996 avg_loss = 3.06200\n",
      "epoch no.5 train no.420280  loss = 2.45833 avg_loss = 3.07784\n",
      "epoch no.5 train no.420290  loss = 4.41798 avg_loss = 3.09985\n",
      "epoch no.5 train no.420300  loss = 3.24423 avg_loss = 3.05210\n",
      "epoch no.5 train no.420310  loss = 3.32606 avg_loss = 3.03051\n",
      "epoch no.5 train no.420320  loss = 1.57567 avg_loss = 2.99669\n",
      "epoch no.5 train no.420330  loss = 3.01608 avg_loss = 3.03017\n",
      "epoch no.5 train no.420340  loss = 2.92546 avg_loss = 3.03762\n",
      "epoch no.5 train no.420350  loss = 3.13655 avg_loss = 3.03972\n",
      "epoch no.5 train no.420360  loss = 2.86542 avg_loss = 3.10759\n",
      "epoch no.5 train no.420370  loss = 2.97714 avg_loss = 3.12439\n",
      "epoch no.5 train no.420380  loss = 2.45466 avg_loss = 3.06041\n",
      "epoch no.5 train no.420390  loss = 2.84098 avg_loss = 3.08241\n",
      "epoch no.5 train no.420400  loss = 3.11508 avg_loss = 3.07579\n",
      "epoch no.5 train no.420410  loss = 4.21909 avg_loss = 3.08635\n",
      "epoch no.5 train no.420420  loss = 1.90718 avg_loss = 3.06155\n",
      "epoch no.5 train no.420430  loss = 2.20634 avg_loss = 3.06315\n",
      "epoch no.5 train no.420440  loss = 3.45591 avg_loss = 3.06316\n",
      "epoch no.5 train no.420450  loss = 2.81805 avg_loss = 3.05048\n",
      "epoch no.5 train no.420460  loss = 3.49239 avg_loss = 3.05491\n",
      "epoch no.5 train no.420470  loss = 3.04933 avg_loss = 3.06994\n",
      "epoch no.5 train no.420480  loss = 3.44007 avg_loss = 3.05887\n",
      "epoch no.5 train no.420490  loss = 3.24254 avg_loss = 3.02775\n",
      "epoch no.5 train no.420500  loss = 3.22253 avg_loss = 3.03731\n",
      "epoch no.5 train no.420510  loss = 3.48182 avg_loss = 3.07061\n",
      "epoch no.5 train no.420520  loss = 3.35258 avg_loss = 3.06868\n",
      "epoch no.5 train no.420530  loss = 2.90543 avg_loss = 3.08110\n",
      "epoch no.5 train no.420540  loss = 3.77156 avg_loss = 3.07318\n",
      "epoch no.5 train no.420550  loss = 2.34919 avg_loss = 3.03949\n",
      "epoch no.5 train no.420560  loss = 3.31326 avg_loss = 3.02568\n",
      "epoch no.5 train no.420570  loss = 2.74629 avg_loss = 3.03975\n",
      "epoch no.5 train no.420580  loss = 4.36847 avg_loss = 3.03545\n",
      "epoch no.5 train no.420590  loss = 3.06455 avg_loss = 3.00322\n",
      "epoch no.5 train no.420600  loss = 2.90124 avg_loss = 3.02288\n",
      "epoch no.5 train no.420610  loss = 3.62982 avg_loss = 3.04659\n",
      "epoch no.5 train no.420620  loss = 3.20685 avg_loss = 3.01482\n",
      "epoch no.5 train no.420630  loss = 2.61107 avg_loss = 2.99851\n",
      "epoch no.5 train no.420640  loss = 3.55355 avg_loss = 3.04187\n",
      "epoch no.5 train no.420650  loss = 2.70117 avg_loss = 3.05853\n",
      "epoch no.5 train no.420660  loss = 3.13236 avg_loss = 3.08228\n",
      "epoch no.5 train no.420670  loss = 2.75390 avg_loss = 3.12438\n",
      "epoch no.5 train no.420680  loss = 2.26224 avg_loss = 3.12762\n",
      "epoch no.5 train no.420690  loss = 3.05167 avg_loss = 3.13693\n",
      "epoch no.5 train no.420700  loss = 4.28371 avg_loss = 3.13631\n",
      "epoch no.5 train no.420710  loss = 3.51428 avg_loss = 3.12846\n",
      "epoch no.5 train no.420720  loss = 2.89364 avg_loss = 3.10886\n",
      "epoch no.5 train no.420730  loss = 2.03534 avg_loss = 3.11524\n",
      "epoch no.5 train no.420740  loss = 3.35972 avg_loss = 3.10500\n",
      "epoch no.5 train no.420750  loss = 2.65677 avg_loss = 3.13138\n",
      "epoch no.5 train no.420760  loss = 3.64993 avg_loss = 3.13275\n",
      "epoch no.5 train no.420770  loss = 3.59301 avg_loss = 3.14824\n",
      "epoch no.5 train no.420780  loss = 3.85093 avg_loss = 3.11884\n",
      "epoch no.5 train no.420790  loss = 2.55184 avg_loss = 3.10224\n",
      "epoch no.5 train no.420800  loss = 2.99154 avg_loss = 3.17392\n",
      "epoch no.5 train no.420810  loss = 5.24401 avg_loss = 3.18257\n",
      "epoch no.5 train no.420820  loss = 2.13548 avg_loss = 3.18266\n",
      "epoch no.5 train no.420830  loss = 3.97467 avg_loss = 3.18466\n",
      "epoch no.5 train no.420840  loss = 2.36833 avg_loss = 3.15209\n",
      "epoch no.5 train no.420850  loss = 3.09712 avg_loss = 3.12587\n",
      "epoch no.5 train no.420860  loss = 4.87883 avg_loss = 3.13710\n",
      "epoch no.5 train no.420870  loss = 3.19555 avg_loss = 3.13909\n",
      "epoch no.5 train no.420880  loss = 4.28945 avg_loss = 3.16965\n",
      "epoch no.5 train no.420890  loss = 3.52767 avg_loss = 3.18573\n",
      "epoch no.5 train no.420900  loss = 2.33837 avg_loss = 3.13446\n",
      "epoch no.5 train no.420910  loss = 3.24356 avg_loss = 3.12875\n",
      "epoch no.5 train no.420920  loss = 2.98256 avg_loss = 3.14656\n",
      "epoch no.5 train no.420930  loss = 3.61369 avg_loss = 3.15594\n",
      "epoch no.5 train no.420940  loss = 2.46194 avg_loss = 3.11548\n",
      "epoch no.5 train no.420950  loss = 2.91473 avg_loss = 3.10580\n",
      "epoch no.5 train no.420960  loss = 2.50109 avg_loss = 3.11565\n",
      "epoch no.5 train no.420970  loss = 3.35217 avg_loss = 3.09962\n",
      "epoch no.5 train no.420980  loss = 2.98550 avg_loss = 3.07167\n",
      "epoch no.5 train no.420990  loss = 2.98384 avg_loss = 3.07512\n",
      "epoch no.5 train no.421000  loss = 2.91084 avg_loss = 3.03850\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁인기', '▁가요', '</s>']\n",
      "추억의 90년대 인기 댄스</s>\n",
      "epoch no.5 train no.421010  loss = 3.80260 avg_loss = 3.03101\n",
      "epoch no.5 train no.421020  loss = 3.02912 avg_loss = 3.03389\n",
      "epoch no.5 train no.421030  loss = 2.25584 avg_loss = 3.08851\n",
      "epoch no.5 train no.421040  loss = 4.78936 avg_loss = 3.09842\n",
      "epoch no.5 train no.421050  loss = 3.37753 avg_loss = 3.11456\n",
      "epoch no.5 train no.421060  loss = 3.06670 avg_loss = 3.12356\n",
      "epoch no.5 train no.421070  loss = 3.95109 avg_loss = 3.12589\n",
      "epoch no.5 train no.421080  loss = 3.58127 avg_loss = 3.11745\n",
      "epoch no.5 train no.421090  loss = 3.11583 avg_loss = 3.14848\n",
      "epoch no.5 train no.421100  loss = 2.95338 avg_loss = 3.14853\n",
      "epoch no.5 train no.421110  loss = 2.91529 avg_loss = 3.10271\n",
      "epoch no.5 train no.421120  loss = 2.90649 avg_loss = 3.07743\n",
      "epoch no.5 train no.421130  loss = 2.36163 avg_loss = 3.03149\n",
      "epoch no.5 train no.421140  loss = 2.73903 avg_loss = 3.07756\n",
      "epoch no.5 train no.421150  loss = 1.95342 avg_loss = 3.12516\n",
      "epoch no.5 train no.421160  loss = 2.75065 avg_loss = 3.11386\n",
      "epoch no.5 train no.421170  loss = 2.54564 avg_loss = 3.06335\n",
      "epoch no.5 train no.421180  loss = 3.50059 avg_loss = 3.07904\n",
      "epoch no.5 train no.421190  loss = 3.88870 avg_loss = 3.06409\n",
      "epoch no.5 train no.421200  loss = 3.22644 avg_loss = 3.06600\n",
      "epoch no.5 train no.421210  loss = 4.15964 avg_loss = 3.12454\n",
      "epoch no.5 train no.421220  loss = 2.97270 avg_loss = 3.12745\n",
      "epoch no.5 train no.421230  loss = 3.06233 avg_loss = 3.12974\n",
      "epoch no.5 train no.421240  loss = 2.54337 avg_loss = 3.10458\n",
      "epoch no.5 train no.421250  loss = 2.73548 avg_loss = 3.10559\n",
      "epoch no.5 train no.421260  loss = 2.98760 avg_loss = 3.11614\n",
      "epoch no.5 train no.421270  loss = 4.31484 avg_loss = 3.15978\n",
      "epoch no.5 train no.421280  loss = 2.08612 avg_loss = 3.18729\n",
      "epoch no.5 train no.421290  loss = 2.27334 avg_loss = 3.14633\n",
      "epoch no.5 train no.421300  loss = 1.51656 avg_loss = 3.12365\n",
      "epoch no.5 train no.421310  loss = 4.34911 avg_loss = 3.11398\n",
      "epoch no.5 train no.421320  loss = 2.87528 avg_loss = 3.13364\n",
      "epoch no.5 train no.421330  loss = 3.38454 avg_loss = 3.12337\n",
      "epoch no.5 train no.421340  loss = 4.41612 avg_loss = 3.14053\n",
      "epoch no.5 train no.421350  loss = 2.89382 avg_loss = 3.14433\n",
      "epoch no.5 train no.421360  loss = 1.80414 avg_loss = 3.11437\n",
      "epoch no.5 train no.421370  loss = 2.52168 avg_loss = 3.10108\n",
      "epoch no.5 train no.421380  loss = 2.96993 avg_loss = 3.10621\n",
      "epoch no.5 train no.421390  loss = 3.60518 avg_loss = 3.12008\n",
      "epoch no.5 train no.421400  loss = 2.61053 avg_loss = 3.08237\n",
      "epoch no.5 train no.421410  loss = 2.45614 avg_loss = 3.09933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.421420  loss = 2.61197 avg_loss = 3.09326\n",
      "epoch no.5 train no.421430  loss = 3.08114 avg_loss = 3.06876\n",
      "epoch no.5 train no.421440  loss = 2.44121 avg_loss = 3.07197\n",
      "epoch no.5 train no.421450  loss = 4.23126 avg_loss = 3.05041\n",
      "epoch no.5 train no.421460  loss = 1.93601 avg_loss = 3.04671\n",
      "epoch no.5 train no.421470  loss = 2.70845 avg_loss = 3.05187\n",
      "epoch no.5 train no.421480  loss = 3.25448 avg_loss = 3.07725\n",
      "epoch no.5 train no.421490  loss = 2.28030 avg_loss = 3.10964\n",
      "epoch no.5 train no.421500  loss = 1.47802 avg_loss = 3.05856\n",
      "epoch no.5 train no.421510  loss = 2.87722 avg_loss = 3.04181\n",
      "epoch no.5 train no.421520  loss = 2.46068 avg_loss = 3.02268\n",
      "epoch no.5 train no.421530  loss = 3.77552 avg_loss = 3.06587\n",
      "epoch no.5 train no.421540  loss = 2.39352 avg_loss = 3.07298\n",
      "epoch no.5 train no.421550  loss = 2.28681 avg_loss = 3.04127\n",
      "epoch no.5 train no.421560  loss = 3.63384 avg_loss = 3.02046\n",
      "epoch no.5 train no.421570  loss = 4.15161 avg_loss = 3.04471\n",
      "epoch no.5 train no.421580  loss = 4.11926 avg_loss = 3.11541\n",
      "epoch no.5 train no.421590  loss = 2.55517 avg_loss = 3.10566\n",
      "epoch no.5 train no.421600  loss = 4.15163 avg_loss = 3.09885\n",
      "epoch no.5 train no.421610  loss = 3.53841 avg_loss = 3.10351\n",
      "epoch no.5 train no.421620  loss = 3.05232 avg_loss = 3.11439\n",
      "epoch no.5 train no.421630  loss = 4.66573 avg_loss = 3.15361\n",
      "epoch no.5 train no.421640  loss = 4.10395 avg_loss = 3.13246\n",
      "epoch no.5 train no.421650  loss = 1.49474 avg_loss = 3.11112\n",
      "epoch no.5 train no.421660  loss = 2.81471 avg_loss = 3.07325\n",
      "epoch no.5 train no.421670  loss = 3.14987 avg_loss = 3.12405\n",
      "epoch no.5 train no.421680  loss = 3.55177 avg_loss = 3.12277\n",
      "epoch no.5 train no.421690  loss = 5.01944 avg_loss = 3.16162\n",
      "epoch no.5 train no.421700  loss = 2.81739 avg_loss = 3.12854\n",
      "epoch no.5 train no.421710  loss = 2.02935 avg_loss = 3.12319\n",
      "epoch no.5 train no.421720  loss = 4.32223 avg_loss = 3.13184\n",
      "epoch no.5 train no.421730  loss = 2.57276 avg_loss = 3.10755\n",
      "epoch no.5 train no.421740  loss = 5.89443 avg_loss = 3.12258\n",
      "epoch no.5 train no.421750  loss = 3.35570 avg_loss = 3.14420\n",
      "epoch no.5 train no.421760  loss = 2.50604 avg_loss = 3.09525\n",
      "epoch no.5 train no.421770  loss = 2.29964 avg_loss = 3.08599\n",
      "epoch no.5 train no.421780  loss = 4.54954 avg_loss = 3.12412\n",
      "epoch no.5 train no.421790  loss = 2.80651 avg_loss = 3.12699\n",
      "epoch no.5 train no.421800  loss = 3.33050 avg_loss = 3.10095\n",
      "epoch no.5 train no.421810  loss = 2.53502 avg_loss = 3.13467\n",
      "epoch no.5 train no.421820  loss = 2.67321 avg_loss = 3.12155\n",
      "epoch no.5 train no.421830  loss = 3.37612 avg_loss = 3.16057\n",
      "epoch no.5 train no.421840  loss = 3.36526 avg_loss = 3.16823\n",
      "epoch no.5 train no.421850  loss = 3.55668 avg_loss = 3.16745\n",
      "epoch no.5 train no.421860  loss = 2.65079 avg_loss = 3.17364\n",
      "epoch no.5 train no.421870  loss = 2.49290 avg_loss = 3.14800\n",
      "epoch no.5 train no.421880  loss = 2.22223 avg_loss = 3.12852\n",
      "epoch no.5 train no.421890  loss = 3.22582 avg_loss = 3.09772\n",
      "epoch no.5 train no.421900  loss = 4.56066 avg_loss = 3.14495\n",
      "epoch no.5 train no.421910  loss = 2.18009 avg_loss = 3.16822\n",
      "epoch no.5 train no.421920  loss = 2.73732 avg_loss = 3.17752\n",
      "epoch no.5 train no.421930  loss = 3.41254 avg_loss = 3.17922\n",
      "epoch no.5 train no.421940  loss = 2.36341 avg_loss = 3.17161\n",
      "epoch no.5 train no.421950  loss = 3.40794 avg_loss = 3.19758\n",
      "epoch no.5 train no.421960  loss = 3.52707 avg_loss = 3.22726\n",
      "epoch no.5 train no.421970  loss = 3.30011 avg_loss = 3.24056\n",
      "epoch no.5 train no.421980  loss = 3.29242 avg_loss = 3.21404\n",
      "epoch no.5 train no.421990  loss = 2.58643 avg_loss = 3.22847\n",
      "epoch no.5 train no.422000  loss = 3.13989 avg_loss = 3.23264\n",
      "3\n",
      "to_tokens: ['▁가을', '▁90', '▁o', 'st', '▁명']\n",
      "추억의 드라마 ost</s>\n",
      "epoch no.5 train no.422010  loss = 3.13769 avg_loss = 3.24654\n",
      "epoch no.5 train no.422020  loss = 2.26898 avg_loss = 3.25669\n",
      "epoch no.5 train no.422030  loss = 2.76444 avg_loss = 3.22094\n",
      "epoch no.5 train no.422040  loss = 5.50518 avg_loss = 3.19652\n",
      "epoch no.5 train no.422050  loss = 3.82991 avg_loss = 3.20772\n",
      "epoch no.5 train no.422060  loss = 2.37649 avg_loss = 3.16712\n",
      "epoch no.5 train no.422070  loss = 2.74001 avg_loss = 3.14047\n",
      "epoch no.5 train no.422080  loss = 2.75333 avg_loss = 3.09118\n",
      "epoch no.5 train no.422090  loss = 2.80969 avg_loss = 3.04722\n",
      "epoch no.5 train no.422100  loss = 1.63729 avg_loss = 3.04408\n",
      "epoch no.5 train no.422110  loss = 2.64593 avg_loss = 3.00383\n",
      "epoch no.5 train no.422120  loss = 2.74833 avg_loss = 3.00407\n",
      "epoch no.5 train no.422130  loss = 2.39005 avg_loss = 3.02245\n",
      "epoch no.5 train no.422140  loss = 3.63515 avg_loss = 3.00925\n",
      "epoch no.5 train no.422150  loss = 2.66857 avg_loss = 3.03517\n",
      "epoch no.5 train no.422160  loss = 4.22101 avg_loss = 3.09087\n",
      "epoch no.5 train no.422170  loss = 2.89771 avg_loss = 3.11531\n",
      "epoch no.5 train no.422180  loss = 1.82749 avg_loss = 3.10670\n",
      "epoch no.5 train no.422190  loss = 3.54755 avg_loss = 3.12489\n",
      "epoch no.5 train no.422200  loss = 1.66148 avg_loss = 3.12900\n",
      "epoch no.5 train no.422210  loss = 2.58615 avg_loss = 3.10373\n",
      "epoch no.5 train no.422220  loss = 2.06636 avg_loss = 3.08536\n",
      "epoch no.5 train no.422230  loss = 3.22487 avg_loss = 3.09448\n",
      "epoch no.5 train no.422240  loss = 3.10092 avg_loss = 3.08668\n",
      "epoch no.5 train no.422250  loss = 2.69895 avg_loss = 3.03998\n",
      "epoch no.5 train no.422260  loss = 3.92902 avg_loss = 3.02694\n",
      "epoch no.5 train no.422270  loss = 4.19612 avg_loss = 3.08234\n",
      "epoch no.5 train no.422280  loss = 2.73186 avg_loss = 3.03877\n",
      "epoch no.5 train no.422290  loss = 3.66083 avg_loss = 3.01681\n",
      "epoch no.5 train no.422300  loss = 5.14141 avg_loss = 3.06877\n",
      "epoch no.5 train no.422310  loss = 3.80890 avg_loss = 3.10412\n",
      "epoch no.5 train no.422320  loss = 2.22083 avg_loss = 3.09916\n",
      "epoch no.5 train no.422330  loss = 2.59891 avg_loss = 3.12433\n",
      "epoch no.5 train no.422340  loss = 2.27183 avg_loss = 3.12819\n",
      "epoch no.5 train no.422350  loss = 3.83626 avg_loss = 3.09986\n",
      "epoch no.5 train no.422360  loss = 2.77446 avg_loss = 3.06750\n",
      "epoch no.5 train no.422370  loss = 3.30978 avg_loss = 3.09713\n",
      "epoch no.5 train no.422380  loss = 3.19761 avg_loss = 3.06744\n",
      "epoch no.5 train no.422390  loss = 4.07712 avg_loss = 3.07524\n",
      "epoch no.5 train no.422400  loss = 2.98447 avg_loss = 3.03397\n",
      "epoch no.5 train no.422410  loss = 3.17672 avg_loss = 3.04173\n",
      "epoch no.5 train no.422420  loss = 2.38837 avg_loss = 3.05954\n",
      "epoch no.5 train no.422430  loss = 3.03885 avg_loss = 3.06561\n",
      "epoch no.5 train no.422440  loss = 2.26478 avg_loss = 3.07067\n",
      "epoch no.5 train no.422450  loss = 3.54122 avg_loss = 3.06853\n",
      "epoch no.5 train no.422460  loss = 4.77563 avg_loss = 3.11542\n",
      "epoch no.5 train no.422470  loss = 2.34460 avg_loss = 3.13634\n",
      "epoch no.5 train no.422480  loss = 5.55672 avg_loss = 3.15032\n",
      "epoch no.5 train no.422490  loss = 3.31435 avg_loss = 3.16265\n",
      "epoch no.5 train no.422500  loss = 3.81381 avg_loss = 3.15441\n",
      "epoch no.5 train no.422510  loss = 2.18217 avg_loss = 3.10940\n",
      "epoch no.5 train no.422520  loss = 3.12654 avg_loss = 3.10755\n",
      "epoch no.5 train no.422530  loss = 3.34285 avg_loss = 3.12817\n",
      "epoch no.5 train no.422540  loss = 3.85481 avg_loss = 3.13536\n",
      "epoch no.5 train no.422550  loss = 2.67595 avg_loss = 3.12715\n",
      "epoch no.5 train no.422560  loss = 3.16469 avg_loss = 3.11111\n",
      "epoch no.5 train no.422570  loss = 3.19909 avg_loss = 3.08318\n",
      "epoch no.5 train no.422580  loss = 1.56305 avg_loss = 3.04629\n",
      "epoch no.5 train no.422590  loss = 3.96702 avg_loss = 3.07240\n",
      "epoch no.5 train no.422600  loss = 3.86805 avg_loss = 3.08109\n",
      "epoch no.5 train no.422610  loss = 3.13688 avg_loss = 3.08017\n",
      "epoch no.5 train no.422620  loss = 4.10605 avg_loss = 3.09208\n",
      "epoch no.5 train no.422630  loss = 3.37860 avg_loss = 3.08710\n",
      "epoch no.5 train no.422640  loss = 3.83009 avg_loss = 3.12214\n",
      "epoch no.5 train no.422650  loss = 2.78684 avg_loss = 3.12349\n",
      "epoch no.5 train no.422660  loss = 3.03818 avg_loss = 3.10048\n",
      "epoch no.5 train no.422670  loss = 3.15431 avg_loss = 3.11402\n",
      "epoch no.5 train no.422680  loss = 2.40103 avg_loss = 3.09308\n",
      "epoch no.5 train no.422690  loss = 4.79326 avg_loss = 3.12449\n",
      "epoch no.5 train no.422700  loss = 3.71853 avg_loss = 3.11006\n",
      "epoch no.5 train no.422710  loss = 3.02451 avg_loss = 3.08219\n",
      "epoch no.5 train no.422720  loss = 2.67872 avg_loss = 3.10513\n",
      "epoch no.5 train no.422730  loss = 2.69053 avg_loss = 3.13391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.422740  loss = 2.41762 avg_loss = 3.09304\n",
      "epoch no.5 train no.422750  loss = 4.31392 avg_loss = 3.11339\n",
      "epoch no.5 train no.422760  loss = 2.75660 avg_loss = 3.07683\n",
      "epoch no.5 train no.422770  loss = 3.58050 avg_loss = 3.11407\n",
      "epoch no.5 train no.422780  loss = 3.63868 avg_loss = 3.11109\n",
      "epoch no.5 train no.422790  loss = 2.25278 avg_loss = 3.08032\n",
      "epoch no.5 train no.422800  loss = 3.72381 avg_loss = 3.06609\n",
      "epoch no.5 train no.422810  loss = 3.28933 avg_loss = 3.05037\n",
      "epoch no.5 train no.422820  loss = 3.17967 avg_loss = 3.03033\n",
      "epoch no.5 train no.422830  loss = 3.76924 avg_loss = 3.03728\n",
      "epoch no.5 train no.422840  loss = 3.37461 avg_loss = 3.03732\n",
      "epoch no.5 train no.422850  loss = 2.87633 avg_loss = 3.02049\n",
      "epoch no.5 train no.422860  loss = 3.18168 avg_loss = 3.04628\n",
      "epoch no.5 train no.422870  loss = 2.08870 avg_loss = 3.03496\n",
      "epoch no.5 train no.422880  loss = 3.25678 avg_loss = 3.03668\n",
      "epoch no.5 train no.422890  loss = 2.52246 avg_loss = 3.03898\n",
      "epoch no.5 train no.422900  loss = 2.57842 avg_loss = 3.04490\n",
      "epoch no.5 train no.422910  loss = 4.23130 avg_loss = 3.11514\n",
      "epoch no.5 train no.422920  loss = 4.74909 avg_loss = 3.15437\n",
      "epoch no.5 train no.422930  loss = 3.09451 avg_loss = 3.15423\n",
      "epoch no.5 train no.422940  loss = 5.23920 avg_loss = 3.18808\n",
      "epoch no.5 train no.422950  loss = 3.20643 avg_loss = 3.18980\n",
      "epoch no.5 train no.422960  loss = 2.62108 avg_loss = 3.18964\n",
      "epoch no.5 train no.422970  loss = 4.29948 avg_loss = 3.18966\n",
      "epoch no.5 train no.422980  loss = 2.13770 avg_loss = 3.16941\n",
      "epoch no.5 train no.422990  loss = 3.63259 avg_loss = 3.14209\n",
      "epoch no.5 train no.423000  loss = 3.02403 avg_loss = 3.12258\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.423010  loss = 3.49182 avg_loss = 3.15179\n",
      "epoch no.5 train no.423020  loss = 4.46445 avg_loss = 3.18356\n",
      "epoch no.5 train no.423030  loss = 2.22661 avg_loss = 3.16727\n",
      "epoch no.5 train no.423040  loss = 2.14055 avg_loss = 3.13782\n",
      "epoch no.5 train no.423050  loss = 2.76757 avg_loss = 3.18506\n",
      "epoch no.5 train no.423060  loss = 3.28512 avg_loss = 3.16566\n",
      "epoch no.5 train no.423070  loss = 2.99972 avg_loss = 3.17393\n",
      "epoch no.5 train no.423080  loss = 3.52844 avg_loss = 3.18126\n",
      "epoch no.5 train no.423090  loss = 3.35660 avg_loss = 3.20194\n",
      "epoch no.5 train no.423100  loss = 4.17904 avg_loss = 3.19327\n",
      "epoch no.5 train no.423110  loss = 3.76341 avg_loss = 3.17614\n",
      "epoch no.5 train no.423120  loss = 4.08562 avg_loss = 3.18694\n",
      "epoch no.5 train no.423130  loss = 4.97366 avg_loss = 3.14904\n",
      "epoch no.5 train no.423140  loss = 3.02907 avg_loss = 3.16314\n",
      "epoch no.5 train no.423150  loss = 4.70810 avg_loss = 3.15662\n",
      "epoch no.5 train no.423160  loss = 4.19692 avg_loss = 3.14663\n",
      "epoch no.5 train no.423170  loss = 3.93594 avg_loss = 3.11689\n",
      "epoch no.5 train no.423180  loss = 3.22964 avg_loss = 3.08490\n",
      "epoch no.5 train no.423190  loss = 2.32426 avg_loss = 3.05793\n",
      "epoch no.5 train no.423200  loss = 3.80502 avg_loss = 3.07420\n",
      "epoch no.5 train no.423210  loss = 3.22947 avg_loss = 3.05974\n",
      "epoch no.5 train no.423220  loss = 4.23610 avg_loss = 3.07382\n",
      "epoch no.5 train no.423230  loss = 3.73181 avg_loss = 3.07418\n",
      "epoch no.5 train no.423240  loss = 4.10068 avg_loss = 3.05958\n",
      "epoch no.5 train no.423250  loss = 3.58773 avg_loss = 3.02702\n",
      "epoch no.5 train no.423260  loss = 3.52864 avg_loss = 3.01390\n",
      "epoch no.5 train no.423270  loss = 1.99565 avg_loss = 2.97872\n",
      "epoch no.5 train no.423280  loss = 2.19210 avg_loss = 2.96859\n",
      "epoch no.5 train no.423290  loss = 2.73148 avg_loss = 2.96414\n",
      "epoch no.5 train no.423300  loss = 2.39001 avg_loss = 2.99726\n",
      "epoch no.5 train no.423310  loss = 3.28267 avg_loss = 3.01222\n",
      "epoch no.5 train no.423320  loss = 2.29237 avg_loss = 3.04010\n",
      "epoch no.5 train no.423330  loss = 3.67260 avg_loss = 3.12248\n",
      "epoch no.5 train no.423340  loss = 3.25470 avg_loss = 3.15810\n",
      "epoch no.5 train no.423350  loss = 3.03980 avg_loss = 3.11583\n",
      "epoch no.5 train no.423360  loss = 1.72853 avg_loss = 3.12041\n",
      "epoch no.5 train no.423370  loss = 3.69370 avg_loss = 3.08681\n",
      "epoch no.5 train no.423380  loss = 3.04712 avg_loss = 3.07138\n",
      "epoch no.5 train no.423390  loss = 3.47666 avg_loss = 3.04170\n",
      "epoch no.5 train no.423400  loss = 2.65356 avg_loss = 3.04415\n",
      "epoch no.5 train no.423410  loss = 3.45226 avg_loss = 3.04922\n",
      "epoch no.5 train no.423420  loss = 2.79438 avg_loss = 3.04911\n",
      "epoch no.5 train no.423430  loss = 5.54035 avg_loss = 3.04756\n",
      "epoch no.5 train no.423440  loss = 2.87457 avg_loss = 3.06786\n",
      "epoch no.5 train no.423450  loss = 2.50955 avg_loss = 3.03585\n",
      "epoch no.5 train no.423460  loss = 2.67288 avg_loss = 3.04083\n",
      "epoch no.5 train no.423470  loss = 3.89909 avg_loss = 3.05227\n",
      "epoch no.5 train no.423480  loss = 3.56168 avg_loss = 3.09642\n",
      "epoch no.5 train no.423490  loss = 3.60733 avg_loss = 3.10286\n",
      "epoch no.5 train no.423500  loss = 4.88074 avg_loss = 3.14797\n",
      "epoch no.5 train no.423510  loss = 4.11923 avg_loss = 3.12427\n",
      "epoch no.5 train no.423520  loss = 3.24887 avg_loss = 3.13093\n",
      "epoch no.5 train no.423530  loss = 3.94046 avg_loss = 3.13662\n",
      "epoch no.5 train no.423540  loss = 4.57531 avg_loss = 3.15361\n",
      "epoch no.5 train no.423550  loss = 2.28086 avg_loss = 3.10625\n",
      "epoch no.5 train no.423560  loss = 3.98416 avg_loss = 3.10238\n",
      "epoch no.5 train no.423570  loss = 3.58386 avg_loss = 3.12824\n",
      "epoch no.5 train no.423580  loss = 2.85424 avg_loss = 3.13479\n",
      "epoch no.5 train no.423590  loss = 4.44567 avg_loss = 3.13682\n",
      "epoch no.5 train no.423600  loss = 3.59657 avg_loss = 3.14020\n",
      "epoch no.5 train no.423610  loss = 3.48166 avg_loss = 3.12663\n",
      "epoch no.5 train no.423620  loss = 3.45572 avg_loss = 3.14663\n",
      "epoch no.5 train no.423630  loss = 4.06354 avg_loss = 3.13386\n",
      "epoch no.5 train no.423640  loss = 3.62264 avg_loss = 3.15094\n",
      "epoch no.5 train no.423650  loss = 5.01280 avg_loss = 3.16953\n",
      "epoch no.5 train no.423660  loss = 3.04423 avg_loss = 3.17684\n",
      "epoch no.5 train no.423670  loss = 2.83453 avg_loss = 3.17790\n",
      "epoch no.5 train no.423680  loss = 2.17313 avg_loss = 3.16380\n",
      "epoch no.5 train no.423690  loss = 4.23788 avg_loss = 3.20726\n",
      "epoch no.5 train no.423700  loss = 2.48466 avg_loss = 3.20148\n",
      "epoch no.5 train no.423710  loss = 2.71728 avg_loss = 3.19520\n",
      "epoch no.5 train no.423720  loss = 4.17530 avg_loss = 3.17879\n",
      "epoch no.5 train no.423730  loss = 5.35294 avg_loss = 3.19891\n",
      "epoch no.5 train no.423740  loss = 2.50657 avg_loss = 3.19871\n",
      "epoch no.5 train no.423750  loss = 2.99349 avg_loss = 3.22583\n",
      "epoch no.5 train no.423760  loss = 2.81770 avg_loss = 3.21110\n",
      "epoch no.5 train no.423770  loss = 1.89132 avg_loss = 3.18897\n",
      "epoch no.5 train no.423780  loss = 3.23141 avg_loss = 3.15229\n",
      "epoch no.5 train no.423790  loss = 2.62812 avg_loss = 3.16118\n",
      "epoch no.5 train no.423800  loss = 2.95810 avg_loss = 3.15869\n",
      "epoch no.5 train no.423810  loss = 2.58596 avg_loss = 3.13833\n",
      "epoch no.5 train no.423820  loss = 1.76871 avg_loss = 3.14338\n",
      "epoch no.5 train no.423830  loss = 5.60346 avg_loss = 3.17650\n",
      "epoch no.5 train no.423840  loss = 2.87325 avg_loss = 3.15559\n",
      "epoch no.5 train no.423850  loss = 2.37128 avg_loss = 3.13581\n",
      "epoch no.5 train no.423860  loss = 2.42663 avg_loss = 3.14986\n",
      "epoch no.5 train no.423870  loss = 4.02658 avg_loss = 3.14673\n",
      "epoch no.5 train no.423880  loss = 1.91698 avg_loss = 3.16686\n",
      "epoch no.5 train no.423890  loss = 3.12407 avg_loss = 3.18953\n",
      "epoch no.5 train no.423900  loss = 4.79309 avg_loss = 3.17486\n",
      "epoch no.5 train no.423910  loss = 3.07092 avg_loss = 3.16093\n",
      "epoch no.5 train no.423920  loss = 4.51685 avg_loss = 3.19124\n",
      "epoch no.5 train no.423930  loss = 3.22639 avg_loss = 3.18219\n",
      "epoch no.5 train no.423940  loss = 2.36709 avg_loss = 3.18325\n",
      "epoch no.5 train no.423950  loss = 3.75131 avg_loss = 3.18294\n",
      "epoch no.5 train no.423960  loss = 2.19859 avg_loss = 3.17372\n",
      "epoch no.5 train no.423970  loss = 2.02993 avg_loss = 3.11671\n",
      "epoch no.5 train no.423980  loss = 2.17326 avg_loss = 3.13760\n",
      "epoch no.5 train no.423990  loss = 2.40365 avg_loss = 3.14652\n",
      "epoch no.5 train no.424000  loss = 3.06889 avg_loss = 3.12544\n",
      "4\n",
      "to_tokens: ['▁가을', '▁싸이', '성', '그룹', '그룹', '</s>']\n",
      "추억의 혼성 댄스그룹</s>\n",
      "epoch no.5 train no.424010  loss = 4.32366 avg_loss = 3.15487\n",
      "epoch no.5 train no.424020  loss = 3.49940 avg_loss = 3.12456\n",
      "epoch no.5 train no.424030  loss = 5.02929 avg_loss = 3.15129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.424040  loss = 3.46948 avg_loss = 3.12631\n",
      "epoch no.5 train no.424050  loss = 2.68100 avg_loss = 3.13374\n",
      "epoch no.5 train no.424060  loss = 2.38374 avg_loss = 3.10350\n",
      "epoch no.5 train no.424070  loss = 2.38774 avg_loss = 3.07082\n",
      "epoch no.5 train no.424080  loss = 1.92694 avg_loss = 3.05842\n",
      "epoch no.5 train no.424090  loss = 3.80760 avg_loss = 3.05940\n",
      "epoch no.5 train no.424100  loss = 4.67926 avg_loss = 3.09420\n",
      "epoch no.5 train no.424110  loss = 3.13997 avg_loss = 3.12932\n",
      "epoch no.5 train no.424120  loss = 4.02656 avg_loss = 3.14927\n",
      "epoch no.5 train no.424130  loss = 3.22060 avg_loss = 3.16602\n",
      "epoch no.5 train no.424140  loss = 4.25642 avg_loss = 3.20971\n",
      "epoch no.5 train no.424150  loss = 3.31675 avg_loss = 3.20615\n",
      "epoch no.5 train no.424160  loss = 2.52116 avg_loss = 3.20397\n",
      "epoch no.5 train no.424170  loss = 2.56777 avg_loss = 3.18323\n",
      "epoch no.5 train no.424180  loss = 2.68083 avg_loss = 3.16357\n",
      "epoch no.5 train no.424190  loss = 2.77387 avg_loss = 3.16982\n",
      "epoch no.5 train no.424200  loss = 2.54012 avg_loss = 3.12567\n",
      "epoch no.5 train no.424210  loss = 2.58587 avg_loss = 3.10237\n",
      "epoch no.5 train no.424220  loss = 2.50628 avg_loss = 3.10573\n",
      "epoch no.5 train no.424230  loss = 2.91683 avg_loss = 3.05124\n",
      "epoch no.5 train no.424240  loss = 3.69668 avg_loss = 3.07231\n",
      "epoch no.5 train no.424250  loss = 4.69327 avg_loss = 3.08749\n",
      "epoch no.5 train no.424260  loss = 2.66237 avg_loss = 3.05813\n",
      "epoch no.5 train no.424270  loss = 2.38420 avg_loss = 3.06869\n",
      "epoch no.5 train no.424280  loss = 2.44380 avg_loss = 3.06360\n",
      "epoch no.5 train no.424290  loss = 2.03600 avg_loss = 3.08530\n",
      "epoch no.5 train no.424300  loss = 4.18054 avg_loss = 3.05610\n",
      "epoch no.5 train no.424310  loss = 2.93096 avg_loss = 3.10074\n",
      "epoch no.5 train no.424320  loss = 3.20128 avg_loss = 3.08605\n",
      "epoch no.5 train no.424330  loss = 3.47706 avg_loss = 3.08659\n",
      "epoch no.5 train no.424340  loss = 1.87456 avg_loss = 3.09479\n",
      "epoch no.5 train no.424350  loss = 3.14956 avg_loss = 3.12082\n",
      "epoch no.5 train no.424360  loss = 3.95922 avg_loss = 3.09786\n",
      "epoch no.5 train no.424370  loss = 2.65506 avg_loss = 3.10807\n",
      "epoch no.5 train no.424380  loss = 3.56743 avg_loss = 3.15891\n",
      "epoch no.5 train no.424390  loss = 3.03829 avg_loss = 3.11740\n",
      "epoch no.5 train no.424400  loss = 4.89712 avg_loss = 3.14662\n",
      "epoch no.5 train no.424410  loss = 2.58604 avg_loss = 3.19180\n",
      "epoch no.5 train no.424420  loss = 2.63877 avg_loss = 3.20677\n",
      "epoch no.5 train no.424430  loss = 3.17277 avg_loss = 3.21571\n",
      "epoch no.5 train no.424440  loss = 3.34539 avg_loss = 3.17764\n",
      "epoch no.5 train no.424450  loss = 4.00259 avg_loss = 3.18442\n",
      "epoch no.5 train no.424460  loss = 2.64528 avg_loss = 3.18499\n",
      "epoch no.5 train no.424470  loss = 2.82105 avg_loss = 3.16758\n",
      "epoch no.5 train no.424480  loss = 2.95727 avg_loss = 3.16778\n",
      "epoch no.5 train no.424490  loss = 3.51181 avg_loss = 3.11506\n",
      "epoch no.5 train no.424500  loss = 2.54708 avg_loss = 3.09977\n",
      "epoch no.5 train no.424510  loss = 3.37470 avg_loss = 3.09725\n",
      "epoch no.5 train no.424520  loss = 2.91528 avg_loss = 3.11997\n",
      "epoch no.5 train no.424530  loss = 3.66546 avg_loss = 3.17431\n",
      "epoch no.5 train no.424540  loss = 2.85518 avg_loss = 3.17844\n",
      "epoch no.5 train no.424550  loss = 4.19660 avg_loss = 3.17279\n",
      "epoch no.5 train no.424560  loss = 1.73656 avg_loss = 3.13514\n",
      "epoch no.5 train no.424570  loss = 5.55418 avg_loss = 3.19189\n",
      "epoch no.5 train no.424580  loss = 3.01845 avg_loss = 3.17887\n",
      "epoch no.5 train no.424590  loss = 4.94964 avg_loss = 3.19344\n",
      "epoch no.5 train no.424600  loss = 2.88226 avg_loss = 3.12647\n",
      "epoch no.5 train no.424610  loss = 4.11630 avg_loss = 3.10816\n",
      "epoch no.5 train no.424620  loss = 2.58518 avg_loss = 3.10294\n",
      "epoch no.5 train no.424630  loss = 4.60017 avg_loss = 3.08786\n",
      "epoch no.5 train no.424640  loss = 2.64386 avg_loss = 3.03814\n",
      "epoch no.5 train no.424650  loss = 3.09510 avg_loss = 3.02874\n",
      "epoch no.5 train no.424660  loss = 3.23422 avg_loss = 3.03755\n",
      "epoch no.5 train no.424670  loss = 3.32284 avg_loss = 3.06483\n",
      "epoch no.5 train no.424680  loss = 3.73251 avg_loss = 3.07698\n",
      "epoch no.5 train no.424690  loss = 2.79073 avg_loss = 3.04628\n",
      "epoch no.5 train no.424700  loss = 3.77471 avg_loss = 3.05632\n",
      "epoch no.5 train no.424710  loss = 2.16373 avg_loss = 3.02660\n",
      "epoch no.5 train no.424720  loss = 3.76591 avg_loss = 3.02974\n",
      "epoch no.5 train no.424730  loss = 2.92764 avg_loss = 3.01046\n",
      "epoch no.5 train no.424740  loss = 2.93988 avg_loss = 2.98022\n",
      "epoch no.5 train no.424750  loss = 3.23584 avg_loss = 2.98889\n",
      "epoch no.5 train no.424760  loss = 3.88372 avg_loss = 3.02117\n",
      "epoch no.5 train no.424770  loss = 3.69682 avg_loss = 3.08728\n",
      "epoch no.5 train no.424780  loss = 2.24907 avg_loss = 3.07047\n",
      "epoch no.5 train no.424790  loss = 3.39989 avg_loss = 3.10129\n",
      "epoch no.5 train no.424800  loss = 2.97486 avg_loss = 3.08679\n",
      "epoch no.5 train no.424810  loss = 3.31441 avg_loss = 3.10137\n",
      "epoch no.5 train no.424820  loss = 3.54274 avg_loss = 3.15644\n",
      "epoch no.5 train no.424830  loss = 4.08065 avg_loss = 3.11305\n",
      "epoch no.5 train no.424840  loss = 3.18970 avg_loss = 3.07824\n",
      "epoch no.5 train no.424850  loss = 4.95826 avg_loss = 3.04042\n",
      "epoch no.5 train no.424860  loss = 1.50328 avg_loss = 2.98762\n",
      "epoch no.5 train no.424870  loss = 2.25082 avg_loss = 2.98021\n",
      "epoch no.5 train no.424880  loss = 2.23889 avg_loss = 2.95993\n",
      "epoch no.5 train no.424890  loss = 3.22426 avg_loss = 2.95818\n",
      "epoch no.5 train no.424900  loss = 3.21554 avg_loss = 2.96559\n",
      "epoch no.5 train no.424910  loss = 3.94320 avg_loss = 3.02869\n",
      "epoch no.5 train no.424920  loss = 4.26741 avg_loss = 3.02252\n",
      "epoch no.5 train no.424930  loss = 2.55095 avg_loss = 3.04391\n",
      "epoch no.5 train no.424940  loss = 3.60920 avg_loss = 3.05978\n",
      "epoch no.5 train no.424950  loss = 3.52067 avg_loss = 3.09500\n",
      "epoch no.5 train no.424960  loss = 4.94403 avg_loss = 3.13793\n",
      "epoch no.5 train no.424970  loss = 3.64151 avg_loss = 3.13549\n",
      "epoch no.5 train no.424980  loss = 4.34467 avg_loss = 3.12870\n",
      "epoch no.5 train no.424990  loss = 4.00228 avg_loss = 3.13890\n",
      "epoch no.5 train no.425000  loss = 1.49075 avg_loss = 3.18435\n",
      "5\n",
      "to_tokens: ['▁비', '▁올드', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.425010  loss = 2.67253 avg_loss = 3.15631\n",
      "epoch no.5 train no.425020  loss = 2.16579 avg_loss = 3.12965\n",
      "epoch no.5 train no.425030  loss = 4.36631 avg_loss = 3.17801\n",
      "epoch no.5 train no.425040  loss = 3.56956 avg_loss = 3.14034\n",
      "epoch no.5 train no.425050  loss = 2.80241 avg_loss = 3.14214\n",
      "epoch no.5 train no.425060  loss = 3.59674 avg_loss = 3.08100\n",
      "epoch no.5 train no.425070  loss = 2.31125 avg_loss = 3.13187\n",
      "epoch no.5 train no.425080  loss = 4.57464 avg_loss = 3.14173\n",
      "epoch no.5 train no.425090  loss = 4.37606 avg_loss = 3.15763\n",
      "epoch no.5 train no.425100  loss = 2.92605 avg_loss = 3.15641\n",
      "epoch no.5 train no.425110  loss = 2.89002 avg_loss = 3.15934\n",
      "epoch no.5 train no.425120  loss = 2.58130 avg_loss = 3.12014\n",
      "epoch no.5 train no.425130  loss = 3.72050 avg_loss = 3.12151\n",
      "epoch no.5 train no.425140  loss = 1.93031 avg_loss = 3.10296\n",
      "epoch no.5 train no.425150  loss = 2.46714 avg_loss = 3.10246\n",
      "epoch no.5 train no.425160  loss = 2.04380 avg_loss = 3.08713\n",
      "epoch no.5 train no.425170  loss = 2.55437 avg_loss = 3.09050\n",
      "epoch no.5 train no.425180  loss = 2.26947 avg_loss = 3.06830\n",
      "epoch no.5 train no.425190  loss = 4.17573 avg_loss = 3.16446\n",
      "epoch no.5 train no.425200  loss = 3.98683 avg_loss = 3.17039\n",
      "epoch no.5 train no.425210  loss = 4.43357 avg_loss = 3.18059\n",
      "epoch no.5 train no.425220  loss = 2.45094 avg_loss = 3.14064\n",
      "epoch no.5 train no.425230  loss = 2.01533 avg_loss = 3.12748\n",
      "epoch no.5 train no.425240  loss = 4.39363 avg_loss = 3.13420\n",
      "epoch no.5 train no.425250  loss = 2.90123 avg_loss = 3.15721\n",
      "epoch no.5 train no.425260  loss = 3.10601 avg_loss = 3.13176\n",
      "epoch no.5 train no.425270  loss = 3.88148 avg_loss = 3.11747\n",
      "epoch no.5 train no.425280  loss = 4.29600 avg_loss = 3.16017\n",
      "epoch no.5 train no.425290  loss = 3.87420 avg_loss = 3.21173\n",
      "epoch no.5 train no.425300  loss = 3.37657 avg_loss = 3.15317\n",
      "epoch no.5 train no.425310  loss = 3.67906 avg_loss = 3.14811\n",
      "epoch no.5 train no.425320  loss = 2.25776 avg_loss = 3.16393\n",
      "epoch no.5 train no.425330  loss = 3.95597 avg_loss = 3.15925\n",
      "epoch no.5 train no.425340  loss = 1.95632 avg_loss = 3.10466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.425350  loss = 2.36205 avg_loss = 3.07102\n",
      "epoch no.5 train no.425360  loss = 2.48406 avg_loss = 3.06237\n",
      "epoch no.5 train no.425370  loss = 2.50750 avg_loss = 3.05585\n",
      "epoch no.5 train no.425380  loss = 2.82597 avg_loss = 3.07697\n",
      "epoch no.5 train no.425390  loss = 3.88130 avg_loss = 3.10552\n",
      "epoch no.5 train no.425400  loss = 2.91635 avg_loss = 3.09158\n",
      "epoch no.5 train no.425410  loss = 2.51060 avg_loss = 3.08702\n",
      "epoch no.5 train no.425420  loss = 2.35156 avg_loss = 3.06855\n",
      "epoch no.5 train no.425430  loss = 3.48675 avg_loss = 3.11148\n",
      "epoch no.5 train no.425440  loss = 4.32886 avg_loss = 3.08141\n",
      "epoch no.5 train no.425450  loss = 3.12757 avg_loss = 3.10978\n",
      "epoch no.5 train no.425460  loss = 4.45325 avg_loss = 3.11385\n",
      "epoch no.5 train no.425470  loss = 3.31889 avg_loss = 3.12823\n",
      "epoch no.5 train no.425480  loss = 2.84097 avg_loss = 3.13987\n",
      "epoch no.5 train no.425490  loss = 3.50290 avg_loss = 3.10182\n",
      "epoch no.5 train no.425500  loss = 3.59267 avg_loss = 3.10766\n",
      "epoch no.5 train no.425510  loss = 3.07922 avg_loss = 3.12407\n",
      "epoch no.5 train no.425520  loss = 3.00669 avg_loss = 3.12221\n",
      "epoch no.5 train no.425530  loss = 2.53844 avg_loss = 3.06706\n",
      "epoch no.5 train no.425540  loss = 3.92089 avg_loss = 3.07327\n",
      "epoch no.5 train no.425550  loss = 3.96319 avg_loss = 3.09256\n",
      "epoch no.5 train no.425560  loss = 3.68860 avg_loss = 3.11094\n",
      "epoch no.5 train no.425570  loss = 3.78862 avg_loss = 3.10227\n",
      "epoch no.5 train no.425580  loss = 2.98355 avg_loss = 3.07340\n",
      "epoch no.5 train no.425590  loss = 3.79834 avg_loss = 3.07091\n",
      "epoch no.5 train no.425600  loss = 2.77078 avg_loss = 3.06917\n",
      "epoch no.5 train no.425610  loss = 3.39848 avg_loss = 3.05334\n",
      "epoch no.5 train no.425620  loss = 3.19430 avg_loss = 3.01980\n",
      "epoch no.5 train no.425630  loss = 2.58550 avg_loss = 2.99409\n",
      "epoch no.5 train no.425640  loss = 2.36319 avg_loss = 2.98242\n",
      "epoch no.5 train no.425650  loss = 3.56547 avg_loss = 2.99713\n",
      "epoch no.5 train no.425660  loss = 3.60780 avg_loss = 3.01364\n",
      "epoch no.5 train no.425670  loss = 2.69303 avg_loss = 2.98154\n",
      "epoch no.5 train no.425680  loss = 2.59724 avg_loss = 2.98861\n",
      "epoch no.5 train no.425690  loss = 2.39517 avg_loss = 3.00331\n",
      "epoch no.5 train no.425700  loss = 3.00932 avg_loss = 3.04058\n",
      "epoch no.5 train no.425710  loss = 2.38004 avg_loss = 3.03975\n",
      "epoch no.5 train no.425720  loss = 4.09111 avg_loss = 3.05648\n",
      "epoch no.5 train no.425730  loss = 2.14626 avg_loss = 3.04458\n",
      "epoch no.5 train no.425740  loss = 3.19696 avg_loss = 3.06137\n",
      "epoch no.5 train no.425750  loss = 2.15184 avg_loss = 3.04061\n",
      "epoch no.5 train no.425760  loss = 3.78890 avg_loss = 3.03694\n",
      "epoch no.5 train no.425770  loss = 2.78968 avg_loss = 3.05425\n",
      "epoch no.5 train no.425780  loss = 2.91552 avg_loss = 3.06417\n",
      "epoch no.5 train no.425790  loss = 1.21461 avg_loss = 3.08649\n",
      "epoch no.5 train no.425800  loss = 2.31679 avg_loss = 3.04946\n",
      "epoch no.5 train no.425810  loss = 2.82555 avg_loss = 3.04394\n",
      "epoch no.5 train no.425820  loss = 2.30055 avg_loss = 3.07006\n",
      "epoch no.5 train no.425830  loss = 4.67180 avg_loss = 3.13840\n",
      "epoch no.5 train no.425840  loss = 2.25607 avg_loss = 3.10780\n",
      "epoch no.5 train no.425850  loss = 4.05392 avg_loss = 3.07118\n",
      "epoch no.5 train no.425860  loss = 2.83873 avg_loss = 3.06753\n",
      "epoch no.5 train no.425870  loss = 4.22129 avg_loss = 3.07734\n",
      "epoch no.5 train no.425880  loss = 2.69394 avg_loss = 3.05470\n",
      "epoch no.5 train no.425890  loss = 2.36024 avg_loss = 3.04808\n",
      "epoch no.5 train no.425900  loss = 3.51814 avg_loss = 3.05220\n",
      "epoch no.5 train no.425910  loss = 1.77983 avg_loss = 3.06598\n",
      "epoch no.5 train no.425920  loss = 2.99774 avg_loss = 3.05614\n",
      "epoch no.5 train no.425930  loss = 1.62917 avg_loss = 3.06713\n",
      "epoch no.5 train no.425940  loss = 2.36891 avg_loss = 3.02097\n",
      "epoch no.5 train no.425950  loss = 3.06277 avg_loss = 3.02180\n",
      "epoch no.5 train no.425960  loss = 2.65671 avg_loss = 2.98997\n",
      "epoch no.5 train no.425970  loss = 2.12582 avg_loss = 2.98448\n",
      "epoch no.5 train no.425980  loss = 4.78227 avg_loss = 2.99527\n",
      "epoch no.5 train no.425990  loss = 3.34653 avg_loss = 3.01156\n",
      "epoch no.5 train no.426000  loss = 1.38595 avg_loss = 2.99599\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁o', 'st', '▁모음', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.5 train no.426010  loss = 4.05174 avg_loss = 3.05018\n",
      "epoch no.5 train no.426020  loss = 2.23116 avg_loss = 3.01181\n",
      "epoch no.5 train no.426030  loss = 2.45790 avg_loss = 3.02581\n",
      "epoch no.5 train no.426040  loss = 1.93403 avg_loss = 3.03872\n",
      "epoch no.5 train no.426050  loss = 3.49957 avg_loss = 3.05178\n",
      "epoch no.5 train no.426060  loss = 3.23416 avg_loss = 3.06487\n",
      "epoch no.5 train no.426070  loss = 2.00374 avg_loss = 3.06139\n",
      "epoch no.5 train no.426080  loss = 1.87578 avg_loss = 3.05991\n",
      "epoch no.5 train no.426090  loss = 2.53894 avg_loss = 3.07791\n",
      "epoch no.5 train no.426100  loss = 2.70336 avg_loss = 3.08877\n",
      "epoch no.5 train no.426110  loss = 3.62733 avg_loss = 3.07827\n",
      "epoch no.5 train no.426120  loss = 2.02112 avg_loss = 3.09062\n",
      "epoch no.5 train no.426130  loss = 3.28302 avg_loss = 3.12295\n",
      "epoch no.5 train no.426140  loss = 3.28050 avg_loss = 3.10203\n",
      "epoch no.5 train no.426150  loss = 4.34302 avg_loss = 3.11327\n",
      "epoch no.5 train no.426160  loss = 3.47351 avg_loss = 3.13064\n",
      "epoch no.5 train no.426170  loss = 1.99607 avg_loss = 3.11273\n",
      "epoch no.5 train no.426180  loss = 2.51509 avg_loss = 3.07586\n",
      "epoch no.5 train no.426190  loss = 4.88400 avg_loss = 3.07966\n",
      "epoch no.5 train no.426200  loss = 3.24193 avg_loss = 3.07330\n",
      "epoch no.5 train no.426210  loss = 3.63953 avg_loss = 3.07088\n",
      "epoch no.5 train no.426220  loss = 3.06430 avg_loss = 3.03289\n",
      "epoch no.5 train no.426230  loss = 2.61424 avg_loss = 3.05286\n",
      "epoch no.5 train no.426240  loss = 3.31622 avg_loss = 3.07006\n",
      "epoch no.5 train no.426250  loss = 2.31155 avg_loss = 3.07470\n",
      "epoch no.5 train no.426260  loss = 2.52679 avg_loss = 3.06449\n",
      "epoch no.5 train no.426270  loss = 2.90660 avg_loss = 3.07007\n",
      "epoch no.5 train no.426280  loss = 3.66597 avg_loss = 3.08257\n",
      "epoch no.5 train no.426290  loss = 2.50684 avg_loss = 3.08800\n",
      "epoch no.5 train no.426300  loss = 2.18287 avg_loss = 3.09522\n",
      "epoch no.5 train no.426310  loss = 4.15292 avg_loss = 3.09352\n",
      "epoch no.5 train no.426320  loss = 2.05694 avg_loss = 3.08416\n",
      "epoch no.5 train no.426330  loss = 1.62773 avg_loss = 3.10410\n",
      "epoch no.5 train no.426340  loss = 4.35024 avg_loss = 3.07270\n",
      "epoch no.5 train no.426350  loss = 1.85283 avg_loss = 3.02404\n",
      "epoch no.5 train no.426360  loss = 4.21864 avg_loss = 3.05941\n",
      "epoch no.5 train no.426370  loss = 1.60551 avg_loss = 3.04020\n",
      "epoch no.5 train no.426380  loss = 3.60903 avg_loss = 3.08613\n",
      "epoch no.5 train no.426390  loss = 3.76603 avg_loss = 3.08300\n",
      "epoch no.5 train no.426400  loss = 1.41753 avg_loss = 3.08157\n",
      "epoch no.5 train no.426410  loss = 3.34941 avg_loss = 3.06498\n",
      "epoch no.5 train no.426420  loss = 2.45302 avg_loss = 3.07994\n",
      "epoch no.5 train no.426430  loss = 3.70508 avg_loss = 3.08395\n",
      "epoch no.5 train no.426440  loss = 2.71177 avg_loss = 3.11897\n",
      "epoch no.5 train no.426450  loss = 2.47300 avg_loss = 3.08017\n",
      "epoch no.5 train no.426460  loss = 3.59730 avg_loss = 3.07486\n",
      "epoch no.5 train no.426470  loss = 2.29178 avg_loss = 3.06123\n",
      "epoch no.5 train no.426480  loss = 3.41552 avg_loss = 3.07301\n",
      "epoch no.5 train no.426490  loss = 4.52246 avg_loss = 3.12036\n",
      "epoch no.5 train no.426500  loss = 2.80483 avg_loss = 3.13504\n",
      "epoch no.5 train no.426510  loss = 4.23748 avg_loss = 3.14119\n",
      "epoch no.5 train no.426520  loss = 3.96325 avg_loss = 3.15044\n",
      "epoch no.5 train no.426530  loss = 3.23892 avg_loss = 3.16278\n",
      "epoch no.5 train no.426540  loss = 2.13419 avg_loss = 3.12261\n",
      "epoch no.5 train no.426550  loss = 2.65705 avg_loss = 3.15168\n",
      "epoch no.5 train no.426560  loss = 2.43846 avg_loss = 3.11019\n",
      "epoch no.5 train no.426570  loss = 4.52496 avg_loss = 3.15242\n",
      "epoch no.5 train no.426580  loss = 3.59248 avg_loss = 3.17887\n",
      "epoch no.5 train no.426590  loss = 2.01607 avg_loss = 3.20108\n",
      "epoch no.5 train no.426600  loss = 2.42551 avg_loss = 3.20474\n",
      "epoch no.5 train no.426610  loss = 3.77310 avg_loss = 3.20748\n",
      "epoch no.5 train no.426620  loss = 4.40574 avg_loss = 3.18513\n",
      "epoch no.5 train no.426630  loss = 3.01426 avg_loss = 3.19588\n",
      "epoch no.5 train no.426640  loss = 2.68780 avg_loss = 3.12414\n",
      "epoch no.5 train no.426650  loss = 2.01721 avg_loss = 3.11342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.426660  loss = 5.04683 avg_loss = 3.15204\n",
      "epoch no.5 train no.426670  loss = 3.08465 avg_loss = 3.14727\n",
      "epoch no.5 train no.426680  loss = 3.65268 avg_loss = 3.19765\n",
      "epoch no.5 train no.426690  loss = 2.48250 avg_loss = 3.19152\n",
      "epoch no.5 train no.426700  loss = 3.34243 avg_loss = 3.16251\n",
      "epoch no.5 train no.426710  loss = 2.43702 avg_loss = 3.14527\n",
      "epoch no.5 train no.426720  loss = 4.30286 avg_loss = 3.13350\n",
      "epoch no.5 train no.426730  loss = 5.60492 avg_loss = 3.11584\n",
      "epoch no.5 train no.426740  loss = 3.31115 avg_loss = 3.14965\n",
      "epoch no.5 train no.426750  loss = 2.32259 avg_loss = 3.12297\n",
      "epoch no.5 train no.426760  loss = 2.80199 avg_loss = 3.15252\n",
      "epoch no.5 train no.426770  loss = 3.30339 avg_loss = 3.13218\n",
      "epoch no.5 train no.426780  loss = 3.73322 avg_loss = 3.14361\n",
      "epoch no.5 train no.426790  loss = 3.75924 avg_loss = 3.13934\n",
      "epoch no.5 train no.426800  loss = 1.75531 avg_loss = 3.10330\n",
      "epoch no.5 train no.426810  loss = 2.93108 avg_loss = 3.08639\n",
      "epoch no.5 train no.426820  loss = 1.96604 avg_loss = 3.05353\n",
      "epoch no.5 train no.426830  loss = 1.96376 avg_loss = 3.07954\n",
      "epoch no.5 train no.426840  loss = 3.45544 avg_loss = 3.08799\n",
      "epoch no.5 train no.426850  loss = 3.48999 avg_loss = 3.08316\n",
      "epoch no.5 train no.426860  loss = 3.66140 avg_loss = 3.11847\n",
      "epoch no.5 train no.426870  loss = 3.66345 avg_loss = 3.07148\n",
      "epoch no.5 train no.426880  loss = 2.72650 avg_loss = 3.06070\n",
      "epoch no.5 train no.426890  loss = 1.17992 avg_loss = 3.07172\n",
      "epoch no.5 train no.426900  loss = 2.99696 avg_loss = 3.07250\n",
      "epoch no.5 train no.426910  loss = 2.89813 avg_loss = 3.05588\n",
      "epoch no.5 train no.426920  loss = 4.30113 avg_loss = 3.05076\n",
      "epoch no.5 train no.426930  loss = 3.79720 avg_loss = 3.07331\n",
      "epoch no.5 train no.426940  loss = 2.24563 avg_loss = 3.08220\n",
      "epoch no.5 train no.426950  loss = 2.83114 avg_loss = 3.07193\n",
      "epoch no.5 train no.426960  loss = 2.69247 avg_loss = 3.08904\n",
      "epoch no.5 train no.426970  loss = 3.73754 avg_loss = 3.10284\n",
      "epoch no.5 train no.426980  loss = 2.98111 avg_loss = 3.11520\n",
      "epoch no.5 train no.426990  loss = 3.09589 avg_loss = 3.10211\n",
      "epoch no.5 train no.427000  loss = 2.63002 avg_loss = 3.06538\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁팝', '송', '</s>']\n",
      "추억의 90년대 팝송</s>\n",
      "epoch no.5 train no.427010  loss = 4.22272 avg_loss = 3.08337\n",
      "epoch no.5 train no.427020  loss = 3.87278 avg_loss = 3.11005\n",
      "epoch no.5 train no.427030  loss = 3.15621 avg_loss = 3.09955\n",
      "epoch no.5 train no.427040  loss = 1.37392 avg_loss = 3.08435\n",
      "epoch no.5 train no.427050  loss = 3.46158 avg_loss = 3.11020\n",
      "epoch no.5 train no.427060  loss = 1.10358 avg_loss = 3.07022\n",
      "epoch no.5 train no.427070  loss = 2.34246 avg_loss = 3.06650\n",
      "epoch no.5 train no.427080  loss = 2.39642 avg_loss = 3.07763\n",
      "epoch no.5 train no.427090  loss = 2.76348 avg_loss = 3.03837\n",
      "epoch no.5 train no.427100  loss = 4.28465 avg_loss = 3.07177\n",
      "epoch no.5 train no.427110  loss = 2.47130 avg_loss = 3.05089\n",
      "epoch no.5 train no.427120  loss = 3.51913 avg_loss = 3.07220\n",
      "epoch no.5 train no.427130  loss = 2.72852 avg_loss = 3.05530\n",
      "epoch no.5 train no.427140  loss = 2.83596 avg_loss = 3.08497\n",
      "epoch no.5 train no.427150  loss = 5.38756 avg_loss = 3.11494\n",
      "epoch no.5 train no.427160  loss = 3.26630 avg_loss = 3.11748\n",
      "epoch no.5 train no.427170  loss = 3.68743 avg_loss = 3.12164\n",
      "epoch no.5 train no.427180  loss = 3.59469 avg_loss = 3.14512\n",
      "epoch no.5 train no.427190  loss = 3.97084 avg_loss = 3.15160\n",
      "epoch no.5 train no.427200  loss = 2.62259 avg_loss = 3.17599\n",
      "epoch no.5 train no.427210  loss = 2.29906 avg_loss = 3.15905\n",
      "epoch no.5 train no.427220  loss = 2.32304 avg_loss = 3.16218\n",
      "epoch no.5 train no.427230  loss = 4.80928 avg_loss = 3.16643\n",
      "epoch no.5 train no.427240  loss = 4.98635 avg_loss = 3.19002\n",
      "epoch no.5 train no.427250  loss = 2.02279 avg_loss = 3.18200\n",
      "epoch no.5 train no.427260  loss = 2.08427 avg_loss = 3.18054\n",
      "epoch no.5 train no.427270  loss = 3.46011 avg_loss = 3.17379\n",
      "epoch no.5 train no.427280  loss = 2.40700 avg_loss = 3.17648\n",
      "epoch no.5 train no.427290  loss = 2.37486 avg_loss = 3.17614\n",
      "epoch no.5 train no.427300  loss = 2.10479 avg_loss = 3.11459\n",
      "epoch no.5 train no.427310  loss = 2.62092 avg_loss = 3.10973\n",
      "epoch no.5 train no.427320  loss = 3.22495 avg_loss = 3.12058\n",
      "epoch no.5 train no.427330  loss = 4.13525 avg_loss = 3.14628\n",
      "epoch no.5 train no.427340  loss = 2.78570 avg_loss = 3.11014\n",
      "epoch no.5 train no.427350  loss = 2.37109 avg_loss = 3.08177\n",
      "epoch no.5 train no.427360  loss = 3.78079 avg_loss = 3.06084\n",
      "epoch no.5 train no.427370  loss = 2.76555 avg_loss = 3.04954\n",
      "epoch no.5 train no.427380  loss = 3.11347 avg_loss = 3.00909\n",
      "epoch no.5 train no.427390  loss = 3.41449 avg_loss = 2.98314\n",
      "epoch no.5 train no.427400  loss = 2.49407 avg_loss = 2.99473\n",
      "epoch no.5 train no.427410  loss = 3.09924 avg_loss = 3.01299\n",
      "epoch no.5 train no.427420  loss = 3.02553 avg_loss = 2.99814\n",
      "epoch no.5 train no.427430  loss = 2.18608 avg_loss = 2.97620\n",
      "epoch no.5 train no.427440  loss = 3.30418 avg_loss = 3.01314\n",
      "epoch no.5 train no.427450  loss = 2.52002 avg_loss = 3.06126\n",
      "epoch no.5 train no.427460  loss = 1.81564 avg_loss = 3.03493\n",
      "epoch no.5 train no.427470  loss = 3.71603 avg_loss = 3.01309\n",
      "epoch no.5 train no.427480  loss = 4.00658 avg_loss = 3.04755\n",
      "epoch no.5 train no.427490  loss = 2.07494 avg_loss = 3.00221\n",
      "epoch no.5 train no.427500  loss = 4.19762 avg_loss = 3.02694\n",
      "epoch no.5 train no.427510  loss = 4.78259 avg_loss = 3.13639\n",
      "epoch no.5 train no.427520  loss = 1.66299 avg_loss = 3.10374\n",
      "epoch no.5 train no.427530  loss = 2.90019 avg_loss = 3.11351\n",
      "epoch no.5 train no.427540  loss = 2.84818 avg_loss = 3.08621\n",
      "epoch no.5 train no.427550  loss = 3.58199 avg_loss = 3.06149\n",
      "epoch no.5 train no.427560  loss = 2.79254 avg_loss = 3.08225\n",
      "epoch no.5 train no.427570  loss = 3.34990 avg_loss = 3.03819\n",
      "epoch no.5 train no.427580  loss = 1.93389 avg_loss = 3.02753\n",
      "epoch no.5 train no.427590  loss = 4.10785 avg_loss = 3.04294\n",
      "epoch no.5 train no.427600  loss = 5.56165 avg_loss = 3.08058\n",
      "epoch no.5 train no.427610  loss = 2.35582 avg_loss = 3.10660\n",
      "epoch no.5 train no.427620  loss = 3.95412 avg_loss = 3.12980\n",
      "epoch no.5 train no.427630  loss = 2.24330 avg_loss = 3.11462\n",
      "epoch no.5 train no.427640  loss = 2.47494 avg_loss = 3.10644\n",
      "epoch no.5 train no.427650  loss = 2.84903 avg_loss = 3.11540\n",
      "epoch no.5 train no.427660  loss = 2.55499 avg_loss = 3.12068\n",
      "epoch no.5 train no.427670  loss = 4.16815 avg_loss = 3.12897\n",
      "epoch no.5 train no.427680  loss = 2.39088 avg_loss = 3.13874\n",
      "epoch no.5 train no.427690  loss = 2.03377 avg_loss = 3.11142\n",
      "epoch no.5 train no.427700  loss = 3.03211 avg_loss = 3.09683\n",
      "epoch no.5 train no.427710  loss = 3.82499 avg_loss = 3.12013\n",
      "epoch no.5 train no.427720  loss = 2.61106 avg_loss = 3.10893\n",
      "epoch no.5 train no.427730  loss = 4.05829 avg_loss = 3.11900\n",
      "epoch no.5 train no.427740  loss = 2.30660 avg_loss = 3.11535\n",
      "epoch no.5 train no.427750  loss = 3.13583 avg_loss = 3.08356\n",
      "epoch no.5 train no.427760  loss = 2.33441 avg_loss = 3.06850\n",
      "epoch no.5 train no.427770  loss = 3.53871 avg_loss = 3.06951\n",
      "epoch no.5 train no.427780  loss = 4.01495 avg_loss = 3.09879\n",
      "epoch no.5 train no.427790  loss = 3.39458 avg_loss = 3.08627\n",
      "epoch no.5 train no.427800  loss = 3.49783 avg_loss = 3.14147\n",
      "epoch no.5 train no.427810  loss = 3.88880 avg_loss = 3.15191\n",
      "epoch no.5 train no.427820  loss = 3.01845 avg_loss = 3.19722\n",
      "epoch no.5 train no.427830  loss = 3.33503 avg_loss = 3.18804\n",
      "epoch no.5 train no.427840  loss = 2.79785 avg_loss = 3.20467\n",
      "epoch no.5 train no.427850  loss = 3.42647 avg_loss = 3.22281\n",
      "epoch no.5 train no.427860  loss = 3.32435 avg_loss = 3.22336\n",
      "epoch no.5 train no.427870  loss = 3.67351 avg_loss = 3.18680\n",
      "epoch no.5 train no.427880  loss = 3.23606 avg_loss = 3.20108\n",
      "epoch no.5 train no.427890  loss = 2.51842 avg_loss = 3.19279\n",
      "epoch no.5 train no.427900  loss = 3.84919 avg_loss = 3.15271\n",
      "epoch no.5 train no.427910  loss = 2.49989 avg_loss = 3.11043\n",
      "epoch no.5 train no.427920  loss = 2.75191 avg_loss = 3.08269\n",
      "epoch no.5 train no.427930  loss = 2.67491 avg_loss = 3.07430\n",
      "epoch no.5 train no.427940  loss = 3.18822 avg_loss = 3.12481\n",
      "epoch no.5 train no.427950  loss = 3.69830 avg_loss = 3.11788\n",
      "epoch no.5 train no.427960  loss = 3.90324 avg_loss = 3.14472\n",
      "epoch no.5 train no.427970  loss = 3.98812 avg_loss = 3.17718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.427980  loss = 3.37517 avg_loss = 3.19717\n",
      "epoch no.5 train no.427990  loss = 4.06626 avg_loss = 3.18630\n",
      "epoch no.5 train no.428000  loss = 3.16124 avg_loss = 3.17442\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '송', '▁모음', '곡', '</s>']\n",
      "추억의 팝송 명곡</s>\n",
      "epoch no.5 train no.428010  loss = 1.76274 avg_loss = 3.09318\n",
      "epoch no.5 train no.428020  loss = 2.62566 avg_loss = 3.13207\n",
      "epoch no.5 train no.428030  loss = 2.01899 avg_loss = 3.15020\n",
      "epoch no.5 train no.428040  loss = 4.37331 avg_loss = 3.16848\n",
      "epoch no.5 train no.428050  loss = 2.93953 avg_loss = 3.18901\n",
      "epoch no.5 train no.428060  loss = 1.94954 avg_loss = 3.18826\n",
      "epoch no.5 train no.428070  loss = 2.69524 avg_loss = 3.17346\n",
      "epoch no.5 train no.428080  loss = 1.83920 avg_loss = 3.16430\n",
      "epoch no.5 train no.428090  loss = 2.54662 avg_loss = 3.12764\n",
      "epoch no.5 train no.428100  loss = 3.22460 avg_loss = 3.14177\n",
      "epoch no.5 train no.428110  loss = 4.21349 avg_loss = 3.13327\n",
      "epoch no.5 train no.428120  loss = 3.72742 avg_loss = 3.16765\n",
      "epoch no.5 train no.428130  loss = 3.72744 avg_loss = 3.16166\n",
      "epoch no.5 train no.428140  loss = 3.51191 avg_loss = 3.17717\n",
      "epoch no.5 train no.428150  loss = 3.92672 avg_loss = 3.16899\n",
      "epoch no.5 train no.428160  loss = 1.98916 avg_loss = 3.19436\n",
      "epoch no.5 train no.428170  loss = 2.31013 avg_loss = 3.17274\n",
      "epoch no.5 train no.428180  loss = 1.75512 avg_loss = 3.17962\n",
      "epoch no.5 train no.428190  loss = 1.67258 avg_loss = 3.15788\n",
      "epoch no.5 train no.428200  loss = 2.29490 avg_loss = 3.14502\n",
      "epoch no.5 train no.428210  loss = 3.18545 avg_loss = 3.13100\n",
      "epoch no.5 train no.428220  loss = 3.82645 avg_loss = 3.11383\n",
      "epoch no.5 train no.428230  loss = 4.43029 avg_loss = 3.12877\n",
      "epoch no.5 train no.428240  loss = 3.21855 avg_loss = 3.11863\n",
      "epoch no.5 train no.428250  loss = 2.36327 avg_loss = 3.10243\n",
      "epoch no.5 train no.428260  loss = 3.59033 avg_loss = 3.06777\n",
      "epoch no.5 train no.428270  loss = 2.24601 avg_loss = 3.05419\n",
      "epoch no.5 train no.428280  loss = 2.27790 avg_loss = 3.04469\n",
      "epoch no.5 train no.428290  loss = 2.19176 avg_loss = 3.03038\n",
      "epoch no.5 train no.428300  loss = 3.41421 avg_loss = 3.07765\n",
      "epoch no.5 train no.428310  loss = 4.70759 avg_loss = 3.06075\n",
      "epoch no.5 train no.428320  loss = 2.97954 avg_loss = 3.08829\n",
      "epoch no.5 train no.428330  loss = 3.93002 avg_loss = 3.09484\n",
      "epoch no.5 train no.428340  loss = 1.88408 avg_loss = 3.12236\n",
      "epoch no.5 train no.428350  loss = 2.21823 avg_loss = 3.09526\n",
      "epoch no.5 train no.428360  loss = 2.58104 avg_loss = 3.06751\n",
      "epoch no.5 train no.428370  loss = 3.01352 avg_loss = 3.06610\n",
      "epoch no.5 train no.428380  loss = 1.27897 avg_loss = 3.04284\n",
      "epoch no.5 train no.428390  loss = 3.28014 avg_loss = 3.05646\n",
      "epoch no.5 train no.428400  loss = 4.42340 avg_loss = 3.11398\n",
      "epoch no.5 train no.428410  loss = 2.06723 avg_loss = 3.11851\n",
      "epoch no.5 train no.428420  loss = 3.61891 avg_loss = 3.13561\n",
      "epoch no.5 train no.428430  loss = 3.06986 avg_loss = 3.15070\n",
      "epoch no.5 train no.428440  loss = 3.27357 avg_loss = 3.14555\n",
      "epoch no.5 train no.428450  loss = 2.55703 avg_loss = 3.10450\n",
      "epoch no.5 train no.428460  loss = 4.02972 avg_loss = 3.07240\n",
      "epoch no.5 train no.428470  loss = 2.29681 avg_loss = 3.03861\n",
      "epoch no.5 train no.428480  loss = 2.80225 avg_loss = 3.05255\n",
      "epoch no.5 train no.428490  loss = 3.29074 avg_loss = 3.03301\n",
      "epoch no.5 train no.428500  loss = 2.29120 avg_loss = 3.01062\n",
      "epoch no.5 train no.428510  loss = 2.60209 avg_loss = 3.04699\n",
      "epoch no.5 train no.428520  loss = 2.89645 avg_loss = 3.03701\n",
      "epoch no.5 train no.428530  loss = 4.47753 avg_loss = 3.06229\n",
      "epoch no.5 train no.428540  loss = 1.52029 avg_loss = 3.06092\n",
      "epoch no.5 train no.428550  loss = 2.89430 avg_loss = 3.07767\n",
      "epoch no.5 train no.428560  loss = 3.19963 avg_loss = 3.01083\n",
      "epoch no.5 train no.428570  loss = 3.86748 avg_loss = 2.95180\n",
      "epoch no.5 train no.428580  loss = 3.02586 avg_loss = 2.93592\n",
      "epoch no.5 train no.428590  loss = 3.32747 avg_loss = 2.93938\n",
      "epoch no.5 train no.428600  loss = 4.06549 avg_loss = 2.94723\n",
      "epoch no.5 train no.428610  loss = 2.32994 avg_loss = 2.90143\n",
      "epoch no.5 train no.428620  loss = 3.26955 avg_loss = 2.90113\n",
      "epoch no.5 train no.428630  loss = 2.21387 avg_loss = 2.91963\n",
      "epoch no.5 train no.428640  loss = 2.76869 avg_loss = 2.94889\n",
      "epoch no.5 train no.428650  loss = 2.82710 avg_loss = 2.92873\n",
      "epoch no.5 train no.428660  loss = 1.96301 avg_loss = 2.93893\n",
      "epoch no.5 train no.428670  loss = 4.10432 avg_loss = 2.95405\n",
      "epoch no.5 train no.428680  loss = 2.69517 avg_loss = 2.97732\n",
      "epoch no.5 train no.428690  loss = 3.64409 avg_loss = 3.03963\n",
      "epoch no.5 train no.428700  loss = 2.30573 avg_loss = 3.04089\n",
      "epoch no.5 train no.428710  loss = 3.88344 avg_loss = 3.06295\n",
      "epoch no.5 train no.428720  loss = 4.22099 avg_loss = 3.10213\n",
      "epoch no.5 train no.428730  loss = 2.71607 avg_loss = 3.08431\n",
      "epoch no.5 train no.428740  loss = 2.16013 avg_loss = 3.07309\n",
      "epoch no.5 train no.428750  loss = 2.82614 avg_loss = 3.08051\n",
      "epoch no.5 train no.428760  loss = 3.97800 avg_loss = 3.09581\n",
      "epoch no.5 train no.428770  loss = 2.76360 avg_loss = 3.07216\n",
      "epoch no.5 train no.428780  loss = 3.30470 avg_loss = 3.05436\n",
      "epoch no.5 train no.428790  loss = 2.89909 avg_loss = 3.07021\n",
      "epoch no.5 train no.428800  loss = 2.75881 avg_loss = 3.04630\n",
      "epoch no.5 train no.428810  loss = 3.17232 avg_loss = 3.05225\n",
      "epoch no.5 train no.428820  loss = 2.19993 avg_loss = 3.04129\n",
      "epoch no.5 train no.428830  loss = 2.61211 avg_loss = 3.03278\n",
      "epoch no.5 train no.428840  loss = 5.10637 avg_loss = 3.03533\n",
      "epoch no.5 train no.428850  loss = 2.14894 avg_loss = 3.03178\n",
      "epoch no.5 train no.428860  loss = 2.91683 avg_loss = 3.05868\n",
      "epoch no.5 train no.428870  loss = 4.41611 avg_loss = 3.07254\n",
      "epoch no.5 train no.428880  loss = 3.68435 avg_loss = 3.09105\n",
      "epoch no.5 train no.428890  loss = 2.70886 avg_loss = 3.09096\n",
      "epoch no.5 train no.428900  loss = 4.21242 avg_loss = 3.07994\n",
      "epoch no.5 train no.428910  loss = 2.65107 avg_loss = 3.11810\n",
      "epoch no.5 train no.428920  loss = 3.93732 avg_loss = 3.11237\n",
      "epoch no.5 train no.428930  loss = 5.51333 avg_loss = 3.14286\n",
      "epoch no.5 train no.428940  loss = 2.55316 avg_loss = 3.14883\n",
      "epoch no.5 train no.428950  loss = 3.16864 avg_loss = 3.13829\n",
      "epoch no.5 train no.428960  loss = 3.88107 avg_loss = 3.15797\n",
      "epoch no.5 train no.428970  loss = 3.14494 avg_loss = 3.15511\n",
      "epoch no.5 train no.428980  loss = 4.04343 avg_loss = 3.13656\n",
      "epoch no.5 train no.428990  loss = 3.73583 avg_loss = 3.12175\n",
      "epoch no.5 train no.429000  loss = 3.59599 avg_loss = 3.14225\n",
      "5\n",
      "to_tokens: ['▁가을', '▁싸이', '월드', '▁b', 'g', 'm', '</s>']\n",
      "추억의 싸이월드 bgm</s>\n",
      "epoch no.5 train no.429010  loss = 3.20374 avg_loss = 3.16752\n",
      "epoch no.5 train no.429020  loss = 2.40897 avg_loss = 3.17796\n",
      "epoch no.5 train no.429030  loss = 4.44363 avg_loss = 3.16297\n",
      "epoch no.5 train no.429040  loss = 2.73875 avg_loss = 3.13576\n",
      "epoch no.5 train no.429050  loss = 3.53177 avg_loss = 3.12162\n",
      "epoch no.5 train no.429060  loss = 2.59892 avg_loss = 3.10698\n",
      "epoch no.5 train no.429070  loss = 2.45222 avg_loss = 3.10066\n",
      "epoch no.5 train no.429080  loss = 5.93569 avg_loss = 3.09950\n",
      "epoch no.5 train no.429090  loss = 3.06530 avg_loss = 3.11925\n",
      "epoch no.5 train no.429100  loss = 4.79064 avg_loss = 3.22875\n",
      "epoch no.5 train no.429110  loss = 3.64909 avg_loss = 3.20374\n",
      "epoch no.5 train no.429120  loss = 3.79286 avg_loss = 3.17013\n",
      "epoch no.5 train no.429130  loss = 3.15653 avg_loss = 3.15775\n",
      "epoch no.5 train no.429140  loss = 4.91583 avg_loss = 3.15223\n",
      "epoch no.5 train no.429150  loss = 2.97581 avg_loss = 3.16413\n",
      "epoch no.5 train no.429160  loss = 3.97935 avg_loss = 3.14213\n",
      "epoch no.5 train no.429170  loss = 4.28892 avg_loss = 3.17255\n",
      "epoch no.5 train no.429180  loss = 3.52391 avg_loss = 3.17378\n",
      "epoch no.5 train no.429190  loss = 2.76619 avg_loss = 3.19924\n",
      "epoch no.5 train no.429200  loss = 2.99641 avg_loss = 3.22828\n",
      "epoch no.5 train no.429210  loss = 2.29677 avg_loss = 3.21073\n",
      "epoch no.5 train no.429220  loss = 1.93026 avg_loss = 3.20795\n",
      "epoch no.5 train no.429230  loss = 1.71275 avg_loss = 3.19024\n",
      "epoch no.5 train no.429240  loss = 3.36804 avg_loss = 3.23091\n",
      "epoch no.5 train no.429250  loss = 2.27186 avg_loss = 3.19695\n",
      "epoch no.5 train no.429260  loss = 2.54116 avg_loss = 3.19540\n",
      "epoch no.5 train no.429270  loss = 3.96515 avg_loss = 3.20622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.429280  loss = 3.75157 avg_loss = 3.21485\n",
      "epoch no.5 train no.429290  loss = 3.48288 avg_loss = 3.17959\n",
      "epoch no.5 train no.429300  loss = 3.23682 avg_loss = 3.18755\n",
      "epoch no.5 train no.429310  loss = 1.90473 avg_loss = 3.16488\n",
      "epoch no.5 train no.429320  loss = 2.34078 avg_loss = 3.13781\n",
      "epoch no.5 train no.429330  loss = 3.64548 avg_loss = 3.15018\n",
      "epoch no.5 train no.429340  loss = 2.16051 avg_loss = 3.15895\n",
      "epoch no.5 train no.429350  loss = 2.68632 avg_loss = 3.12635\n",
      "epoch no.5 train no.429360  loss = 3.72881 avg_loss = 3.16481\n",
      "epoch no.5 train no.429370  loss = 2.83068 avg_loss = 3.16321\n",
      "epoch no.5 train no.429380  loss = 2.85505 avg_loss = 3.17577\n",
      "epoch no.5 train no.429390  loss = 3.02981 avg_loss = 3.21834\n",
      "epoch no.5 train no.429400  loss = 2.02345 avg_loss = 3.22506\n",
      "epoch no.5 train no.429410  loss = 2.40492 avg_loss = 3.24896\n",
      "epoch no.5 train no.429420  loss = 3.35787 avg_loss = 3.24958\n",
      "epoch no.5 train no.429430  loss = 3.45569 avg_loss = 3.22441\n",
      "epoch no.5 train no.429440  loss = 2.41916 avg_loss = 3.18304\n",
      "epoch no.5 train no.429450  loss = 4.06849 avg_loss = 3.14009\n",
      "epoch no.5 train no.429460  loss = 2.81698 avg_loss = 3.16672\n",
      "epoch no.5 train no.429470  loss = 2.10689 avg_loss = 3.16964\n",
      "epoch no.5 train no.429480  loss = 2.30644 avg_loss = 3.11709\n",
      "epoch no.5 train no.429490  loss = 2.01832 avg_loss = 3.06140\n",
      "epoch no.5 train no.429500  loss = 3.34887 avg_loss = 3.05272\n",
      "epoch no.5 train no.429510  loss = 3.45464 avg_loss = 3.03660\n",
      "epoch no.5 train no.429520  loss = 3.45629 avg_loss = 3.07179\n",
      "epoch no.5 train no.429530  loss = 2.85014 avg_loss = 3.05764\n",
      "epoch no.5 train no.429540  loss = 3.64708 avg_loss = 3.05047\n",
      "epoch no.5 train no.429550  loss = 3.37715 avg_loss = 3.06282\n",
      "epoch no.5 train no.429560  loss = 2.81277 avg_loss = 3.08050\n",
      "epoch no.5 train no.429570  loss = 2.58886 avg_loss = 3.11814\n",
      "epoch no.5 train no.429580  loss = 5.07712 avg_loss = 3.17030\n",
      "epoch no.5 train no.429590  loss = 2.38526 avg_loss = 3.17934\n",
      "epoch no.5 train no.429600  loss = 2.25714 avg_loss = 3.16021\n",
      "epoch no.5 train no.429610  loss = 1.84368 avg_loss = 3.12562\n",
      "epoch no.5 train no.429620  loss = 2.36082 avg_loss = 3.10627\n",
      "epoch no.5 train no.429630  loss = 3.22831 avg_loss = 3.08996\n",
      "epoch no.5 train no.429640  loss = 2.34433 avg_loss = 3.06331\n",
      "epoch no.5 train no.429650  loss = 3.42728 avg_loss = 3.07881\n",
      "epoch no.5 train no.429660  loss = 3.33670 avg_loss = 3.10226\n",
      "epoch no.5 train no.429670  loss = 2.40622 avg_loss = 3.09150\n",
      "epoch no.5 train no.429680  loss = 3.07869 avg_loss = 3.09296\n",
      "epoch no.5 train no.429690  loss = 3.29267 avg_loss = 3.09493\n",
      "epoch no.5 train no.429700  loss = 2.67782 avg_loss = 3.06826\n",
      "epoch no.5 train no.429710  loss = 2.62473 avg_loss = 3.04993\n",
      "epoch no.5 train no.429720  loss = 2.71694 avg_loss = 3.02565\n",
      "epoch no.5 train no.429730  loss = 3.13297 avg_loss = 2.99802\n",
      "epoch no.5 train no.429740  loss = 2.53007 avg_loss = 2.99916\n",
      "epoch no.5 train no.429750  loss = 2.86677 avg_loss = 3.03879\n",
      "epoch no.5 train no.429760  loss = 2.41090 avg_loss = 3.06807\n",
      "epoch no.5 train no.429770  loss = 1.37593 avg_loss = 3.06275\n",
      "epoch no.5 train no.429780  loss = 3.94317 avg_loss = 3.05125\n",
      "epoch no.5 train no.429790  loss = 3.46960 avg_loss = 3.02315\n",
      "epoch no.5 train no.429800  loss = 2.58527 avg_loss = 3.02433\n",
      "epoch no.5 train no.429810  loss = 2.29885 avg_loss = 3.04154\n",
      "epoch no.5 train no.429820  loss = 3.88200 avg_loss = 3.05524\n",
      "epoch no.5 train no.429830  loss = 2.98750 avg_loss = 3.08091\n",
      "epoch no.5 train no.429840  loss = 3.00552 avg_loss = 3.05159\n",
      "epoch no.5 train no.429850  loss = 3.41235 avg_loss = 3.05856\n",
      "epoch no.5 train no.429860  loss = 2.46697 avg_loss = 3.08185\n",
      "epoch no.5 train no.429870  loss = 3.12955 avg_loss = 3.07530\n",
      "epoch no.5 train no.429880  loss = 4.76634 avg_loss = 3.11876\n",
      "epoch no.5 train no.429890  loss = 3.46705 avg_loss = 3.11098\n",
      "epoch no.5 train no.429900  loss = 3.25331 avg_loss = 3.10439\n",
      "epoch no.5 train no.429910  loss = 2.68368 avg_loss = 3.11615\n",
      "epoch no.5 train no.429920  loss = 4.21757 avg_loss = 3.09710\n",
      "epoch no.5 train no.429930  loss = 2.38102 avg_loss = 3.06541\n",
      "epoch no.5 train no.429940  loss = 1.90321 avg_loss = 3.07414\n",
      "epoch no.5 train no.429950  loss = 2.75018 avg_loss = 3.07606\n",
      "epoch no.5 train no.429960  loss = 2.38152 avg_loss = 3.04509\n",
      "epoch no.5 train no.429970  loss = 2.64189 avg_loss = 3.04690\n",
      "epoch no.5 train no.429980  loss = 2.91611 avg_loss = 3.04989\n",
      "epoch no.5 train no.429990  loss = 1.84895 avg_loss = 3.06023\n",
      "epoch no.5 train no.430000  loss = 2.08906 avg_loss = 3.05203\n",
      "4\n",
      "to_tokens: ['▁가을', '▁싸이', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.5 train no.430010  loss = 4.51011 avg_loss = 3.07789\n",
      "epoch no.5 train no.430020  loss = 2.56000 avg_loss = 3.13412\n",
      "epoch no.5 train no.430030  loss = 2.89871 avg_loss = 3.16141\n",
      "epoch no.5 train no.430040  loss = 4.39499 avg_loss = 3.13987\n",
      "epoch no.5 train no.430050  loss = 1.90178 avg_loss = 3.17614\n",
      "epoch no.5 train no.430060  loss = 2.73663 avg_loss = 3.17165\n",
      "epoch no.5 train no.430070  loss = 3.36715 avg_loss = 3.18173\n",
      "epoch no.5 train no.430080  loss = 4.40692 avg_loss = 3.16542\n",
      "epoch no.5 train no.430090  loss = 2.18296 avg_loss = 3.11856\n",
      "epoch no.5 train no.430100  loss = 4.60962 avg_loss = 3.15218\n",
      "epoch no.5 train no.430110  loss = 2.81192 avg_loss = 3.14104\n",
      "epoch no.5 train no.430120  loss = 3.13231 avg_loss = 3.12756\n",
      "epoch no.5 train no.430130  loss = 2.20949 avg_loss = 3.09292\n",
      "epoch no.5 train no.430140  loss = 3.23813 avg_loss = 3.09285\n",
      "epoch no.5 train no.430150  loss = 2.25390 avg_loss = 3.06406\n",
      "epoch no.5 train no.430160  loss = 5.57743 avg_loss = 3.03556\n",
      "epoch no.5 train no.430170  loss = 2.79080 avg_loss = 3.06762\n",
      "epoch no.5 train no.430180  loss = 2.23003 avg_loss = 3.07011\n",
      "epoch no.5 train no.430190  loss = 3.77773 avg_loss = 3.04769\n",
      "epoch no.5 train no.430200  loss = 4.08683 avg_loss = 3.08922\n",
      "epoch no.5 train no.430210  loss = 2.34156 avg_loss = 3.08099\n",
      "epoch no.5 train no.430220  loss = 3.81082 avg_loss = 3.09021\n",
      "epoch no.5 train no.430230  loss = 3.13465 avg_loss = 3.08914\n",
      "epoch no.5 train no.430240  loss = 3.78993 avg_loss = 3.08563\n",
      "epoch no.5 train no.430250  loss = 4.22944 avg_loss = 3.05963\n",
      "epoch no.5 train no.430260  loss = 3.92887 avg_loss = 3.08409\n",
      "epoch no.5 train no.430270  loss = 2.45708 avg_loss = 3.08149\n",
      "epoch no.5 train no.430280  loss = 3.96831 avg_loss = 3.12709\n",
      "epoch no.5 train no.430290  loss = 3.51694 avg_loss = 3.11315\n",
      "epoch no.5 train no.430300  loss = 3.34947 avg_loss = 3.17776\n",
      "epoch no.5 train no.430310  loss = 3.46947 avg_loss = 3.21862\n",
      "epoch no.5 train no.430320  loss = 2.37887 avg_loss = 3.19966\n",
      "epoch no.5 train no.430330  loss = 3.28359 avg_loss = 3.22488\n",
      "epoch no.5 train no.430340  loss = 3.13313 avg_loss = 3.21811\n",
      "epoch no.5 train no.430350  loss = 2.32098 avg_loss = 3.16195\n",
      "epoch no.5 train no.430360  loss = 3.04374 avg_loss = 3.14778\n",
      "epoch no.5 train no.430370  loss = 4.18397 avg_loss = 3.11655\n",
      "epoch no.5 train no.430380  loss = 3.26686 avg_loss = 3.14649\n",
      "epoch no.5 train no.430390  loss = 3.13155 avg_loss = 3.12887\n",
      "epoch no.5 train no.430400  loss = 2.95718 avg_loss = 3.14133\n",
      "epoch no.5 train no.430410  loss = 3.31240 avg_loss = 3.13836\n",
      "epoch no.5 train no.430420  loss = 3.94160 avg_loss = 3.12289\n",
      "epoch no.5 train no.430430  loss = 3.20907 avg_loss = 3.13438\n",
      "epoch no.5 train no.430440  loss = 2.95935 avg_loss = 3.12668\n",
      "epoch no.5 train no.430450  loss = 3.02583 avg_loss = 3.13323\n",
      "epoch no.5 train no.430460  loss = 2.62317 avg_loss = 3.10120\n",
      "epoch no.5 train no.430470  loss = 3.39529 avg_loss = 3.06784\n",
      "epoch no.5 train no.430480  loss = 2.63205 avg_loss = 3.06860\n",
      "epoch no.5 train no.430490  loss = 4.80833 avg_loss = 3.11135\n",
      "epoch no.5 train no.430500  loss = 3.21883 avg_loss = 3.12794\n",
      "epoch no.5 train no.430510  loss = 3.81955 avg_loss = 3.14159\n",
      "epoch no.5 train no.430520  loss = 2.65046 avg_loss = 3.11915\n",
      "epoch no.5 train no.430530  loss = 2.63216 avg_loss = 3.08914\n",
      "epoch no.5 train no.430540  loss = 2.81879 avg_loss = 3.08054\n",
      "epoch no.5 train no.430550  loss = 3.53208 avg_loss = 3.08502\n",
      "epoch no.5 train no.430560  loss = 2.17781 avg_loss = 3.11897\n",
      "epoch no.5 train no.430570  loss = 2.90908 avg_loss = 3.08486\n",
      "epoch no.5 train no.430580  loss = 2.85372 avg_loss = 3.10647\n",
      "epoch no.5 train no.430590  loss = 4.27654 avg_loss = 3.14077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.430600  loss = 3.39898 avg_loss = 3.13037\n",
      "epoch no.5 train no.430610  loss = 4.06789 avg_loss = 3.11486\n",
      "epoch no.5 train no.430620  loss = 3.20761 avg_loss = 3.12418\n",
      "epoch no.5 train no.430630  loss = 2.45125 avg_loss = 3.10856\n",
      "epoch no.5 train no.430640  loss = 3.67498 avg_loss = 3.11542\n",
      "epoch no.5 train no.430650  loss = 3.72762 avg_loss = 3.09208\n",
      "epoch no.5 train no.430660  loss = 2.45269 avg_loss = 3.06329\n",
      "epoch no.5 train no.430670  loss = 3.17997 avg_loss = 3.05955\n",
      "epoch no.5 train no.430680  loss = 2.30301 avg_loss = 3.03304\n",
      "epoch no.5 train no.430690  loss = 4.20457 avg_loss = 3.05993\n",
      "epoch no.5 train no.430700  loss = 2.78267 avg_loss = 3.09485\n",
      "epoch no.5 train no.430710  loss = 2.43339 avg_loss = 3.08648\n",
      "epoch no.5 train no.430720  loss = 3.26506 avg_loss = 3.12537\n",
      "epoch no.5 train no.430730  loss = 3.12872 avg_loss = 3.10939\n",
      "epoch no.5 train no.430740  loss = 2.37275 avg_loss = 3.09150\n",
      "epoch no.5 train no.430750  loss = 2.62201 avg_loss = 3.12300\n",
      "epoch no.5 train no.430760  loss = 3.45324 avg_loss = 3.07971\n",
      "epoch no.5 train no.430770  loss = 2.05386 avg_loss = 3.09186\n",
      "epoch no.5 train no.430780  loss = 2.48762 avg_loss = 3.08795\n",
      "epoch no.5 train no.430790  loss = 2.30793 avg_loss = 3.07175\n",
      "epoch no.5 train no.430800  loss = 4.51637 avg_loss = 3.09361\n",
      "epoch no.5 train no.430810  loss = 3.04642 avg_loss = 3.09791\n",
      "epoch no.5 train no.430820  loss = 2.78657 avg_loss = 3.09044\n",
      "epoch no.5 train no.430830  loss = 2.36084 avg_loss = 3.11777\n",
      "epoch no.5 train no.430840  loss = 3.38261 avg_loss = 3.08175\n",
      "epoch no.5 train no.430850  loss = 1.62901 avg_loss = 3.08773\n",
      "epoch no.5 train no.430860  loss = 2.02190 avg_loss = 3.08911\n",
      "epoch no.5 train no.430870  loss = 4.58659 avg_loss = 3.12727\n",
      "epoch no.5 train no.430880  loss = 5.14979 avg_loss = 3.14964\n",
      "epoch no.5 train no.430890  loss = 4.60080 avg_loss = 3.19581\n",
      "epoch no.5 train no.430900  loss = 4.47333 avg_loss = 3.19408\n",
      "epoch no.5 train no.430910  loss = 3.77433 avg_loss = 3.20284\n",
      "epoch no.5 train no.430920  loss = 2.36854 avg_loss = 3.19090\n",
      "epoch no.5 train no.430930  loss = 3.24919 avg_loss = 3.22341\n",
      "epoch no.5 train no.430940  loss = 2.92929 avg_loss = 3.18767\n",
      "epoch no.5 train no.430950  loss = 2.53355 avg_loss = 3.15551\n",
      "epoch no.5 train no.430960  loss = 3.27695 avg_loss = 3.17417\n",
      "epoch no.5 train no.430970  loss = 4.51112 avg_loss = 3.19693\n",
      "epoch no.5 train no.430980  loss = 1.57251 avg_loss = 3.19870\n",
      "epoch no.5 train no.430990  loss = 2.41848 avg_loss = 3.17599\n",
      "epoch no.5 train no.431000  loss = 3.37092 avg_loss = 3.18441\n",
      "6\n",
      "to_tokens: ['▁가을', '▁명', '년대', '▁발라', '▁발라', '▁노래', '곡', '</s>']\n",
      "추억의 2000년대 여자아이돌 댄스곡</s>\n",
      "epoch no.5 train no.431010  loss = 2.82014 avg_loss = 3.16521\n",
      "epoch no.5 train no.431020  loss = 3.91997 avg_loss = 3.15751\n",
      "epoch no.5 train no.431030  loss = 2.50399 avg_loss = 3.15286\n",
      "epoch no.5 train no.431040  loss = 2.46717 avg_loss = 3.16896\n",
      "epoch no.5 train no.431050  loss = 1.75606 avg_loss = 3.13874\n",
      "epoch no.5 train no.431060  loss = 2.37299 avg_loss = 3.09210\n",
      "epoch no.5 train no.431070  loss = 2.23833 avg_loss = 3.07708\n",
      "epoch no.5 train no.431080  loss = 2.82621 avg_loss = 3.10415\n",
      "epoch no.5 train no.431090  loss = 4.07100 avg_loss = 3.11303\n",
      "epoch no.5 train no.431100  loss = 3.74232 avg_loss = 3.09184\n",
      "epoch no.5 train no.431110  loss = 3.18380 avg_loss = 3.11039\n",
      "epoch no.5 train no.431120  loss = 2.67793 avg_loss = 3.11251\n",
      "epoch no.5 train no.431130  loss = 3.56102 avg_loss = 3.16045\n",
      "epoch no.5 train no.431140  loss = 2.48814 avg_loss = 3.16844\n",
      "epoch no.5 train no.431150  loss = 3.23265 avg_loss = 3.15693\n",
      "epoch no.5 train no.431160  loss = 3.30677 avg_loss = 3.24423\n",
      "epoch no.5 train no.431170  loss = 2.81629 avg_loss = 3.24520\n",
      "epoch no.5 train no.431180  loss = 2.83995 avg_loss = 3.26357\n",
      "epoch no.5 train no.431190  loss = 4.25661 avg_loss = 3.27080\n",
      "epoch no.5 train no.431200  loss = 4.54345 avg_loss = 3.27053\n",
      "epoch no.5 train no.431210  loss = 2.90861 avg_loss = 3.29540\n",
      "epoch no.5 train no.431220  loss = 2.79174 avg_loss = 3.25301\n",
      "epoch no.5 train no.431230  loss = 2.07274 avg_loss = 3.24110\n",
      "epoch no.5 train no.431240  loss = 2.49098 avg_loss = 3.22186\n",
      "epoch no.5 train no.431250  loss = 2.15280 avg_loss = 3.18718\n",
      "epoch no.5 train no.431260  loss = 2.40089 avg_loss = 3.18378\n",
      "epoch no.5 train no.431270  loss = 4.82403 avg_loss = 3.21146\n",
      "epoch no.5 train no.431280  loss = 2.57581 avg_loss = 3.21659\n",
      "epoch no.5 train no.431290  loss = 2.86752 avg_loss = 3.18374\n",
      "epoch no.5 train no.431300  loss = 4.34708 avg_loss = 3.16803\n",
      "epoch no.5 train no.431310  loss = 1.91707 avg_loss = 3.14500\n",
      "epoch no.5 train no.431320  loss = 3.11619 avg_loss = 3.12300\n",
      "epoch no.5 train no.431330  loss = 4.82530 avg_loss = 3.13482\n",
      "epoch no.5 train no.431340  loss = 3.97249 avg_loss = 3.19841\n",
      "epoch no.5 train no.431350  loss = 3.42236 avg_loss = 3.14957\n",
      "epoch no.5 train no.431360  loss = 2.99717 avg_loss = 3.13574\n",
      "epoch no.5 train no.431370  loss = 3.74379 avg_loss = 3.21599\n",
      "epoch no.5 train no.431380  loss = 2.86208 avg_loss = 3.21945\n",
      "epoch no.5 train no.431390  loss = 4.10337 avg_loss = 3.20975\n",
      "epoch no.5 train no.431400  loss = 1.96741 avg_loss = 3.20657\n",
      "epoch no.5 train no.431410  loss = 3.24781 avg_loss = 3.20512\n",
      "epoch no.5 train no.431420  loss = 4.22527 avg_loss = 3.16852\n",
      "epoch no.5 train no.431430  loss = 2.78521 avg_loss = 3.18889\n",
      "epoch no.5 train no.431440  loss = 5.14060 avg_loss = 3.22409\n",
      "epoch no.5 train no.431450  loss = 2.74331 avg_loss = 3.17351\n",
      "epoch no.5 train no.431460  loss = 2.82133 avg_loss = 3.15916\n",
      "epoch no.5 train no.431470  loss = 2.17777 avg_loss = 3.15021\n",
      "epoch no.5 train no.431480  loss = 1.92371 avg_loss = 3.14470\n",
      "epoch no.5 train no.431490  loss = 2.49229 avg_loss = 3.13443\n",
      "epoch no.5 train no.431500  loss = 3.32266 avg_loss = 3.12531\n",
      "epoch no.5 train no.431510  loss = 3.07164 avg_loss = 3.11200\n",
      "epoch no.5 train no.431520  loss = 2.58406 avg_loss = 3.10653\n",
      "epoch no.5 train no.431530  loss = 2.99993 avg_loss = 3.09105\n",
      "epoch no.5 train no.431540  loss = 4.29014 avg_loss = 3.10699\n",
      "epoch no.5 train no.431550  loss = 3.42776 avg_loss = 3.14011\n",
      "epoch no.5 train no.431560  loss = 2.96452 avg_loss = 3.10573\n",
      "epoch no.5 train no.431570  loss = 1.44889 avg_loss = 3.09649\n",
      "epoch no.5 train no.431580  loss = 2.15393 avg_loss = 3.08913\n",
      "epoch no.5 train no.431590  loss = 3.43763 avg_loss = 3.10072\n",
      "epoch no.5 train no.431600  loss = 1.56226 avg_loss = 3.09006\n",
      "epoch no.5 train no.431610  loss = 2.24898 avg_loss = 3.06636\n",
      "epoch no.5 train no.431620  loss = 2.99572 avg_loss = 3.08017\n",
      "epoch no.5 train no.431630  loss = 2.45888 avg_loss = 3.06699\n",
      "epoch no.5 train no.431640  loss = 2.54430 avg_loss = 3.05336\n",
      "epoch no.5 train no.431650  loss = 2.23471 avg_loss = 3.01871\n",
      "epoch no.5 train no.431660  loss = 4.76564 avg_loss = 3.07320\n",
      "epoch no.5 train no.431670  loss = 2.86898 avg_loss = 3.05447\n",
      "epoch no.5 train no.431680  loss = 3.39852 avg_loss = 3.09534\n",
      "epoch no.5 train no.431690  loss = 3.75715 avg_loss = 3.10661\n",
      "epoch no.5 train no.431700  loss = 2.83468 avg_loss = 3.10106\n",
      "epoch no.5 train no.431710  loss = 2.46387 avg_loss = 3.08694\n",
      "epoch no.5 train no.431720  loss = 3.68778 avg_loss = 3.11367\n",
      "epoch no.5 train no.431730  loss = 3.50726 avg_loss = 3.06934\n",
      "epoch no.5 train no.431740  loss = 5.37098 avg_loss = 3.10524\n",
      "epoch no.5 train no.431750  loss = 2.33190 avg_loss = 3.08416\n",
      "epoch no.5 train no.431760  loss = 3.55412 avg_loss = 3.11898\n",
      "epoch no.5 train no.431770  loss = 2.23575 avg_loss = 3.14022\n",
      "epoch no.5 train no.431780  loss = 3.62648 avg_loss = 3.15165\n",
      "epoch no.5 train no.431790  loss = 4.43260 avg_loss = 3.17154\n",
      "epoch no.5 train no.431800  loss = 2.87406 avg_loss = 3.14383\n",
      "epoch no.5 train no.431810  loss = 3.61828 avg_loss = 3.13919\n",
      "epoch no.5 train no.431820  loss = 1.29356 avg_loss = 3.12486\n",
      "epoch no.5 train no.431830  loss = 3.02384 avg_loss = 3.16690\n",
      "epoch no.5 train no.431840  loss = 2.06131 avg_loss = 3.15793\n",
      "epoch no.5 train no.431850  loss = 1.98835 avg_loss = 3.15465\n",
      "epoch no.5 train no.431860  loss = 2.48818 avg_loss = 3.19253\n",
      "epoch no.5 train no.431870  loss = 2.56988 avg_loss = 3.16087\n",
      "epoch no.5 train no.431880  loss = 2.04627 avg_loss = 3.14427\n",
      "epoch no.5 train no.431890  loss = 2.67772 avg_loss = 3.16852\n",
      "epoch no.5 train no.431900  loss = 4.00671 avg_loss = 3.17196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.431910  loss = 2.07910 avg_loss = 3.18640\n",
      "epoch no.5 train no.431920  loss = 2.43471 avg_loss = 3.17492\n",
      "epoch no.5 train no.431930  loss = 4.72455 avg_loss = 3.21092\n",
      "epoch no.5 train no.431940  loss = 3.10414 avg_loss = 3.26436\n",
      "epoch no.5 train no.431950  loss = 3.08936 avg_loss = 3.24941\n",
      "epoch no.5 train no.431960  loss = 3.98343 avg_loss = 3.27395\n",
      "epoch no.5 train no.431970  loss = 4.13428 avg_loss = 3.25509\n",
      "epoch no.5 train no.431980  loss = 2.04417 avg_loss = 3.23965\n",
      "epoch no.5 train no.431990  loss = 3.83716 avg_loss = 3.21148\n",
      "epoch no.5 train no.432000  loss = 2.18180 avg_loss = 3.19793\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.5 train no.432010  loss = 3.95720 avg_loss = 3.19081\n",
      "epoch no.5 train no.432020  loss = 5.00543 avg_loss = 3.20011\n",
      "epoch no.5 train no.432030  loss = 1.85132 avg_loss = 3.19028\n",
      "epoch no.5 train no.432040  loss = 1.98410 avg_loss = 3.18533\n",
      "epoch no.5 train no.432050  loss = 3.35981 avg_loss = 3.15362\n",
      "epoch no.5 train no.432060  loss = 5.23525 avg_loss = 3.18584\n",
      "epoch no.5 train no.432070  loss = 3.51830 avg_loss = 3.19323\n",
      "epoch no.5 train no.432080  loss = 1.95306 avg_loss = 3.15441\n",
      "epoch no.5 train no.432090  loss = 2.07631 avg_loss = 3.11571\n",
      "epoch no.5 train no.432100  loss = 2.00392 avg_loss = 3.10301\n",
      "epoch no.5 train no.432110  loss = 2.54377 avg_loss = 3.11782\n",
      "epoch no.5 train no.432120  loss = 3.61043 avg_loss = 3.12724\n",
      "epoch no.5 train no.432130  loss = 3.72430 avg_loss = 3.15418\n",
      "epoch no.5 train no.432140  loss = 2.17754 avg_loss = 3.17531\n",
      "epoch no.5 train no.432150  loss = 2.87086 avg_loss = 3.18242\n",
      "epoch no.5 train no.432160  loss = 1.80150 avg_loss = 3.16095\n",
      "epoch no.5 train no.432170  loss = 3.70884 avg_loss = 3.16791\n",
      "epoch no.5 train no.432180  loss = 2.45737 avg_loss = 3.15125\n",
      "epoch no.5 train no.432190  loss = 3.10329 avg_loss = 3.11786\n",
      "epoch no.5 train no.432200  loss = 2.44340 avg_loss = 3.15386\n",
      "epoch no.5 train no.432210  loss = 2.20651 avg_loss = 3.15746\n",
      "epoch no.5 train no.432220  loss = 3.05864 avg_loss = 3.16820\n",
      "epoch no.5 train no.432230  loss = 2.04400 avg_loss = 3.14331\n",
      "epoch no.5 train no.432240  loss = 3.16529 avg_loss = 3.12891\n",
      "epoch no.5 train no.432250  loss = 2.93185 avg_loss = 3.10727\n",
      "epoch no.5 train no.432260  loss = 4.92685 avg_loss = 3.14437\n",
      "epoch no.5 train no.432270  loss = 3.34403 avg_loss = 3.14193\n",
      "epoch no.5 train no.432280  loss = 2.52990 avg_loss = 3.17716\n",
      "epoch no.5 train no.432290  loss = 1.93360 avg_loss = 3.15366\n",
      "epoch no.5 train no.432300  loss = 2.10193 avg_loss = 3.14520\n",
      "epoch no.5 train no.432310  loss = 2.13152 avg_loss = 3.13621\n",
      "epoch no.5 train no.432320  loss = 2.94875 avg_loss = 3.09756\n",
      "epoch no.5 train no.432330  loss = 3.41971 avg_loss = 3.10023\n",
      "epoch no.5 train no.432340  loss = 4.16806 avg_loss = 3.15146\n",
      "epoch no.5 train no.432350  loss = 2.67569 avg_loss = 3.11404\n",
      "epoch no.5 train no.432360  loss = 4.97186 avg_loss = 3.12028\n",
      "epoch no.5 train no.432370  loss = 3.68149 avg_loss = 3.15454\n",
      "epoch no.5 train no.432380  loss = 3.23134 avg_loss = 3.11992\n",
      "epoch no.5 train no.432390  loss = 1.75265 avg_loss = 3.13171\n",
      "epoch no.5 train no.432400  loss = 3.48153 avg_loss = 3.12899\n",
      "epoch no.5 train no.432410  loss = 4.17552 avg_loss = 3.15346\n",
      "epoch no.5 train no.432420  loss = 3.69525 avg_loss = 3.13093\n",
      "epoch no.5 train no.432430  loss = 2.92523 avg_loss = 3.16203\n",
      "epoch no.5 train no.432440  loss = 3.28796 avg_loss = 3.16916\n",
      "epoch no.5 train no.432450  loss = 3.05535 avg_loss = 3.20420\n",
      "epoch no.5 train no.432460  loss = 2.90051 avg_loss = 3.19406\n",
      "epoch no.5 train no.432470  loss = 2.48684 avg_loss = 3.15859\n",
      "epoch no.5 train no.432480  loss = 2.51639 avg_loss = 3.16666\n",
      "epoch no.5 train no.432490  loss = 2.65641 avg_loss = 3.16167\n",
      "epoch no.5 train no.432500  loss = 3.67994 avg_loss = 3.16504\n",
      "epoch no.5 train no.432510  loss = 2.93912 avg_loss = 3.15405\n",
      "epoch no.5 train no.432520  loss = 2.29354 avg_loss = 3.14719\n",
      "epoch no.5 train no.432530  loss = 3.02334 avg_loss = 3.14229\n",
      "epoch no.5 train no.432540  loss = 3.58035 avg_loss = 3.14441\n",
      "epoch no.5 train no.432550  loss = 2.28986 avg_loss = 3.11872\n",
      "epoch no.5 train no.432560  loss = 3.43746 avg_loss = 3.13556\n",
      "epoch no.5 train no.432570  loss = 2.91577 avg_loss = 3.10193\n",
      "epoch no.5 train no.432580  loss = 2.78973 avg_loss = 3.09517\n",
      "epoch no.5 train no.432590  loss = 2.54117 avg_loss = 3.03942\n",
      "epoch no.5 train no.432600  loss = 3.21077 avg_loss = 3.03298\n",
      "epoch no.5 train no.432610  loss = 4.31174 avg_loss = 3.06662\n",
      "epoch no.5 train no.432620  loss = 2.36329 avg_loss = 3.01547\n",
      "epoch no.5 train no.432630  loss = 2.84440 avg_loss = 2.97845\n",
      "epoch no.5 train no.432640  loss = 3.37121 avg_loss = 3.01293\n",
      "epoch no.5 train no.432650  loss = 3.67927 avg_loss = 2.95148\n",
      "epoch no.5 train no.432660  loss = 2.33642 avg_loss = 2.94496\n",
      "epoch no.5 train no.432670  loss = 3.12272 avg_loss = 2.99697\n",
      "epoch no.5 train no.432680  loss = 1.96879 avg_loss = 2.97726\n",
      "epoch no.5 train no.432690  loss = 3.85729 avg_loss = 2.95746\n",
      "epoch no.5 train no.432700  loss = 5.14927 avg_loss = 2.97090\n",
      "epoch no.5 train no.432710  loss = 2.72819 avg_loss = 2.97839\n",
      "epoch no.5 train no.432720  loss = 3.03744 avg_loss = 3.01072\n",
      "epoch no.5 train no.432730  loss = 2.75797 avg_loss = 3.03075\n",
      "epoch no.5 train no.432740  loss = 3.32057 avg_loss = 3.04182\n",
      "epoch no.5 train no.432750  loss = 2.88185 avg_loss = 3.02969\n",
      "epoch no.5 train no.432760  loss = 3.17814 avg_loss = 3.05098\n",
      "epoch no.5 train no.432770  loss = 4.33743 avg_loss = 3.05616\n",
      "epoch no.5 train no.432780  loss = 3.47446 avg_loss = 3.10490\n",
      "epoch no.5 train no.432790  loss = 4.58523 avg_loss = 3.09370\n",
      "epoch no.5 train no.432800  loss = 3.72841 avg_loss = 3.06665\n",
      "epoch no.5 train no.432810  loss = 3.61043 avg_loss = 3.07206\n",
      "epoch no.5 train no.432820  loss = 2.66964 avg_loss = 3.11678\n",
      "epoch no.5 train no.432830  loss = 2.85267 avg_loss = 3.07979\n",
      "epoch no.5 train no.432840  loss = 3.45558 avg_loss = 3.06598\n",
      "epoch no.5 train no.432850  loss = 4.04824 avg_loss = 3.07670\n",
      "epoch no.5 train no.432860  loss = 3.66792 avg_loss = 3.07663\n",
      "epoch no.5 train no.432870  loss = 2.78394 avg_loss = 3.07336\n",
      "epoch no.5 train no.432880  loss = 3.22411 avg_loss = 3.03394\n",
      "epoch no.5 train no.432890  loss = 3.30363 avg_loss = 3.08156\n",
      "epoch no.5 train no.432900  loss = 3.76635 avg_loss = 3.15080\n",
      "epoch no.5 train no.432910  loss = 3.12413 avg_loss = 3.15393\n",
      "epoch no.5 train no.432920  loss = 2.85527 avg_loss = 3.17291\n",
      "epoch no.5 train no.432930  loss = 2.89638 avg_loss = 3.18301\n",
      "epoch no.5 train no.432940  loss = 2.24548 avg_loss = 3.17646\n",
      "epoch no.5 train no.432950  loss = 1.94321 avg_loss = 3.16028\n",
      "epoch no.5 train no.432960  loss = 2.97028 avg_loss = 3.17924\n",
      "epoch no.5 train no.432970  loss = 4.53087 avg_loss = 3.15416\n",
      "epoch no.5 train no.432980  loss = 2.02974 avg_loss = 3.11724\n",
      "epoch no.5 train no.432990  loss = 2.64422 avg_loss = 3.08940\n",
      "epoch no.5 train no.433000  loss = 1.68066 avg_loss = 3.12044\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.5 train no.433010  loss = 2.54057 avg_loss = 3.10078\n",
      "epoch no.5 train no.433020  loss = 2.37371 avg_loss = 3.08894\n",
      "epoch no.5 train no.433030  loss = 3.50681 avg_loss = 3.11537\n",
      "epoch no.5 train no.433040  loss = 2.51250 avg_loss = 3.06987\n",
      "epoch no.5 train no.433050  loss = 2.46472 avg_loss = 3.09525\n",
      "epoch no.5 train no.433060  loss = 3.34084 avg_loss = 3.10246\n",
      "epoch no.5 train no.433070  loss = 5.26344 avg_loss = 3.14320\n",
      "epoch no.5 train no.433080  loss = 2.88134 avg_loss = 3.18201\n",
      "epoch no.5 train no.433090  loss = 2.18082 avg_loss = 3.12790\n",
      "epoch no.5 train no.433100  loss = 3.60198 avg_loss = 3.12558\n",
      "epoch no.5 train no.433110  loss = 2.62887 avg_loss = 3.09992\n",
      "epoch no.5 train no.433120  loss = 2.36479 avg_loss = 3.11144\n",
      "epoch no.5 train no.433130  loss = 1.90766 avg_loss = 3.10032\n",
      "epoch no.5 train no.433140  loss = 2.23702 avg_loss = 3.08325\n",
      "epoch no.5 train no.433150  loss = 2.82003 avg_loss = 3.07222\n",
      "epoch no.5 train no.433160  loss = 2.87533 avg_loss = 3.07809\n",
      "epoch no.5 train no.433170  loss = 2.97952 avg_loss = 3.03660\n",
      "epoch no.5 train no.433180  loss = 3.54510 avg_loss = 3.07065\n",
      "epoch no.5 train no.433190  loss = 2.19673 avg_loss = 3.09923\n",
      "epoch no.5 train no.433200  loss = 5.52122 avg_loss = 3.10052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.433210  loss = 2.19069 avg_loss = 3.08315\n",
      "epoch no.5 train no.433220  loss = 2.59748 avg_loss = 3.07719\n",
      "epoch no.5 train no.433230  loss = 2.63402 avg_loss = 3.07793\n",
      "epoch no.5 train no.433240  loss = 5.98136 avg_loss = 3.08899\n",
      "epoch no.5 train no.433250  loss = 3.03378 avg_loss = 3.06171\n",
      "epoch no.5 train no.433260  loss = 2.58025 avg_loss = 3.10407\n",
      "epoch no.5 train no.433270  loss = 3.79923 avg_loss = 3.11053\n",
      "epoch no.5 train no.433280  loss = 2.56587 avg_loss = 3.11324\n",
      "epoch no.5 train no.433290  loss = 4.02445 avg_loss = 3.15490\n",
      "epoch no.5 train no.433300  loss = 3.18549 avg_loss = 3.07832\n",
      "epoch no.5 train no.433310  loss = 3.54309 avg_loss = 3.09184\n",
      "epoch no.5 train no.433320  loss = 3.71266 avg_loss = 3.11946\n",
      "epoch no.5 train no.433330  loss = 2.86045 avg_loss = 3.16403\n",
      "epoch no.5 train no.433340  loss = 2.83894 avg_loss = 3.14927\n",
      "epoch no.5 train no.433350  loss = 2.80441 avg_loss = 3.16767\n",
      "epoch no.5 train no.433360  loss = 2.52201 avg_loss = 3.16558\n",
      "epoch no.5 train no.433370  loss = 3.40460 avg_loss = 3.17527\n",
      "epoch no.5 train no.433380  loss = 2.35979 avg_loss = 3.15688\n",
      "epoch no.5 train no.433390  loss = 2.94929 avg_loss = 3.14155\n",
      "epoch no.5 train no.433400  loss = 4.10323 avg_loss = 3.16475\n",
      "epoch no.5 train no.433410  loss = 3.29313 avg_loss = 3.18525\n",
      "epoch no.5 train no.433420  loss = 2.17747 avg_loss = 3.14740\n",
      "epoch no.5 train no.433430  loss = 3.77932 avg_loss = 3.17605\n",
      "epoch no.5 train no.433440  loss = 2.10196 avg_loss = 3.19926\n",
      "epoch no.5 train no.433450  loss = 3.87323 avg_loss = 3.19752\n",
      "epoch no.5 train no.433460  loss = 3.56369 avg_loss = 3.18493\n",
      "epoch no.5 train no.433470  loss = 3.31298 avg_loss = 3.18251\n",
      "epoch no.5 train no.433480  loss = 3.99094 avg_loss = 3.17741\n",
      "epoch no.5 train no.433490  loss = 2.33685 avg_loss = 3.15511\n",
      "epoch no.5 train no.433500  loss = 3.24950 avg_loss = 3.18650\n",
      "epoch no.5 train no.433510  loss = 2.98572 avg_loss = 3.14256\n",
      "epoch no.5 train no.433520  loss = 3.63916 avg_loss = 3.17969\n",
      "epoch no.5 train no.433530  loss = 4.23981 avg_loss = 3.15076\n",
      "epoch no.5 train no.433540  loss = 2.48990 avg_loss = 3.14800\n",
      "epoch no.5 train no.433550  loss = 2.66183 avg_loss = 3.17086\n",
      "epoch no.5 train no.433560  loss = 3.39179 avg_loss = 3.16355\n",
      "epoch no.5 train no.433570  loss = 2.95874 avg_loss = 3.15497\n",
      "epoch no.5 train no.433580  loss = 4.53650 avg_loss = 3.17388\n",
      "epoch no.5 train no.433590  loss = 3.48510 avg_loss = 3.23936\n",
      "epoch no.5 train no.433600  loss = 4.47059 avg_loss = 3.23479\n",
      "epoch no.5 train no.433610  loss = 1.66066 avg_loss = 3.25373\n",
      "epoch no.5 train no.433620  loss = 3.86896 avg_loss = 3.22357\n",
      "epoch no.5 train no.433630  loss = 1.70416 avg_loss = 3.20680\n",
      "epoch no.5 train no.433640  loss = 2.98736 avg_loss = 3.16092\n",
      "epoch no.5 train no.433650  loss = 3.23001 avg_loss = 3.19343\n",
      "epoch no.5 train no.433660  loss = 1.93413 avg_loss = 3.19600\n",
      "epoch no.5 train no.433670  loss = 2.73519 avg_loss = 3.16765\n",
      "epoch no.5 train no.433680  loss = 2.11249 avg_loss = 3.17167\n",
      "epoch no.5 train no.433690  loss = 1.98112 avg_loss = 3.18387\n",
      "epoch no.5 train no.433700  loss = 4.29054 avg_loss = 3.19849\n",
      "epoch no.5 train no.433710  loss = 2.75933 avg_loss = 3.19661\n",
      "epoch no.5 train no.433720  loss = 1.96612 avg_loss = 3.18596\n",
      "epoch no.5 train no.433730  loss = 1.33014 avg_loss = 3.13407\n",
      "epoch no.5 train no.433740  loss = 3.52813 avg_loss = 3.16121\n",
      "epoch no.5 train no.433750  loss = 4.06121 avg_loss = 3.15182\n",
      "epoch no.5 train no.433760  loss = 3.50237 avg_loss = 3.16219\n",
      "epoch no.5 train no.433770  loss = 2.33032 avg_loss = 3.18063\n",
      "epoch no.5 train no.433780  loss = 3.70759 avg_loss = 3.17026\n",
      "epoch no.5 train no.433790  loss = 3.27784 avg_loss = 3.15738\n",
      "epoch no.5 train no.433800  loss = 4.06146 avg_loss = 3.12519\n",
      "epoch no.5 train no.433810  loss = 2.81296 avg_loss = 3.12293\n",
      "epoch no.5 train no.433820  loss = 2.87268 avg_loss = 3.14603\n",
      "epoch no.5 train no.433830  loss = 3.64310 avg_loss = 3.14151\n",
      "epoch no.5 train no.433840  loss = 3.41570 avg_loss = 3.12665\n",
      "epoch no.5 train no.433850  loss = 2.54652 avg_loss = 3.12276\n",
      "epoch no.5 train no.433860  loss = 2.22821 avg_loss = 3.11514\n",
      "epoch no.5 train no.433870  loss = 4.45981 avg_loss = 3.12950\n",
      "epoch no.5 train no.433880  loss = 3.79165 avg_loss = 3.14021\n",
      "epoch no.5 train no.433890  loss = 2.36665 avg_loss = 3.09310\n",
      "epoch no.5 train no.433900  loss = 2.82508 avg_loss = 3.08425\n",
      "epoch no.5 train no.433910  loss = 2.30747 avg_loss = 3.06623\n",
      "epoch no.5 train no.433920  loss = 2.92626 avg_loss = 3.05734\n",
      "epoch no.5 train no.433930  loss = 3.27890 avg_loss = 3.05438\n",
      "epoch no.5 train no.433940  loss = 2.87916 avg_loss = 3.13606\n",
      "epoch no.5 train no.433950  loss = 2.63566 avg_loss = 3.12779\n",
      "epoch no.5 train no.433960  loss = 2.60892 avg_loss = 3.12049\n",
      "epoch no.5 train no.433970  loss = 3.02836 avg_loss = 3.13339\n",
      "epoch no.5 train no.433980  loss = 3.80674 avg_loss = 3.13612\n",
      "epoch no.5 train no.433990  loss = 3.06812 avg_loss = 3.12845\n",
      "epoch no.5 train no.434000  loss = 3.70237 avg_loss = 3.11542\n",
      "5\n",
      "to_tokens: ['▁비', '▁노래', '80', '▁가요', '▁가요', '요', '</s>']\n",
      "추억의 7080년대 인기가요</s>\n",
      "epoch no.5 train no.434010  loss = 4.86534 avg_loss = 3.12521\n",
      "epoch no.5 train no.434020  loss = 3.27715 avg_loss = 3.09386\n",
      "epoch no.5 train no.434030  loss = 2.85795 avg_loss = 3.12624\n",
      "epoch no.5 train no.434040  loss = 1.89146 avg_loss = 3.13149\n",
      "epoch no.5 train no.434050  loss = 1.69704 avg_loss = 3.09795\n",
      "epoch no.5 train no.434060  loss = 1.81450 avg_loss = 3.04170\n",
      "epoch no.5 train no.434070  loss = 2.87318 avg_loss = 3.03588\n",
      "epoch no.5 train no.434080  loss = 4.08656 avg_loss = 3.02577\n",
      "epoch no.5 train no.434090  loss = 3.71191 avg_loss = 3.05085\n",
      "epoch no.5 train no.434100  loss = 3.79261 avg_loss = 3.04968\n",
      "epoch no.5 train no.434110  loss = 2.57514 avg_loss = 3.03057\n",
      "epoch no.5 train no.434120  loss = 2.86341 avg_loss = 3.05706\n",
      "epoch no.5 train no.434130  loss = 2.98811 avg_loss = 3.09511\n",
      "epoch no.5 train no.434140  loss = 3.50068 avg_loss = 3.08465\n",
      "epoch no.5 train no.434150  loss = 3.10821 avg_loss = 3.08575\n",
      "epoch no.5 train no.434160  loss = 2.11665 avg_loss = 3.05371\n",
      "epoch no.5 train no.434170  loss = 4.50124 avg_loss = 3.11886\n",
      "epoch no.5 train no.434180  loss = 2.37554 avg_loss = 3.15347\n",
      "epoch no.5 train no.434190  loss = 3.37873 avg_loss = 3.13281\n",
      "epoch no.5 train no.434200  loss = 2.23068 avg_loss = 3.12065\n",
      "epoch no.5 train no.434210  loss = 2.60255 avg_loss = 3.09315\n",
      "epoch no.5 train no.434220  loss = 4.77462 avg_loss = 3.12534\n",
      "epoch no.5 train no.434230  loss = 3.30978 avg_loss = 3.16449\n",
      "epoch no.5 train no.434240  loss = 2.78135 avg_loss = 3.16948\n",
      "epoch no.5 train no.434250  loss = 2.97457 avg_loss = 3.17857\n",
      "epoch no.5 train no.434260  loss = 3.37345 avg_loss = 3.23882\n",
      "epoch no.5 train no.434270  loss = 4.02418 avg_loss = 3.23771\n",
      "epoch no.5 train no.434280  loss = 2.10567 avg_loss = 3.19864\n",
      "epoch no.5 train no.434290  loss = 2.57758 avg_loss = 3.19149\n",
      "epoch no.5 train no.434300  loss = 4.05099 avg_loss = 3.15329\n",
      "epoch no.5 train no.434310  loss = 1.84844 avg_loss = 3.13614\n",
      "epoch no.5 train no.434320  loss = 2.08039 avg_loss = 3.12673\n",
      "epoch no.5 train no.434330  loss = 3.72714 avg_loss = 3.14368\n",
      "epoch no.5 train no.434340  loss = 2.88583 avg_loss = 3.11146\n",
      "epoch no.5 train no.434350  loss = 2.65632 avg_loss = 3.12394\n",
      "epoch no.5 train no.434360  loss = 3.69978 avg_loss = 3.10441\n",
      "epoch no.5 train no.434370  loss = 3.37413 avg_loss = 3.10768\n",
      "epoch no.5 train no.434380  loss = 2.99629 avg_loss = 3.13672\n",
      "epoch no.5 train no.434390  loss = 3.45220 avg_loss = 3.13523\n",
      "epoch no.5 train no.434400  loss = 5.27033 avg_loss = 3.17514\n",
      "epoch no.5 train no.434410  loss = 1.58390 avg_loss = 3.17607\n",
      "epoch no.5 train no.434420  loss = 2.15456 avg_loss = 3.15968\n",
      "epoch no.5 train no.434430  loss = 3.89137 avg_loss = 3.18776\n",
      "epoch no.5 train no.434440  loss = 3.38147 avg_loss = 3.16251\n",
      "epoch no.5 train no.434450  loss = 2.87440 avg_loss = 3.18246\n",
      "epoch no.5 train no.434460  loss = 3.50253 avg_loss = 3.19807\n",
      "epoch no.5 train no.434470  loss = 3.11485 avg_loss = 3.15902\n",
      "epoch no.5 train no.434480  loss = 2.09657 avg_loss = 3.10499\n",
      "epoch no.5 train no.434490  loss = 4.32811 avg_loss = 3.11700\n",
      "epoch no.5 train no.434500  loss = 2.64659 avg_loss = 3.09991\n",
      "epoch no.5 train no.434510  loss = 2.18314 avg_loss = 3.07188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.434520  loss = 4.20890 avg_loss = 3.06993\n",
      "epoch no.5 train no.434530  loss = 2.41137 avg_loss = 3.10367\n",
      "epoch no.5 train no.434540  loss = 2.95738 avg_loss = 3.13506\n",
      "epoch no.5 train no.434550  loss = 3.12201 avg_loss = 3.11424\n",
      "epoch no.5 train no.434560  loss = 1.87389 avg_loss = 3.05999\n",
      "epoch no.5 train no.434570  loss = 2.37976 avg_loss = 3.02838\n",
      "epoch no.5 train no.434580  loss = 2.67801 avg_loss = 2.99891\n",
      "epoch no.5 train no.434590  loss = 2.34895 avg_loss = 3.01348\n",
      "epoch no.5 train no.434600  loss = 4.58918 avg_loss = 3.04931\n",
      "epoch no.5 train no.434610  loss = 4.66567 avg_loss = 3.08379\n",
      "epoch no.5 train no.434620  loss = 2.67943 avg_loss = 3.07488\n",
      "epoch no.5 train no.434630  loss = 3.02035 avg_loss = 3.09765\n",
      "epoch no.5 train no.434640  loss = 2.29796 avg_loss = 3.12992\n",
      "epoch no.5 train no.434650  loss = 3.97837 avg_loss = 3.14689\n",
      "epoch no.5 train no.434660  loss = 2.22240 avg_loss = 3.13888\n",
      "epoch no.5 train no.434670  loss = 2.36074 avg_loss = 3.14919\n",
      "epoch no.5 train no.434680  loss = 4.22492 avg_loss = 3.17162\n",
      "epoch no.5 train no.434690  loss = 3.23571 avg_loss = 3.17700\n",
      "epoch no.5 train no.434700  loss = 5.88358 avg_loss = 3.20700\n",
      "epoch no.5 train no.434710  loss = 1.84427 avg_loss = 3.17108\n",
      "epoch no.5 train no.434720  loss = 2.51413 avg_loss = 3.16837\n",
      "epoch no.5 train no.434730  loss = 1.35388 avg_loss = 3.13705\n",
      "epoch no.5 train no.434740  loss = 2.75969 avg_loss = 3.13504\n",
      "epoch no.5 train no.434750  loss = 3.37953 avg_loss = 3.11253\n",
      "epoch no.5 train no.434760  loss = 3.22311 avg_loss = 3.17092\n",
      "epoch no.5 train no.434770  loss = 4.06210 avg_loss = 3.16107\n",
      "epoch no.5 train no.434780  loss = 3.75080 avg_loss = 3.10665\n",
      "epoch no.5 train no.434790  loss = 3.20395 avg_loss = 3.09489\n",
      "epoch no.5 train no.434800  loss = 3.58281 avg_loss = 3.12344\n",
      "epoch no.5 train no.434810  loss = 2.73608 avg_loss = 3.12270\n",
      "epoch no.5 train no.434820  loss = 2.22269 avg_loss = 3.10587\n",
      "epoch no.5 train no.434830  loss = 2.54909 avg_loss = 3.09830\n",
      "epoch no.5 train no.434840  loss = 2.77594 avg_loss = 3.08391\n",
      "epoch no.5 train no.434850  loss = 2.03198 avg_loss = 3.07572\n",
      "epoch no.5 train no.434860  loss = 2.53116 avg_loss = 3.11731\n",
      "epoch no.5 train no.434870  loss = 4.01170 avg_loss = 3.09784\n",
      "epoch no.5 train no.434880  loss = 3.16050 avg_loss = 3.10206\n",
      "epoch no.5 train no.434890  loss = 4.74589 avg_loss = 3.15406\n",
      "epoch no.5 train no.434900  loss = 2.75755 avg_loss = 3.14277\n",
      "epoch no.5 train no.434910  loss = 3.10850 avg_loss = 3.14767\n",
      "epoch no.5 train no.434920  loss = 2.88580 avg_loss = 3.14479\n",
      "epoch no.5 train no.434930  loss = 3.51776 avg_loss = 3.16692\n",
      "epoch no.5 train no.434940  loss = 4.32455 avg_loss = 3.14133\n",
      "epoch no.5 train no.434950  loss = 4.45112 avg_loss = 3.12946\n",
      "epoch no.5 train no.434960  loss = 4.14057 avg_loss = 3.16602\n",
      "epoch no.5 train no.434970  loss = 3.06264 avg_loss = 3.13971\n",
      "epoch no.5 train no.434980  loss = 2.02409 avg_loss = 3.12489\n",
      "epoch no.5 train no.434990  loss = 2.40021 avg_loss = 3.14185\n",
      "epoch no.5 train no.435000  loss = 3.27879 avg_loss = 3.13420\n",
      "6\n",
      "to_tokens: ['▁비', '▁명', '▁o', 'st', '▁명', '곡', '</s>', '</s>']\n",
      "추억의 드라마 ost 명곡들</s>\n",
      "epoch no.5 train no.435010  loss = 3.71703 avg_loss = 3.11766\n",
      "epoch no.5 train no.435020  loss = 2.06091 avg_loss = 3.10206\n",
      "epoch no.5 train no.435030  loss = 2.20061 avg_loss = 3.12703\n",
      "epoch no.5 train no.435040  loss = 2.47482 avg_loss = 3.12854\n",
      "epoch no.5 train no.435050  loss = 4.92048 avg_loss = 3.14828\n",
      "epoch no.5 train no.435060  loss = 2.56667 avg_loss = 3.09366\n",
      "epoch no.5 train no.435070  loss = 3.68534 avg_loss = 3.14184\n",
      "epoch no.5 train no.435080  loss = 2.24172 avg_loss = 3.14791\n",
      "epoch no.5 train no.435090  loss = 2.27571 avg_loss = 3.15217\n",
      "epoch no.5 train no.435100  loss = 3.10133 avg_loss = 3.14481\n",
      "epoch no.5 train no.435110  loss = 2.08962 avg_loss = 3.12849\n",
      "epoch no.5 train no.435120  loss = 2.81298 avg_loss = 3.14090\n",
      "epoch no.5 train no.435130  loss = 2.68626 avg_loss = 3.13588\n",
      "epoch no.5 train no.435140  loss = 2.72930 avg_loss = 3.10335\n",
      "epoch no.5 train no.435150  loss = 4.28140 avg_loss = 3.06986\n",
      "epoch no.5 train no.435160  loss = 4.53364 avg_loss = 3.10001\n",
      "epoch no.5 train no.435170  loss = 3.41918 avg_loss = 3.14221\n",
      "epoch no.5 train no.435180  loss = 2.38574 avg_loss = 3.09790\n",
      "epoch no.5 train no.435190  loss = 3.39367 avg_loss = 3.08437\n",
      "epoch no.5 train no.435200  loss = 4.53116 avg_loss = 3.10328\n",
      "epoch no.5 train no.435210  loss = 3.37192 avg_loss = 3.13452\n",
      "epoch no.5 train no.435220  loss = 3.70494 avg_loss = 3.11380\n",
      "epoch no.5 train no.435230  loss = 4.08986 avg_loss = 3.10282\n",
      "epoch no.5 train no.435240  loss = 3.33523 avg_loss = 3.11475\n",
      "epoch no.5 train no.435250  loss = 5.51957 avg_loss = 3.13120\n",
      "epoch no.5 train no.435260  loss = 2.87969 avg_loss = 3.14625\n",
      "epoch no.5 train no.435270  loss = 2.29142 avg_loss = 3.16356\n",
      "epoch no.5 train no.435280  loss = 3.28009 avg_loss = 3.19335\n",
      "epoch no.5 train no.435290  loss = 2.24802 avg_loss = 3.20409\n",
      "epoch no.5 train no.435300  loss = 4.23515 avg_loss = 3.24004\n",
      "epoch no.5 train no.435310  loss = 3.12652 avg_loss = 3.19511\n",
      "epoch no.5 train no.435320  loss = 2.18963 avg_loss = 3.20119\n",
      "epoch no.5 train no.435330  loss = 2.68304 avg_loss = 3.16513\n",
      "epoch no.5 train no.435340  loss = 3.60574 avg_loss = 3.15965\n",
      "epoch no.5 train no.435350  loss = 4.07441 avg_loss = 3.19548\n",
      "epoch no.5 train no.435360  loss = 3.67830 avg_loss = 3.16966\n",
      "epoch no.5 train no.435370  loss = 3.00237 avg_loss = 3.19872\n",
      "epoch no.5 train no.435380  loss = 2.53469 avg_loss = 3.12047\n",
      "epoch no.5 train no.435390  loss = 3.63809 avg_loss = 3.10262\n",
      "epoch no.5 train no.435400  loss = 2.80930 avg_loss = 3.08812\n",
      "epoch no.5 train no.435410  loss = 2.09565 avg_loss = 3.04805\n",
      "epoch no.5 train no.435420  loss = 2.76663 avg_loss = 3.03879\n",
      "epoch no.5 train no.435430  loss = 2.90868 avg_loss = 3.06815\n",
      "epoch no.5 train no.435440  loss = 1.88157 avg_loss = 3.05191\n",
      "epoch no.5 train no.435450  loss = 3.38414 avg_loss = 3.09397\n",
      "epoch no.5 train no.435460  loss = 4.13936 avg_loss = 3.10623\n",
      "epoch no.5 train no.435470  loss = 1.00478 avg_loss = 3.08250\n",
      "epoch no.5 train no.435480  loss = 3.08578 avg_loss = 3.12453\n",
      "epoch no.5 train no.435490  loss = 3.40946 avg_loss = 3.14527\n",
      "epoch no.5 train no.435500  loss = 1.87287 avg_loss = 3.12596\n",
      "epoch no.5 train no.435510  loss = 2.99378 avg_loss = 3.10752\n",
      "epoch no.5 train no.435520  loss = 3.63926 avg_loss = 3.12043\n",
      "epoch no.5 train no.435530  loss = 2.70400 avg_loss = 3.12557\n",
      "epoch no.5 train no.435540  loss = 4.10564 avg_loss = 3.07960\n",
      "epoch no.5 train no.435550  loss = 2.65708 avg_loss = 3.07249\n",
      "epoch no.5 train no.435560  loss = 4.90376 avg_loss = 3.08581\n",
      "epoch no.5 train no.435570  loss = 2.63648 avg_loss = 3.05305\n",
      "epoch no.5 train no.435580  loss = 2.60141 avg_loss = 3.05659\n",
      "epoch no.5 train no.435590  loss = 3.71867 avg_loss = 3.04102\n",
      "epoch no.5 train no.435600  loss = 2.11700 avg_loss = 3.06763\n",
      "epoch no.5 train no.435610  loss = 2.85949 avg_loss = 3.07082\n",
      "epoch no.5 train no.435620  loss = 2.87066 avg_loss = 3.04306\n",
      "epoch no.5 train no.435630  loss = 4.42161 avg_loss = 3.01595\n",
      "epoch no.5 train no.435640  loss = 4.22758 avg_loss = 2.98965\n",
      "epoch no.5 train no.435650  loss = 2.81188 avg_loss = 3.02328\n",
      "epoch no.5 train no.435660  loss = 4.25285 avg_loss = 3.08857\n",
      "epoch no.5 train no.435670  loss = 2.52625 avg_loss = 3.08174\n",
      "epoch no.5 train no.435680  loss = 3.15374 avg_loss = 3.03209\n",
      "epoch no.5 train no.435690  loss = 4.55184 avg_loss = 3.08405\n",
      "epoch no.5 train no.435700  loss = 2.73379 avg_loss = 3.06043\n",
      "epoch no.5 train no.435710  loss = 2.63025 avg_loss = 3.03997\n",
      "epoch no.5 train no.435720  loss = 2.10641 avg_loss = 3.00045\n",
      "epoch no.5 train no.435730  loss = 3.13053 avg_loss = 2.98319\n",
      "epoch no.5 train no.435740  loss = 3.30673 avg_loss = 3.00536\n",
      "epoch no.5 train no.435750  loss = 3.17538 avg_loss = 3.00882\n",
      "epoch no.5 train no.435760  loss = 4.22146 avg_loss = 3.02600\n",
      "epoch no.5 train no.435770  loss = 2.59681 avg_loss = 3.01084\n",
      "epoch no.5 train no.435780  loss = 3.98567 avg_loss = 3.00313\n",
      "epoch no.5 train no.435790  loss = 3.67191 avg_loss = 3.06275\n",
      "epoch no.5 train no.435800  loss = 1.82666 avg_loss = 3.05180\n",
      "epoch no.5 train no.435810  loss = 3.46558 avg_loss = 3.06230\n",
      "epoch no.5 train no.435820  loss = 2.45074 avg_loss = 3.09813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.435830  loss = 2.94275 avg_loss = 3.10697\n",
      "epoch no.5 train no.435840  loss = 4.05859 avg_loss = 3.10978\n",
      "epoch no.5 train no.435850  loss = 2.53163 avg_loss = 3.09738\n",
      "epoch no.5 train no.435860  loss = 3.52432 avg_loss = 3.09296\n",
      "epoch no.5 train no.435870  loss = 3.39543 avg_loss = 3.13824\n",
      "epoch no.5 train no.435880  loss = 2.68383 avg_loss = 3.13067\n",
      "epoch no.5 train no.435890  loss = 3.29637 avg_loss = 3.12478\n",
      "epoch no.5 train no.435900  loss = 4.82539 avg_loss = 3.13391\n",
      "epoch no.5 train no.435910  loss = 3.94787 avg_loss = 3.16308\n",
      "epoch no.5 train no.435920  loss = 3.88842 avg_loss = 3.20996\n",
      "epoch no.5 train no.435930  loss = 4.19969 avg_loss = 3.23377\n",
      "epoch no.5 train no.435940  loss = 3.39887 avg_loss = 3.19780\n",
      "epoch no.5 train no.435950  loss = 3.45973 avg_loss = 3.14554\n",
      "epoch no.5 train no.435960  loss = 2.73815 avg_loss = 3.13425\n",
      "epoch no.5 train no.435970  loss = 3.90331 avg_loss = 3.09818\n",
      "epoch no.5 train no.435980  loss = 2.71769 avg_loss = 3.09575\n",
      "epoch no.5 train no.435990  loss = 2.22327 avg_loss = 3.04857\n",
      "epoch no.5 train no.436000  loss = 2.80996 avg_loss = 3.06958\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.5 train no.436010  loss = 4.06476 avg_loss = 3.09822\n",
      "epoch no.5 train no.436020  loss = 4.08546 avg_loss = 3.10250\n",
      "epoch no.5 train no.436030  loss = 4.15069 avg_loss = 3.08111\n",
      "epoch no.5 train no.436040  loss = 1.99018 avg_loss = 3.03635\n",
      "epoch no.5 train no.436050  loss = 4.67855 avg_loss = 3.02127\n",
      "epoch no.5 train no.436060  loss = 3.64885 avg_loss = 3.05165\n",
      "epoch no.5 train no.436070  loss = 3.29906 avg_loss = 3.05761\n",
      "epoch no.5 train no.436080  loss = 2.38373 avg_loss = 3.06961\n",
      "epoch no.5 train no.436090  loss = 4.77293 avg_loss = 3.05673\n",
      "epoch no.5 train no.436100  loss = 2.59575 avg_loss = 3.03381\n",
      "epoch no.5 train no.436110  loss = 2.45020 avg_loss = 3.01035\n",
      "epoch no.5 train no.436120  loss = 2.85097 avg_loss = 3.00190\n",
      "epoch no.5 train no.436130  loss = 3.88374 avg_loss = 3.01757\n",
      "epoch no.5 train no.436140  loss = 2.35057 avg_loss = 3.10964\n",
      "epoch no.5 train no.436150  loss = 4.04839 avg_loss = 3.14781\n",
      "epoch no.5 train no.436160  loss = 2.82723 avg_loss = 3.18553\n",
      "epoch no.5 train no.436170  loss = 3.12405 avg_loss = 3.17465\n",
      "epoch no.5 train no.436180  loss = 3.84812 avg_loss = 3.19665\n",
      "epoch no.5 train no.436190  loss = 2.67107 avg_loss = 3.20061\n",
      "epoch no.5 train no.436200  loss = 2.96824 avg_loss = 3.20020\n",
      "epoch no.5 train no.436210  loss = 2.84943 avg_loss = 3.19695\n",
      "epoch no.5 train no.436220  loss = 2.04236 avg_loss = 3.17115\n",
      "epoch no.5 train no.436230  loss = 4.22124 avg_loss = 3.24123\n",
      "epoch no.5 train no.436240  loss = 3.48130 avg_loss = 3.21520\n",
      "epoch no.5 train no.436250  loss = 2.58995 avg_loss = 3.24364\n",
      "epoch no.5 train no.436260  loss = 3.54450 avg_loss = 3.21477\n",
      "epoch no.5 train no.436270  loss = 3.94544 avg_loss = 3.22370\n",
      "epoch no.5 train no.436280  loss = 2.08260 avg_loss = 3.19517\n",
      "epoch no.5 train no.436290  loss = 2.21096 avg_loss = 3.12551\n",
      "epoch no.5 train no.436300  loss = 3.46881 avg_loss = 3.13866\n",
      "epoch no.5 train no.436310  loss = 3.97320 avg_loss = 3.13041\n",
      "epoch no.5 train no.436320  loss = 2.80026 avg_loss = 3.14745\n",
      "epoch no.5 train no.436330  loss = 2.74021 avg_loss = 3.13578\n",
      "epoch no.5 train no.436340  loss = 3.83824 avg_loss = 3.14084\n",
      "epoch no.5 train no.436350  loss = 2.41916 avg_loss = 3.17895\n",
      "epoch no.5 train no.436360  loss = 2.22107 avg_loss = 3.13826\n",
      "epoch no.5 train no.436370  loss = 4.66303 avg_loss = 3.13282\n",
      "epoch no.5 train no.436380  loss = 2.21855 avg_loss = 3.09504\n",
      "epoch no.5 train no.436390  loss = 2.78630 avg_loss = 3.09774\n",
      "epoch no.5 train no.436400  loss = 3.21153 avg_loss = 3.07212\n",
      "epoch no.5 train no.436410  loss = 2.68095 avg_loss = 3.04768\n",
      "epoch no.5 train no.436420  loss = 5.07866 avg_loss = 3.09105\n",
      "epoch no.5 train no.436430  loss = 3.90269 avg_loss = 3.07293\n",
      "epoch no.5 train no.436440  loss = 3.46324 avg_loss = 3.06863\n",
      "epoch no.5 train no.436450  loss = 2.01267 avg_loss = 3.08947\n",
      "epoch no.5 train no.436460  loss = 3.90196 avg_loss = 3.08258\n",
      "epoch no.5 train no.436470  loss = 3.34455 avg_loss = 3.08377\n",
      "epoch no.5 train no.436480  loss = 3.11862 avg_loss = 3.07826\n",
      "epoch no.5 train no.436490  loss = 2.58872 avg_loss = 3.10108\n",
      "epoch no.5 train no.436500  loss = 3.11200 avg_loss = 3.08780\n",
      "epoch no.5 train no.436510  loss = 2.11915 avg_loss = 3.10429\n",
      "epoch no.5 train no.436520  loss = 3.06508 avg_loss = 3.07352\n",
      "epoch no.5 train no.436530  loss = 3.31148 avg_loss = 3.07958\n",
      "epoch no.5 train no.436540  loss = 3.72204 avg_loss = 3.05045\n",
      "epoch no.5 train no.436550  loss = 2.41894 avg_loss = 3.04736\n",
      "epoch no.5 train no.436560  loss = 4.11316 avg_loss = 3.07318\n",
      "epoch no.5 train no.436570  loss = 2.30722 avg_loss = 3.13220\n",
      "epoch no.5 train no.436580  loss = 3.30300 avg_loss = 3.15285\n",
      "epoch no.5 train no.436590  loss = 2.95910 avg_loss = 3.17541\n",
      "epoch no.5 train no.436600  loss = 2.46664 avg_loss = 3.17900\n",
      "epoch no.5 train no.436610  loss = 2.62684 avg_loss = 3.16403\n",
      "epoch no.5 train no.436620  loss = 2.17553 avg_loss = 3.11114\n",
      "epoch no.5 train no.436630  loss = 4.04715 avg_loss = 3.12449\n",
      "epoch no.5 train no.436640  loss = 3.61893 avg_loss = 3.11603\n",
      "epoch no.5 train no.436650  loss = 2.61746 avg_loss = 3.11944\n",
      "epoch no.5 train no.436660  loss = 3.77509 avg_loss = 3.11396\n",
      "epoch no.5 train no.436670  loss = 3.49625 avg_loss = 3.10879\n",
      "epoch no.5 train no.436680  loss = 2.18615 avg_loss = 3.16736\n",
      "epoch no.5 train no.436690  loss = 3.06559 avg_loss = 3.12776\n",
      "epoch no.5 train no.436700  loss = 3.57435 avg_loss = 3.15988\n",
      "epoch no.5 train no.436710  loss = 3.04284 avg_loss = 3.14956\n",
      "epoch no.5 train no.436720  loss = 3.36388 avg_loss = 3.14891\n",
      "epoch no.5 train no.436730  loss = 5.03929 avg_loss = 3.18267\n",
      "epoch no.5 train no.436740  loss = 2.81352 avg_loss = 3.22559\n",
      "epoch no.5 train no.436750  loss = 2.18017 avg_loss = 3.21670\n",
      "epoch no.5 train no.436760  loss = 3.09531 avg_loss = 3.22831\n",
      "epoch no.5 train no.436770  loss = 3.31453 avg_loss = 3.23325\n",
      "epoch no.5 train no.436780  loss = 5.67965 avg_loss = 3.23489\n",
      "epoch no.5 train no.436790  loss = 2.78615 avg_loss = 3.22796\n",
      "epoch no.5 train no.436800  loss = 4.34122 avg_loss = 3.23159\n",
      "epoch no.5 train no.436810  loss = 3.62665 avg_loss = 3.21784\n",
      "epoch no.5 train no.436820  loss = 2.38638 avg_loss = 3.17893\n",
      "epoch no.5 train no.436830  loss = 4.24586 avg_loss = 3.20754\n",
      "epoch no.5 train no.436840  loss = 2.45252 avg_loss = 3.17718\n",
      "epoch no.5 train no.436850  loss = 4.61890 avg_loss = 3.13685\n",
      "epoch no.5 train no.436860  loss = 3.55717 avg_loss = 3.12287\n",
      "epoch no.5 train no.436870  loss = 3.11306 avg_loss = 3.13558\n",
      "epoch no.5 train no.436880  loss = 4.08422 avg_loss = 3.10779\n",
      "epoch no.5 train no.436890  loss = 1.65032 avg_loss = 3.12988\n",
      "epoch no.5 train no.436900  loss = 2.83224 avg_loss = 3.13126\n",
      "epoch no.5 train no.436910  loss = 2.02131 avg_loss = 3.13144\n",
      "epoch no.5 train no.436920  loss = 2.80641 avg_loss = 3.10236\n",
      "epoch no.5 train no.436930  loss = 1.38226 avg_loss = 3.12165\n",
      "epoch no.5 train no.436940  loss = 2.85810 avg_loss = 3.08066\n",
      "epoch no.5 train no.436950  loss = 1.94607 avg_loss = 3.04451\n",
      "epoch no.5 train no.436960  loss = 2.34458 avg_loss = 3.05927\n",
      "epoch no.5 train no.436970  loss = 3.44334 avg_loss = 3.08397\n",
      "epoch no.5 train no.436980  loss = 3.25850 avg_loss = 3.07002\n",
      "epoch no.5 train no.436990  loss = 3.33030 avg_loss = 3.08954\n",
      "epoch no.5 train no.437000  loss = 4.26704 avg_loss = 3.09187\n",
      "5\n",
      "to_tokens: ['▁비', '▁명', '송', '▁모음', '집', '집', '</s>']\n",
      "추억의팝송 모음2집</s>\n",
      "epoch no.5 train no.437010  loss = 2.71425 avg_loss = 3.09808\n",
      "epoch no.5 train no.437020  loss = 2.86816 avg_loss = 3.13054\n",
      "epoch no.5 train no.437030  loss = 4.38449 avg_loss = 3.14840\n",
      "epoch no.5 train no.437040  loss = 3.50762 avg_loss = 3.16767\n",
      "epoch no.5 train no.437050  loss = 3.31386 avg_loss = 3.21833\n",
      "epoch no.5 train no.437060  loss = 2.91193 avg_loss = 3.20939\n",
      "epoch no.5 train no.437070  loss = 2.13775 avg_loss = 3.22509\n",
      "epoch no.5 train no.437080  loss = 2.54429 avg_loss = 3.19903\n",
      "epoch no.5 train no.437090  loss = 2.60729 avg_loss = 3.19266\n",
      "epoch no.5 train no.437100  loss = 4.99166 avg_loss = 3.21130\n",
      "epoch no.5 train no.437110  loss = 3.01329 avg_loss = 3.23202\n",
      "epoch no.5 train no.437120  loss = 2.02810 avg_loss = 3.18169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.437130  loss = 2.88916 avg_loss = 3.19002\n",
      "epoch no.5 train no.437140  loss = 2.43353 avg_loss = 3.17801\n",
      "epoch no.5 train no.437150  loss = 3.09803 avg_loss = 3.23238\n",
      "epoch no.5 train no.437160  loss = 3.21360 avg_loss = 3.21043\n",
      "epoch no.5 train no.437170  loss = 3.16018 avg_loss = 3.20078\n",
      "epoch no.5 train no.437180  loss = 3.47184 avg_loss = 3.16389\n",
      "epoch no.5 train no.437190  loss = 2.92826 avg_loss = 3.15430\n",
      "epoch no.5 train no.437200  loss = 2.80796 avg_loss = 3.15537\n",
      "epoch no.5 train no.437210  loss = 2.52943 avg_loss = 3.15796\n",
      "epoch no.5 train no.437220  loss = 3.65501 avg_loss = 3.19095\n",
      "epoch no.5 train no.437230  loss = 2.38420 avg_loss = 3.23105\n",
      "epoch no.5 train no.437240  loss = 4.04898 avg_loss = 3.23671\n",
      "epoch no.5 train no.437250  loss = 2.71143 avg_loss = 3.22160\n",
      "epoch no.5 train no.437260  loss = 1.82834 avg_loss = 3.21890\n",
      "epoch no.5 train no.437270  loss = 4.56949 avg_loss = 3.21759\n",
      "epoch no.5 train no.437280  loss = 2.44168 avg_loss = 3.18689\n",
      "epoch no.5 train no.437290  loss = 2.33419 avg_loss = 3.15514\n",
      "epoch no.5 train no.437300  loss = 2.54033 avg_loss = 3.16476\n",
      "epoch no.5 train no.437310  loss = 3.31111 avg_loss = 3.14561\n",
      "epoch no.5 train no.437320  loss = 1.99335 avg_loss = 3.17836\n",
      "epoch no.5 train no.437330  loss = 4.06590 avg_loss = 3.17084\n",
      "epoch no.5 train no.437340  loss = 5.10512 avg_loss = 3.19608\n",
      "epoch no.5 train no.437350  loss = 3.14671 avg_loss = 3.19423\n",
      "epoch no.5 train no.437360  loss = 2.71749 avg_loss = 3.19071\n",
      "epoch no.5 train no.437370  loss = 4.57743 avg_loss = 3.22950\n",
      "epoch no.5 train no.437380  loss = 3.41080 avg_loss = 3.25352\n",
      "epoch no.5 train no.437390  loss = 1.97694 avg_loss = 3.22863\n",
      "epoch no.5 train no.437400  loss = 2.55510 avg_loss = 3.21665\n",
      "epoch no.5 train no.437410  loss = 2.59731 avg_loss = 3.20647\n",
      "epoch no.5 train no.437420  loss = 2.41998 avg_loss = 3.23694\n",
      "epoch no.5 train no.437430  loss = 3.01093 avg_loss = 3.21704\n",
      "epoch no.5 train no.437440  loss = 3.13678 avg_loss = 3.22546\n",
      "epoch no.5 train no.437450  loss = 3.55297 avg_loss = 3.20436\n",
      "epoch no.5 train no.437460  loss = 3.55683 avg_loss = 3.19391\n",
      "epoch no.5 train no.437470  loss = 2.45175 avg_loss = 3.18586\n",
      "epoch no.5 train no.437480  loss = 2.40324 avg_loss = 3.16422\n",
      "epoch no.5 train no.437490  loss = 2.77790 avg_loss = 3.17275\n",
      "epoch no.5 train no.437500  loss = 2.46316 avg_loss = 3.18075\n",
      "epoch no.5 train no.437510  loss = 2.45722 avg_loss = 3.17251\n",
      "epoch no.5 train no.437520  loss = 2.69469 avg_loss = 3.17477\n",
      "epoch no.5 train no.437530  loss = 4.37740 avg_loss = 3.19704\n",
      "epoch no.5 train no.437540  loss = 2.26746 avg_loss = 3.23529\n",
      "epoch no.5 train no.437550  loss = 1.57223 avg_loss = 3.24479\n",
      "epoch no.5 train no.437560  loss = 2.68415 avg_loss = 3.20048\n",
      "epoch no.5 train no.437570  loss = 3.47486 avg_loss = 3.22043\n",
      "epoch no.5 train no.437580  loss = 2.43451 avg_loss = 3.16863\n",
      "epoch no.5 train no.437590  loss = 3.66726 avg_loss = 3.16737\n",
      "epoch no.5 train no.437600  loss = 3.69317 avg_loss = 3.15778\n",
      "epoch no.5 train no.437610  loss = 2.86983 avg_loss = 3.12325\n",
      "epoch no.5 train no.437620  loss = 2.28973 avg_loss = 3.12207\n",
      "epoch no.5 train no.437630  loss = 2.44622 avg_loss = 3.12453\n",
      "epoch no.5 train no.437640  loss = 3.25796 avg_loss = 3.12678\n",
      "epoch no.5 train no.437650  loss = 1.31322 avg_loss = 3.11004\n",
      "epoch no.5 train no.437660  loss = 5.49151 avg_loss = 3.18009\n",
      "epoch no.5 train no.437670  loss = 3.94329 avg_loss = 3.13723\n",
      "epoch no.5 train no.437680  loss = 1.78664 avg_loss = 3.12073\n",
      "epoch no.5 train no.437690  loss = 4.01600 avg_loss = 3.15683\n",
      "epoch no.5 train no.437700  loss = 2.88984 avg_loss = 3.14781\n",
      "epoch no.5 train no.437710  loss = 3.17288 avg_loss = 3.17946\n",
      "epoch no.5 train no.437720  loss = 2.16301 avg_loss = 3.16125\n",
      "epoch no.5 train no.437730  loss = 2.05871 avg_loss = 3.12317\n",
      "epoch no.5 train no.437740  loss = 3.20377 avg_loss = 3.18622\n",
      "epoch no.5 train no.437750  loss = 3.30732 avg_loss = 3.15357\n",
      "epoch no.5 train no.437760  loss = 4.24591 avg_loss = 3.17816\n",
      "epoch no.5 train no.437770  loss = 4.20834 avg_loss = 3.17080\n",
      "epoch no.5 train no.437780  loss = 2.92676 avg_loss = 3.14102\n",
      "epoch no.5 train no.437790  loss = 2.50170 avg_loss = 3.12942\n",
      "epoch no.5 train no.437800  loss = 3.39548 avg_loss = 3.13178\n",
      "epoch no.5 train no.437810  loss = 2.48519 avg_loss = 3.09215\n",
      "epoch no.5 train no.437820  loss = 2.49033 avg_loss = 3.07203\n",
      "epoch no.5 train no.437830  loss = 4.38501 avg_loss = 3.07185\n",
      "epoch no.5 train no.437840  loss = 2.02998 avg_loss = 3.08365\n",
      "epoch no.5 train no.437850  loss = 4.69457 avg_loss = 3.13157\n",
      "epoch no.5 train no.437860  loss = 1.87183 avg_loss = 3.14995\n",
      "epoch no.5 train no.437870  loss = 2.89616 avg_loss = 3.08159\n",
      "epoch no.5 train no.437880  loss = 2.96744 avg_loss = 3.10227\n",
      "epoch no.5 train no.437890  loss = 3.95013 avg_loss = 3.10221\n",
      "epoch no.5 train no.437900  loss = 3.55594 avg_loss = 3.14311\n",
      "epoch no.5 train no.437910  loss = 2.21850 avg_loss = 3.07680\n",
      "epoch no.5 train no.437920  loss = 2.88429 avg_loss = 3.09352\n",
      "epoch no.5 train no.437930  loss = 2.75567 avg_loss = 3.10074\n",
      "epoch no.5 train no.437940  loss = 3.44655 avg_loss = 3.08246\n",
      "epoch no.5 train no.437950  loss = 2.80506 avg_loss = 3.04472\n",
      "epoch no.5 train no.437960  loss = 2.88341 avg_loss = 3.06568\n",
      "epoch no.5 train no.437970  loss = 3.54168 avg_loss = 3.09067\n",
      "epoch no.5 train no.437980  loss = 4.25822 avg_loss = 3.10380\n",
      "epoch no.5 train no.437990  loss = 2.27656 avg_loss = 3.10721\n",
      "epoch no.5 train no.438000  loss = 2.78506 avg_loss = 3.10391\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.5 train no.438010  loss = 4.63443 avg_loss = 3.06876\n",
      "epoch no.5 train no.438020  loss = 2.88403 avg_loss = 3.04056\n",
      "epoch no.5 train no.438030  loss = 2.35042 avg_loss = 3.04582\n",
      "epoch no.5 train no.438040  loss = 3.55202 avg_loss = 3.04010\n",
      "epoch no.5 train no.438050  loss = 4.19207 avg_loss = 3.05154\n",
      "epoch no.5 train no.438060  loss = 3.29039 avg_loss = 3.02248\n",
      "epoch no.5 train no.438070  loss = 2.76684 avg_loss = 3.04574\n",
      "epoch no.5 train no.438080  loss = 2.86479 avg_loss = 3.08521\n",
      "epoch no.5 train no.438090  loss = 4.13095 avg_loss = 3.10961\n",
      "epoch no.5 train no.438100  loss = 4.26417 avg_loss = 3.10650\n",
      "epoch no.5 train no.438110  loss = 2.76016 avg_loss = 3.08841\n",
      "epoch no.5 train no.438120  loss = 3.35144 avg_loss = 3.09021\n",
      "epoch no.5 train no.438130  loss = 2.82830 avg_loss = 3.09101\n",
      "epoch no.5 train no.438140  loss = 4.69370 avg_loss = 3.09349\n",
      "epoch no.5 train no.438150  loss = 2.94886 avg_loss = 3.08139\n",
      "epoch no.5 train no.438160  loss = 3.10381 avg_loss = 3.10777\n",
      "epoch no.5 train no.438170  loss = 2.85065 avg_loss = 3.09729\n",
      "epoch no.5 train no.438180  loss = 2.88274 avg_loss = 3.10856\n",
      "epoch no.5 train no.438190  loss = 2.95116 avg_loss = 3.13876\n",
      "epoch no.5 train no.438200  loss = 2.64011 avg_loss = 3.10326\n",
      "epoch no.5 train no.438210  loss = 2.68479 avg_loss = 3.08015\n",
      "epoch no.5 train no.438220  loss = 3.49237 avg_loss = 3.05781\n",
      "epoch no.5 train no.438230  loss = 2.03569 avg_loss = 3.07513\n",
      "epoch no.5 train no.438240  loss = 3.08633 avg_loss = 3.07976\n",
      "epoch no.5 train no.438250  loss = 3.54213 avg_loss = 3.10134\n",
      "epoch no.5 train no.438260  loss = 2.66326 avg_loss = 3.08954\n",
      "epoch no.5 train no.438270  loss = 3.29322 avg_loss = 3.06391\n",
      "epoch no.5 train no.438280  loss = 3.58357 avg_loss = 3.09835\n",
      "epoch no.5 train no.438290  loss = 4.04064 avg_loss = 3.11581\n",
      "epoch no.5 train no.438300  loss = 2.73209 avg_loss = 3.12791\n",
      "epoch no.5 train no.438310  loss = 3.06284 avg_loss = 3.11184\n",
      "epoch no.5 train no.438320  loss = 2.55799 avg_loss = 3.06832\n",
      "epoch no.5 train no.438330  loss = 2.17419 avg_loss = 3.04908\n",
      "epoch no.5 train no.438340  loss = 1.18892 avg_loss = 3.06082\n",
      "epoch no.5 train no.438350  loss = 1.65614 avg_loss = 3.07622\n",
      "epoch no.5 train no.438360  loss = 2.31372 avg_loss = 3.09278\n",
      "epoch no.5 train no.438370  loss = 3.07525 avg_loss = 3.13906\n",
      "epoch no.5 train no.438380  loss = 4.14655 avg_loss = 3.19084\n",
      "epoch no.5 train no.438390  loss = 2.19153 avg_loss = 3.19262\n",
      "epoch no.5 train no.438400  loss = 2.97465 avg_loss = 3.16340\n",
      "epoch no.5 train no.438410  loss = 2.78234 avg_loss = 3.14802\n",
      "epoch no.5 train no.438420  loss = 4.40241 avg_loss = 3.17996\n",
      "epoch no.5 train no.438430  loss = 3.41382 avg_loss = 3.15857\n",
      "epoch no.5 train no.438440  loss = 4.81182 avg_loss = 3.17418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.438450  loss = 2.61089 avg_loss = 3.13055\n",
      "epoch no.5 train no.438460  loss = 3.12625 avg_loss = 3.11871\n",
      "epoch no.5 train no.438470  loss = 2.40325 avg_loss = 3.06913\n",
      "epoch no.5 train no.438480  loss = 4.34848 avg_loss = 3.10229\n",
      "epoch no.5 train no.438490  loss = 3.70247 avg_loss = 3.05961\n",
      "epoch no.5 train no.438500  loss = 2.22819 avg_loss = 3.04311\n",
      "epoch no.5 train no.438510  loss = 1.55804 avg_loss = 3.03854\n",
      "epoch no.5 train no.438520  loss = 2.85535 avg_loss = 3.07341\n",
      "epoch no.5 train no.438530  loss = 3.53711 avg_loss = 3.13043\n",
      "epoch no.5 train no.438540  loss = 1.87232 avg_loss = 3.12717\n",
      "epoch no.5 train no.438550  loss = 3.48643 avg_loss = 3.15433\n",
      "epoch no.5 train no.438560  loss = 1.88588 avg_loss = 3.17572\n",
      "epoch no.5 train no.438570  loss = 3.11446 avg_loss = 3.14858\n",
      "epoch no.5 train no.438580  loss = 1.90519 avg_loss = 3.12524\n",
      "epoch no.5 train no.438590  loss = 3.42141 avg_loss = 3.13414\n",
      "epoch no.5 train no.438600  loss = 2.30070 avg_loss = 3.12843\n",
      "epoch no.5 train no.438610  loss = 3.98942 avg_loss = 3.14615\n",
      "epoch no.5 train no.438620  loss = 3.80204 avg_loss = 3.14373\n",
      "epoch no.5 train no.438630  loss = 3.31263 avg_loss = 3.18082\n",
      "epoch no.5 train no.438640  loss = 3.73263 avg_loss = 3.14336\n",
      "epoch no.5 train no.438650  loss = 2.73692 avg_loss = 3.17663\n",
      "epoch no.5 train no.438660  loss = 3.22356 avg_loss = 3.18703\n",
      "epoch no.5 train no.438670  loss = 4.77184 avg_loss = 3.19551\n",
      "epoch no.5 train no.438680  loss = 6.89124 avg_loss = 3.21119\n",
      "epoch no.5 train no.438690  loss = 3.55766 avg_loss = 3.18468\n",
      "epoch no.5 train no.438700  loss = 4.03378 avg_loss = 3.18990\n",
      "epoch no.5 train no.438710  loss = 2.79907 avg_loss = 3.15790\n",
      "epoch no.5 train no.438720  loss = 3.07142 avg_loss = 3.16730\n",
      "epoch no.5 train no.438730  loss = 2.96189 avg_loss = 3.15233\n",
      "epoch no.5 train no.438740  loss = 3.07602 avg_loss = 3.14484\n",
      "epoch no.5 train no.438750  loss = 2.87232 avg_loss = 3.16908\n",
      "epoch no.5 train no.438760  loss = 2.58843 avg_loss = 3.21218\n",
      "epoch no.5 train no.438770  loss = 2.24403 avg_loss = 3.19610\n",
      "epoch no.5 train no.438780  loss = 3.80854 avg_loss = 3.19632\n",
      "epoch no.5 train no.438790  loss = 2.63074 avg_loss = 3.17087\n",
      "epoch no.5 train no.438800  loss = 2.97654 avg_loss = 3.16899\n",
      "epoch no.5 train no.438810  loss = 2.53359 avg_loss = 3.11202\n",
      "epoch no.5 train no.438820  loss = 4.18914 avg_loss = 3.12778\n",
      "epoch no.5 train no.438830  loss = 3.72840 avg_loss = 3.14455\n",
      "epoch no.5 train no.438840  loss = 4.76364 avg_loss = 3.14211\n",
      "epoch no.5 train no.438850  loss = 3.27328 avg_loss = 3.14439\n",
      "epoch no.5 train no.438860  loss = 3.24887 avg_loss = 3.08190\n",
      "epoch no.5 train no.438870  loss = 3.97903 avg_loss = 3.13346\n",
      "epoch no.5 train no.438880  loss = 4.09188 avg_loss = 3.17411\n",
      "epoch no.5 train no.438890  loss = 2.94171 avg_loss = 3.19656\n",
      "epoch no.5 train no.438900  loss = 3.47280 avg_loss = 3.21868\n",
      "epoch no.5 train no.438910  loss = 2.83997 avg_loss = 3.19243\n",
      "epoch no.5 train no.438920  loss = 2.18616 avg_loss = 3.16244\n",
      "epoch no.5 train no.438930  loss = 2.78439 avg_loss = 3.15911\n",
      "epoch no.5 train no.438940  loss = 1.84683 avg_loss = 3.14656\n",
      "epoch no.5 train no.438950  loss = 4.17597 avg_loss = 3.16174\n",
      "epoch no.5 train no.438960  loss = 4.14793 avg_loss = 3.14541\n",
      "epoch no.5 train no.438970  loss = 3.26682 avg_loss = 3.09176\n",
      "epoch no.5 train no.438980  loss = 1.77706 avg_loss = 3.06387\n",
      "epoch no.5 train no.438990  loss = 2.74210 avg_loss = 3.09060\n",
      "epoch no.5 train no.439000  loss = 3.70317 avg_loss = 3.10769\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '송', '들', '2', '</s>']\n",
      "추억의팝송 모음집</s>\n",
      "epoch no.5 train no.439010  loss = 3.51080 avg_loss = 3.09670\n",
      "epoch no.5 train no.439020  loss = 3.22657 avg_loss = 3.10891\n",
      "epoch no.5 train no.439030  loss = 2.62349 avg_loss = 3.07939\n",
      "epoch no.5 train no.439040  loss = 4.27894 avg_loss = 3.08599\n",
      "epoch no.5 train no.439050  loss = 4.15386 avg_loss = 3.08207\n",
      "epoch no.5 train no.439060  loss = 1.84916 avg_loss = 3.08813\n",
      "epoch no.5 train no.439070  loss = 1.58246 avg_loss = 3.04999\n",
      "epoch no.5 train no.439080  loss = 3.74036 avg_loss = 3.09657\n",
      "epoch no.5 train no.439090  loss = 2.97829 avg_loss = 3.09458\n",
      "epoch no.5 train no.439100  loss = 1.37615 avg_loss = 3.03574\n",
      "epoch no.5 train no.439110  loss = 3.13398 avg_loss = 3.03130\n",
      "epoch no.5 train no.439120  loss = 3.22954 avg_loss = 3.05378\n",
      "epoch no.5 train no.439130  loss = 3.59661 avg_loss = 3.07904\n",
      "epoch no.5 train no.439140  loss = 2.06269 avg_loss = 3.07838\n",
      "epoch no.5 train no.439150  loss = 3.78867 avg_loss = 3.06682\n",
      "epoch no.5 train no.439160  loss = 2.98682 avg_loss = 3.12207\n",
      "epoch no.5 train no.439170  loss = 3.85556 avg_loss = 3.14041\n",
      "epoch no.5 train no.439180  loss = 3.19782 avg_loss = 3.15706\n",
      "epoch no.5 train no.439190  loss = 4.58483 avg_loss = 3.12214\n",
      "epoch no.5 train no.439200  loss = 1.89841 avg_loss = 3.13049\n",
      "epoch no.5 train no.439210  loss = 2.74706 avg_loss = 3.12320\n",
      "epoch no.5 train no.439220  loss = 3.78837 avg_loss = 3.13619\n",
      "epoch no.5 train no.439230  loss = 1.64211 avg_loss = 3.12228\n",
      "epoch no.5 train no.439240  loss = 3.39923 avg_loss = 3.15437\n",
      "epoch no.5 train no.439250  loss = 3.02266 avg_loss = 3.18867\n",
      "epoch no.5 train no.439260  loss = 1.93405 avg_loss = 3.16886\n",
      "epoch no.5 train no.439270  loss = 3.94465 avg_loss = 3.19207\n",
      "epoch no.5 train no.439280  loss = 3.88136 avg_loss = 3.20008\n",
      "epoch no.5 train no.439290  loss = 3.48659 avg_loss = 3.20119\n",
      "epoch no.5 train no.439300  loss = 3.23653 avg_loss = 3.21176\n",
      "epoch no.5 train no.439310  loss = 3.01408 avg_loss = 3.20853\n",
      "epoch no.5 train no.439320  loss = 3.86236 avg_loss = 3.21362\n",
      "epoch no.5 train no.439330  loss = 3.74044 avg_loss = 3.23391\n",
      "epoch no.5 train no.439340  loss = 3.38605 avg_loss = 3.24928\n",
      "epoch no.5 train no.439350  loss = 3.45742 avg_loss = 3.23810\n",
      "epoch no.5 train no.439360  loss = 3.61928 avg_loss = 3.18786\n",
      "epoch no.5 train no.439370  loss = 2.66055 avg_loss = 3.16075\n",
      "epoch no.5 train no.439380  loss = 3.53697 avg_loss = 3.18848\n",
      "epoch no.5 train no.439390  loss = 4.40417 avg_loss = 3.18782\n",
      "epoch no.5 train no.439400  loss = 3.44837 avg_loss = 3.17873\n",
      "epoch no.5 train no.439410  loss = 3.27248 avg_loss = 3.18984\n",
      "epoch no.5 train no.439420  loss = 3.35235 avg_loss = 3.18990\n",
      "epoch no.5 train no.439430  loss = 3.10676 avg_loss = 3.18767\n",
      "epoch no.5 train no.439440  loss = 2.62981 avg_loss = 3.20143\n",
      "epoch no.5 train no.439450  loss = 1.96475 avg_loss = 3.16428\n",
      "epoch no.5 train no.439460  loss = 4.37909 avg_loss = 3.16098\n",
      "epoch no.5 train no.439470  loss = 4.22570 avg_loss = 3.16787\n",
      "epoch no.5 train no.439480  loss = 3.78697 avg_loss = 3.18834\n",
      "epoch no.5 train no.439490  loss = 3.40279 avg_loss = 3.18151\n",
      "epoch no.5 train no.439500  loss = 3.09203 avg_loss = 3.15476\n",
      "epoch no.5 train no.439510  loss = 2.66618 avg_loss = 3.12223\n",
      "epoch no.5 train no.439520  loss = 4.42684 avg_loss = 3.11276\n",
      "epoch no.5 train no.439530  loss = 2.36621 avg_loss = 3.10544\n",
      "epoch no.5 train no.439540  loss = 2.88574 avg_loss = 3.12385\n",
      "epoch no.5 train no.439550  loss = 3.00892 avg_loss = 3.14736\n",
      "epoch no.5 train no.439560  loss = 3.96189 avg_loss = 3.16433\n",
      "epoch no.5 train no.439570  loss = 3.31339 avg_loss = 3.12709\n",
      "epoch no.5 train no.439580  loss = 2.23244 avg_loss = 3.12378\n",
      "epoch no.5 train no.439590  loss = 2.82220 avg_loss = 3.11857\n",
      "epoch no.5 train no.439600  loss = 2.82501 avg_loss = 3.10011\n",
      "epoch no.5 train no.439610  loss = 4.45441 avg_loss = 3.11148\n",
      "epoch no.5 train no.439620  loss = 2.16712 avg_loss = 3.08754\n",
      "epoch no.5 train no.439630  loss = 2.12018 avg_loss = 3.11774\n",
      "epoch no.5 train no.439640  loss = 4.28609 avg_loss = 3.17496\n",
      "epoch no.5 train no.439650  loss = 4.74386 avg_loss = 3.18577\n",
      "epoch no.5 train no.439660  loss = 2.13208 avg_loss = 3.15770\n",
      "epoch no.5 train no.439670  loss = 3.60512 avg_loss = 3.19653\n",
      "epoch no.5 train no.439680  loss = 2.42837 avg_loss = 3.17917\n",
      "epoch no.5 train no.439690  loss = 2.68235 avg_loss = 3.16826\n",
      "epoch no.5 train no.439700  loss = 2.39723 avg_loss = 3.11094\n",
      "epoch no.5 train no.439710  loss = 4.51833 avg_loss = 3.12080\n",
      "epoch no.5 train no.439720  loss = 3.33414 avg_loss = 3.11969\n",
      "epoch no.5 train no.439730  loss = 4.14658 avg_loss = 3.12476\n",
      "epoch no.5 train no.439740  loss = 2.57196 avg_loss = 3.11133\n",
      "epoch no.5 train no.439750  loss = 4.37613 avg_loss = 3.05538\n",
      "epoch no.5 train no.439760  loss = 3.05514 avg_loss = 3.07567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.5 train no.439770  loss = 2.92505 avg_loss = 3.07813\n",
      "epoch no.5 train no.439780  loss = 4.53745 avg_loss = 3.10802\n",
      "epoch no.5 train no.439790  loss = 2.61646 avg_loss = 3.09332\n",
      "epoch no.5 train no.439800  loss = 1.25005 avg_loss = 3.05017\n",
      "epoch no.5 train no.439810  loss = 4.02361 avg_loss = 3.05827\n",
      "epoch no.5 train no.439820  loss = 3.34972 avg_loss = 3.06555\n",
      "epoch no.5 train no.439830  loss = 2.86349 avg_loss = 3.03347\n",
      "epoch no.5 train no.439840  loss = 4.71594 avg_loss = 3.07574\n",
      "epoch no.5 train no.439850  loss = 3.03330 avg_loss = 3.09763\n",
      "epoch no.5 train no.439860  loss = 3.11723 avg_loss = 3.09050\n",
      "epoch no.5 train no.439870  loss = 3.46836 avg_loss = 3.15657\n",
      "epoch no.5 train no.439880  loss = 2.84871 avg_loss = 3.17797\n",
      "epoch no.5 train no.439890  loss = 3.80946 avg_loss = 3.17985\n",
      "epoch no.5 train no.439900  loss = 3.18272 avg_loss = 3.19258\n",
      "epoch no.5 train no.439910  loss = 1.95525 avg_loss = 3.17783\n",
      "epoch no.5 train no.439920  loss = 4.00171 avg_loss = 3.17267\n",
      "epoch no.5 train no.439930  loss = 4.57414 avg_loss = 3.23311\n",
      "epoch no.5 train no.439940  loss = 2.74978 avg_loss = 3.20190\n",
      "epoch no.5 train no.439950  loss = 3.17849 avg_loss = 3.15589\n",
      "epoch no.5 train no.439960  loss = 3.14101 avg_loss = 3.15854\n",
      "epoch no.5 train no.439970  loss = 2.80117 avg_loss = 3.12956\n",
      "epoch no.5 train no.439980  loss = 3.57873 avg_loss = 3.13044\n",
      "epoch no.5 train no.439990  loss = 2.99601 avg_loss = 3.10565\n",
      "epoch no.5 train no.440000  loss = 3.44163 avg_loss = 3.12434\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '▁댄스', '곡']\n",
      "추억의 2000년대 아이돌 댄스</s>\n",
      "epoch no.5 train no.440010  loss = 3.11113 avg_loss = 3.11172\n",
      "epoch no.5 train no.440020  loss = 3.37513 avg_loss = 3.09256\n",
      "epoch no.5 train no.440030  loss = 3.48442 avg_loss = 3.12294\n",
      "epoch no.5 train no.440040  loss = 2.91081 avg_loss = 3.07249\n",
      "epoch no.5 train no.440050  loss = 2.80506 avg_loss = 3.10898\n",
      "epoch no.5 train no.440060  loss = 2.93448 avg_loss = 3.10252\n",
      "epoch no.5 train no.440070  loss = 4.72915 avg_loss = 3.11709\n",
      "epoch no.5 train no.440080  loss = 3.10686 avg_loss = 3.13365\n",
      "epoch no.5 train no.440090  loss = 2.85628 avg_loss = 3.14794\n",
      "epoch no.5 train no.440100  loss = 3.57510 avg_loss = 3.17210\n",
      "epoch no.5 train no.440110  loss = 3.24859 avg_loss = 3.14809\n",
      "epoch no.6 train no.440120  loss = 2.66677 avg_loss = 3.11227\n",
      "epoch no.6 train no.440130  loss = 3.57392 avg_loss = 3.08520\n",
      "epoch no.6 train no.440140  loss = 2.76701 avg_loss = 3.04408\n",
      "epoch no.6 train no.440150  loss = 2.98671 avg_loss = 3.01471\n",
      "epoch no.6 train no.440160  loss = 2.85026 avg_loss = 2.99656\n",
      "epoch no.6 train no.440170  loss = 3.41503 avg_loss = 3.01030\n",
      "epoch no.6 train no.440180  loss = 2.74539 avg_loss = 2.97974\n",
      "epoch no.6 train no.440190  loss = 3.64147 avg_loss = 2.96324\n",
      "epoch no.6 train no.440200  loss = 3.30728 avg_loss = 2.92917\n",
      "epoch no.6 train no.440210  loss = 1.89200 avg_loss = 2.90588\n",
      "epoch no.6 train no.440220  loss = 4.42453 avg_loss = 2.93900\n",
      "epoch no.6 train no.440230  loss = 2.09340 avg_loss = 2.92637\n",
      "epoch no.6 train no.440240  loss = 2.85103 avg_loss = 2.90171\n",
      "epoch no.6 train no.440250  loss = 2.63975 avg_loss = 2.89714\n",
      "epoch no.6 train no.440260  loss = 2.89480 avg_loss = 2.89053\n",
      "epoch no.6 train no.440270  loss = 1.62948 avg_loss = 2.84119\n",
      "epoch no.6 train no.440280  loss = 2.85053 avg_loss = 2.85923\n",
      "epoch no.6 train no.440290  loss = 2.98693 avg_loss = 2.88364\n",
      "epoch no.6 train no.440300  loss = 3.75565 avg_loss = 2.89130\n",
      "epoch no.6 train no.440310  loss = 2.52224 avg_loss = 2.87416\n",
      "epoch no.6 train no.440320  loss = 3.41058 avg_loss = 2.85610\n",
      "epoch no.6 train no.440330  loss = 4.81931 avg_loss = 2.87319\n",
      "epoch no.6 train no.440340  loss = 2.46800 avg_loss = 2.85482\n",
      "epoch no.6 train no.440350  loss = 2.88950 avg_loss = 2.85415\n",
      "epoch no.6 train no.440360  loss = 3.40359 avg_loss = 2.82025\n",
      "epoch no.6 train no.440370  loss = 2.86845 avg_loss = 2.81125\n",
      "epoch no.6 train no.440380  loss = 2.65758 avg_loss = 2.80301\n",
      "epoch no.6 train no.440390  loss = 2.06715 avg_loss = 2.79317\n",
      "epoch no.6 train no.440400  loss = 1.84974 avg_loss = 2.77735\n",
      "epoch no.6 train no.440410  loss = 3.15038 avg_loss = 2.77818\n",
      "epoch no.6 train no.440420  loss = 2.88063 avg_loss = 2.76866\n",
      "epoch no.6 train no.440430  loss = 3.88481 avg_loss = 2.74724\n",
      "epoch no.6 train no.440440  loss = 2.52572 avg_loss = 2.75512\n",
      "epoch no.6 train no.440450  loss = 3.25298 avg_loss = 2.76956\n",
      "epoch no.6 train no.440460  loss = 2.51725 avg_loss = 2.77454\n",
      "epoch no.6 train no.440470  loss = 2.13368 avg_loss = 2.72894\n",
      "epoch no.6 train no.440480  loss = 3.73394 avg_loss = 2.74241\n",
      "epoch no.6 train no.440490  loss = 2.98654 avg_loss = 2.77938\n",
      "epoch no.6 train no.440500  loss = 2.67379 avg_loss = 2.79448\n",
      "epoch no.6 train no.440510  loss = 2.86491 avg_loss = 2.78659\n",
      "epoch no.6 train no.440520  loss = 2.09620 avg_loss = 2.79702\n",
      "epoch no.6 train no.440530  loss = 1.85621 avg_loss = 2.82281\n",
      "epoch no.6 train no.440540  loss = 3.45023 avg_loss = 2.80801\n",
      "epoch no.6 train no.440550  loss = 2.63840 avg_loss = 2.80045\n",
      "epoch no.6 train no.440560  loss = 1.94414 avg_loss = 2.85525\n",
      "epoch no.6 train no.440570  loss = 3.32112 avg_loss = 2.86760\n",
      "epoch no.6 train no.440580  loss = 1.89935 avg_loss = 2.81532\n",
      "epoch no.6 train no.440590  loss = 2.05027 avg_loss = 2.79678\n",
      "epoch no.6 train no.440600  loss = 2.46372 avg_loss = 2.81743\n",
      "epoch no.6 train no.440610  loss = 3.09161 avg_loss = 2.80045\n",
      "epoch no.6 train no.440620  loss = 3.29465 avg_loss = 2.84477\n",
      "epoch no.6 train no.440630  loss = 2.44523 avg_loss = 2.83871\n",
      "epoch no.6 train no.440640  loss = 2.01559 avg_loss = 2.83367\n",
      "epoch no.6 train no.440650  loss = 2.54125 avg_loss = 2.82864\n",
      "epoch no.6 train no.440660  loss = 2.59989 avg_loss = 2.80596\n",
      "epoch no.6 train no.440670  loss = 4.01335 avg_loss = 2.81214\n",
      "epoch no.6 train no.440680  loss = 2.71809 avg_loss = 2.82036\n",
      "epoch no.6 train no.440690  loss = 2.84198 avg_loss = 2.79343\n",
      "epoch no.6 train no.440700  loss = 3.48238 avg_loss = 2.79438\n",
      "epoch no.6 train no.440710  loss = 2.46567 avg_loss = 2.79397\n",
      "epoch no.6 train no.440720  loss = 1.95372 avg_loss = 2.82593\n",
      "epoch no.6 train no.440730  loss = 2.35402 avg_loss = 2.78467\n",
      "epoch no.6 train no.440740  loss = 3.25390 avg_loss = 2.81276\n",
      "epoch no.6 train no.440750  loss = 1.96164 avg_loss = 2.78080\n",
      "epoch no.6 train no.440760  loss = 3.60619 avg_loss = 2.78295\n",
      "epoch no.6 train no.440770  loss = 2.13202 avg_loss = 2.77106\n",
      "epoch no.6 train no.440780  loss = 1.23599 avg_loss = 2.76477\n",
      "epoch no.6 train no.440790  loss = 2.62302 avg_loss = 2.79976\n",
      "epoch no.6 train no.440800  loss = 3.66793 avg_loss = 2.81375\n",
      "epoch no.6 train no.440810  loss = 3.75635 avg_loss = 2.81966\n",
      "epoch no.6 train no.440820  loss = 1.98827 avg_loss = 2.84316\n",
      "epoch no.6 train no.440830  loss = 2.88384 avg_loss = 2.83680\n",
      "epoch no.6 train no.440840  loss = 3.18493 avg_loss = 2.85849\n",
      "epoch no.6 train no.440850  loss = 2.39883 avg_loss = 2.87020\n",
      "epoch no.6 train no.440860  loss = 2.57918 avg_loss = 2.86454\n",
      "epoch no.6 train no.440870  loss = 2.56354 avg_loss = 2.88594\n",
      "epoch no.6 train no.440880  loss = 3.79330 avg_loss = 2.88714\n",
      "epoch no.6 train no.440890  loss = 3.69075 avg_loss = 2.89900\n",
      "epoch no.6 train no.440900  loss = 1.77115 avg_loss = 2.91395\n",
      "epoch no.6 train no.440910  loss = 3.35895 avg_loss = 2.88250\n",
      "epoch no.6 train no.440920  loss = 2.00072 avg_loss = 2.87736\n",
      "epoch no.6 train no.440930  loss = 2.89919 avg_loss = 2.90910\n",
      "epoch no.6 train no.440940  loss = 3.02164 avg_loss = 2.87627\n",
      "epoch no.6 train no.440950  loss = 2.31486 avg_loss = 2.86964\n",
      "epoch no.6 train no.440960  loss = 2.48917 avg_loss = 2.85965\n",
      "epoch no.6 train no.440970  loss = 2.01542 avg_loss = 2.84105\n",
      "epoch no.6 train no.440980  loss = 3.59163 avg_loss = 2.84734\n",
      "epoch no.6 train no.440990  loss = 2.55612 avg_loss = 2.83923\n",
      "epoch no.6 train no.441000  loss = 3.32358 avg_loss = 2.85326\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '팝', '</s>']\n",
      "추억의 90년대 댄스팝</s>\n",
      "epoch no.6 train no.441010  loss = 2.83706 avg_loss = 2.86643\n",
      "epoch no.6 train no.441020  loss = 3.14135 avg_loss = 2.85328\n",
      "epoch no.6 train no.441030  loss = 2.77154 avg_loss = 2.86158\n",
      "epoch no.6 train no.441040  loss = 3.32808 avg_loss = 2.89096\n",
      "epoch no.6 train no.441050  loss = 2.80080 avg_loss = 2.87103\n",
      "epoch no.6 train no.441060  loss = 6.03395 avg_loss = 2.88468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.441070  loss = 2.29736 avg_loss = 2.86439\n",
      "epoch no.6 train no.441080  loss = 2.86997 avg_loss = 2.85547\n",
      "epoch no.6 train no.441090  loss = 4.66453 avg_loss = 2.85990\n",
      "epoch no.6 train no.441100  loss = 3.99884 avg_loss = 2.88757\n",
      "epoch no.6 train no.441110  loss = 2.78544 avg_loss = 2.88883\n",
      "epoch no.6 train no.441120  loss = 2.71311 avg_loss = 2.90149\n",
      "epoch no.6 train no.441130  loss = 2.76880 avg_loss = 2.89975\n",
      "epoch no.6 train no.441140  loss = 3.00176 avg_loss = 2.85667\n",
      "epoch no.6 train no.441150  loss = 3.22510 avg_loss = 2.83521\n",
      "epoch no.6 train no.441160  loss = 2.58049 avg_loss = 2.85238\n",
      "epoch no.6 train no.441170  loss = 3.33416 avg_loss = 2.85527\n",
      "epoch no.6 train no.441180  loss = 2.86777 avg_loss = 2.88518\n",
      "epoch no.6 train no.441190  loss = 4.54288 avg_loss = 2.89367\n",
      "epoch no.6 train no.441200  loss = 2.11437 avg_loss = 2.88282\n",
      "epoch no.6 train no.441210  loss = 4.19019 avg_loss = 2.91988\n",
      "epoch no.6 train no.441220  loss = 2.85884 avg_loss = 2.90343\n",
      "epoch no.6 train no.441230  loss = 2.29137 avg_loss = 2.90400\n",
      "epoch no.6 train no.441240  loss = 2.13107 avg_loss = 2.87553\n",
      "epoch no.6 train no.441250  loss = 2.77077 avg_loss = 2.86351\n",
      "epoch no.6 train no.441260  loss = 2.27534 avg_loss = 2.88286\n",
      "epoch no.6 train no.441270  loss = 2.05549 avg_loss = 2.83157\n",
      "epoch no.6 train no.441280  loss = 3.28702 avg_loss = 2.86061\n",
      "epoch no.6 train no.441290  loss = 1.55200 avg_loss = 2.84615\n",
      "epoch no.6 train no.441300  loss = 2.66149 avg_loss = 2.85423\n",
      "epoch no.6 train no.441310  loss = 2.76750 avg_loss = 2.90530\n",
      "epoch no.6 train no.441320  loss = 3.88375 avg_loss = 2.92972\n",
      "epoch no.6 train no.441330  loss = 2.88966 avg_loss = 2.91702\n",
      "epoch no.6 train no.441340  loss = 2.16965 avg_loss = 2.91100\n",
      "epoch no.6 train no.441350  loss = 2.56849 avg_loss = 2.88621\n",
      "epoch no.6 train no.441360  loss = 3.08558 avg_loss = 2.88120\n",
      "epoch no.6 train no.441370  loss = 3.67051 avg_loss = 2.88573\n",
      "epoch no.6 train no.441380  loss = 1.92259 avg_loss = 2.90939\n",
      "epoch no.6 train no.441390  loss = 3.11743 avg_loss = 2.89371\n",
      "epoch no.6 train no.441400  loss = 2.38936 avg_loss = 2.85953\n",
      "epoch no.6 train no.441410  loss = 2.37663 avg_loss = 2.85924\n",
      "epoch no.6 train no.441420  loss = 2.45858 avg_loss = 2.84236\n",
      "epoch no.6 train no.441430  loss = 2.29273 avg_loss = 2.86847\n",
      "epoch no.6 train no.441440  loss = 2.40897 avg_loss = 2.84516\n",
      "epoch no.6 train no.441450  loss = 2.66830 avg_loss = 2.86711\n",
      "epoch no.6 train no.441460  loss = 3.18217 avg_loss = 2.85765\n",
      "epoch no.6 train no.441470  loss = 1.89102 avg_loss = 2.83028\n",
      "epoch no.6 train no.441480  loss = 3.38065 avg_loss = 2.81032\n",
      "epoch no.6 train no.441490  loss = 3.70951 avg_loss = 2.80374\n",
      "epoch no.6 train no.441500  loss = 2.64740 avg_loss = 2.76210\n",
      "epoch no.6 train no.441510  loss = 3.15202 avg_loss = 2.73097\n",
      "epoch no.6 train no.441520  loss = 3.36676 avg_loss = 2.74230\n",
      "epoch no.6 train no.441530  loss = 3.00884 avg_loss = 2.74530\n",
      "epoch no.6 train no.441540  loss = 3.47924 avg_loss = 2.79657\n",
      "epoch no.6 train no.441550  loss = 2.74440 avg_loss = 2.79120\n",
      "epoch no.6 train no.441560  loss = 2.40915 avg_loss = 2.77052\n",
      "epoch no.6 train no.441570  loss = 2.29963 avg_loss = 2.79485\n",
      "epoch no.6 train no.441580  loss = 2.65944 avg_loss = 2.83945\n",
      "epoch no.6 train no.441590  loss = 2.01232 avg_loss = 2.82139\n",
      "epoch no.6 train no.441600  loss = 4.17711 avg_loss = 2.88093\n",
      "epoch no.6 train no.441610  loss = 3.14385 avg_loss = 2.91299\n",
      "epoch no.6 train no.441620  loss = 2.43289 avg_loss = 2.95843\n",
      "epoch no.6 train no.441630  loss = 2.30926 avg_loss = 2.95242\n",
      "epoch no.6 train no.441640  loss = 2.48978 avg_loss = 2.94630\n",
      "epoch no.6 train no.441650  loss = 2.14801 avg_loss = 2.96638\n",
      "epoch no.6 train no.441660  loss = 2.23007 avg_loss = 2.92801\n",
      "epoch no.6 train no.441670  loss = 3.34533 avg_loss = 2.91292\n",
      "epoch no.6 train no.441680  loss = 2.70034 avg_loss = 2.89275\n",
      "epoch no.6 train no.441690  loss = 2.97582 avg_loss = 2.89928\n",
      "epoch no.6 train no.441700  loss = 1.42768 avg_loss = 2.88613\n",
      "epoch no.6 train no.441710  loss = 3.64463 avg_loss = 2.87692\n",
      "epoch no.6 train no.441720  loss = 2.49221 avg_loss = 2.86648\n",
      "epoch no.6 train no.441730  loss = 2.85199 avg_loss = 2.86769\n",
      "epoch no.6 train no.441740  loss = 3.69483 avg_loss = 2.91167\n",
      "epoch no.6 train no.441750  loss = 2.17215 avg_loss = 2.91208\n",
      "epoch no.6 train no.441760  loss = 3.17480 avg_loss = 2.88677\n",
      "epoch no.6 train no.441770  loss = 3.15659 avg_loss = 2.85345\n",
      "epoch no.6 train no.441780  loss = 2.46088 avg_loss = 2.86489\n",
      "epoch no.6 train no.441790  loss = 2.73522 avg_loss = 2.87056\n",
      "epoch no.6 train no.441800  loss = 3.83519 avg_loss = 2.86456\n",
      "epoch no.6 train no.441810  loss = 2.15466 avg_loss = 2.88600\n",
      "epoch no.6 train no.441820  loss = 3.11406 avg_loss = 2.86693\n",
      "epoch no.6 train no.441830  loss = 2.65715 avg_loss = 2.85901\n",
      "epoch no.6 train no.441840  loss = 3.23557 avg_loss = 2.86486\n",
      "epoch no.6 train no.441850  loss = 3.73086 avg_loss = 2.86857\n",
      "epoch no.6 train no.441860  loss = 2.95359 avg_loss = 2.89764\n",
      "epoch no.6 train no.441870  loss = 1.83132 avg_loss = 2.88991\n",
      "epoch no.6 train no.441880  loss = 1.56150 avg_loss = 2.85650\n",
      "epoch no.6 train no.441890  loss = 2.74400 avg_loss = 2.85407\n",
      "epoch no.6 train no.441900  loss = 2.43256 avg_loss = 2.84595\n",
      "epoch no.6 train no.441910  loss = 2.78336 avg_loss = 2.85434\n",
      "epoch no.6 train no.441920  loss = 4.31611 avg_loss = 2.86686\n",
      "epoch no.6 train no.441930  loss = 3.25306 avg_loss = 2.84693\n",
      "epoch no.6 train no.441940  loss = 1.91572 avg_loss = 2.82858\n",
      "epoch no.6 train no.441950  loss = 2.94418 avg_loss = 2.84226\n",
      "epoch no.6 train no.441960  loss = 2.44351 avg_loss = 2.81046\n",
      "epoch no.6 train no.441970  loss = 2.78315 avg_loss = 2.82870\n",
      "epoch no.6 train no.441980  loss = 3.17405 avg_loss = 2.80745\n",
      "epoch no.6 train no.441990  loss = 2.79257 avg_loss = 2.80114\n",
      "epoch no.6 train no.442000  loss = 3.25922 avg_loss = 2.80395\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '드', '▁베스트', '곡', '</s>', '</s>']\n",
      "추억의 발라드 명곡 모음</s>\n",
      "epoch no.6 train no.442010  loss = 2.08650 avg_loss = 2.77264\n",
      "epoch no.6 train no.442020  loss = 2.08800 avg_loss = 2.75989\n",
      "epoch no.6 train no.442030  loss = 3.62480 avg_loss = 2.77507\n",
      "epoch no.6 train no.442040  loss = 2.51836 avg_loss = 2.76791\n",
      "epoch no.6 train no.442050  loss = 3.72821 avg_loss = 2.79533\n",
      "epoch no.6 train no.442060  loss = 2.56415 avg_loss = 2.80013\n",
      "epoch no.6 train no.442070  loss = 3.73958 avg_loss = 2.78647\n",
      "epoch no.6 train no.442080  loss = 3.11578 avg_loss = 2.77807\n",
      "epoch no.6 train no.442090  loss = 3.24239 avg_loss = 2.82547\n",
      "epoch no.6 train no.442100  loss = 3.04332 avg_loss = 2.80262\n",
      "epoch no.6 train no.442110  loss = 3.10137 avg_loss = 2.81199\n",
      "epoch no.6 train no.442120  loss = 2.85146 avg_loss = 2.82863\n",
      "epoch no.6 train no.442130  loss = 1.88931 avg_loss = 2.84052\n",
      "epoch no.6 train no.442140  loss = 1.90498 avg_loss = 2.86188\n",
      "epoch no.6 train no.442150  loss = 2.66352 avg_loss = 2.86243\n",
      "epoch no.6 train no.442160  loss = 2.12512 avg_loss = 2.85931\n",
      "epoch no.6 train no.442170  loss = 2.11686 avg_loss = 2.84455\n",
      "epoch no.6 train no.442180  loss = 3.19690 avg_loss = 2.84831\n",
      "epoch no.6 train no.442190  loss = 1.75841 avg_loss = 2.86623\n",
      "epoch no.6 train no.442200  loss = 2.21454 avg_loss = 2.82446\n",
      "epoch no.6 train no.442210  loss = 3.48595 avg_loss = 2.85160\n",
      "epoch no.6 train no.442220  loss = 2.52440 avg_loss = 2.83575\n",
      "epoch no.6 train no.442230  loss = 2.85011 avg_loss = 2.82282\n",
      "epoch no.6 train no.442240  loss = 2.30855 avg_loss = 2.79053\n",
      "epoch no.6 train no.442250  loss = 3.06076 avg_loss = 2.78867\n",
      "epoch no.6 train no.442260  loss = 2.31596 avg_loss = 2.78561\n",
      "epoch no.6 train no.442270  loss = 5.32291 avg_loss = 2.79201\n",
      "epoch no.6 train no.442280  loss = 2.31739 avg_loss = 2.78217\n",
      "epoch no.6 train no.442290  loss = 2.73636 avg_loss = 2.80726\n",
      "epoch no.6 train no.442300  loss = 2.36717 avg_loss = 2.79332\n",
      "epoch no.6 train no.442310  loss = 2.50449 avg_loss = 2.78282\n",
      "epoch no.6 train no.442320  loss = 3.68770 avg_loss = 2.78092\n",
      "epoch no.6 train no.442330  loss = 2.90589 avg_loss = 2.79018\n",
      "epoch no.6 train no.442340  loss = 1.99462 avg_loss = 2.78655\n",
      "epoch no.6 train no.442350  loss = 1.67393 avg_loss = 2.74258\n",
      "epoch no.6 train no.442360  loss = 2.66486 avg_loss = 2.70715\n",
      "epoch no.6 train no.442370  loss = 2.61448 avg_loss = 2.67541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.442380  loss = 2.87552 avg_loss = 2.67832\n",
      "epoch no.6 train no.442390  loss = 2.05993 avg_loss = 2.69239\n",
      "epoch no.6 train no.442400  loss = 3.37160 avg_loss = 2.73960\n",
      "epoch no.6 train no.442410  loss = 3.09882 avg_loss = 2.72096\n",
      "epoch no.6 train no.442420  loss = 2.74926 avg_loss = 2.71045\n",
      "epoch no.6 train no.442430  loss = 2.87349 avg_loss = 2.73756\n",
      "epoch no.6 train no.442440  loss = 3.87322 avg_loss = 2.73889\n",
      "epoch no.6 train no.442450  loss = 2.10839 avg_loss = 2.72512\n",
      "epoch no.6 train no.442460  loss = 2.82501 avg_loss = 2.72750\n",
      "epoch no.6 train no.442470  loss = 1.99337 avg_loss = 2.72816\n",
      "epoch no.6 train no.442480  loss = 3.08405 avg_loss = 2.75984\n",
      "epoch no.6 train no.442490  loss = 3.78845 avg_loss = 2.75963\n",
      "epoch no.6 train no.442500  loss = 3.81372 avg_loss = 2.74255\n",
      "epoch no.6 train no.442510  loss = 2.58001 avg_loss = 2.72923\n",
      "epoch no.6 train no.442520  loss = 2.98173 avg_loss = 2.70543\n",
      "epoch no.6 train no.442530  loss = 3.13609 avg_loss = 2.71623\n",
      "epoch no.6 train no.442540  loss = 2.15419 avg_loss = 2.69949\n",
      "epoch no.6 train no.442550  loss = 2.24018 avg_loss = 2.71588\n",
      "epoch no.6 train no.442560  loss = 2.74275 avg_loss = 2.73210\n",
      "epoch no.6 train no.442570  loss = 2.46056 avg_loss = 2.75546\n",
      "epoch no.6 train no.442580  loss = 3.29707 avg_loss = 2.76367\n",
      "epoch no.6 train no.442590  loss = 2.28043 avg_loss = 2.71579\n",
      "epoch no.6 train no.442600  loss = 2.12964 avg_loss = 2.72786\n",
      "epoch no.6 train no.442610  loss = 2.93252 avg_loss = 2.71360\n",
      "epoch no.6 train no.442620  loss = 3.52768 avg_loss = 2.76674\n",
      "epoch no.6 train no.442630  loss = 5.36152 avg_loss = 2.80257\n",
      "epoch no.6 train no.442640  loss = 2.56999 avg_loss = 2.75489\n",
      "epoch no.6 train no.442650  loss = 3.55546 avg_loss = 2.77698\n",
      "epoch no.6 train no.442660  loss = 1.62370 avg_loss = 2.78761\n",
      "epoch no.6 train no.442670  loss = 2.94237 avg_loss = 2.82090\n",
      "epoch no.6 train no.442680  loss = 2.58549 avg_loss = 2.84251\n",
      "epoch no.6 train no.442690  loss = 3.79141 avg_loss = 2.82781\n",
      "epoch no.6 train no.442700  loss = 2.77234 avg_loss = 2.81906\n",
      "epoch no.6 train no.442710  loss = 2.81096 avg_loss = 2.81627\n",
      "epoch no.6 train no.442720  loss = 2.43291 avg_loss = 2.84075\n",
      "epoch no.6 train no.442730  loss = 5.54710 avg_loss = 2.88087\n",
      "epoch no.6 train no.442740  loss = 3.31140 avg_loss = 2.87415\n",
      "epoch no.6 train no.442750  loss = 3.24899 avg_loss = 2.87107\n",
      "epoch no.6 train no.442760  loss = 4.44378 avg_loss = 2.83693\n",
      "epoch no.6 train no.442770  loss = 2.36798 avg_loss = 2.86772\n",
      "epoch no.6 train no.442780  loss = 4.20121 avg_loss = 2.88156\n",
      "epoch no.6 train no.442790  loss = 4.30127 avg_loss = 2.89966\n",
      "epoch no.6 train no.442800  loss = 1.77176 avg_loss = 2.88582\n",
      "epoch no.6 train no.442810  loss = 2.44119 avg_loss = 2.88916\n",
      "epoch no.6 train no.442820  loss = 3.30505 avg_loss = 2.86895\n",
      "epoch no.6 train no.442830  loss = 3.39094 avg_loss = 2.84022\n",
      "epoch no.6 train no.442840  loss = 3.31252 avg_loss = 2.81658\n",
      "epoch no.6 train no.442850  loss = 2.35237 avg_loss = 2.81027\n",
      "epoch no.6 train no.442860  loss = 3.49774 avg_loss = 2.81495\n",
      "epoch no.6 train no.442870  loss = 2.43498 avg_loss = 2.79317\n",
      "epoch no.6 train no.442880  loss = 2.67022 avg_loss = 2.78378\n",
      "epoch no.6 train no.442890  loss = 4.26680 avg_loss = 2.80686\n",
      "epoch no.6 train no.442900  loss = 5.85333 avg_loss = 2.87583\n",
      "epoch no.6 train no.442910  loss = 4.46819 avg_loss = 2.84574\n",
      "epoch no.6 train no.442920  loss = 3.64849 avg_loss = 2.84629\n",
      "epoch no.6 train no.442930  loss = 1.75933 avg_loss = 2.83215\n",
      "epoch no.6 train no.442940  loss = 2.86110 avg_loss = 2.83227\n",
      "epoch no.6 train no.442950  loss = 4.25052 avg_loss = 2.85741\n",
      "epoch no.6 train no.442960  loss = 2.95293 avg_loss = 2.82704\n",
      "epoch no.6 train no.442970  loss = 3.50702 avg_loss = 2.84795\n",
      "epoch no.6 train no.442980  loss = 2.05994 avg_loss = 2.83725\n",
      "epoch no.6 train no.442990  loss = 3.05482 avg_loss = 2.80259\n",
      "epoch no.6 train no.443000  loss = 3.65778 avg_loss = 2.79609\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '월드', '▁b', '음악', '</s>']\n",
      "추억의 싸이월드 배경음악</s>\n",
      "epoch no.6 train no.443010  loss = 2.76092 avg_loss = 2.82642\n",
      "epoch no.6 train no.443020  loss = 3.16710 avg_loss = 2.80005\n",
      "epoch no.6 train no.443030  loss = 2.33780 avg_loss = 2.76962\n",
      "epoch no.6 train no.443040  loss = 2.82049 avg_loss = 2.79094\n",
      "epoch no.6 train no.443050  loss = 2.03302 avg_loss = 2.80503\n",
      "epoch no.6 train no.443060  loss = 3.31123 avg_loss = 2.80915\n",
      "epoch no.6 train no.443070  loss = 3.38170 avg_loss = 2.79680\n",
      "epoch no.6 train no.443080  loss = 3.33910 avg_loss = 2.84441\n",
      "epoch no.6 train no.443090  loss = 2.62216 avg_loss = 2.85640\n",
      "epoch no.6 train no.443100  loss = 2.95168 avg_loss = 2.86137\n",
      "epoch no.6 train no.443110  loss = 2.77217 avg_loss = 2.87901\n",
      "epoch no.6 train no.443120  loss = 2.35974 avg_loss = 2.88680\n",
      "epoch no.6 train no.443130  loss = 5.51362 avg_loss = 2.87580\n",
      "epoch no.6 train no.443140  loss = 1.77087 avg_loss = 2.87722\n",
      "epoch no.6 train no.443150  loss = 3.70882 avg_loss = 2.85096\n",
      "epoch no.6 train no.443160  loss = 2.25477 avg_loss = 2.83752\n",
      "epoch no.6 train no.443170  loss = 3.36049 avg_loss = 2.84676\n",
      "epoch no.6 train no.443180  loss = 2.26562 avg_loss = 2.80321\n",
      "epoch no.6 train no.443190  loss = 4.21035 avg_loss = 2.82463\n",
      "epoch no.6 train no.443200  loss = 1.91231 avg_loss = 2.83748\n",
      "epoch no.6 train no.443210  loss = 2.44900 avg_loss = 2.84997\n",
      "epoch no.6 train no.443220  loss = 2.39521 avg_loss = 2.81761\n",
      "epoch no.6 train no.443230  loss = 2.30356 avg_loss = 2.83013\n",
      "epoch no.6 train no.443240  loss = 3.42335 avg_loss = 2.82752\n",
      "epoch no.6 train no.443250  loss = 3.46410 avg_loss = 2.82907\n",
      "epoch no.6 train no.443260  loss = 3.02461 avg_loss = 2.80612\n",
      "epoch no.6 train no.443270  loss = 3.19566 avg_loss = 2.81120\n",
      "epoch no.6 train no.443280  loss = 2.56935 avg_loss = 2.82052\n",
      "epoch no.6 train no.443290  loss = 2.70283 avg_loss = 2.81215\n",
      "epoch no.6 train no.443300  loss = 2.78196 avg_loss = 2.79614\n",
      "epoch no.6 train no.443310  loss = 2.54670 avg_loss = 2.78735\n",
      "epoch no.6 train no.443320  loss = 2.71258 avg_loss = 2.76292\n",
      "epoch no.6 train no.443330  loss = 3.00720 avg_loss = 2.78676\n",
      "epoch no.6 train no.443340  loss = 3.08682 avg_loss = 2.78612\n",
      "epoch no.6 train no.443350  loss = 2.11184 avg_loss = 2.77437\n",
      "epoch no.6 train no.443360  loss = 3.43030 avg_loss = 2.76323\n",
      "epoch no.6 train no.443370  loss = 2.97253 avg_loss = 2.76932\n",
      "epoch no.6 train no.443380  loss = 2.83005 avg_loss = 2.76220\n",
      "epoch no.6 train no.443390  loss = 2.08839 avg_loss = 2.74654\n",
      "epoch no.6 train no.443400  loss = 2.53740 avg_loss = 2.70294\n",
      "epoch no.6 train no.443410  loss = 2.11843 avg_loss = 2.70163\n",
      "epoch no.6 train no.443420  loss = 4.53965 avg_loss = 2.74179\n",
      "epoch no.6 train no.443430  loss = 3.11192 avg_loss = 2.76237\n",
      "epoch no.6 train no.443440  loss = 2.47368 avg_loss = 2.70960\n",
      "epoch no.6 train no.443450  loss = 1.99793 avg_loss = 2.73352\n",
      "epoch no.6 train no.443460  loss = 2.11878 avg_loss = 2.74452\n",
      "epoch no.6 train no.443470  loss = 2.77459 avg_loss = 2.76644\n",
      "epoch no.6 train no.443480  loss = 4.14691 avg_loss = 2.79616\n",
      "epoch no.6 train no.443490  loss = 3.54857 avg_loss = 2.77243\n",
      "epoch no.6 train no.443500  loss = 2.20819 avg_loss = 2.77584\n",
      "epoch no.6 train no.443510  loss = 2.15361 avg_loss = 2.76651\n",
      "epoch no.6 train no.443520  loss = 3.26006 avg_loss = 2.82089\n",
      "epoch no.6 train no.443530  loss = 2.65901 avg_loss = 2.79598\n",
      "epoch no.6 train no.443540  loss = 3.12662 avg_loss = 2.80288\n",
      "epoch no.6 train no.443550  loss = 1.92609 avg_loss = 2.84133\n",
      "epoch no.6 train no.443560  loss = 2.95157 avg_loss = 2.82324\n",
      "epoch no.6 train no.443570  loss = 2.55067 avg_loss = 2.83010\n",
      "epoch no.6 train no.443580  loss = 4.02396 avg_loss = 2.81409\n",
      "epoch no.6 train no.443590  loss = 2.80802 avg_loss = 2.84217\n",
      "epoch no.6 train no.443600  loss = 3.38351 avg_loss = 2.84714\n",
      "epoch no.6 train no.443610  loss = 1.89542 avg_loss = 2.82408\n",
      "epoch no.6 train no.443620  loss = 2.63565 avg_loss = 2.79508\n",
      "epoch no.6 train no.443630  loss = 3.03692 avg_loss = 2.80222\n",
      "epoch no.6 train no.443640  loss = 2.76477 avg_loss = 2.81771\n",
      "epoch no.6 train no.443650  loss = 3.40229 avg_loss = 2.84980\n",
      "epoch no.6 train no.443660  loss = 3.31021 avg_loss = 2.85117\n",
      "epoch no.6 train no.443670  loss = 3.96888 avg_loss = 2.85806\n",
      "epoch no.6 train no.443680  loss = 2.49885 avg_loss = 2.86577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.443690  loss = 4.60925 avg_loss = 2.88927\n",
      "epoch no.6 train no.443700  loss = 3.00793 avg_loss = 2.91299\n",
      "epoch no.6 train no.443710  loss = 1.97854 avg_loss = 2.89075\n",
      "epoch no.6 train no.443720  loss = 2.78177 avg_loss = 2.90320\n",
      "epoch no.6 train no.443730  loss = 2.46111 avg_loss = 2.85255\n",
      "epoch no.6 train no.443740  loss = 1.96829 avg_loss = 2.84631\n",
      "epoch no.6 train no.443750  loss = 2.83067 avg_loss = 2.83679\n",
      "epoch no.6 train no.443760  loss = 2.73722 avg_loss = 2.82770\n",
      "epoch no.6 train no.443770  loss = 2.31038 avg_loss = 2.78069\n",
      "epoch no.6 train no.443780  loss = 2.93558 avg_loss = 2.78963\n",
      "epoch no.6 train no.443790  loss = 2.09139 avg_loss = 2.80849\n",
      "epoch no.6 train no.443800  loss = 3.24539 avg_loss = 2.80885\n",
      "epoch no.6 train no.443810  loss = 2.52132 avg_loss = 2.80847\n",
      "epoch no.6 train no.443820  loss = 2.60193 avg_loss = 2.79131\n",
      "epoch no.6 train no.443830  loss = 3.52718 avg_loss = 2.78782\n",
      "epoch no.6 train no.443840  loss = 2.53897 avg_loss = 2.76054\n",
      "epoch no.6 train no.443850  loss = 2.45371 avg_loss = 2.75934\n",
      "epoch no.6 train no.443860  loss = 2.83770 avg_loss = 2.75044\n",
      "epoch no.6 train no.443870  loss = 2.79571 avg_loss = 2.70916\n",
      "epoch no.6 train no.443880  loss = 2.25332 avg_loss = 2.75869\n",
      "epoch no.6 train no.443890  loss = 3.16730 avg_loss = 2.76557\n",
      "epoch no.6 train no.443900  loss = 2.78102 avg_loss = 2.78032\n",
      "epoch no.6 train no.443910  loss = 2.99145 avg_loss = 2.75291\n",
      "epoch no.6 train no.443920  loss = 3.36634 avg_loss = 2.77432\n",
      "epoch no.6 train no.443930  loss = 2.72090 avg_loss = 2.76198\n",
      "epoch no.6 train no.443940  loss = 3.11211 avg_loss = 2.78172\n",
      "epoch no.6 train no.443950  loss = 1.70828 avg_loss = 2.77102\n",
      "epoch no.6 train no.443960  loss = 3.65165 avg_loss = 2.77641\n",
      "epoch no.6 train no.443970  loss = 2.58553 avg_loss = 2.78824\n",
      "epoch no.6 train no.443980  loss = 3.29920 avg_loss = 2.80978\n",
      "epoch no.6 train no.443990  loss = 3.27554 avg_loss = 2.82362\n",
      "epoch no.6 train no.444000  loss = 3.14431 avg_loss = 2.79077\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '팝', '</s>']\n",
      "추억의 90년대 댄스팝</s>\n",
      "epoch no.6 train no.444010  loss = 2.27311 avg_loss = 2.81084\n",
      "epoch no.6 train no.444020  loss = 1.60067 avg_loss = 2.79887\n",
      "epoch no.6 train no.444030  loss = 4.20272 avg_loss = 2.78622\n",
      "epoch no.6 train no.444040  loss = 2.34208 avg_loss = 2.74065\n",
      "epoch no.6 train no.444050  loss = 2.25441 avg_loss = 2.75636\n",
      "epoch no.6 train no.444060  loss = 2.43471 avg_loss = 2.72084\n",
      "epoch no.6 train no.444070  loss = 2.52197 avg_loss = 2.72900\n",
      "epoch no.6 train no.444080  loss = 4.50493 avg_loss = 2.76397\n",
      "epoch no.6 train no.444090  loss = 3.70739 avg_loss = 2.79310\n",
      "epoch no.6 train no.444100  loss = 2.05745 avg_loss = 2.78452\n",
      "epoch no.6 train no.444110  loss = 1.86962 avg_loss = 2.75640\n",
      "epoch no.6 train no.444120  loss = 2.07298 avg_loss = 2.74100\n",
      "epoch no.6 train no.444130  loss = 2.48937 avg_loss = 2.71070\n",
      "epoch no.6 train no.444140  loss = 2.54752 avg_loss = 2.75401\n",
      "epoch no.6 train no.444150  loss = 2.09990 avg_loss = 2.74976\n",
      "epoch no.6 train no.444160  loss = 3.37399 avg_loss = 2.76051\n",
      "epoch no.6 train no.444170  loss = 2.61361 avg_loss = 2.74258\n",
      "epoch no.6 train no.444180  loss = 2.75296 avg_loss = 2.74719\n",
      "epoch no.6 train no.444190  loss = 3.41128 avg_loss = 2.76674\n",
      "epoch no.6 train no.444200  loss = 2.56944 avg_loss = 2.75738\n",
      "epoch no.6 train no.444210  loss = 2.92202 avg_loss = 2.72687\n",
      "epoch no.6 train no.444220  loss = 2.04906 avg_loss = 2.71563\n",
      "epoch no.6 train no.444230  loss = 2.01259 avg_loss = 2.69950\n",
      "epoch no.6 train no.444240  loss = 3.21090 avg_loss = 2.70203\n",
      "epoch no.6 train no.444250  loss = 4.33687 avg_loss = 2.73032\n",
      "epoch no.6 train no.444260  loss = 2.00322 avg_loss = 2.74166\n",
      "epoch no.6 train no.444270  loss = 3.69228 avg_loss = 2.72914\n",
      "epoch no.6 train no.444280  loss = 2.51957 avg_loss = 2.74312\n",
      "epoch no.6 train no.444290  loss = 3.95161 avg_loss = 2.74124\n",
      "epoch no.6 train no.444300  loss = 3.92382 avg_loss = 2.75470\n",
      "epoch no.6 train no.444310  loss = 2.83286 avg_loss = 2.73893\n",
      "epoch no.6 train no.444320  loss = 1.90639 avg_loss = 2.76179\n",
      "epoch no.6 train no.444330  loss = 3.94520 avg_loss = 2.76586\n",
      "epoch no.6 train no.444340  loss = 2.19304 avg_loss = 2.76314\n",
      "epoch no.6 train no.444350  loss = 2.96923 avg_loss = 2.78066\n",
      "epoch no.6 train no.444360  loss = 2.66782 avg_loss = 2.77858\n",
      "epoch no.6 train no.444370  loss = 2.52651 avg_loss = 2.77770\n",
      "epoch no.6 train no.444380  loss = 3.76619 avg_loss = 2.77761\n",
      "epoch no.6 train no.444390  loss = 3.38589 avg_loss = 2.83221\n",
      "epoch no.6 train no.444400  loss = 2.81209 avg_loss = 2.86901\n",
      "epoch no.6 train no.444410  loss = 3.48759 avg_loss = 2.86985\n",
      "epoch no.6 train no.444420  loss = 3.25442 avg_loss = 2.86129\n",
      "epoch no.6 train no.444430  loss = 3.66254 avg_loss = 2.83856\n",
      "epoch no.6 train no.444440  loss = 3.66011 avg_loss = 2.81478\n",
      "epoch no.6 train no.444450  loss = 2.28429 avg_loss = 2.80080\n",
      "epoch no.6 train no.444460  loss = 4.16934 avg_loss = 2.81986\n",
      "epoch no.6 train no.444470  loss = 3.08926 avg_loss = 2.82728\n",
      "epoch no.6 train no.444480  loss = 2.12423 avg_loss = 2.82686\n",
      "epoch no.6 train no.444490  loss = 2.22932 avg_loss = 2.79293\n",
      "epoch no.6 train no.444500  loss = 2.27834 avg_loss = 2.78682\n",
      "epoch no.6 train no.444510  loss = 1.75654 avg_loss = 2.80060\n",
      "epoch no.6 train no.444520  loss = 1.70386 avg_loss = 2.78321\n",
      "epoch no.6 train no.444530  loss = 3.72123 avg_loss = 2.78896\n",
      "epoch no.6 train no.444540  loss = 1.81731 avg_loss = 2.76130\n",
      "epoch no.6 train no.444550  loss = 2.92709 avg_loss = 2.76546\n",
      "epoch no.6 train no.444560  loss = 2.29392 avg_loss = 2.79911\n",
      "epoch no.6 train no.444570  loss = 2.68180 avg_loss = 2.80157\n",
      "epoch no.6 train no.444580  loss = 2.36085 avg_loss = 2.81847\n",
      "epoch no.6 train no.444590  loss = 2.25051 avg_loss = 2.80070\n",
      "epoch no.6 train no.444600  loss = 1.83524 avg_loss = 2.77623\n",
      "epoch no.6 train no.444610  loss = 2.51104 avg_loss = 2.79028\n",
      "epoch no.6 train no.444620  loss = 2.57403 avg_loss = 2.78120\n",
      "epoch no.6 train no.444630  loss = 1.68749 avg_loss = 2.79285\n",
      "epoch no.6 train no.444640  loss = 3.21758 avg_loss = 2.78915\n",
      "epoch no.6 train no.444650  loss = 3.43521 avg_loss = 2.80736\n",
      "epoch no.6 train no.444660  loss = 3.14147 avg_loss = 2.80719\n",
      "epoch no.6 train no.444670  loss = 1.97148 avg_loss = 2.80658\n",
      "epoch no.6 train no.444680  loss = 2.37007 avg_loss = 2.78976\n",
      "epoch no.6 train no.444690  loss = 3.59779 avg_loss = 2.80829\n",
      "epoch no.6 train no.444700  loss = 5.05988 avg_loss = 2.81494\n",
      "epoch no.6 train no.444710  loss = 3.21974 avg_loss = 2.78298\n",
      "epoch no.6 train no.444720  loss = 1.98343 avg_loss = 2.78148\n",
      "epoch no.6 train no.444730  loss = 2.98168 avg_loss = 2.79909\n",
      "epoch no.6 train no.444740  loss = 2.57941 avg_loss = 2.84950\n",
      "epoch no.6 train no.444750  loss = 2.43874 avg_loss = 2.83767\n",
      "epoch no.6 train no.444760  loss = 4.42943 avg_loss = 2.89458\n",
      "epoch no.6 train no.444770  loss = 3.96641 avg_loss = 2.87982\n",
      "epoch no.6 train no.444780  loss = 2.24768 avg_loss = 2.85183\n",
      "epoch no.6 train no.444790  loss = 2.85700 avg_loss = 2.85782\n",
      "epoch no.6 train no.444800  loss = 2.59172 avg_loss = 2.87273\n",
      "epoch no.6 train no.444810  loss = 3.23485 avg_loss = 2.85113\n",
      "epoch no.6 train no.444820  loss = 2.68771 avg_loss = 2.87346\n",
      "epoch no.6 train no.444830  loss = 2.48180 avg_loss = 2.83993\n",
      "epoch no.6 train no.444840  loss = 2.41841 avg_loss = 2.81558\n",
      "epoch no.6 train no.444850  loss = 3.05105 avg_loss = 2.81775\n",
      "epoch no.6 train no.444860  loss = 1.97351 avg_loss = 2.82146\n",
      "epoch no.6 train no.444870  loss = 2.68612 avg_loss = 2.78062\n",
      "epoch no.6 train no.444880  loss = 3.53123 avg_loss = 2.75661\n",
      "epoch no.6 train no.444890  loss = 3.13667 avg_loss = 2.78977\n",
      "epoch no.6 train no.444900  loss = 3.65159 avg_loss = 2.81937\n",
      "epoch no.6 train no.444910  loss = 3.37351 avg_loss = 2.81034\n",
      "epoch no.6 train no.444920  loss = 2.63063 avg_loss = 2.78330\n",
      "epoch no.6 train no.444930  loss = 3.66005 avg_loss = 2.81353\n",
      "epoch no.6 train no.444940  loss = 3.03851 avg_loss = 2.80506\n",
      "epoch no.6 train no.444950  loss = 1.29346 avg_loss = 2.78451\n",
      "epoch no.6 train no.444960  loss = 2.48257 avg_loss = 2.80414\n",
      "epoch no.6 train no.444970  loss = 3.48181 avg_loss = 2.80474\n",
      "epoch no.6 train no.444980  loss = 2.90553 avg_loss = 2.82328\n",
      "epoch no.6 train no.444990  loss = 2.43065 avg_loss = 2.82760\n",
      "epoch no.6 train no.445000  loss = 3.59668 avg_loss = 2.82931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "to_tokens: ['▁비', '▁그', '송', '▁모음', '2', '</s>']\n",
      "추억의팝송 모음집</s>\n",
      "epoch no.6 train no.445010  loss = 2.73337 avg_loss = 2.85709\n",
      "epoch no.6 train no.445020  loss = 1.37405 avg_loss = 2.81859\n",
      "epoch no.6 train no.445030  loss = 2.79501 avg_loss = 2.82080\n",
      "epoch no.6 train no.445040  loss = 2.62489 avg_loss = 2.83776\n",
      "epoch no.6 train no.445050  loss = 2.74864 avg_loss = 2.84224\n",
      "epoch no.6 train no.445060  loss = 1.66911 avg_loss = 2.85221\n",
      "epoch no.6 train no.445070  loss = 2.91772 avg_loss = 2.88231\n",
      "epoch no.6 train no.445080  loss = 2.67747 avg_loss = 2.87523\n",
      "epoch no.6 train no.445090  loss = 3.47044 avg_loss = 2.84623\n",
      "epoch no.6 train no.445100  loss = 1.96150 avg_loss = 2.84621\n",
      "epoch no.6 train no.445110  loss = 2.02153 avg_loss = 2.83189\n",
      "epoch no.6 train no.445120  loss = 2.18520 avg_loss = 2.84705\n",
      "epoch no.6 train no.445130  loss = 3.01979 avg_loss = 2.83462\n",
      "epoch no.6 train no.445140  loss = 3.22116 avg_loss = 2.81601\n",
      "epoch no.6 train no.445150  loss = 3.19513 avg_loss = 2.83141\n",
      "epoch no.6 train no.445160  loss = 2.12697 avg_loss = 2.83160\n",
      "epoch no.6 train no.445170  loss = 3.58443 avg_loss = 2.85500\n",
      "epoch no.6 train no.445180  loss = 2.38534 avg_loss = 2.84405\n",
      "epoch no.6 train no.445190  loss = 2.91120 avg_loss = 2.83777\n",
      "epoch no.6 train no.445200  loss = 2.90343 avg_loss = 2.86143\n",
      "epoch no.6 train no.445210  loss = 3.04711 avg_loss = 2.88291\n",
      "epoch no.6 train no.445220  loss = 2.21663 avg_loss = 2.88531\n",
      "epoch no.6 train no.445230  loss = 2.33584 avg_loss = 2.87056\n",
      "epoch no.6 train no.445240  loss = 1.52424 avg_loss = 2.88420\n",
      "epoch no.6 train no.445250  loss = 2.54359 avg_loss = 2.84446\n",
      "epoch no.6 train no.445260  loss = 1.78229 avg_loss = 2.82001\n",
      "epoch no.6 train no.445270  loss = 3.08746 avg_loss = 2.80305\n",
      "epoch no.6 train no.445280  loss = 3.13421 avg_loss = 2.81911\n",
      "epoch no.6 train no.445290  loss = 2.66731 avg_loss = 2.85650\n",
      "epoch no.6 train no.445300  loss = 3.27396 avg_loss = 2.83086\n",
      "epoch no.6 train no.445310  loss = 3.41985 avg_loss = 2.84365\n",
      "epoch no.6 train no.445320  loss = 3.85894 avg_loss = 2.86589\n",
      "epoch no.6 train no.445330  loss = 2.87175 avg_loss = 2.86559\n",
      "epoch no.6 train no.445340  loss = 2.90318 avg_loss = 2.85596\n",
      "epoch no.6 train no.445350  loss = 2.71200 avg_loss = 2.85783\n",
      "epoch no.6 train no.445360  loss = 2.73674 avg_loss = 2.84429\n",
      "epoch no.6 train no.445370  loss = 3.28471 avg_loss = 2.83727\n",
      "epoch no.6 train no.445380  loss = 2.66901 avg_loss = 2.86384\n",
      "epoch no.6 train no.445390  loss = 1.84387 avg_loss = 2.82823\n",
      "epoch no.6 train no.445400  loss = 2.72649 avg_loss = 2.84679\n",
      "epoch no.6 train no.445410  loss = 3.64788 avg_loss = 2.82583\n",
      "epoch no.6 train no.445420  loss = 2.96256 avg_loss = 2.78767\n",
      "epoch no.6 train no.445430  loss = 2.62147 avg_loss = 2.78166\n",
      "epoch no.6 train no.445440  loss = 3.04390 avg_loss = 2.79582\n",
      "epoch no.6 train no.445450  loss = 2.24621 avg_loss = 2.77753\n",
      "epoch no.6 train no.445460  loss = 2.57245 avg_loss = 2.77730\n",
      "epoch no.6 train no.445470  loss = 2.39297 avg_loss = 2.78499\n",
      "epoch no.6 train no.445480  loss = 3.14524 avg_loss = 2.74736\n",
      "epoch no.6 train no.445490  loss = 3.69805 avg_loss = 2.73033\n",
      "epoch no.6 train no.445500  loss = 3.76922 avg_loss = 2.75206\n",
      "epoch no.6 train no.445510  loss = 2.44774 avg_loss = 2.74245\n",
      "epoch no.6 train no.445520  loss = 2.45604 avg_loss = 2.78201\n",
      "epoch no.6 train no.445530  loss = 2.76265 avg_loss = 2.77617\n",
      "epoch no.6 train no.445540  loss = 1.80639 avg_loss = 2.74805\n",
      "epoch no.6 train no.445550  loss = 2.20396 avg_loss = 2.73621\n",
      "epoch no.6 train no.445560  loss = 2.85720 avg_loss = 2.72360\n",
      "epoch no.6 train no.445570  loss = 3.48359 avg_loss = 2.73398\n",
      "epoch no.6 train no.445580  loss = 1.62477 avg_loss = 2.73861\n",
      "epoch no.6 train no.445590  loss = 3.55864 avg_loss = 2.75813\n",
      "epoch no.6 train no.445600  loss = 1.05309 avg_loss = 2.73088\n",
      "epoch no.6 train no.445610  loss = 2.77152 avg_loss = 2.72543\n",
      "epoch no.6 train no.445620  loss = 2.55428 avg_loss = 2.71877\n",
      "epoch no.6 train no.445630  loss = 1.91739 avg_loss = 2.72156\n",
      "epoch no.6 train no.445640  loss = 2.37590 avg_loss = 2.72833\n",
      "epoch no.6 train no.445650  loss = 3.37715 avg_loss = 2.71831\n",
      "epoch no.6 train no.445660  loss = 3.12976 avg_loss = 2.70726\n",
      "epoch no.6 train no.445670  loss = 1.99119 avg_loss = 2.71668\n",
      "epoch no.6 train no.445680  loss = 2.75900 avg_loss = 2.73525\n",
      "epoch no.6 train no.445690  loss = 2.10316 avg_loss = 2.72080\n",
      "epoch no.6 train no.445700  loss = 2.88487 avg_loss = 2.69908\n",
      "epoch no.6 train no.445710  loss = 1.40334 avg_loss = 2.68431\n",
      "epoch no.6 train no.445720  loss = 2.88508 avg_loss = 2.71249\n",
      "epoch no.6 train no.445730  loss = 3.62222 avg_loss = 2.73082\n",
      "epoch no.6 train no.445740  loss = 3.32667 avg_loss = 2.76336\n",
      "epoch no.6 train no.445750  loss = 5.01246 avg_loss = 2.79227\n",
      "epoch no.6 train no.445760  loss = 3.74835 avg_loss = 2.84132\n",
      "epoch no.6 train no.445770  loss = 3.07328 avg_loss = 2.84531\n",
      "epoch no.6 train no.445780  loss = 2.38964 avg_loss = 2.83955\n",
      "epoch no.6 train no.445790  loss = 1.64182 avg_loss = 2.81161\n",
      "epoch no.6 train no.445800  loss = 3.03965 avg_loss = 2.82222\n",
      "epoch no.6 train no.445810  loss = 1.94248 avg_loss = 2.80523\n",
      "epoch no.6 train no.445820  loss = 3.01186 avg_loss = 2.81311\n",
      "epoch no.6 train no.445830  loss = 4.20911 avg_loss = 2.80766\n",
      "epoch no.6 train no.445840  loss = 2.33913 avg_loss = 2.84510\n",
      "epoch no.6 train no.445850  loss = 3.01325 avg_loss = 2.85262\n",
      "epoch no.6 train no.445860  loss = 1.86830 avg_loss = 2.79906\n",
      "epoch no.6 train no.445870  loss = 3.01677 avg_loss = 2.80078\n",
      "epoch no.6 train no.445880  loss = 2.60839 avg_loss = 2.82977\n",
      "epoch no.6 train no.445890  loss = 2.61159 avg_loss = 2.82642\n",
      "epoch no.6 train no.445900  loss = 2.82663 avg_loss = 2.81230\n",
      "epoch no.6 train no.445910  loss = 2.36041 avg_loss = 2.81163\n",
      "epoch no.6 train no.445920  loss = 2.28543 avg_loss = 2.79470\n",
      "epoch no.6 train no.445930  loss = 2.83880 avg_loss = 2.78038\n",
      "epoch no.6 train no.445940  loss = 1.96114 avg_loss = 2.78628\n",
      "epoch no.6 train no.445950  loss = 2.29569 avg_loss = 2.77719\n",
      "epoch no.6 train no.445960  loss = 3.21007 avg_loss = 2.80854\n",
      "epoch no.6 train no.445970  loss = 3.65656 avg_loss = 2.84933\n",
      "epoch no.6 train no.445980  loss = 3.10804 avg_loss = 2.86914\n",
      "epoch no.6 train no.445990  loss = 3.61675 avg_loss = 2.94794\n",
      "epoch no.6 train no.446000  loss = 2.93640 avg_loss = 2.92469\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '모', '음', '</s>']\n",
      "추억의 90년대 노래모음</s>\n",
      "epoch no.6 train no.446010  loss = 2.64263 avg_loss = 2.94764\n",
      "epoch no.6 train no.446020  loss = 2.14442 avg_loss = 2.88809\n",
      "epoch no.6 train no.446030  loss = 1.70086 avg_loss = 2.91612\n",
      "epoch no.6 train no.446040  loss = 3.67237 avg_loss = 2.88398\n",
      "epoch no.6 train no.446050  loss = 2.46928 avg_loss = 2.86930\n",
      "epoch no.6 train no.446060  loss = 2.57130 avg_loss = 2.85424\n",
      "epoch no.6 train no.446070  loss = 2.40684 avg_loss = 2.86806\n",
      "epoch no.6 train no.446080  loss = 2.29312 avg_loss = 2.85312\n",
      "epoch no.6 train no.446090  loss = 2.63951 avg_loss = 2.87597\n",
      "epoch no.6 train no.446100  loss = 3.58487 avg_loss = 2.86501\n",
      "epoch no.6 train no.446110  loss = 3.50212 avg_loss = 2.87775\n",
      "epoch no.6 train no.446120  loss = 2.35232 avg_loss = 2.84659\n",
      "epoch no.6 train no.446130  loss = 3.62211 avg_loss = 2.84810\n",
      "epoch no.6 train no.446140  loss = 2.85948 avg_loss = 2.89512\n",
      "epoch no.6 train no.446150  loss = 3.94518 avg_loss = 2.89295\n",
      "epoch no.6 train no.446160  loss = 1.82502 avg_loss = 2.87044\n",
      "epoch no.6 train no.446170  loss = 5.20435 avg_loss = 2.89658\n",
      "epoch no.6 train no.446180  loss = 2.44851 avg_loss = 2.91106\n",
      "epoch no.6 train no.446190  loss = 2.43591 avg_loss = 2.89013\n",
      "epoch no.6 train no.446200  loss = 2.40012 avg_loss = 2.84237\n",
      "epoch no.6 train no.446210  loss = 3.29600 avg_loss = 2.85999\n",
      "epoch no.6 train no.446220  loss = 2.51739 avg_loss = 2.83331\n",
      "epoch no.6 train no.446230  loss = 2.83882 avg_loss = 2.82768\n",
      "epoch no.6 train no.446240  loss = 2.06915 avg_loss = 2.84302\n",
      "epoch no.6 train no.446250  loss = 2.69935 avg_loss = 2.81217\n",
      "epoch no.6 train no.446260  loss = 2.35669 avg_loss = 2.80153\n",
      "epoch no.6 train no.446270  loss = 2.13462 avg_loss = 2.81769\n",
      "epoch no.6 train no.446280  loss = 3.44983 avg_loss = 2.81834\n",
      "epoch no.6 train no.446290  loss = 4.05663 avg_loss = 2.82716\n",
      "epoch no.6 train no.446300  loss = 3.09725 avg_loss = 2.81619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.446310  loss = 3.46315 avg_loss = 2.85182\n",
      "epoch no.6 train no.446320  loss = 2.03734 avg_loss = 2.87431\n",
      "epoch no.6 train no.446330  loss = 3.09280 avg_loss = 2.87026\n",
      "epoch no.6 train no.446340  loss = 1.72769 avg_loss = 2.83187\n",
      "epoch no.6 train no.446350  loss = 2.29313 avg_loss = 2.84411\n",
      "epoch no.6 train no.446360  loss = 1.95295 avg_loss = 2.81304\n",
      "epoch no.6 train no.446370  loss = 3.99438 avg_loss = 2.78514\n",
      "epoch no.6 train no.446380  loss = 2.60431 avg_loss = 2.77240\n",
      "epoch no.6 train no.446390  loss = 3.02731 avg_loss = 2.74186\n",
      "epoch no.6 train no.446400  loss = 2.93505 avg_loss = 2.71807\n",
      "epoch no.6 train no.446410  loss = 2.05497 avg_loss = 2.71426\n",
      "epoch no.6 train no.446420  loss = 3.71808 avg_loss = 2.73830\n",
      "epoch no.6 train no.446430  loss = 3.12785 avg_loss = 2.73532\n",
      "epoch no.6 train no.446440  loss = 2.02065 avg_loss = 2.71374\n",
      "epoch no.6 train no.446450  loss = 2.35273 avg_loss = 2.73904\n",
      "epoch no.6 train no.446460  loss = 3.31105 avg_loss = 2.76888\n",
      "epoch no.6 train no.446470  loss = 4.06559 avg_loss = 2.79651\n",
      "epoch no.6 train no.446480  loss = 2.56435 avg_loss = 2.78454\n",
      "epoch no.6 train no.446490  loss = 3.75554 avg_loss = 2.77368\n",
      "epoch no.6 train no.446500  loss = 2.87481 avg_loss = 2.77100\n",
      "epoch no.6 train no.446510  loss = 3.74435 avg_loss = 2.76699\n",
      "epoch no.6 train no.446520  loss = 3.99889 avg_loss = 2.76910\n",
      "epoch no.6 train no.446530  loss = 2.88668 avg_loss = 2.80987\n",
      "epoch no.6 train no.446540  loss = 2.14200 avg_loss = 2.83997\n",
      "epoch no.6 train no.446550  loss = 2.41084 avg_loss = 2.82019\n",
      "epoch no.6 train no.446560  loss = 3.37776 avg_loss = 2.81872\n",
      "epoch no.6 train no.446570  loss = 1.79310 avg_loss = 2.79391\n",
      "epoch no.6 train no.446580  loss = 2.47695 avg_loss = 2.80326\n",
      "epoch no.6 train no.446590  loss = 2.79473 avg_loss = 2.79067\n",
      "epoch no.6 train no.446600  loss = 4.07666 avg_loss = 2.78671\n",
      "epoch no.6 train no.446610  loss = 3.51487 avg_loss = 2.79574\n",
      "epoch no.6 train no.446620  loss = 4.11066 avg_loss = 2.80583\n",
      "epoch no.6 train no.446630  loss = 3.75092 avg_loss = 2.80692\n",
      "epoch no.6 train no.446640  loss = 4.01372 avg_loss = 2.85088\n",
      "epoch no.6 train no.446650  loss = 3.10827 avg_loss = 2.84542\n",
      "epoch no.6 train no.446660  loss = 2.96030 avg_loss = 2.82718\n",
      "epoch no.6 train no.446670  loss = 2.02031 avg_loss = 2.81567\n",
      "epoch no.6 train no.446680  loss = 2.85482 avg_loss = 2.81447\n",
      "epoch no.6 train no.446690  loss = 1.55706 avg_loss = 2.79836\n",
      "epoch no.6 train no.446700  loss = 3.13132 avg_loss = 2.77045\n",
      "epoch no.6 train no.446710  loss = 2.69496 avg_loss = 2.77201\n",
      "epoch no.6 train no.446720  loss = 2.21503 avg_loss = 2.77600\n",
      "epoch no.6 train no.446730  loss = 2.96762 avg_loss = 2.79046\n",
      "epoch no.6 train no.446740  loss = 2.26746 avg_loss = 2.76364\n",
      "epoch no.6 train no.446750  loss = 3.03771 avg_loss = 2.80642\n",
      "epoch no.6 train no.446760  loss = 2.78076 avg_loss = 2.79949\n",
      "epoch no.6 train no.446770  loss = 2.56357 avg_loss = 2.78763\n",
      "epoch no.6 train no.446780  loss = 2.93997 avg_loss = 2.78483\n",
      "epoch no.6 train no.446790  loss = 2.41059 avg_loss = 2.76464\n",
      "epoch no.6 train no.446800  loss = 3.06386 avg_loss = 2.74522\n",
      "epoch no.6 train no.446810  loss = 1.90710 avg_loss = 2.74099\n",
      "epoch no.6 train no.446820  loss = 2.65156 avg_loss = 2.77613\n",
      "epoch no.6 train no.446830  loss = 4.15640 avg_loss = 2.76782\n",
      "epoch no.6 train no.446840  loss = 2.29386 avg_loss = 2.77597\n",
      "epoch no.6 train no.446850  loss = 3.03875 avg_loss = 2.76435\n",
      "epoch no.6 train no.446860  loss = 2.27979 avg_loss = 2.72748\n",
      "epoch no.6 train no.446870  loss = 3.00067 avg_loss = 2.72844\n",
      "epoch no.6 train no.446880  loss = 1.92831 avg_loss = 2.72044\n",
      "epoch no.6 train no.446890  loss = 2.16436 avg_loss = 2.71564\n",
      "epoch no.6 train no.446900  loss = 2.41034 avg_loss = 2.74404\n",
      "epoch no.6 train no.446910  loss = 3.70208 avg_loss = 2.77144\n",
      "epoch no.6 train no.446920  loss = 2.64312 avg_loss = 2.75186\n",
      "epoch no.6 train no.446930  loss = 2.99907 avg_loss = 2.74670\n",
      "epoch no.6 train no.446940  loss = 2.14471 avg_loss = 2.75306\n",
      "epoch no.6 train no.446950  loss = 3.92064 avg_loss = 2.77504\n",
      "epoch no.6 train no.446960  loss = 3.91498 avg_loss = 2.77287\n",
      "epoch no.6 train no.446970  loss = 2.67163 avg_loss = 2.79455\n",
      "epoch no.6 train no.446980  loss = 3.30657 avg_loss = 2.79775\n",
      "epoch no.6 train no.446990  loss = 2.48722 avg_loss = 2.77264\n",
      "epoch no.6 train no.447000  loss = 2.97987 avg_loss = 2.77087\n",
      "6\n",
      "to_tokens: ['▁비', '▁90', '년대', '대를', '▁떠나는', '하다', '▁음악', '</s>']\n",
      "추억의 90년대를 노래한 노래</s>\n",
      "epoch no.6 train no.447010  loss = 2.64432 avg_loss = 2.76541\n",
      "epoch no.6 train no.447020  loss = 2.16893 avg_loss = 2.77762\n",
      "epoch no.6 train no.447030  loss = 3.01822 avg_loss = 2.73923\n",
      "epoch no.6 train no.447040  loss = 3.02066 avg_loss = 2.73631\n",
      "epoch no.6 train no.447050  loss = 4.49518 avg_loss = 2.81366\n",
      "epoch no.6 train no.447060  loss = 2.91337 avg_loss = 2.84366\n",
      "epoch no.6 train no.447070  loss = 3.41680 avg_loss = 2.84873\n",
      "epoch no.6 train no.447080  loss = 1.93225 avg_loss = 2.84559\n",
      "epoch no.6 train no.447090  loss = 3.08967 avg_loss = 2.86088\n",
      "epoch no.6 train no.447100  loss = 2.81121 avg_loss = 2.89929\n",
      "epoch no.6 train no.447110  loss = 2.81701 avg_loss = 2.86987\n",
      "epoch no.6 train no.447120  loss = 3.27231 avg_loss = 2.85383\n",
      "epoch no.6 train no.447130  loss = 2.37477 avg_loss = 2.81865\n",
      "epoch no.6 train no.447140  loss = 2.28430 avg_loss = 2.80668\n",
      "epoch no.6 train no.447150  loss = 2.35439 avg_loss = 2.82183\n",
      "epoch no.6 train no.447160  loss = 3.35430 avg_loss = 2.82497\n",
      "epoch no.6 train no.447170  loss = 2.11908 avg_loss = 2.81266\n",
      "epoch no.6 train no.447180  loss = 1.99887 avg_loss = 2.79554\n",
      "epoch no.6 train no.447190  loss = 3.15431 avg_loss = 2.80890\n",
      "epoch no.6 train no.447200  loss = 2.37352 avg_loss = 2.78762\n",
      "epoch no.6 train no.447210  loss = 3.04515 avg_loss = 2.79482\n",
      "epoch no.6 train no.447220  loss = 2.57980 avg_loss = 2.78756\n",
      "epoch no.6 train no.447230  loss = 2.42916 avg_loss = 2.76044\n",
      "epoch no.6 train no.447240  loss = 2.93646 avg_loss = 2.78549\n",
      "epoch no.6 train no.447250  loss = 3.17905 avg_loss = 2.76865\n",
      "epoch no.6 train no.447260  loss = 2.11066 avg_loss = 2.79995\n",
      "epoch no.6 train no.447270  loss = 2.56723 avg_loss = 2.82108\n",
      "epoch no.6 train no.447280  loss = 3.31436 avg_loss = 2.81659\n",
      "epoch no.6 train no.447290  loss = 2.33874 avg_loss = 2.79194\n",
      "epoch no.6 train no.447300  loss = 2.19661 avg_loss = 2.78671\n",
      "epoch no.6 train no.447310  loss = 4.58589 avg_loss = 2.79856\n",
      "epoch no.6 train no.447320  loss = 2.00966 avg_loss = 2.78092\n",
      "epoch no.6 train no.447330  loss = 2.79239 avg_loss = 2.82252\n",
      "epoch no.6 train no.447340  loss = 2.93303 avg_loss = 2.84388\n",
      "epoch no.6 train no.447350  loss = 4.06952 avg_loss = 2.88105\n",
      "epoch no.6 train no.447360  loss = 2.44344 avg_loss = 2.89523\n",
      "epoch no.6 train no.447370  loss = 2.77989 avg_loss = 2.91124\n",
      "epoch no.6 train no.447380  loss = 2.91687 avg_loss = 2.90187\n",
      "epoch no.6 train no.447390  loss = 2.22911 avg_loss = 2.90163\n",
      "epoch no.6 train no.447400  loss = 2.37886 avg_loss = 2.88649\n",
      "epoch no.6 train no.447410  loss = 3.83146 avg_loss = 2.88455\n",
      "epoch no.6 train no.447420  loss = 3.95888 avg_loss = 2.87855\n",
      "epoch no.6 train no.447430  loss = 2.16163 avg_loss = 2.88414\n",
      "epoch no.6 train no.447440  loss = 2.57446 avg_loss = 2.87772\n",
      "epoch no.6 train no.447450  loss = 2.72484 avg_loss = 2.90297\n",
      "epoch no.6 train no.447460  loss = 3.09236 avg_loss = 2.90333\n",
      "epoch no.6 train no.447470  loss = 2.97057 avg_loss = 2.91447\n",
      "epoch no.6 train no.447480  loss = 3.59614 avg_loss = 2.91400\n",
      "epoch no.6 train no.447490  loss = 1.47768 avg_loss = 2.86822\n",
      "epoch no.6 train no.447500  loss = 2.48197 avg_loss = 2.83951\n",
      "epoch no.6 train no.447510  loss = 2.47071 avg_loss = 2.85137\n",
      "epoch no.6 train no.447520  loss = 3.42106 avg_loss = 2.85373\n",
      "epoch no.6 train no.447530  loss = 2.25333 avg_loss = 2.84967\n",
      "epoch no.6 train no.447540  loss = 2.37236 avg_loss = 2.84415\n",
      "epoch no.6 train no.447550  loss = 4.33416 avg_loss = 2.84096\n",
      "epoch no.6 train no.447560  loss = 2.66408 avg_loss = 2.84012\n",
      "epoch no.6 train no.447570  loss = 2.72251 avg_loss = 2.80711\n",
      "epoch no.6 train no.447580  loss = 1.51564 avg_loss = 2.77796\n",
      "epoch no.6 train no.447590  loss = 3.32299 avg_loss = 2.83138\n",
      "epoch no.6 train no.447600  loss = 3.81946 avg_loss = 2.85255\n",
      "epoch no.6 train no.447610  loss = 2.94894 avg_loss = 2.84878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.447620  loss = 3.65764 avg_loss = 2.82378\n",
      "epoch no.6 train no.447630  loss = 3.83299 avg_loss = 2.83344\n",
      "epoch no.6 train no.447640  loss = 3.39279 avg_loss = 2.85195\n",
      "epoch no.6 train no.447650  loss = 2.21496 avg_loss = 2.85043\n",
      "epoch no.6 train no.447660  loss = 2.50258 avg_loss = 2.82871\n",
      "epoch no.6 train no.447670  loss = 3.88814 avg_loss = 2.83865\n",
      "epoch no.6 train no.447680  loss = 2.26737 avg_loss = 2.81210\n",
      "epoch no.6 train no.447690  loss = 1.82761 avg_loss = 2.82585\n",
      "epoch no.6 train no.447700  loss = 2.31796 avg_loss = 2.82376\n",
      "epoch no.6 train no.447710  loss = 2.56909 avg_loss = 2.78029\n",
      "epoch no.6 train no.447720  loss = 2.82158 avg_loss = 2.76193\n",
      "epoch no.6 train no.447730  loss = 2.72604 avg_loss = 2.72282\n",
      "epoch no.6 train no.447740  loss = 2.77405 avg_loss = 2.72493\n",
      "epoch no.6 train no.447750  loss = 2.69603 avg_loss = 2.76786\n",
      "epoch no.6 train no.447760  loss = 3.86939 avg_loss = 2.79053\n",
      "epoch no.6 train no.447770  loss = 3.97153 avg_loss = 2.81527\n",
      "epoch no.6 train no.447780  loss = 2.94430 avg_loss = 2.80966\n",
      "epoch no.6 train no.447790  loss = 3.26965 avg_loss = 2.82938\n",
      "epoch no.6 train no.447800  loss = 1.88740 avg_loss = 2.80759\n",
      "epoch no.6 train no.447810  loss = 2.91988 avg_loss = 2.75038\n",
      "epoch no.6 train no.447820  loss = 2.17251 avg_loss = 2.73599\n",
      "epoch no.6 train no.447830  loss = 1.96688 avg_loss = 2.73768\n",
      "epoch no.6 train no.447840  loss = 1.99881 avg_loss = 2.71714\n",
      "epoch no.6 train no.447850  loss = 2.95238 avg_loss = 2.76563\n",
      "epoch no.6 train no.447860  loss = 3.56774 avg_loss = 2.79539\n",
      "epoch no.6 train no.447870  loss = 2.38627 avg_loss = 2.85624\n",
      "epoch no.6 train no.447880  loss = 2.89766 avg_loss = 2.85179\n",
      "epoch no.6 train no.447890  loss = 2.06475 avg_loss = 2.82776\n",
      "epoch no.6 train no.447900  loss = 2.45784 avg_loss = 2.83633\n",
      "epoch no.6 train no.447910  loss = 3.91298 avg_loss = 2.83226\n",
      "epoch no.6 train no.447920  loss = 1.97170 avg_loss = 2.85767\n",
      "epoch no.6 train no.447930  loss = 2.38295 avg_loss = 2.84453\n",
      "epoch no.6 train no.447940  loss = 2.64544 avg_loss = 2.84576\n",
      "epoch no.6 train no.447950  loss = 3.66438 avg_loss = 2.83460\n",
      "epoch no.6 train no.447960  loss = 3.25607 avg_loss = 2.83428\n",
      "epoch no.6 train no.447970  loss = 1.56302 avg_loss = 2.80749\n",
      "epoch no.6 train no.447980  loss = 3.85211 avg_loss = 2.86552\n",
      "epoch no.6 train no.447990  loss = 3.82137 avg_loss = 2.88682\n",
      "epoch no.6 train no.448000  loss = 2.22479 avg_loss = 2.89374\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁00', '년도', '▁노래', '</s>']\n",
      "추억의 90 00년도 노래</s>\n",
      "epoch no.6 train no.448010  loss = 2.48095 avg_loss = 2.85916\n",
      "epoch no.6 train no.448020  loss = 2.54070 avg_loss = 2.84306\n",
      "epoch no.6 train no.448030  loss = 1.92215 avg_loss = 2.80384\n",
      "epoch no.6 train no.448040  loss = 2.83310 avg_loss = 2.78580\n",
      "epoch no.6 train no.448050  loss = 2.69848 avg_loss = 2.77612\n",
      "epoch no.6 train no.448060  loss = 2.10819 avg_loss = 2.76412\n",
      "epoch no.6 train no.448070  loss = 2.67776 avg_loss = 2.76746\n",
      "epoch no.6 train no.448080  loss = 3.34324 avg_loss = 2.80024\n",
      "epoch no.6 train no.448090  loss = 3.76409 avg_loss = 2.82185\n",
      "epoch no.6 train no.448100  loss = 1.90742 avg_loss = 2.82978\n",
      "epoch no.6 train no.448110  loss = 1.83023 avg_loss = 2.78167\n",
      "epoch no.6 train no.448120  loss = 3.12999 avg_loss = 2.76885\n",
      "epoch no.6 train no.448130  loss = 2.48435 avg_loss = 2.78457\n",
      "epoch no.6 train no.448140  loss = 2.17446 avg_loss = 2.78183\n",
      "epoch no.6 train no.448150  loss = 3.00970 avg_loss = 2.79533\n",
      "epoch no.6 train no.448160  loss = 3.18260 avg_loss = 2.80098\n",
      "epoch no.6 train no.448170  loss = 2.60874 avg_loss = 2.77767\n",
      "epoch no.6 train no.448180  loss = 2.91563 avg_loss = 2.74420\n",
      "epoch no.6 train no.448190  loss = 1.91896 avg_loss = 2.77512\n",
      "epoch no.6 train no.448200  loss = 2.92511 avg_loss = 2.76220\n",
      "epoch no.6 train no.448210  loss = 3.92827 avg_loss = 2.77769\n",
      "epoch no.6 train no.448220  loss = 4.04843 avg_loss = 2.79250\n",
      "epoch no.6 train no.448230  loss = 2.61180 avg_loss = 2.81985\n",
      "epoch no.6 train no.448240  loss = 2.23290 avg_loss = 2.80053\n",
      "epoch no.6 train no.448250  loss = 2.46289 avg_loss = 2.79984\n",
      "epoch no.6 train no.448260  loss = 3.20951 avg_loss = 2.79714\n",
      "epoch no.6 train no.448270  loss = 2.59737 avg_loss = 2.74954\n",
      "epoch no.6 train no.448280  loss = 1.98016 avg_loss = 2.75771\n",
      "epoch no.6 train no.448290  loss = 1.74131 avg_loss = 2.73418\n",
      "epoch no.6 train no.448300  loss = 1.70606 avg_loss = 2.74931\n",
      "epoch no.6 train no.448310  loss = 5.79081 avg_loss = 2.76353\n",
      "epoch no.6 train no.448320  loss = 1.67700 avg_loss = 2.75719\n",
      "epoch no.6 train no.448330  loss = 4.57311 avg_loss = 2.77107\n",
      "epoch no.6 train no.448340  loss = 2.43134 avg_loss = 2.79354\n",
      "epoch no.6 train no.448350  loss = 1.77077 avg_loss = 2.77471\n",
      "epoch no.6 train no.448360  loss = 2.58895 avg_loss = 2.78800\n",
      "epoch no.6 train no.448370  loss = 2.09916 avg_loss = 2.74815\n",
      "epoch no.6 train no.448380  loss = 2.61180 avg_loss = 2.78174\n",
      "epoch no.6 train no.448390  loss = 3.04005 avg_loss = 2.77549\n",
      "epoch no.6 train no.448400  loss = 3.14794 avg_loss = 2.81012\n",
      "epoch no.6 train no.448410  loss = 2.59831 avg_loss = 2.80589\n",
      "epoch no.6 train no.448420  loss = 4.42617 avg_loss = 2.81857\n",
      "epoch no.6 train no.448430  loss = 3.01234 avg_loss = 2.79940\n",
      "epoch no.6 train no.448440  loss = 3.18296 avg_loss = 2.81273\n",
      "epoch no.6 train no.448450  loss = 2.55753 avg_loss = 2.82498\n",
      "epoch no.6 train no.448460  loss = 4.44271 avg_loss = 2.83346\n",
      "epoch no.6 train no.448470  loss = 2.30282 avg_loss = 2.84572\n",
      "epoch no.6 train no.448480  loss = 1.89898 avg_loss = 2.88492\n",
      "epoch no.6 train no.448490  loss = 1.95223 avg_loss = 2.86430\n",
      "epoch no.6 train no.448500  loss = 2.73154 avg_loss = 2.83504\n",
      "epoch no.6 train no.448510  loss = 5.13081 avg_loss = 2.84233\n",
      "epoch no.6 train no.448520  loss = 2.96748 avg_loss = 2.80097\n",
      "epoch no.6 train no.448530  loss = 2.86315 avg_loss = 2.87559\n",
      "epoch no.6 train no.448540  loss = 2.05877 avg_loss = 2.90585\n",
      "epoch no.6 train no.448550  loss = 1.94260 avg_loss = 2.90837\n",
      "epoch no.6 train no.448560  loss = 4.93974 avg_loss = 2.89488\n",
      "epoch no.6 train no.448570  loss = 2.17455 avg_loss = 2.92142\n",
      "epoch no.6 train no.448580  loss = 2.94172 avg_loss = 2.89671\n",
      "epoch no.6 train no.448590  loss = 2.26572 avg_loss = 2.87620\n",
      "epoch no.6 train no.448600  loss = 1.16903 avg_loss = 2.84391\n",
      "epoch no.6 train no.448610  loss = 3.26723 avg_loss = 2.82454\n",
      "epoch no.6 train no.448620  loss = 2.76803 avg_loss = 2.83372\n",
      "epoch no.6 train no.448630  loss = 3.62887 avg_loss = 2.83740\n",
      "epoch no.6 train no.448640  loss = 4.91348 avg_loss = 2.84360\n",
      "epoch no.6 train no.448650  loss = 2.29200 avg_loss = 2.81670\n",
      "epoch no.6 train no.448660  loss = 2.60195 avg_loss = 2.80058\n",
      "epoch no.6 train no.448670  loss = 2.87536 avg_loss = 2.82513\n",
      "epoch no.6 train no.448680  loss = 4.15434 avg_loss = 2.82575\n",
      "epoch no.6 train no.448690  loss = 2.58596 avg_loss = 2.80682\n",
      "epoch no.6 train no.448700  loss = 3.06835 avg_loss = 2.78070\n",
      "epoch no.6 train no.448710  loss = 3.31770 avg_loss = 2.81181\n",
      "epoch no.6 train no.448720  loss = 3.20575 avg_loss = 2.81100\n",
      "epoch no.6 train no.448730  loss = 3.45607 avg_loss = 2.81810\n",
      "epoch no.6 train no.448740  loss = 1.48105 avg_loss = 2.80370\n",
      "epoch no.6 train no.448750  loss = 2.64008 avg_loss = 2.81669\n",
      "epoch no.6 train no.448760  loss = 2.53679 avg_loss = 2.82156\n",
      "epoch no.6 train no.448770  loss = 3.07386 avg_loss = 2.83687\n",
      "epoch no.6 train no.448780  loss = 2.77008 avg_loss = 2.85391\n",
      "epoch no.6 train no.448790  loss = 2.23042 avg_loss = 2.80887\n",
      "epoch no.6 train no.448800  loss = 2.90357 avg_loss = 2.79852\n",
      "epoch no.6 train no.448810  loss = 3.27100 avg_loss = 2.83601\n",
      "epoch no.6 train no.448820  loss = 3.25630 avg_loss = 2.83933\n",
      "epoch no.6 train no.448830  loss = 3.87870 avg_loss = 2.84399\n",
      "epoch no.6 train no.448840  loss = 4.34158 avg_loss = 2.84983\n",
      "epoch no.6 train no.448850  loss = 3.80812 avg_loss = 2.83166\n",
      "epoch no.6 train no.448860  loss = 3.07508 avg_loss = 2.85008\n",
      "epoch no.6 train no.448870  loss = 3.06523 avg_loss = 2.88490\n",
      "epoch no.6 train no.448880  loss = 3.98305 avg_loss = 2.87474\n",
      "epoch no.6 train no.448890  loss = 2.28579 avg_loss = 2.89904\n",
      "epoch no.6 train no.448900  loss = 3.44043 avg_loss = 2.87895\n",
      "epoch no.6 train no.448910  loss = 2.54873 avg_loss = 2.83544\n",
      "epoch no.6 train no.448920  loss = 2.46888 avg_loss = 2.79627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.448930  loss = 3.32654 avg_loss = 2.82056\n",
      "epoch no.6 train no.448940  loss = 2.18855 avg_loss = 2.80614\n",
      "epoch no.6 train no.448950  loss = 2.87901 avg_loss = 2.77249\n",
      "epoch no.6 train no.448960  loss = 3.11402 avg_loss = 2.78681\n",
      "epoch no.6 train no.448970  loss = 3.58450 avg_loss = 2.75300\n",
      "epoch no.6 train no.448980  loss = 3.29810 avg_loss = 2.73373\n",
      "epoch no.6 train no.448990  loss = 3.09952 avg_loss = 2.76250\n",
      "epoch no.6 train no.449000  loss = 3.86153 avg_loss = 2.77530\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '▁리메이크', '모', '음', '</s>']\n",
      "추억의노래 리메이크모음</s>\n",
      "epoch no.6 train no.449010  loss = 2.10021 avg_loss = 2.75098\n",
      "epoch no.6 train no.449020  loss = 4.05799 avg_loss = 2.81226\n",
      "epoch no.6 train no.449030  loss = 2.30006 avg_loss = 2.80106\n",
      "epoch no.6 train no.449040  loss = 3.75830 avg_loss = 2.79595\n",
      "epoch no.6 train no.449050  loss = 1.55803 avg_loss = 2.77702\n",
      "epoch no.6 train no.449060  loss = 1.96164 avg_loss = 2.80561\n",
      "epoch no.6 train no.449070  loss = 2.77877 avg_loss = 2.80042\n",
      "epoch no.6 train no.449080  loss = 3.97676 avg_loss = 2.83108\n",
      "epoch no.6 train no.449090  loss = 2.82713 avg_loss = 2.81250\n",
      "epoch no.6 train no.449100  loss = 5.30648 avg_loss = 2.83738\n",
      "epoch no.6 train no.449110  loss = 2.55906 avg_loss = 2.84625\n",
      "epoch no.6 train no.449120  loss = 2.36535 avg_loss = 2.85992\n",
      "epoch no.6 train no.449130  loss = 2.63951 avg_loss = 2.85807\n",
      "epoch no.6 train no.449140  loss = 1.79893 avg_loss = 2.85995\n",
      "epoch no.6 train no.449150  loss = 1.76791 avg_loss = 2.80948\n",
      "epoch no.6 train no.449160  loss = 2.21192 avg_loss = 2.80583\n",
      "epoch no.6 train no.449170  loss = 2.12419 avg_loss = 2.83000\n",
      "epoch no.6 train no.449180  loss = 2.60120 avg_loss = 2.80529\n",
      "epoch no.6 train no.449190  loss = 2.55626 avg_loss = 2.82097\n",
      "epoch no.6 train no.449200  loss = 1.75519 avg_loss = 2.80916\n",
      "epoch no.6 train no.449210  loss = 2.52529 avg_loss = 2.79627\n",
      "epoch no.6 train no.449220  loss = 2.14940 avg_loss = 2.79674\n",
      "epoch no.6 train no.449230  loss = 2.30377 avg_loss = 2.78054\n",
      "epoch no.6 train no.449240  loss = 3.55744 avg_loss = 2.80819\n",
      "epoch no.6 train no.449250  loss = 3.12572 avg_loss = 2.80274\n",
      "epoch no.6 train no.449260  loss = 3.17019 avg_loss = 2.82219\n",
      "epoch no.6 train no.449270  loss = 1.47708 avg_loss = 2.81174\n",
      "epoch no.6 train no.449280  loss = 2.27237 avg_loss = 2.79645\n",
      "epoch no.6 train no.449290  loss = 2.62009 avg_loss = 2.81052\n",
      "epoch no.6 train no.449300  loss = 2.73109 avg_loss = 2.80980\n",
      "epoch no.6 train no.449310  loss = 1.68301 avg_loss = 2.79440\n",
      "epoch no.6 train no.449320  loss = 2.57889 avg_loss = 2.79470\n",
      "epoch no.6 train no.449330  loss = 2.48505 avg_loss = 2.77516\n",
      "epoch no.6 train no.449340  loss = 2.08351 avg_loss = 2.77859\n",
      "epoch no.6 train no.449350  loss = 2.63323 avg_loss = 2.77215\n",
      "epoch no.6 train no.449360  loss = 3.08461 avg_loss = 2.80544\n",
      "epoch no.6 train no.449370  loss = 2.97462 avg_loss = 2.84198\n",
      "epoch no.6 train no.449380  loss = 3.70913 avg_loss = 2.85427\n",
      "epoch no.6 train no.449390  loss = 2.76000 avg_loss = 2.84863\n",
      "epoch no.6 train no.449400  loss = 2.54189 avg_loss = 2.86157\n",
      "epoch no.6 train no.449410  loss = 1.59868 avg_loss = 2.84384\n",
      "epoch no.6 train no.449420  loss = 4.61900 avg_loss = 2.88378\n",
      "epoch no.6 train no.449430  loss = 2.34817 avg_loss = 2.88597\n",
      "epoch no.6 train no.449440  loss = 2.12211 avg_loss = 2.90740\n",
      "epoch no.6 train no.449450  loss = 3.29214 avg_loss = 2.91457\n",
      "epoch no.6 train no.449460  loss = 3.01347 avg_loss = 2.92589\n",
      "epoch no.6 train no.449470  loss = 2.29718 avg_loss = 2.91607\n",
      "epoch no.6 train no.449480  loss = 3.64535 avg_loss = 2.91760\n",
      "epoch no.6 train no.449490  loss = 2.51173 avg_loss = 2.90088\n",
      "epoch no.6 train no.449500  loss = 2.43346 avg_loss = 2.87719\n",
      "epoch no.6 train no.449510  loss = 2.57704 avg_loss = 2.86239\n",
      "epoch no.6 train no.449520  loss = 3.13958 avg_loss = 2.88575\n",
      "epoch no.6 train no.449530  loss = 2.05978 avg_loss = 2.85218\n",
      "epoch no.6 train no.449540  loss = 2.80584 avg_loss = 2.86993\n",
      "epoch no.6 train no.449550  loss = 2.83843 avg_loss = 2.88386\n",
      "epoch no.6 train no.449560  loss = 3.87476 avg_loss = 2.91021\n",
      "epoch no.6 train no.449570  loss = 2.00831 avg_loss = 2.88500\n",
      "epoch no.6 train no.449580  loss = 2.22081 avg_loss = 2.91326\n",
      "epoch no.6 train no.449590  loss = 2.39861 avg_loss = 2.92808\n",
      "epoch no.6 train no.449600  loss = 2.03524 avg_loss = 2.93494\n",
      "epoch no.6 train no.449610  loss = 2.54819 avg_loss = 2.96102\n",
      "epoch no.6 train no.449620  loss = 2.72778 avg_loss = 2.96494\n",
      "epoch no.6 train no.449630  loss = 3.15821 avg_loss = 2.97052\n",
      "epoch no.6 train no.449640  loss = 3.22745 avg_loss = 3.01531\n",
      "epoch no.6 train no.449650  loss = 1.67398 avg_loss = 3.02780\n",
      "epoch no.6 train no.449660  loss = 4.23767 avg_loss = 3.02888\n",
      "epoch no.6 train no.449670  loss = 1.66446 avg_loss = 2.97084\n",
      "epoch no.6 train no.449680  loss = 4.51735 avg_loss = 3.01347\n",
      "epoch no.6 train no.449690  loss = 2.70342 avg_loss = 3.00557\n",
      "epoch no.6 train no.449700  loss = 3.18688 avg_loss = 3.01895\n",
      "epoch no.6 train no.449710  loss = 2.84619 avg_loss = 2.99606\n",
      "epoch no.6 train no.449720  loss = 3.60900 avg_loss = 2.98363\n",
      "epoch no.6 train no.449730  loss = 2.62164 avg_loss = 2.95531\n",
      "epoch no.6 train no.449740  loss = 5.18530 avg_loss = 2.99379\n",
      "epoch no.6 train no.449750  loss = 1.63625 avg_loss = 2.97454\n",
      "epoch no.6 train no.449760  loss = 3.86409 avg_loss = 2.97240\n",
      "epoch no.6 train no.449770  loss = 2.37644 avg_loss = 2.92704\n",
      "epoch no.6 train no.449780  loss = 1.45513 avg_loss = 2.94248\n",
      "epoch no.6 train no.449790  loss = 3.76147 avg_loss = 2.95795\n",
      "epoch no.6 train no.449800  loss = 3.22392 avg_loss = 2.97484\n",
      "epoch no.6 train no.449810  loss = 3.83836 avg_loss = 2.96160\n",
      "epoch no.6 train no.449820  loss = 2.42701 avg_loss = 2.89155\n",
      "epoch no.6 train no.449830  loss = 2.87529 avg_loss = 2.87001\n",
      "epoch no.6 train no.449840  loss = 2.72247 avg_loss = 2.89923\n",
      "epoch no.6 train no.449850  loss = 1.70598 avg_loss = 2.86055\n",
      "epoch no.6 train no.449860  loss = 2.07976 avg_loss = 2.88380\n",
      "epoch no.6 train no.449870  loss = 2.05519 avg_loss = 2.87099\n",
      "epoch no.6 train no.449880  loss = 2.14714 avg_loss = 2.88562\n",
      "epoch no.6 train no.449890  loss = 2.45845 avg_loss = 2.88687\n",
      "epoch no.6 train no.449900  loss = 2.85507 avg_loss = 2.87387\n",
      "epoch no.6 train no.449910  loss = 3.79212 avg_loss = 2.87521\n",
      "epoch no.6 train no.449920  loss = 3.76441 avg_loss = 2.87104\n",
      "epoch no.6 train no.449930  loss = 3.48824 avg_loss = 2.86509\n",
      "epoch no.6 train no.449940  loss = 2.47475 avg_loss = 2.84471\n",
      "epoch no.6 train no.449950  loss = 3.27885 avg_loss = 2.87121\n",
      "epoch no.6 train no.449960  loss = 2.50216 avg_loss = 2.81643\n",
      "epoch no.6 train no.449970  loss = 3.59178 avg_loss = 2.81464\n",
      "epoch no.6 train no.449980  loss = 4.11089 avg_loss = 2.81538\n",
      "epoch no.6 train no.449990  loss = 2.29352 avg_loss = 2.80795\n",
      "epoch no.6 train no.450000  loss = 1.34696 avg_loss = 2.80285\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.6 train no.450010  loss = 3.55088 avg_loss = 2.81545\n",
      "epoch no.6 train no.450020  loss = 2.85142 avg_loss = 2.84133\n",
      "epoch no.6 train no.450030  loss = 3.51585 avg_loss = 2.84135\n",
      "epoch no.6 train no.450040  loss = 2.18201 avg_loss = 2.82616\n",
      "epoch no.6 train no.450050  loss = 3.19585 avg_loss = 2.85248\n",
      "epoch no.6 train no.450060  loss = 2.10253 avg_loss = 2.83826\n",
      "epoch no.6 train no.450070  loss = 2.77581 avg_loss = 2.84973\n",
      "epoch no.6 train no.450080  loss = 3.09837 avg_loss = 2.83591\n",
      "epoch no.6 train no.450090  loss = 3.11080 avg_loss = 2.85718\n",
      "epoch no.6 train no.450100  loss = 4.55961 avg_loss = 2.87908\n",
      "epoch no.6 train no.450110  loss = 2.10878 avg_loss = 2.86322\n",
      "epoch no.6 train no.450120  loss = 2.57282 avg_loss = 2.87403\n",
      "epoch no.6 train no.450130  loss = 3.09893 avg_loss = 2.83956\n",
      "epoch no.6 train no.450140  loss = 2.97875 avg_loss = 2.80319\n",
      "epoch no.6 train no.450150  loss = 4.24892 avg_loss = 2.80070\n",
      "epoch no.6 train no.450160  loss = 1.90480 avg_loss = 2.76563\n",
      "epoch no.6 train no.450170  loss = 4.72731 avg_loss = 2.76835\n",
      "epoch no.6 train no.450180  loss = 3.49009 avg_loss = 2.74306\n",
      "epoch no.6 train no.450190  loss = 4.23701 avg_loss = 2.79900\n",
      "epoch no.6 train no.450200  loss = 4.06767 avg_loss = 2.84374\n",
      "epoch no.6 train no.450210  loss = 2.71683 avg_loss = 2.83829\n",
      "epoch no.6 train no.450220  loss = 2.18221 avg_loss = 2.84379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.450230  loss = 3.00033 avg_loss = 2.85554\n",
      "epoch no.6 train no.450240  loss = 1.97364 avg_loss = 2.82206\n",
      "epoch no.6 train no.450250  loss = 2.08210 avg_loss = 2.83421\n",
      "epoch no.6 train no.450260  loss = 3.13208 avg_loss = 2.84594\n",
      "epoch no.6 train no.450270  loss = 3.23763 avg_loss = 2.80684\n",
      "epoch no.6 train no.450280  loss = 2.12964 avg_loss = 2.80304\n",
      "epoch no.6 train no.450290  loss = 2.26784 avg_loss = 2.82297\n",
      "epoch no.6 train no.450300  loss = 2.03022 avg_loss = 2.81732\n",
      "epoch no.6 train no.450310  loss = 2.61869 avg_loss = 2.84277\n",
      "epoch no.6 train no.450320  loss = 2.62776 avg_loss = 2.87273\n",
      "epoch no.6 train no.450330  loss = 3.87915 avg_loss = 2.86525\n",
      "epoch no.6 train no.450340  loss = 4.56249 avg_loss = 2.87317\n",
      "epoch no.6 train no.450350  loss = 2.50074 avg_loss = 2.85143\n",
      "epoch no.6 train no.450360  loss = 2.45144 avg_loss = 2.83481\n",
      "epoch no.6 train no.450370  loss = 3.26577 avg_loss = 2.85369\n",
      "epoch no.6 train no.450380  loss = 2.88388 avg_loss = 2.85400\n",
      "epoch no.6 train no.450390  loss = 2.58745 avg_loss = 2.82881\n",
      "epoch no.6 train no.450400  loss = 3.00330 avg_loss = 2.86622\n",
      "epoch no.6 train no.450410  loss = 3.46296 avg_loss = 2.85675\n",
      "epoch no.6 train no.450420  loss = 2.93601 avg_loss = 2.90151\n",
      "epoch no.6 train no.450430  loss = 3.77302 avg_loss = 2.89123\n",
      "epoch no.6 train no.450440  loss = 2.23727 avg_loss = 2.88565\n",
      "epoch no.6 train no.450450  loss = 2.51354 avg_loss = 2.87177\n",
      "epoch no.6 train no.450460  loss = 2.42170 avg_loss = 2.83004\n",
      "epoch no.6 train no.450470  loss = 2.33723 avg_loss = 2.81881\n",
      "epoch no.6 train no.450480  loss = 1.92544 avg_loss = 2.82079\n",
      "epoch no.6 train no.450490  loss = 1.83029 avg_loss = 2.77966\n",
      "epoch no.6 train no.450500  loss = 2.63844 avg_loss = 2.76383\n",
      "epoch no.6 train no.450510  loss = 2.99777 avg_loss = 2.75977\n",
      "epoch no.6 train no.450520  loss = 2.76504 avg_loss = 2.79313\n",
      "epoch no.6 train no.450530  loss = 3.07871 avg_loss = 2.83704\n",
      "epoch no.6 train no.450540  loss = 2.50096 avg_loss = 2.86335\n",
      "epoch no.6 train no.450550  loss = 2.96990 avg_loss = 2.85077\n",
      "epoch no.6 train no.450560  loss = 3.20803 avg_loss = 2.86238\n",
      "epoch no.6 train no.450570  loss = 3.10385 avg_loss = 2.85677\n",
      "epoch no.6 train no.450580  loss = 2.70608 avg_loss = 2.86858\n",
      "epoch no.6 train no.450590  loss = 2.37125 avg_loss = 2.88357\n",
      "epoch no.6 train no.450600  loss = 4.04628 avg_loss = 2.90792\n",
      "epoch no.6 train no.450610  loss = 4.13487 avg_loss = 2.89736\n",
      "epoch no.6 train no.450620  loss = 3.22567 avg_loss = 2.86974\n",
      "epoch no.6 train no.450630  loss = 2.12647 avg_loss = 2.90636\n",
      "epoch no.6 train no.450640  loss = 2.46651 avg_loss = 2.87349\n",
      "epoch no.6 train no.450650  loss = 3.18484 avg_loss = 2.88085\n",
      "epoch no.6 train no.450660  loss = 2.21798 avg_loss = 2.87087\n",
      "epoch no.6 train no.450670  loss = 3.23265 avg_loss = 2.87321\n",
      "epoch no.6 train no.450680  loss = 2.85517 avg_loss = 2.86679\n",
      "epoch no.6 train no.450690  loss = 2.95508 avg_loss = 2.87723\n",
      "epoch no.6 train no.450700  loss = 3.09530 avg_loss = 2.86608\n",
      "epoch no.6 train no.450710  loss = 2.37046 avg_loss = 2.86466\n",
      "epoch no.6 train no.450720  loss = 2.37041 avg_loss = 2.83940\n",
      "epoch no.6 train no.450730  loss = 3.15591 avg_loss = 2.83733\n",
      "epoch no.6 train no.450740  loss = 2.35756 avg_loss = 2.81541\n",
      "epoch no.6 train no.450750  loss = 4.57318 avg_loss = 2.88115\n",
      "epoch no.6 train no.450760  loss = 2.49372 avg_loss = 2.92873\n",
      "epoch no.6 train no.450770  loss = 3.23517 avg_loss = 2.96411\n",
      "epoch no.6 train no.450780  loss = 2.79967 avg_loss = 3.00877\n",
      "epoch no.6 train no.450790  loss = 1.82811 avg_loss = 3.00143\n",
      "epoch no.6 train no.450800  loss = 2.60720 avg_loss = 2.99184\n",
      "epoch no.6 train no.450810  loss = 5.46193 avg_loss = 2.99903\n",
      "epoch no.6 train no.450820  loss = 2.29735 avg_loss = 2.92975\n",
      "epoch no.6 train no.450830  loss = 3.24505 avg_loss = 2.91135\n",
      "epoch no.6 train no.450840  loss = 2.32710 avg_loss = 2.90227\n",
      "epoch no.6 train no.450850  loss = 2.98487 avg_loss = 2.87966\n",
      "epoch no.6 train no.450860  loss = 2.23462 avg_loss = 2.82618\n",
      "epoch no.6 train no.450870  loss = 4.02869 avg_loss = 2.82493\n",
      "epoch no.6 train no.450880  loss = 2.25730 avg_loss = 2.84212\n",
      "epoch no.6 train no.450890  loss = 2.13136 avg_loss = 2.85804\n",
      "epoch no.6 train no.450900  loss = 2.93868 avg_loss = 2.86321\n",
      "epoch no.6 train no.450910  loss = 3.82193 avg_loss = 2.87778\n",
      "epoch no.6 train no.450920  loss = 1.92590 avg_loss = 2.85445\n",
      "epoch no.6 train no.450930  loss = 3.65466 avg_loss = 2.83401\n",
      "epoch no.6 train no.450940  loss = 2.85480 avg_loss = 2.86090\n",
      "epoch no.6 train no.450950  loss = 3.15656 avg_loss = 2.86370\n",
      "epoch no.6 train no.450960  loss = 2.33355 avg_loss = 2.86636\n",
      "epoch no.6 train no.450970  loss = 4.20178 avg_loss = 2.86639\n",
      "epoch no.6 train no.450980  loss = 2.32287 avg_loss = 2.83658\n",
      "epoch no.6 train no.450990  loss = 2.10062 avg_loss = 2.85908\n",
      "epoch no.6 train no.451000  loss = 2.33719 avg_loss = 2.86324\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.6 train no.451010  loss = 3.81304 avg_loss = 2.90494\n",
      "epoch no.6 train no.451020  loss = 5.06229 avg_loss = 2.92788\n",
      "epoch no.6 train no.451030  loss = 2.87888 avg_loss = 2.91088\n",
      "epoch no.6 train no.451040  loss = 2.59525 avg_loss = 2.90004\n",
      "epoch no.6 train no.451050  loss = 2.32943 avg_loss = 2.86621\n",
      "epoch no.6 train no.451060  loss = 2.42120 avg_loss = 2.84452\n",
      "epoch no.6 train no.451070  loss = 4.06787 avg_loss = 2.86511\n",
      "epoch no.6 train no.451080  loss = 2.99357 avg_loss = 2.85759\n",
      "epoch no.6 train no.451090  loss = 1.95763 avg_loss = 2.84247\n",
      "epoch no.6 train no.451100  loss = 2.52498 avg_loss = 2.83610\n",
      "epoch no.6 train no.451110  loss = 3.06809 avg_loss = 2.82001\n",
      "epoch no.6 train no.451120  loss = 3.45100 avg_loss = 2.78758\n",
      "epoch no.6 train no.451130  loss = 2.29178 avg_loss = 2.77989\n",
      "epoch no.6 train no.451140  loss = 1.93307 avg_loss = 2.81817\n",
      "epoch no.6 train no.451150  loss = 2.56551 avg_loss = 2.79445\n",
      "epoch no.6 train no.451160  loss = 3.68196 avg_loss = 2.79553\n",
      "epoch no.6 train no.451170  loss = 2.82712 avg_loss = 2.83387\n",
      "epoch no.6 train no.451180  loss = 2.77744 avg_loss = 2.85769\n",
      "epoch no.6 train no.451190  loss = 2.11564 avg_loss = 2.82332\n",
      "epoch no.6 train no.451200  loss = 2.13247 avg_loss = 2.77610\n",
      "epoch no.6 train no.451210  loss = 2.73440 avg_loss = 2.82651\n",
      "epoch no.6 train no.451220  loss = 1.36757 avg_loss = 2.82442\n",
      "epoch no.6 train no.451230  loss = 3.22758 avg_loss = 2.83593\n",
      "epoch no.6 train no.451240  loss = 1.55182 avg_loss = 2.83891\n",
      "epoch no.6 train no.451250  loss = 1.96356 avg_loss = 2.86519\n",
      "epoch no.6 train no.451260  loss = 2.37249 avg_loss = 2.85995\n",
      "epoch no.6 train no.451270  loss = 2.40666 avg_loss = 2.83365\n",
      "epoch no.6 train no.451280  loss = 2.09138 avg_loss = 2.85277\n",
      "epoch no.6 train no.451290  loss = 3.43969 avg_loss = 2.86365\n",
      "epoch no.6 train no.451300  loss = 2.78630 avg_loss = 2.86567\n",
      "epoch no.6 train no.451310  loss = 2.07416 avg_loss = 2.83734\n",
      "epoch no.6 train no.451320  loss = 1.98136 avg_loss = 2.81176\n",
      "epoch no.6 train no.451330  loss = 3.60617 avg_loss = 2.85188\n",
      "epoch no.6 train no.451340  loss = 3.12322 avg_loss = 2.82679\n",
      "epoch no.6 train no.451350  loss = 1.99858 avg_loss = 2.81461\n",
      "epoch no.6 train no.451360  loss = 2.46161 avg_loss = 2.78558\n",
      "epoch no.6 train no.451370  loss = 3.54425 avg_loss = 2.78348\n",
      "epoch no.6 train no.451380  loss = 2.39126 avg_loss = 2.80131\n",
      "epoch no.6 train no.451390  loss = 3.32005 avg_loss = 2.80622\n",
      "epoch no.6 train no.451400  loss = 2.93639 avg_loss = 2.82274\n",
      "epoch no.6 train no.451410  loss = 3.33843 avg_loss = 2.80937\n",
      "epoch no.6 train no.451420  loss = 4.07478 avg_loss = 2.83741\n",
      "epoch no.6 train no.451430  loss = 3.32046 avg_loss = 2.85595\n",
      "epoch no.6 train no.451440  loss = 3.17435 avg_loss = 2.86773\n",
      "epoch no.6 train no.451450  loss = 2.24416 avg_loss = 2.88639\n",
      "epoch no.6 train no.451460  loss = 1.89696 avg_loss = 2.83474\n",
      "epoch no.6 train no.451470  loss = 2.88414 avg_loss = 2.87544\n",
      "epoch no.6 train no.451480  loss = 3.70437 avg_loss = 2.84905\n",
      "epoch no.6 train no.451490  loss = 3.24843 avg_loss = 2.83872\n",
      "epoch no.6 train no.451500  loss = 2.38491 avg_loss = 2.84905\n",
      "epoch no.6 train no.451510  loss = 1.71578 avg_loss = 2.89125\n",
      "epoch no.6 train no.451520  loss = 1.60617 avg_loss = 2.87704\n",
      "epoch no.6 train no.451530  loss = 3.74245 avg_loss = 2.88273\n",
      "epoch no.6 train no.451540  loss = 2.27191 avg_loss = 2.86084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.451550  loss = 4.35862 avg_loss = 2.86456\n",
      "epoch no.6 train no.451560  loss = 2.83876 avg_loss = 2.85944\n",
      "epoch no.6 train no.451570  loss = 3.66776 avg_loss = 2.87825\n",
      "epoch no.6 train no.451580  loss = 3.51367 avg_loss = 2.86261\n",
      "epoch no.6 train no.451590  loss = 2.91004 avg_loss = 2.88199\n",
      "epoch no.6 train no.451600  loss = 2.35663 avg_loss = 2.85135\n",
      "epoch no.6 train no.451610  loss = 2.56506 avg_loss = 2.81608\n",
      "epoch no.6 train no.451620  loss = 2.36229 avg_loss = 2.81602\n",
      "epoch no.6 train no.451630  loss = 3.05948 avg_loss = 2.82530\n",
      "epoch no.6 train no.451640  loss = 2.60741 avg_loss = 2.83007\n",
      "epoch no.6 train no.451650  loss = 4.13349 avg_loss = 2.84308\n",
      "epoch no.6 train no.451660  loss = 2.89147 avg_loss = 2.86108\n",
      "epoch no.6 train no.451670  loss = 4.57214 avg_loss = 2.87504\n",
      "epoch no.6 train no.451680  loss = 2.77397 avg_loss = 2.87020\n",
      "epoch no.6 train no.451690  loss = 3.04774 avg_loss = 2.87333\n",
      "epoch no.6 train no.451700  loss = 4.48244 avg_loss = 2.88886\n",
      "epoch no.6 train no.451710  loss = 2.56974 avg_loss = 2.90635\n",
      "epoch no.6 train no.451720  loss = 3.82136 avg_loss = 2.94113\n",
      "epoch no.6 train no.451730  loss = 2.61239 avg_loss = 2.93401\n",
      "epoch no.6 train no.451740  loss = 3.36784 avg_loss = 2.94652\n",
      "epoch no.6 train no.451750  loss = 3.21001 avg_loss = 2.93883\n",
      "epoch no.6 train no.451760  loss = 3.74225 avg_loss = 2.94034\n",
      "epoch no.6 train no.451770  loss = 3.26425 avg_loss = 2.89837\n",
      "epoch no.6 train no.451780  loss = 3.99034 avg_loss = 2.90743\n",
      "epoch no.6 train no.451790  loss = 1.95167 avg_loss = 2.86065\n",
      "epoch no.6 train no.451800  loss = 2.23782 avg_loss = 2.84807\n",
      "epoch no.6 train no.451810  loss = 3.17120 avg_loss = 2.84950\n",
      "epoch no.6 train no.451820  loss = 3.53067 avg_loss = 2.87844\n",
      "epoch no.6 train no.451830  loss = 2.37285 avg_loss = 2.85057\n",
      "epoch no.6 train no.451840  loss = 2.76003 avg_loss = 2.85058\n",
      "epoch no.6 train no.451850  loss = 2.36519 avg_loss = 2.86900\n",
      "epoch no.6 train no.451860  loss = 2.03215 avg_loss = 2.82227\n",
      "epoch no.6 train no.451870  loss = 2.86231 avg_loss = 2.84168\n",
      "epoch no.6 train no.451880  loss = 3.55212 avg_loss = 2.84724\n",
      "epoch no.6 train no.451890  loss = 2.92119 avg_loss = 2.83277\n",
      "epoch no.6 train no.451900  loss = 2.94369 avg_loss = 2.86101\n",
      "epoch no.6 train no.451910  loss = 2.07872 avg_loss = 2.85854\n",
      "epoch no.6 train no.451920  loss = 3.41416 avg_loss = 2.83591\n",
      "epoch no.6 train no.451930  loss = 3.16942 avg_loss = 2.85248\n",
      "epoch no.6 train no.451940  loss = 3.02370 avg_loss = 2.85456\n",
      "epoch no.6 train no.451950  loss = 2.79104 avg_loss = 2.85507\n",
      "epoch no.6 train no.451960  loss = 2.20987 avg_loss = 2.86806\n",
      "epoch no.6 train no.451970  loss = 3.14089 avg_loss = 2.85335\n",
      "epoch no.6 train no.451980  loss = 3.40389 avg_loss = 2.86601\n",
      "epoch no.6 train no.451990  loss = 2.76313 avg_loss = 2.90215\n",
      "epoch no.6 train no.452000  loss = 2.23797 avg_loss = 2.91290\n",
      "8\n",
      "to_tokens: ['▁가을', '▁90', '년대', '대를', '▁대표하는', '놓은', '▁인기가', '곡', '</s>', '</s>']\n",
      "추억의 90년대를 수놓은 댄스곡들</s>\n",
      "epoch no.6 train no.452010  loss = 3.64345 avg_loss = 2.88488\n",
      "epoch no.6 train no.452020  loss = 3.14628 avg_loss = 2.92382\n",
      "epoch no.6 train no.452030  loss = 3.76788 avg_loss = 2.94106\n",
      "epoch no.6 train no.452040  loss = 2.48261 avg_loss = 2.93276\n",
      "epoch no.6 train no.452050  loss = 4.46228 avg_loss = 2.90786\n",
      "epoch no.6 train no.452060  loss = 3.58756 avg_loss = 2.92233\n",
      "epoch no.6 train no.452070  loss = 3.20411 avg_loss = 2.91256\n",
      "epoch no.6 train no.452080  loss = 2.46222 avg_loss = 2.89791\n",
      "epoch no.6 train no.452090  loss = 1.85312 avg_loss = 2.84017\n",
      "epoch no.6 train no.452100  loss = 2.93333 avg_loss = 2.82113\n",
      "epoch no.6 train no.452110  loss = 2.44485 avg_loss = 2.84034\n",
      "epoch no.6 train no.452120  loss = 4.25988 avg_loss = 2.83935\n",
      "epoch no.6 train no.452130  loss = 2.80585 avg_loss = 2.85856\n",
      "epoch no.6 train no.452140  loss = 3.90967 avg_loss = 2.86625\n",
      "epoch no.6 train no.452150  loss = 2.72546 avg_loss = 2.88181\n",
      "epoch no.6 train no.452160  loss = 2.26736 avg_loss = 2.84686\n",
      "epoch no.6 train no.452170  loss = 3.18187 avg_loss = 2.88262\n",
      "epoch no.6 train no.452180  loss = 2.84850 avg_loss = 2.84904\n",
      "epoch no.6 train no.452190  loss = 1.88594 avg_loss = 2.82363\n",
      "epoch no.6 train no.452200  loss = 2.10038 avg_loss = 2.82780\n",
      "epoch no.6 train no.452210  loss = 3.51271 avg_loss = 2.83345\n",
      "epoch no.6 train no.452220  loss = 2.76943 avg_loss = 2.81898\n",
      "epoch no.6 train no.452230  loss = 3.24514 avg_loss = 2.82967\n",
      "epoch no.6 train no.452240  loss = 3.25757 avg_loss = 2.88508\n",
      "epoch no.6 train no.452250  loss = 2.02564 avg_loss = 2.87046\n",
      "epoch no.6 train no.452260  loss = 2.42902 avg_loss = 2.87297\n",
      "epoch no.6 train no.452270  loss = 1.75534 avg_loss = 2.86637\n",
      "epoch no.6 train no.452280  loss = 2.16465 avg_loss = 2.87269\n",
      "epoch no.6 train no.452290  loss = 5.12630 avg_loss = 2.86793\n",
      "epoch no.6 train no.452300  loss = 1.99568 avg_loss = 2.87830\n",
      "epoch no.6 train no.452310  loss = 3.00580 avg_loss = 2.84187\n",
      "epoch no.6 train no.452320  loss = 2.12536 avg_loss = 2.84860\n",
      "epoch no.6 train no.452330  loss = 2.60838 avg_loss = 2.84306\n",
      "epoch no.6 train no.452340  loss = 3.70140 avg_loss = 2.84938\n",
      "epoch no.6 train no.452350  loss = 1.64543 avg_loss = 2.83537\n",
      "epoch no.6 train no.452360  loss = 3.21607 avg_loss = 2.84908\n",
      "epoch no.6 train no.452370  loss = 2.88848 avg_loss = 2.83615\n",
      "epoch no.6 train no.452380  loss = 2.57954 avg_loss = 2.82949\n",
      "epoch no.6 train no.452390  loss = 1.54002 avg_loss = 2.81880\n",
      "epoch no.6 train no.452400  loss = 2.92039 avg_loss = 2.82455\n",
      "epoch no.6 train no.452410  loss = 2.56064 avg_loss = 2.79928\n",
      "epoch no.6 train no.452420  loss = 2.16424 avg_loss = 2.79717\n",
      "epoch no.6 train no.452430  loss = 3.83807 avg_loss = 2.83431\n",
      "epoch no.6 train no.452440  loss = 3.09521 avg_loss = 2.83969\n",
      "epoch no.6 train no.452450  loss = 3.23785 avg_loss = 2.84146\n",
      "epoch no.6 train no.452460  loss = 3.15189 avg_loss = 2.80080\n",
      "epoch no.6 train no.452470  loss = 2.92039 avg_loss = 2.83751\n",
      "epoch no.6 train no.452480  loss = 3.04595 avg_loss = 2.85020\n",
      "epoch no.6 train no.452490  loss = 2.42831 avg_loss = 2.83450\n",
      "epoch no.6 train no.452500  loss = 2.23159 avg_loss = 2.83420\n",
      "epoch no.6 train no.452510  loss = 2.95887 avg_loss = 2.84849\n",
      "epoch no.6 train no.452520  loss = 2.56522 avg_loss = 2.85995\n",
      "epoch no.6 train no.452530  loss = 2.68077 avg_loss = 2.84651\n",
      "epoch no.6 train no.452540  loss = 2.25864 avg_loss = 2.81265\n",
      "epoch no.6 train no.452550  loss = 1.37253 avg_loss = 2.80421\n",
      "epoch no.6 train no.452560  loss = 2.37668 avg_loss = 2.80476\n",
      "epoch no.6 train no.452570  loss = 1.66601 avg_loss = 2.80097\n",
      "epoch no.6 train no.452580  loss = 1.97298 avg_loss = 2.79532\n",
      "epoch no.6 train no.452590  loss = 2.33348 avg_loss = 2.79955\n",
      "epoch no.6 train no.452600  loss = 1.82060 avg_loss = 2.80831\n",
      "epoch no.6 train no.452610  loss = 3.65575 avg_loss = 2.82506\n",
      "epoch no.6 train no.452620  loss = 3.37326 avg_loss = 2.81503\n",
      "epoch no.6 train no.452630  loss = 1.70587 avg_loss = 2.79275\n",
      "epoch no.6 train no.452640  loss = 1.91385 avg_loss = 2.81120\n",
      "epoch no.6 train no.452650  loss = 3.11214 avg_loss = 2.75267\n",
      "epoch no.6 train no.452660  loss = 3.26079 avg_loss = 2.77946\n",
      "epoch no.6 train no.452670  loss = 2.94114 avg_loss = 2.77921\n",
      "epoch no.6 train no.452680  loss = 2.42881 avg_loss = 2.77601\n",
      "epoch no.6 train no.452690  loss = 3.28381 avg_loss = 2.84910\n",
      "epoch no.6 train no.452700  loss = 2.83396 avg_loss = 2.85349\n",
      "epoch no.6 train no.452710  loss = 3.23797 avg_loss = 2.83348\n",
      "epoch no.6 train no.452720  loss = 2.72437 avg_loss = 2.83109\n",
      "epoch no.6 train no.452730  loss = 2.85928 avg_loss = 2.84063\n",
      "epoch no.6 train no.452740  loss = 2.54366 avg_loss = 2.86096\n",
      "epoch no.6 train no.452750  loss = 2.08406 avg_loss = 2.81307\n",
      "epoch no.6 train no.452760  loss = 4.02592 avg_loss = 2.83533\n",
      "epoch no.6 train no.452770  loss = 3.18550 avg_loss = 2.83249\n",
      "epoch no.6 train no.452780  loss = 2.53213 avg_loss = 2.83448\n",
      "epoch no.6 train no.452790  loss = 3.77019 avg_loss = 2.86056\n",
      "epoch no.6 train no.452800  loss = 2.74808 avg_loss = 2.86798\n",
      "epoch no.6 train no.452810  loss = 1.76970 avg_loss = 2.84833\n",
      "epoch no.6 train no.452820  loss = 3.55324 avg_loss = 2.89200\n",
      "epoch no.6 train no.452830  loss = 2.68739 avg_loss = 2.87306\n",
      "epoch no.6 train no.452840  loss = 3.50865 avg_loss = 2.85344\n",
      "epoch no.6 train no.452850  loss = 2.86480 avg_loss = 2.84867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.452860  loss = 1.91925 avg_loss = 2.81522\n",
      "epoch no.6 train no.452870  loss = 2.67787 avg_loss = 2.77494\n",
      "epoch no.6 train no.452880  loss = 3.56824 avg_loss = 2.81981\n",
      "epoch no.6 train no.452890  loss = 3.81469 avg_loss = 2.84194\n",
      "epoch no.6 train no.452900  loss = 4.61092 avg_loss = 2.84176\n",
      "epoch no.6 train no.452910  loss = 2.94187 avg_loss = 2.85060\n",
      "epoch no.6 train no.452920  loss = 2.44015 avg_loss = 2.86098\n",
      "epoch no.6 train no.452930  loss = 2.68146 avg_loss = 2.91281\n",
      "epoch no.6 train no.452940  loss = 2.47184 avg_loss = 2.90681\n",
      "epoch no.6 train no.452950  loss = 2.44727 avg_loss = 2.88412\n",
      "epoch no.6 train no.452960  loss = 2.49410 avg_loss = 2.86114\n",
      "epoch no.6 train no.452970  loss = 2.40764 avg_loss = 2.83927\n",
      "epoch no.6 train no.452980  loss = 2.45449 avg_loss = 2.83277\n",
      "epoch no.6 train no.452990  loss = 2.65883 avg_loss = 2.80866\n",
      "epoch no.6 train no.453000  loss = 2.20202 avg_loss = 2.80365\n",
      "4\n",
      "to_tokens: ['▁비', '▁싸이', '년대', '▁댄스', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.6 train no.453010  loss = 1.83793 avg_loss = 2.78928\n",
      "epoch no.6 train no.453020  loss = 2.04953 avg_loss = 2.77113\n",
      "epoch no.6 train no.453030  loss = 2.54651 avg_loss = 2.76841\n",
      "epoch no.6 train no.453040  loss = 3.12183 avg_loss = 2.76913\n",
      "epoch no.6 train no.453050  loss = 3.02320 avg_loss = 2.76166\n",
      "epoch no.6 train no.453060  loss = 2.75934 avg_loss = 2.77073\n",
      "epoch no.6 train no.453070  loss = 3.70959 avg_loss = 2.77945\n",
      "epoch no.6 train no.453080  loss = 4.55418 avg_loss = 2.80809\n",
      "epoch no.6 train no.453090  loss = 4.29086 avg_loss = 2.85564\n",
      "epoch no.6 train no.453100  loss = 3.90575 avg_loss = 2.86051\n",
      "epoch no.6 train no.453110  loss = 2.54048 avg_loss = 2.87583\n",
      "epoch no.6 train no.453120  loss = 3.39061 avg_loss = 2.92312\n",
      "epoch no.6 train no.453130  loss = 2.25730 avg_loss = 2.90686\n",
      "epoch no.6 train no.453140  loss = 2.72735 avg_loss = 2.91796\n",
      "epoch no.6 train no.453150  loss = 2.02421 avg_loss = 2.89917\n",
      "epoch no.6 train no.453160  loss = 2.28269 avg_loss = 2.89687\n",
      "epoch no.6 train no.453170  loss = 2.34942 avg_loss = 2.91372\n",
      "epoch no.6 train no.453180  loss = 3.46871 avg_loss = 2.87581\n",
      "epoch no.6 train no.453190  loss = 2.71925 avg_loss = 2.84845\n",
      "epoch no.6 train no.453200  loss = 3.31639 avg_loss = 2.87761\n",
      "epoch no.6 train no.453210  loss = 4.77260 avg_loss = 2.92738\n",
      "epoch no.6 train no.453220  loss = 3.23008 avg_loss = 2.92058\n",
      "epoch no.6 train no.453230  loss = 1.59606 avg_loss = 2.92452\n",
      "epoch no.6 train no.453240  loss = 2.79996 avg_loss = 2.90637\n",
      "epoch no.6 train no.453250  loss = 3.77086 avg_loss = 2.93233\n",
      "epoch no.6 train no.453260  loss = 1.90911 avg_loss = 2.92796\n",
      "epoch no.6 train no.453270  loss = 3.75935 avg_loss = 2.95317\n",
      "epoch no.6 train no.453280  loss = 2.42436 avg_loss = 2.97108\n",
      "epoch no.6 train no.453290  loss = 2.67491 avg_loss = 2.92505\n",
      "epoch no.6 train no.453300  loss = 3.67770 avg_loss = 2.88735\n",
      "epoch no.6 train no.453310  loss = 4.76372 avg_loss = 2.86744\n",
      "epoch no.6 train no.453320  loss = 1.76711 avg_loss = 2.82631\n",
      "epoch no.6 train no.453330  loss = 2.17127 avg_loss = 2.83226\n",
      "epoch no.6 train no.453340  loss = 2.73931 avg_loss = 2.84590\n",
      "epoch no.6 train no.453350  loss = 1.70456 avg_loss = 2.84003\n",
      "epoch no.6 train no.453360  loss = 2.38666 avg_loss = 2.81913\n",
      "epoch no.6 train no.453370  loss = 3.75637 avg_loss = 2.84810\n",
      "epoch no.6 train no.453380  loss = 1.72099 avg_loss = 2.80413\n",
      "epoch no.6 train no.453390  loss = 3.18967 avg_loss = 2.81145\n",
      "epoch no.6 train no.453400  loss = 1.80634 avg_loss = 2.83604\n",
      "epoch no.6 train no.453410  loss = 2.12587 avg_loss = 2.84675\n",
      "epoch no.6 train no.453420  loss = 3.02147 avg_loss = 2.86560\n",
      "epoch no.6 train no.453430  loss = 3.91582 avg_loss = 2.88936\n",
      "epoch no.6 train no.453440  loss = 3.07641 avg_loss = 2.88485\n",
      "epoch no.6 train no.453450  loss = 2.32583 avg_loss = 2.88362\n",
      "epoch no.6 train no.453460  loss = 2.71909 avg_loss = 2.85685\n",
      "epoch no.6 train no.453470  loss = 3.21127 avg_loss = 2.86970\n",
      "epoch no.6 train no.453480  loss = 2.35849 avg_loss = 2.90070\n",
      "epoch no.6 train no.453490  loss = 2.31577 avg_loss = 2.96427\n",
      "epoch no.6 train no.453500  loss = 2.96003 avg_loss = 2.95140\n",
      "epoch no.6 train no.453510  loss = 4.17958 avg_loss = 2.95796\n",
      "epoch no.6 train no.453520  loss = 2.34090 avg_loss = 2.94226\n",
      "epoch no.6 train no.453530  loss = 3.61697 avg_loss = 2.94307\n",
      "epoch no.6 train no.453540  loss = 2.23587 avg_loss = 2.89190\n",
      "epoch no.6 train no.453550  loss = 3.47304 avg_loss = 2.88959\n",
      "epoch no.6 train no.453560  loss = 3.06393 avg_loss = 2.92293\n",
      "epoch no.6 train no.453570  loss = 3.84841 avg_loss = 2.92473\n",
      "epoch no.6 train no.453580  loss = 3.36806 avg_loss = 2.90271\n",
      "epoch no.6 train no.453590  loss = 3.02644 avg_loss = 2.91166\n",
      "epoch no.6 train no.453600  loss = 3.10331 avg_loss = 2.91691\n",
      "epoch no.6 train no.453610  loss = 2.26727 avg_loss = 2.89593\n",
      "epoch no.6 train no.453620  loss = 1.84812 avg_loss = 2.91261\n",
      "epoch no.6 train no.453630  loss = 2.74532 avg_loss = 2.86979\n",
      "epoch no.6 train no.453640  loss = 3.40035 avg_loss = 2.89094\n",
      "epoch no.6 train no.453650  loss = 1.98026 avg_loss = 2.86874\n",
      "epoch no.6 train no.453660  loss = 2.19521 avg_loss = 2.86007\n",
      "epoch no.6 train no.453670  loss = 2.95491 avg_loss = 2.86068\n",
      "epoch no.6 train no.453680  loss = 2.30754 avg_loss = 2.85937\n",
      "epoch no.6 train no.453690  loss = 3.16471 avg_loss = 2.84152\n",
      "epoch no.6 train no.453700  loss = 2.74494 avg_loss = 2.83086\n",
      "epoch no.6 train no.453710  loss = 3.71754 avg_loss = 2.83407\n",
      "epoch no.6 train no.453720  loss = 4.26369 avg_loss = 2.85659\n",
      "epoch no.6 train no.453730  loss = 2.00184 avg_loss = 2.85157\n",
      "epoch no.6 train no.453740  loss = 1.96391 avg_loss = 2.83862\n",
      "epoch no.6 train no.453750  loss = 3.18461 avg_loss = 2.80270\n",
      "epoch no.6 train no.453760  loss = 1.87027 avg_loss = 2.79425\n",
      "epoch no.6 train no.453770  loss = 3.22155 avg_loss = 2.85426\n",
      "epoch no.6 train no.453780  loss = 2.09712 avg_loss = 2.86482\n",
      "epoch no.6 train no.453790  loss = 2.71065 avg_loss = 2.83467\n",
      "epoch no.6 train no.453800  loss = 2.40024 avg_loss = 2.86544\n",
      "epoch no.6 train no.453810  loss = 3.03568 avg_loss = 2.84803\n",
      "epoch no.6 train no.453820  loss = 4.84146 avg_loss = 2.90029\n",
      "epoch no.6 train no.453830  loss = 3.09003 avg_loss = 2.91874\n",
      "epoch no.6 train no.453840  loss = 2.54183 avg_loss = 2.93902\n",
      "epoch no.6 train no.453850  loss = 3.26974 avg_loss = 2.89005\n",
      "epoch no.6 train no.453860  loss = 3.09523 avg_loss = 2.87087\n",
      "epoch no.6 train no.453870  loss = 3.50541 avg_loss = 2.87480\n",
      "epoch no.6 train no.453880  loss = 2.44016 avg_loss = 2.86038\n",
      "epoch no.6 train no.453890  loss = 2.04651 avg_loss = 2.84539\n",
      "epoch no.6 train no.453900  loss = 2.90182 avg_loss = 2.83729\n",
      "epoch no.6 train no.453910  loss = 3.29570 avg_loss = 2.83641\n",
      "epoch no.6 train no.453920  loss = 2.20784 avg_loss = 2.83173\n",
      "epoch no.6 train no.453930  loss = 2.79643 avg_loss = 2.82946\n",
      "epoch no.6 train no.453940  loss = 3.13944 avg_loss = 2.81932\n",
      "epoch no.6 train no.453950  loss = 2.72404 avg_loss = 2.79318\n",
      "epoch no.6 train no.453960  loss = 3.58372 avg_loss = 2.82243\n",
      "epoch no.6 train no.453970  loss = 3.08637 avg_loss = 2.81954\n",
      "epoch no.6 train no.453980  loss = 2.16630 avg_loss = 2.81536\n",
      "epoch no.6 train no.453990  loss = 4.39633 avg_loss = 2.81421\n",
      "epoch no.6 train no.454000  loss = 3.92526 avg_loss = 2.80554\n",
      "4\n",
      "to_tokens: ['▁비', '팝', '년대', '▁댄스', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.6 train no.454010  loss = 3.65257 avg_loss = 2.82026\n",
      "epoch no.6 train no.454020  loss = 3.74638 avg_loss = 2.86184\n",
      "epoch no.6 train no.454030  loss = 2.98047 avg_loss = 2.91141\n",
      "epoch no.6 train no.454040  loss = 2.81737 avg_loss = 2.87927\n",
      "epoch no.6 train no.454050  loss = 4.02290 avg_loss = 2.88903\n",
      "epoch no.6 train no.454060  loss = 2.64978 avg_loss = 2.92074\n",
      "epoch no.6 train no.454070  loss = 2.23290 avg_loss = 2.91330\n",
      "epoch no.6 train no.454080  loss = 2.95212 avg_loss = 2.90545\n",
      "epoch no.6 train no.454090  loss = 3.18494 avg_loss = 2.89489\n",
      "epoch no.6 train no.454100  loss = 2.83213 avg_loss = 2.88267\n",
      "epoch no.6 train no.454110  loss = 2.78771 avg_loss = 2.85054\n",
      "epoch no.6 train no.454120  loss = 2.81046 avg_loss = 2.84012\n",
      "epoch no.6 train no.454130  loss = 4.48055 avg_loss = 2.89830\n",
      "epoch no.6 train no.454140  loss = 2.89902 avg_loss = 2.87911\n",
      "epoch no.6 train no.454150  loss = 4.11011 avg_loss = 2.89164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.454160  loss = 2.82296 avg_loss = 2.90337\n",
      "epoch no.6 train no.454170  loss = 2.91671 avg_loss = 2.90264\n",
      "epoch no.6 train no.454180  loss = 2.88365 avg_loss = 2.93211\n",
      "epoch no.6 train no.454190  loss = 1.97025 avg_loss = 2.91917\n",
      "epoch no.6 train no.454200  loss = 2.44681 avg_loss = 2.91135\n",
      "epoch no.6 train no.454210  loss = 2.84163 avg_loss = 2.90455\n",
      "epoch no.6 train no.454220  loss = 2.01649 avg_loss = 2.90531\n",
      "epoch no.6 train no.454230  loss = 3.03699 avg_loss = 2.88360\n",
      "epoch no.6 train no.454240  loss = 4.22880 avg_loss = 2.89778\n",
      "epoch no.6 train no.454250  loss = 1.96526 avg_loss = 2.87403\n",
      "epoch no.6 train no.454260  loss = 2.80433 avg_loss = 2.86893\n",
      "epoch no.6 train no.454270  loss = 2.73409 avg_loss = 2.89696\n",
      "epoch no.6 train no.454280  loss = 3.61049 avg_loss = 2.91242\n",
      "epoch no.6 train no.454290  loss = 2.79705 avg_loss = 2.92080\n",
      "epoch no.6 train no.454300  loss = 2.39989 avg_loss = 2.91089\n",
      "epoch no.6 train no.454310  loss = 2.14440 avg_loss = 2.87739\n",
      "epoch no.6 train no.454320  loss = 3.30017 avg_loss = 2.88796\n",
      "epoch no.6 train no.454330  loss = 1.89714 avg_loss = 2.88340\n",
      "epoch no.6 train no.454340  loss = 3.87416 avg_loss = 2.90064\n",
      "epoch no.6 train no.454350  loss = 3.56455 avg_loss = 2.90723\n",
      "epoch no.6 train no.454360  loss = 3.84566 avg_loss = 2.92281\n",
      "epoch no.6 train no.454370  loss = 3.30833 avg_loss = 2.90794\n",
      "epoch no.6 train no.454380  loss = 2.54424 avg_loss = 2.89379\n",
      "epoch no.6 train no.454390  loss = 3.03172 avg_loss = 2.90795\n",
      "epoch no.6 train no.454400  loss = 1.17486 avg_loss = 2.86172\n",
      "epoch no.6 train no.454410  loss = 3.37118 avg_loss = 2.86631\n",
      "epoch no.6 train no.454420  loss = 3.31929 avg_loss = 2.87713\n",
      "epoch no.6 train no.454430  loss = 2.16160 avg_loss = 2.86078\n",
      "epoch no.6 train no.454440  loss = 2.60310 avg_loss = 2.85121\n",
      "epoch no.6 train no.454450  loss = 3.41062 avg_loss = 2.82754\n",
      "epoch no.6 train no.454460  loss = 2.80496 avg_loss = 2.84119\n",
      "epoch no.6 train no.454470  loss = 2.84639 avg_loss = 2.82833\n",
      "epoch no.6 train no.454480  loss = 3.27038 avg_loss = 2.84191\n",
      "epoch no.6 train no.454490  loss = 2.17401 avg_loss = 2.83106\n",
      "epoch no.6 train no.454500  loss = 2.20424 avg_loss = 2.83491\n",
      "epoch no.6 train no.454510  loss = 2.37973 avg_loss = 2.82960\n",
      "epoch no.6 train no.454520  loss = 2.72741 avg_loss = 2.80201\n",
      "epoch no.6 train no.454530  loss = 1.93380 avg_loss = 2.85454\n",
      "epoch no.6 train no.454540  loss = 2.16602 avg_loss = 2.81541\n",
      "epoch no.6 train no.454550  loss = 2.01072 avg_loss = 2.87146\n",
      "epoch no.6 train no.454560  loss = 3.18470 avg_loss = 2.89570\n",
      "epoch no.6 train no.454570  loss = 4.50643 avg_loss = 2.89723\n",
      "epoch no.6 train no.454580  loss = 2.64073 avg_loss = 2.89547\n",
      "epoch no.6 train no.454590  loss = 2.38857 avg_loss = 2.88352\n",
      "epoch no.6 train no.454600  loss = 2.55166 avg_loss = 2.86787\n",
      "epoch no.6 train no.454610  loss = 4.17240 avg_loss = 2.85997\n",
      "epoch no.6 train no.454620  loss = 3.00787 avg_loss = 2.86075\n",
      "epoch no.6 train no.454630  loss = 2.65728 avg_loss = 2.85032\n",
      "epoch no.6 train no.454640  loss = 3.59268 avg_loss = 2.87358\n",
      "epoch no.6 train no.454650  loss = 2.86019 avg_loss = 2.86210\n",
      "epoch no.6 train no.454660  loss = 2.70402 avg_loss = 2.84982\n",
      "epoch no.6 train no.454670  loss = 2.10665 avg_loss = 2.87687\n",
      "epoch no.6 train no.454680  loss = 3.02478 avg_loss = 2.86552\n",
      "epoch no.6 train no.454690  loss = 2.56520 avg_loss = 2.84646\n",
      "epoch no.6 train no.454700  loss = 4.04198 avg_loss = 2.85566\n",
      "epoch no.6 train no.454710  loss = 2.70518 avg_loss = 2.85051\n",
      "epoch no.6 train no.454720  loss = 1.65613 avg_loss = 2.88435\n",
      "epoch no.6 train no.454730  loss = 2.10758 avg_loss = 2.88419\n",
      "epoch no.6 train no.454740  loss = 2.74016 avg_loss = 2.85537\n",
      "epoch no.6 train no.454750  loss = 2.10388 avg_loss = 2.86400\n",
      "epoch no.6 train no.454760  loss = 2.53189 avg_loss = 2.88101\n",
      "epoch no.6 train no.454770  loss = 2.48316 avg_loss = 2.90139\n",
      "epoch no.6 train no.454780  loss = 2.83665 avg_loss = 2.89955\n",
      "epoch no.6 train no.454790  loss = 2.96173 avg_loss = 2.87983\n",
      "epoch no.6 train no.454800  loss = 2.98957 avg_loss = 2.88262\n",
      "epoch no.6 train no.454810  loss = 2.64617 avg_loss = 2.89651\n",
      "epoch no.6 train no.454820  loss = 1.78586 avg_loss = 2.86726\n",
      "epoch no.6 train no.454830  loss = 4.44534 avg_loss = 2.86873\n",
      "epoch no.6 train no.454840  loss = 3.28006 avg_loss = 2.85868\n",
      "epoch no.6 train no.454850  loss = 3.14785 avg_loss = 2.86999\n",
      "epoch no.6 train no.454860  loss = 2.67698 avg_loss = 2.84548\n",
      "epoch no.6 train no.454870  loss = 3.16621 avg_loss = 2.86798\n",
      "epoch no.6 train no.454880  loss = 2.52402 avg_loss = 2.87098\n",
      "epoch no.6 train no.454890  loss = 2.69610 avg_loss = 2.86181\n",
      "epoch no.6 train no.454900  loss = 2.55736 avg_loss = 2.87956\n",
      "epoch no.6 train no.454910  loss = 2.61411 avg_loss = 2.90358\n",
      "epoch no.6 train no.454920  loss = 1.48645 avg_loss = 2.89728\n",
      "epoch no.6 train no.454930  loss = 3.36548 avg_loss = 2.88629\n",
      "epoch no.6 train no.454940  loss = 3.66284 avg_loss = 2.87137\n",
      "epoch no.6 train no.454950  loss = 2.89925 avg_loss = 2.86235\n",
      "epoch no.6 train no.454960  loss = 2.41260 avg_loss = 2.84846\n",
      "epoch no.6 train no.454970  loss = 3.04038 avg_loss = 2.80253\n",
      "epoch no.6 train no.454980  loss = 2.80456 avg_loss = 2.82714\n",
      "epoch no.6 train no.454990  loss = 1.88399 avg_loss = 2.83051\n",
      "epoch no.6 train no.455000  loss = 3.19574 avg_loss = 2.83703\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '▁모음', '음', '▁90', '</s>']\n",
      "추억의 가요모음 1</s>\n",
      "epoch no.6 train no.455010  loss = 3.69980 avg_loss = 2.84162\n",
      "epoch no.6 train no.455020  loss = 3.42566 avg_loss = 2.85027\n",
      "epoch no.6 train no.455030  loss = 2.19429 avg_loss = 2.84049\n",
      "epoch no.6 train no.455040  loss = 2.31345 avg_loss = 2.84994\n",
      "epoch no.6 train no.455050  loss = 2.43829 avg_loss = 2.87243\n",
      "epoch no.6 train no.455060  loss = 4.32393 avg_loss = 2.88771\n",
      "epoch no.6 train no.455070  loss = 3.41490 avg_loss = 2.89973\n",
      "epoch no.6 train no.455080  loss = 2.11410 avg_loss = 2.87426\n",
      "epoch no.6 train no.455090  loss = 2.18362 avg_loss = 2.90262\n",
      "epoch no.6 train no.455100  loss = 2.14336 avg_loss = 2.90562\n",
      "epoch no.6 train no.455110  loss = 3.22952 avg_loss = 2.89692\n",
      "epoch no.6 train no.455120  loss = 2.97047 avg_loss = 2.86130\n",
      "epoch no.6 train no.455130  loss = 2.27453 avg_loss = 2.84166\n",
      "epoch no.6 train no.455140  loss = 3.84159 avg_loss = 2.90026\n",
      "epoch no.6 train no.455150  loss = 2.51038 avg_loss = 2.90353\n",
      "epoch no.6 train no.455160  loss = 3.45877 avg_loss = 2.88748\n",
      "epoch no.6 train no.455170  loss = 2.93256 avg_loss = 2.89618\n",
      "epoch no.6 train no.455180  loss = 2.31689 avg_loss = 2.87374\n",
      "epoch no.6 train no.455190  loss = 2.41226 avg_loss = 2.85306\n",
      "epoch no.6 train no.455200  loss = 2.19610 avg_loss = 2.81186\n",
      "epoch no.6 train no.455210  loss = 3.82551 avg_loss = 2.82420\n",
      "epoch no.6 train no.455220  loss = 2.41982 avg_loss = 2.80104\n",
      "epoch no.6 train no.455230  loss = 3.61480 avg_loss = 2.79596\n",
      "epoch no.6 train no.455240  loss = 3.25325 avg_loss = 2.82063\n",
      "epoch no.6 train no.455250  loss = 3.13666 avg_loss = 2.81973\n",
      "epoch no.6 train no.455260  loss = 2.50162 avg_loss = 2.82420\n",
      "epoch no.6 train no.455270  loss = 1.35430 avg_loss = 2.78026\n",
      "epoch no.6 train no.455280  loss = 3.93957 avg_loss = 2.81431\n",
      "epoch no.6 train no.455290  loss = 3.61773 avg_loss = 2.83547\n",
      "epoch no.6 train no.455300  loss = 2.12602 avg_loss = 2.86793\n",
      "epoch no.6 train no.455310  loss = 3.49768 avg_loss = 2.88722\n",
      "epoch no.6 train no.455320  loss = 3.29678 avg_loss = 2.88273\n",
      "epoch no.6 train no.455330  loss = 1.79172 avg_loss = 2.81336\n",
      "epoch no.6 train no.455340  loss = 3.34836 avg_loss = 2.84173\n",
      "epoch no.6 train no.455350  loss = 3.33092 avg_loss = 2.83670\n",
      "epoch no.6 train no.455360  loss = 2.35956 avg_loss = 2.78963\n",
      "epoch no.6 train no.455370  loss = 2.26807 avg_loss = 2.75559\n",
      "epoch no.6 train no.455380  loss = 5.01627 avg_loss = 2.77233\n",
      "epoch no.6 train no.455390  loss = 3.28050 avg_loss = 2.81781\n",
      "epoch no.6 train no.455400  loss = 3.70195 avg_loss = 2.85347\n",
      "epoch no.6 train no.455410  loss = 3.59967 avg_loss = 2.86292\n",
      "epoch no.6 train no.455420  loss = 2.36320 avg_loss = 2.86587\n",
      "epoch no.6 train no.455430  loss = 3.04338 avg_loss = 2.86809\n",
      "epoch no.6 train no.455440  loss = 2.96989 avg_loss = 2.84537\n",
      "epoch no.6 train no.455450  loss = 1.64341 avg_loss = 2.82134\n",
      "epoch no.6 train no.455460  loss = 1.77018 avg_loss = 2.81384\n",
      "epoch no.6 train no.455470  loss = 2.29417 avg_loss = 2.82107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.455480  loss = 2.47655 avg_loss = 2.84719\n",
      "epoch no.6 train no.455490  loss = 1.70557 avg_loss = 2.88814\n",
      "epoch no.6 train no.455500  loss = 1.75242 avg_loss = 2.88426\n",
      "epoch no.6 train no.455510  loss = 2.58244 avg_loss = 2.88505\n",
      "epoch no.6 train no.455520  loss = 3.80752 avg_loss = 2.87709\n",
      "epoch no.6 train no.455530  loss = 3.43084 avg_loss = 2.88906\n",
      "epoch no.6 train no.455540  loss = 2.73848 avg_loss = 2.88002\n",
      "epoch no.6 train no.455550  loss = 2.56511 avg_loss = 2.87198\n",
      "epoch no.6 train no.455560  loss = 3.77136 avg_loss = 2.93342\n",
      "epoch no.6 train no.455570  loss = 3.15533 avg_loss = 2.92854\n",
      "epoch no.6 train no.455580  loss = 3.13094 avg_loss = 2.93182\n",
      "epoch no.6 train no.455590  loss = 3.69716 avg_loss = 2.91490\n",
      "epoch no.6 train no.455600  loss = 3.44391 avg_loss = 2.90319\n",
      "epoch no.6 train no.455610  loss = 3.77731 avg_loss = 2.90385\n",
      "epoch no.6 train no.455620  loss = 4.17173 avg_loss = 2.91636\n",
      "epoch no.6 train no.455630  loss = 2.97235 avg_loss = 2.91891\n",
      "epoch no.6 train no.455640  loss = 4.17007 avg_loss = 2.94779\n",
      "epoch no.6 train no.455650  loss = 2.86155 avg_loss = 2.92599\n",
      "epoch no.6 train no.455660  loss = 2.08138 avg_loss = 2.90524\n",
      "epoch no.6 train no.455670  loss = 2.52708 avg_loss = 2.89175\n",
      "epoch no.6 train no.455680  loss = 1.56107 avg_loss = 2.89030\n",
      "epoch no.6 train no.455690  loss = 4.14288 avg_loss = 2.88435\n",
      "epoch no.6 train no.455700  loss = 2.89774 avg_loss = 2.88291\n",
      "epoch no.6 train no.455710  loss = 2.53319 avg_loss = 2.87191\n",
      "epoch no.6 train no.455720  loss = 3.03749 avg_loss = 2.87490\n",
      "epoch no.6 train no.455730  loss = 1.87224 avg_loss = 2.86375\n",
      "epoch no.6 train no.455740  loss = 2.37569 avg_loss = 2.86043\n",
      "epoch no.6 train no.455750  loss = 3.85222 avg_loss = 2.86235\n",
      "epoch no.6 train no.455760  loss = 2.56329 avg_loss = 2.84442\n",
      "epoch no.6 train no.455770  loss = 3.14456 avg_loss = 2.80944\n",
      "epoch no.6 train no.455780  loss = 2.63875 avg_loss = 2.82783\n",
      "epoch no.6 train no.455790  loss = 2.24184 avg_loss = 2.81951\n",
      "epoch no.6 train no.455800  loss = 2.44214 avg_loss = 2.81176\n",
      "epoch no.6 train no.455810  loss = 3.37832 avg_loss = 2.86485\n",
      "epoch no.6 train no.455820  loss = 2.43946 avg_loss = 2.84942\n",
      "epoch no.6 train no.455830  loss = 2.64668 avg_loss = 2.84081\n",
      "epoch no.6 train no.455840  loss = 2.76254 avg_loss = 2.88438\n",
      "epoch no.6 train no.455850  loss = 3.60838 avg_loss = 2.90078\n",
      "epoch no.6 train no.455860  loss = 3.22612 avg_loss = 2.89749\n",
      "epoch no.6 train no.455870  loss = 2.10353 avg_loss = 2.92121\n",
      "epoch no.6 train no.455880  loss = 2.20210 avg_loss = 2.93685\n",
      "epoch no.6 train no.455890  loss = 2.25214 avg_loss = 2.91902\n",
      "epoch no.6 train no.455900  loss = 3.14204 avg_loss = 2.90088\n",
      "epoch no.6 train no.455910  loss = 2.31564 avg_loss = 2.93976\n",
      "epoch no.6 train no.455920  loss = 2.35625 avg_loss = 2.92582\n",
      "epoch no.6 train no.455930  loss = 2.37929 avg_loss = 2.88788\n",
      "epoch no.6 train no.455940  loss = 3.05329 avg_loss = 2.89819\n",
      "epoch no.6 train no.455950  loss = 4.45699 avg_loss = 2.93299\n",
      "epoch no.6 train no.455960  loss = 3.47520 avg_loss = 2.93062\n",
      "epoch no.6 train no.455970  loss = 1.21472 avg_loss = 2.88790\n",
      "epoch no.6 train no.455980  loss = 2.50274 avg_loss = 2.89035\n",
      "epoch no.6 train no.455990  loss = 2.61658 avg_loss = 2.88159\n",
      "epoch no.6 train no.456000  loss = 3.36910 avg_loss = 2.88664\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '곡', '▁발라', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.6 train no.456010  loss = 5.37208 avg_loss = 2.91999\n",
      "epoch no.6 train no.456020  loss = 2.55331 avg_loss = 2.93204\n",
      "epoch no.6 train no.456030  loss = 2.29274 avg_loss = 2.92914\n",
      "epoch no.6 train no.456040  loss = 3.76570 avg_loss = 2.95981\n",
      "epoch no.6 train no.456050  loss = 2.66330 avg_loss = 2.96860\n",
      "epoch no.6 train no.456060  loss = 4.33245 avg_loss = 3.00150\n",
      "epoch no.6 train no.456070  loss = 2.46654 avg_loss = 2.97933\n",
      "epoch no.6 train no.456080  loss = 3.00460 avg_loss = 2.93883\n",
      "epoch no.6 train no.456090  loss = 3.25563 avg_loss = 2.91821\n",
      "epoch no.6 train no.456100  loss = 2.59524 avg_loss = 2.90611\n",
      "epoch no.6 train no.456110  loss = 2.53170 avg_loss = 2.94175\n",
      "epoch no.6 train no.456120  loss = 1.48847 avg_loss = 2.91167\n",
      "epoch no.6 train no.456130  loss = 3.67761 avg_loss = 2.90720\n",
      "epoch no.6 train no.456140  loss = 2.56803 avg_loss = 2.89597\n",
      "epoch no.6 train no.456150  loss = 2.89918 avg_loss = 2.89864\n",
      "epoch no.6 train no.456160  loss = 1.96279 avg_loss = 2.86284\n",
      "epoch no.6 train no.456170  loss = 4.33553 avg_loss = 2.86760\n",
      "epoch no.6 train no.456180  loss = 3.43845 avg_loss = 2.86367\n",
      "epoch no.6 train no.456190  loss = 3.41692 avg_loss = 2.86098\n",
      "epoch no.6 train no.456200  loss = 3.50295 avg_loss = 2.85203\n",
      "epoch no.6 train no.456210  loss = 3.17506 avg_loss = 2.85423\n",
      "epoch no.6 train no.456220  loss = 4.24775 avg_loss = 2.84450\n",
      "epoch no.6 train no.456230  loss = 2.51362 avg_loss = 2.87280\n",
      "epoch no.6 train no.456240  loss = 2.57809 avg_loss = 2.85279\n",
      "epoch no.6 train no.456250  loss = 1.89797 avg_loss = 2.86509\n",
      "epoch no.6 train no.456260  loss = 3.53811 avg_loss = 2.86905\n",
      "epoch no.6 train no.456270  loss = 2.61075 avg_loss = 2.87134\n",
      "epoch no.6 train no.456280  loss = 2.15806 avg_loss = 2.88195\n",
      "epoch no.6 train no.456290  loss = 2.57685 avg_loss = 2.87817\n",
      "epoch no.6 train no.456300  loss = 2.77526 avg_loss = 2.86971\n",
      "epoch no.6 train no.456310  loss = 3.21146 avg_loss = 2.88091\n",
      "epoch no.6 train no.456320  loss = 2.49318 avg_loss = 2.87979\n",
      "epoch no.6 train no.456330  loss = 3.41822 avg_loss = 2.86770\n",
      "epoch no.6 train no.456340  loss = 4.67653 avg_loss = 2.92420\n",
      "epoch no.6 train no.456350  loss = 3.07939 avg_loss = 2.92527\n",
      "epoch no.6 train no.456360  loss = 2.59447 avg_loss = 2.91156\n",
      "epoch no.6 train no.456370  loss = 3.51681 avg_loss = 2.95059\n",
      "epoch no.6 train no.456380  loss = 2.00685 avg_loss = 2.96256\n",
      "epoch no.6 train no.456390  loss = 3.64441 avg_loss = 2.93471\n",
      "epoch no.6 train no.456400  loss = 4.37296 avg_loss = 2.96490\n",
      "epoch no.6 train no.456410  loss = 3.09270 avg_loss = 2.97372\n",
      "epoch no.6 train no.456420  loss = 3.69168 avg_loss = 2.96975\n",
      "epoch no.6 train no.456430  loss = 2.54521 avg_loss = 2.96939\n",
      "epoch no.6 train no.456440  loss = 3.08938 avg_loss = 2.96847\n",
      "epoch no.6 train no.456450  loss = 2.93319 avg_loss = 2.97827\n",
      "epoch no.6 train no.456460  loss = 2.44324 avg_loss = 2.97965\n",
      "epoch no.6 train no.456470  loss = 3.44389 avg_loss = 2.97065\n",
      "epoch no.6 train no.456480  loss = 3.24044 avg_loss = 2.95051\n",
      "epoch no.6 train no.456490  loss = 2.09323 avg_loss = 2.94841\n",
      "epoch no.6 train no.456500  loss = 2.59910 avg_loss = 2.93393\n",
      "epoch no.6 train no.456510  loss = 3.67818 avg_loss = 2.93269\n",
      "epoch no.6 train no.456520  loss = 1.87764 avg_loss = 2.93270\n",
      "epoch no.6 train no.456530  loss = 3.57628 avg_loss = 2.95706\n",
      "epoch no.6 train no.456540  loss = 3.85397 avg_loss = 2.94799\n",
      "epoch no.6 train no.456550  loss = 3.04741 avg_loss = 2.95683\n",
      "epoch no.6 train no.456560  loss = 3.61044 avg_loss = 2.94730\n",
      "epoch no.6 train no.456570  loss = 2.12876 avg_loss = 2.91849\n",
      "epoch no.6 train no.456580  loss = 3.13667 avg_loss = 2.94472\n",
      "epoch no.6 train no.456590  loss = 2.96411 avg_loss = 2.91913\n",
      "epoch no.6 train no.456600  loss = 2.27882 avg_loss = 2.94664\n",
      "epoch no.6 train no.456610  loss = 2.42084 avg_loss = 2.93643\n",
      "epoch no.6 train no.456620  loss = 2.90935 avg_loss = 2.90614\n",
      "epoch no.6 train no.456630  loss = 2.55653 avg_loss = 2.94050\n",
      "epoch no.6 train no.456640  loss = 2.05569 avg_loss = 2.92665\n",
      "epoch no.6 train no.456650  loss = 4.92126 avg_loss = 2.90315\n",
      "epoch no.6 train no.456660  loss = 2.59985 avg_loss = 2.89275\n",
      "epoch no.6 train no.456670  loss = 2.41114 avg_loss = 2.87209\n",
      "epoch no.6 train no.456680  loss = 1.63532 avg_loss = 2.83914\n",
      "epoch no.6 train no.456690  loss = 3.19744 avg_loss = 2.87045\n",
      "epoch no.6 train no.456700  loss = 3.15813 avg_loss = 2.91616\n",
      "epoch no.6 train no.456710  loss = 3.46014 avg_loss = 2.94947\n",
      "epoch no.6 train no.456720  loss = 2.52588 avg_loss = 2.91655\n",
      "epoch no.6 train no.456730  loss = 3.05884 avg_loss = 2.90722\n",
      "epoch no.6 train no.456740  loss = 1.99478 avg_loss = 2.91360\n",
      "epoch no.6 train no.456750  loss = 2.57062 avg_loss = 2.90624\n",
      "epoch no.6 train no.456760  loss = 3.17162 avg_loss = 2.90425\n",
      "epoch no.6 train no.456770  loss = 4.39436 avg_loss = 2.90875\n",
      "epoch no.6 train no.456780  loss = 2.48749 avg_loss = 2.92531\n",
      "epoch no.6 train no.456790  loss = 2.56141 avg_loss = 2.94299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.456800  loss = 3.41423 avg_loss = 2.92826\n",
      "epoch no.6 train no.456810  loss = 1.94904 avg_loss = 2.90793\n",
      "epoch no.6 train no.456820  loss = 3.39737 avg_loss = 2.95033\n",
      "epoch no.6 train no.456830  loss = 2.71949 avg_loss = 2.94670\n",
      "epoch no.6 train no.456840  loss = 2.17374 avg_loss = 2.93309\n",
      "epoch no.6 train no.456850  loss = 2.41153 avg_loss = 2.88406\n",
      "epoch no.6 train no.456860  loss = 4.12219 avg_loss = 2.89718\n",
      "epoch no.6 train no.456870  loss = 3.24811 avg_loss = 2.91553\n",
      "epoch no.6 train no.456880  loss = 2.35771 avg_loss = 2.87271\n",
      "epoch no.6 train no.456890  loss = 2.08316 avg_loss = 2.85814\n",
      "epoch no.6 train no.456900  loss = 2.22843 avg_loss = 2.84164\n",
      "epoch no.6 train no.456910  loss = 2.71202 avg_loss = 2.85190\n",
      "epoch no.6 train no.456920  loss = 1.32498 avg_loss = 2.85865\n",
      "epoch no.6 train no.456930  loss = 2.96945 avg_loss = 2.85542\n",
      "epoch no.6 train no.456940  loss = 3.52928 avg_loss = 2.88038\n",
      "epoch no.6 train no.456950  loss = 4.80195 avg_loss = 2.88113\n",
      "epoch no.6 train no.456960  loss = 2.72759 avg_loss = 2.89393\n",
      "epoch no.6 train no.456970  loss = 2.91812 avg_loss = 2.88784\n",
      "epoch no.6 train no.456980  loss = 2.21060 avg_loss = 2.86593\n",
      "epoch no.6 train no.456990  loss = 2.64678 avg_loss = 2.85485\n",
      "epoch no.6 train no.457000  loss = 3.05827 avg_loss = 2.88241\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '2000', '▁댄스', '▁모음', '</s>']\n",
      "추억의 90년대 가요 모음</s>\n",
      "epoch no.6 train no.457010  loss = 2.31554 avg_loss = 2.84676\n",
      "epoch no.6 train no.457020  loss = 3.63380 avg_loss = 2.86158\n",
      "epoch no.6 train no.457030  loss = 2.11906 avg_loss = 2.83996\n",
      "epoch no.6 train no.457040  loss = 2.64090 avg_loss = 2.81097\n",
      "epoch no.6 train no.457050  loss = 4.40767 avg_loss = 2.84756\n",
      "epoch no.6 train no.457060  loss = 2.03922 avg_loss = 2.84032\n",
      "epoch no.6 train no.457070  loss = 2.40482 avg_loss = 2.84369\n",
      "epoch no.6 train no.457080  loss = 2.13834 avg_loss = 2.86164\n",
      "epoch no.6 train no.457090  loss = 3.07977 avg_loss = 2.89387\n",
      "epoch no.6 train no.457100  loss = 3.16322 avg_loss = 2.89404\n",
      "epoch no.6 train no.457110  loss = 2.67609 avg_loss = 2.87882\n",
      "epoch no.6 train no.457120  loss = 3.43039 avg_loss = 2.88054\n",
      "epoch no.6 train no.457130  loss = 2.94885 avg_loss = 2.87427\n",
      "epoch no.6 train no.457140  loss = 3.53471 avg_loss = 2.87899\n",
      "epoch no.6 train no.457150  loss = 1.69952 avg_loss = 2.86022\n",
      "epoch no.6 train no.457160  loss = 2.72585 avg_loss = 2.85462\n",
      "epoch no.6 train no.457170  loss = 3.90592 avg_loss = 2.87514\n",
      "epoch no.6 train no.457180  loss = 2.79501 avg_loss = 2.89410\n",
      "epoch no.6 train no.457190  loss = 2.90038 avg_loss = 2.89516\n",
      "epoch no.6 train no.457200  loss = 2.78143 avg_loss = 2.89374\n",
      "epoch no.6 train no.457210  loss = 2.65207 avg_loss = 2.88037\n",
      "epoch no.6 train no.457220  loss = 2.61980 avg_loss = 2.86583\n",
      "epoch no.6 train no.457230  loss = 3.12910 avg_loss = 2.86726\n",
      "epoch no.6 train no.457240  loss = 3.22867 avg_loss = 2.88714\n",
      "epoch no.6 train no.457250  loss = 2.01850 avg_loss = 2.93119\n",
      "epoch no.6 train no.457260  loss = 2.79127 avg_loss = 2.88648\n",
      "epoch no.6 train no.457270  loss = 2.14003 avg_loss = 2.87895\n",
      "epoch no.6 train no.457280  loss = 3.00297 avg_loss = 2.86308\n",
      "epoch no.6 train no.457290  loss = 3.82531 avg_loss = 2.86101\n",
      "epoch no.6 train no.457300  loss = 2.42216 avg_loss = 2.90683\n",
      "epoch no.6 train no.457310  loss = 3.81924 avg_loss = 2.92181\n",
      "epoch no.6 train no.457320  loss = 2.66894 avg_loss = 2.90197\n",
      "epoch no.6 train no.457330  loss = 2.31078 avg_loss = 2.89647\n",
      "epoch no.6 train no.457340  loss = 4.37604 avg_loss = 2.88304\n",
      "epoch no.6 train no.457350  loss = 3.72740 avg_loss = 2.90149\n",
      "epoch no.6 train no.457360  loss = 3.03343 avg_loss = 2.92810\n",
      "epoch no.6 train no.457370  loss = 3.25857 avg_loss = 2.94144\n",
      "epoch no.6 train no.457380  loss = 2.27985 avg_loss = 2.93633\n",
      "epoch no.6 train no.457390  loss = 3.55773 avg_loss = 2.95044\n",
      "epoch no.6 train no.457400  loss = 2.22284 avg_loss = 2.92003\n",
      "epoch no.6 train no.457410  loss = 4.65529 avg_loss = 2.94100\n",
      "epoch no.6 train no.457420  loss = 3.93049 avg_loss = 2.94962\n",
      "epoch no.6 train no.457430  loss = 2.21121 avg_loss = 2.92500\n",
      "epoch no.6 train no.457440  loss = 3.86236 avg_loss = 2.90467\n",
      "epoch no.6 train no.457450  loss = 1.73542 avg_loss = 2.87755\n",
      "epoch no.6 train no.457460  loss = 1.63986 avg_loss = 2.85899\n",
      "epoch no.6 train no.457470  loss = 4.68190 avg_loss = 2.88458\n",
      "epoch no.6 train no.457480  loss = 2.82961 avg_loss = 2.87658\n",
      "epoch no.6 train no.457490  loss = 3.27350 avg_loss = 2.88103\n",
      "epoch no.6 train no.457500  loss = 2.64445 avg_loss = 2.90566\n",
      "epoch no.6 train no.457510  loss = 2.50217 avg_loss = 2.93447\n",
      "epoch no.6 train no.457520  loss = 2.55158 avg_loss = 2.93347\n",
      "epoch no.6 train no.457530  loss = 2.11716 avg_loss = 2.92451\n",
      "epoch no.6 train no.457540  loss = 2.98477 avg_loss = 2.90800\n",
      "epoch no.6 train no.457550  loss = 2.32256 avg_loss = 2.86843\n",
      "epoch no.6 train no.457560  loss = 3.13384 avg_loss = 2.87036\n",
      "epoch no.6 train no.457570  loss = 1.56153 avg_loss = 2.86414\n",
      "epoch no.6 train no.457580  loss = 2.54595 avg_loss = 2.84348\n",
      "epoch no.6 train no.457590  loss = 2.09309 avg_loss = 2.87370\n",
      "epoch no.6 train no.457600  loss = 2.14508 avg_loss = 2.83890\n",
      "epoch no.6 train no.457610  loss = 2.93429 avg_loss = 2.85852\n",
      "epoch no.6 train no.457620  loss = 2.45966 avg_loss = 2.89674\n",
      "epoch no.6 train no.457630  loss = 3.19302 avg_loss = 2.86627\n",
      "epoch no.6 train no.457640  loss = 2.28781 avg_loss = 2.83789\n",
      "epoch no.6 train no.457650  loss = 2.97037 avg_loss = 2.85872\n",
      "epoch no.6 train no.457660  loss = 2.42600 avg_loss = 2.87239\n",
      "epoch no.6 train no.457670  loss = 3.89166 avg_loss = 2.92862\n",
      "epoch no.6 train no.457680  loss = 2.17152 avg_loss = 2.94038\n",
      "epoch no.6 train no.457690  loss = 2.43873 avg_loss = 2.94554\n",
      "epoch no.6 train no.457700  loss = 2.50086 avg_loss = 2.93581\n",
      "epoch no.6 train no.457710  loss = 2.82949 avg_loss = 2.92214\n",
      "epoch no.6 train no.457720  loss = 4.06487 avg_loss = 2.92617\n",
      "epoch no.6 train no.457730  loss = 2.97712 avg_loss = 2.94304\n",
      "epoch no.6 train no.457740  loss = 2.55182 avg_loss = 2.93104\n",
      "epoch no.6 train no.457750  loss = 1.59391 avg_loss = 2.92973\n",
      "epoch no.6 train no.457760  loss = 2.46521 avg_loss = 2.90005\n",
      "epoch no.6 train no.457770  loss = 3.93070 avg_loss = 2.93378\n",
      "epoch no.6 train no.457780  loss = 1.41302 avg_loss = 2.93671\n",
      "epoch no.6 train no.457790  loss = 2.34121 avg_loss = 2.86527\n",
      "epoch no.6 train no.457800  loss = 2.79386 avg_loss = 2.86882\n",
      "epoch no.6 train no.457810  loss = 3.60794 avg_loss = 2.88662\n",
      "epoch no.6 train no.457820  loss = 2.77673 avg_loss = 2.89513\n",
      "epoch no.6 train no.457830  loss = 2.68708 avg_loss = 2.89043\n",
      "epoch no.6 train no.457840  loss = 4.44577 avg_loss = 2.91902\n",
      "epoch no.6 train no.457850  loss = 2.62749 avg_loss = 2.95569\n",
      "epoch no.6 train no.457860  loss = 3.15731 avg_loss = 2.97312\n",
      "epoch no.6 train no.457870  loss = 2.10419 avg_loss = 2.95572\n",
      "epoch no.6 train no.457880  loss = 4.45654 avg_loss = 2.96705\n",
      "epoch no.6 train no.457890  loss = 3.09558 avg_loss = 2.95911\n",
      "epoch no.6 train no.457900  loss = 2.77679 avg_loss = 2.94541\n",
      "epoch no.6 train no.457910  loss = 2.94031 avg_loss = 2.92792\n",
      "epoch no.6 train no.457920  loss = 3.64546 avg_loss = 2.95617\n",
      "epoch no.6 train no.457930  loss = 2.67607 avg_loss = 2.95540\n",
      "epoch no.6 train no.457940  loss = 3.05694 avg_loss = 2.96792\n",
      "epoch no.6 train no.457950  loss = 1.75831 avg_loss = 2.96604\n",
      "epoch no.6 train no.457960  loss = 2.83531 avg_loss = 2.96161\n",
      "epoch no.6 train no.457970  loss = 3.04995 avg_loss = 2.98428\n",
      "epoch no.6 train no.457980  loss = 2.12663 avg_loss = 2.96434\n",
      "epoch no.6 train no.457990  loss = 3.79498 avg_loss = 2.97427\n",
      "epoch no.6 train no.458000  loss = 2.60810 avg_loss = 2.94667\n",
      "4\n",
      "to_tokens: ['▁가을', '▁그', '곡', '들', '▁모음', '</s>']\n",
      "추억의 명곡들 듣기</s>\n",
      "epoch no.6 train no.458010  loss = 3.28085 avg_loss = 2.91574\n",
      "epoch no.6 train no.458020  loss = 2.08397 avg_loss = 2.87534\n",
      "epoch no.6 train no.458030  loss = 2.00108 avg_loss = 2.90531\n",
      "epoch no.6 train no.458040  loss = 2.37599 avg_loss = 2.89811\n",
      "epoch no.6 train no.458050  loss = 3.48595 avg_loss = 2.92328\n",
      "epoch no.6 train no.458060  loss = 2.35150 avg_loss = 2.88033\n",
      "epoch no.6 train no.458070  loss = 3.29564 avg_loss = 2.87527\n",
      "epoch no.6 train no.458080  loss = 2.85313 avg_loss = 2.87711\n",
      "epoch no.6 train no.458090  loss = 3.32615 avg_loss = 2.85694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.458100  loss = 2.09318 avg_loss = 2.85192\n",
      "epoch no.6 train no.458110  loss = 2.46416 avg_loss = 2.85660\n",
      "epoch no.6 train no.458120  loss = 3.42045 avg_loss = 2.87228\n",
      "epoch no.6 train no.458130  loss = 3.67613 avg_loss = 2.88540\n",
      "epoch no.6 train no.458140  loss = 2.60796 avg_loss = 2.90666\n",
      "epoch no.6 train no.458150  loss = 2.39497 avg_loss = 2.87938\n",
      "epoch no.6 train no.458160  loss = 3.14351 avg_loss = 2.88511\n",
      "epoch no.6 train no.458170  loss = 2.49871 avg_loss = 2.89359\n",
      "epoch no.6 train no.458180  loss = 3.71525 avg_loss = 2.90709\n",
      "epoch no.6 train no.458190  loss = 3.17504 avg_loss = 2.90980\n",
      "epoch no.6 train no.458200  loss = 2.86173 avg_loss = 2.90262\n",
      "epoch no.6 train no.458210  loss = 2.63531 avg_loss = 2.93823\n",
      "epoch no.6 train no.458220  loss = 2.54230 avg_loss = 2.92718\n",
      "epoch no.6 train no.458230  loss = 2.35051 avg_loss = 2.95114\n",
      "epoch no.6 train no.458240  loss = 4.14434 avg_loss = 2.94500\n",
      "epoch no.6 train no.458250  loss = 2.93377 avg_loss = 2.94511\n",
      "epoch no.6 train no.458260  loss = 3.63358 avg_loss = 2.94099\n",
      "epoch no.6 train no.458270  loss = 2.30896 avg_loss = 2.93781\n",
      "epoch no.6 train no.458280  loss = 2.13495 avg_loss = 2.93301\n",
      "epoch no.6 train no.458290  loss = 4.62100 avg_loss = 2.89359\n",
      "epoch no.6 train no.458300  loss = 2.90840 avg_loss = 2.89698\n",
      "epoch no.6 train no.458310  loss = 4.11082 avg_loss = 2.87886\n",
      "epoch no.6 train no.458320  loss = 3.08843 avg_loss = 2.84132\n",
      "epoch no.6 train no.458330  loss = 2.93387 avg_loss = 2.85093\n",
      "epoch no.6 train no.458340  loss = 2.55875 avg_loss = 2.83769\n",
      "epoch no.6 train no.458350  loss = 2.69509 avg_loss = 2.82907\n",
      "epoch no.6 train no.458360  loss = 2.81435 avg_loss = 2.84828\n",
      "epoch no.6 train no.458370  loss = 3.59857 avg_loss = 2.88527\n",
      "epoch no.6 train no.458380  loss = 2.44799 avg_loss = 2.89230\n",
      "epoch no.6 train no.458390  loss = 3.12870 avg_loss = 2.90377\n",
      "epoch no.6 train no.458400  loss = 2.31678 avg_loss = 2.90922\n",
      "epoch no.6 train no.458410  loss = 2.86425 avg_loss = 2.96230\n",
      "epoch no.6 train no.458420  loss = 2.67523 avg_loss = 3.00042\n",
      "epoch no.6 train no.458430  loss = 3.23381 avg_loss = 2.96505\n",
      "epoch no.6 train no.458440  loss = 3.18952 avg_loss = 2.95827\n",
      "epoch no.6 train no.458450  loss = 3.52985 avg_loss = 2.96637\n",
      "epoch no.6 train no.458460  loss = 2.46215 avg_loss = 2.96513\n",
      "epoch no.6 train no.458470  loss = 2.48295 avg_loss = 2.94797\n",
      "epoch no.6 train no.458480  loss = 2.11643 avg_loss = 2.93958\n",
      "epoch no.6 train no.458490  loss = 2.42793 avg_loss = 2.90049\n",
      "epoch no.6 train no.458500  loss = 3.68936 avg_loss = 2.92056\n",
      "epoch no.6 train no.458510  loss = 3.09844 avg_loss = 2.97793\n",
      "epoch no.6 train no.458520  loss = 2.44521 avg_loss = 2.96499\n",
      "epoch no.6 train no.458530  loss = 4.31467 avg_loss = 2.95839\n",
      "epoch no.6 train no.458540  loss = 3.44655 avg_loss = 2.98322\n",
      "epoch no.6 train no.458550  loss = 2.19722 avg_loss = 2.91967\n",
      "epoch no.6 train no.458560  loss = 1.72515 avg_loss = 2.90466\n",
      "epoch no.6 train no.458570  loss = 3.84507 avg_loss = 2.92293\n",
      "epoch no.6 train no.458580  loss = 2.11997 avg_loss = 2.89643\n",
      "epoch no.6 train no.458590  loss = 2.69944 avg_loss = 2.91543\n",
      "epoch no.6 train no.458600  loss = 2.94909 avg_loss = 2.93463\n",
      "epoch no.6 train no.458610  loss = 1.77801 avg_loss = 2.90113\n",
      "epoch no.6 train no.458620  loss = 2.57479 avg_loss = 2.93267\n",
      "epoch no.6 train no.458630  loss = 2.70744 avg_loss = 2.89991\n",
      "epoch no.6 train no.458640  loss = 2.55045 avg_loss = 2.89594\n",
      "epoch no.6 train no.458650  loss = 2.73092 avg_loss = 2.89822\n",
      "epoch no.6 train no.458660  loss = 2.42169 avg_loss = 2.87768\n",
      "epoch no.6 train no.458670  loss = 2.83886 avg_loss = 2.88528\n",
      "epoch no.6 train no.458680  loss = 3.17277 avg_loss = 2.92962\n",
      "epoch no.6 train no.458690  loss = 2.14145 avg_loss = 2.90381\n",
      "epoch no.6 train no.458700  loss = 3.65342 avg_loss = 2.92749\n",
      "epoch no.6 train no.458710  loss = 1.98627 avg_loss = 2.92102\n",
      "epoch no.6 train no.458720  loss = 2.62862 avg_loss = 2.89663\n",
      "epoch no.6 train no.458730  loss = 2.16912 avg_loss = 2.89865\n",
      "epoch no.6 train no.458740  loss = 2.59069 avg_loss = 2.87262\n",
      "epoch no.6 train no.458750  loss = 2.21897 avg_loss = 2.86798\n",
      "epoch no.6 train no.458760  loss = 3.71463 avg_loss = 2.87501\n",
      "epoch no.6 train no.458770  loss = 2.49029 avg_loss = 2.84731\n",
      "epoch no.6 train no.458780  loss = 3.28613 avg_loss = 2.88255\n",
      "epoch no.6 train no.458790  loss = 2.76592 avg_loss = 2.84217\n",
      "epoch no.6 train no.458800  loss = 3.10400 avg_loss = 2.81381\n",
      "epoch no.6 train no.458810  loss = 3.00857 avg_loss = 2.80649\n",
      "epoch no.6 train no.458820  loss = 2.93363 avg_loss = 2.78541\n",
      "epoch no.6 train no.458830  loss = 2.61882 avg_loss = 2.77972\n",
      "epoch no.6 train no.458840  loss = 1.86666 avg_loss = 2.75551\n",
      "epoch no.6 train no.458850  loss = 2.60469 avg_loss = 2.74057\n",
      "epoch no.6 train no.458860  loss = 2.41678 avg_loss = 2.72770\n",
      "epoch no.6 train no.458870  loss = 4.58172 avg_loss = 2.73460\n",
      "epoch no.6 train no.458880  loss = 4.73202 avg_loss = 2.73700\n",
      "epoch no.6 train no.458890  loss = 2.53206 avg_loss = 2.74376\n",
      "epoch no.6 train no.458900  loss = 2.32233 avg_loss = 2.74027\n",
      "epoch no.6 train no.458910  loss = 3.63878 avg_loss = 2.77074\n",
      "epoch no.6 train no.458920  loss = 3.01841 avg_loss = 2.77721\n",
      "epoch no.6 train no.458930  loss = 2.58997 avg_loss = 2.74578\n",
      "epoch no.6 train no.458940  loss = 2.73453 avg_loss = 2.76176\n",
      "epoch no.6 train no.458950  loss = 2.72426 avg_loss = 2.75987\n",
      "epoch no.6 train no.458960  loss = 2.05656 avg_loss = 2.75227\n",
      "epoch no.6 train no.458970  loss = 3.81542 avg_loss = 2.76094\n",
      "epoch no.6 train no.458980  loss = 3.29284 avg_loss = 2.76839\n",
      "epoch no.6 train no.458990  loss = 2.59114 avg_loss = 2.75132\n",
      "epoch no.6 train no.459000  loss = 1.06629 avg_loss = 2.73506\n",
      "4\n",
      "to_tokens: ['▁비', '▁팝', '송', '들', '드', '</s>']\n",
      "추억의 팝송 발라드</s>\n",
      "epoch no.6 train no.459010  loss = 2.34081 avg_loss = 2.74976\n",
      "epoch no.6 train no.459020  loss = 2.08698 avg_loss = 2.75221\n",
      "epoch no.6 train no.459030  loss = 3.82766 avg_loss = 2.76621\n",
      "epoch no.6 train no.459040  loss = 3.24007 avg_loss = 2.77238\n",
      "epoch no.6 train no.459050  loss = 3.92190 avg_loss = 2.80148\n",
      "epoch no.6 train no.459060  loss = 3.13672 avg_loss = 2.80856\n",
      "epoch no.6 train no.459070  loss = 3.66210 avg_loss = 2.82039\n",
      "epoch no.6 train no.459080  loss = 2.68387 avg_loss = 2.82008\n",
      "epoch no.6 train no.459090  loss = 3.02341 avg_loss = 2.81799\n",
      "epoch no.6 train no.459100  loss = 4.10472 avg_loss = 2.83321\n",
      "epoch no.6 train no.459110  loss = 2.11681 avg_loss = 2.83700\n",
      "epoch no.6 train no.459120  loss = 3.31126 avg_loss = 2.85752\n",
      "epoch no.6 train no.459130  loss = 4.48865 avg_loss = 2.88856\n",
      "epoch no.6 train no.459140  loss = 1.50989 avg_loss = 2.86239\n",
      "epoch no.6 train no.459150  loss = 2.48900 avg_loss = 2.87115\n",
      "epoch no.6 train no.459160  loss = 3.48299 avg_loss = 2.88565\n",
      "epoch no.6 train no.459170  loss = 2.81079 avg_loss = 2.90114\n",
      "epoch no.6 train no.459180  loss = 5.49538 avg_loss = 2.91873\n",
      "epoch no.6 train no.459190  loss = 3.51812 avg_loss = 2.91808\n",
      "epoch no.6 train no.459200  loss = 3.50590 avg_loss = 2.92931\n",
      "epoch no.6 train no.459210  loss = 2.44671 avg_loss = 2.90200\n",
      "epoch no.6 train no.459220  loss = 2.53697 avg_loss = 2.87370\n",
      "epoch no.6 train no.459230  loss = 2.05596 avg_loss = 2.89581\n",
      "epoch no.6 train no.459240  loss = 2.79756 avg_loss = 2.89919\n",
      "epoch no.6 train no.459250  loss = 3.21616 avg_loss = 2.87085\n",
      "epoch no.6 train no.459260  loss = 2.19314 avg_loss = 2.82665\n",
      "epoch no.6 train no.459270  loss = 3.77663 avg_loss = 2.82633\n",
      "epoch no.6 train no.459280  loss = 1.94142 avg_loss = 2.85488\n",
      "epoch no.6 train no.459290  loss = 4.81671 avg_loss = 2.87159\n",
      "epoch no.6 train no.459300  loss = 2.90916 avg_loss = 2.91967\n",
      "epoch no.6 train no.459310  loss = 2.49358 avg_loss = 2.92676\n",
      "epoch no.6 train no.459320  loss = 2.46054 avg_loss = 2.92527\n",
      "epoch no.6 train no.459330  loss = 3.08293 avg_loss = 2.96750\n",
      "epoch no.6 train no.459340  loss = 3.05364 avg_loss = 2.95158\n",
      "epoch no.6 train no.459350  loss = 1.81417 avg_loss = 2.95160\n",
      "epoch no.6 train no.459360  loss = 3.16719 avg_loss = 2.93027\n",
      "epoch no.6 train no.459370  loss = 3.73845 avg_loss = 2.91885\n",
      "epoch no.6 train no.459380  loss = 2.47569 avg_loss = 2.88288\n",
      "epoch no.6 train no.459390  loss = 3.38289 avg_loss = 2.84470\n",
      "epoch no.6 train no.459400  loss = 1.78564 avg_loss = 2.84895\n",
      "epoch no.6 train no.459410  loss = 3.37699 avg_loss = 2.82475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.459420  loss = 3.05465 avg_loss = 2.78601\n",
      "epoch no.6 train no.459430  loss = 2.22136 avg_loss = 2.76539\n",
      "epoch no.6 train no.459440  loss = 2.08553 avg_loss = 2.74703\n",
      "epoch no.6 train no.459450  loss = 1.64126 avg_loss = 2.75906\n",
      "epoch no.6 train no.459460  loss = 2.33897 avg_loss = 2.72493\n",
      "epoch no.6 train no.459470  loss = 2.09829 avg_loss = 2.72564\n",
      "epoch no.6 train no.459480  loss = 2.55277 avg_loss = 2.75214\n",
      "epoch no.6 train no.459490  loss = 1.95408 avg_loss = 2.74700\n",
      "epoch no.6 train no.459500  loss = 3.22791 avg_loss = 2.76047\n",
      "epoch no.6 train no.459510  loss = 3.57856 avg_loss = 2.73706\n",
      "epoch no.6 train no.459520  loss = 3.03865 avg_loss = 2.73351\n",
      "epoch no.6 train no.459530  loss = 2.08519 avg_loss = 2.72935\n",
      "epoch no.6 train no.459540  loss = 3.40324 avg_loss = 2.73937\n",
      "epoch no.6 train no.459550  loss = 2.58885 avg_loss = 2.73107\n",
      "epoch no.6 train no.459560  loss = 3.54583 avg_loss = 2.76169\n",
      "epoch no.6 train no.459570  loss = 1.97541 avg_loss = 2.77127\n",
      "epoch no.6 train no.459580  loss = 2.09430 avg_loss = 2.76646\n",
      "epoch no.6 train no.459590  loss = 2.48093 avg_loss = 2.74944\n",
      "epoch no.6 train no.459600  loss = 1.40136 avg_loss = 2.71421\n",
      "epoch no.6 train no.459610  loss = 2.85179 avg_loss = 2.71959\n",
      "epoch no.6 train no.459620  loss = 1.79284 avg_loss = 2.72197\n",
      "epoch no.6 train no.459630  loss = 2.49195 avg_loss = 2.73996\n",
      "epoch no.6 train no.459640  loss = 3.60988 avg_loss = 2.75757\n",
      "epoch no.6 train no.459650  loss = 3.08720 avg_loss = 2.71405\n",
      "epoch no.6 train no.459660  loss = 2.33136 avg_loss = 2.73778\n",
      "epoch no.6 train no.459670  loss = 1.88204 avg_loss = 2.73534\n",
      "epoch no.6 train no.459680  loss = 3.50894 avg_loss = 2.76537\n",
      "epoch no.6 train no.459690  loss = 3.02609 avg_loss = 2.77141\n",
      "epoch no.6 train no.459700  loss = 2.46236 avg_loss = 2.74706\n",
      "epoch no.6 train no.459710  loss = 3.31756 avg_loss = 2.76796\n",
      "epoch no.6 train no.459720  loss = 2.11052 avg_loss = 2.77279\n",
      "epoch no.6 train no.459730  loss = 2.13800 avg_loss = 2.76866\n",
      "epoch no.6 train no.459740  loss = 4.08756 avg_loss = 2.78897\n",
      "epoch no.6 train no.459750  loss = 3.23130 avg_loss = 2.79421\n",
      "epoch no.6 train no.459760  loss = 2.82806 avg_loss = 2.79602\n",
      "epoch no.6 train no.459770  loss = 3.37798 avg_loss = 2.81380\n",
      "epoch no.6 train no.459780  loss = 2.37093 avg_loss = 2.79306\n",
      "epoch no.6 train no.459790  loss = 3.23966 avg_loss = 2.79122\n",
      "epoch no.6 train no.459800  loss = 3.29519 avg_loss = 2.81247\n",
      "epoch no.6 train no.459810  loss = 4.29541 avg_loss = 2.84755\n",
      "epoch no.6 train no.459820  loss = 2.39486 avg_loss = 2.85054\n",
      "epoch no.6 train no.459830  loss = 4.28984 avg_loss = 2.90423\n",
      "epoch no.6 train no.459840  loss = 2.69424 avg_loss = 2.86972\n",
      "epoch no.6 train no.459850  loss = 2.63119 avg_loss = 2.85954\n",
      "epoch no.6 train no.459860  loss = 2.55542 avg_loss = 2.82881\n",
      "epoch no.6 train no.459870  loss = 2.91857 avg_loss = 2.80094\n",
      "epoch no.6 train no.459880  loss = 2.11570 avg_loss = 2.83272\n",
      "epoch no.6 train no.459890  loss = 2.59437 avg_loss = 2.84185\n",
      "epoch no.6 train no.459900  loss = 2.35497 avg_loss = 2.86663\n",
      "epoch no.6 train no.459910  loss = 2.64453 avg_loss = 2.85754\n",
      "epoch no.6 train no.459920  loss = 2.95725 avg_loss = 2.85580\n",
      "epoch no.6 train no.459930  loss = 2.30390 avg_loss = 2.83212\n",
      "epoch no.6 train no.459940  loss = 2.95403 avg_loss = 2.85989\n",
      "epoch no.6 train no.459950  loss = 3.80404 avg_loss = 2.83840\n",
      "epoch no.6 train no.459960  loss = 2.48122 avg_loss = 2.86669\n",
      "epoch no.6 train no.459970  loss = 1.86819 avg_loss = 2.81199\n",
      "epoch no.6 train no.459980  loss = 2.31539 avg_loss = 2.81359\n",
      "epoch no.6 train no.459990  loss = 2.16201 avg_loss = 2.76411\n",
      "epoch no.6 train no.460000  loss = 2.34822 avg_loss = 2.77304\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', 'nb', '▁명', '곡', '</s>']\n",
      "추억의 rnb 명곡</s>\n",
      "epoch no.6 train no.460010  loss = 3.07161 avg_loss = 2.77568\n",
      "epoch no.6 train no.460020  loss = 2.39053 avg_loss = 2.76621\n",
      "epoch no.6 train no.460030  loss = 2.40767 avg_loss = 2.74860\n",
      "epoch no.6 train no.460040  loss = 2.80528 avg_loss = 2.74923\n",
      "epoch no.6 train no.460050  loss = 2.47732 avg_loss = 2.73072\n",
      "epoch no.6 train no.460060  loss = 2.03702 avg_loss = 2.75342\n",
      "epoch no.6 train no.460070  loss = 1.12497 avg_loss = 2.75745\n",
      "epoch no.6 train no.460080  loss = 2.80285 avg_loss = 2.78883\n",
      "epoch no.6 train no.460090  loss = 2.62859 avg_loss = 2.81286\n",
      "epoch no.6 train no.460100  loss = 3.76724 avg_loss = 2.82974\n",
      "epoch no.6 train no.460110  loss = 2.37622 avg_loss = 2.80871\n",
      "epoch no.6 train no.460120  loss = 2.61077 avg_loss = 2.78397\n",
      "epoch no.6 train no.460130  loss = 4.17227 avg_loss = 2.82481\n",
      "epoch no.6 train no.460140  loss = 3.50078 avg_loss = 2.82922\n",
      "epoch no.6 train no.460150  loss = 4.41122 avg_loss = 2.85969\n",
      "epoch no.6 train no.460160  loss = 2.31394 avg_loss = 2.85252\n",
      "epoch no.6 train no.460170  loss = 2.70873 avg_loss = 2.82155\n",
      "epoch no.6 train no.460180  loss = 1.65671 avg_loss = 2.80932\n",
      "epoch no.6 train no.460190  loss = 4.94375 avg_loss = 2.85078\n",
      "epoch no.6 train no.460200  loss = 2.84948 avg_loss = 2.88197\n",
      "epoch no.6 train no.460210  loss = 4.65690 avg_loss = 2.88866\n",
      "epoch no.6 train no.460220  loss = 2.86509 avg_loss = 2.90473\n",
      "epoch no.6 train no.460230  loss = 2.84245 avg_loss = 2.86847\n",
      "epoch no.6 train no.460240  loss = 3.02661 avg_loss = 2.89644\n",
      "epoch no.6 train no.460250  loss = 2.28932 avg_loss = 2.89847\n",
      "epoch no.6 train no.460260  loss = 3.52222 avg_loss = 2.88746\n",
      "epoch no.6 train no.460270  loss = 2.01931 avg_loss = 2.90843\n",
      "epoch no.6 train no.460280  loss = 2.20890 avg_loss = 2.92726\n",
      "epoch no.6 train no.460290  loss = 3.72664 avg_loss = 2.98613\n",
      "epoch no.6 train no.460300  loss = 3.52376 avg_loss = 3.00274\n",
      "epoch no.6 train no.460310  loss = 2.77716 avg_loss = 3.01342\n",
      "epoch no.6 train no.460320  loss = 2.97711 avg_loss = 2.97807\n",
      "epoch no.6 train no.460330  loss = 2.64252 avg_loss = 2.94382\n",
      "epoch no.6 train no.460340  loss = 2.96966 avg_loss = 2.93618\n",
      "epoch no.6 train no.460350  loss = 2.20312 avg_loss = 2.88634\n",
      "epoch no.6 train no.460360  loss = 2.27921 avg_loss = 2.89129\n",
      "epoch no.6 train no.460370  loss = 3.96615 avg_loss = 2.87095\n",
      "epoch no.6 train no.460380  loss = 2.55248 avg_loss = 2.88492\n",
      "epoch no.6 train no.460390  loss = 3.06908 avg_loss = 2.85660\n",
      "epoch no.6 train no.460400  loss = 1.40000 avg_loss = 2.82277\n",
      "epoch no.6 train no.460410  loss = 2.92083 avg_loss = 2.84659\n",
      "epoch no.6 train no.460420  loss = 1.66384 avg_loss = 2.81636\n",
      "epoch no.6 train no.460430  loss = 2.52668 avg_loss = 2.82157\n",
      "epoch no.6 train no.460440  loss = 2.82722 avg_loss = 2.86886\n",
      "epoch no.6 train no.460450  loss = 2.53423 avg_loss = 2.89482\n",
      "epoch no.6 train no.460460  loss = 3.06797 avg_loss = 2.92645\n",
      "epoch no.6 train no.460470  loss = 2.46671 avg_loss = 2.92198\n",
      "epoch no.6 train no.460480  loss = 2.25775 avg_loss = 2.90277\n",
      "epoch no.6 train no.460490  loss = 2.47057 avg_loss = 2.91885\n",
      "epoch no.6 train no.460500  loss = 2.49785 avg_loss = 2.90257\n",
      "epoch no.6 train no.460510  loss = 2.34680 avg_loss = 2.87818\n",
      "epoch no.6 train no.460520  loss = 2.42649 avg_loss = 2.90501\n",
      "epoch no.6 train no.460530  loss = 3.45012 avg_loss = 2.91500\n",
      "epoch no.6 train no.460540  loss = 2.29416 avg_loss = 2.93768\n",
      "epoch no.6 train no.460550  loss = 2.97527 avg_loss = 2.89953\n",
      "epoch no.6 train no.460560  loss = 2.37788 avg_loss = 2.90173\n",
      "epoch no.6 train no.460570  loss = 3.47928 avg_loss = 2.84981\n",
      "epoch no.6 train no.460580  loss = 2.29792 avg_loss = 2.85671\n",
      "epoch no.6 train no.460590  loss = 3.41664 avg_loss = 2.87626\n",
      "epoch no.6 train no.460600  loss = 3.23213 avg_loss = 2.88847\n",
      "epoch no.6 train no.460610  loss = 3.55830 avg_loss = 2.88358\n",
      "epoch no.6 train no.460620  loss = 2.46891 avg_loss = 2.86975\n",
      "epoch no.6 train no.460630  loss = 3.08513 avg_loss = 2.85977\n",
      "epoch no.6 train no.460640  loss = 2.80245 avg_loss = 2.87895\n",
      "epoch no.6 train no.460650  loss = 3.95293 avg_loss = 2.84766\n",
      "epoch no.6 train no.460660  loss = 2.69263 avg_loss = 2.85755\n",
      "epoch no.6 train no.460670  loss = 3.66282 avg_loss = 2.86226\n",
      "epoch no.6 train no.460680  loss = 3.12866 avg_loss = 2.85362\n",
      "epoch no.6 train no.460690  loss = 2.77974 avg_loss = 2.85647\n",
      "epoch no.6 train no.460700  loss = 1.62407 avg_loss = 2.86700\n",
      "epoch no.6 train no.460710  loss = 2.20338 avg_loss = 2.88548\n",
      "epoch no.6 train no.460720  loss = 2.64000 avg_loss = 2.85944\n",
      "epoch no.6 train no.460730  loss = 2.81576 avg_loss = 2.82669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.460740  loss = 2.31953 avg_loss = 2.82186\n",
      "epoch no.6 train no.460750  loss = 2.55321 avg_loss = 2.80399\n",
      "epoch no.6 train no.460760  loss = 1.77776 avg_loss = 2.80058\n",
      "epoch no.6 train no.460770  loss = 2.49999 avg_loss = 2.79762\n",
      "epoch no.6 train no.460780  loss = 2.73370 avg_loss = 2.79050\n",
      "epoch no.6 train no.460790  loss = 2.64577 avg_loss = 2.76605\n",
      "epoch no.6 train no.460800  loss = 2.73018 avg_loss = 2.79768\n",
      "epoch no.6 train no.460810  loss = 2.38072 avg_loss = 2.80705\n",
      "epoch no.6 train no.460820  loss = 3.08941 avg_loss = 2.77376\n",
      "epoch no.6 train no.460830  loss = 3.00894 avg_loss = 2.80033\n",
      "epoch no.6 train no.460840  loss = 2.01718 avg_loss = 2.80003\n",
      "epoch no.6 train no.460850  loss = 2.42060 avg_loss = 2.82061\n",
      "epoch no.6 train no.460860  loss = 2.57673 avg_loss = 2.82018\n",
      "epoch no.6 train no.460870  loss = 4.29070 avg_loss = 2.82629\n",
      "epoch no.6 train no.460880  loss = 2.32852 avg_loss = 2.81168\n",
      "epoch no.6 train no.460890  loss = 2.54096 avg_loss = 2.80825\n",
      "epoch no.6 train no.460900  loss = 2.93545 avg_loss = 2.81901\n",
      "epoch no.6 train no.460910  loss = 2.46614 avg_loss = 2.79856\n",
      "epoch no.6 train no.460920  loss = 3.26348 avg_loss = 2.85031\n",
      "epoch no.6 train no.460930  loss = 2.84634 avg_loss = 2.85770\n",
      "epoch no.6 train no.460940  loss = 1.98694 avg_loss = 2.86961\n",
      "epoch no.6 train no.460950  loss = 3.33530 avg_loss = 2.84734\n",
      "epoch no.6 train no.460960  loss = 1.91824 avg_loss = 2.83793\n",
      "epoch no.6 train no.460970  loss = 2.35716 avg_loss = 2.84340\n",
      "epoch no.6 train no.460980  loss = 2.64434 avg_loss = 2.79084\n",
      "epoch no.6 train no.460990  loss = 1.82335 avg_loss = 2.78684\n",
      "epoch no.6 train no.461000  loss = 3.06182 avg_loss = 2.78744\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁노래', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.6 train no.461010  loss = 5.10424 avg_loss = 2.81738\n",
      "epoch no.6 train no.461020  loss = 3.81368 avg_loss = 2.81649\n",
      "epoch no.6 train no.461030  loss = 2.86512 avg_loss = 2.84838\n",
      "epoch no.6 train no.461040  loss = 2.60179 avg_loss = 2.86311\n",
      "epoch no.6 train no.461050  loss = 1.60709 avg_loss = 2.84965\n",
      "epoch no.6 train no.461060  loss = 2.32693 avg_loss = 2.85271\n",
      "epoch no.6 train no.461070  loss = 2.90836 avg_loss = 2.84815\n",
      "epoch no.6 train no.461080  loss = 2.29780 avg_loss = 2.83575\n",
      "epoch no.6 train no.461090  loss = 1.75883 avg_loss = 2.82600\n",
      "epoch no.6 train no.461100  loss = 2.72601 avg_loss = 2.84825\n",
      "epoch no.6 train no.461110  loss = 2.10486 avg_loss = 2.83605\n",
      "epoch no.6 train no.461120  loss = 3.33801 avg_loss = 2.81374\n",
      "epoch no.6 train no.461130  loss = 2.42557 avg_loss = 2.81632\n",
      "epoch no.6 train no.461140  loss = 3.06222 avg_loss = 2.85211\n",
      "epoch no.6 train no.461150  loss = 2.18478 avg_loss = 2.86769\n",
      "epoch no.6 train no.461160  loss = 1.90802 avg_loss = 2.88073\n",
      "epoch no.6 train no.461170  loss = 1.64149 avg_loss = 2.87502\n",
      "epoch no.6 train no.461180  loss = 1.87730 avg_loss = 2.87939\n",
      "epoch no.6 train no.461190  loss = 2.29345 avg_loss = 2.88769\n",
      "epoch no.6 train no.461200  loss = 1.89603 avg_loss = 2.90901\n",
      "epoch no.6 train no.461210  loss = 3.44281 avg_loss = 2.91072\n",
      "epoch no.6 train no.461220  loss = 2.18195 avg_loss = 2.86463\n",
      "epoch no.6 train no.461230  loss = 3.38063 avg_loss = 2.84580\n",
      "epoch no.6 train no.461240  loss = 2.89738 avg_loss = 2.84808\n",
      "epoch no.6 train no.461250  loss = 2.76817 avg_loss = 2.84970\n",
      "epoch no.6 train no.461260  loss = 3.81735 avg_loss = 2.82100\n",
      "epoch no.6 train no.461270  loss = 3.37282 avg_loss = 2.83823\n",
      "epoch no.6 train no.461280  loss = 3.78536 avg_loss = 2.89665\n",
      "epoch no.6 train no.461290  loss = 2.93916 avg_loss = 2.91122\n",
      "epoch no.6 train no.461300  loss = 3.74498 avg_loss = 2.94324\n",
      "epoch no.6 train no.461310  loss = 1.94562 avg_loss = 2.93542\n",
      "epoch no.6 train no.461320  loss = 3.06683 avg_loss = 2.91455\n",
      "epoch no.6 train no.461330  loss = 2.17028 avg_loss = 2.92746\n",
      "epoch no.6 train no.461340  loss = 2.66411 avg_loss = 2.90221\n",
      "epoch no.6 train no.461350  loss = 2.44463 avg_loss = 2.85665\n",
      "epoch no.6 train no.461360  loss = 2.92896 avg_loss = 2.84763\n",
      "epoch no.6 train no.461370  loss = 2.66502 avg_loss = 2.84143\n",
      "epoch no.6 train no.461380  loss = 2.05656 avg_loss = 2.80196\n",
      "epoch no.6 train no.461390  loss = 2.76440 avg_loss = 2.82349\n",
      "epoch no.6 train no.461400  loss = 3.15341 avg_loss = 2.86984\n",
      "epoch no.6 train no.461410  loss = 3.57386 avg_loss = 2.86371\n",
      "epoch no.6 train no.461420  loss = 2.06122 avg_loss = 2.85186\n",
      "epoch no.6 train no.461430  loss = 2.97514 avg_loss = 2.87769\n",
      "epoch no.6 train no.461440  loss = 3.05381 avg_loss = 2.89525\n",
      "epoch no.6 train no.461450  loss = 2.68909 avg_loss = 2.89287\n",
      "epoch no.6 train no.461460  loss = 1.54151 avg_loss = 2.90260\n",
      "epoch no.6 train no.461470  loss = 2.01384 avg_loss = 2.85941\n",
      "epoch no.6 train no.461480  loss = 3.20052 avg_loss = 2.84730\n",
      "epoch no.6 train no.461490  loss = 2.16780 avg_loss = 2.83927\n",
      "epoch no.6 train no.461500  loss = 2.28769 avg_loss = 2.87038\n",
      "epoch no.6 train no.461510  loss = 3.80882 avg_loss = 2.90016\n",
      "epoch no.6 train no.461520  loss = 2.21420 avg_loss = 2.85752\n",
      "epoch no.6 train no.461530  loss = 1.97956 avg_loss = 2.90980\n",
      "epoch no.6 train no.461540  loss = 3.06675 avg_loss = 2.90800\n",
      "epoch no.6 train no.461550  loss = 2.44188 avg_loss = 2.91330\n",
      "epoch no.6 train no.461560  loss = 1.36515 avg_loss = 2.92700\n",
      "epoch no.6 train no.461570  loss = 1.54563 avg_loss = 2.88475\n",
      "epoch no.6 train no.461580  loss = 3.04211 avg_loss = 2.87097\n",
      "epoch no.6 train no.461590  loss = 3.04542 avg_loss = 2.87134\n",
      "epoch no.6 train no.461600  loss = 2.83048 avg_loss = 2.90184\n",
      "epoch no.6 train no.461610  loss = 2.78725 avg_loss = 2.87060\n",
      "epoch no.6 train no.461620  loss = 2.22690 avg_loss = 2.88740\n",
      "epoch no.6 train no.461630  loss = 2.16207 avg_loss = 2.88615\n",
      "epoch no.6 train no.461640  loss = 2.84926 avg_loss = 2.88584\n",
      "epoch no.6 train no.461650  loss = 3.02691 avg_loss = 2.86657\n",
      "epoch no.6 train no.461660  loss = 2.01792 avg_loss = 2.84781\n",
      "epoch no.6 train no.461670  loss = 2.35794 avg_loss = 2.83788\n",
      "epoch no.6 train no.461680  loss = 3.72297 avg_loss = 2.86332\n",
      "epoch no.6 train no.461690  loss = 2.74505 avg_loss = 2.86280\n",
      "epoch no.6 train no.461700  loss = 2.87297 avg_loss = 2.87173\n",
      "epoch no.6 train no.461710  loss = 3.71554 avg_loss = 2.87258\n",
      "epoch no.6 train no.461720  loss = 2.23982 avg_loss = 2.88174\n",
      "epoch no.6 train no.461730  loss = 3.74216 avg_loss = 2.90961\n",
      "epoch no.6 train no.461740  loss = 3.30224 avg_loss = 2.91211\n",
      "epoch no.6 train no.461750  loss = 2.55276 avg_loss = 2.88628\n",
      "epoch no.6 train no.461760  loss = 1.55030 avg_loss = 2.86927\n",
      "epoch no.6 train no.461770  loss = 3.26616 avg_loss = 2.86792\n",
      "epoch no.6 train no.461780  loss = 3.32567 avg_loss = 2.84906\n",
      "epoch no.6 train no.461790  loss = 3.35170 avg_loss = 2.87413\n",
      "epoch no.6 train no.461800  loss = 3.79014 avg_loss = 2.90217\n",
      "epoch no.6 train no.461810  loss = 2.75059 avg_loss = 2.88397\n",
      "epoch no.6 train no.461820  loss = 2.59592 avg_loss = 2.90059\n",
      "epoch no.6 train no.461830  loss = 4.20578 avg_loss = 2.89001\n",
      "epoch no.6 train no.461840  loss = 3.01968 avg_loss = 2.91449\n",
      "epoch no.6 train no.461850  loss = 2.60556 avg_loss = 2.89350\n",
      "epoch no.6 train no.461860  loss = 3.03190 avg_loss = 2.89570\n",
      "epoch no.6 train no.461870  loss = 4.12817 avg_loss = 2.96023\n",
      "epoch no.6 train no.461880  loss = 1.93272 avg_loss = 2.94861\n",
      "epoch no.6 train no.461890  loss = 3.23697 avg_loss = 2.91737\n",
      "epoch no.6 train no.461900  loss = 3.11227 avg_loss = 2.91852\n",
      "epoch no.6 train no.461910  loss = 2.44877 avg_loss = 2.88268\n",
      "epoch no.6 train no.461920  loss = 2.96232 avg_loss = 2.84951\n",
      "epoch no.6 train no.461930  loss = 2.52111 avg_loss = 2.90780\n",
      "epoch no.6 train no.461940  loss = 4.23052 avg_loss = 2.92715\n",
      "epoch no.6 train no.461950  loss = 2.65791 avg_loss = 2.91022\n",
      "epoch no.6 train no.461960  loss = 2.55865 avg_loss = 2.93899\n",
      "epoch no.6 train no.461970  loss = 2.31665 avg_loss = 2.91472\n",
      "epoch no.6 train no.461980  loss = 1.82073 avg_loss = 2.87671\n",
      "epoch no.6 train no.461990  loss = 2.83728 avg_loss = 2.85967\n",
      "epoch no.6 train no.462000  loss = 3.68362 avg_loss = 2.87448\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년', '▁초반', '음악', '</s>']\n",
      "추억의 2000년 초반음악</s>\n",
      "epoch no.6 train no.462010  loss = 2.09909 avg_loss = 2.85728\n",
      "epoch no.6 train no.462020  loss = 3.78501 avg_loss = 2.86302\n",
      "epoch no.6 train no.462030  loss = 3.87200 avg_loss = 2.88624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.462040  loss = 2.64581 avg_loss = 2.87655\n",
      "epoch no.6 train no.462050  loss = 2.17486 avg_loss = 2.89631\n",
      "epoch no.6 train no.462060  loss = 4.57605 avg_loss = 2.95882\n",
      "epoch no.6 train no.462070  loss = 2.39580 avg_loss = 2.92375\n",
      "epoch no.6 train no.462080  loss = 1.60323 avg_loss = 2.88187\n",
      "epoch no.6 train no.462090  loss = 3.48959 avg_loss = 2.86965\n",
      "epoch no.6 train no.462100  loss = 2.51785 avg_loss = 2.88775\n",
      "epoch no.6 train no.462110  loss = 2.04831 avg_loss = 2.86798\n",
      "epoch no.6 train no.462120  loss = 3.01652 avg_loss = 2.88401\n",
      "epoch no.6 train no.462130  loss = 2.87661 avg_loss = 2.89574\n",
      "epoch no.6 train no.462140  loss = 2.53147 avg_loss = 2.90069\n",
      "epoch no.6 train no.462150  loss = 2.41231 avg_loss = 2.92584\n",
      "epoch no.6 train no.462160  loss = 2.37797 avg_loss = 2.90862\n",
      "epoch no.6 train no.462170  loss = 2.94674 avg_loss = 2.94015\n",
      "epoch no.6 train no.462180  loss = 2.18604 avg_loss = 2.90268\n",
      "epoch no.6 train no.462190  loss = 2.84395 avg_loss = 2.86715\n",
      "epoch no.6 train no.462200  loss = 3.60354 avg_loss = 2.87446\n",
      "epoch no.6 train no.462210  loss = 3.53392 avg_loss = 2.87031\n",
      "epoch no.6 train no.462220  loss = 3.07919 avg_loss = 2.86917\n",
      "epoch no.6 train no.462230  loss = 2.07147 avg_loss = 2.85787\n",
      "epoch no.6 train no.462240  loss = 1.94481 avg_loss = 2.86070\n",
      "epoch no.6 train no.462250  loss = 2.15030 avg_loss = 2.88249\n",
      "epoch no.6 train no.462260  loss = 1.90006 avg_loss = 2.88128\n",
      "epoch no.6 train no.462270  loss = 2.08250 avg_loss = 2.88808\n",
      "epoch no.6 train no.462280  loss = 1.88861 avg_loss = 2.85727\n",
      "epoch no.6 train no.462290  loss = 4.35422 avg_loss = 2.89903\n",
      "epoch no.6 train no.462300  loss = 2.26324 avg_loss = 2.89185\n",
      "epoch no.6 train no.462310  loss = 3.07853 avg_loss = 2.89908\n",
      "epoch no.6 train no.462320  loss = 2.48597 avg_loss = 2.88984\n",
      "epoch no.6 train no.462330  loss = 2.90433 avg_loss = 2.90156\n",
      "epoch no.6 train no.462340  loss = 2.51605 avg_loss = 2.86142\n",
      "epoch no.6 train no.462350  loss = 2.47103 avg_loss = 2.86305\n",
      "epoch no.6 train no.462360  loss = 2.17450 avg_loss = 2.88524\n",
      "epoch no.6 train no.462370  loss = 3.75204 avg_loss = 2.88665\n",
      "epoch no.6 train no.462380  loss = 3.48422 avg_loss = 2.84755\n",
      "epoch no.6 train no.462390  loss = 1.37019 avg_loss = 2.82513\n",
      "epoch no.6 train no.462400  loss = 4.18800 avg_loss = 2.83534\n",
      "epoch no.6 train no.462410  loss = 3.12315 avg_loss = 2.85990\n",
      "epoch no.6 train no.462420  loss = 2.84742 avg_loss = 2.84056\n",
      "epoch no.6 train no.462430  loss = 3.89860 avg_loss = 2.85014\n",
      "epoch no.6 train no.462440  loss = 2.61433 avg_loss = 2.86408\n",
      "epoch no.6 train no.462450  loss = 1.70899 avg_loss = 2.87986\n",
      "epoch no.6 train no.462460  loss = 3.46808 avg_loss = 2.92411\n",
      "epoch no.6 train no.462470  loss = 3.12254 avg_loss = 2.91228\n",
      "epoch no.6 train no.462480  loss = 2.87738 avg_loss = 2.94668\n",
      "epoch no.6 train no.462490  loss = 2.12899 avg_loss = 2.96247\n",
      "epoch no.6 train no.462500  loss = 4.51080 avg_loss = 2.99417\n",
      "epoch no.6 train no.462510  loss = 3.25469 avg_loss = 2.97727\n",
      "epoch no.6 train no.462520  loss = 2.25129 avg_loss = 2.95107\n",
      "epoch no.6 train no.462530  loss = 3.51541 avg_loss = 2.93514\n",
      "epoch no.6 train no.462540  loss = 1.96053 avg_loss = 2.94225\n",
      "epoch no.6 train no.462550  loss = 2.82926 avg_loss = 2.93492\n",
      "epoch no.6 train no.462560  loss = 3.17629 avg_loss = 2.93857\n",
      "epoch no.6 train no.462570  loss = 3.59121 avg_loss = 2.95712\n",
      "epoch no.6 train no.462580  loss = 1.82352 avg_loss = 2.90326\n",
      "epoch no.6 train no.462590  loss = 3.14489 avg_loss = 2.91665\n",
      "epoch no.6 train no.462600  loss = 2.65227 avg_loss = 2.89170\n",
      "epoch no.6 train no.462610  loss = 2.68883 avg_loss = 2.90771\n",
      "epoch no.6 train no.462620  loss = 2.62910 avg_loss = 2.92955\n",
      "epoch no.6 train no.462630  loss = 3.45306 avg_loss = 2.93831\n",
      "epoch no.6 train no.462640  loss = 4.13484 avg_loss = 2.94584\n",
      "epoch no.6 train no.462650  loss = 2.19922 avg_loss = 2.92170\n",
      "epoch no.6 train no.462660  loss = 2.35452 avg_loss = 2.92696\n",
      "epoch no.6 train no.462670  loss = 3.49536 avg_loss = 2.93015\n",
      "epoch no.6 train no.462680  loss = 2.59673 avg_loss = 2.90748\n",
      "epoch no.6 train no.462690  loss = 2.62203 avg_loss = 2.93157\n",
      "epoch no.6 train no.462700  loss = 2.94488 avg_loss = 2.94066\n",
      "epoch no.6 train no.462710  loss = 2.57144 avg_loss = 2.93386\n",
      "epoch no.6 train no.462720  loss = 3.66932 avg_loss = 2.94550\n",
      "epoch no.6 train no.462730  loss = 2.81799 avg_loss = 2.91161\n",
      "epoch no.6 train no.462740  loss = 2.47057 avg_loss = 2.88296\n",
      "epoch no.6 train no.462750  loss = 4.81803 avg_loss = 2.90146\n",
      "epoch no.6 train no.462760  loss = 2.99860 avg_loss = 2.90800\n",
      "epoch no.6 train no.462770  loss = 2.73419 avg_loss = 2.90204\n",
      "epoch no.6 train no.462780  loss = 1.89034 avg_loss = 2.85937\n",
      "epoch no.6 train no.462790  loss = 2.74216 avg_loss = 2.85140\n",
      "epoch no.6 train no.462800  loss = 1.57991 avg_loss = 2.82518\n",
      "epoch no.6 train no.462810  loss = 1.96867 avg_loss = 2.81727\n",
      "epoch no.6 train no.462820  loss = 2.77995 avg_loss = 2.81853\n",
      "epoch no.6 train no.462830  loss = 3.71885 avg_loss = 2.78830\n",
      "epoch no.6 train no.462840  loss = 2.17361 avg_loss = 2.80562\n",
      "epoch no.6 train no.462850  loss = 1.55196 avg_loss = 2.75740\n",
      "epoch no.6 train no.462860  loss = 1.77571 avg_loss = 2.74989\n",
      "epoch no.6 train no.462870  loss = 3.80055 avg_loss = 2.76753\n",
      "epoch no.6 train no.462880  loss = 2.02093 avg_loss = 2.76327\n",
      "epoch no.6 train no.462890  loss = 2.39776 avg_loss = 2.76182\n",
      "epoch no.6 train no.462900  loss = 3.98001 avg_loss = 2.79469\n",
      "epoch no.6 train no.462910  loss = 3.84381 avg_loss = 2.83340\n",
      "epoch no.6 train no.462920  loss = 3.81946 avg_loss = 2.83550\n",
      "epoch no.6 train no.462930  loss = 2.90035 avg_loss = 2.85195\n",
      "epoch no.6 train no.462940  loss = 2.21495 avg_loss = 2.85019\n",
      "epoch no.6 train no.462950  loss = 3.32717 avg_loss = 2.88999\n",
      "epoch no.6 train no.462960  loss = 2.20490 avg_loss = 2.89338\n",
      "epoch no.6 train no.462970  loss = 2.65334 avg_loss = 2.89101\n",
      "epoch no.6 train no.462980  loss = 2.51340 avg_loss = 2.84029\n",
      "epoch no.6 train no.462990  loss = 2.69775 avg_loss = 2.83056\n",
      "epoch no.6 train no.463000  loss = 1.93497 avg_loss = 2.86144\n",
      "4\n",
      "to_tokens: ['▁비', '▁그', '년', '▁발라', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.6 train no.463010  loss = 2.95405 avg_loss = 2.86578\n",
      "epoch no.6 train no.463020  loss = 2.52837 avg_loss = 2.88533\n",
      "epoch no.6 train no.463030  loss = 2.86232 avg_loss = 2.85855\n",
      "epoch no.6 train no.463040  loss = 3.09083 avg_loss = 2.84513\n",
      "epoch no.6 train no.463050  loss = 3.72531 avg_loss = 2.87373\n",
      "epoch no.6 train no.463060  loss = 2.80289 avg_loss = 2.85190\n",
      "epoch no.6 train no.463070  loss = 2.08568 avg_loss = 2.84228\n",
      "epoch no.6 train no.463080  loss = 3.55165 avg_loss = 2.83080\n",
      "epoch no.6 train no.463090  loss = 2.92972 avg_loss = 2.84119\n",
      "epoch no.6 train no.463100  loss = 3.09140 avg_loss = 2.85662\n",
      "epoch no.6 train no.463110  loss = 3.23443 avg_loss = 2.86503\n",
      "epoch no.6 train no.463120  loss = 3.19657 avg_loss = 2.85504\n",
      "epoch no.6 train no.463130  loss = 3.17923 avg_loss = 2.90249\n",
      "epoch no.6 train no.463140  loss = 1.85623 avg_loss = 2.90065\n",
      "epoch no.6 train no.463150  loss = 2.44229 avg_loss = 2.91088\n",
      "epoch no.6 train no.463160  loss = 3.35208 avg_loss = 2.91911\n",
      "epoch no.6 train no.463170  loss = 4.12812 avg_loss = 2.92826\n",
      "epoch no.6 train no.463180  loss = 4.44913 avg_loss = 2.92287\n",
      "epoch no.6 train no.463190  loss = 3.30755 avg_loss = 2.93254\n",
      "epoch no.6 train no.463200  loss = 3.31446 avg_loss = 2.92799\n",
      "epoch no.6 train no.463210  loss = 2.86882 avg_loss = 2.91014\n",
      "epoch no.6 train no.463220  loss = 1.94042 avg_loss = 2.87873\n",
      "epoch no.6 train no.463230  loss = 2.35393 avg_loss = 2.85814\n",
      "epoch no.6 train no.463240  loss = 3.34821 avg_loss = 2.89545\n",
      "epoch no.6 train no.463250  loss = 3.58642 avg_loss = 2.90247\n",
      "epoch no.6 train no.463260  loss = 3.78013 avg_loss = 2.93701\n",
      "epoch no.6 train no.463270  loss = 3.36190 avg_loss = 2.94860\n",
      "epoch no.6 train no.463280  loss = 2.56940 avg_loss = 2.94959\n",
      "epoch no.6 train no.463290  loss = 3.00443 avg_loss = 2.97794\n",
      "epoch no.6 train no.463300  loss = 2.54101 avg_loss = 2.95099\n",
      "epoch no.6 train no.463310  loss = 4.71521 avg_loss = 2.95122\n",
      "epoch no.6 train no.463320  loss = 3.26741 avg_loss = 2.94276\n",
      "epoch no.6 train no.463330  loss = 2.32152 avg_loss = 2.89853\n",
      "epoch no.6 train no.463340  loss = 2.56675 avg_loss = 2.88065\n",
      "epoch no.6 train no.463350  loss = 2.86588 avg_loss = 2.88568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.463360  loss = 2.90636 avg_loss = 2.89597\n",
      "epoch no.6 train no.463370  loss = 3.19361 avg_loss = 2.87578\n",
      "epoch no.6 train no.463380  loss = 1.61500 avg_loss = 2.84220\n",
      "epoch no.6 train no.463390  loss = 3.12180 avg_loss = 2.87404\n",
      "epoch no.6 train no.463400  loss = 3.02422 avg_loss = 2.86844\n",
      "epoch no.6 train no.463410  loss = 2.69432 avg_loss = 2.85828\n",
      "epoch no.6 train no.463420  loss = 1.79034 avg_loss = 2.87521\n",
      "epoch no.6 train no.463430  loss = 2.97085 avg_loss = 2.91638\n",
      "epoch no.6 train no.463440  loss = 2.30399 avg_loss = 2.90723\n",
      "epoch no.6 train no.463450  loss = 2.98994 avg_loss = 2.90602\n",
      "epoch no.6 train no.463460  loss = 2.65245 avg_loss = 2.91583\n",
      "epoch no.6 train no.463470  loss = 2.29349 avg_loss = 2.91949\n",
      "epoch no.6 train no.463480  loss = 5.16106 avg_loss = 2.95133\n",
      "epoch no.6 train no.463490  loss = 0.89548 avg_loss = 2.93674\n",
      "epoch no.6 train no.463500  loss = 1.95636 avg_loss = 2.91825\n",
      "epoch no.6 train no.463510  loss = 2.99714 avg_loss = 2.94836\n",
      "epoch no.6 train no.463520  loss = 2.10645 avg_loss = 2.94137\n",
      "epoch no.6 train no.463530  loss = 1.99907 avg_loss = 2.91902\n",
      "epoch no.6 train no.463540  loss = 1.84092 avg_loss = 2.91283\n",
      "epoch no.6 train no.463550  loss = 2.26216 avg_loss = 2.89385\n",
      "epoch no.6 train no.463560  loss = 2.11984 avg_loss = 2.90224\n",
      "epoch no.6 train no.463570  loss = 2.32171 avg_loss = 2.87161\n",
      "epoch no.6 train no.463580  loss = 3.19370 avg_loss = 2.84043\n",
      "epoch no.6 train no.463590  loss = 3.17278 avg_loss = 2.82795\n",
      "epoch no.6 train no.463600  loss = 2.92770 avg_loss = 2.80600\n",
      "epoch no.6 train no.463610  loss = 2.49643 avg_loss = 2.81301\n",
      "epoch no.6 train no.463620  loss = 2.79187 avg_loss = 2.81557\n",
      "epoch no.6 train no.463630  loss = 2.64849 avg_loss = 2.82765\n",
      "epoch no.6 train no.463640  loss = 3.48671 avg_loss = 2.82864\n",
      "epoch no.6 train no.463650  loss = 3.70237 avg_loss = 2.83328\n",
      "epoch no.6 train no.463660  loss = 3.84589 avg_loss = 2.84836\n",
      "epoch no.6 train no.463670  loss = 2.05859 avg_loss = 2.82482\n",
      "epoch no.6 train no.463680  loss = 4.34753 avg_loss = 2.87124\n",
      "epoch no.6 train no.463690  loss = 3.45492 avg_loss = 2.88386\n",
      "epoch no.6 train no.463700  loss = 3.45946 avg_loss = 2.86919\n",
      "epoch no.6 train no.463710  loss = 3.36521 avg_loss = 2.89509\n",
      "epoch no.6 train no.463720  loss = 2.13695 avg_loss = 2.87239\n",
      "epoch no.6 train no.463730  loss = 2.75077 avg_loss = 2.86926\n",
      "epoch no.6 train no.463740  loss = 1.50601 avg_loss = 2.85131\n",
      "epoch no.6 train no.463750  loss = 1.42452 avg_loss = 2.82344\n",
      "epoch no.6 train no.463760  loss = 2.50463 avg_loss = 2.83784\n",
      "epoch no.6 train no.463770  loss = 2.60783 avg_loss = 2.82804\n",
      "epoch no.6 train no.463780  loss = 2.56021 avg_loss = 2.87449\n",
      "epoch no.6 train no.463790  loss = 2.11050 avg_loss = 2.87322\n",
      "epoch no.6 train no.463800  loss = 2.44539 avg_loss = 2.89574\n",
      "epoch no.6 train no.463810  loss = 2.36077 avg_loss = 2.90561\n",
      "epoch no.6 train no.463820  loss = 3.51972 avg_loss = 2.90160\n",
      "epoch no.6 train no.463830  loss = 3.51588 avg_loss = 2.93020\n",
      "epoch no.6 train no.463840  loss = 2.62244 avg_loss = 2.93422\n",
      "epoch no.6 train no.463850  loss = 2.54603 avg_loss = 2.91264\n",
      "epoch no.6 train no.463860  loss = 2.52572 avg_loss = 2.89905\n",
      "epoch no.6 train no.463870  loss = 2.11247 avg_loss = 2.90863\n",
      "epoch no.6 train no.463880  loss = 2.14792 avg_loss = 2.89882\n",
      "epoch no.6 train no.463890  loss = 2.80329 avg_loss = 2.89178\n",
      "epoch no.6 train no.463900  loss = 3.13819 avg_loss = 2.87082\n",
      "epoch no.6 train no.463910  loss = 1.66379 avg_loss = 2.83461\n",
      "epoch no.6 train no.463920  loss = 3.88508 avg_loss = 2.88212\n",
      "epoch no.6 train no.463930  loss = 2.93841 avg_loss = 2.87547\n",
      "epoch no.6 train no.463940  loss = 3.34319 avg_loss = 2.88525\n",
      "epoch no.6 train no.463950  loss = 2.01294 avg_loss = 2.89467\n",
      "epoch no.6 train no.463960  loss = 2.09052 avg_loss = 2.86154\n",
      "epoch no.6 train no.463970  loss = 3.28002 avg_loss = 2.88455\n",
      "epoch no.6 train no.463980  loss = 2.63586 avg_loss = 2.88549\n",
      "epoch no.6 train no.463990  loss = 3.25475 avg_loss = 2.87636\n",
      "epoch no.6 train no.464000  loss = 2.34042 avg_loss = 2.87620\n",
      "4\n",
      "to_tokens: ['▁비', '▁올드', '곡', '들', '▁모음', '</s>']\n",
      "추억의 명곡 리메이크 모음</s>\n",
      "epoch no.6 train no.464010  loss = 1.97567 avg_loss = 2.86127\n",
      "epoch no.6 train no.464020  loss = 3.67729 avg_loss = 2.85668\n",
      "epoch no.6 train no.464030  loss = 1.43766 avg_loss = 2.81031\n",
      "epoch no.6 train no.464040  loss = 2.67638 avg_loss = 2.79430\n",
      "epoch no.6 train no.464050  loss = 1.68847 avg_loss = 2.80187\n",
      "epoch no.6 train no.464060  loss = 1.87977 avg_loss = 2.81337\n",
      "epoch no.6 train no.464070  loss = 2.04468 avg_loss = 2.81086\n",
      "epoch no.6 train no.464080  loss = 3.33025 avg_loss = 2.81253\n",
      "epoch no.6 train no.464090  loss = 2.49720 avg_loss = 2.80635\n",
      "epoch no.6 train no.464100  loss = 2.07854 avg_loss = 2.78217\n",
      "epoch no.6 train no.464110  loss = 2.69584 avg_loss = 2.78932\n",
      "epoch no.6 train no.464120  loss = 2.40377 avg_loss = 2.80597\n",
      "epoch no.6 train no.464130  loss = 2.98510 avg_loss = 2.79567\n",
      "epoch no.6 train no.464140  loss = 2.81615 avg_loss = 2.82784\n",
      "epoch no.6 train no.464150  loss = 2.52529 avg_loss = 2.82094\n",
      "epoch no.6 train no.464160  loss = 3.43351 avg_loss = 2.84808\n",
      "epoch no.6 train no.464170  loss = 2.34804 avg_loss = 2.83577\n",
      "epoch no.6 train no.464180  loss = 3.40397 avg_loss = 2.87051\n",
      "epoch no.6 train no.464190  loss = 2.80544 avg_loss = 2.83614\n",
      "epoch no.6 train no.464200  loss = 3.08183 avg_loss = 2.83659\n",
      "epoch no.6 train no.464210  loss = 4.31350 avg_loss = 2.86853\n",
      "epoch no.6 train no.464220  loss = 3.05196 avg_loss = 2.87332\n",
      "epoch no.6 train no.464230  loss = 2.98124 avg_loss = 2.87189\n",
      "epoch no.6 train no.464240  loss = 2.19056 avg_loss = 2.87027\n",
      "epoch no.6 train no.464250  loss = 3.02147 avg_loss = 2.87238\n",
      "epoch no.6 train no.464260  loss = 2.30616 avg_loss = 2.88615\n",
      "epoch no.6 train no.464270  loss = 2.58816 avg_loss = 2.90524\n",
      "epoch no.6 train no.464280  loss = 2.81342 avg_loss = 2.90833\n",
      "epoch no.6 train no.464290  loss = 2.50213 avg_loss = 2.92804\n",
      "epoch no.6 train no.464300  loss = 2.52129 avg_loss = 2.92653\n",
      "epoch no.6 train no.464310  loss = 3.56178 avg_loss = 2.90330\n",
      "epoch no.6 train no.464320  loss = 3.10869 avg_loss = 2.89113\n",
      "epoch no.6 train no.464330  loss = 4.05215 avg_loss = 2.90453\n",
      "epoch no.6 train no.464340  loss = 3.56796 avg_loss = 2.91274\n",
      "epoch no.6 train no.464350  loss = 3.53858 avg_loss = 2.89415\n",
      "epoch no.6 train no.464360  loss = 3.41405 avg_loss = 2.92789\n",
      "epoch no.6 train no.464370  loss = 3.52198 avg_loss = 2.90226\n",
      "epoch no.6 train no.464380  loss = 5.00737 avg_loss = 2.93506\n",
      "epoch no.6 train no.464390  loss = 2.88407 avg_loss = 2.92301\n",
      "epoch no.6 train no.464400  loss = 2.51868 avg_loss = 2.92966\n",
      "epoch no.6 train no.464410  loss = 2.59288 avg_loss = 2.89135\n",
      "epoch no.6 train no.464420  loss = 3.32897 avg_loss = 2.87280\n",
      "epoch no.6 train no.464430  loss = 2.71098 avg_loss = 2.89246\n",
      "epoch no.6 train no.464440  loss = 2.18217 avg_loss = 2.89804\n",
      "epoch no.6 train no.464450  loss = 2.42489 avg_loss = 2.89730\n",
      "epoch no.6 train no.464460  loss = 3.53073 avg_loss = 2.88082\n",
      "epoch no.6 train no.464470  loss = 4.08372 avg_loss = 2.88874\n",
      "epoch no.6 train no.464480  loss = 3.23822 avg_loss = 2.92333\n",
      "epoch no.6 train no.464490  loss = 3.97636 avg_loss = 2.94470\n",
      "epoch no.6 train no.464500  loss = 2.17857 avg_loss = 2.96611\n",
      "epoch no.6 train no.464510  loss = 1.72484 avg_loss = 2.93898\n",
      "epoch no.6 train no.464520  loss = 2.52911 avg_loss = 2.91271\n",
      "epoch no.6 train no.464530  loss = 3.89292 avg_loss = 2.90565\n",
      "epoch no.6 train no.464540  loss = 4.41165 avg_loss = 2.93945\n",
      "epoch no.6 train no.464550  loss = 2.84730 avg_loss = 2.92828\n",
      "epoch no.6 train no.464560  loss = 3.29399 avg_loss = 2.89761\n",
      "epoch no.6 train no.464570  loss = 2.74117 avg_loss = 2.89718\n",
      "epoch no.6 train no.464580  loss = 3.10130 avg_loss = 2.90821\n",
      "epoch no.6 train no.464590  loss = 3.49092 avg_loss = 2.90673\n",
      "epoch no.6 train no.464600  loss = 3.58437 avg_loss = 2.89152\n",
      "epoch no.6 train no.464610  loss = 3.22317 avg_loss = 2.91662\n",
      "epoch no.6 train no.464620  loss = 1.53506 avg_loss = 2.92762\n",
      "epoch no.6 train no.464630  loss = 3.90648 avg_loss = 2.95923\n",
      "epoch no.6 train no.464640  loss = 3.12030 avg_loss = 2.94792\n",
      "epoch no.6 train no.464650  loss = 2.38255 avg_loss = 2.94482\n",
      "epoch no.6 train no.464660  loss = 2.66202 avg_loss = 2.96533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.464670  loss = 3.76396 avg_loss = 2.96591\n",
      "epoch no.6 train no.464680  loss = 2.28086 avg_loss = 2.96512\n",
      "epoch no.6 train no.464690  loss = 1.79411 avg_loss = 2.96944\n",
      "epoch no.6 train no.464700  loss = 2.15818 avg_loss = 2.95976\n",
      "epoch no.6 train no.464710  loss = 2.43230 avg_loss = 2.95012\n",
      "epoch no.6 train no.464720  loss = 4.29658 avg_loss = 2.94962\n",
      "epoch no.6 train no.464730  loss = 2.68009 avg_loss = 2.95468\n",
      "epoch no.6 train no.464740  loss = 2.56740 avg_loss = 2.95604\n",
      "epoch no.6 train no.464750  loss = 4.05221 avg_loss = 2.98920\n",
      "epoch no.6 train no.464760  loss = 3.68354 avg_loss = 2.99786\n",
      "epoch no.6 train no.464770  loss = 2.98515 avg_loss = 2.98715\n",
      "epoch no.6 train no.464780  loss = 2.80243 avg_loss = 3.00364\n",
      "epoch no.6 train no.464790  loss = 2.72043 avg_loss = 3.03166\n",
      "epoch no.6 train no.464800  loss = 2.78011 avg_loss = 3.01987\n",
      "epoch no.6 train no.464810  loss = 2.72203 avg_loss = 3.01573\n",
      "epoch no.6 train no.464820  loss = 2.89691 avg_loss = 3.02346\n",
      "epoch no.6 train no.464830  loss = 2.43167 avg_loss = 2.98877\n",
      "epoch no.6 train no.464840  loss = 3.61061 avg_loss = 2.99910\n",
      "epoch no.6 train no.464850  loss = 2.91764 avg_loss = 2.96471\n",
      "epoch no.6 train no.464860  loss = 4.19887 avg_loss = 3.02489\n",
      "epoch no.6 train no.464870  loss = 4.61508 avg_loss = 3.02880\n",
      "epoch no.6 train no.464880  loss = 2.30082 avg_loss = 3.01189\n",
      "epoch no.6 train no.464890  loss = 2.87793 avg_loss = 3.05188\n",
      "epoch no.6 train no.464900  loss = 2.35550 avg_loss = 3.01469\n",
      "epoch no.6 train no.464910  loss = 1.60279 avg_loss = 3.00235\n",
      "epoch no.6 train no.464920  loss = 3.01320 avg_loss = 2.97575\n",
      "epoch no.6 train no.464930  loss = 2.50465 avg_loss = 2.95307\n",
      "epoch no.6 train no.464940  loss = 3.27384 avg_loss = 2.94904\n",
      "epoch no.6 train no.464950  loss = 3.22848 avg_loss = 2.96002\n",
      "epoch no.6 train no.464960  loss = 3.05844 avg_loss = 2.94835\n",
      "epoch no.6 train no.464970  loss = 4.03803 avg_loss = 2.96935\n",
      "epoch no.6 train no.464980  loss = 1.63738 avg_loss = 2.94992\n",
      "epoch no.6 train no.464990  loss = 2.29895 avg_loss = 2.92304\n",
      "epoch no.6 train no.465000  loss = 4.38189 avg_loss = 2.97879\n",
      "4\n",
      "to_tokens: ['▁', '▁90', '▁o', 'st', '▁명', '</s>']\n",
      "추억의 드라마 ost 모음</s>\n",
      "epoch no.6 train no.465010  loss = 3.45723 avg_loss = 2.95934\n",
      "epoch no.6 train no.465020  loss = 3.27048 avg_loss = 2.95795\n",
      "epoch no.6 train no.465030  loss = 2.99445 avg_loss = 2.92265\n",
      "epoch no.6 train no.465040  loss = 3.28377 avg_loss = 2.90541\n",
      "epoch no.6 train no.465050  loss = 4.21118 avg_loss = 2.95672\n",
      "epoch no.6 train no.465060  loss = 2.72126 avg_loss = 2.93855\n",
      "epoch no.6 train no.465070  loss = 1.67835 avg_loss = 2.92345\n",
      "epoch no.6 train no.465080  loss = 2.55422 avg_loss = 2.88685\n",
      "epoch no.6 train no.465090  loss = 2.79565 avg_loss = 2.86744\n",
      "epoch no.6 train no.465100  loss = 2.96301 avg_loss = 2.84789\n",
      "epoch no.6 train no.465110  loss = 4.17622 avg_loss = 2.86016\n",
      "epoch no.6 train no.465120  loss = 1.68146 avg_loss = 2.81861\n",
      "epoch no.6 train no.465130  loss = 3.03856 avg_loss = 2.84910\n",
      "epoch no.6 train no.465140  loss = 3.17052 avg_loss = 2.89105\n",
      "epoch no.6 train no.465150  loss = 3.09133 avg_loss = 2.90698\n",
      "epoch no.6 train no.465160  loss = 1.61151 avg_loss = 2.88522\n",
      "epoch no.6 train no.465170  loss = 2.23839 avg_loss = 2.87776\n",
      "epoch no.6 train no.465180  loss = 3.06223 avg_loss = 2.90179\n",
      "epoch no.6 train no.465190  loss = 3.05803 avg_loss = 2.90786\n",
      "epoch no.6 train no.465200  loss = 2.53669 avg_loss = 2.90250\n",
      "epoch no.6 train no.465210  loss = 2.48605 avg_loss = 2.90384\n",
      "epoch no.6 train no.465220  loss = 2.82498 avg_loss = 2.92857\n",
      "epoch no.6 train no.465230  loss = 3.90663 avg_loss = 2.96143\n",
      "epoch no.6 train no.465240  loss = 3.32104 avg_loss = 2.94888\n",
      "epoch no.6 train no.465250  loss = 3.33957 avg_loss = 2.93232\n",
      "epoch no.6 train no.465260  loss = 1.87449 avg_loss = 2.92862\n",
      "epoch no.6 train no.465270  loss = 3.77128 avg_loss = 2.96042\n",
      "epoch no.6 train no.465280  loss = 3.08794 avg_loss = 2.99549\n",
      "epoch no.6 train no.465290  loss = 1.73269 avg_loss = 2.98013\n",
      "epoch no.6 train no.465300  loss = 2.63046 avg_loss = 2.95173\n",
      "epoch no.6 train no.465310  loss = 3.39887 avg_loss = 2.95992\n",
      "epoch no.6 train no.465320  loss = 2.32632 avg_loss = 2.97709\n",
      "epoch no.6 train no.465330  loss = 2.43747 avg_loss = 2.92770\n",
      "epoch no.6 train no.465340  loss = 2.54547 avg_loss = 2.92461\n",
      "epoch no.6 train no.465350  loss = 3.16167 avg_loss = 2.92136\n",
      "epoch no.6 train no.465360  loss = 2.68999 avg_loss = 2.93263\n",
      "epoch no.6 train no.465370  loss = 2.78307 avg_loss = 2.95345\n",
      "epoch no.6 train no.465380  loss = 2.14135 avg_loss = 2.92966\n",
      "epoch no.6 train no.465390  loss = 3.35889 avg_loss = 2.93088\n",
      "epoch no.6 train no.465400  loss = 4.64528 avg_loss = 2.96150\n",
      "epoch no.6 train no.465410  loss = 2.38070 avg_loss = 2.93131\n",
      "epoch no.6 train no.465420  loss = 2.29730 avg_loss = 2.93923\n",
      "epoch no.6 train no.465430  loss = 2.31852 avg_loss = 2.92981\n",
      "epoch no.6 train no.465440  loss = 3.37070 avg_loss = 2.96896\n",
      "epoch no.6 train no.465450  loss = 3.79728 avg_loss = 2.98484\n",
      "epoch no.6 train no.465460  loss = 3.44500 avg_loss = 2.95217\n",
      "epoch no.6 train no.465470  loss = 2.25390 avg_loss = 2.90314\n",
      "epoch no.6 train no.465480  loss = 2.97144 avg_loss = 2.94597\n",
      "epoch no.6 train no.465490  loss = 2.43620 avg_loss = 2.93154\n",
      "epoch no.6 train no.465500  loss = 2.79102 avg_loss = 2.90952\n",
      "epoch no.6 train no.465510  loss = 3.34186 avg_loss = 2.91184\n",
      "epoch no.6 train no.465520  loss = 3.03879 avg_loss = 2.93949\n",
      "epoch no.6 train no.465530  loss = 2.30108 avg_loss = 2.92269\n",
      "epoch no.6 train no.465540  loss = 2.62622 avg_loss = 2.90937\n",
      "epoch no.6 train no.465550  loss = 2.30780 avg_loss = 2.96168\n",
      "epoch no.6 train no.465560  loss = 2.03616 avg_loss = 2.97124\n",
      "epoch no.6 train no.465570  loss = 2.33580 avg_loss = 2.94736\n",
      "epoch no.6 train no.465580  loss = 2.53728 avg_loss = 2.95927\n",
      "epoch no.6 train no.465590  loss = 2.63973 avg_loss = 2.94358\n",
      "epoch no.6 train no.465600  loss = 2.56558 avg_loss = 2.94377\n",
      "epoch no.6 train no.465610  loss = 2.25715 avg_loss = 2.93449\n",
      "epoch no.6 train no.465620  loss = 2.70918 avg_loss = 2.94335\n",
      "epoch no.6 train no.465630  loss = 4.15626 avg_loss = 2.95128\n",
      "epoch no.6 train no.465640  loss = 2.26487 avg_loss = 2.92479\n",
      "epoch no.6 train no.465650  loss = 3.65816 avg_loss = 2.89145\n",
      "epoch no.6 train no.465660  loss = 1.94838 avg_loss = 2.90135\n",
      "epoch no.6 train no.465670  loss = 3.64782 avg_loss = 2.91300\n",
      "epoch no.6 train no.465680  loss = 2.50553 avg_loss = 2.93384\n",
      "epoch no.6 train no.465690  loss = 2.84799 avg_loss = 2.92912\n",
      "epoch no.6 train no.465700  loss = 3.06687 avg_loss = 2.92972\n",
      "epoch no.6 train no.465710  loss = 2.80689 avg_loss = 2.91112\n",
      "epoch no.6 train no.465720  loss = 3.43787 avg_loss = 2.91996\n",
      "epoch no.6 train no.465730  loss = 1.95400 avg_loss = 2.90865\n",
      "epoch no.6 train no.465740  loss = 2.53053 avg_loss = 2.93832\n",
      "epoch no.6 train no.465750  loss = 2.39325 avg_loss = 2.95401\n",
      "epoch no.6 train no.465760  loss = 2.22507 avg_loss = 2.93595\n",
      "epoch no.6 train no.465770  loss = 3.09432 avg_loss = 2.95192\n",
      "epoch no.6 train no.465780  loss = 1.40646 avg_loss = 2.91690\n",
      "epoch no.6 train no.465790  loss = 4.39691 avg_loss = 2.92063\n",
      "epoch no.6 train no.465800  loss = 2.61916 avg_loss = 2.87398\n",
      "epoch no.6 train no.465810  loss = 1.99166 avg_loss = 2.81856\n",
      "epoch no.6 train no.465820  loss = 1.96109 avg_loss = 2.82394\n",
      "epoch no.6 train no.465830  loss = 2.84454 avg_loss = 2.80925\n",
      "epoch no.6 train no.465840  loss = 4.24317 avg_loss = 2.81133\n",
      "epoch no.6 train no.465850  loss = 2.64970 avg_loss = 2.80427\n",
      "epoch no.6 train no.465860  loss = 2.35518 avg_loss = 2.78920\n",
      "epoch no.6 train no.465870  loss = 2.92270 avg_loss = 2.85145\n",
      "epoch no.6 train no.465880  loss = 2.68473 avg_loss = 2.88164\n",
      "epoch no.6 train no.465890  loss = 4.02357 avg_loss = 2.92180\n",
      "epoch no.6 train no.465900  loss = 2.71719 avg_loss = 2.89020\n",
      "epoch no.6 train no.465910  loss = 3.03461 avg_loss = 2.91265\n",
      "epoch no.6 train no.465920  loss = 2.16962 avg_loss = 2.91783\n",
      "epoch no.6 train no.465930  loss = 3.07708 avg_loss = 2.93782\n",
      "epoch no.6 train no.465940  loss = 2.86387 avg_loss = 2.94823\n",
      "epoch no.6 train no.465950  loss = 2.72188 avg_loss = 2.95655\n",
      "epoch no.6 train no.465960  loss = 2.20440 avg_loss = 2.93237\n",
      "epoch no.6 train no.465970  loss = 3.00359 avg_loss = 2.92731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.465980  loss = 2.05115 avg_loss = 2.87127\n",
      "epoch no.6 train no.465990  loss = 3.08291 avg_loss = 2.85693\n",
      "epoch no.6 train no.466000  loss = 2.31735 avg_loss = 2.86092\n",
      "5\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '▁가요', '</s>', '</s>']\n",
      "추억의 90년대 댄스 팝송</s>\n",
      "epoch no.6 train no.466010  loss = 2.97316 avg_loss = 2.88474\n",
      "epoch no.6 train no.466020  loss = 3.10925 avg_loss = 2.93126\n",
      "epoch no.6 train no.466030  loss = 2.60495 avg_loss = 3.00010\n",
      "epoch no.6 train no.466040  loss = 3.10048 avg_loss = 3.01875\n",
      "epoch no.6 train no.466050  loss = 2.55841 avg_loss = 2.98860\n",
      "epoch no.6 train no.466060  loss = 2.78249 avg_loss = 2.97785\n",
      "epoch no.6 train no.466070  loss = 3.13072 avg_loss = 2.94995\n",
      "epoch no.6 train no.466080  loss = 2.96733 avg_loss = 2.95585\n",
      "epoch no.6 train no.466090  loss = 2.32272 avg_loss = 2.93122\n",
      "epoch no.6 train no.466100  loss = 3.38522 avg_loss = 2.91950\n",
      "epoch no.6 train no.466110  loss = 3.22266 avg_loss = 2.93260\n",
      "epoch no.6 train no.466120  loss = 4.10017 avg_loss = 2.93345\n",
      "epoch no.6 train no.466130  loss = 2.80588 avg_loss = 2.90099\n",
      "epoch no.6 train no.466140  loss = 2.47024 avg_loss = 2.88088\n",
      "epoch no.6 train no.466150  loss = 2.49371 avg_loss = 2.86888\n",
      "epoch no.6 train no.466160  loss = 2.97200 avg_loss = 2.87293\n",
      "epoch no.6 train no.466170  loss = 2.60202 avg_loss = 2.86538\n",
      "epoch no.6 train no.466180  loss = 4.75394 avg_loss = 2.89375\n",
      "epoch no.6 train no.466190  loss = 3.61573 avg_loss = 2.88975\n",
      "epoch no.6 train no.466200  loss = 2.98158 avg_loss = 2.86936\n",
      "epoch no.6 train no.466210  loss = 2.44602 avg_loss = 2.87458\n",
      "epoch no.6 train no.466220  loss = 3.74890 avg_loss = 2.89873\n",
      "epoch no.6 train no.466230  loss = 2.79117 avg_loss = 2.90946\n",
      "epoch no.6 train no.466240  loss = 3.07958 avg_loss = 2.93310\n",
      "epoch no.6 train no.466250  loss = 3.92870 avg_loss = 2.96698\n",
      "epoch no.6 train no.466260  loss = 2.38280 avg_loss = 2.94958\n",
      "epoch no.6 train no.466270  loss = 2.73974 avg_loss = 2.95364\n",
      "epoch no.6 train no.466280  loss = 2.03975 avg_loss = 2.95379\n",
      "epoch no.6 train no.466290  loss = 2.61641 avg_loss = 2.92845\n",
      "epoch no.6 train no.466300  loss = 2.41485 avg_loss = 2.89314\n",
      "epoch no.6 train no.466310  loss = 3.72816 avg_loss = 2.88089\n",
      "epoch no.6 train no.466320  loss = 3.67310 avg_loss = 2.90531\n",
      "epoch no.6 train no.466330  loss = 2.76808 avg_loss = 2.88082\n",
      "epoch no.6 train no.466340  loss = 3.80276 avg_loss = 2.86896\n",
      "epoch no.6 train no.466350  loss = 2.85771 avg_loss = 2.86987\n",
      "epoch no.6 train no.466360  loss = 4.06349 avg_loss = 2.88490\n",
      "epoch no.6 train no.466370  loss = 2.39457 avg_loss = 2.86758\n",
      "epoch no.6 train no.466380  loss = 1.64853 avg_loss = 2.88457\n",
      "epoch no.6 train no.466390  loss = 3.66870 avg_loss = 2.87577\n",
      "epoch no.6 train no.466400  loss = 3.22827 avg_loss = 2.90106\n",
      "epoch no.6 train no.466410  loss = 2.77000 avg_loss = 2.92225\n",
      "epoch no.6 train no.466420  loss = 2.80496 avg_loss = 2.91702\n",
      "epoch no.6 train no.466430  loss = 2.09236 avg_loss = 2.89220\n",
      "epoch no.6 train no.466440  loss = 4.82594 avg_loss = 2.92883\n",
      "epoch no.6 train no.466450  loss = 2.42533 avg_loss = 2.93861\n",
      "epoch no.6 train no.466460  loss = 3.37943 avg_loss = 2.92675\n",
      "epoch no.6 train no.466470  loss = 2.88364 avg_loss = 2.91851\n",
      "epoch no.6 train no.466480  loss = 3.90457 avg_loss = 2.89705\n",
      "epoch no.6 train no.466490  loss = 2.70141 avg_loss = 2.88974\n",
      "epoch no.6 train no.466500  loss = 2.29743 avg_loss = 2.89484\n",
      "epoch no.6 train no.466510  loss = 1.97067 avg_loss = 2.88693\n",
      "epoch no.6 train no.466520  loss = 2.18332 avg_loss = 2.88658\n",
      "epoch no.6 train no.466530  loss = 2.24830 avg_loss = 2.89505\n",
      "epoch no.6 train no.466540  loss = 2.36996 avg_loss = 2.89811\n",
      "epoch no.6 train no.466550  loss = 2.42608 avg_loss = 2.88366\n",
      "epoch no.6 train no.466560  loss = 3.73442 avg_loss = 2.85996\n",
      "epoch no.6 train no.466570  loss = 4.35950 avg_loss = 2.92818\n",
      "epoch no.6 train no.466580  loss = 4.38976 avg_loss = 2.94962\n",
      "epoch no.6 train no.466590  loss = 2.77969 avg_loss = 2.92023\n",
      "epoch no.6 train no.466600  loss = 4.73585 avg_loss = 2.92742\n",
      "epoch no.6 train no.466610  loss = 2.48417 avg_loss = 2.90327\n",
      "epoch no.6 train no.466620  loss = 3.27724 avg_loss = 2.91594\n",
      "epoch no.6 train no.466630  loss = 2.56742 avg_loss = 2.88353\n",
      "epoch no.6 train no.466640  loss = 2.43292 avg_loss = 2.86462\n",
      "epoch no.6 train no.466650  loss = 2.33276 avg_loss = 2.80787\n",
      "epoch no.6 train no.466660  loss = 1.96437 avg_loss = 2.80307\n",
      "epoch no.6 train no.466670  loss = 1.60934 avg_loss = 2.80950\n",
      "epoch no.6 train no.466680  loss = 2.76660 avg_loss = 2.81863\n",
      "epoch no.6 train no.466690  loss = 3.68882 avg_loss = 2.81108\n",
      "epoch no.6 train no.466700  loss = 2.88084 avg_loss = 2.82045\n",
      "epoch no.6 train no.466710  loss = 4.24673 avg_loss = 2.82453\n",
      "epoch no.6 train no.466720  loss = 2.21583 avg_loss = 2.80375\n",
      "epoch no.6 train no.466730  loss = 3.10064 avg_loss = 2.84647\n",
      "epoch no.6 train no.466740  loss = 2.46364 avg_loss = 2.85526\n",
      "epoch no.6 train no.466750  loss = 1.69549 avg_loss = 2.88242\n",
      "epoch no.6 train no.466760  loss = 2.00497 avg_loss = 2.86351\n",
      "epoch no.6 train no.466770  loss = 2.71787 avg_loss = 2.84674\n",
      "epoch no.6 train no.466780  loss = 2.77995 avg_loss = 2.84196\n",
      "epoch no.6 train no.466790  loss = 2.20256 avg_loss = 2.84624\n",
      "epoch no.6 train no.466800  loss = 2.99324 avg_loss = 2.84733\n",
      "epoch no.6 train no.466810  loss = 2.79376 avg_loss = 2.84052\n",
      "epoch no.6 train no.466820  loss = 2.85708 avg_loss = 2.84824\n",
      "epoch no.6 train no.466830  loss = 2.69334 avg_loss = 2.85072\n",
      "epoch no.6 train no.466840  loss = 1.77774 avg_loss = 2.83169\n",
      "epoch no.6 train no.466850  loss = 1.72954 avg_loss = 2.80825\n",
      "epoch no.6 train no.466860  loss = 2.85926 avg_loss = 2.78948\n",
      "epoch no.6 train no.466870  loss = 2.36504 avg_loss = 2.81631\n",
      "epoch no.6 train no.466880  loss = 2.77617 avg_loss = 2.83695\n",
      "epoch no.6 train no.466890  loss = 2.29675 avg_loss = 2.83466\n",
      "epoch no.6 train no.466900  loss = 2.48239 avg_loss = 2.84021\n",
      "epoch no.6 train no.466910  loss = 1.84569 avg_loss = 2.83425\n",
      "epoch no.6 train no.466920  loss = 3.51147 avg_loss = 2.81836\n",
      "epoch no.6 train no.466930  loss = 2.39303 avg_loss = 2.79921\n",
      "epoch no.6 train no.466940  loss = 3.08599 avg_loss = 2.81123\n",
      "epoch no.6 train no.466950  loss = 3.59498 avg_loss = 2.83710\n",
      "epoch no.6 train no.466960  loss = 3.34562 avg_loss = 2.86204\n",
      "epoch no.6 train no.466970  loss = 3.35867 avg_loss = 2.89227\n",
      "epoch no.6 train no.466980  loss = 2.55716 avg_loss = 2.89939\n",
      "epoch no.6 train no.466990  loss = 3.36393 avg_loss = 2.90196\n",
      "epoch no.6 train no.467000  loss = 3.13666 avg_loss = 2.90326\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '곡', '들', '▁모음', '</s>']\n",
      "추억의 명곡들 모음</s>\n",
      "epoch no.6 train no.467010  loss = 2.93823 avg_loss = 2.91521\n",
      "epoch no.6 train no.467020  loss = 2.03817 avg_loss = 2.89577\n",
      "epoch no.6 train no.467030  loss = 2.17589 avg_loss = 2.89438\n",
      "epoch no.6 train no.467040  loss = 3.08699 avg_loss = 2.89671\n",
      "epoch no.6 train no.467050  loss = 4.55465 avg_loss = 2.89328\n",
      "epoch no.6 train no.467060  loss = 2.92318 avg_loss = 2.87682\n",
      "epoch no.6 train no.467070  loss = 2.65270 avg_loss = 2.84959\n",
      "epoch no.6 train no.467080  loss = 3.33365 avg_loss = 2.84566\n",
      "epoch no.6 train no.467090  loss = 3.47463 avg_loss = 2.87418\n",
      "epoch no.6 train no.467100  loss = 3.86254 avg_loss = 2.88001\n",
      "epoch no.6 train no.467110  loss = 3.27186 avg_loss = 2.86341\n",
      "epoch no.6 train no.467120  loss = 3.16252 avg_loss = 2.87058\n",
      "epoch no.6 train no.467130  loss = 2.41949 avg_loss = 2.86286\n",
      "epoch no.6 train no.467140  loss = 3.02117 avg_loss = 2.89938\n",
      "epoch no.6 train no.467150  loss = 3.16545 avg_loss = 2.87556\n",
      "epoch no.6 train no.467160  loss = 1.58800 avg_loss = 2.85436\n",
      "epoch no.6 train no.467170  loss = 2.51566 avg_loss = 2.86169\n",
      "epoch no.6 train no.467180  loss = 3.49388 avg_loss = 2.85252\n",
      "epoch no.6 train no.467190  loss = 1.83682 avg_loss = 2.81016\n",
      "epoch no.6 train no.467200  loss = 2.70645 avg_loss = 2.81940\n",
      "epoch no.6 train no.467210  loss = 2.80460 avg_loss = 2.80838\n",
      "epoch no.6 train no.467220  loss = 2.74083 avg_loss = 2.80624\n",
      "epoch no.6 train no.467230  loss = 3.27773 avg_loss = 2.82711\n",
      "epoch no.6 train no.467240  loss = 2.14165 avg_loss = 2.86057\n",
      "epoch no.6 train no.467250  loss = 2.71255 avg_loss = 2.91979\n",
      "epoch no.6 train no.467260  loss = 3.43908 avg_loss = 2.92568\n",
      "epoch no.6 train no.467270  loss = 2.58430 avg_loss = 2.91168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.467280  loss = 3.17544 avg_loss = 2.94879\n",
      "epoch no.6 train no.467290  loss = 2.53728 avg_loss = 2.95005\n",
      "epoch no.6 train no.467300  loss = 2.54957 avg_loss = 2.96393\n",
      "epoch no.6 train no.467310  loss = 2.91932 avg_loss = 2.98545\n",
      "epoch no.6 train no.467320  loss = 2.94859 avg_loss = 2.94822\n",
      "epoch no.6 train no.467330  loss = 2.78037 avg_loss = 2.91583\n",
      "epoch no.6 train no.467340  loss = 3.01549 avg_loss = 2.93257\n",
      "epoch no.6 train no.467350  loss = 2.95351 avg_loss = 2.97594\n",
      "epoch no.6 train no.467360  loss = 2.50833 avg_loss = 2.97528\n",
      "epoch no.6 train no.467370  loss = 1.66005 avg_loss = 2.95943\n",
      "epoch no.6 train no.467380  loss = 2.67289 avg_loss = 2.94764\n",
      "epoch no.6 train no.467390  loss = 4.11156 avg_loss = 2.93850\n",
      "epoch no.6 train no.467400  loss = 2.56899 avg_loss = 2.94751\n",
      "epoch no.6 train no.467410  loss = 2.97632 avg_loss = 2.95486\n",
      "epoch no.6 train no.467420  loss = 3.77356 avg_loss = 2.99954\n",
      "epoch no.6 train no.467430  loss = 3.24699 avg_loss = 2.95951\n",
      "epoch no.6 train no.467440  loss = 3.02734 avg_loss = 2.93860\n",
      "epoch no.6 train no.467450  loss = 2.30006 avg_loss = 2.95552\n",
      "epoch no.6 train no.467460  loss = 2.57130 avg_loss = 2.95959\n",
      "epoch no.6 train no.467470  loss = 2.32268 avg_loss = 2.95624\n",
      "epoch no.6 train no.467480  loss = 2.45029 avg_loss = 2.93910\n",
      "epoch no.6 train no.467490  loss = 2.39511 avg_loss = 2.92014\n",
      "epoch no.6 train no.467500  loss = 1.98045 avg_loss = 2.91348\n",
      "epoch no.6 train no.467510  loss = 2.88210 avg_loss = 2.87876\n",
      "epoch no.6 train no.467520  loss = 1.73619 avg_loss = 2.86954\n",
      "epoch no.6 train no.467530  loss = 3.17029 avg_loss = 2.86298\n",
      "epoch no.6 train no.467540  loss = 2.45496 avg_loss = 2.88715\n",
      "epoch no.6 train no.467550  loss = 2.61798 avg_loss = 2.87580\n",
      "epoch no.6 train no.467560  loss = 2.84444 avg_loss = 2.86186\n",
      "epoch no.6 train no.467570  loss = 2.51905 avg_loss = 2.87878\n",
      "epoch no.6 train no.467580  loss = 2.77533 avg_loss = 2.85067\n",
      "epoch no.6 train no.467590  loss = 1.73208 avg_loss = 2.85728\n",
      "epoch no.6 train no.467600  loss = 1.76858 avg_loss = 2.88487\n",
      "epoch no.6 train no.467610  loss = 3.03626 avg_loss = 2.89131\n",
      "epoch no.6 train no.467620  loss = 3.01645 avg_loss = 2.91767\n",
      "epoch no.6 train no.467630  loss = 2.15974 avg_loss = 2.93337\n",
      "epoch no.6 train no.467640  loss = 1.65273 avg_loss = 2.92853\n",
      "epoch no.6 train no.467650  loss = 3.49745 avg_loss = 2.93007\n",
      "epoch no.6 train no.467660  loss = 2.97003 avg_loss = 2.90670\n",
      "epoch no.6 train no.467670  loss = 2.80449 avg_loss = 2.90909\n",
      "epoch no.6 train no.467680  loss = 2.57257 avg_loss = 2.89211\n",
      "epoch no.6 train no.467690  loss = 2.40807 avg_loss = 2.89998\n",
      "epoch no.6 train no.467700  loss = 2.94664 avg_loss = 2.89347\n",
      "epoch no.6 train no.467710  loss = 3.61608 avg_loss = 2.93990\n",
      "epoch no.6 train no.467720  loss = 1.78231 avg_loss = 2.96085\n",
      "epoch no.6 train no.467730  loss = 2.49930 avg_loss = 2.96925\n",
      "epoch no.6 train no.467740  loss = 3.70863 avg_loss = 2.95025\n",
      "epoch no.6 train no.467750  loss = 3.86339 avg_loss = 2.98793\n",
      "epoch no.6 train no.467760  loss = 2.92737 avg_loss = 2.98098\n",
      "epoch no.6 train no.467770  loss = 2.48542 avg_loss = 2.96802\n",
      "epoch no.6 train no.467780  loss = 3.94373 avg_loss = 2.99354\n",
      "epoch no.6 train no.467790  loss = 2.92696 avg_loss = 3.00484\n",
      "epoch no.6 train no.467800  loss = 2.07654 avg_loss = 2.99254\n",
      "epoch no.6 train no.467810  loss = 4.17059 avg_loss = 2.98254\n",
      "epoch no.6 train no.467820  loss = 1.90691 avg_loss = 2.93512\n",
      "epoch no.6 train no.467830  loss = 2.16014 avg_loss = 2.93736\n",
      "epoch no.6 train no.467840  loss = 2.61941 avg_loss = 2.98597\n",
      "epoch no.6 train no.467850  loss = 2.32598 avg_loss = 3.00827\n",
      "epoch no.6 train no.467860  loss = 2.30973 avg_loss = 2.99471\n",
      "epoch no.6 train no.467870  loss = 3.21113 avg_loss = 2.96632\n",
      "epoch no.6 train no.467880  loss = 3.56613 avg_loss = 2.95870\n",
      "epoch no.6 train no.467890  loss = 3.26614 avg_loss = 2.95277\n",
      "epoch no.6 train no.467900  loss = 2.00593 avg_loss = 2.96323\n",
      "epoch no.6 train no.467910  loss = 3.67601 avg_loss = 2.94776\n",
      "epoch no.6 train no.467920  loss = 3.39101 avg_loss = 2.92650\n",
      "epoch no.6 train no.467930  loss = 2.89464 avg_loss = 2.92839\n",
      "epoch no.6 train no.467940  loss = 3.43651 avg_loss = 2.90965\n",
      "epoch no.6 train no.467950  loss = 3.09921 avg_loss = 2.87319\n",
      "epoch no.6 train no.467960  loss = 3.55865 avg_loss = 2.85157\n",
      "epoch no.6 train no.467970  loss = 1.93515 avg_loss = 2.83582\n",
      "epoch no.6 train no.467980  loss = 3.07472 avg_loss = 2.88061\n",
      "epoch no.6 train no.467990  loss = 3.48616 avg_loss = 2.92992\n",
      "epoch no.6 train no.468000  loss = 2.43556 avg_loss = 2.91128\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '년대', '▁댄스', '드', '</s>']\n",
      "추억의 90년대 발라드</s>\n",
      "epoch no.6 train no.468010  loss = 4.47027 avg_loss = 2.91538\n",
      "epoch no.6 train no.468020  loss = 1.94579 avg_loss = 2.90761\n",
      "epoch no.6 train no.468030  loss = 2.01376 avg_loss = 2.86010\n",
      "epoch no.6 train no.468040  loss = 3.52470 avg_loss = 2.83480\n",
      "epoch no.6 train no.468050  loss = 2.60598 avg_loss = 2.82495\n",
      "epoch no.6 train no.468060  loss = 3.55972 avg_loss = 2.82317\n",
      "epoch no.6 train no.468070  loss = 2.92745 avg_loss = 2.85297\n",
      "epoch no.6 train no.468080  loss = 2.85929 avg_loss = 2.85717\n",
      "epoch no.6 train no.468090  loss = 2.53123 avg_loss = 2.84316\n",
      "epoch no.6 train no.468100  loss = 2.11954 avg_loss = 2.81293\n",
      "epoch no.6 train no.468110  loss = 4.49724 avg_loss = 2.84706\n",
      "epoch no.6 train no.468120  loss = 2.11110 avg_loss = 2.87895\n",
      "epoch no.6 train no.468130  loss = 3.18477 avg_loss = 2.86826\n",
      "epoch no.6 train no.468140  loss = 2.83085 avg_loss = 2.86791\n",
      "epoch no.6 train no.468150  loss = 3.02281 avg_loss = 2.89524\n",
      "epoch no.6 train no.468160  loss = 3.34800 avg_loss = 2.90699\n",
      "epoch no.6 train no.468170  loss = 2.05867 avg_loss = 2.91324\n",
      "epoch no.6 train no.468180  loss = 2.60743 avg_loss = 2.93673\n",
      "epoch no.6 train no.468190  loss = 1.74170 avg_loss = 2.91180\n",
      "epoch no.6 train no.468200  loss = 3.13188 avg_loss = 2.93491\n",
      "epoch no.6 train no.468210  loss = 2.64738 avg_loss = 2.93965\n",
      "epoch no.6 train no.468220  loss = 3.26909 avg_loss = 2.93696\n",
      "epoch no.6 train no.468230  loss = 2.36548 avg_loss = 2.93991\n",
      "epoch no.6 train no.468240  loss = 2.58660 avg_loss = 2.95665\n",
      "epoch no.6 train no.468250  loss = 4.23356 avg_loss = 3.01786\n",
      "epoch no.6 train no.468260  loss = 2.52004 avg_loss = 2.99718\n",
      "epoch no.6 train no.468270  loss = 2.71927 avg_loss = 3.00131\n",
      "epoch no.6 train no.468280  loss = 2.89791 avg_loss = 3.00625\n",
      "epoch no.6 train no.468290  loss = 1.83958 avg_loss = 2.95953\n",
      "epoch no.6 train no.468300  loss = 2.11394 avg_loss = 2.95704\n",
      "epoch no.6 train no.468310  loss = 3.16405 avg_loss = 2.96747\n",
      "epoch no.6 train no.468320  loss = 3.16793 avg_loss = 2.99316\n",
      "epoch no.6 train no.468330  loss = 4.91088 avg_loss = 3.02338\n",
      "epoch no.6 train no.468340  loss = 3.99094 avg_loss = 3.01313\n",
      "epoch no.6 train no.468350  loss = 2.51788 avg_loss = 2.96822\n",
      "epoch no.6 train no.468360  loss = 3.35484 avg_loss = 2.94897\n",
      "epoch no.6 train no.468370  loss = 2.85055 avg_loss = 2.92545\n",
      "epoch no.6 train no.468380  loss = 3.77230 avg_loss = 2.92441\n",
      "epoch no.6 train no.468390  loss = 4.35951 avg_loss = 2.90558\n",
      "epoch no.6 train no.468400  loss = 2.22374 avg_loss = 2.87624\n",
      "epoch no.6 train no.468410  loss = 2.71688 avg_loss = 2.89797\n",
      "epoch no.6 train no.468420  loss = 2.66036 avg_loss = 2.90655\n",
      "epoch no.6 train no.468430  loss = 4.27971 avg_loss = 2.95115\n",
      "epoch no.6 train no.468440  loss = 5.23396 avg_loss = 2.99741\n",
      "epoch no.6 train no.468450  loss = 3.23547 avg_loss = 2.94647\n",
      "epoch no.6 train no.468460  loss = 2.11344 avg_loss = 2.96187\n",
      "epoch no.6 train no.468470  loss = 2.82340 avg_loss = 2.99479\n",
      "epoch no.6 train no.468480  loss = 2.48966 avg_loss = 2.98363\n",
      "epoch no.6 train no.468490  loss = 3.68586 avg_loss = 2.98502\n",
      "epoch no.6 train no.468500  loss = 3.13777 avg_loss = 2.99440\n",
      "epoch no.6 train no.468510  loss = 2.23633 avg_loss = 2.96888\n",
      "epoch no.6 train no.468520  loss = 3.94335 avg_loss = 2.98462\n",
      "epoch no.6 train no.468530  loss = 2.65252 avg_loss = 2.98893\n",
      "epoch no.6 train no.468540  loss = 2.93292 avg_loss = 3.02997\n",
      "epoch no.6 train no.468550  loss = 2.92326 avg_loss = 3.02760\n",
      "epoch no.6 train no.468560  loss = 2.74440 avg_loss = 2.99737\n",
      "epoch no.6 train no.468570  loss = 3.27595 avg_loss = 2.99224\n",
      "epoch no.6 train no.468580  loss = 2.73360 avg_loss = 3.01345\n",
      "epoch no.6 train no.468590  loss = 3.30472 avg_loss = 3.05457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.468600  loss = 3.69828 avg_loss = 3.05404\n",
      "epoch no.6 train no.468610  loss = 4.84165 avg_loss = 3.04834\n",
      "epoch no.6 train no.468620  loss = 3.52506 avg_loss = 3.02801\n",
      "epoch no.6 train no.468630  loss = 3.36936 avg_loss = 3.04082\n",
      "epoch no.6 train no.468640  loss = 2.35024 avg_loss = 3.02958\n",
      "epoch no.6 train no.468650  loss = 2.41019 avg_loss = 2.99449\n",
      "epoch no.6 train no.468660  loss = 2.34561 avg_loss = 2.98099\n",
      "epoch no.6 train no.468670  loss = 2.55111 avg_loss = 2.94154\n",
      "epoch no.6 train no.468680  loss = 2.54095 avg_loss = 2.93835\n",
      "epoch no.6 train no.468690  loss = 2.33955 avg_loss = 2.91427\n",
      "epoch no.6 train no.468700  loss = 3.43841 avg_loss = 2.92702\n",
      "epoch no.6 train no.468710  loss = 2.61180 avg_loss = 2.92396\n",
      "epoch no.6 train no.468720  loss = 3.13119 avg_loss = 2.91176\n",
      "epoch no.6 train no.468730  loss = 3.99148 avg_loss = 2.89925\n",
      "epoch no.6 train no.468740  loss = 1.96314 avg_loss = 2.89143\n",
      "epoch no.6 train no.468750  loss = 2.84760 avg_loss = 2.88896\n",
      "epoch no.6 train no.468760  loss = 3.10202 avg_loss = 2.88830\n",
      "epoch no.6 train no.468770  loss = 3.50466 avg_loss = 2.87916\n",
      "epoch no.6 train no.468780  loss = 2.55457 avg_loss = 2.90998\n",
      "epoch no.6 train no.468790  loss = 4.18668 avg_loss = 2.92055\n",
      "epoch no.6 train no.468800  loss = 2.54722 avg_loss = 2.90316\n",
      "epoch no.6 train no.468810  loss = 2.60515 avg_loss = 2.85504\n",
      "epoch no.6 train no.468820  loss = 3.19840 avg_loss = 2.83179\n",
      "epoch no.6 train no.468830  loss = 2.87427 avg_loss = 2.79840\n",
      "epoch no.6 train no.468840  loss = 5.01411 avg_loss = 2.82297\n",
      "epoch no.6 train no.468850  loss = 2.35345 avg_loss = 2.82422\n",
      "epoch no.6 train no.468860  loss = 2.92003 avg_loss = 2.83312\n",
      "epoch no.6 train no.468870  loss = 2.09826 avg_loss = 2.85050\n",
      "epoch no.6 train no.468880  loss = 3.02525 avg_loss = 2.82839\n",
      "epoch no.6 train no.468890  loss = 3.90492 avg_loss = 2.84483\n",
      "epoch no.6 train no.468900  loss = 3.26515 avg_loss = 2.81797\n",
      "epoch no.6 train no.468910  loss = 4.28944 avg_loss = 2.80614\n",
      "epoch no.6 train no.468920  loss = 3.21478 avg_loss = 2.80536\n",
      "epoch no.6 train no.468930  loss = 3.92031 avg_loss = 2.80233\n",
      "epoch no.6 train no.468940  loss = 3.53758 avg_loss = 2.84703\n",
      "epoch no.6 train no.468950  loss = 1.46038 avg_loss = 2.87434\n",
      "epoch no.6 train no.468960  loss = 2.55645 avg_loss = 2.88724\n",
      "epoch no.6 train no.468970  loss = 2.82102 avg_loss = 2.87814\n",
      "epoch no.6 train no.468980  loss = 2.52868 avg_loss = 2.86365\n",
      "epoch no.6 train no.468990  loss = 1.88520 avg_loss = 2.83745\n",
      "epoch no.6 train no.469000  loss = 3.58244 avg_loss = 2.83091\n",
      "7\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁', '드', '</s>']\n",
      "추억의 2000년대\n",
      "\n",
      "발라드</s>\n",
      "epoch no.6 train no.469010  loss = 3.45973 avg_loss = 2.85579\n",
      "epoch no.6 train no.469020  loss = 4.57120 avg_loss = 2.86920\n",
      "epoch no.6 train no.469030  loss = 2.78757 avg_loss = 2.87061\n",
      "epoch no.6 train no.469040  loss = 3.10420 avg_loss = 2.87338\n",
      "epoch no.6 train no.469050  loss = 2.46191 avg_loss = 2.86842\n",
      "epoch no.6 train no.469060  loss = 2.57854 avg_loss = 2.87867\n",
      "epoch no.6 train no.469070  loss = 2.91328 avg_loss = 2.88205\n",
      "epoch no.6 train no.469080  loss = 3.12582 avg_loss = 2.91371\n",
      "epoch no.6 train no.469090  loss = 3.03997 avg_loss = 2.88859\n",
      "epoch no.6 train no.469100  loss = 3.34257 avg_loss = 2.89780\n",
      "epoch no.6 train no.469110  loss = 2.34056 avg_loss = 2.89687\n",
      "epoch no.6 train no.469120  loss = 2.87617 avg_loss = 2.90353\n",
      "epoch no.6 train no.469130  loss = 2.55797 avg_loss = 2.92327\n",
      "epoch no.6 train no.469140  loss = 3.66643 avg_loss = 2.93512\n",
      "epoch no.6 train no.469150  loss = 2.44703 avg_loss = 2.95510\n",
      "epoch no.6 train no.469160  loss = 3.77870 avg_loss = 2.96923\n",
      "epoch no.6 train no.469170  loss = 1.98201 avg_loss = 2.98142\n",
      "epoch no.6 train no.469180  loss = 2.19674 avg_loss = 2.97070\n",
      "epoch no.6 train no.469190  loss = 3.33701 avg_loss = 2.98856\n",
      "epoch no.6 train no.469200  loss = 2.08076 avg_loss = 3.01834\n",
      "epoch no.6 train no.469210  loss = 2.58745 avg_loss = 3.00382\n",
      "epoch no.6 train no.469220  loss = 3.05205 avg_loss = 2.97559\n",
      "epoch no.6 train no.469230  loss = 2.68993 avg_loss = 2.99044\n",
      "epoch no.6 train no.469240  loss = 2.72021 avg_loss = 3.00520\n",
      "epoch no.6 train no.469250  loss = 5.10997 avg_loss = 3.00526\n",
      "epoch no.6 train no.469260  loss = 1.86645 avg_loss = 2.95515\n",
      "epoch no.6 train no.469270  loss = 3.46116 avg_loss = 2.92085\n",
      "epoch no.6 train no.469280  loss = 2.93844 avg_loss = 2.95659\n",
      "epoch no.6 train no.469290  loss = 2.30569 avg_loss = 2.94770\n",
      "epoch no.6 train no.469300  loss = 2.47003 avg_loss = 2.97134\n",
      "epoch no.6 train no.469310  loss = 3.20777 avg_loss = 2.98342\n",
      "epoch no.6 train no.469320  loss = 2.64752 avg_loss = 2.96757\n",
      "epoch no.6 train no.469330  loss = 4.32344 avg_loss = 2.98627\n",
      "epoch no.6 train no.469340  loss = 3.63766 avg_loss = 2.95909\n",
      "epoch no.6 train no.469350  loss = 3.48082 avg_loss = 2.96358\n",
      "epoch no.6 train no.469360  loss = 2.68242 avg_loss = 2.92399\n",
      "epoch no.6 train no.469370  loss = 2.59519 avg_loss = 2.93817\n",
      "epoch no.6 train no.469380  loss = 2.96296 avg_loss = 2.92707\n",
      "epoch no.6 train no.469390  loss = 2.09860 avg_loss = 2.91054\n",
      "epoch no.6 train no.469400  loss = 3.31545 avg_loss = 2.89419\n",
      "epoch no.6 train no.469410  loss = 2.18021 avg_loss = 2.87969\n",
      "epoch no.6 train no.469420  loss = 3.15239 avg_loss = 2.88642\n",
      "epoch no.6 train no.469430  loss = 3.03812 avg_loss = 2.90199\n",
      "epoch no.6 train no.469440  loss = 2.04821 avg_loss = 2.87349\n",
      "epoch no.6 train no.469450  loss = 4.62247 avg_loss = 2.91018\n",
      "epoch no.6 train no.469460  loss = 3.10741 avg_loss = 2.88987\n",
      "epoch no.6 train no.469470  loss = 2.77315 avg_loss = 2.89146\n",
      "epoch no.6 train no.469480  loss = 2.39154 avg_loss = 2.88896\n",
      "epoch no.6 train no.469490  loss = 3.89254 avg_loss = 2.92715\n",
      "epoch no.6 train no.469500  loss = 2.84132 avg_loss = 2.96959\n",
      "epoch no.6 train no.469510  loss = 1.34485 avg_loss = 2.90328\n",
      "epoch no.6 train no.469520  loss = 2.17645 avg_loss = 2.87140\n",
      "epoch no.6 train no.469530  loss = 2.01957 avg_loss = 2.82976\n",
      "epoch no.6 train no.469540  loss = 3.58201 avg_loss = 2.81046\n",
      "epoch no.6 train no.469550  loss = 3.82227 avg_loss = 2.85038\n",
      "epoch no.6 train no.469560  loss = 3.33327 avg_loss = 2.83615\n",
      "epoch no.6 train no.469570  loss = 2.82655 avg_loss = 2.89489\n",
      "epoch no.6 train no.469580  loss = 2.14790 avg_loss = 2.90716\n",
      "epoch no.6 train no.469590  loss = 3.60282 avg_loss = 2.92364\n",
      "epoch no.6 train no.469600  loss = 2.50042 avg_loss = 2.92208\n",
      "epoch no.6 train no.469610  loss = 3.50662 avg_loss = 2.93049\n",
      "epoch no.6 train no.469620  loss = 3.61399 avg_loss = 2.93716\n",
      "epoch no.6 train no.469630  loss = 2.87961 avg_loss = 2.91904\n",
      "epoch no.6 train no.469640  loss = 3.58878 avg_loss = 2.94056\n",
      "epoch no.6 train no.469650  loss = 4.31132 avg_loss = 2.97426\n",
      "epoch no.6 train no.469660  loss = 1.99325 avg_loss = 2.97520\n",
      "epoch no.6 train no.469670  loss = 2.70309 avg_loss = 2.98211\n",
      "epoch no.6 train no.469680  loss = 4.63842 avg_loss = 2.95479\n",
      "epoch no.6 train no.469690  loss = 3.64154 avg_loss = 2.96284\n",
      "epoch no.6 train no.469700  loss = 3.16796 avg_loss = 2.93435\n",
      "epoch no.6 train no.469710  loss = 3.10631 avg_loss = 2.97837\n",
      "epoch no.6 train no.469720  loss = 2.58336 avg_loss = 2.97915\n",
      "epoch no.6 train no.469730  loss = 3.15799 avg_loss = 2.97147\n",
      "epoch no.6 train no.469740  loss = 1.58451 avg_loss = 2.97062\n",
      "epoch no.6 train no.469750  loss = 2.49942 avg_loss = 2.95113\n",
      "epoch no.6 train no.469760  loss = 2.96370 avg_loss = 2.96419\n",
      "epoch no.6 train no.469770  loss = 2.94522 avg_loss = 2.99517\n",
      "epoch no.6 train no.469780  loss = 1.50945 avg_loss = 2.94497\n",
      "epoch no.6 train no.469790  loss = 1.65005 avg_loss = 2.91415\n",
      "epoch no.6 train no.469800  loss = 2.91240 avg_loss = 2.89662\n",
      "epoch no.6 train no.469810  loss = 2.42984 avg_loss = 2.88145\n",
      "epoch no.6 train no.469820  loss = 3.50046 avg_loss = 2.89520\n",
      "epoch no.6 train no.469830  loss = 4.09462 avg_loss = 2.96737\n",
      "epoch no.6 train no.469840  loss = 1.37974 avg_loss = 2.96228\n",
      "epoch no.6 train no.469850  loss = 3.25977 avg_loss = 2.97221\n",
      "epoch no.6 train no.469860  loss = 2.02842 avg_loss = 2.95071\n",
      "epoch no.6 train no.469870  loss = 2.55170 avg_loss = 2.93015\n",
      "epoch no.6 train no.469880  loss = 2.11201 avg_loss = 2.94297\n",
      "epoch no.6 train no.469890  loss = 3.56482 avg_loss = 2.93856\n",
      "epoch no.6 train no.469900  loss = 3.35554 avg_loss = 2.93484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.469910  loss = 2.73809 avg_loss = 2.94391\n",
      "epoch no.6 train no.469920  loss = 2.74191 avg_loss = 2.97449\n",
      "epoch no.6 train no.469930  loss = 3.93070 avg_loss = 2.94125\n",
      "epoch no.6 train no.469940  loss = 3.68101 avg_loss = 2.95790\n",
      "epoch no.6 train no.469950  loss = 2.84867 avg_loss = 2.96301\n",
      "epoch no.6 train no.469960  loss = 2.88646 avg_loss = 2.93108\n",
      "epoch no.6 train no.469970  loss = 4.00278 avg_loss = 2.95150\n",
      "epoch no.6 train no.469980  loss = 2.73252 avg_loss = 2.92073\n",
      "epoch no.6 train no.469990  loss = 3.09484 avg_loss = 2.94426\n",
      "epoch no.6 train no.470000  loss = 3.03373 avg_loss = 2.90136\n",
      "6\n",
      "to_tokens: ['▁비', '▁90', 'p', 'op', '▁베스트', '곡', '▁모음', '</s>']\n",
      "추억의 jpop 명곡 모음</s>\n",
      "epoch no.6 train no.470010  loss = 1.76896 avg_loss = 2.87726\n",
      "epoch no.6 train no.470020  loss = 2.99566 avg_loss = 2.90065\n",
      "epoch no.6 train no.470030  loss = 3.42842 avg_loss = 2.89323\n",
      "epoch no.6 train no.470040  loss = 3.20696 avg_loss = 2.87193\n",
      "epoch no.6 train no.470050  loss = 2.87191 avg_loss = 2.83761\n",
      "epoch no.6 train no.470060  loss = 2.63381 avg_loss = 2.85815\n",
      "epoch no.6 train no.470070  loss = 3.08219 avg_loss = 2.90338\n",
      "epoch no.6 train no.470080  loss = 3.78439 avg_loss = 2.90092\n",
      "epoch no.6 train no.470090  loss = 2.76460 avg_loss = 2.90415\n",
      "epoch no.6 train no.470100  loss = 2.72127 avg_loss = 2.95551\n",
      "epoch no.6 train no.470110  loss = 3.38950 avg_loss = 2.92985\n",
      "epoch no.6 train no.470120  loss = 2.23156 avg_loss = 2.90130\n",
      "epoch no.6 train no.470130  loss = 4.83313 avg_loss = 2.91600\n",
      "epoch no.6 train no.470140  loss = 2.66584 avg_loss = 2.95515\n",
      "epoch no.6 train no.470150  loss = 3.31604 avg_loss = 2.94416\n",
      "epoch no.6 train no.470160  loss = 2.53582 avg_loss = 2.94217\n",
      "epoch no.6 train no.470170  loss = 2.44832 avg_loss = 2.94583\n",
      "epoch no.6 train no.470180  loss = 1.93567 avg_loss = 2.92071\n",
      "epoch no.6 train no.470190  loss = 2.95273 avg_loss = 2.89863\n",
      "epoch no.6 train no.470200  loss = 2.81424 avg_loss = 2.87728\n",
      "epoch no.6 train no.470210  loss = 3.67479 avg_loss = 2.91249\n",
      "epoch no.6 train no.470220  loss = 2.56123 avg_loss = 2.90941\n",
      "epoch no.6 train no.470230  loss = 2.01443 avg_loss = 2.90128\n",
      "epoch no.6 train no.470240  loss = 2.02570 avg_loss = 2.89950\n",
      "epoch no.6 train no.470250  loss = 2.61284 avg_loss = 2.87975\n",
      "epoch no.6 train no.470260  loss = 2.06344 avg_loss = 2.85100\n",
      "epoch no.6 train no.470270  loss = 1.92309 avg_loss = 2.82195\n",
      "epoch no.6 train no.470280  loss = 2.71129 avg_loss = 2.83840\n",
      "epoch no.6 train no.470290  loss = 1.83081 avg_loss = 2.81422\n",
      "epoch no.6 train no.470300  loss = 2.55211 avg_loss = 2.82809\n",
      "epoch no.6 train no.470310  loss = 2.68852 avg_loss = 2.85407\n",
      "epoch no.6 train no.470320  loss = 2.50591 avg_loss = 2.84979\n",
      "epoch no.6 train no.470330  loss = 2.27899 avg_loss = 2.81812\n",
      "epoch no.6 train no.470340  loss = 2.03643 avg_loss = 2.82150\n",
      "epoch no.6 train no.470350  loss = 3.94252 avg_loss = 2.84713\n",
      "epoch no.6 train no.470360  loss = 4.29674 avg_loss = 2.87217\n",
      "epoch no.6 train no.470370  loss = 1.67304 avg_loss = 2.85148\n",
      "epoch no.6 train no.470380  loss = 2.59936 avg_loss = 2.87509\n",
      "epoch no.6 train no.470390  loss = 3.56998 avg_loss = 2.84930\n",
      "epoch no.6 train no.470400  loss = 3.79496 avg_loss = 2.85111\n",
      "epoch no.6 train no.470410  loss = 1.80667 avg_loss = 2.88832\n",
      "epoch no.6 train no.470420  loss = 2.12529 avg_loss = 2.88407\n",
      "epoch no.6 train no.470430  loss = 3.24413 avg_loss = 2.88042\n",
      "epoch no.6 train no.470440  loss = 2.90115 avg_loss = 2.90497\n",
      "epoch no.6 train no.470450  loss = 2.88241 avg_loss = 2.92166\n",
      "epoch no.6 train no.470460  loss = 4.82063 avg_loss = 2.95414\n",
      "epoch no.6 train no.470470  loss = 3.08394 avg_loss = 2.97783\n",
      "epoch no.6 train no.470480  loss = 3.43152 avg_loss = 2.98305\n",
      "epoch no.6 train no.470490  loss = 3.90453 avg_loss = 2.98773\n",
      "epoch no.6 train no.470500  loss = 2.64346 avg_loss = 2.97008\n",
      "epoch no.6 train no.470510  loss = 2.98065 avg_loss = 2.93282\n",
      "epoch no.6 train no.470520  loss = 3.04595 avg_loss = 2.92987\n",
      "epoch no.6 train no.470530  loss = 2.05146 avg_loss = 2.91468\n",
      "epoch no.6 train no.470540  loss = 3.12921 avg_loss = 2.91283\n",
      "epoch no.6 train no.470550  loss = 2.26551 avg_loss = 2.89970\n",
      "epoch no.6 train no.470560  loss = 1.77687 avg_loss = 2.86193\n",
      "epoch no.6 train no.470570  loss = 3.19139 avg_loss = 2.86156\n",
      "epoch no.6 train no.470580  loss = 1.91041 avg_loss = 2.88643\n",
      "epoch no.6 train no.470590  loss = 2.93204 avg_loss = 2.92690\n",
      "epoch no.6 train no.470600  loss = 2.37042 avg_loss = 2.97626\n",
      "epoch no.6 train no.470610  loss = 2.40971 avg_loss = 2.95606\n",
      "epoch no.6 train no.470620  loss = 3.41972 avg_loss = 2.93263\n",
      "epoch no.6 train no.470630  loss = 4.78852 avg_loss = 2.97241\n",
      "epoch no.6 train no.470640  loss = 3.94643 avg_loss = 2.96805\n",
      "epoch no.6 train no.470650  loss = 2.67946 avg_loss = 2.97462\n",
      "epoch no.6 train no.470660  loss = 2.64120 avg_loss = 2.94022\n",
      "epoch no.6 train no.470670  loss = 2.20414 avg_loss = 2.94176\n",
      "epoch no.6 train no.470680  loss = 3.22093 avg_loss = 2.94460\n",
      "epoch no.6 train no.470690  loss = 3.85294 avg_loss = 2.94669\n",
      "epoch no.6 train no.470700  loss = 3.57474 avg_loss = 2.97291\n",
      "epoch no.6 train no.470710  loss = 2.89274 avg_loss = 2.93726\n",
      "epoch no.6 train no.470720  loss = 2.09664 avg_loss = 2.93710\n",
      "epoch no.6 train no.470730  loss = 2.51274 avg_loss = 2.93710\n",
      "epoch no.6 train no.470740  loss = 2.25568 avg_loss = 2.92533\n",
      "epoch no.6 train no.470750  loss = 2.93494 avg_loss = 2.91022\n",
      "epoch no.6 train no.470760  loss = 2.12999 avg_loss = 2.88768\n",
      "epoch no.6 train no.470770  loss = 3.48131 avg_loss = 2.88879\n",
      "epoch no.6 train no.470780  loss = 1.98254 avg_loss = 2.84067\n",
      "epoch no.6 train no.470790  loss = 3.09981 avg_loss = 2.84921\n",
      "epoch no.6 train no.470800  loss = 4.09150 avg_loss = 2.87760\n",
      "epoch no.6 train no.470810  loss = 2.91376 avg_loss = 2.88736\n",
      "epoch no.6 train no.470820  loss = 3.74470 avg_loss = 2.91246\n",
      "epoch no.6 train no.470830  loss = 2.57817 avg_loss = 2.90027\n",
      "epoch no.6 train no.470840  loss = 2.35330 avg_loss = 2.87502\n",
      "epoch no.6 train no.470850  loss = 2.75682 avg_loss = 2.87192\n",
      "epoch no.6 train no.470860  loss = 4.35830 avg_loss = 2.89142\n",
      "epoch no.6 train no.470870  loss = 3.09902 avg_loss = 2.88816\n",
      "epoch no.6 train no.470880  loss = 2.60206 avg_loss = 2.89800\n",
      "epoch no.6 train no.470890  loss = 2.51600 avg_loss = 2.88630\n",
      "epoch no.6 train no.470900  loss = 1.80784 avg_loss = 2.89159\n",
      "epoch no.6 train no.470910  loss = 2.44100 avg_loss = 2.88741\n",
      "epoch no.6 train no.470920  loss = 2.22804 avg_loss = 2.92418\n",
      "epoch no.6 train no.470930  loss = 2.13018 avg_loss = 2.96408\n",
      "epoch no.6 train no.470940  loss = 1.98873 avg_loss = 2.93611\n",
      "epoch no.6 train no.470950  loss = 3.43999 avg_loss = 2.91951\n",
      "epoch no.6 train no.470960  loss = 2.81622 avg_loss = 2.87920\n",
      "epoch no.6 train no.470970  loss = 1.84644 avg_loss = 2.87887\n",
      "epoch no.6 train no.470980  loss = 3.10566 avg_loss = 2.90983\n",
      "epoch no.6 train no.470990  loss = 3.39576 avg_loss = 2.94270\n",
      "epoch no.6 train no.471000  loss = 3.10563 avg_loss = 2.95360\n",
      "4\n",
      "to_tokens: ['▁비', '▁명', '년대', '▁댄', '곡', '</s>']\n",
      "추억의 90년대 댄스곡</s>\n",
      "epoch no.6 train no.471010  loss = 4.19530 avg_loss = 3.00337\n",
      "epoch no.6 train no.471020  loss = 2.76529 avg_loss = 2.95865\n",
      "epoch no.6 train no.471030  loss = 2.66866 avg_loss = 2.94073\n",
      "epoch no.6 train no.471040  loss = 4.05863 avg_loss = 2.95320\n",
      "epoch no.6 train no.471050  loss = 2.59304 avg_loss = 2.92676\n",
      "epoch no.6 train no.471060  loss = 2.73440 avg_loss = 2.92868\n",
      "epoch no.6 train no.471070  loss = 3.57383 avg_loss = 2.91983\n",
      "epoch no.6 train no.471080  loss = 3.08163 avg_loss = 2.90377\n",
      "epoch no.6 train no.471090  loss = 4.16881 avg_loss = 2.95513\n",
      "epoch no.6 train no.471100  loss = 4.23360 avg_loss = 2.96662\n",
      "epoch no.6 train no.471110  loss = 3.06831 avg_loss = 2.96358\n",
      "epoch no.6 train no.471120  loss = 3.33487 avg_loss = 2.93894\n",
      "epoch no.6 train no.471130  loss = 2.88795 avg_loss = 2.96916\n",
      "epoch no.6 train no.471140  loss = 2.01785 avg_loss = 2.92025\n",
      "epoch no.6 train no.471150  loss = 2.51839 avg_loss = 2.91706\n",
      "epoch no.6 train no.471160  loss = 3.70500 avg_loss = 2.92133\n",
      "epoch no.6 train no.471170  loss = 3.60156 avg_loss = 2.90611\n",
      "epoch no.6 train no.471180  loss = 1.88109 avg_loss = 2.87242\n",
      "epoch no.6 train no.471190  loss = 2.51022 avg_loss = 2.86933\n",
      "epoch no.6 train no.471200  loss = 2.34463 avg_loss = 2.90003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.471210  loss = 3.74943 avg_loss = 2.93241\n",
      "epoch no.6 train no.471220  loss = 2.67106 avg_loss = 2.91705\n",
      "epoch no.6 train no.471230  loss = 1.70875 avg_loss = 2.87240\n",
      "epoch no.6 train no.471240  loss = 2.01551 avg_loss = 2.87774\n",
      "epoch no.6 train no.471250  loss = 2.23579 avg_loss = 2.88483\n",
      "epoch no.6 train no.471260  loss = 3.49417 avg_loss = 2.89102\n",
      "epoch no.6 train no.471270  loss = 2.90815 avg_loss = 2.86944\n",
      "epoch no.6 train no.471280  loss = 1.79390 avg_loss = 2.88274\n",
      "epoch no.6 train no.471290  loss = 4.36763 avg_loss = 2.88729\n",
      "epoch no.6 train no.471300  loss = 2.81279 avg_loss = 2.85950\n",
      "epoch no.6 train no.471310  loss = 3.03009 avg_loss = 2.84727\n",
      "epoch no.6 train no.471320  loss = 2.23164 avg_loss = 2.84559\n",
      "epoch no.6 train no.471330  loss = 2.28217 avg_loss = 2.83693\n",
      "epoch no.6 train no.471340  loss = 2.29292 avg_loss = 2.83152\n",
      "epoch no.6 train no.471350  loss = 2.55148 avg_loss = 2.81082\n",
      "epoch no.6 train no.471360  loss = 3.89493 avg_loss = 2.85144\n",
      "epoch no.6 train no.471370  loss = 4.12845 avg_loss = 2.83122\n",
      "epoch no.6 train no.471380  loss = 2.86762 avg_loss = 2.85545\n",
      "epoch no.6 train no.471390  loss = 3.33315 avg_loss = 2.86417\n",
      "epoch no.6 train no.471400  loss = 2.44401 avg_loss = 2.85643\n",
      "epoch no.6 train no.471410  loss = 3.95220 avg_loss = 2.87639\n",
      "epoch no.6 train no.471420  loss = 2.95095 avg_loss = 2.83642\n",
      "epoch no.6 train no.471430  loss = 2.66325 avg_loss = 2.87437\n",
      "epoch no.6 train no.471440  loss = 3.95686 avg_loss = 2.85995\n",
      "epoch no.6 train no.471450  loss = 2.36816 avg_loss = 2.82232\n",
      "epoch no.6 train no.471460  loss = 2.68707 avg_loss = 2.82080\n",
      "epoch no.6 train no.471470  loss = 2.81589 avg_loss = 2.82791\n",
      "epoch no.6 train no.471480  loss = 2.70853 avg_loss = 2.83606\n",
      "epoch no.6 train no.471490  loss = 1.88076 avg_loss = 2.84053\n",
      "epoch no.6 train no.471500  loss = 3.25762 avg_loss = 2.82508\n",
      "epoch no.6 train no.471510  loss = 2.51042 avg_loss = 2.82107\n",
      "epoch no.6 train no.471520  loss = 2.92784 avg_loss = 2.87261\n",
      "epoch no.6 train no.471530  loss = 2.79103 avg_loss = 2.87278\n",
      "epoch no.6 train no.471540  loss = 2.53659 avg_loss = 2.89109\n",
      "epoch no.6 train no.471550  loss = 2.85114 avg_loss = 2.87949\n",
      "epoch no.6 train no.471560  loss = 3.23700 avg_loss = 2.91312\n",
      "epoch no.6 train no.471570  loss = 2.47120 avg_loss = 2.92116\n",
      "epoch no.6 train no.471580  loss = 2.29508 avg_loss = 2.86954\n",
      "epoch no.6 train no.471590  loss = 2.33057 avg_loss = 2.83633\n",
      "epoch no.6 train no.471600  loss = 3.85208 avg_loss = 2.83909\n",
      "epoch no.6 train no.471610  loss = 2.47912 avg_loss = 2.82123\n",
      "epoch no.6 train no.471620  loss = 2.95101 avg_loss = 2.82064\n",
      "epoch no.6 train no.471630  loss = 2.58310 avg_loss = 2.83210\n",
      "epoch no.6 train no.471640  loss = 2.94870 avg_loss = 2.85435\n",
      "epoch no.6 train no.471650  loss = 2.38701 avg_loss = 2.86493\n",
      "epoch no.6 train no.471660  loss = 3.55854 avg_loss = 2.88461\n",
      "epoch no.6 train no.471670  loss = 3.25058 avg_loss = 2.84252\n",
      "epoch no.6 train no.471680  loss = 2.32180 avg_loss = 2.82859\n",
      "epoch no.6 train no.471690  loss = 3.16564 avg_loss = 2.83028\n",
      "epoch no.6 train no.471700  loss = 2.23408 avg_loss = 2.83525\n",
      "epoch no.6 train no.471710  loss = 2.62547 avg_loss = 2.84888\n",
      "epoch no.6 train no.471720  loss = 3.13410 avg_loss = 2.87256\n",
      "epoch no.6 train no.471730  loss = 2.70030 avg_loss = 2.89385\n",
      "epoch no.6 train no.471740  loss = 4.04791 avg_loss = 2.86850\n",
      "epoch no.6 train no.471750  loss = 3.02804 avg_loss = 2.86613\n",
      "epoch no.6 train no.471760  loss = 2.31098 avg_loss = 2.87032\n",
      "epoch no.6 train no.471770  loss = 3.01281 avg_loss = 2.84539\n",
      "epoch no.6 train no.471780  loss = 2.98904 avg_loss = 2.86083\n",
      "epoch no.6 train no.471790  loss = 3.57439 avg_loss = 2.83927\n",
      "epoch no.6 train no.471800  loss = 3.12912 avg_loss = 2.84053\n",
      "epoch no.6 train no.471810  loss = 3.72959 avg_loss = 2.83812\n",
      "epoch no.6 train no.471820  loss = 2.91236 avg_loss = 2.84411\n",
      "epoch no.6 train no.471830  loss = 3.67547 avg_loss = 2.86568\n",
      "epoch no.6 train no.471840  loss = 3.89749 avg_loss = 2.88537\n",
      "epoch no.6 train no.471850  loss = 4.81260 avg_loss = 2.93456\n",
      "epoch no.6 train no.471860  loss = 2.77994 avg_loss = 2.98943\n",
      "epoch no.6 train no.471870  loss = 2.45079 avg_loss = 2.97276\n",
      "epoch no.6 train no.471880  loss = 3.82927 avg_loss = 2.96854\n",
      "epoch no.6 train no.471890  loss = 3.90867 avg_loss = 2.98205\n",
      "epoch no.6 train no.471900  loss = 3.33513 avg_loss = 2.96206\n",
      "epoch no.6 train no.471910  loss = 1.91701 avg_loss = 2.92391\n",
      "epoch no.6 train no.471920  loss = 1.96559 avg_loss = 2.92984\n",
      "epoch no.6 train no.471930  loss = 3.55163 avg_loss = 2.95859\n",
      "epoch no.6 train no.471940  loss = 3.69517 avg_loss = 2.99455\n",
      "epoch no.6 train no.471950  loss = 3.61933 avg_loss = 3.00356\n",
      "epoch no.6 train no.471960  loss = 4.29089 avg_loss = 3.01369\n",
      "epoch no.6 train no.471970  loss = 3.34196 avg_loss = 2.98381\n",
      "epoch no.6 train no.471980  loss = 2.33931 avg_loss = 2.95198\n",
      "epoch no.6 train no.471990  loss = 2.51990 avg_loss = 2.93525\n",
      "epoch no.6 train no.472000  loss = 2.41807 avg_loss = 2.95156\n",
      "4\n",
      "to_tokens: ['▁가을', '▁90', '년대', '▁발라', '드', '</s>']\n",
      "추억의 2000년대 발라드</s>\n",
      "epoch no.6 train no.472010  loss = 2.80720 avg_loss = 2.92822\n",
      "epoch no.6 train no.472020  loss = 3.62838 avg_loss = 2.96943\n",
      "epoch no.6 train no.472030  loss = 2.81103 avg_loss = 2.97051\n",
      "epoch no.6 train no.472040  loss = 1.70394 avg_loss = 2.93895\n",
      "epoch no.6 train no.472050  loss = 2.84479 avg_loss = 2.92973\n",
      "epoch no.6 train no.472060  loss = 2.18396 avg_loss = 2.94202\n",
      "epoch no.6 train no.472070  loss = 3.22872 avg_loss = 3.02789\n",
      "epoch no.6 train no.472080  loss = 2.48462 avg_loss = 3.00071\n",
      "epoch no.6 train no.472090  loss = 1.84203 avg_loss = 2.98355\n",
      "epoch no.6 train no.472100  loss = 2.23303 avg_loss = 2.96456\n",
      "epoch no.6 train no.472110  loss = 3.41134 avg_loss = 2.98053\n",
      "epoch no.6 train no.472120  loss = 2.27097 avg_loss = 3.00374\n",
      "epoch no.6 train no.472130  loss = 2.60500 avg_loss = 2.99948\n",
      "epoch no.6 train no.472140  loss = 3.19356 avg_loss = 3.00991\n",
      "epoch no.6 train no.472150  loss = 2.01578 avg_loss = 2.97181\n",
      "epoch no.6 train no.472160  loss = 2.89369 avg_loss = 2.97829\n",
      "epoch no.6 train no.472170  loss = 1.68187 avg_loss = 2.96015\n",
      "epoch no.6 train no.472180  loss = 2.70435 avg_loss = 2.95088\n",
      "epoch no.6 train no.472190  loss = 2.44894 avg_loss = 2.92228\n",
      "epoch no.6 train no.472200  loss = 3.61281 avg_loss = 2.94648\n",
      "epoch no.6 train no.472210  loss = 2.35298 avg_loss = 2.91859\n",
      "epoch no.6 train no.472220  loss = 2.71978 avg_loss = 2.89598\n",
      "epoch no.6 train no.472230  loss = 1.38072 avg_loss = 2.90115\n",
      "epoch no.6 train no.472240  loss = 2.39200 avg_loss = 2.91285\n",
      "epoch no.6 train no.472250  loss = 1.72013 avg_loss = 2.94943\n",
      "epoch no.6 train no.472260  loss = 0.99097 avg_loss = 2.89835\n",
      "epoch no.6 train no.472270  loss = 2.39959 avg_loss = 2.91386\n",
      "epoch no.6 train no.472280  loss = 2.13148 avg_loss = 2.91854\n",
      "epoch no.6 train no.472290  loss = 2.79571 avg_loss = 2.91706\n",
      "epoch no.6 train no.472300  loss = 3.51814 avg_loss = 2.97323\n",
      "epoch no.6 train no.472310  loss = 2.92518 avg_loss = 2.94817\n",
      "epoch no.6 train no.472320  loss = 3.00834 avg_loss = 2.96586\n",
      "epoch no.6 train no.472330  loss = 2.84695 avg_loss = 2.96417\n",
      "epoch no.6 train no.472340  loss = 3.58198 avg_loss = 2.94283\n",
      "epoch no.6 train no.472350  loss = 3.45092 avg_loss = 2.93153\n",
      "epoch no.6 train no.472360  loss = 2.90285 avg_loss = 2.93345\n",
      "epoch no.6 train no.472370  loss = 2.58632 avg_loss = 2.96462\n",
      "epoch no.6 train no.472380  loss = 3.45554 avg_loss = 2.94624\n",
      "epoch no.6 train no.472390  loss = 2.07482 avg_loss = 2.88794\n",
      "epoch no.6 train no.472400  loss = 3.16242 avg_loss = 2.92090\n",
      "epoch no.6 train no.472410  loss = 2.38769 avg_loss = 2.91488\n",
      "epoch no.6 train no.472420  loss = 2.74315 avg_loss = 2.93381\n",
      "epoch no.6 train no.472430  loss = 3.15909 avg_loss = 2.92439\n",
      "epoch no.6 train no.472440  loss = 2.66723 avg_loss = 2.87236\n",
      "epoch no.6 train no.472450  loss = 2.81919 avg_loss = 2.89249\n",
      "epoch no.6 train no.472460  loss = 2.76067 avg_loss = 2.89799\n",
      "epoch no.6 train no.472470  loss = 2.52594 avg_loss = 2.83250\n",
      "epoch no.6 train no.472480  loss = 2.02345 avg_loss = 2.81491\n",
      "epoch no.6 train no.472490  loss = 2.52837 avg_loss = 2.80614\n",
      "epoch no.6 train no.472500  loss = 1.94592 avg_loss = 2.78900\n",
      "epoch no.6 train no.472510  loss = 3.29540 avg_loss = 2.82169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.472520  loss = 2.91030 avg_loss = 2.82322\n",
      "epoch no.6 train no.472530  loss = 3.01729 avg_loss = 2.82907\n",
      "epoch no.6 train no.472540  loss = 3.61422 avg_loss = 2.86779\n",
      "epoch no.6 train no.472550  loss = 3.26493 avg_loss = 2.85620\n",
      "epoch no.6 train no.472560  loss = 4.16360 avg_loss = 2.87460\n",
      "epoch no.6 train no.472570  loss = 2.15406 avg_loss = 2.89222\n",
      "epoch no.6 train no.472580  loss = 2.41068 avg_loss = 2.85102\n",
      "epoch no.6 train no.472590  loss = 3.28182 avg_loss = 2.83550\n",
      "epoch no.6 train no.472600  loss = 2.84157 avg_loss = 2.79591\n",
      "epoch no.6 train no.472610  loss = 3.48478 avg_loss = 2.86315\n",
      "epoch no.6 train no.472620  loss = 3.34286 avg_loss = 2.88728\n",
      "epoch no.6 train no.472630  loss = 2.91484 avg_loss = 2.90394\n",
      "epoch no.6 train no.472640  loss = 3.89155 avg_loss = 2.91606\n",
      "epoch no.6 train no.472650  loss = 2.72419 avg_loss = 2.92025\n",
      "epoch no.6 train no.472660  loss = 2.41730 avg_loss = 2.93291\n",
      "epoch no.6 train no.472670  loss = 2.52357 avg_loss = 2.92861\n",
      "epoch no.6 train no.472680  loss = 3.28495 avg_loss = 2.96105\n",
      "epoch no.6 train no.472690  loss = 3.88426 avg_loss = 2.95610\n",
      "epoch no.6 train no.472700  loss = 3.53053 avg_loss = 2.93978\n",
      "epoch no.6 train no.472710  loss = 2.06447 avg_loss = 2.92094\n",
      "epoch no.6 train no.472720  loss = 2.45589 avg_loss = 2.93841\n",
      "epoch no.6 train no.472730  loss = 2.51885 avg_loss = 2.91068\n",
      "epoch no.6 train no.472740  loss = 3.74635 avg_loss = 2.91507\n",
      "epoch no.6 train no.472750  loss = 2.06260 avg_loss = 2.95906\n",
      "epoch no.6 train no.472760  loss = 3.53958 avg_loss = 2.93328\n",
      "epoch no.6 train no.472770  loss = 3.63682 avg_loss = 2.94748\n",
      "epoch no.6 train no.472780  loss = 2.53585 avg_loss = 2.96095\n",
      "epoch no.6 train no.472790  loss = 2.37916 avg_loss = 2.92649\n",
      "epoch no.6 train no.472800  loss = 3.13984 avg_loss = 2.92288\n",
      "epoch no.6 train no.472810  loss = 2.15276 avg_loss = 2.91825\n",
      "epoch no.6 train no.472820  loss = 2.65284 avg_loss = 2.88806\n",
      "epoch no.6 train no.472830  loss = 3.09758 avg_loss = 2.90618\n",
      "epoch no.6 train no.472840  loss = 2.27417 avg_loss = 2.93051\n",
      "epoch no.6 train no.472850  loss = 3.47485 avg_loss = 2.92897\n",
      "epoch no.6 train no.472860  loss = 3.11734 avg_loss = 2.92047\n",
      "epoch no.6 train no.472870  loss = 4.03169 avg_loss = 2.92020\n",
      "epoch no.6 train no.472880  loss = 1.53819 avg_loss = 2.96665\n",
      "epoch no.6 train no.472890  loss = 2.99760 avg_loss = 2.95713\n",
      "epoch no.6 train no.472900  loss = 3.51638 avg_loss = 2.93395\n",
      "epoch no.6 train no.472910  loss = 2.51707 avg_loss = 2.92081\n",
      "epoch no.6 train no.472920  loss = 2.65426 avg_loss = 2.93481\n",
      "epoch no.6 train no.472930  loss = 1.67246 avg_loss = 2.89425\n",
      "epoch no.6 train no.472940  loss = 2.63418 avg_loss = 2.91888\n",
      "epoch no.6 train no.472950  loss = 3.46209 avg_loss = 2.95293\n",
      "epoch no.6 train no.472960  loss = 1.55282 avg_loss = 2.90924\n",
      "epoch no.6 train no.472970  loss = 2.92017 avg_loss = 2.89699\n",
      "epoch no.6 train no.472980  loss = 2.90730 avg_loss = 2.87361\n",
      "epoch no.6 train no.472990  loss = 2.18711 avg_loss = 2.86999\n",
      "epoch no.6 train no.473000  loss = 2.24915 avg_loss = 2.86353\n",
      "5\n",
      "to_tokens: ['▁가을', '▁90', '80', '년대', '스가', '요', '</s>']\n",
      "추억의 7080 댄스가요</s>\n",
      "epoch no.6 train no.473010  loss = 3.90652 avg_loss = 2.89413\n",
      "epoch no.6 train no.473020  loss = 4.06170 avg_loss = 2.87731\n",
      "epoch no.6 train no.473030  loss = 4.40249 avg_loss = 2.89339\n",
      "epoch no.6 train no.473040  loss = 1.94258 avg_loss = 2.86753\n",
      "epoch no.6 train no.473050  loss = 3.42474 avg_loss = 2.82681\n",
      "epoch no.6 train no.473060  loss = 2.23003 avg_loss = 2.82138\n",
      "epoch no.6 train no.473070  loss = 2.91173 avg_loss = 2.83278\n",
      "epoch no.6 train no.473080  loss = 3.34965 avg_loss = 2.84239\n",
      "epoch no.6 train no.473090  loss = 2.58172 avg_loss = 2.86235\n",
      "epoch no.6 train no.473100  loss = 2.39007 avg_loss = 2.87117\n",
      "epoch no.6 train no.473110  loss = 2.49383 avg_loss = 2.84770\n",
      "epoch no.6 train no.473120  loss = 0.89954 avg_loss = 2.83730\n",
      "epoch no.6 train no.473130  loss = 2.40851 avg_loss = 2.81815\n",
      "epoch no.6 train no.473140  loss = 3.40364 avg_loss = 2.82619\n",
      "epoch no.6 train no.473150  loss = 3.04520 avg_loss = 2.83149\n",
      "epoch no.6 train no.473160  loss = 1.85730 avg_loss = 2.82588\n",
      "epoch no.6 train no.473170  loss = 3.49380 avg_loss = 2.84890\n",
      "epoch no.6 train no.473180  loss = 2.57535 avg_loss = 2.84526\n",
      "epoch no.6 train no.473190  loss = 2.02483 avg_loss = 2.83790\n",
      "epoch no.6 train no.473200  loss = 1.83042 avg_loss = 2.84567\n",
      "epoch no.6 train no.473210  loss = 1.96293 avg_loss = 2.85325\n",
      "epoch no.6 train no.473220  loss = 2.30003 avg_loss = 2.84295\n",
      "epoch no.6 train no.473230  loss = 3.18770 avg_loss = 2.85839\n",
      "epoch no.6 train no.473240  loss = 2.51058 avg_loss = 2.82876\n",
      "epoch no.6 train no.473250  loss = 4.11905 avg_loss = 2.85845\n",
      "epoch no.6 train no.473260  loss = 4.10743 avg_loss = 2.86047\n",
      "epoch no.6 train no.473270  loss = 3.82943 avg_loss = 2.87603\n",
      "epoch no.6 train no.473280  loss = 4.08392 avg_loss = 2.92291\n",
      "epoch no.6 train no.473290  loss = 2.14291 avg_loss = 2.91515\n",
      "epoch no.6 train no.473300  loss = 3.14336 avg_loss = 2.97952\n",
      "epoch no.6 train no.473310  loss = 1.96450 avg_loss = 2.95337\n",
      "epoch no.6 train no.473320  loss = 2.66989 avg_loss = 2.94591\n",
      "epoch no.6 train no.473330  loss = 2.55923 avg_loss = 2.90970\n",
      "epoch no.6 train no.473340  loss = 1.90425 avg_loss = 2.91818\n",
      "epoch no.6 train no.473350  loss = 2.19428 avg_loss = 2.88005\n",
      "epoch no.6 train no.473360  loss = 3.67489 avg_loss = 2.93417\n",
      "epoch no.6 train no.473370  loss = 5.81653 avg_loss = 2.93832\n",
      "epoch no.6 train no.473380  loss = 2.18597 avg_loss = 2.92910\n",
      "epoch no.6 train no.473390  loss = 4.99613 avg_loss = 2.96060\n",
      "epoch no.6 train no.473400  loss = 3.57062 avg_loss = 3.00972\n",
      "epoch no.6 train no.473410  loss = 2.07305 avg_loss = 3.02385\n",
      "epoch no.6 train no.473420  loss = 2.61294 avg_loss = 3.02816\n",
      "epoch no.6 train no.473430  loss = 3.96363 avg_loss = 3.01668\n",
      "epoch no.6 train no.473440  loss = 1.98748 avg_loss = 2.94267\n",
      "epoch no.6 train no.473450  loss = 3.24981 avg_loss = 2.89084\n",
      "epoch no.6 train no.473460  loss = 4.01231 avg_loss = 2.90353\n",
      "epoch no.6 train no.473470  loss = 3.46080 avg_loss = 2.96566\n",
      "epoch no.6 train no.473480  loss = 3.03258 avg_loss = 2.94372\n",
      "epoch no.6 train no.473490  loss = 2.44357 avg_loss = 2.95211\n",
      "epoch no.6 train no.473500  loss = 2.51572 avg_loss = 2.95165\n",
      "epoch no.6 train no.473510  loss = 3.26687 avg_loss = 2.92803\n",
      "epoch no.6 train no.473520  loss = 2.49833 avg_loss = 2.92312\n",
      "epoch no.6 train no.473530  loss = 2.46861 avg_loss = 2.91728\n",
      "epoch no.6 train no.473540  loss = 2.32350 avg_loss = 2.87028\n",
      "epoch no.6 train no.473550  loss = 2.74562 avg_loss = 2.87840\n",
      "epoch no.6 train no.473560  loss = 3.10652 avg_loss = 2.89857\n",
      "epoch no.6 train no.473570  loss = 4.16841 avg_loss = 2.88082\n",
      "epoch no.6 train no.473580  loss = 1.91508 avg_loss = 2.88768\n",
      "epoch no.6 train no.473590  loss = 2.24450 avg_loss = 2.87211\n",
      "epoch no.6 train no.473600  loss = 3.72079 avg_loss = 2.91491\n",
      "epoch no.6 train no.473610  loss = 4.61512 avg_loss = 2.92929\n",
      "epoch no.6 train no.473620  loss = 2.89406 avg_loss = 2.93004\n",
      "epoch no.6 train no.473630  loss = 1.91018 avg_loss = 2.91464\n",
      "epoch no.6 train no.473640  loss = 1.67635 avg_loss = 2.89364\n",
      "epoch no.6 train no.473650  loss = 1.74868 avg_loss = 2.83132\n",
      "epoch no.6 train no.473660  loss = 1.45821 avg_loss = 2.90266\n",
      "epoch no.6 train no.473670  loss = 2.12813 avg_loss = 2.89859\n",
      "epoch no.6 train no.473680  loss = 2.18398 avg_loss = 2.87756\n",
      "epoch no.6 train no.473690  loss = 2.68238 avg_loss = 2.86632\n",
      "epoch no.6 train no.473700  loss = 2.08212 avg_loss = 2.86390\n",
      "epoch no.6 train no.473710  loss = 2.94274 avg_loss = 2.85676\n",
      "epoch no.6 train no.473720  loss = 2.92987 avg_loss = 2.84856\n",
      "epoch no.6 train no.473730  loss = 3.24289 avg_loss = 2.83083\n",
      "epoch no.6 train no.473740  loss = 2.34795 avg_loss = 2.86887\n",
      "epoch no.6 train no.473750  loss = 2.25753 avg_loss = 2.82376\n",
      "epoch no.6 train no.473760  loss = 2.62678 avg_loss = 2.77723\n",
      "epoch no.6 train no.473770  loss = 4.35825 avg_loss = 2.79931\n",
      "epoch no.6 train no.473780  loss = 2.29921 avg_loss = 2.82214\n",
      "epoch no.6 train no.473790  loss = 3.54021 avg_loss = 2.83667\n",
      "epoch no.6 train no.473800  loss = 2.58515 avg_loss = 2.83401\n",
      "epoch no.6 train no.473810  loss = 2.84017 avg_loss = 2.83569\n",
      "epoch no.6 train no.473820  loss = 3.28211 avg_loss = 2.84785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.473830  loss = 4.21312 avg_loss = 2.85223\n",
      "epoch no.6 train no.473840  loss = 2.42597 avg_loss = 2.85907\n",
      "epoch no.6 train no.473850  loss = 3.50665 avg_loss = 2.89765\n",
      "epoch no.6 train no.473860  loss = 3.64042 avg_loss = 2.90069\n",
      "epoch no.6 train no.473870  loss = 2.95650 avg_loss = 2.87321\n",
      "epoch no.6 train no.473880  loss = 2.57398 avg_loss = 2.87755\n",
      "epoch no.6 train no.473890  loss = 3.28863 avg_loss = 2.91378\n",
      "epoch no.6 train no.473900  loss = 3.34800 avg_loss = 2.91835\n",
      "epoch no.6 train no.473910  loss = 3.86244 avg_loss = 2.93535\n",
      "epoch no.6 train no.473920  loss = 3.36699 avg_loss = 2.95745\n",
      "epoch no.6 train no.473930  loss = 2.34628 avg_loss = 2.96915\n",
      "epoch no.6 train no.473940  loss = 4.93421 avg_loss = 2.96598\n",
      "epoch no.6 train no.473950  loss = 3.44459 avg_loss = 2.94215\n",
      "epoch no.6 train no.473960  loss = 3.53939 avg_loss = 2.91601\n",
      "epoch no.6 train no.473970  loss = 2.15322 avg_loss = 2.90689\n",
      "epoch no.6 train no.473980  loss = 3.92805 avg_loss = 2.90813\n",
      "epoch no.6 train no.473990  loss = 1.94854 avg_loss = 2.94845\n",
      "epoch no.6 train no.474000  loss = 2.99233 avg_loss = 2.93657\n",
      "4\n",
      "to_tokens: ['▁비', '▁발라', '드', '▁베스트', '년대', '</s>']\n",
      "추억의 발라드 90년대</s>\n",
      "epoch no.6 train no.474010  loss = 3.44179 avg_loss = 2.96039\n",
      "epoch no.6 train no.474020  loss = 2.93219 avg_loss = 2.96452\n",
      "epoch no.6 train no.474030  loss = 1.43737 avg_loss = 2.96617\n",
      "epoch no.6 train no.474040  loss = 2.58311 avg_loss = 2.95287\n",
      "epoch no.6 train no.474050  loss = 3.34260 avg_loss = 2.96619\n",
      "epoch no.6 train no.474060  loss = 3.43989 avg_loss = 2.99690\n",
      "epoch no.6 train no.474070  loss = 3.64748 avg_loss = 3.00358\n",
      "epoch no.6 train no.474080  loss = 3.13167 avg_loss = 3.05209\n",
      "epoch no.6 train no.474090  loss = 4.08739 avg_loss = 3.02170\n",
      "epoch no.6 train no.474100  loss = 4.84789 avg_loss = 3.04943\n",
      "epoch no.6 train no.474110  loss = 4.38691 avg_loss = 3.00520\n",
      "epoch no.6 train no.474120  loss = 2.37220 avg_loss = 2.99567\n",
      "epoch no.6 train no.474130  loss = 4.27711 avg_loss = 3.01032\n",
      "epoch no.6 train no.474140  loss = 3.16911 avg_loss = 2.98898\n",
      "epoch no.6 train no.474150  loss = 1.33922 avg_loss = 2.98193\n",
      "epoch no.6 train no.474160  loss = 3.07738 avg_loss = 2.96230\n",
      "epoch no.6 train no.474170  loss = 2.14569 avg_loss = 2.98227\n",
      "epoch no.6 train no.474180  loss = 2.41244 avg_loss = 2.96533\n",
      "epoch no.6 train no.474190  loss = 2.77476 avg_loss = 2.99127\n",
      "epoch no.6 train no.474200  loss = 3.77306 avg_loss = 3.01908\n",
      "epoch no.6 train no.474210  loss = 3.43412 avg_loss = 2.97931\n",
      "epoch no.6 train no.474220  loss = 2.39134 avg_loss = 2.96960\n",
      "epoch no.6 train no.474230  loss = 2.87317 avg_loss = 2.94271\n",
      "epoch no.6 train no.474240  loss = 3.74256 avg_loss = 2.94694\n",
      "epoch no.6 train no.474250  loss = 2.67725 avg_loss = 2.97574\n",
      "epoch no.6 train no.474260  loss = 3.43666 avg_loss = 3.00686\n",
      "epoch no.6 train no.474270  loss = 2.86587 avg_loss = 2.99041\n",
      "epoch no.6 train no.474280  loss = 3.47735 avg_loss = 2.98745\n",
      "epoch no.6 train no.474290  loss = 2.32352 avg_loss = 2.95808\n",
      "epoch no.6 train no.474300  loss = 3.64585 avg_loss = 2.97298\n",
      "epoch no.6 train no.474310  loss = 2.92276 avg_loss = 2.99396\n",
      "epoch no.6 train no.474320  loss = 1.93339 avg_loss = 2.98434\n",
      "epoch no.6 train no.474330  loss = 2.85088 avg_loss = 2.95840\n",
      "epoch no.6 train no.474340  loss = 2.36640 avg_loss = 2.96661\n",
      "epoch no.6 train no.474350  loss = 3.16661 avg_loss = 2.98630\n",
      "epoch no.6 train no.474360  loss = 3.48785 avg_loss = 2.99149\n",
      "epoch no.6 train no.474370  loss = 1.85924 avg_loss = 2.97951\n",
      "epoch no.6 train no.474380  loss = 3.29628 avg_loss = 2.97230\n",
      "epoch no.6 train no.474390  loss = 2.91681 avg_loss = 2.95905\n",
      "epoch no.6 train no.474400  loss = 4.19318 avg_loss = 2.93551\n",
      "epoch no.6 train no.474410  loss = 2.42308 avg_loss = 2.94840\n",
      "epoch no.6 train no.474420  loss = 2.26777 avg_loss = 2.90488\n",
      "epoch no.6 train no.474430  loss = 2.99245 avg_loss = 2.91727\n",
      "epoch no.6 train no.474440  loss = 2.65524 avg_loss = 2.88283\n",
      "epoch no.6 train no.474450  loss = 2.42063 avg_loss = 2.87808\n",
      "epoch no.6 train no.474460  loss = 2.46942 avg_loss = 2.85925\n",
      "epoch no.6 train no.474470  loss = 2.31732 avg_loss = 2.85640\n",
      "epoch no.6 train no.474480  loss = 3.83422 avg_loss = 2.86151\n",
      "epoch no.6 train no.474490  loss = 2.25181 avg_loss = 2.83455\n",
      "epoch no.6 train no.474500  loss = 2.72032 avg_loss = 2.84475\n",
      "epoch no.6 train no.474510  loss = 2.44899 avg_loss = 2.83391\n",
      "epoch no.6 train no.474520  loss = 4.89276 avg_loss = 2.83298\n",
      "epoch no.6 train no.474530  loss = 2.81876 avg_loss = 2.87156\n",
      "epoch no.6 train no.474540  loss = 2.92994 avg_loss = 2.87769\n",
      "epoch no.6 train no.474550  loss = 3.32807 avg_loss = 2.91228\n",
      "epoch no.6 train no.474560  loss = 3.16471 avg_loss = 2.88676\n",
      "epoch no.6 train no.474570  loss = 2.00803 avg_loss = 2.89121\n",
      "epoch no.6 train no.474580  loss = 3.19756 avg_loss = 2.90870\n",
      "epoch no.6 train no.474590  loss = 2.44474 avg_loss = 2.91361\n",
      "epoch no.6 train no.474600  loss = 2.78918 avg_loss = 2.91833\n",
      "epoch no.6 train no.474610  loss = 2.77703 avg_loss = 2.89981\n",
      "epoch no.6 train no.474620  loss = 2.52043 avg_loss = 2.86890\n",
      "epoch no.6 train no.474630  loss = 2.86446 avg_loss = 2.86743\n",
      "epoch no.6 train no.474640  loss = 3.06165 avg_loss = 2.88698\n",
      "epoch no.6 train no.474650  loss = 2.01861 avg_loss = 2.87625\n",
      "epoch no.6 train no.474660  loss = 1.95417 avg_loss = 2.84018\n",
      "epoch no.6 train no.474670  loss = 1.97095 avg_loss = 2.85447\n",
      "epoch no.6 train no.474680  loss = 2.42843 avg_loss = 2.86686\n",
      "epoch no.6 train no.474690  loss = 3.51140 avg_loss = 2.91582\n",
      "epoch no.6 train no.474700  loss = 2.42699 avg_loss = 2.91848\n",
      "epoch no.6 train no.474710  loss = 2.74315 avg_loss = 2.89952\n",
      "epoch no.6 train no.474720  loss = 5.30610 avg_loss = 2.94429\n",
      "epoch no.6 train no.474730  loss = 1.43187 avg_loss = 2.90154\n",
      "epoch no.6 train no.474740  loss = 2.24845 avg_loss = 2.90247\n",
      "epoch no.6 train no.474750  loss = 3.22594 avg_loss = 2.89912\n",
      "epoch no.6 train no.474760  loss = 2.69972 avg_loss = 2.93451\n",
      "epoch no.6 train no.474770  loss = 4.36991 avg_loss = 2.92027\n",
      "epoch no.6 train no.474780  loss = 2.75243 avg_loss = 2.93906\n",
      "epoch no.6 train no.474790  loss = 3.24859 avg_loss = 2.95458\n",
      "epoch no.6 train no.474800  loss = 2.42743 avg_loss = 2.93745\n",
      "epoch no.6 train no.474810  loss = 3.05855 avg_loss = 2.94605\n",
      "epoch no.6 train no.474820  loss = 1.33095 avg_loss = 2.91469\n",
      "epoch no.6 train no.474830  loss = 3.07147 avg_loss = 2.86803\n",
      "epoch no.6 train no.474840  loss = 3.58678 avg_loss = 2.89202\n",
      "epoch no.6 train no.474850  loss = 2.88786 avg_loss = 2.90616\n",
      "epoch no.6 train no.474860  loss = 3.13978 avg_loss = 2.89272\n",
      "epoch no.6 train no.474870  loss = 3.08133 avg_loss = 2.91034\n",
      "epoch no.6 train no.474880  loss = 2.51161 avg_loss = 2.89026\n",
      "epoch no.6 train no.474890  loss = 2.31684 avg_loss = 2.87441\n",
      "epoch no.6 train no.474900  loss = 1.96049 avg_loss = 2.88187\n",
      "epoch no.6 train no.474910  loss = 3.74616 avg_loss = 2.90056\n",
      "epoch no.6 train no.474920  loss = 2.08118 avg_loss = 2.88384\n",
      "epoch no.6 train no.474930  loss = 5.19739 avg_loss = 2.89867\n",
      "epoch no.6 train no.474940  loss = 2.28280 avg_loss = 2.89586\n",
      "epoch no.6 train no.474950  loss = 4.21252 avg_loss = 2.93507\n",
      "epoch no.6 train no.474960  loss = 2.20641 avg_loss = 2.94957\n",
      "epoch no.6 train no.474970  loss = 1.86816 avg_loss = 2.90428\n",
      "epoch no.6 train no.474980  loss = 2.57135 avg_loss = 2.95754\n",
      "epoch no.6 train no.474990  loss = 3.29254 avg_loss = 2.98120\n",
      "epoch no.6 train no.475000  loss = 3.75125 avg_loss = 2.95945\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '곡', '들', '팝', '</s>', '</s>']\n",
      "추억의 명곡 올드팝송</s>\n",
      "epoch no.6 train no.475010  loss = 4.64133 avg_loss = 2.94722\n",
      "epoch no.6 train no.475020  loss = 2.24480 avg_loss = 2.93298\n",
      "epoch no.6 train no.475030  loss = 4.14063 avg_loss = 2.90635\n",
      "epoch no.6 train no.475040  loss = 3.78068 avg_loss = 2.93350\n",
      "epoch no.6 train no.475050  loss = 4.81976 avg_loss = 2.97467\n",
      "epoch no.6 train no.475060  loss = 1.65445 avg_loss = 2.94802\n",
      "epoch no.6 train no.475070  loss = 2.75270 avg_loss = 2.94885\n",
      "epoch no.6 train no.475080  loss = 3.14470 avg_loss = 2.96396\n",
      "epoch no.6 train no.475090  loss = 3.19930 avg_loss = 2.96820\n",
      "epoch no.6 train no.475100  loss = 3.52920 avg_loss = 2.94388\n",
      "epoch no.6 train no.475110  loss = 2.00085 avg_loss = 2.91901\n",
      "epoch no.6 train no.475120  loss = 3.26702 avg_loss = 2.89247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.475130  loss = 3.78689 avg_loss = 2.91692\n",
      "epoch no.6 train no.475140  loss = 3.04324 avg_loss = 2.89037\n",
      "epoch no.6 train no.475150  loss = 3.07081 avg_loss = 2.91032\n",
      "epoch no.6 train no.475160  loss = 3.46691 avg_loss = 2.88320\n",
      "epoch no.6 train no.475170  loss = 3.91128 avg_loss = 2.89900\n",
      "epoch no.6 train no.475180  loss = 3.17733 avg_loss = 2.90294\n",
      "epoch no.6 train no.475190  loss = 3.27798 avg_loss = 2.90177\n",
      "epoch no.6 train no.475200  loss = 2.16330 avg_loss = 2.94276\n",
      "epoch no.6 train no.475210  loss = 2.14489 avg_loss = 2.94005\n",
      "epoch no.6 train no.475220  loss = 3.26482 avg_loss = 2.91020\n",
      "epoch no.6 train no.475230  loss = 3.36374 avg_loss = 2.93327\n",
      "epoch no.6 train no.475240  loss = 1.75378 avg_loss = 2.89739\n",
      "epoch no.6 train no.475250  loss = 3.16015 avg_loss = 2.89480\n",
      "epoch no.6 train no.475260  loss = 2.23009 avg_loss = 2.87890\n",
      "epoch no.6 train no.475270  loss = 2.60909 avg_loss = 2.88753\n",
      "epoch no.6 train no.475280  loss = 1.47537 avg_loss = 2.87018\n",
      "epoch no.6 train no.475290  loss = 2.53257 avg_loss = 2.89674\n",
      "epoch no.6 train no.475300  loss = 3.84391 avg_loss = 2.88586\n",
      "epoch no.6 train no.475310  loss = 3.15101 avg_loss = 2.87940\n",
      "epoch no.6 train no.475320  loss = 2.57528 avg_loss = 2.90489\n",
      "epoch no.6 train no.475330  loss = 4.71307 avg_loss = 2.89684\n",
      "epoch no.6 train no.475340  loss = 2.77579 avg_loss = 2.85564\n",
      "epoch no.6 train no.475350  loss = 3.30963 avg_loss = 2.86046\n",
      "epoch no.6 train no.475360  loss = 2.07807 avg_loss = 2.85057\n",
      "epoch no.6 train no.475370  loss = 2.02991 avg_loss = 2.84929\n",
      "epoch no.6 train no.475380  loss = 3.26082 avg_loss = 2.85192\n",
      "epoch no.6 train no.475390  loss = 3.15548 avg_loss = 2.86079\n",
      "epoch no.6 train no.475400  loss = 2.96516 avg_loss = 2.86259\n",
      "epoch no.6 train no.475410  loss = 2.49092 avg_loss = 2.85029\n",
      "epoch no.6 train no.475420  loss = 1.76150 avg_loss = 2.86165\n",
      "epoch no.6 train no.475430  loss = 2.14426 avg_loss = 2.84836\n",
      "epoch no.6 train no.475440  loss = 2.37990 avg_loss = 2.86406\n",
      "epoch no.6 train no.475450  loss = 3.43755 avg_loss = 2.89374\n",
      "epoch no.6 train no.475460  loss = 3.41883 avg_loss = 2.90404\n",
      "epoch no.6 train no.475470  loss = 3.30138 avg_loss = 2.91052\n",
      "epoch no.6 train no.475480  loss = 2.28576 avg_loss = 2.87456\n",
      "epoch no.6 train no.475490  loss = 2.16594 avg_loss = 2.85804\n",
      "epoch no.6 train no.475500  loss = 3.16385 avg_loss = 2.85364\n",
      "epoch no.6 train no.475510  loss = 3.00838 avg_loss = 2.90336\n",
      "epoch no.6 train no.475520  loss = 3.78034 avg_loss = 2.94764\n",
      "epoch no.6 train no.475530  loss = 4.81270 avg_loss = 2.93925\n",
      "epoch no.6 train no.475540  loss = 2.24155 avg_loss = 2.90209\n",
      "epoch no.6 train no.475550  loss = 2.21104 avg_loss = 2.90182\n",
      "epoch no.6 train no.475560  loss = 3.10154 avg_loss = 2.91082\n",
      "epoch no.6 train no.475570  loss = 2.36241 avg_loss = 2.91777\n",
      "epoch no.6 train no.475580  loss = 2.46680 avg_loss = 2.91467\n",
      "epoch no.6 train no.475590  loss = 2.95753 avg_loss = 2.93453\n",
      "epoch no.6 train no.475600  loss = 2.56193 avg_loss = 2.93880\n",
      "epoch no.6 train no.475610  loss = 1.98899 avg_loss = 2.94399\n",
      "epoch no.6 train no.475620  loss = 3.57067 avg_loss = 2.92054\n",
      "epoch no.6 train no.475630  loss = 2.67283 avg_loss = 2.94911\n",
      "epoch no.6 train no.475640  loss = 3.79963 avg_loss = 2.94032\n",
      "epoch no.6 train no.475650  loss = 2.94799 avg_loss = 2.92308\n",
      "epoch no.6 train no.475660  loss = 3.19535 avg_loss = 2.95050\n",
      "epoch no.6 train no.475670  loss = 2.13418 avg_loss = 2.97122\n",
      "epoch no.6 train no.475680  loss = 2.35591 avg_loss = 2.94193\n",
      "epoch no.6 train no.475690  loss = 1.94656 avg_loss = 2.94273\n",
      "epoch no.6 train no.475700  loss = 2.23166 avg_loss = 2.94252\n",
      "epoch no.6 train no.475710  loss = 1.53498 avg_loss = 2.90530\n",
      "epoch no.6 train no.475720  loss = 3.67414 avg_loss = 2.89467\n",
      "epoch no.6 train no.475730  loss = 3.84815 avg_loss = 2.91430\n",
      "epoch no.6 train no.475740  loss = 2.79723 avg_loss = 2.89876\n",
      "epoch no.6 train no.475750  loss = 3.65346 avg_loss = 2.91118\n",
      "epoch no.6 train no.475760  loss = 1.89738 avg_loss = 2.89695\n",
      "epoch no.6 train no.475770  loss = 3.81246 avg_loss = 2.92352\n",
      "epoch no.6 train no.475780  loss = 3.01577 avg_loss = 2.88754\n",
      "epoch no.6 train no.475790  loss = 4.05051 avg_loss = 2.88077\n",
      "epoch no.6 train no.475800  loss = 3.42375 avg_loss = 2.89986\n",
      "epoch no.6 train no.475810  loss = 2.48391 avg_loss = 2.91612\n",
      "epoch no.6 train no.475820  loss = 2.71240 avg_loss = 2.89471\n",
      "epoch no.6 train no.475830  loss = 2.42221 avg_loss = 2.88650\n",
      "epoch no.6 train no.475840  loss = 2.93381 avg_loss = 2.93956\n",
      "epoch no.6 train no.475850  loss = 3.57116 avg_loss = 2.93330\n",
      "epoch no.6 train no.475860  loss = 3.69620 avg_loss = 2.97236\n",
      "epoch no.6 train no.475870  loss = 3.50355 avg_loss = 2.99258\n",
      "epoch no.6 train no.475880  loss = 1.90013 avg_loss = 2.99467\n",
      "epoch no.6 train no.475890  loss = 2.72740 avg_loss = 2.99508\n",
      "epoch no.6 train no.475900  loss = 2.56662 avg_loss = 2.97831\n",
      "epoch no.6 train no.475910  loss = 2.84713 avg_loss = 2.97028\n",
      "epoch no.6 train no.475920  loss = 3.42901 avg_loss = 2.95345\n",
      "epoch no.6 train no.475930  loss = 3.83316 avg_loss = 2.96547\n",
      "epoch no.6 train no.475940  loss = 2.63073 avg_loss = 2.93432\n",
      "epoch no.6 train no.475950  loss = 2.23093 avg_loss = 2.89720\n",
      "epoch no.6 train no.475960  loss = 2.45764 avg_loss = 2.88738\n",
      "epoch no.6 train no.475970  loss = 2.17319 avg_loss = 2.87838\n",
      "epoch no.6 train no.475980  loss = 1.96543 avg_loss = 2.88053\n",
      "epoch no.6 train no.475990  loss = 2.35848 avg_loss = 2.85317\n",
      "epoch no.6 train no.476000  loss = 2.70223 avg_loss = 2.84149\n",
      "4\n",
      "to_tokens: ['▁비', '▁드라마', '곡', '들', '집', '</s>']\n",
      "추억의 명곡 모음집</s>\n",
      "epoch no.6 train no.476010  loss = 2.20354 avg_loss = 2.85034\n",
      "epoch no.6 train no.476020  loss = 3.01391 avg_loss = 2.79874\n",
      "epoch no.6 train no.476030  loss = 2.75118 avg_loss = 2.78368\n",
      "epoch no.6 train no.476040  loss = 2.82165 avg_loss = 2.81479\n",
      "epoch no.6 train no.476050  loss = 2.78343 avg_loss = 2.80217\n",
      "epoch no.6 train no.476060  loss = 3.06467 avg_loss = 2.78115\n",
      "epoch no.6 train no.476070  loss = 3.97732 avg_loss = 2.83922\n",
      "epoch no.6 train no.476080  loss = 2.85864 avg_loss = 2.84501\n",
      "epoch no.6 train no.476090  loss = 2.75707 avg_loss = 2.82096\n",
      "epoch no.6 train no.476100  loss = 1.54771 avg_loss = 2.83632\n",
      "epoch no.6 train no.476110  loss = 3.24984 avg_loss = 2.86716\n",
      "epoch no.6 train no.476120  loss = 3.06994 avg_loss = 2.89401\n",
      "epoch no.6 train no.476130  loss = 2.53148 avg_loss = 2.89968\n",
      "epoch no.6 train no.476140  loss = 1.81730 avg_loss = 2.86247\n",
      "epoch no.6 train no.476150  loss = 2.93109 avg_loss = 2.84486\n",
      "epoch no.6 train no.476160  loss = 3.50875 avg_loss = 2.85837\n",
      "epoch no.6 train no.476170  loss = 4.96154 avg_loss = 2.89715\n",
      "epoch no.6 train no.476180  loss = 2.55222 avg_loss = 2.89460\n",
      "epoch no.6 train no.476190  loss = 2.74067 avg_loss = 2.89991\n",
      "epoch no.6 train no.476200  loss = 2.61913 avg_loss = 2.93350\n",
      "epoch no.6 train no.476210  loss = 1.15987 avg_loss = 2.88048\n",
      "epoch no.6 train no.476220  loss = 2.73378 avg_loss = 2.89397\n",
      "epoch no.6 train no.476230  loss = 2.73560 avg_loss = 2.89273\n",
      "epoch no.6 train no.476240  loss = 4.00861 avg_loss = 2.88695\n",
      "epoch no.6 train no.476250  loss = 3.45164 avg_loss = 2.94219\n",
      "epoch no.6 train no.476260  loss = 2.57127 avg_loss = 2.92832\n",
      "epoch no.6 train no.476270  loss = 2.47373 avg_loss = 2.92431\n",
      "epoch no.6 train no.476280  loss = 2.84129 avg_loss = 2.90413\n",
      "epoch no.6 train no.476290  loss = 4.23495 avg_loss = 2.93475\n",
      "epoch no.6 train no.476300  loss = 3.31288 avg_loss = 2.91924\n",
      "epoch no.6 train no.476310  loss = 1.85178 avg_loss = 2.88191\n",
      "epoch no.6 train no.476320  loss = 1.64531 avg_loss = 2.82274\n",
      "epoch no.6 train no.476330  loss = 2.72581 avg_loss = 2.79625\n",
      "epoch no.6 train no.476340  loss = 3.71952 avg_loss = 2.82843\n",
      "epoch no.6 train no.476350  loss = 3.02696 avg_loss = 2.86821\n",
      "epoch no.6 train no.476360  loss = 3.32869 avg_loss = 2.90824\n",
      "epoch no.6 train no.476370  loss = 2.54418 avg_loss = 2.90370\n",
      "epoch no.6 train no.476380  loss = 3.08081 avg_loss = 2.89823\n",
      "epoch no.6 train no.476390  loss = 2.28185 avg_loss = 2.90191\n",
      "epoch no.6 train no.476400  loss = 3.07756 avg_loss = 2.90905\n",
      "epoch no.6 train no.476410  loss = 2.25501 avg_loss = 2.88871\n",
      "epoch no.6 train no.476420  loss = 2.74060 avg_loss = 2.87123\n",
      "epoch no.6 train no.476430  loss = 3.82292 avg_loss = 2.88539\n",
      "epoch no.6 train no.476440  loss = 2.68679 avg_loss = 2.90490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.476450  loss = 2.03128 avg_loss = 2.89675\n",
      "epoch no.6 train no.476460  loss = 1.99179 avg_loss = 2.85861\n",
      "epoch no.6 train no.476470  loss = 2.78978 avg_loss = 2.91011\n",
      "epoch no.6 train no.476480  loss = 3.02867 avg_loss = 2.88618\n",
      "epoch no.6 train no.476490  loss = 3.86634 avg_loss = 2.89490\n",
      "epoch no.6 train no.476500  loss = 3.72814 avg_loss = 2.87389\n",
      "epoch no.6 train no.476510  loss = 3.50437 avg_loss = 2.90389\n",
      "epoch no.6 train no.476520  loss = 4.19697 avg_loss = 2.89575\n",
      "epoch no.6 train no.476530  loss = 1.65412 avg_loss = 2.86785\n",
      "epoch no.6 train no.476540  loss = 2.57675 avg_loss = 2.90347\n",
      "epoch no.6 train no.476550  loss = 2.03923 avg_loss = 2.89279\n",
      "epoch no.6 train no.476560  loss = 2.97466 avg_loss = 2.89332\n",
      "epoch no.6 train no.476570  loss = 1.88693 avg_loss = 2.88149\n",
      "epoch no.6 train no.476580  loss = 3.70843 avg_loss = 2.86577\n",
      "epoch no.6 train no.476590  loss = 2.96756 avg_loss = 2.86365\n",
      "epoch no.6 train no.476600  loss = 1.88102 avg_loss = 2.86695\n",
      "epoch no.6 train no.476610  loss = 3.16993 avg_loss = 2.86127\n",
      "epoch no.6 train no.476620  loss = 2.91905 avg_loss = 2.84246\n",
      "epoch no.6 train no.476630  loss = 3.18017 avg_loss = 2.82847\n",
      "epoch no.6 train no.476640  loss = 3.11609 avg_loss = 2.84758\n",
      "epoch no.6 train no.476650  loss = 1.58921 avg_loss = 2.81659\n",
      "epoch no.6 train no.476660  loss = 2.65672 avg_loss = 2.84285\n",
      "epoch no.6 train no.476670  loss = 3.13470 avg_loss = 2.88520\n",
      "epoch no.6 train no.476680  loss = 1.78005 avg_loss = 2.86281\n",
      "epoch no.6 train no.476690  loss = 4.73484 avg_loss = 2.89428\n",
      "epoch no.6 train no.476700  loss = 2.59338 avg_loss = 2.89590\n",
      "epoch no.6 train no.476710  loss = 4.05899 avg_loss = 2.89161\n",
      "epoch no.6 train no.476720  loss = 2.76453 avg_loss = 2.88814\n",
      "epoch no.6 train no.476730  loss = 3.23683 avg_loss = 2.91949\n",
      "epoch no.6 train no.476740  loss = 2.72091 avg_loss = 2.93034\n",
      "epoch no.6 train no.476750  loss = 3.45418 avg_loss = 2.90137\n",
      "epoch no.6 train no.476760  loss = 1.99263 avg_loss = 2.88755\n",
      "epoch no.6 train no.476770  loss = 2.06122 avg_loss = 2.87222\n",
      "epoch no.6 train no.476780  loss = 2.32940 avg_loss = 2.87863\n",
      "epoch no.6 train no.476790  loss = 2.50067 avg_loss = 2.85960\n",
      "epoch no.6 train no.476800  loss = 3.06278 avg_loss = 2.84902\n",
      "epoch no.6 train no.476810  loss = 2.15452 avg_loss = 2.83689\n",
      "epoch no.6 train no.476820  loss = 3.75864 avg_loss = 2.91315\n",
      "epoch no.6 train no.476830  loss = 5.18771 avg_loss = 2.94085\n",
      "epoch no.6 train no.476840  loss = 3.21283 avg_loss = 2.94030\n",
      "epoch no.6 train no.476850  loss = 3.35142 avg_loss = 2.92689\n",
      "epoch no.6 train no.476860  loss = 3.88544 avg_loss = 2.93600\n",
      "epoch no.6 train no.476870  loss = 3.73091 avg_loss = 2.94695\n",
      "epoch no.6 train no.476880  loss = 3.62976 avg_loss = 2.93960\n",
      "epoch no.6 train no.476890  loss = 3.04945 avg_loss = 2.91014\n",
      "epoch no.6 train no.476900  loss = 2.78360 avg_loss = 2.90785\n",
      "epoch no.6 train no.476910  loss = 1.93979 avg_loss = 2.88327\n",
      "epoch no.6 train no.476920  loss = 2.82326 avg_loss = 2.87412\n",
      "epoch no.6 train no.476930  loss = 2.85156 avg_loss = 2.92013\n",
      "epoch no.6 train no.476940  loss = 4.80794 avg_loss = 2.93275\n",
      "epoch no.6 train no.476950  loss = 3.43720 avg_loss = 2.91368\n",
      "epoch no.6 train no.476960  loss = 2.91357 avg_loss = 2.90239\n",
      "epoch no.6 train no.476970  loss = 2.90284 avg_loss = 2.89945\n",
      "epoch no.6 train no.476980  loss = 2.90845 avg_loss = 2.93994\n",
      "epoch no.6 train no.476990  loss = 2.32461 avg_loss = 2.94126\n",
      "epoch no.6 train no.477000  loss = 2.04598 avg_loss = 2.95317\n",
      "4\n",
      "to_tokens: ['▁비', '▁2000', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.6 train no.477010  loss = 2.06458 avg_loss = 2.93933\n",
      "epoch no.6 train no.477020  loss = 4.58586 avg_loss = 2.98722\n",
      "epoch no.6 train no.477030  loss = 1.59120 avg_loss = 2.91351\n",
      "epoch no.6 train no.477040  loss = 3.21695 avg_loss = 2.91578\n",
      "epoch no.6 train no.477050  loss = 1.78979 avg_loss = 2.90171\n",
      "epoch no.6 train no.477060  loss = 3.00827 avg_loss = 2.91247\n",
      "epoch no.6 train no.477070  loss = 2.58545 avg_loss = 2.94385\n",
      "epoch no.6 train no.477080  loss = 2.14212 avg_loss = 2.96719\n",
      "epoch no.6 train no.477090  loss = 3.73999 avg_loss = 2.98669\n",
      "epoch no.6 train no.477100  loss = 2.55243 avg_loss = 2.95421\n",
      "epoch no.6 train no.477110  loss = 2.16549 avg_loss = 2.91856\n",
      "epoch no.6 train no.477120  loss = 2.23647 avg_loss = 2.91557\n",
      "epoch no.6 train no.477130  loss = 2.44703 avg_loss = 2.92966\n",
      "epoch no.6 train no.477140  loss = 2.97558 avg_loss = 2.94312\n",
      "epoch no.6 train no.477150  loss = 2.53714 avg_loss = 2.92807\n",
      "epoch no.6 train no.477160  loss = 3.58778 avg_loss = 2.89923\n",
      "epoch no.6 train no.477170  loss = 2.60170 avg_loss = 2.89829\n",
      "epoch no.6 train no.477180  loss = 2.69422 avg_loss = 2.89464\n",
      "epoch no.6 train no.477190  loss = 3.64904 avg_loss = 2.88456\n",
      "epoch no.6 train no.477200  loss = 4.39399 avg_loss = 2.85932\n",
      "epoch no.6 train no.477210  loss = 3.31436 avg_loss = 2.87190\n",
      "epoch no.6 train no.477220  loss = 3.32615 avg_loss = 2.90071\n",
      "epoch no.6 train no.477230  loss = 2.96211 avg_loss = 2.90143\n",
      "epoch no.6 train no.477240  loss = 4.08042 avg_loss = 2.94080\n",
      "epoch no.6 train no.477250  loss = 1.81661 avg_loss = 2.94231\n",
      "epoch no.6 train no.477260  loss = 2.28028 avg_loss = 2.91156\n",
      "epoch no.6 train no.477270  loss = 5.61082 avg_loss = 2.93265\n",
      "epoch no.6 train no.477280  loss = 3.27097 avg_loss = 2.91774\n",
      "epoch no.6 train no.477290  loss = 1.80552 avg_loss = 2.88401\n",
      "epoch no.6 train no.477300  loss = 3.15143 avg_loss = 2.87401\n",
      "epoch no.6 train no.477310  loss = 1.81887 avg_loss = 2.86835\n",
      "epoch no.6 train no.477320  loss = 3.99473 avg_loss = 2.86980\n",
      "epoch no.6 train no.477330  loss = 2.38287 avg_loss = 2.88822\n",
      "epoch no.6 train no.477340  loss = 2.78006 avg_loss = 2.88699\n",
      "epoch no.6 train no.477350  loss = 2.80842 avg_loss = 2.89953\n",
      "epoch no.6 train no.477360  loss = 2.59186 avg_loss = 2.86458\n",
      "epoch no.6 train no.477370  loss = 1.97558 avg_loss = 2.90877\n",
      "epoch no.6 train no.477380  loss = 2.37501 avg_loss = 2.93166\n",
      "epoch no.6 train no.477390  loss = 1.97041 avg_loss = 2.95014\n",
      "epoch no.6 train no.477400  loss = 2.36343 avg_loss = 2.94292\n",
      "epoch no.6 train no.477410  loss = 3.56074 avg_loss = 2.94659\n",
      "epoch no.6 train no.477420  loss = 2.70925 avg_loss = 2.93631\n",
      "epoch no.6 train no.477430  loss = 2.84037 avg_loss = 2.91405\n",
      "epoch no.6 train no.477440  loss = 3.03635 avg_loss = 2.89759\n",
      "epoch no.6 train no.477450  loss = 1.37611 avg_loss = 2.86543\n",
      "epoch no.6 train no.477460  loss = 3.45965 avg_loss = 2.84685\n",
      "epoch no.6 train no.477470  loss = 2.61101 avg_loss = 2.86812\n",
      "epoch no.6 train no.477480  loss = 3.18912 avg_loss = 2.88962\n",
      "epoch no.6 train no.477490  loss = 3.15919 avg_loss = 2.90518\n",
      "epoch no.6 train no.477500  loss = 3.19341 avg_loss = 2.93775\n",
      "epoch no.6 train no.477510  loss = 2.54393 avg_loss = 2.96440\n",
      "epoch no.6 train no.477520  loss = 2.30453 avg_loss = 2.92053\n",
      "epoch no.6 train no.477530  loss = 3.67905 avg_loss = 2.95315\n",
      "epoch no.6 train no.477540  loss = 2.45167 avg_loss = 2.92862\n",
      "epoch no.6 train no.477550  loss = 2.41224 avg_loss = 2.95858\n",
      "epoch no.6 train no.477560  loss = 4.66884 avg_loss = 2.98410\n",
      "epoch no.6 train no.477570  loss = 4.63507 avg_loss = 2.97422\n",
      "epoch no.6 train no.477580  loss = 2.91971 avg_loss = 2.99992\n",
      "epoch no.6 train no.477590  loss = 3.53753 avg_loss = 3.02169\n",
      "epoch no.6 train no.477600  loss = 2.77509 avg_loss = 2.98139\n",
      "epoch no.6 train no.477610  loss = 4.81261 avg_loss = 3.03696\n",
      "epoch no.6 train no.477620  loss = 2.11321 avg_loss = 3.01884\n",
      "epoch no.6 train no.477630  loss = 2.52340 avg_loss = 3.01149\n",
      "epoch no.6 train no.477640  loss = 3.18788 avg_loss = 3.01897\n",
      "epoch no.6 train no.477650  loss = 4.32257 avg_loss = 2.99670\n",
      "epoch no.6 train no.477660  loss = 2.00044 avg_loss = 2.97832\n",
      "epoch no.6 train no.477670  loss = 1.68309 avg_loss = 2.95943\n",
      "epoch no.6 train no.477680  loss = 2.73859 avg_loss = 2.94786\n",
      "epoch no.6 train no.477690  loss = 2.32189 avg_loss = 2.93791\n",
      "epoch no.6 train no.477700  loss = 5.42905 avg_loss = 2.94593\n",
      "epoch no.6 train no.477710  loss = 2.91263 avg_loss = 2.93069\n",
      "epoch no.6 train no.477720  loss = 2.60858 avg_loss = 2.90151\n",
      "epoch no.6 train no.477730  loss = 4.49273 avg_loss = 2.88034\n",
      "epoch no.6 train no.477740  loss = 3.32191 avg_loss = 2.88366\n",
      "epoch no.6 train no.477750  loss = 4.38955 avg_loss = 2.88888\n",
      "epoch no.6 train no.477760  loss = 2.94985 avg_loss = 2.88323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.477770  loss = 1.63687 avg_loss = 2.87729\n",
      "epoch no.6 train no.477780  loss = 2.66887 avg_loss = 2.90112\n",
      "epoch no.6 train no.477790  loss = 1.97073 avg_loss = 2.88248\n",
      "epoch no.6 train no.477800  loss = 2.52166 avg_loss = 2.88578\n",
      "epoch no.6 train no.477810  loss = 4.40919 avg_loss = 2.90094\n",
      "epoch no.6 train no.477820  loss = 3.86548 avg_loss = 2.91667\n",
      "epoch no.6 train no.477830  loss = 3.19125 avg_loss = 2.92691\n",
      "epoch no.6 train no.477840  loss = 2.65152 avg_loss = 2.87690\n",
      "epoch no.6 train no.477850  loss = 2.42463 avg_loss = 2.90240\n",
      "epoch no.6 train no.477860  loss = 3.78463 avg_loss = 2.91970\n",
      "epoch no.6 train no.477870  loss = 2.58879 avg_loss = 2.89410\n",
      "epoch no.6 train no.477880  loss = 1.55701 avg_loss = 2.85207\n",
      "epoch no.6 train no.477890  loss = 2.47598 avg_loss = 2.84565\n",
      "epoch no.6 train no.477900  loss = 2.74289 avg_loss = 2.85509\n",
      "epoch no.6 train no.477910  loss = 2.95547 avg_loss = 2.88324\n",
      "epoch no.6 train no.477920  loss = 2.98115 avg_loss = 2.85549\n",
      "epoch no.6 train no.477930  loss = 3.49397 avg_loss = 2.85647\n",
      "epoch no.6 train no.477940  loss = 2.36368 avg_loss = 2.85716\n",
      "epoch no.6 train no.477950  loss = 3.23833 avg_loss = 2.86703\n",
      "epoch no.6 train no.477960  loss = 3.49763 avg_loss = 2.86564\n",
      "epoch no.6 train no.477970  loss = 2.42845 avg_loss = 2.86873\n",
      "epoch no.6 train no.477980  loss = 3.28391 avg_loss = 2.86955\n",
      "epoch no.6 train no.477990  loss = 2.56166 avg_loss = 2.86856\n",
      "epoch no.6 train no.478000  loss = 2.36186 avg_loss = 2.84548\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '▁o', 'st', '▁명', '곡', '</s>']\n",
      "추억의 드라마 ost 명곡</s>\n",
      "epoch no.6 train no.478010  loss = 2.88840 avg_loss = 2.86691\n",
      "epoch no.6 train no.478020  loss = 2.38098 avg_loss = 2.85463\n",
      "epoch no.6 train no.478030  loss = 1.91198 avg_loss = 2.83585\n",
      "epoch no.6 train no.478040  loss = 1.97051 avg_loss = 2.80730\n",
      "epoch no.6 train no.478050  loss = 1.96500 avg_loss = 2.81555\n",
      "epoch no.6 train no.478060  loss = 3.02406 avg_loss = 2.80047\n",
      "epoch no.6 train no.478070  loss = 2.62327 avg_loss = 2.80743\n",
      "epoch no.6 train no.478080  loss = 1.51770 avg_loss = 2.78152\n",
      "epoch no.6 train no.478090  loss = 2.24780 avg_loss = 2.77883\n",
      "epoch no.6 train no.478100  loss = 3.05615 avg_loss = 2.76815\n",
      "epoch no.6 train no.478110  loss = 3.82537 avg_loss = 2.80128\n",
      "epoch no.6 train no.478120  loss = 3.64118 avg_loss = 2.78745\n",
      "epoch no.6 train no.478130  loss = 3.26171 avg_loss = 2.80509\n",
      "epoch no.6 train no.478140  loss = 2.40228 avg_loss = 2.82399\n",
      "epoch no.6 train no.478150  loss = 3.81149 avg_loss = 2.85147\n",
      "epoch no.6 train no.478160  loss = 2.24724 avg_loss = 2.91064\n",
      "epoch no.6 train no.478170  loss = 3.06477 avg_loss = 2.90849\n",
      "epoch no.6 train no.478180  loss = 5.58071 avg_loss = 2.95876\n",
      "epoch no.6 train no.478190  loss = 3.89515 avg_loss = 2.94968\n",
      "epoch no.6 train no.478200  loss = 2.75444 avg_loss = 2.91516\n",
      "epoch no.6 train no.478210  loss = 3.81173 avg_loss = 2.89762\n",
      "epoch no.6 train no.478220  loss = 2.72511 avg_loss = 2.89535\n",
      "epoch no.6 train no.478230  loss = 4.16915 avg_loss = 2.91274\n",
      "epoch no.6 train no.478240  loss = 2.44635 avg_loss = 2.93118\n",
      "epoch no.6 train no.478250  loss = 3.06661 avg_loss = 2.95990\n",
      "epoch no.6 train no.478260  loss = 2.62200 avg_loss = 2.91800\n",
      "epoch no.6 train no.478270  loss = 3.08659 avg_loss = 2.91311\n",
      "epoch no.6 train no.478280  loss = 3.88541 avg_loss = 2.93889\n",
      "epoch no.6 train no.478290  loss = 2.78180 avg_loss = 2.95086\n",
      "epoch no.6 train no.478300  loss = 3.33306 avg_loss = 2.92714\n",
      "epoch no.6 train no.478310  loss = 2.87658 avg_loss = 2.92030\n",
      "epoch no.6 train no.478320  loss = 2.28556 avg_loss = 2.93819\n",
      "epoch no.6 train no.478330  loss = 2.11364 avg_loss = 2.90338\n",
      "epoch no.6 train no.478340  loss = 4.70084 avg_loss = 2.92914\n",
      "epoch no.6 train no.478350  loss = 2.16218 avg_loss = 2.93007\n",
      "epoch no.6 train no.478360  loss = 2.95701 avg_loss = 2.94355\n",
      "epoch no.6 train no.478370  loss = 3.27295 avg_loss = 2.94595\n",
      "epoch no.6 train no.478380  loss = 2.63260 avg_loss = 2.95601\n",
      "epoch no.6 train no.478390  loss = 2.28637 avg_loss = 2.99616\n",
      "epoch no.6 train no.478400  loss = 3.73808 avg_loss = 3.01094\n",
      "epoch no.6 train no.478410  loss = 2.35000 avg_loss = 2.97431\n",
      "epoch no.6 train no.478420  loss = 4.25695 avg_loss = 2.99406\n",
      "epoch no.6 train no.478430  loss = 2.60840 avg_loss = 2.98276\n",
      "epoch no.6 train no.478440  loss = 3.51623 avg_loss = 3.04132\n",
      "epoch no.6 train no.478450  loss = 3.27267 avg_loss = 3.02894\n",
      "epoch no.6 train no.478460  loss = 4.71287 avg_loss = 3.04206\n",
      "epoch no.6 train no.478470  loss = 2.06119 avg_loss = 3.02534\n",
      "epoch no.6 train no.478480  loss = 2.70632 avg_loss = 3.02479\n",
      "epoch no.6 train no.478490  loss = 2.92308 avg_loss = 2.96944\n",
      "epoch no.6 train no.478500  loss = 3.16644 avg_loss = 2.95207\n",
      "epoch no.6 train no.478510  loss = 2.91032 avg_loss = 2.95606\n",
      "epoch no.6 train no.478520  loss = 3.83787 avg_loss = 2.97251\n",
      "epoch no.6 train no.478530  loss = 3.62046 avg_loss = 2.94151\n",
      "epoch no.6 train no.478540  loss = 3.42326 avg_loss = 2.94605\n",
      "epoch no.6 train no.478550  loss = 2.94460 avg_loss = 2.94481\n",
      "epoch no.6 train no.478560  loss = 2.38738 avg_loss = 2.90122\n",
      "epoch no.6 train no.478570  loss = 1.91662 avg_loss = 2.91096\n",
      "epoch no.6 train no.478580  loss = 2.90010 avg_loss = 2.91322\n",
      "epoch no.6 train no.478590  loss = 2.51034 avg_loss = 2.90586\n",
      "epoch no.6 train no.478600  loss = 2.58861 avg_loss = 2.90122\n",
      "epoch no.6 train no.478610  loss = 3.37544 avg_loss = 2.89283\n",
      "epoch no.6 train no.478620  loss = 3.45296 avg_loss = 2.91828\n",
      "epoch no.6 train no.478630  loss = 2.12010 avg_loss = 2.91588\n",
      "epoch no.6 train no.478640  loss = 3.18663 avg_loss = 2.93245\n",
      "epoch no.6 train no.478650  loss = 2.11800 avg_loss = 2.89158\n",
      "epoch no.6 train no.478660  loss = 2.15874 avg_loss = 2.88674\n",
      "epoch no.6 train no.478670  loss = 3.79966 avg_loss = 2.91610\n",
      "epoch no.6 train no.478680  loss = 2.91460 avg_loss = 2.94558\n",
      "epoch no.6 train no.478690  loss = 2.12008 avg_loss = 2.89932\n",
      "epoch no.6 train no.478700  loss = 2.30875 avg_loss = 2.91150\n",
      "epoch no.6 train no.478710  loss = 2.75248 avg_loss = 2.92797\n",
      "epoch no.6 train no.478720  loss = 3.17294 avg_loss = 2.92943\n",
      "epoch no.6 train no.478730  loss = 3.64650 avg_loss = 2.94838\n",
      "epoch no.6 train no.478740  loss = 4.80513 avg_loss = 2.96003\n",
      "epoch no.6 train no.478750  loss = 3.45634 avg_loss = 2.94904\n",
      "epoch no.6 train no.478760  loss = 2.59880 avg_loss = 2.99066\n",
      "epoch no.6 train no.478770  loss = 3.39736 avg_loss = 2.98796\n",
      "epoch no.6 train no.478780  loss = 2.38773 avg_loss = 2.99395\n",
      "epoch no.6 train no.478790  loss = 3.51389 avg_loss = 2.98996\n",
      "epoch no.6 train no.478800  loss = 2.04400 avg_loss = 2.97915\n",
      "epoch no.6 train no.478810  loss = 2.58493 avg_loss = 2.98678\n",
      "epoch no.6 train no.478820  loss = 3.57288 avg_loss = 2.96637\n",
      "epoch no.6 train no.478830  loss = 2.34910 avg_loss = 2.93864\n",
      "epoch no.6 train no.478840  loss = 3.08103 avg_loss = 2.96274\n",
      "epoch no.6 train no.478850  loss = 3.47648 avg_loss = 2.94269\n",
      "epoch no.6 train no.478860  loss = 1.87486 avg_loss = 2.92399\n",
      "epoch no.6 train no.478870  loss = 3.54756 avg_loss = 2.94204\n",
      "epoch no.6 train no.478880  loss = 2.74503 avg_loss = 2.93100\n",
      "epoch no.6 train no.478890  loss = 3.58017 avg_loss = 2.92734\n",
      "epoch no.6 train no.478900  loss = 3.04344 avg_loss = 2.96895\n",
      "epoch no.6 train no.478910  loss = 3.59340 avg_loss = 2.94409\n",
      "epoch no.6 train no.478920  loss = 3.36688 avg_loss = 2.92835\n",
      "epoch no.6 train no.478930  loss = 3.36406 avg_loss = 2.94152\n",
      "epoch no.6 train no.478940  loss = 3.15120 avg_loss = 2.97706\n",
      "epoch no.6 train no.478950  loss = 3.88484 avg_loss = 2.95231\n",
      "epoch no.6 train no.478960  loss = 5.02498 avg_loss = 2.95769\n",
      "epoch no.6 train no.478970  loss = 3.94942 avg_loss = 2.99794\n",
      "epoch no.6 train no.478980  loss = 2.25107 avg_loss = 3.01608\n",
      "epoch no.6 train no.478990  loss = 2.98898 avg_loss = 3.05068\n",
      "epoch no.6 train no.479000  loss = 2.74399 avg_loss = 3.04029\n",
      "4\n",
      "to_tokens: ['▁비', '▁90', '드', '▁명', '곡', '</s>']\n",
      "추억의 발라드 명곡</s>\n",
      "epoch no.6 train no.479010  loss = 2.39752 avg_loss = 3.03117\n",
      "epoch no.6 train no.479020  loss = 3.35330 avg_loss = 3.08761\n",
      "epoch no.6 train no.479030  loss = 2.58375 avg_loss = 3.06171\n",
      "epoch no.6 train no.479040  loss = 1.88067 avg_loss = 3.02819\n",
      "epoch no.6 train no.479050  loss = 2.17912 avg_loss = 3.00248\n",
      "epoch no.6 train no.479060  loss = 3.22320 avg_loss = 2.98195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.479070  loss = 3.78020 avg_loss = 2.99861\n",
      "epoch no.6 train no.479080  loss = 4.89159 avg_loss = 2.98193\n",
      "epoch no.6 train no.479090  loss = 3.14342 avg_loss = 2.97727\n",
      "epoch no.6 train no.479100  loss = 1.58639 avg_loss = 2.99078\n",
      "epoch no.6 train no.479110  loss = 2.70333 avg_loss = 3.00566\n",
      "epoch no.6 train no.479120  loss = 1.98139 avg_loss = 2.96812\n",
      "epoch no.6 train no.479130  loss = 2.31036 avg_loss = 2.95575\n",
      "epoch no.6 train no.479140  loss = 2.36255 avg_loss = 2.92550\n",
      "epoch no.6 train no.479150  loss = 1.93466 avg_loss = 2.94693\n",
      "epoch no.6 train no.479160  loss = 3.11198 avg_loss = 2.90872\n",
      "epoch no.6 train no.479170  loss = 2.21414 avg_loss = 2.94030\n",
      "epoch no.6 train no.479180  loss = 3.76800 avg_loss = 2.97430\n",
      "epoch no.6 train no.479190  loss = 3.28277 avg_loss = 2.98835\n",
      "epoch no.6 train no.479200  loss = 3.70380 avg_loss = 2.95443\n",
      "epoch no.6 train no.479210  loss = 2.78625 avg_loss = 2.93527\n",
      "epoch no.6 train no.479220  loss = 1.92089 avg_loss = 2.93779\n",
      "epoch no.6 train no.479230  loss = 3.27474 avg_loss = 2.92869\n",
      "epoch no.6 train no.479240  loss = 2.57664 avg_loss = 2.93905\n",
      "epoch no.6 train no.479250  loss = 3.28983 avg_loss = 2.93888\n",
      "epoch no.6 train no.479260  loss = 3.60922 avg_loss = 2.92043\n",
      "epoch no.6 train no.479270  loss = 4.00298 avg_loss = 2.91823\n",
      "epoch no.6 train no.479280  loss = 2.94113 avg_loss = 2.88130\n",
      "epoch no.6 train no.479290  loss = 2.69378 avg_loss = 2.86412\n",
      "epoch no.6 train no.479300  loss = 2.07089 avg_loss = 2.86932\n",
      "epoch no.6 train no.479310  loss = 2.35934 avg_loss = 2.85246\n",
      "epoch no.6 train no.479320  loss = 4.28446 avg_loss = 2.83395\n",
      "epoch no.6 train no.479330  loss = 3.05877 avg_loss = 2.86827\n",
      "epoch no.6 train no.479340  loss = 3.06100 avg_loss = 2.90772\n",
      "epoch no.6 train no.479350  loss = 3.64741 avg_loss = 2.89305\n",
      "epoch no.6 train no.479360  loss = 3.76543 avg_loss = 2.90311\n",
      "epoch no.6 train no.479370  loss = 1.78984 avg_loss = 2.85828\n",
      "epoch no.6 train no.479380  loss = 2.81013 avg_loss = 2.87440\n",
      "epoch no.6 train no.479390  loss = 2.68820 avg_loss = 2.87638\n",
      "epoch no.6 train no.479400  loss = 3.21554 avg_loss = 2.87143\n",
      "epoch no.6 train no.479410  loss = 2.11821 avg_loss = 2.87462\n",
      "epoch no.6 train no.479420  loss = 2.22878 avg_loss = 2.87780\n",
      "epoch no.6 train no.479430  loss = 2.83870 avg_loss = 2.90853\n",
      "epoch no.6 train no.479440  loss = 3.81992 avg_loss = 2.92730\n",
      "epoch no.6 train no.479450  loss = 2.47203 avg_loss = 2.90339\n",
      "epoch no.6 train no.479460  loss = 2.90905 avg_loss = 2.89156\n",
      "epoch no.6 train no.479470  loss = 2.58298 avg_loss = 2.90115\n",
      "epoch no.6 train no.479480  loss = 2.76567 avg_loss = 2.88919\n",
      "epoch no.6 train no.479490  loss = 5.21344 avg_loss = 2.90806\n",
      "epoch no.6 train no.479500  loss = 2.25014 avg_loss = 2.90082\n",
      "epoch no.6 train no.479510  loss = 3.62781 avg_loss = 2.91317\n",
      "epoch no.6 train no.479520  loss = 4.48974 avg_loss = 2.94060\n",
      "epoch no.6 train no.479530  loss = 2.43294 avg_loss = 2.92431\n",
      "epoch no.6 train no.479540  loss = 2.73834 avg_loss = 2.91264\n",
      "epoch no.6 train no.479550  loss = 2.93943 avg_loss = 2.90539\n",
      "epoch no.6 train no.479560  loss = 3.59042 avg_loss = 2.95177\n",
      "epoch no.6 train no.479570  loss = 2.95841 avg_loss = 2.91533\n",
      "epoch no.6 train no.479580  loss = 3.20483 avg_loss = 2.90922\n",
      "epoch no.6 train no.479590  loss = 4.52164 avg_loss = 2.96230\n",
      "epoch no.6 train no.479600  loss = 3.31922 avg_loss = 2.99476\n",
      "epoch no.6 train no.479610  loss = 2.54909 avg_loss = 2.99685\n",
      "epoch no.6 train no.479620  loss = 2.42486 avg_loss = 2.97208\n",
      "epoch no.6 train no.479630  loss = 3.43928 avg_loss = 2.95682\n",
      "epoch no.6 train no.479640  loss = 3.86807 avg_loss = 2.98286\n",
      "epoch no.6 train no.479650  loss = 3.64716 avg_loss = 2.99511\n",
      "epoch no.6 train no.479660  loss = 3.59908 avg_loss = 3.01747\n",
      "epoch no.6 train no.479670  loss = 2.99605 avg_loss = 3.03136\n",
      "epoch no.6 train no.479680  loss = 2.91522 avg_loss = 2.99272\n",
      "epoch no.6 train no.479690  loss = 2.27183 avg_loss = 2.95134\n",
      "epoch no.6 train no.479700  loss = 2.61255 avg_loss = 2.91605\n",
      "epoch no.6 train no.479710  loss = 2.52651 avg_loss = 2.90878\n",
      "epoch no.6 train no.479720  loss = 1.84671 avg_loss = 2.91531\n",
      "epoch no.6 train no.479730  loss = 4.24016 avg_loss = 2.94465\n",
      "epoch no.6 train no.479740  loss = 3.72507 avg_loss = 2.99579\n",
      "epoch no.6 train no.479750  loss = 1.76377 avg_loss = 2.95759\n",
      "epoch no.6 train no.479760  loss = 3.26128 avg_loss = 2.98593\n",
      "epoch no.6 train no.479770  loss = 2.17595 avg_loss = 2.98959\n",
      "epoch no.6 train no.479780  loss = 1.87695 avg_loss = 2.99741\n",
      "epoch no.6 train no.479790  loss = 1.70531 avg_loss = 2.96546\n",
      "epoch no.6 train no.479800  loss = 4.02916 avg_loss = 2.92633\n",
      "epoch no.6 train no.479810  loss = 2.50212 avg_loss = 2.92257\n",
      "epoch no.6 train no.479820  loss = 4.53527 avg_loss = 2.93936\n",
      "epoch no.6 train no.479830  loss = 3.08727 avg_loss = 2.94612\n",
      "epoch no.6 train no.479840  loss = 3.62927 avg_loss = 2.94259\n",
      "epoch no.6 train no.479850  loss = 2.23289 avg_loss = 2.90586\n",
      "epoch no.6 train no.479860  loss = 2.50422 avg_loss = 2.89324\n",
      "epoch no.6 train no.479870  loss = 2.75388 avg_loss = 2.85495\n",
      "epoch no.6 train no.479880  loss = 2.95697 avg_loss = 2.89710\n",
      "epoch no.6 train no.479890  loss = 2.09840 avg_loss = 2.90034\n",
      "epoch no.6 train no.479900  loss = 3.23219 avg_loss = 2.89625\n",
      "epoch no.6 train no.479910  loss = 2.46465 avg_loss = 2.89086\n",
      "epoch no.6 train no.479920  loss = 2.04701 avg_loss = 2.90416\n",
      "epoch no.6 train no.479930  loss = 3.46971 avg_loss = 2.86459\n",
      "epoch no.6 train no.479940  loss = 3.29166 avg_loss = 2.90623\n",
      "epoch no.6 train no.479950  loss = 3.62817 avg_loss = 2.92970\n",
      "epoch no.6 train no.479960  loss = 3.63660 avg_loss = 2.92213\n",
      "epoch no.6 train no.479970  loss = 2.44558 avg_loss = 2.92249\n",
      "epoch no.6 train no.479980  loss = 2.45624 avg_loss = 2.93914\n",
      "epoch no.6 train no.479990  loss = 2.28592 avg_loss = 2.94818\n",
      "epoch no.6 train no.480000  loss = 3.01570 avg_loss = 2.95106\n",
      "5\n",
      "to_tokens: ['▁비', '▁2000', '년대', '▁발라', '들', '</s>']\n",
      "추억의 90년대  노래들</s>\n",
      "epoch no.6 train no.480010  loss = 2.53884 avg_loss = 2.94568\n",
      "epoch no.6 train no.480020  loss = 1.77462 avg_loss = 2.92204\n",
      "epoch no.6 train no.480030  loss = 4.46436 avg_loss = 2.93948\n",
      "epoch no.6 train no.480040  loss = 4.34666 avg_loss = 2.97741\n",
      "epoch no.6 train no.480050  loss = 3.24901 avg_loss = 2.97478\n",
      "epoch no.6 train no.480060  loss = 3.83206 avg_loss = 2.95933\n",
      "epoch no.6 train no.480070  loss = 5.24002 avg_loss = 3.01226\n",
      "epoch no.6 train no.480080  loss = 3.24342 avg_loss = 3.01454\n",
      "epoch no.6 train no.480090  loss = 3.56610 avg_loss = 3.04227\n",
      "epoch no.6 train no.480100  loss = 4.05941 avg_loss = 3.01797\n",
      "epoch no.6 train no.480110  loss = 3.89462 avg_loss = 2.98345\n",
      "epoch no.6 train no.480120  loss = 2.73600 avg_loss = 2.96198\n",
      "epoch no.6 train no.480130  loss = 2.78755 avg_loss = 2.95998\n",
      "epoch no.6 train no.480140  loss = 2.18480 avg_loss = 2.98858\n",
      "epoch no.6 train no.480150  loss = 2.33188 avg_loss = 2.96900\n",
      "epoch no.6 train no.480160  loss = 3.22840 avg_loss = 2.94348\n",
      "epoch no.6 train no.480170  loss = 1.59855 avg_loss = 2.89215\n",
      "epoch no.6 train no.480180  loss = 3.06859 avg_loss = 2.87350\n",
      "epoch no.6 train no.480190  loss = 1.87889 avg_loss = 2.90044\n",
      "epoch no.6 train no.480200  loss = 4.70032 avg_loss = 2.93887\n",
      "epoch no.6 train no.480210  loss = 2.45612 avg_loss = 2.92684\n",
      "epoch no.6 train no.480220  loss = 3.81830 avg_loss = 2.93182\n",
      "epoch no.6 train no.480230  loss = 3.48422 avg_loss = 2.88773\n",
      "epoch no.6 train no.480240  loss = 3.42698 avg_loss = 2.89972\n",
      "epoch no.6 train no.480250  loss = 3.41059 avg_loss = 2.86251\n",
      "epoch no.6 train no.480260  loss = 2.94295 avg_loss = 2.87284\n",
      "epoch no.6 train no.480270  loss = 2.79377 avg_loss = 2.85971\n",
      "epoch no.6 train no.480280  loss = 3.05401 avg_loss = 2.87572\n",
      "epoch no.6 train no.480290  loss = 3.42462 avg_loss = 2.89113\n",
      "epoch no.6 train no.480300  loss = 3.43965 avg_loss = 2.88924\n",
      "epoch no.6 train no.480310  loss = 3.19171 avg_loss = 2.87987\n",
      "epoch no.6 train no.480320  loss = 3.25592 avg_loss = 2.91900\n",
      "epoch no.6 train no.480330  loss = 4.00205 avg_loss = 2.97111\n",
      "epoch no.6 train no.480340  loss = 2.26673 avg_loss = 2.91618\n",
      "epoch no.6 train no.480350  loss = 2.34097 avg_loss = 2.90158\n",
      "epoch no.6 train no.480360  loss = 4.46138 avg_loss = 2.90885\n",
      "epoch no.6 train no.480370  loss = 2.77911 avg_loss = 2.89606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.480380  loss = 1.94956 avg_loss = 2.91643\n",
      "epoch no.6 train no.480390  loss = 1.70753 avg_loss = 2.93151\n",
      "epoch no.6 train no.480400  loss = 2.61731 avg_loss = 2.92263\n",
      "epoch no.6 train no.480410  loss = 2.52392 avg_loss = 2.91553\n",
      "epoch no.6 train no.480420  loss = 3.36767 avg_loss = 2.89605\n",
      "epoch no.6 train no.480430  loss = 2.82783 avg_loss = 2.91024\n",
      "epoch no.6 train no.480440  loss = 3.38616 avg_loss = 2.90430\n",
      "epoch no.6 train no.480450  loss = 3.67427 avg_loss = 2.91642\n",
      "epoch no.6 train no.480460  loss = 2.35607 avg_loss = 2.89071\n",
      "epoch no.6 train no.480470  loss = 2.97669 avg_loss = 2.92795\n",
      "epoch no.6 train no.480480  loss = 4.07766 avg_loss = 2.95514\n",
      "epoch no.6 train no.480490  loss = 3.15761 avg_loss = 2.96712\n",
      "epoch no.6 train no.480500  loss = 2.50363 avg_loss = 2.97575\n",
      "epoch no.6 train no.480510  loss = 3.01511 avg_loss = 2.93594\n",
      "epoch no.6 train no.480520  loss = 3.19088 avg_loss = 2.95549\n",
      "epoch no.6 train no.480530  loss = 3.11060 avg_loss = 2.93659\n",
      "epoch no.6 train no.480540  loss = 2.71457 avg_loss = 2.89408\n",
      "epoch no.6 train no.480550  loss = 2.33513 avg_loss = 2.88069\n",
      "epoch no.6 train no.480560  loss = 3.25640 avg_loss = 2.88404\n",
      "epoch no.6 train no.480570  loss = 3.12446 avg_loss = 2.86659\n",
      "epoch no.6 train no.480580  loss = 2.37649 avg_loss = 2.91454\n",
      "epoch no.6 train no.480590  loss = 3.12756 avg_loss = 2.91633\n",
      "epoch no.6 train no.480600  loss = 3.43947 avg_loss = 2.92876\n",
      "epoch no.6 train no.480610  loss = 3.93530 avg_loss = 3.01936\n",
      "epoch no.6 train no.480620  loss = 2.94392 avg_loss = 2.99216\n",
      "epoch no.6 train no.480630  loss = 1.79313 avg_loss = 2.94857\n",
      "epoch no.6 train no.480640  loss = 3.50719 avg_loss = 2.98698\n",
      "epoch no.6 train no.480650  loss = 1.40227 avg_loss = 2.96981\n",
      "epoch no.6 train no.480660  loss = 2.92272 avg_loss = 2.93271\n",
      "epoch no.6 train no.480670  loss = 3.26456 avg_loss = 3.00117\n",
      "epoch no.6 train no.480680  loss = 2.18013 avg_loss = 3.00689\n",
      "epoch no.6 train no.480690  loss = 3.17661 avg_loss = 2.95805\n",
      "epoch no.6 train no.480700  loss = 2.42414 avg_loss = 2.97597\n",
      "epoch no.6 train no.480710  loss = 3.17830 avg_loss = 2.96991\n",
      "epoch no.6 train no.480720  loss = 3.83357 avg_loss = 3.00453\n",
      "epoch no.6 train no.480730  loss = 2.77885 avg_loss = 2.97924\n",
      "epoch no.6 train no.480740  loss = 3.54172 avg_loss = 3.00618\n",
      "epoch no.6 train no.480750  loss = 3.79904 avg_loss = 3.03019\n",
      "epoch no.6 train no.480760  loss = 3.80253 avg_loss = 3.01654\n",
      "epoch no.6 train no.480770  loss = 2.72398 avg_loss = 3.01819\n",
      "epoch no.6 train no.480780  loss = 3.23953 avg_loss = 3.00095\n",
      "epoch no.6 train no.480790  loss = 2.03963 avg_loss = 2.99458\n",
      "epoch no.6 train no.480800  loss = 3.12829 avg_loss = 3.02941\n",
      "epoch no.6 train no.480810  loss = 2.72119 avg_loss = 3.02163\n",
      "epoch no.6 train no.480820  loss = 2.27103 avg_loss = 2.99787\n",
      "epoch no.6 train no.480830  loss = 2.94298 avg_loss = 3.02312\n",
      "epoch no.6 train no.480840  loss = 2.72634 avg_loss = 3.00587\n",
      "epoch no.6 train no.480850  loss = 4.05747 avg_loss = 3.04160\n",
      "epoch no.6 train no.480860  loss = 2.46474 avg_loss = 3.02477\n",
      "epoch no.6 train no.480870  loss = 1.86049 avg_loss = 3.01388\n",
      "epoch no.6 train no.480880  loss = 1.69126 avg_loss = 2.98914\n",
      "epoch no.6 train no.480890  loss = 3.18852 avg_loss = 2.98217\n",
      "epoch no.6 train no.480900  loss = 2.11182 avg_loss = 2.97576\n",
      "epoch no.6 train no.480910  loss = 3.89112 avg_loss = 3.01103\n",
      "epoch no.6 train no.480920  loss = 3.58742 avg_loss = 3.00109\n",
      "epoch no.6 train no.480930  loss = 3.25697 avg_loss = 3.02530\n",
      "epoch no.6 train no.480940  loss = 2.47334 avg_loss = 3.02888\n",
      "epoch no.6 train no.480950  loss = 2.87374 avg_loss = 3.01944\n",
      "epoch no.6 train no.480960  loss = 2.36847 avg_loss = 3.01636\n",
      "epoch no.6 train no.480970  loss = 3.18822 avg_loss = 3.01609\n",
      "epoch no.6 train no.480980  loss = 2.27448 avg_loss = 3.02219\n",
      "epoch no.6 train no.480990  loss = 2.65687 avg_loss = 3.00731\n",
      "epoch no.6 train no.481000  loss = 2.79958 avg_loss = 3.02244\n",
      "Killed\n",
      "time : 28664.81615304947\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "!python3 main.py --epoch=200 --data_file_path=./data/train4_없앤_space.csv --save_path=./checkpoint/1122cp/ --batch_size=1\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
